Under review as a conference paper at ICLR 2022
Learning to Actively Learn: A Robust Ap-
PROACH
Anonymous authors
Paper under double-blind review
Ab stract
This work proposes a procedure for designing algorithms for specific adaptive
data collection tasks like active learning and pure-exploration multi-armed ban-
dits. Unlike the design of traditional adaptive algorithms that rely on concentration
of measure and careful analysis to justify the correctness and sample complexity
of the procedure, our adaptive algorithm is learned via adversarial training over
equivalence classes of problems derived from information theoretic lower bounds.
In particular, a single adaptive learning algorithm is learned that competes with the
best adaptive algorithm learned for each equivalence class. Our procedure takes
as input just the available queries, set of hypotheses, loss function, and total query
budget. This is in contrast to existing meta-learning work that learns an adap-
tive algorithm relative to an explicit, user-defined subset or prior distribution over
problems which can be challenging to define and be mismatched to the instance
encountered at test time. This work is particularly focused on the regime when the
total query budget is very small, such as a few dozen, which is much smaller than
those budgets typically considered by theoretically derived algorithms. We per-
form synthetic experiments to justify the stability and effectiveness of the training
procedure, and then evaluate the method on tasks derived from real data including
a noisy 20 Questions game and a joke recommendation task.
1	Introduction
Closed-loop learning algorithms use previous observations to inform what measurements to take
next in a closed loop in order to accomplish inference tasks far faster than any fixed measurement
plan set in advance. For example, active learning algorithms for binary classification have been
proposed that under favorable conditions require exponentially fewer labels than passive, random
sampling to identify the optimal classifier (Hanneke et al., 2014; Katz-Samuels et al., 2021). And
in the multi-armed bandits literature, adaptive sampling techniques have demonstrated the ability to
identify the “best arm” that optimizes some metric with far fewer experiments than a fixed design
(Garivier & Kaufmann, 2016; Fiez et al., 2019). Unfortunately, such guarantees often either require
simplifying assumptions that limit robustness and applicability, or algorithmic use of concentration
inequalities that are very loose unless the number of samples is very large.
This work proposes a framework for producing algorithms that are learned through simulated expe-
rience to be as effective and robust as possible, even on a tiny measurement budget (e.g., 20 queries)
where most theoretical guarantees do not apply. Our work fits into a recent trend sometimes referred
to as learning to actively learn and differentiable meta-learning in bandits (Konyushkova et al.,
2017; Bachman et al., 2017; Fang et al., 2017; Boutilier et al., 2020; Kveton et al., 2020) which tune
existing algorithms or learn entirely new active learning algorithms by policy optimization. Previous
works in this area learn a policy by optimizing with respect to data observed through prior experi-
ence (e.g., meta-learning or transfer learning) or an assumed explicit prior distribution of problem
parameters (e.g. a Gaussian prior over the true weight vector for linear regression). In contrast, our
approach makes no assumptions about what parameters are likely to be encountered at test time,
and therefore produces algorithms that do not suffer from mismatching priors at test time. Instead,
our method learns a policy that attempts to mirror the guarantees of frequentist algorithms with in-
stance dependent sample complexities: there is an intrinsic difficulty measure that orders problem
instances and given a fixed budget, higher accuracies can be obtained for all easier instances than
1
Under review as a conference paper at ICLR 2022
harder instances. This difficulty measure is most naturally derived from information theoretic lower
bounds.
But unlike information theoretic bounds that hand-craft adversarial instances, inspired by the robust
reinforcement learning literature, we formulate a novel adversarial training objective that automat-
ically train minimax policies and propose a tractable and computationally efficient relaxation. This
allows our learned policies to be very aggressive while maintaining robustness over difficulty in
problem instances, without resorting to using loose concentration inequalities in the algorithm. In-
deed, this work is particularly useful in the setting where relatively few rounds of querying can be
made. The learning framework is general enough to be applied to many active learning settings of
interest and is intended to be used to produce robust and high performing algorithms. We imple-
ment the framework for the pure-exploration combinatorial bandit problem — a paradigm including
problems such as active binary classification and the 20 question game. We empirically validate our
framework on a simple synthetic experiment before turning our attention to datasets derived from
real data including a noisy 20 Questions game and a joke recommendation task which are also em-
bedded as combinatorial bandits. As demonstrated in our experiments, in the low budget setting, our
learned algorithms are the only ones that both enjoy robustness guarantees (as opposed to greedy
and existing learning to actively learn methods) and perform non-vacuously and instance-optimally
(as opposed to statistically justified algorithms).
2	Proposed Framework for Robust Learning to Actively Learn
From a birds-eye perspective, whether learned or defined by an expert, any algorithm for active
learning can be thought of as a policy from the perspective of reinforcement learning. To be precise,
at time t, based on an internal state st, the policy π defines a distribution π(st) over the set of
potential actions X. It then takes action Xt ∈ X,xt 〜 ∏(st) and receives observation yt, updates
the state and the process repeats.
Fix a horizon T ∈ N, and a problem instance θ* ∈ Θ ⊆ Rd which parameterizes the observation
distribution. For t = 1, 2, . . . , T
•	state st ∈ S is a function of the history, {(xi, yi)}it=-11,
•	action xt ∈ X is drawn at random from the distribution π(st) defined over X, and
•	next state st+ι ∈ S is constructed by taking action Xt in state St and observing yt 〜f (∙∣θ* ,st,χt)
until the game terminates at time t = T and the learner receives a loss LT which is task specific.
Note that LT is a random variable that depends on the tuple (∏, {(xi, yi)}T=ι, θ*). We assume that
f is a known parametric distribution to the policy but the parameter θ is unknown to the policy. Let
Pπ,θ, Eπ,θ denote the probability and expectation under the probability law induced by executing
policy ∏ in the game with θ* = θ to completion. Note that P∏,θ includes any internal randomness of
the policy ∏ and the random observations yt 〜f (∙∣θ, st,xt). Thus, P∏,θ assigns a probability to any
trajectory {(χi,yi)}T=ι∙ For a given policy ∏ and θ* = θ, the metric of interest We wish to minimize
is the expected loss '(∏, θ) := E∏,θ [Lt] where LT as defined above is the loss observed at the end
of the episode. For a fixed policy ∏, '(∏, θ) defines a loss surface over all possible values of θ. This
loss surface captures the fact that some values of θ are just intrinsically harder than others, but also
that a policy may be better suited for some values of θ versus others.
Finally, we assume we are equipped with a positive function C : Θ → (0, ∞) that assigns a score
to each θ ∈ Θ that intuitively captures the “difficulty” of a particular θ, and can be used as a partial
ordering of Θ. Ideally, C(θ) is a monotonic transformation of '(∏*,θ) for some “best” policy ∏*
that we will define shortly. Our plan is now as follows, in Section 2.1, we ground the discussion
and describe C(θ) for the combinatorial bandit problem. Then in Section 2.2, we zoom out to define
our main objective of finding a min-gap optimal policy, finally providing an adversarial training
approach in Section 3.
2.1	Complexity for Combinatorial Bandits
A concrete example of the framework above is the combinatorial bandit problem. The learner has
access to sets X = {eι, ∙∙∙ , ed} ⊂ Rd, where ei is the i-th standard basis vector, and Z ⊂ {0,1}d.
In each round the learner chooses an Xt ∈ X according to a policy π({(Xi, yi)}it=-11) and observes
2
Under review as a conference paper at ICLR 2022
yt with E[yt∣xt, θ*] = g, θ*> for some unknown θ* ∈ Rd. The goal of the learner is BEST
Arm Identification. Denote z*(θ*) = arg maxz∈z hz,θ*i, then at time T the learner outputs
a recommendation Z and incurs loss Lbait = 1{z* = z}. This setting naturally captures the 20
question game. indeed assume there are d T = 20 potential yes/no questions that can be asked
at each time, corresponding to the elements of X, and that each element of Z is a binary vector
representing the answers to these questions for a given item. if answers yt are deterministic then
θ* ∈ {-1,1}d, but this framework also captures the case θ* ∈ [-1,1]d when answers are stochastic,
or answered incorrectly with some probability. Then a policy π at each time decides which question
to ask based on the answers so far to determine the item closest to an unknown vector θ*.
As described in Sections 5 and Appendix A, combinatorial bandits generalizes standard multi-
armed bandits, and all of binary classification, and thus has received a tremendous amount of
interest in recent years. A large portion of this work has focused on providing precise charac-
terization of the information theoretic limit on the mimimal number of samples needed to iden-
tify z*(θ*) with high probability a quantity denoted as ρ*(θ*) which is the solution to an op-
timization problem (SOare et al., 2014; Fiez et al., 2019; Degenne et al., 2020) ρ*(θ*)-1 :=
maxλ∈4χ min θ>∈θ	Pχ∈χ λχ(x, θ* — θ0)2 for some set of alternatives Θ. This quantity
z*(θ0)=z*(θ*)
provides a natural complexity measure C(θ*) = ρ*(θ*) fora given instance θ* and we describe it in
a few specific cases below.
As a warmup example, consider the standard best-arm identification problem where Z = X =
{ei : i ∈ [d]} and choosing action Xt ∈ X results in reward yt 〜 BemOUlli(θij. Let i*(θ)=
argmaxz∈z z>θ = argmaxg θ%. Then in this case ρ*(θ) ≈ Pi=i*(θ)R*(θ) — θi)-2 and it,s been
shown that there exists a constant c0 > 0 such that for any sufficiently large ν > 0 we have
min max 'bai(∏, θ) ≥ exp(-c°T∕ν)
π θ:p* (θ)≤ν
in other words, more difficult instances correspond to θ with a small gap between the best arm
and any other arm. Moreover, for any θ ∈ Rd there exists a policy e that achieves '(∏, θ) ≤
c1 exp(-czT∕ρ*(θ*)) where c1,c2 capture constant and low-order terms (Carpentier & Locatelli,
2016; Karnin et al., 2013; Garivier & Kaufmann, 2016). Said plainly, the above correspondence
between the lower bound and the upper bound for the multi-armed bandit problem shows that ρ*(θ*)
is a natural choice for C(θ) in this setting.
in recent years, algorithms for the more general combinatorial bandit setting have been established
with instance-dependent sample complexities matching ρ*(θ*) (up to logarithmic factors) (Karnin
et al., 2013; Chen et al., 2014; Fiez et al., 2019; Chen et al., 2017; Degenne et al., 2020; Katz-
Samuels et al., 2020). Another complexity term that appears in Cao & Krishnamurthy (2017) for
combinatorial bandits is
d
ρe(θ) = X max
i=1 z:zi 6=z*,i (θ)
kz-z*(θ)k2
hz -z*(θ),θi2 .
(1)
One can show ρ* (θ) ≤ e(θ) (Katz-Samuels et al., 2020) and in many cases track each other. Because
p(θ) can be computed much more efficiently compared to ρ*(θ), we take C(θ) = e(θ).
2.2	Objective: Responding to All Difficulties
As described above, though there exists algorithms for the combinatorial bandit problem that are
instance-optimal in the fixed-confidence setting along with algorithms for the fixed-budget, they do
not work well with small budgets as they rely on statistical guarantees. indeed, for their guarantees
tobe non-vacuous, we need the budget T tobe sufficiently large enough to compare to the potentially
large constants in upper bounds. in practice, they are so conservative that for the first 20 samples
they would just sample uniformly. To overcome this, we now provide a different framework that for
policy learning in a worst-case setting that is effective even in the small budget regime.
The challenge is in finding a policy that performs well across all potential problem instances si-
multaneously. it is common to consider minimax optimal policies which attempt to perform well
on worst case instances — but as a result, may perform poorly on easier instances. Thus, an ideal
policy ∏ would perform uniformly well over a set of θ′s that are all equivalent in “difficulty”. Since
3
Under review as a conference paper at ICLR 2022
each θ ∈ Θ is equipped with an inherent notion of difficulty, C(θ), we can stratify the space of
all possible instances by difficulty. A good policy is one whose worst case performance over all
possible problem difficulties is minimized. We formalize this idea below.
For any set of problem instances Θ ⊂ Θ and r ≥ 0 define
'(π, Θ) := max'(∏,θ)	and	Θ(r) := {θ : C(θ) ≤ r}.
θ∈Θe
For a fixed r > 0 (including r = ∞), a policy π0 that aims to minimize just '(π0, Θ(r)) will be
minimax for Θ(r) and may not perform well on easy instances. To overcome this shortsightedness
we introduce a new objective by focusing on '(π, Θ(r)) — min∏o '(π0, Θ(r)); the sub-optimality
gap of a given policy π relative to an r-dependent baseline policy trained specifically for each r.
Objective: Return the policy
π* := arg min max ('(π, Θ(r)) - min'(π0, O(r)))	(2)
π r>0	π0
which minimizes the worst case sub-optimality gap over all r > 0.
Figure 1 illustrates these definitions. The blue curve (r-
dependent baseline) captures the best possible performance
min∏o '(π0, Θ(r)) that is possible for each difficulty level r.
In other words, the r-dependent baseline defines a different
policy for each value of r. Therefore, the blue curve may be
unachievable with just a single policy. The green curve cap-
tures a policy that achieves the minima (r-dependent baseline)
at a given r0 . Though it is the ideal policy for this difficulty, it
could be sub-optimal at any other difficulty. The orange curve
is the performance of our optimal policy π* ——it is willing
to sacrifice performance for any given r to achieve an overall
better worst case gap from the baseline.
Figure 1: Performance curves for var-
ious policies.
3	MAPO: Adversarial Training Algorithm
Identifying ∏ naively requires the computation of min∏o '(π0, Θ(r)) for all r > 0. However, in
practice given an increasing sequence ri < …< rκ that indexes nested sets of problem instances
of increasing difficulty, Θ(r1) ⊂ Θ(r2) ⊂ •一⊂ Θ(rK), we wish to identify a policy b that minimizes
the maximum sub-optimality gap with respect to this sequence. Explicitly, we seek to learn
b = arg min max ('(∏, Θ(rk)) - '(∏k, Θ(rk))) where	∏k ∈ arg min max '(π,θ).	(3)
∏ k≤K	∏ θ((θ)≤rk
Note that as K → ∞ and SuPk r⅛+1 → 1, equation 2 and equation 3 are essentially equivalent
rk
under benign smoothness conditions on C(θ), in which case b → π*. In practice, We choose Θ(rK)
contains all problems that can be solved within the budget T relatively accurately, and a small > 0,
where maxk rk+1 = 1 + e. In Algorithm 1, our algorithm MAPO efficiently solves this objective
rk
by first computing ∏ for all k ∈ [K] to obtain '(∏k, Θ(rk)) as benchmarks, and then uses these
benchmarks to train πb. The next section will focus on the challenges of the optimization problems
in equation 4 and equation 5.
3.1	Differentiable policy optimization
The critical part of running MAPO (Algorithm 1) is to solve for equation 4 and equation 5. Note
that equation 5 is an optimization of the same form with equation 4 after shifting the loss by the
scalar value b(rk(θ)). Consequently, to learn {πek}k and πb, it suffices to develop a training procedure
to solve min∏ maxθ∈Ω '0(∏, θ) for an arbitrary set Ω and generic loss function '0(π, θ).
We would like to solve this saddle-point problem using an alternating gradient descent/ascent
method in Algorithm 2 that we describe now. Instead of optimizing over all possible policies, we
4
Under review as a conference paper at ICLR 2022
Algorithm 1 MAPO: Min-gap Adversarial Policy Optimization
Input: sequence {rk}kK=1, complexity function C.
Define k(θ) ∈ [K] such that rk(θ)-1 < C(θ) ≤ rk(θ) for all θ with C(θ) ≤ rK.
for k ∈ 1, ..., K do
Obtain policy πk by solving:
∏k := arg min'(π, Θ(rk)) = arg min max '(π,θ)	and	b(rk) := '(∏k, Θ(rk))	(4)
π	π	θ∈Θ(rk)
end for
Training for min-gap optimal policy: Solve the following:
^ ∙
π = arg min max
π	θ∈Θ(rK)
h'(π,θ) - b(rk(叫
(5)
Output: πb (a solution to equation 3).
restrict the policy class to neural networks that take state representation as input and output a prob-
ability distribution over actions, parameterized by weights ψ. In practice, '0(∏ψ,θ) may be poorly
behaved in (ψ, θ) so a gradient descent/ascent procedure may get stuck in a neighborhood of a crit-
ical point that is not an optimal solution to the saddle point problem. To avoid this, we instead track
over many different possible θ's (intuitively corresponding to different initializations):
min max '0(πψ ,θ) = min max max '0(πψ, θi).
ψ θ∈ω	ψ θiN⊂Ω i∈[N]
= min max max Ei〜λ'0(∏ψ,θi).
ψ el：N⊂Ω X∈δn
(6)
(7)
(8)
min max
ψ w∈RN ,θι∙N ⊂Ω
人 SOFTMAX(W) ['0(∏ψ,θi)].
In the first equality We replace the maximum over all Ω to a maximum over all subsets Θ = θ±N of
size N. The resulting maximum over the N points is still a discrete optimization. To smooth it out,
we utilize the fact that a max over a set is just the same as the maximum over of the expectation over
all distributions on that set. In the last equality, we reparameterize the set of distributions with the
softmax to weight the different values of θ. In each round, we backpropagate through W and θ±N.
Now we discuss the optimization routine outlined in Algorithm 2. For the inside optimization,
ideally, in each round we would build an estimate of the loss function at our current choice of πψ
for each of the θ±N's under consideration. To do so, we rollout the policy for each θ ∈ θ±N
under consideration L times and then average the resulting losses (this also allows us to construct a
stochastic gradient of the loss). In practice we can,t consider all θ ∈ θ±N, so instead we sample M
of them from w. This has a computational benefit by allowing us to be strategic by considering θ's
each round that are closest to the arg maxeι N ' (∏ψ, θ).
After this we then backpropagate through w and Θ using the stochastic gradients learned from the
rollouts. Finally, we then update π by backpropagation through the neural network under consider-
e
ation. The gradient steps are taken with unbiased gradient estimates gw(i, τ), gΘ(i, τ) and gψ(i, τ),
which are computed by using the score-function identity and is described in detail in Appendix C.
We outline more implementation details in Appendix B along with the below algorithm with explicit
gradient estimate formulas. Hyperparamters can be found in Appendix D.
4	Experiments
We now evaluate the approach described in the previous section for combinatorial bandits with
X = {ei : i ∈ [d]} and Z ⊂ {0, 1}d. This setting generalizes both binary active classification
for arbitrary model class and active recommendation, which we evaluate by conducting experiments
on two respective real datasets. We evaluated based on two criteria: instance-dependent worst-
case and average-case. For instance-dependent worst-case, we measure, for each rk and policy π ,
5
Under review as a conference paper at ICLR 2022
Algorithm 2 Gradient Based Optimization of equation 8
Input: partition Ω, number of iterations Nit, number of problem samples M, number of rollouts per problem
L, and loss variable LT at horizon T (see beginning of Section 2).
Goal: Compute the optimal policy argmin∏ maxθ∈Ω '0(π,θ) = argmin∏ maxθ∈Ω E∏,θ[Lt]. Note in the
case of '0(π, θ) = '(π, θ) — b(rk(θ)), LT is inherently subtracting the scalar value b(rk(θ)).
Initialization: w, finite set Θ = Θln and ψ.
for t = 1, ..., Nit do
for m = 1, ..., M do
SampleIm 守 SOFTMAX(w).
Collect L independent rollout trajectories, denoted as TTm ,i：l, by the policy πψ for θιm.
end for
Update the generating distribution by taking ascending steps on gradient estimates:
M	L	ML
Θ ,w - Θ +ɪ X Vθ Lballier (θlm,Ω)+ X gθ (Imfl) ,W + ɪ X X gw (Im,Tm,l)
ML	ML
m=1	l=1	m=1 l=1
where Lbarlier is a differentiable barrier loss that heavily penalizes the θιm,s outside Ω.
Update the policy by taking descending step on gradient estimate:
1 ML
ψ 一ψ 一而E XXgψ(Im,Tm,l)
m=1 l=1
end for
'(π, Θ(rk)) := max '(π, θ) and plot this value as a function of rk. We note that our algorithm
θ∈Θ(rk)
is designed to optimize for such a metric. For the secondary average-case metric, we instead mea-
sure, for policy ∏ and some collected set Θ, ∣Θ∣ Pθ∈θ '(∏, θ). Performances of instance-dependent
worst-case metric are reported in Figures 2, 3, 4, 6, and 7 below while the average case performances
are reported in the tables and Figure 5. Full scale of the figures can also be found in Appendix F.
4.1	Algorithms
We compare against a number of baseline active learning algorithms (see Section 5 for a review).
UNCERTAINTY SAMPLING at time t computes the empirical maximizer of hz, θi and the runner-up,
and samples an index uniformly from their symmetric difference (i.e thinking of elements of Z as
subsets of [d]); if either are not unique, an index is sampled from the region of disagreement of the
winners (see Appendix G for details). The greedy methods are represented by soft generalized binary
search (SGBS) (Nowak, 2011) which maintains a posterior distribution over Z and samples to
maximize information gain. A hyperparameter β ∈ (0, 1/2) of SGBS determines the strength of the
likelihood update. We plot or report a range of performance over β ∈ {.01, .03, .1, .2, .3, .4}. The
agnostic algorithms for classification (Dasgupta, 2006; Hanneke, 2007b;a; Dasgupta et al., 2008;
Huang et al., 2015; Jain & Jamieson, 2019) or combinatorial bandits (Chen et al., 2014; Gabillon
et al., 2016; Chen et al., 2017; Cao & Krishnamurthy, 2017; Fiez et al., 2019; Jain & Jamieson,
2019) are so conservative that given just T = 20 samples, they are all exactly equivalent to uniform
sampling and hence represented by Uniform. To represent a policy based on learning to actively
learn with respect to a prior, we employ the method of Kveton et al. (2020), denoted Bayes-LAL,
with a fixed prior P constructed by drawing a z uniformly at random from Z and defining θ =
2z - 1 ∈ [-1, 1]d (details in Appendix H). When evaluating each policy, we use the successive
halving algorithm (Li et al., 2017; 2018) for optimizing our non-convex objective with randomly
initialized gradient descent and restarts (details in Appendix E).
4.2	Synthetic Dataset: Thresholds
We begin with a very simple instance to demonstrate the instance-dependent performance achieved
by our learned policy. For d = 25, let X = {ei : i ∈ [d]}, Z = {Pik=1 ei : k = 0, 1, . . . , d},
and f(∙∣θ,x) is a Bernoulli distribution over {-1,1} with mean hx,θ) ∈ [-1,1]. Appendix A
shows that z*(θ*) = argmaxz(z, θ*> is the best threshold classifier for a label distribution induced
6
Under review as a conference paper at ICLR 2022
by (θ* + 1)/2. We trained baseline policies {∏k}k=ι for the BEST IDENTIFICATION metric with
C(θ) = e(X, Z, θ) and rk = 23+i/2 for i ∈ {0,..., 8}.
First we compare the base
policies πk to πb.	Fig-
Ure 2 presents '(π,Θ(r)) =
maxθ0(θ)≤r '(∏,θ)	=
maxθMθ)≤r P∏,θ (b = z*(θ))
as a function of r for our
base policies {πk}k and the
global policy πb, each as an
individual curve. Figure 3
plots the same information in
terms of gap:	'(π, Θ(r))—
min	'(∏k, Θ(rk)).	We
k:rk-1 <r≤rk
observe that each πk performs
best in a particular region and πb
performs almost as well as the
r-dependent baseline policies
over the range of r.
Figure 3: Sub-optimality of in-
dividual policies, lower is better
Figure 2: Learned policies,
lower is better
IOO
80
60
40
20
§
N
<κ
ω S
g
SGBS, various β
----Uncertainty
----Uniform
——r-dependent baseline
Bayes-LAL
20	40	60	80	100	120
Ooooooo
7 6 5 4 3 2 1
I.NH⅛-⅞<φ⅛8 I
0.5	0.6	0.7	0.8	0.9	1.0
h
Figure 4: Max {θ : p(θ) ≤ r}, Figure 5: Average E0〜7九口,
lower is better	lower is better
Under the same conditions as
Figure 2, Figure 4 compares the
performance of πb to the algo-
rithm benchmarks. Since SGBS
and Bayes-LAL are deterministic, the adversarial training finds a θ that tricks them into catastrophic
failure. Figure 5 trades adversarial evaluation for evaluating with respect to a parameterized prior:
For each h ∈ {0.5,0.6,..., 1}, θ ZPh is defined by drawing a Z uniformly at random from Z and
then setting θi = (2zi — 1)(2α? — 1) where α% 〜BernOulli(h). Thus, each sign of 2z — 1 is flipped
with probability h. We then compute Eθ〜Ph [P∏,θ(b = z*(θ))] = Eθ〜Ph['(π, θ)]. While SGBS
now performs much better than uniform and uncertainty sampling, our policy πb is still superior to
these policies. However, Bayes-LAL is best overall which is expected since the support of Ph is
essentially a rescaled version of the prior used in Bayes-LAL.
4.3	Real Datasets
20 Questions. Our dataset is constructed from the real data of Hu et al. (2018). Summarizing
how we used the data, 100 yes/no questions were considered for 1000 celebrities. Each question
i ∈ [100] for each person j ∈ [1000] was answered by several annotators to construct an empirical
probability Pyj ∈ [0,1] denoting the proportion of annotators that answered “yes.” To construct our
instance, We take X = {e? : i ∈ [100]} to encode questions and Z = {z(j) : [z(j)]i = 1{p(jj >
1/2}} ⊂ {0, 1}1000. Just as before, we trained {πk}4k=1 for the BEST IDENTIFICATION metric with
C(θ) = ρe(X, Z, θ) and ri = 23+i/2 for i ∈ {1, . . . , 4}. See Appendix I for details.
Jester Joke Recommendation. We now turn our attention away from BEST IDENTIFICATION to
Simple Regret where '(π,θ) = E∏,θ[hz*(θ) — b,θ)]. We consider the Jester jokes dataset of
Goldberg et al. (2001) that contains jokes ranging from innocent puns to grossly offensive jokes. We
filter the dataset to only contain users that rated all 100 jokes, resulting in 14116 users. A rating of
each joke was provided on a [—10, 10] scale which was rescaled to [—1, 1] and observations were
simulated as Bernoulli’s like above. We then clustered the ratings of these users (see Appendix J for
details) to 10 groups to obtain Z = {z(k) : k ∈ [10], z(k) ∈ {0, 1}100} where zi(k) = 1 corresponds
to recommending the ith joke in user cluster z(k) ∈ Z. See Appendix J for details.
4.3	. 1 Instance-dependent Worst-case
Figure 6 and Figure 7 are analogous to Figure 4 but for the 20 questions
and Jester joke instances, respectively. The two deterministic policies, SGBS
and Bayes-LAL, fail on these datasets as well against the worst-case instances.
7
Under review as a conference paper at ICLR 2022
On the Jester joke dataset, our
policy alone nearly achieves the
r-dependent baseline for all r.
But on 20 questions, uncertainty
sampling performs remarkably
well. These experiments on
real datasets demonstrate that
our policy obtains near-optimal
instance dependent sample com-
plexity.
Figure 6: 20 Questions	Figure 7: Jester Joke
















4.3.2 Average Case Performance
While the metric of the previous section rewarded algorithms that perform uniformly well over all
possible environments that could be encountered, in this section we consider the performance of an
algorithm with respect to a distribution over environments, which we denote as average case.
Table 1: 20 Questions, higher the better		Table 2: Jester Joke, lower the better	
Method	ACCUracy(%)	Method	Regret
π* (Ours)	T79	π* (Ours)	3.209
SGBS	{26.5, 26.2, 27.2,	SGBS	{3.180, 3.224, 3.278,
	26.5, 21.4, 12.8}		3.263, 3.153, 3.090}
Uncertainty	14.3	Uncertainty	3.027
Bayes-LAL	4.1	Bayes-LAL	3.610
Uniform	6.9		Uniform	3.877	
While heuristic based algorithms (such as SGBS, uncertainty sampling and Bayes-LAL) can per-
form catastrophically for worst-case instances, they can perform very well with respect to a benign
distribution over instances. Here we demonstrate that our policy not only performs optimally under
the instance-dependent worst-case metric but also remain comparable even when evaluated under
the average case metric. To measure the average performance, we construct prior distributions P
based on the individual datasets:
•	For the 20 questions dataset, to draw a θ 〜 P, we uniformly at random select a J ∈ [1000] and
sets θi = 2p(j)- 1 for all i ∈ [d].
•	For the Jesterjoke recommendation dataset, to draw a θ 〜P, we uniformly sample a user and
employ their ratings to each joke.
On the 20 questions dataset, as shown in Table 1, SGBS and πb are the winners. Bayes-LAL performs
much worse in this case, potentially because of the distribution shift from P (prior we train on) to
Pb (prior at test time). The strong performance of SGBS may be due to the fact that sign(θi) =
2z*(θ)i — 1 for all i and θ 〜 P, a realizability condition under which SGBS has strong guarantees
(Nowak, 2011). On the Jester joke dataset, Table 2 shows that despite our policy not being trained
for this setting, its performance is still among the top.
5	Related work
Learning to actively learn. Previous works vary in how the parameterize the policy, ranging from
parameterized mixtures of existing expertly designed active learning algorithms (Baram et al., 2004;
Hsu & Lin, 2015; Agarwal et al., 2016), parameterizing hyperparameters (e.g., learning rate, rate of
forced exploration, etc.) in an existing popular algorithm (e.g, EXP3) (Konyushkova et al., 2017;
Bachman et al., 2017; Cella et al., 2020), and the most ambitious, policies parameterized end-to-end
like in this work (Boutilier et al., 2020; Kveton et al., 2020; Sharaf & DaUme III, 2019; Fang et al.,
2017; Woodward & Finn, 2017). These works take an approach of defining a prior distribution
either through past experience (meta-learning) or expert created (e.g., θ 〜 N(0, Σ)), and then
evaluate their policy with respect to this prior distribution. Defining this prior can be difficult, and
moreover, if the θ encountered at test time did not follow this prior distribution, performance could
suffer significantly. Our approach, on the other hand, takes an adversarial training approach and can
8
Under review as a conference paper at ICLR 2022
be interpreted as learning a parameterized least favorable prior (Wasserman, 2013), thus gaining a
much more robust policy as an end result.
Robust and Safe Reinforcement Learning. Our work is also highly related to the field of robust
and safe reinforcement learning, where our objective can be considered as an instance of minimax
criterion under parameter uncertainty (Garcia & Fernandez, 2015). Widely applied in applications
such as robotics (Mordatch et al., 2015; Rajeswaran et al., 2016), these methods train a policy in a
simulator like Mujoco (Todorov et al., 2012) to minimize a defined loss objective while remaining
robust to uncertainties and perturbations to the environment (Mordatch et al., 2015; Rajeswaran
et al., 2016). Ranges of these uncertainty parameters are chosen based on potential values that could
be encountered when deploying the robot in the real world. In our setting, however, defining the set
of environments is far less straightforward and is overcome by the adoption of the C(θ) function.
Active Binary Classification Algorithms. The literature on active learning algorithms can be
partitioned into model-based heuristics like uncertainty sampling, query by committee, or model-
change sampling (Settles, 2009), greedy binary-search like algorithms that typically rely on a form
of bounded noise for correctness (Dasgupta, 2005; Kaariainen, 2006; Golovin & Krause, 2011;
Nowak, 2011), and agnostic algorithms that make no assumptions on the probabilistic model (Das-
gupta, 2006; Hanneke, 2007b;a; Dasgupta et al., 2008; Huang et al., 2015; Jain & Jamieson, 2019;
Katz-Samuels et al., 2020; 2021). Though the heuristics and greedy methods can perform very well
for some problems, it is typically easy to construct counter-examples (e.g., outside the assumptions)
in which they catastrophically fail as demonstrated in our experiments. The agnostic algorithms
have strong robustness guarantees but rely on concentration inequalities, and consequently require
at least hundreds of labels to observe any deviation from random sampling (see Huang et al. (2015)
for comparison). Therefore, they were implicitly represented by uniform in our experiments.
Pure-exploration Multi-armed Bandit Algorithms. In the linear structure setting, for sets
X, Z ⊂ Rd known to the player, pulling an “arm” X ∈ X results in an observation(x, θ*>+ zero-
mean noise, and the objective is to identify arg maxz∈z (z, θ*) for a vector θ* unknown to the player
(Soare et al., 2014; Karnin, 2016; Tao et al., 2018; Xu et al., 2017; Fiez et al., 2019). A special case
of linear bandits is combinatorial bandits where X = {ei : i ∈ [d]} and Z ⊂ {0, 1}d (Chen et al.,
2014; Gabillon et al., 2016; Chen et al., 2017; Cao & Krishnamurthy, 2017; Fiez et al., 2019; Jain
& Jamieson, 2019). Active binary classification is a special case of combinatorial pure-exploration
multi-armed bandits (Jain & Jamieson, 2019), which we exploit in the threshold experiments. While
the above works have made great theoretical advances in deriving algorithms and information theo-
retic lower bounds that match up to constants, the constants are so large that these algorithms only
behave well when the number of measurements is very large. When applied to the instances of our
paper (only 20 queries are made), these algorithms behave no differently than random sampling.
6	Discussion and Future Directions
We see this work as an exciting but preliminary step towards realizing the full potential of this
general approach. Although our experiments has been focusing on applications of combinatorial
bandit, we see this framework generalizing with minor changes to many more widely applicable
settings such as multi-class active classification, contextual bandits, etc. To generalize C(θ) to these
settings, one can refer to existing literature for instance-dependent lower bounds (Katz-Samuels
et al., 2021; Agarwal et al., 2014). Alternatively, when such a lower bound does not exist, we
conjecture that a heuristic scoring function could also serve as C(θ). For example, in a chess game,
one could simply use the scoring function of the pieces left on board as a proxy for difficulty.
From a practical perspective, training a πb can take many hours of computational resources for even
these small instances. Scaling these methods to larger instances is an important next step. While
training time scales linearly with the horizon length T , we note that one can take multiple samples
per time step. With minimal computational overhead, this could enable training on problems that
require larger sample complexities. In our implementation we hard-coded the decision rule for zb
given sT, but it could also be learned as in (Luedtke et al., 2020). Likewise, the parameterization of
the policy and generator worked well for our purposes but was chosen somewhat arbitrarily-are there
more natural choices? Finally, while we focused on stochastic settings, this work naturally extends
to constrained fully adaptive adversarial sequences which is an interesting direction of future work.
9
Under review as a conference paper at ICLR 2022
References
Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming
the monster: A fast and simple algorithm for contextual bandits. In International Conference on
Machine Learning ,pp.1638-1646. PMLR, 2014.
Alekh Agarwal, Haipeng Luo, Behnam Neyshabur, and Robert E Schapire. Corralling a band of
bandit algorithms. arXiv preprint arXiv:1612.06246, 2016.
VM Aleksandrov, VI Sysoyev, and SHEMENEV. VV. Stochastic optimization. Engineering Cyber-
netics, (5):11-+, 1968.
Philip Bachman, Alessandro Sordoni, and Adam Trischler. Learning algorithms for active learning.
In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 301-
310. JMLR. org, 2017.
Yoram Baram, Ran El Yaniv, and Kobi Luz. Online choice of active learning algorithms. Journal of
Machine Learning Research, 5(Mar):255-291, 2004.
Craig Boutilier, Chih-Wei Hsu, Branislav Kveton, Martin Mladenov, Csaba Szepesvari, and Manzil
Zaheer. Differentiable bandit exploration. arXiv preprint arXiv:2002.06772, 2020.
Tongyi Cao and Akshay Krishnamurthy. Disagreement-based combinatorial pure exploration: Effi-
cient algorithms and an analysis with localization. stat, 2017.
Alexandra Carpentier and Andrea Locatelli. Tight (lower) bounds for the fixed budget best arm
identification bandit problem. In Conference on Learning Theory, pp. 590-604, 2016.
Leonardo Cella, Alessandro Lazaric, and Massimiliano Pontil. Meta-learning with stochastic linear
bandits. arXiv preprint arXiv:2005.08531, 2020.
Lijie Chen, Anupam Gupta, Jian Li, Mingda Qiao, and Ruosong Wang. Nearly optimal sampling
algorithms for combinatorial pure exploration. In Conference on Learning Theory, pp. 482-534,
2017.
Shouyuan Chen, Tian Lin, Irwin King, Michael R Lyu, and Wei Chen. Combinatorial pure ex-
ploration of multi-armed bandits. In Advances in Neural Information Processing Systems, pp.
379-387, 2014.
Sanjoy Dasgupta. Analysis of a greedy active learning strategy. In Advances in neural information
processing systems, pp. 337-344, 2005.
Sanjoy Dasgupta. Coarse sample complexity bounds for active learning. In Advances in neural
information processing systems, pp. 235-242, 2006.
Sanjoy Dasgupta, Daniel J Hsu, and Claire Monteleoni. A general agnostic active learning algo-
rithm. In Advances in neural information processing systems, pp. 353-360, 2008.
Remy Degenne, Pierre Menard, XUedong Shang, and Michal Valko. Gamification of pure explo-
ration for linear bandits. In International Conference on Machine Learning, pp. 2432-2442.
PMLR, 2020.
Meng Fang, Yuan Li, and Trevor Cohn. Learning how to active learn: A deep reinforcement learning
approach. arXiv preprint arXiv:1708.02383, 2017.
Tanner Fiez, Lalit Jain, Kevin G Jamieson, and Lillian Ratliff. Sequential experimental design for
transductive linear bandits. In Advances in Neural Information Processing Systems, pp. 10666-
10676, 2019.
Victor Gabillon, Alessandro Lazaric, Mohammad Ghavamzadeh, Ronald Ortner, and Peter Bartlett.
Improved learning complexity in combinatorial pure exploration bandits. In Artificial Intelligence
and Statistics, pp. 1004-1012, 2016.
Javier Garcia and Fernando Fernandez. A comprehensive survey on safe reinforcement learning.
Journal of Machine Learning Research, 16(1):1437-1480, 2015.
10
Under review as a conference paper at ICLR 2022
AUrelien Garivier and Emilie Kaufmann. Optimal best arm identification with fixed confidence. In
Conference on Learning Theory, pp. 998-1027, 2016.
Ken Goldberg, Theresa Roeder, Dhruv Gupta, and Chris Perkins. Eigentaste: A constant time
collaborative filtering algorithm. information retrieval, 4(2):133-151, 2001.
Daniel Golovin and Andreas Krause. Adaptive submodularity: Theory and applications in active
learning and stochastic optimization. Journal of Artificial Intelligence Research, 42:427-486,
2011.
Steve Hanneke. A bound on the label complexity of agnostic active learning. In Proceedings of the
24th international conference on Machine learning, pp. 353-360, 2007a.
Steve Hanneke. Teaching dimension and the complexity of active learning. In International Con-
ference on Computational Learning Theory, pp. 66-81. Springer, 2007b.
Steve Hanneke et al. Theory of disagreement-based active learning. Foundations and Trends® in
Machine Learning, 7(2-3):131-309, 2014.
Botao Hao, Tor Lattimore, and Csaba Szepesvari. Adaptive exploration in linear contextual bandit.
arXiv preprint arXiv:1910.06996, 2019.
Wei-Ning Hsu and Hsuan-Tien Lin. Active learning by learning. In Twenty-Ninth AAAI conference
on artificial intelligence, 2015.
Huang Hu, Xianchao Wu, Bingfeng Luo, Chongyang Tao, Can Xu, Wei Wu, and Zhan Chen. Playing
20 question game with policy-based reinforcement learning. arXiv preprint arXiv:1808.07645,
2018.
Tzu-Kuo Huang, Alekh Agarwal, Daniel J Hsu, John Langford, and Robert E Schapire. Efficient and
parsimonious agnostic active learning. In Advances in Neural Information Processing Systems,
pp. 2755-2763, 2015.
Lalit Jain and Kevin G Jamieson. A new perspective on pool-based active classification and false-
discovery control. In Advances in Neural Information Processing Systems, pp. 13992-14003,
2019.
Matti Kaariainen. Active learning in the non-realizable case. In International Conference on Algo-
rithmic Learning Theory, pp. 63-77. Springer, 2006.
Zohar Karnin, Tomer Koren, and Oren Somekh. Almost optimal exploration in multi-armed bandits.
In International Conference on Machine Learning, pp. 1238-1246, 2013.
Zohar S Karnin. Verification based solution for structured mab problems. In Advances in Neural
Information Processing Systems, pp. 145-153, 2016.
Julian Katz-Samuels, Lalit Jain, Zohar Karnin, and Kevin Jamieson. An empirical process approach
to the union bound: Practical algorithms for combinatorial and linear bandits. arXiv preprint
arXiv:2006.11685, 2020.
Julian Katz-Samuels, Jifan Zhang, Lalit Jain, and Kevin Jamieson. Improved algorithms for agnostic
pool-based active classification. arXiv preprint arXiv:2105.06499, 2021.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Ksenia Konyushkova, Raphael Sznitman, and Pascal Fua. Learning active learning from data. In
Advances in Neural Information Processing Systems, pp. 4225-4235, 2017.
Branislav Kveton, Martin Mladenov, Chih-Wei Hsu, Manzil Zaheer, Csaba Szepesvari, and Craig
Boutilier. Differentiable meta-learning in contextual bandits. arXiv preprint arXiv:2006.05094,
2020.
Tor Lattimore and Csaba Szepesvari. The end of optimism? an asymptotic analysis of finite-armed
linear bandits. arXiv preprint arXiv:1610.04491, 2016.
11
Under review as a conference paper at ICLR 2022
Liam Li, Kevin Jamieson, Afshin Rostamizadeh, Ekaterina Gonina, Moritz Hardt, Benjamin
Recht, and Ameet Talwalkar. Massively parallel hyperparameter tuning. arXiv preprint
arXiv:1810.05934, 2018.
Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband:
A novel bandit-based approach to hyperparameter optimization. The Journal of Machine Learning
Research,18(1):6765-6816, 2017.
Alex Luedtke, Marco Carone, Noah Simon, and Oleg Sofrygin. Learning to learn from data: Us-
ing deep adversarial learning to construct optimal statistical procedures. Science Advances, 6
(9), 2020. doi: 10.1126/sciadv.aaw2140. URL https://advances.sciencemag.org/
content/6/9/eaaw2140.
Shie Mannor and John N Tsitsiklis. The sample complexity of exploration in the multi-armed bandit
problem. Journal of Machine Learning Research, 5(Jun):623-648, 2004.
Igor Mordatch, Kendall Lowrey, and Emanuel Todorov. Ensemble-cio: Full-body dynamic motion
planning that transfers to physical humanoids. In 2015 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS), pp. 5307-5314. IEEE, 2015.
Robert D Nowak. The geometry of generalized binary search. IEEE Transactions on Information
Theory, 57(12):7893-7906, 2011.
Jungseul Ok, Alexandre Proutiere, and Damianos Tranos. Exploration in structured reinforcement
learning. In Advances in Neural Information Processing Systems, pp. 8874-8882, 2018.
Aravind Rajeswaran, Sarvjeet Ghotra, Balaraman Ravindran, and Sergey Levine. Epopt: Learning
robust neural network policies using model ensembles. arXiv preprint arXiv:1610.01283, 2016.
Burr Settles. Active learning literature survey. Technical report, University of Wisconsin-Madison
Department of Computer Sciences, 2009.
Amr Sharaf and Hal Daume III. Meta-Iearning for contextual bandit exploration. arXiv preprint
arXiv:1901.08159, 2019.
Max Simchowitz and Kevin G Jamieson. Non-asymptotic gap-dependent regret bounds for tabular
mdps. In Advances in Neural Information Processing Systems, pp. 1151-1160, 2019.
Max Simchowitz, Kevin Jamieson, and Benjamin Recht. The simulator: Understanding adaptive
sampling in the moderate-confidence regime. arXiv preprint arXiv:1702.05186, 2017.
Marta Soare, Alessandro Lazaric, and Remi Munos. Best-arm identification in linear bandits. In
Advances in Neural Information Processing Systems, pp. 828-836, 2014.
Chao Tao, Saul Blanco, and Yuan Zhou. Best arm identification in linear bandits with linear dimen-
sion dependency. In International Conference on Machine Learning, pp. 4877-4886, 2018.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033.
IEEE, 2012.
Alexandre B Tsybakov. Introduction to nonparametric estimation. Springer Science & Business
Media, 2008.
Bart Van Parys and Negin Golrezaei. Optimal learning for structured bandits. 2020.
Larry Wasserman. All of statistics: a concise course in statistical inference. Springer Science &
Business Media, 2013.
Mark Woodward and Chelsea Finn. Active one-shot learning. arXiv preprint arXiv:1702.06559,
2017.
Liyuan Xu, Junya Honda, and Masashi Sugiyama. Fully adaptive algorithm for pure exploration in
linear bandits. arXiv preprint arXiv:1710.05552, 2017.
12
Under review as a conference paper at ICLR 2022
A Instance Dependent Sample Complexity
Identifying forms of C(θ) is not as difficult a task as one might think due to the proliferation of tools
for proving lower bounds for active learning (Mannor & Tsitsiklis, 2004; Tsybakov, 2008; Garivier
& Kaufmann, 2016; Carpentier & Locatelli, 2016; Simchowitz et al., 2017; Chen et al., 2014). One
can directly extract values of C(θ) from the literature for regret minimization of linear or other
structured bandits (Lattimore & Szepesvari, 2016; Van Parys & Golrezaei, 2020), contextual bandits
(Hao et al., 2019), and tabular as well as structured MDPs (Simchowitz & Jamieson, 2019; Ok et al.,
2018). Moreover, we believe that even reasonable surrogates of C(θ) should result in a high quality
policy π
*.
We review some canonical examples:
• Multi-armed bandits. In the best-arm identification problem, there are d ∈ N Gaussian distri-
butions where the ith distribution has mean θi ∈ R for i = 1, . . . , d. In the above formulation,
this problem is encoded as action Xt = it results in observation yt 〜 BernOUlli(θij and the loss
'(π, θ) := E∏,θ[1{i = i*(θ)}] where i is π s recommended index and i*(θ) = argmaxi θi. It S
been shown that there exists a constant c0 > 0 such that for any sufficiently large ν > 0 we have
min max	'(π,θ) ≥ exp(-c°T∕ν)
∏ S：Cmab (θ)≤ν
where CMAB(θ) :=	(θi*(θ) - θi)-2
i6=i* (θ)
Moreover, for any θ ∈ Rd there exists a policy e that achieves '(e, θ) ≤ ci exp(-c2T/CMAB (θ))
where c1, c2 capture constant and low-order terms (Carpentier & Locatelli, 2016; Karnin et al.,
2013; Simchowitz et al., 2017; Garivier & Kaufmann, 2016).
The above correspondence between the lower bound and the upper bound suggests that CMAB (θ)
plays a critical role in determining the difficulty of identifying i* (θ) for any θ. This exercise extends
to more structured settings as well:
•	Content recommendation / active search. Consider n items (e.g., movies, proteins) where the
ith item is represented by a feature vector xi ∈ X ⊂ Rd and a measurement xt = xi (e.g.,
preference rating, binding affinity to a target) is modeled as a linear response model such that
yt 〜N(〈Xi, θ), 1) for some unknown θ ∈ Rd. If '(∏, θ) := E∏,θ[1{b = i*(θ)}] as above then
nearly identical results to that of above hold for an analogous function of CMAB (θ) (Soare et al.,
2014; Karnin, 2016; Fiez et al., 2019).
•	Active binary classification. For i = 1, . . . , d let φi ∈ Rp be a feature vector of an unlabeled
item (e.g., image) that can be queried for its binary label y% ∈ {-1,1} where yi 〜BernOUlli(θi)
for some θ ∈ Rd . Let H be an arbitrary set of classifiers (e.g., neural nets, random forest,
etc.) such that each h ∈ H assigns a label {-1, 1} to each of the items {φi}id=1 in the pool.
If items are chosen sequentially to observe their labels, the objective is to identify the true risk
minimizer h*(θ) = arg minh∈H Pid=1 Eθ[1{h(φi) 6= yi}] using as few requested labels as pos-
sible and '(∏,θ) := E∏,θ[1{h = h*(θ)}] where h ∈ H is n's recommended classifier. Many
candidates for C(θ) have been proposed from the agnostic active learning literature (Dasgupta,
2006; Hanneke, 2007b;a; Dasgupta et al., 2008; Huang et al., 2015; Jain & Jamieson, 2019) but
we believe the most granular candidates come from the combinatorial bandit literature (Chen
et al., 2017; Fiez et al., 2019; Cao & Krishnamurthy, 2017; Jain & Jamieson, 2019). To make
the reduction, for each h ∈ H assign a z(h) ∈ {0, 1}d such that [z(h)]i := 1{h(φi) = 1} for all
i = 1, . . . , d and set Z = {z(h) : h ∈ H}. It is easy to check that z* (θ) :
arg maxz∈Z
satisfies z*(θ) = z(h* (S)). Thus, requesting the label of example i is equivalent to
hz, θi
sam-
pling from Bernoulli(hei, θi) ∈ {-1, 1}, completing the reduction to combinatorial bandits:
X = {ei : i ∈ [d]}, Z ⊂ {0, 1}d. We then apply the exact same C(θ) as above for linear
bandits.
B	Gradient Based Optimization Algorithm Implementation
First, we restate the algorithm with explicit gradient estimator formulas derived in Appendix C.
13
Under review as a conference paper at ICLR 2022
Algorithm 3 Gradient Based Optimization of equation 8 (Algorithm 2) with explicit gradient esti-
mators.___________________________________________________________________________________
Input: partition Ω, number of iterations Nit, number of problem samples M, number of rollouts
per problem L, and loss variable LT at horizon T (see beginning of Section 2).
Goal: Compute the optimal policy
argmin∏ maXθ∈Ω '0(∏,θ) = argmin∏ maXθ∈Ω E∏,θ[Lt].
Initialization: w, finite set Θ and ψ .
for t = 1, ..., Nit do
Collect rollouts of play:
for m = 1, ..., M do
Sample problem index Im i" SOFTMAX(w).
Collect L independent rollout trajectories ({}), denoted as 丁①，上工,by the policy ∏ψ for prob-
ψ
lem instance θIm and observe losses ∀1 ≤ l ≤ L, LT(πψ, τm,l, θIm ).
end for
Optimize worst cases in Ω:
Update the generating distribution by taking ascending steps on gradient estimates:
1M	L
W 一 W + ML X Vw log(SOFTMAX(w)im) ∙ (X Lτ(πψ8,乐皿))	(9)
m=1	l=1
1 ML
θ J θ + ML X X (vΘ Lbarrier (θIm ,⑷ + vΘ LT B ,τm,l ,θIm )
m=1 l=1
+Lτ(∏ψ ,Tm,l, elm ) ∙Vθ lθg(P∏ψ ,eIm (Tm,ι)))	(10)
where Lbarrier is a differentiable barrier loss that heavily penalizes the θιm S outside Ω.
Optimize policy:
Update the policy by taking descending step on gradient estimate:
1 ML
ψ Jψ - ML X X LT (πψ ,τm,l ,θIm ) ∙ vψ lθg(PnΨ,eIm (Tm,l)).	(II)
m=1 l=1
end for
14
Under review as a conference paper at ICLR 2022
In the above algorithm, the gradient estimates are unbiased estimates of the true gradients with
respect to ψ, w and Θ (shown in Appendix C). We choose N large enough to avoid mode collapse,
and M , L as large as possible to reduce variance in gradient estimates while fitting the memory
constraint. We then find the appropriate large number of optimization iterations so that the variance
of the gradient estimates is reduced dramatically by averaging over time. We use Adam optimization
(Kingma & Ba, 2014) in taking gradient updates.
Note the decomposition for log(Pπψ,θ0 (τ)) in equation 10 and equation 11, where rollout τ =
{(xt, yt)}tT=1, and
iog(P∏ψa({(χt,yt)}T=ι)) = log (∏ψ(χι) ∙ f (yι∣θ0,sι) ∙ QT=2 ∏ψ(st,χt) ∙ f(yt∣θ0, st,χt))∙
Here πψ and f are only dependent on ψ and Θe respectively. During evaluation of a fixed policy
∏, We are interested in solving maxθ∈Ω '0(∏, θ) by gradient ascent updates like equation 10. The
decoupling of πψ and f thus enables us to optimize the objective without differentiating through a
policy π, Which could be non-differentiable like a deterministic algorithm.
B.1 Implementation Details
Training. When training our policies for BEST IDENTIFICATION, We Warm start the training With
optimizing Simple Regret. This is because a random initialized policy performs so poorly that
Best Identification is nearly alWays 1, making it difficult to improve the policy. After training
∏i:K in MAPO (Algorithm 1), we warm start the training of b with b = ∏∖κ∕2C. In addition, our
generating distribution parameterizations exactly folloWs from Section 3.1.
Loss functions. Instead of optimizing the approximated quantity from equation 8 directly, we add
regularizers to the losses for both the policy and generator. First, we choose the Lbarrier in equation 10
to be λbarrier ∙ max{0, log(C(X, Z, θ)) - log(rk)}, for some large constant λbarτier. To discourage
the policy from over committing to a certain action and/or the generating distribution from covering
only a small subset of particles (i.e., mode collapse), we also add negative entropy penalties to both
policy’s output distributions and SOFTMAX(w) with scaling factors λPol-reg and λGen-reg.
State representation. We parameterize our state space S as a flattened |X | × 3 matrix where
each row represents a distinct x ∈ X. Specifically, at time t the row of st corresponding to some
x ∈ X records the number of times that action x has been taken Pts-=11 1{xs = x}, its inverse
(Pts-=11 1{xs = x})-1, and the sum of the observations Pts-=11 1{xs = x}ys.
Policy MLP architecture. Our policy πψ is a multi-layer perceptron with weights ψ. The pol-
icy take a 3|X| sized state as input and outputs a vector of size |X| which is then pushed through
a soft-max to create a probability distribution over X. At the end of the game, regardless of the
policy’s weights, we set zb = arg maxz∈Z hz, θi where θ is the minimum `2 norm solution to
arg minθ PsT=1(ys - hxs, θi)2.
Our policy network is a simple 6-layer MLP, with layer sizes {3|X |, 256, 256, 256, 256, |X|} where
3|X | corresponds to the input layer and |X | is the size of the output layer, which is then pushed
through a Softmax function to create a probability over arms. In addition, all intermediate layers
are activated with the leaky ReLU activation units with negative slopes of .01. For the experiments
for 1D thresholds and 20 Questions, they share the same network structure as mentioned above with
|X| = 25 and |X | = 100 respectively.
C Gradient Estimate Derivation
Here we derive the unbiased gradient estimates equation 9, equation 10 and equation 11 in Algo-
rithm 2. Since each the gradient estimates in the above averages over M ∙ L identically distributed
trajectories, it is therefore sufficient to show that our gradient estimate is unbiased for a single prob-
lem θi and its rollout trajectory {(xt, yt)}tT=1.
15
Under review as a conference paper at ICLR 2022
For a feasible w, using the score-function identity (Aleksandrov et al., 1968)
▽wEi〜SOFTMAX(w) ['5ψ,θiR
=Ei〜SOFTMAX(W) h'(∏ψ,θi) ∙ Vw log(SΟFTMAX(w)i)i .
Observe that if i 〜SOFTMAX(W) and {(xt, yt)}T=ι is the result of rolling out a policy πψ on θi
then
gw(i, {(xt,yt)}T=ι) ：= Lt(∏ψ, {(xt,yt)}t=ι,θi) ∙ Vw log(SOFTMAX(w)i)
is an unbiased estimate of Vw Ei〜SOFTMAX(W)
['(∏ψ,θi)].
1 -	Γ∙	∙1 1	. Px 1	1	∕'	∙ . ∙	Γ∙ fl /	Zi∖
For a feasible set Θ, by definition of '(π, θ),
vΘ Ei 〜SOFTMAX(W)
=Ei 〜SOFTMAX(W)
=Ei 〜SOFTMAX(W)
['(∏ψ ,θi)]	(12)
hVΘe Eπ,θei hLT (π, {(xt,yt)}tT=1, θei)ii
[E∏,ei [VθLt(∏, {(xt,yt)}T=ι,θi) + Lt(∏, {(xt,yt)}T=ι,θi) F iog(P∏ψ,ei({(χt,yt)}T=ι))ii
where the last equality follows from chain rule and the score-function identity (Aleksandrov
e
et al., 1968). The quantity inside the expectations, call it gΘ(i, {(xt, yt)}tT=1), is then an unbi-
ased estimator of VθEi〜SOFTMAX(W) ['(∏ψ, θi)] given i and {(xt, yt)}T=ι are rolled out accord-
ingly. Note that if Lbarrier = 0, VθLbarrier(θi, Ω) is clearly an unbiased gradient estimator of
ττn	Γττn Γ r>	/ ∕Γ ∖ T T	■ ι ιι	ι ι	ι∙ t
Ei〜SOFTMAX(W) [Eπ,ei [Lbarrier(θi, Ω)]] given i and rollout are sampled accordingly.
Likewise, for policy,
gψ(i, {(xt,yt)}T=ι) ：= LT(∏ψ, {(xt,yt)}T=ι,e) ∙Vψ log(P∏ψ,e({(xt,yt)}T=ι))
is an unbiased estimate of VψEi〜SOFTMAX(W) ['(∏ψ, %)].
D Hyper-parameters
In this section, we list our hyperparameters. First we define λbinary to be a coefficient that gets
multiplied to binary loses, so instead of 1{z*(θ*) = b}, we receive loss λbi∩ary ∙ 1{z*(θ*) = b}. We
choose λbinary so that the recieved rewards are approximately at the same scale as SIMPLE REGRET.
During our experiments, all of the optimizers are Adam. All budget sizes are T = 20. For fairness of
evaluation, during each experiment (1D thresholds or 20 Questions), all parameters below are shared
for evaluating all of the policies. To elaborate on training strategy proposed in MAPO (Algorithm 1)
more, we divide our training into four procedures, as indicated in Table 3:
• Init. The initialization procedure takes up a rather small portion of iterations primarily for the
purpose of optimizing for Lbarrier so that the particles converge into the constrained difficulty
sets. In addition, during the initialization process we initialize and freeze w = ~0, thus putting an
uniform distribution over the particles. This allows us to utilize the entire set of particles without
w converge to only a few particles early on. To initialize Θ, we sample 2/3 of the N particles
uniformly from [-1,1]|X| and the rest 1/3 of the particles by sampling, for each i ∈ [|Z|], 3^
particles uniformly from {θ : argmaxjhθ, zji = i}. We initialize our policy weights by Xavier
initialization with weights sampled from normal distribution and scaled by .01.
• Regret Training, πei Training with S IMPLE REGRET objective usually takes the longest among
the Procedures. The primary purpose for this process is to let the policy converge to a reasonable
warm start that already captures some essence of the task.
• Fine-tune πi. Training with BEST IDENTIFICATION objective run multiple times for each πi with
their corresponding complexity set Θi . During each run, we start with a warm started policy, and
reinitialize the rest of the models by running the initialization procedure followed by optimizing
the B est Identification objective.
16
Under review as a conference paper at ICLR 2022
		Experiment		
Procedure	Hyper-parameter	1D Threshold |X| =25	20 Questions |X| = 100	Jester Joke |X| = 100
Init	Nit ψ learning rate Θ learning rate w learning rate		20000 (all) 10-4 (all) 10-3 (all) 0 (all)	
Regret Training	Nit ψ learning rate Θ learning rate w learning rate		480000 (all) 10-4 (all) 10-3 (all) 10-3 (all)	
Fine-tune	Nit for πei Nit for πi Nit for π* ψ learning rate Θ learning rate w learning rate	200000 200000 500000	0 1500000 250000 10-4 (all) 10-3 (all) 10-3 (all)	200000 N/A 500000
Adam Optimizer	β1 β2		.9 (all) .999 (all)	
Table 3: Number of Iterations and Learning Rates
Procedure	Hyper-parameter	Experiment		
		1D Threshold |X| =25	20 Questions |X| = 100	Jester Joke |X| = 100
	N	1000 X |Z|	300 × |Z|	2000 × |Z|
	M	1000	500	500
	L	10	30	30
Init + Train +	λbinary	7.5	30	30
Fine-tune	λPol-reg(regret)	.2	.8	.8
	λPol-reg(fine-tune)	.3	.8	.8
	λGen-reg	.05	.1	.05
	λbarrier		103 (all)	
Table 4: Parallel Sizes and Regularization coefficients
•	Fine-tune b This procedure optimizes equation 3, with baselines mink '(∏k, Θ(rk)) evaluated
based on each πi learned from the previous procedure. Similar to fine-tuning each individual πi ,
We warm start a policy π∖κ∕2c and reinitialize W and Θ by running the initialization procedure
again.
To provide a general strategy of choosing hyper-parameters, we note that L, firstly, λbinary, λPol-reg
are primarily parameters tuned for |X | as the noisiness and scale of the gradients, and entropy over
the arms X grows with the size |X |. Secondly, λGen-reg is primarily tuned for |Z| as it penalizes
the entropy over the N arms, which is a multiple of |Z |. Thirdly, learning rate of θ is primarily
tuned for the convergence of constraint ρ* into the restricted class, thus LbarTier becoming 0 after
the specified number of iterations during initialization is a good indicator. Finally, we choose N
and M by memory constraint of our GPU. The hyper-parameters for each experiment was tuned
with less than 20 hyper-parameter assignments, some metrics to look at while tuning these hyper-
parameters includes but are not limited to: gradient magnitudes of each component, convergence of
each loss and entropy losses for each regularization term (how close it is to the entropy ofa uniform
probability), etc.
17
Under review as a conference paper at ICLR 2022
E Policy Evaluation
When evaluating a policy, we are essentially solving the following objective for a fixed policy π :
max '(π, θ)
θ∈Ω
where Ω is a set of problems. However, due to non-concavity of this loss function, gradient descent
initialized randomly may converge to a local maxima. To reduce this possibility, we randomly
initialize many initial iterates and take gradient steps round-robin, eliminating poorly performing
trajectories. To do this with a fixed amount of computational resource, we apply the successive
halving algorithm from Li et al. (2018). Specifically, we choose hyperparamters: η = 4, r = 100,
R = 1600 and s = 0. This translates to:
•	Initialize ∣Θ∣ = 1600, optimize for 100 iterations for each θi ∈ Θ
•	Take the top 400 of them and optimize for another 400 iterations
•	Take the top 100 of the remaining 400 and optimize for an additional 1600 iterations
We take gradient steps with the Adam optimizer (Kingma & Ba, 2014) with learning rate of 10-3
β1 = .9 and β2 = .999.
18
Under review as a conference paper at ICLR 2022
F Figures at Full Scale
O
12
O
'O
1
O
8
O
2
40	60
r
Figure 8: Full scale of Fi
O
12
O
'O
1
一80
∞
O
4
O
2
droo
Figure 9:	Full scale of Figure 3
19
Under review as a conference paper at ICLR 2022
Ooooo
0 8 6 4 2
1
、Jvl(606
U9*Z≠<N6UadnS (％r。缶
----ft (Ours)
---- SGBS, various β
----Uncertainty
----Uniform
一 一 r-dependent baseline
----Bayes-LAL
20	40	60	80 IOO 120
Figure 10:	Full scale of Figure 4
----rτ* (Ours)
----SGBSr various β
----Uncertainty
----Uniform
----Bayes-LAL
0.5	0.6	0.7	0.8	0.9	1.0
Figure 11: Full scale of Figure 5
20
Under review as a conference paper at ICLR 2022
----ft (Ours)
---- SGBS, various β
----Uncertainty
----Uniform
一 一 r-dependent baseline
----Bayes-LAL
15
20
25
30
Figure 12: Full scale of Figure 6
dns 既」6次
Figure 13: Full scale of Figure 7
G
Uncertainty Sampling
We define the symmetric difference of a set of binary vectors, SymDiff({z1, ..., zn}) = {i : ∃j, k ∈
[n] s.t., zj(i) = 1 ∧ zk(i) = 0}, as the dimensions where inconsistencies exist.
21
Under review as a conference paper at ICLR 2022
Algorithm 4 Uncertainty sampling in very small budget setting
Input: X, Z
for t = 1, ..., T do
θbt-1 = arg minθ PsT=1(ys - hxs, θi)2
Z = {z ∈ Z : maxz0∈Zhz0,θt-1i = hz, θt-1i}
if |Zb| = 1 then
Zt = Z U{z ∈ Z ： maxz0∈(z∖b) hz , θt-ii = hz, θt-ιi}
else
Zbt = Zb
end if
Uniformly sample It from SymDiff(Zbt)
Pull xIt and observe yt
end for
H	Learning to Actively Learn Algorithm
To train a policy under the learning to actively learn setting, we aim to solve for the objective
min Eθ 〜P ['(πψ ,θ)]
where our policy and states are parameterized the same way as Appendix ?? for a fair comparison.
To optimize for the parameter, we take gradient steps like equation 11 but with the new sampling and
rollout where θi 〜P. This gradient step follows from both the classical policy gradient algorithm
in reinforcement learning as well as from recent learning to actively learn work by Kveton et al.
(2020).
Moreover, note that the optimal policy for the objective must be deterministic as justified by deter-
ministic policies being optimal for MDPs. Therefore, it is clear that, under our experiment setting,
the deterministic Bayes-LAL policy will perform poorly in the adversarial setting (for the same
reason why SGBS performs poorly).
I 20 Questions Setup
Hu et al. (2018) collected a dataset of 1000 celebrities and 500 possible questions to ask about each
celebrity. We chose 100 questions out of the 500 by first constructing 夕0, X0 and Z0 for the 500
dimensions data, and sampling without replacement 100 of the 500 dimensions from a distribution
derived from a static allocation. We down-sampled the number of questions so our training can run
with sufficient M and L to de-noise the gradients while being prototyped with a single GPU.
Specifically, the dataset from Hu et al. (2018) consists of probabilities of people answering Yes /
No / Unknown to each celebrity-question pair collected from some population. To better fit the
combinatorial bandit scenario, we re-normalize the probability of getting Yes / No, conditioning on
the event that these people did not answer Unknown. The probability of answering Yes to all 500
questions for each celebrity then constitutes vectors po(1),…,po(1000) ∈ R500, where each dimension
0(j)
of a give Pi r represents the probability of yes to the ith question about the j th person. The action
set X0 is then constructed as X0 = {ei : i ∈ [500]}, while Z0 = {z(j) : [z(j)] = 1{p(j) > 1/2}} ⊂
{0, 1}1000 are binary vectors taking the majority votes.
To sub-sample 100 questions from the 500, we could have uniformly at random selected the ques-
tions, but many of these questions are not very discriminative. Thus, we chose a “good” set of
queries based on the design recommended by ρ* of Fiez et al. (2019). If questions were being an-
swered noiselessly in response to a particular z ∈ Z0, then equivalently we have that for this setting
θ = 2z - 1. Since ρ* optimizes allocations λ over X0 that would reduce the number of required
queries as much as possible (according to the information theoretic bound of (Fiez et al., 2019)) if
we want to find a single allocation for all z0 ∈ Z simultaneously, we can perform the optimization
22
Under review as a conference paper at ICLR 2022
problem
kz0 - z k2P λ T -1
( i λixixi )
min max max --------------------、、η .
λ∈∆(∣χ∣-i) z'∈Z' z=z0 ((z0 - Z)T(2z0 - 1))2
We then sample elements from X0 according to this optimal λ without replacement and add them to
X until |X | = 100.
J	Jester Joke Recommendation Setup
We consider the Jester jokes dataset of Goldberg et al. (2001) that contains jokes ranging from pun-
based jokes to grossly offensive. We filter the dataset to only contain users that rated all 100 jokes,
resulting in 14116 users. A rating of each joke was provided on a [-10, 10] scale which was shrunk
to [-1,1]. Denote this set of ratings as ΘΘ = {θi : i ∈ [14116], θ% ∈ [-1,1]100}, where θ% encodes
the ratings of all 100 jokes by user i. To construct the set of arms Z, we then clustered the ratings of
these users to 10 groups to obtain Z = {zi : i ∈ [10], zi ∈ {0, 1}100} by minimizing the following
metric:
14116
min > max	hz*,θii- maxhz,θii.
Z:|Z| = 10 气 z* ∈{0,1}100	z∈Z
To solve for Z, we adapt the k - means algorithm, with the metric above instead of the L -2 metric
used traditionally.
23