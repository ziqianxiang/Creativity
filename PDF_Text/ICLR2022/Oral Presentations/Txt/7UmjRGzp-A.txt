Published as a conference paper at ICLR 2022
Understanding over-squashing and
BOTTLENECKS ON GRAPHS VIA CURVATURE
Jake Topping121,Francesco Di Giovanni3 ^, Benjamin P, Chamberlain3,
Xiaowen Dong1 , and Michael M. Bronstein23
1University of Oxford 2Imperial College London 3Twitter
Abstract
Most graph neural networks (GNNs) use the message passing paradigm, in which
node features are propagated on the input graph. Recent works pointed to the
distortion of information flowing from distant nodes as a factor limiting the effi-
ciency of message passing for tasks relying on long-distance interactions. This
phenomenon, referred to as ‘over-squashing’, has been heuristically attributed to
graph bottlenecks where the number of k-hop neighbors grows rapidly with k . We
provide a precise description of the over-squashing phenomenon in GNNs and
analyze how it arises from bottlenecks in the graph. For this purpose, we introduce
a new edge-based combinatorial curvature and prove that negatively curved edges
are responsible for the over-squashing issue. We also propose and experimentally
test a curvature-based graph rewiring method to alleviate the over-squashing.
1	Introduction
In the past few years, deep learning on
graphs and in particular graph neural net-
works (GNNs) (Sperduti, 1994; Goller &
Kuchler, 1996; Sperduti & Starita, 1997;
Frasconi et al., 1998; Gori et al., 2005;
Scarselli et al., 2008; Bruna et al., 2014;
Defferrard et al., 2016; Kipf & Welling,
2017; Gilmer et al., 2017) have become
very popular in the machine learning com-
munity due to their ability to deal with
broad classes of systems of relations and
interactions, ranging from social networks
to particle physics (Shlomi et al., 2021).
The vast majority of GNNs follow the
message passing paradigm (Gilmer et al.,
Figure 1: Top: evolution of curvature on a surface may
reduce the bottleneck. Bottom: this paper shows how
the same may be done on graphs to improve GNN per-
formance. Blue/red shows negative/positive curvature.
2017), using learnable non-linear functions to diffuse information on the graph. Multiple popular
GNN architectures such as GCN (KiPf & Welling, 2017) and GAT (VelickoVic et al., 2018) can be
posed as particular flavors of this scheme and considered instances of a more general framework of
geometric deeP learning (Bronstein et al., 2021).
Some of the drawbacks of the message Passing Paradigm haVe now been identified and formalized,
including the limits of exPressiVe Power (Xu et al., 2019; Morris et al., 2019; Maron et al., 2019)
and the Problem of oVer-smoothing (NT & Maehara, 2019; Oono & Suzuki, 2020). On the other
hand, much less is known about the Phenomenon of over-squashing, consisting in the distortion of
messages being ProPagated from distant nodes. Alon & YahaV (2021) ProPosed rewiring the graPh
as a way of reducing the bottleneck, defined as those toPological ProPerties in the graPh leading to
oVer-squashing. This aPProach is in line with multiPle other results e.g. using connectiVity diffusion
(KlicPera et al., 2019) as a PreProcessing steP to facilitate graPh learning. Yet, the exact understanding
of the oVer-squashing and how it originates from the bottlenecks in the toPology of the underlying
tEqual contribution
1
Published as a conference paper at ICLR 2022
graph are still elusive. Consequently, there is currently no consensus on the right method (either
based on graph rewiring or not) to address the bottleneck and hence alleviate the over-squashing.
In this paper, we address these questions using tools from differential geometry, which traditionally
is concerned with the study of manifolds. It offers an appealing framework to study the properties
of graphs, in particular arguing that graphs, like manifolds, exhibit curvature that makes them more
suitable to be realized in spaces with hyperbolic geometry (Liu et al., 2019; Chami et al., 2019;
Boguna et al., 2021). One notion of curvature that has received attention for graph learning is Ricci
curvature (Hamilton, 1988), also known in geometry for its use in Ricci flow and the subsequent
proof of the Poincare conjecture (Perelman, 2θ03). Certain graph analogues of the Ricci curvature
(Forman, 2003; Ollivier, 2009; Sreejith et al., 2016) were used in Ni et al. (2018) for a discrete version
of Ricci flow to construct a metric between graphs. Graph Ricci flow was also used in Ni et al. (2019)
for community detection. Both of these methods use the edge weights as a substitute for the metric of
a manifold, and do not change the topological structure of the graph.
Contributions and Outline. This paper, to our knowledge, is the first theoretical study of the
bottleneck and over-squashing phenomena in message passing neural networks from a geometric
perspective. In Section 2, we propose the Jacobian of node representations as a formal way of
measuring the over-squashing and we show that the graph topology may compromise message
propagation in graph neural networks by creating a bottleneck. In Section 3, we investigate how such
a bottleneck is induced which leads to the over-squashing of information. To this aim, we introduce a
new combinatorial edge-based curvature called Balanced Forman curvature that constitutes a sharp
lower bound to the standard Ollivier curvature on graphs, and prove that negatively curved edges are
responsible for the formation of bottlenecks (and hence for over-squashing). In Section 4, we present
a new curvature-based method for graph rewiring called Stochastic Discrete Ricci Flow. According
to the theoretical results in Section 3, this rewiring method is suited to address the graph bottleneck
and hence alleviate the over-squashing by surgically targeting the edges responsible for the issue.
By contrast, we rigorously show that a recently introduced diffusion-based rewiring scheme might
generally fail to reduce the bottleneck. Finally, in Section 5, we compare different rewiring strategies
experimentally on several standard graph learning datasets.
2	Analysis of the over-squashing phenomenon
2.1	Preliminaries
Let G = (V,E) be a simple, undirected, and connected graph, where (i, j) ∈ E iff i 〜j. We focus
on the unweighted case, although the theory extends to the weighted setting as well. We denote
the adjacency matrix by A and let A = A + I be the adjacency matrix augmented with self-loops.
1	1 1	1 . 1Λ	1	1	1	.	1	1Λ1 1Λ1
Similarly we let D = D + I, with D the diagonal degree matrix, and let A = D 2 AD 2 be the
normalized augmented adjacency matrix (self-loops are commonly included in GNN architecture,
and in Section 2 we formally explain why GNNs are expected to propagate information more reliably
when self-loops are taken into account). Given i ∈ V , we denote its degree by di and let
Sr(i) :=	{j	∈ V : dG(i,j)	=	r},	Br(i)	:=	{j	∈ V :	dG(i,j) ≤ r},
where dG is the standard shortest-path distance on the graph and r ∈ N. The set Br(i) represents the
receptive field of an r-layer message passing neural network at node i.
Message passing neural networks (MPNNs). Assume that the graph G is equipped with node
features X ∈ Rn×p0 where xi ∈ Rp0 is the feature vector at node i = 1, . . . , n = |V |. We denote by
h(') ∈ Rp' the representation of node i at layer ' ≥ 0, with hi(0) = xi . Given a family of message
functions ψ' : Rp' X Rp' → Rp' and update functions φ' : Rp' X Rp' → Rp'+1, we can write the
(` + 1)-st layer output ofa generic MPNN as follows (Gilmer et al., 2017):
hi'+1) = Φ' 卜,XX AijΨ'(h('),hj'))	(1)
Here we have used the augmented normalized adjacency matrix to propagate messages from each
node to its neighbors, which simply leads to a degree normalization of the message functions ψ'. To
2
Published as a conference paper at ICLR 2022
avoid heavy notations the node features and representations are assumed to be scalar from now on;
these assumptions simplify the discussion and the vector case leads to analogous results.
2.2	The over-squashing problem
Multiple recent papers observed that MPNNs tend to perform poorly in situations when the learned
task requires long-range dependencies and at the same time the structure of the graph results in
exponentially many long-range neighboring nodes. We say that a graph learning problem has
long-range dependencies when the output of a MPNN depends on representations of distant nodes
interacting with each other. If long-range dependencies are present, messages coming from non-
adjacent nodes need to be propagated across the network without being too distorted. In many cases
however (e.g. in ‘small-world’ graphs such as social networks), the size of the receptive field Br(i)
grows exponentially with r. If this occurs, representations of exponentially many neighboring nodes
need to be compressed into fixed-size vectors to propagate messages to node i, causing a phenomenon
referred to as over-squashing of information (Alon & Yahav, 2021). In line with Alon & Yahav
(2021), we refer to those structural properties of the graph that lead to over-squashing as a bottleneck1.
Sensitivity analysis. The hidden feature hi(`) = hi(`) (x1, . . . , xn) computed by an MPNN with `
layers as in equation 1 is a differentiable function of the input node features {x1, . . . , xn} as long as
the update and message functions φ' and ψ' are differentiable. The over-squashing of information
can then be understood in terms of one node representation hi(`) failing to be affected by some input
feature xs of node s at distance r from node i. Hence, we propose the Jacobian ∂hi(r) /∂xs as an
explicit and formal way of assessing the over-squashing effect2.
Lemma 1. Assume an MPNNas in equation 1. Let i,s ∈ V with S ∈ Sr+ι(i). If ∣Vφ'∣ ≤ α and
∣Vψ'∣ ≤ β for 0 ≤ ' ≤ r ,then
≤ (αβ)r+1(Ar+1)is.	(2)
Lemma 1 states that if φ' and ψ' have bounded derivatives, then the propagation of messages is
controlled by a suitable power of A. For example, if dG(i, s) = r + 1 and the sub-graph induced on
Br+ι(i) is a binary tree, then (Ar+1)is = 2-13-r, which gives an exponential decay of the node
dependence on input features at distance r, as also heuristically argued by Alon & Yahav (2021).
The sensitivity analysis in Lemma 1 relates the over-squashing - as measured by the Jacobian of
the node representations - to the graph topology via powers of the augmented normalized adjacency
matrix. In the next section we explore this connection further by analyzing which local properties of
the graph structure affect the right hand side in equation 2, hence causing the bottleneck. We will
address this problem by introducing a new combinatorial notion of edge-based curvature and showing
that negatively curved edges are those responsible for the over-squashing phenomenon.
3	Graph curvature and b ottleneck
A natural object in Riemannian geometry is the Ricci curvature, a bilinear form determining the
geodesic dispersion, i.e. whether geodesics starting at nearby points with ‘same’ velocity remain
parallel (Euclidean space), converge (spherical space), or diverge (hyperbolic space). To motivate
the introduction of a Ricci curvature for graphs, we focus on these three cases. Consider two nodes
i 〜j and two edges starting at i and j respectively. In a discrete spherical geometry (Figure 2a),
the edges would meet at k to form a triangle (complete graph). In a discrete Euclidean geometry
(Figure 2b), the edges would stay parallel and form a 4-cycle based at i 〜j (orthogonal grid). Finally,
in a discrete hyperbolic geometry (Figure 2c), the mutual distance of the edge endpoints would have
grown compared to that of i and j (tree). Therefore, a Ricci curvature for graphs should provide us
with more sophisticated tools than the degree to analyze the neighborhood of an edge.
1We note that the over-squashing issue is different from the problem of under-reaching; the latter simply
amounts to a MPNN failing to fully explore a graph when the depth is smaller than the diameter (BarCel6 et al.,
2019). The over-squashing phenomenon instead may occur even in deep GNNs with the number of layers larger
than the graph diameter, as tested experimentally in Alon & Yahav (2021).
2The Jacobian of a GNN-output was also used by Xu et al. (2018) to set a similarity score among nodes.
∂h(r+1)
∂Xs
3
Published as a conference paper at ICLR 2022
Curvatures on graphs. The main ex-
amples of edge-based curvature are
the Forman curvature F (i, j) (Forman,
2003) and the Ollivier curvature κ(i, j)
in Ollivier (2007; 2009) (see Appendix).
While F(i, j) is given in terms of combi-
natorial quantities (Sreejith et al., 2016),
results are scarce and the definition is bi-
ased towards negative curvature. The the-
ory on κ(i, j) instead is richer (Lin et al.,
2011; Munch, 2019) but its formulation
makes it hard to control local quantities.
(a) Clique (> 0)	(b) Grid (= 0)	(c) Tree (< 0)
Figure 2: Different regimes of curvatures on graphs anal-
ogous to spherical (a), planar (b), and hyperbolic (c) ge-
ometries in the continuous setting.
Balanced Forman curvature. We propose a new curvature to address the shortcomings of the
existing candidates. We use the following definitions to describe the neighborhood of an edge i 〜j
and we refer to the Appendix for a more complete discussion:
(i)	]∆(i,j) := Sι(i) ∩ Sι(j) are the triangles based at i 〜j.
(ii)	]i(i,j) := {k ∈ S1(i)\S1(j),k 6=j : ∃w ∈ (S1(k) ∩ S1(j)) \ S1(i)} are the neighbors
of i forming a 4-cycle based at the edge i 〜j without diagonals inside.
(iii)	γmax(i,j) is the maximal number of 4-cycles based at i 〜j traversing
a common node (see Definition 4).
In line with the discussion about geodesic dispersion, one expects ]∆ to be
related to positive curvature (complete graph), ]i to zero curvature (grid), and
the remaining outgoing edges to negative curvature (tree). Our new curvature
formulation reflects such an intuition and recovers the expected results in the
classical cases. In the example in Figure 3 we have ]0 (0, 1) = {2, 3} while
]1(0, 1) = {5}, both without 4,6 because of the triangle 1-6-0. The degeneracy
factor γmax(0, 1) = 2, as there exist two 4-cycles passing through node 5.
Figure 3: 4-cycle
contribution.
Definition 1 (Balanced Forman curvature). For any edge i 〜j in a simple, unweighted graph G,
we let Ric(i, j) be zero if min{di, dj} = 1 and otherwise
2 i 2	|]∆(i,j) |	.	l]∆(i,j) |	.	(Ymax)	i j
Ric(i,j) := di + 不 — 2 + 2max{di,dj } + min{di,dj } + max{dl,dj } (|]口| + l]°l),	⑶
where the last term is set to be zero if |]i| (and hence |]j|) is zero. In particular Ric(i,j) > -2.
The curvature is negative when i 〜j behaves as a bridge between Sι(i) and Sι(j), while it is
positive when Sι(i) and Sι(j) stay connected after removing i 〜j. We refer to Ric as Balanced
Forman curvature. We can relate the Balanced Forman curvature to the Jacobian of hidden features,
while also extending many results valid for the Ollivier curvature κ(i, j) thanks to our next theorem.
Theorem 2. Given an unweighted graph G, for any edge i 〜j we have κ(i,j) ≥ Ric(i, j).
Theorem 2 generalizes Jost & Liu (2014, Theorem 3) (see Appendix).
We also note that the computational complexity for κ scales as
O(|E|d3max), while for our Ric we have O(|E|d2max), with dmax the
maximal degree. From Theorem 2 and Paeng (2012), we find:
Corollary 3. If Ric(i,j) ≥ k > 0 for any edge i 〜j, then there
exists a polynomial P such that
|Br (i)| ≤ P (r), ∀i ∈ V.
Graph G	RicG
C3	3 2
Cycles	C4	1
	Cn≥5	0
Complete Kn	n n —1	
Grid Gn	0
Tree Tr	^^—	2 r+1	2
Therefore, when the curvature is positive everywhere, the bottleneck Table 1: Examples of the
effect should not play a crucial role as the receptive field of each Balanced Forman curvature.
node will be polynomial in the hop-distance. This limit case shows
how the curvature determines whether a learning task on a graph would suffer from over-squashing.
4
Published as a conference paper at ICLR 2022
Curvature and over-squashing. Thanks to our new combinatorial curvature and the sensitivity
analysis in Lemma 1, we are able to relate local curvature properties to the Jacobian of the node
representations. This leads to one of the main results of this paper: negatively curved edges are
those causing the graph bottleneck and thus leading to the over-squashing phenomenon:
Theorem 4. Consider a MPNN as in equation 1. Let i 〜j with d ≤ dj and assume that:
(i)	∣Vφ'∣ ≤ α and ∣Vψ'∣ ≤ β for each 0 ≤ ' ≤ L — 1, with L ≥ 2 the depth ofthe MPNN.
(ii)	There exists δ s.t. 0 < δ < (max{di, dj})-2, δ < Y~-1x, and Ric(i,j) ≤ —2 + δ.
Then there exists Qj ⊂ S2(i) satisfying |Qj| > δ-1 and for 0 ≤ `0 ≤ L — 2 we have
i	I a(.{0+2)) I
由 Xj l⅛d<(αβ)2δ 4.	⑷
Condition (i) is always satisfied and allows us to control the message passing functions. The
requirement (ii) instead means that the curvature of (i, j) is negative enough when compared to the
degrees of i andj (recall that Ric(i, j) > —2). The further condition on γmax is to avoid pathological
cases where we have a large number of degenerate 4-cycles passing through the same three nodes.
To understand the conclusions in Theorem 4, let us fix `0 = 0. The equation 4 shows that negatively
curved edges are the ones causing bottlenecks, interpreted as how the graph topology prevents a
representation h(k2) to be affected by non-adjacent features hi(0) = xi . Theorem 4 implies that if we
have a negatively curved edge as in (ii), then there exist a large number of nodes k such that GNNs -
on average - struggle to propagate messages from i to k in two layers despite these nodes k being at
distance 2 from i. In this case the over-squashing occurs as measured by the Jacobian in equation 4
and hence the propagation of information suffers. If the task at hand has long-range dependencies,
then the over-squashing caused by the negatively curved edges may compromise the performance.
Bottleneck via Cheeger constant. We now relate the previous discussion about bottlenecks and
curvature to spectral properties of the graph. In particular, since the spectral gap of a graph can be
interpreted as a topological obstruction to the graph being partitioned into two communities, we
argue below that this quantity is related to the graph bottleneck and should hence be controllable
by the curvature. We start with an intuitive explanation: suppose we are given a graph G with two
communities separated by few edges. In this case, we see that the graph can be easily disconnected.
This property is encoded in the classical notion of the Cheeger constant (Chung & Graham, 1997)
hG := min hS ,	hS :
S⊂V
∣∂S∣
min . ,一 ____________ …
s⊂v min{vol(S), vol (V \ S)}
(5)
where ∂S = {(i,j) : i ∈ S, j ∈ V \ S} and vol(S) =	i∈S di. The main result about the Cheeger
constant is the Cheeger inequality (Cheeger, 2015; Chung & Graham, 1997):
2hG ≥ λι ≥ 号
(6)
where λ1 is the first non-zero eigenvalue of the normalized graph Laplacian, often referred to as
the spectral gap. A graph with two tightly connected communities (S and V \ S := S) and few
inter-community edges has a small Cheeger constant hG. For nodes in different communities to
interact with each other, all messages need to go through the same few bridges hence leading to the
over-squashing of information (a similar intuition was explored in Alon & Yahav (2021)). Therefore,
hG can be interpreted as a rough measure of graph ‘bottleneckedness’, in the sense that the smaller its
value, the more likely the over-squashing is to occur across inter-community edges. Since Theorem 4
implies that negatively curved edges induce the bottleneck, we expect a relationship between hG and
the curvature of the graph. The next proposition follows from Theorem 2 and Lin et al. (2011):
Proposition 5. If Ric(i,j) ≥ k > 0 forall i 〜j, then λι∕2 ≥ hg ≥ ).
Therefore, a positive lower bound on the curvature gives us a control on hG and hence on the
spectral gap of the graph. In the next section, we show that diffusion-based graph-rewiring methods
might fail to significantly alter hG and hence correct the graph bottleneck potentially induced by
inter-community edges. This will lead us to propose an alternative curvature-based graph rewiring.
5
Published as a conference paper at ICLR 2022
4 Curvature-based rewiring methods
The traditional paradigm of message passing graph neural networks assumes that messages are
propagated on the input graph (Gilmer et al., 2017). More recently, there is a trend to decouple
the input graph from the graph used for information propagation. This can take the form of graph
subsampling or resampling to deal with scalability (Hamilton et al., 2017) or topological noise (Zhang
et al., 2019), using larger motif-based (Monti et al., 2018) or multi-hop filters (Rossi et al., 2020),
or changing the graph either as a preprocessing step (Klicpera et al., 2019; Alon & Yahav, 2021) or
adaptively for the downstream task (Wang et al., 2019; Kazi et al., 2020). Such methods are often
generically referred to as graph rewiring.
In the context of this paper, we assume that graph rewiring attempts to produce a new graph
G0 = (V, E0) with a different edge structure that reduces the bottleneck and hence potentially
alleviates the over-squashing of information. We propose a method that leverages the graph curvature
to guide the rewiring steps in a surgical way by modifying the negatively-curved edges, so to decrease
the bottleneck without significantly compromising the statistical properties of the input graph. We
also rigorously show that a random-walk based rewiring method might generally fail to obtain an edge
set E0 with a significant improvement in its bottleneckedness as measured by the Cheeger constant.
Curvature-based graph rewiring. Since according to Theorem 4 negatively curved edges induce
a bottleneck and are hence responsible for over-squashing, a curvature-based rewiring method should
attempt to alleviate a graph’s strongly-negatively curved edges. To this end we implement a simple
rewiring method called Stochastic Discrete Ricci Flow (SDRF), described in Algorithm 1.
Algorithm 1: Stochastic Discrete Ricci Flow (SDRF)
Input: graph G, temperature τ > 0, max number of iterations, optional Ric upper-bound C+
Repeat
1)	For edge i 〜j with minimal Ricci curvature Ric(i, j):
Calculate vector x where xkl = Rickl (i, j) - Ric(i, j), the improvement to Ric(i, j)
from adding edge k 〜l where k ∈ Bι(i), l ∈ Bι(j);
_ Sample index k, l with probability softmax(τx)k and add edge k 〜l to G.
2)	Remove edge i 〜j with maximal Ricci curvature Ric(i,j) if Ric(i, j) > C+.
Until convergence, or max iterations reached;
At each iteration this preprocessing step adds an edge to ‘support’ the graph’s most negatively curved
edge, and then removes the most positively curved edge. The requirement on the added edge k 〜l
that k ∈ B1 (i) and l ∈ B1 (j) ensures that we’re adding either an extra 3- or 4-cycle around the
negative edge i 〜j so that this is a local modification. The graph edit distance between the original
and preprocessed graph is bounded above by 2 × the max number of iterations. The temperature
τ determines how stochastic the edge addition is, with τ = ∞ being fully deterministic (the best
edge is always added). At each step we remove the edge with most positive curvature to balance the
distributions of curvature and node degrees. We use Balanced Forman curvature as in equation 3 for
Ric(i, j). C+ can be chosen to stop the method skewing the curvature distribution negative, including
C+ = ∞ to not remove any edges. The method is inspired by the continuous (backwards) Ricci flow
with the aim of homogenizing edge curvatures. This is different from more direct extensions of Ricci
flow on graphs where it becomes increasingly expensive to propagate messages across negatively
curved edges (as in other applications such as Ni et al. (2019)). An example alongside its continuous
analogue can be seen in Figure 1.
Can random-walk based rewiring address bottlenecks? A good way of understanding the effec-
tiveness of SDRF in reducing the graph bottleneck is through comparison with random-walk based
rewiring strategies. Recall that, as argued in Section 3, the Cheeger constant hG of a graph constitutes
a rough measure of its bottleneckedness as induced by the inter-community edges (a small hG is
indicative of a bottleneck). Suppose we are given a graph G with a small hG and wish to rewire it into
a graph G0 with a significantly improved Cheeger constant in order to reduce the inter-community
bottleneck. A random-walk based rewiring method such as DIGL (Klicpera et al., 2019) acts by
smoothing out the graph adjacency and hence tends to promote connections among nodes at short
6
Published as a conference paper at ICLR 2022
diffusion distance (Coifman & Lafon, 2006). Accordingly, such a rewiring method might fail to
correct structural features like the bottleneck, which is instead more prominent for nodes that are at
long diffusion distance.3 To emphasize this point, we consider a classic example: given α ∈ (0, 1),
the Personalized Page Rank (PPR) matrix is defined by (Brin & Page, 1998) as
Ra ：= XX θPPR(DTA)k = α XX ((1 - ɑ)(D-1A)↑ .
k=0	k=0
Assume that we rewire the graph using Rα as in Klicpera et al. (2019) with the PPR kernel, meaning
that we replace the given adjacency A with Rα . Since Rα is stochastic, the new Cheeger constant of
the rewired graph can be computed as
=IdS|a ≡ ɪ XX(R )..
Sa	vola(S)≡∣S∣ ⅛¾(Ra)ij.
i∈S j∈S
By applying (Chung, 2007, Lemma 5), we show that we cannot improve the Cheeger constant
(and hence the bottleneck) arbitrarily well (in contrast to a curvature-based approach). We refer to
Proposition 17 and Remark 18 in Appendix E for results that are more tailored to the actual strategy
adopted in Klicpera et al. (2019) where we also take into account the effect of the sparsification.
Theorem 6. Let S ⊂ V with vol (S) ≤ Vol(G) / 2 Then hs,a ≤ (1-a) davg(s) hs, where davg(S)
and dmin (S) are the average and minimum degree on S, respectively.
The property that the new Cheeger constant is directly controlled by the old one stems from the
fact that a random-walk approach like in Klicpera et al. (2019) is meant to act more relevantly
on intra-community edges rather than inter-community edges because it prioritizes short diffusion
distance nodes. This is also why this method performs well on high-homophily datasets, as discussed
below. In particular, for a fixed α ∈ (0, 1), the bound in Theorem 6 can be very small. As a specific
example, consider two complete graphs Kn joined by one bridge. Then hG = (n(n - 1) + 1)-1,
which means that the bound on the right hand side is O(n-2).
Theorem 6 implies that a diffusion approach such as DIGL might fail to yield a new edge set E0
with a sufficiently improved bottleneck. By contrast, from Theorem 4 and Proposition 5, we deduce
that a curvature-based rewiring method such as SDRF properly addresses the edges that cause the
bottleneck.
Graph structure preservation. Although a graph-rewiring approach aims at providing a new edge
set E0 potentially more beneficial for the given learning task, it is still desirable to control how far
E0 is from E. In this regard, we note that a curvature-based rewiring is surgical in nature and hence
more likely to preserve the structure of the input graph better than a random-walk based approach.
Consider, for example, that we are given ρ > 0 and wish to rewire the graph such that the new edge
set E0 is within graph-edit distance ρ from the original E. Theorem 4 tells us how to do the rewiring
under such constraints in order to best address the over-squashing: the topological modifications
need to be localized around the most negatively-curved edges. We can do this with SDRF, with the
maximum number of iterations set to ρ∕2.
Secondly, We also point out that Ric(i,j) < -2 + δ implies that min{di, dj} > 2∕δ. Therefore, if
we mostly modify the edge set at those nodes i,j joined by an edge with large negative curvature,
then We are perturbing nodes With high degree Where such a change is relatively insignificant, and
thus overall statistical properties of the reWired graph such as degree distribution are likely to be
better preserved. Moreover, graph convolutional netWorks tend to be more stable to perturbations of
high degree nodes (ZUgner et al., 2020; Kenlay et al., 2021), making curvature-based rewiring more
suitable for the doWnstream learning tasks With popular GNN architectures.
Homophily and bottleneck. As a final remark, note that the graph rewiring techniques considered
in this paper (both DIGL and SDRF) are based purely on the topological structure of the graph and
completely agnostic to the node features and to whether the dataset is homophilic (adjacent nodes
have same labels) or heterophilic. Nonetheless, the different nature of these rewiring methods allows
us to draw a few broad conclusions about their suitability in each of these settings. A random-walk
3We refer to the right hand side of equation 2 where the power of the normalized augmented adjacency is
measuring the number of walks of distance r from i to s.
7
Published as a conference paper at ICLR 2022
H(G)	Cornell 0.11	Texas 0.06	Wisconsin 0.16	Chameleon 0.25	Squirrel 0.22	Actor 0.24	Cora 0.83	Citeseer 0.71	Pubmed 0.79
None	52.69 ± 0.21	61.19 ± 0.49	54.60 ± 0.86	41.80 ± 0.41	39.83 ± 0.14	28.70 ± 0.09	81.89 ± 0.79	72.31 ± 0.17	78.16 ± 0.23
Undirected	53.20 ± 0.53	63.38 ± 0.87	51.37 ± 1.15	42.63 ± 0.30	40.77 ± 0.16	28.10 ± 0.11	-	-	-
+FA	58.29 ± 0.49	64.82 ± 0.29	55.48 ± 0.62	42.33 ± 0.17	40.74 ± 0.13	28.68 ± 0.16	81.65 ± 0.18	70.47 ± 0.18	79.48 ± 0.12
DIGL (PPR)	58.26 ± 0.50	62.03 ± 0.43	49.53 ± 0.27	42.02 ± 0.13	34.38 ± 0.11	30.79 ± 0.10	83.21 ± 0.27	73.29 ± 0.17	78.84 ± 0.08
DIGL + Undirected	59.54 ± 0.64	63.54 ± 0.38	52.23 ± 0.54	42.68 ± 0.12	33.36 ± 0.21	29.71 ± 0.11	-	-	-
SDRF	54.60 ± 0.39	64.46 ± 0.38	55.51 ± 0.27	43.75 ± 0.31	40.97 ± 0.14	29.70 ± 0.13	82.76 ± 0.23	72.58 ± 0.20	79.10 ± 0.11
SDRF + Undirected	57.54 ± 0.34	70.35 ± 0.60	61.55 ± 0.86	44.46 ± 0.17	41.47 ± 0.21	29.85 ± 0.07	-	-	-
Table 2: Experimental results on common node classification benchmarks. Top two in bold.
approach such as DIGL tends to improve the connectivity among nodes that are at short diffusion
distance; since for a high-homophily dataset these nodes often share the same label, a rewiring method
like DIGL is likely to act as graph denoising and yield improved performance. On the other hand, for
datasets with low homophily, nodes at short diffusion distance are more likely to belong to different
label classes, meaning that a diffusion-based rewiring might inject noise and hence compromise
performance as also noted in Klicpera et al. (2019). Conversely, on a low-homophily dataset, a
curvature-based approach as SDRF modifies the edge set mainly around the most negatively curved
edges, meaning that it decreases the bottleneck without significantly increasing the connectivity
among nodes with different labels. In fact, long-range dependencies are often more relevant in
low-homophily settings, where nodes sharing the same labels are in general not neighbors. This
observation is largely confirmed by experimental results reported in the next section.
5 Experimental Results
Experiment setup. To demonstrate the theoretical results in this paper we ran a suite of semi-
supervised node classification tasks comparing our curvature-based rewiring method SDRF to DIGL
from Klicpera et al. (2019) (GDC with the PPR kernel) and the +FA method from Alon & Yahav
(2021), where the last layer of the GNN is made fully connected. We evaluate the methods on
nine datasets: Cornell, Texas and Wisconsin from the WebKB dataset4; Chameleon and Squirrel
(Rozemberczki et al., 2021) along with Actor (Tang et al., 2009); and Cora (McCallum et al., 2000),
Citeseer (Sen et al., 2008) and Pubmed (Namata et al., 2012). Statistics for these datasets can be found
in Appendix F.1. Our base model is a GCN (Kipf & Welling, 2017). Following Shchur et al. (2018)
and Klicpera et al. (2019) we optimized hyperparameters for all dataset-preprocessing combinations
separately by random search over 100 data splits. Results are reported as average accuracies on a
test set used once with 95% confidence intervals calculated by bootstrapping. We compared the
performance on graphs with no preprocessing, making the graph undirected, +FA, DIGL, SDRF, and
the given combinations. For DIGL + Undirected we symmetrized the diffusion matrix as in Klicpera
et al. (2019), and for SDRF + Undirected we made the graph undirected before applying SDRF. For
more details on the experiments and datasets see Appendix F, and for the hyperparameters used for
each model and preprocessing see Appendix F.4.
Node classification results. Table 2 shows the results of the experiments. As well as reporting
results we give a measure of homophily H(G) proposed by Pei et al. (2019) (restated in Appendix F,
equation 21), by which we can see our experiment set is diverse with respect to homophily. We see
that SDRF improves upon the baseline in all cases, and that the largest improvements are seen on the
low-homophily datasets. We also see that SDRF matches or outperforms DIGL and +FA on most
datasets, supporting our argument that curvature-based rewiring is a viable candidate for improving
GNN performance.
Graph topology change. Furthermore, SDRF preserves the graph topology to a far greater extent
than DIGL due to its surgical nature. Table 3 shows the number of edges added / removed by
the two preprocessings on each dataset as a percentage of the original number of edges. We see
that for optimal performance DIGL makes the graph much denser, which significantly affects the
node degrees and may negatively impact the time and space complexity of the downstream GNN,
which typically are O(E). In comparison, SDRF adds and removes a similar number of edges and
approximately preserves the degree distribution. The effect on the full degree distribution for three
4http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/www/wwkb/
8
Published as a conference paper at ICLR 2022
W1 (Original, SDRF) = 0.28
(b) Actor:
W1(Original,DIGL) = 243.81
W1 (Original, SDRF) = 1.03
(c) Pubmed:
W1 (Original, DIGL) = 247.01
W1 (Original, SDRF) = 0.03
Figure 4: Comparing the degree distribution of the original graphs to the preprocessed version. The x-
axis is node degree in log2 scale, and the plots are a kernel density estimate of the degree distribution.
In the captions we see the Wasserstein distance W1 between the original and preprocessed graphs.
datasets is shown in Figure 4. We see that the de-
gree distribution following SDRF is close to, or
in some cases indistinguishable from, the orig-
inal distribution, whereas DIGL has a more no-
ticeable effect. We also compute the Wasser-
stein distance between the degree distributions,
denoted by W1 in the figure captions, to numer-
ically confirm this observation. For this analysis
extended to all nine datasets see Appendix F.2.
This difference is also visually evident from Fig-
ure 6 in Appendix F.3, where we again observe
that SDRF largely preserves the topology of the
Cornell graph and that the curvature (encoded
by the edge color) is homogenized across the
graph. We also see that the entries of the trained
	DIGL	SDRF
Cornell	351.1% / 0.0%	7.8% / 33.3%
Texas	483.3% / 0.0%	2.4% / 10.4%
Wisconsin	300.6% / 0.0%	1.4% / 7.5%
Chameleon	336.1% / 11.8%	6.4% / 6.4%
Squirrel	228.8% / 1.9%	0.7% / 0.7%
Actor	2444.0% / 2.3%	5.4% / 9.9%
Cora	3038.0% / 0.5%	1.0% / 1.0%
Citeseer	2568.3% / 0.0%	1.1% / 1.1%
Pubmed	2747.1% / 0.1%	0.2% / 0.2%
Table 3: % edges added / removed by method.
network’s Jacobian between a node’s prediction and the features of its 2-hop neighbors (encoded as
node colors) are increased over the graph, which we may attribute to both DIGL and SDRF’s ability
to alleviate the upper bound presented in Theorem 4 and thus reduce over-squashing.
6 Conclusion
In this paper, we studied the graph bottleneck and the over-squashing phenomena limiting the
performance of message passing graph neural networks from a geometric perspective. We started
with a Jacobian approach to determine how the over-squashing phenomenon is dictated by the graph
topology as in equation 2. We then investigated further how the topology induces the bottleneck
and hence causes over-squashing. We introduced a new notion of edge-based Ricci curvature called
Balanced Forman curvature, relating it to the classical Ollivier curvature (Theorem 2). We then
proved in Theorem 4 that negatively-curved edges are responsible for over-squashing, calling for a
possibility of curvature-based rewiring of the graph in order to improve its bottleneckedness. We
show one such possibility (Algorithm 1), inspired by the classical Ricci flow and comment on the
advantages of surgical method such as this. We show both theoretically and experimentally that the
proposed method can be advantageous compared to a diffusion-based rewiring approach, opening the
door for curvature-based rewiring methods for improving GNN performance going forward.
Limitations and future directions. Our paper establishes a geometric perspective on the graph
bottleneck and over-squashing, providing new tools to study and cope with these phenomena. One
limitation of our work is that the theoretical results presented here do not currently extend to multi-
graphs. In addition, the current methodology is agnostic to information beyond the graph topology,
such as node features. In future works, we will develop a notion of the curvature and the corresponding
rewiring method that can take into account such information.
Acknowledgements. This research was supported in part by the EPSRC CDT in Modern Statistics
and Statistical Machine Learning (EP/S023151/1) and the ERC Consolidator Grant No. 724228
(LEMAN). X.D. gratefully acknowledges support from the Oxford-Man Institute of Quantitative
Finance and the EPSRC (EP/T023333/1).
9
Published as a conference paper at ICLR 2022
References
Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications.
In International Conference on Learning Representations, 2021. URL https://openreview.
net/forum?id=i80OPhOCVH2.
Pablo Barcel6, Egor V Kostylev, Mikael Monet, Jorge Perez, Juan Reutter, and Juan Pablo Silva. The
logical expressiveness of graph neural networks. In ICLR, 2019.
Marian Boguna, Ivan Bonamassa, Manlio De Domenico, Shlomo Havlin, Dmitri Krioukov, and
M Angeles Serrano. Network geometry. Nature Reviews Physics, pp. 1-22, 2021.
Sergey Brin and Lawrence Page. The anatomy of a large-scale hypertextual web search engine.
Computer networks and ISDN systems, 30(1-7):107-117, 1998.
Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Velickovic. Geometric deep learning:
Grids, groups, graphs, geodesics, and gauges. arXiv:2104.13478, 2021.
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally
connected networks on graphs. In 2nd International Conference on Learning Representations,
ICLR 2014, 2014.
Ines Chami, Rex Ying, Christopher Re, and Jure Leskovec. Hyperbolic graph convolutional neural
networks. In NeurIPS, 2019.
Jeff Cheeger. A lower bound for the smallest eigenvalue of the laplacian. In Problems in analysis, pp.
195-200. Princeton University Press, 2015.
Fan Chung. Four proofs for the cheeger inequality and graph partition algorithms. In Proceedings of
ICCM, volume 2, pp. 378. Citeseer, 2007.
Fan RK Chung and Fan Chung Graham. Spectral graph theory. Number 92. American Mathematical
Soc., 1997.
Ronald R Coifman and Stephane Lafon. Diffusion maps. Applied and computational harmonic
analysis, 21(1):5-30, 2006.
Michael Defferrard, Xavier Bresson, and Pierre Vndergheynst. Convolutional neural networks on
graphs with fast localized spectral filtering. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29. Curran As-
sociates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/file/
04df4d434d481c5bb723be1b6df1ee65- Paper.pdf.
Robin Forman. Discrete and computational geometry, 2003.
Paolo Frasconi, Marco Gori, and Alessandro Sperduti. A general framework for adaptive processing
of data structures. IEEE Trans. Neural Networks, 9(5):768-786, 1998.
Linton C Freeman. A set of measures of centrality based on betweenness. Sociometry, pp. 35-41,
1977.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In International Conference on Machine Learning, pp.
1263-1272. PMLR, 2017.
Christoph Goller and Andreas Kuchler. Learning task-dependent distributed representations by
backpropagation through structure. In ICNN, 1996.
Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains.
In Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., volume 2,
pp. 729-734. IEEE, 2005.
Richard Hamilton. The ricci flow on surfaces. In Mathematics and general relativity, Proceedings
of the AMS-IMS-SIAM Joint Summer Research Conference in the Mathematical Sciences on
Mathematics in General Relativity, Univ. of California, Santa Cruz, California, 1986, pp. 237-262.
Amer. Math. Soc., 1988.
10
Published as a conference paper at ICLR 2022
William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In NeurIPS, 2017.
Jurgen Jost and ShiPing Liu. Ollivier's ricci curvature, local clustering and curvature-dimension
inequalities on graphs. Discrete & Computational Geometry, 51(2):300-322, 2014.
Anees Kazi, Luca Cosmo, Nassir Navab, and Michael Bronstein. Differentiable graPh module
(dgm) graph convolutional networks. arXiv preprint arXiv:2002.04999, 2020. URL https:
//arxiv.org/pdf/2002.04999.pdf.
Henry Kenlay, Dorina Thanou, and Xiaowen Dong. Interpretable stability bounds for spectral graph
filters. arXiv preprint arXiv:2102.09587, 2021.
Thomas N. Kipf and Max Welling. Semi-Supervised Classification with Graph Convolutional
Networks. In Proceedings of the 5th International Conference on Learning Representations, ICLR
’17, 2017. URL https://openreview.net/forum?id=SJU4ayYgl.
Johannes Klicpera, Stefan WeiBenberger, and Stephan Gunnemann. Diffusion improves graph
learning. In Proceedings of the 33rd International Conference on Neural Information Processing
Systems, 2019.
Yong Lin, Linyuan Lu, and Shing-Tung Yau. Ricci curvature of graphs. Tohoku Mathematical
Journal, Second Series, 63(4):605-627, 2011.
Qi Liu, Maximilian Nickel, and Douwe Kiela. Hyperbolic graph neural networks. In
H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch6-Buc, E. Fox, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso-
ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
103303dd56a731e377d01f6a37badae3-Paper.pdf.
Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph
networks. In NeurIPS, pp. 2153-2164, 2019.
Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Automating the
construction of internet portals with machine learning. Information Retrieval, 3(2):127-163, 2000.
F. Monti, K. Otness, and M. M. Bronstein. Motifnet: A motif-based graph convolutional network for
directed graphs. In IEEE Data Science Workshop, 2018.
Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav
Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks.
In AAAI Conference on Artificial Intelligence, pp. 4602-4609. AAAI Press, 2019.
Florentin Munch. Non-negative ollivier curvature on graphs, reverse poincar\’e inequality, buser
inequality, liouville property, harnack inequality and eigenvalue estimates. arXiv preprint
arXiv:1907.13514, 2019.
Galileo Namata, Ben London, Lise Getoor, Bert Huang, and UMD EDU. Query-driven active
surveying for collective classification. In 10th International Workshop on Mining and Learning
with Graphs, volume 8, pp. 1, 2012.
Chien-Chun Ni, Yu-Yao Lin, Jie Gao, and Xianfeng Gu. Network alignment by discrete ollivier-ricci
flow. In International Symposium on Graph Drawing and Network Visualization, pp. 447-462.
Springer, 2018. URL https://arxiv.org/abs/1809.00320.
Chien-Chun Ni, Yu-Yao Lin, Feng Luo, and Jie Gao. Community detection on networks with ricci
flow. Scientific reports, 9(1):1-12, 2019.
Hoang NT and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass filters.
2019.
Yann Ollivier. Ricci curvature of metric spaces. Comptes Rendus Mathematique, 345(11):643-646,
2007.
11
Published as a conference paper at ICLR 2022
Yann Ollivier. Ricci curvature of markov chains on metric spaces. Journal of Functional Analysis,
256(3):810-864, 2009.
Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node
classification. In ICLR, 2020.
Seong-Hun Paeng. Volume and diameter of a graph and ollivier’s ricci curvature. European Journal
of Combinatorics, 33(8):1808-1819, 2012.
Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric
graph convolutional networks. 2019.
Grisha Perelman. Finite extinction time for the solutions to the ricci flow on certain three-manifolds.
arXiv preprint math/0307245, 2003.
Emanuele Rossi, Fabrizio Frasca, Ben Chamberlain, Davide Eynard, Michael M. Bronstein, and
Federico Monti. Sign: Scalable inception graph neural networks. CoRR, abs/2004.11198, 2020.
URL https://arxiv.org/abs/2004.11198.
Benedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding. Journal
of Complex Networks, 9(2):cnab014, 2021.
Areejit Samal, RP Sreejith, Jiao Gu, ShiPing Liu, Emil Saucan, and Jurgen Jost. Comparative analysis
of two discretizations of ricci curvature for complex networks. Scientific reports, 8(1):1-16, 2018.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The
graph neural network model. IEEE transactions on neural networks, 20(1):61-80, 2008.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI magazine, 29(3):93-93, 2008.
Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Gunnemann. Pitfalls
of graph neural network evaluation. In NIPS workshop, 2018.
Jonathan Shlomi, Peter Battaglia, and Jean-Roch Vlimant. Graph neural networks in particle physics.
Machine Learning: Science and Technology, 2(2):021001, jan 2021. doi: 10.1088/2632-2153/
abbf9a. URL https://doi.org/10.1088/2632- 2153/abbf9a.
Alessandro Sperduti. Encoding labeled graphs by labeling RAAM. In NIPS, 1994.
Alessandro Sperduti and Antonina Starita. Supervised neural networks for the classification of
structures. IEEE Trans. Neural Networks, 8(3):714-735, 1997.
R P Sreejith, Karthikeyan Mohanraj, Jurgen Jost, Emil Saucan, and Areejit Samal. Forman curvature
for complex networks. Journal of Statistical Mechanics: Theory and Experiment, 2016(6):
063206, Jun 2016. ISSN 1742-5468. doi: 10.1088/1742-5468/2016/06/063206. URL http:
//dx.doi.org/10.1088/1742-5468/2016/06/063206.
Jie Tang, Jimeng Sun, Chi Wang, and Zi Yang. Social influence analysis in large-scale networks. In
Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, KDD ’09, pp. 807-816, New York, NY, USA, 2009. Association for Computing
Machinery. ISBN 9781605584959. doi: 10.1145/1557019.1557108. URL https://doi.org/
10.1145/1557019.1557108.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lid, and Yoshua
Bengio. Graph attention networks. In International Conference on Learning Representations,
2018. URL https://openreview.net/forum?id=rJXMpikCZ.
Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon.
Dynamic graph CNN for learning on point clouds. ACM Trans. Graphics, 38(5):1-12, 2019.
Melanie Weber, Emil Saucan, and Jurgen Jost. Coarse geometry of evolving networks. Journal of
complex networks, 6(5):706-732, 2018.
12
Published as a conference paper at ICLR 2022
Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie
Jegelka. Representation learning on graphs with jumping knowledge networks. In International
Conference on Machine Learning, pp. 5453-5462. PMLR, 2018.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In ICLR. OpenReview.net, 2019.
Y. Zhang, S. Pal, M. Coates, and D. Ustebay. Bayesian graph convolutional neural networks for
semi-supervised classification. In AAAI Conference on Artificial Intelligence, 2019.
Daniel Zugner, Oliver Borchert, Amir Akbarnejad, and StePhan Gunnemann. Adversarial attacks
on graph neural networks: Perturbations and their patterns. ACM Transactions on Knowledge
Discovery from Data (TKDD), 14(5):1-31, 2020.
13
Published as a conference paper at ICLR 2022
Appendix
The Appendix is structured as follows:
(i)	In Appendix A we prove Lemma 1 and a side-result about the role of self-loops. We also
summarize how to extend the analysis in Lemma 1 to message passing models with sum
aggregations (not average), meaning those architectures where we do not normalize the
adjacency matrix.
(ii)	In Appendix B we introduce and describe different quantities we use to characterize the
neighbourhood of a given edge i 〜j. These objects are all essential to studying the new
notion of balanced Forman curvature. The focus is on how we can distinguish 4-cycles in a
computationally tractable way without losing too much accuracy.
(iii)	In Appendix C we provide a brief literature review about existing curvature candidates. In
particular, we report the definitions of (modified) Ollivier curvature and Forman curvature.
(iv)	In Appendix D we prove the statements in Section 3, i.e. Theorem 2, Corollary 3, Theorem
4 and Proposition 5. We also comment on the role of the assumptions and compare the
bound in Theorem 2 with the existing literature. Finally, we relate the classical notion of
betweenness centrality to the over-squashing effect and the negatively curved edges in a
graph.
(v)	In Appendix E we prove the results in Section 4, namely Theorem 6 and an analogous result.
(vi)	In Appendix F we describe more fully the experiments from Section 5, including a full anal-
ysis on degree distribution (Appendix F.2) and the hyperparameters used in our experiments
(Appendix F.4).
(vii)	In Appendix G we comment on hardware specifications.
A	Proofs of results in Section 2
Lemma 1. Assume an MPNNas in equation 1. Let i,s ∈ V with S ∈ Sr+ι(i). If ∣Vφ'∣ ≤ α and
∣Vψ'∣ ≤ β for 0 ≤ ' ≤ r ,then
≤ (αβγ^+r(J∖^r+1)is.	(2)
∂h(r+1)
∂Xs
Proof. Let i ∈ V and s ∈ Sr+1(i). We recall that to ease the notations we assume that node features
and hidden representations are scalar. The proof in the more general higher-dimensional case follows
without any modification. We compute
dh∂——=∂ιφr(...)∂χs h(r)
∂xs
n
+ d2φr (...) X aijr (d1ψr (h(r), hjr))dxs hf' + d2ψr(h(', h(r))dXs j∖
jr =1
We can iterate the computation above and see that the right hand side can be expanded as
∂h(r+1)
∂Xs
EE… E	aijr akrjr-1 ...akljθ ZMr kr jr - 1 …kl jθ (X )dXs hjo ,
jr,...,j0 kr ∈{i,jr}	k1 ∈{i,jr,...,j1 }
for some functions Zijr...k1j0 of the input features obtained as products of r + 1 partial derivatives of
the maps φ' and r + 1 partial derivatives of the maps ψ'. Since H⑼=X, we have
∂xs hj0 = δj0
14
Published as a conference paper at ICLR 2022
meaning that the previous sum becomes
∂h(r+1)
∂Xs
E E …E	^ijr ^krjr-1 …^klsZijr -.3)
jr,...,j1 kr ∈{i,jr} k1 ∈{i,jr,...,j1}
Since dG(i, s) = r + 1, the only non-vanishing terms in the sum above are the minimal walks from i
to s. In fact, if there existed a different choice of coefficients yielding a non-zero term then we would
find a walk joining i to s of length lesser than r + 1, which is in contradiction with the definition of
geodesic distance. Since Zi...s(X) is a product of r + 1-partial derivatives of the aggregation and
update maps and by assumption their gradients are bounded by α and β respectively, we conclude
that
≤ (αβ)r + 1 X ^ijr ajrjr_1 …&j\S =。,+1 (Ar+1)is
jr ,...,j1
∂h(r + 1)
∂Xs
which completes the proof of the Lemma.
□
As a byproduct of this analysis, we can also provide a rigorous motivation for the role of self-loops in
GNNs (see Appendix for details):
Corollary 7. If h('+1) = PjcJi ψ'(h(')) ,then hf+1) only depends on nodes that can be reached
via walks of length exactly ` + 1. By adding self-loops, the GNN also takes into account nodes that
can be reached via walks of length r ≤ ` + 1.
Proof. If h('+1) =
Lemma 1 and find
PjCi ψ'(hj')) for each ' ∈ [0,L - 1], then We can argue as in the proof of
∂h('+1)
∂Xs
E aij` ...ajιsψ' (hj')) ∙∙∙ ψ0 (Xs).
j`,...,j1
The combinatorial coefficient aij` . . . aj1 s is non-zero iff there exists a Walk from i to s of length
exactly '+1, since We are not taking into account the contribution coming from self-loops. Conversely,
if each term aj` was replaced by ^j' then we would find that ^j'... ^jιs is non-zero iff there exists
a walk from i to S of length at most r + 1, since the diagonal entries are now positive.	□
Remark 8. As a specific instance of Corollary 7, we note that if we do not include self-loops in
the adjacency matrix, then the output of a 2-layer simplified graph neural network at node i is
independent of the features of neighbours k that do not form a triangle with i. Once again here the
dependence is precisely measured via the Jacobian of the hidden features with respect to the input
features.
Remark 9. We note that the role of self-loops has also implicitly been noted inXu et al. (2018) where
the analysis of the Jacobian of node representations on the graph augmented with self-loops has been
related to lazy random-walks.
GNNs with different aggregations We note that similar conclusions extend to message passing
architectures where the aggregations are sums and not averages meaning that we take the augmented
adjacency without normalizing by the degree matrices. Consistently with Lemma 1, we restrict to
the setting where features and node representations at each layer are scalars to make the discussion
simpler. In line with the Xu et al. (2018) we consider a GNN-model of the form
h('+1) = ReLU (X hj')w`
∖j∈N
Note that the augmented neighbourhood Ni is defined as Ni ∪{i}. Differently from the setting of
Theorem 1 in Xu et al. (2018), the aggregation here is not an average but a simple sum. Let us now
take nodes i and s such that s ∈ Sr+1(i) as in the statement of Lemma 1. In this case, instead of
simply considering the quantity ∣∂h(r+1)∕∂xs |, we normalize the Jacobian entries - obtaining what
15
Published as a conference paper at ICLR 2022
is referred to as influence score in Xu et al. (2018):
Jr+1(i, s)
∂hir+1)
∂Xs
Ekl ∂xk
This of course represents now a relative importance of feature xs on the representation of node i
at layer r + 1. If - similarly to Theorem 1 in Xu et al. (2018) - we assume that all paths in the
computational graph of the model are activated with the same probability, then we obtain that on
average
Jr+1 (i, s)
Ar+1	≤
Pk Ar+1 — VOl(Br+1 (i)) ,
where A = A +1 and vol(S) = Pj∈s dj. In particular, We again find that if We have a tree structure,
then the right hand side decays expOnentially as 2-(r+1) .
B Preliminary analysis of an edge-neighborhood
Given an edge i 〜j, we introduce the sets below:
(i)	]∆(i,j) ：= Sι(i) ∩ Sι(j), the number of triangles based at the edge i 〜j.
(ii)	]i(i,j) := {k∈ S1(i)\S1(j),k 6=j : ∃w ∈ (S1(k) ∩ S1(j)) \ S1(i)}, the number of
nodes k ∈ Sι(i) forming a 4-cycle based at i 〜j without diagonals inside.
(iii)	Qi(j) ：= S1(i) \ ({j} ∪ ]∆(i,j) ∪ ]i(i,j)), simply the complement of the neighbours of i
with respect to the sets introduced in (i) and (ii) once we also exclude j.
In the following we simply write ]△,]自 and Qi when the edge i 〜j is clear from the context.
4-cycle contributions. In general the sets ]i and ]j may differ. This may occur when there exists
a node k belonging to ]i that admits multiple solutions w as in the definition of ]i . This feature
needs to be taken into account when comparing Ollivier’s Ricci curvature to the new notion we
present below. We first introduce the following class to ease the notations.
Definition 2. For any simple, undirected graph G = (V, E), if U ⊂ V, then we set
D(U) ：= {ψ ： U → V, |U| = ∣φ(U儿(z,中(Zy) ∈ E, ∀z ∈ U}.
We note that any 夕 ∈ D(U) is injective.
We may now define a quantity which measures the maximal number of 1-1 pairings that can be
performed from ]i to ]j.
Definition 3. For any edge i 〜j we let
]m(i,j) ：= max {|U| ： U ⊂ 蛤，期：U → 昭，中 ∈D(U )}∙
We often simply write ]m. While the quantity ]m plays a role in the derivation of the Ollivier curvature
of i 〜j it is not COmPutationally-friendly, as to determine ]m we need to identify and distinguish all
possible 4-cycles based at i 〜j and then choose a maximal pairing map. Accordingly, we consider a
looser term which is easier to compute:
Definition 4. For any pair ofadjaCent nodes i 〜j we define
)
γmax(i, j) ：= max
I max{(Ak ∙ (Aj — Ai Θ Aj)) — 1}, max{(Aw ∙ (Ai — Aj Θ Ai)) — 1}
k∈]i	w∈]j
where As denotes the s-th row of the adjacency matrix. We usually simply write γmax.
16
Published as a conference paper at ICLR 2022
Remark 10. We note that given k ∈ Sι(i) \ Sι(j) the term (Ak ∙ (Aj — Ai Θ Aj)) — 1 yields the
number of nodes W forming a 4-cycle of the form i 〜 k 〜 W 〜 j 〜i with no diagonals inside. The
value γmax measures the maximal degeneracy of edges forming 4-cycles, meaning that it is equal to 1
ifffor each k ∈ ]自 there exists a unique node W ∈ ] J such that i 〜 k 〜 W 〜 j 〜i is a 4-cycle.
We now end the discussion about 4-cycle contributions by proving the following inequality, which
allows us to avoid to compute directly the term ]mJ up to giving up some accuracy.
Lemma 11. For any edge i 〜j we have
IHml〉max{的屈 |}
l]□l ≥-------------.
γmax
Proof. The proof is based on a combinatorial argument. Let ]i□ = {k1, . . . , kr} and let ]m□ =
{kι,..., k'}, with ' < r. By definition there exists 夕：{kι,..., k'} → {wι,..., W'}, with k 〜Wi
and Wi ∈ ]J, for 1 ≤ i ≤ '. Given k ∈ ]J \ {kι,..., k'}, then there are no W ∈ ]J \ {wi,..., W'}
such that k 〜w, otherwise we could extend 夕 by setting k → 夕(k) := W and we would then get
∣]m∣ = ' + 1. Accordingly, We have
`
X(Aws ∙ (Ai - Aj θ Ai))-I) ≥ ι]□ι,
s=1
which implies
`
Ymax ∣]m∣ ≡ Ymax ' ≥ ^X(Aws ∙ (Ai - Aj θ Ai)) — 1) ≥ ∣]□∣∙
s=1
□
C Existing curvature candidates
Ollivier Ricci curvature For i ∈ V and α ∈ [0, 1) we define a probability measure on B1(i) by:
{α, j = i
⅛α, j ∈ Sι(i),
0, otherwise.
Before we introduce the Ollivier curvature, we recall that the transportation distance between two
finitely supported probability measures as above can be computed as
Wι(μi1,μα) ：=inf X X MkwdG(k,W),
k∈S1 (i) w∈S1(j)
where dg(∙, ∙) is the geodesic distance on the graph and the infimum is taken over all matrices M
satisfying the marginal constraints:
X Mkw = μα(W),	X Mkw = μiα(k).
k∈S1(i)	w∈S1(j)
We are now ready to define the Ollivier Ricci curvature: the formulation below is due to Lin et al.
(2011).
Definition 5. Given i 〜j we define the α-Ollivier curvature by
Kα(i,j) :=1 — Wι(μiα,μα).	(7)
Since κα(1 — α)-1 is increasing and bounded the quantity below is well-defined:
1 — Wi (μα, μα)
κ(i,j) ：= lim ——Jij .	(8)
α→1	1 — α
17
PUbHShed as a COnferenCe PaPer at ICLR 2022
Forman RiCCi ClIrVatlIre In the following We report a formula for the augmented FOnnaIl RiCCi
CUrVatUre on UIIWeighted graphs SamaI et aL (2018We also IIOte that Fonnan CUrVatUre on graphs
has also been StUdied in Sreth et ah (2016)； Weber et al，(2018
Dennition 6・ FOr any edge i 〜j Ihe augmented FOrman CIlrVamre is giveby
∙) U 4 — <⅛ lj+-∙
We note that SUCh formulation OfCUrVatUre does not distinguish COntribUtions COming from'cycle-
IIlfaCL for the OrthOgOIIaI grid With degree≥ 4》Fonnan RiCCi CUrVatUre is equal to 2(2 I ⅛ Λ O-
ThiS does IIOtrefIeCtthatthe T—hop IIeighbOUrhOod for SUCh a graph grows PYnOmiany in r.
We COnelude this appendix by reporting a°Wer bound for the Ollivier RiCCi CUrVatUre derived in JOSt
祥 LiU (2014We recall that KR With P mpI) WaS definedequation
Theorem 12 (JoSt 际 LiU (2014))∙ FOr any edge i 〜With <⅜ ≤ £ 1he following bound is SaHSAed:
D PRooFS OF RESULTS IN SECTIoN 3
We first recall OUr defi≡∙t5'n Of BalanCed Fonnalr
DennitionFOr any edge •〜JWe E RiC(Ube ZerOymidj} UL OiherWiSe
Ricj).—— 2 + U — 2 + 2 一- - 一一 - (⅛M0 一 (- ÷
d- maxMmi<¾max匚
Where ihe Ias 二 erm is sio be NerO*and hence)NerO.
We-so extend the PreVioUS definition to the Weighted CaSe∙ In this Setting We IetGU(V, E" E) be a
Sim" IoCany finit尸 UIldireCted graph With IIonnaIiZed WeightWe first report the formula for the
augmented FOnnan in the Weighted CaSeSamaI et aL (2018N
3-τε≡+εs+ M "
∈∩c
M ε
∈∖
M
∈∖
Where ε> is taken to be the HerOIlfOnn-a for the area Of a trian-e WhiIe E(∙) denotes SOme
Weighting SCheme for the nodes as WL We PrOPoSe a SimiIar definition for the Weighted Cas-WhiCh
reduces to the Olle discussed above in the COmbmatoriaI SettiIIWe recall that W is the Weighted
adjaceIICy matriX WhiIe』is the COmbatorial OS- MOreOVgWe WriteH — A)+ and
SimiIarly for
DefinitionFOranyPair of adjacenl nodeSiQj m V WedeAneRlCj)be O Vmin 士 <¾L) H
L OiherWiSe We s
18
PUbHShed as a COnferenCe PaPer at ICLR 2022
w-h
ε> 可3(-+-÷ Er"2 m Sls∩ SlQL
and. for a ghw 苹
ImPortant convention，WithOUt°Sing generalityy in the following We-ways assume that 1 ≤ <⅜ ≤
dJn PartiCUlary We Write dl d and di d +-for Some s ≥pomitting to SPeCify that both d and
S are OfCOUrSe depending § P and j. MOreOVeL from now On We Only focus On the UnWeighted case.
We CaIl III PrOVe OUrmaCOmPariSOIl theorem，
Theorem 2∙ GiVen an ImWei.ghled graph G》for any edge •〜JWe have K(Uj) ≥ RiC(Uj)∙
proof. We StiCktO the aforementioned convention dl ddj U d+ S》SMo. The Strategy Ofthe
ProOf amountS to finding a transportationan Providg an UPPer bound for Wl (Wj) and hence
a Iler bound for the CUrVatUre K(UyIn PartiCUlary we COIISiderplans moving the mass0m
to⅛) -otols∙
Ifu L then the OPtimtransport Plan COnSiStS Ofmoving the mass P from P to j and the remaining
mass 1 IaOnjto Sl (j). ThiS yields a Unit WaSSerStn distance between 在 a=dM and hence Zero
OlHvier CUrVatUre> j)》WhiCh COcides With the VaIUe Of balanced FOnnaIl RiC (Uy
ASSUme IlOW that≥A (PoSSibIy IlOII—optimal) transport PlaIlfrOm 吊 to is given byi
(i)	MOVe mass (Ila)/κ+ S) from each node s∩≡5sisunique image in 羊 Under a
bijection E as Per defitn Of
(ii)	The remaining mass On each node WiII =eed to travel by at most distance 3 to S1Q)∙
(Hi) The extra—mass (Ila)(I/l l∕+ s)) On each COmmOn neighbour *> WiII need to
travel by at most distance 2 to sl∙
(iv) MOVe the mass (Ila)/from ysl Q) ∙
(V) MoVe the mass P from QtoyThiSIeaVeSIeft—oVer mass (1 — α)∕+ S) at0me
distribution 七ThiS mass Can be ComPenSated from mass in sl(WhiCh is at distance One∙
(Vi)na=y" WemoVe the mass (1 — α)Of any untouched IlOde in SIto SIalong
a Path OflengthleSSer S equal than threNOte that the remaining mass is equal to
((i — a))— 1 — -- — M) — (1 —+where the last terms COmeS from (V)∙
If We SUm all the COlltribUtionS We find
Wl(KyjKy) ≤ (1 — α)(」一 ÷ 3- (3 — 工+)÷ 2>一 (3 — 工 )+ 3) + α + 工+
J <d+s /d d + s」 /d d-∖- s- d) d +S
+ 3(llα) ()— 一 — 一-— 一-) —
/d d + s J
U(I — α)(工卜(—*I*—2) + 3 τ≡>- — 2) + 2) + 1
- S一
U(Ila)(工卜(—3 一- —*— 2一/— 4 — 23 + 2+ S))) +L
TherefOre We have
K(Uj) U =$ (>)( (*+*+*+ 4 + 2W — 2+ S))) •
p→l 1—a <d + sy d d ))
19
PUbHShed as a COnferenCe PaPer at ICLR 2022
By US5'g Lemma ILWe Can bound the right hand Side as
-J ≥≡-+-r÷smax)L≡-+=-) + 4 + 2"l2κ+s)))URic(ZJ
WhiCh COmPleteS the proof， □
Remark 1∙ByinsPeCtion RiC(Uj) ≥-尸 j-w-h e(u j) as in Theorem 12. have Ihree CaSeE
s^u± H 2-+ 2∕κ+ S)I 2 +-r一/κ+ S) +≡--≤ RiC(U)-because RiC(Uj)
IakeSinIo ClCCOIm 二 he PosiHVe ConlribU,In Of4—cycles as WelL
(H)^c∙) U —1 + 1-+ 1∕(M+ S) +-r-∕κ+ S-Whieh happens M
S
d+;2「'>v。
and
S S
+sl2la-1*-l-*-≤
FrOm Ihe PreVioU,s inequ,aUHes We deve
RiC(ZMe (Z + W
(2 — M— s +≡-+-r一 十-≥^u j)∙
Uii) e-j) U V-/-+S) w--iseq-v-e=52。
S
+sl2ll 一- ≤ O.
d
In this CaSe We hpve
RiC(F.7) ≥ e(F± + j运
d d+s
Coro=ary 3・ FRiCj)k V OfOr any edge i 〜j. Ihen Ihere exis-s a Polynomia 一 P SllCh Ihat
氐3≤n4 WmM
proof. ThiS flls immediateIy from TheOrem 2 and a BiShoP—Gromov type Of result for discrete
OlHvier CUrVatUre on graphs PaelIg (2012□
Ho address the PrOOf Of Theorem 4》We first IIeed the Lemma bell∙
Lemma 1GiVen i 〜With <⅛ ≤ ¥ VRiC(Uj) ≤ —2 + 9 for some O 八 b 八(1 + ——J -hen
Q -
Γ一+ 1

Proof ACCOrdiIlg to OUr COIIVeIltiOIl We Iet 廿 U d and di d 十-for s ≥∙oWe also recall that
QjU SI∖(u≡⅞)If we multiply equation 3 byf¾∙ U 友+we SeethatRiCF J) ≤ —2 + b
iff
4 + 2+- +- + Tm"*+) ≤1 +++ 一 Q),
By assumptionA 一 S* — b) ≥meaning that
S S
1 + 一一 Q) ≥ 4 ++*+*≥ 4 +L
d d
ThereforWe COnClUde
一 Qj 一 √ 11-
θ^l^l
□
Theorem 4・ Consider a MPNN as in eqllals'n 1. Let Q 〜)w-h d$ ≤ d- and assume tha-
2。
PUbHShed as a COnferenCe PaPer at ICLR 2022
s-Ve-≤ P Cmd -v^-≤ βfor each 0 ≤ g ≤ L — L With L ≥ 20"deplh Oflhe MPNN.
(a) There exisls b s0 八 b 八(max:也)——b Λ TQ y and RiC(Uj) ≤ —2 + b∙
Then Ihere exisls QjC S2(°sas∙榛s∙g 一Qj- Vb——1 and for 0 ≤°≤ L — 2 We ha^e
i 4 a⅞+2)
昌葩
K£3 L
Λ S--.
(4)
Proof AS USUaI We Iet e.l d and d-i口也+-for SOme s ≥∙owe first ObSerVe that from the
requirement b2κ+ S) ≤ IiIl (-We derive RiC(Uj) ≤ —2 + b iff
4 + 2+- + Tm"*+) ≤ b+ S).
TherefOre We have
*(3 +) ≤b2+ S)”
meaning that
一 ≤ L (1。)
FrOm IloW on We Iet Qj denote again the complement Sl∖ (SlU 羊 UWithOUtIOSS Of
generality We SetUO and hence h) H 2"the Very Same PrOOf applies to any Other ChOiCe OfeO.
GiVeIl k m Qt SinCe k m S2(We Can apply Lemma 1 and derive
一一 ≤ 6( A， (11)
We may expand the Pler Ofthe augmented IIOnnaliZed adj aceIICy matriX as
, 一 —— R 一 .
√(c⅛ + 1)(C⅞ + 1) Γ + 一
If We introduce the SetQjU-m Q⅛∙QMs-VIh We CaIl then Write
s-∈Qj
s-∈Qj
' 1 — D Ib
√(c⅛ + 1)(C⅞ + 1)京SIG⅛)∩s⅛) Γ 十 一
+ M
冢甲Qj
tc∈--)∩-s∩sls
(12)
Whereinth-ast equality we have again used the fact that/ɛ m Q-∙ff ther2∙s * m sls∩sls∩sls∙
Ho avoid heavy IlOtationWe introduce K USl (∩sl∩sl∙ Let US first focus on the first
SUmequation 1We have
IA二二 Q一
A - AL
(13)
We IlI COnSiderthe SeCOlld SUm5'equation 12, We assume≡-≥ LOtherWiSe QjUθWe Iet
≈∙-ʌ-7cm≡> *^八
二Qj
CΓ一
+Ql
for some CVO-Obe ChOSen beow∙ τhen=he SeCOnd SUm in equation 12 Can be Spli- as
(14)
21
PUbHShed as a COnferenCe PaPer at ICLR 2022
SinCe any IVmKhaS degree at least threeyWe Can bound the first term in equation 14 as
1"dp
We now ObSerVe rhs
一不 ∩ll≈A
g ⅛dl
-7w) m E .. k C
maX
tc∈Q
Since≡≤ (I ∕c-α一 /≡-+ 2/CfOr any IVmQWe See that the first term in equation 14 CaIl be
bounded by
1 一不 ∩p
(1叵+咚IP< f "叵+咚度
KΓ一 C" 4 I KΓ4C) 4
(15)
by defi≡∙t5-n Of"We IlI bound the SeCOlld term in equation 14 as
s-∈α

⅛u + 1
I 0>一
Where We have USed that-1 ≤ C 一r一-Q-if * m K ∖Q∙ SinCe
一K∖Q^te一 八 M一
⅛d I ⅛3 I <3
≤ ≤ v⅜∑i
We See rhs
1 0l>一
λ∕*⅛ +1 -Q-
"∖p≤
sl^-l>v⅜∑↑g>= 0>F.
一 e
(16)
We are now ready to complete the Pr∞f Ofthe theorem。ACCOrdmg to equation Ilit suffices to ShOW
rhs
M (⅛⅛≤ ~1 卞
ke Qe
FrOm equation 12 and equation 13 We derive that the Ieft hand Side Ofthe equation above is bounded
by
tc∈ V⅛
Where in the IaStmeqUaHty We have USed Lemma 14 to bound 一 Q-——1 by-In PartiCUIar We note that
ifuthen QIand the bound WOUId be Simply COntrled by b as claimed，WheIl MV O-
We Can USe equation 15 and equation 16 to estimate the SeCOlld term from above by
C 0Q之
+
E
Q -
S3.
22
Published as a conference paper at ICLR 2022
1
IQjl
By applying Lemma 14 we get
1(1	2l]∆l ʌ	Cl]∆1 piɪ-1	1 / 1 . 9 δ ʌ , r^δ pF-ι
4 (C + CQjl) + E，|力| ≤ 4 (C + 2c) + Cδ，|力|.
We now choose C = δ-1/4, so that the previous quantity can be bounded by
C + 2C) + Cδp∣]∆ | ≤ 4 (δ4 + 2δ4) + δ4 Pδ∣]∆∣ ≤ 4∙ (δ4 + 2δ4) + δ4
where in the last inequality we have used equation 10. Therefore, we have shown that
X (A2)ik ≤ δ + 4 G4 + 2δ4) + δ1 ≤ 3δ1
k∈Qj
where we have used that δ < 1. This completes the proof (once we absorb the extra factor 3 in the
constant αβ in equation 11).	□
Remark 15. The requirement δ,max{di, dj} < 1 can be replaced by a more general bound
δy∕max{di, dj } < r. The argument above extends to this case up to renaming the constant αβ in
the statement so to include an extra factor r.
We note that the condition δ max{di, dj} < r would be stronger than the one appearing in (ii) of
Theorem 4. In this regard, we recall that for a d -tree the curvature satisfies Ric(i,j) = -2 + d.
We can also prove Proposition 5:
Proposition 5. If Ric(i,j) ≥ k > 0 forall i 〜j, then λι∕2 ≥ hg ≥ 2.
Proof. This follows as an immediate Corollary of Theorem 2 and (Lin et al., 2011, Theorem 4.2). □
Betweenness centrality to measure bottleneck. In equation 2 we have derived how the topology
of the graph affects the dependence of the hidden node representation hi(r+1) on the input feature xs,
for nodes i,s at distance r + 1. We note that in this case A；+1 is exactly measuring the number of
minimal paths from i to s. If the receptive field Br+1(i) is a binary tree, then we have seen that the
entry of the power matrix decays exponentially. The reason for such decay stems from the existence
of exponentially many nodes in the receptive field combined with the lack of multiple minimal paths
(shortcuts). When such conditions hold, most of the minimal paths go through the same nodes,
which is exactly what happens for the tree where each node is in the minimal paths between different
branches. Since the frequency in which a node appears in the minimal path of distinct pairs of nodes
is measured by the betweenness centrality Freeman (1977), we propose a topological characterization
of the ‘bottleneckedness’ of a graph as follows:
Definition 9 (bottleneck). The bottleneck-value of G is bg := n Pn=ι b(i), where b(i) denotes the
betweenness centrality on node i.
From a standard combinatorial argument it follows that if G is connected, then
bG = 1 X (dG(i,j) - 1) .	(17)
n
i,j
We note that bG = 0 iff G is the complete graph Kn . Therefore, bG determines how far the given
topology is from Kn , with the latter representing the limit case of a fully connected layer Alon &
Yahav (2021) where no bottleneck may occur as any pair of nodes would be neighbours. This further
supports our intuition that the betweenness centrality is a good topological candidate for providing a
global measurement of bottleneckedness in the graph.
It also follows from equation 17 that any update to the graph topology consisting of edge additions
would decrease bG and thus reduce the bottleneck. The quantity bG is global in nature though and
hence lacks robustness. As a pedagogical example, consider a barbell G(m, 2r + 1), with m the size
of the two cliques joined by a path of length 2r + 1 and focus on the edge i 〜j in the middle of
such path. Nodes i and j are central to the graph, in the sense that most minimal paths go through
them and indeed their betweenness centrality is b(i) = b(j) = (m + r)2 + (m + r). If now we add a
23
PUbHShed as a COnferenCe PaPer at ICLR 2022
Sin~e edgejoining the two cliques κγn. the values bsand bsdecrease dramatic-Iy by-m2 +τ)∙
SinCe the OPemtion is IIOil—10CaLWe See that the representation hOf any MPNN is imaffected by
the edge additIIfOr anym (O r).and similarIy for j. Eventif We keep adding edgethe
receptive fields BSWi= be affected for SmalI ValUeS Of S as welthe drawback Of SUCh approach
is that the resulting adjacency may be SiglImCantly differenconverselyy the CUrVatUre ProvideS a
more precisIoCaI and hence robuWay OfCOntrHng the bottleneck and hence the OVer—squashing
problem，NOnetheIeSWe relate the betweenness Centrality to the JaCObian Ofthe hidden feature
TheoremIGiVen •〜Iet Q:口 sl∩ slCy RiC (∙) ≤ —2 + 9 for O Λ Λ
(1 + m岂——-hen
Proof We rewrite the quantity5'the Statement as
Γl一+ 1fMba+s∙
By defitgiven a IIOde 不 the betweenness CeIltrality Of k is given by
-E∈y 一 sɪ-fɪ*
Where Qst, is the IIUmber Of minimal PathS between S and t WhiIe Q--is the IIUmber Of minimal
PathS from StOE PaSSilIg through FFOT ConveniencWe introduce the Set QjC Qj defined by
Qj .♦口 m QjlQa-W VIy♦
EqUiVaIelltlyyQj COIISiStS OfthOSe IIOdesin sls∖ slsWhiChfOnn a 4—CyCIe based a'∙,∙ With
a diagonal inside，IlIdeea am Qj and Qi^ VL Bs there exists more than Olle minimal Path
between i and 6 in additionthe OllePaSSing through nodeFor any SUCh Path there exists
k m Sls∩ Sl(n7∙Since W m Qj and Qj ∩5su-we derive that k m SlQ) as WeIL We then get
QSt(k) V
s-∈≡r
s-m0>-E∈y一 sɪ-fɪ*
QSt,
M M
Δ, WmQJ
QiW
M kr
s-2>
By SUmmmg Q 曰(-for all *> we Obr-∙n all Lhe 2—1Ong minimal pshs b2Ween i and W Wirh the
exception Of the OIle PaSSing through N

(18)
s-∈r

On rhe Orher hand We also have
bs
Qst1-V
Q"s
Q St
Qi.z
Qi.z
(19)
By COmbi≡∙ng equation 18 and equation 19 We finally get
M-+l
Mbs+bs≥
s-∈r
≡-+l
≡-+l
一Qj
≡-+l
M卜
— Qiw
冬∈Q'
QiZ
Where in the IaSt equality We have USed that by definition QaZ UI for-1 ZmQj / Qj. By Lemma 14
the last quantity iarger than b——1 ∙ □
24
Published as a conference paper at ICLR 2022
E Proofs of results in Section 4
Theorem 6. Let S ⊂ V with vol (S) ≤ Vol(G) / 2 Then hs,α ≤ (1-α) davg(S) hs, where davg(S)
and dmin (S) are the average and minimum degree on S, respectively.
Proof. Given a signal f : V → R on the vertex set and U ⊂ V, analogously to Chung (2007), we
introduce the notation
f(U) :=	f(i).
i∈U
Let us rewrite the new Cheeger constant hS,α as follows:
hS,α = |S|	X (Ra )ij = |S| XS Ra(S)
i∈S,j∈S
with χS the characteristic function of the subset S, i.e. χS(i) = 1 iff i ∈ S. Since the graph G is
connected, we can bound hS,a from above as
QS灰⑸="JiS(Ra)ij ≤" i∈X∈s(Ra)ijdmd⅛
iSi XSDRa(S) dmn(Sy.
It was proven in (Chung, 2007, Lemma 5) that
XSDRa(S) ≤ 1-α ∣∂S∣.	(20)
α
By applying equation equation 20 to the bound for the Cheeger constant hS,a we finally see that
1	1	1	1 1 -α	1
hS,a =网 XS Ra(S) ≤ 同 XS DRa(S) E ≤ 冏 丁
W -hS Vol(S) 藐W
1 - α h davg (S)
ɑ S dmin(S)
□
We also report an equivalent result, again relying on (Chung, 2007, Lemma 5).
Proposition 17. Let S ⊂ V with vol(S) ≤ vol(G)/2. For any k ∈ N, there exists Sk,a ⊂ S with
vol(Sk,a) ≥ vol(S)(1 - (2k)-1) such that
X1-α
(Ra)ij ≤ k	hS,
j∈S
for all i ∈ Sk,a.
Proof. Let k ∈ N. By modifying slightly the argument in (Chung, 2007, Lemma 5), we derive that
Sk a := {i ∈ S : XiRa(S) ≥ k1α hS}
,α
satisfies
1α	1α
Ra-∣∂S∣≥ vol(Sk,a)k—hS.
Therefore, we obtain
,zQ0、“1 Vol(S)
Vol(Sk,a) ≤ £ 一2 —.
We then conclude that the complement of Sk0 ,a has volume greater or equal than vol(S)(1 - (2k)-1),
which completes the proof.	□
Remark 18. The previous proposition shows that after sparsifying the personalized page rank
operator Ra as suggested in Klicpera et al. (2019) by setting entries below some threshold equal to
zero, there will still be only few edges connecting different communities, once again highlighting that
random-walk based methods are generally not suited to tackle the graph bottleneck.
25
Published as a conference paper at ICLR 2022
F Experiments
Our experiments in this paper are semi-supervised node classification (semi-supervised in that the
graph structure provides some unlabelled information) on nine common graph learning datasets.
Cornell, Texas and Wisconsin are small heterophilic datasets based on webpage networks from
the WebKB dataset. Chameleon and Squirrel (Rozemberczki et al., 2021) are medium heterophilic
datasets based on Wikipedia networks, along with Actor, the actor-only induced subgraph of the film-
director-actor-writer network (Tang et al., 2009). Cora (McCallum et al., 2000), Citeseer (Sen et al.,
2008) and Pubmed (Namata et al., 2012) are medium homophilic datasets based on citation networks.
As in Klicpera et al. (2019), for all experiments we consider the largest connected component of the
graph.
When splitting the data into train/validation/test sets, we first separate the data into a development set
and the test set. This is done once to ensure the test set is not used for any training or hyperparameter
fitting before the final evaluation. For each of the 100 random splits the development set is divided
randomly into a train set and a validation set, where we train models on the train set and evaluate
on the validation set. We fit hyperparameters by random search, maximising the mean accuracy
across the validation sets. The accuracy then reported in Table 2 is the mean accuracy on the test set
from models trained on the train sets with the chosen hyperparameters, along with a 95% confidence
interval calculated by bootstrapping the test set accuracies with 1000 samples. For Cora, Citeseer
and Pubmed the development set contains 1500 nodes with the rest kept for the test set, and for each
random split the train set is chosen to contain 20 nodes of each class while the rest form the validation
set. As this is the same method as Klicpera et al. (2019) and we use the same random seeds, we
are using the same test set and expect to have comparable results. For the remaining datasets we
perform a 60/20/20 split, with 20% of nodes set aside as the test set and then for each random split
the remaining 80% is split into 60% training, 20% validation.
The homophily index H(G) proposed by Pei et al. (2019) is defined as
1 「Number of v's neighbors Who have the same label as V
|V|	Number of v's neighbors
(21)
F.1 Datasets
For datasets With disconnected graphs, the statistics shoWn here are for the largest connected compo-
nent.
	Cornell	Texas	Wisconsin	Chameleon	Squirrel	Actor	Cora	Citeseer	Pubmed
H(G)	0.11	0.06	0.16	0.25	0.22	0.24	0.83	0.72	0.79
Nodes	140	135	184	832	2186	4388	2485	2120	19717
Edges	219	251	362	12355	65224	21907	5069	3679	44324
Features	1703	1703	1703	2323	2089	931	1433	3703	500
Classes	5	5	5	5	5	5	7	6	3
Directed?	Yes	Yes	Yes	Yes	Yes	Yes	No	No	No
26
Published as a conference paper at ICLR 2022
F.2 Degree distributions
W1 (Original, SDRF) = 1.01
W1 (Original, SDRF) = 0.39
W1 (Original, SDRF) = 0.28
W1 (Original, DIGL) = 96.30
W1 (Original, SDRF) = 1.93
W1 (Original, DIGL) = 135.41
W1 (Original, SDRF) = 0.50
W1 (Original, SDRF) = 1.03
(g)	Cora:
W1 (Original, DIGL) = 247.84
W1 (Original, SDRF) = 0.14
(h)	Citeseer:
W1 (Original, DIGL) = 178.28
W1 (Original, SDRF) = 0.15
(i)	Pubmed:
W1 (Original, DIGL) = 247.01
W1 (Original, SDRF) = 0.03
Figure 5: Comparing the degree distribution of the original graphs to the preprocessed version. The x-
axis is node degree in log2 scale, and the plots are a kernel density estimate of the degree distribution.
In the captions we see the Wasserstein distance W1 between the original and preprocessed graphs.
27
Published as a conference paper at ICLR 2022
F.3 Visualizing curvature and sensitivity to features
Figure 6: Rewiring of the Cornell graph. Left-to-right: original graph, DIGL, and SDRF rewiring.
Edges are colored by curvature; nodes are colored by the maximum absolute entry of a trained 2-layer
GCN’s Jacobian between the GCN’s prediction for that node and the features of the nodes 2 hops
away in the original graph. SDRF homogenizes curvature and so lifts the upper bound on the Jacobian
from Theorem 4. DIGL also does to an extent, though at the expense of preserving graph topology.
F.4 Hyperparameters
Table 4: Hyperparameters for GCN with no preprocessing (None).
Dataset	Dropout	Hidden depth	Hidden dimension	Learning rate	Weight decay
Cornell	0.3060	1	128	0.0082	0.1570
Texas	0.2346	1	128	0.0072	0.0037
Wisconsin	0.2869	1	64	0.0281	0.0113
Chameleon	0.7304	3	128	0.0248	0.0936
Squirrel	0.5974	2	64	0.0136	0.1346
Actor	0.7605	1	64	0.0290	0.0619
Cora	0.4144	1	64	0.0097	0.0639
Citeseer	0.7477	1	128	0.0251	0.4577
Pubmed	0.4013	1	64	0.0095	0.0448
Table 5: Hyperparameters for GCN with the input graph made undirected (Undirected).
Dataset	Dropout	Hidden depth	Hidden dimension	Learning rate	Weight decay
Cornell	0.6910	1	64	0.0185	0.0285
Texas	0.2665	1	128	0.0069	0.0035
Wisconsin	0.2893	2	128	0.0142	0.0001
Chameleon	0.4657	3	64	0.0189	0.0423
Squirrel	0.5944	2	64	0.0081	0.0309
Actor	0.6626	2	64	0.0195	0.0219
28
Published as a conference paper at ICLR 2022
Table 6: Hyperparameters for GCN with the last layer made fully connected (+FA from Alon &
Yahav (2021)).
Dataset	Dropout	Hidden depth	Hidden dimension	Learning rate	Weight decay
Cornell	0.2643	1	128	0.0216	0.0760
Texas	0.2207	1	128	0.0102	0.4450
Wisconsin	0.2613	3	64	0.0057	0.0131
Chameleon	0.7783	3	64	0.0156	0.0108
Squirrel	0.3654	3	64	0.0077	0.1922
Actor	0.3824	1	64	0.0165	0.1168
Cora	0.7840	2	128	0.0149	0.1429
Citeseer	0.5460	2	64	0.0066	0.0758
Pubmed	0.3376	2	128	0.0204	0.0215
Table 7: Hyperparameters for GCN with DIGL preprocessing, or Graph Diffusion Convolution with
PPR kernel (DIGL). Descriptions for α, k and can be found in Klicpera et al. (2019).
Dataset	Dropout	Hidden depth	Hidden dimension	Learning rate	Weight decay	α	k	
Cornell	0.6294	1	64	0.0134	0.0258	0.1795	64	-
Texas	0.2382	2	128	0.0063	0.0153	0.0206	32	-
Wisconsin	0.2941	1	128	0.0083	0.0226	0.1246	-	0.0001
Chameleon	0.4191	1	128	0.0052	0.0001	0.0244	64	-
Squirrel	0.6844	1	128	0.0056	0.4537	0.0395	32	-
Actor	0.7820	1	64	0.0170	0.0102	0.1584	128	-
Cora	0.3315	1	64	0.0284	0.0572	0.0773	128	-
Citeseer	0.5561	1	64	0.0094	0.5013	0.1076	-	0.0008
Pubmed	0.4915	2	128	0.0057	0.0597	0.1155	128	-
Table 8: Hyperparameters for GCN with DIGL preprocessing followed by symmetrizing the graph
diffusion matrix (DIGL + Undirected).
Dataset	Dropout	Hidden depth	Hidden dimension	Learning rate	Weight decay	α	k	
Cornell	0.6294	1	64	0.0134	0.0258	0.1795	64	-
Texas	0.2382	2	128	0.0063	0.0153	0.0206	32	-
Wisconsin	0.2941	1	128	0.0083	0.0226	0.1246	-	0.0001
Chameleon	0.4191	1	128	0.0052	0.0001	0.0244	64	-
Squirrel	0.7094	1	64	0.0172	0.0192	0.1610	64	-
Actor	0.4012	1	64	0.0161	0.0141	0.0706	-	0.0016
Cora	0.3315	1	64	0.0284	0.0572	0.0773	128	-
Citeseer	0.5561	1	64	0.0094	0.5013	0.1076	-	0.0008
Pubmed	0.4915	2	128	0.0057	0.0597	0.1155	128	-
29
Published as a conference paper at ICLR 2022
Table 9: Hyperparameters for GCN with SDRF preprocessing (SDRF). Max iterations, τ and C+ are
the SDRF parameters described in Algorithm 1.
Dataset	Dropout	Hidden depth	Hidden dimension	Learning rate	Weight decay	Max iterations	τ	C+
Cornell	0.2411	1	128	0.0172	0.0125	135	130	0.25
Texas	0.5954	1	128	0.0278	0.0623	47	172	2.25
Wisconsin	0.6033	1	128	0.0295	0.1920	27	32	0.5
Chameleon	0.7265	1	128	0.0180	0.2101	832	77	3.35
Squirrel	0.7401	2	16	0.0189	0.2255	6157	178	0.5
Actor	0.6886	1	128	0.0095	0.0727	1010	69	1.22
Cora	0.3396	1	128	0.0244	0.1076	100	163	0.95
Citeseer	0.4103	1	64	0.0199	0.4551	84	180	0.22
Pubmed	0.3749	3	128	0.0112	0.0138	166	115	14.43
Table 10: Hyperparameters for GCN with the input graph made undirected followed by SDRF
preprocessing (SDRF + Undirected).
Dataset	Dropout	Hidden depth	Hidden dimension	Learning rate	Weight decay	Max iterations	τ	C+
Cornell	0.2911	1	128	0.0056	0.0336	126	145	0.88
Texas	0.2160	1	64	0.0229	0.0137	89	22	1.64
Wisconsin	0.2452	1	64	0.0113	0.1559	136	12	7.95
Chameleon	0.4886	1	32	0.0268	0.4056	2441	252	2.84
Squirrel	0.4249	1	64	0.0295	0.1397	787	43	17.19
Actor	0.6705	1	128	0.0115	0.0447	1141	44	11.17
G Hardware specifications
Our experiments were performed on a server with the following specifications:
Component	Specification
Architecture CPU GPU RAM OS	x86_64 40x Intel(R) Xeon(R) Silver 4210R CPU @ 2.40GHz 4x GeForce RTX 3090 (24268MiB/GPU) 126GB Ubuntu 20.04.2 LTS
30