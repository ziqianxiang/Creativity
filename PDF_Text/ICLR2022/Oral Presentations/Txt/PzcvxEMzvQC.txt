Published as a conference paper at ICLR 2022
GeoDiff: a Geometric Diffusion Model for
Molecular Conformation Generation
Minkai Xu1,2, Lantao Yu3, Yang Song3, Chence Shi1,2, Stefano Ermon3*, Jian Tang1,4,5*
1 Mila - QUebec AI Institute, Canada 2UniversitC de Montreal, Canada
3Stanford University, USA 4HEC MontreaL Canada 5CIFAR AI Research Chair
{minkai.xu,chence.shi}@umontreal.ca
{lantaoyu,yangsong,ermon}@cs.stanford.edu
jian.tang@hec.ca
Ab stract
Predicting molecular conformations from molecular graphs is a fundamental prob-
lem in cheminformatics and drug discovery. Recently, significant progress has been
achieved with machine learning approaches, especially with deep generative mod-
els. Inspired by the diffusion process in classical non-equilibrium thermodynamics
where heated particles will diffuse from original states to a noise distribution, in
this paper, we propose a novel generative model named GeoDiff for molecular
conformation prediction. GeoDiff treats each atom as a particle and learns to
directly reverse the diffusion process (i.e., transforming from a noise distribution
to stable conformations) as a Markov chain. Modeling such a generation process
is however very challenging as the likelihood of conformations should be roto-
translational invariant. We theoretically show that Markov chains evolving with
equivariant Markov kernels can induce an invariant distribution by design, and
further propose building blocks for the Markov kernels to preserve the desirable
equivariance property. The whole framework can be efficiently trained in an end-to-
end fashion by optimizing a weighted variational lower bound to the (conditional)
likelihood. Experiments on multiple benchmarks show that GeoDiff is superior or
comparable to existing state-of-the-art approaches, especially on large molecules.1
1	Introduction
Graph representation learning has achieved huge success for molecule modeling in various tasks rang-
ing from property prediction (Gilmer et al., 2017; Duvenaud et al., 2015) to molecule generation (Jin
et al., 2018; Shi et al., 2020), where typically a molecule is represented as an atom-bond graph.
Despite its effectiveness in various applications, a more intrinsic and informative representation
for molecules is the 3D geometry, also known as conformation, where atoms are represented as
their Cartesian coordinates. The 3D structures determine the biological and physical properties of
molecules and hence play a key role in many applications such as computational drug and material de-
sign (Thomas et al., 2018; Gebauer et al., 2021; Jing et al., 2021; Batzner et al., 2021). Unfortunately,
how to predict stable molecular conformation remains a challenging problem. Traditional methods
based on molecular dynamics (MD) or Markov chain Monte Carlo (MCMC) are very computationally
expensive, especially for large molecules (Hawkins, 2017).
Recently, significant progress has been made with machine learning approaches, especially with
deep generative models. For example, Simm & Hernandez-Lobato (2020); Xu et al. (2021b) studied
predicting atomic distances with variational autoencoders (VAEs) (Kingma & Welling, 2013) and
flow-based models (Dinh et al., 2017) respectively. Shi et al. (2021) proposed to use denoising score
matching (Song & Ermon, 2019; 2020) to estimate the gradient fields over atomic distances, through
which the gradient fields over atomic coordinates can be calculated. Ganea et al. (2021) studied
generating conformations by predicting both bond lengths and angles. As molecular conformations
are roto-translational invariant, these approaches circumvent directly modeling atomic coordinates by
leveraging intermediate geometric variables such as atomic distances, bond and torsion angles, which
1Code is available at https://github.com/MinkaiXu/GeoDiff.
1
Published as a conference paper at ICLR 2022
are roto-translational invariant. As a result, they are able to achieve very compelling performance.
However, as all these approaches seek to indirectly model the intermediate geometric variables, they
have inherent limitations in either training or inference process (see Sec. 2 for a detailed description).
Therefore, an ideal solution would still be directly modeling the atomic coordinates and at the same
time taking the roto-translational invariance property into account.
In this paper, we propose such a solution called GeoDiff, a principled probabilistic framework
based on denoising diffusion models (Sohl-Dickstein et al., 2015). Our approach is inspired by the
diffusion process in nonequilibrium thermodynamics (De Groot & Mazur, 2013). We view atoms
as particles in a thermodynamic system, which gradually diffuse from the original states to a noisy
distribution in contact with a heat bath. At each time step, stochastic noises are added to the atomic
positions. Our high-level idea is learning to reverse the diffusion process, which recovers the target
geometric distribution from the noisy distribution. In particular, inspired by recent progress of
denoising diffusion models on image generation (Ho et al., 2020; Song et al., 2020), we view the
noisy geometries at different timesteps as latent variables, and formulate both the forward diffusion
and reverse denoising process as Markov chains. Our goal is to learn the transition kernels such
that the reverse process can recover realistic conformations from the chaotic positions sampled
from a noise distribution. However, extending existing methods to geometric generation is highly
non-trivial: a direct application of diffusion models on the conformation generation task lead to poor
generation quality. As mentioned above, molecular conformations are roto-translational invariant,
i.e., the estimated (conditional) likelihood should be unaffected by translational and rotational
transformations (Kohler et al., 2020). To this end, We first theoretically show that a Markov process
starting from an roto-translational invariant prior distribution and evolving with roto-translational
equivariant Markov kernels can induce an roto-translational invariant density function. We further
provide practical parameterization to define a roto-translational invariant prior distribution and a
Markov kernel imposing the equivariance constraints. In addition, we derive a weighted variational
lower bound of the conditional likelihood of molecular conformations, which also enjoys the roto-
translational invariance and can be efficiently optimized.
A unique strength of GeoDiff is that it directly acts on the atomic coordinates and entirely bypasses
the usage of intermediate elements for both training and inference. This general formulation enjoys
several crucial advantages. First, the model can be naturally trained end-to-end without involving
any sophisticated techniques like bilevel programming (Xu et al., 2021b), which benefits from small
optimization variances. Besides, instead of solving geometries from bond lengths or angles, the
one-stage sampling fashion avoids accumulating any intermediate error, and therefore leads to more
accurate predicted structures. Moreover, GeoDiff enjoys a high model capacity to approximate the
complex distribution of conformations. Thus, the model can better estimate the highly multi-modal
distribution and generate structures with high quality and diversity.
We conduct comprehensive experiments on multiple benchmarks, including conformation generation
and property prediction tasks. Numerical results show that GeoDiff consistently outperforms
existing state-of-the-art machine learning approaches, and by a large margin on the more challenging
large molecules. The significantly superior performance demonstrate the high capacity to model the
complex distribution of molecular conformations and generate both diverse and accurate molecules.
2	Related Work
Recently, various deep generative models have been proposed for conformation generation. Among
them, CVGAE (Mansimov et al., 2019) first proposed a VAE model to directly generate 3D atomic
coordinates, which fails to preserve the roto-translation equivariance property of conformations and
suffers from poor performance. To address this problem, the majority of subsequent models are
based on intermediate geometric elements such as atomic distances and torsion angles. A favorable
property of these elements is the roto-translational invariance, (e.g. atomic distances does not
change when rotating the molecule), which has been shown to be an important inductive bias for
molecular geometry modeling (Kohler et al., 2020). However, such a decomposition suffers from
several drawbacks for either training or sampling. For example, GraphDG (Simm & Hernandez-
Lobato, 2020) and CGCF (Xu et al., 2021a) proposed to predict the interatomic distance matrix
by VAE and Flow respectively, and then solve the geometry through the Distance Geometry (DG)
technique (Liberti et al., 2014), which searches reasonable coordinates that matches with the predicted
2
Published as a conference paper at ICLR 2022
distances. ConfVAE further improves this pipeline by designing an end-to-end framework via bilevel
optimization (Xu et al., 2021b). However, all these approaches suffer from the accumulated error
problem, meaning that the noise in the predicted distances will misguide the coordinate searching
process and lead to inaccurate or even erroneous structures. To overcome this problem, ConfGF (Shi
et al., 2021; Luo et al., 2021) proposed to learn the gradient of the log-likelihood w.r.t coordinates.
However, in practice the model is still aided by intermediate geometric elements, in that it first
estimates the gradient w.r.t interatomic distances via denoising score matching (DSM) (Song &
Ermon, 2019; 2020), and then derives the gradient of coordinates using the chain rule. The problem
is, by learning the distance gradient via DSM, the model is fed with perturbed distance matrices,
which may violate the triangular inequality or even contain negative values. As a consequence, the
model is actually learned over invalid distance matrices but tested with valid ones calculated from
coordinates, making it suffer from serious out-of-distribution (Hendrycks & Gimpel, 2016) problem.
Most recently, another concurrent work (Ganea et al., 2021) proposed a highly systematic (rule-based)
pipeline named GEOMOL, which learns to predict a minimal set of geometric quantities (i.e. length
and angles) and then reconstruct the local and global structures of the conformation in a sophisticated
procedure. Besides, there has also been efforts to use reinforcement learning for conformation
search Gogineni et al. (2020). Nevertheless, this method relies on rigid rotor approximation and can
only model the torsion angles, and thus fundamentally differs from other approaches.
3	Preliminaries
3.1	Notations and Problem Definition
Notations. In this paper each molecule with n atoms is represented as an undirected graph G = hV, Ei,
where V = {vi}in=1 is the set of vertices representing atoms and E = {eij | (i, j) ⊆ |V| × |V|} is
the set of edges representing inter-atomic bonds. Each node vi ∈ V describes the atomic attributes,
e.g., the element type. Each edge eij ∈ E describes the corresponding connection between vi and
vj , and is labeled with its chemical type. In addition, we also assign the unconnected edges with a
virtual type. For the geometry, each atom in V is embedded by a coordinate vector c ∈ R3 into the
3-dimensional space, and the full set of positions (i.e., the conformation) can be represented as a
matrix C = [ci, c?,…，Cn] ∈ Rn×3.
Problem Definition. The task of molecular conformation generation is a conditional generative
problem, where we are interested in generating stable conformations for a provided graph G . Given
multiple graphs G, and for each G given its conformations C as i.i.d samples from an underlying
Boltzmann distribution (Noe et al., 2019), our goal is learning a generative model pθ (C|G), which is
easy to draw samples from, to approximate the Boltzmann function.
3.2	Equivariance
Equivariance is ubiquitous in machine learning for atomic systems, e.g., the vectors of atomic dipoles
or forces should rotate accordingly w.r.t. the conformation coordinates (Thomas et al., 2018; Weiler
et al., 2018; Fuchs et al., 2020; Miller et al., 2020; Simm et al., 2021; Batzner et al., 2021). It has
shown effectiveness to integrate such inductive bias into model parameterization for modeling 3D
geometry, which is critical for the generalization capacity (Kohler et al., 2020; Satorras et al., 2021a).
Formally, a function F : X → Y is equivariant w.r.t a group G if:
F ◦ Tg(x) = Sg ◦ F(x),	(1)
where Tg and Sg are transformations for an element g ∈ G, acting on the vector spaces X and
Y, respectively. In this work, we consider the SE(3) group, i.e., the group of rotation, translation
in 3D space. This requires the estimated likelihood unaffected with translational and rotational
transformations, and we will elaborate on how our method satisfy this property in Sec. 4.
4	GeoDiff Method
In this section, we elaborate on the proposed equivariant diffusion framework. We first present
a high level description of our 3D diffusion formulation in Sec. 4.1, based on recent progress of
denoising diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020). Then we emphasize several
3
Published as a conference paper at ICLR 2022
Figure 1: Illustration of the diffusion and reverse process of GEODIFF. For diffusion process,
noise from fixed posterior distributions q(Ct |Ct-1) is gradually added until the conformation is de-
stroyed. Symmetrically, for generative process, an initial state CT is sampled from standard Gaussian
distribution, and the conformation is progressively refined via the Markov kernels pθ (C t-1 |G, Ct).
non-trivial challenges of building diffusion models for geometry generation scenario, and show how
we technically tackle these issues. Specifically, in Sec. 4.2, We present how We parameterize pθ (C|G)
so that the conditional likelihood is roto-translational invariant, and in Sec. 4.3, we introduce our
surgery of the training objective to make the optimization also invariant of translation and rotation.
Finally, we briefly show how to draw samples from our model in Sec. 4.4.
4.1	Formulation
Let C0 denotes the ground truth conformations and let Ct for t = 1, ∙ ∙ ∙ , T be a sequence of latent
variables with the same dimension, where t is the index for diffusion steps. Then a diffusion
probabilistic model (Sohl-Dickstein et al., 2015) can be described as a latent variable model with two
processes: the forward diffusion process, and the reverse generative process. Intuitively, the diffusion
process progressively injects small noises to the data C0 , while the generative process learns to revert
the diffusion process by gradually eliminating the noise to recover the ground truth. We provide a
high-level schematic of the processes in Fig. 1.
Diffusion process. Following the physical insight, we model the particles C as an evolving thermo-
dynamic system. With time going by, the equilibrium conformation C0 will gradually diffuse to the
next chaotic states Ct , and finally converge into a white noise distribution after T iterations. Different
from typical latent variable models, in diffusion model this forward process is defined as a fixed
(rather than trainable) posterior distribution q(C1:T |C0). Specifically, we define it as a Markov chain
according to a fixed variance schedule β1, . . . , βT :
T
q(CLT|C0) = Y q(Ct|Ct-1),	q(Ct|Ct-1) = N(Ct； P1-βCt-1 ,βtI).	⑵
t=1
Note that, in this work we do not impose specific (invariance) requirement upon the diffusion process,
as long as it can efficiently draw noisy samples for training the generative process pθ (C0).
Let αt = 1 — βt and at = Qts=1 αs, a special property of the forward process is that q(Ct|C0) of
arbitrary timestep t can be calculated in closed form q(Ct |C0) = N(Ct; √OtC0, (1 — αt)I)2. This
indicates with sufficiently large T, the whole forward process will convert C0 to whitened isotropic
Gaussian, and thus it is natural to set p(CT) as a standard Gaussian distribution.
Reverse Process. Our goal is learning to recover conformations C0 from the white noise CT , given
specified molecular graphs G . We consider this generative procedure as a reverse dynamics of the
above diffusion process, starting from the noisy particles CT 〜p(CT). We formulate this reverse
dynamics as a conditional Markov chain with learnable transitions:
T
Pθ (C 0:T T |G, C T) = Y pθ (Ct-1 |G, Ct),	pθ (Ct-1 |G, Ct )= N (Ct-1 ； μ° (G, C t,t),σ2l).⑶
t=1
Herein μθ are parameterized neural networks to estimate the means, and σt can be any user-defined
variance. The initial distribution p(CT) is set as a standard Gaussian. Given a graph G, its 3D structure
is generated by first drawing chaotic particles CT fromp(CT), and then iteratively refined through
the reverse Markov kernels pθ (C t-1 |G, Ct).
2Detailed derivations are provided in the Appendix A.
4
Published as a conference paper at ICLR 2022
Having formulated the reverse dynamics, the marginal likelihood can be calculated by pθ(C0|G)=
R p(CT)pθ (C0:T-1 |G, CT)dC1:T. Herein a non-trivial problem is that the likelihood should be
invariant w.r.t translation and rotation, which has proved to be a critical inductive bias for 3D object
generation (Kohler et al., 2020; Satorras et al., 2021a). In the following subsections, We will elaborate
on how we parameterize the Markov kernels pθ (Ct-1 |G, Ct) to achieve this desired property, and also
how to maximize this likelihood by taking the invariance into account.
4.2	Equivariant Reverse Generative Process
Instead of directly leveraging existing methods, we consider building the density pθ (C0) that is
invariant to rotation and translation transformations. Intuitively, this requires the likelihood to be
unaffected by translations and rotations. Formally, let Tg be some roto-translational transformations
of a group element g ∈ SE(3), then we have the following statement:
Proposition 1. Let p(xT ) be an SE(3)-invariant density function, i.e., p(xT ) = p(Tg (xT)). If
Markov transitions p(xt-1|xt) are SE(3)-equivariant, i.e., p(xt-1 |xt) = p(Tg (xt-1)|Tg (xt)), then
we have that the density pθ (x0) = p(xT)pθ (x0:T-1 |xT)dx1:T is also SE(3)-invariant.
This proposition indicates that the dynamics starting from an invariant standard density along an
equivariant Gaussian Markov kernel can result in an invariant density. Now we provide a practical
implementation of GEODIFF based on the recent denoising diffusion framework (Ho et al., 2020).
Invariant Initial Density p(CT ). We first introduce the invariant distribution p(CT ), which will
also be employed in the equivariant Markov chain. We borrow the idea from Kohler et al. (2020)
to consider systems with zero center of mass (CoM), termed CoM-free systems. We define p(CT )
as a “CoM-free standard density” P(C), built upon an isotropic normal density ρ(C): for evaluating
the likelihood P(C) we can firstly translate C to zero CoM and then calculate ρ(C), and for sampling
from P(C) we can first sample from P(C) and then move the CoM to zero.
We provide a formal theoretical analysis of P(C) in Appendix A. Intuitively, the isotropic Gaussian
is manifestly invariant to rotations around the zero CoM. And by considering CoM-free system,
moving the particles to zero CoM can always ensure the translational invariance. Consequently, P(C)
is constructed as a roto-transitional invariant density.
Equivariant Markov Kernels p(Ct-1 |G, Ct). Similar to the prior density, we also consider equip-
ping all intermediate structures Ct as CoM-free systems. Specifically, given mean μθ(G, Ct, t) and
variance σt, the likelihood of CtT will be calculated by P(C——-2]G,C 储).The CoM-free Gaussian
ensures the translation invariance in the Markov kernels. Consequently, to achieve the equivariant
property defined in Proposition 1, we focus on the rotation equivariance.
Then in general, the key requirement is to ensure the means μθ (G, Ct,t) to be roto-translation
equivariant w.r.t Ct. Following Ho et al. (2020), we consider the following parameterization of μθ:
“θ(CYt = √α (C'-√β⅛eθ(G, Ct,t)
(4)
where θ are neural networks with trainable parameters θ. Intuitively, the model θ learns to predict the
noise necessary to decorrupt the conformations. This is analogous to the physical force fields (Schutt
et al., 2017; Zhang et al., 2018; Hu et al., 2021; Shuaibi et al., 2021), which also gradually push
particles towards convergence around the equilibrium states.
Now the problem is transformed to constructing θ to be roto-translational equivariant. We draw
inspirations from recent equivariant networks (Thomas et al., 2018; Satorras et al., 2021b) to design an
equivariant convolutional layer, named graph field network (GFN). In the l-th layer, GFN takes node
embeddings hl ∈ Rn×b (b denotes the feature dimension) and corresponding coordinate embeddings
xl ∈ Rn×3 as inputs, and outputs hl+1 and xl+1 as follows:
mij = φm (hi, hj, kxi - Xjk 2, eij; θm)
hi+1 = φh(hi, X mij； θh)
(5)
(6)
j∈N(i)
x；+l = X ~Γ~ (Ci- Cj) φx (mij； θx)
j∈N (i) dij
(7)
5
Published as a conference paper at ICLR 2022
where Φ are feed-forward networks and dij denotes interatomic distances. N (i) denotes the neighbor-
hood of ith node, including both connected atoms and other ones within a radius threshold τ , which
enables the model to explicitly capture long-range interactions and support molecular graphs with
disconnected components. Initial embeddings h0 are combinations of atom and timestep embeddings,
and x0 are atomic coordinates. The main difference between proposed GFN and other GNNs lies
in equation 7, where x is updated as a combination of radial directions weighted by Φx : Rb → R.
Such vector field xL enjoys the roto-translation equivariance property. Formally, we have:
Proposition 2. Parameterizing θ(G, C, t) as a composition of L GFN layers, and take the xL after
L updates as the output. Then the noise vector field θ is SE(3) equivariant w.r.t the 3D system C.
Intuitively, given hl already invariant and xl equivariant, the message embedding m will also be
invariant since it only depends on invariant features. Since x is updated with the relative differences
ci - cj weighted by invariant features, it will be translation-invariant and rotation-equivariant. Then
inductively, composing θ with L GFN layers enables equivariance with Ct . We provide the formal
proof of equivariance properties in Appendix A.
4.3	Improved Training Objective
Having formulated the generative process and the model parameterization, now we consider the
practical training objective for the reverse dynamics. Since directly optimizing the exact log-likelihood
is intractable, we instead maximize the usual variational lower bound (ELBO)3 :
E [logPθ(C0IG)] = Eh log Eq(ci：T|C0)pθCCTCG) i
T
≥-Eq [XDκL(q(Ct-1∣ct,C0)kPθ(Ct-1∣ct, G))]
t=1
-LELBO
(8)
where q(Ct-1∣Ct, C0) is analytically tractable as N(√≤-⅛βtC0 + *1-^-1)Ct, ⅛-iβt)3. Most
recently, Ho et al. (2020) showed that under the parameterization in equation 4, the ELBO of the
diffusion model can be further simplified by calculating the KL divergences between Gaussians as
weighted L2 distances between the means θ and 3. Formally, we have:
Proposition 3. (Ho et al., 2020) Under the parameterization in equation 4, we have:
T
LELBO = EYtE{C0,G}〜q(C0,G),e 〜N (0,I)
t=1
k-θ(G,Ct,t)k22
(9)
where Ct = √OC0 + √1 — αte. The weights Yt = 2二"1&M ])for t > 1, and Yi = 217
The intuition of this objective is to independently sample chaotic conformations of different timesteps
from q(Ct-i |Ct, C0), and use eθ to model the noise vector e. To yield a better empirical performance,
Ho et al. (2020) suggests to set all weights Yt as 1, which is in line with the the objectives of recent
noise conditional score networks (Song & Ermon, 2019; 2020).
As eθ is designed to be equivariant, it is natural to require its supervision signal e to be equivariant
with Ct . Note that once this is achieved, the ELBO will also become invariant. However, the e in
the forward diffusion process is not imposed with such equivariance, violating the above properties.
Here We propose two approaches to obtain the modified noise vector ^, which, after replacing e in the
L2 distance calculation in equation 9, achieves the desired equivariance:
Alignment approach. Considering the fact that e can be calculated by ° √√-1tC , we can first rotate
and translate C0 to C0 by aligning w.r.t Ct, and then compute e as ct√√1C0. Since the aligned
conformation (C0 is equivariant with Ct, the processed ^ will also enjoy the equivariance. Specifically,
the alignment is implemented by first translating C0 to the same CoM of Ct and then solve the optimal
rotation matrix by Kabsch alignment algorithm (Kabsch, 1976).
3The detailed derivations and full proofs are provided in Appendix A.
6
Published as a conference paper at ICLR 2022
Chain-rule approach. Another meaningful observation is that by reparameterizing the Gaussian
distribution q(Ct∣C0) as Ct = √αC0 + √1 - ae, e can be viewed as a weighted score function
√1 - aNCt q(Ct∣C0). Shi et al. (2021) recently shows that generally this score function NCt q(Ct∣∙)
can be designed to be equivariant by decomposing it into ∂Cdt Ndtq(Ct∣∙) with the chain rule, where
dt can be any invariant features of the structures Ct such as the inter-atomic distances. We refer
readers to Shi et al. (2021) for more details. The insight is that as gradient of invariant variables w.r.t
equivariant variables, the partial derivative ∂Ct dt will always be equivalent with Ct. In this work,
under the common assumption that d also follows a Gaussian distribution (Kingma & Welling, 2013),
our practical implementation is to first approximately calculate Ndt q(Ct∣C0) as d -√αd , and then
compute the modified noise vector e as √1 - α ∂C dt (d--VadO) = dctd √d-√αtd ).
-αt	1-αt
4.4 Sampling
With a learned reverse dynamics
eθ(G, Ct, t), the transition means
μθ (G, Ct, t) can be calculated by
equation 4. Thus, given a graph
G, its geometry C 0 is generated
by first sampling chaotic parti-
cles CT 〜 P(CT), and then
progressively sample CtT 〜
Pθ(CtT|G,Ct) for t = T,T -
1,…，1. This process is Marko-
vian, which gradually shifts the
previous noisy positions towards
Algorithm 1 Sampling Algorithm of GEODIFF.
Input: the molecular graph G, the learned reverse model e6.
Output: the molecular conformation C .
1:	Sample CT 〜P(CT) = N(0,I)
2:	for S = Τ,Τ 一 1,…，1 do
3:	Shift Cs to zero CoM
4:	Compute μθ(Cs, G, S) from e&(Cs, G, S) using equation 4
5:	Sample CST 〜N(Cs-1; μθ(Cs, G, s), σtI)
6:	end for
7:	return C 0 as C
equilibrium states. We provide the pseudo code of the whole sampling process in Algorithm 1.
5 Experiment
In this section, we empirically evaluate GeoDiff on the task of equilibrium conformation generation
for both small and drug-like molecules. Following existing work (Shi et al., 2021; Ganea et al.,
2021), we test the proposed method as well as the competitive baselines on two standard benchmarks:
Conformation Generation (Sec. 5.2) and Property Prediction (Sec. 5.3). We first present the
general experiment setups, and then describe task-specific evaluation protocols and discuss the results
in each section. The implementation details are provided in Appendix C.
5.1	Experiment Setup
Datasets. Following prior works (Xu et al., 2021a;b), we also use the recent GEOM-QM9 (Ramakr-
ishnan et al., 2014) and GEOM-Drugs (Axelrod & Gomez-Bombarelli, 2020) datasets. The former
one contains small molecules while the latter one are medium-sized organic compounds. We borrow
the data split produced by Shi et al. (2021). For both datasets, the training split consists of 40, 000
molecules with 5 conformations for each, resulting in 200, 000 conformations in total. The valid
split share the same size as training split. The test split contains 200 distinct molecules, with 22, 408
conformations for QM9 and 14, 324 ones for Drugs.
Baselines. We compare GEODIFF with 6 recent or established state-of-the-art baselines. For the ML
approaches, we test the following models with highest reported performance: CVGAE (Mansimov
et al., 2019), GRAPHDG (Simm & Hernandez-Lobato, 2020), CGCF (Xu et al., 2021a), CONF-
VAE (Xu et al., 2021b) and CONFGF (Shi et al., 2021). We also test the classic RDKIT (Riniker &
Landrum, 2015) method, which is arguably the most popular open-source software for conformation
generation. We refer readers to Sec. 2 for a detailed discussion of these models.
5.2	Conformation Generation
Evaluation metrics. The task aims to measure both quality and diversity of generated conformations
by different models. We follow Ganea et al. (2021) to evaluate 4 metrics built upon root-mean-square
7
Published as a conference paper at ICLR 2022
Table 1: Results on the GEOM-Drugs dataset, without FF optimization.
Models	COV-R (%) ↑		MAT-R (A)；		COV-P (%) ↑		MAT-P (A)J	
	Mean	Median	Mean	Median	Mean	Median	Mean	Median
CVGAE	0.00	0.00	3.0702	2.9937	-	-	-	-
GraphDG	8.27	0.00	1.9722	1.9845	2.08	0.00	2.4340	2.4100
CGCF	53.96	57.06	1.2487	1.2247	21.68	13.72	1.8571	1.8066
ConfVAE	55.20	59.43	1.2380	1.1417	22.96	14.05	1.8287	1.8159
GeoMol	67.16	71.71	1.0875	1.0586	-	-	-	-
CONFGF	62.15	70.93	1.1629	1.1596	23.42	15.52	1.7219	1.6863
GeoDiff-A	88.36	96.09	0.8704	0.8628	60.14	61.25	1.1864	1.1391
GeoDiff-C	89.13	97.88	0.8629	0.8529	61.47	64.55	1.1712	1.1232
* The COV-R and MAT-R results of CVGAE, GRAPHDG, CGCF, and CONFGF are borrowed from Shi
et al. (2021). The results of GeoMol are borrowed from a most recent study Zhu et al. (2022). Other
results are obtained by our own experiments. The results of all models for the GEOM-QM9 dataset
(summarized in Tab. 5) are collected in the same way.
deviation (RMSD), which is defined as the normalized Frobenius norm of two atomic coordinates
matrices, after alignment by Kabsch algorithm (Kabsch, 1976). Formally, let Sg and Sr denote the
sets of generated and reference conformers respectively, then the Coverage and Matching metrics (Xu
et al., 2021a) following the conventional Recall measurement can be defined as:
COV-R(Sg, Sr)
C ∈ Sr∣RMSD(C,C) ≤
1
MAT-R(Sg,Sr)=百 Emin RMSD(C,C),
δ,C ∈ Sg0∣,
(10)
(11)
where δ is a pre-defined threshold. The other two metrics COV-P and MAT-P inspired by Precision
can be defined similarly but with the generated and reference sets exchanged. In practice, Sg is set
as twice of the size of Sr for each molecule. Intuitively, the COV scores measure the percentage
of structures in one set covered by another set, where covering means the RMSD between two
conformations is within a certain threshold δ. By contrast, the MAT scores measure the average
RMSD of conformers in one set with its closest neighbor in another set. In general, higher COV rates
or lower MAT score suggest that more realistic conformations are generated. Besides, the Precision
metrics depend more on the quality, while the Recall metrics concentrate more on the diversity. Either
metrics can be more appealing considering the specific scenario. Following previous works (Xu et al.,
2021a; Ganea et al., 2021), δ is set as 0.5A and 1.25A for QM9 and Drugs datasets respectively.
Results & discussion. The results are summarized in Tab. 1 and Tab. 5 (left in Appendix. D). As
noted in Sec. 4.3, GEODIFF can be trained with two types of modified ELBO, named alignment
and chain-rule approaches. We denote models learned by these two objectives as GEODIFF-A and
GeoDiff-C respectively. As shown in the tables, GeoDiff consistently outperform the state-of-the-
art ML models on all datasets and metrics, especially by a significant margin for more challenging
large molecules (Drugs dataset). The results demonstrate the superior capacity of GeoDiff to model
the multi modal distribution, and generative both accurate and diverse conformations. We also notice
that in general GEODIFF-C performs slightly better than GEODIFF-A, which suggests that chain-rule
approach leads to a better optimization procedure. We thus take GEODIFF-C as the representative in
the following comparisons. We visualize samples generated by different models in Fig. 2 to provide a
qualitative comparison, where GeoDiff is shown to capture better both local and global structures.
On the more challenging Drugs dataset, we further test RDKit. As shown in Tab. 2, our observation
is in line with previous studies (Shi et al., 2021) that the state-of-the-art ML models (shown in Tab. 1)
perform better on COV-R and MAT-R. However, for the new Precision-based metrics we found that
ML models are still not comparable. This indicates that ML models tend to explore more possible
representatives while RDKit concentrates on a few most common ones, prioritizes quality over
diversity. Previous works (Mansimov et al., 2019; Xu et al., 2021b) suggest that this is because
RDKit involves an additional empirical force field (FF) (Halgren, 1996) to optimize the structure,
and we follow them to also combine GeoDiff with FF to yield a more fair comparison. Results in
8
Published as a conference paper at ICLR 2022
Graph				
Reference	¾)v⅛ ∙⅛¾ɑ			标 ⅛
GeODiff				
ConfGF	T 4 洛		‰A	∕⅞	W
GraPhDG		物 冲礴	* * 碟	
Figure 2: Examples of generated structures from Drugs dataset. For every model, We show the
conformation best-aligned with the ground truth. More examples are provided in Appendix E.
Table 2: Results on the GEOM-Drugs dataset, with FF optimization.
Models	COV-R (%) ↑		MAT-R (A) J		COV-P (%) ↑		MAT-P (A) J	
	Mean	Median	Mean	Median	Mean	Median	Mean	Median
RDKIT	60.91	65.70	1.2026	1.1252	72.22	88.72	1.0976	0.9539
GeoDiff + FF	92.27	100.00	0.7618	0.7340	84.51	95.86	0.9834	0.9221
Tab. 2 demonstrate that GEODIFF +FF can keep the superior diversity (Recall metrics) while also
enjoy significantly improved accuracy ((Precision metrics)).
5.3 Property Prediction
Evaluation metrics. This task estimates the molecular ensemble properties (Axel-	Table 3: MAE of predicted ensemble properties in eV.					
	Method	I E	E	AI	AI Imin	ʌ U	 Imax
rod & Gomez-Bombarelli, 2020) over a set			min			
of generated conformations. This can pro-	RDKIT GraphDG	0.9233 9.1027	0.6585 0.8882	0.3698 1.7973	0.8021 4.1743	0.2359 0.4776
vide an direct assessment on the quality	CGCF	28.9661	2.8410	2.8356	10.6361	0.5954
of generated samples. In specific, we fol-	CONFVAE	8.2080	0.6100	1.6080	3.9111	0.2429
low Shi et al. (2021) to extract a split from	CONFGF	2.7886	0.1765	0.4688	2.1843	0.1433
GEOM-QM9 covering 30 molecules, and	GeoDiff	I 0.25974	0.1551	0.3091	0.7033	0.1909
generate 50 samples for each. Then we use
the chemical toolkit Psi4 (Smith et al., 2020) tocalculate each conformer,s energy E and HOMO-
LUMO gap G and compare the average energy E, lowest energy Emin, average gap ∆6, minimum
gap ∆min, and maximum gap ∆max with the ground truth.
Results & discussions. The mean absolute errors (MAE) between calculated properties and the
ground truth are reported in Tab. 3. CVGAE is excluded due to the poor performance, which is also
reported in Simm & Hernandez-Lobato (2020); Shi et al. (2021). The properties are highly sensitive
to geometric structure, and thus the superior performance demonstrate that GeoDiff can consistently
predict more accurate conformations across different molecules.
6 Conclusion
We propose GeoDiff, a novel probabilistic model for generating molecular conformations. GeoDiff
marries denoising diffusion models with geometric representations, where we parameterize the reverse
generative dynamics as a Markov chain, and novelly impose roto-translational invariance into the
density with equivariant Markov kernels. We derive a tractable invariant objective from the variational
lower bound to optimize the likelihood. Comprehensive experiments over multiple tasks demonstrate
that GeoDiff is competitive with the existing state-of-the-art models. Future work includes further
improving or accelerating the model with other recent progress of diffusion models, and extending
our method to other challenging structures such as proteins.
9
Published as a conference paper at ICLR 2022
Acknowledgement
Minkai thanks Huiyu Cai, David Wipf, Zuobai Zhang, and Zhaocheng Zhu for their helpful discus-
sions and comments. This project is supported by the Natural Sciences and Engineering Research
Council (NSERC) Discovery Grant, the Canada CIFAR AI Chair Program, collaboration grants be-
tween Microsoft Research and Mila, Samsung Electronics Co., Ltd., Amazon Faculty Research Award,
Tencent AI Lab Rhino-Bird Gift Fund and a NRC Collaborative R&D Project (AI4D-CORE-06).
This project was also partially funded by IVADO Fundamental Research Project grant PRF-2019-
3583139727. The Stanford team is supported by NSF(#1651565, #1522054, #1733686), ONR
(N000141912145), AFOSR (FA95501910024), ARO (W911NF-21-1-0125) and Sloan Fellowship.
References
Mohammed AlQuraishi. End-to-end differentiable learning of protein structure. Cell systems, 8(4):
292-301, 2019.
Simon Axelrod and Rafael Gomez-Bombarelli. Geom: Energy-annotated molecular conformations
for property prediction and molecular generation. arXiv preprint arXiv:2006.05531, 2020.
Simon Batzner, Tess E Smidt, Lixin Sun, Jonathan P Mailoa, Mordechai Kornbluth, Nicola Molinari,
and Boris Kozinsky. Se (3)-equivariant graph neural networks for data-efficient and accurate
interatomic potentials. arXiv preprint arXiv:2101.03164, 2021.
Julian Chibane, Thiemo Alldieck, and Gerard Pons-Moll. Implicit functions in feature space for 3d
shape reconstruction and completion. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 6970-6981, 2020.
Sybren Ruurds De Groot and Peter Mazur. Non-equilibrium thermodynamics. Courier Corporation,
2013.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. In
ICLR, 2017.
David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Aldn
Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular
fingerprints. In Advances in neural information processing systems, pp. 2224-2232, 2015.
Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. Se(3)-transformers: 3d roto-
translation equivariant attention networks. NeurIPS, 2020.
Octavian-Eugen Ganea, Lagnajit Pattanaik, Connor W Coley, Regina Barzilay, Klavs F Jensen,
William H Green, and Tommi S Jaakkola. Geomol: Torsional geometric generation of molecular
3d conformer ensembles. arXiv preprint arXiv:2106.07802, 2021.
Niklas WA Gebauer, Michael Gastegger, Stefaan SP Hessmann, Klaus-Robert Muller, and Kristof T
Schutt. Inverse design of 3d molecular structures with conditional generative neural networks.
arXiv preprint arXiv:2109.04824, 2021.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pp. 1263-1272. JMLR. org, 2017.
T. Gogineni, Ziping Xu, Exequiel Punzalan, Runxuan Jiang, Joshua A Kammeraad, Ambuj Tewari,
and P. Zimmerman. Torsionnet: A reinforcement learning approach to sequential conformer search.
ArXiv, abs/2006.07078, 2020.
Thomas A Halgren. Merck molecular force field. v. extension of mmff94 using experimental data,
additional computational data, and empirical rules. Journal of Computational Chemistry, 17(5-6):
616-641, 1996.
Paul CD Hawkins. Conformation generation: the state of the art. Journal of Chemical Information
and Modeling, 57(8):1747-1756, 2017.
10
Published as a conference paper at ICLR 2022
Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution
examples in neural networks. arXiv preprint arXiv:1610.02136, 2016.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. arXiv preprint
arXiv:2006.11239, 2020.
Weihua Hu, Muhammed Shuaibi, Abhishek Das, Siddharth Goyal, Anuroop Sriram, Jure Leskovec,
Devi Parikh, and Larry Zitnick. Forcenet: A graph neural network for large-scale quantum
chemistry simulation. 2021.
John Ingraham, Adam J Riesselman, Chris Sander, and Debora S Marks. Learning protein structure
with a differentiable simulator. In International Conference on Learning Representations, 2019.
Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for
molecular graph generation. arXiv preprint arXiv:1802.04364, 2018.
Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael John Lamarre Townshend, and Ron Dror.
Learning from protein structure with geometric vector perceptrons. In International Conference on
Learning Representations, 2021.
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,
Kathryn TUnyasUvUnakooL RUss Bates, AUgUstin 右dek, Anna Potapenko, et al. Highly accurate
protein structure prediction with alphafold. Nature, 596(7873):583-589, 2021.
Wolfgang Kabsch. A solUtion for the best rotation to relate two sets of vectors. Acta Crystallographica
Section A: Crystal Physics, Diffraction, Theoretical and General Crystallography, 32(5):922-923,
1976.
Diederik P. Kingma and Max Welling. AUto-encoding variational bayes. In 2nd International
Conference on Learning Representations, 2013.
Jonas Kohler, Leon Klein, and FrankNoe. EqUivariant flows: Exact likelihood generative learning for
symmetric densities. In Proceedings of the 37th International Conference on Machine Learning,
2020.
Leo Liberti, Carlile Lavor, Nelson MacUlan, and Antonio MUcherino. EUclidean distance geometry
and applications. SIAM review, 56(1):3-69, 2014.
Shitong LUo and Wei HU. DiffUsion probabilistic models for 3d point cloUd generation. ArXiv,
abs/2103.01458, 2021.
Shitong LUo, Chence Shi, Minkai XU, and Jian Tang. Predicting molecUlar conformation via dynamic
graph score matching. Advances in Neural Information Processing Systems, 34, 2021.
Elman Mansimov, Omar Mahmood, Seokho Kang, and KyUnghyUn Cho. MolecUlar geometry
prediction Using a deep generative graph neUral network. arXiv preprint arXiv:1904.00314, 2019.
B. Miller, M. Geiger, T. Smidt, and F. No6. Relevance of rotationally eqUivariant ConvoIUtions for
predicting molecUlar properties. ArXiv, abs/2008.08461, 2020.
Frank No6, Simon Olsson, Jonas Kohler, and Hao Wu. Boltzmann generators: Sampling eqUilibriUm
states of many-body systems with deep learning. Science, 365(6457), 2019.
RaghUnathan Ramakrishnan, Pavlo O Dral, Matthias RUpp, and O Anatole Von Lilienfeld. QUantUm
chemistry strUctUres and properties of 134 kilo molecUles. Scientific data, 1(1):1-7, 2014.
Sereina Riniker and Gregory A. LandrUm. Better informed distance geometry: Using what we know
to improve conformation generation. Journal of Chemical Information and Modeling, 55(12):
2562-2574, 2015.
Victor Garcia Satorras, Emiel Hoogeboom, Fabian B FUchs, Ingmar Posner, and Max Welling. E (n)
eqUivariant normalizing flows for molecUle generation in 3d. arXiv preprint arXiv:2105.09016,
2021a.
11
Published as a conference paper at ICLR 2022
Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivariant graph neural networks,
2021b.
Kristof Schutt, Pieter-Jan Kindermans, HUziel Enoc SaUceda Felix, Stefan Chmiela, Alexandre
Tkatchenko, and Klaus-Robert Muller. Schnet: A continuous-filter convolutional neural network
for modeling qUantUm interactions. In Advances in Neural Information Processing Systems, pp.
991-100LCUrran Associates, Inc., 2017.
Andrew W Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green,
Chongli Qin, Augustin 之idek, Alexander WR Nelson, Alex Bridgland, et al. Improved protein
structure prediction using potentials from deep learning. Nature, 577(7792):706-710, 2020.
Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang. Graphaf: a
flow-based autoregressive model for molecular graph generation. arXiv preprint arXiv:2001.09382,
2020.
Chence Shi, Shitong Luo, Minkai Xu, and Jian Tang. Learning gradient fields for molecular
conformation generation. ArXiv, 2021.
Muhammed Shuaibi, Adeesh Kolluru, Abhishek Das, Aditya Grover, Anuroop Sriram, Zachary Ulissi,
and C Lawrence Zitnick. Rotation invariant graph neural networks using spin convolutions. arXiv
preprint arXiv:2106.09575, 2021.
Gregor Simm and Jose Miguel Hernandez-Lobato. A generative model for molecular distance
geometry. In Hal DaUme III and Aarti Singh (eds.), Proceedings of the 37th International
Conference on Machine Learning, volume 119, pp. 8949-8958. PMLR, 2020.
Gregor N. C. Simm, Robert Pinsler, Gdbor Csdnyi, and Jose Miguel Herndndez-Lobato. Symmetry-
aware actor-critic for 3d molecular design. In International Conference on Learning Representa-
tions, 2021.
Daniel G. A. Smith, L. Burns, A. Simmonett, R. Parrish, M. C. Schieber, Raimondas Galvelis,
P. Kraus, H. Kruse, Roberto Di Remigio, Asem Alenaizan, A. M. James, S. Lehtola, Jonathon P
Misiewicz, et al. Psi4 1.4: Open-source software for high-throughput quantum chemistry. The
Journal of chemical physics, 2020.
Jascha Sohl-Dickstein, Eric A Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. arXiv preprint arXiv:1503.03585, 2015.
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv
preprint arXiv:2010.02502, 2020.
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
In Advances in Neural Information Processing Systems, pp. 11918-11930, 2019.
Yang Song and Stefano Ermon. Improved techniques for training score-based generative models.
NeurIPS, 2020.
N. Thomas, T. Smidt, Steven M. Kearnes, Lusann Yang, L. Li, Kai Kohlhoff, and P. Riley. Tensor
field networks: Rotation- and translation-equivariant neural networks for 3d point clouds. ArXiv,
2018.
M. Weiler, M. Geiger, M. Welling, W. Boomsma, and T. Cohen. 3d steerable cnns: Learning
rotationally equivariant features in volumetric data. In NeurIPS, 2018.
Minkai Xu, Shitong Luo, Yoshua Bengio, Jian Peng, and Jian Tang. Learning neural generative
dynamics for molecular conformation generation. In International Conference on Learning
Representations, 2021a.
Minkai Xu, Wujie Wang, Shitong Luo, Chence Shi, Yoshua Bengio, Rafael Gomez-Bombarelli,
and Jian Tang. An end-to-end framework for molecular conformation generation via bilevel
programming. arXiv preprint arXiv:2105.07246, 2021b.
12
Published as a conference paper at ICLR 2022
Linfeng Zhang, Jiequn Han, Han Wang, Roberto Car, and Weinan E. Deep Potential Molecular
Dynamics: A Scalable Model with the Accuracy of Quantum Mechanics. Physical Review Letters,
120(14):143001, 2018.
Jinhua Zhu, Yingce Xia, Chang Liu, Lijun Wu, Shufang Xie, Tong Wang, Yusong Wang, Wengang
Zhou, Tao Qin, Houqiang Li, et al. Direct molecular conformation generation. arXiv preprint
arXiv:2202.01356, 2022.
13
Published as a conference paper at ICLR 2022
A Proofs
A. 1 Properties of the Diffusion Model
We include proofs for several key properties of the probabilistic diffusion model here to be self-
contained. For more detailed discussions, please refer to Ho et al. (2020). Let {β0, ..., βT } be a
sequence of variances, and at = 1 - βt and(y.t = Q：=i a§. The two following properties are crucial
for deriving the final tractable objective in equation 9.
Property 1. Tractable marginal of the forward process:
1:(t I)= N(ct； √0tc0, (1 — at)I).
q(Ct |C0) =	q(C1:t|C0)dC
Proof. Let i ’s be independent standard Gaussian random variables. Then, by definition of the
Markov kernels q(Ct|Ct-1) in equation 2, we have
ct = √atc t-1 + √βtet________
=√αtαt-iCt-2 + a atβt-i∈t-i + √βtet
=√atat-1at-1Ct-3 + P at at-1βt-2et-2 + P atβt-1et-1 + √βtet	(12)
=♦♦♦
= √atC0 + ʌ/a.at-i …a2β1 eι + … +、/atβt-et-i + √βtet
Therefore q(Ct∣C0) is still Gaussian, and the mean of Ct is √atC0, and the variance matrix is
(αtat-1 ∙∙∙ a2βι + •一+ atβt-ι + βt)I = (1 — aJI. Then we have:
q(Ct∣C0)= NC; √ac0,(1- at)I).
This property provides convenient closed-form evaluation of Ct knowing C0 :
Ct = √atc0 + √1 — a e,
where e 〜N(0,I).
Besides, it is worth noting that,
q(CT|C0)= N(CT； √aTC0, (1 - aτ)I),
where c^t = QT=I(I - βt) approaches zero with large T, which indicates the diffusion process can
finally converge into a whitened noisy distribution.	□
Property 2. Tractable posterior of the forward process:
q(Ct-1∣Ct,C0) = N(Ct-1; √α-1βtC0 + √at(1 - αt-1)Ct, (I-a"βtI).
1 — Ct	1 — Ct	1 — Ct
Proof. Let βt = 1--O-1 βt, then we can derive the posterior by Bayes rule:
q(Ct|Ct-1) q(Ct-1|C0)
q(Ct-1|Ct, C0)
q(Ct∣C 0)
N(Ct; √CCt-1,βtI) N(Ct-1; √⅞-C0, (1 - Ct-I)I)
N(Ct; √⅛C0, (1 - Ct)I)
(2∏βt)-d(2π(1 - Ct-i))-d(2π(1 - Ct))d X
exp -
kCt-√CCt-1k2
d
(2πβt)-2 exp
2βt
1
-2βt
Ct
-1
kCt-1-√0-C0k2 + kC t-√αC0k2
2(1 - Ct-i)	+	2(1 - Ct)
-√αt-ιβt c 0 - √at(I - Ct-I) c t 2∖
1 -(y-t	1 -(y-t
(13)
Then we have the posterior q(Ct-i |Ct, C0) as the given form.
□
14
Published as a conference paper at ICLR 2022
A.2 Proof of Proposition 1
Let Tg be some roto-translational transformations of a group element g ∈ SE(3), and let p(xT) be a
density which is SE(3)-invariant, i.e., p(xT) = p(Tg(xT)). If the Markov transitions p(xt-1 |xt) are
SE(3)-equivariant, i.e., p(xt-1|xt) = p(Tg(xt-1)|Tg(xt)), then we have that the density pθ(x0) =
p(xT)pθ (x0:T-1 |xT)dx1:T is also SE(3)-invariant.
Proof.
pθ(Tg(x0)) =
=Z
p(Tg (xT))pθ (Tg (x0:T -1)|Tg (xT))dx1:T
p(Tg (xT))ΠtT=1 pθ (Tg (xt-1)|Tg (xt))dx1:T
/ p(xτ )ΠT=ιPθ(Tg (xt-ι)∣Tg (xt))dx1:T (invariant prior p(xT))
J p(χτ )∏T=ιPθ(χt-ι |xt)dxi：T (equivariant kernels p(χt-ι |xt))
/ p(xτ )pθ(X0：T-ι∣xτ )dxi:T
pθ(x0)
(14)
□
A.3 Proof of Proposition 2
In this section we prove that the output x of GFN defined in equation 5, 6 and 7 is translationally
invariant and rotationally equivariant with the input C . Let g ∈ R3 denote any translation transforma-
tions and orthogonal matrices R ∈ R3×3 denote any rotation transformations. let Rx be shorthand
for (Rxι,…,RXN). Formally, We aim to prove that the model satisfies:
Rxl+1, hl+1 = GFN(Rxl, RC + g, hl).	(15)
This equation indicates that, given Xl already rotationally equivalent With C , and hl already invariant,
then such property can propagate through a single GFN layer to Xl+1 and hl+1.
Proof. Firstly, given that hl already invariant to SE(3) transformations, We have that the messages
mij calculated from equation 5 Will also be invariant. This is because it sorely relies on the
distance betWeen tWo atoms, Which are manifestly invariant to rotations kRXli - RXlj k2 = (Xli -
Xlj)>R>R(Xli - Xlj) = (Xli - Xlj)> I (Xli - Xlj) = kXli - Xlj k2. Formally, the invariance of messages
in equation 5 can be Written as:
mi,j = Φm hli,hlj, RXli - RXlj 2 , eij = Φm hli,hlj, Xli - Xlj2 ,eij .	(16)
And similarly, the ht+1 updated from equation 6 Will also be invariant.
Next, We prove that the vector X updated from equation 7 preserves rotational equivariance and
translational invariance. Given mij already invariant as proven above, We have that:
X	-R (RCi + g -	Rcj- g) φx	(mi,j) = R X	-c-	(Ci-	Cj)	φx (mi,j) = Rχi+1.	(17)
j∈N(i) dij	j∈N(i) dij
Therefore, We have that rotating and translating C results in the same rotation and no translation on
Xl+1 by updating through equation 7.
Thus we can conclude that the property defined in equation 15 is satisfied.	□
Having proved the equivariance property of a single GFN layer, then inductively, We can draW
conclusion that a composition of L GFN layers will also preserve the same equivariance.
15
Published as a conference paper at ICLR 2022
A.4 Proof of Proposition 3
We first derive the variational lower bound (ELBO) objective in equation 8. The ELBO can be
calculated as follows:
E log pθ (C0 |G) = E log Eq(C1:T |C0)
PΘ(C0:T-1|G, CT) X P(CT)
.	q(C1:T ∣C0)
≥ Eq log
Pθ (C 0:T-1|G, C T )x P(CT)
q(C 1:T |C 0)
Eqh log P(CT) - X log pθqθr i
Eqh logPCT ) - log pθ(e^:)
q(	| )
T
X log
t=2
Pθ (C t-1∣G, C) + lo。q(C t-1∣C0) M
q(Ct-1∣Ct, C0) + g q(Ct ∣C0) H
—
p(Ct)	0	1、 V	Pθ(Ct-1∣G, Ct)↑
EqllOg q(CT∣C0)	ogpθ(IG,	)	Σlog q(Ct-1∣Ct,C0)l
T
-Eq [kl (q(Ct∣C0)kp(CT)) + XKL (q(Ct-1∣Ct,C0)|脑(Ct-1∣G,Ct)) - logpθ(C0|G,C1)].
t=2
(18)
It can be noted that the first term KL q(CT |C0)kP(CT) is a constant, which can be omit-
ted in the objective. Furthermore, for brevity, we also merge the final term log Pθ (C0 |G, C1)
into the second term (sum over KL divergences), and finally derive that LELBO =
PT=IDKL(q(Ct-1∣Ct,C0)kpθ(Ct-1∣G,Ct)) as in equation 8.
Now we consider how to compute the KL divergences as the proposition 3. Since both q(Ct-1 |Ct, C0)
andpθ(Ct 1|G, Ct) are GaUssian share the same covariance matrix βtI, the KL divergence between
them can be calculated by the squared '2 distance between their means weighed by a certain weights
-L. By the expression of q(Ct∣C0), we have the reparameterization that Ct = √~BttC0 + √1 - αe.
2βt
Then we can derive:
Eq KL (q(C t-1∣Ct, C0)kpθ (G,Ct-1∣Ct))
=ɪEco Il √α-1βtC0 + √αt(1 - αt-1)Ct - ɪ (Ct-
2βt	Il 1 - αt	1 - αt	√αt I
√⅛ eθ(C t,G ,t)
——Ec0 °
2βt	C ,e
1
√at-ιβt Ct - √τ-αte ι √αt(1 - &-1)
----------------------1-------------
βt2
1 - at
Ct
βt
√1 — a
eθ(Ct,G,t)
•
2βt at(1 - αt)
βt2
EC0,e Il0 ∙ Ct + e - eθ(Ct, G, t) ∣l
2 1--α-1 Btat(I - αt)
γtEC0, IIe - eθ(Ct, t)II
EC0, IIe - eθ (Ct, G, t)II
2
(19)
1 - a
2
where Yt represent the wights 2α(1-t%-). And We finish the proof.
A.5 Analysis of the invariant density in Sec. 4.2
Given a geometric system X ∈ RN∙3, we obtain the CoM-free X by subtracting its CoM. This can be
considered as a linear transformation:
X = Qx, where Q = I3 乳(IN - N1N 1N)	(20)
where Ik denotes the k X k identity matrix and 1k denotes the k-dimensional vector filled with ones.
It can be noted that Q is a symmetric projection operator, i.e., Q2 = Q and QT = Q. And we also
16
Published as a conference paper at ICLR 2022
have that rank[Q] = (N - 1) ∙ 3. Furthermore, let U represent the space of CoM-free systems, We
can easily have that Qy = y for any y ∈ U since the CoM of y is already zero.
Formally, let n = N ∙ 3 and set Rn with an isotropic normal distribution P = N(0, In), then the
CoM-free density can be formally written as p = N(0, QInQT) = N(0, QQT). Thus, sampling
from p can be trivially achieved by sampling from P and then projecting with Q. And ρ(y) can be
calculated by p(y) since for any y ∈ U we have ∣∣yk2 = k Qy k 2 ,and thus p(y) = p(y).
And in this paper, with the SE(3)-equivariant Markov kernels of the reverse process, any CoM-free
system will transit to another CoM-free system. And thus we can induce a well-defined Markov chain
on the subspace spanned by Q.
B Other related work
Protein structure generation. There has also been many recent works working on protein structure
folding. For example, Boltzmann generators No6 et al. (2019) use flow-based models to generate
the structure of protein main chains. AlQuraishi (2019) uses recurrent networks to model the amino
acid sequences. Ingraham et al. (2019) proposed neural networks to learn an energy simulator to
infer the protein structures. Most recently, AlphaFold Senior et al. (2020); Jumper et al. (2021) has
significantly improved the performance of protein structure generation. Nevertheless, proteins are
mainly linear backbone structures while general molecules are highly branched with various rings,
making protein folding approaches unsuitable for our setting.
Point cloud generation. Recently, some other works (Luo & Hu, 2021; Chibane et al., 2020) has
also been proposed for 3D structure generation with diffusion-based models, but focus on the point
cloud problem. Unfortunately, in general, point clouds are not considered as graphs with various
atom and bond information, and equivariance is also not widely considered, making these methods
fundamentally different from our model.
C Experiment details
In this section, we introduce the details of our experiments. In practice, the means θ are parameterized
as compositions of both typical invariant MPNNS (Schutt et al., 2017) and the proposed equivariant
GFNs in Sec. 4.2. As a default setup, the MPNNs for parameterizing the means θ are all implemented
with 4 layers, and the hidden embedding dimension is set as 128. After the MPNNs, we can obtain
the informative invariant atom embeddings, which we denote as h0 . Then the embeddings h0 are
fed into equivariant layers and updated with equation 5, equation 6, and equation 7 to obtain the
equivariant output. For the training of GeoDiff, we train the model on a single Tesla V100 GPU
with a learning rate of 0.001 until convergence and Adam (Kingma & Welling, 2013) as the optimizer.
The practical training time is ~48 hours. The other hyper-parameters of GeoDiff are summarized in
Tab. 4, including highest variance level βT , lowest variance level βT , the variance schedule, number
of diffusion timesteps T, radius threshold for determining the neighbor of atoms τ, batch size, and
number of training iterations.
Table 4: Additional hyperparameters of our GeoDiff.
Task	β1	βT	β scheduler	T	τ	Batch Size	Train Iter.
QM9	1e-7	2e-3	sigmoid	5000	10A	64	1M
Drugs	1e-7	2e-3	sigmoid	5000	10A	32	1M
D Additional experiments
D.1 Results for GEOM-QM9
The results on the GEOM-QM9 dataset are reported in Tab. 5.
17
Published as a conference paper at ICLR 2022
Table 5: Results on the GEOM-QM9 dataset, without FF optimization.
Models	COV-R (%) ↑		MAT-R (A) 1		COV-P (%) ↑		MAT-P (A) 1	
	Mean	Median	Mean	Median	Mean	Median	Mean	Median
CVGAE	0.09	0.00	1.6713	1.6088	-	-	-	-
GraphDG	73.33	84.21	0.4245	0.3973	43.90	35.33	0.5809	0.5823
CGCF	78.05	82.48	0.4219	0.3900	36.49	33.57	0.6615	0.6427
ConfVAE	77.84	88.20	0.4154	0.3739	38.02	34.67	0.6215	0.6091
GeoMol	71.26	72.00	0.3731	0.3731	-	-	-	-
ConfGF	88.49	94.31	0.2673	0.2685	46.43	43.41	0.5224	0.5124
GeoDiff-A	90.54	94.61	0.2104	0.2021	52.35	50.10	0.4539	0.4399
GeoDiff-C	90.07	93.39	0.2090	0.1988	52.79	50.29	0.4448	0.4267
Table 6: Additional results on the GEOM-Drugs dataset, without FF optimization.
Models	COV-R (%) ↑		MAT-R (A)； Mean Median	COV-P (%) ↑		MAT-P (A)； Mean Median
	Mean	Median		Mean	Median	
GEODIFF (T=1000)	82.96	96.29	0.9525	0.9334	48.27	46.03	1.3205	1.2724
D.2 Ablation study with fewer diffusion steps
We also test our method with fewer diffusion steps. Specifically, we test the setting with T = 1000,
β1 =1e-7 and βT =9e-3. The results on the more challenging Drugs dataset are shown in Tab. 6.
Compared with the results in Tab. 1, we can observe that when setting the diffusion steps as 1000,
though slightly weaker than the performance with 5000 decoding steps, the model can already
outperforms all existing baselines. Note that, the most competitive baseline ConfGF (Shi et al., 2021)
also requires 5000 sampling steps, which indicates that our model can achieve better performance
with fewer computational costs compared with the state-of-the-art method.
E More Visualizations
We provide more visualization of generated structures in Fig. 3. The molecules are chosen from the
test split of GEOM-Drugs dataset.
18
Published as a conference paper at ICLR 2022
Graph	Conformations
<A°	
	
	碱曝斜竹黎 f
6a¼-	
	蟀欷衰笺烂寿谑御■博
	症存■当津 J位⅛λ片弥&
	1飞飞盛力瞪力舞袅警
	%二%tf4' ‰ ⅜ ⅝ ¾
-¼Cerŋ	飞震产邓# ^	⅜ # ⅛
	方滑曲赛激橙廿嗓穿辫
	凝¥ ¥感霜鬻春凝施闻
	^⅜‰-^⅛⅛r¾⅛‰,⅛
Figure 3: Visualization of drug-like conformations generated by GeoDiff.
19