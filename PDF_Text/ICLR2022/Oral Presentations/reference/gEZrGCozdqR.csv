title,year,conference
 Muppet: Massive multi-task representations with pre-finetuning,2021, arXiv preprintarXiv:2101
 Massively multilingual neural machinetranslation in the wild: Findings and challenges,2019, arXiv preprint arXiv:1907
 Program synthesis with largelanguage models,2021, arXiv preprint arXiv:2108
 Domain adaptation via pseudo in-domain dataselection,2011, In Proceedings of the 2011 Conference on Empirical Methods in Natural LanguageProcessing
 The Fifth PASCAL RecognizingTextUal Entailment Challenge,2009, In TAC
 PIQA: ReasoningaboUt physical commonsense in natUral langUage,2020, In Thirty-Fourth AAAI Conference on ArtificialIntelligence
 Onthe opportunities and risks of foundation models,2021, arXiv preprint arXiv:2108
 A largeannotated corpus for learning natural language inference,2015, In Proceedings of the 2015 Conferenceon Empirical Methods in Natural Language Processing
 Language models are few-shot learners,2020, In Advances in Neural Information Pro-cessing Systems
 Evaluating large language models trainedon code,2021, arXiv preprint arXiv:2107
 QuAC: Question answering in context,2018, In Proceedings of the 2018 Conferenceon Empirical Methods in Natural Language Processing
 Hybrid emoji-based masked language models for zero-shot abusive language detection,2020, In Findings of theAssociation for Computational Linguistics: EMNLP 2020
 The PASCAL Recognising Textual Entailmentchallenge,2005, In Proceedings of the First International Conference on Machine Learning Challenges:Evaluating Predictive Uncertainty Visual Object Classification
 Semi-supervised sequence learning,2015, In Proceedings of the Confer-ence on Neural Information Processing Systems
 BERT: Pre-training ofdeep bidirectional transformers for language understanding,2019, In Proceedings of the 2019 Con-ference of the North American Chapter of the Association for Computational Linguistics: Hu-man Language Technologies
 GLaM: Efficient scaling of languagemodels with mixture-of-experts,2021, arXiv preprint arXiv:2112
 Edinburgh¡¯s phrase-basedmachine translation systems for WMT-14,2014, In Proceedings of the Ninth Workshop on StatisticalMachine Translation
 Semantic noise matters for neural naturallanguage generation,2019, In Proceedings of the 12th International Conference on Natural LanguageGeneration
 Multi-neWs: A large-scalemulti-document summarization dataset and abstractive hierarchical model,2019, In Proceedings of the57th Annual Meeting of the Association for Computational Linguistics
 Switch transformers: Scaling to trillion parametermodels with simple and efficient sparsity,2021, arXiv preprint arXiv:2101
 Making pre-trained language models better few-shotlearners,2021, In Proceedings of the 59th Annual Meeting of the Association for Computational Linguis-tics and the 11th International Joint Conference on Natural Language Processing (Volume 1: LongPapers)
 The WebNLGchallenge: Generating text from RDF data,2017, In Proceedings of the 10th International Conferenceon Natural Language Generation
 The third PASCAL recognizingtextual entailment challenge,2007, In Proceedings of the ACL-PASCAL Workshop on Textual Entailmentand Paraphrasing
 SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization,2019, In Proceedings of the 2nd Workshopon New Frontiers in Summarization
 Learning from natural instructions,2014, Machine learn-ing
 Newsroom: A dataset of 1,2018,3 million summaries withdiverse extractive strategies
 The Second PASCAL Recognising Textual Entailment Challenge,2006, In Proceedingsof the Second PASCAL Challenges Workshop on Recognising Textual Entailment
 Question-answer driven semantic role labeling:Using natural language to annotate natural language,2015, In Proceedings of the 2015 Conferenceon Empirical Methods in Natural Language Processing
 Surface formcompetition: Why the highest probability answer isn¡¯t always right,2021, In Proceedings of the2021 Conference on Empirical Methods in Natural Language Processing
 To-ward semantics-based answer pinpointing,2001, In Proceedings of the First International Confer-ence on Human Language Technology Research
 Universal language model fine-tuning for text classification,2018, InProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume1: Long Papers)
 Cosmos QA: Machine readingcomprehension with contextual commonsense reasoning,2019, In Proceedings of the 2019 Conferenceon Empirical Methods in Natural Language Processing and the 9th International Joint Conferenceon Natural Language Processing (EMNLP-IJCNLP)
 Google¡¯s multilingual neural machine translation system: Enabling zero-shot transla-tion,2017, Transactions of the Association for Computational Linguistics
 TriviaQA: A large scale distantlysupervised challenge dataset for reading comprehension,2017, In Proceedings of the 55th AnnualMeeting of the Association for Computational Linguistics (Volume 1: Long Papers)
 Lookingbeyond the surface: A challenge set for reading comprehension over multiple sentences,2018, InProceedings of the 2018 Conference of the North American Chapter of the Association for Compu-tational Linguistics: Human Language Technologies
 UNIFIEDQA: Crossing format boundaries with a single QA system,2020, InFindings of the Association for Computational Linguistics: EMNLP 2020
 From group to individuallabels using deep features,2015, Proceedings of the 21th ACM SIGKDD International Conferenceon Knowledge Discovery and Data Mining
 Sentencepiece: A simple and language independent subwordtokenizer and detokenizer for neural text processing,2018, In Eduardo Blanco and Wei Lu (eds
 Ask me anything: Dynamic memory networks fornatural language processing,2016, In Proceedings of the International Conference on Machine Learning
 Natural Questions: A benchmark for question answering research,2019, Transactions of theAssociationfor Computational Linguistics
 WikiLingua: A newbenchmark dataset for cross-lingual abstractive summarization,2020, In Findings of the Associ-ation for Computational Linguistics: EMNLP 2020
 Learning to detect unseen objectclasses by between-class attribute transfer,2009, In 2009 IEEE Conference on Computer Vision andPattern Recognition
 From zero to hero: On thelimitations of zero-shot language transfer with multilingual Transformers,2020, In Proceedings of the2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)
 Latent retrieval for weakly supervisedopen domain question answering,2019, In Proceedings of the 57th Annual Meeting of the Associationfor Computational Linguistics
 Gshard: Scaling giant models with conditionalcomputation and automatic sharding,2020, In International Conference on Learning Representations
 The power of scale for parameter-efficient prompttuning,2021, In Proceedings of the Conference on Empirical Methods in Natural Language Processing
 The Winograd Schema Challenge,2012, InThirteenth International Conference on the Principles of Knowledge Representation and Reasoning
 Zero-shot relation extractionvia reading comprehension,2017, In Proceedings of the 21st Conference on Computational NaturalLanguage Learning (CoNLL 2017)
 Prefix-tuning: Optimizing continuous prompts for generation,2021, InProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)
 A unifiedMRC framework for named entity recognition,2020, In Proceedings of the 58th Annual Meet-ing of the Association for Computational Linguistics
 Learning question classifiers,2002, In COLING 2002: The 19th International Confer-ence on Computational Linguistics
 CommonGen: A constrained text generation challenge for generative commonsensereasoning,2020, In Findings of the Association for Computational Linguistics: EMNLP 2020
 Multi-task deep neural networks fornatural language understanding,2019, In Proceedings of the 57th Annual Meeting of the Associationfor Computational Linguistics
 Multi-tasksequence to sequence learning,2016, Proceedings of ICLR
 Learning word vectors for sentiment analysis,2011, In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Human Language Technologies
 The natural languagedecathlon: Multitask learning as question answering,2018, arXiv preprint arXiv:1806
 Programs with common sense,1960, RLE and MIT computation center
 Can a suit of armor conductelectricity? A new dataset for open book question answering,2018, In Proceedings of the 2018Conference on Empirical Methods in Natural Language Processing
 Metaicl: Learning to learnin context,2021, arXiv preprint arXiv:2110
 Natural Instructions:Benchmarking generalization to new tasks from natural language instructions,2021, arXiv preprintarXiv:2104
 A corpus and cloze evaluation for deeper understandingof commonsense stories,2016, In Proceedings of the 2016 Conference of the North American Chapterof the Association for Computational Linguistics: Human Language Technologies
 Annotated Gigaword,2012, In Pro-ceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scaleKnowledge Extraction (AKBC-WEKEX)
 AdversarialNLI: A new benchmark for natural language understanding,2020, In Proceedings of the 58th AnnualMeeting of the Association for Computational Linguistics
 Training language models to follow instructions with human feedback,2022, Preprint
 Deep contextualized word representations,2018, In Proceedings of the 2018Conference of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies
 Improving zero-shot translationwith language-independent constraints,2019, In Proceedings of the Fourth Conference on MachineTranslation (Volume 1: Research Papers)
 WiC: the word-in-context dataset forevaluating context-sensitive meaning representations,2019, In Proceedings of the 2019 Confer-ence of the North American Chapter of the Association for Computational Linguistics: Hu-man Language Technologies
 Improvinglanguage understanding by generative pre-training,2018, https://blog
 Exploring the limits of transfer learning with a unifiedtext-to-text transformer,2020, Journal of Machine Learning Research
 Resolving complex cases of definite pronouns: The Winogradschema challenge,2012, In Proceedings of the 2012 Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational Natural Language Learning
 Know what you don¡¯t know: Unanswerable questionsfor SQuAD,2018, In Proceedings of the 56th Annual Meeting of the Association for ComputationalLinguistics (Volume 2: Short Papers)
 CoQA: A conversational question answeringchallenge,2019, Transactions of the Association for Computational Linguistics
 Arecipe for arbitrary text style transfer with large language models,2021, arXiv preprint arXiv:2109
 Choice of plausible alternatives: Anevaluation of commonsense causal reasoning,2011, In AAAI Spring Symposium Series
 An embarrassingly simple approach to zero-shotlearning,2015, In Proceedings of the International Conference on Machine Learning
 An overview of multi-task learning in deep neural networks,2017, arXiv preprintarXiv:1706
 WinoGrande: An adver-sarial winograd schema challenge at scale,2020, In Proceedings of the AAAI Conference on ArtificialIntelligence
 Multitask prompted trainingenables zero-shot task generalization,2021, Proceedings of the International Conference on LearningRepresentations
 Analysing mathematicalreasoning abilities of neural models,2019, Proceedings of the International Conference on LearningRepresentations
 Exploiting cloze-questions for few-shot text classification andnatural language inference,2021, In Proceedings of the 16th Conference of the European Chapterof the Association for Computational Linguistics: Main Volume
 Get to the point: Summarization withpointer-generator networks,2017, In Proceedings of the 55th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers)
 Edinburgh neural machine translation systemsfor WMT 16,2016, In Proceedings of the First Conference on Machine Translation: Volume 2
 Adafactor: Adaptive learning rates with sublinear memorycost,2018, In International Conference on Machine Learning
 Zero-shot learning of classifiers from naturallanguage quantification,2018, In Proceedings of the 56th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers)
 Improvingand simplifying pattern exploiting training,2021, In Proceedings of the 2021 Conference on EmpiricalMethods in Natural Language Processing (EMNLP)
 Lamda: Language models for dialogapplications,2022, arXiv preprint arXiv:2201
 Meta-learning: A survey,2018, arXiv preprint arXiv:1810
 Seq2seq and multi-task learning for joint intent and contentextraction for domain specific interpreters,2018, arXiv preprint arXiv:1808
 GLUE:A multi-task benchmark and analysis platform for natural language understanding,2018, In Proceedingsof the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks forNLP
 Superglue: A stickier benchmark for general-purpose languageunderstanding systems,2019, Conference on Neural Information Processing Systems (NeurIPS)
 Multi-agent dual learning,2019, In Proceedings of the International Conference on Learning Representations(ICLR) 2019
 Towards zero-label language learning,2021, arXivpreprint arXiv:2109
 Chain of thought prompting elicits reasoning in large language models,2022, arXiv preprintarXiv:2201
 A broad-coverage challenge corpus for sen-tence understanding through inference,2018, In Proceedings of the 2018 Conference of the North Ameri-can Chapter of the Association for Computational Linguistics: Human Language Technologies
 Recursively summarizing books with human feedback,2021, arXiv preprint arXiv:2109
 CorefQA: Coreference resolution asquery-based span prediction,2020, In Proceedings of the 58th Annual Meeting of the Associationfor Computational Linguistics
 Crossfit: A few-shot learning challenge for cross-taskgeneralization in NLP,2021, In Proceedings of the 2021 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP)
 This email could save your life: Introducing the task of email subjectline generation,2019, In Proceedings of the 57th Annual Meeting of the Association for ComputationalLinguistics
 Character-level convolutional networks for text clas-sification,2015, In NIPS
 Meta-tuning language models to answerprompts better,2021, arXiv preprint arXiv:2104
