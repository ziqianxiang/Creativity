title,year,conference
 A latent variable modelaPProach to Pmi-based word embeddings,2016, Transactions of the Association for ComputationalLinguistics
 ReciPeNLG: A cooking reciPes dataset for semi-structured text gener-ation,2020, In Proceedings of the 13th International Conference on Natural Language Generation
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Taskmaster-1: Toward a realistic and diverse dialog dataset,2019, In Proceedings of the 2019 Conference on Em-pirical Methods in Natural Language Processing and the 9th International Joint Conference onNatural Language Processing (EMNLP-IJCNLP)
 Evaluation Benchmarks and Learning Criteria forDiscourse-Aware Sentence RePresentations,2019, In Proceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and the 9th International Joint Conference on NaturalLanguage Processing (EMNLP-IJCNLP)
 Evaluation benchmarks and learning criteria fordiscourse-aware sentence rePresentations,2019, In Proc
 A simple framework forcontrastive learning of visual representations,2020, In International conference on machine learning
 BERT: Pre-training of DeepBidirectional Transformers for Language Understanding,2019, arXiv:1810
 Enabling language models to fill in the blanks,2020, InProceedings of the 58th Annual Meeting of the Association for Computational Linguistics
 Empirically estimating order constraints for contentplanning in generation,2001, In Proceedings of the 39th Annual Meeting on Association for Com-putational Linguistics
 Simcse: Simple contrastive learning of sentenceembeddings,2021, arXiv preprint arXiv:2104
 Lagging inference net-works and posterior collapse in variational autoencoders,2018, In International Conference on LearningRepresentations
 Pair: Planning and iterative refinement in pre-trained transformers forlong text generation,2020, In Proceedings of the 2020 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP)
 Unsupervised Feature Extraction by Time-ContrastiveLearning and Nonlinear ICA,2016, arXiv:1605
 Nonlinear ica using auxiliary variables andgeneralized contrastive learning,2019, In The 22nd International Conference on Artificial Intelligenceand Statistics
 Pretraining with Contrastive SentenceObjectives Improves Discourse Performance of Language Models,2005, arXiv:2005
 Speech & language processing,2000, Pearson Education India
 Globally coherent text generation with neuralchecklist models,2016, In Proceedings of the 2016 conference on empirical methods in natural lan-guage processing
 Albert: A lite bert for self-supervised learning of language representations,2019, arXiv preprintarXiv:1909
 ROUGE: A package for automatic evaluation of summaries,2004, In Text SummarizationBranches Out
 Limitations of autore-gressive models and their alternatives,2021, In Proceedings of the 2021 Conference of the North Amer-ican Chapter of the Association for Computational Linguistics: Human Language Technologies
 Contrastive learning of strong-mixingcontinuous-time stochastic processes,2103, arXiv:2103
 SimCLS: A Simple Framework for Contrastive Learning of AbstractiveSummarization,2021, In Proceedings of the 59th Annual Meeting of the Association for ComputationalLinguistics and the 11th International Joint Conference on Natural Language Processing (Volume2: Short Papers)
 Step-by-step: Separating planning from real-ization in neural data-to-text generation,2267, In Proceedings of the 2019 Conference of the NorthAmerican Chapter of the Association for Computational Linguistics: Human Language Tech-nologies
 A corpus and cloze evaluation for deeper under-standing of commonsense stories,2016, In Proceedings of the 2016 Conference of the North AmericanChapter of the Association for Computational Linguistics: Human Language Technologies
 DisSent: Learning Sentence Representations from Ex-plicit Discourse Relations,2019, In Proceedings of the 57th Annual Meeting of the Association for Com-putational Linguistics
 Bleu: a method for automaticevaluation of machine translation,2002, In Proceedings of the 40th annual meeting of the Associationfor Computational Linguistics
 On variationalbounds of mutual information,2019, In International Conference on Machine Learning
 Languagemodels are unsupervised multitask learners,2019,2019
 Sentence-bert: Sen-tence embeddings using siamese bert-networks,2019, In Proceedings of the 2019 Conference on Empir-ical Methods in Natural Language Processing
 Deep unsupervisedlearning using nonequilibrium thermodynamics,2015, In International Conference on Machine Learn-ing
 Score-based generative modeling through stochastic differential equations,2020, In Interna-tional Conference on Learning Representations
 Trainable sentence planning for complex in-formation presentation in spoken dialog systems,2004, In Proceedings of the 42nd Annual Meetingon Association for Computational Linguistics
 Language through a prism: A spectral approachfor multiscale language representations,2020, Advances in Neural Information Processing Systems
 Generating narrative text in a switching dynamical system,2020, In Proceedings of the 24thConference on Computational Natural Language Learning
 Transformers: State-of-the-artnatural language processing,2020, In Proceedings of the 2020 Conference on Empirical Methods inNatural Language Processing: System Demonstrations
 Discourse-aware neural extractive textsummarization,2020, In Proceedings of the 58th Annual Meeting of the Association for Computa-tional Linguistics
 Bertscore: Evaluat-ing text generation with bert,2019, In International Conference on Learning Representations
 Learning discourse-level diversity for neuraldialog models using conditional variational autoencoders,2017, In Proceedings of the 55th AnnualMeeting of the Association for Computational Linguistics (Volume 1: Long Papers)
