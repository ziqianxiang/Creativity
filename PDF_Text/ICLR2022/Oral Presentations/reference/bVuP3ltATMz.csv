title,year,conference
 Deep learning with differential privacy,2016, In Proceedings of the 2016 ACM SIGSACconference on computer and communications security
 Large-scale differen-tially private bert,2021, arXiv preprint arXiv:2108
 Bitfit: Simple parameter-efficient fine-tuningfor transformer-based masked language-models,2021, arXiv e-prints
 Towards private synthetic text generation,2019, InNeurIPS 2019 Machine Learning with Guarantees Workshop
 On the opportuni-ties and risks of foundation models,2021, arXiv preprint arXiv:2108
 Fast and memory efficient differentially private-sgd via jl projections,2021, arXivpreprint arXiv:2102
 The secret sharer:Evaluating and testing unintended memorization in neural networks,2019, In 28th {USENIX} SecuritySymposium ({USENIX} Security 19)
 Extracting training datafrom large language models,2020, arXiv preprint arXiv:2012
 A stability-based validation procedure for differentiallyprivate machine learning,2013, Advances in Neural Information Processing Systems
 Gs-wgan: A gradient-sanitized approachfor learning differentially private generators,2020, arXiv preprint arXiv:2006
 Privacy-preserving neural representations oftext,2018, arXiv preprint arXiv:1808
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Gaussian differential privacy,2019, arXiv preprintarXiv:1905
 An efficient dp-sgd mecha-nism for large scale nlp models,2021, arXiv preprint arXiv:2107
 Calibrating noise to sensitivity inprivate data analysis,2006, In Theory of cryptography conference
 The algorithmic foundations of differential privacy,2014, Foundationsand Trends in Theoretical Computer Science
 Making pre-trained language models better few-shotlearners,2020, arXiv preprint arXiv:2012
 Efficient per-example gradient computations,2015, arXiv preprint arXiv:1510
 Logan: Membershipinference attacks against generative models,2019, In Proceedings on Privacy Enhancing Technologies(PoPETs)
 The curious case of neural textdegeneration,2019, arXiv preprint arXiv:1904
 Learning and evaluating a differentiallyprivate pre-trained language model,2021, In Proceedings of the Third Workshop on Privacy in NaturalLanguage Processing
 Parameter-efficient transfer learning fornlp,2019, In International Conference on Machine Learning
 Lora: Low-rank adaptation of large language models,2021, arXiv preprint arXiv:2106
 Texthide: Tackling dataprivacy in language understanding tasks,2020, arXiv preprint arXiv:2010
 Instahide: Instance-hiding schemes forprivate distributed learning,4507, In International Conference on Machine Learning
 Fast dimension independent private adagrad onpublicly estimated subspaces,2020, arXiv preprint arXiv:2008
 Differentially private language models benefit frompublic pre-training,2020, arXiv preprint arXiv:2009
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Computing tight differential privacy guaranteesusing fft,2560, In International Conference on Artificial Intelligence and Statistics
 Scaling up differentially private deep learning with fast per-examplegradient clipping,2020, arXiv preprint arXiv:2009
 The power of scale for parameter-efficient prompttuning,2021, arXiv preprint arXiv:2104
 A diversity-promotingobjective function for neural conversation models,2015, arXiv preprint arXiv:1510
 Prefix-tuning: Optimizing continuous prompts for generation,2021, arXivpreprint arXiv:2101
 Private selection from private candidates,2019, In Proceedings of the51st Annual ACM SIGACT Symposium on Theory of Computing
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Compacter: Efficient low-rankhypercomplex adapter layers,2021, arXiv preprint arXiv:2106
 Emergentlinguistic structure in artificial neural networks trained by self-supervision,2020, Proceedings of theNational Academy of Sciences
 Learning differentially privaterecurrent language models,2017, arXiv preprint arXiv:1710
 Towards automatic generation of shareable synthetic clinicalnotes using neural language models,2019, arXiv preprint arXiv:1905
 Mixed precisiontraining,2017, arXiv preprint arXiv:1710
 Privacy regularization: Joint privacy-utility optimization in languagemodels,2021, arXiv preprint arXiv:2103
 Renyi differential privacy,2017, In 2017 IEEE 30th Computer Security FoundationsSymposium (CSF)
 R\¡¯enyi differential privacy of the sampled gaussianmechanism,2019, arXiv preprint arXiv:1908
 Dart: Open-domain structured datarecord to text generation,2020, arXiv preprint arXiv:2007
 Private post-gan boosting,2020, arXivpreprint arXiv:2007
 The e2e dataset: New challenges forend-to-end generation,2017, arXiv preprint arXiv:1706
 Semi-supervised knowledge transfer for deep learning from private training data,2016, arXiv preprintarXiv:1610
 Scalable private learning with pate,2018, arXiv preprint arXiv:1802
 Automatic differentiation inpytorch,2017,2017
 Adapter-fusion: Non-destructive task composition for transfer learning,2020, arXiv preprint arXiv:2005
 Privacy-adaptive bert for natural language understanding,2021, arXiv preprint arXiv:2104
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 Training production language models without memorizing user data,2020, arXivpreprint arXiv:2009
 Adapterdrop: On the efficiency of adapters in transformers,2020, arXiv preprintarXiv:2010
 Membership inference attacksagainst machine learning models,2017, In 2017 IEEE Symposium on Security and Privacy (SP)
 Stochastic gradient descent with differen-tially private updates,2013, In 2013 IEEE Global Conference on Signal and Information Processing
 Enabling fast differentially private sgdvia just-in-time compilation and vectorization,2020, arXiv preprint arXiv:2010
 Differentially private synthetic medical datageneration using convolutional gans,2020, arXiv preprint arXiv:2012
 Dp-cgan: Differentially privatesynthetic data and label generation,2019, In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition Workshops
 Attention is all you need,2017, arXiv preprint arXiv:1706
 Finetuned language models are zero-shot learners,2021, arXiv preprintarXiv:2109
 The multi-genre nli corpus,2018,2018
 Learning neural templates for textgeneration,2018, arXiv preprint arXiv:1808
 On a utilitarian approachto privacy preserving text generation,2021, arXiv preprint arXiv:2104
 Differentially private fine-tuning oflanguage models,2021, arXiv preprint arXiv:2110
 Do not let privacy overbill utility: Gradientembedding perturbation for private learning,2021, arXiv preprint arXiv:2102
 Large scale private learning vialow-rank reparametrization,2021, arXiv preprint arXiv:2106
 Understandingdeep learning requires rethinking generalization,2016, CoRR
 Dialogpt: Large-scale generative pre-training for conversationalresponse generation,2019, arXiv preprint arXiv:1911
 Bypassing the ambient dimension: Privatesgd with gradient subspace identification,2020, arXiv preprint arXiv:2007
