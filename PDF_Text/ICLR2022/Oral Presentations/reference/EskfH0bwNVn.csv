title,year,conference
 Second-order stochastic optimization for machinelearning in linear time,2017, The Journal ofMachine Learning Research
 Machine learning for encrypted malware traffic classifica-tion: Accounting for noisy labels and non-stationarity,2017, In SIGKDD
 Unsupervisedlabel noise modeling and loss correction,2019, In ICML
 A closer look at memorization in deep networks,2017, In ICML
 Discriminative learning under covariateshift,2009, Journal of Machine Learning Research
 Smote: syntheticminority over-sampling technique,2002, Journal of artificial intelligence research
 Generalizeddataweighting via class-level gradient manipulation,2021, In A
 Characterizations of an empirical influence function fordetecting influential cases in regression,1980, Technometrics
 Cleaning crowdsourced labelsusing oracles for statistical classification,2018, Proc
 Rethinking importance weighting fordeep learning under distribution shift,2020, In Advances in Neural Information Processing Systems 33:Annual Conference on Neural Information Processing Systems 2020
 Training deep neural-networks using a noise adaptationlayer,2017, In 5th International Conference on Learning Representations
 RECORD: resource constrained semi-supervised learningunder distribution shift,2020, In SIGKDD
 Co-teaching: Robust training of deep neural networks with extremely noisy labels,2018, InNeurIPS
 Learning from imbalanced data,2009, IEEE Transactions on knowledgeand data engineering
 Understanding black-box predictions via influence functions,2017, InICML
 On the accuracy of influencefunctions for measuring group effects,2019, In NeurIPS
 Cleannet: Transfer learning forscalable image classifier training with label noise,2018, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Dividemix: Learning with noisy labels as semi-supervised learning,2020, In 8th International Conference on Learning Representations
 Scalable betweenness centralitymaximization via sampling,2016, In SIGKDD
 Decoupling ”when to update” from ”how to update”,2017, InNeurIPS
 Deep learning via hessian-free optimization,2010, In ICML
 Makingdeep neural networks robust to label noise: A loss correction approach,2017, In 2017 IEEE Conferenceon Computer Vision and Pattern Recognition
 REA: robust cross-lingual entity alignmentbetween knowledge graphs,2020, In SIGKDD
 Not all unlabeled data are equal:Learning to weight data in semi-supervised learning,2020, In Advances in Neural Information Process-ing Systems 33: Annual Conference on Neural Information Processing Systems 2020
 The central role of the propensity score in observationalstudies for causal effects,1983, Biometrika
 Data dropout: Optimizing training data for convolutionalneural networks,2018, In 2018 IEEE 30th International Conference on Tools with Artificial Intelligence(ICTAI)
 Denoising implicit feed-back for recommendation,2020, arXiv preprint arXiv:2006
 Symmetric cross en-tropy for robust learning with noisy labels,2019, In Proceedings of the IEEE International Conferenceon ComPuter Vision
 Less is better: Un-weighted data subsampling via influence function,2020, In AAAI
 Combating noisy labels by agreement: A jointtraining method with co-regularization,2020, In CVPR
 Influence function for unbiased recommendation,2020, In SIGIR
 Understandingdeep learning requires rethinking generalization,2017, In 5th International Conference on LearningRePresentations
 Training set debugging using trusted items,2018, InThirty-second AAAI conference on artificial intelligence
 Focused context balancing forrobust offline policy evaluation,2019, In SIGKDD
