title,year,conference
 Etc: Encoding long and structuredinputs in transformers,2020, In Proceedings of the 2020 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP)
 Some recent advances in forecasting and control,1968, Journalof the Royal Statistical Society
 Multiscalestochastic modeling for tractable inference and data assimilation,2008, Computer Methods in AppliedMechanics and Engineering
 Hierarchical multiscale recurrent neural net-works,2019, In 5th International Conference on Learning Representations
 Character-based neural machine translation,2016, In Pro-ceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2:Short Papers)
 Residual lstm: Design of a deep recurrentarchitecture for distant speech recognition,2017, arXiv preprint arXiv:1701
 Reformer: The efficient transformer,2019, InInternational Conference on Learning Representations
 Modeling long-and short-termtemporal patterns with deep neural networks,2018, In The 41st International ACM SIGIR Conferenceon Research & Development in Information Retrieval
 Enhancing the locality and breaking the memory bottleneck of transformer on time seriesforecasting,2019, Advances in Neural Information Processing Systems
 Document level neuralmachine translation with hierarchical attention networks,2018, In Proceedings of the Conference onEmpirical Methods in Natural Language Processing (EMNLP)
 Deepant: A deeplearning approach for unsupervised anomaly detection in time series,2018, Ieee Access
 Deepar: Probabilistic fore-casting with autoregressive recurrent networks,2020, International Journal of Forecasting
 Bi-directional recurrent neural networks for speech recognition,1996, In Proceeding of IEEECanadian Conference on Electrical and ComputerEngineering
 Multi-scaletransformer language models,2020, arXiv preprint arXiv:2005
 Deep high-resolution representation learning forhuman pose estimation,2019, In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition
 Forecasting at scale,2018, The American Statistician
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Pyramid vision transformer: A versatile backbonefor dense prediction without convolutions,2021, 2021
 Bp-transformer: Modellinglong-range context via binary partitioning,2019, arXiv preprint arXiv:1911
 Variational wishart approximation for graphical modelselection: Monoscale and multiscale models,2019, IEEE Transactions on Signal Processing
 Temporal regularized matrix factorization forhigh-dimensional time series prediction,2016, Advances in neural information processing systems
 Transformer hawkesprocess,2020, In International Conference on Machine Learning
