Figure 1: Portrait of Neural Collapse. Topfigure depicts the last-layer features, class-means, and classifiers with which NC isdefined—as well as the Simplex ETF to whichthey all converge with training. Bottom figureshows the deviations of features from theircorresponding class-means. Reproduced andmodified from Figure 1 of Papyan, Han, andDonoho (2020).
Figure 2: Decomposition of MSE loss: Each array column shows a benchmark image classificationdataset while each row shows a canonical deep net architecture trained with MSE loss. The red verticalline indicates the epoch at which zero training error was achieved. In each array cell, we plot terms ofthe MSE loss decomposition L(W, H) = LNC1 (H) + LNC2/3 (H) + LL⊥S(W , H) from Section 2.
Figure 3: Plots analogous to Figure 2 in Papyan et al. (2020), but on networks trained with MSE Loss.
Figure 4: Plots analogous to Figure 3 in Papyan et al. (2020), but on networks trained with MSE Loss.
Figure 5: Plots analogous to Figure 4 in Papyan et al. (2020), but on networks trained with MSE Loss.
Figure 6: Plots analogous to Figure 5 in Papyan et al. (2020), but on networks trained with MSE Loss.
Figure 7: Plots analogous to Figure 6 in Papyan et al. (2020), but on networks trained with MSE Loss.
Figure 8: Plots analogous to Figure 7 in Papyan et al. (2020), but on networks trained with MSE Loss.
Figure 9: Plots analogous to Figure 8 in Papyan et al. (2020), but on networks trained with MSE Loss.
Figure 10: Activation collapse under MSE loss vs. CE loss: Comparison of activation collapseobserved in this paper for networks trained under MSE loss (Figure 7) with that observed in Papyanet al. (2020) under CE loss. Networks trained with MSE loss tend to achieve faster activation collapsethan those trained with CE.
Figure 11: Adversarial robustness under MSE loss vs. CE loss: Comparison of adversarial robustnessobserved in this paper for networks trained under MSE loss (Figure 9) with that observed in Papyanet al. (2020) under CE loss. Robustness tends to be better—sometimes several magnitudes better—when the networks are trained with MSE loss than with CE loss.
Figure 12: Activation collapse on test data for both losses: Activation collapse observed on test datafor models trained with MSE loss (from this current paper) and CE loss (posted by Papyan, Han, andDonoho (2020) on Stanford Data Repository). On the test data, activation collapse still visibly occurson multiple dataset-network combinations: Albeit the rate of collapse is much slower on the test datacompared to that on the train data, and the plotted measure (described in Figure 6 of Papyan, Han,and Donoho (2020)) at the last epoch is larger than that on the train data. Also interesting is thatthe value at the last epoch is roughly monotonic with the difficulty of the dataset. See discussion inSection A.8.
Figure 13: Fiber bundle. Any full-rank features matrix, H, has a representative element, X, on X.
