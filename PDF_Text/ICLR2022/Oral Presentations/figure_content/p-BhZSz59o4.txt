Figure 1: Overview of BEIT pre-training. Before pre-training, We learn an “image tokenizer” Viaautoencoding-style reconstruction, where an image is tokenized into discrete visual tokens accordingto the learned vocabulary. During pre-training, each image has two views, i.e., image patches, andvisual tokens. We randomly mask some proportion of image patches (gray patches in the figure) andreplace them with a special mask embedding [M]. Then the patches are fed to a backbone visionTransformer. The pre-training task aims at predicting the visual tokens of the original image basedon the encoding vectors of the corrupted image.
Figure 2: Self-attention map for different reference points. The self-attention mechanism in BEIT isable to separate objects, although self-supervised pre-training does not use manual annotations.
