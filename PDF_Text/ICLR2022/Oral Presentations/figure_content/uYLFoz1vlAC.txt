Figure 1: (Left) State Space Models (SSM) parameterized by matrices A, B, C, D map an input signal u(t) tooutput y(t) through a latent state x(t). (Center) Recent theory on continuous-time memorization derives specialA matrices that allow SSMs to capture LRDs mathematically and empirically. (Right) SSMs can be computedeither as a recurrence (left) or convolution (right). However, materializing these conceptual views requiresutilizing different representations of its parameters (red, blue, green) which are very expensive to compute. S4introduces a novel parameterization that efficiently swaps between these representations, allowing it to handle awide range of tasks, be efficient at both training and inference, and excel at long sequences.
Figure 2: Visualizations of a trained S4 model on LRA Path-X. SSM convolution kernels K ∈ R16384 arereshaped into a 128 × 128 image. (Left) Example from the Path-X task, which involves deducing if the markersare connected by a path (Top) Filters from the first layer (Bottom) Filters from the last layer.
Figure 3: Comparison of S4 and specialized time-series models for forecasting tasks. (Top Left) The forecastingtask involves predicting future values of a time-series given past context. (Bottom Left) We perform simpleforecasting using a sequence model such as S4 as a black box. (Right) Informer uses an encoder-decoderarchitecture designed specifically for forecasting problems involving a customized attention module (figuretaken from Zhou et al. (2021)).
Figure 4: (Convolutional filters on Pathfinder) A random selection of filters learned by S4 in the first layer(top 2 rows) and last layer (bottom 2 rows) of the best model.
