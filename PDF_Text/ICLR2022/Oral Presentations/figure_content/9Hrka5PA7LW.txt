Figure 1: Illustration of supervised and unsupervised continual learning. The objective of SCL is to learnthe ability to classify labeled images in the current task while preserving the past tasksâ€™ knowledge, where thetasks are non-iid to each other. On the other hand, UCL aims to learn the representation of images without thepresence of labels and the model learns general-purpose representations during sequential training.
Figure 2: Evaluation on Few-shot trainingfor Split CIFAR-100 across different numberof training instances per task. The results aremeasured across three independent trials.
Figure 3: CKA Feature similarity between two in-dependent UCL models (red), two independent SCLmodels (blue), and UCL and SCL model (green) fordifferent strategies on Split CIFAR-100 test distribution.
Figure 4: Visualization of feature maps for the second block representations learnt by SCL and UCL strategies(with Simsiam) for ResNet-18 architecture after the completion of CL for Split CIFAR-100 dataset (n = 20).
Figure 5: Loss landscape visualization of T0 after the completion of training on task T0 (top) and T19 (bottom)for Split CIFAR-100 dataset on ResNet-18 architecture. We use Simsiam for UCL methods.
