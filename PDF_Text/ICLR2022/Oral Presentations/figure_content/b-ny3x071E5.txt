Figure 1: Bootstrapped Meta-Gradients.
Figure 2: Non-stationary grid-world (Section 5.1). Left: Comparison of total returns under anactor-critic agent over 50 seeds. Right: Learned entropy-regularization schedules. The figure depictsthe average regularization weight () over 4 task-cycles at 6M steps in the environment.
Figure 3: BMG ε-greedy explo-ration under a Q(λ)-agent.
Figure 4: Human-normalized score across the 57 games in Atari ALE. Left: per-game difference inscore between BMG and our implementation of STACX* at 200M frames. Right: Median scoresover learning compared to published baselines. Shading depict standard deviation across 3 seeds.
Figure 5: Ablations on Atari. Left: human normalized score decomposition of TB w.r.t. optimizer(SGD, RMS), matching function (L2, KL, KL & V), and bootstrap steps (L). BMG with (SGD,L2, L = 1) is equivalent to STACX. Center: episode return on Ms Pacman for different L. Right:distribution of episode returns over all 57 games, normalized per-game by mean and standarddeviation. All results are reported between 190-200M frames over 3 independent seeds.
Figure 6: MiniImagenet 5-way-5-shot meta-test performance. Left: performance as a function ofmeta-training batches. Center: performance as a function of wall-clock time. Right: best reportedperformance under each K . Error bars depict standard deviation across 3 seeds.
Figure 7:Two-colorsGrid-world.
Figure 8: Total rewards on two-colors with actor-critics. Shading: standard deviation over 50 seeds.
Figure 9:	Range of the entropy of a softmax-policy over time (2-colors). Each shaded area shows thedifference between the entropy 3333 steps after the agent observes a new entropy and the entropyafter training on the reward-function for 100000 steps. Meta-gradients without explicit entropy-regularization (left) reduce entropy over time while Bootstrapped meta-gradients (right) maintainentropy with a large enough meta-learning horizon. Averaged across 50 seeds.
Figure 10:	Ablations for actor-critic agent with BMG. Each shaded area shows the range of entropyregularization weights generated by the meta-learner. The range is computed as the differencebetween at the beginning and end of each reward-cycle. Left: entropy regularization weight rangewhen K = 1 and L = 7. Center: entropy regularization weight range when K = 1 and L = 1. Right:For K = 1 effect of increasing L with or without meta-entropy regularization. Result aggregatedover 50 seeds.
Figure 11: Total reward on two-colorswith an actor-critic agent and differentmatching functions for BMG. Shading:standard deviation over 50 seeds.
Figure 12: Results on two-colors under a Q(λ) agent with meta-learned ε-greedy exploration underBMG. Averaged over 50 seeds.
Figure 13: Atari BMG decomposition.
Figure 14: Atari, learning curves on MSPacman for KL &V . L = 4, R com-putes the Lthe step on only replay data.
Figure 15: Atari experience replay ablation. We report episode returns, normalized to be in the range[0, max return] for each game for ease of comparison. Shading depicts standard deviation across 3seeds. D denotes default BMG configuration for L = 1, with L = 4 analgously defined. R denotesL = 1, but with additional replay in the meta-objective to match the amount of replay used in L = 4.
Figure 16:	Atari K vs L ablation. We report episode returns, normalized to be in the range[0, max return] for each game for ease of comparison. Shading depicts standard deviation across 3seeds. D denotes default BMG configuration for L = 1, with L = 4 analogously defined. K = 2denotes L = 1, but K = 2 steps on agent parameters.
Figure 17:	Atari: Computational characteristics as a function of network size (see Appendix C.4)and meta-learning horizon H . When H = K , we vary the number of update steps to backpropagatethrough (with L = 1 for BMG). When H = L, we vary the number of target update steps (withK = 1). Measurements are taken over the first 20 million learning frames on the game Pong.
Figure 18: Atari BMG, alternative meta-update strategies. NS re-computes theagent-update the meta-update, akin to atrust-region method. DB uses the boot-strap target as the next agent parame-ters. Shading depicts standard deviationacross 3 seeds.
Figure 19: Atari, per-game performance across 3 seeds. Shading depicts standard deviation.
