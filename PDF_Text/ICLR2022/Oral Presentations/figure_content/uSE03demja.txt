Figure 1: A gallery of our four environments (left to right) across three rendering domains (top tobottom). For each environment, we train a RISP with images under varying lighting, background,and materials generated from a differentiable render (top). Each environment then aims to find propersystem and control parameters to simulate and render the physical system (middle) so that it matchesthe dynamic motion of a reference video (bottom) with unknown rendering configurations. Wedeliberately let three rows use renderers with vastly different rendering configurations.
Figure 2: An overview of our method (Sec. 3). We first train RISP using images rendered with randomstates and rendering parameters (top). We then append RISP to the output of a differentiable renderer,leading to a fully differentiable pipeline from system and control parameters to states predicted fromimages (middle). Given reference images generated from unknown parameters (dashed gray boxes) inthe target domain (bottom), we feed them to RISP and minimize the discrepancies between predictedstates (rightmost green-gray box) to reconstruct the underlying system parameters, states, or actions.
Figure 3: Imitation learning in the hand environment (Sec. 4.4). Given a reference video (bottom row,shown as five intermediate frames), the goal is to reconstruct a sequence of actions that resembles itsmotion. We show the motions generated using a randomly chosen initial guess of the actions (toprow) and optimized actions using our method with rendering gradients (middle row).
Figure 4: Imitation learning in the real-world experiment (Sec. 4.5). Given a reference video (toprow), the goal is to reconstruct a sequence of actions sent to a virtual quadrotor that resembles itsmotion. We illustrate the motions reconstructed using our method (bottom row).
Figure 5: Imitation learning in the real-world experiment. Given a reference video (top), the goal is toreconstruct a sequence of actions that resembles its motion. We illustrate the motions reconstructedusing our method (upper middle), the pixelwise loss (lower middle), and its enhanced variant (bottom).
Figure 6: (a) The number of epoch versus loss curves. The solid and dashed curves represent thetraining curves of ours and ours-no-grad respectively. The color of the lines indicates the number ofrendering configurations in training set where green and orange are 1 and 10, and red curve samples adifferent rendering configuration for every training data. (b) The loss versus Youngâ€™s modulus curves.
