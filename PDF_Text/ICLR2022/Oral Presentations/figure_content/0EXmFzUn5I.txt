Figure 1: Graphs of commonly used neural network models for sequence data.
Figure 3: Coarser-Scale construction module: B is the batch size and D is the dimension of a node.
Figure 4: Comparison of the time and memory consumption between the full, the prob-sparse, andthe TVM implementation of the pyramidal attention: (a) computation time; (b) memory occupation.
Figure 6: Visualization of prediction results on the synthetic dataset.
Figure 7:	Visualization of the extracted features across time in second channel at different scales:(a) scale 1; (b) scale 2; (c) scale 3.
Figure 8:	Time series with different lengths in the ETTm1 dataset. The sequence length in (a) and(b) is 672, and that in (c) and (d) is 1344. The time series in (a) and (b) corresponds to the latter halfof those in (c) and (d) respectively.
