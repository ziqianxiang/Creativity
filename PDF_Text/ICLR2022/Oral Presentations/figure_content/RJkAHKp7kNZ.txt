Figure 1: Illustration suggesting the role that visual perspective can play in facilitating the acquisition of sym-metries with respect to certain transformations on the world state s. T0 : planar translation of the end-effectorand cube. T1: vertical translation of the table surface, end-effector, and cube. T2: addition of distractor objects.
Figure 2: DAgger and DrQ results for cube grasping.
Figure 3: DAC results for cube grasping. Left: base variant (initial object and end-effector position random-ization) with no distribution shift between demo collection and training. Center: base variant with table heightshift between collection of 25 demos and training. Right: base variant plus three distractor objects with nodistribution shift between demo collection and training. Across the three experiment variants, the hand-centricperspective enables the agent to generalize in- and out-of-distribution more efficiently and effectively. Shadedregions indicate the standard error of the mean over five random seeds.
Figure 4: Sample observations from O3 (left) and Oh π ◦ O3+p	20∙0	12.5	15.8(right) in our real robot apparatus.
Figure 5: The Meta-World tasks used in the experiments in Section 4. The top row contains third-person obser-vations o3, and the bottom row contains corresponding hand-centric observations oh . Initial object positionsare randomized. The last two tasks, reach-hard and peg-insert-side-hard, are custom-made; there, the green goaland the green peg are randomly initialized either to the left or to the right of the gripper with equal probability,and they are not initially visible to the hand-centric perspective. Because of the severely limited hand-centricobservability, the third-person perspective is crucial for learning to direct the gripper to the correct location.
Figure 6: DrQ-v2 results for Meta-World. Each row contains results for two manipulation tasks that roughlyexhibit the same level of hand-centric observability, which decreases from top to bottom (high, moderate,low). Using the proposed approach (both perspectives with a VIB on the third-person perspective’s representa-tion) leads to the best out-of-distribution generalization performance for all levels of hand-centric observability(though it is matched by the hand-centric perspective when hand-centric observability is high, as expected).
Figure 7: Visualization of the table height distribution shift used in the cube grasping experiments. From leftto right, zshift is -0.10, -0.05, 0, +0.05, +0.10. The top and bottom rows contain the third-person and hand-centric perspectives, respectively. Positions of the cube and end-effector are not randomized in this visualizationfor the sake of clarity.
Figure 8: Visualization of the distractor objects distribution shift used in the cube grasping experiments. Fromleft to right, we have “mix” (1 red, 1 green, 1 blue), 3 red, 3 green, 3 blue, 3 brown, 3 white, and 3 black.
Figure 9: Visualization of the table textures distribution shift used in the cube grasping experiments. The toptwo rows contain the third-person and hand-centric perspectives of the five table textures used during trainingfor DAgger and DrQ, and the bottom two rows contain the perspectives of five held-out textures used at testtime (out of twenty total held-out textures). Positions of the cube and end-effector are not randomized inthis visualization for the sake of clarity. The textures were acquired from the describable textures dataset(DTD) (Cimpoi et al., 2014).
Figure 10: DrQ results for cube grasping with (left) and without (right) image augmentation. Note that theleft half of this figure is an exact replica of the right half of Figure 2. Ablating the image augmentationcomponent of DrQ reveals its importance; without it, training fails to converge even with much larger amountsof environment interaction. However, the hand-centric perspective still facilitates faster training than the third-person perspective for the first two experiment variants. Shaded regions indicate the standard error of the meanover three random seeds.
Figure 11: Table height distribution shifts. Columns from left to right: -0.05 m, -0.025 m, train, +0.025 m,+0.05 m. Top (bottom): observations from O3 (Oh).
Figure 12: Distractor object distribution shifts. Columns from left to right: train, distractor test set 1, distractortest set 2, distractor test set 3. Top (bottom): observations from O3 (Oh).
Figure 13: Table texture distribution shifts. Columns from left to right: train, blue floral, green watercolorgarden, rainbow floral. Top (bottom): observations from O3 (Oh).
Figure 14: Visualizations of the two sides that the green goal site or peg can be initialized to in the reach-hardand peg-insert-side-hard tasks in Meta-World, respectively. One of the two sides is chosen via “coin flip” at thebeginning of each episode.
Figure 15: Visualization of the train and test distributions in the six Meta-World environments used in theexperiments in Section 4. The three columns in the left half of the figure show three sets of initial objectpositions randomly sampled from the training distribution; the three columns in the right half correspond tothe test distribution. From top to bottom are handle-press-side, button-press, soccer, peg-insert-side, reach-hard, and peg-insert-side-hard. Both the third-person and hand-centric perspectives are shown for each randominitialization.
Figure 16: Visualizations of the two third-person perspectives used in the ablation agent π ◦ O3+3+p + VIB(z3)discussed in Section 4.3. The top row contains the original third-person perspective; the bottom row containsthe second third-person perspective used only in the ablation.
Figure 17: Ablation studies on π ◦ Oh+3+p + VIB(z3) in Meta-World. Shaded regions indicate the standarderror of the mean over three random seeds. Note that for π ◦ Oh+3+p +VIB(zh) +VIB(z3) in peg-insert-side,DrQ-v2 training did not converge within the specified number of training steps for one of three random seeds(hence the larger shaded regions and the lower out-of-distribution generalization performance). In addition, forπ ◦ Oh+3+p + VIB(zh) + VIB(z3) in peg-insert-side-hard, DrQ-v2 training did not converge for any of thethree seeds.
Figure 18: DrQ-v2 results for Meta-World for π ◦ Op, i.e. an agent that operates solely from proprioception.
Figure 19: Visualizations of the two modified versions of the peg-insert-side task in Meta-World, where thefirst and second rows contain the third-person and hand-centric perspectives of the initial configurations, re-spectively. Left: original peg-insert-side setup. Center: end-effector initially rotated 90 degrees about thevertical axis (corresponding to the left half of Figure 20). Right: peg initially rotated 90 degrees about thevertical axis (corresponding to the right half of Figure 20).
Figure 20: DrQ-v2 results for the two modified versions of the Meta-World peg-insert-side task (visualizedin Figure 19). For the first modified version, we see generalization performance trends similar to those in theoriginal rotation-less peg-insert-side task (second row, second column of Figure 6). In the second modifiedversion, π ◦ Oh+3+p + VIB(z3) outperforms π ◦ Oh+3+p in terms of sample complexity and generalization,and π ◦ Oh+p in terms of sample complexity. However, test performance begins to droop after 1.6M steps - Weattribute this to overfitting on the training distribution, which would likely occur to the other agents as well ifthey were trained post-convergence to a similar extent. Shaded regions indicate the standard error of the meanover three random seeds.
