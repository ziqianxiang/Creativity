Figure 1: Multivariate distributions over 4K-1. Standard distributions, like the Logistic-Normal(left), assign zero probability to all faces but ri(4K-1). Our mixed distributions support assigningprobability to the full simplex, including its boundary: the Gaussian-Sparsemax (right) induces adistribution over the 1-dimensional edges (shown as a histogram), and assigns Pr{(1, 0, 0)} = .022.
Figure 2: Left: Decomposition of a simplex as thedisjoint union of the relative interior of its faces.
Figure 3: tSNE plots of: posterior samples y ∈4K-1 (left), predicted log-potentials w ∈ RK(right). Colors encode digit label (not available tomodels). Clusters are formed in latent space and inhow digits are assigned to faces (recall that w pa-rameterizes a Gibbs distribution over F(4k-i)).
Figure 4: Maximum entropies for mixed distributions for several values of bit precision N , as afunction of the simplex dimensionality K - 1. Shown are also the maximum entropies for thecorresponding discrete and continuous cases, for comparison.
Figure 5: tSNE plots of posterior samples, from left-to-right: Gaussian, Dirichlet, Mixed Dirichlet,Categorical, Gumbel-Softmax ST.
Figure 6:	Pixel-wise average of 100 model samples generated by conditioning on each of the tenvertices of the simplex. From left-to-right: Mixed Dirichlet, Categorical, Gumbel-Softmax ST, andDirichlet (as the Dirichlet does not support the vertices of the simplex, we add uniform noise (between0 and 0.1) to each coordinate of a vertex and renormalize).
Figure 7:	Conditional generation. For each instance of each class in the validation set (note thatnone of the models has access to the label), we sample a latent code conditioned on the digit, andre-sample a digit from the model. The illustration displays the pixel-wise average across all validationinstances of the same class. From left-to-right: Gaussian, Dirichlet, Mixed Dirichlet, Categorical,Gumbel-Softmax ST.
Figure 8:	Unconditional generation. For each model, we sample a digit via ancestral sampling (i.e., zis sampled from the prior, then, given z, x is sampled from the observation model). We gather 5000such samples and group them by class as predicted by a 5-nearest neighbour classifier trained onthe MNIST training set (we use kd tree from scikit-learn), the classifier achieves 95% F1 on theMNIST test set. Each image in the illustration is a pixel-wise average of the samples in the cluster,we also report the percentage of digits in each cluster. From left-to-right: Gaussian, Dirichlet, MixedDirichlet, Categorical, Gumbel-Softmax ST.
Figure 9: Test error (root-mean-square error on the left, mean absolute error on the right) ofgeneralized linear regression towards 5-dimensional vote proportions (UK election data). We compare3 likelihood functions: Dirichlet, Continuous Categorical (CC), and Mixed Dirichlet.
