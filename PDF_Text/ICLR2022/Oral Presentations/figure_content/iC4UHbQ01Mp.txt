Figure 1: An image with a16 × 16 backdoor patch.
Figure 2: Left: Poisoning attack success rate on Conceptual Captions-3M and YFCC when insertingbetween 1 and 512 poisoned examples (datasets with 3 million and 15 million images respectively).
Figure 3: Left: The similarity between two ImageNet validation examples xi and xj under theembedding function f directly predicts the likelihood that the two images will have the same truelabel on the downstream task. Right: By poisoning 0.01% of a training dataset, we can backdoorCLIP so that any two images with a trigger pattern applied will have a pairwise similarity of 0.78.
Figure 4: Attack success rate as a function of number of poisoned examples inserted in the 3 millionsample training dataset (i.e., ranging from 0.0025% to 0.05%). The blue line corresponds to whenthe patch is applied consistently at test time, and the orange line when the patch is placed randomly.
Figure 5: Evaluating the scalability of our attack. Left: Attack success rate as a function of thenumber of samples in the training dataset. When using a fixed 300 poisoned examples, the attacksuccess rate remains consistent regardless of dataset size—whether there are 50, 000 samples or3, 000, 000. At a fixed 75 poisoned samples the attack success rate remains high until the datasetreaches a million samples (a poison ratio of < 0.01%), but degrades at two and three million samples.
Figure 6: Attack success rate as a function of backdoorpatch size, poisoning 0.0025% of the dataset. As thepatch increases to 4 × 4 the attack begins to succeed.
