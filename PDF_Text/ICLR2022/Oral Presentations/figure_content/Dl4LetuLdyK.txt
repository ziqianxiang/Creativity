Figure 1: Visualization of the joint distribution for the different shifts we consider on the dSpritesexample. The lighter the color, the more likely the given sample. figure 1a-1c visualise differentshifts over ptrain(yl, ya) discussed in 2.2: spurious correlation (SC), low-data drift (LDD), and un-seen data shift (UDS). figure 1d visualises the test set, where the attributes are uniformly distributed.
Figure 2: Dataset samples. Each row fixes an attribute (e.g. color for dSprites, MPI3D,Shapes3D; azimuth for S mallNorb; hospital for Camelyon 1 7; and location for iWildCam).
Figure 3: Spurious Correlation. We use allcorrelated samples and vary the number of sam-ples N from the true, uncorrelated distribution.
Figure 4: Low-data drift. We use all sam-ples from the high data regions and vary thenumber of samples N from the low-data region.
Figure 5: Unseen data shift. We rank themethods (where best is 1, worst 19) for eachdataset and seed and plot the rankings, with theoverall median rank as the black bar. Pretrain-ing on ImageNet and ImageNet augmentationperform consistently best. DANN, CycleGANand other heuristic augmentations perform consis-tently well.
Figure 6: Condition 1: Noisy labels. We varythe amount of noise p in the labels. We plot thepercentage change over the baseline ResNet, av-eraged over all seeds and datasets.
Figure 7: Condition 2: Fixed data. We vary thetotal size of the dataset T. We plot the percentagechange over the baseline ResNet, averaged overall seeds and datasets.
