Figure 1: Analysis of the weighted training algorithms. We analyze the impact of two crucial factorson the improvement of the weighted training algorithms: the ratio between the training sizes of thesource and target datasets, and the training size of the target dataset. In this figure, we use NER asthe target task and source tasks include PoS tagging and predicate detection. In the first subfigure,we keep the training size of the target tasks as 500 and change the ratio from 1 to 128 on a log scale.
Figure 2: Extension to weighted-sample training. The weighted training algorithm can be easilyextended from the weights on tasks to the weights on samples. As for the experiments on weighted-sample training, we use the PoS tagging on entity words as the source task and named entityclassification on entity words as the target task. Note that the settings for the weighted-sampletraining are quite different from those for the weighted-task training in the remaining parts becausethe weighted-sample training is much more costly compared to the weighted-task training.
Figure 3: Illustration for the task distance. We find that the source data is more beneficial when thetask distance between the source data and the target data is smaller or the size of the targetdata is smaller. In this part, we keep the training size of the source task as 10000 and changethe training size of the target task from 10 to 10000 in a log scale. STL denotes the single-tasklearning only with the target data. LTL denotes the learning to learn paradigm, where we first learnthe representations in the source data and then learn the task-specific function in the target data. Forthe learning to learn paradigm, we consider the source task with different flip rates from 0.0 to 1.0,where the flip rate is an important factor in generating the source data and lower flip rate indicates asmaller task distance between the source data and the target data.
