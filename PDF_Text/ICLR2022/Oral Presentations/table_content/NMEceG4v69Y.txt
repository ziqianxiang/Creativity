Table 2: ImageNet-1K classification				Table 3: Comparison with SOTA models on					for MLP-like models.				ImageNet-1K without extra data.					Stage. The blocks with the same architecture are stacked to form one Stage (He et al., 2016). Thenumber of tokens (feature scale) is maintained within each stage. At each stage transition, thechannel capacity of the processed tokens is expanded while the number of tokens is reduced. Thisstrategy effectively reduces the spatial resolution complexity. Overall, each of our model variantshas four stages, and the output feature at the last stage has a shape of H X W2 X C4. These stagesettings are widely utilized in both CNN (Simonyan & Zisserman, 2014; He et al., 2016) and Trans-former (Wang et al., 2021b; Liu et al., 2021b) models. Therefore, CycleMLP can conveniently serveas a general-purpose visual backbone and a generic replacement for existing backbones.
Table 4: Ablation on three parallel branches.
Table 5: Stepsize ablation: CycleMLPachieves the highest mIoU on ADE20Kwhen stepsize is 7. However, the stepsizehas negligible influence on the ImageNetclassification.
Table 6: Object detection and instance segmentation on COCO val2017 (Lin et al., 2014).
Table 7: The instance segmentation results of different backbones on the COCO val2017 dataset.
Table 8: Semantic segmentation on ADE20K (Zhouet al., 2017) val. All models are equipped with Se-mantic FPN (Kirillov et al., 2019). * Results are fromGFNet (Rao et al., 2021).
Table 9: The semantic segmentation results of different backbones on the ADE20K validation set.
Table 10: Robustness on ImageNet-C (Hendrycks & Dietterich, 2019). The mean corruptionerror (mCE) normalized by AlexNet (Krizhevsky et al., 2012) errors is used as the robustness metric.
Table 11: Instantiations of the CycleMLP with varying complexity. The Ei and Li denote theexpand ratio and number of repeated layers. Our design principle is inspired by the philosophy ofResNet (He et al., 2016), where the channel dimension increases while the spatial resolution shrinkswith the layer going deeper.
Table 12: The semantic segmentation results of different backbones on the ADE20K validation set.
Table 13: Comparison with dilated and randomsampling. For random sampling, we conduct theexperiments for three independent trials with threeseeds (S=1, 2, 3).
Table 14: Comparison with dense sampling: Onthe consideration of training time, we only train bothmodels for 100 epochs for fair comparison.
Table 15: Comparison on different stepsizes (e.g.,even stepsize and odd stepsize), including 7×2,4×4.
