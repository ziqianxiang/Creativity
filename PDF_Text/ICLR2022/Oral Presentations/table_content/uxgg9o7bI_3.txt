Table 1: Classification accuracy (%) averaged over 10 runs on node classification.
Table 2: Classification accuracy (%) averaged over 10 runs on graph classification. The results ofWL and RetGK are taken from (Du et al., 2019), GraphSAGE from (Xu et al., 2019), DGCNN from(Maron et al., 2019) and others from their original papers. f indicates the reporting setting used inGIN and further details on the experimental settings are discussed in Appendix B.
Table 3: Classification accuracy (%) averaged over 10 runs on graph classification, where λ = 2. Theresults of the baselines are taken from (Hu et al., 2020) and the leaderboard of the OGB website.
Table 4: Classification accuracy (%) averaged over 10 runs on graph classification, where λ = 2.
Table 5: Classification accuracy (%) averaged over 10 runs on node classification with standard splits.
Table 6: Classification accuracy (%) averaged over 10 runs on graph classification with random splits.
Table 7: Classification accuracy (%) averaged over 10 runs on Cora dataset.
Table 8: Comparison of the aggregation schemes Used in existing message-passing GNNsComplexity AnalysisTable 9 sUmmarizes the time and space complexities of several popUlar message-passing GNNs andGraphSNN, where n and m are the nUmbers of vertices and edges in a graph, respectively, k refers tothe nUmber of layers, f and d are the dimensions of inpUt and oUtpUt featUre vectors, respectively, ais the nUmber of attention heads Used in GAT, and s is the nUmber of neighbors sampled for eachnode at each layer in GraphSAGE.
Table 9: Time and space complexities of message-passing GNNs and GraphSNN.
Table 10: Statistics for node classification datasets.
Table 11: Statistics for small graph classification datasets.
Table 12: Statistics for large graph classification dataset (OGB graph datasets).
Table 13: Classification accuracy (%) averaged over 10 random splits on node classification.
Table 14: Classification accuracy (%) averaged over 10 runs on graph classification.
Table 15: Running time of the prepocessing step for large graph datasets averaged over 5 runs.
Table 16: Oversmoothing analysis of GIN and spectral GNN (DFNet) on cora dataset.
Table 17: Analysis the effects of our structural coefficients with substructure counts, i.e, triangle and4-clique counts. Classification accuracy (%) averaged over 10 runs on graph classification.
Table 18: Analysis the effects of our structural coefficients with substructure counts, i.e, cycle counts.
