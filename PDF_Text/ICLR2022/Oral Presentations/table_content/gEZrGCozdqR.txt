Table 1: Results for translation and struct-to-text tasks. [k] indicates the number of few-shotexemplars. #t indicates the number of templates that FLAN is evaluated on.aT5-11B, cEdunov et al.
Table 2: Results for eight NLU task clusters. All values shown are for accuracy (or exact match)except DROP, MultiRC, and SQuAD v1 and v2, which are F1. [k] indicates the number of few-shotexemplars. #t indicates the number of templates that FLAN is evaluated on.aT5-11B, bBERT-large.
Table 3: Ablation study result using models where instructions are removed from the finetuningprocess. In “no template,” only inputs and outputs are given, which does not distinguish amongtasks during multi-task finetuning. In “task/dataset name”, inputs during multi-task finetuning areprepended with the name of the task and dataset (e.g., “[Translation: WMT’14 to French] The dogruns”) NLI datasets: ANLI R1-R3, CB, and RTE; reading comprehension datasets: BoolQ, MWtiRC,and OpenbookQA; closed-book QA datasets: ARC-c, ARC-e, NQ, and TQA; translation datasets:WMT’14 Fr什En, WMT'16 De什En, and WMT'16 Ro»En. Notably, training with task/datasetname achieved a high NLI score largely because it achieved a score of 83.9 on the CB dataset, forwhich the validation set only has 56 examples (FLAN also gets 83.9 with the best dev template, butthe average template was only 64.1).
Table 4: FLAN (instruction tuning) responds better to continuous inputs attained via prompt tuningthan LaMDA-PT (no instruction tuning). When prompt tuning on a given dataset, no tasks from thesame cluster as that dataset were seen during instruction tuning.
Table 5: Overlap statistics for the subset of datasets that are also used in GPT-3, sorted from dirtiestto cleanest. An evaluation example was dirty if it had any n-gram collision with the pretrainingcorpus. We computed these results for FLAN’s performance using only a single template for eachdataset, so they differ slightly compared with the results for average performance over all templates.
Table 6:	Example input and target for Adversarial NLI (ANLI). ANLI (Nie et al., 2020) is alarge-scale NLI benchmark with adversarial examples collected iteratively with a human and modelin the loop. The task is to determine whether a hypothesis is entailed by a premise (entailment, notentailment, or impossible to say). There are three rounds, R1-R3. Of the three training sets with16,946, 45,460, and 100,459 examples, we use 16,946, 30,000, and 30,000 for train and 200 fromeach of the three TFDS validation sets for dev. We use the TFDS “test” sets of 1,000, 1,000, and1,200 examples as our test set for reporting numbers.
Table 7:	Example input and target for Commitment Bank (CB). CB (De Marneffe et al., 2019) is acorpus of texts in which a hypothesis is extracted from a premise, and the task is to determine whetherthe hypothesis is entailed by the premise (entailment, not entailment, or impossible to say). Of thetraining set with 250 examples, we use 200 for train and 50 for dev. We use the TFDS validation setof 56 examples as our test set for reporting numbers.
Table 8:	Example input and target for Recognizing Textual Entailment (RTE). RTE (Dagan et al.,2005; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) asks whether a secondsentence is entailed by a first (binary, either entailed or not entailed). Of the training set with 2490examples, we use 2,290 for train and 200 for dev. We use the TFDS validation set of 277 examples asour test set for reporting numbers.
Table 9:	Example input and target for Boolean Questions (BoolQ). BoolQ Clark et al. (2019a) asks ayes/no question based on a passage and a question. Of the training set with 9,427 examples, we use9,227 for train and 200 for dev. We use the TFDS validation set of 3,270 examples as our test set forreporting numbers.
Table 10:	Example input and target for Multi-Sentence Reading Comprehension (MultiRC). MultiRCKhashabi et al. (2018) asks an open-ended question given a paragraph that contains the answer. Ofthe training set with 27,243 examples, we use 27,043 for train and 200 for dev. We use the TFDSvalidation set of 4,848 examples as our test set for reporting numbers.
Table 11: Example input and target for Openbook Question Answering (OBQA). OBQA (Mihaylovet al., 2018) asks 4-way multiple choice questions based facts. Of the training set with 4,957 examples,we use all for train and 200 in the TFDS validation set of 500 examples for dev. We use the TFDStest set of 500 examples as our test set for reporting numbers.
Table 12:	Example input and target for Choice of Plausible Alternatives (COPA). COPA (Roemmeleet al., 2011) is a causal reasoning task that asks to infer either a cause of effect of a premise fromtwo choices. Of the training set with 400 examples, we use 350 for train and 50 for dev. We use theTFDS validation set of 100 examples as our test set for reporting numbers.
Table 13:	Example input and target for Commonsense Sentence Completion (HellaSwag). HellaSwag(Zellers et al., 2019) tests for sentence completion that requires common sense, asking for the mostprobable ending given four contexts. Of the training set with 39,905 examples, we use 30,000 fortrain and 200 for dev. We use the TFDS validation set of 10,042 examples as our test set for reportingnumbers.
Table 14:	Example input and target for Physical Question Answering (PiQA). PiQA (Bisk et al.,2020) is a commonsense QA benchmark for naive physics reasoning, where a solution to a goal mustbe selected from two choices. Of the training set with 16,113 examples, we use 16,013 for train and100 for dev. We use the TFDS validation set of 1,838 examples as our test set for reporting numbers.
Table 15:	Example input and target for The Story Cloze Test (StoryCloze). StoryCloze (Mostafazadehet al., 2016) is a commonsense reasoning framework for story generation, where a system choosesthe correct ending to a four-sentence story. We use the 2016 version on TFDS. Of the validation setwith 1,871 examples (no training set is available), we use 1,671 for train and 200 for dev. We use theTFDS test set of 1,871 examples as our test set for reporting numbers.
Table 16:	Example input and target for The AI2 Reasoning Challenge (ARC). ARC (Clark et al.,2018) asks grade-school level 4-way multiple choice science questions. There is a challenge set andan easy set, where the challenge set questions were answered incorrectly by both a retrieval-basedalgorithm and a co-occurrence algorithm. Of the training sets with 1,119 examples (challenge) and2,251 (easy), we use we use 919 and 2,051 respectively for train and 200 each for dev. We use theTFDS test sets of 1,172 and 2,376 examples respectively as our test set for reporting numbers.
Table 17: Example input and target for Natural Questions (Open) (NQ). NQ (Lee et al., 2019;Kwiatkowski et al., 2019) asks for an open-ended answer given a question, where all questions can beanswered using the contents of Wikipedia. Of the training set of 87,925 examples, we use 30,000 fortrain and 200 for dev. We use the TFDS validation set of 3,610 examples as our test set for reportingnumbers.
Table 18: Example input and target for Trivia Question Answering (TriviaQA). TriviaQA Joshi et al.
Table 19: Example input and target for Adversarial Winograd Schema Challenge (Winogrande).
Table 20: Example input and target for Winograd Schema Challenge (WSC273). WSC273 (Levesqueet al., 2012) tests for coreference resolution by asking a model to complete the sentence in a fashionthat requires understanding the entities in the sentence. Of the 0 examples in the training set (WSC273is test-set only), we use none for train and none for dev. We use the TFDS test set as our test set forreporting numbers.
Table 21:	Example input and target for Reading Comprehension with Commonsense Reasoning(ReCoRD). ReCoRD (Zhang et al., 2018) asks for the answer to a cloze-style question where anentity is masked out. Of the the training set of 100,730 examples, we use 30,000 for train and 200 fordev. We use the TFDS validation set of 10,000 examples as our test set for reporting numbers.
Table 22:	Example input and output for translation. This example is from WMT’16 English-German;all languages use the same translation templates.
