Table 1: Discourse coherence accuracy measured by the test accuracy of the trained linear classifier, reportingμ± standard error over 3 runs. Random accuracy is 50%. Values are bolded if they are within range of thehighest mean score and its corresponding standard error. The highest mean score are marked in gray cells.
Table 2: BLEU on ground Table 3: Percentage of length Table 4: Section lengths deviating from ex-truth infill and generated sen- mismatch (MM) during genera- pected length in forced long text generationtence.	tion.	reported in % (1).
Table 5: Ordering in forced long text generation. ROC Stories and TM-2 omitted because they are not applicable.
Table 8: Example of forced long text generation on TicketTalk with Time Control vs. fine-tuned GPT2. Bothmodels are forced to extrapolate when generating long texts. They start coherently, but only Time Controlextrapolates coherently. For space reasons, some of the text has been removed, marked with [...].
Table 9: Perplexity after fine-tuning.
Table 10: Example of text infilling with Donahue et al. (2020)’s LM model on ROCStories.
Table 11: Example of text infilling with Donahue et al. (2020)’s ILM model on ROCStories.
Table 12:	Example of text infilling with Time Control (d=8) on ROCStories.
Table 13:	Example of text infilling with Time Control (d=16) on ROCStories.
Table 14:	Example of text infilling with Time Control (d=32) on ROCStories.
Table 15:	Example of text infilling with Variational Auto-Encoder on ROCStories.
Table 16:	Example of text infilling with Implicit Dynamics on ROCStories.
Table 17: BERTScore (Zhang et al., 2019), ROUGE, and BLEURT (Sellam et al., 2020) on groundtruth infilled sentence and the generated sentence.
