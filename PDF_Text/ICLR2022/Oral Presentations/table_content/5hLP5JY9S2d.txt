Table 1: Comparisons of our improved baselines (MSP+, MLS) against state-of-the-art meth-ods on the standard OSR benchmark datasets. All results indicate the area under the Receiver-Operator curve (AUROC) averaged over five ‘known/unknown’ class splits. ‘+’ indicates priormethods augmented with improved closed-set optimization strategies, including: MSP+ (Neal et al.,2018), OSRCI+ (Neal et al., 2018) and (ARPL + CS)+ (Chen et al., 2021).
Table 2: Statistics of the Semantic Shift Benchmark. Weshow ‘#Classes(#Test Images)' for the known classes, and forthe ‘Easy', ‘Medium' and ‘Hard' open-set classes.
Table 3: OSR results on the Semantic Shift Benchmark. We measure the closed-set classificationaccuracy and AUROC on the binary open-set decision. We also report OSCR, which measures thetrade-off between open and closed-set performance. OSR results are shown on ‘Easy / Hard’ splits.
Table 4: Standard deviations of our experiments in fig. 2 of the main paper. We report thestandard deviations for both the closed-set and open-set performance (accuracy/AUROC) across thefive ‘known/unknown’ class splits.
Table 5: Breakdown of methods used to improve the closed-set classification accuracy ofthe baseline method. All experiments were conducted with a VGG32 backbone over five‘known/unknown’ splits of the TinyImageNet dataset. The bracketed number with the Cosinescheduler indicates the number of learning rate restarts used during training. We find a PearsonProduct-Moment correlation of 0.93 between the closed-set accuracy and the open-set AUROC.
Table 6: Comparing our improved baseline with other deep learning based OSR methods onthe standard benchmark datasets. All results indicate the area under the Receiver-Operator curve(AUROC) as a percentage. We also show the backbone architecture used for each method, showingresults with multiple backbones when reported.
Table 7: Results on out-of-distribution detection benchmarks. We evaluate two MLS models:one represents a model which we train ourselves; the second represents a strong pre-trained modelfrom (Lim et al., 2019).
Table 8: Results of our strong baseline on the full OoD benchmark suite. We take strongWideResNet-40 models from (Lim et al., 2019) and run our MLS baseline on top. Models aretrained on CIFAR10 and CIFAR100 as ‘in-distribution’ and we report AUROC averaged across sixOoD datasets. All compared figures are taken from (Du et al., 2022) and Liu et al. (2020).
Table 9: Average Precision (AP) results on the proposed benchmark datasets for ‘Easy’ /‘Medium’ / ‘Hard’ splits.
