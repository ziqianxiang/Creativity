Table 1: ID accuracies with 90% confidence intervals over 3 runs—fine-tuning does betterthan linear probing on all datasets except DomainNet (which could be because the version of theDomainNet training dataset from Tan et al. (2020) is fairly small, with around 20K examples). LP-FTdoes the best on all except FMoW where itis in between linear probing and fine-tuning.
Table 2: OOD accuracies with 90% confidence intervals over 3 runs. Linear probing does betterthan fine-tuning on all datasets except CIFAR-10.1 and ImageNetV2, where the ID and OOD aresimilar (consistent with our theory). LP-FT does the best on all 10 datasets.
Table 3: OOD accuracies with 90% confidence intervals over 3 runs, for each of the three OODdomains in the split of DomainNet used by Tan et al. (2020); Prabhu et al. (2021). LP does better thanFT across the board, and LP-FT does the best.
Table 4: OOD accuracies with 90% confidence intervals over 3 runs, when fine-tuning gets to chooselearning rate and early stop, and linear probing gets to choose `2 regularization weights, on OOD data.
Table 5: In-distribution (ID): Average distance that features move before and after fine-tuning orLP-FT, multiplied by 100 to make things easier to read. For linear probing the numbers are all 0, sincethe features are not tuned. As predicted by our theory, we see that features for ID examples (this table)move more than features for OOD examples (Table 6). Both sets of features change substantially lessfor LP-FT. As usual we show 90% confidence intervals over three runs.
Table 6: Out-of-distribution (OOD): Average distance that features move before and afterfine-tuning or LP-FT, multiplied by 100 to make things easier to read. For linear probing the numbersare all 0, since the features are not tuned. As predicted by our theory, we see that features for IDexamples (Table 5) move more than features for OOD examples (this table). Both sets of featureschange substantially less for LP-FT. As usual we show 90% confidence intervals over three runs.
Table 7: ID and OOD accuracies on Living-17 using a CLIP ResNet-50 model pretrained on theWebImageText dataset, instead of unlabeled ImageNet examples. Similar findings hold—herefine-tuning does similarly to linear probing ID, but does worse than linear probing OOD. LP-FT doesbetter than both ID, and closes 86% of the gap OOD. As usual we show 90% confidence intervalsover three runs.
Table 8: ID and OOD accuracies on Living-17 using a CLIP ViT-B/16 (Vision Transformer) modelpretrained on the WebImageText dataset, instead of unlabeled ImageNet examples. This is the largestpublicly available CLIP model that we could find. The same findings hold—fine-tuning does betterthan linear probing ID, but does worse than linear probing OOD. LP-FT does better than both ID, andcloses 75% of the gap OOD. As usual we show 90% confidence intervals over three runs.
Table 9: ID and OOD accuracies on Living-17 including three additional fine-tuning heuristics,where we (1) Use a 10× larger learning rate for the head, or (2) Regularize the Euclidean distance ofthe feature extractor weights to the pretrained initialization, and (3) side-tuning where we freeze thepretrained model but add a side network that is fine-tuned. As a sanity check, all methods do betterthan training from scratch ID and OOD, and we show 90% confidence intervals over three runs. Asper the intuitions from the feature distortion theory, these methods do mitigate feature distortion tosome extent and improve OOD accuracy over fine-tuning. LP-FT does better than all methods ID andOOD—nonetheless, we believe that LP-FT is just the first step and hope that our theory can be usedto inspire or derive better algorithms.
