Table 1: 8-bit quantization with conven-tional training for ResNet18 and MobileNetV1∕V2b. Following Yao et al. (2021), we ab-breviate Integer-Only Quantization as “Int”,INT8-Multiplication-Only Quantization as“8-bit”, the Baseline Accuracy as “BL”, andTop-1 Accuracy as “Top-1”. All models arefor 8-bit weight and activation quantization.
Table 2: 8-bit quantization with tiny fine-tuningon well-trained full-precision model. Follow-ing Yao et al. (2021), we abbreviate Integer-OnlyQuantization as “Int”, INT8-Multiplication-OnlyQuantization as “8-bit”, Layer-Wise Quantiza-tion as “Layer”, the baseline accuracy as “BL”,Top-1 Accuracy as “Top-1”, and Top-1 AccuracyDrop with respect to the baseline as “Drop”. Weuse two baselines for ResNet50, one from Py-torchCV (Semery, 2021) (Baseline #1) and anotherfrom Nvidia (Nvidia, 2021) (Baseline #2), and weuse ResNet50b version. Note that the OMPQ (Maet al., 2021b) is mixed-precision quantization.
Table 3: Analysis of the impact of the searching space for fractional length (ResNet50 on ImageNet).
