Table 1: The performance for 8 medium-sized datasets. The first 6 datasets are binary classification(ordered by samples) and shown the AUC (%). The last 2 are regression datasets and shown the RootMean Squared Error (RMSE). We show the standard deviation of 5-fold cross validation results. Wecalculate average rank (Rank, lower the better) and average Normalized Score (NS, higher the better).
Table 2: The performance for 6 large datasets used in NODE paper. The first 3 datasets (Click,Epsilon and Higgs) are classification datasets and shown the Error Rate. The last 3 (Microsoft, Yahooand Year) are shown in Mean Squared Error (MSE). We show the relative improvement (Rel Imp) ofour model NODE-GAM to EBM and find it consistently outperforms EBM up to 7%.
Table 3: Comparison to NAM (Agarwal et al., 2020) with and without the ensemble.
Table 4: Comparison to NAM with normal units v.s. ExU units, and with and without ensemble.
Table 5: The default performance for 6 large datasets. The NODE-GA2M-Default is the model withdefault hyperparameter, and the Rel Diff is the relative difference of performance between defaultand tuned NODE-GA2M. The first 3 datasets (Click, Epsilon and Higgs) are classification datasetsand shown the Error Rate. The last 3 (Microsoft, Yahoo and Year) are shown in Mean Squared Error(MSE).
Table 6: All dataset statistics and descriptions.
Table 8: The best hyperparameters for NODE-GA2M architecture.
Table 9: The best hyperparameters for NODE architecture.
Table 10: The best hyperparameters for NODE-GAM architecture for 6 large datasets.
Table 11: The best hyperparameters for NODE-GA2M architecture for 6 large datasets.
