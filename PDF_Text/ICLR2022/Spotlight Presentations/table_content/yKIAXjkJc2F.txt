Table 1: Rotating MNIST: Reported MSE for the proposed InImNet (where n is the number of MLPlayers) and state-of-the-art methods, results from other methods are taken from Yildiz et al. (2019)and Vialard et al. (2020). (**) The standard deviation given is more than half the mean value asstated in Vialard et al. (2020)GT 0HH0≡≡≡SHHHH≡≡Ξ0	Bbbbbeeeeeeebp = 0 ΞΞΞΞΞΞΞΞEaΞΞΞΞΞΞΞ	RkekekkkkekekP = -1 E9EaE9E900 回回回国目的	BnBEEHEEHHEHBP = -2 EEa∣3EaSBHH0aE≡□ElB3B3B	BSBEBBHEEHBEBp = -3 SQQQE9SSQaB3IQBIE3QaB	BSBEBBHEEHBEBp = -4 HE3E3Qll3EaE≡QE3B3□E3E3SElE9	Figure 3: Left: samples from ‘Rotating MNIST’ experiment with pmin = -4 and a two-layer MLP.
Table 2:	Hyperparameters for the rotating MNIST experiment. (*) The learning rate decays expo-nentially at steps of 30 epochs.
Table 3:	Hyperparameters for the bouncing balls experiment. (*) The learning rate decays exponen-tially at steps of 30 epochs.
Table 4:	Hyperparameters for the convolutional Bouncing balls experiment.
