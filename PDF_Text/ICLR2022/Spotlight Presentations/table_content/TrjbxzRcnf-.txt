Table 4: Average token-level perplexities of each model when trained for 500k steps.
Table 5: Finetuning for 20K steps to make use of a larger memory on the arXiv data set.
Table 8: Examples of memory retrieval in the Isabelle dataset. The model is able to find the definition ofa lemma from a reference to it. The retrieved surrounding context (highlighted) is the definition body of themathematical object highlighted in the querying context.
Table 14: Different layer index.
Table 15: Number of neighbors.
Table 16: Random seeds.
Table 17: The table shows several examples of which tokens were retrieved during language modelling of arXivmath dataset. The model is retrieving names of the references from previous passages.
Table 18: Examples of memory retrieval in the Github dataset. The model looks up how functions are usedelsewhere in the repository.
