Table 1: Results on the regression benchmark datasets.
Table 2: Obj. Detectionwith CDFRMethod	CRPSGaussian	6.85CDFR	6.15Models. We modified the architecture of He et al. (2019) which uses aKL-loss to fit a Gaussian distribution for each bounding box. The existingarchitecture has a MobileNet-v2 backbone (Sandler et al., 2019). Keepingthe backbone unchanged, the quantile regressor method adds an alternatehead with separate layers, which adds uncertainty bounds to the originaloutput. The original Gaussian method was trained for a total of 60 epochs,with a learning rate of 2e-4 which is decremented by a factor of 10 atepochs 30 and 45. The quantile flow layer is trained for an additional 20epochs, with the same learning rate.
Table 3: Results on the object detection taskResults. We implement QFR in conjunction in astandard model, as well as one that uses variational dropout (VD; Gal and Ghahramani (2016)). In	Method	Cal. MAE	MAP@50	CRPSeach case, the QFR layer improves uncertainty	Gaussian	0.133	0.796	8.89estimation (as measured by CRPS; Table 3), with	QFR	0.103	0.796	8.59the best results coming from a model trained with	Gauss. VD	-	0.771	8.73QFR and VD. MAP@50 is defined in the appendix. This suggests our methods improve both aleatoric	QFR VD	-	0.772	7.99and epistemic uncertainties (the latter being estimated by VD). We illustrate examples of predicteduncertainties on real images in Figure 1. We also swapped QFR for CDFR, while keepingapproximately the same architecture, and compared to the Gaussian baseline using the CRPS. CDFRalso outperforms the Gaussian baseline (Table 2).
Table 4: Time series experimentsMethod	ND	RMSE	CHKDeepAR	9.9%	0.692	89.15MQ-RNN	9.9%	0.712	88.91AQF (ours)	9.8%	0.723	88.02accuracy (ND and RMSE are as defined in Salinas et al. (2019), but we have included them in theappendix as well), and AQF produces better uncertainties. We also found that for 37.2% of timeseries, DeepAR and AQF performed comparably for CRPS (CRPS within 5% of each other); on30.3% of time series DeepAR fared better, and on 32.1% AQF did best.
Table 5: CRPS on UCI datasets.
Table 6: Image Generation ExperimentsMethod	CRPS	LL	Q-LoSS	D-LoSSMAF-LL	2.31	-45	0.625	0.978MAF-QL	2.16	-6187	0.281	0.779NAQF	2.14	n/a	0.253	0.723having a considerably worse NLL, the samples generated are still more representative of the data,according to the discriminative model, and visually the sample quality is much higher.
Table 8: Calibration Metrics and AccuracyMetric	AQF	Gaussian.
Table 9: Results on the 1D synthetic data benchmarks.
Table 10: Results on Multidimensional Synthetic DatasetsDataset	CRPS (Marginal)				 CRPS (JOint)					AQF	GauSSian	Quantile	True	AQF	Gaussian	Quantile	True2D	0.624	-0669-	0.634	0.585	0.686	-0701-	0.695-	0.64650D	1.045	1.133	1.058	0.970	1.238	1.345	1.281	1.16315E AlgorithmAlgorithm 1 Training a Neural Network f with the Quantile LossSample α = U(0,1)Sample a training instance (x0, x) from the dataset (or a batch of instances)Compute prediction ypred = f(x0, α)Compute quantile loss Lα (x, xpred).
Table 11: MNIST Generation ExperimentsMethod	U-CRPS	CRPS	NLLPixelMAF-LL	.128	.279	-6011PixelMAF-QL	.099	.215	-5888Results. From the samples in Figure 7, the results generated from the MAF-QL seems to be muchmore visually accurate than MAF-LL, which is trained on a LL-based loss, even though they have thesame architecture.
