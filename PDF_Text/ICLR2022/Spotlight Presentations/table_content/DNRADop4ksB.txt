Table 1: The cross-domain experiments for the DC method with and without Firth bias reduction.
Table A2: The mathematical notations used throughout the paper.
Table A3: Experimental settings used for the standard backbone experiments. The table is partitionedinto 5 sections, where the first section shows the global hyper-parameters used in all standardbackbone experiments. The same set of Firth bias reduction and L2 regularization coefficientswere used for all validation experiments. *The Firth regularization coefficients were chosen forEquation (13) in the main paper. **We defined the L2-regularization as the mean squared value ofall classifier parameters, which is why the normalized set of coefficients seems large. The typicalunnormalized regularization coefficients can be obtained by dividing these normalized coefficients bythe number of classifier parameters.
Table A4: Comparing Firth bias reduction against the confidence penalty label smoothingtechnique. The confidence penalty is defined as a DKL (Pi kU) regularization term, whereas Firthbias reduction for logistic and cosine classifiers reduces to a DKL(UkPi) penalty. The experimentalsetting is the same as Figure A7 with the ResNet-10 backbone. This suggests that the improvementsobtained by Firth are the result of its bias suppression property, and Firth cannot be replaced bystandard label smoothing techniques.
Table A5: The Firth bias reduction accuracy improvements on the tiered-ImageNet datasetwhen 0 or 750 artificial samples were generated from the calibrated normal distribution in Yanget al. (2021).
Table A6: The Firth bias reduction improvements on the CIFAR-FS dataset shown in Figure 4 inthe main paper. “Before” stands for the novel set accuracy without having any Firth bias reduction,and “After” stands for the novel set accuracy after applying Firth bias reduction. Note that theconfidence intervals are much smaller for the improvement column, thanks to the random-effectmatching procedure we used in this study. The “Before” confidence intervals were similar to the“After” confidence intervals, and thus not repeated due to space constraints.
Table A7: The Firth bias reduction improvements on the mini-ImageNet dataset shown inFigure 4 in the main paper. “Before” stands for the novel set accuracy without having any Firth biasreduction, and “After” stands for the novel set accuracy after applying Firth bias reduction. Note thatthe confidence intervals are much smaller for the improvement column, thanks to the random-effectmatching procedure we used in this study. The “Before” confidence intervals were similar to the“After” confidence intervals, and thus not repeated due to space constraints.
Table A8: The Firth bias reduction improvements on the tiered-ImageNet dataset shown inFigure 4 in the main paper. “Before” stands for the novel set accuracy without having any Firth biasreduction, and “After” stands for the novel set accuracy after applying Firth bias reduction. Note thatthe confidence intervals are much smaller for the improvement column, thanks to the random-effectmatching procedure we used in this study. The “Before” confidence intervals were similar to the“After” confidence intervals, and thus not repeated due to space constraints.
Table A9: The Firth bias reduction improvements on the mini-ImageNet dataset shown inFigure 2 in the main paper. “Before” stands for the novel set accuracy without having any Firth biasreduction, and “After” stands for the novel set accuracy after applying Firth bias reduction. Note thatthe confidence intervals are much smaller for the improvement column, thanks to the random-effectmatching procedure we used in this study. The “Before” confidence intervals were similar to the“After” confidence intervals, and thus not repeated due to space constraints. It is worth noting that wedeliberately did not engineer strong features for this experiment (stronger feature backbone resultsare shown in Sections 4.5 and 4.6 in the main paper). This diversifies Firth’s performance portfolio,demonstrating its robustness to the strength of the feature backbones; even with weak features, Firthbias reduction substantially improves the accuracy with high relative improvements as shown hereand in Figure A8.
