Table 1:	Parameter decomposition of the encoder scaling models. The total number of parametersincludes 98M parameters representing the Softmax and embedding layers.
Table 2:	Parameter decomposition of the decoder scaling models. The total number of parametersincludes 98M parameters representing the softmax and embedding layers. Note that the 6L-6L modelis the baseline model We used for hyper-parameter tuning.
Table 3: Parameter decomposition of the symmetric scaling models trained for English→Germantranslation task. The total number of parameters includes 98M parameters representing the softmaxand embedding layers.
Table 4: Variability of final test loss for each test dataset (averaged over all models).
Table 5: Parameter decomposition of the randomly sampled models trained for English→Germantranslation task. The total number of parameters includes 98M parameters representing the softmaxand embedding layers.
Table 6: Description of the models used for alternative scaling.
