Table 1: Results comparison on the ImageNet validation set with the MobileNetV2 (1.0×, 0.75×,0.5×) backbones trained for 150 epochs. For our ODConv, we setr = 1/16. Best results are bolded.
Table 2: Results comparison on the ImageNet validation set with the ResNet18 and ResNet50 back-bones trained for 100 epochs. For our ODConv, we set r = 1/16. * denotes the results are from thepaper of WE (Quader et al., 2020) as its code is not publicly available. Best results are bolded.
Table 3: Results comparison on the ImageNet validation set with the ResNet101 backbone trainedfor 100 epochs. For our ODConv, we set r = 1/16. Best results are bolded.
Table 4: Results comparison on the MS-COCO 2017 validation set. Regarding Params or MAdds,the number in the bracket is for the pre-trained backbone models excluding the last fully connectedlayer, which is almost the same to that shown in Table 1 and Table 2, while the other number is forthe whole object detector. Best results are bolded.
Table 5: Results comparison of the ResNet18 models based on ODConv with different settings ofthe reduction ratio r. All models are trained on the ImageNet dataset. Best results are bolded.
Table 6: Results comparison of the ResNet18 models based on ODConv with different numbers ofconvolutional kernels n. All models are trained on the ImageNet dataset. Best results are bolded.
Table 7: Investigating the complementarity of four types of attentions proposed in ODConv. In theexperiments, when αwi is used, we set n = 4, and otherwise n = 1. For the optimal comparison, weuse the best r setting reported in Table 5. All ResNet18 models are trained on the ImageNet dataset.
Table 8: Comparison of the inference speed (frames per second) for different dynamic convolutionmethods. All pre-trained models are tested on an NVIDIA TITAN X GPU (with batch size 200) anda single core of Intel E5-2683 v3 CPU (with batch size 1) separately, and the input image size is224 × 224 pixels.
Table 9: Results comparison of adding ODConv to different layers of the MobileNetV2 (0.5×)backbone. All models are trained on the ImageNet dataset for 150 epochs, and we set r = 1/16 andn = 1. For the inference speed, all pre-trained models are tested on an NVIDIA TITAN X GPU(with batch size 200) and a single core of Intel E5-2683 v3 CPU (with batch size 1) separately, andthe input image size is 224 × 224 pixels. Best results are bolded.
Table 10: Analyzing the effect of the temperature annealing when training ResNet18 with ODConv.
Table 11: Results comparison of ODConv with different activation functions. All models are trainedon the ImageNet dataset, and we set r = 1/4. Best results are bolded.
Table 12: Results comparison of ODConv with or without using the attention sharing strategy. Allmodels are trained on the ImageNet dataset. Best results are bolded.
Table 13: Results comparison on the ImageNet validation set with MobileNetV2 (1.0×, 0.75×,0.5×) as the backbones. All models are trained for 300 epochs. Best results are bolded.
Table 14: Results comparison on the ImageNet validation set with the ResNet18+SE backbonetrained for 100 epochs. For our ODConv, we set r = 1/16. Best results are bolded.
Table 15: Results comparison of the ResNet18 models based on ODConv with aggressive dataaugmentations and a longer training schedule. All models are trained on the ImageNet dataset. Bestresults are bolded.
Table 16: Comparison of the feature pooling strategy in ODConv using different spatial sizes for thereduced features. All ResNet18 models are trained on the ImageNet dataset. Best results are bolded.
Table 17: Comparison of the training cost for different dynamic convolution methods. All modelsare trained on the ImageNet dataset using the server with 8 NVIDIA TITAN X GPUs. We reportresults in terms of three metrics (seconds per batch, minutes per epoch, and the total number ofhours for the whole training).
