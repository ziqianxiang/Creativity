Table 1: Performance (%) on the GLUE development set by the original BERT (top row) and variousBERT variants with different degrees of self-attention matrix sharing. Numbers in parentheses arethe layers that share the self-attention matrix (e.g., BERT (1-12) means that the A’s from layers 1-12are shared). The 山St ColUmn ShoWs the FLOPs in the self-attention modules._______________________________	MNLI (m/mm)	QQP	QNLI	SST-2	COLA	STS-B	MRPC	RTE	Average	FLOPsBERT	85.4/85.8	88.2	91.5	92.9	62.1	88.8	90.4	69.0	83.8	2.7GBERT (11-12)	84.9/85.0	88.1	91.0	93.0	62.3	89.7	91.1	70.8	84.0	2.4GBERT (9-12)	85.3/85.1	88.1	90.1	92.9	62.6	89.3	91.2	68.5	83.7	2.1GBERT (7-12)	84.2/84.8	88.0	90.6	92.1	62.7	89.2	90.5	68.2	83.4	1.8GBERT (5-12)	84.0/84.3	88.0	89.7	92.8	64.1	89.0	90.3	68.2	83.4	1.5GBERT (3-12)	82.5/82.4	87.5	88.6	91.6	57.0	87.9	88.4	65.7	81.3	1.2GBERT (1-12)	81.3/81.7	87.3	88.5	92.0	57.7	87.4	87.5	65.0	80.9	1.1GFigure 3 shows the cosine similarities obtained. As can be seen, the similarities at the last few layersare high,2 While those at the first feW layers are different from each other. In other Words, the attentionpatterns at the first feW layers are changing, and become stable at the upper layers.
Table 2: Performance (in %) of the various BERT variants on the GLUE development data set.
Table 3: Performance (in %) on the SWAG andSQUAD development sets.
Table 4: Hyper-parameters for different downstream tasks.
