Table 1: Final test MSEs after training UCI and Kin8nm datasets for4 000 full-batch epochs from eachof 200 random hyperparameter initialisations, showing best and bootstrapped average performance.
Table 2: Final test * cross-entropy (^perplexity) on larger datasets. Bold values are the lowest in class.
Table 3: System configurations used to run our experiments.
Table 4: Licences under which we use datasets in this work.
Table 5: Transformation and initialisation parameters used in our hyperparameter optimisation.
Table 6: Repeat of Table 1: final test MSE on selected UCI and Kin8nm datasets. Now includes 32repetitions of 30-sample Bayesian Optimisation, which is not considered in our emboldening.
Table 7:	Partial repeat of Table 2: final test cross-entropy on Fashion-MNIST. Now includes 32repetitions of 30-sample Bayesian Optimisation, which is not considered in our emboldening.
Table 8:	Quantitative analysis of final test errors from Figure 13. Bold values are the lowest in class.
Table 9: QUantitative analysis of final test cross-entropies from FigUre 15 â€” investigating the Use ofbatch normalisation on Fashion-MNIST. Bold valUes are the lowest in class.
Table 10: Quantitative analysis of UCI and Kin8nm final test performance from Figure 16, includingfull-horizon Long Diff-through-Opt on shorter problems as described in Appendix B.7.1. Bold valuesare the lowest in class.
Table 11: Quantitative analysis of Fashion-MNIST final test performance from Figure 16, includingfull-horizon Long Diff-through-Opt on a shorter problem as described in Appendix B.7.1. Bold valuesare the lowest in class.
Table 12: QUantitative analysis of Fashion-MNIST final test performance from FigUre 17, inclUdingMedium Diff-through-Opt with a 200-step look-back horizon as described in Appendix B.7.2. BoldvalUes are the lowest in class.
Table 13: Quantitative analysis of UCI Energy final test performance from Figure 18, including FullDiff-through-Opt with a 4 000-step look-back horizon as described in Appendix B.7.3. Bold valuesare the lowest in class.
Table 14: Final test MSEs after training UCI Energy as in Table 1, with dataset splits modified to theindicated training/validation/test proportions. Uncertainties are standard errors, and the values maybe directly compared to Table 1.
Table 15: Extended reprise of Table 1: final test MSEs after training UCI Energy for 4000 iterations,averaged over 200 initialisations. Also shown are statistics for the initialisations which sUrvive tofUll training on ASHA (oUt of 114 286 considered) and the final models prodUced by PBT (oUt of200 training paths). Uncertainties are standard errors; bold valUes lie in the error bars of the bestalgorithm.
