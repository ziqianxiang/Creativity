Table 1: Median performance on diverse NLP and computer vision tasks: GLUE, object classifi-cation with (Moco v2) and without pretraining (CLS), machine translation (MT), and large-scalelanguage modeling (LM). While 32-bit Adafactor is competitive with 8-bit Adam, it uses almosttwice as much memory and trains slower. 8-bit Optimizers match or exceed replicated 32-bit per-formance on all tasks. We observe no instabilities for 8-bit optimizers. Time is total GPU time onV100 GPUs, except for RoBERTa and GPT3 pretraining, which were done on A100 GPUs.
Table 2: With 8-bit optimizers, larger models can be finetuned with the same GPU memory com-pared to standard 32-bit optimizer training. We use a batch size of one for this comparison.
Table 3: Ablation analysis of 8-bit Adam for small (2 GPU days) and large-scale (≈1 GPU year)transformer language models on the RoBERTa corpus. The runs without dynamic quantization uselinear quantization. The percentage of unstable runs indicates either divergence or crashed trainingdue to exploding gradients. We report median perplexity for successful runs. We can see thatdynamic quantization is critical for general stability and block-wise quantization is critical for large-scale stability. The stable embedding layer is useful for both 8-bit and 32-bit Adam and enhancesstability to some degree.
Table 4: Breakdown of GLUE scores. Each column is the median of 10 random seeds. The mean isthe mean over medians.
Table 5: Runtime performance of 8-bit optimizers vs commonly used 32-bit optimizers in millisec-onds per update per 1B parameters for 32-bit gradients. This comparision was run on a V100 GPU.
Table 6: Mean relative Adam and absolute quantization error for the first Adam state for differentquantization methods. Results show mean±standard error. We can see that Dynamic Quantizationhas best relative error and that both Dynamic methods have the best absolute error.
Table 7: AdaGrad compared to Adam performance for a 209M parameter language model on theRoBERTa corpus. The 8-bit methods use stable embedding layer. AdaGrad hyperparamters aretaken from (Keskar et al., 2019).
