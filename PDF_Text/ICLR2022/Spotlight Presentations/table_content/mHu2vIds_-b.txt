Table 1: CIFAR10 average certified radius (ACR) and certifiedaccuracy at different radii for ensembles of k ResNet110 (k = 1are individual models) at σ = 0.5. Larger is better.
Table 2: ImageNet average certified radius (ACR) and certifiedaccuracy at different radii for ensembles of k ResNet50 (k = 1are individual models) at σ = 1.0. Larger is better.
Table 3: Adaptive Sampling and K-ConSenSuSaggregation on CIFAR10 for 10 CONSISTENCYtrained ResNet110.
Table 4: Adaptive sampling for different {nj} on CIFAR10 at σ = 0.25. SampleRF and TimeRFare the reduction factors compared to standard sampling with n = 1000000 (larger is better). ASRj isthe portion of certification attempts returned in phase j .
Table 5: K-Consensus aggregation atσ = 0.25 on CIFAR10.
Table 7: Reference training times for individual models on GeForce RTX2080 TisDataset	Training	Architecture	#GPUs	time per Epoch [s]	total time [h]	Gaussian	ResNet110	1	41.8	1.74		ResNet20	1	9.6	0.40CIFAR10	Consistency	ResNet110	1	79.5	3.31		ResNet20	1	18.5	0.77	SMOOTHADV (σ = 0.25)	ResNet110	1	2471	102.96	SMOOTHADV (σ ∈ {0.5, 1.0})	ResNet110	1	660	27.5ImageNet	Consistency	ResNet50	4	3420	85G.5 Experiment TimingsTo evaluate the ensembles of ResNet20 and ResNet110 on CIFAR10, we use single GeForce RTX2080 Ti, and to evaluate ensembles of ResNet50 on ImageNet, we use double 2080 Tis.
Table 8: Average certified radius (ACR) and certified accuracy at various radii for ensembles of kmodels (k = 1 are single models) for a various training methods and model architectures on CIFAR10.
Table 9: Average certified radius (ACR) and certified accuracy at various radii for ensembles of kmodels (k = 1 are single models) for a various training methods on ImageNet. Larger is better.
Table 10: Comparison of various aggregation methods for ensembles of 10 ResNet110 at σ = 0.25on CIFAR10. Larger is better.
Table 11: Effect of ensemble size k on ACR and certified accuracy at different radii, for GAUSSIANand Consistency trained ResNet20. Larger is better.
Table 12: Mean and standard deviation of ACR and certified accuracy for CIFAR10 at various radiifor 50 ResNet20 with different random seeds either evaluated as 10 ensembles with k = 5 or asindividual models at σ = 0.25.
Table 13: Comparing and ensemble and an individual model at an equal number of total inferenceson CIFAR10: average certified radius (ACR) and certified accuracy at various radii for ensembles ofk models (k = 1 are single models) for a various sampling sizes n. Larger is better.
Table 16: Denoised smoothing en-sembles on CIFAR10 at σ = 0.25k	ACR	Radius r					0.0	0.25	0.50	0.751	0.378	76.4	56.6	36.8	19.24	0.445	75.2	62.2	45.8	29.028Published as a conference paper at ICLR 2022Table 14: Comparing and ensemble and an individual model at an equal number of total inferenceson ImageNet: average certified radius (ACR) and certified accuracy at various radii for ensembles ofk models (k = 1 are single models) for a various sampling sizes n. Larger is better.
Table 14: Comparing and ensemble and an individual model at an equal number of total inferenceson ImageNet: average certified radius (ACR) and certified accuracy at various radii for ensembles ofk models (k = 1 are single models) for a various sampling sizes n. Larger is better.
Table 15: Building ensembles with differently trained base models: average certified radius (ACR)and certified accuracy at various radii for ensembles and their constituting models. Larger is better.
Table 17: Effect of ensemble size k on ACR and certified accuracy at different radii for l0 norm onMNIST. Larger is better.
Table 18: Effect of ensemble size k on ACR and certified accuracy at different radii for l0 norm onImageNet. Larger is better.
Table 19: Effect of ensemble size k on ACR and certified accuracy at different radii for l1 norm onCIFAR10 for models with ResNet20 architecture. Larger is better.
Table 20: Adaptive sampling ({nj } = {100, 10000, 100000, 1200000}) and 5-Consensus agg. onCIFAR10 for 10 CONSISTENCY ResNet110. SampleRF and TimeRF are the reduction factors incomparison to standard sampling with n = 1000000 (larger is better). ASRj is the % of certificationattempts returned in phase j . KCR is the % of inputs for which only K classifiers were evaluated.
Table 21: Adaptive sampling with {nj} = {100, 10000, 100000, 1200000} and K-consensus earlystopping with K = 5 on CIFAR10 with an ensemble of 10 GAUSSIAN trained ResNet110. Sampleand time gain are the reduction factor in comparison to standard sampling with n = 1000000 (largeris better). ASRj is the percentage of samples returned in phase j . KCR is the percentage of samplesfor which only K classifiers were evaluated.
Table 22: Effect of adaptive sampling with {nj} = {100, 10000, 100000, 1200000} for ensembles of10 GAUSSIAN respectively CONSISTENCY trained ResNet110 on CIFAR10 with σ = 0.25. ASAjand ASCj are the percentages of samples abstained from and certified in phase j , respectivelyRadius	Training	acccert [%]	ASA1	ASC1	ASA2	ASC2	ASA3	ASC3	ASA4	ASC4	SampleRF	TimeRF	GAUSSIAN	70.6	62.4	13.2	10.0	7.6	3.8	1.2	0.4	1.4	30.48	31.300.25	CONSISTENCY	70.4	73.6	10.4	7.0	4.8	3.0	0.6	0.0	0.6	66.73	67.93	GAUSSIAN	42.2	0.0	39.0	0.0	9.8	37.8	4.8	5.6	3.0	6.16	6.320.75	CONSISTENCY	52.0	0.0	27.4	0.0	8.8	51.8	3.4	4.8	3.8	5.68	5.83Table 23: Adaptive sampling with {nj} = {100, 10000, 100000, 1200000} on ImageNet with anensemble of 3 CONSISTENCY trained ResNet50. SampleRF and TimeRF are the reduction factorin comparison certification with CERTIFY with n = 1000000. ASRj is the percentage of samplesreturned in phase j .
Table 23: Adaptive sampling with {nj} = {100, 10000, 100000, 1200000} on ImageNet with anensemble of 3 CONSISTENCY trained ResNet50. SampleRF and TimeRF are the reduction factorin comparison certification with CERTIFY with n = 1000000. ASRj is the percentage of samplesreturned in phase j .
Table 24:	Effect of adaptive sampling for individual ResNet110 on CIFAR10 at σ = 0.25. ASAjand ASCj are the percentages of samples abstained from and certified in phase j, respectively. Wecompare multiple sampling count progressions {nj }.
Table 25:	Effect of K-Consensus aggregation on CONSISTENCY trained ensembles of 10 ResNet110and 50 ResNet20 on CIFAR10 at σ = 0.25.
