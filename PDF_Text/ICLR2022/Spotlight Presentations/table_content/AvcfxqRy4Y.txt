Table 1: Phoneme index for the analysis. Phonemes are extracted from the LibriSpeech lexicon andcollapsed into 36 classes. Phonemes are reordered according to their phonological properties.
Table 2: Word error rate (%) for different attention map reuse configurations. “HX” indicates thatthe number of attention heads in the self-attention layer is set to X. All configurations carry almostthe same number of parameters. No external language model is used.
Table 3: Effect of different configurations on speed. The numbers inside of the parentheses indicatethe speed-up ratio. The front convolutional sub-sampling is not included. Changing the number ofheads does not make much difference to the speed.
Table 4: Conformer-M implementation details.
Table 5: Training details including optimizer, scheduler, augmentation and other hyper-parameters.
Table 6: Comparison of the word error rate between the proposed attention reuse and the maskedattention. “-” indicates that the attention range is not restricted (unlimited).
