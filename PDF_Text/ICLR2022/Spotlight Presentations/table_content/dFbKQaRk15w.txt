Table 1: TUDatasets. Gray background indicates that ESAN outperforms the base encoder. SoTAline reports results for the best performing model for each dataset.
Table 2: Test results for OGB datasets. Gray back-ground indicates that ESAN outperforms the baseencoder.
Table 3: ZINC12k dataset. Gray background in-dicates that ESAN outperforms the base encoder.
Table 4: TUDatasets. The top three are highlighted by First, Second, Third. Gray backgroundindicates that ESAN outperforms the base encoder.
Table 5: Results for ogbg-molhiv and ogbg-moltox2 1. Gray background indicates that ESANoutperforms the base encoder. Note that ESAN achieves much higher training accuracies with re-spect to base graph encoders.
Table 6: Results for RNI datasets. Gray background indicates that ESAN outperforms the base en-coder. DS/DSS-GNN can boost the performance of both base graph encoders, GIN and GraphConv,from random to perfect.
Table 7: Results of our stochastic sampling approach on TUDatasets, where each graph sees 100%,50%, 20%, 5% of the subgraphs in the selected policy (100% corresponds to full bags). Graybackground indicates that ESAN outperforms the base encoder.
Table 8: Results of our stochastic sampling approach on OGB and RNI datasets, where each graphsees 100%, 50%, 20%, 5% of the subgraphs in the selected policy. Gray background indicates thatESAN outperforms the base encoder.
Table 9: Number of parameters of the best models		MUTAG	PTC	PROTEINS	NCI1	NCI109	IMDB-B	IMDB-MGIN (Xu et al., 2019)	8296	8692	8164	9286	9319	39305	40091ESAN (GIN)	18625	5601	10865	47137	20481	33729	26883C.3 Additional experimentsC.3. 1 Stochastic Sampling resultsWe performed our stochastic sampling using GIN as a base encoder. We considered three differentsubgraph sub-sampling ratios: 5%, 20% and 50%. While training, we randomly sampled a subset ofsubgraphs for each graph in each epoch. During evaluation, we performed majority voting using 5different subgraph sampling sets for each graph. We performed hyper parameter tuning as detailedin Appendix C.2, and compared the performances of the sub-sampling method both to the baseencoder, and to the corresponding approach without the sub-sampling procedure (reported as 100%).
Table 10: Results with larger number of trainable weights	MUTAG	PROTEINS	NCI1	NCI109	 GIN (Xu et al., 2019) BIG-GIN	89.4±5.6 89.9±4.9	76.2±2.8 75.8±5.5	82.7±1.7 82.9±1.8	82.2±1.6 81.6±1.5ESAN (GIN)	91.1±7.0	77.1±4.6	83.8±2.4	83.1±0.8Table 11: Timing comparison per epoch on a RTX 2080 GPU. Time taken for a single epoch withbatch size 32 on the NCI1 dataset and with batch size 128 on the ZINC dataset. All values are inseconds.
Table 11: Timing comparison per epoch on a RTX 2080 GPU. Time taken for a single epoch withbatch size 32 on the NCI1 dataset and with batch size 128 on the ZINC dataset. All values are inseconds.
Table 12: 1-WL on an edge-deleted subgraph of the Rook’s graph, where we delete edge (1,2). Thetable on the right shows M(v), which is a tuple containing the current color of node v along withthe multiset of neighbor colors. In each iteration of 1-WL, M(v) is updated to HASH(M(v)).
Table 13: 1-WL on an edge-deleted subgraph of the Shrikhande graph, where we delete edge (1,2).
Table 14: Complexity of graph networks that are more expressive than 1-WL. ∆max denotes themaximum degree over all nodes.
