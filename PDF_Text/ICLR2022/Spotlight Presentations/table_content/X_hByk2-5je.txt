Table 1: An (incomplete) summary of our empirical results. “Comp.” stands for compression.						Method	MNIST (10,000 test images)			Method	ImageNet32	ImageNet64	Theoretical bpd	Comp. bpd	En- & decoding time		Theoretical bpd	Theoretical bpdPC (small)	1.26	130	53	PC+IDF	3.99	3.71PC (large)	1.20	1.24	168	IDF	4.15	3.90IDF	1.90	196	880	RealNVP	4.28	3.98BitSwap	1.27	1.31	904	Glow	4.09	3.813.1	Background: Probabilistic CircuitsProbabilistic Circuits (PCs) are an umbrella term for a widevariety of Tractable Probabilistic Models (TPMs). They pro-vide a set of succinct definitions for popular TPMs such asSum-Product Networks (Poon & Domingos, 2011), ArithmeticCircuits (Shen et al., 2016), and Probabilistic Sentential Deci-sion Diagrams (Kisa et al., 2014). The syntax and semantics ofa PC are defined as follows.
Table 2: Efficiency and optimality of the (de)compressor. The compression (resp. decompression)time are the total computation time used to encode (resp. decode) all 10,000 MNIST test samples ona single TITAN RTX GPU. The proposed (de)compressor for structured-decomposable PCs is 5-40xfaster than IDF and BitSwap and only leads to a negligible increase in the codeword bpd compared tothe theoretical bpd. HCLT is a PC model that will be introduced in Sec. 4.1.
Table 3: Compression performance of PCs on MNIST, FashionMNIST, and EMNIST in bits-per-dimension (bpd). For all neural compression algorithms, numbers in parentheses represent thecorresponding theoretical bpd (i.e., models, test-set likelihood in bpd).
