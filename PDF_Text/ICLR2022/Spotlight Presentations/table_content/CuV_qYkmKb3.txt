Table 1: Results using the fully labeled training data, only 25% of the labeled training data, and thefull training data subject to 30% label noise. Shown is the average relative gain in accuracy whenadding the pre-training methods (columns) to the reference methods (rows). Like the box-plots, wefilter out datasets using p-value 0.20. We see that SCARF consistently outperforms alternatives, notonly in improving control but also in improving methods designed specifically for the setting.
Table 2: Absolute test accuracy (percent; averaged over 30 trials) for the 100% training data setting,for each dataset and every baseline, including a gradient-boosted decision tree (XGBoost) baseline(max depth of 3, 100 estimators).
Table 3: Absolute test accuracy (percent; averaged over 30 trials) for the 30% label noise setting, foreach dataset and every baseline, including a gradient-boosted decision tree (XGBoost) baseline (maxdepth of 3, 100 estimators).
Table 4: Absolute test accuracy (percent; averaged over 30 trials) for the 25% training data (semi-supervised) setting, for each dataset and every baseline, including a gradient-boosted decision tree(XGBoost) baseline (max depth of 3, 100 estimators).
Table 5: Number of actual training epochs used (averaged over 30 trials) for the 100% training datasetting, for each dataset and every baseline.
