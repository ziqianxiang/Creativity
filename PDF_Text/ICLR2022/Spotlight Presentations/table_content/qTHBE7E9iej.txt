Table 1: Analysis of zero-shot exploration at the startof RL, in terms of reward and state coverage (vari-ance over an episode of different subsets of the agentâ€™sstate). Results are averaged over 1000 episodes.
Table 3: Details of state variables used by the agent.
Table 4: Action space details for the Sawyer environment.
Table 5: Hyperparameters and architecture details for HeLMS, for both offline training and RL.
