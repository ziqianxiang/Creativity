Table 1: Number of parameters, NTK condition number κ, Hessian dominate eigenvalue λmax ,training error at convergence Ltrain , average flatness LtNrain , accuracy on ImageNet, and accu-racy/robustness on ImageNet-C. ViT and MLP-Mixer suffer divergent κ and converge at sharp re-gions; SAM rescues that and leads to better generalization.
Table 2: Performance of ResNets, ViTs, and MLP-Mixers trained from scratch on ImageNet withSAM (improvement over the vanilla model is shown in the parentheses). We use the Inception-stylepreprocessing (with resolution 224) rather than a combination of strong data augmentations.
Table 3: Accuracy and robustness of two hybridarchitectures.
Table 4: Dominant eigenvalue λmax of the sub-diagonal Hessians for different network components,and norm of the model parameter w and the post-activation ak of block k. Each ViT block consistsof a MSA and a MLP, and MLP-Mixer alternates between a token MLP a channel MLP. Shallowerlayers have larger λmax . SAM smooths every component.
Table 5: Data augmentations, SAM, and their combination applied to different model architecturestrained on ImageNet and its subsets from scratch.
Table 7: Comparison under the adversarial training framework on ImageNet (numbers in the paren-theses denote the improvement over the standard adversarial training without SAM). With similarmodel size and throughput, ViTs-SAM can still outperform ResNets-SAM for clean accuracy andadversarial robustness.
Table 8: Specifications of the ViT and MLP-Mixer architectures used in this paper. We train all thearchitectures with image resolution 224 × 224.
Table 9: Hyperparameters for downstream tasks. All models are fine-tuned with 224 × 224 resolu-tion, a batch size of 512, cosine learning rate decay, no weight decay, and grad clipping at globalnorm 1.
Table 10: Accuracy on downstream tasks of the models pre-trained on ImageNet. SAM improvesViTs and MLP-Mixers’ transferabilities. ViTs transfer better than ResNets of similar sizes.
Table 11: The SAM perturbation strength ρ for training on ImageNet. ViTs and MLP-Mixers favorlarger ρ than ResNets does. Larger models with longer patch sequences need stronger strengths.
Table 12: Hyperparameters for training from scratch on ImageNet with basic Inception-style pre-processing and 224 × 224 image resolution.
Table 13: ImageNet top-1 accuracy (%) of ViT-B/16 and Mixer-B/16 when trained from scratchwith different perturbation strength ρ in SAM.
Table 14: ImageNet accuracy and curvature analysis for ViT-B/16 when we vary the weight decaystrength in Adam (AdamW).
