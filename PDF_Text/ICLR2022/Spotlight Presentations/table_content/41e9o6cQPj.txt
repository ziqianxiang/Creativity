Table 1: Examples of the MCQA task for each of the datasets evaluated in this work.
Table 2: Performance comparison on Commonsense QA in-house split (controlled experiments).
Table 3: Test Accuracy comparison on OpenBook QA. Experiments are		Table 4: Test accuracy comparison to public Open-				BookQA model implementations.	*UnifiedQA (11B	controlled using the same	seed LM	params) and T5 (3B) are 30x and 8x larger than our model.		for all LM+KG methods.		Model	Acc.	# ParamsModel	Acc.	ALBERT (Lan et al., 2020) + KB	81.0	〜235M—		HGN (Yan et al., 2020)	81.4	≥355MAristoRoBERTa (no KG)	78.4	AMR-SG (Xu et al., 2021)	81.6	〜361M+ RGCN	74.6	ALBERT + KPG (Wang et al., 2020)	81.8	≥235M+ GconAttn	71.8	QA-GNN (Yasunaga et al., 2021)	82.8	〜360M+ RN	75.4	T5* (Raffel et al., 2020)	83.2	〜3B+ MHGRN	80.6	T5 + KB (Pirtoaca)	85.4	≥11B+ QA-GNN	82.8	UnifiedQA* (Khashabi et al., 2020)	87.2	〜11BGREASELM (Ours)	84.8	GreaseLM (Ours)	84.8	〜359MLarge (Liu et al., 2019) for CommonsenseQA, and AristoRoBERTa2 (Clark et al., 2019) for Open-bookQA. For MedQA-USMLE, we use a state-of-the-art biomedical language model, SapBERT (Liuet al., 2021), which is an augmentation of PubmedBERT (Gu et al., 2022) that is trained with entitydisambiguation objectives to allow the model to better understand entity knowledge.
Table 5: Performance of GREASELM on the CommonsenseQA IH-dev set on complex questionswith semantic nuance such as prepositional phrases, negation terms, and hedge terms.
Table 6: Performance on MedQA-USMLEMethods	Acc. (%)Baselines (Jin et al., 2021)	Chance	25.0PMI	31.1IR-ES	35.5IR-Custom	36.1clinicalBERT-Base	32.4BioRoBERTa-Base	36.1BioBERT-Base	34.1BioBERT-Large	36.7Baselines (Our implementation)	SapBERT-Base (w/o KG)	37.2QA-GNN	38.0GREASELM (Ours)	38.52020). While these results are promising as they suggest that GreaseLM is an effective aug-mentation of pretrained LMs for different domains and KGs (i.e., the medical domain with the DDB+ Drugbank KG), there is still ample room for improvement on this task.
Table 7: Hyperparameter settings for models and experiments				Category	Hyperparameter	Dataset				CommonsenseQA	OpenbookQA	MedQA-USMLE	Number of GREASELM layers M	5	6	3	Number of Unimodal LM layers N	19	18	9Model architecture	Number of attention heads in GNN	2	2	2	Dimension of node embeddings and the messages in GNN	200	200	200	Dimension of MLP hidden layers (except MInt operator)	200	200	200	Number of hidden layers of MLPs	1	1	1	Dimension of MInt operator hidden layer	400	200	400Regularization	Dropout rate of the embedding layer, GNN layers and fully-connected layers	0.2	0.2	0.2	Learning rate of parameters in LM	1.00E-05	1.00E-05	5.00E-05	Learning rate of parameters not in LM	1.00E-03	1.00E-03	1.00E-03Optimization	Number of epochs in which LM’s parameters are kept frozen	4	4	0	Optimizer	RAdam	RAdam	RAdam	Learning rate schedule	constant	constant	constant	Batch size	128	128	128	Number of epochs	30	70	20	Max gradient norm (gradient clipping)	1.0	1.0	1.0Data	Max number of nodes	200	200	200
Table 8: Ablation study of our model components, using the CommonsenseQA IH-dev set.
Table 9: Performance on the in-house splits of Com-monsenseQA for different LM initializations of ourmethod, GreaseLM.
Table 10: Initialization on MedQA-USMLEMethods	Acc. (%)SAPBERT-BASE	37.2+ GREASELM (Ours)	38.5Biobert-Base	34.1+ GREASELM (Ours)	34.6PUBMEDBERT-BASE	38.0+ GREASELM (Ours)	38.7To evaluate whether our method is agnostic to the LM used to seed the GreaseLM layers, we replacethe LMs we use in previous experiments (RoBERTa-large for CommonsenseQA and SapBERT forMedQA-USMLE) with RoBERTa-base for CommonsenseQA, and BioBERT and PubmedBERTfor MedQA-USMLE. Across multiple LM initializations in two domains, our results demonstratethat GreaseLM can provide a consistent improvement for multiple LMs when used as a modalityjunction between KGs and language.
