Table 1: Results in G-FL accuracy and P-FL accuracy (%). ?: methods with no G-FL models and we combinetheir P-FL models. ยง: official implementation. Blue/bold fonts highlight the best baseline/our approach.
Table 2: Ablation study on vari-ants of Fed-RoD. FT: fine-tuningTeStSet	IG-FLl P-FL	MethOd/Model	GM IGM PMCentralized	85.4 85.4 -FedAvg FEDAVG (BRM) FEDAVG (BRM, FT) FED-ROD (linear, BRM) FED-ROD (hyper, BRM)	68.6 69.4 85.1 76.8 76.7 76.1 76.8 76.7 84.5 76.9 76.8 86.4 76.9 76.8 86.8Table 3: FED-ROD with dif-ferent balanced losses. ?: BSMTeStSet	IG-FLl P-FL	Loss/Model	GM IGM PMCross entropy (Hsu et al., 2020) (Cao et al., 2019) (Ye et al., 2020) (ene .,20	)?	68.6 69.4 85.1 65.8 65.8 80.1 75.7 75.9 83.3 75.2 75.0 85.1 76.9 76.8 86.8MethodIFMNIST CIFAR-10 CIFAR-100Table 4: P-FL accuracy on future non-IIDclients (Dir(0.3) for all datasets). Each cellis before/after local fine-tuning.
Table 3: FED-ROD with dif-ferent balanced losses. ?: BSMTeStSet	IG-FLl P-FL	Loss/Model	GM IGM PMCross entropy (Hsu et al., 2020) (Cao et al., 2019) (Ye et al., 2020) (ene .,20	)?	68.6 69.4 85.1 65.8 65.8 80.1 75.7 75.9 83.3 75.2 75.0 85.1 76.9 76.8 86.8MethodIFMNIST CIFAR-10 CIFAR-100Table 4: P-FL accuracy on future non-IIDclients (Dir(0.3) for all datasets). Each cellis before/after local fine-tuning.
Table 4: P-FL accuracy on future non-IIDclients (Dir(0.3) for all datasets). Each cellis before/after local fine-tuning.
Table 5: Balanced risk and loss functions. We ignore the normalization in Lm (w). Red highlights themodifications by the balanced losses. Blue highlights the terms learned with meta-learning (see subsection B.5).
Table 6: # of parameters in ConvNets for EMNIST/FMNIST and CIFAR-10/100Module	EMNIST/FMNIST	CIFAR-10	CIFAR-100Feature extractor	92,646	1,025,610	1,025,610Generic head	500	640	6400Total	93,146	1,026,250	1,032,010Hypernetworks	8,160 (+8.8%)	20,800 (+2.0%)	104,000 (+10.0%)One may wonder what if the global distribution is class-imbalanced? Will BRM still be beneficial toFL? In subsection D.5, we perform experiments to show that Fed-RoD with BRM can still improveon FedAvg since BRM seeks to learn every class well. Even though the global distribution might beskewed, BRM provides a novel alternative to mitigate the non-IID problem by making every clientoptimize a more consistent objective as we discussed above. Designing better losses for BRM in FLwill be interesting future work.
Table 7: EMNIST and FMNIST results in G-FL accuracy and P-FL accuracy (%). ?: methods with no G-FLmodels and we combine their P-FL models. ยง: official implementation.
Table 8: CIFAR-10 results in G-FL accuracy and P-FL accuracy (%). ?: methods with no G-FL models and wecombine their P-FL models. ยง: official implementation.
Table 9: CIFAR-100 results in G-FL accuracy and P-FL accuracy (%). ?: methods with no G-FL models andwe combine their P-FL models. ยง: official implementation.
Table 10: Class-imbalanced global training distribution. ? : methods with no global models and we combinetheir P-FL models. Gray rows: meta-learning with 100 labeled server data.
Table 11: G-FL accuracy on class-imbalanced test data. Here we use CIFAR-10 Dir(0.3).
Table 12: Main results in G-FL accuracy and P-FL accuracy (%), following Table 1 of the main paper.
Table 13: The P-FL accuracy by the two local models of DITTO (Li et al., 2021a).
Table 14: DITTO with adversary attacks. We report the averaged personalized accuracy on benign clients.
Table 15: FED-ROD on CIFAR-10, Dir(0.3).
Table 16: Main results in G-FL accuracy and P-FL accuracy (%), following Table 1 of the main paper.
