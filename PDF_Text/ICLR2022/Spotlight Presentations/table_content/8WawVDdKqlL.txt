Table 1: Benchmarks used for evaluationTask	Feature Extractor	Specialized Approach	Dataset	Benchmark	Label range/ Quantization levels	θ	ResNet50	Regression+classification	BIWI	HPE1	-100-100/200	10Landmark-free 2D head		(Ruiz et al., 2018)	300LP/AFLW2000	HPE2	-100-100/200	10pose estimation	RAFANet	Direct regression	BIWI	HPE3	-180-180/360	50		(Beheraet al.,2021)	300LP/AFLW2000	HPE4~~	-180-180/360	50		Heatmap regression (Wang et al., 2020; Xu et al., 2020)	COFW	-_FLD1	0-256/256	10Facial Landmark	HRNetV2-		300W	-_FLD2	0-256/256	10Detection	W18		WFLW	-_FLD3	0-256/256	10			AFLW	-~FLD4~~	0-256/256	30Age estimation	ResNet50	Ordinal regression	MORPH-II	AEI	0-64/64	10	/ResNet34	(Cao et al.,2020)	AFAD	AE2	0-32/32	10End-to-end autonomous driving	PilotNet	Direct regression (Bojarski et al., 2017)	PilotNet	PN	0-670/670	10Training loss functions: A deep neural network with multiple output binary classifiers can betrained using the binary cross-entropy (BCE) loss LBCE(Zi, E(Qi)). However, this loss minimizesthe mismatch between predicted and target code but does not directly minimize the error between thetarget and predicted values. Decoding functions DGEN and DGEN-EX can be used to calculate the lossand minimize the mismatch between decoded predictions and target values directly. Decoding func-tion DGEN finds the correlation between each row of the code matrix (C" and the output Z%. CZigives the correlation vector, and the index with the highest correlation is used as the predicted label. In
Table 2: Comparison of BEL with different regression approaches. “Specialized approach” described in Table 1Error (MAE or NME) / Model size				Approach	HPE1	HPE2	HPE3	HPE4Specialized approach	-	-	3.40 / 69.8M	4.14 / 69.8MDirect regression	4.76 ±0.35 / 23.5M	-5.65 ±0.13∕23.5M-	3.40 ±0.26 / 69.8M	4.14 ±0.12 / 69.8MMulticlass classification	4.49 ±0.24 / 24.2M	-5.31 ±0.05∕24.8M-	4.54 ±0.04 / 72.0M	5.14 ±0.08 / 72.0MBEL	3.56±0.01 / 23.6M	-4.77 ±0.05/23.6M-	3.30±0.04 / 69.8M	3.90 ±0.03 / 69.8MBEL E /D/L functions	U/GEN-EX/L2	-U/GEN-EX/BCE	B1JDJ/GEN-EX/BCE	U/GEN-EX/BCEApproach	FLD1	FLD2	FLD3	FLD4Specialized approach	3.45 / 9.6M	3.32 / 9.6M	4.32 / 9.6M	1.57 / 9.6MDirect regression	3.60 ±0.02 / 10.2M	-3.54 ±0.03/10.2M-	4.64 ±0.03 / 10.2M	1.51 ±0.01 / 10.2MMulticlass classification	3.58 ±0.03 / 25.4M	-3.51 ±0.02/45.2M-	4.50 ±0.01 / 61.3M	1.56 ±0.01 / 20.1MBEL	3.34 ±0.02 / 10.6M	-3.40 ±0.02/11.2M-	4.36 ±0.02 / 11.7M	1.47 ±0.00 / 10.8MBEL E /D/L functions	HEXJ/GEN-EX/CE	-U/GEN-EX/CE	B1JDJ/GEN-EX/CE	B1JDJ/GEN-EX/CEApproach	AE1	AE2	PN	Specialized approach	2.49 / 21.3M	3.47/21.3M	4.24 / 1.8M	Direct regression	2.44 ±0.01 / 23.1M	-3.21 ±0.02/23.1M-	4.24 ±0.45 / 1.8M	Multiclass classification	2.75 ±0.03 / 23.1M	-3.38 ±0.05/23.1M-	5.54 ±0.00 / 1.9M	BEL	2.27 ±0.01 / 23.1M	-3.11 ±0.00/23.1M-	3.11 ±0.01 / 1.8M	BEL E /D/L functions	J/BEL-J/BCE	-B1JDJ/GEN-EX/L1 ^^	J/GEN/CE	
Table 3: Comparison of BEL design parameters on MAE for head pose estimation with BIWI dataset andResNet50 feature extractor (HPE1).
Table 4: Comparison of BEL design parameters on MAE for head pose estimation with 300LP/AFLW2000datasets and ResNet50 feature extractor (HPE2).
Table 5: Comparison of BEL design parameters on MAE for head pose estimation with BIWI dataset andRAFA-Net feature extractor (HPE3).
Table 6: Comparison of BEL design parameters on MAE for head pose estimation with 300LP/AFLW2000datasets and RAFA-Net feature extractor (HPE4).
Table 7: Comparison of BEL design parameters on NME for facial landmark detection with COFW dataset andHRNetV2-W18 feature extractor (FLD1).
Table 8: Comparison of BEL design parameters on NME for facial landmark detection with 300W dataset andHRNetV2-W18 feature extractor (FLD2).
Table 9: Comparison of BEL design parameters on NME for facial landmark detection with WFLW dataset andHRNetV2-W18 feature extractor (FLD3).
Table 10: Comparison of BEL design parameters on NME for facial landmark detection with AFLW dataset andHRNetV2-W18 feature extractor (FLD4).
Table 11: Comparison of BEL design parameters on MAE for age estimation with MORPH-II dataset andResNet50 feature extractor (AE1).
Table 12: Comparison of BEL design parameters on MAE for age estimation with AFAD dataset and ResNet50feature extractor (AE2).
Table 13: Comparison of BEL design parameters on MAE for end-to-end autonomous driving with PilotNetdataset and feature extractor (PN).
Table 14: Impact of the quantization and decoding functions on NME for facial landmark detection.
Table 15: Effect of training dataset size on optimal encoding function for facial landmark detection. BCE lossfunction and GEN-EX decoding function are used for the training and evaluation.
Table 16: Effect of reflected binary conversion for B1JDJ encoding on facial landmark detection. Here, BCEloss and GEN-EX decoding functions are used.
Table 17: Comparison of BEL with heatmaps for facial landmark detection. Here, BCE loss and GEN-EXdecoding functions are used.
Table 18: Impact increasing number of fully connected layers in direct regression and multiclass classificationon the error (MAE or NME).
Table 19: Impact increasing number of fully connected layers in direct regression, multiclass classification, andBEL. GEN-EX decoding function and BCE loss function are used for BEL.
Table 20: Comparison of BEL with different regression approaches. Specialized approaches for each benchmarkare described in Table 1Error (MAE or NME) / Model size				Approach (% training set)	HPE1	HPE2	HPE3	HPE4Specialized approach (100%)	-	-	3.40 / 69.8M	4.14 / 69.8MSpecialized approach (80%)	-	-	4.08±0.11 / 69.8M	4.69±0.02 / 69.8MDirect regression (80%)	6.12±0.02 / 23.5M	^^5.97±0.09/23.5M~~	4.08±0.11 / 69.8M	4.67+4.70 / 69.8MMulticlass classification (80%)	5.38±0.03 / 24.2M	^^5.60±0.13/24.8M~~	5.58±0.04 / 72.0M	5.86±0.10 / 72.0MBEL (80%)	3.91±0.08 / 23.6M	^^4.91±0.10/23.6M~~	3.50±0.08 / 69.8M	3.99±0.04 / 69.8MBEL E/D/L functions	U/GEN-EX/L2	^^U/GEN-EX/BCE	B1JDJ/GEN-EX/BCE	U/GEN-EX/BCEApproach (%training set)	FLD1	FLD2	FLD3	FLD4Specialized approach (100%)	3.45 / 9.6M	3.32 / 9.6M	4.32 / 9.6M	1.57 / 9.6MDirect regression (80%)	3.70±0.04 / 10.2M	^^3.69±0.06/10.2M~~	4.71±0.02 / 10.2M	1.51±0.01 / 10.2MMulticlass classification (80%)	3.64±0.02 / 25.4M	^^3.68±0.02/45.2M~~	4.77±0.02 / 61.3M	1.56 ±0.01 / 20.1MBEL (80%)	3.35±0.02 / 10.6M	^^3.40±0.03/11.2M~~	4.37±0.01 / 11.7M	1.48±0.01 / 10.8MBEL E/D/L functions	HEXJ/GEN-EX/CE	^^U/GEN-EX/CE	B1JDJ/GEN-EX/CE	B1JDJ/GEN-EX/CEApproach (% training set)	AE1	AE2	PN	Specialized approach (100%)	2.49 / 21.3M	3.47/21.3M	4.24 / 1.8M	Direct regression (80%)	2.45 ±0.01 / 23.1M	^^3.34 ±0.02/23.1M-	4.56 ±0.45 / 1.8M	Multiclass classification (80%)	2.85 ±0.03 / 23.1M	^^3.47 ±0.05/23.1M-	6.37 ±0.00 / 1.9M	
Table 21: Training parameters for head pose estimation with protocol 1.
Table 22: Training parameters for head pose estimation with protocol 2.							Approach	Label range/Quantization levels	Optimizer	Epochs	Batch size	Learning rate	Learning rate schedule	Training time (GPU hours)HPE2	[—99。, 99。]/200	Adam, weight decay=0, momentum = 0	20	16	0.00001	1/10 after 10 Epochs	4HPE4	[—179。, 180。]/360	RMSProp, momentum=0, rho = 0.9	100	16	0.001	-	48Related work Existing approaches for head pose estimation include stage-wise soft regres-sion (Yang et al., 2018; fsa, 2019), a combination of classification and regression (Mukherjee& Robertson, 2015; Ruiz et al., 2018), and ordinal regression (Hsu et al., 2019). SSR-Net (Yang et al.,28Published as a conference paper at ICLR 20222018) proposes the use of stage-wise soft regression to use the softmax values of classification outputto refine the label. FSA-Net (fsa, 2019) proposes extending stage-wise estimation to head poseestimation using feature aggregation. HopeNet (Ruiz et al., 2018) uses a combination of classificationand regression loss to train a model for head pose estimation. Whereas, QuatNet (Hsu et al., 2019)proposes a combination of L2 loss and a custom ordinal regression loss. RAFA-Net (Behera et al.,2021) uses an attention based approach for feature extraction with direct regression.
Table 23: Landmark-free 2D Head poses estimation evaluation for protocol 1 (HPE1 and HPE3).
Table 24: Landmark-free 2D Head poses estimation evaluation for protocol 2 (HPE2 and HPE4).
Table 25: Training parameters for facial landmark detection for HRNetV2-W18 feature extractor.
Table 26: Facial landmark detection results on COFW dataset (FLD1). The failure rate is measured at thethreshold 0.1. θ = 30 is used for BEL.
Table 27: Facial landmark detection results on 300W dataset (FLD2). θ = 10 is used for BEL.
Table 28: Facial landmark detection results (NME) on WFLW test (FLD3) and 6 subsets: pose, expression(expr.), illumination (illu.), make-up (mu.), occlusion (occu.) and blur. θ = 10 is used for BEL.
Table 29: Facial landmark detection results on AFLW dataset (FLD4). θ = 30 is used for BEL.
Table 30: Training parameters for age estimation using MORPH-II and AFAD datasetOptimizer	Epochs	Batch size	Learning rate	Learning rate scheduleAdam, weight decay=0, momentum=0	50	64	0.0001	-Related work Existing approaches for age estimation include ordinal regression (Niu et al., 2016;Cao et al., 2020), soft regression (Yang et al., 2018), and expected value ordinal regression (Panet al., 2018; Gao et al., 2018). OR-CNN (Niu et al., 2016) proposed the use of ordinal regressionvia binary classification to predict the label. CORAL-CNN (Cao et al., 2020) refined this approachby enforcing the ordinality of the model output. SSR-Net (Yang et al., 2018) proposed the use ofstage-wise soft regression using the softmax of the classification output to refine the predicted label.
Table 31: Age estimation results on MORPH-II dataset (AE1). θ = 10 is used for BEL.
Table 32: Age estimation results on AFAD dataset (AE2). θ = 10 is used for BEL.
Table 33: Training parameters for end-to-end autonomous driving using PilotNet.
Table 34: End-to-end autonomous driving results on PilotNet dataset (PN) and architecture (Bojarski et al., 2017;2016).
