Table 1: Performance and convergence comparison for group one environments averaged over 10random seeds. The performance section compares the mean normalized final distances to goals ofextracted programs π-PRL with standard deviations; the convergence section compares averagednumbers of environment steps (in millions) until convergence for policy architecture search πT -PRL,policy extraction π-PRL, and training a program on the same architecture as π-PRL from scratch.
Table 2: Ablation study on different configurations of our algorithm. π-Affine denotes a non-compositional programmatic policy with affine controllers (the depth bound of its program derivationtree is set to 6). πdepth-Ensem represents a compositional program with primitive ensembles learnedby setting the corresponding program derivation tree depth bound. π-oracle denotes a policy learnedby imitating its oracle (Verma et al., 2018). d shows mean abstract syntax tree depth of learnedpolicies. The mean normalized final distances to goals are averaged over 10 random seeds.
Table 3: Comparison between our programmatic RL algorithm with oracle-based programmatic RLalgorithms that learn policies in the form of programs (using the same DSL as ours) (Verma et al.,2018) and decision trees (Bastani et al., 2018) on Mujoco/OpenAI environments. Neural oracles andour programmatic policies were trained using 3 million environment steps and we report the averagedfinal reward performance of three repeated experiments.
Table 4: Comparison between our programmatic affine policies with neural network policies bothlearned based on a TRPO agent (Schulman et al., 2015). The training and test distributions are variedto evaluate whether policies can produce an arbitrary number of repetitions. We report the results ontest distributions by measuring the fraction of rollouts (out of 500) that safely reach the goal.
Table 5: The average running time of over 10000 random executions of program derivation trees,single programmatic policies, and Bidirectional LSTM models (Qureshi et al., 2020).
Table 6: Network Structures.
Table 7: Performance comparison for group two environments averaged over 5 random seeds. Themean normalized final distances to goals and success rates with their standard deviations are reported.
Table 8: Comparison results of policy generalizability in Ant Reshaped Maze environment (Fig. 21).
Table 9: We compare the final episode reward of our programmatic policies π-RPL with thatof SAC (Haarnoja et al., 2018) and PPO (Schulman et al., 2017) agents on Humanoid-v3 andLunarLander-v2. We report the averaged results of three repeated experiments where “stoc” refersto a Gaussian policy with actions sampled from it, while “mean” refers to using mean of the policy.
