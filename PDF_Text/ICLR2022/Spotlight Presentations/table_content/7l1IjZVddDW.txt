Table 1: Verification performances (%) on various benchmarks.
Table 2: Verification performances (%) on various benchmarks with FedSGD and ArcFace.
Table 3: Verification performances (%) on various benchmarks.					Method	RFW	IJB-B African Asian Caucasian Indian TAR@FAR=Ie-4				IJB-C TAR@FAR=1e-4φ	83.50	83.08	90.26	87.32	58.62	60.98φ + W	84.03	83.58	90.33	87.26	34.63 (-23.99)	35.82 (-25.16)φ + P	83.80	83.08	90.32	87.38	68.25 (+9.63)	71.24 (+10.26)A.2.3 Necessity of Federated LearningBesides using federated learning, one can also choose to finetune client-expert models on localdatasets. We also implement this approach by training four independent models on sub-datasets ofBUPT-Balancedface and test their performances on benchmarks. The training procedure is consis-tent with what we described before in Sec. 4. Specifically, we finetune each model from φ0 byArcFace for 10 epochs with learning rate 0.001, batch size 512 and weight decay 5e-4.
Table 4: Verification performances (%) on various benchmarks.
Table 5: Details of the reconstruction network. Note that all the convtranspose2d layers use stride 2 and kernelsize 4 X 4.
Table 6: Comparisons of FedFace and PrivacyFace.
Table 7: Verification performances (%) with different initial models.
