Published as a conference paper at ICLR 2022
Hybrid Local SGD for Federated Learning
with Heterogeneous Communications
Yuanxiong Guo
The University of Texas at San Antonio
San Antonio, Texas 78249 USA
yuanxiong.guo@utsa.edu
Ying Sun
Pennsylvania State University
State College, PA 16801 USA
ysun@psu.edu
Rui Hu & Yanmin Gong
The University of Texas at San Antonio
San Antonio, Texas 78249 USA
{rui.hu@my.,yanmin.gong@}utsa.edu
Ab stract
Communication is a key bottleneck in federated learning where a large number
of edge devices collaboratively learn a model under the orchestration of a central
server without sharing their own training data. While local SGD has been pro-
posed to reduce the number of FL rounds and become the algorithm of choice
for FL, its total communication cost is still prohibitive when each device needs to
communicate with the remote server repeatedly for many times over bandwidth-
limited networks. In light of both device-to-device (D2D) and device-to-server
(D2S) cooperation opportunities in modern communication networks, this paper
proposes a new federated optimization algorithm dubbed hybrid local SGD (HL-
SGD) in FL settings where devices are grouped into a set of disjoint clusters with
high D2D communication bandwidth. HL-SGD subsumes previous proposed al-
gorithms such as local SGD and gossip SGD and enables us to strike the best
balance between model accuracy and runtime. We analyze the convergence of
HL-SGD in the presence of heterogeneous data for general nonconvex settings.
We also perform extensive experiments and show that the use of hybrid model
aggregation via D2D and D2S communications in HL-SGD can largely speed up
the training time of federated learning.
1	Introduction
Federated learning (FL) is a distributed machine learning paradigm in which multiple edge devices
or clients cooperate to learn a machine learning model under the orchestration of a central server,
and enables a wide range of applications such as autonomous driving, extended reality, and smart
manufacturing (Kairouz et al., 2021). Communication is a critical bottleneck in FL as the clients are
typically connected to the central server over bandwidth-limited networks. Standard optimization
methods such as distributed SGD are often not suitable in FL and can cause high communication
costs due to the frequent exchange of large-size model parameters or gradients. To tackle this issue,
local SGD, in which clients update their models by running multiple SGD iterations on their local
datasets before communicating with the server, has emerged as the de facto optimization method in
FL and can largely reduce the number of communication rounds required to train a model (McMahan
et al., 2017; Stich, 2019).
However, the communication benefit of local SGD is highly sensitive to non-iid data distribution as
observed in prior work (Rothchild et al., 2020; Karimireddy et al., 2020). Intuitively, taking many
local iterations of SGD on local dataset that is not representative of the overall data distribution will
lead to local over-fitting, which will hinder convergence. In particular, it is shown in (Zhao et al.,
2018) that the convergence of local SGD on non-iid data could slow down as much as proportionally
to the number of local iteration steps taken. Therefore, local SGD with a large aggregation period
1
Published as a conference paper at ICLR 2022
can converge very slow on non-iid data distribution, and this may nullify its communication benefit
(Rothchild et al., 2020).
Local SGD assumes a star network topology where each device connects to the central server for
model aggregation. In modern communication networks, rather than only communicating with the
server over slow communication links, devices are increasingly connected to others over fast com-
munication links. For instance, in 5G-and-beyond mobile networks, mobile devices can directly
communicate with their nearby devices via device-to-device links of high data rate (Asadi et al.,
2014; Yu et al., 2020). Also, edge devices within the same local-area network (LAN) domain can
communicate with each other rapidly without traversing through slow wide-area network (WAN)
(Yuan et al., 2020). This gives the potential to accelerate the FL convergence under non-iid data
distribution by leveraging fast D2D cooperation so that the total training time can be reduced in FL
over bandwidth-limited networks.
Motivated by the above observation, this paper proposes hybrid local SGD (HL-SGD), a new dis-
tributed learning algorithm for FL with heterogeneous communications, to speed up the learning
process and reduce the training time. HL-SGD extends local SGD with fast gossip-style D2D com-
munication after local iterations to mitigate the local over-fitting issue under non-iid data distribution
and accelerate convergence. A hybrid model aggregation scheme is designed in HL-SGD to inte-
grate both fast device-to-device (D2D) and slow device-to-server (D2S) cooperations. We analyze
the convergence of HL-SGD in the presence of heterogeneous data for general nonconvex settings,
and characterize the relationship between the optimality error bound and algorithm parameters. Our
algorithm and analysis are general enough and subsume previously proposed SGD variations such
as distributed SGD, local SGD and gossip SGD.
Specifically, we consider the FL setting in which all devices are partitioned into disjoint clusters,
each of which includes a group of connected devices capable of communicating with each other
using fast D2D links. The clustering can be a natural result of devices belonging to different LAN
domains so that those devices connected to the same LAN domain are considered as one cluster. In
another example, clustering is based on the geographic locations of mobile devices so that devices
in a cluster are connected to each other through D2D communication links.
In summary, the paper makes the following main contributions:
•	We propose a novel distributed learning algorithm for FL called HL-SGD to address the commu-
nication challenge of FL over bandwidth-limited networks by leveraging the availability of fast D2D
links to accelerate convergence under non-iid data distribution and reduce training time.
•	We provide the convergence analysis of HL-SGD under general assumptions about the loss func-
tion, data distribution, and network topology, generalizing previous results on distributed SGD, local
SGD, and gossip SGD.
•	We conduct extensive empirical experiments on two common benchmarks under realistic network
settings to validate the established theoretical results of HL-SGD. Our experimental results show
that HL-SGD can largely accelerate the learning process and speed up the runtime.
2	Background and Related Work
Large-scale machine learning based on distributed SGD has been well studied in the past decade, but
often suffers from large network delays and bandwidth limits (Bottou et al., 2018). Considering that
communication is a major bottleneck in federated settings, local SGD has been proposed recently to
reduce the communication frequency by running SGD independently in parallel on different devices
and averaging the sequences only once in a while (Stich, 2019; Lin et al., 2019; Haddadpour et al.,
2019; Yu et al., 2019; Wang et al., 2021). However, they all assume the client-server architecture and
do not leverage the fast D2D communication capability in modern communication networks. Some
studies (Liu et al., 2020; Abad et al., 2020; Castiglia et al., 2021) develop hierarchical FL algorithms
that first aggregate client models at local edge servers before aggregating them at the cloud server or
with neighboring edge servers, but they still rely on D2S communication links only and suffer from
the scalability and fault-tolerance issues of centralized setting. On the other hand, while existing
works on decentralized or gossip SGD consider D2D communications (Tsitsiklis, 1984; Boyd et al.,
2006), they assume a connected cluster with homogeneous communication links and will converge
2
Published as a conference paper at ICLR 2022
very slow on the large and sparse network topology that is typically found in FL settings. Unlike
previous works, HL-SGD leverages both D2S and D2D communications in the system.
Some recent studies aim to encapsulate variants of SGD under a unified framework. Specifically,
a cooperative SGD framework is introduced in (Wang & Joshi, 2021) that includes communica-
tion reduction through local SGD steps and decentralized mixing between clients under iid data
distribution. A general framework for topology-changing gossip SGD under both iid and non-iid
data distributions is proposed in (Koloskova et al., 2020). Note that all of the above works assume
undirected network topology for communications in every iteration. In comparison, our proposed
HL-SGD is different: the D2S communication is asymmetric due to the use of device sampling
and model broadcasting in each global aggregation round and cannot be modeled in an undirected
graph. Therefore, the convergence analysis of HL-SGD does not fit into the prior frameworks and
is much more challenging. Moreover, our major focus is on the runtime of the algorithm rather than
its convergence speed in iterations.
3	System Model
In this section, we introduce the FL system model, problem formulation, and assumptions we made.
Notation. All vectors in this paper are column vectors by default. For convenience, we use 1 to
denote the all-ones vector of appropriate dimension, 0 to denote the all-zeros vector of appropriate
dimension, and [n] to denote the set of integers {1, 2,...,n} with any positive integer n. Let ∣∣∙∣∣
denote the '2 vector norm and FrobeniUS matrix norm and ∣∣∙k2 denote the spectral norm of a matrix.
We consider a FL system consisting of a central server and K disjoint clusters of edge devices.
Devices in each clUster k ∈ [K] can commUnicate with others across an Undirected and connected
graph Gk = (V, Ek), where Vk denotes the set of edge devices in the clUster, and edge (i, j) ∈ Ek
denotes that the pair of devices i, j ∈ Vk can commUnicate directly Using D2D as determined by the
commUnication range of D2D links. Besides, each device can directly commUnicate with the central
server Using D2S links. Denote the set of all devices in the system as V := Sk∈[K] Vk, the nUmber
of devices in each clUster k ∈ [K] as n := |Vk|, and the total nUmber of devices in the system as
N := Pk∈[K] n 1.
The FL goal of the system is to solve an optimization problem of the form:
min f(X) := -1 X fi(x) := -1 X fk(X),
x∈Rd	N i∈V	K k∈[K]
(1)
where fi(x) := Ez〜D」'i(x; z)] is the local objective function of device i, fk(x) ：=
(1/n) Pi∈V fi (X) is the local objective fUnction of clUster k, and Di is the data distribUtion of
device i. Here `i is the (non-convex) loss function defined by the learning model and z represents a
data sample from data distribution Di .
When applying local SGD to (1) in FL with heterogeneous communications, the communications
between the server and devices in FL are all through D2S links that are bandwidth-limited, particu-
larly for the uplink transmissions. Therefore, the incurred communication delay is high. Due to the
existing of high-bandwidth D2D links that are much more efficient than low-bandwidth D2S links,
it would be highly beneficial if we can leverage D2D links to reduce the usage of D2S links such
that the total training time can be reduced. This motivates us to design a new learning algorithm for
FL with heterogeneous communications.
4	Hybrid Local SGD
In this section, we present our HL-SGD algorithm suitable for the FL setting with heterogeneous
communications. Algorithm 1 provides pseudo-code for our algorithm.
At the beginning of r-th global communication round, the server broadcasts the current global model
Xr to all devices in the system via cellular links (Line 4). Note that in typical FL systems, the down-
1For presentation simplicity, we assume each cluster contains the same number of devices here. The results
of this paper can be extended to the case of clusters with different device numbers as well.
3
Published as a conference paper at ICLR 2022
Algorithm 1 HL-SGD: Hybrid Local SGD
Input: initial global model x0, learning rate η, communication graph Gk and mixing matrix Wk for
all clusters k ∈ [K], and fraction of sampled devices in each cluster p.
Output: final global model xR
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18	: for each round r = 0, . . . , R - 1 do :	for each cluster k ∈ [K] in parallel do :	for each device i ∈ Vk in parallel do :	xr,0 = xr :	for s = 0, . . . , τ - 1 do :	Compute a stochastic gradient gi over a mini-batch ξi sampled from Di χr,s+2 = χrrrs — ηgi(χ[3)	. local update xr,s+1 = Pj∈Nk (Wk)i,jxrr,s+2	. gossip averaging :	end for :	end for :	end for :	for each cluster k ∈ [K] do m — max(p ∙ n, 1) Sr J (random set of m clients in Vk)	. device sampling :	end for xr+1 = K1 Pk∈[κ] m1 Pi∈sr xr,τ	. global aggregation : end for : return xR
link communication is much more efficient than uplink communication due to the larger bandwidth
allocation and higher data rate. Therefore, devices only consume a smaller amount of energy when
receiving data from the server compared with transmitting data to the server.
After that, devices in each cluster initialize their local models to be the received global model and
run T iterations of gossip-based SGD via D2D links to update their local models in parallel (lines 5-
9). Let xir,s denote the local model of device i at the r-th local iteration of s-th round. Here
each gossip-based SGD iteration consists of two steps: (i) SGD update, performed locally on each
device (lines 6-7), followed by a (ii) gossip averaging, where devices average their models with
their neighbors (line 8). In the gossip averaging protocol, Nik denotes the neighbors of device i,
including itself, on the D2D communication graph Gk of cluster k, and Wk ∈ [0, 1]n×n denotes
the mixing matrix of cluster k with each element (Wk)i,j being the weight assigned by device i to
device j. Note that (Wk)i,j > 0 only if devices i and j are directly connected via D2D links.
Next, a set Skr of m devices are sampled uniformly at random (u.a.r.) with probability p without
replacement from each cluster k ∈ [K] by the server (lines 13-14), and their final updated local
models {xir,τ, ∀i ∈ Skr} are sent to the server via D2S links. After that, the server updates the global
model xr+1 by averaging the received local models from all sampled devices (line 16). Note that
only m devices per cluster will upload their models to the server in each round to save the usage of
expensive D2S uplink transmissions. The intuition is that after multiple iterations of gossip-based
SGD, devices have already reached approximate consensus within each cluster, and the sampled
average can well represent the true average. By trading D2D local aggregation for D2S global
aggregation, the total communication cost can be reduced. We will empirically validate such benefits
later in the experiments.
It is worth noting that HL-SGD inherits the privacy benefits of classic FL schemes by keeping the
raw data on device and sharing only model parameters. Moreover, HL-SGD is compatible with
existing privacy-preserving techniques in FL such as secure aggregation (Bonawitz et al., 2017; Guo
& Gong, 2018), differential privacy (McMahan et al., 2018; Hu et al., 2020; 2021), and shuffling
(Girgis et al., 2021) since only the sum rather than individual values is needed for the local and
global model aggregation steps.
Runtime analysis of HL-SGD. We now present a runtime analysis of HL-SGD. Here we ignore
the communication time of downloading models from the server by each device since the download
bandwidth is often much larger than upload bandwidth for the D2S communication in practice (?).
In each round of HL-SGD, we denote the average time taken by a device to compute a local update,
4
Published as a conference paper at ICLR 2022
perform one round of D2D communication and one round of D2S communication as ccp, cd2d and
cd2s, respectively. Assume the uplink bandwidth between the server and devices is fixed and evenly
shared among the sampled devices in each round, then cd2s is linearly proportional to the sampling
ratio p. Similarly, ccp depends on the D2D network topology Gk and typically increases with the
maximum node degree ∆(Gk). The total runtime of HL-SGD after R communication rounds is
R × [τ × (ccp + cd2d ) + cd2s ] .	(2)
The specific values of ccp, cd2d and cd2s depend on the system configurations and applications. In
comparison, the total runtime of local SGD after R communication rounds is R × [τ × ccp + cd2s].
Previous algorithms as special cases. When devices do not communicate with each other, i.e.,
Wk = I, ∀k ∈ [K], and sampling ratio p = 1, HL-SGD reduces to distributed SGD (when τ = 1)
or local SGD (when τ > 1) where each device only directly communicates with the server with D2S
links. Also, when τ → ∞, HL-SGD reduces to gossip SGD where devices only cooperate with their
neighboring devices through a gossip-based communication protocol with D2D links to update their
models without relying on the server. Therefore, HL-SGD subsumes existing algorithms and enables
us to strike the best balance between runtime and model accuracy by tuning τ, Wk , and p. However,
due to the generality of HL-SGD, there exist significantly new challenges in its convergence analysis,
which constitutes one of the main contributions of this paper as elaborated in the following section.
5	Convergence Analysis of HL-SGD
In this section, we analyze the convergence of HL-SGD with respect to the gradient norm of the
objective function f (∙), specifically highlighting the effects of T and p. Before stating our results,
we make the following assumptions:
Assumption 1 (Smoothness). Each local objective function fi : Rd → R is L-smooth for all i ∈ V,
i.e., for all x, y ∈ Rd,
kVfi(x) -Vfi(y)k ≤ LIIx - yk,	∀i ∈V.
Assumption 2 (Unbiased Gradient and Bounded Variance). The local mini-batch stochastic gra-
dient in Algorithm 1 is unbiased, i.e., Eξi [gi(x)] = Vfi(x), and has bounded variance, i.e.,
Eξi Igi(x) - Vfi(x)I2 ≤ σ2 , ∀x ∈ Rd, i ∈ V, where the expectation is over all the local mini-
batches.
Assumption 3 (Mixing Matrix). For any cluster k ∈ [K], the D2D network is strongly connected
and the mixing matrix Wk ∈ [0, 1]n×n satisfies Wk1 = 1, 1>Wk = 1>, null(I - Wk) = span(1).
We also assume ||Wk - (1/n)11> ||2 ≤ ρk for some ρk ∈ [0, 1).
Assumption 4 (Bounded Intra-Cluster Dissimilarity). There exists a constant k ≥ 0 such that
(1/n) ∑i∈vfc ∣Vfi(x) 一 Vfk (x)k2 ≤ Ek for any x ∈ Rd and k ∈ [K ]. If local functions are
identical to each other within a cluster, then we have k = 0.
Assumption 5 (Bounded Inter-Cluster Dissimilarity). There exist constants α ≥ 1, E ≥ 0 such that
(1/K) Pk∈[κ] kVfk(x)∣2 ≤ α2 ∣∣Vf (x)k2 + eg forany x ∈ Rd. Iflocalfunctions are identical to
each other across all clusters, then we have α = 1, Eg = 0.
Assumptions 1-3 are standard in the analysis of SGD and decentralized optimization (Bottou et al.,
2018; Koloskova et al., 2019). Assumptions 4-5 are commonly used in the federated optimization
literature to capture the dissimilarities of local objectives (Koloskova et al., 2020; Wang et al., 2020).
5.1	Main Results
We now provide the main theoretical results of the paper in Theorem 1 and Theorem 2. The detailed
proofs are provided in the appendices. Define the following constants:
ρmax = max ρk ,
k∈[K]
k=1
(3)
5
Published as a conference paper at ICLR 2022
and let
r0
8(f(x0) - f(x?)), r1 = 16L
r2 = 16C1L2τ22g + 16C1L2
(4)
Theorem 1 (Full device participation). LetAssumptions 1-5 hold, and let L, σ, WL, Eg, Dτ,ρ, PmaX,
r0, r1, and r2 be as defined therein. If the learning rate η satisfies
η = min (4⅛ɑ
1
TL
(5)
then for any R > 0, the iterates of Algorithm 1 with full device participation for HL-SGD satisfy
min EIIVf(xr,s)∣∣2 = O
r,s
σ + (T2€2 + TPmaxDτ,ρ比 + τ (-1 + PmaX) σ2) 3
√NτR	(TR) 3
(6)
where xr,s = N PN=I xr,s.
In the following, we analyze the iteration complexity of HL-SGD and compare it with those of
some classic and state-of-the-art algorithms relevant to our setting in Table 1. First, we consider
two extreme cases of HL-SGD where ρmaX = 0 and ρk = 1, ∀k ∈ [K], and show that our analysis
recovers the best known rate of local SGD.
Fully Connected D2D networks. In this case, ρmaX = 0, and each cluster can be viewed as a single
device, and thus HL-SGD reduces to local SGD with K devices. Substuting ρmaX = 0 into (6),
the iteration complexity of HL-SGD reduces to O(σ∕√NτR + (τ2eg + T ∙ (σ2∕n))1/3/(τR)2/3 +
1/R). This coincides with the complexity of local SGD provided in Table 1 with device number K
and stochastic gradient variance σ2/n thanks to the fully intra-ClUster averaging.
Disconnected D2D networks. In this case, HL-SGD reduces to local SGD with N devices.
Substituting PmaX = 1 into (6), the iteration complexity of HL-SGD becomes OWyNTR +
(τ2(eg + 6L) + τσ2)“3 /(τR)2∕3 + 1/R). This coincides with the complexity of local SGD with
N devices, stochastic gradient variance σ2, and gradient heterogeneity of order eg2 + eW2L.
Table 1: Comparison of Iteration Complexity. 2
Local SGD
Gossip SGD
Gossip PGA (Chen et al., 2021)
HL-SGD (this work)
O
O
O
O
(T22g
(t 2e2+Tσ2
~
(tR) 3
2	2
P 3 e 3
2
(τR)3 (1-ρ)
二
cT,pdT,p'P 瓢2 +
(TR)3
(TR)23 (1-ρ)3
C3PP3 σ 2
(τR)3
+ 二__P^
十(1-p)tR
+ ρ⅛
十τPmaxDτ,ρCL) 3 + (T ( 1 +P；
2^
(TR) 3
2
max
T
(TR) 3
)σ2)3 +
+
ɪ +
P 3 σ 3
Next, we compare the complexities of HL-SGD, local SGD, gossip SGD and gossip PGA.
Comparison to Local SGD. Comparing (6) and the complexity of local SGD, we can see the intra-
cluster D2D communication provably improves the iteration complexity by reducing the transient
iterations. This is reflected in the smaller coefficient associated with the O((T R)-2/3) term. In par-
ticular, improving D2D communication connectivity will lead to a smaller PmaX and consequently,
mitigate the impact of both local data heterogeneity and stochastic noise on the convergence rate.
1The convergence rates for gossip SGD and local SGD are from (Koloskova et al. (2020)). The parameters
in the table are given by the following: σ2 : stochastic gradient variance; ρ: network connectivity; €2 : data
heterogeneity of order €2 + 就;Cτ,ρ，Pk-1 Pk, Dτ∕ = min{1∕(1 — ρ),τ}. Note that Dτ,ρ = Dτρ.
6
Published as a conference paper at ICLR 2022
Comparison to Gossip SGD. Under the condition that ρ = ρmax, i.e., the connectivity of D2D
network in gossip SGD is the same as that of HL-SGD, Table 1 shows HL-SGD outperforms gossip
SGDWhen τ∕n ≤ ρ2/ (1 -ρ). In other words, HL-SGD is beneficial for weakly connected networks,
which is the case in FL settings where a large number of devices are often loosely connected or
disconnected into several disjoint clusters via D2D communications only.
Comparison to Gossip PGA. Gossip PGA improves local SGD by integrating gossiping among all
devices in one round using a connected network. Compared to gossip SGD, gossip PGA has one
extra full averaging step with period τ. The complexity of gossip PGA improves both by reduc-
ing the transient iterations. HL-SGD (full participation) differs from gossip PGA in the sense that
gossiping is performed within multiple clusters instead of a single one. The benefit comes from the
fact that for many commonly used D2D network topologies, the spectral gap 1 - ρ decreases as the
network size decreases, see Table 2. Therefore, when employing the same D2D network topology,
HL-SGD enjoys a smaller connectivity number ρmax than ρ. Considering the scenario where τ and
n are fixed while the cluster number K grows, the total device number N = nK grows and hence
P → 1 for gossip PGA. In the case when T = Dτ∕ ≈ Cτ,ρ, the fastest decaying O(1∕τR) terms
are comparable for both algorithms. However, the O((τ R)-2/3) term of gossip GPA can be larger
than that of HL-SGD since ρ increases with N . This observation shows for large-scale networks,
it is advantageous to use HL-SGD with multiple connected clusters instead of gossip GPA with a
single cluster under the D2D network topology.
Our next result shows the iteration complexity of HL-SGD with partial device participation. We
assume the devices participate in synchronizing their models at the end of each FL round following
the sampling rule given by Assumption 6.
Assumption 6 (Sampling strategy). Each Skr contains a subset of m indices uniformly sampled from
{1, . . . , n} without replacement. Furthermore, Skr is independent of Skr00 for all (k, r) 6= (k0, r0).
Theorem 2 (Partial device participation). Let Assumptions 1-6 hold, and let L, σ,立,Eg, Dτ,ρ,
ρmax, r0, r1, and r2 be as defined therein. If the network connectivity satisfies
Pmax ≤ 1 — 1/T,	(7)
then for suitably chosen learning rate η, the iterates of Algorithm 1 with partial device participation
for HL-SGD satisfy
min EIIVf(xr,s)∣∣2
r,s
=O σ σ + E (fg ,立，σ, PmaX) [ (T 2 £2 + TPmaxDT次L +τ (1 + Pmax) M) 3
一[	√NτR	(TR) 3
where xr,s = N PN=I XT,
+ max{1,GpDτ,ρPmax}
+	R	),
(8)
E2 (Eg, eL, σ, PmaX) = (EgDτ,ρ + PmaxDτ,ρEL + σ^ ) ∙ GPDT,ρρmaxN + m ∙ Tρmaxσ^ .
and
(9)
(10)
n-m
P = m(n - 1),
Compared to Theorem 1, Theorem 2 shows partial device participation deteriorates the rate
by O(E(Eg,EL,σ, ρmaχ)∕√NTR). From the expression of E, we observe that as PmaX → 0,
E(Eg, EEL, σ, Pmax) vanishes, which indicates that the loss caused by device sampling can be com-
pensated by increasing network connectivity uniformly for all clusters.
The next corollary finds the critial PmaX so that E2 = O(1), and the order of convergence rate of
partial device participation matches that of the full participation case.
Corollary 1. Under the same assumptions as Theorem 2, if the network connectivity satisfies
PmaX ≤ 4N min{m, τ - 1}.
then
min ElIVf(xr,s)k2 = O
r,s
σ σ + £g + WL + (T%2 + TPmaxDτ,ρ<⅜ + T (n1 + Pmax) σ2) 3 + ɪ
[	√NTR	(TR)22	+ R
(11)
(12)
7
Published as a conference paper at ICLR 2022
Corollary 1 reveals the tradeoff between sampling intensity and network connectivity. More con-
nected D2D networks result in smaller ρmax , and thus (11) can be satisfied by a smaller m. This
means we can sample fewer devices at the end of each round and reduce the D2S communication
delay when the D2D network is more connected.
6 Experimental Evaluation
6.1	Experimental Settings
We use two common datasets inFL literature (McMahan et al., 2017; Reddi et al., 2021; Wang et al.,
2020): Federated Extended MNIST (Caldas et al., 2019) (FEMNIST) and CIFAR-10 (Krizhevsky
et al., 2009). The 62-class FEMNIST is built by partitioning the data in Extended MNIST (Cohen
et al., 2017) based on the writer of the digit/character and has a naturally-arising device partition-
ing. CIFAR-10 is partitioned across all devices using a Dirichlet distribution Dir(0.1) as done in
(Hsu et al., 2019; Yurochkin et al., 2019; Reddi et al., 2021; Wang et al., 2020). We evaluate our
algorithms by training CNNs on both datasets, and the CNN models for FEMNIST and CIFAR-10
were taken from (Caldas et al., 2019) and (McMahan et al., 2017) with around 6.5 and 1 million
parameters, respectively. For each dataset, the original testing set (without partitioning) is used to
evaluate the generalization performances of the trained global model.
We consider a FL system consisting of a central server and 32 devices. The devices are evenly
divided into four clusters, and each cluster has a ring topology by default, which provides a con-
servative estimation for the cluster connectivity and convergence speed. In our experiments, the
mixing matrix of each cluster Wk is set according to the MetroPoliS-HaStingS weights (Nedic et al.,
2018). According to the real-world measurements in (Yuan et al., 2020; Yang et al., 2021), we set
the average time for a device to Perform a local uPdate, a round of D2D communication under ring
toPology, and a round of D2S communication with one device samPled Per cluster to be ccP = 0.01h,
cd2d(∆ = 2) = 0.005h and cd2s(p = 1/8) = 0.05h, resPectively, in the runtime model (2). For ar-
bitrary device samPling ratio and D2D network toPology, we consider a linear-scaling rule (Wang
et al., 2019) and let Cd2d(△) = (∆∕2) × 0.005h and Cd2s(P) = 8p × 0.05h.
We comPare HL-SGD with local SGD in the exPeriments. For local SGD, devices will only com-
municate with the central server Periodically. In all exPeriments, we let the local iteration Period
τ to be the same for both local SGD and HL-SGD to have a fair comParison. On the FEMNIST
dataset, we fix the batch size as 30 and tune the learning rate η from {0.005, 0.01, 0.02, 0.05, 0.08}
for each algorithm seParately. On the CIFAR-10 dataset, we fix the batch size as 50 and tune η from
{0.01, 0.02, 0.05, 0.08, 0.1} for each algorithm seParately. We run each exPeriment with 3 random
seeds and rePort the average. All exPeriments in this PaPer are conducted on a Linux server with 4
NVIDIA RTX 8000 GPUs. The algorithms are imPlemented by PyTorch. More details are Provided
in APPendix F.
6.2	Experimental Results
(a) FEMNIST
(b) FEMNIST
(c) CIFAR-10
(d) CIFAR-10
Figure 1: Convergence rate and runtime comParisons of HL-SGD and local SGD under ring toPology when
τ = 50 and p = 1 for FEMNIST and CIFAR-10 datasets. (a) and (c) show how the accuracy changes over
communication round; (b) and (d) show how the accuracy changes over runtime.
We first comPare the convergence sPeed and runtime of HL-SGD and local SGD while fixing τ = 50
and p = 1. We measure the best test accuracy of the global model on the server in every FL round.
Figure 1 shows the convergence Process. From the figure, we can observe that HL-SGD can largely
accelerate the model convergence while imProving model accuracy in FL. On FEMNIST, the best
accuracy of HL-SGD achieved over 100 rounds is 4.78% higher than that of local SGD (i.e., 83.76%
8
Published as a conference paper at ICLR 2022
vs. 79.94%), and its runtime necessary to achieve a target test accuracy of 75% is only 17.64% of
that of the baseline (i.e., 5.67× speedup). On CIFAR-10, the best accuracy of HL-SGD achieved
over 100 rounds is 9.32% higher than that of local SGD (i.e., 68.71% vs. 63.68%), and its runtime
necessary to achieve a target test accuracy of 60% is 15.67% less than that of local SGD (i.e., 1.186×
speedup).
Ooooooo
7 6 5 4 3 2 1
AU-κ3
→- HL-SGD (τ= 5)
HL-SGD(T=IO)
HL-SGD (T= 20)
T- HL-SGD (τ=50)
+ Local SGD (τ = 5)
-X- Local SGD (τ = 10)
Local SGD (τ = 20)
Local SGD (τ = 50)
Ooooo
7 6 5 4 3
Aue」mu<⅛5g
O 50 IOO 150	200	250
Runtime (Hour)
Figure 2:	Convergence rate (left) and runtime (right) comparisons of HL-SGD and local SGD on CIFAR-10
under different τ and ring topology when p = 1.
Next, to give a more comprehensive analysis on the runtime benefits of HL-SGD, we vary τ from
{5, 10, 20, 50} and compare the performances of HL-SGD and local SGD on CIFAR-10 in Figure 2.
From the figure, we can observe that HL-SGD can consistently outperform local SGD across a wide
range of τ. In particular, on CIFAR-10, the best accuracy of HL-SGD achieved over 100 rounds is
2.49%, 3.99%, 4.05%, and 7% higher than that of local SGD, respectively, as τ increases from 5 to
50. At the same time, the runtime of HL-SGD needed to achieve a target test accuracy of 60% is
9.66%, 19.76%, 33.46%, and 45.88% less than that of local SGD, respectively.
Ooo
6 4 2
Aue」I-DUfSg
Round Number
→- HL-SGD (p = 0.125)
-B- HL-SGD (p = 0.25)
HL-SGD (p = 0.5)
T- HL-SGD(p=l)
Runtime (Hour)
(a) FEMNIST	(b) FEMNIST	(c) CIFAR-10	(d) CIFAR-10
Figure 3:	Effect of sampling ratio p on the convergence rate and runtime of HL-SGD under ring topology when
τ = 50 for FEMNIST and CIFAR-10 datasets. (a) and (c) show how the accuracy changes over communication
round in HL-SGD; (b) and (d) show how the accuracy changes over runtime.
Finally, we investigate how the sampling ratio p affects the performance of HL-SGD. We select
p from {0.125, 0.25, 0.5, 1}, corresponding to sampling {1, 2, 4, 8} devices from each cluster to
upload models to the server. Figure 3 depicts the best value of test accuracy achieved over all prior
rounds. As can be observed from the figures, sampling one device per cluster only results in slightly
lower model accuracy, e.g., neligible and 1.92% drop compared to full participation on FEMNIST
and CIFAR-10, respectively. This matches the theoretical result in Corollary 1 that device sampling
does not affect the order of convergence rate under certain conditions. However, decreasing p can
lead to faster training speed due to its shorter D2S communication delay as observed in Figures 3b
and 3d. In practice, the optimal value of p needs to be tuned to strike a good balance between model
accuracy and runtime.
7 Conclusion
In this paper, we have proposed a new optimization algorithm called HL-SGD for FL with heteroge-
neous communications. Our algorithm leverages the D2D communication capabilities among edge
device to accelerate the model convergence while improving model accuracy in FL. We have pro-
vided the theoretical convergence analysis of HL-SGD and conducted experiments to demonstrate
the benefits of HL-SGD. In the future, we plan to extend HL-SGD to handle straggler issues under
device heterogeneity and provide rigorous privacy protection for HL-SGD.
9
Published as a conference paper at ICLR 2022
Acknowledgments
The work of Y. Guo was supported in part by NSF under the Grant CNS-2106761. The work of Y.
Sun was partially supported by the Office of Naval Research under the Grant N00014-21-1-2673.
The work of R. Hu and Y. Gong was supported in part by NSF under the Grants CNS-2047761 and
CNS-2106761.
References
Mehdi Salehi Heydar Abad, Emre Ozfatura, Deniz Gunduz, and Ozgur Ercetin. Hierarchical fed-
erated learning across heterogeneous cellular networks. In IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP),pp. 8866-8870. IEEE, 2020.
Arash Asadi, Qing Wang, and Vincenzo Mancuso. A survey on device-to-device communication in
cellular networks. IEEE Communications Surveys & Tutorials, 16(4):1801-1819, 2014.
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar
Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacy-
preserving machine learning. In Proceedings of the 2017 ACM SIGSAC Conference on Computer
and Communications Security, pp. 1175-1191, 2017.
Leon Bottou, Frank E. Curtis, and Jorge NocedaL Optimization methods for large-scale machine
learning. SIAM Review, 60(2):223-311, 2018.
Stephen Boyd, Arpita Ghosh, Balaji Prabhakar, and Devavrat Shah. Randomized gossip algorithms.
IEEE Transactions on Information Theory, 52(6):2508-2530, 2006.
Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Konecny, H Brendan MCMa-
han, Virginia Smith, and Ameet Talwalkar. LEAF: A benchmark for federated settings. In Work-
shop on Federated Learning for Data Privacy and Confidentiality, 2019.
Timothy Castiglia, Anirban Das, and Stacy Patterson. Multi-Level Local SGD: Distributed SGD for
heterogeneous hierarchical networks. In International Conference on Learning Representations,
2021.
Yiming Chen, Kun Yuan, Yingya Zhang, Pan Pan, Yinghui Xu, and Wotao Yin. Accelerating gossip
SGD with periodic global averaging. In Proceedings of the 38th International Conference on
Machine Learning, pp. 1791-1802. PMLR, 2021.
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. EMNIST: Extending
MNIST to handwritten letters. In 2017 International Joint Conference on Neural Networks
(IJCNN), pp. 2921-2926. IEEE, 2017.
Antonious Girgis, Deepesh Data, Suhas Diggavi, Peter Kairouz, and Ananda Theertha Suresh. Shuf-
fled model of differential privacy in federated learning. In International Conference on Artificial
Intelligence and Statistics, pp. 2521-2529. PMLR, 2021.
Yuanxiong Guo and Yanmin Gong. Practical collaborative learning for crowdsensing in the internet
of things with differential privacy. In 2018 IEEE Conference on Communications and Network
Security (CNS), pp. 1-9. IEEE, 2018.
Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, and Viveck Cadambe. Local
SGD with periodic averaging: Tighter analysis and adaptive synchronization. In Advances in
Neural Information Processing Systems, pp. 11082-11094, 2019.
Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data
distribution for federated visual classification. arXiv preprint arXiv:1909.06335, 2019.
Rui Hu, Yuanxiong Guo, Hongning Li, Qingqi Pei, and Yanmin Gong. Personalized federated
learning with differential privacy. IEEE Internet of Things Journal, 7(10):9530-9539, 2020.
Rui Hu, Yanmin Gong, and Yuanxiong Guo. Federated learning with sparsification-amplified pri-
vacy and adaptive optimization. In Proceedings of the Thirtieth International Joint Conference
on Artificial Intelligence, IJCAI-21, pp. 1463-1469, 2021.
10
Published as a conference paper at ICLR 2022
Peter Kairouz, H Brendan McMahan, Brendan Avent, AUrelien BelleL Mehdi Bennis, Arjun Nitin
Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Ad-
vances and open problems in federated learning. Foundations and Trends® in Machine Learning,
14(1-2):1-210, 2021.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. SCAFFOLD: Stochastic controlled averaging for federated learning.
In International Conference on Machine Learning, pp. 5132-5143. PMLR, 2020.
Anastasia Koloskova, Sebastian Stich, and Martin Jaggi. Decentralized stochastic optimization and
gossip algorithms with compressed communication. In International Conference on Machine
Learning, pp. 3478-3487, 2019.
Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian Stich. A unified
theory of decentralized SGD with changing topology and local updates. In International Confer-
ence on Machine Learning, pp. 5381-5393. PMLR, 2020.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Tao Lin, Sebastian U Stich, Kumar Kshitij Patel, and Martin Jaggi. Don’t use large mini-batches,
use local SGD. In International Conference on Learning Representations, 2019.
Lumin Liu, Jun Zhang, SH Song, and Khaled B Letaief. Client-edge-cloud hierarchical federated
learning. In IEEE International Conference on Communications, pp. 1-6. IEEE, 2020.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial intelli-
gence and statistics, pp. 1273-1282. PMLR, 2017.
H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private
recurrent language models. In International Conference on Learning Representations, 2018.
Angelia Nedic, Alex Olshevsky, and Michael G Rabbat. Network topology and CommUnication-
computation tradeoffs in decentralized optimization. Proceedings of the IEEE, 106(5):953-976,
2018.
Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub KoneCny,
Sanjiv Kumar, and Hugh Brendan McMahan. Adaptive federated optimization. In International
Conference on Learning Representations, 2021.
Daniel Rothchild, Ashwinee Panda, Enayat Ullah, Nikita Ivkin, Ion Stoica, Vladimir Braverman,
Joseph Gonzalez, and Raman Arora. FetchSGD: Communication-efficient federated learning
with sketching. In International Conference on Machine Learning, pp. 8253-8265. PMLR, 2020.
Sebastian Urban Stich. Local SGD converges fast and communicates little. In International Con-
ference on Learning Representations, 2019.
John Nikolas Tsitsiklis. Problems in decentralized decision making and computation. Technical
report, Massachusetts Inst of Tech Cambridge Lab for Information and Decision Systems, 1984.
Jianyu Wang and Gauri Joshi. Cooperative SGD: A unified framework for the design and analysis
of local-update SGD algorithms. Journal of Machine Learning Research, 22(213):1-50, 2021.
Jianyu Wang, Anit Kumar Sahu, Zhouyi Yang, Gauri Joshi, and Soummya Kar. MATCHA: Speed-
ing up decentralized SGD via matching decomposition sampling. In 2019 Sixth Indian Control
Conference, pp. 299-300. IEEE, 2019.
Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective
inconsistency problem in heterogeneous federated optimization. Advances in Neural Information
Processing Systems, 33:7611-7623, 2020.
Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat,
Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A field guide to feder-
ated optimization. arXiv preprint arXiv:2107.06917, 2021.
11
Published as a conference paper at ICLR 2022
Chengxu Yang, Qipeng Wang, Mengwei Xu, Zhenpeng Chen, Kaigui Bian, Yunxin Liu, and Xu-
anzhe Liu. Characterizing impacts of heterogeneity in federated learning upon large-scale smart-
phone data. In Proceedings Ofthe Web Conference 2021 ,pp. 935-946, 2θ21.
Bicheng Ying, Kun Yuan, Yiming Chen, Hanbin Hu, Pan Pan, and Wotao Yin. Exponential graph
is provably efficient for decentralized deep training. Advances in Neural Information Processing
Systems, 34, 2021.
Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted SGD with faster convergence and less
communication: Demystifying why model averaging works for deep learning. In Proceedings of
the AAAI Conference on Artificial Intelligence, volume 33, pp. 5693-5700, 2019.
Zhe Yu, Yanmin Gong, Shimin Gong, and Yuanxiong Guo. Joint task offloading and resource
allocation in UAV-enabled mobile edge computing. IEEE Internet of Things Journal, 7(4):3147-
3159, 2020.
Jinliang Yuan, Mengwei Xu, Xiao Ma, Ao Zhou, Xuanzhe Liu, and Shangguang Wang. Hierarchical
federated learning through LAN-WAN orchestration. arXiv preprint arXiv:2010.11612, 2020.
Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and
Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In Interna-
tional Conference on Machine Learning, pp. 7252-7261. PMLR, 2019.
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated
learning with non-iid data. arXiv preprint arXiv:1806.00582, 2018.
A Preliminaries
Intra-cluster dynamics. To facilitate the analysis, we introduce matrices Xk ∈ Rn×d and Gk ∈
Rn×d constructed by stacking respectively xi and gi for i ∈ Vk row-wise. Similarly, we define
the pseudo-gradient NFk(Xk) ∈ Rn×d associated to cluster k by stacking Nfi(Xi) for i ∈ Vk
row-wise. In addition, define the following intra-cluster averages for each cluster k:
1
Xk , 一)J Xi
i∈Vk
and gk, n χ gi.
i∈Vk
(13)
The update within each cluster then can be written compactly in matrix form as
Xkr,s+1 =Wk(Xkr,s-ηGrk,s),	∀k=1,...,K.	(14)
Since each Wk is bi-stochastic, we obtain the following update of the intra-cluster average
xk,s+1 = XkS-η∙ grks.	(15)
We proceed to derive the update of the intra-cluster consensus error. Define the averaging matrix
n
J = ɪɪ ∙ 1> with 1 = [1,..., 1].
(16)
X~^{z~}
n
Multiplying both sides of (14) from the left by (I-J) leads to the following update of the consensus
error:
(I - J)Xkr,s+1 = (I - J)Wk(Xkr,s - ηGrk,s)
V---------{--------}
X r,s+1
k,⊥
(Wk-J)(Xkr,s
- ηGrk,s )
(17)
(Wk-J)(Xkr,,s⊥-ηGrk,s).
Global average dynamics. Define the global average among all Xi ’s as
1N
1
X , N 工 x
i=1
(18)
12
Published as a conference paper at ICLR 2022
Then accordingly to (15) We have the following update of X for all S = 0,...,τ - 1:
1K	1K
Xr,s+1 = NN X nxk,s+1 = N X n (xk,s -陆、
k=1	k=1
1N	1N
=N X(W,s-ηgr,s) = xr,s - ηNN X gr,s.
i=1	i=1
(19)
Filtration. Let G = [G1; . . . ; GK] ∈ RN×d be the matrix constructed by stacking all the stochastic
gradients. We introduce the following filtration
Fr,s = σ (G0,0, ...,G0,τ-1, So,G1,0,...,G1,τ-1,..., Sr-ι, Gr，0,...,Gr，s)
Fr = σ (G0,0, ...,G0,τ-1, So,G1,0,...,G1,τ-1,..., Sr-ι).
(20)
Therefore we have xir,0 = xr ∈ Fr for r ≥ 1, and xir,s ∈ Fr,s-1 for 1 ≤ s ≤ τ. For simplicity
the conditional expectation E( ∙ ∣Fr,s) is denoted as E—, and we define the noise in the stochastic
gradient as
ξr,s，gr,s-vfi(W,s).
(21)
Since at the end of round r all nodes are picked with equal probability, the sampling procedure
preserves average in expectation:
Er,τ-2Xr+1 = E(E(Xr+1∣Fr,τ-l)∣Fr,τ-2)
m X χ7r,τlFr,τ-1 ] lFr,τ—2ʌ
i∈Skr
m X I(i ∈ Sr )xr,τ ∣Fr,τ —1 ) ∣Fr,τ-2
m i∈Vk
Er,τ-2(Xr,τ )
(22)
where the last equality holds since P(i ∈ Sr |i ∈ Vk) = m.
B Convergence Analysis
To prove the convergence we first establish in Sec. B.1 that the objective value Ef (Xr ) is descending
at each round r, up to some consensus error terms. Subsequently, bounds on the error terms are
provided in Sec. B.2-B.4. Based on these results, the proof of convergence of Algorithm 1 with full
and partial device participation are given in Sec. B.5 and B.6, respectively. The proofs of the main
propositions are given in Sec. C and that of the supporting lemmas are deferred to Sec. D.
13
Published as a conference paper at ICLR 2022
B.1 Objective Descent
Lemma 1. Let {xir,s} be the sequence generated by Algorithm 1 under Assumptions 1-6. If η > 0,
then the following inequality holds for all r ∈ N+:
Ef(xr+1)
≤Ef(xr)-
τ-1	τ-1	N	2
4 X EkVf(Xr,s)k2-4 x Ell nn x Vfi(Xr，s)∣∣
s=0	s=0	i=1
τ-1	K
-4 x e∣i ⅛ x Vfk Ms)H
s=0	k=1
τ-1	N	2 τ-1	K	2 (23)
+4xEHVf(Xr,s)-nnxVfi(Xr,s)∣∣ + 4xEHVf(Xr,s)-KKxVfk(Xk，s)∣∣	(
s=0	i=1	s=0	k=1
τ-1	N	K	2
+4 X EH N X Vfi(Xr,s)- K X Vfk(Xk,s)∣∣
s=0	i=1	k=1
+ X E (L kXr,s+1 - Xr,sk2) + E GkXr+1 -Xr,τTk2).
Proof. The proof is a standard application of the descent lemma and the sampling rule applied at
iteration T to obtain Xr+1. See Appendix D.1.	口
Lemma 1 shows the objective value f(Xr) is descending in expectation up to the following error
terms:
∣	1N	∣2
Ti= EHVf(Xr,s)- NN X Vfi(Xr,s)∣∣
N i=1
∣	1K	∣2
T2 = EHVf (Xr,s)- N X Vfk(Xk，s)∣∣
k=1
1K	1N	2
T3=eH K X Vfk Ms)- N X Vfi(Xr,s)∣∣
k=1	i=1
T4 = EkXr,s+1 — Xr,sk2, T5 = EkXr+1 — Xr,τ-1k2.
(24)
In the sequel, we will show these quantities can be bounded by the optimality gap measured in terms
OfthegradientnOTms ∣∣Vf(Xr,s)k2, k(1∕K) PK=I Vfk(Xk,s)k2,and k(1∕N) PN=ι Vfi(Xr,s)k2.
B.2 BOUNDING T1, T2 AND T3.
Define
ρmax = max ρk.
k=1,...,K
Therefore it holds 0 ≤ ρmax ≤ 1 by Assumption 3.
(25)
14
Published as a conference paper at ICLR 2022
Since each f is L-Smooth by Assumption 1, We have fk and f are also L-Smooth. Using this fact
and the convexity of ∣∣ ∙ k2 we can bound Ti, T2 and T3 as
1 K	1 N	2
Ti = ElVf(Xr,s) ± KK X Vfk(Xk,s) - NN X Vfi(Xr,s) Il
k=1	i=1
1K	K1
≤ 2K X L2Ekxr,s - Xk,sk2 + 2 X N X L2Ekxk,s - xr,sk2,
k=i	k=i	i∈Vk
1K	2	1K	2
T2= E∣∣Vf(Xr,s)- KK X Vfk(Xk,s) Il ≤ IK X L2E卜,s -xk,s∣∣ ,
k=i	k=i
1K	1N	2 K 1
T3=e∣∣N XVfk(XA NN XVfi(Xr,s)∣∣ ≤ XNN X L2Ekχk,s - χr,sk2.
k=i	i=i	k=i	i∈Vk
(26)
Clearly, in order to bound Ti,2,3 we first need to bound the inter-cluster consensus error ∣∣xr,s-Xrk,sk
and the intra-cluster consensus error ∣Xk,s — x：,s ∣.
Lemma 2 (Inter-Cluster Consensus Error Bound). Let {Xir,s } be the sequence generated by Algo-
rithm 1 under Assumptions 1, 2, 3, and 5. If the learning rate η > 0 satisfies
21
η ≤ 24τ (4τ - 1)L2 ,	(27)
then for all s = 0, . . . , τ - 1 it holds
1K	1K
KK XEkXr,s+1-Xk,s+1k2 ≤ CT灭 XE∣Xr,s - Xk,sk2
k=i	k=i
+ 12τη2L2 G X E∣∣Xr',⊥∣∣) +12τη2 (α2EkVf(Xr,s)k2 + eg) + η2KN-1 σ2 (28)
where
CT , 1 + 3 ——1一.	(29)
2 4τ - 1	' '
Proof. See Appendix D.2.	口
Lemma 3 (Intra-Cluster Consensus Error Bound). Let {Xir,s } be the sequence generated by Algo-
rithm 1 under Assumptions 1-5. Ifη > 0, then for all s = 0, . . . , τ - 1 it holds
1K	1K
一VEkXr,s+1k2 ≤	max ρ2 (1 + Z-1) + rj2ρL ∙ 4L2 — VEkXr,s k2
N	k,⊥	k∈[K]ρk	k η ρL	N	k,⊥
k=i	∈[ ]	k=i
1K
+ 4η2ρLL2K EEkXk,s - ±，叫|2 + 4η2ρL(α2EkVf(Xr,S)∣2 + eg) + 4η2ρL稼 + η2σ2ρmaχ,
k=i
(30)
where ρmax is defined in (25) and
1K
pl , m max. {ρk (1 + Zk)},	eL, KEek	(31)
k=i,...,K	K
,,	k=i
with Zk > 0 being a free parameter to be chosen properly for all k = 1, . . . , K .
Proof. See Appendix D.3.
□
15
Published as a conference paper at ICLR 2022
Combining Lemma 2 and 3 we can obtain the following bound on the sum of intra- and inter-
consensus errors using gradient kVf (xr,s)k2.
Proposition 1. Let {xir,s} be the sequence generated by Algorithm 1 under Assumptions 1-5. If the
learning rate η > 0 satisfies
η2 ≤ 24τ (4τ - 1)L2 ,	(32)
then for all s = 0, . . . , τ - 1 it holds
1K	K1
1「Ek >r,s+1 — >r，s + ”|2 + b 土 IlX r,s + "∣2
K Z Ekx - Xk k + N kXk,⊥ k
k=1	k=1
s
≤ XC1η2 (τ + PmaxDτ,ρ) (α2E∣Vf (xr,')∣∣2 + W) +。1*2*视乂DT温	(33)
'=0
+ C1τη2ρmaxσ2 + CI(T + DT,ρτ-1P2nax)η2 1 σ2
max	τ,ρ max n
where
DM , min 卜 τ-⅛}	(34)
and C1 > 0 is some universal constant.
Proof. See Appendix C.1.	□
Notice that according to (26) the gradient difference terms in Lemma 1 can be bounded as
1N	2	1K	2
4E∣vf(χr,s) - N X Vfi(Xr,s" + 4EllVf(Xr,s) - K XVfkMs”
i=1	k=1
N	K2
+ 4EHNXVfi(XD- K X Vfk(Xk，s)∣∣
i=1	k=1
≤4 (K X L2EkXr,s - Xk,sk2 + X N X L2EkXk,s - Xr,sk2!
k=1	k=1	i∈Vk
K	2K
+ 4 ⅛XL2E∣∣Xr,s -Xk,s∣∣ + XN X L2EkXk,s -Xr,sk2
k=1	k=1	i∈Vk
≤ηL2 (K X EkXr,s - Xk,s k2 + X N kxr,⊥ k2)
k=1	k=1
for all s = 1, . . . , τ . Therefore Proposition 1 immediately leads to the following result.
Corollary 2. Under the same setting as Proposition 1, it holds
(35)
τ	N	2τ	K	2
X 4 q Vf(Xr,s) - N X Vfi(Xr,s)∣∣ + X 4 用 Vf(Xr,s) - K X Vfk(Xk，s)∣∣
s=0	i=1	s=0	k=1
τ	τ-1	N	K	2
+X 4 X EH N X Vfi(Xr,s) - K X Vfk(Xk，s) ∣∣
s=0	s=0	i=1	k=1
τ-1
≤ X CιL2η3 (τ2 + TPmaxDτ,ρ) (α2E∣Vf (Xr,s)k2 + V) +。力"37"m^。”亮
s=0
2
+ Cιτ 2L2η3ρmaxσ2 + CiL2 (τ2 +。斯^^对餐
(36)
16
Published as a conference paper at ICLR 2022
We conclude this section by providing a separate bound on the consensus error
N PK=I ErkXr'⊥-1 k2 that will be useful in bounding T5.
Proposition 2. Under the same setting as Proposition 1, if PmaX ≤ 1 一 1 ,then we have
K	τ-2
NNEX kXr,⊥-1k2 ≤ 2C2 XDT,ρPmaxη2(α2kVf (xr,s)k2 + W)
k=1	s=0
+ C2D：,PTPmaxn就 + C2 (n-T,ρ + 1) PmaXTDτ,Pη2σ2.	(37)
for some universal constant C2 > 0.
Proof. See Appendix C.2.
□
Proposition 2 shows that the average intra-ClUster consensus error NN PK=IIlXr'⊥-1 k2 decreases as
the network connectivity improves, and vanishes if Pmax goes to zero.
B.3	BOUNDING T4
Proposition 3. Under the same setting as Lemma 1, we have
N
2
Ekxr,s+1 一 xr,sk2 = η2E
N2
NN X Vfi(Xr,s)	+ η2σN
(38)
i=1
2
for s = 0, . . ., T 一 1 and r ∈ N+.
Proof. Recall the algorithmic update at iteration s for all s = 0, . . ., T 一 1:
X r,s+1
Wk Xkr,s 一 ηWk Grk,s
r,s+1	r,s
xk	= xk
Therefore, it holds under Assumption 2 that
E∣∣xr,s+1 — xr,sk2
r,s
一 ηgk .
(39)
=E
N
N X(gr,s ±vfi(χr,s)
N i=1
2
=E
N
N X Vfi(Xr，s)
i=1
2 σ2
+η N.
(40)
2
□
B.4	BOUNDING T5
We provide the bound on T5 separately for the full device participation and partial participation
cases.
Full participation.
When the sampling probability p = 1, we have
Xr+1 — ɪ 'X^ Xr,τ — Xr,τ
X = N 乙 Xi = X .
i=1
In this case, it follows from Proposition 3 that
1 N	2	2
E∣Xr+1 - Xr,τ-1 k2 = η2E NN XVfi(Xr,τ-1)	+ η2N.
i=1
Partial participation.
(41)
17
Published as a conference paper at ICLR 2022
We proceed to bound T5 for
1 ≤ m ≤ n- 1.
Define p = m/n. Recall the algorithmic update at iteration τ - 1:
Xkr,τ = WkXkr,τ-1 -ηWkGrk,τ-1
and
xr+1
1K1
1 X _ X χr,τ
Krmri
k=1	i∈Skr
(42)
(43)
(44)
Therefore, with (Wk)i,j being the ij-th element of matrix Wk we have under Assumption 2:
Ekxr+1
/
xr，T-1k2
=E
1K
N XX χr,τ-χr,τ-1
k=1 i∈Skr
=E
1K
N XX(X(Wk)i,j(χj,τ-1 - ηgrr,τ-1)) - χr,τ-1
p k=1 i∈Skr j∈Vk
K
XX I(i ∈ Sk)( X (Wk )i,j (χj,τ-1-ηVfj (X 厂1)) -χr,τ-1
(45)
k=1 i∈Vk
j∈Vk
+ η2 E
1
Np
K
XX I(i ∈ Sk)( X (Wk)i,j (Nfj (χk,τ-1) - gj,τ-1)).
k=1 i∈Vk
j∈Vk
-A{27
E
|
—
2
∖
|
2
2
}
2
}
Proposition 4. Let {xir,s} be the sequence generated by Algorithm 1 under Assumptions 1-6. If the
learning rate η > 0 satisfies
η2 ≤
then we have the following bounds on A2,1:
1 K
A2,1 ≤2η2E K X Nfk(XkT-I)
k=1
where
Gp
Proof. See Appendix C.3.
1
24τ(4τ — 1)L2，
+ 8 (Gp + ~2 ) (N X EkXr,⊥Tk2
n-m
m(n - 1) .
(46)
(47)
(48)
□
Proposition 5. Under the same setting as Proposition 4, A2,2 can be bounded as
2
σ2	n 2
A2,2 ≤ N (2+ m ∙ ρmax.
(49)
Proof. See Appendix C.4
□
18
Published as a conference paper at ICLR 2022
B.5 Proof of Theorem 1 (full participation)
We first prove the descent of the objective value under suitable choice of η .
Proposition 6. If the learning rate satisfies
η≤ 4Cα∙ TL
(50)
then we have
τ-1
Ef (χr+1) ≤ Ef(Xr) - η XEkVf(Xr,s)k2 + Rfull(η),
8 s=0
(51)
where
RfUll㈤=CIL2η3τ2 (T + PmaxDτ,ρ) eg
+ 2ciL2η3 T22pmaχDτ,ρeL + T2σ2 n— + Pmax
2 σ2
+ η LTN
(52)
C1 > 0 is some universal constant.
Proof. See Appendix C.5.
To attain the expression of the convergence rate, we sum (51) over r = 0, . . . , R:
min min	EkVf (xr,s)k2
r∈[R] s=0,...,τ -1
8(f(χ0)- f (χ?)) + 8Rfuiι(η)
ητ(R+ 1)
8(f(x0) - f(x?))
ηT(R+ 1)
I	,
ηT
2
+ 16ηLN
(53)
{^^^^^^^≡
centralized SGD
□
≤
+ 16C1L2τ2η2g2 + 16C1L2η2
、
Dτ,ρ eL + τσ2 —- + Pmax))
"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"^^{^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^""^
network effect
The first two terms of (53) corresponds to the impact of stochastic noise and is of the same order
as the centralized SGD algorithm. The last term is of order η2 and corresponds to the deterioration
of convergence rate due to the fact that we are not computing the average gradients of all devices at
each iteration.
Denote
r0 = 8(f(X0) - f(X?)), r1 = 16L
r2 = 16C1L2T2e2g + 16C1L2
DTML+τσ2 In+Pmax
(54)
The rest of the proof follows the same argument as (?, Appendix B.5) and thus we omit the details.
B.6 Proof of Theorem 2 and Corollary 1 (partial participation)
Proposition 7. Let {Xir,s} be the sequence generated by Algorithm 1 under Assumption 1-5. If the
learning rate η and the network connectivity satisfies
η≤
1
C3αTL
• min h C γ/---------
αGp Dτ,ρ Pmax
and Pmax ≤ 1 - 彳
(55)
19
Published as a conference paper at ICLR 2022
then
τ-1
Ef(xr+1) ≤ Ef (Xr) - 8 X EkVf (xr,s)k2 + Rpa)rt(η) + Rpa)rt(η)	(56)
8 s=0
with
Rpa)rt(η) =C1L2η3τ2 (τ + PmaxDτ,ρ) eg + 2CιL2η3，*2,温 + τ2σ2 (； + Pmax))
2 σ2
+ LTn N，
(57)
Rpart(n) = (8C2G,pLD*,ρTρmx) η2eg
+ 4C2LGpn2 (D2,ρτρmax稼 + (η―T,ρ + 1) PmaxTDτ,ρσ2) + 2 (mρmnax) η2 N
C1 , C3 > 0 are some universal constants, and
Gp = Gp + ±,	with Gp = n - m .	(58)
p	T 2	m(n - 1)
Proof. See Appendix C.6
□
Comparing (56) to (51) we can see that R(p1a)rt(η) is of the same order as Rf(u1l)l(η), while R(p2a)rt(η) is
an extra loss term introduced by sampling.
Following the same steps as the proof of Theorem 1 gives
min min EkVf (xr,s)k2
r∈[R] s=0,...,τ -1
=O f σ + E(eg,立,σ, Pmax) + (T 2eg+TPmaxDTML+T( n+Pmax) σ2) 1
一 I	√NTr	(TR) 3
+ RmaX{1,GpDτ,ρPmax} ,
where
E2 (Eg, eL, σ, Pmax) = (EgDτ,ρ + Dτ,ρeL + σ2) ∙ GPDT,pRaxN + 嬴∙ TPmaxσ'
(59)
(60)
Our last step simplifies the overall conditions on Pmax so that E2(eg, El, σ, Pmax) = O(1):
1	1m
Pmax ≤ 1 - T,	GPDT,ρPmax ≤ ^N, Pmax ≤ ~ ∙ t	(61)
We claim to fulfill (61) it suffices to require
Pmax ≤ 4N min{m,T — 1}.	(62)
When T = 1, the condition trivially requires Pmax = 0. We then consider the case for T ≥ 2. By
definition, it can be verified that
First notice that
GP ≤ —I—2.
p m	T2
m < 1 <	1
4N ≤ 4 ≤ - T
and
Tm
4N ≤ n ∙ T.
Therefore, it remains to prove (62) implies
(63)
(64)
(65)
20
Published as a conference paper at ICLR 2022
Using the fact that under (62) ρmax ≤ 1/4 we have
≤ 16 Pmax(Lg
9 m	τ2
16	1	1	1	1
≤m + T2 4Nmin{m,τ}≤N
This proves the claim.
C Proof of Main Propositions
C.1 Proof of Proposition 1
Denote for short
Mr,s
1K
N X Ekχr,⊥k2
k = 1
1	K
IK XEkxk,s-xr,sk2
K k=1
(66)
(67)
Invoking Lemma 2 and Lemma 3 we obtain that under the condition that the learning rate η > 0
satisfies
η2 ≤ 24τ(4τ1- 1)L2 ,	(68)
the following inequality is satisfied for all s = 0, . . . , τ - 1:
Mr,s+1 ≤ G ∙ Mr，s + Br，s,	(69)
where
G = (maχk∈[K] Pk(I + ζ-1) + η2PL ∙4L2 η2ρ々∙ 4L2)
B r,s = (4ρLη2(α2EkVf(Xr,S)k2 + eg ) + 4η2 PLe2L + η2ρmaχσ2
V	12τη2(α2EkV f (xr,s)k2 + eg) + η2 限
(70)
(71)
The inequality in (69) is defined elementwise.
Unrolling (69) yields
s
Mr，s+1 ≤ X GeBrrSτ,	(72)
'=0
where we have used the fact that Mr,0 = 0 due to full synchronization of the xi ’s at the beginning
of each round r.
We first provide a bound on the sum of the two elements of G'Βr,s-'. For simplicity We omit the
round index r in the superscript for the rest of this section.
Lemma 4. Let b；— and b>2-' be the first and second element of Bs-e, respectively. Suppose the
learning rate η > 0 then
(1,1)Ge	Bl ≤ λ2(b1-e + bS-e) + λ2 - λ1 η2 ∙ (12τL2b1-' + 4ρLL2b2-e)	(73)
λ2 - λ1
where λ1 ≤ λ2 are the eigenvalues of G; and PL is defined in (31).
Proof. See Appendix D.4.
□
21
Published as a conference paper at ICLR 2022
From Lemma 4 We immediately get
S
X(1,1) ∙ GeBl
'=0
≤ X (λ2M-e + 仿-')+ λ2-^η2 ∙ (12τL2b1-e + 4ρLL2b2-e)
e=0 '	2	1
Since λ2 ≥ CT > 1, we have
(74)
㈡=λ2-1 X 停)S ≤ λ2-1 mm 1 "，'} ≤ λ mm { ɪ,'
λ2 — ʌl	S=0 ∖λc2 )	I 入2 — ʌl J	I 入2 — ʌl
(75)
and thus
s
X(1,1) ∙ GeBl
e=0
S	S
≤ Xλ2(b1-e + b2-e) + X
e=0	e=0
(12τL2b1-e + 4ρLL2b2-e)
(76)
Recall the definition of PL given by (31):
Pl = max ρPk (1 + Zk) ∙
k=1,...,K
By the Gershgorin,s theorem, since η > 0, we can upperbound λ2 as
λ2 ≤ max < max Pk(1 + Z-I) + η2Pl ∙ 8L2,Cτ + 12τη2L2
一	[k∈[K]	,
≤max {max1Pk (1+Z-1)+(4TPL1)3t，1+4⅛}，
where the last inequality is due to the bound on η:
(77)
(78)
η2 ≤ -------------.
—24τ (4τ — 1)L2
(79)
Define constant
We consider two cases.
• Case 1:
(80)
PmaX ≤ 1 — 一 ⇒	1	≤ T.
(81)
Thus Dτ,ρ = 1/(1 — Pmax). WeIet Zk = Pk/(1 - Pk) and it gives
max. Pk (1 + ZkI)= Pmax,
k∈[K]
PL
max
k = 1,...,K
PmaX
1 - Pmax
PmaXDT,ρ.
(82)
Substituting into the bound of λ2 [cf. (78)] gives
λ2 ≤ max P Pmax +
P2maX
(1 — PmaX)3t (4t — 1)
,1 + 4⅛
≤ max < 1 — ɪ +
1 - T )2
3(4t — 1), 1 + 4τ — 1
< 1 + 4⅛,
2
(83)
22
Published as a conference paper at ICLR 2022
where in the second inequality we used the condition (81).
Since S ≤ τ and λ2 ≥ 1, we obtain the following bound
S
X λ2b 尸 ≤
2=0
二)T)
≤ 3 ∙ (X b1
v=0
(84)
M+
Moreover, since
ρmax + η2pL ∙ 4L2 ≤ pmax +
_______PmaX________
(1 - Pmax)(4τ - 1)6τ
亭1-1+尸2
T 6(4τ — 1)
(85)
≤ CT,
we can bound λ2 一 ʌi as
λ2 — λ1 ≥ CT 一 Pmax 一 η2 PL ∙ 4L2
≥ CT 一
Pmax +
P2max
(1 - Pmax)(4τ - 1
(81)
≥ CT 一
Pmax + Pmax
1 — 1
T
6(4τ — 1)
(86)
1
≥ 1 + 4τ — 1
—
Pmax + Pmax
1
4τ — 1
(1 一 pmax) I 1 +
1
4τ — 1
≥
1 一 pmax∙
Collecting (84) and (86) we can bound (76) as
S
χ(1,1) ∙ GeBl
'=0
SS
≤ X(b1 + b2) ∙ 3 + X η2 ∙ (12τL2b1 +4pLL2b2) ∙3
e=0	e=0
SS
≤ X(b1 + 诚)∙ 3 + Xη2 ∙ (12τL2跌 + α4乂4乙2诚)∙皿#
e=0	e=0
(79)
≤ ∑(bf + 质)∙ 3 + E UM + DiPmax4b2) ∙ 3Dt,p
(87)
1
(4τ — 1)24τ
≤ X(b1 + b2) ∙ 3 + X (12τb1 + DT,ρPmax4b2) ∙ | ^TTP
e=0	e=0
=X(b1 + b2) ∙ 3 + X (12τb1 + DT,ρPmax4b2) ∙ 8 ^TTP ∙
e=0	e=0
23
Published as a conference paper at ICLR 2022
Substituting the expression of b`1 and b`2 gives
s	s	s D2
X(1,1) • GeBs-' ≤ X(b1 + b2) • 5 + X -ρr • Pmaxbb2
e=o	e=o	e=o
=X5 (4η2(PL + 3τ)(a2EkVf(Xr，'32 + eg) + 4η2ρLl⅛ + η2ρmaχσ2 + η2；σ2
e=o '	n
+ X ~τ~ ∙ pLax (12τη2g2Ekvf (Xr,')k2 + e2) + η2—σ2)
e=o T	∖	n ×
s
≤ X -21 η2 (PL + T + PmaXDT,ρ TT)(α2 EkVf (xr,')k2 + eg ) + -21 η2τPLeL
e=o
+ CITnlPmaXσ + CI (T + DT,ρτ-1 PmaX)η2 1 σ2
(88)
s
≤ X Cin2 (t + PmaXDτ,ρ) (α2Ek Vf(Xr,')k2 + eg) + Ciη2TpmaχDτ,ρ<e1
e=o
+ CITn2Pmmaxb2 + CI(T + DT,ρT-'Pmaxln2 nσ2
where Ci is some universal constant. The last inequality holds since PL = P2maXDτ,ρ and Dτ,ρ ≤ T.
• Case 2:
PmaX > 1 - T ⇒ Dτ,ρ = t.
In such a case, we let ζk = (4T - 1) and thus
max P2k(1 + ζk-i) = P2maX(1 + (4T - 1)-i),	PL = 4T P2maX = 4P2maXDτ,ρ.
k∈[K]
(89)
(90)
Substituting into the bound of λ2 given in (78), applying again the learning rate condition (79) and
using the fact that Dτ,ρ = T :
λ2 ≤ max P2maX(1 + (4T - 1)-i) +
4PmaX	ι +
3(4t - 1), +
≤1+
3
4t — 1
(91)
(4⅛) }
Therefore by (76), (79), (84), and the fact that
min ∖、1、, '] ≤ t = Dτ,ρ
λ2 - λi
we obtain
s
X(1,1) • GeBI
e=o
ss
≤ X(b1 + b2) • 3 + Xn2 • (12τL2b1 + 16PmaXDT,ρL2b2) • 3Dτ,ρ
s=0	e=0
≤ X(b1 + b2) • 3 + X (12τb1 + 16PmaXDτ,ρb2) 8^T2ρ
e=0	e=0	T
s	s	D2
≤ X(b1 + 电. 5 + X 2Df PmaXb2.
e=0	e=0	T
(92)
(93)
Substituting the expression of bi and b2 and using the fact that
PL = 4T PmaX = 4PmaXDτ,ρ
we arrive at the same bound as in Case 1, possibly with a different constant Ci.
24
Published as a conference paper at ICLR 2022
C.2 Proof of Proposition 2
We are in Case 1 described in the proof of Proposition 1. By letting Zk = Pk/(1 - Pk) We have
G
2
■/max ∙ η ∙ 4L2
1-ρmax
12τη2L2
Pmax
1-ρmax
• η2 ∙ 4L2
Cτ
(94)
and the folloWing bound on the difference of the eigenvalues of G:
λ2 - λ1 ≥ 1 - Pmax .
(95)
Notice that according to (72) and (148)
Therefore
1K
N X nW
k=1
1
12τη2L2(λ2 - λι)
k=1
1	τ-2
de⅛) X …
τ-2
X ((CT- λι) (λfl2τη2L2b12-' - λ1(λ2 - CT/-2-'))
'=0
1	τ-2
+ i2τη2L2(λ2 - λ1) X ((λ2 - CT) (λ212τη2 L2bτ-2- + λ2(CT - λI)bτ-2-))
(96)
1
’l2η2L2τ(1 - PmaX)
T-2
X ((CT- λI) λ1 (12τη2 L2bτ-2-' -(12 - CT )bτ-2-'))
'=0
1
12η2L2τ(1 - PmaX)
T-2
X ((λ2 - CT) λ (12τη2L2bT-2-' + (CT- λι)bT-2-')).
'=0
In the folloWing, We bound λ1 and λ2 - CT as a function of PmaX . For notation simplicity We omit
the subscript of PmaX in the rest of the proof. Further, We introduce the folloWing shorthand notation
for the elements of G :
22
f (ρ) = P +  -η2 • 4L2, g(ρ) =  -η2 • 4L2, and h(τ) = 12τη2L2.	(97)
1-P	1-P
Applying the Gershgorin’s theorem We obtain
λι ≥ min {p,Ct — 12τη2L2 } ≥ P ≥ 0,
and
λ2 ≤ max{f (P) + h(P),g(P) + CT}.
Under the learning rate condition (79) We can shoW
g(P) + CT - (f (P) + h(P))
22
=-P- • η2 • 4L2 + CT - P - -^― • η2 • 4L2 - 12τη2L2
1-P	1-P
31
=1 + 2 • 4 -1 - P - 12τη L ≥ 0.
(98)
(99)
(100)
Therefore,
λ2 - CT ≤ g(P).
+
25
Published as a conference paper at ICLR 2022
Substituting the bounds into (96) gives
1 K
N X
k = 1
1	τ-2
≤ m (I-P) X	(12τη2L2bT-2-' - (λ2 - CT )bτ-2-'))
1	T-2
+ 12η2L2τ(I-P) X ((λ2 - CT) λ2 (I2τη2L2bτ-2-' + CTbT-2-'))
1	t -2
≤ 12η2L2τ(I-P) X(CTf(P)' (12τη2L2L-'))
1	/-2
+ 12n2L2T(I-P) X (g(ρ)λ2 (12τη2L2bT-2-' + ST-2-'))
(101)
+ 匕g(P)λT (X bl)
+	2；TC	、MT (X小
12η2L2T(I-P)	\公)
≤Dt,pCt (X bl) +12D3l2p2 (X bl) + FP2DT,p (X b'
where We have used the bound λι ≤ f (P) < 1, λ > 1 and XT < 3.
Plug in the expression of bi and b2 and using the fact that CT < 2, PL = P2Dτ,ρ gives
k=1
τ-2
≤(2Dτ,ρ + 12D2,ρη2L2p2)p2 X (4η2Dτ,ρ(α2∣∣Vf 甲s)k2 + W) + 例汕温 + η2σ2)
s=0
+ DTPCT P2 X(12τη2(α2kVf (xr,s)k2 + W) + η2 °—)
τ	z—z	g n
s=0
D2	τ-2
≤(2Dτ,ρ + Df ρ2) XP2 (4η2Dτ,ρ(α2∣∣Vf/)∣∣2 + eg)+4η2Dτ温 + η2σ2)
s=0
D2	τ-2	1
+ 2-TPP2 X(12τη2(α2∣∣Vf (Ns)II2 + eg) + η21 σ2)
τ	z—z	g n
s=0
T-2
≤3Dτ,ρP2 X 的6”(0210(^2)112 + eg) + 4η2DT温 + η2σ2)
s=0
T-2	1
+ 2DT,ρP2τ-i X(12τη2(α2kVf(Xr,s)k2 + eg)+ η2 n°2)∙
s=0
(102)
26
Published as a conference paper at ICLR 2022
Tidy up the expression gives
K	τ-2
NN X kxr,⊥-1k2 ≤ X 3Dτ,ρP2 (4η2Dτ,ρ(α2kVf(Xr,s)k2 + W) + 4η2Dτ温 + η2σ2)
k=1	s=0
+ X 2D2,ρP2τT(12τη2m2kVf(Xr,S)k2 + eg) + η2 - σ2)
s=0	n
τ-2
≤ C X (D2,ρP2) η2(α2kVf(Xr,s)k2 + eg)
s=0
+ C2(Dτ,ρ)2 τρ2η2eL + C2τDτ,ρρ2η2σ2 + C2(Dτ,ρ)2ρ2η2 - σ2
τ-2
≤ 2C2 X DT,ρP2 η2(α2kVf (Xr,s)k2 + eg)+ C2(Dτ,ρ)2τρ2η 储
s=0
+ C (— DTP + ]) ρ2τDτ,ρη2σ2
for some C2 > 0.
(103)
C.3 Proof of Proposition 4
We prove (47) by splitting the terms A2,1 follows:
1 K
A2,1 =)E Np XX I(i ∈ Sr)( X (Wk )i,j (χj,τ-1 - ηVfj (Xj,τ-1) - Xk,τ-1)
k=1 i∈Vk	j∈Vk
2
(104)
2
≤2E
• -ηVfk (XkT-I)
1 K	2
+ 2EIINp XX I(i ∈ Sr 仍小
k=1 i∈Vk
where
rik，X (Wk)i,j(X『T-Xr-1-η(Vfj(xj,τ-1) - Vfk(Xk,τ-1)).
j∈Vk
Equality (a) holds since
K	KK
XXX (Wk)i,j Xk,τ-1 = XX Xk,τ-1 = X m • Xk,τ-1
k=1 i∈Skr j∈Vk	k=1 i∈Skr	k=1
K
=XpXxrr,τ-1 = NpXrrT-',
k=1 i∈Vk
and similarly,
XX
(Wk )i,j Vfk (Xk,τ-1) = -pVfk (XkTT).
i∈Skr j∈Vk
(105)
(106)
(107)
Since samples are taken according to the rule specified by Assumption 6, the following probabilities
hold:
p(i ∈	Sk	| i ∈	Vk)	=	p,	P(i,j ∈	Sk	l,i,j	∈ Vk) = P •	-p_1,	(108)
-- 1
P(i ∈ Sr, j ∈ Sr | i ∈ Vk, j ∈V',k = `) = P2.	(109)
Consequently, we can evaluate the second term in (104) and obtain
II 1 K	II2
A2,1=2E KKEnVfk(Xk,τ-1)
I k=1	I
27
Published as a conference paper at ICLR 2022
KK
+ (N⅛ E 卜 XX krikk2 + P ∙ np⅛ XX r> j
k=1 i∈Vk	k=1 i,j ∈Vk
+ (N⅛ ∙ P2 XXX E(rik j
k P'	k='i∈Vk j∈V'
=2E
1K
KK X ηVfk (χk,τ-1)
k=1
2
2	nP
+ (Np)2 (P ' V
k=1 i∈Vk
E l∑∑ E r>k rj'
∖k=' i∈Vk j∈V'
2
⅛∣h X X krikk2
k=1 i∈Vk
+
2	p(1 - p)
(N p)2 n - 1
≤2E
1K
KK X ηvfk (xk,τ-1)
k=1
≤2E
+
+
2
+ (NPy
E P ∙ np
n
+
2	p(1 - p)
(N p)2 n - 1
k=1 i∈Vk
K
(K -1)nXXEkrikk2.
k=1 i∈Vk
P(1 - P)n
n-1
K
XX krikk2
k=1 i∈Vk
(110)
By substituting the expression of rik we can bound terms
PkK=1 Pi∈Vk krikk2as
kPkK=1Pi∈Vkrikk2 and
k=1 i∈Vk
E(Wk)i,j (X厂 1 - x『-1
k=1 i∈Vk j∈Vk
-η(Vfj (χr,τ-1)-vfk (xk,T-1))『
2
η2
K
XXX (Wk )i,j (Vfj (χj,τ-1) - Vfk (XL))
k=1 i∈Vk j ∈Vk
η2
K
XXX (Wk)i,j (Vfj (XT-I)- Vfj (χk,τ-1))
k=1 i∈Vk j ∈Vk
2
K
≤ η2N XXX (Wk )i,j L2kχr,τ-1 - xk,τ-1k2
k=1 i∈Vk j∈Vk
KK
≤ η2NX X L2kχr,τ-1 - xrkτ-1k2 = η2 ∙ NL2X kxr,⊥-1k2
k=1 i∈Vk	k=1
(111)
28
Published as a conference paper at ICLR 2022
and
K
XX krikk2
k=1 i∈Vk
K
=XXX (Wk)i,j(X厂 1-xL-ηNfj(χj,τ-1) - Nfk(XT-I))
k=1 i∈Vk j∈Vk
2
K2
≤ XX X (Wk )i,j 卜厂1 - xk,τ-1 - η(Nfj D-Nfj (xk,τ-1))∣∣
k=1 i∈Vk j ∈Vk
≤ X X X (Wk)i,j(2 依TT-Xk,τ-1∣∣2 +2η2 ∣∣Nfj(X厂I)-Nfj(xD『
k=1 i∈Vk j ∈Vk
K	2	K
≤ XX 2 卜厂1 - Xkr1II + 2η2L2 XX kxr，T-1 - Xkr1 k2
k=1 i∈Vk
K
X2(1+η2L2)kXkr,,τ⊥-1k2.
k=1
k=1 i∈Vk
(112)
Tidy up the expression leads to the following bound of A2,1:
I 1 K	I2	2
A2,1 ≤2E KK X ηNfk(Xk,τ-1)	+(NpF
np - 1
n-1
EIIIXK X
k=1 i∈Vk
2	N K
+ (N )2p(1 p) ⊂Γ ∑∑Ekrikk2
(Np)2	n-1k=1i∈Vk
≤2E
1K
KK XηNfk(Xk,τ-1)
k=1
2
+ ~212 P∙ ∙ np-1 η2 ∙ NL2 X EkXr,⊥-1k2
(N p)2	n - 1	k,⊥
+ 京 p(1 - p) nN1 (X 2(1 + η2 L2)EkXr『k2
≤2η2E
1K
K X Nfk (Xk,τ-1)
k=1
22 K
+ N η2 L2X EkXr,⊥-1k2
k=1
K
+ Np⅛⅛ X(1 + η2L2)Ekχr,,⊥-1k2
p	k=1
I K	I2	K
≤2η2E K X Nfk(XL) + N (p(n⅛ + 声)XEkXr,⊥ k2
I k=1	I	p	k=1
(113)
where the last inequality holds under the learning rate condition
η2 ≤
1
24τ (4τ — 1)L2
(114)
This completes the proof of (47).
29
Published as a conference paper at ICLR 2022
C.4 Proof of Proposition 5
We bound A2,2 in following the same rationale as Proposition 4.
A2,2 =E
I(i ∈ Sr)(X (Wk)i,j(Vfj(Xjn- gjrs)『
j∈Vk
I
eik
(NF (p ∙ ≡1 ElIXX M2 + p(1 -PPn-1 X XEkeikk2
k=1 i∈Vk	k=1 i∈Vk
(115)
2	p(1 - p)
(N p)2 n - 1
E E ΣΣ eik j
1	np - 1
(NpP Ip ∙ ^n-ιE
∖k='i∈Vk j∈V'	)
K2	K
lllXX eiklll +p(1 -p)n --1XXEkeikk2
k=1 i∈Vk	k=1 i∈Vk
+
where the last equality is due to fact that the inter-cluster stochastic noise is zero mean and indepen-
dent.
Recall the definition ξj,s，gj,s - Vfi(Xr,s). Using again the independence of the ξi's We get
ElllXK X eiklll2 =	Elll	XK	XX(Wk)i,jξjr,slll2=ElllXN	ξir,slll2=Nσ2,	(116)
k=1 i∈Vk k=1 i∈Vk j ∈Vk	i=1
and
K	K	2 K
X X Ekeikk2 = X X Ell X (Wk)i,jξjr,sll2 = X kWkk2σ2
k=1 i∈Vk	k=1 i∈Vk	j ∈Vk k=1
Kn
= X σ 2 X dj2
k=1	j=1
K
≤ X σ2 (1 + (n — 1)ρk) ≤ Kσ2 + (n — 1) Kρmaxσ2
k=1
=(1 + (n - 1) Pmax) Kσ2.
(117)
where di ≤ d2 ≤ ∙∙∙ ≤ dn = 1 are the singular values of Wk. Therefore,
A2,2 ≤ (N F ip ∙ ~p^^T ^ Nσ2 + P(I - P)-( (1 + (n - 1) Pmax) Kσ2)
(N p)2	n - 1	n - 1	max
=N (np-4 σ2)+ N (n-τ(1 + (n -1) Pmax) σ2)
σ2	σ2 p-1 - 1	2	σ2	1 2
≤ N + Nn - I (1 + (n - 1) Pmax) ≤ N (2+ P Pmax
The last inequality is due to p ≥ 1/n.
(118)
30
Published as a conference paper at ICLR 2022
C.5 Proof of Proposition 6
Invoking the descent inequality Lemma 1 and the error bound for T1-T5 given by Corollary 2,
Proposition 3 and Eq. (41):
Ef(xr+1)
τ-1	τ-1	1 N	2
≤Ef(xr) - 4 XEkVf(Xr,s)k2 - 4 XEIlNN X Vfi(Xr，s)∣∣
s=0	s=0	i=1
τ-1	K
-4 X Ell KK X Vfk(Xk，s)
s=0	k=1
2
τ-1
+ CιL2η3τ (τ + PmaxDτ,ρ) X(α2kvf(χr,s)k2 + eg) + CιL2η3τ2ρmaχDτ,ρeL
s=0
2
+ Cιτ2L2η3ρmaxσ2 + CιL2(τ2 + D^PmUn3 彳
+
(119)
2
2L τ-1	l 1 N	l
⅞- XE NN XVfi(Xr,s)	+
τ -1
s=0
i=1
η2 L σ2
F TN
2
N
τ -1
≤Ef(xr) - 4 XEkVf(Xr,s)k2
4 s=0
τ-1
+ CιL2η3τ (τ + pmaxDτ,ρ) X(α2EkVf(Xr,s)k2 + eg) + CιL2η3τ2(mx-D>-rρeeL
s=0
22
+ Cιτ 2L2η3ρlaxσ2 + CιL2(τ2 + D^PmUn3 亍 + η2LτN
The last inequality holds under the condition that
1
η ≤ 2L.
If we further enforce
CIL2η3τ(τ + PPmXDTP) α2 ≤ 8	⇔	η2 ≤ 8CιL2τ (τ +ρmaxDτ,ρ) am，
then
Ef(Xr+1)
τ-1
≤Ef(Xr) - n XEkVf(Xr,s)k2
8
s=0
+ CιL2η3τ2 (τ + PmaxDQ eg + 孰^^丁？PmaxDτ,ρ就
22
+ Cιτ 2L2η3ρmaxσ2 + CιL2(τ2 + 叫曲黑^n彳 + η2Lτ^
τ-1
=Ef(Xr)-8 XEkVf(Xr,s)k2 + C1L2η3τ2 (τ + PmaxDτ,ρ) eg
8 s=0
22
+ CIL2η3 卜2Pmax(Dτ,ρeL + σ2 ) + (τ2 + DT,ρPmax)应)+ η2LTN
τ-1	2
≤Ef(Xr) - 8 XEkVf(Xr,s)k2 + η2LτN
8 s=0	N
+ CιL2η3τ2 (τ + PmaxDτ,ρ) eg + 2CιL2η3 }"^_。7苣 + τ2σ2 (； + Pmax))
(120)
(121)
(122)
the last inequality is due to Dτ,ρ ≤ T and Pmax ≤ 1.
31
Published as a conference paper at ICLR 2022
C.6 Proof of Proposition 7
Invoking the descent inequality Lemma 1 and the error bound for T1-T5 given by Corollary 2,
Proposition 3, and 4:
Ef(Xr+1)
τ-1	τ-1	1 N	2
≤ Ef (Xr) - 4 X EkVf (Xr,s)k2 - 4 X EIIN X Vfi(Xr，s)II
s=0	s=0	i=1
τ-1	K
-	4 X EIIK X Vfk (Xk，s)II
s=0	k=1
τ-1	1 N	2	τ-1	1 K	2
+ 4 X EIIVf (Xr,s) - N X Vfi(Xr,s)II + 4 X EIIVf (Xr,s) - K X Vfk (Xk，s)II
—
s=0	i=1	s=0
τ-1	N	K	2
+ 4 X EIIN X Vfi(Xr,s)- K X Vfk (Xk，s)II
s=0	i=1	k=1
+ X E (2kXr,s+1 -Xr,sk2) +E (2 kXr+i - Xr,τ-1k2
τ-1
k=1
—
τ-1
≤Ef(χr) - 4 XEkVf(Xr,s)k2 - 4 XE
N2
Il N X Vfi(Xr，S) Il
s=0	s=0	i=1
τ-1	K
-	4 XEIIK X Vfk (χk,s)∣∣
s=0	k=1
τ-1
+ CI L2n3τ (τ + PmaxDτ,ρ) X(α2kvf (Xr,s)k2 + e2 ) + ClL2n T 2 Pmns^xDrp^L
s=0
2
+ Ci τ 2L2n3ρmaxσ2 + CιL2(τ2 + D"ρmx)n 三
L2
+ 2 η
τ-2
τ-2
XE NN X"Hs)
s=0
i=1
I1
+ Ln2E K X Vfk (χk,τ-1
k=1
+ 2 (Gp +
2
L 2	σ2
+ 2η (T-1) N
N X Ekχr,⊥Tk2) + L n2 N (2+ mnρmax
k=1
(123)
K
N
2
)
Denote for short
Gp , Gp + T.
Further applying the bounds on the consensus error derived in Proposition 2:
Ef(Xr+1)
τ-1	τ-1	1 N	2
≤Ef (Xr)- 4 X EkVf (χr,s)k2 - 4 X Ell N X Vfi(Xr，s) II
s=0	s=0	i=1
τ-1	K
-	4 X EIIK X Vfk (Xk，s)II
s=0	k=1
(124)
τ-1
+ C1L2n3τ(τ + PmaxDτ,ρ) X(α2kvf(xr,s)k2 + eg) + C1L2n3τ2ρmaχ
s=0
Dτ,ρ
32
Published as a conference paper at ICLR 2022
2
+ Cιτ 2L2η3ρmaχσ2 + CiL2 (τ2 + 以请黑加亍
L2
+ 2 n2
τ-2
τ-2	N
XE NN XWi(Xr，s)
s=0
i=1
2
L 2 σ2
+ 2n (T- I) N
1 K
+ Lη2E N X Vfk(x'
rk,τ-1)
k=1
+ 4LGp (2C2 X DT,ρPmaxn2(α2∣∣Vf(Xr,s)∣∣2 + Eg))
+ 4LGp (C2D2,pTpm&xn生L + C2 (n-T,ρ + 1) PmaxTDτ,ρn2σ2
L σ2 n
+ 2n N (2+ mρmax)
(125)
Rearranging terms and tidy up the expression we have
Ef(Xr+1)
τ-1	τ-1
≤Ef (Xr) - 4 X EkVf (Xr,s)k2 - 4 X E
1
+ Ln2 E K £ Vfk (X1
s=0
2 + 2 n
2
τ-2
k=1
rk,τ-1)
τ-2
XE NN XVfi(Xn
s=0
i=1
N
K
K
2
N
2
τ -1
+ CiL2n3τ (τ + PmaxDτ,ρ) X(α2kVf(Xr,s)k2 + eg)
s=0
+ 8C2GpLDT,ρPmaxn2 (X(α2kVf(Xr,s)k2 + eg))
2
+ CiL2n3 (τ2Dτ,ρPma曷 + τ2ρmaxσ2 + (τ2 + Dg,ρρmax)σ-
L	2 σ2 L n 2	2 σ2
+ 2(T- 1)n N +2(2+ /'max) n N
+4C2 LGpn2 (DT,ρτρmax0L+(n-τ,ρ+1) PmaxTDτ,ρσ2)
Notice that if the following conditions on the learning rate are satisfied
η2
4 ≥ nL,
8 ≥ CIL臂T (T + PmaxDτ,ρ) α2 + 8C2 GpLD；PPmaxn2a2 ,
(126)
(127)
33
Published as a conference paper at ICLR 2022
then the terms associated to the gradients will be negative and
Ef(xr+1)
τ-1
≤Ef(xr) - η XEkVf(Xr,s)k2
8
s=0
+ ciL2η3τ2 (T + PmaXDT,ρ) eg + 2ClL2η3 卜2ρ2ma.χDτ,ρ^L + T2σ2 ^n + PmaX))
2
+ Lτη2 N + (8C2GpLDT,ρτpmax) η2eg
+4c2LGpη2 (。除丁*&乂 江 + (n~τ,ρ + 1) PmaX 丁。丁,。。2) + — (mρmax)η2 N
In the last step we clean the condition on the learning rate η. Collecting all the conditions on η:
η2 ≤ -------------,
—24L2τ (4τ — 1)
4 ≥ η2L,
8 ≥ C1L2η3τ (τ + PmaXDT,ρ) α2 +8C2GpLDT,ρρmaχη2α2.
Clearly, (128) implies (129). To ensure (130) it suffices to require
16 ≥ C1L2η3τ (τ + PmaXDT,ρ) α2
16 ≥ 8C2GpLD2,。*α乂力。2.
Recall the definition of G0p :
Gp = Gp + T2,	and Gp = m(n — 1).
It can be verified that if
1
η ≤ C3α2τL'
for some C30 > 0 large enough, then
16 ≥ C1L2η3τ (τ + PmaXDT,ρ) α2
3— ≥ 8C2 (T2) LDT,p^maX^a2.
It remains to guarantee
3― ≥ 8C2GpLDT,p^maX^a2.
Rearranging terms gives the condition
/	1
η ≤ 256C2GpD" PmmaXa2L.
(128)
(129)
(130)
(131)
(132)
(133)
(134)
(135)
Combining with (133) and using the fact that DT,ρPmaX ≤ T provides the final condition on η as
η≤
1
C3aTL
• min 1 1, C C1------
αGp DT,ρ PmaX
(136)
34
Published as a conference paper at ICLR 2022
D Supporting lemmas
D.1 PROOF OF LEMMA 1
Since the global average of the local copies follows the update [cf. (19)]:
1N
xr,s+1 = xr,s - 〃耳£成2, VS = 0, ... ,τ - 1.
(137)
i=1
Under Assumption 1, We can apply the descent lemma at points Xr,s+1 and Xr,s for S = 0,...,τ - 2,
conditioned on Fr,s-1:
Er,s-1f (Xr,s + 1) ≤ f (Xr,s) + Vf(Xr，s )τEr,s-1 化，* - ”) + E,,s-1 (^1"1-产产
(.)	1	LL
=f (xr,s) - ηVf(χr,S)T (N X Vfi(Xr,s)) + Er,s-1	2 WS+1 -xr,sk2
i=1	'
1	丁 / 1 A 一、
=f(χr,s) - -ηVf (xr.s)τ (N X Vfi(Xr,s))
i=1
—
1	- 1 ɪ	1 E .	、	/L	.
2ηVf (xr,s)τ (W X Vfi(WS)± K X Vfk(xk,s))+ Er,s-1 (2kxr,s+1-xr,sk2
i=1
(b)	1 I 1
≤f(xr,s)- 2	2皿(x*k2
2(1 kVf (f∣2
—
k = 1	'	/
Vfi(Xr,s)∣ I 2 -1∣∣ Vf (Xr's) - N X Vfi(Xr's)∣∣2
i=1	i=1
Vfk (Xk's)∣ ∣ 2 - 1∣∣Vf(Xr,s) - K X Vf(Xk,s)∣∣2)
k=1	k=1
1 N	1 K
+ 2 kVf(χr.s)k∣∣ N X Vfi(Xr,s)- K X Vfk (χk,s)
i=1	k=1
+ Er,s-1 (刍邛S+1-'/2
(C)	八	^ll 1 ɪ
≤f(Xr,s) - 4IVf(Xr's)k2 - 4片 X Vfi(xr's)
i=1
N
+ 4∣∣ Vf(Xr's) - N X Vfi(Xr,s)∣∣ + 4恒(Xr,s)-
i=1
1	N	1 K	2
+ 4∣∣N X Vfi(Xr，S) - K X Vfk (Xk，S)|| + Er,s-1
i=1	k=1
2
k = 1
1 K	2
K X Vfk (现S)Il
k = 1
(2Ws+1-Nsk2
where (a) is due to Assumption 2, (b) is due to 2ab = ∣∣α∣∣2 + ∣∣b∣∣2 -∣∣α - b∣∣2 and ab ≤ Ilalll∣b∣∣,
and (c) is due to Ilakkbk ≤ ɪ∣∣a∣2 + ɪ∣∣b∣2. Notation a ± b stands for adding and subtracting, i.e.,
a ± b = a + b — b.
For the pair (xr,τ-1,xr+1) we have according to (19) and (22):
Er,τ-2Xr + 1 = Er,τ-2(Xr" ) = Er,τ-2	厂1
1 N
1	r,τ-1
ηNt现
i=1
—
Applying the descent lemma in the same way as before yields
Er,τ-2f(xr+1)
≤f(Xr,τ-1) + Vf(XiT)TEr,τ-2(xr+1
xr,τ-1)+2 Er,τ-2∣χr+1
”T||2
—
—
35
Published as a conference paper at ICLR 2022
Taking expectation, summing over the iterations in round r over s = 0, . . . , τ - 1and using the fact
that xr = Xr,0 completes the proof.
D.2 Proof of Lemma 2
Recall the average update of the k-th cluster and that of the global average given by (15) and (19),
respectively, for s = 0, . . . , τ - 1:
r,s+1 r,s	r,s
Xk'	= Xk - η ∙ gk,
1N
Xr,s+1 = Xr,s - η ∙ N X gj,s
N i=1
(138)
(139)
Taking the difference gives
Ekxr,s+1 -Xk,s+1 k2
=e (Xr,s-Xk,s)-η(nXVfi(Xr,s)-NXVfi(Xr's)
i∈Vk
i=1
1 1
+η2E	n X ξr,s- N X ξr,s
i∈Vk
i=1
(140)
≤(1 + E)EkXr,s — xk,sk2 + (1 + e-1)η2E
1	1N
n X Wi (Xr) - N X W)
i∈Vk
i=1
N
N
2
2
2
1	1N
+η2E	n X ξr,s- N X ξr,s
2
i∈Vk
i=1
where E > 0 is some constant to be chosen.
Averaging over k = 1, . . . , K :
1K
K XEkxr,s+1-xk,s+1k2
k=1
N
1K
≤(i + E)改 EEkXr
k=1
1K
+ η2 ⅛ X E
1K
,s - xk,sk2 + (1 + e-1)η2 天 X E
k=1
1	1N
n X Wi (Xr) - N X Vfi(Xr，s)
i∈Vk
i=1
i∈Vk
i=1
2
2
1K
(=)(i+E) IK X EkXr,s - Xk,sk2
k=1
+ (1+E-1)η2(⅛ X Ell n X Vfi(Xr,s)『-Ell N X Vfi(Xr,s)∣∣2
k=1	i∈Vk
i=1
36
Published as a conference paper at ICLR 2022
+η2(⅛ X EII1X『II2-EK X『II2
∖ k=1	i∈Vk	i=1
1K
≤(i + E) K EEkxr
1K
=(1 + C)K ∑Ekxr
4 - Xk,sk2 + (1 + C-1)η2 (K Xe I I 1 X Vfi(Xk's) I ∣2) + η2KNIσ2
∖ k=1	i∈Vk	)
,s-xk,sk2
+ (1 + Cf2 (K X E|| n X (Wi(Xr,s)±Vfk(xk,s)±Vfk (Xr.s))∣∣2) + η2 KN1 σ2
∖	k=1	i∈Vk
1K
≤(1 + C)K X Ekxr,S-Xkn2 + (1 + C-1
k=1
)η2 (K XEII1 X (Vfi(Xk's)-vfk(Xkn)∣∣2
3K
+ (1 + C-1)η2 K X E ∣ ∣ Vfk (Xkn- Vfk(Xk,s)∣ ∣
k=1
l 2 K - 1 2
+η-σ
k=1	i∈V⅛
2	3 K
+ (1 + C-1)η2K EEkVfk(Xk,s)k2
k=1
⑹	1 _K	K JL	„2\
≤(1 + C)KXEkXk,s-Xk,sk2 + (1 + C-1)η2 INXL2E∣ χk,i∣∣
k=1	k=1
+ (1 + C-1)η2 W XL2E ∣ 后s - Xk,s∣∣2 + (1 + C-1 )η2" XE∣IVfk/)『+ η2
k=1	k=1
KK
= (1 + C + 3L2η2(1 + e-1)) K X EkXi - xk∏2 + (1 + e-1)η2	N X L2E∣∣X,
k=1	k=1
-r,s
k,⊥
K -1 2
N σ
Il2λ
+ (1 + C-1)η2KK XE ∣ ∣ Vfk(XkoI ∣ 2 + η2KNIσ2.
k=1
In (a) we used the fact that
1 L 1 L DF / k,sʌ 1 L ʊʃ / k,sʌ 1 L 1 L tk,s	1 L tk,s
KTn TVfi(Xi ) = N IyVfi(X'	KTnT ξi	= NUi
(141)
(142)
k=1 i∈Vk
and
i=1
k=1
i∈Vk	i=1
K
K
EkXi-Xk2 = EkXik2 —单区『with
i=1	i=1
In (b) we applied the L-smoothness of fi and f.
Choosing C =亓-ɪ and using the condition that
21
η2 ≤ -；------7
—24τ (4τ — 1)L2
1K
x= KEXi
k = 1
(143)
we have
1K
ɪ XEkXE1-Xk,s+1k2
k=1
≤ (1 + J + 2(4⅛) K XEkXk,s -*『+ 12τη2L2 (B Xe∣∣芍』2
37
Published as a conference paper at ICLR 2022
+ 12τη2KK XXEmk(xr,s)∣∣2 + η2KN-1 σ2
k=1
1 K	1 K	2
≤Cτ 斤 X Ekxr,s-xk,sk2 + 12τη2L2[ NN X EilXr,⊥∣∣
k=1	k=1
+ 12τη2 (a2EkVf(Xr,S)k2 + eg) + η2KN-1 σ2.
In the last inequality we applied Assumption 5 on the inter-cluster heterogeneity.
D.3 Proof of Lemma 3
We follow the perturbed average consensus analysis. Recall the update equation of the consensus
error given in (17):
Xkr,,s⊥+1 = (Wk -J)(Xkr,,s⊥ -ηGrk,s).	(144)
Squaring both sides and conditioning:
Ekxr,⊥+1k2 = E (E(k(Wk - J)(Xr,⊥ ± ηVFk(XrS) - ηGk,s)k2∣Fr,s-1))
≤ Ek(Wk -J)(Xkr,,s⊥ -ηVFk(Xkr,s))k2+η2ρ2knσ2
≤ Pk(1 + Z-1) ∙ EkXr,⊥k2 + Pk(1 + Zk)η2EkVFk(X[,s)k2 + η2ρknσ2,
where ζk > 0 is some free parameter to be properly chosen. Next, we bound the norm of the
pseudo-gradient VFk (Xkr,S).
kVFk(Xkr,S)k2= X kVfi(xir,S)k2
i∈Vk
=X kVfi(χr,s) ± Vfi(Xk) ± Vfk(xk,s) ± Vfk(xr,s)k2
i∈Vk
≤ X (4kVfi(xr,s) - Vfi(Xk)k2 +4kVfi(xk,s) - Vfk(xk,s)k2 + 4kVfk(xk,s) - Vfk(Xr,s)k2)
i∈Vk
+ X 4kVfk(Xr,s)k2	(145)
i∈Vk
≤ X(4kVfi(xk,s)-Vfk(xk,s)k2 +4L2kxr,s - Xk,sk2 +4L2kxk,s - xr,sk2 +4kVfk(Xr,s)k2)
i∈Vk
≤ 4L2kXr,⊥k2 +4L2nkxk,s -Xr,sk2 +4n∣∣Vfk(xr,s)k2 +4nek.
The last inequality is due to Assumption 4 on the intra-cluster heterogeneity.
Averaging over k = 1, . . . , K clusters:
1K
NN X EkXr,⊥+1 k2
k=1
1 K	1 K	1 K
≤N XPk(1 + Z-1)EkXr,⊥k2 + IN XPk(1 + Zk)η2EkVFk(Xr,s)k2 + η2 (K XPk) σ2
1K	1K
≤ N X Pk (1 + Z-I)EkXr,⊥k2 + η2 N X Pk (1 + Zk) ∙ 4L2E∣∣Xr,⊥∣∣2
k=1	k=1
1K	1K
+ η2KK EPk(1 + Zk)4L2Ekxk,s - Xr,sk2 + η2KK EPk(1 + Zk)4EkVfk(Xr,s)k2
k=1	k=1
1K
+ η2 K EPk (1 + Zk )4ek + η2 P2maxσ2	(146)
k=1
38
Published as a conference paper at ICLR 2022
1K
≤( max Pk(1 + Z-1) + η2 ∙ 4L2 max {pk(1 + Zk)} ) MX EkXrik2
∖k∈[κ] k'	k )	'	k∈[κ] i k' N J N 11 k,⊥"
`-----------'	k=1
{z
PL
C	- C	、	c 1 E ―	C
+ η2 max {pk(1 + Ck)} ∙ 4L 评 X Ekxk, - xr,s k2
k∈K	K k=1
1 K
+ η2 max, {Pk⅛(1 + ζk)} ∙ 4(α2EkV/(xr's)k2 + W) + η2 — EPk(1 + Zk)4^ + η2pLxb2.
k∈[K]	g	K z—z
k=1
D.4 PROOFOFLEMMA 4
To simplify the notation We omit the superscript in Br,s-` in this section.
Let Λ = diag(λι, λ2) and the eigendecomposition of G = TΛT-1, we can obtain the closed form
expression of T as
T _ ( λ1 - CT λ2 - Cτ∖
T = 112τη2L2 12τη2 L2J
and
T-1 _	1	( 12τη2L2	—λ2 + CT、
det(T) 1—12τη2L2	λ1 — CT J ,
where
det(T) = 12τη2L2(λ1 — λ2).
(147)
Consequently
det(T) ∙ G'B = det(T) ∙ TNT-1B
f 1— — CT	λ2 — CT、(ʌf	0、( 12τη2L2	—λ2 + CT、/‰、
(12τη2L2	12τη2L2J	10	λ“	1—12τη2L2 11— CT) [b2J
A1 — CT	λ2 — CtA	Af	0 ∖	(12τη2L2b1 + (—12 + CT)bΛ
(12τη2L2	12τη2L2J	10	λ^	^—12τη2L2b1 + (11 — CT)b2J	(148)
(11 — Ct 12 — Ct ʌ (1112τη2L2b1 + 11( — 12 + Ct州2、
(12τη2L2 12τη2L2J 1—1212τη2L2b1 + 1§ (11 — CT)b2J
(；2)
with
t1 = (11 — CT) (1112τη2L2 b1 + 1，(—12 + CT )b2)
+ (12 — CT) (—1212τη2L2b1 + 1$(11 — CT也),
t2 = 12τη2L2 (1112τη2L2b1 + 11(—12 + CT则)
+ 12τη2L2 (—1g12τη2L2b1 + 1g(11 — CT泡).
Therefore
det(T )(1,1)T MT-1B = t1 +12
=(11 — CT) (111L212τη2b1 + 11(—12 + CT泡)
+ (12 — CT) (—1gL212τη2b1 + 1g(11 — CT)b2)
+ L212τη2 (11L212τη2b1 + 11(—12 + CT)b?)
+ L212τη2 (—1gL212τη2b1 + 1g(11 — CT泡).
(149)
(150)
(151)
Substituting the expression of det(T) and dividing both sides of the equality by 12τη2(11 — 12) we
have
L2(1,1)T λ't-1b
39
Published as a conference paper at ICLR 2022
1
12τ η2(λ1 - λ2)
(λι — CT) (λfL212τη2bi + λ1 (―λ2 + CT)b2)
+ 75~~2Γ∖-----ΓX (λ2 - CT) (-λ2L212τη2 bi + λ2 (λi - CT )b2)
12τ η2(λ1 - λ2)
+ K-----Tʒ^L2 (λ1L212τη2bi + λ1(-λ2 + CT)b2)
(λi - λ2)
+ K-----T^^L2 (-λ2L212τη2bi + λ2(λi - CT)b2)
(λi - λ2)
=77----7-T (λi - CT) λιL2bi + K--(λ2 - CT) λ2L2bi
(λi - λ2)	(λi - λ2)
+ c \ 、L4λ112τη2bi -	1	L4λ212τη2bi
(λi - λ2)	(λi - λ2)
+ 12τη2(λι - λ2) (λi- Ct) λ1(-λ2 + Ct)b2 + 12τη2(λι - λ2) (λ2 - Ct) λ2(λi - Ct)b2
+ F-----——L2λ1(-λ2 + CT)b2 + γτ---——L2λ2(λi - CT)b2
(λi - λ2)	(λi - λ2)
=K-----rʒ (-λ2(λ2 - CT) - λ1(Cτ - λi)) L2bi + γτ--^^12τη"λ1 - λ2)L4bi
(λi - λ2)	(λi - λ2)
1 12τη2(λi -I2)(λi - Cτ) λ1(-λ2+Cτ )b2 +12τη2(λi -而(λ2 - CT) λ2(λi - CT)b2
+ 77-----^∙L2λ1(-λ2 + Cτ)b2 + F---——L2λ2(λi - Cτ)b2
(λi - λ2)	(λi - λ2)
≤λ2L2bi + λ2-λ1 ∙ 12τη2L4bi + λ2--41 (λ2 - CT)(CT - λi)-1^b2 + λ2L2b2,	(152)
λ2 - λi	λ2 - λi	12τη2
where in the last inequality we used the fact that λi ≤ CT ≤ λ2 .
Note that
(λ2 - CT )(CT - λi )
-	CT2 -λiλ2 +(λi +λ2)CT
-	CT2 - det(G) + Tr(G)CT
-	CT - (CT(max ρk(I + Z-I) + η2PL ∙ 4L2) - 48PLτη4L4)	(153)
k∈[K]
+	CT( max ρk (1 + Z-I) + η2ρL ∙ 4L2 + CT)
k∈[K]
48PLτη4L4.
Therefore, we further obtain
L2(1,1)T Λ'T-1B
≤λ2L2(bι + b2) + λ2--41 η2 ∙ (12τL4bι + 4ρLL4b2)
λ2 - λi
(154)
Dividing both sides by L2 completes the proof.
E	Network Connectivity Conditions in Theorems and Corollary
Both Theorem 2 and Corollary 1 impose some sufficient conditions on the network connectivity
Pmax for convergence. This can be satisfied in practice as follows. For Theorem 2, as long as
Pmax < 1, we can choose τ large enough so that (7) is fulfilled. Corollary 1 strengthens the result of
Theorem 2 by requiring no loss in the order of convergence rate compared to full device participa-
tion. This naturally leads to a more stringent condition on Pmax given by (11). For any given D2D
network topology, this can be satisfied by running multiple D2D gossip averaging steps per SGD
40
Published as a conference paper at ICLR 2022
update in Algorithm 1. Since the right hand side of (11) depends only on the algorithmic parame-
ters, we can choose the suitable gossip averaging steps to fulfill this condition before launching the
algorithm.
F More Experimental Details
In this section, we provide additional experimental results on CIFAR-10 dataset. We follow the
same CNN model and non-iid data partition strategy as before and run each experiments for 3 times
with different random seeds to report the mean values of best test accuracy. Instead of using a
constant learning rate, we decay the local learning rate η by half after finishing 50% and 75% of the
communication rounds and tune the initial learning rate from {0.01, 0.02, 0.05, 0.08, 0.1} for each
algorithm.
Ooooooo
7 6 5 4 3 2 1
Ausnzo<⅛QL
0	20	40	60	80
Round Number
(a)
0	25	50	75	100	125
Runtime (Hour)
(b)
Figure 4: Convergence rate and runtime comparisons of HL-SGD and local SGD on CIFAR-10 under ER
random D2D network topology. The device sampling ratio p = 1/8 and local iteration period τ = 50.
∣→- HL-SGD(Z=I)
HL-SGD (/=2)
HL-SGD (/=5)
T- HL-SGD (/=10)
T- Local SGD
0	20	40	60	80	0	20	40	60	80	100
Round Number	Runtime (Hour)
(a)	(b)
Figure 5: Convergence rate and runtime comparisons of HL-SGD and local SGD on CIFAR-10 under ring
topology and multiple SGD updates before gossip averaging. The device sampling ratio p = 1, and the local
iteration period τ = 50.
First, we evaluate the convergence processes of HL-SGD and local SGD under varying D2D network
topologies in Figure 4. We generate random network topologies by Erdos-Renyi model with edge
probability from {0.2, 0, 5, 0.8, 1} and use Metropolis-Hastings weights to set Wk, corresponding to
spectral norm ρmax = {0.9394, 0.844, 0.5357, 0}. As observed in Figure 4a, a more connected D2D
network topology (i.e., a smaller value of ρmax) generally accelerates the convergence and leads to
a higher model accuracy achieved over 100 communication rounds in HL-SGD. However, in terms
of runtime, a more connected D2D network topology corresponds to a larger D2D communication
delay cd2d per round, and hence the total runtime is larger as well, which can be clearly observed
in Figure 4b. Therefore, to achieve a target level of model accuracy within the shortest time in HL-
SGD, a sparse D2D network topology could work better than the fully connected one in practice.
41
Published as a conference paper at ICLR 2022
Second, we consider an extension of HL-SGD by allowing each device to perform multiple SGD
updates before the gossip averaging step in Algorithm 1 and empirically evaluate its performance.
Specifically, each device performs l = {1, 5, 10} steps of SGD update before aggregating models
with their neighbors in the same cluster. Note that l = 1 corresponds to the original version of HL-
SGD in Algorithm 1. As observed in Figure 5a, when communicating and aggregating models with
neighbors more frequently, HL-SGD with l = 1 has the best convergence speed and will converge
to the highest level of test accuracy. In terms of runtime, choosing a value of l > 1 might be
favorable in some cases due to the reduced D2D communication delay per round. For instance, to
achieve a target level of 60% test accuracy, HL-SGD with l = 5 needs 5.22% less amount of time
than l = 1. It is an interesting direction to rigorously analyze the convergence properties of HL-
SGD with arbitrary l and find the best hyperparameter tuning method for minimizing the runtime to
achieve a target level of model accuracy in the future.
G Relationship between Spectral Gap and Network Topology
ring/path 2D-grid 2-D torus Erdos-Renyi exponential
1 — P O(1∕n2)	θ(ɪ/(nlogn))	O(1∕n)	O(1)	O(1/ log(n))
Table 2: Spectral gap of some commonly used graphs, where n denotes the number of nodes. Results are taken
from (Nedic et al. (2018); Ying et al. (2021)).
42