Published as a conference paper at ICLR 2022
IMPROVED DETERMINISTIC l2 ROBUSTNESS ON
CIFAR-10 AND CIFAR-100
Sahil Singla1, Surbhi Singla2, Soheil Feizi1
University of Maryland, College Park
{ssingla,sfeizi}@umd.edu1 , surbhisingla1995@gmail.com2
Ab stract
Training convolutional neural networks (CNNs) with a strict Lipschitz constraint
under the l2 norm is useful for provable adversarial robustness, interpretable gradi-
ents and stable training. While 1-Lipschitz CNNs can be designed by enforcing
a 1-Lipschitz constraint on each layer, training such networks requires each layer
to have an orthogonal Jacobian matrix (for all inputs) to prevent the gradients
from vanishing during backpropagation. A layer with this property is said to
be Gradient Norm Preserving (GNP). In this work, we introduce a procedure to
certify the robustness of 1-Lipschitz CNNs by relaxing the orthogonalization of
the last linear layer of the network that significantly advances the state of the art
for both standard and provable robust accuracies on CIFAR-100 (gains of 4.80%
and 4.71%, respectively). We further boost their robustness by introducing (i) a
novel Gradient Norm preserving activation function called the Householder acti-
vation function (that includes every GroupSort activation) and (ii) a certificate
regularization. On CIFAR-10, we achieve significant improvements over prior
works in provable robust accuracy (5.81%) with only a minor drop in standard
accuracy (-0.29%). Code for reproducing all experiments in the paper is available
at https://github.com/singlasahil14/SOC.
1	Introduction
Given a neural network f : Rd → Rk, the Lipschitz constant1 Lip(f) enforces an upper bound on
how much the output is allowed to change in proportion to a change in the input. Previous work has
demonstrated that a small Lipschitz constant is useful for improved adversarial robustness (Szegedy
et al., 2014; Cisse et al., 2017), generalization bounds (Bartlett et al., 2017; Long & Sedghi, 2020),
interpretable gradients (Tsipras et al., 2018) and Wasserstein distance estimation (Villani, 2008).
Lip(f) also upper bounds the increase in the norm of gradient during backpropagation and can thus
prevent gradient explosion during training, enabling us to train very deep networks (Xiao et al., 2018).
While heuristic methods to enforce Lipschitz constraints (Miyato et al., 2018; Gulrajani et al., 2017)
have achieved much practical success, they do not provably enforce a bound on Lip(f) globally and
it remains challenging to achieve similar results when Lip(f) is provably bounded.
Using the property: Lip(g ◦ h) ≤ Lip(g) Lip(h), the Lipschitz constant of the neural network can
be bounded by the product of the Lipschitz constant of all layers. While this allows us to construct
1-Lipschitz neural networks by constraining each layer to be 1-Lipschitz, Anil et al. (2018) identified
a key difficulty with this approach. Because a 1-Lipschitz layer can only reduce the norm of gradient
during backpropagation, backprop through each layer reduces the gradient norm, resulting in small
gradient values for layers closer to the input, making training slow and difficult. To address this
problem, they introduce Gradient Norm Preserving (GNP) architectures where each layer preserves
the gradient norm during backpropagation. This involves constraining the Jacobian of each linear
layer to be an orthogonal matrix and using a GNP activation function called GroupSort. GroupSort
activation function (Anil et al., 2018) first separates the vector of preactivations z ∈ Rm into groups
of pre-specified sizes, sorts each group in the descending order and then concatenates these sorted
groups. When the group size is 2, the resulting activation function is called MaxMin.
1Unless specified, we assume the Lipschitz constant under the l2 norm in this work.
1
Published as a conference paper at ICLR 2022
For 1-Lipschitz CNNs, the robustness certificate for a sample X from class l is computed as
Mf (x)/√2 where Mf(X)=力(x) - maxi6=l fi(X). Naturally, larger values of fl(X) and smaller
values of maxj6=l fj(X) will lead to larger certificates. This requires the last weight matrix in the
network, denoted by W ∈ Rk×m (k is the number of classes, m is the dimension of the penultimate
layer, m > k), to enforce the following constraints throughout training:
∀j, kWj,:k2 = 1,	i 6=j, Wj,: ⊥ Wi,:
where Wi,: denotes the ith row of W. Now suppose that for some input image with label l, we want
to update W to increase the logit for the lth class. Since kWl,: k2 is constrained to be 1, the logit can
only be increased by changing the direction of the vector Wl,:. Because the other rows {Wi,:, i 6= l}
are constrained to be orthogonal to Wl,:, this further requires an update for all the rows of W. Thus
during training, any update made to learn some class must necessitate the forgetting of information
relevant for the other classes. This can be particularly problematic when the number of classes k and
thus the number of orthogonality constraints per row (i.e., k - 1) is large (such as in CIFAR-100).
To address this limitation, we propose to keep the last weight layer of the network unchanged. But
then the resulting function is no longer 1-Lipschitz and the certificate Mf (x)/ √2 is not valid. Thus,
we introduce a new certification procedure that does not require the last weight layer of the network
W to be orthogonal. Our certificate is then given by the following equation:
min 力(X)-fi(X)
i=ι kWi,：- Wi,： k
2
However, a limitation of using the above certificate is that because the weight layers are completely
unconstrained, larger norms of rows (i.e., kWi,： k) can result in larger values of kWl,： - Wi,： k2 and
thus smaller certificate values. To address this limitation, we normalize all rows to be of unit norm
before computing the logits. While this still requires all the rows to be of unit norm, their directions
are now allowed to change freely thus preventing the need to update other rows and forgetting of
learned information. We show that this provides significant improvements when the number of classes
is large. We call this procedure Last Layer Normalization (abbreviated as LLN). On CIFAR-100, this
significantly improves both the standard (> 3%) and provable robust accuracy (> 4% at ρ = 36/255)
across multiple 1-Lipschitz CNN architectures (Table 1). Here, ρ is the l2 attack radius.
Another limitation of existing 1-Lipschitz CNNs (Li et al., 2019b; Trockman & Kolter, 2021; Singla
& Feizi, 2021) is that their robustness guarantees do not scale properly with the l2 radius ρ. For
example, the provable robust accuracy of (Singla & Feizi, 2021) drops 〜30% at P = 108/255
compared to 36/255 on CIFAR-10 (Table 2). To address this limitation, we introduce a certificate
regularization denoted by CR (Section 5) that when used along with the Householder activation
results in significantly improved provable robust accuracy at larger radius values with minimal loss in
standard accuracy. On the CIFAR-10 dataset, we achieve significant improvements in the provable
robust accuracy for large ρ = 108/255 (min gain of +4.96%) across different architectures with
minimal loss in the standard accuracy (max drop of -0.56%). Results are in Table 2.
Additionally, we characterize the MaxMin activation function as a special case of the more general
Householder (HH) activations. Recall that given z ∈ Rm , the HH transformation is a linear function
reflecting z about the hyperplane vTX = 0 (kvk2 = 1), given by (I - 2vvT)z where I - 2vvT is
orthogonal because kvk2 = 1. The Householder activation function σv is defined below:
z,	vTz > 0,
σv(z)= (I - 2vvT)z,	vTz≤0.
(1)
First, note that since z = (I - 2vvT )z along vTz = 0, σv is continuous. Moreover, the Jacobian
▽z σv is either I or I - 2vvT (both orthogonal) implying σv is GNp Since these properties hold
∀ v : kvk2 = 1, v can be learned during the training. In fact, we prove that any GNP piecewise linear
function that changes from Q1z to Q2z (Q1, Q2 are square orthogonal matrices) along vTz = 0
must satisfy Q2 = Q1 I - 2vvT to be continuous (Theorem 1). Thus, this characterization proves
that every GroupSort activation is a special case of the more general Householder activation function
(example in Figure 1, discussion in Section 6).
In summary, in this paper, we make the following contributions:
2
Published as a conference paper at ICLR 2022
•	We introduce a certification procedure without orthogonalizing the last linear layer called Last
Layer Normalization. This procedure significantly enhances the standard and provable robust
accuracy when the number of classes is large. Using the LipConvnet-15 network on CIFAR-100,
our modification achieves a gain of +4.71% in provable robust accuracy (at ρ = 36/255) with a
gain of +4.80% in standard accuracy (Table 1).
•	We introduce a Certificate Regularizer that significantly advances the provable robust accuracy
with a small reduction in standard accuracy. Using LipConvnet-15 network on CIFAR-10, we
achieve +5.81% improvement in provable robust accuracy (at ρ = 108/255) with only a -0.29%
drop in standard accuracy over the existing methods (Table 2).
•	We introduce a class of piecewise linear GNP activation functions called Householder or HH
activations. We show that the MaxMin activation is a special case of the HH activation for certain
settings. We prove that Householder transformations are necessary for any GNP piecewise linear
function to be continuous (Theorem 1).
2	Related work
Provably Lipschitz convolutional neural networks: The class of fully connected neural networks
(FCNs) which are Gradient Norm Preserving (GNP) and 1-Lipschitz were first introduced by Anil
et al. (2018). They orthogonalize weight matrices and use GroupSort as the activation function
to design each layer to be GNP. While there have been numerous works on enforcing Lipschitz
constraints on convolution layers (Cisse et al., 2017; TsUzUkU et al., 2018; Qian & Wegman, 2019;
Gouk et al., 2020; Sedghi et al., 2019), they either enforce loose Lipschitz bounds or are not scalable
to large networks. To ensUre that the Lipschitz constraint on convolUtional layers is tight, mUltiple
recent works try to constrUct convolUtion layers with an orthogonal Jacobian matrix (Li et al., 2019b;
Trockman & Kolter, 2021; Singla & Feizi, 2021). These approaches avoid the aforementioned issUes
and allow the training of large, provably 1-Lipschitz CNNs while achieving impressive resUlts.
Provable defenses against adversarial examples: A provably robUst classifier is one for which
we can gUarantee that the classifier’s prediction remains constant within some region aroUnd the
inpUt. Most of the existing methods for provable robUstness either boUnd the Lipschitz constant
of the neUral network or the individUal layers (Weng et al., 2018; Zhang et al., 2019; 2018; Wong
et al., 2018; Wong & Kolter, 2018; RaghUnathan et al., 2018; Croce et al., 2019; Singh et al., 2018;
Singla & Feizi, 2020; Zhang et al., 2021; 2022; Wang et al., 2021; HUang et al., 2021). However,
these methods do not scale to large and practical networks on the ImageNet dataset (Deng et al.,
2009). To scale to sUch large networks, randomized smoothing (LiU et al., 2018; Cao & Gong,
2017; LecUyer et al., 2018; Li et al., 2019a; Cohen et al., 2019; Salman et al., 2019; Levine et al.,
2019; KUmar et al., 2020a;b) has been proposed as a probabilistically certified defense. However,
certifying robUstness with high probability reqUires generating a large nUmber of noisy samples
leading to high inference-time compUtation. In contrast, the defense we propose is deterministic and
hence not comparable to randomized smoothing. While Levine & Feizi (2021) provide deterministic
robUstness certificates Using randomized smoothing, their certificates are in the l1 norm and not
directly applicable for the l2 threat model stUdied in this work. We discUss the differences between l1
and l2 certificates in Appendix Section C.
3	Problem setup and Notation
For a vector v, vj denotes its jth element. For a matrix A, Aj,: and A:,k denote the jth row and kth
colUmn respectively. Both Aj,: and A:,k are assUmed to be colUmn vectors (thUs Aj,: is the transpose
of jth row of A). Aj,k denotes the element in jth row and kth colUmn of A. A:j,:k denotes the
matrix containing the first j rows and k colUmns of A. The same rUles are directly extended to higher
order tensors. I denotes the identity matrix, R to denote the field of real nUmbers. For θ ∈ R, J+ (θ)
and J-(θ) denote the orthogonal matrices with determinants +1 and -1 defined as follows:
J+(θ)
cos θ sin θ
- sin θ cos θ
J-(θ)
cos θ sin θ
sin θ - cos θ
(2)
We constrUct a 1-Lipschitz neUral network, f : Rd → Rk (d is the inpUt dimension, k is the
nUmber of classes) by composing 1-Lipschitz convolUtion layers and GNP activation fUnctions.
3
Published as a conference paper at ICLR 2022
To certify robustness for some input x with prediction l, we first define the margin of prediction:
Mf (x) = max (0, fl(x) - maxi6=l fi(x)) where fi(x) is the logit for class i and l is the correct
label. Using Theorem 7 in Li et al.(2019b), We can derive the robustness certificate (in the l2 norm)
as Mf (x)/√2. Thus, the l2 distance of X to the decision boundary is lower bounded by Mf (x)/√2:
min
i6=l
min	kx*
fi(x*) = fl(x*)
- Xk2
Mf(X)
√2
(3)
We often use the abbreviation fi - fj : RD → R to denote the function so that:
(fi - fj) (X) = fi(X) - fj (X),	∀X ∈ RD
Our goal is to train the neural network f to achieve the maximum possible provably robust accuracy
while also simultaneously improving (or maintaining) standard accuracy.
4	Last Layer Normalization
To ensure that the network is 1-Lipschitz so that the certificate in equation (3) is valid, existing
1-Lipschitz neural networks require the weight matrices of all the linear layers of the network to be
orthogonal. For the weight matrix in the last layer of the network (that maps the penultimate layer
neurons to the logits), W ∈ Rk×m (k is the number of classes, m is the dimension of the penultimate
layer, m > k), this enforces the following constraints on each row Wi,: ∈ Rm :
∀j, kWj,:k2 = 1,	∀i 6=j, Wj,: ⊥ Wi,:	(4)
Now suppose that for some input X with label l, we want to update W to increase the logit for the
lth class. Since kWl,: k2 is constrained to be 1, the gradient update can only change the direction of
the vector Wl,:. But now, because the other rows {Wi,:, i 6= l} are constrained to be orthogonal to
Wl,:, this further requires an update for all the rows of W. This has the negative effect that during
training, any update made to learn some class must necessitate the forgetting of information relevant
for the other classes. This can be particularly problematic when the number of classes k is large (such
as in CIFAR-100) and thus the number of orthogonality constraints per row i.e. k - 1 is large.
To address this limitation, first observe that the neural network from the input layer to the penultimate
layer (i.e excluding the last linear layer) is 1-Lipschitz. Let g : Rd → Rm be this function so that
f(X) = Wg(X) + b. This equation suggests that even if W is not orthogonal, the Lipschitz constant
of the function fl - fi, can be computed by multiplying the Lipschitz constant of g (which is 1) and
that of (Wl,: - Wi,:) (which is kWl,: - Wi,: k2). The robustness certificate can then be computed
as Mf (X)/kWl,: - Wi,: k2. This procedure leads to the following proposition:
Proposition 1. Given 1-Lipschitz continuous function g : Rd → Rm and W ∈ Rk×m, b ∈ Rk (k
is the number of classes), construct a new function f : Rd → Rk defined as: f (X) = Wg(X) + b.
Let fl(X) > maxi6=l fi(X). The robustness certificate (under the l2 norm) is given by:
min
i6=l
min ∣∣x*
fl(x*)=fi(x*)
- xk2
≥ min 力(X)- fi(X)
≥ i=ι kWι,: — Wi,:k
2
Proof is in Appendix Section A.1.
However, in our experiments, we found that using this procedure directly i.e without any constraint
on the weight matrix W often results in large norms of row vectors kWi,: k2, and thus large values
of kWl,: - Wi,: k2 and smaller certificates (Theorem 1). To address this problem, we normalize all
rows of the matrix to be of unit norm before computing the logits so that for the input X, the logit
gi (X) can be computed as follows:
gi(X)
(Wi,:)T f(x)
kWi,：k2
+ bi
The robustness certificate can then be computed as follows:
min —g(X) - gi(X)一	where∀j W(n) = Wj，: σ-(χ) = (w(n)∖T f(χ) + b
m=n kw(n)-w(n)k2,	where ∀j, Wj,: = ∣∣Wj,J∣2,gj (X)=(W" f(X) + bj
While each row Wi,: is still constrained to be of unit norm, unlike with orthogonality constraints
(equation (4)), their directions are allowed to change freely. This provides significant improvements
when the number of classes is large. We call this procedure Last Layer Normalization (LLN).
4
PUbHShed as a COnferenCe PaPer at ICLR 2022
(a) CaSeh ZI SiiI(6/2)——N2 COS(6/2) V 0
(b) CaSe N ZI SilI(6/2)——N2 COS(6/2) ≤ 0
FigUre h Hlustration Of the HOUSehoIder activatiopQ6∙ In each COIOred regiopQ6 is l5∙ear∙ The
JaCObian is IWhen (Zl 32) HeSin the pink region (CaSe I) and I ——2VVN in the OtherregiOn (CaSe 2)
T
Where V= -Sin(6/2) I COS(6/2)一 ∙ BOth Of these matrices are OrthOgOnaIimPIying Qφ is GNP∙
5 CERTlFICATE REGULARIZATION
A Hmitation Of USing CrOSS entroPyOSS for training I—Lipschitz CNNSiS that it is Ilot explicitly
designed to maximize the margin Λ4~ (X) and thu∞the robustness Cert≡cate∙ Thati∞once the CrOSS
entroPyOSS becomes smanythe gradients Wi= Ilolongertry to further5∙crease the margin Λ41 (X)
even though the network may have the CaPaCity to Ieam bigger margins∙
TO address this HmitatiOpwe Can SimPly SubtraCtthe CertiHCate Le IΛ4f (X)∕ΛZ^ from the usual
CrOSS entroPyOSS function during training。ObSerVe that we SUbtraCtthe Cert≡cate because WeWanttO
maximize the Cert≡cate ValUeS WhiIe minims:ng the CrOSS entroPyOSS ∙ HOWeVeL in OUr experiments
We found that this regularization term excessively PenaHZeS the network for the misclass≡ed examples
and as a resulLthe Cert≡cate ValUeS for the COrreCtly CIaSS≡ed inputs are not large∙ τhu∞we propose
to USe the following regularizedOSSfUnCtiOn during trainings
s≡n IE(Xɔ〜D CCMX)J) —— qrelu (5)
In the above equatiop fg denotes the 1—Lipschitz neural network ParametriZed byPTQ(X) denotes
the logits for the input X》H (fQ(XL Z) is the CrOSS entroPyOSS for input X WithIabelZ and q V。is
the regularization COefHCient for maximizing the CertiHCate∙ We have the minus Sign5∙front Of the
regularization term q relu(Λ4 - (X) / √¾ because We WanttO maxim 慧 the Cert≡cate while minims:ng
the CrOSS entropyoss∙ FOrWrOngly CIaSS≡ed input∞Λ4- (X) / 台 Λ O relu(Λ4 - (X) / 荏)H O-
ThiS ensures that the OPtimiZatiOn tries to increase the CertiHCateS only for the COrreCtly ClaSSiHed
inputs∙ We call the above mentioned PrOCedUre CertiACate RegIdarqatiOn (abbreviated as CR).
6 HOUSEHOLDER ACTlVATlON FUNCTIONS
ReCalIthat given Z ∈ IR≡the HOUSeholder (HH) transformation reflects Z about the hyperplane
VNX = O Where =V = 2 = L The linear transformation is given by the equation (I ——2vvτ)z Where
I ——2 VVN is OrthOgOnaI because =V = 2 = L NOWCOnSider the nonlinear function QV defined beowi
DefinitiOn L(HOUSehOIderACHVaHOn Oforderl) The aciivaiion funCiiOn QV - IR≡applied
OnNQ 而Tny -S Called the m—dimensional HOllSeholderACiiVaiiorI Oforderd
' J ——R VNZ Vɔ
QV(Z)—— hi —2vvτκV7z≤o∙ (6)
Since QV is linear When VNZ VOOr VNZ 八Pit is also COnts∙uous in both cases∙ Atthe hyperplane
SeParat5∙g the two CaSeS (LejVTZ = O) We have (I ——2vvτ)z = z ——2(vτz)v = Z (both Hnear
functions are equal)，Thus〉QV is COntinUOUS < N m ^≡∙ MOreOVeLthe JaCObian is either I S
Published as a conference paper at ICLR 2022
I - 2vvT which are both square orthogonal matrices. Thus, σv is also GNP and 1-Lipschitz. Since
these properties hold for all v satisfying kvk2 = 1, v can be made a learnable parameter.
While the above arguments suggest that HH transformations are sufficient to ensure such functions
are continuous, we also prove that they are necessary. That is, we prove that if a GNP piecewise
linear function g : Rm → Rm transitions between different linear functions Q1z and Q2z (in an
open set S ⊂ Rm) along a hyperplane vTz = 0 (where kvk2 = 1), then g is continuous in S if
and only if Q2 = Q1(I - 2vvT). This theoretical result provides a general principle for designing
piecewise linear GNP activation functions. The formal result is stated in the following Theorem:
Theorem 1. Given an open set S ⊂ Rm, orthogonal square matrices Q1 6= Q2, and vector v ∈ Rm
(kv∣∣2 = 1) such that S ∩{z : VTZ = 0} = ",thefUnction g defined asfollows:
Q1z,	z ∈ S, vTz > 0,
g(z) = Q2z,	z∈S,vTz≤0	(7)
is continuous in S if and only ifQ2 = Q1 (I - 2vvT).
Proof of Theorem 1 is in Appendix A.2. Note that since the matrix I - 2vvT has determinant -1,
the above theorem necessitates that det(Q1) = - det(Q2) i.e the determinant of the Jacobian must
change sign whenever the Jacobian of a piecewise linear GNP activation function changes.
Recall that for the MaxMin activation function, MaxMin(z1, z2) = (z1, z2) if z1 > z2 and (z2, z1)
otherwise. Thus, the Jacobian of MaxMin for z1 > z2 case is I = J+ (0) while for z1 ≤ z2 is
J- (n/2). Using Theorem 1, We can easily prove that MaxMin is a special case of the more general
Householder activation functions where the Jacobian J-(∏∕2) is replaced with J-(θ) and the
conditions zι > z2 are replaced with zι sin(θ∕2) > z2 cos(θ∕2) (similarly for ≤). The construction
of Householder activations in 2 dimensions, denoted by σθ , is given in the following corollary:
Corollary 1. The function σθ : R2 → R2 defined as
σθ (z1, z2)
if z1 sin (θ∕2) - z2 cos (θ∕2) > 0
(8)
if z1 sin (θ∕2) - z2 cos (θ∕2) ≤ 0
is continuous and is called 2D Householder Activation of Order 1.
The two cases are demonstrated in Figure 1a and Figure 1b, respectively. Since σθ is continuous, GNP
and 1-Lipschitz ∀ θ ∈ R, θ is a learnable parameter. For θ = π∕2 in equation (8), σθ is equivalent to
MaxMin. Thus, σθ is at least as expressive as MaxMin.
A major limitation of using σv (equation (6)) directly is that it has only 2 linear regions and is thus
limited in its expressive power. In contrast, MaxMin first divides the preactivation z ∈ Rm (assuming
m is divisible by 2) into m∕2 groups of size 2 each. Since each group has 2 linear regions, we get
2m/2 linear regions from the m∕2 groups. Thus, to increase the expressive power, we similarly divide
z into m∕2 groups of size 2 each and apply the 2-dimensional Householder activation function of
Order 1 (σθ) to each group resulting in 2m/2 linear regions (same as MaxMin).
To apply σθ to the output of a convolution layer z ∈ Rm×n×n (m is the number of channels and
n × n is the spatial size), we first split z into 2 tensors along the channel dimension giving the tensors:
z:m/2,:,: and zm/2:,:,:. Each of these tensors is of size m∕2 × n × n giving n2m∕2 groups. We use
the same θ for each pair of channels (irrespective of spatial location) resulting in m∕2 learnable
parameters. We initialize each θ = π∕2 so that σθ is equivalent to MaxMin at initialization.
7	Experiments
Our goal is to evaluate the effectiveness of the three changes proposed in this work: (a) Last Layer
Normalization, (b) Certificate regularization and (c) Householder activation functions. We perform
experiments under the setting of provably robust image classification on CIFAR-10 and CIFAR-100
datasets using the same 1-Lipschitz CNN architectures used by Singla & Feizi (2021) (LipConvnet-5,
10, 15, . . . , 40) due to their superior performance over the prior works. We compare with the three
6
Published as a conference paper at ICLR 2022
orthogonal convolution layers in the literature: SOC (Singla & Feizi, 2021), BCOP (Li et al., 2019b)
and Cayley (Trockman & Kolter, 2021) using MaxMin as the activation function.
We use SOC with MaxMin as the primary baseline for comparison in the maintext due to their superior
performance over the prior works (BCOP, Cayley). Results using BCOP and Cayley convolutions are
given in Appendix Sections H and I for completeness. We use the same implementations for these
convolution layers as given in their respective github repositories. We compare the provable robust
accuracy using 3 different values of the l2 perturbation radius ρ = 36/255, 72/255, 108/255. In
both Tables 1 and 2, for all networks, we use SOC as the convolution layer. Using SOC, we achieve
the same bound on the approximation error of an orthogonal Jacobian as achieved in Singla & Feizi
(2021) i.e. 2.42 × 10-6 across all convolution layers in all networks. Thus, even for the network with
40 layers, this results in maximum Lipschitz constant of (1 + 2.42 × 10-6)40 = 1.000097 ≤ 1.0001.
Thus, the Lipschitz constant across all our networks is bounded by 1.0001. The symbol HH (in
Tables 1, 2) is for the 2D Householder Activation of order 1 or σθ (defined in equation (8)).
All experiments were performed using 1 NVIDIA GeForce RTX 2080 Ti GPU. All networks were
trained for 200 epochs with initial learning rate of 0.1, dropped by a factor of 0.1 after 100 and 150
epochs. For Certificate Regularization (or CR), we set the parameter γ = 0.5.
7.1	RESULTS ON CIFAR- 1 00
In Table 1, for each architecture, the row "SOC + MaxMin" uses the MaxMin activation, "+ LLN"
adds Last Layer Normalization (uses MaxMin), "+ HH" replaces MaxMin with σθ (also uses LLN)
, "+ CR" also adds Certificate Regularization with γ = 0.1 (uses both σθ and LLN). The column,
"Increase (Standard)" denotes the increase in standard accuracy relative to "SOC + MaxMin".
By adding LLN (the row "+ LLN"), we observe gains in standard (min gain of 1.10%) and provable
robust accuracy (min gain of 1.71% at ρ = 36/255) across all the LipConvnet architectures (gains
relative to "SOC + MaxMin"). These gains are smallest for the LipConvnet-40 network with the
maximum depth. However, replacing MaxMin with the σθ activation further improves the standard
(min gain of 3.65%) and provable robust accuracy (min gain of 4.46% at ρ = 36/255) across
all networks (again relative to "SOC + MaxMin"). We observe that replacing MaxMin with σθ
significantly improves the performance of the deeper LipConvnet-35, 40 networks.
Adding CR further improves the provable robust accuracy while only slightly reducing the standard
accuracy. Because LLN significantly improves the standard accuracy, we compare the standard
accuracy numbers between rows "+ CR" and "+ LLN" to evaluate the drop due to CR. We observe
a small drop in standard accuracy (-0.04%, -0.11%) only for LipConvnet-5 and LipConvnet-15
networks. For the other networks, the standard accuracy actually increases.
7.2	Results on CIFAR- 1 0
In Table 2, for each architecture, the row "SOC + MaxMin" uses the MaxMin activation, the row "+
HH" uses σθ activation (replacing MaxMin) and the row "+ CR" also adds Certificate Regularization
with γ = 0.1 (again using σθ as the activation). Due to the small number of classes in CIFAR-10, we
do not observe significant gains using Last Layer Normalization or LLN (Appendix Table 10). Thus,
we do not include LLN for any of the results in Table 2. The column, "Increase (108/255)" denotes
the increase in provable robust accuracy with ρ = 108/255 relative to "SOC + MaxMin".
For LipConvnet-25, 30, 35, 40 architectures, we observe gains in both the standard and provable
robust accuracy by replacing MaxMin with the HH activation (i.e σθ). Similar to what we observe
for CIFAR-100 in Table 1, the gains in provable robust accuracy (ρ = 108/255) are significantly
higher for deeper networks: LipConvnet-35 (3.65%) and LipConvnet-40 (4.35%) with decent gains
in standard accuracy (1.71 and 1.61% respectively).
Again similar to CIFAR-100, adding CR further boosts the provable robust accuracy while slightly
reducing the standard accuracy. Comparing "+ CR" with "SOC + MaxMin", we observe small
drops in standard accuracy for LipConvnet-5, 10, . . . , 30 networks (max. drop of -0.56%), and
gains for LipConvnet-35 (+0.52%) and LipConvnet-40 (+0.96%). For provable robust accuracy
(ρ = 108/255), we observe very significant gains of > 4.96% for all networks and > 8% for the
deeper LipConvnet-35, 40 networks.
7
Published as a conference paper at ICLR 2022
Architecture	Methods	Standard Accuracy	Provable Robust Acc. (ρ =)			Increase (Standard)
			36/255	72/255	108/255	
	SOC + MaxMin	42.71%^^	27.86%	17.45%	9.99%		
LipConvnet-5	+ LLN	45.86%	31.93%	21.17%	13.21%	+3.15%
	+ HH	46.36%	32.64%	21.19%	13.12%	+3.65%
	+ CR	45.82%	32.99%	22.48%	14.79%	+3.11%
	SOC + MaxMin	43.72%^^	29.39%	18.56%	11.16%		
LipConvnet-10	+ LLN	46.88%	33.32%	22.08%	13.87%	+3.16%
	+ HH	47.96%	34.30%	22.35%	14.48%	+4.24%
	+ CR	47.07%	34.53%	23.50%	15.66%	+3.35%
	SOC + MaxMin	42.92%^^	28.81%	17.93%	10.73%		
LipConvnet-15	+ LLN	47.72%	33.52%	21.89%	13.76%	+4.80%
	+ HH	47.72%	33.97%	22.45%	13.81%	+4.80%
	+ CR	47.61%	34.54%	23.16%	15.09%	+4.69%
	SOC + MaxMin	43.06%^^	29.34%	18.66%	11.20%		
LipConvnet-20	+ LLN	46.86%	33.48%	22.14%	14.10%	+3.80%
	+ HH	47.71%	34.22%	22.93%	14.57%	+4.65%
	+ CR	47.84%	34.77%	23.70%	15.84%	+4.78%
	SOC + MaxMin	43.37%^^	28.59%	18.18%	10.85%		
LipConvnet-25	+ LLN	46.32%	32.87%	21.53%	13.86%	+2.95%
	+ HH	47.70%	34.00%	22.67%	14.57%	+4.33%
	+ CR	46.87%	34.09%	23.41%	15.61%	+3.50%
	SOC + MaxMin	42.87%^^	28.74%	18.47%	11.21%		
LipConvnet-30	+ LLN	46.18%	32.82%	21.52%	13.52%	+3.31%
	+ HH	46.80%	33.72%	22.70%	14.31%	+3.93%
	+ CR	46.92%	34.17%	23.21%	15.84%	+4.05%
	SOC + MaxMin	42.42%^^	28.34%	18.10%	10.96%		
LipConvnet-35	+ LLN	45.22%	32.10%	21.28%	13.25%	+2.80%
	+ HH	46.21%	32.80%	21.55%	14.13%	+3.79%
	+ CR	46.88%	33.64%	23.34%	15.73%	+4.46%
	SOC + MaxMin	41.84%^^	28.00%	17.40%	10.28%		
LipConvnet-40	+ LLN	42.94%	29.71%	19.30%	11.99%	+1.10%
	+ HH	45.84%	32.79%	21.98%	14.07%	+4.00%
	+ CR	45.03%	32.57%	22.37%	14.76%	+3.19%
Table 1: Results for provable robustness against adversarial examples on the CIFAR-100 dataset.
Results with BCOP and Cayley convolutions are in Appendix Tables 13 and 14.
8	Conclusion
In this work, we introduce a procedure to certify robustness of 1-Lipschitz convolutional neural
networks without orthogonalizing of the last linear layer of the network. Additionally, we introduce a
certificate regularization that significantly improves the provable robust accuracy for these models at
higher l2 radii. Finally, we introduce a class of GNP activation functions called Householder (or HH)
activations and prove that the Jacobian of any Gradient Norm Preserving (GNP) piecewise linear
function is only allowed to change via Householder transformations for the function to be continuous
8
Published as a conference paper at ICLR 2022
Architecture	Methods	Standard Accuracy	Provable Robust Acc. (ρ =)			Increase (108/255)
			36/255	72/255	108/255	
	SOC + MaxMin	75.78%^^	59.18%	42.01%	27.09%		
LipConvnet-5	+ HH	76.30%	60.12%	43.20%	27.39%	+0.30%
	+ CR	75.31%	60.37%	45.62%	32.38%	+5.29%
	SOC + MaxMin	76.45%^^	60.86%	44.15%	29.15%		
LipConvnet-10	+ HH	76.86%	61.52%	44.91%	29.90%	+0.75%
	+ CR	76.23%	62.57%	47.70%	34.15%	+5.00%
	SOC + MaxMin	76.68%^^	61.36%	44.28%	29.66%		
LipConvnet-15	+ HH	77.41%	61.92%	45.60%	31.10%	+1.44%
	+ CR	76.39%	62.96%	48.47%	35.47%	+5.81%
	SOC + MaxMin	76.90%^^	61.87%	45.79%	31.08%		
LipConvnet-20	+ HH	76.99%	61.76%	45.59%	30.99%	-0.09%
	+ CR	76.34%	62.63%	48.69%	36.04%	+4.96%
	SOC + MaxMin	75.24%^^	60.17%	43.55%	28.60%		
LipConvnet-25	+ HH	76.37%	61.50%	44.72%	29.83%	+1.23%
	+ CR	75.21%	61.98%	47.93%	34.92%	+6.32%
	SOC + MaxMin	74.51%^^	59.06%	42.46%	28.05%		
LipConvnet-30	+ HH	75.25%	59.90%	43.85%	29.35%	+1.30%
	+ CR	74.23%	60.64%	46.51%	34.08%	+6.03%
	SOC + MaxMin	73.73%^^	58.50%	41.75%	27.20%		
LipConvnet-35	+ HH	75.44%	61.05%	45.38%	30.85%	+3.65%
	+ CR	74.25%	61.30%	47.60%	35.21%	+8.01%
	SOC + MaxMin	71.63%^^	54.39%	37.92%	24.13%		
LipConvnet-40	+ HH	73.24%	58.12%	42.24%	28.48%	+4.35%
	+ CR	72.59%	59.04%	44.92%	32.87%	+8.74%
Table 2: Results for provable robustness against adversarial examples on the CIFAR-10 dataset.
Results with BCOP and Cayley convolutions are in Appendix Tables 7 and 8.
which provides a general principle for designing piecewise linear GNP functions. These ideas lead to
improved deterministic `2 robustness certificates on CIFAR-10 and CIFAR-100.
9
Published as a conference paper at ICLR 2022
Acknowledgements
This project was supported in part by NSF CAREER AWARD 1942230, a grant from NIST
60NANB20D134, HR001119S0026-GARD-FP-052, HR00112090132, ONR YIP award N00014-22-
1-2271, Army Grant W911NF2120076.
Reproducibility
The code for reproducing all experiments in the paper is available at https://github.com/
singlasahil14/SOC.
Ethics S tatement
This paper introduces novel set of techniques for improving the provable robustness of 1-Lipschitz
CNNs. We do not see any foreseeable negative consequences associated with our work.
References
Cem Anil, James Lucas, and Roger B. Grosse. Sorting out lipschitz function approximation. In
ICML, 2018.
Peter L. Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Proceedings of the 31st International Conference on Neural Information
Processing Systems, NIPS'17,pp. 6241-6250, USA, 2017. Curran Associates Inc. ISBN 978-1-
5108-6096-4. URL http://dl.acm.org/citation.cfm?id=3295222.3295372.
Xiaoyu Cao and Neil Zhenqiang Gong. Mitigating evasion attacks to deep neural networks via
region-based classification. In Proceedings of the 33rd Annual Computer Security Applications
Conference, ACSAC 2017, pp. 278-287, New York, NY, USA, 2017. Association for Computing
Machinery. ISBN 9781450353458. doi: 10.1145/3134600.3134606. URL https://doi.org/
10.1145/3134600.3134606.
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann N. Dauphin, and Nicolas Usunier. Parseval
networks: Improving robustness to adversarial examples. In Doina Precup and Yee Whye Teh (eds.),
Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW,
Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pp. 854-
863. PMLR, 2017. URL http://proceedings.mlr.press/v70/cisse17a.html.
Jeremy M. Cohen, Elan Rosenfeld, and J. Zico Kolter. Certified adversarial robustness via randomized
smoothing. In ICML, 2019.
Francesco Croce, Maksym Andriushchenko, and Matthias Hein. Provable robustness of relu networks
via maximization of linear regions. AISTATS 2019, 2019.
J. Deng, Wei Dong, R. Socher, L. Li, K. Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248-255,
2009.
Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael J. Cree. Regularisation of neural
networks by enforcing lipschitz continuity, 2020.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C
Courville. Improved training of wasserstein gans. In I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances
in Neural Information Processing Systems, volume 30, pp. 5767-5777. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
892c3b1c6dccd52936e27cbd0ff683d6- Paper.pdf.
10
Published as a conference paper at ICLR 2022
Yujia Huang, Huan Zhang, Yuanyuan Shi, J Zico Kolter, and Anima Anandkumar. Training certifiably
robust neural networks with efficient local lipschitz bounds. In A. Beygelzimer, Y. Dauphin,
P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems,
2021. URL https://openreview.net/forum?id=FTt28RYj5Pc.
Aounon Kumar, Alexander Levine, Soheil Feizi, and Tom Goldstein. Certifying confidence via
randomized smoothing. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.),
Advances in Neural Information Processing Systems, volume 33, pp. 5165-5177. Curran Asso-
ciates, Inc., 2020a. URL https://proceedings.neurips.cc/paper/2020/file/
37aa5dfc44dddd0d19d4311e2c7a0240- Paper.pdf.
Aounon Kumar, Alexander Levine, Tom Goldstein, and Soheil Feizi. Curse of dimensional-
ity on randomized smoothing for certifiable robustness. In Hal DaUme In and Aarti Singh
(eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of
Proceedings of Machine Learning Research, pp. 5458-5467. PMLR, 13-18 Jul 2020b. URL
http://proceedings.mlr.press/v119/kumar20b.html.
Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and S. K. K. Jana. Certified
robustness to adversarial examples with differential privacy. In IEEE S&P 2019, 2018.
Alexander Levine and Soheil Feizi. Improved, deterministic smoothing for l1 certified robustness. In
ICML, 2021.
Alexander Levine, Sahil Singla, and Soheil Feizi. Certifiably robust interpretation in deep learning,
2019.
Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Certified adversarial robustness with ad-
ditive noise. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alche-Buc, E. Fox, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 32, pp. 9464-9474. Curran As-
sociates, Inc., 2019a. URL https://proceedings.neurips.cc/paper/2019/file/
335cd1b90bfa4ee70b39d08a4ae0cf2d- Paper.pdf.
Qiyang Li, Saminul Haque, Cem Anil, James Lucas, Roger Grosse, and Jorn-Henrik Jacobsen.
Preventing gradient attenuation in lipschitz constrained convolutional networks. Conference on
Neural Information Processing Systems, 2019b.
Xuanqing Liu, Minhao Cheng, Huan Zhang, and C. Hsieh. Towards robust neural networks via
random self-ensemble. In ECCV, 2018.
Philip M. Long and Hanie Sedghi. Generalization bounds for deep convolutional neural networks. In
International Conference on Learning Representations, 2020. URL https://openreview.
net/forum?id=r1e_FpNFDr.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. In International Conference on Learning Representations, 2018.
URL https://openreview.net/forum?id=B1QRgziT-.
Haifeng Qian and Mark N. Wegman. L2-nonexpansive neural networks. In International Confer-
ence on Learning Representations, 2019. URL https://openreview.net/forum?id=
ByxGSsR9FQ.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Semidefinite relaxations for certifying
robustness to adversarial examples. In NeurIPS, 2018.
Hadi Salman, Jerry Li, Ilya Razenshteyn, Pengchuan Zhang, Huan Zhang, Sebastien Bubeck, and
Greg Yang. Provably robust deep learning via adversarially trained smoothed classifiers. In
H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett (eds.), Ad-
vances in Neural Information Processing Systems, volume 32, pp. 11292-11303. Curran As-
sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
3a24b25a7b092a252166a1641ae953e7- Paper.pdf.
Hanie Sedghi, Vineet Gupta, and Philip M. Long. The singular values of convolutional layers. In
International Conference on Learning Representations, 2019. URL https://openreview.
net/forum?id=rJevYoA9Fm.
11
Published as a conference paper at ICLR 2022
Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus Puschel, and Martin T. Vechev. Fast and
effective robustness certification. In NeurIPS, 2018.
Sahil Singla and Soheil Feizi. Second-order provable defenses against adversarial attacks. In
Hal Daume In and Aarti Singh (eds.), Proceedings ofthe 37th International Conference on Machine
Learning, volume 119 of Proceedings of Machine Learning Research, pp. 8981-8991. PMLR,
13-18 Jul 2020. URL http://Proceedings .mlr.press∕v119/Singla20a.html.
Sahil Singla and Soheil Feizi. Skew orthogonal convolutions. In ICML, 2021. URL https:
//arxiv.org/abs/2105.11417.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning
Representations, 2014. URL http://arxiv.org/abs/1312.6199.
Asher Trockman and J Zico Kolter. Orthogonalizing convolutional layers with the cayley transform.
In International Conference on Learning Representations, 2021. URL https://openreview.
net/forum?id=Pbj8H_jEHYv.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
Robustness may be at odds with accuracy. In ICLR, 2018.
Yusuke Tsuzuku, I. Sato, and Masashi Sugiyama. Lipschitz-margin training: Scalable certification of
perturbation invariance for deep neural networks. In NeurIPS, 2018.
Cedric Villani. Optimal transport, old and new, 2008.
Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, and J Zico Kolter.
Beta-CROWN: Efficient bound propagation with per-neuron split constraints for neural network
robustness verification. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.),
Advances in Neural Information Processing Systems, 2021. URL https://openreview.
net/forum?id=ahYIlRBeCFw.
Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning, and
Inderjit Dhillon. Towards fast computation of certified robustness for ReLU networks. In Jennifer
Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine
Learning, volume 80 of Proceedings of Machine Learning Research, pp. 5276-5285. PMLR,
10-15 Jul 2018. URL https://proceedings.mlr.press/v80/weng18a.html.
Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th In-
ternational Conference on Machine Learning, volume 80 of Proceedings of Machine Learning
Research, pp. 5286-5295, Stockholmsmassan, Stockholm Sweden, 10-15 Jul 2018. PMLR. URL
http://proceedings.mlr.press/v80/wong18a.html.
Eric Wong, Frank R. Schmidt, Jan Hendrik Metzen, and J. Zico Kolter. Scaling provable adversarial
defenses. In NeurIPS, 2018.
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, and Jeffrey Pennington.
Dynamical isometry and a mean field theory of CNNs: How to train 10,000-layer vanilla con-
volutional neural networks. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th
International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning
Research, pp. 5393-5402, Stockholmsmassan, Stockholm Sweden, 10-15 Jul 2018. PMLR. URL
http://proceedings.mlr.press/v80/xiao18a.html.
Bohang Zhang, Tianle Cai, Zhou Lu, Di He, and Liwei Wang. Towards certifying linfinity robustness
using neural networks with linfinity-dist neurons. In ICML, 2021.
Bohang Zhang, Du Jiang, Di He, and Liwei Wang. Boosting the certified robustness of l-infinity
distance nets. In International Conference on Learning Representations, 2022. URL https:
//openreview.net/forum?id=Q76Y7wkiji.
12
Published as a conference paper at ICLR 2022
Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural network
robustness certification with general activation functions. In Advances in Neural Information
Processing Systems (NIPS), arXiv preprint arXiv:1811.00866, dec 2018.
Huan Zhang, Pengchuan Zhang, and Cho-Jui Hsieh. Recurjac: An efficient recursive algorithm for
bounding jacobian matrix of neural networks and its applications. In AAAI Conference on Artificial
Intelligence (AAAI), arXiv preprint arXiv:1810.11783, dec 2019.
13
Published as a conference paper at ICLR 2022
A Proofs
A.1 Proof of Proposition 1
Proof. We proceed by computing the Lipschitz constant of the function fl - fi .
The gradient of the function: fl - fi at x can be computed using the chain rule:
Vx (fl- fi) = (Wι,:- Wi,:)T Vxg
Since g is given to be 1-Lipschitz, the Lipschitz constant of fl - fi can be computed using the above
equation as follows:
kVx(fl-fi)k2 ≤ k (Wl,: - Wi,:)T (Vxg) k2
kVx (fl -fi)k2 ≤ kWl,: -Wi,:k2kVxgk2 ≤ kWl,: -Wi,:k2
Thus, the distance of x to the decision boundary fl - fi = 0, is lower bounded by:
min	kx*
fl(x*) = fi(x*)
- xk2
≥	力(X) — fi(X)
≥ kWl,: - Wi,：k2
Thus, the distance to decision boundary across all classes i 6= l is lower bounded by:
min
i6=l
min	kx*
fl(x*)=fi(x*)
- Xk2
≥ min 力(X)- fi(X)
≥ i=ι kWι,: - Wi,：k2
□
A.2 Proof of Theorem 1
Proof. We first prove that if Q2 = (I - 2vvT)Q1, then the function g is continuous.
First, observe that for vTz > 0, g(z) = Q1z which is continuous.
Similarly, for vTz < 0, g(z) = Q2z which is again continuous.
This proves that the function g is continuous when vTz > 0 or vT z < 0.
Thus, to prove continuity ∀ z ∈ S, we must prove that:
Q1z = Q2z	∀z : vTz = 0	(9)
Since Q2 = Q1(I - 2vvT), we have:
Q2 - Q1 = -2Q1vvT
(Q2 - Qi) Z = -2 (QIVVT) Z = -2QιV (VTZ)
The above equation directly proves (9).
Now, we prove the other direction i.e if g is continuous in S then, Q2 = Q1(I - 2VVT).
Since g is continuous for all Z : VTZ = 0, we have:
Q2Z = Q1Z ∀Z : VTZ = 0
(Q2 - Q1) Z = 0	∀ Z : VTZ = 0
Since Z ∈ Rm, we know that the set of vectors: {Z : VTZ = 0} spans a m - 1 dimensional subspace.
Thus, the null space of Q2 - Q1 is of size m - 1.
This in turn implies that Q2 - Q1 is a rank one matrix given by the following equation:
Q2 - Q1 = uVT	(10)
where the vector u remains to be determined.
Since Q1 and Q2 are orthogonal matrices, we have the following set of equations:
QTQ2 = (Qi + UVT)T (Qi + UVT)	(11)
Q2QT = (Qi + UVT) (Qi + UVT)T	(12)
14
Published as a conference paper at ICLR 2022
We first simplify equation (11):
QT Q2 = (QT + vuT )(Qι +uvT)
I = I + V (QTu)T + (QTu) vτ + (UTU) vvT
0 = v (QTu)T + (QTu) VT + (uTu) vvT
-(uTu) vvT = V (uTQ1) + (QTu) vT
Right multiplying both sides by V and using ∣∣ v∣∣2 = 1,we get:
-(uTu) V = (uT Q1v) V + QTU
QTU = — (uTU + uTQιv) v = λv
U = λQ1v, where λ = - (uTU + uT Q1v)	(13)
Substituting U using equation (13) in equation (10), we get:
Q2 - Qi = λQιvvT
Q2 = Qi (I + λvvT)
Since Q2T Q2 = I, we have:
QTQ2 = (Qi (I + λvvT))t Qi (I + λvvT)
QTQ2 = (I + λvvτ) QTQi (I + λvvτ)
I = (I + λvvτ) (I + λvvτ)
I = I + 2λvvτ + λ2vvτ
=⇒ λ = 0 or λ = -2
Since λ = 0 would imply Qi = Q2 which is not allowed by the assumption of the Theorem that
Qi = Q2.
λ = -2 is the only possibility allowed.
This proves the other direction i.e:
Q2 = Qi (I - 2vvτ)
□
A.3 Proof OF Corollary 1
Proof. Subsitute Qi, V as follows in Theorem 1.
0
1
Qi = 10
v=	+ sin(θ∕2)^ —cos(θ∕2)						
Q2 =	I - 2vvT						
Q2 =		10 01	-2	-sin(θ∕2)- —cos(θ∕2)		[sin (θ∕2) - cos	(θ∕2)]
Q2 =		10 01	-2	sin2 (θ∕2)	- sin (θ∕2) cos (θ∕2) -sin (θ∕2) cos (θ∕2)	cos2 (θ∕2)			
Q2 =		一 1 - 2 sin	2 sin (θ∕2)	2(θ∕2)	2 cos(θ∕2)		sin (θ∕2) cos (θ∕2) 1 + 2cos2 (θ∕2)	
Q2 =		cos(θ)	sin(θ) sin(θ)	- cos(θ)					
Theorem 1 then directly implies the corollary.
□
15
Published as a conference paper at ICLR 2022
A.4 Proof of Theorem 2
Theorem 2. Given: 0 ≤ θo < θι …< θ2n = 2π + θo such that Pn-l(θ2i+1 - θ2i) = π and
αi = 2 ij=0 θi-j (-1)j, The function σΘ : R2 → R2 is continuous, GNP and 1-Lipschitz where
Θ = [θ0, θ1, . . . , θ2n-1] (also called 2D Householder Activation of order n):
cos αi	sin αi	z1
σθ(z1,z2) = [(-1)i Sin ɑi (-1)i+1 cos αj 图	θi ≤ ψ < θi+1	(14)
where φ ∈ 仇，θ2n	=	2π	+ θ0)	and cos(φ)	= z∖NZ2	+ z2,	sin(夕)=z2∕√Z1÷^2∙
Proof. We are given the following:
n-1	i
X(θ2i+1 - θ2i ) = π,	αi = 2 X θi-j(-1)j	(15)
Note that by definition (equation (14)), the function is linear for θi < φ < θi+ι and hence
continuous.
Furthermore, since 夕 ∈ [θo, θ2n,), We proceed to prove continuity for the following two cases:
Case 1: θi — e< 夕 <θi + 6,	€ > 0,i ≥ 1
Case 2: θo < 夕 < θ° + €,	θ2n — € < 夕 < θ2n € > 0
Proof for Case 1:
Using equation (14), we know that。㊀ realizes different linear functions for θi - € < 中 < θi and
θi < φ < θi + €.
Thus, for σΘ to be continuous, we require that the two linear functions be the same at the boundary
i.e φ = θi.
We first write the input (zι, z2) in terms of shifted polar coordinates i.e: (r cos(夕)，r sin(夕))where
r = √z2 + z2 and cos φ = ZINZ2 + z2, sin φ = z2∕√z2 + z2,夕 ∈ [θ0,θ0 + 2π)
Thus, the function for θi - € <φ < θi is given by:
σθ (r cos 夕,r sin 夕)
cos αi-1
(-1)i-1 sin αi-1
Sin αi-ι	r CoS φ
(-1)i cos ai-ι I Ir sin φ
Similarly, the function for θi < φ <θi + € is given by:
/	ʌ Γ cos αi	sin αi ^l Γr cos φ
σθ (r cos 夕,r Sm 夕)=(-1)i sin ɑi	(-1)i+1 cos aj [r sin φ
(16)
(17)
The difference in the function values at the boundary i.e φ = θi, obtained by subtracting equations
(17) and (16) is given as follows:
cos αi
(-1)i sin αi
sin αi	r cos θi	cos αi-1
(-1)i+1 cos αi	r sin θi - (-1)i-1 sin αi-1
sin αi-1
(-1)i cos αi-1
rcosθi
r sin θi
cos αi - cos αi-1	sin αi - sin αi-1	cos θi
r (-1)i sin αi - (-1)i-1 sin αi-1	(-1)i+1 cos αi - (-1)i cos αi-1	sin θi
r
cos αi - cos αi-1	sin αi - sin αi-1	cos θi
(-1)i(sin αi + sin αi-1) (-1)i+1(cos αi + cos αi-1)	sin θi
Using sum-to-product trigonometric identities, the above equals:
2sin (ɑi-2-ai) sin (αi-2+αi)	2sin (αi-2α"1) CoS (即十厂1)
2(-1)i sin (α+α-ι) cos (α-α-ι)	2(-1)i+1 cos (『-%) cos (α-2+α∙
2r
2r
sin	sin (α-2+ɑ∙)
(-1)i sin (α+α-ι) cos (ɑ∙*
- sin αi
(-1)i+1 cos (
户)cos
ai-i-ai λ
-2-
α +ɑi-ι
2
cos (αi-2+αi
cos θi
sin θi
cos θi
sin θi
sin (aiTα∙)
(-1)i cos (αi-2-αi
[sin (α-2+α
-cos (αi-2+αi)]
cos θi
sin θi
16
Published as a conference paper at ICLR 2022
Using equation (15), we directly have: αi = 2θi - αi-1. Thus, the above equation reduces to:
sin (θi - αi)
(-1)i cos (θi - αi)
[sin(θi)
- cos (θi)]
cos θi
sin θi
0.
0.
Hence, the linear functions given by equations (16) and (17) are equal at φ = θi. This proves that the
function is continuous for Case 1.
Proof for Case 2:
Using equation (14), We know that qθ realizes different linear functions for θo < 夕 < θ0 + E and
θ2n - e < 夕 < θ2n∙
Thus, for σΘ to be continuous, we require that the two linear functions be the same at the boundary
i.e φ = θo.
As before, we first write the input (z1, z2) in terms of shifted polar coordinates i.e:
(rcos(夕),rsin(夕)).
Thus, the function for θo < 夕 < θ0 + E is given by:
/	.	、	CoS αo	sin αo ] Γr CoS φ	~c、
σθ (r CoS φ, r Sin S=	.	.	(18)
θ '	*'	*'	Sinao	— cos αο	r Sinφ	' "
Similarly, the function for θ2n - E < φ < θ2n is given by:
/	ʌ 「	coSα2n-i	Sinα2n-i	] ΓrCoS2]	zιm
σθ Cr CoS Ar Sm。)=[( —U Sin α2n-i	( -I)'+1 cos ɑ2n-J [r Sin "	(19)
Using equation (15), α2n-1 is given as follows:
2n-1	n-1
α2n-1 = 2 X θ2n-1-i(-1)i = 2 X(θ2i+1 一 θ2i) = 2π
i=0	i=0
Thus, equation (19) reduces to:
σθ (r cos 夕,r sin 夕)
1 0	r CoS φ
0 1	r sin φ
(20)
The difference in the function values at the boundary i.e φ = θo, obtained by subtracting equations
(20) and (18) is given as follows:
1 0	r CoS θ0
0 1	r Sin θ0
1 — CoS α0
r — Sin α0
—
CoS α0 Sin α0 r CoS θ0
Sin α0 — CoS α0 r Sin θ0
— Sin α0	CoS θ0
1 + CoS α0 Sin θ0
Using the trigonometric identities: 1 — Cos(θ) = 2sin2(θ∕2), 1 + Cos(θ) = 2cos2(Θ∕2) and
sin(θ) = 2sin(θ∕2) Cos(θ∕2), we have:
Γ	2sin2(α0)	—2sin(a20)cos(α20)] IcoSθo
r 1—2sin(a20)Cos(等)	2cos2(a20)	] [sinθο
2r Γ Sin(篙 J [sin(号)—Cos(署)]FoS θ0]
—Cos(a0)J L ' 2，	2 2 yj [sin θοj
Using equation (15), we have: α0 = 2θ0. Thus, the above equation reduces to:
sin(θ0)
— Cos(θ0)
[sin(θ0)
— Cos(θ0)]
Cos θ0
sin θ0
0.
0.
Hence, the linear functions given by equations (18) and (20) are equal at φ = θ0.
This proves that the function is continuous for Case 2.
□
17
Published as a conference paper at ICLR 2022
B SELECTION OF γ USING CROSS VALIDATION
Using 5000 held out samples from CIFAR-10, we tested 6 different values of γ shown in Table 3 and
selected γ = 0.5 because it resulted in less than 0.5% decrease in standard accuracy while 4.96%
increase in provably robust accuracy. We used the LipConvnet-5 network with the 2D Householder
activation function of order 2 i.e σθ .
Architecture	γ	Standard Accuracy	Provable Robust Acc. (P =)			Increase (108/255)
			36/255	72/255	108/255	
	0.	75.82%^^	59.66%	42.78%	26.92%		
	0.10	75.58%	59.74%	42.94%	28.04%	+0.94%
LipConvnet-5	0.25	75.54%	60.22%	43.98%	29.50%	+2.58%
	0.50	75.30%	60.08%	45.36%	31.88%	+4.96%
	0.75	74.14%	60.36%	46.08%	33.44%	+6.52%
	1.00	73.86%	60.30%	46.80%	34.60%	+7.68%
Table 3: Results for provable robustness against adversarial examples on the CIFAR-10 dataset for
cross validation using 5000 held out samples from the training set.
C DIFFERENCES BETWEEN l1 AND l2 CERTIFICATE
By the equivalence of norms, we have the following result:
x ∈ Rd,
1
√d
kxk1 ≤ kxk2 ≤ √dkχk1
(21)
Let us assume that we have an l1 certificate for the input x so that the prediction of a neural network
remains constant in a region of l1 radius ρ1 around the input x i.e:
kx*- xkι ≤ Pi	(22)
We want to compute the l2 certificate implied by the certificate given in equation (22). Let ρ2 be the
l2 certificate so that:
kx* - xk2 ≤ P2 =⇒ kx* - xk1 ≤ Pi
Using equation (21), we have the following set of equations:
l∣x* - xk2 ≤ P2 =⇒ √12kx* - xki ≤ P2 =⇒ l∣x* - xki ≤ √dρ2
Using equation (22), we have the following:
√dρ2 ≤ Pi =⇒ P2 ≤ √√^
Hence, the l2 norm certificate induced by the li norm certificate can be significantly smaller for high
dimensional inputs. Using the li certificate used in Levine & Feizi (2021) i.e pi = 4 and d = 32√3
for CIFAR-10 and CIFAR-100, We get an implied certificate of p2 = 4/(32√3) = 0.07225. In
contrast, in this work we study l2 robustness for much higher certificate values i.e 36/255 = 0.14118.
D Results using revised Lipschitz constants
In Tables 4 and 5, We shoW results Where the Lipschitz constant Was computed using product of
Lipschitz constant of all layers. The Lipschitz constant of each layer Was computed using direct poWer
iteration on the linear layer (using 50 iterations) and not using the approximation bound provided in
Singla & Feizi (2021).
18
Published as a conference paper at ICLR 2022
Architecture	Methods	Standard Accuracy	Provable Robust Acc. (ρ =)			Increase (Standard)
			36/255	72/255	108/255	
	SOC + MaxMin	42.71%^^	27.86%	17.45%	9.99%	
	+ LLN	45.86%	31.93%	21.17%	13.21%	+3.15%
LipConvnet-5	+ HH	46.36%	32.64%	21.19%	13.12%	+3.65%
	+ CR	45.82%	32.99%	22.48%	14.79%	+3.11%
	SOC + MaxMin	43.72%^^	29.39%	18.56%	11.16%	
	+ LLN	46.88%	33.32%	22.08%	13.87%	+3.16%
LipConvnet-10	+ HH	47.96%	34.30%	22.35%	14.48%	+4.24%
	+ CR	47.07%	34.53%	23.50%	15.66%	+3.35%
	SOC + MaxMin	42.92%^^	28.81%	17.93%	10.73%	
T ∙inf''cnτ∩αf-1 5	+ LLN	47.72%	33.52%	21.89%	13.76%	+4.80%
LipConvnet-15	+ HH	47.72%	33.97%	22.45%	13.81%	+4.80%
	+ CR	47.61%	34.54%	23.16%	15.09%	+4.69%
	SOC + MaxMin	43.06%^^	29.34%	18.66%	11.20%	
T inrcnxmat-On	+ LLN	46.86%	33.48%	22.14%	14.10%	+3.80%
LipConvnet-20	+ HH	47.71%	34.22%	22.93%	14.57%	+4.65%
	+ CR	47.84%	34.77%	23.70%	15.83%	+4.78%
	SOC + MaxMin	43.37%^^	28.59%	18.17%	10.85%	
T ∙inf''cn∖mαf-z7 5	+ LLN	46.32%	32.87%	21.53%	13.86%	+2.95%
LipConvnet-25	+ HH	47.70%	34.00%	22.67%	14.57%	+4.33%
	+ CR	46.87%	34.09%	23.41%	15.61%	+3.50%
	SOC + MaxMin	42.87%^^	28.74%	18.47%	11.20%	
T `inr`'en`vŋfʌt-af'l	+ LLN	46.18%	32.82%	21.52%	13.52%	+3.31%
LipConvnet-30	+ HH	46.80%	33.72%	22.70%	14.31%	+3.93%
	+ CR	46.92%	34.17%	23.21%	15.84%	+4.05%
	SOC + MaxMin	42.42%^^	28.34%	18.10%	10.96%	
T ∙inf''cn∖mαf-2 5	+ LLN	45.22%	32.10%	21.28%	13.25%	+2.80%
LipConvnet-35	+ HH	46.21%	32.80%	21.55%	14.13%	+3.79%
	+ CR	46.88%	33.64%	23.34%	15.73%	+4.46%
	SOC + MaxMin	41.84%^^	28.00%	17.40%	10.28%	
T ⅜τ⅜Λ^^r⅜-r>t m>2>f ΛC∖	+ LLN	42.94%	29.71%	19.30%	11.99%	+1.10%
LipConvnet-40	+ HH	45.84%	32.79%	21.98%	14.07%	+4.00%
	+ CR	45.03%	32.56%	22.37%	14.76%	+3.19%
Table 4: Results for provable robustness against adversarial examples on the CIFAR-100 dataset. For
all networks in this table, the maximum deviation of the Lipschitz constant from 1 was measured to
be 2.4609 × 10-5. The values shown in red were reduced by 0.01% from the corresponding values
in Table 1 due to the correction in Lipschitz constant. All other values remained unchanged.
19
Published as a conference paper at ICLR 2022
Architecture	Methods	Standard Accuracy	Provable Robust Acc. (ρ =)			Increase (108/255)
			36/255	72/255	108/255	
	SOC + MaxMin	75.78%^^	59.18%	42.01%	27.09%	
LipConvnet-5	+ HH	76.30%	60.12%	43.20%	27.39%	+0.30%
	+ CR	75.31%	60.37%	45.62%	32.38%	+5.29%
	SOC + MaxMin	76.45%^^	60.86%	44.15%	29.15%	
LipConvnet-10	+ HH	76.86%	61.52%	44.91%	29.90%	+0.75%
	+ CR	76.23%	62.57%	47.70%	34.15%	+5.00%
	SOC + MaxMin	76.68%^^	61.36%	44.27%	29.66%	
LipConvnet-15	+ HH	77.41%	61.92%	45.60%	31.10%	+1.44%
	+ CR	76.39%	62.96%	48.47%	35.47%	+5.81%
	SOC + MaxMin	76.90%^^	61.87%	45.79%	31.08%	
LipConvnet-20	+ HH	76.99%	61.76%	45.59%	30.99%	-0.09%
	+ CR	76.34%	62.63%	48.68%	36.04%	+4.96%
	SOC + MaxMin	75.24%^^	60.17%	43.54%	28.60%	
LipConvnet-25	+ HH	76.37%	61.50%	44.72%	29.83%	+1.23%
	+ CR	75.21%	61.98%	47.93%	34.92%	+6.32%
	SOC + MaxMin	74.51%^^	59.06%	42.46%	28.04%	
LipConvnet-30	+ HH	75.25%	59.90%	43.85%	29.35%	+1.30%
	+ CR	74.23%	60.64%	46.51%	34.08%	+6.04%
	SOC + MaxMin	73.73%^^	58.50%	41.75%	27.20%	
LipConvnet-35	+ HH	75.44%	61.05%	45.38%	30.85%	+3.65%
	+ CR	74.25%	61.30%	47.60%	35.21%	+8.01%
	SOC + MaxMin	71.63%^^	54.39%	37.92%	24.13%	
LipConvnet-40	+ HH	73.24%	58.12%	42.23%	28.48%	+4.35%
	+ CR	72.59%	59.04%	44.92%	32.87%	+8.74%
Table 5: Results for provable robustness against adversarial examples on the CIFAR-10 dataset. For
all networks in this table, the maximum deviation of the Lipschitz constant from 1 was measured to
be 2.2072 × 10-5. The values shown in red were reduced by 0.01% from the corresponding values
in Table 2 due to the correction in Lipschitz constant. All other values remained unchanged.
20
Published as a conference paper at ICLR 2022
(a) The output of σθ always lies in the pink
region. Applying σφ on this where φ 6= θ +
2nπ, n ∈ Z further divides this region into
two linear regions (light and original pink).
(b) In each colored region, the function is linear with the
Jacobian mentioned. For the function to be continuous,
we must have θ3 - θ2 + θ1 - θ0 = π. Thus, any 3 of
{θ0, θ1, θ2, θ3} can be chosen as learnable parameters.
Figure 2: Constructing Higher Order Householder activations (J+ and J- defined in equation (2))
E VERIFICATION THAT σθ (z1 , z2) ALWAYS LIES ON ONE SIDE OF THE
HYPERPLANE
a1 cos θ sin θ	z1
a2 = sin θ - cos θ z2
Consider the case: z1 Sin (θ∕2) - z2 cos (θ∕2) > 0
In this case σθ(z1, z2) = (z1, z2) and the result follows directly.
Consider the other case: z1 sin (θ∕2) - z2 cos (θ∕2) ≤ 0
z1 cos θ + z2 sin θ
z1 sin θ - z2 cos θ
a1 sin (θ∕2) - a2 cos (θ∕2) = (z1 cosθ + z2 sin θ) sin (θ∕2) - (z1 sinθ - z2 cos θ) cos (θ∕2)
= z1 (cos θ sin (θ∕2) - sin θ cos (θ∕2)) + z2 (sin θ sin (θ∕2) + cos θ cos (θ∕2))
= -z1 sin (θ∕2) + z2 cos (θ∕2)
Since z1 sin (θ∕2) - z2 cos (θ∕2) ≤ 0, we have -z1 sin (θ∕2) + z2 cos (θ∕2) ≥ 0.
F Higher order Householder activation functions
We know that MaxMin(z1, z2) = (max(z1, z2), min(z1, z2)) where z1, z2 ∈ R. Because
max(z1, z2) > min(z1, z2), applying MaXMin again gives the same result i.e MaXMin◦ MaXMin =
MaxMin. Now consider the function σθ (discussed in the maintext, given below for convenience):
1 0	z1
0 1	z2
cos θ sin θ	z1
sin θ - cos θ z2
if z1 sin (θ∕2) - z2 cos (θ∕2) > 0
if z1 sin (θ∕2) - z2 cos (θ∕2) ≤ 0
From Figure 1, we observe that if (u1, u2) = σθ(z1, z2), then (u1, u2) always lies on the right side
of the hyperplane (pink colored region). In other words, u1 sin (θ∕2) - u2 cos (θ∕2) > 0 ∀ z1, z2
(proof in Appendix E). This further implies that σθ ◦ σθ = σθ.
However from Figure 2a in the maintext, we observe that if we use a different angle φ where
φ 6= θ + 2nπ for some n ∈ Z, then σφ(u1, u2) 6= (u1, u2) for all (u1, u2) in the pink colored region
(u1 sin (θ∕2) - u2 cos (θ∕2) > 0). This motivates us to construct the function σ(n) : R2 → R2
defined as follows:
σ(n) = σθ ◦ σθ ◦ σθ . . . ◦ σθ	(23)
1 {z }
n times, θ,s can be different
Clearly, σ(n) has a larger number of linear regions than σθ and thus expected to have more expressive
power. However, a drawback of using σ(n) is that it requires a sequential application of σθ which can
21
Published as a conference paper at ICLR 2022
be expensive if the number of iterations n is large. To address this limitation, first observe that σθ re-
alizes the same linear function for (z1,z2) and (cz1,cz2) when c > 0 i.e Ng/2)σθ = V(czι,czz) σθ.
Since σθ is piecewise linear, σθ(cz1, cz2) = cσθ(z1, z2). Thus, the input of the next function in
the iteration is scaled by c as well and its linear function (or the Jacobian) remains unchanged. By
induction, same holds for all the subsequent iterations. By chain rule, the Jacobian of composition
of functions is equal to the product of Jacobian of each individual function. Since the Jacobian
of each function is unchanged on scaling by c > 0, the Jacobian N(z1,z2) σ(n) also remains un-
changed: N(z1,z2) σ(n) = N(cz1,cz2) σ(n). This suggests that it is possible to determine the Jacobian
V(z1,z2)σ(n) for the input (z1,z2) by first converting to the polar coordinates (,z2 + Z2,夕)and
then using the phase angle 夕 alone (where cos(夕)=z∖//z? + Z2, sin(夕)=z2/∖/Z2 + z22).
This motivates us to construct another GNP piecewise linear activation that only depends on the
phase of the input but unlike σθ, it is allowed to have more than 2 linear regions without requiring a
sequential application. This construction is given in the following theorem (example in Figure 2b):
Theorem 2. Given: 0 ≤ θo < θι …< θ2n = 2π + θ0 such that Pn-0l(θ2i+1 — 02i) = ∏ and
αi = 2 ij=0 θi-j (-1)j, The function σΘ : R2 → R2 is continuous, GNP and 1-Lipschitz where
Θ = [θ0, θ1, . . . , θ2n-1] (also called Householder Activation of order n in 2 dimensions):
cos αi	sin αi	z1
σθ(z1,z2) = [( -1)i sin ɑi ( -1)i+1 cos αi][z2]	θi ≤ φ < θi+1
where φ ∈ [θo, θ2n = 2π + θ0) and cos(φ) = zi/，z2 + z2, sin(夕)=z2/，z2 + z2.
Using the definition of αi , α2n-1 can be computed as follows:
2n-1	n-1	n-1
α2n-1 = 2 X θ2n-1-j (—1)j = 2 X (θ2n-1-2j — θ2n-2-2j) = 2 X (θ2j+1 — θ2j) = 2π
j =0	j=0	j=0
σθ(z1,z2)= [ cos α2n-1	sin α2n-1]卜 1] =(z1,z2)	θ2n-1 ≤ 夕<θ2n(=θ0 + 2π)
5 1, 2	[— Sin a2n-1 Cos α2n-1J [z2_|	k 1, 2	2n 1 一中	2n	0	/
By continuity, σθ(z1, z2) = (z1,z2) for 夕=θ0. Thus if we set θo = 0,。㊀ is fixed to be identity
when 夕=0 (or z2 = 0). However, a learnable θo offers the flexibility of choosing arbitrary intervals
around the 夕=θo to be the identity function (while of course allowing θo = 0). Since we can choose
any interval of 2∏ for the phase angle, we choose 夕 ∈ [θo, θ2n, = θo + 2∏) instead of the usual
[0, 2π) to allow this possibility. We call (,z2 + Z2,夕)，the shifted polar coordinates.
Additionally, we make the following observations about Theorem 2. First, by construction σΘ has 2n
linear regions. Second, since Pin=-01(θ2i+1 — θ2i) = π, the sum of angles subtended by linear regions
with determinant —1 equals π. This in turn implies that sum of angles subtended by linear regions
with determinant +1 must also equal π. Third, again using Pjn=-01 (θ2j+1 — θ2j) = π, we know that
only 2n — 1 of the 2n parameters in [θ0, θ1, θ2, . . . , θ2n-1] can be chosen independently implying
that σΘ has 2n — 1 learnable parameters. In contrast, σ(n) has only n learnable parameters. Fourth,
when n = 1, σΘ reduces to the original σθ activation.
Because every function of the form σ(n) (equation (23)) can have potentially 2n linear regions, while
σΘ has only 2n linear regions, σΘ cannot express every function of the form σ(n). The primary
benefit of using qθ is that it can be easily applied by first determining the angle 夕(using shifted
polar coordinates), the region [θi, θi+1) to which 夕 belongs and the Jacobian for this region. This
requires 1 multiplication with the Jacobian instead of n required for σ(n).
G Extension to higher dimensions
We introduced Householder activation function of Order 1 in m dimensions in maintext Definition 1.
However, it suffers from the limitation that it has only 2 linear regions thus limiting its expressive
power. The construction given in Appendix Section F allows more than 2 linear regions but is valid
only for 2 dimensional inputs. This motivates us to construct Householder activations that depend on
all the m components of input z ∈ Rm, (m ≥ 3) while allowing for more than 2 linear regions.
22
Published as a conference paper at ICLR 2022
A straightforward way of constructing such an activation function is to apply an orthogonal matrix
Q ∈ Rm×m, followed by dividing the output Qz into groups of size 2 each and then applying σθ to
each group. However, since 1-Lipschitz neural networks involve multiplication with an orthogonal
weight matrix followed by GNP activation anyway, this construction is trivial because it does not
lead to additional gains in expressive power over using 2 dimensional σΘ activation functions.
Recall that the function σv is given by the following equation:
z,	vTz > 0,
σv(z)= (I - 2vvT)z,	vTz≤0.
(24)
By a similar analysis as for the 2-dimensional case (Figure 2 in maintext), a repeated application of
σv leads to increased number of linear regions and thus higher expressive power. This motivates us
to construct the function σ(m,n) : Rm → Rm by applying the function σv (equation (24)) n times
iteratively with different learnable parameter v at each iteration:
σ(m,n) = σv ◦ σv ◦ σv . . . ◦ σv
、----------V-----------}
n times, v’s can be different
Since σv realizes the same linear function for both the inputs Z and Cz i.e Nz σv = Ncz σv when
c > 0, σ(m,n) satisfies this property as well. This suggests that it is possible to determine the Jacobian
of σ(m,n) for the given input z by projecting z onto a unit sphere: z/kzk2. Moreover, we want our
constructed function to have at least 2n linear regions while requiring k iterations of σv where k is
independent of n. This motivates the following open question:
Open Problem. Can non-trivial order-n (n > 1) householder activation functions with 2n linear
regions be constructed for m dimensional input (m > 2) using k iterations of σv where k is
independent of n (but may depend on m)?
H	Additional results on CIFAR-10
The rows "BCOP", "Cayley" and "SOC (baseline)" all use the MaxMin activation function. "SOC
+ HH" replaces MaxMin with 2D Householder activation of order 1 (σθ), "+ CR" adds Certificate
Regularization (CR) with γ = 0.1 (while using σθ as the activation function).
In Table 9, the row "SOC + HH(2)" uses Householder activation of order 2 (σΘ defined in Theorem
2), "+ CR" adds Certificate Regularization (CR) with γ = 0.1 (while using the HH activation of order
2 i.e σΘ as the activation function).
None of the results in Tables 7, 8 and 9 include Last Layer Normalization (LLN).
23
Published as a conference paper at ICLR 2022
Architecture	Running times (seconds) MaxMin HH	
LipConvnet-5	3.7864	3.86
LipConvnet-10	5.3677	5.6014
LipConvnet-15	7.234	7.3503
LipConvnet-20	9.536	9.3753
LipConvnet-25	11.0512	11.2692
LipConvnet-30	12.5135	13.6866
LipConvnet-35	14.5539	15.0921
LipConvnet-40	17.1907	17.1928
Table 6: Inference times for various networks on the complete test dataset of CIFAR-10 with 10000
samples. None of these networks include Last Layer Normalization (LLN).
Architecture	Methods	Standard Accuracy	Provable Robust Acc. (ρ =)			Increase (108/255)
			36/255	72/255	108/255	
	BCOP	74.25%~~	58.01%	40.34%	25.21%	-1.88%
	Cayley	72.37%	55.92%	38.65%	24.27%	-2.82%
LipConvnet-5	SOC (baseline)	75.78%	59.18%	42.01%	27.09%	(+0%)
	SOC + HH	76.30%	60.12%	43.20%	27.39%	+0.30%
	+ CR	75.31%	60.37%	45.62%	32.38%	+5.29%
	BCOP	74.47%~~	58.48%	40.77%	26.16%	-2.99%
	Cayley	74.30%	57.99%	40.75%	25.93%	-3.22%
LipConvnet-10	SOC (baseline)	76.45%	60.86%	44.15%	29.15%	(+0%)
	SOC + HH	76.86%	61.52%	44.91%	29.90%	+0.75%
	+ CR	76.23%	62.57%	47.70%	34.15%	+5.00%
	BCOP	73.86%~~	57.39%	39.33%	24.86%	-4.80%
	Cayley	71.92%	54.55%	37.67%	23.50%	-6.16%
LipConvnet-15	SOC (baseline)	76.68%	61.36%	44.28%	29.66%	(+0%)
	SOC + HH	77.41%	61.92%	45.60%	31.10%	+1.44%
	+ CR	76.39%	62.96%	48.47%	35.47%	+5.81%
	BCOP	69.84%~~	52.10%	34.75%	21.09%	-9.99%
	Cayley	68.87%	51.88%	35.56%	21.72%	-9.36%
LipConvnet-20	SOC (baseline)	76.90%	61.87%	45.79%	31.08%	(+0%)
	SOC + HH	76.99%	61.76%	45.59%	30.99%	-0.09%
	+ CR	76.34%	62.63%	48.69%	36.04%	+4.96%
Table 7: Results for provable robustness on the CIFAR-10 dataset using shallow networks.
None of these results include Last Layer Normalization (LLN).
24
Published as a conference paper at ICLR 2022
Architecture	Methods	Standard Accuracy	Provable Robust Acc. (ρ =)			Increase (108/255)
			36/255	72/255	108/255	
	BCOP	68.47%^^	49.92%	31.99%	18.62%	-9.98%
	Cayley	64.00%	45.55%	29.24%	16.99%	-11.61%
LipConvnet-25	SOC (baseline)	75.24%	60.17%	43.55%	28.60%	(+0%)
	SOC + HH	76.37%	61.50%	44.72%	29.83%	+1.23%
	+ CR	75.21%	61.98%	47.93%	34.92%	+6.32%
	BCOP	64.11%^^	43.39%	25.02%	12.15%	-15.90%
	Cayley	58.83%	38.68%	22.07%	10.68%	-17.37%
LipConvnet-30	SOC (baseline)	74.51%	59.06%	42.46%	28.05%	(+0%)
	SOC + HH	75.25%	59.90%	43.85%	29.35%	+1.30%
	+ CR	74.23%	60.64%	46.51%	34.08%	+6.03%
	BCOP	63.05%^^	41.71%	23.30%	11.36%	-15.84%
	Cayley	53.55%	32.37%	16.18%	6.33%	-20.87%
LipConvnet-35	SOC (baseline)	73.73%	58.50%	41.75%	27.20%	(+0%)
	SOC + HH	75.44%	61.05%	45.38%	30.85%	+3.65%
	+ CR	74.25%	61.30%	47.60%	35.21%	+8.01%
	BCOP	60.17%^^	38.86%	21.20%	9.08%	-15.05%
	Cayley	51.26%	27.90%	12.06%	3.81%	-20.32%
LipConvnet-40	SOC (baseline)	71.63%	54.39%	37.92%	24.13%	(+0%)
	SOC + HH	73.24%	58.12%	42.24%	28.48%	+4.35%
	+ CR	72.59%	59.04%	44.92%	32.87%	+8.74%
Table 8: Results for provable robustness against adversarial examples on the CIFAR-10 dataset.
None of these results include Last Layer Normalization (LLN).
25
Published as a conference paper at ICLR 2022
Architecture	Methods	Standard Accuracy	Provable Robust Acc. (ρ =)			Increase (108/255)
			36/255	72/255	108/255	
LipConvnet-5	SOC + HH(2)	75.85%~~	59.66%	42.68%	27.44%	+0.35%
	+ CR	74.85%	60.56%	44.96%	31.98%	+4.59%
LipConvnet-10	SOC + HH(2)	76.80%~~	61.44%	44.91%	29.70%	+0.55%
	+ CR	76.30%	62.11%	47.85%	34.49%	+5.34%
LipConvnet-15	SOC + HH(2)	77.41%~~	62.21%	45.11%	30.49%	+0.83%
	+ CR	75.73%	62.21%	47.92%	35.26%	+5.60%
LipConvnet-20	SOC + HH(2)	76.69%~~	61.58%	45.39%	30.89%	-0.19%
	+ CR	75.72%	62.61%	48.30%	35.29%	+4.21%
LipConvnet-25	SOC + HH(2)	76.12%~~	61.24%	44.81%	29.63%	+1.03%
	+ CR	75.38%	61.94%	47.67%	34.22%	+5.62%
LipConvnet-30	SOC + HH(2)	75.09%~~	60.01%	44.22%	29.39%	+1.34%
	+ CR	74.88%	61.23%	46.63%	34.02%	+5.97%
LipConvnet-35	SOC + HH(2)	73.93%~~	58.61%	42.29%	28.47%	+1.27%
	+ CR	74.14%	60.72%	46.67%	34.64%	+7.44%
LipConvnet-40	SOC + HH(2)	70.90%~~	54.96%	38.94%	24.90%	+0.77%
	+ CR	72.28%	57.67%	43.00%	30.66%	+6.53%
Table 9: Results for provable robustness on CIFAR-10 using HH activation of Order 2 (σΘ).
Increase (108/255) is calculated with respect to SOC baseline (from Tables 7, 8).
None of these results include Last Layer Normalization (LLN).
Architecture	Methods	Standard Accuracy	Provable Robust Acc. (ρ =)			Increase (Standard)
			(36/255)	(72/255)	(108/255)	
LipConvnet-5	SOC (no LLN)	75.78%~~	59.18%	42.01%	27.09%	(+0%)
	SOC + LLN	75.78%	59.58%	42.45%	27.20%	+0.00%
LipConvnet-10	SOC (no LLN)	76.45%~~	60.86%	44.15%	29.15%	(+0%)
	SOC + LLN	76.69%	61.08%	44.04%	29.19%	+0.24%
LipConvnet-15	SOC (no LLN)	76.68%~~	61.36%	44.28%	29.66%	(+0%)
	SOC + LLN	76.84%	61.94%	45.51%	30.28%	+0.16%
LipConvnet-20	SOC (no LLN)	77.05%^^	61.87%	45.79%	31.08%	(+0%)
	SOC + LLN	76.71%	61.44%	44.92%	30.19%	-0.34%
LipConvnet-25	SOC (no LLN)	75.24%~~	60.17%	43.55%	28.60%	(+0%)
	SOC + LLN	76.54%	61.21%	44.18%	29.47%	+1.30%
LipConvnet-30	SOC (no LLN)	74.51%^^	59.06%	42.46%	28.05%	(+0%)
	SOC + LLN	74.26%	58.97%	41.82%	26.93%	-0.25%
LipConvnet-35	SOC (no LLN)	73.73%~~	58.50%	41.75%	27.20%	(+0%)
	SOC + LLN	74.32%	59.05%	42.34%	28.14%	+0.59%
LipConvnet-40	SOC (no LLN)	71.63%~~	54.39%	37.92%	24.13%	(+0%)
	SOC + LLN	74.03%	58.27%	41.75%	27.12%	+2.40%
Table 10: Results for provable robustness on the CIFAR-10 dataset with and without LLN
26
Published as a conference paper at ICLR 2022
I Additional Results on CIFAR- 1 00
All results in Tables 13, 14 and 15 include Last Layer Normalization (LLN).
The rows "BCOP", "Cayley" and "SOC (baseline)" all use the MaxMin activation function. "SOC
+ HH" replaces MaxMin with 2D Householder activation of order 1 (σθ), "+ CR" adds Certificate
Regularization (CR) with γ = 0.1 (while using σθ as the activation function).
In Table 15, the row "SOC + HH(2)" uses Householder activation of order 2 (σΘ defined in Theorem
2), "+ CR" adds Certificate Regularization (CR) with γ = 0.1 (while using the HH activation of order
2 i.e σΘ as the activation function).
Architecture	Methods	Standard Accuracy	Provable Robust Acc. (ρ =)			Increase (Standard)
			36/255	72/255	108/255	
	SOC + MaxMin	42.71%~~	27.86%	17.45%	9.99%		
	+ LLN	45.86%	31.93%	21.17%	13.21%	+3.15%
ponvnet-	+ HH	46.36%	32.64%	21.19%	13.12%	+3.65%
	+ CR	45.82%	32.99%	22.48%	14.79%	+3.11%
	SOC + MaxMin	43.72%~~	29.39%	18.56%	11.16%		
	+ LLN	46.88%	33.32%	22.08%	13.87%	+3.16%
LipConvnet-10	+ HH	47.96%	34.30%	22.35%	14.48%	+4.24%
	+ CR	47.07%	34.53%	23.50%	15.66%	+3.35%
Table 11: Results for provable robustness against adversarial examples on the CIFAR-100 dataset.
Architecture	Running times (in seconds)		
	MaxMin (no LLN) MaxMin (LLN)		HH (LLN)
LipConvnet-5	3.7568	3.5002	3.6673
LipConvnet-10	5.3714	5.5482	5.5533
LipConvnet-15	7.3092	7.2595	7.3127
LipConvnet-20	9.005	9.2043	9.308
LipConvnet-25	10.9321	10.7868	11.726
LipConvnet-30	12.3198	13.2168	13.6275
LipConvnet-35	14.427	14.575	15.7069
LipConvnet-40	16.0911	16.2535	17.1015
Table 12: Inference times for various networks on the CIFAR-100 test dataset. Similar to CIFAR-10
(in Table 6), these numbers are for the whole test dataset with 10000 samples.
27
Published as a conference paper at ICLR 2022
Architecture	Methods	Standard Accuracy	Provable Robust Acc. (ρ =)			Increase (108/255)
			36/255	72/255	108/255	
	BCOP	46.31%^^	31.55%	20.34%	12.52%	-0.69%
	Cayley	44.61%	31.01%	19.84%	12.43%	-0.78%
LipConvnet-5	SOC (baseline)	45.86%	31.93%	21.17%	13.21%	(+0%)
	SOC + HH	46.36%^^	32.64%	21.19%	13.12%	-0.09%
	+ CR	45.82%	32.99%	22.48%	14.79%	+1.58%
	BCOP	45.36%^^	31.71%	20.48%	12.40%	-1.47%
	Cayley	45.79%	31.91%	20.69%	12.78%	-1.09%
LipConvnet-10	SOC (baseline)	46.88%	33.32%	22.08%	13.87%	(+0%)
	SOC + HH	47.96%^^	34.30%	22.35%	14.48%	+0.61%
	+ CR	47.07%	34.53%	23.50%	15.66%	+1.79%
	BCOP	43.70%^^	30.11%	19.85%	12.29%	-1.47%
	Cayley	45.05%	31.60%	20.32%	12.93%	-0.83%
LipConvnet-15	SOC (baseline)	47.72%	33.52%	21.89%	13.76%	(+0%)
	SOC + HH	47.72%~~	33.97%	22.45%	13.81%	+0.05%
	+ CR	47.61%	34.54%	23.16%	15.09%	+1.33%
	BCOP	39.77%^^	27.20%	17.44%	10.49%	-3.61%
	Cayley	39.68%	26.93%	17.06%	10.48%	-3.62%
LipConvnet-20	SOC (baseline)	46.86%	33.48%	22.14%	14.10%	(+0%)
	SOC + HH	47.71%^^	34.22%	22.93%	14.57%	+0.47%
	+ CR	47.84%	34.77%	23.70%	15.84%	+1.74%
Table 13: Results for provable robustness on the CIFAR-100 dataset using shallow networks.
All of these results include Last Layer Normalization (LLN).
28
Published as a conference paper at ICLR 2022
Architecture	Methods	Standard Accuracy	Provable Robust Acc. (ρ =)			Increase (108/255)
			36/255	72/255	108/255	
	BCOP	34.15%^^	21.57%	13.52%	7.97%	-5.89%
	Cayley	33.93%	21.93%	13.68%	8.19%	-5.67%
LipConvnet-25	SOC (baseline)	46.32%	32.87%	21.53%	13.86%	(+0%)
	SOC + HH	47.70%~~	34.00%	22.67%	14.57%	+0.71%
	+ CR	46.87%	34.09%	23.41%	15.61%	+1.75%
	BCOP	29.73%^^	18.69%	10.80%	6%	-7.52%
	Cayley	28.67%	18.05%	10.43%	6.09%	-7.43%
LipConvnet-30	SOC (baseline)	46.18%	32.82%	21.52%	13.52%	(+0%)
	SOC + HH	46.80%^^	33.72%	22.70%	14.31%	+0.79%
	+ CR	46.92%	34.17%	23.21%	15.84%	+2.32%
	BCOP	25.65%^^	14.88%	8.47%	4.30%	-8.95%
	Cayley	27.75%	16.37%	9.52%	5.40%	-7.85%
LipConvnet-35	SOC (baseline)	45.22%	32.10%	21.28%	13.25%	(+0%)
	SOC + HH	46.21%^^	32.80%	21.55%	14.13%	+0.88%
	+ CR	46.88%	33.64%	23.34%	15.73%	+2.48%
	BCOP	30.66%^^	18.68%	10.46%	5.92%	-6.07%
	Cayley	25.54%	14.91%	8.37%	4.40%	-7.59%
LipConvnet-40	SOC (baseline)	42.94%	29.71%	19.30%	11.99%	(+0%)
	SOC + HH	45.84%^^	32.79%	21.98%	14.07%	+2.08%
	+ CR	45.03%	32.57%	22.37%	14.76%	+2.77%
Table 14: Results for provable robustness on the CIFAR-100 dataset using deeper networks.
All of these results include Last Layer Normalization (LLN).
29
Published as a conference paper at ICLR 2022
Architecture	Methods	Standard Accuracy	Provable Robust Acc. (ρ =)			Increase (108/255)
			36/255	72/255	108/255	
LipConvnet-5	SOC + HH(2)	46.61%~~	32.50%	21.34%	13.22%	+0.01%
	+ CR	46.69%	33.22%	22.34%	14.30%	+1.09%
LipConvnet-10	SOC + HH(2)	47.47%~~	33.32%	21.84%	13.75%	-0.01%
	+ CR	47.53%	34.52%	23.06%	15.07%	+1.31%
LipConvnet-15	SOC + HH(2)	47.19%~~	33.67%	22.36%	13.78%	-0.09%
	+ CR	47.22%	34.04%	22.98%	15.28%	+1.41%
LipConvnet-20	SOC + HH(2)	47.86%^^	33.93%	22.44%	14.41%	+0.31%
	+ CR	47.54%	34.32%	23.53%	15.54%	+1.44%
LipConvnet-25	SOC + HH(2)	47.86%^^	33.97%	22.78%	14.59%	+0.73%
	+ CR	47.50%	34.38%	23.92%	15.92%	+2.06%
LipConvnet-30	SOC + HH(2)	46.23%~~	32.64%	21.95%	14.00%	+0.48%
	+ CR	46.36%	33.20%	22.70%	14.85%	+1.33%
LipConvnet-35	SOC + HH(2)	46.06%^^	32.35%	21.33%	13.65%	+0.40%
	+ CR	45.78%	33.24%	22.39%	14.78%	+1.53%
LipConvnet-40	SOC + HH(2)	43.81%~~	30.59%	20.08%	12.56%	+0.57%
	+ CR	45.61%	32.50%	22.36%	14.84%	+2.85%
Table 15: Results for provable robustness on CIFAR-100 using HH activation of Order 2 (σΘ).
Increase (108/255) is calculated with respect to SOC baseline (from Tables 13, 14).
All of these results include Last Layer Normalization (LLN).
Architecture	Methods	Standard Accuracy	Provable Robust Acc. (ρ =)			Increase (Standard)
			(36/255)	(72/255)	(108/255)	
LipConvnet-5	SOC (no LLN)	42.71%~~	27.86%	17.45%	9.99%	(+0%)
	SOC + LLN	45.86%	31.93%	21.17%	13.21%	+3.15%
LipConvnet-10	SOC (no LLN)	43.72%~~	29.39%	18.56%	11.16%	(+0%)
	SOC + LLN	46.88%	33.32%	22.08%	13.87%	+3.16%
LipConvnet-15	SOC (no LLN)	42.92%~~	28.81%	17.93%	10.73%	(+0%)
	SOC + LLN	47.72%	33.52%	21.89%	13.76%	+4.80%
LipConvnet-20	SOC (no LLN)	43.06%~~	29.34%	18.66%	11.20%	(+0%)
	SOC + LLN	46.86%	33.48%	22.14%	14.10%	+3.80%
LipConvnet-25	SOC (no LLN)	43.37%~~	28.59%	18.18%	10.85%	(+0%)
	SOC + LLN	46.32%	32.87%	21.53%	13.86%	+2.95%
LipConvnet-30	SOC (no LLN)	42.87%~~	28.74%	18.47%	11.21%	(+0%)
	SOC + LLN	46.18%	32.82%	21.52%	13.52%	+3.31%
LipConvnet-35	SOC (no LLN)	42.42%~~	28.34%	18.10%	10.96%	(+0%)
	SOC + LLN	45.22%	32.10%	21.28%	13.25%	+2.80%
LipConvnet-40	SOC (no LLN)	41.84%~~	28.00%	17.40%	10.28%	(+0%)
	SOC + LLN	42.94%	29.71%	19.30%	11.99%	+1.10%
Table 16: Results for provable robustness on the CIFAR-100 dataset with and without LLN
30