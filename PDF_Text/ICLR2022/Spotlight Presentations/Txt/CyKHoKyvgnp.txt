Published as a conference paper at ICLR 2022
Transition to Linearity of Wide Neural Net-
works is an Emerging Property of Assembling
Weak Models
Chaoyue Liu
Depart. of Computer Science
Ohio State University
liu.2656@osu.edu
Libin Zhu	Mikhail Belkin
Depart. of Computer Science HDSI
UC, San Diego	UC, San Diego
l5zhu@ucsd.edu	mbelkin@ucsd.edu
Ab stract
Wide neural networks with linear output layer have been shown to be near-linear,
and to have near-constant neural tangent kernel (NTK), in a region containing
the optimization path of gradient descent. These findings seem counter-intuitive
since in general neural networks are highly complex models. Why does a linear
structure emerge when the networks become wide? In this work, we provide a
new perspective on this “transition to linearity” by considering a neural network
as an assembly model recursively built from a set of sub-models corresponding
to individual neurons. In this view, we show that the linearity of wide neural
networks is, in fact, an emerging property of assembling a large number of diverse
“weak” sub-models, none of which dominate the assembly.
1	Introduction
Success of gradient descent methods for optimizing neural networks, which generally correspond
to highly non-convex loss functions, has long been a challenge to theoretical analysis. A series
of recent works including Du et al. (2018; 2019); Allen-Zhu et al. (2019); Zou et al. (2018); Oy-
mak & Soltanolkotabi (2020) showed that convergence can indeed be shown for certain types of
wide networks. Remarkably, Jacot et al. (2018) demonstrated that, when the network width goes
to infinity, the Neural Tangent Kernel (NTK) of the network becomes constant during training with
gradient flow, a continuous time limit of gradient descent. Based on that Lee et al. (2019) showed
that the training dynamics of gradient flow in the space of parameters for the infinite width network
is equivalent to that of a linear model.
As discussed in (Liu et al., 2020), the constancy of the Neural Tangent Kernel stems from the fact
that wide neural networks with linear output layer “transition to linearity” as their width increases to
infinity. The network becomes progressively more linear in O(1)-neighborhoods around the network
initialization, as the network width grows. Specifically, consider the neural network as a function
f(θ) of its parameters θ and write the Taylor expansion with the Lagrange remainder term:
f (θ) = f (θo) + Vf (θo)T(θ - θo) + 1(θ - θo)TH(ξ)(θ -θo),
(1)
where H is the Hessian, the second derivative matrix of f, and ξ is a point between θ and θ0 . The
“transition to linearity” is the phenomenon when the quadratic term 2(θ 一 θ0)TH(ξ)(θ — θ0) tends
to zero in an O(1) ball around θ0 as the network width increases to infinity.
In particular, transition to linearity provides a method for showing that the square loss function of
wide neural networks satisfy the Polyak-Lojasiewicz (PL) inequality which guarantees convergence
of (Stochastic) Gradient Descent to a global minimum (Liu et al., 2022).
While the existing analyses demonstrate transition to linearity mathematically, the underlying struc-
ture of that phenomenon does not appear to be fully clear. What structural properties account for it?
Are they specific to neural networks, or also apply to more general models?
In this paper, we aim to explain this phenomenon from a more structural point of view and to reveal
some underlying mechanisms. We provide a new perspective in which a neural network, as well as
1
Published as a conference paper at ICLR 2022
each of its hidden layer neurons before activation, can be viewed as an “assembly model”, built up
by linearly assembling a set of sub-models, corresponding to the neurons from the previous layer.
Specifically, a (l + 1)-layer pre-activation neuron
α(l+1)
m
1	(l)
√m 工 Wia，，
i=1
as a linear combination of the l-th layer neurons αi(l), is considered as an assembly model, whereas
the l-layer neurons a(l) are considered as sub-models. Furthermore, each pre-activation α(l) is
also an assembly model constructed from (l - 1)-th layer neurons αi(l-1). In this sense, the neural
network is considered as a multi-level assembly model. To show O(1)-neighborhood linearity, we
prove that the quadratic term in the Taylor expansion Eq.(1) vanishes in O(1)-neighborhoods of
network initialization, as a consequence of assembling sufficiently many sub-models .
To illustrate the idea of assembling, we start with a simple case: assembling independent sub-models
in Section 2. The key finding is that, as long as the assembly model is not dominated by one
or a few sub-models, the quadratic term in the Taylor expansion Eq.(1) becomes small in O(1)-
neighborhoods of the parameter space when the number of sub-models m is tends to infinity. This
means that as m increases to infinity, the assembly model becomes a linear function of parameters.
Since we put almost no requirements on the form of sub-models, it is the assembling process that
results in the linearity of the assembly model. This case includes two-layer neural networks as
examples.
For deep neural networks (Section 3), we consider a neural network as a multi-level assembly model
each neuron is considered as an assembly model constructed iteratively from all the neurons from
the previous layer. We then follow an inductive argument: using near-linearity of previous-layer
pre-activation neurons to show the linearity of next-layer pre-activation neurons. Our key finding
is that, when the network width is large, the neurons within the same layer become essentially
independent to each other in the sense that their gradient directions are orthogonal in the parameter
space. This orthogonality allows for the existence ofanew coordinate system such that these neuron
gradients are parallel to the new coordinate axes. Within the new coordinate system, one can apply
the argument of assembling independent sub-models, and obtain the O(1)-neighborhood linearity
of the pre-activation neurons, as well as the output of the neural network.
We further point out that this assembling viewpoint and the O(1)-neighborhood linearity can be
extended to more general neural network architectures, e.g., DenseNet.
We end this section by commenting on a few closely related concepts and point out some of the
differences.
Boosting. Boosting (Schapire, 1990; Freund, 1995), which is a popular method that combines
multiple “weak” models to produce a powerful ensembe model, has a similar form with the assembly
model, Eq.(4), see, e.g., Friedman et al. (2000). However, we note a few key differences between the
two. First, in boosting, each “weak” model is trained separately on the dataset and the coefficients
(i.e., vi in Eq.(4)) of the weak models are determined by the model performance. In training an
assembly model, the “weak” sub-models (i.e., hidden layer neurons), are never directly evaluated on
the training dataset, and the coefficients vi are considered as parameters of the assembly model and
trained by gradient descent. Second, in boosting, different data samples may have different sample
weights. In assembling, the data samples always have a uniform weight.
Bagging and Reservoir computing. Bagging (Breiman, 1996) and Reservoir computing (Jaeger,
2001) are two other ways of combining multiple sub-models to build a single model. In bagging,
each sub-model is individually trained and the ensemble model is simply the average or the max of
the sub-models. In reservoir computing, each sub-model is fixed, and only the coefficients of the
linear combination are trainable.
Notation. We use bold lowercase letters, e.g., v, to denote vectors, capital letters, e.g., W, to
denote matrices. We use ∣∣ ∙ ∣∣ to denote the Euclidean norm of vectors and spectral norm (i.e.,
matrix 2-norm) for matrices. We denote the set {1,2,…，n} as [n]. We use Vwf to denote the
2
Published as a conference paper at ICLR 2022
partial derivative of f with respect to w. When w represents all of the parameters of f, we omit the
subscript, i.e., Vf.
2 Assembling Independent Models
In this section, we consider assembling a sufficiently large number m of independent sub-models
and show that the resulting assembly model is (approximately) linear in O(1)-neighborhood of the
model parameter space.
Ingredients: sub-models. Let’s consider a set of m (sub-)models {gi}im=1, where each model gi,
with a set of model parameters wi ∈ Di ⊂ Pi = Rpi, takes an input x ∈ Dx ⊂ Rd and outputs a
scalar prediction gi (wi ; x). Here Pi is the parameter space of sub-model gi and Dx is the domain
of the input. By independence, we mean that any two distinct models gi and gj share no common
model parameters:
Pi ∩ Pj = {0},	∀i 6= j ∈ [m].	(2)
In this sense, the change of gi ’s output, as a result of change of it parameters wi, does not affect the
outputs of the other models gj where j 6= i.
In addition, we require that there is no dominating sub-models over the others. Specifically, we
assume the following in this section:
Assumption 1 (No dominating sub-models). There exists a constant c ∈ (0, 1) independent of m
such that, for any parameter setting {wi : wi ∈ Di }im=1 and input x ∈ Dx,
median[∣gι(wι; x)|,…，|gm(wm； x)|]、
--------------------------------≥ c.
max[∣gl(wi; x)|,…，|gm(Wm； x)|] 一 ∙
(3)
Furthermore, we assume that max[∣gι(wι; x)|, ∙∙∙ , |gm(wm； x)|] ∈ [a,b] ⊂ R for some constants
a > 0 and b > 0.
Remark 1. This assumption guarantees that most of the outputs of the sub-models are at the same
order, typically O(1). It makes sure that the resulting assembly model is not dominated by one or a
minor portion of the sub-models. This is typically seen for the neurons of neural networks.
We further make the following technical assumption, which is common in the literature.
Assumption 2 (Twice differentiablity and smoothness). Each sub-model gi is twice differentiable
with respect to the parameters wi, and is β-smooth in the parameter space: there exists a positive
number β such that, for any i ∈ [m],
kVgi(wi;x) - Vgi(w0i;x)k ≤ βkwi - wi0 k.
This assumption makes sure that for each gi, the gradient is well-defined and its Hessian has a
bounded spectral norm.
Assembly model. Based on these sub-models {gi}im=1, we construct an assembly model (or super-
model) f, using linear combination as follows:
1m
f(θ; χ) ：= s(m)E^igi(Wi χ),	(4)
where vi is the weight of the sub-model gi, and 1/s(m), as a function ofm, is the scaling factor.
Here, we denote θ as the set of parameters of the assembly model, θ := (w1, . . . , wm). The total
number of parameters that f has is p = Pim=1 pi . Typical choices for the weights vi are setting
vi = 1 for all i ∈ [m], or random i.i.d. drawing vi from some zero-mean probability distribution.
Remark 2. For the ease of analysis and simplicity of notation, we assumed that the outputs of
assembly model f and sub-models gi are scalars. It is not difficult to see that our analysis below
also applies to the scenarios of finite dimensional model outputs.
The scaling factor 1/s(m). The presence of the scaling factor 1/s(m) is to keep the output of
assembly model f at the order O(1) w.r.t. m. In general, we expect that s(m) grows with m. In
particular, when vi are chosen i.i.d. from a probability distribution with mean zero, e.g., {-1, 1},
3
Published as a conference paper at ICLR 2022
the sum in Eq.(4) is expected to be of the order √m, under the Assumption 1. In this case, the
scaling factor 1∕s(m) = O(1∕√m).
Now, we will show that the assembly model f has a small quadratic term in its Taylor expansion,
when the size of the set of sub-models i.e. m, is sufficiently large.
Theorem 1. Consider the assembly model f constructed in Eq.(4) with each vi either set to be 1
or randomly drawn from {-1, 1}, and suppose Assumption 1 and 2 hold. Given a positive number
R > 0 and a parameter setting θ0 ∈ Rp, for any θ ∈ Rp such that kθ - θ0 k ≤ R, the absolute value
of the quadratic term in Taylor expansion Eq.(1) is bounded by:
1(θ -θo)TH(ξ)(θ - θo) ≤
βR2
2s(m) .
(5)
Proof. In what follows, we don’t explicitly write out the dependence on θ and x in the Hessian
H := df, for simplicity of notation. We further denote Hg := ∂⅛ as the Hessian of gi.
∂θ	i	∂wi
We decompose the assembly model Hessian H as a linear combination of Hgi. By the definition
of the assembly model in Eq.(4), an arbitrary entry Hjk of the Hessian can be written as: Hjk =
S(m)Pm=I Vidd⅛. Here θj, θk are two individual parameters of the assembly model f. Note that
∂2g
∂θ.∂θk is non-zero, only if both θj and θk are parameters of sub-model gi, i.e., θj, θk ∈ Pi. Hence,
the assembly model Hessian can be decomposed as a linear combination of sub-model Hessians:
H = s(m)Pm=I ViHgi. Therefore, the quadratic term becomes
m
"7^X Vi
s(m)
i=1
1(θ - θo)τHgi(ξ)(θ - θo)
1m	1
——-Vi	-	(Wi	—
s(m)	|_2	l
Wi,0)τHgi (ξ)(Wi - Wi,0)
and its absolute value is bounded by
12(θ - θo)τh(ξ)(θ - θo)∣ ≤
m
2SmX |viHlHgik ∙kwi- wi,ok2
2s(m)
m
kwi -wi,0k2
i=1
In the second inequality, we used that |Vi| = 1 and that gi is β-smooth. Because of the independence
of the sub-models as seen in Eq.(2), the summation in the above equation becomes kθ - θ0k2, which
is bounded by R2, as stated in the theorem condition. Therefore, We conclude the theorem. □
It is important to note that β is a constant and 1∕s(m) = O(1/√m). Then we have the following
corollary in the limiting case.
Corollary 1. Consider the assembly model f under the same setting as in Theorem 1. If the number
of sub-models m increases to infinity, then for all parameter θ0 and input x, the quadratic term
∣2(θ -θo)τH(ξ)(θ -θo)∣→ 0,	(6)
as long as θ is within an O(1)-neighborhood of θ0, i.e., kθ - θ0k2 ≤ Rfor some constant R > 0.
Example: Two-layer neural networks. A good example of this kind of assembly model is the
two-layer neural network. A two-layer neural network is mathematically defined as:
1m
f (W, v; x) = √m^uiσ(WTx),
(7)
where m is the number of hidden layer neurons, σ(∙) isthe activation function, X ∈ Rd is the network
input, and W ∈ Rm×d and u ∈ Rm are the parameters for the first and second layer, respectively.
Here, we can view the i-th hidden neuron and all the parameters wi and ui that connect to it as the
i-th sub-model: gi = uiσ (wiτx). We see that these sub-models do not share parameters, and each
sub-model has d + 1 parameters. In addition, the weights of the sub-models are all 1. By Corollary
1, the two-layer neural network becomes a linear model in any O(1)-neighborhoods, as the network
width m increases to infinity. This is consistent with the previous observation that a two-layer neural
network transitions to linearity Liu et al. (2020).
4
Published as a conference paper at ICLR 2022
Gaussian distributed weights vi. Another common way to set the weights vi of the sub-models is
to independently draw each Vi fromN(0,1). In this case, Vi is unbounded, but with high probability
the quadratic term is still O(log(m)/√m). Please see the detailed analysis in Appendix A.
Hierarchy of assembly models. In principle, we can consider the assembly model f, together
with multiple similar models independent to each other, as sub-models, and construct a higher-level
assembly model. Repeating this procedures, we can have a hierarchy of assembly models. Our
analysis above also applies to this case and each assembly model is also O(1)-neighborhood linear,
when the number of its sub-models is sufficiently large. However, one drawback of this hierarchy
is that the total number of parameters of the highest level assembly model increase exponentially
with the number of levels. We will see shortly that wide neural networks, as a hierarchy of assembly
models, allows overlapping between sub-models, but still keeping the O(1)-neighborhood linearity.
3	Deep Neural Networks as Assembling Models
In this section, we view deep neural networks as multi-level assembly models and show how the
O(1)-neighborhood linearity arises naturally as a consequence of assembling.
Setup. We start with the definition of multi-layer neural networks. A L-layer full-connected neural
network is defined as follows:
α(0) = x,
α(I) = σ(α(I)), α(I) = , 1	W(I)α(IT), ∀l = 1, 2,…，L,	(8)
√m-r
f = α(L),
where σ(∙) is the activation function and is applied entry-wise above. We assume σ(∙) is twice
differentiable to make sure that the network f is twice differentiable and its Hessian is well-defined.
We also assume that σ(∙) is an injective function, which includes the commonly used sigmoid,tanh,
SoftPlus, etc. In the network, with m? being the width of l-th hidden layer, a(l) ∈ Rml (called
pre-activations) and α(l) ∈ Rml (called post-activations) represent the vectors of the l-th hidden
layer neurons before and after the activation function, respectively. Denote θ := (W(1), . . . , W(L)),
with W(l) ∈ Rml-1 ×ml, as all the parameters of the network, and θ(l) := (W(1), . . . , W(l)) as the
parameters before layer l. We also denote p and p(l) as the dimension of θ and θ(l), respectively.
Denote θ(αi(l)) as the set of the parameters that neuron αi(l) depends on.
This neural network is typically initialized following Gaussian random initialization: each param-
eter is independently drawn from normal distribution, i.e., (W0l))j 〜 N(0,1). In this paper, we
focus on the O(1)-neighborhood of the initialization θ0. As pointed out by Liu et al. (2020), the
whole gradient descent trajectory is contained in a O(1)-neighborhood of the initialization θ0 (more
precisely, a Euclidean ball B(θ0, R) with finite R).
Overview of the methodology. As it is defined recursively in Eq.(8), we view the deep neural
network as a multi-level assembly model. Specifically, a pre-activation neuron at a certain layer l is
considered as an assembly model constructed from the post-activation neurons at layer l - 1, at the
same time its post-activation also serves as a sub-model for the neurons in layer l + 1. Hence, a pre-
activation αil) is an l-level assembly model, and the post-activation α(l) is a (l + 1)-level sub-model.
To prove the O(1)-neighborhood linearity of the neural network, we start from the linearity at the
first layer, and then follow an inductive argument from layer to layer, up to the output. The main
argument lies in the inductive steps, from layer l to layer l + 1. Our key finding is that, in the infinite
width limit, the neurons within the same layer become independent to each other, which allows us
using the arguments in Section 2 to prove the O(1)-neighborhood linearity. Since the exact linearity
happens in the infinite width limit, in the following analysis we take m1, m2, . . . , mL-1 → ∞
sequentially, the same setting as in Jacot et al. (2018). Note that for neural networks that finite but
large network width, the linearity will be approximate; the larger the width, the closer to linear.
5
Published as a conference paper at ICLR 2022
3.1	Base case: First and second hidden layer.
The first hidden layer pre-activations are defined as: α(1) = √1= W(1)x, where m° = d. It is
obvious that each element of ɑ(1) is linear in its parameters. Each second layer pre-activation
α(1), i ∈ [m2] can be considered as a two-layer neural network with parameters {W(1), w(2)},
where wi(2) is the i-th row of W(2). As we have seen in Section 2, the sub-models of a two-layer
neural network share no common parameters, and the assembly model, which is the network itself,
is O(1)-neighborhood linear in the limit of m1 → ∞.
3.2	FROM LAYER l TO LAYER l + 1.
First, we make the induction hypothesis at layer l.
Assumption 3 (Induction hypothesis). Assume that allthe l-th layer pre-activation neurons α(l) are
O(1)-neighborhood linear in the limit of m1, . . . , ml-1 → ∞.
Under this assumption, we will first show two key properties about the sub-models, i.e., the l-th layer
post-activation neurons α(l): (1) level sets of these neurons are hyper-planes with co-dimension 1 in
O(1)-neighborhoods; (2) normal vectors of these hyper-planes are orthogonal with probability one
in the infinite width limit.
Setup. Note that l-th layer neurons only depend on the parameters θ(l). Without ambiguity, denote
the parameter space spanned by θ(l) as P. We can write θ(l) as
θ(l) = θ(l-1) ∪ w(1l) ∪ . . . ∪ w(l),
1	ml
where wi(l) is the i-th row of the weight matrix W(l). One observation is that the set of parameters
of neuron αi(l) is θ(αi(l)) = θ(l-1) ∪ wi(l). Namely, θ(l-1) are shared by all l-layer neurons and
each parameter in wi(l) is privately owned by only one neuron αi(l). Hence, we can decompose the
parameter space P as: P = Lim=l0 Pi, where common space P0 is spanned by θ(l-1) and the private
spaces Pi, i 6= 0, are spanned by wi(l). Define a set of projection operators {πi}im=l0 such that, for
any vector z ∈ P, πi(z) ∈ Pi, and Pim=l0 πi(z) = z.
Property 1: Level sets of neurons are hyper-planes. The first key observation is that each of the
l-th layer post-activations α(l) has linear level sets in the O(1)-neighborhoods.
Theorem 2 (Linear level sets of neurons). Assume the induction hypothesis Assumption 3 holds.
Given a fix input x, for each post-activation αi(l), i ∈ [ml], the level set
Sc ：= {θ(l) ： α(l)(θ(l); X)= c}∩N(θo),	(9)
is linear or an empty set, for all c ∈ R, where N(θ0) is an O(1)-neighborhood of θ0. Moreover,
these level sets are parallel to each other: for any c1,c2 ∈ R, if Sci = 0 and Sc2 = 0, then Sci and
Sc2 are parallel to each other.
Remark 3. With abuse of notation, we did not explicitly write out the dependence of the level set
on the neuron αi(l), the input x, and the network initialization θ0.
Proof. First, note that all the pre-activation neurons α(l) are linear in N(θ0) in the limit of
m1 , . . . , ml-1 → ∞, as assumed in the induction hypothesis. This linearity of these function
guarantees that all the level sets of the pre-activations {θ(l) : α(l)(θ(l); x) = c} ∩ N(θ0), for all
c ∈ R and i ∈ [ml] are either linear or empty sets, and they are parallel to each other. Since the
activation function σ(∙) is element-wisely applied to the neurons, it does not change the shape and
direction of the level sets. Specifically, if {θ(l) : αi(l)(θ(l); x) = c} is non-empty, then
{θ(l) : α(l)(θ(l); x) = c} = {θ(l) : α(l)(θ(l); x) = σ-1(c)}.
Therefore, {θ(l) : α(l)(θ; x) = c}∩N(θo) is linear or an empty set, and the activation function σ(∙)
preserves the parallelism.	□
6
Published as a conference paper at ICLR 2022
This means that, although each of the neurons α(l)
is constructed by a large number of neurons in pre-
vious layers containing tremendously many param-
eters and is transformed by non-linear functions, it
actually has a very simple geometric structure. A
geometric view of the post-activation neurons is il-
lustrated in Figure 1.
As the neurons have scalar outputs and σ(∙) is in-
jective by assumption, each level set is a piece of
a co-dimension 1 hyper-plane in the p-dimensional
parameter space P . Hence, at each point of the
hyper-plane there exists a unique (up to a negative
sign) unit-length normal vector n, which is perpen-
dicular to the hyper-plane. A direct corollary of the
Figure 1: A geometric view of the post-
activation neurons. Level sets are parallel
hyper-planes.
linearity of the level sets, Theorem 2, is that the normal vector n is identical everywhere in the O(1)-
neighborhood N (θ0).
Corollary 2. Assume the induction hypothesis Assumption 3 holds. Given a specific neuron αi(l)
and an input x, for any θ1(l), θ2(l) ∈ P ∩ N (θ0), n(θ1(l)) = n(θ2(l)).
Hence, we can define ni as the normal vector for each neuron αi(l) . The next property is about the
set of normal vectors {ni}im=l1.
Property 2: Orthogonality of normal vectors ni . Now, let’s look at the directions of the normal
vectors ni . Note that the neuron αi(l) does not depend on the parameters wj(l) for any j 6= i, hence
πj (ni) = 0 for all j ∈/ {i, 0}. That means:
ProPosition 1. For all i ∈ [mi ], the normal vector n% resides in the sub-space Pi ㊉ Po, and can be
decomposed as ni = πi (ni) + π0 (ni).
As πi (ni) ∈ Pi, and Pi ∩ Pj = {0} for i 6= j, the components {πi (ni)}im=l1 are orthogonal to each
other:
πi(ni) ⊥ πj(nj),	for all i 6= j ∈ [ml].	(10)
By Proposition 1, to show the orthogonality of the normal vectors {ni}im=l1, it suffices to show the
orthogonality of {π0(ni)}im=l1.
Since it is perpendicular to the corresponding level sets, the normal vector ni is parallel to the
gradient Vαil), UP to a potential negative sign. Similarly, the projection ∏o (n2) is parallel to the
partial gradient V°(i-i) a(l).
By the definition of neurons in Eq.(8) and the constancy of ni in the neighborhood N (θ0), we have
(Vθ(i-1) α(l))T = √^(w(l0 )T Vα(IT).	(11)
ml-1	,
Here, because a(l-1) is an mi-dimensional vector, Vα(I-I) is an mi X P(I-I) matrix, where P(I-I)
denotes the size of θ(l-1). It is important to note that the matrix Vα(I-I) is shared by all the neurons
in layer l and is independent of the index i, while the vector wi(,i0) , which is wi(i) at initialization, is
totally private to the l-th layer neurons.
Recall that the vectors {wi(,i0)}im=l1 are independently drawn from N (0, Iml-1 ×ml-1 ). As is well-
known, when the vector dimension mi-1 is large, these independent random vectors {wi(,i0) } are
nearly orthogonal. When in the infinite width limit, the orthogonality holds with probability 1:
Ve > 0, lim P ∣ —1— ∣(w(l0)TWjlO | ≥ e) = 0, for all i = j ∈ [mi].	(12)
ml-1→∞	mi-1	,	,
From Eq.(11), We see that the partial gradients {Vθ(i-i) (0(l}miι are in fact the result of applying
a universal linear transform, i.e., Vα(I-I), onto a set of nearly orthogonal vectors. The following
lemma shows that the vectors remain orthogonal even after this linear transformation.
7
Published as a conference paper at ICLR 2022
Lemma 1.
Ve > 0, ^lim OoP (go(i-i)α(l 2))TVθ(i-υαjl) ∣ ≥ e) = 0, for all i = j ∈ [mi].	(13)
See the proof in Appendix C.
Recalling Eq.(10) and the fact that the normal vectors π0(ni) are parallel to the gradients
Vθ(i-υ ɑ(I), we immediately have that these normal vectors are orthogonal with probability 1, in
the limit of ml-1 → ∞, as stated in the following theorem.
Theorem 3 (Orthogonality of normal vectors).
Ve > 0, lim P (InTnj∙ I ≥ e) = 0, for all i = j ∈ [ml].	(14)
ml-1→∞
Remark 4. As seen in Section 2, a two-layer neural network Eq.(7) is an assembly model with
independent sub-models. The normal vectors {ni}im=l1 are exactly orthogonal to each other, even for
finite hidden layer width.
See Appendix B for an numerical verification of
this orthogonality. This orthogonality allows the
existence of a new coordinate system such that all
the normal vectors are along the axes. Specifically,
in the old coordinate system O, each axis is along
an individual neural network parameter θi ∈ θ with
i ∈ [p]. After an appropriate rotation, O can be
transformed to a new coordinate system O0 such
that each normal vector ni is parallel to one of the
new axes. See Figure 2 for an illustration. Denote
θ0 as the set of new parameters that are long the
axes of new coordinate system O0 : θ0 = Rθ, where
R is a rotation operator.
The interesting observation is that each neuron αi(l)
essentially depends on only one new parameter
Figure 2: Coordinate systems: O0 (new) vs. O
(old). normal vectors ni and gradients Nai are
along an axis of O0.
θi0 ∈ θ0 , because its gradient direction is parallel with one normal vector ni and ni is along one
axis in the new coordinate system O0 . Moreover, different neurons depends on different new pa-
rameters, as normal vectors are never parallel. Hence, these neurons are essentially independent to
each other. This view actually allows us to use the analysis for assembling independent models in
Section 2 to show the O(1)-neighborhood linearity at layer l + 1, as follows.
Linearity of the pre-activations in layer l+1. Having the properties for neurons in layer l discussed
above, We are now ready to analyze the pre-aCtiVation neurons a(l+1) in layer l + 1 as assembly
models. Recall that each pre-activation α(l+1) is defined as:
ml
a(l+1)= ɪ X w(l+1)ajl).
√mli=1 j j
(15)
Without ambiguity, we omitted the index i for the pre-activation α(l+1) and the weights W(I+1).
We want to derive the quadratic term (i.e., the Lagrange remainder term) of α(l+1) in its Taylor ex-
pansion and to show it is arbitrarily small, in the O(1)-neighborhood N (θ0). First, let’s consider the
special case where the parameters W(I+1)are fixed, i.e., w(l+1) = w0l+1), and α(l+1) only depends
on θ(l). This case conveys the key concepts of the O(1)-neighborhood linearity after assembling.
We will relax this constraint in Appendix D.
Consider an arbitrary parameter setting θ ∈ N(θ0), and let R := kθ - θ0k = O(1). By the
definition of assembly model α(l+1) in Eq.(15), the quadratic term in its Taylor expansion Eq.(1)
can be decomposed as:
1	1 ml
2(θ - θo) Hα(i+i)(ξ)(θ - θo) = -y= E wifi
2	ml i=1
1(θ - θ0)TH0(i)(ξ)(θ-θ0
(16)
8
Published as a conference paper at ICLR 2022
where H⅛0+i)= 叱；+1) and H(1)= d02 are the Hessians of α(l+1) and α(l), respectively, and
α	∂θ	αi	∂θ
ξ ∈ P is on the line segment between θ(l) and θ0(l).
We bounded the term in the square bracket using a treatment analogous to Theorem 1. First, as seen
in Property 1, the level sets of αi(l) are hyper-planes with co-dimension 1, perpendicular to ni. Then
the value of αi(l) only depends on the component (θ - θ0)Tni, and Hessian H (l) is rank 1 and can
αi
be written as H (l) = cni niT, with some constant c ≤ β. Hence, we have
αi
∣(θ - θo)τHa(i) (ξ)(θ — θo)∣ = C ((θ — θo)τni)2 ≤ β ((θ — θo)τni)2 .
Here β is the smoothness of the sub-model, i.e., post-activation α(l). By Eq.(16), we have:
1	β	ml	2
2	l(θ -θO) Ha(l + 1) (ξ)(θ — θO)I ≤ 2 r— max(|wi,0 I) X ((θ — θO) ni) .	(17)
2	2 ml	i=1
Second, by the orthogonality of normal vectors as in Theorem 3, we have
X ((θ — θo)Tni)2 ≤kθ — θok2 = R2.	(18)
i=1
Combining the above two equations, we obtain the following theorem which upper bound the mag-
nitude of the quadratic term of a(l+1):
Theorem 4 (Bounding the quadratic term). Assume that the parameters in layer l + 1 are fixed to
the initialization. For any parameter setting θ ∈ N (θO),
2 ∣(θ — θo)THa(l+1) (ξ)(θ — θo)∣ ≤ 2^7= max(Iw(I+1)}	(19)
2	2 ml
For the Gaussian random initialization, the above upper bound is of the order O(log(mι)∕√ml).
Hence, as m√ → ∞, the quadratic term of pre-activation α(l+1) in the (l + 1)-th layer vanishes, and
the function ɑ(l+1) becomes linear.
Concluding the induction analysis. Applying the analysis in Section 3.2, Theorem 2 - 4, with
a standard induction argument, we conclude that the neural network, as well as each of its
hidden pre-activation neurons, is O(1)-neighborhood linear, in the infinite network width limit,
m1, . . . , mL-1 → ∞.
Extension to other architectures. In Appendix E, we further show that the assembly analysis and
the O(1)-neighborhood linearity also hold for DenseNet (Huang et al., 2017).
4 Conclusion and Future directions
In this work, we viewed a wide fully-connected neural network as a hierarchy of assembly models.
Each assembly model corresponds to a pre-activation neuron and is linearly assembled from a set of
sub-models, the post-activation neurons from the previous layer. When the network width increases
to infinity, we observed that the neurons within the same hidden layer become essentially indepen-
dent. With this property, we shown that the network is linear in an O(1)-neighborhood around the
network initialization.
We believe the assembly analysis and the principles we identified, especially the essential indepen-
dence of sub-models, and their iterative construction, are significantly more general than the specific
structures we considered in this work, and, for example, apply to a broad range of neural architec-
tures. In future work, we aim to apply the analysis to general feed-forward neural networks, such as
architectures with arbitrary connections that form an acyclic graph.
9
Published as a conference paper at ICLR 2022
Acknowledgements
We are grateful for the support of the NSF and the Simons Foundation for the Collaboration on the
Theoretical Foundations of Deep Learning1 through awards DMS-2031883 and #814639. We also
acknowledge NSF support through IIS-1815697 and the TILOS institute (NSF CCF-2112665).
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252, 2019.
Leo Breiman. Bagging predictors. Machine learning, 24(2):123-140, 1996.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675-
1685, 2019.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2018.
Yoav Freund. Boosting a weak learning algorithm by majority. Information and computation, 121
(2):256-285, 1995.
Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic regression: a statistical
view of boosting (with discussion and a rejoinder by the authors). The annals of statistics, 28(2):
337-407, 2000.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700-4708, 2017.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing systems, pp. 8571-
8580, 2018.
Herbert Jaeger. The “echo state” approach to analysing and training recurrent neural networks-with
an erratum note. Bonn, Germany: German National Research Center for Information Technology
GMD Technical Report, 148(34):13, 2001.
Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selec-
tion. Annals of Statistics, pp. 1302-1338, 2000.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in neural information processing systems, pp. 8570-8581,
2019.
Chaoyue Liu, Libin Zhu, and Mikhail Belkin. On the linearity of large non-linear models: when
and why the tangent kernel is constant. Advances in Neural Information Processing Systems, 33,
2020.
Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Loss landscapes and optimization in over-
parameterized non-linear systems and neural networks. Applied and Computational Harmonic
Analysis, 2022.
Samet Oymak and Mahdi Soltanolkotabi. Toward moderate overparameterization: Global con-
vergence guarantees for training shallow neural networks. IEEE Journal on Selected Areas in
Information Theory, 1(1):84-105, 2020.
Robert E Schapire. The strength of weak learnability. Machine learning, 5(2):197-227, 1990.
1https://deepfoundations.ai/
10
Published as a conference paper at ICLR 2022
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010.
Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888, 2018.
11
Published as a conference paper at ICLR 2022
A Random sub-model weights with Gaussian distribution
Let’s consider the case where the weights vi of the sub-models are randomly drawing from Gaussian
distributions, for example the normal distribution N(0, 1). That is
1m
f(θ; X) =	X Vigi(Wi； x),	(20)
s(m) i=1
where vi’s are randomly drawn from N(0, 1).
In this case, there is no strict upper bounded on the absolute values of weights |vi |. But with high
probability, we can still bound them using the following lemma.
Lemma 2. Let v1, ..., vm be i.i.d. Gaussian variables. Then with probability at least 1 -
2e-0.5 log2 m+log m
max |vi | ≤ log m.
i∈[m]
The above lemma can be obtained by letting t = logm in Eq.(2.10) in Vershynin (2018) and using
union bound.
Using the same analysis as in the proof of Theorem 1, we have, with probability at least
1 - 2e-0.5 log2 m+log m,
1	1 m	βlogm m
12(θ - θO) H(ξ)(θ - θO)I ≤ 2s(m) X |vi| ∙ kHgik ∙ kwi - wi,0k ≤ 2s(m) X IlWi - wi,ok
Under Assumption 2, β is a constant. Note that as the number of m goes to infinity, the probability
1 一 2e-0.5log2 m+logm converges to 1. Since 1∕s(m) is O(1∕√m), hence, We get
∣2(θ -θo)TH(ξ)(θ -θo)∣= O (l√m).	(21)
Whenm→ ∞, the quadratic term of the Taylor expansion converges to 0, With probability 1.
B Experimental verification of the orthogonality in Theorem 3
In this section, We run experiments to verify the theoretical finding in Theorem 3, that the normal
vectors ni Within the same hidden layer tends to become more and more orthogonal, as the layer
Width increases.
Specifically, We run tWo experiments. In the first one, We use full-batch gradient descent to train
4-layer fully-connected neural netWorks on a simple dataset D: 20 images randomly selected from
CIFAR-10 of the classes “airplane” or “bird”. In the netWork, all the hidden layers have the same
Width m. Given a training sample and any two neurons <⅛ and 访 in the same layer, we compute the
cosine cos θj between the gradients Yai and Naj as
cos θ. .= "αi, Vaji
cos θij = ∣vai∣∙∣vai∣.
(22)
As a metric to measure the orthogonality of the gradients, the average absolute cosine | cos θ∣ is
calculated by averaging | cos θij | over all pairs of (i, j) with i 6= j and over all data samples. Left
panel of Figure 3 shows the numerical results of the average absolute cosine | cos θ∣ as a function
of network width m. We see that, as m increases, | cos θ∣ monotonically decreases towards zero,
verifying that the neuron gradients becomes more and more orthogonal.
In the second experiment, we consider a bottleneck network, which has three hidden layers, with
the 1st and 3rd layers with large width m = 1000 and 2nd layer with width mb as the bottleneck
layer. We train this bottleneck network using full-batch gradient descent on the same dataset D as in
the first experiment. Given a training sample and any two neurons a and dj in the bottleneck layer
(i.e., 2nd hidden layer), we compute the cosine of the angle between their gradients using Eq. (22),
12
Published as a conference paper at ICLR 2022
Figure 3: Absolute cosine of the angle between two neurons, averaged over all neuron pairs and data
samples. Left: 4-layer fully-connected neural network, all hidden layers of which have equal width
m; Right: Bottleneck network with three hidden layers, m1 = m3 = 1000 and m2 = mb. In both
cases, the average absolute cosine | CoS θ∣ monotonically decreases towards zero, consistent With the
theoretical prediction of orthogonality between neuron gradient in Theorem 3.
and then take the average of its absolute value over all pairs of different neurons and over all data
samples. We use this averaged absolute cosine | CoS θ∣ as the metric to measure the orthogonality
of the gradients. Right panel of Figure 3 shows the relation between | CoS θ∣ and the bottleneck
width mb. It is clear that | CoS θ∣ monotonically decreases towards zero as mb increase, verifying the
orthogonality of gradients of bottleneck neurons.
C Proof of Lemma 1
As in Eq. (11), the partial gradient Vθ(i-i)α(I) can be written as:	(Vθ(i-i)α(l))T =
— (w(l))TVα(l-1). Hence, we have
√mi-i v i,07	'
(Vθ(i-i) ɑil) )T Vθ(i-i) j =亡(w(l0 )T Va(IT) (V."I))T(Wjl0).
Note that wi(,l0) and wj(,l0) are independent to each other when i 6= j, and Vα(l-1) is a fixed matrix
independent of Wi(,l0) and Wj(,l0) . Denote aj:= Vα(l-1)(Vα(l-1))TWj(,l0) ∈ Rml-1 . Then,
(Vθ(i-i) al )T Vθ(i-i) aj = ɪ(w(lθ )Taj,	(23)
ml-1	,
the inner product of two independent vectors Wi(,l0) and aj with a scaling factor1/ml-1.
When conditioned on the vector aj, the quantity in Eq.(23) is a Gaussian variable:
1
ml-1
(Wi(,l0))T aj
because Wi(l) is initialized following N (0, I). In the limit of ml-1 → ∞, if the variance
aT a
2
ml2-1
converges to 0, then this Gaussian variable should also converges to zero with probability 1, from
aT aj
which we can conclude the lemma. In the following, we will show that the variance jɪ converges
ml-1
to 0 with probability 1 in this limit.
First, we need the following lemma to upper bound the spectral norm of Va(l). The proof is deferred
to Section C.1.
13
Published as a conference paper at ICLR 2022
Lemma 3. Consider the neural networks defined in Eq.(8) with random Gaussian initialized pa-
rameters. There exist a constant C > 0, such that, with probability at least 1 - 4(l + 1)e-m/2, we
have
kVα(I)(Va(I))Tk ≤ C,
with m = min{m1, m2, . . . , ml-1}.
Since aj = Vα(l-1)(Vα(l-1))T wj(,l0) , using Lemma 3, we can bound kajT aj k by
kajTajk ≤ kVα(l-1)(Vα(l-1))T k2kwj(,l0) k2 ≤C2kwj(,l0)k2.
Recall that w;l0 ∈ Rml-I follows the Gaussian distribution N(0, I). Then IlWjlO∣∣2 〜χ2(mι-ι).
By Lemma 1 in Laurent & Massart (2000), we have with probability at least 1 - e-ml-1,
∣wj(l,0) ∣2 ≤ 5ml-1.
By union bound, we have with probability 1 - 2(L + 1)e-m/2 - e-ml-1, the variance
aT aj	5C 2
—≤——
ml2-1 ml-1
In the infinite network width limit m1 , . . . , ml-1 → ∞, we see that the variance converges to 0 and
the probability converges to 1.
C.1 Proof of Lemma 3
Proof. First, note that Vα(l-1)(Vα(l-1))T can be decomposed as
l-1
Vα(l-1)(Vα(l-1))T = X VW(l0)α(l-1)(VW(l0)α(l-1))T.
l0=1
Then, its spectral norm can be bounded by
l-1
∣Vα(l-1)(Vα(l-1))T ∣ = ∣ X VW(l0)α(l-1)(VW(l0)α(l-1))T∣
l0=1
l-1
≤ X ∣VW(l0)α(l-1)(VW(l0)α(l-1))T ∣
l0=1
l-1
≤X
l0=1
∂a(Il) Y	∂a(IlO) ∂ ∂a(Il) Y ∂a(IlO)
E Ji+1 daF-Iy IdWM ”1 daF-Iy
l-1
≤X
ll=1
∂a(Il)
∂W(Ii
2	l-1
Y
lll =ll+1
∂a(IlO)
∂α(lllT)
(24)
In the following, We need to bound the terms ∣∣∂a(ll)∕∂W(Il) ∣∣ and k∂a(lll)∕∂a(IOlT) ∣∣.
To simplify the presentation of the proof, we assume in the following that each network hidden layer
has the same width m. We leave the general analysis for the readers.
We use the following lemma to bound the spectral norm of the weight matrices at initialization.
Lemma 4. If each component of W0(l), l ∈ [L], is i.i.d. drawn from N(0, 1), then with probability
at least 1 - 2e-m/2,
∣w0ηk ≤ 3√m.
14
Published as a conference paper at ICLR 2022
See the proof in Section C.2.
There is also a lemma that bounds the Euclidean norm of each hidden layer neuron vector α(l) .
Lemma 5 (Modified Lemma F.3 of (Liu et al., 2020)). There exists constants Cα, Bα > 0 such that,
with probability at least 1 - 2(L + 1)e-m/2, for all l ∈ [L],
kα(l)k ≤ Ca√m + Bα.	(25)
Using the above two lemmas, we can bounded those two terms. At initialization, we have, with
probability at least 1 - 2e-m/2,
2
∂α(I)
∂α(IT)
sup
kvk=1
2
=SUp -1 k∑0(I)W((I)Vk2
kvk=1 m	0
≤ -1 k∑0(l)k2kw0l)k2
m
≤ 9L2σ ,
and with probability at least 1- 2(L + 1)e-m/2, for all l ∈ [L],
≤	-1 k∑0(l)k2kα(l-1)k2
m
≤	LσCa+ɪ LσBa.
σα mσ α
(26)
Here Σ0(I) is a diagonal matrix, with the diagonal entry Σ0(? = σ0(α(l)) and Lσ is the degree of
LiPschitz continuity of the activation function σ(∙).
Now, using the above results, we can upper bound the spectral norm of the matrix
▽a(IT)(▽ɑ(IT))T. With probability 1 一 4(L + 1)e-m/2, we have
l-1
kVα(l-1)(Vα(l-1))tk≤ X
l0=1
∂a(l0)
∂W (ID
2 l-1
Y
l00=l0+1
∂a(l00)
∂a(l00-1)
2
≤ a- i)9ι-1Lσ (Ca+ɪ Ba).	q7)
m
Since l ≤ L, we can see that, for sufficient large network width m, the spectral norm
kVa(IT)(Va(IT))T ∣∣ is bounded by a constant.	□
C.2 Proof of Lemma 4
Proof. Consider an arbitrary random matrix A ∈ Rma×mb With each entry Aij 〜 N (0, 1). By
Corollary 5.35 of Vershynin (2010), for any t > 0, we have with probability at least 1 一 2exp(-t2),
∣∣A∣ ≤ √ma + √mb + t.	(28)
For the initial neural network weight matrices, we have
l∣w0i1)l∣2 ≤ √d+√m+1,
∣W((l)∣2 ≤ 2√m +1,	l ∈ {2, 3,…,L},
kW((L+1)k2 ≤√m + 1 +1.
Letting t = √m and noting that m > d,we finish the proof.
□
15
Published as a conference paper at ICLR 2022
D INCLUDING w(l+1) AS PARAMETERS
Now, we consider the more general case where parameters in layer l + 1 are not fixed. Then, the
quadratic term of the pre-activation α(l+1) is written as:
2(θ - θO)THα(l+1) (ξ)(θ - θO)
i w w(l+1)—w0l+1) !T
2 ∖ θ(I)- θ0l)	)
∂2α0+1) (ξ)
(∂w(l + 1) )2 (ξ)
∂2α(l+1) (ξ)
∂w(l + 1) ∂θ(l)《J
∂2a(l+1)	(ξ)
∂θ(I)∂w(I+ 1)《J
d2a(l+1) (ξ)
(∂θ(l))2 (ξ)
w(l+1) - w(Ol+1)
θ(l) - θO(l)
2(w(i+i)- wθl+ι))τ (∂Wα+⅜ (ξ)(w(l+ι)
- wO(l+1))	(=: A)
+2(θ(l)-θOl))T ∂w (ξ)(θ(I)-θol))
+(w(l+I)-Woι+I))T ∂5S∂o K*(I)
where ξ is some point between θO and θ.
(=: B)
- θ0(l))	(=: C)
Note that α(l+1) is linear in w(l+1), hence, (∂W¾(++))2
is always a zero matrix. Hence, term A ≡ 0.
Moreover, by Theorem 4, the term B becomes zero, as the width ml increases to infinity. For the
term C , we note that
∂2α(l+1)	1	∕∩
-------------=-------Vα(l).
∂w(l+D∂θ(I)	√mι
Here, α(l) = (α(1l), . . . , α(ml)l ) is the vector of the neurons in layer l and Vα(l) is the matrix of the
first-order derivative of the vector a(l). Using Lemma 3, We have an upper bound on the spectral
norm: k Va(I) ∣∣ ≤ √C, with C > 0 being a constant. Therefore, we have
lcl≤√⅛ kw(l+I)-WOl+1)kkva(l)kkθ(I)- θ0l)k≤ Rf.
(29)
In the limit ofml → ∞, term C converges to 0. Therefore, we have proved that each (l + 1)-th
layer pre-activation α(l+1) has a vanishing quadratic term in its Taylor expansion Eq.(1), and hence
becomes linear, in the limit ofml → ∞.
E DenseNet
In a DenseNet (Huang et al., 2017), a l-th layer neuron takes all previous layer neurons as inputs:
Q(I) = σ(a(l)),	a(I) = √= W(I) * Concat[a(I-I),...,a(I), x],	(30)
where the “Concat” function concatenates all the vector arguments into a single long vector with
m being the size, and * is the matrix multiplication. The pre-activation can be viewed as the sum
of l linear functions, a(I) = Pl0=1 4U ), where each ag ) := √mU(Il)a(Il) has a similar form as
the pre-activation in full-connected neural network Eq.(8), but with parameters U(l0 ) being a subset
of W(l). Hence, using the analysis in Section 3, in the infinite network width limit and in O(1)-
neighborhoods of the initialization, each of the pre-activations of the DenseNet is a summation
of l ≤ L linear functions, and hence is linear. Therefore, the DenseNet is also linear in O(1)-
neighborhoods of the initialization.
16