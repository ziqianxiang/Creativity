Published as a conference paper at ICLR 2022
Graph-Assisted Predictive State Representa-
tions for Multi-Agent Partially Observable
Systems
Zhi Zhang1 Zhuoran Yang2 Han Liu3 Pratap Tokekar4 Furong Huang5
1,4,5University of Maryland, College Park 2Yale University 3Northwestern University
1 zhizhang5763@gmail.com, {4furongh,5tokekar}@umd.edu
2 zhuoran.yang@yale.edu, 3hanliu@northwestern.edu
Ab stract
We study reinforcement learning for partially observable multi-agent systems where
each agent only has access to its own observation and reward and aims to maximize
its cumulative rewards. To handle partial observations, we propose graph-assisted
predictive state representations (GAPSR), a scalable multi-agent representation
learning framework that leverages the agent connectivity graphs to aggregate local
representations computed by each agent. In addition, our representations are readily
able to incorporate dynamic interaction graphs and kernel space embeddings of
the predictive states, and thus have strong flexibility and representation power.
Based on GAPSR, we propose an end-to-end MARL algorithm that simultaneously
infers the predictive representations and uses the representations as the input of
a policy optimization algorithm. Empirically, we demonstrate the efficacy of the
proposed algorithm provided on both a MAMuJoCo robotic learning experiment
and a multi-agent particle learning environment.
1	Introduction
In partially observable decision-making systems, it is pivotal for the agents to infer the latent state
of the system from past observations. The predictive state representation (PSR) (Littman et al.,
2001) is a representation of the history by a vector of predictions of set of tests conditioning on the
history. In particular, a history is a sequence of part observations and actions. A test is sequences
of future actions and future observations, which is true if and only if all the observations occur,
given that all the actions taken. Each entry of the predictive state of representation of is given by
the conditional probability that the corresponding test holds true given that history. A fundamental
assumption of PSR model is that these representations of the history are a sufficient statistics for
predicting future observations and thus are able to represent the underlying state which is unobserved
(Littman et al., 2001; Hefny et al., 2015; Sun et al., 2016). A key feature of PSR is that it only
involves observable actions and observations and also encodes the latent state. Thus, the system
dynamics can be recovered from data without resorting to the estimation of the unobserved latent
states. Furthermore, using kernel embeddings of conditional distributions, PSR can be embedded
into reproducing kernel Hilbert spaces (RKHS) (Boots et al., 2013), which further enhances the
representation powerful of PSR. Furthermore, Littman et al. (2001); James & Singh (2004); Singh
et al. (2012) shows that PSR offers an alternatively more compact state representation than POMDP
models. Therefore, PSR is considered as a powerful representation learning tool and has been used as
an indispensable component of various model-based single-agent reinforcement learning methods
(James & Singh, 2004; Boots et al., 2011; 2013; Hamilton et al., 2014; Sun et al., 2016; Hefny et al.,
2015; 2018a).
Furthermore, when it comes to multi-agent systems with partial observations, is desirable to construct
predictive states for multi-agent reinforcement learning (MARL). However, directly extending single-
agent PSR would require building tests based on joint observations and joint actions. As a result,
learning the PSR involves tensor decomposition which is computationally prohibitive (Chen et al.,
2020). Moreover, another drawback of such an approach is that oftentimes the agents only have local
observations. Furthermore, in multi-agent systems, the observations of each agent are influenced by
the actions of other agents. As a result, the representations should also take the other agents into
account. Specifically, we aim to address the following question:
1
Published as a conference paper at ICLR 2022
Can we construct multi-agent predictive representations based on local information for reinforcement
learning?
In this work, we provide a positive answer to this question. Specifically, we propose a new framework
of multi-agent PSR named as Graph-Assisted Predictive State Representations (GAPSR), which
utilizes local information possessed by each agent to construct scalable representations. Specifically,
we assume each agent has local observations but is able to observe joint actions. Let n be the number
of agents. For any agent i, we first construct n primitive interaction predictions {qi,j }j∈[n] via
single-agent PSR, where qi,j is constructed using agent i’s local observations and agent j’s local
actions. Then agent i’s final predictive representations qi is obtained by aggregating {qi,j }j∈[n]
according to the underlying agent connectivity graph which characterize the proximity among agents.
Finally, these graph assisted representations are used as the proxy of state variable and fed into a
standard MARL algorithm, e.g., multi-agent actor-critic for decision making.
Our GAPSR framework enjoys a few desired properties. First, GAPSR aggregates the information
about other agents via the agent connectivity graph, which encodes the interaction among agents.
Such an aggregation mechanism enables us to implement GAPSR on a single-agent level while
maintaining other agents’ interactions through the encoding of the interactive graph topology. Second,
the GAPSR of each agent involve the actions of all the other agents connected to that agent. In
other words, GAPSR correctly captures the non-stationarity caused by the other agents’ actions.
Besides, the agent connectivity graphs can take various forms. In particular, we build our model
under three common graphs, namely static complete graphs, static non-complete graphs, and dynamic
graphs, covering as many real-world scenarios as possible. Third, utilizing kernel embedding of
conditional distributions, the predictive representations lie in RKHS with strong representation power.
Finally, GAPSR are readily to be incorporated into any state-of-the-art MARL algorithm as a proxy
of the state variable in an end-to-end fashion. Thus, we believe that GAPSR could be a promising
representation learning framework for multi-agent partially observable systems.
Finally, we implement an instantiation of the proposed algorithm that combines GAPSR with multi-
agent actor-critic, where we simultaneously learn GAPSR and the agents’ policies in an end-to-end
manner with fully differentiable neural network structures. We test our algorithm through systematic
numerical experiments on MAMuJoCo robotic learning experiments (de Witt et al., 2020) and multi-
agent particle learning environments (Ryu et al., 2020), and compare our proposed method against
two baselines as detailed in Section 6. The results demonstrate the efficiency of our method.
2	Related Works
Partially Observable Environment. Real-world agents often experience situations that the observed
signals are aliased and do not fully determine their state in the system. This is particularly true for
multiple agents environments where agents have partial observability due to limited communication
(Oliehoek & Amato, 2016). In accommodation to the partially observable environment, POMDP has
been adopted by Kaelbling et al. (1998), and the algorithms (Kaelbling et al., 1998; Cassandra, 1998;
Thrun, 1999; Pineau et al., 2003; Poupart & Vlassis, 2008; Platt Jr et al., 2010) for determining an
optimal policy have shifted to using the probability distributions (belief state) over the state space
instead of exact state space. In general, they have high complexity or suffer from local optima.
Moreover, the most common POMDP policy learning assumes the agent has access to a priori
knowledge of the system. The access to such prior knowledge has a premise that the agent has
considerable domain knowledge (Kaelbling et al., 1998). However, it is expected that the real-world
agents learn the system model and thus a planning policy further without knowledge of the domain.
Overview of PSR. Littman et al. (2001); James & Singh (2004) introduced the PSR over an expressive
and robust framework for modeling dynamical systems and defined PSR as a representation of state
by using a vector of predictions of fully observable quantities (tests) conditioned on past events
(histories). A predictive model is constructed directly from execution traces in the PSR framework,
utilizing minimal prior information about the domain. The PSR paradigm subsumes POMDP as a
special case (Littman et al., 2001). PSR is considered much more compact than POMDP (Aberdeen
et al., 2007). The spectral learning method has been proved to show success for learning the PSR
(Boots et al., 2011; Jiang et al., 2018). There are other classes of dynamical system learning algorithms
that are based on likelihood-based optimization or sampling approaches (Frigola et al., 2013), but
they are prone to poor local optima. The spectral learning represents the estimated state by sufficient
statistics of future observations and estimates the model parameters by method of moments. However,
2
Published as a conference paper at ICLR 2022
this line of algorithms is hard to incorporate prior information (Hefny et al., 2015). Thus, Hefny et al.
(2015; 2018a) introduce the supervised learning method to learn PSR and proves its convergence.
Although many works study PSR in discrete action space (Hsu et al., 2012; Siddiqi et al., 2010;
Boots et al., 2011), Boots et al. (2013) propose Hilbert space embedding (HSE)-PSR to deal with
continuous actions. Hefny et al. (2018a) use an approximation of HSE-PSR by Random Fourier
transform (RFF) and built a more principled generalization of PSR to deal with high dimensions.
However, all of these studies aim for the single agent scenario.
PSR and RL. The predictive states estimated by the PSR are considered as states in a fully observable
Markov Decision Process so that the value function is learned on these states. This line of work has
been done in the single-agent environment (Boots & Gordon, 2010; Boots et al., 2011; Hamilton
et al., 2014; Venkatraman et al., 2017; Hefny et al., 2018b). Especially, Hefny et al. (2018b) propose
the recurrent predictive state in the RNN network. Moreover, the learning PSR and policy functions
are connected with the end-to-end training.
MAPSR and MARL. A close related work Chen et al. (2020), which proposes a multi-agent PSR
model that is formulated as a n-way tensor, where n is the number of agents. As a result, learning the
parameters is achieved by decomposing the n-way tensor, which becomes computationally prohibitive
when the number of agents is large. Moreover, it seems challenging to incorporate benign structures
such as sparsity into the model, as doing so leads to a more complicated set of moment equations.
Furthermore, our work belongs to the vast literature on MARL for (partially observable) Markov
Games. See, e.g., Lyu & Amato (2020); Son et al. (2019); Zhang et al. (2019); Rashid et al. (2018);
Foerster et al. (2018); Lowe et al. (2017); Baker et al. (2019); Yu et al. (2021) and the references
therein. Also see Zhang et al. (2021); Gronauer & Diepold (2022); Canese et al. (2021) for recent
surveys on MARL. Recent works propose more sophisticated deep MARL algorithms for multi-agent
problems under the paradigm of centralized training with decentralized execution (Zhou et al., 2020;
Sunehag et al., 2017; Lowe et al., 2017; Foerster et al., 2018; Rashid et al., 2018). Our work seems
not directly comparable to these works as our goal is to learn good representations in the partially
observable multi-agent systems via model-based RL, whereas most of these works adopt model-free
methods or focus on Markov games without partial observations.
3	Background: Single-Agent PSR
In this section, we introduce the background on single-agent PSR. Also see §A.1 for a more detailed
introduction.
Predictive State Representation (PSR). A prediction of a state is defined as the conditional probabil-
ity of seeing a test’s observations in sequence given that actions of the test are taken in sequence from a
history (Littman et al., 2001). Givn a finite observation space O (o ∈ O) and action space A (a ∈ A).
A test of length k at time t, is defined as a sequence of action-observation pairs that starts at time t
and ends at time t + k — 1,	{(oι,	aι)}t+k	1 =	{ot,	at, ot+ι, at+ι, ∙∙∙	,oι,aι,…,ot+k-1,	at+k-ι}.
A history, at time t is a sequence of action-observation pairs that start from the beginning of time
and ends attime t - 1, {(oι, aι)}t-1 = {o1, a1,…，oι, aι,…，0t-1,at-1}.
Hilbert Space Embeddings of Predictive State Representations. In this work, we are interested in
extending PSR to decision makings of controlled systems with continuous actions. So we use the
model introduced by Hefny et al. (2018a). In this model, predictive state qt and extended predictive
state pt satisfies
qtψa = E[ψo∣ψa;ψh],	pξ =园归0糜；ψh],	⑴
(i.e., qt and pt are conditional linear expectation operators which maps to the conditional expectation
of future observations), where
ψo ：= Φo({oι}t+k-1), ψa ：= ΦA({aι}t+k-1),	⑵
are feature maps by kernels kO , kA over future observation and action features. The extended
predictive state compared to predictive state adds one more pair of {(at+k, ot+k)} to the prediction.
The ξta and ξto are the corresponding extended feature maps, which satisfy
ξ = ψo 0 φo, ξ = ψa 0。：.	(3)
Here φto := φo(ot) and φta := φa(at) are the shorthands for one time feature map by ko and ka. We
Use 0 to denote the transposed Khatri-Rao product for two matrices with the same number of rows,
and each row of the resultant matrix is the vectorzied outer product of the corresponding row vectors
3
Published as a conference paper at ICLR 2022
in the two matrices. Also, we use
ψth := ψh({(ol, al)}lt=-11)	(4)
to define a set of features extracted from previous observations and actions (typically from a fixed
length window ending at t - 1).
3.1	Notation of PSR For Multi-Agent setting
In the subsequent section, we will bring the HSE-PSR model to the multi-agent settings. In the
following, we introduce the notation for the multi-agent setup. For any agent i in an n-agent system,
given a set of kernels {kOi}in=1,{kAi}in=1,{koi}in=1,{kai}in=1 for every agent, we define agent i’s
feature maps using the single-agent PSR presented in the previous section. In particular, for all i ∈ [n],
we let ψto,i and ψta,i be defined as in (2) using agent i’s local observations and actions, respectively.
Similarly, we let ξto,i and ξta,i be defined in (3), and let φto,i and φta,i be defined using φoi and φai,
respectively. Furthermore, for all i, j ∈ [n], we let ψth,i,j be defined as in (4) using the observations
of agent i and the actions of agent j. Moreover, we use the qt,i and pt,i to denote its predictive state
and extended predictive state, whose definitions will be presented in the next section.
4	Dynamic Interaction Graph For GAPSR Model
Many works have considered graph representation of the multi-agent network (Liu et al., 2020b; Ryu
et al., 2020). In general, the relationship between agents is characterized by an undirected graph.
We introduce a dynamic interaction graph to represent the GAPSR by considering the interaction
between agents.
Definition 1. Let G = (V, E), including the set V of nodes and set E of the edges. Each node
represents the agent entry, and the edge represents the relationship between the two adjacent agents.
Here we suppose the graph structure is given, that means the number of nodes (n), the number of
edges (m), the edge weights, and the maximum number of degrees (k) are available to us. We think
that this kind of presupposition is very reasonable because in the real world, for example, there
are multiple robots; we can quickly get the geographic position of the robots through sensors, then
calculate the structure of the graph formed by them.
4.1	Static Complete Graph
We starts with a static complete graph Gc, where the relationship between nodes are invariant to time
change. A complete graph has m = n(n - 1)/2 edges, where m = |E|. For each agent, we can
represent its PSR by considering other agents’ interactions by
qt,i := g ({qt,i,j }n=1) =	qt,i,j,	⑸
j
same for pt,i . To consider the interactive behavior between agents, we introduce two additional
notations qt,i,j and pt,i,j. Let {(i, j)}in,j=1 represents a pair of agents on a n multi-agent system.
The qt,i,j is a primitive predictive state of i’s observation (ψto,i) by intervening agent j’s action (ψta,j)
and observing agent i’s observation history and agent j’s action history (ψth,i,j ), and pt,i,j is the
extended counterpart, where they satisfy the relationship for single agent case in (1)
qt,i,jψa,j = EWZiMaj; ψhi,j],	pt,i,jξaj = E[ξ0,ilξa,j; ψhi,j]∙	⑹
If i = j , then it becomes an exact single agent scenario. Similarly, we use the same approach to
represent pt,i as shown in (5). Each agent’s PSR qt,i and extended PSR pt,i are modeled by fully
considering all other available agents.
Based on equation (5), in practice, estimating qt,i and pt,i denoted as qbt,i and pbt,i requires us to get
qbt,i,j and pbt,i,j at first.
4.1.1	ESTIMATION OF qt,i,j AND pt,i,j
To estimate the pt and qt for just one agent, Hefny et al. (2018a) use the supervised learning method.
They show that bt = Mψo∣ψa㈤h = Cψoψa∣ψh (Cψaψa∣φh + λI)-1, where Ma\b；C is a linear operator
that satisfies E[A∣B = b; C = c] = MAiB；cb, and CXY := E[φ(X) 0 φ(Y)] is the uncentered
covariance operator, and CXY∣z is covariance of X and Y given Z = z. They estimate Cψoψa∣ψh,
Cψaψa∣φh by sampling data, then get the estimation qt. The same procedure is used to estimate Pt
by replacing the features ψto and ψta with their extend counterparts ξto and ξta .
4
Published as a conference paper at ICLR 2022
Similar to the single-agent case above, the representation of qt,i,j can be achieved as
bt,i,j = Mψo,i∣ψa,j∙2t,i,j = cΨθ,iΨa,j∣Ψh,i,j (Cψa,j吵力1。3“ + λI)	.
(7)
To estimate	Cψo	ψa	∣ψh	and	Cψa	ψa	∣ψh	, We learn two linear maps	Tij	and	Uij	such that
ψt,i ψt,j | ψt,i,j	ψt,j ψt,j | ψt,i,j	,j	,
cψo,iΨa,j∣ψh,i,j	≈ Tij (ψhi,j)	and CΨa,jψa,j∣ψh,i,j	≈ Ui,j (ψhi,j).	Thetraining examples for Tij	and
Ui,j consist of pairs (ψ hi,j, Ψ0,i X Ψa,j) and (Ψhi,j, Ψaj X Ψa,j)∙ The learning ofPt,i,j Can be done
in a similar way. More details can be found in §A.2.1
After calculation of qbt,i,j and pbt,i,j , under the static complete graph setting where the interaction is
considered, defined in equation (5), we can get the estimate of qbt,i and pbt,i .
4.1.2	THEORETICAL GUARANTEE OF ESTIMATION OF qi AND pi UNDER THE STATIC
Complete Graph
Theoretically, we show that the difference between qi and its estimator qbi is bounded with high
probability.
Theorem 1. Let πΘ be a data collection policy and H is the range of πΘ on joint histories. If
Equation (5) and (7) used, then for all h ∈ H and any ∈ (0, 1), such that N >
t2Aj log(2dAj /)
where N is the number of time points we collect sample, then kqbi - qik is bounded as below with
3,	kqbi - qi k ≤ n∆,
(8)
probability at least 1 -
where
iAk2+2u(%adJiAk+，+ ||%0吵？|吵“141+921|%?您j+192力91”
v(CΨa∣ψhJ (I-Y)+λ
"Cψa∣ψhJ (I-Y)2+λ
Here ∆ι, ∆2 are two other relevant bounds, we provide them in §E.1. u(∙), v(∙) denote the largest,
t2A log(2dAj /)
SmaUeSt eigenvalue of a matrix. And Y = -j7---------Yj—— < 1 is a constant that depends on the
v Cψja N
magnitude of the norm ofψja (we assume ψja ≤ tAj), the dimension ofψja (dAj), the (uncentered)
covariances (Cψa := E ψjaψjaT ) and the sample size (N).
Theorem 1 says we need at least N samples for the bound in equation (8) to be valid. We give the
proofs in §E. It is not hard to obtain a bound for pi using the same approach. We omit that.
4.2 Extension to Static Non-Complete Graph
In large-scale multi-agent systems, the number of agents is large, and not all agents need to interact
with each other. A static non-completed graph can perfectly represent such a situation. For example,
in a given static non-complete graph Gs, we know its maximum number of degree k, and we use the
binary n × n matrix with each entry as Ii,j to indicate the interaction between two agents. Then the
GAPSR for each agent will be
qt,i := g ({qt,i,j}n=l) = ^X Ii,jqt,i,j.	(9)
Lemma 1. Under the same environment depicted in Theorem 1 and given a Gs with k maxi-
mum number of the degree to represent agents, then the bound in Theorem 1 can be rewritten as
kqbi - qi k ≤ k∆.
The conclusion of Lemma 1 is evident since we replace the n with the k neighbors, the total error
bound is also decreased approximately as k.
4.3	Dynamic Graph
Real-world multi-agents can also formulate a time-dependent dynamic graph Gd, rather than a static
graph. A dynamic graph has its structure dynamically changing with time. In other words, the
edges can be inserted or deleted across time. The dynamic graph brings more challenges to the
representation as the interaction relationship among agents changes constantly.
5
Published as a conference paper at ICLR 2022
Braha & Bar-Yam (2009), Ma et al. (2017) and Zhao et al. (2010) consider a dynamic graph as
a set of ordered static graphs. For each time point, we are given a static graph such that Gd =
{Gdi, Gd2,…，Gdt}, and a time-dependent given binary matrix with It,i,j indicating the interaction
between two agents at each time. Then we have the agent-wise PSR as
qt,i := g {qt,i,j }jn=1 =	It,i,j qt,i,j .	(10)
Compared to (9), the coefficient It,i,j is time-dependent, which brings a challenge to our theoretical
bound. We consider a dynamic graph experiences a trajectory path, assuming every node has a
chance at least p to interact by connection with another node at any time point. For example, for a
node i, if we take the union set of the nodes interacted with i over the path, then the union set could
form a static complete graph; in other words, if i connects j , then we can obtain a valid sample to
estimate qi,j as the complete static graph does, if not, then we skip to the next time point. For the
complete static graph, we need the trajectory to run at least N time points to collect enough data
to estimate our conditional operator qi,j accurately. Furthermore, The total number of time points
needed by i until the Nth interaction with j follows a negative binomial distribution N B(N, p). On
average, we need N time points before we see i,j completely connecting N times. Now we consider
node i could interact with every node in a set of nodes (J : |J | = n - 1) for N number of time
points. We assume the chances being interacted between two nodes (i,j ∈ J) does not affect their
interaction with other nodes. Thus we have a set of independent negative binomial random variables
{Ji }n=-L1 〜NB(N,p) to characterize the interaction of i with j ∈ J. So We are interested in the
expectation of the maximum of J1, . . . , Jn-1, a statistics that tells us the expected maximum number
of time points of collection of measurements needed for the node i to be able to connect with every
node j ∈ J for at least N number of time points. We denote it as J{1,...,n-1} and we have
E{J{1,…,n-i}} = E{max(Jι,…，Jn-ι)} = X (qN + NpqNT +------------+ (NN- j PNTq)	. (11)
Lemma 2. Under the same environment depicted in 1 and given a dynamic graph Gd with every
node has a chance at least p to interact with another node in a one-time point, let N be the number
of time points we collect data of measurements in order to get the bound in Theorem 1, if Equation 10
and 7 used, then we need at least
N0
N — Γ) + K (q, n, N) — ,Y^ + F K (q, n, N)] +。⑴,	(12)
2	log1/q (1/q)
total number of time points, where q = 1 - p, K (q, n, N) := log1/q (n - 1) + (N -
1) log1/q log1/q (n - 1) + (N - 1) log1/q p - log1/q (N - 1)!, F is a periodic C ∞ -function of
period 1 and mean value 0 whose Fourier-Coefficients are given by F(k)
⅛ r(-
2kπi
log( 1)
for k ∈ Z \ {0}. Then kqbi - qi k achieves the same bound as in Theorem 1 with probability at
least 1 - 3. In other words, kqbi - qi k ≤ n∆. Moreover, (12) is an asymptotic expansion of the
right-hand of (11). We give the proof in §E.3.
—
)
Lemma 2 gives the worst-case bound for our estimation under the dynamic graph. The result tells us
that if we need 1 more sample of measurement for our algorithm to converge on the complete static
graph, we need roughly log1/q[log1/q(n - 1)] more samples on the dynamic graph. So as long as we
allow enough learning time, the algorithm can converge with high probability.
Complexity with Increasing Agents. For the complete static graph, we need to evaluate qbi,j and
pbi,j every time point, which requires O(n2) operations and space. Overall, with an increased number
of agents, our GAPSR has a polynomial O(n2) scaled complexity, which is feasible for learning in
a large number of agents environment and is more efficient compared to the centralized MAPSR
(Chen et al., 2020) theoretically, which is combinatorially sample complex, the analysis is shown in
§A.4. For a non-complete static graph with k maximum number of degrees for k n, which is more
common in the real world, because a very far-away robot will not likely affect the targeted robot, the
operation will be significantly decreased to O(k2 ).
4.4	The Estimation for GAPSR Model Components
As stated by Littman et al. (2001), a complete PSR model can build a recursive rule to update
itself. In other words, given the qt,i,j , the GAPSR model can calculate qt+1,i,j by incorporating
a new observation, a so called filtering process in the dynamical system. We achieve this with
6
Published as a conference paper at ICLR 2022
three models proposed by Hefny et al. (2018a): For any agent i, pt,i,j = Wi,j (qt,i,j), qt+1,i,j =
Fi,j (pt,i,j, ot,i, at,j), and ot,i = Zi,j (qt,i,j, at,j). The last model is used to predict the observation
ot,i with action at,j and PSR qt,i,j. Typically, Wi,j , Zi,j are learnable linear maps, and Fi,j is
non-linear and differentiable but known in advance. The Wi,j, Zi,j are learned from regression after
we estimate qt,i,j and pt,i,j, see §A.2.2 for details of estimation of Wi,j, Zi,j.
5	Decision-Making Framework With GAPSR Model
Previous work on single-agent proposed an end-to-end training algorithm (Hefny et al., 2018a) for
PSR model and policy learning. Here we design an algorithm for multi-agent settings and incorporate
our GAPSR model containing the interactive graph component.
We propose an online learning algorithm to learn the GAPSR and agent policies simultaneously. Our
algorithm is shown in §B.2. We use a diagram in §B.3 to illustrate this process.
Our algorithm has three components. Firstly, we estimate the GAPSR the model parameters Wi,j
and Zi,j (line 4-11). The agents use an existed T length trajectory generated based on the iteration
k - 1’s policy (a random policy is used if it is the first time), and learn the GAPSR parameters under
the given interactive graph by regressions with details introduced in 4.4.
Secondly, we use the just learned GAPSR to generate the predictive state representations q and p
(line 13-28). In particular, by starting from the initialized GAPSR and executing the iteration k - 1’s
policy, the agents experience a T length trajectory. The agents perform a series of extension, filtering,
and prediction steps to generate the qt and pt . Every agent also obtains action by executing its policy
function that maps predictive states qt,i to action at,i 〜∏k-1(qt,i). Agents then save trajectories
(actions, observations, predictive states, and rewards) over the path.
Lastly, learning and planning via multi-agent actor-critic (line 30-34): agents use learned rep-
resentations as the state and feed them into a MARL algorithm, e.g., MADDPG (Lowe et al.,
2017). The MARL algorithm conducts the policy learning. Here we give two examples of
GAPSR based MARL algorithms, which are implemented for our experiments. They follow
the actor-critic framework. We first develop an algorithm under partially observable environ-
ments, where each agent has its own critic and actor independently, it is analogous to the in-
dependent actor-critic (IAC) (Foerster et al., 2018). The gradient of the policy is written as
Vθi J (θi)= Eτi~p(τi∣θi) [PT=1 Vθi logπθi(at,i|qt,i)(ri + YVw■乂qt+ι,i) - VWii(qt,i))], and the
independent critic is updated by minimizing the loss Lc = Eτ.^p^τ,∖θi)[(Vw∏i(qi) - yi)2]. Here
the predictive states qi generated by the GAPSR are considered as states to fit the value func-
tion. πθ : Q × A → [0, 1] is the stochastic policy that maps to the probability density of ac-
tions with parameter θ. We call it GAPSR-1. As we know, IAC would not work well since the
environment is not stationary under a multi-agent setting, the MADDPG by Lowe et al. (2017)
solves a non-stationary environment by considering other agents actions; however, every agent
needs a separate critic that has the global information. We used a centralized critic to minimize
the loss Lc = Eq,r,o [(Qw(qι,…，qn, aι,…，&n) - y)2]. So our policy gradient is written
as Vθi J (θi) = Eq,r,o[Vθi ∏θi (qi)V°i Qw(qι,…，qn,aι,…，an) ^=^3)]. To fully consider
other agents’ information, it uses the joint predictive states as input. Here by abuse of notation, we
use πθ : Q → A to indicate the deterministic policy with parameter θ. We call it GAPSR-2. We put
our integration details in §B.4.
Our algorithm has the following characteristics. First, due to its decoupled structure, it is a general
algorithm in the sense that planning can utilize any MARL algorithm for multi-agent MDP.
Second, our algorithm is an end-to-end framework, in the implementation, we build an additive loss
function and fully differentiate it with respect to model parameters. In particular, we update the
MARL and GAPSR model parameter Θ = {ΘGAPSR, ΘMARL} (line 32) by minimizing the following
additive objective function:
L(Θ) = α1L1(ΘMARL) + α2L2 (ΘGAPSR, ΘMARL) ,	(13)
L1(ΘMARL) = -J(θ) + Lc(w),	(14)
n
L2(θGAPSR, θMARL) = X : Eτ~p(τ|Θ) [∑ IIZi,j (Fi,j(Wi,j(qt-ι,i,j))㊈φa,j) - Φ0,i∣l2]. (15)
i=1	j
7
Published as a conference paper at ICLR 2022
Here the L1(ΘMARL) is the objective function for MARL, for example, if the actor-critic used, it will
be the negative action value function with policy parameter θ plus the critic loss with parameter w, and
ΘMARL = (θ, w). L2 (ΘGAPSR, ΘMARL) is the MSE between prediction and actual observation. And
p(τ ∣Θ) is the distribution over trajectories induced by the policy and GAPSR. Θmapsr = {Wi,j, Zij }
denotes GAPSR’s parameters. The α1 and α2 are hyper-parameters to penalize differently on two
losses.
Third, PSR, as a function of action, can bring agents’ action information into the policy gradient; our
interactive GAPSR even brings the effect of other agents’ actions to the policy gradient as well.
6 Experiments
Environments. We evaluate the performance
of our GAPSR on a collection of MARL tasks
under some OpenAI Gym MAMuJoco environ-
ments (de Witt et al., 2020), such as multi-agent
swimmer, hopper, and ant. Each robotic agent
is represented as a body graph, where vertices
(joints) are connected by adjacent edges (body
segments) as shown in Appendix Figure 5. Each
agent controls its joints based on the local infor-
mation observed. All tasks are learned under the
partially observable environments by manually
hiding some observations for each agent. The
goals of the multi-agent systems are aligned with
(a) 2-Agents Swimmer (b) 3-Agents Hopper
80
iterations =jsteo^j×le!
StePS(Xle
1250
⅛00°
§750
S?
⅛500
≡
$250
(c) 4-Agents Ant
O O C
2 1
PJeM&¥
(e) 3-Agents Hopper
(d) 2-Agents Swimmer
f∖J'∖
their corresponding single-agent ones. How-
ever, different from the single-agent system, the
agents in the multi-agent system need to collab-
orate to reach their goals. For the interactive
graph, for simplicity, we considered a complete
static graph, where we assume every agent is
connected with all other agents in the graph. We
defer the details of the robotic agents to §C.1 and
the details of the experimental setup to §C.2.
Baselines and Evaluation. We run 50 iter-
ations for each experiment and collect M =
100 trajectories in every iteration with a max-
imum of 1000 steps in every trajectory. After
each iteration, we compute the average return
R = M Pn=I PM:1 PT=I rt,b on a batch Of
M trajectories, where, Tb is the length of the
bth trajectory. We repeat this process using ten
(f) 4-Agents Ant
iterat?ons =^stei
Figure 1: Performance of our method under MA-
MuJoco partially observable environments, the ab-
lation study- IPSR, in which We do not consider
the agent interaction, and the baseline, which is
not using GAPSR. (a)-(c):GAPSR-2 (It uses cen-
tralized critic, and uses gradients of value function
with respect to policy parameter, its baseline is
MADDPG), (d)-(f):GAPSR-1 (Its baseline is IAC).

We run 10 times and the shaded area is the 95%
confidence interval
different random seeds and report the average and a standard deviation. To verify the effectiveness of
interactive graph, we introduce a baseline called Independent PSR learning (IPSR); in this model,
we do not consider the graph, so we formulate n independent single PSR without considering their
interactions, which means there are no qi,j any more but only qi . The architecture of GAPSR-1 and
GAPSR-2 remains the same. To verify the advantage of PSR, we introduce another baseline (MARL)
where we take out the GAPSR entirely, so it matches with the MARL run on a partially observable
environment. For GAPSR-1, its MARL baseline is IAC (Foerster et al., 2018), and for GAPSR-2, is
MADDPG (Lowe et al., 2017).
Results And Discussion Figure 1 illustrates the empirical average return vs. the number of
interactions with the environment measured in time steps. Our GAPSR methods consistently out-
perform IPSR and get the highest rewards under partially observable environments, which justifies
the representation power of the interactive graph to assist agents in learning in the non-stationary
environment with limited observation when the MARL algorithm has a defect (IAC). Moreover, it
can also boost the performance of the existing good MARL algorithm (MADDPG). To further verify
the effect of learning the PSR part, we also plotted the predicted trajectory to verify the GAPSR’s
performance for predicting the observations in Figure 2. We plotted the predicted observations
vs. actual observations in iterations 1, and 40, respectively, for GAPSR-2. We plotted a row ×
8
Published as a conference paper at ICLR 2022
ri .uφlq
(a) 2-Agents Swimmer
(b) 3-agents Hopper
0.5-
0.0-
-0.5-
⅞0∙5∙
Φ 0.0
ɑ⅛5
(c) 4-agents Ant
0.5-
0.0-
-0.5-
(d) 2-Agents Swimmer
Agent O Agentl
o∙
o a o a o a o a
(e) 3-agents Hopper
AgentO Agentl , Agent 2
O 50 IOO O 50 IOO O 50 IOO
则
。寸αLd

(f) 4-agents Ant
AgentO Agentl Agent 2 Agent 3
Figure 2: Predicted Trajectories (colored) vs Actual Observations (black). (a)-(c) First iteration; (d)
- (f) Iteration 40. The X axis represents the part of steps encountered for one trajectory under that
iteration, and the Y axis represents the numerical value of the observation, i.e. (a) has three rows to
represent its three coordinates of its observation. We also provide iteration 10, 20 in §D.1
columns figure, with each row representing the observation feature and each column representing
each agent. As we can see from the figures, the first iteration of the learning does not predict the
actual observation very well; it has some mismatches. However, as learning progresses, the predic-
tions get increasingly more accurate. Note that the actual trajectory is changing according to the
current policy, and the current policy is optimized based on the further accurate learning of GAPSR.
To enrich our algorithm environment, we also test our
algorithm (GAPSR-2) in multi-agent particle environ-
ments, using the benchmark by (Lowe et al., 2017),
please check §D.2 for experiment details. We test
our algorithm for large n cases. In this environment,
the agents can have cooperative goals such that all
agents must maximize a shared return and conflict-
ing competitive goals. We set up the environments
where agents can only perform physical actions but
not communication; however, to achieve the goals,
agents need explicit communication about others’ lo-
cations to achieve the best reward. These partially
observable environments give us the motivation to
test our method. We report the rewards in Figure 3.
We see that GAPSR outperforms IPSR and baseline,
in terms of the convergence speed and final attained
rewards, with a different number of agents. The pre-
dictive states convey the information that can help
communication between agents in limited communi-
cation and observation environments.
(a) Predator Prey (n=3)	(b) Predator Prey (n=15)
(c) Predator Prey (n=100)	(d) Cooperative Push (n=3)
iterations =steps(×le6)"
(e) Cooperative Push(n=15) (f) Cooperative Push (n=30)
Figure 3: Performance of our method (GAPSR-2)
under multi-agent particle partially observable en-
vironments. We use a different number of agents in
predator-prey and cooperative push environments.
We run ten times, and the shaded area is the 95%
confidence interval.
7 Conclusion
We propose a GAPSR model, extending ideas from
single-agent predictive state representations to a
multi-agent scenario, during the process, we intro-
duce the dynamic interactive graph to model agents’ interactions. Furthermore, we provide the
theoretical guarantees of the GAPSR model. Finally, a learning algorithm that supports gradient-
based deep MARL methods is developed. Our method provides a model-based MARL framework
under a partially observable environment. The experiments proved that our model assumption is valid
by observing the highest return while reducing the observations’ prediction error over trajectories.
9
Published as a conference paper at ICLR 2022
References
Douglas Aberdeen, Olivier Buffet, and Owen Thomas. Policy-gradients for psrs and pomdps. In
Artificial Intelligence and Statistics,pp. 3-10. PMLR, 2007.
Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor
Mordatch. Emergent tool use from multi-agent autocurricula. arXiv preprint arXiv:1909.07528,
2019.
Byron Boots and Geoffrey J Gordon. Predictive state temporal difference learning. arXiv preprint
arXiv:1011.0041, 2010.
Byron Boots, Sajid M Siddiqi, and Geoffrey J Gordon. Closing the learning-planning loop with
predictive state representations. The International Journal of Robotics Research, 30(7):954-966,
2011.
Byron Boots, Geoffrey Gordon, and Arthur Gretton. Hilbert space embeddings of predictive state
representations. arXiv preprint arXiv:1309.6819, 2013.
Dan Braha and Yaneer Bar-Yam. Time-dependent complex networks: Dynamic centrality, dynamic
motifs, and cycles of social interactions. In Adaptive Networks, pp. 39-50. Springer, 2009.
Lorenzo Canese, Gian Carlo Cardarilli, Luca Di Nunzio, Rocco Fazzolari, Daniele Giardino, Marco
Re, and Sergio Spano. Multi-agent reinforcement learning: A review of challenges and applications.
Applied Sciences, 11(11):4948, 2021.
Anthony Rocco Cassandra. Exact and approximate algorithms for partially observable Markov
decision processes. Brown University, 1998.
Bilian Chen, Biyang Ma, Yifeng Zeng, Langcai Cao, and Jing Tang. Tensor Decomposition for
Multi-agent Predictive State Representation. arXiv:2005.13706 [cs], May 2020.
Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 32, 2018.
Roger Frigola, Fredrik Lindsten, Thomas B. Schon, and Carl E. Rasmussen. Bayesian Inference and
Learning in Gaussian Process State-Space Models with Particle MCMC. arXiv:1306.2861 [cs,
stat], December 2013.
Peter J Grabner and Helmut Prodinger. Maximum statistics of n random variables distributed by the
negative binomial distribution. Combinatorics, Probability and Computing, 6(2):179-183, 1997.
Sven Gronauer and Klaus Diepold. Multi-agent deep reinforcement learning: a survey. Artificial
Intelligence Review, 55(2):895-943, 2022.
William Hamilton, Mahdi Milani Fard, and Joelle Pineau. Efficient learning and planning with
compressed predictive states. The Journal of Machine Learning Research, 15(1):3395-3439, 2014.
Ahmed Hefny, Carlton Downey, and Geoffrey J Gordon. Supervised learning for dynamical system
learning. Advances in neural information processing systems, 28:1963-1971, 2015.
Ahmed Hefny, Carlton Downey, and Geoffrey Gordon. An efficient, expressive and local minima-free
method for learning controlled dynamical systems. In Thirty-Second AAAI Conference on Artificial
Intelligence, 2018a.
Ahmed Hefny, Zita Marinho, Wen Sun, Siddhartha Srinivasa, and Geoffrey Gordon. Recurrent
predictive state policy networks. In International Conference on Machine Learning, pp. 1949-
1958. PMLR, 2018b.
Daniel Hsu, Sham M Kakade, and Tong Zhang. A spectral algorithm for learning hidden markov
models. Journal of Computer and System Sciences, 78(5):1460-1480, 2012.
Michael R James and Satinder Singh. Learning and discovery of predictive state representations
in dynamical systems with reset. In Proceedings of the twenty-first international conference on
Machine learning, pp. 53, 2004.
10
Published as a conference paper at ICLR 2022
Nan Jiang, Alex Kulesza, and Satinder P Singh. Completing state representations using spectral
learning. In NeurIPS,pp. 4333-4342, 2018.
Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in partially
observable stochastic domains. Artificial intelligence, 101(1-2):99-134, 1998.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Michael L Littman, Richard S Sutton, and Satinder P Singh. Predictive representations of state. In
NIPS, volume 14, pp. 30, 2001.
Iou-Jen Liu, Raymond A Yeh, and Alexander G Schwing. Pic: permutation invariant critic for
multi-agent deep reinforcement learning. In Conference on Robot Learning, pp. 590-602. PMLR,
2020a.
Yong Liu, Weixun Wang, Yujing Hu, Jianye Hao, Xingguo Chen, and Yang Gao. Multi-agent game
abstraction via graph attention neural network. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 34, pp. 7211-7218, 2020b.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-Agent
Actor-Critic for Mixed Cooperative-Competitive Environments. arXiv:1706.02275 [cs], June
2017.
Xueguang Lyu and Christopher Amato. Likelihood Quantile Networks for Coordinating Multi-Agent
Reinforcement Learning. arXiv:1812.06319 [cs, stat], June 2020.
Shuai Ma, Renjun Hu, Luoshu Wang, Xuelian Lin, and Jinpeng Huai. Fast computation of dense
temporal subgraphs. In 2017 IEEE 33rd International Conference on Data Engineering (ICDE),
pp. 361-372. IEEE, 2017.
Frans A Oliehoek and Christopher Amato. A concise introduction to decentralized POMDPs. Springer,
2016.
Judea Pearl et al. Models, reasoning and inference. Cambridge, UK: CambridgeUniversityPress, 19,
2000.
Joelle Pineau, Geoff Gordon, Sebastian Thrun, et al. Point-based value iteration: An anytime
algorithm for pomdps. In IJCAI, volume 3, pp. 1025-1032. Citeseer, 2003.
Robert Platt Jr, Russ Tedrake, Leslie Kaelbling, and Tomas Lozano-Perez. Belief space planning
assuming maximum likelihood observations. 2010.
Pascal Poupart and Nikos Vlassis. Model-based bayesian reinforcement learning in partially observ-
able domains. In Proc Int. Symp. on Artificial Intelligence and Mathematics,, pp. 1-2, 2008.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shi-
mon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement
learning. In International Conference on Machine Learning, pp. 4295-4304. PMLR, 2018.
Heechang Ryu, Hayong Shin, and Jinkyoo Park. Multi-agent actor-critic with hierarchical graph
attention network. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34,
pp. 7236-7243, 2020.
Sajid Siddiqi, Byron Boots, and Geoffrey Gordon. Reduced-Rank Hidden Markov Models. In
Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp.
741-748. JMLR Workshop and Conference Proceedings, March 2010.
Satinder Singh, Michael James, and Matthew Rudary. Predictive state representations: A new theory
for modeling dynamical systems. arXiv preprint arXiv:1207.4167, 2012.
Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning to
factorize with transformation for cooperative multi-agent reinforcement learning. In International
Conference on Machine Learning, pp. 5887-5896. PMLR, 2019.
11
Published as a conference paper at ICLR 2022
James H Stock, Mark W Watson, et al. Introduction to econometrics, volume 3. Pearson New York,
2012.
Wen Sun, Arun Venkatraman, Byron Boots, and J Andrew Bagnell. Learning to filter with predictive
state inference machines. In International conference on machine learning, pp. 1197-1205. PMLR,
2016.
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi,
Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z. Leibo, Karl Tuyls, and Thore Graepel.
Value-Decomposition Networks For Cooperative Multi-Agent Learning. arXiv:1706.05296 [cs],
June 2017.
Ming Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In Proceedings
of the tenth international conference on machine learning, pp. 330-337, 1993.
Sebastian Thrun. Monte carlo pomdps. In NIPS, volume 12, pp. 1064-1070, 1999.
Joel A. Tropp. An Introduction to Matrix Concentration Inequalities. arXiv:1501.01571 [cs, math,
stat], January 2015.
Arun Venkatraman, Nicholas Rhinehart, Wen Sun, Lerrel Pinto, Martial Hebert, Byron Boots, Kris M
Kitani, and J Andrew Bagnell. Predictive-state decoders: Encoding the future into recurrent
networks. arXiv preprint arXiv:1709.08520, 2017.
Christian Schroeder de Witt, Bei Peng, Pierre-Alexandre Kamienny, Philip Torr, Wendelin Bohmer,
and Shimon Whiteson. Deep Multi-Agent Reinforcement Learning for Decentralized Continuous
Cooperative Control. arXiv:2003.06709 [cs, stat], December 2020.
Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising
effectiveness of ppo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021.
Kaiqing Zhang, Zhuoran Yang, and Tamer BaSar. Multi-agent reinforcement learning: A selective
overview of theories and algorithms. Handbook of Reinforcement Learning and Control, pp.
321-384, 2021.
Zhi Zhang, Jiachen Yang, and Hongyuan Zha. Integrating independent and centralized multi-agent
reinforcement learning for traffic signal network optimization. arXiv:1909.10651 [cs, stat],
September 2019.
Qiankun Zhao, Yuan Tian, Qi He, Nuria Oliver, Ruoming Jin, and Wang-Chien Lee. Communication
motifs: a tool to characterize social communications. In Proceedings of the 19th ACM international
conference on Information and knowledge management, pp. 1645-1648, 2010.
Meng Zhou, Ziyu Liu, Pengwei Sui, Yixuan Li, and Yuk Ying Chung. Learning Implicit Credit
Assignment for Cooperative Multi-Agent Reinforcement Learning. arXiv:2007.02529 [cs, stat],
October 2020.
12
Published as a conference paper at ICLR 2022
A Method Details
A. 1 Mathematical Model of PSR
Here, we give the mathematical model for PSR. As in Section 3, we let τt := {(ol, al)}lt=+tk-1 as a
test (a sequence of action-observation pairs) at time t, then we have a subset τto := {ol}lt=+tk-1 for
observations, and τta := {al}lt=+tk-1 for actions. And we let ht := {(ol, al)}lt=-11 as the history at time
t. A test is executed at time t if we intervene to select the sequence of actions specified by the test. It
is said to succeed at time t if it is executed and the sequence of observations in the test matches the
observations by the system (Boots et al., 2013).
The prediction of a length-k test τt is defined as the probability distribution that the test succeed at
time t, given history ht： Pr(To∣τ∕, ht) = PProa"*)
Let T = {τz},z ∈ N is a finite set of core tests. And We use To = {τf, .…，丁鬲} and Ta =
{τa, ∙∙∙, τa∏} to indicate its observation and action parts.
A.1.1 Linear PSR
A linear PSR is the prediction vector
qt := IPr(To∣τa,ht),...Pr(唱 ∣4aφht)]	(16)
that contains the probabilities of success of the tests in T, if and only if for any test τ
Pr(τ o∣τa ,ht ) = fτ(qt),	(17)
Where fτ : [0, 1]|T| 7→ [0, 1] is a linear projection function. For simplicity of notation, We use the
same notations for qtpt , W, F, Which only folloWs the meaning defined in this section.
It means that knoWing the probabilities for the tests in T is sufficient for computing the probabilities
for all other tests in the system. The prediction vector is a sufficient statistic for the system at time t,
so We call it as state for the PSR at time t. Therefore, PSR can represent state by using a vector of
predictions of fully observable quantities (tests) conditioned on past events (histories). The prediction
function fτis linear and has one-to-one relationship to a test T such that fτ(qt) = fτ>qt ∀t. The
linear PSR can still represent systems With nonlinear dynamics.
To maintain predictions in T, We need to update the state qt. To do that, We predict the success of any
core test Tz prepended by a neW action a and observation o at time t + k - 1, Which We call oaTz.
Based on Bayes rule, We have
and faoτz
Pr(Tzo,t+1|Tza,t+1,ht+1 = (ht, o, a)))
Pr(OTo,t+1|aTa,t+1,3
Pr(o∣a, ht)
fa>oqt
(18)
fao ∈ R|T| are linear operators such that (∀Tz ∈ T, ∀a ∈ A, ∀o ∈ O). Then let Faoτ be
the matrix With its columns as faoτz for all Tz ∈ T. Then the updated state in PSR is obtained by
qt+1 = F>oτqt.	(19)
fa>oqt
Given the initial prediction vector q1, the PSR can update With equation 19. This recursive application
of Bayes rule to a belief state is called the Bayes filter. NoW We have seen the extended prediction
vector, so We define extended predictive state as：
pt := IPr(aT1o|aT1a, ht), . . . Pr(aT|oT||aT|aT|,ht)i>.	(20)
Clearly, We can see there exists a linear extension operator W such that pt = W (qt). And given the
neW observation, We can have a Bayes filter F such that qt+1 = F(pt, o, a). We also introduced
these operators in section 4.4.
A.1.2 Representation of State as Conditional Expectation of Sufficient
Statistics
Instead of learning the distribution, here We recover the idea of representation of states as a conditional
expectation of sufficient statistics and using the supervised learning method to learn (Hefny et al.,
13
Published as a conference paper at ICLR 2022
2015). Let history hto is a sequence of observation that starts from the beginning of time and ends at
time t - 1.
We define the belief state bt = Pr(st|hto), where st is the current state of the world. bt+1 =
Pr(st+1|hto+1) is the next time belief state. We call bt as "belief state", which represents the
probability distribution over state space, also represents the knowledge and uncertainty about the
true state of the system. In dynamic system, the task of getting the updated bt+1 with given bt and
new observation ot is called filtering. The task of estimating the Pr(st+1 |hto) with given current bt
without incorporating any new observation is called one-step prediction.
Instead maintaining a belief bt over states, spectral algorithms try to recover observable operators
that can be used to perform filtering and prediction directly, by maintaining the expected value of a
sufficient statistic of future observations.
Let a test τto is a length k sequence of observation defined before. As the recursive Bayes rule
holds, We can get the next time prediction vector Pr(τt+∕hO+ι) using the new observation ot+k
and the current prediction Pr(To|h，), we also define an extended prediction vector Pr(Tt+ι∣hf).
These prediction vectors characterize the state of the system and they can be estimated by observ-
able quantities. Given Pr(Tto+1|hto), filtering becomes the task of getting the updated prediction
vector Pr(Tto+1 |hto+1), "conditioning" on ot. One-step prediction becomes getting the Pr(Tto+1 |hto),
"marginalizing" over ot .
Therefore, the spectral algorithms avoid explicitly estimating the latent state or the initial, transition,
or observation distributions. We let qt = E [νt∣hθ], where Vt a vector of features that determines
the distribution of future observations Pr(Tto|hto). For simplicity of notation, we use the same
notations for qt, pt, W, F. Let Pt = E [夕t∣hθ], where Wt is a vector of features that determines
the distribution of observations Pr(Tto+1 |hto). We call qt is the transformed predictive state. So
qt+ι = E [νt+ι∣hθ+ι], is the updated predictive state, and Pt is the extended predictive state. Let h；
be the feature vector of history hto .
In Hidden Markov Models and Kalman filters, the extended state pt is linearly related to the predictive
state qt . Which is pt = Wqt . Estimation of the W can be done using linear regression with samples
νt and Wt, however, due to the overlap between observation windows, the noise terms on νt and Wt
are correlated, which will cause biased estimate. The instrumental regression (Pearl et al., 2000;
Stock et al., 2012) is employed. hto is a instrumental variable that do not overlap with sequence
{l}ll==tt+k-1 and {l}ll==tt+k. The correlation corr(hto, e(Wt)) = 0 and corr(hto, e(νt)) = 0, where e is a
measure of error. By taking the conditional expectation of pt = Wqt given hto , we have
E[pt|hto] =E[Wqt|hto],	(21)
E[E[Wt∣hO]∣hO] = W E[E[νt∣hO]∣hO],
E[Wt|hto] =WE[qt|hto].
Based on the above relationship, we first estimate the E[Wt|hto] and E[qt|hto] by sample hto, νt, and
Wt, we then use the estimates to compute W. So if we start with q1, we can compute p1 = Wq1,
and get the qt+1 = F(p1, o1), where F is the Bayes filter to update the state.
A.2 Learning for GAPSR in Details
Here we introduce the details about learning GAPSR, which supplements the section 4.4 and 4.1.1.
A.2.1 LEARNING FOR qbt,i,j AND pbt,i,j
To calculate qbt,i,j, we introduce two sets of linear operators Ti,j and Ui,j, such that Ti,j (ψth,i,j) ≈
Cψo ψa ∣ψh and Uij(ψtjii) ≈ Cψa ψa ∣ψh . We estimate them by using two ridge regressions:
t,i t,j t,i,j	,	t,j t,j t,i,j
T
argmin X L(TijMijy),Ψ0i 乳 Ψt,j + R(Tij),	(22)
Ti,j	t=1
T
argminX L(Uij(ψhi,j),ψaj 氧 ψaj) + R(Uij).	(23)
Ui,j	t=1
14
Published as a conference paper at ICLR 2022
L represents the ridge regression loss, and R represents the regularizer. After learning Ti,j and Ui,j,
We can get the estimate of Cψo ψa 他『 and Cψa ψa ∣ψh . Then We get the qt,i,j by equation 7.
Similarly, we use extended features to obtain pbt,i,j .
A.2.2 Extension, Filtering and Prediction Functions
Filtering. To obtain qbt+1,i,j from qbt,i,j , pbt,i,j , We use filtering. We denote Fi,j as the filtering
function. We describe the filtering process as beloW. From {oi}in=1 and {aj}jn=1, We obtain the
embedding {φto,i}in=1 and {φta,j}jn=1. We then compute the observation covariance
Cot,i,ot,i lht,i,j ,at,j = MΦo,i<8Φo,i∣Φa,jΨ,i,j φa,j.	(24)
We then multiply the extended state by inverse observation covariance to change predicting φto,i into
conditioning on φto,i .
Mψo+ι,ilψa+ι,j,φo,i,φα,jM,i,j = Mψo+ι,i蜜φo,ilψa+ι,j,φa,j×φo,i (Cot,ig,”』,at,j + λI) ' (25)
× here is to denote n - mode (matrix or vector) product, ×φo means multiplying the tensor by a
matrix (or vector) in mode φto,i .
We condition on φto,i and φta,j to obtain shifted state.
qt+1,i,j := Mψo+ι,i∣ψa+ι,jM,i,φa,j,ψh,i = MΨθ+ι,i∣ψa+ι,j,Φθ,i,Φa,jM,i ×φo,i φo,i ×% φa∕	(26)
Based on the updating rule, qt+1,i,j = Fi,j (pt,i, ot,i, at,j ), and pt,i = Wi,j (qt,i,j), We Write the
filtering equation.
qt+1,i,j = Fi,j (pt,i, ot,i, at,j )	(27)
:= MΨ0+ι,i<8Φ0,i∣ψa+ι,j,Φa,jΨ,i,j ×φo,i(MΦ0,i<8Φ0,i∣Φa,j鸿,i,jφa,j+ λI) 1 ×φo,i φo,i ×% φa,j,
where pt,i ：= Mψo+1 .0φ^ ∣ψa十］，φa .∙ψh . Fi,j usually is known because it is obtained through the
above calculation using knoWn quantities, hoWever, Wi,j , Zi,j must have to be learned by using
regressions.
Extension. The Wi,j can be learned by kernel regression if we know the qbt,i,j and pbt,i,j . Previous
work (Hefny et al., 2018a) demonstrated the kernel regression model for learning single-agent PSR,
here we extend to the GAPSR. We set the model parameter Wi,j .
We optimized a ridge regression problem for Wi,j ,
T
arg min X L(Wi,j (qbt,i,j), pbt,i,j ) + R(Wi,j).	(28)
Wi,j t=1
Prediction. We can also get the prediction about the next one time observation ot by the regression
function such as:
bbt,i := E(Ot,i|qt,i,j ,at,j ) = Zi,j (qt,i,j % φa,j ) .	(29)
We solve the prediction regression function Zi,j by another ridge regression:
T
arg min X L(Zi,j@,i,j 乳 φa,j), φo,i) + R(Zi).	(30)
Zi,j	t=1
A.3 Tensor Decomposition of MAPSR
Existed Formulation. (Chen et al., 2020) use a n + 1 multi-dimensional tensor called system
dynamics tensor D ∈ RlT1l× ×lτnl×lHl to represent the system dynamics of the MAPSR, with |T1|
representing the cardinality of the tests for agent 1, n denoting the number of agents, and the |H|
being the cardinality of the joint history. Each element of the tensor is a probability of a joint test
given joint histories. Given the system dynamics tensor, finding the latent predictive state can be
transferred into finding a minimal linearly independent set from the system dynamics tensor, and it
can be solved by spectral method such as tensor decomposition.
As mentioned earlier, this formulation can not satisfy our needs. In Proposition 1 of Appendix A.4,
we also give the sample complexity analysis such that the sample size needed to formulate D scales
exponentially with the length of tests and number of agents.
15
Published as a conference paper at ICLR 2022
Here we give a summary of tensor decomposition of their method. Given a system dynamic tensor D
such that:
R
D≈ [λ; D1,…，Dn,F] = X λr Dr。…。DnFr.	(31)
r=1
Here。is the outer product. D1,…，Dn, F are matrices. The factor matrices D1, Dn, F consist
of the vectors, i.e., D1 = [D&D：、…D：R] ∈ RlT1l×R. A colon is used to indicate all elements of a
mode, thus, the Rth column of D1 is denoted by D:1R. For any i1 ∈ {1, . . . , |T1|}, in ∈ {1, . . . , |Tn|},
and k ∈ {1, . . . , |H|}, after the decomposition, we get
R
Dil,∙∙∙ ,in,k = X λrD1ιr …DinrFkr,	(32)
r=1
where {i1, ∙∙∙ , in, k} are index corresponding to the specific dimension of the D. The last dimension
of the tensor D is compressed in a matrix F, and its row vector xk = [xk(1) . . . xk(R)] ∈ R1×R, k ∈
{1, . . . , |H|} is a summary of joint history and can be considered as a compressed version of the
system predictive state vectorp(Q|hk), the joint history hk ∈ H(k ∈ [1, |H|]) at time step s = |hk|,
where Q is the core joint test set. The whole fibers listed in the set Q form a basis of the space spanned
by the mode-(n+1) fibers of tensor D. Thus, by constructing the vector m = (λ * D1 ： * …Dn：)T,
where * is Hadamard product. D^： denotes the i1-th row vector of D1. Then We could rewrite the
previous equation as
R
Dil,…,in,k =〉： m(r)xk (r) = Xk (λ * DiI ： * …Drnn)T = xkm.	(33)
r=1
m(r) is a scalar that m(r) = λr∙D11ι丁 .…Dnr. Andxk is the system state vector and m is the
prediction parameter, both of them are obtained by the tensor decomposition.
A.4 Analysis of Sample Complexity for Formulating the System Dynamic
Tensor
The paper (Chen et al., 2020) does not analyze the sample complexity to construct D, which is a
n + 1 multi-dimensional tensor D ∈ RlT1l× ×lTnl×lHl. We give this analysis. The Di、,…,in,k :=
Pr(t1i1, ∙∙∙ ,tnin∣hk) is an element of that tensor D such that " is the i1 -th test of agent 1,
similarly, the tnin is the in -th test of agent n, and hk is the joint history.
Proposition 1. In a n-agents system, assume every agent has the same observation and action space
|O|, |A|, fora length-k test, to formulate a complete system dynamics tensor D defined in equation 32.
Assume each entry of the tensor needs S samples to give a sufficient estimation using Monte-Carlo
roll-out method, then the total sample size is at least (|O||A|)knS, if the agents are homogeneous, in
other words, they are permutation invariant such that identity does not matter, then the total sample
size is (PHAnk+n-1)S.
Proof. At one time step, for any agent, it has |O||A| different combinations for the joint test, then
for a length of k tests, it follows that (|O||A|)k number of different choices. Then the tensor D
would need (|O||A|)kn elements to cover all the possible length k tests. So the total sample size is
(|O||A|)knS. If the agents are homogeneous, then the ordering does not matter, for each agent, we
have (|O||A|)k number of different choices for length k test, so the total choices for n agents are
(|O||An+nT), then we need (|O||An+n-1)S samples.	口
The sample complexity is exponentially scaled with the number of agents and length of tests.
B	Algorithm and Integrating with MARL method
We first introduce some backgrounds on two multi-agent frameworks: Multi-agent Markov Decision
Process (MMDP) and Multi-agent Partially observable Markov Decision Process (MPOMDP) since
the algorithms in our paper are developed based on the framework of MPOMDP.
16
Published as a conference paper at ICLR 2022
B.1	Multi-agent MDP and Multi-agent POMDP model
B.1.1	MMDP
MMDP model is a tuple S, N, {Oi}i∈[n]{Ai}i∈[n] , T, R , where S and N are finite sets of states
and agents. Ai is a finite set of actions available to agent i; T : S ×Aι ×∙∙∙×An ×S → [0,1] is a
transition function; and R : S → R is the reward function. Each agent i obtains reward as function
of the state and agent’s action ri : S × Ai → R, and receives a private observation from the state by
the observation channel oi : S → Oi. The state has distribution d : S → [0, 1]. Each agent aims to
maximize its own total expected return ri = PtT=0 γtrit where γ is a discounted factor and T is the
time horizon. In a shared reward situation, there is a team reward function r : SXAi ∙∙∙×An → R,
agents aim to maximize one shared total expected return r = PtT=0 γtrt .
B.1.2	MPOMDP
A MPOMDP model is a tuple (S, N, {Oi}i∈[n]{Ai}i∈[n], {Ωi}i∈[n],T, R), where
(S, Ai, Ti, Oi, Ωi, R) describe a single-agent POMDP.Oi is the set of observations the agent i can
make. Ωi : S × Ai × Oi → [0,1] is the agent,s observation channel function, which specifies
probabilities of observations given agent’s actions and resulting states. (S, Ai, Ti, Ri) describes
a single agent MDP; and each agent i obtaines reward as function of the state and agetn’s action
ri : S × Ai → R. Each agent aims to maximize its own total expected return ri = PtT=0 γtrit where
γ is a discounted factor and T is the time horizon. In POMDP, an agent’s belief about the sate is repre-
sented as probability distribution over S. The agent has prior belief b0,i The agent’s current belief, bt,i
over S, is continuously revised based on new observations and expected results of performed actions.
The belief update takes into account changes in initial belief, bt-1,i, due to action at,i, executed at
time t - 1, and the new observation, ot,i. The new time belief state can be obtained from basic
probability theory as follows: bi(st) = βΩi(ot,i, st, at-ι,i) Pst-1∈s bt-1,i(st-1)T(st, at,i, st-ι),
where β is the normalizing factor.
B.2	Algorithm: GAPSR
We give the details of GAPSR in Algorithm 1.
B.3	GAPSR IN DIAGRAM
We also use a diagram (Fig 4) to depict the algorithm. The algorithm runs k iterations; each iteration
first uses the policy to roll out data and uses the regressions to obtain the PSR parameters. Then it uses
the PSR as the input of policy to generate action and using the current PSR parameters to update the
predictive state. At the end of this phase, it updates both the PSR and policy parameters. The policy
parameters and PSR parameters Wi and Zi are parameterized by the neural network, section C.3
has the network architectures. The loss is a composite loss that includes loss from actor-critic and
the loss from the predictive state representation. The next iteration will re-learn the PSR parameters
using the newly generated data based on the current policy obtained from the previous iteration; then,
it does a soft update to update the PSR parameters with ones obtained at the previous iteration.
B.4	Integrating Two Common MARL Algorithms Into GAPSR Model
Here we provide a brief intuitive introduction about how we connect existed MARL algorithms with
GAPSR. Please look at Fig 4 for demonstration. ψto,i, ψta,j, and ψth,i,j are embedding of (ot:t+k-1,i,
at:t+k-1,j), and (o1:t-1,i, a1:t-1,j). We also have embedding for extended part, labeled as ξ. qt,i,j
and pt,i,j are estimated by regression using the embedding vectors. G represents the given graph,
the estimation of qt,i and pt,i are based on graph G and equation 5, equation 9 and equation 10 for
reference. Please also go to section 4 for detailed description. The policy network uses the predictive
state as input to return the action or its distribution. The agent takes the action to get the observation.
The filter Fi,j takes predictive state, action, and observation as inputs to get the next predictive state.
We present a centralized critic and give the description in paragraph B.4.2. The loss is composed into
two parts, we give a detailed explanation in section 5. Please also go to section 5 and Algorithm B.2
for more details about the framework.
17
Published as a conference paper at ICLR 2022
Algorithm 1 GAPSRL
1:	Input: Learning rate η, a graph G, a static complete graph Gc or static non-compete graph Gs
or dynamic graph Gt
2:	Initialize MARL Policy ΘMARL randomly
3:	for k = 1,2, 3,•…iterations do
4:	GAPSR Model Estimation Phase
5:	Sample b = 1, 2, 3,…M batch of initial trajectories: {(ob, ab)}M=ι from existed policy
obtained from previous iteration k - 1: {πik-1}in=1
6:	Let πik = πik-1 if available or the initial policy
7:	Given G, calculate qbt,i,j and pbt,i,j, and obtain the initial Wi,j, Zi,j, Fi,j:
8:	(1) Regression qt,i,j = Ti,j ◦ Ui,j (ht,i,j) to get the qbt,i,j and pbt,i,j
9:	(2) Giventheqbt,i,j, pbt,i,j, compute Wi,j, Zi,j, Fi,j
10:	(3) Get initial q1,i,j and p1,i,j by using the Ti,j and Ui,j with the early window of observa-
tions.
11:	(4) Given the graph G, using equation 5, 9, or 10 to obtain q1,i, p1,i
12:
13:	Generation of Predictive Representations
14:	Initialize GAPSR parameters ΘGAPSR = {Wi,j, Zi,j} from GAPSR Model Estimation Phase
and previous iteration by a soft-update:
Wi,j = βWi,j + (1 - β)Wik,j-1, Zi,j = βZi,j + (1 - β)Zik,-j 1, Fi,j = Fi,j
15:	for b = 1, 2,3,…M batch of trajectories from {∏k-1}n=ι do
16:	Reset episode: ab1 , ob1 for all agents
17:	for t = 1,2, ∙∙∙ T roll-in in each trajectory do
18:	for Each agent i do
19:	Get observation otb,i and reward rtb,i and its neighbor’s action atb,j
20:	Extension ptb,i,j = Wi,j(qtb,i,j)
21:	Filtering qtb+1,i,j = Fi,j (qtb,i,j, atb,j, otb,i, Wi,j)
22:	Predict obtb,i = Zi,j (qtb,i,j, atb,j)
23:	Given the graph G, obtain qtb+1,i
24:	EXeCUte ab+ι,i ~ πk-1(qb+ι,i)
25:	Collect otb,i , obtb,i, atb,i , rtb,i, qtb,i
26:	end for
27:	end for
28:	end for
29:
30:	Learning Multi-agent Actor-Critic
31:	Update Θ Using D = {{{otb,i,obtb,i,atb,i,rtb,i,qtb,i,qtb,i,j}in=1}tT=1}bM=1:
Θ — UPdate(Θk-1,D, η) as in Equation (13):
32:	L(Θ) = α1L1(ΘMARL) + α2L2 (ΘGAPSR, ΘMARL)
L1(ΘMARL) = -J(θ) + Lc(w)
Lc = Eq,r,o [(QW (q1, ∙∙∙ , qn, a1,…。，an) - y)]
Vθi J (θi) = Eq,r,o [Vθi ∏θi (Qi)VaiQW(Ql,…，Qn,a1,…，an ) | ai=∏θi3)]
L2(θGAPSR, θMARL) = Pn=ι Eτ~p(τ ∣Θ) [Pj || Zij(Fij (Wij (Qt-1,i,j )) X φa,j) - φo,ill2]
33:	Get Wik,j ,Zik,j , and πik
34:	end for
35:	Output: Return Θ = (ΘGAP SR, ΘMARL)
B.4.1 GAPSR- 1
It is based on the IAC (Foerster et al., 2018) which directly applies the single-agent policy gradient to
have each agent learn independently, with the idea behind independent Q-learning (Tan, 1993), with
actor-critic in place of Q-learning.
18
Published as a conference paper at ICLR 2022
KβMAPSR)
Agent 1
Agent n
Moy1
v。"⑸)
marl)
71^n(QVιJ
Figure 4: GAPSR architecture combining GAPSR and MARL. The left part is the GAPSR model,
which corresponds to the GAPSR Model Estimation Phase in Algorithm 1. The right part is the
MARL, which corresponds to the generation of predictive representations and learning the multi-
agent actor-critic. We use the actor-critic framework, which contains a centralized critic and many
decentralized actors. We could also give a parameter sharing actor network that maps individual PSR
to parameters of a Gaussian distribution over the individual action space if agents are homogeneous.
Both actor and critic are parameterized by neural networks. Section B.4 and section C.3 give a
detailed description about this architecture.
IAC trains an actor-critic pair for each agent, resulting in actors πi(ai|oi) and critics Vi(oi,ai).
T
NOiJ (θi) = Eo,a,r [X NOi log ∏i (at,i∣θt,i) (ri + YVa (θt+ι,i ,at+ι,i) - Vni (ot,i, at,i))]. (34)
t=0
While IAC agents display a strong ability to optimize individual rewards (Tan, 1993), the lack of
global information and a mechanism for cooperation means they are likely to settle for sub-optimal
solutions.
Here we use the predictive state qi to fit the value and policy functions. And we train an actor-critic
pair for each agent, resulting in actors πOi (ai|qi) and critics Vi(qi).
T
Nθi J(θi) = ETi『a∣θi) [X Nθi log ∏θi (at,i∣qt,i)(ri + Nw (qt+ι,i) - V∏i (qt,i))].	(35)
t=0
If the agents are homogeneous, we can share the critic and actor network. We have the critic loss as
Lc = ETi 〜p(τ ∣θi)[(Ci (qi) - yi )2 ],	yi = ri + YVi (qt+ι,i).	(36)
Here V is the target value function. Thus, the update of parameters is given by:
δt,i =ri+YVbπi(qt+1,i) -Vπi(qi),	(37)
1T
W0 = Wi + TfnW δt,iNg V πi (qi),	(38)
t=0
1T
θ0 = θi + Tfn δt,iNOiIOg πθi (at,i|qt,i).	(39)
t=0
B.4.2 GAPSR-2
The algorithm extends from MADDPG (Lowe et al., 2017) using the deterministic policy gradient of
the PSR-value function.
MADDPG is an extension of deep deterministic actor-critic policy gradient (DDPG) (Lillicrap et al.,
2015) to multi-agent setting such that let each agent’s own critic is augmented with extra information
about the actions of other agents, while their individual actor maintains a local state or observation.
19
Published as a conference paper at ICLR 2022
The gradient of each agent is:
Vθi J(θi)= Eo[Vθi∏θi(θi)VaiQ∏(oι,…,0n,aι,…,an)∣ai = π(oj].
(40)
The critic loss is
L = Eo,a,r [(Qn (ot,1,…,ot,n, at,1,…,at,n) - yi)2]	yi = ri + YQn (ot+1,1,…,ot+1,n, at+1,1,…,at+1,n),
(41)
1	C σr 1 •	,1	C .	1 C	.,1 FFF	,
where {Qiπ} is the set of target value functions with delayed parameters.
Our method considers the centralized critic. Also, in order to solve the non-stationary environments
when each agent is learning, it uses the joint predictive states with joint actions as input.
Vθi J (θi) = Eq,r,o[Vθi ∏θi (qi)Vai Qw (qi,…，qn,ai,…，an)∣αi=∏(q"
The critic loss is defined as below:
Lc = Eq,r,o[(Qw (qt,1, ∙∙∙ , qt,n, at,1,…，,at,n) ― y)^],
π
y = r + YQw (qt+1,1,…,qt+1,n, at+1,1,…,at+1,n).
Thus, the update of parameters is given by:
(42)
(43)
nn
δt = rt + YQ (qt+1,1,…,qt+1,n, at+1,1,…,at+1,n) - Q (qt,1,….,qt,n, at,1,…,at,n),
(44)
1T
W = W + T ɪ2nw&VwQn (qt,1, ∙∙∙ , qt,n, at,1,…,at,n),
T t=0
1T
θi = θi + T ^-^ηθV6in6i(aiIqi)VaiQn(q1,…，qn, a1,…，an)|ai=π(qi).
T t=0
(45)
(46)
C Environment and Experiment
C.1 MAMujoCo Environment Setup
(d)
(a)
(c)
(e)
(f)
!岛
,"⅞
(b)
Figure 5: The illustration of three environments swimmer, hopper, ant, and their corresponding
MAMuJoCo version. (a) Single swimmer; (b) Single hopper; (c) Single ant; (d) n-agents swimmer;
(e) 3-agents hopper; (f) 4-agents ant.
Here, we give the details of setting up our multi-agent environment - MAMuJoCo. As introduced in
section 6, the many agents are constructed by separating a existed single agent into parts, and each
agent will only control a part of the whole agent (Figure 5).
Partially Observable space: MAMuJoCo is a simulated robotic environment, the partially observable
property is achieved by only allowing partial information to the agents. For all environments, only
the angles of the agent’s joints are visible to the network; the velocities are hidden.
Action: Each agent’s action space in MAMuJoCo is given by the joint action space overall motors
controllable by that agent.
Observation: For each agent i, observations are constructed by inferring which body segments and
joints are observable by an agent i. Each agent can always observe all joints within its sub-graph. A
configurable parameter k ≥ 0 determines the maximum graph distance to the agent’s subgraph at
which joints are observable. For example, k = 0 means agents can only observe their own joints and
body parts, while k = 1 means it can observe its adjacent joints, which has 1 graph distance to the
20
Published as a conference paper at ICLR 2022
Table 1: Configurations for MAMuJoCo environment
Environments
Swimmer
k	0
R	2Pi(篝)+o.000ir
r	r = - Ilak2 is a regularize] forjoint action a
Hopper
k	2
R	Pi (篝) + 0.001r +1.0
r	r = -∣∣a∣∣2 is a regularizer forjoint action a
Ant
k	0
R Pi( ∆∆ti) + 5 ∙ 1e(-3) 11 external contact forces k 2 + 0.0001r
r	r = -∣∣a∣l2 is a regularizer forjoint action a
agent. The agent observation is then given by a fixed order concatenation of each observable graph
element’s representation vector. Depending on the environment and configuration, representation
vectors may include attributes such as position, velocity, and external body forces. In addition to
joint and body segment-specific observation categories, agents can also be configured to observe the
robot’s central torso’s position and velocity attributes.
C.2 Experiment setup
We select three experiments from MAMuJoCo and give a detailed description of the experiments’
setup. In all environments, the agent has the goal to maximize the velocity of the first coordinate for
the team. We use k to denote the maximum observation distances to the subgraph. We use ∆d to
denote the first coordinate position difference between a time difference ∆t. Finally, we use R to
denote the reward function. Table 1 has the configuration details for these parameters.
C.3 Neural Network Architecture
We implement all algorithms using deep neural networks as function approximators. We ensure
that all policy and and action-value functions have the same neural network architecture among all
algorithms to the extent each algorithm allows for a fair comparison.
Usually, in a continuous environment, each agent’s policy will be parameterized by its actor network
that outputs the mean and diagonal covariance of a Gaussian distribution over the continuous action
space. For our experiments with continuous action spaces, a Gaussian distribution with a diagonal
covariance matrix is used. The policy network maps from the input feature to a Gaussian distribution
vector μ. Moreover, μ = [mean, std], where mean is a vector specifies the action means, and std
vector specifies the standard deviation. For deterministic policy, it maps to the action vector.
In the implementation of actor-critic method, all the actor-network is parameterized by a multi-layer
perceptron (MLP) with two hidden layers of size 400 and 300 respectively and ReLU activation,
which takes in the individual agent’s predictive state and outputs the mean and covariance of a
Gaussian policy for stochastic policy, or the action vector for deterministic policy. The critic network
is also an MLP with two hidden layers with 400 and 300 units, respectively. For GAPSR-1, the critic
network is used to approximate per-agent utilities, which receives each agent’s predictive state as
input. For IAC, same as GAPSR-1, it receives agent local observation and individual action as input.
In GAPSR-2, there is a shared critic network that approximates all agents utilities, which receives
all agents’ predictive states and the joint action of all agents as input. The global state consists of
the complete state information from the environment. In MADDPG, the critic receives the global
state and the joint action of all agents as input. Each decentralized actor (i.e., policy) network takes
in each agent’s observation and outputs the agent’s action vector.
21
Published as a conference paper at ICLR 2022
Table 2: Model parameters for MAMuJoCo environment
	Environments		
	Swimmer	Hopper	Ant
n	2	3	4
μ	0	0	0
σ	0.1	0.1	0.1
γ	0.99	0.99	0.99
Soft target network	0.001	0.001	0.001
α1	0.7	0.65	0.7
α2	0.3	0.35	0.3
β	0.6	0.45	0.5
η learning rate of Adam	0.001	0.001	0.001
λ ridge regression regularization	0.01	0.01	0.01
Total iterations	50	50	50
Number of trajectories	100	100	100
Maximum number of steps per trajectory	1000	1000	1000
Length of test window	8	12	10
Length of history window	8	12	10
C.4 Model Parameters
The hyper parameters for the the MAMuJoCo environments are in Table 2.
D	Supplement Experiments
D. 1 MAMujoCo Supplement Experimental Results
We plotted the predictive observations compared to actual observations in Figure 2 at the beginning
of the learning process (iteration 1) and end of the learning (iteration 40). We also show the results
of iteration ten and iteration 20 in Figure 6. By comparing with iterations 1 and iterations 40 in
Figure 2, we see that the iteration 40 has the smallest difference between predictive observation and
true observation, and the difference gets increased as the iteration goes to the earlier stage of the
learning process. So the predictive accuracy is improved incrementally with the learning progresses.
D.2 Multi-Agent Particle Environment
We also test GAPSR into another environment, multi-agent particle environment (Lowe et al., 2017).
The agents are displaced into a 2-dimensional coordinate. This environment does not assume that all
agents have identical action and observation spaces. We run experiments using a different number of
agents on two environments, the predator-prey, and cooperative-push. We use the same configuration
in (Liu et al., 2020a). The network has the same design as MAMuJoCo in section C.3, except we use
MLP with two hidden layers with the same 128 units respectively, so we do not repeat the description.
In Table 3, we report the hyperparameters for multi-agent particle environments.
In the supplementary materials we provide the code and instructions.
22
Published as a conference paper at ICLR 2022
(a) 2-Agents Swimmer
(b) 3-agents hopper
,Ag叫， Agentl
O -l '	' Ξ- £'	'	'= O 0
打 ⅜M⅛ ;⅛⅜W『；
1 2二,：.；. I ； 1-l
J WW jy⅛Λ⅛ 5
O 50 O 50 O 50
(c) 4-agents Ant
O 200	400 O 200	400
(d) 2-Agents Swimmer
(e) 3-agents hopper
Figure 6: Additional Experiments Results, Predicted Trajectories vs Actual Observations for Multi-
Agent Environments. (a) - (c) Iteration 10; (d) - (f) Iteration 20; This is the supplement for Figure 2
Table 3: Model parameters for multi-agent particle environment
	Environments					
	Predator-prey			Cooperative-push		
	n=3	n=15	n=100	n=3	n=15	n=30
γ	0.99	0.99	0.99	0.99	0.99	0.99
Soft target network	0.001	0.001	0.001	0.001	0.001	0.001
α1	0.8	0.75	0.5	0.8	0.75	0.7
α2	0.2	0.25	0.5	0.2	0.25	0.3
β	0.45	0.5	0.45	0.55	0.5	0.5
η learning rate of Adam	0.05	0.05	0.05	0.05	0.05	0.05
λ ridge regression regularization	0.01	0.01	0.01	0.01	0.01	0.01
Total iterations	30	30	30	30	30	30
Number of trajectories	100	100	100	100	100	100
Maximum number of steps per trajectory	500	500	500	500	500	500
Length of test window	5	5	5	5	5	5
Length of history window	5	5	5	5	5	5
E Detailed Proofs
We give the proof of Theorem 1 in Appendix E.1, and we first introduce the following Lemmas to
prepare the proof.
Definition 2. Let X1 . . . Xk be independent random variables of dimensionality dX1 . . . dXk such
that kXk k < txk. Let {(x1j . . . , xkj)}jN=1 be the N i.i.d samples from distribution of X1, . . . , Xk,
the Cχι := E[XιXT] and CbXi = N P^=ι xijx[, and CX、x2 := E[XιXT] and CXι,χ?=
N PN=I xijxTj. Also, we use the ∙),v(∙) to denote the largest, SmaIIeSt eigenvalue ofa matrix.
Lemma 3 ((Tropp, 2015)). Let aj be a finite sequence of independent random, Hermitian matrices
with dimension d. Assume that 0 ≤ v(aj ) and u(aj ) ≤ L for each j. Let S = j aj , then for any
η ∈ [0, 1], it follows that
-η
Pr(V(S) ≤ (1 - η)v(E[S])) ≤ d[	-ɪ-]v(E[S])/L ≤ 2de-nv(E[S])/L	(47)
(1 - η)1-η
23
Published as a conference paper at ICLR 2022
Corollary 1. Let X be a random variable, for any ∈ (0, 1) such that N >
following holds with probability at least 1 -
v(CbX) >
txlog(2dx Ia
V(CX )N
In other words, if N large enough,
then CbX and CX will be close enough.
tx log(2dxIG
-V(CX)-
the
Proof. Define Sj = 1IN xj xjT . Then it follows that u(Sj) ≤ L = t2xIN and define :=
2dχe-σNv(CX )ltx, which implies that σ = tx 'V(C1)XN/'. Then it follows from Matrix Chernoff
Inequality in Lemma 3 that Pr (V(CX) ≤ (1 - σ)v(Cχ)) ≤ e.	口
Lemma 4 ((Tropp, 2015)). A finite sequence {aj} of independent, random matrices with common
dimensions a × b, and assume that E[aj] = 0 and kaj k ≤ L for each j, let S = j aj as a random
matrix. Let V ar(S) be the variance statistics such that V ar(S) = max{E[SST] , E[STS]},
then
-c2∕2
Pr(∣∣Sk > c) ≤ (a + b)e Var(S)+Lc/3.	(48)
Corollary 2. With at least probability 1 — E that
IIA C Il / 22log(dγ + dx)/evar	2log((dγ + dx”E)L
产YX - CYXII ≤ V-----------------N-----------+-----------3N-----------.
where L = tytx + kCYXk ≤ 2tytx and var = max{t2y kCXk ,t2x kCY k} + kCYXk2 ≤ 2t2yt2x.
Proof. Let X, Y be two random variables, and let a finite sequence {aj } of independent random
matrices to satisfy aj = yjxjT - CYX. So the aj will have dimensions dX × dY. Let random matrix
S = Pj aj. It follows that E[aj] = 0 and kajk = IIyj xjT - CYX II ≤ kykk kxkk + kCYXk ≤
tytx + kCYX k,
IE[SST]I =
E (叫yiχTXjyτ] - CyxCXY)
i,j	II
X (EUIxik2 yiyT] - CyxCxy) + X (E[yixT]叫XjyT] - CYXCXY)
i	i6=j
X(E[kxik2 yiyiT] - CYXCXY)III
i
≤N(t2xkCYk+kCYXk2).
Similarly, IIE[SST]II ≤ N (ty2 kCXk + kCYXk2). By applying lemma 4, we have E = Pr(kSk ≥
(-Nc2∕2 ʌ
Nc) ≤ (dx + dγ)e× Var+Lc/3) and therefore, it implies that
(log(dx + dγ/e))2L2	2log((dχ + dγ)/E)Var
9N2	+	N
c≤
log((dX + dY)/E)L
3N
+
≤
2log((dχ + dγ )∕e)L
3N
+
2 log((dX + dY)/E)v ar
N
□
Corollary 3. For random variable X with dimensionality dX and kXk ≤ tx, with probability 1 -
itfollows that
lCχ1/2(Cχ) - Cx 11 ≤ 2tx22log(NXIE)+2⅛d
t2
where L = / :	+ tχ
√V(CX)	X
24
Published as a conference paper at ICLR 2022
Proof. The proof is similarly to the the proof of corollary 2, define aj
S = P ajthen it follows that E[aj] = 0 and ∣∣aj k ≤ Jtx	+ tχ,
v(CX)
E[STS] = E[SST] ≤N(t2x+∣CX∣2) ≤2Nt2x.
Applying lemma 4 to get
P-X1/2
xj xjT -
it follows that
-Nc2/2
E = Pr(IlSk ≥ Nc) ≤ 2dχe2琮十~3,
c≤
2log(2dχ/e)L
3N
(49)
(50)
(51)
□
Lemma 5. For two random variables X, Y , let CY X
CYX + ∆YX,
and CbX
CX + ∆X where
E[∆YX] and E[∆X] are not necessarily zero and CbX is symmetric positive semidefinite. Define
A = CYXCX-1 and Ab = CbY X (CbX + λ)-1. Then it follows that:
M - A∣∣< [不(^vC Wχ1∕2gχ + λ ] +	∣δYXk .
v(CX)	v(CbX) + λ	v(CbX) +λ
+2tx 严!”ɪ.
Proof.
Ab - A = CY X ((CX + ∆X + λI)-1 -CX-1) + ∆YX(CX + ∆X + λI)-1 = M1 + M2 .
It follows that
kM2 k ≤
△yx
v(CbX) +λ
For M1, by using facts U-1 - V-1 = U-1(V - U)V -1 and CYX =CY1/2PCX1/2, where P is a
correlation matrix with kP k ≤ 1,
Mi = -CYX Cχ1(∆χ + λI )(Cχ + ∆χ + λI )-1
=-cγ∕2PCχ1∕2(∆χ + λI )(Cχ + ∆χ + λI )-1,
∣Mik ≤ VZu(CY)
IlCX%XIl+λ ∣∣cχ1∕2∣∣
v(CbX) +λ
S U(CY)Pv(CX)|cx1/2^x||+λ
Vv(CX)	V(CX) + λ
□
Corollary 4. Let {(xk, yk)}kN=1 be i.i.d samples from two random variables X, Y with dimensions
dX and dY and (uncentered) covariances CX and CY. Assume kXk ≤ tx and kY k ≤ ty. Define
A = CγχCχ1 and A = CYX(CX + λ)-i. For any E ∈ (0,1) such that N > tχ lOjC：：") the
following holds with probability at least 1 一 3e:
llb_ A∣l ≤ /U(CY) P PV(CX)α+λ ! ,_β_____
ll 一 ll- VV(CX)(V(CX )(1 一 Y) + λj + v(Cχ )(1 一 γ) + λ,
where	________
9. ∕2log(2dχ/e)	2log(2dχ/叽	cX	.
ɑ = 2tχi∕1(	+ tχ ),
XV N +	3N	(pV(CX) + x),
∕log(dγ + dχ )/e	3tytX log((dY + dX )/E)
β = 2tytχ V----N-+-------3N------,
=tXlog(2dχ/e)
Y =	V(CX )N	.
25
Published as a conference paper at ICLR 2022
Proof. It follows by applying Corollaries 1,2,3 to Lemma 5. By union bound, each condition has
probability 1 - , so the total events are bounded by
33
Pr(bounds satisfied) := 1 - Pr([ Ai) ≥ 1 - X Pr(Ai) = 1 - 3.
i=1	i=1
□
ɪ .	χ- I '	1	-	1 1 ΛΛ A Λ 1 , Z-J	C	. Λ	1 zɜ	C . Λ	1
Lemma 6. For two random variables X, Y, let CYX = CYX + ∆YX, and CX = CX + ∆X where
E[∆Y X] and E[∆X] are not necessarily zero and CbX is symmetric but not positive semidefinite.
Define A = CYXCX-1 and Ab = CbYXCbX(CbX2 + λI)-1. Then it follows that:
|以-All ≤ S U(CY)k∆χk2 + 2u(Cχ) k∆χk+ λ + IICYXkk∆χk + k∆γχIIilCXk + ∣∣∆γχ∣∣∣∣∆χk
Il ll — Vv(CX )3	V(CX)+ λ	V(CX )2 + λ
Proof.
Ab - A = (CYX + ∆yx)(CX + ∆X)((CX + ∆X)2 + λI)-1 - CYXCXCX-2
= CY X CX (((CX + ∆X)2 + λI)-1 - CX-2) + (CYX∆X + ∆YX CX + ∆Y X ∆X)((CX + ∆X)2 + λI)
= M1 + M2 .
For M1, by using facts U-1 - V-1 = U-1(V - U)V -1 and CYX = CY1/2PCX1/2, where P is a
correlation matrix with IPI ≤ 1, it follows that
Ml = -Cγ∕2PCX3∕2(∆X + CX Δx + Δx CX + λI )((Cx + Δx )2 + λI)-
Therefore,
IM1 I ≤
/ U(CY) ∣∆χ∣2+2u(CX2∣∆X∣+^
V V(CX )3	V(CX ) + λ
IM2 I ≤
ICyx ∣∣∣∣Δx k + ∣Cyx ∣∣∣∣Cx | + ∣∣Cyx ∣∣∣∣Δx 1
V(CbX)2 +λ
□
Corollary 5. Let {(xk, yk)}kN=1 be i.i.d samples from two random variables X, Y with dimensions
dX and dY and (uncentered) covariances CX and CY. The E[∆YX] and E[∆X] is not necessarily
zero and Cx is symmetric but not necessarily positive semidefinite. Assume IX I ≤ tx and IY I ≤ ty.
Define A = CY X CX-1 and Ab = CbYXCbX(CbX2 +λ)-1. For any ∈ (0, 1) such that N >
the following holds with probability at least 1 - 3:
tx log(2dx/4
-V(CX)-
IU(CY ) k∆χk2 +2U(CX ) k∆χ k+ λ + IICYX kk∆χ k + k∆γχ IIkCX k + k∆γχ kk∆χ k
V v(Cχ)3	v(Cx)(1 - γ)+ λ +	v(Cχ)2(1 - γ)2 + λ
where
tχ log(2dχ/e)
V(CX )N
Proof. It follows by applying Corollaries 1,2,3 to Lemma 6. Also by union bound, so the total events
are bounded 1 - 3.
□
Theorem 2. Grabner 1997 (3.3) (Grabner & Prodinger, 1997) Consider there are n independent
copies X1, . . . Xn i.i.d negative binomial random variables, with parameters defined as N B(b, p),
and our goal is to calculate the expectation of the maximum of these N random variables En =
E{max(X1, . . . , Xn)} then we have following asymptotic solution:
1γ
En = log1 (n) + (b - 1)log1 log 1 (n) + (b - 1)log 1 P + (b - 1) - log 1 (b - 1)! + 2 + ^
q
+ F(logι (n) + (b — 1)log 1 logι(n) + (b — 1)log 1 P - logι(b — 1)!) + o(1),	(52)
26
Published as a conference paper at ICLR 2022
(where F is a periodic C∞ - f unction of period 1 and mean value 0 whose Fourier-coefficients are
given by F(k)= -l0g(T) Γ(-IIg(πi)) for k ∈ Z \ {0} ,and q = 1 - P).
We omit the proof, interested readers could go to (Grabner & Prodinger, 1997) for details.
E.1 Proof of Theorem 1
We first prove the bound for kqbi,j - qi,j k.
Proposition 2. Let πΘ be a data collection policy and H is the range of πΘ on joint histories. If
Equation 7 used, then for all h ∈ H and any ∈ (0, 1), qbi,j (ψh) - qi,j(ψh) is bounded as below
with probability at least 1 - 3.
U u(cψo∣ψhj) I∣∆1k2 + 2u(cψa∣ψh.) I∣δi∣∣ + λ
kqi,j - qi,jk ≤ U ——----------7------、-------------
t v(cΨa∣ΨhJ	v(CΨa%)(I-Y)+ λ
+ ∣∣Cψoψa∣ψh/ ∣∣∆ι∣∣ +1∣∆2∣∣ ∣Cψɑ∣ψh,J∣ + k∆2kk∆ιk
V (cψa∣ψhj	(I-Y)2 + λ
where ∆1 follows the bound (55) and ∆2 follows the bound (53), and γ
t2A log(2dAj /)
3	J
Cψoψa∣ψh,3 - cΨ0Ψa∣Ψh,j ∣ ≤
∣∣CΨa% - Cψa*∣ ≤ ∣
Proof. Let Tij is the tensor such that Cψoψa∣ψh. = Tij Xh ψihj, and Ui,j is the tensor such
that Cψa∣ψh = Uij ×h ψihj and for simplicity without loss meaning, We use Cψa∣ψh to denote
Cψaψa∣ψh . Then We have
∣∣∣Tbi,j-Ti,j∣∣∣ ∣∣ψih,j∣∣,
Ubi,j - Ui,j ∣∣∣ ∣∣ψih,j ∣∣ .
We finish the above proof by proofing the kTi,j - Ti,j k and kUi,j - Ui,jk are bounded by using
Corollary 4.
Y) + 入)
α+λ
+
V
β
(Cψhj) (I-Y)+ λ
(53)
Where
α
2th
N
3N
2log(2dh∕e) + 2log(2dh∕e)
∖
/
β
2tOi tAj th
log(dθidAj + dh)/e + 4tOitAjth log((dθidAj + dh)/e)
N
3N
Y
th log(2dh∕e)
-7-----∖---；
V	Cψih,j N
(54)
and
27
Published as a conference paper at ICLR 2022
Y) + λ)
α+λ
+
β
V (Cψh,j) (I-Y) + λ
(55)
where
∖
2th
α
2log(2dh/e) + 2log(2dh∕e)
N
3N
+ th
)
2tAj th
β
log(dAj + dh)/e + 4t4th log((dAj + dh)/t)
N
3N
th log(2dh∕c)
V (Cψh,j) N
(56)
Then using the equation (7) and corollary 5 to obtain the bound for qi,j
U u(Cψo∣ψh)心1『+2u (Cψa∣ψh) IAk + λ
kqi,j- qi,jk ≤ U ʒ一-----------7------、-------------
tv(Cψak)	v(CΨa%) (1-γ) + λ
JcΨoΨa∣Ψ3∣∣ IdU IAkIKMjII + IAklZ
V (Cψa∣ψhj) (I-Y)2 + λ
t2A log(2dAj /)
where ∆ι follows the bound (55) and ∆2 follows the bound (53), and Y = -j7-Vj-.
v Cψja N
□
Now we start to prove our Theorem 1.
Proof. The Equation (5) says:
qt,i := g ({qt,i,j}n=l) = ^X qt,i,j，
j
here we assume the agents are homogeneous, in other words, each pair of {(oi, ai)}in=1 coming from
the same spaces O, A. They are permutation invariant and their identities do not matter. Thus, the
bound of qi,j is invariant to agents. Under the assumption of static fully complete graph, for any
agent qi = Pjn=1 qi,j , thus
kqbi - qik	= IIIPj qbi,j - Pj qi,jIII
≤ n kqbi,j - qi,jk .	(57)
□
E.2 Proof of Lemma 1
Here we prove Lemma 1.
Proof. Equation (9) says
qt,i := g ({qt,i,j }n=J = ^X Ii,j qt,i,j,
28
Published as a conference paper at ICLR 2022
where Ii,j is an indicator function to denote if two agents are connected. Under the assumption that
the static non-complete graph has maximum of number of degrees k and the agents are homogeneous,
we have Pj Ii,j <= k. Thus,
kqbi - qik = PjIi,jqbi,j - Pj Ii,jqi,j
≤ k kqi,j - qi,jIl.	(58)
□
E.3 Informal Proof of Lemma 2
Here we make some intuitions for the proof of Lemma 2. As we already give the proof sketch
in our main paper. Lemma 2 is a direct application of Theorem 2. Here our random variable
Ji, ∙∙∙ ,J 〜 NB(r,p), where {(J )}n=ι represents the number of time points node i needs before
it meets node j number of r times.
For the complete static graph, we need at least N sample for the bound in equation 8 to be valid; in
other words, we need the trajectory to run at least N time points to collect enough data to estimate
our conditional operator accurately qi,j .
For the dynamic graph, each time t, the two nodes are randomly connected with probability p, if it
connects, then we can obtain a valid sample to estimate qi,j; if not, then we skip to the next time step.
For the dynamic graph node i, if we take the union set of the nodes i connected over the trajectory
path, then the union set could form a static complete graph. We say the two graphs are equivalent. The
number of time points needed by i until the Nth connection with j for each of the pairs (i, j) follows
the same distribution J 〜NB(N, p). Then We are interested in the expectation of the maximum of
J1, . . . , Jn-1, we denote it as J{1,...,n-1}.
E{J{1,...,n-1}} means on average, how many time points (N0) we need for all nodes other than i
at least meets N times with i. Obviously, this N0 >= N since p ∈ [0, 1]. The calculation of this
expectation is solved by (Grabner & Prodinger, 1997). And we also put their result in Theorem 2.
29