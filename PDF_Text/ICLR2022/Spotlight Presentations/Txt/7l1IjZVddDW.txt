Published as a conference paper at ICLR 2022
Improving Federated Learning Face Recogni-
tion via Privacy-Agnostic Clusters
Qiang Meng1, Feng Zhou1, Hainan Ren1, Tianshu Feng2, Guochao Liu1, Yuanqing Lin1
1 Algorithm Research, Aibee Inc. 2 Independent Researcher
Ab stract
The growing public concerns on data privacy in face recognition can be greatly
addressed by the federated learning (FL) paradigm. However, conventional FL
methods perform poorly due to the uniqueness of the task: broadcasting class
centers among clients is crucial for recognition performances but leads to privacy
leakage. To resolve the privacy-utility paradox, this work proposes PrivacyFace,
a framework largely improves the federated learning face recognition via commu-
nicating auxiliary and privacy-agnostic information among clients. PrivacyFace
mainly consists of two components: First, a practical Differentially Private Local
Clustering (DPLC) mechanism is proposed to distill sanitized clusters from local
class centers. Second, a consensus-aware recognition loss subsequently encour-
ages global consensuses among clients, which ergo results in more discriminative
features. The proposed framework is mathematically proved to be differentially
private, introducing a lightweight overhead as well as yielding prominent per-
formance boosts (e.g., +9.63% and +10.26% for TAR@FAR=1e-4 on IJB-B and
IJB-C respectively). Extensive experiments and ablation studies on a large-scale
dataset have demonstrated the efficacy and practicability of our method.
1	Introduction
Face recognition technique offers great benefits when used in right context, such as public safety,
personal security and convenience. However, misuse of this technique is a concern as it involves
unique and irrevocable biometric data. The rapid commercial applications based on face recognition
and facial analysis techniques have stimulated a global conversation on AI ethics, and have resulted
in various actors from different countries issuing governance initiatives and guidelines. EU’s Gen-
eral Data Protection Regulation (GDPR) (Voigt & Von dem Bussche, 2017), California Consumer
Privacy Act (CCP) and Illinois Personal Information Protection Act (IPI) enforces data protection
“by design and by default” in the development of any new framework. On the nationwide “315
show” of year 2021, the China Central Television (CCTV) called out several well-known brands for
illegal face collection without explicit user consent. As researchers, it is also our duty to prevent
the leakage of sensitive information contained in public datasets widely used by the research com-
munity. Therefore, faces in ImageNet (Deng et al., 2009) were recently all obfuscated (Yang et al.,
2021) and a large face dataset called MS-Celeb-1M (Guo et al., 2016) was pulled of the Internet.
In the wake of growing social consensus on data privacy, the field of face recognition calls for a
fundamental redesign about model training while preserving privacy. A potential solution is the
paradigm called Federated Learning (FL) (McMahan et al., 2017a). Given C clients with local
datasets {D1, D2,…,DC} as shown in Fig. 1a, FL decentralizes the training process by combining
local models fine-tuned on each client’s private data and thus hinders privacy breaches. Typical
examples of these clients include personal devices containing photo collections of a few family
members, or open-world scenarios such as tourist attractions visited by tens of thousands of people.
In most circumstances, we can safely assume very few classes would co-exist in two or more clients.
Despite the numerous FL-based applications in various domains (Kairouz et al., 2019) ranging from
health to NLP, there are very little progress (Aggarwal et al., 2021; Bai et al., 2021; Liu et al., 2021)
in training face recognition models with FL schemes. Unlike other tasks, parameters of the last
classifier for a face recognition model are crucial for recognition performance but strongly associated
with privacy. These parameters can be regarded as mean embeddings of identities (Wang et al., 2018;
1
Published as a conference paper at ICLR 2022
羲 local dataset 禽 feature extractor ..........A train local models — —> gather/deploy class information ∣
㊁ feature space G not sharable 顾 differential privacy > gather/deploy feature extractors
(a)	(b)
Figure 1: Under the federated setting, multiple clients communicate non-sensitive model parameters φc (ex-
cluding the last fully connected layer Wc which are greatly tied to privacy) under the orchestration by a central
server. (a) Since Wc's are kept locally in conventional FL updating, the embedding space could overlap for
different classes during training. (b) In contrast, the proposed PrivacyFace framework learns an improved face
embedding by aggregating discriminative embedding clusters that are proved to achieve differential privacy.
Meng et al., 2021b; Shen et al., 2020) (also called as class centers), from where individual privacy
could be spied out as studied by plenty of works (Kumar Jindal et al., 2018; Boddeti, 2018; Mai
et al., 2020; Dusmanu et al., 2021). That prevents the FL approach from broadcasting the whole
model among clients and the central server, and consequently leads to conflicts in the aggregation
of local updates. As depicted in the global feature distribution of Fig. 1a, both clients try to spread
out their own classes in the same area (pointed by the arrow) of the normalized feature space. Thus,
the training loss could oscillate to achieve consensus given the sub-optimal solutions from multiple
clients. On the other hand, a large batch with sufficient negative classes is necessary to learn a
discriminative embedding space for advanced face recognition algorithms. During the conventional
FL updates, each class is only aware of local negative classes while those from other clients are
untouchable. This further limits performances of FL approaches in face recognition.
The privacy-utility paradox motivates us to introduce PrivacyFace, a framework improves feder-
ated learning face recognition by broadcasting sanitized information of local class globally. In the
framework, a novel algorithm called Differentially Private Local Clustering (DPLC) first gener-
ates privacy-agnostic clusters of class centers while any specific individual in the cluster cannot be
learned, irrespective of attacker’s prior knowledge, information source and other holds. Recall that
the privacy cost of a differential privacy scheme is propotional to the l2 -sensitivity while inversely
propotional to the query number. Our DPLC reaches a low l2-sensitivity by restricting the cluster
size as well as covering sufficient class centers. In addition, the number of necessary centers to com-
municate in DPLC is irrelevant to the number of classes in the training data. These characteristics
jointly equip DPLC with much smaller privacy cost than the naive alternative, which sanitizes each
class center individually by Gaussian noise. In our experiments, DPLC’s privacy cost is only 1.7e-7
of that of the naive approach. That persuasively reveals the high security level of our approach.
The second part of PrivacyFace is the consensus-aware face recognition loss. Following principles
of Federated Averaging (FedAvg) (McMahan et al., 2017a), a server iteratively gathers feature ex-
tractors and privacy-agnostic clusters from clients, averages parameters of feature extractors and
distributes them to clients. Accordingly, the consesus-aware loss notifies each client not to embed
samples in the inappropriate zone (differential private clusters marked by DP) of the feature space
during the local optimization, as shown in Fig. 1b. This process aids each client to train more dis-
criminative features as well as align all consensuses. Compared to the conventional approach, our
PrivacyFace boosts performances by +9.63% and +10.26% for TAR@FAR=1e-4 on IJB-B and IJB-
C respectively with only single-digit privacy cost. Moreover, the additional computational cost as
well as communication cost are negligible (e.g., the extra clusters to broadcast only occupy 16K stor-
age while the backbone already takes 212M). In a word, PrivacyFace is an efficient algorithm which
improves conventional federated learning face recognition by a large margin on performances, while
requires little privacy cost as well as involves lightweight computational/communication overheads.
2
Published as a conference paper at ICLR 2022
2	Preliminaries
2.1	Deep Face Recognition
Most of early works in deep face recognition rely on metric-learning based loss, including con-
trastive loss (Chopra et al., 2005), triplet loss (Schroff et al., 2015) and N-pair loss (Sohn, 2016).
These methods are usually inefficient in training on large-scale datasets. One possible reason is
that their embedding spaces at each iteration are constructed only by a positive sample and limited
negative ones. Therefore, the main body of research (Ranjan et al., 2017; Liu et al., 2017; Wang
et al., 2018; Deng et al., 2019; Xu et al., 2021; Meng et al., 2021a;c) has focused on devising more
effective classification-based loss and achieved leading performances on a number of benchmarks.
Suppose that we are given a training dataset D with N face samples {xi , yi}iN=1 of n identities,
where each Xi is a face image and yi ∈ {1, ∙∙∙ ,n} denotes its associated class label. Consider-
ing a feature extractor φ generating the embedding fi = φ(xi) and a classifier layer with weights
W = [wι,…，Wn], a series of face recognition losses can be summarized as
N	eu(wyi ,fi)
Lcls (φ, W) = -A log eu(wyi f + Pnj 加ev(Wjfi) ∙	⑴
The specific choices of the similarity functions u(w, f), v(w, f) yield different variants, e.g.:
COSFaCe(Wangetal.,2018) : u(w, f) = S ∙ (cos θ — m), v(w, f) = S ∙ cos θ,	(2)
ArCFaCe(Dengetal.,2019) : u(w, f) = S ∙ cos(θ — m), v(w, f) = S ∙ cosθ,	(3)
where S, m are hyper-parameters and θ = arccos(wT f /kwT f k) is the angle between w and f.
2.2	Differential Privacy
Differential Privacy (DP) (Dwork et al., 2006) is a well-established framework under which very
little about any specific individual can be learned in the process irrespective of the attacker’s prior
knowledge, information source and other holds (Feldman et al., 2017; Nissim et al., 2016; Stemmer
& Kaplan, 2018). We state the relevant definitions and theories (Dwork & Roth, 2014) below.
Definition 1 (Differential Privacy). A randomized algorithm M : X → Y is (, δ)-DP if for every
pair of neighboring datasets X, X0 ∈ X (i.e., X and X0 differ in one row), and every possible
output T ∈ Y the following equality holds: Pr[M (X) ∈ T] ≤ ePr[M (X0) ∈ T] + δ.
Here , δ ≥ 0 are privacy loss parameters, which we consider a privacy guarantee meaningful if
δ = o( §), where n is the size of dataset.
Definition 2 (l2-sensitivity). The l2-sensitivity of a function f : X → Rd is
∆2(f)=	max	kf(X)-f(X0)k2.	(4)
X,X0∈X
X,X 0 are neighbors
Definition 3 (Gaussian Mechanism). Suppose a function f : X → Rd and Id is the d-
dimensional identity matrix. The mechanism M (X) = f(X) + N(0, σ2Id) is (, δ)-DP if
σ ≥ δ2γ-lq2ln( 1δ3 * 5).
Definition 4 (Composition of Differentially Private Algorithms (Dwork & Roth, 2014; Dwork &
Lei, 2009)). Suppose M = (M1,M2, .…,Mk) is a Sequence of algorithms, where Mi is (q, δi)-
differentially private, and the Mi ’s are potentially chosen sequentially and adaptively. Then M is
(Pik=1 , Pik=1 δk)-differentially private.
3	PrivacyFace
This section details the PrivacyFace framework as illustrated in Fig. 2. At its core is a novel clus-
tering algorithm that extracts non-sensitive yet informative knowledge about the local class distri-
bution (Fig. 2a). After the central server broadcasting local DP-guaranteed outputs (Fig. 2b), each
client optimizes over a consensus-aware objective that takes into account both the local data and the
privacy-agnostic clusters to learn a discriminative embedding space (Fig. 2c) for face recognition.
3
Published as a conference paper at ICLR 2022
(a) Local clustering.
(b) Gaussian mechanism.
(C) Optimization w. consensus.
Figure 2: Compared to conventional federated learning methods, PrivacyFace learns more discriminative fea-
tures by three additional steps in each client: (a) Find a cluster with margin P and calculate the average P
of class centers covered in the cluster; (b) Perturb P with Gaussian noise v, which makes outputted P to be
differentially private. (c) After the server gathering and distributing p, a consensus-aware face recognition loss
enables each class to be separable with local negative classes as well as class clusters from other clients.
3.1	DIFFERENTIALLY PRIVATE LOCAL CLUSTERING
Our target is to improve performances of federated learning face recognition by communicating
auxiliary and privacy-agnostic information among clients. To distill useful information from each
client and resolve the privacy-utility paradox, we propose a specialized algorithm called Differential
Private Local Clustering (DPLC) with rigorous theoretical guarantees.
Problem Design. To facilitate the discussion, let us first introduce the margin parameter ρ ∈ [0, π],
which defines the boundary of a cluster centered at p by saying that any w in the cluster satisfies1
arccos(wTp) ≤ ρ. We design the local clustering problem with two principles. First, the clustering
results are expected to carry information only about population rather than individual from local
dataset. Supported by Theorem 2 proved later, this principle ensures PrivacyFace to gain insight
about the underlying distribution, while preserving the privacy of individual record. Second, the
spaces confined by the clusters should be tight because they represent the inappropriate zones to
escape from in other clients’ perspectives. A cluster with large margin ρ would occupy a huge
proportion of the sphere, leaving limited room for other embeddings (e.g., half of sphere would be
occupied if P = π∕2). More specifically, we quantify the occupancy ratio by the following theorem:
Theorem 1. Assume there is a unit and d-dimensional sphere Sd and an embedding point p ∈ Sd.
Embeddings with angles less than P to P (i.e., {f : arccos(f TP) ≤ ρ}) occupy 11sin2(P) (d-1, ɪ)
of the surface area of Sd . Here Ix(a, b) is the regularized incomplete beta function.
Proved in Sec. A.1.3 of the appendix, Theo-
rem 1 indicates that the occupancy ratio increases
monotonically with respect to the margin P given
a feature dimension d. Fig. 3 plots a series of
functions between cluster margin P and occu-
pancy ratio under different dimensions. Take the
rightmost curve corresponding to a typical setting
d = 512 in face recognition as an example. The
occupancy ratios are 0.055, 5 ∙ 10-5,4 ∙ 10-10
when ρ = 1.5,1.4,1.3 respectively. In another
word, the whole feature space would be fully
occupied if we sample 20, 000 clusters over the
original face classes when P = 1.4. It is obvious
Figure 3: Visualization of the occupancy ratio curves.
that the more space left for optimization, the higher likelihood to achieve better performance. There-
fore, we seek for a margin-constrained clustering of local data with fixed P ≤ 1.4 when d = 512 for
better privacy-utility trade-offs. A counter example to the latter principle is the classical k-means
algorithm, which have been extended to differentially private versions in a number of works (Nissim
et al., 2016; Feldman et al., 2017; Stemmer & Kaplan, 2018). However, k-means naturally has no
control about the scopes of the generated clusters, not to mention the difficult hyper-parameter k to
pick. That motivates us to propose a specified clustering algorithm for our task.
1We adopt cosine similarity by assuming all vectors lie on a unit sphere in face recognition.
4
Published as a conference paper at ICLR 2022
An Approximated Solution. Given the margin ρ, the problem of finding clusters to cover all class
centers is a generalization of the well-studied 2D disk partial covering problem which is unfortu-
nately NP-complete (Xiao et al., 2004). We take a greedy approach by iteratively finding a cluster
centered at p* with margin P to cover the most class centers Wi and weeding out those covered
before the next round. We cast the problem of identifying such a cluster at each iteration as:
Definition 5 (Spherical Cap Majority Covering Problem). Denote 1(∙) as an indicator function.
Assuming a unit d-dimensional sphere Sd and class Centers {wι, w2, •…,Wn} ∈ Sd, the target is
to find a cluster center p* ∈ Sd where p* = arg maXp∈sd PZi 1(arccos(wT P) ≤ ρ).
Finding the optimal p* is also NP-ComPlete. To address this, We first sort out the densest area S in
terms of the neighbor count for each w. An efficient approximation of p* is then the average of class
centers in the area, P =看 Pi∈s Wi. Lines 3-5 of Algorithm 1 summarize the detailed procedure,
by which the approximate cluster centers pi can be quickly enumerated as depicted in Fig. 2a. In
line 8, the class centers would be further normalized in accordance with the DP mechanism.
1
2
3
4
5
6
7
8
9
10
Algorithm 1: Differentially Private Local Clustering (DPLC)
Data: Class center embeddings W = {wi, w2,…，wn} encoded in the classifier layer.
Parameters: Margin ρ; Minimum cluster size T; Maximum #queries Q; Privacy budget , δ.
Result: Privacy-agnostic cluster centers p's.
Let I = {1, 2, ∙∙∙ , n} and calculate θ%,j = arccos(wf Wj) for i,j ∈ {1,2, ∙∙∙ , n};
for q = 1, 2,…Q do
For each {wi, i ∈ I}, find the indexes of its neighbors by Si = {j : θij ≤ ρ,j ∈ I};
Find a set with the most elements, i.e., S = arg max |Si| ;
P J 吉 ∑2i∈s Wi ;	// approximate solution for problem 5
if |S| ≥ T then	____________________
Sample a V from distribution N(0, σ2Id) where σ = 品 J(I 一 cos(2ρ)) ln( 1δ25);
_ Pq J kp+Vk, I J I∕{arccos(WT kPk2) ≤ P,i ∈ I};
else
Break out of the loop ;
Differential Privacy Endorsement. Although the centers p reveal the population-level property
of the local training set, they are still outcomes of a deterministic algorithm, thereby vulnerable to
adversary attack during the FL updating. We prove below that p can be perturbed to achieve DP:
Theorem 2. Define a function P，f (S)=看 ∑2i∈s Wi. Then the Gaussian Mechanism P，
M(S) = f (S) + N(0, σ2Id) is (e, δ)-DP if σ ≥ 昂，(1 - cos(2ρ))ln(ɪf).
Proof. Let S be the set of indexes of class centers which have cosine similarities larger than cos ρ
with respect to a center Wo . Assuming that S, S0 are neighbors differed at W and W0, where vectors
are normalized, i.e., kWk = kW0k = kWok = 1, then we have WoTW ≥ cosρ and WoT W0 ≥ cos ρ.
T0	T
Lemma 2 in appendix states that ∣∣wW2∣Wi2 ≥ cos(arccos ",w∣2
{W, W0 , Wo }. Therefore, the lower bound of WTW0 is
T0
+ arccos T——roη~) for all
kwok2kw0k2
wTw0
wTw0
kwk2kw0k2
T
woTw
≥ cos(arccos  -———-——+ arccos --
kwok2kwk2	kw
WT w0	)
O∣2kw0k2)
≥ cos(ρ + ρ) = cos(2ρ)
Following this inequality, the l2-sensitivity of f(S) is 这 vz2 - 2cos(2ρ) as Ilf(S) 一 f(S0)k2 =
* ∣∣w 一 w0∣2 =看√2 - 2wtw0 ≤ 看,2 - 2cos(2ρ). By Definition 3, we easily conclude that
M(S) = τ⅜ Pi∈s Wi + N(0, σ2Id) is (e, δ)-DP if σ ≥ 昂，(1 一 cos(2ρ))ln(吟).	□
Additional proofs can be found in Sec. A.1.2 of appendix. Theorem 2 outlines the setting of noise
variance σ given the privacy budget , δ and the cluster margin ρ. Basically, the lower bound of
5
Published as a conference paper at ICLR 2022
σ is proportional to 看.That implies when the size of cluster |S| is large enough, adding small
noise is sufficient to achieve promising privacy level. We found setting minimum cluster size to
T , min |S| = 512 yields empirically stable performance. It is worth emphasizing that the number
of original classes n does not directly influence the introduced noise σ . This advantage ensures that
DPLC maintains a high utility when dealing with large-scale face recognition problems.
Full algorithm. Algorithm 1 presents the full DPLC and we highlight several key properties below.
Properties 1, 2 state that DPLC is efficient and differentially private in theory. Besides the efficency
and privacy, another major concern of the DP algorithm is the utility. Specifically, as p^ is a Combi-
nation of the source cluster center p and Gaussian noise v, we should prevent the noise v to have
overwhelming effects to the output, which is further verified by our Property 3 and Property 4.
Property 1. The complexity ofthe DPLC algorithm is O(Q ∙ n2).
Proof. The computational complexity of calculating θi,j is O(n2). In each loop, we also compare
the value θi,j, ∀i, j ∈ S with ρ, whose worst time complexity is O(n2). Because remaining steps
are of linear complexity, the total complexity is O(n2 + Q ∙ n2) = O(Q ∙ n2).	□
Property 2. DPLC is a (Q ∙ e, Q ∙ δ)-differentially private algorithm.
Proof. The mechanism f(S) + N(0, σ2Id) is (, δ)-differentially private according to Lemma 2.
As DPLC queries at most Q results, it,s easy to conclude that our DPLC algorithm is (Q ∙ e, Q ∙ δ)-
differentially private based on Definition 4.	□
Property 3. Denote Φ as the cumulative distribution function of a standard Gaussian distribution.
For vector magnitudes ofp, V, we have I∣pk2 ∈ (Cos ρ, 1] and P (Ilvk2 ≤ r) ' Φ( :	一 d-1).
σ2 2(d-1)	2
Proof. For p, the upper bound is 1 as ∣∣p∣2 = k * pi∈s Wik2 ≤ 意 pi∈s ∣∣Wi∣∣2 = LAS S
denotes the set of indexes of class centers which has cosine similarities larger than cos ρ with a
center wo, the lower bound of ∣p∣2 can be calculated by
kpk2
kpk2kwok2 ≥ pTwo
i∈S
i∈S
T
wi wo
≥ 西(I + (ISI -
|S|
1) cos ρ) > cos ρ.
Therefore, we have ∣p∣2 ∈ (cos ρ, 1].
For v 〜N (0, σ2Id), let v = [vι, v2,…,vd], We have Vi following the standard normal distribution
for all i. Thus, the sum of square of V follows a χ2-distribution with d 一 1 degrees of freedom, i.e.,
σ2l∣v∣2 = Pd=I (vi)2 〜χd-ι. The mean and variance OfkVk2 are E(∣v∣2) = σ2(d - 1) and
V ar(∣v∣22) = 2σ4(d 一 1), based on properties of the χ2-distribution. By the central limit theorem,
kvk22 converges to N(σ2 (d 一 1), 2σ4(d 一 1)) when d is large enough. In practice, for d ≥ 50, kvk22
is sufficiently close to N(σ2 (d 一 1), 2σ4(d 一 1)) (Box et al., 1978). Therefore, we have
P(kvk2≤r)
P(kvk2- σ2(d- 1)
√2σ4(d - 1)
≤ r2-σ2(d- 1))' Φ( —Jr	-ʌ 尸)
一 √2σ4(d - 1)	σ2√2(d - 1)	2	2 /
Note that in our work, σ can be a small value with little privacy cost. Thus, with high probability,
the norm of V is close to zero.	□
Property 4. If ∣p∣∣2 ≥ ∣∣v∣2, the cosine similarity between P and P is always greater than
P - kvk2/|Pk2.
Proof. Denote magnitudes of p, v as lp , lv and cosine similarity between p and v as x (i.e., x
kPk UVk ∈ [一1, 1]). Let a = lr < 1 and S be the cosine similarity between P + V and p, then
S = kP++v涵2 = lp√⅜⅞XvX = √++⅛ .The first order derivative of S with respect to X
is ds =	a2(a+X)
dx	(1+a2 + 2ax) 2
It,s easy to conclude that S takes the smallest value when X = -a, which can
be reached as a is always smaller than 1. In the end, we have S ≥
,1 一 ιv∕ιp.
□
6
Published as a conference paper at ICLR 2022
In DPLC, we reasonably require large local clusters S at line 6 while preserving privacy in terms of
, δ. That constraint ensures the synthesized noise v to have a small magnitude (property 3), which
thereby results in slight offsets of the sanitized vector p^ to the source P (property 4). With these
theoretical guarantees, DPLC can obtain good end-to-end utility with low privacy cost.
3.2	Optimization with Global Consensus
PrivacyFace employs a Federated Learning (FL) paradigm for optimization. Suppose there are C
clients, each of which C = 1, ∙∙∙ , C owns a training dataset Dc with Nc images from n identi-
ties. For client c, its backbone is parameterized by φc and the last classifier layer encodes class
centers in Wc = [wc, ∙∙∙ , Wnc]. Without loss of generality, we assume that each class center Wc
is normalized. Central to PrivacyFace is the DPLC algorithm, which generates for each client, Qc
clusters defined by the centers Pc = {P1,…,PQJ with margin ρ. As presented in Algorithm 2,
the training alternates the following updates in a sufficient number of rounds M .
Client-side. In line 3 of Algorithm 2, the server broadcasts current feature extractor φt-1 to each
client. Then each client updates its local cluster centers Pc that can be safely shared to others in
line 5. After receiving P = {P1,…，PC}, each client executes one-round optimization over the
consensus-aware face recognition loss on local dataset Dc in line 7:
Nc	eu(wyic ,fic)
L(φ, W)=- i=ιlog eu(wfy+Pnj^f+PfTkZP=^f), (5)
where ff is the feature extracted by φc on i-th instance in client c. μ(p, f ,ρ) = S ∙ cos(max(θ -
ρ, 0)) computes the similarity between f and the cluster centered at P with margin ρ. As illustrated
in Fig. 2c, PrivacyFace aims to learn a globally consistent embedding that can not only classify local
classes, but also achieve consensuses with other clients on the incompatible clusters p.
Server-side. Similar to other FL approaches, a central server orchestrates the training process and
receives the contributions of all clients to the new feature extractor at line 8. The well-known Fe-
dAvg (McMahan et al., 2017a) is then utilized to compute an average of all local models. Compared
to the conventional FL method as described in Fig. 1a, PrivacyFace empirically achieves better con-
vergence thanks to the consensus-aware loss that implicitly takes the data distribution of other clients
into account. Built on the client-wise DPLC algorithm, the framework is potentially compatible with
other optimizer, e.g., FedSGD (Shokri & Shmatikov, 2015) as shown in Sec. A.2.1 of appendix.
1
2
3
4
5
6
7
8
9
Algorithm 2: The PrivacyFace Training Scheme.
Data: Dcs exclusively owned by each of the C clients; A pre-trained extractor φo.
Parameters : DPLC-related ρ, T, Q; Privacy budget , δ; Maximum #communications M .
Result: A general recognition model φM.
for t = 1,…，M do
for each CIient C = 1,…，C do
Synchronize the local extractor φ(C-I = φt-ι as the up-to-date one in server;
Generate privacy-agnostic Pc = {P1, P2,…，pqc} via Algorithm 1 ;
The server gathers Pc and distributes P = {P1, P2,…，PC} to all clients;
for each CIient C = 1,…，C do
Update local model {φc-1, Wt-1} to {φc, W(c} by optimizing the loss (Eq. 5);
Communicate the feature extractor φc to the server while keeping Wc locally;
The server updates the model by φt = CC PC=I φc;
4	Experiments
This section together with the appendix describes extensive experiments on challenging benchmarks
to illustrate the superiority of PrivacyFace in training face recognition with privacy guarantee.
Datasets. CASIA-WebFace (Yi et al., 2014) contains 0.5M images from 10K celebrities and serves
as the dataset for pre-training. BUPT-Balancedface (Wang & Deng, 2020), which comprises four
7
Published as a conference paper at ICLR 2022
⑶ € = 1,Q = 1
(b) ρ = 1.3, Q = 1
(C)P = 1.3, Q = 3
Figure 4: Distributions of cosine similarities of p and P under different parameters With 1000 runs.
sub-datasets categorized by racial labels (including African, Asian, Caucasian and Indian) and each
sub-dataset contains 7K classes and 0.3M images, is used to simulate the federated setting. We adopt
RFW (Wang et al., 2019), IJB-B (Whitelam et al., 2017) and IJB-C (Maze et al., 2018) for evaluation.
RFW is proposed to study racial bias and shares the same racial taxonomy as BUPT-Balancedface.
IJB-B and IJB-C are challenging ones, containing 1.8K and 3.5K subjects respectively from large-
volume in-the-Wild images/videos. All images are aligned to 112 × 112 based on five landmarks.
Training. We assign one client for each of the four sub-datasets and use the perfect federated
setting (i.e., no client goes offline during training). For the pre-trained model φ0, We adopt an open-
source one2 trained on CASIA-WebFace, Which builds on ResNet18 and extracts 512-d features. We
finetune φ0 by SGD for M = 10 communication rounds on BUPT-Balancedface, With learning rate
0.001, batch size 512 and Weight decay 5e-4. For reproducibility and fair comparison, all models
are trained on 8 1080Ti GPUs With a fixed seed. To alleviate the large domain gaps across sub-
datasets, We build a lightWeight public dataset With the first 100 classes from CASIA-WebFace to
finetune local models before gathered by the server. Unless stated otherWise, the parameters for
PrivacyFace are default to T = 512, Q = 1, ρ = 1.3 and = 1. As the parameter δ is required to be
O( |DD|) (DWOrk & Roth, 2014), we set δ =局口 ≈ 5e-5 in BUPT-Balancedface.
Baselines. In the absence of related methods, We compare PrivacyFace With or Without Gaussian
noise added, and against a conventional FL method as indicated in Fig. 1a. We denote these methods
as φ + p^, φ + P and φ respectively, based on communicated elements among the server and clients.
Besides, We implement centralized training on the global version of BUPT-Balancedface and denote
the trained model as “global training”. The model is also finetuned from φ0 for 10 epochs With
learning rate of 0.001 and serves as an upper bound in all experiments.
Privacy-Utility Trade-Offs of the DPLC Algorithm. The goal of DPLC is to broadcast discrim-
inative yet privacy-preserving knoWledge through a federated netWork among clients. This part
investigates its effectiveness in terms of similarities betWeen the noise-free center P and the per-
turbed one P by the Gaussian Mechanism with respect to different parameters. We first derive the
class centers W from the Caucasian sub-dataset of BUPT-Balancedface by φ0. Taking W as input,
DPLC generates 1000 p0s for each p, yielding the distribution of cosine similarities shown in Fig. 4.
As a common sense, two face features holding cosine similarity over 0.7 are recognized as from the
same identity with a high probability. Fig. 4a reveals that ρ > 1.2 can always lead to accurate P
with e = 1 and Q = 1. If {ρ, Q} = {1.3,1}, we conclude that p, P are similar with privacy cost e
over 0.3, as indicated by Fig. 4b. Although Q = 1 leads to good recognition performance, Fig. 4c
studies the effect when more queries are required (Q = 3) at different cost e. At ρ = 1.3, we can
generate three clusters with descending sizes (1773, 743, 581) w.r.t the query index. By choosing a
strict privacy cost e = 0.33, queries 2 and 3 are too noisy to carry useful information. Setting e = 1
to a reasonable privacy level, however, all queries will convey accurate descriptions of features.
Ablation Studies on Hyper-parameters. We conduct several ablation studies on PrivacyFace and
present recognition performances on IJB-C in Fig. 5. Models are trained by ArcFace with default
parameters if not specifically stated. In Fig. 5a, poor recognition performance and unstable training
process can be observed for conventional FL method φ mainly due to the insufficient number of
local classes and inconsistent stationary points achieved by different clients. These drawbacks can
be significantly relieved by PrivacyFace φ + p. Fig. 5b reveals that performances of PrivacyFace
improves as privacy cost e increases. The performance is nearly saturated when e > 0.3, closing to
2https://github.com/IrvingMeng/MagFace.
8
Published as a conference paper at ICLR 2022
the method φ + P without privacy protection. This end-to-end evidence indicates DPLC achieves
high privacy-utility trade-offs as analyzed in Fig. 4b. The effect of cluster margin ρ is explored in
Fig. 5c and the optimal ρ is around 1.3. A small ρ leads to over-fined clusters with inadequate class
centers, adding requirements to increase noise and hurt recognition. Alternatively, the performance
would drop by adopting a large ρ which generates trivial clusters with high occupancy ratio.
(a) = 1, ρ = 1.3
(b) ρ = 1.3, varies
Figure 5: Effects of hyper-parameters on PrivacyFace performances.
【％)u.m≡ u。Q，«TUMVIL@MV.L
71"69686766
1.20 1.25 1.30 1.35 1.40
Cluster margin
15
(c) = 1, ρ varies
Performances on Benchmarks. Tab. 1 presents verification performances on various benchmarks.
By finetuning on BUPT-Balancedface, φ, φ+p^ and φ+p all achieve performance boosts. Compared
to the conventional FL method (φ), performances of our PrivacyFace (φ+P) are consistently higher
regardless of which training loss used. Specifically, improvements on TAR@FAR=1e-4 on IJB-B
and IJB-C are 3.54% and 3.56% with CosFace, and 9.63% and 10.26% with ArcFace, which are
significant and demonstrate the superiority of our method. We also observe that φ + P and φ + P
achieve very close results in all benchmarks. That implies the robustness of the proposed method.
Additional experiments can be found in appendix. We implement FedSGD (Shokri & Shmatikov,
2015) in Sec. A.2.1 to show the scalability of the PrivacyFace. Sec. A.2.2 compares the DPLC with
a naive approach while Sec. A.2.3 discusses the necessity of FL methods in federated setting. In
Sec. A.2.4, we analyze experimental attacks to further verify privacy guarantees of our framework.
Table 1: Verification performances (%) on various benchmarks.
Loss	Method	RFW					IJB-B AR@FAR=1e-4	IJB-C TAR@FAR=1e-4
		African Asian		IaUCaSian	Indian		
-	φ0	81.08	82.13	89.13	86.55	5.94	8.79
	φ	83.40	83.38	89.62	87.23	68.09	70:16
CosFace	φ + P	83.50	83.38	89.93	87.28	71.63 (+3.54)	73.72 (+3.56)
	φ + P	83.50	83.47	89.95	87.27	71.62	73.73
	Global Training	86.30	84.58	91.48	88.92	77.35	83.20
	φ	83.50	83.08	90.26	87.32	58:62	60:98
ArcFace	φ + P	83.80	83.08	90.32	87.38	68.25 (+9.63)	71.24 (+10.26)
	φ + P	83.82	82.97	90.32	87.38	68.57	71.66
	Global Training	87.32	84.55	9203	8890	71.26	79.74
Cost Analysis. PrivacyFace introduces little computational cost thanks to the efficient DPLC algo-
rithm as well as the consensus-aware loss. Apart from the backbone (over 200M) to distribute as
in the conventional FL, the extra variables to communicate are p's, which only occupy about 16K
storage. Thus, additional communication cost is negligible. Moreover, the total privacy cost is still
ofa low level with number M = 10 (i.e., communication rounds times the cost for each round).
5	Conclusions
With the carefully designed DPLC algorithm and a novel consensus-aware recognition loss, we im-
prove federated learning performances on face recognition by communicating auxiliary embedding
centers among clients, while achieving rigorous differential privacy. The framework runs efficiently
with lightweight communication/computational overheads. Besides, PrivacyFace can be potentially
extended to other metric-learning tasks such as re-identification and image retrieval. In the future,
more efforts can be spent on designing a more accurate clustering algorithm in conjunction with the
FL optimization, e.g., adaptive querying on the confusion areas instead of a brute-force sampling.
9
Published as a conference paper at ICLR 2022
6	Reproducibility Statement
Sec. 3.1 provides key proofs as well as properties of the proposed method. Additional proofs are
described in Sec. A.1 of the appendix.
The involved training/test datasets and training configurations are detailed in Sec. 4 when using
FedAvg scheme. Similar settings are applied for FedSGD in Sec. A.2.1 in appendix. Sec. A.2.4 of
the appendix shows visualizations for two potential attacks: K-nearest neighbor attack and inversion
attack. We as well present involved datasets, network structures, training losses as well as training
schedules for these attacks. Those are sufficient for reproducibility.
References
California consumer privacy act. https://oag.ca.gov/privacy/ccpa.
Illinois personal information protection act. https://bit.ly/3cPJeR1.
Divyansh Aggarwal, Jiayu Zhou, and Anil K Jain. FedFace: Collaborative learning of face recogni-
tion model. arXiv Preprint arXiv:2104.03008, 2021.
Fan Bai, Jiaxiang Wu, Pengcheng Shen, Shaoxin Li, and Shuigeng Zhou. Federated face recognition.
arXiv preprint arXiv:2105.02501, 2021.
VishnU Naresh Boddeti. Secure face matching using fully homomorphic encryption. In IEEE
Conference on CompUter Vision and Pattern Recognition, pp. 1-10. IEEE, 20l8.
George EP Box, William H Hunter, and Stuart Hunter. StatiStiCS for experimenters, volume 664.
John Wiley and sons New York, 1978.
Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning a similarity metric discriminatively, with
application to face verification. In IEEE COnferenCe on COmpUter ViSiOn and Pattern ReCOgnition,
volume 1,pp. 539-546. IEEE, 2005.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale
hierarchical image database. In IEEE COnferenCe on COmpUter ViSiOn and Pattern ReCOgnition,
pp. 248-255, 2009.
Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. ArcFace: Additive angular margin
loss for deep face recognition. In IEEE COnferenCe on COmpUter ViSiOn and Pattem ReCOgnition,
pp. 4690T699, 2019.
Mihai Dusmanu, Johannes L Schonberger, Sudipta N Sinha, and Marc Pollefeys. Privacy-preserving
image features via adversarial affine subspace embeddings. In IEEE COnference on COmpUter
ViSiOn and Pattern ReCOgnition, pp. 14267-14277, 2021.
Cynthia Dwork and Jing Lei. Differential privacy and robust statistics. In Proceedings of the
forty-first annual ACM SympOSiUm on TheOry of computing, pp. 371-380, 2009.
Cynthia Dwork and Aarons Roth. The algorithmic foundations of differential privacy. FOUndatiOnS
and TrendS in Theoretical COmpUter Science, 9(3-4):21—07, 2014.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity
in private data analysis. In TheOry of CryptOgraphy conference, pp. 265-284. Springer, 2006.
Dan Feldman, Chongyuan Xiang, Ruihao Zhu, and Daniela Rus. Coresets for differentially pri-
vate k-means clustering and applications to privacy in mobile sensor networks. In ACM/IEEE
International COnferenCe on InfOrmatiOn PrOCeSSing in SenSOr NetWOrkS (IPSN), pp. 3-16. IEEE,
2017.
Robin C Geyer, Tassilo Klein, and Moin Nabi. Differentially private federated learning: A client
level perspective. arXiv preprint arXiv:1712.07557, 2017.
10
Published as a conference paper at ICLR 2022
Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. MS-Celeb-1M: A dataset
and benchmark for large-scale face recognition. In EUropean Conference on CompUter Vision,
pp. 87-102. Springer, 2016.
Peter Kairouz, H Brendan McMahan, Brendan Avent, AUreIien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, and Rachel CUmmings. Advances
and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.
ArUn KUmar Jindal, Srinivas Chalamala, and Santosh KUmar Jami. Face template protection Us-
ing deep convolutional neural network. In IEEE ConferenCe on CompUter ViSion and Pattern
Recognition, pp. 462T70, 2018.
Shengqiao Li. Concise formulas for the area and volume of a hyperspherical cap. ASian JOUrnaI of
MathematiCS and StatiStics, 4(1):66-70, 2011.
Chih-Ting Liu, Chien-Yi Wang, Shao-Yi Chien, and Shang-Hong Lai. FedFR: Joint opti-
mization federated framework for generic and personalized face recognition. arXiv preprint
arXiv:2112.12496, 2021.
Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep hy-
persphere embedding for face recognition. In IEEE COnferenCe on COmpUter ViSiOn and Pattern
Recognition, pp. 212-220, 2017.
Guangcan Mai, Kai Cao, Xiangyuan Lan, and Pong C Yuen. SecureFace: Face template protection.
IEEE TranSaCtiOnS on InfOrmatiOn FOrenSiCS and Security, 16:262-277, 2020.
Brianna Maze, Jocelyn Adams, James A Duncan, Nathan Kalka, Tim Miller, Charles Otto, Anil K
Jain, W Tyler Niggel, Janet Anderson, and Jordan Cheney. IARPA Janus benchmark-C: Face
dataset and protocol. In International COnferenCe on Biometrics, pp. 158-165. IEEE, 2018.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Ar-
cas. Communication-efficient learning of deep networks from decentralized data. In ArtifiCiaI
Intelligence and StatiStics, pp. 1273-1282, 2017a.
H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private
recurrent language models. arXiv preprint arXiv:1710.06963, 2017b.
Qiang Meng, Xiaqing Xu, Xiaobo Wang, Yang Qian, Yunxiao Qin, Zezheng Wang, Chenxu Zhao,
Feng Zhou, and Zhen Lei. Poseface: Pose-invariant features and pose-adaptive loss for face
recognition. arXiv preprint arXiv:2107.11721, 2021a.
Qiang Meng, Chixiang Zhang, Xiaoqiang Xu, and Feng Zhou. Learning compatible embeddings.
In International COnferenCe on COmpUter ViSion, 2021b.
Qiang Meng, Shichao Zhao, Zhida Huang, and Feng Zhou. MagFace: A universal representation
for face recognition and quality assessment. In IEEE COnferenCe on COmpUter ViSiOn and Pattern
Recognition, 2021c.
Kobbi Nissim, Uri Stemmer, and Salil Vadhan. Locating a small cluster privately. In ACM
SIGMOD-SIGACT-SIGAI SympOSiUm on PrinCipIeS OfDatabaSe Systems, pp. 413U27, 2016.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Rajeev Ranjan, Carlos D Castillo, and Rama Chellappa. L2-constrained softmax loss for discrimi-
native face verification. arXiv preprint arXiv:1703.09507, 2017.
Florian Schroff, Dmitry Kalenichenko, and James Philbin. FaceNet: A unified embedding for face
recognition and clustering. In IEEE COnferenCe on COmpUter ViSiOn and Pattern Recognition, pp.
815-823,2015.
Yantao Shen, Yuanjun Xiong, Wei Xia, and Stefano Soatto. Towards backward-compatible repre-
Sentation learning. In IEEE COnferenCe on COmpUter ViSiOn and Pattern Recognition, pp. 6368-
6377, 2020.
11
Published as a conference paper at ICLR 2022
Reza Shokri and Vitaly Shmatikov. Privacy-preserving deep learning. In Proceedings of the 22nd
ACM SIGSAC Conference on CompUter and CommUnications security, pp. 1310-1321, 2015.
KihyUk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In AnnuaI
ConferenCe on NeUraI Information ProCessing Systems, pp. 1857-1865, 2016.
Uri Stemmer and Haim Kaplan. Differentially private k-means with constant multiplicative error. In
AnnUaI COnference on NeUraI InfOrmatiOn PrOCeSSing Systems, 2018.
Stacey Truex, Nathalie Baracaldo, Ali Anwar, Thomas Steinke, Heiko Ludwig, Rui Zhang, and
Yi ZhoU. A hybrid approach to privacy-preserving federated learning. In PrOCeedingS of the 12th
ACM WOrkShOP on ArtifiCiaI Intelligence and SeCUrity, pp. 1-11, 2019.
PaUl Voigt and Axel Von dem BUssche. The EU general data protection regUlation (GDPR). A
PraCtiCaI GUide, 1st Ed., Cham: SPringer International PUblishing, 10:3152676, 2017.
Hao Wang, Yitong Wang, Zheng ZhoU, Xing Ji, Dihong Gong, Jingchao ZhoU, Zhifeng Li, and
Wei LiU. CosFace: Large margin cosine loss for deep face recognition. In IEEE COnference on
COmPUter ViSiOn and Pattern Recognition, pp. 5265-5274, 2018.
Mei Wang and Weihong Deng. Mitigating bias in face recognition Using skewness-aware reinforce-
ment learning. In IEEE COnferenCe on COmPUter ViSiOn and Pattern Recognition, pp. 9322-9331,
2020.
Mei Wang, Weihong Deng, Jiani HU, XUnqiang Tao, and Yaohai HUang. Racial faces in the wild:
RedUcing racial bias by information maximization adaptation network. In IEEE COnferenCe on
COmPUter ViSiOn and Pattern Recognition, pp. 692-702, 2019.
Cameron Whitelam, Emma Taborsky, AUstin Blanton, Brianna Maze, Jocelyn Adams, Tim Miller,
Nathan Kalka, Anil K Jain, James A DUncan, and Kristen Allen. IARPA JanUs benchmark-B face
dataset. In IEEE COnferenCe on COmPUter ViSiOn and Pattern ReCOgnitiOn WOrkShop, pp. 90-98,
2017.
Bin Xiao, Jiannong Cao, Qingfeng ZhUge, Yi He, and Edwin H.-M Sha. Approximation algo-
rithms design for disk partial covering problem. In 7th International SymPOSiUm on Parallel
ArChiteCtUres, Algorithms and Networks, pp. 104-109, 2004.
Xiaqing XU, Qiang Meng, YUnxiao Qin, JianzhU GUo, ChenxU Zhao, Feng ZhoU, and Zhen Lei.
Searching for alignment in face recognition. In AAAI COnferenCe on ArtifiCiaI Intelligence,
volUme 35, pp. 3065-3073, 2021.
KaiyU Yang, JacqUeline YaU, Li Fei-Fei, Jia Deng, and Olga RUssakovsky. A stUdy of face obfUsca-
tion in ImageNet. arXiv PrePrint arXiv:2103.06191, 2021.
Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. Learning face representation from scratch. arXiv
PrePrintarXiV:1411.7923, 2014.
12
Published as a conference paper at ICLR 2022
A Appendix
A. 1 Mathematical Proofs
In this section, we present extra proofs for PrivacyFace.
A.1.1 Proof for Theorem 1
Proof. A spherical cap is a portion of a d-dimensional sphere with radius r cut off by a plane. As
shown in (Li, 2011), the surface area of a spherical cap is
A = 2Adrd-II(2rh-h2)∕r2 (-2-, 2
Here h is the height of the cap, and A, Ad are the surface area of the spherical cap and the sphere,
respectively. Because the set {f : arccos(f T p) ≤ ρ, f ∈ Sd} is the spherical cap with height
h = r ∙ (1 - cos P) and the radius is 1 in our problem, it can be derived that the occupancy ratio is
A 1	d-1	d - 1 1
A^ 二 2Adr	I(2r2∙(1-cos ρ)-r2∙(l-cos ρ)2)∕r2 I —%—,~2
=1 Ad IrdTl (2-3W -1-cos2 p+jic&Sp ) ( - 2- , 2)
=11	(d - 1 1 ʌ
=2Isin2(ρ) (	2	, 2	.
□
A.1.2 A weak version of Theorem 2
In this part, we present a straightfward but weak version of the Theorem 2:
Theorem 3. Define a function P，f (S)=看 f∈ Wi. Then the Gaussian Mechanism P
M(S) =啬 pi∈s Wi + N(O, σ2Id) is (e, δ)-dp ifσ ≥ 扇 q(I-COSP) ln( 1δ5).
Proof. S stores indexes of class centers which have cosine similarities larger than cosP with a center
(denoted as Wo in this proof). Assuming that S , S0 are neighbors differed at w, w0 , then
................. 1 ..	,..	1 ...	.......... ...	2 I--------------
kf (S) TS )k2 =网kw -W k2 ≤ 网(kw - wok2 + kw - wok2) ≤ |S| P 2cosP ⑹
According to Definition 3, we conclude that M(S)= 看 Pi∈s Wi + N(O, σ2Id) is (e, δ)-DP if
σ ≥ ∆f q2ln( 1δ5)=昂 q(1 - COS P)ln( ɪf).	□
This lower bound of σ is
号^ J(I - cos ρ)ln( 1δ25), which is larger than our final bound because
(导 J(I- CosP)ln(125))2 - (∕∖l(1- cos(2ρ))ln(⅛5))2
|S| ∙ € V	δ	|S| ∙ € V	δ
二∣s∣4∙ €2 ln(1∣5) ∙ (4 - 4cosP - 1 + cos(2ρ))
=∣g∣4 2 ln(125) ∙ (3 - 4cos P + 2cos2 P - 1)
∣S∣2 ∙ €2	δ
=∣S∣4∙ €2 ln(1∣5) ∙(2 - 2cosP)2 ≥ 0.
A tighter bound can reduce the privacy cost with same level of noise added. For example, the privacy
cost in our experiments can be dropped by around 21% if using P = 1.3 (from = 1 to = O.793).
13
Published as a conference paper at ICLR 2022
A.1.3 Proof for Theorem 1
Lemma 1. For any x, y, z ∈ Rd, the following inequality holds:
(XTXzTZ - XTZxTZ) ∙ (yτyzTZ - yτZyTZ) ≥ (XTZzTy - xτyzTz)2.
Proof. Let X = (χ1,χ2,…，xd), y = (y1,y2,…，yd), Z = (z1,z2,…，zd). Then
XTXZTZ - XTZXTZ	dd	d =X χ2∙X Z -(X XiZi)2 i=1	j=1	i=1 dd	d	d XXXi2Zj2-XXiZi ∙ XXjZj i=1 j=1	i=1	j=1 dd	dd XX Xi2Zj2-XX XiZiXjZj i=1 j=1	i=1 j=1 dd (Xi2Zj2 - XiZiXjZj) i=1 j=1 1d d 二 5 £ ∑2(x2Z2 + X2Z2 - 2xiZiXjZj) i=1 j=1 1d d 二 512£(XiZj - XjZi) i=1 j=1
Similarly, it's easy to prove yTyZTZ - yTZyTZ = ɪ Pid=i Pd=I(yiZj - y7-Zi)2. In addition, We
have
XTZZTy - XTyZTZ =	dd	dd =ΣXiZi ∙	Zjyj-	Xiyi ∙	ZjZj i=1	j=1	i=1	j=1 dd (XiZiZjyj - XiyiZjZj) 1d d =2 E E(XiZiZj yj - Xiyi Zj Zj + Xj Zj Ziyi - Xjyj ZiZi) i=1 j=1 dd 4∑∑(XiZj . (Ziyj - UiZj) + XjZi ∙ (Zjyi - yjZi)) i=1 j=1 dd 4∑∑(XiZj - XjZi)(Ziyj - yiZj ) i=1 j=1
Then We can deduce that
dd	dd
T T	T T	T T	T T	1	21	2
(X Xz Z - X Zx z) ∙ (y yz Z - y Zy z) = 2 2^2^(xiZj - XjZi) 2 工工gj - UjZi)
i=1 j=1	i=1 j=1
≥
1 dd	2
4 ∑∑(χizj - xjZi)(Ziyj - yiZj)
i=1 j=1
(XT ZZT y - XTyZTZ)2
Here the second step is the CaUchy-Schwarz inequality.
□
T	T0
Lemma 2. Assume that w, w0, Wo	∈ Rd and T——∣or~= CoS α, T——∣oη~= CoS β, 0	≤ α,β	≤
,	, o	kwok2kwk2	, kwok2kw0k2	,	,
T0
∏. Then 口d2总|[2 ≥ cos(α + β) always holds.
14
Published as a conference paper at ICLR 2022
T0
Proof. Let 口小2苣［2 = CosY∙ We thereby need to prove cosY ≥ cos(α + β) = cosacosβ -
sin α sin β = cos α cos β -	(1 - cos1 2 α)(1 - cos2 β ), which is equivalent to
ʌ/(l - cos2 α)(1 — cos2 β) ≥ cos α cos β — cos γ.
If cos α cos β ≤ cos γ, the inequality equation 7 holds.
If cos α cos β > cos γ , then
(1 - cos2 α)(1 - cos2 β) ≥ (cos α cos β - cosγ)2
T	T0	T	T0
^⇒ (1 - (--o----)2)(1 - (-o----)2) ≥ (--o--------o----
((kWok2kwk2))(	( kWok2kw0k2) ) ≥( 1匹13阿2 1吐口2 1凹2
(7)
WT W0	)2
kwMkw'M
—
^⇒ ((IlWo∣∣2∣∣w∣∣2)2 — (wTw)2)(kwok2kw0k2)2 — (wTW0)2) ≥ ((WTW) ∙ (wTW0) — WTW0∣∣Wok2)2
V⇒ (WTWWTWo — WTWoWTWo) ∙ (w'TW0WTWo — W°TW°W0TWo) ≥ (WTWoWTW — WTWOWTWo)2.
The last step is correct according to Lemma 1.
□
A.2 Extra Experiments
A.2.1 PrivacyFace with FedSGD
Algorithm 2 is mainly built on FedAvg (McMahan et al., 2017a) where model parameters are com-
municated between clients and the server. In this part, we show that our PrivacyFace is also compat-
ible with another popular FL method called FedSGD (Shokri & Shmatikov, 2015). FedSGD mainly
differs FedAvg in two aspects: (1) During training locally, the model is frozen to ensure gradients
across mini-batches are from a same model. (2) Gradients rather than model parameters are aggre-
gated and distributed among clients and the server. Apart from changing FedAvg to FedSGD, we let
other training setting consistent with those described before and present the results in Tab. 2. Com-
pared to the conventional FL method φ, performances of our PriVacyFace φ + P are close to those
on RFW and noteworthily higher in IJB-B/IJB-C benchmarks. Besides, φ + P and φ + P achieve
very close results. All these phenomenons are in accord with those observed when using FedAvg,
which shows the scalability of the proposed PrivacyFace.
However, FedSGD does not achieve that significant improvements as FedAvg. A possible reason is
that FedSGD naturally requires more communication rounds than FedAvg to achieve comparable re-
sults, which may be caused by low-frequent updates during training. Experiments in McMahan et al.
(2017a) showed that FedSGD requires about 23 times more communication rounds to achieve sim-
ilar performances with FedAvg. With such a fact, the improvements using FedSGD in PrivacyFace
is still remarkable.
Table 2: Verification performances (%) on various benchmarks with FedSGD and ArcFace.
Method	RFW				IJB-B AR@FAR=1e-4	IJB-C TAR@FAR=1e-4
	African Asian Caucasian Indian「					
φ0	81.08	82.13	89.13	86.55	5.94	8.79
φ	81.40	82.38	89.08	86.68	63:24	68:35
φ + P	81.40	82.35	89.12	86.77	63.72(+0.49)	68.73(+0.38)
φ + P	81.40	82.35	89.12	86.77	63.72	68.73
Global Training	87.32	84.55	92.03	8890	71.26	79.74
A.2.2 Comparison with a Naive DP Approach
A naive alternative to DPLC is to directly make each class center of W to be differential private by
adding noise. However, that can lead to a large privacy cost due to two reasons:
1. With the same noise added, the privacy cost is propotional to the l2-sensitivity based on
definition 2. The l2-sensitivity of the naive approach is 2 as maxinum distances between
two class centers distributed in a unit sphere are 2. In contrast, the l2-sensitivity DPLC is
|SS|，2 - 2cos(2ρ). In our experiment, we let |S| > 512 and P = 1.4, which makes the
l2-sensitivity to be smaller than 0.0024.
15
Published as a conference paper at ICLR 2022
2. The training dataset are of a large number of identities. According to the composition
rule (Definition 3), the privacy cost grows linearly with respect to the queries (i.e., number
of identities). In DPLC, low privacy cost as the clustering mechanism highly reduce the
number of queries.
During training, the number of query is 1. Therefore, the privacy cost in an local client (with 7000
classes) of DPLC is ⅛⅛024 ≈ 1.7e - 7 of that of the naive approach.
We experimentally compare the performances of our method and the naive approach. For fair com-
parisons, the privacy cost in each round is fixed to be 1 and all training settings are consistent with
those in the main text. Tab. 3 are the results, where the φ + W represents the naive approach. It can
be seen that φ + W can achieve similar results on RFW benchmark as learning local discriminative
features with introduced fixed class centers are relatively easy (for example, if introduced class cen-
ter distributed in the south of the embedding space, placing local features to the north is a solution).
However, as the class centers from other clients are very noisy, the global embedding space will
become a mess. φ + W even achieves much worse results than the conventional FL methods φ on
the challenging benchmarks. The tar decreases 23.99% on IJB-B and 25.16% on IJB-C when far is
1e-4. In contrast, PrivacyFace increases the performances by 9.63% and 10.26% respectively, which
shows the superiority of our framework.
Table 3: Verification performances (%) on various benchmarks.					
Method	RFW	IJB-B African Asian Caucasian Indian TAR@FAR=Ie-4				IJB-C TAR@FAR=1e-4
φ	83.50	83.08	90.26	87.32	58.62	60.98
φ + W	84.03	83.58	90.33	87.26	34.63 (-23.99)	35.82 (-25.16)
φ + P	83.80	83.08	90.32	87.38	68.25 (+9.63)	71.24 (+10.26)
A.2.3 Necessity of Federated Learning
Besides using federated learning, one can also choose to finetune client-expert models on local
datasets. We also implement this approach by training four independent models on sub-datasets of
BUPT-Balancedface and test their performances on benchmarks. The training procedure is consis-
tent with what we described before in Sec. 4. Specifically, we finetune each model from φ0 by
ArcFace for 10 epochs with learning rate 0.001, batch size 512 and weight decay 5e-4.
We present our results in Tab. 4, where the “African/Asian/Caucasian/Indian” in the first column
means the model trained on the corresponding sub-dataset of BUPT-Balancedface. Models finetuned
on African and Caucasian sub-dataset achieve satisfying results on local benchmarks while those on
Asian and Indian have even worse performances than the initial model φ0. Moreover, all these local
models perform poorly on the IJB-B and IJB-C benchmarks, which indicates the lack of generality.
To conclude, local training highly relies on the properties and qualities of local datasets, and can
not lead to general models. In contrast, models trained by conventional FL method and PrivacyFace
can achieve competitive performances on all benchmarks, and therefore are more robust to various
scenarios.
Table 4: Verification performances (%) on various benchmarks.
Method	RFW				IJB-B R@FAR=1e-4	IJB-C TAR@FAR=1e-4
	African Asian Caucasian Indian IA					
φ0	81.08	82.13	89.13	86.55	5.94	8.79
African	87.53	-	-	-	8.56	11.26
Asian	-	64.08	-	-	0.30	0.40
Caucasian	-	-	91.87	-	53.51	57.89
Indian	-	-	-	83.62	39.01	39.62
Conventional FL	83.50	83.08	90.26	87.32^^	58.62	60.98
PrivacyFace	83.80	83.08	90.32	87.38	68.25	71.24
A.2.4 Visualization
In this section, we experimentally examine the privacy attacks to demonstrate that class centers lead
to privacy leakage while our sanitized clusters are resistent to these attacks. We mainly consider two
types of attacks: K-nearest neighbor attack and reversion attack.
16
Published as a conference paper at ICLR 2022
K-Nearest Neighbor Attack We assume that an attacker owns a gallery of faces with enormous
identities and attacks the exposed class centers by finding their neighbors. To simulate the K-nearest
neighbor attack, class centers are extracted on the BUPT-Balancedface dataset and three faces are
sampled from each identity on a large dataset called MS1M-V2 (Deng et al., 2019). Fig. 6 presents
the results. It can be observed that the K-nearest neighbor attack successfully find faces of the
corresponding identity from the class centers, which enables the attacker to know which identities
are contained in the training dataset. The observation further verifies the fact that sharing class
centers leads to privacy leakage in face recognition.
Figure 6: Real mean faces and the top-3 nearest faces of the corresponding mean class centers.
Reversion Attack We design a reconstruction network (modified from DCGAN (Radford et al.,
2015)) as described in Tab. 5 which projects a512 dimensional embedding to a face image. CASIA-
WebFace dataset is used as the training dataset. Specifically, We extract features from images by the
initial model φo. With supervision of features and images, we train the network by Adam optimizer
with L1 loss, which penalizes the absolute value of pixel differences between reconstructed and real
images. The learning rate is initialized from 0.001 and divided by 10 every 10 epochs, and we stop
the training at the 50th epoch. The weight decay is 5e - 4 and batch size is 512 during our training.
Table 5: Details of the reconstruction network. Note that all the convtranspose2d layers use stride 2 and kernel
size 4 X 4.
Layer type	input dim.	output dim.
fc + BN	-312	8192
reshape	8192	[512, 4, 4]
convtranspose2d + BN + ReLU	[512, 4, 4]	[256, 8, 8]
convtranspose2d + BN + ReLU	[256, 8, 8]	[128,16,16]
convtranspose2d + BN + ReLU	[128,16,16]	[64, 32, 32]
convtranspose2d + BN + ReLU	[64, 32, 32]	[32, 64, 64]
convtranspose2d + Sigmoid	[32, 64, 64]	[3, 128, 128]
We test our model on BUPT-Balancedface dataset and present reconstruction visualizations. In
each image pair of Fig. 7, the left one is the average of faces from one identity while the right
one is the reconstructed image from the corresponding class center. It can be seen that reconstructed
images reveal the real individual identity, which demonstrates that w carries human face privacy and
therefore should be protected. Fig. 8 shows the reconstructions from p and P under various privacy
cost . The left column is the images by p, which are much blurred than those from w and only the
group properties can be identified. For example, the first image seems like an African male while
17
Published as a conference paper at ICLR 2022
Figure 7: Real mean faces (first image in each pair) and faces reconstructed from class centers W (second image
in each pair).

P I e = 0.1	£ = 0.3	£ = 0.5	£ = 0.7	£ = 1.0	£ = 1.3	£ = 1.5 )
P
Figure 8: Reconstructed faces from cluster centers P and the differentially private ones P under differentprivacy
cost €.
the second last one are very likely to be Caucasian female. The output p^ from our DPLC algorithm
18
Published as a conference paper at ICLR 2022
can further protect privacy as one can hardly recognize a face from the generated images, especially
when is small. The hardness of a successful identification is negative related to according to the
visualization.
A.2.5 Comparisons with FedFace
FedFace (Aggarwal et al., 2021) is a recent work applying federated learning to face recognition.
In each communication round, the FedFace uploads all local class centers to the server and con-
sequently utilizes the spreadout regularizer to separate class centers from different clients. In this
section, we adjust FedFace to fit our federated setting in the main text. Specifically, in each commu-
nication round, each local client uploads its 7K class centers to the server. The spreadout regularizer
then computes distances of 588 million (7K × 7K × 3 × 4) pairs of class centers and back propa-
gates the model parameters once. The updated parameters are then distributed to each of the clients.
However, directly computing all distances consumes 11,962M GPU memory, which is out of the
capacity of our machine. Therefore, we sample 1K out of 7K class centers from each client for
spreadout regularizer in our machine. For hyper-parameters of the spreadout regularizer, the authors
recommend loss weight λ = 10 and margin v = 0.9. For fair comparisons, we further run 18
experiments with λ ∈ {0.5, 1, 1.5, 5, 10, 15} and v ∈ {0.3, 0.6, 0.9} and report the best results of
FedFace.
Table 6: Comparisons of FedFace and PrivacyFace.
Method	TAR@FAR=Ie-4 (%)		Privacy Cost	Additional cost per round		
	IJB-B	IJB-C		Communication	Computation	GPU memory
FedAvg	58.62^^	60.98	0	0	0	0
FedFace	59.40^^	63.25	∞	14,000 vectors	High	1525M
PrivacyFace	68.25	71.24	1	5 vectors	Low	< 1M
Tab. 6 summarizes the overall comparisons between FedFace and PrivacyFace. We observe that
•	Performance. FedFace only boosts the FedAvg by 0.82%, 2.27% on TAR@FAR=1e-4 for
IJB-B and IJB-C. In contrast, PrivacyFace achieves 9.63%, 10.26%, respectively. Besides
the sampling strategy, the main reason can be that FedFace only separates classes from
different clients for one time (the spreadout regularizer) in a communication round, while
the model is locally updated 0.3 million times (number of local images) without considering
the conflicts among clients. In contrast, our consensus-aware recogntion loss takes the
conflicts into account for each local update.
•	Privacy cost. In each round, FedFace uploads 7, 000 original class centers to the server.
Therefore, the privacy cost is 7000 × ∞ = ∞. For PrivacyFace, the number is only 1 from
the outputted noisy cluster center in the client.
•	Additional communication cost. For each client in communication round, FedFace addi-
tionally uploads 7, 000 512-d class centers to the server and sends back the updated class
centers. In PrivacyFace, a local client uploads one 512-d cluster centers to the server
and downloads 4 cluster centers from the server. The additional cost from FedFace is
7001+4000 = 2, 800 times of that from PrivacyFace.
•	Additional computational cost. The additional computations in FedFace is from the
spreadout regularizer, which calculates 588 million pairs of 512-d features and updates
the model parameters on the server side by one-time back propagation. That requires enor-
mous computation in each round. In contrast, only extra items in the consensus-aware loss
requires additional computation in PrivacyFace, which is of tiny volume.
•	Additional usage of GPU memory. Even though we sample 1, 000 out of 7, 000 clients
to reduce GPU usage of the spreadout regularizer, FedFace still consumes 1, 525M extra
GPU memories. One the other hand, the additional usage of PrivacyFace is negligible, as
shown in Tab. 6.
A.2.6 Ablation Studies on Initial Models
In this section, we examine the effects of different initial models on the proposed method. Besides
the φ0 we used before, we further adopt two intermediate checkpoints at epoch 20, 10 when training
on the CASIA-WebFace (Yi et al., 2014) and denote them as φ1, φ2, respectively. One additional
19
Published as a conference paper at ICLR 2022
model φ3 is trained on MS1M-V2 (Deng et al., 2019) for 10 epochs with learning rate 0.1 and the
ArcFace loss. Note that we do not follow the conventional training configuration where the learning
rate is decreased stepwisely until 0.001. That’s because such a configuration will lead to a strong
model that does not require finetuning on BUPT-BalancedFace (Wang & Deng, 2020).
With other training settings same as those in the main text, we finetune the models on BUPT-
BalancedFace (Wang & Deng, 2020) with the ArcFace loss. Tab. 7 presents the comparisons of plain
FedAvg and PrivacyFace. On the IJB-B/IJB-C benchmarks, PrivacyFace can consistently achieve
better results than the plain FedAvg. For TAR@FAR=1e-4 on IJB-C, PrivacyFace can boost the per-
formances by 10.26%, 12.03%, 3.43%, 22.00% with the four initial models. It can be observed that
improvements are affected by the initial model. For example, the poorest initial model φ2 achieves
the lowest improvements. The reason may be that the training loss are overwhelmed by separating
local classes as initial features are not discriminative enough. Consequently, increasing inter-client
distances is of the secondary importance to the training in this scenario. For the best initial model
φ3 , the performances on IJB-B/C decreases when using federated learning. This is because the
finetuning process also focuses on improving local performances as the global consensus is already
reached. In this senario, FedAvg decreases the TAR@FAR=1e-4 by more than 40% on the bench-
marks. In contrast, PrivacyFace can highly alleviate the performance degradations by taking global
consensus into account during training. Finetuing on BUPT-BalancedFace mainly benefits the per-
formances on the RFW benchmarks as these two datasets are both designed for racial bias. If the
initial model is good enough, the finetuning procedure will focus more on fitting the racial issue
instead of improving the general performances. This may be the reason why model φ3 leads to the
largest improvements.
Table 7: Verification performances (%) with different initial models.
Initial Model	Method	RFW					IJB-B TAR@FAR=1e-4	IJB-C TAR@FAR=1e-4
		African Asian		IaUcasian Indian '			
φ0	φ0 FedAvg	81.08 83.50	82.13 83.08	89.13 90.26	86.55 87.32	5.94 58:62	8.79 60:98
	PrivacyFace	83.80	83.08	90.32	87.38	68.25 (+9.63)	71.24 (+10.26)
φ1 -	φ1	81.37	82.13	89.22	86.75	2.04	256
	FedAvg	83.33	81.70	89.73	86.65	5070	53:97
	PrivacyFace	83.43	82.47	90.02	86.97	61.97 (+11.27)	66.00 (+12.03)
从C	φ2	76.72	77.90	84.90	82.11	1.72	1.63
φ2	FedAvg	78.01	77.62	85.03	82.78	35:83	36:17
	PrivacyFace	78.93	78.50	85.55	83.05	38.47 (+2.64)	39.6 (+3.43)
小C	φ3	83.65	84.45	89.08	87.55	6920	72.00
φ3	FedAvg	87.07	86.48	92.08	89.15	26:70	26:14
	PrivacyFace	87.60	86.80	92.37	89.85	48.10(+21.40)	48.14 (+22.00)
A.2.7 Effects of Client Disconnections
Previously we use a perfect federated setting where all clients are available during training. In real-
world applications, some clients may drop for certain rounds of communication due to network
issues. We further simulate the case by randomly select one of the four clients and exclude it from
the training for an offline probability in each communication round. For example, if we set the
offline probability to 100%, there will be exactly one client offline per round. Note here we only
block one client because the total number of clients is small. We keep other settings unchanged
and compare performances of PrivacyFace and plain FedAvg on the IJB-C benchmark with different
offline probability. We refer the offline policy as disconnecting patterns during training, and use the
same offline policy for each pair of experiments on PrivacyFace and FedAvg for fair purpose. To
reduce the randomness, we run each experiment with 3 different offline policies and report the mean
TAR@FAR=1e-4.
Fig. 9 presents our results. When the offline probability is 25% and 50%, PrivacyFace improves the
performances of federated learning by about 6%. The improvement decreases to 2.4% with 75%
offline probability. When each round witnesses one random client offline, the performances of two
methods are below 45% while the improvement brought by our approach is only 0.4%. That reveals
the performance boosts of PrivacyFace can be influenced by the network conditions.
20
Published as a conference paper at ICLR 2022
Figure 9: Effects of client disconnections during training.
A.3 DIFFERENCES WITH DIFFERENTIAL PRIVATE FEDERATED LEARNING
In PrivaCyFace, We utilize the differential privacy to improve the performance of conventional feder-
ated learning in face recognition. Our work basically distills differentially private information from
clients to achieve that purpose. We notice that there are plenty of recent works on differential private
federated learning (DP-FL) methods (Geyer et al., 2017; McMahan et al., 2017b; Truex et al., 2019)
and would like to emphasize the differences:
1.	PrivacyFace and current DP-FL methods aim to solve different privacy leakages. As dif-
ferent data points lead to different model updates, DP-FL methods essentially add noise to
model update to prevent data from being inspected. Recall the model update is essentially
the average of gradients, which has a small l2-sensitivity (definition 2) thanks to the tremen-
dous number of data points. Then the protection is relatively easy with the low privacy cost
(propotional to the l2-sensitivity). In contrast, PrivacyFace aims to protects class centers
in face recognition which are directly linked to individual privacy. The problem is much
harder as these class centers are not naturally averaged like model updates and therefore
leads to large privacy cost.
2.	The targets to protect is different. Current DP-FL methods protect the data points in each
local dataset. In contrast, PrivacyFace protects some of the model parameters (i.e., the
classifier weights), which is the first work in federated learning to our best knowledge.
3.	Current DP-FL methods theoretically decrease performances of federated learning as they
use noisy updates. However, PrivacyFace distill differentially private information from
each client. The extra information can broadcasted to improve the performances of feder-
ated learning face recognition.
4.	We can directly combine DP-FL method and PrivacyFace, which lead to a more secure
framework protecting both data points as well as class centers together.
21