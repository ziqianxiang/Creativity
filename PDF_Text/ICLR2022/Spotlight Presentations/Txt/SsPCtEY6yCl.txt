Published as a conference paper at ICLR 2022
On the Uncomputability of Partition Functions
In Energy-Based Sequence Models
Chu-Cheng Lin & Arya D. McCarthy
Center for Language and Speech Processing, Johns Hopkins University
Abstract
In this paper, we argue that energy-based sequence models backed by expressive
parametric families can result in uncomputable and inapproximable partition functions.
Among other things, this makes model selection — and therefore learning model
parameters — not only difficult, but generally undecidable. The reason is that there are
no good deterministic or randomized estimators of partition functions. Specifically,
we exhibit a pathological example where under common assumptions, no useful
importance sampling estimators of the partition function can guarantee to have variance
bounded below a rational number. As alternatives, we consider sequence model families
whose partition functions are computable (if they exist), but at the cost of reduced
expressiveness. Our theoretical results suggest that statistical procedures with asymptotic
guarantees and sheer (but finite) amounts of compute are not the only things that make
sequence modeling work; computability concerns must not be neglected as we consider
more expressive model parametrizations.
1	Introduction
Modeling discrete sequences is central to natural language processing and bioinformatics. Many common
parametric sequence models / are (or can be cast as) energy-based (LeCUn et al., 2006): they yield a weight
/(x) for any given string x. Although energy-based models (EBMS) of sequences need not represent
probability distributions, they have been proposed as such to counter the inexpressivity of autoregressive
sequence models (Bakhtin et al., 2021; Lin et al., 2021, §4.1). EBMs with finite partition functions
Z/，^x 歹(x) ∈ R>o define distributions P(x)，/(x)∕zp over strings, which are often queried under the
principle of probabilistic inference (Ghahramani, 2015).1
Energy-based sequence models are often parametrized as neural models. Each parameter vector θ in
some parametric neural network family Θ ⊆ Rd identifies a parametric model /仇 which then defines a
parametric distribution over strings po, assuming Z诩 exists (Chen et al., 2018). Contemporary neural
networks have been shown to be very powerful. In particular, some popular parametric neural sequence
model families, such as RNNs (Siegelmann & Sontag, 1995) and Transformers (Bhattamishra et al.,
2020; Perez et al., 2021), have been formally shown to recognize recursively enumerable languages:
given any Turing machine M, there is a parameter vector θ ∈ Θ such that the parametrized sequence
model NG recognizes the same language as M does. In other words, these sequence model families are
Turing-complete.
It is therefore intuitive that energy-based sequence models, backed by such powerful neural networks, form
expressive families of string distributions (§3.1): for any decision algorithm M, there is a parameter vector
θM in a Turing-complete parametric family Θ, such that PGM exists, and PGM (x) is high if and only if M
accepts x. It would seem assuring to work with such expressive family of sequence distributions, Θ:
assuming the true string probabilities indeed can be computed in polytime, Θ is well-specified. Moreover,
one may assume that given we can sample from PGM , we would be able to use consistent estimators to find
θ 0 ∈ Θ, where θ 0 ≈ θM.
Unfortunately, we find that with such an expressive distribution family Θ, whether the identifiability
assumption holds — required for most consistent estimators — itself is undecidable (Turing, 1937).
iMany popular energy-based sequence models compute normalized probabilities directly (i.e., P(x) = P(x))
(Jelinek, 1980; Katz, 1987; Bengio et al., 2003; Brown et al., 2020, inter alia). This makes both training and querying
with string prefixes much easier, at the cost of expressiveness (Lin et al., 2021).
1
Published as a conference paper at ICLR 2022
Moreover, model selection on any held-out data is also undecidable. Even worse, we show that there exists
a vector θ0 ∈ Θ, such that as long as a parametric family Θ0 ⊆ Θ contains θ0, model selection will not be
possible for Θ0 either ——even when Θ0 itself is not necessarily expressive (e.g., Θ0 can be fixed-size
Transformer EBMs, which cannot parametrize all EBMs that require more parameters). We construct one
such 'pathological’ distribution p/ as a Transformer EBM.
These negative results stem from the uncomputability and inapproximability of Z. A main technical
contribution of this paper is that there is no algorithm (either deterministic or randomized) that can
approximate Z well. An immediate consequence is that sampling-based statistical procedures are not useful
in this scenario, either, as long as they terminate in finite time. However, we will see that for less expressive
model families, such uncomputability issues do not arise. Our negative results are summarized in Table A.1.
To ensure that model selection is still possible (such that we can compare different parameter vectors with
confidence),2 we have no choice but to resort to less expressive string distributions.
The paper is structured as follows. In §2 we review definitions and known results of weighted languages,
sequence model families, and formalize weighted Turing machines, EC-complete parametric families, and
computable estimators. In §§3-5 we describe our main technical results: there exist pathological EBM
sequence models that have uncomputable partition functions, which cannot be approximated well under
randomized estimators, and do not have asymptotic estimators that have any good guarantees. In §6 we that
argue our negative results make model selection impossible for expressive model families, and discuss why
common estimation methods fail. Finally, we discuss three ‘palliative’ parametrization choices in §7, which
all guarantee computable partition functions, at the cost of expressiveness.
2	Background
2.1	Energy-based sequence models
Energy-based models (EBMs) of sequences (LeCun et al., 2006; Rosenfeld et al., 2001; Sandbank, 2008;
Huang et al., 2018; Bakhtin et al., 2021) are a family of discrete sequence models. Given a string x ∈ V*
over a finite vocabulary V, an EBM / computes string weight /(x), but not the (normalized) string
probability P (x)=/(X)/χ* /(X). In this work, we focus on EBMs whose weight is efficiently computable,
i.e., polytime in the length of x. Previous work (Bakhtin et al., 2021; Lin et al., 2021) showed that EBMs
define expressive distributions; but this requires normalization. While EBMs are often intractable to
normalize (e.g., the Ising model), the infinite domain of finite strings opens the door to uncomputable
probabilities, which invite the impossibility of model selection and comparison.
2.2	Weighted languages
EBMs give weights to strings. Here we formally characterize these weighted strings as a weighted
language. An unweighted language L ⊆ V * is a set of finite strings X over a finite vocabulary V. A
weighted language P is a function /：V * → R≥o.3 In this work, we discuss Boolean languages, such
that V = B , {0, 1}. We also focus on weighted languages where distributions over strings exist. Following
Lin et al. (2021), we say such weighted languages P are normalizable: Z/，^x∈b* p(x) ∈ R>o. Z/ is
also called the partition function of p.4 We can then normalize P into a distribution P over B* such
that P(x) = P(x)/Z/, and thereby ^x∈b* P(x) = 1.
The efficiently computable (weighted) languages (EC; Lin et al., 2021) are those weighted languages
P, where a string's weight (which must be non-negative) is a polytime function of the string. Weighted
languages defined by most EBM sequence models fall into this class, since they score every string in finite
time (and usually in polytime); and the scores ultimately are computed by some algorithm.
Intuitively, string weights of an EC language can be obtained as a side product from a weighted Turing
machine that recognizes x (in polytime). However, Lin et al. (2021) did not precisely describe how a
Turing machine maps input string X to its (rational) weight P(x) in finite time. In this work, such a
construction (out of many possible ones) is necessary, as we need to show that this string weighting can be
2Of course, model selection (and computing Z/) are not always needed——for example, simply deciding whether
P(xι) > P(x2) for two given strings X{1,2}. In such case the uncomputability issues we discuss are not a concern.
3With a slight abuse of notation, we also define Support(户)，{x : P(X) > 0}.
4We use the convention that Z° is the partition function of P, and Zq of ¢, etc. The subscript is omitted in
unambiguous cases.
2
Published as a conference paper at ICLR 2022
done by parametric sequence model families (see Appendix B).
2.3	Locally normalized distributions
A popular subclass of weighted languages is locally normalized weighted languages (LN), where
conditional local distributions given prefix £: P(∙ | £) can be computed in finite time. Since they
automatically define a distribution over strings, we use the term locally normalized distributions
interchangeably. If P(∙ | £) can be computed in polynomial time, we call such distributions efficiently
locally normalized distributions (ELN) — this is the weighted language class of most autoregressive
models, just as its superset EC is the weighted language class of most energy-based sequence models.
Locally normalized distributions over sequences have P(£)= P(£), and Z = ^x∈b∙ P(£)= 1. They are
consistent: the probability that a string is infinitely long under such distributions is zero. Equivalently,
given any β > 0, we can approximate Z with a finite sum of string weights:
Proposition 1 (Consistency of LN distributions (Booth & Thompson, 1973; Chen et al., 2018)). Let
P ∈ LN be a locally normalized distribution over strings. All strings of a given length or longer have their
probabilities bounded. That is, for all positive real numbers E, there is a length n at which all strings £ of
length at least n have P(£)< E.
The consistency property of locally normalized distributions implies that they have an exact sampling
procedure that almost surely terminates in finite time. Therefore, they (and in particular ELN distributions)
are an attractive choice when we need to sample from the distribution they define, e.g., in sampling-based
parameter estimation procedures.
2.4	EC-complete parametric families
We have introduced energy-based sequence models (§2.1) and their characterization as weighted languages
(§2.2). However, we usually do not work with weighted Turing machines directly in machine learning. Here
the most common models of computation are (neural) sequence model families, such as RNNs and
Transformers. While these computational models can appear quite dissimilar to state-machine-based models
of computation (e.g., Turing machines), they have been shown to possess the same computation power
(Siegelmann & Sontag, 1995; PereZ et al., 2021). That is, they are Turing-complete.
Just as we extend the definition of Turing machines to weighted Turing machines, we likewise formalize
weighted sequence model families. We thus introduce EC-complete parametric families as a sequence
model counterpart of the weighted language class EC. At a high level, a parametric family Θ is EC-complete
if given any P ∈ EC (as a description of a weighted Turing machine), We can construct a parameter vector
θ ∈ Θ ⊆ Q* which defines P's corresponding sequence model. The model produces an output embedding
which we then decide in polytime to be either a ‘halting embedding’ or not. If it is, we can then extract the
weight P(£). (A rigorous exposition is in Appendix C.)
We show that with a modification to positional embeddings, the family of arbitrary precision, hard attention
Transformers defined in Perez et al. (2021) is EC-complete:5
Theorem 1. The class of one-encoder-layer, four-decoder-layer Transformer networks with positional
encodings (n, 1/n, 1/n2,2n) is EC-complete.
Proof sketch. We extend the construction from Perez et al. (2021), adding an additional layer to accumulate
the string weight over time steps.
2.5	Estimators
A main result of this work is that partition functions in an EC-complete family are uncomputable. Moreover,
randomness does not help estimation; and correct asymptotic estimators are not useful. We define these
estimators here in order to discuss the power of different estimators concretely.
Let Θ be a parametric family. The function f : Θ → Q is an exact estimator if there exists a weighted
deterministic IUring machine that takes θ as input and, upon halting, outputs f (θ) ∈ Q in finite time.
5All proofs omitted from the main text are included in Appendix D. Proof sketches for major theorems are in the
main text.
3
Published as a conference paper at ICLR 2022
Many estimation procedures are anytime algorithms: they do not have a predetermined runtime, and they
can be stopped at any time before completion to produce a valid output. Output quality of an anytime
algorithm may improve with increased runtime. Moreover, many of these algorithms have asymptotic
guarantees, in that their outputs are optimal in some sense (e.g., consistency) in the limit. We capture these
algorithms with asymptotic estimators: a function f (θ) is an asymptotic estimator if there exists a
weighted Turing machine that takes both θ and an index i as input, then outputs a value fp,i ∈ Q in finite
time, such that the outputs converge toward f (θ) as i increases toward infinity. (An example is 2asym
introduced at §5.)
We now extend both exact and asymptotic estimators to the stochastic case, where we compute the estimates
using randomized algorithms instead of deterministic ones. As is conventional for randomized algorithms,
we assume the use of probabilistic Turing machines. These have access to an infinitely long random tape.
The tape describes an infinite binary sequence from tossing a fair two-sided coin infinitely many times. We
call the random tape distribution P τ .6 We define a randomized exact estimator f as a weighted Turing
machine with two input tapes — the random tape 丁 ∈ BN, and an input tape that has θ — and outputs
f (a T) in finite time. Likewise, we say fg∕ is a randomized asymptotic estimator if there exists a
function f (θ) ∈ R and a weighted Turing machine that takes (θ, i), T on two input tapes, so that for all
random Boolean tapes T ∈ BN, we converge with limf→∞ fθ,i,τ = f (θ). Many Monte Carlo estimators
can be seen as randomized asymptotic estimators, including rejection sampling and importance sampling
estimators.
3	Expressiveness and uncomputability: pathological EBMs
3.1	Expressive sequence distributions
To illustrate the uncomputability issues of energy-based models, we construct families of string distributions
that are expressive: they require the full power of an EC-complete sequence model family.
We define Gk = {pM,k : k ∈ Q>0} to be a set of weighted languages, where Pm# is parametrized by
deterministic Turing machine M that takes an empty string as input ('input-free’). Let LM ⊆ B* be a
prefix-free Boolean language of computation sequences of a Turing machine — that is, encodings of the
trace of the Turing machine M’s operations. We define
pM,k (x)='
'1/3|x |+1 + k
1/31* 1+1
if x ∈ LM, and x encodes a valid accepting
trace of M.
otherwise.
(1)
The weight of any string x, where |x | =% under PM,k can be computed in time O (Polym)), by verifying
whether X is an accepting execution trace of M, from the initial state to an halting state. That is, P ∈ EC
(§2.2). We also know that for any (deterministic) machine M, the language's partition function ZPM k
exists, and it must equal either 1 or 1 + k, since there is either one halting trace (zPM k = 1 + k),or none
(ZPM k = 1). Therefore, each P ∈ Gk defines a string distribution.7
Since for all k ∈ Q>0, Gk ⊂ EC, all weighted languages 歹M,k have an equivalent parameter vector θM,k
in any EC-complete family Θ. Also, since each Gk is a bijection between the set of all input-free Turing
machines and a subset of Θ, it follows that there is no exact estimator (§2.5) of the partition function of any
EC-complete family (e.g., Transformer EBMs (Theorem 1)), by a reduction from Halt:8
Theorem 2.	Let Θ be a parametric EC-complete sequence model family. And let Θk ⊂ Θ be bijective
with Gk. There is no k ∈ Q>0 for which there exists an exact estimator Zk that takes as input θ ∈ Θk as
input, and computes Zk (θ) = ZPe infinite time.
Proof sketch. Θk contains parametric vectors for all input-free Turing machines M. For any of these
6Formally speaking, we define a probability space (Ω, A, P) where Ω = BN is our sample space, A = {A⅛ :
Ay is the set of all sequences ∈ Ω that have prefix b ∈ B*} is our event space, and P(A) = 2-n where n is the
length of the longest shared prefix of A, is our probability function (Stroock, 2014).
7Our construction of expressive sequence distributions is inspired by a weighted language construction in Lin et al.
(2021). See Appendix E for further discussion.
8Speaking loosely, Halt is the task of recognizing (deciding) whether a given program on an ideal computer will
properly terminate in finite time or not (Sipser, 2013; Arora & Barak, 2006).
4
Published as a conference paper at ICLR 2022
weighted languages, knowing the exact value of Z/M k ∈ {1, k + 1} is enough to decide whether M halts
(M halts iff Z/Mj = k + 1). As HALt is undecidable for input-free Turing machines, Z左 cannot exist. □
3.2 An EBM whose partition function is uncomputable
Theorem 2 states there is no estimator that 'works’ for a subset of parameter vectors. While every Gk is
much smaller than its superset EC, Gk is still (countably) infinite. Here under the assumption that ZFC is
consistent, we construct one weighted language b ∈ Gi (for simplicity; it holds for arbitrary k), where ZG
is uncomputable:
Theorem 3.	Assuming ZFC axioms and that assuming they is consistent, there exists an EC weighted
language b ∈ Gi such that (a) Z^ = C ∈ {1,2} exists, but (b) there is no proof of Z^ = c.
Proof sketch. We construct b such that ZG = 2 iff ZFC is inconsistent. If 由ere were a proof ZG = 1 then
we proved ZFC is consistent, which is impossible under Godel's 2nd incompleteness theorem (Jech, 1994).
If there were a proof ZG = 2, then we proved ZFC is not consistent, which violated our assumption. □
The existence of b suggests that if there is an algorithm that approximates Z/, and produces a witness of
its approximation quality, then this algorithm will not work on any set of parameter vectors that can
parametrize b — even when this algorithm may work for some subsets of Θ. This is useful in allowing us to
show negative results regarding finite subsets of Θ. Appendix F gives one such example.
4 No rAndomized ALgorithm cAn estimAte Z AccurAteLy
We’ve shown by Theorem 2 that for an EC-complete family, there are no exact estimators than can get Z
perfectly right. In this section, we show that no randomized exact estimator for this is unbiased. Further,
there isn't even an estimator whose bias is within some factor E, regardless of the variance's magnitude.
Lemma 1. Let Θ be an EC-complete parametric family. There is no multiplicativefactor E ∈ Q>i for
which every θ ∈ Θ can have its partitionfunction approximated with Z e (θ) within a factor of E - with
probability greater than 2/3. That is, we cannot have
P((1∕e) ZA) ≤ Ze(6)≤ EZW) > 2∕3.	⑵
Proof sketch. Because Z e computes an estimate in finite time, it can only use finitely many distinct
random tape segments. We therefore derandomize Z e by enumerating finitely many ‘random' tapes, and
can always return correct answer, given the 2∕3 success probability assumption, and would be able to decide
Halt, making use of distributions in Ge2.	□
Taken together, Theorem 2 and Lemma 1 state that no exact estimator Z — whether randomized or
deterministic — can approximate Z with good confidence. In Theorem 4 below, we will make an even
stronger claim: regardless of the dispersion magnitude, it is impossible to bound the mean of random exact
estimators of Z to within any (computable) multiplicative factor. This is because the mean of Ze can be
computed in finite time, by derandomizing Z e similarly to our proof of Lemma 1:
Theorem 4.	Let Θ be an EC-complete parametric family. There is no multiplicative bound E ∈ Q>1 such
that there exists a randomized exact estimator Z e that guarantees 1∕e ≤ E[Ze (β)]∕zpθ ≤ e ,for every
θ ∈ Θ where 讥 is normalizable.
Proof sketch. We derandomize the expectation of Ze .	□
5
Published as a conference paper at ICLR 2022
5	Common asymptotic estimators do not give useful guarantees
Let’s now recap the progress we’ve made so far. We’ve shown that no (deterministic) exact estimator can get
Z exactly right in general (Theorem 2), lest it need to solve Halt. Further, no randomized exact estimator
can approximate it within any given relative tolerance, with good confidence (Theorem 4).
But what about asymptotic estimators? We do know there are correct asymptotic estimators of Z. For
example, consider the following asymptotic estimator Z(θ) backed by a weighted Turing machine that
takes θ ∈ Θ and i ∈ N as inputs, and returns fg,i，2**∈b∙ JX∣≤i BG(x). We have limr→∞ Z0,t = Z说,
so Z is asymptotically correct. However, Zasym does not have a convergence rate guarantee: for any i ∈ N,
k Z0,i - Zjyθ k is uncomputable. We also do not know how much can we improve our estimator when we
increment i. As Corollary 2 suggests, we likely cannot have such a guarantee.
In this section, we formalize this intuition for two popular asymptotic estimators: rejection and importance
sampling methods (with other asymptotic estimators left as future work). Specifically, we show that any
parametric family that is able to parametrize b from §3.2 cannot have provably useful locally normalized
distributions (§2.3) as proposal distributions.
5.1	Rejection sampling estimator of Z cannot be guaranteed to be possible.
Rejection sampling (Owen, 2013) is a common exact sampling method, applicable even when we cannot
sample from an unnormalized distribution 歹.We instead sample from an easy-to-sample distribution q,
then stochastically reject samples, to ensure the probability 由at a sample X is kept is proportional to /(x).
For rejection sampling to work, the candidate q's support must contain the target TrS entire support, so that
all true points can be sampled. We also need some finite constant C so that cq envelops /：
∃c ∈ R>o such that ∀x ∈ B*, (/(x)/q(x)) ≤ c.
We will show that for certain EBMs, one cannot formally guarantee the existence of an eligible q ∈ LN.
Theorem 5. Using ZFC as our axiom set and assuming they are consistent, then there exists a normalizable
EC weighted language /, where there does not exist a consistent locally normalized proposal distribution
q ∈ LN, and Cq ∈ Q>o, such that it can be proven ∀x ∈ B*,户(X)/q (χ) < Cq.
Proof sketch. Let / = b. If there were such q and Cq, we could in finite time prove Z/ = 1 or Z/ = 2,
which contradicts Theorem 3.
Theorem 5 implies that there is no way of ensuring rejection sampling works, not only for any EC-complete
families, but also for any parametric family that can parametrize b.
5.2	Importance sampling estimator of Z cannot be guaranteed to be effective.
Similar to the case of rejection sampling, one cannot guarantee an importance-sampling estimator of Z to be
‘good’ — in this case, we mean that there cannot be a proof that the importance sampling variance is finite.
We first formalize importance sampling estimators of Z as randomized asymptotic estimators (§2.5). Let
Zq」y τ⅞ (X(R))
θ,n ~ N 4 q(x (R))
be an N-sample importance sampling estimator of Z诩 under q, so all X(R) are samples from q ∈ LN.
We generally want to minimize the variance of Z(G N under q: Varq 佟(GN^ (Owen & Zhou, 2000). And
we certainly do not want Varq (ZCqq N) = ∞. Unfortunately, for certain EBMs, we cannot guarantee there is
a good locally normalized proposal distribution that has finite variance:
Theorem 6. Let Θ be an EC-complete parametric family. Assuming ZFC axioms and assuming they are
consistent, there exists θ ∈ Θ where there does not exist a consistent locally normalized “proposal”
distribution q ∈ LN such that it can be proven Varq (ZXNj < c ≠ ∞, where C ∈ Q>o.
Proof sketch. Proven in a manner similar to the proof of Theorem 5.
6
Published as a conference paper at ICLR 2022
6	UNcomPUtABLe Z causes parameter estimation problems
Theorems 2 and 3 state that it is generally impossible to estimate partition functions in an expressive
parametric family, such as certain subsets of an EC-complete family. Here we show how parameter
estimation is made difficult as well: parameter identifiability is formally undecidable for an EC-complete
family. Model selection is not possible, either, despite attempts to circumvent this (Table 1).
6.1	Parameter identifiability under EC-complete families is undecidable
Consistency of many estimators relies upon the condition of parameter identifiability (Lehmann & Casella,
2006) — two different parameter vectors should define different string distributions. But we in general
cannot ascertain whether this condition holds, even for finite subsets of an EC-complete family:
Theorem 7. Let Θ be an EC-Completefamily. There is no algorithm that takes θ 1 ∈。，θ2 ∈ Θ as input,
and decides whether 瓜 and 限 are the Same weighted language.
6.2	Model selection is generally impossible for EC-complete sequence model families
Theorem 7 suggests that consistency guarantees of common parameter estimators do not hold, when
used to estimate vectors from Θ. Nonetheless, such ‘off-label’ use of consistent parameter estimators
in an expressive parametric family is quite common ——one usually just selects the best model θ*
among finitely many candidates (Say {θ 1... θN}) that achieves highest held-out likelihood: θ* =
arg maxθn：i ≤n≤N ∏x∈d Pon (ɪ), where D is a finite set of strings.
However, Theorem 8 implies that exact log-likelihood-based model selection is generally impossible for
EC-complete sequence model families:
Theorem 8. For any P ∈ EC and for any EC-complete family Θ, there is no algorithm Better / that
takes two parameter vectors θ 1 ∈ Θ, θ 2 ∈ Θ, and returns YES if KL( p∣∣ ρoj ≥ KL( p∣∣ p02), and NO
otherwise, where p,poι,Po? are string distributions defined by p,pθι, P02 respectively.
7	Palliative alternatives
What is a practitioner to do, given that this class of models is unlearnable in general? We emphasize that a
model family does not have to be EC-complete to suffer from model selection problems (§6.2) —— for
example, model selection is impossible for fixed-size Transformer networks with large enough d's either, by
an extension to Theorem 8 (see Theorem 11 in Appendix G).9 10 In other words, to ensure the problem of
uncomputability does not haunt us, the best we can do is to cripple P so severely that uncomputability is
impossible.
We identify three palliative choices that restrict the family of EBMs. Each cripples the model P in its own
way, affording computability at the cost of expressiveness. The choice of triage infuses the model with an
inductive bias; we must tailor our models based on our prior beliefs about the problem domain.
Restricting SuPPort(P) to be finite. If P assigns non-zero weights to only finitely many strings, then
Z/ is a finite sum of rational numbers, and is also rational. Here, sampling-based inference and estimation
methods return to their usefulness. One way to ensure support(P) is finite is by upper-bounding the
maximum string length (e.g., Bakhtin et al. (2021)).
The finite-support restriction imposes an obvious limitation that it cannot handle long strings. Moreover,
while Z/ is computable when support (P) is finite, this quantity can still be practically inapproximable,
assuming that P is expressive (e.g., P is an EC weighted language that has finite support), except for very
short strings. Let n be the longest string under P to have non-zero weight. Assuming NP * P/poly,
no randomized estimator of Z/ that is a good approximation can have a guaranteed O (Polys)) time
complexity (Chandrasekaran et al., 2008). However, if P has limited expressiveness (e.g., when P is an Ising
model where a string weight is the sum of pairwise weights), then FPRAS algorithms for approximating Z/
9In addition to model selection issues, it may also be difficult to acquire unbiased gradients of log Z:
V (log Z)，1/z VZ, which are needed for MLE-based training.
10Limiting ourselves to small d's to avoid UnCOmPUtabiHty issues may not be practical; we leave finding the largest d
that provably does not involve uncomputability problems —— if it is even possible —— as future work.
7
Published as a conference paper at ICLR 2022
may exist when / describe a low-degree (≤ 2) graph (Jerrum & Sinclair, 1993; Luby & Vigoda, 1999).
However, for high-degree graphs (≥ 3) it can be shown no FPRAS algorithm for approximating Z/ exists,
assuming RP ≠ NP (Galanis et al., 2016).
Autoregressive parametrization of 歹. An alternative choice is to confine ourselves to autoregressive
models, i.e., locally normalized string distributions (§2.3). Under an autoregressive model p, ZP = 1 by
definition. We also note that any (unnormalized) distribution / obtained by removing probability mass from
P will have a computable partition function, as long as / ∈ EC:
Theorem 9. Let P be any LN weighted language. Any P ∈ EC where ∀x ∈ V*, p(x) ≤ P(x) has a
computable ZP.
Proof sketch. We construct an algorithm that approximates ZP to any arbitrary error level in finite time,
exploiting the consistency property of P (Proposition 1).
Theorem 9 implies that conditionalization operations on P, which remove strings from the support of P to
get weighted language p, result in a computable ZP (as long as We can decide which strings are removed);
and such a P is therefore not subjected to the limitations of Theorem 4.
Unlike the 'finite support’ fix, an autoregressively parametrized (or subsequently conditionalized) P can
have an infinite support. A conditionalized P can have an intractable (but computable) partition function,
and they are still subject to the expressive limitations imposed on LN languages: namely there is an EC
language whose string weight rankings cannot be honored by any such conditionalized P (Lin et al., 2021).
P as low-treewidth factor graph grammars. Finally, we may limit ourselves to weighted languages
defined by low-treewidth factor graph grammars (Chiang & Riley, 2020). Factor graph grammars generalize
factor graphs, which cover many classes of graphical sequence models, such as n-gram, HMM, and
whole-sentence language models (Jelinek, 1980; Kuhn et al., 1994; Rosenfeld et al., 2001), linear CRF
models (Lafferty et al., 2001), and weighted FSAs in general (Dreyer & Eisner, 2009): a factor graph
grammar describes a (possibly infinite) set of factor graphs, generated from a hyperedge replacement graph
grammar.
Assuming that an FGG G contains at most one n-observed-node factor graph for all n ∈ N, it then defines a
weighted language PG (x) = ∏IM∈ψ∣x∣ ψ, where factor ψ is a positive function of nodes. The treewid由 of
an FGG W(G) is the maximum number of nodes any ψ can be a function of, and Ψ∣X ∣ is the set of all
factors of string length |x|.
If ZPG ∈ R exists, it can be computed exactly by an algorithm, in time exponential in W(G) following
Chiang & Riley (2020). Exact computation of ZPG may be manageable as long as W(G) is small, which
would allow us to exactly compute held-out data likelihood, and also train with a (marginalized) MLE
objective function. However, limiting W(G) directly limits the expressiveness of /.
8	Related work
Turing completeness of formal languages and associated uncomputability issues emerge repeatedly
in computer science. For example, Turing completeness may emerge as an unwanted side effect in
programming languages, since it implies undecidability. One of the best known examples is the Turing
completeness of the C++ grammar (Veldhuizen, 2003; Haberman, 2013), which makes both parsing and
compiling C++ programs undecidable. Similar problems exist for Java (Grigore, 2016) and Haskell with
(unrestricted) instance declarations (Wansbrough, 1998). Another example is the (possibly inadvertently
introduced) Turing completeness of the page fault handling mechanism on modern computer systems,
which raises security concerns in the context of trusted computing (Bangert et al., 2013).
Our work is not the first to discuss the consequences of computability in machine learning: assuming
we can acquire training data from an oracle, under a supervised learning setting, recognition of an
undecidable language is PAC-learnable (Lathrop, 1996). Agarwal et al. (2020) extended the definition of
PAC learnability (Valiant, 1984) to computable learners. By contrast, we are focused on the computability
of EBMs for sequences, such as a language model as a component of a larger system for automatic
speech recognition. Designing an appropriate, efficient loss functional is a challenge that several prior
works have compared. With the plethora of learning strategies for EBMs, it is untenable to point out the
8
Published as a conference paper at ICLR 2022
				
	Energy-based	Infinite	Scoring function	
	(i.e., globally	language	has unbounded	
Technique	normalized)	support	treewidth	Consistency
Noise-contrastive estimation (Ma & Collins (2018);				
used in Lin et al. (2021); Bakhtin et al. (2021))	✓	X	✓	✓
MLE with variable elimination in factor graph grammars (Chiang & Riley (2020); used in Eisner (2001); Finkel et al. (2008), inter alia)	✓	✓	X	✓
MLE with autoregressive parametrization (Mikolov et al., 2010)	X	✓	✓	✓
Contrastive divergence (Hinton, 2002)	✓	✓	✓	X
Contrastive estimation (Smith & Eisner, 2005)	✓	✓	✓	X
Table 1: Deficiencies of some common alternatives to overly expressive EBMs.
deficiency in each. Table 1 gives a handful of examples; none share the four properties we desire: 1. Global
normalization (without which the model would be in LN) 2. Support over infinite languages (of finite
strings) 3. Unbounded treewidth in the function assigning weights to strings 4. Estimator consistency (i.e.,
asymptotic guarantee to recover the true parameters).
Lin et al. (2021) noted autoregressive factors of EC languages can be uncomputable (see also Theorem 10).
They also noted that a weighted language can have an uncomputable partition function (presumably
resulting from the sum over infinitely many string weights). But they did not dwell on the question whether
such a weighted language could lie within the EC class, much less providing a constructive proof (see also
Appendix E). Instead, they emphasized that under the assumption that oracular access to trained parameter
strings is possible, arbitrarily good approximations of the (possibly uncomputable) partition function can be
memorized in the (autoregressive) model parameters. There is an interesting constrast between the stances
of Lin et al. (2021) and our work: Lin et al. (2021) saw the uncomputability of Z as a trivial issue from the
model capacity viewpoint, since good approximations take few bits in the parameters to store. On the other
hand, we see that the uncomputability problem can be a parameter estimation disaster — there will be no
guarantee of good approximations can be found in finite time at all.
9	Conclusion and future work
Energy-based models are posed as an efficient tool for decision problems, circumventing probabilities and
expensive normalization (LeCun et al., 2006). Extending this vision to generic sequence models, though,
can involve complexity/computability problems that are difficult, or even impossible. We’ve shown that as
energy-based sequence models become more powerful, the partition function becomes uncomputable —
even when we are restricted to polytime-computable weighted languages. Exact estimators, even if
randomized, cannot have accuracy guarantees. Popular asymptotic estimators on the other hand are not
useful either. Furthermore, model selection is generally impossible, even if we limit ourselves to fixed-size
sequence model families.
This paper continues a discussion started by Lin et al. (2021), who posture energy-based models as a more
powerful alternative to autoregressive sequence models. Autoregressive sequence models, after all, have
wide adoption and empirical successes (Radford et al., 2019; Brown et al., 2020). By contrast, more general
neural energy-based sequence models have not caught on. Why not? We give unlearnability — due to
uncomputability — as a possible explanation: unless we give up the ability to learn parameters from data,
we likely cannot use the full expressiveness afforded by powerful neural networks. Just like the model
capacity problems brought up by Lin et al. (2021), this result is independent of the amount of training data.
We emphasize that our results do not invalidate the findings of Lin et al. (2021): regardless of the actual
neural parametrization, autoregressive models can never capture certain distributions that energy-based
models can. Instead, one of our main messages is that we may not be able to find those EBM parameters in
finite time, if we do not know what the parameters are. Of course, if we know the task perfectly well
and can in fact manually assign the model parameters, we will not need to learn from data at all. The
middle ground — when we have some prior knowledge about the task, but cannot really design the
parameter vectors — is an interesting direction for future work: the three palliative alternatives outlined
in §7 do not take task-specific information into account at all. Can we do better than that, without suffering
uncomputability problems?
9
Published as a conference paper at ICLR 2022
Acknowledgments
We thank the four reviewers for their comments, especially Reviewer NAcm for shortening the proof of
Theorem 4. Additionally, we thank Alexandra DeLucia, Matthew Francis-Landau, Chin-Fu Liu, Suzanna
Sia, Neha Verma, and Chenghao Yang (sorted alphabetically) for discussions that improved the presentation;
and Jason Eisner, discussions with whom motivated the original exploration of this work.
References
Sushant Agarwal, Nivasini Ananthakrishnan, Shai Ben-David, Tosca Lechner, and Ruth Urner. On
learnability wih computable learners. In Aryeh Kontorovich and Gergely Neu (eds.), Proceedings of the
31st International Conference on Algorithmic Learning Theory, volume 117 of Proceedings of Machine
Learning Research, pp. 48-60. PMLR, 08 Feb-11 Feb 2020. URL https://proceedings.mlr.
press/v117/agarwal20b.html.
S. Arora and B. Barak. Computational Complexity: A Modern Approach. Cambridge University Press,
2006. ISBN 978-0-521-42426-4. URL https://theory.cs.princeton.edu/complexity/
book.pdf.
Anton Bakhtin, Yuntian Deng, Sam Gross, Myle Ott, Marc’Aurelio Ranzato, and Arthur Szlam. Residual
energy-based models for text generation. JMLR, 22(40):1-41, 2021. URL http://jmlr.org/
papers/v22/20-326.html.
Julian Bangert, S. Bratus, Rebecca Shapiro, and Sean W. Smith. The page-fault weird machine: Lessons in
instruction-less computation. In WOOT, 2013.
Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic
language model. J. Mach. Learn. Res., 3(null):1137-1155, mar 2003. ISSN 1532-4435. URL
https://www.jmlr.org/papers/v3/bengio03a.html.
Satwik Bhattamishra, Arkil Patel, and Navin Goyal. On the computational power of Transformers and its
implications in sequence modeling. In Proceedings of the 24th Conference on Computational Natural
Language Learning, pp. 455-475, Online, November 2020. Association for Computational Linguistics.
doi: 10.18653/v1/2020.conll-1.37. URL https://aclanthology.org/2020.conll-1.37.
T.L. Booth and R.A. Thompson. Applying probability measures to abstract languages. IEEE Transactions
on Computers, C-22(5):442-450, 1973. doi: 10.1109/T-C.1973.223746.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan,
and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877-1901.
Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.
Venkat Chandrasekaran, Nathan Srebro, and Prahladh Harsha. Complexity of inference in graphical models.
In UAI, 2008. URL https://dl.acm.org/doi/10.5555/3023476.3023485.
Yining Chen, Sorcha Gilroy, Andreas Maletti, Jonathan May, and Kevin Knight. Recurrent neural networks
as weighted language recognizers. In Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1
(Long Papers), pp. 2261-2271, New Orleans, Louisiana, June 2018. Association for Computational
Linguistics. doi: 10.18653/v1/N18-1205. URL https://aclanthology.org/N18-1205.
David Chiang and Darcey Riley. Factor graph grammars. In H. Larochelle, M. Ranzato, R. Hadsell,
M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
6648-6658. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/
2020/file/49ca03822497d26a3943d5084ed59130-Paper.pdf.
10
Published as a conference paper at ICLR 2022
Stephen A. Cook. The complexity of theorem-proving procedures. In Proceedings of the Third Annual
Acm Symposium on Theory of Computing, STOC '71, pp. 151-158, New York, NY, USA, 1971.
Association for Computing Machinery. ISBN 9781450374644. doi: 10.1145/800157.805047. URL
https://doi.org/10.1145/800157.805047.
Markus Dreyer and Jason Eisner. Graphical models over multiple strings. In Proceedings of the 2009
Conference on Empirical Methods in Natural Language Processing, pp. 101-110, Singapore, August
2009. Association for Computational Linguistics. URL https://aclanthology.org/D09-1011.
Jason Eisner. Expectation semirings: Flexible EM for finite-state transducers. In Gertjan van Noord (ed.),
Proceedings of the ESSLLI Workshop on Finite-State Methods in Natural Language Processing (FSMNLP),
Helsinki, August 2001. URL http://cs.jhu.edu/~jason/papers/#eisner- 2001- fsmnlp.
Extended abstract (5 pages).
Jenny Rose Finkel, Alex Kleeman, and Christopher D. Manning. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL-08: HLT, pp. 959-967, Columbus, Ohio, June 2008.
Association for Computational Linguistics. URL https://aclanthology.org/P08-1109.
Andreas Galanis, Daniel Stefankovic, and Eric Vigoda. Inapproximability of the partition function for the
antiferromagnetic Ising and hard-core models. Combinatorics, Probability and Computing, 25:500 -
559, 2016.
Zoubin Ghahramani. Probabilistic machine learning and artificial intelligence. Nature, 521(7553):452-459,
2015. doi: 10.1038/nature14541. URL https://doi.org/10.1038/nature14541.
Radu Grigore. Java generics are Turing complete. CoRR, abs/1605.05274, 2016. URL http:
//arxiv.org/abs/1605.05274.
Josh Haberman. Parsing C is literally undecidable, Aug 2013. URL https://blog.reverberate.
org/2013/08/parsing- c- is- literally- undecidable.html.
Geoffrey E. Hinton. Training products of experts by minimizing contrastive divergence. Neural
Comput., 14(8):1771-1800, August 2002. ISSN 0899-7667. doi: 10.1162/089976602760128018. URL
https://doi.org/10.1162/089976602760128018.
Y. Huang, A. Sethy, K. Audhkhasi, and B. Ramabhadran. Whole sentence neural language models. In
ICASSP, April 2018. doi: 10.1109/ICASSP.2018.8461734. URL https://ieeexplore.ieee.org/
document/8461734.
Thomas Jech. On Godel,s second incompleteness theorem. Proceedings OftheAmerican Mathematical
Society, 121(1):311-313, 1994. doi: 10.2307/2160398.
Frederick Jelinek. Interpolated estimation of Markov source parameters from sparse data. In Proc.
Workshop on Pattern Recognition in Practice, 1980, 1980.
Mark Jerrum and Alistair Sinclair. Polynomial-time approximation algorithms for the Ising model. SIAM
Journal on computing, 22(5):1087-1116, 1993.
S.	Katz. Estimation of probabilities from sparse data for the language model component of a speech
recognizer. IEEE Transactions on Acoustics, Speech, and Signal Processing, 35(3):400-401, 1987. doi:
10.1109/TASSP.1987.1165125.
T.	Kuhn, H. Niemann, and E.G. Schukat-Talamazzini. Ergodic hidden Markov models and polygrams for
language modeling. In Proceedings of ICASSP ’94. IEEE International Conference on Acoustics, Speech
and Signal Processing, volume i, pp. I/357-I/360 vol.1, 1994. doi: 10.1109/ICASSP.1994.389282.
John D. Lafferty, Andrew McCallum, and Fernando Pereira. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In ICML, 2001.
Richard H. Lathrop. On the learnability of the uncomputable. In ICML, 1996.
Yann LeCun, Sumit Chopra, Raia Hadsell, Marc’Aurelio Ranzato, and Fu-Jie Huang. A tutorial on energy-
based learning. In G. Bakir, T. Hofman, B. Scholkopf, A. Smola, and B. Taskar (eds.), Predicting Structured
Data. MIT Press, 2006. URL http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf.
11
Published as a conference paper at ICLR 2022
Erich L Lehmann and George Casella. Theory of point estimation. Springer, 2006. URL https:
//link.springer.com/book/10.1007%2Fb98854.
Chu-Cheng Lin, Aaron Jaech, Xin Li, Matthew R. Gormley, and Jason Eisner. Limitations of autoregressive
models and their alternatives. In Proceedings of the 2021 Conference of the North American Chapter of
the Associationfor Computational Linguistics: Human Language Technologies, pp. 5147-5173, Online,
June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.405. URL
https://aclanthology.org/2021.naacl-main.405.
Michael Luby and Eric Vigoda. Fast convergence of the Glauber dynamics for sampling independent sets.
Random Structures & Algorithms, 15(3-4):229-241, 1999.
Zhuang Ma and Michael Collins. Noise contrastive estimation and negative sampling for conditional
models: Consistency and statistical efficiency. In EMNLP, 2018. URL https://www.aclweb.org/
anthology/D18- 1405.pdf.
Tomas Mikolov, M. Karafiat, L. Burget, J. Cernocky, and S. KhUdanpur. Recurrent neural network based
language model. In INTERSPEECH, 2010.
Art B. Owen. Monte Carlo theory, methods and examples. Unpublished, 2013. URL https:
//artowen.su.domains/mc/.
Art B. Owen and Yi Zhou. Safe and effective importance sampling. Journal of the American Statistical
Association, 95:135-143, 2000.
Jorge Perez, Pablo Barcelo, and Javier Marinkovic. Attention is Turing-complete. Journal OfMaChine
Learning Research, 22(75):1-35, 2021. URL http://jmlr.org/papers/v22/20-302.html.
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are
unsupervised multitask learners. Unpublished, 2019. URL https://d4mucfpksywv.cloudfront.
net/better-language-models/language-models.pdf.
Ronald Rosenfeld, Stanley Chen, and Xiaojin Zhu. Whole-sentence exponential language models: A
vehicle for linguistic-statistical integration. Computer Speech & Language, 15(1):55-73, January 2001.
URL https://doi.org/10.1006/csla.2000.0159.
Ben Sandbank. Refining generative language models using discriminative learning. In EMNLP, 2008. URL
https://www.aclweb.org/anthology/D08- 1006.pdf.
H.T. Siegelmann and E.D. Sontag. On the computational power of neural nets. Journal of Computer and
System Sciences, 50(1):132-150, 1995. ISSN 0022-0000. doi: https://doi.org/10.1006/jcss.1995.1013.
URL https://www.sciencedirect.com/science/article/pii/S0022000085710136.
Michael Sipser. Introduction to the Theory of Computation. Course Technology, Boston, MA, third edition,
2013. ISBN 113318779X.
Noah A. Smith and Jason Eisner. Contrastive estimation: Training log-linear models on unlabeled data. In
Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),
pp. 354-362, Ann Arbor, Michigan, June 2005. Association for Computational Linguistics. doi:
10.3115/1219840.1219884. URL https://aclanthology.org/P05-1044.
Daniel W. Stroock. An Introduction to Markov Processes, volume 230 of Graduate Texts in Mathematics.
Springer, Heidelberg, 2 edition, 2014. ISBN 978-3-642-40522-8. doi: 10.1007/978-3-642-40523-5.
A. M. Turing. On Computable Numbers, with an Application to the Entscheidungsproblem. Proceedings of
the London Mathematical Society, s2-42(1):230-265, 01 1937. ISSN 0024-6115. doi: 10.1112/plms/
s2-42.1.230. URL https://doi.org/10.1112/plms/s2-42.1.230.
L. G. Valiant. A theory of the learnable. Commun. ACM, 27(11):1134-1142, November 1984. ISSN
0001-0782. doi: 10.1145/1968.1972. URL https://doi.org/10.1145/1968.1972.
Todd L. Veldhuizen. C++ templates are Turing complete. Technical report, Indiana University Computer
Science, 2003. URL https://rtraba.files.wordpress.com/2015/05/cppturing.pdf.
12
Published as a conference paper at ICLR 2022
Keith Wansbrough. Instance declarations are universal, 1998. URL https://www.lochan.org/
keith/publications/undec.html.
Adam B. Yedidia and S. Aaronson. A relatively small Turing machine whose behavior is independent of set
theory. ArXiv, abs/1605.04343, 2016.
13
Published as a conference paper at ICLR 2022
	exact	asymptotic
deterministic	X (Theorem 2)	✓ (§5; but no useful guarantee in finite time)
randomized	X (Theorem 4)	?; but X for rejection sampling (Theorem 5) and X for importance sampling (Theorem 6) when paired with autoregressive proposal distributions
Table A.1: Summary of negative results: neither deterministic or randomized algorithms can estimate EBM partition
functions accurately. On the other hand, popular sampling schemes such as rejection and importance sampling require
their autoregressive proposal distributions to be uncomputable.
A Summary of negative results
In Table A.1, we give an overview of the negative results in this paper: deficiencies of several estimators of
the partition function in an energy-based sequence model.
B Weighted Turing machines
We extend the definition of Turing machines into weighted Turing machines as follows. A weighted
Turing machine M is described as a 7-tuple (Q, Σ, δ, ^山比，F, UPdate_num, UPdate_denom), where the
first 5 components (Q, Σ, δ, qinn, F) have definitions following that of standard Turing machines. The
last two additions are weight-update functions: both UPdate_nUm and UPdate_denom have signature
Q → A, where A , {same, carry}.
At the end of time step 力，assuming the current state is q, we define
(NUmf-I	if UPdate_num(q) = same
NUmj =
2f + NUmf-1 if UPdate_nUm(q) = carry
and similarly,
deNomj-1	if UPdate_denom(q) = same
deNomj =	j
2j + deNomj-1 if UPdate_denom(q) = carry.
To keep our proof of model family capacity brief, we (arbitrarily) define that ∀q ∈ F ∪ {qinit},
UPdate_num(q) = UPdate_denom(q) = same. Finally, upon arriving at a halting state ∈ F in r time
steps, we say Num,/denom, is the weight of an input x ∈ B* under M, if Num,/denom, is a rational number.n
C DefiNitioN of seqUeNce model families aNd EC-complete parametric
families
Following Perez et al. (2021), we say a sequence model family is a set of seq-to-seq models
N = {Nθ : θ ∈ Θ ⊆ Q*}, where every N ∈ N recognizes language LG if and only if the seq-to-seq
model Ng, paired with embedding function f : B → Q, initial states S ∈ Q, and polytime
termination decision function g : Qd → B, accepts every string X ∈ LG. And we define NG to
accept x = [xι... xτ] ∈ B* if and only if there exists r ∈ N, such that NG upon input embeddings
[于(xι)…f (XT)], produces output embeddings yr at the r-th time step, where g(yr) = 1.
A sequence family Θ is EC-complete if given any / ∈ EC (as a description of a weighted Turing machine),
we can construct a parameter vector θ ∈ Θ such that there exist polytime functions WP : Qd → N ∪ {0}
and wq : Qd → N ∪ {0}, where whenever on input X = [xι... Xt], if g(yr) = 1 for some output
embeddings y「(that is, NG accepts X in r time steps),桃P(%)∕½⅛(%) = /(x).
11When Num"deNom尸 is not a rational number (because dENomr = 0), we say the weight of input string X is
undefined. However, we do not encounter that in this work: all strings in an EC language have rational weights. In any
case, we only require that our family contain weighted Turing machines that return rational numbers, not that they be
the only members.
14
Published as a conference paper at ICLR 2022
D Proofs of theorems from main text
Theorem 1.	The class of one-encoder-layer, four-decoder-layer Transformer networks with positional
encodings (n, 1/n, 1/n2,2n) is EC-complete.
Proof. To model all weighted Turing machines defined in §2.2, we extend the Turing-complete one-
encoder-layer three-decoder-layer Transformer network construction introduced by Perez et al. (2021) with
one additional layer. We also modify the original positional embedding function Pos: N → Q to include
a new component. In this proof, we let
Pos(Z) = [0,..., 0,1, i, 1/i, 1/i2,2']
instead of pos(i) = [0,..., 0,1, i, 1∕i, 1∕i2] as in Perez et al. (2021).
For the sake of clarity, our construction is a ‘stack-on’ construction: the encoder layer and the first 3 decoder
layers are largely identical to the design of Perez et al. (2021), with the only difference being necessary
changes to accommodate our one additional positional embeddings component. 12 It may be possible to
strengthen our results by showing that one-encoder-layer three-decoder-layer Transformer networks
with the original positional embeddings — the parametrization family Perez et al. (2021) showed to
be Turing-complete — are EC-complete as well, with a more involved construction. We leave such an
improvement as future work.
We claim our fourth layer of the decoder has output yr = y； + Wr, where
•	y； is a zero-padded version of the original Transformer output embeddings from Perez et al.
(2021). yr0 is of the form
[J∕ K,J∕ Im f, 0,...,0]
where JarK denotes an one-hot vector of size |01 where the qr-th component is 1 (again following
the notation of Perez et al. (2021)).
•	And Wr is of the form
[0g, 0s, 0, NuMr, DeNoMr, 0,..., 0]	(3)
where numr ∈ N ∪ {0} and denomr ∈ N ∪ {0} are defined in §2.2.
Now we describe how Wr can be computed from [y00 . . . yr0 ] using the attention mechanism, with the help
of our new positional embeddings. Specifically, we want to show that we can construct feedforward
networks 04, K4, and % such that
y = Att@(yO0),K4 (Y00),% (Y00))
where yi00 = yi0 + Pos(i), Y0 = [y10 . . . yi0], and Yi00 = Y0 + [Pos(1), . . . , Pos(i)]. We let
04(y00) = [0,..., 0,1,0,0,0,0] be a constant vector, and K4 (Y00) = Yθ0. Finally, we let V4 (Y00)=
心, 0s, 0,I(update_num(qi) = CARRy)2i ,I(update_denom(qi) = CARRy)2i, 0,..., 0].
04 is a constant function (such that it always attends to the unity component of Yi00), so it can be
implemented as a single-layer feedforward network. K4 is the identity function, which can also be
implemented as a single-layer feedforward network. V4 on the hand can be implemented as a fixed-size
feedforward network, with the piecewise-linear sigmoidal function ^: in the case of DeNoMi, the network
would first project qi to a = [I(update_denom(qi) = CaRRy),I(update.denom(qi) = same)] (using
the one-hot JqiK segment from y0), multiply it by b = [2i, 0] (with the help of nonlinearity from b), and
put a Θ b at the position of DeNoMi in equation (3). The component at NuMi in equation (3) can be
computed likewise.
12Since we increase the output embeddings’ dimension by 1, we also need to pad all matrices in the original
construction by additional zero columns/rows, such that our new positional embeddings’ new component has no effect
on any computation in the encoder layer, and the first 3 decoder layers.
15
Published as a conference paper at ICLR 2022
Given any position r ∈ N, we have
% = Att(Q4(M0),K4(YM (Y00)) = &,
0s,
0,
1
r
r
>,I(UPdate_num(/) = CARRy)2,,
i=1
1
r
r
>,I(UPdate.denom(qi) = CaRRy)2i,
i=1
0,
0].
Let extract_avg_num(yr) be an affine transformation that extracts the (∣Q| + ∣Σ∣ + 2)nd component
fromyr, and extract_avg_denom(yr) be an affine transformation that extracts the (|Q| + ∣Σ∣ + 3)rd
component from yr, we have
extract_avg_num(%)	2；=i I(UPdate.num(qi) = CaRRy)2i
extract_avg_denom(%)	Er=I I(UPdate.denom(qi) = CaRRy)2i
numr
denomr
which is the weight of input x = [ɪi... xτ ] as we defined in §2.2.	口
Theorem 2.	Let Θ be a parametric EC-complete sequence model family. And let。左 ⊂ Θ be bijective
with Gk. There is no k ∈ Q>o for which there exists an exact estimator 2左 that takes as input θ ∈ Θ左 as
input, and computes 2k (。) = Z& infinite time.
Proof. We can reduce Halt to computing our partition function. For the sake of contradiction, let us
assume that for some k ∈ Q>o, the exact estimator Zk exists. Our reduction from HaLt of input-free Turing
machines is as follows: Given any deterministic input-free Iuring machine M, We build a weighted
deterministic Turing machine: M0 (Appendix B). M0 takes as input X ∈ B*, and outputs weight 1∕3|x|+1 + k
when x encodes an accepting trace of M0. Otherwise, M0 outputs weight 1∕3|x l+1.
M0 always returns a weight for x in polytime. By the assumptions of EC-complete families (§2.4), we
can build a parameter vector θ ∈ Θ such that BM‘ = /e. Since from the definitions of Gk We know
DM0 =歹M,k, we have θ ∈ Θk.
We have thus completed our reduction: if Zk existed, we could decide whether any given input-less
deterministic Turing machine M halts, by first constructing the weighted Turing machine M0, then the
corresponding θ. By our assumption, Zk (/e) = k + 1 if and only if ∃x ∈ B* that is an accepting path of
M0, which is true if and only if M halts for some finite steps. Since whether M halts is undecidable, we
have arrived at a contradiction (TUring,1937; Sipser, 2013). Therefore for all k ∈ Q, the algorithm Zk does
not exist.	口
Theorem 3.	Assuming ZFC axioms and that assuming they is consistent, there exists an EC weighted
language b ∈ G1 such that (a) Z^ = C ∈ {1,2} exists, but (b) there is no proof of Z& = c.
Proof. Our proof hinges on Godel’s second incompleteness theorem: no consistent axiomatic system
which includes Peano arithmetic (e.g., ZFC) can prove its own consistency. We construct a Turing machine
Mb that enumerates all provable propositions under ZF, and halts if and only if Mb proves the proposition
1 = 0. One such Mb with 7,918 states has been built by Yedidia & Aaronson (2016)."
Mb is an input-free deterministic Turing machine. We construct a weighted Turing machine M0 from Mb,
13Subsequent efforts have led to a construction of Mb with 748 states: httPs://tUringmachinesimUlator.
com/shared/vgimygPUwi.
16
Published as a conference paper at ICLR 2022
in the manner of the proof of Theorem 2. We let M0 return weight 1 + 1∕3 |Xl+1, in the case Mb halts with
trace x. M0 returns a weight in polytime, and therefore defines a weighted language B ∈ Gi ⊂ EC. We
know from the definitions of Gi that Zb is either 1 or 2.
Assume to the contrary that there exists a proof that Zb = 1. Then we know Mb did not halt. And therefore
the proof would also imply that ZFC is consistent, which violates Godel’s second incompleteness theorem.
On the other hand, if there were a proof that Zb = 2, it would imply Mb halted and ZFC is not consistent.
We therefore arrive at a contradiction.
Lemma 1. Let Θ be an EC-complete parametric family. There is no multiPlicativefactor E ∈ Q>1 for
which every θ ∈ Θ can have its partitionfunction approximated With Ze (θ) within a factor of E - With
probability greater than 2∕3. That is, we cannot have
P ((1∕e) ZPe ≤ Ze (θ) ≤ EZA)) > 2/3.	⑵
Proof. In this proof, we make use of the distribution family Ge2 (§3.1). We assume to the contrary
that a multiplicative bound E satisfying equation (2) exists. Recall that our assumptions state that
P (1∕e ≤ Ze (θ)∕zpθ ≤ E) > 2∕3.14 LetM be an input-free Turing machine. And let∕⅛ = j)M,e2 ∈ Ge2 where
θ ∈ Θ.If 由eTuringmachineM halts,ZPM = 1+e2; therefore,P ((1 + E2)1∕e ≤ Ze (θ) ≤ (1 + E2)e) >
2∕3. Similarly, if M does not halt, we have ZP杨 = 1; therefore, P (1∕e ≤ Ze (θ) ≤ e) > 2∕3. By
combining the two conditions, we know that P (I(M halts)) = P (I(Ze (θ) ≥ E + 1∕e ∧ Ze (θ) > E))=
P(I(Ze (θ) ≥ E + 1∕e)).
Therefore, given (1∕e) ZPe ≤ Ze (θ) ≤ eZPe, we can decide if M halts by checking if Ze (θ) ≥ E + 1∕e.
Since the condition (1∕e) ZPe ≤ Ze (θ) ≤ eZPe only holds 2∕3 of all time, we then derandomize the
randomized Z e to get a deterministic estimator: recall that a randomized exact estimator finishes
computation in finite time, regardless of the content of the random tape τ. There, there are only finitely many
finitely long possible 'random' sequences that Ze will read, which we can enumerate. More concretely:
P(Ze (θ) ≥ E + 1∕e) = E [I(Ze (仇 τ) ≥ E + 1∕e)]
T
= 2 1∕2"e,e I(Ze (θ,τ) ≥ E + 1∕e)	(4)
T ∈Bme,β
where mg,e ∈ N is the maximum prefix length of the random tape that Ze (仇 τ) will use on any τ ∈ Bn.
Again mo, e is guaranteed to exist because of our assumption Ze (θ, τ) ends in finite time. Since equation (4)
is a finite sum of computable functions, it is also computable.
From the computability of equation (4), it follows that we can derive a deterministic algorithm from Ze
that decides whether an arbitrary input-free Turing machine halts, which is not possible. Therefore,
there is no E ∈ Q>1, such that the randomized exact estimator Z e satisfies the multiplicative bound
P(1∕e ≤ Ze (G)∕z ≤ E) > 2∕3 for all θ ∈ Θ.	□
Theorem 4.	Let Θ be an EC-complete parametric family. There is no multiplicative bound E ∈ Q>1 such
that there exists a randomized exact estimator Z e that guarantees 1∕e ≤ E[Z e(O)]∕z0e ≤ e ,for every
θ ∈ Θ where po is normalizable.
Proof. We define τ to be distributed according P「Therefore the mean E [Ze (θ)] can be expanded as
ET〜PT [Ze (θ)] = ET∈bn Pt (τ)Ze (θ,τ). Following the same derandomization technique in the proof of
Lemma 1, we can find some m ∈ N such that 2丁∈bn PT (τ)Ze (θ,τ) = 2丁∈b和 1∕2和Ze (θ,τ) in finite
time. And subsequently, we can compute E [Ze (θ)] exactly in finite time. Let the exact estimator be Ze.
We then have Ze (θ) = E [Ze (θ)].
14As is common, e.g., in defining the complexity class BPP (see Arora & Barak 2006), the fraction 2∕3 is arbitrary.
Any proportion bounded away from 1∕2 will work.
17
Published as a conference paper at ICLR 2022
Assuming to the contrary that We could guarantee 1/e ≤ Ze (θ) ≤ 6. Since Ze is a deterministic estimator,
We can write P(1/e ≤ Ze (θ) ≤ 6) = 1, which contradicts Lemma 1. Therefore such an estimator Ze (θ)
does not exist.
Theorem 5.	Using ZFC as our axiom set and assuming they are consistent, then there exists a normalizable
EC weighted language /, where there does not exist a consistent locally normalized proposal distribution
q ∈ LN, and Cq ∈ Q>o, such that it can be proven ∀x ∈ B*,户(x)/q (χ) < Cq.
Proof. By contradiction; let P = b, where b is first introduced in §3.2. Assume that there exists q ∈ LN
where it is proven that ∀x ∈ B*,户(χ)∕q(χ) < c < ∞, where C ∈ Q>o.
We can either prove ZFC is consistent, or is inconsistent, as follows:
1.	By our assumptions, q ∈ LN scores every string X as q.(x) f (θ) ∈ Q>o. Also, because q is
consistent and locally normalized, there exists n ∈ N such that we can prove ∀x ∈ X>n {x : X ∈
B*, |X| > n],q(x) < 1∕c (Proposition 1). We willjust prove the existence of n constructively by
enumerating the strings in B* in increasing length. Let the complement set X≤n = B* - X>n.
2.	The proof then examines the finitely many strings in X≤n.
(a)	If any of these strings X 0 has b (x 0) > 1, then we know X 0 encodes an accepting trace of Mb
(§3.2). Therefore, Mb halts, which implies there is a proof of the inconsistency of ZFC.
(b)	If none of these strings ∈ X≤n has b (x) > 1, then we know there is also no string X 00 ∈ X>n,
such that b (X 00) > 1. This is because of our assumption C ≥ b (χ)∕q(χ), which in turn means
that b(x) is less than 1 on these long strings X>n. Therefore, Mb does not halt, which
implies there is a proof of the consistency of ZFC.
Assuming ZFC is consistent, neither a proof of ZFC, or a disproof of ZFC, is possible. We have therefore
therefore arrived at a contradiction. Therefore, q does not exist.
Theorem 6.	Let Θ be an EC-complete parametric family. Assuming ZFC axioms and assuming they are
consistent, there exists θ ∈ Θ where there does not exist a consistent locally normalized “proposal”
distribution q ∈ LN SUCh that it can be proven Varq (Zq w I < C ≠ ∞, where C ∈ Q>o.
Proof. let P = b, where b is first introduced in §3.2. Assume that there exists q ∈ LN where it is proven
that Varq 闿3)=k < c ∈ Q<o ≠ ∞.
We have
where p.(x)，^l(X). After some manipulation, we have
Nk + Zj⅛2
Z加2
Σ
χ∈B*
Pj (X)
q (x)
| {z }
{z
=S ≤ Nk+1
Since ∀x ∈ B*, JqX)) ≥ 0, we have
∀x ∈ B*,
P. (X)一
一Lr ≤ s;
q(x)
(5)
18
Published as a conference paper at ICLR 2022
in particular, if x0 encodes a halting trace of Mb, from §3.1We know
(0、	PG(x0)
PG (x )=—一
4s
g+1
=1 + (3)
2
> 1/2.
(6)
Combining equations (5) and (6), we have for any halting trace x0 of Mb,
1
4q (χ 0)
< 里 ≤ s.
q (χ 0)
After some more arrangement,
q(x0) ≥ 4∑ ≥ 4(Nk + 1)
≥ 4(NC + 1) > 0.
As in the proof of Theorem 5, the existence of such a q allows us to either prove or disprove the consistency
of ZFC. For the sake of completeness, we include a proof sketch below:
1.	By our assumptions, q ∈ LN scores every string X as q.(x) f (θ) ∈ Q>0. Also, because q is
consistent and locally normalized, there exists n ∈ N such that We can prove ∀x ∈ X>n {x :
X ∈ B*, |x | > n},q(x) < 4(n：+.(Proposition 1). We will just prove the existence of n
constructively by enumerating the strings in B* in increasing length. Let the complement set
X≤n = B*-X>n.
2.	The proof then examines the finitely many strings in X≤n.
(a)	If any of these strings X 0 has PG (x 0) > 1, then we know X0 encodes an accepting trace of
Mb (§3.2). Therefore, Mb halts, which implies there is a proof of the inconsistency of ZFC.
(b)	If none of these strings ∈ X≤n has PG(x) > 1, then we know there is also no string
X00 ∈ X>n, such that PG(x00) > 1, since PG(X0) > 1 <^⇒ b(x0) > 1 <^⇒ Mb halts;
and we have already shown that for any accepting trace X0 of Mb, q(X0) ≥ 1∕4(nc+1).
Therefore, Mb does not halt, which implies there is a proof of the consistency of ZFC.
Assuming ZFC is consistent, neither a proof of ZFC, or a disproof of ZFC, is possible. We have therefore
therefore arrived at a contradiction. Therefore, such a q does not exist.
Theorem 7.	Let Θ be an EC-Completefamily. There is no algorithm that takes θ 1 ∈。，θ2 ∈ Θ as input,
and decides whether Pg、and pG2 are the Same weighted language.
Proof. Assuming to the contrary that such an algorithm Distinct exists. We would be able to reduce Halt
of input-free Turing machines to Distinct: we first construct θ 1 ∈ Θ such that PrGI (x) = 1∕3 |"l+1. We then
construct θ2 ∈ Θ such that Pg? = Pm,i ∈ Gi (§3.1). DistiNct(θ 1, θ2) is YES if and only if M halts. □
Theorem 8.	For any P ∈ EC and for any EC-complete family Θ, there is no algorithm Better / that
takes two parameter vectors θ 1 ∈ Θ, θ 2 ∈ Θ, and returns YES if KL( p∣∣ pg、) ≥ KL( p∣∣ pg2 ), and NO
otherwise, where P, PG1 , PG2 are string distributions defined by Pr, PrG1 , PrG2 respectively.
Proof. By contradiction; assume that for some P ∈ EC, BelTeR/ exists. We know there exists a weighted
Turing machine (also denoted as P) that on any string x, terminates in O(Poly(IX|)) and outputs P(x)
(Appendix B).
We show how we can reduce from HALt to BeITeR/. Given any arbitrary input-less Turing machine M,
we can define a new weighted Turing machine Pr0m that on input string x, Pr0m first simulates Pr on it and
keeps a record of Pr(x) somewhere on the tape. Pr0m then verifies whether x is an encoded accepting trace
of M. If x is indeed an encoded accepted trace of M, Pr0m outputs Pr(x) + 1. Otherwise Pr0m outputs Pr(x).
Let θ 1 ∈ Θ parametrize PM, and let θ2 ∈ Θ parametrize p, such that Pg、(x) = PM (x),P⅛2(x)=
Pr(x),∀x ∈ B*.
19
Published as a conference paper at ICLR 2022
We know that KL(p||P) = 0 for any distribution p. better/(θ 1, θ2) returns YES if and only if
KL(p||poi) = 0 ≥ KL(p||po1), which implies thatp5o1 and p5oγ define the same distribution, which in
turn implies that ∀x ∈ B*,pr(x) = p，M (χ), and that M never halts. Similarly, Better/ (θ 1, θ2) returns
NO if and only if M halts. We have thus completed our reduction from Halt; and therefore algorithm
Better/ does not exist for all P ∈ EC.	□
Theorem 9.	Let P be any LN weighted language. Any P ∈ EC where ∀x ∈ V*, p(x) ≤ P(x) has a
computable Z/.
Proof. We prove Theorem 9 by constructing an algorithm Z/ : Q>0 → Q ≥o that approximates Z/ within
any desired positive rational error e: namely |Z/ (E) 一 Z/1 ≤ E.
Let Zn = £7=0 2%：|X∣=ι p(x). We first observe that limn→∞ Zn = 1 by Proposition 1. In other words,
limn→∞ 1 一 Zn = 0, or equivalently, given any e > 0, there exists n ∈ N such that for all nf ≥ n,nf ∈ N,
(1 - Zn，)<E.
Therefore, given any e > 0, ∃n ∈ N that divides B* in two sets: X≥n = {x : X ∈ B*, ∣x∣ ≥ n}, where
∑x∈χ≥ p (x) < E, and X<n = B* - X≥n. We are guaranteed to find n by enumerating all candidates from
N.
We can thus implement Z/ as the following program: given E ∈ Q>0, we first find the smallest n ∈ N,
and partition B* into two sets X<n, X≥n as we described in the previous paragraph. We then return
ZP (E) =工 ∈X<n p(x).
We argue 由at ZP is a computable function. We first repeat 由at since n ∈ N exists, we will find n in finite
time, simply by enumerating. And since the set X<n ⊂ B* is finite, ZP (E) = 2*∈χ<n P(x) can computed
in finite time (under our assumption P ∈ EC, ∀x ∈ B*, P(x) can be computed in time O(poly(∣X∣))).
Since the program we described above terminates in finite time, ZP is a computable function.
What remains is to show that the approximation error ∣ZP - ZP(E)∣ is no greater than E; i.e., that
∣ Z P - ZP (E )∣≤ E.
It is easy to show that 0 ≤ ZP 一 ZP (e), after which we only need to show that ZP - ZP(e) ≤ E.
Expressing both terms as sums, we have that 2*∈b* P(x) 一 2*∈χ<n P(x) = 2*∈χ≥n P(x), which is a
sum of nonnegative terms and thus nonnegative.
To show that ZP - ZP (e) ≤ E, we express both terms as sums again:
y /(x)- 2 /(x) = 2 /(x).
x ∈B*	x ∈X<八	x ∈X≥ 八
By the definition of P we have 2*∈χ≥n P(x) ≤ 2*∈χ≥n P(x). The right-hand side is equal to 1 - Zn,
which by construction is less than e. Therefore, by substituting to get ZP - ZP (e) ≤ 1 - Zn and using the
transitive property of inequality, we have that ZP - ZP (e) < e. Combined with the previous paragraph’s
result, we have shown that ∣ZP - ZP(e)∣ ≤ E.
□
E Connection between expressive sequence distributions and Lin et al.
(2021)’s proof that EC ≠ ELN
Our construction of expressive sequence distributions is similar to Lin et al. (2021)’s construction of an EC
weighted language that is not in ELN. Whereas each weighted machine Pm,左 ∈ Gk corresponds to
execution traces of a deterministic Turing machine M, Lin et al. (2021) defined a single weighted language
PELN for all Turing machines. Fore the sake of completeness, we describe the definition of 歹ELN from Lin
et al. (2021) below. Let enc be a prefix-free encoding function that maps a deterministic Turing machine M
to a Boolean string where the function inverse enc-1 exists. And let PELN be a weighted language over
Boolean strings, where PrELN(x) = 1∕3M+1 if X is of the form X(1)X ⑵,where X(1) encodes some Turing
machine M, and x(2) encodes an accepting execution path of M on an empty input. (Such a path may be
20
Published as a conference paper at ICLR 2022
represented as a sequence of transitions of M that begins with an initial state and ends at an accepting
state.) Lin et al. (2021) then showed that prefix probabilities of ∕⅛ln
since HALt reduces to computing Z(x⑴).
are uncomputable under an ELN,
Theorem 9 implies that ∕⅛n We have just described above has a computable partition function:
Theorem 10.	Let Z be the partition function of 歹ELN. Z is computable.
Proof. Let P(x) = 1/3|x|+1. P ∈ ELN ⊂ LN because P(∙ | X) = 1/3 for all valid prefixes X. Since
∕eln ∈ EC by Lin et al. (2021, Theorem 5) and ∀x ∈ B*, PrELN(x) ≤ P(x), We have Z is computable by
Theorem 9.
Similarly, the ‘sparse version’ of PrELN introduced in (Lin et al., 2021, Theorem 6) can be shown to have a
computable partition function as well.
Since one of our goals is to clearly demonstrate that we can construct a single weighted language ∈ EC that
has an uncomputable partition function assuming ZFC (e.g., b in §3.2), We define weighted languages in
Gk to have at most one 'high' weight, as opposed to P in Lin et al. (2021, Theorem 5), where each X ⑴ that
encodes a halting machine has a ‘high weight’ suffix x(2) .
In fact, under our construction we can directly show EC ≠ ELN, using the uncomputability of b from
Theorem 3:
Corollary 1 (EC ≠ ELN under ZFC; slightly weaker version of Theorem 5 in Lin et al. (2021)). Assuming
the axiomatic system of ZFC, EC ≠ ELN.
Proof. We know there exists a weighted language b ∈ EC (§3.2) that has an uncomputable partition
function. Since b ∈ EC, b(x) ∈ Q≥o, ∀x ∈ B*. Therefore for all strings X ∈ B*, b(x) = b(X)/z石 is an
uncomputable number.
Assuming to the contrary that b ∈ ELN. By definition b(x) = ∏ b(xt | x<t), where each b(xt | X<) ∈
Q≥0 and is computable. Since the set of computable numbers is closed under multiplication, b(x) would
also be computable, which contradicts our assumption. Therefore br ≠ ELN, which implies EC ≠ ELN.
F NegAtive resuLts possibLe on finite pArAmeter subspAces
In §3.2, we mention that the pathological EBM b can show negative results regarding finite subsets of Θ.
Here, we provide a concrete example.
Corollary 2. Assuming ZFC axioms and assuming they are consistent, there exists θ ∈ Θ such that
ZOe ∈ Q>o exists, but there is no algorithm Zproof that takes θ ∈ Θ as input, and outputs a set of strings
{xn : n < N}, N ∈ N where
•	there is a proofthat £3 PG(x) ≥ 1∕2Z诩;or
•	there is a proofthat £3 Pa (x) < 1∕2Z诩.
Proof. Assuming to the contrary that Zproof existed. We construct θ ∈ Θ such that ∀x ∈ B* ,pg (x) = b (x).
Either proof resulting from Zproof(θ): {xn : n < N} can be used to prove or disprove the consistency of
ZFC.
Corollary 2 states that there exists a ‘problematic EBM’ — namely br — where we cannot guarantee to well
approximate its partition function, by accumulating finitely many string weights, regardless of the manner
of accumulation (i.e., how we choose strings) or the number of strings we enumerate over. We discuss this
in further details in §2.5.
G ImpossibiLity of modeL seLection in fixed-size TrAnsformer EBM fAmiLies
Theorem 11 is a sibling theorem to Theorem 8. Just as we show Zbr is uncomputable (§3.2), we can prove
that model selection is not only impossible for EC-complete parametric families (where the length of a
parameter vector is unbounded), but also impossible for fixed-size Transformer EBMs with a large enough
21
Published as a conference paper at ICLR 2022
embedding size.15
Theorem 11.	Assuming ZFC as our axiomatic system, for any / ∈ EC, there exists do ∈ N such thatfor
all d ≥ do, / can be captured by one-encoder-layer four-decoder-layer Transformer networks Θ( d')
(Theorem 1) with embedding size d, where there is no provably correct algorithm Better/ S that takes
two parameter vectors θ 1 ∈ Θ(rf), θ 2 ∈ Θ ⑷,and returns YES if KL (p || pgi) ≥ KL (p || pg2), andNO
otherwise, where p,pox ,p% are string distributions defined by p,po`, p的 respectively.
Proof. Let Mb be the input-free unweighted Turing machine We built in our proof of Theorem 3, whose
behavior is independent of ZFC. Let Mo be any weighted Turing machine that defines the EC weighted
language p. And let Mi be a weighted Turing machine that weights x as P(x) + 1 if and only if x encodes
an accepting trace of Mb; and Mi weights X as P(x) otherwise. Since checking whether X is a valid trace
of Mb is in O(Poly(IX|)), Mi defines an EC weighted language.
Let PMi be the string distribution defined by Mi. By an argument similar to our proof of Theorem 8, no
algorithm that is provably correct can decide if KL(p∣∣PmJ ≥ KL(p∣∣p).
We note that fr any weighted Turing machine M with fewer than n states, we can build another weighted
Turing machine M 0 which has n states, such that M and M 0 define the same weighted language, simply by
having (finitely many) additional unreachable states in M0. Since any weighted Turing machine with n
states can be implemented as a i-encoder-layer-4-decoder-layer Transformer networks with an embedding
size ∈ O(n), it follows that there exists do ∈ N such that both Mo and Mi can be encoded as parameter
vectors within a fixed-size model family with d ≥ do .
i5We use Θ(d') to denote a Transformer family with embedding size d.
22