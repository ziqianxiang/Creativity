Published as a conference paper at ICLR 2022
Pessimistic Bootstrapping for Uncertainty-
Driven Offline Reinforcement Learning
Chenjia Bai	Lingxiao Wang	Zhuoran Yang
Harbin Institute of Technology	Northwestern University	Princeton University
baichenjia255@gmail.com
Zhihong Deng	Animesh Garg	Peng Liu
University of Technology Sydney University of Toronto	Harbin Institute of Technology
Vector Institute, NVIDIA
Zhaoran Wang
Northwestern University
Ab stract
Offline Reinforcement Learning (RL) aims to learn policies from previously col-
lected datasets without exploring the environment. Directly applying off-policy
algorithms to offline RL usually fails due to the extrapolation error caused by
the out-of-distribution (OOD) actions. Previous methods tackle such problems
by penalizing the Q-values of OOD actions or constraining the trained policy to
be close to the behavior policy. Nevertheless, such methods typically prevent the
generalization of value functions beyond the offline data and also lack a precise
characterization of OOD data. In this paper, we propose Pessimistic Bootstrap-
ping for offline RL (PBRL), a purely uncertainty-driven offline algorithm without
explicit policy constraints. Specifically, PBRL conducts uncertainty quantifica-
tion via the disagreement of bootstrapped Q-functions, and performs pessimistic
updates by penalizing the value function based on the estimated uncertainty. To
tackle the extrapolating error, we further propose a novel OOD sampling method.
We show that such OOD sampling and pessimistic bootstrapping yields a provable
uncertainty quantifier in linear MDPs, thus providing the theoretical underpinning
for PBRL. Extensive experiments on D4RL benchmark show that PBRL has better
performance compared to the state-of-the-art algorithms.
1 Introduction
Deep Reinforcement Learning (DRL) (Sutton & Barto, 2018) achieves remarkable success in a va-
riety of tasks. However, in most successful applications, DRL requires millions of interactions with
the environment. In real-world applications such as navigation (Mirowski et al., 2018) and health-
care (Yu et al., 2019), acquiring a large number of samples by following a possibly suboptimal policy
can be costly and dangerous. Alternatively, practitioners seek to develop RL algorithms that learn
a policy based solely on an offline dataset, where the dataset is typically available. However, di-
rectly adopting online DRL algorithms to the offline setting is problematic. On the one hand, policy
evaluation becomes challenging since no interaction is allowed, which limits the usage of on-policy
algorithms. On the other hand, although it is possible to slightly modify the off-policy value-based
algorithms and sample solely from the offline dataset in training, such modification typically suffers
from a significant performance drop compared with their online learning counterpart (Levine et al.,
2020). An important reason for such performance drop is the so-called distributional shift. Specifi-
cally, the offline dataset follows the visitation distribution of the behavior policies. Thus, estimating
the Q-functions of the corresponding greedy policy with the offline dataset is biased due to the dif-
ference in visitation distribution. Such bias typically leads to a significant extrapolation error for
DRL algorithms since the estimated Q-function tends to overestimate the out-of-distribution (OOD)
actions (Fujimoto et al., 2019).
1
Published as a conference paper at ICLR 2022
To tackle the distributional shift issue in offline RL, previous successful approaches typically fall
into two categories, namely, policy constraints (Kumar et al., 2019; Fujimoto & Gu, 2021) and con-
servative methods (Kumar et al., 2020; Yu et al., 2020; 2021). Policy constraints aim to restrict
the learned policy to be close to the behavior policy, thus reducing the extrapolation error in pol-
icy evaluation. Conservative methods seek to penalize the Q-functions for OOD actions in policy
evaluation and hinge on a gap-expanding property to regularize the OOD behavior. Nevertheless,
since policy constraints explicitly confine the policy to be close to the behavior policy, such method
tends to be easily affected by the non-optimal behavior policy. Meanwhile, although the conserva-
tive algorithms such as CQL (Kumar et al., 2020) do not require policy constraints, CQL equally
penalizes the OOD actions and lacks a precise characterization of the OOD data, which can lead
to overly conservative value functions. To obtain a more refined characterization of the OOD data,
uncertainty quantification is shown to be effective when associated with the model-based approach
(Yu et al., 2020; Kidambi et al., 2020), where the dynamics model can be learned in static data thus
providing more stable uncertainty in policy evaluation. Nevertheless, model-based methods need
additional modules and may fail when the environment becomes high-dimensional and noisy. In
addition, the uncertainty quantification for model-free RL is more challenging since the Q-function
and uncertainty quantifier need to be learned simultaneously (Yu et al., 2021).
To this end, we propose Pessimistic Bootstrapping for offline RL (PBRL), an uncertainty-driven
model-free algorithm for offline RL. To acquire reliable Q-function estimates and their correspond-
ing uncertainty quantification, two components of PBRL play a central role, namely, bootstrapping
and OOD sampling. Specifically, we adopt bootstrapped Q-functions (Osband et al., 2016) for un-
certainty quantification. We then perform pessimistic Q-updates by using such quantification as a
penalization. Nevertheless, solely adopting such penalization based on uncertainty quantification is
neither surprising nor effective. We observe that training the Q-functions based solely on the offline
dataset does not regularize the OOD behavior of the Q-functions and suffers from the extrapolation
error. To this end, we propose a novel OOD sampling technique as a regularizer of the learned Q-
functions. Specifically, we introduce additional OOD datapoints into the training buffer. The OOD
datapoint consists of states sampled from the training buffer, the corresponding OOD actions sam-
pled from the current policy, and the corresponding OOD target based on the estimated Q-function
and uncertainty quantification. We highlight that having such OOD samples in the training buffer
plays an important role in both the Q-function estimation and the uncertainty quantification. We
remark that, OOD sampling controls the OOD behavior in training, which guarantees the stability of
the trained bootstrapped Q-functions. We further show that under some regularity conditions, such
OOD sampling is provably efficient under the linear MDP assumptions.
We highlight that PBRL exploits the OOD state-action pairs by casting a more refined penalization
over OOD data points, allowing PBRL to acquire better empirical performance than the policy-
constraint and conservatism baselines. As an example, ifan action lies close to the support of offline
data but is not contained in the offline dataset, the conservatism (Kumar et al., 2020) and policy
constraint (Fujimoto et al., 2019) methods tend to avoid selecting it. In contrast, PBRL tends to
assign a small Q-penalty for such an action as the underlying epistemic uncertainty is small. Hence,
the agent trained with PBRL has a higher chance consider such actions if the corresponding value
estimate is high, yielding better performance than the policy constraint and conservatism baselines.
Our experiments on the D4RL benchmark (Fu et al., 2020) show that PBRL provides reasonable
uncertainty quantification and yields better performance compared to the state-of-the-art algorithms.
2	Preliminaries
We consider an episodic MDP defined by the tuple (S, A, T, r, P), where S is the state space, A is
the action space, T ∈ N is the length of episodes, r is the reward function, and P is the transition
distribution. The goal of RL is to find a policy π that maximizes the expected cumulative rewards
E PiT=-01 γtri], where γ ∈ [0, 1) is the discount factor in episodic settings. The corresponding
Q-function of the optimal policy satisfies the following Bellman operator,
T Qθ (s,a) := r(s,a) + 7旧5，〜T (∙∣s,α)[ mɑ X Qθ- (s',a')].	(1)
where θ is the parameters of Q-network. In DRL, the Q-value is updated by minimizing the TD-
error, namely E(s,a,r,s0) [(Q-TQ)2]. Empirically, the target TQ is typically calculated by a separate
target-network parameterized by θ- without gradient propagation (Mnih et al., 2015). In online RL,
2
Published as a conference paper at ICLR 2022
one typically samples the transitions (s, a, r, s0) through iteratively interacting with the environment.
The Q-network is then trained by sampling from the collected transitions.
In contrast, in offline RL, the agent is not allowed to interact with the environment. The experiences
are sampled from an offline dataset Din = {(sti, ati, rti, sit+1)}i∈[m] . Naive off-policy methods such
as Q-learning suffer from the distributional shift, which is caused by different visitation distribution
of the behavior policy and the learned policy. Specifically, the greedy action a0 chosen by the target
Q-network in s0 can be an OOD-action since (s0 , a0) is scarcely covered by the dateset Din. Thus,
the value functions evaluated on such OOD actions typically suffer from significant extrapolation
errors. Such errors can be further amplified through propagation and potentially diverges during
training. We tackle such a challenge by uncertainty quantification and OOD sampling.
3 Pessimistic B ootstrapping for Offline RL
3.1	Uncertainty Quantification with B ootstrapping
In PBRL, we maintain K bootstrapped Q-functions in critic to quantify the epistemic uncertainty.
Formally, we denote by Qk the k-th Q-function in the ensemble. Qk is updated by fitting the
following target
k
TQθ (s, a) : = r(s, a) + YEsO〜P(∙∣s,a),a0〜∏(∙∣s)
Qθk- (s0, a0) .
(2)
Here we denote the empirical Bellman operator by T, which estimates the expectation
E[Qθk- (s0, a0) | s, a] empirically based on the offline dataset. We adopt such an ensemble technique
from Bootstrapped DQN (Osband et al., 2016), which is initially proposed for the online exploration
task. Intuitively, the ensemble forms an estimation of the posterior distribution of the estimated Q-
functions, which yields similar value on areas with rich data and diversely on areas with scarce data.
Thus, the deviation among the bootstrapped Q-functions yields an epistemic uncertainty estimation,
which we aim to utilize as a penalization in estimating the Q-functions. Specifically, we introduce
the following uncertainty quantification U(s, a) based on the Q-functions {Qk}k∈[K] ,
U(s, a)
Std
(Qk(s,a)) = ʌ/ɪ XK1 (Qk(s,a) — Q(s,
K k=1
a)2
(3)
Here We denote by Q the mean among the ensemble of Q-functions. From the Bayesian perspective,
such uncertainty quantification yields an estimation of the standard deviation of the posterior of Q-
functions. To better understand the effectiveness of such uncertainty quantification, We illustrate
With a simple prediction task. We use 10 neural netWorks With identical architecture and different
initialization as the ensemble. We then train the ensemble With 60 datapoints in R2 plane, Where
the covariate x is generated from the standard Gaussian distribution, and the response y is obtained
by feeding x into a randomly generated neural netWork. We plot the datapoints for training and the
uncertainty quantification in Fig. 1(a). As shoWn in the figure, the uncertainty quantification rises
smoothly from the in-distribution datapoints to the OOD datapoints.
In offline RL, We perform regression (s, a) → TbQk (s, a) in Din to train the bootstrapped Q-
functions, Which is similar to regress x → y in the illustrative task. The uncertainty quantification
alloWs us to quantify the deviation of a datapoint from the offline dataset, Which provides more
refined conservatism compared to the previous methods (Kumar et al., 2020; Wu et al., 2019).
3.2	Pessimistic Learning
We noW introduce the pessimistic value iteration based on the bootstrapped uncertainty quantifica-
tion. The idea is to penalize the Q-functions based on the uncertainty quantification. To this end, We
propose the folloWing target for state-action pairs sampled from Din,
TinQk(s, a) ：= r(s, a) + γEs0〜P([s,。),。，〜∏(∙∣s) IQk- (s', a0) — βin M)- (s', a0) ],	(4)
Where Uθ- (s0, a0) is the uncertainty estimation at (s0, a0) based on the target netWork, and βin is a
tuning parameter. In addition, the empirical mean Es，〜P(,⑶。),。，〜∏(∙∣s) is obtained by sampling the
3
Published as a conference paper at ICLR 2022
(a) Uncertainty
(b) PBRL: overall architecture
Figure 1: (a) Illustration of the uncertainty estimations in the regression task. The white dots rep-
resent data points, and the color scale represents the bootstrapped-uncertainty values in the whole
input space. (b) Illustration of the workflow of PBRL. PBRL splits the loss function into two com-
ponents. The TD-error (in) represents the regular TD-error for in-distribution data (i.e., from the
offline dataset), and pseudo TD-error (ood) represent the loss function for OOD actions. In the
update of Q-functions, both losses are computed and summed up for the gradient update.
transition (s, a, s0) from Din and further sampling a0 〜π(∙ | s0) from the current policy π. We denote
by TinQk (s, a) the in-distribution target of Qk (s,a) and write Tin to distinguish the in-distribution
target from that of the OOD target, which we introduce in the sequel.
i
We remark that there are two options to penalize the Q-functions through the operator Tin . In
Eq. (4), we penalize the next-Q value Qθk- (s0, a0) with the corresponding uncertainty Uθ- (s0, a0).
Alternatively, we can also penalize the immediate reward by r(s, a) := r(s, a) - U(s, a) and use
r(s, a) in place of r(s, a) for the target. Nevertheless, since the datapoint (s, a) ∈ Din lies on rich-
data areas, the penalization Uθ(s, a) on the immediate reward is usually very small thus having less
effect in training. In PBRL, we penalize the uncertainty of (s0, a0) in the next-Q value.
Nevertheless, our empirical findings in §D suggest that solely penalizing the uncertainty for in-
distribution target is insufficient to control the OOD performance of the fitted Q-functions. To
enforce direct regularization over the OOD actions, we incorporate the OOD datapoints directly in
training and sample OOD data of the form (sood, aood) ∈ Dood. Specifically, we sample OOD states
from the in-distribution dataset Din. Correspondingly, we sample OOD actions aood by following
the current policy π(∙ | sood). We highlight that such OOD sampling requires only the offline dataset
Din and does not require additional generative models or access to the simulator.
It remains to design the target for OOD samples. Since the transition P(∙ | sood, aood) and reward
r(sood , aood ) are unknown, the true target of OOD sample is inapplicable. In PBRL, we propose a
novel pseudo-target for the OOD datapoints,
ToodQk(sood, aood) := Qk(sood, aood) - β°°d Uθ(sood, aood) ,	(5)
which introduces an additional uncertainty penalization Uθ(sood, aood) to enforce pessimistic Q-
function estimation, and βood is a tuning parameter. For OOD samples that are close to the in-
distribution data, such penalization is small and the OOD target is close to the Q-function estima-
tion. In contrast, for OOD samples that are distant away from the in-distribution dataset, a larger
penalization is incorporated into the OOD target. In our implementation, we introduce an addi-
tional truncation to stabilize the early stage training as max{0, T oodQθk(sood, aood)}. In addition,
we remark that βood is important to the empirical performance. Specifically,
•	At the beginning of training, both the Q-functions and the corresponding uncertainty quantifi-
cations are inaccurate. We use a large βood to enforce a strong regularization on OOD actions.
•	We then gradually decrease βood in the training process since the value estimation and uncer-
tainty quantification becomes more accurate in training. We remark that a smaller βood requires
more accurate uncertainty estimate for the pessimistic target estimation Tb oodQk . In addition,
a decaying parameter βood stabilizes the convergence of Qθk (sood, aood) in the training from
the empirical perspective.
4
Published as a conference paper at ICLR 2022
Incorporating both the in-distribution target and OOD target, we conclude the loss function for critic
in PBRL as follows,
Lcritic= E(s,a,r,s，)〜Dhl [(T inQk - Qk )2] + E s』〜加“』〜π [(ToodQk - Qk)2] ,	(6)
where we iteratively minimize the regular TD-error for the offline data and the pseudo TD-error for
the OOD data. Incorporated with OOD sampling, PBRL obtains a smooth and pessimistic value
function by reducing the extrapolation error caused by high-uncertain state-action pairs.
Based on the pessimistic Q-functions, we obtain the corresponding policy by solving the following
maximization problem,
∏Ψ := maxEs〜Din,a〜Π(∙∣s) LjminK Qk(s, a)],
(7)
where φ is the policy parameters. Here We follow the previous actor-critic methods (Haarnoja et al.,
2018; Fujimoto et al., 2018) and take the minimum among ensemble Q-functions, which stablizes
the training of policy network. We illustrate the overall architecture of PBRL in Fig. 1(b).
3.3	Theoretical Connections to LCB -penalty
In this section, we show that the pessimistic target in PBRL aligns closely with the recent theoretical
investigation on offline RL (Jin et al., 2021; Xie et al., 2021a). From the theoretical perspective, an
appropriate uncertainty quantification is essential to the provable efficiency in offline RL. Specifi-
cally, the ξ-uncertainty quantifier plays a central role in the analysis of both online and offline RL
(Jaksch et al., 2010; Azar et al., 2017; Wang et al., 2020a; Jin et al., 2020; 2021; Xie et al., 2021a;b).
Definition 1 (ξ-Uncertainty Quantifier (Jin et al., 2021)). The set of penalization {Γt}t∈[T] forms a
ξ-Uncertainty Quantifier if it holds with probability at least 1 - ξ that
|TbVt+1(s, a) - T Vt+1(s, a)| ≤ Γt(s, a)
for all (s, a) ∈ S × A, where T is the Bellman equation and Tb is the empirical Bellman equation
that estimates T based on the offline data.
In linear MDPs (Jin et al., 2020; Wang et al., 2020a; Jin et al., 2021) where the transition kernel and
reward function are assumed to be linear to the state-action representation φ : S × A → Rd, The
following LCB-penalty (Abbasi-Yadkori et al., 2011; Jin et al., 2020) is known to be a ξ-uncertainty
quantifier for appropriately selected {βt}t∈[T] ,
Γlcb(st, at) = βt∙ [φ(st, at)>Λ-1φ(st, at)] 1/2,	(8)
where Λt = Pm=I φ(st, at)φ(st, at)> + λ ∙ I accumulates the features of state-action pairs in Din
and plays the role of a pseudo-count intuitively. We remark that under such linear MDP assump-
tions, the penalty proposed in PBRL and Γlcb(st, at) in linear MDPs is equivalent under a Bayesian
perspective. Specifically, we make the following claim.
Claim 1. In linear MDPs, the proposed bootstrapped uncertainty βt ∙ U (st, at) is an estimation to
the LCB-penalty Γlcb(st, at) in Eq. (8) for an appropriately selected tuning parameter βt.
We refer to §A for a detailed explanation and proof. Intuitively, the bootstrapped Q-functions esti-
mates a non-parametric Q-posterior (Osband et al., 2016; 2018a). Correspondingly, the uncertainty
quantifier U(st, at) estimates the standard deviation of the Q-posterior, which scales with the LCB-
penalty in linear MDPs. As an example, we show that under the tabular setting, Γlcb(st, at) is
approximately proportional to the reciprocal pseudo-count of the corresponding state-action pair in
the dataset (See Lemma 2 in §A). In offline RL, such uncertainty quantification measures how trust-
worthy the value estimations on state-action pairs are. A low LCB-penalty (or high pseudo-count)
indicates that the corresponding state-action pair aligns with the support of offline data.
Under the linear MDP or Bellman-consistent assumptions, penalizing the estimated value function
based on the uncertainty quantification is known to yield an efficient offline RL algorithm (Jin et al.,
2021; Xie et al., 2021a;b). However, due to the large extrapolation error of neural networks, we find
that solely penalizing the value function of the in-distribution samples is insufficient to regularize
the fitted value functions of OOD state-action pairs.
5
Published as a conference paper at ICLR 2022
A key to the success of linear MDP algorithms (Jin et al., 2020; Wang et al., 2020a; Jin et al., 2021)
is the extrapolation ability through L2-regularization in the least-squares value iteration (LSVI),
which guarantees that the linear parameterized value functions behave reasonably on OOD state-
action pairs. From a Bayesian perspective, such L2-regularization enforces a Gaussian prior on the
estimated parameter of linear approximations, which regularizes the value function estimation on
OOD state-action pairs with limited data available. Nevertheless, our empirical study in §D shows
that L2-regularization is not sufficient to regularize the OOD behavior of neural networks.
To this end, PBRL introduces a direct regularization over an OOD dataset. From the theoretical
perspective, we observe that adding OOD datapoint (sood, aood, y) into the offline dataset leads to an
equivalent regularization to the L2-regularization under the linear MDP assumption. Specifically, in
linear MDPs, such additional OOD sampling yields a covariate matrix of the following form,
Λe = Xim=1 φ(sit, ait)φ(sit, ait)> + X(sood,aood,y)∈Dood φ(sood, aood)φ(sood, aood)>,	(9)
where the matrix Λ0od = P(§ood °ood y)∈D0Od φ(sood, aood)φ(sood, aood)> plays the role of the λ ∙ I prior
in LSVI. It remains to design a proper target y in the OOD datapoint (sood, aood, y). The following
theorem show that setting y = TVh+1(sood, aood) leads to a valid ξ-uncertainty quantifier under the
linear MDP assumption.
Theorem 1. Let Λood 占 λ ∙ I. Under the linear MDP assumption, for all the OOD datapoint
(sood, aood, y) ∈ Dood, ifwe set y = T Vt+ι(sood, aood), it then holds for βt =O(T ∙ √d ∙ log (T∕ξ))
thatΓltcb(st, at) = βt φ(st, at)>Λt-1φ(st,at )1/2 forms a valid ξ-uncertainty quantifier.
We refer to §A for a detailed discussion and proof. Theorem 1 shows that if we set y =
TVt+1 (sood, aood), the bootstrapped uncertainty based on disagreement among ensembles is a valid
ξ-uncertainty quantifier. However, such an OOD target is impossible to obtain in practice as it re-
quires knowing the transition at the OOD datapoint (sood, aood). In practice, ifTD error is sufficiently
minimized, then Q(sood, aood) should well estimate the target T Vt+1. Thus, in PBRL, we utilize
y = Q(sood, aood) -Γlcb(sood,aood)	(10)
as the OOD target, where we introduce an additional penalization Γlcb(sood, aood) to enforce pes-
simism. In addition, we remark that in theory, we require that the embeddings of the OOD sample
are isotropic in the sense that the eigenvalues of the corresponding covariate matrix Λood are lower
bounded. Such isotropic property can be guaranteed by randomly generating states and actions. In
practice, we find that randomly generating states is more expensive than randomly generating ac-
tions. Meanwhile, we observe that randomly generating actions alone are sufficient to guarantee
reasonable empirical performance since the generated OOD embeddings are sufficiently isotropic.
Thus, in our experiments, we randomly generate OOD actions according to our current policy and
sample OOD states from the in-distribution dataset.
4	Related Works
Previous model-free offline RL algorithms typically rely on policy constraints to restrict the learned
policy from producing the OOD actions. In particular, previous works add behavior cloning (BC)
loss in policy training (Fujimoto et al., 2019; Fujimoto & Gu, 2021; Ghasemipour et al., 2021),
measure the divergence between the behavior policy and the learned policy (Kumar et al., 2019;
Wu et al., 2019; Kostrikov et al., 2021), apply advantage-weighted constraints to balance BC and
advantages (Siegel et al., 2020; Wang et al., 2020b), penalize the prediction-error of a variational
auto-encoder (Rezaeifar et al., 2021), and learn latent actions (or primitives) from the offline data
(Zhou et al., 2020; Ajay et al., 2021). Nevertheless, such methods may cause overly conservative
value functions and are easily affected by the behavior policy (Nair et al., 2020; Lee et al., 2021b).
We remark that the OOD actions that align closely with the support of offline data could also be
trustworthy. CQL (Kumar et al., 2020) directly minimizes the Q-values of OOD samples and thus
casts an implicit policy constraint. Our method is related to CQL in the sense that both PBRL and
CQL enforce conservatism in Q-learning. In contrast, we conduct explicit uncertainty quantification
for OOD actions, while CQL penalizes the Q-values of all OOD samples equally.
In contrast with model-free algorithms, model-based algorithms learn the dynamics model directly
with supervised learning. Similar to our work, MOPO (Yu et al., 2020) and MOReL (Kidambi
6
Published as a conference paper at ICLR 2022
et al., 2020) incorporate ensembles of dynamics-models for uncertainty quantification, and penalize
the value function through pessimistic updates. Other than the uncertainty quantification, previous
model-based methods also attempt to constrain the learned policy through BC loss (Matsushima
et al., 2020), advantage-weighted prior (Cang et al., 2021), CQL-style penalty (Yu et al., 2021), and
Riemannian submanifold (Tennenholtz et al., 2021). Decision Transformer (Chen et al., 2021) builds
a transformer-style dynamic model and casts the problem of offline RL as conditional sequence
modeling. However, such model-based methods may suffer from additional computation costs and
may perform suboptimally in complex environments (Chua et al., 2018; Janner et al., 2019). In
contrast, PBRL conducts model-free learning and is less affected by such challenges.
Our method is related to the previous online RL exploration algorithms based on uncertainty quan-
tification, including bootstrapped Q-networks (Bai et al., 2021; Lee et al., 2021a), ensemble dynam-
ics (Sekar et al., 2020), Bayesian NN (O’Donoghue et al., 2018; Azizzadenesheli et al., 2018), and
distributional value functions (Mavrin et al., 2019; Nikolov et al., 2019). Uncertainty quantification
is more challenging in offline RL than its online counterpart due to the limited coverage of offline
data and the distribution shift of the learned policy. In model-based offline RL, MOPO (Yu et al.,
2020) and MOReL (Kidambi et al., 2020) incorporate ensemble dynamics-model for uncertainty
quantification. BOPAH (Lee et al., 2020) combines uncertainty penalization and behavior-policy
constraints. In model-free offline RL, UWAC (Wu et al., 2021) adopts dropout-based uncertainty
(Gal & Ghahramani, 2016) while relying on policy constraints in learning value functions. In con-
trast, PBRL does not require additional policy constraints. In addition, according to the study in
image prediction with data shift (Ovadia et al., 2019), the bootstrapped uncertainty is more robust to
data shift than the dropout-based approach. EDAC (An et al., 2021) is a concurrent work that uses
the ensemble Q-network. Specifically, EDAC calculates the gradients of each Q-function and diver-
sifies such gradients to ensure sufficient penalization for OOD actions. In contrast, PBRL penalizes
the OOD actions through direct OOD sampling and the associated uncertainty quantification.
Our algorithm is inspired by the recent advances in the theory of both online RL and offline RL.
Previous works propose provably efficient RL algorithms under the linear MDP assumption for both
the online setting (Jin et al., 2020) and offline setting (Jin et al., 2021), which we follow for our
analysis. In addition, previous works also study the offline RL under the Bellman completeness
assumptions (Modi et al., 2021; Uehara et al., 2021; Xie et al., 2021a; Zanette et al., 2021) and the
model-based RL under the kernelized nonlinear regulator (KNR) setting (Kakade et al., 2020; Mania
et al., 2020; Chang et al., 2021). In contrast, our paper focus on model-free RL.
5	Experiments
In experiments, we include an additional algorithm named PBRL-prior, which is a slight modi-
fication of PBRL by incorporating random prior functions (Osband et al., 2018b). The random
prior technique is originally proposed for online exploration in Bootstrapped DQN (Osband et al.,
2016). Specifically, each Q-function in PBRL-Prior contains a trainable network Qθk and a prior
network pk , where pk is randomly initialized and is fixed in training. The prediction of each
Q-function is the sum of the trainable network and the fixed prior, Qpk = Qθk + pk , where Qθk
and pk shares the same network architecture. The random prior function increases the diver-
sity among ensemble members and improves the generalization of bootstrapped functions (Osband
et al., 2018b). We adopt SAC (Haarnoja et al., 2018) as the basic actor-critic architecture for both
PBRL and PBRL-prior. We refer to §B for the implementation details. The code is available at
https://github.com/Baichenjia/PBRL.
In D4RL benchmark (Fu et al., 2020) with various continuous-control tasks and datasets, we com-
pare the baseline algorithms on the Gym and Adroit domains, which are more extensively studied in
the previous research. We compare PBRL and PBRL-Prior with several state-of-the-art algorithms,
including (i) BEAR (Kumar et al., 2019) that enforces policy constraints through the MMD distance,
(ii) UWAC (Wu et al., 2021) that improves BEAR through dropout uncertainty-weighted update,
(iii) CQL (Kumar et al., 2020) that learns conservative value functions by minimizing Q-values of
OOD actions, (iv) MOPO (Yu et al., 2020) that quantifies the uncertainty through ensemble dynam-
ics in a model-based setting, and (v) TD3-BC (Fujimoto & Gu, 2021), which adopts adaptive BC
constraint to regularize the policy in training.
7
Published as a conference paper at ICLR 2022
BEAR	UWAC	CQL	MOPO^^	TD3-BC	PBRL PBRL-Prior			
					
日	HalfCheetah	2.3 ±0.0	2.3	±0.0	∣17.5 ±1.5	∣35.9	±2.9 H	Hopper	3.9 ±2.3	2.7	±0.3	7.9 ±0.4	16.7	±12.2 W	Walker2d	∣12.8 ±10.2	2.0	±0.4	5.1 ±1.3	4.2	±5.7	11.0 ±1.1	11.0 ±5.8		13.1 ±1.2	
	8.5 ±0.6	26.8 ±9.3		31.6 ±0.3	
	1.6 ±1.7	8.1 ±4.4		8.8 ±6.3	
日	HalfCheetah	43.0 ±0.2	42.2	±0.4	47.0	±0.5 ∣73.1 ±2.4 H	Hopper	51.8 ±4.0	50.9	±4.4	53.0	±28.5 38.3 ±34.9 N	Walker2d	-0.2 ±0.1	75.4	±3.0	73.3	±17.7 41.2 ±30.8	48.3 ±0.3	57.9 ±1.5		58.2 ±1.5	
	59.3 ±4.2	75.3 ±31.2		81.6 ±14.5	
	83.7 ±2.1	89.6 ±0.7		90.3 ±1.2	
日 AHaIfCheetah	36.3 ±3.1	35.9 ±3.7	45.5	±0.7 ∣69.2	±1.1 H H	Hopper	52.2	±19.3	25.3 ±1.7	88.7	±12.9 32.7	±9.4 N X	Walker2d	7.0	±7.8	23.6 ±6.9	∣81.8	±2.7 73.7	±9.4	44.6 ±0.5	45.1 ±8.0		49.5 ±0.8	
	60.9 ±18.8 100.6 ±1.0 100.7 ±0.4				
	81.8 ±5.5	77.7 ±14.5		86.2 ±3.4	
日 M HalfCheetah	46.0	±4.7	42.7	±0.3	75.6	±25.7	70.3	±21.9 H 荽 Hopper	50.6	±25.3	44.9	±8.1	105.6	±12.9	60.6	±32.5 S ω Walker2d	22.1	±44.9	96.5	±9.1	107.9	±1.6	77.4	±27.9	90.7 ±4.3	92.3 ±1.1		93.6 ±2.3	
	98.0 ±9.4	110.8 ±0.8 111.2 ±0.7			
	110.1 ±0.5	110.1 ±0.3 109.8 ±0.2			
H	HalfCheetah	92.7	±0.6	92.9	±0.6	∣96.3 ±1.3 81.3	±21.8 H	Hopper	54.6	±21.0 110.5	±0.5	96.5 ±28.0 62.5	±29.0	96.7 ±1.1	92.4 ±1.7		96.2 ±2.3	
	107.8 ±7	110.5 ±0.4 110.4 ±0.3			
Walker2d 106.6 ±6.8 108.4 ±0.4 108.5 ±0.5 62.4 ±3.2 ∣	110.2 ±0.3	108.3 ±0.3 108.8 ±0.2			
Average 38.78 ±10.0 50.41 ±2.7 67.35 ±9.1 53.3 ±16.3 67.55 ±3.8 ∣		74.37 ±5.3 ∣76.66 ±2.4			
Table 1: Average normalized score and the standard deviation of all algorithms over five seeds in
Gym. The highest performing scores are highlighted. The score of TD3-BC is the reported scores
in Table 7 of Fujimoto & Gu (2021). The scores for other baselines are obtained by re-training with
the ‘v2’ dataset of D4RL (Fu et al., 2020).
Results in Gym domain. The Gym domain includes three environments (HalfCheetah, Hop-
per, and Walker2d) with five dataset types (random, medium, medium-replay, medium-expert,
and expert), leading to a total of 15 problem setups. We train all the baseline algorithms in the
latest released ‘v2’ version dataset, which is also adopted in TD3-BC (Fujimoto & Gu, 2021)
for evaluation. For methods that are originally evaluated on the ‘v0’ dataset, we retrain with
their respective official implementations on the ‘v2’ dataset. We refer to §B for the training de-
tails. We train each method for one million time steps and report the final evaluation perfor-
mance through online interaction. Table 1 reports the normalized score for each task and the cor-
responding average performance. We find CQL and TD3-BC perform the best among all base-
lines, and PBRL outperforms the baselines in most of the tasks. In addition, PBRL-Prior slightly
outperforms PBRL and is more stable in training with a reduced variance among different seeds.
We observe that compared with the baseline algo-
rithms, PBRL has strong advantages in the non-
optimal datasets, including medium, medium-replay,
and medium-expert. In addition, compared with the
policy-constraint baselines, PBRL exploits the opti-
mal trajectory covered in the dataset in a theoreti-
cally grounded way and is less affected by the be-
havior policy. We report the average training curves
in Fig. 2. In addition, we remark that the perfor-
mance of PBRL and PBRL-Prior are weaker than
TD3-BC and CQL in the early stage of training, in-
dicating that the uncertainty quantification is inaccu-
rate initially. Nevertheless, PBRL and PBRL-Prior
converge to better policies that outperform the base-
Figure 2: Average training curve in Gym.
lines in the learning of uncertainty quantifiers, demonstrating the effectiveness of uncertainty penal-
ization and OOD sampling.
Results in Adroit domain. The adroit tasks are more challenging than the Gym domain in task
complexity. In addition, the use of human demonstration in the dataset makes the task even more
challenging in the offline setting. We defer the results to §E. We observe that CQL and BC have the
best average performance in all baselines, and PBRL outperforms baselines in most of the tasks.
8
Published as a conference paper at ICLR 2022
(a) Walker2d-Medium-Replay
Figure 3: The uncertainty and Q-value for different state-action sets in the training process.
Gradient Steps (thousands)
Walker2d-medium-v2 (Uncertainty)
(b) Walker2d-Medium
Uncertainty quantification. To verify the effectiveness of the bootstrapped uncertainty quantifi-
cation, we record the uncertainty quantification for different sets of state-action pairs in training. In
our experiments, we consider sets that have the same states from the offline dataset but with differ-
ent types of actions, including (i) aoffline , which is drawn from the offline in-distribution transition;
(ii) apolicy , which is selected by the training policy; (iii) arand , which is uniformly sampled from
the action space of the corresponding task; (iv) anoise1 = aoffline + N (0, 0.1), which adds a small
Gaussian noise onto the offline action to represent state-action pair that is close to in-distribution
data; and (v) anoise2 = aoffline + N(0, 0.5), which adds a large noise to represent the OOD action.
We compute the uncertainty and Q-value in ‘Walker2d’ task with two datasets (‘medium-replay’ and
‘medium’). The results are shown in Fig. 3. We observe that (i) PBRL yields large uncertainties for
(s, anoise2) and (s, arandom), indicating that the uncertainty quantification is high on OOD samples.
(ii) The (s, aoffline ) pair has the smallest uncertainty in both settings as it is an in-distribution sample.
(iii) The (s, anoise1) pair has slightly higher uncertainty compared to (s, aoffline), showing that the
uncertainty quantification rises smoothly from the in-distribution actions to the OOD actions. (iv)
The Q-function of the learned policy (s, apolicy) is reasonable and does not deviate much from
the in-distribution actions, which shows that the learned policy does not take actions with high
uncertainty due to the penalty with uncertainty quantification. In addition, we observe that there is no
superior maximum for OOD actions, indicating that by incorporating the uncertainty quantification
and OOD sampling, the Q-functions obtained by PBRL does not suffer from the extrapolation error.
Ablation study. In the following, we briefly report the result of the ablation study. We refer to
§C and §D for the details. (i) Number of bootstrapped-Q. We attempt different numbers K of
bootstrapped-Q in PBRL, and find the performance to be reasonable for K ≥ 6. (ii) Penalization
i
in Tin. We conduct experiments with the penalized reward discussed in §3.2 and find the penalized
i
reward does not improve the performance. (iii) Factor βin in Tin . We conduct experiments with
different βin from {0.1, 0.01, 0.001, 0.0001} to study the sensitiveness. Our experiments show that
PBRL performs the best for βin ∈ [0.0001, 0.01]. (iv) Factor βood. We use a large βood at the initial
training stage to enforce strong regularization over the OOD actions, and gradually decrease βood
in training. We conduct experiments by setting βood to different constants, and find our decaying
strategy generalizes better among tasks. (v) The learning target of OOD actions. We change the
learning target of OOD actions to the most pessimistic zero target y = 0 and find such a setting
leads to overly pessimistic value functions with suboptimal performance. (vi) Regularization types.
We conduct experiments with different regularization types, including our proposed OOD sampling,
L2-regularization, spectral normalization, pessimistic initialization, and no regularization. We find
OOD sampling the only reasonable regularization strategy and defer the complete report to §D.
6	Conclusion
In this paper, we propose PBRL, an uncertainty-based model-free offline RL algorithm. We pro-
pose bootstrapped uncertainty to guide the provably efficient pessimism, and a novel OOD sampling
technique to regularize the OOD actions. PBRL is closely related to the provable efficient offline RL
algorithm under the linear MDP assumption. Our experiments show that PBRL outperforms several
strong offline RL baselines in the D4RL environments. PBRL exploits the optimal trajectories con-
tained in the suboptimal dataset and is less affected by the behavior policy. Meanwhile, we show
that PBRL produces reliable uncertainty quantifications incorporated with OOD sampling.
9
Published as a conference paper at ICLR 2022
Reproducibility S tatement
The code of our work is available at https://github.com/Baichenjia/PBRL. The en-
vironments, datasets, and hyper-parameters of our experiments are are given in the appendix. All
settings, assumptions, lemmas, and theorems are proved and are discussed in detail in our appendix.
Acknowledgements
This work was supported in part by the National Natural Science Foundation of China under Grant
51935005, in part by the Fundamental Research Program under Grant JCKY20200603C010. The
authors thank the anonymous reviewers, whose invaluable comments and suggestions have helped
us to improve the paper.
References
Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic
bandits. In Advances in neural information processing Systems, volume 24, pp. 2312-2320, 2011.
Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G Bellemare.
Deep reinforcement learning at the edge of the statistical precipice. In Advances in neural infor-
mation processing systems, 2021.
Anurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and Ofir Nachum. Opal: Offline prim-
itive discovery for accelerating offline reinforcement learning. In International Conference on
Learning Representations, 2021.
Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline rein-
forcement learning with diversified q-ensemble. In Advances in neural information processing
systems, 2021.
Mohammad GheShIaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforce-
ment learning. In International Conference on Machine Learning, 2017.
Kamyar Azizzadenesheli, Emma Brunskill, and Animashree Anandkumar. Efficient exploration
through bayesian deep q-networks. In 2018 Information Theory and Applications Workshop (ITA),
pp. 1-9. IEEE, 2018.
Chenjia Bai, Lingxiao Wang, Lei Han, Jianye Hao, Animesh Garg, Peng Liu, and Zhaoran Wang.
Principled exploration via optimistic bootstrapping and backward induction. In International
Conference on Machine Learning, pp. 577-587. PMLR, 2021.
Catherine Cang, Aravind Rajeswaran, Pieter Abbeel, and Michael Laskin. Behavioral priors and
dynamics models: Improving performance and domain transfer in offline rl. arXiv preprint
arXiv:2106.09119, 2021.
Jonathan D Chang, Masatoshi Uehara, Dhruv Sreenivas, Rahul Kidambi, and Wen Sun. Mitigat-
ing covariate shift in imitation learning via offline data without great coverage. arXiv preprint
arXiv:2106.03207, 2021.
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter
Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning
via sequence modeling. In Advances in neural information processing systems, 2021.
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learn-
ing in a handful of trials using probabilistic dynamics models. Advances in Neural Information
Processing Systems, 31, 2018.
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep
data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.
Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. In
Advances in neural information processing systems, 2021.
10
Published as a conference paper at ICLR 2022
Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-
critic methods. In International Conference on Machine Learning, pp.1587-1596. PMLR, 2018.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pp. 2052-2062. PMLR, 2019.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In International Conference on Machine Learning, pp. 1050-1059.
PMLR, 2016.
Seyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu. Emaq: Expected-
max q-learning operator for simple yet effective offline and online rl. In International Conference
on Machine Learning, pp. 3682-3691. PMLR, 2021.
Florin Gogianu, Tudor Berariu, Mihaela C Rosca, Claudia Clopath, Lucian Busoniu, and Razvan
Pascanu. Spectral normalisation for deep reinforcement learning: An optimisation perspective.
In International Conference on Machine Learning, volume 139, pp. 3734-3744, 2021.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Confer-
ence on Machine Learning, pp. 1861-1870. PMLR, 2018.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. Journal of Machine Learning Research, 2010.
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-
based policy optimization. Advances in Neural Information Processing Systems, 32:12519-
12530, 2019.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement
learning with linear function approximation. In Conference on Learning Theory, pp. 2137-2143.
PMLR, 2020.
Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? In
International Conference on Machine Learning, pp. 5084-5096. PMLR, 2021.
Sham Kakade, Akshay Krishnamurthy, Kendall Lowrey, Motoya Ohnishi, and Wen Sun. Infor-
mation theoretic regret bounds for online nonlinear control. arXiv preprint arXiv:2006.12466,
2020.
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-
based offline reinforcement learning. In Advances in Neural Information Processing Systems,
volume 33, pp. 21810-21823, 2020.
Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement learning
with fisher divergence critic regularization. In International Conference on Machine Learning,
pp. 5774-5783. PMLR, 2021.
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy
q-learning via bootstrapping error reduction. In Advances in Neural Information Processing Sys-
tems, volume 32, pp. 11784-11794, 2019.
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline
reinforcement learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.),
Advances in Neural Information Processing Systems, volume 33, pp. 1179-1191, 2020.
Byungjun Lee, Jongmin Lee, Peter Vrancx, Dongho Kim, and Kee-Eung Kim. Batch reinforcement
learning with hyperparameter gradients. In International Conference on Machine Learning, pp.
5725-5735. PMLR, 2020.
Kimin Lee, Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Sunrise: A simple unified frame-
work for ensemble learning in deep reinforcement learning. In International Conference on Ma-
chine Learning, pp. 6131-6141. PMLR, 2021a.
11
Published as a conference paper at ICLR 2022
Seunghyun Lee, Younggyo Seo, Kimin Lee, Pieter Abbeel, and Jinwoo Shin. Offline-to-online
reinforcement learning via balanced replay and pessimistic q-ensemble. In Annual Conference on
Robot Learning, 2021b.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-
Fei, Silvio Savarese, Yuke Zhu, and Roberto Martin-Martin. What matters in learning from offline
human demonstrations for robot manipulation. In Annual Conference on Robot Learning, 2021.
Horia Mania, Michael I Jordan, and Benjamin Recht. Active learning for nonlinear system identifi-
cation with guarantees. arXiv preprint arXiv:2006.10277, 2020.
Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Ofir Nachum, and Shixiang Gu. Deployment-
efficient reinforcement learning via model-based offline optimization. In International Confer-
ence on Learning Representations, 2020.
Borislav Mavrin, Hengshuai Yao, Linglong Kong, Kaiwen Wu, and Yaoliang Yu. Distributional
reinforcement learning for efficient exploration. In International conference on machine learning,
pp. 4424—4434. PMLR, 2019.
Piotr Mirowski, Matt Grimes, Mateusz Malinowski, Karl Moritz Hermann, Keith Anderson, Denis
Teplyashin, Karen Simonyan, Andrew Zisserman, Raia Hadsell, et al. Learning to navigate in
cities without a map. Advances in Neural Information Processing Systems, 31:2419-2430, 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518:529-533, 2015.
Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal. Model-free
representation learning and exploration in low-rank MDPs. arXiv preprint arXiv:2102.07035,
2021.
Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement
learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.
Nikolay Nikolov, Johannes Kirschner, Felix Berkenkamp, and Andreas Krause. Information-
directed exploration for deep reinforcement learning. In International Conference on Learning
Representations, 2019.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped dqn. In Advances in Neural Information Processing Systems, volume 29, pp. 4026-
4034, 2016.
Ian Osband, John Aslanides, and Albin Cassirer. Randomized prior functions for deep reinforcement
learning. Advances in Neural Information Processing Systems, 31, 2018a.
Ian Osband, John Aslanides, and Albin Cassirer. Randomized prior functions for deep reinforcement
learning. Advances in Neural Information Processing Systems, 31, 2018b.
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D Sculley, Sebastian Nowozin, Joshua Dillon,
Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty? evaluating
predictive uncertainty under dataset shift. Advances in Neural Information Processing Systems,
32:13991-14002, 2019.
Brendan O’Donoghue, Ian Osband, Remi Munos, and Volodymyr Mnih. The uncertainty bellman
equation and exploration. In International Conference on Machine Learning, pp. 3836-3845,
2018.
12
Published as a conference paper at ICLR 2022
Shideh Rezaeifar, Robert Dadashi, Nino Vieillard, Leonard Hussenot, Olivier Bachem, Olivier
Pietquin, and Matthieu Geist. Offline reinforcement learning as anti-exploration. arXiv preprint
arXiv:2106.06431, 2021.
Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak.
Planning to explore via self-supervised world models. In International Conference on Machine
Learning,pp. 8583-8592. PMLR, 2020.
Noah Y Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Ne-
unert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. Keep doing what
worked: Behavioral modelling priors for offline reinforcement learning. In International Confer-
ence on Learning Representations, 2020.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Guy Tennenholtz, Nir Baram, and Shie Mannor. GELATO: geometrically enriched latent model for
offline reinforcement learning. CoRR, abs/2102.11327, 2021.
Masatoshi Uehara, Xuezhou Zhang, and Wen Sun. Representation learning for online and offline
RL in low-rank MDPs. arXiv preprint arXiv:2110.04652, 2021.
Ruosong Wang, Simon S Du, Lin F Yang, and Ruslan Salakhutdinov. On reward-free reinforce-
ment learning with linear function approximation. In Advances in neural information processing
systems, 2020a.
Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh S Merel, Jost Tobias Springenberg, Scott E
Reed, Bobak Shahriari, Noah Siegel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized
regression. Advances in Neural Information Processing Systems, 33, 2020b.
Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.
arXiv preprint arXiv:1911.11361, 2019.
Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua M. Susskind, Jian Zhang, Ruslan Salakhutdi-
nov, and Hanlin Goh. Uncertainty weighted actor-critic for offline reinforcement learning. In
International Conference on Machine Learning, volume 139, pp. 11319-11328, 2021.
Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent
pessimism for offline reinforcement learning. In Advances in neural information processing sys-
tems, 2021a.
Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu Bai. Policy finetuning: Bridg-
ing sample-efficient offline and online reinforcement learning. arXiv preprint arXiv:2106.04895,
2021b.
Lin Yang and Mengdi Wang. Sample-optimal parametric Q-learning using linearly additive features.
In International Conference on Machine Learning, pp. 6995-7004. PMLR, 2019.
Chao Yu, Jiming Liu, and Shamim Nemati. Reinforcement learning in healthcare: A survey. arXiv
preprint arXiv:1908.08796, 2019.
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea
Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. In Advances in Neural
Information Processing Systems, volume 33, pp. 14129-14142, 2020.
Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn.
Combo: Conservative offline model-based policy optimization. In Advances in neural information
processing systems, 2021.
Andrea Zanette, Martin J Wainwright, and Emma Brunskill. Provable benefits of actor-critic meth-
ods for offline reinforcement learning. Advances in neural information processing systems, 2021.
Wenxuan Zhou, Sujay Bajracharya, and David Held. Plas: Latent action space for offline reinforce-
ment learning. In Conference on Robot Learning, 2020.
13
Published as a conference paper at ICLR 2022
A Theoretical Proof
A.1 Background of LCB -penalty in linear MDPs
In this section, we introduce the provably efficient LCB-penalty in linear MDPs (Abbasi-Yadkori
et al., 2011; Jin et al., 2020; 2021). We consider the setting of γ = 1 in the following. In linear
MDPs, the feature map of the state-action pair takes the form of φ : S × A → Rd , and the transition
kernel and reward function are assumed to be linear in φ. As a result, for any policy π, the state-
action value function is also linear in φ (Jin et al., 2020), that is,
Qπ(sit,ait)=wbt>φ(sit,ait).	(11)
The parameter wt can be solved in the closed-form by following the Least-Squares Value Iteration
(LSVI) algorithm, which minimizes the following loss function,
m
wbt = mind X(φ(Si, ai )>w - r(St, at) - Vt+1(St+J)2 + λ ∙ kwk2,	(12)
where Vt+1 is the estimated value function in the (t + 1)-th step, and r(Sit, ait) + Vt+1(Sit+1) is the
target of LSVI. The explicit solution to (12) takes the form of
mm
Wt = Λ-1 X φ(st, at) (Vt+ι(st+ι) + r(st, at)), Λt = X φ(Si, at)φ(si, ai)> + λ ∙ I. (13)
i=1	i=1
Here Λt accumulate the state-action features from the training buffer. Based on the solution of wt,
the action-value function can be estimated by Qt(St, at) ≈ wbt>φ(St, at). In addition, in offline RL
with linear function assumption, the following LCB-penalty yields an uncertainty quantification,
Γlcb(st, at) = βt ∙ [φ(st, at)>Λ-1φ(st, at)] 1/2,
(14)
which measures the confidence interval of the Q-functions with the given training data (Abbasi-
Yadkori et al., 2011; Jin et al., 2020; 2021). In offline RL, the pessimistic value function Qt(St, at)
penalizes Qt by the uncertainty quantification Γlcb(St, at) as a penalty as follows,
Qt(St, at) = Qt(St, at) - Γ	(St, at)
= wt>φ(St, at) - Γlcb(St, at),
(15)
where wt is defined in Eq. (12). Under the linear MDP setting, such pessimistic value iteration
is known to be information-theoretically optimal (Jin et al., 2021). In addition, exploration with
Γlcb (St, at) as a bonus is also provably efficient in the online RL setting (Abbasi-Yadkori et al.,
2011; Jin et al., 2020).
A.2 CONNECTION BETWEEN THE BOOTSTRAPPED UNCERTAINTY AND Γlcb
In the sequel, we consider a Bayesian linear regression perspective of LSVI in Eq. (12). According
to the Bellman equation, the objective of LSVI is to approximate the Bellman target bit = r(Sit, ait) +
Vt+1(Sit+1) with the Q-function Qt, where Vt+1 is the estimated value function in the (t+ 1)-th step.
In linear MDPs, We parameterize the Q-function by Qt(St, at) = wbt>φ(St, at). We further define
the noise in this least-square problem as follows,
= bit - wt>φ(St, at),	(16)
where wt is the underlying true parameter. In the offline dataset with Din = {(Sit, ait, Sit+1)}i∈[m],
we denote by wbt the Bayesian posterior of w given the dataset Din . In addition, we assume that
We are given a Gaussian prior of the parameter w 〜N(0, I∕λ) as a non-informative prior. The
following Lemma establishes connections between bootstrapped uncertainty and the LCB-penalty
Γlcb.
Lemma 1 (Formal Version of Claim 1). We assume that follows the standard Gaussian distribution
N(0, 1) given the state-action pair (Sit, ait). It then holds for the posterior wt given Din that
Varwt (Qt(St at)) = Varwt (φ(st, at)>Wt) = φ(st, at)>Λ-1φ(st, at),	∀(st, at) ∈S× A. (17)
14
Published as a conference paper at ICLR 2022
Proof. The proof follows the standard analysis of Bayesian linear regression. Under the assumption
that E 〜N(0,1), We obtain that
bt∣(st,at),W 〜N(Wb>φ(st,at),l).	(18)
Recall that We have the prior distribution W 〜 N(0, I∕λ). Our objective is to compute the posterior
density wbt = w | Din . It holds from Bayes rule that
log p(wb | Din) = log p(wb) + log p(Din | wb) + Const.,	(19)
where p(∙) denote the probability density function of the respective distributions. Plugging (18) and
the probability density function of Gaussian distribution into (19) yields
m
log p(wb | Din) = -kwbk2/2 - X kwb>φ(sit, ait) - ytik2/2 + Const.
i=1
=-(W — μt)>Λ-1(Wb — μt)∕2 + Const.,
Where We define
mm
μt = Λ-1 X φ(st,at)yi,	Λt = X φ(si,ai)φ(st,at)> + λ ∙ I.	(20)
i=1	i=1
Then We obtain that Wbt = w | Din 〜N(μt, Λ-1). Itthenholdsforall (st, at) ∈ S × A that
Var(φ(si, at)>Wt) = Var(Qt(St, a：)) = φ(si, ai)τΛ-1φ(st, at),	(21)
which concludes the proof.	□
In Lemma 1, We shoW that the standard deviation of the Q-posterior is equivalent to the LCB-
penalty Var Q(st, at) = φ(st, at)τΛt-1φ(st, at) introduced in §A.1. Recall that our proposed
bootstrapped uncertainty takes the form of
U (St, at) ≈ Std(Qk (St, at)),	(22)
which is the standard deviation of the bootstrapped Q-functions. Such bootstrapping serves as an
estimation of the posterior of Q-functions (Osband et al., 2016). Thus, our proposed uncertainty
quantification can be seen as an estimation of the LCB-penalty under the linear MDP assumptions.
In addition, under the tabular setting where the states and actions are finite, the LCB-penalty takes a
simpler form, which we show in the following lemma.
Lemma 2. In tabular MDPs, the bootstrapped uncertaintyU(S, a) is approximately proportional to
the reciprocal-count of (S, a), that is,
U(S,a) ≈ rlcb(s,a)∕βt =	/ Q
Ns,a + λ
(23)
Proof. In tabular MDPs, we consider the joint state-action space d = |S| × |A|. Then j-th state-
action pair can be encoded as a one-hot vector as φ(S, a) ∈ Rd, where j ∈ [0, d- 1], thus is a special
case of the linear MDP (Yang & Wang, 2019; Jin et al., 2020). Specifically, we define
[0-∣	「0 …0 …0-
il ∈ Rd,	φ(sj,aj)φ(sj,aj)τ = 0 " 1	0 ∈ Rd×d,	(24)
.I	.....
0」	L0…0…0_
where the value of φ(Sj , aj ) is 1 at the j-th entry and 0 elsewhere. Then the matrix Λj
τ
Pm=0 φ(sj, aj)φ(sj, aj)τ + λ ∙ I is the sum of φ(sj, aj)φ(sj, aj)
takes the form of
over (Sj , aj ) ∈ Din, which
no+λ 0	…	0
0	nι+λ	…	0
Λj =	0	…%+λ 0	,	(25)
0	…	… nd-1+λ
15
Published as a conference paper at ICLR 2022
where the j-th diagonal element of Λj is the corresponding counts for state-action (sj , aj ), i.e.,
nj = Nsj,aj .
It thus holds that
[φ(sj ,aj ^A-1φ(Sj,aj )]1/2 = /N 1 q ∖,	(26)
Nsj ,aj + λ
which concludes the proof.	□
A.3 Regularization with OOD Sampling
In this section, we discuss how OOD sampling plays the role of regularization in RL, which regu-
larizes the extrapolation behavior of the estimated Q-functions on OOD samples.
Similar to §A.2, we consider the setup of LSVI-UCB (Jin et al., 2020) under linear MDPs. Specifi-
cally, we assume that the transition dynamics and reward function takes the form of
Pt(St+1 |	St, at)	= hψ(St+1),	φ(St, at)i,	r(St,	at)	= θ>φ(St, at),	∀(St+1, at, St)	∈ S × A ×	S,
(27)
where the feature embedding φ : S × A 7→ Rd is known. We further assume that the reward function
r : S × A 7→ [0, 1] is bounded and the feature is bounded by kφk2 ≤ 1. Given the dataset Din, LSVI
iteratively minimizes the least-square loss in Eq. (12). Recall that the explicit solution to Eq. (12)
takes the form of
mm
Wt = Λ- X φ(st, at)(Vt+ι(st+ι) + r(sit,时,Λt = X φ(s% aii)φ(sii, a；)> + λ ∙ I. (28)
i=1	i=1
We remark that for the regression problem in Eq. (12), the L2-regularize] λ Tlwk2 enforces a Gaus-
sian prior under the notion of Bayesian regression. Such regularization ensures that the linear func-
tion approximation φ>wti extrapolates well outside the region covered by the dataset Din.
However, as shown in §D, we observe that such L2-regularization is ineffective for offline DRL.
To this end, we propose OOD sampling in our proposed PBRL. To demonstrate the effectiveness of
OOD sampling as a regularizer, we consider the following least-squares loss with OOD sampling
and without the L2-regularizer,
Wt = min X(φ(st,at)>w - r(sii,ait) - VUI(St+ι))2 + X	(φ(sood,aood)>w -y)2.
w∈ i=1	(sood,aood,y)∈Dood
(29)
The explicit solution of Eq. (29) takes the form of
Wi = Λ-1 (X Φ(st,at)(r(si,ai) + 匕+1(st+1)) + X	φ(sood,aood)y) ,	(30)
i=1	(sood,aood,y)∈Dood
where
m
Λet =Xφ(Sit,ait)φ(Sti,ait)>+ X	φ(Sood,aood)φ(Sood,aood)>.	(31)
i=1	(sood,aood,y)∈Dood
Hence, ifwe further set y = 0 for all (Sood, aood, y) ∈ Dood, then (29) enforces a Gaussian prior with
the covariance matrix Λo-o1d, where we define
Λood =	φ(Sood,aood)φ(Sood,aood)>
(sood,aood,y)∈Dood
(32)
Specifically, if We further enforce Dood = {(sj, aj, 0)}j∈[d] with φ(sj, aj) = λ ∙ ej, where ej ∈ Rd
is the unit vector with the j-th entry equals one, it further holds that Λ00d = λ ∙ I and Eq. (29)
is equivalent to Eq. (12). In addition, under the tabular setting, by following the same proof as in
Lemma 2, having such OOD samples in the training is equivalent to setting the count in Eq. (26) to
be
Nesj,aj = Nsj,aj + Nsojo,daj,
16
Published as a conference paper at ICLR 2022
where Nsj,aj is the occurrence of (sj, aj) in the dataset and Nsojo,daj is the occurrence of (sj, aj) in
the OOD dataset.
However, to enforce such a regularizer without affecting the estimation of value functions, we need
to set the target y of the OOD samples to zero. In practice, we find such a setup to be overly
pessimistic. Since the Q-network is smooth, such a strong regularizer enforces the Q-functions to
be zero for state-action pairs from both the offline data and OOD data, as show in Fig. 12 and Fig. 13
of §C. We remark that adopting a nonzero OOD target y does not hinder the effect of regularization
as it still imposes the same prior in the covariate matrix Λet . However, adopting nonzero OOD target
may introduce additional bias in the value function estimation and the corresponding uncertainty
quantification. To maintain a consistent and pessimistic estimation of value functions, one needs to
carefully design the nonzero OOD target y.
To this end, we recall the definition of a ξ-uncertainty quantifier in Definition 1 as follows.
Definition 2 (ξ-Uncertainty Quantifier (Jin et al., 2021)). The set of penalization {Γt}t∈[T] forms a
ξ-Uncertainty Quantifier if it holds with probability at least 1 - ξ that
|Tb Vt+1 (s, a) - T Vt+1 (s, a)| ≤ Γt(s, a)
for all (s, a) ∈ S × A, where T is the Bellman operator and Tb is the empirical Bellman operator
that estimates T based on the data.
We remark that here we slightly abuse the notation T of Bellman operator and write TV (s, a) =
E[r(s, a)+V (s0) | s, a]. Under the linear MDP setup, the empirical estimation T Vt+1 is obtained via
fitting the least-squares loss in Eq. (29). Thus, the empirical estimation T Vt+1 takes the following
explicit form,
>
TVt+1(st, at) = φ(st,at)>wet,
where wet is the solution to the least-squares problem defined in Eq. (30). We remark that such ξ-
uncertainty quantifier plays an important role in the theoretical analysis of RL algorithms, both for
online RL and offline RL (Abbasi-Yadkori et al., 2011; Azar et al., 2017; Wang et al., 2020a; Jin
et al., 2020; 2021; Xie et al., 2021a;b). Our goal is therefore to design a proper OOD target y such
that we can obtain ξ-uncertainty quantifier based on the bootstrapped value functions. Our design is
motivated by the following theorem.
Theorem 2. Let Λ。。d占 λ ∙ I. For all the OOD datapoint (sood, aood, y) ∈ Dood, if we set y =
TVt+ι(sood, aood), it then holdsfor βt =O(T ∙ √d ∙ log(T∕ξ)) that
Γltcb(st, at) = βt φ(st, at)>Λt-1φ(st, at)1/2
forms a valid ξ-uncertainty quantifier.
Proof. Recall that we define the empirical Bellman operator Tb as follows,
>
TVt+1(st, at) = φ(st, at)>wet,
It suffices to upper bound the following difference between the empirical Bellman operator and
Bellman operator
T Vt+1(s, a) - TbVt+1(s, a) = φ(s, a)>(wt - wet).
Here we define wt as follows
wt = θ +	Vt+1(st+1)ψ(st+1)dst+1,	(33)
where θ and ψ are defined in Eq. (27). It then holds that
TVt+1 (s, a) - TbVt+1(s, a) = φ(s, a)>(wt - wet)
m
=φ(s,a)>wt -φ(s,a)>A-1Xφ(si,ai)(r(st,at) + Vt+ι(St+1))
i=1
-φ(S,a)>Λet-1	X	φ(Sood, aood)y.	(34)
(sood,aood,y)∈Dood
17
Published as a conference paper at ICLR 2022
where we plug the solution of wet in Eq. (30). Meanwhile, by the definitions of Λt and wt in Eq. (31)
and Eq. (33), respectively, we have
φ(s, a)>wt = φ(s, a)>Λet-1Λetwt
=φ(s,a)>Λet-1 Xm φ(sit,ait)TVt+1(st,at)+	X	φ(sood, aood)T Vt+1 (sood, aood) .
i=1	(sood,aood,y)∈Dood
(35)
Plugging (35) into (34) yields
T Vt+1 (s, a) - Tb Vt+1 (s, a) = (i) + (ii),	(36)
where we define
m
⑴=φ(s, a)>Λ-1 X φ(st, at)(TVt+ι(st,硝一r(st, at) - Vi+ι(st+ι)),
i=1
(ii) = φ(s,a)>Λ-1	X	φ(sood,aood)(T Vt+1(sood,aood)-y).
(sood,aood,y)∈Dood
Following the standard analysis based on the concentration of self-normalized process (Abbasi-
Yadkori et al., 2011; Azar et al., 2017; Wang et al., 2020a; Jin et al., 2020; 2021) and the fact that
Λood 占 λ ∙ I ,it holds that
|(i)| ≤ βt ∙ [Φ(st, at)>Λ-1φ(st, at)]1/2,	(37)
with probability at least 1 一 ξ, where βt = O(T ∙ √d ∙ log(T∕ξ)). Meanwhile, by setting y =
TVt+1 (sood, aood), it holds that (ii) = 0. Thus, we obtain from (36) that
|TVt+1(s,a) -TVt+1(s, a)| ≤ βt ∙ [φ(st,at)>Λ-1φ(st,at)]"	(38)
for all (s, a) ∈ S × A with probability at least 1 一 ξ.	□
Theorem 2 allows us to further characterize the optimality gap of the pessimistic value iteration. In
particular, the following corollary holds.
Corollary 1 (Jin et al. (2021)). Under the same conditions as Theorem 2, it holds that
T
V*(s1) — Vπ1(s1) ≤ XE∏* [Γtcb(st,at) | s1]
t=1
Proof. See e.g., Jin et al. (2021) for a detailed proof.	□
We remark that the optimality gap in Corollary 1 is information-theoretically optimal under the linear
MDP setup with finite horizon (Jin et al., 2021). Intuitively, for an offline dataset with sufficiently
good coverage on the optimal trajectories such as the experience from experts, such gap is small.
For a dataset collected from random policy, such a gap can be large. Our experiments also support
such intuition empirically, where the score obtained by training with the expert dataset is higher than
that with the random dataset.
Theorem 2 shows that if we set y = TVt+1(sood, aood), then our estimation based on disagreement
among ensembles is a valid ξ-uncertainty quantifier. However, such OOD target is impossible to
obtain in practice as it requires knowing the transition at the OOD datapoint (sood, aood). In practice,
ifTD error is sufficiently minimized, then Qt+1(s, a) should well estimate the target T Vt+1. Thus,
in practice, we utilize
y = Qt+1(sood, aood) 一 Γt+1(sood, aood)
as the OOD target, where we introduce an additional penalization Γt+1(sood, aood) to enforce the
pessimistic value estimation.
In addition, we remark that in theory, we require that the embeddings of the OOD sample are
isotropic, in the sense that the eigenvalues of the corresponding covariate matrix Λood are lower
18
Published as a conference paper at ICLR 2022
bounded. Such isotropic property can be guaranteed by randomly generating states and actions. In
practice, we find that randomly generating states is more expensive than randomly generating ac-
tions. Meanwhile, we observe that randomly generating actions alone are sufficient to guarantee
reasonable empirical performance since the generated OOD embeddings are sufficiently isotropic.
Thus, in our experiments, we randomly generate OOD actions according to our current policy and
sample OOD states from the in-distribution dataset.
B	Implementation Detail
B.1	Algorithmic Description
Algorithm 1 PBRL algorithm
1:	Initialize: K bootstrapped Q-networks and target Q-networks with parameter θ and θ-, policy
∏ with parameter 夕，and hyper-parameters /由，βood
2:	Initialize: total training steps H, current frame h = 0
3:	while h < H do
4:	Sample mini-batch transitions (s, a, r, s0) from the offline dataset Din
5:	# Critic Training for offline data.
6:	Calculate the bootstrapped uncertainty Uθ- (s0, a0) through the target-networks.
7:	Calculate the Q-target in Eq. (4) and the resulting TD-Ioss |Qk(s, a) - TInQk(s, a)|2.
8:	# Critic Training for OOD data
9:	Perform OOD sampling and obtains Nood OOD actions aood for each s.
10:	Calculate the bootstrapped uncertainty Uθ(s, aood) for OOD actions through Q-networks.
11:	Calculate the Q-target in Eq. (5) and the pseudo TD-loss ∣Qθ(s,aood)-T oodQk (s, aοοd)∣2.
12:	Minimize ∣Qk(s,a)-TinQk(s,a)∣2 + ∣Qθ(s,aood)-TοοdQk(s,aοοd)∣2 to train θ by SGD.
13:	# Actor Training
14:	Improve πφ by maximizing mink Qk(s, a∏) - logπψ(a∏|s) with entropy regularization.
15:	Update the target Q-network via θ- J (1 - T)θ- + τθ.
16:	end while
B.2	Hyper-parameters
Most hyper-parameters of PBRL follow the SAC implementations in https://github.com/
rail-berkeley/rlkit. We use the hyper-parameter settings in Table 2 for all the Gym domain
tasks. We use different settings of βin and βood for the experiment for Adroit domain and fix the
other hyper-parameters the same as Table 2. See §E for the setup of Adroit. In addition, we use the
same settings for discount factor, target network smoothing factor, learning rate, and optimizers as
CQL (Kumar et al., 2020).
Table 2: Hyper-parameters of PBRL
Hyper-parameters	Value	Description
K	10	The number of bootstrapped networks.
Q-network	FC(256,256,256)	Fully Connected (FC) layers with ReLU activations.
βin	0.01	in The tuning parameter of in-distribution target Tin .
βood	5.0 → 0.2	The tuning parameter of OOD target Tb ood . We perform linearly decay within the first 50K steps, and perform expo- nentially decay in the remaining steps.
τ	0.005	Target network smoothing coefficient.
γ	0.99	Discount factor.
lr of actor	1e-4	Policy learning rate.
lr of critic	3e-4	Critic learning rate.
Optimizer	Adam	Optimizer.
H	1M	Total gradient steps.
Nood	10	Number of OOD actions for each state.
19
Published as a conference paper at ICLR 2022
Baselines. We conduct experiments on D4RL with the latest ‘v2’ datasets. The dataset is released
at http://rail.eecs.berkeley.edu/datasets/offline_rl/gym_mujoco_v2_
old/. We now introduce the implementations of baselines. (i) The implementation of CQL
(Kumar et al., 2020) is adopted from the official implementation at https://github.com/
aviralkumar2907/CQL. In our experiment, we remove the BC warm-up stage since we
find CQL performs better without warm-up for ‘v2’ dataset. (ii) For BEAR (Kumar et al.,
2019), UWAC (Wu et al., 2021) and MOPO (Yu et al., 2020), we adopt their official implemen-
tations at https://github.com/aviralkumar2907/BEAR, https://github.com/
apple/ml-uwac, and https://github.com/tianheyu927/mopo, respectively. We
adopt their default hyper-parameters in training. (iii) Since the original paper of TD3-BC (Fujimoto
& Gu, 2021) reports the performance of Gym in ‘v2’ dataset in the appendix section, we directly cite
the reported scores in Table 1. The learning curve reported in Fig. 2 is trained by implementation
released at https://github.com/sfujim/TD3_BC.
Computational cost comparison. In the sequel, we compare the computational cost of PBRL
against CQL. We conduct such a comparison based on the Halfcheetah-medium-v2 task. We mea-
sure the number of parameters, GPU memory, and runtime per epoch (1K gradient steps) for both
PBRL and CQL in the training. We run experiments on a single A100 GPU. We summarize the re-
sult in Table 3. We observe that, similar to the other ensemble-based methods such as Bootstrapped
DQN (Osband et al., 2016), IDS (Nikolov et al., 2019), and Sunrize (Lee et al., 2021a), our method
requires extra computation to handle the ensemble of Q-networks. In addition, we remark that a
large proportion of computation for CQL is due to the costs of logsumexp over multiple sampled
actions (Fujimoto & Gu, 2021), which we do not require.
Table 3: Comparison of computational costs.
	Runtime (s/epoch)	GPU memory	Number of parameters
CQL	30.3	1.1G	0.42M
PBRL	52.1	1.7G	1.52M
B.3	Remarks on formulations of pessimism in actor and critic
We use different formulations to enforce pessimism in actor and critic. In the critic training, we
use the penalized Q-function as in Eq. (4) and Eq. (5). While in the actor training, we use the
minimum of ensemble Q-function as in Eq. (7). According to the analysis in EDAC [6], using the
minimum of ensemble Q-function minj=1,...,K Qj as the target is approximately equivalent to using
Q - βo ∙ Std(Qj) with a fixed βo as the target. In contrast, in the critic training of PBRL, We tune
different factors (i.e., βin and βood) for the in-distribution target and the OOD target, which yields
better performance for the critic estimation.
In the actor training, since we already have pessimistic Q-functions learned by the critic, it is not
necessary to enforce large penalties in the actor. To see such a fact, we refer to the ablation study
in Fig. 9, where utilizing the mean as the target achieves reasonable performances. We remark that
taking the minimum as the target avoids possible large values at certain state-action pairs, which
may arise due to the numerical computation in fitting neural networks. As suggested by our abla-
tion study, taking the minimum among the ensemble of Q-functions as the target achieves the best
performance. Thus, we use the minimum among the ensemble of Q-functions as the target in PBRL.
20
Published as a conference paper at ICLR 2022
C Ablation Study
In this section, we present the ablation study of PBRL. In the training, we perform online interac-
tions to evaluate the performance for every 1K gradient steps. Since we run each method for 1M
gradient steps totally, each method is evaluated 1000 times in training. The experiments follow such
evaluation criteria, and each curve is drawn by 1000 evaluated scores through online interaction.
Number of bootstrapped-Q. We conduct experiments with different numbers of bootstrapped
Q-networks in PBRL, and present the performance comparison in Fig. 4. We observe that the
performance of PBRL is improved with the increase of the bootstrapped networks. We remark that
since the training of offline RL is more challenging than online RL, it is better to use sufficient
bootstrapped Q-functions to obtain a reasonable estimation of the non-parametric Q-posterior. We
observe from Fig. 4 that using K = 6, 8, and 10 yields similar final scores, and a larger K leads to
more stable performance in the training process. We adopt K = 10 for our implementation.
K K =2	—*— K=4	―∙— K=6	▼ K=8	K K =10
Walker2d-medium-v2	hopper-medium-replay-v2
Ooooo
0 8 6 4 2
0	200	400	600	800	1000
Gradient Steps (thousands)
Figure 4:	The Ablation on the number of bootstrapped Q-functions.
Uncertainty of in-distribution target. We compare different kinds of uncertainty penalization in
i
Tin for in-distribution data. (i) Penalizing the immediate reward only. (ii) Penalizing the next-Q
value only, which is adopted in PBRL. (iii) Penalizing both the immediate reward and next-Q value.
We present the comparison in Fig. 5. We observe that the target-Q penalization performs the best,
and adopt such penalization in the proposed PBRL algorithm.
—Penalty only for reward —Penalty only for next-Q
Walker2d-medium-v2
80-
Ooo
6 4 2
①」Ox μ∣ψ
0-;____________________________________________
200	400	600	800	1000
00
—Penalty for both reward and next-Q
hopper-medium-replay-v2
100-
Oooo
8 6 4 2
200	400
600	800	1000
Gradient Steps (thousands)
Figure 5:	The Ablation on penalty strategy for the in-distribution data.
Tuning parameter βin. We conduct experiments with βin ∈ {0.1, 0.01, 0.001, 0.0001} to study
the sensitivity of PBRL to the tuning parameter βin for the in-distribution target. We present the
21
Published as a conference paper at ICLR 2022
comparison in Fig. 6. We observe that in the ‘medium’ dataset generated by a single policy, the
performance of PBRL is insensitive to βin. One possible reason is that since the ‘medium’ dataset is
generated by a single policy, the offline dataset tends to concentrate around a few trajectories and has
low uncertainty. Thus, the magnitude of βin has a limited effect on the penalty. Nevertheless, in the
‘medium-replay’ dataset, since the data is generated by various policies, the uncertainty of offline
data is larger than that of the ‘medium’ dataset (as shown in Fig. 3). Correspondingly, the perfor-
mance of PBRL is affected by the magnitude of βin. Our experiment shows that PBRL performs the
best for βin ∈ [0.0001, 0.01]. We adopt βin = 0.01 for our implementation.
→- βin =0.0001	→- βin = 0.001
Walker2d-medium-v2
①」Ous μ∣ψ
βin = 0.01	T- βin = 0.1
hopper-medium-replay-v2
Oooo
0 8 6 4
Gradient Steps (thousands)
Figure 6:	The Ablation on the tuning parameter βin in the in-distribution target.
Tuning parameter βood . We use a large βood initially to enforce strong OOD regularization in
the beginning of training, and then decrease βood linearly while the training evolves. We conduct
experiments with constant settings of βood ∈ {0.01, 0.1, 1.0}. We observe that in the ‘medium’
dataset, a large βood = 1.0 performs the best since the samples are generated by a fixed policy with a
relatively concentrated in-distribution dataset. Thus, the OOD samples tend to have high uncertainty
and are less trustworthy. In contrast, in the ‘medium-replay’ dataset, a small βood ∈ {0.1, 0.01}
performs reasonably well since the mixed dataset has larger coverage of the state-action space and
the uncertainty of OOD data is smaller than that of the ‘medium’ dataset. Thus, adopting a smaller
βood for the ‘medium-replay’ dataset allows the agent to exploit the OOD actions and gain better
performance. To match all possible situations, we propose the decaying strategy. Empirically, we
find such a decaying strategy performs well in both the ‘medium’ and ‘medium-replay’ datasets.
—Constant-0.01	—⅛- Constant-0.1 →- Constant-1.0
Decay (ours)
Oooo
8 6 4 2
①」Ous μ∣ψ
100-
80
hopper-medium-replay-v2
40-
20-
200	400
60-
^600	800	1000
Gradient Steps (thousands)
Figure 7:	The Ablation on the tuning parameter βood in the OOD target.
We also record the Q-value for (s, a)〜Din in the training process. As shown in Fig. 8, since both
the in-distribution actions and OOD actions are represented by the same Q-network, a large con-
22
Published as a conference paper at ICLR 2022
stant βood makes the Q-value for in-distribution action overly pessimistic and leads to sub-optimal
performance. It is desirable to use the decaying strategy for βood in practice.
Decay (ours)
hopper-medium-replay-v2
cons Constant-0.01	—⅛- ConStant-0.1	1*- ConStant-1.0
Walker2d-medium-v2
Ooooo
5 0 5 0 5
2 2 11
①UJO σ
Ooo
5 0 5
Gradient Steps (thousands)
Figure 8: With different settings of βood , we show the Q-value for state-action pairs sampled from
Din in the training process.
Actor training. We evaluate different actor training schemes in Eq. (7), i.e., the actor follows the
gradients of ‘min’ (in PBRL), ‘mean’ or ‘max’ value among K Q-functions. The result in Fig. 9
shows training the actor by maximizing the ‘min’ among ensemble-Q performs the best. In the
‘medium-replay’ dataset, since the uncertainty estimation is difficult in mixed data, taking ‘mean’
in the actor can be unstable in training. Taking ‘max’ in the actor performs worse in both tasks due
to overestimation of Q-functions.
—⅛- Actor: Max -Actor: Mean —Actor: Min
Walker2d-medium-v2
hopper-medium-replay-v2
Gradient Steps (thousands)
Figure 9: The Ablation on action selection scheme of the actor.
Number of OOD samples. The pessimism in critic training is implemented by sampling OOD
actions and then performing uncertainty penalization, as shown in Eq. (6). In each training step,
we perform OOD sampling and sample Nood actions from the learned policy. We perform an abla-
tion study with Nood ∈ {0, 2, 5, 10}. According to Fig 10, the performance is poor without OOD
sampling (i.e., Nood = 0). We find the performance becomes better with a small number of OOD
actions (i.e., Nood = 2). Also, PBRL is robust to different settings of Nood.
OOD Target. In §A.3, we show that setting the learning target of OOD samples to zero enforces a
Gaussian prior to Q-function under the linear MDP setting. However, in DRL, we find such a setup
leads to overly pessimistic value function and performs poorly in practice, as shown in Fig. 11.
Specifically, we observe that such a strong regularizer enforces the Q-functions to be close to zero
for both in-distribution and OOD state-action pairs, as shown in Fig. 12 and Fig. 13. In contrast,
23
Published as a conference paper at ICLR 2022
N-o- N-OOD: 0	N-OOD: 2	N-OOD: 5	→- N-OOD: 10
Gradient Steps (thousands)
Figure 10:	The Ablation on different number of OOD actions. the performance becomes better even
with very small amout of OOD actions.
our proposed PBRL target performs well and does not yield large extrapolation errors, as shown in
Fig. 12 and Fig. 13.
→- Zero target for OOD →- PBRL
Gradient Steps (thousands)
Figure 11:	The Ablation on OOD target with yood = 0 (normalized scores).
→- Zero target for OOD →- PBRL
Walker2d-medium-v2
hopper-medium-replay-v2

1000
1000
Gradient Steps (thousands)
Figure 12:	The Ablation on OOD target with yood = 0. Q-offline is the Q-value for (s, a) pairs
sampled from the offline dataset, where a follows the behavior policy.
24
Published as a conference paper at ICLR 2022
-A- Zero target for OOD -*- PBRL
Awod.Uno σ
Gradient Steps (thousands)
Figure 13:	The Ablation on OOD target with yood = 0. Q-CurrPolicy is the Q-value for (s, aπ)
pairs, where a∏ 〜π(a∣s) follows the training policy π.
25
Published as a conference paper at ICLR 2022
D Regularization for PBRL
In this section, we compare different regularization methods that are popular in other Deep Learning
(DL) literature to solve the extrapolation error in offline RL. Specifically, we compare the following
regularization methods.
•	None. We remove the OOD-sampling regularizer in PBRL and train bootstrapped Q-functions
i
solely based on the offline dataset through Tin .
•	L2-regularizer. We remove the OOD-sampling regularizer and use L2 normalization instead.
As we discussed in §3.3, in linear MDPs, LSVI utilizes L2 regularization to control the ex-
trapolation behavior of Q-functions in OOD samples. In DRL, we add the L2-norm of weights
in the Q-network in loss functions to conduct L2-regularization. We attempt two scale factors
{1e - 2, 1e - 4} in our experiments.
•	Spectral Normalization (SN). We remove the OOD-sampling regularizer and use SN instead.
SN constrains the Lipschitz constant of layers, which measures the smoothness of the neural
network. Recent research (Gogianu et al., 2021) shows that SN helps RL training when applied
to specific layers of the Q-network. In our experiment, we follow Gogianu et al. (2021) and
consider two cases, namely, applying SN in the output layer (denoted by SN[-1]) and applying
SN in both the output layer and the one before it (denoted by SN[-1,-2]), respectively.
•	Pessimistic Initialization (PI). Optimistic initialization is simple and efficient for RL explo-
ration, which initializes the Q-function for all actions with a high value. In online RL, such
initialization encourages the agent to explore all actions in the interaction. Motivated by this
method, we attempt a pessimistic initialization to regulate the OOD behavior of the Q-network.
In our implementation, we draw the initial value of weights and a bias of the Q-networks from
the uniform distribution Unif (a, b). We try two settings in our experiment, namely, (i) PI-small
that sets (a, b) to (-0.2, 0), and (ii) PI-large that sets (a, b) to (-1.0, 0). In both settings, we
remove the OOD sampling and use PI instead.
We illustrate (i) the normalized performance in the training process, (ii) the Q-value along the trajec-
tory of the training policy, and (iii) the uncertainty quantification along the trajectory of the training
policy. We present the results in Fig. 14, Fig. 15, and Fig. 16, respectively. In the sequel, we discuss
the empirical results.
•	We observe that OOD sampling is the only regularization method with reasonable performance.
Though L2-regularization and SN yield reasonable performance in supervised learning, they
do not perform well in offline RL.
•	In the ‘medium-replay’ dataset, we observe that PI and SN can gain some score in the early
stage of training. Nevertheless, the performance drops quickly along with the training process.
We conjecture that both PI and SN have the potential to be effective with additional parameter
tuning and algorithm design.
In conclusion, previous regularization methods for DL and RL are not sufficient to handle the dis-
tribution shift issue in offline RL. Combining such regularization techniques with policy constraints
and conservatism methods may lead to improved empirical performance, which we leave for future
research.
26
Published as a conference paper at ICLR 2022
Ay°d 3uo」」no ə-JOΦ-∙,> O
PI-Iarge	-A— PI-Small . SN[-1,-2] S SN[-1]	*	L 2(1e-4)	. L 2(1e-2) τ None P PBRL
ə,joos μ 寸P
Figure 14: Comparision of different regularizers (normalized score)
PI-Small ∙ SN[-1,-2]	—⅛— SN[-1]	* L 2(1e-4)	∙ L 2(1e-2)	▼ None	—⅛— PBRL
PI-Iarge
Figure 15: Comparision of different regularizers (Q-value along trajectories of the training policy)
hopper-medium-replay-v2
200
400
600
800
1000
-i— PI-Iarge -PI-Small	—SN[-1,-2]	—SN[-1]	—L2(1e-4)	—L2(1e-2)	—ψ- None
T- PBRL
walker2d-medium-v2
Ay°d 3uo」」no ə-JO-U一s-JΦOU∩
∞
O
3
25-
20-
15-
10-
5-
0-
200	400	600	800
hopper-medium-replay-v2
1000
Gradient Steps (thousands)
∞
O
5
∞
4
Figure 16: Comparision of different regularizers (Uncertainty along trajectories of the training
policy)
27
Published as a conference paper at ICLR 2022
E Experiments in Adroit Domain
In Adroit, the agent controls a 24-DoF robotic hand to hammer a nail, open a door, twirl a pen,
and move a ball, as shown in Fig. 17. The Adroit domain includes three dataset types, namely,
demonstration data from a human (‘human’), expert data from an RL policy (‘expert’), and fifty-fifty
mixed data from human demonstrations and an imitation policy (‘cloned’). The adroit tasks are more
challenging than the Gym domain in task complexity. In addition, the use of human demonstration
in the dataset also makes the task more challenging. We present the normalized scores in Table 4.
We set βin = 0.0001 and βood = 0.01. The other hyper-parameters in Adroit follows Table 2.
In Table 4, the scores of BC, BEAR, and CQL are adopted from the D4RL benchmark (Fu et al.,
2020). We do not include the scores of MOPO (Yu et al., 2020) as it is not reported and we cannot
find suitable hyper-parameters to make it work in the Adroit domain. We also attempt TD3-BC
(Fujimoto et al., 2018) with different BC weights and fail in getting reasonable score for most of
the tasks. In addition, since UWAC (Wu et al., 2021) has a different evaluation process, we re-train
UWAC with the official implementation and report the scores in Table 4.
In Table 4, we add the average score without ‘expert’ dataset since the scores of ‘expert’ dataset
dominate that of the rest of the datasets. We find CQL (Kumar et al., 2020) and BC have the best
average performance among all baselines. Meanwhile, we observe that PBRL slightly outperforms
CQL and BC. We remark that the human demonstrations stored in ‘human’ and ‘cloned’ are inher-
ently different from machine-generated ‘expert’ data since (i) the human trajectories do not follow
the Markov property, (ii) human decision may be affected by unobserved states such as prior knowl-
edge, distractors, and the action history, and (iii) different people demonstrate differently as their
solution policies. Such characteristic makes the ‘human’ and ‘cloned’ dataset challenging for of-
fline RL algorithms. In addition, we remark that the recent study in robot learning from human
demonstration also encounter such challenges (Mandlekar et al., 2021).
Figure 17: Illustration of tasks in Adroit domain.
	BC	BEAR	UWAC	CQL	MOPO	TD3-BC	PBRL
Pen	34.4	-1.0	10.1 ±3.2	37.5	-	0.0	35.4 ±3.3
Hammer	1.5	0.3	1.2 ±0.7	4.4	-	0.0	0.4 ±0.3
Door	0.5	-0.3	0.4 ±0.2	9.9	-	0.0	0.1 ±0.0
Relocate	0.0	-0.3	0.0 ±0.0	0.2	-	0.0	0.0 ±0.0
Pen	-56.9	26.5	-230 ±6.9	392	-	0.0	∣74.9 ±9.8
Hammer	0.8	0.3	0.4 ±0.0	2.1	-	0.0	0.8 ±0.5
Door	-0.1	-0.1	0.0 ±0.0	0.4	-	0.0	4.6 ±4.8
Relocate	-0.1	-0.3	-0.3 ±0.0	-0.1	-	0.0	-0.1 ±0.0
Pen	~~85.1	-105.9	―982 ±9.1	107.0	-	0.3	137.7 ±3.4
Hammer	125.6	127.3	107.7 ±21.7	86.7	-	0.0	127.5 ±0.2
Door	34.9	103.4	104.7 ±0.4	101.5	-	0.0	95.7 ±12.2
Relocate	101.3	98.6	105.5 ±3.2	95.0	-	0.0	84.5 ±12.2
Average	36.73	38.41	37.57 ±3.8	40.31	-	0.02	46.79 ±3.9
Average w/o expert	11.74	3.21	4.35 ±1.4	11.70	-	0.02	14.52 ±2.3
Table 4: Average normalized score over 3 seeds in Adroit domain. The highest performing scores
are highlighted. Among all methods, PBRL and CQL outperform the best of the baselines.
28
Published as a conference paper at ICLR 2022
F Reliable Evaluation for Statistical Uncertainty
Recent research (Agarwal et al., 2021) proposes reliable evaluation principles to address the statisti-
cal uncertainty in RL. Since the ordinary aggregate measures like mean can be easily dominated by
a few outlier scores, Agarwal et al. (2021) presents several efficient and robust alternatives that are
not unduly affected by outliers and have small uncertainty even with a handful of runs. In this paper,
We adopt these evaluation methods for each method in Gym domain with Mtask * Nseed runs.
•	Stratified Bootstrap Confidence Intervals. The Confidence Intervals (CIs) for a finite-sample
score estimates the plausible values for the true score. Bootstrap CIs with stratified sampling
can be applied to small sample sizes and is better justified than sample standard deviations.
•	Performance Profiles. Performance profiles reveal performance variability through score dis-
tributions. A score distribution shows the fraction of runs above a certain score and is given by
F(T) = F(T； x1:M,1:N) = M PM=I N PN=I l[xm,n ≥ T]∙
•	Aggregate Metrics. Based on bootstrap CIs, we can extract aggregate metrics from score distri-
butions, including median, mean, interquartile mean (IQM), and optimality gap. IQM discards
the bottom and top 25% of the runs and calculates the mean score of the remaining 50% runs.
Optimality gap calculates the amount of runs that fail to meet a minimum score of η = 50.0.
The result comparisons are give in Fig. 18 and Fig. 19. Specifically, Fig. 18 shows aggregate metrics
based on 95% bootstrap CIs, and Fig. 19 shows performance profiles based on score distribution.
For both evaluations, our PBRL and PBRL-prior outperforms other methods with small variability.
Median	IQM
PBRL-Prior	n
PBRL	I	I
TD3-BC	I	I
CQL	I	I
MOPO	I	I
UWAC I	I
BEAR ■ I	I
^40	60	80	—40	60	80
Optimality Gap
10	15	20
Figure 18: Aggregate metrics on D4RL with 95% CIs based on 15 tasks and 5 seeds for each task.
Higher mean, median and IQM scores, and lower optimality gap are better. The CIs are estimated
using the percentile bootstrap with stratified sampling. Our methods perform better than baselines.
Figure 19: Performance profiles on D4RL based on score distributions (left), and average score
distributions (right). Shaded regions show pointwise 95% confidence bands based on percentile
bootstrap with stratified sampling. The T value where the profiles intersect y = 0.5 shows the
median, and the area under the performance profile corresponds to the mean.
29