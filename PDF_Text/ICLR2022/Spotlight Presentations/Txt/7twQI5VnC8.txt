Published as a conference paper at ICLR 2022
Learning Causal Models from Conditional
Moment Restrictions by Importance Weight-
ING
Masahiro Kato1,2, Masaaki Imaizumi2, Kenichiro McAlinn3, Shota Yasui1, and Haruo Kakehi1
1AI Lab, CyberAgent, Inc.
2The University of Tokyo
3Temple University
Ab stract
We consider learning causal relationships under conditional moment restrictions.
Unlike causal inference under unconditional moment restrictions, conditional
moment restrictions pose serious challenges for causal inference, especially in high-
dimensional settings. To address this issue, we propose a method that transforms
conditional moment restrictions to unconditional moment restrictions through
importance weighting, using a conditional density ratio estimator. Using this
transformation, we successfully estimate nonparametric functions defined under
conditional moment restrictions. Our proposed framework is general and can
be applied to a wide range of methods, including neural networks. We analyze
the estimation error, providing theoretical support for our proposed method. In
experiments, we confirm the soundness of our proposed method.
1	Introduction
Consider learning the causal relationship between airline ticket prices and demand. As one might
expect, prices and demand rise and fall through the seasons, being affected by other events like
vacation periods, which are called confounders and may or may not be observable. Due to confounders,
naively inferring from this pattern that higher (lower) prices increase (decrease) demand would be
incorrect, and potentially detrimental. Thus, controlling for confounding effects is essential. This
issue frequently arises in practice, especially when learning causal (structural) relationships is
essential to answer counterfactual questions regarding policy intervention and outcome (Hansen,
2022).
One approach to deal with confounding effects (like in the airline example above) is the instrumental
variable (IV) approach (Wooldridge, 2002; Greene, 2003). In the IV approach, the conditional
moment restriction is defined as such that the causal model satisfies the restriction of zero expected
value given IVs, thus conditioning out the confounding effect. The simplest representation of this idea
is the two-step least squares (2SLS) for linear models (Wooldridge, 2002; Greene, 2003). However,
given the complex nature of the causal effect and confounding effect (and their relation), assuming
a linear relation can be too strong. Thus, in this paper, we focus on nonparametric IV (NPIV)
regressions, allowing for much more flexible estimation (Newey & Powell, 2003).
NPIV can be viewed as an instance of a more general framework of causal inference under conditional
moment restrictions. In this light, the machinery for inference under conditional moment restrictions
also applies to NPIV, as well as its shortcomings. One major issue with using the conditional moment
restrictions for causal inference is that one must approximate the conditional expectation, which
is often difficult to do (see, e.g., Newey (1993); Donald et al. (2003) for parametric and Newey &
Powell (2003); Ai & Chen (2003) for nonparametric IVs using sieves). For instance, Lewbel (2007)
and Otsu (2011) estimate the conditional expectation by local kernel density estimation. However,
local kernel density estimation suffers under high dimensionality. For this problem, recent methods
suggest the use of machine learning methods, such as neural networks (Hartford et al., 2017).
1
Published as a conference paper at ICLR 2022
In this paper, we propose transforming conditional moment restrictions into unconditional moment
restrictions by importance weighting using the conditional density ratio, which is defined as the ratio
of the conditional probability density, conditioned on the IVs, to the unconditional probability density.
We show that the unconditional expectation of a random variable weighted by the conditional density
ratio is equal to the conditional expectation. Further, we show that it is possible to estimate the
conditional density ratio with the least-squares method with a neural network. Once the conditional
density ratio is estimated, the usual method of moments, such as GMM, can be used straightforwardly.
The contribution of this paper is as follows: (i) we propose a novel approach to convert conditional
moment restrictions to unconditional moment restrictions by importance weighting; (ii) using our
proposed transformation, we develop methods for NPIV; (iii) we analyze the estimation error
2	Setup and notation
Among various problems of learning causal relationships from conditional moment restrictions, we
focus on the NPIV regression for ease of discussion. Note that our proposed method can be applied
to more general settings, similar to Ai & Chen (2003).
Suppose that the observations {(Yi, Xi, Zi)}in=1 are i.i.d., where Yi ∈ Y ⊆ R is an observable scalar
random variable, Xi ∈ X ⊆ RdX is a dX dimensional explanatory variable, Zi ∈ Z ⊆ RdZ is a
dZ dimensional random variable, called an IV, and X and Z are compact with nonempty interior.
We also assume that the probability densities of (Yi, Xi, Zi), (Yi, Xi), Zi exist and denote them by
p(y, x, z), p(y, x), andp(z), respectively. Let us define the causal relationships between Yi and Xi as
Yi = f *(Xi) + εi,
where f * : X → Y is a structural function, ε is the SUb-GaUssian error term with mean zero. To
learn f *, suppose the IV Zi satisfies the following conditional moment restrictions:
E [εi∣Zi] =0 ∀i ∈ {1, 2,...,n}.	(1)
Then, we also assume that under the conditional moment restriction, we can uniquely identify f*.
Our goal is to learn f* from the conditional moment restrictions in (1). If Zi = Xi , this problem
boils down to the estimation of the conditional expectation (regression function) E[Yi|Xi]. However,
when E[ε∕Xi] = 0, E[Yi∣Xi] is not equivalent to f *(Xi); that is, typical regression analysis, such as
least squares, may not return the correct estimate of f*(Xi).
3	Preliminaries and literature review
In this section, we briefly review causal inference under moment restrictions.
3.1	IV method for linear structural functions
One of the basic cases of using the IV is when f* is a linear model Xi>θ* with dX dimensional vector
θ * and the error term εi is correlated with the explanatory variable Xi . In this case, the parameter θ*
of the linear model can be estimated if there are IVs of dimension dX or more that satisfy the uncon-
ditional moment restrictions, E[Ziεi] = 0dZ , where 0d is a d dimensional zero vector. In the just-
identified case (dχ = dz), we can estimate θ* as Θiv = (n PZi ZiX>) 1 n Pn=ι ZiYi. In the
over-identified case (dX ≤ dZ), we can estimate it by the 2SLS. In the 2SLS, we first regress Xi by Zi;
then using a dχ dimensional function g(Zi) obtained from the first stage regression, we estimate θ* as
Θ2SLS = (n Pi=I XiX>)-11 P乙 XM，where Xd“ = Z>(^ j Z3Z>)-1 ɪ £着 ZjXdj
>
and Xi = (X1,i, . . . , XdX,i)> .
More generally, we can formulate the estimation of the linear structural function by uncondi-
tional moment restrictions defined using the IV; that is, E[m(θ*; Yi, Xi, Zi)] = 0dm, where
θ * ∈ Θ is a parameter representing the causal effect, Θ is the parameter space, and m :
Θ × R ×X ×Z → Rdm is a dm dimensional moment function. Then, the GMM estimator is defined as
Θgmm = argminθ∈θ (1 PNI m(θ Yi,Xi,Zi))> Wdm (1 PNI m(θ Yi, Xi, Zi)), where Wdm
is a dm X dm weight matrix. If Wdm is chosen as WdE H E[m(θ*; Yi, Xi, Zi)>m(θ*; Yi, Xi, Zi)],
2
Published as a conference paper at ICLR 2022
.1	. ∙	. A	♦ t'Γ' ∙ .	1 . 1	∙ . 1	.	1 ∙ . ∙	Tk T . . 1 . .1 y- ∙> » 1 K 1 ∙ 1 1 , 1
the estimator θGMM is efficient under the posited moment conditions. Note that the GMM includes the
2SLS as a special case where εi has the same variance among i ∈ {1, . . . , n} and the zero covariance.
Three methods have been proposed to obtain WdE ； two-step GMM, iterative GMM (Hayashi, 2000),
and continuous updating GMM (CU estimator; CUE, Hansen et al., 1996). These estimators have the
same asymptotic distribution, but different non-asymptotic properties. In particular, CUE is known as
a special case of the generalized empirical likelihood (GEL) estimator (Owen, 1988; Smith, 1997),
which plays an important role in estimation with many moment restrictions (Newey & Smith, 2004).
3.2	NPIV and learning from conditional moment restrictions
In linear structural functions, zero covariance between the IV and error term suffices for identification.
However, in nonparametric structural functions, we require a stronger restriction: the error term has
conditional expectation zero given the IVs. Then, we can characterize the solution of NPIV as that of
an integral equation K(Z) = E[Yi∣Zi = z] = E[f*(Xi)∣z] = R f *(x)dF(x|z) with F denoting the
conditional c.d.f of x given z. The identification also results in the uniqueness of the solution.
Several estimators have been proposed. Newey & Powell (2003) proposes a nonparametric analogue
of the 2SLS for linear structural functions. They first define the linear-in-parameter model as a model
to approximate the nonparametric structural function f*, where they use a linear approximation with
basis expansion, such as sieve (series) regression. Let us denote the approximation as f* (Xi) ≈
ψ(Xi)>θ*, where ψ : X → Rdψ, dψ ≤ n is a vector-valued function consisting of outputs of basis
functions. Then, they conduct the 2SLS as follows: (i) they define a linear-in-parameter model as an
approximation to the nonparametric model of Z%, and estimate E[ψ(Xi) |Zi] using that model; (ii) they
regress 匕 by E[ψ(Xi)|Zi] to estimate θ*. In contrast, Ai & Chen (2003) proposes a nonparametric
analogue of the GMM. Ai & Chen (2003) also approximate the nonparametric structural function
f* by a linear-in-parameter model. However, unlike Newey & Powell (2003), Ai & Chen (2003)
estimates E[(匕 一f (Xi))∣Zi], instead of estimating E[ψ(Xi)∣Zi], by another linear-in-parameter
model. Using the estimator of E[(Yi - f(Xi))|Zi], Ai & Chen (2003) transforms conditional moment
restrictions into unconditional ones and apply the GMM to estimate f*. In addition to these two
typical methods, Darolles et al. (2011), Carrasco et al. (2007a) propose their own methods for NPIV.
This is how the NPIV problem is typically cast into the framework of conditional moment restrictions,
where a parameter representing the causal relationship satisfies E[m(θ*; Yi, Xi, Zi)|Zi] = 0dm. As
with NPIV, we often define the moment function as a function of the nonparametric function f instead
of the parameter θ. We can estimate f * defined under conditional moment restrictions using variants
of GMM or empirical likelihood, e.g,, Ai & Chen (2003); DOmingUez & Lobato (2004); Otsu (2011);
Lewbel (2007), by transforming the conditional moment restrictions to unconditional ones.
3.3	Approaches in the machine learning literature
2SLS with more flexible models. Hartford et al. (2017) extends the two-stage approach by employ-
ing a neural network density estimator, and Singh et al. (2019) does it by conditional mean embedding
in RKHS. For example, in Hartford et al. (2017), they first approximate K(z) by estimating F(x|z)
with neural networks. Then, for B samples {Xj}B=ι generated from the estimator F(x|z), they train
neural networks by minimizing the empirical risk * Pni==ι (匕 一 B PX^i^(x3↑zi) f (Xj)).
In contrast, Xu et al. (2021a) first approximates f(Xi) by neural networks and regard the last layer as
ψ(Xi) in the 2SLS OfNPrV.Then, they predict ψ(Xi) by another model 夕(Zi). Because ψ(Xi) is
transitive in the learning process, they alternatively train the models 夕(Zi) and f (Xi).
Minimax approach. There has also been a recent surge in interest in minimax approaches that
reformulate conditional moment restrictions as a minimax optimization problem (Bennett et al., 2019;
Bennett & Kallus, 2020; Muandet et al., 2020; Chernozhukov et al., 2020; Liao et al., 2020). For this
approach, Dikkala et al. (2020) shows a strong theoretical guarantee on the estimation error bound.
3
Published as a conference paper at ICLR 2022
4	Proposed method: NPIV by importance weighting
Suppose that p(y, x) > 0 for all (y, x) ∈ Y × X. Note that we have already assumed the existence
of p(y, x|z) for all (y, x, z) ∈ Y × X × Z by assuming the existence of the conditional moment
restrictions. Define the conditional density ratio function r : Y × X × Z → (0, C) as
*/ I、_ p(y,xlz) _ p(y,χ,z)
r (y,x|Z) =	=	,
p(y, x)	p(y, x)p(z)
for 0 < C < ∞. We assume the existence of the conditional density ratio function.
Assumption 1. For all (y, x, z) ∈ Y ×X × Z ,the conditional density ratio r* (y, x|z) exists.
Then, we transform conditional moment restrictions into unconditional moment restrictions as
E[(Yi - f(Xi))|z]= Z (y - f(x))Ppyyay p(y,x)dydx = E [(Yi - f (Xi)∖r*(Yi,Xi∣z)].
Thus, if we know the conditional density ratio function r* (y, x|z), we can approximate E[(Yi -
f(Xi))|z] for z ∈ Z by the following sample average:
1n
-E(Yi — f(Xi))r*(K,Xi∣z)∙
n i=1
Based on this property, we propose the following two-stage method: (i) estimate the conditional
density ratio function r*; (ii) approximate conditional moment restrictions by the sample average of
Yi - f(Xi) using the estimate of the conditional density ratio function r*. Since we do not know
the true value of the conditional density ratio r* and cannot calculate the expected value, we consider
replacing r* with an estimator r and approximating the expected value with the sample mean.
4.1	Conditional density ratio estimation
First, we consider estimating r* (y, x|z). While it is possible to estimate the probability density func-
tions of the numerator and the denominator, individually, following Vapnik’s principle (Vapnik, 1998),
we should avoid solving more difficult intermediate problems. Sugiyama et al. (2012) summarizes
various methods to estimate the density ratio directly. Inspired by least-squares importance fitting
(LSIF) of Kanamori et al. (2009), we estimate the conditional density ratio by minimizing
r = arg min UEYX
r∈R 2	,
[Ez [(r*(Yi,Xi∣Zj) — r(Yi,Xi∣Zj))2]],
where R denotes a set of measurable functions and for a function g : Y × X × Z → R,
Eγ,x [Ez [g(Yi, Xi, Zj)]] denotes J g(y, x, z)p(y, X)P(Z)dydxdz. It is easy to confirm that r = r*
by taking the first derivative of the risk. Because this risk includes the unknown r*, it may seem
intractable objective function. However, we can obtain the risk that does not include r* as
argmin- (Eγ,χ [Ez [r*2(Yi,Xi|Zj) — 2r*(Yi,Xi∣Zj)r(γi,Xi∣Zj) + r2(Yi,Xi∣Zj)]])
r∈R 2
argmin [-Ey,x [Ez [r*(Yi,Xi∣Zj)r(Yi,Xi∣Zj)]] + -Eγ,χ [Ez Y(Yi,Xi∣Zj)]]
r∈R I	2
=argmin [-Eγ,x,z [r(Yi,Xi∣Zi)] + -Eγ,χ [Ez [r2(Yi,Xi∣Zj)]]].
r∈R I	2	J
Here,weusedEγ,χ [Ez 卜*(Yi,X∕Zj)r(Yi,X∕Zj)]] = Eγ,χ 忸Z [做：X)MZj)r(Yi,Xi∣Zj)口 =
EY,X,z [r(Yi, Xi|Zi)]. For some hypothesis class R, by approximating the risk with its sample
approximation, we estimate the conditional density ratio as
- n	-- n - n
r = argmin{ - - Er(Yi,乂邑)+ 2" E -ɪ^r (Yi,Xi|Zj) }■
r∈R	n i=1	2 n j=1 n i=1
For the hypothesis class R, we can use various models, such as linear-in-parameter models and neural
networks. Suzuki et al. (2008) also proposes a similar formulation based on maximum likelihood
estimation with constraints, which is not easy to solve with neural networks.
4
Published as a conference paper at ICLR 2022
4.2	NPIV regression by importance weighting
If We know the conditional density ratio function r*, We can obtain unconditional moment
restrictions Eγ,χ [(ρ(f; Yi,Xi,Zι,r*) ρ(f; Yi,Xi,Z∙2,r*) ∙ ∙ ∙ ρ(f; Yi, X，，Zn, r*))>] = 0n,
where ρ(f;匕,Xi,z,r*) = (匕 一 f(Xi))r*(K,X∕z). By replacing r* and its expectation
with its estimator and the sample average, we obtain the sample vector moment restrictions,
n Pn=ι (ρ(f；匕,Xi, Zι,^ ρ(f; Y Xi, Z2,r)…ρ(f； Yi,Xi, Znr))>.
Once we obtain the sample average, we can apply various methods for learning f * from the uncondi-
tional moment restriction. For instance, for a hypothesis class F, we estimate f * by minimizing
1n 1n	2
Rn(f,r) = -∑ -∑(yi — f(Xi))r(Yi,Xi∣Zj).
n j=1 n i=1
(2)
This objective function is closely related with the projected mean squared error (MSE) introduced in
Dikkala et al. (2020), defined as
L(f, r*) :=EZ h(EY,X[(f*(Xi)-f(Xi))r*(Yi,Xi|Zj)])2i ,	(3)
The objective function (2) is a special case of the GMM and GEL. In the GMM, we estimate f* by min-
imizing 1 Pn=I (n∙ PNι(K - f (Xi))r(Yi, Xi|Zj))> Wj (1 Pn=ι(匕 — f (Xi))^Yi,Xi∣Zj)),
where wj > 0 is a weight (Ai & Chen, 2003). Thus, our framework of transforming conditional
moment restrictions to unconditional moment restrictions using importance weighting allows us to
conduct causal inference using NPIV with conditional moment restrictions using a variety of models,
such as linear-in-parameter models with basis functions and neural networks.
Linear-in-parameter models. As an example, we introduce a linear-in-parameter model with some
basis functions. Here, let us consider using the Gaussian kernel as the basis function. For x ∈ X,
let 夕(x; σ2) = (K(x, Xi； σ2),..., K(x, Xn; σ2))>, where K(x, Xu； σ2) = exp (— kx-Xuk2) is
the Gaussian kernel with a hyperparameter σ2 > 0 and ∣∣ ∙ k2 is the L2 norm. Then, we define a
linear-in-parameter model as f (x; σ2) = β>夕(x; σ2) + βo, where β ∈ Rn, and βo ∈ R.
Neural networks. We can also use neural networks for approximating f*. In this case, we need to
carefully determine the network structures because we cannot uniquely determine the solution owing
to the overparameterization. In existing studies, it is assumed that the network modes are well defined
for the complexity of the nonparametric function f * and the sample size n.
Advantages of our proposed method. Our method has the following three advantages: (i) our
method is applicable to general causal inference problems, and is not limited to the NPIV problem
formulated in Section 2, as well as the Ai & Chen (2003), (ii) our method can deal with high
dimensional variables, owing to the machine learning technique. This is in contrast to related studies
Ai & Chen (2003) and Otsu (2011), which use a sieve and Nadaraya-Watson estimator, respectively,
and hence do not work in high dimension settings, and (iii) our method is computationally efficient by
the use of the importance weight approach. A similar study Hartford et al. (2017) requires additional
sampling of Yi from an estimated conditional density p(y|z, y). Another related study Dikkala et al.
(2020) requires a difficult algorithm to solve its minimax optimization problem. Our importance
weight approach avoids such computational burden.
4.3	Learning with overparametrized models
In learning from moment restrictions, the structural function f* is determined by the equations that
the expected value of random variables satisfy. When approximating the structure function f * using a
model with more parameters than the sample size, the solution cannot be uniquely determined. When
minimizing the prediction error directly, such overparameterization may not pose a major problem,
and, in fact, may be necessary to improve the generalization error, as a recent finding suggests in
the case of linear regression (Bartlett et al., 2020). However, in NPIV, the objective function is
5
Published as a conference paper at ICLR 2022
the set of conditional moment restrictions. Unlike in direct prediction error minimization, a model
trained to minimize empirically approximated conditional moment restrictions does not necessarily
minimize the MSE for f * (E[(f (Xi) - f *(Xi))2]). In fact, We empirically confirm that methods
using neural network sometimes do not work well partly because of their overfitting to empirical
moment minimization, not to the empirical MSE for f*. Despite these potential problems, there is
a strong motivation to use neural netWorks, oWing to the reported superiority in some applications,
such as computer vision (Xu et al., 2021a;b; Yuan et al., 2021), natural language processing (Ash
et al., 2019; Chen et al., 2020) tasks. For this reason, We introduce a heuristic to avoid this problem.
As We explained, the parameters are not uniquely determined by conditional moment restrictions
alone. Therefore, from the set of parameters satisfying conditional moment restrictions, We need to
select a set of parameters that Works Well in prediction. We consider training a model that has the
minimum MSE With Yi While satisfying conditional moment restrictions as folloWs:
minE(Yi-f(Xi))2	s.t.E[(Yi-f(Xi))|Zj]	=0	∀j	∈	{1,2,...,n}.
f∈F
Since it is difficult to solve constrained optimization With neural netWorks, We propose to solve the
penalized optimization. In the case of linear combination With penalties, We train the model by
n	nn	2
min-X(Yi - f(Xi))2 + ηX -X(匕-f(Xi))r(Yi,Xi∣Zj)	,	(4)
f∈F n i=1	j=1 n i=1
Where η ≥ 0 is a regularization coefficient.
The motivation of this heuristic is to select a function f(x) that is the closest to E[Yi|Xi = x], among
multiple functions satisfying the conditional moment restriction. This heuristic Works Well When
f* (x) takes a near value of E[Yi|Xi = x] While f* (x) 6= E[Yi|Xi = x].
5 Estimation error analysis
We shoW the estimation error of the conditional density ratio r* and structural function f*. We denote
the sample counterpart of L(f, r*) as
-n -n	2
Ln(f,r*) = n∑ -∑(f *(Xi)- f (Xi ))r*(Yi,Xi∣Zj).
n j=1 n i=1
Let us denote the distributions of (Yi, Xi) and Zi by P and Q, respectively, and define the L2 risk
of a function g With P and Q as kgk2L2(P×Q) =	g2(w)dP dQ. Let us denote the distribution of
(Yi, Xi, Zi) by O, and define the L2 risk of a function g With O as kgk2L2(O) = g2 (w)dO. We put
the folloWing assumptions on the error term εi .
Assumption 2. The error term εi is sub-Gaussian random variables. In addition, the distributions
of Xi and Zi have probability densities that are finite and bounded away from zero.
Note that the randomness of Yi depends on εi and Xi. Define the hypothesis classes of the conditional
density ratio r* and structural function f * as R and F, respectively. Suppose that the hypothesis
classes are Vapnik-Chervonenkis (VC) class (for rigorous definition, see Section 2.6 in van der vaart
& Wellner (1996)). This class include the true models, and the hypothesises are bounded.
Assumption 3. The hypothesis class R is VC class, includes the true model, r* ∈ R, and all r ∈ R
are uniformly bounded by B > 0.
Assumption 4. The hypothesis class F is VC class, includes the true model, f* ∈ F, and all f ∈ R
are uniformly bounded by B > 0.
We define a measure of complexity of the hypothesis classes: for F , We define the com-
plexity CB(F) := RB PlogN(δ0,F, k∙ ∣∣l∞)dδ0 with a covering number N(δ0, F, k ∙ k):=
inf {N |{fj}jN=1 s.t. F ⊂ ∪j=1{f |kf - fjk ≤ δ0}} in terms of a sup-norm kf kL∞ = supx |f (x)|.
In this following parts, for the conditional density ratio estimation, we derive the bound of the MSE
∣∣r - r*∣∣L2(p×q); for the structural function estimation, we derive the bound of the projected MSE
L(f, r*), which corresponds to the upper bound of the MSE of ∣f - f * ||l2(o)(Section 5.3).
6
Published as a conference paper at ICLR 2022
5.1	MSE of the conditional density ratio
First, we consider the MSE of the conditional density ratio. For a multilayer perception with ReLU
activation function (Definition 1), we show the following bound. The proof is shown in Appendix B.2.
Lemma 1 (MSE of r*). Suppose thatAssumptions 1-3 hold. Let I(r) be a non-negative function on
R and I(r*) < ∞. Define RM = {r ∈ R : I(r) ≤ M} satisfying R = UM≥ι RM. Suppose that
there exist co > 0 and 0 < γ < 2 such that supg∈RM ∣∣r 一 r*k ≤ c0M and sup	r∈RM ∣∣r 一
∣∣r-r*kL2(p )≤δ
r*k∞ = coM for all δ > 0, and that logN(δ, RM, ∣∣∙∣∣l∞) = O (M∕δ)γ. Then,
∣r - r*∣L2(o) = Op (n-1"2+γ))	(5)
5.2	PROJECTED MSE OF STRUCTURAL FUNCTION f *
Next, We consider bounding the projected MSE of the structural function f *. To bound the projected
MSE, we use a technique associated with U-statistics. We first obtain the following lemma. Let
(Yi, Xi) be Wi, and Wi0 be the i.i.d. copy of Wi. The proof is shoWn in Appendix B.3.
Lemma 2. Suppose that Assumptions 1-4 hold. For any f ∈ F andr ∈ R, L(f, r) = L(f, r)+op(1)
holds as a freely chosen hyper-parameter as n → ∞, where
L(f,r) = Ew,wo,z [(f*(Xi)- f(Xi))r(Wi∣Zi)(f*(X0) - f (X0))r(W∕∣Zi)],
Regarding the form, We define an empirical version of L(f, r) in an U-statistic form:
nn
Ln(f,r) = n X n(n--ɪʌ	X	(f*(Xi)- f(Xi))r(Wi∣Zj)(f*(Xi，)- f(Xi，))r(Wi，|Zj).
j=1	i,i0=1,i6=i0
By a property of U-statistics (for example, see Arcones & Gine (1993)), Ln(f, r) = Ln(f, r) +
Op (-/n) clearly holds. Then, We can decompose the projected MSE L(f, r*) as folloWs:
L(f,r*)
=∆r
~ , ʌ . ~ , ʌ ,
-Ln(f,r)+Ln(f,r) + Op Qln.
--	J
{z^^^^^^^^^
(6)
二∆f
We can handle ∆r by the estimation error of r* as shoWn in Lemma 6 in Appendix B.4. We evaluate
the projected MSE by combining (6) With Lemma 1 and Lemma 6:
Theorem 1. Assume that the conditions of Lemmas 1-2 hold. Then, for any any δ ∈ (0, -), there
exists a constant c > 0 such that the following inequality holds with n ≥ - and with probability at
least - δ, for any γ ∈ (0, ):
L(f,r*) ≤ c
CB(F) +CB(R)
+ Op max -
-
n1/(2+γ)
))
This result reveals the following two findings: (i) the projected MSE is affected separately by the
complexity of F and R, and (ii) the overall convergence is O(-1√n) when these complexities are
finite. For instance, when using neural networks with Definition 1, the assumptions on the hypothesis
classes are satisfied (Lemma 3).
5.3 FROM PROJECTED MSE TO MSE OF f*
Following Chen & Pouzo (2012) and Dikkala et al. (2020, Appendix C.4), we discuss the derivation
of the upper bound of the MSE of f * from the projected MSE. Let us define the measure of ill-
posedness (Chen & Pouzo, 2012; Dikkala et al., 2020) with respect to the function class F as
T := SuPf ∈R kf Lff,rL)(O). Then, the MSE of f * is upper bounded as ∣∣f - f *∣∣L29) ≤ T 2L(f,r*)
(Chen & Pouzo, 2012; Dikkala et al., 2020). Thus, Theorem 1 also implies
kf - f *kL (O) =CT 2 C (F)+CB(R) + Op (2 max ( ^^, J )).
()	ʌ/n	∖	I V n	n1/(2+Y) /
Note that Dikkala et al. (2020) derived ln rate on the MSE of f*.
7
Published as a conference paper at ICLR 2022
Figure 1: The log10 scaled MSEs of the setting in Newey & Powell (2003). The left graph shows the
results using the original dataset. The right graph shows the results with additional IVs.
.50.50.5Q.5.OLQ
NNLL0.0.0.LL
---
(əraus 0⅛ODUJSW WdEeslJOIlnO
0i∙raUS 0⅛OD 山 SW 9dEeslJolano
==申一	壬
∣w'LS IW MM IW Krnl LS DeepIV DeepGMM KIV DFIV
Figure 2: The log10 scaled MSEs of the original setting in Ai & Chen (2003). The left graph shows
the result with R = 0.1 and the right graph shows the result with R = 0.9.
3 2 10T-2
Z-3s 0⅛OD 山 SW qdEes—Jollno
3 2 10T
&_3S 0⅛OD 山 SW WdEesIJOIlnO
Figure 3: The log10 scaled MSEs of the setting in Ai & Chen (2003) with additional IVs. The left
graph shows the result with R = 0.1 and the right graph shows the result with R = 0.9.
6	Experiments
We implement the following three methods based on our proposed method: first, we use neural
networks to predict f * and train the model by penalized least-squares in (4) (IW-LS); second, We Use
neural networks to predict f * and train the model by minimizing the sum of approximated moment
restrictions in (2) (IW-MM), which is the same as IW-LS except for the penalized term in the IW-LS;
third, we use a linear-in-parameter model with the Gaussian kernel to predict f * and train the model
by GMM (IW-Krnl). For all cases, we use neural networks for estimating r* . We compare our
proposed methods with four methods: DeepGMM (Bennett et al. (2019)), DFIV (Xu et al. (2021a)),
DeepIV (Hartford et al. (2017)), and KIV (Singh et al. (2019)). We use the datasets proposed in
Newey & Powell (2003), Ai & Chen (2003), and Hartford et al. (2017). In addition, we train neural
networks by simple least squares (LS), ignoring the dependency between Xi and εi , as a comparison.
To fairly evaluate the performances, for DeepGMM, DFIV, DeepIV, and KIV, we use the code and
hyperparameters used in Xu et al. (2021a)1. For IW-LS, IW-MM, IW-Krnl, and LS, we also follow
the model and hyperparameters of the code as possible. More details are shown in Appendix C.
6.1	Experiments with datasets of Newey & Powell (2003) and Ai & Chen (2003)
First, we investigate the performances of the proposed methods using econometric settings of Newey
& Powell (2003) and Ai & Chen (2003). These settings have simpler structures than recently proposed
settings, such as in Hartford et al. (2017). When using complex and high-dimensional datasets, there
is an inherent difficulty in learning due to its complexity, separate from the problem setting. Therefore,
we use simple datasets to check whether the proposed method can actually learn f*.
Newey & Powell (2003) generates {(Yi, Xi, Zi)}in=1 as follows: first, they generate {(εi, Ui, Zi)}in=1
from the multivariate normal distribution N 0 ,	0.5
00
0.5
1
0
010!!!
; then, they generate
Xi = Zi+UiandYi =f*(Xi)+εi,wheref*(Xi) = ln(|Xi - 1| + 1) sgn(Xi - 1).
1https://github.com/liyuan9988/DeepFeatureIV
8
Published as a conference paper at ICLR 2022
Ol6o)山 SWJJdEenno
0⅛OD 山 SW ΦHEroω⅛⅛o
守主壬胃口
⅞ ⅛ φ
IW LS IW MM IW Krnl LS DeepIV DeepGMM KIV DFIV
Figure 4: The log10 scaled MSEs of the demand design experiments with 1, 000 samples. The left
graph show the results with ρ = 0.25 and the right graph shows the results with ρ = 0.75.
Ai & Chen (2003) generates {(Yi, Xi, Zi)}in=1 as follows: first, they generate {(εi, X1i, Vi, Ui)}in=1
as / 〜N (0,χ2i + Vi2), Xii i^ Unif[0,1], Vi i'. Unif[0,1], and C ,出 N (0,X2i + 匕2)；
second, they generate X2i = X1i + Vi + R × εi + Ui and Y1 = X1iγ0 + h0 (X2i) + εi, where
h0 (X2i) = exp (X2i) / (1 + exp (X2i)) and R is chosen as 0.9; then, obtain Xi = (X1i X2i)>
and Zi = (X1i Vi). Here, εi and Ui are unobservable, and f * (Xi) = X1iγ0 + h0 (X2i), where the
function h0 and γ0 are unknown.
We run each algorithm 20 times on each dataset with n = 1, 000 and calculate the mean squared
error (MSE). In the left graph of Figure 1 and Figure 2, we report the MSEs. In the dataset of Newey
& Powell (2003), DFIV and our proposed IW-MM produce smaller MSE. The dataset only has a
one-dimensional Xi, which mitigates the identification problem of neural networks. In contrast, in the
dataset of Ai & Chen (2003), IW-LS leads to smaller MSE. In Appendix C.4, using the experimental
setting of Newey & Powell (2003), we also show additional experimental results on the empirical
convergence of the MSE of the IWMM and a comparison between the IWMM and classical 2SLS.
Because the dimensions of the IVs are low in the original settings, we add more IVs to the original
settings and investigate the performances of the algorithms. The detailed settings are described in
Appendix C.1. The results are shown in the right graph of Figures 1 and Figure 3.
6.2	Simulation studies using demand design datasets
To investigate the performance with more complicated and high-dimensional datasets, we use the
demand design dataset for synthetic airline ticket sales proposed by Hartford et al. (2017). In this
dataset, we observe (Yi , Pi , Ti , Si , Ci), where Yi is sales, Pi is price, Ti is time, Si is consumer’s
emotion, and Ci is cost to use as IV. Here, Ti and Si are covariates, that is, Xi = (Ti, Si), and the
IV is Zi = (Ci,Ti,Si). The sales Yi is generated as Yi = 100 + (10 + Pi)Sih(Ti) - 2Pi +εi,
where h(t) = 2 ((t—5) + exp (-4(t - 5)2) + 点-2). Since Pi is an endogenous variable and
correlated with the IV, we generate Pi to contain Ci as Pi = 25 + (Ci + 3)h(Ti) + Vi . In the
simulation, we assume εi 〜N (PVi, 1 — ρ2), Vi 〜N(0,1), Ci 〜N(0,1), Ti is sampled from
the uniform distribution with the continuous support [0, 10], and Si is sampled from the uniform
distribution with the discrete support {1,..., 7}. Here note that E[εi ∣Ci, Ti, Si] = E[εi ∣Zi] = 0. The
extent of correlation between Pi and Vi is controlled by ρ ∈ {0.25, 0.75} and the larger the ρ is, the
more severe the correlation problem becomes.
Figure 4 shows the results with 1, 000 samples. The results under other settings are reporetd in
Appendix C.2. In this setting, IW-LS and LS outperform the other methods. We consider this is
because f*(Xi) takes large values compared to the error term. Under this situation, a model trained to
predict Yi may perform well because the influence of E[εi ∣Xi] = 0 is limited. However, the purpose
of using NPIV in the first place is because the influence is large, or else effects of E[εi ∣Xi] = 0 can
be ignored. Therefore, this dataset is often used in the existing studies, it may not be suitable.
7	Conclusion
This paper proposed a method for learning causal relationships from conditional moment restrictions.
Our method is based on importance weighting using the conditional density ratio. The proposed
method showed superior performance in experiments. We point out potential problems in recently
proposed methods concerning identification and empirical performances.
9
Published as a conference paper at ICLR 2022
Acknowledgments
The authors would like to thank Liyuan Xu for his constructive advice.
References
Chunrong Ai and Xiaohong Chen. Efficient estimation of models with conditional moment restrictions
containing unknown functions. Econometrica, 71(6):1795-1843, 2003.
Miguel A Arcones. A bernstein-type inequality for u-statistics and u-processes. Statistics &
probability letters, 22(3):239-247, 1995.
Miguel A Arcones and Evarist Gin6. Limit theorems for U-Processes. The Annals OfProbability, pp.
1494-1542, 1993.
Elliott Ash, Daniel Chen, Xinyue Zhang, Zhe Huang, and Ruofan Wang. Deep iv in laa: Analysis of
appellate impacts on sentencing using high-dimensional instrumental variable, 2019.
Peter L. Bartlett, Philip M. Long, Ggbor Lugosi, and Alexander Tsigler. Benign overfitting in linear
regression. Proceedings of the National Academy of Sciences, 117(48):30063-30070, 2020.
Andrew Bennett and Nathan Kallus. The variational method of moments, 2020.
Andrew Bennett, Nathan Kallus, and Tobias Schnabel. Deep generalized method of moments for
instrumental variable analysis. In Advances in Neural Information Processing Systems, volume 32.
Curran Associates, Inc., 2019.
Herman J. Bierens. Consistent model specification tests. Journal of Econometrics, 20(1):105-134,
1982.
Herman J. Bierens. A consistent conditional moment test of functional form. Econometrica, 58(6):
1443-1458, 1990.
Richard Blundell, Xiaohong Chen, and Dennis Kristensen. Semi-nonparametric iv estimation of
shape-invariant engel curves. Econometrica, 75(6):1613-1669, 2007.
Marine Carrasco, Jean-Pierre Florens, and Eric Renault. Linear inverse problems in structural
econometrics estimation based on spectral decomposition and regularization. In Handbook of
Econometrics, volume 6B, chapter 77. Elsevier, 1 edition, 2007a.
Marine Carrasco, Jean-Pierre Florens, and Eric Renault. Linear inverse problems in structural
econometrics estimation based on spectral decomposition and regularization. In J.J. Heckman and
E.E. Leamer (eds.), Handbook of Econometrics, volume 6B, chapter 77. Elsevier, 1 edition, 2007b.
Gary Chamberlain. Efficiency bounds for semiparametric regression. Econometrica, 60(3):567-596,
1992.
Jiafeng Chen, Daniel L. Chen, and Greg Lewis. Mostly harmless machine learning: Learning optimal
instruments in linear iv models, 2020.
Xiaohong Chen and Sydney C. Ludvigson2009Ludvigson. Land of addicts? an empirical investigation
of habit-based asset pricing models. Journal of Applied Econometrics, 24(7):1057-1093, 2009.
Xiaohong Chen and Demian Pouzo. Estimation of nonparametric conditional moment models with
possibly nonsmooth generalized residuals. Econometrica, 80(1):277-321, 2012.
Victor Chernozhukov and Christian Hansen. An iv model of quantile treatment effects. Econometrica,
73(1):245-261, 2005.
Victor Chernozhukov, Guido W. Imbens, and Whitney K. Newey. Instrumental variable estimation of
nonseparable models. Journal of Econometrics, 139(1):4-14, 2007.
Victor Chernozhukov, Whitney Newey, Rahul Singh, and Vasilis Syrgkanis. Adversarial estimation
of riesz representers, 2020.
10
Published as a conference paper at ICLR 2022
Serge Darolles, Yanqin Fan, Jean-Pierre Florens, and Eric Renault. Nonparametric instrumental
regression. Econometrica, 79(5):1541-1565, 2011.
Nishanth Dikkala, Greg Lewis, Lester Mackey, and Vasilis Syrgkanis. Minimax estimation of
conditional moment models. In Advances in Neural Information Processing Systems, volume 33,
pp. 12248-12262. Curran Associates, Inc., 2020.
Manuel A. Dom⅛guez and Ignacio N. Lobato. Consistent estimation of models defined by conditional
moment restrictions. Econometrica, 72(5):1601-1615, 2004.
Stephen G. Donald, Guido W. Imbens, and Whitney K. Newey. Empirical likelihood estimation and
consistent tests with conditional moment restrictions. Journal of Econometrics, 117(1):55-93,
2003.
Patrick Gagliardini and Olivier Scaillet. Nonparametric instrumental variable estimation of structural
quantile effects. Econometrica, 80(4):1533-1562, 2012.
Evarist Gin6 and Richard Nickl. MathematicalfOundatiOns Ofinfinite-dimensional statistical models.
Cambridge University Press, 2021.
William H. Greene. Econometric Analysis. Pearson Education, 2003.
Peter Hall and Joel L. Horowitz. Nonparametric methods for inference in the presence of instrumental
variables. The Annals of Statistics, 33(6):2904 - 2929, 2005.
Bruce E. Hansen. Econometrics. 2022.
Lars Peter Hansen, John Heaton, and Amir Yaron. Finite-sample properties of some alternative gmm
estimators. Journal of Business & Economic Statistics, 14(3):262-280, 1996.
Jason Hartford, Greg Lewis, Kevin Leyton-Brown, and Matt Taddy. Deep IV: A flexible approach
for counterfactual prediction. In Proceedings of the 34th International Conference on Machine
Learning, volume 70 of Proceedings of Machine Learning Research, pp. 1414-1423. PMLR,
06-11 Aug 2017.
Fumio Hayashi. Econometrics. Princeton Univ. Press, Princeton, NJ [u.a.], 2000.
Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the
American Statistical Association, 58(301):13-30, 1963.
Joel L. Horowitz and Sokbae Lee. Nonparametric instrumental variables estimation of a quantile
regression model. Econometrica, 75(4):1191-1208, 2007a.
Joel L. Horowitz and Sokbae Lee. Nonparametric instrumental variables estimation of a quantile
regression model. Econometrica, 75(4):1191-1208, 2007b.
Takafumi. Kanamori, Shohei Hido, and Masashi Sugiyama. A least-squares approach to direct
importance estimation. Journal of Machine Learning Research, 10(Jul.):1391-1445, 2009.
Masahiro Kato and Takeshi Teshima. Non-negative bregman divergence minimization for deep
direct density ratio estimation. In Proceedings of the 38th International Conference on Machine
Learning, 2021.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database, 2010.
Arthur Lewbel. A local generalized method of moments estimator. Economics Letters, 94(1):124-128,
2007.
Hong Li, Chuanbao Ren, and Luoqing Li. U-processes and preference learning. Neural computation,
26(12):2896-2924, 2014.
Luofeng Liao, You-Lin Chen, Zhuoran Yang, Bo Dai, Mladen Kolar, and Zhaoran Wang. Provably
efficient neural estimation of structural equation models: An adversarial approach. In Advances in
Neural Information Processing Systems, volume 33, pp. 8947-8958. Curran Associates, Inc., 2020.
11
Published as a conference paper at ICLR 2022
Krikamol Muandet, Arash Mehrjou, Si Kai Lee, and Anant Raj. Dual instrumental variable regression.
In Advances in Neural Information Processing Systems, volume 33, pp. 2710-2721. Curran
Associates, Inc., 2020.
Whitney K. Newey. Efficient estimation of models with conditional moment restrictions. In Econo-
metrics, volume 11 of Handbook of Statistics, pp. 419-454. Elsevier, 1993.
Whitney K. Newey and James L. Powell. Instrumental variable estimation of nonparametric models.
Econometrica, 71(5):1565-1578, 2003.
Whitney K. Newey and Richard J. Smith. Higher order properties of gmm and generalized empirical
likelihood estimators. Econometrica, 72(1):219-255, 2004.
Taisuke Otsu. Empirical likelihood estimation of conditional moment restriction models with
unknown functions. Econometric Theory, 27(1):8-46, 2011.
Art B. Owen. Empirical likelihood ratio confidence intervals for a single functional. Biometrika, 75
(2):237-249, 06 1988.
Olav ReiersOL Confluence analysis by means of instrumental sets of variables, 1945.
Andres Santos. Inference in nonparametric instrumental variables with partial identification. Econo-
metrica, 80(1):213-275, 2012.
Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU activa-
tion function. Annals of Statistics, 48(4):1875-1897, 2020.
Rahul Singh, Maneesh Sahani, and Arthur Gretton. Kernel instrumental variable regression. In
Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.
Richard J Smith. Alternative Semi-parametric Likelihood Approaches to Generalised Method of
Moments Estimation. Economic Journal, 107(441):503-519, March 1997.
Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density Ratio Estimation in Machine
Learning. Cambridge University Press, New York, NY, USA, 1st edition, 2012.
Taiji Suzuki, Masashi Sugiyama, Jun Sese, and Takafumi Kanamori. Approximating mutual infor-
mation by maximum likelihood density ratio estimation. In Proceedings of the Workshop on New
Challenges for Feature Selection in Data Mining and Knowledge Discovery at ECML/PKDD 2008,
volume 4, pp. 5-20, 2008.
Sara van de Geer. Empirical Processes in M-Estimation, volume 6. Cambridge university press,
2000.
Aad van der vaart and Jon Wellner. Weak Convergence and Empirical Processes: With Applications
to Statistics (Springer Series in Statistics). Springer, 3 1996.
Vladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience, 1998.
Jeffrey M. Wooldridge. Econometric analysis of cross section and panel data. MIT Press, 2002 2002.
Philip Green Wright. The Tariff on Animal and Vegetable Oils. Investigations in international
commercial policies. Macmillan, 1928.
Liyuan Xu, Yutian Chen, Siddarth Srinivasan, Nando de Freitas, Arnaud Doucet, and Arthur Gretton.
Learning deep features in instrumental variable regression. In International Conference on Learning
Representations, 2021a.
Liyuan Xu, Heishiro Kanagawa, and Arthur Gretton. Deep proxy causal learning and its application to
confounded bandit policy evaluation. In Thirty-Fifth Conference on Neural Information Processing
Systems, 2021b.
Junkun Yuan, Xu Ma, Kun Kuang, Ruoxuan Xiong, Mingming Gong, and Lanfen Lin. Learning
domain-invariant relationship with instrumental variable for domain generalization, 2021.
12
Published as a conference paper at ICLR 2022
A	Literature Review of IV Methods
In the field of economics, the causal effect is often referred to as the structural effect, because
the causal relationship arises from some economic structure. The idea of endogeneity has a close
relationship with the estimation of the structural effect. For example, in estimation problems with
supply/demand models, because the supply and demand are determined simultaneously, there is a
simultaneous equation bias, which causes the correlation between the explanatory variable and error
term. This correlation is called enodogeneity. Under the enodogeneity, the OLS does not yield a
consistent estimator of the structural model. To obtain a consistent estimator to capture the causal
effect, the method of IVs have been used Wright (1928); Reiersol (1945).
A.1 IV Methods for Linear Structural Models
First, we consider a linear structural model. For two random variables Y ∈ R, X ∈ Rd , endogeneity
refers to a situation, such that
Y = X>β + εi,	(7)
where εi ∈ R is the error term, β is the parameter of interest, and
E[X ε] 6= 0.	(8)
This regression model is called a structural equation and β is the structural parameter. Under
endogeneity, the least squares method does not yield a consistent estimator.
Let us consider an IV Z ∈ Rk such that
E[Zε] = 0.	(9)
For the estimation of the structural parameter under endogeneity, the IV Z plays an important.
A.2 IV Methods for Nonparametric S tructural Models
Nonparametric estimation of structural models under conditional moment restrictions is an important
topic in statistics and econometrics, because it allows us to model economic relationships flexibly.
Let h*(∙) = (hj(∙),..., hq(∙)) be unknown nonparametric structural functions, where each function
h'(∙) may depend on Xi and 匕.In general, we can consider estimating the unknown nonparametric
structural functions h*(∙) defined as
E[ρ(Y,X; θo,h*(∙))∣Z]=0,	(10)
where ρ(∙; θ0,h*(∙))) is a vector of residuals with functional forms. Note that the conditional
distribution of Y given X is not specified.
This model is a generalization of semi/nonparametric estimation with conditional moment restrictions
considered in Chamberlain (1992) and Newey & Powell (2003). This model includes many structural
models as special cases. A typical example is the target structural model of the NPIV problem
defined in Section 2 (Newey & Powell, 2003; Darolles et al., 2011). The NPIV problem also includes
the estimation of a shape-invariant system of Engel curves with an endogenous total expenditure
(Blundell et al., 2007). Another important special case of (10) is the quantile instrumental variables
(IV) treatment effect model of Chernozhukov & Hansen (2005), and the nonparametric quantile
instrumental variables regression of Chernozhukov et al. (2007) and Horowitz & Lee (2007a). There
are other applications, such as asset pricing models (Chen & Ludvigson2009Ludvigson, 2009) and
reinforcement learning.
There are three approaches to the estimation of the structural models under the conditional moment
restrictions: the sieve minimum distance (SMD) method, the function space Tikhonov regularized
minimum distance (TMD) method, and the minimax optimization method.
SMD method. The SMD procedure minimizes a consistent estimate of the minimum distance
criterion over some finite-dimensional compact sieve space. Newey & Powell (2003) extends the
2SLS methods for linear structural models to the NPIV problem. The authors approximate the
nonparametric structural function with a sieve estimator, which is a linear combination of growing
13
Published as a conference paper at ICLR 2022
feature basis functions. Ai & Chen (2003) and Chen & Pouzo (2012) propose the penalized minimum
distance sieve estimator to a more general problem defined as (10).
When applying the minimum distance method to the NPIV problem, we consider solving the following
problem:
min EZ [EY,X [Y -h(X)|Z]2] + λR(h),
where R(h) is a regularizer. Chen & Pouzo (2012) approximates the function class Hn by linear
functions in growing feature space with the sample size n. Subsequently, the authors also estimate
the function m(z) = E[y - h(x) | z] based on another growing sieve.
TMD method. In the TMD method, we minimize a consistent penalized estimate of the minimum
distance criterion over the whole infinite dimensional function space H, in which the penalty function
is of the classical Tikhonov type (Hall & Horowitz, 2005; Horowitz & Lee, 2007b; Carrasco et al.,
2007b; Darolles et al., 2011; Gagliardini & Scaillet, 2012). In the NPIV problem, this approach is
equivalent to solving
min EZ [EY,X [Y -h(X)|Z]] +λkhk22.
h∈H
Note that this is equivalent to the minimum distance estimation, such as Chen & Pouzo (2012).
(11)
Minimax optimization method. Recently, in the machine learning literature, the minimax ap-
proach has garnered attention (Bennett et al., 2019; Dikkala et al., 2020). In this approach, we solve
the following minimax optimization problem:
min max
f∈F h∈H
E[(Y - f(X))h(Z)] - khk22.
(12)
Such a representation of the conditional moment restrictions has long been used in statistics and
econometrics. One of the related studies is the specification testing by Bierens (1982; 1990). The
authors assume that a statistical model that satisfies the conditional moment restrictions is correct
and propose a specification testing method to investigate whether the model is correct. These studies
transform the conditional moment restrictions to unconditional moment restrictions by using the
product of any function of a random variable used in the conditioning and the moment function. This
idea of transformation to unconditional moment restrictions is also used in other related work, such
as Santos (2012).
In particular, Dikkala et al. (2020) shows the minimax optimality of a method based on such minimax
optimization. This minimax optimization technique is known to be applicable, not only to the NPIV
problem, but also to a wider range of problems. It is also closely related to debiased machine learning
literature (Chernozhukov et al., 2020).
B Proofs in Section 5
B.1 Mathematical tools
Concentration inequalities for empirical processes Given a probability distribution P and a
random variable g(X), we denote the expectation of g(X) under P by gdP . Given samples
X1 , X2 , . . . , Xn from P , the empirical distribution is denoted by Pn, and the empirical mean is
denoted by R gdPn； that is, R gdPn = 1 Pn=I g(Xi) We also denote R gdP - 1 Pn=I g(Xi) by
R gd(P - Pn).
We also define
PK(g) = 2K2 / (exp (|g|/K - 1 -∣g∣∕K)) dP,	K > 0.	(13)
Let G satisfy
sup ρK (g) ≤ R
g∈G
Then, we summarize some tools used in the theoretical analysis.
(14)
14
Published as a conference paper at ICLR 2022
Proposition 1 (Theorem 5.11 in van de Geer (2000)). C is a (sufficiently large) universal constant,
whereas a, C0, and C1 may be chosen, but do have to satisfy the following conditions:
•	a ≤ C1√nR2/K;
•	a ≤ 8√nR;
a ≥ C0
(max ■{以26√n) plogN(u, G, k ∙ k)du, R})
•	C02 ≥ C2(C1 + 1).
Then,
P sup
g∈G
√n / gd(Pn - P)
≥ a) ≤ C exp (- C 2(Cιa+l)R2)
(15)
Proposition 2 (Lemma 5.14 in van de Geer (2000)). Let G ⊂ L2 (P) be a function class and the map
I(g) be a complexity measure of g ∈ G, where I is a non-negative function on R and I(g0) < ∞ for
a fixed g0 ∈ G. We now define GM = {g ∈ G : I(g) ≤ M} satisfying G = M≥1 GM. Suppose that
there exist c0 > 0 and 0 < γ < 2 such that
sup kg - g0k ≤ c0M,	sup	kg - g0k∞ ≤ c0M, for all δ > 0,
g∈GM	r∈GM
kg-g0kL2(P)≤δ
and that log N(δ, GM, P) = O (M∕δ)γ. Then, we have
IR(g -gO)d(P - Pn)I ʃ. ∩× /	、
sup J-----------------L = Op (1)，(n → ∞),
g∈G	D(g)
where D(g) is defined by
D(g)
max
kg - gokL-γP2ι(g)Y/2
√n
i(g)
n2/(2+Y)
Lemma 3 (Lemma 9 of Kato & Teshima (2021)). Let ` : (br, Br) → R be a ν-Lipschitz continuous
function. Let log N (δ, F, k ∙ ∣∣l2 (P)) denote the bracketing entropy of F with respect to a distribution
P. Then, for any distribution P, any γ > 0, any M ≥ 1, and any δ > 0, we have
log N (δ,' ◦Hm ,k∙ ∣l2(p)) ≤ (S +；(2")Y ( M )Y.
Moreover, there exists c0 > 0 such that for any M ≥ 1 and any distribution P,
sup	k' ◦ r — ' ◦ r*∣∣L2(ρ)≤ C0νM,
'ot∈'oHm
sup k' ◦ r — ' ◦ r*k∞ ≤ covM, for all δ> 0.
'or∈'oH M
k'or-'or*kL2(P )≤δ
Concentration inequalities for U-statistics. Given a probability distributions P and Q and a
random variable g(X, Z), we denote the expectation of g(X, Z) under P and Q by gdP dQ; that
is, EX EZ g(X, Z)	= R R gdP dQ. Given samples X1, X2, . . . , Xn from P and Z1, Z2, . . . , Zn
from Q, the empirical distributions are denoted by Pn and Qn , and the empirical mean is denoted by
R R gdPndQn； that is, R R gdPndQn = n Pn=I n Pn=I g(Xi, Zj). We also denote R R gdPdQ -
1 pn=1 1 Pn=1 g(Xi, Zj ) by RR gd(P - Pn)d(Q - Qn).
We also define
PK(g) = 2K2 / (exp (|g|/K - 1 -∣g∣∕K)) dP,	K > 0.	(16)
Let G satisfy
sup ρK (g) ≤ R	(17)
g∈G
Then, we summarize some tools used in the theoretical analysis.
15
Published as a conference paper at ICLR 2022
Proposition 3 (Concentration inequality on empirical processes with U-statistics: Theorem 5 in
Arcones (1995), adjusted to our setting). Suppose that S1, ..., Sn are S -valued i.i.d. random variables
and consider a class of symmetric functions H ⊂ L2(S × S). Also, suppose that any function in H is
uniformly bounded by b > 0 and define σ2 = suph∈H VarS (ES 0 [h(S, S0)]) where S0 is an i.i.d. copy
of S. Ifthe N (ε, H, ∣∣∙ ∣∣2) ≤ (A∕ε)ν for some A,ν > 0 ,for any t ≥ C with some c > 0, we obtain
P I n1/2 SUp < / 1 XX	h(Si,Si0) - ESS [h(S, S)] ∖ ≥ t ∣
1h∈H [ n(n- I) i,i,f,)
≤ 8exp(-t2∕217(σ2 + tbn-1/2)) + 8A2ν(σ2 + 2tbn-1/2)-v exp(-n(σ2 + tbn-1/2/2)∕2b2)
+ 2 exp(—t2∕(211bc(σ2 + tbn-1/2))).
Proposition 4 (Bemsteins' inequality for U-Statistics (Hoeffding, 1963; Arcones & Gina 1993)).
Let ∣g∣∞ <	c,	gdP dQ	= 0,	and σ2	=	g2 dP dQ.	Then, for any	a0	>	0:
P (/ Z gdPndQn > a') ≤exp Q2 n；4Ca
This implies that for any a > 0,
P
(ZZ g - g0d(P - Pjd(Q - Qj I > √n) ≤ exp ( 6σ2 + 2ca/√n).
As well as Suzuki et al. (2008), by applying this result, we can obtain the following result.
Proposition 5 (From Proof of Theorem 1 in Suzuki et al. (2008)). Let G ⊂ L2(P × Q) be a function
class and the map I (g) be a complexity measure of g ∈ G, where I is a non-negative function on
R and I(g0) < ∞ for a fixed g0 ∈ G. We now define GM = {g ∈ G : I(g) ≤ M} satisfying
G = M≥1 GM. Suppose that there exist C0 > 0 and 0 < γ < 2 such that
sUp ∣g - g0∣ ≤ C0M,	sUp ∣g - g0∣∞ ≤ C0M, for all δ > 0,
g∈GM	r∈GM
kg-g0kL2(P ×Q)≤δ
and that logN(δ, GM, P X Q) = O (M∕δ)γ. Then, we have
IRR(g - gO)d(P - Pn)d(Q - Qn)I n ∕n Z	、
SUp J------------------------------l = Op (1), (n →∞),
g∈G	D(g)
where D(g) is defined by
m 、	I kg - g0kL-γP×Q)I(M2	I(g)
D(g) = max --------√n---------, n27WY)
SUp
g∈R
(g - g0)d(P - Pn)d(Q - Qn)
Op (l∕√n) , (n → ∞).
(18)
Complexities of neural networks.
Definition 1 (ReLU neural networks; Schmidt-Hieber, 2020). For L ∈ N andp = (p0, . . . , pL+1) ∈
NL+2,
F(L,p) ：={f ： x → WLσvL WL-iσvL7 …Wισ0] W°x :
Wi ∈ Rpi+1×pi,vi ∈ Rpi(i = 0, . . . , L)},
where σv(y) := σ(y — v), and σ(∙) = max{∙, 0} is applied in an element-wise manner. Then, for
s ∈ N, F ≥ 0, L ∈ N, andp ∈ NL+2, define
L
H(L, p, s, F) := {f ∈ F(L, p) : X ∣Wj ∣0 + ∣vj ∣0 ≤ s, ∣f∣∞ ≤ F},
j=0
16
Published as a conference paper at ICLR 2022
where ∣∣∙∣∣0 denotes the number of non-zero entries ofthe matrix or the vector, and ∣∣ ∙ k∞ denotes
the supremum norm. Now, fixing L,p,s ∈ N as well as F > 0, we define
Indz,p ：= {(L,P) ： L ∈ N, L ≤ L,P ∈ [p]L+2},
and we consider the hypothesis class
H= U	H(L,P,s,F)
(L,P)∈IndL,p
H := {r ∈ H : Im(r) ⊂ (br, Br)}.
Moreover, we define Ii : IndL,p → R and I: H → [0, ∞) by
I1(L,P) :=2|IndL,pls+1 (L + 1)V2,
I(r) := max ∣r∣∞,	min	I1 (L, P)	,
I	(LM∈IndL,p	I
r∈H(L,p,s,F)
where V := QlL=+01(Pl + 1), and we define
HM := {r ∈H : I(r) ≤ M}.
Lemma 4 (Lemma 5 in Schmidt-Hieber (2020)). For L ∈ N andP ∈ NL+2, let V := QlL=+01(Pl + 1).
Then, for any δ > 0,
log N (δ, H(L,p, s, ∞), k ∙ k∞) ≤ (s + 1) log(2δ-1(L + 1)V2).
Definition 2 (Derived function class and bracketing entropy). Given a real-valued function class F,
define ' ◦ F := {' ◦ f : f ∈ F}. By extension, we define I : ' ◦H → [1, ∞) by I(' ◦ r) = I(r)
and ' ◦ HM := {' ◦ r : r ∈ HM}. Note that, as a result, ' ◦ HM coincides with {' ◦ r ∈ ' ◦ H :
I(' ◦ r) ≤ M}.
Notations. Let us denote a pair of random variables (Yi , Xi ) by Wi . We denote the distribution of
(Wi , Zi ) by R and its empirical distribution as On . Besides, we denote the distributions of Wi and
Zi by P and Q, and their empirical distributions by Pn and Qn. For a function g(X, Z), we define
the L2 risk over the distribution P and Q as
IlgllL2(p ×q)=rEx hEz [g2(X,Z)]i=Zz g g2dP dQ.
B.2 Proof of Lemma 1: estimation error in conditional density ratio estimation
Lemma 5 (Decomposition of MSE).
∣r -门隹(O) ≤
EW,Z[r*(WIZ)- r(W1Z)]- n
n
X (r*(WiIZi) - r(Wi∣Zi))
i=1
nn
EZ [EW [r*2(WIZ)- r2(WIZ)]] - n X n X (r*2
j=1	i=1
(WiIZj) - r2(Wi∣Zj).
ProofofLemma 5. Since The estimator r is the minimizer of the empirical risk, it satisfies the
inequality
1 n	11 n 1 n
--Xr(WiIZi) + χ- X-Xr2(WiIZj) ≤-
n	2n n
i=1	j=1	i=1
n Xr*(WiIZi) +2n X - Xr*2(WiIZj).
n	nn
i=1	j=1 i=1
(19)
1
+ 2
17
Published as a conference paper at ICLR 2022
Additionally, We consider an expectation of squared residuals as
2EZ [Ew [(r(W|Z) - r*(W|Z))2口
=1EZ [Ew [r2(W|Z) - 2r(W|Z)r*(W|Z) + r*2(W|Z)]]
=1 Ez [Ew [r2(W|Z)]] - Ew,z [^(W|Z)] + 1 Ew,z [r*(W|Z)]
+ Ew,z [r*(W|Z)] - Ew,z [r*(W|Z)]
=1EZ [Ew [r2(W|Z)]] - Ew,z [r(W|Z)] - ； Ew,z [r*(W|Z)] +Ew,z [r*(W|Z)]
2	2 '-----{------'
=EZ [Ew [r*2(W|Z)]]
=-2EZ [Ew 卜*2(W|Z) - r2(W|Z)]] + Ew,z 卜*(W|Z) - r(W|Z)] .	(20)
Taking sum of the both hand sides of (19) and (20), then we obtain the following by subtracting
-1 Pn=Ir(WiIZi)+21 Pn=ι 1 Pn=I r2(WiIZj)from the both side as
1 Ew,z [(r(W|Z) - r*(W|Z))2]
≤ -2EZ [Ew [r*2(W|Z) - r2(W|Z)]] + Ew,z [r*(W|Z) - r(W|Z)]
1 1 ʌ 1	„	、1 工/	、
+ 2- X - X 卜(Wi£)-户(明|4)) - - X I(WiIZi)-代阴邑)).
j=1	i=1	i=1
Then, we obtain the statement.	口
Our remaining task is to bound the following target values:
sup
r ∈R
Inln
EZ [Ew [r2(W|Z)]] - — X — Xr2(Wi£)
n j = 1 n i=1
and
1 n
sup Ew,z [r(W|Z)] - — Er(WiIZi)
r∈R	nE
(21)
(22)
ProofofLemma 1. Since 0 < γ < 2, we can apply Propositions 2 and 5 in combination with
Lemma 3 to obtain
sup
r∈H
|Ez [Ew [r2(W |Z)] ]- 1 Pn=I 1 Pn=I r2(Wi|Zj )|
D1W
I Ew,z [r(W|Z)] - 1 Pn=I r(Wi|Zi) ∣
SUH	D2 (r)
Op (1),
Op (1),
where
D1(r) = max
kr2 - r*2kL-(P×Q)1(r2)γ∕2	I(r2)
n2∕(2+γ)
and
D2(r) = max
kr -r* ∣∣¾∕2/(r)Y/2	ʃ (r)
,n2∕(2+γ)
18
Published as a conference paper at ICLR 2022
Noting that supr∈H I(r) < ∞ and supr∈H I(r2) < ∞. Then, for any 0 < γ < 2, we have
2
nn
EZ [EW [r2(W∣Z)]] — n X - Xr2(Wi∣Zj)
n j=1 n i=1
L2 (P ×Q)
max
kr2-r*2kL-YP× Q)	1
n2/(2+γ)
, and
2
1n
EW,Z [r(w|Z)] — — Er(WiIZi)
n
i=1
L2(P×Q)
max
kr -r*kL-γO2
1
,n2∕(2+γ)
These inequalities imply
kr - r"∣L2(P×Q) ≤Op
□
B.3 Proof of Lemma 2
We start with the expected loss function for a learner f : X → R and a conditional density function
r : X ×Y × Z→Ras
L(f,r) ：= Ez [(EW [(f*(X) - f(X))r(W|Z)])2],
where W = (Y, X). For T ∈ T, we consider T i.i.d. random variables Wi = (Xi, Yi), i =
generated from a conditional distribution P and approximate the inner expectation term as
n
EW [(f*(X) - f(X))r(WIZ)] = n-1 X(f*(Xi)- f (Xi))r(Wi∣Z) + Kn(f,r),
i=1
(23)
1,..., n
where
n
κn(f, r) := EW [(f"(X) - f(X))r(WIZ)] -n-1X(f"(Xi) - f (Xi))r(WiIZ)
i=1
is a residual. We substitute this form into (3) and obtain
L(f, r) = EZknT Xf*(Xi)- f 区)斤(用3)+ Kn(f, r))
EZ n-2 XT (f"(Xi) - f(Xi))r(WiIZ)(f"(Xi，) - f(Xi，))r(Wi，IZ)
k,k，=1
n
+ 2EZ
κn
(f, r)n-1 £(f *(Xi)- f (Xi))r(Wi∣Z) + Ez[κ∕f, r)2]
1^	।----------}
i=1
=:T1
^z^ιf^
=:T2
EZ
n
n-2	X	(f *(Xi) - f (Xi ))r(Wi∣Z )(f "(Xi，)- f(Xi，))r(Wi，|Z)
i,i0=1,i0 6=i
^z∖∕f^^
=:T *
+ EZ
n-2
n
X(f"(Xi)-f(Xi))2r(WiIZ)2 +T1 + T2.
i=1
{zf^
=:T0

}


}
}
19
Published as a conference paper at ICLR 2022
For T0, the uniformly bounded property of f *, f and r implies that it is T0 = O(1∕n). For Ti and T2,
the VC-class property and the Glivenko-Cantelli theorem (Section 2.8.1 in van der vaart & Wellner
(1996)), we obtain κn (f, r) → 0 in probability as T → ∞ uniformly on F × R. Associate with
the boundedness of f*, f and r, we obtain that T1 = op(1) and T2 = op(1). About T*, which is an
U-statistic, a convergence theorem (Theorem 3.1 in ArcOnes & Gin6 (1993)) as n → ∞ implies that
the U-statistic converges to EW,W0|Z [(f* (X) - f(X))r(W|Z)(f*(X0) - f(X0))r(W 0|Z)], where
W0 = (Y 0, X0) is an i.i.d. copied random element of W. Hence, we obtain
L(f, r) =EZ EW,W0|Z [(f*(X) - f(X))r(W|Z)(f*(X0) - f(X0))r(W0|Z)] +op(1)
= EW,W0,Z [(f*(X) - f(X))r(W|Z)(f*(X0) - f(X0))r(W0|Z)] + op(1),
for any f ∈ F and r ∈ R as T → ∞.
B.4 ESTIMATION ERROR BOUND OF ∆r
We can bound ∆r as follows:
Lemma 6. Suppose that Assumptions 1 holds. Then,
∆r = O(kr - r*kL2(p×Q))	(24)
Proof. Let us define ξ^ := f - f *. We bound △『by the LiPSChitz continuity of L(f, r) in r. For
any r, r0 ∈ R, we obtain
L(f, r) — L(f, r0) = E [ξf^(X )ξf^(X 0)(r(W ∣Z)r(W 0|Z) - r0(W |Z )r0 (W '∣Z))]
=E [ξf^(X)ξf^(X0)r(W0∣Z)(r(W0∣Z) - r0(WlZ))]
+ E [ξf^(X)ξf^(X0)r0(W∣Z)(r(W0|Z) - r0(W0|Z))]
. 32B05kr - r0kL2(O),
by the Cauchy-Schwartz inequality and the uniformly bounded property over F and R. As a result,
if the conditional density ratio Ppwwpzz) is bounded, We obtain
△r = O(kr - r*kL2(0)) = O(kr - r*kL2(P ×Q)).
□
B.5 Proof of Theorem 1: estimation error bound
We use the folloWing lemma to shoW Theorem 1.
Lemma 7.
~ , ʌ , ʌ ,
Ln(f,r) ≤ Γ(f,r) + Op (1/n).
Proof. We start with the following basis inequality following the definition of f; since it is an
minimizer of the empirical risk with r, we obtain
2nn	2
≤ n X nX(Yi - f*(Xi))r(WiIZj).
j=1	i=1
1n 1n
n X - X(Y - f(χi))r(WiIZj)
n j=1 n i=1
Since Yi = f*(Xi) + ε, the above inequality updated as
nn	2nn	2
n X n X(εi - ξf(Xi))r(Wi∣Zj)	≤ n X n X εir(Wi∣Zj)
n j=1 n i=1	n j=1 n i=1
(25)
20
Published as a conference paper at ICLR 2022
where We define ξ^ := f - f *. Since the left-hand Side is expanded as
nn	2
n X	- X(ε,-ξ∕(Xi))r(Wi∣Zj)
j=1	i=1
-X (- X”(WiIZj)) + -X (- Xξ^(Xi)^Wi∣Zj)
n j=1	n i=1	n j=1 n i=1
'-------------{z-------------
,ʌ , .
=Ln(f,r) + Op (1/n)
-n- n	-
-2—	εiεi0	ξ∕(Xi)ξ∕(Xi0)r(WiIZj	)r(Wi0	IZj	) - 2 滔
n j=1 n i,i0=1,i6=i0	n
`---------------------------------------------------} '----
z
,ʌ .
=：r(f,r)
n
X ε2ξf(Xi)2r(Wi∣Zj)2 .
i,j=1
_ - /
{z
=Op(1/n)
Substituting this expansion into (25), we obtain
,ʌ , ʌ , ,
Ln(f, r) ≤ Γ(f, r) + Op (l/n) ≤ SuP Γ(f, r) + Op (l/n).
f ∈F,r∈R
Therefore,
~ , ʌ , ʌ , , ʌ , ,
Ln(f, r) = Ln(f, r) + Op (1/n) = Γ(f, r) + Op (1/n) ≤ SuP Γ(f, r) + Op (1/n).
f ∈F,r∈R
□
Then we show Theorem 1.
Step (i): Bound of Ln(f,r). A goal of this step is to show that the definition of f implies that for
some ≥ 0, with high probability,
~	, O .、
Ln(f,r) ≤ e.
From Lemma 7,
~ , ʌ , ʌ , ,
Ln(f, r) ≤ Γ(f, r) + Op (1/n) ≤ sup Γ(f, r) + Op (1/n).
f ∈F,r∈R
A rest of this step is to bound SuPf ∈F,r∈R Γ(f, r). We bound the tail-probability by the Talagrand’s
inequality (Theorem 3.3.9 in Gine & Nickl (2021)) for U-statistics (Theorem 5 in Arcones (1995), or
Theorem 1 in Li et al. (2014)) as the following inequality; for any δ > 0, with probability at least
- δ, we obtain
f∈SFu,rP∈RΓ(f,r).ED f∈SFu,rP∈RΓ(f,r)
iog(1/s)
n
provided the uniformly bounded and finite variance properties over F × R.
To the end, we study an expectation of Γ(f, r) in terms of the dataset D = {(Yi, Xi, Zi)}n=ι and
obtain
ED	SuP Γ(f, r)
f ∈F,r∈R
≤ED[r(f*,r*)]+E f,fo∈FPf0・Rlr(f,r)τ(f")1
B
. 0 + n-1/2
0
VzlogN(δ,F×R,k ∙ k)dδ.
Here, for the joint set F × R, we define a distance k(f, r)k := kfk + krk. The last inequality follows
Γ(f *,r*) = 0 and the sub-Gaussianity inequality (Corollary 2.2.8 in van der vaart & Wellner (1996))
associated with the sub-Gaussianity ofεi and the Lipschitz continuity of Γ(f, r) in (f, r).
21
Published as a conference paper at ICLR 2022
By these results, we obtain
,ʌ .
Ln(f,r)
B
≤ n-1/2 Z0
√logN(δ, F ×R, k ∙ k)dδ + O
log(1∕δ)
n
.n-1/2 ZOB PlogN(δ0,F,∣Hl)dδ0 + n-1/2 /B ,logN(δ0, R, ∣∏∣)dδ0 + O ( Jlogn/δ)
which follows log N (δ, F × R, k ∙ k) . log N(δ,F, k ∙ k) + log N (δ, R, ∣H∣).
Step (ii): Bound of ∆f. To bound ∆f = L(f,r*) 一 Ln(f,r*), We apply the concentration
inequality on empirical processes (displayed in Proposition 3, which is originally developed in
Theorem 5 in Arcones (1995)). We regard h in Proposition 3 as (y, x, y0, χ0) → (y 一 f * (χ))(y0 —
f *(x0))∆f (x)∆f (x0)r(y, x|Zj)r(y0, XiZj) and obtain
∆f = EW,W0,Z [(f*(X) 一 f(X))r(W|Z)(f*(X0) 一 f(X0))r(W0|Z)]
nn
- -X nn-1	X	(f*(Xi) - f (Xi))r(Wi∣Zj)(f *(Xi0) - f(Xi，))r(Wi，|Zj)
n j=1 n n	i,i0=1,i6=i0
.
log(1∕δ)
n
with probability at least δ for any δ ∈ (0, ).
Step (iii): Conclusion. Finally, by combining the above results, with probability at least - - δ with
δ > 0,
O
O
O
O
O
O





," .. ," .. ," .. ," .. ," . ," .
L(f, r*) = L(f, r*) - Ln(f, r*) + Ln(f, r*) - Ln(f, ^ +Ln(f, ^
|
}
I
}
z
二∆f
z
= ∙.∆r
产
≤ ∆f + ∆r + n-1/2	√logN(δ0,F ×R, k ∙ k)dδ0
0
log(1∕δ)
n
B ,______
.O(kr - r*k)+ n-1/2	Plog N (δ,F ,∣H∣) dδ
0
+n-1/2 / B Plog ms, r,∣h∣) dδ+o (rogn/^ɪ)
产 ________ B ___________
=n-1/2	plogN(δ, F, k ∙ k)dδ + n-1/2	，logN(δ, R, k ∙ k)dδ
00
+ O (max { Jlogn"), nι∕(2+γ) })	(from Lemma I)∙
C Experimental settings and additional results
C.1 Simulation studies using economics datasets
Here, we report the details of the model and hyperparameters used in our experiments. Since the
network structures used in Ai & Chen (2003) and Newey & Powell (2003) have many common
features, we describe the network structure in Newey & Powell (2003).
For DeepGMM, DFIV, DeepIV, and KIV, we use the same model and hyperparameters published
by Xu et al. (2021a). For IW-LS, we use the following two networks for estimating density ratio
and predicting f* respectively : FC(4,128)-FC(128,128)-FC(128,1) and FC(1,128)-FC(128,128)-
FC(128,1). Each fully-connected layer (FC) is followed by leaky ReLU activations with leakiness
22
Published as a conference paper at ICLR 2022
(里 eUS Ol:60_)UJ5W qdEESIJollno
Figure 5: Demand design experiments with 5, 000 samples. The left graph show the results with
ρ = 0.25 and the right graph shows the results with ρ = 0.75.
5 0 5 0 5 0 5
.7.52 0.7.52
4 4 4 43.3.3
ω3s OT60)UJSW @dEeSnno
亩一eUS Ol6O_)UJSW<υ-dEeSljoI:Ino
IW LS IW MM IW Krnl LS DeepIV DeepGMM KIV DFIV
Figure 6: Demand design experiments with stronger correlation between Xi and εi . The sample sizes
are 1, 000. The left graph show the results with ρ = 0.25 and the right graph shows the results with
ρ = 0.75.

O

α = 0.2. A regularization coefficient η is set to 0.001 as a result of cross-validation. For LS, we
use the same network structure in IW-LS for predicting f *. For IW-MM, We use the same network
structure as IW-LS. For IW-Krnl, the same network structure as IW-LS to estimate the density ratio
and ζ and σ2 are selected via cross-validation. In Ai & Chen (2003) experiment, we change only the
first layer in the network structure to match the dimension of Xi , and the rest of the network structure
is the same.
In addition to the original settings of Newey & Powell (2003) and Ai & Chen (2003), we investigate
cases where we add more IVs.
In Newey & Powell (2003), we generate {(Yi,Xi, Zi,1, Zi,2, Zi,3, Zi,4)}in=1, where Zi,2, Zi,3, Zi,4
are additional IVs, as follows: first, they generate {(εi, Ui, Zi,1, Zi,2, Zi,3, Zi,4)}in=1 from the mul-
tivariate normal distribution N
0
0
0
0
0
0
1	0.5	0 0 0 0
0.5	1	0 0 0 0
0	0	1 0 0 0
0	0	0 1 0 0
0	0	0010
0	0	0001
; then, they generate Xi =
Zi,1+Zi,2+Zi,3+Zi,4+UiandYi =f*(Xi)+εi,wheref*(Xi) =ln(|Xi-1|+1)sgn(Xi-1).
Here, εi and Ui are unobservable.
In Ai & Chen (2003), they generate {(Yi,Xi, Zi, Wi,1, Wi,2, Wi,3)}in=1, where Wi,1, Wi,2, Wi,3
are additional IVs, as follows: first, we generate {(εi, X1i, Vi, Ui)}n=1 as εi 〜N(0, X2i + 匕2),
Xii i÷d∙ Unif[0,1], V 叼 Unif[0,1], (Wi,2) i如 N ((0),(俏0:
Wi,3	0	0.3	0.3
001..33!!!, Wi
P3=1 Wi,j, and UiN(0, X2i + Vi2 + | Wi|); second, we generate X2i = Xii + Vi + Wi +
R × εi + Ui and Y1 = X1iγ0 + h0 (X2i) + εi, where h0 (X2i) = exp (X2i) / (1 + exp (X2i)) and
R is chosen as 0.9; then, obtain Xi = (Xii X2i)> and Zi = (Xii Vi Wi,i Wi,2 Wi,3). Here, εi and
Ui are unobservable, and f* (Xi) = Xiiγ0 + h0 (X2i), where the function h0 and γ0 are unknown.
C.2 Simulation studies using demand design datasets
For DeepGMM, DFIV, DeepIV, and KIV, we use the exact same model and hyperparameters in Xu
et al. (2021a). For LS and IW-LS, we use the network structure based on the economics datasets
network. η is 0.001 as a result of cross-validation. For IW-Krnl, we use the same network structure
as IW-LS to estimate the density ratio and select ζ and σ2 via cross-validation.
23
Published as a conference paper at ICLR 2022
Figure 7: Demand design experiments with stronger correlation between Xi and εi . The sample sizes
are 5, 000. The left graph show the results with ρ = 0.25 and the right graph shows the results with
ρ = 0.75.
Figure 5 shows the results with 5000 samples. First, our proposed IW-LS outperforms the existing
methods and minimizes the MSE. Second, LS, which is a naive nonlinear regression without IV,
outperforms the other existing methods using IV. In contrast, IW-LS outperforms LS.
We consider the case where endogeneity causes a larger change in outcome. Since the original
demand design dataset has a small effect of the bias, we slightly change Vi in the price equation:
Pi = 25 + (Ci + 3)h(Ti) + 10Vi. In the original dataset, Yi is generated as Yi = 100 + (10 +
Pi)Sih(Ti) - 2Pi + εi. However, because the impact of εi on the price is very limited, because
the variance of εi is relatively small compared to that of Yi . This is the reason why the LS also
performs well in the previous result; that is, we can obtain good performance even when ignoring
endogeneity (because f * (Xi)) is close to E[Yi ∣Xi]). For this reason, this dataset is not appropriate
for investigating the performances of the methods, although it is used in existing studies. Here, we
consider the different model Yi = 100 + (10 + Pi)Sih(Ti) - 2Pi + 100εi, in which we multiply
the error term by 100. In this case, the bias has a more serious impact on the values of price and
outcome. Figure 6 shows the results with 1, 000 samples, and Figure 7 shows the results with
5, 000 samples. Regardless of the sample size and ρ, our proposed method outperforms existing
methods. Even in this experiment, the LS still performs well. We consider that this is because
the correlation between Xi and εi is not strong. In the dataset, Vi , which is a cause of the bias,
follows a standard normal distribution and has a limited impact on price due to the constant term 25
(Pi = 25 + (Ci + 3)h(Ti) + Vi), that is, in the variation of Pi, Vi has a small effect compared with
the other variables.
In this dataset, f*(Xi) takes large values compared to the error term. Under this situation, a model
trained to predict Yi may perform well because the influence of E[εi |Xi] = 0 is limited. However,
the purpose of using NPIV in the first place is because the latter influence is large, or else effects
of E[εi |Xi] = 0 can be ignored. AS expected, in our experiments using Hartford et al. (2017), the
least-squares method also performs well, even though the training process ignores the problem of
NPIV. To investigate the performance for causal inference, we recommend using simpler datasets,
before using more complicated datasets.
We also show how the MSE of the IWMM decreases as the sample size grows in Appendix C.4
C.3 Simulation studies using MNIST datasets
We also investigate how our proposed method performs on high-dimensional data. By using MNIST
dataset (LeCun & Cortes, 2010), we convert the low dimensional IVs of economics artificial datasets
used in Newey & Powell (2003) and Ai & Chen (2003) to high-dimensional IVs. We compare our
proposed methods, IW-LS and IW-MM, with the LS and KIV.
Extension of the dataset in Newey & Powell (2003): For creating high-dimensional IVs, we equip
the IVs used in Newey & Powell (2003) to the feature vector of the MNIST dataset. Let Di ∈ R100
be a randomly chosen feature vector of the MNIST dataset of the number 0. We reduce the dimension
to 100 by using the principle component analysis. We multiply the original IV in Newey & Powell
(2003) by the feature vector to convert the low-dimensional IV to the high-dimensional IV; that is,
we create a new IVs Zi ∈ R100 as Zi = Zi Di. Instead of Zi, we use this new IV Zi and estimate the
structural function f*. The sample size is 2, 500. The other settings are the same as Section 6.1. The
experimental results are shown in the left figure of Figure 8.
24
Published as a conference paper at ICLR 2022
Figure 8: The log10 scaled MSEs using the setting in Newey & Powell (2003). The left graph shows
the results using the original dataset with the MNIST dataset. The right graph shows the results with
additional IVs and the MNIST dataset.
Figure 9: The log10 scaled MSEs of dataset in Ai & Chen (2003) with the MNIST dataset. The left
graph shows the result with R = 0.1 and the right graph shows the result with R = 0.9.
Next, We add an IV to the original dataset in Newey & PoWen (2003) as {(Yi, Xi, Zi,1, Zi,2)}忆1 ~
0
0
0
0
1	0.5
0.5	1
00
00
00
0 0
10
01
, Where Zi,2 is an additional IV. Then, We generate Xi = Zi,1 +
Zi,2 + Ui and Yi = f * (Xi) + ε, Where f * (Xi) = ln(∣Xi - 1| + 1) Sgn(Xi- 1). Here, εi and Ui
are unobservable. Let Di,1 ∈ R100 be a randomly chosen feature vector of the MNIST dataset of
the number 0, and Di,2 ∈ R100 be a randomly chosen feature vector of the MNIST dataset of the
number 1. We reduce the dimension to 100 by using the principle component analysis. We multiply
the original IVs by the feature vector to convert the loW-dimensional IVs to high-dimensional IVs;
that is, we create a new IV Zi,1 ∈ R100 as Zi,1 = Zi,1 Di,1 and Zi,2 ∈ R100 as Zi,2 = Zi,2Di,2. We
use these new IVs Zi,1 and Zi,2 to estimate the structural function f *. The experimental results are
shown in the right figure of Figure 8.
N
Extension of the dataset in Ai & Chen (2003): We also equip the MNIST dataset to the IVs
of the dataset used in Ai & Chen (2003). Let Di,1 ∈ R100 be a randomly chosen feature vector
of the MNIST dataset with the number 0, and Di,2 ∈ R100 be a randomly chosen feature vector
of the MNIST dataset with the number 1. We reduce the dimension to 100 by using the principle
component analysis. We transform the original IVs in Ai & Chen (2003), Wi,1, Wi,2 ∈ R, to the
high-dimensional IVs by multiplying them with the MNIST feature vectors; that is, we create a new
IV Wi,ι ∈ R100 as Wi,ι = Wi, 1 Di,ι and Wi,2 ∈ R100 as Wi,2 = Wi,2Di,2. We use these new IVs,
Wi,1 and Wi,2, to estimate the structural function f *. The sample size is 2, 500. The other settings
are the same as Section 6.1. The experimental results are shown in Figure 9.
C.4 Simulation Studies on the Rate of the MSE
We show experimental results on the MSEs of the NPIV and the classical 2SLS under various sample
sizes. Here, we have two goals: (i) we validate the theoretical MSE derived in Section 5.3, and (ii)
we show the relative performance of the NPIV method against the classical 2SLS in the learning
problem with conditional moment restrictions.
In Figure 10, we show the MSEs of the IWMM in the setting of Newey & Powell (2003) with various
sample sizes. We follow the same setting used in Section 6.1, except for the choices of sample sizes.
We investigate whether the empirical MSE of IWMM converges to 0 with Op (1/√n) for sample size
n, which is theoretically provided in Section 5.3 under some assumptions. We compute the empirical
MSEs with sample sizes {500, 1000, 1500, 2000, 3500, 4000, 4500, 5000} with 10 trials, then obtain
25
Published as a conference paper at ICLR 2022
Figure 10: MSEs of the IWMM in the setting of Newey & Powell (2003) with various sample sizes.
The light blue region represents the standard deviation.
Figure 11: MSE of the IWMM in the setting of Newey & Powell (2003).
its mean and standard deviation. In Figure 10, We compare the empirical MSEs with the line that
represents 1 /√n. As in our theoretical results, the MSEs decays in Op(1 /√n).
With the same setting, we also compare the MSEs of IWMM with the classical 2SLS in Figure 11, to
understand the effectiveness of the NPIV method. We consider three models for the 2SLS:
Yi = β0 + β1 Xi + εi ,
Yi = β0 + β1 Xi + β2Xi2 + εi ,
Yi = β0 + β1 Xi + β2Xi2 + β3Xi3 + εi .
The difference between these three models lies in the choice of the polynomial basis. For instance, the
last model approximates the structure function by a cubic function. If a model introduces an infinite
number of polynomial bases, they can approximate various smooth functions by the series expansion.
In other words, as the number of the polynomial basis increases, the classical 2SLS approaches the
NPIV. This method of introducing a basis is called sieve regression in econometrics, and Newey &
Powell (2003) proposed using it to solve NPIV. The experimental result in Figure 11 also shows that
the second and third models, with the polynomial bases, Xi2 and Xi3 , perform closer to the NPIV
method than the first model using only Xi .
It is important to note that the 2SLS with a polynomial basis function is difficult to implement in
high-dimensional situations. In the setting of Newey & Powell (2003), because both X and Z have
one dimension, polynomial approximation is effective. However, if the dimension increases, the series
expansion of multivariate functions requires a huge number of basis functions, hence approximation
becomes very difficult. In machine learning, for instance, Singh et al. (2019) proposes to introduce
RKHS to avoid this difficulty.
26