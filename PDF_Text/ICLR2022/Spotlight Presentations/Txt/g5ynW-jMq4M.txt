Published as a conference paper at ICLR 2022
Properties from mechanisms:
AN EQUIVARIANCE PERSPECTIVE ON IDENTIFIABLE
REPRESENTATION LEARNING
Kartik Ahuja*, Jason Hartford "& Yoshua Bengio
Mila - Quebec AI Institute, Universite de Montreal
Quebec, Canada
{kartik.ahuja,jason.hartford,yoshua.bengio}@mila.quebec
Ab stract
A key goal of unsupervised representation learning is “inverting” a data generating
process to recover its latent properties. Existing work that provably achieves this
goal relies on strong assumptions on relationships between the latent variables
(e.g., independence conditional on auxiliary information). In this paper, we take a
very different perspective on the problem and ask, “Can we instead identify latent
properties by leveraging knowledge of the mechanisms that govern their evolution?”
We provide a complete characterization of the sources of non-identifiability as
we vary knowledge about a set of possible mechanisms. In particular, we prove
that if we know the exact mechanisms under which the latent properties evolve,
then identification can be achieved up to any equivariances that are shared by the
underlying mechanisms. We generalize this characterization to settings where we
only know some hypothesis class over possible mechanisms, as well as settings
where the mechanisms are stochastic. We demonstrate the power of this mechanism-
based perspective by showing that we can leverage our results to generalize existing
identifiable representation learning results.These results suggest that by exploiting
inductive biases on mechanisms, it is possible to design a range of new identifiable
representation learning approaches.
1	Introduction
Modern unsupervised learning techniques can generate images of our world with intricate detail (e.g.
Karras et al., 2019; Song et al., 2020; Razavi et al., 2019), and yet, the latent representations from
which these images are generated remain entangled and challenging to interpret (SCholkOPf et al.,
2021; Locatello et al., 2019). At the same time, the success of pre-trained transformers (Radford
et al., 2021; Brown et al., 2020) shows that advances in our ability to extract latent representations
can lead to dramatic improvements in the sample complexity of downstream tasks (Bengio & LeCun,
2007; Bengio et al., 2013). In order to consistently replicate this success, we need methods that can
reliably invert the data generating process into its underlying generative factors. This is a challenging
task because unsupervised representation learning with independent and identically distributed data is
hopelessly unidentified: even in the nice case in which observations, x, are generated with independent
latent variables, p(z) = Qi p(zi), there exists an infinitely many distributions with independent
latents P(Z) that are consistent with the observed marginal distribution, p(χ), and only one of them
corresponds to the true latent distribution p(z) (Locatello et al., 2019; Khemakhem et al., 2020a).
If we want to build systems which are able to provably identify * 1 the true generative factors z that
generated our observed data x = g(z) for some observation model g, then we need to exploit
structural assumptions that constrain this set of possible distributions. Most of the prior work that
can provide such guarantees was developed in the independent component analysis (ICA) literature.
The key ICA assumption is that the latent factors are (conditionally) independent and non-Gaussian.
Then, if the observation model, g : Z → X is linear and invertible, z = g-1(x) is identified (Comon,
* equal contribution, author order selected randomly.
1A problem is identified if the true generative factors are a unique solution in the infinite data limit.
1
Published as a conference paper at ICLR 2022
Figure 1: This simple data generating process illustrates that if we know the set of mechanisms that
govern the evolution of an environment, this constrains the set of possible representations to any
equivariances of these mechanisms. At each time step, we observe an environment of bouncing balls
in pixel space as images, xt . These images are produced by some rendering engine, g, as a function
of the true latent representation zt , which in this case gives positions and velocities of each ball
(illustrated by the location and length of the arrows in the latent representation). At each time step, the
state evolves according to a mechanism, mt . Any candidate model that is consistent with the observed
data has to satisfy the observation identity. If an encoder produces either the true representation
(shown in green), or a representation transformed by some equivariance of the mechanism (e.g. the
blue rotated representation) then the observation identity is satisfied. However, models that produce
representations that are arbitrary transformations of the true representation (e.g. the pink warped
representation) can be discarded as they are not consistent with the observation identity.
1994), and for nonlinear g there are a number of recent approaches that leverage non-stationarity in
the distributions over z to identify g-1 (Hyvarinen & Morioka, 2016; 2017; Hyvarinen et al., 2019;
Khemakhem et al., 2020a). These results give a tantalizing demonstration that representation learning
with identification guarantees is possible, but the requirement that the latent factors are statistically
(conditionally) independent is limiting2 (Higgins et al., 2018; SCholkoPf et al., 2021).
In this paper we take a very different approach. We study how the mechanisms that govern an
environment’s evolution constrain the set of possible latent representations that are consistent with
the data. As a simple concrete example, consider the bouncing ball environment shown in Figure
1. The latent state can be completely described by a vector, z, containing the position, velocity and
acceleration of the balls3, and given this latent state, the images, x, shown in Figure 1, are produced
via some rendering engine g : Z → X. Our task is to leverage sequences of observations x1, . . . , xT
and knowledge of m to recover z1, . . . , zT. If we can show that this task has a unique solution that is
consistent with the observations and mechanisms, then the problem is identified.
Our main result is that when we know the true mechanism, m, the system is identified up to any
equivariances of the mechanism; or equivalently, in Section 2.2 we prove that we can identify z
up to any invertible transformation a : Z → Z that commutes with m, such that the composition
m ◦ a = a ◦ m. For example, in the bouncing balls environment shown in Figure 1, the laws of physics
are equivariant with respect to your choice of units of measurement—changing from representing z in
meters to inches leaves the output of the mechanism unchanged up to a corresponding unit change—
and hence we can only hope to identify z up to some scaling function a(z) which corresponds to an
arbitrary choice of units of measurement. Interestingly, when the environment evolves according to
multiple known mechanisms, the sources of non-identifiability are even further constrained: such a
system is identified up to equivariances that are shared by all n mechanisms.
2As a simple example of dependence between latent variables, assume the bouncing balls shown in Figure 1
have different masses indicated by their colors. If the initial conditions were such that all the balls have the same
momentum, then mass and velocity will be inversely correlated.
3A complete generative model of these images would also need to track the colors and shapes of the elements;
we will return to this issue in the discussion of the results.
2
Published as a conference paper at ICLR 2022
Perfectly knowing the true mechanisms and when they are applied is unlikely, but in Section 2.3 we
show that we can relax that assumption to a setting where we instead know a hypothesis class of
possible mechanisms that could have been applied. This weaker assumption leads to an additional
source of non-identifiability: the mechanisms in our hypothesis class can imitate each other if there
exists an invertible transformation a such that for any two mechanisms m1 and m2 in our hypothesis
class, m2 = a-1 ◦ m1 ◦ a. For example, if we are in a setting where our hypothesis class includes
both a product mechanism, m1(z) = Qi zi, and a sum mechanism, m2(z) = Pi zi, then m1 can
imitate m2 if a(z) = exp(z), since Pi zi = log(Qi exp(zi)). As before, this result is complete, in
the sense that these two sources of non-identifiability—equivariance and imitation—are the only
sources of non-identifiability in such systems. This gives us a natural way of thinking about the way
knowledge of deterministic mechanisms constrains a representation learning task: with complete
knowledge, the only source of non-identifiability is any equivariances inherent in the mechanism, but
by allowing a hypothesis class of possible mechanisms, we introduce potential non-identifiability
via imitation. That said, the relationship between the size of the hypothesis class and the size of the
set of a’s that commute via imitation is not necessarily monotonic: for environments governed by
multiple mechanisms, a larger hypothesis class can lead to fewer imitations.
Section 3, shows that we can derive analogous results for stochastic mechanisms, m(z, U), where
the mechanism defines a conditional distribution p(zt+1|zt). This generalization gives us a way of
comparing our mechanism-based perspective with existing identifiability results. We demonstrate
this in Section 4 by showing that it is possible to view the distributional assumptions made in Klindt
et al. (2020) as a particular choice of mechanism, and by doing so, we can leverage our theory to give
alternative proofs of these results. This strategy required weaker distributional assumptions, thereby
generalizing their result. Finally, we give a mechanism-based perspective on the related work in
Section 5 and Section 6 concludes with a discussion of the open problems that need to be addressed
in order to reliably leverage this approach in practice.
2	Mechanism based identification
2.1	Data generation process
The state of the system at time t ∈ {1, ∙∙∙ ,T} is given by Zt ∈ Z ⊆ Rd. At each time t, We observe
g(zt) = xt ∈ X ⊆ Rn, which is some transformation of the latent zt. We can think of g : Z → X
as a function that transforms the (typically loW dimensional) state variables to the (typically high
dimensional) observed variables; for example, in the bouncing ball environment described in the
introduction, g is the rendering engine that produces the images shoWn in Figure 1. We assume g is
injective 4 With respect to Rn—i.e. g(z1) = g(z2) implies z1 = z2; or equivalently, any change to
the underlying state, z, is reflected in some pixel-level change to the observation x—and We make g
bijective by restricting its inverse g-1 to any x on the data manifold Which We denote X (i.e. X is
the image of g). The state transition from time t to t + 1 is governed by a mechanism mt : Z → Z .
There may be multiple mechanisms in a given environment. For example in Figure 1 the transition
from z1 to z2 does not involve any collisions so the state evolves according to NeWton’s first laW of
motion (NeWton, 1687); the transition from z2 to z3 involves a collision betWeen tWo of the balls that
is described by NeWton’s third laW. Together mt and g describe the data-generation from time t to
t + 1 as folloWs,
Xt J g(zt),	Zt+1 J mt(zt).	(1)
If We fix the initial conditions, z0 , this data generation process is deterministic. In Section 2.2, We
provide results When the underlying mechanism is knoWn and in Section 2.3, We We extend those
results to the case When the mechanisms are not knoWn. In Section 3, We extend our results to
stochastic mechanisms, m(Zt, U) that take samples from some distribution U 〜Uniform(0,1) as
input. These stochastic mechanisms can represent any conditional distribution P (Zt+1 |Zt) (Austin
(2015, Lemma 3.1))
2.2	Identifying encoders when the underlying mechanism is known
We begin in the simplest version of the system described by equation (1): assume that at each time t
the same mechanism m : Z → Z is used, and We knoW this mechanism. From these assumptions We
4for ball collision example, if Z equals position and velocity, We can achieve injectivity by frame stacking.
3
Published as a conference paper at ICLR 2022
can derive an identity that describes how the observations xt and xt+1 are related,
zt+1 = m(zt),	g-1 (xt+1) = m ◦g-1(xt), xt+1 = g ◦m ◦ g-1(xt).	(2)
It may be helpful to think of this identity as describing an autoencoder where we require that the
encoder g-1(xt) inverts the data generating process to produce some latent zt from xt, and that the
decoder has to reproduce xt+1 from m(zt); i.e. the representation transformed by the mechanism.
Importantly, this identity describes the true data generating process, rather than some model of it. Our
hypothesis class over the possible encoder / decoder functions is the set, G, of all bijective functions
from Z → X . By assuming that bijectivity we are essentially assuming that the reconstuction task
is solved:5 G is the set of all autoencoders that perfectly reproduce the data, such that for any x on
the data manifold X, and any encoder / decoder pair (g-1, g) with g ∈ G, their composition is the
identity function, X = g ◦ g-1 ◦ x. Because we assumed that the true g is bijective, we know that it
is in our search space, G. We can constrain this set using our knowledge of the mechanism by only
considering solutions that also satisfy the identity given in equation 2, such that for every pair of
observations (xt, xt+1), an analogous identity holds,
xt+1 = gg ◦ m ◦ gg-1(xt)	(3)
Now suppose that we have access to observations xt from the entire set X, then the above identities
have to hold for all xt , ∈ X with corresponding xt+1 from equation 2, so we can conclude that the
following functions are equal,
g ◦ m ◦ g-1 = gg ◦ m ◦ gg-1	(4)
This relationship will hold for any of the possible decoders gg (and corresponding encoders gg-1)
that are observationally equivalent given our assumptions. We denote the set of all such decoders,
Gid = {g | g ∈G,g ◦ m ◦ g-1 = g ◦ m ◦ g-1}. If Gid = {g} then we have shown that the problem is
exactly identified, which means that if we manage to find a gg that satisfies equation 2, then gg = g; but
if Gid , also includes other functions gg 6= g, then these functions are the sources of non-identifiability.
Equivariant mechanisms To see an example of a setting where the problem is not exactly iden-
tified, consider a mechanism which is equivariant with respect to some bijective transformation
a : Z → Z. A mechanism m is said to be equivariant w.r.t a if a ◦ m = m ◦ a. For example,
a mechanism derived from Newtonian mechanics may be equivariant with respect to your choice
of units of measurement, such that m(cz) = cm(z) for some scaling constant c. Similarly, if a
mechanism transforms sets of items, any permutation of z would lead to a corresponding permutation
of the mechanism’s output.
If we have a mechanism that is equivariant with respect to some transformation a (where a is not
identity map), that implies that there exists a function gg 6= g in Gid , so the problem is not exactly
identified. We can see this as follows,
-1	-1	-1	-1	-1	-1
g ◦ m ◦ g- = g ◦ a- ◦ a ◦ m ◦ g- = g ◦ a- ◦ m ◦ a ◦ g- = g ◦ m ◦ g-
where the first equality uses the fact that a-1 ◦ a is the identity function, the second applies the
definition of equivariance, and the final equality defines gg := g ◦ a-1 and gg-1 := a ◦ g-1. This
shows that if the mechanism is equivariant, an encoder, gg-1, can output a transformed zg = a(z),
and the decoder, gg, inverts this transformation before producing its output, thereby leaving the
observed variables, x, unchanged. If we denote the set of all equivariances of the mechanism
E = {a | a is a bijection, a ◦ m = m ◦ a}, then we can define the set of all such sources of
non-identification as, Geq = {gg | gg = g ◦ a-1, a ∈ E}. This is a natural source of non-identification:
given that we are relying on the mechanism to constrain the encoder gg-1, it is unsurprising we cannot
prevent transformations that are not affected by the mechanism. The more interesting observation,
which we will show below, is that this set is the only source of non-identification when the mechanism
is known, and hence we recover the true z up to equivariances in the mechanism.
To state this theorem, we first define the notion of identifiability up to an equivalence class defined by
a family of bijections A, where a ∈ A is a map a : Z → Z .
Definition 1. Identifiability up to A. If the learned encoder gg-1 and the true encoder g-1 are
related by some bijection a ∈ A, such that gg-1 = a ◦ g-1 (or equivalently gg = g ◦ a-1), then gg-1 is
Said to learn g-1 up to bijections in A. We denote this g-1 〜A g-1.
5This is obviously a strong assumption—learning autoencoders that perfectly reconstructed the data is not at
all easy—but it focuses the discussion on the identification issues that remain after reconstruction is solved.
4
Published as a conference paper at ICLR 2022
Suppose, for example, A is a family of a permutations. Identifiability up to A implies that the true
latent variables will be recovered, but that they would not necessarily be ordered in the same way as
they were in the original data generation process. In this setting where the mechanism, m, is known,
the following theorem shows that A is just E, the set of equivariances of m.
Theorem 1. If the data generation process follows equation 2, the encoders that solve the observation
identity in equation 4 identify true encoder g-1 UP to equivariances of m (g-1 〜E g-1).
To prove the above theorem, we need to establish that the set of solutions to the identity in equation 4
and the set of maps derived from equivariances are equal Gid = Geq . The proof is given in Section
A.1 of the appendix. From Theorem 1, we can derive a number of observations. First, notice that if
we have a standard autoencoder6 then the mechanism is the identity map, m(z) = z, and its set of
equivariances E is any invertible function a, and hence Theorem 1 shows that the encoder is essentially
unconstrained. However, if the mechanism is any non-trivial function, m(z0) 6= z0 for some z0, then
the space of possible encoders is significantly reduced to just those invertible transformations that
commute with m. If the system involves multiple known mechanisms, {m1, . . . , mT}, where at
each time zt+1 = mt(zt), then the encoder is even further constrained. Define the set of all the
mechanisms that are used at least once in the evolution of the system as M* = ∪T=ι{mt}. Suppose
Ei denotes the equivariances of mi ∈ M*. Define the equivariances that are shared across all the
mechanisms as E* = ∩iEi ;
Corollary 1. If the data generation Process follows equation 1, then the encoders that satisfy
observation identity in equation 4 for all m ∈ M* identify true encoder g-1 uP to the equivariances
shared across all the mechanisms in M*, E* (g-1 〜e* g-1).
The proof of the above claim is in Section A.3. This corollary implies a blessing that comes with more
complex environments: if an object is transformed by multiple mechanisms which are diverse (in the
sense that they share few equivariances), then it becomes easier to identify. Given access to inputs
and outputs of a mechanism, if we cannot tell apart whether some transformation was applied to the
input or the output, then the mechanism is equivariant with respect to the transformation. Together
Theorem 1 and Corollary 1, state that we can learn to invert the data generation process but we cannot
distinguish latents that were transformed by equivariances shared across the mechanisms.
2.2.1 Identifying encoders for affine mechanisms
Theorem 1 shows us that an encoder g-1 is identified up to any equivariances of the known mechanism,
but given some mechanism, it does not tell us what equivariances it may exhibit. This section gives
an example of how one might go about finding all sources of equivariance for a given mechanism.
We derive the equivariances for affine mechanisms, and in doing so we show conditions under which
affine mechanisms lead to identification up to some fixed offset. Affine mechanisms are broadly
applicable because with a sufficiently short time interval, they approximate a wide variety of physical
systems as the Euler discretization of some linear ordinary differential equation. In such systems,
the mechanism is given by m(z) = Mzt + bt where M ∈ Rd×d is an invertible diagonalizable
matrix (with eigendecomposition given as M = SΛS-1, where S is the matrix of eigenvectors and
Λ is a diagonal matrix of eigenvalues), bt ∈ Rd is the offset parameter at time t, and the analog of
equation 2 is,
xt+1 = g(M g-1(xt) + bt).
We search for some encoder g-1 such that this relationship holds for all X and t. Define an offset
function o(z) = z +p, where the offset function shifts the latent by a vector p. Define O to be the set
of all the offset functions. We show that the encoder is identified up to O when we have at least two
distinct offset terms bt and a regularity condition (Assumption 2).7
Assumption 1. In the data generation Process in equation 1, we set m(zt) = Mzt + bt, where M is
invertible and diagonalizable. We assume that the offset bt takes at least d + 1 distinct values, which
we denote by {b1, ∙∙∙ , bd+1}. The set {b2 一 b1, ∙∙∙ , bd+1 - b1} of vectors is linearly independent.
Assumption 2. a : Z → Z is analytic and satisfies the following assumPtion. For each comPonent
i ∈ {1,…，d} of ai(z) and each b ∈ Rd, define the set Sij = {θ | Vjai(z + b) = Vjai(z) +
Vjai(θ)b, Z ∈ Rd}. Each set Sij has a non-zero Lebesgue measure in Rd.
6With the constraint that the encoder is the inverse of the decoder such that g-1 is bijective.
7We conjecture that the regularity condition holds for all analytic functions and is thus not needed. Since we
do not have a proof of this claim, we include it as an assumption.
5
Published as a conference paper at ICLR 2022
Theorem 2.	If the data generation process follows equation 1 with affine mechanisms, m(z) =
Mzt + bt, Assumptions 1, 2 hold, the eigenvalues of the mechanism M are all distinct, and each
component of S-1(bi - bj) is non-zero for some i 6= j, then the encoders that solve the observation
identity in equation 4 identify true encoder g-1 up to offsets O such that g-1 〜o g-1.
The proof is given in Section A.4 in the Appendix. The above theorem shows the power of using
multiple mechanisms. It can be shown that if there is only one mechanism, then we cannot do better
than linear identification. However, if we use two mechanisms as is the case in the above theorem, the
constraint of shared equivariances (Theorem 1 and Corollary 1) enforces almost exact identifiability
(only offset-based errors remain).
2.3 Identifying encoders when the mechanisms are not known
We have seen in the previous section that with complete knowledge of the mechanisms under which a
system evolves, we can learn an encoder up to equivariances. In practice, however, we are unlikely
to have such complete knowledge. In this section, the system still evolves according to some
deterministic mechanism, zt+ι — mt(zt), but We assume that you only know some hypothesis class
M of possible mechanisms which could have been used, without knowing which mt ∈ M is used at
every time step.
A candidate solution now needs to propose both an encoder g-1 and a mechanism mt ∈ M for every
(xt, xt+1) pair such that,
χt+ι = g ◦ mt ◦ g-1(χt).
(5)
As before, this relationship holds for all xt ∈ X , where xt+1 is generated from mt, so any candidate
solution that is consistent with the x’s that we observe must satisfy
11
g ◦ mt ◦ g- = g ◦ mt ◦ g-
(6)
We partition the hypothesis class of mechanisms, M = M* ∪ M0, into the mechanisms that are
used at least once in the evolution of the system, M* (M* = ∪T=1{mt}), and mechanisms which
are hypothesized but not used, M0. We define the set of all decoders gg (with corresponding encoders
gg-1) that solve equation 6 across all the time steps as as Gid = {gg | gg is a bijection , for each mt ∈
M*, ∃ mg t ∈ M, such that g ◦ mt ◦ g-1 = gg ◦ mg t ◦ gg-1}. This set looks very much like the set Gid
that we defined in Section 2.2, but the fact that we have to select mg ∈ M rather than knowing the
true m implies a new source of non-identifiability: imitator mechanisms.
Definition 2. Equivariances and imitators w.r.t M. Define a set of functions E that satisfy com-
mutativity w.r.t the set of mechanisms M in the following sense. The set E comprises of all the
bijections, a(∙), that satisfy the following condition. Iffor each m1 ∈ M*, ∃ m? ∈ M such that
a ◦ m1 = m2 ◦ a, then a ∈ E.
EI	,	∙	,	l' .	.	l' 1	, -rɪ T ∙11	,	,	,1 ∙	,1	F	1	l' . IL J
The set E consists of two types of elements. We illustrate this through an example of set M =
M* = {m1, m2}. If a is a bijection that commutes with both m1 and m2, i.e., a ◦ m1 = m1 ◦ a and
a ◦ m2 = m2 ◦ a, then a ∈ Eg. From this we can see that Eg consists of elements in the intersection
of the equivariances of the respective mechanisms. Alternatively, if a satisfies a ◦ m1 = m2 ◦ a
and a ◦ m2 = m1 ◦ a, then we say m2 “imitates” m1 and vice-versa because you can produce m1’s
output from m2 for any z using the following relationship m1 = a-1 ◦ m2 ◦ a. Further simplifcation
of this yields that a2 = a ◦ a is an equivariance of both m1 and m2. This example shows that when
we know the list of mechanisms M = M* but do not know which mechanism is used, the set E can
be expressed in terms of the equivariances of the mechanisms. For further details see the Appendix
Section A.11. Define the set of maps that are identified up to E as Geq = {gg | gg = g ◦ a-1, a ∈ E}.
Theorem 3.	If the data generation process follows equation 1, then the set of all the encoders
that satisfy equation 6 identify true encoders g-1 up to equivariances and imitators w.r.t M, E
(g-1 〜E g-1).
To prove the above theorem, we follow a similar strategy as in Theorem 1 and establish Gid = Geq .
The proof of the above is in Section A.5 of the Appendix. Equivariances and imitators play similar
roles in the way that they relax constraints on the encoder gg-1—any bijection that commutes with
6
Published as a conference paper at ICLR 2022
either is a source of non-identifiability—but they are different from the perspective of how we should
think about designing representation learning algorithms. Recall that M is composed of two sets of
mechanisms, M = M* ∪ M , mechanisms in M* that are used at least once in the evolution of the
environment and those mechanisms in M which are hypothesized but never used. Equivariances are
dictated only by M*, which characterizes the evolution of the environment. Any increases to the
number of distinct mechanisms in M* will potentially decrease the number of equivariances shared
by all mechanisms. This can only be achieved by modifying the environment in some way, either
through an explicit intervention that modifies its mechanisms or by collecting data from multiple
environments with diverse set of mechanisms. For example, in the bouncing balls example given in
Figure 1, one could change the environment by varying the mass of the balls or observing it under
different gravity conditions; or one could intervene by, say, changing the shape of balls in the system
such that you get a different bouncing mechanism.
Imitators, by contrast, are a function of both the mechanisms M* that were used and those that were
hypothesized, M0. An imitator is just some mechanism that can imitate another via some bijection,
so one would expect that as we grow the number of mechanisms in M, the size of the set of imitators
can only grow; but interestingly, this is not always the case for mechanisms from M* . Recall that
any a ∈ E produces an encoder of the form g-1 = a ◦ g-1, so the same transformation has to be used
for all imitations and equivariances among the mechanisms in M*. Because of this, it is possible that
increasing the size of M* reduces the number of imitators. For example, if there is some mechanism,
mi , that does not commute with any non-trivial a in E (either by imitation or equivariance), then
adding mi to M* will make the problem exactly identified. On the other hand, growing the size
of the set of unused mechanisms, M0, can result in significant non-identifiability. For example, if
M consisted of a flexible class of functions (e.g. a multi-layer perceptron) then it would be easy to
construct imitators of the form m1 = a-1 ◦ m2 ◦ a.
Illustrating Theorem 3. We consider the same setting as in Theorem 2. For each t ∈{1,…，d +1},
m(zt) = Mzt + bt and xt = g(zt). We only know that the mechanism is affine and only the offset
bt is changing, but parameters M and bt are not known. Let us construct the set E corresponding to
the above setting. We can show that the set E consists of affine functions (See the Appendix A.6 for
details). From Theorem 3, we can thus conclude that for this data generation process even with very
little knowledge of the mechanism, we get linear identifiability. This is weaker than the offset based
identifiability in Theorem 2, but there we were required to know the entire affine mechanism.
3 Stochastic mechanisms
The results thus far relied on the assumption that the evolution of a system could be described in
terms of deterministic mechanisms. This deterministic approach models settings where the full latent
state is observable (via some unknown encoder g-1) at a short enough time interval that there is no
uncertainty about the system’s evolution. To generalize to cases where there is some uncertainty
about the latent state’s evolution, we now develop analogous identification results for stochastic
mechanisms that induce conditional distributions over latent states.The systems evolves as
Xt — g(Zt), Zt+1 - mt(Zt, Ut),	⑺
where each Ut is noise with each component sampled independently from standard uniform distribu-
tion Uniform[0,1], Zi 〜PZ, mt : ZX [0,1]d → Z, and the decoder g : Z → X isa diffeomorphism
(i.e. a smooth bijection with an invertible Jacobian, see definition A.1 (Kass & Vos, 2011)).
In this section, we will focus on the case where the true mechanism is unknown; the case where
mechanism is known is a special case with no imitator, i.e., mt = mt for all t. The goal is to search for
an encoder g-1, which is a diffeomorphism, that generates Xt+i = g ◦ mt(g-1(χt), Ut), where Ut is
a random vector with each component from Uniform[0, 1]. In the deterministic case, we had required
that any candidate encoder, g-1, was point-wise consistent with the pairs of observations (xt+ι,χt).
Here, encoders are only required to match the observed conditional distributions. An encoder that
is consistent with the observed data can be used to generate Xt+1 such that the distribution of
Xt+1 |Xt = xt matches the distribution ofXt+1|Xt = xt for all xt ∈ X,
Xt+1∣Xt = Xt = Xt+1∣Xt = Xt
g ◦ mt(g-1(χt),Ut) = g ◦ m t(g-1(χt),Ut)
(8)
7
Published as a conference paper at ICLR 2022
Now, define the set of all candidate decoders g (with corresponding encoders g-1) that solve the
above equation 8 as Gisd. We can extend the notion of equivariance and imitation to the stochastic
case by replacing equality in value by equality in distribution, and show that, as before, these are the
only sources of non-identifiability. In the special case where mt is known, Es, defined below, only
contains maps that result from equivariance. We continue to use the M* - set of mechanisms that are
used at least once in the evolution of the system and M - hypothesis class of all the mechanisms.
Definition 3. Equivariance and imitators in distribution w.r.t M. Define a set of functions E s that
satisfy commutativity w.r.t the set of distributions M in the following sense. The set E s comprises all
the diffeomorphisms a that satisfy thefollowing condition: a ∈ Es if and only iffor each m ∈ M*,
∃ m0 ∈ M such that for all z ∈ Z, a ◦ m(z, U) =d m0 (a(z), U), where each component of U is
sampled independently from Uniform[0, 1] .
Define the set of maps that identify true g up to ES as Geq = {g∣g = g ◦ a-1,a ∈ Es}
Theorem 4.	If the data generation process follows equation 7, then the set of encoders that solve
stochastic observation identity equation 8 identify the true g-1 up to the equivariances and imitators
in distribution w.r.t M, Es(g-1 ~es g-1).
To prove the above theorem, we follow a similar strategy as in Theorem 1 and establish Gisd = Gesq .
The proof of the above is provided in Section A.7 in the Appendix. Theorem 4 is consistent with
the results that we developed for the deterministic case (Theorem 3), but because the mechanism is
allowed to be stochastic, it allows us to generalize known results based on distributional assumptions;
we give an example of this in the next section.
4	A mechanism-based perspective on existing results
Our primary motivation for understanding how mechanistic knowledge can aid identification, is to
develop methods that do not require independence assumptions over the latent variables. However,
independence assumptions are not incompatible with the mechanism-based perspective: they simply
define a particular kind of mechanism, which then implies identification up to the mechanism’s
associated equivariances and imitators. We demonstrate this by re-deriving recent identification
results from Klindt et al. (2020) using the mechanisms implied by their respective distributional
assumptions. We begin by describing the data generation process used by Klindt et al. (2020) as a
stochastic mechanism of the form of equation 7. For each t ∈ {1,…，T}
Zt+1 = Zt + Vt,	Vt = f (Ut),	Ut 〜UnifOrm[0,1]d	(9)
where f is an inverse CDF such that each component of Vt ∈ Rd is sampled from the generalized
Laplace distribution centred at zero with norm parameter α = 2, Zi 〜PZ. Next, we want to use
Theorem 4 to derive all the solutions to the observation identity in equation 8.
Theorem 5. If the data generation process follows equation 9, and PZ and the parameters defining f
are known (same assumption as in Klindt et al. (2020)), then the solution to the stochastic observation
identity equation 8 leads to identifying the true representations up to permutation, sign-flip and offset.
The proof is given in Section A.8 in the Appendix. The above theorem generalizes Theorem 1 from
Klindt et al. (2020) as we do not require α < 2 and rather we work with α 6= 2. Alternatively,
analogous results to those in Klindt et al. (2020); Hyvarinen & Morioka (2017) can be derived
in settings where we do not know the distribution of Vt but instead assume that we observe Xt
often enough that the difference between Zt and Zt+1 is small. In particular, suppose that the data
generation process follows equation 9 except each component of Vt is an i.i.d. draw from a non-
Gaussian with zero mean and |Vt | < δ. Then as δ → 0 the true latent is identified up to permutation,
sign-flip and offset. See Section A.9 for details.
5	Related works
Non-linear independent component analysis (ICA) is a highly unidentified problem; several works
(Hyvarinen & Pajunen, 1999; Locatello et al., 2019) have shown that it is impossible to invert the
data generation process without placing restrictions on the data and models. In recent years, a lot of
progress has been made on the problem of non-linear identification. Hyvarinen & Morioka (2016;
8
Published as a conference paper at ICLR 2022
2017) provide the first proofs for identification in non-linear ICA. Hyvarinen & Morioka (2016)
showed that if the latent variables are mutually independent, with each component evolving in time
following a non-stationary time series without temporal dependence, then non-linear identification
is possible. Hyvarinen & Morioka (2017) showed that non-linear identification is also possible
if the latent variables are mutually independent, with each component evolving in time following
a stationary time series with temporal dependence. In HalVa & Hyvarinen (2020), the authors
combine non-stationarity (Hyvarinen & Morioka, 2016) and temporal dependency (Hyvarinen &
Morioka, 2017) and extend identifiability guarantees in somewhat more general settings. Khemakhem
et al. (2020a); Hyvarinen et al. (2019); Khemakhem et al. (2020b) further generalized the previous
results; in these works instead of using time the authors require observation of auxiliary information.
Klindt et al. (2020) departs from other non-linear ICA works as it explicitly exploits the sparsity
in the transitions of the latent variables (further details on Klindt et al. (2020) can be found in the
previous section.). In Zimmermann et al. (2021), the authors show that minimizing contrastive
losses commonly used in self-supervised learning can also guarantee identification provided the
data (contrastive pairs) follow a specific choice of data generation process (e.g., contrastive pair is
generated from a von Mises-Fisher distribution). In another line of work Locatello et al. (2020); Shu
et al. (2019), the authors study the role of weak supervision in assisting disetanglement. In a recent
work, Gresele et al. (2021), propose to add new form of constraints to non-linear ICA. The constraint
is based on the observation that the decoder g that gives rise to the image x is composed of simpler
functions that are mutually algorithmically independent; the authors exploit this inductive bias on the
structure of g to invert the data generation process. In another recent work von Kugelgen et al. (2021),
the authors consider models where the latent variables are divided into two blocks - content and style
block, where the latents are not necessarily independent. Using data augmentation on the style latents,
the authors show block-wise identification for content block. In Section 4 we argued the existing
distributional assumptions could be interpreted as particular choices of stochastic mechanisms; for
more details, see Table 1 in the Appendix, where we describe the form of the respective mechanisms.
In short, prior work has focused on identification guarantees under assumptions on the dependence
between the different random variables, which is in sharp contrast to our approach, which focuses on
identification under varying degrees of the knowledge of mechanisms that govern latent dynamics.
Equivariance There is significant recent interest in leveraging equivariance assumptions to design
more efficient deep network architectures; for a recent survey, see Bronstein et al. (2021). The
general recipe of this line of work is to design functions (deep network architectures) that enforce
equivariances. Our setting inverts this recipe, in that we have some known function, m(z), and
we are interested in finding all of its equivariances. The relationship between distributions and
their equivariances has a long history in the statistics literature (see e.g. Eaton, 1989, and the
references therein). Our characterization of stochastic equivariances was inspired by the functional
representations given in Bloem-Reddy & Teh (2020). Finally, the importance of group symmetries in
representation learning was discussed in Higgins et al. (2018). Higgins et al. focus on the relationship
between the symmetries of the environment and a model’s representations, whereas we focus on
how symmetries in the environment’s transition function constrain the representation. The two
perspectives are complementary, and in future work we hope to unify them.
6	Discussion and future work
This paper has presented the first systematic study of how mechanisms governing the dynamics of
high-level variables can be used to identify these variables from low-level observations, and up to what
equivariances, which depend on the mechanisms. We show that this perspective is both powerful—
yielding significant constraints in the space of possible representation—and that it generalizes many
known approaches. Moving forward, a natural direction is to build methods founded on this theory. We
describe two natural losses based on the identity g◦ m◦ IT(Xt) = xt+i. i) Loss using observations:
We minimize next observation prediction error ming∈H %三h 沅∈m
Pt E [kXt+1 - g ◦ m ◦ h(χt)k2]
where H, M is the hypothesis class of functions for decoder and mechanisms respectively. ii) Loss us-
ing latents: Alternatively, one could re-write the identity as g_1 ◦ xt+i = m◦ g-1 ◦ Xt and use square
error or ContraStiVe loss given as ming∈H,m∈m Pt E[ - log (二区十])t)⅞⅝1+¾⅞XTMmg(Xt))]
where τ represents other time instances, i.e., τ 6= t + 1. The positive pair in the contrastive loss is
formed by the adjacent time instances and the negative pair is formed by non-adjacent time instances.
In Appendix Section A.12 we share our initial results for the contrastive approach applied to 3d
identification dataset (Zimmermann et al., 2021) .
9
Published as a conference paper at ICLR 2022
References
Tim Austin. Exchangeable random measures. Annales de l'Institut Henri Poincare, Probabilites
et Statistiques, 51(3), Aug 2015. ISSN 0246-0203. doi: 10.1214/13-aihp584. URL http:
//dx.doi.org/10.1214/13-AIHP584.
Yoshua Bengio and Yann LeCun. Scaling learning algorithms towards AI. In Large Scale Kernel
Machines. MIT Press, 2007.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828,
2013.
Benjamin Bloem-Reddy and Yee Whye Teh. Probabilistic symmetries and invariant neural networks.
J. Mach. Learn. Res., 21:90-1, 2020.
Michael M. Bronstein, Joan Bruna, Taco Cohen, and Petar Velickovic. Geometric deep learning:
Grids, groups, graphs, geodesics, and gauges. CoRR, abs/2104.13478, 2021. URL https:
//arxiv.org/abs/2104.13478.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,
Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever, and Dario Amodei. Language models are few-shot learners. CoRR, abs/2005.14165,
2020. URL https://arxiv.org/abs/2005.14165.
Pierre Comon. Independent component analysis, a new concept? Signal processing, 36(3):287-314,
1994.
Morris H DeGroot. Probability and statistics. Pearson, 2012.
Morris L Eaton. Group invariance applications in statistics. In Regional conference series in
Probability and Statistics, pp. i-133. JSTOR, 1989.
Luigi Gresele, Paul K Rubenstein, AraSh Mehrjou, Francesco Locatello, and Bernhard Scholkopf.
The incomplete rosetta stone problem: Identifiability results for multi-view nonlinear ica. In
Uncertainty in Artificial Intelligence, pp. 217-227. PMLR, 2020.
Luigi Gresele, Julius von Kugelgen, Vincent Stimper, Bernhard Scholkopf, and Michel Besserve.
Independent mechanism analysis, a new concept? arXiv preprint arXiv:2106.05200, 2021.
Hermanni Halva and Aapo Hyvarinen. Hidden markov nonlinear ica: Unsupervised learning from
nonstationary time series. In Conference on Uncertainty in Artificial Intelligence, pp. 939-948.
PMLR, 2020.
Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende,
and Alexander Lerchner. Towards a definition of disentangled representations. arXiv preprint
arXiv:1812.02230, 2018.
Aapo Hyvarinen and Hiroshi Morioka. Unsupervised feature extraction by time-contrastive learning
and nonlinear ica. Advances in Neural Information Processing Systems, 29:3765-3773, 2016.
Aapo Hyvarinen and Hiroshi Morioka. Nonlinear ica of temporally dependent stationary sources. In
Artificial Intelligence and Statistics, pp. 460-469. PMLR, 2017.
Aapo Hyvarinen and Petteri Pajunen. Nonlinear independent component analysis: Existence and
uniqueness results. Neural networks, 12(3):429-439, 1999.
Aapo Hyvarinen, Hiroaki Sasaki, and Richard Turner. Nonlinear ica using auxiliary variables and
generalized contrastive learning. In The 22nd International Conference on Artificial Intelligence
and Statistics, pp. 859-868. PMLR, 2019.
10
Published as a conference paper at ICLR 2022
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 4401-4410, 2019.
Robert E Kass and Paul W Vos. Geometrical foundations of asymptotic inference, volume 908. John
Wiley & Sons, 2011.
Ilyes Khemakhem, Diederik Kingma, Ricardo Monti, and Aapo Hyvarinen. Variational autoencoders
and nonlinear ica: A unifying framework. In International Conference on Artificial Intelligence
and Statistics, pp. 2207-2217. PMLR, 2020a.
Ilyes Khemakhem, Ricardo Pio Monti, Diederik P Kingma, and Aapo Hyvarinen. Ice-beem:
Identifiable conditional energy-based deep models based on nonlinear ica. arXiv preprint
arXiv:2002.11537, 2020b.
David Klindt, Lukas Schott, Yash Sharma, Ivan Ustyuzhaninov, Wieland Brendel, Matthias Bethge,
and Dylan Paiton. Towards nonlinear disentanglement in natural data with temporal sparse coding.
arXiv preprint arXiv:2007.10930, 2020.
Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Scholkopf,
and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentan-
gled representations. In International Conference on Machine Learning, pp. 4114-4124. PMLR,
2019.
Francesco Locatello, Ben Poole, Gunnar Ratsch, Bernhard Scholkopf, Olivier Bachem, and Michael
Tschannen. Weakly-supervised disentanglement without compromises. In International Conference
on Machine Learning, pp. 6348-6359. PMLR, 2020.
Boris Mityagin. The zero set of a real analytic function. arXiv preprint arXiv:1512.07276, 2015.
Isaac Newton. Newton’s Principia : the Mathematical Principles of Natural Philosophy. 1687.
Bogdan Nica. The mazur-ulam theorem. arXiv preprint arXiv:1306.2380, 2013.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In International Conference on Machine Learning, pp.
8748-8763. PMLR, 2021.
Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with
vq-vae-2. In Advances in neural information processing systems, pp. 14866-14876, 2019.
Bernhard Scholkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner,
Anirudh Goyal, and Yoshua Bengio. Toward causal representation learning. Proceedings of the
IEEE, 109(5):612-634, 2021. doi: 10.1109/JPROC.2021.3058954.
Rui Shu, Yining Chen, Abhishek Kumar, Stefano Ermon, and Ben Poole. Weakly supervised
disentanglement with guarantees. arXiv preprint arXiv:1910.09772, 2019.
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. In International
Conference on Learning Representations, 2020.
Julius von Kugelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Scholkopf, Michel
Besserve, and Francesco Locatello. Self-supervised learning with data augmentations provably
isolates content from style. arXiv preprint arXiv:2106.04619, 2021.
Roland S Zimmermann, Yash Sharma, Steffen Schneider, Matthias Bethge, and Wieland Brendel.
Contrastive learning inverts the data generating process. arXiv preprint arXiv:2102.08850, 2021.
11
Published as a conference paper at ICLR 2022
Approach
Time contrastive (Hyvarinen & Morioka,
2016)
Permutation contrastive (Hyvarinen &
Morioka, 2017)
Slow VAE (Klindt et al., 2020)
Conditional VAE (Khemakhem et al., 2020a;
Hyvarinen et al., 2019)
Independently modulated component analysis
(Khemakhem et al., 2020b)
Contrastive learning (Zimmermann et al.,
2021)
Multi-view ICA (Gresele et al., 2020)
Assumptions
Zt — Ut, each Ut is independent and non-stationary
Each Ztj+ι — m(Ztj, Ut), stationary and not quasi-Gaussian
Zt+1 — Zt + f (Ut), each f (Ut) is independent and gener-
alized Laplace
Z — m(O, U), all components of Z are independent condi-
tional on O
Z — m(O, U), m has a special structures allowing to relax
conditional independence
Z — m(Z,U), m is such that Z 〜conditional von MiSeS-
Fisher
ZI — m(Z0, U), X0 — g0(ZO), X1 — gl(Z1), all compo-
nents of Z0 and U are independent
Table 1: Table comparing different works and the assumptions made for identifiability.
A	Appendix
A.1 Proof of Theorem 1
Proof. First we show that Gid ⊆ Geq .
Consider a g ∈ Gid. For each X ∈ X
g ◦ m ◦ g-1 (x) = g ◦ m ◦ g-1(x)
g-1 ◦	(g ◦ m ◦	g-1(x))	= g-1 ◦ (] ◦ m	◦ g-1(x))	(Compose g-1 with both sides)
g-1 ◦	(g ◦ m ◦	g-1(x))	= m ◦ OT(X)	(g-1 ◦ g(z) = Z)
Since g is invertible we can substitute x in the above equation with x = g(z) and obtain for each
z∈Z
gO-1 ◦ g	◦ m ◦ g-1 ◦ g(z)	= m ◦	gO-1 ◦	g(z)
gO-1 ◦g ◦ m(z)	= m ◦	gO-1 ◦	g(z)	(10)
Define gO-1 ◦ g = a. Observe that a is invertible and from equation 10 we gather that a ∈ E. Also,
since gO = g ◦ a-1, we can conclude that gO ∈ Geq, which proves the first part of the claim. For the
second part, we show that Geq ⊆ Gid .
Consider a gO ∈ Geq = {gO | gO = g ◦ a-1, a ∈ E}. By definition, can express gO = g ◦ a-1. For each
X ∈ X we write
g ◦ m ◦ g-1 (x) = (g ◦ a-1) ◦ m ◦ (g ◦ a-1) 1(x)
= g ◦ a-1 ◦ m ◦ a ◦ g-1 (X)
= g ◦ a-1 ◦ a ◦ m ◦ g-1(X)
= g ◦ m ◦ g-1(X)
Observe that gg is both a bijection and satisfies the identity in equation 4. Therefore, gg ∈ Gid . This
proves the second part of the claim. Therefore, Gid = Geq.	□
A.2 LINEAR MECHANISM M AND LINEAR G.
The focus of the main text is on nonlinear identification, but in this section we show how the analysis
for general functions g and m applies to the special case when g and m are linear and affine maps
respectively. This special case is useful both as a concrete example, and as a stepping stone to
12
Published as a conference paper at ICLR 2022
Theorem 2 (nonlinear g with affine m) which will reuse some of the same proof strategies. We write
the data generation process as follows.
zt+1 = Mzt + b,
xt = Gzt,
(11)
where M ∈ Rd×d is a diagonalizable matrix describing the mechanism, b ∈ Rd is the offset parameter,
G ∈ Rd×d is an invertible matrix determining how the data transforms from latent space to the
observable space. We write the eigendecomposition of M as follows M = SΛS-1, where S is
the matrix of eigenvectors and Λ is a diagonal matrix of the eigenvalues. On the same lines as the
equation 2, we can obtain an identity between xt and xt+1 as follows.
zt+1 = Mzt + b
G-1xt+1 = MG-1xt +b	(12)
xt+1 = GMG-1xt+Gb
If the learner knows M and b, it tries to solve for an invertible G that satisfies
xt+1 = GMG-1xt + Gb	(13)
Theorem 6. If the data generation process follows equation 11 and the eigenvalues of the mechanism
M are all distinct and each component of the vector S-1b is non-zero, then the only solution to the
identity in equation 13 is the true mechanism G.
Proof. We take the difference of the equations 12 and 13 to get the following condition. For each
xt ∈ Rd
. -1 ~ ~ -1 . . ~ .
(GMGT — GMGT)Xt + (G - G)b = 0	(14)
Because, equations 12 and 13 hold for all xt, we can substitute xt = 0 in the above to get
.	~ .
(G - G)b = 0	(15)
We plug the above condition in equation 15 back into equation 14 to get the following condition. For
each xt ∈ Rd
. -1 ~ ~ -1 .
(GMGT — G MGT)Xt = 0	(16)
If equation 16 holds for d linearly independent vectors xt ∈ Rd , then we can conclude that,
GMGT — GMG T = 0
GT (GMGT — GMGT) = 0
MGT — GTGMG T = 0
(MGT — GTG MGT)G = 0
MGTG = GTG M	(17)
Let A = G-1G. We substitute A and the eigendeComPosition of M (M = SΛS-1, where Λ =
diag(λ1,…，λd)) in equation 17 to get
M = AM A-1
SΛS-1 = ASΛS-1A-1
Λ = (S-1AS)Λ(S-1A-1S)
Λ = CΛC-1	(where C = S-1AS)
ΛC = CΛ	(18)
13
Published as a conference paper at ICLR 2022
We further simplify equation 18 and compare each element of the matrix ΛC and CΛ to get the
following condition. For all i,j ∈ {1,…，d}
[ΛC]ij = Cijλi, [CΛ]ij = Cijλj
Cij(λi - λj) = 0	(19)
Consider the above equation 19 for i 6= j to get
Cij (λi - λj ) = 0 =⇒ Cij = 0 (we use the assumption that λi 6= λj )	(20)
From the above, it follows that C is a diagonal matrix. We obtain an expression for A in terms of C
matrix below.
S-1AS = C
A= SCS-1	(21)
From equation 15 we get that
(G - G)b = 0 g=⇒a (I - A)b = 0 Eq=(21) (I - SCST)b = 0
ι=⇒}	STb - CSTb =⇒ (C - I)S-1b = 0	(22)
Left multiply S-1
Since C is a diagonal matrix, we can simplify the above condition further to get (cii - 1)(S-1b)i = 0.
Since (S-1b)i = 0 =⇒ Cii = 1, We obtain C = I =⇒ G-1G = I =⇒ G = G.
□
A.3 Proof of Corollary 1
Proof. This proof follows essentially the same strategy as the proof of Theorem 1 with a set of
mechanisms, M* instead of a single mechanism m; We include it for completeness. First We show
that Gid UG：q.
Consider a g ∈ Gid. For each X ∈ X and for each m ∈ M：
g ◦ m ◦ g-1(x) = g ◦ m ◦ g-1(x),	(23)
and by folloWing the same steps as the proof of Theorem 1, We can shoW that,
(g-1 ◦ g) ◦ m(z) = m ◦ (g-1 ◦ g)(z)	(24)
As before, define g-1 ◦ g = a. Observe that a is invertible and from equation 24 we gather that
a ∈ E：. Also, since g = g ◦ a-1, we can conclude that g ∈ G：q, which proves the first part of the
claim. In the second part, we need to show that Ge：q ⊆ Gi：d .
Consider a g ∈ G* = {g | g = g ◦ a-1, a ∈ E：}. By definition, can express g = g ◦ a-1. For each
x ∈ X and for each m ∈ M： we write,
gg ◦ m ◦ gg-1 (x) = g ◦ a-1 ◦ m ◦ a ◦ g-1 (x) =
g ◦ a-1 ◦ a ◦ m g-1(x)	(since a commutes with each m ∈ M) =	(25)
g ◦ m ◦ g-1 (x) for all m
Observe that gg is both a bijection and satisfies the observation identity in equation 4. Therefore,
gg ∈ Gi：d. This proves the second part of the claim. Therefore, Gi：d = Ge：q.
□
A.4 Proof of Theorem 2
Proof. We showed in Theorem 1 that the only source of non-identifiability are the bijections, a, in
the set E ; our task here is to explicitly find all of these bijections for affine mechanisms. If a ∈ E,
then it satisfies a ◦ m = m ◦ a. We replace m with the affine mechanism to obtain the following
condition. For each z ∈ Rd
14
Published as a conference paper at ICLR 2022
a(Mz + b) = Ma(z) + b	(26)
Next, recall that a : Rd → Rd ; we take gradient of the function in the LHS and RHS of the above
equation 26 separately w.r.t z. Consider the jth component of a(Mz + b) denoted as aj (Mz + b).
We first take the gradient of aj(Mz + b) w.r.t z
▽zaj (Mz + b) = (dz) Nyaj (y),	(27)
where y = Mz + b, Nyaj (y) is the gradient of aj w.r.t y and % denotes the Jacobian of y w.r.t z.
We simplify the above further to get
Nzaj(Mz + b) = MTNyaj (y) = MTNyaj (Mz + b)	(28)
We can write the above for each component of a as follows.
[Vzαι(Mζ + b), ∙∙∙ , Vzad(Mz + b)] = [MTNya∖(Mz + b), ∙∙∙ , MT▽ yad(Mz + b)]
=MT[Vyα1(Mz + b),…，Vyad(Mz + b)] = MTJT(Mz + b),	(^
where J(Mz + b) is the Jacobian of a computed at Mz + b. Next, we take the gradient of the jth
component of the RHS in equation 26 and let mj denote the jth column of M,
mj1
mj2
VzimTa(Z) + bj] = EmjiVza%(z) = [Vaι(z),…，Vad(z)].
i.
mjd
(30)
We can write the above for each component in the RHS of equation 26 as follows
[Vz [mTa(z)+bι],…，Vz [mTa(z)+bdf∣ = [Vaι(z), ∙∙∙ , Vad(z)]
mil,…，mdi
m12, ∙∙∙ , md2
..
..
..
.mid,…,mdd一
JT(z)MT
(31)
We equate the gradient of LHS and RHS in equation 26 using the expressions derived in equation 29
and equation 31 to obtain
a(Mz +b) = Ma(z) +b =⇒ MTJT(Mz +b) - JT(z)MT = 0	(32)
We write the same expression at another offset b0 6= b below
a(Mz+b0) = Ma(z) +b =⇒ MTJT(Mz +b0) - JT(z)MT = 0	(33)
Taking the difference of equation 32 and equation 33 we get MTJT(Mz + b) = MTJT(Mz + b0 ).
Since M is invertible, we get J(Mz + b) = J(Mz + b0). Consider row j of this identity. For each
z ∈ Rd
ΓV2aj- (θi)η
0	0	V22aj (θ2)	0
Vaj(Mz+b)-Vaj∙(Mz+b )=0 =⇒ Vaj(z)-Vaj-(z+b —b) = 0 =⇒	.	(b-b )=0
.
.
V2daj(θd)
(34)
where V2aj is the Hessian of aj and V2kaj(θk) corresponds to the kth row of the Hessian matrix.
Note that in the above expansion there is a different θk for each row (mean value theorem applied
15
Published as a conference paper at ICLR 2022
to each component of Vaj yields a different point θk on the Iinejoinining Z and Z + b 一 b0. From
Assumption 2 and based on the fact that M is invertible, it follows that Vkaj(θk)(b 一 b ) = 0 over a
measurable set. Since aj is analytic V2kaj(z)(b 一 b0) is also analytic. Therefore, from (Mityagin,
2015), we can conclude that V2kaj(z)(b 一 b0) = 0 for all z. We can make the same argument for
each component k and conclude that V2 aj(z)(b 一 b0) = 0. From Assumption 1, it follows that
V2aj(z)(bj 一 b1) = 0 for all j ∈ {2,…，d + 1} and since the set {b2 一 b1,…，bd+1 - b1} is
linearly independent V2 aj(z) = 0 for all z. This implies a(z) = Az + p. Plug a(z) = Az + p into
a(Mz + b) = Ma(z) + b to get
A(Mz + b) +p = MA(z + p) + b =⇒ (AM 一 MA)z+ (A 一 I)b+ (I 一 M)p = 0	(35)
We write the same expression for offset b0
A(Mz + b0) + p = MA(z + p) + b0 =⇒ (AM 一 MA)z + (A 一 I)b0 + (I 一 M)p = 0 (36)
We take the difference of equation 35 and equation 36 to get
(A 一 I)(b 一 b ) = 0	(37)
Substitute z = 0 in equation 35 to get
(A 一 I)b + (I 一 M)p = 0	(38)
Substitute the above condition in equation 38 into equation 35 to get the following. For each z
(AM - MA)Z = 0	(39)
We can now leverage the proofs from the linear setting in Section A.2. The above equation 39 is
the same as equation 17 and the equation 37 is the same as equation 22, with b replaced by b 一 b0 .
Following the same analysis as before, we get that latent variables are exactly identified; we show
all the steps below for completeness. By choosing d linearly independent Z and substituting in
equation 39 we get the following,
MA = AM
M = AMA-1,
SΛS-1 = ASΛS-1A-1,
Λ= S-1ASΛS-1A-1S,
Λ = CΛC-1,
(40)
ΛC = CΛ,
where C = S-1AS, M = SAS-1, A = diag(λι,…，λd). We further simplify equation 18
and compare each element of the matrix ΛC and CΛ to get the following condition. For all
i,j ∈ {1,…，d}
[AC]ij = Cijλi, [CA]ij = Cij λj
Cij (λi 一 λj ) = 0
Consider the above equation 41 for i 6= j to get
(41)
Cij (λi 一 λj) = 0 =⇒ Cij = 0 (we use the assumption that λi 6= λj)	(42)
From the above, it follows that C is a diagonal matrix. We obtain an expression for A in terms of C
matrix below.
S-1AS = C
A = SCS-1
(43)
From equation 37 we get that
0	Q	0
(G - G)(b - b ) = 0 g=⇒a (I - A)(b - b ) = 0 =⇒ (I - SCST)b = 0
ι=⇒}	ST(b 一 b0) 一 CS-1(b 一 b0) =⇒ (C 一 I)S-1(b 一 bo) = 0	(44)
Left multiply S-1
Since C is a diagonal matrix, we can simplify the above condition further to get (cii 一 1)(S-1(b 一
b ))i = 0. Since (S-1e一b ))i = 0 =⇒ Cii = 1, We obtain C = I =⇒ A = I =⇒ a(z) = z+p.
This proves that the latents are identified up to an offset.	口
16
Published as a conference paper at ICLR 2022
(45)
(46)
(47)
A.5 Proof of Theorem 3
Proof. We first show that Gid ⊆ Geq. Consider a g ∈ Gid. We rewrite equation 6 below. For all X ∈ X
g ◦ mt ◦ g-1(χ) = g ◦ mt ◦ IT(X)
(g-1 ◦ g) ◦ (mt ◦ g-1(x)) = mt ◦ g-1(x)
Since g is bijective, we can write X = g(z) to get
(g-1 ◦ g) ◦ mt(z) = mt ◦ g-1 ◦ g(z)
Since the above equality holds for all z ∈ Z we can conclude that
(g-1 ◦ g) ◦ mt = mt ◦ (g-1 ◦ g),
a ◦ mt = mt ◦ a,
The above conclusion in equation 47 holds for all mt ∈ M*. Therefore, a in equation 47 and
{mt}T=ι (where mt ∈ M) together satisfy the condition that for all m ∈ M*, a ◦ m = m ◦ a, where
m ∈ M. We can rewrite g-1 ◦ g = a as g = g ◦ a-1. From this it follows that g ∈ Geq. This proves
the first part of the theorem.
Now let us consider the second part of the theorem. Consider a g ∈ Geq. We can write g = g ◦ a 1,
where a ∈ E . At time t, some mechanism mt ∈ M* is used to transform the latents. Since a ∈ E,
select the mechanism mt ∈ M for which a ◦ mt = mt ◦ a and as a consequence
g ◦ mt ◦ g-1 = g ◦ a-1 ◦ mt ◦ a ◦ g-1 = g ◦ mt ◦ g-1	(48)
In the first equality above, we use a ◦ mt = mt ◦ a and in the second equality we use the definition
of g. We can repeat the above exercise for all t and corresponding mt using the same a. Therefore, g
is in Gid. This shows the second part of the theorem, i.e., Geq ⊆ Gid.
□
A.6 LEVERAGING THEOREM 3 WHEN MECHANISM IS LINEAR AND g IS NON-LINEAR
We write the data generation process as follows. For each t ∈ {1, ∙∙∙ ,d +1}
zt+1 = Mtzt + bt ,
Xt = g(zt),
(49)
Let us construct the set Eg corresponding to the above setting. For each z ∈ Rd,
a(Mz +b) = M0a(z) +gb =⇒ MTJT(Mz + b) - JT(z)M0,T = 0,
a(Mz +b0) = M0a(z) +gb0 =⇒ MTJT(Mz +b0) - JT(z)M0,T = 0,	(50)
where (M, b) and (M, b0) are the true mechanisms and (M0, b0) and (M0,gb0) are the imtitating
mechanisms chosen by the learner, J is the Jacobian of a. Note here the learner only exploits the
knowledge that b changes to gb, which is why it keeps M0 fixed and only changes the offset. We take
the difference of the RHS in the above two equations to get
MTJT(Mz + b) = MTJT(Mz + b0)	(51)
Since M is invertible we get J(Mz + b) - J(Mz + b ) = 0 for all z. We can follow the same
justification as was used in equation 33 to conclude that J(z) is constant and a is thus an affine map.
We substitue the affine map a(z) = Az + p back into equation 50 to get the following. For all z
AMz+Abj +p= M0Az+b0,j +M0p
(52)
Substitute z = 0 to get b0,j = Abj +p - M0p. Substitute this condition back into the above equation,
we get AM = M0A =⇒ M0 = AMA-1.
17
Published as a conference paper at ICLR 2022
A.7 Proof of Theorem 4
Before stating the proof of Theorem 4, we state two existing results that we use.
Result 1. (Change of variables formula (DeGroot, 2012)) Given a continuous random variable
X ∈ Rd with pdf pX and its transformation Y = f(X), where f : Rd → Rd is a diffeomorphism,8
then pY (f (x))|det(Jf (x))| = pX (x), where Jf is the Jacobian of f computed at x.
Lemma 1. If X and Y are two continuous random variables that take values in Rd that are equal in
distribution, i.e., X =d Y. If f : Rd → Rd is a diffeomorphism, then f(X) =d f(Y ).
Proof. Since X and Y are equal in distribution, they have the same pdfs, i.e. pX (x) = pY (x) for all
x ∈ Rd . We can use the change of variables formula in Resut 1 above to get the following. Let W =
f(X), pX(f-1(w))|det(Jf-1(w))| = pW (w) and let V = f(Y),pY(f-1(v))|det(Jf-1(v))| =
pV (v). Comparing the two expressions when w = v we get pW (w) = pV (w). This proves the
result.	口
We stated Result 1 and Lemma 1 for continuous random variables. When the random variables are
discrete, Lemma 1 holds for any function f .
Lemma 2. (Kass & Vos, 2011) If f : Z → Z and g : Z → Z are diffeomorphisms, then f ◦ g is a
diffeomorphism.
Proof. We first show that Gisd ⊆ Gesq .
From the observation identity in equation 8 We get that g, {rh力乙 satisfy the following for all
t ∈{1,…，T}
g ◦ mt(g-1(xt),Ut) = g ◦ mt(g-1(xt),Ut)
(3—1 ◦ g) ◦ mt(g-1 (Xt),Ut) = mt(g-1(xt),Ut)
(53)
In the second step in the above equation, we transformed the random variables in the first step using
the same transform g3-1. g3-1 is a diffeomorphism; we compose both sides of the first step LHS and
RHS with g3-1. We use Lemma 1 to get from the first step to the second step in the above equation
equation 53. In the above equation xt is a fixed value and the only source of randomness is from
Ut in LHS and Ut in the RHS. We substitute xt = g(zt) to further simplify the above expression in
equation 53
(g—1 ◦ g) ◦ mt(g-1 ◦ g(zt), Ut) = mt(g-1 ◦ g(zt), Ut)
(g—1 ◦ g) ◦ mt (zt, Ut) = mt (g—1 ◦ g(zt), Ut)
Substitute a = g3-1 ◦ g in the above to get the following
a ◦ mt(zt, Ut) = mt(a(zt),Ut)
a ◦ mt(zt,Ut) = mt(a(zt),Ut)
(54)
(55)
From Lemma 2 it follows that a in the above is a diffeomorphism. Since we assume that ∪tT=1{mt} =
M* it follows that a in equation 47 and {mt}T=1 (where mt ∈ M) together satisfy the condition for
membership in Es. Since g3 = g ◦ a-1 we obtain that g3 ∈ Gesq .
We now show that Gesq ⊆ Gisd. Consider a g3 ∈ Gesq. We use g3 = g ◦ a-1, where a ∈ Es to simplify the
following random variable
8http://math.mit.edu/~larsh∕teaching∕F2007∕handouts∕Changeofvariables.
pdf
18
Published as a conference paper at ICLR 2022
g ◦	mt(g-1(xt), Ut)	=	(g ◦ a-1	◦	a)	◦ mt(g-1(xt),	Ut)	= g ◦ a ◦ mt(g-1(xt),	Ut)	(56)
Since a ∈ Es we have
a ◦ mt(g-1(t), Ut) = mt(α ◦ g-1(xt), Ut)
From Lemma 2 it follows that g is a diffeomorphism. From Lemma 1 it follows that
g ◦ a ◦ mt(g-1 (Xt), Ut) = g ◦ mt(α ◦ g-1(xt), Ut)= g ◦ mt(g-1(xt), Ut)	(57)
Combining equation 56 and equation 57 and using the fact that Ut = Ut we get
g ◦ mt(g-1(xt), Ut) = g ◦ mt(g-1(xt), Ut) = g ◦ mt(g-1(xt),Ut)	(58)
From the definition of Es it follows with the same choice of a the condition continues to hold for all
mt ∈ M*. Therefore, g ∈ G∖ This proves the second part of the theorem.
□
A.8 Proof of Theorem 5
Proof. In Theorem 4, we showed that all the solutions to the observation identity in equation 8 can
be characterized in terms of the equivariances in distribution defined by the set Es . Let us analyze the
set Es for the class of mechanisms considered in Klindt et al. (2020). Consider a a ∈ Es . For each
z ∈ Rd
α(z + V) = a(z) + V,	(59)
Define Y = a(z) + V. Since V = V We write the probability density function (pdf) of Y as
fY(y) = fv(y - a(Z))	(60)
Define Y = a(z + V). a : Rd → Rd is a diffeomorphism. We use the change of variables result
(Result 1) to write the pdfY as follows. For each y ∈ Rd
1
fY (y)
det J a-1(y)
fv (a-1(y) - z),
(61)
where J(a-1(y)) is the Jacobian ofa computed at a-1(y), and det is the determinant.
We substitute equation 60 and equation 61 in the equivariance condition in equation 59 to obtain the
following. For each y ∈ Rd
Y = Y
fV(y - a(z))
fv (a-1(y) - Z)
∣det(J (a-1(y)))∣
(62)
fv (a(W)- a(Z))= IfVt(J (Wz；，
where W = a-1(y). In the above we equated the conditionals for each Z, we now equate the
marginals.
g。(Z + V) = g ◦ (Z + V)
a ◦ (Z + V) = Z + V
(63)
19
Published as a conference paper at ICLR 2022
We follow Klindt et al. (2020) and assume Z = Z and V = V. Therefore, Z + V = Z + V. We use
this condition to restate equation 63 as
a ◦ (Z + V)== Z + V 0 a(W) == W,	(64)
where W = Z + V. We translate equation 64 into the condition on the pdfs as follows. For each
w ∈ Rd
fW (w)|det(J (w))| = fW (w) =⇒ |det(J (w))| = 1	(65)
Substituting the above equation equation 65 into equation 62 we get
fV (a(w) - a(z))
fv (W - Z)
Idet(J (W))I
=⇒ fV (a(w) - a(z)) = fV(w - z)
(66)
ka(W) - a(z)kα = kW - zkα,
where in the last condition in the above expression we exploit the fact that fV is a generalized
Laplacian distribution. From Mazur-Ulam theorem Nica (2013) it follows that a is affine. We now
write a as a matrix A with offset vector q and simplify the condition in equation 59.
For each z∈ Rd we have
A(Z + V) + q == Az + V + q
=⇒ AV == V
(67)
=E [AVV tAt] = E[V V t] = AAT = I
Since A is a square matrix and AAT it follows that ATA = I. Therefore, A is an orthonormal matrix.
Observe that all the elements of V are independent. Since AV = V it follows that all the elements
of AV are also independent. Define AV = Q. Observe that A is an orthonormal matrix that is
multiplied with a vector V with all independent elements (each of which is non-Gaussian as α 6= 2)
and outputs a vector that has all independent components. From Theorem 11 in Comon (1994) we
get that A is a composition of permutation and scaling. Since A is also orthonormal, each term in the
diagonal scaling matrix can only be 1 or -1. Therefore, A = ΠΛ, where Π is a permutation matrix
and Λ is a diagonal matrix with +1, -1 elements. Finally, a(Z) = ΠΛZ + q.
□
A.9 Alternative identification result for small transitions
In this section, we analyze time series models similar to one in Hyvarinen & Morioka (2017) under
the condition that the transitions are small in magnitude to arrive at permutation and scaling based
identification. Let us analyze the set Es for this class of mechanisms. We assume that the learner
knows that the mechanism is additive, and that the noise components are all independent. In the
analysis below we consider bijections that are analytic (each component of the bijection is an analytic
function). Consider an a ∈ Es
a(z + V) = a(z) + V.	(68)
We write the first-order approximation of the above identity below
a(z) + J(Z)V = a(z) + V
J(Z)V = V
(69)
d
where V 6= V. Note that the set of solutions a to equation 68 and equation 69 become equal in
the limit of δ → 0, where δ is the bound on each component of IV I. We analyze the solution to
equation 69 below.
E[(J(Z)V)(J(Z)V)T] = J(Z)E[V V T]J(Z)T = σ2J(Z)J(Z)T
σ2E[V V t] = σ2I
σ2J(Z)J(Z)T = σ2 I =⇒ J(Z)J(Z)T = I =⇒ J(Z)TJ(Z) = I
(70)
20
Published as a conference paper at ICLR 2022
Since J(Z)V = V and each component of V is independent, We can deduce that all the components
of J(z)V are independent as well. From Theorem 11 in Comon (1994), we can deduce that J(z) is
composed of permutation times a diagonal matrix. Since the matrix is orthonormal, each scaling
component can only be ±1. We can apply this same analysis at another point Z in the neighborhood of
z and continue to find that the jacobian matrix is a permutation times diagonal matrix (that describes
sign flips). Note that the permutation matrix times scaling used to express the Jacobian cannot change
between the points Z and Z (if it does change then that violates the Jacobian,s continuity). Since the
Jacobian is equal to a fixed permutation times a fixed scaling matrix over a neighborhood, We can
extend this to the entire space (here we use the fact that the a is analytic and Mityagin (2015)). As a
result, a is of the form ΠΛZ + q, where Π is a permutation matrix, Λ is a diagonal matrix.
A.10 Analyzing auxiliary information models
We now discuss how our machinery can be used in models when auxiliary information is available
(Khemakhem et al. (2020a)) to arrive at permutation and scaling based identification. We define
the data generation process compatible with Khemakhem et al. (2020a). Suppose the latent Z is
generated from a mechanism m : O × [0, 1]d that takes as input some observed auxiliary information
O and uniform independent noise vector U ∈ [0, 1]d:
Z J m(O,U),
X J g(Z),
(71)
where g : Z → X is a bijection. Suppose the learner knows the mechanism. The learner selects g and
outputs X = g ◦ m(o, U), where U is a random vector with each component sampled independently
from Uniform[0, 1]. The learner’s goal is to match
X |O = o = X |O = o
g ◦ m(o, U) = g ◦ m(o, U).
(72)
for all possible observations o ∈ O. Define the set of solutions to equation equation 72 as Giod .
Consider bijection a s.t. the following identity holds for all o ∈ O
a ◦ m(o, U) = m(o, U)	(73)
Define the set of all bijections a that satisfy the condition in equation equation 73 as Eeoq . The set Eeoq
consists of intersection of measure preserving transformations of Z|O = o. Define the set of g that
are identifiable up to Eeq as Geq = {g | g = g ◦ a-1, a ∈ Eeq}.
Theorem 7. If the data is generated as described in equation 71, then the set of solutions to the
identity in equation 72 identify true g up to intersection of all the measure preserving maps in Eeoq
(g-1 〜Eoq g-1)
Proof. Consider a g ∈ Gol.
g ◦ m(o, U) = g ◦ m(o, U)
g-1 ◦ g ◦ m(o,U) = m(o, U)	(74)
a ◦ m(o, U) = m(o, U)
From the above it follows that g ∈ Goq. Consider a g ∈ GOq.
g ◦ m(o, U) = g ◦ a-1 ◦ a ◦ m(o, U) = g ◦ m(o, U)	(75)
From the above it follows that g ∈ Go∣. This completes the proof.	□
Let us consider additive mechanisms of the form m(o, U) = m(o) + U, where U has independent
components. Suppose the learner knows that the mechanism is additive and the noise has independent
components. The learner solves the following identity.
g ◦ (m(o) + U) = g ◦ (m (o) + U)
g-1 ◦ g(m(o) + U) = m (o) + U	(76)
0
a(m(o) + U) = m (o) + U
21
Published as a conference paper at ICLR 2022
Suppose that the absolute value of each component of U is really small and bounded by δ. We can
use the first-order Taylor expansion and obtain
a(m(o)) + J(m(o))U == m (o) + U,	(77)
where J is the Jacobian of a. Suppose the noise has zero mean. Take the expectation w.r.t. U and
0
U on the two sides respectively to get a(m(o)) = m (o). Substitute this back into the equation We
d
get that J(m(o))U = U. We can now follow the analysis that We carried out after equation 69 and
conclude that a is equal to permutation times a scaling matrix along with some offset.
,	~	~	,	二	，/	，，工
A.11	Analyzing E WHEN M = M*
In this section, we analyze imitators when we know the set of mechanisms that are deployed, we
do not know which mechanism is used when. If a ∈ E and M = M . From the definition of a, it
follows that for each m ∈M*, ∃ m ∈M* such that a ◦ m = m ◦ a. We claim that two distinct
m ∈ M* cannot share the same m (imitator). Suppose there was a common m imitating m and m.
a ◦ m = m0 ◦ a
0	(78)
a ◦ m = m ◦ a
We take the difference of the above two equations to get
a ◦ m = a ◦ m	(79)
Since a is a bijection, We can conclude that m = m, which is a contradiction of the fact that m and
m are distinct.
This claim implies that for a given a there is an injective map from M* to M*. If the set M* is
finite, then from Pigeonhole principle it follows that this injective map is a bijection.
Let us index the mechanism M* = {m1, •一，mn}. We call the bijection map π : M* → M*
Consider the element i. We claim ∃ l ∈ {1,…，n} πl(i) = i. We write the chain starting from i as
i → π(i) → ∏2(i),…,πk(i). Since the chain (π(i) → ∏2(i),…,πk(i)) has n steps there have to
be at least two elements that are equal. Suppose p > q and πp(i) = πq(i)
πp(i) = πq(i) =⇒ πp-1(i) = πq-1(i) =⇒ …..πp-q(i) = i	(80)
In the above at each step we use the fact that π is a bijection and that shows the claim that πl (i) = i.
We now use this observation to carry out the following simplification
mi = a-1 ◦ mπ(i) ◦ a
mπ(i) = a-1 ◦ mπ2 (i) ◦ a
(81)
.
.
.
mπk (i) = a-1 ◦ mi ◦ a
Substituting the second equation mπ(i) into first, and then the third mπ2 (i) and so on we get
mi = a-k ◦ mi ◦ ak	(82)
Therefore, for each mi , ∃ k such that ak is its equivariance.
To summarize, if a ∈ E and M = M* , where M is a finite set, then for each mechanism m ∈ M,
∃k ∈ {1,…，|M|} such that ak is its equivariance.
A.12 Translating identity into loss functions and preliminary experiments
We restate the two loss choices based on our identity g ◦ m ◦ g-1(χt) = xt+i below.
22
Published as a conference paper at ICLR 2022
•	Loss based on observations. The identity above immediately implies an autoencoder-style
algorithm where one minimizes a reconstruction loss of the form
min
-一 ~ _ .
g∈H,h∈H,m ∈M
X E kXt+1
t
-g ◦ m ◦ h(Xt)k2]
(83)
where H is the hypothesis class of functions for g and M is the hypothesis class of mecha-
nisms.
•	Loss based on latents. Alternatively, one could re-write the observation identity as g-1 ◦
xt+1 = m ◦ g-1 ◦ xt and use a contrastive loss as follows
• v^^ g( g 1 g____________g(χt+ι)Tmg(χt)_____________λ^∣	/Qd∖
min:EI- g vg(Xt+ι)Tτmg(Xt) + ∑τg(Xτ)T»mg(Xt))1	( )
where τ represents other time instances, i.e., τ 6= t + 1. The positive pair in the contrastive
loss is formed by the adjacent time instances and the negative pair is formed by non-adjacent
time instances. Identifying which of these two losses works better in practice is an important
empirical question. Note how in both the losses above, we have explicitly not enforced
invertibility for the learned g.
We present our initial experiments on 3dIdent dataset from [Zimmerman et al. 2021], using the
contrastive loss described above. With contrastive pairs generated by a (fixed) random orthogonal
matrix U applied to the latents, we obtain the following values for linear disentanglement score (R2
of the predictions of the true representation using a linear model). We report median scores over 10
seeds.
•	Standard contrastive learning. Linear disentanglement score of 0.29
•	Contrastive leveraging with exact mechanism knowledge. Leveraging the true U as the
mechanism in the contrastive loss achieves a median score of 0.76.
•	Contrastive leveraging with some knowledge of mechanism If we use a random orthog-
onal matrix U 6= U, that achieves a median score of 0.64. The random matrix can be
interpreted as sampling a random m from the hypothesis class M; it seems likely that better
results are possible by optimizing over M.
23