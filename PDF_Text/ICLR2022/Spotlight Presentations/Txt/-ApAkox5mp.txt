Published as a conference paper at ICLR 2022
SHINE: SHaring the INverse Estimate from the
forward pass for bi-level optimization and im-
PLICIT MODELS
Zaccharie Ramzi
CEA (Neurospin & Cosmostat)
Inria (Parietal)
Gif-sur-Yvette, France
zaccharie.ramzi@inria.fr
Florian Mannel
University of Graz
Graz, Austria
Shaojie Bai
Carnegie Mellon University
Pittsburgh, USA
Jean-Luc Starck
AIM, CEA, CNRS
Gif-sur-Yvette, France
Philippe Ciuciu
CEA (Neurospin), Inria (Parietal)
Gif-sur-Yvette, France
Thomas Moreau
Inria (Parietal)
Gif-sur-Yvette, France
Ab stract
In recent years, implicit deep learning has emerged as a method to increase the
effective depth of deep neural networks. While their training is memory-efficient,
they are still significantly slower to train than their explicit counterparts. In Deep
Equilibrium Models (DEQs), the training is performed as a bi-level problem, and
its computational complexity is partially driven by the iterative inversion of a
huge Jacobian matrix. In this paper, we propose a novel strategy to tackle this
computational bottleneck from which many bi-level problems suffer. The main idea
is to use the quasi-Newton matrices from the forward pass to efficiently approximate
the inverse Jacobian matrix in the direction needed for the gradient computation.
We provide a theorem that motivates using our method with the original forward
algorithms. In addition, by modifying these forward algorithms, we further provide
theoretical guarantees that our method asymptotically estimates the true implicit
gradient. We empirically study this approach and the recent Jacobian-Free
method in different settings, ranging from hyperparameter optimization to large
Multiscale DEQs (MDEQs) applied to CIFAR and ImageNet. Both methods reduce
significantly the computational cost of the backward pass. While SHINE has a
clear advantage on hyperparameter optimization problems, both methods attain
similar computational performances for larger scale problems such as MDEQs at
the cost of a limited performance drop compared to the original models.
1 Introduction
Implicit deep learning models such as Neural ODEs (Chen et al., 2018), OptNets (Amos and
Zico Kolter, 2017) or Deep Equilibrium models (DEQs) (Bai et al., 2019; 2020) have recently
emerged as a way to train deep models with infinite effective depth without the associated memory
cost. Indeed, while it has been observed that the performance of deep learning models increases with
their depth (Telgarsky, 2016), an increase in depth also translates into an increase in the memory
footprint required for training, which is hardware-constrained. While other works such as invertible
neural networks (Gomez et al., 2017; Sander et al., 2021) or gradient checkpointing (Chen et al.,
2016) also tackle this issue, implicit models bear an O(1) memory cost and with constraints on the
architecture that are usually not detrimental to the performance (Bai et al., 2019). These models have
been successfully applied to large-scale tasks such as language modeling (Bai et al., 2019), computer
vision (Bai et al., 2020) and inverse problems (Gilton et al., 2021; Heaton et al., 2021).
In general, the formulation of DEQs can be cast as a bi-level problem of the following form:
arg min L(z?) subject to gθ (z?) = 0.
θ
(1)
We will refer to the root finding problem gθ (z?) = 0 as the inner problem, and call its resolution
the forward pass. On the other hand, we will refer to arg minθ L(z?) as the outer problem, and
1
Published as a conference paper at ICLR 2022
call the computation of the gradient of L(z?) w.r.t. θ the backward pass. The core idea for DEQs
is that their output z? is expressed as a fixed point of a parametric function fθ from Rd to Rd, i.e.,
gθ(z?) = z? -fθ(z?) = 0.1 This model is said to have infinitely many weight-tied layers as z? can be
obtained by successively applying the layer fθ infinitely many times, provided fθ is contractive. In
practice, DEQs’ forward pass is not computed by applying successively the function but usually relies
on quasi-Newton (qN) algorithms, such as Broyden’s method (Broyden, 1965), which approximates
efficiently the Jacobian matrix d∂gθ and its inverse for root-finding.
To compute DEQs’ gradient efficiently and avoid high memory cost, one does not rely on back-
propagation but uses the implicit function theorem (Krantz and Parks, 2013) which gives an analytical
?
expression of the Jacobian of z? With respect to θ, 喻∙ While this method is memory efficient, it
requires the computation of matrix-vector products involving the inverse of a large Jacobian matrix,
Which is computationally demanding. To make this computation tractable, one needs to rely on an
iterative algorithm based on vector-Jacobian products, Which renders the training particularly sloW, as
highlighted by the original authors (Bai et al., 2020) (see also the break doWn of the computational
effort in Section E.4).
Moreover, the formulation (1) alloWs us to also consider general bi-level problems such as hyper-
parameter optimization under the same frameWork. For instance, hyperparameter optimization for
Logistic Regression (LR) can be Written as
min Lval(z*) subject to	z* = minrθ(Z)，Ltrain(z) + θ∣∣z∣∣2,	(2)
Where Ltrain and Lval correspond to the training and validation losses from the LR problem (Pedregosa,
2016). Here, z corresponds to the Weights of the LR model While θ is the regularisation parameter. As
the training loss is smooth and convex, the inner problem can be written as in (1) with gθ = Nzr to
fit (1). Similarly to DEQ, the inner problem is often solved using qN methods, Which approximate the
inverse of the Hessian in the direction of the steps, such as the LBFGS algorithm (Liu and Nocedal,
1989), and the gradient computation suffers from the same drawback as it is also obtained using the
implicit function theorem. Lorraine et al. (2020) review the different hypergradient approximations
for bi-level optimization and evaluate them on multiple tasks.
With the increasing popularity of DEQs and the ubiquity of bi-level problems in machine learning, a
core question is how to reduce the computational cost of the resolution of (1). This would make these
methods more accessible for practitioners and reduce the associated energy cost. In this work, we
propose to exploit the estimates of the (inverse of the) Jacobian/Hessian produced by qN methods in
the hypergradient computation. Moreover, we also propose extra updates of the qN matrices which
maintain the approximation property in the direction of the steps, and ensure that the inverse Jacobian
is approximated in an additional direction. In effect, we can compute the gradient using the inverse of
the final qN matrix instead of an iterative algorithm to invert the Jacobian in the gradient’s direction,
while stressing that the inverse of a qN matrix, and thus the multiplication with it, can be computed
very efficiently.
We emphasize that the goal of this paper is neither to improve the algorithms used to compute z? , nor
is it to demonstrate how to perform the inversion of a matrix in a certain direction as a stand-alone
task. Rather, we are describing an approach that combines the resolution of the inner problem with
the computation of the hypergradient to accelerate the overall process. Our work is the first to
consider modifying the inner problem resolution in order to account for the bi-level structure of the
optimization The idea to use additional updates of the qN matrices to ensure additional approximation
properties is not new, and it is also known that a full matrix inversion can be accomplished in this
way. For instance, Gower and RiCht疝ik (2017) used sketching to design appropriate extra secant
conditions in order to obtain guarantees of uniform convergence towards the inverse of the Jacobian.
The novelty in our work is that we integrate additional update to yield the inverse in a specific
direction, which is substantially cheaper than computing the inverse. A concurrent work by Fung
et al. (2021) is also concerned with the acceleration of DEQs’ training, where the inverse Jacobian
is approximated with the identity. Under strong contractivity and conditioning assumptions, it is
proven that the resulting approximation is a descent direction and the authors show good empirical
performances for small scale problems.
The contributions of our paper are the following:
1Here, we do not explicitly write the dependence of fθ on the input x of the DEQ, usually referred to as the
injection.
2
Published as a conference paper at ICLR 2022
•	We introduce a new method to greatly accelerate the backward pass of DEQs (and generally,
the differentiation of bi-level problems) using qN matrices that are available as a by-product
of the forward computations. We call this method SHINE (SHaring the INverse Estimate).
•	We enhance this method by incorporating knowledge from the outer problem into the inner
problem resolution. This allows us to provide strong theoretical guarantees for this approach
in various settings.
•	We additionally showcase its use in hyperparameter optimization. Here, we demonstrate
that it provides a gain in computation time compared to state-of-the-art methods.
•	We test it for DEQs for the classification task on two datasets, CIFAR and ImageNet.
Here, we show that it decreases the training time while remaining competitive in terms of
performance.
•	We extend the empirical evaluation of the Jacobian-Free method to large scale multiscale
DEQs and show that it performs well in this setting. We also show that it is not suitable for
more general bi-level problems.
•	We propose and evaluate a natural refinement strategy for approximate Jacobian inversion
methods (both SHINE and Jacobian-Free) that allows a trade-off between computational
cost and performances.
2	Hypergradient Optimization with Approximate Jacobian Inverse
2.1	SHINE: Hypergradient Descent with Approximate Jacobian Inverse
Hypergradient Optimization Hypergradient
optimization is a first-order method used to solve
(1). We recall that in the case of smooth convex
optimization,鳖 is the Hessian of the inner op-
timization problem, while for deep equilibrium
models, it is the Jacobian of the root equation.
In the rest of this paper, with a slight abuse of
notation, we will refer to both these matrices
with Jgθ whenever the results can be applied to
both contexts. To enable Hypergradient Opti-
mization, i.e. gradient descent on L with respect
to θ, Bai et al. (2019, Theorem 1) show the
following theorem, which is based on implicit
differentiation (Krantz and Parks, 2013):
Theorem 1 (Hypergradient (Bai et al., 2019;
Krantz and Parks, 2013)). Let θ ∈ Rp be a
set of parameters, let L : Rd → R be a loss
function and gθ : Rd → Rd be a root-defining
function. Let z? ∈ Rd such that gθ (z?) = 0 and
Jgθ (z?) is invertible, then the gradient of the
loss L wrt. θ, called Hypergradient, is given by
VZL(Z*YJgθ(z*)-1 d∂gθ∣z? . (3)
In practice, we use an algorithm to approximate
Algorithm 1: qN method to solve gθ (z?) = 0
Result: Root z?, qN matrix B
b = true if using Broyden’s method,
b= false if using BFGS
n = 0, z0 = 0, B0 = I
while not converged do
pn = -Bn- gθ (zn), zn+1 = zn + αnpn
// αn can be 1 or determined
by line-search
yn = gθ (zn+1) - gθ (zn)
sn = zn+1 - zn
if b then
I Bn+1 = arg min IlX - BnIlF
I	X： Xsn=yn
else
Bn+1 =
arg min	IX-1 - Bn-1 I
X: X=XT ∧ Xsn=yn
// The norm used in BFGS
is a weighted Frobenius
norm
end
n - n + 1
end
z? = zn, B = Bn
z? , and Theorem 1 gives a plug-in formula for
the backward pass. Note that this formula is independent of the algorithm chosen to compute z? .
Moreover, as opposed to explicit networks, we do not need to store intermediate activations, resulting
in the aforementioned training time memory gain for DEQs. Once z? has been obtained, one of
the major bottlenecks in the computation of the Hypergradient is the inversion of Jgθ(z?) in the
directions 鬻 ∣ or V%L(z*).
∂ L I
∂θ L
Quasi-Newton methods In practice, the forward pass is often carried out with qN methods. For
instance, in the case of bi-level optimization for Logistic Regression, Pedregosa (2016) used L-
BFGS (Liu and Nocedal, 1989), while for Deep Equilibrium Models, Bai et al. (2019) used Broyden’s
3
Published as a conference paper at ICLR 2022
method (Broyden, 1965), later adapted to the multi-scale case in a limited-memory version (Bai et al.,
2020).
These quasi-Newton methods were first inspired by Newton’s method, which finds the root of gθ via
the recurrent Jacobian-based updates zn+1 = zn - Jgθ (zn)-1gθ(zn). Specifically, they replace the
Jacobian Jgθ (zn ) by an approximation Bn that is based on available values of the iterates zn and gθ
rather than its derivative. These Bn , called qN matrices, are defined recursively via an optimization
problem with constraints called secant conditions. Solving this problem leads to expressing Bn as a
rank-one or rank-two update of Bn-1, so that Bn is the sum of the initial guess B0 (in our settings,
the identity) and n low-rank matrices (less than n in limited memory settings). This low rank structure
allows efficient multiplication by Bn and Bn-1. We now explain how the use of qN methods as inner
solver can be exploited to resolve this computational bottleneck.
SHINE Roughly speaking, our proposition is to use B-1 = limn→∞ Bn-1 as a replacement for
Jgθ (z?)-1 in (3), i.e. to share the inverse estimate between the forward and the backward passes.
This gives the approximate Hypergradient
Pθ = VzL(z?)BT WL	(4)
In practice We will consider the nonasymptotical direction p(n = VzL(Zn)Bn' d∂gθ I	. Thanks
zn
to the Sherman-Morrison formula (Sherman and Morrison, 1950), the inversion of Bn can be done
very efficiently (using scalar products) compared to the iterative methods needed to invert the true
Jacobian Jgθ(z?). In turn, this significantly reduces the computational cost of the Hypergradient
computation.
Relationship to the Jacobian-Free method Because B0 = I in our setting, we may regard B as
an identity matrix perturbed by a few rank-one updates. In the directions that are used for updates, B
is going to be different from the identity, and hopefully closer to the true Jacobian in those directions.
However, in all orthogonal directions we fall exactly into the setting of the Jacobian-Free method
introduced by FUng et al. (2021). In that work, Jge (z?)-1 is approximated by I, and the authors
highlight that this is equivalent to using a preconditioner on the gradient. Under strong assumptions
on gθ they show that this preconditioned gradient is still a descent direction.
Transition to the exact Jacobian Inverse. The approximate gradient p(θn) can also be used as the
initialization of an iterative algorithm for inverting Jgθ(z?) in the direction VzL(z?). With a good
initialization, faster convergence can be expected. Moreover, if the iterative algorithm is also a qN
method, which is the case in practice in the DEQ implementation, we can use the qN matrix B from
the forward pass to initialize the qN matrix of this algorithm. We refer to this strategy as the refine
strategy. Because the refine strategy is essentially a smart initialization scheme, it recovers all the
theoretical guarantees of the original method (Bai et al., 2019; 2020; Pedregosa, 2016).
2.2	Convergence to the true gradient
To further justify and formalize the idea of SHINE, we show that the direction p(θn) converges to the
Hypergradient ∂L I ∙ We now collect the assumptions that will be used for this purpose.
∂ θ z?
Assumption 1 (Uniform Linear Independence (ULI) (Li et al., 1998)). There exist a positive constant
ρ > 0 and natural numbers n0 ≥ 0 and m ≥ d with the following property: For any n ≥ n0 we
can find indices n ≤ n1 ≤ . . . ≤ nd ≤ n + m such that, for pn defined in Algorithm 1, the smallest
singular value of the d × d matrix
p pn1 pn2	Pnd、
l^kPnj, ¥nj,…，>nj)
is no smaller than ρ.
Assumption 2 (Smoothness and convergence to the fixed point). (i) Pn∞=0 kzn - z? k < ∞ for some
z? with gθ(z?) = 0; (ii) gθ is C1, Jgθ is Lipschitz continuous near z?, and Jgθ(z?) is invertible; (iii)
Vz L is continuous, and ∀θ,翳 is continuous.
Remark. The Assumption 2 (i) implies limn→∞ zn = z?. The existence of the Jacobian and its
inverse are assumptions that are already made in the regular DEQ setting just to train the model.
4
Published as a conference paper at ICLR 2022
Theorem 2 (Convergence of SHINE to the Hypergradient using ULI). Let us denote p(θn), the SHINE
direction for iterate n in Algorithm 1 with b = true. Under Assumptions 1 and 2, for a given
parameter θ, (zn) converges q-superlinearly to z? and
n→∞ 度=dθlz?.
Proof. From More and Trangenstein (1976, Theorem 5.7) we obtain that limn→∞ Bn = Jgθ (z?).
We can then conclude using the continuity of the inversion operator on the space of invertible matrices
and of the right and left matrix vector multiplications. A complete proof is given in Section B.1. □
Theorem 2 establishes convergence of the SHINE direction to the true Hypergradient, but relies on
Assumption 1 (ULI). While ULI is often used to prove convergence results for qN matrices, e.g.
in (Conn et al., 1991; Li et al., 1998; Nocedal and Wright, 2006), it is a strong assumption whose
satisfaction in practice is debatable, cf., e.g., (Fayez Khalfan et al., 1993). For Broyden’s method,
ULI is violated in all numerical experiments in (Mannel, 2020; 2021a;b), and those works also prove
that ULI is necessarily violated in certain settings (but the setting of this work is not covered). In the
following we therefore derive results that do not involve ULI.
2.3	Outer Problem Awarenes s
The ULI assumption guarantees convergence of Bn-1 to Jgθ (z?)-1. However, (3) only requires the
multiplication of Jg§ (z?)-1 with 鬻 |z? from the right and NzL(z?) from the left.
BFGS with OPA In order to strengthen Theorem 2, let us consider the setting of bi-level op-
timization with a single regularizing hyperparameter θ. There, the partial derivative 嘴lz? is a
d-dimensional vector and it is possible to compute its approximation 鬻 ∣zn at a reasonable cost.
We propose to incorporate additional updates of the quasi-Newton matrix Bn into Algorithm 1
that improve the approximation quality of B-1 in the direction 翳 ∣zn (thus asymptotically in the
direction dgθ |z?). Given a current iterate pair (Zn, Bn), these additional updates only change Bn, but
not zn . We will demonstrate that a suitable update direction en ∈ Rd is given by
en =tnB-1 ~∂θl ,	⑸
where (tn ) ⊂ [0, ∞) satisfies Pn tn < ∞. This update direction will be used to create an extra
secant condition X-1(gθ(zn + en) - gθ(zn)) = en for the additional update of Bn. Since this extra
update is based on the outer problem, we refer to this technique as Outer-Problem Awareness (OPA).
The complete pseudo code of the OPA method in the LBFGS algorithm (Liu and Nocedal, 1989) is
given in Appendix A.
We now prove that if extra updates are applied at a fixed frequency, then fast (q-superlinear) conver-
gence of (zn) to z? is retained, while convergence of the SHINE direction to the true Hypergradient
is also ensured. To show this, we use the following assumption.
Assumption 3 (Assumptions for BFGS). Let gθ (z) = Nzrθ (z) for some C2 function rθ : Rd → R.
Consider Algorithm 1 with b = false. We assume some regularity on r and that an appropriate
line search is used. An extended version of this assumption is given in Section B.2 (Assumption 5).
Theorem 3 (Convergence of SHINE to the Hypergradient for BFGS with OPA). Let us consider p(θn),
the SHINE direction for iterate n in Algorithm 1 that is enriched by extra updates in the direction en
defined in (5). Under Assumptions 2 (ii-iii) and 3, for a given parameter θ, we have the following:
Algorithm 1, for any symmetric and positive definite matrix B0, generates a sequence (zn) that
converges q-superlinearly to z?, and there holds
lim Pθn) = dLl .	(6)
n→∞	∂θ z?
Proof. It follows from known results that the extra updates do not destroy the q-superlinear conver-
gence of (zn). The proof of (6) relies firstly on the fact that by continuity of the derivative of gθ,
we have limn→∞ 翳 ∣zn =翳 |z?. Due to the extra updates we can show convergence of the qN
matrices to the true Hessian in the direction of the extra steps en , from which (6) follows. A full
proof is provided in Section B.2.	□
5
Published as a conference paper at ICLR 2022
Remark. Theorem 3 also holds without line searches (i.e., αn = 1 for all n) and any C2 function rθ
(such that gθ(Z) = Nzrθ(Z)) with locally Lipschitz continuous Hessian if zo is close enough to some
z? with Vzrθ(z?) = 0 and V2ztq(z?) positive definite.
We note that Theorem 3 guarantees fast convergence of the iterates (Zn) and that Z0 does not have to
be close to z? for that guarantee. Also, there is no restriction on B0 other than being symmetric and
positive definite (which is satisfied for our choice B0 = I). Finally, Theorem 3 does not rely on ULI.
From a practical standpoint we thus regard Theorem 3 as a much stronger result than Theorem 2.
Adjoint Broyden with OPA It is not practical to use the partial derivative 嘴 in the DEQ setting
because it is a huge Jacobian that we do not have access to in practice. In order to still leverage the
core idea of OPA, We propose to use extra updates that ensure that B-1 approximates Jge (z?)-1
in the direction Vz L(z?) applied by left-multiplication, as required by (3). An appropriate secant
condition is given by
vnT Bn+1 = vnTJgθ(zn+1),	(7)
Where
vnT = VzL(zn )Bn-1.	(8)
To incorporate the secant condition (7), We use the Adjoint Broyden’s method (Schlenkrich et al.,
2010), a qN method relying on the efficient vector-Jacobian multiplication by Jgθ using auto-
differentiation tools. To prove convergence of the SHINE direction for this method, We need the
folloWing assumption.
Assumption 4 (Uniform boundedness of the inverse qN matrices). The sequence (Bn ) generated by
Algorithm 1 satisfies
sup kBn-1 k < ∞.
n∈N
Remark. Convergence results for quasi-Newton methods usually include showing that Assumption 4
holds, cf. Broyden et al. (1973, Theorem 3.2) for Broyden’s method and the BFGS method, respectively,
Schlenkrich et al. (2010, Theorem 1) for the Adjoint Broyden’s method. It can also be proved that
Assumption 4 holds for globalized variants of these methods, e.g., for the line-search globalizations
of Broyden’s method proposed by Li and Fukushima (2000). We point out that Assumption 1 entails
lim Bn = Jgθ (z?) and thus lim B-1 = Jgθ (z?)-1, so it is clearly stronger than Assumption 4.
Theorem 4 (Convergence of SHINE to the Hypergradient for Adjoint Broyden With OPA). Let us
consider p(θn) , the SHINE direction for iterate n in Algorithm 1 with the Adjoint Broyden secant
condition (7) and extra update in the direction vn defined in (8). Under Assumptions 2 and 4, for a
given parameter θ, we have q-superlinear convergence of (zn ) to z? and
nlim∞ 度=dθL*∙
Proof. The q-superlinear convergence of (zn ) folloWs from Schlenkrich et al. (2010, Theorem 2).
To establish convergence of the SHINE direction, We proceed in three steps. First, it is shoWn that
for VzL(z?) = 0 the claim holds due to continuity and Assumption 4. Then VzL(z?) 6= 0 is
considered and it is proved that the desired convergence holds on the subsequence that corresponds to
the additional updates. Lastly, this result is transferred to the entire sequence by involving the fixed
frequency of the additional updates. The complete proof is provided in Section B.3.	□
Using the Adjoint Broyden’s method comes at a computational cost. Indeed, because We noW rely
on Jgθ, We have to store the activations of gθ(z) (Which has a computational cost in addition to a
memory cost), but also perform the vector-Jacobian product in addition to the function evaluation.
3	Results
We test our method in 3 different setups and compare it to the original iterative inversion and its closest
competitor, the Jacobian-Free method (Fung et al., 2021). We draW the reader’s attention to the fact
that although the Jacobian-Free method (Fung et al., 2021) is used outside the assumptions needed
to have theoretical guarantees2 of descent, it still performs relatively Well in the Deep Equilibrium
setting. The same is true for SHINE: While the ULI assumption is not met (and We are in practice far
from the fixed point convergence), it performs Well in practice.
2See the results on contractivity in Section E.3.
6
Published as a conference paper at ICLR 2022
SHINE (ours)
HOAG
20news
SHINE refine (ours)
..... JaCobian-Free
一一 ■ Grid search
real-sιm
0	10	20	30	40	50	60	0	5	10	15	20	25	30
Time (S)	Time (S)
Figure 1: Bi-level optimization: Convergence of held-out test loss for different hyperparameter opti-
mization methods on the '2-regularized logistic regression problem for the 2 datasets (20news (Lang,
1995) and real-sim (lib)) SHINE achieves the best performances for both problems while the
Jacobian-Free method is much slower, in particular on 20news. Note that the kink for HOAG on
real-sim does not mean it is better as the optimization stops once the validation loss has converged
and not the test one. The typical loss order of magnitude is 102. An extended figure with more
methods is provided in Section E.1.
Implementations. All the bi-level optimization experiments were done using the HOAG code (Pe-
dregosa, 2016)3, which is based on the Python scientific ecosystem (Harris et al., 2020; Pedregosa
et al., 2011; Virtanen et al., 2020). Deep Equilibrium experiments were done using the Py-
Torch (Paszke et al., 2019) code for Multiscale DEQ (Bai et al., 2020)4, which was distributed under
the MIT license. Plots were done using Matplotlib (Hunter, 2007), with Science Plots style (Garrett
and Peng, 2021). DEQ trainings were done in a publicly funded HPC, on nodes with 4 V100 GPUs.
In practice, we never reach convergence of (zn), hence the approximate gradient might be far from
the true gradient. To improve the approximation quality, we now propose a variant of our method.
Fallback in the case of wrong inversion. Empirically, we noticed that using B can sometimes
produce bad approximations, although with very low probability. We propose to detect this with by
monitoring a telltale sign based on the norm of the approximation, as we verified on several examples
that cases with a huge norm compared to the correct inversion also had a very bad correlation with
the correct inversion. In these cases, we can simply fallback onto another inversion method. For the
Deep Equilibrium experiments, when the norm of the inversion using SHINE is 1.3 times above the
norm of the inversion using the Jacobian-Free method (which is available at no extra computational
cost), we use the Jacobian-Free inversion. We refer to this strategy as the fallback strategy.
3.1	BI-LEVEL OPTIMIZATION — HYPERPARAMETER OPTIMIZATION IN LOGISTIC REGRESSION
We first test SHINE in the simple setting ofbi-level optimization for '2-regularized LR, using the code
from Pedregosa (2016) and the same datasets. Convergence on unseen data is illustrated in Figure 1.5
An acceptable level of performance is reached twice faster for the SHINE method compared to any
other competitor. Another finding is that the refine strategy does not provide a definitive improvement
over the vanilla version of SHINE. In order to verify that the performance gain of SHINE is not
simply driven by truncated inversion, we also run HOAG with limited number of inversion iteration
and showed that this degrades its performances (see HOAG limited backward in Section E.1).
We also tested our implementation of OPA on the 20news dataset and present the results in Figure 2. In
order to get a fair comparison, we implemented both SHINE, SHINE-OPA and HOAG using the same
3https://github.com/fabianp/hoag
4https://github.com/locuslab/mdeq
5To facilitate the reader’s understanding of the figures, we plot the empirical suboptimality, but we do remind
them that there is no guarantee of convergence on held-out test data ; the kink present in the case of the real-sim
dataset is an example of that.
7
Published as a conference paper at ICLR 2022
-Ire日Iκoqns SSoa≈3
103
102
101
100
10-1
Direction
• Prescribed K Krylov R Random
1.000
g 0.998
8
∞ 0.996
O
0.994
20	40	60	80	100	120	140	1.00	1.05	1.10
Time (S)
例
Figure 2: Bi-level optimization with OPA: (left) Convergence of different hyperparameter opti-
mization methods on the '2-regularized LR problem for the 2θnews dataset (Lang, 1995) on held-out
test data. SHINE with OPA achieves similar performance as SHINE without OPA but with better con-
vergence guarantees. (right) Evaluation of the inversion quality in direction v using OPA b = Bn-1v
compared to the exact inverse a = Jgθ (z?)-1v for 3 different directions: the prescribed direction,
the Krylov direction and a random direction. The points represent the cosine similarity between a
and b as a function of the ratio of their norm and the closer to (1, 1) the better. The inverse in the
prescribed direction is better than in random direction.
full Python code instead of relying on the original code which relied on the Fortran implementation
of L-BFGS from (Virtanen et al., 2020). While SHINE with OPA does not outperform the vanilla
SHINE, it reaches similar performances, outperforming HOAG, and comes with strong theoretical
grounding. Additional results on hyperparameter optimization for the regularized nonlinear least
squares problem are available in Section E.2.
We also showed on a smaller dataset, the breast cancer dataset (Dua and Graff, 2017), that OPA
indeed ensures a better approximation of the inverse in the prescribed direction. For a given split
of the data, we compared the quality of the approximation of the inversion in three different direc-
tions: a prescribed direction chosen randomly but used for the OPA update, the Krylov direction
d¾gZθ J (Zn - zn-ι) and a random direction not used in the qN algorithm. The results for 100 runs
with zdifferent random seeds are depicted in Figure 2, where we can observe that OPA indeed ensures
a better inversion in the prescribed direction compared to a random direction. We also notice that a
poor direction for the inversion seems correlated with a small magnitude.
3.2	Deep Equilibrium Models
Next, we tested SHINE on the more challenging DEQ setup. Two experiments illustrate the perfor-
mance of SHINE on the image classification task on two datasets. For both datasets, we used the same
model configuration as in the original Multiscale DEQ paper (Bai et al., 2020) and did not fine tune
any hyperparameter. For the different DEQ training methods, models for a given seed share the same
unrolled-pretraining steps. We do not include OPA in the DEQ results because while the gradients
are well correlated with the true ones (see Figure E.3), we observe a sharp initial performance drop
that reduces its performance on Imagenet. We provide partial results in Section E.5.
CIFAR-10. The first dataset is CIFAR-10 (Krizhevsky, 2009) which features 60,000 32×32 images
representing 10 classes. For this dataset, the size of the multi-scale fixed point is d = 50k. We train
the models for five different random seeds.
The results in Figure 3 show that for the vanilla version, SHINE slightly outperforms the Jacobian-
Free method (Fung et al., 2021). Additionally, our results suggest that SHINE (in its vanilla version)
is able to reduce the time taken for the backward pass almost 10-fold compared to the original method
while retaining a competitive performance (on par with Res-Net-18 (He et al., 2016) at 92.9%).
Finally, we do highlight that the Jacobian-Free method (Fung et al., 2021) is able to perform well
outside the scope of its theoretical assumptions, albeit with slightly worse performance than SHINE.
We conjecture that the batched stochastic gradient descent helps accelerated methods by averaging
out the errors made in the approximation.
ImageNet. The second dataset is the ImageNet dataset (Deng et al., 2009) which features 1.2
million images cropped to 224 × 224, representing 1000 classes. This dataset is recognized as a
large-scale computer vision problem and the dimension of the fixed point to find is d = 190k.
8
Published as a conference paper at ICLR 2022
0	100	200	300	400	500	600	700	800
Backward pass wall-clock time [ms]
Figure 3: DEQ: Top-1 accuracy function of backward pass runtime for the different methods
considered to train DEQs, on CIFAR (Krizhevsky, 2009) and ImageNet (Deng et al., 2009). The
original DEQ training method corresponds to the Full backward pass points and the vanilla SHINE
and Jacobian-Free methods correspond to direct use of the inverse approximation without further
refinement. The other points correspond to further refinements of the different methods with different
number of iterations used to invert Jg§ (z*) in the direction of NzL(z?). This highlights the trade-off
between computations and performances driving the refinement choice.
For this challenging task, we noticed that the vanilla version of SHINE was suffering a big drop just
after the transition from unrolled pre-training to actual equilibrium training. To remedy partly this
problem, we introduced the fallback to Jacobian-Free inversion. The results for a single random seed
presented in Figure 3 for the ImageNet dataset are given for SHINE with fallback. The fallback is
barely used : in 1000 batches of size 32, only 2 samples used fallback, a proportion of 6.25 × 10-5 .
Despite the drop suffered at the beginning of the equilibrium training, SHINE in its refined version
is able to perform on par with the Jacobian-Free method (Fung et al., 2021). We also confirm the
importance of choosing the right initialization to perform accelerated backpropagation, by showing
that with a limited iterative inversion, the performance of the original method deteriorates. Finally,
while the drop in performance for the accelerated methods is significant when applied in their vanilla
version, we remind the reader that no fine-tuning was performed on the training hyperparameters,
making those results encouraging (on par with architectures like ResNet-18 (He et al., 2016)).
The key take-away from Figure 3 is that both SHINE and Jacobian-Free approximation methods
allow to accelerate the DEQ’s backward pass at a relatively low accuracy cost.6 Moreover, using the
proposed refined versions of these methods, the performance drop can be traded-off for acceleration.
4	Conclusion and Discussion
We introduced SHINE, a method that leverages the qN matrices from the forward pass to obtain an
approximation of the gradient of the loss function, thereby reducing the time needed to compute this
gradient. We showed that this method can be used on a wide range of applications going from bi-level
optimization to small and large scale computer vision tasks. We found that both SHINE and the
Jacobian-Free method reduce the required amount of time for the backward pass of implicit models,
potentially lowering the barriers for training implicit models.
As those methods still suffer from a small performance drop, there is room for further improvement.
In particular, a potential experimentation avenue would be to understand how to balance the efforts of
the Adjoint Broyden method in order to come closer to guaranteeing the asymptotical correctness
of the approximate inversion. On the theoretical side, this may involve the rate of convergence of
the approximated gradient. It also seems desirable to develop a version of Theorem 4 in which
convergence of (zn) to z? is not an assumption but rather follows from the assumptions, as achieved
in Theorem 3. We have no doubt that the contraction assumption used for the Jacobian-Free method
would allow to prove such a result, but expect that a significantly weaker assumption will suffice.
6More on the overall computational effort can be found in Table E.2
9
Published as a conference paper at ICLR 2022
Reproducibility S tatement
We provide with the submission of this paper the full code necessary to reproduce the figures and
the other quantitative results of the paper, from the training, to the evaluation and the actual figure
drawing. We made sure to use seeds and verified that the seeding was indeed allowing reproducible
results. We also provide time estimates for the reproduction of the figures. We made sure to provide
the full proofs for our theorems in the supplementary material of this manuscript. The core concepts
used in the proofs, and their sketches are also laid out in the main text.
References
Libsvm datasets.	https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/
datasets/. Accessed: 2021-05-06.
B. Amos and J. Zico Kolter. OptNet: Differentiable Optimization as a Layer in Neural Networks. In
ICML, 2017.
S. Bai, J. Z. Kolter, and V. Koltun. Deep equilibrium models. In NeurIPS, 2019.
S. Bai, V. Koltun, and J. Z. Kolter. Multiscale deep equilibrium models. In NeurIPS, 2020.
A. S. Berahas, M. Jahani, P. Richtarik, and M. Takac. Quasi-NeWton Methods for Machine Learning:
Forget the Past, Just Sample. Optimization Methods and Software, 2021.
J. Bergstra and Y. Bengio. Random Search for Hyper-Parameter Optimization Yoshua Bengio.
Journal of Machine Learning Research ,13:281-305, 2012.
C.	G. Broyden. A Class of Methods for Solving Nonlinear Simultaneous Equations. Mathematics of
Computation, 19(92):577-593, 1965.
C. G. Broyden, J. E. jun. Dennis, and J. J. More. On the local and superlinear convergence of
quasi-NeWton methods. Journal of the Institute of Mathematics and its Applications, 12:223-245,
1973.
R. H. Byrd and J. Nocedal. A tool for the analysis of quasi-NeWton methods With application to
unconstrained minimization. SIAM Journal on Numerical Analysis, 26(3):727-739, 1989.
R. H. Byrd, R. B. Schnabel, and G. A. Shultz. Parallel quasi-NeWton methods for unconstrained
optimization. Mathematical Programming. Series A. Series B, 42(2 (B)):273-306, 1988.
R. T. Chen, Y. Rubanova, J. Bettencourt, and D. Duvenaud. Neural Ordinary differential equations.
In NeurIPS, 2018.
T. Chen, B. Xu, C. Zhang, and C. Guestrin. Training Deep Nets With Sublinear Memory Cost.
Technical report, 2016.
A. R. Conn, N. I. Gould, and P. L. Toint. Convergence of quasi-NeWton matrices generated by the
symmetric rank one update. Mathematical Programming, 50(1-3):177-195, 1991. ISSN 00255610.
doi: 10.1007/BF01594934.
J. Deng, W. Dong, R. Socher, L.-J. Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical
image database. In CVPR. Institute of Electrical and Electronics Engineers (IEEE), 2009.
D.	Dua and C. Graff. UCI machine learning repository, 2017. URL http://archive.ics.
uci.edu/ml.
H. Fayez Khalfan, R. H. Byrd, and R. B. Schnabel. A theoretical and experimental study of the
symmetric rank-one update. SIAM Journal on Optimization, 3(1):1-24, 1993.
S. W. Fung, H. Heaton, Q. Li, D. Mckenzie, S. Osher, and W. Yin. Fixed Point NetWorks: Implicit
Depth Models With Jacobian-Free Backprop. Technical report, 2021.
J.	D. Garrett and H.-H. Peng. garrettj403/SciencePlots, Feb. 2021. URL http://doi.org/10.
5281/zenodo.4106649.
10
Published as a conference paper at ICLR 2022
D. Gilton, G. Ongie, and R. Willett. Deep Equilibrium Architectures for Inverse Problems in Imaging.
Technical report, 2021.
A. N. Gomez, M. Ren, R. Urtasun, and R. B. Grosse. The Reversible Residual Network: Backpropa-
gation Without Storing Activations. In NIPS, 2017.
R. M. GoWer and P Richtdrik. Randomized quasi-Newton updates are linearly convergent matrix
inversion algorithms. SIAM Journal on Matrix Analysis andApplications, 38(4):1380-1409, 2017.
C. R. Harris, K. J. Millman, S. J. van der Walt, R. Gommers, P. Virtanen, D. Cournapeau, E. Wieser,
J. Taylor, S. Berg, N. J. Smith, R. Kern, M. Picus, S. Hoyer, M. H. van Kerkwijk, M. Brett,
A. Haldane, J. F. del Rio, M. Wiebe, P Peterson, P GCrard-Marchant, K. Sheppard, T. Reddy,
W. Weckesser, H. Abbasi, C. Gohlke, and T. E. Oliphant. Array programming with NumPy. Nature,
585(7825):357-362, 9 2020.
K.	He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.
H.	Heaton, S. W. Fung, A. Gibali, and W. Yin. Feasibility-based Fixed Point Networks. Technical
report, 2021.
J. D. Hunter. Matplotlib: A 2D graphics environment. Computing in Science and Engineering, 9(3):
90-95, 2007.
D. P. Kingma and J. L. Ba. Adam: A method for stochastic optimization. In ICLR. International
Conference on Learning Representations, ICLR, 12 2015.
S. G. Krantz and H. R. Parks. The Implicit Function Theorem: History, Theory, and Applications.
Springer New York, 1 2013.
A. Krizhevsky. Learning Multiple Layers of Features from Tiny Images. Technical report, 2009.
K. Lang. NewsWeeder: Learning to Filter Netnews. In ICML, pages 331-339. Elsevier, 1995.
D. Li and M. Fukushima. A derivative-free line search and global convergence of Broyden-like
method for nonlinear equations. Optimization Methods & Software, 13(3):181-201, 2000.
D. Li, J. Zeng, and S. Zhou. Convergence of Broyden-Like Matrix. Applied Mathematics Letter, 11
(5):35-37, 1998.
D. C. Liu and J. Nocedal. On the limited memory BFGS method for large scale optimization.
Mathematical Programming, Series B, 45(3):503-528, 1989.
J. Lorraine, P. Vicol, and D. Duvenaud. Optimizing Millions of Hyperparameters by Implicit
Differentiation. In AISTATS, 2020.
F. Mannel. On the convergence of the Broyden-like matrices. 2020.
F. Mannel. Convergence properties of the Broyden-like method for mixed linear-nonlinear systems
of equations. Numerical Algorithms, pages 1-29, 2021a.
F. Mannel. On the convergence of Broyden’s method and some accelerated schemes for singular
problems. 2021b.
J. More and J. Trangenstein. On the global convergence of Broyden’s method. Mathematics of
Computation, 30:523-540, 1976.
J. Nocedal and S. Wright. Quasi-Newton Methods. In Numerical Optimization, pages 135-163.
2006.
A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,
L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy,
B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: An Imperative Style, High-Performance
Deep Learning Library. In NeurIPS, 12 2019.
11
Published as a conference paper at ICLR 2022
F. Pedregosa. Hyperparameter optimization with approximate gradient. 33rd International Conference
onMachine Learning, ICML 2016, 2:1150-1159, 2016.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine Learning in Python Gael Varoquaux Bertrand Thirion Vincent
Dubourg Alexandre Passos PEDREGOSA, VAROQUAUX, GRAMFORT ET AL. Matthieu Perrot.
Journal of Machine Learning Research, 12:2825-2830, 2011.
M. E. Sander, P. Ablin, M. Blondel, and G. Peyr6. Momentum Residual Neural Networks. Technical
report, 2021.
S. Schlenkrich, A. Griewank, and A. Walther. On the local convergence of adjoint Broy-
den methods. Mathematical Programming, 121(2):221-247, 2010. ISSN 14364646. doi:
10.1007/s10107-008-0232-y.
J. Sherman and W. J. Morrison. Adjustment of an inverse matrix corresponding to a change in one
element of a given matrix. The Annals of Mathematical Statistics, 21(1):124-127, 1950.
M. Telgarsky. Benefits of depth in neural networks. Journal of Machine Learning Research, 49(June):
1517-1539, 2 2016.
P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski,
P. Peterson, W. Weckesser, J. Bright, S. J. van der Walt, M. Brett, J. Wilson, K. J. Millman,
N. Mayorov, A. R. Nelson, E. Jones, R. Kern, E. Larson, C. J. Carey, I. Polat, Y. Feng, E. W. Moore,
J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen, E. A. Quintero, C. R. Harris,
A. M. Archibald, A. H. Ribeiro, F. Pedregosa, P. van Mulbregt, A. Vijaykumar, A. P. Bardelli,
A. Rothberg, A. Hilboll, A. Kloeckner, A. Scopatz, A. Lee, A. Rokem, C. N. Woods, C. Fulton,
C. Masson, C. Haggstrom, C. Fitzgerald, D. A. Nicholson, D. R. Hagen, D. V. Pasechnik, E. Olivetti,
E. Martin, E. Wieser, F. Silva, F. Lenders, F. Wilhelm, G. Young, G. A. Price, G. L. Ingold, G. E.
Allen, G. R. Lee, H. Audren, I. Probst, J. P, Dietrich, J. Silterra, J. T. Webber, J. Slavic, J. Nothman,
J. Buchner, J. Kulick, J. L. Schonberger, J. V. de Miranda Cardoso, J. Reimer, J. Harrington, J. L. C.
Rodriguez, J. Nunez-Iglesias, J. Kuczynski, K. Tritz, M. Thoma, M. Newville, M. Kummerer,
M. Bolingbroke, M. Tartre, M. Pak, N. J. Smith, N. Nowaczyk, N. Shebanov, O. Pavlyk, P. A.
Brodtkorb, P. Lee, R. T. McGibbon, R. Feldbauer, S. Lewis, S. Tygier, S. Sievert, S. Vigna,
S. Peterson, S. More, T. Pudlik, T. Oshima, T. J. Pingel, T. P. Robitaille, T. Spura, T. R. Jones,
T. Cera, T. Leslie, T. Zito, T. Krauss, U. Upadhyay, Y. O. Halchenko, and Y. Vazquez-Baeza. SciPy
1.0: fundamental algorithms for scientific computing in Python. Nature Methods, 17(3):261-272,
3 2020.
P. Xu, F. Roosta-Khorasani, and M. W. Mahoney. Second-Order Optimization for Non-Convex
Machine Learning: An Empirical Study. In Proceedings of the 2020 SIAM International Conference
on Data Mining, 2020.
12
Published as a conference paper at ICLR 2022
A OPA algorithm
Algorithm LBFGS: (Limited memory) BFGS method with OPA
Input: initial guess (z0, B0-1), where B0-1 is symmetric and positive definite, tolerance > 0,
frequency of additional updates M ∈ N, memory limit L ∈ N ∪ {∞}, (tn) a null
sequence of positive numbers with Pn tn < ∞
Let F ：= Vzgθ
for n = 0, 1, 2, . . . do
if kF(zn)k ≤ then let z? ：= zn and let B ：= Bn; STOP
Let B-1 := B-1
if (n mod M) = 0 then
let en ：= tn B-1 d∂gθ∣ , yn ：= F(Zn + en) - F (Zn) and rn ：= (en)T Vn
zn
if ^n > 0 then
let an ：= en — B-1Vn and let
^-1 . r-1 I an(en) +
Bn ：= * B * * *n H----
(an)T Vn
en(en)

Let B-1 ：= B-1
if n ≥ L then remove update n - L from Bn-1
Let pn ：= -Bn F (Zn )
Obtain αn via line-search and let sn ：= αnpn
Let Zn+1 ：= Zn + sn, Vn ：= F(Zn+1) - F(Zn) and rn ：= (sn) Vn
if rn > 0 then
let an ：= sn - Bn-1Vn and let
—-1 ._ R-I _i_ an(sn)T + sn(an),	(an),yn
Bn+1 ：= Bn +	~	(r. )2 sn
else let Bn-+11 ：= Bn-1
if n ≥ L then remove update n - L from Bn-+11
Output: Z?, B
Remark. A possible choice for (tn) is to use an arbitrary t0 > 0 and tn ：= ksn-1 k for n ≥ 1.
B	Proofs of SHINE convergence
To facilitate reading, we restate the results before proving them.
B.1 Convergence using ULI
(n)
Theorem 2 (Convergence of SHINE to the Hypergradient using ULI). Let us denote pθ , the SHINE
direction for iterate n in Algorithm 1 with b = true. Under Assumptions 1 and 2, for a given
parameter θ, (Zn ) converges q-superlinearly to Z? and
nlim∞ pθn) = dθlz? ∙
Proof. Under Assumptions 1 and 2, More and Trangenstein (1976, Theorem 5.7) shows that Bn
satisfies
lim Bn = Jgθ(Z?)
n→∞
The inversion operator is continuous in the space of invertible matrices, so we have:
lim Bn1 = Jge (z?)T
n→∞
Because Vz L and 鬻 are continuous at z? by Assumption 2 (iii), We also have thanks to AssUmP-
tion 2 (i):
nlim∞ VzL(Zn)= Vz L(Z?)	and	nlim∞ 羽 Zn = ∂i∖ z.
13
Published as a conference paper at ICLR 2022
By continuity we then deduce that, as claimed,
∂ LI	L
=—I	□
z?	∂θ Iz?
lim Pn= lim NzL(Zn)B-粤(Zn) = Vz L(z^)Jgθ (z?)-1 黑
n→∞	n→∞	∂θ	∂θ
B.2 Convergence for BFGS with OPA
Assumption 5 (Extended Assumptions for BFGS). Let gθ (Z) = Vzrθ (Z) for some C2 function
rθ : Rd → R. Consider Algorithm 1 with b = false and suppose that
Lthe set Ω := {z ∈ Rd : r (Z) ≤ r (zo)} is convex;
2.	rθ is StrOngIy convex in an open superset of Ω (this implies that rθ has a unique global
minimizer Z?) and has a Lipschitz continuous Hessian near Z?;
3.	there are positive constants η1, η2 such that the line search used in the algorithm ensures
that for each n ≥ 0 either
rθ(Zn+1) ≤ rθ(Zn) - η1
Vrθ (Zn)TPn ] 2
,TPnk__
or rθ(Zn+1) ≤ rθ(Zn) + η2Vrθ(Zn)Tpn
is satisfied;
4.	the line search has the property that αn = 1 will be used if both
Il(Bn - Jgθ (Zn))Snk
ksnk
and kZn - Z? k
are sufficiently small.
Remark. The requirements 3. and 4. on the line search are, for instance, satisfied under the
well-known Wolfe conditions, see Byrd et al. (1988, section 3) for further comments.
Theorem 3 (Convergence of SHINE to the Hypergradient for BFGS with OPA). Let us consider p(θn),
the SHINE direction for iterate n in Algorithm 1 that is enriched by extra updates in the direction en
defined in (5). Under Assumptions 2 (ii-iii) and 3, for a given parameter θ, we have the following:
Algorithm 1, for any symmetric and positive definite matrix B0, generates a sequence (Zn) that
converges q-superlinearly to Z?, and there holds
nl→∞pθn) = >lz? ∙
(6)
Proof. The proof is divided into four steps. The first step is to establish the q-superlinear convergence
of (Zn) to Z?. Denoting by Ne ⊂ {0, M, 2M, ∙ ∙ ∙} the set of indices of extra updates that are actually
applied, the second step consists of showing
lim
Ne 3n→∞
(Bn - Jgθ (Z?)) ⅛
(9)
where, in this proof, Bn always represents the matrix from Algorithm LBFGS before the update in
the direction en is applied, i.e., the matrix whose inverse appears in the definition of en while Bn
always represents the matrix from Algorithm LBFGS after the update in the direction en has been
applied; if the update in the direction en is not applied, then Bn = Bn The third step is to prove that
(9) implies the desired convergence (6) of the SHINE direction if the limit n → ∞ is replaced by
Ne 3 n → ∞, i.e., the limit is taken on the subsequence corresponding to Ne. The fourth step is
then to transfer the convergence to the entire sequence.
It is easy to check that instead of updating B-1, respectively, B-1, We can also obtain the sequences
ʌ
(Bn ) and (Bn ) by updating according to
Bn+1
T
R . ynJn
Bn +--T-----
ynT sn
for the usual update (skipping the update if ynT sn ≤ 0), respectively,
^ _ y . ynyT	* * Bnen(Bnen)T
Bn = Bn十大亍 一 T c
yT en	en Bnen
for the extra update (skipping the update if yT en ≤ 0). Here, the quantities yn, r^n and en are
defined as in Algorithm LBFGS. We can now argue essentially as in the proof of Byrd et al. (1988,
14
Published as a conference paper at ICLR 2022
Theorem 3.1) to show that (zn) converges q-superlinearly to z?. As part of that proof we obtain that
Bn = Bn for atleast「0.5Q] of the indices n = 0,M, 2M,..., QM for any Q ∈ N (namely for all
n ∈ Ne satisfying n ≤ QM) and that we can apply Byrd and Nocedal (1989, Theorem 3.2), which
yields
nl→∞ (Brn-Jg (Z?))商=0 and Ne3in→∞ (Brn-Jg (Z?)) ⅛ =0∙	(10)
For the third step, We abbreviate vn := 嘴 ∣zn. From the definition of en and (10) We infer that
0 = Ne3n→∞ (Bn-Jgθ (Z?)) / = Ne3in→∞ (I- Jgθ (Z?)B—1)
After multiplication with Jgθ (Z?)-1 this entails
vn
kB-1Vnk
Ne3im÷∞ (Jg, (Z?)T- B-I) ⅛ =0，
which shows that
Nlim B-Ivn = Mlim Jgθ (Z*)Tvn = Jgθ (Z?尸 dgθ |z?
Ne 3n→∞	Ne 3n→∞	∂θ
by Assumption 2 (iii). Using Assumption 2 (iii) again it folloWs that
Ne3in→∞ P? = Ne3in→∞ VzL(Zn)B-l 器 Ln
VzL(z?)Jg。(z*)T dgθ∖z
∂ LI
∂θ Iz
concluding the third step. To infer that (6) holds, it suffices to shoW thatlimNe3n→∞ kBn -Bjn k = 0
for any sequence (jn)n∈Ne ⊂ N such that {jn,jn + 1, ∙∙∙, n-1}∩Ne = 0 for all n ∈ Ne sufficiently
large. Indeed, since for C := max{supn kBnk, supn kBn-1k}, Which is finite by Byrd and Nocedal
(1989, Theorem 3.2), there holds
(Bn) ⊂ nA ∈ Rd×d : A-1 exists, kAk ≤ C, kA-1k ≤ Co
and the set on the right-hand side of the inclusion is compact by the Banach lemma, inversion is a
uniformly continuous operation on this set, hence limNe3n→∞ kBn-1 - Bj-1 k = 0, so
lim
Ne 3n→∞
kp(θn) -p(θjn)k =0
by continuity, and therefore
lim	Pyn) = lim	pθn) = dLI
Ne 3n→∞	Ne 3n→∞	∂θ z?
by the third step, establishing the claim.
It remains to shoW the validity of limNe3n→∞ kBn - Bjn k = 0 for any sequence (jn)n∈Ne such
that {jn,jn + 1, ∙ ∙ ∙ , n - 1} ∩ Ne = 0 for all n ∈ Ne sufficiently large. Since at least every second
extra update is actually carried out, the condition on the intersection implies n - jn ≤ 2M - 1 for
all these n.	NoW let	(jn)n∈Ne	be any such sequence. Then	Bn	- Bjn	=	Pnm-=1j	Bm+1 -	Bm	is a
sum of at most 2M - 1 BFGS updates in search directions, but contains no extra updates. Hence,
the secant conditions Bn-lsn-1-l = yn-1-l, l ∈ {0, 1, ∙ ∙ ∙ , n - jn}, are satisfied, alloWing us to
deduce
kBn-l - Bn-l-1 k
k (Bn-I - Bn — l — 1)sn—l — 1 k
Il sn-l-1 k
≤ kyn-1 -1 - Jgθ (Z?)sn —l—1k . Il(Bn-1 -1 - Jgθ (Z?))sn —l —1 k
≤ Fm +	Fn--I
for all l ∈ {0, 1, ∙ ∙ ∙ , n - jn - 1}. For each of these l, both terms on the right-hand side tend to zero
for Ne 3 n → ∞ (for the second term this folloWs from the first identity in (10) due to Bn—l—1 =
Bn-1-1). Recalling that Bn — Bjn = Pm=jn B%+ι - Bm we find limM3n→∞ kBn — Bjnk = 0,
which finishes the fourth step and thus concludes the proof.	□
15
Published as a conference paper at ICLR 2022
B.3 Convergence for Adjoint B royden with OPA
Theorem 4 (Convergence of SHINE to the Hypergradient for Adjoint Broyden with OPA). Let us
(n)
consider pθ , the SHINE direction for iterate n in Algorithm 1 with the Adjoint Broyden secant
condition (7) and extra update in the direction vn defined in (8). Under Assumptions 2 and 4, for a
given parameter θ, we have q-superlinear convergence of (zn) to z? and
nlim∞pθn) = Hz? ∙
Proof. Due to Assumption 2, the superlinear convergence of (zn) follows from Schlenkrich et al.
(2010, Theorem 2). The proof of the remaining claim is divided into two cases.
Case 1:	Suppose that YzL(z?) = 0. By continuity this implies limn→∞ YzL(zn) = 0. Since the
sequence (Bn1 架 |zn) is bounded by Assumption 4, it follows that
lim pθn) = lim YzL(Zn)BnIdgθ∣ = 0 = 1LI ,
n→∞ θ n→∞	n ∂θ zn	∂θ z?
as claimed.
Case 2:	Suppose that YzL(z?) = 0. By continuity this implies YzL(Zn) = 0 for all sufficiently
large n ∈ N. Let us denote by Ne ⊂ N the set of indices of extra updates. We stress that this set
is infinite since, by construction, every M-th update is an extra update. We have vn 6= 0 for all
sufficiently large n ∈ Ne, hence Schlenkrich et al. (2010, Lemma 3) yields
lim
Ne 3n→∞
∣∣YzL(zn)(I- Bn1Jgθ(z?))k =	lim
k(Yz L(Zn)Bn1)τ k	Ne 3n→∞
k(Vn)T(Bn- Jgθ (Z?))∣∣
kvnk
0.
This implies
lim	kYzL(Zn)(Jgθ (z?)-1- B-1)k = 0
Ne3n→∞	∣Yz L(Zn)B-1I	,
thus necessarily
lim	kYz L(zn)(Jgθ (z?)T- B-I)Il = 0.
Ne 3n→∞
Since limNe3n→∞ VzL(Zn) Jg§ (z?)-1 = YzL(ζ?)Jgθ (z?)-1 by continuity, we find
lim	Yz L(Zn)BnI = Yz L(ZnJgθ (z?)-1,
Ne 3n→∞
whence
Ne3in→∞ pθn) = Ne3in→∞ YzL(Zn)BnI ∂θ∖zn
YzL(Z*)Jgθ(Z?)-1 -∂θθ∣	= ∂θ∣ *, (II)
∂θ z?	∂θ z?
where we have used continuity again. To prove that these limits hold not only for Ne 3 n → ∞
but in fact for all N 3 n → ∞, we establish, as intermediate claim, that for any fixed m ∈ N we
have limn→∞ kBn+m - Bn k = 0. Note that this claim is equivalent to limn→∞ kBn+1 - Bn k = 0.
Denoting by L ≥ 0 the Lipschitz constant of Jgθ near Z?, we find
kBn+1 - Bnk = K Ji BnW ≤ kJgθ (Zn+O - J (Z+ W醺 (ZI-BnVnk
≤ L∣zn+1- z?k+kEτvnk ∙
Both terms on the right-hand side go to zero as n goes to infinity: the first one due to limn→∞ Zn = Z?
and the second one since limn→∞ 曦 Vnk = 0 by Schlenkrich et al. (2010, Lemma 3). This shows
that limn→∞ kBn+1 - Bnk = 0, which concludes the proof of the intermediate claim.
From limn→∞ kBn+m - Bn k = 0 for any fixed m ∈ N it follows that for any sequence
(jn ) ⊂ N with supn |jn - n| < ∞ there holds limn→∞ kBjn - Bn k = 0. This implies for
any such sequence (jn) the limit limn→∞ kBj-1 - Bn-1 k = 0. To establish this, note that for
C := max{supn kBnk, supn kBn-1k}, which is finite by Assumption 4 and the combination of the
bounded deterioration principle (Schlenkrich et al., 2010, Lemma 2) with Assumption 2 (i), the set
nA ∈ Rd×d : A-1 exists, kAk ≤ C, kA-1k ≤ Co
16
Published as a conference paper at ICLR 2022
includes the sequence (Bn) and is compact by the Banach lemma, so inversion is a uniformly
continuous operation on this set.
Now let us construct a sequence (jn) ⊂ Ne by defining, for every n ∈ N, jn := arg minm∈Ne |n-m|.
That is, for every n, jn denotes the member of Ne with the smallest distance to n. It is clear that
|n - jn| ≤ M - 1 for all n, hence limn→∞ kBj-1 - Bn-1 k = 0. Using this and, again, continuity it
is easy to see that
lim kp(θn) - p(θjn) k = 0,
n→∞
which implies by (11) that
lim pθ") = lim Pθjn) = IVlim Pθn) = ||| ,
n→∞	n→∞	Ne 3n→∞	∂θ z?
thereby establishing the claim.	□
Remark. An inspection of the proof reveals that if Bnis never updated in the direction zn, but only
updated in the direction vn defined in (8), then Assumption 4 can be replaced by the significantly
weaker assumption that the sequence (B-1 d∂gθ ∣zn) is bounded. The price to Pay is that the Conver-
gence rate of (zn) to z? will be slower (q-linear instead of q-superlinear) since the updates in the
direction zn are critical for ensuring fast convergence of (zn) to z?.
C Logistic Regression Hyperparameters
For both datasets we split the data randomly (with a different seed for each run) between training-
validation-test, with the following proportions: 90%-5%-5%. The hyperparameters are the same as in
the original HOAG work (Pedregosa, 2016), except:
•	We use a memory limitation of 30 updates (not grid-searched) for accelerated methods
(Jacobian-Free and SHINE), compared to 10 for the original method. This is because the
approximation should be better using more updates. We verified that using 30 updates for
the original method does not improve the convergence speed. That number is 60 for OPA.
•	We use a smaller exponential decrease of 0.78 (not grid-searched) for the accelerated
methods, compared to 0.99 for the original method. This is because in the very long run, the
approximation can cause oscillations.
We also use the same setting as Pedregosa (2016) for the Grid and Random Search. Finally, we
highlight that warm restart is used for both the inner problem and the Hessian inversion in the
direction of the gradient.
OPA inversion experiments For the OPA experiments, we used a memory limitation of 60, and a
tolerance of 10-6 . The OPA update is done every 5 regular updates.
D DEQ training details
The training details are the same as the original Multiscale DEQ paper (Bai et al., 2020): all the
hyperparameters are kept the same and not fine-tuned, and the data split is the same. We recall here
some important aspects. For both datasets, the network is first trained in an unrolled weight-tied
fashion for a few epochs in order to stabilize the training.
We also underline that the DEQ models, in addition to having a fixed-point-defining sub-network,
also have a classification and a projection head.
Finally, for Figure 3, the median backward pass is computed with 100 samples on a single V100
GPU for a batch size of 32.
D.1 CIFAR
The Adam optimizer (Kingma and Ba, 2015) is used with a 10-3 start learning rate, and a cosine
annealing schedule.
D.2 ImageNet
The Stochastic Gradient Descent optimizer is used with a 5 × 10-2 start learning rate, and a cosine
annealing schedule.
The images are downsampled 2 times before being fed to the fixed-point defining sub-network.
17
Published as a conference paper at ICLR 2022
A~IBm-doqns SSOa 卢S①I
real-sim
SHINE (ours)
SHINE refine (ours)
Grid search
Random search
一HOAG
HOAG - lim. backward
・・・ Jacobian-Free
Figure E.1: Bi-level optimization: Convergence of different hyperparameter optimization methods
on the '2-regularized logistic regression problem for two datasets (20news (Lang, 1995) and real-
sim (lib)) on held-out test data.
E Additional results
E.1 Bi-level optimization extended
In order to make sure that SHINE was indeed improving over HOAG (Pedregosa, 2016), we also
looked at the results obtained when performing an inversion with a precision lower than that pre-
scribed by Pedregosa (2016) originally (i.e. truncating the iterative inversion). These results, also
complemented with Random Search (Bergstra and Bengio, 2012), can be seen in Figure E.1. They
confirm that the advantage provided by SHINE cannot be retrieved with a looser tolerance on the
inversion.
E.2 Regularized Nonlinear Least Squares
In order to further validate the efficiency of SHINE compared to competing methods, we also
benchmarked it on the regularized nonlinear least squares task. For a training set (xtrain,i, ytrain,i)iN=1
and a test set (xtest,i, ytest,i)iM=1, this problem reads
1M
min2 E l∣ytest,i - σ((z*)>xtest,i)k2
i=1
z*
1N	>	2 θ 2
argmin2 E Ilytrainj — σ(z> Xtrain,j )∣2 + ] ∣∣Z∣2
z	j=1
(12)
where σ denotes the sigmoid function σ(x) = ι+1-x. For a fixed hyper-parameter θ, this task is
typically solved using L-BFGS (Berahas et al., 2021; Xu et al., 2020).
18
Published as a conference paper at ICLR 2022
Xsstτπlκoqns SSOa +səɪ
SHINE (ours) ・・・ Jacobian-Free SHINE refine (ours) SHINE - OPA (ours)
一HOAG
Figure E.2: Bi-level optimization on regularized nonlinear least squares: Convergence of dif-
ferent hyperparameter optimization methods on the '2-regularized nonlinear least squares for the
20news (Lang, 1995) dataset on held-out test data.
Table E.1: Nonlinear spectral radius obtained by the power method for the fixed-point defining
sub-network for the 3 different methods.
Method	Nonlinear spectral radius
Original	^30.5
Jacobian-Free	^T93^
SHINE 一	234.2	—
We can see in Figure E.2 that SHINE clearly outperforms the Jacobian-Free method and it is also
quicker to converge compared to HOAG. We can also notice the benefit of OPA compared to the
vanilla SHINE method is more pronounced. We hypothesize that this is due to the nonconvex nature
of the inner problem making the Hessian inverse approximation more difficult, as was noted by
Berahas et al. (2021).
E.3 Contractivity assumption
One of the main limiting assumptions in the original Jacobian-Free method work (Fung et al., 2021),
is the contractivity assumption. We showed here that it was not important to enforce this in order
to achieve excellent results, but one can wonder whether this assumption is not met in practice
thanks to the unrolled pretraining of DEQs. We looked at the contractivity of the fixed-point defining
sub-network empirically by using the power-method applied to a nonlinear function, in the CIFAR
setting. The results, summarized in Table E.1, show that the fixed-point defining sub-network is not
contractive at all.
E.4 Time gains
Because the total training time is not only driven by backward pass but also by the forward pass and
the evaluation, we show for completeness in Table E.2 the time gains for the different acceleration
methods for the overall epoch. We do not report in this table the time taken for pre-training which is
equivalent across all methods, and is not something on which SHINE has an impact. It is clear in
Table E.2 that accelerated methods can have a significant impact on the training of DEQs because we
see that half the time of the total pass is spent on the backward pass (more on ImageNet (Deng et al.,
2009)). We also notice that while SHINE has a slightly slower backward pass than the Jacobian-Free
method (Fung et al., 2021), the difference is negligible when compared to the total pass computational
cost.
19
Published as a conference paper at ICLR 2022
Table E.2: The time required for each method on the different datasets during the equilibrium training.
For the forward and backward passes, the time is measured offline, for a single batch of 32 samples,
with a single GPU, using the median to avoid outliers. This time is given in milliseconds. For the
epochs, the time is measured by taking an average of the 6 first epochs, and given in hours-minutes
for Imagenet and minutes-seconds for CIFAR. The epoch time for SHINE without improvement on
Imagenet is not given because it never reaches the 26 forward steps: the implicit depth is too short.
Fallback is not used for CIFAR. Numbers in parenthesis indicate the number of inversion steps for
the refined versions.
Dataset Name	CIFAR (Krizhevsky,2009)			ImageNet (Deng et al., 2009)		
Method Name	Forward	Backward	Epoch	Forward	Backward	Epoch
Original (Bai et al., 2020)	-256-	210	4min40	-644-	798	3h38
Jacobian-Free (Fung et al., 2021)	-249	12.9	3min10	-621	13.5	2h02
SHINE Fallback (ours)	-218	160	3min20	-622	353	2h13
SHINE Fallback refine (5, ours)-	-272	966	3min50	-622	212	2h44
Jacobian-Free refine (5)	-260	865	3min40	-620	186	2h43
Original limited backprop	281	86.4	3min50	653	187	2h40
Method
JaCobian-Free
SHINE w. Broyden
SHINE w. Adj. Broyden
SHINE w. Adj. Broyden / OPA
Figure E.3: Quality of the inversion using OPA in DEQs : Ratio of the inverse approximation
over the exact inverse function of the cosine similarity between the inverse approximation b =
VzL(z?)B-1 and the exact inverse a = ▽%£(,?) Jg9 (z?)-1 for different methods. For OPA, the
extra update frequency is 5. 100 runs were performed with different batches.
E.5 DEQ OPA results
We can clearly see in Figure E.3 that in the case of DEQs, OPA also significantly improves the
inversion over the other accelerated methods. We also see that the improvements of SHINE over the
Jacobian-Free method without OPA are marginal.
Because the inversion is so good, we would expect that the performance of SHINE with OPA would
be on par with the original method’s. However, this is not what we see in the results presented
in Table E.3. Indeed, OPA does improve on SHINE with only Adjoint Broyden, but it does not
outperform SHINE done with Broyden.
Table E.3: CIFAR DEQ OPA results : Top-1 accuracy of different methods on the CIFAR dataset,
and epoch mean time.
Methode name	Top-1 Accuracy (%)	Epoch mean time
Original	-9351	4min40
Jacobian-Free	-9309	3min10
SHINE (BrOyden)	^^937T4	3min20
SHINE (Adj. BrOyden)	-92：89	4min
SHINE (Adj. BrOyden/OPAr	93.04	4min40
20