Published as a conference paper at ICLR 2022
The MultiBerts: Bert Reproductions for
Robustness Analysis
Thibault Sellam； SteveYadlowsky； Ian Tenney； Jason Weij Naomi SaphraJ
Alexander D’Amour, Tal Linzen, Jasmijn Bastings, Iulia Turc,
Jacob Eisenstein, Dipanjan Das, and Ellie Pavlick
{tsellam, yadlowsky, iftenney, epavlick}@google.com
Google Research
Ab stract
Experiments with pre-trained models such as Bert are often based on a single
checkpoint. While the conclusions drawn apply to the artifact tested in the ex-
periment (i.e., the particular instance of the model), it is not always clear whether
they hold for the more general procedure which includes the architecture, training
data, initialization scheme, and loss function. Recent work has shown that repeat-
ing the pre-training process can lead to substantially different performance, sug-
gesting that an alternate strategy is needed to make principled statements about
procedures. To enable researchers to draw more robust conclusions, we intro-
duce the MultiB erts, a set of 25 Bert-Base checkpoints, trained with similar
hyper-parameters as the original Bert model but differing in random weight ini-
tialization and shuffling of training data. We also define the Multi-Bootstrap, a
non-parametric bootstrap method for statistical inference designed for settings
where there are multiple pre-trained models and limited test data. To illustrate
our approach, we present a case study of gender bias in coreference resolution, in
which the Multi-Bootstrap lets us measure effects that may not be detected with
a single checkpoint. We release our models and statistical library,* 1 along with
an additional set of 140 intermediate checkpoints captured during pre-training to
facilitate research on learning dynamics.
1	Introduction
Contemporary natural language processing (NLP) relies heavily on pretrained language models,
which are trained using large-scale unlabeled data (Bommasani et al., 2021). Bert (Devlin et al.,
2019) is a particularly popular choice: it has been widely adopted in academia and industry, and
aspects of its performance have been reported on in thousands of research papers (see, e.g., Rogers
et al., 2020, for an overview). Because pre-training large language models is computationally ex-
pensive (Strubell et al., 2019), researchers often rely on the release of model checkpoints through
libraries such as HuggingFace Transformers (Wolf et al., 2020), which enable them to use large-scale
language models without repeating the pre-training work. Consequently, most published results are
based on a small number of publicly released model checkpoints.
While this reuse of model checkpoints has lowered the cost of research and facilitated head-to-head
comparisons, it limits our ability to draw general scientific conclusions about the performance of a
particular class of models (Dror et al., 2019; D’Amour et al., 2020; Zhong et al., 2021). The key
issue is that reusing model checkpoints makes it hard to generalize observations about the behavior
of a single model artifact to statements about the underlying pre-training procedure which created
it. Pre-training such models is an inherently stochastic process which depends on the initializa-
tion of the model’s parameters and the ordering of training examples; for example, D’Amour et al.
* Equal contribution.
t Work done as a Google AI resident.
^ Work done during an internship at Google.
1http://goo.gle/multiberts
1
Published as a conference paper at ICLR 2022
(2020) report substantial quantitative differences across multiple checkpoints of the same model ar-
chitecture on several “stress tests” (Naik et al., 2018; McCoy et al., 2019). It is therefore difficult to
know how much of the success of a model based on the original Bert checkpoint is due to Bert’s
design, and how much is due to idiosyncracies of a particular artifact. Understanding this differ-
ence is critical if we are to generate reusable insights about deep learning for NLP, and improve the
state-of-the-art going forward (Zhou et al., 2020; Dodge et al., 2020; Aribandi et al., 2021).
This paper describes the MultiBerts, an effort to facilitate more robust research on the Bert model.
Our primary contributions are:
•	We release the MultiBERTs, a set of 25 BERT-Base, Uncased checkpoints to facilitate stud-
ies of robustness to parameter initialization and order of training examples (§2). Releas-
ing these models preserves the benefits to the community of a single checkpoint release
(i.e., low cost of experiments, apples-to-apples comparisons between studies based on these
checkpoints), while enabling researchers to draw more general conclusions about the Bert
pre-training procedure.
•	We present the Multi-Bootstrap, a non-parametric method to quantify the uncertainty of ex-
perimental results based on multiple pre-training seeds (§3), and provide recommendations
for how to use the Multi-Bootstrap and MultiB erts in typical experimental scenarios. We
implement these recommendations in a software library.
•	We illustrate the approach with a practical use case: we investigate the impact of coun-
terfactual data augmentation on gender bias, in a B ert-based coreference resolution sys-
tems (Webster et al., 2020) (§4). Additional examples are provided in Appendix E, where
we document challenges with reproducing the widely-used original Bert checkpoint.
The release also includes an additional 140 intermediate checkpoints, captured during training for 5
of the runs (28 checkpoints per run), to facilitate studies of learning dynamics. Our checkpoints and
statistical libraries are available at: http://goo.gle/multiberts.
Additional Related Work. The MultiB ERTs release builds on top of a large body of work that
seeks to analyze the behavior of Bert (Rogers et al., 2020). In addition to the studies of robust-
ness cited above, several authors have introduced methods to reduce Bert’s variability during fine-
tuning (Zhang et al., 2021; Mosbach et al., 2021; Dodge et al., 2020; Lee et al., 2020; Phang et al.,
2018). Other authors have also studied the time dimension, which motivates our release of interme-
diate checkpoints (Liu et al., 2021; Hao et al., 2020; Saphra & Lopez, 2019; Chiang et al., 2020;
Dodge et al., 2020). Similarly to §3, authors in the NLP literature have recommended best practices
for statistical testing (Koehn, 2004; Dror et al., 2018; Berg-Kirkpatrick et al., 2012; Card et al., 2020;
S0gaard et al., 20l4; Peyrard et al., 2021), many of which are based on existing tests to estimate the
uncertainty of test sample. In concurrent work, Deutsch et al. (2021) considered bootstrapping
methods similar to the Multi-Bootstrap, in the context of summarization metrics evaluation. Also
in concurrent work, the Mistral project (Karamcheti et al., 2021) released a set of 10 GPT-2 models
with intermediate checkpoints at different stages of pre-training. Our work is complementary, fo-
cusing on BERT, introducing a larger number of pre-training seeds, and presenting a methodology
to draw robust conclusions about model performance.
2	Release Description
We first describe the MultiB erts release: how the checkpoints were trained and how their perfor-
mance compares to the original Bert on two common language understanding benchmarks.
2.1	Training
Overview. The MultiBERTs checkpoints are trained following the code and procedure of Devlin
et al. (2019), with minor hyperparameter modifications necessary to obtain comparable results on
GLUE (Wang et al., 2019); a detailed discussion of these differences is provided in Appendix E.
We use the Bert-Base, Uncased architecture with 12 layers and embedding size 768. We trained
the models on a combination of BooksCorpus (Zhu et al., 2015) and English Wikipedia. Since the
2
Published as a conference paper at ICLR 2022
Figure 1: Distribution of the performance on GLUE dev sets (Wang et al., 2019), averaged across
fine-tuning runs for each checkpoint. The dashed line indicates the performance of the original
Bert release.
exact dataset used to train the original Bert is not available, we used a more recent version that was
collected by Turc et al. (2019) with the same methodology.
Checkpoints. We release 25 models trained for two million steps each, each training step involv-
ing a batch of 256 sequences. For five of these models, we release 28 additional checkpoints captured
over the course of pre-training (every 20,000 training steps up to 200,000, then every 100,000 steps).
In total, we release 165 checkpoints, about 68 GB of data.
Training Details. As in the original BERT paper, we used batch size 256 and the Adam opti-
mizer (Kingma & Ba, 2014) with learning rate 1e-4 and 10,000 warm-up steps. We used the default
values for all the other parameters, except the number of steps, which we set to two million, and se-
quence length, which we set to 512 from the beginning with up to 80 masked tokens per sequence.2
We follow the Bert code and initialize the layer parameters from a truncated normal distribution,
using mean 0 and standard deviation 0.02. We train using the same configuration as Devlin et al.
(2019)3, with each run taking about 4.5 days on 16 Cloud TPU v2 chips.
Environmental Impact Statement. We estimate compute costs at around 1728 TPU-hours for
each pre-training run, and around 208 GPU-hours plus 8 TPU-hours for associated fine-tuning ex-
periments (§2.2, including hyperparameter search and 5x replication). Using the calculations of
Luccioni et al. (2019)4, we estimate this as about 250 kg CO2e for each of our 25 models. Counting
the 25 runs each of CDA-incr and CDA-full from §4, associated coreference models (20 GPU-hours
per pretraining model), and additional experiments of Appendix E, this gives a total of about 12.0
metric tons CO2e before accounting for offsets or clean energy. Based on the report by Patterson
et al. (2021) of 78% carbon-free energy in Google Iowa (us-central1), we estimate that reproducing
these experiments would emit closer to 2.6 tons CO2e, or slightly more than two passengers on a
round-trip flight between San Francisco and New York. By releasing the trained checkpoints pub-
licly, we aim to enable many research efforts on reproducibility and robustness without requiring
this cost to be incurred for every subsequent study.
2.2	Performance Benchmarks
GLUE Setup. We report results on the development sets of the GLUE tasks: CoLA (Warstadt
et al., 2019), MNLI (matched) (Williams et al., 2018), MRPC (Dolan & Brockett, 2005),
QNLI (v2) (Rajpurkar et al., 2016; Wang et al., 2019), QQP (Chen et al., 2018), RTE (Bentivogli
et al., 2009), SST-2 (Socher et al., 2013), and SST-B (Cer et al., 2017). In all cases we follow the
same approach as Devlin et al. (2019). For each task, we fine-tune Bert for 3 epochs using a batch
2Specifically, we keep the sequence length constant (the paper uses 128 tokens for 90% of the training then
512 for the remaining 10%) to expose the model to more tokens and simplify the implementation. As we were
not able to reproduce original Bert exactly using either 1M or 2M steps (see Appendix E for discussion),
we release MultiB erts trained with 2M steps under the assumption that higher-performing models are more
interesting objects of study.
3We use https://github.com/google-research/bert with TensorFlow (Abadi et al., 2015)
version 2.5 in v1 compatibility mode.
4https://mlco2.github.io/impact/
3
Published as a conference paper at ICLR 2022
size of 32. We run a parameter sweep on learning rates [5e-5, 4e-5, 3e-5, 2e-5] and report the best
score. We run the procedure five times for each of the 25 models and average the results.
SQuAD Setup. We report results on the development sets of
SQuAD versions 1.1 and 2.0 (Rajpurkar et al., 2016; 2018),
using a setup similar to that of Devlin et al. (2019). For both
sets of experiments, we use batch size 48, learning rate 5e-5,
and train for 2 epochs.
Results. Figures 1 and 2 show the distribution of the
MultiBerts models’ performance on the development sets
of GLUE and SQuAD, in comparison to the original Bert
checkpoint.5 On most tasks, original Bert’s performance
falls within the same range as MultiBerts (i.e., original Bert
is between the minimum and maximum of the MultiBerts’
Figure 2: Performance distribution
on the dev sets of SQuAD v1.1 and
v2.0 (Rajpurkar et al., 2016; 2018).
scores). However, original Bert outperforms all MultiBerts models on QQP, and under-performs
them on SQuAD. The discrepancies may be explained by both randomness and differences in train-
ing setups, as investigated further in Appendix E.
To further illustrate the performance variability inherent to pre-training and fine-tuning, we analyze
the instance-level agreement between the models in Appendix C.
3	Hypothesis Testing Using Multiple Checkpoints
The previous section compared MultiBerts with the original Bert, finding many similarities but
also some differences (e.g., in the case of SQuAD). To what extent can these results be explained by
random noise? More generally, how can we quantify the uncertainty of a set of experimental results
when there are multiple sources of randomness?
In parallel to the MultiBerts release, we propose a more principled and standardized method
to compare training procedures. We recommend a non-parametric bootstrapping procedure, the
“Multi-Bootstrap”, which enables us to make inference about model performance in the face of
multiple sources of uncertainty: the randomness due to the pre-training seed, the fine-tuning seed,
and the finite test data. The main idea is to use the average behavior over seeds as a means of
summarizing expected behavior in an ideal world with infinite samples.
Although we present Multi-Bootstrap in the context of analyzing the MultiBERTs, the method could
be applied in all setups that involve a set of checkpoints pre-trained with the same method, a finite
test set, and (possibly) multiple rounds of fine-tuning. The Multi-Bootstrap is implemented as a
Python library, included with the MultiB erts release.
3.1	Interpreting Statistical Results
The Multi-Bootstrap provides an estimate of the amount of remaining uncertainty when summariz-
ing the performance over multiple seeds. The following notation will help us state this precisely.
We assume access to model predictions f (x) for each instance x in the evaluation set. We consider
randomness arising from:
1.	The choice of pre-training seed S 〜M
2.	The choice of fine-tuning seed T 〜N
3.	The choice of test sample (X, Y)〜D
The Multi-Bootstrap procedure allows us to account for all of the above. Specifically, MultiBerts
enables us to estimate the variance due to the choice of pre-training seed (1), which would not be
possible with a single artifact. Note that multiple fine-tuning runs are not required in order to use
the procedure.
5We used https://storage.googleapis.com/bert_models/2020_02_20/uncased_
L-12_H-768_A-12.zip, as linked from https://github.com/google- research/bert.
4
Published as a conference paper at ICLR 2022
For each pre-training seed s, let fs (x) denote the learned model’s prediction on input features x
and let L(s) denote the expected performance metric of fs on a test distribution D over features X
and labels Y . For example, the accuracy would be L(s) = E[1{Y = fs (X)}]. We can use the test
sample (which we will assume has nx examples) to estimate the performance for each of the seeds
in MUltiBERTs, which We denote as L(s).
The performance L(s) depends on the seed, but we are interested in summarizing the model over
all seeds. A natural summary is the average over seeds, ES〜M[L(S)], which We will denote by θ.
Then, using ns independently sampled seeds, we can compute an estimate θ as
ns
帝=；E L(Sj).
ns
s j=1
Because θ is computed under a finite evaluation set and finite number of seeds, it is necessary to
quantify the uncertainty of the estimate. The goal of Multi-Bootstrap is to estimate the distribution
of the error in this estimate, θ - θ, in order to compute confidence intervals and test hypotheses
about θ, such as whether it is above some threshold of interest. Below, we describe a few common
experimental designs in NLP that can be studied with these tools.
Design 1: Comparison to a Fixed Baseline. In many use cases, we want to compare BERT’s
behavior to that of a single, fixed baseline. For instance, does Bert encode information about
syntax as a feature-engineered model would (Tenney et al., 2019; Hewitt & Manning, 2019)? Does
it encode social stereotypes, and how does it compare to human biases (Nadeem et al., 2021)? Does it
encode world knowledge, similarly to explicit knowledge bases (Petroni et al., 2019)? Does another
model such as RoBERTa (Liu et al., 2019) outperform Bert on common tasks such as those from
the GLUE benchmark?
In all these cases, we compare MultiB erts to some external baseline of which we only have a single
estimate (e.g., random or human performance), or against an existing model that is not derived from
the MultiB e rts checkpoints. We treat the baseline as fixed, and assess only the uncertainty that
arises from MultiBerts’ random seeds and the test examples.
Design 2: Paired Samples. Alternatively, we might seek to assess the effectiveness of a specific
intervention on model behavior. In such studies, an intervention is proposed (e.g., representation
learning via a specific intermediate task, or a specific architecture change) which can be applied to
any pre-trained Bert checkpoint. The question is whether the procedure results in an improvement
over the original B ert pre-training method: does the intervention reliably produce the desired effect,
or is the observed effect due to the idiosyncracies of a particular model artifact? Examples of such
studies include: Does intermediate tuning on NLI after pre-training make models more robust across
language understanding tasks (Phang et al., 2018)? Does pruning attention heads degrade model
performance on downstream tasks (Voita et al., 2019)? Does augmenting Bert with information
about semantic roles improve performance on benchmark tasks (Zhang et al., 2020)?
We refer to studies like the above as paired since each instance of the baseline model fs (which
does not receive the intervention) can be paired with an instance of the proposed model fs′ (which
receives the stated intervention) such that fs and fs′ are based on the same pretrained checkpoint
produced using the same seed. Denoting θf and θf′ as the expected performance defined above
for the baseline and intervention model respectively, our goal is to test hypotheses about the true
difference in performance δ = θf′ - θf using the estimated difference δ = θf′ - θf.
In a paired study, Multi-Bootstrap allows us to estimate both of the errors θf - θf and θf′ - θf′, as
well as the correlation between the two. Together, these allow us to approximate the distribution of
the overall estimation error δ - δ = (θf - θf′) - (θf - θf′), between the estimate δ and the truth
δ. With this, we can compute confidence intervals for δ, the true average effect of the intervention
on performance over seeds, and test hypotheses about δ, as well.
Design 3: Unpaired Samples. Finally, we might seek to compare a number of seeds for both the
intervention and baseline models, but may not expect them to be aligned in their dependence on the
seed. For example, the second model may use a different architecture so that they cannot be built
5
Published as a conference paper at ICLR 2022
from the same checkpoints, or the models may be generated from entirely separate initialization
schemes. We refer to such studies as unpaired. Like in a paired study, the Multi-Bootstrap allows
us to estimate the errors θf - θf and θf′ - θf′; however, in an unpaired study, we cannot estimate
the correlation between the errors. Thus, we assume that the correlation is zero. This will give a
conservative estimate of the error (θf - θf′) - (θf - θf′), as long as θf - θf and θf′ - θf′ are
not negatively correlated. Since there is little reason to believe that the random seeds used for two
different models would induce a negative correlation between the models’ performance, we take this
assumption to be relatively safe.
Hypothesis Testing. Given the measured uncertainty, we recommend testing whether or not the
difference is meaningfully different from some arbitrary predefined threshold (i.e., 0 in the typical
case). Specifically, we are often interested in rejecting the null hypothesis that the intervention does
not improve over the baseline model, i.e.,
H0 : δ ≤ 0	(1)
in a statistically rigorous way. This can be done with the Multi-Bootstrap procedure described below.
3.2 Multi-Bootstrap Procedure
The Multi-Bootstrap is a non-parametric bootstrapping procedure that allows us to estimate the dis-
tribution of the error θ - θ over the seeds and test instances. The algorithm supports both paired and
unpaired study designs, differentiating the two settings only in the way the sampling is performed.
To keep the presentation simple, we will assume that the performance L(s) is an average of a per-
example metric l(x, y, fs) over the distribution D of (X, Y), such as accuracy or the log likelihood,
and L(S) is similarly an empirical average with the observed n test examples,
nx
L(S)= ED [l(X,Y,fs)], and L⑸=_fl(Xi,Y"s)
nx i=1
We note that the mapping D → L(S) is linear in D, which is required for our result in Theorem 1.
However, we conjecture that this is an artifact of the proof; like most bootstrap methods, the method
here likely generalizes to any performance metric which behaves asymptotically like a linear map-
ping of D, including AUC, BLEU score (Papineni et al., 2002), and expected calibration error.
Building on the rich literature on bootstrap methods (e.g., Efron & Tibshirani, 1994), the Multi-
Bootstrap is a new procedure which accounts for the way that the combined randomness from the
seeds and test set creates error in the estimate θ. The statistical underpinnings of this approach have
theoretical and methodological connections to inference procedures for two-sample tests (Van der
Vaart, 2000), where the samples from each population are independent. However, in those settings,
the test statistics naturally differ as a result of the scientific question at hand.
In our procedure, we generate a bootstrap sample from the full sample with replacement sep-
arately over both the randomness from the pre-training seed S and from the test set (X, Y).
That is, We generate a sample of pre-training seeds (S；,SB,...,SnS) with each Sj drawn
randomly with replacement from the pre-training seeds, and we generate a test set sample
((X1j, Y1j), (X2j, Y2j), . . . , (Xnjx, Ynjx)) with each (X, Y) pair drawn randomly with replacement
from the full test set. Then, we compute the bootstrap estimate θj as
ns	nx
θ* =-3 L*(Sj), where L*(s)=£ Nl(X；,Y；,fs).
To illustrate the procedure, we present a minimal Python implementation in Appendix A. For suf-
ficiently large -x and -s, the distribution of the estimation error θ - θ is approximated well by the
distribution of θj - θ over re-draws of the bootstrap samples, as stated precisely in Theorem 1.
Theorem 1. Assume that E[12(X, Y, fs)] < ∞. Furthermore, assume that for each S,
E[12(X, Y, fs)] < ∞, and for almost every (x,y) Pair E[l2(X,Y,fs) | X = x, Y = y] < ∞.
Let-= -x +-s, and assume that 0 < ps = -s /-< 1 stays fixed (up to rounding error) as -→ ∞.
Then, there exists 0 < σ2 < ∞ such that √n(θ — θ) → G with G 〜N(0,σ2). Furthermore,
conditionally on ((X1,Y1), (X2,Y2),...), √n(θj — θ) → G.
6
Published as a conference paper at ICLR 2022
The proof of Theorem 1 is in Appendix B, along with a comment on the rate of convergence for the
approximation error. The challenge with applying existing theory to our method is that while the
seeds and data points are each marginally iid, the observed losses depend on both, and therefore are
not iid. Therefore, we need to handle this non-iid structure in our method and proof.
For nested sources of randomness (e.g., if for each pre-training seed s, we have estimates from multi-
ple fine-tuning seeds), we average over all of the inner samples (fine-tuning seeds) in every bootstrap
sample, motivated by Field & Welsh (2007)’s recommendations for bootstrapping clustered data.
Paired Samples (design 2, continued). In a paired design, the Multi-Bootstrap proce-
dure can additionally tell us the joint distribution of θf′ - θf′ and θf - θf. To do so,
one must use the same bootstrap samples of the seeds (S；,Sg,...,Sns) and test examples
((Xj,Y*), (Xg,γ*),..., (Xnχ,Yζ)) for both models. Then, the correlation between the errors
θf' - θf' and θf - θf is well approximated by the correlation between the bootstrap errors θf' - θf'
.^ ..
and θf - θf.
In particular, recall that we defined the difference in performance between the intervention f′ and
the baseline f to be δ, and defined its estimator to be δ. With the Multi-Bootstrap, we can estimate
the bootstrapped difference
^, ^, ^,
δ* = θf' - θf.
With this, the distribution of the estimation error δ - δ is well approximated by the distribution of
δ* - δ over bootstrap samples.
Unpaired Samples (design 3, continued). For studies that do not match the paired format, we
adapt the Multi-Bootstrap procedure so that, instead of sampling a single pre-training seed that is
shared between f and f′, we sample pre-training seeds for each one independently. The remainder
of the algorithm proceeds as in the paired case. Relative to the paired design discussed above, this
additionally assumes that the errors due to differences in pre-training seed between θf′ - θf′ and
θf - θf are independent.
Comparison to a Fixed Baseline (design 1, continued). Often, we do not have access to multiple
estimates of L(s), for example, when the baseline f against which we are comparing is an estimate
of human performance for which only mean accuracy was reported, or when f is the performance
of a previously-published model for which there only exists a single artifact or for which we do not
have direct access to model predictions. When we have only a point estimate θf = L(S1 ) of θf for
the baseline f with a single seed S1 , we recommend using Multi-Bootstrap to compute a confidence
interval around θf′ and reporting where the given estimate of baseline performance falls within
that distribution. An example of such a case is Figure 1, in which the distribution of MultiBerts
performance is compared to that from the single checkpoint of the original Bert release. In general
such results should be interpreted conservatively, as we cannot make any claims about the variance
of the baseline model.
Hypothesis Testing. A valid p-value for the hypothesis test described in Equation 1 is the fraction
.—.
of bootstrap samples from the above procedure for which the estimate δ is negative.
4	Application: Gender Bias in Coreference Systems
We present a case study to illustrate how MultiB erts and the Multi-Bootstrap can help us draw
more robust conclusions about model behavior.
The use case is based on gendered correlations. For a particular measure of gender bias, we take a
single BERT checkpoint and measure a value of 0.35. We then apply an intervention, foo, designed
to reduce this correlation, and measure 0.25. In an effort to do even better, we create a whole
new checkpoint by applying the foo procedure from the very beginning of pre-training. On this
checkpoint, we measure 0.3. How does one make sense of this result?
7
Published as a conference paper at ICLR 2022
Winogender bias correlation (r) by pretrain seed
1.0
Base
CDA-incr
0.8
■ ð
T⅛HT
.Tw
密.
..: 一
♦ ≡ ♦
÷♦T
TB+?
♦V占
♦To
♦
I □ ♦
♦ - ♦ S ♦
iφ
iπτ
..早
f D
φ≡
.!?♦ 」.
R^I
a .l□.
⅛l⅞
.64 2.0
6 6 6s
U) UO 4eαjb0uSe∞
'0	1	2	3	4	5	6	7	8	9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24
Pretraining Seed
Figure 3: Bias correlation on Winogender for each pre-training seed. Each box represents the distri-
bution of the score over five training runs of the coreference model. Dark boxes represent each base
MultiB erts checkpoint, while lighter boxes (CDA-incr) are the corresponding checkpoints after
50k steps of additional pretraining with CDA. Some seeds are better than others on this task (for
example, seed 23), but the CDA-incr consistently reduces the bias correlation for most seeds.
As a concrete example, we analyze gender bias in coreference systems (Rudinger et al., 2018) and
showing how MultiBerts and the Multi-Bootstrap can help us understand the effect of an interven-
tion, counterfactual data augmentation (CDA). We follow a set-up similar to Webster et al. (2020),
which augments the Bert pretraining data with counterfactual sentences created by randomly swap-
ping English binary-gendered pronouns. The goal is to weaken the correlation between gendered
pronouns and other words such as occupation terms (e.g., doctor, nurse). We compare our baseline
MultiB ERTs models to two strategies for CDA. In the first (CDA-incr), we continue pre-training
each MultiB erts model for an additional 50K steps on the counterfactual data of Webster et al.
(2020). In the second, we train BERT models from scratch (CDA-full) on the same dataset.
The Winogender dataset consists of template sentences covering 60 occupation terms and instan-
tiated with either male, female, or neutral pronouns. We follow Webster et al. (2020) and train a
gold-mention coreference system using a two-layer feedforward network that takes span representa-
tions from a frozen BERT encoder as input and makes binary predictions for mention-referent pairs.
The model is trained on OntoNotes (Hovy et al., 2006) and evaluated on the Winogender examples
for both per-sentence accuracy and a bias score, defined as the Pearson correlation between the per-
occupation bias score (Figure 4 of Rudinger et al. 2018) and the occupational gender statistics from
the U.S. Bureau of Labor Statistics.6 For each pre-training run, we train five coreference models,
using the same encoder but different random seeds to initialize the classifier weights and to shuffle
the training data.
4.1	Paired analysis: CDA-incr vs. Base
We investigate the impact of the intervention on performance and bias. Overall accuracy is fairly
consistent across pre-training seeds, at 62.6±1.2% for the base model, with only a small and not
statistically significant change under CDA-incr (Table 1). However, as shown in Figure 3, there
is considerable variation in bias correlation, with r values between 0.1 and 0.7 depending on pre-
training seed.7 The range for CDA-incr overlaps somewhat, with values between 0.0 and 0.4; how-
ever, because the incremental CDA is an intervention on each base checkpoint, we can look at the
individual seeds and see that in most cases there appears to be a significant improvement. A paired
Multi-Bootstrap allows us to quantify this and further account for noise due to the finite evaluation
6We use the occupation data as distributed with the Winogender dataset, https://github.com/
rudinger/winogender- schemas.
7Some of this variation is due to the classifier training, but on this task there is a large intrinsic contribution
from the pretraining seed. See Appendix D for a detailed analysis.
8
Published as a conference paper at ICLR 2022
sample of 60 occupations. The results are shown in Table 1, which show that CDA-incr significantly
reduces bias by δ = -0.162 with P = 0.001.
		Accuracy ∣ Bias Corr. (r)	
Base	θf θf′	0.626	0.423
CDA-incr		0.623	0.261
Avg. Diff.	δ= θf′ -θf	-0.004	-0.162
p-value		0.210	0.001
Table 1: Paired Multi-Bootstrap results for CDA intervention over the base MultiBerts checkpoints
on Winogender. Accuracy is computed by bootstrapping over all 720 examples, while for bias corre-
lation we first compute per-occupation bias scores and then bootstrap over the 60 occupation terms.
For both, we use 1,000 bootstrap samples. A lower value of r indicates less gender-occupation bias.
	Accuracy ∣ Bias Corr. (r)			Seeds	Examples
CDA-incr	θf	0.623	0.256	0.264	0.259
CDA-full	θf′	0.622	0.192	0.194	0.193
Avg. Diff.	δ = θf′ -θf	-0.001	-0.064	-0.070	-0.067
p-value		0.416	0.132	0.005	0.053
Table 2: Unpaired Multi-Bootstrap results comparing CDA-full to CDA-incr on Winogender. Ex-
amples are treated as in Figure 1. The “Seeds” column bootstraps only over pre-training seeds
while using the full set of 60 occupations, while the “Examples” column bootstraps over examples,
averaging over all pre-training seeds. For all tests we use 1,000 bootstrap samples.
4.2	Unpaired analysis: CDA-full vs. CDA-incr
We can also test if we get any additional benefit from running the entire pre-training with
counterfactually-augmented data. Similar to MultiBerts, we trained 25 CDA-full checkpoints for
2M steps on the CDA dataset.8 Because these are entirely new checkpoints, independent from the
base MultiB erts runs, we use an unpaired version of the Multi-Bootstrap, which uses the same set
of examples but samples pretraining seeds independently for CDA-incr and CDA-full. As shown
in Table 2, overall accuracy does not change appreciably (0.622 vs. 0.623, p = 0.416), while bias
correlation seems to decrease but not significantly (0.256 vs 0.192, δ = -0.064 with p = 0.132).
As an ablation, we also experiment with sampling over either only seeds (taking the set of examples,
i.e. occupations, as fixed), or over examples (taking the set of 25 seeds as fixed). As shown in
Table 2, we find lower p-values (0.005 and 0.053) in both cases—showing that failing to account for
finite samples along either dimension could lead to overconfident conclusions.
In Appendix E, we present two additional examples: a paired study where we increase pre-
training time from 1M to 2M steps, as well as an unpaired comparison to the original
bert-base-uncased checkpoint.
5	Conclusion
To make progress on language model pre-training, itis essential to distinguish between the properties
of specific model artifacts and those of the training procedures that generated them. To this end, we
have presented two resources: the MultiBerts, a set of 25 model checkpoints to support robust
research on Bert, and the Multi-Bootstrap, a non-parametric statistical method to estimate the
uncertainty of model comparisons across multiple training seeds. We demonstrated the utility of
these resources by showing how to quantify the effect of an intervention to reduce a type of gender
bias in coreference systems built on Bert. We hope that the release of multiple checkpoints and
the use of principled hypothesis testing will become standard practices in research on pre-trained
language models.
8Following Webster et al. (2020), we use 20 masks per sequence instead of the 80 from Devlin et al. (2019).
9
Published as a conference paper at ICLR 2022
References
Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew
Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath
Kudlur, Josh Levenberg, Dandelion Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris
Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker,
Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wat-
tenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learn-
ing on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software
available from tensorflow.org.
Vamsi Aribandi, Yi Tay, and Donald Metzler. How reliable are model diagnostics? In Findings of the
Associationfor Computational Linguistics: ACL-IJCNLP 2021, pp. 1778-1785, Online, August
2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.155. URL
https://aclanthology.org/2021.findings-acl.155.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. The
fifth PASCAL recognizing textual entailment challenge. In TAC. National Institute of Standards
and Technology, 2009.
Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein. An empirical investigation of statistical
significance in nlp. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural Language Learning, pp. 995-1005, 2012.
Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,
Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson,
Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel,
Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Ste-
fano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren
Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Pe-
ter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard,
Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte
Khani, Omar Khattab, Pang Wei Kohd, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya
Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li,
Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell,
Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie,
Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadim-
itriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob
Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Re,
Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin,
Rohan Taori, Armin W. Thomas, Florian Tramer, Rose E. Wang, WilliamWang, Bohan Wu, Jiajun
Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael
Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang.
On the opportunities and risks of foundation models, 2021.
Francesco Cantelli. Sulla probabilita` come limite della frequenza. Rendiconti della Reale Accademia
dei Lincei, 26(1):39-45, 1917.
Dallas Card, Peter Henderson, Urvashi Khandelwal, Robin Jia, Kyle Mahowald, and Dan Jurafsky.
With little power comes great responsibility. In Proceedings of the 2020 Conference on Em-
pirical Methods in Natural Language Processing (EMNLP), pp. 9263-9274, Online, November
2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.745. URL
https://aclanthology.org/2020.emnlp-main.745.
Daniel Cer, Mona Diab, Eneko Agirre, Ifiigo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task
1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of
the 11th International Workshop on Semantic Evaluation (SemEval-2017), pp. 1-14, Vancouver,
Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/S17-2001.
URL https://www.aclweb.org/anthology/S17- 2001.
10
Published as a conference paper at ICLR 2022
Zihan Chen, Hongbo Zhang, Xiaoji Zhang, and Leqi Zhao. Quora question pairs. University of
Waterloo, 2018.
Cheng-Han Chiang, Sung-Feng Huang, and Hung-yi Lee. Pretrained language model embryol-
ogy: The birth of ALBERT. In Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pp. 6813-6828, Online, November 2020. Associ-
ation for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.553. URL https:
//aclanthology.org/2020.emnlp-main.553.
William G Cochran. Sampling techniques. John Wiley & Sons, 2007.
Alexander D’Amour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi, Alex Beu-
tel, Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D Hoffman, et al. Under-
specification presents challenges for credibility in modern machine learning. arXiv preprint
arXiv:2011.03395, 2020.
Daniel Deutsch, Rotem Dror, and Dan Roth. A statistical analysis of summarization evaluation met-
rics using resampling methods. Transactions of the Association for Computational Linguistics, 9:
1132-1146, 2021. doi:10.1162/tacl_a_00417. URL https://aclanthology.org/2 021 .
tacl-1.67.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, Minneapolis, Minnesota, June
2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https:
//www.aclweb.org/anthology/N19-1423.
Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith.
Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping.
arXiv preprint arXiv:2002.06305, 2020.
William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.
In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. URL
https://www.aclweb.org/anthology/I05-5002.
Rotem Dror, Gili Baumer, Segev Shlomov, and Roi Reichart. The hitchhiker’s guide to testing
statistical significance in natural language processing. In Proceedings of the 56th Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1383-1392, 2018.
Rotem Dror, Segev Shlomov, and Roi Reichart. Deep dominance - how to properly compare deep
neural models. In Proceedings of the 57th Annual Meeting of the Association for Computa-
tional Linguistics, pp. 2773-2785, Florence, Italy, July 2019. Association for Computational Lin-
guistics. doi: 10.18653/v1/P19-1266. URL https://www.aclweb.org/anthology/
P19-1266.
Bradley Efron and Robert J Tibshirani. An introduction to the bootstrap. CRC Press, 1994.
M EmIle Borel. Les Probabilites denombrables et leurs applications arithmetiques. Rendiconti del
Circolo Matematico di Palermo (1884-1940), 27(1):247-271, 1909.
Nasrollah Etemadi. An elementary proof of the strong law of large numbers. Zeitschrift fur
Wahrscheinlichkeitstheorie und verwandte Gebiete, 55(1):119-122, 1981.
Christopher A Field and Alan H Welsh. Bootstrapping clustered data. Journal of the Royal Statistical
Society: Series B (Statistical Methodology), 69(3):369-390, 2007.
Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Investigating learning dynamics of BERT fine-tuning.
In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computa-
tional Linguistics and the 10th International Joint Conference on Natural Language Processing,
pp. 87-92, Suzhou, China, December 2020. Association for Computational Linguistics. URL
https://aclanthology.org/2020.aacl-main.11.
11
Published as a conference paper at ICLR 2022
John Hewitt and Christopher D. Manning. A structural probe for finding syntax in word representa-
tions. In Proceedings of the 2019 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Pa-
pers), pp. 4129-4138, Minneapolis, Minnesota, June 2019. Association for Computational Lin-
guistics. doi: 10.18653/v1/N19-1419. URL https://www.aclweb.org/anthology/
N19-1419.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. Ontonotes:
The 90% solution. In Proceedings of the Human Language Technology Conference of the NAACL,
Companion Volume: Short Papers, NAACL-Short ’06, pp. 57-60, Stroudsburg, PA, USA, 2006.
Association for Computational Linguistics. URL http://dl.acm.org/citation.cfm?
id=1614049.1614064.
Siddharth Karamcheti, Laurel Orr, Jason Bolton, Tianyi Zhang, Karan Goel, Avanika Narayan, Rishi
Bommasani, Deepak Narayanan, Tatsunori Hashimoto, Dan Jurafsky, Christopher D. Manning,
ChriStoPher Potts, Christopher Re, and Percy Liang. Mistral - ajourney towards reproducible lan-
guage model training, 2021. URL https://github.com/stanford-crfm/mistral.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Philipp Koehn. Statistical significance tests for machine translation evaluation. In Proceedings of
the 2004 conference on empirical methods in natural language processing, pp. 388-395, 2004.
Cheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang. Mixout: Effective regularization to finetune
large-scale pretrained language models. In International Conference on Learning Representa-
tions, 2020. URL https://openreview.net/forum?id=HkgaETNtDB.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019.
Zeyu Liu, Yizhong Wang, Jungo Kasai, Hannaneh Hajishirzi, and Noah A. Smith. Probing across
time: What does RoBERTa know and when? In Findings of the Association for Computational
Linguistics: EMNLP 2021, pp. 820-842, Punta Cana, Dominican Republic, November 2021.
Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.71. URL
https://aclanthology.org/2021.findings-emnlp.71.
Sasha Luccioni, Victor Schmidt, Alexandre Lacoste, and Thomas Dandres. Quantifying the car-
bon emissions of machine learning. In NeurIPS 2019 Workshop on Tackling Climate Change
with Machine Learning, 2019. URL https://www.climatechange.ai/papers/
neurips2019/22.
Nuno Luzia. A simple proof of the strong law of large numbers with rates. Bulletin of the Australian
Mathematical Society, 97(3):513-517, 2018.
R. Thomas McCoy, Junghyun Min, and Tal Linzen. BERTs of a feather do not generalize to-
gether: Large variability in generalization across models with similar test set performance. In
Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Net-
works for NLP, pp. 217-227, Online, November 2020. Association for Computational Lin-
guistics. doi: 10.18653/v1/2020.blackboxnlp-1.21. URL https://www.aclweb.org/
anthology/2020.blackboxnlp-1.21.
Tom McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic
heuristics in natural language inference. In Proceedings of the 57th Annual Meeting of the As-
sociation for Computational Linguistics, pp. 3428-3448, Florence, Italy, July 2019. Association
for Computational Linguistics. doi: 10.18653/v1/P19-1334. URL https://www.aclweb.
org/anthology/P19-1334.
Marius Mosbach, Maksym Andriushchenko, and Dietrich Klakow. On the stability of fine-
tuning BERT: Misconceptions, explanations, and strong baselines. In International Confer-
ence on Learning Representations, 2021. URL https://openreview.net/forum?id=
nzpLWnVAyah.
12
Published as a conference paper at ICLR 2022
Moin Nadeem, Anna Bethke, and Siva Reddy. StereoSet: Measuring stereotypical bias in pretrained
language models. In Proceedings of the 59th Annual Meeting of the Association for Computa-
tional Linguistics and the 11th International Joint Conference on Natural Language Processing
(Volume 1: Long Papers), pp. 5356-5371, Online, August 2021. Association for Computational
Linguistics. doi: 10.18653/v1/2021.acl-long.416. URL https://aclanthology.org/
2021.acl-long.416.
Aakanksha Naik, Abhilasha Ravichander, Norman Sadeh, Carolyn Rose, and Graham Neubig.
Stress test evaluation for natural language inference. In Proceedings of the 27th International
Conference on Computational Linguistics, pp. 2340-2353, Santa Fe, New Mexico, USA, Au-
gust 2018. Association for Computational Linguistics. URL https://www.aclweb.org/
anthology/C18-1198.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pp. 311-318, Philadelphia, Pennsylvania, USA, July 2002.
Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https:
//aclanthology.org/P02-1040.
David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild,
David So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training. arXiv
preprint arXiv:2104.10350, 2021.
Fabio Petroni, Tim Rocktaschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and
Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference
on Empirical Methods in Natural Language Processing and the 9th International Joint Confer-
ence on Natural Language Processing (EMNLP-IJCNLP), pp. 2463-2473, Hong Kong, China,
November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1250. URL
https://www.aclweb.org/anthology/D19-1250.
Maxime Peyrard, Wei Zhao, Steffen Eger, and Robert West. Better than average: Paired evaluation
of NLP systems. In Proceedings of the 59th Annual Meeting of the Association for Computa-
tional Linguistics and the 11th International Joint Conference on Natural Language Processing
(Volume 1: Long Papers), pp. 2301-2315, Online, August 2021. Association for Computational
Linguistics. doi: 10.18653/v1/2021.acl-long.179. URL https://aclanthology.org/
2021.acl-long.179.
Jason Phang, Thibault Fevry, and Samuel R Bowman. Sentence encoders on stilts: Supplementary
training on intermediate labeled-data tasks. arXiv preprint arXiv:1811.01088, 2018.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions
for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pp. 2383-2392, Austin, Texas, November 2016. Association
for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://www.aclweb.
org/anthology/D16-1264.
Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable
questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers), pp. 784-789, Melbourne, Australia, July
2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2124. URL https:
//aclanthology.org/P18-2124.
TA Ramasubban. The mean difference and the mean deviation of some discontinuous distributions.
Biometrika, 45(3/4):549-556, 1958.
Eric Rieders. Marcinkiewicz-type strong laws for partially exchangeable arrays. Journal of Multi-
variate Analysis, 38(1):114-140, 1991.
Anna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in BERTology: What we know about
how BERT works. Transactions of the Association for Computational Linguistics, 8:842-866,
2020. doi: 10.1162/tacl_a_00349. URL https://www.aclweb.org/anthology/2 02 0 .
tacl-1.54.
13
Published as a conference paper at ICLR 2022
Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. Gender bias in
coreference resolution. In Proceedings of the 2018 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies, New Orleans,
Louisiana, June 2018. Association for Computational Linguistics.
Naomi Saphra and Adam Lopez. Understanding learning dynamics of language models with
SVCCA. In Proceedings of the 2019 Conference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short
Papers), pp. 3257-3267, Minneapolis, Minnesota, June 2019. Association for Computational Lin-
guistics. doi: 10.18653/v1/N19-1329. URL https://aclanthology.org/N19-1329.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng,
and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language
Processing, pp. 1631-1642, Seattle, Washington, USA, October 2013. Association for Computa-
tional Linguistics. URL https://www.aclweb.org/anthology/D13-1170.
AnderS S0gaard, AnderS Johannsen, Barbara Plank, Dirk Hovy, and Hector Martinez Alonso.
What’s in a p-value in NLP? In Proceedings of the Eighteenth Conference on Computational
Natural Language Learning, pp. 1-10, Ann Arbor, Michigan, June 2014. Association for Com-
putational Linguistics. doi: 10.3115/v1/W14-1601. URL https://aclanthology.org/
W14-1601.
Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for
deep learning in NLP. In Proceedings of the 57th Annual Meeting of the Association for Com-
putational Linguistics, pp. 3645-3650, Florence, Italy, July 2019. Association for Computational
Linguistics. doi: 10.18653/v1/P19-1355. URL https://www.aclweb.org/anthology/
P19-1355.
Ian Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline. In
Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp.
4593-4601, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/
v1/P19-1452. URL https://www.aclweb.org/anthology/P19-1452.
Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better:
On the importance of pre-training compact models. arXiv preprint arXiv:1908.08962, 2019.
Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge University Press, 2000.
Aad W van der Vaart, Aad van der Vaart, Adrianus Willem van der Vaart, and Jon Wellner. Weak
convergence and empirical processes: with applications to statistics. Springer Science & Busi-
ness Media, 1996.
Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head
self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the
57th Annual Meeting of the Association for Computational Linguistics, pp. 5797-5808, Florence,
Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1580. URL
https://www.aclweb.org/anthology/P19-1580.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding. In
Proceedings of International Conference on Learning Representations, 2019.
Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments.
Transactions of the Association for Computational Linguistics, 7:625-641, March 2019. doi:
10.1162/tacl_a_00290. URL https://aclanthology.org/Q19-1040.
Kellie Webster, Xuezhi Wang, Ian Tenney, Alex Beutel, Emily Pitler, Ellie Pavlick, Jilin Chen,
Ed Chi, and Slav Petrov. Measuring and reducing gendered correlations in pre-trained models.
arXiv preprint arXiv:2010.06032, 2020.
14
Published as a conference paper at ICLR 2022
Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technolo-
gies, Volume 1 (Long Papers), pp. 1112-1122, New Orleans, Louisiana, June 2018. Association
for Computational Linguistics. doi: 10.18653/v1/N18-1101. URL https://www.aclweb.
org/anthology/N18-1101.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick
von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gug-
ger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art
natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations, pp. 38-45, Online, October 2020. As-
sociation for Computational Linguistics. URL https://www.aclweb.org/anthology/
2020.emnlp- demos.6.
Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q Weinberger, and Yoav Artzi. Revisiting few-
sample BERT fine-tuning. In International Conference on Learning Representations, 2021. URL
https://openreview.net/forum?id=cO1IH43yUF.
Zhuosheng Zhang, Yuwei Wu, Hai Zhao, Zuchao Li, Shuailiang Zhang, Xi Zhou, and Xiang Zhou.
Semantics-aware BERT for language understanding. Proceedings of the AAAI Conference on
Artificial Intelligence, 34(05):9628-9635, Apr. 2020. doi: 10.1609/aaai.v34i05.6510. URL
https://ojs.aaai.org/index.php/AAAI/article/view/6510.
Ruiqi Zhong, Dhruba Ghosh, Dan Klein, and Jacob Steinhardt. Are larger pretrained language
models uniformly better? comparing performance at the instance level. In Findings of the As-
sociation for Computational Linguistics: ACL-IJCNLP 2021, pp. 3813-3827, Online, August
2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.334. URL
https://aclanthology.org/2021.findings-acl.334.
Xiang Zhou, Yixin Nie, Hao Tan, and Mohit Bansal. The curse of performance instability in analysis
datasets: Consequences, source, and suggestions. In Proceedings of the 2020 Conference on Em-
pirical Methods in Natural Language Processing (EMNLP), pp. 8215-8228, Online, November
2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.659. URL
https://aclanthology.org/2020.emnlp-main.659.
Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning books
and movies: Towards story-like visual explanations by watching movies and reading books. In
2015 IEEE International Conference on Computer Vision (ICCV), pp. 19-27, Dec 2015. doi:
10.1109/ICCV.2015.11.
15
Published as a conference paper at ICLR 2022
A Minimal Implementation of the Multi-Bootstrap
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
Below, we present a simplified Python implementation of the Multi-Bootstrap algorithm presented
in Section 3.2. It describes a single-sided version of the procedure, which could be used, e.g., to test
that a model’s performance is greater than 0. The input is a matrix of predictions where row indices
correspond to test examples and column indices to random seeds. The functions returns an array of
nboot samples [θ1, . . . , θnboot].
def multibootstrap(predictions, labels, metric_fun, nboot):
"""
Generates bootstrap samples of a model's performance.
Input:
predictions: 2D NumPy array with the predictions for different seeds.
labels: 1D NumPy array with the labels.
metric_fun: Python function. Takes a pair of arrays as input, and
returns a metric or loss.
nboot: Number of bootstrap samples to generate.
Output:
Numpy array with nboot samples.
# Checks the data format.
n_samples, n_seeds = predictions.shape
assert labels.shape == (n_samples,)
thetas = np.zeros(nboot)
for boot_ix in range(nboot):
#	Samples n_samples test examples and n_seeds pre-training seeds.
x_samples = np.random.choice(n_samples, Size=n_samples, replace=True)
s_samples = np.random.choice(n_seeds, Size=n_seeds, replace=True)
#	Computes the metric over the bootstrapping samples.
sampled_predictions = PrediCtions[np.ix_(x_samples, s_samples)]
sampled_labels = labels[x_samples]
sampled_metrics =[
metric fun(sampled_PrediCtions[:j], sampled_labels)
for j in range(n_seeds)
]
#	Averages over the random seeds.
thetas[boot_ix] = np.mean(sampled_metrics)
return thetas
We provide the complete version of the algorithm on our repository http://goo.gle/
multiberts. Our implementation is optimized and supports all the experiment designs described
in Section 3, including paired and unpaired analysis as well as multiple fine-tuning runs for each
pretraining seed.
16
Published as a conference paper at ICLR 2022
B Proof of Theorem 1
Before giving the proof, we define some useful notation that will simplify the argument considerably.
We let Dn be the empirical measure over the nx observations (Zi = (Xi, Yi))in=1, and Mn be the
empirical measure over the ns observations (Sj)jn=1. For a function f : V → R and a distribution
P over V , we will use the shorthand Pf to denote the expectation of f under P,
Pf = EV 〜p[f(V)].
For example, this allows us to write
θ = DMI = EZ〜DES〜MI(Z, fS),
nx	ns
1 x1 s
and θ = DnMnI = — V2 — 52 l(Zi, fSj).
nx	ns	j
x i=1 s j=1
For the bootstrapped distributions, let Dn denote the distribution over the bootstrap data sam-
ples (Zι, Z2,..., Znχ) and Mn denote the distribution over the bootstrapped seed samples,
(Si, S，2,..., Sns), both conditional on the observed samples (Zi)nx1 and (Sj)n= 1. Note that the
empirical average over a bootstrapped sample
nx	ns
n Σ n Σ l(Zi*,f⅛)
nx	ns
x i=1 s j =1
can be written as
nx	ns
ɪzɪrAiBj 1( Zi ,fSj),
nx i=1 ns j =1
where Ai is the number of times Zi appears in the bootstrapped sample (Zki )kn=x 1, and Bj is the
number of times Sj appears in the bootstrapped sample (Ski )kn=s 1 . With this in mind, we will abuse
notation, and also denote Dni as the distribution over the Ai and Mni as the distribution over the Bj .
Finally, we will use Ei and Vari to denote the expectation and variance of random variables defined
with respect to Dni or Mni, conditional on Dn and Mn.
We will use P to denote the distribution P = D × M. Throughout, all assertions made with respect
to random variables made without a note about their probability of occurrence hold P -almost surely.
Proof. The challenge with applying existing theory to our method is that because the performance
metric (l(Zi, fsj∙ )n=ι over the n observations for a given seed Sj all depend on the same Sj, they
are not independent. Similarly for the performance on a given observation, over seeds. Therefore,
we need to handle this non-iid structure in our proof for the multi-bootstrap.
There are conceptually three steps to our proof that allow us to do just that. The first is to show that
θ has an asymptotically linear representation as
√n(- θ) = √n(Dn - D)Ml + √n(Mn - M)Dl + op(1).	(2)
The second is to show that conditional on Dn and Mn the multi-bootstrapped statistic θi = DnMnαl
has an asymptotically linear representation as
W - θ) = √n(Dn - Dn)mi+Vn(Mn - Mn)DI+吁 ⑴，	⑶
where Dn and Mn are multiplier bootstrap samples coupled to the bootstrap Dn and Mn which We
define formally in the beginning of Step 2. The third step is to use standard results for the multiplier
bootstrap of the mean of iid data to show that the distributions of the above linearized statistics
converge to the same limit.
Because we have assumed that l(Z,fs) < ∞, E[l(Z,fs) | S] < ∞, and E[l(Z,fs) | Z] < ∞,
Fubini’s theorem allows us to switch the order of integration over Z and S as needed.
We will assume that DMl(X, Y, fs) = 0. This is without loss of generality, because adding and
subtracting √nDMl to the bootstrap expression gives
√n(θ* - θ) = √n(DnMni - DnMnI)
=√n(DnMni - DMi + DMi - DnMni)
=√n(DnMn(i - DMe)- DnMn(i - DMi)),
17
Published as a conference paper at ICLR 2022
so if we prove that the result holds with the mean zero assumption, it will imply that the result holds
for I with a nonzero mean.
This theorem guarantees consistency of the Multi-Bootstrap estimates. One question that comes
up is whether it is possible to get meaningful / tight rates of convergence for the approximation.
Unfortunately, getting OP (1/n) convergence as found in many bootstrap methods (Van der Vaart,
2000) is difficult without the use of Edgeworth expansions, by which the Multi-Bootstrap is not well-
adapted to analysis. That said, many of the remainder terms already have variance of order O(1∕n),
or could easily be adapted to the same, suggesting an OP(1∕√n) convergence. The main difficulty,
however, is showing rates of convergence for the strong law on separately exchangeable arrays (see
the proof of Lemmas 2, 4-5). Showing a weaker notion of convergence, such as in probability,
may perhaps allow one to show that the remainder is OP(1∕√n), however the adaptation of the
aforementioned Lemmas is nontrivial.
一 一 一一.一 ^ δ _ _ _ . 一一 δ _ I^	一、 一 一一
Step 1 Recalling that θ = DnMnl and θ = DMl, we can expand √n(θ - θ) as follows,
√n(DnMnl - DMe) = √n(DnMnl - DMnl + DMnl - DMl)
=√n((Dn - D)MnI + D(Mn - M)l)
=√n((Dn - D)Mnl + (Dn - D)Ml - (Dn - D)Ml + D(Mn - M)l)
=√n((Dn - D)Ml + (Dn - D)(Mn - M)l + D(Mn - M)l)
The following lemma shows that √n(Dn - D)(Mn - M)l is a lower order term.
Lemma 1. Under the assumptions ofTheorem 1, √n(Dn — D)(Mn — M)l = op(1).
Therefore,
√n(DnMnI- DMI) = /	√nX(Dn - D)MI +-----^√nj(Mn - M)DI + op ⑴.
√1 — PS	√ps
Step 2 One of the challenges with working with the bootstrap sample Dn and Mn is that the
induced per-sample weights {Ai }in=x1 and {Bj }jn=s 1 do not have independent components, because
they each follow a multinomial distribution over nx items and ns items, respectively. However, they
are close enough to independent that we can define a coupled set of random variables {A◦}n=ι and
{ Bj }n= ι that do have independent components, but behave similarly enough to {Ai} and { Bj } that
using these weights has a negligible effect on distribution of the bootstrapped estimator, as described
concretely below.
First, we discuss the coupled multiplier bootstrap sample Dn and Mn. The creation of this sequence,
called “Poissonization” is a standard technique for proving results about the empirical bootstrap that
require independence of the bootstrap weights (van der Vaart et al., 1996). We describe this for Dn
as the idea is identical for Mn. Because our goal is to couple this distribution to Dn, we define it on
the same sample space, and extend the distribution P*, expectation E* and variance Var* to be over
Dn and Mn, conditionally on Dn and Mn, as with Dn and Mn.
To construct the distribution Dn, from the empirical distribution Dn and a bootstrap sample D"
start with the distribution Dn* and modify it as follows: We draw a Poisson random variable Nnx
with mean nx. If Nnx > nx, then we sample Nnx -nxiid observations from Dn, with replacement,
and add them to the bootstrap sample initialized with Dn to produce the distribution Dn. If Nnx <
nx, we sample nx- Nnx observations from Dn* , without replacement, and remove them from the
bootstrap sample to produce the distribution Dn. If Nnx = nχ, then Dn = Dn. Recalling that
Ai is the number of times the i-th sample is included in Dn, similarly define Aj as the number of
times the i-th sample is included in Dn. Note that by the properties of the Poisson distribution,
Aj 〜Poisson(I), and {Aj}rn=Xι are independent. Note that the natural normalization for Dn would
be Nnx . However, it will be useful to maintain the normalization by nχ, so abusing notation, for a
function f (z), we will say that Dnf = nχ EnxI Ajf (Zi).
18
Published as a conference paper at ICLR 2022
Define θ as the following empirical estimator of θ under the distribution Dn X Mn,
nx	ns
1	x1 s
θ = DnMnI = n- E n EAiBjI(Zi /“
nx i=1 ns j=1
Lemma 2 shows that √n(θ* 一 θj) = op* (1), and so √n(θ* 一 θ) = √n(θj 一 θ) + op * (1).
Lemma 2. Under the assumptions ofTheorem 1, and that DMl = 0, √n(θ* - θj) = op* (1).
With this, the expansion of √n(θj - θ) begins mutatis mutandis the same as in Step 1, to get that
I—	,^	^	1	_____ I—.一	..一
√n(θ 一 θ) = ∕ι ∙√nX(Dn - Dn)MnI + √n(Dn - Dn)(Mn 一 Mn)I
1	一 ps n	n	n
+--彳√nS(Mn - Mn)Dnl∙
√pS
As with Step 1, We provide Lemma 3 showing that the remainder term √n(Dn - Dn)(Mn - Mn)l
will be lower order.
Lemma 3. Under the assumptions ofTheorem 1, √n(Dn - Dn)(Mn - Mn)l = op * (1).
Therefore,
E MnI-DnMnI) = √⅛ E(Dn-Dn)Mnl +	E(Mn - Mn)DnI + oP* ⑴.
一	.	----^, O. .	______ _、一 C	一 .	—	—	.	一 _______
Then, to write √n(θ*-θ) in terms of √nS(Mn-Mn)DI as wanted in Eq. (3), instead of √nS(Mnj -
Mn)Dnl, we must additionally show that the functional has enough continuity that the error term
√nS(Mnj - Mn)(Dn - D)l is lower order. The following lemma shows exactly this.
Lemma 4. Under the assumptions of Theorem 1, conditionally on the sequences Z1, Z2, . . . and
S1,S2,...,
(a)	√n(Dn - Dn)(Mn - M )l = op * (1), and
(b)	√n(Dn - D)(Mn - Mn)l = op* (1).
Altogether, these imply that
√n(DnMnI- DnMnI) = J——E(Dn - Dn)MI + ɪEMn - Mn)Dl + op* (1).
√1 — Ps	√pS
□
Step 3 Noting that Ml(∙, fs) = Ed×m[l(∙, fs) | Z = ∙] is a real-valued random variable with
finite variance (similarly for Dl(Z, ∙)), and recalling that the nx samples used for Dn and n§ samples
for Mn satisfy n = nx/(1 - Ps) and n = ns/Ps, for 0 < Ps < 1, the conventional central limit
theorem shows that for some positive semi-definite matrix Σ ∈ R2×2, and G 〜N(0, Σ),
厂((Dn - D)Ml ∖ _ 1 i-p^√nX(Dn - D)MI
Vnl (Mn - M)Dl ) = I Ps√nS(Mn - M)Dl
Note that Dn and Mn are independent, so G is, in fact, a diagonal matrix.
→d G.
Additionally, the conditional multiplier CLT (van der Vaart et al., 1996, Lemma 2.9.5, pg. 181)
implies that conditionally on Z1 , Z2 , . . . and S1 , S2 , . . . ,
N ((Dn - Dn)Ml ∖ & G
Vn 卜(Mn- Mn)Dl ) → G.
Finally, applying the delta method (see Theorem 23.5 from Van der Vaart (2000)) along with the
_____________________________________________________________.—. ___________.—.	.—.
results from Steps 1 and 2 shows that the distributions of √n(θ - θ) and √n(θ* - θ) converge to
N(0,σ2), where σ2 = ∑ιι∕(1 - PS) + ∑22∕Ps.
19
Published as a conference paper at ICLR 2022
B .1 Proof OF Lemma 1
Fix e > 0. Note that E[(Dn - D)(Mn — M)1] = 0, so by Chebyshev,s inequality,
P (|√n(Dn - D)(Mn - M)l| > e) ≤
Var(v⅛(Dn - D)(Mn - M)1)
e2
Therefore, it suffices to show that limn→∞ Var(√n(Dn - D)(Mn - M)1) = 0. To do so, we apply
the law of total variance, conditioning on Dn, and bound the resulting expression by C/n.
Var(√n(Dn - D)(Mn - M)1) = nE[Var((Dn - D)(Mn - M)1 ∣ Dn)]
+ nVar(E[(Dn - D)(Mn - M)1 ∣ Dn])
=nE[Var((Dn - D)(Mn - M)1 ∣ Dn)]
=nE[Var((Mn - M)(Dn - D)l ∣ Dn)]
E
E
ns
n E^ Var((Dn - D)l3 fSj ) ∣ Dn)
S j = 1
n
一Var((Dn - D)l(∙,fSι ) ∣ Dn)
nS	i
2
E
PSEM . E I(ZifSi)- E[l(Zi,fSι) ∣ S1] I ∣{Zi}n:
E
1 / 1 N .	一. 「一
—I — l(Zi，fSI)- E[l(Zi, fSI) ∣ S1]
PS \nx£
2
E
1 nx nx
---2 EE(I(Zi, fSi ) - E[l(Zi,fSI) ∣ SID(I(Zk,fSι)-
PSnXi=I k=1
E
E[l(Zk,fSi) ∣ Si])
] nx
---2 E(I(Zi, fSι) - E[l(ZijSI) ∣ S1])2
PSnXi=I
PSo⅛n E …I)- E"S])2]
≤ C → 0,
n
B .2 PROOF OF LEMMA 2
First, note the following representation for θ* - θo:
]_nx [ ns	[ n^	JnS
a - a = ~ E ~ E AiBjI(Zi ,fSj)- - E ~ E AOBOI(Zi,fSj)
X i=1 S j = 1	X i=1 S j=1
1 ns
£Σ
ns ∙	1
j=i
、
◦ D	Do、 nχ	1 nχ
(B-B^E AMZif+nχE
-----------Z 、---
^ʃi
nχ
「A) E Bj "i,fSj),
nS
j=i
^^∙Z"
δI2
J
Let e > 0. Noting that E* [I1] = E* [I2] = 0, applying Chebyshev,s inequality gives
P * (√n∣m - θ°∖ > e) ≤ n
Var*(θ* - θ°)
e2
≤ 2n
Var*(∕1) + Var*(∕2)
e2
It suffices to show that nVar*(I1) → 0 and nVar*(I^2) → 0. The arguments for each term are
mutatis mutandis the same, and so we proceed by showing the proof for I2.
By the law of total variance,
Var*(I2) = Var*(E*[I2 ∣{Bj >n= 1])+ E*[Var*(I2 ∣{Bj >n= 1)].
20
Published as a conference paper at ICLR 2022
Because E*[Ai] = E*[Af] and {Bj}；= 1 ⊥ Ai,Af, it follows that E*[I2 | {Bj}；= / = 0. Taking
the remaining term and re-organizing the sums in I2,
Var*(I2) = E*
nx
Var* 一 £(Ai- A°]
、nx i=1
ns
n^EBj l(Zi,fSj)
ns j=1
| {Bj}jn=s 1
(4)
Next, We apply the law of total variance again, conditioning on Nnx = Ei A，. First,
nx	ns
E*[I2 I Nnx ,{Bj }n= l] = Nnχ-nx-E - EBj l(Zi, fSj ),
nx	nx	ns
x x i=1 s j=1
and so
2
1	1 nx 1 ns
Var* (E*[I2 I Nnx ,{Bj j 1] I {Bj j 1) = nx I nnχ∑ ns^ Bj l(Zi,fSj )1
Then, conditionally on Nnx (and {Bj}), I2 is the (centered) empirical average of INn - nI samples
from a finite population of size n, rescaled by INn - nI/n. Therefore, applying Theorem 2.2 of
Cochran (2007) gives the conditional variance as
(
1
nχ - 1
∖
、
nx
E
i=1
1 ns
nf Bj l(Zi,fSj)
ns	1
nx
nx
-1
nx	ns
n； C ns CBj l(zi,fSj)
x i=1 s j=1
^^^™
=∆V2 * *
j
2
	
To take the expectation over Nnx, notice that because E* [Nnx] = nx, this is the mean absolute de-
viation (MAD) of Nnx . Using the expression for the MAD ofa Poisson variable from Ramasubban
(1958) gives
E* INnx - nxI = 2nx nnx exp(-nx),
x nx!
and using Stirling,s approximation, this is bounded by C√nx, for some 0 < C < ∞.
Combining this with the above term for the variance of the conditional expectation, we have
/
nx
ns
Var*
TE(Ai-Ar) -EBjι(Zi,fsj) ∣{Bj}n= 1
nx i=1	ns j=1
1
≤——
nx
nx	ns
nx∑ ns CBjI(Zi,fSj)
x i=1 s j=1
1
1
2
+卡V 2
(5)
Noting that E*[Bj2] = E*[BjBk] = 1, we get the following bound:
1 / 1 N 1 N ∖	1 一
Var*(l2) ≤ nxx IJ nsEl(Zi,fsj) I + KV2
nx nx i=1 ns j=1	nx
where
2
V2
nx
,E
nx -1 =
ns
-∑jι(Zi,fSj)
s
s j=1
nx
nx - 1
nx	ns
:E n e i(Zi,fSj)
nn
x i=1 s j=1
	
2
Because of the assumption that DMl = 0, the SLLN adapted to separately exchangeable arrays
(Rieders, 1991, Theorem 1.4) implies that
1 nx 1 ns
lim — E — EI(Zi, fSj) = 0,
n→∞ nx	ns
x i=1 s j=1
21
Published as a conference paper at ICLR 2022
almost surely. Therefore, the first term of (5) is o(1∕n).
Note that V2 is the empirical variance of the conditional expectation of Q(Zi, fsj) given {Zi}f=1.
Therefore, the law of total variance shows that
T Tnx ns
V2 ≤ ɪɪ EE ι2(Zi,fSj)-
nτ ns z—z z—z	J
x s i=1j=1
一 nx ns	∖2
n n E jg I(Zi T
By the SLLN adapted to separately exchangeable arrays (Rieders, 1991, Theorem 1.4), both of the
terms converge almost surely to DMl2 < ∞ and (DMI)I2, respectively. and therefore,
lim nVar*(∕s) ≤ lim ——
n→∞	n→∞ nχ
Inxl ns	∖2
nxE VSE I(Zifj + ⅛ V 2
0.
B.3 PROOF OF LEMMA 3
As with Lemma 1, the main idea of the proof is to apply Chebyshev,s inequality, and show that the
variance tends to zero. Indeed, choosing an arbitrary e > 0,
P*(| A(DIn - Dn)(Mn - Mn)l∣
≥e) ≤
Var* (Si(Dn - Dn)(Mn - Mn))
T2
Therefore, it suffices to show that the variance in the above display goes to zero. To do this, we start
by re-writing the expression in terms of A° and Bj, and then apply the law of total variance.
(1	nx ns
—EE(Aj - 1)(Bj - 1)l(Zi, fsj)
nxns i=1j=1
nVar*
+ nE*
f
E*
∖
Var*
nx ns
—∑∑(A- - 1)(Bj - 1)l(Zi,fsj) ∣ {Aj}*ι
/ ljrr∙ Iba
X S i=1j=1
nx ns
—T. E(Aj -1)(Bj -ι)1(Zif) ∣{Aj}n:ι
X S i=1j=1
Because {Bj}n= 1 are independent of {Aj}n=ι, and have mean 1, the conditional expectation in the
first term is 0 almost surely. Expanding out the second term, using that Var* (Bj) = 1, and that the
{Bj}n= ι are uncorrelated,
nE*
(1	nx ns
W >Aj-I)(Bj - I)I(Zi川|{Ai}n=i
nE*
ns	nx
滔 EVar* (Bj - 1) TE(Aj- 1)1(Zi,fsj) ∣{Aj}n:ι
nS j = 1	∖	nx i=1
2
nE*
nE*
ns	nx
R E 卜 E (Aj - I)I(Zi,fSj)
ns	nx nx
3 E j2 EE(Aj
ns j=ι nx i=ι k=1
-1)(Ak - 1)l(Zi,fsj )l(Zk,fsj).
22
Published as a conference paper at ICLR 2022
Now, noting that Var*(A;) = 1, and that the {Af}n=ι are uncorrelated, this simplifies to
nE*
ns	nx
n Σ n Σ(Ai- 1)l IZiil fSj)
s j=1 x i=1
ns nx
ɪ ɪ E ɪ E "fsi
nsnx ns j =1 nx i=1	j
Because Ed×m[l2(Z, fs)] < ∞, the SLLN adapted to separately exchangeable arrays (Rieders,
1991, Theorem 1.4) implies that this converges almost surely to 0.
B.4 Proof of Lemma 4
We prove (a) of the Lemma, as (b) follows from applying Fubini’s theorem and following mutatis
mutandis the same argument. Without loss of generality, we will assume that l(Zi, fsj) ≥ 0.
Because Var(I(Zi, fsj)) < ∞, we can always decompose l(∙, ∙) into a positive and negative part,
and show that the result holds for each individually.
Once again, we prove (a) by turning to Chebyshev,s inequality. Fix e > 0, and observe that
/ L	V	Var* (√n(D2 - Dn)(Mn - M))
P * (∣√n(Dn - Dn)(Mn - M )l| > e) ≤ ——n &	——LL,
so it is sufficient to show that Var* (√n(Dn - Dn)(Mn - M)) → 0.
Writing the above in terms of A；, we have
Var* (√n(Dn - Dn)(Mn - M))
/
Var*
乎 E(A； - 1) I n1 E l(ZiIfSj)- E[l(Zi, fSj) | Zi]
x i=1	s j =1
∖
nx
1 ns	∖
一ɪ^1(ZiI fSj ) - E[l(ZiI fSj ) | Zi] I
ns j =1
1 ns	∖2
一£JI(ZiIfSj ) - E[l(Zi, fSj ) | Zi] I .
ns j=1
Now, we want to show that the last display converges almost surely to 0. Notice that each term within
the outer sum will obviously converge due to the SLLN. Showing that the outer sum also converges
almost surely is technically difficult, but conceptually follows the same argument used to prove the
SLLN (specifically, we follow the one done elegantly by Etemadi (1981); Luzia (2018) provides a
more detailed account of this proof technique that is helpful for developing a deeper understanding).
We show the following version of almost sure convergence: that for any e > 0,
/
P
∖
nx ( 1 ns	∖ 2	ʌ
-⅞Σ [-E1(ZifSj)- Ell(Zi,fSj) | Sj ]	>e i∙o.∣ =0,
x i=1 s j =1
where i.o. stands for infinitely often.
Define the shorthand Lij = l(Zi, fSj∙) and let Lj = Lj1{Lj < ij} be a truncated version of
Lij. The proof of Theorem 2 of Etemadi (1981) implies that P(Lij = Lij i.o.) = 0, because the
assumption Var(Lij) < ∞ implies the assumption used in Etemadi (1981), and independence of
{Lij }i,j is not needed for this result. Therefore,
22
1	nx 1 ns	1 nx 1 ns
nx E (ns E Lij-LijJ → 0∣and nχ E (ns E E[Lj | Zi] - EiLj | & J → 0∙
23
Published as a conference paper at ICLR 2022
Together, these imply that if we can prove that the truncated sum converges, ie.,
1 n / 1 ns	∖ 2
n £ (n C Lij-E[Lijι Zi] I →1. 0,	⑹
x i=1	s j=1
this is sufficient to show that the un-truncated version converges almost surely.
To prove (6), we show two things: first, that there is a subsequence kn such that (6) holds when
restricted to the subsequence, and then we show that the sequence is a Cauchy sequence, which
together imply the result.
Let α > 1 and let kn = αn . For convenience, denote knx as the number of data samples and kns as
the number of seed samples when knx + kns = kn total samples are drawn. We will ignore integer
rounding issues, and assume knx = (1 - ps)αn, and kns = psαn.
The following lemma shows that the subsequence defined by kn converges almost surely.
Lemma 5. Let α > 1, and kn = αn. Under the assumptions of Theorem 1 and that Lij ≥ 0
£ Lij- E[L ij | Zi ]) >e i.o. j =0.
k
nx
nx ns
i=1
P
We now must show that the sequence in (6) is a Cauchy sequence. Note that the SLLN implies that
1 nx
—E E[Lij | Zi] → E[E[Lij | Zi] ],
nx i=1
and the LLN for exchangeable arrays (Rieders, 1991, Theorem 1.4) implies that
1	nx 1 ns
—E — E LijE[Lij | Zi] a→. E[E[LijI Zi]2].
nx i=1 ns j=1
Therefore,
2
nx ns
knx ( kns	∖
LLij	a→.s. E[E[LLij | Zi]2].
i=1 j=1
(7)
1
Notice that because Lij ≥ 0, the sum En=I (En= 1 Lij) is monotone increasing in ns and nx.
With this in mind, for any m > 0, let nbe such that kn ≤ m < kn+1 . Then, by the montonicity,
kn	1	3 knx kns L 2
(Ekn)	⅛ (jζ ij)	≤
^(1-ps)m 1gpsm L ʌ 2
i=1	j=1 Lij
ps2 (1 - ps )m3
3 k(n+1)x k(n+1)s
≤I占)E (ELj
2
From (7), the left hand side converges to α3E[E[Lj | Zi]2], and the right hand side converges to
a3E[E[Lj | Zi]2]. Because α is arbitrary, this proves that the sequence
^^(1-Ps)m (yPsm
2=1=1	〈乙 j = 1
2
ps2 (1 - ps )m3
∖
m=1,...
is almost surely Cauchy. Together with Lemma 5, this implies (6).
B.5 Proof of Lemma 5
We will show that
∞
E p
n=1
(
1
2 k2
nx ns
∖
2
knx kns
∑ IjC Lij- E[L ij- । Zi]j >[< ∞.
24
Published as a conference paper at ICLR 2022
This, along with the first Borel-Cantelli lemma (Emile Borel, 1909; Cantelli, 1917) implies the
result.
Applying Markov’s inequality and using the fact that Lij and L/ are independent conditional on Zi
gives
∞
E P
n=1
(
1
2	k2
nχ ns
∖
knx / kns	∖
E(ELij- E[Lj∣ Zi]) >∣
i=1 j=1
1	∞
≤ IEE
n=1
1 ∞
IE
n=1
1
2 k2
nnχnns
1
2	k2
nχ ns
knx
2	∖	2'
knx	/ kns	∖
E(ELij- E[Lj| %])
i=1	V=1	)
kns
, ~ nx ∙ ~ ns
Σ jE E[(Lij- E[Lijl ZiD ]
≤ 1 E 1
1 n=ι knχk2s
knx kns
∑ 2 E[Lj ],
where the last line follows from the law of total variance. To simplify the remaining algebra, we will
use a < b to denote that there is some constant 0 < c < ∞ such that a < cb. Continuing, We have
o 1∞	[ knx kns
∣Er⅛E ∑e[L2j]<
C -l nnxnns .	.
n=1	ns i=1 j=1
[OO knx kns -ι
∣EEΣ⅛[L j]
1	-i -i - -I n∣r,
n=1 i=1 j=1 n
1	∞	∞	∞	1
=Ie ∑e[lj E 高
i=1 j=1	n=n(i,j)
.1 33	1	― Cr
~ C 匚 ɪ1 max{i∕(1 -Ps),j∕Ps}3	ij
∞ ∞
< I∑ ∑max⅛ E[L 2j]
i=1 j=1
]∞ ∞	]
=Ie Ema⅛y E[L j
i=1 j=1
where n(i,j) is shorthand for n(i,j) = logα max{i∕(1 - PS)Ij/Ps} is the first n such that knχ ≥ i
and kns ≥ j.
Now, define Q as the distribution of Ln induced by Zi and Si. Additionally, split the inner sum into
two pieces, one for when j < i and so max{i,j} = i and one for when j ≥ i and so max{i,j} = j.
1 ∞ ∞	1	1 ∞ I i 1 fij	∞ fij	∖
I E E max{i,j}3 ELjl = I E (jE 涓/ χ2 dQ(x) + E / x2 dQ(X))
[∞ i i-1 [ ij Ztk	∞ ij M
=I∑ Σ⅛Σ	X2 dQ(x) + ∑∑	X2 dQ(x)
i=1 ∖j = 1 Z k=1，k-1	j=i k=1Jk-^1
25
Published as a conference paper at ICLR 2022
switching the order of the indices over j and k, using that 1 ≤ k ≤ ij and the constraints on j
relative to i,
∞ J∞	/ i-1 1 _ij M	^∞ _ij	rk
ɪ 之(g i3 S Jk-F "十 gg Jk-F ”
∞ ∞ ∕iL1—八 /八 Ck	∞	∞	Irk
S ɪ S(S (2-i≠2/ x2 dQ(x) + S S \	x2 dQ(x)
i=1 ∖k=1	k 1	k=1 j=max{i,k∕i}	k 1
1 ∞ ∕i2 — 1
A ɪ S(S
i=1 k k=1
「 J dQ(x)+ S m⅛ψ J dQ(x))
i J k - 1	k=1 maX{i,k∕i/ J k - 1	)
Switching the order of summation over i and k, and separating out the terms where k/i < i and
k/i ≥ i,
1 S S (i- k/i)
e ⅛(⅛	i3
Zk	∞
I x2 dQ(x)+ £
k=1
1	Ck
max{i,k∕i}2 Jk-I x2 dQ(X)
<
(i - k/i)	@ 1	Wk i2
—2 +Σ江+Σ庐
i=√k	i=1
ɪ j x1∙5 dQ(x) < ∞.
e Jo
26
Published as a conference paper at ICLR 2022
C Instance-Level Agreement of MultiBerts on GLUE
We present additional performance experiments to complement Section 2.
Table 3 shows per-example agreement rates on GLUE predictions between pairs of models pre-
trained with a single seed (“same”) and pairs pre-trained with different seeds (“diff”); in all cases,
models are fine-tuned with different seeds. With the exception of RTE, we see high agreement (over
90%) on test examples drawn from the same distribution as the training data, and note that agreement
is 1-2% lower on average for the predictions of models pre-trained on different seeds compared to
models pre-trained on the same seed. However, this discrepancy becomes significantly more pro-
nounced if we look at out-of-domain “challenge sets” which feature a different data distribution from
the training set. For example, if we evaluate our MNLI models on the anti-sterotypical examples
from HANS (McCoy et al., 2019), we see agreement drop from 88% to 82% when comparing across
pre-training seeds. Figure 4 shows how this can affect overall accuracy, which can vary over a range
of nearly 20% depending on the pre-training seed. Such results underscore the need to evaluate
multiple pre-training runs, especially when evaluating a model’s ability to generalize outside of its
training distribution.
	Same	Diff.	Same - Diff.
CoLA	91.5%	89.7%	1.7%
MNLI	93.6%	90.1%	3.5%
HANS (all)	92.2%	88.1%	4.1%
HANS (neg)	88.3%	81.9%	6.4%
MRPC	91.7%	90.4%	1.3%
QNLI	95.0%	93.2%	1.9%
QQP	95.0%	94.1%	0.9%
RTE	74.3%	73.0%	1.3%
SST-2	97.1%	95.6%	1.4%
STS-B	97.6%	96.2%	1.4%
Table 3: Average per-example agreement between model predictions on each task. This is computed
as the average “accuracy” between the predictions of two runs for classification tasks, or Pearson
correlation for regression (STS-B). We separate pairs of models that use the same pre-training seed
but different fine-tuning seeds (Same) and pairs that differ both in their pre-training and fine-tuning
seeds (Diff). HANS (neg) refers to only the anti-stereotypical examples (non-entailment), which
exhibit significant variability between models (McCoy et al., 2020).
干
ð
T^T
.D
⅛
⅛
♦□
⅛
■
■
■
T
■
♦一 ♦
.. .
♦T
■
■
■
■
.口
⅛
宇
4.32 1
6 6 6s
(SqdlUeXO PwelUWUOU) Aue-lr∞<
O 1	2	3	4	5	6	7	8	9 IO 11 12 13 14 15 16 17 18 19 20 21 22 23 24
Pretraining Seed
Figure 4:	Accuracy of MNLI models on the anti-stereotypical (non-entailment) examples from
HANS (McCoy et al., 2020), grouped by pre-training seed. Each column shows the distribution
of five fine-tuning runs based on the same initial checkpoint.
27
Published as a conference paper at ICLR 2022
Winogender bias correlation (r) by pretrain seed
8 6 4 2
SO.O.O.
(J) ⊂o-⅛-φboo SB-CQ
-0.2 -----------------------------------------------------------------------------------------------------------
0	1	2	3	4	5	6	7	8	9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24
Pretraining Seed
Figure 5:	Bias correlation on Winogender for each pre-training seed. Each box represents the dis-
tribution of the score over five training runs of the coreference model over each MultiBerts base
checkpoint. This is the same data as Figure 3, but showing only the base checkpoints.
Figure 6: Bias correlation on Winogender for
five pretraining seeds, with 25 coreference runs
per seed.
Bias (r)			
Seed 0	θf		0.368
Seed 1	θf′		0.571
Avg. Diff.	δ = θ	f′ - θf	0.203
p-value			0.009
Table 4: Unpaired Multi-Bootstrap on Wino-
gender bias correlation, comparing pretraining
seed 0 to pretraining seed 1.
D	Cross-Seed Variation
Figure 5 shows variation in Winogender bias correlation (S4) between each MultiB erts pretraining
seed. Each box shows the distribution over five runs, and some of the variation between seeds may
simple be due to variation in training the coreference model. If we average the scores for each seed
then look at the distribution of this per-seed average score, we get 0.45±0.11. What if pretraining
didn’t matter? If we ignore the seed and randomly sample sets of five runs from this set with
replacement, we get scores of 0.45±0.05 - telling us that most of the variance can only be explained
by differences between the pretraining checkpoints.
We can confirm this by taking a subset of our pretraining seeds and training additional 25 randomly-
initialized coreference models. Figure 6 shows the result: seeds 0, 2, 3, and 4 appear closer together
than in Figure 5, but seed 1 clearly has different properties with respect to our Winogender metric.
We can confirm this with an unpaired multibootstrap analysis, taking seed 0 as base and seed 1 as
experiment: we observe a significant effect of δ = 0.203 (p = 0.009), as shown in Table 4.
28
Published as a conference paper at ICLR 2022
	MNLI	RTE	MRPC
θf (1M steps)	0.837	0.644	0.861
θf′ (2M steps)	0.844	0.655	0.860
δ = θf′ - θf	0.007	0.011	-0.001
p-value (H0 that δ ≤ 0)	< 0.001	0.141	0.564
Table 5: Expected scores (accuracy), effect sizes, and p-values from Multi-Bootstrap on selected
GLUE tasks. We pre-select the best fine-tuning learning rate by averaging over runs; this is 3e-5
for checkpoints at 1M steps, and 2e-5 for checkpoints at 2M pre-training steps. All tests use 1000
bootstrap samples, in paired mode on the five seeds for which both 1M and 2M steps are available.
E	Case S tudy: MultiBERTs vs. Original Bert
As an additional example of application, we discuss challenges in reproducing the performance of
the original Bert checkpoint, using the Multi-Bootstrap procedure.
The original bert-base-uncased checkpoint appears to be an outlier when viewed against the
distribution of scores obtained using the MultiBerts reproductions. Specifically, in reproducing the
training recipe of Devlin et al. (2019), we found it difficult to simultaneously match performance
on all tasks using a single set of hyperparameters. Devlin et al. (2019) reports training for 1M
steps. However, as shown in Figure 1 and 2, models pre-trained for 1M steps matched the original
checkpoint on SQuAD but lagged behind on GLUE tasks; if pre-training continues to 2M steps,
GLUE performance matches the original checkpoint but SQuAD performance is significantly higher.
The above observations suggest two separate but related hypotheses (below) about the Bert pre-
training procedure.
1.	On most tasks, running Bert pre-training for 2M steps produces better models than 1M
steps.
2.	The MultiB erts training procedure outperforms the original Bert procedure on SQuAD.
Let us use the Multi-Bootstrap to test these hypotheses.
E.1 How many steps to pretrain?
Let f be the predictor induced by the BERT pre-training procedure using the default 1M steps, and let
f′ be the predictor resulting from the proposed intervention of training to 2M steps. From a glance at
the histograms in Figure 8, we can see that MNLI appears to be a case where 2M is generally better,
while MRPC and RTE appear less conclusive. With the MultiB erts, we can test the significance
of the results. The results are shown in Table 5. We find that MNLI conclusively performs better
(δ = 0.007 with p < 0.001) with 2M steps; for RTE and MRPC we cannot reject the null hypothesis
of no difference (p = 0.14 and p = 0.56 respectively).
As an example of the utility of this procedure, Figure 7 shows the distribution of individual samples
of L for the intervention f' and baseline f from this bootstrap procedure (which We denote as L'
and L, respectively). The distributions overlap significantly, but the samples are highly correlated
due to the paired sampling, and we find that individual samples of the difference (L' - L) are nearly
always positive.
E.2 Does the MultiB erts procedure outperform original Bert on SQuAD?
To test our second hypothesis, i.e., that the MultiBerts procedure outperforms original Bert on
SQuAD, we must use the unpaired Multi-Bootstrap procedure. In particular, we are limited to the
case in which we only have a point estimate of L'(S), because we only have a single estimate of
the performance of our baseline model f′ (the original BERT checkpoint). However, the Multi-
Bootstrap procedure still allows us to estimate variance across our MultiB erts seeds and across the
examples in the evaluation set. On SQuAD 2.0, we find that the MultiB erts models trained for 2M
29
Published as a conference paper at ICLR 2022
steps outperform original BERT with a 95% confidence range of 1.9% to 2.9% and p < 0.001 for
the null hypothesis, corroborating our intuition from Figure 2.
Figure 7: Distribution of estimated performance on MNLI across bootstrap samples, for runs
with 1M or 2M steps. Individual samples of L(S, (X, Y)) and L'(S, (X, Y)) on the left, deltas
L'(S, (X, Y)) - L(S, (X, Y)) shown on the right. Bootstrap experiment is run as in Table 5, which
gives δ = 0.007 with p < 0.001.
deltas
Pretraining Steps
QQP (acc)
RTE (acc)
io

0.907 0.908 0.909 0.910 0.911	0.55 0.60 0.65 0.70
2M	IM
Figure 8: Distribution of the performance on GLUE dev sets, showing only runs with the best
selected learning rate for each task. Each plot shows 25 points (5 fine-tuning x 5 pre-training)
for each of the 1M and 2M-step versions of each of the pre-training runs for which we release
intermediate checkpoints (§2).
30