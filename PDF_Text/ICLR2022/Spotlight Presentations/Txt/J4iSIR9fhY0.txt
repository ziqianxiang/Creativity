Published as a conference paper at ICLR 2022
Representation Learning for Online and Of-
fline RL in Low-rank MDPs
Masatoshi Uehara	Xuezhou Zhang
Department of Computer Science	Department of Electrical and Computer Engineering
Cornell University, Ithaca, NY 14850, USA Princeton University, NJ 08544,USA
mu223@cornell.edu	xz7392@princeton.edu
Wen Sun
Department of Computer Science, Cornell University, Ithaca, NY 14850, USA
ws455@cornell.edu
Ab stract
This work studies the question of Representation Learning in RL: how can we
learn a compact low-dimensional representation such that on top of the represen-
tation we can perform RL procedures such as exploration and exploitation, in a
sample efficient manner. We focus on the low-rank Markov Decision Processes
(MDPs) where the transition dynamics correspond to a low-rank transition matrix.
Unlike prior works that assume the representation is known (e.g., linear MDPs),
here we need to learn the representation for the low-rank MDP. We study both the
online RL and offline RL settings. For the online setting, operating with the same
computational oracles used in Flambe(Agarwal et al., 2020b)—-the state-of-art
algorithm for learning representations in low-rank MDPs, we propose an algo-
rithm Rep-UCB—Upper Confidence Bound driven Representation learning for
RL, which significantly improves the sample complexity from O(A9d7/(10 (1 -
γ)22)) for FLAMBE to O(d4A2/(2(1 - γ)5)) with d being the rank of the transi-
tion matrix (or dimension of the ground truth representation), A being the number
of actions, and γ being the discount factor. Notably, REP-UCB is simpler than
Flambe, as it directly balances the interplay between representation learning,
exploration, and exploitation, while Flambe is an explore-then-commit style ap-
proach and has to perform reward-free exploration step-by-step forward in time.
For the offline RL setting, we develop an algorithm that leverages pessimism to
learn under a partial coverage condition: our algorithm is able to compete against
any policy as long as it is covered by the offline data distribution.
1	Introduction
When applying Reinforcement Learning (RL) to large-scale problems where data is complex and
high-dimensional, learning effective transformations of the data, i.e., representation learning, can
often significantly improve the sample and computation efficiency of the RL procedure. Indeed,
several empirical works have shown that leveraging representation learning techniques developed
in supervised or unsupervised learning settings can accelerate the search for good decision-making
strategies (Silver et al., 2018; Stooke et al., 2021; Srinivas et al., 2020; Yang & Nachum, 2021).
However, representation learning in RL is far more subtle than it is for non-sequential and non-
interactive learning tasks (e.g., supervised learning). Prior works have shown that even if one is
given the magic representation that exactly linearizes the optimal policy (Du et al., 2019b) or the
optimal value functions (Wang et al., 2020; Weisz et al., 2021), RL is still challenging (i.e., one may
still need exponentially many samples to learn). This indicates that an effective representation that
permits efficient RL needs to encode more information about the underlying Markov Decision Pro-
cesses (MDPs). Despite the recent empirical success of representation learning in RL , its statistical
guarantee and theoretical properties remain under-investigated.
In this work, we study the representation learning question under the low-rank MDP assumption.
Concretely, a low-rank MDP assumes that the MDP transition matrix admits a low-rank factoriza-
tion, i.e., there exists two unknown mappings μ(s0), φ(s, a), such that P(s0∣s, a) = μ(s0)>φ(s, a)
for all s, a, s0, where P(s0|s, a) is the probability of transiting to the next state s0 under the current
1
Published as a conference paper at ICLR 2022
state and action (s, a). The representation φ in a low-rank MDP not only linearizes the optimal
state-action value function of the MDP (Jin et al., 2020a), but also linearizes the transition operator.
A low-rankness assumption on large stochastic matrices is a common and natural assumption
and has enabled successful development of algorithms for real world applications such as movie
recommendation systems (Koren et al., 2009). We note that a low-rank MDP strictly generalizes
the linear MDP model (Yang & Wang, 2020; Jin et al., 2020a) which assumes φ is known a priori.
The unknown representation φ makes learning in low-rank MDPs much more challenging than that
in linear MDPs since one can no longer directly use linear function approximations. On the other
hand, the fact that linear MDPs can be solved statistical and computational efficiently if φ is known
a priori implies that if one could learn the representation of the low-rank MDP, one could then
efficiently learn the optimal policy.
Indeed, prior works have shown that learning in low-rank MDPs is statistically feasible (Jiang et al.,
2017; Sun et al., 2019; Du et al., 2021) via leveraging rich function approximators. However, these
algorithms are version space algorithms and are not computationally efficient. Recent work Flamb e
proposes an oracle-efficient algorithm1 that learns in low-rank MDPs with a polynomial sample
complexity, where the computation oracle is Maximum Likelihood Estimation (MLE) operating
under the standard supervised learning style Empirical Risk Minimization (ERM) setting. In this
work, we follow the same setup from Flambe (Agarwal et al., 2020b), and propose a new algo-
rithm — Upper Confidence Bound driven Representation Learning, Exploration and Exploitation
(REP-UCB), which can learn a near optimal policy for a low-rank MDP with a polynomial sample
complexity and is oracle-efficient. Comparing to FLAMBE, our algorithm significantly improves the
sample complexity from O(d7A9/(10(1 - γ)22) for FLAMBE to O(d4A2 */(2 (1 - γ)5), where d
is the rank of the transition matrix (or dimension of the true representation), A is the number of
actions, is the suboptimality gap and γ ∈ [0, 1) is the discount factor in the MDP. Our algorithm
is also arguably much simpler than Flambe: Flambe is an explore-then-commit algorithm, has
to explore in a layer-by-layer forward way, and does not permit data sharing across different time
steps. In contrast, Rep-UCB carefully trades exploration versus exploitation by combining the re-
ward signal and exploration bonus (constructed using the latest learned representation), and enables
data sharing across all time steps.2 Our sample complexity nearly matches the ones from those
computationally inefficient algorithms (Jiang et al., 2017; Sun et al., 2019; Du et al., 2021). We
summarize the comparison with the prior works that study representation learning in Table 1.
In addition to the online exploration setting, we also show that our new techniques can be directly
used for designing offline RL algorithms for low-rank MDPs under partial coverage. More specif-
ically, we propose an algorithm REP-LCB—Lower Confidence Bound driven Reprepresentation
Learning for offline RL, that given an offline dataset, can learn to compete against any policy
(including history-dependent policies) as long as it is covered by the offline data where the coverage
is measured using the relative condition number (Agarwal et al., 2021) associated with the ground
truth representation. Thus, our offline RL result generalizes prior offline RL works on linear MDPs
(Jin et al., 2020b; Zhang et al., 2021b) which assume representation is known a priori and use linear
function approximation. Computation-wise, our approach uses one call to the MLE computation
oracle, and hence is oracle-efficient. REP-LCB is the first oracle efficient offline algorithm for
low-rank MDP enjoying the aforementioned statistical guarantee. See Section 2 for a more detailed
comparison with the existing literature on representation learning in offline RL.
Our contributions. We develop new representation learning RL algorithms that enable sample
efficient learning in low-rank MDPs under both online and offline settings:
1.	In the online episodic learning setting, our new algorithm Rep-UCB integrates representa-
tion learning, exploration, and exploitation together, and significantly improves the sample
complexity of the prior state-of-art algorithm Flambe;
1The oracle generally refers to supervised learning style empirical risk minimization oracle. We seek to
design an algorithm that runs in polynomial time with each oracle call counting as O(1). The reduction to
supervised learning has lead to many successful provable and practical algorithms in contextual bandit (Agarwal
et al., 2014; Dudlk et al., 2017; Foster & Rakhlin, 2020) and RL (Du et al., 2019a; Misra et al., 2020).
2Our algorithm and analysis can be easily extended to finite horizon non-stationary setting. We choose
the discounted infinite horizon setting to contrast our results to FLAMBE: FLAMBE is not capable of learning
stationary policies under the discounted infinite horizon setting.
2
Published as a conference paper at ICLR 2022
Methods	Setting	Sample Complexity	Computation
OLIVE (Jiang et al., 2017)	Low Bellman rank	d2A /(1-Y)4	Inefficient
Witness rank (Sun et al., 2019)	Low Witness rank	d2A /(1-Y)4	Inefficient
BLin-UCB (DU et al.,2021)	Bilinear Class	d2A F(1-Y)7	Inefficient
Moffle (Modi et al., 2021)	Low-nonnegative-rank MDP	d6A13 	e2η5(i-γ)5		Oracle efficient
Flambe Agarwal et al. (2020b)	Low-rank MDP	d7A9 H0(1-γ)22	Oracle efficient
Rep-UCB (Ours)	Low-rank MDP	d4A2 e2(l-γ)5		Oracle efficient
Table 1: Comparison among different provable representation learning algorithms in online RL.
Algorithms such as OLIVE, Witness rank, and BLin-UCB work for settings which are more gen-
eral than low-rank MDPs and have tight sample complexity. However, these algorithms are version
space algorithms and thus are not computationally efficient. Moffle is an oracle-efficient algorithm
(with a much stronger oracle than the one in Flambe and ours), but the assumptions under which
Moffle operates essentially imply that the MDP’s transition has low non-negative matrix rank (nnr).
Note that a nnr is at least as large as and could be exponentially larger than the rank (Agarwal et al.,
2020b). Finally, Flambe operates under the same function approximation setting and the computa-
tion oracle as ours. Our algorithm significantly improves the sample complexity from FLAMBE in all
parameters. Note the horizon dependence is not exactly comparable as these prior works originally
considered the finite horizon setting with nonstationary transition, and we convert their results to the
discounted setting by simply replacing the finite horizon H by Θ(1∕(1 - Yy).
2.	In the offline learning setting, we propose a natural concentrability coefficient (i.e., relative
condition number under the true representation) that captures the partial coverage condition in
low-rank MDP, and our algorithm Rep-LCB learns to compete against any policy (including
history-dependent ones) under such a partial coverage condition.
2	Related Work
Online Setting We list the comparison as follows, which is summarized in Table 1. Additional
related works are discussed in Section A.
Flambe (Agarwal et al., 2020b) was a state-of-the-art oracle-efficient algorithm for low-rank
MDPs. In all parameters, the statistical complexity is much worse than Rep-UCB . Our algorithm
and Flambe operate under the same computation oracle. Flambe does not balance exploration
and exploitation, and uses explore-then-committee style techniques (i.e., constructions of absorbing
MDPs (Brafman & Tennenholtz, 2002)) which results in its worse sample complexity.
With a more complex oracle, Moffle (Modi et al., 2021) is a model-free algorithm for low-rank
MDPs, with two additional assumptions: (1) the transition has low non-negative rank (nnr), and
(2) reachability in latent states. The first assumption significantly restricts the scope of low-rank
MDPs as there are matrices whose nnr is exponentially larger than the rank (Agarwal et al., 2020b).
The sample complexity ofMoffle can scale O(d6∣A∣13∕(e2η5(1 - γ)5)), where η is the reachability
probability, and 1∕η could be as large as nnr1/2 (Proposition 4 in Agarwal et al. (2020b)), which
essentially means that Moffle has a polynomial dependence on the nnr.
OLIVE (Jiang et al., 2017), Witness rank (Sun et al., 2019) and Bilinear-UCB (Du et al., 2021),
when specialized to low-rank MDPs, have slightly tighter dependence on d (e.g., O(d2/e2 )). But
these algorithms are computationally inefficient as they are version space algorithms. Dann et al.
(2021) shows that with a policy class, solving a low-rank MDP can take Ω(2d) samples. In this work,
similar to Witness rank (Sun et al., 2019) and Flambe, we use function approximators to model the
transition. Thus our positive result is not in contradiction to the result from Dann et al. (2021).
VALOR (Dann et al., 2018), PCID (Du et al., 2019a), HOMER (Misra et al., 2020), RegRL (Foster
et al., 2020), and the approach from Feng et al. (2020) are algorithms for block MDPs which is a
more restricted setting than low-rank MDPs. These works require additional assumptions such as
deterministic transitions (Dann et al., 2018), reachability (Misra et al., 2020; Du et al., 2019a), strong
Bellman closure (Foster et al., 2020), and strong unsupervised learning oracles (Feng et al., 2020).
Offline Setting We discuss related works in offline RL. Additional related works are discussed in
Section A.
3
Published as a conference paper at ICLR 2022
Uehara & Sun (2021) obtained similar statistical results for offline RL on low-rank MDPs. Though
the sample complexity in their algorithm is slightly tighter, our algorithm is oracle-efficient, while
the CPPO algorithm from Uehara & Sun (2021) is a version space algorithm.
Xie et al. (2021) propose a (general) pessimistic model-free algorithm in the offline setting. We can
also apply their algorithm to low-rank MDPs and show some finite-sample guarantee. However, it is
unclear whether the final bounds in their results can be characterized by the relative condition num-
ber only using the true representation, and whether they can compete with history-dependent poli-
cies. Thus, our result is still considered superior on low-rank MDPs. The detail is given in Section E.
3	Preliminaries
We consider an episodic discounted infinite horizon Markov Decision Process M =
hS , A, P, r, γ , d0 i specified by a state space S, a discrete action space A, a transition model
P : S × A → ∆(S), a reward function r : S × A → R, a discount factor γ ∈ [0, 1), and an
initial distribution d0 ∈ ∆(S). To simplify the presentation, we assume r(s, a) and d0 are known
(e.g., when d0 is a probability mass only on s0, agent always starts from a fixed initial state s0)3.
Following prior work (Jiang et al., 2017; Sun et al., 2019), we assume trajectory reward is normal-
ized, i.e., for any trajectory {sh, ah}h∞=0, we have Ph∞=0 γhr(sh, ah) ∈ [0, 1]. Since the ground
truth P ? is unknown, we need to learn it by interacting with environments in an online manner or
utilizing offline data at hand. We remark that the extension of our all results to the finite horizon
nonstationary case is straightforward. For example, refer to Zhang et al. (2022).
We use the following notation. Given a policy π : S → ∆(A), we define the value function
VPπ(s) = E Ph∞=0 γhr(sh, ah)|s0 = s, P, π to represent the expected total discounted reward of
π under P starting at s. Similarly, we define the state-action Q function QπP (s, a) := r(s, a) +
YEs，~p(∙∣s,a)Vp(s0). The expected total discounted reward of a policy ∏ under transition P and
reward r is denoted as VPnr := Es0~do VPn(so). We define the discounted state-action occupancy
distribution dπP (s, a) = (1 - γ) Pt∞=0 γtdπP,t(s, a), where dπP,t(s, a) is the probability of π visiting
(s, a) at time step t under π and P. We slightly abuse the notation, and denote dπP (s) as the state
visitation, which is equal to Pa∈A dπP (s, a). When P is the true transition model P?, we drop
the subscript and simply use dπ(∙). Unless otherwise noted, Π denotes the class of all polices
{S → ∆(A)}. We denote total variation distance of P1 and P2 by kP1 - P2k1. Finally, given a
vector a, We define ka∣∣B = √a>Ba. co,cι,… are universal constants.
We study low-rank MDPs defined as follows (Jiang et al., 2017; Agarwal et al., 2020b). The condi-
tions on the upper bounds of the norm of φ?, μ? are just for normalization.
Definition 1 (Low-rank MDP). A transition model P? : S × A → ∆(A) admits a low rank
decomposition with rank d ∈ N ifthere exists two embedding functions φ? μ? such that
∀s, s0 ∈ S,a ∈ A : P?(s0 | s,a) = μ*(s0)>φ*(s, a)
where ∣∣φ*(s, a)k2 ≤ 1 forall (s, a) andfor anyfunction g : S → [0,1], k / μ?(s)g(s)d(s)∣∣2 ≤ √d.
An MDP is a low rank MDP if P? admits such a low rank decomposition.
Low-rank MDPs capture the latent variable model (Agarwal et al., 2020b) where φ? (s, a) is a dis-
tribution over a discrete latent state space Z. The block-MDP model (Du et al., 2019a) is a special
instance of the latent variable model with φ*(s, a) being a one-hot encoding vector. Note the linear
MDPs (Yang & Wang, 2020; Jin et al., 2020a) assume φ? is known.
Next, we explain two settings: the online learning setting and the offline learning setting. Then, we
present our function approximation setup and computational oracles.
Episodic Online learning In online learning, our overall goal is to learn a stationary policy ∏ so
that it maximizes VPL lr, where P? is the ground truth transition. We assume that We operate under
the episodic learning setting where we can only reset to states sampled from the initial distribution
d0 (e.g., to emphasize the challenge from exploration, we can consider the special case where we
can only reset to a fixed s0). In the episodic setting, given a policy π, sampling a state s from the
3Extension to the unknown case is straightforward. Recall the major challenging of RL is due to the un-
known transition model.
4
Published as a conference paper at ICLR 2022
state visitation dp is done by the following roll-in procedure: starting at so 〜do, at every time step
t, We terminate and return St with probability 1 - Y, and otherwise We execute at 〜∏(st) and move
to t + 1, i.e., st+ι 〜P(∙∣st, at). Such a sampling procedure is widely used in the policy gradient
and policy optimization literature (e.g., (Kakade & Langford, 2002; Agarwal et al., 2021; 2020a)).
Offline learning In the offline RL, we are given a static dataset in the form of quadruples:
D = {s(i), a(i),r(i), s0(i)}n=ι 〜ρ(s,a)δ(r = r(s, a))P?(s0 | s,a).
For simplicity, we assume ρ = dπPb?, where πb ∈ [S → ∆(A)] is a fixed behavior policy. We denote
ED [f (s, a, s0)] = 1/n P(s,a,s0)∈D f(s, a, s0). To succeed in offline RL, we in general need some
coverage property of ρ. One common assumption is that ρ globally covers every possible policies’
state-action distribution, i.e., max∏,s,a dp? (s, a)∕ρ(s, a) < ∞ (Antos et al., 2008). In this work, we
relax such a global coverage assumption and work under the partial coverage condition where ρ may
not cover distributions of all possible policies. Instead of competing against the optimal policy under
the global coverage, we aim to compete against any policies covered by the offline data. In section 5,
we precisely define the partial coverage condition using the concept of the relative condition number.
Function approximation setup and computational oracles Since μ? and φ? are unknown, we
use function classes to capture them. Our function approximation and computational oracles are ex-
actly the same as the ones used in FLAMBE. For completeness, we state the function approximation
and computational oracles below.
Assumption 2. We have a model class M = {(μ,φ) : μ ∈ Ψ,φ ∈ Φ}, where μ? ∈ Ψ, φ? ∈ Φ.
Following the norm bounds on μ?, φ? we similarly assume that the same norm bounds hold
for our function approximator, i.e., for any μ ∈ Ψ,φ ∈ Φ, ∣∣φ(s,a)k2 ≤ 1, ∀(s,a) and
k Rμ(s)g(s)d(s)∣2 ≤ √d,∀g : S → [0,1],and Rμ>(s0)φ(s,a)d(s0) = 1, ∀(s,a).
As for computational oracles, we use a supervised learning style MLE oracle.
Definition 3 (Maximum Likelihood Oracle (MLE)). Consider the model class M and a dataset D
0
in the form of (s, a, s ), the MLE oracle returns the maixmum likelihood estimator P := (μ, φ)=
argmax(μ,φ)∈M ED ln(μ(s0)>φ(s, a)).
We also invoke a planning procedure for known linear MDPs with potentially nonlinear rewards,
which can be done in polynomial time (we know that online learning in linear MDPs can be
done statistically and computationally efficient). Given a reward r and a model P := (μ, φ) with
P(s0∣s, a) = μ(s0)>φ(s, a) (i.e., a known linear transition with a known feature φ), we can compute
the optimal policy arg maxπ Vpπ,r by standard least square value iteration which uses linear regres-
sion. A planning procedure for a known linear MDP is also used in Flambe, see Section 5.1 in
Agarwal et al. (2020b) how to implement this procedure with polynomial computation complexity.
4	Representation Learning in Online Setting
We consider the online episodic learning setting where the agent can only reset based on the initial
state distribution do . To find a near-optimal policy for a low-rank MDP efficiently, we need to
carefully interleave representation learning, exploration, and exploitation.
4.1	Algorithm
We present our algorithm in the online setting in Algorithm 1. We first describe the data collection
process. Every iteration, Algorithm 1 rollouts its current policy π to collect a tuple (s, a, s0, a0, s)
where S 〜dp ?, a 〜U (A),s0 〜P *(∙∣s,a),a0 〜U (A), s 〜P ?(• | s, a) where U (A) is a
uniform distribution over actions (note that we take two uniform actions here). Recall that to sample
S 〜dp?, we start at so 〜do, at every time step t, we terminate and return St with probability 1 - γ ,
and otherwise we execute at 〜∏(st) and move to t + 1, i.e., st+1 〜 P*(∙∣st, at). Thus collecting
one tuple requires exactly one roll-in, i.e., one trajectory. We can verify that with high probability
the roll-in terminates with O((1 - γ)-1) steps which is often called the effective horizon.
After collecting new data and concatenating it with the existing data, we perform representation
learning, i,e, learning a factorization and a representation by MLE (line 6), set the bonus based on
5
Published as a conference paper at ICLR 2022
Algorithm 1 UCB-driven representation learning, exploration, and exploitation (REP-UCB)
1:	Input: Regularizer λn, parameter αn,, Models M = {(μ, φ) : μ ∈ Ψ,φ ∈ Φ}, Iteration N
2:	Initialize ∏o(∙ | S) to be uniform; set D° = 0, D0 = 0
3:	for episode n = 1, ∙∙∙ , N do
4:	Collect a tuple (s, a, s0, a0, s) with
s 〜dpn?-1, a 〜U (A), s0 〜P *(∙∣s, a), a0 〜U (A), S 〜P *(∙∣s0, a0)
5:	Update datasets by adding triples (s, a, s0) and (s0, a0, ss):
Dn = Dn-1 + {(s, a, s0)},	Dn0 = Dn0 -1 + {(s0, a0, ss)}
6:	Learn representation via ERM (i.e., MLE):
Pn := (μn,φn) = argmax(μ,φ)∈M EDn+Dn [ln μ> (s0)φ(s, a)]
7:	Update empirical covariance matrix Σn = s,a∈D φn(s, a)φn(s, a)> + λnI
8:	Set the exploration bonus:
7^	/	∖	♦
bn(s, a) := min
q> -1
φn (s, a) Σn φn (s, a), 2
(1)
9:	Update policy ∏n = arg max∏ VJ i
10:	end for	n , n
11:	Return ∏ι, ∙∙∙ , ∏n
the learned feature (Eq. 1), and update the policy via planning inside the learned model with the
bonus-enhanced reward (Line 9). Note the learned transition P from MLE is linear with respect to
the learned feature φ, and planning in a known linear MDP is known as computationally efficient
(Jin et al., 2020a) (see the explanation after Definition 3 as well).
Computation of the MLE oracle The MLE oracle in general could be a non-convex optimization
procedure when φ and μ are general nonlinear function approximators. However, this is a standard
supervised learning ERM oracle and one can easily optimize it via stochastic gradient descent style
approaches if μ and φ are differentiable. For special cases where the MDP is a tabular MDP, the MLE
objective is convex and the optimal solution has closed-form. For linear MDPs (Yang & Wang, 2020)
where P?(s0|s, a) = (ψ*(s0))>M*φ*(s,a) with known μ? and ψ? but unknown M?, the MLE
objective again is convex with respect to parameter M? . Thus when specializing to specific settings
such as tabular MDPs and linear MDPs where computationally efficient approaches exist, Rep-UCB
is also provably computationally efficient. In contrast, more general approaches such as Olive (Jiang
et al., 2017) are provably computationally inefficient even when specialized to tabular MDPs. We
note that our setting does not directly capture the linear MDP setting from Jin et al. (2020a) since
there μ? might not be captured by a model class Ψ that has bounded statistical complexity.
4.2	Analysis
Theorem 4 (PAC Bound for REP-UCB). Fix δ ∈ (0,1), e ∈ (0,1). Let π be a uniform mixture of
∏ι, ∙∙∙ , ∏n and π? := argmax∏ Vp?r as the optimal policy. By setting parameters asfollows:
αn = O(P(|A| + d2) γln(∣M∣n∕δ)) ,	λn = O (dln(∣M∣n∕δ)),
with probability at least 1 - δ, we have
?
Vp?,r - Vp?,r ≤ ,
where the number of collected samples is at most
(d4∣A∣2 ln(∣M∣∕δ)	ʌ
。((1-γ )5e2	•少
where ν only contains log terms and the dependence on |M| is at most ln(ln(|M|)).
The theorem shows that Rep-UCB learns in low-rank MDPs in a statistically efficient and oracle-
efficient manner. To the best of our knowledge, this algorithm has the best sample complexity among
6
Published as a conference paper at ICLR 2022
all oracle efficient algorithms for low-rank MDPs. Extension to continuous function class Ψ and Φ
using statistical complexities such as covering dimension is possible since our analysis only uses
standard uniform convergence analysis on Ψ and Φ.
Highlight of the analysis Below we highlight our key lemmas and proof techniques.
First, why is learning in a low-rank MDP harder than learning in models with linear structures?
Unlike standard linear models such as linear MDPs (Yang & Wang, 2020; Jin et al., 2020a),
KNRs (Kakade et al., 2020; Abbasi-Yadkori & Szepesvari, 2011; Mania et al., 2020; Song &
Sun, 2021), and GP / kernel models (Chowdhury & Gopalan, 2019; Curi et al., 2020), we can-
not get uncertainty quantification on the model in a point-wise manner. When models are linear,
one can get the following style of point-wise uncertainty quantification for the learned model P :
∀s, a : '(P(∙∣s, a),P?(∙∣s,α)) ≤ σ(s,a) where σ(s,a) is the uncertainty measure, and ' is some
distance metric (e.g., `1 norm). With proper scaling, the uncertainty measure σ(s, a) is then used
for the bonus. For example, in linear MDPs (i.e., low-rank MDP with known feature φ?), given a
dataset D = {s, a, s0}, We can learn a non-parametric model P(s0∣s, a) := μ(s0) > φ*(s, a), and get
point-wise uncertainty quantification:
∀(S, a),	|/f (SO)μ>(SO)φ?(S,	a)d(SO)- /f (SO)μ?τ(SO)φ?(S,	a)d(SO)I	≤ ckφ?(S,	a)k∑-j	⑵
for some family of functions f : S → R with Σφ? = Esa∈d φ?(S,α)φ*(S,a)+ λI (Lykouris et al.,
2021; Neu & Pike-Burke, 2020). To set the scaling c properly, since φ? is known a priori, the linear
regression analysis applies here, and one can apply Cauchy-Schwarz inequality to the LHS of (2) to
pull out φ? and get an upper bound in the form of
kφ?(
S,
a)k∑-i k [
八J
(a)
f(S){μ(S)-μ?(S)}d(S)k∑Φ*
_ - /
{z
(b)
z
where c is set to be the linear regression training error measured in the term (b) above.
However, when we jointly learn μ and φ, since nonlinear function approximation is used, we can-
not get point-wise uncertainty quantification via linear regression-based analysis. We stress that
our bonus is not designed to capture the uncertainty quantification on the model error between
P(∙∣S,a) = μτφ(S,a) and P?(∙∣S,α) = μ?τφ*(S,a) in a point-wise way, which is not tractable as
P and P? does not even share the same representation. Instead, the bonus is carefully designed so
that it only provides near-optimism at the initial state distribution. This is formalized as follows.
Lemma 5 (Almost Optimism at the Initial State Distribution). Set the parameters as in Theorem 4.
With probability 1 - δ,
∀n ∈ [1,…，N ], ∀∏ ∈ ∏, Vπ,r+brι — VP*,r ≥ —ci qiAlln(Mln∕δ)(1H.
We remark that the idea of optimism with respect to the initial state distribution has been used in
prior works (Jiang et al., 2017; Sun et al., 2019; Du et al., 2021; Zanette et al., 2020). However, these
algorithms are not computationally efficient (i.e., they use version space instead of reward bonus),
and their version-space based analysis is different from ours.
Proof sketch for Lemma 5 We start by using the simulation lemma (Lemma 20) inside the learned
model which is important since our bonus bn uses φn associated with the learned model Pn :
Vpn,r+bn — VP?,r ≥ (I-Y) 1Es,a 〜dp [bn (S, a) — kPn(1S, a) — P?(，|S, a)kl],
from where we show that Es,a〜d∏ ∣∣P(∙∣s, a) — P*(∙∣s, a)ki as a whole nearly lower bounds the
average bonus Es,a〜d∏ [bn(s, a)]. Thus the proof of optimism is fundamentally different from the
proofs in tabular and linnear MDPs which are done via induction in a point-wise manner. The detailed
procedure is illustrated in Lemma 7 in Appendix B.
7
Published as a conference paper at ICLR 2022
Second, our bonus is using representation φn that is being updated every episode, and our empirical
covariance matrix Σn is also updated whenever we update φn, which means that standard elliptical
potential based analysis (i.e., analysis used in linear bandits/MDPs with known features) cannot
work here as our feature changes every episode. Instead, in our analysis, we have to keep tracking a
potential function that is defined using the unknown ground truth representation φ?, i.e., the elliptical
Potential ∣∣φ*(s, a)k∑-ι	, where
ρn,φ?
ςPn,Φ? = nE(s,a)〜Pn φ (s, a)φ (s, a)	+ λnI,
and ρn(s, a) = Pin=-01 dπPi? (s, a)/n. Since this potential function uses the fixed representation φ?,
we can apply the standard elliptical potential argument to track the progress that our algorithm makes
during learning. Below We illustrate the procedure of linking the bonus under φn to the potential
function ∣∣φ*(s, a)k∑-ι defined with respect to the true feature φ?.
ρn,φ?
Linking bonus under φn to the elliptical potential function under φ? With near optimism,
using the simulation lemma (Lemma 20) inside the real model, we can upper bound the per-iteration
regret as follows:
VP?,r - VPnn,r ≤ (I- Y 厂1E(s,a)〜dpn [bn(s, a) + (1 - Y)Tfn(S, a)] + PAIZn(I- Y)T ,
where Zn = O(1∕n), and fn(s, a) := ∣∣Pη(∙ ∣ s,a) - P?(• ∣ s,a)kι. To connect the first term in the
right-hand side of the above inequality to the elliptical potential under the fixed feature φ?, we show
that for any function g ∈ S ×A → [0, B] for B ∈ R+,
E(s,a)〜dPn [g(s, a)] ≤ (I - Y)TE(S,a)〜dpn [k φ?(s, a) k ∑-1 φ? ] JnYAIEPn [g2(s,a)] + YλndB2
+ J(I - Y)|A|Epn [g2(s,a)],
where ρ0n(s, a) = 1/n Pin=-01 dπi (s)u(a) and u(a) = 1/IAI. See Lemma 12 in Appendix B. By
substituting g with Ln + fn∕(1 - Y), the first term of the RHS of the above inequality can be upper
bounded as:
2(1 -
卜 Φ?(s,a)k∑-二
ρn,φ
{z
(G1)
i jn∣A∣Epn
"n(s,a)
.0-77
+ b2n (s, a)
+ λnd.
^{^™
(G2)
In the term (G2), we expect nEP0 [fn2(s, a)] to be O(1) as EP0 [fn2(s, a)] is in order of 1/n due to
the fact that it is the generalization bound of the MLE estimator Pn which is trained on the data
drawn from Pn . For nEp^ [bn(s,a)], we expect it to be in the order of d as the (unnormalized)
data covariance matrix Σn in the bonus bn uses training data from ρ0n , i.e., we are measuring the
expected bonus under the training distribution. In other words, the term (G2) scales in order of
poly(d). For the term (G1), since it contains the potential function based on φ?, the sum of the term
(G1) over all episodes can be controlled by the standard elliptical potential argument (see Lemma 18
and Lemma 19). This concludes the proof sketch of our main theorem.
In summary, our analysis relies on the standard idea of optimism in the face of uncertainty, but
with novel techniques to achieve optimism under nonlinear function approximation with the MLE
supervised learning style generalization bound, and to track regret under changing representations.
5 Representation Learning in Offline Setting
In this section, we study representation learning in the offline setting. We consider the setting where
the offline data does not have a full global coverage. We present our algorithm Lower Confidence
Bound driven Representation Learning in offline RL (REP-LCB) in Algorithm 2. Our proposed
algorithm consists of three parts. The first part is MLE which learns a model P and a representation
φ. The second part is the construction of a penalty term b. Using the learned representation φ, we
use a standard bonus in linear bandits as the penalty term as ifφ were the true feature. The third part
is planning with the learned model P and reward r - b.
8
Published as a conference paper at ICLR 2022
Algorithm 2 LCB-driven Representation Learning in offline RL (REP-LCB)
1:	Input: Regularizer λ, Parameter a, Model classes M = {μ>φ : μ ∈ Ψ,φ ∈ Φ}, Dataset D.
2:	Learn a model P by MLE: P = μ 1 φ = argmaxp∈m ED [ln P(s0 | s, a)].
3:	Set the empirical covariance matrix Σ =	(s,a)∈D φ(s, a)φ>(s, a) + λI.
4:	Set the reward penalty:
b(s, a) = min
5:	Solve π = arg maxπ V∏	ʌ.
π P,r-b
(αj φ(s, a)>Σ-1 φ(s, a), 2
We present the PAC guarantee of Rep-LCB. Before proceeding, we define a relative condition
number as a mean to measure the deviation between a comparator policy π and the offline data:
x>
Cπ? = sup
x∈Rd x
Edp? [φ?(S,a)φ*>(S,a)]x
>Eρ[φ*(s, a)φ?>(s, a)]x
In the special case where the MDP is just a tabular MDP (i.e., φ? is a one-hot encoding vector), this
is reduced to a density ratio C∞ = maxs,a dp? (s, a)∕ρ(s, a). The relative condition number Cn is
always no larger than the density ratio and could be much smaller for MDPs with large state spaces.
Note that we quantify the relative condition number using the unknown true representation φ? . With
the above setup, now we are ready to state the main theorem for Rep-LCB.
Theorem 6 (PAC Bound for REP-LCB). Let ω = maxa,s(1∕∏b(a | s)). Denote π as the output of
REP-LCB. There exists a set of parameters such that with probability at least 1 - δ, for any policy
π (including history-dependent non-Markovian policies),
vn	vn	< . ∕d4 5ω2C∏log(∣M∣∕δ)
VP?,r - VP?,r ≤ CV —^Pn—
See Theorem 14 in Appendix B for the detailed parameters. We explain several implications. First of
all, this theorem shows that we can uniformly compete with any policy including history-dependent
non-Markovian policies 4 satisfying the partial coverage Cπ? < ∞. Particularly, if the optimal policy
π? is covered by the offline data, i.e., Cπ?? < ∞, then our algorithm is able to compete against it
5. Note that assuming offline data covers π? is still a weaker assumption than the global coverage
such as supπ sup(s,a) dπP? (S, a)∕ρ(S, a) in prior offline RL works (Antos et al., 2008; Chen & Jiang,
2019). Second, our coverage condition is measured by a relative condition number defined using the
unknown ground truth representation φ? but not depending on other features. Prior works that use
relative condition numbers as measures of coverage are restricted to the settings where the ground
truth representation φ? is known (Jin et al., 2020b; Chang et al., 2021; Zanette et al., 2021b).
To sum up, our algorithm is the first oracle efficient algorithm which does not need to know φ? ,
and requires partial coverage only in terms of φ? . Note while Uehara & Sun (2021) has a similar
guarantee on low-rank MDPs, their algorithm is not oracle-efficient as itis a version space algorithm.
6	Conclusion
We study online/offline RL on low-rank MDPs, where the ground truth feature is not known a pri-
ori. For online RL, our new algorithm Rep-UCB significantly improves the sample complexity of
the piror state-of-the-art algorithm Flambe in all parameters while using the same computational
oracles. Rep-UCB has the best sample complexity among existing oracle efficient algorithms for
low-rank MDPs by a margin. Comparing to prior representation learning works on low-rank MDPs
and block MDPs that rely on a forward step-by-step reward-free exploration framework, our algo-
rithm interleaves representation learning, exploration, and exploitation together, and learns a single
stationary policy. For offline RL, our new algorithm Rep-LCB is the first oracle efficient algorithm
for low-rank MDPs that has a PAC guarantee under a partial coverage condition measured by the
relative condition number defined with the true feature representation.
4Given π = {∏i}∞=o where ∏i depends on s0,a0,...si, V∏?r and d∏p? (s, a) are still well-defined.
5We also require ω < ∞, which is a mild assumption since it does not involve P?. Indeed, it is much
weaker than the global coverage type assumption 1∕ρ(s,a) < ∞, ∀(s,a).
9
Published as a conference paper at ICLR 2022
Acknowledgments
The authors would like to thank Alekh Agarwal, Praneeth Netrapalli and Ming Yin for valuable
feedback.
Masatoshi Uehara is partially supported by Masason foundation.
References
Yasin Abbasi-Yadkori and Csaba Szepesvari. Regret bounds for the adaptive control of linear
quadratic systems. In Proceedings of the 24th Annual Conference on Learning Theory, pp. 1-
26. JMLR Workshop and Conference Proceedings, 2011.
Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming
the monster: A fast and simple algorithm for contextual bandits. In International Conference on
Machine Learning, pp. 1638-1646. PMLR, 2014.
Alekh Agarwal, Mikael Henaff, Sham Kakade, and Wen Sun. Pc-pg: Policy cover directed explo-
ration for provable policy gradient learning. NeurIPS, 2020a.
Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. Flambe: Structural complex-
ity and representation learning of low rank mdps. In Advances in Neural Information Processing
Systems, volume 33, pp. 20095-20107, 2020b.
Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy
gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning
Research, 22(98):1-76, 2021.
AndraS Antos, Csaba Szepesvari, and Remi Munos. Learning near-optimal policies with bellman-
residual minimization based fitted policy iteration and a single sample path. Machine Learning,
71:89-129, 2008.
Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for near-
optimal reinforcement learning. Journal of Machine Learning Research, 3(Oct):213-231, 2002.
Jonathan D Chang, Masatoshi Uehara, Dhruv Sreenivas, Rahul Kidambi, and Wen Sun. Mitigating
covariate shift in imitation learning via offline data without great coverage. 2021.
Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning.
In Proceedings of the 36th International Conference on Machine Learning, volume 97, pp. 1042-
1051, 2019.
Sayak Ray Chowdhury and Aditya Gopalan. Online learning in kernelized markov decision pro-
cesses. In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 3197-
3205. PMLR, 2019.
Sebastian Curi, Felix Berkenkamp, and Andreas Krause. Efficient model-based reinforcement learn-
ing through optimistic policy search and planning. arXiv preprint arXiv:2006.08684, 2020.
Christoph Dann, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E
Schapire. On oracle-efficient pac rl with rich observations. arXiv preprint arXiv:1803.00606,
2018.
Christoph Dann, Yishay Mansour, Mehryar Mohri, Ayush Sekhari, and Karthik Sridharan. Ag-
nostic reinforcement learning with low-rank mdps and rich observations. arXiv preprint
arXiv:2106.11519, 2021.
Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford.
Provably efficient rl with rich observations via latent state decoding. In International Conference
on Machine Learning, pp. 1665-1674. PMLR, 2019a.
Simon S Du, Sham M Kakade, Ruosong Wang, and Lin F Yang. Is a good representation sufficient
for sample efficient reinforcement learning? arXiv preprint arXiv:1910.03016, 2019b.
10
Published as a conference paper at ICLR 2022
Simon S Du, Sham M Kakade, Jason D Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and
Ruosong Wang. Bilinear classes: A structural framework for provable generalization in rl. ICML,
2021.
Miroslav Dudik, Nika Haghtalab, HaiPeng Luo, Robert E SchaPire, Vasilis Syrgkanis, and Jen-
nifer Wortman Vaughan. Oracle-efficient online learning and auction design. In 2017 ieee 58th
annual symposium onfoundations ofcomputer Science (focs), pp. 528-539. IEEE, 2017.
Fei Feng, Ruosong Wang, Wotao Yin, Simon S Du, and Lin F Yang. Provably efficient exPloration
for reinforcement learning using unsupervised learning. arXiv preprint arXiv:2003.06898, 2020.
Dylan Foster and Alexander Rakhlin. Beyond ucb: Optimal and efficient contextual bandits with
regression oracles. In International Conference on Machine Learning, pp. 3199-3210. PMLR,
2020.
Dylan J Foster, Alexander Rakhlin, David Simchi-Levi, and Yunzong Xu. Instance-dependent
complexity of contextual bandits and reinforcement learning: A disagreement-based perspective.
arXiv preprint arXiv:2010.03104, 2020.
Botao Hao, Yaqi Duan, Tor Lattimore, Csaba Szepesvari, and Mengdi Wang. Sparse feature Selec-
tion makes batch reinforcement learning more sample efficient. In International Conference on
Machine Learning, pp. 4063-4073. PMLR, 2021a.
Botao Hao, Tor Lattimore, Csaba Szepesvari, and Mengdi Wang. Online sparse reinforcement learn-
ing. In International Conference on Artificial Intelligence and Statistics, pp. 316-324. PMLR,
2021b.
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Con-
textual decision processes with low bellman rank are pac-learnable. In International Conference
on Machine Learning, 2017.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement
learning with linear function approximation. In Conference on Learning Theory, pp. 2137-2143.
PMLR, 2020a.
Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? arXiv
preprint arXiv:2012.15085, 2020b.
Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
In Proc. 19th International Conference on Machine Learning. Citeseer, 2002.
Sham Kakade, Akshay Krishnamurthy, Kendall Lowrey, Motoya Ohnishi, and Wen Sun. Infor-
mation theoretic regret bounds for online nonlinear control. In Advances in Neural Information
Processing Systems, volume 33, pp. 15312-15325, 2020.
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-
based offline reinforcement learning. In Advances in Neural Information Processing Systems,
volume 33, pp. 21810-21823. Curran Associates, Inc., 2020.
Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender
systems. Computer, 42(8):30-37, 2009.
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline
reinforcement learning. arXiv preprint arXiv:2006.04779, 2020.
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Provably good batch off-policy
reinforcement learning without great exploration. In Advances in Neural Information Processing
Systems, volume 33, pp. 1264-1274, 2020.
Rui Lu, Gao Huang, and Simon S Du. On the power of multitask representation learning in linear
mdp. arXiv preprint arXiv:2106.08053, 2021.
Thodoris Lykouris, Max Simchowitz, Alex Slivkins, and Wen Sun. Corruption-robust exploration
in episodic reinforcement learning. In Conference on Learning Theory, pp. 3242-3245. PMLR,
2021.
11
Published as a conference paper at ICLR 2022
Horia Mania, Michael I Jordan, and Benjamin Recht. Active learning for nonlinear system identifi-
cation with guarantees. arXiv preprint arXiv:2006.10277, 2020.
Dipendra Misra, Mikael Henaff, Akshay Krishnamurthy, and John Langford. Kinematic state ab-
straction and provably efficient rich-observation reinforcement learning. In International confer-
ence on machine learning, pp. 6961-6971. PMLR, 2020.
Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal. Model-free
representation learning and exploration in low-rank mdps. arXiv preprint arXiv:2102.07035,
2021.
Gergely Neu and Ciara Pike-Burke. A unifying view of optimism in episodic reinforcement learning.
arXiv preprint arXiv:2007.01891, 2020.
Chengzhuo Ni, Anru Zhang, Yaqi Duan, and Mengdi Wang. Learning good state and action repre-
sentations via tensor decomposition. arXiv preprint arXiv:2105.01136, 2021.
Matteo Papini, Andrea Tirinzoni, Marcello Restelli, Alessandro Lazaric, and Matteo Pirotta. Lever-
aging good representations in linear contextual bandits. arXiv preprint arXiv:2104.03781, 2021.
Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging offline rein-
forcement learning and imitation learning: A tale of pessimism. arXiv preprint arXiv:2103.12021,
2021.
Devavrat Shah, Dogyoon Song, Zhi Xu, and Yuzhe Yang. Sample efficient reinforcement learning
via low-rank matrix estimation. Advances in Neural Information Processing Systems, 33:12092-
12103, 2020.
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement
learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140-
1144, 2018.
Yuda Song and Wen Sun. Pc-mlp: Model-based reinforcement learning with policy cover guided
exploration. In International Conference on Machine Learning, pp. 9801-9811. PMLR, 2021.
Aravind Srinivas, Michael Laskin, and Pieter Abbeel. Curl: Contrastive unsupervised representa-
tions for reinforcement learning. arXiv preprint arXiv:2004.04136, 2020.
Adam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin. Decoupling representation learning
from reinforcement learning. In International Conference on Machine Learning, pp. 9870-9879.
PMLR, 2021.
Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based
rl in contextual decision processes: Pac bounds and exponential improvements over model-free
approaches. In Conference on learning theory, pp. 2898-2933. PMLR, 2019.
Masatoshi Uehara and Wen Sun. Pessimistic model-based offline rl: Pac bounds and posterior
sampling under partial coverage. arXiv preprint arXiv:2107.06226, 2021.
Ruosong Wang, Dean P Foster, and Sham M Kakade. What are the statistical limits of offline rl with
linear function approximation? arXiv preprint arXiv:2010.11895, 2020.
Gellert Weisz, Philip Amortila, and Csaba Szepesvari. Exponential lower bounds for planning in
mdps with linearly-realizable optimal action-value functions. In Algorithmic Learning Theory,
pp. 1237-1264. PMLR, 2021.
Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent
pessimism for offline reinforcement learning. arXiv preprint arXiv:2106.06926, 2021.
Lin Yang and Mengdi Wang. Reinforcement learning in feature space: Matrix bandit, kernels, and
regret bound. In Proceedings of the 37th International Conference on Machine Learning, pp.
10746-10756, 2020.
12
Published as a conference paper at ICLR 2022
Mengjiao Yang and Ofir Nachum. Representation matters: Offline pretraining for sequential deci-
sion making. arXiv preprint arXiv:2102.05815, 2021.
Ming Yin, Yu Bai, and Yu-Xiang Wang. Near-optimal offline reinforcement learning via double
variance reduction. arXiv preprint arXiv:2102.01748, 2021.
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea
Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. In Advances in Neural
Information Processing Systems, volume 33, pp.14129-14142, 2020.
Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near op-
timal policies with low inherent bellman error. In International Conference on Machine Learning,
pp. 10978-10989. PMLR, 2020.
Andrea Zanette, Ching-An Cheng, and Alekh Agarwal. Cautiously optimistic policy optimization
and exploration with linear function approximation. arXiv preprint arXiv:2103.12923, 2021a.
Andrea Zanette, Martin J Wainwright, and Emma Brunskill. Provable benefits of actor-critic meth-
ods for offline reinforcement learning. arXiv preprint arXiv:2108.08812, 2021b.
Weitong Zhang, Jiafan He, Dongruo Zhou, Amy Zhang, and Quanquan Gu. Provably efficient
representation learning in low-rank markov decision processes. arXiv preprint arXiv:2106.11935,
2021a.
Xuezhou Zhang, Yiding Chen, Jerry Zhu, and Wen Sun. Corruption-robust offline reinforcement
learning. arXiv preprint arXiv:2106.06630, 2021b.
Xuezhou Zhang, Yuda Song, Masatoshi Uehara, Mengdi Wang, Wen Sun, and Alekh Agarwal.
Efficient reinforcement learning in block mdps: A model-free representation learning approach.
arXiv preprint arXiv:2202.00063, 2022.
13
Published as a conference paper at ICLR 2022
A More Related Work
Here, we mention several additional related works.
Online setting We mention works that tackle representation learning in quite different settings.
Hao et al. (2021b) consider feature selection in sparse linear MDPs with a given exploratory distri-
bution. Zhang et al. (2021a) consider how to choose the best representation among correct repre-
sentations inspired by Papini et al. (2021) (i.e., the MDP is a linear MDP under any representation
in the function class Φ). Thus, it still falls into the linear function approximation setting. In contrast,
we only assume the MDP is linear under some unknown φ? ∈ Φ.
Offline setting In addition to the two work we mentioned, the pessimistic approach in offline RL
has been extensively investigated. Empirically, it can work on simulation control tasks (Kidambi
et al., 2020; Yu et al., 2020; Kumar et al., 2020; Liu et al., 2020; Chang et al., 2021). On the theo-
retical side, pessimism allows us to obtain the PAC guarantee on various models when a comparator
policy is covered by offline data in some forms (Jin et al., 2020b; Rashidinejad et al., 2021; Yin et al.,
2021; Zanette et al., 2021b; Zhang et al., 2021b; Chang et al., 2021). However, these algorithms and
their analysis rely on a known representation and linear function approximation.
We mention works that tackle representation learning from different viewpoints and settings. Lu
et al. (2021) consider multitask representation learning under a generative model assumption. Hao
et al. (2021a) study the feature selection problem in sparse linear MDPs and Ni et al. (2021) study
dimensionality reduction in a given kernel space, under the assumption that the offline data admits
some form of full coverage condition. Shah et al. (2020) studies learning on the assumption that the
optimal Q-function admits a low-rank structure on the generative model setting.
B Proof of the theoretical property of Rep-UCB
Notation We summarize the notations we frequently use. First of all, hereafter, we assume
co,cι,…，are some universal constants, and the notation
f (1/(1 - Y), |A|, ln(1∕δ), ln(∣M∣),d,n) < g(1∕(1 - Y), |A|,ln(1∕δ), ln(∣M∣),d,n)
means there exists some constant c1 > 0, such that
f (1∕(1 - Y), |A|, ln(1∕δ), ln(|M|), d, n) ≤ c1g(1∕(1 - Y), |A|, ln(1∕δ), ln(|M|), d, n)
for any 0 ≤ Y < 1, |A|, ln(1∕δ), ln(|M|), d, n.
We define
n-1
Pn(Sy= - X dpi? (s).
n i=0
With slight abuse of notation, we overload the above notation and use ρn for 1∕n Pin=-01 dπPi? (s, a).
Next, define ρ0n ∈ [S → R] as a marginal distribution of s0 for a triple
(s, a, s0) ~ pn(s)U(a)P?(s0 | s, a).
We define three matrices as follows:
^nXU (A),φ = nEs~Pn,a~U (A)[φ(s,a)φ>(s,a)] + λnI,
*Pn,φ = nE(s,a)^ρn [φ(s, a)φ>(s, a)] + λnI,
∑ n,φ = nE(s,a)~Dn[φφ>]+ λnI.
Note that for a fixed φ, Σn,φ is an unbiased estimate of Σρn×u(A),φ.
14
Published as a conference paper at ICLR 2022
Optimism First, we prove the optimism at the initial distribution. This is proved by using a sim-
ulation lemma inside the learned model which is important since both the bonus and the learned
model use φ. In high level, We will show that the
expected bonus Es,a〜d∏ bn(s,a)
is in the same
order of the expected model error Es,a 〜d∏ ∣∣2(∙∣s,α) - P *(∙∣s,α)∣∣ι. Note that the expectation is
with respect to dP .
Lemma 7 (Almost Optimism at the Initial Distribution). Consider an episode n (1 ≤ n ≤ N) and
set
αn = O(，(|A| + d2) γln(M∣n∕δ)),
λn = O (dln(M∣n∕δ)) ,Zn = O (MMn/δ)
With probability 1 - δ, we have
∀n ∈ [1,…,N], ∀π ∈ π, VPτ,r+bιτ - VP?,r ≥ - λ/(I- T)-IA|Zn.
Proof. In this proof, letting fn(s, a) = ∣∣2(∙ | s, a) - P?(• | s,a)k 1, we condition on the event
∀n, Es〜ρn,a〜U(A) [fn(s,a)] ≤ Zn,	Es〜ρn,a〜U(A)fn(s,a)] ≤ Zn,
∀n, ∀φ, kφ(s,a)k∑-1 φ = θ(kφ(s,a)k∑τ r"八 J
n ,	ρn×U(A),φ
From Lemma 10 and Lemma 17, this event happens with probability 1 - δ. Then, for any policy π ,
from simulation lemma 20,
(I-Y)(VPn,r+bn - VP--T )
=E(s,a)~dp hbn(s,a)+ γEso〜Pn(s,a) [VP>?,r (s0)] - 7Es0-P*(s,a) [VP*,r(s0)]i
& E(s,a)-d¾	min ( αnkφn(s, a)k∑τ	. , 2 ) + YEs，〜Pn(sa) [VP* ,r (s')] - YEs，〜P *(s,a) [VP*,r(s')]
PnL ∖	ρn×U(A),φn	)	,	，
(3)
where in the last step, we replaced the empirical covariance by the population covariance. Note the
notation . is up to universal constants. Here, since kVPπ*,rk∞ ≤ 1 (since we assume trajectory-wise
total reward is normalized between [0, 1]), we have:
忖s,a)〜dp九{Es0〜Pn(s,a) [Vπ*,r (SO)] - Es0〜P*(s,a) [Vn*,r (SO)]} | ≤ E(s,a)〜dp九 {fn(s,a)} .
The above is further bounded by Lemma 11:
|E(s,a)〜d^^ {fn(s, a)} | ≤ E(g,G)〜d^^ kφn(E,G)k∑-1	八 VY J{n|A|Es 〜Pn ,a〜U (A) fn (S,a)]} + 4λnd + 4nζn
+	(1 - Y)|A|Es
~pn,a~U (A) [fn2(S,a)].
Then,
E(s,a)〜dπ^ {fn(s,a)} . PanE(E,G)〜d∏ kφn(g,G)k∑-1	+ p/|A|Zn(1 - Y).	(4)
Pn	Pn	ρn×U(A),φn
where
αfn = γ{n∣A∣Zn + λnd + nZn} . Y (|A| + d2) ln(M∣n∕δ).
NOteWehereUSe fn(s,a) ≤ 2, Es〜ρn,a〜U (A) [fn (s,a)2] ≤ Zn and Es 〜ρn,a〜U (A) [fn(s,a)2] ≤ Zn.
Combining all things together,
忸(s,a)〜d^^ {Eso〜Pn(s,a) [VP*,r(s )] - EsO〜P*(s,a) [Vπ*,r (S0)]O∣ ≤ 2E(s,a)〜d∏， {fn(s,a)}
.PanE⑸G)〜dP kφn(3,G)k∑τ , + p∕(1 - Y)IAIZn
n	Pn×u (A) ,φn
≤ αnE(G,a)〜d∏, kBn(g,&)b-i	. + √(1 - Y)∣A∣Zn,	where。η：=√0n^.	(
Pn	Pn × U(A) ,φn
(5)
15
Published as a conference paper at ICLR 2022
Going back to (3), we have
(I-Y)(VPn,r+bn - VS*,r)
& E(s,af “
min I ankφn(s, a)k∑-ι	. , 2 ) + YES，〜PTl(Sa) [VP* ,r (s0)] - YEs，〜P *(s,a) [VP* ,r(S0)]
∖	Pn×U(A),Φn	J	n ''	'
min ( ankφn(s,a)k∑-ι	, 2 I - min ( ankφn(s,a)k∑-ι	+ √(1 - γ)∣A∣Zn, 2
∖	Pn×U(A) ,φn )	∖	Ρn×U(A),φn
≥ - VZ(I - Y )|A|Zn .
From the second line to the third line, we again use kVPπ*,r k∞ = O(1) and (4). This concludes the
proof.	□
Next, we obtain the upper bound of PnN=0 VPπ**,r - VPπ*n,r . Recall π? is the optimal policy. Though
this form is the same as a standard regret form, since we are not exactly deploying πn in episode n
(recall that we play a uniform action at the end of the episode), we cannot get the regret guarantee.
However, it suffices for the PAC guarantee.
Lemma 8 (Regret). With probability 1 - δ, we have
N
XVPπ**,
n=1
-VPn,r . qnln(1 + N)ln(NMl∕δ)
|A|d2
(I- γ).
>λ	∕'	1~t ∙	∙1	. τ	ri,」	f /	∖	I I 7^∖	/ I	∖	/ I	∖ I I	1 , ♦	. 1
Proof. Similar to Lemma 7, letting fn(s, a) = ∣∣2(∙ | s, a) - P?(• | s, a)kι, We condition on the
event
∀n,	ES 〜ρn,a 〜U (A) [fn(s,a)] ≤ Cn, ∀φ, kφ(s,a)k∑ -1 φ = θ( k φ(s,a) k ∑-1 ,	⑹
n ,	ρn ×U,φ
From Lemma 10 and Lemma 17, this event happens With probability 1 - δ .
For any fixed episode n and any policy π, We have
Vπ*,r - VPnr
≤ VXr+b“ - VPn,r + P∣A∣Zn(1 - Y)-1	(Lemma7)
≤ VPn,r+bτι - VPn,r + P|A|Zn(1 - Y)-1	(πn = argmax∏ VPn,r+b∕
=(1 - Y )-1E(s,a)〜dPn [bn(s,a)+ YEPn(S0∣s,a)[VPn,r+bn (S0)] - YEP *(s，Ιs,a) [々r+bn (C]] + p∣A∣Zn(1 - Y )-1 .
We use the 2nd form of simulation Lemma 20 in the last display.
Then, noting IlbnI∣∞
expansion, We have
(VPπ**,r - VPπ*n,r )
≤ 2, Wehave kVPn,r+bJ∞
≤ 2∕(1 - Y). Combining this fact With the above
≤ (I- y)	E(s,a)〜dpn [bn(s, a)] +
'----------{z----------}
(a)
((T⅛) E(s,a)〜dPn fn(s,a)], + P∣A∣Zn(1-Y)-1 . (7)
(b)
First, we calculate the first term (a) in Inequality 7. Following Lemma 12 and noting the bonus bn
is O(1), We have
E(s,a)〜dpn [bn(s,a)]
min ( αn∣φn(s,a)k∑-ι	, 2
∖	Pn×U (A),Φn
( From (6))
+ S IAjanES~pn,a~U (A)
-1
ρn,φ*
jnY|A|anEs 〜Pn ,a 〜U (A)
kφn(s,a)k∑-ι .
Pn × U (A) ,φn -
+ dYλn
-1
ρn ×U (A),φn
(1 - Y).
16
Published as a conference paper at ICLR 2022
Note that we use the fact that B = 2 when applying Lemma 12. In addition, we have
nEs-Pn,a-U (A) kφnGa)k∑-ι	.	= n Tr(EPn×U (A)[φ>nφI]{nEPn×U (A)[φnφ>]+ λnI }-1) ≤ d
_	Pn×U(A) ,φn _
Then,
E(s,a)〜dPn [bn(s, a)] ≤ ⅜,a)〜dpn kφ?氏训Σ-1 / PYdlAlαn + Ydλn + PdAIan (I - Y)/n
Second, we calculate the term (b) in inequality 7. Following Lemma 12 and noting fn2 (s, a) is
upper-bounded by 4 (i.e., B = 4 in Lemma 12), we have
E(s,a)〜dPn? [fn(s, a)]
≤ E⑸G)〜dPn kΦ?(S, a)kΣ-1 短《{n|A|YEs〜ρn,a〜U(A) [/nGa)]} +4Yλnd
+	|A|Es
~pn,a~U (A)[fn2(s,a)(1-γ)]
≤ E(G,G)〜dπn kφ?氏 G)k∑-1 * Pn Alγζn + 4γλnd + PAIZn (I - Y)
P	ρn,φ?
≤ E(G,G)〜dπn kφ?(3, G)k∑-1 *αn + PAIZn(I- Y),
P	ρn,φ?
where in the second inequality, We use Es〜ρn,a〜U(a) [fn(s, a)] ≤ Zn, and in the last line, recall
√7√n∣AlZn + λnd + nZn . an.
Then, by combining the above calculation of the term (a) and term (b) in inequality 7, we have:
Hr- v∏n,r. zɪʒ (E(G,a)〜dPn kΦ*(g,G)k∑-ι * Pd∣A^an+dλn +、∕dlAlαn(I-Y))
(1 - Y)	P	ρn,φ?	n
+ (1 - γ)2 (E(G,a)〜dpn kφ*(3,G)k∑-∖* αn + P|A|Zn(1 - γ)) ∙
Hereafter, we take the dominating term out. First, recall
an . P{∣A∣ + d2}ln(NM∣∕δ)) . PAId2ln(N∣M∣∕δ).
Second, we also use
N
X E(G,a)〜dπ* kφ?⑸ a)k∑-匚*
P	ρn ,φ*
n=1
uN
≤ t N X E(G,a)〜dPn [Φ?⑸ a)>∑-1,φ*Φ*(G,G)]
n=1
(CS inequality)
N
lndet(E E(G,a)〜dpn [φ?(S, a)φ?(S, a)>]) - lndet(λιI)
n=1
(Lemma 18 and λι ≤ ∙∙∙ ≤ Xn)
(Potential function bound, Lemma 19 noting ∣∣φ*(s, a)∣∣2 ≤ 1 for any (s, a).)
17
Published as a conference paper at ICLR 2022
Finally,
XXX VP*,r- VPn,r . (ɪ) (jdNln (1 + dN) qd∖A∖αN + dλN + X r
d∖A∖αn(ι - γ)
n
+ D
dNln (1 + dN)αN + Xq P∖A∖Zn(1 - Y)
(Γ⅛ ∖∕dN ln (1+dNι) qd∖A∖αN+ (Γ⅛ jdN ln (1+W) αN
(Some algebra. We take the dominating term out.)
dN ln
∖A∖d3/2 ln1/2(N∖M∖∕δ)
(1-τP
This concludes the proof.
□
Using Lemma 8, we can immediately obtain the PAC guarantee.
Theorem 9 (PAC guarantee of REP-UCB). By interacting with the environment for a number of
steps at most
N log(N∕δ), N = O (dT^ln2(1 +
d4∖A∖2 ln(∖M∖∕δ)))
(1-Y )5e2	))'
with probability 1 一 δ, we can ensure Vp? 『 一 Vp?『≤ e.
Proof. From Lemma 8 and Lemma 22, when N is
O (d4∖A∖2 ln(∖M∖∕δ) , 2 (Il d4∖A∖2 ln(∖M∖∕δ)))
I (1-Y)4e2	I +	(1-Y)4e2	)) ,
with probability 1 - δ, we can ensure
1N ?
NEVP*,r - VPn,r ≤ J
n=1
With probability 1 -δ, we need (1 -γ)-1ln(1∕δ) interactions with the environment to get one tuple
(s, a, s0, a, s) f⅛om one roll-in of π. Thus, the total sample complexity is O(N(1 一 Y)-1 ln(N∕δ)).
□
Next, we provide an important lemma to ensure the concentration of the bonus term. The version
for fixed φ is proved in Zanette et al. (2021a, Lemma 39). Here, we take a union bound over the
whole feature φ ∈ Φ. Recall
ρn (∙)
1 n-1
n
i=0
Lemma 10 (Concentration of the bonus term). Set λn = Θ(dln(n∖Φ∖∕δ)) for any n. Define
n-1
∑iρn,φ = nEs^ρn,a^U (A)[φ(s,a)φ>(s,a)] + λnI, ς n,φ = X φ(s ⑺,a(i) )φ> (s(i), a(i) ) + λnI.
i=0
With probability 1 一 δ, we have
∀n ∈ n+, ∀φ ∈ φ, cιkφ(s, a)k∑-ι	wkθ(s,a)ia-1 ≤ c2kφ(s,a)k∑-ι	.
ρn ×U (A),φ	n,φ	ρn ×U (A),φ
18
Published as a conference paper at ICLR 2022
For any g ∈ S ×A → R, The next lemma shows that E(s,a)〜d∏ {g(s, a)} can be upper-bounded
using E(s,a)^d¾ kΦn(g,a)k∑-ι . as long as we have the convergence guarantee for
Pn	Pn,Φn
Es〜Pn,a〜U(A) [g (s, a)] and Es〜ρn,a〜U(A) [g (s, a)].
Lemma 11 (One-step back inequality for the learned model). Take any g ∈ S × A → R such that
kgk∞ ≤ B. We condition on the event where the MLE guarantee (17):
Es〜Pn,a〜U(A) [fn(s, a)] . Zn,
holds. Then, for any policy π, we have
lE(s,a)〜d]^ {g(s,a)}|
≤ EGs,a)-dπ, kφ,n(s, a)k∑Tχcr(⑷& q {n|A|Es 〜ρn,a 〜U (A) [g2(s, a)]} + B2λnd + nB Zn
+ J(1 - Y)|A|Es〜Pn,a〜U(A) [g2(s, a)].
>
Recan，pnxu(A),φn = nEs^ρn,a^U(A) [φn(s, a)φn (S, a)] + λnI.
Proof. First, we have an equality:
E(s,a)〜dp {g(s, a)} = YE(5,a)〜d∏ ,s〜PnR,a),α〜∏(s) {g(s, a)} + (1 - Y)Es〜do,a〜π(so) {g(s, a)},
Pn	Pn
(8)
The second term in (8) is upper-bounded by
(1 - Y) Jmax 处I：； 1 s' Es〜ρn,a〜U(A) [g2 (s, a)]
(s,a) ρn(s)u(a)	n
≤ (1 - Y )1/max /id0，?:，： |S)、Es 〜ρn,a 〜U (A) [g2 (S,a)] ≤
(s,a) (1 - Y)d0(s)u(a)	n
(1 - Y)|A|Es
~pn,a~U (A) [g2(s, a)].
Next we consider the first term in (8). By CS inequality, we have
E(i,a)-dπ^ ,s〜Pn(s,a),a〜∏(s)
Pn
>
{g(s, a)} = E(g,a)〜d^ φn (s, a)
μ Eμn(s)π(a |
a
s)g(s, a)d(s)
,-πτ>	Il ? (~ ~∖∖∖
≤ E(g,G)〜d^^ k φn (s, α) k ∑-1	,
Pn	Pn×U(A),Φn
I y^μn(s)π(a | s)g(s, a)d(s)
aΣ
Pn ×U(A),φn
Then,
k / Eμn(s)π(a | s)g(s,a)d(s)k∑	^
Pn ×U (A),φn
a
X μn(s)π(a | s)g(s,a)d(s)} nnEs^ρn,a^U (A) [φn φ> ]+ λnI} / X μn(s)π(a | s)g(s,a)d(s) ।
≤ nEg〜ρn,G〜U(A)
/ X μn(s)>Φn(3,G)∏(a | s)g(s,a)d(s)
> + B2λnd
(Use the assumption ∣∣ Pa π(a | s)g(s, a)k∞ ≤ B and / ∣∣μn(s)h(s)d(s)k2 ≤ √d for any h : S → [0,1].)
nEg~Pn,G~U (A) {Es 〜Pn(g,g),a 〜∏(s) [g(s,a)]}	+ B λnd
≤ nEs^ρn,a^U(A) [{Es〜P*(g,g),a〜∏(s) [g(s, a)]} ] + B λnd + nB Zn
≤ nEg~ρn,G~U(A),s~P*(g,G),a〜∏(s) [g2(s,a)] + B2λnd + B2nZn.
≤ nIAI {Eg〜ρn,a〜U(A),s〜P*(g,a),a〜U(A) [g2(s, a)] } + B2λnd + B2nζn
≤ n∣A∣Es〜ρn,a〜U(A) [g2(s,a)] + B2λnd + Β2nZn.
(MLE guarantee)
(Jensen)
(Importance sampling)
(Definition of ρ0n)
19
Published as a conference paper at ICLR 2022
Then, the final statement is immediately concluded.
□
Below, we show a similar lemma as Lemma 11. The difference is we aim for calculating
E(s,a)〜d∏? {g(s, a)} instead of E(s,a)〜d∏ {g(s, a)} . For any g ∈ S ×A→ R, this lemma shows
that E(s,a)〜djι* {g(s, a)} can be upper-bounded using E(Ma)〜4n* ∣∣φ*(E, G)∣∣ς-i as long as We
P	P	Pn,Φ*
have the convergence guarantee for Es〜ρn,a〜U(A) [g2(s, a)]. Note comparing to Lemma 11, this
is not a probabilistic statement. Note that ∣∣φ*(s,a)k∑-ι	is the usual elliptical potential function
ρn,φ*
under the fixed representation φ? .
Lemma 12 (One-step back inequality for the true model ). Take any g ∈ S × A → R such that
kgk∞ ≤ B. Then,
E(s,a)〜dp? {g(s,a)}≤ E(S①〜dp*kΦ*(g,a)k∑-L √Y√n∣A∣Es〜Pn,a〜U(A) [g2(S,a)] + λndB2
P	P	ρn ,φ?
+ Qq-y)∣a∣Es~pn,a~U (A) [g2 (s, a)].
Recall ∑ρn,φ? = nE(s,a)〜ρn[φ?(s, a)φ?(s, a)>] + λnI.
Proof. First, we have
E(s,a)〜dP* {g(s, a) } = γE(a,a)〜dP* ,s〜P *(a,a),a 〜π(s) {g(s, a) } + (I - Y)Es 〜do,a 〜π(so) {g(s, a) } .
(9)
The second term in (9) is upper-bounded by
do(s)π(a | S)	Q	/	~τ~	~~	^
(I - Y)Vmax P (S)U(a) Es〜Pn…(A) [g2(s,a)] ≤ M|AIEs〜ρn,a〜U(A) [g2(s,a)](1 - Y).
By CS inequality, the first term in (9) is further bounded as follows:
E(a,a)〜dP*,s〜P*(a,a),a〜∏(s) {g(s,a)} = E(a,a)〜dP*φ*(s, a)> / £〃？(s)n(a | s)g(s, a)d(S)
a
*	X
μ?(s)π(a ∣ s)g(s, a)d(s)
a
Σρn,φ*
Here, we have
k X Xμ*(s)π(a | s)g(s,a)d(s)k∑ φ*
ρn,φ
a
Xμ?(s)∏(a I s)g(s,a)d(s)} {nE(s,a)〜。n[φ?(s,a){φ*(s,a)}>] + λιτI} {Z Xμ?(s)π(a | s)g(s,a)d(s)}
Xμ?(s)>φ*(3,G)π(a | s)g(s, a)d(s)
a
Z + λn dB2
≤ n {E(a,a)〜ρn,s〜P*(a,a),a〜∏(s) [g (s, a)]} + λndB .
Therefore,
n {E(a,a)〜Pn,s〜P*(a,a),a〜∏(s) [g (s, a)]} + λndB
≤ n|A| {E(a,a)〜ρn,s〜P*(a,a),a〜U(A) [g2(s, a)] } + λndB2
≤ n|A| { Y Es〜ρn,a〜U (A) [g2(s, a)] } + λndB2 .
(Jensen)
(Importance sampling)
20
Published as a conference paper at ICLR 2022
In the last line, we use the following inequality:
Es~Pn,a~U(A) [g (S, a)]
=γE(g,G)~ρn,s~P*(g,a),a~U(A) [g2(s, a)] + (I - Y)Es0~d0,a~U(A) [g2(s, a)]
≥ YE(g,G)~ρn,s~P*(g,a),a~U(A) [g2(s, a)].
Finally, we have
E(s,a)~dp? {g(s,a)} ≤ E(g,G)~dp? kφ?氏ek∑-∖*√γJ{n|A|Es~Pn,a~U(A) [g2(S,a)]} + λndB
+	|A|E
s~pn,a~U (A) [g2(S,a)](1 -γ).
This concludes the proof.	口
C Proof of the theoretical property of Rep-LCB
This section provides the detailed proofs for our results in the offline setting.
Below We first prove that Vπ∏ T ^ is an almost pessimistic estimator of V∏? 丁.
Lemma 13 (Almost Pessimism at the Initial Distribution). Let ω = maxa,s 1∕∏b(a | S). Set
α = cιP(ω + d2) Yln(∣M∣∕δ),	λ = O(dln(∣M∣∕δ)),Z = O (In(M加)
With probability 1 - δ, for any policy π, we have
I∕∏	I∕∏	< rω(1 -Y)Tln(IMDδ)
VP,r-b — VP?,r ≤ V --------n----------.
Proof. We define
Σρ,φ = nE(s,a)~ρ[φφ>]+ λI, Σ φ = nED[φφ>] + λI.
where λ = O(dln(∣M∣∕δ)). In this proof, letting f(s,a) = IlP(T s, a) - P?(• ∣ s,a)kι, we
condition on the events:
E(s,a)~ρ[f2(s, a)] ≤ Z,	∀φ ∈ φ : kφ(s, a)k∑-1 = θ(kφ(s,a)k∑T).	(IO)
φ	ρ,φ
where Z = O(ln(∣M∣∕δ)∕n). From the offline version of Lemma 17 and Lemma 10 6, this event
happens with probability 1 - δ .
Then, from simulation lemma (Lemma 20),
(I-Y)(Vπ,r-b - VP?,r )
=E(s,a)~dp h-b(s,a) + YEso~P(s,a) [VP?,r (s')] - YEs，~P*(s,a) [VP?,r (s')]]
.E(s,a)~dπ3	- min	(αkφ(S,α0k∑-1.,	2)	+	Y Eso~P(s,a)	[VPπ*,r(SO)]	-	YEs0~P *(s,a)	[VP* ,r (SO)]
PL	p,φ
(From (10))
Here, we have
忸(s,a)~dp {Eso~P(s,a) [VPπ*,r (S0)] - Es0~P*(s,a) [Vn*,r (S')]} | ≤ E(s,a)-dP {f (S,a)},
noting IVPπ*,r I∞ ≤ 1. This is further bounded by Lemma 15:
E(s,a)~d∏ {f (s, a)} . √O7E⑸G)~d∏ ∣∣φ(s, a)k∑-ι + pωZ(1 - Y).	(11)
' ''P	' ' ' p	p,Φ
6We can remove ln n since n is fixed in the offline setting.
21
Published as a conference paper at ICLR 2022
where
α = nγωZ + γ2λd + YInZ . (ω + d2) Yln(∣M∣∕δ).
Here, we use f (s,a) ≤ 2 in Lemma 15 and E(s,a)〜ρ[f 2(s,a)] ≤ Z.
Thus,
∣E(s,a)〜dP {Es，〜P(s,a) [VP*,r(S0)] - Es，〜P *(s,a) [VP* ,r 3)]} | ≤ E(s,a)〜dP {f(s,a)}
≤ √α0E(g,a)〜α/向氏向卜-1 + pωζ(1 - γ)
=αE(g,G)〜d^ ∣∣φ(S, a)k∑-1 + √ωζ (1 - Y), α = √α0.
Going back to the simulation lemma 20, we have
(1 - γ)(VPn,r-b - VPπ*,r )
.E(s,a)〜dp - min (αkφ(S,a)k∑-1^ , 2) + Es，〜P(s,a) [VPπ*,r (s0)] - Es，〜P *(s,a) [VPπ*,r (s0)]
≤ E(s,a)〜dj, — min (α∣φ(s, a)∣∑-ι, 2)+ min (α∣φ(s, a)∣∑-ι + ,ωZ(1 - Y), 2)
≤ Pωζ(1 - Y).
This concludes the proof.	□
With the above lemma, now we can proceed to prove the main theorem.
Theorem 14 (PAC guarantee of REP-LCB). Set the parameters as in Lemma 13. With probability
1 - δ, for any comparator policy π including history-dependent non-Markovian policies, we have
Vπ	Vπ	V	ωd2 r C ln(∣M∣∕δ)
VP*,r - VP*,r 〜(1 - γ)2 V	n ,
where Cπ? is the relative condition number under φ? :
C? >_ SUP x>E(s,a)〜dp* [φ*(S, a){φ*(S, a)}>]x
π ' χ∈R x>E(s,a)〜ρ[φ*(s,a){φ*(s,a)}>]x .
Proof. In this proof, letting f (s, a) = ∣P(∙ | s, a) 一 P*(∙ | s, a)∣ι we condition on the events:
E(s,a)〜ρf2(S,a)] ≤ Z, ∀φ ∈ φ : kφ(s,a)k∑-I = θ(kφ(s,a)k∑-ι ).	(12)
φ	ρ,φ
From Lemma 10 and Lemma 17, this event happens with probability 1 - δ .
For any policy π , we have
VPπ*,r - Vπ*,r
≤ Vp*r 一 VP,— + PωZ(1 一 Y)T	(Lemma 13)
≤ VPπ*,r - VP	b + PωC(1 - Y)T
,r-
.(1 一 Y)T E(s,a)〜dp* [b(S, a)] + ( 1 _ ) E(s,a)〜dp* [f (S, a)] +PωC(I - Y)-1.
--{Z}	1 一 Y '-}
(a)	(b)
Recall f(S,a) = ∣∣P(∙ | S,a) — P?(∙ | s, a)∣1.
From the second line to the third line, note though ∏ is the argmax over Markoovian polices, ∏ is
also the argmax over all history-dependent polices. In the last line, we use a simulation lemma 20,
which is tailored to a time-inhomogeneous policy. We here use ∣∣V^∏ T ^k∞ ≤ 2/((1 - Y)). noting
,
IlbI∣∞ = O(1).
22
Published as a conference paper at ICLR 2022
We further calculate the first term (a). Considering 16 and noting kbk∞ ≤ 2, we have
E(s,a)〜dp* [b(s,a)] . E(g,a)〜dp* kφ*(s,s)k∑-1 * Jnω {γE(s,a)〜P [^Ga)]} + Yλd
+ Pω(1- Y){EP[b2(s,a)]}1/2.
From (12), we have
nE(s,a)〜P [b (s, a)] ≤ nE(s,a)〜P
≤ Tr[nE(s,a)〜ρ[φφ>]{nE(s,a)〜ρ[φφ>] + λI} 1
≤ Tr[n(E(s,a)〜ρ[φφ>]+ λI){nE(s,a)〜ρ[φφ>]+ λI} 1] ≤ d.
Thus,
-1
^ 工
ρ,φ
(13)
(14)
(15)
E(s,a)〜dp*[b(s,a)] ≤ E(g,a)〜dp* kφ*(g,50k∑-1 * Pωda2Y + Yλd + J
n
Second, we further calculate the second term (b). Considering the offline version of Lemma 12 and
noting f2(s, a) is upper-bounded by 4,
E(s,a)~dp* [f (s, a)]
=ER, a)〜dp* kφ? (s, a0kΣ-1 * qnω { YE(s,a)〜P [f 2 (s, a)] } + 4Yλd + JωE(s,a)〜P [f2(s, a)] (1 - Y)
.E(g,a)〜dp* kφ?(S, a)∣∑-1 * Pω {nγζ} + γλd + PωC(I - Y)
.E(g,a)〜dp* kφ*(S, a)k∑-1 *α + Pωζ(1-γ).
In the final line, recall ,ω {nγZ} + γλd + YnZ ≤ α.
Finally, by combining the calculation of the first term (a) and the second term (b), we have
—
1
VP*,r . L
α
E(5,a)〜dp* kφ*(g,别卜-1 ? Pda2ωγ + γλd + J
ωα2d(1 - γ)-1
n
+ (1-Y2
E(s,a)~dp*kφ?(S,s)k∑-,φ* + j (1 -:)3
Now, We use the fact E(g,a)〜d∏* ∣∣φ*(3, a)∣∣∑τ* is upper-bounded as
E(g,a)~dP*kΦ*(g, a)k∑-l * ≤ rE(Z0∑P*mS'⅛Γ ≤ 卜 E(s,a)~ρkφ*(g,a)k∑-1*
(Refer to Lemma 21)
(From (13))
Finally, we have
□
23
Published as a conference paper at ICLR 2022
The lemma below is a key technical lemma for our proof. It shows that one can relate the expected
value of any function f (s,a) with respect to dP (i.e., inside the learned model P) to the potential
function With respect to dP, i.e., E(s,a)〜d∏ ∣∣φ(s, 0)k∑-ι,. Pairing φ and P is important since P is
the low-rank transition model defined using φ. As we have seen in the above analysis, when using
the lemma below, we instantiate f (s,a) := ∣∣P (∙∣s,α) — P *(∙∣s,α)∣∣ι.
Lemma 15 (One-step back inequality for the learned model in offline setting). Take any f ⊂ S ×
A → R s.t. ∣f∣∞ ≤ B. We condition on the event where the MLE guarantee holds:
E(s,a)〜P∣P(∙ I s,a) - P?(∙ I s,a)k2 . Z.
Then, letting ω = maxs,α(1∕∏b(a ∣ S)), for any policy π, we have
|E(s,a)〜dj, {fGa)} | ≤ E(g,a)〜dj, llφ(s, a)k∑-1 q {nωE(s,a)-ρ [f 2(s, a)]} + γ2 λdB2 + nγ2 zb 2
+ J ωE(s,a)〜P [f 2(s, a)] (I - Y).
Proof. First, we have an equality:
E(s,a)〜dP {f (s, a)} = YER㈤〜d^“P(s,…(S) {f (s, a)} + (1 -Y)Es〜d0,a〜∏(s0) {f (s, a)}.
(16)
The second term in (16) is upper-bounded by
Es〜do,a〜π(so) {f (S, a)} ≤ Es〜do,a〜π(so){f2(s,a)}}1/2 = JωE(s,a)〜P [f2(s,a)]∕(1-γ).
Next we consider the first term in (16). By CS inequality, we have
E
(s,a)〜d^^ ,s〜P(s,a),a〜π(s)
{f(s,a)}∣ = E(Ma)〜dj, φ(s,a)τ X X μ(s)π(a | s)f (S, a)d(S)
a
≤ E(J,a)-d¾ ∣∣φ(5,5)k∑-1 k X X μ(s)π(a |
P PaJ a
s)f(s,α)d(S)k∑ρ,φ.
Then,
Il /R(S)n(a |S)f(S,a)d(S, a)∣∑ρ,φ
X R(S)n(a | S)f(S,a)d(S)) {nE(s,a)〜ρ[φφ>] + λI O ( Z X 4(S)n(a | S)f(S,a)d(S))
≤ nE(g,a)〜P
X X μ(S)>φ(s, a)π(a | S)f (S,a)d(S)
a
2I +B2λd
(Use the assumption ∣∣ Pa f (s, a)∣∞ ≤ B and ∣∣ Rμ(S)h(S)d(S)∣2 ≤ √d for h : S → [0,1].)
nE(g,a)~p{Es〜p(E,G),a〜∏(s)[
nE(g,a)〜ρ {Es 〜P ? (g,G),a〜π(s)
≤ n {E(g,a)〜ρ,s〜P?(s,a),a 〜π(s)
[f(S, a)]2} + B2λd
) [f(S, a)]2} + B2λd + nB2ζ
(MLE guarantee and ∣Ea 〜∏
f2(S,a)} + B2λd + nB2ζ.
(∙)[f2(∙,a)]∣∞ ≤ B2.)
(Jensen)
Finally, the first term in (16) is upper-bounded by
n {E(g,a)〜ρ,s〜P*(g,a),a〜π(s) [f (s, a)]} + λdB + nB Z
≤ nω {E(g,G)〜ρ,s〜P*(g,a),a〜∏b(s) [f (s, a)]} + λdB + nB Z
≤ nω [1 E(s,a)〜p [f2(S, a)] } + λdB2 + nB2Z.
(Importance sampling)
(Definition of ρ)
24
Published as a conference paper at ICLR 2022
In the last line, we use the following equality:
E(s,a)〜ρ [f (S, a)] = γE(g,G)〜ρ,s〜P*(g,a),a〜∏b(s) [f (S, a)] + (I - Y)Es~d0,a~∏b [f (S, a)] .
Based on the above discussion, the final statement is immediately concluded.
□
We can prove the similar inequality for the true model. The proof is omitted since it is quite similar
to the one of Lemma 15.
Lemma 16 (One-step back inequality for the true model in offline setting). Take any f ⊂ S × A →
R s.t. kf k∞ ≤ B. Then, letting ω = maxs,α(1∕∏b(a | S)),foranypolicy π, we have
∣E(s,a)〜dP? {f(s, a)} I ≤ E(g,a)~dP*kΦ* (S, a)k∑-1 ? /{ns%")〜P [f2(s,a)]} + γ2λdB2
+ /ωE(s,a)〜P [f2(s, a)] (1 - Y).
D	Auxiliary lemmas
First, we present the MLE guarantee. Regarding the proof, refer to Agarwal et al. (2020b, Theorem
21). Note Pn and ∏n are the quantities appearing in the proposed online algorithm. We can also
immediately obtain the statement to the offline case.
Lemma 17 (MLE guarantee). For a fixed episode n, with probability 1 - δ,
EsY0.5ρn+0.5ρn},a〜U(A)[kPn(∙ I s,a) — P?(∙ I s,a)k2] . Z, Z := In(M"δ).
As a straightforward corollary, with probability 1 - δ,
∀n ∈ N+, Es〜{0.5Pn +0.5ρn},a〜U(A)[kPPn(∙ | s, a) -产(，| s, a)k2] .。方金，Zn = ―n1	-
(17)
The following is a standard inequality to prove regret bounds for linear models. Refer to Agarwal
et al. (2020a, Lemma G.2.)
Lemma 18. Consider thefollowingprocess. For n = 1,…，N, Mn = Mn-ι+Gn with Mo = λ°I
and Gn being a positive semidefinite matrix with eigenvalues upper-bounded by 1. We have that:
N
2 ln det(MN) - 2lndet(λ0I) ≥ XTr(GnMn--11).
n=1
Lemma 19 (Potential function lemma). Suppose Tr(Gn) ≤ B2.
NB2
2lndet(MN) — 2lndet(λ0I) ≤ dln ( 1 +-------).
dλ0
Proof. Let σι, ∙∙∙ ,σd be the set of singular values of MN recalling MN is a positive Semidefinite
matrix. Then, by the AM-GM inequality,
ln det(MN)/ det(λ0 I) = lnY! (σi∕λo) ≤ ln d(d XT (σi∕λo))∣
i=1	i=1
Since We have Pi σi = Tr(MN) ≤ dλo + NB2, the statement is concluded.	□
Lemma 20 (Simulation lemma). Given two MDPs (P0, r+b) and (P, r), for any policy π, we have:
VP0,r+b - VP,r = J — γ E(s,a)〜dp0 [b(s,a)+ γEP0(s01 s,a) [QP,r ( s0,π)] -YEP(s0 |s,a) [QP,r (s0,π)]]
and
VPπ0,r+b - VPnr = J — γ E(s,a)〜dp [b(s,a)+ γEP0(s01 s,a) [QP,r+b (s0,π)] -YEP (s0∣s,a)[QP 0,r+b (s0,π)]] ∙
25
Published as a conference paper at ICLR 2022
Proof. We use
Vπ - f (S0,π) = 1γEdP [r(s, a) + YEP(s0∣s,a) [f (s0, π)] - f (S, a)]]
Then,
V∏0 ,r+b - VPnr = 1 - γ E(s,a)~dp 0 [r(s, a) + b(S, a) + γEP 0(s0∣s,a) [Q∏,r (s0,π)] - QP,r(S,a)]]
=1--γ E(s,a)~dp 0 [b(s,a) + YEP0(s0∣s,a)[QP,r (S0,n)] - Y EP (s，| s,a) [QP,r (Sin)]] ∙
Similarly,
VPnr - VPπ0,r+b = ι - γ E(s,a)~dp [r (S, a) + Y EP (s0∣s,a)[QP 0,r+b(S0,n)] - QP 0,r+b (S, a)]]
=ι - γ E(s,a)~dp [ — b(s, a) + Y EP (s0∣s,a) [QP 0,r+b (S0, n)] - YEP0(s0∣s,a) [QP,r (S0, n)]].
□
The following lemma is used to deal with the distribution shift in the offline setting. For the proof,
refer to Chang et al. (2021).
Lemma 21 (Distribution shift lemma). Consider any policy π and state-action distribution ρ, and
any representation φ?, we have:
E(s,a)~dP* [Φ*(S, a){Φ*(S, a)}>] ≤ C*Eρ[Φ*(S, a){Φ*G a)}>],
x>E(s，a)~dp*[O?{O?}>]x
X∈R X>E(s,a)~ρ[Φ?{Φ?}>]x
This is some auxiliary lemma to convert the finite sample error bound into the sample complexity.
Lemma 22 (Conversion of finite sample error bounds into sample complexities). By taking
N = 1∕e'2 × ln2(1 + 1∕e02), e' =	. 1/2,-----Vi 1/2<-------7.
a1 ln1/2(e + a2) ln1/2(e + a3)
It satisfies
α1P1∕Νln1/2(1 + a2N)ln1∕2(1 + a3N) < ce.
where c is a constant independent of a1 , a2 , a3.
Proof. We first have
a1 P1∕Νln1∕2(1 + a2N) ln1∕2(1 + a3N) ≤ a1 max(ln1/2(1 + a2) ln1/2(1 + a3), 1)P1∕Νln(1 + N).
Here, we use
ln1/2(1 + a2N) ≤ {ln(1 + a2) + ln(1 + N)}1/2 ≤ pmax(1, ln(1 + a2))ln(1 + N).
Then, we prove when N = 1∕2 × ln2 (1 + 1∕2 ).
P1∕Nln(1 + N) < e.
This is proved by
P1∕Nln(1 + N) ≤ e ×
ln(1 + 1∕e2 × ln2(1 + 1∕e2))
ln(1 + 1∕e2)
≤×
ln(1 + 1∕e2)+ln(1+ln2(1 + 1∕e2))
ln(1 + 1∕e2)
≤ +×
≤ +×
ln(1 + ln2(1 + 1∕e2))
ln(1 + 1∕e2)
0.5{1 + ln2(1 + 1∕e2))}1∕2 - 1
ln(1 + 1∕e2)
. .
From the third line to the fourth line, we use ln(x) ≤ 0.5(x1∕2 - 1) for x > 0. Then, the final
statement is concluded.	口
26
Published as a conference paper at ICLR 2022
E More comparison to Xie et al. (2021)
We briefly explain the guarantee when we use Algorithm 1 (Xie et al., 2021). For a given reward r,
we first define a new feature class Φr+ .
Definition 23 (Augmented feature). Let φ = [φι, .…，φd].
φ+ = {φ+; φ ∈ φ}, φ+ = [φ1,…，φd,r].
Then, we set
F = {a>φ+ ∣∣∣a∣∣2 ≤ c√d + 1, φ+ ∈ Φ+}.
where c is some suitable constant. Given the hypothesis class F for the Q-function, we can run
Algorithm 1 in (Xie et al., 2021). We denote the output policy as ∏.
We check two assumptions to ensure the algorithm works. The first assumption is realizability. This
is satisfied since for any policy ∏ ∈ Π (Π is the class of all Markovian polices), We have QP? r ∈ F.
The second assumption is completeness. This is also satisfied since TPπ? ,rF ⊂ F for any policy
∏ ∈ Π where T∏? Ir is a Bellman-operator s.t.
TP?,r : {S×A→ R}3 f → r(s, a) + γEs,〜P?3.)[f (s0, π)] ∈{S×A→ R},
where we denote f (s, ∏) = Ea〜∏(s)f (s, a). Then, by invoking their Corollary 5, we have
Theorem 24 (PAC bound based on Xie et al. (2021)). With probability 1 - δ,
∀∏ ∈ Π: vn?r - VPLr
≤c
(1-γ)2
(d + 1)log(1∕δ) log |A| [1/5
n
where
C∏,r
sup sup
φr+ ∈Φr+ a∈Rd+1
a>Edp*[φ+m+∏>]a
a>Eρ[φ+{φ+}>]a
We compare the above result with our result in Theorem 6. First, since Cr includes r and all pos-
sible features in Φ, this partial coverage condition is stronger than ours (recall our partial coverage
condition is only related to the true representation φ?), and we always have Cπ? ≤ Cn r. Secondly,
the dependence on n is much worse. Third, it is unclear whether the learned policy can compete
against any history-dependent policy. Recall in Theorem 6, we show that our algorithm can compete
with any history-dependent policies.
27