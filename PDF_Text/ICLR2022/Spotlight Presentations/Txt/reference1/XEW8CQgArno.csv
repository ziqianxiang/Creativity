title,year,conference
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, In Jennifer G
 A mathematical model for automatic differentiation in ma-chine learning,2020, In H
 TheLoss Surfaces of Multilayer Networks,2015, In Guy Lebanon and S
 Optimization and Nonsmooth Analysis,1983, Wiley New York
 Stochastic subgradientmethod converges on tame functions,2020, Foundations of Computational Mathematics
 Neocognitron: A self-organizing neural network model for a mechanism ofpattern recognition unaffected by shift in position,1980, Biological Cybernetics
 Implicit bias of gradient de-scent on linear convolutional networks,2018, In S
 Deep relu networks have surprisingly few activation pat-terns,2019, In H
 Deep residual learning for imagerecognition,2016, In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE Conference on Computer Vision and Pat-tern Recognition
 The low-ranksimplicity bias in deep networks,2021, arXiv
 Gradient descent aligns the layers of deep linear networks,2019, In In-ternational Conference on Learning Representations
 Directional convergence and alignment in deep learning,2020, InH
 Deep learning without poor local minima,2016, In D
 Deep learning,2015, Nature
 Gradient descent maximizes the margin of homogeneous neural net-works,2020, In International Conference on Learning Representations
 Path-sgd: Path-normalized op-timization in deep neural networks,2015, In C
 Critical points of linear neural networks: Analytical forms andlandscape properties,2018, In International Conference on Learning Representations
