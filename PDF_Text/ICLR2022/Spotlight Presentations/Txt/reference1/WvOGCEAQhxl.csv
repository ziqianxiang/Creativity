title,year,conference
 A closer look at memorization in deep networks,2017, In Proceedings of the 34thInternational Conference on Machine Learning
 Don¡¯t just blame over-parametrizationfor over-confidence: Theoretical analysis of calibration in binary classification,2021, arXiv preprintarXiv:2102
 Spectrally-normalized margin bounds forneural networks,2017, In Advances in Neural Information Processing Systems 30: Annual Conferenceon Neural Information Processing Systems 2017
 To understand deep learning we need to under-stand kernel learning,2018, In Proceedings of the 35th International Conference on Machine Learning
 Reconciling modern machine-learning practice and the classical bias-variance trade-off,2019, Proceedings of the National AcademyOfSciences
 On the reproducibility of neural network predictions,2021, arXiv preprintarXiv:2102
 Bagging predictors,1996, Mach
 Detecting er-rors and estimating accuracy on unlabeled data with self-training ensembles,2021, arXiv preprintarXiv:2106
 Estimating generalization under distri-bution shifts via domain-invariant representations,2020, In Proceedings of the 37th International Con-ference on Machine Learning
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Unsupervised supervised learn-ing I: estimating classification and regression errors without labels,2010, J
 In search of robust measures of generaliza-tion,2020, arXiv preprint arXiv:2010
 To annotate or not? predicting performance drop under do-main shift,2019, In Proceedings of the 2019 Conference on Empirical Methods in Natural LanguageProcessing and the 9th International Joint Conference on Natural Language Processing
 Deep ensembles: A loss landscape per-spective,2019, arXiv preprint arXiv:1912
 RATT: leveragingunlabeled data to guarantee generalization,2021,2021
 On calibration of modern neuralnetworks,2017, In International Conference on Machine Learning
 Estimating the accuracies of multiple classifiers withoutlabeled data,2015, In Proceedings of the Eighteenth International Conference on Artificial Intelligenceand Statistics
 Early-stopped neural networks are consistent,2021,2021
 Predicting the generalization gapin deep networks with margin distributions,2018, arXiv preprint arXiv:1810
 Fantas-tic generalization measures and where to find them,2020, In International Conference on LearningRepresentations
 Moment multi-calibration for uncertainty estimation,2020,2020
 SGD on neural networks learns functions of increasing complex-ity,2019, In Advances in Neural Information Processing Systems 32: Annual Conference on NeuralInformation Processing Systems 2019
 Verified uncertainty calibration,2019, In Advances in Neu-ral Information Processing Systems 32: Annual Conference on Neural Information ProcessingSystems 2019
 Simple and scalable predictiveuncertainty estimation using deep ensembles,2017, In Advances in Neural Information ProcessingSystems 30: Annual Conference on Neural Information Processing Systems 2017
 Network in network,2013, arXiv preprint arXiv:1312
 The implicit fairness criterion of unconstrainedlearning,2019, In Proceedings of the 36th International Conference on Machine Learning
 Co-validation: Using model disagree-ment on unlabeled data to validate classification algorithms,2004, In Advances in Neural InformationProcessing Systems 17 [Neural Information Processing Systems
 Launch and iterate: Reducingprediction churn,2016, Advances in Neural Information Processing Systems
 Obtaining well calibratedprobabilities using bayesian binning,2015, In Proceedings of the Twenty-Ninth AAAI Conference onArtificial Intelligence
 Uniform convergence may be unable to explain general-ization in deep learning,2019, In Advances in Neural Information Processing Systems 32
 Deterministic pac-bayesian generalization bounds for deepnetworks via generalizing noise-resilience,2019, arXiv preprint arXiv:1905
 Generalization in deep networks: The role of distance frominitialization,2019, arXiv preprint arXiv:1901
 Deepdouble descent: Where bigger models and more data hurt,2020, In 8th International Conference onLearning Representations
 Representation based complexity measures for predicting gener-alization in deep learning,2020,2020
 In defense of uniform convergence:Generalization via derandomization with an application to interpolating predictors,2020, In Proceed-ings of the 37th International Conference on Machine Learning
 Readingdigits in natural images with unsupervised feature learning,2011,2011
 In search of the real inductive bias: On therole of implicit regularization in deep learning,2014, arXiv preprint arXiv:1412
 Exploring gener-alization in deep learning,2017, In Advances in Neural Information Processing Systems 30
 A pac-bayesianapproach to spectrally-normalized margin bounds for neural networks,2018, International Conferenceon Learning Representations (ICLR)
 Measur-ing calibration in deep learning,2019, In CVPR Workshops
 Estimating accuracyfrom unlabeled data: A probabilistic logic approach,2017, In Advances in Neural Information Process-ing Systems 30: Annual Conference on Neural Information Processing Systems 2017
 Learning to validate the predictions ofblack box classifiers on unseen data,2020, In Proceedings of the 2020 International Conference onManagement of Data
 Identifying and understandingdeep learning phenomena,2019, ICML 2019 Workshop
 Sample complexity of uniform convergence formulticalibration,2020, In Advances in Neural Information Processing Systems 33: Annual Conferenceon Neural Information Processing Systems 2020
 Unsupervised risk estimation using only conditional indepen-dence structure,2016, In Advances in Neural Information Processing Systems 29: Annual Conferenceon Neural Information Processing Systems 2016
 Pre-dicting neural network accuracy from weights,2020,2020
 Evaluating model calibration in classification,2019, In The 22nd International Con-ference on Artificial Intelligence and Statistics
 A theory of the learnable,1984, Communications ofthe ACM
 Chervonenkis: On the uniform convergence of relative frequencies ofevents to their probabilities,1971,1971
 Calibration tests in multi-class classifica-tion: A unifying framework,2019, In Advances in Neural Information Processing Systems 32: AnnualConference on Neural Information Processing Systems 2019
 Towards task and architecture-independent gen-eralization gap predictors,2019,2019
 Obtaining calibrated probability estimates from decision treesand naive bayesian classifiers,2001, In Proceedings of the Eighteenth International Conference onMachine Learning (ICML 2001)
 Understandingdeep learning requires rethinking generalization,2017,2017
 On uniform convergence and low-norm inter-polation learning,2020, In Advances in Neural Information Processing Systems 33
