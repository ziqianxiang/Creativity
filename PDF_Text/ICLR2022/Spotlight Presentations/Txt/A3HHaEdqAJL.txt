Published as a conference paper at ICLR 2022
Task	Relatedness-Based	Generalization
Bounds for Meta Learning
Jiechao Guan
School of Information, Renmin University of China, Beijing, China
2014200990@ruc.edu.cn
ZhiWu Lu *
Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China
Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China
luzhiwu@ruc.edu.cn
Ab stract
Supposing the n training tasks and the new task are sampled from the same envi-
ronment, traditional meta learning theory derives an error bound on the expected
loss over the new task in terms of the empirical training loss, uniformly over the
set of all hypothesis spaces. However, there is still little research on how the relat-
edness of these tasks can affect the full utilization of all mn training data (with m
examples per task). In this paper, we propose to address this problem by defining
a new notion of task relatedness according to the existence of the bijective trans-
formation between two tasks. A novel generalization bound of O( √=) for meta
learning is thus derived by exploiting the proposed task relatedness. Moreover,
when investigating a special branch of meta learning that involves representation
learning with deep neural networks, we establish spectrally-normalized bounds
for both classification and regression problems. Finally, we demonstrate that the
relatedness requirement between two tasks is satisfied when the sample space pos-
sesses the completeness and separability properties, validating the rationality and
applicability of our proposed task-relatedness measure.
1 Introduction
By leveraging knowledge distilled from the training tasks* 1, meta learning (Thrun & Pratt, 1998;
Baxter, 2000) learns to perform well on a new but related task. One important branch of meta
learning achieving great success in practical machine learning applications is representation learning
(Krizhevsky et al., 2012; Bengio et al., 2013; He et al., 2016), where one first learns a shared feature
extractor (e.g., deep neural networks) over the training tasks, and then learns a prediction function
for the new task on the top of the features constructed from the extractor, within a few gradient steps
(Finn et al., 2017; Li et al., 2017) or with a feedforward process (Snell et al., 2017; Sung et al.,
2018) . If the feature extractor can capture the common information across tasks, it is possible that
utilizing representation learning can help learners generalize well to a new task with much less data.
To build up a rigorous framework to support this intuition, the pioneering meta learning theory as-
sumes that both the n training tasks and the new task are i.i.d. generated from the same environment
(Baxter, 2000). Then, analogous to the single task learning whose goal is to select from the hypoth-
esis space H a hypothesis h to achieve the minimal expected loss on the task, meta learning expects
to choose from the hypothesis space family H a hypothesis space H(∈ H) that contains a good solu-
tion to any task sampled from the environment. Under the PAC learning framework (Valiant, 1984;
Vapnik, 1989), (Baxter, 2000) derives a uniform bound of
O( y/lCn + ∖TCF) on the expected loss
of a hypothesis space H over the new task in the environment, according to the empirical loss of H
* Corresponding author.
1In this work, a task represents a data distribution, or say a probability measure on the sample space.
1
Published as a conference paper at ICLR 2022
over the n training tasks, where C1 , C2 are different logarithms of the covering number (Anthony
& Bartlett, 2002) about H. However, compared with the single task learning whose error bound
O( ∖[~m) of any h ∈ H can utilize all m training samples, where C represents certain complexity
indicators such as the VC-dimension (Blumer et al., 1989) or the entropy (Pollard, 1984) ofH, Bax-
ter’s meta learning generalization bound of any H ∈ H cannot fully utilize all mn training samples
(e.g., with a bound of O(√1m), without extra terms of O(√n) or O(√m)). Nevertheless, there is
still little theoretical research on how the relatedness of these tasks can affect the full utilization of
all mn training data in meta learning under Baxter’s proposed i.i.d. task environmetn framework.
In this paper, we propose to address this problem by studying the task relatedness and provide a new
generalization bound. To achieve full utilization of all training sample, our motivation is that differ-
ent tasks need to be ‘related’ enough such that samples from various measures can be assumed to be
generated from an ‘almost’ identical distribution. The equivalence relation of different data distribu-
tions actually corresponds to the measure-preserving isomorphism of their induced measure spaces2
(see Definition 7). Therefore in this paper, we define a new notion of task relatedness called almost
Π-relatedness in Definition 3 according to the existence of a measure-preserving bijective transfor-
mation between two measure spaces associated with different tasks. A PAC-style generalization
error bound that fully utilize all training data is thus provided in Theorem 3, by exploiting the pro-
posed task relatedness. We further employ this task relatedness notation to analyze the representation
learning with deep neural networks, and establish non-parameter-count-based spectrally-normalized
bounds for both classification and regression problems under the meta learning framework. Finally,
we demonstrate the rationality of our proposed task-relatedness notion from a theoretical standpoint.
Our main contributions are summarized as follows:
(1)	We propose a new notion of task relatedness, called almost Π-relatedness, by exploring the
existence of a bijective transformation between two tasks in the environment. A novel PAC-style
generalization error bound of O(ʌ/ɪ) is thus derived for general meta learning by exploiting the
proposed task relatedness, where C captures the logarithm of the covering number of a hypothesis
space family. Such bound thus can fully utilize the whole n * m training data in meta learning.
(2)	For meta learning that involves representation learning, we bound the covering number in Con-
tribution (1) with two covering numbers that are both defined over a single task, making our results
suitable to be combined with recent works of deep neural network in the single task learning. In
particular, we provide the spectrally-normalized bounds of O( J。1惠。2) for classification and re-
gression problems in meta learning, where C1 , C2 are certain complexities that are dependent on the
matrix norms, but independent of the total number of the parameters, in deep neural networks.
(3)	We rigorously demonstrate that, any two tasks are almost Π-related if the sample space is a com-
plete separable metric space, validating the applicability of our proposed task-relatedness measure.
2	Related Work
Meta Learning Theory. The first theoretical analysis for meta learning is performed by (Baxter,
2000), which gives a bound on the expected loss on the unseen task of any hypothesis space in
terms of the empirical training loss. Under this framework, (Pentina & Ben-David, 2015) studies the
generalization error bound of the hypothesis space in the case of kernel learning. Although many
follow-up works also assume that all tasks are sampled from the same environment, they bound dif-
ferent kinds of generalization error from various perspectives. One important branch is to bound the
transfer risk over the new task of a deterministic procedure, such as the linear feature transformation
algorithm (Maurer, 2009), and the Bayes algorithm in PAC-Bayes theory (Pentina & Lampert, 2014;
Amit & Meir, 2018). Among them, (Chen et al., 2020) derive a bound of O( √n) from the algorithm
stability perspective, and (Pentina & Lampert, 2014) derives a bound of O(√η + √√m )(we suppress
the complexity factor in the numerator for concision, similarly hereinafter). Another branch aims to
bound the excess risk of the task specific function returned by ERM algorithm over the unseen task.
They study meta learning theory from different views, such as multitask representation learning
2A measure space is a triple (Z,A, P), where Z is the set, A is the σ-algebra on Z, P is a measure on A.
2
Published as a conference paper at ICLR 2022
(Maurer et al., 2016) and online learning (Khodak et al., 2019). Among them, (Du et al., 2021) and
(Tripuraneni et al., 2020) escape from Baxter’s proposed task environment setting, by defining task
similarity or task diversity notion, obtaining similar excess risk bounds of O( √= + √=) in the case
of high-dimensional transfer learning. Our work follows Baxter’s proposed i.i.d. task environment
framework, and gives an improved bound on the generalization error of the hypothesis space. The
detailed comparisons between our results and that in (Baxter, 2000) are presented in Appendix C.1.
Task Relatedness. Another related work (Ben-David & Schuller, 2003) also proposes a task-
relatedness concept, by defining the bijective transformation π on the input space X . But in our
work, the bijective transformation π is imposed on the sample space Z = X × Y (where Y is the
output space). It is not difficult to see that the task relatedness considered in (Ben-David & Schuller,
2003) is actually a special case of our defined task-relatedness measure. We further validate the
rationality of our proposed notion by revealing the existence of the bijective transformation π on
Z when Z is a complete separable metric space, making our results more applicable. The differ-
ences between our proposed task-relatedness notion and that in (Ben-David & Schuller, 2003) can
be found in Appendix C.2. Besides, (Khodak et al., 2019) and (Du et al., 2021) also consider task-
relatedness notions such as parameter-closeness or sharing a low-rank subspace for meta learning.
To distinguish the notions proposed in this paper with others is truly one of our future directions.
3	Preliminary
In this paper, we use uppercase letters (e.g., Z) to represent different spaces. The boldface z repre-
sents a matrix, with zi as its i-th row and z;j its j-th column. ~z denotes a vector. All detailed proofs
of our theoretical results are deferred to the Appendix B for reader’s benefits.
3.1	Meta Learning
The formulation of meta learning problem can be summarized as follows. We are given a sample
space Z = X × Y , where X is an input space and Y an output space. In this paper, we assume that
Z is a complete separable metric space. A loss function is defined as l : Y × Y → R, and we assume
that l has the range [0, 1], or equivalently, with rescaling technique, we assume that l is bounded. An
environment (P, Q) is a two-tuple, where P is the set of all probability measures\distributions\tasks
on X × Y and Q is a probability measure on P . A hypothesis space family is formulated as H =
{H}, where each H ∈ H is a set of hypotheses\functions h : X → Y . In single task learning, the
learner needs to choose an optimal hypothesis h ∈ H to minimize the expected loss of h over a
probability measure P(∈ P) on the product space X × Y : erP (h) = X×Y l(h(x), y)dP (x, y).
Similarly, the goal of meta learning is to find a hypothesis space H* ∈ H which contains a good
solution to any task sampled from the environment, and minimize the following expected loss of H
over a measure Q on P (Baxter, 2000, Eq.(6)): erQ(H) = P infh∈H erP (h)dQ(P). In practice, it
is hard to minimize erQ (H) directly since we do not know the exact distribution of the environment
measure Q. We can only capture the information about the environment (P, Q) by observing the
training data z generated from the n training tasks Pj(j = 1, ..., n) that are i.i.d. sampled from the
environment measure Q. Formally, this is achieved in the following manner: (1) Sample n times
from P according to Q to generate (i.i.d.) probability measures P1 , ..., Pn. (2) Sample m times
from X × Y according to Pj to generate {(x1j, y1j), ..., (xmj, ymj)} (1 ≤ j ≤ n). (3) Denote
zij = (xij, yij) ∈ Z, and an (m, n)-sample will be generated, denoted by z and written as a matrix
Z = (Zij)m×n. We then choose to minimize the empirical loss erz(H) over the training data Z for
meta learning, which is defined as 介Z(H) = ɪ Pn=I infh∈h 介Zj (h), where Zj is the j-th column
of matrix z. Let P = (Pi,…，Pn). We also consider the following loss erp(H) as the empirical
estimate of the expected loss erq(H): erP(H) = n Pn=I infh∈H erPj (h).
Definition 1 (Baxter, 2000) For any hypothesis h : X → Y , define hl as the composition of
loss function and hypothesis: hl : X × Y → [0, 1] by hl(x, y) = l(h(x), y). For any hy-
pothesis space H ∈ H, define Hl = {hl : h ∈ H} as the function space on Z. For
any sequence of n hypothesises (hi, ..., hn), define (hi, ..., hn)l : (X × Y )n → [0, 1] by
(hi, ..., hn)l(xi, yi, ...,xn, yn) = 1/n in=i l(hi(xi), yi). We will use hl to denote (hi, ..., hn)l.
∀H ∈ H, define Hln = {(hi, ..., hn)l : hi, ..., hn ∈ H} and Hln = SH∈H Hln. Define
P(hl) = X×Y hl (x, y)dP (x, y) and P(Hl) = inf hl ∈Hl P(hl) where P is a probability measure
on Z, and we have P(hl) = erP (h) and erP (H) = P(Hl).
3
Published as a conference paper at ICLR 2022
Definition 2 Let (M, d) be a pseudo-metric3 space. ∀e > 0, a subset T is called an E-cover of
T ⊆ M if ∀t ∈ T, ∃t0 ∈ T such that d(t,t0) ≤ 匕 Let P = Pi X …X Pn be the product
measure on Zn. For any (Hln 3)hl, h0l : Zn → [0, 1], define the pseudo-metric dP (hl, h0l ) =
Zn |hl(~z) - h0l(~z)|dP(~z). Then for any E > 0, the covering number N(E, Hln, dP) is defined as
min{|T |T is an E-cover of Hln under the dP pseudo-metric}, where |T| is the cardinality ofT.
We hope to use the empirical loss e^⅛(H) or erP(H) to approximate the expected loss erq(H).
Instead of the traditional absolute value function d(x, y) = |x-y|, we consider the following metric
function (ν > 0, x, y ≥ 0) dν [x, y]
x|+-+v to measure the distance between X and y, WhiCh is
first used by (Haussler, 1992). In Section 4, we will bound the deviation dν[erz(H), erQ(H)] by
bounding dν[e^`z(H),erp(H)] and dν[erp(H),erQ(H)], respectively.
3.2	ALMOST Π-RELATED TASKS
In this section, we first propose a new concept of task relatedness, which is called almost Π-
relatedness. This notion can be considered as the extension of the tΠ-relatedness, notion proposed
by (Ben-David & Schuller, 2003). The distinctions between our proposed task relatedness notion
and that of Ben-David & Schuller (2003) can be found in Section C.2 of Appendix C.
Definition 3 (Almost Π-Related Tasks) Let Π be a set of transformations π : Z → Z and let
P, P1 be probability measures on Z = X X Y . We say that P, P1 are almost Π-related probability
measures\tasks, if the following conditions are satisfied:
(1)	∃N, N1 ⊆ Z such that P(N) = P1(N1) = 0, and
(2)	∃π ∈ Π, π is a one-to-one mapping from (Z\N, P) onto (Z\N1, P1). ∀A ⊆ Z\N,A is P-
measurable if and only if π(A) = {π(x, y)(x, y) ∈ A} ⊆ (Z\N1) is P1-measurable, and
(3)	Z\N 1AdP = Z\N 1π(A)dP1, where 1A is the indicator function on the set A, and
(4)	the image π(N) is a P1-measurable set, and the inverse image π-1(N1) is a P -measurable set.
Note that condition (4) is a weak requirement only to ensure the measurability as well as the integra-
bility of the mapping π (or π-1) on the measure zero set N (or N1). Actually, to validate whether
two tasks (say P and P1 ) are almost Π-related, the most important step is to find the bijective
transformation π which satisfies the listed conditions (1)-(3) in Definition 3. Then, we can extend
the transformation π to the measure zero set N by defining π(N) as a P1-measurable set (e.g. a
P1 -measure zero set) and extend π-1 to N1 in a similar way, such that the extended transformation
satisfies the condition (4). The existence of such transformation between two tasks can be theoreti-
cally guaranteed by some general topological properties of the given sample space Z, which will be
discussed in detail in Section 4.4. We next give a closure property assumption of the space Hl ∈ Hl
when the transformation set Π is imposed on Hl . Similar to (Ben-David & Schuller, 2003) which
assumes the closure property of any hypothesis space under the transformation Π, we also assume
that the closure condition is satisfied by any Hl ∈ Hl for deriving better generalization bound.
Definition 4 Let Π be the set of transformations on the complete separable metric space Z =
X X Y . We say that the function space Hl is closed under the transformations of Π, if for any
hl ∈ Hl, any π ∈ Π, we have the composition function hg∏ ∈ Hi.
We need to give more explanations to the rationality of the closure property of the function space Hl,
as the closure property is a very important assumption to derive our generalization bounds. Actually,
the almost Π-relatedness defined in Definition 3 can induce an equivalence relationship between two
tasks, according to the existence of one bijective function π ∈ Π. The function π can also induce an
equivalence relationship between two functions in the space Hl. That means, ifa function space Hl
contains a good solution to one task P, then Hl should also contain a good solution to the almost
Π-related task P1 of P. Since the two tasks P and P1 are equivalent to some extent, the function
space Hl should simultaneously contain the good solutions to these two similar tasks. With such
closure property assumption, the complexity of the function space Hl is related to the degree of
relatedness between different tasks from the environment. If the tasks in the environment are all
similar (e.g., almost Π-related), then it is sufficient for Hl to contain one good function as well as its
3A pseudo-metric is a metric without the condition that d(x, y) = 0 ⇒ x = y.
4
Published as a conference paper at ICLR 2022
Π-related variants to generalize well to all tasks. Such closure property assumption of the function
space is also considered in Ben-David & Schuller (2003). They have demonstrated that as long
as the function space is rich enough to contain some function as well as its equivalent class, then
the closure property assumption can be fulfilled. Thus in this work we assume such basic closure
assumption of function space Hl holds to derive novel theoretical insights for meta-learning.
We introduce another concept induced from the π-related tasks, called almost Π-related environ-
ment, and its theoretical properties, which will be used to derive our theoretical analysis in Section 4.
Definition 5 (Almost Π-related Environment) In meta learning set up, an environment (P, Q) on
X × Y is called an almost Π-related environment, if there exists a common probability measure
P on X × Y, such that for any measures Pi ∈ P(i ∈ I,I is the index set), P and Pi are almost
Π-related in the sense of Definition 3.
Lemma 1 Let (P, Q) be an almost Π-related environment on X × Y, Hl a function space on
X × Y . Let P be the underlying common distribution as defined in Definition 5, and N be the
P -measure zero set as defined in Definition 3. Then for any P -measurable set A ⊆ Z\N, for any
Pi ∈ P(i ∈ I), hi ∈ Hl, we have P(A) = Pi(∏i(A)),Pi(hι◦π-1) = P(hι), where ∏i is the
corresponding transformation between Π-related tasks P and Pi (as defined in Definition 3).
Lemma 2 Let (P, Q) be an almost Π-related environment on X × Y, P be the common distribution
that is almost Π-related to any distribution from P. If a function space Hl (on X × Y ) is closed
under the transformations of Π, then for any probability measure Pi ∈ P, erP (H) = erPi (H).
We provide two insights into the theoretical result in Lemma 2 to further explain the motivation of
our proposed task relatedness notion: (1) To fully utilize all training samples, two different tasks
need to be similar enough so that the hypothesis space which performs well on one of these two
tasks can also perform well on another. In other words, the ’best’ performance of the hypothesis
space need to be similar in both tasks, which is the result in above lemma. (2) To achieve the goal in
(1), the hypothesis need to be large enough to contain good solutions that are invariant to the trans-
formation between different tasks. Hence, insight (1) motivates us to define the Π-relatedness notion
to measure the similarity between two tasks in Definition 3, and insight (2) motivates us to assume
the closeness property assumption of the hypothesis space in Definition 4. More explanations for
the motivation of our proposed task relatedness concept can be found in Appendix A.
4	Theoretical Results
We first give a novel generalization bound for meta learning in the almost Π-related environment
in Section 4.1. This is particularly achieved by employing the theoretical properties of the almost
Π-related environment in Lemma 2. As shown in Theorem 6 in Section 4.4, assuming the envi-
ronment to be almost Π-related is reasonable when the sample space Z possesses the completeness
and separability (which are general topological properties satisfied by many metric spaces). Sec-
tion 4.2 derives a covering number bound for representation learning, which is an active area of
meta learning. In Section 4.3, by bounding the covering number with the Lipschitz constants of the
matrix and nonlinearity in each layer of the deep neural network, we further establish a spectrally-
normalized bound that is independent on the number of the total parameters in neural network in
Theorem 5. Moreover, we apply this theoretical result to analyze several practical scenarios like
binary\multiclass classification and regression problems under the meta learning framework. The
three main technical novelties of our work have been clarified in Remark 2 of Appendix C, to show
the technical contributions of this work and outgoing research directions for meta-learning.
4.1	A COVERING NUMBER BOUND FOR META LEARNING IN ALMOST Π-RELATED
Environment
To bound ∣erz(H) - erQ(H)|, We choose to bound the deviation d”[e^Γz(H),erp(H)] and
dν[erp(H),erQ(H)], respectively. We first give an explicit PAC-Style generalization bound on
dν[e‹Tz(H),erp(H)], which can be considered as an inference of Theorem 18 in (Baxter, 2000).
5
Published as a conference paper at ICLR 2022
Theorem 1 Let Hln ⊆ Hl ㊉•…㊉ Hl be the permissible4 class of functions mapping (X X Y )n into
[0,1], where Hl = {hl : h ∈ H : H ∈ H},㊉ means direct product. Let Z be generated by m ≥
2∕(νe2) independent trials from (X × Y )n according to the product measure P = Pi ×∙∙∙× Pn.
For any ν > 0, 0 < < 1, for any H ∈ H, with probability at least 1 - δ over z, we have
力 LS-	r 8 I 4N("/8, Hn,dz)
dν [er Z (H), er P (H)] ≤ ∖ -ln--------彳--------,
νmn	δ
where Z ∈ Z(2m×n), the top half of z is actually Z itself and the bottom half of Z is the copy of Z.
∀hl, h0 ∈ Hn,dz(hl, h0) = 1/2m p2mι ∣hl(Zi) - h∣(Zi)∣.
On the other hand, We can actually bound the deviation dν [erp (H), erQ (H)] as follow, by using the
theoretical properties of the almost Π-related environment in Lemma 2.
Theorem 2 Let (P , Q) be an almost Π-related environment on the complete separable metric space
Z = X × Y. Let P ∈ Pn be generated by n independent trials from P according to some product
probability measure Qn. Thenforany ν > 0, any H ∈ H, we have dν[erp(H), erQ(H)] = 0.
Combining Theorem 1 and the Theorem 2, we can obtain a novel generalization bound of conver-
gencerate O( √=) on ∣erz(H)-erQ (H) | that fully utilizes all mn training data under the proposed
meta learning framework in (Baxter, 2000), uniformly over the set of all hypothesis space H.
Theorem 3 Let (P, Q) be an almost Π-related environment on the complete separable metric space
Z = X × Y. Let Z be an (m, n)-sample generated by the process described in Section 3.1. Let
H = {H} be any permissible hypothesis space family. Then for any H ∈ H,	∈ (0, 1), with
probability at least 1 - δ over Z, we have
le^z(H)-erQ(H)∣≤ J⅛n4"(个Hn,dz).
mn	δ
Combining the above theorem and Lemma 2, we subsequently give a corollary which reveals the
relationship between task-related meta learning setting and i.i.d. single task learning setting.
Corollary 1 In the setting of Theorem 3, let Pn+1 be the new task sampled from the environment.
Then for any H ∈ H, ∈ (0, 1), with probability at least 1 - δ over Z, we have
R rJ 64 I 4M("4, H%dz)
erPn+1 (H) ≤ erz (H) + ∖ ——ln ---------L--------.
n+	mn	δ
There are two important points to note about Corollary 1:
(1)	If the hypothesis space H contains only one hypothesis and all probability measures in the en-
vironment are the same, then the we can choose the bijective transformation π between different
distributions as the identity transformation, and the result in Corollary 1 degrades to the traditional
covering number based generalization bound in single task learning (see Chapter 10 in (Anthony &
Bartlett, 2002)), which utilizes all mn i.i.d. training data. In this sense, meta learning theory can be
considered as the extension of the conventional single task learning theory.
(2)	When bounding the generalization error of a hypothesis space H under Baxter’s proposed meta
learning framework, even though different tasks may have different distributions, we can still prop-
erly estimate the expected loss on the new task according to the empirical loss on the training tasks,
and fully leverage all mn (not necessarily i.i.d.) training data with the use of assumed task related-
ness in the environment. Actually, the relatedness condition can be satisfied by tasks that are focused
on the complete separable sample space (see more explanations in Section 4.4).
4.2	Covering Number Meta Learning Bounds with Representation Learning
In single-task learning setup, representation learning aims to find a good feature embedding f ∈ F
(e.g., deep neural network) which maps the input space X into the feature space V . A prediction
4Permissibility (Pollard, 1984) is a weak measure-theoretic condition satisfied by almost all ”real-world”
hypothesis space families. Without loss of generality, we assume that all hypothesis space families discussed
through out this paper are permissible .
6
Published as a conference paper at ICLR 2022
function g ∈ G then projects the feature space V into the output space Y . Hence, the hypothesis
space can be written as H = Gf, (f ∈ F). Further, let Hl = {Hi}, each Hl = Gif, f ∈ F,
where Gl = {gι}, gi = log is the composition function of the prediction function g and the loss
function l. We will omit the subscript l of gl ∈ Gl for simplicity where the context is clear. In
meta learning setup, representation learning chooses to learn a common feature embedding across
n training tasks and learn n task-specific functions respectively for n tasks, so we can denote the
hypothesis space family Hn = {gιf ㊉•一㊉ gnf : gi,…,gn ∈ Gl, f ∈ F} = {gι ㊉•一㊉ gn。/ :
gi ㊉・…㊉ gn ∈ Gn, f ∈ F} = Gn。F, by defining F 3 f : (X X Y)n → (V X Y)n With
f(x1,y1, ...,xn ,yn) = (f (x1),y1,…，f (xn), yn). We then define two pseudo-metrics over feature
embedding space F and prediction function space Gn respectively.
Definition 6 ∀z = (Zij)2m×n ∈ z(2m×n), define the empirical measure PG on Zn which puts point
mass 1/2m on each row Zi,i = 1,∙∙∙, 2m. ∀s = ((f(xj),yj))2m×n = ((Vij,yj))2m×n, where
f ∈ F, define an empirical measure Ps which puts mass 1/2m on each row si, i = 1, ..., 2m.
∀f, /0 ∈ F, define the pseudo-metric d[Pz,Gn] (f, f!) = 1/2m P2m1 supg∈°n ∣gof(Zi) - g∕0(Zi)∣,
and ∀g, g0 ∈ Gln, define the pseudo-metric dPs (g, g0) = 1/2m Pi2=mi |g(si) - g0(si)|.
Then the following two propositions bound the covering number of the hypothesis space family Hln
with two covering numbers that are both defined over the single task.
Proposition 1 Let (P, Q) be an environment on V X Y.	Denote N(, Gln, 2m) =
SupP~Qn suPs~p2m N(e, Gn,dps). Then for any E = €1 + e2, any z ∈ Z2m×n, we have
N (€1 + €2, Hn, dZ ≤ N (€1, F, d[Pz,Gn)N (€2, Gf, 2m).
Proposition 2 Let Zj be the j-th column of the data matrix z. Let N(€, Gl, 2m) =
SUpP~Q sup~~p2m N(€, Gl,dp~), where dp~(g,g0) = 1/2mP2m1 ∣g(si) - g0(si)∣, ∀g,g0 ∈ Gl,
s = (si,…，s2m). Thenfor any € > 0, we have N(e, F, d[pz,Gn]) ≤ maxι≤j≤n N(e, F, d[pzj,Gl]),
andN(e, Gn, 2m) ≤ N(e, Gl, 2m)n.	"
Recalling Theorem 3 and Propositions 1-2, we can further establish the following covering number
bound for meta learning with the representation learning.
Theorem 4 Let (P, Q) be an almost Π-related environment on the complete separable metric space
Z = X x Y. Let H = {H} be the set of hypothesis spaces of the form H = Gf, f ∈ F. Thenfor
any H ∈ H, any 0 < € < 1, with probability at least 1 - δ over z, we have
Ierz(H)- erQ(H)∣≤
ln4+J6"T
δ mn 1≤j≤n
lnN(|, F, d[Pzj ,G1]) + nlnN(E, Gl, 2m))
We need to highlight the important role of Theorem 4: the covering number of the hypothesis
space family Hn = Gl1。F for meta learning (over n tasks) in Theorem 3 is converted into the
multiplication of the covering number of the class F of the feature embeddings (over one task) and
the n-th power of the covering number of the class Gl of the prediction functions (over one task).
This means that we can introduce recent covering number based theoretical results of deep neural
network from single task learning into meta learning (see Remark 2). In particular, by bounding the
covering number in Theorem 4 with the Lipschitz constants of the function in each layer of the deep
neural network, we can achieve non-parameter-count-based bounds (or say norm-constraint-based
bounds (Neyshabur et al., 2017)) for meta learning, which will be detailed in the next section.
4.3	Spectrally-Normalized Bounds for Meta Learning with Neural Network
Based on the theoretical results in Section 4.2, we now aim to derive non-parameter-count-based
spectrally-normalized bounds for meta learning with deep neural network. We consider the L-layer
depth fully-connected networks with nonlinearities (e.g., activation functions, pooling operators) for
each layer, which computes an embedding function f : Rd0 → Rd, where d0 , d are the dimension
of input data and embedded feature, respectively. Each layer of the network has a weight matrix
Ai and a ρi-Lipschitz nonlinearity σi, with σi (0) = 0 . Then the composition function is given as
7
Published as a conference paper at ICLR 2022
σi ◦Ai : RdiT → Rdi, ∀i ∈ [L]. Forany A = (Ai, ∙ ∙ ∙ ,Al), any input data X ∈ X, define f∕(x)=
Ol(Algl—MAl-i …σι(Aιx)…))∈ Rd. Let F denote the class of functions computed by the
corresponding networks, and D the maximum of {d0, d1, ..., dL-1, d}.
Further, define a sequence of reference matrix (M1, ..., ML) with the same dimensions as
(A1, ..., AL), where Mi = 0 in AlexNet (Krizhevsky et al., 2012) and Mi = I in ResNet (He
et al., 2016) to ensure good generalization performances. Let |卜|b denote the spectral norm, and let
k∙kp,q denote the (p, q)-norm defined by k Akp,q = ∣∣(∣∣Ajkp,…，||A；k ∣∣p)∣∣q for matrix A ∈ Rd×k.
We next give a spectrally-normalized meta-learning bound with deep neural network. For the ease
of presentation, We bound O( ∖^C+C) With O( ^J~C + \J~C).
Theorem 5 Let (P , Q) be an almost Π-related environment on the complete separable metric space
Z = X ×Y. Let H = {Ha} = {GfA : fA ∈F, A =(Ai, ∙∙∙, Al), ∣∣4∣∣σ ≤ s'，∣∣A>-M>∣∣2,1 ≤
bi, i ∈ [L]} be a hypothesis space family where each HA is of the form HA = Gf/ = {g0f∕ (∙):
g = σοW, W ∈ Rk×d, ∣W >k2,1 ≤ θ}, where σ is an element-wise function with Lipschitz constant
θσ. Suppose that ∃b > 0, for any x ∈ X ⊆ Rd0, ∣x∣2 ≤ b. Suppose that the loss function l satisfies
two conditions: (1) when composed with g, gι(∙, y) is an ɑ-Lipschitz function w.r.t. the norm ∣∣∙∣2,
∀ fixed y ∈ Y; (2) ∀ fixed V ∈ Rd, fixed y ∈ Y, ∀g = σ◦W, g0 = σ°W0 ∈ G, ∃β > 0, such
that |gl (v, y) - gl0(v, y)| ≤ β∣Wv - W 0v∣2∙ Then for any HA ∈ H, for any 0 <	< 1/8, with
probability at least 1 - δ over z, we have
IerZ(HA) - erQ (HA) ∣≤ r mn ιn δ + 8b*m1nlp Iapln(2d2)(x( si) 3)2 + βθρn ln (2dk)].
When the loss function l(∙, y) is a Y-Lipschitz function w.r.t. the norm ∣∙∣2, we can set a = γθσθ,
β = γθσ. This is a very useful corollary for a large number of general applications, such as the
multiclass classification and regression problems in the following subsections. We also show in
Remark 1 of Appendix B that the above bound is informative for two-layer neural network.
4.3.1	A Spectrally-Normalized B ound for Binary Classification
We first consider the binary classification problem under the meta learning framework, where Y =
{0, 1}. We choose the classical logistic regression model due to its simplicity and wide applicability
in binary classification scenarios. Formally, let G = {σοw : W ∈ Rd, ∣w∣ι ≤ θ} 5 be the class
of prediction functions, where σ is the sigmoid activation function σ(v) = [J—v (∈ [0,1]). Let
gl ◦f (x, y) = gl(v, y) be the loss function on (x, y) where gl(v, y) = -y ln(σ(w>v)) - (1 -
y) ln(1 - σ(w>v)) is the cross-entropy loss. We then have the following claim.
Claim 1 In the binary classification problem as described above, ∀y ∈ Y, gι(∙, y) is a 2θ-Lipshcitz
function w.r.t. l2-norm. Further, fix v ∈ Rd, y ∈ {0, 1}, then we have ∀w1, w2 ∈ Rd, Il(w1>v, y) -
l(w2>v, y)I ≤ 2Iw1> v - w2>vI. Thus, we can set α = 2θ, β = 2 in Theorem 5 to obtain a spectrally-
normalized bound for binary classification with logistic regression model in meta learning.
4.3.2	A Spectrally-Normalized B ound for Multiclass Classification
We further consider the multiclass classification problem under the meta learning framework, where
Y = {1, ∙∙∙, k}(k ≥ 3). Let G = {W ∈ Rk×d : ∣W> ∣2,1 ≤ θ} be the class of prediction functions,
and Φρ°M(gοf (x), y)(f ∈ F, g ∈ G) be the loss on (x, y). Φρ(v) = min(1, max(0,1 一 P)) is
called the margin loss and P is a positive margin parameter. M(gf (x), y) = maxj=y g。/(χ)[j] 一
g。/(x)[y] is defined as the margin on (x, y). From Lemma A.3 in (Bartlett et al., 2017) about the
Lipschitz property of the margin loss, we have another claim.
Claim 2 In the multiclass classification problem as described above, for any y ∈ Y, the loss func-
tion l(∙, y) = Φρ◦M(∙, y) is2/p-Lipschitz w.r.t. the norm ∣∣∙∣2. Then we can set Y = 2/p and θσ = 1
(σ is the identity map in this case) in Theorem 5 and derive a spectrally-normalized margin bound
for multiclass classification in meta learning.
5In the binary classification, HA = GfA can be considered as the set of the functions from X → R (not
X → Y = {0,1}), but the loss functions erQ(Ha) and erz(HA) are still well-defined. We use this notation
just for simplicity and concision. Similar treatment can also be found in the multiclass classification problem.
8
Published as a conference paper at ICLR 2022
4.3.3	A Spectrally-Normalized B ound for Regression
For completeness, we finally consider the regression problem under the meta learning framework,
where Y = [0,1]. Let G = {σ0w : W ∈ Rd, kwkι ≤ θ} be the class of prediction functions, where
σ is the sigmoid activation function σ(v)=[+；—(∈ [0,1]) with the LiPschitz constant 1/4 (note
that the derivative σ0(v) = σ(v)(1 - σ(v)) ≤ 1/4). Let the loss function l be the squared loss
function by defining gl ◦f (x, y) = (σ(w>f (x)) - y)2 (f ∈ F, g ∈ G).
Claim 3 In the regression problem as described above, for any y ∈ Y, the loss function l(∙,y) is
2-Lipschitz w.r.t. k∙k2. Then we can set Y = 2 and θσ = 1/4 (Q is the SigmoidfunctioninthiScaSe)
in Theorem 5 and derive a spectrally-normalized bound for regression problem in meta learning.
4.4	WHEN THE ENVIRONMENT IS ALMOST Π-RELATED?
In this section, we explore when the given environment (P, Q) is almost Π-related, i.e., whether
there exists a common underlying distribution P that is almost Π-related with any measure Pi sam-
pled from the environment. As the discussion below the Definition 3 shows, the most crucial step is
to find an (almost) bijective transformation π which satisfies the conditions (1)-(3) in Definition 3.
We claim that this bijective transformation is actually equivalent to the almost isomorphism between
two probability measure spaces (Chapter 9 in (Bogachev, 2007)). We first give the definition of the
almost isomorphism and the proof of Theorem 6 can be found in the Appendix.
Definition 7 (Almost Isomorphism) Let (Zι, A, μ) and (Z2, B, V) be two measure spaces.
(1)A point isomorphism π ofthese spaces is a one-to-one mapping of Zi on to Z2 such that μo∏-^1 =
ν and π(A) = B. That is, ∀A ∈ A, π(A) ∈ B, and vice versa.
(2) (Zi, A, μ) and (Z2, B, V) are called almost isomorphic if there exist sets N ∈ Aμ, N ∈ BV
with μ(Nι) = V(N2) = 0 and a point isomorphism π of the spaces Z1∖N1 and Z2∖N2 that are
equipped with the restriction ofthe measures μ and V and the complete σ-algebra Aμ and BV.
With elaborate treatment, we can reveal the existence of the almost isomorphism between any two
complete separable metric spaces. That means, there exists a common distribution P that is almost
Π-related to any distribution sampled from the same environment. It is formally stated as below.
Theorem 6 Let (P, Q) be an environment on the complete separable metric space Z. Then for any
atomeless Pi, Pj ∈ P(i 6= j, i, j ∈ I), the probability measure spaces (Z, Bi, Pi) and (Z, Bj, Pj )
are almost isomorphic. In other words, the two measures Pi and Pj are almost Π-related.
From Theorem 6 we have seen that, any two atomless probability measures Pi, Pj (i 6= j) on the
complete separable metric space Z are almost Π-related. Therefore, we can choose P1 ∈ P as the
common distribution that is almost Π-related to any probability measure Pi(i ∈ I) sampled from the
set P . Hence, we demonstrate the existence of the common distribution P defined in Definition 5.
Further, we can drop the condition of the Π-relatedness of all tasks sampled from the environment
(P , Q) in Theorems 2-5, since this task relatedness requirement is fulfilled by the fact that Z is a
complete separable metric space. We also need to point out that, the completeness and separability
are both very general topological properties that can be satisfied by a number of metric spaces such
as the real space Rd, the closed subspace in Rd, and the product space of the complete separable
metric spaces. For example, in d-dimensional regression problem, the sample space Z = Rd × [a, b]
(a, b ∈ R) is also a complete separable metric space.
5 Conclusions
This paper provides a covering number based generalization bound for meta learning by exploit-
ing the task relatedness of the environment. When analyzing the meta learning with deep neural
network, we derive spectrally-normalized bounds for classification and regression problems. Our
bounds rely on two basic assumptions: the relatedness between different tasks and the closure prop-
erty of the hypothesis space. We demonstrate that, the first task-relatedness assumption can be
satisfied if the sample space is a complete separable metric space. We also show that the closure
property assumption holds when the hypothesis space contains good solutions as well as their equiv-
alent variants. Our ongoing research includes analyzing the convolutional neural network as well as
establishing sharper generalization bounds for meta learning via algorithmic analysis.
9
Published as a conference paper at ICLR 2022
Acknowledgements
Jiechao Guan sincerely thanks Dr. Qi Meng from MSRA for helpful discussions and insightful
comments on the writing of this paper. We thank anonymous reviewers for spotting a mistake
in the original proof of this paper. We also thank all reviewers for their constructive suggestions
to improve the quality of this paper. This work was supported in part by National Natural Sci-
ence Foundation of China (61976220 and 61832017), Beijing Outstanding Young Scientist Pro-
gram (BJJWZYJH012019100020098), China Unicom Innovation Ecological Cooperation Plan, and
Large-Scale Pre-Training Program 468 of Beijing Academy of Artificial Intelligence (BAAI).
References
Ron Amit and Ron Meir. Meta-learning by adjusting priors based on extended PAC-Bayes theory.
In ICML,pp. 205-214, 2018.
Martin Anthony and Peter L. Bartlett. Neural Network Learning - Theoretical Foundations. Cam-
bridge University Press, 2002.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. In ICML, pp. 254-263, 2018.
Peter L. Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for
neural networks. In NeurIPS, pp. 6240-6249, 2017.
Peter L. Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight VC-dimension
and Pseudodimension bounds for piecewise linear neural networks. JMLR, 20:63:1-63:17, 2019.
Jonathan Baxter. A model of inductive bias learning. Journal of Artificial Intelligence Research, 12:
149-198, 2000.
Shai Ben-David and Reba Schuller. Exploiting task relatedness for mulitple task learning. In COLT,
pp. 567-580, 2003.
Yoshua Bengio, Aaron C. Courville, and Pascal Vincent. Representation learning: A review and
new perspectives. IEEE TPAMI, 35(8):1798-1828, 2013.
Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. Learnability and
the Vapnik-Chervonenkis dimension. JACM, 36(4):929-965, 1989.
V. Bogachev. Measure Theory. Springer-Verlag Berlin Heidelberg, 2007.
Jiaxin Chen, Xiao-Ming Wu, Yanke Li, Qimai LI, Li-Ming Zhan, and Fu-Lai Chung. A closer look
at the training strategy for modern meta-learning. In NeurIPS, pp. 396-406, 2020.
Simon S. Du, Wei Hu, Sham M. Kakade, Jason D. Lee, and Qi Lei. Few-shot learning via learning
the representation, provably. In ICLR, 2021.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In ICML, pp. 1126-1135, 2017.
Suriya Gunasekar, Blake E. Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Sre-
bro. Implicit regularization in matrix factorization. In NeurIPS, pp. 6151-6159, 2017.
David Haussler. Decision theoretic generalizations of the PAC model for neural net and other learn-
ing applications. Information and Computation, 100(1):78-150, 1992.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In ECCV, pp. 630-645, 2016.
Mikhail Khodak, Maria-Florina Balcan, and Ameet S. Talwalkar. Adaptive gradient-based meta-
learning methods. In NeurIPS, pp. 5915-5926, 2019.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convo-
lutional neural networks. In NeurIPS, pp. 1106-1114, 2012.
10
Published as a conference paper at ICLR 2022
Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-SGD: Learning to learn quickly for few
shot learning. arXiv:1707.09835, 2017.
Andreas Maurer. Transfer bounds for linear feature learning. Machine Learning, 75(3):327-350,
2009.
Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The benefit of multitask
representation learning. JMLR, 17:81:1-81:32, 2016.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring general-
ization in deep learning. In NeurIPS, pp. 5947-5956, 2017.
Anastasia Pentina and Shai Ben-David. Multi-task and lifelong learning of kernels. In ALT, pp.
194-208, 2015.
Anastasia Pentina and Christoph H. Lampert. A PAC-Bayesian bound for lifelong learning. In
ICML,pp. 991-999, 2014.
David Pollard. Convergence of Stochastic Processes. Springer-Verlag, New York, 1984.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In
NeurIPS, pp. 40774087, 2017.
Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H. S. Torr, and Timothy M. Hospedales.
Learning to compare: Relation network for few-shot learning. In CVPR, pp. 1199-1208, 2018.
Sebastian Thrun and Lorien Pratt (eds.). Learning to Learn. Kluwer Academic Publishers, 1998.
Nilesh Tripuraneni, Michael I. Jordan, and Chi Jin. On the theory of transfer learning: The impor-
tance of task diversity. In NeurIPS, pp. 7852-7862, 2020.
Leslie G. Valiant. A theory of the learnable. Communication ofACM, 27(11):1134-1142, 1984.
Vladimir Vapnik. Inductive principles of the search for empirical dependences (methods based on
weak convergence of probability measures). In COLT, pp. 3-21, 1989.
John von Neumann. Einige Satze uber messbare abbildungen. Annals of Mathematics, 33(3):574-
586, 1932.
Colin Wei and Tengyu Ma. Improved sample complexities for deep neural networks and robust
classification via an All-Layer margin. In ICLR, 2020.
11
Published as a conference paper at ICLR 2022
APPENDIX
A Motivation of Task Relatedness
We simply explain our motivation of exploring ‘task relatedness’ by comparing the differences be-
tween single task learning and meta learning in Table 1, to fully utilize all training samples.
Table 1: The differences between traditional single task learning and meta learning framework
proposed by Baxter (Baxter, 2000), where erQ(H) - erz (H) = infh∈H erP (h)dQ(P) -
pn=ι infh∈H erPi(h). The proposal of our task relatedness notion comes from the motivation
that probabilities measures {Pi}in=1 sampled from the environment (P, Q) need to be equivalent,
namely, almost Π-related.
Setting	Single-Task Learning	Meta-Learning
Goal	choose hypothesis h ∈ H	choose hypothesis space H ∈ H
Dealing Objective	i.i.d. sample ~ = {zi}m=ι	i.i.d.	probability measures {Pi*	
Generalization Gap	er P (h) — er~ (h)		er Q (H) — erz(H)	
Requirement	{zi}m=I are similar enough i.e., identical distributed samples.	{Pi}n=i need to be similar enough i.e., equivalent measures.
B Detailed Proofs of Our theoretical Results
B.1	PROOF OF THEORETICAL PROPERTIES OF ALMOST Π-RELATED ENVIRONEMENT
Proof of Lemma 1 in the main paper.
From Definition 3, ∃Ni ⊆ Z, such that Pi(Ni) = 0. Then for any P -measurable set A ⊆ Z\N,
P (A)= ∕1a∩(z\n ) + 1a∩n dP =
Z
1AdP
Z\N
J 1∏i(A)dPi =J1∏i(A)dPi =J1ao∏-1 dPi =JlAdPi ◦ni =Pi(∏i(A)),
where the third equality holds due to the condition (3) in Definition 3, and the sixth equality holds
due to the equivalent integral transformation f f ◦g-1dP = f f dP ◦g. Similarly, recalling P (hi)
from Definition 1 and noticing πi-1 (Ni) is P -measurable from condition (4) in Definition 3, we
have
Pi(hι-∏-λ
)=(/ + ) )hi ^∏-1dPi = [hi dP = P (hi).
Z\Ni Ni	Z\N
The above second equality holds due to the fact that JN hι◦∏-1dPi = 0 (since Pi(Ni) = 0 and h
is a bounded function), and the condition (3) of πi-1 in Definition 3.
Proof of Lemma 2 in the main paper.
To prove infh∈H erP (h) = infh∈H erPi (h), it is equivalent to prove infhl ∈Hl P(hi) =
infhl ∈Hl Pi(hi), then it suffices to show that ∀hi ∈ Hi, ∃h0i ∈ Hi we have Pi(h0i) ≤ P(hi).
Since the symmetricity of P and Pi (i.e., P◦n-1 = Pi, Pi◦∏i = P holds almost everywhere), We
can find another h∣0 such that P(h∣0) ≤ Pi(hi). In fact, let h∣ = hι◦π-1, from Lemma 1, we have
Pi(h0i) ≤ P(hi).
B.2	Proof of the Covering Number Bound for Meta Learning in Almost
Π-RELATED ENVIRONMENT
Lemma 3 (Theorem 18 in (Baxter, 2000)) Let H ⊆ Hi ㊉…®Hn be a permissible class offunctions
mapping (X X Y)n into [0,1]. Let Z ∈ (X X Y)(m,n) be generated by m ≥ 2∕(e2ν) independent
12
Published as a conference paper at ICLR 2022
trialsfrom (X X Y )n according to some product probability measure P = Pi ×∙∙∙× Pn. For any
ν > 0, 0 < < 1, for any h ∈ H, we have
Pr{z ∈ (X × Y)(m,n) : supdν[erz(h), erp(h)] > e} ≤ 4N(eν∕8, H, dz) exp(-c2νmn∕8).
H
Proof of Theorem 1 in the main paper.
Pr{z ∈ (X × Y)(m,n) : supdν[erz(H), e^>(H)] > e}
H
≤ Pr{z ∈ (X × Y)(m,n) : supdν[erz(h), erp(h)] > e}
Hln
≤ 4N(eν∕8, Hn, dz) exp(-e2νmn∕8),
where the first inequality holds due to the Lemma 24 in (Baxter, 2000), and the second inequality
holds due to the Lemma 3 in the supplementary material. Let the r.h.s. of the above inequality be
less than the confidence parameter δ ∈ (0,1), then We have 心 ≥ Vmn ln 4N("/^Hl ,dz), which
gives the explicit PAC-Style generalization bound on dν[e^⅛(H), erp(H)] in Theorem 1.	■
Proof of Theorem 2 in the main paper.
According to Lemma 2, there exists a common distribution P ∈ P such that erP (H) = erPi (H)
for any Pi ∈ P. Then erQ(H) = RP erPi(H)dQ(Pi) = RP erP (H)dQ(Pi) = erP (H) =
1/n Pn=I erPj (H) = erp(H), which indicates ∣erq(H) - erp(H)∣ = 0.	■
Proof of Theorem 3 in the main paper.
By the fact that the loss function l has range [0, 1], and the triangle inequality of metric dν,
lerz(HV + erQ(H)I ≤ dν[erz(H),erQ(H)]
(旬 八(旬	」8 ] 4N(EV/8, Hn, dZ)
≤ dν[erz(H),erp(H)] + dν[erp(H),erQ(H)] ≤ ∖ -----ln---------z------.
Vmn	δ
Letting V = 2 (to minimize √V + √) gives the result.	■
B.3	Proof of Covering Number Bounds for Meta Learning with
Representation Learning
Proof of Proposition 1 in the main paper.
Fix an empirical measure PZ on (X × Y)n, and let F be a minimum Size ei-coverfor (F, d[p乞，°川)，
then we have ∣F∣ = N(ei,F,”层,°川)，and ∀f ∈ F,∃f ∈ F such that d[Pz,Gn](f,f) ≤ ei.
i
For any f ∈ F, let PZof 1 be the induced probability measure on (V × Y)n by defining
PZ J-1(A) = Pz(f-1(A)), ∀A ∈ σ-algebra on (V × Y)n. Let Gf be the minimum size e2-cover
for(Gn,dpz0∕-ι). Hence ∣G^∣ = N(0 Gn,dpzf-ι) ≤ N®, Gn, 2m). Let N = {gf : f ∈ F
and g ∈ Gf }, then we have ∣N∣ ≤ N(ei, F,"层,°川)”(⑶ Gn 2m), which satisfies the required
cardinality condition.
It remains to show that N is an ei + ⑦一cover	of Hp. Actually,	∀gι ㊉•…㊉	gnf ∈	Hp,
choose f ∈ F such that ”层,°川(/,f) ≤ ei,	and choose g1 ㊉•…㊉ gnb	∈	Gf such	that
dp_ f - (gi ㊉•…㊉ gn,gi ㊉•…㊉ gn) ≤ E2. Then the triangle inequality of the distance met-
ric dZz implies that
dz(g1 ㊉•…㊉ gnof, g1 ㊉•…㊉ gn◦/)
≤dz(g1 ㊉•…㊉ gnof, g1 ㊉•…㊉ gnof) + dz(g1 ㊉•…㊉ gnof, g1 ㊉•…㊉ gn◦/)
≤d[Pz,Gn]f, f) + dPmfT (g1 ㊉•…㊉gn, g1㊉•…㊉gn)
≤e1 + e2.
13
Published as a conference paper at ICLR 2022
The second inequality holds
1∕2m PimI suPg∈Gn |g。/(Zi)
due
gf(Zi)l
to the definition of	d[Pz,Gn∖(f, f	=
and the equivalent integral transformation
R gdPzf-1 = R gfdPz, ∀g ∈Gln.
Proof of Proposition 2 in the main paper.
For any f, f0 ∈ F, we have
d[Pz,Gn](f，f0)
1 2m
丁 V SUP Igof(Zi) - g-f'(Zi)∖
2m i=1 g∈Gln
1	2m	1 n
2m ∑2sup | n∑2gjf (Zij) - gj ◦f 0 (Zij )|
i=1 Gln	j =1
n	2m
≤ n X 2m X
j=1
≤ max
1≤j ≤n
i=1
SUP |gj ◦f(zij ) - gj◦f (Zij ) |
gj ∈Gl
d[Pz. 〃Gi](f，f 0),
;j
—
which completes the proof of the first inequality. Similarly, for the second one, ∀P 〜 Qn S〜
P2m , ∀g, g0 ∈ Gln ,
2m
dPs (g,g0) = 5- X Ig(Si) - g0(Si)I
s	2m
i=1
1 2m 1 n
=2m ∑i n∑gj (vij , yij ) - gj (vij, yij)I
m i=1 n j=1
1 n 1 2m
≤ n ^X 2m ^X | gj(Vij, yij) - gj(Vij, yij) |
j=1	i=1
1 2m
≤ ι≤≤aXn 2m	lgj (Vij，yij ) - gj (Vij，yij ) |
i=1
= 1m≤ja≤xn dPs;j (gj,gj0).
Therefore, N(e, Gn dps)	≤	(maxι≤j≤nN(e, Gι, dp” ))n and thus N(e, Gn 2m)	≤
N(, Gl, 2m)n.
B.4 Proof of Spectrally-Normalized B ounds for Meta Learning with Deep
Neural Network
To demonstrate Theorem 5 in the main paper, we need to give the following two important lemmas,
which gives the non-parameter-count-based spectrally-normalized bounds for deep neural network
in the single task learning.
Lemma 4 (Bartlett et al., 2017) Let positive reals (α, β, ) and positive integer k be given. Let
matrix V> ∈ R2m×d be given with kV> k2 ≤ β. Then
α2β2
N({V>A : A ∈ Rd×k, kAki,ι ≤α},e, k∙ki)≤(2dk)dFe.
Lemma 5 (Bartlett et al., 2017) Let fixed nonlinearities (σ1, ..., σL) and reference matrices
(M1 , ..., ML) be given, where σi is ρi-Lipschitz and σi (0) = 0. Let spectral norm bounds
(s1, ..., sL), and matrix (2, 1)-norm bounds (b1, ..., bL) be given. Let the input data matrix
X ∈ R2m×d0 be given, where the m rows correspond to data points. Let HX denote the family
of matrices obtained by evaluating X with all choices of network fA : HX = {fA(X>)IA =
14
Published as a conference paper at ICLR 2022
(A1, ..., AL), kAikσ ≤ si, kAi> - Mi> k2,1 ≤ bi}, where each matrix has dimension at most D
along each axis. Then for any > 0,
ln N (Hχ,^∙k2)≤≡≡≠D) (Y1 s2ρ2)(XX (bi)3 )3
s
j=1	i=1 i
Proof of Theorem 5 in the main paper.
First, We bound lnN(e, F,"及⑨]).For any P ∈ P, ~ 〜P2m, for any 于八,于八『∈ F,
1 2m
d[P~ ,Gι](fA, fA0) = 2m	Sup |gl°fA(Zi) - glfA0 (Zi)I
2m
=L X
2m
i=1
2m
≤ -α T
2m
i=1
sup |gl(fA(xi),yi) - gl(fA0(xi),yi)|
gl ∈Gl
kfA(xi) - fA0 (xi)k2 (Lipschitz)
≤ √mkfA(X>) - fA0(X>)k2. (Jensen)
Applying Lemma 5, We then have
ln N(e,F ,d[Pz,Gι]) ≤ ln NC^am~, F Mk2)
≤ °2嗯1；；产2) (Yl SjT (Si)3 )3 ≤
m	j=1	i=1 si
α2b2 ln (2D2)
2
Next, We bound lnN(e, Gl, dp~). Actually, for any P 〜Q,~ 〜
condition (2) of the loss function l and Jensen inequality, We have
1	2m
dp~(g,g0) = 丁Igl(Vi,yi) -gι(vi,yi)1
s	2m
i=1
•(YSM)(T(bi)2)3.	(1)
j=1	i=1 si
P2m, for any gl , gl0 ∈ Gl, using
β 2m	β
≤ɪ EkWvi- W0Vik2 ≤ ɪkV>W> - V>W0>k2,
2m	2m
i=1
Where V = (v1 , ..., v2m ) ∈ Rd×2m . Then combining the above results and Lemma 4, We have
lnN(,Gl,dP~s)
≤ lnN(雪，{V>W> ： kW>k2,1 ≤ θ}, k∙k2)
β
≤
β2θ2V>k2 ln(2dk) ≤ β2θ2b2 Q=I SsPP ln(2dk).
(2)
The last inequality holds due to the Lipschitz property of the activation σl and the matrix Al (l =
1, ..., L), ∀i ∈ [2m]
IlviIl2 = ∣∣gl(Al …σ1A1(xi)…)-。工(0州2
L
≤SLPL∣∣σL-i(AL-i …σιAι(xi)…)ks ≤ bɪɪslpl.
l=1
Recalling Theorem 4 in the main paper and Eqs.(1)-(2) in the supplementary gives the result.
Remark 1 (A situation where our spectrally-normalized bound for meta-learning is informative)
We further provide a situation Where the neural netWork is a tWo-layer netWork (i.e. composed of a
15
Published as a conference paper at ICLR 2022
hidden layer with D units and an output layer) for k-class classification problem, to provide more
information of our spectrally-normalized bound from two aspects: (i) Our bound in Theorem 5 is
more informative than the traditional VC-dimension bound. Actually, our bound in Theorem 5 for
two-layer network can be rewritten as follow:
8bS1ρi Iapln (2D2)(b1) + βθ/nln(2dk)],
mn	s1
which is of order O( b1√√nD2 + s1 v√n(Dk)). Under the Same setting, the VC-dimension based
nm	m
meta-learning bound for neural networks (i.e. obtained with techniques from Theorem 8 in Baxter
(2000)) is of order O(pnvm + Pm). Further note that the VC-dimension of the neural networks
is v ≈ W ln W (where W is the total number of the parameters, see Bartlett et al. (2019)), thus
the VC-dimension for two-layer neural networks is about v ≈ (D2 + Dk) ln (D2 + Dk) and the
meta-learning bound is about O(J(D2+Dk)或(02+应 + J(D2+Dk)m(D2+Dk)), which is looser
than our spectrally-normalized meta-learning bound of O( b1√√nD2 + s1 "√√mDk)). (ii) OUr bound in
Theorem 5 is informative under the implicit regularization framework of SGD. Note that SGD can
always find a minimum trace/nuclear norm for neural networks (especially for two-layer networks,
see Gunasekar et al. (2017)), therefore the norm parameters s1 and b1 in our bound for two-layer
neural networks can be small, and hence our spectrally-normalized meta-learning bound of order
O( b1√√lnD2 + s1√√n(Dk)) can be informative.
nm	m
Proof of the Corollary under Theorem 5 in the main paper.
∀v1, v2 ∈ Rd, ∀y ∈ Y,
|gl(v1,y) - gl(v2, y)| = |l(g(v1), y) - l(g(v2), y)|
= ∣l(σoWvι,y) - l(σ°Wv2,y)∣ ≤ γθσ∣∣Wvι - Wv2∣∣2 ≤ γθσθ∣∣vι - V2∣∣2,
where the second inequality holds since kW> k2,1 is a kind of matrix norm of W. Hence we can set
α = γθσ θ. Similarly, ∀gl, gl0 ∈ Gl,v ∈ Rd,y ∈ Y, we have |gl(v, y) - gl0(v, y)|
=∣l(σ°Wv, y) — l(σ°W0v, y)| ≤ γθσ ∣∣ Wv — W0v∣∣2∙
Then we can obtain β = γθσ. Combining the above results with Theorem 5 gives the result.
Proof of Claim 1 in the main paper.
∀v ∈ Rd, gl (v, y) = -y ln(σ(w>v)) - (1 - y) ln(1 - σ(w>v)). Therefore, ∀v1, v2 ∈ Rd, we have
|gl (v1, y) - gl (v2 , y)|
1	+ ew>v1
=Iy(W>v2 - w>vι) + ln 1 + ew>v2 |
>	>	1 + ew>v1
≤lw v2 - W v1| + | ln 1 + ew>v2 |
≤2∣w>v2 — w>v11 (ln(1 + ex) is 1 — LipsChitZ)
≤2θ∣v2 - v1∣2 (Schwarz and ∣w∣2 ≤ ∣w∣1).
Similarly, since l(W>v, y) = -y ln(σ(W>v)) - (1 - y) ln(1 - σ(W>v)), for any y ∈ {0, 1} we can
obtain
|l(W1>v, y) - l(W2>v, y)| = |y(W1>v - W2>v) + ln
1 + ew>v
1 + ew>v
| ≤ 2|W1> v - W2> v |.
Proof of Claim 3 in the main paper.
For any fixed y ∈ [0, 1], for any v1, v2 ∈ Rd, notice that both g(v1) and g(v2) also lie into the
interval [0, 1], then
|l(g(v1), y) - l(g(v2), y)| = |(g(v1) - y)2 - (g(v2) - y)2| ≤ 2|g(v1) - g(v2)|.
16
Published as a conference paper at ICLR 2022
B.5 PROOF OF WHEN THE ENVIRONMENT IS ALMOST Π-RELATED?
To obtain a complete proof of Theorem 6 in the main paper, we first give an important lemma that
states the existence of the almost isomorphism between a complete separable metric space and the
unit interval [0, 1]. We also introduce the formal definitions of metric Boolean algebra and metric
Boolean isomorphism (Bogachev, 2007).
Theorem 7 (Bogachev, 2007) Let (Z, μ) be a complete separable metric space with a Borel prob-
ability measure μ. Then (Z, μ) is almost isomorphic to the space ([0,1], ν), where V is some Borel
probability measure. If μ is an atomless measure, then one can take for V Lebesgue measure.
Definition 8 (Metric Boolean Algebra) Let (Z, B, μ) be a measure space with a finite nonnegative
measure μ. Let d(A, B) = μ(A4B), A, B ∈ B. The function d is called the Frechet-Nikodym
metric and we can introduce thefollowing equivalence relation on B: A 〜B if d(A, B ) = 0. Then
the metric space (B∕μ, d) is called the metric Boolean algebra, or measure algebra, often denoted
by Eu.
Further, the metric space (B∕μ, d) is SeParable,i.e.,Contains a countable everywhere dense subset,
if and only if the corresponding measure μ is separable. The separability of μ is equivalent to the
existence of a countable collection of sets Bn ∈ B such that ∀B ∈ B and > 0, there exists an
integer n with μ(B4Bn) ≤ e. In addition, a metric space (B∕μ, d) is complete if B is a σ-algebra
and μ is countably additive (e.g., μ is a measure).
Definition 9 (Metric Boolean Isomorphism) Two measure algebras Eu1 and Eu2 generated by mea-
sure spaces (Zι, B1,μ1) and (Z2, B2,μ2) are called isomorphic ifthere exists a one-to-one mapping
J from Eμι onto E*2 (called a metric Boolean isomorphism) such that J preserves the measure,
i.e., μ2(J (A))= μι(A),∀A ∈ E*「
Theorem 8 (Bogachev, 2007) Every separable atomless measure algebra is isomorphic to the mea-
sure algebra of some interval (e.g., [0,1]) with Lebesgue measure.
Definition 10 (Lebesgue-Rohlin Space) A measure space (Z, B, μ) is called a Lebesgue-Rohlin
space if it is almost isomorphic to some measure space (Z0, B0, μ0) with a countable basis with
respect to which Z0 is complete.
Example 1 The space ([0, 1], B([0, 1]), λ), where λ is Lebesgue measure, has a countable basis
with respect to which it is complete.
Theorem 9 (von Neumann, 1932) Let (Zι, B1,μ1) and (Z2, B2,μ2) be Lebesgue-Rohlin spaces
with probability measures. Ifthe corresponding measure algebra Eμι and E*2 are isomorphic in the
sense of Definition 9, then there exists an almost isomorphism between these spaces. In particular,
this is the case if both measures are atomless.
Proof of Theorem 6 in the main paper.
The proof contains 3 main steps: (1) Since Z is the complete separable metric space and the prob-
ability measures Pi, Pj are atomless, then from Theorem 7, (Z, Bi, Pi) and (Z, Bj, Pj) are both al-
most isomorphic to the measure space ([0, 1], B([0, 1]), λ) where λ is the Lebesgue measure. From
Definition 10 and Example 1, (Z, Bi, Pi) and (Z, Bj, Pj) are Lebesgue-Rohlin spaces. (2) From Ex-
ample 1, the complete measure space ([0, 1]\M, B([0, 1])λ, λ) (w.r.t. the measure λ and λ(M) = 0)
has a countable basis {Bn}n∞=1 ⊂ B([0, 1])λ. The almost isomorphism πi from ([0, 1], B([0, 1]), λ)
into (Z, Bi, Pi) can induce a countable basis {πi(Bn)}n∞=1 ⊂ Bi, which guarantees the separability
of the measure algebras EPi. The separability of the measure algebras EPj can be guaranteed in
the same way. Then from Theorem 8, the measure algebras EPi and EPj , generated by measure
spaces (Z, Bi, Pi) and (Z, Bj, Pj) , are both isomorphic to the measure algebra of the interval [0,1]
with Lebesgue measure. Therefore, the two measure algebra EPi and EPj are isomorphic (since
isomorphism is an equivalence relation). (3) Combining (1) and (2), and recalling Theorem 9, the
two measure spaces (Z, Bi, Pi) and (Z, Bj, Pj) are almost isomorphic. So Pi and Pj are almost
Π-related in the sense of Definition 3.
17
Published as a conference paper at ICLR 2022
C More detailed Comparisons with Related Works
C.1 Detailed Comparison with Generalization B ounds of (Baxter, 2000)
This paper can be considered as the extension of the meta learning theoretical work in (Baxter,
2000), by further exploring the task relatedness for the environment. In this section, we detail our
improvements over this pioneering work. First, We introduce a new notation called H*. For any
hypothesis space H ∈ H, define H* : P → [0,1] by H* (P) = infh∈H erp(h), and for any
hypothesis space family H, define H* = {H* : H ∈ H}. Although Baxter does not give the explicit
PAC-Style learning bound in his main paper, its bound on ∣erz(H) - erQ (H) | can be expressed as:
r里 ln8C空,Hn) + r空 ln8C(* H*),	⑶
mn	δ	n	δ
where C(, Hln) = supP N (, Hln, dP), C(, H*) = supQ N (, H*, dQ) (see its Theorem 4). Our
improvements can be summarized in the following three aspects:
(1)	We exploit the proposed task relatedness to reduce the complexity C(, H*) in Eq. (3) to zero in
Theorems 3. In Theorem 6, we further show the rationality of our task relatedness assumption when
the sample space Z is a complete separable metric space. Given that (Baxter, 2000) also assumes
Z to be a separable metric space to obtain theoretical results, our derived meta learning bound in
Theorem 3, albeit depending on a slightly stronger assumption, is a non-trivial enhancement. Nev-
ertheless, we admit that our results also rely on the closure property of the function class Hl.
(2)	Our covering number results (e.g., Theorem 4) are based on the metric defined w.r.t. the empir-
ical measure, instead of the abstract measure in Baxter’s work (e.g., Theorem 6 in (Baxter, 2000)).
Therefore it is more suitable to combine our results with the recent theoretical results of covering
number bounds for deep neural networks in single task learning.
(3)	When bounding the covering number (or say capacity) of the neural network C(2, F) (i.e., The-
orem 8 in (Baxter, 2000)), Baxter uses traditional Pseudo-dimension indicator which is developed
by (Haussler, 1992), resulting in a parameter-count-based bounds for meta learning with neural net-
work. However, this is not suitable for the analysis of modern overparameterized deep networks. In
this paper, we introduce a new complexity indicator, spectrally-normalized bound for deep neural
network (Bartlett et al., 2017), into the meta learning framework. The obtained bounds for meta
learning with deep network for classification and regression problems in Claims 1-3 are all indepen-
dent of the size of total parameters, outperforming the results in Theorem 8 of (Baxter, 2000).
Remark 2 (The three main technical difficulties when applying spectrally-normalized margin
bounds for meta-learning with deep neural networks)
Now, we give more explanations about the three main technical difficulties when applying spectrally-
normalized margin bounds (Bartlett et al., 2017) for meta-learning with deep neural networks.
(i)	We need to remove the second covering number complexity (i.e. the covering number complexity
ofH*) in the meta-learning bound in Eq. (3) in Section C.1 (i.e. the original bound in Theorem 4 of
(Baxter, 2000)). Since such covering number complexity is defined (and can only be defined) based
on the distance with respect to the abstract measure (instead of the empirical measure, see Defini-
tions 3-4 in (Baxter, 2000)), we cannot use any modern theoretical results in deep learning (e.g., the
spectral norm of the neural networks in (Bartlett et al., 2017), the compression bound in (Arora et al.,
2018) and the ALL-layer margin in (Wei & Ma, 2020)) but the traditional VC-dimension to bound
such covering number complexity, hence leading to the vacuous parameter-count-based bounds for
deep neural networks. The aforementioned challenge motivated us to propose the Π-relatedness
notation to measure the similarity between different tasks and finally removed the second covering
number complexity in Eq. (3) to obtain our main generalization bound in Theorem 3 that can fully
utilize the whole n * m training samples.
(ii)	For the first covering number complexity in Eq. (3) (which is still defined based on the dis-
tance w.r.t. the abstract measure in Baxter’s original paper), we still need to transform it into the
complexity defined on the distance w.r.t. the empirical measure with our proposed techniques in
Propositions 1-2 (also see our Definition 6), hence we can extend our generalization bound in The-
orem 3 to the representation setting and make our meta-learning bound with representation learning
(i.e. our Theorem 4) much easier to be combined with modern theoretical results (Bartlett et al.,
2017; Arora et al., 2018; Wei & Ma, 2020) for deep neural network in single-task learning.
(iii)	The last difficulty is to connect the covering number complexity in meta-learning setting (e.g.
18
Published as a conference paper at ICLR 2022
see our Definition 6, w.r.t. the empirical measure) with the spectrally-normalized based cover-
ing number complexity from Bartlett et al. (2017) (i.e. Lemmas 4-5 in our Appendix B.4). Such
difficulty is overcome by using the Lipschitness property of neural networks and the theoretical
properties of covering number (see the proof of Theorem 5 in Appendix B.4).
C.2 Detailed Comparison with Task Relatednes s Notion of (Ben-David &
Schuller, 2003)
In this section, we detail more distinctions between our proposed almost Π-relatedness notation and
the task relatedness notion defined in (Ben-David & Schuller, 2003). We first recall this previous
concept in (Ben-David & Schuller, 2003, Definition 2.1).
Definition 11 (Ben-David & Schuller, 2003) Let F be a set of transformations f : X → X ,
and let P1, P2 be probability distributions over X × {0, 1}. We say that P1, P2 are F -related
distributions if there exists some f ∈ F such that for any T ⊂ X × {0, 1}, T is P1-measurable iff
f[T] = {(f (x), b)|(x, b) ∈ T} is P2-measurable and P1(T) = P2(f[T]).
Remark 3 (The main differences between our task relatedness notation and that of (Ben-David
& Schuller, 2003))
Note that in Definition 11, the condition P1 (T) = P2(f[T]) means that P1 (T) = P2 ◦ (f × I)[T]
for all T ⊂ X × {0, 1}. Therefore P1 = P2 ◦ (f × I) and hence f × I is a (bijective) measure-
preserving transformation on the product space X × {0, 1}. Recall the definition of our proposed
task relatedness concept in Definition 3 in the main paper. It’s not difficult to see that there are two
main differences between these two task-relatedness notions: (i) The bijective transformation f in
Definition 11 of David’s work is defined between the input space X and the input space X. While
in our work, the bijective transformation π in Definition 3 of the main paper is defined between
the sample space Z(= X × Y ) and the sample space Z. Note that the bijective transformation f in
Definition 11 can be viewed as the bijective transformation f × I on the product space X × Y , where
I is the identity map on the output space Y . Therefore the transformation given by (Ben-David
& Schuller, 2003) can be considered as a special case of our proposed task-relatedness concept.
(ii) Furthermore, the bijective transformation f given in Definition 11 is a function map imposed
on the whole input space X, whereas our proposed bijective transformation π is imposed on the
almost whole sample space Z excluding a measure-zero set. In other words, our proposed π is a
bijective map between Z\N and Z\N1, where N, N1 are P -measure zero and P1-measure zero
sets, respectively. Such design will allow more flexibility of the choice of the bijective map π,
and can help us theoretically guarantee the existence of the almost bijective function on the whole
sample space Z (see more explanations in Section B.5). In contrast, (Ben-David & Schuller, 2003)
does not provide a rigorous demonstration of the existence of its defined bijective transformation
f . Actually, it is not easy to find such bijective transformation f on the input space X, which
meanwhile satisfies the condition that the bijective transformation f × I is a measure-preserving
bijective on the product space X × Y . In this sense, our work can be considered as the extension
version of that in (Ben-David & Schuller, 2003).
As for our proposed task-relatedness concept, the explicit form of the bijective transformation π in
Theorem 6 of main paper can be found in (Bogachev, 2007, Theorem 6.5.7). In that result, Bogachev
takes π as the composition of functions Pn∞=1 IEn and (Pn∞=1 IBn)-1, where En belongs to the σ-
algebra Bμ over the sample space Z associated with the measure μ, Bn belongs to the σ-algebra BV
over the sample space Z associated with the measure ν, I is the indicator function from the σ-algebra
over Z to the interval [0, 1]. A simpler (but not rigorous) example just for intuitive comprehension
can be found in the regression problem, where the sample space is Z = Rd+1 = X × Y = Rd × R,
the focused measure is Lebesgue measure and the bijective function π : π(z~) = ~z + ~a, ~a ∈ Z is a
translation over high-dimensional Euclidean space.
19