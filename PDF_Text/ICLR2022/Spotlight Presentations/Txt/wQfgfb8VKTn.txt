Published as a conference paper at ICLR 2022
Context-Aware Sparse Coordination Graphs
Tonghan Wang* 1,*, Liang Zeng1,*, Weijun Dong1, Qianlan Yang1, Yang Yu2, Chongjie Zhang1
1Institute for Interdisciplinary Information Sciences (IIIS), Tsinghua University
2National Key Laboratory of Novel Software Technology, Nanjing University
tonghanwang1996@gmail.com
{zengl18, dwj18, ygl18}@mails.tsinghua.edu.cn
yuy@nju.edu.cn, chongjie@tsinghua.edu.cn
Ab stract
Learning sparse coordination graphs adaptive to the coordination dynamics among
agents is a long-standing problem in cooperative multi-agent learning. This paper
studies this problem and proposes a novel method using the variance of payoff
functions to construct context-aware sparse coordination topologies. We theoret-
ically consolidate our method by proving that the smaller the variance of payoff
functions is, the less likely action selection will change after removing the corre-
sponding edge. Moreover, we propose to learn action representations to effectively
reduce the influence of payoff functions’ estimation errors on graph construction.
To empirically evaluate our method, we present the Multi-Agent COordination
(MACO) benchmark by collecting classic coordination problems in the literature,
increasing their difficulty, and classifying them into different types. We carry out
a case study and experiments on the MACO and StarCraft II micromanagement
benchmark to demonstrate the dynamics of sparse graph learning, the influence of
graph sparseness, and the learning performance of our method1.
1	Introduction
Many real-world problems involve the cooperation of multiple agents, such as unmanned aerial
vehicles (Pham et al., 2018; Xu et al., 2018) and sensor networks (Stranders et al., 2009). Like in
single-agent settings, learning control policies for multi-agent teams largely relies on the estimation
of action-value functions, no matter in value-based (Sunehag et al., 2018; Rashid et al., 2018; 2020)
or policy-based approaches (Lowe et al., 2017; Foerster et al., 2018; Wang et al., 2021c). However,
learning action-value functions for complex multi-agent tasks remains a major challenge. Learning
individual action-value functions (Tan, 1993) is scalable but suffers from learning non-stationarity
because it treats other learning agents as part of its environment. Joint action-value learning (Claus &
Boutilier, 1998) is free from learning non-stationarity but requires access to global information that is
often unavailable during execution due to partial observability and communication constraints.
Factored Q-learning (Guestrin et al., 2002a) combines the advantages of these two methods. Learning
the global action-value function as a combination of local utilities, factored Q functions maintain learn-
ing scalability while avoiding non-stationarity. Enjoying these advantages, fully decomposed Q func-
tions significantly contribute to the recent progress of multi-agent reinforcement learning (Samvelyan
et al., 2019; Wang et al., 2021b). However, when fully decomposed, local utility functions only
depend on local observations and actions, which may lead to miscoordination problems in partially
observable environments with stochastic transition functions (Wang et al., 2020; 2021a) and a game-
theoretical pathology called relative overgeneralization (Panait et al., 2006; Bohmer et al., 2020).
Relative overgeneralization renders optimal decentralized policies unlearnable when the employed
value function does not have enough representational capacity to distinguish other agents’ effects on
local utility functions.
Coordination graphs (Guestrin et al., 2002b) provide a promising approach to solving these problems.
Using vertices to represent agents and (hyper-) edges to represent payoff functions defined over the
* Equal Contribution.
1The MACO benchmark and codes are publicly available at https://github.com/TonghanWang/
CASEC-MACO-benchmark.
1
Published as a conference paper at ICLR 2022
joint action-observation space of the connected agents, a coordination graph expresses a higher-order
value decomposition among agents. Finding actions with the maximum value in a coordination graph
can be achieved by distributed constraint optimization (DCOP) algorithms (Cheng, 2012), which
consists of multiple rounds of message passing along the edges. Recently, DCG (BOhmer et al.,
2020) scales coordination graphs to large state-action spaces, shows its ability to solve the problem of
relative overgeneralization, and obtains competitive results on StarCraft II micromanagement tasks.
However, DCG focuses on predefined static and dense topologies, which largely lack flexibility for
dynamic environments and induce intensive and inefficient message passing.
The question is how to learn dynamic and sparse coordination graphs sufficient for coordinated
action selection. This is a long-standing problem in multi-agent learning. Sparse cooperative
Q-learning (Kok & Vlassis, 2006) learns value functions for sparse coordination graphs, but the
graph topology is static and predefined by prior knowledge. Zhang & Lesser (2013) propose to
learn minimized dynamic coordination sets for each agent, but the computational complexity grows
exponentially with the neighborhood size of an agent. Recently, Castellini et al. (2019) study the
representational capability of several sparse graphs but focus on random topologies and stateless
games. In this paper, we push these previous works further by proposing a novel deep method that
learns context-aware sparse coordination graphs adaptive to the dynamic coordination requirements.
For learning sparse coordination graphs, we propose to use the variance of pairwise payoff functions
as an indicator to select edges. Sparse graphs are used when selecting greedy joint actions for
execution and the update of Q-function. We provide a theoretical insight into our method by proving
that the probability of greedy action selection changing after an edge is removed decreases with
the variance of the corresponding payoff function. Despite the advantages of sparse topologies,
they raise the concern of learning instability. To solve this problem, we further equip our method
with network structures based on action representations for utility and payoff learning to reduce the
influence of estimation errors on sparse topologies learning. We call the overall learning framework
Context-Aware SparsE Coordination graphs (CASEC).
For evaluation, we present the Multi-Agent COordination (MACO) benchmark. This benchmark
collects classic coordination problems raised in the literature of multi-agent learning, increases their
difficulty, and classifies them into 6 classes. Each task in the benchmark represents a type of problem.
We carry out a case study on the MACO benchmark to show that CASEC can discover the coordination
dependence among agents under different situations and to analyze how the graph sparsity influences
action coordination. We further show that CASEC can largely reduce the communication cost
(typically by 50%) and perform significantly better than dense, static graphs and several alternative
methods for building sparse graphs. We then test CASEC on the StarCraft II micromanagement
benchmark (Samvelyan et al., 2019) to demonstrate its scalability and effectiveness.
2	Background
In this paper, we focus on fully cooperative multi-agent tasks that can be modelled as a Dec-
POMDP (Oliehoek et al., 2016) consisting of a tuple G=(I, S, A, P, R, Ω, O, n, Y)，where I is the
finite set of n agents, γ ∈ [0, 1) is the discount factor, and s ∈ S is the true state of the environment.
At each timestep, each agent i receives an observation o% ∈ Ω drawn according to the observation
function O(s, i) and selects an action ai ∈ A. Individual actions form a joint action a ∈ An, which
leads to a next state s0 according to the transition function P(s0 |s, a), a reward r = R(s, a) shared by
all agents. Each agent has local action-observation history Ti ∈ T ≡ (Ω X A)* X Ω. Agents learn to
collectively maximize the global return Qtot(s, a) = Es°,∞,。。必[P∞=0 YtR(st, at)∣so = s, ao = a].
In a coordination graph (Guestrin et al., 2002b) G = hV, Ei, each vertex vi ∈ V represents an agent
i, and (hyper-) edges in E represent coordination dependencies among agents. In this paper, we
consider pairwise edges, and such a coordination graph induces a factorization of the global Q:
,ai) + 西	^X	qij (Tij, aij ),
i	{i,j}∈E
(1)
where qi and qij is utility functions for individual agents and pairwise payoff functions, respectively.
τij = hτi, τji and aij = hai, aji is the joint action-observation history and action of agent i and j.
2
Published as a conference paper at ICLR 2022
Within a coordination graph, the greedy action selection required by Q-learning can not be completed
by simply computing the maximum of individual utility and payoff functions. Instead, distributed
constraint optimization (DCOP) (Cheng, 2012) techniques can be used. Max-Sum (Stranders et al.,
2009) is a popular implementation of DCOP, which finds optimal actions on a coordination graph
G = hV, Ei via multi-round message passing on a bipartite graph Gm = hVa, Vq , Emi. Each node
i ∈ Va represents an agent, and each node g ∈ Vq represents a utility (qi) or payoff (qij ) function.
Edges in Em connect g with the corresponding agent node(s). Message passing on this bipartite graph
starts with sending messages from node i ∈ Va to node g ∈ Vq :
mi→g (ai) =	mh→i (ai) + cig,	(2)
h∈Fi \g
where Fi is the set of nodes connected to node i in Vq, and cig is a normalizing factor preventing the
value of messages from growing arbitrarily large. The message from node g to node i is:
mg→i (ai) = max
ag \ai
q(ag) +	mh→g(ah)
h∈Vg∖i
(3)
where Vg is the set of nodes connected to node g in Va, ag= {ah|h ∈ Vg}, ag\ai= {ah|h ∈ Vg\{i}},
and q represents utility or payoff functions conditioned on ag . After several iterations of message
passing, each agent i can find its optimal action by calculating a* = argmax0i Ph∈f mh→i (ai).
A drawback of Max-Sum or other message passing methods (e.g., max-plus (Pearl, 2014)) is that
running them for each action selection through the whole system results in intensive computation and
communication among agents, which is impractical for most applications with limited computational
resources and communication bandwidth. In the following sections, we discuss how to solve this
problem by learning sparse coordination graphs.
Previous works (Naderializadeh et al., 2020; Li et al., 2021) study soft versions of fully-connected
coordination graphs based on attention mechanisms. Specifically, Li et al. (2021) uses graphs whose
edge weights are learned by self-attention so that agents attend to observations of other agents
differently. The information is used in local actors or a centralized critic. Naderializadeh et al. (2020)
learns soft full graphs in a similar way, but the graph is used to mix local utilities conditioned on
local action-observation history. Different from our work, these methods do not learn pairwise payoff
functions, and the learned graphs are still fully-connected.
3	Learning Context-Aware Sparse Graphs
In this section, we introduce our methods for learning context-aware sparse graphs. We first introduce
how we construct a sparse graph for effective action selection in Sec. 3.1. After that, we introduce our
learning framework in Sec. 3.2. Although sparse graphs can reduce communication overhead, they
raise the concern of learning instability. We discuss this problem and how to alleviate it in Sec. 3.3.
3.1	Construct Sparse Graphs
Action values, especially the pairwise payoff functions, contain much information about mutual
influence between agents. Let’s consider two agents i and j. Intuitively, agent i needs to coordinate
its action selection with agent j if agent j ’s action exerts significant influence on the expected utility
of agent i. For a fixed action ai, Varaj [qij (τij , aij )] can measure the influence of agent j on the
expected payoff. This intuition motivates us to use the variance of payoff functions
ζiqjvar = max Varaj [qij (τij, aij)] ,	(4)
ai
as an indicator to construct sparse graphs. The maximization operator guarantees that the most affected
action is considered. When ζiqjvar is large, the expected utility of agent i fluctuates dramatically with
the action of agent j , and they need to coordinate their actions. Therefore, with this measurement,
to construct sparse coordination graphs, we can set a sparseness controlling constant λ ∈ (0, 1) and
select λ∣V∣(∣V∣- 1) edges with the largest Zqvar values.
To justify this approach, we theoretically prove that, the smaller the value of ζiqjvar is, the more likely
that the Max-Sum algorithm will select the same actions after removing the edge (i, j).
3
Published as a conference paper at ICLR 2022
Proposition 1. For any two agents i, j and the edge eij connecting them in the coordination graph,
after removing edge eij, greedy actions of agent i andj selected by the Max-Sum algorithm keep
unchanged with a probability larger than
2	(m — min°j m(aj))(max°j m(aj) — m) -F	/	]2	1 ,	(5) [Zqar + 2M2 + 2JM2 (M2 + Zj)J
where m(aj) = meij→j (aj), m is the average of m(aj), M = max°j [max@i r (ai,aj) — r (ai, aj)],
and r(ai, aj) = q(ai, aj) + mi→eij (aj).
Detailed proof can be found in Appendix A. The lower bound in Proposition 1 increases with a
decreasing ζiqjvar . Therefore, edges with a smaller ζiqjvar are less likely to influence the results of
Max-Sum, justifying the way we construct sparse graphs.
3.2	Learning Framework
Like conventional Q-Iearning, CASEC consists of two main components - learning value functions
and selecting greedy actions. The difference is that these two steps are now carried out on dynamic
and sparse coordination graphs.
In CASEC, agents learn a shared utility function qξu (∙∣Ti), parameterized by ξu, and a shared pairwise
payoff function qξp (∙∣τj), parameterized by ξp. The global Q value function is estimated as:
Qtot(T, a) = M ^X qξu (ai|Ti) + ∣v∣(∣v∣ - 1) ^X qξp(aij lτij )，
i	i6=j
(6)
which is updated by the TD loss:
LTD(ξu,ξp) = ED ](r + YQtot(τ0,Max-SUm(q^u,qξp)) — Qtot(T,a))].	⑺
Max-Sum(∙, ∙) is the greedyjoint action selected by Max-Sum, Qtot is a target network with param-
eters ξu, ξp periodically copied from Qtot, and the expectation is estimated with uniform samples
from a replay buffer D. Meanwhile, we also minimize a sparseness loss
Lqparse (ξp) = ∣y∣(∣y∣ — 1)|A| XX
Varaj [qij (τij , aij)] ,	(8)
i6=j ai
which is a regularization on ζiqjvar. Introducing a scaling factor λsparse ∈ (0, 1] and minimizing
LTD(ξu, ξp) +λsparseLsqpvaarrse(ξp) builds in inductive biases which favor minimized coordination graphs
that would not sacrifice global returns.
Actions with the maximized value are selected for Q-learning and execution. In our framework,
such action selections are finished by running Max-Sum on sparse graphs induced by ζiqjvar (while we
update Q functions on the full graph). Running Max-Sum requires the message passing through each
node and edge. To speed up action selections, similar to previous work (BGhmer et al., 2020), we use
multi-layer graph neural networks without parameters to process messages in a parallel manner.
3.3	Stabilize Learning
One question with estimating qij is that there are |A| × |A| action-pairs, each of which requires an
output head in conventional deep Q networks. As only executed action-pairs are updated during
Q-learning, parameters of many output heads remain unchanged for long stretches of time, resulting
in estimation errors. Previous work (BGhmer et al., 2020) uses a low-rank approximation to reduce
the number of output heads. However, it is largely unclear how to choose the best rank K for different
tasks, and still only JAj of the output heads are selected in one Q-learning update.
This problem of estimation errors becomes especially problematic in CASEC, where building
coordination graphs relies on the estimation of qij . A negative feedback loop is created because the
built coordination graphs also affect the learning of qij . This loop renders learning unstable (Fig. 5).
4
Published as a conference paper at ICLR 2022
We propose to solve this question and stabilize training by 1) periodically fixing the way we construct
graphs via using the target payoff function to build graphs; and 2) accelerating the training of
payoff function between target network updates to reduce the estimation errors via learning action
representations.
Specifically, for 2), we propose to condition the utility and payoff functions on action representations
to improve sample efficiency. We train an action encoder fξa (a), whose input is the one-hot encoding
of an action a and output is its representation za . We adopt the technique introduced by Wang et al.
(2021b) to train an effect-based action encoder. Specifically, action representation za , together with
the current local observations, is used to predict the reward and observations at the next timestep. The
prediction loss is minimized to update the action encoder fξa (a). For more details, we refer readers
to Appendix D. The action encoder is trained with few samples when learning begins and remains
unchanged for the rest of the training process.
Using action representations, the utility and payoff functions can now be estimated as:
qξu(τi,ai) = hξu(τi)Tzai;
qξp (τij , aij) = hξp (τij) [zai , zaj ],
(9)
where h includes a GRU (Cho et al., 2014) to process sequential input and output a vector with the
same dimension as the corresponding action representation. [∙, ∙] means vector concatenation. Using
Eq. 9, no matter which action is selected for execution, all parameters in the framework (ξu and ξp)
would be updated. The detailed network structure can be found in Appendix E.
4	Multi-Agent Coordination Benchmark
To evaluate our sparse graph learning algorithm, we collect classic coordination problems from the
cooperative multi-agent learning literature, improve their difficulty, and classify them into different
types. Then, 6 representative problems are selected and presented as a new benchmark called
Multi-Agent COordination (MACO) challenge (Table 1).
At the first level, tasks are classified
as factored and non-factored games,
where factored games present an ex-
plicit decomposition of global re-
wards. Factored games are further
categorized according to two proper-
ties - whether the task requires pair-
wise or higher-order coordination, and
whether the underlying coordination
relationships change temporally. For
non-factored games, we divide them
into two classes by whether the task
characterizes static coordination rela-
Table 1: Multi-agent coordination benchmark.
Task	Factored	Pairwise	Dynamic	# Agents
Aloha	√	√		10
Pursuit	√	√	√	10
Hallway	√			12
Sensor	√		√	15
Gather		-		5
Disperse		-	√	12
tionships among agents. To better test
the performance of different methods, we increase the difficulty of the included problems by ex-
tending stateless games to temporally extended settings (Gather and Disperse), complicating the
reward function (Pursuit), or increasing the number of agents (Aloha and Hallway). We now
briefly describe the setting of each game. For detailed description, we refer readers to Appendix B.
Aloha (Hansen et al., 2004; Oliehoek, 2010) consists of 10 islands in a 2 × 5 array. Each island has a
backlog of messages to send. They send one message or not at each timestep. When two neighboring
agents send simultaneously, messages collide and have to be resent. Islands start with 1 package in
the backlog. At each timestep, with a probability 0.6 a new packet arrives if the maximum backlog
(5) has not been reached. Each agent observes its position and the number of packages in its backlog.
Agents receive a global reward of 0.1 for each successful transmission, and -10 for a collision.
Pursuit, also called Predator and Prey, is a classic coordination problem (Benda, 1986; Stone &
Veloso, 2000; Son et al., 2019). Ten agents (predators) roam a 10 × 10 map populated with 5 random
walking preys for 50 environment steps. One prey is captured if two agents catch it simultaneously,
after which the catching agents and the prey are removed from the map, resulting in a team reward
5
Published as a conference paper at ICLR 2022
Figure 1: Left: Learning curves (return and the number of successfully scanned targets) of
CASEC and DCG on Sensor. Middle: The influence of graph sparseness on performance (re-
turn and the number of scanned targets). Here we show the case of the best seed. The plot of other
seeds can be found in Fig. 21. Right: An example coordination graph learned by our method.
of 1. If only one agent tries to catch the prey, the prey would not be captured and the agents will be
punished. We consider a challenging version of Pursuit by setting the punishment to 1, which is
the same as the reward obtained by a successful catch.
Hallway (Wang et al., 2020) is a multi-chain Dec-POMDP. We increase the difficulty of Hallway
by introducing more agents and grouping them (Fig. 6). One agent randomly spawns at a state in
each chain. Agents can observe their own position and choose to move left, move right, or keep still
at each timestep. Agents win with a global reward of 1 if they arrive at state g simultaneously with
other agents in the same group. If ng > 1 groups attempt to move to g at the same timestep, they
keep motionless and agents receive a global punishment of -0.5 * ng.
Sensor has been extensively studied (Lesser et al., 2012; Zhang & Lesser, 2011). We consider 15
sensors in a 3 × 5 matrix. Sensors can scan the eight nearby points. Each scan induces a cost of -1,
and agents can do noop to save the cost. Three targets wander randomly in the gird. If k ≥ 2 sensors
scan a target simultaneously, the system gets a constant reward of 3, which is independent of the
number of sensors. Agents can observe the id and position of targets nearby.
Gather is an extension of the Climb Game (Wei & Luke, 2016). In Climb Game, each agent has
three actions: A = {a0, a1, a2}. Action a0 yields no reward (0) if only some agents choose it, but a
high reward (10) if all choose it. Otherwise, if no agent chooses action a0, a reward 5 is obtained. We
increase the difficulty of this game by making it temporally extended and introducing stochasticity.
Actions are no longer atomic, and agents need to learn policies to realize these actions by navigating
to goals g1, g2 and g3 (Fig. 7). Moreover, for each episode, one of g1, g2 and g3 is randomly selected
as the optimal goal (corresponding to a0 in the original game).
Disperse consists of 12 agents. At each timestep, agents can choose to work at one of 4 hospitals
by selecting an action in A = {a0, a1, a2, a3}. At timestep t, hospital j needs xtj agents for the next
timestep. One hospital is randomly selected and its xtj is a positive number, while the need of other
hospitals is 0. If ytj+1 < xtj agents go to the selected hospital, the whole team would be punished
ytj+1 - xtj . Agents observe the local hospital’s id and its need for the next timestep.
5	Case Study: Learning Sparse Graphs on Sensor
We are particularly interested in the dynamics and results of sparse graph learning. Therefore, we
carry out a case study on Sensor. When training CASEC on this task, we select 10% edges with
largest ζiqjvar values to construct sparse graphs.
Interpretable sparse coordination graphs. In Fig. 1 right, we show a screenshot of the game with
the learned coordination graph at a certain timestep. We can observe that all edges in the learned
graph involve agents around the targets. Let’s see the case of agent 8. The action proposed by
the individual utility function of agent 8 is to scan target 1. After coordinating its action with
other agents, agent 8 changes its action selection and scans target target 2, resulting in an optimal
solution for the given configuration. This result is in line with our theoretical analysis in Sec. 3.1.
The most important edges can be characterized by a large ζ value.
Influence of graph sparseness on performance. It is worth noting that with fewer edges in the
coordination graph, CASEC has better performance than DCG on Sensor (Fig. 1 left, where the
6
Published as a conference paper at ICLR 2022
CASEC (sparse graph with 10% edges)
CASEC w/o sparse loss
(sparse graph with 10% edges)
DCG (sparse graph with 10% edges)
0	1	2	3	4	5	6	7	8
Targets and '
・ their ids
Figure 2: Coordination graphs learned by different methods on Sensor.
DCG (full graph)
0	1	2	3	4	5	6	7	8
OAgentS and
their ids
——CASEC (Ours)	—— VDN	≈	Weighted QMIX (CW-QMIX)	DICG
DCG	—— QMIX	—— Weighted QMIX (OW-QMIX)	GraphMix
Aloha	Pursuit	Hallway
Kθ 03 LO L5 ∑y
T (mil)
Sensor
£6neo Aa∣ds⅝L
100
75
50
25
O
0.0
T (mil)
0.4	0.5
IOO
Sζ 75
I 50
⅛
,25
Ko 03 LO L5 2T
T (mil)
Gather
0.0	0.2	0.4	0.6	0.8	1.0
T (mil)
Figure 3: Performance comparison with baselines on the MACO benchmark.
0.2	0.4	0.6
T (mil)
Disperse
0.0	0.2	0.4	0.6
T (mil)
% WM ttfi∙
median performance and 25%-75% percentiles are shown). This observation may be counter-intuitive
at the first glance. To study this problem, we load the model after convergence learned by CASEC and
DCG, gradually remove edges from the full graph in the ascending order of ζiqjvar , and check the
change of scanned targets and the obtained reward. Results are shown in Fig. 1 middle and Fig. 21.
It can be observed that the performance of DCG (the number of scanned targets) does not change with
the number of edges. In another word, only the individual utility function contributes to scanning
targets. Screenshots shown in Fig. 2 (right column) align with this observation. With more edges,
DCG makes a less optimal decision: agent 4, 5, and 9 no longer scan target 1.
In contrast, the performance of CASEC grows with more edges in the coordination graph. By
referring to Fig. 2 (left column), we can conclude that CASEC first selects edges that help agents scan
more targets, and then selects edges that can eliminate useless scan actions. These results demonstrate
that our method can distinguish the most important edges on Sensor.
We also study the influence of the sparseness loss (Eq. 8). As shown in Fig. 1 middle, CASEC without
the sparseness loss consistently gets fewer rewards than CASEC. For example, target 1 and 3 are not
captured in the case shown in Fig. 2 (middle column) as only one agent scans them. These results
highlight the function of the sparseness loss.
6	Experiments
In this section, we design experiments to answer the following questions: (1) How much communi-
cation can be saved by our method? How does communication threshold influence performance on
factored and non-factored games? (Sec. 6.1) (2) How does our method compare to state-of-the-art
multi-agent learning methods? (Sec. 6.2, 6.3) (3) Is our method efficient in settings with larger
7
Published as a conference paper at ICLR 2022
action-observation spaces? (Sec. 6.3) For results in this section, we show the median performance
with 8 random seeds as well as the 25-75% percentiles.
6.1	Graph Sparseness
An important advantage of learning sparse coordination graphs is
reduced communication costs. The complexity of running Max-
Sum for each action selection is O (k (|V||A| + ∣E∣∣A∣2)), where
k is the number of iterations of message passing. Sparse graphs
cut down communication costs by reducing the number of edges.
We carry out a grid search to find the communication threshold
under which sparse graphs have the best performance. We find
that most implementations of dynamically sparse graphs require
similar numbers of edges to prevent performance from dropping significantly. In Table 2, we show the
communication cut rates we use when benchmarking our method. Generally speaking, non-factored
games typically require more messages than factored games, while, for most tasks, at least 50%
messages can be saved without sacrificing the learning performance.
Table 2: Percentage of commu-
nication saved for each task.
Aloha	Pursuit	Hallway
80.0%	70.0%	50.0%
Sensor	Gather	Disperse
90.0%	30.0%	60.0%
Communication threshold vs. performance In
Fig. 4, we show the performance of our method
under different communication thresholds which
control the sparseness of edges. We can observe
that, on the factored game Sensor, performance
first grows then drops when more edges are in-
cluded in the coordination graphs. These ob-
servations are in line with the fact that sparse
graphs can outperform complete graphs and
fully-decomposed value functions on this task.
In contrast, for the non-factored game Gather,
performance stabilizes beyond a certain thresh-
old. Non-factored games usually involve com-
Sensor (A Factored Game)
Figure 4: The influence of graph sparseness (1.0
represents complete graphs) on the performance
on factored games (Sensor, left) and non-factored
games (Gather, right).
plex coordination relationships, and denser topologies are suitable for this type of questions.
6.2	MACO: Multi-Agent Coordination Benchmark
We compare our method with state-of-the-art fully-decomposed value-based methods (VDN (Sunehag
et al., 2018), QMIX (Rashid et al., 2018), and Weighted QMIX (Rashid et al., 2020)), coordina-
tion graph learning method (DCG (BOhmer et al., 2020)), and attentional graph learning methods
(DICG (Li et al., 2021) and GraphMIX (Naderializadeh et al., 2020)) on MACO (Fig. 3). Since the
number of actions is not very large in MACO, we do not use action representations when estimating
the utility and payoff function for CASEC.
We can see that our method significantly outperforms fully-decomposed value-based methods. The
reason is that fully-decomposed methods suffer from the relative overgeneralization issue and
miscoordination problems in partially observable environments with stochasticity. For example, on
task Pursuit (Benda, 1986), if more than one agent catches one prey simultaneously, these agents
will be rewarded 1. However, if only one agent catches prey, it fails and gets a punishment of -1. For
an agent with a limited sight range, the reward it obtains when taking the same action (catching a prey)
under the same local observation depends on the actions of other agents and changes dramatically.
This is the relative overgeneralization problem. Another example is Hallway (Wang et al., 2020),
where several agents need to reach a goal state simultaneously without knowing each other’s location.
Fully-decomposed methods cannot solve this problem if the initial positions of agents are stochastic.
For DCG, we use its default settings of complete graphs and no low-rank approximation. We observe
that DCG is less effective on tasks characterized by sparse coordination interdependence like Sensor.
We hypothesize this is because coordinating actions with all other agents requires the shared estimator
to express payoff functions of most agent pairs accurately enough, which needs more samples to
learn, hurting the performance of DCG on loosely coupled tasks.
8
Published as a conference paper at ICLR 2022
VDN	----- DCG (rank 1)	----CASEC (w∕o action repr.) -----Full (action repr.)	GraphMIX
QMIX	—— CASEC (Ours)	—— CASEC (rank 1)	DICG
Figure 5: Performance and TD errors compared to baselines and ablations on the SMAC benchmark.
6.3	StarCraft II Micromanagement Benchmark
We compare our method against the state-of-the-art coordination graph learning method (DCG (BGh-
mer et al., 2020)) and fully decomposed value-based MARL algorithms (VDN (Sunehag et al., 2018),
QMIX (Rashid et al., 2018)). For CASEC, we use action representations to estimate the payoff
function. We train the action encoder for 50k samples and keep action representations unchanged
afterward. In Fig. 5, we show results on 5m_vs_6m and MMM2. Detailed hyperparameter settings of
our method can be found in Appendix E.
For DCG, we use its default settings, including a low-rank approximation for learning the payoff
function. We can see that CASEC outperforms DCG by a large margin. The result proves that sparse
coordination graphs provide better scalability to large action-observation spaces than dense and static
graphs. In DCG’s defense, low-rank approximation still induces large estimation errors. We replace
low-rank approximation with action representations and find that DCG (Full (action repr.)) achieves
similar performance to CASEC after 5M steps, but CASEC is still more sample-efficient. Moreover,
taking advantage of higher-order value decomposition, CASEC is able to represent more complex
coordination dynamics than fully decomposed value functions and thus performs better.
Ablation study Our method is characterized by two contributions: context-aware sparse topologies
and action representations for learning the utility and payoff function. In this section, we design
ablations to show their contributions.
The effect of sparse topologies can be observed by comparing CASEC to Full (action repr.), which is
the same as CASEC other than using complete coordination graphs. We observe that sparse graphs
enjoy better sample efficiency than full graphs, and the advantage becomes less obvious as more
samples are collected. This observation indicates that sparse graphs introduce inductive biases that
can accelerate training, and their representational capacity is similar to that of full graphs.
From the comparison between CASEC to CASEC using conventional Q networks (w/o action repr.),
we can see that using action representations can significantly stabilize learning. For example, learning
diverges on 5m_vs_6m without action representations. As analyzed before, this is because a negative
feedback loop is created between the inaccurate payoff function and coordination graphs.
To further consolidate that action representations can reduce the estimation errors and thus alleviate
learning oscillation as discussed in Sec. 3.3, we visualize the TD errors of CASEC and ablations
during training in Fig. 5 right. We can see that action representations can dramatically reduce the TD
errors. For comparison, the low-rank approximation can also reduce the TD errors, but much less
significantly. Smaller TD errors prove that action representations provide better estimations of the
value function, and learning with sparse graphs can thus be stabilized (Fig. 5 left).
7	Conclusion
We study how to learn dynamic sparse coordination graphs, which is a long-standing problem in
cooperative MARL. We propose a specific implementation and theoretically justify it. Empirically,
we evaluate the proposed method on a new multi-agent coordination benchmark. Moreover, we
equip our method with action representations to improve the sample efficiency of payoff learning and
stabilize training. We show that sparse and adaptive topologies can largely reduce communication
overhead as well as improve the performance of coordination graphs. We expect our work to extend
MARL to more realistic tasks with complex coordination dynamics.
9
Published as a conference paper at ICLR 2022
One limitation of our method is that the learned sparse graphs are not always cycle-free. Since the
Max-Sum algorithm guarantees optimality only on acyclic graphs, our method may select sub-optimal
actions. In Appendix F, we study this problem in depth.
Another limitation is that we fix the communication threshold when training. It is an important
question how to automatically and accurately find the minimum threshold that can guarantee the
learning performance. In Appendix I, we study two ways to adaptively select the threshold.
Reproducibility The source code for all the experiments along with a README file with in-
structions on how to run these experiments is attached in the supplementary material. In addition,
the settings and parameters for all models and algorithms mentioned in the experiment section are
detailed in Appendix E.
References
Miroslav Benda. On optimal cooperation of knowledge sources: an empirical investigation. Technical
Report, Boeing Advanced Technology Center, 1986.
Wendelin Bohmer, Vitaly Kurin, and Shimon Whiteson. Deep coordination graphs. In Proceedings
of the 37th International Conference on Machine Learning, 2020.
Jacopo Castellini, Frans A Oliehoek, Rahul Savani, and Shimon Whiteson. The representational
capacity of action-value networks for multi-agent reinforcement learning. In Proceedings of the
18th International Conference on Autonomous Agents and MultiAgent Systems, pp. 1862-1864.
International Foundation for Autonomous Agents and Multiagent Systems, 2019.
Shanjun Cheng. Coordinating decentralized learning and conflict resolution across agent boundaries.
PhD thesis, The University of North Carolina at Charlotte, 2012.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for
statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pp. 1724-1734, 2014.
Caroline Claus and Craig Boutilier. The dynamics of reinforcement learning in cooperative multiagent
systems. AAAI/IAAI, 1998(746-752):2, 1998.
Jakob N Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In Thirty-Second AAAI Conference on Artificial
Intelligence, 2018.
Carlos Guestrin, Daphne Koller, and Ronald Parr. Multiagent planning with factored mdps. In
Advances in neural information processing systems, pp. 1523-1530, 2002a.
Carlos Guestrin, Michail Lagoudakis, and Ronald Parr. Coordinated reinforcement learning. In
ICML, volume 2, pp. 227-234. Citeseer, 2002b.
Eric A Hansen, Daniel S Bernstein, and Shlomo Zilberstein. Dynamic programming for partially
observable stochastic games. In AAAI, volume 4, pp. 709-715, 2004.
Jelle R Kok and Nikos Vlassis. Collaborative multiagent reinforcement learning by payoff propagation.
Journal of Machine Learning Research, 7(Sep):1789-1828, 2006.
Victor Lesser, Charles L Ortiz Jr, and Milind Tambe. Distributed sensor networks: A multiagent
perspective, volume 9. Springer Science & Business Media, 2012.
Sheng Li, Jayesh K Gupta, Peter Morales, Ross Allen, and Mykel J Kochenderfer. Deep implicit co-
ordination graphs for multi-agent reinforcement learning. In Proceedings of the 20th International
Conference on Autonomous Agents and MultiAgent Systems, pp. 764-772, 2021.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent
actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information
Processing Systems, pp. 6379-6390, 2017.
10
Published as a conference paper at ICLR 2022
Michael Mitzenmacher and Eli Upfal. Probability and computing: Randomization and probabilistic
techniques in algorithms and data analysis. Cambridge university press, 2017.
Navid Naderializadeh, Fan H Hung, Sean Soleyman, and Deepak Khosla. Graph convolutional value
decomposition in multi-agent reinforcement learning. arXiv preprint arXiv:2010.04740, 2020.
Julius Nagy. Uber algebraische gleichungen mit lauter reellen Wurzeln. Jahresbericht der Deutschen
Mathematiker-Vereinigung, 27:37-43, 1918.
Frans Oliehoek. Value-based planning for teams of agents in stochastic partially observable environ-
ments. Amsterdam University Press, 2010.
Frans A Oliehoek, Christopher Amato, et al. A concise introduction to decentralized POMDPs,
volume 1. Springer, 2016.
Liviu Panait, Sean Luke, and R Paul Wiegand. Biasing coevolutionary search for optimal multiagent
behaviors. IEEE Transactions on Evolutionary Computation, 10(6):629-645, 2006.
Judea Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. Elsevier,
2014.
Huy Xuan Pham, Hung Manh La, David Feil-Seifer, and Aria Nefian. Cooperative and distributed
reinforcement learning of drones for field coverage, 2018.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder Witt, Gregory Farquhar, Jakob Foerster,
and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent
reinforcement learning. In International Conference on Machine Learning, pp. 4292-4301, 2018.
Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted qmix: Expanding
monotonic value function factorisation for deep multi-agent reinforcement learning. Advances in
Neural Information Processing Systems, 33, 2020.
Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli,
Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The
starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.
KyunghWan Son, DaeWoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning to
factorize With transformation for cooperative multi-agent reinforcement learning. In International
Conference on Machine Learning, pp. 5887-5896, 2019.
Peter Stone and Manuela Veloso. Multiagent systems: A survey from a machine learning perspective.
Autonomous Robots, 8(3):345-383, 2000.
Ruben Stranders, Alessandro Farinelli, Alex Rogers, and Nick Jennings. Decentralised coordination
of mobile sensors using the max-sum algorithm. In Twenty-First International Joint Conference on
Artificial Intelligence, 2009.
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max
Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition
netWorks for cooperative multi-agent learning based on team reWard. In Proceedings of the
17th International Conference on Autonomous Agents and MultiAgent Systems, pp. 2085-2087.
International Foundation for Autonomous Agents and Multiagent Systems, 2018.
Ming Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In Proceedings
of the tenth international conference on machine learning, pp. 330-337, 1993.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling
multi-agent q-learning. International Conference on Learning Representations (ICLR), 2021a.
11
Published as a conference paper at ICLR 2022
Tonghan Wang, Jianhao Wang, Chongyi Zheng, and Chongjie Zhang. Learning nearly decomposable
value functions with communication minimization. In Proceedings of the International Conference
on Learning Representations (ICLR), 2020.
Tonghan Wang, Tarun Gupta, Anuj Mahajan, Bei Peng, Shimon Whiteson, and Chongjie Zhang. Rode:
Learning roles to decompose multi-agent tasks. In Proceedings of the International Conference on
Learning Representations (ICLR), 2021b.
Yihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang. Dop: Off-policy
multi-agent decomposed policy gradients. In Proceedings of the International Conference on
Learning Representations (ICLR), 2021c.
Ermo Wei and Sean Luke. Lenient learning in independent-learner stochastic cooperative games. The
Journal of Machine Learning Research ,17(1):2914-2955, 2016.
Zhao Xu, Yang Lyu, Quan Pan, Jinwen Hu, Chunhui Zhao, and Shuai Liu. Multi-vehicle flocking
control with deep deterministic policy gradient method. 2018 IEEE 14th International Conference
on Control and Automation (ICCA), Jun 2018. doi: 10.1109/icca.2018.8444355. URL http:
//dx.doi.org/10.1109/ICCA.2018.8444355.
Chongjie Zhang and Victor Lesser. Coordinated multi-agent reinforcement learning in networked
distributed pomdps. In Twenty-Fifth AAAI Conference on Artificial Intelligence, 2011.
Chongjie Zhang and Victor Lesser. Coordinating multi-agent reinforcement learning with limited
communication. In Proceedings of the 2013 international conference on Autonomous agents
and multi-agent systems, pp. 1101-1108. International Foundation for Autonomous Agents and
Multiagent Systems, 2013.
12
Published as a conference paper at ICLR 2022
A Mathematical Proof
In this section, we provide proof to Proposition 1.
Without loss of generality, we consider two agents 1 and 2 and the edge between them (1, 2). We
prove our idea by comparing the action selection of agent 2 before and after removing edge (1, 2). In
the following proof, we use i (i = 1, 2) to denote agent i and e to denote edge (1, 2).
Action of agent 2 is determined by
a2 = arg max，mh→2(a2)	(10)
a2
h∈F2
= arg max me→2(a2) +	mh→2(a2) ,	(11)
a2
h∈F2∕{e}
and We first see the influence of me→2(a2) on ag. For clarity, We use m(a2) to denote me→2(a2)
and l(a2) to denote Ph∈F /{e} mh→2(a2). We are interested in whether arg maxa2 l(a2) =
arg maxa2 [m(a2) + l(a2)]. The probability of this event holds if the folloWing inequality holds:
Range [m(a2)] ≤ min (l(aj2) - l(a2)),	(12)
a2 6=a2
Where Range(x) denotes the largest elements in vector x minus the smallest one and aj2 =
arg maxa2 l(a2). We reWrite Eq. 12 and obtain
Pr Range [m(a2)] ≤ min (l(aj2) - l(a2))	(13)
a2 6=aj2
=Pr min m(a2) < m(a2) < min m(a2) + min (l(aj2) - l(a2)) .	(14)
a2	a2	a2 6=aj2
According to the Asymmetric tWo-sided Chebyshev’s inequality (Mitzenmacher & Upfal, 2017), We
get a loWer bound of this probability:
4
(m — min。？ m(a2))(maxa2 m(a2) — m) 一 σ2
hmina2 6=aj2 (l(aj2) - l(a2))i2
(15)
where σ is the variance of m(a2), and m is the average of m(a2).
Suppose that We take |A| actions independently. According to the von Szokefalvi Nagy inequal-
ity (Nagy, 1918), We can further get the loWer bound as folloWs:
(m — min。？ m(a2))(maxa2 m(a2) — m) — σ
4
hmina2 = aj2(l(aj2)—l(a2))i2
2
■R
2〉4 (m - min02 m(a2))(maxa2 m(a2) — m) — σ2
2∣A∣σ2
(m - min。？ m(a2))(maxa2 m(a2) — m) *
σ2
(16)
1
Note that
m(a2) = max [q(a1, a2) + m1→e(a1)] ,	(17)
。1
and We are interested in q(a1, a2). We noW study the relationship betWeen m(a2) and
maxaι [q(aι,a2)]. For clarity, we use r(aι,a2) to denote q(aι,a2) + m1→e(a1), and r(aɪ2,ak)
to denote max。】 r(aι,a2). Then we have Vara2 max。】 r(aι,a2) = Vara2r(aɪ2, a2).
For a given a2, we have
Varoz r(aι,a2) = Var。2 r (a12 , a2) — s2 .	(18)
13
Published as a conference paper at ICLR 2022
Here s2 ≥ 0, ∀a1, because i2 = arg maxi r(a1, a2).
Since
Vara2 r(ai12, a2) — s2
=Vara2 r(ai12, a2) + Var [s2] — 2Cov(r(ai12, a2), s2)
(19)
(20)
and
Cov(r(ai12,a2), s2) ≤	Vara2 r(ai12, a2) Var[s2],
(21)
it follows that
Vara2 r(ai12, a2) — s2
≥Var02 [r(a12,a2)] — 2 JVara2 [r(a；2 ,a2)] Var [s2].
(22)
(23)
Thus,
ζ12 [r(a1, a2)] = max Vara2 [r(a1, a2)] ≥ Vara2 max [r(a1, a2)] — 2 Vara2 [r(ai12, a2)] Var [s2].
a1	a1
Observing that ζ12 [r(a1 , a2 )]
ζ12 [q(a1, a2)], we have
maxa1 Vara2 [r(a1, a2)]
(24)
maxa1 Vara2 [q(a1, a2)]	=
σ ≤ ζ12 [q(a1, a2)] + 2 Vara2 [r(a12 , a2)] Var [s2]
(25)
=Z12 [q(aι, a2)] + 2√σS,
where σ = Vara2 maxa1 [r(a1, a2)] and Var [s2] = S. According to the fixed-point theorem, the term
σ satisfies Z12 [q(a1,a2)]+2√σS = σ. We can solve this quadratic form and get σ = Z12 [q(a1, a2)] +
2S ± 2，S (S + Z12 [q(a1,a2)]). Because the σ term is larger than Z12 [q(a1, a2)] + 2S, we get
σ = Z12 [q(a1, a2)] + 2S + 2y/S (S + Z12 [q(aι, a2)]). By inserting this inequality to the lower
bound (Eq. 16), we get a lower bound related to q(a1, a2):
2
|A|
(m - min。？ m(a2))(maxa2 m(a2) — m)	]
[q(aι, a2)] + 2S + 2pS (s + ζ12 [q(aι, a2)])i
(26)
When a vector X is larger than 0 and the the cardinality of X is n, We have: Var(X) = n Pin=1(xi 一
§ pn=1 Xi)2 ≤ § Pn=1 χ2 ≤ maxi χ2. Thus we can further get the following bound:
S = Vara2
maxr (a1, a2) — r (a1, a2)
a1
≤ max2a2
maxr (a1, a2) — r (a1, a2) .
a1
(27)
Let M = maxa2 [maxa1 r (a1, a2) — r (a1, a2)], thus we have S ≤ M2. We have ζ12 [q(a1, a2)]
ζ1q2var by the definition (Eq. 4) and can get the final lower bound:
2
问
(rm — mina2 m(a2))(maxa2 m(a2) — rn)
_______________________1 2
+ 2M2 + 2√M2 (M2 + ζq2ar)
-1
B MACO: Multi-Agent Coordination Benchmark
(28)
In this paper, we study how to learn context-aware sparse coordination graphs. For this purpose,
we propose a new Multi-Agent COordination (MACO) benchmark (Table 1) to evaluate different
implementations and benchmark our method. This benchmark collects classic coordination problems
in the literature of cooperative multi-agent learning, increases their difficulty, and classifies them into
different types. We now describe the detailed settings of tasks in the MACO benchmark.
14
Published as a conference paper at ICLR 2022
B.1 Task Settings
Factored Games are characterized by a clear factorization of global rewards. We further classify
factored games into 4 categories according to whether coordination dependency is pairwise and
whether the underlying coordination graph is dynamic (Table 1).
Aloha (Oliehoek (2010), also similar to the Broadcast Channel benchmark problem proposed by
Hansen et al. (2004)) consists of 10 islands, each equipped with a radio tower to transmit messages
to its residents. Each island has a backlog of messages that it needs to send, and agents can choose
to send one message or not at each timestep. Due to the proximity of islands, communications
from adjacent islands interfere. This means that when two neighboring agents attempt to send
simultaneously, a collision occurs and the messages have to be resent. Each island starts with 1
package in its backlog. At each timestep, with probability 0.6 a new packet arrives if the maximum
backlog (set to 5) has not been reached. Each agent observes its position and the number of packages
in its backlog. A global reward of 0.1 is received by the system for each successful transmission,
while punishment of -10 is induced if the transmission leads to a collision.
Pursuit, also called Predator and Prey, is a classic coordination problem (Benda, 1986; Stone &
Veloso, 2000; Son et al., 2019). In this game, ten agents (predators) roam a 10 × 10 map populated
with 5 random walking preys for 50 environment steps. Based on the partial observation of any
adjacent prey and other predators, agents choose to move in four directions, keep motionless, or
catch prey (specified by its id). One prey is captured if two agents catch it simultaneously, after
which the catching agents and the prey are removed from the map, resulting in a team reward of 1.
However, if only one agent tries to catch the prey, the prey would not be captured and the agents will
be punished. The difficulty of Pursuit is largely decided by the relative scale of the punishment
compared to the catching reward (BOhmer et al., 2020), because a large punishment exacerbates the
relative overgeneralization pathology. In the MACO benchmark, we consider a challenging version
of Pursuit by setting the punishment to 1, which is the same as the reward obtained by a successful
catch.
Hallway (Wang et al., 2020) is a multi-chain Dec-POMDP whose stochasticity and partial observ-
ability lead to fully-decomposed value functions learning sub-optimal strategies. In the MACO bench-
mark, we increase the difficulty of Hallway by introducing more agents and grouping them (Fig. 6).
One agent randomly spawns at a state in each chain. Agents can observe its own position and choose
to move left, move right, or keep still at each timestep. Agents win if they arrive at state g simultane-
ously with other agents in the same group. In Fig. 6, different groups are drawn in different colors.
Each winning group induces a global reward of 1. Other-
wise, if any agent arrives at g earlier than others, the sys-
tem receives no reward and all agents in that group would
be removed from the game. If ng > 1 groups attempt
to move to g at the same timestep, they keep motionless
and agents receive a global punishment of -0.5 * ng. The
horizon is set to maxi li + 10 to avoid an infinite loop,
where li is the length of chain i.
Sensor (Fig. 3 in the main text) has been extensively
studied in cooperative multi-agent learning (Lesser et al.,
2012; Zhang & Lesser, 2011). We consider 15 sensors
arranged in a 3 by 5 matrix. Each sensor is controlled by
Figure 6: Task Hallway (Wang et al.,
2020). To increase the difficulty of the
game, we consider a multi-group ver-
sion. Different colors represent different
groups.
an agent and can scan the eight nearby points. Each scan induces a cost of -1, and agents can choose
noop to save the cost. Three targets wander randomly in the gird. If k ≥ 2 sensors scan a target
simultaneously, the system gets a constant reward of 3, which is independent of the number of sensor.
Agents can observe the id and position of targets nearby.
Non-factored games do not present an explicit decomposition of global rewards. We classify non-
factored games according to whether the game can be solved by a static (sparse) coordination graph
in a single episode.
Gather is an extension of the Climb Game (Wei & Luke, 2016). In Climb Game, each agent has
three actions: A = {a0, a1, a2}. Action a0 yields no reward if only some agents choose it, but a high
reward if all choose it. The other two actions are sub-optimal actions but can induce a positive reward
15
Published as a conference paper at ICLR 2022
Figure 8: Performance of different implementations on Aloha. Different colors indicate different
topologies. Performance of different losses is shown in different sub-figures.
without requiring precise coordination:
{10 #a0 = n,
0	0 < #a0 < n,
5 otherwise.
(29)
We increase the difficulty of this game by making it temporally extended and introducing stochasticity.
We consider three actions. Actions are no longer atomic, and agents need to learn policies to realize
these actions by navigating to goals g1, g2 and g3 (Fig. 7).
Moreover, for each episode, one of g1, g2 and g3 is randomly
selected as the optimal goal (corresponding to a0 in Eq. 29).
Agents spawn randomly, and only agents initialized near the
optimal goal know which goal is optimal. Agents need to
simultaneously arrive at a goal state to get any reward. If all
agents are at the optimal goal state, they get a high reward
of 10. If all of them are at other goal states, they would be
rewarded 5. The minimum reward would be received if only
some agents gather at the optimal goal. We further increase the
difficulty by setting this reward to -5. It is worth noting that,
for any single episode, Gather can be solved using a static and
sparse coordination graph - for example, agents can collectively
coordinate with an agent who knows the optimal goal.
Figure 7: Task Gather. To increase
the difficulty of this game, we con-
sider a temporally extended version
and introduce stochasticity.
Figure 9: Performance of different implementations on Pursuit. Different colors indicate different
topologies. Performance of different losses is shown in different sub-figures.
0.0	0.2	0.4	0.6	0.8	1.0
T (mil)
ŋ 5 O 5
0 7 5 2
% WM ttfi∙
Rand. ------- 6maχ ------- qvar ------ ovar
75
50
25
0
i 0.6	0.8	1.0
T (mil)
% WM ttfi∙
0；0	0：2	0：4	0：6	0：8 LO
T (mil)
0.2
Figure 10: Performance of different implementations on Hallway. Different colors indicate different
topologies. Performance of different losses is shown in different sub-figures.
16
Published as a conference paper at ICLR 2022
Figure 11: Performance of different implementations on Sensor. Different colors indicate different
topologies. Performance of different losses is shown in different sub-figures.
FUll ------ Rand. -------- βmax ------- qvar ------- ðvar
CSarSe	^⅛rse	以掰Se
0.0	0.2	0.4	0.6	0.8	1.0
T (mil)
0.2
0.4 C
T (mil)
0.0	0.2	0.4 C
T (mil)
Figure 12: Performance of different implementations on Gather. Different colors indicate different
topologies. Performance of different losses is shown in different sub-figures.
Disperse consists of 12 agents. At each timestep, agents can choose to work at one of 4 hospitals
by selecting an action in A = {a0, a1, a2, a3}. At timestep t, hospital j needs xtj agents for the next
timestep. One hospital is randomly selected and its xtj is a positive number, while the need of other
hospitals is 0. If ytj+1 < xtj agents go to the selected hospital, the whole team would be punished
ytj+1 - xtj . Agents observe the local hospital’s id and its need for the next timestep.
B.2 Other Possible Implements and Performance Comparison
With this benchmark in hand, we are now able to evaluate our method for constructing sparse graphs.
We compare our method with the following approaches.
Maximum utility difference qi (or qj ) is the expected utility agent i (or j) can get without the
awareness of actions of other agents. After specifying the action of agent j or i, the joint expected
utility changes to qij . Thus the measurement
Zijmax = max | δij (Tij a j) |	(30)
aij
can describe the mutual influence between agent i and j . Here
δij (τij , aij ) = qij (τij , aij ) - qi (τi , ai ) - qj (τj , aj )	(31)
is the utility difference function.
We use a maximization operator here because two agents need to coordinate with each other if such
coordination significantly affects the probability of selecting at least one action pair.
Variance of utility difference As discussed before, the value of utility difference δij and variance of
payoff functions can measure the mutual influence between agent i and j . In this way, the variance of
δij serves as a second-order measurement, and we can use
ζiδjvar = max Varaj [δij(τij, aij)]	(32)
ai
to rank the necessity of coordination relationships between agents. Again we use the maximization
operation to base the measurement on the most influenced action.
For these three measurements (Eq. 8, 30, and 32), the larger value of ζij is, the more important the
edge (i, j) is. For example, when ζiqjvar = maxai Varaj [qij (τij, aij)] is large, the expected utility of
17
Published as a conference paper at ICLR 2022
Full ------- Rand. --------- δmax --------- qvar -------- OVar
^⅛!arse	^sparse	^sparse
0.0	0.2	0.4	0.6	0.8	1.0	0.0	0.2	0.4	0.6	0.8	1.0
T (mil)	T (mil)
Figure 13: Performance of different implementations on Disperse. Different colors indicate different
topologies. Performance of different losses is shown in different sub-figures.
0.0	0.2	0.4	0.6	0.8	1.0
T (mil)
agent i fluctuates dramatically with the action of agent j, and they need to coordinate their actions.
Therefore, with these measurements, to learn sparse coordination graphs, we can set a sparseness
controlling constant λ ∈ (0,1) and select λ∣V∣(∣V∣ - 1) edges with the largest Zij values. To make
the measurements more accurate in edge selection, we minimize the following losses for the two
measurements, respectively:
L∣δ∣
sparse
1
M(M-I)W
ΣΣ ]δij (Tij, aij ) |;
Lδvar
sparse
1
M(M-i)∣A∣
ΣΣVaraj [δij (τij , aij )] .
i6=j ai
(33)
(34)
We scale these losses with a factor λsparse and optimize them together with the TD loss. It is worth
noting that these measurements and losses are not independent. For example, minimizing Lsδpvaarrse
would also reduce the variance of qij . Thus, in the next section, we consider all possible combinations
between these measurements and losses.
Observation-Based Approaches In partial observable environments, agents sometimes need to
coordinate with each other to share their observations and reduce their uncertainty about the true
state (Wang et al., 2020). We can build our coordination graphs according to this intuition.
Agents use an attention model (Vaswani et al., 2017) to select the information they need. Specifically,
we train fully connected networks fk and fq and estimate the importance of agent j’s observations to
agent i by:
αij = fk (τi)Tfq (τj ).	(35)
Then we calculate the global Q function as:
(36)
where αj = eaij / Ei=j∙ eaij. Then both fk and fq can be trained end-to-end with the standard
TD loss. When building coordination graphs, given a sparseness controlling factor λ, we select
λ∣V∣(∣V∣ - 1) edges with the largest αj values.
B.3 Which method is better for learning dynamically sparse coordination
graphs ?
We show the learning curves of value-based implementations in Fig. 8-13 and compare our method
(ζiqjvar & Lsqpvaarrse) against the (attentional) observation-based method in Fig. 14. We can see that our
proposed method generally performs better than the observation-based method, except for the task
Disperse. Compared to other games, observations provided by Disperse can reveal all the game
information. In this case, the observation-based method can make better use of local observations
and can easily learn an accurate coordination graph.
C The SMAC benchmark
On the SMAC benchmark, we compare our method against fully decomposed value function learning
methods (VDN (Sunehag et al., 2018) & QMIX (Rashid et al., 2018)) and a deep coordination graph
18
Published as a conference paper at ICLR 2022
----qvar& 戏■------------- Attn.
Aloha	Pursuit	Hallway
£6neo Aa∣ds⅝L
0.0
0.5	1.0
T (mil)
Gather
% WM ttfi∙
O
0.0	0.1	0.2	0.3	0.4	0.5	0.0	0.2	0.4	0.6
T (mil)	T (mil)
Figure 14: Performance comparison between our method (ζiqjvar & Lsqpvaarrse) and the (attentional)
observation-based approach on the MACO benchmark.
u∙lnsHs⅝L
1.0	0.0	0.2	0.4	0.6	0.8	1.0
T (mil)
learning method (DCG (BGhmer et al., 2020)). Experiments are carried out on a hard map 5m_vs_6m
and a super hard map MMM2. For the baselines, we use the code provided by the authors and their
default hyper-parameters settings that have been fine-tuned on the SMAC benchmark. We also notice
that both our method and all the considered baselines are implemented based on the open-sourced
codebase PyMARL2, which further guarantees the fairness of the comparisons.
D	Action Representation Learning
As discussed in Sec. 3.3 of the main text, we use action repre-
sentations to reduce the influence of utility difference function’s
estimation errors on graph structure learning. In this section,
we describe the details of action representation learning (the
related network structure is shown in Fig. 15). We use the
technique proposed by Wang et al. (2021b) and learn an action
encoder fe(∙; θe): R1A1 → Rd, parameterized by θe, to map
one-hot actions to a d-dimensional representation space. With
the encoder, each action a has a latent representation za, i.e.,
za = fe(a; θe). The representation za is then used to predict
the next observation o0i and the global reward r, given the cur-
rent observation oi of an agent i, and the one-hot actions of
other agents, a-i . This model is a forward model, which is
trained by minimizing the following loss:
Le(θe, ξe) = E(o,a,r,o0)~D [): Ilpo (Zai , oi, a-i) - oik 2
i
+ λe	(pr (zai, oi, a-i) - r)2 ,
i
(37)
Predictions
Figure 15: Framework for learning
action representations, reproduced
from Wang et al. (2021b).
where po and pr is the predictor for observations and rewards, respectively. We use ξe to denote the
parameters of po and pr . λe is a scaling factor, D is a replay buffer, and the sum is carried out over
all agents.
In the beginning, we collect samples and train the predictive model shown in Fig. 15 for 50K
timesteps. Then policy learning begins and action representations are kept fixed during training.
Since tasks in the MACO benchmark typically do not involve many actions, we do not use action
representations when benchmarking our method. In contrast, StarCraft II micromanagement tasks
usually have a large action space. For example, the map MMM2 involves 16 actions, and a conventional
deep Q-network requires 256 output heads for learning utility difference. Therefore, we equip our
2https://github.com/oxwhirl/pymarl
19
Published as a conference paper at ICLR 2022
method with action representations to estimate the utility difference function when testing it on the
SMAC benchmark.
E Architecture, Hyperparameters, Infrastructure, and Time
Complexity
In CASEC, each agent has a neural network to estimate its local utility. The local utility network
consists of three layers—a fully-connected layer, a 64 bit GRU, and another fully-connected layer—
and outputs an estimated utility for each action. The utility difference function is also a 3-layer
network, with the first two layers shared with the local utility function to process local action-
observation history. The input to the third layer (a fully-connected layer) is the concatenation of the
output of two agents’ GRU layer. The local utilities and pairwise utility differences are summed to
estimate the global action value (Eq. 11 in the paper).
For all experiments, the optimization is conducted using RMSprop with a learning rate of 5 × 10-4,
α of 0.99, RMSProp epsilon of 0.00001, and with no momentum or weight decay. For exploration,
we use -greedy with annealed linearly from 1.0 to 0.05 over 50K time steps and kept constant
for the rest of the training. Batches of 32 episodes are sampled from the replay buffer. The default
iteration number of the Max-Sum algorithm is set to 5. The communication threshold depends on the
number of agents and the task, and we set it to 0.3 on the map 5m_vs_6m and 0.35 on the map MMM2.
We test the performance with different values (1e-3, 1e-4, and 1e-5) of the scaling weight of the
sparseness loss Lsqpvaarrse on Pursuit, and set it to 1e-4 for both the MACO and SMAC benchmark.
The whole framework is trained end-to-end on fully unrolled episodes. All experiments on StarCraft
II use the default reward and observation settings of the SMAC benchmark.
All the experiments are carried out on NVIDIA Tesla P100 GPU. We show the estimated running
time of our method on different tasks in Table 3 and 4. Typically, CASEC can finish 1M training
steps within 8 hours on MACO tasks and in about 10 hours on SMAC tasks. In Table 5, we compare
the computational complexity of action selection for CASEC and DCG, which is the bottleneck of
both algorithms. CASEC is slightly faster than DCG by virtue of graph sparsity.
Table 3:	Approximate running time of CASEC on tasks from the MACO benchmark.
Aloha	Pursuit	Hallway	Sensor	Gather	Disperse
13h(2M)	17h(2M)	7h(1M)	4.5h(0.5M)	6.5h(1M)	8h(1M)
Table 4:	Approximate running time of CASEC on tasks from the SMAC benchmark.
5m_vs_6m MMM2
18h (2M)	21h (2M)
Table 5:	Average time (millisecond) for 1000 action selection phases of CASEC/DCG. CASEC
uses a graph with sparseness 0.2 while DCG uses the full graph. To ensure a fair comparison, both
Max-Sum/Max-Plus algorithms pass messages for 8 iterations. The batch size is set to 10.
	5 actions
5 agents	2.90/3.11
10 agents	3.17/3.45
15 agents	3.41/3.67
10 actions	15 actions
3.15/3.39	3.42/3.67
3.82/4.20	5.05/5.27
5.14/5.4	7.75/8.02
F Influence of cycles on Max-Sum
One limitation of our method is that we cannot guarantee cycle-free coordination graphs. Running
Max-Sum on loopy graphs (graph that contains loops) may lead to sub-optimal actions being selected.
In this section, we investigate the influence of cycles on the optimality of Max-Sum algorithm.
20
Published as a conference paper at ICLR 2022
Specifically, we first compare the results of Max-Sum against optimal joint actions on Aloha from
the MACO benchmark. To this end, we sample 1000 Aloha configurations. We then run Max-Sum
under different sparseness degrees (with no guarantee of cycle-free graphs) and compare with optimal
joint actions. Since finding the optimal action is NP-hard, we use a brute-force method and enumerate
all possible joint actions and choose the one with the largest Qtot value.
Ak>ha: Agents=IO; ACtIOnS=2； Threshold=0.2;
_______Pol nts=IO0。； DlStanCe=3.6994
En'xew ⅛0
750-
500-
250-
En'xew ⅛0
-250-
Aloha： Agents=IO; ACtIOnS=2； ThreshoId=I.O;
PoInts=IOOO; Dlstaπce=188.27
IOOO 1250 1500 1750 2000 2250
Qti)t： optimal
Norma 1=(0,10)： Agents=15; Actions= 2；
Threshold=O.1: PoInts=IOOO: DlStanCe=I.42
- ,
En'xew
Figure 16: Compare actions selected by Max-Sum and the optimal joint action on 1000 different
configurations of Aloha. Left: On sparse graphs with 20% edges. Qtot values of the actions are
shown. Middle: On full graphs. Qtot values of the actions are shown. Right: How many actions
selected by Max-Sum are optimal under different sparseness degrees.
Normal=(0,10): Agents=15; Actions=2;
ThreshoId=I.O: PoInts=IOOO: Distance= 2.91
Randorn
200	400	6∞	800
Qtot： optimal
-IOOO O IOOO 2000	3000 40∞	5000
Qtoti optimal
Figure 17: Compare actions selected by Max-Sum and the optimal joint action on 1000 random
configurations. Left: On sparse graphs with 10% edges. Qtot values of the actions are shown.
Middle: On full graphs. Qtot values of the actions are shown. Right: How many actions selected by
Max-Sum are optimal under different sparseness degrees.
lɪ _________________________
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.6 0.9 1.0
Communication threshold
Results are shown in Fig. 16. We can see that Max-Sum on sparse graphs selects optimal actions
in around 95% of the cases. Qtot values are also satisfactory, with most points falling near the line
y = x. In comparison, the quality of Max-Sum solutions decreases significantly on full graphs, as
shown in Fig. 16 middle and right.
We further investigate the case of random graphs. 1000 graphs are generated randomly with utility
and payoff values conforming to a Gaussian distribution with a mean of 0 and a variance of 10, and
we carry out experiments similar to those on Aloha. As shown in Fig. 17, we find that Max-Sum on
both sparse and full graphs can select more than 95% optimal actions. The optimization objective,
Qtot , is also very close to the optimal value.
We can conclude that, although it can not guarantee optimality consistently, Max-Sum on sparse
graphs can select the optimal action with a large probability on the tested cases. In comparison,
optimal actions are less likely to be selected on full graphs. This may shed light on why CASEC
can outperform DCG on some tasks. For future work, we plan to investigate how to learn cycle-free
sparse coordination graphs so that action optimality can be guaranteed.
G Empirical evaluation of the b ound in EQ. 5
To figure how loose the bound in Eq. 5 is, we randomly generate 10000 graphs, select the edge
between agent 0 and 1, and put them into 100 bins according to the value of ζ0q1var . In each bin,
we calculate the number of graphs where the actions of agent 0 and 1 selected by Max-Sum keep
unchanged after removing the edge between them. Also for each bin, we average the bound in
Equation 5 of each graph instance. Then, in Fig. 18, we compare our bound against the frequency of
unchanged actions. We observe that, on average, the bound is 36.9% lower than the real frequency.
21
Published as a conference paper at ICLR 2022
-O 100
(υ
至R∩
E BU
S
I 60
苫40
20
0
Figure 18: Comparison between the bound in Proposition 1 and the probability in real cases.
——CASEC -- DCG
120
OOOO
4 3 2 1
P9UU3S ⅛ωπ
——CASEC --' DCG
Figure 19: Comparison between CASEC and DCG on Aloha (Left) and Sensor (Right) with two
times number of agents.
H Tasks with a Larger Number of Agents
Intuitively, sparse graphs are expected to perform better in tasks with more agents. In this section, we
compare CASEC with DCG on a large version of Aloha and Sensor.
The new version of Aloha has 20 agents in a 2 × 10 array. We compare CASEC against DCG in
Fig. 19 left. We can see that DCG can no longer send any messages, but CASEC can send about 110
of them. For Sensor, there are 30 sensors and 6 targets. Results are shown in Fig. 19 right. We can
see that DCG does not learn to scan any targets, while CASEC can capture about 40 of them. The
gap between sparse and full coordination graphs is more significant on these tasks.
I Decide Sparseness Adaptively
A limitation of our method is that we fix the communication threshold when training. In this section,
we study how to select the threshold adaptively and investigate the following two methods.
The first method is based on the observation that the performance of sparse graphs would degrade
dramatically when the sparseness degree is below a certain value. To find this value, during testing,
we check the performance of graphs with different sparseness degrees and select the degree below
which the performance would drop. We change the threshold every 50K, 150K, and 200K training
timesteps and show the performance in Fig. 20 (the first row). We can see that training with such
an adaptive threshold performs similarly with the original CASEC algorithm after convergence and
learns slightly better during the initial learning stage. The found threshold is smaller than the one that
we get through a grid search.
The second method is based on Proposition 1. The intuition is that we can cut off the edges which
exert limited influence on Max-Sum. Specifically, during testing, we count the number of edges that
lead to different Max-Sum results with a probability smaller than 0.36 after being removed. The
percentage of these edges is set as the communication threshold. Again, we change the threshold
every 50K, 150K, and 200K training timesteps. The results are shown in Fig. 20 (the second row).
This second method leads to higher final performance but learns slower initially. The adaptive
threshold is less stable compared to the first method, and the selected thresholds are larger than the
hand-crafted one.
22
Published as a conference paper at ICLR 2022
Sparseness Adapt interval IOOK
Sparseness Adapt interval 15OK
----Ongmal Sensor
----Sparseness Adapt Interval 50K
0.20
Sparseness Adapt Interval IOOK
Sparseness Adapt Interval 150K
----Original Sensor
— Sparseness Adapt interval 50K
301----------------------------
----Original Sensor
— Sparseness Adapt interval 50K
301---------------------------
Sparseness Adapt interval IOOK
Sparseness Adapt interval 15OK
Pα,uu3stta,h-
Onginal Sensor
Sparseness Adapt Interval 50K
Sparseness Adapt Interval IOOK
Sparseness Adapt Interval 150K
-0UM3∙l⅛L
二
Figure 20: Learning curves (Left) and the changing process (Right) of the communication threshold
of the two methods proposed in Appendix I.
For future work, it is an important question how to develop more principled methods that can find the
minimum communication threshold which can guarantee learning performance.
Eni,a ttWH
Ooooo
Oool 2345
3 2 Io- - - - -
Figure 21: Performance of DCG on Sensor with different numbers of edges in the coordination
graph.
23