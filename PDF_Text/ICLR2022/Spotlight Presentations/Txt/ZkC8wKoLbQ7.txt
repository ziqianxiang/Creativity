Understanding and Preventing Capacity Loss
in Reinforcement Learning
Clare Lyle
Department of Computer Science
University of Oxford*
Mark Rowland & Will Dabney
DeepMind
Ab stract
The reinforcement learning (RL) problem is rife with sources of non-stationarity,
making it a notoriously difficult problem domain for the application of neural net-
works. We identify a mechanism by which non-stationary prediction targets can
prevent learning progress in deep RL agents: capacity loss, whereby networks
trained on a sequence of target values lose their ability to quickly update their
predictions over time. We demonstrate that capacity loss occurs in a range of RL
agents and environments, and is particularly damaging to performance in sparse-
reward tasks. We then present a simple regularizer, Initial Feature Regularization
(InFeR), that mitigates this phenomenon by regressing a subspace of features to-
wards its value at initialization, leading to significant performance improvements
in sparse-reward environments such as Montezuma’s Revenge. We conclude that
preventing capacity loss is crucial to enable agents to maximally benefit from the
learning signals they obtain throughout training.
1	Introduction
Deep reinforcement learning has achieved remarkable successes in a variety of tasks (Mnih et al.,
2015; MoravCk et al., 2017; Silver et al., 2017; AbreU et al., 2019), but its impressive performance
is mirrored by its brittleness and sensitivity to seemingly innocuous design choices (Henderson
et al., 2018). In sparse-reward environments in particular, even different random seeds of the same
algorithm can attain dramatically different performance outcomes. This presents a stark contrast
to supervised learning, where existing approaches are reasonably robust to small hyperparameter
changes, random seed inputs, and GPU parallelisation libraries. Much of the brittleness of deep RL
algorithms has been attributed to the non-stationary nature of the prediction problems to which deep
neural networks are applied in RL tasks. Indeed, naive applications of supervised learning methods
to the RL problem may require explicit correction for non-stationarity and bootstrapping in order to
yield similar improvements (Bengio et al., 2020; Raileanu et al., 2020).
We hypothesize that the non-stationary prediction problems agents face in RL may be a driving
force in the challenges described above. RL agents must solve a sequence of similar prediction
tasks as they iteratively improve their value function accuracy and their policy (Dabney et al., 2021).
Solving each subproblem (at least to the extent that the agent’s policy is improved) in this sequence
is necessary to progress to the next subproblem. Ideally, features learned to solve one subproblem
would enable forward transfer to future problems. However, prior work on both supervised and
reinforcement learning (Ash & Adams, 2020; Igl et al., 2021; Fedus et al., 2020) suggests that the
opposite is true: networks trained on a sequence of similar tasks are prone to overfitting, exhibiting
negative transfer.
The principal thesis of this paper is that over the course of training, deep RL agents lose some of
their capacity to quickly fit new prediction tasks, and in extreme cases this capacity loss prevents
the agent entirely from making learning progress. We present a rigorous empirical analysis of this
* Correspondence to clare.lyle@cs.ox.ac.uk
1
phenomenon which considers both the ability of networks to learn new target functions via gradient-
based optimization methods, and their ability to linearly disentangle states’ feature representations.
We confirm that agents’ ability to fit new target functions declines over the course of training in
several environments from the Atari suite (Bellemare et al., 2013) and non-stationary reward pre-
diction tasks. We further find that the ability of representations to linearly distinguish different
states, a proxy for their ability to represent certain functions, quickly diminishes in sparse-reward
environments, leading to representation collapse, where the feature outputs for every state in the
environment inhabit a low-dimensional - or possibly even zero - subspace. Crucially, We find evi-
dence that sufficient capacity is a necessary condition in order for agents to make learning progress.
Finally, we propose a simple regularization technique, Initial Feature Regularization (InFeR), to
prevent representation collapse by regressing a set of auxiliary outputs towards their value under the
network’s initial parameters. We show that this regularization scheme mitigates capacity loss in a
number of settings, and also enables significant performance improvements in a number of RL tasks.
One striking take-away from our results is that agents trained on so-called ‘hard exploration’ games
such as Montezuma’s Revenge can attain significant improvements over existing competitive base-
lines without using smart exploration algorithms, given a suitable representation learning objec-
tive. This suggests that the poor performance of deep RL agents in sparse-reward environments is
not solely due to inadequate exploration, but rather also in part due to poor representation learn-
ing. Investigation into the interplay between representation learning and exploration, particularly in
sparse-reward settings, thus presents a particularly promising direction for future work.
2	Background
We consider the reinforcement learning problem wherein an agent interacts with an environment
formalized by a Markov Decision Process M = (X , A, R, P, γ), where X denotes the state space,
A the action space, R the reward function, P the transition probability function, and γ the discount
factor. We will be primarily interested in value-based RL, where the objective is to learn the value
function Qπ : X × A → R associated with some (possibly stochastic) policy π : X → P(A),
defined as Qπ(x, a) = Eπ,P [Pk∞=0 γkR(xk, ak)|x0 = x, a0 = a]. In particular, we are interested
in learning the value function associated with the optimal policy ∏* which maximizes the expected
discounted sum of rewards from any state.
In Q-Learning (Watkins & Dayan, 1992), the agent performs updates to minimize the distance be-
tween a predicted action-value function Q and the bootstrap target defined as
T Q(x, a) = E[R(x0, a0) + γ max Q(x1, a0)|x0 = x, a0 = a] .	(1)
a0∈A
In most practical settings, updates are performed with respect to sampled transitions rather than
on the entire state space. The target can be computed for a sampled transition (xt, at, rt, xt+1) as
TQ(xt, at) = rt + γ maxa Q(xt+1, a).
When a deep neural network is used as a function approximator (the deep RL setting), Q is defined
to be the output of a neural network with parameters θ, and updates are performed by gradient
descent on sampled transitions τ = (xt, at, rt, xt+1). A number of tricks are often used to improve
stability: the sample-based objective is minimized following stochastic gradient descent based on
minibatches sampled from a replay buffer of stored transitions, and a separate set of parameters
G is used to compute the targets Qg(Xt+1, at+ι) which is typically updated more slowly than the
network’s online parameters. This yields the following loss function, given a sampled transition τ :
'td (Qθ ,τ) = (Rt+1 + Y max Qd(Xt+1,a0) - Qθ (Xt, At))2 .	(2)
a0
In this work we will be interested in how common variations on this basic learning objective shape
agents’ learning dynamics, in particular the dynamics of the learned representation, or features.
We will refer to the outputs of the final hidden layer of the network (i.e. the penultimate layer)
2
1.0
Target-fitting error over iterative tasks
Pgenbs UeQW
0	5	10	15	20	25
Target iterations
Figure 1: Networks trained to fit a se-
quence of different targets on MNIST data
see increasing error on new target func-
tions with the number of tasks.
S^BJ-EopueJ uoujsw
amidar
Figure 2: Networks see reduced ability to fit new
targets over the course of training in two demon-
strative Atari environments.
pong
Millions of frames
as its features, denoted φθ(x). Our choice of the penultimate layer is motivated by prior literature
studying representations in RL (Ghosh & Bellemare, 2020; Kumar et al., 2021), although many
works studying representation learning consider the outputs of earlier layers as well. In general,
the features of a neural network are defined to be the outputs of whatever layer is used to compute
additional representation learning objectives.
3	Capacity Loss
Each time a value-based RL agent discovers a new source of reward in its environment or, in the
case of temporal difference methods, updates its value estimate, the prediction problem it needs
to solve changes. Over the course of learning, such an agent must solve a long sequence of target
prediction problems as its value function and policy evolve. Studies of neural networks in supervised
learning suggest that this sequential fitting of new targets may be harmful to a network’s ability to
adapt to new targets (Achille et al., 2018, see Section 5 for further details, e.g.). This presents a
significant challenge to deep RL agents undergoing policy improvement, for which it is necessary
to quickly make significant changes to the network’s predictions even late in the training process. In
this section, we show that training on a sequence of prediction targets can lead to a reduced ability to
fit new targets in deep neural networks, a phenomenon that we term capacity loss. Further, we show
that an agent’s inability to quickly update its value function to distinguish states presents a barrier to
performance improvement in deep RL agents.
3.1	Target-fitting capacity
The parameters of a neural network not only determine the network’s current outputs, but also influ-
ence how these outputs will evolve over time. A network which outputs zero because its final-layer
weights are zero will evolve differently from one whose ReLU units are fully saturated at zero de-
spite both outputting the same function Maas et al. (2013) - in particular, it will have a much easier
time adapting to new targets. It is this capacity to fit new targets that is crucial for RL agents to ob-
tain performance improvements, and which frames our perspective on representation learning. We
are interested in identifying when an agent’s current parameters are flexible enough to allow it to
perform gradient updates that meaningfully change its predictions based on new reward information
in the environment or evolving bootstrap targets, a notion formalized in the following definition.
Definition 1 (Target-fitting capacity). Let PX ∈ P(X) be some distribution over inputs X and PF
a distribution over a family of real-valued functions F with domain X. Let N = (gθ , θ0) represent
the pairing of a neural network architecture with some initial parameters θ0, and O correspond
to an optimization algorithm for supervised learning. We measure the target-fitting capacity of N
under the optimizer O to fit the data-generating distribution D = (PX , PF ) as follows:
C (N,O,D) = Ef 〜PF [Ex 〜PX [(gθo(x) - f (x))2]]	where θ0 = O(θo,Pχ ,f).
(3)
3
Our definition of capacity measures the ability ofa network to reach a new set of targets within a lim-
ited optimization budget from its current parameters and optimizer state. The choice of optimization
budget and target distribution are left as hyperparameters, and different choices result in different
notions of capacity. In reinforcement learning we ultimately care about the network’s ability to fit its
Bellman targets quickly, however the ability on its own will not necessarily be a useful measure: for
example, a network which can only output the zero function will attain low Bellman error immedi-
ately on a sparse-reward environment, but will fail to produce useful updates to improve the policy.
Our evaluations of this measure will use target functions that are independent of the current network
parameters to avoid these pathologies; the effect of this choice is explored further in Appendix B.2.
The process of training a neural network to fit a set of labels must by necessity change some prop-
erties of the network. Works studying the information bottleneck principle (Tishby & Zaslavsky,
2015), for example, identify a compression effect of training on the latent representation, where
inputs with similar labels are mapped to similar feature vectors. This compression can benefit gen-
eralization on the current task, but in the face of the rapidly-changing nature of the targets used in
value iteration algorithms may harm the learning process by impeding the network’s ability to fit
new targets. This motivates two hypotheses. First: that networks trained to iteratively fit a sequence
of dissimilar targets will lose their capacity to fit new target functions (Hypothesis 1), and second:
the non-stationary prediction problems in deep RL also result in capacity loss (Hypothesis 2).
To evaluate Hypothesis 1, we construct a series of toy iterative prediction problems on the MNIST
data set, a widely-used computer vision benchmark which consists of images of handwritten digits
and corresponding labels. We first fit a series of labels computed by a randomly initialized neural
network fθ: we transform input-label pairs (x, y) from the canonical MNIST dataset to (x, fθ(x)),
where fθ (x) is the network output. To generate a new task, we simply reinitialize the network.
Given a target function, we then train the network for a fixed budget from the parameters obtained
at the end of the previous iteration, and repeat this procedure of target initialization and training 30
times. We use a subset of MNIST inputs of size 1000 to reduce computational cost. In Figure 1 we
see that the networks trained on this task exhibit decreasing ability to fit later target functions under
a fixed optimization budget. This effect is strongest in the smaller networks, matching the intuition
that solving tasks which are more challenging for the network will result in greater capacity loss.
We consider two other tasks in Appendix B.2, obtaining similar results, as well as a wider range
of architectures. We find that sufficiently over-parameterized networks (on the order of one million
parameters for a task with one thousand data points) exhibit positive forward transfer, however
models which are not over-parameterized relative to the task difficulty consistently exhibit increasing
error as the number of targets trained on grows. This raises a question concerning our second
hypothesis: are the deep neural networks used by value-based RL agents on popular benchmarks in
the over- or under-parameterized regime?
To evaluate Hypothesis 2, we train an agent’s network checkpoints sampled over the course of
training to fit randomly generated target functions. We provide full details of this procedure in Ap-
pendix C. We generate target functions by randomly initializing neural networks with new parame-
ters, and use the outputs of these networks as targets for regression. We then load initial parameters
from an agent checkpoint at some time t, sample inputs from the replay buffer, and regress on the
random target function evaluated on these inputs. We then evaluate the mean squared error after
training for fifty thousand steps. We consider a DQN (Mnih et al., 2015), a QR-DQN (Dabney et al.,
2018), and a Rainbow agent (Hessel et al., 2018). We observe in all three cases that as training pro-
gresses agents’ checkpoints on average get modestly worse at fitting these random targets in most
environments; due to space limitations we only show two representative environments where this
phenomenon occurs in Figure 2, and defer the full evaluation to Appendix C.3.
3.2	Representation collapse and performance
The notion of capacity in Definition 1 measures the ability of a network to eventually represent a
given target function. This definition reflects the intuition that capacity should not increase over time.
4
montezuma_revenge
00
42
knar erutaeF
600
400
200
0
0	50	100	150	200
Millions of frames
U-Imgφpo-d∙lU-E-Il
montezuma_revenge
pong
00
42
knar erutaeF
50	100	150	200
Millions of frames
nruter edosipe niar
pong
0
Figure 3:	Feature rank and performance over the course of training for Montezuma’s Revenge (left)
and Pong (right). We observe that feature rank is higher for environments and auxiliary tasks which
provide denser reward signals than for sparse reward problems.
However, deep RL agents must quickly update their predictions in order to make efficient learning
progress. We present an alternate measure of capacity that captures this property which we call the
feature rank, as it corresponds to an approximation of the rank of a feature embedding. Intuitively,
the feature rank measures how easily states can be distinguished by updating only the final layer
of the network. This approximately captures a network’s ability to quickly adapt to changes in the
target function, while being significantly cheaper to estimate than Definition 1.
Definition 2 (Feature rank). Let φ : X → Rd be a feature mapping. Let Xn ⊂ X be a set of n
states in X sampled from some fixed distribution P. Fix ε ≥ 0, and let φ(Xn) ∈ Rn×d denote the
matrix whose rows are the feature embeddings of states x ∈ Xn. Let SVD(M) denote the multiset
of singular values of a matrix M. The feature rank of φ given input distribution P is defined to be
ρ(φ,P,e)= lim Eχn~p[∣{σ ∈ SVD( ɪ φ(Xn)) ∣σ > ε}∣]	(4)
n→∞	n
for which a consistent estimator can be constructed as follows, letting X ⊆ X, |X| = n
Pn(Φ, X, e) = ∣{σ ∈ SVD (ɪφ(X)) ∣σ > ε}∣ .	(5)
The numerical feature rank (henceforth abbreviated to feature rank) is equal to the dimension of the
subspace spanned by the features when ε = 0 and the state space X is finite, and its estimator is equal
to the numerical rank (Golub et al., 1976; Meier & Nakatsukasa, 2021) of the sampled feature matrix.
For e > 0, it throws away small components of the feature matrix. We show that ρ is well-defined
and that Pn is a consistent estimator in Appendix A.1. Our analysis of the feature rank resembles that
of Kumar et al. (2021), but differs in two important ways: first, our estimator does not normalize by
the maximal singular value. This allows us to more cleanly capture representation collapse, where
the network features, and thus also their singular values, converge to zero. Second, we are interested
in the capacity of agents with unlimited opportunity to interact with the environment, rather than in
the data-limited regime. We compare our findings on feature rank against the srank used in prior
work in Appendix B.2.
In our empirical evaluations, we train a double DQN (DDQN) agent, a quantile regression (QRDQN)
agent, and a double DQN agent with an auxiliary random cumulant prediction task (RC DQN) (Dab-
ney et al., 2021), on environments from the Atari suite, then evaluate Pn with n = 5000 on agent
checkpoints obtained during training. We consider two illustrative environments: Montezuma’s
Revenge (sparse reward), and Pong (dense reward), deferring two additional environments to Ap-
pendix C.3. We run 3 random seeds on each environment-agent combination.
We visualize agents’ feature rank and performance in Figure 3. Non-trivial prediction tasks, ei-
ther value prediction in the presence of environment rewards or auxiliary tasks, lead to higher
feature rank. In Montezuma’s Revenge, the higher feature rank induced by RC DQN corresponds
to higher performance, but this auxiliary loss can have a detrimental effect on learning progress in
complex, dense-reward games presumably due to interference between the random rewards and the
true learning objective. Unlike in target-fitting capacity, we only see a consistent downward trend
in sparse-reward environments, where a number of agents, most dramatically QR-DQN, exhibit
representation collapse. We discuss potential mechanisms behind this trend in Appendix A.2.
5
Rainbow
O
50 05
. . . .
00 10
erocS dezilamroN-namuH
100	200	300	400	500
InFeR
a asteroids
τ bowling
c centipede
g gravitar
m A montezuma_revenge
■ ■ m S-PaCman
* pitfall
♦ P Private_eye
♦ seaquest
X skiing
, solaris
τ venture
0
b) 2
100	200	300	400	500
Effective Rank
sparse pong
000
1 12
u-ln≡k 一 一
UO-SU"Lu5
a
0
.
Figure 4:	(a): Agent capacity vs human-normalized score in games where Rainbow does not achieve
superhuman performance. While feature rank does not appear to solely determine agent perfor-
mance, there is a positive correlation between feature rank and human-normalized score. Bottom
row contains Rainbow agents trained with the regularizer presented in Equation 6. (b) An ‘unlucky’
seed from our evaluations on the sparsified version of Pong, where learning progress occurs only
after the agent recovers from representation collapse.
Figure 4a reveals a correlation between learning progress and feature rank for the Rainbow agent
(Hessel et al., 2018) trained on challenging games in the Atari 2600 suite where it fails to achieve
human-level performance; this trend is also reflected for other agents described in the next section.
The points on the scatterplot largely fall into two clusters: those with low feature rank, which attain
less than half of the average human score, and those with high feature rank, which tend to attain
higher scores. Having a sufficiently high feature rank thus appears to be a necessary condition for
learning progress, as demonstrated by the learning curves shown in Figure 4b, which highlights an
unlucky agent trained on a variant of Pong (described in Appendix C.2) which experienced represen-
tation collapse, and only solved the task after it had overcome this collapse. However, high feature
rank does not appear to be sufficient for learning progress. Other properties of an agent, such as
its ability to perform accurate credit assignment, the stability of its update rule, the suitability of its
optimizer, its exploration policy, and countless others, must be appropriately tuned to a given task
in order for progress to occur. Simply mapping inputs to a relatively uniform distribution in feature
space will not overcome failures in other components of the RL problem. An agent must be able to
both collect useful learning signals from the environment and effectively update its predictions in
response to those signals in order to make learning progress. This section has shown that at least in
some instances poor performance can be attributed to the latter property.
4	InFeR: Mitigating Capacity Loss with Feature Regularization
The previous section showed that capacity loss occurs in deep RL agents trained with online data,
and in some cases appears to be a bottleneck to performance. We now consider how it might be mit-
igated, and whether explicitly regularizing the network to preserve its initial capacity improves per-
formance in environments where representation collapse occurs. Our approach involves a function-
space perspective on regularization, encouraging networks to preserve their ability to output linear
functions of their features at initialization.
4.1	InFeR: Feature-space regularization
Much like parameter regularization schemes seek to keep parameters close to their initial values, we
wish to keep a network’s ability to fit new targets close to its initial value. We motivate our approach
with the intuition that a network which has preserved the ability to output functions it could easily
fit at initialization should be better able to adapt to new targets. To this end, we will regress a
set of network outputs towards the values they took at initialization. Our method, Initial Feature
Regularization (InFeR), applies an `2 regularization penalty on the output-space level by regressing
6
(a)
(b)
(c)3 1e3
Q(X,一 £td(Q(χ; θt), a, xz,r)
0.0
MLP Network Loss on
Iterative Regression Task
spets k5 retfa
rorre derauqs nae
0	5	10 15 20 25 30
Target iteration
----Regression -------- InFeR
O 50 IOO 150	200
Millions of frames
----DDQN	----Rainbow
----DDQN+InFeR --------RainboW+InFeR
)wobniaR sv(
tnemevorpmI %
Human-Normalized Return on
Capped Normalized Return
Human-Normalized Return
Figure 5: (a) Visualization of InFeR. (b) Analysis of the effect of InFeR on capacity loss. (c)
Effect of InFeR on performance in Montezuma’s Revenge with respect to Rainbow and Double
DQN baselines. (d) Performance of InFeR relative to Rainbow on all 57 Atari games.
a set of auxiliary network output heads to match their values at initialization. Similar perspectives
have been used to prevent catastrophic forgetting in continual learning (Benjamin et al., 2019).
In our approach, illustrated in Figure 5, we begin with a fixed deep Q-learning neural network with
parameters θ, and modify the network architecture by adding k auxiliary linear prediction heads gi
on top of the feature representation φθ. We take a snapshot of the agent’s parameters at initialization
θ0, and use the outputs of the k auxiliary heads under these parameters as auxiliary prediction
targets. We then compute the mean squared error between the outputs of the heads under the current
parameters gi(x; θt) and their outputs at initialization gi(x; θ0). This approach has the interpretation
of amplifying and preserving subspaces of the features that were present at initialization. In practice,
we find that scaling the auxiliary head outputs by a constant β increases this amplification effect.
This results in the following form of our regularization objective, where we let B denote the replay
buffer sampling scheme used by the agent:
k
LInFeR(θ,θθ； B,β) = Ex〜B £(gi(x； θ) - βgiE θθ))2 .	(6)
i=1
We evaluate the effect of incorporating this loss in both DDQN (Van Hasselt et al., 2016) and Rain-
bow (Hessel et al., 2018) agents, and include the relative performance improvement obtained by the
InFeR agents over Rainbow on 57 games from the Atari 2600 suite in Figure 5, deferring the com-
parison to DDQN, where the regularizer improved performance slightly on average but only yielded
significant improvements on sparse-reward games, to the appendix. We observe a net improvement
over the Rainbow baseline by incorporating the InFeR objective, with significant improvements in
games where agents struggle to obtain human performance. The evaluations in Figure 5 are for
k = 10 heads with β = 100 and α = 0.1, and we show the method’s robustness to these hyperpa-
rameters in Appendix C.1. We further observe in Figure 5 that the InFeR loss reduces target-fitting
error on the non-stationary MNIST prediction task described in the previous section. We show in
Appendix C.2 that InFeR tends to increase the feature rank of agents trained on the Atari domain
over the entire course of training; we study the early training period in Appendix C.3.
7
The striking improvement obtained in the sparse-reward Montezuma’s Revenge environment begs
the question of whether such results can be replicated in other RL agents. We follow the same
experimental procedure as before, but now use the DDQN agent; see Figure 5. We find that adding
InFeR to the DDQN objective produces a similar improvement as does adding it to Rainbow, leading
the DDQN agent, which only follows an extremely naive -greedy exploration strategy and obtains
zero reward at all points in training, to exceed the performance of the noisy networks approach taken
by Rainbow in the last 40 million training frames. This leads to two intriguing conclusions: first,
that agents which are explicitly regularized to prevent representation collapse can make progress in
sparse reward problems without the help of good exploration strategies; and second, that this form
of regularization yields significantly larger performance improvements in the presence of additional
algorithm design choices that are designed to speed up learning progress.
4.2	Understanding How InFeR Works
While InFeR improves performance on average across the Atari games, its improvements are con-
centrated principally on games where the baseline rainbow agent performs significantly below the
human baseline. It further slows down progress in a subset of environments such as Asteroids and
Jamesbond. We now investigate two hypothesized mechanisms by which this regularizer may shape
the agent’s representation, in the hopes of explaining this differential effect on performance. Hy-
pothesis 1: InFeR improves performance by preserving a random subspace of the representation
that the final linear layer can use to better predict the value function. The effect of the regularizer on
other aspects of the representation learning dynamics does not influence performance. Hypothesis
2: The InFeR loss slows down the rate at which the learned features at every layer of the network
can drift from their initialization in function space, improving the learning dynamics of the entire
network to prevent feature collapse and over-fitting to past targets. The precise subspace spanned by
the auxiliary weights is not directly useful to value function estimation.
To evaluate Hypothesis 1, we concatenate the outputs of a randomly initialized network to the feature
outputs of the network used to learn the Q-function, and train a linear layer on top of these joint
learned and random features. If Hypothesis 1 were true, then we would expect this architecture to
perform comparably to the InFeR agents, as the final linear layer has access to a randomly initialized
feature subspace. Instead, Figure 6 shows that the performance of the agents with access to the
random features to be comparable to that of the vanilla Rainbow agents, confirming that the effect
of InFeR on earlier layers is crucial to its success.
We now consider Hypothesis 2. InFeR limits the degrees of freedom with which a network can
collapse its representation, which may reduce the flexibility of the network to make the changes
necessary to fit new value functions, slowing down progress in environments where representation
collapse is not a concern. In such cases, increasing the dimension of the layer to which we apply
InFeR should give the network more degrees of freedom to fit its targets, and so reduce the per-
formance gap induced by the regularization. We test this hypothesis by doubling the width of the
penultimate network layer and comparing the performance of InFeR and Rainbow on games where
0.0
0
2.0
1.0
0.5
asteroids
1e5
1.5
100
Millions of frames
200
phoenix montezuma_revenge berzerk
---Rainbow+RF -------- Rainbow ------ Rainbow+InFeR
Figure 6: Left: agent performance does not improve over baseline when random features are added
to the representation. Right: doubling the width of the neural network narrows the performance gap
in games on which InFeR under-performed relative to Rainbow.
----DoUbleRainbow ------- DoUbleRainbow+InFeR ------Rainbow -------Rainbow+InFeR
8
InFeR hurt performance in the original network. We refer to this agent as DoubleRainbow. We
see in Figure 6 that increasing the network’s size reduces, eliminates, or in some cases reverses
the performance gap induced by InFeR in the smaller architecture. We therefore conclude that the
principal mechanism by which InFeR affects performance is by regularizing the entire network’s
learning dynamics.
5	Related Work
Suitably designed auxiliary tasks have been shown to improve performance and encourage learned
representations to satisfy desirable properties in a wide range of settings (Jaderberg et al., 2017;
Veeriah et al., 2019; Gelada et al., 2019; Machado et al., 2018), with further insight given by prior
analysis of the geometry (Bellemare et al., 2019) and stability (Ghosh & Bellemare, 2020) of value
functions in RL. Our analysis of linear algebraic properties of agents’ representations is comple-
mented by prior works which leverage similar ideas to analyze implicit under-parameterization (Ku-
mar et al., 2021) and spectral normalization (Gogianu et al., 2021) in deep RL agents, and by the
framework proposed by Lyle et al. (2021) to study learning dynamics in deep RL agents. In contrast
to prior work, which treats the layers of the network which come before the features as a black box,
we explicitly study the properties and learning dynamics of the whole network.
A separate line of work has studied the effect of interference between sub-tasks in both reinforcement
learning (Schaul et al., 2019; Teh et al., 2017; Igl et al., 2021) and supervised learning settings
(Sharkey & Sharkey, 1995; Ash & Adams, 2020; Beck et al., 2021). Of particular interest has been
catastrophic forgetting, with prior work proposing novel training algorithms using regularization
(Kirkpatrick et al., 2017; Bengio et al., 2014; Lopez-Paz & Ranzato, 2017) or distillation (Schwarz
et al., 2018; Silver & Mercer, 2002; Li & Hoiem, 2017) approaches. Methods which involve re-
initializing a new network have seen particular success at reducing interference between tasks in
deep reinforcement learning (Igl et al., 2021; Teh et al., 2017; Rusu et al., 2016; Fedus et al., 2020).
A closer relative of our approach is that of Benjamin et al. (2019), which also applies a function-
space regularization approach, but which involves saving input-output pairs into a memory bank
with the goal of mitigating catastrophic forgetting. Unlike prior work, InFeR seeks to maximize
performance on future tasks, works without task labels, and incurs a minimal, fixed computational
cost independent of the number of prediction problems seen during training.
6	Conclusions
This paper has demonstrated a fundamental challenge facing deep RL agents: loss of the capacity to
distinguish states and represent new target functions over the course of training. We have shown that
this phenomenon is particularly salient in sparse-reward settings, in some cases leading to complete
collapse of the representation and preventing the agent from making learning progress. Our analysis
revealed a number of nuances to this phenomenon, showing that larger networks trained on rich
learning signals are more robust to capacity loss than smaller networks trained to fit sparse targets.
To address this challenge, we proposed a regularizer to preserve capacity, yielding improved per-
formance across a number of settings in which deep RL agents have historically struggled to match
human performance. Further investigation into this method suggests that it is performing a form
of function-space regularization on the neural network, and that settings where it appears the task
reduces performance are actually instances of under-parameterization relative to the difficulty of the
environment. Particularly notable is the effect of incorporating InFeR in the hard exploration game
of Montezuma’s Revenge: its success here suggests that effective representation learning can allow
agents to learn good policies in sparse-reward environments even under naive exploration strategies.
Our findings open up a number of exciting avenues for future work in reinforcement learning and
beyond to better understand how to preserve plasticity in non-stationary prediction tasks.
9
Acknowledgements
Thanks to Georg Ostrovski, Michael Hutchinson, Joost van Amersfoort, Daniel Guo, Diana Borsa,
Anna Harutyunyan, Razvan Pascanu, Caglar Gulcehre, Srivatsan Srvinivasan, and Remi Munos
for helpful discussions and feedback on early versions of this paper. CL is supported by an Open
Philanthropy AI Fellowship.
10
References
Miguel Abreu, Luis Paulo Reis, and Nuno Lau. Learning to run faster in a humanoid robot soccer
environment through reinforcement learning. In Robot World Cup, pp. 3-15. Springer, 2019.
Alessandro Achille, Matteo Rovere, and Stefano Soatto. Critical learning periods in deep networks.
In International Conference on Learning Representations, 2018.
Jordan Ash and Ryan P Adams. On warm-starting neural network training. In Neural Information
Processing Systems (NeurIPS), 2020.
Nathan Beck, Durga Sivasubramanian, Apurva Dani, Ganesh Ramakrishnan, and Rishabh Iyer. Ef-
fective evaluation of deep active learning on image classification tasks. arXiv, 2021.
Marc Bellemare, Will Dabney, Robert Dadashi, Adrien Ali Taiga, Pablo Samuel Castro, Nicolas
Le Roux, Dale Schuurmans, Tor Lattimore, and Clare Lyle. A geometric perspective on optimal
representations for reinforcement learning. In Neural Information Processing Systems (NeurIPS),
2019.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-
ment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:
253-279, 2013.
Emmanuel Bengio, Joelle Pineau, and Doina Precup. Correcting momentum in temporal difference
learning. In NeurIPS Deep Reinforcement Learning Workshop, 2020.
Yoshua Bengio, Mehdi Mirza, Ian Goodfellow, Aaron Courville, and Xia Da. An empirical inves-
tigation of catastrophic forgeting in gradient-based neural networks. In International Conference
on Learning Representations (ICLR), 2014.
Ari Benjamin, David Rolnick, and Konrad Kording. Measuring and regularizing networks in func-
tion space. In International Conference on Learning Representations (ICLR), 2019.
Will Dabney, Mark Rowland, Marc Bellemare, and Remi Munos. Distributional reinforcement
learning with quantile regression. In Proceedings of the AAAI Conference on Artificial Intelli-
gence, volume 32, 2018.
Will Dabney, Andre Barreto, Mark Rowland, Robert Dadashi, John Quan, Marc G Bellemare, and
David Silver. The value-improvement path: Towards better representations for reinforcement
learning. In AAAI Conference on Artificial Intelligence, 2021.
William Fedus, Dibya Ghosh, John D Martin, Marc G Bellemare, Yoshua Bengio, and Hugo
Larochelle. On catastrophic interference in Atari 2600 games. arXiv, 2020.
Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc G Bellemare. DeepMDP:
Learning continuous latent space models for representation learning. In International Conference
on Machine Learning (ICML), 2019.
Dibya Ghosh and Marc G Bellemare. Representations for stable off-policy reinforcement learning.
In International Conference on Machine Learning (ICML), 2020.
Florin Gogianu, Tudor Berariu, Mihaela Rosca, Claudia Clopath, Lucian Busoniu, and Razvan Pas-
canu. Spectral normalisation for deep reinforcement learning: an optimisation perspective. arXiv
preprint arXiv:2105.05246, 2021.
Gene Golub, Virginia Klema, and Gilbert W Stewart. Rank degeneracy and least squares problems.
Technical report, STANFORD UNIV CA DEPT OF COMPUTER SCIENCE, 1976.
Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger.
Deep reinforcement learning that matters. In Proceedings of the AAAI conference on artificial
intelligence, volume 32, 2018.
11
Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan
Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in
deep reinforcement learning. In AAAI conference on artificial intelligence, 2018.
Maximilian Igl, Gregory Farquhar, Jelena Luketina, Wendelin Boehmer, and Shimon Whiteson.
Transient non-stationarity and generalisation in deep reinforcement learning. In International
Conference on Learning Representations (ICLR), 2021.
Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David
Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. In
International Conference on Learning Representations (ICLR), 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR (Poster),
2015.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hass-
abis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting
in neural networks. ProceedingsoftheNationalAcademyofSciences, 114(13):3521-3526, 2017.
Aviral Kumar, Rishabh Agarwal, Dibya Ghosh, and Sergey Levine. Implicit under-parameterization
inhibits data-efficient deep reinforcement learning. In International Conference on Learning Rep-
resentations (ICLR), 2021.
Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis
and machine intelligence, 40(12):2935-2947, 2017.
David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In
Neural Information Processing Systems (NIPS), 2017.
Clare Lyle, Mark Rowland, Georg Ostrovski, and Will Dabney. On the effect of auxiliary tasks on
representation dynamics. In Artificial Intelligence and Statistics (AISTATS), 2021.
Andrew L Maas, Awni Y Hannun, Andrew Y Ng, et al. Rectifier nonlinearities improve neural
network acoustic models. In Proceedings of the International Conference on Machine Learning.
Citeseer, 2013.
Marlos C. Machado, Clemens Rosenbaum, Xiaoxiao Guo, Miao Liu, Gerald Tesauro, and Murray
Campbell. Eigenoption discovery through the deep successor representation. In International
Conference on Learning Representations (ICLR), 2018.
Maike Meier and Yuji Nakatsukasa. Fast randomized numerical rank estimation. arXiv e-prints, pp.
arXiv-2105, 2021.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518(7540):529-533, 2015.
Matej MoravClk, Martin Schmid, Neil Burch, Viliam Lisy, Dustin Morrill, Nolan Bard, Trevor
Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. DeepStack: Expert-level artificial
intelligence in heads-up no-limit poker. Science, 356(6337):508-513, 2017. ISSN 0036-8075.
John Quan and Georg Ostrovski. DQN Zoo: Reference implementations of DQN-based agents,
2020. URL http://github.com/deepmind/dqn_zoo.
Roberta Raileanu, Maxwell Goldstein, Denis Yarats, Ilya Kostrikov, and Rob Fergus. Automatic
data augmentation for generalization in reinforcement learning. arXiv, 2020.
12
Andrei A. Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, GUillaUme Desjardins, James Kirk-
patrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy distil-
lation. In International Conference on Learning Representations (ICLR), 2016.
Tom Schaul, Diana Borsa, Joseph Modayil, and Razvan Pascanu. Ray interference: a source of
plateaus in deep reinforcement learning. arXiv, 2019.
Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, Yee Whye
Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable framework for contin-
ual learning. In International Conference on Machine Learning (ICML), 2018.
Noel E Sharkey and Amanda JC Sharkey. An analysis of catastrophic interference. Connection
Science, 1995.
Daniel L Silver and Robert E Mercer. The task rehearsal method of life-long learning: Overcoming
impoverished data. In Conference of the Canadian Society for Computational Studies of Intelli-
gence, pp. 90-101,2002.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan
Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering
the game of Go without human knowledge. Nature, 550(7676):354-359, 2017.
Yee Whye Teh, Victor Bapst, Wojciech M Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell,
Nicolas Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning. In Neural
Information Processing Systems (NIPS), 2017.
Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In
2015 ieee information theory workshop (itw), pp. 1-5. IEEE, 2015.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double Q-
learning. In AAAI Conference on Artificial Intelligence, 2016.
Vivek Veeriah, Matteo Hessel, Zhongwen Xu, Richard Lewis, Janarthanan Rajendran, Junhyuk Oh,
Hado van Hasselt, David Silver, and Satinder Singh. Discovery of useful questions as auxiliary
tasks. In Neural Information Processing Systems (NeurIPS), 2019.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279-292, 1992.
13
A Theoretical Results
A. 1 Estimator Consistency
We here show that our estimator of the agent’s feature rank is consistent. First recall
(√nφn) (√nφn) = n ^X φ(Xi)φ(Xi)>.	⑺
The following property of the expected value holds
1n
Ex〜p[φ(x)φ(x)>] = E - y^φ(χi)φ(χi)>
n i=1
(8)
It is then straightforward to apply the strong law of large numbers. To be explicit, we consider an
element of M = E[φφ>], Mij.
n 1	as
E[(φ(x)φ(x)>)ij] = Mij= E[φi(x)φj(x)] =⇒ E -φi(xk)φj(Xk) →→, Mij .	(9)
k=1 n
Since we have convergence for any Mij , we get convergence of the resulting matrix to M. Because
the singular values of Φ are the eigenvalues of M and the eigenvalues are continuous functions
of that matrix, the eigenvalues of Mn converge to those of M almost surely. Then for almost all
values of e, the threshold estimator N (λι,...,λk; E) = ∣{λi > e}| will converge to N (SPec(M); e).
Specifically, the estimator will be convergent for all values of which are not eigenvalues of M
itself.
A.2 Feature Dynamics
We aPPly similar analysis to that of Lyle et al. (2021) to better understand the effect of sParse-reward
environments on rePresentation collaPse. To do so, we consider the setting where Φt are features and
wt a linear function aPProximator which jointly Parameterize a value function Vt = hΦt(X), wti. We
will be interested in studying a continuous-time aPProximation to TD learning, where the discrete-
time exPected uPdates
Φt J Φt + αVφVt[(γPπ - I)Vt + Rπ]	(10)
Wt J Wt + βVwVt(YPF - I)Vt + Rπ]	(11)
are translated into a continuous-time flow, described by the following equations.
∂tΦt = α(γPπ - I)Φt(WtWt>) + RπWt>	(12)
∂tWt = βΦt>[(γPπ - I)ΦtWt +Rπ] ,	(13)
where Pπ ∈ RX ×X is the matrix of state-transition Probabilities under π, and Rπ ∈ RX is the
vector of exPected rewards.
One of the key take-aways of Prior works is that under certain assumPtions, a tabular value function
following continuous-time TD dynamics will converge to its limiting value Vπ along the PrinciPal
comPonents of the environment’s transition matrix. In the function-aPProximation case described
above, the dynamics of the features Φt are somewhat more comPlex. However, it turns out that under
certain training regimes, we can obtain similar convergence results for the features. We therefore
turn our attention to ensemble Prediction, where M linear Prediction ‘heads’, each using a seParate
weight vector Wtm (m = 1, . . . , M) are all trained to regress on the TD targets using the shared
feature rePresentation of the state as inPut, resulting in the following dynamics.
M
∂tΦtM =α X(Rπ+γPπΦtMWtm-ΦtMWtm)(Wtm)> ,	(14)
m=1
∂tWtm =β(ΦtM)>(Rπ +γPπΦtMWtm - ΦtWtm) .	(15)
14
We now restate the result of Lyle et al. (2021) regarding the behaviour of the representation in the
limit of many ensemble heads.
Theorem 1 (Lyle et al., 2021). For M ∈ N, let (ΦtM)t≥0 be the solution to Equation 14, with each
wtm for m = 1, . . . , M initialised independently from N (0, σM2 ), and fixed throughout training
(β = 0). We consider two settings: first, where the learning rate a is scaled as -M and σ221 = 1 for
all M, and second where σ22f =吉 and the learning rate a is equal to 1. These two settings yield
the following dynamics, respectively:
lim ∂tΦtM =P - (I - γPπ)ΦtM , and	(16)
M→∞	t	t
lim ∂tΦM D - (I - YPπ)ΦM + Rne > ,e ~N(0, I).	(17)
M→∞
The corresponding limiting trajectories for a fixed initialisation Φ0 ∈ RX ×d, are therefore given
respectively by
lim ΦtM =P exp(-t(I - γPπ))Φ0 , and	(18)
M→∞ t
lim ΦtM =Dexp(-t(I-γPπ))(Φ0 - (I-γPπ)-1Rπε>)
M→∞ t
+ (I - YPπ)-1Rπε> , e ~ N(0, I).	(19)
One important corollary of this result occurs in sparse-reward environments under sub-optimal poli-
cies, where Rπ = 0. In this case, we see that the representation converges precisely to the zero
vector.
Corollary 1. Let ΦtM, (w)iM=1 be defined as in Theorem 1. Then ifRπ = 0, the feature represen-
tation converges to the zero vector for every state, independent of whether the learning rate α is
scaled as 吉 or the linear weight initialization variance scales as 吉.In particular:
lim lim ΦtM =P 0 .	(20)
t→∞ M→∞
As a result, we have that the feature rank of Φ will also tend to zero
∀e > 0 lim lim ∣{σ ∈ SVD(Φ*)∣σ > e}| D 0 .	(21)
t→∞ M→∞
Proof. The proof of this result follows from a straightforward application of Theorem 1, setting
Rπ = 0 and letting t → ∞. We can obtain an analogous result for the srank of ΦtM when Pπ
is diagonalizable by noting that for any eigenvector vi of Pπ, the value of vi> ΦtMvi evolves as
c exp(-tλi ) for some constant c that depends on Φ0M. In this case, we obtain a limiting value of 1
for the Srank so long as Pπ corresponds to an ergodic Markov chain.	口
The setting of this result is distinct from that of deep neural network representation dynamics, as
neural networks use discrete optimization steps, finite learning rates, and typically do not leverage
linear ensembles. However, we emphasize two crucial observations that suggest the intuition devel-
oped in this setting may be relevant: first, in sparse reward environments the representation will be
pushed to zero along dimensions spanned by the linear weights used to compute outputs. Once suffi-
ciently many independent weight vectors are being used to make predictions, this effectively forces
every dimension of the representation to fit the zero vector output. We would therefore expect rep-
resentation collapse to be particularly pronounced in the QR-DQN agents trained on sparse-reward
environments, as in this setting we obtain many independently initialized heads all identically trying
to fit the zero target.
Second, in the presence of ReLU activations and stochastic optimization, the trajectories followed
by the learned features in deep neural networks run the risk of getting ‘trapped’ in negative values.
If these features would normally tend to small values close to zero (as we would expect in agents
following similar dynamics to those obtained in Theorem 1), this increases the risk of unit saturation,
where the representation may get trapped in bad local minima. This appears to be what happens in
the QR-DQN agents trained on sparse-reward environments such as Montezuma’s Revenge.
15
B Sequential Supervised Learning
B.1	Details: Target-fitting Capacity in Non- stationary MNIST
In addition to our evaluations in the Atari domain, we also consider a variant of the MNIST dataset
in which the labels change over the course of training.
•	Inputs and Labels: We use 1000 randomly sampled input digits from the MNIST dataset and
assign either binary or random targets.
•	Distribution Shift: We divide training into N = 30 or N = 10 iterations depending on the
structure of the target function. In each iteration, a target function is randomly sampled, and the
network’s parameters obtained at the end of the previous iteration are used the initial values for
a new optimization run. We use the Adam (Kingma & Ba, 2015) optimizer with learning rate
1e-3, and train to minimize the mean squared error between the network outputs and the targets
for either 3000 or 5000 steps depending on the nature of the target function.
•	Architecture: we use a standard fully-connected architecture with ReLU activations, and vary
with width and depth of the network. The parameters at the start of the procedure are initialized
following the defaults in the Jax Haiku library.
We note that the dataset sizes, training budgets, and network sizes in the following experiments are
all relatively small. This was chosen to enable short training times and decrease the computational
budget necessary too replicate the experiments. The particular experiment parameters were selected
to be the fastest and cheapest settings in which we could observe the capacity loss phenomenon,
while still being nontrivial tasks. In general, we found that capacity loss is easiest to measure in a
’sweet spot’ where the task for a given architecture is simple enough for a freshly-initialized network
to attain low loss, but complex enough that the network cannot trivially solve the task. In the findings
of the following section, we see how some of the larger architectures don’t exhibit capacity loss on
‘easier’ target functions, but do on more challenging ones that exhibit less structure. This suggests
that replicating these results in larger networks will be achievable, but will require re-tuning the task
difficulty to the larger network’s capacity.
B.2	Additional Evaluations
We expand on the MNIST target-fitting task shown in the main paper by considering how network
size and target function structure influences capacity loss.
•	Random-MNIST (smooth) this task uses the images from the MNIST dataset as inputs.
The goal is to perform regression on the outputs of a randomly initialized, fixed neural
network. We use a small network for this task, consisting of two width-30 fully connected
hidden layers with ReLU activations which feed into a final linear layer which outputs
a scalar. Because the network outputs are small, we scale them by 10 so that it is not
possible to get a low loss by simply predicting the network’s bias term. This task, while
randomly generated, has some structure: neural networks tend to map similar inputs to
similar outputs, and so the inductive bias of the targets will match that of the function
approximator we train on them.
•	Hash-MNIST (non-smooth) uses the same neural network architecture as the previous task
to generate targets, however rather than using the scaled network output as the target, we
multiply the output by 1e3 and feed it into a sine function. The resulting targets no longer
have the structure induced by the neural network. This task amounts to memorizing a set
of labels for the input points.
•	Threshold-MNIST (sparse) replaces the label of an image with a binary indicator variable
indicating whether the label is smaller than some threshold. To construct a sequence of
tasks, we set the threshold at iteration i to be equal to i. This means that at the first iteration,
16
Target-fitting error: non-smooth
200
150
ioo
600
500
400 X
300
200
100
0
Figure 7: Mean squared error at the end of training on each iteration of the hash-MNIST task.
Target-fitting error increases over time in smaller networks, but increasing the depth or width of the
network slows down capacity loss, enabling positive transfer in the largest networks we studied.
the labels are of the form (x, 0) for all inputs x. At the second iteration, they are of the
form (x, δ(y < 1)), where y is the digit in the image x, and so on.
We consider MLP networks of varying widths and depths, noting that the network architecture used
to generate the random targets is fixed and independent of the approximating architecture. We are
interested in evaluating whether factors such as target function difficulty, network parameterization,
and number of target functions previously fit influence the network’s ability to fit future target func-
tions. Our results are shown in Figure 7, 8, and 9. We visualize srank and feature rank of the features
output at the network’s penultimate layer, in addition to the loss obtained at the end of each iteration.
B.3	Effect of InFeR on target-fitting capacity in MNIST
In addition to our study of the Atari suite, we also study the effect of InFeR on the non-stationary
MNIST reward prediction task with a fully-connected architecture; see Figure 10. We find that it
significantly mitigates the decline in target-fitting capacity demonstrated in Figure 1.
C	Atari Evaluations
We now present full evaluations of many of the quantities described in the paper, along with a study
of the sensitivity of InFeR to its hyperparameters. We use the same training procedure for all of the
figures in this section, loading agent parameters from checkpoints to compute the quantities shown.
C.1 Hyperparameter sensitivity of InFeR in deep reinforcement learning
AGENTS
We report results of hyperparameter sweeps over the salient hyperparameters relating to InFeR, so
as to assess the robustness of the method. For both the DDQN and Rainbow agents augmented with
InFeR, we sweep over the number of auxiliary predictions (1, 5, 10, 20), the cumulant scale used in
the predictions (10, 100, 200), and the scale of the auxiliary loss (0.01, 0.05, 0.1, 0.2). We consider
17
Target-fitting error: smooth
256x4 MLP
Loss
1024x2 MLP
Figure 8: Mean squared error after 2e3 training steps on the random-MNIST task. Target-fitting
error increases over time in under-parameterized networks, but increasing the depth or width of the
network slows down capacity loss, enabling positive transfer in the largest network we studied.
Figure 9: Mean squared error after 2e3 training steps on the threshold-MNIST task. Target-fitting
error increases over time in under-parameterized networks, but increasing the depth or width of the
network slows down capacity loss, enabling positive transfer in the largest network we studied.
18
Target-fitting error on hash prediction task over time
Figure 10: Effect of adding InFeR to the regression objective in a random reward prediction problem
on the non-stationary MNIST environment studied previously. We see that the InFeR objective pro-
duces networks that can consistently outperform those trained with a standard regression objective,
exhibiting minimal capacity loss in comparison to the same network architecture trained on the same
sequence of targets.
200
20
W
0.20
B
二 0.10
n
ro
0.05
Figure 11:	Hyperparameter sweeps for the DDQN+InFeR agent. Each contour plot shows average
capped human-normalized score at the end of training marginalized over all hyperparameters not
shown on its axes.
the capped human-normalized return across four games (Montezuma’s Revenge, Hero, James Bond,
and MsPacman), and run each hyperparameter configuration with 3 seeds. Results are shown in
Figure 11 for the DDQN agent; we compare performance as each pair of hyperparameters varies
(averaging across the other hyperparameter, games, and seeds, and the last five evaluation runs of
each agent). Corresponding results for Rainbow are given in Figure 12.
m IOO
aux-scale
10
200
100
cu mu Iant^scale
5	10	20
num_aux
Figure 12:	Hyperparameter sweeps for the Rainbow+InFeR agent. Each contour plot shows average
capped human-normalized score at the end of training marginalized over all hyperparameters not
shown on its axes.
19
knar erutaeF
montezuma_revenge
0
dense pong
20
10
0
sparse pong
20
10
dense seaquest
sparse seaquest
0
O
000
00
42
nruter edosipe niarT
50	100	150	200	0
Millions of frames
50
100
Millions of frames
150	200
50
100
Millions of frames
150	200
0
RC-DQN
DDQN
QR-DQN
50
100
Millions of frames
150	200
0
0
Figure 13:	Feature rank and performance of RL agents on demonstrative Atari environments.
•	Agent: We train a Rainbow agent (Hessel et al., 2018) with the same architecture and hyper-
parameters as are described in the open-source implementation made available by Quan & Os-
trovski (2020). We additionally add InFeR, as described in Section 4, with 10 heads, gradient
weight 0.1 and scale 100.
•	Training: We follow the training procedure found in the Rainbow implementation mentioned
above. We train for 200 million frames, with 500K evaluation frames interspersed every 1M
training frames. We save the agent parameters and replay buffer every 10M frames to estimate
feature dimension and target-fitting capacity.
C.2 Feature rank
We first extend the results shown in Figure 3 to two additional games: Seaquest, and a sparsified
version of Pong in which the agent does not receive negative rewards when the opponent scores. In
these settings, we stored agent checkpoints once every 10M frames in each 200M frame trajectory,
and used 5000 sampled inputs from the agent’s replay buffer to estimate the feature rank, using the
cutoff = 0.01. Results are shown in Figure 13.
We further evaluate the evolution of feature rank in agents trained on all 57 games in the arcade
learning environment. We find that the decline in dimension after the first checkpoint at 10M frames
shown across the different agents in the selected games also occurs more generally in Rainbow
agents across most environments in the Atari benchmark. We also show that in most cases adding
InFeR mitigates this phenomenon. Our observations here do not show a uniform decrease in feature
rank or a uniformly beneficial effect of InFeR. The waters become particularly muddied in settings
where neither the Rainbow nor Rainbow+InFeR agent consistently make learning progress such as
in tennis, solaris, and private eye. It is outside the scope of this work to identify precisely why the
agents do not make learning progress in these settings, but it does not appear to be due to the type
of representation collapse that can be effectively prevented by InFeR.
Procedure. We compute the feature rank by sampling n = 50000 transitions from the replay buffer
and take the set of origin states as the input set. We then compute a n × d matrix whose row i is
given by the output of the penultimate layer of the neural network given input Si . We then take the
singular value decomposition of this matrix and count the number of singular values greater than
0.01 to get an estimate of the dimension of the network’s representation layer.
In most games, we see a decline in feature rank after the first checkpoint at 10M frames. Strikingly,
this decline in dimension holds even in the online RL setting where the agent’s improving policy
presumably leads it to observe a more diverse set of states over time, which under a fixed represen-
tation would tend to increase the numerical rank of the feature matrix. This indicates that even in the
face of increasing state diversity, agents’ representations face strong pressure towards degeneracy.
It is worth noting, however, that the agents in dense-reward games do tend to see their feature rank
increase significantly early in training; this is presumably due to the network initially learning to
disentangle the visually similar states that yield different bootstrap targets.
20
alien
amidar
512-			510	
512-		—	505	—
atlantis	bankheist
500- 450			500 400	
				
				
	bowling			boxing
assault
asterix	asteroids
512		500						
510			
		400	
	beam rider		berzerk
500		510	,
475-		500	
centipede chopper_command
300 200 100	—	500 450		
				
crazyclimber	defender
510		510	
505-		500		
	fishing_derby		freeway
			
500- 450	S	500 475		500 400	
	demon attack —		double dunk		enduro
500-		250-	-		500	rʌɔ
400		0						,=	450	
	frostbite		gopher		gravitar
500	———	500 400	—	510 E∩∩		512- 510		500	
475-						ɔuu 490		508		450	
hero
510
500
490
kung_fu_m aster
ice_hockey	jamesbond	kangaroo	krull
500 400 ——	510- 500 490 ge	_ 			450 400		500 475	y
montezuma_reven		ms_pacman		name_this_game		phoenix
500	512-		500		512	—
_						475-		510	一一
250	511-				508	
pong		private_eye		qbert		riverraid
			510			一,	512	
450-	400		500	J		510	
400	300		490		508	
robotank		seaquest		skiing		Solaris
500 A	510		500	—	500	
475-	505		450		475	
StajgUnner		surround		tennis		time_pilot
512-			500					500	Aj 		∙―		
510	-		450		450	
	tutankham		up_n_down		venture
video_pinball
510
500
490
wizard_of_wor
500
475
yars_revenge
500
400
zaxxon
500- __________________
250 ―
0	100	200
Millions of frames
0	100	200
Millions of frames
400^----------.---------
0	100	200
Millions of frames
500
400
0	100	200
Millions of frames
500
450
0	100	200
Millions of frames
----Rainbow
Rainbow+lnFeR
Figure 14: feature rank of agent representations over the course of training on all 57 games in
the Atari benchmark. We compare Rainbow against Rainbow+InFeR. Rainbow+InFeR does not
uniformly prevent decreases in feature rank across all games, but on average it has a beneficial effect
on preserving representation dimension.
21
C.3 Target-fitting Capacity
In this section we examine the target-fitting capacity of neural networks trained with DQN, QR-
DQN, and Rainbow over the course of 50 million environment frames on five games in the Atari
benchmark (amidar, montezuma’s revenge, pong, bowling, and hero). Every 1 million training
frames we save a checkpoint of the neural network weights and replay buffer. For each checkpoint,
we generate a random target network by initializing network weights with a new random seed. We
then train the checkpoint network to predict the output of this random target network for 10000
mini-batch updates (batch size of 32) under a mean squared error loss, for states sampled from the
first 100, 000 frames in the checkpoint’s replay buffer. Furthermore, we repeat this for 10 seeds used
to initialize the random target network weights.
The results of this experiment are shown in Figure 15 (in orange), where the solid lines show means
and shaded regions indicate standard deviations over all seeds (both agent seeds (5) and target fitting
seeds (10), for a total of 50 trials). We also show srank and feature rank of the features output at
the network’s penultimate layer for each of the checkpointed networks used for target fitting. These
are computed using the network features generated from 1000 states sampled randomly from that
checkpoint’s replay buffer. For feature rank, averages and standard deviations are only over the 5
agent seeds.
,ωtw pw,lenbs Ueww ,ωtw pw,lensUeww
,lotw pw,lenbs Ueww
乂Ueil
bowling (QR-DQN)
,lotw pw,lenbs Ueww
乂Ueil
0.0020
0.0010
0.0000
hero (DQN)
mqnt.zuma_revenge (DQN)
0	20	40
Millions of frames
0
1e-5 pong (DQN)
1 ,
,lotw pw,lenbs Ueww
0	20	40
Millions of frames
0	20	40
Millions offrames
一。」」WPw」EnbS UEa≡
乂Ueil
Oooo
Oooo
4 3 2 1
0.003-
0.002-
0.001-
0.000-
0
0	20	40
Millions of frames
一。」」WPw」EnbS UEa≡
乂Ueil
Ooo
0 5 0 0
2 Il 5
,ωtw pw,lenbs Ueww
0	20	40
Millions offrames
』。」」"p±Enbs UEa≡
乂Ueil
Oooo
Oooo
4 3 2 1
bowling (Rainbow)
0.0125-
0.0100-
0.0075-
0.0050-
0.0025-
-0	0.0000-
0	20	40
Millions of frames
0.003-
0.002-
0.001-
0.000-
,lotw pw,lenbs Ueww
乂Ueil
hero (QR-DQN)
0	20	40
Millions of frames
,ωtw pw,lens
pw,lenbs Ueww
(QR-DQN)
0.000250-
0.000225-
0.000200-
0.000175-
montezuma revenge
0.000125-
0.000150-
150
100
0	20	40
Millions of frames
montezuma_revenge (Rainbow)
0.0006
0.0005
0.0004
0.0003
0.0002
0.0001
0.0000-
,ωtw pw,lensUeww
0.006
0.000
0.006-
0.004-
0.002-
0.000-
0	20	40
Millions of frames
,ωtw pw,lenbs Ueww
0
0	20	40
Millions of frames


--- Loss -- Feature num. rank - Feature srank
Figure 15: Mean squared error, after 10000 training steps for the target-fitting on random network
targets. We also show the corresponding feature rank of the pre-trained neural network (before
target-fitting).
C.4 Performance
We provide full training curves for both Rainbow and Rainbow+InFeR on all games in Figures 16 &
17 (capped human-normalized performance), and 18 & 19 (raw evaluation score). We also provide
evaluation performance curves for DDQN and DDQN+InFeR agents in Figure 20.
22
1
0
ι
0
bowling
amidar
assault
ι.o
0.5
asterix
asteroids
1.0
0.5
/	ɪ 0	
		
beam rider
berzerk
bank heist
0
ι
	1.0	
	0.5	C
chopper_command
0.25
0.00
crazyclimber
ι.o
boxing
0.0
defender
ι.o
0.5
demon attack
centipede
0.75
0.50
0.25
double dunk
-2.5
0.5
freeway
ι.o
0.5
frostbite
ι
0
gopher
gravitar
0
fishing_derby
ι
hero
ι
0
road runner
ι.o
1.0
0.5
0.5
0.0
jamesbond
krull
phoenix
0.5
ice_hockey
1.0
0.5
montezuma_revenge
private_eye
qbert
riverraid
ι
0
ms pacman
ι.o
kangaroo
Γ	1
I	0
name this game
robotank
seaquest
ι.o
0.5
skiing
Solaris
0.5
1
0.5
0.0
-0.5
	0.25-	
	0.00	
tennis
SpaceJnvaders
ι
0
tutankham
star_gunner
ι
1.0
0.5
0
0
1.0
0.5
video_pinball
5
time_pilot
ι
0
wizard of wor
0
100
0
200
0
100
200
0
100
Millions of frames
Millions of frames
yars revenge
1
100
200
0
100
Millions of frames
Millions of frames
200
Millions of frames
200
Rainbow
Rainbow+lnFeR
0
0

F
Figure 16:	Full evaluation of capped human-normalized performance on Atari benchmarks for the
default Rainbow architecture.
23
amidar
assault
ι.o
0.5
0.5
bank heist
battle zone
asterix
ι.o
0.5
beam rider
ι
o
boxing
breakout
centipede
le-l
choppercommand
bowling
le-l
ι.o
ι
ι.o
0
ι.o
0.5
0.5
ice_hockey
jamesbond
ι.o
ι
o
0.5
kung_fu_master
0.5
kangaroo
krull
hero
ι
	
„		1
		0
o
name_this_game
1.0
ι.o
0.5
2.5
o.o
-2.5
5
0
pitfail
le-2
road runner
ι.o
0.5
SpaceJnvaders
ι.o
phoenix
0.5
qbert
pong
ι.0
robotank
seaquest
ι
ms pacman
le-l 一
montezuma revenge
le-l 一
0.5
skiing
tennis
up_n_down
video_pinball
5
o
-5
Γ	1T 0.5-W
time_pilot
ι
0
star gunner
0.5
ι.o
0.5
tutankham
ι.o
0.5
yars_revenge
zaxxon
0
100
Millions of frames
200
1.0
0.5
0
100
200
0
100
Millions offrames
Millions of frames
200
o
wizard of wor
0
100
200
0
100
Millions of frames
Millions offrames
200
Rainbow
Rainbow+lnFeR

Figure 17:	Full evaluation of capped human-normalized performance on Atari benchmarks in the
double-width Rainbow architecture.
24
alien
amidar
assault
le4
5
o
asterix
le5
5
o
asteroids
le4
5
O
battle zone
le4 -
2
O
beam rider
le4 一
2
berzerk
le4
o
choppercommand
bowling
5
0
boxing
le2
1
breakout
le2
0
demon attack
le5 一
ι.o
0.5
centipede
le4
0
crazy climber
5
defender
ι
double dunk
Iel 一
2
gravitar
Ie3
le4
ɪ 产_hockey
0
0
kung_fu_master
o
frostbite
1
2
gopher
le5
kangaroo
le4
ɪ ^)rivate_eye
0
seaquest
le5
0
name_this_game
0
skiing
Ie4
0
krull
2
riverraid
le4
Solaris
Ie3
2
0.0
2.5
o.o
湃 CjinVaderS
2
ɪ ^ip_n_down
tutankham
0
Millions offrames
o
o
0——
surround
Millions of frames
0
0
tennis
lei
ɪ FdeO_pinball
5
100
Millions of frames
200
2∙5 - ⅛>w<34*v<j∕⅛<3mcs-c⅛Λ(>
Millions offrames
2
zaxxon
le4
100
Rainbow
Rainbow+lnFeR
Millions of frames
200

5
Γ

Figure 18:	Full evaluation of raw scores on Atari benchmarks for the default Rainbow architecture.
25
alien
bowling
Iel
5
amidar
assault
5
o
asterix
le5
2
O
beam rider
le4 一
asteroids
O
boxing
le2
O
crazy climber
Ie5 一
2
51≡2
0
demon attack
le5 一
5.0
2.5
centipede
le3
7.5
1
f⅛hingderby
2
O
double dunk
Iel 一
defender
le5
gopher
--------
choppercommand
3
2
freeway
Iel
5
hero
le4
0
ɪ 产Jockey
2.5
o
kung fu master
le4 一 一
5.0
pitfail
Ie2
0
frostbite
-2
montezuma revenge
21≡3	^
0
1
kangaroo
le4
le5
o
name this game
le4 -	-
1.5
ι.o
0.5
qbert
Ie4
2.5
0.0
0
pong
2.5 lel
0.0
0
ɪ ^)rivate_eye
2.5
skiing
Ie4
road runner
robotank
2
ɪ ^ip_n_down
0.0
o
seaquest
le5
2
riverraid
0
Millions offrames
0
0
surround
Millions of frames
tennis
lei
5
ɪ FdeO_pinball
100
Millions of frames
200
Millions offrames
2
zaxxon
le4
100
Rainbow
Rainbow+lnFeR
Millions of frames
200


0
0
Figure 19:	Full evaluation of raw scores on Atari benchmarks for the double-width Rainbow archi-
tecture.
26
alien
amidar
le3
assault
5
5
o
bowling
lei
ι
crazy climber
le5 一
o
beam rider
asteroids
c hog pe rcom mand
ι
o
o
fishingderby
le2 一
2.5
freeway
Iel
2.5
o.o
frostbite
le3
o.o
5.0
2.5
Iel
5
o
jamesbond
le3
5
ɪ ms_pacman
ɪ ^)rivate_eye
5
o
5
0
starg u n ne r
0
0
space invaders
1⅛4 一
2
o
2
tutankham
le2
2.5
0.0
up n down
Ie4
2
o
i 嚷 Sjevenge
100
Millions offrames
200
1
zaxxon
le4
100
Millions of frames
200
double dunk
ι
name this game
le4 -	-
0
qbert
le4
2
skiing
ie4
o
enduro
le3
2
gravitar
ιle3
0
krull
0
Ie4
2
Solaris
le3
2.5-
seaquest
le4
5
0.0
0
surround
tennis
0
100
200
0
100
Millions of frames
ι
time pilot
Ie4 一
0
wizard of wor
le4 一 一
Millions of frames
200
——DDQN
DDQN + lnFeR
1
100
Millions offrames
200
0
0
0
0
0


0
0
Figure 20: Evaluations of the effect of InFeR on performance of a Double DQN agent. Overall
we do not see as pronounced an improvement as in Rainbow, but note that the average human-
normalized score over the entire benchmark is nonetheless slightly higher for the InFeR agent, and
that the performance improvement obtained by InFeR in Montezuma’s Revenge is still significant
in this agent.
27