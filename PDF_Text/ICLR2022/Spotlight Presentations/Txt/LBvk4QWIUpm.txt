Under review as a conference paper at ICLR 2022
Tighter Sparse Approximation Bounds for
ReLU Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
A well-known line of work (Barron, 1993; Breiman, 1993; Klusowski & Barron,
2018) provides bounds on the width n of a ReLU two-layer neural network needed
to approximate a function f over the ball BR(Rd) up to error , when the Fourier
based quantity Cf = RRd ∣∣ξ∣∣2∣∕(ξ)∣ dξ is finite. More recently Ongie et al. (2019)
used the Radon transform as a tool for analysis of infinite-width ReLU two-layer
networks. In particular, they introduce the concept of Radon-based R-norms and
show that a function defined on Rd can be represented as an infinite-width two-
layer neural network if and only if its R-norm is finite. In this work, we extend
the framework of (Ongie et al., 2019) and define similar Radon-based semi-norms
(R, U -norms) such that a function admits an infinite-width neural network repre-
sentation on a bounded open setU ⊆ Rd when its R, U-norm is finite. Building on
this, we derive sparse (finite-width) neural network approximation bounds that re-
fine those of Breiman (1993); Klusowski & Barron (2018). Finally, we show that
infinite-width neural network representations on bounded open sets are not unique
and study their structure, providing a functional view of mode connectivity.
1 Introduction
Extensive work has shown that for a neural network to be able to generalize, the size or magnitude
of the parameters is more important than the size of the network, when the latter is large enough
(Bartlett, 1997; Neyshabur et al., 2015; Zhang et al., 2016). Under certain regimes, the size of the
neural networks used in practice is so large that the training data is fit perfectly and an infinite-width
approximation is appropriate. In this setting, what matters to obtain good generalization is to fit
the data using the right inductive bias, which is specified by how network parameters are controlled
(Wei et al., 2020) together with the training algorithm used (Lyu & Li, 2020).
The infinite-width two-layer neural network model has been studied from several perspectives due
to its simplicity. One can replace the finite-width ReLU network 1 Pn=I αi(hωi, Xi -句)+ by an
integral over the parameter space with respect to a signed Radon measure: (hω, xi - b)+ dα(ω, b).
Thus, controlling the magnitude of the neural network parameters is akin to controlling the mea-
sure α according to a certain norm. Bach (2017) introduced the F1-space, which is the infinite-
width neural network space with norm inf{/ |b| d∣α∣(ω, b)}, derived from the finite-width regular-
izer 1 Pn=ι |ai ∣k (ω^, bi) ∣∣2 (the infimum is over all the measures α which represent the function at
hand). A different line of work (Savarese et al., 2019; Ongie et al., 2019) consider the infinite-width
spaces with norm inf{∣∣α∣∣TV = J d∣α∣(ω, b)}, which is derived from the finite-width regularizer
1 Pn=ι ∣a∕kωi∣∣2 (i.e. omitting the bias term). Both of these works seek to find expressions for this
norm, leading to characterizations of the functions that are representable by infinite-width networks.
Savarese et al. (2019) solves the problem in the one-dimensional case: they show that for a function
f on R, this norm takes value max{ JR |f 00(χ)∣ dx, ∣f0(-∞) + f (∞)∣}. Ongie etal. (2019) give an
expression for this norm (the R-norm) for functions on Rd, making use of Radon transforms (see
Subsec. 2.3).
Although we mentioned in the first paragraph that in many occasions the network size is large
enough that the specific number of neurons is irrelevant, when the target function is hard to approx-
1
Under review as a conference paper at ICLR 2022
imate it is interesting to have an idea of how many neurons one needs to approximate it. The first
contribution in this direction was by Cybenko (1989); Hornik et al. (1989), which show that two-
layer neural networks with enough neurons can approximate any reasonable function on bounded
sets in the uniform convergence topology. Later on, Barron (1993); Breiman (1993) provided sparse
approximation bounds stating that if a function f is such that a certain quantity Cf constructed from
the Fourier transform f is finite, then there exists a neural network of width n such that the L2 ap-
proximation error with respect to a distribution of bounded support is lower than O(Cf /n). More
recently, Klusowski & Barron (2018) provided alternative sparse approximation bounds of Breiman
(1993) by restricting to networks with bounded weights and a slightly better dependency on n at the
expense of a constant factor increasing with d (see Subsec. 2.2).
Contributions. In our work, we seek to characterize the functions that coincide with an infinite-
width two-layer neural network on a fixed bounded open set. This endeavor is interesting in itself
because in practice, we want to learn target functions for which we know samples on a bounded
set, and we are typically unconcerned with the values that the learned functions take at infinity.
Moreover, the tools that we develop allow us to derive state-of-the-art sparse approximation bounds.
Our main contributions are the following:
•	In the spirit of the R-norm introduced by Ongie et al. (2019), for any bounded open setU ⊆ Rd
we define the R, U -norm of a function on Rd, and show that when the R, U -norm of f is finite,
f(x) can admits a representation of the form Sd-1 ×R(hω, xi - b)+ dα(ω, b) + hv, xi + c for
x ∈ U , where v ∈ Rd, c ∈ R and α is an even signed Radon measure.
•	Using the R, U -norm, we derive function approximation bounds for neural networks with a fixed
finite width. We compute the R, U -norm ofa function in terms of its Fourier representation, and
show that it admits an upper bound by the quantity Cf . This shows that our approximation
bound is tighter than the previous bound by Breiman (1993), and meaningful in more instances
(e.g. for finite-width neural networks). We also show R, U-norm-based bounds analogous to the
ones of Klusowski & Barron (2018).
•	Setting U as the open unit ball of radius R, we show that neural network representations of f on
U hold for multiple even Radon measures, which contrasts with the uniqueness result provided
by Ongie et al. (2019) for the case of Rd . We study the structure of the sets of Radon measures
which give rise to the same function on U . The non-uniqueness of the measure representing a
measure could be linked to the phenomenon of mode connectivity.
Additional related work. There have been other recent works which have used the Radon transform
to study neural networks in settings different from ours (Parhi & Nowak, 2021a; Bartolucci et al.,
2021). These two works consider the R-norm as a regularizer for an inverse problem, and proceed
to prove representer theorems: there exists a solution of the regularized problem which is a two-
layer neural network equal to the number of datapoints. Regarding infinite-width network spaces, E
& Wojtowytsch (2020) present several equivalent definitions and provides a review. A well-known
line of work (Mei et al., 2018; Chizat & Bach, 2018; Rotskoff & Vanden-Eijnden, 2018) studies the
convergence of gradient descent for infinite-width two-layer neural networks.
2	Framework
2.1	Notation
Sd-1 denotes the (d - 1)-dimensional hypersphere (as a submanifold of Rd) and BR(Rd) is the
Euclidean open ball of radius R. For U ⊆ Rd measurable, the space C0(U) of functions vanishing
at infinity contains the continuous functions f such that for any > 0, there exists compact K ⊆
U depending on f such that |f (x)| < for x ∈ U \ K. P(U) is the set of Borel probability
measures, M(U) is the space of finite signed Radon measures (which may be seen as the dual of
C0(U)). Throughout the paper, the term Radon measure refers to a finite signed Radon measure
for shortness. If γ ∈ M(U), then kγkTV is the total variation (TV) norm of γ. MC(U) denotes
the space of complex-valued finite signed Radon measures, defined as the dual space of C0 (U, C)
(the space of complex-valued functions vanishing at infinity). We denote by S(Rd) the space of
2
Under review as a conference paper at ICLR 2022
Schwartz functions, which contains the functions in C∞(Rd) whose derivatives of any order decay
faster than polynomials of all orders, i.e. for all k,p ∈ (No)d, supχ∈Rd |xk∂(P)夕(x)| < +∞. For
f ∈ L1 (Rd), we use f to denote the unitary Fourier transforms with angular frequency, defined
as f(ξ) = (2∏)d∕2 RRd f(x)e-ihξ,xidx. If f ∈ LI(Rd) as well, We have the inversion formula
f (x) = (2∏)d∕2 RRd /(ξ)eihξ,xi dx. The Fourier transform is a continuous automorphism on S(Rd).
2.2	Existing sparse approximation bounds
One of the classical results of the theory of two-layer neural networks (Breiman (1993), building
on (Barron, 1993)) states that given a probability measure p ∈ P(BR(Rd)) and a function f :
BR(Rd) → R admitting a Fourier representation of the form f (x)= =n(^ JRd eihξ,xi df(ξ), where
f ∈ Mc (Rd) is a complex-valued Radon measure such that Cf =(「冗/分 RRd ∣∣ξk2 d∖f∖(ξ) < +∞,
1n
there exists a two-layer neural network f (x) = n Ei=I a(hx, ω^ - bi)+ such that
2	(2R)4 Cf2 *
(f (x) - f(x))2 dx ≤ --------
∖
BR(Rd)
(1)
n
These classical results do not provide bounds on the magnitude of the neural network weights.
More recently, Klusowski & Barron (2018) showed similar approximation bounds for two-layer
ReLU networks under additional l1 and l0 bounds on the weights αi,ωi. Namely, if Cf =
1
(2∏)d∕2 JRdkξ∣∣2 d|f ∣(ξ) < +∞ there exists a two-layer neural network f (x) = a0 + hωo,x) +
K Pn=I ai(hwi,xi — bi)+ with ∣αi∣ ≤ 1, |囱|| ≤ 1, bi ∈ [0,1], and K ≤ 2Cf, and
sup	|f (x) — f(x)∖ ≤ CCf √d + log n n-1/2-1/d,	(2)
x∈[-1,1]d
where c is a universal constant.
2.3	REPRESENTATION RESULTS ON Rd BASED ON THE RADON TRANSFORM
One defines Pd denotes the space of hyperplanes on Rd, whose elements may be represented by
points in Sd-1 × R by identifying {x∖hω, xi = b} with both (ω, b) and (-ω, -b). Thus, functions
on Pd are even functions on Sd-1 × R and we will use both notions interchangeably1.
The Radon transform and the dual Radon transform. If f : Rd → R is a function which is
integrable over all the hyperplanes of Rd, we may define the Radon transform Rf : Pd → R as
Rf(ω, b) =	f(x) dx,
∀(ω, b) ∈ Sd-1 × R.
That is, one integrates the function f over the hyperplane (ω, b). If Φ : Pd → R is a continuous
function, the dual Radon transform R*Φ : Rd → R is defined as
R*Φ(x)
Φ(ω, hω, xi) dω,
Sd-1
∀x ∈ Rd ,
where the integral is with respect to the Hausdorff measure over Sd-1. R and R are adjoint
operators in the appropriate domains (see Lemma 13).
The Radon inversion formula. When f ∈ C∞(Rd), one has that (Theorem 3.1, Helgason (2011))
f = Cd(-∆)(dτ"2R*Rf
(3)
where Cd = 2(2∏)d-1 and (-∆)s/2 denotes the (negative) fractional Laplacian, defined via its
,一.....................................
Fourier transform as (-∆)s∕2f(ξ) = ∣∣ξ∣∣sf(ξ).
1Similarly, the space M(Pd) of Radon measures over Pd contains the even measures in M(Sd-1 × R).
If α ∈ M(Pd), JSd-i×r 夕(ω,b) dα(ω,b) is well defined for any measurable function g on SdT X R, but
JPd 夕(ω, b) dα(ω, b) is only defined for even 中.
3
Under review as a conference paper at ICLR 2022
kfkR =
The R-norm. Given a function f : Rd → R, Ongie et al. (2019) introduce the quantity
sup{-cdhf, (-∆)(d+1"2R*ψi∣ ψ ∈ S(SdT X R), ψ even, kψk∞ ≤ 1} if f LiPschitz
+∞	otherwise.
(4)
They call it the R-norm of f, although it is formally a semi-norm. Here, the sPace S (Sd-1 × R) of
Schwartz functions on Sd-1 × R is defined, in analogy with S(Rd), as the sPace of C∞ functions
ψ on Sd-1 × R which for any integers k, l ≥ 0 and any differential oPerator D on Sd-1 satisfy
suP(ω,b)∈sd-ι×R 1(1 + |b|k)∂k(Dψ)(ω,b)∣ < +∞ (HelgasOn (2011), p. 5). Moreover, S(Pd)=
{ψ ∈ S (Sd-1 × R) | ψ even}, which means the conditions on ψ in (4) can be written as ψ ∈
S(Pd), kψk∞ ≤ 1.
The finiteness of the R-norm indicates whether a function on Rd admits an exact representation as
an infinitely wide neural network. Namely, Ongie et al. (2019) in their Lemma 10 show that kfkR
is finite if and only if there exists a (unique) even measure α ∈ M(Sd-1 × R) and (unique) v ∈ Rd,
c ∈ R such that for any x ∈ Rd,
f(x) =	hω, xi - b	dα(ω, b) + hv, xi + c,
Sd-1×R	+
(5)
in which case, kfkR = kαkTV.
Remark the following differences between this result and the bounds by (Breiman, 1993; Klusowski
& Barron, 2018) shown in equations (1) and (2):
(i)	in (5) we have an exact representation with infinite-width neural networks instead of an
approximation result with finite-width,
(ii)	in (5) the representation holds on Rd instead of a bounded domain.
In our work, we derive representation results similar to the ones of Ongie et al. (2019) for functions
defined on bounded open sets, which naturally give rise to sparse approximation results that refine
those of (Breiman, 1993; Klusowski & Barron, 2018).
One property that makes the Radon transform and its dual useful to analyze neural networks can
be understood at a very high level via the following argument: if f(x) = Sd-1 ×R(hω, xi -
b)+ρ(ω, b) d(ω, b) + hv, xi + c for some smooth rapidly decreasing function ρ, then ∆f (x) =
Rsd-ι×R δhω,Xi=bp(ω, b) d(ω, b) = Rsd-i ρ(ω, <ω, Xi) dω = (R*ρ)(x). For a general function f of
the form (5), one has similarly that(△/, φ) = (α, R0 for any 夕 ∈ S(Rd). This property relates the
evaluations of the measure α to the function ∆f via the Radon transform, and is the main ingredient
in the proof of Lemma 10 of Ongie et al. (2019). While we also rely on it, we need many additional
tools to deal with the case of bounded open sets.
3	Representation results on bounded open sets
Schwartz functions on open sets. Let U ⊆ Rd be an open subset. The space of Schwartz func-
tions on U may be defined as S (U) = Tz∈Rd∖u Tk∈(N0)d{f ∈ S (Rd) | ∂(k)f (z) = 0}, i.e. they
are those Schwartz functions on Rd such that the derivatives of all orders vanish outside of U (c.f.
Def. 3.2, Shaviv (2020)). The structure ofS(U) is similar to S(Rd) in that its topology is given by a
family of semi-norms indexed by ((N0)d)2: Ilfllk,k，= suPχ∈u ∣xk ∙ f (k0)(x)∣. Similarly, if V ⊆ Pd
isopen, Wedefine S(V) = T(ω,b)∈(Sd-ι×R)∖V Tk∈(No)2 {f ∈ S(Pd) | dk1 △ k2 f(ω,b) = 0}, where
△ is the spherical Laplacian.
The R, U-norm. Let U ⊆ Rd be a bounded open set, and let U := {(ω, <ω, Xi) ∈ SdT × R | X ∈
U}. For any function f : Rd → R, We define the R, U -norm of f as
kf ∣R,U = sup{-Cdhf, (-∆)(d+1"2R*Ψi | ψ ∈ S (U), ψ even, kΨk∞ ≤ 1}.	(6)
4
Under review as a conference paper at ICLR 2022
Note the similarity between this quantity and the R-norm defined in (4); the main differences are
that the supremum here is taken over the even SchWartz functions on U instead of SdT X R, and
that the non-Lipschitz case does not need a separate treatment. Remark that kf kR,U ≤ kfkR. If f
has enough regularity, We may Write kf kR,U = JU ∣R(-∆)(d+1"2f ∣(ω, b) d(ω, b), using that the
fractional Laplacian is self-adjoint and R is the adjoint of R.
Define PdU to be the bounded open set of hyperplanes of Rd that intersect U, Which in anal-
ogy with Subsec. 2.3, is equal to U UP to the identification of (ω, b) with (一ω, -b). Similarly,
note that S (PU) = {ψ ∈ S (U), ψ even}, which allows to rewrite the conditions in (6) as
ψ∈S(PdU),kψk∞ ≤ 1.
The following proposition, which is based on the Riesz-Markov-Kakutani representation theorem,
shows that when the R, U -norm is finite, it can be associated to a unique Radon measure over PdU.
Proposition 1. If kf kR,U < +∞, there exists a unique Radon measure α ∈ M(PdU) such that
-cdhf, (-∆)(d+1"2R"Ψi = JPd ψ(ω,b) dα(ω,b) for any ψ ∈ S(PU). Moreover, kf kr,u =
kαkTV.
Building on this, we see that a neural network representation for bounded U holds when the R, U-
norm is finite:
Theorem 1.	LetU be a open, bounded subset ofRd. Let f : Rd → R such that kf kR,U < +∞. Let
α ∈ M(PU) be given by Proposition 1. For any 夕 ∈ S (U), there exist unique V ∈ Rd and C e R
such that
/ f(x)q(x) dx = / (/ (hω,x>- t)+ dα(ω,t) +〈v,x〉+ C)夕(x) dx,
(7)
That is, f (x) = JU(<ω,x>- t)+ dα(ω,t) +(v, Xi + C for X a.e. (almost everywhere) in U. If f is
continuous, then the equality holds for all x ∈ U.
Remark that this theorem does not claim that the representation given by α, v, C is unique, unlike
Lemma 10 by Ongie et al. (2019) concerning analogous representations on Rd . In Sec. 5 we see that
such representations are in fact not unique, for particular choices of the setU. We want to underline
that the proof of Theorem 1 uses completely different tools from the ones of Lemma 10 by Ongie
et al. (2019): their result relies critically on the fact that the only harmonic Lipschitz functions on
Rd are affine functions, which is not true for functions on bounded subsets in our setting.
4 SPARSE APPROXIMATION FOR FUNCTIONS WITH BOUNDED R, U -NORM
In this section, we show how to obtain approximation bounds of a function f on a bounded open
set U using a fixed-width neural network with bounded coefficients, in terms of the R, U -norm
introduced in the previous section.
Theorem 2.	Let U ⊆ BR(Rd) be a bounded open set. Suppose that f : Rd → R is such that
IlfIlR,u is finite, where ∣∣ ∙ ∣∣r,u is defined in ( ). Let V ∈ Rd,c ∈ R as in Theorem '. Then, there
exists {(ωi, bi)}n=ι ⊆ U and {ai}n=ι ⊆ {±1} such that thefunction f : Rd → R defined as
f(x) = kf kR,u Xai((ωi,xi - bi)+ + (v,x)+ C
n
i=1
fulfills, for X a.e. in U,
If(X)- f(x)∣ ≤ Rk√kR,u .	(8)
n
The equality holds for all X ∈ U if f is continuous.
The proof of Theorem 2 (in App. B) uses the neural network representation (7) and a probabilis-
tic argument. If one samples {(ωi, bi)}in=1 from a probability distribution proportional to ∣α∣, a
5
Under review as a conference paper at ICLR 2022
Rademacher complexity bound upper-bounds the expectation of the supremum norm between f and
f , which yields the result.
Note the resemblance of (8) with the bound (1); the R,U norm of f replaces the quantity Cf. We can
also use the R, U -norm to obtain a bound analogous to (2), that is, with a slightly better dependency
in the exponent of n at the expense of a constant factor growing with the dimension.
Proposition 2. Let f : Rd → R and U ⊆ B1 (Rd) open such that kf kR,U < +∞. Then, then there
exist {ai}n=ι ⊆ [-1,1], {ωi}‰ι ⊆{ω ∈ Rd∣∣∣ω∣∣ι = 1} and {bi}f=ι ⊆ [0,1] and K < √d∣∣f∣∣R,u
such that the function
n
f(x) = — Eai(hωi,x> - bi)+
n i=1
fulfills, for x a.e. in U and some universal constant c > 0,
|f(x)—/(x)| ≤ cκPd + lognn-1/2-1/d.
The proof of this result (in App. B) follows readily from the representation (7) and Theorem 1 of
Klusowski & Barron (2018).
4.1 Links with the Fourier sparse approximation bounds
The following result shows that setting U = BR(Rd), the R, U -norm can be bounded by the Fourier-
based quantities Cf, Cf introduced in Subsec. 2.2.
Theorem 3.	Assume that the function f : Rd → R admits a Fourier representation of the form
f(x) = (2∏)d∕2 JRd eihξ,xidf(ξ) with f ∈ MC(Rd) a complex-valued Radon measure. Let Cf be
the quantity used in the sparse approximation bound by Breiman (1993) (see Subsec. 2.2). Then,
one has that
kf kR,BR(Rd) ≤ 2RCf
(9)
As a direct consequence of Theorem 3, When U = BR(Rd) the right-hand side of (8) can be
upper-bounded by R2Cf /√n. This allows to refine the bound (1) from Breiman (1993) to a
bound in the supremum norm over BR(Rd), and where the approximating neural network f(χ)=
W Pn=1 ai(hx, ωii - bi)+ + hv, Xi + C fulfills |ai| ≤ IlfllR,BR(Rd), ∣∣ωi∣∣2 ≤ 1 and bi ∈ ( —R, R).
While we are able to prove the bound (9), the Fourier representation of f does not allow for a
manageable expression for the measure α described in Proposition 1. For that, the following theorem
starts from a slightly modified Fourier representation off, under which one can describe the measure
α and provide a formula for the R, U -norm.
Theorem 4.	Let f : Rd → R admitting the representation
f(x) =	eibhω,xi
Sd-1×R
dμ(ω, b),
(10)
for some complex^valuedRadon measures μ ∈ MC(SdT XR) such that dμ(ω, b) = dμ(-ω, —b)=
dμ(-ω, b) = dμ(ω, —b), and JSd-i×r b2d∣μ∣(ω, b) < +∞. Choosing U = BR(Rd), the unique
measure α ∈ M(PdR) specified by Proposition 1 takes the following form:
dα(ω, b)
t2e-itb
dμ(ω, t) db,
—
R
where K = 2πd∕2∕Γ(d). Note that α is real-valued because JR t2e-itb dμ(ω,t) ∈ R as
t2 dμ(ω,t) = (—t)2 dμ(ω, —t). Consequently, the R, BR(Rd)-norm of f is
R
IfIR,BR(Rd)
IαITV
R
-R	Sd-1	R
-R Sd
t2e-itb dμ(ω,t)
db.
(11)
6
Under review as a conference paper at ICLR 2022
T~1	1,1	,	∙	, 1	1 C	F C ,1	1	,1	∙	A-	/ ɪ A- / I I A- I I ɪ A∙∖ -t-r Tl	. 1
Remark that μ is the pushforward of the measure f by the mappings ξ → (土ξ∕kξk, ±ξ). When the
Fourier transform f admits a density, one may obtain the density of μ Via a change from Euclidean
to spherical coordinates: dμ(ω,b) = 2Vol(SdT)f(bω)∣b∣d-1 d(ω,b). Hence, Theorem 4 provides
an operatiVe way to compute the R, U -norm of f if one has access to the Fourier transform of f .
Note that equation (11) implies that the R, BR(Rd)-norm of f increases with R, and in particular is
smaller than the R-norm of f, which corresponds to setting R = ∞.
Theorems 3	and 4 are proVen jointly in App. B. Note that from the expression (11) one can easily
see that kfkR,BR(Rd) is upper-bounded by RCf :
R
R
-R Sd-1 R
-R Sd
t2e-itb
dμ(ω,t) db ≤ ZR Z串 J眼 t2 d∣μ∣(ω,t) db = 2R ZKJξ∣∣2d∣f∣(ξ)(12)
1	.1	1 ∙ ,	111	∙	,1	Fb1τr^<	J ∕< A、 1	, J
where the equality holds since μ is the pushforward of f. Equation (12) makes apparent the norm
kfkR,BR(Rd) is sometimes much smaller than the quantities Cf, Cf, as is showcased by the follow-
ing one-dimensional example (see the proof in App. B). In these situations, the sparse approximation
bounds that we proVide in Theorem 2 and Proposition 2 are much better than the ones in (1)-(2).
Example 1. Take the function f : R → R defined as f(x) = cos(x) - cos((1 + )x), with
e > 0. f admits the Fourier representation f (x) = 7冗)/ JR ʌ/ɪ(δι(ξ) + δ-ι(ξ) 一 δι+e(ξ) 一
δ-ι-e(ξ))eiξx dξ. We have that Cf = 2 + 2e + e2, and IlfkR,万回”)≤ R (Re + 2e + e2).
kf kR,BR(Rd) goes to zero as e → 0, while Cf converges to 2.
An interesting class of functions for which kf ||汽,后回邯壮)is finite but Cf ,Cf are infinite are func-
tions that can be written as a finite-width neural network on BR(Rd), as shown in the following
proposition.
Proposition 3. Let f : Rd → R defined as f(x) = n1 Pn=ι αi(<ωi, Xi — bi)+ for all X ∈ Rd, with
{ωi}in=1 ⊆ Sd-1, {ai}in=1, {bi}in=1 ⊆ R. Then, for any bounded open set U, we have kf kR,U ≤
1n
1	∑i=ι lai∣, while Cf, Cf = +∞ if f is not an affinefunCtion.
Proposition 3 makes use of the fact that the R, U -norm is always upper-bounded by the R-norm,
which also means that all the bounds deVeloped in Ongie et al. (2019) apply for the R, U -norm.
The fact that finite-width neural networks haVe infinite Cf was stated by E & Wojtowytsch (2020),
that used them to show the gap between the functions with finite Cf and functions representable by
infinite-width neural networks (belonging to the Barron space, in their terminology). It remains to
be seen whether the gap is closed when considering functions with finite R, U -norm, i.e., whether
any function admitting an infinite-width representation (7) on U has a finite R, U -norm.
Moving to the non-linear Radon transform. In many applications the function of interest f may
be better represented as J((ω,夕(x)i — t)+ dα(ω, t) +(v, Xi + c, where 夕 is a fixed finite dimen-
sional, non-linear and bounded feature map. Our results triVially extend to this case where in the
Radon transform hyperplanes are replaced by hyperplanes in the feature space. This can be seen
as the “kernel trick” applied to the Radon transform. The corresponding ∣∣f ∣R,φ(u)corresponds
to the sparsity of the decomposition in the feature space, and we haVe better approximation when
kf kR,φ(U) < Ilf IR,U . This giVes a simple condition for when transfer learning is successful, and
explains the success of using random fourier features as a preprocessing in implicit neural represen-
tations of images and surfaces (Tancik et al., 2020). In order to go beyond the fixed feature maps and
tackle deeper ReLU networks, we think that the non-linear Radon transform (Ehrenpreis, 2003) is an
interesting tool to explore. We note that Parhi & Nowak (2021b) introduced recently a representer
theorem for deep ReLU networks using Radon transforms as a regularizer.
5	Infinite-width representations are not unique on b ounded sets
Ongie et al. (2019) show that when the R-norm off is finite, there is a unique measure α ∈ M(Rd)
such that the representation (5) holds for X ∈ Rd . In this section we show that when we only ask
7
Under review as a conference paper at ICLR 2022
the representation to hold for x in a bounded open set, there exist several measures that do the job;
in fact, they span an infinite-dimensional space.
Let U = BR(Rd) be the open ball of radius R > 0 in Rd, which means that U = SdT X (—R,R)
and PU is the set of hyperplanes {x∣hω,x) = b} such that ∣∣ωk = 1 and b ∈ (—R,R), which
we denote by PdR for simplicity. In the following we will construct a space of Radon measures
α ∈ M(PdR) whose neural network representation (5) coincide for all x ∈ BR(Rd). Note that since
any bounded subset of Rd is included in some open ball, our results imply that such representations
are non-unique on any bounded set.
Remark 1. When one considers representations on BR(Rd) of the sort (5) with the measure α lying
in the larger space M(Sd-1 × R), the non-uniqueness is apparent because there are two ‘trivial’
kinds of symmetry at play:
(i)	Related to parity: when the measure α is odd, we have Sd-1 ×R(hω, xi — b)+ dα(ω, b) =
2 Jsd-ι×R(hω,xi — b)+ 一 (-hω,xi + b)+ dα(ω,b) = h 1 JSd-1×R ωdα(ω,b),xi 一
1 JSd-I ×r bdα(ω, b), which is an affinefUnction of X.
(ii)	Related to boundedness: if (ω, b) ∈ Sd-1 × (R\ (—R, R)), x 7→ (hω, xi — b)+ restricted to
BR(Rd) isan affine function ofx. Hence, ifα is supported on Sd-1 × Sd-1 × (R\(—R, R)),
x 7→ Sd-1 ×R (hω, xi — b)+ dα(ω, b) is an affine function when restricted to BR (Rd).
Since in Sec. 3 we restrict our scope to measures α lying in M(PdU), these two kinds of symmetries
are already quotiented out in our analysis. The third kind of non-uniqueness that we discuss in this
section is conceptually deeper, taking place within M(PdU).
Let {Yk,j | k ∈ Z+, 1 ≤ j ≤ Nk,d} be the orthonormal basis of spherical harmonics of the space
L2(Sd-1) (Atkinson & Han, 2012). It is well known that for any k, the functions {Yk,j | 1 ≤ j ≤
Nk,d} are the restrictions to Sd-1 of homogeneous polynomials of degree k, and in fact Nk,d is the
dimension of the space of homogeneous harmonic polynomials of degree k. Consider the following
subset of even functions in C∞(Sd-1 × (—R, R)):
A = {Yk,j ③ X k | k, j, k0 ∈ Z+, k ≡ k0 (mod 2), k0 < k — 2, 1 ≤ j ≤ Nd,k },
where Xk0 denotes the monomial of degree k0 on (—R, R). We have the following result regarding
the non-uniqueness of neural network representations:
Theorem 5. If α ∈ M(PdR) is such that α ∈ clw(span(A)), then we have that 0 =
Sd-1 ×(-R,R) (hω, xi — b)+ dα(ω, b) for any x ∈ BR(Rd). That is, α yields a neural network repre-
sentation of the zero-function on BR(Rd). Here, we consider span(A) as a subset ofM(PdR) by the
Riesz-Markov-Kakutani representation theorem via the action hg,夕)= JPd 夕(ω,b)g(ω, b) d(ω, b)
R
for any g ∈ span (A),夕 ∈ Co(PR), and clw denotes the closure in the topology ofweak convergence
of M(Sd-1 × R).
In particular, any measure whose density is in the span of A will yield a function which is equal to
zero when restricted to BR(Rd). As an example of this result, we show a simple measure in M(P1d)
which represents the zero function on B1(R2).
Example 2 (Non-zero measure representing the zero function on B1(R2)). We define the even Radon
measure α ∈ M(S1 × ( — 1,1)) with density dα(ω, b) = (8ω4 — 8ω0 + 1) d(ω, b) where ω = (ω°, ωι).
Then, for any x ∈ B1(R2), 0 = S1 ×(-1,1) (hω, xi — b)+ dα(ω, x).
On the one hand, Proposition 1 states that there exists a unique measure α ∈ M(PdU) such that
一cdhf, (—∆)(d+1"2R"Ψi = RPd ψ(ω, b) dα(ω, b) for any ψ ∈ S(PU) if IIfkR,u is finite.On the
other hand, Theorem 5 claims that functions admit distinct representations by measures in M(PdU).
The following theorem clarifies these two seemingly contradictory statements. Consider the follow-
ing subset of even functions in C∞ (Sd-1 × (—R, R)), which contains A: B
B = {Yk,j 0 Xk0 | k,j,k0 ∈ Z+, k ≡ k0 (mod 2), k0 <k, 1 ≤ j ≤ Nd,k}.
8
Under review as a conference paper at ICLR 2022
Proposition 4. Let 0 < R < R0. Let f : Rd → R such that kf kR,B 0 (Rd) < +∞ and let
α ∈ M(PdR) be the unique measure specified by Proposition 1. Then, α is the unique measure in
M(PdR) such that
∀Ψ ∈ S(BR(Rd)),
hα, Roi =[
BR(Rd))
f (x)∆o(x) dx,
∀k, j, k0 ∈ Z+ s.t. k0 ≡ k (mod 2), k0 < k, 1 ≤ j ≤ Nk,d,
hα, Ykj 函 X k0i = -Cdhf (-A)(d+1)∕2R* (Yk,j Q 1|X|<RXk )i.
(13)
(14)
The condition (13) holds for any measure α0 ∈ M(PdR) for which f admits a representation of the
form (7) on BR(Rd). Thus, α can be characterized as the unique measure in M(PdR) such that f
admits a representation of the form (7) on BR(Rd) and the condition (14) holds.
In ( ), the quantity hf, (-A)(d+1)∕2R*(Yk,j Q 1|X|<RXk0)i is well defined despite 1|X|<RXk0
not being continuous on R; We define it as hf, (-A)(d+1)∕2R*((Yk,j Q 1∣χ ∣<r X k0) + g)), where g
is any function in S (PRo) such that (Ykj Q 1∣χ ∣<r X k0)+ g ∈ S (PRo) (which do exist, see、pp.C).
In short, Proposition 4 characterizes the measure α from Proposition 1 in terms of its evaluations
on the spaces R(S(BR(Rd))) and span(B), and by Corollary 1 the direct sum of these two spaces
dense in C0 (PdR), which by the Riesz-Markov-Kakutani representation theorem is the predual of
M(PdR). Interestingly, the condition (13) holds for any measure α ∈ M(PdR) which represents the
function f on BR(Rd), but it is easy to see that the condition (14) does not: by Theorem 5 we have
that if ψ ∈ span(A) ⊆ span(B), the measure α0 defined as dα0 (ω, b) = dα(ω, b) + ψ(ω, b) db
represents the function f on BR(Rd), and hα0, ψi = hα, ψi + kψk22.
It remains an open question to see whether Theorem 5 captures all the measures which represent the
zero function on BR(Rd), which we hypothesize. If that was the case, we would obtain a complete
characterization of the Radon measures which represent a given function on BR(Rd).
Mode connectivity. Mode connectivity is the phenomenon that optima of neural network losses
(at least the ones found by gradient descent) turn out to be connected by paths where the loss value
is almost constant, and was observed empirically by Garipov et al. (2018); Draxler et al. (2018).
Kuditipudi et al. (2019) provided an algorithmic explanation based on dropout, and an explanation
based on the noise stability property. Theorem 5 suggests an explanation for mode connectivity
from a functional perspective: one can construct finitely-supported measures which approximate
a measure α ∈ clw(span(A)), yielding finite-width neural networks with non-zero weights which
approximate the zero function on BR (Rd). Assuming that the data distribution is supported in
BR(Rd), adding a multiple of one such network to an optimal network will produce little change
in the loss value because the function being represented is essentially unchanged. More work is
required to confirm or discard this intuition.
6 Conclusion
We provided in this paper tighter sparse approximation bounds for two-layer ReLU neural networks.
Our results build on the introduction of Radon-based R, U -norms for functions defined on a bounded
open set U . Our bounds refine Fourier-based approximation bounds of Breiman (1993); Klusowski
& Barron (2018). We also showed that the representation of infinite width neural networks on
bounded open sets are not unique, which can be seen as a functional view of mode connectivity
observed in training deep neural networks. We leave two open questions: whether any function
admitting an infinite-width representation on U has a finite R, U -norm, and whether Theorem 5
captures all the measures which represent the zero function on BR(Rd). Finally, in order to extend
our theory to deeper ReLU networks we believe that non-linear Radon transforms (Ehrenpreis, 2003)
are interesting tools to explore.
9
Under review as a conference paper at ICLR 2022
References
Kendall Atkinson and Weimin Han. Spherical Harmonics and Approximations on the Unit Sphere:
An Introduction, volume 2044. Springer, 01 2012.
Francis Bach. Breaking the curse of dimensionality with convex neural networks. Journal of Ma-
chine Learning Research, 18(19):1-53, 2017.
Andrew Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE
Transactions on Information Theory, 39:930 - 945, 1993.
Peter Bartlett. For valid generalization the size of the weights is more important than the size of the
network. In Advances in Neural Information Processing Systems, volume 9. MIT Press, 1997.
Francesca Bartolucci, Ernesto De Vito, Lorenzo Rosasco, and Stefano Vigogna. Understanding
neural networks with reproducing kernel banach spaces, 2021.
L. Breiman. Hinging hyperplanes for regression, classification, and function approximation. IEEE
Transactions on Information Theory, 39(3):999-1013, 1993.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-
parameterized models using optimal transport. In Advances in neural information processing
systems, pp. 3036-3046, 2018.
G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control,
Signals, and Systems (MCSS), 2(4):303-314, 1989.
Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht. Essentially no barriers
in neural network energy landscape. In Proceedings of the 35th International Conference on
Machine Learning, volume 80, pp. 1309-1318, 2018.
Weinan E and Stephan Wojtowytsch. Representation formulas and pointwise properties for barron
functions, 2020.
L. Ehrenpreis. The Universality of the Radon Transform. Oxford, 2003.
Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, and Andrew G Wilson. Loss
surfaces, mode connectivity, and fast ensembling of dnns. In Advances in Neural Information
Processing Systems, volume 31. Curran Associates, Inc., 2018.
S. Helgason. Geometric Analysis on Symmetric Spaces. Mathematical surveys and monographs.
American Mathematical Society, 1994.
Sigurdur Helgason. Integral Geometry and Radon Transforms. Springer, 2011.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni-
versal approximators. Neural Networks, 2(5):359-366, 1989.
Sham M Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction:
Risk bounds, margin bounds, and regularization. In Advances in Neural Information Processing
Systems, volume 21, pp. 793-800. Curran Associates, Inc., 2009.
Jason M. Klusowski and Andrew R. Barron. Approximation by combinations of relu and squared
relu ridge functions with `1 and `0 controls. IEEE Transactions on Information Theory, 64(12):
7649-7656, 2018.
Rohith Kuditipudi, Xiang Wang, Holden Lee, Yi Zhang, Zhiyuan Li, Wei Hu, Rong Ge, and Sanjeev
Arora. Explaining landscape connectivity of low-cost solutions for multilayer nets. In Advances
in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.
Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks.
In 8th International Conference on Learning Representations, ICLR 2020. OpenReview.net, 2020.
10
Under review as a conference paper at ICLR 2022
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-
layer neural networks. Proceedings ofthe National Academy ofSciences, 115(33):E7665-E7671,
2018.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning.
The MIT Press, 2012.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. In ICLR (Workshop), 2015.
Greg Ongie, Rebecca Willett, Daniel Soudry, and Nathan Srebro. A function space view of bounded
norm infinite width relu nets: The multivariate case. In International Conference on Learning
Representations (ICLR 2020), 2019.
Rahul Parhi and Robert D. Nowak. Banach space representer theorems for neural networks and
ridge splines. Journal ofMachine Learning Research, 22(43):140, 2021a.
Rahul Parhi and Robert D. Nowak. What kinds of functions do deep neural networks learn? insights
from variational spline theory, 2021b.
Grant M Rotskoff and Eric Vanden-Eijnden. Neural networks as interacting particle systems:
Asymptotic convexity of the loss landscape and universal scaling of the approximation error.
arXiv preprint arXiv:1805.00915, 2018.
W. Rudin. Functional Analysis. International series in pure and applied mathematics. McGraw-Hill,
1991.
Jorge Salazar. Fubini-tonelli type theorem for non product measures in a product space, 2018.
Pedro Savarese, Itay Evron, Daniel Soudry, and Nathan Srebro. How do infinite width bounded
norm networks look in function space? In Conference on Learning Theory, 2019.
Ary Shaviv. Tempered distributions and schwartz functions on definable manifolds. Journal of
Functional Analysis, 278(11), 2020.
Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan,
Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng. Fourier features let net-
works learn high frequency functions in low dimensional domains. NeurIPS, 2020.
Colin Wei, Jason D. Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and
optimization of neural nets v.s. their induced kernel, 2020.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
11