Published as a conference paper at ICLR 2022
Long Expressive Memory for Sequence
Modeling
T. Konstantin Rusch
ETH Zurich
trusch@ethz.ch
Siddhartha Mishra
ETH Zurich
smishra@ethz.ch
N. Benjamin Erichson
University of Pittsburgh
erichson@pitt.edu
Michael W. Mahoney
ICSI and UC Berkeley
mmahoney@stat.berkeley.edu
Ab stract
We propose a novel method called Long Expressive Memory (LEM) for learn-
ing long-term sequential dependencies. LEM is gradient-based, it can efficiently
process sequential tasks with very long-term dependencies, and it is sufficiently
expressive to be able to learn complicated input-output maps. To derive LEM,
we consider a system of multiscale ordinary differential equations, as well as a
suitable time-discretization of this system. For LEM, we derive rigorous bounds
to show the mitigation of the exploding and vanishing gradients problem, a well-
known challenge for gradient-based recurrent sequential learning methods. We
also prove that LEM can approximate a large class of dynamical systems to high
accuracy. Our empirical results, ranging from image and time-series classification
through dynamical systems prediction to keyword spotting and language model-
ing, demonstrate that LEM outperforms state-of-the-art recurrent neural networks,
gated recurrent units, and long short-term memory models.
1	Introduction
Learning tasks with sequential data as inputs (and possibly outputs) arise in a wide variety of con-
texts, including computer vision, text and speech recognition, natural language processing, and time
series analysis in the sciences and engineering. While recurrent gradient-based models have been
successfully used in processing sequential data sets, it is well-known that training these models
to process (very) long sequential inputs is extremely challenging on account of the so-called ex-
ploding and vanishing gradients problem (Pascanu et al., 2013). This arises as calculating hidden
state gradients entails the computation of an iterative product of gradients over a large number of
steps. Consequently, this (long) product can easily grow or decay exponentially in the number of
recurrent interactions.
Mitigation of the exploding and vanishing gradients problem has received considerable attention
in the literature. A classical approach, used in Long Short-Term Memory (LSTM) (Hochreiter &
Schmidhuber, 1997) and Gated Recurrent Units (GRUs) (Cho et al., 2014), relies on gating mecha-
nisms and leverages the resulting additive structure to ensure that gradients do not vanish. However,
gradients might still explode, and learning very long-term dependencies remains a challenge for
these architectures (Li et al., 2018). An alternative approach imposes constraints on the structure
of the hidden weight matrices of the underlying recurrent neural networks (RNNs), for instance by
requiring these matrices to be unitary or orthogonal (Henaff et al., 2016; Arjovsky et al., 2016; Wis-
dom et al., 2016; Kerg et al., 2019). However, constraining the structure of these matrices might
lead to significantly reduced expressivity, i.e., the ability of the model to learn complicated input-
output maps. Yet another approach relies on enforcing the hidden weights to lie within pre-specified
bounds, leading to control on gradient norms. Examples include Li et al. (2018), based on indepen-
dent neurons in each layer, and Rusch & Mishra (2021a), based on a network of coupled oscillators.
Imposing such restrictions on weights might be difficult to enforce, and weight clipping could reduce
expressivity significantly.
1
Published as a conference paper at ICLR 2022
This brief survey highlights the challenge of designing recurrent gradient-based methods for se-
quence modeling which can mitigate the exploding and vanishing gradients problem, while at the
same time being sufficiently expressive and possessing the ability to learn complicated input-output
maps efficiently. We seek to address this challenge by proposing a novel gradient-based method.
The starting point for our method is the observation that realistic sequential data sets often contain
information arranged according to multiple (time, length, etc., depending on the data and task)
scales. Indeed, if there were only one or two scales over which information correlated, then a simple
model with a parameter chosen to correspond to that scale (or, e.g., scale difference) should be
able to model the data well. Thus, it is reasonable to expect that a multiscale model should be
considered to process efficiently such multiscale data. To this end, we propose a novel gradient-
based architecture, Long Expressive Memory (LEM), that is based on a suitable time-discretization
of a set of multiscale ordinary differential equations (ODEs). For this novel gradient-based method
(proposed in Section 2):
•	we derive bounds on the hidden state gradients to prove that LEM mitigates the exploding
and vanishing gradients problem (Section 4);
•	we rigorously prove that LEM can approximate a very large class of (multiscale) dynamical
systems to arbitrary accuracy (Section 4); and
•	we provide an extensive empirical evaluation of LEM on a wide variey of data sets, includ-
ing image and sequence classification, dynamical systems prediction, keyword spotting,
and language modeling, thereby demonstrating that LEM outperforms or is comparable to
state-of-the-art RNNs, GRUs and LSTMs in each task (Section 5).
We also discuss a small portion of the large body of related work (Section 3), and we provide a brief
discussion of our results in a broader context (Section 6). Much of the technical portion of our work
is deferred to Supplementary Materials.
2	Long Expressive Memory
We start with the simplest example of a system of two-scale ODEs,
ddt = Ty (σ (Wyz + VyU + by) - y), 与 = Tz (σ (Wzy + VzU + bz) - Z).	(1)
Here, t ∈ [0, T] is the continuous time, 0 < τy ≤ τz ≤ 1 are the two time scales, y(t) ∈ Rdy , z(t) ∈
Rdz are the vectors of slow and fast variables and U = U(t) ∈ Rm is the input signal. For simplicity,
we set dy = dz = d. The dynamic interactions between the neurons are modulated by weight
matrices Wy,z, Vy,z, bias vectors by,z and a nonlinear tanh activation function σ(u) = tanh(u).
Note that refers to the componentwise product of vectors.
However, two scales (one fast and one slow), may not suffice in representing a large number of
scales that could be present in realistic sequential data sets. Hence, we need to generalize (1) to a
multiscale version. One such generalization is provided by the following set of ODEs,
，=σ(W2y + V2u + b2) Θ (σ (WyZ + VyU + by) - y),
dz
d = σ(Wιy + Vιu + bi) Θ (σ (Wzy + VzU + bz) - z).
(2)
In addition to previously defined quantities, we need additional weight matrices W1,2, V1,2, bias
vectors by,z and sigmoid activation function σ(u) = 0.5(1 + tanh(u∕2)). As σ is monotone,
we can set W1,2 = V1,2 ≡ 0 and (b1)j = by, (b2)j = bz, for all 1 ≤ j ≤ d, with
σ(by,z) = Ty,z to observe that the two-scale system (1) is a special case of (2). One can read-
ily generalize this construction to obtain many different scales in (2). Thus, we can interpret
(τz(y,t),τy(y,t)) = (σ(Wιy + Viu + bi) ,σ(W2y + VU + b2)) in (2) as input and state
dependent gating functions, which endow ODE (2) with multiple time scales. These scales can be
learned adaptively (with respect to states) and dynamically (in time). Moreover, it turns out that the
multiscale ODE system (2) is of the same general form (see SM§C) as the well-known Hodgkin-
Huxley equations modeling the dynamics of the action potential for voltage-gated ion-channels in
biological neurons (Hodgkin & Huxley, 1952).
2
Published as a conference paper at ICLR 2022
Next, we propose a time-discretization of the multiscale ODE system (2), providing a circuit to our
sequential model architecture. As is common with numerical discretizations of ODEs, doing so
properly is important to preserve desirable properties. To this end, we fix ∆t > 0, and we discretize
(2) with the following implicit-explicit (IMEX) time-stepping scheme to arrive at LEM, written in
compact form as,
∆tn = ∆tσ(Wιyn-i + VlUn + bl),
∆tn = ∆tσ(W2yn-i + VUn + b2),
zn = (1 - ∆tn) zn-1 +∆tn σ(Wzyn-1 +Vzun + bz),
Zn = (1 — ∆tn) Θ yn-1 + ∆tn Θ σ(W/n + VyUn + by)∙
(3)
For steps 1 ≤ n ≤ N, the hidden states in LEM (3) are yn, zn ∈ Rd, with input state Un ∈ Rm. The
weight matrices are W1,2,z,y ∈ Rd×d and V1,2,z,y ∈ Rd×m and the bias vectors are b1,2,z,y ∈ Rd.
We also augment LEM (3) with a linear output state ωn ∈ Ro with ωn = Wyyn, and Wy ∈ Ro×d.
3	Related Work
We start by comparing our proposed model, LEM (3), to the widely used LSTM of Hochreiter &
Schmidhuber (1997). Observe that ∆tn, ∆tn in (3) are similar in form to the input, forget and
output gates in an LSTM (see SM§D), and that LEM (3) has exactly the same number of parameters
(weights and biases) as an LSTM, for the same number of hidden units. Moreover, as detailed in
SM§D, We show that by choosing very specific values of the LSTM gates and the ∆tn, ∆tn terms
in LEM (3), the two models are equivalent. However, this analysis also reveals key differences
between LEM (3) and LSTMs, as they are equivalent only under very stringent assumptions. In
general, as the different gates in both LSTM and LEM (3) are learned from data, one can expect
them to behave differently. Moreover in contrast to LSTM, LEM stems from a discretized ODE
system (2), which endows it with (gradient) stable dynamics.
The use of multiscale neural network architectures in machine learning has a long history. An early
example was provided in Hinton & Plaut (1987), who proposed a neural network with each connec-
tion having a fast changing weight for temporary memory and a slow changing weight for long-term
learning. More recently, one can view convolutional neural networks as multiscale architectures for
processing multiple spatial scales in data (Bai et al., 2020).
The use of ODE-based learning architectures has also received considerable attention in recent years
with examples such as continuous-time neural ODEs (Chen et al., 2018; Queiruga et al., 2020;
2021) and their recurrent extensions ODE-RNNs (Rubanova et al., 2019), as well as RNNs based
on discretizations of ODEs (Chang et al., 2018; Erichson et al., 2021; Chen et al., 2020; Lim et al.,
2021; Rusch & Mishra, 2021a;b). In addition to the specific details of our archiecture, we differ
from other discretized ODE-based RNNs in the explicit use of multiple (learned) scales in LEM.
4	Rigorous Analysis of LEM
Bounds on hidden states. The structure of LEM (3) allows us to prove (in SM§E.1) that its hidden
states satisfy the following pointwise bound.
Proposition 4.1. Denote tn = n∆t and assume that ∆t ≤ 1. Further assume that the initial hidden
states are z0 = y0 ≡ 0. Then, the hidden states zn , yn of LEM (3) are bounded pointwise as,
max
1≤i≤d
max{∣zn∣, |yn∣} ≤ min。, ∆√tn) ,	∀1 ≤ n, with ∆ = 1+ 二
2 - ∆t
(4)
On the exploding and vanishing gradient problem. For any 1 ≤ n ≤ N , let Xn ∈ R2d, denoted
the combined hidden state, given by Xn = z1n , yn1 , ....... , zin , yin , . ,	zdn , ynd . For simplicity
of the exposition, we consider a loss function: En = 2 Ilyn — Nn ∣∣2, with Nn being the underlying
ground truth. The training of our proposed model (3) entails computing gradients of the above loss
function with respect to its underlying weights and biases θ ∈ Θ = [W1,2,y,z, V1,2,y,z, b1,2,y,z],
3
Published as a conference paper at ICLR 2022
at every step of the gradient descent procedure. Following Pascanu et al. (2013), one uses chain rule
to show,
∂En = X ∂Enk)	∂Enk) = ∂En ∂Xn ∂+Xk
~∂θ^ 一乙 ∂θ , ∂θ = ∂Xn∂Xk ∂θ
1≤k≤n	n
(5)
∂E(k)
In general, for recurrent models, the partial gradient , which measures the contribution to the
∂E(k)
hidden state gradient at step n arising from step k of the model, can behave as 〜 Yn-k, for
some γ > 0 Pascanu et al. (2013). Ifγ > 1, then the partial gradient grows exponentially in sequence
length, for long-term dependencies k << n, leading to the exploding gradient problem. On the other
hand, if γ < 1, then partial gradients decays exponentially for k << n, leading to the vanishing
gradient problem. Thus, mitigation of the exploding and vanishing gradient problem entails deriving
bounds on the gradients. We start with the following upper bound (proved in SM§E.2),
Proposition 4.2. Let zn , yn be the hidden states generated by LEM (3). We assume that ∆t << 1
is chosen to be sufficiently small. Then, the gradient of the loss function En with respect to any
parameter θ ∈ Θ is bounded as
∂En
~∂θ~
ʌ .	ʌ . n	ʌ	..	..
≤ (1 + Y)tn + (1 + Y)Γtn, Y= kynk∞,
(6)
η = max{kW1k∞, kW2k∞,kWzk∞, kWyk∞},	Γ=2(1+η)(1+3η)
If we choose the hyperparameter ∆t = O(n-1) (see SM (17) for the order-notation), then one
readily observes from (6) that the gradient ∂θEn is uniformly bounded for any sequence length n and
the exploding gradient problem is clearly mitigated for LEM (3). Even if one chooses ∆t = O(n-s),
for some 0 ≤ s ≤ 1, we show in SM Remark E.1 that the gradient can only grow polynomially (e.g.
as O(n) for s = 1/2), still mitigating the exploding gradient problem.
Following Pascanu et al. (2013), one needs a more precise characterization of the partial gradient
∂θE(nk), for long-term dependencies, i.e., k << n, to show mitigation of the vanishing gradient
problem. In SM§E.3, we state and prove proposition E.2, which provides a precise formula for the
asymptotics of the partial gradient. Here, we illustrate this formula in a special case as a corollary,
Proposition 4.3. Letyn, zn be the hidden states generated by LEM (3) and the ground truth satisfy
Nn 〜 O(1). Then,for any k << n (long-term dependencies) we have,
∂E(nk)
∂θ
3
Here, constants in O(∆t2) depend on only (
(7)
on η (6) and η = ∣∣W2∣∣1 and are independent of n, k.
3
This formula (7) shows that although the partial gradient can be small, i.e., O(∆t2), it is in fact
independent of k, ensuring that long-term dependencies contribute to gradients at much later steps
and mitigating the vanishing gradient problem.
Universal approximation of general dynamical systems. The above bounds on hidden state gra-
dients show that the proposed model LEM (3) mitigates the exploding and vanishing gradients
problem. However, this by itself, does not guarantee that it can learn complicated and realistic
input-output maps between sequences. To investigate the expressivity of the proposed LEM, we will
show in the following proposition that it can approximate any dynamical system, mapping an input
sequence un to an output sequence on, of the (very) general form,
Φn = f (Φn-1, Un) , On = θ(Φn), ∀ 1 ≤ n ≤ N,	(8)
with φn ∈ Rdh , on ∈ Rdo denoting the hidden and output states, respectively. The input signal is
Un ∈ Rdu and maps f : Rdh × Rdu 7→ Rdh and o : Rdh 7→ Rdo are Lipschitz continuous. For
simplicity, we set the initial state φ0 = 0.
Proposition 4.4. For all 1 ≤ n ≤ N, let φn , on be given by the dynamical system (8) with input
signal Un. Under the assumption that there exists a R > 0 such that max{kφn k, kUnk} < R, for
all 1 ≤ n ≤ N, then for any given > 0 there exists a LEM of the form (3), with hidden states
yn, zn ∈ Rdy and output state ωn = Wyyn ∈ Rdo,for some dy such that the following holds,
kon - ωn k ≤ , ∀1 ≤ n ≤ N.	(9)
4
Published as a conference paper at ICLR 2022
From this proposition, proved in SM§E.4, we conclude that, in principle, the proposed LEM (3) can
approximate a very large class of dynamical systems.
Universal approximation of multiscale dynamical systems. While expressing a general form of
input-output maps between sequences, the dynamical system (8) does not explicitly model dynamics
at multiple scales. Instead, here we consider the following two-scale fast-slow dynamical system of
the general form,
φn =f(φn-1,ψn-1,un),	ψn =τg(φn,ψn-1,un), on =o(ψn).	(10)
Here, 0 < τ << 1 and 1 are the slow and fast time scales, respectively. The underlying maps
(f, g) : Rdh ×dh ×du 7→ Rdh are Lipschitz continuous. In the following proposition, proved in
SM§E.5, we show that LEM (3) can approximate (10) to desired accuracy.
Proposition 4.5. For any 0 < τ << 1, and for all 1 ≤ n ≤ N, let φn, ψn, on be given by the
two-scale dynamical system (10) with input signal un. Under the assumption that there exists a
R > 0 such that max{kφnk, kψnk, kunk} < R, for all 1 ≤ n ≤ N, then for any given > 0,
there exists a LEM of the form (3), with hidden states yn, zn ∈ Rdy and output state ωn ∈ Rdo with
ωn = Wyn such that the following holds,
kon - ωnk ≤ , ∀1 ≤ n ≤ N.	(11)
Moreover, the weights, biases and size (number of neurons) of the underlying LEM (3) are indepen-
dent of the time-scale τ .
This argument can be readily generalized to more than two time scales (see SM Proposition E.4).
Hence, we show that, in principle, the proposed model LEM (3) can approximate multiscale dy-
namical systems, with model size being independent of the underlying timescales. These theoretical
results for LEM (3) point to the ability of this architecture to learn complicated multiscale input-
output maps between sequences, while mitigating the exploding and vanishing gradients problem.
Although useful prerequisities, these theoretical properties are certainly not sufficient to demonstrate
that LEM (3) is efficient in practice. To do this, we perform several benchmark evaluations, and we
report the results below.
5	Empirical results
We present a variety of experiments ranging from long-term dependency tasks to real-world applica-
tions as well as tasks which require high expressivity of the model. Details of the training procedure
for each experiment can be found in SM§A. As competing models to LEM, we choose two different
types of architectures—LSTMs and GRUs—as they are known to excel at expressive tasks such as
language modeling and speech recognition, while not performing well on long-term dependency
tasks, possibly due to the exploding and vanishing gradients problem. On the other hand, we choose
state-of-the-art RNNs which are tailor-made to learn tasks with long-term dependencies. Our ob-
jective is to evaluate the performance of LEM and compare it with competing models. All code to
reproduce our results can be found at https://github.com/tk-rusch/LEM.
Very long adding problem. We start with the well-known adding problem (Hochreiter & Schmid-
huber, 1997), proposed to test the ability of a model to learn (very) long-term dependencies. The
input is a two-dimensional sequence of length N , with the first dimension consisting of random
numbers drawn from U ([0, 1]) and with two non-zero entries (both set to 1) in the second dimen-
sion, chosen at random locations, but one each in both halves of the sequence. The output is the
sum of two numbers of the first dimension at positions, corresponding to the two 1 entries in the
second dimension. We consider three very challenging cases, namely input sequences with length
N = 2000, 5000 and 10000. The results of LEM together with competing models including state-
of-the-art RNNs, which are explicitly designed to solve long-term dependencies, are presented in
Fig. 1. We observe in this figure that while baseline LSTM is not able to beat the baseline mean-
square error of 0.167 (the variance of the baseline output 1) in any of the three cases, a proper weight
initialization for LSTM, the so-called chrono-initialization of Tallec & Ollivier (2018) leads to much
better performance in all cases. For N = 2000, all other architectures (except baseline LSTM) beat
the baseline convincingly. However for N = 5000, only LEM, chrono-LSTM and coRNN are able
5
Published as a conference paper at ICLR 2022
to beat the baseline. In the extreme case of N = 10000, only LEM and chrono-LSTM are able to
beat the baseline. Nevertheless, LEM outperforms chrono-LSTM by converging faster (in terms of
number of training steps) and attaining a lower test MSE than chrono-LSTM in all three cases.
Figure 1: Results on the very long adding problem for LEM, coRNN, DTRIV∞ (Casado, 2019),
FastGRNN (Kusupati et al., 2018), LSTM and LSTM with chrono initialization (Tallec & Ollivier,
2018) based on three very long sequence lengths N, i.e., N = 2000, N = 5000 and N = 10000.
Sequential image recognition. We consider three experiments based on two widely-used image
recognition data sets, i.e., MNIST (LeCun et al., 1998) and CIFAR-10 (Krizhevsky et al., 2009),
where the goal is to predict the correct label after reading in the whole sequence. The first two
tasks are based on MNIST images, which are flattened along the rows to obtain sequences of length
N = 784. In sequential MNIST (sMNIST), the sequences are fed to the model one pixel at a time
in streamline order, while in permuted sequential MNIST (psMNIST), a fixed random permutation
is applied to the sequences, resulting in much longer dependency than for sMNIST. We also con-
sider the more challenging noisy CIFAR-10 (nCIFAR-10) experiment (Chang et al., 2018), where
CIFAR-10 images are fed to the model row-wise and flattened along RGB channels, resulting in
96-dimensional sequences, each of length 32. Moreover, a random noise padding is applied after
the first 32 inputs to produce sequences of length N = 1000. Hence, in addition to classifying the
underlying image, a model has to store this result for a long time. In Table 1, we present the results
for LEM on the three tasks together with other SOTA RNNs, which were explicitly designed to solve
long-term dependency tasks, as well as LSTM and GRU baselines. We observe that LEM outper-
forms all other methods on sMNIST and nCIFAR-10. Additionally on psMNIST, LEM performs as
well as coRNN, which has been SOTA among single-layer RNNs on this task.
Table 1: Test accuracies on sMNIST, psMNIST and nCIFAR-10, where M denotes the total number
of parameters of the corresponding model. Results of other models are taken from the respective
original paper referenced in the main text, except that the results for LSTM are taken from Helfrich
et al. (2018), for GRU from Chang et al. (2017) and the results indicated by * are added by us.
Model	MNIST			CIFAR-10	
	sMNIST	PsMNIST	# units / M	nCIFAR-10	# units / M
GRU	99.1%	94.1%	256 / 201k	43.8%*	128 / 88k
LSTM	98.9%	92.9%	256 / 267k	11.6%	128 / 116k
chrono-LSTM	98.9%*	94.6%*	128 / 68k	55.9%*	128 / 116k
anti.sym. RNN	98.0%	95.8%	128/ 10k	48.3%	256/ 36k
Lipschitz RNN	99.4%	96.3%	128 / 34k	57.4%	128 / 46k
expRNN	98.4%	96.2%	360 / 69k	52.9%*	360/ 103k
coRNN	99.3%	96.6%	128/ 34k	59.0%	128 / 46k
LEM	99.5%	96.6%	128 / 68k	60.5%	128 / 116k
EigenWorms: Very long sequences for genomics classification. The goal of this task (Bagnall
et al., 2018) is to classify worms as belonging to either the wild-type or four different mutants, based
on 259 very long sequences (length N = 17984) measuring the motion of a worm. In addition to
the nominal length, it was empirically shown in Rusch & Mishra (2021b) that the EigenWorms
sequences exhibit actual very long-term dependencies (i.e., longer than 10k).
6
Published as a conference paper at ICLR 2022
Table 2: Test accuracies on EigenWorms using 5 re-trainings of each best performing network (based
on the validation set), where all other results are taken from Rusch & Mishra (2021b) except that the
NRDE result is taken from Morrill et al. (2021) and the results indicated by * are added by us.
Model	test accuracy	# units	# params
NRDE	83.8% ± 3.0%	32	35k
expRNN	40.0% ± 10.1%	64	2.8k
IndRNN (2 layers)	49.7% ± 4.8%	32	1.6k
LSTM	38.5% ± 10.1%*	32	5.3k
BiLSTM+1d-conv	40.5% ± 7.3%*	22	5.8k
chrono-LSTM	82.6 % ± 6.4%*	32	5.3k
coRNN	86.7% ± 3.0%	32	2.4k
UnICORNN (2 layers)	90.3% ± 3.0%	32	1.5k
LEM	92.3% ± 1.8%	32	5.3k
Following Morrill et al. (2021) and Rusch & Mishra (2021b), we divide the data into a train, valida-
tion and test set according to a 70%, 15%, 15% ratio. In Table 2, we present results for LEM together
with other models. As the validation and test sets, each consist of only 39 sequences, we report the
mean (and standard deviation of) accuracy over 5 random initializations to rule out lucky outliers.
We observe from this table that LEM outperforms all other methods, even the 2-layer UnICORNN
architecture, which has been SOTA on this task.
Healthcare application: Heart-rate prediction. In this experiment, one predicts the heart rate
from a time-series of measured PPG data, which is part of the TSR archive (Tan et al., 2020) and
has been collected at the Beth Isreal Deaconess medical center. The data set, consisting of 7949
sequences, each of length N = 4000, is divided into a train, validation and test set according to
a 70%,15%,15% ratio, (Morrill et al., 2021; Rusch & Mishra, 2021b). The results, presented in
Table 3, show that LEM outperforms the other competing models, including having a test L2 error
of 35% less than the SOTA UnICORNN.
Table 3: Test L2 error on heart-rate prediction using PPG data. All results are obtained by running
the same code and using the same fine-tuning protocol.
Model	test L2 error	# units	# params
LSTM	9.93	128	67k
chrono-LSTM	3.31	128	67k
expRNN	1.63	256	34k
IndRNN (3 layers)	1.94	128	34k
coRNN	1.61	128	34k
UnICORNN (3 layers)	1.31	128	34k
LEM	0.85	128	67k
Multiscale dynamical system prediction. The FitzHugh-Nagumo system (Fitzhugh, 1955)
v0 = V — ^----------W + Iext, w0 = T (V + a — bw),
(12)
is a prototypical model for a two-scale fast-slow nonlinear dynamical system, with fast variable V
and slow variable w and τ << 1 determining the slow-time scale. This relaxation-oscillator is
an approximation to the Hodgkin-Huxley model (Hodgkin & Huxley, 1952) of neuronal action-
potentials under an external signal Iext ≥ 0. With τ = 0.02, Iext = 0.5, a = 0.7, b = 0.8 and initial
data (V0, w0) = (c, 0), with c randomly drawn from U ([—1, 1]), we numerically approximate (12)
with the explicit Runge-Kutta method of order 5(4) in the interval [0, 400] and generate 128 training
and validation and 1024 test sequences, each of length N = 1000, to complete the data set. The
results, presented in Table 4, show that LEM not only outperforms LSTM by a factor of 6 but also
all other methods including coRNN, which is tailormade for oscillatory time-series. This reinforces
our theory by demonstrating efficient approximation of multiscale dynamical systems with LEM.
7
Published as a conference paper at ICLR 2022
Table 4: Test L2 error on FitzHugh-Nagumo system prediction. All results are obtained by running
the same code and using the same fine-tuning protocol.
Model	error (×10-2)	# units	# params
LSTM	1.2	16	1k
expRNN	2.3	50	1k
LipschitzRNN	1.8	24	1k
FastGRNN	2.2	34	1k
coRNN	0.4	24	1k
LEM	0.2	16	1k
Google12 (V2) keyword spotting. The Google Speech Commands data set V2 (Warden, 2018) is
a widely used benchmark for keyword spotting, consisting of 35 words, sampled at a rate of 16 kHz
from 1 second utterances of 2618 speakers. We focus on the 12-label task (Google12) and follow
the pre-defined splitting of the data set into train/validation/test sets and test different sequential
models. In order to ensure comparability of different architectures, we do not use performance-
enhancing tools such as convolutional filtering or multi-head attention. From Table 5, we observe
that both LSTM and GRU, widely used models in this context, perform very well with a test accuracy
of around 95%. Nevertheless, LEM is able to outperform both on this task and provides the best
performance.
Table 5: Test accuracies on Google12. All results are obtained by running the same code and using
the same fine-tuning protocol.
Model	test accuracy	# units	# params
tanh-RNN	73.4%	128	27k
anti.sym. RNN	90.2%	128	20k
LSTM	94.9%	128	107k
GRU	95.2%	128	80k
FastGRNN	94.8%	128	27k
expRNN	92.3%	128	19k
coRNN	94.7%	128	44k
LEM	95.7%	128	107k
Language modeling: Penn Tree Bank corpus. Language modeling with the widely used small
scale Penn Treebank (PTB) corpus (Marcus et al., 1993), preprocessed by Mikolov et al. (2010),
has been identified as an excellent task for testing the expressivity of recurrent models (Kerg et al.,
2019). To this end, in Table 6, we report the results of different architectures, with a similar number
of hidden units, on the PTB char-level task and observe that RNNs, designed explicitly for learning
long-term dependencies, perform significantly worse than LSTM and GRU. On the other hand,
LEM is able to outperform both LSTM and GRU on this task by some margin (a test bpc of 1.25
in contrast with approximately a bpc of 1.36). In fact, LEM provides the smallest test bpc among
all reported single-layer recurrent models on this task, to the best of our knowledge. This superior
performance is further illustrated in Table 7, where the test perplexity for different models on the
PTB word-level task is presented. We observe that not only does LEM significantly outperform (by
around 40%) LSTM, but it also provides again the best performance among all single layer recurrent
models, including the recently proposed TARNN (Kag & Saligrama, 2021). Moreover, the single-
layer results for LEM are better than reported results for multi-layer LSTM models, such as in Gal
& Ghahramani (2016) (2-layer LSTM, 1500 units each: 75.2 test perplexity) or Bai et al. (2018)
(3-layer LSTM, 700 units each: 78.93 test perplexity).
6 Discussion
The design of a gradient-based model for processing sequential data that can learn tasks with long-
term dependencies while retaining the ability to learn complicated sequential input-output maps is
8
Published as a conference paper at ICLR 2022
Table 6: Test bits-per-character (bpc) on PTB character-level for single layer LEM and other single
layer RNN architectures. Other results are taken from the papers cited accordingly in the table,
while the results for coRNN are added by us.
Model	test bpc	# units	# params
anti.sym RNN (Erichson et al., 2021)	1.60	1437	1.3M
Lipschitz RNN (Erichson et al., 2021)	1.42	764	1.3M
expRNN (Kerg et al., 2019)	1.51	1437	1.3M
coRNN	1.46	1024	2.3M
nnRNN (Kerg et al., 2019)	1.47	1437	1.3M
LSTM (Krueger et al., 2017)	1.36	1000	5M
GRU (Bai et al., 2018)	1.37	1024	3M
LEM	1.25	1024	5M
Table 7: Test perplexity on PTB word-level for single layer LEM and other single layer RNN archi-
tectures. ___________________________________________________________________________________
Model	test PerPlexity	# units	# Params
LiPschitz RNN (Erichson et al., 2021)	115.4	160	76k
FastRNN (Kag & Saligrama, 2021)	115.9	256	131k
LSTM (Kag & Saligrama, 2021)	116.9	256	524k
SkiPLSTM (Kag & Saligrama, 2021)	114.2	256	524k
TARNN (Kag & Saligrama, 2021)	94.6	256	524k
LEM	72.8	256	524k
very challenging. In this paper, we have proposed Long Expressive Memory (LEM), a novel re-
current architecture, with a suitable time-discretization of a specific multiscale system of ODEs (2)
serving as the circuit to the model. By a combination of theoretical arguments and extensive empiri-
cal evaluations on a diverse set of learning tasks, we demonstrate that LEM is able to learn long-term
dependencies while retaining sufficient expressivity for efficiently solving realistic learning tasks.
It is natural to ask why LEM performs so well. A part of the answer lies in the mitigation of the
exploding and vanishing gradients problem. Proofs for gradient bounds (6),(7) reveal a key role
played by the hyperparameter ∆t. We observe from SM Table 8 that small values of ∆t might
be needed for problems with very long-term dependencies, such as the EigenWorms dataset. On
the other hand, no tuning of the hyperparameter ∆t is necessary for several tasks such as language
modeling, keyword spotting and dynamical systems prediction and a default value of∆t = 1 yielded
very good performance. The role and choice of the hyperparameter ∆t is investigated extensively in
SM§B.1. However, mitigation of exploding and vanishing gradients problem alone does not explain
high expressivity of LEM. In this context, we proved that LEMs can approximate a very large class of
multiscale dynamical systems. Moreover, we provide experimental evidence in SM§B.2 to observe
that LEM not only expresses a range of scales, as itis designed to do, but also these scales contribute
proportionately to the resulting multiscale dynamics. Furthermore, empirical results presented in
SM§B.2 show that this ability to represent multiple scales correlates with the high accuracy of
LEM. We believe that this combination of gradient stable dynamics, specific model structure, and
its multiscale resolution can explain the observed performance of LEM.
We conclude with a comparison of LEM and the widely-used gradient-based LSTM model. In ad-
dition to having exactly the same number of parameters for the same number of hidden units, our
experiments show that LEMs are better than LSTMs on expressive tasks such as keyword spotting
and language modeling, while also providing significantly better performance on long-term depen-
dencies. This robustness of the performance of LEM with respect to sequence length paves the way
for its application to learning many different sequential data sets where competing models might not
perform satisfactorily.
9
Published as a conference paper at ICLR 2022
Acknowledgements.
The research of TKR and SM was performed under a project that has received funding from the
European Research Council (ERC) under the European Union’s Horizon 2020 research and inno-
vation programme (grant agreement No. 770880). NBE and MWM would like to acknowledge
IARPA (contract W911NF20C0035), NSF, and ONR for providing partial support of this work. Our
conclusions do not necessarily reflect the position or the policy of our sponsors, and no official
endorsement should be inferred.
The authors thank Dr. Ivo Danihelka (DeepMind) for pointing out that the hidden states for LEM
satisfy the maximum principles (19), (20).
References
Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In
International Conference on Machine Learning, pp.1120-1128, 2016.
Anthony Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron Bostrom, Paul
Southam, and Eamonn Keogh. The uea multivariate time series classification archive, 2018. arXiv
preprint arXiv:1811.00075, 2018.
Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional
and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018.
Shaojie Bai, Vladlen Koltun, and J Zico Kolter. Multiscale deep equilibrium models. In Advances
in Neural Information Processing Systems, pp. 770-778, 2020.
A. R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE
Trans. Inform. Theory., 39(3):930-945, 1993.
Mario Lezcano Casado. Trivializations for gradient-based optimization on manifolds. In Advances
in Neural Information Processing Systems, pp. 9154-9164, 2019.
Bo Chang, Minmin Chen, Eldad Haber, and Ed H Chi. Antisymmetricrnn: A dynamical system
view on recurrent neural networks. In International Conference on Learning Representations,
2018.
Shiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui, Michael
Witbrock, Mark A Hasegawa-Johnson, and Thomas S Huang. Dilated recurrent neural networks.
In Advances in Neural Information Processing Systems, pp. 77-87, 2017.
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differential equations. In Advances in Neural Information Processing Systems, pp. 6571-6583,
2018.
Zhengdao Chen, JianyU Zhang, Martin Arjovsky, and Leon Bottou. Symplectic recurrent neural net-
works. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,
Ethiopia, April 26-30, 2020, 2020.
Kyunghyun Cho, B van Merrienboer, Caglar Gulcehre, F Bougares, H Schwenk, and Yoshua Ben-
gio. Learning phrase representations using rnn encoder-decoder for statistical machine translation.
In Conference on Empirical Methods in Natural Language Processing (EMNLP 2014), 2014.
N Benjamin Erichson, Omri Azencot, Alejandro Queiruga, and Michael W Mahoney. Lipschitz
recurrent neural networks. In International Conference on Learning Representations, 2021.
Richard Fitzhugh. Mathematical models of threshold phenomena in the nerve membrane. Bull.
Math. Biophysics, 17:257-278, 1955.
Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent
neural networks. Advances in neural information processing systems, 29:1019-1027, 2016.
Felix A Gers, JUrgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction
with lstm. Neural computation, 12(10):2451-2471, 2000.
10
Published as a conference paper at ICLR 2022
Kyle Helfrich, Devin Willmott, and Qiang Ye. Orthogonal recurrent neural networks with scaled
Cayley transform. In International Conference on Machine Learning, pp. 1969-1978. PMLR,
2018.
Mikael Henaff, Arthur Szlam, and Yann LeCun. Recurrent orthogonal networks and long-memory
tasks. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd In-
ternational Conference on Machine Learning, volume 48 of Proceedings of Machine Learning
Research, pp. 2034-2042, 2016.
G.E. Hinton and D.C. Plaut. Using fast weights to deblur old memories. In Proceedings of the ninth
annual conference of the cognitive science society, pp. 177-186, 1987.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
A.L. Hodgkin and A.F. Huxley. A quantitative description of membrane current and its application
to conduction and excitation in nerve. Journal of Physiology, 117:500-544, 1952.
Anil Kag and Venkatesh Saligrama. Time adaptive recurrent neural network. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 15149-15158,
June 2021.
Giancarlo Kerg, Kyle Goyette, Maximilian Puelma Touzel, Gauthier Gidel, Eugene Vorontsov,
Yoshua Bengio, and Guillaume Lajoie. Non-normal recurrent neural network (nnrnn): learn-
ing long time dependencies while improving expressivity with transient dynamics. In Advances
in Neural Information Processing Systems, pp. 13591-13601, 2019.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
David Krueger, Tegan Maharaj, Janos Kramar, Mohammad Pezeshki, Nicolas Ballas, Nan Rose-
mary Ke, Anirudh Goyal, Yoshua Bengio, Aaron C. Courville, and Christopher J. Pal. Zoneout:
Regularizing rnns by randomly preserving hidden activations. In 5th International Conference
on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track
Proceedings. OpenReview.net, 2017.
Christian Kuehn. Multiple time scale dynamics, volume 191. Springer, 2015.
Aditya Kusupati, Manish Singh, Kush Bhatia, Ashish Kumar, Prateek Jain, and Manik Varma. Fast-
grnn: A fast, accurate, stable and tiny kilobyte sized gated recurrent neural network. In Advances
in Neural Information Processing Systems, pp. 9017-9028, 2018.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo Gao. Independently recurrent neural network
(indrnn): Building a longer and deeper rnn. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 5457-5466, 2018.
Soon Hoe Lim, N Benjamin Erichson, Liam Hodgkinson, and Michael W Mahoney. Noisy recurrent
neural networks. arXiv preprint arXiv:2102.04877, 2021.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated
corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313-330, 1993.
TOmas Mikolov, Martin Karafiat, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Recurrent
neural network based language model. In Eleventh Annual Conference of the International Speech
Communication Association, 2010.
James Morrill, Cristopher Salvi, Patrick Kidger, and James Foster. Neural rough differential equa-
tions for long time series. In Proceedings ofthe 38th International Conference on Machine Learn-
ing, volume 139 of Proceedings of Machine Learning Research, pp. 7829-7838. PMLR, 18-24
Jul 2021.
11
Published as a conference paper at ICLR 2022
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural
networks. In Proceedings of the 30th International Conference on Machine Learning, volume 28
of ICML,13, pp. 111-1310-111-1318. JMLR.org, 2013.
A. F. Queiruga, N. B. Erichson, D. Taylor, and M. W. Mahoney. Continuous-in-depth neural net-
works. Technical Report Preprint: arXiv:2008.02389, 2020.
Alejandro Queiruga, N Benjamin Erichson, Liam Hodgkinson, and Michael W Mahoney. Com-
pressing deep ode-nets using basis function expansions. arXiv preprint arXiv:2106.10820, 2021.
Yulia Rubanova, Ricky TQ Chen, and David Duvenaud. Latent odes for irregularly-sampled time
series. In Proceedings of the 33rd International Conference on Neural Information Processing
Systems, pp. 5320-5330, 2019.
T. Konstantin Rusch and Siddhartha Mishra. Coupled oscillatory recurrent neural network (cornn):
An accurate and (gradient) stable architecture for learning long time dependencies. In Interna-
tional Conference on Learning Representations, 2021a.
T. Konstantin Rusch and Siddhartha Mishra. Unicornn: A recurrent model for learning very long
time dependencies. In Proceedings of the 38th International Conference on Machine Learning,
volume 139 of Proceedings of Machine Learning Research, pp. 9168-9178. PMLR, 2021b.
Corentin Tallec and Yann Ollivier. Can recurrent neural networks warp time? In 6th International
Conference on Learning Representations, ICLR, 2018.
Chang Wei Tan, Christoph Bergmeir, Francois Petitjean, and Geoffrey I Webb. Monash university,
uea, ucr time series regression archive. arXiv preprint arXiv:2006.10996, 2020.
Pete Warden. Speech commands: A dataset for limited-vocabulary speech recognition. arXiv
preprint arXiv:1804.03209, 2018.
Scott Wisdom, Thomas Powers, John Hershey, Jonathan Le Roux, and Les Atlas. Full-capacity
unitary recurrent neural networks. In Advances in Neural Information Processing Systems, pp.
4880-4888, 2016.
12
Published as a conference paper at ICLR 2022
Supplementary Material for:
Long Expressive Memory for Sequence Modeling
A Training details
All experiments were run on CPU, namely Intel Xeon Gold 5118 and AMD EPYC 7H12, except
for Google12, PTB character-level and PTB word-level, which were run on a GeForce RTX 2080 Ti
GPU. All weights and biases of LEM (3) are initialized according to U(-1 /√d, 1 /√d), where d is
the number of hidden units.
Table 8: Rounded hyperparameters of the best performing LEM architecture for each experiment.
If no value is given for ∆t, it means that ∆t is fixed to 1 and no fine-tuning is performed on this
hyperparameter.
experiment	learning rate	batch size	∆t
Adding (N = 10000)	2.6 X 10-3	50	2.42 × 10-2
sMNIST	1.8 × 10-3	128	2.1 × 10-1
psMNIST	3.5 × 10-3	128	1.9 × 100
nCIFAR-10	1.8 × 10-3	120	9.5 × 10-1
EigenWorms	2.3 × 10-3	8	1.6 × 10-3
Healthcare	1.56 × 10-3	32	1.9 × 10-1
FitzHugh-Nagumo	9.04 × 10-3	32	/
Google12	8.9 × 10-4	100	/
PTB character-level	6.6 × 10-4	128	/
PTB word-level	6.8 × 10-4	64	/
The hyperparameters are selected based on a random search algorithm, where we present the
rounded hyperparameters for the best performing LEM model (based on a validation set) on each
task in Table 8.
We base the training for the PTB experiments on the following language modelling code:
https://github.com/deepmind/lamb, where we fine-tune, based on a random search algorithm, only
the learning rate, input-, output- and state-dropout, L2-penalty term and the maximum gradi-
ent norm.
We train LEM for 100 epochs on sMNIST, psMNIST and nCIFAR-10, after which we decrease the
learning rate by a factor of 10 and proceed training for 20 epochs. Moreover, we train LEM for
50, 60 as well as 400 epochs on EigenWorms, Google12 and FitzHugh-Nagumo. We decrease the
learning rate by a factor of 10 after 50 epochs on Google12. On the Healthcare task, we train LEM
for 250 epochs, after which we decrease the learning rate by a factor of 10 and proceed training for
250 epochs.
B Further Experimental Results
B.1	ON THE CHOICE OF THE HYPERPARAMETER ∆t.
The hyperparameter ∆t in LEM (3) measures the maximum allowed (time) step in the discretization
of the multi-scale ODE system (2). In propositions 4.1, 4.2 and 4.3, this hyperparameter ∆t plays
a key role in the bounds on the hidden states (4) and their gradients (6). In particular, setting ∆t =
O(N-1) will lead to hidden states and gradients, that are bounded uniformly with respect to the
underlying sequence length N . However, these upper bounds on the hidden states and gradients
account for worst-case scenarios and can be very pessimistic for the problem at hand.
Thus, in practice, we determine ∆t through a hyperparameter tuning procedure as described in
section A. To this end, we perform a random search within ∆t < 2 and present the resulting optimal
values of ∆t for each of the considered data sets in Table 8. From this table, we observe that for
data sets such as PTB, FitzHugh-Nagumo and Google 12 we do not need any tuning of ∆t and a
13
Published as a conference paper at ICLR 2022
Figure 2: Sensitivity study on hyperparameter
∆t in (3) using the EigenWorms experiment.
Figure 3: Average (over ten different initializa-
tions each) test mean-square error on the adding
problem of LEM for different sequence lengths
N, where the hyperparameter ∆t of LEM (3) is
fixed to ∆t = 1 / √N.
default value of ∆t = 1 resulted in very good empiricial performance. On the other data sets such
as sMNIST, nCIFAR-10 and the healthcare example, where the sequence length (N = O(103)) is
larger, we observe that values of ∆t ≈ 0.1 yielded the best performance. The notable exception to
this was for the EigenWorms data set, with a very long sequence length of N = 17984 as well as
demonstrated very long range dependencies in the data, see Rusch & Mishra (2021b). Here, a value
of∆t = 1.6 × 10-3 resulted in the best observed performance. To further investigate the role of the
hyperparameter ∆t in the EigenWorms experiment, we perform a sensitivity study where the value
of ∆t is varied and the corresponding accuracy of the trained LEM is observed. The results of this
sensitivity study are presented in Fig. 2, where we plot the test accuracy (Y-axis) vs. the value of
∆t (X-axis). From this figure, we observe that the accuracy is rather poor for ∆t ≈ 1 but improves
monotonically as ∆t is reduced till a value of approximately 10-2, after which it saturates. Thus,
in this case, a value of ∆t = O(N-2) (for sequence length N) suffices to yield the best empirical
performance.
Given this observation, We further test whether ∆t = O(N- 1) suffices for other problems with
long-term dependencies. To this end, we consider the adding problem and vary the input sequence
length by an order of magnitude, i.e., from N = 250 to N = 2000. The value of ∆t is now fixed at
∆t = √= and the resulting test loss (Y-axis) VS the number of training steps (X-axis) is plotted in
Fig. 3. We see from this figure that this value of ∆t sufficed to yield very small average test errors
for this problem for all considered sequence lengths N. Thus, empirically a value of∆t in the range
√N ≤ ∆t ≤ 1 yields very good performance.
Even if we set ∆t = O(√=), it can happen for very long sequences N >> 1 that the gradient can
be quite small from the gradient asymptotic formula (7). This might lead to saturation in training,
resulting in long training times. However, we do not observe such long training times for very long
sequence lengths in our experiment. To demonstrate this, we again consider Fig. 3 where the number
of training steps (X-axis) is plotted for sequence lengths that vary an order of magnitude. The figure
clearly shows that the approximately the same number of training steps are needed to attain a low
test error, irrespective of the sequence length. This is further buttressed in Fig. 1, where similar
number of training steps where needed for obtaining the same very low test error, even for long
sequence lengths, with N up to 10000. Moreover, from section A, we see that the number of epochs
for different data sets is independent of the sequence length. For instance, only 50 epochs were
necessary for EigenWorms with a sequence length of N = 17984 and ∆t = 1.6 × 10-3 whereas
400 epochs were required for the FitzHugh-Nagumo system with a ∆t = 1.
14
Published as a conference paper at ICLR 2022
B.2	Multiscale Behavor of LEM.
LEM (3) is designed to represent multiple scales, with terms ∆tn, ∆tn being explicitly designed
to learn possible multiple scales. In the following, we will investigate if in practice, LEM learns
multiple scales and uses them to yield the observed superior empirical performance with respect to
competing models.
To this end, we start by recalling the proposition 4.5 where we showed that in principle, LEM
can learn the two underlying timescales of a fast-slow dynamical system (see proposition E.4 for
a similar result for the universal approximation of a r-time scale (with r ≥ 2) dynamical system
with LEM). Does this hold in practice ? To further investigate this issue, we consider the FitzHugh-
Nagumo dynamical system (12) which serves as a prototype for a two-scale dynamical system.
We consider this system (12) with the two time-scales being T = 0.02 and 1 and train LEM for this
system. In Fig. 4, we plot the empirical histogram thatbins the ranges oflearned scales ∆tn, ∆tn ≤
∆t = 2 (for all n and d) and counts the number of occurrences of ∆tn, ∆tn in each bin. From
this figure, we observe that there is a clear concentration of learned scales around the values 1 and
τ = 0.02, which exactly correspond to the underlying fast and slow time scales. Thus, for this model
problem, LEM is exactly learning what it is designed to do and is able to learn the underlying time
scales for this particular problem.
Figure 4: Histogram of (∆tn)i and (∆tn)i for all n = 1,...,N and i = 1,...,d of LEM (3) after
training on the FitzHugh-Nagumo fast-slow system (12) using ∆t = 2.
Nevertheless, one might argue that these learnable mutliple scales ∆tn, ∆tn are not necessary and
a single scale would suffice to provide good empirical performance. We check this possibility on the
FitzHugh-Nagumo data set by simply setting ∆tn, ∆tn ≡ ∆t1 (with 1 being the vector with all
entries set to 1), for all n and tuning the hyperparameter ∆t. The comparative results are presented
in Table 9. We see from this table by not allowing for learnable ∆tn, ∆tn and simply setting them
to a single scale parameter ∆t and tuning this parameter only leads to results that are comparable to
the baseline LSTM model. On the other hand, learning ∆tn, ∆tn resulted in an error that is a factor
of 6 less than the baseline LSTM test error. Thus, we demonstrate the importance of the ability of
the proposed LEM model to learn multiple scales in this example.
Table 9: Test L2 error on FitzHugh-Nagumo system prediction.
Model	error (×10-2)	# units	# params
LSTM	1.2	16	1k
LEM w/o multi-scale	1.1	16	1k
LEM	0.2	16	1k
15
Published as a conference paper at ICLR 2022
Figure 5: Histogram of (∆tn)i and (∆tn)i for all n
training on the GoogIe12 data set
1,...,N and i = 1,...,d of LEM (3) after
Hence, the multiscale resolution of LEM seems essential for the fast-slow dynamical system. Does
this multiscale resolution also appear for other datasets and can it explain aspects of the observed
empirical performance ? To this end, We consider the Google12 Keyword spotting data set and start
by pointing out that given the spatial (with respect to input dimension d) and temporal (with respect
to sequence length N) heterogeneities, a priori, it is unclear if the underlying data has a multiscale
structure. We plot the empirical histograms of ∆tn, ∆tn in Fig. 5 to observe that even for this
problem, the terms ∆tn, ∆tn are expressed over a range of scales, amounting to 2 - 3 orders of
magnitude. Thus, a range of scales are present in the trained LEM even for this example, but do they
affect the empirical performance of LEM ? We investigate this question by performing an ablation
study and reporting the results in Fig. 6. In this study, we clip the values of ∆tn, ∆tn to lie within
the range [2-i, 1], for i = 0,1,..., 7 and plot the statistics of the observed test accuracy of LEM.
We observe from Fig. 6 that by clipping ∆tn, ∆tn to lie near the default (single scale) value of 1
results in very poor empirical performance of an accuracy of ≈ 65%. Then the accuracy jumps to
around 90% when an order of magnitude range for ∆tn, ∆tn is considered, before monotonically
and slowly increasing to yield the best empirical performance for the largest range of values of
∆tn, ∆tn, considered in this study. A closer look at the empirical histograms plotted in Fig. 5 reveal
that the proportion of occurrences of ∆tn, ∆tn decays as a power law, and not exponentially, with
respect to the scale amplitude. This, together with results presented in Fig. 6 suggest that not only
do a range of scales occur in learned ∆tn, ∆tn, the small scales also contribute proportionately to
the dynamics and enable the increase in performance shown in Fig. 6.
Figure 6: Average (and standard deviation of) test accuracies of 5 runs each for LEM on Google12,
where ∆tn and ∆tn in (3) are clipped to the ranges [*,1] for i = 0,..., 7 during training.
16
Published as a conference paper at ICLR 2022
Finally, in Fig. 7, We plot the empirical histograms of ∆tn and ∆tn for the learned LEM on the
SMNIST data set to observe that again a range of scales are observed and the observed occurrences
of ∆tn and ∆tn at each scale decays as a power law with respect to scale amplitude. Hence, we
have sufficient empirical evidence to claim that the multi-scale resolution of LEM seems essential to
its observed performance. However, further investigation is required to elucidate the precise mech-
anisms through this multiscale resolution enables superior performance, particularly on problems
where the multiscale structure of the underlying data may not be explicit.
104
103
102
101
100
10-7	10-5	10-3	10-1
10-6	10-4	10-2
∆t
∆t
Figure 7: Histogram of (∆tn)i and (∆tn)i for all n = 1,...,N and i = 1,...,d of LEM (3) after
training on the sMNIST data set
B.3	On gradient-stable initialization.
Specialized weight initialization is a popular tool to increase the performance of RNNs on long-term
dependencies tasks. One particular approach is the so-called chrono initialization (Tallec & Ollivier,
2018) for LSTMs, where all biases are set to zero except for the bias of the forget gate as well as the
input gate (bf and bi in the LSTM (15)), which are sampled from
bf 〜log(U[1,Tmax - 1])
bi = -bf,
where Tmax denotes the maximal temporal dependency of the underlying sequential data. We can
see in Table 2 that the chrono initialization significantly improves the performance of LSTM on the
EigenWorms task. Hence, we are interested in extending the chrono initialization to LEMs. One
possible manner for doing this is as follows: Initialize all biases of LEM to zero except for b1 in (3),
which is sampled from
b1 〜-Iog(U[1, Tmaxδ% - 1]).
Table 10: Test accuracies on EigenWorms using 5 re-trainings of each best performing network
(based on the validation set), where we train LSTM and LEM with and without chrono intialization,
as well as LEM WithOUt chrono initialization but With tuned ∆t.__________________________________
Model	test accuracy	# units	# params	chrono	tuning ∆t
LSTM	38.5% ± 10.1%	32	5.3k	NO	/
LSTM	82.6 % ± 6.4%	32	5.3k	YES	/
LEM	57.9% ± 7.7%	32	5.3k	NO	NO
LEM	88.2% ± 6.9%	32	5.3k	YES	NO
LEM	92.3% ± 1.8%	32	5.3k	NO	YES
We test the chrono initialization for LEM on the EigenWorms dataset, where we train LEM (without
tuning ∆t, i.e., setting ∆t = 1), with and without chrono initialization. We provide the results in
Table 10, where we show again the results of LSTM with and without chrono initialization as well
as the LEM result with tuned ∆t and without chrono initialization from Table 2 for comparison. We
17
Published as a conference paper at ICLR 2022
see from Table 10 that when ∆t is fixed to 1, the chrono initialization significantly improves the
result of LEM. However, if we tune ∆t, but do not use the chrono initialization, we significantly
improve the performance of LEM again. We further remark that tuning ∆t as well as using chrono
initialization for LEM does not improve the results obtained with simply tuning ∆t in LEM. Thus,
we conclude that chrono initialization can successfully be adapted to LEM. However, tuning ∆t
(which controls the gradients) is still advisable in order to obtain the best possible results.
C Relation between LEM and The Hodgkin-Huxley equations
We observe that the multiscale ODEs (2), on which LEM is based, are a special case of the following
ODE system,
dz = Fz	(y,t)-	Gz(y,t)	Θ z,	dy	= Fy	(z,t) Θ H	(y,t) -	Gy(y,t) Θ y.	(13)
dt	dt
As remarked in the main text, it turns out the well-known Hodgkin-Huxley equations Hodgkin &
Huxley (1952), modeling the the dynamics of the action potential of a biological neuron can also be
written down in the abstract form (13), with dy = 1, dz = 3 and the variables y = y modeling the
voltage and z = (z1, z2, z3) modeling the concentration of Potassium activation, Sodium activation
and Sodium inactivation channels.
The exact form of the different functions in (13) for the Hodgkin-Huxley equations is given by,
Fz (y) = (α1(y), α2(y), α3(y)),
Gz (y) = (α1 (y) + β1(y), α2 (y) + β2(y), α3(y) + β3(y)) ,
α1(y)
0.01(10 + y - y) z .0.1(25 + y - y)	/、_nn7 ⅛0y
10 + y-y	, α2(y) =	25 + y-y	， a3(y) = 0.07e ,	门心
e 10— 1	e 10— 1	(14)
βι(y) = 0.125ey-y,	β2(y) = 4ey-y, l⅛(y) = —1,…,
1 + e1+ F-
Fy (Zt) = U⑴ + z4 + z3z3, H(y) = cι(y - y) + c2(y - y), Gy (y) = c3,
with input current U and constants y, y, ci 2 3, whose exact values can be read from (Hodgkin &
Huxley, 1952).
Thus, the multiscale ODEs (2) and the Hodgkin-Huxley equations are special case of the same
general family (13) of ODEs. Moreover, the gating functions Gy,z(y), that model voltage-gated ion
channels in the Hodgkin-HUxley equations, are similar in form to ∆tn, ∆tn in (2).
It is also worth highlighting the differences between our proposed model LEM (and the underlying
ODE system (2)) and the Hodgkin-Huxley ODEs modeling the dynamics of the neuronal action
potential. Given the complicated form of the nonlinearites Fy,z , Gy,z , H in the Hodgkin-Huxley
equations (14), we cannot use them in designing any learning model. Instead, building on the
abstract form of (13), we propose bespoke non-linearities in the ODE (2) to yield a tractable learning
model, such as LEM (3). Moreover, it should be emphasized that the Hodgkin-Huxley equations
only model the dynamics of a single neuron (with a scalar voltage and 3 ion channels), whereas the
hidden state dimension dof (2) can be arbitrary.
D Relation between LEM and LSTM
The well-known LSTM (Hochreiter & Schmidhuber, 1997) (in its mainly-used version using a forget
gate (Gers et al., 2000)) is given by,
fn = σ(Wf hn-1 + Vfun + bf )
in = 3(Wihn-i + Viun + bi)
On = σ(Wohn-i + Voun + b。)	(15)
cn = fn Θcn-1+in Θ σ(Whn-1 + Vun +b)
hn = On Θ σ(cn).
Here, for any 1 ≤ n ≤ N, hn ∈ Rd is the hidden state and cn ∈ Rd is the so-called cell state. The
vectors in , fn , On ∈ Rd are the input, forget and output gates, respectively. un ∈ Rm is the input
18
Published as a conference paper at ICLR 2022
signal and the weight matrices and bias vectors are given by W, Wf,i,o ∈ Rd×d, V, Vf,i,o ∈ Rm×d
and b, bf,i,o ∈ Rd, respectively.
It is straightforward to relate LSTM (15) and LEM (3) by first setting the cell state cn = zn, for all
1 ≤ n ≤ N and the hidden state hn = yn .
We further need to assume that the input state in = ∆tn and the forget state has to be fn = 1 -∆tn.
Finally, the output state of the LSTM (15) has to be
On = ∆tn = 1, ∀1 ≤ n ≤ N.
Under these assumptions and by setting ∆t = 1, we can readily observe that the LEM (3) and LSTM
(15) are equivalent.
A different interpretation of LEM, in relation to LSTM, is as follows; LEM can be thought of a
variant of LSTM but with two cell states γn Zn per unit and no output gate. The input gates are
∆tn and ∆tn and the forget gates are coupled to the input gates. Given that the state Zn is fed into
the update for the state yn , one can think of one of the cell states sitting above the other, leading to
a more sophisticated recursive update for LEM (3), when compared to LSTM (15).
Table 11: Test accuracies on EigenWorms using 5 re-trainings of each best performing network
(based on the validation set) for LSTMs with ∆t-scaled input and forget gates, as well as LSTMs
with sub-sampling routines, baseline LSTM and LEM.
Model	test accuracy	# units	# params
t-BPTT LSTM	57.9% ± 7.0%	32	5.3k
sub-samp. LSTM	69.2% ± 8.3%	32	5.3k
LSTM	38.5% ± 10.1%	32	5.3k
∆t-LSTM v1	53.3% ± 8.2%	32	5.3k
∆t-LSTM v2	56.9% ± 6.7%	32	5.3k
LEM	92.3% ± 1.8%	32	5.3k
Another key difference between LEM and LSTM is the scaling of the learnable gates in LEM by
the small hyperparameter ∆t. It is natural to examine whether scaling LSTM with such a small
hyperparameter ∆t will improve its performance on sequential tasks with long-term dependencies.
To this end, we propose to scale the input and forget gate ofan LSTM with a small hyperparameter
in two different ways, where we denote the new forget gate as fn and the new input gate as in . The
first version is (∆t-LSTM v1)
fn = ∆tfn, in = ∆tin,
while the second version is (∆t-LSTM v2)
△ , , _ ʌ .
fn = (1 - ∆t)fn, in = ∆t%.
We can see in Table 11 that both ∆t-scaled versions of the LSTM lead to some improvements
over the baseline LSTM for very long-sequence Eigenworms dataset, while still performing very
poorly when compared to LEM. Moreover, we can see that standard sub-sampling routines, such
as a truncation of the BPTT algorithm or random sub-sampling, applied to LSTMs lead to better
improvements than ∆t-scaling the forget and input gate.
E S upplement to the rigorous analysis of LEM
In this section, we will provide detailed proofs of the propositions in Section 4 of the main article.
We start with the following simplifying notation for various terms in LEM (3),
An-1	W1yn-1 +V1un +b1,
Bn-1	W2yn-1 + V2un + b2,
Cn-1	Wzyn-1 + Vzun + bz,
Dn	WyZn + Vyun + by .
19
Published as a conference paper at ICLR 2022
Note that for all 1 ≤ n ≤ N, An , Bn , Cn , Dn ∈ Rd. With the above notation, LEM (3) can be
written componentwise, for each component 1 ≤ i ≤ d as,
Zn = zn-1 + ∆tσ(An-i)σ(Cn-i) - ∆tσ(An-i)zn-i
y = ynn-1 + ∆tσ(Bn-i)σ(Dn) - ∆tσ(Bn-i)yn-1.
Moreover, we will use the following order-notation,
β = O(α), for α, β ∈ R+ if there exists constants C, C such that Cα ≤ β ≤ Cα.
M = O(α), for M ∈ Rd1×d2, α ∈ R+ if there exists constant C such that ∣∣Mk ≤ Cɑ.
(16)
(17)
Note that the techniques of proof in this following three sub-sections burrow heavily from those
introduced in Rusch & Mishra (2021a).
E.1 Proof of Proposition 4.1 of main text.
First, we prove Proposition 4.1, which yields the bound (4) for the hidden states of LEM.
Proof. The proof of the bound (4) is split into 2 parts. We start with the first equation in (16) and
rewrite it as,
Zn = (1 - ∆tσ(An-i)) Zin-I + ∆tσ(An-i)σ(Cn-i).
Noting that the activation functions are such that 0 ≤ σ(x) ≤ 1, for all X and -1 ≤ σ(x) ≤ 1, for
all x and using the fact that ∆t ≤ 1, we have from the above expression that,
Zin ≤ (1 - δt6(An-I)) maχ (Zn-1, 1) + δt6(An-I) maχ (Zn-1,1),
≤ max Zin-1, 1 .
By a symmetric argument, one can readily show that,
Zin ≥ min(-1,Zin-1).
Combining the above inequalities yields,
min(-1, Zn-i) ≤ Zn ≤ max(Zn-i, 1) .	(18)
Iterating (18) over n and using z0i = 0 for all 1 ≤ i ≤ d leads to,
-1 ≤	Zin	≤ 1,	∀n,	∀1	≤ i ≤ d.	(19)
An argument, identical to the derivation of (19), but for the hidden state y yields,
-1 ≤yin	≤ 1,	∀n,	∀1	≤i ≤ d.	(20)
Thus, we have shown that the hidden states remain in the interval [-1, 1], irrespective of the se-
quence length.
Next, we will use the following elementary identities in the proof,
b(a - b) = % - y - 2(a - b)2,	QI)
for any a, b ∈ R, and also,
ab ≤ ^^2—+丞，∀e > 0.	(22)
We fix 1 ≤ i ≤ d and multiply the first equation in (16) with Zn-1 and apply (21) to obtain,
20
Published as a conference paper at ICLR 2022
夺=(ɪɪ + ∆tσ(An-1)σ(Cn-1)zn-1 — ∆tσ(An-1)(zn-1)2 + (Zn- Zn-1)2
=色/ + ∆tσ(An-1)σ(Cn-1)zn-1 — ∆tσ(An-1)(zn-1)2
∆t2	2
1	2 (σ(An-1)σ(Cn-1) — σ(An-1)zn-1) , (from (16))
≤ (ɪɪ + ∆tσ(An-i)∣σ(Cn-i)||zn-1∣— ∆tσ(An-i)(zn-i)2
∆t2	∆t2
+ F (σ(An-1)σ(Cn-1))2 + ^rσ(An-ι )2(zn-1)2
+^t2σ(An-I)2∣σ(Cn-I)∣∣zn-1∣ 侬(。一b)2 ≤ «2+b2+2∣a∣∣b∣)
We fix € = 1-∆t in the elementary identity (22) to yield,
∣σ(Cn-i)∣∣zn-∕≤
σ(CD2
2e
l €(Zn-1)2
+ -2—
Applying this to the inequality for (zn)2 leads to,
亨 ≤ 也/ + (∆tσ(An-1) + ∆t2σ(An-1)2)
σ(CLι)2
2
,	, -	、	€
一 ∆tσ(An-I) 1 - £
	
∆tσ(An-ι) ∆tσ(An-ι)e]
__________ _ __________
2
2
(Zn-a2.
Using the fact that 0 ≤ σ(x) ≤ 1 for all x ∈ R, σ2 ≤ 1 and that € = ɪ-ʌt, We obtain from the last
line of the previous equation that,
(Zn )2 ≤ (Zn-I)2 + 坦+竺 ≤ (Zn-I)2 +
€
∆t(1 + ∆t)2
2 — ∆t	，∀1 ≤ In
Iterating the above estimate over n = 1,... ,n,for any 1 ≤ n and setting n = n yields,
(zn)2 ≤ (z0)2+n
∆t(1 + ∆t)2
⇒	(zn)2 ≤ tn (1+∙?
2 — ∆t
2 — ∆t ,
2
—	as z0 = 0, tn = n∆t.
Taking a square root in the above inequality yields,
∣zn∣ ≤ ∆√tn,	∀n, ∀1 ≤ i ≤ d.
with ∆ defined in the expression (4).
We can repeat the above argument with the hidden state y to obtain,
卜n ∣ ≤ ∆√tn,	∀n, ∀1 ≤ i ≤ d.
(23)
(24)
Combining (23) and (24) with the pointwise bounds (19) and (20) yields the desired bound (4). □
E.2 Proof OF Proposition 4.2 OF main text.
Proof. We can apply the chain rule repeatedly (for instance as in Pascanu et al. (2013)) to obtain,
∂En = X	∂En ∂Xn ∂ +Xfc
~∂θ	=ι⅛	dX； ∂Xk ~∂Γ'
1≤k≤n、	’"_、._ J
^^{^^
aE(k)
dEn
∂θ
(25)
21
Published as a conference paper at ICLR 2022
TT	,1	. J	∂+Xλ.	C . . 1 ∙	.1	C FT" .,1	. . . 1	.
Here, the notation dθ k refers to taking the partial derivative of Xk with respect to the parameter
θ, while keeping the other arguments constant.
A straightforward application of the product rule yields,
∂Xn
∂Xk
π
∂X'
∂X∏
(26)
For any k < ' ≤ n, a tedious yet straightforward computation yields the following representation
formula,
^XX- = I2d×2d + ∆tE'j + ∆t2 F','-1.	(27)
dx'-1
Here E',`-1 ∈ R2d×2d is a matrix whose entries are given below. For any 1 ≤ i ≤ d, we have,
E，M-i ≡ 0, j= i
E2 匚；2i-1 = -σ(A'-ι),
E2i-ι12j = (Wι)i,jσ0(A'-1) (σ(C'-i) - z'-J + (Wz)^σ(A'-i)σ0(C'-i), ∀1 ≤ j ≤ d
E24-1 = (Wy)i,jσ(B'-i)σ0(D'), ∀1 ≤ j ≤ d
E2名1 = (W2)i,jσ0(B'-i) (σ(D') - y'-J , j = i
E2'-1 = -σ(B'-i) + (W2)i,6'(B'-i) (σ(D') - y'-i).
_	(28)
Similarly, F','-1 ∈ R2d×2d is a matrix whose entries are given below. For any 1 ≤ i ≤ d, we have,
F2i-ιj ≡ 0,	∀ 1 ≤ j ≤ 2d,
F2怎-1 = -(Wy)i,jσ(Aj-i)σ(B'-i)σ0(D'),	1 ≤ j ≤ d,
d
F2i'-1 = σ(B'-i)σ0(D') X(Wy)i,λ ((σ(Cλ-i) - zλ-i) 6'(A3)(Wi)i,j + 6(A32©-i)(Wz)λ,j∙).
λ=1
(29)
Using the fact that,
sup max (∣σ(x)∣, ∣σ0(x)∣, ∣σ(x)∣, ∣σ 0(x)∣} ≤ 1,
x∈R
the pointwise bounds (4), the notation tn = n∆t for all n, the definition of η (6) and the definition
of matrix norms, we obtain that,
∣∣E','-1k∞ ≤ max {1 + IlWzh + (1 + min(1,∆√¾)kWιk∞, 1 + IlWyh + (1 + min(1,∆√¾)kW2k∞}
≤ 1 + (2 + min(1, ∆√^))η.
(30)
By similar calculations, we obtain,
∣∣F','-1∣∣∞ ≤ IlWy∣∣∞ (1 + (1+min(1,∆√5))∣∣Wι∣∣∞ + IlWz∣∣∞)
≤ η(1 + (2 + min(1, ∆√tl))η).
Applying (30) and (31) in the representation formula (27) and observing that ∆t ≤ 1 and ' ≤ n, we
obtain.
∂X'
∂X'-1
≤ 1 + (1 + (2 + min(1, ∆√t7))η) ∆t + η (1 + (2 + min(1, ∆√t^))η) ∆t2,
∞
≤ 1 + 2∆t,
With
Γ = 2(1+ η)(1 + 3η)
(32)
Using the expression (26) with the above inequality yields,
(33)
22
Published as a conference paper at ICLR 2022
Next, we choose ∆t << 1 small enough such that the following holds,
n-k
≤ 1 +Γ(n — k)∆t,
(34)
for any 1 ≤ k < n.
Hence applying (34) in (33), we obtain,
∂Xr
∂xk
≤ 1 + Γ(n - k)∆t.
∞
(35)
For the sake of definiteness, we fix any
1 ≤ α, β ≤ d and set θ = (Wy)α,β in the following.
The following bounds for any other choice of θ ∈ Θ can be derived analogously. Given this, it is
straightforward to calculate from the structure of LEM (3) that entries of the vector
given by,
∂+Xk
∂(Wy )α,β
are
∂+Xk
∂(Wy)α,β
∂+Xk
∂(Wy)α,β 2α
≡ 0, ∀ j 6= 2α,
j
=∆tσ(Bα-i)σ0(Dα)zβ.
(36)
Hence, by the pointwise bounds (4), we obtain from (36) that
∂+Xk
∂(Wy)α,β
≤ ∆t min(1, ∆√k).
∞
(37)
Finally, it is straightforward to calculate from the loss function En = ɪ ∣∣yn — yrk2 that
羡=[0, yn -y1,
,0, yn- yd].
(38)
Therefore, using the pointwise bounds (4) and the notation Y = ∣∣y∣∣∞, we obtain
∂Er
∂Xn
≤ Y + min(1, ∆√n).
∞
(39)
Applying (35), (37) and (39) in the definition (25) yields,
∂E(nk)
∂(Wy)α,β
≤ ∆t min(1, ∆√k (Y + min(1, ∆√tn)) (1 + Γ(n — k)∆t).
(40)
Observing that 1 ≤ k ≤ n, we see that n - k ≤ n and tk ≤ tn . Therefore, (41) can be estimated for
any 1 ≤ k ≤ n by,
∂E(nk)
∂(Wy)α,β
≤ ∆t min(1, ∆√tn) (Y + min(1, ∆√η)) (1+Γtn), 1 ≤ k ≤ n.
(41)
Applying the bound (41) in (25) leads to the following bound on the total gradient,
∂En	, XX	∂Erfc)
∂(Wy)α,β ≤ k=1 ∂(Wy)α,β
≤ tn min(1, ∆√tn) (Y + min(1, ∆√tn)) (1 + Γtn)	(42)
≤ tn(1 + Y)(I + rtn)
≤ (i+Y)tn+(1+Y)Ttn
which is the desired bound (6) for θ = (Wy)α,β.
23
Published as a conference paper at ICLR 2022
Moreover, for long-term dependencies i.e., k << n, we can set tk = k∆t < 1, with k independent
of sequence length n, in (40) to obtain the following bound on the partial gradient,
∂Enk)
∂(Wy )α,β
≤ ∆t3 ∆√k(1 + Y) (1 + Γtn),
1 ≤ k << n.
(43)
□
Remark E.1. The bound (6) on the total gradient depends on tn = n∆t, with n being the sequence
length and ∆t ≤ 1, a hyperparameter which can either be chosen a priori or determined through a
hyperparameter tuning procedure. The proof of the bound (42) relies on ∆t being sufficiently small.
It would be natural to choose ∆t 〜n-s, for some S ≥ 0. Substituting this expression in (6) leads
to a bound of the form,
∂∂θn = O (n2(I-S))	(44)
If s = 1, then clearly ∣ d∂Eθn ∣ = O(1) i.e., the total gradient is bounded. Clearly, the exploding
gradient problem is mitigated in this case.
On the other hand, if S takes another value, for instance S = 2 which is empirically observed
during the hyperparameter training (see Section B.1, then we can readily observe from (44) that
∣ dEn ∣ = O(n). Thus in this case, the gradient can grow with SeqUenCe length n but only linearly
and not exponentially. Thus, the exploding gradient problem is also mitigated in this case.
E.3 Proof of Proposition 4.3 of main text.
To mitigate the vanishing gradient problem, we need to obtain a more precise characterization of
∂ E(k)
the gradient defined in (25). For the sake of definiteness, We fix any 1 ≤ α,β ≤ d and set
θ = (Wy)α,β in the following. The following formulas for any other choice ofθ ∈ Θ can be derived
analogously. Moreover, for simplicity of notation, we set the target function Xn ≡ 0.
Proposition 4.3 is a straightforWard corollary of the folloWing,
Proposition E.2. Let yn , zn be the hidden states generated by LEM (3), then we have the following
representation formula for the hidden state gradient,
∂ Enk)
∂θ
∆tσ(Bα-i)σ0(Dα)zβ " yθ)
dn
+ ∆t2σ(Bα-i)σ0(Dα)zβ X ⑼-yn) X σ0(Bj-J (σ(D') - y'-J (W2加。
j = 1	'=k+1
n
+ ∆t2σ(Bα-1)σ0(Dα)zβ	X σ(Bα-ι) (yα - ya)	+ O(∆t3).
_'=k+1	.
(45)
Here, the constants in O could depend on η defined in (6) (main text).
∂ E(k)
Proof. The starting point for deriving an asymptotic formula for the hidden state gradient -n^- is
to observe from the representation formula (27), the bound (31) on matrices F','-1 and the order
notation (17) that,
~^X~ = I2d×2d + ∆tE','-1 + O(∆t2),	(46)
∂X'-ι
as long as η is independent of ∆t.
By using induction and the bounds (30),(31), it is straightforward to calculate the following repre-
sentation formula for the product,
∂Xn = Y M = I2d×2d + ∆t X E','-1 + O(∆t2).
∂Xk	∂ ∂ ∂X'-ι
k	k<'≤n	'-1	'=k+1
(47)
24
Published as a conference paper at ICLR 2022
Recall that we have set θ = (Wy)α,β. Hence, by the expressions (38) and (36), a direct but tedious
calculation leads to,
∂E	∂+X
K I2d×2d Rk = ∆tσ(Bα-ι)σ0(Dα)zβ (yα - yα),
∂Xn	∂θ
(48)
XX ∂En E','-1 ∂+Xk
'=k+l dXn	dθ
(49)
∆tσ(Bα-i)σ0(Dα)zβ
dn	n
X j y) X σ0(Bj-i) (σ(Dj) - y'-J (W2 j,2α - X σ(BOa-i) (yα
j = 1	'=k+1	'=k+1
yα)
(50)
Therefore, by substituting the above expression into the representation formula (47) yields the de-
sired formula (45).
In order to prove the formula (7) (see Proposition 4.3 of main text), we focus our interest on long-
term dependencies i.e., k << n. Then, a closer perusal of the expression in (48), together with the
pointwise bounds (4) which implies that yk-ι ≈ O(√∆t), results in the following,
∂En	∂+Xk
∂XnI2d×2d ∂θ
(51)
Similarly, we also obtain,
n
∆t X
'=k+1
dEn e','- 1 d+Xk
∂Xn	∂θ
Combining (51) and (52) results in the desired asymptotic bound (7).
(52)
□
Remark E.3. The upper bound on the gradient (6) and the gradient asymptotic formula (7) impact
the choice ofthe timestep hyperparameter ∆t. Forsequence length n, ifwe choose ∆t 〜n-s, with
s ≥ 0, we see from Remark E.1 that the upper bound on the total gradient scales like O(n2(1-s)).
On the other hand, from (7), the gradient contribution from long-term dependencies will scale like
-3s
O(n"ɪ). Hence, a small value of S ≈ 0, will ensure that the gradient with respect to long-term
dependencies will be O(1). However, the total gradient will behave like O(n2) and possibly blow
up fast. Similarly, setting s ≈ 1 leads to a bounded gradient, while the contributions from long-term
dependencies decay as fast as n-3. Hence, one has to find a value of S that balances both these
requirements. Equilibrating them leads to S = 4, ensuring that the total gradient grows Sub-Iinearly
while long-term dependencies still contribute with a sub-linear decay. This value is very close to the
empirically observed value of S = ɪ which also ensures that the total gradient grows linearly and
the contribution of long-term dependencies decays sub-linearly in the sequence length n.
E.4 Proof of Proposition 4.4
Proof. To prove this proposition, we have to construct hidden states yn , zn, output state ωn, weight
matrices W1,2,y,z , Wy, V1,2,y,z and bias vectors b1,2,y,z such that LEM (3) with output state ωn =
Wyyn approximates the dynamical system (8).
Let R* > R >> 1 and e* < e, be parameters to be defined later. By the theorem for universal
approximation of continuous functions with neural networks with the tanh activation function σ =
tanh (Barron, 1993), given e*, there exist weight matrices Wi ∈ Rd1×dh, Vi ∈ Rd1×du, W? ∈
Rdh ×d1 and bias vector b1 ∈ Rd1 such that the tanh neural network defined by,
Ni (h, u) = W2σ(Wih+Viu+ bi) ,
(53)
approximates the underlying function f in the following manner,
max
max(khk,kuk)<R*
Ilf(h,u)- Ni(h,u)k ≤ E*.
(54)
Similarly, one can readily approximate the identity function g(h, u) = h with a tanh neural network
of the form,
N(h) = W2σ (Wih),	(55)
25
Published as a conference paper at ICLR 2022
such that
kh*R*k"- N2(h)k ≤ e*.	(56)
Next, we define the following dynamical system,
Zn = W2σ (WIyn-I + VlUn + bl),
Nn = W2σ (WIzn),
(57)
with initial states zo = yo = 0.
Using the approximation bound (54), we derive the following bound,
kφn - ynk = Hf (φn-1, Un) - zn + zn - yn ||
≤ kf (Φn-1, Un) - W2σ (WIyn-I + VIun + bl) k + ∣∣g(zn) — W2。(WIzn) k
≤ kf (Φn-1, Un) — f (yn-1, Un) k + kf (Nn-1, Un) — 卬2。(W 1 Nn-1 + VlUn + bl) k
+ kg(zn) — W2。(W1zn) k
≤ Lip(f)kΦn-ι — yn-ik +2e*	(from (54), (56))∙
Here, Lip(f) is the Lipschitz constant of the function f on the compact set {(h, u) ∈
Rdh×du : khk, ∣u∣ < R*}. Note that one can readily prove using the fact that yo = zo = 0,
bounds (54), (56) and the assumption kΦnk,kUnk < R, that kznk, kNnk < R* = 2R.
Iterating the above inequality over n leads to the bound,
n-1
kΦn - ynk ≤ 2e* XLip(f)λ.	(58)
λ=o
Hence, using the Lipschitz continuity of the output function o in (8), one obtains,
n-1
kθn — o(yn)k ≤ 2e*Lip(o) XLip(f)λ,	(59)
λ=o
with Lip(o) being the Lipschitz constant of the function o on the compact set {h ∈ Rdh : khk <
R*}.
Next, we can use the universal approximation theorem for neural networks again to conclude that
given a tolerance 工 there exist weight matrices W3 ∈ Rd2 ×dh, W4 ∈ Rdh ×d2 and bias vector
b2 ∈ Rd2 such that the tanh neural network defined by,
N3(h) = W4。 (W3h + b2) ,	(60)
approximates the underlying output function o in the following manner,
llmax ko(h) — N3(h)k ≤ C	(61)
khk<R*
Now defining,
ωn = W4σ (W3yn + b2),	(62)
we obtain from (61) and (59) that,
n-1
kθn — ωnk ≤ e + 2e*Lip(o) X Lip(f)λ.	(63)
λ=o
Next, we introduce the notation,
zn = σ (WIyn-I + VIun + bi) , Nn =。(W1zn) ∙	(64)
From (57), we see that
zn = W2zn,	yn = W2yn	(65)
26
Published as a conference paper at ICLR 2022
Thus from (65) and (63), we have
ωn = W4σ (W3W2yn + b2),
=W4σ (W3W2σ (W1W2Zn) + b2).
(66)
Define the function R : Rdh × Rdu 7→ Rdo by,
R(Z) = W4σ (W3W2σ (W1W2z) + b2) .	(67)
The function, defined above, is clearly Lipschitz continuous. We can apply the universal ap-
Proximation theorem for tanh neural networks to find, for any given tolerance e, weight matrices
W5 ∈ Rd3×d4, W6 ∈ Rdo ×d3, V2 ∈ Rd3×du and bias vector b3 ∈ Rd3 such that the following holds,
max	IlR(Z) - W6σ(W5z + b3)k ≤ e.	(68)
max(kz∣∣)<R*
Denote G^ = W6σ(W5Zn + b3), then from (68) and (66), We obtain that
kωn - ωnk ≤ C.
Combining this estimate with (63) yields,
n-1
kon - ωnk ≤ € + ^ + 2E*LiP(O) ^X Lip(f)λ.	(69)
λ=0
Now, we collect all ingredients to define the LEM that can aPProximate the dynamical system (8).
To this end, we define hidden states zn , yn ∈ R2dh as
Zn = [zn, zn] , yn = [yn, yn] ,
with Zn, ^n, yn, yn, ∈ Rdh. These hidden states are evolved by the dynamical system,
zn⊥ = σ	W10W2	00	yn⊥-1 + [V1un,0]⊥ + [b1,0]⊥	,
yn⊥ = σ	WW1W2	00	Zn⊥ + [0, 0]⊥ +	[0,b3]⊥
and the outPut state is calculated by,
ωn⊥ = [0, W6]yn⊥ .
(70)
(71)
Finally, we can recast the dynamical system (70), (71) as a LEM of the form (3) for the hidden states
yn, Zn, defined in (70), with the following parameters, Now, define the hidden states Nn, Gn ∈ Rdy
for all 1 ≤ n ≤ N by the LEM (3) with the following parameters,
∆t = 1, dy = 2dh,
W1 = W2 = V1 = V2 = 0
b1 = b2 = b∞,
	Wz	=	W1IW2 0	0 0	,	Vz =	[V1,0],	bz= [b1,0]	(72)
	Wy	=	WIW2 W5	0 0	,	Vy =	0,	bz = [0, b3].	
	Wy	= [0,W6].					
Here, b∞ ∈	Rdh is defined	as					
			b∞	[b∞, b∞, . .		. , . . . , b∞],	
with 1 << b	∞ is such that			|1	-σ(b∞) |	≤ δ.	(73)
The nature of the sigmoid function guarantees the existence of such a b∞ for any δ . As δ decays
exponentially fast, we set it to 0 in the following for notational simplicity.
27
Published as a conference paper at ICLR 2022
It is straightforward to verify that the output state of the LEM (3) with parameters given in (72) is
ωn = ωn.
Therefore, from (69) and by setting e < 3, W < f and
*	e
e <	N-1
6Lip(o) P Lip(f)λ
λ=0
we prove the desired bound (9).
□
E.5 Proof of Proposition 4.5
Proof. The proof of this proposition is based heavily on the proof of Proposition 4.4. Hence, we
will highlight the main points of difference.
As the steps for approximation of a general Lipschitz continuous output map are identical to the
corresponding steps in the proof of proposition 4.4 (see the steps from Eqns. (59) to (69)), we will
only consider the following linear output map for convenience herein,
o(ψn) = Wcψn.
(74)
Let R* > R >> 1 and * < , be parameters to be defined later. By the theorem for universal
approximation of continuous functions with neural networks with the tanh activation function σ =
tanh, given *, there exist weight matrices W1f, W2f ∈ Rd1 ×dh, V1f ∈ Rd1×du , W3f ∈ Rdh ×d1 and
bias vector b1f ∈ Rd1 such that the tanh neural network defined by,
Nf(h,c,u) = W3fσ W1fh+ W2fc+ V1fu+bf1 ,	(75)
approximates the underlying function f in the following manner,
max
maχ(Ilhk,kck,kUk)<R*
kf(h,c,u) - Nf (h, c, u)k ≤ *.
(76)
Next, we define the following map,
G(h, c, u) = g(h, c, u) +
(77)
for any τ > 0.
By the universal approximation theorem, given *, there exist weight matrices W1g, W2g ∈
Rd2 ×dh , V1g ∈ Rd2 ×du, W3g ∈ Rdh ×d2 and bias vector bg1 ∈ Rd2 such that the tanh neural net-
work defined by,
Ng(h,c,u) = W3gσ(W1gh+ W2gc+ V1gu+ b1g) ,	(78)
approximates the function G (77) in the following manner,
max
maχ(khk,kck,kuk)<R*
kG(h, c, u) - Nf (h, c, u)k ≤ * .
(79)
Note that the sizes of the neural network Ng can be made independent of the small parameter τ by
simply taking the sum of the neural networks approximating the functions g and g(h, c, u) = C with
tanh neural networks. As neither of these functions depend on the small parameter τ , the sizes of
the corresponding neural networks are independent of the small parameter too.
Next, as in the proof of proposition 4.4, one can readily approximate the identity function
f(h, c, u) = h with a tanh neural network of the form,
Nf(h) = W2σ (Wιh),
(80)
such that
max
khk,kck,kuk<R*
..ʌ, ...
kf(h,c,u)- Nf(h)k ≤ e*,
(81)
28
Published as a conference paper at ICLR 2022
and with the same weights and biases, one can approximate the identity function g(h, C) u)
the tanh neural network,
Ng (c) = W2σ (WIC),
c with
(82)
such that
max
khk,kck,kuk<R*
I∣g(h, c,u) - Ng(c)k ≤ e*.
(83)
Next, We define the following dynamical system,
Zn = Wf σ (wfVn-1 + Wfyn-1 + VfUn + bf),
Zn = W2σ (W1yn-1),
勺n = (1 - T)yn-1 + τWgσ (WgZn + WgZn + VgUn + bg),
yn = W2σ (WιZn),
(84)
with hidden states Zn, Zn, yn, yn ∈ Rdh and with initial states Zq = Zq = yo = yo = 0.
We derive the following bounds,
Il φn - Znk = ∣∣f (φn-1, ψn-1, Un)- Wfσ (wf yn-1 + Wf yn-1 + VLf un + bf) ∣∣
≤ l∣f(Φn-1, Ψn-1, Un) - f(yn-1, Zn-1, Un)∣,
+ ∣∣f (yn-1, Zn-1, Un)- Wf σ (Wfyn-I + Wfyn-I + Vf Un + bf) ∣∣
≤ LiP(f) (lφn-1 - Zn-1∣∣ + 2∣yn-1 - Zn-1∣∣ + ∣∣ψn-1 - yn-1∣) + e* (by (79))
≤ Lip(f) (I∣φn-1 - Zn-111 + I∣ψn-1 - yn-ι∣) + (1 + 2Lip(f)) E* (by (81), (84)),
and
llψn - yn∣ = II(I-T)(ψn-1 - yn-1)+ T(G(On, ψn-1, Un)- Wgσ (WgZn + WgZn + VLgUn + b1 )) ∣∣
≤ ∣∣Ψn-1 - yn-1∣ + T∣∣G(φn, Ψn-1, Un)- G(Zn, Zn, Un)Il
+ T ∣∣G(Zn, Zn, Un)- W∙f σ (Wy Zn + WgZn + Vg Un + bg ) ∣∣
≤ llψn-1 - yn-1)∣∣ + TLiP(G) (∣φn - Zn! + IIZn - yn-1∣ + ∣∣ψn-1 - yn-11) + 丁乙
≤ (1 + TLiP(G))(1+Lip(f))∣ψn-1 - yn-11 + TLiP(G)LiP(f)∣φn-1 - Zn-IIl
+ T (1+LiP(G)(2 + 2LiP(f )))e*,
where the last inequality follows by using the previous inequality together with (84) and (83).
As T < 1, it is easy to see from (77) that LiP(G) < LiP(g) + ∣. Therefore, the last inequality
reduces to,
llψn - yn∣∣ ≤ (3 + TLiP(g))(1+LiP(f))∣∣ψn-1 - yn-11 + (2 + TLiP(g))LiP(f)∣φn-1 - Zn-IIl
+ (t +(2 + T LiP(g))(2 + 2LiP(f)))E".
Adding we obtain,
Ilφn - Zn|| + ∣∣ψn - yn ∣∣ ≤ C* (∣φn-1 - Zn-1∣∣ + ∣∣ψn-1 - yn-1∣) + D*e*,
where,
C * = max{(3 + LiP(g))LiP(f),LiP(f)(3 + LiP(g))(1 + LiP(f))},
D* = 1 + (2 + LiP(g))(2 + 2LiP(f)).
(85)
(86)
Iterating over n leads to the bound,
n-1
∣φn - Znk + IΨn - yn I ≤ e*。* X(C *)λ.	(87)
λ=0
Here, LiP(f), LiP(g) are the Lipschitz constants of the functions f, g on the compact set {(h,c,u) ∈
Rdh×dh×du : ∣∣h∣, ∣c∣, ∣u∣ < R*}. Note that one can readily prove using the zero val-
ues of initial states, the bounds (81), (83) and the assumption IIOnIJ^nIJUnII < R, that
∣Zn ∣∣,∣∣Zn∣∣,∣∣yn∣,∣yn∣ <R*=2R.
29
Published as a conference paper at ICLR 2022
Using the definition of the output function (11) and the bound (87) that,
n-1
kon - o(yn)k ≤ kWcke*D* X(C*)λ.	(88)
λ=0
Defining the dynamical system,
Zn = σ (Wf W2yn-i + Wf Wgyn_1 + Vf Un + bf)
Zn = σ (W1Wg yn-1)
(89)
yn = (1- T)yn-i + τσ(WgWfZn + WgW2In + VgUn + bg),
yn = σ (WIWf zn).
By multiplying suitable matrices to (84), we obtain that,
Zn =	Wf zn,	Zn = W2Zn,	ifn	=	Wg 濡,0n	=	W2 Nn∙	(90)
Finally, in addition to b∞ defined in (58), for any given τ ∈ (0, 1], we introduce bτ ∈ R defined by
σf(bτ) = τ.	(91)
The existence of a unique bτ follows from the fact that the sigmoid function σf is monotone. Next,
we define the two vectors b∞ , bτ ∈ R2dh as
bi∞ = b∞,	∀ 1 ≤ i ≤ 2dh,
biτ =bτ,	∀1 ≤i ≤dh,
(92)
biτ =b∞,	∀dh+1 ≤i ≤ 2dh.
We are now in a position to define the LEM of form (3), which will approximate the two-scale
dynamical system (10). To this end, we define the hidden states Zn, yn ∈ R2dh such that Zn =
[Zn, Zn] and Yn = [yn, Nn]. The parameters for the corresponding LEM of form (3) given by,
∆t = 1, dy = 2dh
W1 = W2 = V1 = V2 ≡ 0,
b1 = b∞,	b2 = bτ,
Wz =	[WfW∣	Wf)W2]	,	Vz =	[Vf0],	bz	=	[bf, 0],	(93)
Wy =	WgWf	WgW2	,	Vz =	Mg0],	bz	=	[bg,0],
and with following parameters defining the output states,
Wy = [WcW3g 0] ,	(94)
yields an output state ωn = Wyyn .
It is straightforward to observe that ωn ≡ o(yfn). Hence, the desired bound (11) follows from (87)
by choosing,
*	e
C =---------------
N-1
D* P (C*)λ
λ=0
□
The proof of proposition 4.5 can be readily extended to prove the following proposition about a
general r-scale dynamical system of the form,
φ1n = τ1f1(φ1n-1,φ2n-1, . . . ,φrn-1Un),
φ2n = τ2f2 (φ1n-1 , φ2n-1 , . . . , φrn-1 Un),
..........................................
(95)
..........................................
φrn = τr f r (φ1n-1 , φ2n-1 , . . . , φrn-1 Un),
on = o(φ1n,φ2n, . . .,φrn).
30
Published as a conference paper at ICLR 2022
Here, τ1 ≤ τ2 ..... ≤	τr ≤ 1, with r > 1, are the r-time scales of the dynamical system (95).
We assume that the underlying maps f 1,2,...,r are Lipschitz continuous. We can prove the following
proposition,
Proposition E.4. For all 1 ≤ n ≤ N, let φ1n,2,...,r, on be given by the r-scale dynamical sys-
tem (95) with input signal un . Under the assumption that there exists a R > 0 such that
max{kΦnk,∣φnk,...,∣Φnk,kun k} < R, for all 1 ≤ n ≤ N, then for any given > 0, there
exists a LEM of the form (3), with hidden states yn , zn and output state ωn with ωn = Wyn such
that the following holds,
kon - ωn k ≤ , ∀1 ≤ n ≤ N.	(96)
Moreover, the weights, biases and size (number of neurons) of the underlying LEM (3) are indepen-
dent of the time-scales τ1,2,...,r.
F LEMs emulate Heterogeneous multiscale methods for ODEs
Following Kuehn (2015), we consider the following prototypical example of a fast-slow system of
ordinary differential equations,
φ0 ⑴=1 (f (ψ)- φ),
ψ0(t) = g(φ, ψ).
(97)
Here φ, ψ ∈ Rm are the fast and slow variables respectively and 0 < τ << 1 is a small parameter.
Note that we have rescaled time and are interested in the dynamics of the slow variable ψ(t) in the
time interval [0, T].
A naive time-stepping numerical scheme for (97) requires a time step size δt 〜 O(T). Thus, the
computation will entail time updates N 〜 O(1∕τ). Hence, one needs a multiscale ODE solver
to approximate the solutions of the system (97). One such popular ODE solver can be derived by
using the Heterogenous multiscale method (HMM); see Kuehn (2015) and references therein. This
in turns, requires using two time stepping schemes, a macro solver for the slow variable, with a time
step ∆t of the form,
ψn = ψn-1 + ∆tg(φn, ψn-1)∙	(98)
Here, the time step ∆t < 1 is independent of the small parameter T.
Moreover, the fast variable is updated using a micro solver of the form,
φ(nk-)1=φ(nk--11)-δt(f(ψn-1)-φ(nk--11)),	1≤k≤K.
φn = φnK-1,	(99)
φ(n0-) 1 = φn-1.
Note that the micro time step size δt and the number of micro time steps K are assumed to indepen-
dent of the small parameter T.
It is shown in Kuehn (2015) (Chapter 10.8) that for any given small tolerance > 0, one can choose
a macro time step ∆t, a micro time step δt, the number K of micro time steps, the number N of
macro time steps, independent of T, such that the discrete states ψn approximate the slow-variable
ψ(tn) (with tn = n∆t) of the fast-slow system (97) to the desired accuracy of .
Our aim is to show that we can construct a LEM of the form (3) such that the states φn , ψn , defined
in (98), (99) can be approximated to arbitrary accuracy. By combining this with the accuracy of
HMM, we will prove that LEMs can approximate the solutions of the fast-slow system (97) to
desired accuracy, independent of the small parameter T in (97).
Proposition F.1. Let φn , ψn ∈ Rm, for 1 ≤ n ≤ N, be the states defined by the HMM dynamical
system (98), (99). For any given > 0, there exists a LEM of the form (3) with hidden states [zn, yn],
where zn, yn ∈ Rdm and output states ωnh, ωnc such that the following holds,
max {kΦn - ωn∣∣,∣∣ψn - ωf ∣∣} ≤ e, ∀1 ≤ n ≤ N.	(100)
31
Published as a conference paper at ICLR 2022
Proof. We start by using iteration on the micro solver (99) from k = 1 to k = K to derive the
following,
φn = δtφn-1 + (I- δt)f (ψn-1),
δt = (1 — δt)K .
(101)
As δt < 1, We have that δt < 1.
By the universal approximation theorem for tanh neural networks, for any given tolerance e*, there
exist weight matrices W1f ∈ Rd1 ×m, W2f ∈ Rm×d1 and bias vector bf1 ∈ Rd1 such that the tanh
neural network defined by,
Nf (C) = W2fσ W1fC+ b1f ,
approximates the underlying function f in the following manner,
(102)
kcmk<axR* kf (C) - Nf (C)k ≤*.
(103)
Next, we define the following map,
G(h, c) = g(h, c) + c,
(104)
By the universal approximation theorem, given e*, there exist weight matrices Wg, Wg ∈
Rd2 ×m, W3g ∈ Rm×d2 and bias vector b1g ∈ Rd2 such that the tanh neural network defined by,
Ng(h,c) = W3gσ(W1gh+W2gc+bg1) ,
approximates the function G (104) in the following manner,
(105)
max(kmakXk)<R. kG(h，C)- Ng (h,c)k≤ J
(106)
Next, as in the proof of propositions 4.4 4.5, one can readily approximate the identity function
O ,,	一 . 一	一	一	一 一 一 一
f(h, c) = h with a tanh neural network of the form,
Nf(h) = W2σ (Wιh),
(107)
such that
ll^lmax p*kf(h, C)- Nf(A)k ≤ e*,
khk,kck<R*
and with the same weights and biases, one can approximate the identity function g(h, c)
the Tanh neural network,
(108)
c with
such that
Ng (C) = W2σ (Wιc),
(109)
”％*" C)-Ng(C)k ≤ e*.
Then, we define the following dynamical system,
(110)
with hidden states Z
Zn =	= δtZn + (1- δt)Wf σ (WfNn-1 + bf),	
Zn =	=W2σ (WWn-1),	
yn =	二(1 - Δt)yn-1 + ∆tWgσ (WgZn + WgZn	+bg1),
Nn 二	二 W2σ (WIZn),	
n, zn,	yn, yn ∈ Rm and with initial states Zo = Zo	=yo = yo
0.
(111)
Completely analogously as in the derivation of (87), we can derive the following bound,
kφn - znk + kψn - ynk ≤ C*e*,
with constant C = C* (n, Lip(f), Lip(g)).
(112)
32
Published as a conference paper at ICLR 2022
Defining the dynamical system,
Zn = δtzn + (1- δt)σ (wf Wgyn-1 + bf)
Zn = σ (WIW3gyn-1)
y = (i- ∆t)ynT + ∆tσ (wf Wf Zn+wg W况+bg)
yn = σ (WIWf zn).
By multiplying suitable matrices to (113), we obtain that,
Zn =	Wf	Zn,	Zn	=	W2Zn,	yn = Wg ynl,	Wn	=	W2pn∙
(113)
(114)
In addition to b∞ defined in (58), for δt ∈ (0,1], We introduce bδ ∈ R defined by
σ(bδ) = δt.	(115)
Similarly for ∆t ∈ (0,1], we introduce b∆ ∈ R defined by
σ(b∆) = ∆t.	(116)
The existence of unique bδ and b∆ follows from the fact that the sigmoid function σ is monotone.
Next, we define the two vectors b∞, bδ, b∆ ∈ R2m as
biδ = bδ,	∀ 1 ≤ i ≤ m,
biδ = b∞,	∀ m + 1 ≤ i ≤ 2m,
bi∆ = b∆, ∀ 1 ≤ i ≤ m,
bi∆ = b∞,	∀ m + 1 ≤ i ≤ 2m.
(117)
We define the LEM of form (3), which will approximate the HMM (98),(99). To this end, we define
the hidden states Zn, Yn ∈ R2m such that Zn = [z/, Z" and Yn = [yn, yn]. The parameters for the
corresponding LEM of form (3) given by,
∆t = 1, dy = 2m
W1 = W2 = V1 = V2 ≡ 0,
b1	bδ,	b2		= b∆,				
Wz		W1f W3g W 1W∙f	0 0	,	Vz = 0,		bz = [bf1 , 0],	(118)
Wy		W1g W3f W 1Wf	C -	-l Wg W2 0		,	Vz	= 0,	bz = [bg1 , 0].	
The output states are defined by,							
		ωnh	W	2fZnn,	ωnh =	W3gynn	(119)
Zn, ωn = Yn. Hence, the desired bound (100) follows
It is straightforward to observe that ωnh
from (112) by choosing,
n
Cn.
□
33