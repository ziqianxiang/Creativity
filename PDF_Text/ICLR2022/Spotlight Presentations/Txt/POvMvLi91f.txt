Published as a conference paper at ICLR 2022
DR3: Value-Based Deep Reinforcement
Learning Requires Explicit Regularization
Aviral Kumar1,2	Rishabh Agarwal2,3
Tengyu Ma4 Aaron Courville3 George Tucker2 Sergey Levine1,2
1 uc Berkeley 2 Google Research 3 MiLA 4 stanford university
aviralk@berkeley.edu
Ab stract
Despite overparameterization, deep networks trained via supervised learning are
easy to optimize and exhibit excellent generalization. one hypothesis to explain
this is that overparameterized deep networks enjoy the benefits of implicit regu-
larization induced by stochastic gradient descent, which favors parsimonious so-
lutions that generalize well on test inputs. it is reasonable to surmise that deep
reinforcement learning (RL) methods could also benefit from this effect. in this
paper, we discuss how the implicit regularization effect of sGD seen in supervised
learning could in fact be harmful in the offline deep RL setting, leading to poor
generalization and degenerate feature representations. our theoretical analysis
shows that when existing models of implicit regularization are applied to temporal
difference learning, the resulting derived regularizer favors degenerate solutions
with excessive “aliasing”, in stark contrast to the supervised learning case. We
back up these findings empirically, showing that feature representations learned
by a deep network value function trained via bootstrapping can indeed become
degenerate, aliasing the representations for state-action pairs that appear on either
side of the Bellman backup. To address this issue, we derive the form of this im-
plicit regularizer and, inspired by this derivation, propose a simple and effective
explicit regularizer, called DR3, that counteracts the undesirable effects of this
implicit regularizer. When combined with existing offline RL methods, DR3 sub-
stantially improves performance and stability, alleviating unlearning in Atari 2600
games, D4RL domains and robotic manipulation from images.
1	Introduction
Deep neural networks are overparameterized, with billions of parameters, which in principle should
leave them vulnerable to overfitting. Despite this, supervised learning with deep networks still learn
representations that generalize well. A widely held consensus is that deep nets find simple solutions
that generalize due to various implicit regularization effects [6, 48, 4, 23, 47, 30]. We may surmise
that using deep neural nets in reinforcement learning (RL) will work well for the same reason,
learning effective representations that generalize due to such implicit regularization effects. But is
this actually the case for value functions trained via bootstrapping?
in this paper, we argue that, while implicit regularization leads to effective representations in super-
vised deep learning, it may lead to poor learned representations when training overparameterized
deep network value functions. in order to rule out confounding effects from exploration and non-
stationary data distributions, We focus on the offline RL setting - where deep value networks must
be trained from a static dataset of experience. There is already evidence that value functions trained
via bootstrapping learn poor representations: value functions trained with offline deep RL eventually
degrade in performance [2, 28] and this degradation is correlated with the emergence of low-rank
features in the value network [28]. our goal is to understand the underlying cause of the emer-
gence of poor representations during bootstrapping and develop a potential solution. Building on
the theoretical framework developed by Blanc et al. [6], Damian et al. [11], we characterize the im-
plicit regularizer that arises when training deep value functions with TD learning. The form of this
implicit regularizer implies that TD-learning would co-adapt feature representations at state-action
tuples that appear on either side of a Bellman backup.
We show that this theoretically predicted aliasing phenomenon manifests in practice as feature co-
adaptation, where the features of consecutive state-action tuples learned by the Q-value network
1
Published as a conference paper at ICLR 2022
become very similar in terms of their dot product (Section 3). This co-adaptation co-occurs with
oscillatory learning dynamics, and training runs that exhibit feature co-adaptation typically converge
to poorly performing solutions. Even when Q-values are not overestimated, prolonged training in
offline RL can result in performance degradation as feature co-adaptation increases. To mitigate
this co-adaptation issue, which arises as a result of implicit regularization, we propose an explicit
regularizer that we call DR3 (Section 4). While exactly estimating and cancelling the effects of
the theoretically derived implicit regularizer is computationally difficult, DR3 provides a simple and
tractable theoretically-inspired approximation that mitigates the issues discussed above. In practice,
DR3 amounts to regularizing the features at consecutive state-action pairs to be dissimilar in terms
of their dot-product similarity. Empirically, we find that DR3 prevents previously noted pathologies
such as feature rank collapse [28], gives methods that train for longer and improves performance
relative to the base offline RL method employed in practice.
Our first contribution is the derivation of the implicit regularizer that arises when training deep
net value functions via TD learning, and an empirical demonstration that it manifests as feature
co-adaptation in the offline deep RL setting. Feature co-adaptation accounts at least in part for
some of the challenges of offline deep RL, including degradation of performance with prolonged
training. Second, we propose a simple and effective explicit regularizer for offline value-based RL,
DR3, which minimizes the feature similarity between state-action pairs appearing in a bootstrapping
update. DR3 is inspired by the theoretical derivation of the implicit regularizer, it alleviates co-
adaptation and can be easily combined with modern offline RL methods, such as REM [2], CQL [27],
and BRAC [49]. Empirically, using DR3 in conjunction with existing offline RL methods provides
about 60% performance improvement on the harder D4RL [18] tasks, and 160% and 25% stability
gains for REM and CQL, respectively, on offline RL tasks in 17 Atari 2600 games. Additionally, we
observe large improvements on image-based robotic manipulation tasks [38].
2	Preliminaries
The goal in RL is to maximize the long-term discounted reward in an MDP, defined as
(S, A, R, P, γ) [36], with state space S, action space A, a reward function R(s, a), dynamics
P(S 1S, a) and a discount factor Y ∈ [0,1). The Q-function Qn(s, a) for a policy π(a|s) is the
expected sum of discounted rewards obtained by executing action a at state S and following ∏(a|S)
thereafter. Qπ(s, a) is the fixed point of Q(s, a) := R(s, a) + YEs，“(,∣S,a,ao〜∏(.∣So)[Q(S0, a0)].
We study the offline RL setting, where the algorithm must learn a policy only using a given dataset
D = {(Si, ai, S0i, ri)}, generated from some behavior policy, πβ (a|S), without active data collection.
The Q-function is parameterized with a neural net with parameters θ. We will denote the penultimate
layer of the deep network (the learned features) φθ(S, a), such that Qθ(S, a) = wT φ(S, a), where
w ∈ Rd. Standard deep RL methods [33, 24] convert the Bellman equation into a squared temporal
difference (TD) error objective for Qθ :
Ltd(θ)=	X (R(S, a) + YQθ(S0, a0) - Qθ(S, a))2,	(1)
S,a,s0〜D
where Qθ is a delayed copy of same Q-network, referred to as the target network and a0 is computed
by maximizing the target Q-function at state S0 for Q-learning (i.e., when computing Q*) and by
sampling a0 〜π(∙∣s) when computing the Q-value Qπ of a policy π.
A major problem in offline RL is the issue of distributional shift between the learned policy and the
behavior policy [29]. Since our goal is to study the effect of implicit regularization in TD-learning
and not distributional shift, we build on top of existing offline RL methods in our experiments:
CQL [27], which penalizes erroneous Q-values during training, REM [2], which utilizes an ensemble
of Q-functions, and BRAC [49], which applies a policy constraint. An overview of these methods is
provided in Appendix E.
3	Implicit Regularization in Deep RL via TD-Learning
While the “deadly-triad” [39] suggests that training value function approximators with bootstrap-
ping off-policy can lead to divergence, modern deep RL algorithms have been able to successfully
combine these properties [43]. However, making too many TD updates to the Q-function in offline
deep RL is known to sometimes lead to performance degradation and unlearning, even for otherwise
effective modern algorithms [17, 16, 2, 28]. Such unlearning is not typically observed when training
overparameterized models via supervised learning, so what about TD learning is responsible for it?
We show that one possible explanation behind this pathology is the implicit regularization induced
2
Published as a conference paper at ICLR 2022
Large feature dot products arise when
out-of-sample actions are used in TD-Iearning
compared to SARSAz despite similar Q-values
Large feature dot products eventual∣v
correlate with divergent Q-values
-----DQN -------- Supervised
Large feature dot products arise when Q-Iearning
backs up unseen actions, despite no divergence
Figure 1: Feature dot-products φ(s, a)>φ(s0, a0) increase during training when backing up from out-of-
sample but in-distribution actions (TD-learning: left, Q-learning: right), though the average Q-value con-
verges and stays relatively constant. Using only seen state-action pairs for backups (offline SARSA) or not
performing Bellman backups (i.e., supervised regression) avoids this issue, with stable and relatively low
dot products. Left: TD-learning with high feature dot products eventually destabilizes and produces incorrect
Q-values, Right: DQN attains extremely large feature dot products, despite a relatively stable trend in Q-values.
by minimizing TD error on a deep Q-network. Our theoretical results suggest that this implicit
regularization “co-adapts” the representations of state-action pairs that appear in a Bellman backup
(we will define this more precisely below). Empirically, this typically manifests as “co-adapted”
features for consecutive state-action tuples, even with specialized TD-learning algorithms that ac-
count for distributional shift, and this in turn leads to poor final performance both in theory and
in practice. We first provide empirical evidence of this co-adaptation phenomenon in Section 3.1
(additional evidence in Appendix A.1) and then theoretically characterize the implicit regularization
in TD learning, and discuss how it can explain the co-adaptation phenomenon in Section 3.2.
3.1	Feature Co-Adaptation And How It Relates To Implicit Regularization
In this section, we empirically identify a feature co-adaptation phenomenon that appears when
training value functions via bootstrapping, where the feature representations of consecutive state-
action pairs exhibit a large value of the dot product φ(s, a)>φ(s0, a0). Note that feature co-adaptation
may arise because of high cosine similarity or because of high feature norms. Feature co-adaptation
appears even when there is no explicit objective to increase feature similarity.
Experimental setup. We ran supervised regression and three variants of approximate dynamic pro-
gramming (ADP) on an offline dataset consisting of 1% of uniformly-sampled data from the replay
buffer of DQN on two Atari games, previously used in Agarwal et al. [2]. First, for comparison, we
trained a Q-function via supervised regression to Monte-Carlo (MC) return estimates on the offline
dataset to estimate the value of the behavior policy. Then, we trained variants of ADP which differ
in the selection procedure for the action a0 that appears in the target value in LTD(θ) (Equation 1).
The offline SARSA variant aims to estimate the value of the behavior policy, Qπβ , and sets a0 to
the actual action observed at the next time step in the dataset, such that (s0, a0) ∈ D. The TD-
learning variant also aims to estimate the value of the behavior policy, but utilizes the expectation
of the target Q-ValUe over actions a0 sampled from the behavior policy ∏β, a0 ~ ∏β(∙∣s0). We do
not have access to the functional form of πβ for the experiment shown in Figure 1 since the dataset
corresponds to the behavior policy induced by the replay buffer of an online DQN, so we train a
model for this policy using supervised learning. However, we see similar results comparing offline
SARSA and TD-learning on a gridworld domain where we can access the exact functional form of
the behavior policy in Appendix A.6.2. All of the methods so far estimate Qπβ using different target
value estimators. We also train Q-learning, which chooses the action a0 to maximize the learned Q-
function. While Q-learning learns a different Q-function, we can still compare the relative stability
of these methods to gain intuition about the learning dynamics. In addition to feature dot products
φ(s, a)>φ(s0, a0), we also track the average prediction of the Q-network over the dataset to measure
whether the predictions diverge or are stable in expectation.
Observing feature co-adaptation empirically. As shown in Figure 1 (right), the average dot prod-
uct (top row) between features at consecutive state-action tuples continuously increases for both
Q-learning and TD-learning (after enough gradient steps), whereas it flatlines and converges to a
3
Published as a conference paper at ICLR 2022
small value for supervised regression. We might at first think that this is simply a case of Q-learning
failing to converge. However, the bottom row shows that the average Q-values do in fact converge
to a stable value. Despite this, the optimizer drives the network towards higher feature dot products.
There is no explicit term in the TD error objective that encourages this behavior, indicating the pres-
ence of some implicit regularization phenomenon. This implicit preference towards maximizing the
dot products of features at consecutive state-action tuples is what we call “feature co-adaptation.”
When does feature co-adaptation emerge? Observe in Figure 1 (right) that the feature dot products
for offline SARSA converge quickly and are relatively flat, similarly to supervised regression. This
indicates that utilizing a bootstrapped update alone is not responsible for the increasing dot-products
and instability, because while offline SARSA uses backups, it behaves similarly to supervised MC
regression. Unlike offline SARSA, feature co-adaptation emerges for TD-learning, which is sur-
prising as TD-learning also aims to estimate the value of the behavior policy, and hence should
match offline SARSA in expectation. The key difference is that while offline SARSA always uti-
lizes actions a0 observed in the training dataset for the backup, TD-learning may utilize potentially
unseen actions a0 in the backup, even though these actions a0 〜∏β(∙∣S0) are within the distribution
of the data-generating policy. This suggests that utilizing out-of-sample actions in the Bellman
backup, even when they are not out-of-distribution, critically alters the learning dynamics. This is
distinct from the more common observation in offline RL, which attributes training challenges to
out-of-distribution actions [29], but not out-of-sample actions. The theoretical model developed in
Section 3.2 will provide an explanation for this observation with a discussion about how feature
co-adaption caused due to out-of-sample actions can be detrimental in offline RL.
3.2	Theoretically Characterizing Implicit Regularization in TD-Learning
Why does feature co-adaptation emerge in TD-learning and what do out-of-sample actions have to
do with it? To answer this question, we theoretically characterize the implicit regularization effects
in TD-learning. We analyze the learning dynamics of TD learning in the overparameterized regime,
where there are many different parameter vectors θ that fully minimize the training set temporal
difference error. We base our analysis of TD learning on the analysis of implicit regularization in
supervised learning, previously developed by Blanc et al. [6], Damian et al. [11].
Background. When training an overparameterized fθ (x) via supervised regression using the
squared loss, denoted by L, many different values of θ will satisfy L(θ) = 0 on the training set
due to overparameterization, but Blanc et al. [6] show that the dynamics of stochastic gradient de-
scent will only find fixed points θ* that additionally satisfy a condition which can be expressed as
NθR(θ*) = 0, along certain directions (that We will describe shortly). This function R(θ) is referred
to as the implicit regularizer. The noisy gradient updates analyzed in this model have the form:
θk +1 . θk - nNθL(θ) + ηεk , εk ~ N(0,M) .	⑵
Blanc et al. [6] and Damian et al. [11] show that some common SGD techniques fall into this frame-
work, for example, when the regression targets in supervised learning are corrupted with N(0, 1)
label noise, then the resulting M = P|iD=|1 Nθfθ (xi)Nθfθ (xi)> and the induced implicit regular-
izer R is given by R(θ) = PD1 ∣∣Nθfθ(xi)∣∣2. Any solution θ* found by Equation 2 must satisfy
ΝθR(θ*) = 0 along directions V ∈ Rlθl which lie in the null space of the Hessian of the loss
N2L(θ*) at θ*, v ∈ Null(N2L(θ*)). The intuition behind the implicit regularization effect is that
along such directions in the parameter space, the Hessian is unable to contract θk when running
noisy gradient updates (Equation 2). Therefore, the only condition that the noisy gradient updates
converge/stabilize at θ* is given by the condition that NR(θ*) = 0. This model corroborates empir-
ical findings [34, 11] about the solutions found by SGD, which motivates our use.
Our setup. Following this framework, we analyze the fixed points of noisy TD-learning. We
consider noisy pseudo-gradient (or semi-gradient) TD updates with a general noise covariance M :
θk+ι = θk - η (X RθQ(Si, ai) (Qθ(Si, ai)-(ri+γQθ(Si ai)))) +*k, εk 〜N(0, M) (3)
|----------------------------------------V------------------------}
:=g(θ)
We use a deterministic policy a0i = π(S0i) to simplify exposition. Following Damian et al. [11], we
can set the noise model M as M = Pi NθQ(Si, ai)NθQ(Si, ai)>, or utilize a different choice of
4
Published as a conference paper at ICLR 2022
M, but We will derive the general form first. Let θ* denote a stationary point of the training TD
error, such that the pseudo-gradient g(θ*) = 0. Further, we denote the derivative of g(θ) w.r.t. θ as
the matrix G (θ) ∈ R 网 × ⑹，and refer to it as the Pseudo-Hessian: although G (θ) is not actually the
second derivative of any well-defined objective, since TD updates are not proper gradient updates,
as we will see it will play a similar role to the Hessian in gradient descent. For brevity, define
G = G(θ^), g = g(θ^), yG = RaG(θ^) ∈ R∣θ∣×∣θ∣×∣θ∣, and let %(P) denote the i-th eigenvalue
of matrix P, when arranged in decreasing order of its (complex) magnitude ∣λi (P) ∣ (note that an
eigenvalue can be complex).
Assumptions. To simplify analysis, we assume that matrices G and M (i.e., the noise covariance
matrix) span the same n-dimensional basis in d-dimensional space, where d is the number of param-
eters and n is the number of datapoints, and n d due to overparameterization. We also require
θ* to satisfy a technical criterion that requires approximate alignment between the eigenspaces of G
and the gradient of the Q-function, without which noisy TD may not be stable at θ*. We summarize
all the assumptions in Appendix C, and present the resulting regularizer below.
Theorem 3.1 (Implicit regularizer at TD fixed points). Under the assumPtions so far, a fixed Point
of TD-learning, θ*, where Qa* (si, ai) = r + γQa*(si, ai) for every (si, ai, Si) ∈ D is stable
(atttractive) if: (1) it satisfies Re(λ%(G)) ≥ 0, ∀i and Re(λ%(G)) > 0 if ∣Imag(λ%(G)) | > 0, and (2)
along directions V ∈ Rdim(θ), V ∈ Null(G), θ* is the stationary point ofthe implicit regularizer:
|D|	|D|
RTD(θ)= η E RQθ(Si, ai)>∑MRQθ(Si, ai) -ηγ E tr ([[RQθ(si, ai)>]] > ∑MRQθ(si, ai)),
i=1	i=1
I	I I	I
implicit regularizer for noisy GD in supervised learning	additional term in TD learning
(4)
where (Si, ai) and (Sii, aii) denote state-action pairs that appear together in a Bellman update, [[]]
denotes the stop-gradient function, which does not pass partial gradients w.r.t. θ into □. Σ M is the
fixed point ofthe discrete Lyapunov equation: Σ M :=( I — ηG )Σ M (I — ηG) > + η2 M.
A proof of Theorem 3.1 is provided in Appendix C. Next, we explain the intuition behind this result
and provide a proof sketch. To derive the induced implicit regularizer for a stable fixed point θ*
of TD error, we study the learning dynamics of noisy TD learning (Equation 3) initialized at θ*,
and derive conditions under which this noisy update would stay close to θ* with multiple updates.
This gives rise to the two conditions shown in Theorem 3.1 which can be understood as controlling
stability in mutually exclusive directions in the parameter space. If condition (1) is not satisfied, then
even under-parameterized TD will diverge away from θ*, since I — ηG would be a non-contraction
as the spectral radius, P(I — ηG) ≥ 1 in that case. Thus, θk — θ* will grow or not decrease in some
direction. When (1) is satisfied for all directions in the parameter space, there are still directions
where both the real and imaginary parts of the eigenvalue λi(G) are 0 due to overparameterization1.
In such directions, learning is governed by the projection of the noise under the tensor RG, which
appears in the Taylor expansion of θk — θ* around the point θ*:
θk+1 =	θk	—仆 +	G(θk	— θ*) +	2VG[θk	—	θ*, θk — θ*f) +	εk,	εk 〜N(0,	M) (5)
η
^⇒ Vk + 1 = (I — ηG) Vk — 2 VG [ Vk, Vk ] + εk,	(6)
where we reparameterize in terms of Vk := θk — θ*. The proof shows that θ* is stable if it is a
stationary point of the implicit regularizer RTD (condition (2)), which ensures that total noise (i.e.,
accumulated εk over iterations k) accumulated by VG does not lead to a large deviation in Vk in
directions where I — ηG does not contract.
Interpretation of Theorem 3.1. While the choice of the noise model M will change the form of the
implicit regularizer, in practice, the form of M is not known as this corresponds to the noise induced
via SGD. We can consider choices of M for interpretation, but Theorem 3.1 is easy to qualitatively
interpret for M such that ΣM = I. In this case, we find that the implicit preference towards local
minima of RTD(θ) can explain feature co-adaptation. In this case, the regularizer is simpler:
Rtd(θ) := X l∣VQθ(si, ai)||2 — γVQθ(si, ai)V[[Qθ(si, ai)]]∙
i
1To see why this is the case, note that rank(G) ≤ |D|	dim(θ), and so some eigenvalues of G are 0.
5
Published as a conference paper at ICLR 2022
Figure 2: Even when current offline RL algorithms are initialized at a high-performing checkpoint that attains
small feature dot products, feature dot products increase with further training and the performance degrades.
The first term is equal to the squared per-datapoint gradient norm, which is same as the implicit
regularizer in supervised learning obtained by Blanc et al. [6], Damian et al. [11] with label
noise. However, RTD (θ) additionally includes a second term that is equal to the dot product of
the gradient of the Q-function at the current and next states, Nθ Qθ (Si, ai) > Nθ Qθ (Si, ai), and
thus this term is effectively maximized. When restricted to the last-layer parameters of a neural
network, this term is equal to the dot product of the features at consecutive state-action tuples:
Pi NθQθ(Si,ai)>NθQθ(S0i,a0i) = Pi φ(Si,ai)>φ(S0i,a0i). The tendency to maximize this quan-
tity to attain a local minimizer of the implicit regularizer corroborates the empirical findings of
increased dot product in Section 3.1.
Explaining the difference between utilizing seen and unseen actions in the backup. If all state-
action pairs (S0i, a0i) appearing on the right-hand-side of the Bellman update also appear in the dataset
D, as in the case of offline SARSA (Figure 1), the preference to increase dot products will be
balanced by the affinity to reduce gradient norm (first term of RTD (θ) when ΣM = I): for ex-
ample, for offline SARSA, when (S0i , a0i) are permutations of (Si , ai), RTD is lower bounded by
(1 — Y) ∑2i l∣NθQθ(Xi)||2 and hence minimizing RTD(θ) would minimize the feature norm instead
of maximizing dot products. This also corresponds to the implicit regularizer we would obtain when
training Q-functions via supervised learning and hence, our analysis predicts that offline SARSA
with in-sample actions (i.e., when (S0 , a0) ∈ D) would behave similarly to supervised regression.
However, the regularizer behaves very differently when unseen state-action pairs (S0i , a0i) appear
only on the right-hand-side of the backup. This happens with any algorithm where a0 is not the
dataset action, which is the case for all deep RL algorithms that compute target values by selecting
a0 according to the current policy. In this case, we expect the dot product of gradients at (S, a) and
(S0 , a0) to be large at any attractive fixed point, since this minimizes RTD (θ). This is precisely a
form of co-adaptation: gradients at out-of-sample state-action tuples are highly similar to gradients
at observed state-action pairs measured by the dot product. This observation is also supported by
the analysis in Section 3.1. Finally, note that the choice of M is a modelling assumption, and to
derive our explicit regularizer, later in the paper, we will make a simplifying choice of M . However,
we also empirically verify that a different choice of M, given by label noise, works well.
Why is implicit regularization detrimental to policy performance? To answer this question,
we present theoretical and empirical evidence that illustrates the adverse effects of this implicit
regularizer. Empirically, we ran two algorithms, DQN and CQL, initialized from a high-performing
Q-function checkpoint, which attains relatively small feature dot products (i.e., the second term of
RTD (θ) is small). Our goal is to see if TD updates starting from such a “good” initialization still
stay around it or diverge to poorer solutions. Our theoretical analysis in Section 3.2 would predict
that TD learning would destabilize from such a solution, since it would not be a stable fixed point.
Indeed, as shown in Figure 2, the policy immediately degrades, and the the dot-product similarities
start to increase. This even happens with CQL, which explicitly corrects for distributional shift
confounds, implying that the performance drop cannot be directly explained by the typical out-of-
distribution action explanations. To investigate the reasons behind this drop, we also measured the
training loss function values for these algorithms (i.e., TD error for DQN and TD error + CQL
regularizer for CQL) and find in Figure 2 that the loss values are generally small for both CQL and
DQN. This indicates that the preference to increase dot products is not explained by an inability to
minimize TD error. In Appendix A.7, we show that this drop in performance when starting from
good solutions can be effectively mitigated with our proposed DR3 explicit regularizer for both
DQN and CQL. Thus we find that not only standard TD learning degrades from a good solution in
favor of increasing feature dot products, but keeping small dot products enables these algorithms to
remain stable near the good solution.
6
Published as a conference paper at ICLR 2022
To motivate why co-adapted features can lead to poor performance in TD-learning, we study the
convergence of linear TD-learning on co-adapted features. Our theoretical result characterizes a
lower bound on the feature dot products in terms of the feature norms for state-action pairs in the
dataset D, which if satisfied, will inhibit convergence:
Proposition 3.2 (TD-learning on co-adapted features). Assume that the features Φ = [φ(s, a)]s,a
are UsedfOr linear TD-learning. Then, if Es a So∈v φ(s, a)>φ(s0, a0) ≥ γ Es a∈v φ(s, a)>φ(s, a),
linear TD-learning using features Φ will not converge.
A proof of Proposition 3.2 is provided in Appendix D and it relies on a stability analysis of linear TD.
While features change during training for TD-learning with neural networks, and arguably linear TD
is a simple model to study consequences of co-adapted features, even in this simple linear setting,
Proposition 3.2 indicates that TD-learning may be non-convergent as a result of co-adaptation.
4	DR3: Explicit Regularization for Deep TD -Learning
Since the implicit regularization effects in TD-learning can lead to feature co-adaptation, which in
turn is correlated with poor performance, can we instead derive an explicit regularizer to alleviate this
issue? Inspired by the analysis in the previous section, we will propose an explicit regularizer that at-
tempts to counteract the second term in Equation 4, which would otherwise lead to co-adaptation and
poor representations. The explicit regularizer that offsets the difference between the two implicit reg-
Ularizers is given by: ∆(θ) = P - trace [∑ M>^θ Qθ (s i, a i) N § Qθ (s i, a i) >], which represents the
second term of RTD (θ). Note that we drop the stop gradient on Qθ (s0i, a0i) in ∆(θ), as it performs
slightly better in practice (Table A.1), althoUgh as shown in that Table, the version with the stop
gradient also significantly improves over the base method. The first term of RTD (θ) corresponds to
the regUlarizer from sUpervised learning. OUr proposed method, DR3, simply combines approxima-
tions to ∆(θ) with varioUs offline RL algorithms. For any offline RL algorithm, ALG, with objective
LALG(θ), the training objective with DR3 is given by: L(θ) := LALG(θ) + c0∆(θ), where c0 is the
DR3 coefficient. See Appendix E.3 for details on how we tUne c0 in this paper.
Practical version of DR3. In order to practically instantiate DR3, we need to choose a particUlar
noise model M . In general, it is not possible to know beforehand the “correct” choice of M (EqUa-
tion 3), even in sUpervised learning, as this is a complicated fUnction of the data distribUtion, neUral
network architectUre and initialization. Therefore, we instantiate DR3 with two heUristic choices of
M : (i) M indUced by label noise stUdied in prior work for sUpervised learning and for which we
need to rUn a compUtationally heavy fixed-point compUtation for M, and (ii) a simpler alternative
that sets ΣM = I. We find that both of these variants generally perform well empirically (Figure 6),
and improve over the base offline RL method, and so we Utilize (ii) in practice dUe to low compU-
tational costs. Additionally, because computing and backpropagating through per-example gradient
dot products is slow, we instead approximate ∆(θ) with the contribution only from the last layer
parameters (i.e., Pi NwQθ(si, ai)>NwQθ(s0i, a0i)), similarly to tractable Bayesian neural nets. As
shown in Appendix A.5, the practical version of DR3 performs similarly to the label-noise version.
Explicit DR3 regularizer :	R exp(θ) = X φ (s -, a -) > φ (s i, a i).	(7)
i∈D
5	Experimental Evaluation of DR3
Our experiments aim to evaluate the extent to which DR3 improves performance in offline RL in
practice, and to study its effect on prior observations of rank collapse. To this end, we investigate
if DR3 improves offline RL performance and stability on three offline RL benchmarks: Atari 2600
games with discrete actions [2], continuous control tasks from D4RL [18], and image-based robotic
manipulation tasks [38]. Following prior work [18, 22], we evaluate DR3 in terms of final offline
RL performance after a given number of iterations. Additionally, we report training stability, which
is important in practice as offline RL does not admit cheap validation of trained policies for model
selection. To evaluate stability, we train for a large number of gradient steps (2-3x longer than
prior work) and either report the average performance over the course of training or the final
performance at the end of training. We expect that a stable method that does not unlearn with more
gradient steps, should have better average performance, as compared to a method that attains good
peak performance but degrades with more training. See Appendix E for further details.
Offline RL on Atari 2600 games. We compare DR3 to prior offline RL methods on a set of offline
Atari datasets of varying sizes and quality, akin to Agarwal et al. [2], Kumar et al. [28]. We evaluated
7
Published as a conference paper at ICLR 2022
Table 1: IQM normalized average performance (training stability) across 17 games, with 95% CIs in parenthe-
sis, after 6.5M gradient steps for the 1% setting and 12.5M gradient steps for the 5%, 10% settings. Individual
performances reported in Tables F.4-F.9. DR3 improves the stability over both CQL and REM.
Data	CQL	CQL + DR3	REM	REM + DR3
1%	43.7 (39.6, 48.6)	56.9 (52.5,61.2)	4.0 (3.3, 4.8)	16.5 (14.5, 18.6)
5%	78.1 (74.5, 82.4)	105.7 (101.9,110.9)	25.9 (23.4, 28.8)	60.2 (55.8, 65.1)
10%	59.3 (56.4, 61.9)	65.8 (63.3, 68.3)	53.3 (51.4, 55.3)	73.8 (69.3, 78)
75%
50%
25%
0%
1% Uniform Replay
5% Uniform Replay
90%
60%
30%
0%
-0~50 100 150 200
Gradient Steps (X 62.5K)
10% Initial Replay
0	50 100 150 200
Gradient Steps (x 62.5K)
Figure 3: Performance of DR3 + COG on
two manipulation tasks using only 5% and
25% of the data used by Singh et al. [38]
to make these more challenging. COG +
DR3 outperforms COG in training and at-
tains higher average and final performance.
O~50 100 150 200
Gradient Steps (x 62.5K)
10% Initial Replay
0	50 100 150 200
Gradient Steps (x 62.5K)
Gradient Steps (X 62.5K)
Figure 4: Normalized performance across 17 Atari games
for REM + DR3 (top), CQL + DR3 (bottom). x-axis rep-
resents gradient steps; no new data is collected. While naive
REM suffers from a degradation in performance with more
training, REM + DR3 not only remains generally stable with
more training, but also attains higher final performance. CQL
+ DR3 attains higher performance than CQL. We report IQM
with 95% stratified bootstrap CIs [3].
on three datasets: (1) 1% and 5% samples drawn uniformly at random from DQN replay; (2) a
dataset with more suboptimal data consisting of the first 10% samples observed by an online DQN.
Following Agarwal et al. [3], we report the interquartile mean (IQM) normalized scores across 17
games over the course of training in Figure 4 and report the IQM average performance in Table 1.
Observe that combining DR3 with modern offline RL methods (CQL, REM) attains the best final and
average performance across the 17 Atari games tested on, directly improving upon prior methods
across all the datasets. When DR3 is used in conjunction with REM, it prevents severe unlearning
and performance degradation with more training. CQL + DR3 improves by 20% over CQL on
final performance and attains 25% better average performance. While DR3 is not unequivocally
“stable”, as its performance also degrades relative to the peak it achieves (Figure 4), it is more stable
relative to base offline RL algorithms. We also compare DR3 to the srank(Φ) penalty proposed
to counter rank collapse [28]. Directly taking median normalized score improvements reported by
Kumar et al. [28], CQL + DR3 improves by over 2x (31.5%) over naive CQL relative to the srank
penalty (14.1%), indicating DR3,s efficacy.
Offline RL on robotic manipulation from images.
Next, we aim to evaluate the efficacy of DR3 on two
image-based robotic manipulation tasks [38] (visual-
ized on the right) that require composition of skills (e.g.,
opening a drawer, closing a drawer, picking an obstruc-
tive object, placing an object, etc.) over extended horizons using only a sparse 0-1 reward. As shown
in Figure 3, combining DR3 with COG improves over COG.
Offline RL on D4RL tasks. Finally, we evaluate DR3 in conjunction with CQL on the antmaze-v2
domain in D4RL [18]. To assess if DR3 is stable and able to prevent unlearning that eventually
appears in CQL, we trained CQL+DR3 for 9x longer: 2M and 3M steps with 3x higher learning
rate. This is different from prior works [18] that report performance at the end of 1M steps. Observe
in Table 2, that CQL + DR3 outperforms CQL (statistical significance shown in Appendix A.8),
indicating that DR3 significantly improves CQL. We also evalute DR3 on kitchen domains in D4RL
in Appendix A.8, where we also find that DR3 improves CQL. Finally, we also compare CQL+DR3
and CQL in terms of performance and stability on MuJoCo tasks previously studied in Kumar et al.
8
Published as a conference paper at ICLR 2022
Table 2: Performance of CQL, CQL + DR3 after 2M and 3M gradient steps with a learning rate of
3e-4 for the Q-function averaged over 3 seeds. This is training for 6x and 9x longer compared to CQL defaults.
Observe that CQL + DR3 outperforms CQL at 2M and 3M steps, indicating is efficacy in preventing unlearning.
We present the statistical significance of these results in Appendix A.8.
D4RL Task	∣	CQL (2M)	CQL + DR3 (2M) ∣	I CQL (3M)	CQL + DR3 (3M)
antmaze-umaze-v2	84.00 ± 2.67	85.33 ± 4.16	87.00 ± 1.73	90.00 ± 4.00
antmaze-umaze-diverse-v2	45.67 ± 8.50	40.67 ± 11.84	36.33 ± 7.09	52.00 ± 11.26
antmaze-medium-play-v2	24.00 ± 28.16	73.00 ± 4.00	16.00 ± 26.85	71.33 ± 1.52
antmaze-medium-diverse-v2	32.67 ± 9.29	67.00 ± 2.00	48.33 ± 6.11	61.67 ± 3.21
antmaze-large-play-v2	3.33 ± 2.51	28.00 ± 4.35	0.33 ± 0.57	26.33 ± 11.93
antmazeTarge-diverse-v2	1.33 ± 2.30	25.67 ± 0.57	0.00 ± 0.00	28.33 ± 1.52
[28] in Appendix A.3. These tasks are constructed by uniformly subsampling transitions from the
full-replay-v2 MuJoCo datasets in D4RL and are much harder than the typical Gym-MuJoCo tasks
from Fu et al. [18] because succeeding on these tasks critically relies on estimating accurate Q-
values for out-of-sample actions and all actions at certain states are out-of-sample. As shown in
Appendix A.3, CQL+DR3 is significantly more stable, and does not unlearn with more training,
unlike CQL whose performance degrades very quickly. We also evaluate DR3 in conjunction with
BRAC [49], a policy constraint method, and find that BRAC+DR3 improves over BRAC in 13.8
median normalized performance (Table F.2).
DR3 does not suffer from rank collapse. Prior
work [28] has shown that implicit regularization
can lead to a rank collapse issue in TD-learning,
preventing Q-networks from using full capacity.
To see if DR3 addresses the rank collapse issue,
we follow Kumar et al. [28] and plot the effec-
Asterix	Seaquest
Figure 5: Trend of effective rank, srank(Φ) of
features Φ learned by the Q-function When trained
With TD error (red, “Without DR3”) and With TD er-
ror + DR3 (blue, “With DR3”) on three Atari games
using the 5% dataset. Note that DR3 alleviates rank
collapse, Without explicitly aiming to.
Breakout
500
450
400
350
300
250
200
150
Gradh
tive rank of learned features with DR3 in Figure 5
(DQN, REM in Appendix A.4). While the value
of the effective rank decreases during training with
naive bootstrapping, We find that rank of DR3 fea-
tures typically does not collapse, despite no ex-
plicit term encouraging this. Finally, We test the
robustness/sensitivity of each layer in the learned Q-netWork to re-initialization [52] during training
and find that DR3 alters the features to behave similarly to supervised learning (Figure A.2).
Comparing explicit regularizers for different
choices of noise covariance M. Finally, We inves-
tigate the behavior of different implicit regularizers
derived via tWo choices of M in Equation 4 and the
corresponding explicit regularizers. While the ex-
plicit regularizer We use in practice is a simplifying
choice that Works Well, another choice of M is the
covariance matrix induced by label noise, Which re-
quires explicit computation of ΣM. Observe in Fig-
ure 6 that the explicit regularizer for our simplify-
ing choice is not Worse than the different choice of
M . This justifies utilizing our simplified, heuristic
choice of setting ΣM = I in practice. Results on
five Atari games are shoWn in Appendix A.5.
Discussion. We characterized the implicit prefer-
Figure 6: Comparing DR3 regularizers for our
simplifying choice of M and M induced by la-
bel noise, With base CQL and DQN algorithms.
Note that both of these penalties When applied
over CQL improve performance.
ence of TD-learning toWards solutions that maximally co-adapt gradients (or features) at consecutive
state-action tuples that appear in Bellman backup. This regularization effect is exacerbated When
out-of-sample state-action samples are used for the Bellman backup and it can lead to poor policy
performance. Inspired by the theory, We propose a practical explicit regularizer, DR3, that yields
substantial improvements in stability and performance on a Wide range of offline RL problems. We
believe that understanding the learning dynamics of deep Q-learning Will lead to more robust and
stable deep RL algorithms and enable predicting such instability issues, Well in advance, Which
can inspire cross-validation and model selection strategies. This is an important, open challenge in
offline RL, for Which existing off-policy evaluation techniques are not practically sufficient [19].
9
Published as a conference paper at ICLR 2022
Acknowledgements
We thank Dibya Ghosh, Xinyang Geng, Dale Schuurmans, Marc Bellemare, Pablo Castro and Ofir
Nachum for informative discussions, discussions on experimental setup and for providing feedback
on an early version of this paper. We thank the members of RAIL at UC Berkeley for their support
and suggestions. We thank anonymous reviewers for feedback on an early version of this paper.
This research is funded in part by the DARPA Assured Autonomy Program and in part, by compute
resources from Microsoft Azure and Google Cloud. TM acknowledges support of Google Faculty
Award, NSF IIS 2045685, the Sloan fellowship, and JD.com.
References
[1]	Joshua Achiam, Ethan Knight, and Pieter Abbeel. Towards characterizing divergence in deep
q-learning. arXiv preprint arXiv:1903.08894, 2019.
[2]	Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on
offline reinforcement learning. In International Conference on Machine Learning (ICML),
2020.
[3]	Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G Belle-
mare. Deep reinforcement learning at the edge of the statistical precipice. Advances in Neural
Information Processing Systems, 2021.
[4]	Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. arXiv preprint arXiv:1802.06509, 2018.
[5]	Emmanuel Bengio, Joelle Pineau, and Doina Precup. Interference and generalization in tem-
poral difference learning. arXiv preprint arXiv:2003.06350, 2020.
[6]	Guy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant. Implicit regularization for deep
neural networks driven by an ornstein-uhlenbeck like process. In Conference on learning
theory, pp. 483-513. PMLR, 2020.
[7]	Xavier Bouthillier, Pierre Delaunay, Mirko Bronzi, Assya Trofimov, Brennan Nichyporuk,
Justin Szeto, Nazanin Mohammadi Sepahvand, Edward Raff, Kanika Madan, Vikram Voleti,
et al. Accounting for variance in machine learning benchmarks. Proceedings of Machine
Learning and Systems, 3, 2021.
[8]	Niladri S Chatterji, Behnam Neyshabur, and Hanie Sedghi. The intriguing role of module
criticality in the generalization of deep networks. arXiv preprint arXiv:1912.00528, 2019.
[9]	Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learn-
ing. ICML, 2019.
[10]	Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. arXiv
preprint arXiv:2011.10566, 2020.
[11]	Alex Damian, Tengyu Ma, and Jason Lee. Label noise sgd provably prefers flat global mini-
mizers. arXiv preprint arXiv:2106.06530, 2021.
[12]	Daniela Pucci De Farias. The linear programming approach to approximate dynamic program-
ming: Theory and application. PhD thesis, 2002.
[13]	Yaqi Duan, Zeyu Jia, and Mengdi Wang. Minimax-optimal off-policy evaluation with linear
function approximation. In International Conference on Machine Learning, pp. 2701-2709.
PMLR, 2020.
[14]	Ishan Durugkar and Peter Stone. Td learning with constrained gradients. 2018.
[15]	Amir-massoud Farahmand, Csaba SzePesvðri, and Remi Munos. Error propagation for ap-
proximate policy and value iteration. In Advances in Neural Information Processing Systems
(NIPS), 2010.
10
Published as a conference paper at ICLR 2022
[16]	William Fedus, Prajit Ramachandran, Rishabh Agarwal, Yoshua Bengio, Hugo Larochelle,
Mark Rowland, and Will Dabney. Revisiting fundamentals of experience replay. arXiv preprint
arXiv:2007.06700, 2020.
[17]	Justin Fu, Aviral Kumar, Matthew Soh, and Sergey Levine. Diagnosing bottlenecks in deep
Q-learning algorithms. arXiv preprint arXiv:1902.10250, 2019.
[18]	Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for
deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.
[19]	Justin Fu, Mohammad Norouzi, Ofir Nachum, George Tucker, ziyu wang, Alexander Novikov,
Mengjiao Yang, Michael R Zhang, Yutian Chen, Aviral Kumar, Cosmin Paduraru, Sergey
Levine, and Thomas Paine. Benchmarks for deep off-policy evaluation. In International Con-
ference on Learning Representations, 2021. URL https://openreview.net/forum?
id=kWSeGEeHvF8.
[20]	Dibya Ghosh and Marc G Bellemare. Representations for stable off-policy reinforcement
learning. arXiv preprint arXiv:2007.05520, 2020.
[21]	Jean-Bastien Grill, Florian Strub, Florent Altch6, Corentin Tallec, Pierre H Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Ghesh-
laghi Azar, et al. Bootstrap your own latent: Anew approach to self-supervised learning. arXiv
preprint arXiv:2006.07733, 2020.
[22]	Caglar Gulcehre, ZiyU Wang, Alexander Novikov, Tom Le Paine, Sergio G6mez Colmenarejo,
Konrad Zolna, Rishabh Agarwal, Josh Merel, Daniel Mankowitz, Cosmin Paduraru, et al. Rl
unplugged: Benchmarks for offline reinforcement learning. 2020.
[23]	Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati
Srebro. Implicit regularization in matrix factorization. In Advances in Neural Information
Processing Systems, pp. 6151-6159, 2017.
[24]	Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic:
Off-policy maximum entropy deep reinforcement learning with a stochastic actor. CoRR,
abs/1801.01290, 2018. URL http://arxiv.org/abs/1801.01290.
[25]	Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in Neural Information Processing Systems 31.
2018.
[26]	Aviral Kumar, Abhishek Gupta, and Sergey Levine. Discor: Corrective feedback in reinforce-
ment learning via distribution correction. arXiv preprint arXiv:2003.07305, 2020.
[27]	Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for
offline reinforcement learning. arXiv preprint arXiv:2006.04779, 2020.
[28]	Aviral Kumar, Rishabh Agarwal, Dibya Ghosh, and Sergey Levine. Implicit under-
parameterization inhibits data-efficient deep reinforcement learning. In International Confer-
ence on Learning Representations, 2021. URL https://openreview.net/forum?
id=O9bnihsFfXU.
[29]	Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning:
Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
[30]	Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial
large learning rate in training neural networks. In Advances in Neural Information Processing
Systems, pp. 11674-11685, 2019.
[31]	Hamid R. Maei, Csaba Szepesvdri, Shalabh Bhatnagar, Doina Precup, David Silver, and
Richard S. Sutton. Convergent temporal-difference learning with arbitrary smooth function
approximation. In Proceedings of the 22nd International Conference on Neural Information
Processing Systems, 2009.
11
Published as a conference paper at ICLR 2022
[32]	A Rupam Mahmood, Huizhen Yu, Martha White, and Richard S Sutton. Emphatic temporal-
difference learning. arXiv preprint arXiv:1507.01569, 2015.
[33]	Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G
Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Pe-
tersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran,
Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep rein-
forcement learning. Nature, 518(7540):529-533, feb 2015. ISSN 0028-0836.
[34]	Rotem Mulayoff and Tomer Michaeli. Unique properties of flat minima in deep networks. In
International Conference on Machine Learning, pp. 7108-7118. PMLR, 2020.
[35]	Tobias Pohlen, Bilal Piot, Todd Hester, Mohammad Gheshlaghi Azar, Dan Horgan, David
Budden, Gabriel Barth-Maron, Hado Van Hasselt, John Quan, Mel Vecer⅛ et al. Observe and
look further: Achieving consistent performance on atari. arXiv preprint arXiv:1805.11593,
2018.
[36]	Martin L Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming.
John Wiley & Sons, Inc., 1994.
[37]	Martin Riedmiller. Neural fitted q iteration-first experiences with a data efficient neural re-
inforcement learning method. In European Conference on Machine Learning, pp. 317-328.
Springer, 2005.
[38]	Avi Singh, Albert Yu, Jonathan Yang, Jesse Zhang, Aviral Kumar, and Sergey Levine. Cog:
Connecting new skills to past experience with offline reinforcement learning. arXiv preprint
arXiv:2010.14500, 2020.
[39]	Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. Second
edition, 2018.
[40]	Richard S. Sutton, A. Rupam Mahmood, and Martha White. An emphatic approach to the
problem of off-policy temporal-difference learning. J. Mach. Learn. Res., 17(1):26032631,
January 2016. ISSN 1532-4435.
[41]	Yuandong Tian, Lantao Yu, Xinlei Chen, and Surya Ganguli. Understanding self-supervised
learning with dual deep networks. arXiv preprint arXiv:2010.00578, 2020.
[42]	Yuandong Tian, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning dy-
namics without contrastive pairs. arXiv preprint arXiv:2102.06810, 2021.
[43]	Hado van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, and Joseph
Modayil. Deep reinforcement learning and the deadly triad. ArXiv, abs/1812.02648, 2018.
[44]	Hado Van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, and Joseph
Modayil. Deep reinforcement learning and the deadly triad. arXiv preprint arXiv:1812.02648,
2018.
[45]	Ruosong Wang, Dean Foster, and Sham M. Kakade. What are the statistical limits of offline {rl}
with linear function approximation? In International Conference on Learning Representations,
2021. URL https://openreview.net/forum?id=30EvkP2aQLD.
[46]	Ruosong Wang, Yifan Wu, Ruslan Salakhutdinov, and Sham M Kakade. Instabilities of offline
rl with pre-trained neural representation. arXiv preprint arXiv:2103.04947, 2021.
[47]	Colin Wei, Jason Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and
optimization of neural nets vs their induced kernel. 2019.
[48]	Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay
Golan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models.
arXiv preprint arXiv:2002.09277, 2020.
[49]	Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learn-
ing. arXiv preprint arXiv:1911.11361, 2019.
12
Published as a conference paper at ICLR 2022
[50]	Tengyang Xie and Nan Jiang. Q* approximation schemes for batch reinforcement learning: A
eoretical comparison. 2020.
[51]	Andrea Zanette. Exponential lower bounds for batch reinforcement learning: Batch rl can be
exponentially harder than online rl. arXiv preprint arXiv:2012.08005, 2020.
[52]	Chiyuan Zhang, Samy Bengio, and Yoram Singer. Are all layers created equal? arXiv preprint
arXiv:1902.01996, 2019.
13
Published as a conference paper at ICLR 2022
Appendices
A Additional Visualizations and Experiments for DR3
In this section, we provide visualizations and diagnostic experiments evaluating various aspects of
feature co-adaptation and the DR3 regularizer. We first provide more empirical evidence showing
the presence of feature co-adaptation in modern deep offline RL algorithms. We will also visualize
DR3 inspired from the implicit regularizer term in TD-learning alleviates rank collapse discussed
in Kumar et al. [28]. We will compare the efficacies of the explicit regularizer induced for different
choices of the noise covariance matrix M (Equation 4), understand the effect of dropping the stop
gradient term in ou practical regularizer and finally, perform diagnostic experiments visualizing if the
Q-networks learned with DR3 resemble more like neural networks trained via supervised learning,
measured in terms of sensitivity and robustness to layer reinitialization [52].
A.1 More Empirical Evidence of Feature Co-Adaptation
In this section, we provide more empirical evidence demonstrating the existence of the feature co-
adaptation issue in modern offline RL algorithms such as DQN and CQL. As shown below in Fig-
ure A.1, while the average dataset Q-value for both CQL and DQN exhibit a flatline trend, the
dot product similarity for consecutive state-action tuples generally continues to increase throughout
training and does not flatline. While DQN eventually diverges in Seaquest, the dot products increase
with more gradient steps even before divergence starts to appear.
6000
4000
2000
0
CQL	DQN
50	100	0
50	100	0
50	100
Gradient Updates (X 62.5k) Gradient Updates (X 62.5k) Gradient Updates (X 62.5k) Gradient Updates (X 62.5k) Gradient Updates (X 62.5k)
CQL	DQN
Figure A.1: Demonstrating feature co-adaptation on five Atari games with standard offline DQN and
CQL, averaged over 3 seeds. Observe that the feature dot products continue to rise with more training for
both CQL and DQN, indicating the presence of co-adaptation. On the other hand, the average Q-values exhibit
a converged trend, except on Seaquest. Further, note that the dot products continue to increase for CQL even
though CQL explicitly corrects for out-of-distribution action inputs.
A.2 Layer-wise structure of a Q-network trained with DR3
To understand if DR3 indeed makes Q-networks behave as if they were trained via supervised learn-
ing, utilizing the empirical analysis tools from Zhang et al. [52], we test the robustness/sensitivity
of each layer in the learned network to re-initialization, while keeping the other layers fixed. This
tests if a particular layer is critical to the predictions of the learned neural network and enables us to
reason about generalization properties [52, 8]. We ran CQL and REM and saved all the intermediate
checkpoints. Then, as shown in Figure A.2, we first loaded a checkpoint (x-axis), and computed
policy performance (shaded color; colorbar) by re-initializing a given layer (y-axis) of the network
to its initialization value before training for the same run.
Note in Figure A.2, that while almost all layers are absolutely critical for the base CQL algorithm,
utilizing DR3 substantially reduces sensitivity to the latter layers in the Q-network over the course of
training. This is similar to what Zhang et al. [52] observed for supervised learning, where the initial
14
Published as a conference paper at ICLR 2022
1 10 20 30 40 50 60 70 80 901∞
Checkpoint
1 10 20 30 40 50 60 70 80 90100
Checkpoint
Illlllir
III
1 10 20 30 40 50 60 70 80 90100
Checkpoint
8
∖n
Q
+
LU
Performance is
less sensitive to
the last layer.
Figure A.2: CQL vs CQL + DR3 and REM vs REM + DR3. Average robustness of the learned Q-
function to re-initialization of all layers to different checkpoints over the course of training created based on the
protocol from Zhang et al. [52]. The colors in the heatmap indicate performance of the reinitialized checkpoint,
normalized w.r.t. the checkpoint without any change to layers. Note that while CQL and REM are more
sensitive (i.e., less robust) to reinitialization of all the layers especially the last layer, CQL + DR3 and REM +
DR3 behave closer to supervised learning, in the sense that they are more robust to reinitialization of layers of
the network, especially the last layer.
layers of a network were the most critical, and the latter layers primarily performed near-random
transformations without affecting the performance of the network. This indicates that utilizing DR3
alters the internal layers of a Q-network trained with TD to behave closer to supervised learning.
A.3 Results on MuJoCo Domains
In this section, we provide the results of applying DR3 on the MuJoCo tasks shown in Figure A.5.
(Appendix A) of Kumar et al. [28]. To briefly describe the setup, in these tasks we train on the three
gym tasks (Hopper-v2, Ant-v2, Walker2d-v2) using 20% of the offline data, uniformly subsampled
from the run of an online SAC agent, mimicking the setup from Kumar et al. [28]. Rather than
retraining an SAC agent to collect data, we subsampled the Gym-MuJoCo *-full-replay-v2
replay buffers from the latest D4RL [18]. In these cases we plot the srankδ values, the feature dot
products and the corresponding performance values with and without the DR3 regularizer for 4M
steps (Kumar et al. [28] showed their plots for just under 4M steps) in Figure A.3.
Observe in Figure A.3, that while the standard CQL algorithm performs poorly and suffers from
performance degradation within about 1M-1.5M steps for Walker2d and Ant, CQL + DR3 is able
to prevent the performance degradation and trains stably. Base CQL demonstrates oscillatory per-
formance on Hopper, but CQL + DR3 stabilizes the performace of CQL. This indicates that DR3 is
effective on MuJoCo domains, and prevents the instabilities with CQL.
For details, the weight on the CQL regularizer in this case is equal to 5.0 across all the tasks, and
weight on the DR3 regularizer is 0.01. We also attempted to tune the CQL coefficient for the baseline
CQL algorithm within {1.0, 2.0, 5.0, 10.0, 20.0} to see if it address the performance degradation
15
Published as a conference paper at ICLR 2022
hopper-full-replay-v2 (20%)	walker2d-full-replay-v2 (20%)
O IOO 200	300	400	500
Environment Steps (xlθk)
hopper-full-replay-v2
100
50
0
ant-full-replay-vz (20%)
*V∖Λ	ιp⅛Λ⅛*
SaUnPaJd IoP amleelL
0	100	200	300	400	500
Environment Steps (xlθk)
0	100	200	300	400	500
Environment Steps (xlθk)
0	100	200	300	400	500	0	100	200	300	400	500	0	100	200	300	400	500
Environment Steps (xlθk)	Environment Steps (×10k)	Environment Steps (xlθk)
hopper-full-replay-v2 (20%)	walker2d-full-replay-v2 (20%)	ant-full-replay-v2 (20%)
0	100	200	300	400	500
Environment Steps (xlθk)
0	100	200	300	400	500
Environment Steps (xlθk)
0	100	200	300	400	500
Environment Steps (xlθk)

Figure A.3: Comparison of CQL and CQL + DR3 on the offline MuJoCo Gym domains, mimicking
the setup of Kumar et al. [28]. The data is generated by randomly sampling 20% of the transitions of the
D4RL [18] full-replay-v2 datasets, which are collected via the run of an online SAC agent. The performance is
shown in terms of the D4RL normalized score, where 0.0 denotes the performance of a random policy and 100.0
denotes the performance of an expert online SAC policy. Observe that adding DR3 stabilizes the performance
on Hopper, and prevents performance collapse on Walker2d and Ant. In addition note that the srank values
attained by CQL + DR3 is higher than base CQL and more importantly, the feature dot products are much
smaller for CQL + DR3 compared to CQL.
issues, but did not find any difference in the collapsing behavior of base CQL. Our CQL baseline is
therefore well-tuned, and DR3 improves the performance over this baseline.
A.4 Rank Collapse is Alleviated With DR3

3∞
2∞
100-
-~e)jfUE
Without DR3
With DR3
0	.	. V .	V ,	.	■	.
0	50	100	150	200	0	50	100	150	200	0	50	100	150	200	0	50	100 ISO	0	50	100	150	2∞
Gradient Updates (x62.5k) Gradient Updates (x62.5k) Gradient Updates (x62.5k) Gradient Updates (x62.5k) Gradient Updates (x62.5k)
Figure A.4: Comparing the feature ranks for CQL and CQL + DR3. Observe that utilizing DR3 success-
fully alleviates the rank collapse issue noted in prior work without explicitly correcting for it.
Prior work [28] has shown that implicit regularization in TD-learning can lead to a feature rank
collapse phenomenon in the Q-function, which hinders the Q-function from using its full represen-
tational capacity. Such a phenomenon is absent in supervised learning, where the feature rank does
not collapse. Since DR3 is inspired by mitigating the effects of the term in the implicit regular-
izer (Equation 4) that only appears in the case of TD-learning, we wish to understand if utilizing
DR3 also alleviates rank collapse. To do so, we compute the effective rank srankδ (φ) metric of
the features learned by Q-functions trained via CQL and CQL with DR3 explicit regularizer. As
shown in Figure A.4, for the case of five Atari games, utilizing DR3 alleviates the rank collapse
16
Published as a conference paper at ICLR 2022
issue completely (i.e., the ranks do not collapse to very small values when CQL + DR3 is trained for
long). We do not claim that the ranks with DR3 are necessarily higher, and infact as we show below,
a higher srank of features may not always imply a better solution. The fact that DR3 can prevent
rank collapse is potentially surprising, because no term in the practical DR3 regularizer explicitly
aims to increase rank: feature dot products can be made smaller while retaining low ranks by simply
rescaling the feature vectors. But, as we observe, utilizing DR3 enables learning features that do not
exhibit collapsed ranks, thus we hypothesize that correcting for appropriate terms in RTD (θ) can
address some of the previously observed pathologies in TD-learning.
DQN	DQN + DR3
AstenX	Breakout	Q*Bert
80 -I	10000-
50	100	150	200
50	100	150	200
50	100	150	200
Seaquest	Space Invaders
600-
50	100	150	200
Gradient Updates (x 62.5k)
Gradient Updates (x 62.5k)
Gradient Updates (x 62.5k)
Gradient Updates (x 62.5k)
Gradient Updates (x 62.5k)
50	100	150	200
Gradient Updates (x 62.5k)
Figure A.5: Performance and srank values for DQN and DQN + DR3. Observe that the srank values
increase for DQN + DR3, while they collapse for DQN on Asterix, Seaquest and SpaceInvaders with more
training. Thus, DQN + DR3 does not suffer from a sudden rank collapse. However, a higher srank does not
imply a better return, and so while initially DQN does have a high rank, DQN + DR3 performs superiorly.
We now investigate the feature ranks of a Q-network trained when DR3 is applied in conjunction
with a standard DQN and REM [2] on the Atari domains. We plot the values of srankδ(φ), the
feature dot products and the performance of the algorithm for DQN in Figure A.5 and for REM in
Figure A.6. In the case of DQN, we find that unlike the base DQN algorithm for which feature rank
does begin to collapse with more training, the srank for DQN + DR3 is increasing. We also note that
DQN + DR3 attains a better performance compared to DQN, throughout training.
Luns≈ΦCTra⅛><
AsteriX
50	100	150	200
50	100	150	200
REM	REM + DR3
50	100	150	200	50	100	150	200
Gradient Updates (x 62.5k)	Gradient Updates (x 62.5k)
Space Invaders
700-
50	100	150	200
Gradient Updates (x 62.5k)
Gradient Updates (x 62.5k)
Gradient Updates (x 62.5k)
Gradient Updates (x 62.5k)
Gradient Updates (x 62.5k)
Gradient Updates (x 62.5k)
Figure A.6: Comparing the performance and srank values for REM and REM + DR3. Observe that
while REM + DR3 outperforms REM, the srank values attained by REM are much larger than REM + DR3,
and none of these ranks have collapsed. Thus, while REM + DR3 maintains non-collapsed features, for the case
of REM, it reduces the value of srank and attains better performance. This does not contradict the observations
from Kumar et al. [28] as we discuss in the text.
17
Published as a conference paper at ICLR 2022
Table A.1: Normalized interquartile mean performance with 95% stratified bootstrap CIs [3] across 17 Atari
games of REM, REM + ∆0(Φ) (Stop gradient in DR3), REM + DR3 after 6.5M gradient steps for the 1%
setting and 12.5M gradient steps for the 5%, 10% settings. Observe that REM + ∆0(φ) also improves over the
base REM method significantly, by about 130%, even though ∆0 (φ) is generally comparable and somewhat
worse than the DR3 regularizer used in the main paper.
Data	REM	REM + ∆' (Φ)	REM+DR3
1%	4.0 (3.3, 4.8)	15.0 (13.4,16.6)	16.5 (14.5, 18.6)
5%	25.9 (23.4, 28.8)	55.5 (50.8, 59.8)	60.2 (55.8, 65.1)
10%	53.3 (51.4, 55.3)	67.7 (64.7,71.3)	73.8 (69.3, 78)
However, we note that the opposite trend is true for the case of REM: while REM + DR3 attains
a better performance than REM, adding DR3 leads to a reduction in the srank value compared to
base REM. At a first glance, this might seem contradicting Kumar et al. [28], but this is not the
case: to our understanding, Kumar et al. [28] establish a correlation between extremely low rank
values (i.e., rank collapse) and poor performance, but this does not mean that all high rank features
will lead to good performance. We suspect that since REM trains a multi-headed Q-function with
shared features and randomized target values, it is able to preserve high-rank features, but this need
not mean that these features are useful. In fact, as shown in Figure A.7, we find that the base REM
algorithm does exhibit feature co-adaptation. This case is an example where the srank metric from
Kumar et al. [28] may not indicate poor performance.
---REM ------ REM + DR3
ASteriX
Breakout
Q*Bert
104
104
103
103
102
Gradient Updates (x 62.5k) Gradient Updates (x 62.5k) Gradient Updates (x 62.5k) Gradient Updates (x 62.5k) Gradient Updates (x 62.5k)
Figure A.7: Feature dot products for REM and REM + DR3 on log scale. REM does suffer from feature
co-adaptation despite high-rank features.
A.5 Induced Implicit Regularizer: Theory And Practice
In this section, we compare the performance of our practical DR3 regularizer to the regularizers
(Equation 4) obtained for different choices of M, such as M induced by noise, studied in previous
work, and also evaluate the effect of dropping the stop gradient function from the practical version
of our regularizer.
Empirically comparing the explicit regularizers for different noise covariance matrices, M.
The theoretically derived regularizer (Equation 4) suggests that for a given choice of M, the follow-
ing equivalent of feature dot products should increase over the course of training:
∆M(θ) := E trace [∑MNQ0 a)VQ(s0, a0)τ] . (Generalized dot products) (A.1)
s,a∈D
We evaluate the efficacy of the explicit regularizer that penalizes the generalized dot products,
∆M (θ), in improving the performance of the policy, with the goal of identifying if our practical
method performs similar to this regularize] on generalized dot products.. While ΣM must be explic-
itly computed by running fixed point iteration for every parameter iterate θ found during TD-learning
- which makes this method significantly computationally expensive2, We evaluated it on five Atari
games for 50 × 62.5k gradient steps as a proof of concept. As shown in Figure A.8, the DR3 penalty
with the choice of M which corresponds to label noise, and the dot product DR3 penalty, which is
our main practical approach in this paper generally perform similarly on these domains, attaining
almost identical learning curves on 4/5 games, and clearly improving over the base algorithm. This
hints at the possibility of utilizing other noise covariance matrices to derive an explicit regularizer.
2In our implementation, we run 20 steps of the fixed-point computation of Σ as shown in Theorem 3.1 for
each gradient step on the Q-function, and this increases the runtime to about 8 days for 50 iterations on a P100
GPU.
18
Published as a conference paper at ICLR 2022
Gradient Updates (x 62.5k)
CQL + DR3 (label noise)	CQL + DR3	DQN + DR3 (label noise)	CQL	DQN
Gradient Updates (x 62.5k)
Gradient Updates (x 62.5k)
Figure A.8: Comparing the performance of explicit penalties for two different choices of the covariance
matrix M. Observe that in all the five games the DR3 regularizer derived for the choice of M from Blanc et al.
[6] also leads to a substantial increase in performance over the base algorithm, and in four of five games, DR3
(label-noise) works just as well as DR3.
Deriving more computationally efficient versions of the regularizer for a general M and identifying
the best choice of M are subject to future work.
Effect of stop gradient. Finally, we investigate the effect of utilizing a stop gradient in the DR3
regularizer. We run a variant of DR3: ∆0(φ) = Ps,a,s0 φ(s, a)> [[φ(s0, a0)]], with the stop gradient
on the second term (s0, a0) and present a comparison to the one without the stop gradient in Table A.1
for REM as the base offline method, averaged over 17 games. Note that this version of DR3, with the
stop gradient, also improves upon the baseline offline RL method (i.e., REM) by 130%. While this
performs largely similar, but somewhat worse than the complete version without the stop gradient,
these results do indicate that utilizing ∆0 (φ) can also lead to significant gains in performance.
A.6 Understanding Feature Co-Adaptation Some More
In this section, we present some more empirical evidence to understand feature co-adaptation. The
three factors we wish to study are: (1) the effect of target update frequency on feature co-adaptation;
(2) understand the trend in normalized similarities and compare these to the trend in dot products;
and (3) understand the effect of out-on-sample actions in TD-learning and compare it to offline
SARSA on a simpler gridworld domain. We answer these questions one by one via experiments
aiming to verify each hypothesis.
A.6.1 Effect of Target Update Frequency on Feature Co-Adaptation
We studied the effect of target update
frequency on feature co-adaptation,
on some gridworld domains from
Fu et al. [17]. We utilized the
grid16smoothobs environment,
where the goal of the agent is to nav-
igate from the center of a 16 × 16
gridworld maze to one of its corners
while avoiding obstacles and “lava”
cells. The observations provided to
the RL algorithm are given by a high-
dimensional random transformation
of the (x, y) coordinates, smoothed
over neighboring cells in the grid-
world. We sampled an offline dataset
of 256 transitions and trained a Q-
network with two hidden layers of size
7 6 5 4 3 2 1
Ooooooo
《601》23np<Md _OPojzeəu.
grid 16smoothob5 : Standard FQI
0	50	100	150	200	250
Gradient Updates
《601》s⅞snpo∙ιd _OPoJn
grid 16smoothobs ： CQL (α=1.0)
IO3
IO2
≡,5…….
=10
=50
=200
=500
IO1 L..-----77∏;_- ð „ __— C
O	50	IOO 150 200 250
Gradient Updates
Figure A.9: Comparing the feature dot products for various
target update delays, where a smaller N implies a faster update
and a larger N corresponds to a slower target update. Observe
that while slower updates to the target network may reduce co-
adaptation, very slow target updates may still lead to excesssive
co-adaptation.
(1024, 1024) via fitted Q-iteration (FQI) [37].
We evaluated the feature dot products for Q-functions trained with a varying target update frequen-
cies, given generically as: updating the target network using a hard target update once perN gradient
steps, where N takes on values N = 5, 10, 50, 100, 200, 500, and present the results in Figure A.9
(left), averaged over 3 random seeds. Thee feature dot products initially decrease from N = 5 to
N = 10, because the target network is updated slower, but then starts to rapidly increase when when
the target network is slowed down further to N = 50 and N = 200 in one case and N = 500 in the
other case.
19
Published as a conference paper at ICLR 2022
We also evaluated the feature dot products when using CQL as the base offline RL algorithm. As
shown in Figure A.9 (right), while CQL does reduce the absolute range of the feature dot products,
slow target updates with N = 500 still lead to the highest feature dot products as training progresses.
Takeaway: While it is intuitive to think that a slower target network might alleviate co-adaptation,
we see that this is not the case empirically with both FQI and CQL, suggesting a deeper question
that is an interesting avenue for future study.
A.6.2 Gridworld Experiments Comparing TD-learning vs Offline SARSA
To supplement the analysis in Section 3.1, we ran some experiments in the gridworld domains from
Fu et al. [17]. In this case, we used the grid16smoothsparse and grid16randomsparse
domains, which present challenging navigation tasks in a maze under a 0-1 sparse reward signal,
provided at the end of the trajectory. Additionally, the observations available to the offline RL agent
do not consist of the raw (x, y) locations of the agent in the maze, but rather high-dimensional
randomly chosen transformations of (x, y) in the case of grid16randomsparse, which are
additionally smoothed locally around a particular state to obtain grid16smoothsparse.
Since our goal is to compare feature co-adaptation in TD-learning and offline SARSA, we consider a
case where we evaluate a “mixed” behavior policy that chooses the optimal action with a probability
of 0.7 at a given state, and chooses a random, suboptimal action with 0.3. We then generate a
dataset of size 256 transitions and train offline SARSA and TD-learning on this data. While SARSA
backups the next action observed in the offline dataset, TD-learning computes a full expectation
of the Q-function Ea，〜穴日(∙∣S，)[Q(S0, a0)] under the behavior policy for computing Bellman backup
targets. The behavior policy is fully known to the TD-learning agent. Our Q-network consists of
two hidden layers of size (1024, 1024) as before.
We present the trends in the feature
dot products for TD-learning and of-
fline SARSA in Figure A.10, aver-
aged over three seeds. Observe that
the trends in the dot product values
for TD-learning and offline SARSA
closely follow each other for the ini-
tial few gradient steps, soon, the dot
products in TD-learning start grow-
ing faster. In contrast, the dot prod-
ucts for SARSA either saturate or
start decreasing. The only difference
between TD-learning and SARSA is
the set of actions used to compute
Bellman targets - while the actions
used for computing Bellman backup
gridIGsmoothsparse: TD vs SARSA
Gradient Updates
grid 16randomsparse: TD vs SARSA
Gradient Updates
Figure A.10: Comparing the feature dot products for TD-
learning and offline SARSA, used to compute the value of the
behavior policy using a dataset of size 256 on two gridworld do-
mains. Observe that the feature dot products are higher in the case
of TD-learning compared to offline SARSA.
targets in SARSA are in-sample actions and are observed in the dataset, the actions used by TD-
learning may be out-of-sample, but are still within the distribution of the data-generating behavior
policy. This supports our empirical evidence in the main paper showing that out-of-sample actions
can lead to feature co-adaptation.
A.6.3 Feature Co-Adaptation and Normalized Feature Similarities
Note that we characterized feature co-adaptation via the dot products of features. In this section,
we explore the trends in other notions of similarity, such as cosine similarity between φ(s, a) and
φ(s0, a0) which measures the dot product of feature vectors at consecutive state-action tuples after
normalization. Formally,
cos(φ(s, a), φ(s0, a0)) :
Φ(S, a)>φ(S: a0)
W^FwaB'
We plot the trend in the cosine similarity with and without DR3 for five Atari games in Figure A.11
with CQL, DQN and REM, and for the three MuJoCo tasks studied in Appendix A.3 in Figure A.12.
We find that the cosine similarity is generally very high on the Atari domains, close to 1, and not
indicative of performance degradation. On the Ant and Walker2d MuJoCo domains, we find that
20
Published as a conference paper at ICLR 2022
0
100
200
Breakout
100
200
---DQN ------- DQN + DR3
Space Invaders
100
Gradient Updates (X 62.5k) Gradient Updates (X 62.5k) Gradient Updates (X 62.5k) Gradient Updates (X 62.5k)
200
Gradient Updates (x 62.5k)
0
0
---REM ------ REM + DR3
Alμe-Eωφu∞oo
Q*Bert
1.00
0.98
100
200
Gradient Updates (x 62.5k) Gradient Updates (x 62.5k) Gradient Updates (x 62.5k)
A--.ɪe--eu-s OU-SOO
1.0
Seaquest
100
1.00
0.98
0.96
200
Gradient Updates (x 62.5k)
Space Invaders
100
200
Gradient Updates (x 62.5k)
■1.0004
Q*Bert
SeaqUest
----------------------1.000 1
0.975
0.9
0.950-
0.8
---CQL ------- CQL + DR3
0.950-
.925-
0.975-
Space Invaders
0
0
0
0	100	200 0	100	200 0	100	200 0	100	200 0	100	200
Gradient Updates (X 62.5k) Gradient Updates (X 62.5k) Gradient Updates (X 62.5k) Gradient Updates (X 62.5k) Gradient Updates (X 62.5k)
Figure A.11: Cosine similarities of DQN, DQN + DR3, REM, REM + DR3 and CQL, CQL + DR3. Note
that DQN, REM and CQL attain close to 1 cosine similarities, and addition of DR3 does reduce the cosine
similarities of consecutive state-action features.
Environment Steps (xlθk)
hopper-full-replay-v2	walker2d-full-replay-v2	ant-full-replay-v2
Environment Steps (xlθk)
Environment Steps (xlθk)
Figure A.12: Cosine similarities of CQL and CQL + DR3 on MuJoCo domains. Note that the cosine
similarities of CQL grow to 1 and roughly stabilize for Ant and Walker2d, but start decreasing for Hopper. This
happens despite the oscillatory trends in performance of CQL on Hopper A.3. This means that a low cosine
similarity need not imply poor performance, and DR3 can improve performance even when cosine similarity
of base CQL is decreasing. We also notice that DR3 does actually reduce cosine similarity.
the cosine similarity first rises up close to 1 and roughly saturates there. On the Hopper domain, the
cosine similarity even decreases over training. However we observe that the feature dot products
are increasing for all the domains. Applying DR3 in both cases improves performance (as shown
in earlier Appendix), and generally gives rise to reduced cosine similarity values, though it can also
increase the cosine similarity values occasionally. Furthermore, even when the cosine similarities
were decreasing for the base algorithm (e.g., in the case of Hopper), addition of DR3 reduced the
feature dot products and helped improve performance. This indicates that both the norm and di-
rectional alignment are contributors to the co-adaptation issue, which is what DR3 aims to fix and
independently directional alignment does not indicate poor performance.
A.7 Stability of DR3 From a Good Solution
In this appendix, we study the trend of CQL + DR3 when starting learning from a good initialization,
which was studied in Figure 2. As shown in Figure A.13, while the performance for baseline CQL
degrades significantly (from 5000 at initialization on Asterix, performance degrades to 〜2000 by
21
Published as a conference paper at ICLR 2022
Asterix
U 5000
40 4000
S 3000
⅞
2000
0	25	50	75	100	0	25	50	75	100	0	25	50	75	100	0	25	50	75	100	0	25	50	75	100
Gradient Updates (x 62.5k) Gradient Updates (x 62.5k) Gradient Updates (x 62.5k) Gradient Updates (x 62.5k) Gradient Updates (x 62.5k)
Figure A.13: Running CQL + DR3 and CQL in the setup of Figure 2 to evaluate the stability of CQL +
DR3 when starting training from a good solution. Observe that the performance of base CQL decays quickly
from the good solution, but CQL + DR3 is relatively more stable.
DQN + DR3	DQN
Figure A.14: Running DQN + DR3 and DQN in the setup of Figure 2 to evaluate the stability of DQN
+ DR3 when starting training from a good solution. Observe that the performance of base DQN decays
quickly from the good solution, but DQN + DR3 is relatively more stable.
100 iterations for base CQL), whereas the performance ofDR3 only moves from 5000 to 〜4300. A
similar trend holds for Breakout. This means that the addition of DR3 does stabilize the learning
relative to the baseline algorithm. Please note that we are not claiming that DR3 is unequivocally
stable, but that improves stability relative to the base method.
A.8 Statistical Significance of DR3 and Franka Kitchen Results
Table A.2: Performance of CQL, CQL + DR3 after 2M gradient steps on the Franka Kitchen domains
averaged over 3 seeds. This is training for 6x longer compared to CQL defaults. Observe that CQL + DR3
outperforms CQL at 2M steps, indicating is efficacy in preventing long term performance degradation.
D4RL Task	CQL	CQL + DR3
kitchen-mixed	27.67 ± 12.66	37.00 ± 11.53
kitchen-partial	20.67 ± 15.57	40.67 ± 4.04
kitchen-complete	28.00 ± 14.73	38.67 ± 6.66
We present the results comparing CQL and CQL+DR3 on the Franka Kitchen tasks from D4RL in
Table A.2. Observe that CQL+DR3 outperforms CQL, and to test the statistical significance of these
results, we analyze the probability of improvement of CQL+DR3 over CQL next.
Figure A.15: Statistical significance of the results of CQL + DR3 vs CQL (Table 2) as measured by
average probability of improvement [3], with stratified bootstrap confidence intervals for this statistic. Since
the lower CI for this statistic is > 0.5, CQL + DR3 significantly improves over base CQL, and since the mean
and upper CI are ≥ 0.75, this improvement is also meaningful.
In order to assess the statistical significance of our D4RL Antmaze and Kitchen results, we follow
the recommendedations by Agarwal et al. [3] for comparing deep RL algorithms considering their
22
Published as a conference paper at ICLR 2022
statistical uncertainties. Specifically, we computed the average probability of improvement [3] of
CQL + DR3 over CQL on the antmaze and kitchen domains, and we find that DR3 does significantly
and meaningfully improve over CQL on both the Kitchen and AntMaze domains. Before presenting
the results, let us first describe the metric we compute.
Probability of improvement and statistical significance. For two given algorithms Alg1 and Alg2,
and runs Xk, 1 ,Xk,2,…，Xk,m from Algi and runs Yk, 1 ,Yk,2,…，Yk,n from Alg2 on task k, the
probability of improvement of Algi over Alg2 is given by P(Algi > Alg2)=六 PK=I P(Algk 〉
Alg2k). The probability of improvement on a given task k, P (Alg1k > Alg2k) is computed using the
Mann-Whitney U-statistic and is given by:
1	M N	1,	if y	<	x,
P (Algik >	Alg2k)	=	MnT,T.S (Xk,i,Yk,j)	where	S (x,y)	=	1,	if y	=	x,
i=1 j=i	(0,	if y	>	x.
Algi leads to statistically significant improve over Alg2 if the lower CI for P(Algi > Alg2 ) is
larger than 0.5. Per the Neyman-Pearson statistical testing criterion in Bouthillier et al. [7], Algi
leads to statistically meaningful improvement over Alg2 if the upper confidence interval (CI) of
P(Algi > Alg2 ) is larger than 0.75.
Figure A.15 presents the value of P(CQL + DR3 > CQL) on the AntMaze and Kitchen domains at
2M gradient steps along with the 95% CI for this statistic. DR3 improves over CQL on the AntMaze
domains with probability 0.83 with 95% CI (0.7, 0.96) and on the Kitchen domains with probability
0.8 with 95% CI (0.6, 1.0). These values pass the criterion of being both statistically significant
and meaningful per the above definitions, implying that DR3 does significantly and meaningfully
improve upon CQL on these domains.
B	Related Works
In this section, we briefly review some extended related works, and in particular, try to connect
feature co-adaptation and implicit regularization to various interesting results pertaining to RL lower-
bounds with function approximation and self-supervised learning.
B.1	Brief S ummary of Related Work
Prior analyses of the learning dynamics in RL has focused primarily on analyzing error propaga-
tion in tabular or linear settings [e.g., 9, 13, 50, 45, 46, 15, 12], understanding instabilities in deep
RL [1, 5, 26, 44] and deriving weighted TD updates that enjoy convergence guarantees [31, 32, 40],
but these methods do not reason about implicit regularization or any form of representation learning.
Ghosh & Bellemare [20] focuses on understanding the stability of TD-learning in underparameter-
ized linear settings, whereas our focus is on the overparameterized setting, when optimizing TD
error and learning representations via SGD. Kumar et al. [28] studies the learning dynamics of
Q-learning and observes that the rank of the feature matrix, Φ, drops during training. While this
observation is related, our analysis characterizes the implicit preference of learning towards feature
co-adaptation (Theorem 3.1) on out-of-sample actions as the primary culprit for aliasing. Addition-
ally, while the goal of our work is not to increase srank(Φ), utilizing DR3 not only outperforms the
srank(Φ) penalty in Kumar et al. [28] by more than 100%, but it also alleviates rank collapse, with
no apparent term that explicitly enforces high rank values. Somewhat related to DR3, Durugkar &
Stone [14], Pohlen et al. [35] heuristically constrain gradients of TD to prevent changes in target
Q-values to prevent divergence. Contrary to such heuristic approaches, DR3 is inspired from a the-
oretical model of implicit regularization, and does not prevent changes in target values, but rather
reduces feature dot products.
B.2	Extended Related Work
Lower-bounds for offline RL. Zanette [51] identifies hard instances for offline TD learning of linear
value functions when the provided features are “aliased”. Note that this work does not consider
feature learning or implicit regularization, but their hardness result relies heavily on the fact the
given linear features are aliased in a special sense. Aliased features utilized in the hard instance
inhibit learning along certain dimensions of the feature space with TD-style updates, necessitating an
23
Published as a conference paper at ICLR 2022
exponential sample size for near-accurate value estimation, even under strong coverage assumptions.
A combination of Zanette [51]’s argument, which provides a hard instance given aliased features,
and our analysis, which studies the emergence of co-adapted/similar features in the offline deep RL
setting, could imply that the co-adaptation can lead to failure modes from the hard instance, even on
standard Offline RL problems, when provided with limited data.
Connections to self-supervised learning (SSL). Several modern self-supervised learning meth-
ods [21, 10] can be viewwed as utilizing some form of bootstrapping where different augmentations
of the same input (x + Aug1, x + Aug2) serve as consecutive state-action tuples that appear on two
sides of the backup. If we may extrapolate our reasoning of feature co-adaptation to this setting,
it would suggest that performing noisy updates on a self-supervised bootstrapping loss will give us
feature representations that are highly similar for consecutive state-action tuples, i.e., the representa-
tions for φ(x+Aug1)>φ(x+Aug2) will be high. Intuitively, an easy way for obtaining high feature
dot products is for φ(∙) to capture only that information in ∙, which is agnostic to data augmentation,
thus giving rise to features that are invariant to transformations. This aligns with what has been
shown in self-supervised learning [41, 42]. Another interesting point to note is that while such an
explanation would indicate that highly co-adapted features are beneficial in SSL, such features can
be adverse in value-based RL as discussed in Section 3.
Preventing divergence in deep TD-learning. Finally, we discuss Achiam et al. [1] which proposes
to pre-condition the TD-update using the inverse the neural tangent kernel [25] matrix so that the
TD-update is always a contraction, for every θk found during TD-learning. Intuitively, this can be
overly restrictive in several cases: we do not need to ensure that TD always contracts, but that is
eventually stabilizes at good solution over long periods of running noisy TD updates, Our implicit
regularizer (Equation‘4) derives this condition, and our theoretically-inspired DR3 regularizer shows
that empirically, it suffices to penalize the dot product similarity in practice.
C	Proof of Theorem 3.1
In this section, we will derive our implicit regularizer RTD (θ) that emerges when performing TD
updates with a stochastic noise model with covariance matrix M . We first introduce our notation
that we will use throughout the proof, then present our assumptions and finally derive the regularizer.
Our proof utilizes the analysis techniques from Blanc et al. [6] and Damian et al. [11], which ana-
lyze label-noise SGD for supervised learning, however key modifications need to be made to their
arguments to account for non-symmetric matrices that emerge in TD learning. As a result, the form
of the resulting regularizer is very different. To keep the proof concise, we will appeal to lemmas
from these prior works which will allow us to bound certain concentration terms.
C.1 Notation
The noisy TD-learning update for training the Q-function is given by:
θk+1 = θk — η (X NθQ(Si, ai)(Qθ(Si, ai)-(ri+γQθ(Si ai))))+?%,	εk 〜N(0,M) (C.1)
|~i--------------------V-----------------------}
:=g(θ)
where g(θ) denotes the parameter update. Note that g(θ) is not a full gradient of a scalar objective,
but it is a form of a “pseudo”-gradient or “semi”-gradient. Let εk denote an i.i.d.random noise that
is added to each update. This noise is sampled from a zero-mean Gaussian random variable with
covariance matrix M, i.e., N(0, M).
Let θ* denote a point in the parameter space such that in the vicinity of θ*, g(θ) ≤ C, for a small
enough C. Let G(θ) denote the derivative of g(θ) w.r.t. θ: G(θ) = Nθg(θ) and let NG(θ) denote
the third-order tensor V2g(θ). For notation clarity, let G = G(θ*), NG = NG(θ^). Let ei denote
the signed TD error for a given transition (si, ai, Si) ∈ D at θ*:
ei = Qθ* (si, ai) - (ri + YQθ* (si, ai))∙	(Cz
Since θ* is a fixed point of the training TD error, ei = 0. Following Blanc et al. [6], We will assume
that the learning rate in gradient descent, η, is small and we will ignore terms that scale as O(η1+δ),
for δ > 0. Our proof will rely on using a reference Ornstein-Uhlenbeck (OU) process which the TD
24
Published as a conference paper at ICLR 2022
parameter iterates will be compared to. Let ζk denote the k-th iterate of an OU process, which is
defined as:
Zk+1 = (I - ηG)Ck + ηεk, εk 〜N(0, M)	(C∙3)
We will drop θ from j to indicate that the gradient is being computed at θ*, and drop (Si, ai) from
Q(si, ai) and instead represent it as Qi for brevity; we will represent Q(s0i, a0i) as Q0i. We assume
that V2Qi is L-Lipschitz and V3Qi is L3-Lipschitz throughout the parameter space Θ.
C.2 Proof Strategy
For a given point θ* to be an attractive fixed point of TD-learning, our proof strategy would be to
derive the condition under which it mimics a given OU noise process, which as we will show stays
close to the parameter θ*. This condition would then be interpreted as the gradient of a “induced”
implicit regularizer. If the point θ* is not a stationary point of this regularizer, We will show that
the movement θ is large when running the noisy TD updates, indicating that the regularizer, atleast
in part guides the dynamics of TD-learning. To show this, we would write out the gradient update,
isolate some terms that will give rise to the implicit regularizer, and bound the remaining terms us-
ing contraction and concentration arguments. The contraction arguments largely follow prior work
(though with key exceptions in handling contraction with asymmetric and complex eigenvalue ma-
trices), while the form of the implicit regularizer is different. Finally, we will interpret the resulting
update over large timescales to show that learning is indeed guided by the implicit regularizer.
C.3 Assumptions and Conditions
Next, we present some key assumptions we will need for the proof. Our first assumption is that the
matrix G ∈ Rd×d is of maximal rank possible, which is equal to the number of datapoints n and
n d, the dimensionality of the parameter space. Crucially, this assumption do not imply that G is
of full rank - it cannot be, because We are in the overparameterized regime.
Assumption A1 (G spans an n-dimensional basis.). Assume that the matrix G spans n-possible
directions in the parameter space and hence, attains the maximal possible rank it can.
The second condition we require is that the matrices Pi VQi VQi> and M share the same n-
dimensional basis as matrix G:
Assumption A2.	i VQiVQi>, M, and G span identical n-dimensional subspaces.
This is a technical condition that is required. If this condition is not met, as we will show the learning
dynamics of noisy TD will not be a contraction in certain direction in the parameter space and TD-
learning will not stabilize at such a solution θ*. In fact, we will utilize a stronger version of this
statement for TD-learning to converge, and we will discuss this shortly.
C.4 Lemmas Used In The Proof
Next, we present some lemmas that would be useful for proving the theoretical result.
Lemma C.1 (Expressions for the first and and second-order derivatives of g(θ).). The following
definitions and expansions apply to our proof:
G(θ*) = X V2Qiei + X VQi(VQi - YN(Qi)>
ii
VG(θ*)[v, v] = 2 X V2QiVV> (VQi - γVQi,) + X tr ((V2Qi- γV2Qi)vv>) NQi + V3Qiei
ii
Lemma C.1 presents a decomposition of the matrix G and the directional derivative of the third order
tensor VG[v, v] in directions v and v, which will appear in the Taylor expansion layer. Note that
at θ* since ei = 0, the first term in G(θ*) and the third term in VG(θ* )[v, v] vanish. Lemma C.2
derives a fixed-point recursion for the covariance matrix of the total noise accumulated in the OU-
process with covariance matrix M and this will appear in our proof.
25
Published as a conference paper at ICLR 2022
Lemma C.2 (Covariance of the random noise process ζk). Let ζk denote the OU process satisfying:
Zk+ι = (I — ηG)Zk + ηεk, where εk ~ N(0, M), where M < 0. Then, Zk +ι ~ N(0, Σ), where Σ
satisfies the discrete Lyapunov equation:
∑ M = (I — ηG )∑ M (I — ηG) > + η2 m.
Proof. For the OU process, Zk+1 = (I — ηG)Zk + ηεk, since εk is a Gaussian random variable, by
induction so is Zk+1, and therefore the covariance matrix of Zk+1 is given by:
Σk+1 := (I — ηG)Σk (I — ηG>) + η2M.	(C.4)
Solving for the fixed point for Σk gives the desired expression.	口
In our proofs, we will require the following contraction lemmas to tightly bound the magnitude of
some zero-mean terms that will appear in the noisy TD update under certain scenarios. Unlike the
analysis in Damian et al. [11] and Blanc et al. [6] for supervised learning with label noise, where the
contraction terms like (I — ηG)k G are bounded by ≈ Jkn intuitively because I — ηG is a contraction
in the subspace spanned by matrix G. However, this is not true for TD-learning directly since
terms like (I — ηG)k S appear for a different matrix S. Therefore, TD-learning will diverge from
θ* unless matrices G and M have their corresponding eigenvectors assigned to the top eigenvalues
be approximately “aligned”. We formalize this definition next, and then provide a proof of the
concentration guarantee.
Definition 1 ((ω, C0)-alignment). Given a positive semidefinite matrix A, letA = UAΛAUA> denote
its eigendecomposition. Without loss of generality assume that the eiegenvalues are arranged in
decreasing order, i.e., ∀i > j, ΛA(i) ≤ ΛA(j). Given another matrix B, let B = UB ΛB UBH denote
its complex eigendecomposition, where eigenvalues in ΛB are arranged in decreasing order of their
complex magnitudes, i.e., ∀i > j, |AB (i) ∣ ≤ ∣AB (j) |. Then the matrix pair (A, B) is said to be
(ω, C0)-alignedif ∣UH (i) UA (i) ∣ ≤ ω and if ∀ i, A A (i) ≤ C01A B (i) ∣ fora constant C0.
If two matrices are (ω, C0)-aligned, this means that the corresponding eigenvectors when arranged
in decreasing order of eigenvalue magnitude roughly align with each other. This condition would
be crucial while deriving the implicit regularizer as it will quantify the rate of contraction of certain
terms that define the neighborhood that the iterates of noisy TD-learning will lie in with high prob-
ability. We will operate in the setting when the matrix G and P - NQi^Q> are (ω, C0)-aligned
with each other, and matrix M and G are also (ω, C0)-aligned (note that we can consider ω0, C00 ),
which will not change our bounds and therefore we go for less notational clutter). Next we utilize
this notion of alignment to show a particular contraction bound that extends the weak contraction
bound in Damian et al. [11].
Lemma C.3. Assume we are given a matrix G such that ∣λi (I — ηG) ∣ ≤ P0 < 1 for all λ- such that
λi 6= 0. Let G = UAUH be the complex eigenvalue decomposition ofG (since almost every matrix
is complex-diagonalizable). For a positive semi-definite matrix S that is (ω, C0)-aligned with G, if
S = US AS US> is its eigenvalue decomposition, the following contraction bound holds:
Il (I — ηG )ks∣l =。(黄)
Proof. To prove this statement, we can expand (I — ηG) using its eigenvalue decomposition only
in the subspace that is jointly shared by G and M, and then utilize the definition of ω-alignment to
bound the terms.
Il ( i — ηG)k S Il = Il (I-nU A UH)k US A S u>∣∣	(c.5)
=(UUH — nU A U UH)k US A S US Il	(c.6)
=U (I — n A) k UH US A S U>∣∣	(C.7)
≤ ω -II (I — n A)k I∣∙ A S	(C.8)
≤ ω ∙ CO ∙ (max 11 — nA(i) Ik IA(i) I)	(C.9)
26
Published as a conference paper at ICLR 2022
Now we need to solve for the inner maximization term. When Λ(i) is not complex for any i, the
term above is . 1 /ηk using the result from Damian et al. [11], but When Λ(i) is complex, this bound
can only hold under certain conditions. To note when this quantity is bounded, we expand 11 一 ηχ∖k
for some complex number x = r(cos θ + ι sin θ):
∖1 一 ηx∖k = ∖(1 一 ηr cos θ) + ιηr sin θ∖	(C.10)
= q∕(Γ^ηrcosθ)2^+η2r2sin2^θ =(1 + η2 r2 — 2 ηr cos θ) k/2 (C.11)
=⇒ ∖ 1 — ηx∖k ∖x∖ =(1 + η2r2 — 2ηr cos θ) k/2 r	(C.12)
.ɪ if η ≤ min，：(，!?)and ∞ otherwise.	(C.13)
ηk	i	∖Λ(i)∖
Plugging back the above expression in the bound above completes the proof.	□
The proof of Lemma C.3 indicates that unless the learning rate η and the matrix G are such that the
∖λi (I — ηG)∖ ≤ ρ < 1 in directions spanned by matrix S, such an expression may not converge.
This is expected since the matrix I — ηG will not contract in directions of non-zero eigenvalues if
the real part r cos θ is negative or zero. Additionally, we note that under Definition 1, we can extend
several weak-contraction bounds from Damian et al. [11] (Lemmas 9-14 in Damian et al. [11]) to
our setting.
Next, Lemma C.4 shows that the OU noise iterates are bounded with high probability when Defini-
tion 1 holds:
Lemma C.4 (Zk is bounded with high probability). With probability atleast 1 — δ and under Defini-
tion1, ∖∖Zk∖∖ ≤ nω√ηC0log 1 = O(√η).
Proof. To prove this lemma, we first bound the trace of the covariance matrix Σk+1 and then apply
high probability bounds on the Martingale norm concentration. The trace of the covariance matrix
Σk+1 can be bounded as follows (all the equations below are restricted to the dimensions of non-zero
eigenvalues of G):
tr[Σk+1]=	tr (I — ηG)jM(I — ηG>)j	(C.14)
j≤k
=X tr [(UU H — ηU Λ U H)j M (UU H — ηU Λ U H)j ]	(C.15)
j ≤k
=X tr [U (I — η Λ)j U H UM Λ M UM U (I — η Λ)j U H ]	(C.16)
j ≤k
=X nω2C0tr [∖I — ηΛ∖j - ∖Λ∖ ∙ ∖I — ηΛ∖j]	(C.17)
j ≤k
≤ nω2C0 X n ∙ max(∖1 — ηλ∖2j ∙ ∖λ∖) ≤ ηn2C0ω2	(C.18)
j ≤k
Now, we can apply Corollary 1 from Damian et al. [11] to obtain a bound on ∖∖ζk ∖∖ as with high
probability, atleast 1 — δ,
∖∖Zk ∖∖ ≤ J2tr(Σ)log : = nω√ηC0log 1.
□
C.5 Main Proof of Theorem 3.1
In this section, we present the main proof of Theorem 3.1. The proof involves two components:
(1) the part where we derive the regularizer, and (2) bounding additional terms via concentration
inequalities. Part (1) is specific to TD-learning, while a lot of the machinery for part (2) is directly
taken from prior work [11] and Blanc et al. [6]. We focus on part (1) here.
Our strategy is to analyze the learning dynamics of noisy TD updates that originate at θ*. Ina small
neighborhood around θ*, we can expand the noisy TD update (Equation 3) using Taylor,s expansion
27
Published as a conference paper at ICLR 2022
around θ* which gives:
θk + 1 = θk - ηg(θk ) + ηεk, εk ~ N(0,M)	(C∙19)
=⇒ θk+ι = θk - η (g + G® - θ^) - 2G[θk - θ^, θk - θ^]) + ηεk + O(η∣∣θk - θ*∣∣3)∙
(C.20)
Denoting Vk := θk - θ^, using the fact that ∣∣g (θ*) || ≤ C, we find that Vk can be written as:
Vk + 1 = (I - ηG) Vk + εk + 2 G [ Vk, Vk ] + O (ηl∖νk || 3 + nC)	(C∙21)
Since the OU process Zk stays in the vicinity of the point θ*, and follows a similar recursion to the
one above, our goal would be to design a regularizer so that Equation C.21 closely follows the OU
process. Thus, we would want to bound the difference between the variable Vk and the variable ζk,
denoted as rk to be within a small neighborhood:
rk + 1 = Vk +1 - Zk +1 = (I - nG)( Vk - Zk ) + AG [ Vk, Vk ] + O (nl\Vk || 3 + ηC) ∙
I______I 2
rk
We can write down an expression for rk summing over all the terms:
rk+1 = - 2 X(I - ηG)lΝG [ Vk ,Vk ]+ X(I - r1G)j [ O (η∣∣Vk || 3 + ηC)] ∙	(C.22)
j ≤k	j≤k
I-Σ------------------1 L∑----------------------1
term (a)	term (b)
Term (a) in the above equation is the one that can induce a displacement in rk as k increases and
would be used to derive the regularizer, whereas term (b) primarily consists of terms that concentrate
to 0. We first analyze term (a) and then we will analyze the concentration terms later.
To analyze term (a), note that the term NG[Vk, Vk], by Lemma C.1, only depends on Vk via the
covariance matrix VkVk> . So we will partition this term into two terms: (i) a term that utilizes the
asymptotic covariance matrix of the OU process and (ii) errors due to a finite k and stochasticity that
will concentrate.
2×(a) = η X(I - ηG)k-jNG[Vk, Vk]	(C.23)
=X(I- ηG)k-jNG[ζ*,ζ*]+ X(I- ηG)k-jNG([Vk, Vk] - [ζZK*]),	(C.24)
j ≤k	j≤k
The first term is a “bias” term and doesn’t concentrate to 0, and will give rise to the regularizer. We
can break this term using Lemma C.1 as:
NG[ZZ,ZZ] =2XN2QiΣZM(NQi-γNQ0i)+Xtr[(N2Qi-γN2Q0i)ΣZM]NQi	(C.25)
ii
The regularizer RTD(θ) is the function such that:
NθRTD(θ) =XN2QiΣZM(NQi -γNQ0i)	(C.26)
i
=⇒ RTD(θ) = X NQiΣMNQ> - γ X trace (∑MNQi [[NQi]]τ),	(C.27)
ii
where [[∙]] denotes the stop gradient operator. If the point θz is a stationary point of the regularizer
RTD (θ), then Equations C.26 and C.27 imply that the first term of Equation C.25 must be 0. There-
fore in this case to show that θZ is attractive, we need to show that the other terms in Equations C.25,
C.24 and term (b) in Equation C.22 concentrate around 0 and are bounded in magnitude. The re-
maining part of the proof shown in Appendix C.7 provides these details, but we first summarize the
main takeaways in the proof to conclude the argument.
28
Published as a conference paper at ICLR 2022
C.6 Summary of the Argument
We will show how to concentrate terms in Equation C.26 besides the regularizer largely following
the techniques from prior work, but we first summarize the entire proof. The overall update to
the vector r% which measures the displacement between the parameter vector θk — θ* and the OU-
process ζk can be written as follows, and it is governed by the derivative of the implicit regularizer
(modulo error terms):
Tk+1 = — 2 X(I — ηG)lNθRtd(Θ*) + O (√nt ∙ poly修,L, L3,3, C0)) . (C.28)
2 j≤k
An important detail to note here is that since the regularizer consists of ΣM and the size of ΣM (i.e,
its eigenvalues), as shown in Lemma C.4 depends on one factor of η. So, effectively the first term
in Equation C.28 does depend on two factors ofη. Using Equation C.28, we can write the deviation
between θ* and θk as:
Vk+1 = Zk+1 - η X(I — ηG)1 NθRTD(θ*) + O (√η - poly(C, L, L3, ω, C0)) . (C.29)
2 j≤k
The OU process Zk converges to θ* in the subspace spanned by G, since the condition P(I — ηG) < 1
is active in this subspace (if the condition that ρ(I — ηG) < 1 in the subspace spanned by G is not
true, then as Ghosh & Bellemare [20] show, TD can diverge). Now, given G satisfies this spectral
radius condition, Zk would converge to θ* within a timescale of O (1) within this subspace, which
as Blanc et al. [6] put it is the strength of the “mean-reversion” term. On the remaining directions
(note that d n), the dynamics is guided by the regularizer, although with a smaller weight of η2 .
C.7 Additional Proof Details : Concentrating Other Terms
We first concentrate the terms in Equation C.25. The cumulative effect of the second term in Equa-
tion C.25 is given by:
η X(I — ηGLNQitr [(V2Qi- N2Q[)ΣM]	(C.30)
≤ ηX(I — ηG)j-kVQi ∙O (L (1 + Y)σ) ≤ O (nʌ^0C0L (1 + Y)σ) ,	(C.31)
which follows from the fact that N2Qi is L2-Lipschitz, and using Lemma C.3 for contracting the
remaining terms.
Next, we turn to concentrating the second term in Equation C.24. This term corresponds to the con-
tribution of difference between the empirical covariance matrix νk νk> and the asymptotic covariance
matrix Z«*τ. We expand this term below using the form of G from Lemma C.1, and bound it one
by one.
X(I-ηG Y--VG ([ Vk ,Vk ] — [ Z *,Z *])	(C.32)
j≤k
=XX(I—ηG)k-jV2Qi (VkVk — Z*Z*τ) (VQi — γVQi)) + O (Pkω0C0L(1 + Y)σ)
j ≤k i
(C.33)
Now, we note that the term ∆k+1 := Vk +1 vT+i — Z^Z*τ can itself be written as a recursion:
∆k+1 = (I — η G)(∆k)(I — ηG)τ + (I — ηG)Zk ετ + εZkτ (I — ηG)τ + εετ — ηM (C.34)
1_____________________________________________________I I-------1
Afc	Bk
29
Published as a conference paper at ICLR 2022
Expanding the term ∆k+1 in terms of a summation over k, and plugging it into the expression from
Equation C.33 we get
XX(I
—ηG)k-jV2Qi(I - ηG)j∆o(I - ηG>)j	(C.35)
i j≤k
+XXX(I
- ηG)k-jV2Qi(I - ηG)j-p-1(Ap + Bp)(I - ηG>)j-p-1
i j ≤k p≤j
Now by noting that if G and VQi are (ω, C0)-aligned, then so are G> and VQi, we can finish the
proof by repeating the calculations used by Damian et al. [11] (Appendix B, Equations 67-73) to
bound the terms in Equation C.35 by O(√ηk)), but with an additional factor of ω2C2.
Term (b) in Equation C.22. When C is small enough, we can bound the term (b) using O(√ηk),
similar to Damian et al. [11].
D Proof of Proposition 3.2
In this section, we will prove Proposition 3.2. First, we refer to Proposition 3.1 in Ghosh &
Bellemare [20], which shows that TD-learning is stable and converges if and only if the matrix
Mφ = Φ>(Φ - γΦ0) has eigenvalues with all positive real entries. Now note that if,
φ(s, a)>φ(s, a) ≤ γ	φ(s0, a0)>φ(s, a)	(D.1)
s,a	s,a,s0
=⇒ trace (Φ>Φ) ≤ Ytrace (Φ>Φ0)	(D.2)
=⇒ trace Φ> (Φ - γΦ0) ≤ 0.	(D.3)
Since the trace of a real matrix is the sum of real components of eigenvalues, if for a given matrix
M, trace(M) ≤ 0, then there exists atleast one eigenvalue λi such that Re(λi) ≤ 0. If λi < 0, then
the learning dynamics of TD would diverge, while if λi = 0 for all i, then learning will not contract
towards the TD fixed point. This concludes the proof of this result.
E Experimental Details of Applying DR3
In this section, we discuss the practical experimental details and hyperparameters in applying our
method, DR3 to various offline RL methods. We first discuss an overview of the offline RL methods
we considered in this paper, and then provide a discussion of hyperparameters for DR3.
E.1 Background on Various Offline RL Algorithms
In this paper, we consider four base offline RL algorithms that we apply DR3 on. These methods
are detailed below:
REM. Random ensemble mixture [2] is an uncertainty-based offline RL algorithm uses multiple
parameterized Q-functions to estimate the Q-values. During the Bellman backup, REM computes
a random convex combination of the target Q-values and then trains the Q-function to match this
randomized target estimate. The randomized target value estimate provides a robust estimate of
target values, and delays unlearning and performance degradation that we typically see with standard
DQN-style algorithms in the offline setting. For instantiating REM, we follow the instantiation
provided by the authors and instantiate a multi-headed Q-function with 200 heads, each of which
serves as an estimate of the target value. These multiple heads branch off the last-but-one layer
features of the base Q-network. The objective for REM is given by:
m^n Es,a,r,S0〜D
Eαι ,...,ακ〜∆
'λ (XXαkQθk(s, a) - r - γmaa0xXk αkQθk0(s0, a0)	(E.1)
where lλ denotes the Huber loss while P∆ denotes the probability distribution over the standard (K
1)-simplex.
30
Published as a conference paper at ICLR 2022
CQL. Conservative Q-learning [27] is an offline RL algorithm that learns a conservative value
function such that the estimated performance of the policy under this learned value function lower-
bounds its true value. CQL modifies the Q-function training to incorporate a term that minimizes
the overestimated Q-values in expectation, while maximizing the Q-values observed in the dataset,
in addition to standard TD error. This CQL regularizer is typically multiplied by a coefficient α, and
we pick α = 0.1 for all our Atari experiments following Kumar et al. [28] and α = 5.0 for all our
kitchen and antmaze D4RL experiments. Using yk (s, a) to denote the target values computed via
the Bellman backup (we use actor-critic backup for D4RL experiments and the maxa0 backup for
standard Q-learning in our Atari experiments following Kumar et al. [27]), the objective for training
CQL is given by:
吗n Q (Es〜D log ɪ2 exp(Q(s, a))

`,a 〜D [ Q (s, a)]) +2Es,a, S 0 〜D h( Q (S, a) — yk (S, a))2 i ∙
a
The deep Q-network utilized by us is a ReLU network with four hidden layers of size
(256, 256, 256, 256) for the D4RL experiments, while for Atari we utilize the standard convolutional
neural network from Agarwal et al. [2], Kumar et al. [28] with 3 convolutional layers borrowed from
the nature DQN network and then a hidden feedforward layer of size 512.
BRAC. Behavior-regularized actor-critic [49] is a policy-constraint based actor-critic offline RL
algorithm which regularizes the policy to stay close to the behavior policy πβ to prevent the selection
of “out-of-distribution” actions. In addition, BRAC subtracts this divergence estimate from the target
Q-values when performing the backup, to specifically penalize target values that come from out-of-
distribution action inputs at the next state (s0, a0).
Q-function:	m^n Es, a 〜D [(r (s, a)+ Y Ea 0「、小(.∖ S 0)[ Q θ (s 0, a 0)+ β log π β (a 0∣ S 0)] — Qθ (s, a))[ ∙
Policy: max Es〜d,a〜∏φ(∙∣s) [Qθ(s, a) + βlog∏β(a∣s) — Qlog∏φ(a∣s)].
φ
(E.2)
COG. COG [38] is an algorithmic framework for utilizing large, unlabeled datasets of diverse be-
havior to learn generalizable policies via offline RL. Similar to real-world scenarios where large
unlabeled datasets are available alongside limited task-specific data, the agent is provided with two
types of datasets. The task-specific dataset consists of behavior relevant for the task, but the prior
dataset can consist of a number of random or scripted behaviors being executed in the same envi-
ronment/setting. The goal in this task is to actually stitch together relevant and overlapping parts
of different trajectories to obtain a good policy that can work from a new initial condition that was
not seen in a trajectory that actually achieved the reward. COG utilizes CQL as the base offline RL
algorithm, and following Singh et al. [38], we fix the hyperparameter Q = 1.0 in the CQL part for
both base COG and COG + DR3. All other hyperparameters including network sizes, etc are kept
fixed as the prior work Singh et al. [38] as well.
E.2 Tasks and Environments Used
Atari 2600 games used. For all our experiments, we used the same set of 17 games utilized by Ku-
mar et al. [28] to test rank collapse. In the case of Atari, we used the 5 standard games (Asterix,
Qbert, Pong, Seaquest, B reakout) for tuning the hyperparameters, a strategy followed by sev-
eral prior works [22, 2, 28]. The 17 games we test on are: Asterix, Qbert, Pong, Seaquest,
Breakout, Double Dunk, James Bond, Ms. Pacman, Space Invaders, Zaxxon, Wiz-
ard of Wor, Yars’ Revenge, Enduro, Road Runner, BeamRider, Demon Attack, Ice
Hockey.
Following Agarwal et al. [3], we report interquartile mean (IQM) normalized scores across all runs
as mean scores can be dominated by performance on a few outlier tasks while median is independent
of performance on all except 1 task - zero score on half of the tasks would not affect the median.
IQM which corresponds to 25% trimmed mean and considers the performance on middle 50% of
the runs. IQM interpolates between mean and median, which correspond to 0% and almost 50%
trimmed means across runs.
D4RL tasks used. For our experiments on D4RL, we utilize the Gym-MuJoCo-v0 environments
for evaluating BRAC, since BRAC performed somewhat reasonably on these domains [18], whereas
we use the harder AntMaze and Franka Kitchen domains for evaluating CQL, since these domains
are challenging for CQL [27].
31
Published as a conference paper at ICLR 2022
Table E.1: Hyperparameters used by the offline RL Atari agents in our experiments. Following
Agarwal et al. [2], the Atari environments used by us are stochastic due to sticky actions, i.e., there
is a25% chance at every time step that the environment will execute the agents previous action again,
instead of the new action commanded. We report offline training results with same hyperparameters
over 5 random seeds of the offline dataset, game simulator and network initialization.
Hyperparameter	Setting (for both variations)
Sticky actions	Yes
Sticky action probability	0.25
Grey-scaling	True
Observation down-sampling	(84, 84)
Frames stacked	4
Frame skip (Action repetitions)	4
Reward clipping	[-1, 1]
Terminal condition	Game Over
Max frames per episode	108K
Discount factor	0.99
Mini-batch size	32
Target network update period	every 2000 updates
Training environment steps per iteration	250K
Update period every	4 environment steps
Evaluation	0.001
Evaluation steps per iteration	125K
Q-network: channels	32, 64, 64
Q-network: filter size	8 × 8, 4 × 4, 3 × 3
Q-network: stride	4, 2, 1
Q-network: hidden units	512
Robotic manipulation tasks from COG [38]. These tasks consist of a 6-DoF WidowX robot,
placed in front of two drawers and a larger variety of objects. The robot can open or close a drawer,
grasp objects from inside the drawer or on the table, and place them anywhere in the scene. The task
here consists of taking an object out of a drawer. A reward of +1 is obtained when the object has
been taken out, and zero otherwise. There are two variants of this domain: (1) in the first variant,
the drawer starts out closed, the top drawer starts out open (which blocks the handle for the lower
drawer), and an object starts out in front of the closed drawer, which must be moved out of the way
before opening, and (2) in the second variant, the drawer is blocked by an object, and this object
must be removed before the drawer can be opened and the target object can be grasped from the
drawer. The prior data for this environment is collected from a collection of scripted randomized
policies. These policies are capable of opening and closing both drawers with 40-50% success rates,
can grasp objects in the scene with about a 70% success rate, and place those objects at random
places in the scene (with a slight bias for putting them in the tray).
E.3 The DR3 Regularizer Coefficient
We utilize identical hyperparameters of the base offline RL algorithms when DR3 is used, where
the base hyper-parameters correspond to the ones provided in the corresponding publications. DR3
requires us to tune the additional coefficient c0, that weights the DR3 explicit regularizer term. In
order to find this value on our domains, we followed the tuning strategy typically followed on Atari,
where we evaluated four different values of c0 ∈ {0.001, 0.01, 0.03, 0.3} on 5 games (ASTERIX,
SEAQUEST, B REAKOUT, PONG and SPACEINVADERS) on the 5% replay dataset settings, picked c0
that wprked best on just these domains, and used it to report performance on all 17 games, across
all dataset settings (1% replay and 10% initial replay) in Section 5. This protocol is standard in
Atari and has been used previously in Agarwal et al. [2], Gulcehre et al. [22], Kumar et al. [28] in
the context of offline RL. The value of the coefficient found using this strategy was c0 = 0.001 for
REM and c0 = 0.03 for CQL.
For CQL on D4RL, we ran DR3 with multiple values of c0 ∈ {0.0001, 0.001, 0.01, 0.5, 1.0, 10.0},
and picked the smallest value of c0 which did not lead to eventually divergent (either negatively
diverging or positively diverging) Q-values, in average. For the antmaze domains, this corresponded
to c0 = 0.001 and for the FrankaKitchen domains, this corresponded to c0 = 1.0.
32
Published as a conference paper at ICLR 2022
F Complete Results on All Domains
In this section, we present the results obtained by running DR3 on the Atari and D4RL domains
which were not discussed in the main paper due to lack of space. We first understand the effect
of applying DR3 on BRAC [49], which was missing from the main paper, and then present the
per-game Atari results.
Table F.1: Normalized interquartile mean (IQM) final performance (last iteration return) of CQL, CQL +
DR3, REM and REM + DR3 after 6.5M gradient steps for the 1% setting and 12.5M gradient steps for the 5%,
10% settings. Intervals in brackets show 95% CIs computed using stratified percentile bootstrap [3]
Data	CQL	CQL + DR3	REM	REM + DR3
1%	44.4 (31.0, 54.3)	61.6 (39.1, 71.5)	0.0 (-0.7, 0.1)	13.1 (9.9, 18.3)
5%	89.6 (67.9, 98.1)	100.2 (90.6, 102.7)	3.9 (3.1, 7.6)	74.8 (59.6, 84.4)
10%	57.4 (53.2, 62.4)	67.0 (62.8, 73.0)	24.9 (15.0, 29.1)	72.4 (65.7, 81.7)
Table F.2: Performance of DR3 when applied in conjunction with BRAC [49]. Note that DR3
attains a larger final performance (at the end of 2M steps of training) as well as a higher average
performance (i.e. stability score) across all iterations of training.
Task	Average Performance across Iterations		Final Performance	
	BRAC	BRAC + DR3	BRAC	BRAC + DR3
halfcheetah-expert-v0	1.7 ± 1.9	49.9 ± 16.7	2.1 ± 3.3	71.5 ± 24.9
halfcheetah-medium-v0	43.5 ± 0.2	43.2 ± 0.2	45.1 ± 0.8	44.9 ± 0.6
halfcheetah-medium-expert-v0	17.0 ± 5.4	6.0 ± 5.5	24.8 ± 9.3	6.7 ± 7.3
halfcheetah-random-v0	24.4 ± 0.4	18.4 ± 0.3	24.9 ± 0.8	18.2 ± 1.0
halfcheetah-medium-replay-v0	44.9 ± 0.3	44.1 ± 0.4	45.0 ± 1.4	44.9 ± 0.5
hopper-expert-v0	15.7 ± 1.5	21.8 ± 3.2	16.6 ± 6.0	20.8 ± 5.3
hopper-medium-v0	32.8 ± 1.4	46.3 ± 7.1	36.2 ± 1.7	58.3 ± 13.7
hopper-medium-expert-v0	40.2 ± 5.7	37.0 ± 2.9	31.7 ± 11.8	21.8 ±4.9
hopper-random-v0	11.7 ± 0.0	11.2 ± 0.0	12.2 ± 0.0	11.1 ± 0.0
hopper-medium-replay-v0	31.6 ± 0.3	30.3 ± 0.8	31.3 ± 1.2	36.1 ± 5.7
walker2d-expert-v0	25.5 ± 14.4	33.6 ± 11.8	54.0 ± 31.0	60.6 ± 20.2
walker2d-medium-v0	81.3 ± 0.3	80.8 ± 0.2	83.8 ± 0.2	83.4 ± 0.3
walker2d-medium-expert-v0	5.8 ± 5.2	6.4 ± 3.4	22.4 ± 22.0	39.5 ± 23.3
walker2d-random-v0	1.4 ± 0.8	1.7 ± 0.9	0.0 ± 0.1	2.9 ± 2.1
walker2d-medium-replay-v0	26.1 ± 6.4	47.4 ±4.1	11.7 ± 7.0	38.7 ± 9.6
33
Published as a conference paper at ICLR 2022
Table F.4: Mean evaluation returns per Atari game across 5 runs with standard deviations for
1% dataset. The coefficient for DR3 is 0.03 with a CQL coefficient of 1.0. The average performance
is computed over 20 checkpoints spaced uniformly over training for 100 iterations where 1 iteration
corresponds to 62,500 gradient updates.
Game	Final Performance		Average Performance across Iterations	
	CQL	CQL + DR3	CQL	CQL + DR3
Asterix	656.9 ± 91.0	821.4 ± 75.1	650.2 ± 65.3	814.1 ± 25.1
Breakout	23.9 ± 3.8	32.0 ± 3.2	23.8 ± 0.5	32.8 ± 3.1
Pong	16.7 ± 1.7	14.2 ± 3.3	15.7 ± 2.0	15.1 ±2.3
Seaquest	449.0 ± 11.0	446.6 ± 26.9	474.5 ± 30.3	456.1 ± 17.0
Qbert	8033.8 ± 1513.2	9162.7 ± 993.6	7980.0 ± 379.9	9000.7 ± 225.2
SpaceInvaders	386.0 ± 123.2	351.9 ± 77.1	371.7 ± 47.5	440.6 ± 29.6
Zaxxon	829.4 ± 813.3	1757.4 ± 879.4	834.6 ± 504.0	1634.0 ± 673.9
YarsRevenge	11848.2 ± 2977.7	16011.3 ± 1409.0	15077.9 ± 1301.9	17741.6 ± 613.6
RoadRunner	37000.7 ± 1148.5	24928.7 ± 7484.5	35899.9 ± 653.1	32063.3 ± 1011.4
MsPacman	1869.8 ± 167.2	2245.7 ± 193.8	1991.9 ± 55.1	2224.1 ± 80.8
BeamRider	780.3 ± 64.5	617.9 ± 25.1	782.0 ± 36.1	619.9 ± 20.9
Jamesbond	558.5 ± 124.8	460.5 ± 102.0	524.6 ± 118.5	484.2 ± 89.4
Enduro	198.4 ± 34.2	253.5 ± 14.2	259.8 ± 16.4	276.1 ± 16.9
WizardOfWor	771.1 ± 358.2	904.6 ± 343.7	833.7 ± 168.4	935.2 ± 174.4
IceHockey	-8.7 ± 1.3	-7.8 ± 0.9	-8.8 ± 0.9	-7.9 ± 0.7
DoubleDunk	-15.1 ± 1.9	-14.0 ± 2.8	-15.3 ± 0.9	-14.5 ± 1.0
DemonAttack	1970.2 ± 161.3	386.2 ± 75.3	1338.8 ± 298.4	414.0 ± 46.0
Table F.5: Mean evaluation returns per Atari game across 5 runs with standard deviations for
5% dataset. The coefficient for DR3 is 0.03 with aCQL coefficient of 0.1. The average performance
is computed over 20 checkpoints spaced uniformly over training for 200 iterations where 1 iteration
corresponds to 62,500 gradient updates.
Game	Final Performance		Average Performance across Iterations	
	CQL	CQL + DR3	CQL	CQL + DR3
Asterix	1798.2 ± 168.6	3318.5 ± 301.7	1812.7 ± 64.0	3790.5 ± 218.0
Breakout	94.1 ± 44.4	166.0 ± 23.1	105.1 ± 10.4	196.5 ±4.4
Pong	13.1 ±4.2	17.9 ± 1.1	15.2 ± 1.3	17.4 ± 1.2
Seaquest	1815.9 ± 722.8	2030.7 ± 822.8	1382.3 ± 258.1	3722.3 ± 969.5
Qbert	10595.7 ± 1648.5	9605.6 ± 1593.5	9552.0 ± 925.6	10830.7 ± 783.1
SpaceInvaders	758.9 ± 56.9	1214.6 ± 281.8	662.0 ± 58.1	1323.7 ± 94.4
Zaxxon	1501.0 ± 1165.7	4250.1 ± 626.2	1508.8 ± 437.5	3556.5 ± 531.3
YarsRevenge	24036.7 ± 3370.6	17124.7 ± 2125.6	22733.1 ± 1175.3	18339.8 ± 1299.7
RoadRunner	40728.4 ± 3318.9	38432.6 ± 1539.7	42338.4 ± 471.4	41260.2 ± 1008.6
MsPacman	2975.9 ± 522.1	2790.6 ± 353.1	2923.6 ± 251.3	3101.2 ± 381.6
BeamRider	1897.6 ± 473.7	785.8 ± 43.5	2218.5 ± 242.4	775.9 ± 12.5
Jamesbond	108.8 ± 49.1	96.8 ± 43.2	76.5 ± 4.6	106.1 ± 34.8
Enduro	764.3 ± 168.7	938.5 ± 63.9	797.7 ± 47.8	923.2 ± 40.3
WizardOfWor	943.2 ± 380.3	612.0 ± 343.3	1004.3 ± 314.7	1007.4 ± 313.2
IceHockey	-17.3 ± 0.6	-15.0 ± 0.7	-16.6 ± 0.5	-12.0 ± 0.3
DoubleDunk	-18.1 ± 1.5	-16.2 ± 1.7	-17.3 ± 1.0	-16.0 ± 1.6
DemonAttack	4055.8 ± 499.7	8517.4 ± 1065.9	4062.4 ± 465.8	8396.7 ± 689.4
34
Published as a conference paper at ICLR 2022
Table F.6: Mean returns per Atari game across 5 runs with standard deviations for initial 10%
dataset. The coefficient for DR3 is 0.03 with a CQL coefficient of 0.1. The average performance is
computed over 20 checkpoints spaced uniformly over training for 200 iterations.
Game	Final Performance		Average Performance across Iterations	
	CQL	CQL + DR3	CQL	CQL + DR3
Asterix	2803.9 ± 294.6	3906.2 ± 521.3	2903.2 ± 217.7	4692.2 ± 377.0
Breakout	64.7 ± 7.3	70.8 ± 5.5	65.6 ± 5.7	75.4 ± 6.0
Pong	5.3 ± 6.8	5.5 ± 6.2	7.3 ± 5.0	8.1 ± 5.2
Seaquest	222.3 ± 219.5	1313.0 ± 220.0	704.9 ± 254.5	1327.9 ± 250.0
Qbert	4803.2 ± 489.5	5395.3 ± 1003.6	4492.5 ± 240.8	4708.5 ± 463.0
SpaceInvaders	704.9 ± 121.5	938.1 ± 80.3	737.8 ± 23.8	902.1 ± 60.0
Zaxxon	231.6 ± 450.9	836.8 ± 434.7	394.4 ± 385.1	725.7 ± 370.3
YarsRevenge	13076.2 ± 2427.0	12413.9 ± 2869.7	12493.2 ± 543.6	12395.6 ± 1044.2
RoadRunner	45063.5 ± 1749.7	45336.9 ± 1366.7	45522.7 ± 1068.1	44808.0 ± 911.7
MsPacman	2459.5 ± 381.3	2427.5 ± 191.3	2528.1 ± 149.2	2488.3 ± 109.8
BeamRider	4200.7 ± 470.2	3468.0 ± 238.0	4729.5 ± 94.8	3344.3 ± 289.0
Jamesbond	84.6 ± 25.4	89.7 ± 15.6	108.7 ± 34.1	111.7 ± 10.9
Enduro	946.7 ± 289.7	1160.2 ± 81.5	1013.9 ± 29.7	1136.2 ± 32.5
WizardOfWor	520.4 ± 451.2	764.7 ± 250.0	499.8 ± 238.5	792.2 ± 101.3
IceHockey	-18.1 ± 0.7	-16.0 ± 1.3	-17.6 ± 0.5	-15.2 ± 1.0
DoubleDunk	-21.2 ± 1.1	-20.6 ± 1.0	-20.6 ± 0.3	-19.7 ± 0.5
DemonAttack	4145.2 ± 400.6	7152.9 ± 723.2	4839.4 ± 586.7	7278.5 ± 701.3
Table F.7: Mean returns per Atari game across 5 runs with standard deviations for 1% dataset.
The coefficient for DR3 is 0.001 while we use a multi-headed REM with 200 Q-heads [2]. The aver-
age performance is computed over 20 checkpoints spaced uniformly over training for 100 iterations.
Game	Final Performance		Average Performance across Iterations	
	REM	REM + DR3	REM	REM + DR3
Asterix	240.4 ± 29.1	405.7 ± 46.5	304.4 ± 9.3	413.7 ± 39.6
Breakout	0.7 ± 0.7	14.3 ± 2.8	6.3 ± 1.0	10.3 ± 1.1
Pong	-14.2 ± 1.7	-7.7 ± 6.3	-14.1 ± 2.2	-15.3 ± 3.0
Seaquest	81.0 ± 78.5	293.3 ± 191.5	246.6 ± 49.5	489.9 ± 128.6
Qbert	239.6 ± 133.2	436.3 ± 111.5	255.5 ± 76.0	471.0 ± 116.5
SpaceInvaders	152.8 ± 27.5	206.6 ± 77.6	188.6 ± 5.8	262.7 ± 22.4
Zaxxon	534.9 ± 731.3	2596.4 ± 1726.4	1807.9 ± 478.2	707.7 ± 577.4
YarsRevenge	1452.6 ± 1631.0	5480.2 ± 962.3	4018.8 ± 987.8	7352.0 ± 574.7
RoadRunner	0.0 ± 0.0	3872.9 ± 1616.4	1601.2 ± 637.9	14231.9 ± 2406.0
MsPacman	698.8 ± 129.5	1275.1 ± 345.6	690.4 ± 69.7	860.4 ± 57.1
BeamRider	703.0 ± 97.4	522.9 ± 42.2	745.5 ± 30.7	592.2 ± 27.7
Jamesbond	41.0 ± 27.0	157.6 ± 65.0	53.3 ± 12.1	88.8 ± 27.2
Enduro	0.5 ± 0.4	132.4 ± 16.1	21.7 ±4.0	197.5 ± 19.1
WizardOfWor	362.5 ± 321.8	1663.7 ± 417.8	552.1 ± 253.1	1460.8 ± 194.8
IceHockey	-16.7 ±0.9	-9.1 ± 5.1	-12.1 ±0.8	-4.8 ± 1.8
DoubleDunk	-21.8 ± 1.0	-17.6 ± 1.5	-20.4 ± 0.6	-17.1 ± 1.6
DemonAttack	102.0 ± 17.3	162.0 ± 34.7	124.0 ± 10.7	145.6 ± 27.2
35
Published as a conference paper at ICLR 2022
Table F.8: Mean returns per Atari game across 5 runs with standard deviations for the 5%
dataset. The coefficient for DR3 is 0.001 while we use a multi-headed REM with 200 Q-heads [2].
The average performance is computed over 20 checkpoints spaced uniformly over training for 200
iterations.
Game	Final Performance		Average Performance across Iterations	
	REM	REM + DR3	REM	REM + DR3
Asterix	876.8 ± 201.1	2317.0 ± 838.1	958.9 ± 50.9	1252.6 ± 395.1
Breakout	15.2 ± 4.9	33.4 ± 4.0	16.3 ± 3.4	17.7 ± 2.4
Pong	7.5 ± 5.2	-0.7 ± 9.9	-4.7 ± 3.0	-12.0 ± 3.2
Seaquest	1276.0 ± 417.3	2753.6 ± 1119.7	1484.3 ± 367.7	1602.0 ± 603.7
Qbert	2421.4 ± 1841.8	7417.0 ± 2106.7	1330.7 ± 431.0	4045.8 ± 898.9
SpaceInvaders	431.5 ± 23.3	443.5 ± 67.4	349.5 ± 22.6	362.1 ± 33.6
Zaxxon	6738.2 ± 966.6	1609.7 ± 1814.1	3630.7 ± 751.4	346.1 ± 512.1
YarsRevenge	14454.2 ± 1644.4	16930.4 ± 2625.8	14628.3 ± 1945.1	12936.5 ± 1286.0
RoadRunner	15570.9 ± 12795.6	46601.6 ± 2617.2	22740.3 ± 1977.2	33554.1 ± 1880.4
MsPacman	1272.2 ± 215.3	2303.1 ± 202.7	1147.7 ± 126.1	1438.7 ± 140.4
BeamRider	1922.5 ± 589.1	674.8 ± 21.4	886.9 ± 82.1	698.3 ± 21.5
Jamesbond	189.6 ± 77.0	130.5 ± 45.7	120.2 ± 9.3	88.6 ± 41.5
Enduro	172.7 ± 55.9	583.9 ± 108.7	236.8 ± 11.3	457.7 ± 39.3
WizardOfWor	838.4 ± 670.0	2661.6 ± 371.4	1281.3 ± 66.7	1863.7 ± 261.2
IceHockey	-9.7 ± 4.2	-6.5 ± 3.1	-8.1 ± 0.7	-4.1 ± 1.5
DoubleDunk	-18.4 ± 0.9	-17.6 ± 2.6	-19.6 ± 1.0	-17.8 ± 1.9
DemonAttack	507.7 ± 120.1	5602.3 ± 1855.5	581.6 ± 207.0	1452.3 ± 765.0
Table F.9: Mean returns per Atari game across 5 runs with standard deviations for initial 10%
dataset. The coefficient for DR3 is 0.001 while we use a multi-headed REM with 200 Q-heads [2].
The average performance is computed over 20 checkpoints spaced uniformly over training for 200
iterations.
Game	Final Performance		Average Performance across Iterations	
	REM	REM + DR3	REM	REM + DR3
Asterix	2254.7 ± 403.6	5122.9 ± 328.9	2684.6 ± 184.4	3432.1 ± 257.5
Breakout	81.2 ± 13.9	96.8 ± 21.2	63.5 ± 4.6	62.4 ± 6.1
Pong	8.8 ± 3.1	7.6 ± 11.1	2.6 ± 2.1	-2.5 ± 5.6
Seaquest	1540.2 ± 354.6	981.3 ± 605.9	1029.5 ± 260.6	836.2 ± 234.3
Qbert	4330.7 ± 250.2	4126.2 ± 495.7	3478.0 ± 248.0	3494.7 ± 380.3
SpaceInvaders	895.2 ± 68.3	799.0 ± 28.3	699.7 ± 31.4	653.1 ± 21.5
Zaxxon	950.7 ± 897.4	0.0 ± 0.0	490.2 ± 306.6	0.0 ± 0.0
YarsRevenge	10913.1 ± 1519.1	11924.8 ± 2413.8	11508.5 ± 290.0	10977.7 ± 1026.9
RoadRunner	45521.7 ± 2502.1	49129.4 ± 1887.9	37997.4 ± 638.6	41995.2 ± 1482.1
MsPacman	2177.4 ± 393.0	2268.8 ± 455.0	1930.5 ± 141.7	2126.6 ± 147.6
BeamRider	2921.7 ± 308.7	4154.9 ± 357.2	3727.5 ± 304.3	2871.0 ± 44.3
Jamesbond	197.8 ± 73.8	149.3 ± 304.5	149.0 ± 120.5	83.3 ± 162.4
Enduro	529.5 ± 200.7	832.5 ± 65.5	584.6 ± 85.3	801.8 ± 39.3
WizardOfWor	606.5 ± 823.2	920.0 ± 497.0	838.3 ± 343.7	926.3 ± 318.5
IceHockey	-4.3 ± 0.6	-5.9 ± 5.1	-7.0 ± 1.1	-5.4 ± 3.7
DoubleDunk	-17.7 ± 3.9	-19.5 ± 2.5	-16.9 ± 0.5	-16.7 ± 1.0
DemonAttack	6097.9 ± 1251.3	9674.7 ± 1600.6	4649.1 ± 514.6	5141.9 ± 361.4
36
Published as a conference paper at ICLR 2022
Table F.10: Average returns across 5 runs for the random agent and the average performance of
the trajectories in the DQN (Nature) dataset. For Atari normalized scores reported in the paper, the
random agent is assigned a score of0 while the average DQN replay is assigned a score of 100. Note
that the random agent scores are also evaluated on Atari 2600 games with sticky actions.
Game	Random	Average DQN-Replay
Asterix	279.1	3185.2
Breakout	1.3	104.9
Pong	-20.3	14.5
Seaquest	81.8	1597.4
Qbert	155.0	8249.7
SpaceInvaders	149.5	1529.8
Zaxxon	10.6	1854.1
YarsRevenge	3147.7	21015.0
RoadRunner	15.5	38352.3
MsPacman	248.0	3108.8
BeamRider	362.0	4576.4
Jamesbond	27.6	560.3
Enduro	0.0	671.9
WizardOfWor	686.6	1128.5
IceHockey	-9.8	-8.5
DoubleDunk	-18.4	-11.3
DemonAttack	166.0	4407.5
37
Published as a conference paper at ICLR 2022
F.1 Per-Game Learning Curves for Atari Games
---CQL ------- CQL + DR3
Asterix
3」0。S 36e,l3><
Beam Rider
Breakout
Oooooo
5 0 5 0 5
2 2 11
0	50	100	150	200
Gradient Updates (x 62.5k)
0	50	100	150	200
Gradient Updates (x 62.5k)
0	50	100	150	200
Gradient Updates (x 62.5k)
a>」0。S ΦCTSΦ><
al。。。ΦCTSΦ><
50	100	150	200
Gradient Updates (x 62.5k)
0	50	100	150	200
Gradient Updates (x 62.5k)
5000-
0	50	100	150	200
Gradient Updates (x 62.5k)
Ms. Pac-Man
4000-
3000-
2000-
1000-
0-
0	50	100	150	200
Gradient Updates (x 62.5k)
3」0。S 36e,l3><
0	50	100	150	200
Gradient Updates (x 62.5k)
SeaqUest
8000-
0	50	100	150	200
Gradient Updates (x 62.5k)
Space Invaders
0	50	100	150	200
Gradient Updates (x 62.5k)
Wizard Of Wor
0	50	100	150	200
Gradient Updates (X 62.5k)
50	100	150	200
Gradient Updates (x 62.5k)
al。。。ΦCTSΦ><
Figure F.1: Per-game learning curves of CQL and CQL + DR3 on the 5% uniform replay dataset, for
which the normalized average learning curve is shown in Figure 4. Note that CQL + DR3 attains a higher
performance than CQL for a majority of games, and rises up to a higher peak.
38
Published as a conference paper at ICLR 2022
3」0。S 36e,l3><
0>」0。S ΦCTSΦ><
---REM ------ REM + DR3
ICe Hockey
James Bond
0 5 0 5 0
-112
- - -
3」0。S 36e,l3><
3000-
2000-
1000-
0-
50	100	150	200
Gradient Updates (X 62.5k)
a>」0。S ΦCTra⅛><
Yars's Revenge	Zaxxon
8000」
0i__________I_________I_________I_________I
0	50	100	150	200
Gradient Updates (x 62.5k)
0	50	100	150	200
Gradient Updates (x 62.5k)
Figure F.2: Per-game learning curves of REM and REM + DR3 on the 5% uniform replay dataset, for
which the normalized average learning curve is shown in Figure 4. Note that REM + DR3 attains a higher
performance than REM for a majority of games.
39
Published as a conference paper at ICLR 2022
F.2 Dot Product Similarities For CQL+DR3 and REM+DR3 on 17 Games
---CQL ------- CQL + DR3
至」®一UJ_s-δnpo⅛Joα
1	2	3	4	5
Gradient Updates (x 62.5k)	1e7
Demon Attack
0	12	3	4
Gradient Updates (x 62.5k)
3 2 10 1
Oooo-)
IIIIO
UB=E-Snp2CJ
0	1	2	3	4	5
Gradient Updates (x 62.5k)	1e7
Double Dunk
4	Enduro
10 :
;---------------"""u"
10:
10 :
4 2 0
Ooo
111
UB=E-S Jonpo」
)1	2	3	4	5
Gradient Updates (x 62.5k)	1e7
Ice Hockey
J)I(IR1∙wj≡∙“e∣≡∣w"π""—
Ms. Pac-Man
104 -
:
103；
102
0	12	3	4
Gradient Updates (x 62.5k)

0	1	2	3	4	5
Gradient Updates (x 62.5k)	1e7
0	1	2	3	4	5
Gradient Updates (x 62.5k)	1e7
0	1	2	3	4	5
Gradient Updates (x 62.5k)	1e7
-
O
一」B=E-Snp2CJ
4 3 2 1 0
Ooooo
11111
Q*Bert	Road Runner
0	1	2	3	4	5
Gradient Updates (x 62.5k)	1e7
0	1	2	3	4	5
Gradient Updates (x 62.5k)	1e7
0	1	2	3	4	5
Gradient Updates (x 62.5k)	1e7
-UB=E一S,snp2'5CJ
4 3 2
Ooo
111
Io Oo
SeaqUest
1	2	3	4	5
Gradient Updates (X 62.5k)	1e7
4	Yars's Revenge
10	......................
2 -u
O O
UB=E-Snp2dJ0
MlIφlj∣∣∣∣∣⅛(IjjHIWJWWIIlWWto**∙rt∣⅛rt⅛*4*tf
0	1	2	3	4	5
Gradient Updates (x 62.5k)	1e7
0	1	2	3	4	5
Gradient Updates (x 62.5k)	1e7
Figure F.3: Per-game feature dot products (in log scale) of CQL and CQL + DR3 on the 5% uniform
replay dataset. Note that CQL + DR3 attains a smaller value of the feature dot product.
40
Published as a conference paper at ICLR 2022
---REM ------ REM + DR3
4 3 2 1
Oooo
1111
μE=UJnp2dJ0Cl
Breakout
101
100

1	2	3	4	5
Gradient Updates (x 62.5k)	1e7
Demon Attack
0	1	2	3	4	5
Gradient Updates (x 62.5k)	1e7
Double Dunk
0	12	3	4
Gradient Updates (x 62.5k)
Uooo
— 111
-M-E-S^np。」5Cl
0	1	2	3	4	5
Gradient Updates (x 62.5k)	1e7
James Bond
.-3 ^
10
10%
101 ；
Enduro
0	1	2	3	4	5
Gradient Updates (x 62.5k)	1e7
0	1	2	3	4	5
Gradient Updates (x 62.5k)	1e7
至」®一UJ_s-δnpo⅛Joα
Ms. Pac-Man
104:
103-
0	1	2	3	4	5
Gradient Updates (X 62.5k)	1e7
Pong
0	1	2	3	4	5
Gradient Updates (x 62.5k)	1e7
0	1	2	3	4	5
Gradient Updates (x 62.5k)	1e7
2 10 12
1o0o1°一10一10
UB=E-Snp2CJ
SeaqUest
Space Invaders
A⊂B=UJ -s-Gnpo⅛Joα
104
106
104
102
100
0	1	2	3	4	5
Gradient Updates (x 62.5k)	1e7
0	1	2	3	4	5
Gradient Updates (x 62.5k)	1e7
0	1	2	3	4	5
Gradient Updates (x 62.5k)	1e7
至」®一UJ_s-δnpo⅛Joα
Figure F.4: Per-game feature dot products (in log scale) of REM and REM + DR3 on the 5% uniform
replay dataset Note that REM + DR3 attains a higher performance than REM for a majority of games. Note
that the dot products for REM+DR3 stabilize are small, and decreases for a majority of the training steps for a
number of games, or stabilize at a small value.
41