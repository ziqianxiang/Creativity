Published as a conference paper at ICLR 2022
Meta Discovery: Learning to Discover Novel
Classes given Very Limited Data
Haoang Chi1,2* Feng Liu3* Bo Han2 Wenjing YangIt Long LanIt Tongliang Liu4
Gang Niu5 Mingyuan Zhou6 Masashi Sugiyama5,7
1National University of Defense Technology 2 Hong Kong Baptist University
3University of Technology Sydney 4The University of Sydney 5RIKEN AIP
6The University of Texas at Austin 7The University of Tokyo
{haoangchi618,fengliu.ml,gang.niu.ml}@gmail.com, bhanml@comp.hkbu.edu.hk,
{wenjing.yang,long.lan}@nudt.edu.cn, tongliang.liu@sydney.edu.au,
mingyuan.zhou@mccombs.utexas.edu, sugi@k.u-tokyo.ac.jp
Ab stract
In novel class discovery (NCD), we are given labeled data from seen classes and
unlabeled data from unseen classes, and we train clustering models for the unseen
classes. However, the implicit assumptions behind NCD are still unclear. In this
paper, we demystify assumptions behind NCD and find that high-level semantic
features should be shared among the seen and unseen classes. Based on this finding,
NCD is theoretically solvable under certain assumptions and can be naturally
linked to meta-learning that has exactly the same assumption as NCD. Thus, we
can empirically solve the NCD problem by meta-learning algorithms after slight
modifications. This meta-learning-based methodology significantly reduces the
amount of unlabeled data needed for training and makes it more practical, as
demonstrated in experiments. The use of very limited data is also justified by the
application scenario of NCD: since it is unnatural to label only seen-class data,
NCD is sampling instead of labeling in causality. Therefore, unseen-class data
should be collected on the way of collecting seen-class data, which is why they are
novel and first need to be clustered.
1	Introduction
With the development of high-performance computing, we can train deep networks to achieve various
tasks well (Deng et al., 2009; Liu & Tao, 2015; Song et al., 2019; Jing et al., 2020; Song et al.,
2020; Liu et al., 2020b; Han et al., 2020a; Xia et al., 2020). However, the trained networks can
only recognize the classes seen in the training set (i.e., known/seen classes), and cannot identify and
cluster novel classes (i.e., unseen classes) like human beings. A prime example is that human can
easily tell a novel animal category (e.g., okapi) after learning a few seen animal categories (e.g., horse
and dog). Namely, human can effortlessly discover (cluster) novel categories of animals. Inspired by
this fact, previous works formulated a novel problem called novel class discovery (NCD) (Hsu et al.,
2018; Han et al., 2019), where we train a clustering model using plenty of unlabeled novel-class and
labeled known-class data.
However, if NCD is labeling in causality (X → Y ), there exists two issues: NCD might not be a
theoretically solvable problem. For example, if novel classes are completely different from known
classes, then it is unrealistic to use the known classes (like animals) to help precisely cluster novel
classes (like cars, Figure 1(a)). Moreover, NCD might not be a realistic problem in some scenarios
where novel classes might only be seen once or twice. This does not satisfy the assumptions
considered in existing NCD methods. These issues naturally motivate us to find out when NCD can
be theoretically solved and what assumptions are considered behind NCD.
In this paper, we revisit NCD and find that NCD will be a well-defined problem if NCD is sampling
in causality (Y → X), i.e., novel and known classes are sampled together (Figure 1(b)). In this
* Equal contribution. Work done when Haoang Chi remotely visited HKBU.
,Corresponding author.
1
Published as a conference paper at ICLR 2022
Horse	Bird
Fish
!「------1	「-----1
∣⅛orse	B Bird ∣
I_______I	I______I
IΓ------1	Γ-----1
I Cat I	I Fish I
I 1	I I
Cat
(a) Experts annotate data (labelling in causality). (b) Experts sample data (sampling in causality).
Figure 1: NCD aims to discover novel classes (i.e., clustering novel-class data) with the help of labeled
known-class data. There exists two ways to obtain data in NCD: (a) labeling in causality, e.g., we first obtain
unlabeled images and then hire experts to label them and (b) sampling in causality, e.g., we are given a label
set, and then sample images regarding these labels. In (a), experts have to go through all images and find
out novel classes. However, the novel classes (like cars) might be totally different from known classes (like
animals), which makes NCD become a theoretically unsolvable problem. In this paper, we revisit NCD from (b),
where novel-class data are collected on the same way of sampling known-class data. In this view, NCD can be
theoretically solved, since novel classes and known classes are highly related. The yellow rectangles represent
the identified novel classes.
sampling process, data are often obtained because of a given purpose, and novel classes and known
classes are obtained in the same scenario. For instance, botanists sample plant specimens for research
purposes in the forests. Except for the plants they are interested in (i.e., known classes), they also find
scarce plants never seen before (i.e., finding novel classes). Since a trip to forests is relatively costly
and toilsome, botanists had better sampled these scarce plants passingly for future research. From
this example, it can be seen that botanists will have plenty of labeled data with known classes, and
few unlabeled data with novel classes. Since both data are sampled together from the same scenario
(e.g., plants in the forests), it is reasonable to leverage knowledge of known classes to help cluster
novel classes, which is like “discovering novel categories” happened in our daily life.
Therefore, we argue that the key assumption behind NCD is that, known classes and novel classes
share high-level semantic features. For example, known classes and novel classes are different plants
but all of them have the leaf, the stem, and the roots. Then, we reformulate the NCD problem and
show that NCD is theoretically solvable under mild assumptions (including the key assumption
above). We also show an impossibility theorem for previous NCD setting. This theorem shows that it
might not be necessary to introduce known-class data to help cluster novel-class data if known classes
and novel classes do not share high-level semantic features. Namely, NCD might be an ill-defined
problem if known and novel class do not share high-level semantic features.
Although NCD is theoretically solvable under mild assumptions, we still need abundant data from
known and novel classes to ensure NCD can be empirically solved. However, as we mentioned
previously, the novel classes might only be seen once or twice in some scenarios. In such scenarios,
we find that previous NCD methods do not work well (Figure 2). To address the NCD given very
limited data (NCDL), we link NCD to meta-learning that also assumes that known and unknown
(novel) classes share the high-level semantic features (Maurer, 2005; Chen et al., 2020a).
The key difference between meta-learning and NCDL lies in their inner-tasks. In meta-learning, the
inner-task is a classification task, while in NCDL, it is a clustering task. Thus, we can modify the
training strategies of the inner-tasks of meta-learning methods such that they can discover novel
classes, i.e., meta discovery. Specifically, we first propose a novel method to sample training
tasks for meta-learning methods. In sampled tasks, labeled and unlabeled data share high-level
semantic features and the same clustering rule (Figure 3). Then, based on this novel sampling
method, we realize meta discovery using two representative meta-learning methods: model-agnostic
meta-learning (MAML) (Finn et al., 2017) and prototypical network (ProtoNet) (Snell et al., 2017).
Figure 2 demonstrates two realizations of meta discovery (i.e., meta discovery with MAML (MM)
and meta discovery with ProtoNet (MP)) can perform much better than existing methods in NCDL.
We conduct experiments on four benchmarks and compare our method with five competitive baselines
(MacQueen et al., 1967; Hsu et al., 2018; 2019; Han et al., 2019; 2020b). Empirical results show
that our method outperforms these baselines significantly when novel-class data are very limited.
2
Published as a conference paper at ICLR 2022
(a) 5-way 5-observation
——MP -→- MM →- RS →- KCL	MCL →- DTC
Figure 2： We conducted experiments on CIFAR-10 and SVHN and reported the average clustering accuracy
(ACC (%), Section 5) when using existing and our methods to address the NCDL. We can observe that existing
methods cannot address the NCDL, while our methods (MM and MP) can address the NCDL well.
SVHN
Epoch
(d) 5-way 1-observation
Moreover, we provide a new practical application to the prosperous meta-learning community (Chen
et al., 2020a), which lights up a novel road for NCDL.
2	Related Work
Our proposal is mainly related to NCD, transfer learning and meta-learning that are briefly reviewed
here. More detailed reviews can be found in Appendix A.
Novel class discovery. NCD was proposed in recent years, aiming to cluster unlabeled novel-class
data according to underlying categories. Compared to unsupervised learning (Barlow, 1989), NCD
also requires labeled known-class data to help cluster novel-class data. The pioneering methods
include the KL-Divergence-based contrastive loss (KCL) (Hsu et al., 2018), the meta classification
likelihood (MCL) (Hsu et al., 2019), deep transfer clustering (DTC) (Han et al., 2019), and the
rank statistics (RS) (Han et al., 2020b). Compared to existing works (Hsu et al., 2018; Zhong et al.,
2021c;b), we focus on clustering unlabeled data when their quantity is very limited.
Transfer learning. Transfer learning aims to leverage knowledge contained in source domains to
improve the performance of tasks in a target domain, where both domains are similar but different
(Gong et al., 2016). Representative transfer learning works are domain adaptation (Long et al., 2018)
and hypothesis transfer (Liang et al., 2020), where mainly focus on classification or prediction tasks
in the target domain. NCD problem can be also regarded a transfer learning problem that aims to
complete the clustering task in a target domain via leveraging knowledge in source domains.
Meta-learning. Meta-learning is also known as learning-to-learn, which trains a meta-model over a
wide variety of learning tasks (Ravi & Larochelle, 2017; Chen et al., 2020b; Yao et al., 2021; Wei
et al., 2021). In meta-learning, we often assume that data share the same high-level features, which
ensures that meta-learning can be theoretically addressed (Maurer, 2005). According to Hospedales
et al. (2020), there are three common approaches to meta-learning： optimization-based (Finn et al.,
2017), model-based (Santoro et al., 2016), and metric-based (Snell et al., 2017). In Jiang & Verma
(2019), researchers trained a recurrent model that learns how to cluster given multiple types of
training datasets. Compared to our meta discovery, Jiang & Verma (2019) learn a clustering model
with multiple unlabeled datasets, while meta discovery aims to learn a clustering model with labeled
known data and (abundant or few) unlabeled novel data from the same dataset.
3	Assumptions behind NCD and Analysis of Solvability
Since NCD focuses on clustering novel-class data, we first show definitions regarding the separation
of a random variable (r.v.) X 〜PX defined on X ⊂ Rd. Then, we give a formal definition of NCD
and introduce assumptions behind NCD. Finally, we present one theorem to show NCD is solvable in
theory and one theorem to show a failure situation where previous setting encounters. The proofs of
both theorems can be seen in Appendix B.
Definition 1 (K-e-Separable r.v.). Given the r.v. X 〜PX, X is K - e-separable With a non-empty
function set F = {f : X → I} if ∀f ∈ F
τ(X, f(X)) := max PX(RX|f(X)=i ∩ RX|f(X)=j) = e,
i,j ∈I,i6=j
(1)
3
Published as a conference paper at ICLR 2022
Color	Frame	We can cluster these objects using at least
three rules, i.e., colors, shapes and frames.
Motivated by the rule of clustering, when ad-
dressing the NCDL problem, we need to sam-
Ple inner-tasks, where data share the same
Shape	rule (Algorithm 1).
Figure 3: When sampling tasks for meta discovery, we need to care about clustering rules.
where I = {i1, . . . , iK} is an index set, f(X) is an induced r.v. whose source of randomness is X
exclusively, and RX|f(X)=i = supp(PX |f (X)=i) is the support set of PX|f(X)=i.
In equation 1, T(∙, ∙) represents the largest overlap between any different clusters in the sense of the
probability measure PX . If = 0, then we can perfectly partition observations of X into K clusters
using some distance-based clustering algorithm, e.g., K-means (MacQueen et al., 1967). However,
when X is a complex space and the dimension d is much larger than the intrinsic dimension of X
(e.g., images), it is not reliable to measure the distance between observations from X using original
features (Liu et al., 2020b), which will result in poor clustering performance.
Non-linear transformation. To overcome issues caused by the complex space, researchers suggest
apply a non-linear transformation to extract high-level features of X (Fang et al., 2020; Liu et al.,
2020b). Based on these features, we can measure the distance between two observations well (Liu
et al., 2020b). Let π : X → Rdr be a transformation where dr is the reduced dimension and dr d,
and we expect that the transformed r.v. π(X) can be K--separable as well. Namely, we hope the
following function set exists.
Definition 2 (Consistent K--separable Transformation Set). Given r.v. X that is K--separable
with a function set F, a transformed r.v. π(X) is K --separable with F if ∀f ∈ F,
T(π(X),f (X)) := . maXz ∙P∏(X)(Rn(X)If(X)=i ∩ R∏(X)∣f(X)=j) = e,	⑵
i,j∈I,i6=j
where π : X → Rdr is a transformation. Then, a consistent K -e-separable transformation set is a
non-empty set Π satisfying that ∀f ∈ F, ∀π ∈ Π, T(π(X), f(X)) = e.
Remark 1. If dr d and e = 0, we will need much less observations to estimate the density of
π(X) compared to X, thus it will be much easier to perfectly separate π(X) than X. For example,
there probably exists a linear function g : Rdr → I such that f(X) = g ◦ π(X). It is clear that such
linear function g is easier to find than directly finding f using K-means.
Problem Setup of NCD. Based on the above definitions, we will formally define the NCD problem
below. In NCD, we have two r.v.s Xl, Xu defined on X, the ground-truth labeling function fl : X →
Y for Xl and a function set F = {f : X → I}, where Y = {il1, . . . , ilKl} and I = {iu1, . . . , iuKu}.
Based on Definitions 1 and 2, we have the following assumptions in NCD.
(A)	The support set of Xl and the support set of Xu are disjoint, and underlying classes of Xl
are different from those of XU (i.e., I ∩ Y = 0);
(B)	Xl is Kl-el-separable with Fl = {fl} and Xu is Ku-eu-separable with Fu, where el =
T(Xl,fl(Xl)) < 1andeu = minf∈F T(Xu, f(Xu)) < 1;
(C)	There exist a consistent Kl-el-separable transformation set Πl for Xl and a consistent
Ku-eu-separable transformation set Πu for Xu;
(D)	Πl ∩ Πu 6= 0.
(A) ensures that known and novel classes are disjoint. (B) implies that it is meaningful to separate
observations from Xl and Xu. (C) means that we can find good high-level features for Xl or Xu.
Based on these features, it is much easier to separate Xl or Xu . (D) says that the high-level features
of Xl and Xu are shared, as demonstrated in the introduction. Then, we can define NCD formally.
Problem 1 (NCD). Given Xl, Xu and fl defined above and assume (A)-(D) hold, in NCD, we aim
to learn a function ∏ : X → Rdr via minimizing J (∏) = T (π(Xl), fl(X l)) + T (∏(Xu), f u(X u)),
where fu ∈ F and dr《d. We expect that π(Xu) is KU-eu-separable.
Theorem 1 (NCD is Theoretically Solvable). Given Xl, Xu and fl defined above and assume (A)-(D)
hold, then π(Xu) is KU-eu-separable. If eu = 0, thenNCD is theoretically solvable.
4
Published as a conference paper at ICLR 2022
Theorem 1	means that it is possible to find a good transformation ∏ such that Π(XU) is separable
although we introduce Xl1. Then we show that NCD might be ill-defined if (D) does not hold.
Theorem 2	(Impossibility Theorem). Given Xl, Xu and fl defined above and assume (A)-(C) hold,
if maxπ∈Πl τ(π(Xu), fu(Xu)) < minπ∈Π-Πl τ(π(Xl), fl(Xl)), (D) does not hold and l ≤ u, then
T(Π(XU),fu(Xu)) > eu, where Π = {π : X → Rd。} and fu ∈ F.
Remark 2. Condition maxπ∈Πl τ (π(Xu), fu(Xu)) < minπ∈Π-Πl τ(π(Xl), fl(Xl)) indicates that
the worst case of clustering novel classes with transformations that are suitable for known classes is
better than the best case of clustering known classes with transformations that are not suitable for
known classes. Besides, el ≤ eu indicates that the transformations in Xl is more separable than Xu.
In previous setting, data acquiring process is unclear. As we discussed, if NCD is labelling in
causality, then novel and known classes might not share high-level semantic features (Figure 1(a)).
Namely, (D) might not hold, resulting in that Xl might bring negative effects under some conditions
(Theorem 2). Thus, based on Theorems 1 and 2, it is clear that (D) plays a key role in NCD, which
also justifies that NCD will be well-defined if it is sampling in causality. Noting that sampling in
causality (Y → X) is the sufficient unnecessary condition of (D). For example, if there are some
constraints to make the novel-class data (to be annotated) are obtained in the same scenario with
known-class data, then data generated by labeling (X → Y ) also satisfy (D).
Based on Theorem 1, if there are abundant observations to estimate J(∏), then we can find the
optimal transformation ∏ to help obtain a good partition for observations from Xu. However, as
discussed before, the novel classes might only be seen once or twice in some scenarios. In such
scenarios, we find that previous NCD methods do not work well empirically (Figure 2). To address
the NCD given very limited data (NCDL), we link NCD to meta-learning that also assumes that
known and unknown (novel) classes share the high-level semantic features (Chen et al., 2020a), which
is exactly the same as (D) in NCD. Thus, it is natural to address NCDL by meta-learning.
4 Meta Discovery for NCDL
Meta-learning has been widely used to solve few-shot learning problems (Chen et al., 2020a; Wang
et al., 2020). The pipeline of meta-learning consists of three steps: 1) randomly sampling data to
simulate many inner-tasks; 2) training each inner-task by minimizing its empirical risk; 3) regarding
each inner-task as a data point to update meta algorithm. Compared to meta-learning, the inner-task of
NCDL is clustering instead of classification in meta-learning. Thus, we can modify the loss function of
inner-task to be suitable for clustering and follow the framework of meta-learning, i.e., meta discovery.
In Appendix C, we give NCDL a definition from the view of meta-learning and further prove NCDL
is theoretically solvable. In this section, we let Sl = {(xli, yil) : i = 1, . . . , nl} be known-class data
drawn from r.v. (Xl, fl(Xl)) and yil ∈ {1, . . . , Kl}, and let Su = {xiu : i = 1, . . . , nu} be unlabeled
novel-class data drawn from r.v. Xu, where 0 < nu nl.
Due to the difference of inner-task, if we randomly sample data to compose an inner-task like existing
meta-learning methods, these data may negatively influence each other in the training procedure. This
is because these randomly sampled data in an inner-task have different clustering rules (Figure 3).
Thus, in meta discovery, the key is to propose a new task sampler that takes care of clustering rules.
CATA: Clustering-rule-aware Task Sampler. From the perspective of multi-view learning (Blum
& Mitchell, 1998), data usually contain different feature representations. Namely, data have multiple
views. However, there are always one view or a few views that are dominated for each instance,
and these dominated views are similar with high-level semantic meaning (Li et al., 2019b; Liu
et al., 2020d; 2021a). Therefore, we propose to use dominated views to replace with clustering
rules, and design a novel task-sampling method called clustering-rule-aware task sampler (CATA,
Algorithm 1). CATA is based on a multi-view network containing a feature extractor G : X → Rds
and M classifiers {Fi : Rds → Y}iM=1 (Figure 4). M is empirically chosen according to the data
complexity. Specifically, CATA learns a low-dimension projection and set of orthogonal classifiers.
It then assigns each data point to a group defined by which of the orthogonal classifiers was most
strongly activated by the observation, and inner tasks are sampled from each group.
1Note that, the objective of clustering problem is different from that of NCD. In clustering problem, we aim
to find ∏ via minimizing τ(Π(Xu), fu(XU)) rather than τ(Π(Xl), fl(Xl)) + τ(Π(Xu), f U(Xu)).
5
Published as a conference paper at ICLR 2022
Sl
Well-trained
CATA is a novel sampling method
of meta-learning for NCDL. Here
we show the inference process of
assigning labeled data of known
classes to three different views
with the well-trained G and
{Fi}i3=1. V (x) is the voting
function defined as Eq. (4). The
weights of the first layers of F1 ,
F2 , and F3 are constrained to or-
thogonal mutually.
Figure 4: The structure of the clustering-rule-aware task sampler (CATA).
The feature extractor G provides the shared data representations for M different classifiers {Fi}iM=1.
Each classifier classifies data based on its own view. The feature extractor G learns from all gradients
from {Fi}iM=1. To ensure that different classifiers have different views, we constrain the weight vector
of the first fully connected layer of each classifier to be orthogonal. Take Fi and Fj as an example,
we add the term |WiT Wj | to the sampler’s loss function, where Wi and Wj denote the weight vectors
of the first fully connected layer of Fi and Fj respectively. |WiT Wj | tending to 0 means that Fi and
Fj are nearly independent (Saito et al., 2017). Thus, the loss function of CATA is defined as follows,
1 M N	2λ
Ls(θG, {θFi }M=1) = MN XX 'ce(Fj ◦ G(Xi), yi) + M (M- 1) X IWiT Wj∣,⑶
j=1 i=1	i6=j
where 'æ is the standard cross-entropy loss function and λ is a trade-off parameter.
After we obtain the well-trained feature extractor G and classifiers {Fi}iM=1, we input a training data
point x to our sampler and then we will get the probabilities that x belongs to class y in each classifier,
i.e. {Pi(y|x)}iM=1, where y is the label ofx. Therefore, the view which x belongs to is defined as
V (x) = arg maxPi(y|x).	(4)
i
Now we have assigned a data point to M subsets according to their views, i.e. {Vi = {x ∈ X :
V (x) = i}}iM=1. Then, we can directly randomly sample a certain number of data (e.g., N -way,
K-observation) from one subset to compose an inner task. According to the number of data in
each subset, we sample inner tasks from each subset with different frequencies. We also compare
CATA with commonly used samplers in meta-learning in Appendix A. Note that, CATA is a heuristic
method, and we will give clustering rule a formal definition and explore the reason why CATA
succeed theoretically in the future.
Realization of Meta Discovery with MAML (MM). Here, we solve the NCDL problem based on
MAML. A feature extractor πmm : X → Rdr is given to obtain an embedding of data, following a
classifier g with the output dimension equalling to the number of novel classes (Ku). As novel classes
share the same high-level semantic features of known classes, the feature extractor πmm should be
applicable to known and novel classes. The key idea is that similar data should belong to the same
class. For data pair (xi, xj), let sij = 1 if they come from the same class; otherwise, sij = 0.
Following Han et al. (2020b), we adopt a more robust pairwise similarity called ranking statistics.
For zi = πmm(xi) and zj = πmm(xj), we rank the values of zi and zj by the magnitude. Then we
check if the indices of the values of top-k ranked dimensions are the same. Namely, sij = 1 if they
are the same, and sij = 0 otherwise. We use the pairwise similarities {sij }1≤i,j≤nl as pseudo labels
to train feature extractor πmm and classifier g. As mentioned above, g is a classifier with softmax
layer, so the inner product g(zi)Tg(zj ) is the cosine similarity between xi and xj , which serves as
the score for whether xi and xj belong to the same class. After sampling training tasks {Ti}in=1, we
train a model by the inner algorithm that optimizes the binary cross-entropy (BCE) loss function:
1 nl nl
LTi (%Onmm) =------12	ɪs[sij lθg(g(zi)Tg(Zj)) + (I-Sij) IOg(I - g(Zi)Tg(Zj ))].	(5)
6
Published as a conference paper at ICLR 2022
Algorithm 1 Clustering-rule-aware task sampler (CATA)
Input: known-class data S* l, feature extractor G, classifiers {Fi}iM=l1, learning rates ω1, ω2; view index: j.
1:	Initialize θG and {θFi }iM=l1;
for t = 1, . . . , T do
2:	Compute VθgLS and {Vθf, Ls}Mι using Sl and LS in Eq. (3);
3:	UpdateΘg = Θg 一ωιVθGLs(Θg, {θFi}Mι),。号=。&—ω2V$营电LS(Θg, {θpi }K==ι), i = 1,..., Ml;
end
4:	Compute {Fi(G(xl))}iM=l1 to obtain {Pi(yl|xl)}iM=l1 for each (xl, yl) ∈ Sl;
5:	Compose Vi = {xl : V (xl) = i} using function V in Eq. (4), i = 1, . . . , Ml;
6:	Sample an inner-task Ti = (Si,tr, S;ts)〜Vj
Output: an inner-task Ti
Algorithm 2 MM for NCDL.
Input: known-class data: Sl ; learning rate: α, η; feature extractor: πmm ; classifier: g
1:	Initialize θg°∏mm;
while not done do
2:	Sample tasks {Ti = (S|，tr, Sys)〜Sl}n=I by CATA (Alg.1);
for all Ti do
3:	EValUate vθg◦∏mm LTi (θg◦∏mm ) USing ^," and LTi in EQ∙ (5)：
4:	ComPUte adapted parameters: θgo∏mm,i = θg°∏mm — αVθg°∏mm LTi (θg°∏mm )：
end
5:	UPdate θg°∏mm = θg°∏mm - 〃Sg◦ ∏.m LA d∏mm ) using each ^," and LA in EQ∙⑹；
end
Output: clustering algorithm A.
Entire procedures of NCDL by MAML are shown in Algorithm 2. Following MAML, the parameters
of clustering algorithm A are trained by optimizing the following loss function:
n
LA (θ9°∏mm ) = ELTi (θ9°∏mm - α^θg^∏mm LTi ∕g0∏mm D,	⑹
i=1
where α > 0 is the learning rate of the inner-algorithm. Then we conduct the meta-optimization to
update the parameters of clustering algorithm A as follows:
θg0∏mm J θgO∏mm - ηVθg◦∏mm LA(θgθ∏mm ),
(7)
where η > 0 denotes the meta learning rate. After finishing meta-optimization, we finetune the
clustering algorithm A with the novel-class data to yield a new clustering algorithm that is adapted to
novel classes. More specifically, we perform line 4 and line 5 in Algorithm 2 with Su.
Realization of Meta DiscoVery with ProtoNet (MP). Following Snell et al. (2017), we denote πmp
as a feature extractor, which maps data to their representations. In training task Ti , the mean vector
of representations of data from class-s (i.e., Sil,,tsr) is defined as prototype ck :
1
ci,s (Sil,,tsr) =
πmp(xli).
(xli,yil)∈Sil,,tsr
(8)
Here, we define the Euclidean distance dist : Rdr × Rdr → [0, +∞) to measure the distance between
data from the test set and the prototype. Then, we represent p(y = s|x) using the following equation.
exp(-dist(πmp(x), cs))
Py 5 P	PSO exp(-dist(∏mp(x), Cs0)) .
(9)
We train the feature extractor by optimizing the negative log-probability, i.e., - log p(y = s|x). So
the loss function of ProtoNet is defined as follows:
LTi = -k X X logp(y = six),
s∈[Ku] x∈Sl,ts
*. a
(10)
7
Published as a conference paper at ICLR 2022
Algorithm 3 MP for NCDL.
Input: known-class data: Sl ; learning rate: γ; feature extractor: πmp
1:	Initialize θπmp ;
while not done do
for all episodes do
2:	Sample Ku elements from {1, . . . , Kl} as set CI;
3:	Sample tasks Ti = (S1,tr, S>ts)〜Sllyι∈cI by CATA (Alg. 1);
for s in CI do
4:	Sample m training data of class-s, i.e., S;Sr 〜S>tr & ∣S;Sr∣ = m;
5:	Compute ci,s (Sil,,tsr) using Eq. (8);
6:	Sample k test data of class-s, i.e., S：：，〜Si,ts & ∣S;Ss∣ = k;
end
7:	UPdate θ∏mp = θ∏mp - Nθ∏mp LTi (θ∏mp ) USing {七}：9 and LTi in EQ∙ (10);
end
end
Output: featUre extractor πmp .
→- KCL
MP
MM →- RS
(a) 20-way 1-observation
(b) 20-way 5-observation
Figure 5: We conducted experiments on CIFAR-100 and OmniGlOt and reported the average clustering
accuracy (ACC (%), Section 5) when using existing and our methods to address the NCDL problem. The
experimental results showed that MM and MP tend to outperform existing methods.
MCL →- DTC
(d) 20-way 5-observation
OmniGlot
0	50	1∞
Epoch
ι∞
安ɛo
< 60
40
(c) 20-way 1-observation
where [Ku] denotes the KU classes selected from {1,..., Kl}. S；'Ss is the test set of labeled data of
class-s from task Ti. The full procedures of training ∏mp are shown in Algorithm 3. After training
the feature extractor πmp well, we use the training set of Su to obtain the prototypes. For x in the test
set of Su , we compute the distance between x and each prototype, and then the class corresponding
to the nearest prototype is the class of x.
5 Experiments
Baselines. To verify the performance of our meta-based NCDL methods (i.e., MM and MP), we
compare them with 5 competitive baselines, including K-means (MacQueen et al., 1967), KCL (Hsu
et al., 2018) , MCL (Hsu et al., 2019), DTC (Han et al., 2019), and RS (Han et al., 2020b). We modify
these baselines by only reducing the amount of novel-class data, with other configurations invariable.
We clarify the implementation details of CATA, MM, and MP in Appendix E.
Datasets. To evaluate the performance of our methods and baselines, we conduct experiments on
four popular image classification benchmarks, including CIFAR-10 (Krizhevsky & Hinton, 2009),
CIFAR-100 (Krizhevsky & Hinton, 2009), SVHN (Netzer et al., 2011), and OmniGlot (Lake et al.,
2015). Detailed introductions and partitions of known classes and novel classes of these four datasets
is in Appendix D. Following the protocol of few-shot learning (Park et al., 2018; Liu et al., 2019b;
Wang et al., 2020; Ziko et al., 2020), for SVHN and CIFAR-10, we perform the few-observation
tasks of 5-way 1-observation and 5-way 5-observation, and we perform the few-observation tasks of
20-way 1-observation and 20-way 5-observation for CIFAR-100 and OmniGlot.
Evaluation metric. For a clustering problem, we use the average clustering accuracy (ACC) to
evaluate the performance of clustering, which is defined as follows,
1N
m∈aχ NE i{% = φ(yι)},
φ∈	i=1
(11)
8
Published as a conference paper at ICLR 2022
Table 1: Ablation Study on four datasets. In this table, we report the ACC (%)±standard deviation of
ACC (%) on four datasts, where w/o represents “without”. It is clear that CATA improves the ACC.
Methods		MM	MM w/o CATA	MP	MP w/o CATA
SVHN (5-way)	5-observation	47.3±0.3	40.1±0.3	61.0±0.5	60.5±0.2
	1-observation	42.7±0.3	39.7±0.4	52.8±0.4	50.2±0.3
CIFAR-10 (5-way)	5-observation 1-observation	45.3±0.1 41.3±0.4	42.7±0.3 40.3±0.3	58.5±0.2 51.7±0.2	57.9±0.1 47.6±0.2
CIFAR-100 (20-way)	5-observation	42.0±0.4	39.9±0.3	45.5±0.3	44.1±0.1
	1-observation	39.2±0.2	37.1±0.3	38.8±0.4	37.0±0.2
OmniGlot (20-way)	5-observation	82.1±0.4	80.1±0.4	98.4±0.2	96.7±0.3
	1-observation	77.3±0.4	78.5±0.4	94.6±0.3	91.2±0.2
where y and yi denote the ground-truth label and assigned cluster indices respectively. L is the set
of mappings from cluster indices to ground-truth labels.
Results on CIFAR-10. As shown in Figures 2(a) and 2(b), MM and MP outperform all baselines
significantly, and the ACC of MP is much higher than that of MM. The main reason is that MP
makes full use of the labels of known-class data in the training process, while MM does not. MM
only uses the labels of known-class data in the sampling process. Besides, as shown in Table 2 in
Appendix F, K-means performs better on CIFAR-10 than other datasets. The reason is that clustering
rules contained in data of CIFAR-10 are simple and suitable for K-means.
Results on SVHN. Figures 2(d) and 2(c) show that our methods still outperform all baselines. In the
task of 5-way 1-observation, RS performs as well as MM (Figure 2(d)). The reason is that RS trains
the embedding network with self-supervised learning method, RotationNet (Gidaris et al., 2018),
under 1-observation case, which partly overcomes this problem by data augment. The SVHN is
simpler than other datasets, thus such a data augmentation works better.
Results on CIFAR-100. It is clear that we outperform all baselines. Differ from tasks on other
datasets, MM performs equally even a little better than MP shown in Figure 5(a). The reason is that
the amount of known classes is relatively large and the data distribution of CIFAR-100 is complex, so
we cannot accurately compute prototypes with very limited data.
Results on OmniGlot. As shown in Figures 5(c) and 5(d), our methods still have the highest
ACC. We find that the ACC of K-means are merely 2.2% for both 1-observation and 5-observation,
indicating that K-means hardly works on OmniGlot. Although this result looks very bad, this is
reasonable. This is because the number of novel classes is too large (i.e., 659) and K-means is an
unsupervised method that requires many training data. As a simple benchmark in few-shot learning,
existing meta-learning methods (Ramalho & Garnelo, 2019; Li et al., 2019a) have completed solved
it, which achieved the accuracy of 99.9% on 20-way 5-shot task. Thus our method MP also achieves
a high accuracy of 98.4% without novel-class labels. Note that, we also show results of all methods
on NCD problem in Table 3 (Appendix G).
Ablation study. To verify the effectiveness of CATA, we conduct ablation study by removing CATA
from MM and MP. According to Table 1, CATA significantly improves the performance of MM and
MP. However, there exists an abnormal phenomenon in OmniGlot, i.e., MM w/o CATA outperforms
MM in the task of 20-way 1-observation. Although we need 16 (= |Sl,tr| + |Sl,ts| = 1 + 15) data
for each class in an inner-task, the total amount of data for each class is only 20. Therefore, there are
not enough data for CATA to sample, which makes CATA cannot improve the ACC of MM.
6	Conclusions
In this paper, we study an important problem called novel class discovery (NCD) and demystify the
key assumptions behind this problem. We find that NCD is sampling instead of labeling in causality,
and, furthermore, data in the NCD problem should share high-level semantic features. This finding
motivates us to link NCD to meta-learning, since meta-learning also assumes that the high-level
semantic features are shared between seen and unseen classes. To this end, we propose to discover
novel classes in a meta-learning way, i.e., the meta discovery. Results show that meta-learning based
methods outperform all existing baselines when addressing a more challenging problem NCD given
very limited data (NCDL) where only few novel-class data can be observed, which lights up a novel
road for NCD/NCDL.
9
Published as a conference paper at ICLR 2022
7	Acknowledgements
This work was partially supported by the National Natural Science Foundation of China (No.
91948303-1, No. 61803375, No. 12002380, No. 62106278, No. 62101575, No. 61906210)
and the National Grand R&D Plan (Grant No. 2020AAA0103501). BH was supported by NSFC
Young Scientists Fund No. 62006202 and RGC Early Career Scheme No. 22200720. TLL was
supported by Australian Research Council Projects DE-190101473 and DP-220102121. MS was
supported by JST CREST Grant Number JPMJCR18A2.
8	Ethics Statement
This paper does not raise any ethics concerns. This study does not involve any human subjects,
practices to data set releases, potentially harmful insights, methodologies and applications, potential
conflicts of interest and sponsorship, discrimination/bias/fairness concerns, privacy and security
issues, legal compliance, and research integrity issues.
9	Reproducibility statement
To ensure the reproducibility of experimental results, we have provided codes of MM and MP at
github.com/Haoang97/MEDI.
References
Horace B Barlow. UnsUPervised learning. Neural computation, 1(3):295-311, 1989.
Avrim Blum and Tom M. Mitchell. Combining labeled and unlabeled data with co-training. In COLT,
1998.
Jiaxin Chen, Xiao-Ming WU, Yanke Li, Qimai LI, Li-Ming Zhan, and FU-Lai ChUng. A closer look
at the training strategy for modern meta-learning. In NeurIPS, 2020a.
Xiaohan Chen, Zhangyang Wang, SiyU Tang, and Krikamol MUandet. MATE: PlUgging in model
awareness to task embedding for meta learning. In NeurIPS, 2020b.
Haoang Chi, Feng LiU, Wenjing Yang, Long Lan, Tongliang LiU, Bo Han, William K. CheUng,
and James T. Kwok. TOHAN: A one-steP aPProach towards few-shot hyPothesis adaPtation. In
NeurIPS, 2021.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: A large-scale
hierarchical image database. In CVPR, 2009.
JiahUa Dong, Yang Cong, Gan SUn, Bineng Zhong, and Xiaowei XU. What can be transferred:
UnsUPervised domain adaPtation for endoscoPic lesions segmentation. In CVPR, 2020.
JiahUa Dong, Yang Cong, Gan SUn, Zhen Fang, and Zhengming Ding. Where and how to transfer:
Knowledge aggregation-indUced transferability PercePtion for. UnsUPervised domain adaPtation.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021a.
JiahUa Dong, Zhen Fang, Anjin LiU, Gan SUn, and Tongliang LiU. Confident-anchor-indUced
mUlti-soUrce-free domain adaPtation. In NeurIPS, 2021b.
Charles Elkan and Keith Noto. Learning classifiers from only Positive and Unlabeled data. In KDD,
2008.
Tongtong Fang, Nan LU, Gang NiU, and Masashi SUgiyama. Rethinking imPortance weighting for
deeP learning Under distribUtion shift. In NeurIPS, 2020.
Zhen Fang, Jie LU, Anjin LiU, Feng LiU, and GUangqUan Zhang. Learning boUnds for oPen-set
learning. In ICML, 2021a.
10
Published as a conference paper at ICLR 2022
Zhen Fang, Jie Lu, Feng Liu, Junyu Xuan, and Guangquan Zhang. Open set domain adaptation:
Theoretical bound and algorithm. IEEE Transactions on Neural Networks and Learning Systems,
pp. 4309-4322, 2021b.
Enrico Fini, Enver Sangineto, StePhane LathuiliEe, ZhUn Zhong, Moin Nabi, and Elisa Ricci. A
unified objective for novel class discovery. In ICCV, 2021.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In ICML, 2017.
Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by
predicting image rotations. In ICLR, 2018.
Mingming Gong, Kun Zhang, Tongliang Liu, Dacheng Tao, Clark Glymour, and Bernhard Scholkopf.
Domain adaptation with conditional transferable components. In ICML, 2016.
Bo Han, Gang Niu, Xingrui Yu, Quanming Yao, Miao Xu, Ivor Tsang, and Masashi Sugiyama. Sigua:
Forgetting may make learning with noisy labels more robust. In ICML, 2020a.
Kai Han, Andrea Vedaldi, and Andrew Zisserman. Learning to discover novel visual categories via
deep transfer clustering. In ICCV, 2019.
Kai Han, Sylvestre-Alvise Rebuffi, Sebastien Ehrhardt, Andrea Vedaldi, and Andrew Zisserman.
Automatically discovering and learning new visual categories with ranking statistics. In ICLR,
2020b.
Kai Han, Sylvestre-Alvise Rebuffi, Sebastien Ehrhardt, Andrea Vedaldi, and Andrew Zisserman.
Autonovel: Automatically discovering and learning novel visual categories. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 2021.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In CVPR, 2016.
Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in neural
networks: A survey. arXiv:2004.05439, 2020.
Yen-Chang Hsu, Zhaoyang Lv, and Zsolt Kira. Learning to cluster in order to transfer across domains
and tasks. In ICLR, 2018.
Yen-Chang Hsu, Zhaoyang Lv, Joel Schlosser, Phillip Odom, and Zsolt Kira. Multi-class classification
without multi-class labels. In ICLR, 2019.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In ICML, 2015.
Xuhui Jia, Kai Han, Yukun Zhu, and Bradley Green. Joint representation learning and novel category
discovery on single- and multi-modal data. In ICCV, 2021.
Yibo Jiang and Nakul Verma. Meta-learning to cluster. arXiv:1910.14134, 2019.
Yongcheng Jing, Xiao Liu, Yukang Ding, Xinchao Wang, Errui Ding, Mingli Song, and Shilei Wen.
Dynamic instance normalization for arbitrary style transfer. In AAAI, 2020.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
Ryuichi Kiryo, Gang Niu, Marthinus C Du Plessis, and Masashi Sugiyama. Positive-unlabeled
learning with non-negative risk estimator. NeurIPS, 2017.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Ilja Kuzborskij and Francesco Orabona. Stability and hypothesis transfer learning. In ICML, 2013.
Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning
through probabilistic program induction. Science, 350(6266):1332-1338, 2015.
11
Published as a conference paper at ICLR 2022
Aoxue Li, Tiange Luo, Tao Xiang, Weiran Huang, and Liwei Wang. Few-shot learning with global
class representations. In ICCV, 2019a.
Xiao-Li Li and Bing Liu. Learning from positive and unlabeled examples with different data
distributions. In ECML, 2005.
Yingming Li, Ming Yang, and Zhongfei Zhang. A survey of multi-view representation learning.
IEEE Transactions on Knowledge and Data Engineering, 31(10):1863-1883, 2019b.
Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source
hypothesis transfer for unsupervised domain adaptation. In ICML, 2020.
Chenghao Liu, Zhihao Wang, Doyen Sahoo, Yuan Fang, Kun Zhang, and Steven C. H. Hoi. Adaptive
task sampling for meta-learning. In ECCV, 2020a.
Feng Liu, Jie Lu, Bo Han, Gang Niu, Guangquan Zhang, and Masashi Sugiyama. Butterfly: A
panacea for all difficulties in wildly unsupervised domain adaptation. In NeurIPS Workshop on
Learning Transferable Skills, 2019a.
Feng Liu, Wenkai Xu, Jie Lu, Guangquan Zhang, Arthur Gretton, and Danica J. Sutherland. Learning
deep kernels for non-parametric two-sample tests. In ICML, 2020b.
Feng Liu, Guangquan Zhang, and Jie Lu. Heterogeneous domain adaptation: An unsupervised
approach. IEEE Transactions on Neural Networks and Learning Systems, 31(12):5588-5602,
2020c.
Lu Liu, Tianyi Zhou, Guodong Long, Jing Jiang, and Chengqi Zhang. Attribute propagation network
for graph zero-shot learning. In AAAI, 2020d.
Lu Liu, Tianyi Zhou, Guodong Long, Jing Jiang, Xuanyi Dong, and Chengqi Zhang. Isometric
propagation network for generalized zero-shot learning. In ICLR, 2021a.
Tongliang Liu and Dacheng Tao. Classification with noisy labels by importance reweighting. IEEE
Transactions on pattern analysis and machine intelligence, 38(3):447-461, 2015.
Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sung Ju Hwang, and Yi Yang.
Learning to propagate labels: Transductive propagation network for few-shot learning. In ICLR,
2019b.
Yanbin Liu, Juho Lee, Linchao Zhu, Ling Chen, Humphrey Shi, and Yi Yang. A multi-mode
modulator for multi-domain few-shot classification. In ICCV, 2021b.
Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I. Jordan. Conditional adversarial
domain adaptation. In NeurIPS, 2018.
James MacQueen et al. Some methods for classification and analysis of multivariate observations. In
Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, 1967.
Andreas Maurer. Algorithmic stability and meta-learning. Journal of Machine Learning Research, 6:
967-994, 2005.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading
digits in natural images with unsupervised feature learning. In NeurIPS Workshop on Deep
Learning and Unsupervised Feature Learning, 2011.
Gang Niu, Marthinus Christoffel du Plessis, Tomoya Sakai, Yao Ma, and Masashi Sugiyama. Theo-
retical comparisons of positive-unlabeled learning against positive-negative learning. NeurIPS,
2016.
Minseop Park, Saehoon Kim, Jungtaek Kim, Yanbin Liu, and Seungjin Choi. Taeml: Task-adaptive
ensemble of meta-learners. In NeurIPS Workshop on Meta learning, 2018.
Tiago Ramalho and Marta Garnelo. Adaptive posterior learning: few-shot learning with a surprise-
based memory module. In ICLR, 2019.
12
Published as a conference paper at ICLR 2022
Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In ICLR, 2017.
Kuniaki Saito, Yoshitaka Ushiku, and Tatsuya Harada. Asymmetric tri-training for unsupervised
domain adaptation. In ICML, 2017.
Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy P. Lillicrap.
Meta-learning with memory-augmented neural networks. In ICML, 2016.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In ICLR, 2015.
Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototypical networks for few-shot learning. In
NeurIPS, 2017.
Jie Song, Yixin Chen, Xinchao Wang, Chengchao Shen, and Mingli Song. Deep model transferability
from attribution maps. In NeurIPS, 2019.
Jie Song, Yixin Chen, Jingwen Ye, Xinchao Wang, Chengchao Shen, Feng Mao, and Mingli Song.
DEPARA: deep attribution graph for deep knowledge transferability. In CVPR, 2020.
Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning
Research ,15(1):1929-1958,2014.
Boyu Wang, Jorge A. Mendez, Mingbo Cai, and Eric Eaton. Transfer learning via minimizing the
performance gap between domains. In NeurIPS, 2019.
Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. Generalizing from a few examples:
A survey on few-shot learning. ACM Computing Surveys, 53(3):1-34, 2020.
Ying Wei, Peilin Zhao, and Junzhou Huang. Meta-learning hyperparameter performance prediction
with neural processes. In ICML, 2021.
Xiaobo Xia, Tongliang Liu, Bo Han, Nannan Wang, Mingming Gong, Haifeng Liu, Gang Niu,
Dacheng Tao, and Masashi Sugiyama. Part-dependent label noise: Towards instance-dependent
label noise. NeurIPS, 2020.
Junyuan Xie, Ross B. Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering
analysis. In ICML, 2016.
Huaxiu Yao, Ying Wei, Junzhou Huang, and Zhenhui Li. Hierarchically structured meta-learning. In
ICML, 2019.
Huaxiu Yao, Longkai Huang, Linjun Zhang, Ying Wei, Li Tian, James Zou, Junzhou Huang, and
Zhenhui Li. Improving generalization in meta-learning via task augmentation. In ICML, 2021.
Zhun Zhong, Enrico Fini, Subhankar Roy, Zhiming Luo, Elisa Ricci, and Nicu Sebe. Neighborhood
contrastive learning for novel class discovery. In CVPR, 2021a.
Zhun Zhong, Enrico Fini, Subhankar Roy, Zhiming Luo, Elisa Ricci, and Nicu Sebe. Neighborhood
contrastive learning. In CVPR, 2021b.
Zhun Zhong, Linchao Zhu, Zhiming Luo, Shaozi Li, Yi Yang, and Nicu Sebe. Openmix: Reviving
known knowledge for discovering novel visual categories in an open world. In CVPR, 2021c.
Joey Tianyi Zhou, Ivor W. Tsang, Sinno Jialin Pan, and Mingkui Tan. Multi-class heterogeneous
domain adaptation. Journal of Machine Learning Research, 20(57):1-31, 2019.
Imtiaz Masud Ziko, Jose Dolz, Eric Granger, and Ismail Ben Ayed. Laplacian regularized few-shot
learning. In ICML, 2020.
13
Published as a conference paper at ICLR 2022
A	Detailed Related Work
Novel class discovery. NCD is proposed in recent years, aiming to cluster unlabeled novel-class
data according to their underlying categories (Han et al., 2021; Jia et al., 2021). Compared with
unsupervised learning (Barlow, 1989), NCD also requires labeled known-class data to help cluster
novel-class data. The pioneering methods include KLD-based contrastive loss (KCL) (Hsu et al.,
2018), meta classification likelihood (MCL) (Hsu et al., 2019), deep transfer clustering (DTC) (Han
et al., 2019), rank statistics (RS) (Han et al., 2020b), OpenMix (Zhong et al., 2021c), neighborhood
contrastive learning (NCL) (Zhong et al., 2021a), and unified objective (UNO) (Fini et al., 2021). In
this paper, we update and present the results of our methods regarding NCD.
In KCL (Hsu et al., 2018), a method based on pairwise similarity is introduced. They first pre-trained
a similarity prediction network on labeled data of known classes and then use this network to predict
the similarity of each unlabeled data pair, which acts as the supervision information to train the main
model. Then, MCL (Hsu et al., 2019) changed the loss function of KCL (the KL-divergence based
contrastive loss) to the meta classification likelihood loss.
In DTC (Han et al., 2019), they first learned a data embedding with metric learning on labeled data,
and then they employed the DEC (Xie et al., 2016) to learn the cluster assignments on unlabeled data.
In RS (Han et al., 2020b), they used the rank statistics to predict the pairwise similarity of data. To
keep the performance on data of known classes, they pre-trained the data embedding network with
self-supervised learning method (Gidaris et al., 2018) on both labeled data and unlabeled data.
In OpenMix (Zhong et al., 2021c), they proposed to mix known-class and novel-class data to learn a
joint label distribution, benefiting to find their finer relations.
In NCL (Zhong et al., 2021a), they used neighborhood contrastive learning to learn discriminate
features with both the labeled and unlabeled data with the local neighborhood to take the knowledge
from more positive samples. In addition, they used the hard negative generation to produce hard
negative to improve NCL.
In UNO (Fini et al., 2021), they used pseudo-labels in combination with ground-truth labels in a
UNified Objective function (UNO) that enabled better cooperation and less interference without
self-supervised learning.
Meta-Learning. Meta-learning is also known as learning-to-learn, which train a meta-model over a
large variety of learning tasks (Ravi & Larochelle, 2017; Liu et al., 2019b; 2021b). In meta-learning,
we often assume that data share the same high-level features, which ensures that meta-learning can
be theoretically addressed (Maurer, 2005). According to (Hospedales et al., 2020), there are three
common approaches to meta-learning: optimization-based (Finn et al., 2017), model-based (Santoro
et al., 2016), and metric-based (Snell et al., 2017).
Optimization-based methods include those where the inner-level task is literally solved as an optimiza-
tion problem, and focus on extracting meta knowledge required to improve optimization performance.
In model-based methods, the inner learning step is wrapped up in the feed-forward pass of a single
model. Metric-based methods perform non-parametric learning at the inner-task level by simply
comparing validation points with training points and predicting the label of matching training points.
Since meta-learning and the NCDL have the same assumption that data share the same high-level
semantic features (introduced in Section 1), we link NCDL to meta-learning problem, providing a
way to formulate and analyze the NCDL.
Positive-unlabeled learning. Positive-unlabeled (PU) learning (Li & Liu, 2005) is an important
branch of semi-supervised learning, aiming to learn a binary classifier with positive data and unlabeled
data. Thus, PU learning is a special case of NCD, where there exists only one known class and
one novel class. The basic solution is to view unlabeled data as negative data to train a standard
classifier (Elkan & Noto, 2008). Niu et al. (2016) gives the conditions when PU learning outperforms
supervised learning through upper bounds on estimation errors. Kiryo et al. (2017) proposes a
non-negative risk estimator to prevent flexible models overfitting on negative data in PU learning.
Transfer learning. Transfer learning aims to leverage knowledge contained in source domains to
improve the performance of tasks in a target domain, where both domains are similar but different
(Gong et al., 2016; Long et al., 2018; Zhou et al., 2019; Liu et al., 2019a; Wang et al., 2019; Liu et al.,
14
Published as a conference paper at ICLR 2022
2020c; Liang et al., 2020; Chi et al., 2021; Fang et al., 2021a;b). Representative transfer learning
works are domain adaptation (Gong et al., 2016; Long et al., 2018; Zhou et al., 2019; Liu et al.,
2019a; 2020c; Dong et al., 2020; 2021b;a) and hypothesis transfer (Kuzborskij & Orabona, 2013;
Liang et al., 2020; Chi et al., 2021), which mainly focus on classification or prediction tasks in the
target domain. NCD problem can be also regarded a transfer learning problem that aims to complete
the clustering task in a target domain via leveraging knowledge in source domains.
Compared to samplers in meta-learning. Tasks in meta-learning are heterogeneous in some
scenarios, which can not be handled via globally sharing knowledge among data. Therefore, it is
crucial to address the task-sampling problem in meta-learning. Yao et al. (2019) assigned many tasks
that are randomly sampled from different clusters using their similarities, and only used the most
related task cluster for training. This method solves the task-sampling problem in the view of tasks.
Liu et al. (2020a) proposed a greedy class-pair based sampling method, which selects difficult tasks
according to the class-pair potentials. This method solves the sampling problem in the view of classes.
In our paper, we propose CATA based on clustering rules regarding data, which is in the view of data.
B Main Theoretical Results
Theorem 1 (NCD is Theoretically Solvable). Given Xl, Xu and fl defined above and assume (A)-(D)
hold, then Π(XU) is KU-eu-separable. If eu = 0,thenNCD is theoretically solvable.
Proof. The key to this proof is that the optimized π* is in Πl ∩ Πu.
Case1.	When fU ∈ FU, according to Problem 1, let
∏* = argmin T (Π(X l),fl(Xl))+ T (Π(X u),f U(X u)),	(12)
π∈Π
where Π = {π : X → Rdr} and fu ∈ Fu. Let τ* = min∏∈∏,fu∈fu J(π). If π* ∈ Πl ∩ Πu, then,
according to definitions of Πl and Πu, T* > el + eu. This means that there exists π0 ∈ Πl ∩ Πu
sUch that J(π 0) = T (π0(X l), fl(Xl)) + T(π 0(XU), fU(XU)) = el + eU < T*. Namely, T* is not the
minimUm valUe in the set {J (π) : π ∈ Π}, which leads to a contradiction to the definition ofT*.
Case2.	When fU ∈ Fu, let τ** = min∏∈∏,fu∈fU J(π). According to definition of eu, it is clear that
T** > el + eU = J(π 0). Namely, T** is not the minimUm valUe in the set {J(π ) : π ∈ Π}.
NCD is solvable. If eu = 0, according to definition of Πu, τ(∏* (Xu), fu(XU)) = 0, which means
that we can perfectly separate ∏* (Xu). Namely, NCD is theoretically solvable.	□
Theorem 2 (Impossibility Theorem). Given Xl, XU and fl defined above and assume (A)-(C) hold,
if maxπ∈Πl τ(π (XU), fU(XU)) < minπ∈Π-Πl τ (π(Xl), fl(Xl)), (D) does not hold and el ≤ eU, then
τ(π(XU),fu(XU)) > eu, where Π = {π : X ― Rdr} and fu ∈ F.
Proof. If fu ∈ Fu, then we naturally have T(π(Xu), fu(Xu)) > eu according to the definition of eu.
Then, the key to this proof is that the optimized ∏* is in Πl. According to Problem 1,
∏* = argmin T (π(X l),fl(Xl))+ T (π(X u),f u(X u)).	(13)
π∈Π
Let T* = min∏∈∏ J(π). If ∏* ∈ Π - Πl, then, according to definition of Πu, eu ≥ el, and
maxπ∈Πl T(π (XU), fU(XU)) < minπ∈Π-Πl T(π (Xl), fl(Xl)), we have the following ineqUality.
T* = J(∏*) ≥ T(π*(Xl), fl(Xl)) + eu > maχt(π(Xu),fu(Xu)) + el.	(14)
π∈Πl
Let π0 = arg maxπ∈Πl T (π(XU), fU(XU)). Since π 0 ∈ Πl, we know that T (π0(Xl), fl(Xl)) = el
according to definition of Πl . ThUs we have
maxT(π(XU),fU(XU))+el=T(π0(XU),fU(XU))+T(π0(Xl),fl(Xl))=J(π0).	(15)
π∈Πl
Hence, we find a π0 ∈ Π such that J(π0) < J(∏*) = T*, which leads to a contradiction to the
definition of τ*. Thus, ∏* ∈ Πl. Based on the definition of Πu, τ(∏(Xu), fu(Xu)) > eu.	□
15
Published as a conference paper at ICLR 2022
C NCDL in The View of Meta-learning
Since we address NCDL based on the meta-learning framework, it is interesting to analyze if NCDL
can be addressed based on the sampled tasks T = {Ti}in=1. We show that, under some assumptions,
NCDL can be well addressed.
Problem Setup of NCDL in the View of Meta-Iearning. In NCDL, We have a task space T* =
{(X, f) : X is any r.v.s defined on X, f : X → C}, a task distribution P(T*) defined on T*, a
r.v. Xu, sampled tasks T = {X∖, fi}〜P(T*) (i = 1,..., n), an index set I = {iU,..., i%U}, a
function set F = {f : X → C}, a transformation set Π = {π : X → Rdr }, and the loss function
' :Rdr ×F → R+ that is the loss function such that, Vf ∈ F and ∀∏ ∈ Π, EPX ['(∏(X), f (X))]=
τ (π(X), f(X)), Where fl : X → Y is the ground-truth labeling function for Xil, Y = {il1 , . . . , ilKl},
C = Y ∪ I and PX is the distribution corresponding to a r.v. X. Based on Definitions 1 and 2, We
have the folloWing assumptions in NCDL.
(A1) The union of support set of Xil (i = 1, . . . , n) and the support set of Xu are disjoint, and
union of underlying classes of Xil (i = 1, . . . , n) are different from those of Xu;
(B1) Xil	is Kl-li-separable With	Fl	=	{fil}	and Xu	is	Ku-u-separable With	Fu,	Where	li	=
T(Xi,fi(Xi)) < 1, eu = minf∈{∕χ→i} T(Xu, f (Xu)) < 1;
(C1) There exist a consistent Kl-li-separable transformation set Πli for Xil and a consistent
Ku-u-separable transformation set Πu for Xu;
(D1) ∩n=1∏i ∩ Πu = 0;
(E1) There exists fu ∈ Fu such that {Xu, fu} is also draWn from the task distribution P(T*).
(A1) ensures that knoWn and novel classes are disjoint. (B1) implies that it is meaningful to separate
observations from Xil and Xu. (C1) means that We can find good high-level features for Xil or Xu.
Based on these features, it is much easier to separate Xil or Xu. (D1) says that the high-level features
of Xil and Xu are shared, as demonstrated in the introduction. (E1) represents that our target task
Tt = {Xu, fu} and sampled tasks {Ti}in=1 are from the same task distribution P(T*). Then, We can
define NCDL formally.
Problem 2 (NCDL). Given {Ti = {Xil, fil}}in=1 andXu defined above and assume (A1)-(E1) hold,
let meta-samples Sl = {Si,tr ∪ Si,ts}n=1 are drawn from the {Xj}n=1, where Si,tr〜(Xi )m and
Si,ts〜(Xi)k are the training set and the test setofthe task Ti with the sizes m and k, respectively,
and each task Ti can output an inner-task clustering algorithm A(Sl) : Xm → Π. In NCDL,
we aim to propose a meta-algorithm A to train an inner-task clustering algorithm A(Sl) with Sl
via minimizing R(A(Sl), {Sil,tr, fil}in=1 ) = Pin=1 T(A(Sl)(Sil,tr)(Xil), fil(Xil))/n. We expect that
A(Sl)(Siu)(Xu) is K u -u -separable, where Su are observations ofXu with size m.
Remark 3. Compared to meta-learning, NCDL aims to train an inner-task clustering algorithm
A(Sl) : Xm → Π rather than a classification algorithm often used in meta learning. Besides, in
NCDL, We can only observe features from the target task, While We can observe the labeled data from
the neW task in the meta-learning. In NCDL, m is a very small number.
Then, We shoW that the risk used in Problem 2 can be estimated under certain conditions. Based on
Problem 2, We turn the objective function R(A(Sl), {Sil,tr}in=1 ) into a more general meta-learning
risk:
R(A(Sl),P (T *))= ET =(χ,f)〜P (t*)Es 〜(PX)m Ex 〜PX '(A(Sl)(S)(x), f (x)),	(16)
R(A(Sl), P(T*)) is the expectation of the generalized error W.r.t. the task distribution P(T*) and
can measure the performance of each inner-task clustering algorithm. In practice, the meta-clustering
algorithm of NCDL is optimized by minimizing the average of the empirical error on the training
tasks, called the empirical multi-task error:
1n1
R(A(Sl),{Sl, Fl}) = n£ - E '(A(Sl)(Si,tr)(xj),fl(Xij)),	(17)
i=1	l,ts
xij ∈ i
16
Published as a conference paper at ICLR 2022
where and Sil = Sil,tr ∪ Si,ts 〜(Pχi)m. Then, the generalization bound of inner-task clustering
algorithm A(Sl) of meta-based NCDL algorithms can be obtained from the uniform stability β of
the meta-algorithm A.
Definition 3 (Uniform Stability (Maurer, 2005)). A meta-algorithm A has uniform stability β
w.r.t. the loss function ` if the following holds for any meta-samples S and ∀i ∈ {1, . . . , n},
∀T = {X, f}〜P(T), ∀Str 〜Pm, ∀Sts 〜PX:
∣L(A(S)(Str)(Sts),f(Sts)) - L(A(S∖i)(Str)(Sts), f (Sts))∣ ≤ β,
where
L(A(S)(Str)(Sts),f(Sts)) = 1 X '(A(S)(Str)(xj),f(xj)).
xj∈Sts
Given training meta-samples S = {Sitr∪Sits}in=1, we modify S by replacing the i-th element to obtain
Si = {S1tr ∪ S1ts,. ..,Sit-r1 ∪ Sit-s 1, Sitr0 ∪ Sits0, Sit+r 1 ∪ Sit+s 1, .. ., Sntr ∪ Snts}, where the replacement
sample Si0 is assumed to be drawn from D and is independent from S. In addition, we modify S by
removing the i-th element to obtain S\i = {S1tr ∪ S1ts, . . . , Sit-r 1 ∪ Sit-s 1, Sit+r 1 ∪ Sit+s 1, . . . , Sntr ∪ Snts}
In the same way, given a training set S = {z1, . . . , zi-1, zi, zi+1, . . . , zn}, we can obtain Si =
{z1, . . . , zi-1, zi0, zi+1, . . . , zn} and S\i = {z1, . . . , zi-1, zi+1, . . . , zn}.
Lemma 1 (McDiarmid Inequality). Let S and Si defined as above, let F : Zn → R be any
measurable function for which there exits constants ci (i = 1, . . . , n) such that
sup	|F(S)-F(Si)| ≤ci,
S∈Zm,zi0 ∈Z
then
PS [F (S) - ES[F(S)] ≥ ] ≤ exp(
P⅛).
Theorem 3. For any task distribution P(T*) andmeta-samples Sl with n tasks, ifa meta-aIgorithm
A has uniform stability β w.r.t. a loss function ` bounded by M, then the following statement holds
with probability at least 1 - δ for any δ ∈ (0, 1):
R(A(Sl), P(T*)) ≤ R(A(Sl), Sl) + e(n, β),
(18)
where e(n, β) = 2β + (4nβ + M),吗：/"∙
Proof∙ The proof of Theorem 3 mainly follows (Chen et al., 2020a).
Let F(Sl) = R(A(Sl),P(T*)) - R(A(Sl), Sl) and F(Sl，i) = R(A(Sl"),P(T*))-
R(A(Sl，i), Sl，i). We have
|F(Sl)-F(Sl，i)| ≤ R(A(Sl),P(T*))-R(A(Sl,i),P(T*))∣+∣R(A(Sl), Sl)-R(A(Sl，i), Sl，i)|.
(19)
The first term in Eq. (19) can be written as
R(A(Sl),P(T*)) -R(A(Sl，i),P(T*))| ≤ R(A(Sl),P(T*)) -R(A(Sl\i),P(T*))∣
+ |R(A(Sl，i),P(T*)) - R(A(Sl∖i),P(T*))∣.
We can upper bound the first term in Eq. (19) by studying the variation when a sample set Sil of
training task Ti is deleted,
∣R(A(Sl), P(T*)) - R(A(Sl∖i), P(T*))|
≤ ET =(x,f)〜P(t*)Es〜(PX)mEx〜PXl'(A(Sl)(Sl)(x),f(x)) - '(A(Sl'i)(Sl)(x),f(x))|
≤	SUp	l'(A(Sl)(Sl)(x),f (x)) - '(A(Sl∖i)(Sl)(x),f(x))∣
T =(X,f)〜P (T*),S 〜(PX )m,x 〜PX
≤ β.
17
Published as a conference paper at ICLR 2022
Similarly, We have ∣R(A(Sl,i), P(T*)) - R(A(Sl\i), P(T*))∣ ≤ β. So the first term of Eq. (19) is
upper bounded by 2β. The second factor in Eq. (19) can be guaranteed likewise as follows,
R(A(Sl), {Sl, Fl}) - R(A(Sl,i), {Sl, Fl})∣
≤ 1 X
n
q6=i
1 X ('(A(Sl)(Sq,tr)(Χqj ),fq (Xqj ))- '(A(Sl,i)(Sq,tr)(Xqj ),fq (Xqj )))
xqj ∈Sq, s
+ nιk	X	'(A(sι)(si,tr)(χij),fq(Xij)) - X	'(a(sl,i)(si,l,tr)(χij),fq(Xij))
l,ts	0,l,ts
xij ∈ i	xij ∈ i
≤ 2β + M.
n
Hence, |F(Sl) - F(Sl,i) | satisfies the condition of Lemma 1 with Ci = 4β + M. It remains to bound
ESl [F(Sl)] = ESl [R(A(Sl),P(T*))] - ESl [R(A(Sl),{Sl, Fl})]. The first term canbe written
as follows,
ESl R(A(Sl ),P(T*))] = Esi,si,l,tr ,so,l,ts ɪ	X	'(A(Sl)(Si,l,tr)(Xij ),fi(Xij)).
xij∈Si0,l,ts
Similarly, the second term is,
1n1
ESl R(A(S l),{Sl, Fl})]= ESl n£ k E	'(A(Sl)(Si,tr)(Xij ),fil(Xij))
i=1	l,ts
xij ∈ i
=ESl,s0,l,tr 1 X '(A(Sl)(S0,l,tr)(Xij ),fi(Xij))
xij ∈Sil,ts
=ESl,s0,l,tr,s0,l,ts 1 X	'(A(Sl,i )(S0,l,tr)(Xij ),fi(Xij))
0,l,ts
xij ∈ i
where Fl = {fil}in=1. Hence, ESl[F(Sl)] is upper bounded by 2β,
ESl[R(A(Sl), P(T*))] - ESl R(A(Sl), {Sl, Fl})]
=ESl,s0,l,tr,s0,l,ts	1 X	'(A(Sl)(Si,l,tr)(Xij),fi(Xij)) - 1 X	'(A(Sl,i)网,l,tr)(Xij),fil(Xij))
0,l,ts	0,l,ts
xij ∈ i	xij ∈ i
≤ 2β.
Plugging the above inequality in Lemma 1, we obtain
PSl[R(A(Sl),P(T*))-R(A(Sl), Sl) ≥ 2β + e] ≤ exp
-2e2
Pn=IWW
Finally, setting the right side of the above inequality to δ, the following result holds with probability
of 1 - δ,
R(A(Sl), P(T*)) ≤ R(A(Sl), Sl) +2β + (4nβ + M) Jlogf 他.
2n
□
By Theorem 3, the generalization bound depends on the number of the training tasks n and the
uniform stability parameter β. If β < O(1∕√n), we have e(n, β) → 0 as n → ∞. Hence, given a
sufficiently small β, the error R(A(Sl), P(T*)) converges to training error R(A(Sl), Sl) as the
number of training tasks n grows. Theorem 3 indicates that we can minimize the risk in the NCDL
problem in probability if we can control the uniform stability of a meta-algorithm (like MAML did
via support-query learning (Chen et al., 2020a)) and sample the assumed tasks for training (sampler
matters in meta discovery).
18
Published as a conference paper at ICLR 2022
Table 2: Results of K-means on all four datasets.
Dataset	CIFAR-10 (5-way)	SVHN (5-way)	CIFAR-100 (20-way)	OmniGlot (20-way)
1-observation	30.2±3.60	23.5±0.66	9.7±1.18	2.0±0.16
5-observation	32.8±2.13	23.7±0.35	12.4±1.15	2.8±0.13
D Dataset Introductions and Splits
CIFAR-10 dataset contains 60, 000 images with sizes of 32 × 32. Following (Han et al., 2019), for
NCDL, we select the first five classes (i.e. airplane, automobile, bird, cat, and deer) as known classes
and the rest of classes as novel classes. The amount of data from each novel class is no more than 5.
CIFAR-100 dataset contains 100 classes. Following (Han et al., 2020b), we select the first 80 classes
as known classes and select the last 20 classes as novel classes.
SVHN contains 73, 257 training data and 26, 032 test data with labels 0-9. Following (Han et al.,
2019), we select the first five classes (0-4) as known classes and select the (5-9) as novel classes.
OmniGlot constains 1, 632 handwritten characters from 50 different alphabets. Following (Hsu et al.,
2019), we select all the 30 alphabets in background set (964 classes) as known classes and select
each of the 20 alphabets in evaluation set (659 classes) as novel classes.
E Implementation Details
We implement all methods by PyTorch 1.7.1 and Python 3.7.6, and conduct all the experiments on
two NVIDIA RTX 3090 GPUs.
CATA. We use ResNet-18 (He et al., 2016) as the feature extractor and use three fully-connected
layers with softmax layer as the classifier. We also use BN layer (Ioffe & Szegedy, 2015) and Dropout
(Srivastava et al., 2014) in network layers. In this paper, we select the number of views M = 3 for all
four datasets. In other words, there are three classifiers following by the feature extractor. Both the
feature extractor and 3 classifier use Adam (Kingma & Ba, 2015) as their optimizer. The number
of training steps is 50 and the learning rates of feature extractor and classifiers are 0.01 and 0.001
respectively. We use the tradeoff λ of 1/3.
MM for NCDL. We use VGG-16 (Simonyan & Zisserman, 2015) as the feature extractor for all
four datasets. We use SGD as meta-optimizer and general gradient descent as inner-optimizer for
all four datasets. For all experiments, we sample 1000 training tasks by CATA for meta training
and finetune the meta-algorithm after every 200 episodes with data of novel classes. We note that
the inner-tasks of OmniGlot are sampled by in order, instead of randomly sampling like other three
datasets. Thus the errors of OmniGlot only come from the training procedure, while the errors of
other datasets come from both sampling procedure and training procedure. The output dimension of
feature extractor πmm is set to dr = 512. The meta learning rate and inner learning are 0.4 and 0.001
respectively. We use a meta batch size (the amount of training tasks per training step) of 16\8 for
{CIFAR-10,SVHN}\{CIFAR-100,Omniglot}. In addition, we choose k to be 10 which is suitable
for all datasets. For each training task, we update the corresponding inner-algorithm by 10 steps.
MP for NCDL. We use a neural network of four convolutional blocks as the feature extractor for all
datasets following (Snell et al., 2017). Each block comprises a 64-filter 3 × 3 convolution, BN layer
(Ioffe & Szegedy, 2015), a ReLU function and a 2 × 2 max-pooling layer. We use the same feature
extractor for embedding both training data and test data and its output dimension is set to dr = 512.
For all experiments, we train the models via Adam (Kingma & Ba, 2015), and we use an initial
learning rate of 0.001 and cut the learning rate in half every 20 steps. We train the feature extractor
for 200 steps with 1000 training tasks sampled by CATA. The difference in sampling procedure and
error source are the same with MM for NCDL.
F	Results of K-means
This section shows the results of our methods and all the baselines in Table 2.
19
Published as a conference paper at ICLR 2022
Table 3: Results of NCD with abundant novel class data. In this table, we report the ACC
(%)±standard deviation of ACC (%) of baselines and our methods (MM and MP) given abun-
dant novel class data. We still evaluate these methods on four benchmarks (SVHN, CIFAR-10,
CIFAR-100, and OmniGlot).
Methods	K-means	KCL	MCL	DTC	RS	MM	MP
SVHN	42.6±0.0	21.4±0.6	38.6±10.8	60.9±1.6	95.2±0.2	93.1±2.1	77.1±0.8
CIFAR-10	65.5±0.0	66.5±3.9	64.2±0.1	87.5±0.3	91.7±0.9	92.3±0.9	73.2±1.9
CIFAR-100	56.6±1.6	14.3±1.3	21.3±3.4	56.7±1.2	75.2±4.2	69.8±1.3	58.3±2.2
OmniGlot	77.2	82.4	83.3	89.0	89.1	88.6±0.7	98.4±0.2
G Results of NCD
In this section, we show the results of NCD with abundant novel-class data in Table 3. Table 3
shows that MM is comparable with the representative methods but cannot outperform the RS and MP
performs worse than MM. Compared with RS, MM samples many inner-tasks for training, while
RS uses the whole data. Incomplete data makes MM unable to learn the global distribution of novel
classes. MP is not as well as MM on NCD tasks. As the absence of labels of novel class data, we
cannot finetune the model used for calculating data embedding, which is trained by known-class data.
Although this model cannot adapt to novel classes, we can calculate more accurately prototypes with
abundant novel-class data. Hence, with MP, the results of NCD are obviously better than the results
of NCDL.
H Complexity Analysis
We give a brief analysis of time complexity for each algorithm. As MM and MP are two-step methods,
we first analyze the sampling algorithm CATA, and then analyze the main parts of MM and MP.
CATA The time complexity of CATA is O(E * D/B * T), where F is number of training tasks, E
is number of epochs, D is size of dataset, B is meta batch size, and T is the time complexity of each
iteration. We can future decompose O(T) = O(L * n), where L is the average time complexity of
each layer, and n is number of layers. Then, we can decompose O(L) = O(M * N * K2 * H * W ),
where M and N are numbers of channels of input and output, K is size of convolutional kernel, and
H and W are height and weight of feature space.
MM (Main part) The time complexity of MM is O(F * E * D/B * T), where F is number
of training tasks, E is number of epochs, D is size of dataset, B is meta batch size, and T is
the time complexity of each iteration. We can future decompose O(T) = O(L * n), where L is
the average time complexity of each layer, and n is number of layers. Then, we can decompose
O(L) = O(M * N * K2 * H * W), where M and N are numbers of channels of input and output,
K is size of convolutional kernel, and H and W are height and weight of feature space.
MP (Main part) The time complexity ofMP is O(F*E*D/B*T), where F is number of training
tasks, E is number of epochs, D is size of dataset, B is meta batch size, and T is the time complexity
of each iteration. We can future decompose O(T) = O(L*n), where L is the average time complexity
of each layer, and n is number of layers. Then, we can decompose O(L) = O(M * N * K2 * H * W),
where M and N are numbers of channels of input and output, K is size of convolutional kernel, and
H and W are height and weight of feature space.
20