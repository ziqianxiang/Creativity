Published as a conference paper at ICLR 2022
Constrained Policy Optimization via
Bayesian World Models
Yarden As *	Ilnura Usmanova, Sebastian Curi t
Andreas Krause
ETH Zurich
ETH Zurich
ETH Zurich
Ab stract
Improving sample-efficiency and safety are crucial challenges when deploying
reinforcement learning in high-stakes real world applications. We propose
LAMBDA, a novel model-based approach for policy optimization in safety
critical tasks modeled via constrained Markov decision processes. Our approach
utilizes Bayesian world models, and harnesses the resulting uncertainty to
maximize optimistic upper bounds on the task objective, as well as pessimistic
upper bounds on the safety constraints. We demonstrate LAMBDA’s state of the
art performance on the Safety-Gym benchmark suite in terms of sample efficiency
and constraint violation.
1 Introduction
A central challenge in deploying reinforcement learning (RL)
agents in real-world systems is to avoid unsafe and harmful
situations (Amodei et al., 2016). We thus seek agents that can
learn efficiently and safely by acting cautiously and ensuring
the safety of themselves and their surroundings.
A common paradigm in RL for modeling safety critical envi-
ronments are constrained Markov decision processes (CMDP)
(Altman, 1999). CMDPs augment the common notion of the
reward by an additional cost function, e.g., indicating unsafe
state-action pairs. By bounding the cost, one obtains bounds on
the probability of harmful events. For the tabular case, CMDPs
can be solved via Linear Programs (LP) (Altman, 1999). Most
prior work for solving non-tabular CMDPs, utilize model-free
algorithms (Chow et al., 2015; Achiam et al., 2017; Ray et al.,
2019; Chow et al., 2019). Most notably, these methods are
typically shown to asymptotically converge to a constraint-
satisfying policy. However, similar to model-free approaches
in unconstrained MDPs, these approaches typically have a very
high sample complexity, i.e., require a large number of - poten-
tially harmful - interactions with the environment.
A promising alternative to improve sample efficiency is to
use model-based reinforcement learning (MBRL) approaches
(Deisenroth & Rasmussen, 2011; Chua et al., 2018; Hafner et al., 2019a; Janner et al., 2019). In
particular, Bayesian approaches to MBRL quantify uncertainty in the estimated model (Depeweg
et al., 2017), which can be used to guide exploration, e.g., via the celebrated optimism in the face
of uncertainty principle (Brafman & Tennenholtz, 2003; Auer & Ortner, 2007; Curi et al., 2020a).
While extensively explored in the unconstrained setting, Bayesian model-based deep RL approaches
for solving general CMDPs remain largely unexplored. In this paper, we close this gap by proposing
LAMBDA, a Bayesian approach to model-based policy optimization in CMDPs. LAMBDA learns a
safe policy by experiencing unsafe as well as rewarding events through model-generated trajectories
instead of real ones. Our main contributions are as follows:
■ Task o⅛ectιve
■ Safety at the end of training
■ Safety during training
ι.o
0.5
IJUU
0.0
Figure 1: Normalized perfor-
mance and safety metrics, aver-
aged across tasks of the Safety-
Gym (Ray et al., 2019) SG6
benchmark. LAMBDA achieves
constraint satisfaction in all tasks
of the SG6 benchmark while sig-
nificantly improving performance
and sample efficiency. See Ap-
pendix H for further details on
normalization.
S
I
P
Q
* Correspondence to: yardas@ethz.ch
,Equal contribution.
1
Published as a conference paper at ICLR 2022
(a) PointGoal1 (b) PointGoal2 (c) CarGoal1 (d) PointPush1 (e) PointButton1 (f) DoggoGoal1
Figure 2: Safety-Gym SG6 benchmark tasks. In our experiments, we use first-person-view images of
size 64×64 pixels as observations. Green objects represent goals that should be reached by the robot.
Apart from the yellow box, that should be pushed to the goal area in the PointPush1 task, hitting all
other types of objects is considered an unsafe behavior. In Safety-Gym, stochasticity is achieved by
performing a random number of simulation steps before exposing the agent with a new observation.
•	We show that it is possible to perform constrained optimization by back-propagating gra-
dients of both task and safety objectives through the world-model. We use the Augmented
Lagrangian (Nocedal & Wright, 2006; Li et al., 2021) approach for constrained optimiza-
tion.
•	We harness the probabilistic world model to trade-off between optimism for exploration
and pessimism for safety.
•	We empirically show that LAMBDA successfully solves the SG6 benchmark tasks in
Safety-Gym (Ray et al., 2019) benchmark suite with first-person-view observations as illus-
trated in Figure 2. Furthermore, we show that LAMBDA outperforms other model-based
and model-free methods in this benchmark.
2	Related work
Interpretations of safety in RL research We first acknowledge that there exist different defi-
nitions for safety (Garcia et al., 2015). One definition uses reversible Markov decision processes
(Moldovan & Abbeel, 2012) that, informally, define safety as reachability between states and use
backup policies (Eysenbach et al., 2017) to ensure safety. Another definition adopts robust Markov
decision processes (Nilim & Ghaoui, 2005; Tamar et al., 2013; Tessler et al., 2019), which try to
maximize performance under the worst-case transition model. Finally, in this work we use non-
tabular CMDPs (Altman, 1999) that define the safety requirements as a cost function that should be
bounded by a predefined threshold. Similarly, Achiam et al. (2017); Dalal et al. (2018); Ray et al.
(2019); Chow et al. (2019); Stooke et al. (2020); Bharadhwaj et al. (2021); Turchetta et al. (2021) use
non-tabular CMDPs as well, but utilize model-free techniques together with function approximators
to solve the constrained policy optimization problem. As we further demonstrate in our experiments,
it is possible to achieve better sample efficiency with model-based methods.
Safe model-based RL A successful approach in applying Bayesian modeling to low-dimensional
continuous-control problems is to use Gaussian Processes (GP) for model learning. Notably,
Berkenkamp et al. (2017) use GPs to construct confidence intervals around Lyapunov functions
which are used to optimize a policy such that it is always within a Lyapunov-stable region of attrac-
tion. Furthermore, Koller et al. (2018); Wabersich & Zeilinger (2021) use GP models to certify the
safety of actions within a model predictive control (MPC) scheme. Likewise, Liu et al. (2021) apply
MPC for high-dimensional continuous-control problems together with ensembles of neural networks
(NN), the Cross Entropy Method (CEM) (Kroese et al., 2006) and rejection sampling to maximize
expected returns of safe action sequences. However, by planning online only for short horizons and
not using critics, this method can lead to myopic behaviors, as we later show in our experiments.
2
Published as a conference paper at ICLR 2022
Lastly, similarly to this work, Zanger et al. (2021) use NNs and constrained model-based policy op-
timization. However, they do not use model uncertainty within an optimistic-pessimistic framework
but rather to limit the influence of erroneous model predictions on their policy optimization. Even
though using accurate model predictions can accelerate policy learning, this approach does not take
advantage of the epistemic uncertainty (e.g., through optimism and pessimism) when such accurate
predictions are rare.
Exploration, optimism and pessimism Curi et al. (2021) and Derman et al. (2019) also take
a Bayesian optimistic-pessimistic perspective to find robust policies. However, these approaches
do not use CMDPs and generally do not explicitly address safety. Similarly, Bharadhwaj et al.
(2021) apply pessimism for conservative safety critics, and use them for constrained model-free
policy optimization. Finally, Efroni et al. (2020) lay a theoretical foundation for the exploration-
exploitation dilemma and optimism in the setting of tabular CMPDs.
3	Preliminaries
3.1	Safe model-based reinforcement learning
Markov decision processes We consider an episodic Markov decision process with discrete time
steps t ∈ {0, . . . , T }. We define the environment’s state as st ∈ Rn and an action taken by the
agent, as at ∈ Rm. Each episode starts by sampling from the initial-state distribution s0 〜 ρ(s0).
After observing s0 and at each step thereafter, the agent takes an action by sampling from a policy
distribution at 〜 ∏(∙∣st). The next state is then sampled from an unknown transition distribution
st+ι 〜p(∙∣st, at). Given a state, the agent observes a reward, generated by r 〜p(∙∣st, at). We
define the performance of a pair of policy π and dynamics p as
J(π, p) = Eat
^∏,St + 1^P,S0
T
Xrts0
t=0
(1)
〜P
Constrained Markov decision processes To make the agent adhere to human-defined safety con-
straints, we adopt the constrained Markov decision process formalism of Altman (1999). In CMPDs,
alongside with the reward, the agent observes cost signals Ct generated by Ct 〜 p(∙∣st, at) where
i = 1, . . . , C denote the distinct unsafe behaviors we want the agent to avoid. Given cit, we define
the constraints as
T
Ji(π,p)
= Eat~π,st+ι~p,so~ρ	Cits0 ≤di, ∀i∈ {1,...,C},	(2)
t=0
where di are human-defined thresholds. For example, a common cost function is Ci(st) = 1st ∈Hi,
where Hi is the set of harmful states (e.g., all the states in which a robot hits an obstacle). In this
case, the constraint (2) can be interpreted as a bound on the probability of visiting the harmful states.
Given the constraints, we aim to find a policy that solves, for the true unknown dynamics p? ,
max J(π, p?) s.t. Ji (π, p?) ≤ di, ∀i ∈ {1, . . . , C}.	(3)
π∈Π
Model-based reinforcement learning Model-based reinforcement learning revolves around the
repetition of an iterative process with three fundamental steps. First, the agent gathers observed
transitions (either by following an initial policy, or an offline dataset) of {st+1, st, at} into a dataset
D. Following that, the dataset is used to fit a statistical model p(st+1 |st, at, θ) that approximates
the true transition distribution p? . Finally, the agent uses the statistical model for planning, either
within an online MPC scheme or via offline policy optimization. In this work, we consider the cost
and reward functions as unknown and similarly to the transition distribution, we fit statistical models
for them as well. Modeling the transition density allows us to cheaply generate synthetic sequences
of experience through the factorization p(s「T+h |s「-ι, aT—i：T+h-ι,θ) = QT=+H p(st+ι∣st, at,θ)
whereby H is a predefined sequence horizon (see also Appendix I). Therefore, as already shown em-
pirically (Deisenroth & Rasmussen, 2011; Chua et al., 2018; Hafner et al., 2019b), MBRL achieves
superior sample efficiency compared to its model-free counterparts. We emphasize that in safety-
critical tasks, where human supervision during training might be required, sample efficiency is par-
ticularly important since it can reduce the need for human supervision.
3
Published as a conference paper at ICLR 2022
Learning a world model Aiming to tighten the gap between RL and real-world problems, we
relax the typical full observability assumption and consider problems where the agent receives an
observation ot 〜 p(∙∣st) instead of St at each time step. To infer the transition density from ob-
servations, we base our world model on the Recurrent State Space Model (RSSM) introduced in
Hafner et al. (2019a). To solve this inference task, the RSSM approximates the posterior distri-
bution St：T+h 〜 qφ(∙∖θτ：t+h, aT—i：T+h-i) Via an inference network parameterized by φ. We
utilize the inference network to filter the latent state as new observations arrive, and use the in-
ferred latent state as the policy’s input. Furthermore, the RSSM models the predictive distribution
P(St：T+h∖sτ-ι, aτ-1:T+H-ι,θ) as a differentiable function. This property allows us to perform
the constrained policy optimization by backpropagating gradients through the model. We highlight
that the only requirement for the world model is that it models p(s「T+h∖st-i, a/—i：T+H-ι,θ) as
a differentiable function. Hence, it is possible to use other architectures for the world model, as long
as p(sτ:T+h∖sτ-ι, aτ—i：T+H-ι,θ) is differentiable (e.g., see GP-models in Curi et al. (2020b)).
3.2	A Bayesian view on model-based reinforcement learning
The Bayesian predictive distribution A common challenge in MBRL is that policy optimization
can “overfit” by exploiting inaccuracies of the estimated model. A natural remedy is to quantify
uncertainty in the model, to identify in which situations the model is more likely to be wrong.
This is especially true for safety-critical tasks in which model inaccuracies can mislead the agent
into taking unsafe actions. A natural approach to uncertainty quantification is to take a Bayesian
perspective, where we adopt a prior on the model parameters, and perform (approximate) Bayesian
inference to obtain the posterior. Given such a posterior distribution over the model parameters
p(θ∖D), we marginalize over θ to get a Bayesian predictive distribution. Therefore, by maintaining
such posterior, p(s「T+h ∖st-i, a/—i：T+h-i) is determined by the law of total probability
P(ST:t+H ∖sτ ―1, aτ —1:T+H-1) = Eθ〜p(θ∣D) [p(sτ:t+H ∖st —1, aτ —1:T +H-1, θ)] ∙	(4)
Such Bayesian reasoning forms the basis of celebrated MBRL algorithms such as PETS (Chua et al.,
2018) and PILCO (Deisenroth & Rasmussen, 2011), among others.
Maintaining a posterior over model parameters Since exact Bayesian inference is typically
infeasible, we rely on approximations. In contrast to the popular approach of bootstrapped ensem-
bles (Lakshminarayanan et al., 2017) widely used in RL research, we use the Stochastic Weight
Averaging-Gaussian (SWAG) approximation (Maddox et al., 2019) of p(θ∖D). Briefly, SWAG con-
structs a posterior distribution over model parameters by performing moment-matching over iterates
of stochastic gradient decent (SGD) (Robbins, 2007). The main motivation behind this choice is the
lower computational and memory footprint of SWAG compared to bootstrapped ensembles. This
consideration is crucial when working with parameter-rich world models (such as the RSSM). Note
that our approach is flexible and admits other variants of approximate Bayesian inference, such as
variational techniques (Graves, 2011; Kingma & Welling, 2014).
4	Lagrangian Model-based Agent (LAMBDA)
We first present our proposed approach for planning, i.e., given a world model representing a CMDP,
how to use it to solve the CMPD. Following that, we present our proposed method for finding the
optimistic and pessimistic models with which the algorithm solves the CMDP before collecting
more data.
4.1	S olving CMPDs with model-based constrained policy optimization
The Augmented Lagrangian We first consider the optimization problem in Equation (3). To find
a policy that solves Equation (3), we take advantage of the Augmented Lagrangian with proximal
relaxation method as described by Nocedal & Wright (2006). First, we observe that
C
max min
π∈Π λ≥0
J(∏) - ∑λi (Ji(π) - di)
i=1
J(π)	ifπ is feasible
max
π∈Π -∞ otherwise
(5)
is an equivalent form of Equation (3). Hereby λi are the Lagrange multipliers, each corresponding
to a safety constraint measured by Ji(π). In particular, if π is feasible, we have Ji(π) ≤ di ∀i ∈
4
Published as a conference paper at ICLR 2022
{1, . . . , C} and so the maximum over λi is satisfied if λi = 0 ∀i ∈ {1, . . . , C}. Conversely, if π is
infeasible, at least one λi can be arbitrarily large to solve the problem in Equation (5). Particularly,
Equation (5) is non-smooth when π transitions between the feasibility and infeasibility sets. Thus,
in practice, we use the following relaxation:
max min
π∈Π λ≥0
CC
J(∏) - X λi (Ji(∏) - di) + — X (λi - λk)2
i=1	μk i=1
(*)
where μk is a non-decreasing penalty term corresponding to gradient step k. Note how the last term
in (*) encourages λi to stay proximal to the previous estimate λik and as a consequence making (*)
a smooth approximation of the left hand side term in Equation (5). Differentiating (*) with respect
to λi and substituting back leads to the following update rule for the Lagrange multipliers:
∀i∈ {1,...,C} : λik+1
λk + μk(Ji(π) - di)	ifλk + μk(Ji(π) - di) ≥ 0
otherwise
We take gradient steps of the following unconstrained objective:
C
J(π; λk,μk) = J(∏) — XΨi(∏; λk,μk),
i=1
where
Ψi(∏ λk,μk)
λk(Ji(∏)- di) + μ (Ji(∏) — di)2
(λk)2
2 μ k
ifλk + μk(Ji(∏)- di) ≥ 0
otherwise.
(6)
(7)
(8)
0
Task and safety critics For the task and safety critics, we use the reward value function v(st) and
the cost value function of each constraint vi(st) ∀i ∈ {1, . . . , C}. We model v(st) and vi (st) as
neural networks with parameters ψ and ψi respectively. Note that we omit here the dependency of
the critics in π to reduce the notational clutter. As in Hafner et al. (2019b), we use TD(λ) (Sutton
& Barto, 2018) to trade-off the bias and variance of the critics with bootstrapping and Monte-Carlo
value estimation. To learn the task and safety critics, we minimize the following loss function
1 τ+H	2
Lvn (ψ) = E∏,p(sτ :T + H ∣Sτ-1 , aτ — 1:t + H — 1 ,θ )	2H E (vψ(St)- Vλ(St))
t=τ
(9)
which uses the model to generate samples from p(s/:T+h∣Sτ-ι, aT-i：T+h-i, θ). We denote vψ
as the neural network that approximates its corresponding value function and Vλ as the TD(λ)
value as presented in Hafner et al. (2019b). Similarly, Viλ is the TD(λ) value of the ith constraint
∀i ∈ {1, . . . , C}. Note that, while we show here only the loss for the task critic, the loss function
for the safety critics is equivalent to Equation (9).
Policy optimization We model the policy as a Gaussian distribution via a neural network with
parameters ξ such that ∏ξ (a/s. = N (at； NNg(sj NNg (st)). We sample a sequence s「T+h 〜
p(ST:T+H|ST-1, aT-1:T+H-1, θ), utilizing πξ to generate actions on the fly (see Appendix I). To
■
estimate J(∏; λk ,μk), we compute Vλ and Vλ for each state in the sampled sequence. This approx-
imation leads to the following loss function for policy learning
Cnξ (£)	∙^πξ ,p(sτ :t + H | sτ — 1 , aτ — 1: τ+ H—1 ,θ)
1 T+H
H X -Vλ(St)
t=T
C
+ X Ψi(∏ξ； λk,μk).
i=1
(10)
We approximate Ji(∏ξ) in Ψi(∏ξ; λk, μk) (recall Equation (8)) as
T+H
J (πξ ) ≈ E∏ξ ,p(sτ ：T + H ∣Sτ-1 , aτ — 1:t + H — 1 ,θ )
T+H
H X VxStt .
(11)
t=T
Moreover, the first expression in the policy’s loss (10) approximates J(πξ). We backpropagate
through p(ST:T+H|ST-1, aT-1:T+H-1, θ) using path-wise gradient estimators (Mohamed et al.,
2020).
5
Published as a conference paper at ICLR 2022
Figure 3: Posterior sampling: We sample j = 1,...,N models θj 〜 p(θ∣D) (e.g., in this illus-
tration, N = 5). For each model, we simulate trajectories that are conditioned on the same policy
and initial state. Objective and constraints: For a given posterior sample θj , we use the simulated
trajectories to estimate J (π, pθj ) and Ji (π, pθj ) ∀i ∈ {1, . . . , C} with their corresponding critics.
Upper bounds: Choose largest estimate for each of J (π, pθj ) and Ji (π, pθj ) ∀i ∈ {1, . . . , C}
among their N realizations.
4.2	Adopting optimism and pessimism to explore in CMDPs
Optimistic and pessimistic models As already noted by Curi et al. (2020a), greedily maximizing
the expected returns by averaging over posterior samples in Equation (4) is not necessarily the best
strategy for exploration. In particular, this greedy maximization does not deliberately leverage the
epistemic uncertainty to guide exploration. Therefore, driven by the concepts of optimism in the
face of uncertainty and upper confidence reinforcement learning (UCRL) (Auer et al., 2009; Curi
et al., 2020a), we define a set of statistically plausible transition distributions denoted by P and let
pθ ∈ P be a particular transition density in this set. Crucially, we assume that the true model p? is
Within the support of P, and that by sampling θ 〜p(θ∣D) the conditional predictive density satisfies
p(st+1 |st, at, θ) = pθ ∈ P. These assumptions lead us to the following constrained problem
max max J(π, pθ) s.t. max Ji (π, pθi ) ≤ di, ∀i ∈ {1, . . . , C}.
π∈Π pθ ∈P	pθi ∈P
(12)
The main intuition here is that jointly maximizing J(π,pθ) With respect to π and pθ can lead the
agent to try behaviors With potentially high reWard due to optimism. On the other hand, the pes-
simism that arises through the inner maximization term is crucial to enforce the safety constraints.
Being only optimistic can easily lead to dangerous behaviors, While being pessimistic can lead the
agent to not explore enough. Consequently, We conjecture that optimizing for an optimistic task ob-
jective J(π,pθ) combined With pessimistic safety constraints Ji(π,pθi) alloWs the agent to explore
better task-solving behaviors While being robust to model uncertainties.
Estimating the upper bounds We propose an algorithm that estimates the objective and con-
straints in Equation (12) through sampling, as illustrated in Figure 3. We demonstrate our method in
Algorithm 1. Importantly, We present Algorithm 1 only With the notation of the task critic. HoWever,
We use it to approximate the model’s upper bound for each critic independently, i.e., for the costs as
Well as the reWard value function critics.
Algorithm 1 Upper confidence bounds estimation via posterior sampling
Require: N,p(θ∖D~),p(sτ:T+h|s「-1, aT-1：T+H-ι,θ), Sτ-ι,∏(at, |st).
1:	Initialize V = {}	# Set of objective estimates, under different posterior samples.
2:	for j = 1 to N do
3:	θ ~ p(θ∣D).	# Posterior sampling (e.g., via SWAG).
4:	St：T+h 〜p(sτ：t+h∣Sτ-ι, aτ—i：T+H-1, θ) .	# Sequence sampling, see Appendix I.
5:	Append V — V ∪ PT=TH Vλ(st).
6:	end for
7:	return max V.
The LAMBDA algorithm Algorithm 2 describes hoW all the previously shoWn components in-
teract With each other to form a model-based policy optimization algorithm. For each update step,
We sample a batch of B sequences With length L from a replay buffer to train the model. Then, We
sample N models from the posterior and use them to generate novel sequences With horizon H from
every state in the replay buffer sampled sequences.
6
Published as a conference paper at ICLR 2022
Algorithm 2 LAMBDA
1:	Initialize D by following a random policy or from an offline dataset.
2:	while not converged do
3:	for u = 1 to U update steps do
4:	Sample B sequences {(a/，-i：T'+l-i, Ot't+l/t,：T'+L,cT，：T'+l)}〜D uniformly.
5:	Update model parameters θ and φ.	# E.g., see Hafner et al. (2019a) for the RSSM.
6:	Infer sτ0:T0+L 〜qφ(IoT:T +L, aτ0 — 1:T0 + L-1).
7:	Compute PtT=+TH Vλ(st), PtT=+TH Viλ(st) via Algorithm 1. Use each state in sT0:T0+L as an
initial state for sequence generation.
8:	Update ψ and ψi via Equation (9) with PtT=+TH Vλ(st) and PtT=+TH Viλ (st).
9:	Update ξ according to Equation (10) with PtT=+TH Vλ (st) and PtT=+TH Viλ (st).
10:	Update λi via Equations (6) and (11).
11:	end for
12:	for t = 1 to T do
13:	Infer St 〜qφ(∙∣ot, at—ι, st—i).
14:	Sample at 〜∏ξ(∙∣st).
15:	Take action at, observe rt, cit, ot+1 received from the environment.
16:	end for
17:	Update dataset D — D∪ {oi：T, ai：T, ri：T, c；：T}.
18:	end while
5 Experiments
We conduct our experiments with the SG6 benchmark as described by Ray et al. (2019), aiming to
answer the following questions:
•	How does our model-based approach compare to model-free variants in terms of perfor-
mance, sample efficiency and constraint violation?
•	What is the effect of replacing our proposed policy optimization method with an online
planning method? More specifically, how does LAMBDA’s policy compare to CEM-MPC
of Liu et al. (2021)?
•	How does our proposed optimism-pessimism formulation compare to greedy exploitation
in terms of performance, sample efficiency and constraint violation?
We provide an open-source code for our experiments, including videos of the trained agents at
https://github.com/yardenas/la-mbda.
5.1	SG6 Benchmark
Experimental setup In all of our experiments with LAMBDA, the agent observes 64×64 pixels
RGB images, taken from the robot’s point-of-view, as shown in Figure 2. Also, since there is only
one safety constraint, we let Jc(π) ≡ Ji(π). We measure performance with the following metrics,
as proposed in Ray et al. (2019):
•	Average undiscounted episodic return for E episodes: J(∏) = E PiE=； PTpO rt
•	Average undiscounted episodic cost return for E episodes: Jc(∏) = E Pi=； PTpO Ct
•	Normalized sum of costs during training, namely the cost regret: for a given number of
total interaction steps T, We define ρc(∏) = PtT0 CC as the cost regret.
We compute J(π) and Jc(π) by averaging the sum of costs and reWards across E = 10 evaluation
episodes of length Tep = 1000, Without updating the agent’s netWorks and discarding the interac-
tions made during evaluation. In contrast to the other metrics, to compute ρc(π), We sum the costs
accumulated during training and not evaluation episodes. The results for all methods are recorded
once the agent reached 1M environment steps for PointGoal1, PointGoal2, CarGoal1 and 2M steps
7
Published as a conference paper at ICLR 2022
J (∏)	Jc(∏)	pc(π)
Figure 4: Experimental results for the SG6 benchmark. As done in Ray et al. (2019), we normalize
the metrics and denote J(∏), Jc(∏),pc(∏) as the normalized metrics (See also Appendix H). We
note that LAMBDA achieves better performance while satisfying the constraints during test time.
Furthermore, LAMBDA attains similar result to the baseline in terms of the cost regret metric. We
also note that CPO performs similarly to LAMBDA in solving some of the tasks but fails to satisfy
the constraints in all tasks.
for PointButton1, PointPush2, DoggoGoal1 environments respectively. To reproduce the scores of
TRPO-Lagrangian, PPO-Lagrangian and CPO, we follow the experimental protocol of Ray et al.
(2019). We give further details on our experiment with CEM-MPC at the ablation study section.
Practical aspects on runtime The simulator’s integration step times are 0.002 and 0.004 seconds
for the Point and Car robots, making each learning task run roughly 30 and 60 minutes in real life re-
spectively. The simulator’s integration step time of the Doggo robot is 0.01 seconds thereby training
this task in real life would take about 167 minutes. These results show that in principle, the data ac-
quisition time in real life can be very short. However in practice, the main bottleneck is the gradient
step computation which takes roughly 0.5 seconds on a single unit of Nvidia GeForceRTX2080Ti
GPU. In total, it takes about 18 hours to train an agent for 1M interaction steps, assuming we take
100 gradient steps per episode. In addition, for the hyperparameters in Appendix C, we get a total
of 12M simulated interactions used for policy and value function learning per episode.
Results The results of our experiments are summarized in Figure 4. As shown in Figure 4,
LAMBDA is the only agent that satisfies the safety constraints in all of the SG6 tasks. Further-
more, thanks to its model-based policy optimization method, LAMBDA requires only 2M steps to
successfully solve the DoggoGoal1 task and significantly outperform the other approaches. In Ap-
pendix B we examine further LAMBDA’s sample efficiency compared to the model-free baseline
algorithms. Additionally, in the PointGoal2 task, which is denser and harder to solve in terms of
safety, LAMBDA significantly improves over the baseline algorithms in all metrics. Moreover, in
Figure 1 we compare LAMBDA’s ability to trade-off between the average performance and safety
metrics across all of the SG6 tasks. One main shortcoming of LAMBDA is visible in the Point-
Push1 environment where the algorithm fails to learn to push to box to the goal area. We attribute
this failure to the more strict partial observability of this task and further analyse it in Appendix D.
5.2	Ablation study
Unsafe LAMBDA As our first ablation, we remove the second term in Equation (10) such that
the policy’s loss only comprises the task objective (i.e., with only optimistic exploration). We make
the following observations: (1) Both LAMBDA and unsafe LAMBDA solve the majority of the
SG6 tasks, depending on their level of partial observability; (2) in the PointButton1, PointGoal1
and CarGoal1, LAMBDA achieves the same performance of unsafe LAMBDA, while satisfying the
safety constraints; (3) LAMBDA is able to reach similar performance to unsafe LAMBDA in the
PointGoal2 task which is strictly harder than the other tasks in terms of safety, as shown in Figure 5.
We note that partial observability is present in all of the SG6 tasks due to the restricted field of view
8
Published as a conference paper at ICLR 2022
0.0	0.2 0.4 0.6	0.8 1.0	0.0 0.2 0.4	0.6 0.8 1.0
Training steps ×106	Training steps ×106
—CPO (10M steps, Proprio) — PPO Lagrangian (10M steps, Proprio) — TRPO Lagrangian (10M steps, Proprio)
—LAMBDA — Unsafe LAMBDA
Figure 5: Learning curves of LAMBDA and its unsafe version on the PointGoal2 environment. As
shown, LAMBDA exhibits similar performance in solving the task while maintaining the safety
constraint and significantly improving over the baseline algorithms.
of the robot; i.e., the goal and obstacles are not always visible to the agent. In the DoggoGoal1 task,
where the agent does not observe any information about the joint angles, LAMBDA is still capable
of learning complex walking locomotion. However, in the PointPush1 task, LAMBDA struggles to
find a task-solving policy, due to the harder partial observability of this task (see Appendix D). We
show the learning curves for this experiment in Appendix E.
Ablating policy optimization Next, we replace our actor-critic procedure and instead of perform-
ing policy optimization, we implement the proposed MPC method of Liu et al. (2021). Specifically,
for their policy, Liu et al. (2021) suggest to use the CEM to maximize rewards over action sequences
while rejecting the unsafe ones. By utilizing the same world model, we are able to directly compare
the planning performance for both methods. As shown in Figure 11, LAMBDA performs consid-
erably better than CEM-MPC. We attribute the significant performance difference to the fact that
the CEM-MPC approach does not use any critics for its policy, making its policy short-sighted and
limits its ability to address the credit assignment problem.
Optimism and pessimism compared to greedy exploitation Finally, we compare our upper
bounds estimation procedure with the more common approach of greedily maximizing the expected
performance. More specifically, instead of employing Algorithm 1, we use Monte-Carlo sampling to
estimate Equation (4) by generating trajectories with N = 5 sampled models and taking the average
trajectory over the samples. By doing so, we get an estimate of the mean trajectory over sampled
models together with the intrinsic stochasticity of the environment. We report our experiment results
in Appendix G and note that LAMBDA is able to find safer and more performant policies than its
greedy version.
6	Conclusions
We introduce LAMBDA, a Bayesian model-based policy optimization algorithm that conforms
to human-specified safety constraints. LAMBDA uses its Bayesian world model to generate
trajectories and estimate an optimistic bound for the task objective and pessimistic bounds for
the constraints. For policy search, LAMBDA uses the Augmented Lagrangian method to solve
the constrained optimization problem, based on the optimistic and pessimistic bounds. In our
experiments we show that LAMBDA outperforms the baseline algorithms in the Safety-Gym
benchmark suite in terms of sample efficiency as well as safety and task-solving metrics. LAMBDA
learns its policy directly from observations in an end-to-end fashion and without prior knowledge.
However, we believe that introducing prior knowledge with respect to the safety specifications (e.g.,
the mapping between a state and its cost) can improve LAMBDA’s performance. By integrating
this prior knowledge, LAMBDA can potentially learn a policy that satisfies constraints only from its
model-generated experience and without ever violating the constraints in the real world. This leads
to a notable open question on how to integrate this prior knowledge within the world model such that
the safety constraints are satisfied during learning and not only at the end of the training process.
9
Published as a conference paper at ICLR 2022
7	Acknowledgments and Disclosure of Funding
This project has received funding from the European Research Council (ERC) under the European
Union’s Horizon 2020 research and innovation programme grant agreement No 815943, the Swiss
National Science Foundation under NCCR Automation, grant agreement 51NF40 180545 and the
Swiss National Science Foundation, under grant SNSF 200021 172781.
10
Published as a conference paper at ICLR 2022
References
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization, 2017.
E. Altman. Constrained Markov Decision Processes. Chapman and Hall, 1999.
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mane. Con-
crete problems in ai safety, 2016.
Peter Auer and Ronald Ortner. Logarithmic online regret bounds for undiscounted reinforcement
learning. In B. Scholkopf, J. Platt, and T. Hoffman (eds.), Advances in Neural Information Pro-
cessing Systems, volume 19. MIT Press, 2007. URL https://proceedings.neurips.
cc/paper/2006/file/c1b70d965ca504aa751ddb62ad69c63f-Paper.pdf.
Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for re-
inforcement learning. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou
(eds.), Advances in Neural Information Processing Systems, volume 21. Curran Asso-
ciates, Inc., 2009. URL https://proceedings.neurips.cc/paper/2008/file/
e4a6222cdb5b34375400904f03d8e6a5- Paper.pdf.
Felix Berkenkamp, Matteo Turchetta, Angela P. Schoellig, and Andreas Krause. Safe model-based
reinforcement learning with stability guarantees, 2017.
Homanga Bharadhwaj, Aviral Kumar, Nicholas Rhinehart, Sergey Levine, Florian Shkurti, and Ani-
mesh Garg. Conservative safety critics for exploration, 2021.
Ronen I. Brafman and Moshe Tennenholtz. R-max - a general polynomial time algorithm for near-
optimal reinforcement learning. J. Mach. Learn. Res., 3(null):213-231, March 2003. ISSN
1532-4435. doi: 10.1162/153244303765208377. URL https://doi.org/10.1162/
153244303765208377.
Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained re-
inforcement learning with percentile risk criteria. CoRR, abs/1512.01629, 2015. URL http:
//arxiv.org/abs/1512.01629.
Yinlam Chow, Ofir Nachum, Aleksandra Faust, Edgar Duenez-Guzman, and Mohammad
Ghavamzadeh. Lyapunov-based safe policy optimization for continuous control, 2019.
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement
learning in a handful of trials using probabilistic dynamics models. CoRR, abs/1805.12114, 2018.
URL http://arxiv.org/abs/1805.12114.
Djork-Arne Clevert, Thomas Unterthiner, and SePP Hochreiter. Fast and accurate deep network
learning by exponential linear units (elus), 2016.
Sebastian Curi, Felix Berkenkamp, and Andreas Krause. Efficient model-based reinforcement learn-
ing through optimistic policy search and planning, 2020a.
Sebastian Curi, Silvan Melchior, Felix Berkenkamp, and Andreas Krause. Structured variational
inference in partially observable unstable gaussian process state space models. In Learning for
Dynamics and Control, pp. 147-157. PMLR, 2020b.
Sebastian Curi, Ilija Bogunovic, and Andreas Krause. Combining pessimism with optimism for
robust and efficient model-based deep reinforcement learning, 2021.
Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Paduraru, and Yuval
Tassa. Safe exploration in continuous action spaces, 2018.
Marc Peter Deisenroth and Carl Edward Rasmussen. Pilco: A model-based and data-efficient ap-
proach to policy search. In Proceedings of the 28th International Conference on International
Conference on Machine Learning, ICML’11, pp. 465-472, Madison, WI, USA, 2011. Omnipress.
ISBN 9781450306195.
Stefan Depeweg, Jose Miguel Hernandez-Lobato, Finale Doshi-Velez, and Steffen Udluft. Decom-
position of uncertainty in bayesian deep learning for efficient and risk-sensitive learning, 2017.
11
Published as a conference paper at ICLR 2022
Esther Derman, Daniel Mankowitz, Timothy Mann, and Shie Mannor. A bayesian approach to
robust reinforcement learning, 2019.
Yonathan Efroni, Shie Mannor, and Matteo Pirotta. Exploration-exploitation in constrained mdps,
2020.
Benjamin Eysenbach, Shixiang Gu, Julian Ibarz, and Sergey Levine. Leave no trace: Learning to
reset for safe and autonomous reinforcement learning, 2017.
Javier Garcia, Fern, and o Fernandez. A comprehensive survey on safe reinforcement learning.
Journal of Machine Learning Research, 16(42):1437-1480, 2015. URL http://jmlr.org/
papers/v16/garcia15a.html.
Alex Graves. Practical variational inference for neural networks. In J. Shawe-
Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Q. Weinberger (eds.), Advances
in Neural Information Processing Systems, volume 24. Curran Associates, Inc.,
2011.	URL https://proceedings.neurips.cc/paper/2011/file/
7eb3c8be3d411e8ebfab08eba5f49632- Paper.pdf.
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft actor-critic algo-
rithms and applications, 2019.
Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. In International Conference on
Machine Learning, pp. 2555-2565, 2019a.
Danijar Hafner, Timothy P. Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to con-
trol: Learning behaviors by latent imagination. CoRR, abs/1912.01603, 2019b. URL http:
//arxiv.org/abs/1912.01603.
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson.
Averaging weights leads to wider optima and better generalization, 2019.
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-
based policy optimization, 2019.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes, 2014.
Torsten Koller, Felix Berkenkamp, Matteo Turchetta, and Andreas Krause. Learning-based model
predictive control for safe exploration. In 2018 IEEE Conference on Decision and Control (CDC),
pp. 6059-6066. IEEE, 2018.
Dirk P. Kroese, Sergey Porotsky, and Reuven Y. Rubinstein. The cross-entropy method for con-
tinuous multi-extremal optimization. Methodology and Computing in Applied Probability, 8
(3):383-407, Sep 2006. ISSN 1573-7713. doi: 10.1007/s11009-006-9753-0. URL https:
//doi.org/10.1007/s11009-006-9753-0.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles, 2017.
Jingqi Li, David Fridovich-Keil, Somayeh Sojoudi, and Claire J. Tomlin. Aug-
mented lagrangian method for instantaneously constrained reinforcement learning prob-
lems. 2021. URL https://people.eecs.berkeley.edu/~sojoudi/Augmented_
Lagrangian_Constrained_RL.pdf.
Zuxin Liu, Hongyi Zhou, Baiming Chen, Sicheng Zhong, Martial Hebert, and Ding Zhao. Con-
strained model-based reinforcement learning with robust cross-entropy method, 2021.
Wesley Maddox, Timur Garipov, Pavel Izmailov, Dmitry Vetrov, and Andrew Gordon Wilson. A
simple baseline for bayesian uncertainty in deep learning, 2019.
12
Published as a conference paper at ICLR 2022
Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient esti-
mation in machine learning, 2020.
Teodor Mihai Moldovan and Pieter Abbeel. Safe exploration in markov decision processes. CoRR,
abs/1205.4810, 2012. URL http://arxiv.org/abs/1205.4810.
Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines.
In Proceedings of the 27th International Conference on International Conference on Machine
Learning ,ICML'10,pp. 807-814, Madison, WL USA, 2010. OmniPress.ISBN 9781605589077.
Arnab Nilim and Laurent El Ghaoui. Robust Control of Markov Decision Processes
with Uncertain Transition Matrices. Operations Research, 53(5):780-798, October 2005.
doi: 10.1287/opre.1050.0216. URL https://ideas.repec.org/a/inm/oropre/
v53y2005i5p780-798.html.
Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer, New York, NY, USA,
second edition, 2006.
Alex Ray, Joshua Achiam, and Dario Amodei. Benchmarking Safe Exploration in Deep Reinforce-
ment Learning. 2019.
Herbert E. Robbins. A stochastic approximation method. Annals of Mathematical Statistics, 22:
400-407, 2007.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms, 2017.
Adam Stooke, Joshua Achiam, and Pieter Abbeel. Responsive safety in reinforcement learning by
pid lagrangian methods, 2020.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. A Bradford
Book, Cambridge, MA, USA, 2018. ISBN 0262039249.
Aviv Tamar, Huan Xu, and Shie Mannor. Scaling up robust mdps by reinforcement learning, 2013.
Chen Tessler, Yonathan Efroni, and Shie Mannor. Action robust reinforcement learning and appli-
cations in continuous control, 2019.
Matteo Turchetta, Andrey Kolobov, Shital Shah, Andreas Krause, and Alekh Agarwal. Safe rein-
forcement learning via curriculum induction, 2021.
Kim P. Wabersich and Melanie N. Zeilinger. A predictive safety filter for learning-based control of
constrained nonlinear dynamical systems, 2021.
Moritz A. Zanger, Karam DaaboUL and J. Marius Zollner. Safe continuous control with constrained
model-based policy optimization, 2021.
13
Published as a conference paper at ICLR 2022
A Learning curves for the SG6 benchmark
CarGoalI
20
06
X
UJnsJ as。。eωBJeAV
PointPUshI
- - -
4 2 0
UJnJ piUMBJ eBJeAV
06
X
UJnsJ aso。eωBJeAV
DoggoGoalI
UJnsJ PJBMeJ eωBJeAV
20
06
X
UJnsJ aso。eωBJeAV
PointGoal2
UJna。J PJuMeJ eωBJeAV
5 O
1 1
06
X
UJnsJs∞ eωBJeAV
PointBUttonI
UJnsJ piUMBJ eωBJeAV
06
X
UJna。Js00 eωBJeAV
PointGoalI
0.0	0.2	0.4	0.6	0.8	1.0
Training steps ×106
—LAMBDA -- CPO (ProPrio)
UJnsJ aso。eωBJeAV
PPO Lagrangian (proprio) — TRPO Lagrangian (proprio)
UJnsJ piUMBJ eωBJeAV
O
- -S
O
2
.
O
6
.
O
8
.
O
O
.
1

O
.
O
O
.
1
O
.
2
W
O
.
O
5 O
1 1
O
.
1
O
.
2
2
.
O
6
.
O
8
.
O
O
.
1

O
.
1
O
.
2
Figure 6:	Benchmark results of LAMBDA. Solid red lines indicate the threshold value d = 25.
Dashed lines correspond to the benchmark results for the baseline algorithms after 10M training
steps for all tasks except the DoggoGoal1, which is trained for 100M environment steps, as in Ray
et al. (2019). Shaded areas represent the 5% and 95% confidence intervals across 5 different random
seeds.
14
Published as a conference paper at ICLR 2022
B	S ample efficiency
We record LAMBDA'S performance J(∏) atthe end of training and find the average number of steps
(across seeds) required by the baseline model-free methods to match this performance. We demon-
strate LAMBDA’s sample efficiency with the ratio of the amount of steps required by the baseline
methods and amount of steps at the end of LAMBDA’s training. As shown in Figure 7, in the major-
ity of tasks, LAMBDA is substantially more sample efficient. In the PointPush1 task LAMBDA is
outperformed by the baseline algorithms, however we assign this to the partial observability of this
task, as further analyzed in Appendix D. It is important to note that by taking LAMBDA’s perfor-
mance at the end of training, we take a conservative approach, as convergence can occur with many
less steps, as shown in Figure 6.
Required amount of steps to reach LAMBDA's performance
××
10 1
oitar spetS
CPO
PPO-Lagrangian
TRPO-Lagrangian
Figure 7:	LAMBDA solves most of the tasks with significantly less interactions with the envi-
ronment, compared to the baseline model-free methods. Green check marks and red ‘x’ indicate
constraint satisfaction after reaching the required number of steps.
15
Published as a conference paper at ICLR 2022
C Hyperparameters
In this section we specify the most prominent hyperparameters for LAMBDA, however we encour-
age the reader to visit https://github.com/yardenas/la-mbda as it holds many specific
(but important) implementation details. Table 1 summarizes the hyperparameters used in our algo-
rithm.
World model As mentioned before, for our world model we use the RSSM proposed in Hafner
et al. (2019a). Therefore, most of the architectural design choices (e.g., specific details of the con-
volutional layers) are based on the work there.
Cost function We exploit our prior knowledge of the cost function’s structure in the Safety-Gym
tasks. Since we know a-priori that the cost function is implemented as an indicator function, our
neural network approximation for it is modeled as a binary classifier. As training progresses, the
dataset becomes less balanced as the agent observes less unsafe states. To deal with this issue, we
give higher weight to the unsafe interactions in the binray classification loss.
Posterior over model parameters in supervised learning, SWAG is typically used under the as-
sumption of a fixed dataset D, such that weight averaging takes place only during the last few
training epochs. To adapt SWAG to the dataset’s distributional shifts that occur during training in
RL, we use an exponentially decaying running average. This allows us to maintain a posterior even
within early stages of learning, as opposed to the original design of SWAG that normally uses the
last few iterates of SGD to construct the posterior. Furthermore, we use a cyclic learning rate (Iz-
mailov et al., 2019; Maddox et al., 2019) to help SWAG span over different regions of the weight
space.
Policy The policy outputs actions such that each element in at is within the range [-1, 1]. How-
ever, since We model ∏ξ(at∣st) as N(at； NNμ(st), NNg (st)), We perform squashing of the Gaus-
sian distribution into the range [-1, 1], as proposed in Haarnoja et al. (2019) and Hafner et al.
(2019b) by transforming it through a tangent hyperbolic bijector. We scale the actions in this Way so
that the model, Which takes the actions as an input is more easily optimized. Furthermore, to improve
numerical stability, the standard deviation term NNξσ(st) is passed through a softplus function.
Value functions To ensure learning stability for the critics, We maintain a shadoW instance for each
value function Which is used in the bootstrapping in Equation (9). We clone the shadoW instance
such that it lags U update steps behind its corresponding value function. Furthermore, We stop
gradient on Vλ When computing the loss function in Equation (9).
General Parameter update for all of the neural netWorks is done With ADAM optimizer (Kingma
& Ba, 2014). We use ELU (Clevert et al., 2016) as activation function for all of the netWorks except
for the convolutional layers in the World model in Which We use ReLU (Nair & Hinton, 2010).
16
Published as a conference paper at ICLR 2022
Table 1: Hyperparameters for LAMBDA. For other safety tasks, we recommend first tuning the
initial Lagrangian, penalty and penalty power factor at different scales and then fine-tune the safety
discount factor to improve constraint satisfaction. We emphasize that it is possible to improve the
performance of each task separately by fine-tuning the hyperparameters on a per-task basis.
Name	Symbol	Value	Additional
World model			
Batch size	B	32	
Sequence length	L	50	
Learning rate		1e-4	
SWAG			
Burn-in steps		500	Steps before weight averaging starts.
Period steps		200	Use weights every ‘Period steps’ to update weights running average.
Models		20	Averaging buffer length.
Decay		0.8	Exponential averaging decay factor.
Cyclic LR factor		5.0	End cyclic lr period with the base LR times this factor.
Posterior samples	N	5	
Safety			
Safety critic learning rate		2e-4	
Initial penalty	μo	5e-9	
Initial Lagrangian	λ10	1e-6	
Penalty power factor		1e-5	Multiply μk by this factor at each gradient step.
Safety discount factor		0.995	
General			
Update steps	U	100	
Critic learning rate		8e-5	
Policy learning rate		8e-5	
Action repeat		2	Repeat same action for this amount of steps.
Discount factor		0.99	
TD(λ) factor	λ	0.95	
Sequence generation horizon	H	15	
17
Published as a conference paper at ICLR 2022
D PointPush 1 environment with a transparent box
Figure 8: PointPush1 with partially transparent box. When the color of the box is solid LAMBDA
struggles in solving the task due to occlusion of the goal by the box. By changing the transparency
of the box, we make the PointPush1 task less partially observable and thus easier to solve.
In all of our experiments with the PointPush1 task, we provide the agent image observations in
which the box is observed as a solid non-transparant object. As previously shown, we note that
in this setting, LAMBDA and unsafe LAMBDA fail to learn the task. We maintain that this failure
arises from the fact that this task is significantly harder in terms of partial observability, as the goal is
occluded by the box while the robot pushes the box.1 Furthermore, in Ray et al. (2019), the authors
eliminate issues of partial observability by using what they term as “pseudo-LiDAR” which is not
susceptible to occlusions as it can see through objects. To verify our hypothesis, we change the
transparency of the box such that the goal is visible through it, as shown in Figure 8 and compare
LAMBDA’s performance with the previously tested PointPush1 task. We present the learning curves
of the experiments in Figure 9. As shown, LAMBDA is able to safely solve the PointPush1 task if
PJEmeJ
100 -
2.0
× 106
0.5	1.0	1.5
Training steps
UJnsJ∙jsοο 8MEJ8AV
150 -
50
j¾L-fc A---r--
0.0
0 -
0.0
0.08-
0.06 -
0.04
0.02
2.0
× 106
0.5	1.0	1.5
Training steps
0.5	1.0	1.5
Training steps
2.0
× 106
0.0
—CPO (10M steps, ProPrio) — PPO Lagrangian (10M steps, ProPrio) — TRPO Lagrangian (10M steps, ProPrio)
--------------------------------Transparent box ---- Opaque box
Figure 9:	Learning curves for the PointPush1 task and ObservablePointPush1 task which uses par-
tially transparent box. We also show the baseline algorithms performance with a “pseudo-LiDAR”
observation.
the goal is visible through the box. We conclude that the PointPush1 task makes an interesting test
case for future research on partially observable environments.
1Please see https://github.com/yardenas/la-mbda for a video illustration.
18
Published as a conference paper at ICLR 2022
E UNSAFE LAMBDA
POintBUttonI
UlnsJ pɪeməj əMaJ9>v
UlnsJ aso。əMaJ9>v
300
DoggoGoalI
CarGoalI
PointGoal2
OOOC
6 4 2
UJnJ pɪeməj əmpjəʌ,v
0.5	1.0	1.5
PomtGoalI
PointPUshI
0.5	1.0	1.5	2.
X106
0 5 0 5c
2 11
UlnJ pɪeməj əJ9>v
0.2	0.4	0.6	0.8
100-
50-
0-
UJnsJ aso。əMaJ9>v
0.0	0.5	1.0	1.5	2.0
X106
0 O
O 5
UlnJ aso。əJ9>v
0.0	0.2
0.4	0.6	0.8	1.0
× 106
0 5 0 5 c
2 11
UJnJ pɪeməj əJ9>v
0.2	0.4	0.6	0.8	1.0
Training steps	× 106
0 0
O O
2 1
UlnJ aso。əJ9>v
0.2	0.4	0.6	0.8	1.0
Training steps	× 106
0.0
0.0
0.08 -
I 0.06 -
S
。0.04 -
0.02 -
0.08 -
⅛ 0.06 -
0 0.04 -
O
0.02 -
0.125 -
0.100 -
0.075 -
0.050 -
0.025 -
0.3 -
2 1
00
JS9J asoo
0.0	0.2	0.4	0.6	0.8	1.0
Training steps	× 106
—LAMBDA - Unsafe LAMBDA
Figure 10:	Benchmark results of LAMBDA and its unsafe implementation. In the majority of the
tasks, LAMBDA is able to find policies that perform similarly to the unsafe version while satisfying
the constraints. Interestingly, apart from the DoggoGoal1 and PointGoal2 tasks LAMBDA’s policies
are able to achieve similar returns while learning to satisfy the constraints.
19
Published as a conference paper at ICLR 2022
F Comparison with CEM-MPC
CarGoalI
UJnsJ piUMBJ eωBJeAV
O O
O 5
1x
UJnJ as。。eBJeAV
PointPUShI
UJnsJ PJBMeJ eωBJeAV
2 10 1
-
DoggoGoalI
PointGoal2
PointGoalI
UJn∙jeJ PJuMeJ eωBJeAV
0.08 -
0.06 -
0.04 -
0.
0.08 -
0.06 -
0.04 -
0.02 -
0.
0.10 -
0.08 -
0.06 -
0.04 -
0.02 -
0.5	1.0	1.5	2.0
×106
0.5	1.0	1.5	2.0
×106
UJnsJ piUMBJ eωBJeAV
Oooo
0 5 0 5
2 11
UJna。J00 eBJeAV
PointBUttonI
UJnsJ piUMBJ eωBJeAV
2 200 -
⅛
o
O
效 100 -
∙⅞
0 一
- - -
0 5 0
2 II
UJnsJ eωBJeAV
0.0	0.2	0.4	0.6	0.8	1.0
Training steps	×106
0.0	0.2	0.4	0.6	0.8	1.0
×106
300
0.2	0.4	0.6	0.8	1.0
×106
0.0	0.2	0.4	0.6	0.8	1.0
Training steps	×106
—LAMBDA - CEM-MPC
0.08 -
0.06 -
0.04 -
0.02 -
AAN Jin a

Figure 11:	Comparison of LAMBDA when ablating the policy optimization and using CEM-MPC
with rejection sampling as introduced in Liu et al. (2021). As shown, LAMBDA performs substan-
tially better than CEM-MPC. We believe that when the goal is not visible to the agent, CEM-MPC’s
policy fails to locate it and drive the robot to it. On the contrary, in our experiments, we observed
that LAMBDA typically rotates until the goal becomes visible, thus allowing the robot to gather
significantly more goals.
20
Published as a conference paper at ICLR 2022
G Optimism and pessimism compared to greedy exploitation
CarGoalI
PointPUShI
DoggoGoalI
0.08 -
0.06 -
0.04 -
0.02 -
0.0	0.5	1.0	1.5	2.0
×106
PointGoal2
PointBUttonI
PointGoalI
0.0	0.2	0.4	0.6	0.8	1.0
Training steps	×106
0.0
0.5	1.0	1.5	2.0
×106
Oooo
0 5 0 5
2 11
UJnJ∞ eBJeAV
Training steps
—Greedy LAMBDA
0.2	0.4	0.6	0.8	1.0
×106
LAMBDA


Figure 12:	Comparison of LAMBDA and greedy exploitation. LAMBDA generally solves that
tasks with better performance and with improved sample efficiency. Notably, the greedy exploitation
variant fails to solve the the pointButton1 task.
21
Published as a conference paper at ICLR 2022
H Comparing algorithms in the SG6 benchmark
To get a summary of how different algorithms behave across all of the SG6 tasks, we follow the
proposed protocol in Ray et al. (2019). That is, we obtain “characteristic metrics” for each of the
environments by taking the metrics recorded at the end of training of an unconstrained PPO agent
(Schulman et al., 2017). We denote these metrics as JPPO, JPPO and PPPO. We then normalize the
recorded metrics for each environment as follows:
J(∏)
ʌ
J(∏)
ʌ
JPPO
Jc(∏)
max(0, Jc (π ) - d)
max(10-6, JPPO — d)
(13)
Pc(π)
Pc(π)
PPPO .
By performing this normalization with respect to the performance of PPO, we can take an average
of each metric across all of the SG6 environments.
To produce Figure 1, we scale the normalized metrics to [0, 1], such that for each metric, the best
performing algorithm attains a score of 1.0 and the worst performing algorithm attains a score of 0.
22
Published as a conference paper at ICLR 2022
I Backpropagating gradients through a sequence
Algorithm 3 Sampling from the predictive density pθ(s/:T+h|s「-ι, aT—i：T+h-i, θ)
Require: ∏ξ (a/sjps (st+ι∣st, at), ST-1
1:	for t = τ - 1 to τ + H do
2:	at 〜∏ξ(・|stop_gradient(St))	# Stop gradient from St when conditioning the policy on it.
3:	St+ι 〜p(∙∣st, at,θ)
4:	end for
5:	return ST:T+H
We use the reparametrization trick (Kingma & Welling, 2014) to compute gradients through sam-
pling procedures as both πξ and pθ are modeled as normal distributions. Backpropagating gradients
through the model can be easily implemented with modern automatic differentiation tools.
Importantly, we stop the gradient computation in Algorithm 3 when conditioning the policy on
St-1. We do so to avoid any recurrent connections between an action at-1 and the preceding states
to St such that eventually, backpropagation to actions occurs only from their dependant succeeding
values. We further illustrate this in Figure 13.
(a) Forward pass of trajectory sampling. We stop gra- (b) Backward pass from values to their inducing ac-
dient flow on the magenta dashed arrows.	tions.
Figure 13: Computational graphs for the backward and forward passes of Algorithm 3.
23
Published as a conference paper at ICLR 2022
J Scores for SG6
The .json format files, summarizing the scores for the experiments of this work are available at
https://github.com/yardenas/la-mbda.
Table 2: LAMBDA’s unnormalized scores for the SG6 tasks.
	J⑴	ʌ Jc(∏)	ρc(π)
PointGoal1	18.822	11.200	0.034
PointGoal2	13.300	9.100	0.043
CarGoal1	16.745	23.100	0.036
PointPush1	0.314	21.400	0.017
PointButton1	5.372	21.700	0.038
DoggoGoal1	5.867	11.400	0.046
Average	10.07	16.317	0.0360
Table 3: Experiment results for the SG6 benchmark. We present the results with the tuple
(J(∏), Jc(∏), pc(∏)) of the normalized metrics.
	TRPO-Lagrangian	PPO-Lagrangian	CPO	LAMBDA
PointGoal1	0.51, 0.004, 0.405	0.24, 0.0, 0.419	0.898, 0.302, 0.599	1.077, 0.0, 0.483
PointGoal2	0.119, 0.059, 0.304	0.09, 0.197, 0.349	0.306, 0.132, 0.377	0.902, 0.0, 0.229
CarGoal1	0.501, 0.0, 0.522	0.255, 0.0, 0.474	1.579, 0.604, 0.924	1.284, 0.0, 0.704
PointPush1	0.714, 0.0, 0.315	0.185, 0.0, 0.249	1.606, 0.311, 0.687	0.203, 0.0, 0.309
PointButton1	0.077, 0.0, 0.223	0.058, 0.0, 0.242	0.516, 0.343, 0.495	0.287, 0.0, 0.302
DoggoGoal1	-1.257, 0.227, 0.624	-0.891,0.293,0.707	-0.723, 0.643, 0.769	5.400, 0.0, 0.770
SG6 (average)	0.111, 0.048, 0.399	-0.011,0.082, 0.407	0.697, 0.389, 0.642	1.526, 0.0, 0.466
24