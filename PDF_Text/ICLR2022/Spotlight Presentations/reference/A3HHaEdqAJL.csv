title,year,conference
 Neural Network Learning - Theoretical Foundations,2002, Cam-bridge University Press
 Stronger generalization bounds fordeep nets via a compression approach,2018, In ICML
 Spectrally-normalized margin bounds forneural networks,2017, In NeurIPS
 Nearly-tight VC-dimensionand Pseudodimension bounds for piecewise linear neural networks,2019, JMLR
 A model of inductive bias learning,2000, Journal of Artificial Intelligence Research
 Exploiting task relatedness for mulitple task learning,2003, In COLT
 Representation learning: A review andnew perspectives,2013, IEEE TPAMI
 Learnability andthe Vapnik-Chervonenkis dimension,1989, JACM
 Measure Theory,2007, Springer-Verlag Berlin Heidelberg
 A closer lookat the training strategy for modern meta-learning,2020, In NeurIPS
 Model-agnostic meta-learning for fast adaptationof deep networks,2017, In ICML
 Implicit regularization in matrix factorization,2017, In NeurIPS
 Decision theoretic generalizations of the PAC model for neural net and other learn-ing applications,1992, Information and Computation
 Identity mappings in deep residualnetworks,2016, In ECCV
 Adaptive gradient-based meta-learning methods,2019, In NeurIPS
 Imagenet classification with deep convo-lutional neural networks,2012, In NeurIPS
 Meta-SGD: Learning to learn quickly for fewshot learning,2017, arXiv:1707
 Transfer bounds for linear feature learning,2009, Machine Learning
 The benefit of multitaskrepresentation learning,2016, JMLR
 Exploring general-ization in deep learning,2017, In NeurIPS
 A PAC-Bayesian bound for lifelong learning,2014, InICML
 Convergence of Stochastic Processes,1984, Springer-Verlag
 Prototypical networks for few-shot learning,2017, InNeurIPS
 On the theory of transfer learning: The impor-tance of task diversity,2020, In NeurIPS
 A theory of the learnable,1984, Communication ofACM
 Einige Satze uber messbare abbildungen,1932, Annals of Mathematics
 Improved sample complexities for deep neural networks and robustclassification via an All-Layer margin,2020, In ICLR
