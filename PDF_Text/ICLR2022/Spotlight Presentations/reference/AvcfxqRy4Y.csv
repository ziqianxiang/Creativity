title,year,conference
 Mixture modelattention: Flexible streaming and non-streaming automatic speech recognition,2021, Proc
 wav2vec 2,2020,0: A frame-work for self-supervised learning of speech representations
 Unsupervised speech recogni-tion,2021, arXiv preprint arXiv:2105
 What does bert lookat? an analysis of bertâ€™s attention,2019, In Proceedings of the 2019 ACL Workshop BlackboxNLP:Analyzing and Interpreting Neural Networks for NLP
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, In ACL (1)
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In NAACL-HLT (1)
 Efficient training of bertby progressively stacking,2337, In International Conference on Machine Learning
 Connectionist tem-poral classification: labelling unsegmented sequence data with recurrent neural networks,2006, InProceedings of the 23rd international conference on Machine learning
 How far does bert look at: Distance-based clustering and analysis of bert 0 s attention,2020, arXiv preprint arXiv:2011
 Conformer: Convolution-augmentedtransformer for speech recognition,2020, In Proc
 Recent develop-ments on espnet toolkit boosted by conformer,2021, In ICASSP 2021-2021 IEEE International Con-ference onAcoustics
 Hubert: Self-supervised speech representation learning by maskedprediction of hidden units,2021, arXiv preprint arXiv:2106
 Averaging weights leads to wideroptima and better generalization,2018, In 34th Conference on Uncertainty in Artificial Intelligence2018
 Attention is not only a weight:Analyzing transformers with vector norms,2020, In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP)
 Revealing the dark secretsof bert,2019, In Proceedings of the 2019 Conference on Empirical Methods in Natural LanguageProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)
 Sentencepiece: A simple and language independent subwordtokenizer and detokenizer for neural text processing,2018, In Proceedings of the 2018 Conference onEmpirical Methods in Natural Language Processing: System Demonstrations
 Mockingjay: Un-supervised speech representation learning with deep bidirectional transformer encoders,2020, InICASSP 2020-2020 IEEE International Conference on Acoustics
 Tera: Self-supervised learning of transformer encoderrepresentation for speech,2021, IEEE/ACM Transactions on Audio
 Decoupled weight decay regularization,2018, In International Confer-ence on Learning Representations
 Probing acoustic representations for phonetic prop-erties,2021, In ICASSP 2021-2021 IEEE International Conference on Acoustics
 Pushing the limits of non-autoregressive speech recognition,2021, arXiv preprint arXiv:2104
 Representation learning with contrastive predic-tive coding,2018, arXiv preprint arXiv:1807
 Scaling neural machine translation,2018, InProceedings of the Third Conference on Machine Translation: Research Papers
 Librispeech: an asr corpusbased on public domain audio books,2015, In 2015 IEEE international conference on acoustics
 Sanvis: Visual analytics for understandingself-attention networks,2019, In 2019 IEEE Visualization Conference (VIS)
 Specaugment on large scale datasets,2020, In ICASSP 2020-2020 IEEE InternationalConference on Acoustics
 Blockwise self-attention for long document understanding,2020, In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing: Findings
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2020, Journal of Machine Learning Research
 A primer in bertology: What we know abouthow bert works,2020, Transactions of the Association for Computational Linguistics
 Neural machine translation of rare words withsubword units,2016, In Proceedings of the 54th Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers)
 Bi-directional block self-attention for fast and memory-efficient sequence modeling,2018, In International Conference on Learn-ing Representations
 Weak-attention suppression for transformer based speech recog-nition,2020, In Interspeech
 Emformer: Efficient memory transformer based acoustic model for lowlatency streaming speech recognition,2021, In ICASSP 2021-2021 IEEE International Conference onAcoustics
 Effective attention sheds light on interpretability,2021, arXiv preprintarXiv:2105
 Bert rediscovers the classical nlp pipeline,2019, In Proceed-ings of the 57th Annual Meeting of the Association for Computational Linguistics
 Transformer trans-dUcer: One model Unifying streaming and non-streaming speech recognition,2020, arXiv preprintarXiv:2010
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Efficient conformer with prob-sparse attentionmechanism for end-to-endspeech recognition,2021, In INTERSPEECH
 Sharing attention weights forfast transformer,2019, In Proceedings of the Twenty-Eighth International Joint Conference on ArtificialIntelligence (IJCAI-19)
 Understanding self-attention of self-supervisedaudio transformers,2020, In Proc
 Streaming attention-based models with augmented memory for end-to-endspeech recognition,2021, In 2021 IEEE Spoken Language Technology Workshop (SLT)
 Transformer transducer: A streamable speech recognition model with transformer en-coders and rnn-t loss,2020, In ICASSP 2020-2020 IEEE International Conference on Acoustics
 Stochastic attention head removal:A simple and effective method for improving transformer based asr models,2021, In Proc
 On the usefulness of self-attentionfor automatic speech recognition with transformers,2021, In 2021 IEEE Spoken Language TechnologyWorkshop (SLT)
