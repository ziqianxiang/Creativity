title,year,conference
 Tree-structured compositionin neural networks without tree-structured architectures,2015, In Proc
 A fast unified model for parsing and sentence understanding,2016, In Proceedingsof the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: LongPapers)
 Learning context-free grammars: Capabilities andlimitations of a recurrent neural network with an external stack memory,1992, In Proc
 Recurrent neural networkgrammars,1024, In Proc
 SyntaxGym: An online platformfor targeted evaluation of language models,2020, In Proc
 Learning to trans-duce with unbounded memory,2015, In Proc
 The hardest context-free language,1973, SIAM J
 Context-free transductions with neural stacks,2018, In Proc
 A systematic assessment ofsyntactic generalization in neural language models,2020, In Proc
 Inferring algorithmic patterns with stack-augmented recurrentnets,2015, In Proc
 Compound probabilistic context-free grammars forgrammar induction,2019, In Proc
 Conditional random fields:Probabilistic models for segmenting and labeling sequence data,2001, In Proceedings of the EighteenthInternational Conference on Machine Learning
 Deterministic techniques for efficient non-deterministic parsers,1974, In Proc
 Does syntax need to grow on trees? Sources ofhierarchical inductive bias in sequence-to-sequence networks,2020, Trans
 Regularizing and optimizing LSTM lan-guage models,2018, In Proc
 Empirical evaluationand combination of advanced language modeling techniques,2011, In Proc
 ListOps: A diagnostic dataset for latent tree learning,2018, InProc
 Recurrent dropout without memory loss,2016, InProc
 In Proc,2019, NeurIPS
 Ordered neurons: Integratingtree structures into recurrent neural networks,2019, In Proc
 Quantity doesnâ€™t buy quality syntax with neurallanguage models,2019, In Proc
 Memory architectures in recurrent neural network language models,2018, In Proc
