title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 Perturbed identity matrices have high rank: Proof and applications,2009, Combinatorics
 On the optimization of deep networks: Implicit accelerationby overparameterization,2018, In International Conference on Machine Learning
 Fine-grained analysis of optimization andgeneralization for overparameterized two-layer neural networks,2019, In International Conference on MachineLearning
 On exactcomputation with an infinitely wide neural net,2019, arXiv preprint arXiv:1904
 Reconciling modern machine-learning practiceand the classical bias-variance trade-off,2019, Proceedings of the National Academy of Sciences
 On the opportunities and risks offoundation models,2021, arXiv preprint arXiv:2108
 Stable signal recovery from incomplete andinaccurate measurements,2006, Communications on Pure and Applied Mathematics: A Journal Issued by theCourant Institute of Mathematical Sciences
 Generalization error bounds of gradient descent for learning over-parameterizeddeep relu networks,2020, In Proceedings of the AAAI Conference on Artificial Intelligence
 Slide:In defense of smart algorithms over hardware acceleration for large-scale deep learning systems,2019, arXivpreprint arXiv:1903
 Scatterbrain: Unifying sparseand low-rank attention,2021, In NeurIPS
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 Unifying orthogonal Monte Carlomethods,2019, In International Conference on Machine Learning
 The diagonal decomposition technique applied to the dynamic programmingsolution of elliptic partial differential equations,1971, Journal of Mathematical Analysis and Applications
 CUDA Programming: A Developerâ€™s Guide to Parallel Computing with GPUs,9780, MorganKaufmann Publishers Inc
 Learning fast algorithms forlinear transforms using butterfly factorizations,2019, In International conference on machine learning
 A two-pronged progressin structured dense matrix vector multiplication,2018, In Proceedings of the Twenty-Ninth Annual ACM-SIAMSymposium on Discrete Algorithms
 Imagenet: A large-scale hierarchicalimage database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Learning to prune deep neural networks via layer-wiseoptimal brain surgeon,2017, arXiv preprint arXiv:1705
 An image is worth16x16 words: Transformers for image recognition at scale,2020, arXiv preprint arXiv:2010
 Gradient descent provably optimizes over-parameterized neural networks,2019, In ICLR
 Rigging the lottery: Makingall tickets winners,2020, In International Conference on Machine Learning
 Sparse coding in the primate cortex,2003, The handbook of brain theory and neural networks
 Variable selection is hard,2015, In Conference on Learning Theory
 Stabilizing the lotteryticket hypothesis,2019, arXiv preprint arXiv:1903
 Linear mode connectivityand the lottery ticket hypothesis,3259, In International Conference on Machine Learning
 Resprop: Reuse sparsified backpropagation,2020, In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR)
 Gpu kernels for block-sparse weights,2017, arXiv preprintarXiv:1711
 Learning both weights and connections for efficientneural networks,2015, arXiv preprint arXiv:1506
 Training dynamics of deep networks using stochasticgradient descent via neural tangent kernel,2019, arXiv preprint arXiv:1905
 The hardware lottery,2020, arXiv preprint arXiv:2009
 Neural tangent kernel: Convergence and generalizationin neural networks,2018, arXiv preprint arXiv:1806
 Clustering partially observed graphs via convexoptimization,2011, In ICML
 Highly accurate protein structureprediction with alphafold,2021, Nature
 Scaling laws for neural language models,2020, arXiv preprintarXiv:2001
 Reformer: The efficient transformer,2020, In The InternationalConference on Machine Learning (ICML)
 Learning multiple layers of features from tiny images,2009, 2009
 Optimal brain damage,1990, In Advances in neural informationprocessing systems
 Snip: Single-shot network pruning based onconnection sensitivity,2018, arXiv preprint arXiv:1810
 Pruning filters for efficientconvnets,2016, arXiv preprint arXiv:1608
 Learning overparameterized neural networks via stochastic gradient descent onstructured data,2018, In NeurIPS
 Runtime neural pruning,2017, In I
 Finding trainable sparse networks through neural tangent transfer,2020, InInternational Conference on Machine Learning
 High dimensional low rank and sparse covariance matrix estimation via convex minimization,2011, arXivpreprint arXiv:1111
 Proving the lottery ticket hypothesis:Pruning is all you need,2020, In International Conference on Machine Learning
 Fast approximation of rotations and Hessians matrices,2014, arXiv preprintarXiv:1404
 One ticket to win them all: generalizinglottery ticket initializations across datasets and optimizers,2019, arXiv preprint arXiv:1906
 Quadrature-based featuresfor kernel approximation,2018, In S
 Deep doubledescent: Where bigger models and more data hurt,2019, arXiv preprint arXiv:1912
 Deep learning recom-mendation model for personalization and recommendation systems,2019, arXiv preprint arXiv:1906
 Logarithmic pruning is all you need,2020, Advances inNeural Information Processing Systems
 Random butterfly transformations with applications in computational linear algebra,1995, 1995
 Optimal lotterytickets via subsetsum: Logarithmic over-parameterization is sufficient,2020, arXiv preprint arXiv:2006
 Language modelsare unsupervised multitask learners,2019, OpenAI blog
 Sparse weight activation training,2020, arXiv preprint arXiv:2001
 Weighted low rank approximations with provableguarantees,2016, In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing
 Quadratic suffices for over-parametrization via matrix chernoff bound,2019, arXivpreprint arXiv:1906
 Pruning neural networks without anydata by iteratively conserving synaptic flow,2020, arXiv preprint arXiv:2006
 Long range arena: A benchmark for efficient transformers,2020, arXivpreprint arXiv:2011
 The penn treebank: an overview,2003, Treebanks
 Regression shrinkage and selection via the lasso,1996, Journal of the Royal Statistical Society:Series B (Methodological)
 Butterfly transform: Anefficient fft based neural architecture design,2020, In 2020 IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR)
 Picking winning tickets before training by preservinggradient flow,2020, arXiv preprint arXiv:2002
 Nystromformer: A Nystrom-based algorithm for approximating self-attention,2021, arXiv preprintarXiv:2102
 Tokens-to-token vit: Training vision transformers from scratch on imagenet,2021, arXiv preprintarXiv:2101
 Scaling vision transformers,2021, arXivpreprint arXiv:2106
 Cpm: A large-scale generative Chinese pre-trained language model,2021, AI Open
