title,year,conference
 Layer normalization,2016, arXiv preprintarXiv:1607
 Cascade r-cnn: high quality object detection and instancesegmentation,2019, IEEE Trans
 Pre-trained image processing transformer,2020, arXiv preprintarXiv:2012
 Twins: Revisiting spatial attention design in vision transformers,2021, arXiv preprintarXiv:2104
 MMSegmentation: Openmmlab semantic segmentation toolbox andbenchmark,2020, https://github
 On the relationship between self-attention and convolutional layers,2020, In Int
 Imagenet: A large-scalehierarchical image database,2009, In IEEE Conf
 Representative batchnormalization with feature calibration,2021, In IEEE Conf
 Beyond self-attention: Externalattention using two linear layers for visual tasks,2021, arXiv preprint arXiv:2105
 A survey on visual transformer,2020, arXiv preprintarXiv:2012
 Transformer intransformer,2021, arXiv preprint arXiv:2103
 Deep residual learning for imagerecognition,2016, In IEEE Conf
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Gather-excite: Exploiting featurecontext in convolutional neural networks,2018, In Adv
 Deep networks withstochastic depth,2016, In Eur
 Interlacedsparse self-attention for semantic segmentation,2019, CoRR
 Ccnet:Criss-cross attention for semantic segmentation,2019, In Int
 Shuffle transformer:Rethinking spatial shuffle for vision transformer,2021, arXiv preprint arXiv:2106
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In Int
 Transformers in vision: A survey,2021, arXiv preprint arXiv:2101
 Pattern Recog,2021,
 Pay attention to mlps,2021, arXiv preprintarXiv:2105
 Swin transformer: Hierarchical vision transformer using shifted windows,2021, arXiv preprintarXiv:2103
 Shufflenet v2: Practical guidelines forefficient cnn architecture design,2018, In Eur
 Rectified linear units improve restricted boltzmann machines,2010, InInt
 Scalable visual transformers withhierarchical pooling,2021, arXiv preprint arXiv:2103
 Hopfield networksis all you need,2020, arXiv preprint arXiv:2008
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In IEEE Conf
 Igcv3: Interleaved low-rank grouP convolutionsfor efficient deeP neural networks,2018, In Brit
 DeeP high-resolution rePresentation learning forhuman Pose estimation,2019, In IEEE Conf
 Efficientnetv2: Smaller models and faster training,2021, arXiv preprintarXiv:2104
 MlP-mixer: Anall-mlP architecture for vision,2021, arXiv preprint arXiv:2105
 Training data-efficient image transformers & distillation through attention,2020, arXiv preprintarXiv:2012
 ResmlP: Feedforwardnetworks for image classification with data-efficient training,2021, arXiv preprint arXiv:2105
 Pattern Recog,2021,
 DeeP high-resolution rePresentationlearning for visual recognition,2020, IEEE Trans
 Pvtv2: ImProved baselines with Pyramid vision transformer,2021, arXiv preprintarXiv:2106
 Pyramid vision transformer: A versatile backbone for dense Prediction withoutconvolutions,2021, arXiv preprint arXiv:2102
 Pay less attention withlightweight and dynamic convolutions,2019, In Int
 Cvt:Introducing convolutions to vision transformers,2021, arXiv preprint arXiv:2103
 Unified perceptual parsing forscene understanding,2018, In Eur Conf
 Aggregated residualtransformations for deep neural networks,2017, In IEEE Conf
 In IEEE Conf,2021, Comput
 Incorporatingconvolution designs into visual transformers,2021, arXiv preprint arXiv:2103
 Tokens-to-token vit: Training vision transformers from scratch on imagenet,2021, arXivpreprint arXiv:2101
 mixup: Beyond empiricalrisk minimization,2018, In Int
 Rest: An efficient transformer for visual recognition,2021, arXivpreprint arXiv:2105
 Exploring self-attention for image recognition,2020, InIEEE Conf
 Deep convolutional neural networks with merge-and-run mappings,2018, In J6r6meLang (ed
 Random erasing data augmenta-tion,2020, In Assoc
 Sceneparsing through ade20k dataset,2017, In IEEE Conf
