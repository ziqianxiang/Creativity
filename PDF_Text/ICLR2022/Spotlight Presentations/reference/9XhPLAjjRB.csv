title,year,conference
 Escaping saddles with stochasticgradients,1155, In International Conference on Machine Learning
 The heavy-tail phenomenon in sgd,3964, InInternational Conference on Machine Learning
 How to escape saddle pointsefficiently,1724, In International Conference on Machine Learning
 Gradient descent only converges tominimizers,1246, In Conference on learning theory
 Ageneric approach for escaping saddle points,1233, In International conference on artificial intelligenceand statistics
 Fast and faster convergence of sgd for over-parameterized models and an accelerated perceptron,1195, In The 22nd International Conference onArtificial Intelligence and Statistics
 On the noisy gradient descentthat generalizes as sgd,1036, In International Conference on Machine Learning
 The anisotropic noise in stochastic gradientdescent: Its behavior of escaping from sharp minima and regularization effects,7654, In InternationalConference on Machine Learning
