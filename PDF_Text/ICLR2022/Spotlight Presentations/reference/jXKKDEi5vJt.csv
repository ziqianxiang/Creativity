title,year,conference
 Byzantine stochastic gradient descent,2018, In NeurIPS -Advances in Neural Information Processing Systems
 Byzantine-resilient non-convexstochastic gradient descent,2021, In ICLR 2021 - International Conference of Learning Representations
 Howto backdoor federated learning,2020, In AISTATS - Proceedings of the Twenty Third InternationalConference on Artificial Intelligence and Statistics
 A Little Is Enough: Circumventing Defenses ForDistributed Learning,2019, NeurIPS
 signSGD withmajority vote is communication efficient and fault tolerant,2018, arXiv preprint arXiv:1810
 Machine Learning withAdversaries: Byzantine Tolerant Gradient Descent,2017, In NeurIPS - Advances in Neural InformationProcessing Systems 30
 Towardsfederated learning at scale: System design,2019, In SysML - Proceedings of the 2nd SysML Conference
 Draco: Byzantine-resilient distributed training via redundant gradients,2018, arXiv preprint arXiv:1803
 Geometric medianin nearly linear time,2016, In Proceedings of the forty-eighth annual ACM symposium on Theory ofComputing
 Aggregathor: Byzantine machine learning via robustgradient aggregation,2019, Conference on Systems and Machine Learning (SysML) 2019
 Byzantine-resilient high-dimensional sgd with local iterations onheterogeneous data,2021, In ICML 2021 - 37th International Conference on Machine Learning
 The hidden vulnerability ofdistributed learning in byzantium,2018, In International Conference on Machine Learning
 Distributed momentum for byzantine-resilient stochastic gradient descent,2021, In ICLR 2021 - International Conference on LearningRepresentations
 Robust federated learning in aheterogeneous environment,2019, arXiv preprint arXiv:1906
 Resilience in collaborative optimization: redundant andindependent cost functions,2020, arXiv preprint arXiv:2003
 Byzantine-robust decentralized learning viaself-centered clipping,2022, arXiv preprint arXiv:2202
 Advanees and open problems infederated learning,2019, arXiv preprint arXiv:1912
 Error FeedbaekFixes SignSGD and other Gradient Compression Sehemes,2019, In ICML 2019 - Proceedings of the36th International Conference on Machine Learning
 SCAFFOLD: Stoehastie eontrolled averaging for federated learning,2020, InICML 2020 - International Conference on Machine Learning
 Learning from history for byzantine robustoptimization,2021, In ICML 2021 - 37th International Conference on Machine Learning
 Tighter theory for loeal sgd on identiealand heterogeneous data,2020, In AISTATS 2020 - International Conference on Artificial Intelligence andStatistics
 The byzantine generals problem,2019, In Concur-rency: the Works ofLeslie Lamport
 Gradient-based learning applied todoeument reeognition,1998, Proceedings of the IEEE
 RSA: Byzantine-robuststoehastie aggregation methods for distributed learning from heterogeneous datasets,2019, In Proceed-ings of the AAAI Conference on Artificial Intelligence
 The power of interpolation: Understanding theeffeetiveness of sgd in modern over-parametrized learning,2018, In International Conference onMachine Learning
 Mlitb:machine learning in the browser,2376, PeerJ Computer Science
 Fastand furious convergence: Stochastic second order methods under interpolation,2020, In InternationalConference OnArtificial Intelligence and Statistics
 Geometric median and robust estimation in banach spaces,2015, Bernoulli
 Simplified neuron model as a principal component analyzer,1982, Journal of mathematicalbiology
 Detox: A redundancy-based framework for faster and more robust gradient aggregation,2019, arXiv preprint arXiv:1907
 On the byzantine robustness of clustered federatedlearning,2020, In ICASSP 2020 - 2020 IEEE International Conference on Acoustics
 Fast convergence of stochastic gradient descent under a stronggrowth condition,2013, arXiv preprint arXiv:1308
 Fast and faster convergence of sgd for over-parameterized models and an accelerated perceptron,2019, In AISTATS 2019 - The 22nd InternationalConference on Artificial Intelligence and Statistics
 Federated variance-reducedstochastic gradient descent with robustness to byzantine attacks,2020, IEEE Transactions on SignalProcessing
 Zeno: Distributed stochastic gradient descentwith suspicion-based fault-tolerance,2019, In ICML 2019 - 35th International Conference on MachineLearning
 Fall of Empires: Breaking Byzantine-tolerantSGD by Inner Product Manipulation,2020, In UAI - Proceedings of The 35th Uncertainty in ArtificialIntelligence Conference
 Basgd: Buffered asynchronous sgd for byzantine learning,2021, In MarinaMeila and Tong Zhang (eds
 Byzantine-robust distributedlearning: Towards optimal statistical rates,2018, arXiv preprint arXiv:1803
 Parallel restarted sgd with faster convergence and lesscommunication: Demystifying why model averaging works for deep learning,2019, In AAAI 2019 -Conference on Artificial Intelligence
