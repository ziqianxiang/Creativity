title,year,conference
 Equilateral triangles: A challenge for connectionist vision,2009,2009
 In the theatre of consciousness,1997, global workspace theory
 Neural machine translation by jointlylearning to align and translate,2015, CoRR
 Deep equilibrium models,2019, arXiv preprintarXiv:1909
 The consciousness prior,2017, arXiv preprint arXiv:1709
 A meta-transfer objective for learning to disentangle causalmechanisms,2019, arXiv preprint arXiv:1901
 Compositional generaliza-tion via neural-symbolic stack machines,2020, arXiv preprint arXiv:2008
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 Rethinking attentionwith performers,2020, arXiv preprint arXiv:2009
 The devil is in the detail: Simple tricksimprove systematic generalization of transformers,2021, arXiv preprint arXiv:2108
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Animage is worth 16X16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Addressingsome limitations of transformers with feedback memory,2020, arXiv preprint arXiv:2002
 Recurrent independent mechanisms,2019, arXiv preprint arXiv:1909
 Object files and schemata: Factorizing declarative andprocedural knowledge in dynamical systems,2020, arXiv preprint arXiv:2006
 Neural production systems,2021, arXiv preprintarXiv:2103
 Coordination among neuralmodules through a shared global workspace,2021, arXiv preprint arXiv:2103
 Neural turing machines,2014, arXiv preprintarXiv:1410
 Compositional attention networks for machinereasoning,2018, arXiv preprint arXiv:1803
 Untangling tradeoffs between recurrence and self-attention inartificial neural networks,2020, Advances in Neural Information Processing Systems
 Measuring composi-tional generalization: A comprehensive method on realistic data,2019, arXiv preprint arXiv:1912
 Reformer: The efficient transformer,2020, arXivpreprint arXiv:2001
 Generalization without systematicity: On the compositional skills ofsequence-to-sequence recurrent networks,2018, In Jennifer Dy and Andreas Krause (eds
 Transformers with competitive ensembles of independent mechanisms,2021, arXiv preprintarXiv:2103
 Compositional generalization forprimitive substitutions,2019, arXiv preprint arXiv:1910
 Memorize or generalize? searching for acompositional RNN in a haystack,2018, CoRR
 Effective approaches to attention-basedneural machine translation,2015, arXiv preprint arXiv:1508
 Fast and slow learning of recurrent independent mechanisms,2021, arXiv preprint arXiv:2105
 Pointer sentinel mixturemodels,2016, arXiv preprint arXiv:1609
 Learning to combine top-down and bottom-up signals inrecurrent neural networks with attention over modules,2020, In International Conference on MachineLearning
 The eos decision andlength extrapolation,2020, arXiv preprint arXiv:2010
 Investigating the limitations of transformers withsimple arithmetic tasks,2021, arXiv preprint arXiv:2102
 A simple neural network module for relational reasoning,2017, arXivpreprint arXiv:1706
 Efficient transformers: A survey,2020, arXivpreprint arXiv:2009
 Attention is all you need,2017, CoRR
 Linformer: Self-attention withlinear complexity,2020, arXiv preprint arXiv:2006
 Emergent symbols through binding in externalmemory,2020, arXiv preprint arXiv:2012
