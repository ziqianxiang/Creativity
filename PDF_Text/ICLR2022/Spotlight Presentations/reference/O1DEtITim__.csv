title,year,conference
 Rezero is all you need: Fast convergence at large depth,2020, arXiv preprintarXiv:2003
 Once-for-all: Train onenetwork and specialize it for efficient deployment,2019, arXiv preprint arXiv:1908
 The lottery tickets hypothesis for supervised and self-supervised pre-trainingin computer vision models,2020, arXiv preprint arXiv:2012
 The lottery ticket hypothesis for pre-trained bert networks,2020, arXiv
 Only train once: A one-shot neural network training and pruningframework,2021, Advances in Neural Information Processing Systems
 Projection-free adaptive gradientsfor large-scale optimization,2020, arXiv preprint arXiv:2009
 Binarizedneural networks: Training deep neural networks with weights and activations constrained to+ 1or-1,2016, arXiv preprint arXiv:1602
 Structured sparsity inducing adaptive optimizers for deep learn-ing,2021, arXiv preprint arXiv:2102
 Exploiting linearstructure within convolutional networks for efficient evaluation,2014, In Advances in neural informa-tion processing systems
 An algorithm for quadratic programming,1956, Naval researchlogistics quarterly
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Gdp: Stabilized neuralnetwork pruning via gates with differentiable polarization,2021, In Proceedings of the IEEE/CVFInternational Conference on Computer Vision
 Variance-reduced and projection-free stochastic optimization,2016, InInternational Conference on Machine Learning
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Channel pruning for accelerating very deep neural net-works,2017, In Proceedings ofthe IEEE International Conference on Computer Vision
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 A data-driven neuron pruning approach towardsefficient deep architectures,2016, arXiv preprint arXiv:1607
 One-cyclepruning: Pruning convnets under a tight training budget,2021, arXiv preprint arXiv:2107
 Snip: Single-shot network pruningbased on connection sensitivity,2018, arXiv preprint arXiv:1810
 A signal propagationperspective for pruning neural networks at initialization,2019, arXiv preprint arXiv:1906
 Pruning filters forefficient convnets,2016, arXiv preprint arXiv:1608
 Dynamic model pruningwith feedback,2020, arXiv preprint arXiv:2006
 Rethinking the value ofnetwork pruning,2018, arXiv preprint arXiv:1810
 Shufflenet v2: Practical guidelines forefficient cnn architecture design,2018, In Proceedings of the European conference on computer vision(ECCV)
 Stochastic conditional gradient methods:From convex minimization to submodular maximization,2020, Journal of machine learning research
 Deep neural network training with frank-wolfe,2020, arXiv preprint arXiv:2010
 Xnor-net: Imagenetclassification using binary convolutional neural networks,2016, In European conference on computervision
 Stochastic frank-wolfe methods fornonconvex optimization,2016, In 2016 54th Annual Allerton Conference on Communication
 Fitnets: Hints for thin deep nets,2014, arXiv preprint arXiv:1412
 Complexities in projection-free stochastic non-convex minimization,2019, In The 22nd International Conference on ArtificialIntelligence and Statistics
 Very deep convolUtional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 PrUning neUral networkswithoUt any data by iteratively conserving synaptic flow,2020, arXiv preprint arXiv:2006
 Picking winning tickets before training bypreserving gradient flow,2020, arXiv preprint arXiv:2002
 Once-for-all adversarial training: In-sitU tradeoff between robUstness and accUracy for free,2020, Advances inNeural Information Processing Systems
 Deepk-means: Re-training and parameter sharing with harder clUster assignments for compressingdeep convolUtions,2018, In International Conference on Machine Learning
 Stochastic recUrsive gradient-based methods for projection-free online learning,2019, 2019
 Drawing early-bird tickets: Toward more efficient trainingof deep networks,2020, In International Conference on Learning Representations
 On compressing deep models by lowrank and sparse decomposition,2017, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Conditional gradient methods via stochastic path-integrated differential estimator,2019, In International Conference on Machine Learning
 FixUp initialization: ResidUal learning withoUtnormalization,2019, arXiv preprint arXiv:1901
 ShUfflenet: An extremely efficientconvolUtional neUral network for mobile devices,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Gra-dinit: Learning to initialize neUral networks for stable and efficient training,2021, arXiv preprintarXiv:2102
