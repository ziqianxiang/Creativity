title,year,conference
 On the accuracy of self-normalized log-linear models,2015, arXiv preprint arXiv:1506
 Implicit generation and generalization in energy-based models,2019, arXivpreprint arXiv:1903
 Notes on noise contrastive estimation and negative sampling,2014, arXiv preprintarXiv:1410
 Flowcontrastive estimation of energy-based models,2020, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition
 Simulating normalizing constants: From importance samplingto bridge sampling to path sampling,1998, Statistical science
 On the convergence of monte carlo maximum likelihood calculations,1994, Journal ofthe Royal Statistical Society: Series B (Methodological)
 Generative adversarial networks,2014, arXiv preprintarXiv:1406
 Your classifier is secretly an energy based model and you should treat it likeone,2019, arXiv preprint arXiv:1912
 Bregman divergence as general framework to estimateunnormalized statistical models,2011, In Proceedings of the Conference on Uncertainty in ArtificialIntelligence (UAI)
 Noise-contrastive estimation: A new estimation principlefor unnormalized statistical models,2010, In Proceedings of the Thirteenth International Conference onArtificial Intelligence and Statistics
 Simple and optimalhigh-probability bounds for strongly-convex stochastic gradient descent,2019, arXiv preprintarXiv:1909
 Logistic regression: Tight bounds for stochastic andonline optimization,2014, In Conference on Learning Theory
 Beyond convexity: Stochastic quasi-convexoptimization,2015, arXiv preprint arXiv:1507
 Data-efficient image recognition with contrastive predictive coding,2020, In InternationalConference on Machine Learning
 Learning deep representations by mutual information estimationand maximization,2018, arXiv preprint arXiv:1808
 Unsupervised feature extraction by time-contrastive learningand nonlinear ica,2016, arXiv preprint arXiv:1605
 A mutual information maximization perspective of language representation learning,2020, InInternational Conference on Learning Representations
 Learning with noise-contrastive estimation: Easing train-ing by learning to scale,2018, In Proceedings of the 27th International Conference on ComputationalLinguistics
 Linking losses for density ratio and class-probability estima-tion,2016, In International Conference on Machine Learning
 Learning word embeddings efficiently with noise-contrastiveestimation,2013, Advances in neural information processing systems
 Annealed importance sampling,2001, Statistics and computing
 f-gan: Training generative neural samplersusing variational divergence minimization,2016, arXiv preprint arXiv:1606
 Representation learning with contrastive predic-tive coding,2018, arXiv preprint arXiv:1807
 A family of computationally efficient andsimple estimators for unnormalized statistical models,2010, In Proceedings of the Conference on Un-certainty in Artificial Intelligence (UAI)
 Telescoping density-ratio estimation,2020, arXivpreprint arXiv:2006
 Generative ratio matchingnetworks,2020, In International Conference on Learning Representations
 Density Ratio Estimation in MachineLearning,0521, Cambridge University Press
 Heavy-tailedstreaming statistical estimation,2021, arXiv preprint arXiv:2108
 A unified statis-tically efficient estimation framework for unnormalized models,2020, In International Conference onArtificial Intelligence and Statistics
 Training deep energy-based models withf-divergence minimization,1095, In International Conference on Machine Learning
