title,year,conference
 OptNet: Differentiable Optimization as a Layer in Neural Networks,2017, InICML
 Deep equilibrium models,2019, In NeurIPS
 Multiscale deep equilibrium models,2020, In NeurIPS
 A Class of Methods for Solving Nonlinear Simultaneous Equations,1965, Mathematics ofComputation
 A tool for the analysis of quasi-NeWton methods With application tounconstrained minimization,1989, SIAM Journal on Numerical Analysis
 Parallel quasi-NeWton methods for unconstrainedoptimization,1988, Mathematical Programming
 Convergence of quasi-NeWton matrices generated by thesymmetric rank one update,0025, Mathematical Programming
 ImageNet: A large-scale hierarchicalimage database,2009, In CVPR
 A theoretical and experimental study of thesymmetric rank-one update,1993, SIAM Journal on Optimization
 Fixed Point NetWorks: ImplicitDepth Models With Jacobian-Free Backprop,2021, Technical report
 The Reversible Residual Network: Backpropa-gation Without Storing Activations,2017, In NIPS
 Randomized quasi-Newton updates are linearly convergent matrixinversion algorithms,2017, SIAM Journal on Matrix Analysis andApplications
 Array programming with NumPy,2020, Nature
 Deep residual learning for image recognition,2016, In CVPR
 Feasibility-based Fixed Point Networks,2021, Technicalreport
 Matplotlib: A 2D graphics environment,2007, Computing in Science and Engineering
 Adam: A method for stochastic optimization,2015, In ICLR
 Learning Multiple Layers of Features from Tiny Images,2009, Technical report
 NewsWeeder: Learning to Filter Netnews,1995, In ICML
 A derivative-free line search and global convergence of Broyden-likemethod for nonlinear equations,2000, Optimization Methods & Software
 Optimizing Millions of Hyperparameters by ImplicitDifferentiation,2020, In AISTATS
 On the convergence of the Broyden-like matrices,2020, 2020
 Convergence properties of the Broyden-like method for mixed linear-nonlinear systemsof equations,2021, Numerical Algorithms
 On the convergence of Broyden’s method and some accelerated schemes for singularproblems,2021, 2021b
 On the global convergence of Broyden’s method,1976, Mathematics ofComputation
 Hyperparameter optimization with approximate gradient,2016, 33rd International ConferenceonMachine Learning
 Momentum Residual Neural Networks,2021, Technicalreport
 On the local convergence of adjoint Broy-den methods,1436, Mathematical Programming
 Adjustment of an inverse matrix corresponding to a change in oneelement of a given matrix,1950, The Annals of Mathematical Statistics
 Benefits of depth in neural networks,2016, Journal of Machine Learning Research
 Second-Order Optimization for Non-ConvexMachine Learning: An Empirical Study,2020, In Proceedings of the 2020 SIAM International Conferenceon Data Mining
