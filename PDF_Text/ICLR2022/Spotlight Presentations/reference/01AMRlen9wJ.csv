title,year,conference
 Metareg: Towards domain gen-eralization using meta-regularization,2018, Advances in neural information processing systems
 Gradient-based optimization of hyperparameters,2000, Neural computation
 Random search for hyper-parameter optimization,2012, Journal ofmachine learning research
 Generic methods for optimization-based modeling,2012, In Artificial Intelligence andStatistics
 Model-agnostic meta-learning for fast adaptationof deep networks,2017, In International conference on machine learning
 Transferringknowledge across learning processes,2018, In International Conference on Learning Representations
 Meta-learning with warped gradient descent,2019, In International Conference on LearningRepresentations
 Forward and reversegradient-based hyperparameter optimization,2017, In International Conference on Machine Learning
 Drmad: distilling reverse-mode automatic differentiation for optimizing hyperparameters of deep neural networks,2016, In Pro-ceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence
 Generalized inner loop meta-learning,2019, arXiv preprint arXiv:1910
 Online hyperparameter optimization byreal-time recurrent learning,2021, arXiv preprint arXiv:2102
 Meta-learning representations for continual learning,2019, Advancesin Neural Information Processing Systems
 Adam: A method for stochastic optimization,2015, In ICLR (Poster)
 Tiny imagenet visual recognition challenge,2015, CS 231N
 Meta dropout: Learning to perturblatent features for generalization,2019, In International Conference on Learning Representations
 Gradient-based meta-learning with learned layerwise metric andsubspace,2018, In International Conference on Machine Learning
 Learning to generalize: Meta-learning for domain generalization,2018, In Thirty-Second AAAI Conference on Artificial Intelligence
 Meta-sgd: Learning to learn quickly for few-shot learning,2017, arXiv preprint arXiv:1707
 Darts: Differentiable architecture search,2018, InInternational Conference on Learning Representations
 Scalable gradient-based tuningof continuous regularization hyperparameters,2016, In International conference on machine learning
 Gradient-based hyperparameter optimiza-tion through reversible learning,2113, In International conference on machine learning
 Non-greedy gradient-based hyperparameter optimization over longhorizons,2020, 2020
 On first-order meta-learning algorithms,2018, arXivpreprint arXiv:1803
 Hyperparameter optimization with approximate gradient,2016, In International con-ference on machine learning
 Rapid learning or feature reuse?towards understanding the effectiveness of maml,2019, In International Conference on Learning Rep-resentations
 Optimization as a model for few-shot learning,2016, 2016
 Learning to reweight examples forrobust deep learning,2018, In International conference on machine learning
 Metaperturb: Transferableregularizer for heterogeneous tasks and architectures,2020, Advances in Neural Information ProcessingSystems
 Truncated back-propagationfor bilevel optimization,2019, In The 22nd International Conference on Artificial Intelligence andStatistics
 Large-scale meta-learning withcontinual trajectory shifting,9603, In International Conference on Machine Learning
 Meta-weight-net: Learning an explicit mapping for sample weighting,2019, Advances in neural informationprocessing systems
 Practical bayesian optimization of machinelearning algorithms,2012, Advances in neural information processing systems
 Cross-domain few-shotclassification via learned feature-wise transformation,2020, In International Conference on LearningRepresentations
 Matching networks for oneshot learning,2016, Advances in neural information processing systems
 Backpropagation through time: what it does and how to do it,1990, Proceedings of theIEEE 
 A learning algorithm for continually running fully recurrentneural networks,1989, Neural computation
 Understanding short-horizon bias instochastic meta-optimization,2018, In International Conference on Learning Representations
