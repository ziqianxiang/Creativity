title,year,conference
 Robust bi-tempered logistic lossbased on bregman divergences,2019, arXiv preprint arXiv:1906
 Deep k-nn for noisy labels,2020, In International Conferenceon Machine Learning
 Unilmv2: Pseudo-masked language models for unified languagemodel pre-training,2020, In International Conference on Machine Learning
 Openml benchmarking suites,2017, arXiv preprintarXiv:1708
 Unsupervised learning by predicting noise,2017, In InternationalConference on Machine Learning
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Self-supervised gan to counter forgetting,2018, arXiv preprintarXiv:1810
 Electra: Pre-training textencoders as discriminators rather than generators,2020, arXiv preprint arXiv:2003
 Randaugment: Practical automateddata augmentation with a reduced search space,2020, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition Workshops
 Bertje: A dutch bert model,2019, arXiv preprint arXiv:1912
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Adversarial feature learning,2016, arXiv preprintarXiv:1605
 Unified language model pre-training for natural language understandingand generation,2019, arXiv preprint arXiv:1905
 Openml-python: an extensible pythonapi for openml,2019, arXiv
 Scaling deep contrastive learning batch size with almost constant peakmemory usage,2021, arXiv preprint arXiv:2101
 Unsupervised representation learning bypredicting image rotations,2018, arXiv preprint arXiv:1803
 Affinity and diversity:Quantifying mechanisms of data augmentation,2020, arXiv preprint arXiv:2002
 Generative adversarial networks,2014, arXiv preprintarXiv:1406
 Bootstrap your own latent: A new approach to self-supervised learning,2020, arXiv preprintarXiv:2006
 Noise-contrastive estimation: A new estimation principlefor unnormalized statistical models,2010, In Proceedings of the Thirteenth International Conference onArtificial Intelligence and Statistics
 Contrastive multi-view representation learning ongraphs,2020, In International Conference on Machine Learning
 Momentum contrast forunsupervised visual representation learning,2020, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition
 Data-efficient image recognition with contrastive predictive coding,2020, In InternationalConference on Machine Learning
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Learning deep representations by mutual information estimationand maximization,2018, arXiv preprint arXiv:1808
 Population based augmentation:Efficient learning of augmentation policy schedules,2019, In International Conference on MachineLearning
 Globally and locally consistent imagecompletion,2017, ACM Transactions on Graphics (ToG)
 Self-supervised feature learning by learning to spot artifacts,2018, InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Self-supervised visual feature learning with deep neural networks: Asurvey,2020, IEEE Transactions on Pattern Analysis and Machine Intelligence
 Spanbert:Improving pre-training by representing and predicting spans,2020, Transactions of the Association forComputational Linguistics
 Cross-lingual language model pretraining,2019, arXiv preprintarXiv:1901
 Albert: A lite bert for self-supervised learning of language representations,2019, arXiv preprintarXiv:1909
 Learning representations for automaticcolorization,2016, In European conference on computer vision
 Colorization as a proxy task forvisual understanding,2017, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 UnsUPervised learning of edges,2016, InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Fast autoaugment,2019, arXivpreprint arXiv:1905
 Roberta: A robustly oPtimized bert PretrainingaPProach,2019, arXiv preprint arXiv:1907
 ImProvingrobustness without sacrificing accuracy with Patch gaussian augmentation,2019, arXiv preprintarXiv:1906
 Distributed rePre-sentations of words and Phrases and their comPositionality,2013, arXiv preprint arXiv:1310
 Learning word embeddings efficiently with noise-contrastiveestimation,2013, Advances in neural information processing systems
 Representation learning with contrastive predictivecoding,2018, arXiv preprint arXiv:1807
 The effectiveness of data augmentation in image classification usingdeep learning,2017, arXiv preprint arXiv:1712
 Unsupervised representation learning with deepconvolutional generative adversarial networks,2015, arXiv preprint arXiv:1511
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Strong baselines for neural semi-supervised learning underdomain shift,2018, arXiv preprint arXiv:1804
 Learning internal representations byerror propagation,1985, Technical report
 Standard dropout as remedy for training deep neural networks with label noise,2020, InInternational Conference on Dependability and Complex Systems
 Mass: Masked sequence to sequencepre-training for language generation,2019, arXiv preprint arXiv:1905
 Mpnet: Masked and permutedpre-training for language understanding,2020, arXiv preprint arXiv:2004
 Rethinkingthe inception architecture for computer vision,2016, In Proceedings of the IEEE conference on computervision and pattern recognition
 Viewmaker networks: Learning views for unsupervisedrepresentation learning,2020, arXiv preprint arXiv:2010
 Contrastive multiview coding,2019, arXiv preprintarXiv:1906
 A bayesian data augmentationapproach for learning deep models,2017, arXiv preprint arXiv:1710
 Self-supervisedlearning of motion capture,2017, arXiv preprint arXiv:1712
 Openml: Networked science inmachine learning,2013, SIGKDD Explorations
 Extracting andcomposing robust features with denoising autoencoders,2008, In Proceedings of the 25th internationalconference on Machine learning
 Stacked denoising autoencoders: Learning useful representations in a deep networkwith a local denoising criterion,2010, Journal of machine learning research
 Denoising based sequence-to-sequence pre-training for text generation,2019, arXiv preprint arXiv:1908
 Unsupervised feature learning via non-parametric instance-level discrimination,2018, arXiv preprint arXiv:1805
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
 Self-supervised learning for large-scale itemrecommendations,2020, arXiv preprint arXiv:2007
 Unsupervised word sense disambiguation rivaling supervised methods,1995, In 33rdannual meeting of the association for computational linguistics
 Vime: Extending the success ofself-and semi-supervised learning to tabular domain,2020, Advances in Neural Information ProcessingSystems
 BarloW twins: Self-supervisedlearning via redundancy reduction,2021, arXiv preprint arXiv:2103
 S4l: Self-supervised semi-supervised learning,2019, In Proceedings of the IEEE/CVF International Conference on ComputerVision
 mixup: Beyond empiricalrisk minimization,2017, arXiv preprint arXiv:1710
 Be yourown teacher: Improve the performance of convolutional neural networks via self distillation,2019, InProceedings of the IEEE/CVF International Conference on Computer Vision
 Colorful image colorization,2016, In Europeanconference on computer vision
 Adversarial autoaugment,2019, arXiv preprintarXiv:1912
 Tri-training: Exploiting unlabeled data using three classifiers,2005, IEEETransactions on knowledge and Data Engineering
