title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 Bagging predictors,1996, Machine learning
 Gradient descent finds globalminima of deep neural networks,2019, In International Conference on Machine Learning
 Gradient descent provably optimizesover-parameterized neural networks,2018, In International Conference on Learning Representations
 Boosting a weak learning algorithm by majority,1995, Information and computation
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In Advances in neural information processing systems
 The ¡°echo state¡± approach to analysing and training recurrent neural networks-withan erratum note,2001, Bonn
 Adaptive estimation of a quadratic functional by model selec-tion,2000, Annals of Statistics
 Wide neural networks of any depth evolve as linear modelsunder gradient descent,2019, In Advances in neural information processing systems
 On the linearity of large non-linear models: whenand why the tangent kernel is constant,2020, Advances in Neural Information Processing Systems
 Loss landscapes and optimization in over-parameterized non-linear systems and neural networks,2022, Applied and Computational HarmonicAnalysis
 Toward moderate overparameterization: Global con-vergence guarantees for training shallow neural networks,2020, IEEE Journal on Selected Areas inInformation Theory
 The strength of weak learnability,1990, Machine learning
 Stochastic gradient descent optimizesover-parameterized deep relu networks,2018, arXiv preprint arXiv:1811
