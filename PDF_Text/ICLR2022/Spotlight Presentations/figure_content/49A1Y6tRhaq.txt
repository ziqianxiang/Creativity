Figure 1: Our framework. (a) We train a referential game on natural images, and use the trainedspeaker to generate a corpus of emergent messages (bold box) to pre-train for (b) languagemodeling and (c) image captioning. We also propose to (d) translate emergent language (yellow) intocorresponding natural language (blue) with same perceptual grounding to evaluate their closeness.
Figure 2: Test perplexity for language modeling on ten natural languages, when either pre-trainedon a corpus of tokens from EC (ec, blue), Spanish (es, orange), well-balanced brackets (paren-zipf,green) or without any pre-training (scratch, red, dotted, constant with respect to source sizes).
Figure 3: Different metrics and downstream Romanian perplexities (negated) with respect to trainingsteps, averaged over four trials with vocabulary limit 10000 and sequence length limit 15. Our metricbetter correlates with downstream performance across time steps.
Figure 4: Unigram distributions of (1) es and paren-zipf, (2) ec, and (3) ec with random speaker.
Figure 5: The validation CIDEr (Vedantam et al., 2015) score across different fine-tuning epochs,when using 5,000, 50,000, or the all samples of MS-COCO training samples.
Figure 6: 200 data points with X axis being metrics (validation acuracy, translation ROUGELtopographic) and y axis being downstream performance (negated perplexity) on Romanian (ro) andHebrew (he), respectively.
Figure 7: How metrics and downstream performances change with respect to vocabulary size (1000,4035, 10000). Sequence length is 15 and training step is 1000.
Figure 8:	How metrics and downstream performances change with respect to sequence length (5, 15,25). Vocabulary size is 4035 and training step is 1000.
Figure 9:	Images from COCO and their natural and emergent captions.
