Figure 1: Results on the very long adding problem for LEM, coRNN, DTRIV∞ (Casado, 2019),FastGRNN (Kusupati et al., 2018), LSTM and LSTM with chrono initialization (Tallec & Ollivier,2018) based on three very long sequence lengths N, i.e., N = 2000, N = 5000 and N = 10000.
Figure 2: Sensitivity study on hyperparameter∆t in (3) using the EigenWorms experiment.
Figure 3: Average (over ten different initializa-tions each) test mean-square error on the addingproblem of LEM for different sequence lengthsN, where the hyperparameter ∆t of LEM (3) isfixed to ∆t = 1 / √N.
Figure 4: Histogram of (∆tn)i and (∆tn)i for all n = 1,...,N and i = 1,...,d of LEM (3) aftertraining on the FitzHugh-Nagumo fast-slow system (12) using ∆t = 2.
Figure 5: Histogram of (∆tn)i and (∆tn)i for all ntraining on the GoogIe12 data set1,...,N and i = 1,...,d of LEM (3) afterHence, the multiscale resolution of LEM seems essential for the fast-slow dynamical system. Doesthis multiscale resolution also appear for other datasets and can it explain aspects of the observedempirical performance ? To this end, We consider the Google12 Keyword spotting data set and startby pointing out that given the spatial (with respect to input dimension d) and temporal (with respectto sequence length N) heterogeneities, a priori, it is unclear if the underlying data has a multiscalestructure. We plot the empirical histograms of ∆tn, ∆tn in Fig. 5 to observe that even for thisproblem, the terms ∆tn, ∆tn are expressed over a range of scales, amounting to 2 - 3 orders ofmagnitude. Thus, a range of scales are present in the trained LEM even for this example, but do theyaffect the empirical performance of LEM ? We investigate this question by performing an ablationstudy and reporting the results in Fig. 6. In this study, we clip the values of ∆tn, ∆tn to lie withinthe range [2-i, 1], for i = 0,1,..., 7 and plot the statistics of the observed test accuracy of LEM.
Figure 6: Average (and standard deviation of) test accuracies of 5 runs each for LEM on Google12,where ∆tn and ∆tn in (3) are clipped to the ranges [*,1] for i = 0,..., 7 during training.
Figure 7: Histogram of (∆tn)i and (∆tn)i for all n = 1,...,N and i = 1,...,d of LEM (3) aftertraining on the sMNIST data setB.3	On gradient-stable initialization.
