Figure 1: Diagram showing unsupervised Scarf pre-training (Top) and subsequent supervised fine-tuning (Bottom). During pre-training, networks f and g are learned to produce good representationsof the input data. After pre-training, g is discarded and a classification head h is applied on top of thelearned f and both f and h are subsequently fine-tuned for classification.
Figure 2: Top: Win matrices comparing pre-training methods against each other, and their im-provement to existing solutions. Bottom: Box plots showing the relative improvement of differentpre-training methods over baselines (y-axis is zoomed in). We see that S carf pre-training adds valueeven when used in conjunction with known techniques.
Figure 3: SCARF boosts baseline performance even when 30% of the training labels are corrupted.
Figure 4: S CARF shows even more significant gain in the semi-supervised setting where 25% ofthe data is labeled and the remaining 75% is not. Strikingly, pre-training with S CARF boosts theperformance of self-training and tri-training by several percent.
Figure 5: Win matrix for various batch sizes (Left) and corruption rates (Right) for the fully labeled,noiseless setting.
Figure 6: Left: Win matrix comparing different corruption strategies when z-score feature normaliza-tion is used in the fully labeled, noiseless setting. Right: The same matrix but when min-max featurescaling is used. We see that Scarf is better than alternative corruption strategies for different typesof feature scaling.
Figure 7: Training and validation loss curves for Scarf pre-training on Phonemes (dataset id 1489).
Figure 8: Left: Barlow Twins loss performs similar to InfoNCE while Uniform-Align performsworse. Right: InfoNCE softmax temperature 1 performs well.
Figure 9: Left: Pre-training with SCARF beats co-training for a range of weights on the co-trainingterm. Right: The same is true for additive noise autoencoders.
Figure 10: Left: Using SCARF only for data augmentation during supervised training performs worsethan using it for pre-training. Right: Using InfoNCE error instead of InfoNCE loss as the validationmetric for early stopping degrades performance.
Figure 11: Top left: Corrupting one view is better than corrupting both. Top right: Using differentrandom feature indices for each example in the mini-batch is better than using a same set across thebatch. Bottom left: Selecting a variable number of feature indices via coin flips performs similarto the method described in Algorithm 1. Bottom right: Corrupting by replacing the features by thefeatures of a single drawn example for the whole mini-batch performs worse.
Figure 12: Left: Scarf’s corruption strategy remains competitive for “mean” feature scaling, as wasthe case for both z-score and min-max scaling. Right: Comparison of different hyperparameters forthe uniform-align loss. a and u are the weights on the align and uniform terms respectively.
