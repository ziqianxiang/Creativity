Figure 1: Illustration of different types of temporal structure for two-mode exploration. Left: Eachline A-G depicts an excerpt of an experiment (black lines show episode boundaries, experimentcontinues on the right), with colour denoting the active mode (blue is exploit, magenta is explore).
Figure 2: Illustrating the space of design decisions for intra-episodic exploration (see also Figure 9).
Figure 3: Human-normalized performance results aggregated over 7 Atari games and 3 seeds, com-paring the four levels of exploration granularity. Left two: uniform explore mode XU. Right two:RND intrinsic reward explore mode XI . In each case, the baselines are pure modes X and G, step-level switching with ε-greedy, and episodic switching (with a bandit-adapted proportion). Note thatthe XI -step-level experiment uses both an intrinsic and an extrinsic reward, as in Burda et al. (2018).
Figure 4: Rows 1 and 3: Summary characteristics pX and rmedX of induced exploration behaviour,for different variants of intra-episodic exploration (and an episodic baseline for comparison), on asubset of 4 Atari games. Bandit adaptation can change these statistics over time, hence squareand cross markers show averages over first and last 10% of training, respectively. Rows 2 and 4:Corresponding final scores (averaged over final 10% of training). Error bars show the span betweenmin and max performance across 3 seeds. Note how different variants cover different parts ofcharacteristic space, and how the bandit adaptation shifts the statistics into different directions fordifferent games. See main text for further discussion and Appendix C for other games and variants.
Figure 6: Starting mode effect. Final mean episode return for two blind intra-episode experimentsthat differ only in start mode, greedy (blue) or explore (orange). Scores are normalised so that 1 is themaximum result across the two start modes. Either choice can reliably boost or harm performance,depending on the game. Left: uniform explore mode XU . Right: intrinsic reward explore mode XI .
Figure 5: Left and center: Illustration of detailed temporal structure within individual episodes,on Frostbite (top) and Gravitar (bottom), contrasting two trigger mechanisms. Each subplotshows 15 randomly selected episodes (one per row) that share the same overall exploration amountpX = 0.1. Each vertical bar (magenta) represents an exploration period of fixed length nX =10; each blue chunk represents an exploitation period. Left: blind, step-based trigger leads toequally spaced exploration periods. Center: a trigger signal informed by value promise leads to verydifferent within-episode patterns, with some parts being densely explored, and others remaining inexploit mode for very long. Right: the corresponding learning curves show a clear performancebenefit for the informed trigger variant (orange) in this particular setting. Appendix C has similarplots for many more variants and games.
Figure 7: Left and center: Contrasting the behavioural characteristics between two forms of blindswitching, step-based (left) and probabilistic (center), on the example of Frostbite. Each pointis an actor episode, with colour indicating time in training (blue for early, red for late). Note thehigher diversity of pX when switching probabilistically. Right: Corresponding performance curvesindicate that the probabilistic switching (red) has a performance benefit, possibly because it createsthe opportunity for ‘lucky’ episodes with much less randomness in a game where random actionscan easily kill the agent. For more games, please see the Appendix C.
Figure 8:	Preliminary results comparing different informed triggers: value-discrepancy, action-mismatch, and variance-based, when using XI exploration mode.
Figure 9:	Extra example illustrating the space of design decisions for intra-episodic exploration.
Figure 10: Extension of Figure 3, showing performance results as the mean episode return (witherror bars spanning the min and max performance across 3 seeds) for the 7 Atari games tried. Wecompare the four levels of exploration granularity as described in section 2.1 for XU mode (top tworows) and XI mode (bottom two rows). Intra-episodic switching (red curve) is superior or compara-ble to existing, monolithic or well-established approaches. Note that here we compare with the sameintra-episodic switching mechanism (i.e., with XU- or XI-intra(10,informed,p*,X)), butthere is at least one intra-episodic variant for each game which results in clear performance gains(just that it is not the same intra-episodic variant across games or as the one shown here).
Figure 11: Extension of figure 4 to all Atari games and intra-episodic switching variants tried.
Figure 12: Extension of Figure 5 to the 7 Atari games we experimented with. First two columns:temporal structures for a blind, step-based trigger; the 15 episodes we randomly selected correspondto 100 and 1000 fixed switching steps; the exploration period was fixed to 10 steps. Last twocolumns: temporal structures obtained with an equivalent informed trigger and corresponding totarget rates of 0.01 and 0.001, respectively.
Figure 13: Extension of Figure 7, showing behavioural characteristics (exploration proportion PX)between two forms of blind switching, step-based (left) and probabilistic (center), with their corre-sponding performances (right).
Figure 14: Extension of Figure 6, showing the performance differences between two blind intra-episode experiments, starting either in explore (X, rows 2 and 4) or in exploit mode (G, rows 1and 3). We show the bandit arm probabilities for each of the step sizes nX and how they changeover the course of learning for XU (top two rows) and for XI modes (bottom two rows). Findings:for symmetric blind triggers, starting with exploitation results in slower rates of switching (highnX = nG like red and green); in contrast, starting with exploration results in behaviours promotinghigher switching rates (small nX = nG like blue and orange). Note that these preferences are notmatching perfectly across all games, and thus results are domain-dependent.
Figure 15: Results of a probe experiment around off-policy correction in XI mode. Red is WatkinsQ(λ) with λ = 1 while blue is uncorrected k-step Q-learning, in each case with returns of length atmost k = 5. The performance is the same (or even slightly better) without off-policy correction,showing that it is not critical in our current setting.
Figure 16: Comparing 3 different X modes on the same 4 experimental settings and across 7 Atarigames: uniform exploration (XU, left), soft-epsilon-based exploration (XS, center), and intrinsicexploration (XI, right).
