Figure 1: Performance of our method under MA-MuJoco partially observable environments, the ab-lation study- IPSR, in which We do not considerthe agent interaction, and the baseline, which isnot using GAPSR. (a)-(c):GAPSR-2 (It uses cen-tralized critic, and uses gradients of value functionwith respect to policy parameter, its baseline isMADDPG), (d)-(f):GAPSR-1 (Its baseline is IAC).
Figure 2: Predicted Trajectories (colored) vs Actual Observations (black). (a)-(c) First iteration; (d)- (f) Iteration 40. The X axis represents the part of steps encountered for one trajectory under thatiteration, and the Y axis represents the numerical value of the observation, i.e. (a) has three rows torepresent its three coordinates of its observation. We also provide iteration 10, 20 in §D.1columns figure, with each row representing the observation feature and each column representingeach agent. As we can see from the figures, the first iteration of the learning does not predict theactual observation very well; it has some mismatches. However, as learning progresses, the predic-tions get increasingly more accurate. Note that the actual trajectory is changing according to thecurrent policy, and the current policy is optimized based on the further accurate learning of GAPSR.
Figure 3: Performance of our method (GAPSR-2)under multi-agent particle partially observable en-vironments. We use a different number of agents inpredator-prey and cooperative push environments.
Figure 4: GAPSR architecture combining GAPSR and MARL. The left part is the GAPSR model,which corresponds to the GAPSR Model Estimation Phase in Algorithm 1. The right part is theMARL, which corresponds to the generation of predictive representations and learning the multi-agent actor-critic. We use the actor-critic framework, which contains a centralized critic and manydecentralized actors. We could also give a parameter sharing actor network that maps individual PSRto parameters of a Gaussian distribution over the individual action space if agents are homogeneous.
Figure 5: The illustration of three environments swimmer, hopper, ant, and their correspondingMAMuJoCo version. (a) Single swimmer; (b) Single hopper; (c) Single ant; (d) n-agents swimmer;(e) 3-agents hopper; (f) 4-agents ant.
Figure 6: Additional Experiments Results, Predicted Trajectories vs Actual Observations for Multi-Agent Environments. (a) - (c) Iteration 10; (d) - (f) Iteration 20; This is the supplement for Figure 2Table 3: Model parameters for multi-agent particle environment	Environments						Predator-prey			Cooperative-push			n=3	n=15	n=100	n=3	n=15	n=30γ	0.99	0.99	0.99	0.99	0.99	0.99Soft target network	0.001	0.001	0.001	0.001	0.001	0.001α1	0.8	0.75	0.5	0.8	0.75	0.7α2	0.2	0.25	0.5	0.2	0.25	0.3β	0.45	0.5	0.45	0.55	0.5	0.5η learning rate of Adam	0.05	0.05	0.05	0.05	0.05	0.05λ ridge regression regularization	0.01	0.01	0.01	0.01	0.01	0.01Total iterations	30	30	30	30	30	30Number of trajectories	100	100	100	100	100	100Maximum number of steps per trajectory	500	500	500	500	500	500Length of test window	5	5	5	5	5	5Length of history window	5	5	5	5	5	5E Detailed ProofsWe give the proof of Theorem 1 in Appendix E.1, and we first introduce the following Lemmas to
