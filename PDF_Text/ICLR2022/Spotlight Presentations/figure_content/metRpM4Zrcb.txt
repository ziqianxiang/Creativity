Figure 1: Illustration of the proposed continual learning method with filter atom swapping. Within each CNNlayer, we decompose a filter Wi ∈ Rc×c0 ×k×k over a filter subspace spanned by m filter atoms Di ∈ Rm×k×kas Wi = αiDi, where αi ∈ Rc×c0 ×m are the subspace coefficients, c and c0 are the number of input andoutput channels, k is the spatial size of each atom. With task-shared coefficients, we learn for each task a newfilter subspace as filter atoms, and store those atoms, typically a few hundred of parameters, in a small footprintatom memory. At time T, we can recall the past model at t (t < T) through filter reconstruction Wit = αiDit,with Dit fetched from the atom memory, to fully recover the previous model.
Figure 3: Intra-task ensemble with Ew = 2.
Figure 2: Inter-task ensemble with Ec = 1.
Figure 4: Left & Middle: Ablation study on the number of atoms (m) and intra-task ensemble members (Ew)on 20-Split CIFAR100. Right: Parameter memory growth per-task for 20-Split CIFAR100. The proposedmethod shows significantly lower memory growth than other expansion methods.
Figure 5: (Plot) Ensemble effect of the base model with the most relevant and irrelevant past model. (Table)Ablation studies on number of ensemble selections n.
Figure A:	Correlation analysis between task similarities calculated with CCA (Raghu et al., 2017)and task similarities assessed with filter subspace distance.
Figure B:	Class incremental learning results on 10-Split CIFAR100.
