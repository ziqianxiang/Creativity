Figure 1: Our model and prompt format. T0 is an encoder-decoder model that consumes textualinputs and produces target responses. It is trained on a multitask mixture of NLP datasets parti-tioned into different tasks. Each dataset is associated with multiple prompt templates that are usedto format example instances to input and target pairs. Italics indicate the inserted fields from the rawexample data. After training on a diverse mixture of tasks (top), our model is evaluated on zero-shotgeneralization to tasks that are not seen during training (bottom).
Figure 2: T0 datasets and task taxonomy. (T0+ and T0++ are trained on additional datasets. SeeTable 5 for the full list.) Color represents the level of supervision. Yellow datasets are in the trainingmixture. Green datasets are held out and represent tasks that were not seen during training. HotpotQA is recast as closed-book QA due to long input length.
Figure 3: Prompt templates from the P3 prompt collection. Each dataset has multiple prompt tem-plates consisting of an input and a target template. These use the fields of the raw data examples aswell as template metadata, e.g., the left paraphrasing identification prompts use Choices, a template-level list variable ['Not duplicates', 'Duplicates']. These templates are materializedto produce the prompted instance shown in Figure 1. The complete set of prompt templates used inT0 is given in Appendix H.
Figure 4: Results for T0 task generalization experiments compared to GPT-3 (Brown et al., 2020).
Figure 5: Results for a subset of BIG-bench which has available baselines. The baseline modelsare Transformer-based language models provided by BIG-bench maintainers, who also provide oneprompt per dataset. T0, T0+ and T0++ are identical except for increasing the number of trainingdatasets (ยง5). BIG-bench Tasks are all zero-shot for all the reported models.
Figure 6: Effect of more prompts per dataset. Zero-shot performance of T0 and T5+LM when in-creasing number of training prompts per dataset. Each dot is the performance of one evaluationprompt. The main T0 model (p = 8.03) includes non-original-task prompts (see Section 3). Addingmore training prompts consistently leads to higher median performance and generally lower in-terquartile range for unseen tasks.
Figure 7: Effect of prompts from more datasets. Zero-shot performance of three models with varyingnumber of datasets (T0, T0+, T0++). Adding more datasets consistently leads to higher medianperformance but does not always reduce interquartile range for unseen tasks.
Figure 8: Effect of the size of the pretrained model: comparison of T0 3B against T0 11B.
