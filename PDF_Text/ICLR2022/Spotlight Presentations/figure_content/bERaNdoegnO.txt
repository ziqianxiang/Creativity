Figure 1: The number of considered actions and their visit counts N(a), when using SequentialHalving with Gumbel on a k-armed stochastic bandit. The search uses n = 200 simulations andstarts by sampling m = 16 actions without replacement. Sequential Halving divides the budget ofn simulations equally to log2 (m) phases. In each phase, all considered actions are visited equallyoften. After each phase, one half of the actions is rejected. From the original k actions, only the bestactions will remain.
Figure 2: Elo on 9x9 Go, when training with n ∈ {2, 4, 16, 32, 200} simulations. Evaluation uses800 simulations. Shades denote standard errors from 2 seeds.
Figure 3: Gumbel MuZero ablations on 9x9 Go. (a) Policy loss ablations, when training withn ∈ {2, 4, 16, 200} simulations. Gumbel MuZero uses the policy loss with completed Q-values.
Figure 4: Large-scale experiments with n = 400 simulations per move. (a) Elo on 19x19 Go, whentraining MuZero. (b) Elo on chess, when training AlphaZero.
Figure 5: Atari results. (a) Mean return on ms_pacman, when training GUmbel MUZero andMuZero with n ∈ {2, 4, 16, 18, 50} simulations. MuZero fails to learn from 4 or less simula-tions. (b) Mean return on beam_rider for Gumbel MuZero with Cscale ∈ {0.01,0.1,1,10,100},compared to MuZero with n = 50 simulations. Shades denote standard errors from 10 seeds.
Figure 6: Detailed policy loss ablations. Gumbel MuZero uses the policy loss with Q-values com-Millions of frames	Millions of frames	Millions of frames	Millions of framesFigure 7:	A comparison of different action selections at the non-root nodes. Gumbel MuZero usesthe unmodified (deterministic) MuZero action selection at non-root nodes. Full Gumbel MuZerouses the deterministic action selection from Equation 14, which we compare to stochastic samplingfrom π0 at non-root nodes.
Figure 7:	A comparison of different action selections at the non-root nodes. Gumbel MuZero usesthe unmodified (deterministic) MuZero action selection at non-root nodes. Full Gumbel MuZerouses the deterministic action selection from Equation 14, which we compare to stochastic samplingfrom π0 at non-root nodes.
Figure 8:	Additional Gumbel MuZero ablations on 9x9 Go. (a) Sensitivity to Q-value scaling bycvisit . (b) On the perfect-information game, Gumbel MuZero used zero Gumbel noise at evaluation.
Figure 9: Gumbel MuZero Elo on 9x9 Go, evaluated with different numbers of simulations. Theevaluation with n = 1 simulation acts with the most probable action from the policy network.
Figure 10: Scaling of value Mean Squared Error (MSE), policy accuracy and playing strength inElo vs inference speed for different network architectures. In the legend, btl indicates bottleneckresidual blocks; broad/8 and pool/8 indicate broadcast (Figure 11) and pooling blocks (Figure 12)in every 8th layer. 128p, 256p, etc. indicate the number of hidden planes.
Figure 11:	Bottleneck and broadcast blocks used by the board game network, implemented in JAX(Bradbury et al., 2018) with the neural network library Haiku.
Figure 12:	Pooling block, based on Wu (2019).
Figure 13:	Usage of modules defined in Figure 11 to create the network used for board game exper-iments. Alternatively, PoolResBlock can be used instead of BroadcastResBlock.
