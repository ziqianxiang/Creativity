Figure 1: The pessimistic exploration problem. Because skill discovery objectives do not distin-guish between aleatoric and epistemic uncertainty, they penalize exploration.
Figure 2: Methods. (a) The skill discovery process, where joint optimization of a skill-conditionedpolicy and skill discriminator ensure reliable and distinct behavior for each skill. (b) DISDAIN:disagreement between an ensemble of skill discriminators informs exploration.
Figure 3: Four Rooms results. (a) Skills learnt for top 10 of 20 seeds for each method. Mean ±std over seeds. (b) Performance on the downstream task of reaching a target state, averaged acrossall possible target states. Mean ± std over seeds. (c-d) Example states reached with and withoutDISDAIN. Plots depict counts of final states reached after one rollout per skill. Columns correspondto different points during training. With DISDAIN, agents learn to reach all states, while without,they barely make it out of the first room. (e) Per-state DISDAIN bonuses for the policy depicted in(d). In the beginning, all exploration is encouraged. In the middle, the checkerboard pattern emergesbecause agents try to space out their skills. By the end of training, DISDAIN gracefully fades away.
Figure 4: Number of skills learnt on Atari. Left: Effective skills (see equation 4) over training,measured by interquartile mean (IQM). Center: Distribution of skills learnt across seeds and games.
Figure 5: Qualitative analysis of skills learnt onAtari. Both plots depict improvement over Un-bonused skill learning aggregated across seeds andgames. Left: Probability of improvement on zero-shot reward evaluation. Right: Distribution of per-centile improvements on lifetime coverage. Errorbars depict 95% stratified boostrap CIs.
Figure 6: Skills learnt per-seed and per-game on all 57 Atari games.
Figure 7: Per-game boosts for each method. (b-d) Boosts in skills learnt on each game overUnbonused skill learning. Mean ± standard deviation over 3 seeds. All 3 plots use the same y-axisrange magnitude so that bar heights are comparable. DISDAIN improves skill learning on 52/57(91%) of games, with boosts of >5 skills on 18/57 (32%) and >10 on 6/57 (11%) of games. RND andthe Ensemble-only ablation perform closer to chance with improvements on 28/57 (49%) and 35/57(61%), with boosts of >5 skills on 2/57 (4%) and 9/57 (16%) and >10 skills on 1/57 (2%) and 0/57(0%) of games, respectively. a) Skills learnt on each game for each method for reference. Sorted bymagnitude of DISDAIN boost, as in (b).
Figure 8: DISDAIN robustness to key hyperparameters. (a) Sweeping bonus weight (λ) withfixed ensemble size N = 160 (see Algorithm 1). (b) Sweeping ensemble size (N) with fixed bonusweight λ = 180. Curves averaged over 57 games and 3 seeds (for results broken out by game andseed, see figures 9 and 10). For both hyperparameters, DISDAIN’s improvements over baselines arerobust over more than an order of magnitude of variation. All other experiments use λ = 180 andN = 40 unless otherwise stated.
Figure 9: Per-game bonus weight sweep for DISDAIN across all 57 Atari games.
Figure 10: Per-game ensemble size sweep for DISDAIN across all 57 Atari games.
Figure 11: RND bonus weight sweep. RND fails to significantly improve skill learning as theweighting on its contribution to the reward is increased. As soon as the RND bonus becomes ofa similar order of magnitude as the skill learning reward (bonus weight λ ≈ 1), skill learningperformance begins to decay, suggesting that the kind of exploration encouraged by RND is notconducive to skill learning. Results averaged over 57 games and 3 seeds (see figure 12 for resultsbroken out by game and seed).
Figure 12: Per-game bonus weight sweep for RND across all 57 Atari games.
Figure 13: Per-game task reward attainment curves throughout training. We emphasize thatagents are trained only to maximize the skill learning objective and any associated exploration bonus(i.e. DISDAIN or RND), and not the task reward. Thus, these plots depict zero-shot reward attainmentwhile uniformly randomly switching between skills.
Figure 14: Lifetime coverage for all games, methods, and seeds. All policies quickly achieve thesame score on ms_pacman and seaquest, so those levels are removed from the analysis in themain text.
Figure 15: Episodic coverage boosts over unbonused skill learning. DISDAIN provides significantboosts over unbonused skill learning on hero and montezuma_revenge, while all methodsperform similarly on the other four levels analyzed. Since state coverage metrics are particularly wellsuited to montezuma_revenge, the boost there is especially interesting. Results averaged over 3seeds. For results over training for each seed, see figure 16.
Figure 16: Episodic coverage for all games, methods, and seeds.
Figure 17:	rskill vs rDISDAIN during learning for all seeds per-game on all 57 Atari games. Eachpanel includes data from 3 seeds. DISDAIN reward tends to dominate early but fades away as skilllearning converges.
Figure 18:	Example DISDAIN skill rollouts on Four Rooms. Each panel shows rollout for oneskill. Panels are labeled with skill index and the probability the discriminator assigns to the correctskill label. Color indicates time within episode, moving from green (beginning) to yellow (end).
Figure 19: Example Unbonused skill rollouts on Four Rooms. Each panel shows rollout for oneskill. Panels are labeled with skill index and the probability the discriminator assigns to the correctskill label. Color indicates time within episode, moving from green (beginning) to yellow (end).
Figure 20: Four Rooms training curves broken out by seed. The exploration bonuses dramaticallyimprove skill learning in most cases, though they also slow learning and add variance due to thetraining of an additional separate value function (and for DISDAIN, an ensemble of discriminators).
