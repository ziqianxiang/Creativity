Figure 1: In critically-damped Langevin diffusion, the data xt is augmented with a velocity vt . A diffusioncoupling xt and vt is run in the joint data-velocity space (probabilities in red). Noise is injected only into vt .
Figure 2: Top: Difference ξ(t) (via L2norm) between score of diffused data andscore of Normal distribution. Bottom:Frobenius norm of Jacobian JF (t) of theneural network defining the score functionfor different t. The underlying data distri-bution is a mixture of Normals. Insets: Dif-ferent axes (see App. E.1 for detailed defi-nitions of ξ(t) and JF (t)).
Figure 3: CIFAR-10 samples.
Figure 4: CelebA-HQ-256 samples.
Figure 5: Langevin dynamics in different damping regimes. Each pair of visualizations corresponds to the(coupled) evolution of data xt and velocities vt . We show the marginal (red) probabilities and the projectionsof the (green) trajectories. The probabilities always correspond to the same optimal setting Γ = Γcritical (recallthat Γcritical = 2λ∕M and Γmax = M/(β(t)δt); see Sec. A.2). The trajectories correspond to different Langevintrajectories run in the different regimes with indicated friction coefficients Γ. We see in (b), that for criticaldamping the xt trajectories quickly explore the space and converge according to the distribution indicatedby the underlying probability. In the under-damped regime (a), even though the trajectories mix quickly weobserve undesired oscillatory behavior. For over-damped Langevin dynamics, (c) and (d), the xt trajectoriesmix and converge only very slowly. Note that the visualized diffusion uses different hyperparameters comparedto the diffusion shown in Fig. 1 in the main text: Here, we have chosen a much larger β, such that also the slowoverdamped Langevin dynamics trajectories shown here mix a little bit over the visualized diffusion time (whilethe probability distribution and the trajectories for critical damping converge almost instantly).
Figure 6: Comparison of 'HSM (in green) and 'DSM (in orange) for our main hyperparameter settingwith M = 0.25 and Y = 0.04. In contrast to 'DSM, 'HSM is analytically bounded. Nevertheless,numerical computation can be unstable (even when using double precision) in which case addinga numerical stabilization of num = 10-9 to the covariance matrix before computing `t suffices tomake HSM work (see App. B.4).
Figure 7: Traces of the estimated covariance matrices.
Figure 8: Conceptual visualization of our new SSCS sampler and comparison to Euler-Maruyama(for image synthesis): (a) In EM-based sampling, in each integration step the entire SDE is inte-grated using an Euler-based approximation. This can be formally expressed as solving the full-step{δt(L⅛ + Cʌpropagator expS) via Euler-based approximation for N small steps of size δt (Seered steps; for simplicity, this visualization assumes constant δt). (b): In contrast, in our SSCS thepropagator is partitioned into an analytically tractable component exp{ δ2t L%} (blue) and the scoremodel term exp{δtLS} (brown). Only the latter requires numerical approximation, which resultsin an overall more accurate integration scheme.
Figure 9: Toy experiments for mixture of Normals dataset.
Figure 10: Mixture of Normals: data and trained models (samples).
Figure 11: Mixture of Normals: numerical simulation with analytical score function for differentdiffusions (VPSDE with EM vs. CLD with EM/SSCS) and number of synthesis steps n. A visual-ization of the data distribution can be found in Fig. 10a.
Figure 12: Frobenius norm JF (t) of the neural network defining the score function for different t.
Figure 13: Data distribution and model samples for multi-scale toy experiment.
Figure 14: Additional samples using EM-QS with 2000 function evaluations. This setup gave us ourbest FID score of 2.23.
Figure 15: Additional samples using SSCS-QS. This setup resulted in an FID score of 3.07 usingonly 150 function evaluations.
Figure 16: Samples generated by our model on the CelebA-HQ-256 dataset using our SSCS solver.
Figure 17: Samples generated by our model on the CelebA-HQ-256 dataset using a Runge-Kutta4(5) adaptive ODE solver to solve the probability flow ODE. We show the effect of the ODE solvererror tolerance on the quality of samples ((a), (b), (c) and (d) were generated using the same priorsamples). Little visual differences can be seen between 10-5 and 10-4. Low frequency artifacts canbe observed at 10-3. Deterioration starts to set in at 10-2.
Figure 18: Generation paths of samples from our CelebA-HQ-256 model (RUnge-KUtta 4(5) solver;mean NFE: 288). Odd and even rows visualize data and velocity variables, respectively. The eightcolumns correspond to times t ∈ {1.0, 0.5, 0.3, 0.2, 0.1, 10-2, 10-3, 10-5} (from left to right). Thevelocity distribution converges to a Normal (different variances) for both t → 0 and t → 1. SeeApp. F.3 for visualization details and discussion.
Figure 19: Generation paths of samples from our CelebA-HQ-256 model (SSCS-QS using only 150steps). Odd and even rows visualize data and velocity variables, respectively. The eight columnscorrespond to times t ∈ {1.0, 0.5, 0.3, 0.2, 0.1, 10-2, 10-3, 10-5} (from left to right). The velocitydistribution converges to a Normal (different variances) for both t → 0 and t → 1. See App. F.3 forvisualization details and discussion.
