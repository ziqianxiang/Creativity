Figure 1: A Context-free DSL Grammarfor programmatic policies.
Figure 3: An Ant Cross Maze program PCrOSS With three branches. A program input X includescurrent Ant position x, y along with the target location Gx, Gy (sampled from one of the threegoals in Fig. 2). arctany and ∣∣x, y ∣∣2 are functions of X and y. Each branch composes primitivefunctions: πup, πDOWN, ∏LEFT, and πRIGHT. Composition weights are shown in percentage.
Figure 2: Ant Cross MazeFig. 3 depicts a synthesized program PCrOSS with three branches forsolving the Ant Cross Maze environment. As specified in Equation 1, our semantics of a branchingconstruct is approximated by the sigmoid function σ. The value of the predicate in a Boolean conditiondetermines the activation of the controller guarded by the Boolean condition. At each state, branchactivation determines the strength of each of the controllers in the program. For example, the activationof branch 1 is σ(θ1C+θ1T ∙X), and the activation of branch 2 is (1 -σ(θ1C+θ1T ∙X)) ∙σ(θ2C+θ2T ∙X).
Figure 4: Branch activation as functions of Ant position (x, y) for program Pcross.
Figure 5: Ant Cross Maze Program Derivation Tree with program input X . ~π refers to the primitivesof Ant moving up πUP, down πDOWN, left πLEFT and right πRIGHT that take the Ant’s own observations.
Figure 6: Model-based high-level planning for Ant ⊃-Maze.
Figure 7: Comparison results against baselines. The y-axis records the agent’s distance towards itsgoal normalized by the agent initial distance from the goal so values close to 1 or higher show failures.
Figure 8: Ablation study on the importance of compositionality. Results are averaged over 10 randomseeds. The y-axis records the agent’s distance towards its goal. We normalize the distances to a finalgoal by the agent initial distance from the goal so values close to 1 or higher show failures.
Figure 9: Ablation study on the impact of the depth bound of a program derivation tree. Resultsare averaged over 10 random seeds. The y-axis records the agent’s distance towards its goal. Wenormalize the distances to a final goal by the agent initial distance from the goal so values close to 1or higher show failures.
Figure 10: Navigation and manip-ulation benchmarks that requireagents to reach or move the objectto the given targets (representedby a red circle for pusher and bygreen spheres for the rest). Addi-tionally, Ant Cross Maze and Ant⊃-Maze are depicted in Fig. 2and Fig. 6a respectively. In allant environments, the definitionsof up, down, left, right, and for-ward are moving towards posi-tive/negative x/y axes directions,respectively.
Figure 11: Branch activation as functions of Ant position (x, y) for program Pcross.
Figure 12: UP policy strength of a Ant Cross Maze program PcrossFigure 13: LEFT and RIGHT policy strength of a Ant Cross Maze program Pcross .
Figure 13: LEFT and RIGHT policy strength of a Ant Cross Maze program Pcross .
Figure 14: An Ant Random Goal program Prandom powered by four primitive skill policies:nUP(s), ∏DOWN(s), ∏LEFT(S) and ∏riGHT(s). Program input X includes current Ant positionx, y, target goal Gx, Gy, arctanX, and ∣∣x, y∣∣2.
Figure 15: Branch activation of an Ant Random Goal program Prandom , (Gx , Gy) = (-3, -3).
Figure 16: DOWN and RIGHT policy strength of program Prandom , (Gx , Gy) = (-3, -3).
Figure 17: A Pusher program PPUSher powered by two primitive skill policies: πpUSH-DoWN(S)and ∏push-left(s). Program input X includes norms over Xobj, y°bj and Xarm, yarm.
Figure 18: Branch activation and policy strength graph of a pusher program Ppusher .
Figure 19: A HalfCheetah Hurdle program PhC powered by two primitive skill policies:∏FORWARD(S) and ∏JUMP(s). Program input X includes the position of the next hurdle Hnextand the distance from HalfCheetah's back foot to next hurdle ∣xback - Hnext |. The programPhC shown is a special program which does not contain any if-then-else structure, i.e., thereexists a fixed strength of invoking each primitive policy.
Figure 20: UP, RIGHT and LEFT policy strength of a transferred program PCross .
Figure 21: Ant Reshaped MazeWe directly apply Pcross to Ant Reshaped Maze and com-pare the generalizability of Pcross with the BiLSTM policylearned in the Ant Cross Maze environment as well usingComposition-SAC (Qureshi et al., 2020). We compare with Composition-SAC because the twopolicies both learn to compose primtivie skills to adapt to new complex behavior. However, Pcross ismore interpretable than a BiLSTM model. Table 8 shows that Pcross successfully realizes the changeof the three goals and drive the Ant in the novel environment with 0.16 normalized distance to thenew goals. In constrast, the Composition-SAC policy does not solve the novel environment well. Wefurther increase the original radius of goals to 1.5 and 2 in the novel environment, both Pcross andthe Composition-SAC policy get close to goals with Pcross continuing to perform much better.
Figure 22: Continuous MountainCar Example. In the branch activation graph, a yellow regionindicates the area of states on which the control signal in branch 1 is activated.
Figure 23: A MoUntainCar program with two branches. Program input X includes current carposition and velocity.
Figure 24: Pendulum Example. In the branch activation graph, a yellow region indicates the area ofstates on which the control signal in branch 1 is activated.
Figure 25: A pendulum program With two branches. In the system, ω is the angle the pendulummakes with the vertical. Program input X includes cos(ω), sin(ω), ω.
