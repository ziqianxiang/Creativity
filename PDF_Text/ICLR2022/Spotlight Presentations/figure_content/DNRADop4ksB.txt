Figure 1: The MLE bias and Firth bias reduction visualized in a geometric experiment. Here,the task is to estimate the coin flip probability based upon the number of tosses until the first headshows up. (Left) The average MLE (shown in blue) and the Firth bias-reduced estimator (shownin red) against increasing number of samples. The true generative parameter Î²* = 0.5 is annotatedwith a star, and the MLE bias is also visualized. The Firth bias-reduced estimator removes theleading O(N -1) term from the MLE bias even with small N. (Right) To show that the MLE bias isasymptotically of O(N-1), MLE bias is plotted against the sample size N in the log-log scale.
Figure 2: Firth bias reduction produces statistically significant improvements over a wide rangeof backbone architectures and number of shots, whereas the common L2-regularization cannot.
Figure 3: Penalizing KL to the imbalanced class distribution is worse than using the Firthbias reduction. The accuracy improvement is compared (1) when DKL(UkPi) is penalized (i.e.,the blue bars with the uniform prior label) vs. (2) when the KL divergence to the imbalanced classdistribution A (i.e., DKL(AkPi)) is penalized (i.e., the red bars with the non-uniform prior label).
Figure 4: Firth bias reduction consistently improves the accuracy of the cosine classifier (abetter few-shot classifier) in the presence of strong backbone features, regardless of (1) thetype of dataset, and (2) the number of classes in few-shot classification. For each dataset, anadvanced backbone architecture (WideResNet-28-10) with strong mixup regularization and self-supervision was trained. The error bars are almost non-existent (i.e., less than 0.02%), since over10,000 trials with matching randomization factors were performed for each point. Firth biasreduction improvements are always positive, and it never hurts to use the Firth bias reductiontechnique. Absolute and delta accuracies are provided in Tables A6, A7, and A8 in the Appendix.
Figure A5: Firth bias reduction is effectively the same as minimizing the KL divergence to theuniform class probabilities for logistic regression. Here, Pi denotes the predicted distribution ofclasses for the sample xi , and U denotes the uniform distribution of classes. The logistic objectiveminimizes the KL-divergence between the true label yi and Pi, while Firth bias reduction LFirth triesto tie Pi with a KL divergence rope to the uniform distribution over the classes.
Figure A6: Improvements of Firth-penalized logistic classifier over the baseline on mini-ImageNet with 5 and 10 number of classes. Except for the number of classes, the experiments wereperformed in the same setting as in Figure 2.
Figure A7: Same experiment as Figure 2 in the main paper but with a 3-layer logistic classi-fier instead of a 1-layer logistic classifier (in the main paper). Again, Firth bias reduction improvesthe accuracy for different backbones and number of samples (left), whereas L2-regularization is noteffective at all (right). In the very hard cases, i.e., 1- and 5-shots, there is a mild improvement byFirth and no deterioration. The error bars, representing the 95% confidence intervals, are covered bythe blobs.
Figure A8: Relative accuracy improvements corresponding to Figure A7 with the 3-layer logisticclassifier. The behavior of the relative accuracy improvements is similar to the absolute accuracyimprovements for all the backbones in both techniques.
Figure A9: Absolute test accuracy values of 16-way classification for the non-penalized 3-layerand single-layer logistic classifiers on the novel set of mini-ImageNet for different backbonesand varying number of samples.
