Figure 1: (a) shows the weight magnitude distributions of DNNs trained by SGD and SFW respec-tively, averaged over 2 architectures {ResNet18, VGG16} and 2 datasets {CIFAR-10, CIFAR-100}.
Figure 2: Test performance of unstructured pruning networks with different sparsity ratios withoutretraining. We compare three settings: SFW-pruning, SGD training with one-shot weight-magnitudepruning, SGD-training with one-shot random pruning. All initial learning rate is fixed to α0 = 1.0.
Figure 3: Our one-shot SFW-pruning (no retrain-ing) can even achieve competitive performanceagainst “SGD-pruning with extra retraining costs”(ResNet-18, CIFAR-10).
Figure 4: We study SFW-pruning with and without SFWInit. Test accuracies of the pruned DNNswith different sparsity ratios are obtained without retraining. All have initial learning rate α0 = 1.0.
Figure 5: Comparison with SOTA unstructuredpruning methods on VGG-16 and CIFAR-10.
Figure 6: Ablation study of the influence of τ and K in K -sparse polytope constraint on the pruningperformance (ResNet18 on CIFAR-10). We consider τ ∈ {5, 10, 15} and Kl ∈ {5%, 10%}.
Figure 7:	Ablation study of the influence of τ and K in K -sparse polytope constraint on the weightdistributions (ResNet18 on CIFAR-10). We consider τ ∈ {5, 10, 15} and Kl ∈ {5%, 10%}.
Figure 8:	Comparison of pruning performance and weight distributions induced by SFW-pruningfor VGG16 on CIFAR-10 with different initial learning rates α0 ∈ {0.1, 0.2, 0.4, 0.6, 0.8, 1.0}B.3 Performance on More Architectures and DatasetsFinally, we show the weight distributions induced by SFW-pruning across four different architectureand dataset combinations, and we compare them to the weight distributions induced by SGD trainingas well. Specifically, we consider {ResNet18, VGG16} and {CIFAR-10, CIFAR-100} respectively.
Figure 9:	Comparison of pruning performance and weight distributions induced by SFW-pruningfor ResNet18 on TinyImageNet. The results show that even on complex dataset like TinyImageNet,appropriately more smaller weights can still benefit the pruning procedure.
Figure 10: Comparison of weight distributions induced by SFW and SGD over different architec-tures and datasets. For SFW-pruning, we choose τ = 15 and Kl = 5%, with α0 = 1.0.
