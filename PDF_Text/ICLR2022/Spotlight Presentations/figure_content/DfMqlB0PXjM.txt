Figure 1: Qualitative pixel-noise removal with HIERARCHICAL DIVNOISING (HDN). For a subset of thedatasets we tested (rows), we show denoising results for one representative input image. Columns show, from leftto right, the input image and an interesting crop region, results of CARE, two randomly chosen HDN samples,a difference map between those samples, the per-pixel variance of 100 diverse samples, the MMSE estimatederived by averaging these 100 samples, and the ground truth image.
Figure 2: Accuracy of the HDN generative model. We show real images (bottom row), unconditionallygenerated images from HIERARCHICAL DIVNOISING (middle row) and those from DIVNOISING (top row) atdifferent resolutions for the DenoiSeg Flywing, FU-PN2V Convallaria and Kanji datasets. The generated imagesfrom our method look much more realistic compared to those generated by DivNoising, thereby validating thefact that our method is much more expressive and powerful compared to DivNoising.
Figure 3: Visualizing what HIERARCHICAL DIVNOISING encodes at different latent layers. Here weinspect the contribution of each latent layer in our n “ 6 latent layer hierarchy when trained with data containingstructured noise. To inspect contribution of latent variable Zi, We fix the latent variables of layers j > i, drawk “ 6 conditional samples for layer i, and take the mean of the conditional distributions at layers m V igenerating then a total of k denoised samples (columns). This is repeated for every latent variable layer (rows).
Figure 5: HIERARCHICAL DIVNOISING (HDN) network architecture. The network architecture used forMNIST dataset is shown. For all other datasets, 6 stochastic latent groups are used instead of the 3 groups shownhere.
Figure 6: Illustration of structured noise/artefacts in microscopy images. To better illustrate the structurednoise in the considered datasets, we take the noise (first row) and compute their spatial autocorrelation (secondrow). For structured Convallaria dataset, the noise is obtained by subtracting the ground truth image with noisyraw image. For the three photon mouse and spinning disk MSC datasets, ground truth images are not availableand hence, the noise is estimated at background regions (empty areas) of the images, which supposedly onlycontain noise. For reference, we also show the spatial autocorrelation of Gaussian noise (pixel independentnoise) in first column. For each case, we show the autocorrelation of noise for one pixel in the center (shownin red). For all three real microscopy datasets, there is a spatial correlation between noise unlike the Gaussiannoise case where the noise is independent per pixel. Also note that the spatial extent of the structured noise isdifferent for different datasets. Still, our proposed HDN3_6 makes no assumption on the prior knowledge ofspatial extent of these artefacts and successfully removes them for all considered datasets whereas other methodssuch as Noise2Void only remove pixel noise and recover structured noise (see Fig. 4 in main text).
Figure 7: Accuracy of the learned posterior. We show real images (top row), generated images fromHIERARCHICAL DIVNOISING (middle row) and generated images from DIVNOISING (bottom row) at differentresolutions for the DenoiSeg Flywing dataset. The images from our method look much more realistic comparedto those generated by DivNoising, thereby validating the fact that our method learns a much better posteriordistribution of clean images compared to DivNoising.
Figure 8: Accuracy of the learned posterior. We show real images (top row), generated images fromHIERARCHICAL DIVNOISING (middle row) and generated images from DIVNOISING (bottom row) at differentresolutions for the FU-PN2V Convallaria dataset. The images from our method look much more realisticcompared to those generated by DivNoising, thereby validating the fact that our method learns a much betterposterior distribution of clean images compared to DivNoising.
Figure 9: Accuracy of the learned posterior. We show real images (top row), generated images fromHIERARCHICAL DIVNOISING (middle row) and generated images from DIVNOISING (bottom row) for theKanji dataset in first half and the MNIST dataset in second half. The images from our method look much morerealistic compared to those generated by DivNoising, thereby validating the fact that our method learns a muchbetter posterior distribution of clean images compared to DivNoising.
Figure 10: Accuracy of the learned posterior. We show generated images from HIERARCHICALDIVNOISING (top row) and generated images from DIVNOISING (bottom row) at different resolutions fornatural images. The images from our method look much more realistic compared to those generated byDivNoising, although there seems to be plenty of room for improvement. The generated images by HDNvaguely resemble structures such as rock, clouds, etc. More meaningful structures like people, animals, etc. arenot generated at all. This indicates that our method is not powerful enough to capture the tremendous diversityin natural image datasets but still gives SOTA denoising performance on natural image benchmarks (see Table 1in main text and Appendix Table 5).
Figure 11: Qualitative results of structured noise experiments with HDN. As explained in the main text,HDN owing to its high expressivity captures striping artefacts and fails to remove them, thereby leading toinferior restoration performance (See Table 3). Hence, We propose the modified HDN3—6 for microscopystructured noise removal (see Fig. 4 and Table 3 in main text) which establishes a new state-of-the-art.
Figure 12: Comparison of HDN MMSE and MAP estimates for two representative MNIST inputs. Weshow noisy input images (first column), HIERARCHICAL DIVNOISING MMSE estimates computed from 100samples (second column), HIERARCHICAL DIVNOISING MAP estimates (third column) computed using amean-shift clustering approach with the same 100 samples and ground truth images (fourth column). Note thatthe MMSE estimate is a compromise between all possible denoised solutions and hence the obtained MMSEresults have faint overlays suggesting a compromise between the digits 9 and 4 (first row) and the digits 3 and 8(second row) while the MAP estimate is committing to one interpretation.
Figure 13: Comparison of HDN MMSE and MAP estimates for eBook dataset. We show noisy inputimage (first column), DIVNOISING MMSE estimate taken from Prakash et al. (2021) (second column), ourHIERARCHICAL DIVNOISING MMSE estimates computed from 100 samples (third column), DIVNOISINGMAP estimate taken from Prakash et al. (2021) (fourth column), our HIERARCHICAL DIVNOISING estimate(fifth column) and ground truth image (sixth column). Note that the MMSE estimate is a compromise between allpossible denoised solutions and hence the obtained MMSE results have faint overlays suggesting a compromisebetween different possible letters while the MAP estimate commits to one interpretation of letters.
Figure 14: Application of diverse samples for instance segmentation. We use the diverse denoised samplesfrom HDN to obtain diverse segmentation results using the procedure presented in Prakash et al. (2021). Wethen use the diverse segmentation results as input to a consensus algorithm which leverages the diversitypresent in the samples to obtain a higher quality segmentation estimate than any of the individual diversesegmentation maps. The segmentation performance is compared against available ground truth in termsof Average Precision (Lin et al., 2014) and SEG score (Ulman et al., 2017). Higher scores represent bettersegmentation. The segmentation performance corresponding to our HDN MMSE estimate obtained by averagingdifferent number of denoised samples improves with increasing number of samples used for obtaining thedenoised MMSE estimate. Clearly, the segmentation results obtained with consensus method using our HDNsamples outperforms even segmentation on high SNR images using as little as 3 diverse segmentation samples.
Figure 15: Measuring sample diversity with increasing noise levels on imput images. We syntheticallycorrupt DenoiSeg Flywing dataset with different levels of Gaussian noise (σ “ 30, 50, 70) and train differentHDN networks for these noise levels. For each of these networks, we sampled 100 HDN denoised solutionscorresponding to each noisy image in the test set. For the 100 samples, we computed the standard deviation perpixel and report the average standard deviation over all test images. The higher the noise level, the greater thestandard deviation per pixel is. This is expected because more noisy data is more ambiguous and hence HDNaccounts for this uncertainty in the increasingly noisy input images by generating more diverse samples.
