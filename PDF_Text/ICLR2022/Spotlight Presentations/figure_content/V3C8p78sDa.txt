Figure 1: The performance of different downstream (DS) tasks vs of upstream (US) based on more than 1500different Vision Transformers, 1400 MLP-Mixers and 16 best performing ResNets (Although the number ofResNet samples are small, this does not hurt our investigations. See Appendix G.1 for details), with differentconfigurations. The models are pre-trained on JFT and evaluated in few-shot settings (25 shots). Figure F.1in Appendix F shows the same plot but with more than 4800 experiments including two different upstreamtasks of JFT, ImageNet21K and 1, 25 shots. We consider the convex hull of the points as well since it capturesthe performance of a randomized classifier made by choosing these models with different probabilities. Asthe upstream performance improves, the downstream performance starts to saturate. Even if US accuracyreaches 100% accuracy, the DS accuracy will not reach the 100% accuracy and saturates at a lower value. Weobserve a non-linear relationship between upstream and downstream accuracy and model the relationship witha power law function to predict the DS performance given the US performance. The horizontal line is thepredicted downstream accuracy if upstream accuracy reaches 100%. We investigate DS-vs-US plots instead ofthe usual DS-vs-scale plots to capture the effect of hyper-parameter choices and to account for the fact that thescaling impacts DS performance through US performance. Figure F.2 depicts the same plot with log scaling ofaccuracies as done in many related works. Figure F.3 depicts the same plot when upstream is ImageNet21K.
Figure 3: The effect of sample size on power lawcurves. The curves are fitted to the convex hull of ex-periments as well as all data points from Figure F.1.
Figure 2: Effect of number of shots and DS/UStask on eIR of the power law curves. We note thatall of them impact eIR. To see this effect for allpower law parameters see Figures F.4, F.5.
Figure 4: Controlled scale up experiments of the model size (number of parameters), data size (portion of thepre-trained data), and compute (number of epochs) on different downstream tasks and JFT as upstream. Weobserve similar trends to Figure 1. (1) As we increase US accuracy, DS performance saturates. (2) Increasingmodel size, US data size, and compute all lead to the same curve. (3) The variation from the curve is due totraining hyper-parameters. (4) US accuracy has a stronger predictive power for DS accuracy compared to modelsize, US data size, and compute.
Figure 5: The overlay of the convex hull of ImageNet (Red) DS-vs-US plot on DS-vs-US plots of all DS tasksfrom Figure 1(Blue). US task is JFT. We observe that best performing ImageNet models perform very similar tobest performing models in several DS tasks but not all of them. Moreover, as the US performance increases, thegap between best performing ImageNet models and best performing DS task models reduces significantly.
Figure 6: The effect of choosing representations from different layers on the downstream tasks performanceoverlay-ed with the effect of scaling (model, data, and compute) on downstream performance when upstreamtask is JFT. The red triangles are performance on downstream task when representation used in the few-shotlearning is from different layers of the model. The green circles overlay the DS versus US performance ofdifferent experiments from Figure 4 on each task. Red triangles use the x-axis on the bottom and the greencircles use the x-axis on the top. We note that for those DS tasks that are similar to US, such as ImageNet, thehigher the layer the better performance on DS. On the contrary, for those DS tasks that saturate fast, such asUC-Merced and col_hist, the optimal layer is not the last one.
