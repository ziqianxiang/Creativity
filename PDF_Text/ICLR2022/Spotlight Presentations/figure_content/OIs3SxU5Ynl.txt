Figure 1: Diagram of the VAE trade-off. The optimal solution Θvae is “in between” the maximumlikelihood solution Θml and the best solution in the class Θφ of consistent VAEs, where the posteriorapproximation error function F(θ) vanishes. We give an explicit characterization of this consistentset. At Θvae there is a balance between the gradient of -F (blue arrow) and the gradient of the datalog-likelihood (black arrow).
Figure 2: Artificial example. (a) Color-coded posterior distribution p* (z | x). (b-e): Decision maps(arg maXz) of: (b) true posterior p*(z | x), (c) factorized encoder q°(Z | x) after joint learning, (d)model posterior pθ (z | x) after joint learning, (e) RBM trained on the same data. Gaussian centersμ(z) are shown as black dots. (f) Probability simplex of distributions over the four binary config-urations. The vertices correspond to pure (deterministic) binary states represented by the code andits respective color. The surface shows the manifold of factorized distributions realizable by q(z | x).
Figure 3: Results for the encoder learned by su-pervised conditional likelihood. Top row: train-ing samples X 〜p*(x). Second row: the Cor-responding reconstructions from mean values ofz, i.e. x 〜p*(x∣ μe (X)). Third row: reconstruc-tions from sampled z, i.e. Z 〜N(μe(X),σ2(X))followed by X 〜p*(x| Z).
Figure 4: Visual comparison - images drawnfrom the original/learned models. Each col-umn corresponds to a particular value of Z 〜N(0, I). Top row: the ground truth model, mid-dle row: learned decoder with fixed σd , bottomrow: decoder with learned σd .
