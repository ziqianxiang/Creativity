Figure 1: Overview of simulated environments & real robot trajectories. Top row shows the simulatedenvironments. For each sample, the left image is the environment and the right image is the correspondingobservation. Agents are tasked to move forward while avoiding black obstacles and collecting red spheres.
Figure 2: Network Architecture. We process proprioceptivestates with a MLP and depth images with a ConvNet. We takeproprioceptive embedding as a single token, split the spatial visualfeature representation into N × N tokens and feed all tokens intothe Transformer encoder. The output tokens are further processedby the projection head to predict value or action distribution.
Figure 3: Self-attention from our shared Transformer module. We visualize the self-attentionbetween the proprioceptive token and all visual tokens in the last layer of our Transformer model. Weplot the attention weight over raw visual input where warmer color represents larger attention weight.
Figure 4: Training and evaluation curves on simulated environments (Concrete lines and shaded areas showsthe mean and the std over 5 seeds, respectively). For environment without sphere (in (a)), our method achievecomparable training performance but much better evaluation performance on unseen environments (in (b)). Formore challenging environment (in (c) and (d)) our method achieve better performance and sample efficiency.
Figure 5: Real World Samples We evaluate ourmethod in real-world scenarios with different ob-stacles on complex terrain.
Figure 6: Experiment results in the real-world:We perform real-world experiment on Indoor &Obs. and Forest environments.
Figure 7: Additional Attention Visualization We visualize more attention map visualization for betterunderstanding of how our LocoTransformer works. Each row shows a sequence of attention map to present howthe attention of agent evolves.
Figure 8: Training curves of vision-guided whole body controller. For multi-modal input setting, Loco-Transformer still outperforms the baseline.
Figure 9: Some failure cases of vision-guided whole-body controller due to limitation of agility. The leftfigure shows that when walking through a narrow path, the robot can not quickly turn around and collide into thewall. The right figure denotes that it may collide to the tree in the wild, due to the shape of the obstacles are outof the training distribution and the robot can’t adjust quickly enough to avoid.
Figure 10: Experiment results in the real-world: We perform real-world experiment on Indoor & Obs. andForest environments to compare our method and the vision-guided whole-body controller(a) Indoor & Obs.
