Figure 1: Overview: Our model generates multi-ple possible futures for agents based on several inputtimesteps. The input trajectories are encoded into acontext tensor that captures their respective behav-ior and interaction with surrounding agents. WhileTransformer-based architectures are often autoregres-sive, we decode all future steps at once. This is ac-complished via learnable seed parameters. Our de-coder transforms these, together with the map and thecontext into the possible future trajectories that wecall “modes”.
Figure 2: Architecture Overview. Our model takes as input a tensor of dimension K, M, t. A row-wisefeed-forward network (rFFN) is applied to each row along the t × M plane transforming vectors of dimensionK to dK . After adding positional encoding (PE) to the t axis, the encoder passes the tensor through L repeatedlayers of multi-head attention blocks (MAB) that are applied to the time axis (time encoding) and the agentaxis (social encoding) before outputting the context tensor. In the decoder, the encoded map and the learnableseed parameters tensor are concatenated and passed through an rFFN before being passed through L repeatedlayers of a multi-head attention block decoder (MABD) along the time axis (using the context from the encoder)followed by a MAB along the agent axis. This figure shows the process for predicting one possible futuretrajectory (“mode”). When multiple modes are predicted, each mode has its own learnable seed parameters andthe decoder is rolled out once for each mode.
Figure 3: NuScenes Trajectory Prediction. Toprow: birds-eye-view of road network with groundtruth trajectory data where given trajectory is cyanand held-out trajectory information is pink. Bottomrow: diverse trajectories generated by the differentmodes of AutoBot-Ego. The model generates trajec-tories that adhere to the road network and capturesdistinct possible futures.
Figure 4: Omniglot qualitative results. Left: stroke completion task. We show examples of charactersgenerated using AutoBot and an LSTM baseline. The first two columns show the ground-truth image of thecharacter and the corresponding ground-truth strokes. In this task, the models are provided with the first halfof all strokes (cyan) and are tasked with generating the other half (pink). The four other columns show thegenerated future strokes, one for each latent mode. We can see that AutoBot produces more consistent andrealistic characters compared to the LSTM baseline. Right: character completion task. We show two examplesof characters completed using AutoBot-Ego and an LSTM baseline. The first two columns show the ground-truthimage of the character as before. In this case, the models are provided with two complete strokes and are taskedwith generating a new stroke to complete the character. We observe that AutoBot-Ego generates more realisticcharacters across all modes, given ambiguous input strokes.
Figure 5:	AutoBots Context Encoder.
Figure 6:	TrajNet Map Example. Ex-ample birds-eye-view map provided bythe TrajNet benchmark. The footage wascaptured by a drone and pedestrians andbicycles are freely moving through thescene.
Figure 7: AutoBots’s Inference Rate To show the impact of using a seed parameter decoder as opposed toan autoregressive decoder, we compare the inference rate of AutoBots with its autoregressive but otherwiseidentical ablation. We plot how the inference rate varies with increasing size of input/output variables. Fromleft-to-right, these variables correspond to 1) number of agents, 2) number of observable timesteps for eachagent, 3) prediction horizon, and 4) number of modes. The top row corresponds to AutoBot-Ego while thebottom row corresponds to AutoBot.
Figure 8: Influence of entropy loss onparticle experiment. The input trajec-tory (cyan, only shown in top row) is iden-tical in all cases. The model trained with-out an entropy loss term (middle row) cov-ers all modes with high variance, whilethe model with moderate entropy regu-larization (bottom row) is able to learnthe modes with low variance and withoutoverlap.
Figure 9: TrajNet++ qualitative results. In this example scene, we see five agents moving towards eachother with their input trajectories (blue) and their ground-truth future trajectories (black) shown in the leftfigure. The different rows show three of the c = 6 scene predictions made by each model variant. AutoBot-Egoand AutoBot-AntiSocial produce some modes containing collisions or socially inconsistent trajectories, whileAutoBot, which has social attention in the decoder, achieves consistent trajectories.
Figure 10: TrajNet qualitative results. In this example, we see two agents moving together in the past (cyan)and future (pink, left). We compare how different variants of AutoBots make predictions in this social situation.
Figure 11: TrajNet qualitative results 1/4. Example scene with multiple agents moving together.
Figure 12: TrajNet qualitative results 2/4. Example scene with two agents moving separately in aroad setting. We want to highlight this interesting scenario where some modes of AutoBots-Solo andAutoBots-AntiSocial results in trajectories that lead into the road, while AutoBot seems to producestrajectories more in line with the ground-truth, and lead the agent to cross the road safely.
Figure 13: TrajNet qualitative results 3/4. Additional example scenes with multiple agents movingtogether. Again, we wish to highlight the advantage of modelling the scene jointly, which is evidentby the results of AutoBot.
Figure 14: TrajNet qualitative results 4/4. Example scenes with two agents moving together.
Figure 15: Omniglot Task 1 Additional Results. These are some additional random characters fromthe Omniglot stroke completion task. We can see that AutoBot produces plausible characters on thetest set, while the LSTM struggles to create legible ones.
Figure 16: Omniglot Task 1 Failure Cases. There were some characters where AutoBot failed tolearn the correct character completion. We can currently only speculate why that is the case. Our firstintuition was that this might occur when there are 90 degree or less angles in the trajectory that isto be predicted but in Fig. 15, we can see that there are examples where this does not seem to be aproblem. We believe that this might be due to a rarity of specific trajectories in the dataset but moreinvestigation is required.
Figure 17: Omniglot Task 2 Additional Results. These are some additional random charactersfrom the Omniglot character completion task. Again, we can see that AutoBot produces plausiblecharacters on the test set, where different modes capture plausible variations (e.g. F to H and H toPI), while the LSTM struggles to capture valid characters in its discrete latents.
