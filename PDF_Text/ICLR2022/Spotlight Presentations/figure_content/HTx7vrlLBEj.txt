Figure 1: Results of the learning problem of section 2.1. Optimization is performed with a) a well-conditioned loss and b) an ill-conditioned loss. Plots show loss curves over training time (left), dataset mean and standard deviation of the output of a neuron output over training time (middle), andthe training trajectory of a data point (right).
Figure 2: Nonlinear oscillator system: a) Time evolution controlled by a HIG-trained neuralnetwork. Its inferred output is shown in blue in the background. b) Loss curves for differentoptimization methods. c) Loss curves for Adam, GN, and HIG with different batch sizes b.
Figure 3: Poisson problem: a) Example of a source distribution P (bottom) and inferred potentialfield (top). b) Loss curves of Adam and HIG training for different learning rates η . c) Loss curvesof Adam (η = 0.0001), and HIG (η = 0.02) pretrained with Adam.
Figure 4: Quantum dipole: a) A transition of two quantum states in terms of probability amplitude∣Ψ(t,χ)∣2, controlled by a HIG-trained neural network. Its inferred output is shown in blue in thebackground. b) Loss curves with Adam and HIGs for different η . c) Low-energy (LE) and high-energy (HE) loss with Adam (η = 0.0001) and HIG (η = 0.5).
Figure 5: Control of nonlinear oscillators: Additional exPeriments with (a) Adadelta, (b) Adagrad,(c) stochastic gradient descent , and (d) RMSProP. Each showing different learning rates η and batchsizes b.
Figure 6:	Control of nonlinear oscillators: Additional exPeriments with Adam for different learningrates η and batch sizes b.
Figure 7:	Poisson problem: a) Loss curves for Adam and HIG per epoch for different learningrates, b) Loss curves of Adam (η =1e-04), of HIG (η = 0.02) pretrained with Adam, and of Adam(η =1e-04) pretrained with the HIGs.
Figure 8: Quantum dipole: Additional experiments with Adam for different learning rates η andbatch sizes b.
Figure 9: Quantum dipole: Additional experiments with HIGs for different learning rates η , batchsizes b, and truncation parameters τoverly critical for HIGs. As long as numerical noise is suppressed with τ > 10-6, and the actualinformation about scaling of network parameters and physical variables is not cut off. The lattercase is visible for an overly large τ = 0.01 in the last graph on the right.
Figure 10: Quantum dipole with Convolutional Neural Network: Experiments with Adam fordifferent learning rates η and batch sizes b.
Figure 11: Quantum dipole with Convolutional Neural Network: Experiments with HIGs fordifferent learning rates η, batch sizes b, and truncation parameters τLoss Functions. While training is evaluated in terms of the regular inner product as loss function:L(Ψa, Ψb) = 1 - ∣hψa, Ψbi∣2, We use the following modified losses to evaluate low- and high-energy states for figure 4c. Let Ψ1 be the first excited state, then we define the low-energy lossas:L(Ψa, Ψb) = (∣hΨa, Ψli∣ — ∣hΨl, Ψbi∣)2Correspondingly, we define the high-energy loss with the second excited state Ψ2 :L(Ψa, Ψb) = (lhΨa, Ψ2i∣-∣hΨ2, Ψbi∣)2Additional Experiments with a Convolutional Neural Network. Our method is agnostic tospecific network architectures. To illustrate this, we conduct additional experiments with aconvolutional neural network. The setup is the same as before, only the fully-connected neuralnetwork is replaced by a network with 6 hidden convolutional layers each with kernel size 3, 20features and tanh activation, followed by an 384 neuron dense output layer with linear activationgiving the network a total of 21984 trainable parameters.
Figure 12: a) Ablation experiments with the β-hyperparameter, and b) with the κ-hyperparameter.
Figure 13: Ablation experiments with the τ -hyperparameter.
