Figure 1: Evolution of the test loss as a function of the total model parameters for English→German.
Figure 2: Evolution of test loss as a function of the model size for English→German models. Eq. (1)is jointly fitted to the empirical loss values from encoder scaling and decoder scaling experiments.
Figure 3: Comparison of the out-of-sample predictions of the scaling law with the empirical test lossvalues from symmetric scaling English→German models. Eq. (1) is fitted only using the encoder/ decoder scaling data and then just evaluated on (Ne , Nd) pairs corresponding to the symmetricscaling model. Our scaling law is able to almost fully capture the variation in the data (R2 = 99.8%).
Figure 4: Comparison of the out-of-sample predictions of the scaling law with the empirical test lossvalues from the randomly sampled English→German models. The predictions are more than 99%correlated with the empirical test loss values. See Appendix F for similar results on other test sets.
Figure 5:	Fitted scaling laws for other language pairs. Eq. (1) is jointly fitted to the empirical lossvalue from encoder/decoder scaling experiments. Similar to En→De case, the law is able to describethe empirical scaling behavior of the models with high accuracy. See the appendix for more details.
Figure 6:	Scaling behavior of models trained on back-translated data. Right: Increasing the modelsize successfully reduces the loss on the training distribution. However, on the test data (left andcenter) the loss increases after approximately 400M parameters.
Figure 7:	Scaling behavior of models trained on forward translated data. Left / center: early stoppingtest loss on Web-Domain. Right: loss at the end of the training for a subset of the training data.
Figure 8:	Log-log plot of the evolution of BLEU score as a function of test loss. For each scalingapproach, warmer colors represent larger models. Each individual color represents different check-points of a single model during training. On target original data, improvements to cross-entropy losslead to consistent improvements in BLEU score. Dashed lines correspond to power-law fits. Therelationship breaks down for source original data. See Appendix J for more details.
Figure 9: Fitted scaling exponents for En-Zh and En-De translation tasks. Across all the test setsunder consideration, we observe pd > pe .
Figure 10: We use our fitted scaling laws to evaluate the effect of encoder / decoder parameterallocation ratio when proportionally scaling the encoder and the decoder. Left: a#/α* for differentparameter allocation schemes. Right: The predicted additive loss penalty, (L - Lopt), for a modelwith 5 × 108 total (non-embedding) parameters. Each line corresponds to a different test set.
Figure 11: A comparison of scaling behavior across source and target original test sets. We use ourfitted scaling laws to estimate the evolution of reducible loss for each test set. All scaling trendscorrespond to symmetrically scaling the encoder and decoder layers.
Figure 12: Fitted scaling law (Eq. (1)) for English→German translation task. The scaling lawcaptures the scaling behavior of the models over a diverse collection of test sets and domains. Thelast row describes the evolution of the cross-entropy loss on the training data (with and withoutregularization effect).
Figure 13: Fitted scaling law (Eq. (1)) for English→German translation task. Here, we use a log-logplot in order to inspect the fit more closely. Shaded cyan regions correspond to the uncertainty regiongiven by ±2×standard deviation. Per test set standard deviations are provided in Table 4.
Figure 14: Out-of-sample prediction accuracy of English→German scaling laws on symmetric scalingmodels. Scaling laws are fitted only using the encoder and decoder scaling models. Nevertheless,they accurately predict the scaling behavior of symmetric scaling models.
Figure 15: Fitted scaling law (Eq. (1)) for German→English translation task. The scaling lawcaptures the scaling behavior of the models over a diverse collection of test sets and domains.
Figure 16: German→English scaling law fits on log-log scale.
Figure 17: Chinese→English scaling law fits.
Figure 18: Chinese→English scaling law fits on log-log scale.
Figure 19: English→Chinese scaling law fits.
Figure 20: English→Chinese scaling law fits on log-log scale.
Figure 21: Variability of the final test loss across four different seeds.
Figure 22: Comparison of the out-of-sample predictions of the scaling law with the empirical testloss values from the randomly sampled English→German models.
Figure 23: Percentage deviation from the scaling laws fitted to the English-to-German modelspresented in the main text. Different scaling approaches seem to yield similar results; for the majorityof our models, different scaling approaches deviate only 1 - 2% from the scaling trends presented inthe main text. Deviation from the trend increases for smaller models suggesting that model shapeplays an important role in the performance of small scale models. Models that experienced severetraining instabilities / NaNs during training are removed from the plots.
Figure 24: Log-log plot of the evolution of BLEU score as a function of cross-entropy loss fordifferent models. For each scaling approach, warmer colors represent larger models. Each individualcolor represents different checkpoints of a single model during training. On target original data(left column), improvements to cross-entropy loss lead to consistent improvements in BLEU score.
Figure 25: Models with shallow decoders tend to outperform predictions of Eq. (13). Points withdark green color represent different checkpoints of 6L2L, 6L4L, and 6L6L models.
Figure 26: On some of the test sets, data points corresponding to early training checkpoints exhibitdeviations from the overall trend.
Figure 27: The evolution of BLEU score during the training for English-to-German Web Domain testsets. Warmer colors correspond to larger models. Top row: On source original test data, our largestmodels achieve lower BLEU scores compared to mid-sized models throughout the training. Bottomrow: On target original test data, increasing the model size yields consistent improvements in BLEUscore throughout the training.
Figure 28: The evolution of BLEU score during the training for German-to-English Web Domain testsets. Warmer colors correspond to larger models. Top row: On source original test data, our largestmodels achieve lower BLEU scores compared to mid-sized models throughout the training. Bottomrow: On target original test data, increasing the model size yields consistent improvements in BLEUscore throughout the training.
Figure 29: The evolution of BLEURT score as a function of cross-entropy loss for different models.
Figure 30: The evolution of BLEURT score as a function of cross-entropy loss for different models.
