Figure 1: Top row: Single-occupancy grid environments in which agents can either move to a freeadjacent cell or stay still. The green apple (reward +1) can only be obtained by the leader agent (ingreen) and no other external rewards exist. Grey cells are blocked. Bottom row: Visualisation of theestimated choice of the leader agent when positioned at the respective cells. Left to right: discretechoice DCL3 , entropic ECL3 and immediate choice ICL (eq. 1, 2 and 3).
Figure 2: Left: In Level Based Foraging (LBF) two agentsmust cooperate to receive rewards for eating apples. Theleader agent is green, altruistic agent is blue. Right: In Tag,a leader (green) tries to escape from adversaries (red colors).
Figure 4: Normalized reward of theleader agent (right vert. axis, green)when the altruistic agent is trained tomaximize the leaderâ€™s choice (ours), acting(random), receiving the same reward asthe leader (supervised), or maximizing thestate entropy (Mutti et al., 2020). Left vert.
Figure 3: For two representative positions of theleader agent, its immediate choice (IC) estimates aregiven as a percentage of the maximum possible. Left(LBF): The leader agent has low IC when havingto wait for another agent to support it in foragingan apple and high IC when having the option ofapproaching multiple apples. Right (Tag): Theleader has low IC when chased by the adversaries andhigh IC when it has multiple escape paths.
Figure 5: Example behaviour of altruistic agents (blue) thatlearned to actively defend the leader agent (green) fromthe adversaries (red) in Tag. Obstacles are black. Thetrajectories taken by some of the agents in the last 10 stepsare shown as dotted lines.
