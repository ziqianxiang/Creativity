Figure 1: Networks trained to fit a se-quence of different targets on MNIST datasee increasing error on new target func-tions with the number of tasks.
Figure 2: Networks see reduced ability to fit newtargets over the course of training in two demon-strative Atari environments.
Figure 3:	Feature rank and performance over the course of training for Montezuma’s Revenge (left)and Pong (right). We observe that feature rank is higher for environments and auxiliary tasks whichprovide denser reward signals than for sparse reward problems.
Figure 4:	(a): Agent capacity vs human-normalized score in games where Rainbow does not achievesuperhuman performance. While feature rank does not appear to solely determine agent perfor-mance, there is a positive correlation between feature rank and human-normalized score. Bottomrow contains Rainbow agents trained with the regularizer presented in Equation 6. (b) An ‘unlucky’seed from our evaluations on the sparsified version of Pong, where learning progress occurs onlyafter the agent recovers from representation collapse.
Figure 5: (a) Visualization of InFeR. (b) Analysis of the effect of InFeR on capacity loss. (c)Effect of InFeR on performance in Montezuma’s Revenge with respect to Rainbow and DoubleDQN baselines. (d) Performance of InFeR relative to Rainbow on all 57 Atari games.
Figure 6: Left: agent performance does not improve over baseline when random features are addedto the representation. Right: doubling the width of the neural network narrows the performance gapin games on which InFeR under-performed relative to Rainbow.
Figure 7: Mean squared error at the end of training on each iteration of the hash-MNIST task.
Figure 8: Mean squared error after 2e3 training steps on the random-MNIST task. Target-fittingerror increases over time in under-parameterized networks, but increasing the depth or width of thenetwork slows down capacity loss, enabling positive transfer in the largest network we studied.
Figure 9: Mean squared error after 2e3 training steps on the threshold-MNIST task. Target-fittingerror increases over time in under-parameterized networks, but increasing the depth or width of thenetwork slows down capacity loss, enabling positive transfer in the largest network we studied.
Figure 10: Effect of adding InFeR to the regression objective in a random reward prediction problemon the non-stationary MNIST environment studied previously. We see that the InFeR objective pro-duces networks that can consistently outperform those trained with a standard regression objective,exhibiting minimal capacity loss in comparison to the same network architecture trained on the samesequence of targets.
Figure 11:	Hyperparameter sweeps for the DDQN+InFeR agent. Each contour plot shows averagecapped human-normalized score at the end of training marginalized over all hyperparameters notshown on its axes.
Figure 12:	Hyperparameter sweeps for the Rainbow+InFeR agent. Each contour plot shows averagecapped human-normalized score at the end of training marginalized over all hyperparameters notshown on its axes.
Figure 13:	Feature rank and performance of RL agents on demonstrative Atari environments.
Figure 14: feature rank of agent representations over the course of training on all 57 games inthe Atari benchmark. We compare Rainbow against Rainbow+InFeR. Rainbow+InFeR does notuniformly prevent decreases in feature rank across all games, but on average it has a beneficial effecton preserving representation dimension.
Figure 15: Mean squared error, after 10000 training steps for the target-fitting on random networktargets. We also show the corresponding feature rank of the pre-trained neural network (beforetarget-fitting).
Figure 16:	Full evaluation of capped human-normalized performance on Atari benchmarks for thedefault Rainbow architecture.
Figure 17:	Full evaluation of capped human-normalized performance on Atari benchmarks in thedouble-width Rainbow architecture.
Figure 18:	Full evaluation of raw scores on Atari benchmarks for the default Rainbow architecture.
Figure 19:	Full evaluation of raw scores on Atari benchmarks for the double-width Rainbow archi-tecture.
Figure 20: Evaluations of the effect of InFeR on performance of a Double DQN agent. Overallwe do not see as pronounced an improvement as in Rainbow, but note that the average human-normalized score over the entire benchmark is nonetheless slightly higher for the InFeR agent, andthat the performance improvement obtained by InFeR in Montezuma’s Revenge is still significantin this agent.
