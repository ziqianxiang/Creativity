Figure 1: Normalized perfor-mance and safety metrics, aver-aged across tasks of the Safety-Gym (Ray et al., 2019) SG6benchmark. LAMBDA achievesconstraint satisfaction in all tasksof the SG6 benchmark while sig-nificantly improving performanceand sample efficiency. See Ap-pendix H for further details onnormalization.
Figure 2: Safety-Gym SG6 benchmark tasks. In our experiments, we use first-person-view images ofsize 64×64 pixels as observations. Green objects represent goals that should be reached by the robot.
Figure 3: Posterior sampling: We sample j = 1,...,N models θj 〜 p(θ∣D) (e.g., in this illus-tration, N = 5). For each model, we simulate trajectories that are conditioned on the same policyand initial state. Objective and constraints: For a given posterior sample θj , we use the simulatedtrajectories to estimate J (π, pθj ) and Ji (π, pθj ) ∀i ∈ {1, . . . , C} with their corresponding critics.
Figure 4: Experimental results for the SG6 benchmark. As done in Ray et al. (2019), we normalizethe metrics and denote J(∏), Jc(∏),pc(∏) as the normalized metrics (See also Appendix H). Wenote that LAMBDA achieves better performance while satisfying the constraints during test time.
Figure 5: Learning curves of LAMBDA and its unsafe version on the PointGoal2 environment. Asshown, LAMBDA exhibits similar performance in solving the task while maintaining the safetyconstraint and significantly improving over the baseline algorithms.
Figure 6:	Benchmark results of LAMBDA. Solid red lines indicate the threshold value d = 25.
Figure 7:	LAMBDA solves most of the tasks with significantly less interactions with the envi-ronment, compared to the baseline model-free methods. Green check marks and red ‘x’ indicateconstraint satisfaction after reaching the required number of steps.
Figure 8: PointPush1 with partially transparent box. When the color of the box is solid LAMBDAstruggles in solving the task due to occlusion of the goal by the box. By changing the transparencyof the box, we make the PointPush1 task less partially observable and thus easier to solve.
Figure 9:	Learning curves for the PointPush1 task and ObservablePointPush1 task which uses par-tially transparent box. We also show the baseline algorithms performance with a “pseudo-LiDAR”observation.
Figure 10:	Benchmark results of LAMBDA and its unsafe implementation. In the majority of thetasks, LAMBDA is able to find policies that perform similarly to the unsafe version while satisfyingthe constraints. Interestingly, apart from the DoggoGoal1 and PointGoal2 tasks LAMBDA’s policiesare able to achieve similar returns while learning to satisfy the constraints.
Figure 11:	Comparison of LAMBDA when ablating the policy optimization and using CEM-MPCwith rejection sampling as introduced in Liu et al. (2021). As shown, LAMBDA performs substan-tially better than CEM-MPC. We believe that when the goal is not visible to the agent, CEM-MPC’spolicy fails to locate it and drive the robot to it. On the contrary, in our experiments, we observedthat LAMBDA typically rotates until the goal becomes visible, thus allowing the robot to gathersignificantly more goals.
Figure 12:	Comparison of LAMBDA and greedy exploitation. LAMBDA generally solves thattasks with better performance and with improved sample efficiency. Notably, the greedy exploitationvariant fails to solve the the pointButton1 task.
Figure 13: Computational graphs for the backward and forward passes of Algorithm 3.
