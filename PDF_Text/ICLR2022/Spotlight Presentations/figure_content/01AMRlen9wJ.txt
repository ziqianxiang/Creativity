Figure 1: Collecting gsSO1)	gsSO: Note that DrMAD in Algorithm 2 (line 6) sequentially back-propagates gSO for t =T, . . . , 1. The important observation is that, at step t, this incomplete second-order term gSO =T∑2i=t ατAT AT-1 •…Ai+1Bi Can be seen as the valid second-order term computed over the hori-zon of size s = T - t + 1. This is because the reparameterization s = T - t + 1 givessgSO =	αs+(T-S As (T- As-1+(T-s) ∙∙∙ Ai+1 (T-S Bi (T-s)	(16)i=1for s = 1, . . . , T, nothing but shifting the trajectory index by T -s steps so that the last step is alwaysT. Therefore, we can efficiently obtain the valid second-order term gsSO for all s = 1, . . . , T throughthe single backward travel along the interpolated trajectory (See Figure 1). Each gsSO requires tocompute only one or two additional JVPs. Also, as we use DrMAD instead of RMD, we only storew0 such that the memory cost is constant w.r.t. the total horizon size T .
Figure 2: Meta-training convergence measured in Lval (wT , λ) with T = 100 inner-steps. We report meanand and 95% confidence intervals over 5 meta-training runs.
Figure 3:	Cosine similarity to exact RMD in terms of (a) hypergradients gFO + gSO. (b, C) second-order termgSO. The curves in (b) correspond to Eq. (7) with various Y.
Figure 4:	(a,b) Samples collected from Algorithm 4 and correspondingly fitted linear estimators (θ). (c) Afitted estimator and the range of actual ground-truth estimator (the shaded area is one sigma). (d) The stabilityof θ estimation. (e) Meta-test performance vs. computational cost in terms of the number of JVPs per inner-opt.
Figure 5: Meta-validation performance. We report mean and and 95% confidence intervals over 5 meta-training runs.
Figure 6:	Meta-test performance by varying the value of γ. Red stars denote the actuall γ we used for eachexperiment (we found them with a meta-validation set) and the corresponding performance.
Figure 7: Learned loss weighting function with each algorithm.
Figure 8: Meta-convergenceMSEFO	0.567±0.1931-step	0.670±0.283DrMAD	1.086±0.176N.IFT	0.502±o.i46HyPerDistill	0.327±0.052Table 4:	Meta-test performance.
Figure 9: Meta-train convergenceFigure 10: Meta-test convergenceTest ACC1-step	70.15±i.24N.IFT	70.85±0.63HyPerDistill 72.68±ι.i5Table 5:	Meta-test performance.
Figure 10: Meta-test convergenceTest ACC1-step	70.15±i.24N.IFT	70.85±0.63HyPerDistill 72.68±ι.i5Table 5:	Meta-test performance.
Figure 11: short horizon bias16Published as a conference paper at ICLR 2022J Experiments with FOMAMLFigure 12: Meta-train convergence Figure 13: Meta-val. convergenceTest ACCFO	45.01±1.291-step	46.03±0.59HyPerDistill 51.43±o.98Table 6:	Meta-test performance.
Figure 12: Meta-train convergence Figure 13: Meta-val. convergenceTest ACCFO	45.01±1.291-step	46.03±0.59HyPerDistill 51.43±o.98Table 6:	Meta-test performance.
