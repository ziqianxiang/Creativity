Figure 1:	High scores in brain predictivity can be achieved with few supervised updates (log x-axes). A Average brain predictivity of models trained with a range of supervised updates (differentcombinations of epochs × images). Each dot is a different hypothesis of how the ventral visualstream might have developed and shows the adult brain-likeness score that is achieved by that model.
Figure 2:	“At-birth” synaptic connectivity yields reasonable scores in brain predictivity. ASpecifying the initial weight distribution: Kaiming Normal (KN, He et al., 2015) samples froma generic Gaussian. Weight Compression (WC) compresses trained weights into low-parameterclustered distributions that weights can be initialized from. B Visualization of WC compressedparameters: Gabor filters for first layer and cluster centers for following layers with kernel size > 3.
Figure 3:	Training only critical layers reduces the number of updated synapses while maintain-ing high brain predictivity. A We could naively reduce the parameters of a fully-trained model byfreezing layers from the bottom up, training only the top layers (“Downstream Training DT”; graybox). We instead propose Critical Training (CT) which only trains thin “critical” layers (e.g. down-sampling layers, blue box). B CORnet-S circuitry. CT only trains critical layers, leaving the restfrozen. C Naively reducing parameters from standard training (black dot, top right) quickly deteri-orates brain predictivity scores (DT, gray line) whereas Critical Training reduces parameters whileretaining high scores (blue line, CT). D Like C, but measuring ImageNet score. CT retains nearlyhalf the score with a fraction of parameters.
Figure 4: Benefits of combining training reductions for high brain predictivity with few super-vised synaptic updates and limitations of techniques in high-update regimes. A (log x-axis) Byreducing updates with a combination of fewer supervised updates (Figure 1), improved initializationWC (Figure 2), and training only down-sampling layers CT (Figure 3), the resulting models (darkblue dots; fewer supervised updates alone in light blue) maintain high brain predictivity scores whilerequiring only a fraction of supervised synaptic updates compared to standard CORnet-S (black dot,top right). B Comparison between WC-initialized models trained with CT versus standardly initial-ized models training all weights, when varying training epochs and labeled images. Colors representtheir percent point difference in brain predictivity scores. WC+CT improve performance in regimeswith few epochs and images, but fall behind in regimes with many labeled images and epochs.
Figure 5: Transfer to othernetworks and individual scorescomparison. A Transfer toother networks. We sample fromWC initializations determined onCORnet-S, followed by CriticalTraining of only down-samplinglayers. B Absolute scores onindividual benchmarks of combi-nations of initialization (KN/WC,Figure 2), and with critical training(CT, Figure 3) techniques.
Figure 6: Findings generalize acrosshyperparameters. Notations like inFigure 1. To test whether fast learningcould be achieved by purely changinghyperparameters, we evaluated dif-ferent regularizations (weight decay0.001 and 0.00001 vs. 0.0001), learn-ing rates (initial 1.0 and 0.01 vs. 0.1in the main manuscript) and learn-ing rate schedules (“Scheduler alter-native” decreases learning rate moreagressively). Trends are qualitativelysimilar across all choices, and quanti-tatively nearly optimal with respect tofast convergence to high brain predic-tivity for the hyperparameters used inthe manuscript. We suspect that morecareful tuning of e.g. the learning ratecould lead to an improved trade-offbetween supervised updates and brain
Figure 7: Alternative weightcompression methods Compar-ison of different initializationsthat compress weights, “at birth”i.e. without any training (gray)and after training critical layers(shades of blue) for 6 epochs.
Figure 8: Detailed analysis of WC+CT. A When reducing the number of supervised synaptic up-dates, adding critical training (dark grey) and adding weight compression initialization (dark blue)both improve scores in brain predictivity at the same number of supervised synaptic updates, incomparison to a model with standard initialization and training all weights (bright blue). B Brainpredictivities for the WC+CT model when trained with a range of epochs and labeled images. CSame as B, but for a standardly initialized (KN) model training all weights.
Figure 9: Individual brain benchmark scores for WC+CT model A Individual brain predictiv-itiy scores of WC+CT models trained with a range of epochs on all images. These models scoreespecially high on V1, V2 and V4 already after one epoch in comparison to a model with standardinitialization training all weights. IT and Behavior benchmarks continuously improve over laterepochs as well but fall short of a fully trained model. B Like A, but with models trained until con-vergence on different numbers of labeled images, up to the full dataset of 1.28M images (rightmostpoints). As in A we see > 80% V1, V2 and V4 scores with only 100,000 images. For comparableIT and behavioral scores, more images are required.
