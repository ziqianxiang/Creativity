Figure 1: Level plots showing the evolution of two regression MTL problems with/without RotoGrad,see §4. RotoGrad is able to reach the optimum (☆) for both tasks. (a) In the space of z, RotoGradrotates the funCtion-spaCes to align task gradients (blue/orange arrows), finding shared features z(green arrow) closer to the (matched) optima. (b) In the space of rk, RotoGrad rotates the sharedfeature z, providing per-task features rk that better fit each task.
Figure 2: Hard-parameter sharing architecture, in-cluding the rotation matrices Rk of RotoGrad.
Figure 3: Test error on the sum of digitstask for different values of RotoGrad’slearning rate on multi-MNiST.
Figure 4: Similarity between task and update gradientsfor different methods on CIFAR10, averaged over tasksand five runs.
Figure 5: Samples extracted from the modified MNIST and SVHN datasets.
Figure 6: Level plots showing the illustrative examplesof Figure 1 for RotoGrad. Top: Convex case. Bot-tom: Non-convex case. Left: Active transformation(trajectories of rk and the level plot of L1 + L2 . Right:Passive transformation (trajectory of z and level plot of(Li ◦ Ri) + (L2 ◦ R2)).
Figure 7: Logistic regression for opposite classifica-tion tasks. Test data is plotted scattered as gray dots.
Figure 8: RotoGrad’s performance on all tasks for the experiments in §6.1 for all metrics. We canobserve training instabilities/stiffness on all tasks as we highly increase/decrease RotoGrad’s learningrate, as discussed in the main manuscript.
Figure 9: Cosine similarity between the task gradients and the update gradient on CIFAR10. Resultsare averaged over tasks and five runs, showing 90 % confidence intervals (shaded areas).
