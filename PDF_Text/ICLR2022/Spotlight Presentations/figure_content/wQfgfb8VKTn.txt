Figure 1: Left: Learning curves (return and the number of successfully scanned targets) ofCASEC and DCG on Sensor. Middle: The influence of graph sparseness on performance (re-turn and the number of scanned targets). Here we show the case of the best seed. The plot of otherseeds can be found in Fig. 21. Right: An example coordination graph learned by our method.
Figure 2: Coordination graphs learned by different methods on Sensor.
Figure 3: Performance comparison with baselines on the MACO benchmark.
Figure 4: The influence of graph sparseness (1.0represents complete graphs) on the performanceon factored games (Sensor, left) and non-factoredgames (Gather, right).
Figure 5: Performance and TD errors compared to baselines and ablations on the SMAC benchmark.
Figure 6: Task Hallway (Wang et al.,2020). To increase the difficulty of thegame, we consider a multi-group ver-sion. Different colors represent differentgroups.
Figure 8: Performance of different implementations on Aloha. Different colors indicate differenttopologies. Performance of different losses is shown in different sub-figures.
Figure 7: Task Gather. To increasethe difficulty of this game, we con-sider a temporally extended versionand introduce stochasticity.
Figure 9: Performance of different implementations on Pursuit. Different colors indicate differenttopologies. Performance of different losses is shown in different sub-figures.
Figure 10: Performance of different implementations on Hallway. Different colors indicate differenttopologies. Performance of different losses is shown in different sub-figures.
Figure 11: Performance of different implementations on Sensor. Different colors indicate differenttopologies. Performance of different losses is shown in different sub-figures.
Figure 12: Performance of different implementations on Gather. Different colors indicate differenttopologies. Performance of different losses is shown in different sub-figures.
Figure 13: Performance of different implementations on Disperse. Different colors indicate differenttopologies. Performance of different losses is shown in different sub-figures.
Figure 14: Performance comparison between our method (Î¶iqjvar & Lsqpvaarrse) and the (attentional)observation-based approach on the MACO benchmark.
Figure 15: Framework for learningaction representations, reproducedfrom Wang et al. (2021b).
Figure 16: Compare actions selected by Max-Sum and the optimal joint action on 1000 differentconfigurations of Aloha. Left: On sparse graphs with 20% edges. Qtot values of the actions areshown. Middle: On full graphs. Qtot values of the actions are shown. Right: How many actionsselected by Max-Sum are optimal under different sparseness degrees.
Figure 17: Compare actions selected by Max-Sum and the optimal joint action on 1000 randomconfigurations. Left: On sparse graphs with 10% edges. Qtot values of the actions are shown.
Figure 18: Comparison between the bound in Proposition 1 and the probability in real cases.
Figure 19: Comparison between CASEC and DCG on Aloha (Left) and Sensor (Right) with twotimes number of agents.
Figure 20: Learning curves (Left) and the changing process (Right) of the communication thresholdof the two methods proposed in Appendix I.
Figure 21: Performance of DCG on Sensor with different numbers of edges in the coordinationgraph.
