Figure 1: Visualization of the aleatoric andpredictive uncertainty estimates of NatPN ontwo toy regressions and classification tasks.
Figure 2: Overview of Natural Posterior Network. Inputs x(i) are first mapped to a low-dimensionallatent representation z(i) by the encoder fφ. From z(i), the decoder gψ derives the parameter updateχ(i) while a normalizing flow Pω yields the evidence update n(i). Posterior parameters are obtainedfrom a weighted combination of prior and update parameters according to npost,(i).
Figure 3: Averaged accuracy and confidence under15 dataset shifts on CIFAR-10 (Hendrycks & Di-etterich, 2019). On more severe perturbations (i.e.
Figure 4: Visualization of the aleatoric and epistemic uncertainty on a 2D toy dataset with 3 classeswith 900 training samples for each class.
Figure 5: Visualization of the aleatoric and epistemic uncertainty on a 2D toy dataset with 3 classeswith 900 training samples for class 1 (green), 600 training samples for class 2 (red) and 300 trainingsamples for class 2 (blue).
Figure 6: t-SNE visualization of the latent space of NatPN on MNIST (ID) vs KMNIST (OOD). Onthe left, The ID data (MNIST in green) can easily be distinguished from the OOD data (KMNIST inred). On the right, NatPN correctly assigns higher likelihood to ID data.
Figure 7: Histogram of the entropy of the posterior distribution accounting for the predictive uncer-tainty of NatPN on MNIST (ID) vs KMNIST, Fashion-MNIST, Out-Of-Domain (OOD) and NYU(ID) vs LSUN classroom and LSUN church + KITTI (OOD). In both cases, low entropy is a goodindicator of in-distribution data.
Figure 8: Visualization of the predicted depth and predictive uncertainty estimates of NatPN per pixelon the NYU Depth v2 dataset. NatPN predicts accurate depth uncertainty and reasonably assignshigher uncertainty to object edges.
