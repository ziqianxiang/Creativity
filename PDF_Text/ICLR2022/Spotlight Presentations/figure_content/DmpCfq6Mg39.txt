Figure 1: A schematic comparison of (a) DyConv (CondConv uses GAP+FC+Sigmoid) and (b)ODConv. Unlike CondConv and DyConv which compute a single attention scalar αwi for the con-volutional kernel Wi, ODConv leverages a novel multi-dimensional attention mechanism to computefour types of attentions αsi , αci , αfi and αwi for Wi along all four dimensions of the kernel spacein a parallel manner. Their formulations and implementations are clarified in the Method section.
Figure 2: Illustration of multiplying four types of attentions in ODConv to convolutional kernelsprogressively. (a) Location-wise multiplication operations along the spatial dimension, (b) channel-wise multiplication operations along the input channel dimension, (c) filter-wise multiplication op-erations along the output channel dimension, and (d) kernel-wise multiplication operations along thekernel dimension of the convolutional kernel space. Notations are clarified in the Method section.
Figure 3: Comparison of model accuracy and size for the pre-trained MobileNetV2 models basedon different dynamic convolution methods. All models are trained for 150 epochs on the ImageNetdataset. It can be seen that our ODConv (1×) makes a better accuracy and size tradeoff for the light-weight MobileNetV2 backbones compared to CondConv and DyConv. On the larger ResNet18 andResNet50 backbones, even better results are obtained by ODConv, as can be seen from the resultsshown in Table 2 of the main paper.
Figure 4: Curves of top-1 training accuracy (dashed line) and validation accuracy (solid line) ofthe ResNet18 models trained on the ImageNet dataset with CondConv (8×), DyConv (4×), ourODConv (1×) and ODConv (4×), respectively. Comparatively, our ODConv (1×) outperformsboth CondConv and DyConv yet has only 14.68%|26.26% parameters of the model trained withCondConv|DyConv. Our ODConv (4×) converges with the best validation accuracy, which outper-forms CondConv|DyConv by 1.98%|1.21% top-1 gain with fewer parameters.
Figure 5: Comparison of illustrative visualization results with Grad-CAM++ (Chattopadhyay et al.,2018). Results are obtained from the pre-trained ResNet18 models (reported in Table 7) with αsi,αci , αfi , αwi, and all of them (i.e., ODConv (4×)), separately. (a) The model with αsi and themodel with all four attentions make the right predications, while the other three models with onesingle attention fail. (b) The model with αci and the model with all four attentions make the rightpredications, while the other three models with one single attention fail. (c) The model with αfi andthe model with all four attentions make the right predications, while the other three models with onesingle attention fail. (d) The model with αwi and the model with all four attentions make the rightpredications, while the other three models with one single attention fail. (e) Only the model with allfour attentions makes the right predications, while the other four models with one single attentionfail. These visualization results further backup the conclusion observed from Table 7. Best viewedwith zoom-in.
Figure 6: Comparison of the statistical distributions of the learnt attention values regarding αsi ,αci, αfi and αwi across different layers of the pre-trained ResNet18 model with ODConv (4×).
