Figure 1: An overview of the proposed GNN-LM model pipeline. Left: Given an input contextCt = (wι, ∙∙∙ ,wt-ι) (here the context is “The movie is")，a base LM model encodes it into ahigh-dimensional representation ht, which is then used to query the training datastore to retrievethe nearest contexts along with the visited tokens (marked in red). Right: The tokens in the inputcontext and the retrieved tokens comprise a graph and are viewed as two types of nodes: nodes fromthe original text and nodes from the neighbor text. Intra-context edges link tokens within the sameinput, and inter-context edges link tokens from the retrieved contexts to the original context. Aftermodeling the graph as a whole with GNNs, we use the updated representation of wt-ι (token “is” inthis example) to compute the likelihood of the next token.
Figure 2: Comparisons between base LM and GNN-LM on WikiText-103 with respect to different k.
