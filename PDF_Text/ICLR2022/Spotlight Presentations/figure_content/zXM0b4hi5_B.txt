Figure 1: Distances in autoencoders. De is the euclidean distance (green), Dr is the reconstructiondistance (blue), Ds is the self-distance, Din is the inner distance. Red circles illustrate the allowederror for the points xι and x2, i.e. the reconstructed points (Xi and X2) should live inside the circle.
Figure 2: Illustrating Observations 2, 3, and 4. Spearman correlations ρS pD, ppx1qq betweendifferent distances and the probability of point x1 are shown. Each point corresponds to the correlationfor one autoencoder trained for a particular rate regime.
Figure 3: Illustrating Observation 5. Spearman correlations ρS between induced distances (Dror Din) and mean opinion score (MOS) for images from TID 2013 dataset (Ponomarenko et al.,2013). Pretrained compressive autoencoders at different bitrates were used. factorized-mse denotesnetworks trained using MSE D “ ||x1 ´ x2||22 in Eq. 3, and factorized-msssim networks useD “ 1 — MS-SSIM(xι — X2)in Eq. 3.
Figure 4: Illustrating observation 6. Figure shows the relative performance of the models forsamples across the support of the respective data distributions. Both training with the probability (left)and the perceptual distance (right) cause the model to allocate more resources to high probabilityregions. Left: models trained with D “ P(X) ∙ ||xi — X2∣∣2 in Eq. 3 divided by performance ofmodels trained with D “ ||x1 ´ x2 ||22 on the 2D Student-t and evaluated using samples along x-axis(see Appendix C). Right: models trained for image compression with D “ 1 — MS-SSIM(X1, X2) inEq. 3 divided by performance of models trained with D “ ||X1 — X2 ||22.
Figure 5: Illustrating observation 6. The perceptual distance provides regularization when one hasno samples from the training distribution. DeCoded image (Compressed at 0.25 bpp) enCoded withnetworks trained using data from a uniform distribution and EuClidean vs. perCeptual losses.
Figure 6: Illustration of Oberservation 1. Spearman correlations ρS between the sensitivity of theperceptual distances NLPD and MS-SSIM and logpppxqq (in red/gray). Distances are computedbetween x, and a distorted version with additive Gaussian noise, x'∆x, with deviation σ. Correlationof RMSE with perceptual distortions (in orange/brown) and of RMSE with logpppxqq (in blue) areincluded for comparison. MS-SSIM is a similarity index, so 1-(MS-SSIM) is a distortion measure.
Figure 7: Scatter plots showing logpppxq and perceptual distances NLPD and MS-SSIM betweenimages from CIFAR-10 with additive Gaussian noise for σ “ 1, 50Fig. 8 shows the distribution of logpppx ` Np0, σ2qqq for σ “ t0, 1, 2, 5, 10, 20, 30, 40, 50u. Apretrained PixelCNN++ model was used to evaluate logpppxqq. It is clear that when more noise isintroduced in the images the probability of being a natural image decrease (As expected).
Figure 8: Distributions of logpppx ` Np0, σ2qqq for various σ where logpppxqq is estimated usingPixelCNN++ model.
Figure 9: Magnitude of the deviation introduced by denoising autoencoders in a Gaussian PDF (firstcolumn) and in different PDFs proposed to model natural images (rest of the columns). The scorematching, BlOgBxPxqq, is correlated to Ppxy in the mode of the distributions. Zoomed-in in the last row.
Figure 10: The network architecture for the 2D example where e is the encoder, d the decoder, Q isthe quantisation step, AE is a arithmetic encoder and AD is the arithmetic decoder. The quantisationQ used is rounding to the nearest integer, which is approximated by additive uniform noise at trainingtime.
Figure 11: The 2D Student-t distribution used throughout the paper and one compressed representationfound by minimizing Eq. 3. For c) the lines represent quantisation boundaries (within a bin, all pointsare compressed to the same point by e and the dots represent code vectors (where these points areprojected to by d.
Figure 12: Resulting compression when using the 2D Student-t distribution and including theprobability distribution in the loss function. b) is seen as including information about the distributionin the loss function and c) is seen as removing it. BPP is bits per pixel (the rate) and SSE is sumsquare errors (distortion) evaluated on a validation set.
Figure 13: Resulting compression when using a 2D uniform distribution across the space, whereleft) optimized the rate-distortion equation over the uniform distribution and right) optimized forthe probability belonging to 2D Student-t weighting the rate-distortion equation over the uniformdistribution.
Figure 14: Relative performance of networks for samples along a line through the support of therespective distributions. Left: networks trained with D “ P(X) ∙ ||xi — X2∣∣2 in Eq. 3 divided byperformance of networks trained with D “ ||x1 ´ x2||22 on the 2D Student-t and evaluated usingsamples along x-axis. Left) shows 20 degree polynomial fit to each network and right) shows the rawvalues.
Figure 15: Architecture for networks used in Sec. 4 & 5.1 which is the the factorized prior modelfrom (Balle et al., 2018) where e is the encoder, d the decoder, Q is the quantisation step, AE is aarithmetic encoder and AD is the arithmetic decoder. The quantisation Q used is rounding to thenearest integer, which is approximated by additive uniform noise at training time. GDN denotesa generalized divisive normalization activation, and Conv2d is a 2-d convolution operation. Theconvolution parameters are filters × kernel height × kernel width - down- or upsampling stride. Forthe 5 lower bit rates, N “ 128, M “ 192 and for higher rates N “ 192, M “ 320.
Figure 16: Example of altering the contrast of an image from the Kodak Image dataset varying α inEq. 14values are sampled. Fig. 16 shows an example for samples generated for one image. We comparenetworks with probability included in the loss function, in the form of perceptual distances, to thosewithout. The ratio defined earlier will be used, where P1 is the distortion/rate for networks optimizedwith MS-SSIM and P0 is distortion/rate for networks optimized for MSE. The ratio defined inEq. 13, where the numerator denotes networks optimized using perceptual metric MS-SSIM and thedenominator denotes networks optimized for MSE. All networks were pretrained and taken from theTensorflow Compression package. Fig. 4 shows the ratio of performance as we vary α.
Figure 17: Architecture of networks used in Sec. 5.2 where e is the encoder, d the decoder, Q is thequantisation step, AE is a arithmetic encoder and AD is the arithmetic decoder. The quantisationQ used is rounding to the nearest center defined by L, which is approximated by Eq. 15 at trainingtime. GDN denotes a generalized divisive normalization activation, and Conv2d is a 2-d convolutionoperation. The convolution parameters are filters × kernel height × kernel width - down- or upsamplingstride. For all rates N “ 128, M “ 64Setting an upper bound on the entropy of the encoding with L centers c1, ..., cL, the soft differentiable20Published as a conference paper at ICLR 2022approximation isLyi “ Σj“1exp(一s(yi — Cj )2)ΣL=ι eχp(一s(yi — Ckq2q j(15)where s is the quantisation scale parameter which we fix to 1. Given that we know the dimensionalityof y and a maximum of L integers that can be represented, an upper bound on the entropy can beobtained;,、 W ^ H	,、
Figure 18: The reconstruction of image 1 from Kodak dataset from the various networks trainedto compress to a maximum entropy bits per pixel (bpp) specified optimized using the OpenImagesdataset (Krasin et al., 2017).
Figure 19: The reconstruction of image 1 from Kodak dataset from the various networks trained tocompress to a maximum entropy bits per pixel (bpp) specified optimized using random uniform noise.
Figure 20: Gain in using NLPD over MSE as a loss function evaluated in terms of MSE loss on test set(Kodak dataset) using batch size of 1 and a small learning rate, fixing random seeds. XNLPD denotesthe reconstruction of X with a network optimized for NLPD, and XMSE for a network optimized forMSE. The mean (solid line) and standard deviation (solid fill) was taken over 5 runs with differentrandom seeds, i.e. different network initialization and training image ordering. The dashed linerepresents if the two networks had the same expected MSE on the test set.
