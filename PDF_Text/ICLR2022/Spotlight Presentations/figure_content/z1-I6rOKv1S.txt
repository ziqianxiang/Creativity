Figure 1: Examples of bounding box uncertainties on the object detection task. The thicker boundaryrepresents the original model prediction, and the thinner lines represent ten quantiles (between0.1 and 0.9) predicted via QFR. High-confidence boundaries (such as those of the car) are closertogether, while low-confidence boundaries (e.g., the bus which is blurred and partially occluded) areasymmetrically spread out, reflecting the advantages of non-Gaussian uncertainty estimation.
Figure 2: Examples of Predictive Uncertainties on the Times Series Forecasting Task8BSDS 300POOq 重= 6cπa>⅛61uN.25,20,15.10----Trained with LL Objective. QL---- Trained With QL Objective, QLTtained with LL objective, NLLTrained with QL objective. NLL——7i-ained with LLObjective. QL-----Itained with QL Objective. QL■---- Hained with LL objective. NLL---- Itained with QL objective, NLLPowerFigure 3: Log likelihood and CRPS error curves for the MAF on three UCI Datasets.
Figure 3: Log likelihood and CRPS error curves for the MAF on three UCI Datasets.
Figure 4: Images of digits sampled from four autoregressive flow methods.
Figure 6: 80% Confidence Intervals obtained via AQF (left) and Gaussian regression (right). Notethat confidence intervals obtained from Gaussian regression (right, blue) differ significantly from thetrue confidence intervals (right, red). Confidence intervals obtained from AQF (left, blue) perfectlyoverlap with the true intervals (left, red).
Figure 7: MNIST samplesTable 11: MNIST Generation ExperimentsMethod	U-CRPS	CRPS	NLLPixelMAF-LL	.128	.279	-6011PixelMAF-QL	.099	.215	-5888Results. From the samples in Figure 7, the results generated from the MAF-QL seems to be muchmore visually accurate than MAF-LL, which is trained on a LL-based loss, even though they have thesame architecture.
