Table 1: Posterior probabilities given by a Bayesian signed-rank test comparison of the proposedmethods against the baselines. {>}, {<}, {=} refer to the respective events that the row method isbetter, the column method is better, or that they are equivalent.
Table 3: Performance comparison of GP-VAE (B-LS2T) with the baseline methodsMethod	HMNIST			Sprites	Physionet	NLL	MSE	AUROC	MSE	AUROCMean imputation	-	0.168 ± 0.000	0.938 ± 0.000	0.013 ± 0.000	0.703 ± 0.000Forward imputation	-	0.177 ± 0.000	0.935 ± 0.000	0.028 ± 0.000	0.710 ± 0.000VAE	0.599 ± 0.002	0.232 ± 0.000	0.922 ± 0.000	0.028 ± 0.000	0.677 ± 0.002HI-VAE	0.372 ± 0.008	0.134 ± 0.003	0.962 ± 0.001	0.007 ± 0.000	0.686 ± 0.010GP-VAE	0.350 ± 0.007	0.114 ± 0.002	0.960 ± 0.002	0.002 ± 0.000	0.730 ± 0.006GP-VAE (B-LS2T)	0.251 ± 0.008	0.092 ± 0.003	0.962 ± 0.001	0.002 ± 0.000	0.743 ± 0.007BRITS	-	-	-	-	0.742 ± 0.008Results. To make the comparison, we ceteris paribus re-ran all experiments the authors originallyincluded in their paper (Fortuin et al., 2020), which are imputation of Healing MNIST, Sprites,and Physionet 2012. The results are in Table 3, which report the same metrics as used in Fortuinet al. (2020), i.e. negative log-likelihood (NLL, lower is better), mean squared error (MSE, loweris better) on test sets, and downstream classification performance of a linear classifier (AUROC,higher is better). For all other models beside our GP-VAE (B-LS2T), the results were borrowedfrom Fortuin et al. (2020). We observe that simply adding the B-LS2T layer improved the resultin almost all cases, except for Sprites, where the GP-VAE already achieved a very low MSE score.
