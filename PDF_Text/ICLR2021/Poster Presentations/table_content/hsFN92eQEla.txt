Table 1: NLP task statistics and descriptionsCorpus	|Train|	∣Test∣	#classes	Metric	DomainMRPC (Dolan & Brockett, 2005)	3.7K	1.7K	2	acc./F1	newsSST-2 (Socher et al., 2013)	67K	1.8K	2	acc.	movie reviewsQNLI (Rajpurkar et al., 2016)	105K	5.4K	2	acc.	WikipediaQQP (Iyer et al., 2017)	364K	391K	2	acc./F1	social QA questionsTable 2 gives the accuracy and Table 3 gives the F1 scores of the neural models on NLP tasks. AsTable 2: NLP results, accuracyModel	Task	train with square loss (%)	train with cross-entropy (%)	square loss w/ same epochs as CE (%)	MRPC	83.8	82.1	83.6BERT	SST-2	94.0	93.9	93.9(Devlin et al., 2018)	QNLI	90.6	90.6	90.6	QQP	88.9	88.9	88.8	MRPC	717	70.9	71.5LSTM+Attention				(Chen et al., 2017)	QNLI QQP	79.3 83.4	79.0 83.1	79.3 83.4	MRPC	73.2	69.4	72.5LSTM+CNN	QNLI	76.0	76.0	76.0(He & Lin, 2016)	QQP	84.3	84.4	84.3can be seen in Table 2, in 9 out of 10 tasks using the square loss has better/equal accuracy compared
Table 2: NLP results, accuracyModel	Task	train with square loss (%)	train with cross-entropy (%)	square loss w/ same epochs as CE (%)	MRPC	83.8	82.1	83.6BERT	SST-2	94.0	93.9	93.9(Devlin et al., 2018)	QNLI	90.6	90.6	90.6	QQP	88.9	88.9	88.8	MRPC	717	70.9	71.5LSTM+Attention				(Chen et al., 2017)	QNLI QQP	79.3 83.4	79.0 83.1	79.3 83.4	MRPC	73.2	69.4	72.5LSTM+CNN	QNLI	76.0	76.0	76.0(He & Lin, 2016)	QQP	84.3	84.4	84.3can be seen in Table 2, in 9 out of 10 tasks using the square loss has better/equal accuracy comparedwith using the cross-entropy, and in terms of F1 score (see Table 3), 5 out of 6 tasks training withthe square loss outperform training with the cross-entropy loss. Even with same epochs, i.e. withsame computation cost, using the square loss has equal/better accuracy in 8 out of 10 tasks , and hashigher F1 score in 5 out of 6 tasks.
Table 3: NLP results, F1 SCoreSModel	Task	train with square loss (%)	train with cross-entropy (%)	square loss w/ same epochs as CE (%)BERT	MRPC	8871	86.7	88.0(Devlin et al., 2018)	QQP	70.9	70.7	70.7LSTM+Attention	MRPC	809	80.6	80.7(Chen et al., 2017)	QQP	62.6	62.3	62.6LSTM+CNN	MRPC	8170	78.2	81.0(He & Lin, 2016)	QQP	60.3	60.5	60.3We observe the relative improvements brought by training with the square loss vary with differentmodel architectures, and other than LSTM+CNN model on QQP dataset, all architectures trained4Published as a conference paper at ICLR 2021with the square loss have better/equal accuracy and F1 score. The performance of loss functionsalso varies with data size, especially for MRPC, which is a relatively small dataset, all model archi-tectures trained with the square loss gives significantly better results than the cross-entropy.
Table 4: ASR task statistics and descriptionsCorpus	| Train|	| Test|	#classes	Metric	DomainTIMIT	1.15M	54K	42	PER	3.2 hours (training set)(Garofolo et al., 1993)			27	CER	telephone EnglishWSJ	28.8M	252K	52*	WER	80 hours (training set)(Paul & Baker, 1992)				CER	read newspapersLibrispeech	36M	1M	1000*	WER	100 hours (training set)(Panayotov et al., 2015)				CER	audio booksThis is the number of classes used for training the acoustic model.
Table 5: ASR results, error rateModel	Task	train with square loss (%)	train with cross-entropy (%)	square loss w/ same epochs as CE (%)Attention+CTC	TIMIT (PER)	208	20.8	20.8(Kim et al., 2017)	TIMIT (CER)	32.5	33.4	32.5VGG+BLSTMP	WSJ (WER)	5.1	5.3	5.1(Moritz et al., 2019)	WSJ (CER)	2.4	2.5	2.4VGG+BLSTM	Librispeech (WER)	9.8	10.6	10.3(Moritz et al., 2019)	Librispeech (CER)	9.7	10.7	10.2We see that the square loss performs better (equal for TIMIT PER result) in all of our tasks. It isinteresting to observe that the performance advantage of the square loss reported in Table 5 increaseswith dataset size. In particular, the relative advantage of the square loss (9.3% relative improvementon CER, and 7.5% on WER, respectively) is largest for the biggest dataset, Librispeech. On WSJ,using the square loss has 〜4% relative improvement on both CER and WER, while the resultson TIMIT for the square loss and cross-entropy are very similar. The question of whether thisdependence between the data size and the relative advantage of the square loss over cross-entropy isa coincidence or a recurring pattern requires further investigation.
Table 6: Vision task statistics and descriptionsCorpus	| Train	|Test|	#classes	Metric	DomainMNIST (LeCun et al., 1998)	-^60K^^	10K	10	acc.	28 × 28CIFAR-10 (Krizhevsky & Hinton, 2009)	50K	10K	10	acc.	32 × 32ImageNet (Russakovsky et al., 2015)	〜1.28M	50K3	1000	acc. Top-5 acc.	224 × 224As in Table 7, on MNIST and CIFAR-10, training with the square loss and the cross-entropy havecomparable accuracy. On much larger ImageNet, with ResNet-50 architecture, the accuracy andTop-5 accuracy of using the square loss are comparable with the ones got by using the cross-entropyloss. While with EfficientNet, using the cross-entropy shows better results. The performance ofdifferent loss functions varies among different architectures. On MNIST and CIFAR-10, we useexactly the same hyper-parameters well-selected for the cross-entropy loss. For ImageNet, we adjustthe learning rate and add a simple rescaling scheme (see Section 5), all other hyper-parameters arethe same as for the cross-entropy loss. The performance of using the square loss can improve withmore hyper-parameter tuning.
Table 7: Vision results, accuracyModel	Task	train with square loss (%)	train with cross-entropy (%)	square loss w/ same epochs as CE (%)TCNN (Bai et al., 2018)	MNIST (acc.)	97.7	97.7	97.7W-Resnet (Zagoruyko & Komodakis, 2016)	CIFAR-10 (acc.)	95.9	96.3	95.9ReSNet-50	ImageNet (acc.)	76.2	76.1	76.0(He et al.,2016)	ImageNet (Top-5 acc.)	93.0	93.0	92.9EfiCientNet	ImageNet (acc.)	74.6	77.0	74.6(Tan & Le, 2019)	ImageNet (Top-5 acc.)	92.7	93.3	92.7For all three datasets, training with the square loss converges as fast as training with the cross-entropy, and our two experimental protocols for the square loss result in same accuracy performance(except ImageNet with ResNet-50 model).
Table 8: Standard deviation of test accu- racy/error. Smaller number is bolded.			Model	Dataset	Square loss	CE	MRPC	0.484	0.766BERT	SST-2	0.279	0.173	QNLI	0.241	0.205	QQP	0.045	0.063LSTM +Attention	MRPC	0.484	0.786	QNLI QQP	0.210 0.566	0.371 0.352LSTM +CNN	MRPC	0.322	0.383	QNLI	0.173	0.286	QQP	0.458	0.161Attention	TIMIT (PER)	0.508	0.249+CTC	TIMIT (CER)	0.361	0.873VGG+	WSJ(WER)	0.184	0.249BLSTMP	WSJ(CER)	0.077	0.118VGG+	Libri (WER)	0.126	0.257BLSTM	Libri (CER)	0.148	0.316TCNN	MNIST	0.161	0.173W-ResNet	CIFARA0^^	0.184	0.481ResNet-50	I-Net (ToP-I)	0.032	0.045
Table 9: Rescaling parametersDataset	#classes	k	MMRPC	-^2	1	1SST-2	2	1	1QNLI	2	1	1QQP	2	1	1TIMIT (CER)	27	1	1TIMIT (WER)	42	1	15WSJ	52	1	15Librispeech	1000	15	30MNIST	10	1	1CIFAR-10	10	1	1ImageNet	1000	15	30of output labels and y = [0, . . . , |{1z}, 0, . . . , 0] is the corresponding one-hot encoding vector of thelabel c. We denote our model by f : Rd → RC .
Table 10: Hyper-parameters for NLP tasksModel	Task	Batch size	max_seq length	Learning rate w/		Epochs training w/					square loss	CE	square loss	CEBERT	MRPC	32	128	5e-5	-2e-Γ	5	-3-	SST-2	32	128	2e-5		3	-3-	QNLI	32	128	2e-5		3	-3-	QQP	32	128	2e-5	-2e-F	3	-3-LSTM+Attention	MRPC	64	80	2e-4	1e-4	25	20	QNLI	32	SentJlen*	1e-4	1e-4	20	20	QQP	64	120	1e-4	1e-4	30	30LSTM+CNN	MRPC	64	80	2e-4	1e-4	20	20 20	QNLI	32	sentJen*	8e-5	1e-4	20		QQP	32	120	1e-3	ɪɪ"	20	20The max sequence length equals the max sentence length of the training set.
Table 11: Hyper-parameters for ASR tasksModel	Task	Hyper-parameters	Epochs training w/				square loss	CEAttention+CTC	-TIMIT-	Conftrainyamp	20	20VGG+BLSTMP	WSJ*	一	conf/tuning/train_rnn.yaml	15	15VGG+BLSTM	LibriSPeech	conf/tuning/train_rnn.yaml^	30	20* For WSJ, we use the language model given by https://drive.google.com/open?id=1Az-4H25uwnEFa4lENc-EKiPaWXaijcJp.	\ We set mtlalpha=0.3,batch-size=30. ♦ We set elayers=4, as we use 100 hours training data.
Table 12: Hyper-parameters for vision tasksModel	Task	Hyper-parameters	Epochs training w/				square loss	CETCNN 一	MNIST∖	the default in (Bai et al., 2018)	20	20Wide-ResNet	CIFAR-10	the default in W-ResNet, except wide-factor=20	200	200ResNet-50	ImageNet	the default in ResNet, for square loss, learning rate=0.3	168885*	112590*EfficientNet	ImageNet	the default in EfficientNet-B0 Of(Tan &Le, 2019)	218949*	218949*\ We are doing the permuted MNIST task as in Bai et al. (2018).
Table 13: NLP results on validation set, accuracyModel	Task	train with square loss (%)	train with cross-entropy (%)	square loss w/ same epochs as CE (%)	MRPC	85.3	85.0	85.3BERT	SST-2	91.2	91.5	91.2(Devlin et al., 2018)	QNLI	90.8	90.7	90.8	QQP	90.8	90.7	90.6LSTM+Attention	MRPC	765	74 8	75 3				(Chen et al., 2017)	QNLI QQP	79.7 86.0	79.7 85.5	79.7 86.0LSTM+CNN	MRPC	76.0	73.3	76.0				(He & Lin, 2016)	QNLI	76.8	76.8	76.8	QQP	84.0	85.3	84.013Published as a conference paper at ICLR 2021Table 14: NLP results on Validation set, F1 ScoresModel	Task	train with square loss (%)	train with cross-entropy (%)	square loss w/ same epochs as CE (%)BERT	MRPC	89.5	89.6	89.5(Devlin et al., 2018)	QQP	87.5	87.4	87.4LSTM+Attention	MRPC	83.7	83.3	83.5
Table 14: NLP results on Validation set, F1 ScoresModel	Task	train with square loss (%)	train with cross-entropy (%)	square loss w/ same epochs as CE (%)BERT	MRPC	89.5	89.6	89.5(Devlin et al., 2018)	QQP	87.5	87.4	87.4LSTM+Attention	MRPC	83.7	83.3	83.5(Chen et al., 2017)	QQP	82.1	81.7	82.1LSTM+CNN	MRPC	82.6	81.4	82.6(He & Lin, 2016)	QQP	77.4	80.2	77.4We report the results for validation set of NLP tasks in Table 13 for accuracy and Table 14 for F1scores.
Table 15: ASR results on validation set, error rateModel	Task	train with square loss (%)	train with cross-entropy (%)	square loss w/ same epochs as CE (%)Attention+CTC	TIMIT (PER)	18∏	18.3	18.1(Kim et al., 2017)	TIMIT (CER)	30.4	31.4	30.4VGG+BLSTMP	WSJ (WER)	8.5	8.8	8.5(Moritz et al., 2019)	WSJ (CER)	3.9	4.0	3.9VGG+BLSTM	Librispeech (WER)	9.3	10.7	9.9(Moritz et al., 2019)	Librispeech (CER)	9.4	11.1	10.2We report the training result for NLP tasks in Table 16 for accuracy and F1 score in Table 17. Thetraining results for ASR tasks and vision tasks are in Table 18 and Table 19, respectively.
Table 16: NLP results on training and test set, accuracyModel	Task	train with square loss (%)		train with cross-entropy (%)		square loss w/ same epochs as CE (%)			Train	Test	Train	Test	Train	Test	MRPC	99.7	83.8	99.9	82.1	99.6	83.6BERT	SST-2	98.6	94.0	99.2	93.9	98.6	93.9(Devlin et al., 2018)	QNLI	98.0	90.6	97.5	90.6	98.0	90.6	QQP	96.2	88.9	98.0	88.9	96.2	88.8LSTM+Attention (Chen et al., 2017)	MRPC	94.6	71.7	84.9	70.9	93.2	71.5	QNLI QQP	87.7 93.7	79.3 83.4	90.8 91.5	79.0 83.1	87.7 93.7	79.3 83.4LSTM+CNN (He & Lin, 2016)	MRPC	98.3	73.2	92.5	69.4	98.3	72.5	QNLI QQP	92.8 91.3	76.0 84.3	90.7 95.7	76.0 84.4	92.8 91.3	76.0 84.3Table 17: NLP results on training and test set, F1 scorestrain with	train with square loss w/ sameModel	Task square loss (%) cross-entropy (%) epochs as CE (%)	Train		Test	Train	Test	Train	TestBERT	MRPC	99.8	88.1	99.9	86.7	99.7	88.0(Devlin et al., 2018)	QQP	94.5	70.9	97.2	70.7	94.5	70.7LSTM+Attention	MRPC	96.1	80.9	89.5	80.6	94.7	80.7(Chen et al., 2017)	QQP	91.9	62.6	89.2	62.3	91.9	62.6LSTM+CNN	MRPC	98.8	81.0	94.5	78.2	98.8	81.0
Table 17: NLP results on training and test set, F1 scorestrain with	train with square loss w/ sameModel	Task square loss (%) cross-entropy (%) epochs as CE (%)	Train		Test	Train	Test	Train	TestBERT	MRPC	99.8	88.1	99.9	86.7	99.7	88.0(Devlin et al., 2018)	QQP	94.5	70.9	97.2	70.7	94.5	70.7LSTM+Attention	MRPC	96.1	80.9	89.5	80.6	94.7	80.7(Chen et al., 2017)	QQP	91.9	62.6	89.2	62.3	91.9	62.6LSTM+CNN	MRPC	98.8	81.0	94.5	78.2	98.8	81.0(He & Lin, 2016)	QQP	88.0	60.3	94.2	60.5	88.0	60.314Published as a conference paper at ICLR 2021Table 18: ASR results on training and test set, error rateModel	Task	train with square loss (%)		train with cross-entropy (%)		square loss w/ same epochs as CE (%)			Train	Test	Train	Test	Train	TestAttention+CTC	TIMIT (PER)	-09-	20.8	4.8	20.8	0.9	20.8(Kim et al., 2017)	TIMIT (CER)	4.5	32.5	11.6	33.4	4.5	32.5VGG+BLSTMP	WSJ(WER)*	0.7	5.1	0.3	5.3	0.7	5.1(Moritz et al., 2019)	WSJ (CER)*	0.3	2.4	0.1	2.5	0.3	2.4VGG+BLSTM	Librispeech (WER)*	0.8	9.8	0.4	10.6	0.8	10.3
Table 18: ASR results on training and test set, error rateModel	Task	train with square loss (%)		train with cross-entropy (%)		square loss w/ same epochs as CE (%)			Train	Test	Train	Test	Train	TestAttention+CTC	TIMIT (PER)	-09-	20.8	4.8	20.8	0.9	20.8(Kim et al., 2017)	TIMIT (CER)	4.5	32.5	11.6	33.4	4.5	32.5VGG+BLSTMP	WSJ(WER)*	0.7	5.1	0.3	5.3	0.7	5.1(Moritz et al., 2019)	WSJ (CER)*	0.3	2.4	0.1	2.5	0.3	2.4VGG+BLSTM	Librispeech (WER)*	0.8	9.8	0.4	10.6	0.8	10.3(Moritz et al., 2019)	Librispeech (CER)*	0.6	9.7	0.3	10.7	0.6	10.2* For WSJ and Librispeech, we take 10% of the training set for the evaluation of the training error rate.
Table 19: Vision results on training and test set, accuracytrain with	train with square loss w/ sameModel	Task	square loss (%) cross-entropy (%) epochs as CE (%)	Train		Test	Train	Test	Train	TestTCNN (Bai et al., 2018)	MNIST (acc.)	98.3	97.7	99.5	97.7	98.3	97.7W-Resnet (Zagoruyko & Komodakis, 2016)	CIFAR-10 (acc.)	100.0	95.9	100.0	96.3	100.0	95.9ResNet-50	ImageNet (acc.)	77.7	76.2	80.5	76.1	77.7	76.0(He et al.,2016)	ImageNet (Top-5 acc.)	93.2	93.0	93.4	93.0	93.2	92.9EfiCientNet	ImageNet (acc.)	75.1	74.6	81.4	77.0	75.1	74.6(Tan & Le, 2019)	ImageNet (Top-5 acc.)	93.0	92.7	94.0	93.3	93.0	92.7D Our results compared with the original workWe list our results for the models trained with the cross-entropy (CE) loss and compare them to theresults reported in the literature or the toolkits in Table 20. As we observe, our results are comparableto the original reported results.
Table 20: Training with the cross-entropy loss, our results and the reported onesModel	Task	Our CE result	CE result in the literature	MRPC (acc./F1)	-85.0/89.6-	85.29/89.47 (Wolf et al., 2019)BERT*	SST-2 (acc.)	91.5	91.97 (Wolf et al., 2019)	QNLI (acc.)	90.7	87.46 (Wolf et al., 2019)	QQP (acc./F1)	90.7/87.4	88.40/84.31 (Wolf et al., 2019)LSTM+Attention			N/ALSTM+CNN			N/AAttention+CTC	TIMIT (PER)	207	20.5 (Watanabe et al., 2018)	TIMIT (CER)	32.7	33.7 (Watanabe et al., 2018)VGG+BLSTMP	WSJ (WER)	54	5.3 (Watanabe et al., 2018)	WSJ (CER)	2.6	2.4 (Watanabe et al., 2018)VGG+BLSTM	Librispeech (WER)	10.8	N/A	Librispeech (CER)	11.0	N/ATCNN	MNIST (acc.)	98.0	97.2 (Bai et al., 2018)Wide-ResNet	CIFAR-10 (acc.)	96.5	96.11 (Zagoruyko & Komodakis, 2016)ResNet-50	ImageNet (acc./Top-5 acc.)	-76.1/93.0-	76.0/93.0 (Tan & Le, 2019)EfficientNet	ImageNet (acc./Top-5 acc.)	77.2/93.4 —	77.3/93.5 (Tan & Le, 2019)The implementation in (Wolf et al., 2019) is using bert-base-uncased model, we are using bert-base-cased,which will result in a little difference. Also, as they didn’t give test set results, here for BERT, we give the
Table 21: Regularization term for each taskModel	Task	dropout*	batch norm	Regularization TermBERT	MRPC/SST-2/QNLI/QQP	01	N	0LSTM+Attention	MRPC/QNLI/QQP	0.5	N	0LSTM+CNN	MRPC/QNLI/QQP	0.0	N	0Attention+CTC	TIMIT	0.0	N	0VGG+BLSTMP	WSJ	0.0	N	label smoothing basedVGG+BLSTM	Librispeech	0.0	N	0TCN	MNIST	0.05	N	0Wide-ResNet	CIFAR-10	0.0	N	0ResNet-50	ImageNet	0.0	Y	10-4 Pn=1 W2EfficientNet	ImageNet	0.0	Y	10-5 Pn=I w2* For dropout, 0.0 means have not apply dropout.
