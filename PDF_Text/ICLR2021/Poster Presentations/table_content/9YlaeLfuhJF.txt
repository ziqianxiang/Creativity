Table 1: Comparison of metrics and losses for classifier training. Here Pz and Pz are marginal distributions of(x, y) for the subgroup z, and αθ (x, y) = I[(arg max fθ(x)) = y] denotes correct prediction on an example.
Table 2: A comparison between CAMEL and other methods on 3 benchmark datasets. Evaluation metricsinclude robust & aggregate accuracy and the subgroup performance gap, calculated on the test set. Results areaveraged over 3 trials (one standard deviation indicated in parentheses).
Table 3: Estimated MI between predictions andsubgroups computed on MNIST-Correlation.
Table 4: Ablation analysis (Section 4.2.1) that varies the consistency penalty coefficient λ. For brevity, wereport the maximum subgroup performance gap over all classes.
Table 5: MNIST-Correlation results whentraining to predict Z labels and testing on Ylabels. Test robust accuracy bolded.
Table 6: Comparison on ISIC. Aver-age of 3 trials (one standard deviationindicated in parentheses).
Table 8: Number of training, validation and test examples in each dataset.
Table 9: Ablation analysis (Section 4.2.1) that varies the consistency penalty coefficient λ on the MNIST-Correlation dataset. For brevity, we report the maximum subgroup performance gap over all classes.
Table 10: Performance on the ISIC validation set.
Table 11: Comparisons to GAN Baselines on Waterbirds and CelebA-Undersampled.
Table 12: The values of the best hyperparameters found for each dataset and method.
