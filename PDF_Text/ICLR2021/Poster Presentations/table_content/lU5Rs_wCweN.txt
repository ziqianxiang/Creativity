Table 1: Average GLUE score of all methods onthe dev set when the pre-training finished, i.e.,at 1e6 iterations. Results of GPT, BERT andELECTRA are from Clark et al. (2019). The re-sult of SpanBERT is obtained by fine-tuning thereleased checkpoint from Joshi et al. (2019). Wealso reproduce BERT and ELECTRA in our sys-tem for fair comparison. We report their resultsas BERT(ours) and ELECTRA(ours).
Table 2: Performance of different models on downstream tasks. Results show that TNF outperformsbackbone methods on the majority of individual tasks. We also list the performance of two variantsof TNF. Both of them leverage the node dictionary during fine-tuning. Specifically, TNF-F uses fixednote dictionary and TNF-U updates the note dictionary as in pre-training. Both models outperformsthe baseline model while perform slightly worse than TNF.
Table 3: Hyper-parameters for the pre-training and fine-tuning on all language pre-training methods,include both backbone methods and TNFs.
Table 4: Experimental results on the sensitivity of BERT-TNF’s hyper-parameter k, λ and γ.
