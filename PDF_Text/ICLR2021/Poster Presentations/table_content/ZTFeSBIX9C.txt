Table 1: All samples that contain the source word “纽马基特” in raw and distilled training corpora, whichare different in target sides (RAW-TGT vs. KD-TGT).
Table 2: Results of different metrics on the MaskT model trained on different datasets. “KD (X)”denotes the distilled data produced by the AT model with X setting. “CoD” denotes the complexityof data metric proposed by Zhou et al. (2020), and “AoLC” is our proposed metric to evaluate theaccuracy of lexical choice in NAT models.
Table 3: Ablation Study on raw data priors across different language pairs using the MaskT Model.
Table 4: Comparison with previous work on WMT14 En-De and WMT16 Ro-En datasets. “Iter.”column indicate the average number of refined iterations. "t" indicates statistically significantdifference (p < 0.05) from baselines according to the statistical significance test (Collins et al., 2005).
Table 7: Improvement of our approach over theMaskT+KD model on AoLC.
Table 8: Ratio of low-frequency target words inthe MaskT model generated translations.
Table 5: Performance of several data manip-ulation strategies on En-De dataset. Base-line is the MaskT+KD model and Ours isour proposed approach.
Table 9: AoLC and Ratio of different prior schemes onLow-Frequency Tokens (“LFT”). We list the performanceson different linguistic roles, i.e. content words and func-tion words. Note that Ratio of LFT means the ratio of lowmore on the understanding and genera- frequency tokens in generated translation. “N/A” meanstion of content tokens, while SDD brings MaskT+KD baseline.
Table 10: Different teachers on the En-De dataset.
Table 11: Results of AT models on En-De when knowledge distillation is used. LFT denotes low-frequency tokens and Ratio of LFT means the ratio of low-frequency tokens in generated translation.
