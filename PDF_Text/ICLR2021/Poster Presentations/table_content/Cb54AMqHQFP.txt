Table 1: Results (accuracy) for soft filters pruning (He et al., 2018). The “w/o FT” column demonstratethe performance of networks follow original work (do not retrain after final prune). “FT” columnindicates results of networks when fine-tuning for 200 more epochs. “CLR-x” columns show accuracyof networks after retraining with CLR for x more epochs.“%PF” indicates number of pruned filters.
Table 2: Results (accuracy) for 'ι-norm based filter pruning (Li et al., 2016). Configurations ofModeland Pruned Model are both from the original paper (Li et al., 2016). The results of “Scratch-E” and“Scratch-B” on ImageNet are taken directly from work of Liu et al. (2019). Top- and second-rankedresults are highlighted in bold blue and blue.
Table 3: Comparing the performance of pruned network via PFEC + CLR and GAL on ImageNet.
Table 4: Comparing the performance of pruned network via PFEC + CLR and HRankPlus onImageNet. The results of HRankPlus are taken directly from official Github repository.
Table 5: Results of networks when applying random pruning and methodically pruning algorithms.
Table 6: Training configuration for unpruned models. To train CIFAR-10, we use Nesterov SGD withβ = 0.9, batch size 64, weight decay 0.0001 for 160 epochs. To train ImageNet, we use NesterovSGD with β = 0.9, batch size 32, weight decay 0.0001 for 90 epochs.
Table 7: Results (accuracy) for HRank ilters pruning (Lin et al., 2020a) on CIFAR-10. “Pruned +FTf " is the model pruned from the large model with original fine-tuning scheme that are reported.
Table 8: Results (accuracy) for Taylor filters pruning (Molchanov et al., 2019) on ImageNet. Allcolumns have same meaning with corresponding columns in Table 7.
Table 9: Results of ResNet-56 and ResNet-110 on CIFAR-10. The performance of other pruningalgorithms are taken directly from original papers.
Table 10: Comparing the performance of pruned network via PFEC + CLR and Taylor Pruning onImageNet. The results of Taylor Pruning are taken directly from original papers.
Table 11: Comparing the performance of pruned network via PFEC + CLR and DCP on ImageNet.
Table 12: Comparing the performance of pruned network via PFEC + CLR and Provable FiltersPruning (PFP) on ImageNet. The results of PFP are taken directly from original paper (Liebenweinet al., 2020).
Table 13: Training configurations of original pruning methods and random pruning.
Table 14: Performance of ResNet-56 pruned from models trained with CLR and conventional step-wise learning rate schedule on CIFAR-10. “Epochs” column indicates the number of epochs forretraining trimmed networks. “Schedule” colum indicates the learning rate schedule used for trainingbaseline (unpruned) networks. The best and second best methods are highlighted in bold blue andblue respectively.
