Table 1: Face part independence across models (lower means more independent), measured as the variationoutside of the replaced part, computed over difference replacements.
Table 2: Table of masked L1 and LPIPS-Alexnet reconstruction errors for encoder networks trained with alllosses, no latent loss, and no perceptual loss.
Table 3: Quantitative comparison of automated collaging and latent regression variations. The latent regressorand generator pulls the unrealistic composite images closer to the real manifold, yielding high density andcoverage and lower FID on output, compared to the collaged inputs and Poisson blending. For ProGAN models,we use PyTorch models from Bau et al. (2019); for StyleGAN models, we use a Pytorch conversion of theTensorflow models from Karras et al. (2019a).
Table 4: On a set of 200 input collages constructed from random image parts, we compare compositionalproperties of several image reconstruction approaches including methods based on autoencoder-style networksthat do not leverage a pretrained GAN, methods based on optimization on the latent code of a pretrained GAN,and methods based on encoder networks to regress the latent code. Autoencoder-based and optimization-basedmethods can achieve lower reconstruction error (L1; lower is better), but are less realistic (Density; higher isbetter). Another tradeoff is optimization time, as feed-forward methods are orders of magnitude faster thanoptimization-based approaches. We also report FID (lower is better), but our sample size is limited due to thelatency of optimization-based approaches.
Table 5: We construct 200 input collages from random face image parts, and compare to several imagereconstruction methods. Again, we find a tradeoff between better reconstruction (low L1) and better realism(higher Density, lower FID), due to the imperfect nature of the input collages. There is no ground truth imagethat perfectly reconstructs the input yet is realistic.
