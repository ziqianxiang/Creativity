Table 1: Relative parameter and FLOP counts (%) of the auxiliary networks over the ResNet-101 onImageNet. Our Aux i heads are much more efficient than the previous MLP-SR (Belilovsky et al.,2020) and PredSim (N0kland & Eidnes, 2019).
Table 2: Error rates (%) on CIFAR-10 and Tiny-ImageNet with backprop, PredSim (N0kland & Ei-dnes, 2019), DGL (Belilovsky et al., 2020) and SEDONA (ours). For PredSim, DGL and SEDONA,we set K = 4. For CIFAR-10, we use 0.25× width of the original networks.
Table 3: Error rates (%) on ImageNet with backprop, DGL and SEDONA (ours). Aux. Ens. denotesthe ensemble of last two blocks’ auxiliary networks.
Table 4: Comparison of error rates (%) on CIFAR-10 between (learned α, learned β), (random α,learned β) and (learned α, random β) when K = 4.
Table 5: Error rates (%) on ImageNet with setting of Belilovsky et al. (2020) when K = 2. Theresults of Backprop and DGL are cited from Belilovsky et al. (2020).
Table 6: Error rates (%) on CIFAR-10 with Backprop, SEDONA (Cont.) and SEDONA (K = 4).
Table 7: Error rates (%) on train and validation splits of ImageNet with Backprop, DGL and SE-DONA. For DGL and SEDONA, we set K = 4.
Table 8: Performances and search costs of SEDONA after removing some key components one byone. Without pretraining, we 8K additional iterations for convergence.
Table 9: Architecture of Aux 4 on ImageNet. For Aux i ≤ 4, we remove final i invertied residualblocks. Nc denotes the number of channels at the convolutional layer on which the auxiliary head isaugmented.
Table 10: ResNet-152: 16 layers with the highest learned values of a，) and their correspondingauxiliary headers.
Table 11: ResNet-101: auxiliary headers.		16 layers with the highest learned values of ɑ									(1l) and their corresponding				Layer Index ∣ 7	11	12	22	25	27	28	29	19	32	31	30	26	24	23	20Aux. Header ∣ 4	4	4	4	4	4	4	4	4	3	2	4	4	4	4	4Table 12: ResNet-50: 15 layers with the highest learned values of a，) and their correspondingauxiliary header.
Table 12: ResNet-50: 15 layers with the highest learned values of a，) and their correspondingauxiliary header.
Table 13: VGG-19: 15 layers with the highest learned values of αf) and their corresponding auxil-iary header.
Table 14: Search costs of SEDONA.
Table 15: Wall-clock training times (hours) of Backprop, DGL and SEDONA (ours) on ImageNet .
