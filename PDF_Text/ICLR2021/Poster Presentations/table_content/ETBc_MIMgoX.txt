Table 1: Comparison of Mean Extrinsic Reward at the end of training (averaging over a batch ofepisodes as in IMPALA). Each entry shows the result of the best observation configuration, for eachbaseline, from Tables 2-5 of Appendix A.
Table 2: Fully observed intrinsic reward, fully observed policy.
Table 3: Partially observed intrinsic reward, fully observed policy.
Table 4: Fully observed intrinsic reward, partially observed policy.
Table 5: Partially observed intrinsic reward, partially observed policy.
Table 6: Ablations and Alternatives. Number of steps (in millions) for models to learn to reach itsfinal level of reward in the different environments (0 means the model did not learn to get any extrinsicreward). Full Model is the main algorithm described above. NoExtrinsic does not provide anyextrinsic reward to the teacher. NoEnvChange removes the reward for selecting goals that changeas a result of episode resets. withNovelty adds a novelty bonus that decreases depending on thenumber of times an object has been successfully proposed. Gaus sian and Linear-Exponentialexplore alternative reward functions for the teacher.
