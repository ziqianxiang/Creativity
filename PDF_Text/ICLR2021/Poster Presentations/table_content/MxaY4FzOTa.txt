Table 1: Comparison on ImageNet fordifferent number of experts. All modelshave the same number of BOPs, includ-ing Martinez et al. (2020).
Table 2: Comparison on ImageNet for differentnumber of experts and expansion rates. All mod-els have the same number of BOPs, includ-ing Martinez et al. (2020).
Table 3: Comparison on ImageNet betweenstage I models with different block arrange-ments.
Table 4: Comparison on ImageNet between a few structures explored. All methods have approxi-mately the same number of FLOPS: 1.1 × 108. Ni: number of blocks in stage i, E: width expansionratio, Gi : group size of convs at stage i. * - denotes model trained using AT+KD (Martinez et al.,2020).
Table 5: Comparison with state-of-the-art binary models on ImageNet. The upper section includesmodels that increase the network size/capacity (last column shows the capacity scaling), while themiddle one binary NAS methods. * - denotes models trained using AT+KD (Martinez et al., 2020).
Table 6: Detailed network definitions for each of the constellations presented in Fig. 2. The locationin each architecture is numbered from left to right (models with more BOPs are located towards theright).
Table 9: Comparison on ImageNet be-tWeen various types of doWnsampling lay-ers (Stage I models). All decompositionsreduce the complexity by 4x.
Table 7: Comparison with state-of-the-art binary models on ImageNet, including against methodsthat use low-bit quantization (upper section) and ones that increase the network size/capacity (secondsection). The third section compares against binary NAS methods. Last column shows the capacityscaling used, while * - denotes models trained using AT+KD (Martinez et al., 2020).去-denotesours with an improved training scheme, see Section A.6.
Table 8: Comparison with state-of-the-art binary models on CIFAR100. Last column shows thecapacity scaling used, while * - denotes models trained using AT+KD (Martinez et al., 2020).
Table 10: Impact of temperature τ on ac-curacy (Stage I models) on ImageNet.
Table 11: Comparison with state-of-the-art binary models on ImageNet, including against methodsthat use low-bit quantization (upper section) and ones that increase the network size/capacity (secondsection). The third section compares against binary NAS methods. Last column shows the capacityscaling used, while * - denotes our model trained using AT+KD (Martinez et al., 2020). ∣ - denotesours with an improved training scheme, see Section A.6.
Table 12: Impact of the teacher used on the final accuracy of the model on ImageNet.
