Table 1: Predictive Error (PE) and calibration errors (ECE, MCE) for the datasets BIH and IMDB(lower is better for all metrics). ST-τ offers the best and reliable trade-off between predictive errorand calibration errors. Furthermore, it does not require more parameters as BBB (double) or DeepEnsemble (order of magnitude more) nor does a hyperparameter has to be tuned as in VD. Stochasticpredictions are averaged across 10 independent runs and their variance is reported. Best and secondbest results are marked in bold and underlined (PE: bold models are significantly different at levelp ≤ 0.005). An ablation experiment with post-hoc temperature scaling is in Appendix B.3.1.
Table 2: Results (averaging on 10 runs for VD, BBB, ST-τ . Ensembles are based on 10 mod-els) of the out-of-distribution (OOD) detection with max-probability based (top), variance of max-probability based (middle) and ensembles (bottom). ST-τ exhibits very competitive performance.
Table 3: Overview of used datasets, if applicable, numbers are rounded down.
Table 4: Overview of the different hyperparameters for the different datasets. Validation rate indi-cates after how many updates validation is performed.
Table 5: Same as table1 but with post-hoc temperature scaling. Among the models (Ensemble,BBB, VD) that can estimate uncertainty, ST-τ is very competitive, i.e., the second best on both theBIH and the IMDB datsaet. LSTM is not able to provide any uncertainty information. PredictiveError (PE) and calibration errors (ECE, MCE) for the various RNNs on the datasets BIH and IMDB(lower is better for all metrics). ST-τ offers the best and reliable trade-off between predictive errorand calibration errors. Furthermore, it does not require double the parameters as BBB nor does a hy-perparameter have to be tuned as in VD. Stochastic predictions are averaged across 10 independentruns and their variance is reported. For VD, we report the best dropout probability on the valida-tion set. Best and the second best results are marked in bold and underlined (PE: bold models aresignificantly different at level p ≤ 0.005 to non-bold models).
