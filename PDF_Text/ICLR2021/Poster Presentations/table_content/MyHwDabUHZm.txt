Table 1: Classification accuracy on CIFAR-10 and CIFAR-100 for all label representations. Speechlabels, shuffled speech labels, and composition of Gaussian labels all achieve comparable accuracieswith categorical labels. Constant matrix labels perform slightly worse than the others.
Table 2: Different basic statistics of all types of label representations. Labels that encourage morerobust and effective feature learning also have higher entropy than other label forms.
Table 3: Total number of parameters of the category and high-dimensional models for CIFAR-10datasetA.5 DatasetTo demonstrate the effectiveness of our proposed method, we evaluate our models on the CIFAR-10 and CIFAR-100 datasets Krizhevsky (2009). For each dataset, we train different models onthe same training set and evaluate the models on the same validation set using the same randomseeds for fair comparisons. To preprocess the training images, we randomly crop them with apadding size 4 and perform random horizontal flips. All CIFAR images are normalized with mean(0.4914, 0.4822, 0.4465) and standard deviation (0.2023, 0.1994, 0.2010) of the training set.
Table 4: Total number of parameters of the category and high-dimensional models for CIFAR-100datasetLayer	Input	Output	Kernel	Stride	PaddingDense	Ie out	64	-	-	-ConvTranspose 2D	64 X 1 X 1	64 X 4 X 4	4X4	1X1	0ConvTranspose 2D	64 X 4 X 4	32 X 8 X 8	4X4	2X2	1X1ConvTranspose 2D	32 X 8 X 8	16 X 16 X 16	4X4	2X2	1X1ConvTranspose 2D	16X16X16	8 X 32 X 32	4X4	2X2	1X1ConvTranspose 2D	8 X 32 X 32	1 X 64 X 64	4X4	2X2	1X1Table 5: The architecture of the high-dimensional label decoder Ild . The input dimension of thefirst dense layer is the dimension of the output of the image encoder Ie . The output of the lastConvTranspose2d layer is the target label.
Table 5: The architecture of the high-dimensional label decoder Ild . The input dimension of thefirst dense layer is the dimension of the output of the image encoder Ie . The output of the lastConvTranspose2d layer is the target label.
