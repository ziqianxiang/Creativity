Table 1: Numerical values of results presented in fig. 1. The ‘Rel’column shows the error normalised by the error of the unmodifieddataset. The error on the test-data deteriorates worse than on the trainingdata, not only in absolute, but also relativ terms.
Table 2: Hyper-parameters used inthe supervised learning experiment onCIFAR-10Hyper-parameter ValueSGD: Learning rate 3 × 10-4SGD: Momentum 0.9SGD: Weight decay 5 × 10-4βsIE∙sH jο AOBJn84Figure 6: Left: Same results as in fig. 5 (middle), but with the fraction of correct data points f on the x-Axis.
Table 3: Hyper-parameters used for MultiroomHyper-parameter	ValuePPO: λEntropy Loss	0.01PPO: λTD	0.5PPO: Clip	0.2PPO Epochs	4PPO Minibatch Size	2048Parallel Environments	32Frames per Env per Update	256λGAE	0.95γ	0.99Adam: Learning rate	7 × 10-4Adam:	1 × 10-5Table 4: Hyper-parameters used for ProcGenHyperparameter	ValuePPO: λEntropy Loss	0.01PPO: λTD	0.5PPO: Clip	0.2PPO Epochs	3PPO Nr. Minibatches	8
Table 4: Hyper-parameters used for ProcGenHyperparameter	ValuePPO: λEntropy Loss	0.01PPO: λTD	0.5PPO: Clip	0.2PPO Epochs	3PPO Nr. Minibatches	8Parallel Environments	64Frames per Env per Update	256λGAE	0.95γ	0.999Adam: Learning rate	5 × 10-4Adam:	1 × 10-5Adam: Weight decay	1 × 10-4Figure 8: Left: Boxoban example layout. The green agent needs to push (or pull) yellow boxes on the red targets,avoiding walls. Right: Additional Boxoban results showing consequences of choosing a wrong distillationlength, either too short or too long. Note that ITER Too short uses the same distillating length as the resultsin fig. 2, but continues with distillation past 0.5e9 steps.
