Table 1: Summary of the studied GNN models.
Table 2: Test set accuracies on MNIST superpixel datasetNode feature	MLP	GCN	GIN	GAT	CayleyNet	ChebNetNode degree	11.29±0.5	15.81±0.8	32.45±1.2	31.72±1.5	45.61±1.7	46.23±1.8Pixel value	12.11±0.5	11.35±1.1	64.96±3.9	62.61±2.9	88.41±2.1	91.10±1.9Both	25.10±1.2	52.98±3.1	75.23±4.1	82.73±2.1	90.31±2.3	92.08±2.2pixel values and the last one uses both information. Implementation details and hyperparametertuning can be found in Appendix L.
Table 3: Sum of squared errors. All models have roughly 30k trainable parameters.
Table 4: ChebNet’s sum of squared errors on band-pass tasks with respect to S kernels and L stackedlayers. All models have roughly 30k trainable parameters.
Table 5: Test set accuracy and binary cross entropy loss.
Table 6: Comparison of methods on the transductive learning problems using publicly defined train,validation and test sets. Results are on accuracy		CiteSeer	PubMedMethod	Cora		MLP	0.551	0.465	0.714ChebNet	0.812	0.698	0.744CayleyNet	0.819	0.701	0.751GCN	0.819	0.707	0.789GAT	0.830	0.725	0.790LowPassConv	0.827 ± 0.006	0.717 ± 0.005	0.794 ± 0.005there is an unlabeled and undirected edge between the corresponding nodes. Binary features on thenodes indicate the presence of specific keywords in the corresponding paper. The learning task is toattribute a class to each node (i.e., paper) of the graph using for training the graph itself and a verylimited number of labeled nodes. Labeled data ratios are 5.1%, 3.6% and 0.3% for Cora, CiteSeerand PubMed respectively. Since the connected node’s probability of being in the same class is high(0.83, 0.71, 0.79 for Cora, CiteSeer and PubMed respectively in Liu et al. (2020)), these graphs areclassified as assortative graphs. When the connected nodes are highly likely to be in the same class,label propagation based low-pass effected algorithms can give reasonable results.
Table 7: Summary of the datasets used in our experiments.
