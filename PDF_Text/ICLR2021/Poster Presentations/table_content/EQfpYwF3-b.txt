Table 1: In the table above, we present the compressed models with the best accuracy achieved forspecific compression rates (or intervals) of the embedding layer. We report their drop in accuracy,and the used values of k and j . Specifically, in each entry the “accuracy drop” is presented abovethe used k and j values. The last column is the average accuracy drop over all tested tasks. Observethat: (i) negative values presents improvements in the accuracy upon the non-compressed versionof the corresponding model, and (ii) the results in this table can be improved if we allow to use thebest model from higher compression rates also, e.g., in the task RTE on the network DistilBERT, weachieved 2.5 accuracy increase when we compressed 60% of the embedding layer, however, in thistable we did not add this result for the smaller compression rates of 20, 40 and 50.
Table 2: In the table above, we report the accuracy achieved by “ALBERT (base-v2)” model on thetasks from GLUE, and we compare them to the results achieved on another model of the same size(up to 0.18% increase) which we call “MESSI-ALBERT (base-v2)”. This model is exactly the sameas the original “ALBERT (base-v2)” model up to one change, where the original embedding layerof “ALBERT (base-v2)” (consists of 30k rows and 128 columns) is modified to the new suggestedMESSI architecture, with k = 7, and j = 125 according the pipeline at Section 2, and withoutfine-tuning. It can be seen by the table, that the new architecture achieved a better results.
Table 3: In the table above, we compare two approaches to cluster the rows of the input matrix A(and check how they fit in the MESSI pipeline), the first approach is projective clustering with `1error (PC-'1), and the second is the standard protective clustering with '2 error that We used in allthe other experiments.
