Table 1: Impact on performance of four typical quantization implementations for INT8 and INT4. Theconfiguration that resulted in best performing models for each dataset-model pair is bolded. Hyperparametersfor each experiment were fine-tuned independently. As expected, adding clipping does not change performancewith min/max but does with momentum. A major contribution of this work is identifying that seeminglyunimportant choices in quantization implementation cause dramatic changes in performance.
Table 2: This table is divided into three sets of rows with FP32 baselines at the top. We provide two baselines forINT8 and INT4: standard QAT and stochastic QAT (nQAT). Both are informed by the analysis in 4.1, with nQATachieving better performance in some cases. Models trained with Degree-Quant (DQ) are always comparable tobaselines, and usually substantially better, especially for INT4. DQ is a stable method which requires littletuning to obtain excellent results across a variety of architectures and datasets.
Table 3: Results for DQ-INT8 GIN modelsperform nearly as well as at FP32. For INT4,DQ offers a significant increase in accuracy.
Table 4: INT8 latency results run on a 22 core 2.1GHz IntelXeon Gold 6152 and, on a GTX 1080Ti GPU. Quantizationprovides large speedups on a variety of graphs for CPU andnon-negligible speedups with unoptimized INT8 GPU kernels.
Table 5: High level description of the architectures evaluated for citation networks (Cit), MNIST (M), CIFAR-10(C), ZINC (Z) and REDDIT-BINARY (R). We relied on Adam optimizer for all experiments. For all batchedexperiments, we used 128 batch-sizes. All GAT models used 8 attention heads. All GIN architectures used2-layer MLPs, except those for citation networks which used a single linear layer.
Table 6: Number of parameters for each of the evaluated architecturesFor QAT experiments, all elements of each network are quantized: inputs to each layer, the weights,the messages sent between nodes, the inputs to aggregation stage and its outputs and, the outputsof the update stage (which are the outputs of the GNN layer before activation). In this way, allintermediate tensors in GNNs are quantized with the exception of the attention mechanism in GAT;we do not quantize after the softmax calculation, due to the numerical precision required at this12Published as a conference paper at ICLR 2021stage. With the exception of Cora and Citeseer, the models evaluated in this work make use of BatchNormalization (Ioffe & Szegedy, 2015). For deployments of quantized models, Batch Normalizationlayers are often folded with the weights (Krishnamoorthi, 2018). This is to ensure the input to the nextlayer is within the expected [qmin, qmax] ranges. In this work, for both QAT baselines and QAT+DQ,we left BN layers unfolded but ensure the inputs and outputs were quantized to the appropriatenumber of bits (i.e. INT8 or INT4) before getting multiplied with the layer weights. We leave asfuture work proposing a BN folding mechanism applicable for GNNs and studying its impact fordeployments of quantized GNNs.
Table 7: Statistics for each dataset used in the paper. Some datasets are only referred to in fig. 1A.3 Quantization ImplementationsIn section 4.1 we analyse different readily available quantization implementations and how theyimpact in QAT results. First, vanilla STE, which is the reference STE (Bengio et al., 2013) that letsthe gradients pass unchanged; and gradient clipping (GC), which clips the gradients based on themaximum representable value for a given quantization level. Or in other words, GC limits gradientsif the tensorâ€™s magnitudes are outside the [qmin, qmax] range.
Table 8: Final test accuracies for FP32 andDQ-INT8 models whose validation losscurves are shown in fig. 10Figure 10: Validation loss curves for GIN models evaluatedon REDDIT-BINARY. Results averaged across 10-fold cross-validation. We show four DQ-INT8 experiments each with a differ-ent values for (pmin,pmax) and our FP32 baseline.
Table 9: Ablation study against the two elements of Degree-Quant (DQ). The first two rows of results areobtained with only the stochastic element of Degree-Quant enabled for INT8 and INT4. Percentile-basedquantization ranges are disabled in these experiments. The bottom row of results were obtained with noisyquantization (nQAT) at INT4 with the use of percentiles. DQ masking alone is often sufficient to achieveexcellent results, but the addition of percentile-based range tracking can be beneficial to increase stability. Wecan see that using nQAT with percentiles is not sufficient to achieve results of the quality DQ provides.
Table 10: INT8 latency results run on a 22 core 2.1GHz Intel Xeon Gold 6152 and, on a GTX 1080Ti GPU. Alllayers have 128 in/out features. For CIFAR-10 we used batch size of 1K graphs.
