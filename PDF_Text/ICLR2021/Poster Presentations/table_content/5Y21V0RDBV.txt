Table 1: Properties of previous scalable multimodal VAEs and our proposed model. Note that todeal with missing modalities, the MVAE requires sub-sampling of unimodal ELBOs, which yieldsan invalid bound on the joint log-likelihood (Wu & Goodman, 2019).
Table 2: Linear classification accuracy of latent representations for MNIST-SVHN-Text. We eval-uate all subsets of modalities Xk where the abbreviations of subsets are as follows: M: MNIST; S:SVHN; T: Text; M,S: MNIST and SVHN; M,T: MNIST and Text; S,T: SVHN and Text; M,S,T: all.
Table 3: Generation coherence for MNIST-SVHN-Text. For conditional generation, the letter abovethe horizontal line indicates the modality which is generated based on the subsets Xk below. Wereport the mean values over 5 runs. Standard deviations are included in Appendix C.3.
Table 4: Test set log-likelihoods on MNIST-SVHN-Text. We report the test set log-likelihoods of thejoint generative model conditioned on the variational posterior of subsets of modalities qφ(Z∣Xk).
Table 5: Classification and coherence results on the bimodal CelebA experiment. For latent rep-resentations and conditionally generated samples, we report the mean average precision over allattributes (I: Image; T: Text; Joint: I and T).
Table 6: Generation Coherence for MNIST-SVHN-Text. For every subtable, the modality abovethe wide horizontal line is generated based on the subsets below the same line—except for jointcoherence. The abbreviations of the different modalities are as follows: M:MNIST; S: SVHN; T:Text. Combinations thereof separated by commas result in the subsets consisting of the modalities.
Table 7: Comparison of objectives: Equation (5) and Definition 1. We report the test set log-likelihoods of the joint generative model conditioned on the variational posterior of subsets ofmodalities qφ(z∣Xk). (XM: MNIST; XS: SVHN; XT: Text; X = (XM, XS, XT)). For both ob-jectives we use β = 2.5Model	X	X|xM	X|xS	X|xT	X|xM, xS	X|xM , xT	X|xS, xTEQ. (5)	-1810	-1993	-1831	-2039	-1811	-2000	-1839MOPOE	-1815±12.4	-1990±4.4	-1858±13.2	-2024±1.2	-1819±13.4	-1986±2.5	-1848±11.518Published as a conference paper at ICLR 2021D POLYMNISTD. 1 DatasetFor the creation of the PolyMNIST dataset, we fuse each MNIST image with a random crop of size28x28 from the background image of the respective modality. In particular, we binarize the MNISTimage and invert the colors of the random crop at those locations where the binarized MNIST digitis visible. We use the following background images:1.	John Burkardt. Licensed under GNU LGPL. https://people.sc.fsu.edu/~jburkardt/data/jpg/fractal_tree.jpg [Online; retrieved 27.09.2020]2.	Edvard Munch. The Scream. Public domain. https://upload.wikimedia.org/wikipedia/commons/f/f4/The_Scream.jpg [Online; retrieved 27.09.2020]3.	The Waterloo Image Repository. Lena. Copyright belongs to the author.
