Table 1: Comparison of five baselines (training without augmentation, EDA, senMixup, back-translation from German and back-translation from Spanish) with modals on two text datasetsfor different amounts of training examples (s: 10%, m: 50%)	w/o Aug.	EDA	senMixup	BT (DE)	BT (ES)	MODALSSST2 (s)	73.01	74.52	73.27	73.49	73.04	75.67SST2 (m)	79.70	79.77	79.50	79.52	79.48	81.35SST2	81.91	82.97	82.85	82.90	83.32	84.14TREC6 (s)	81.10	81.10	78.06	81.20	80.40	83.20TREC6 (m)	90.44	91.13	88.67	92.47	92.27	93.00TREC6	92.67	93.26	93.07	93.60	94.00	94.004.2	Tabular DataWe also perform an experiment with multiple tabular datasets from the UCI repository (Dua &Graff, 2017), including the Iris, Breast Cancer, Arcene (Guyon et al., 2005), Abalone, and HTRU2(Lyon et al., 2016) datasets. The data are standardized and trained with a 2-layer MLP that encodesthe inputs to 128-dimensional latent codes. We compare the accuracy with baseline methods usingno augmentation and using Mixup. The experiment is repeated with different numbers of trainingexamples. Table 2 shows that modals outperforms the baseline methods in 13 out of 15 settings.
Table 2: Comparison of two baselines (training without augmentation and Mixup) with modals onfive tabular datasets for different amounts of training examples (s: 20%, m: 50%)Dataset	w/o Aug.	Mixup	MODALS	Dataset	w/o Aug.	Mixup	MODALSIris (s)	86.67	86.67	96.67	Abalone (s)	62.80	62.84	63.40Iris (m)	96.67	98.00	98.67	Abalone (m)	64.04	64.67	64.95Iris	96.67	98.89	98.89	Abalone	64.59	65.67	65.71B. Cancer (s)	95.32	95.61	96.67	HTRU2 (s)	97.78	97.85	98.10B. Cancer (m)	96.78	97.08	97.37	HTRU2 (m)	98.01	98.03	98.14B. Cancer	96.78	97.66	99.41	HTRU2	98.04	98.16	98.16Arcene (s)	64.33	69.67	71.00				Arcene (m)	78.00	78.00	79.33				Arcene	83.33	83.33	83.67				7Published as a conference paper at ICLR 20214.3	Time-series DataFor time-series data, we use the HAR (Anguita et al., 2013) and Malware (Catak, 2019) datasets.
Table 3: Comparison of two baselines (training without augmentation and Mixup) with modals ontwo tabular datasets for different amounts of training examples (s: 10%, m: 50%)Dataset	w/o Aug.	Mixup	MODALS	Dataset	w/o Aug.	Mixup	MODALSHAR (s)	49.93	52.20	57.71	Malware (s)	79.24	81.14	84.07HAR (m)	85.44	86.05	89.06	Malware (m)	84.76	85.57	86.24HAR	88.64	91.60	91.87	Malware	86.40	87.01	87.124.4	Image DataFor image data, we apply modals to CIFAR-10, SVHN, and the reduced versions of these twodatasets, as in (Ho et al., 2019). CIFAR-10 and its reduced version contain 50,000 and 4,000 train-ing images, respectively, while SVHN and its reduced version contain 73,537 and 1,000 trainingimages, respectively. We use Wide-ResNet-40-2 as the feature extraction model in all experiments(Zagoruyko & Komodakis, 2016). We compare our method against three baselines. For the firstbaseline, we apply randomized cropping, horizontal flipping, and color normalization. For the sec-ond baseline, Cutout randomly occludes a patch of size 16×16 from the image on top of the simpleaugmentation in the first baseline. The third baseline is PBA, which uses the same search strategy asmodals. Both PBA and modals are deployed with their corresponding searched policy scheduleson top of Cutout. On the two original and the two reduced datasets, modals outperforms simpleaugmentation and Cutout but underperforms PBA (see Table 4). We suspect that the input-spaceaugmentations used in PBA are able to cover most variations in these datasets due to the continuousnature of image data. As the input-space augmentations are independent of the latent space augmen-
Table 4: Comparison of three baselines (training with simple augmentation, Cutout, and PBA) withmodals on two image datasets for different amounts of training examplesDataset	Simple Aug.	Cutout	PBA	MODALSReduced CIFAR-10	75.62	78.90	81.00	79.36CIFAR-10	92.04	92.48	92.72	92.51Reduced SVHN	76.17	76.83	83.52	81.23SVHN	96.37	96.14	96.54	96.465	Ablation S tudyLatent space augmentation. We study the effect of latent space augmentation techniques inmodals. We remove the latent space augmentation to isolate the effect from applying the tripletloss and adversarial loss. Table 5 summarizes the performance of modals across eight datasetsfrom different modalities. Our study shows that the use of latent space augmentation contributes toadditional performance gain when trained with different loss objectives. The detailed breakdown islisted in Appendix A.2.
Table 5: Comparison of average accuracy when trained under different loss settings and augmenta-tion techniques	Lclf	Lclf + Ladv	Lclf + Ltri	Lclf + Ladv + Ltriw/ Aug.	86.35	88.18	88.37	89.12 (MODALS)w/o Aug.	84.71	86.64	86.74	87.69Hard examples. We study the effectiveness of our proposed hard augmentation in creating uncertainexamples on eight datasets. Taking the difference of the predicted probability between the mostlikely and second most likely labels as a measurement of prediction certainty, our study shows thatthe proposed hard example interpolation and extrapolation create 7.23% and 2.34%, respectively,less certain examples on average (see Table 6). We further validate that the augmented examplesimprove the classification performance on hard examples (see Appendix A.2.2, A.2.3).
Table 6: Comparison of average change in prediction certainty when applied with and without hardaugmentationw/o Aug.	+Hard Interpolation	+Hard Extrapolation0.7093	0.6581 (-7.22%)	0.6927 (-2.34%)6	ConclusionIn this paper, we introduce modals to apply automated data augmentation in the latent space usingfour proposed modality-agnostic transformations trained with additional loss metrics and hard ex-ample augmentation techniques. Our method is tested on text, tabular, time-series, and image dataand can be readily integrated with popular deep learning models. Beyond the data modalities tested,modals also work on all other data modalities given proper feature extraction models. We believethat modals makes larger improvements to data modalities in which the input space augmentationis less trivial to be defined, like text data, video data or even graph data in low-resource regimes.
Table 7: Average standard deviation of the normalized distances for all classes as the latent spacedimensionality d variesd	SST2	TREC6	CIFAR-10	SVHN	IRIS	B. Cancer	HAR	Malware2	0.7801	0.8684	0.6068	0.6273	0.6688	0.8229	1.0847	0.62334	0.7592	0.7664	0.4286	0.5458	0.6507	0.6229	1.1415	0.48318	0.7688	0.6725	0.4059	0.6125	0.8674	0.6289	0.9768	0.398316	0.7774	0.6600	0.3789	0.5639	0.5668	0.5267	0.8000	0.340932	0.7812	0.6161	0.3783	0.5496	0.5642	0.5286	1.0814	0.307864	0.7747	0.5743	0.3829	0.5007	0.5526	0.4694	0.6396	0.2793128	0.7534	0.5206	0.3799	0.4639	0.5568	0.3329	0.8639	0.2590256	0.7297	0.4791	0.3709	0.4681	0.5407	0.4826	0.6127	0.2500A.2 Ablation StudyTable 8: Summary of the performance under different settings (Aug.: latent space augmentation;Ladv : adversarial loss; Ltri : triplet loss)Datasets	MODALS	-Aug.	-Ladv	-Ltri	-Ladv - LtriTREC6	94.00	92.33	93.60	93.20	91.60SST2	84.14	81.36	83.54	82.28	82.01Iris	98.89	96.67	97.78	98.89	97.78B. Cancer	99.41	98.83	99.41	98.83	97.36Reduced CIFAR-10	79.36	79.40	78.88	79.33	74.68
Table 8: Summary of the performance under different settings (Aug.: latent space augmentation;Ladv : adversarial loss; Ltri : triplet loss)Datasets	MODALS	-Aug.	-Ladv	-Ltri	-Ladv - LtriTREC6	94.00	92.33	93.60	93.20	91.60SST2	84.14	81.36	83.54	82.28	82.01Iris	98.89	96.67	97.78	98.89	97.78B. Cancer	99.41	98.83	99.41	98.83	97.36Reduced CIFAR-10	79.36	79.40	78.88	79.33	74.68Reduced SVHN	81.23	80.65	79.25	78.12	74.01HAR	91.87	89.01	91.72	90.81	90.96Malware (s)	84.07	83.27	82.76	83.94	82.46Average	89.12	87.69	88.37	88.18	86.36Change	-	-1.63%	-0.85%	-1.07%	-3.20%(cont’)		- Aug.	- Aug.	- Aug.			-Ltri - Ladv	-Ladv	-Ltri	TREC6	94.00	82.67	92.60	92.33	SST2	84.14	81.91	82.15	79.52	Iris	98.89	96.67	96.67	96.67	B. Cancer	99.41	96.78	97.95	97.67	Reduced CIFAR-10	79.36	75.62	78.06	79.02	
Table 9: Quantitative results for within-class and between-class distances under the effect of tripletloss on the TREC6 dataset. (d: dimensionality; cw : within-class distance; cb: between-class dis-tance)w/o triplet loss	w/ triplet lossd	cw	cb	CblCW	Cw	Cb	Cb /Cw2	9.32	25.86	2.77	1.55	7.68	4.924	6.40	20.26	3.16	1.79	7.11	3.968	7.40	21.09	2.84	2.02	7.14	3.5316	8.10	19.51	2.40	2.04	6.56	3.2032	9.19	22.17	2.41	2.37	7.07	2.9764	9.34	22.07	2.36	2.56	7.01	2.73128	9.67	21.26	2.19	3.04	7.53	2.4614Published as a conference paper at ICLR 2021A.2.2 Hard Example AugmentationNext, we study the effectiveness of our proposed transformation in creating hard examples. In ourexperiment, we measure the uncertainty of an example by the difference in predicted probabilitybetween the most likely and second most likely labels (Scheffer et al., 2001). We call this metric asmargin. A smaller margin implies a more uncertain prediction.
Table 10: Comparison of the margin between models trained without augmentation and with hardinterpolation or extrapolationDataset	w/o Aug.	+hard interpolation	+hard extrapolationSST2	0.4498	0.4168(-7.33%)	0.4492 (-0.13%)TREC	0.9681	0.9468 (-2.20%)	0.9273 (-4.22%)Iris	0.5220	0.4884 (-6.44%)	0.4946 (-5.25%)B. Cancer	0.8822	0.8130 (-7.85%)	0.8634 (-2.14%)Reduced SVHN	0.7770	0.7066 (-9.07%)	0.7655 (-1.48%)Reduced CIFAR-10	0.8129	0.7719 (-5.04%)	0.7867 (-3.22%)HAR	0.5069	0.4514 (-10.95%)	0.5044 (-0.49%)Malware (s)	0.7561	0.6697(-11.43%)	0.7510 (-0.67%)A.2.3 Hard Example TrainingWe also study whether the hard examples created using the proposed transformation benefit modeltraining. In the ablation test, we sample the top 5% hardest examples based on the classificationloss from each class and compare the changes in the margin and accuracy of the hard examples aftertraining with different augmentation settings (see Table 11). We use a fixed augmentation instead ofa fine-tuned policy schedule. In the experiment, the models trained with hard example interpolationand extrapolation achieve a higher accuracy on the sampled hard examples. The larger increases inmargin show that the models can learn to predict the hard examples with higher certainty.
Table 11: Comparison of the change in margin and classification accuracy on the hard examplesbetween models trained with different augmentation schemesDataset	w/o Aug.		+hard interpolation		+hard extrapolation		margin	accuracy	margin	accuracy	margin	accuracySST2	0.0092	0.4167	0.0719	0.4667	0.0588	0.4667TREC6	0.0981	0.0580	0.0777	0.0592	0.0887	0.0580Reduced CIFAR-10	0.3572	0.1900	0.3847	0.2533	0.3729	0.2133Reduced SVHN	0.2731	0.7926	0.3514	0.8222	0.3441	0.8667Iris	-0.0833	0.0370	0.1122	0.2592	0.0231	0.0740B. Cancer	0.0542	0.3589	0.1077	0.3895	0.0666	0.4102HAR	0.3808	0.0895	0.4037	0.1461	0.3530	0.1525Malware (s)	-0.0162	0.0044	0.0074	0.0126	-0.1121	0.0259Finally, we compare the end-to-end training with simple interpolation and extrapolation to our pro-posed hard augmentation methods. Table 12 presents the accuracy of the model trained with hardaugmentation and simple interpolation and extrapolation.
Table 12: Comparison of models trained with simple interpolation and extrapolation and with hardinterpolation and extrapolationDataset	Non-hard	HardReduced OFAR-10	78.11	79.36Reduced SVHN	80.11	81.23TREC6	91.40	94.00SST2	82.28	84.14Iris	98.89	98.89B. Cancer	96.49	99.41HAR	88.72	91.87Malware (s)	81.29	84.07A.3 Experiment DetailsIn this section, we present the detailed configuration and augmentation schedule for each of the datamodalities. In all the experiments, the augmentation policy is searched using 50% of the data asthe validation set. We use 16 child models in PBA implemented using the Ray Tune framework(https://docs.ray.io/en/latest/tune/index.html). The child models are evalu-ated and perturbed every three epochs. For the discriminator, we employ a 2-layer MLP model with256 hidden units and ReLU activations. The discriminator is trained using the Adam optimizer withlearning rate 0.01.
