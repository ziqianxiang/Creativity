Table 1: Average episode rewards ± standard deviation over 50 episodes on PPO and SA-PPOagents. We report natural rewards (no attacks) and rewards under six adversarial attacks, includinga simple random noise attack, the critic based attack in Pattanaik et al. (2018),MAD and RS attacksin Zhang et al. (2020b), Snooping attack proposed in Inkawhich et al. (2019), and the optimal attackproposed in this paper. In each row we bold the best (lowest) attack reward over all five attacks.
Table 2: Average episode rewards ± standard deviation over 50 episodes on ATLA agents and base-lines. We report natural rewards (no attacks) and the best (lowest) attack rewards among six typesof adversarial attacks, including a simple random noise attack, the critic based attack in (Pattanaiket al., 2018), MAD and RS attacks in Zhang et al. (2020b), Snooping attack proposed in Inkawhichet al. (2019), and the optimal attack proposed in this paper. For each environment, we bold the mostrobust agent. Since both RS attack and our “optimal” attack are parameterized attacks, the “bestattack” column represents the worst case agent performance under hundreds of adversaries. SeeAppendix A.1 for more details.
Table 3: Average episode rewards ± standard deviation over 50 episodes on five baselines and SA-PPO (Zhang et al., 2020b). We report natural episode rewards (no attacks) and episode rewards undersix adversarial attacks, including a simple random noise attack, the critic based attack in (Pattanaiket al., 2018), MAD and RS attacks in Zhang et al. (2020b), Snooping attack proposed in Inkawhichet al. (2019), and the optimal attack proposed in this paper. In each row we bold the best (lowest)attack reward over all five attacks. The row for the most robust method is highlighted.
Table 4: Natural and RS attack rewards of ATLA-PPO (LSTM)+ SA Reg checkpoints during train-ing. We report Average rewards± standard deviation over 50 episodes.
Table 5: Hyperparameters for all environments and settings. For vanilla environments, we use thehyperparameters from Zhang et al. (2020b) and Engstrom et al. (2020) if they are available forthat environment (Hopper and Walker2d). Other environments’ hyperparameter for the vanilla PPOmodel is found by a grid search. For SA-PPO and ATLA-PPO (MLP), the same set of hyperpa-rameters as in the vanilla models are used, except that for SA-PPO we tune the parameter κ and forATLA-PPO (MLP) we tune the entropy bonus coefficients as well as learning rates for the adversary.
