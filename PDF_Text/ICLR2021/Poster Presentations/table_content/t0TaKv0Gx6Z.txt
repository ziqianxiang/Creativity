Table 1: Test NLL for different dimensional ICA with different objective functions. The above resultsare averaged over 5 independent runs of each methods.
Table 2: Average log likelihood on first 5, 000 test images for different D of latent dimensions.				Table 3: Label entropy and accuracy for imputed images.		Method	D=16	Latent Dim		Method	Entropy	Accuracy		D=32	D=48	D=64			Vanilla VAE	-91.50	-90.39	-90.58	-91.50	Vanilla VAE	^0297^^	0.718SVGD VAE	-88.58	-90.43	-93.47 -94.88	SVGD VAE	0.538	0.691S-SVGD VAE	-89.17	-87.55	-87.74 -87.78	S-SVGD VAE	0.542	0.7284.2.2	Amortized SVGDFinally, we consider training VAEs with implicit encoders on dynamically binarized MNIST. Thedecoder is trained as in vanilla VAEs, but the encoder is trained by amortization (Feng et al., 2017; Puet al., 2017), which minimizes the mean square error between the initial samples from the encoder,and the modified samples driven by the SVGD/S-SVGD dynamics (Algorithm 3 in appendix J.4).
