Table 1: Aggregated results on the transfer streams over multiple relevant baselines (complete table with more baselines provided in Appendix E). * correspond to models using an Alexnet backbone.
Table 2: Results on the long evaluation stream. *correspond to models using an Alexnet backbone.See Tab. 18 for more baselines and error bars.
Table 3: Datasets used in the CTrL benchmark.
Table 4: Details of the streams used to evaluate the transfer properties of the learner. F-MNIST is Fashion-MNIST and R-MNIST is a variant of Rainbow-MNIST, using only different background colors and keepingthe original scale and rotation of the digits.
Table 5: Details of the tasks used in S long, part 1.
Table 6: Details of the task in S long, part 2.
Table 7: Permuted MNIST Model.
Table 8: Resnet architecture used throughout our experiments.
Table 9: Details of the Alexnet architecture used in Serr`a et al. (2018) and in our experiments.
Table 10: Memory complexity of the different baselines at train time and at test time, where N is the size of the backbone, T the number of tasks, r the size of the memory buffer per task, k the number of source columns used by MNTDP, b the number of blocks in the backbone, S the scale factor used for wide-HAT and p the average number of new parameters introduced per task.
Table 11 and 12 report performance across all axes of evaluation on the standard Permuted MNIST and Split CIFAR 100 streams.
Table 12: Results on the standard Split Cifar 100 stream. Each task is composed of 10 new classes.* correspond to models using an Alexnet backbone, † to models using stream-level cross-validation (see Section5.2).
Table 13: Results in the T (S-) evaluation stream. In this stream, the last task is the same as the first with an order of magnitude less data. * correspond to models using an Alexnet backbone, † to models using stream-level cross-validation.
Table 14: Results in the S+ evaluation stream. In this stream, the 5th task is the same as the first with an order of magnitude more data. Tasks 2, 3, and 4 are distractors. * correspond to models using an Alexnet backbone, † to models using stream-level cross-validation.
Table 15: Results in the transfer evaluation stream with input perturbation. In this stream, the last task is the same as the first one with a modification applied to the input space and with an order of magnitude less data. Tasks 2, 3, 4 and 5 are distractors. * correspond to models using an Alexnet backbone, † to models using stream-level cross-validation.
Table 16: Results in the transfer evaluation stream with output perturbation. In this stream, the last task uses the same classes as the first task but in a different order and with an order of magnitude less data. Tasks 2, 3, 4 and 5 are distractors. * correspond to models using an Alexnet backbone, † to models using stream-level cross-validation.
Table 17: Results in the plasticity evaluation stream. In this stream, we compare the performance on the probe task when it is the first problem encountered by the model and when it as already seen 4 distractor tasks.* correspond to models using an Alexnet backbone, † to models using stream-level cross-validation (see Section 5.2).
Table 18: Results on the long evaluation stream. We report the mean and standard error using 3 different instances of the stream, all generated following the procedure described in A.* correspond to models using an Alexnet backbone, † to models using stream-level cross-validation (see Section5.2).
