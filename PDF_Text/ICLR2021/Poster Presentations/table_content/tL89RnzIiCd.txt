Table 1: Results for MIL datasets Tiger, Fox, Elephant, and UCSB Breast Cancer in terms of AUC. Resultsfor all methods except the first are taken from either a( KUgUkaScI & Baydogan, 201 ) or b( Carbonneau et al.,2016), depending on which reports the higher AUC.
Table A.1: Results immune repertoire classification across all datasets. Results are given in termsof AUC of the competing methods on all datasets. The reported errors are standard deviationsacross 5 cross-validation (CV) folds (except for the column “Simulated”). Real-world CMV:Average performance over 5 CV folds on the cytomegalovirus (CMV) dataset Emerson et al. (2017).
Table A.2: Hyperparameter search-space of a manual hyperparameter selection on the respectivevalidation sets of the Elephant, Fox, Tiger and UCSB breast cancer datasets.
Table A.3: Hyperparameter search-space for grid-search on small UCI benchmark datasets. Allmodels were trained for 100 epochs using stochastic gradient descent with early stopping based onthe validation set accuracy and a minibatch size of 4 samples. The number of stored patterns isdepending on the number of target classes of the individual tasks.
Table A.4: Hyperparameter search-space for grid-search on HIV, BACE, BBBP and SIDER. Allmodels were trained if applicable for 4 epochs using Adam and a batch size of 1 sample.
Table A.5: Results on drug design benchmark datasets. Predictive performance (ROCAUC) on testset as reported by Jiang et al. (2020) for 50 random splitsModel	HIV	BACE	BBBP	SIDERSVM	0.822 ± 0.020	0.893 ± 0.020	0.919 ± 0.028	0.630 ± 0.021XGBoost	0.816 ± 0.020	0.889 ± 0.021	0.926 ± 0.026	0.642 ± 0.020RF	0.820 ± 0.016	0.890 ± 0.022	0.927 ± 0.025	0.646 ± 0.022GCN	0.834 ± 0.025	0.898 ± 0.019	0.903 ± 0.027	0.634 ± 0.026GAT	0.826 ± 0.030	0.886 ± 0.023	0.898 ± 0.033	0.627 ± 0.024DNN	0.797 ± 0.018	0.890 ± 0.024	0.898 ± 0.033	0.627 ± 0.024MPNN	0.811 ± 0.031	0.838 ± 0.027	0.879 ± 0.037	0.598 ± 0.031Attentive FP	0.822 ± 0.026	0.876 ± 0.023	0.887 ± 0.032	0.623 ± 0.026Hopfield (ours)	0.815 ± 0.023	0.902 ± 0.023	0.910 ± 0.026	0.672 ± 0.019A.6 PyTorch Implementation of Hopfield LayersThe implementation is available at: https://github.com/ml-jku/hopfield-layersA.6.1 IntroductionIn this section, we describe the implementation of Hopfield layers in PyTorch (Paszke et al., 2017;2019) and, additionally, provide a brief usage manual. Possible applications for a Hopfield layer in adeep network architecture comprise:•	multiple instance learning (MIL) (Dietterich et al., 1997),•	processing of and learning with point sets (Qi et al., 2017a;b; Xu et al., 2018),
