Table 1: Error rate (%) on the CIFAR-10 and CIFAR-100 test set. Methods With f are pseudo-labeling based,whereas others are consistency regularization methods.
Table 2: Error rate (%) on CIFAR-10 With differ-ent backbones Wide ResNet-28-2 (WRN) and Shake-Shake (S-S).
Table 3: Accuracy (%) on the UCF-101 test set. Meth-ods with * use scores reported in (Jing et al., 2020).
Table 4: mAP scores on the Pascal VOC2007 test set.
Table 5: Ablation Study on CIFAR-10 dataset (Er-ror Rate (%)). UPS with no uncertainty-aware (UA)selection, selects using only confidence-based criteria.
Table 6: Comparison of methods for uncertainty esti-mation on CIFAR-10 (1000 labels) (Error Rate (%))Method	1000 labels	4000 labelsMC-Dropout	8Γ4	6.36MC-SpatialDropout	8.28	6.60MC-DropBlock	9.76	7.50DataAug	8.28	6.72MC-Dropout (Gal & Ghahramani, 2016) to ob-tain the uncertainty measure. Ideally, approximateBayesian inference methods (Graves, 2011; Blun-dell et al., 2015; Louizos & Welling, 2016) canbe used to obtain prediction uncertainties; how-ever, Bayesian NNs are computationally costlyand more difficult to implement than non-Bayesian NNs. Instead, methods like (Wan et al., 2013;Lakshminarayanan et al., 2017; Tompson et al., 2015; Ghiasi et al., 2018) can be used withoutextensive network modification to obtain an uncertainty measure directly (or through Monte Carlosampling) that can easily be incorporated into UPS. To this end, we evaluate UPS using three otheruncertainty estimation methods using MC sampling with SpatialDropout (Tompson et al., 2015) andDropBlock (Ghiasi et al., 2018), as well as random data augmentation (DataAug). The experimentalsettings are described in Section D of the Appendix. Without using uncertainty estimation method
Table 7: Fully supervised classification score on Pascal VOC2007 and UCF-101 dataset. The metrics used foreach dataset are mAP and accuracy, respectively.
Table 8: Error rates on CIFAR-10 dataset with very few training samples.
Table 9: The effect of using the Mixup augmentation with UPS on CIFAR-10 dataset with 1000 labels.
Table 10: Error rates on the CIFAR-10 test set with no input augmentations used during training.
Table 11: Number of selected pseudo-labels from each class of CIFAR-10.
Table 12: Performance on the CIFAR-10 and Pascal VOC2007 test sets.
Table 13: Pseudo-Labeling accuracy on the CIFAR-100 (4000 labels)Method	Tp = 0.7	τp = 0.8	τp = 0.9Conf.-Based Selection	-54.92-	58.75	64.18Conf.-Based Selection (Cal)	64.75	70.48	77.18UPS	83.16	83.37	83.09Based on the trend present in this table, it is feasible that there exists some confidence-based thresholdfor a network (uncalibrated or calibrated) that could achieve strong pseudo-labeling performance.
