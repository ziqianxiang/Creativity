Table 1: Overall results on XTREME benchmark. Results of mBERT (Devlin et al., 2019),XLM (Conneau & Lample, 2019) and XLM-R (Conneau et al., 2020) are from XTREME (Hu et al.,2020). Results of ∣ are from our in-house replication. HNS is short for “Hard Negative Samples”.
Table 2: Comparison with existing methods on XTREME tasks.
Table 3: Ablation study on XTREME tasks.
Table 4: BLEU scores [%] on high-resource tasks. Results with f and 去 are from VECO(LUoet al., 2020) and our in-house implementation, respectively. In our implementation, we use XLM-Rand the best version of HiCTL (pre-traind with CCNet-100 and hard negative samples) to initializethe encoder, respectively.
Table 5: BLEU scores [%] on low-resource tasks. Results with ∣ are from our in-house imple-mentation. We provide additional experimental results (to follow experiments in Zhu et al. (2020))on IWSLT’14 English→Spanish (En→Es) task. HICTLBase represents the BASE sized model that ispre-trained on CCNet-100 with hard negative samples.
Table 6: BLEU scores [%] on Zero-shot MT via Language Transfer. We bold the highest trans-ferring score for each language family.
Table 7: The statistics of CCNet corpus used for pretraining.
Table 8: Parallel data used for pre-training.
Table 9: Hyperparameters used for pre-training.
Table 10: Results on Cross-lingual Natural Language Inference (XNLI) for each language. Wereport the accuracy on each of the 15 XNLI languages and the average accuracy of our Hictl as wellas five baselines: BiLSTM (Conneau et al., 2018), mBERT (Devlin et al., 2019), XLM (Conneau &Lample, 2019), Unicoder(HUang et al., 2019) and XLM-R (ConneaU et al., 2020). Results of 去 arefrom our in-house replication.
Table 11: PAWS-X accuracy scores for each language.
Table 12: POS results (Accuracy) for each language.
Table 13: NER results (F1) for each language.
Table 14: Tatoeba results (Accuracy) for each languageModel	af	ar	bg	bn	de	el	es	et	eu	fa	fi	fr	he	hi	hu	id	it	jaTranslate-train-all																		XLM-R	59.7	50.5	72.2	45.4	89.5	61.3	77.6	51.7	38.6	71.7	72.8	76.9	66.3	73.1	65.1	77.5	68.5	63.1HICTL, Wiki-15 +MT	61.5	51.4	76.1	47.9	92.1	63.4	80.5	55.9	37.8	74.6	76.7	78.0	68.4	74.5	68.8	80.4	70.2	63.9HICTL, CCNet-100 + MT	63.0	50.9	76.8	47.0	94.6	68.8	80.9	59.3	41.5	77.3	78.2	80.3	70.2	77.9	72.1	81.3	73.7	66.2+Hard Negative Samples	68.9	57.7	83.2	55.4	98.2	74.5	88.5	62.4	47.7	80.2	82.9	85.5	79.1	85.0	76.8	90.3	80.8	72.7	jv	ka	kk	ko	ml	mr	nl	pt	ru	sw	ta	te	th	tl	tr	ur	vi	zhXLM-R	15.8	53.3	51.2	63.1	66.2	59.0	81.0	84.4	76.9	19.8	28.3	37.8	28.9	36.7	68.9	26.6	77.9	69.8HICTL, Wiki-15 +MT	18.7	55.8	51.0	65.5	67.3	61.2	82.9	84.4	78.3	22.2	28.6	41.4	33.5	41.6	71.2	26.7	80.2	73.6HICTL, CCNet-100 + MT	19.6	57.3	54.6	68.0	71.8	62.0	88.1	88.9	77.7	26.1	32.9	39.5	32.9	43.2	71.2	27.8	79.9	74.7+Hard Negative Samples	27.2	63.0	61.5	72.6	75.3	67.8	92.8	92.8	85.4	32.0	36.7	47.8	41.5	49.8	77.0	34.3	84.3	81.3Figure 4: Visualizations (t-SNE projection) of sentence embeddings output by HICTL (left) andXLM-R (right). We collect 10 sets of samples from WMT’14-19, each of them contains 100 parallelsentences distributed in 5 languages (i.e., English, French, German, Russian, and Spanish). Eachset is identified by a color and different languages marked by different shapes. We can see thata set of sentences under the same meaning are clustered more densely for HICTL than XLM-R,which reveals the strong capability of HICTL on learning universal representations across differentlanguages.
