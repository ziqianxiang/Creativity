Table 1: AVSD@DSTC7 test results: f uses visual features other than I3D, e.g. ResNeXt, scene graphs. ∣incorporates additional video background audio inputs. § indicates finetuning methods on additional data orpre-trained language models. Metric notations: B-n: BLEU-n, M: METEOR, R: ROUGE-L, C: CIDEr.
Table 2: Dataset Summary of the AVSD benchmark with both test splits @DSTC7 and @DSTC8.
Table 3: AVSD@DSTC8 test results: f uses visual features other than I3D, e.g. ResNeXt, scene graphs. ∣incorporates additional video background audio inputs. § indicates finetuning methods on additional data orpre-trained language models. Metric notations: B-n: BLEU-n, M: METEOR, R: ROUGE-L, C: CIDEr.
Table 4: Ablation of AVSD@DSTC7 test results: We experiment with graphs that are compositional semantics,global semantics, and fully-connected with bidirectional or temporally ordered edges, and with graph-based orpath-based feature propagation. Metric notations: B-n: BLEU-n, M: METEOR, R: ROUGE-L, C: CIDEr.
Table 5: Comparison of AVSD@DSTC7 test results between learned paths and paths as sequences of thelast n turns: We experiment with paths predicted by our path generator and paths as a sequence of the last nturns, i.e. {t, ..., max(0, t - n)}. Metric notations: B-n: BLEU-n, M: METEOR, R: ROUGE-L, C: CIDEr.
Table 6: Comparison of dialogue context graphs built by local semantics and global semantics.
