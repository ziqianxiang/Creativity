Table 1: Comparison of MPT-1/32 withtrained binary-1/32 networks on CIFAR-10.
Table 2: Comparison of MPT-1/1 withtrained binary-1/1 networks on CIFAR-10.
Table 3: Comparison of MPT-1/32 withtrained binary-1/32 networks on ImageNet.
Table 4: Comparison of MPT-1/1 withtrained binary-1/1 networks on ImageNet.
Table 5: Hyperparameter Configurations for CIFAR-10 ExperimentsA.2 Hyperparameters for Section 3.2In these experiments, the weights are initialized using the Kaiming Normal distribution (He et al.,2015) for all the models except for MPT-1/32 on ImageNet where we use the Signed Constantinitialization (Ramanujan et al., 2020) as it yielded slightly better performance. All training routinesmake use of a cosine decay learning rate policy. For ImageNet training we used a label smoothingvalue of 0.1 and a learning rate warmup length of 5 epochs.
Table 6: Hyperparameter Configurations for CIFAR-10 ExperimentsMethod	Model	Optimizer	LR	Momentum	Weight Decay	Batch	EpochsMPT-1/32	WRN-50	SGD	0.256	0.875	3.051757812e-5	256	120MPT-1/32+BN	WRN-50		SGD	0.256		0.875	3.051757812e-5	256		120MPT-1/1		WRN-34	Adam	2.56e-4	-	3.051757812e-5	256		250MPT-1/1+BN	WRN-34	Adam	2.56e-4	-	3.051757812e-5	256		250Table 7: Hyperparameter Configurations for ImageNet ExperimentsB Existence of B inary-Weight Subnetwork ApproximatingTarget NetworkIn the following analysis, note that we write Bin({-1, +1}m×n) to denote matrices of dimensionm × n whose components are independently sampled from a binomial distribution with elements{-1, +1} and probability p = 1/2.
Table 7: Hyperparameter Configurations for ImageNet ExperimentsB Existence of B inary-Weight Subnetwork ApproximatingTarget NetworkIn the following analysis, note that we write Bin({-1, +1}m×n) to denote matrices of dimensionm × n whose components are independently sampled from a binomial distribution with elements{-1, +1} and probability p = 1/2.
Table 8: Comparison of MPT-1/32 with Trained Binary (1/32) Networks on CIFAR-10Method	Model	Top-1	ParamsBNN	VGG-Small	89.9	4.6MXNOR-Net		VGG-Small	89.8	4.6MDoReFa-Net		ResNet-20	79.3	0.27MBBG		ResNet-20	-85.3	0.27MLAB		VGG-Small	…87.7	4.6MDSQ		VGG-Small	91.7	4.6MIR-Net		ResNet-18	91.5	4.6MFull-Precision	VGG-Small	93.6	4.6MMPT(75, 1.25x)	VGG-Small	88.49	1.44MMPT(75,1.25x)+BN	VGG-Small	91.9	1.44MTable 9: Comparison of MPT-1/1 with Trained Binary (1/1) Networks on CIFAR-10Method	Model	Top-1	ParamsABC-Net	ResNet-18	62.8	11.2MBWN		ResNet-18	60.8	11.2MBWNH		ResNet-18	64.3	11.2MPACT		ResNet-18	65.8	11.2MIR-Net		ResNet-34	70.4	21.8MQuantization-Networks	ResNet-18	66.5	11.2M
Table 9: Comparison of MPT-1/1 with Trained Binary (1/1) Networks on CIFAR-10Method	Model	Top-1	ParamsABC-Net	ResNet-18	62.8	11.2MBWN		ResNet-18	60.8	11.2MBWNH		ResNet-18	64.3	11.2MPACT		ResNet-18	65.8	11.2MIR-Net		ResNet-34	70.4	21.8MQuantization-Networks	ResNet-18	66.5	11.2MQuantization-Networks	ResNet-50	72.8	25.6MFull-Precision	ResNet-34	73.27	21.8MMPT (80)	-WRN-50	72.67	13.7MMPT(80)+BN		WRN-50	74.03	13.7M-Table 10: Comparison of MPT-1/32 with Trained Binary (1/32) Networks on ImageNetE	Comparis on to edgepopup for MPT-1/32Note that binarization step of biprop can be avoided while finding MPT-1/32 - by initializing (andpruning) our backbone neural network with binary initialization (e.g., edgepopup with Signed Con-stant initialization (Ramanujan et al., 2020)). In this specific instance, biprop boils down to edge-popup with proper scaling. Next, we compare the performance of MPT-1/32 networks identifiedusing these two approaches. Both networks presented below use the same hyperparameter config-urations and are trained for 250 epochs on the CIFAR-10 dataset. We initialize the networks iden-
Table 10: Comparison of MPT-1/32 with Trained Binary (1/32) Networks on ImageNetE	Comparis on to edgepopup for MPT-1/32Note that binarization step of biprop can be avoided while finding MPT-1/32 - by initializing (andpruning) our backbone neural network with binary initialization (e.g., edgepopup with Signed Con-stant initialization (Ramanujan et al., 2020)). In this specific instance, biprop boils down to edge-popup with proper scaling. Next, we compare the performance of MPT-1/32 networks identifiedusing these two approaches. Both networks presented below use the same hyperparameter config-urations and are trained for 250 epochs on the CIFAR-10 dataset. We initialize the networks iden-tified with edgepopup using the Signed Constant initialization as it yielded their best performance.
Table 11: Comparison of MPT-1/1 with Trained Binary (1/1) Networks on ImageNetFigure 5. We find that the performance of MPT-1/32 identified with biprop outperforms networksidentified using edgepopup. This highlights the benefit of binarization (in conjunction with pruning)as a learning strategy.
