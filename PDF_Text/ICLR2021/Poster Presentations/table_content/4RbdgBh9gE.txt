Table 1: Mean accuracy and 95% confidence interval across 1000 test-time tasks. On common few-shotlearning benchmarks, using example weighting for the support set improves the MAML baseline’s accuracy.
Table 2: Learned augmentation commentaries result in competitive or improved model accuracy andloss on CIFAR10/100. Using the learned, label-dependent augmentation commentary during training resultsin models that are competitive with mixup and superior to other baselines. We show mean/standard deviationover three different initialisations.
Table 3: Learned attention masks offer improved robustness to spurious background correlation. Wetrain a commentary network to produce attention masks on a version of the CUB-200-2011 dataset that containsspurious background correlations, with the correlations present in the training/validation set not present in thetest set (the test set has a distribution shift). On both top 1 and top 5 accuracy, using the masks results inimproved model performance on the shifted test set.
Table B.1: Mean accuracy and 95% confidence interval across 1000 test-time tasks. On common few-shotlearning benchmarks, using example weighting for the support set improves the MAML baseline’s accuracy.
Table C.1: Learned augmentation commentaries result in competitive or improved model accuracy andloss on CIFAR10 and 100. Compared to not using a commentary and using a randomised label-dependentcommentary, the original learned commentary results in models that are competitive with mixup and improvecompared to other baselines. We show mean/standard deviation over three different initialisations.
Table D.1: Performance of different masking strategies on CIFAR10 and 100. Using the appropriate per-image learned masks improves (in both loss and accuracy) on permuting the masks across the entire dataset andrandomly selecting mask regions of the appropriate scale.
