Table 1: Atari performance at 200M steps. The scores of the 55 games are aggregated using thefour different protocols described in Section 3. To overcome limitations of the previous metrics, werecommend the task mean of clipped record normalized scores as a robust measure of algorithmperformance, shown in the right-most column. DreamerV2 outperforms previous single-GPU agentsacross all metrics. The baseline scores are taken from Dopamine Baselines (Castro et al., 2018).
Table 2: Ablations to DreamerV2 measured by their Atari performance at 200M frames, sorted by thelast column. The this experiment uses a slightly earlier version of DreamerV2 compared to Table 1.
Table 3: Conceptual comparison of recent RL algorithms that leverage planning with a learned model.
Table D1: Atari hyper parameters of DreamerV2. When tuning the agent for a new task, werecommend searching over the KL loss scale β ∈ {0.1, 0.3, 1, 3}, actor entropy loss scale η ∈{3 ∙ 10-5,10-4, 3 ∙ 10-4,10-3}, and the discount factor Y ∈ {0.99,0.999}. The training frequencyupdate should be increased when aiming for higher data-efficiency.
Table K1: Atari individual scores. We select the 55 games that are common among most papers inthe literature. We compare the algorithms DreamerV2, IQN, and Rainbow to the baselines of randomactions, DeepMind’s human gamer, and the human world record. Algorithm scores are highlightedin bold when they fall within 5% of the best algorithm. Note that these scores are already averagedacross seeds, whereas any aggregated scores must be computed before averaging across seeds.
