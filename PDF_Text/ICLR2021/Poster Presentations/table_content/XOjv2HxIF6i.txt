Table 1: Accuracy results on the Omniglot dataset averaged over 1000, 5-way, K (tr) -shot downstreamtasks with K(val) = 15 for each task. ± indicates the 95% confidence interval. The top threeunsupervised results are reported in bold. The baseline results are from Hsu et al. (2019) section 4.1.
Table 2: Accuracy results of unsupervised learning on CelebA for different unsupervised methods.
Table 3: Results on CelebA attributes benchmark 2-way, 5-shot tasks with K(val) = 5. The resultsare averaged over 1000 downstream tasks and ± indicates 95% confidence interval. The top threeunsupervised results are reported in bold. The baseline results are from Hsu et al. (2019) section 4.1.
Table 4: Results on Mini-Imagenet benchmark for 5-way, K(tr) -shot tasks with K (val) = 15. Theresults are averaged over 1000 downstream tasks and ± indicates 95% confidence interval. The topthree unsupervised results are reported in bold. The baselines are from Hsu et al. (2019) section4.1. Note that the BigBiGAN is trained on the unlabeled Imagenet dataset which is larger thanMini-ImageNet training set that used by Hsu et al. (2019) for training.
Table 5: LASIUM-MAML hyperparameters summaryHyperparameter	Omniglot	CelebA	CelebA attributes	Mini-ImageNetNumber of classes	5	5	2	5Input size	28 × 28 × 1	84 × 84 × 3	84 × 84 × 3	84 × 84 × 3Inner learning rate	0.4	0.05	0.05	0.05Meta learning rate	0.001	0.001	0.001	0.001Meta-batch size	4	4	4	4K(tr) meta-learning	1	1	5	1K (val) meta-learning	5	5	5	5K(val) evaluation	15	15	5	15Meta-adaptation steps	5	5	5	5Evaluation adaptation steps	50	50	50	50Table 6: LASIUM-ProtoNets hyperparameters summaryHyperparameter	Omniglot	CelebA	CelebA attributes	Mini-ImageNetNumber of classes	5	5	2	5Input size	28 × 28 × 1	84 × 84 × 3	84 × 84 × 3	84 × 84 × 3Meta learning rate	0.001	0.001	0.001	0.001Meta-batch size	4	4	4	4K(tr) meta-learning	1	1	5	1K(val) meta-learning	5	5	5	5
Table 6: LASIUM-ProtoNets hyperparameters summaryHyperparameter	Omniglot	CelebA	CelebA attributes	Mini-ImageNetNumber of classes	5	5	2	5Input size	28 × 28 × 1	84 × 84 × 3	84 × 84 × 3	84 × 84 × 3Meta learning rate	0.001	0.001	0.001	0.001Meta-batch size	4	4	4	4K(tr) meta-learning	1	1	5	1K(val) meta-learning	5	5	5	5K(val) evaluation	15	15	5	15We also report the ablation studies on different strategies for task construction in Table 7. We run allthe algorithm for just 1000 iterations and compared between them. We also apply a small translationto Omniglot images.
Table 7: Accuracy of different proposed strategies on Omniglot. For the sake of comparison, westop meta-learning after 1000 iterations. Results are reported on 1000 tasks with a 95% confidenceinterval.___________________________________________________________________________________Sampling Strategy	Hyperparameters	GAN-MAML	VAE-MAML	GAN-Proto	VAE-ProtoLASIUM-N	σ2=0.5	77.16 ± 0.65	70.41 ± 0.71	62.16 ± 0.79	61.57 ± 0.80LASIUM-N	σ2=1.0	71.10 ± 0.70	68.26 ± 0.71	60.95 ± 0.78	62.17 ± 0.80LASIUM-N	σ2=2.0	63.18 ± 0.71	65.18 ± 0.71	59.81 ± 0.78	64.88±0.78LASIUM-RO	α=0.2	77.62 ± 0.64	75.02±0.66	62.24±0.79	62.17 ± 0.80LASIUM-RO	α=0.4	75.79 ± 0.65	71.31±0.70	64.19±0.76	62.20±0.80LASIUM-OC	α=0.2	74.70 ± 0.68	74.98±0.67	61.79 ± 0.79	62.16 ± 0.78LASIUM-OC	α=0.4	73.40 ± 0.68	68.79 ± 0.73	64.59±0.76	63.08±0.79Besides, we perform a hyperparameter search on CelebA attributes benchmark. Table 8 demonstratesthe results for our experiments. We see that searching for hyperparameters for CelebA is almost aseasy as doing the same thing for Omniglot. LASIUM-N with σ2 = 0.25 outperforms state-of-the-artin this benchmark. We also see a bad performance in the case of LASIUM-OC, which we expectedas the number of classes in this benchmark’s tasks is N = 2. Thus samples generated duringmeta-learning are limited to only instances on the line connecting two anchor latent vectors. It is notthe case for LASIUM-N and LASIUM-RO since we can sample latent codes in the neighborhood orany direction from anchor points in the latent space.
Table 8: Accuracy of different proposed strategies on CelebA attributes task for GAN with 2-way,5-shot tasks with K(val) = 5. The results are averaged over 1000 downstream tasks and ± indicates95% confidence interval.
Table 9: Accuracy of LASIUM-N with σ2 = 0.5 on Omniglot dataset with respect to different valuesof . ± indicates 95% confidence interval.
Table 10: Accuracy of 5-way 1-shot learning on the Fungi dataset (part of the proposed Meta-datasetby Triantafillou et al. (2020)). For each system we indicate the dataset on which the meta-trainingphase was performed. The results for supervised first-order MAML are from Triantafillou et al.
