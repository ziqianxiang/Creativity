Table 1: Comparison between pruning schedules: one-shot pruning vs. our proposed GReg-1. Eachsetting is randomly run for 3 times, mean and std accuracies reported.
Table 2: Comparison of different methods on the CIFAR10 and CIFAR100 datasets.
Table 3: Acceleration comparison on ImageNet. FLOPs: ResNet34: 3.66G, ResNet50: 4.09G.
Table 4: Compression comparison on ImageNet with ResNet50. #Parameters: 25.56M.
Table 5: Training setting summary. For the SGD solver, in the parentheses are the momentum andweight decay. For ImageNet, batch size 64 is used for pruning instead of the standard 256, which isbecause we want to save the training time.
Table 6: Pruning ratio summary.
Table 7: Comparison between pruning schedules: one-shot pruning vs. our proposed GReg-1 usingthe Hessian-based criterion introduced in OBD LeCun et al. (1990). Each setting is randomly runfor 3 times, mean and std accuracies reported. We vary the global pruning ratio from 0.7 to 0.95so as to cover the major speedup spectrum of interest. Same as Tab. 1, the pruned weights here areexactly the same for the two methods under each speedup ratio. The finetuning processes (numberof epochs, LR schedules, etc.) are also the same to keep fair comparison.
Table 8: Hyper-parameters of our methods.
Table 9: Sensitivity analysis of Ku on CIFAR10/100 datasets with the proposed GReg-1 algorithm.
Table 10: Comparison between pruning schedules: one-shot vs. GReg-1. Pruning ratio is 90% forResNet56 and 70% for VGG19. In each run, the weights to prune are picked randomly before thetraining starts.
