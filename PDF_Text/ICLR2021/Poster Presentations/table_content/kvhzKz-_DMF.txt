Table 1: Average results and standard deviation in parentheses over 3 runs on low-resource data in GLUE.
Table 2: Test accuracies in the low-resource setting on text classification and NLI datasets under varyingsizes of training data (200, 500, 800, 1000, 3000, and 6000 samples). We report the average and standarddeviation in parentheses across three runs. We show the highest average result in each setting in bold.
Table 3: Test accuracy of models transferring to new target datasets. All models are trained on SNLI orMNLI and tested on the target datasets. ∆ are absolute differences with BERT.
Table 4: Hypothesis-only accuracy when freezing the encoder from models trained on SNLI/MNLI inTable 2 and retraining a hypothesis-only classifier (BERT, VIBERT), and baseline results when the encoderis not frozen (H-only). Lower results show more successful debiasing.
Table 5: Performance evaluation for all methods. ∆% are relative differences with BERT.
Table 6: Average ablation results over 3 runs with std in parentheses on GLUE. BERT and VIBERT’s results are from Table 1.					Model	MRPC		STS-B		RTE	Accuracy	F1	Pearson	Spearman	AccuracyBERT	87.80 (0.5)	83.20 (0.6)	84.93 (0.1)	83.53 (0.0)	67.93 (1.5)VIBERT (β=0) VIBERT	88.57 (0.6) 89.23 (0.1)	84.27 (0.7) 85.23 (0.2)	87.10 (0.4) 87.63 (0.3)	86.00 (0.5) 86.50 (0.4)	69.63 (1.3) 70.53 (0.5)5	Related WorkLow-resource Setting Recently, developing methods for low-resource NLP has gained attention (Cherryet al., 2019). Prior work has investigated improving on low-resource datasets by injecting large unlabeledin-domain data and pretraining a unigram document model using a variational autoencoder and use itsinternal representations as features for downstream tasks (Gururangan et al., 2019). Other approachespropose injecting a million-scale previously collected phrasal paraphrase relations (Arase & Tsujii, 2019)and data augmentation for translation task (Fadaee et al., 2017). Due to relying on the additional sourceand in-domain corpus, such techniques are not directly comparable to our model.
Table 7: Datasets used in our experiments.
