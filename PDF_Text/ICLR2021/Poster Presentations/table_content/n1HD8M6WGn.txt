Table 1: Results of existing encoder layer fusionmethods on the WMT16 Ro-En translation task.
Table 2: Results of the proposed SurfaceFusion methods on the Seq2Seq tasks. “FGLA” denotesfine-grained layer attention. The existing results are Ghazvininejad et al. (2019) for Ro-En, Ott et al.
Table 3: Cosine similarities between alignedsource and target word embeddings. “All” and“Non-Shared” denotes keeping or removing thealigned pair when the source and target words arethe same, which are easier to be aligned.
Table 4: Statistics of the datasets and hyperparameters for the experiments. All the data have beentokenized and split into joint sub-word units (Sennrich et al., 2016). “Batch” denotes the number ofsource tokens and target tokens used in each training step. “DP” denotes the dropout value (Srivastavaet al., 2014). “LP” denotes the length penalty (Wu et al., 2016). “Base” and “Big” denote the twokinds of model variants of Transformer. We chose the checkpoint with best validation PPl for testing.
Table 5: Examples from the Ro-En translation task. Red words are good predictions, while bluewords are bad predictions. Masking the embedding layer (“Mask Emb”) of the fine-grained layerattention model leads to hallucinatory predictions, prolonging the prediction length. While maskingthe output layer (“Mask Out”) leads to prediction omissions, shortening the length.
Table 6: Examples from the CNN/DM summarization task.
Table 7: Examples from the CONLL correction task.
