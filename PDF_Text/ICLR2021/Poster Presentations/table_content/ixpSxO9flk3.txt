Table 1: Features of EBM training approaches. Trade-offs must be made when training unnormal-ized models and no approach to date satisfies all of these properties.
Table 2: Fitting NICE models using various learning approaches for unnormalized models. Resultsfor SSM, DCM, CP-SM taken from Song et al. (2020).
Table 3: Classification on image datasets.				Table 4: FID on CIFAR10.	Unconditional samples can be seen from our CIFAR10 and CIFAR100 models in Figure 4. Samplesare refined through a simple iterative procedure using the latent space of our generator, explained inAppendix B.7.1. Additional conditional samples can be found in Appendix C.5上才口。巨富”!SlBl且口细 R 后♦队τκe*slaM∣曲巴・盟史Figure 4: Unconditional samples on CIFAR10 (left) and CIFAR100 (right).
Table 5: Out-of-distribution Detection. Model trained on CIFAR10. Values are AUROC (↑).				6.1	Tabular DataTraining with VERA is much more stable and easy to apply to domains beyond images where EBMtraining has been extensively tuned. To demonstrate this we show that JEM models trained withVERA can provide a benefit to semi-supervised classification on datasets from a variety of domains.
Table 6: Accuracy of semi-supervised learning on tabular data with 10 labeled examples per class.				et al. (2019) utilize a Mutual Information estimator to approximate the generator’s entropy whereaswe approximate the gradients of the entropy directly. The method of Kumar et al. (2019) requiresthe training of an additional MI-estimation network, but our approach only requires the optimizationof the posterior variance which has considerably fewer parameters. As demonstrated in Section 5.1,their approach does not perform as well as VERA for training NICE models and their generatorcollapses to a single point. This is likely due to the notorious difficulty of estimating MI in highdimensions and the unreliability of current approaches for this task (McAllester & Stratos, 2020;Song & Ermon, 2019a).
Table 7: Hyperparameters for VERA.
Table 8: Basic information about each tabular dataset.
Table 9: Hyperparameters for semi-supervised learning on tabular data21Published as a conference paper at ICLR 2021C Additional ResultsC.1 Training Mixtures of GaussiansWe present additional results training mixure of Gaussian models using VERA and PCD. Eachmodel consits of 100 diagonal Gaussians and mixing weights. Our experimental setup and hyper-parameter search was identical to that presented in Appendiex B.2. We see in Table 10 that VERAoutperforms PCD.
Table 10: Fitting a mixture of 100 diagonal Gaussians using ML, MCMC approximate ML andgenerator approximate ML.
