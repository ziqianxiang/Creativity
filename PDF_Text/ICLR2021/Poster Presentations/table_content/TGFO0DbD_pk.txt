Table 1: Performance in the evaluation phaseModel	Reward	Step	Time (s)Rainbow	36.2"	370	42SGRainbow	39.1	269	29PPO	45^	98	27ERL	4.6	96	27GPPO	4.8	89	22(agents navigate towards the target, collecting positive rewards), but time and number of steps (i.e.,trajectory length) differ significantly (≈31% and ≈37.5% for SGRainbow over Rainbow; ≈18%and ≈8% for GPPO over PPO and ERL, respectively). In both tasks, Supe-RL outperforms DRLalgorithms and ERL in every considered metric.
Table 2: Verification results of: (top) indoor task; (bottom) aquatic task	Violation (%)			Time (s)			Memory (MB)Model	ΘI,0	ΘI,1	ΘI,2	Θi,0	ΘI,1	ΘI,2	ΘI,0 ΘI,1	ΘI,2Rainbow	2.21	9.11	0	79.7	75.5	92.6	3.74	3.96	6.92SGRainbow 0		4.75	0	66.7	74.1	80.5	2.18	2.91	4.1		Violation (%)		Time (s)		Memory (MB)		Model	ΘA,0	ΘA,1	Θa,o	ΘA,1	ΘA,0	ΘA,1	PPO	0.9	1.2	3.4	124	0.1	5.8	ERL	0.5	0.7	3.4	3.4	0.1	0.15	GPPO	0	0.1	3.1	3.2	0.1	0.14.3	Robustness of Supe-RL using Formal VerificationAn important result in both tasks is the limited variance shown by Supe-RL approaches, across runswith different network initialization seeds. Appendix E shows a more detailed analysis of our results,where Supe-RL based approaches seem to not suffer from detrimental network initialization seeds.
