Table 1: Experimental results on commonsense reasoning datasets. The first group of models arebaselines. The models in the middle group and last group except the CALM model are trained withthe proposed objectives independently and the final CALM model is trained by joint training. Bestmodels are bold and second best ones are Underlined within each metric.
Table 2: Experimental results on large model. Comparison between large models of other PTLMsand CALM. Best models are bold and second best ones are Underlined within each metric.
Table 3: Comparison between PTLMs on CommonGEN.
Table 4: Analysis on Contrastive and Generative objectives. Left table shows the performanceon downstream tasks by pre-training with different generative objective (COR, C2S, and originalobjective for pre-training T5). Right table shows the performance on downstream tasks by pre-trainingwith different task formats of contrastive objective.
Table 5: Comparison of generated sentences with same concept-set.
Table 6: Experimental results on Knowledge Probing. Left table shows the mean precision onLAMA probing task of ConceptNET. Right table shows the performance on Fact checking and Entitylinking, which are from KILT task.
Table 7: Fine-tuning hyperparameters.
Table 8: Properties of Commonsense benchmark datasets.
Table 9: Experimental results with BART as backbone model. Best models are bold.
Table 10: Experimental results with Noun/Verb as Concepts. Best models are bold.
