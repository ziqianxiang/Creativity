Table 1: HyperparametersName	ValueOptimizer	Adam (Kingma & Ba, 2014)Learning rate	0.0003Discount factor (γ)	0.99Batch size	128Target weights update coefficient (τ)	0.005Gradient steps / environment steps	1Reward scale	5.0Entropy scale	1.0Replay memory size	1000000Number of samples before training starts	10000Number of critics	2NB: the target weights are updated according to the following running average: θ — τθ +(1 - T)θC Visual examplesFigure 8: Left: Example of Constantly Delayed MDP, with an action delay of three time-steps and anobservation delay of two time-steps. Here, actions are indexed by the time at which they started being produced.
