Table 1: Model ablation studya on the GLUE devset. All models have the bottom half layers frozen.
Table 2: Adapters with layer freezing vs. ST/MT on GLUE test set. F1 scores are reported for QQP/MRPC,Spearman’s correlation for STS-B, accuracy on the matched/mismatch sets for MNLI, Matthew’s correlation forCoLA and accuracy for other tasks. * Individual scores not available. ST=Single Task, MTL=Multitask, g.e.=greater or equal to. Results from: 1Devlin et al. (2018) 2Stickland et al. (2019). 3Houlsby et al. (2019) .
Table 3: Domain adaptation results on dev. sets for BASEmodels. 1Liu et al. (2019b), 2Jiang et al. (2020)% data used	SciTail					0.1%	SNLI		100%	0.1%	1%	10%	100%			1%	10%	BERTBASE 1	51.2	82.2	90.5	94.3		52.5	78.1	86.7	91.0MT-DNN1	81.9	88.3	91.1	95.7		81.9	88.3	91.1	95.7MT-DNNsmart 2	82.3	88.6	91.3	96.1		82.7	86.0	88.7	91.6CA-MTLBERT	83.2	88.7	91.4	95.6		82.8	86.2	88.0	91.5tested several pretrained and randomly initialized task embeddings in a zero-shot setting. The com-plete set of experiments with all task embeddings can be found in the Appendix, Section A.4. Wethen selected the best task embedding for our results in Table 3. STS-B and MRPC MTL-trained taskembeddings performed best on SciTail and SNLI respectively. CA-MTLBERT-BASE has faster adapta-tion than MT-DNNSMART (Jiang et al., 2020) as evidenced by higher performances in low-resourceregimes (0.1% and 1% of the data). When trained on the complete dataset, CA-MTLBERT-BASE is onpar with MT-DNNSMART. Unlike MT-DNNSMART however, we do not add context from a semanticsimilarity model - MT-DNNSMART is built off HNN (He et al., 2019). Nonetheless, with a largermodel, CA-MTL surpasses MT-DNNSMART on the full SNLI and SciTail datasets in Table 6.
Table 4: 24-task CA-MTL vs. ST and vs. 24-task MTLwith frozen layers on GLUE, SuperGLUE, MRQA andNER development sets. ST=Single Task, MTL=Multitask,g.e.= greater or equal to. Details in section A.5.
Table 5: Our 24-task CA-MTL vs. other large models on GLUE. F1is reported for QQP/MRPC, Spearman’s corr. for STS-B, Matthew’scorr. for CoLA and accuracy for other tasks. *Split not available.
Table 6: CA-MTL test performance vs. SOTA.
Table 7: List of acronyms used in this paper.
Table 8: CA-MTL is flexible and extensible to new tasks. However, CA-MTL is sensitive to the new task’sembedding. We tested multiple task embeddings that worked best on either SciTail or SNLI by checkingperformance in a zero shot setting or using 0% of the data.
Table 9: F1 scores are reported for QQP/MRPC, Spearman’s correlation for STS-B, accuracy on thematched/mismatch sets for MNLI, Matthew’s correlation for CoLA and accuracy for other tasks. ST=Single Task,MTL=Multitask. *QNLI v1 (we report v2) **F1 score or Spearman’s correlation is not reported. ***Unknownrandom seeds. Results from: 1Stickland et al. (2019) 2Liu et al. (2019b) 3Phang et al. (2018) 4Liu et al. (2019c).
Table 10: 8-task CA-MTLBERT-LARGE (see section 4.3) for various layer freezing configurations. F1 scores arereported for QQP/MRPC, Spearman’s correlation for STS-B, accuracy on the matched/mismatch sets for MNLI,Matthew’s correlation for CoLA and accuracy for other tasks. FBA = Full Block AttentionMethod	% frozen layers	# tasks g.e ST	GLUE											CoLA	MNLI	MRPC QNLI		QQP	RTE	SST-2	STS-B	AVgLARGE Models — Dev set Results											ST BERT-LARGE (ours)	0%	—	63.6	86.5/86.0	91.4	91.0	88.5	70.2	93.1	88.2	84.3CA-MTL	0%	7	60.2	86.2/86.0	92.0	91.5	88.7	76.3	93.3	89.5	84.9CA-MTL	25%	6	63.7	86.1/85.8	89.1	91.2	88.6	79.7	92.9	88.5	85.1CA-MTL	50%	3	63.2	85.5/85.5	91.8	90.9	88.3	81.4	93.0	90.1	85.5CA-MTL FBA	50%	0	60.2	81.7/81.1	88.0	85.8	85.7	78.7	88.6	87.1	81.8A.9 Dataset DescriptionThe datasets that were used for the domain adaptation experiments were SciTail5 and SNLI6. Wejointly trained a CA-MTLRoBERTa-LARGE model on 9 GLUE tasks, 8 Super-GLUE7 tasks, 6 MRQA8tasks, and on WNUT20179 (Derczynski et al., 2017).
Table 11: GLUE (Wang et al., 2018) dataset description.
Table 12: Super-GLUE (Wang et al., 2019b) dataset description. References: 1Clark et al. (2019a), 2de Marneffeet al. (2019), 3Gordon et al. (2012), 4Khashabi et al. (2018), 5Zhang et al. (2018), 6Wang et al. (2019b), 7Poliaket al. (2018), 8Levesque (2011)Acronym	Corpus	∣Train∣	TaSk		DomainBoolQ1 - CB2 COPA3 MultiRC4 ReCoRD5 RTE6 WiC7 WSC8	Boolean Questions CommitmentBank Choice of Plausible Alternatives Multi-Sentence Reading Comprehension Reading Comprehension and Commonsense Reasoning Recognition Textual Entailment Word-in-Context Winograd Schema Challenge	^9l4K- 250 400 5.1K 101K 2.5K 6K 554	acceptability sentiment detection paraphrase detection textual similarity paraphrase detection inference word sense disambiguation coreference resolution	Google queries, Wikipedia miscellaneous blogs, encyclopedia miscellaneous news news, Wikipedia WordNet, VerbNet fiction booksTable 13: MRQA (Fisch et al., 2019) dataset description. References: 1Rajpurkar et al. (2016a), 2Trischler et al.
Table 13: MRQA (Fisch et al., 2019) dataset description. References: 1Rajpurkar et al. (2016a), 2Trischler et al.
Table 14: SNLI (Bowman et al., 2015) and SciTail (Khot et al., 2018) datasets description.
