Table 1: Iterative training starting with documents retrieved with BERT and BM25. Iteration 0corresponds to the performance of the reader trained on the set of initial support documents. Wereport all metrics on the validation set.
Table 2: Comparison to state-of-the-art models on NaturalQuestions and TriviaQA.
Table 3: Performance on NarrativeQA.
Table 4: Comparison of training objectives on NaturalQuestions after one iteration. We report allthe metrics on the validation set.
Table 5: Comparison of attention aggregation schemes on NaturalQuestions after one iteration. Theindex i corresponds to output tokens, j corresponds to input tokens of a given passage, h to headsand k to layers of the decoder. We report all metrics on the validation set.
Table 6: Hyperparameters for retriever and reader training.
Table 7: Iterative training starting with documents retrieved with DPR. Iteration 0 corresponds tothe performance of the reader trained on the set of initial support documents. We report all metricson the validation set. Contrary to results reported in Table 1, the reader model was not re-initializedbetween each iteration.
