Table 1: Factors of variation in the proposeddataset. Values are linearly spaced in the spec-ified intervals. Joint angles are in radians, cubepositions in meters.
Table 2: Encoder (left) and decoder (right) architectures. The latent space dimensionality is denotedby d, and K = 3 indicates the number of image channels. Last line in the encoder architecture:the fully connected layer parameterizing the log variance of the approximate posterior distributionsof the latent variables has custom initialization. The weights are initialized with 1/10 standarddeviation than the default value, and the biases are initialized to -1 instead of 0. Empirically, thistogether with (learnable) LayerNorm was beneficial for training stability at the beginning of training.
Table 3: Architecture of one residual block. The scalar gate is implemented by multiplying thetensor by a learnable scalar parameter before adding it to the block input. Initializing the residualblock to the identity by setting this parameter to zero has been originally proposed by Bachlechneret al. (2020). The tensor shape is constant throughout the residual block.
