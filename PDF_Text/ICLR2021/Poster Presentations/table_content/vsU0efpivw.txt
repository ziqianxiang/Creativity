Table 1: Model performance (loss for synthetic or accuracy for others, averaged over 50 runs).
Table 2: Accuracies of Deep ShapNets for images, comparable CNNs and state-of-the-art models.
Table 3: Average difference from exact Shapley values for different models.
Table 4: Explanation regularization experiments with Deep ShapNets (averaged over 50 runs).
Table 5: Time for computing explanations (averaged over 1000 runs)			Models ExPIanations^^^^^	Deep ShapNet	DNN (eq. comp.)	DNN (eq. param.)Ours	20.47	N/A	N/ADeep SHAP	46.10	83.38	8.56Kernel SHAP (40)	60.78	130.04	11.52Kernel SHAP (77)	72.61	201.88	13.53Kernel SHAP (def.)	598.79	789.08	142.85G	Lower-dimensional comparison between ShapNet explanationsand post-hoc explanationsFor lower-dimensional datasets, we seek validation for the explanations h(x) provided by DeepShapNets by comparing to the true Shapley values of our models (which can be computed exactlyor aproximated well in low dimensions). Our metric is the normalized `1 norm between the trueShapley values denoted φ and the explanation denoted γ defined as:kφ k llφ - YIli	Q'(φ,γ) = Fr，	⑻where the number of dimension d is used as an effort to make the distance Note that we measurethe average difference from vector-output classifiers by means discussed in subsection G.1. Theexperimental setup and results are presented in subsection G.2.
Table 6: Standard deviation of Shapley approximation errors on different datasetsExPIanatiOnS Models	Ours	DeepSHAP	Kernel SHAP (77)	Kernel SHAP (def.)	Kernel SHAP (ext.)Untrained models					Deep ShapNet	0.734	5.23	7.67e+08	4.25e+09	9.20e-03Shallow ShapNet	7.55e-07	1.57	0.124	8.02e-03	2.98e-03Regression models on synthetic dataset					Deep ShapNet	3.28e-03	5.15	3.13e-03	8.79e-05	2.23e-05Shallow ShapNet	3.09e-04	1.28	4.75e-03	1.62e-04	6.33e-06Classification models on Yeast dataset					Deep ShapNet	0.105	0.358	0.0288	0	0Shallow ShapNet	5.35e-07	0.894	0.0304	0	0Classification models on Breast Cancer Wisconsin (Diagnoistic)					Deep ShapNet	0.0104	0.0359	0.0987	3.61e-03	1.72e-03Shallow ShapNet	0.0109	0.246	0.0141	1.93e-03	2.91e-04Results of the average difference can be seen in Table 3. From the results in Table 3, we can see thatindeed our Shallow ShapNet model produces explanations that are the exact Shapley values (errorby floating point computation) as Theorem 2 states. Additionally, we can see that even though ourDeep ShapNet model does not compute exact Shapley values, our explanations are actually similarto the true Shapley values.
Table 7: The boost in performance is statistically significant as, shown below, the t-scores forShapNets vs. different methodsJ'''``'`-—_Explanations Digits	Degrees of Freedom	Integrated Gradients	Input×Gradients	DeepLIFT	DeepSHAP	Saliency9 7→ 1	1008	28.0404	40.2129	28.7084	118.6641	101.51944 7→ 1	981	54.4659	68.6895	63.1221	139.4350	122.92928 7→ 3	973	56.8543	67.6736	56.8097	128.9730	94.60118 7→ 6	973	41.5245	57.2258	49.9137	97.5942	90.1728I.3.2	Training recipeLearning process As discussed before, all the models are trained with Adam in PyTorch with thedefault parameters. The batch size is set to 64 for all the vision tasks. No warm-up or any otherscheduling techniques are applied. The FashionMNIST & MNIST models in Table 1 are trainedfor 10 epochs while the MNIST models with which we investigate the new capabilities (layer-wiseexplanations & Shapley explanation regularizations) in Section 3 are trained with just 5 epochs. TheCifar-10 models are trained for 60 epochs.
