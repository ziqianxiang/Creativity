Table 1: Results. NBDT outperforms competing decision-tree-based methods by up to 18% and can alsooutperform the original neural network by 〜1%. “Expl?” indicates the method retains interpretable properties:pure leaves, sequential decisions, non-ensemble. Methods without this check see reduced interpretability. Webold the highest decision-tree-based accuracy. These results are taken directly from the original papers (n/adenotes results missing from original papers): XOC (Alaniz & Akata, 2019), DCDJ (Baek et al., 2017), NofE(Ahmed et al., 2016), DDN (Murthy et al., 2016), ANT (Tanno et al., 2019), CNN-RNN (Guo et al., 2018). Wetrain DNDF (Kontschieder et al., 2015) with an updated R18 backbone, as they did not report CIFAR accuracy.
Table 2: Comparisons of Hierarchies. We demonstrate that our weight-space hierarchy bests taxonomyand data-dependent hierarchies. In particular, the induced hierarchy achieves better performance than (a) theWordNet hierarchy, (b) a classic decision tree’s information gain hierarchy, built over neural features (“InfoGain”), and (c) an oblique decision tree built over neural features (“OC1”).
Table 3: Comparisons of Losses. Training the NBDT using tree supervision loss with a linearly increasingweight (“TreeSup(t)”) is superior to training (a) with a constant-weight tree supervision loss (“TreeSup”), (b)with a hierarchical softmax (“HrchSmax”) and (c) without extra loss terms. (“None”). ∆ is the accuracydifference between our soft loss and hierarchical softmax.
Table 4: Mid-Training Hierarchy. Constructing and using hierarchies early and often in training yields thehighest performing models. All experiments use ResNet18 backbones. Per Sec 3.4, βt ,ωt are the loss termcoefficients. Hierarchies are reconstructed every “Period” epochs, starting at “Start” and ending at “End”.
Table 6: Zero-Shot Superclass Generalization. We eval-uate a CIFAR10-trained NBDT (ResNet18 backbone) in-ner node’s ability to generalize beyond seen classes. Welabel TinyImageNet with superclass labels (e.g. label Dogwith Animal) and evaluate nodes distinguishing betweensaid superclasses. We compare to the baseline ResNet18:check if the prediction is within the right superclass.
Table 5: Original Neural Network. Wecompare the model’s accuracy before and af-ter the tree supervision loss, using ResNet18,WideResNet on CIFAR100, TinyImageNet.
