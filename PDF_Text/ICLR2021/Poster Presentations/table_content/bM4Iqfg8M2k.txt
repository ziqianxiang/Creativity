Table 1: Classification accuracy. The pooling methods yield pooling aggregation while the back-bones yield mean aggregation. The proposed GIB method with backbones yields subgraph embed-ding by aggregating the nodes in subgraphs.
Table 2: The mean and standard deviation of absolute property bias between the graphs and thecorresponding subgraphs.
Table 3: Ablation study on Lcon and LMI . Note that we try several initiations for GIB w/o Lconand LMI to get the current results due to the instability of optimization process.
Table 4: Quantitative results on graph denoising. We report the classification accuracy (Acc), num-ber of real edges over total real edges (Recall) and number of real edges over total edges in subgraphs(Precision) on the test setMethod	GCN	DiffPool	GCN+Att05	GCN+Att07	GCN+GIBRecall	-	-	0.226±0.047	0.324± 0.049	0.493± 0.035Precision	-	-	0.638± 0.141	0.675± 0.104	0.692 ±0.061Acc	0.617	0.658	0.649	0.667	0.684Figure 2: The molecules with their interpretable subgraphs discovered by different methods. Thesesubgraphs exhibit similar chemical properties compared to the molecules on the left.
Table 5: Average number of disconnected substruc-tures per graph selected by different methodsMethod	QED	DRD2	HLM	MLMGCN+Att05	3.38	1.94	3.11	5.16GCN+Att07	2.04	1.76	2.75	3.00GCN+GIB	1.57	1.08	2.29	2.06able interpretation to the property of molecules confirmed by chemical experts. More results are2We follow the protocol in https://github.com/rusty1s/pytorch_geometric/tree/master/benchmark/kernel8Published as a conference paper at ICLR 2021Table 6: The influence of the hyper-parameter α of Lcon to the size of subgraphs.
Table 6: The influence of the hyper-parameter α of Lcon to the size of subgraphs.
Table 7: Statistics of datasets in improvement of graph classification.
Table 8: Statistics of datasets in graph interpretation.
Table 9: Size of the chosen subgraphs on four datasets in percent.
Table 10: Size of largest connected parts used for graph interpretation in percent.
Table 11: Classification accuracy on graph classification. We implement the mutual informationestimator with Jensen-Shannon Divergence (JSD) and χ2 Divergence (χ2)Method	QED	DRD2	HLM-CLint	MLM-CLintGCN	0.743± 0.110	0.719± 0.041	0.707± 0.037	0.725± 0.046GCN+GIB(DV)	0.776± 0.075	0.748± 0.046	0.722± 0.039	0.765± 0.050GCN+GIB(JSD)	0.758± 0.087	0.741± 0.040	0.718± 0.044	0.759± 0.057GCN+GIB(χ2)	0.756± 0.097	0.746± 0.057	0.721± 0.033	0.755± 0.049Table 12: The overlap between the chosen subgraphs with different initialization.
Table 12: The overlap between the chosen subgraphs with different initialization.
