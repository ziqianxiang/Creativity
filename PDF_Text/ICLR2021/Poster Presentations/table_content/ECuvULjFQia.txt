Table 1: Hyperparameters for LDT in toy task BHyperparameter	ValueInner optimizer	AdamInner learning rate	1e—3Batch size	32Meta-optimizer	AdamMeta-learning rate	1e—3Teaching coefficient α	104Inner loop optimization steps n	64Validation split	0.5B.2	MUJOCOWe now describe additional details for the MuJoCo reward-prediction experiments.
Table 2: Hyperparameter ranges for the method ‘model-free‘ (MF)Hyperparameter	RangeLearning rate	{1e-3,1e-2}Weight decay	{0, 1e-5, 1e-4}Batch size	{8,16, 32}Table 3: Hyperparameter ranges for the method ‘auxiliary‘ (AUX)Hyperparameter	RangeLearning rate	{1e-3, 1e-2}Weight decay	{0, 1e-5, 1e-4}Auxiliary task weight	{1, 2, 4}Table 4: Hyperparameter ranges for our proposed method (LDT)Hyperparameter	Rangeβ1 of Meta-optimizer (Adam)	{0.0, 0.9}Learning rate of Inner-loop optimizer (SGD)	{1e-3, 5e-3, 1e-2}Teaching coefficient (log10)	{2.0, 2.5, 3.0}Validation split	{0.3, 0.5, 0.7}Table 5: Best hyperparameters found in grid search for LDTHyperparameter	Swimmer-v2	Walker2d-v2	Hopper-v2	HalfCheetah-v2β1 of Meta-optimizer (Adam)	0.9	0.9	0.0	0.0Learning rate of Inner-loop optimizer (SGD)	1e-2	1e-2	1e-2	1e-2
Table 3: Hyperparameter ranges for the method ‘auxiliary‘ (AUX)Hyperparameter	RangeLearning rate	{1e-3, 1e-2}Weight decay	{0, 1e-5, 1e-4}Auxiliary task weight	{1, 2, 4}Table 4: Hyperparameter ranges for our proposed method (LDT)Hyperparameter	Rangeβ1 of Meta-optimizer (Adam)	{0.0, 0.9}Learning rate of Inner-loop optimizer (SGD)	{1e-3, 5e-3, 1e-2}Teaching coefficient (log10)	{2.0, 2.5, 3.0}Validation split	{0.3, 0.5, 0.7}Table 5: Best hyperparameters found in grid search for LDTHyperparameter	Swimmer-v2	Walker2d-v2	Hopper-v2	HalfCheetah-v2β1 of Meta-optimizer (Adam)	0.9	0.9	0.0	0.0Learning rate of Inner-loop optimizer (SGD)	1e-2	1e-2	1e-2	1e-2Teaching coefficient (log10)	2.5	3.0	2.5	2.5Validation split	0.3	0.5	0.3	0.7Table 6: Other hyperparameters for LDT (all environments)Hyperparameter	Value (all envs)Learning rate of Meta-optimizer (Adam)	5e-4
Table 4: Hyperparameter ranges for our proposed method (LDT)Hyperparameter	Rangeβ1 of Meta-optimizer (Adam)	{0.0, 0.9}Learning rate of Inner-loop optimizer (SGD)	{1e-3, 5e-3, 1e-2}Teaching coefficient (log10)	{2.0, 2.5, 3.0}Validation split	{0.3, 0.5, 0.7}Table 5: Best hyperparameters found in grid search for LDTHyperparameter	Swimmer-v2	Walker2d-v2	Hopper-v2	HalfCheetah-v2β1 of Meta-optimizer (Adam)	0.9	0.9	0.0	0.0Learning rate of Inner-loop optimizer (SGD)	1e-2	1e-2	1e-2	1e-2Teaching coefficient (log10)	2.5	3.0	2.5	2.5Validation split	0.3	0.5	0.3	0.7Table 6: Other hyperparameters for LDT (all environments)Hyperparameter	Value (all envs)Learning rate of Meta-optimizer (Adam)	5e-4Learning rate of Student-optimizer (Adam)	1e-3Batch size	24Momentum of inner-loop optimizer (SGD)	0.75Weight decay (L2) coefficient in inner-loop	1e-8Number of inner-loop steps (ninner)	96
Table 5: Best hyperparameters found in grid search for LDTHyperparameter	Swimmer-v2	Walker2d-v2	Hopper-v2	HalfCheetah-v2β1 of Meta-optimizer (Adam)	0.9	0.9	0.0	0.0Learning rate of Inner-loop optimizer (SGD)	1e-2	1e-2	1e-2	1e-2Teaching coefficient (log10)	2.5	3.0	2.5	2.5Validation split	0.3	0.5	0.3	0.7Table 6: Other hyperparameters for LDT (all environments)Hyperparameter	Value (all envs)Learning rate of Meta-optimizer (Adam)	5e-4Learning rate of Student-optimizer (Adam)	1e-3Batch size	24Momentum of inner-loop optimizer (SGD)	0.75Weight decay (L2) coefficient in inner-loop	1e-8Number of inner-loop steps (ninner)	96Teacher c1	96Teacher c2	256Teacher c3	768Teacher nconv	4Number of student training steps per meta-step	24Performances When using the hyperparameters described above, we obtain the minimum test
Table 6: Other hyperparameters for LDT (all environments)Hyperparameter	Value (all envs)Learning rate of Meta-optimizer (Adam)	5e-4Learning rate of Student-optimizer (Adam)	1e-3Batch size	24Momentum of inner-loop optimizer (SGD)	0.75Weight decay (L2) coefficient in inner-loop	1e-8Number of inner-loop steps (ninner)	96Teacher c1	96Teacher c2	256Teacher c3	768Teacher nconv	4Number of student training steps per meta-step	24Performances When using the hyperparameters described above, we obtain the minimum testMSEs in the eight evaluation runs shown in Table 7.
Table 7: Minimum mean squared errors on all environments	no-teacher	auxiliary	LDTHalfCheetah-v2	0.0485	0.0477	0.0427Hopper-v2	0.00270	0.00224	0.00132Swimmer-v2	0.0169	0.0146	0.00964Walker2d-v2	0.0238	0.0211	0.01643×102×10Swιmmer-v2Walker2d-v210 2PT RT-8 RT-16 AR FTAblation typePT RT-8 RT-16 AR FTAblation type山 s≡lsωlHaIfCheetah-VZRT-8 RT-16 AR FTAblation typeOoo
