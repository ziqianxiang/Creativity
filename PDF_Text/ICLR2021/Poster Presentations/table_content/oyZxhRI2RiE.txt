Table 1: Comparison of CSP datasets. Examples from two of the datasets are shown in Figure 1.
Table 2: The S ParC and CoSQL accuracy over all questions (QM) and all interactions (IM). Thescores of IGSQL + BERT and R2SQL + BERT are from the official leaderboards.
Table 3: Joint goal accuracies (JGA) on MWoZ2.1 test set. All models use a BERT-like en-coder/GPT.
Table 4: Question (QM) and interaction (IM)accuracy on the SQA test set.
Table 5: The effect of SCoRe pre-training objectives. Im-provements are shown in the parentheses.
Table 6: Detailed results on the dev set of SParC.
Table 7: Effect of synthetic dataas training data augmentation.
Table 8: Performance of SCoRe pre-trainedon different synthesized data on MWoZ.
Table 9: Performance ofthis end, we compare RoBERTa and SCoRe under a few-shot settingon SQA when only 10% of training data is available. We choose SQA SCORE on 10% trainingbecause its annotation is most different from the synthetic text-to-SQL data of SQA.
Table 10: Detailed results of COSQL on the dev set. Qi is the accuracy of the ith question in theconversation.
Table 11: Detailed results of SQA on the test set. Qi is the accuracy of the ith question in theconversation.
Table 12: An example of synthetic conversational text-to-SQL data.
