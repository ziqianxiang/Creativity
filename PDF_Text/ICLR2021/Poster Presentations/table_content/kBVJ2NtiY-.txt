Table 1: Average returns achieved by the policies learned through various methods, for differentnumbers of input states. The states are sampled from a policy trained using SAC on the true rewardfunction; the return of that policy is given as a comparison. Besides the SAC policy return, all valuesare averaged over 3 seeds and the standard error is given in parentheses. We donâ€™t report Waypointson 1 state as it is identical to AverageFeatures on 1 state.
Table 2: Ablation of the gradient weighting heuristic described in Section 2.4. We report averagereturns (over 3 random seeds) achieved by the policies learned with and without the heuristic, fordifferent numbers of input states. Experiment setup is the same as in Table 1.
