Table 2: Breeds benchmarks constructed using imageNet. Here, “level” indicates the depth ofthe superclasses in the class hierarchy (task granularity), and the number of “subpopulations” (persuperclass) is fixed to create balanced datasets. We also construct specialized tasks by focusing onsubtrees in the hierarchy, e.g., only living (Living- 1 7) or non-living (Non-living-26) objects.
Table 7: Superclasses constructed by Huh et al. (2016) via bottom-up clustering of WordNet to obtain36 superclasses—for brevity, we only show superclasses with at least 20 ImageNet classes each.
Table 8: Superclasses used for the Entity- 1 3 task, along with the corresponding subpopulationsthat comprise the source and target domains.
Table 9: Superclasses used for the Entity- 3 0 task, along with the corresponding subpopulationsthat comprise the source and target domains.
Table 10: Superclasses used for the Living- 1 7 task, along with the corresponding subpopulationsthat comprise the source and target domains.
Table 11: Superclasses used for the Non-living-26 task, along with the corresponding subpopula-tions that comprise the source and target domains.
Table 13: Models used in our analysis.
Table 14: Additional hyperparameters for robustness interventions.
Table 22: Effect of adversarial training on model robustness to subpopulation shift. All modelsare trained on samples from the source domain—either using standard training (ε = 0.0) or usingadversarial training. Models are then evaluated in terms of: (a) source accuracy, (b) target accuracyand (c) target accuracy after retraining the linear layer of the model with data from the target domain.
Table 23: Effect of various train-time interventions on model robustness to subpopulation shift. Allmodels are trained on samples from the the source domain. Models are then evaluated in terms of:(a) source accuracy, (b) target accuracy and (c) target accuracy after retraining the linear layer ofthe model with data from the target domain. Confidence intervals (95%) obtained via bootstrapping.
