Table 1: Double-addition task: accuracy [%] of LSTMs and FNN on the two pairs. In case of LSTM(forced) only one input is presented at a time (to prevent interference). The header shows on whichpair the mask was trained on.  denotes an inverted mask.
Table 2: Accuracy of addition/multiplication task on addition and multiplication with FNN andLSTM. The header shows on what the applied mask was trained on.  denotes an inverted mask2 10-Il6-θM -O-ON(a) FNNFigure 12: Addition/multiplication taks: number of weights per operation for each layer in (a)feedforward network, (b) LSTM.
Table 3: Double-addition task: accuracy [%] of LSTMs and FNN on the two pairs. In case of LSTM(forced) only one input is presented at a time (to prevent interference). The header shows on whichpair the mask was trained on.  denotes an inverted mask for the hidden layers, while the regularmask (for the full task) is applied to the input and output layers. For further details please refer toSec. C.5.1of the previous task are initialized to 2 (corresponding to P ≈ 0.88), the logits for newly initializedweights to either 0 (P = 0.5, Fig. 14a) or -1 (P ≈ 0.27, Fig. 14b). Compared to Fig. 3, the sharingis significantly increased.
Table 4: Hyperparameters for different tasks on the Mathematics Dataset25Published as a conference paper at ICLR 2021C.8 CNN EXPERIMENTS ON CIFAR10C.8.1 SIMPLE CNNWe use a learning rate of 10-3 and β = 10-4. We train the network for 20k steps before freezing itsweights and then use an additional 20k steps for training each of the masks, including the referencemask. See Table 5 for details regarding the architecture.
Table 5: Architecture of the simple CNN used for CIFAR 10 experimentsFig. 18 shows the confusion matrix difference for all classes of CIFAR 10. The most surprisingobservation is that the decrease in performance for each of the classes is substantial, ranging from 40to 60%. This shows the heavy reliance on class-exclusive features.
