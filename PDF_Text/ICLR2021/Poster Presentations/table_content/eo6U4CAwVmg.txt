Table 1: Comparison of the best FID score and IS on unconditional image generation of CIFAR-10and CIFAR-100. Values in the rows marked by * are from those reported in its reference.
Table 2: Comparison of the best FID score and IS on unconditional image generation of CelebA-HQ-128 with SNDCGAN. We report the mean and standard deviation of best scores across 3 trials.
Table 3: Comparison of classification accuracy under linear evaluation protocol on CIFAR-10 andCIFAR-100. We report the mean and standard deviation across 3 runs of the evaluation.
Table 4: Comparison of FID (lower is better) from the class-wise subsets of CIFAR-10 test set (1Kimages per class). “Random” indicates unconditional generation, and “Train-1K” indicates a 1Krandom subsamples from the CIFAR-10 train set of the same class.
Table 5: Comparison of the bestFID and IS on CIFAR-10 with SND-CGAN when stronger (bCR andDiffAug) or weaker augmentations(ContraD) are used for each method.
Table 6: Comparison of the best FID score and IS onCIFAR-10 for ablations of our proposed components. Allthe models are trained with batch size 256. For the ablationof “MLP hd”, We replace hd with a linear model. "sg(∙)”indicates the use of stop-gradient operation.
Table 7: Comparison of the best FID score (lower is better) on unconditional image generation ofAFHQ datasets (Choi et al., 2020) with StyleGAN2. Values in the rows marked by * are from thosereported in its reference. We set our results bold-faced whenever the value improves the StyleGAN2baseline (“Baseline”). Underlined indicates the best score among tested for each dataset.
Table 8: Comparison of the best FID score and IS on conditional generation of ImageNet (64×64)with BigGAN architecture. Values in the rows marked by * are from those reported in its reference.
Table 9: Comparison linear evaluation and transfer learning performance across 6 natural imageclassification datasets for BigGAN discriminators pretrained on ImageNet (64 × 64). We report thetop-1 accuracy except for ImageNet and SUN397, which we instead report the top-5 accuracy.
Table 10: SNDCGAN generator__________Table 11: SNDCGANdiscriminatorLayer	Kernel	Output			Layer	Kernel	Output		Latent z	-			128	Conv, LeakyReLU	[3, 3,1]	h	×w ×	64Linear, BN, ReLU	-	h/8	× w/8 ×	512	Conv, LeakyReLU	[4, 4, 2]	h/2	× w/2 ×	128DeConv, BN, ReLU	[4, 4, 2]	h/4	× w/4 ×	256	Conv, LeakyReLU	[3, 3, 1]	h/2	× w/2 ×	128DeConv, BN, ReLU	[4, 4, 2]	h/2	× w/2 ×	128	Conv, LeakyReLU	[4, 4, 2]	h/4	× w/4 ×	256DeConv, BN, ReLU	[4, 4, 2]	h	×w ×	64	Conv, LeakyReLU	[3, 3, 1]	h/4	× w/4 ×	256DeConv, Tanh	[3, 3,1]	h	×w ×	3	Conv, LeakyReLU Conv, LeakyReLU	[4, 4, 2] [3, 3,1]	h/8 h/8	× w/8 × × w/8 ×	512 512									BigGAN. We use the official PyToch implementation of BigGAN (https://github.com/ajbrock/BigGAN-PyTorch). In order to adapt ContraD into the class-conditional BigGANarchitecture, we apply the stop-gradient operation not only on the penultimate representation beforecomputing hd , but also before applying the class-conditional projection (Miyato & Koyama, 2018).
