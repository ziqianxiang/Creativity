Table 1: The effective dimension d(0.8)Layer	0	1	2	3	4	5	6	7	8	9	10	11BERT	262	273	271	273	276	283	288	282	282	282	283	270D-BERT	244	226	232	227	217	175						GPT	265	141	65	76	173	210	205	217	221	253	269	307GPT2	114	73	1	1	1	1	1	8	26	66	116	1ELMo	455	367										Table 1 rePorts d(0.8) for different layers and models. It is sUrPrising that GPT2 has so few effectivedimensions, esPecially, d(0.8) = 1 for layer 2 to 6. The sUrPrisingly small effective dimensionalityis another way of saying that GPT2 vectors fall in a narrow cone, and conseqUently, their Pairwisecosines are large. If all the vectors lie on a 1-D line, all the cosines woUld be 1, and there woUld behardly any model caPacity. These observations motivate Us to look deePer into the embedding sPace.
Table 2: Number of clusters |C|Layer	BERT	D-BERT	GPT	GPT2	ELMo0	-6^^	7	1	2	21	6	10	2	2	22	4	15	2	2	3	4	14	2	2	4	3	10	2	2	5	14	2	2	2	6	6		2	2	7	2		2	2	8	2		2	2	9	11		1	2	10	2		1	2	11	9		1	2	Figure 3: The MMS for all the models. GPT2has significantly higher MMS scores than othermodels from layer 1 to layer 11. This means thecluster effects are more severe in GPT2.
Table 3: A comparison of LIDs (using cosinesimilarity) among contextual and static embed-ding spaces.
Table 4: K by MMS(left) vs MDB(right)Layer01234567891011BERT D-BERT GPT GPT2 ELMo66443
