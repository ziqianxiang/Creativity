Table 1: Accuracy and perplexity on test data over three seeds of each modelâ€™s best hyperparameterssetting according to our hyperparameter search. Detailed hyperparameters and results can be foundin the appendix section F.
Table 2: Best perplexity on the test data of Penn Treebank (PTB) and WikiText-2 (WT2) fromthree seeds. Detailed results can be found in the appendix in table 5. All PTB models haveroughly 24M parameters and all WT2 models have roughly 37M parameters. The AWD-TXL isthe Transformer-XL architecture as reported by Dai et al. (2019) with the necessary AWD-styleregularisation, model averaging, and softmax temperature tuning (see appendix section H).
Table 3: Statistics of the catbAbI dataset based on our preprocessing of the regular bAbI data.
Table 4: Best hyperparameters of the FWM for our language modelling experimentsdataset droupout dropoute dropouth dropouti wdrop batch size ADAM lr ASGD lrPTB	0.4	0.1	0.3	0.5	0.66	20	0.001	2.0WT2	0.4	0.1	0.25	0.7	0.61	80	0.001	0.5H. 1 ResultsTable 5: The detailed evaluation results of the FWM and Transformer-XL language model for alldata partitions of the PTB and WT2 datasets using a batch size of 1. Experiment logs can be foundin our git repository.
Table 5: The detailed evaluation results of the FWM and Transformer-XL language model for alldata partitions of the PTB and WT2 datasets using a batch size of 1. Experiment logs can be foundin our git repository.
