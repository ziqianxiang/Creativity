Table 1: We contrast our work with leading approaches in audio conversion (Kaneko et al., 2019b;Qian et al., 2019) and audio-to-video generation (Chung et al., 2017; Suwajanakorn et al., 2017).
Table 2: Objective Evaluation for Audio Translation: We evaluate on VCTK, which providespaired data. The speaker-classification accuracy (SCA) criterion enables us to study the naturalnessof generated audio samples and similarity to the target speaker, where higher is better. Typicalmethods measure reoconstruction error with Mel-Cepstral distortion (MCD) (where lower is bet-ter), but our prior analysis show that this will be dominated by matching the content words (Fig. 3).
Table 3: Human Studies for Audio: We conduct extensive human studies on Amazon MechanicalTurk. We report Mean Opinion Score (MOS) and user preference (percentage of time that methodranked best) for naturalness, target voice similarity (VS), source content consistency (CC), and thegeometric mean of VS-CC (since either can be trivially maximized by reporting a target/input sam-ple). Higher the better, on a scale of 1-5. The off-the-shelf Auto-VC model struggles to generalizeto CelebAudio, indicating the difficulty of in-the-wild zero-shot conversion. Fine-tuning on Cele-bAudio dataset significantly improves performance. When restricting Auto-VC to the same trainingdata as our model (scratch), performance drops a small but noticeable amount. Our results stronglyoutperform Auto-VC for VS-CC, suggesting exemplar autoencoders are able to generate speech thatmaintains source content consistency while being similar to the target voice style. We provide moreablation analysis in the Appendix.
Table 4: Fake Audio Discriminator: a) Speaker Agnostic Discriminator detects fake audio whileagnostic of the speaker. We train a real/fake audio classifier on 6 identities from CelebAudio, andtest it on both within-set speakers and out-of-set speakers. The training datasets contains (1) Realspeech of 6 identities: John F Kennedy, Alexei Efros, Nelson Mandela, Oprah Winfrey, Bill Clinton,and Takeo Kanade. Each has 200 sentences. (2) Fake speech: generated by turning each sentencein (1) into a different speaker in those 6 identities. The testing set contains (1) speaker within thetraining set; (2) speaker out of the training set: Barack Obama, Theresa May. Each identity in thetesting set contains 100 real sentences and 100 fake ones. For within-set speakers, results showour model can predict with very low error rate ( 1%). For out-of-set speakers, our model can stillclassify real speeches very well, and provide a reasonable prediction for fake detection. b) SpeakerSpecific Discriminator detects fake audio of a specific speaker. We train a real/fake audio classifieron a specific style of Barack Obama, and test it on 4 styles of Obama (taken from speeches spanningdifferent ambient environments and stylistic deliveries including presidential press conferences anduniversity commencement speechs). The training set contains 1200 sentences evenly split betweenreal and fake. Each style in the testing set contains 300 sentences evenly split as well. Results showour classifier provides reliable predictions on fake speeches even on out-of-sample styles.
Table 5: Verification of the reprojection property: To verify the reprojection property that Eq. 7(main paper) describes, we randomly sample 2 sets of words (Normal and Tongue-twister) spokenby 2 speakers (A and B) in VCTK dataset. For the autoencoder trained by A, all the words spokenby B are out-of-sample data. Then for each word wB, we generate wB→A which is the projectionto A’s subspace. We need to verify 2 properties - (i) The output wB→A lies in A’s subspace; (ii)The autoencoder minimizes the input-output error. To verify (i), we train a speaker classificationnetwork on speaker A and B, and predict the speaker of wB→A . We report the softmax output toshow how much likely wB→A is to be classified as A’s speech (Likelihood in the table). To verify(ii), we calculate (1) distance from wB to wB→A; (2) minimum distance from wB to any sampledwords by A. For normal words, the input wB is much closer (sometimes as close as) to the projectionwB→A than any other sampled point in A’s subspace. Furthermore, such minimum is reached whenthe content is kept the same. For tongue-twister words, which are more confusable, we additionallycalculate the mean distance from A’s subspace to wB . Distance from wB to wB→A is close to theminimum, and much smaller than the mean. This empirically suggests that nonlinear autoencoderbehave similar to their linear counterparts (i.e., they approximately minimize the reconstruction errorof the out-of-sample input).
Table 6: Ablation Analysis of WaveNet: We replace the WaveNet vocoder with Griffin-Lim tradi-tional vocoder, and compare our method with two zero-shot methods: (1) AutoVC without Wavenet,and (2) Chou et al. (Chou et al., 2019). We measure Voice Similarity by speaker-classification accu-racy (SCA) criterion, where higher is better. The results show our approach without WaveNet stilloutperform other zero-shot approaches not using WaveNet. We also report MCD same as Table 2 inmain paper as MCD is only related to Freq. features. For reference, we also list the results of “withWavenet” experiments.
Table 7: Ablation Analysis of Video Synthesis: To verify the effectiveness of a pre-trained audiomodel and audio decoder, we construct 2 extra baselines: a. Train only the audio-to-video translator(1st Baseline). b. Jointly train audio decoder and video decoder from scratch (2nd Baseline). Andwe compare these 2 baselines with our method: First train an autoencoder of audio, then train videodecoder while finetuning the audio part. From the results, we can see the performance clearly dropswithout the help of finetuning and audio decoder.
