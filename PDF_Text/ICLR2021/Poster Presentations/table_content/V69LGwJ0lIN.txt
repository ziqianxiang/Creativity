Table 1: Average success rate (%) (over 4 seeds) of offline RL methods: BC, BEAR (Kumar et al.,2019), EMAQ (Ghasemipour et al., 2020), CQL (Kumar et al., 2020b) and CQL+OPAL (ours).
Table 2: Average success rate (%) (over 4 seeds) of few-shot IL methods: BC, BC+OPAL, andBC+SVAE (Wang et al., 2017).
Table 3: Average success rate (%) (over 4 seeds) of online RL methods: HIRO (Nachum et al.,2018a), SAC+BC, SAC+OPAL, and DDQN+DDCO (Krishnan et al., 2017). These methods wereran for 2.5e6 steps for antmaze medium environments and 17.5e6 steps for antmaze large environ-ments.
Table 4: Due to improved exploration, PPO+OPAL outperforms PPO and SAC on MT10 and MT50in terms of average success rate (%) (over 4 seeds).
Table 5: Average success rate (%) (over 4 seeds) of CQL+OPAL for different values of dim(Z). Wefix c = 10.
Table 6: Average success rate on antmaze medium (diverse) (%) (over 4 seeds) of CQL combinedwith offline DADS and offline CARML for different values of k.
Table 7: Average success rate (%), cumulative dense reward, and cumulative dense reward (last 5steps) (over 4 seeds) of CQL combined with different offline skill discovery methods on antmazemedium (diverse). For CQL + (Offline) DADS and CQL + (Offline) CARML, we use k = 10. Notethat CQL+OPAL outperforms both other methods for unsupervised skill discovery on all of thesedifferent evaluation metrics.
