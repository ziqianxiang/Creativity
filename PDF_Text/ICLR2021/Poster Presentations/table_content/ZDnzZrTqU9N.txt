Table 1: Average and robust accura-cies on BiasedSST. Underlining indi-cates statistically significant differencecompared to ERM (p < 0.05)	Robust	AverageERM	2.15 ± 0.97	95.09 ± 0.16Topic CVaR	5.18 ± 1.46	95.00 ± 0.10NonParam	28.11 ± 2.16	92.45 ± 1.55P-DRO	34.98 ± 9.39	84.21 ± 2.11Oracle DRO	67.71 ± 3.03	77.91 ± 4.49method for DRO on NLP where the uncertainty set is determined by mixtures of a topic model.
Table 2: Effect of different optimal stopping and hyper-parameter selection strategies on robustvalidation accuracy.
Table 3: Ablation of P-DRO to train thelinear model on the toy task. We reportaccuracy on both domains, as well asrobust accuracy.
Table 4: Robust test accuracy on the DWMW17 and FDCL18 toxicity detection tasks.
Table 5: Average and robust accuracies on BiasedSST when P-DRO is trained with an LSTMadversary. Underlining indicates statistically significant difference compared to ERM (p < 0.05)	Robust	AverageERM	2.15 ± 0.97	95.09 ± 0.16Topic CVaR	5.18 ± 1.46	95.00 ± 0.10NonParam	28.11 ± 2.16	92.45 ± 1.55P-DRO	43.68 ± 4.93	86.58 ± 1.77Oracle DRO	67.71 ± 3.03	77.91 ± 4.49Table 6: Effect of hyper-parameters on robust validation accuracy on BiasedSSTRobust accuracy	Minmax stopping		Oracle stoppingλ	10-5	28.62 ± 12.37	45.10 ± 4.50λ	10-4	44.74 ± 3.24	50.43 ± 5.05λ	10-3	25.57 ± 10.33	38.70 ± 2.97τ=	0.1	39.72 ± 5.55	50.00 ± 4.98τ=	0.01	44.74 ± 3.24	50.43 ± 5.05τ=	0.001	44.74 ± 3.24	50.87 ± 5.09k=	1	41.98 ± 4.48	49.60 ± 5.39k=	5	44.74 ± 3.24	50.43 ± 5.05k=	10	32.17 ± 11.20	50.95 ± 5.01
Table 6: Effect of hyper-parameters on robust validation accuracy on BiasedSSTRobust accuracy	Minmax stopping		Oracle stoppingλ	10-5	28.62 ± 12.37	45.10 ± 4.50λ	10-4	44.74 ± 3.24	50.43 ± 5.05λ	10-3	25.57 ± 10.33	38.70 ± 2.97τ=	0.1	39.72 ± 5.55	50.00 ± 4.98τ=	0.01	44.74 ± 3.24	50.43 ± 5.05τ=	0.001	44.74 ± 3.24	50.87 ± 5.09k=	1	41.98 ± 4.48	49.60 ± 5.39k=	5	44.74 ± 3.24	50.43 ± 5.05k=	10	32.17 ± 11.20	50.95 ± 5.01C.3 Influence of hyper-parameters on P-DROWe study the influence of the 3 hyper-parameters τ (temperature), k (size of the renormalizationwindow) and λ (learning rate of the adversary) on the performance of P-DRO. All experimentsare run on the BiasedSST dataset, and the analysis proceeds as follows: starting from configurationτ = 0.01, k = 5 and λ = 10-4 and vary each of the hyper-parameters independently. We report twonumbers for each configuration: robust accuracy of the best model using Greedy-Minmax stoppingand using Oracle stopping. The latter is useful to disentangle the effect of the stopping criterion.
