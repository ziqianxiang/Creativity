Table 1: Results of combination of various computation complexity levels for MNIST dataset. Fullresults can be found in Table 7.
Table 2: Results of combination of various computation complexity levels for CIFAR10 dataset.
Table 3: Results of combination of various computation complexity levels for WikiText2 dataset.
Table 4: Ablation Study of IID scenarios. The single-letter models ’a’ and ’e’ are FedAvg equipedwith various normalization methods. The sBN significantly outperforms other existing normaliza-tion methods, including the InstanceNorm (IN), GroupNorm (GN) (the number of group G=4), andLayerNorm (LN). Scaler is used for HeteroFL to train models of different sizes and moderatelyimprove the results.
Table 5: Ablation Study of Non-IID scenarios. The Masked CrossEntropy is used for Non-IIDexperiments. It significantly improves the local performance and moderately improves global per-formance. Single letter model ’a’ and ’e’ are FedAvg equipped with various normalization methods.
Table 6: Hyperparameters and model architecture used in our experiments.
Table 7: Results of combination of various computation complexity levels for MNIST dataset.
Table 8: Results of combination of various computation complexity levels for CIFAR10 dataset.
Table 9: Results of combination of various computation complexity levels for WikiText2 dataset.
