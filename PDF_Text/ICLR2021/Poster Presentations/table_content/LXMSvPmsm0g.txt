Table 1: Comparison results across full dense model, BU Tickets with different ticket initialization, and `1 BUTickets when training incrementally on CIFAR-10. T1~i denotes the learned sequential tasks Tl - Ti.
Table A2: Comparison between our dense model and two previous SOTA CIL methods on CIFAR-10 and CIFAR-100. Reported performance it the final accuracy for each task T. Simple baselinedonates the dense CIL model (Zhang et al., 2019). Full model represents our proposed frameworkwhich combines lottery teaching technique with the simple baseline.
Table A3: Evaluation performance of TD tickets (6.87%) pruned from different task ranges.
Table A4: Evaluation performance of TD tickets at different sparsity levels on CIFAR-10. Re-ported performance is the final accuracy for each task T. Differences (+/-) are calculated w.r.t. thefull/dense model performance.
Table A5: Evaluation performance of TR-BU/TD tickets on CIFAR-10. T1 〜i, i ∈ {1,2, 3,4,5} do-nates that models have learned from T1,…，T incrementally. l∣lml∣l; represents the current networksparsity.
Table A6: Evaluation performance of TR-BU/TD tickets when training incrementally on CIFAR-100.
Table A7: Evaluation performance of TR-BU/TD tickets when training incrementally on Tiny-ImageNet.
Table A8: Evaluation performance of TR-BU Tickets when models incrementally learn 20 tasks onCIFAR-100.
