Table 1: Prediction accuracy on CUB all classes using the 10-way 5-shot incremental setting.
Table 2: Prediction accuracy on miniImageNet all classes using the 5-way 5-shot incremental setting									Method	sessions									12		3	4	5	6	7	8	9Fine-tune	64.25	30.11	18.53	6.31	2.86	2.68	1.87	1.56	1.42Joint train	64.25	58.80	55.26	52.38	49.71	48.37	45.91	44.68	43.38iCaRL (Rebuffi et al., 2017)	64.25	48.04	43.13	38.28	30.01	24.46	21.85	19.84	17.76Rebalancing (Hou et al., 2019)	64.25	49.21	44.17	37.71	30.11	22.92	19.99	17.96	16.25ProtoNet (Snell et al., 2017)	64.25	55.12	51.67	48.91	46.52	44.25	41.91	40.07	38.42ILVQ (Xu et al., 2012)	64.25	56.01	52.43	49.31	46.98	44.37	42.06	40.11	38.43SDC (Yu et al., 2020)	64.62	59.63	55.39	50.92	48.30	45.28	42.97	42.51	41.24Imprint (Qi et al., 2018)	64.71	59.85	55.71	52.47	49.90	47.31	44.57	42.57	41.26IDLVQ-C	64.77	59.87	55.93	52.62	49.88	47.55	44.83	43.14	41.84Ablation studies are conducted to analyze how individual components affect the performance ofincremental few-shot learning. We study five variants of our methods: (a) new reference vectors areinitialized as class centroids and no tuning is done for feature extractor or old reference vectors; (b)Lintra is not used in incremental learning sessions; (c) LF is not used in the incremental learningsessions; (d) shift in old reference vectors are not compensated; (e) replace the margin based loss LMwith the cross entropy loss LCE . Table 3 shows the results of our ablation studies on CUB dataset.
Table 3: Ablation study on CUB using the 10-way 5-shot incremental setting.										Method	sessions										2	3	4	5	6	7	8	9	10	11No tuning	71.93	67.14	64.21	62.61	60.13	59.04	58.47	55.64	54.25	53.66w.o. Lintra	74.75	70.26	66.89	65.05	63.18	61.84	61.36	58.61	58.14	57.24w.o. LF	73.85	69.54	66.21	64.02	62.74	60.28	59.49	56.97	56.38	55.46w.o. δi	74.67	70.01	66.74	64.81	63.90	61.42	60.73	58.16	57.62	56.79LM → LCE	73.22	69.41	66.03	63.93	63.07	61.14	60.98	58.67	58.11	57.32IDLVQ-C	74.72	70.28	67.13	65.34	63.52	62.10	61.54	59.04	58.68	57.81samples in novel classes, including 5-shot, 10-shot and 20-shot settings. One reference vector isassigned to each class in all few-shot settings. As shown in Fig. 2 in the appendix, the performanceof incremental learning improves as the number of samples per class increases. When the trainingsamples are scarce, the training samples may not well present the generative distribution of trainingdata. Therefore, the learned reference vectors could be biased and classification accuracy is low.
Table 4: Normalized RMSE of incremental few-shot regression on 3D spatial dataMethod	sessions			1	2	3Joint train offline	0.02174(2e-4)	0.02232(2e-4)	0.02296(2e-4)Fine-tune w. novel data	0.02174(2e-4)	0.08462(4e-4)	0.11870(6e-4)Fine-tune w. exemplars	0.02174(2e-4)	0.02988(2e-4)	0.03128(2e-4)IDLVQ-R	0.02181(2e-4)	0.02641(2e-4)	0.02817(2e-4)The normalized root mean squared errors (RMSE) between actual and predicted altitude in thetest set are listed in Table 4. The prediction accuracy drops significantly when the model is fine-tuned with novel data only. Catastrophic forgetting can be alleviated using exemplars from previoustasks. IDLVQ-R achieves better results than fine tuning with exemplars. The good performance ofIDLVQ-R can be attributed to two reasons. First, IDLVQ-R learns a number of reference vectorsand targets to preserve the knowledge in encountered tasks. Compared with a linear layer on topof neural networks, a number of reference vectors represent richer information about the trainingdata. Second, IDLVQ-R is nonparametric and can represent local and nonlinear relationship withoutlearning any regression coefficient from few-shot data.
Table 5: Prediction accuracy on CUB base and novel classes using the 10-way 5-shot incrementalsetting.											Base classes	sessions											12		3	4	5	6	7	8	9	10	11Fine-tune	77.30	44.23	36.28	27.52	25.96	23.05	17.68	13.07	11.78	10.99	10.71Joint train	77.30	75.83	75.25	74.51	74.58	73.74	73.95	73.25	73.11	73.25	73.18iCaRL (Rebuffi et al., 2017)	77.30	59.38	58.81	54.43	48.27	43.28	39.17	34.91	32.43	29.36	25.87Rebalancing (Hou et al., 2019)	77.30	66.43	60.32	55.36	46.39	41.76	37.12	32.58	31.26	27.03	24.25ProtoNet (Snell et al., 2017)	77.30	72.55	72.21	72.06	71.64	71.29	71.02	70.94	70.67	70.60	70.53ILVQ (Xu et al., 2012)	77.30	74.18	73.57	72.66	72.56	71.57	71.14	71.12	71.02	70.98	70.85SDC (Yu et al., 2020)	77.34	76.05	75.21	74.12	72.36	71.81	71.68	71.43	71.25	71.27	70.96Imprint (Qi et al., 2018)	77.02	74.76	74.57	73.69	72.69	70.88	70.34	70.12	70.07	69.84	69.27IDLVQ-C	77.37	76.32	75.90	75.91	75.49	74.86	74.58	74.37	74.02	73.39	73.32Novel classes						sessions						1	2	3	4	5	6	7	8	9	10	11Fine-tune	-	66.23	26.86	18.12	16.16	15.85	13.76	13.68	12.23	11.37	10.81Joint train	-	47.78	36.55	34.77	36.68	38.52	38.91	40.37	41.50	42.51	43.28iCaRL (Rebuffi et al., 2017)	-	35.18	33.97	27.04	21.99	23.99	23.04	24.01	22.96	24.04	24.59Rebalancing (Hou et al., 2019)	-	45.53	35.24	20.39	20.25	19.65	20.91	21.29	21.95	21.60	22.97ProtoNet (Snell et al., 2017)	-	41.86	35.01	29.72	29.43	28.72	28.65	30.02	29.56	30.59	30.19
Table 6: Prediction accuracy on miniImageNet base and novel classes using the 5-way 5-shot incre-mental setting.
Table 7: Prediction accuracy on CUB using the 10-way 10-shot incremental setting.
Table 8: Prediction accuracy on CUB using the 10-way 20-shot incremental setting.
