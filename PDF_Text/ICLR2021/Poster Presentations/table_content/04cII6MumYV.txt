Table 1: Test accuracy (mean±CI%95) over 600 few-shot tasks. URT and the most recent methods,which are listed in the first column, are compared on Meta-Dataset (Triantafillou et al., 2020), whichare listed in the first row. The numbers in bold have intersecting confidence intervals with the mostaccurate method.
Table 2: Test accuracy (mean±CI%95) over 600 few-shot tasks. All methods use parametric net-work family (pf) backbones.
Table 3: Meta-Dataset performance variation on ablations of elements of the URT layer.
Table 4: Validation performance on Meta-Dataset using different number of heads	1	2	3	4	5	6	7	8Average Accuracy	74.605	77.145	76.943	76.984	76.602	75.906	75.454	74.473Average Rank	2.875	1.000	1.000	1.000	2.250	2.250	2.25	2.50In general, we observe a large jump in performance when using multiple heads instead of just one.
Table 5: Test performance (mean+CI%95) over 600 few-shot tasks on additional datasets.
Table 6: Test accuracy (mean±CI%95) over 600 few-shot tasks. URT and the most recent methods,which are listed in the first column, are compared on Meta-Dataset (Triantafillou et al., 2020), whichare listed in the first row. The numbers in bold have intersecting confidence intervals with the mostaccurate method.
Table 7: Test accuracy (mean±CI%95) over 600 few-shot tasks. All methods use parametric net-work family (pf) backbones.
