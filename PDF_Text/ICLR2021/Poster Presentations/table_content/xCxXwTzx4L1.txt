Table 1: Performance scores for pruning PreResNet-164 architecture on CIFAR-10 and CIFAR-100 datasets for Network Slimming and ChipNet (ours). The number of parameters and FLOPsfor the unpruned networks are 1.72 million and 5.03 × 108, respectively. Here budget refers to thepercentage of total channels remaining. Abbreviations ‘Acc.’ and ‘Params.’ refer to accuracy andnumber of parameters, all scores are reported in %, and parameters and FLOPs are reported relativeto those in the unpruned network.
Table 2: Performance scores for pruning ResNet-50 architecture on CIFAR-100 and CIFAR-10 forBAR and ChipNet (ours) with volume budget (V) and channel budget (C). The number of parametersand FLOPS for the unpruned networks are 23.7 million and 2.45 × 109, respectively. Here budgetrefers to the percentage of total channels/volume remaining. Abbreviations ‘Acc.’ and ‘Param.’ referto accuracy and number of parameters, all scores are reported in %, and parameters and FLOPs arereported relative to those in the unpruned network.
Table 3: Accuracy values (%) onCIFAR-100 dataset for ResNet-101pruned with different choices of channelbudget (%) on CIFAR-100 (Base) andwith masks from Tiny ImageNet (Trans-fer).
Table 4: Performance scores for pruning ResNet-50 architecture on CIFAR-100/CIFAR-10 for BARand ChipNet (ours) with volume budget (V) and channel budget (C). The number of parameters andFLOPS for the unpruned networks are 23.7 million and 2.45 × 109, respectively. Abbreviations‘Acc.’ and ‘Param.’ refer to accuracy and number of parameters, all scores are reported in %, andparameters and FLOPs are reported relative to those in the unpruned network.
Table 5: Performance scores for pruning WideResNet architecture on CIFAR-10, CIFAR-100 andTiny ImageNet datasets for BAR (Lemaire et al., 2019), MorphNet (Gordon et al., 2018), ID (Dentonet al., 2014), WM (Han et al., 2015b), Random Pruning and ChipNet (ours). All results are reportedin % accuracyMethod	Budget (%)	CIFAR-10 ↑	Tiny ImageNet ↑	CIFAR-100 ↑	50	92.7	52.4	74.1BAR	25	92.8	52	73.6	12.5	92.8	51.4	72.6	6.25	91.6	52.0	70.5	50	93.3	58.2	73.6MorphNet	25	92.9	55.8	70.4	12.5	90.7	51.7	69.9	6.25	86.4	39.2	55.5	50	91.09	49.96	69.29ID	25	91.44	49.55	69.75	12.5	90.37	45.77	66.03	6.25	86.92	39.72	59.13	50	91.11	49.01	68.98WM	25	91.20	49.67	69.10	12.5	89.68	47.72	65.42
Table 6: Grid search on WRN-C100 for 16x volume pruning factor. Here Acc refers to the validationaccuracy of hard pruned model during pruning.
Table 7: Senstivity analysis on WRN-C100 for 16x volume pruning factor. Here Accuracy refers tothe test accuracy of hard pruned model after finetuning.
Table 8: Accuracy values (%) on CIFAR-100 dataset for ResNet-101 pruned with different choicesof channel budget (%) on CIFAR-100 (Base) and with masks from Tiny ImageNet (Host).
Table 9: Performance scores for pruning MobileNetV2 architecture on Cifar-10 for ChipNet withchannel budget.
Table 10: Performance scores for pruning ResNet-50 architecture on Tiny-ImageNet for ChipNetwith volume budget (V) and channel budget (C).
Table 11: Accuracy values (%) on CIFAR-10 dataset for ResNet-110 pruned using volume budget.
