Table 1: Table of best hyperparameters for biased PAVFsLearning rate policy	Policy:	[]	[64,64]	Metric:	avg	avgSwimmer-v3		1e-3	1e-4Hopper-v3		1e-4	1e-4Learning rate critic			Swimmer-v3		1e-4	1e-4Hopper-v3		1e-3	1e-3Noise for exploration			Swimmer-v3		1.0	1.0Hopper-v3		0.1	0.1ARS For ARS, we used the official implementation provided by the authors and we modified itin order to use nonlinear policies. More precisely, we used the implementation of ARSv2-t (Maniaet al., 2018), which uses observation normalization, elite directions and an adaptive learning ratebased on the standard deviation of the return collected. To avoid divisions by zero, which mayhappen if all data sampled have the same return, we perform the standardization only in case thestandard deviation is not zero. In the original implementation of ARS (Mania et al., 2018), thesurvival bonus for the reward in the Hopper environment is removed to avoid local minima. Sincewe wanted our PSSVF to be close to their setting, we also applied this modification. We did notremove the survival bonus from all TD algorithms and we did not investigate how this could affect
Table 2: Average return with standard deviation (across 20 seeds) for hypermarameters optimizingthe average return during training using deterministic policies. Square brackets represent the numberof neurons per layer of the policy. [] represents a linear policy.
Table 3: Final return with standard deviation (across 20 seeds) for hypermarameters optimizing thefinal return during training using deterministic policies.
Table 4: Table of best hyperparameters for PSSVFs using deterministic policiesLearning rate policy	Policy: Metric:	[]		[32]		[64,64]			avg	last	avg	last	avg	lastAcrobot-v1		1e-2	1e-3	1e-4	1e-4	1e-4	1e-4MountainCarContinuous-v0		1e-2	1e-3	1e-4	1e-4	1e-4	1e-4CartPole-v1		1e-3	1e-3	1e-3	1e-3	1e-4	1e-4Swimmer-v3		1e-3	1e-3	1e-3	1e-3	1e-2	1e-4InvertedPendulum-v2		1e-3	1e-3	1e-3	1e-3	1e-4	1e-4Reacher-v2		1e-4	1e-4	1e-4	1e-4	1e-4	1e-4Hopper-v3		1e-4	1e-4	1e-4	1e-3	1e-4	1e-4Learning rate critic							Acrobot-v1		1e-2	1e-3	1e-2	1e-2	1e-2	1e-2MountainCarContinuous-v0		1e-3	1e-2	1e-3	1e-2	1e-2	1e-2CartPole-v1		1e-2	1e-2	1e-3	1e-3	1e-2	1e-2Swimmer-v3		1e-3	1e-3	1e-2	1e-2	1e-3	1e-2InvertedPendulum-v2		1e-2	1e-2	1e-3	1e-2	1e-3	1e-3Reacher-v2		1e-3	1e-3	1e-3	1e-3	1e-4	1e-4Hopper-v3		1e-3	1e-3	1e-2	1e-2	1e-2	1e-2Noise for exploration							Acrobot-v1		1.0	1.0	1e-1	1e-1	1e-1	1e-1
Table 5: Table of best hyperparameters for ARSLearning rate policy	Policy: Metric:	[]		[32]		[64,64]			avg	last	avg	last	avg	lastAcrobot-v1		-^1e-2^^	1e-3	1e-2	1e-2	1e-2	1e-2MountainCarContinuous-v0		1e-2	1e-2	1e-2	1e-2	1e-2	1e-2CartPole-v1		1e-2	1e-2	1e-2	1e-2	1e-2	1e-2Swimmer-v3		1e-2	1e-2	1e-2	1e-2	1e-2	1e-2InvertedPendulum-v2		1e-2	1e-2	1e-2	1e-2	1e-2	1e-2Reacher-v2		1e-2	1e-2	1e-3	1e-2	1e-3	1e-3Hopper-v3		1e-2	1e-2	1e-2	1e-2	1e-2	1e-2Number of directions and elite directions							Acrobot-v1		(4,4)	(4,4)	(1,1)	(1,1)	(1,1)	(1,1)MountainCarContinuous-v0		(1,1)	(1,1)	(1,1)	(16,4)	(1,1)	(1,1)CartPole-v1		(4,4)	(4,4)	(1,1)	(1,1)	(4,1)	(4,1)Swimmer-v3		(1,1)	(1,1)	(1,1)	(4,1)	(1,1)	(1,1)InvertedPendulum-v2		(4,4)	(4,4)	(1,1)	(4,4)	(4,1)	(16,1)Reacher-v2		(16,16)	(16,16)	(1,1)	(16,4)	(1,1)	(1,1)Hopper-v3		(4,1)	(4,1)	(1,1)	(1,1)	(1,1)	(1,1)Noise for exploration							Acrobot-v1		1e-2	1e-3	1e-1	1e-1	1e-1	1e-1
Table 6: Table of best hyperparameters for PSVFs using deterministic policiesLearning rate policy	Policy: Metric:	[]		[32]		[64,64]			avg	last	avg	last	avg	lastAcrobot-v1		1e-2	1e-2	1e-4	1e-4	1e-4	1e-2MountainCarContinuous-v0		1e-2	1e-3	1e-2	1e-4	1e-3	1e-4CartPole-v1		1e-2	1e-2	1e-2	1e-4	1e-3	1e-4Swimmer-v3		1e-3	1e-3	1e-3	1e-3	1e-3	1e-3InvertedPendulum-v2		1e-2	1e-3	1e-4	1e-4	1e-4	1e-4Reacher-v2		1e-3	1e-2	1e-4	1e-4	1e-4	1e-4Hopper-v3		1e-3	1e-3	1e-4	1e-4	1e-4	1e-3Learning rate critic							Acrobot-v1		1e-3	1e-4	1e-2	1e-2	1e-3	1e-2MountainCarContinuous-v0		1e-4	1e-3	1e-2	1e-4	1e-3	1e-3CartPole-v1		1e-2	1e-2	1e-2	1e-3	1e-2	1e-4Swimmer-v3		1e-4	1e-4	1e-4	1e-4	1e-4	1e-4InvertedPendulum-v2		1e-3	1e-2	1e-3	1e-4	1e-4	1e-3Reacher-v2		1e-2	1e-2	1e-3	1e-3	1e-4	1e-4Hopper-v3		1e-2	1e-2	1e-4	1e-4	1e-2	1e-4Noise for exploration							Acrobot-v1		1.0	1.0	1e-1	1e-1	1e-1	1e-1
Table 7: Table of best hyperparameters for PSSVFs and PSVFs using stochastic policies	Algo:	PSSVF		PSVF	Learning rate policy	Policy:	[]	[64,64]	[]	[64,64]	Metric:	avg	avg	avg	avgAcrobot-v1		1e-2	1e-2	1e-2	1e-3MountainCarContinuous-v0		1e-2	1e-3	1e-2	1e-3CartPole-v1		1e-3	1e-4	1e-2	1e-3Swimmer-v3		1e-2	1e-4	1e-3	1e-4InvertedPendulum-v2		1e-3	1e-4	1e-2	1e-3Reacher-v2		1e-4	1e-3	1e-2	1e-2Hopper-v3		1e-4	1e-4	1e-3	1e-4Learning rate critic					Acrobot-v1		1e-2	1e-4	1e-4	1e-2MountainCarContinuous-v0		1e-2	1e-2	1e-3	1e-3CartPole-v1		1e-2	1e-3	1e-2	1e-2Swimmer-v3		1e-2	1e-3	1e-3	1e-4InvertedPendulum-v2		1e-3	1e-3	1e-3	1e-2Reacher-v2		1e-3	1e-3	1e-3	1e-3Hopper-v3		1e-3	1e-2	1e-2	1e-4Noise for exploration					
Table 8: Table of best hyperparameters for PAVFs using deterministic policiesLearning rate policy	Policy: Metric:	[]		[32]		[64,64]			avg	last	avg	last	avg	lastMountainCarContinuous-v0		1e-2	1e-3	1e-3	1e-4	1e-4	1e-4Swimmer-v3		1e-3	1e-3	1e-3	1e-3	1e-3	1e-3InvertedPendulum-v2		1e-2	1e-3	1e-3	1e-4	1e-4	1e-4Reacher-v2		1e-3	1e-3	1e-4	1e-4	1e-4	1e-4Hopper-v3		1e-3	1e-4	1e-4	1e-4	1e-4	1e-3Learning rate critic							MountainCarContinuous-v0		1e-4	1e-4	1e-4	1e-3	1e-4	1e-3Swimmer-v3		1e-4	1e-4	1e-4	1e-4	1e-4	1e-4InvertedPendulum-v2		1e-3	1e-2	1e-2	1e-4	1e-2	1e-3Reacher-v2		1e-3	1e-3	1e-3	1e-2	1e-3	1e-3Hopper-v3		1e-4	1e-3	1e-3	1e-2	1e-4	1e-3Noise for exploration							MountainCarContinuous-v0		1.0	1e-1	1e-1	1e-1	1e-1	1e-1Swimmer-v3		1.0	1.0	1.0	1.0	1.0	1.0InvertedPendulum-v2		1.0	1.0	1e-1	1e-1	1e-1	1e-1Reacher-v2		1e-1	1e-1	1e-1	1.0	1.0	1.0Hopper-v3		1.0	1.0	1e-1	1e-1	1e-1	1.0
Table 9: Table of best hyperparameters for DDPGLearning rate policy	Policy: Metric:	[]		[32]		[64,64]			avg	last	avg	last	avg	lastMountainCarContinuous-v0		1e-2	1e-2	1e-2	1e-4	1e-3	1e-3Swimmer-v3		1e-3	1e-3	1e-2	1e-2	1e-2	1e-2InvertedPendulum-v2		1e-4	1e-4	1e-3	1e-3	1e-3	1e-4Reacher-v2		1e-4	1e-3	1e-2	1e-2	1e-3	1e-3Hopper-v3		1e-2	1e-2	1e-2	1e-4	1e-2	1e-2Learning rate critic							MountainCarContinuous-v0		1e-4	1e-4	1e-4	1e-3	1e-3	1e-3Swimmer-v3		1e-3	1e-3	1e-3	1e-3	1e-2	1e-3InvertedPendulum-v2		1e-3	1e-3	1e-3	1e-4	1e-3	1e-3Reacher-v2		1e-3	1e-3	1e-3	1e-3	1e-3	1e-3Hopper-v3		1e-3	1e-3	1e-4	1e-4	1e-4	1e-4Noise for exploration							MountainCarContinuous-v0		1e-2	1e-2	1e-2	1e-1	1e-1	1e-1Swimmer-v3		1e-1	1e-1	1e-2	1e-2	1e-2	1e-1InvertedPendulum-v2		1e-1	1e-1	1e-2	1e-2	1e-2	1e-2Reacher-v2		1e-1	1e-2	1e-1	1e-1	1e-1	1e-1Hopper-v3		1e-1	1e-1	1e-1	1e-2	1e-1	1e-2
