Table 1: GLUE dev set results. We report the results of our BERT12 teacher model, the 6-layerDistilBERT, and 3- and 6-layer MixKD student models with various ablations. DistilBERT resultstaken from Sanh et al. (2019). For MRPC and QQP, we report F1/Accuracy.
Table 3: GLUE test server results. We show results for the full variants of the 3- and 6-layer MixKDstudent models (SM+TMKD+BT). Knowledge distillation (KD) and Patient Knowledge Distillation(PKD) results are from Sun et al. (2019a).
Table 2: Computation coSt compariSon of teacherand Student modelS on SST-2 with batch Size of16 on a Nvidia TITAN X GPU.
Table 4: We compare our approach with the dataaugmentation module proposed by TinyBert (Jiaoet al., 2019).
Table 5: Mean and variance reported for BERT6-TMKD+BT,BERT6-SM+TMKD+BT,BERT3-TMKD+BT and BERT3-SM+TMKD+BT.
