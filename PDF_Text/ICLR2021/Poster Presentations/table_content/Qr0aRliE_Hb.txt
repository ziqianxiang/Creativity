Table 1: Comparison of quantization results. (Comp: compression ratio; Acc ∆: accuracy loss; ’-’:no results reported in the original paper)	ResNet20		AlexNet		ResNet50		ComP	Acc ∆	ComP	Acc ∆	ComP	Acc ∆ADRL (i.e., P-ADRL)	10.3X	0	10.1X	0	10.0X	0HAQ (Wang et al., 2018)	10.1X	0.41	10.8X	0	10.1X	1.168ReLeQ (Yazdanbakhsh et al., 2018)	1.88X	0.12	3.56X	0.08	-	-HAWQ (Dong et al., 2019)	13.1X	0.15	-	-	12.8X	1.91ZeroQ (Cai et al., 2020)	-	-	-	-	8.4X	1.64Compression Ratios and Accuracy Table 1 compares the compression ratios and accuracy. Forthe effects of the augmentation of the approximator, ADRL is the only method that delivers acomparable compression ratio without causing any accuracy loss on all three networks. Note thatADRL is more as a complement rather than a competitor to these work: The augmentation strategy itintroduces can help complement the approximators in the existing DRL especially when they are notyet well trained.
Table 2: Execution time of each stage of original HAQ, HAQ with early termination (HAQ+E) andprofiling based augmented DRL (P-ADRL) and the speedups of HAQ+E and P-ADRL over HAQ.
