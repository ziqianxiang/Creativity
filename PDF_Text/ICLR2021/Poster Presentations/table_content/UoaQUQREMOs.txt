Table 1: Two design principles to build effective video representation and efficient convolution.
Table 2: Ablation studies on Something-Something V1. All models use ResNet50 as the backboneWe follow the same strategy in Non-local (Wang et al., 2018) to pre-process the frames and take 3crops of 256 × 256 as input. Because some multi-clip models in Table 3 and Table 4 sample cropsof 256× 256, we simply multiply the GFLOPs reported in the corresponding papers by (256/224)2for a fair comparison. When considering efficiency, we use just 1 clip per video and the final crop isscaled to 256 × 256 to ensure comparable GFLOPs.
Table 3: Comparison with the state-of-the-art on Something-Something V1&V2. Our CT-Net16f outperforms all the single-clip models in Something-Something and even better than most ofthe multi-clip models. And our CT-NetEN outperforms all methods with very lower computation.
Table 4: Comparison with the state-of-the-art on Kinetics-400. It shows that CT-Net-R5016f cansurpass all existing lightweight models and even SlowFast-R5040f . When fusing different models,our model is 2.4× faster than SlowFast-R10180f and shows an 0.9% performance gain.
Table 5: Comparison results on UCF101 and HMDB51.
Table 6: More results on Something-Something V1&V2.
Table 7: More ablation studies on Mini-Kinetics and Something-Something V2.
Table 8: Adapting different pre-trained ImageNet architectures as CT-Net.
