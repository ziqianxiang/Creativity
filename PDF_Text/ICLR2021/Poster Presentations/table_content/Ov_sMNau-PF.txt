Table 1: Pearson and Spearman correlation (x100) on various unsupervised semantic textual simi-larity tasks.
Table 2: Pearson and Spearman correlation (x100) on the STS-b test set.
Table 3: Pearson / Spearman correlation (x100) on the STS test sets of various languages.
Table 4: Unsupervised Pearson / Spearman correlation (x100) on the STS-b test set when performingCT with various corpora.
Table 5: Step-wise learning schedule applied for all training with Contrastive Tension.
Table 6: Pearson and Spearman correlation (x100) on various unsupervised semantic textual simi-larity tasks.
Table 7: Min, max and mean difference between CT paired models in Pearson and Spearman corre-lation (x100) in regards to the mean score of the unsupervised semantic textual similarity tasks.
Table 8: Results on the downstream tasks supplied with the SentEval package. For the semanticrelated tasks SICK-R and STS-b, the Pearson correlation (x100) is reported.
Table 9: Results on the fine grained analysis tasks tasks supplied with the SentEval package.
Table 10: GLUE Test results, returned by the GLUE evaluation server (https://gluebenchmark.com/leaderboard). Following Devlin et al. (2019), the WNLI set has beenexcluded from the computation of the average score. F1 score is reported for QQP and MRPC,Spearman correlation (x100) for STS-b and accuracy is reported for the rest of the tasks.
Table 11: Model checkpoints used in these experiments.
Table 12: Wikipedia dumps used in these experiments.
