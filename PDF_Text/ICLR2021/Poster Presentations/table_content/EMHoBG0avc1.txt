Table 1: Retrieval performance in recall at k retrieved passages and precision/recall/F1.
Table 2: HotpotQA reranked retrieval results (input passages for final answer prediction).		Method	SPEM	Ans RecallSemantic Retrieval	63.9	77.9Graph Rec Retriever	75.7	87.5MDR (direct)	65.9	75.4MDR (reranking)	81.2	88.2Table 3: Retriever Model Ablation on HotpotQAretrieval. Single-hop here is equivalent to the DPRmethod (Karpukhin et al., 2020).
Table 3: Retriever Model Ablation on HotpotQAretrieval. Single-hop here is equivalent to the DPRmethod (Karpukhin et al., 2020).
Table 4: Comparison with decomposed dense retrievalwhich uses oracle question decomposition (test on100 bridge questions). See text for details about thedecomposed settings.
Table 5: HotPotQA-fullwiki test results.
Table 6: Reader comparison on HotpotQA dev set.
Table 7: Multi-Evidence FEVER Fact Verification Results. Loose-Multi represents the subset thatrequires multiple evidence sentences. Strict-Multi is a subset of Loose-Multi that require multipleevidence Sentences from different documents._________________________________Method	LooSe-MUlti(1,960)		Strict-Multi (1,059)		LA	FEVER	LA	FEVERGEAR	66.4	38.0	-	-GAT	66.1	38.2	-	-KGAT with ESIM rerank	65.9	39.2	51.5	7.7KGAT with BERT rerank	65.9	40.1	51.0	6.2Ours + KGAT with BERT rerank	77.9	42.0	72.1	16.2Inference Efficiency To compare with exist-ing multi-hop QA systems in terms of efficiency,we follow Dhingra et al. (2020) and measurethe inference time with 16 CPU cores and batchsize 1. We implement our system with a fastapproximate nearest neighbor search method,i.e., HNSW (Malkov & Yashunin, 2018), whichachieves nearly the same performance as exactsearch. With an in-memory index, we observethat the retrieval time is negligible compared
Table 8: Error cases where our model predicts a passage sequence that is also correct. ImportantClUes are marked in blue.________________________________________________________________Q: What languages did the son of Sacagawea speak?Ground-truth SP Passage Titles: Charbonneau, Oregon; Jean BaPtiste CharbonneaUPredicted:1.	Museum of Human Beings: Museum of Human Beings, included in the National AmericanIndian Heritage Month Booklist, November 2012 and 2013 is a novel written by Colin Sargent,which delves into the heart-rending life of Jean-BaPtiste Charbonneau, the son of Sacagawea.
Table 9: Sampled retrieval errors (marked in red) only made by the decomposed system. These errorscould be potentially avoided if the model has access to the full information in the original question orprevious hop results. The important clue for correctly retrieving the documents or avoiding errors ismarked in blue. Once decomposed, the marked information are not longer available in one of thedecomposed retrieval hop.
Table 10: Answer EM using top 50 re-trieved passage chainsModel	Overall	Comp (20%)	Bridge (80%)ELECTRA	61.7	79.0	57.4FiD	61.7	75.3	58.3hypothesize the worse performance of multi-hop RAG compared to FiD is partially due to thesmaller pretrained model used in RAG, i.e., BART is only half the size of T5-large. Also, as RAGback-propagate the gradients to the query encoder, it needs more memory footprint and can onlytake in fewer retrieved contexts. Our RAG implementation largely follows the implementation of theoriginal paper and we did not use the PyTorch checkpoint (as used by FiD) to trade computation formemory. We conjecture the multi-hop RAG performance will also improve if we augment the currentimplementation with memory-saving tricks. However, given the same amount of context and readmodel size, the multi-hop RAG is still worse than the extractive ELECTRA reader, i.e., with only thetop 1 retrieved passage sequence, our ELECTRA reader gets 53.8 EM compared to the 51.2 answerEM achieved by multi-hop RAG when using more context.
Table 11: Hyperparameters of Retrieverlearning rate	2e-5batch size	150maximum passage length	300maximum query length at initial hop	70maximum query length at 2nd hop	350warmup ratio	0.1gradient clipping norm	2.0traininig epoch	50weight decay	0Table 12: Hyperparameters of Extractive Reader (ELECTRA)learning rate	5e-5batch size	128maximum sequence length	512maximum answer length	30warmup ratio	0.1gradient clipping norm	2.0traininig epoch	7weight decay	0# of negative context per question	5
Table 12: Hyperparameters of Extractive Reader (ELECTRA)learning rate	5e-5batch size	128maximum sequence length	512maximum answer length	30warmup ratio	0.1gradient clipping norm	2.0traininig epoch	7weight decay	0# of negative context per question	5weight of SP sentence prediction loss	0.02516Published as a conference paper at ICLR 2021B.2	Further Details about Reader ModelsB.2.1	Extractive ReaderThe extractive reader is trained with four loss functions. With the [CLS] token, we predict areranking score based on whether the passage sequence match the groundtruth supporting passages.
Table 13: Comparing the unified retrieval model with models specifically trained for each task. Wetest the retrieval performance with a single merged corpus. For easy comparison, all three models arebased on BERT-base encoder which we find achieves stronger performance than RoBERTa-base onNQ. AR@K denotes answer recall at top-K retrieved passage sequences.
