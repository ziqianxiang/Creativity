Table 1: Average of cumulative rewards under SAC and TD3 on MuJoCo Environments after 500,000training steps across five instances with random seeds. Bold values represent the highest results,and the number in a bracket indicates the improvement due to NERS, compared to that of the bestbaseline on each environment.
Table 2: Average of cumulative rewards under Rainbow on each Atari environments after 100,000training steps across five instances. Bold values represent the highest results, and the number ina bracket indicates the improvement due to NERS, compared to that of the best baseline on eachenvironment.
Table 3: Sampled transitions’ statistical values for Q-values and TD-errors on Pendulum-v0 underSAC at 10,000 training steps with initially 1,000 random actions. Here, STDEV and AVG mean thestandard deviation and the average, respectively. PER has the highest STDEV of TD-errors but lowestSTDEV of Q-errors. NERS has higher STDEV of both TD-errors and Qvalues than RANDOM andERO.
Table A.1: Dimensions of observation and action spaces for continuous control environments14Published as a conference paper at ICLR 2021A.3 Discrete control environmentTo evaluate sampling methods under Rainbow Hessel et al. (2018), we consider the following Atarienvironments. RL agents should learn their policy by observing the RGB screen to acheive highscores for each game.
Table B.1: Hyper-parameters5β increases to 1.0 by the rule β = 0.4η + 1.0(1 - η), where η = the current step/the maximum steps.
