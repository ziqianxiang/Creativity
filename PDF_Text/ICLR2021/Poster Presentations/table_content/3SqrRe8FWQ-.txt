Table 1: Average overflow rate (in 8 bits) of each layer for a low-precision network and correspond-ing test accuracy using either 32-bit or 8-bit accumulators during inference on CIFAR10.
Table 2: Results for different transition slopes for cyclic function; * denotes divergence.
Table 4: Results for fine-tuning with theoverflow penalty (Ro).
Table 3: Results for different quantization step-sizesbased on overflow rate p(%). * denotes divergence.
Table 5: Results for adaptation to bit-packing with 8-bit accumulator. (v) denotes no carry contami-nation as in a vector instruction; (c) denotes carry propagation between different numbers.
Table 6: Top-1 test accuracy for both CIFAR-10 and ImageNet with different architectures. Here,“Acc” represents accumulator, and “QA” represents quantized activation.
Table 7: Time cost (ms) for typical 3 × 3 con-volution kernels in ResNet using different ac-cumulator bitwidths.
Table 8: Time cost (ms) for 3× 3 convolution ker-nels in ResNet with no vector instructions usingbit packing.
Table 9: Results for different types of cyclic activationCyclic Function	ReLU	slope k	Accuracy(%)Proposed	√	2	90.52Proposed		2	89.28ReLU-like	√	1	90.25ReLU-like	√	2	90.31ReLU-like	√	3	90.15ReLU-like		1	88.62ReLU-like		2	89.01ReLU-like		3	88.53Absolute	√	—	90.17Absolute		—	89.19B.2	Full Overflow Penalty ResultsTable 10 shows the results for fine-tuning our WrapNet with different coefficients for the overflowpenalty. When applying the overflow penalty, the overflow rate decreases and we can achieve a13Published as a conference paper at ICLR 2021higher accuracy. In addition, when we apply the regularizer to a network with low-resolution accu-mulators that does not use our cyclic activation, the network still suffers from performance degra-dation unless a large coefficient is used. However, a strong penalty kills almost all of the overflow,
Table 10: Comparison for fine-tuning network without cyclic activation and our WrapNet, withoverflow penalty Ro.
Table 11: Hardware implementation results for one multiply-accumulate (MAC) unit in 28nmCMOSBits			Cyclic act. slope k	Cycle time (ns)	Throughput (Gops)	Cell area (μm2)	Power (mW)Act.	Weight	Acc.					8	8	32	一	0.31	6.5	1 298	2.783	1	32	一	0.29	7.0	732	1.908	8	8	2	0.24	8.3	521	1.608	8	8	4	0.25	8.1	523	1.643	1	8	2	0.24	8.3	290	0.933	1	8	4	0.24	8.3	285	0.87Table 12: Area breakdown of one multiply-accumulate (MAC) unit in 28nm CMOSBits	Cyclic act.	Cell area efficiency (μm2/GoPS)Act.	Weight	Acc.	slope k	Multiplier	Accumulator	Cyclic act.	Auxiliary	Total8	8	32	—	96 (48%)	91 (46%)	—	12 (6%)	1993	1	32	—	3 (2%)	93 (89%)	—	9 (9%)	1058	8	8	2	31 (49%)	12 (19%)	10 (16%)	10 (16%)	638	8	8	4	33 (50%)	12 (19%)	8 (13%)	12 (18%)	653	1	8	2	2 (5%)	17 (46%)	15 (41%)	3 (8%)	373	1	8	4	2 (5%)	18 (52%)	12 (35%)	3 (8%)	35D	Using more weight bits
Table 12: Area breakdown of one multiply-accumulate (MAC) unit in 28nm CMOSBits	Cyclic act.	Cell area efficiency (μm2/GoPS)Act.	Weight	Acc.	slope k	Multiplier	Accumulator	Cyclic act.	Auxiliary	Total8	8	32	—	96 (48%)	91 (46%)	—	12 (6%)	1993	1	32	—	3 (2%)	93 (89%)	—	9 (9%)	1058	8	8	2	31 (49%)	12 (19%)	10 (16%)	10 (16%)	638	8	8	4	33 (50%)	12 (19%)	8 (13%)	12 (18%)	653	1	8	2	2 (5%)	17 (46%)	15 (41%)	3 (8%)	373	1	8	4	2 (5%)	18 (52%)	12 (35%)	3 (8%)	35D	Using more weight bitsSince ARM provides arithmetic operations that handle multiplication between various 8-bit num-bers in parallel, we further conduct experiments in which more bits are used for weight quantization.
Table 13: Energy breakdown of one multiply-accumulate (MAC) unit in 28nm CMOSBits			Cyclic act. slope k	Energy efficiency (fJ/op)				Act.	Weight	Acc.		Multiplier	Accumulator	Cyclic act.	Auxiliary	Total8	8	32	—	144 (34%)	173 (40%)	—	111 (26%)	4283	1	32	—	10 (4%)	197 (73%)	—	64 (23%)	2718	8	8	2	48 (25%)	29 (15%)	17 (9%)	98 (51%)	1928	8	8	4	53 (26%)	28 (14%)	17 (8%)	105 (52%)	2033	1	8	2	8 (7%)	42 (37%)	23 (20%)	42 (36%)	1153	1	8	4	6(6%)	42 (40%)	24 (23%)	33 (31%)	105Table 14 displays the classification accuracy, as well as the overflow rate of the final models. Sur-prisingly, in some cases, we may have a lower overflow rate even when using more bits for theweight quantization. We also collect the accuracy degradation from the full precision network. Ourresults show that the best performance is achieved when we use 4-bit weights, which is close to thefull-precision result (around 0.7% degradation).
Table 14: Results for WrapNet with more bits for weight quantization, where we use ternary weightsfor 2-bit.
