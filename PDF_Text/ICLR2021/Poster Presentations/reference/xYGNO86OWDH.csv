title,year,conference
 Intrinsic dimension of datarePresentations in deeP neural networks,2019, In Advances in Neural Information Processing Systems
 Cross-lingual language model Pretraining,2019, In Advances inNeural Information Processing Systems
 A cluster seParation measure,1979, IEEE transactions on patternanalysis and machine intelligence
 Bert: Pre-training of deePbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 A density-based algorithm fordiscovering clusters in large sPatial databases with noise,1996, In Kdd
 RePresentation degenerationProblem in training natural language generation models,2019, In International Conference on LearningRepresentations
 Generalized exPansion dimension,2012, In 2012IEEE 12th International Conference on Data Mining Workshops
 ImProving bilingual lexicon induction for lowfrequency words,2020, In Proceedings of the 2020 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP)
 Billion-scale similarity search with gpus,2017, arXivpreprint arXiv:1702
 Unsupervised post-processing of word vectors via conceptornegation,2019, In Proceedings of the AAAI Conference on Artificial Intelligence
 Characterizing adversarial subspaces using localintrinsic dimensionality,2018, In International Conference on Learning Representations
 Building a large annotatedcorpus of english: The penn treebank,1993, 1993
 Pointer sentinel mixturemodels,2016, arXiv preprint arXiv:1609
 Efficient estimation of word representa-tions in vector space,2013, arXiv preprint arXiv:1301
 Distributed representationsof words and phrases and their compositionality,2013, In Advances in neural information processingsystems
 The strange geometry of skip-gram with negative sampling,2017, InEmpirical Methods in Natural Language Processing
 All-but-the-top: Simple and effective post-processing for wordrepresentations,2018, In 6th International Conference on Learning Representations
 Glove: Global vectors for wordrepresentation,2014, In Proceedings of the 2014 conference on empirical methods in natural languageprocessing (EMNLP)
 Deep contextualized word representations,2018, arXiv preprint arXiv:1802
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 Visualizing and measuring the geometry of bert,2019, In Advances in Neural InformationProcessing Systems
 Ernie: Enhanced representation through knowledge integration,2019, arXivpreprint arXiv:1904
