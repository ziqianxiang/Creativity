title,year,conference
 Fast randomized kernel ridge regression with statisticalguarantees,2015, In Advances in Neural Information Processing Systems (NeurIPS)
 Sharp analysis of low-rank kernel matrix approximations,2013, In Proceedings of the 26thAnnual Conference on Learning Theory Conference on Learning Theory (COLT)
 Convergence rates of kernel conjugate gradient for randomdesign regression,2016, Analysis and Applications
 Nytro: When sub-sampling meets early stopping,2016, In Proceedings of the 19th International Conference on ArtificialIntelligence and Statistics (AISTATS)
 On the impact of kernel approximation onlearning accuracy,2010, In Proceedings of the 13th International Conference on Artificial Intelligenceand Statistics (AISTATS)
 Communication-efficient accurate statisticalestimation,2019, arXiv preprint arXiv:1906
 Learning theory of distributed spectralalgorithms,2017, Inverse Problems
 Random design analysis of ridge regression,2012, InConference on learning theory
 Implicitregularization of random feature models,2020, In Proceedings of the 37th International Conference onMachine Learning (ICML)
 Nystrom type subsampling analyzed asa regularized projection,2017, Inverse Problems
 Communication-efficient distributed optimiza-tion in networks with gradient tracking,2019, arXiv preprint arXiv:1909
 Communication-efficient distributed optimiza-tion in networks with gradient tracking and variance reduction,2020, In International Conference onArtificial Intelligence and Statistics (AISTATS)
 Distributed learning with random features,2019, arXiv preprintarXiv:1906
 Multi-class learning using unlabeled samples:Theory and algorithm,2019, In Proceedings of the 28th International Joint Conference on ArtificialIntelligence (IJCAI)
 Approximate manifold regularization: scalablealgorithm and generalization analysis,2019, In Proceedings of the 28th International Joint Conferenceon Artificial Intelligence (IJCAI)
 Optimal distributed learning with multi-pass stochastic gradientmethods,2018, In Proceedings of the 35th International Conference on Machine Learning (ICML)
 Distributed kernel ridge regression with communica-tions,2020, arXiv preprint arXiv:2003
 Eigenvalues ratio for kernel selection of kernel methods,2015, In Proceedingsof the 29th AAAI Conference on Artificial Intelligence (AAAI 2015)
 Eigenvalues perturbation of integral operator for kernel se-lection,2013, In Proceedings of the 22nd ACM International Conference on Information and KnowledgeManagement (CIKM)
 Efficient approximation of cross-Validation for kernelmethods using Bouligand influence function,2014, In Proceedings of the 31st International Conferenceon Machine Learning (ICML)
 Infinite kernel learning:Generalization bounds and algorithms,2017, In Proceedings of the 31st AAAI Conference on ArtificialIntelligence (AAAI)
 Fast cross-Validation for kernel-based algorithms,2020, IEEE Transactions on Pattern Analysis and MachineIntelligence
 Recursive sampling for the Nystrom method,2017, In Advancesin Neural Information Processing Systems (NeurIPS)
 Random features for large-scale kernel machines,2007, In Advances inNeural Information Processing Systems (NeurIPS)
 Weighted sums of random kitchen sinks: Replacing minimizationwith randomization in learning,2008, In Advances in Neural Information Processing Systems (NeurIPS)
 Decentralised learning with randomfeatures and distributed gradient descent,2020, In Proceedings of the 37th International Conference onMachine Learning (ICML)
 On the sample complexity of subspacelearning,2013, In Advances in Neural Information Processing Systems
 Less is more: Nystrom computationalregularization,2015, In Advances in Neural Information Processing Systems (NeurIPS)
 Falkon: An optimal large scale kernelmethod,2017, In Advances in Neural Information Processing Systems (NeurIPS)
 On fast leverage scoresampling and optimal learning,2018, In Advances in Neural Information Processing Systems (NeurIPS)
 An Introduction to Support Vector Machines and otherKernel-based Learning Methods,0521, Cambridge University Press
 Computationally efficient nystrom approximation usingfast transforms,2016, In Proceedings of the 33rd International Conference on Machine Learning (ICML)
 Shannon sampling ii: connections to learning theory,2005, Appliedand Computational Harmonic Analysis
 Learning theory estimates via integral operators and theirapproximations,2007, Constructive Approximation
 Support Vector Machines,2008, Springer Verlag
 Distributed meanestimation with limited communication,2017, In Proceedings of the 34th International Conference onMachine Learning (ICML)
 The Nature of Statistical Learning Theory,2000, Springer Verlag
 Divide-and-conquer learning withNystrom: optimal rate and algorithm,2020, In Proceedings ofthe 34th AAAI Conference on ArtificialIntelligence (AAAI)
 Orthogonal random features,2016, In Advances in Neural Information ProcessingSystems (NeurIPS)
 Divide and conquer kernel ridge regression,2013, InProceedings of the 26th Annual Conference on Learning Theory (COLT)
 Thefollows hold:â–¡II,2021,fM 
