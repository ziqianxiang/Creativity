title,year,conference
 High-dimensional dynamics of generalization error inneural networks,2017, arXiv:1710
 Reconciling modern machine-learning practice and the classical bias-variance trade-off,2019, Proceedings of the National AcademyOfSciences
 Around the circular law,2012, Probability surveys
 Convex optimization,2004, Cambridge university press
 Geometric Measure Theory,1969, Springer
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Matrix Computations,1989, The Johns Hopkins UniversityPress
 A vest of the pseudoinverse learning algorithm,2018, arXiv:1805
 Surprises in high-dimensional ridgeless least squares interpolation,2019, arXiv:1903
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Trends in extreme learning machines:A review,2015, Neural Networks
 Learning capability and storage capacity of two-hidden-layer feedforward net-works,2003, IEEE Transactions on Neural Networks
 Extreme learning machine: theory andapplications,2006, Neurocomputing
 Self-normalizingneural networks,2017, In Advances in neural information processing systems
 An overview of periodic elliptic operators,2015, arXiv:1510
 Just interpolate: Kernel “ridgeless” regression can gener-alize,2020, Annals of Statistics
 Multivariate analysis,1979, Probability and mathematicalstatistics
 The generalization error of random features regression: Preciseasymptotics and double descent curve,2019, arXiv:1908
 The zero set ofa real analytic function,2015, arXiv:1512
 Deepdouble descent: Where bigger models and more data hurt,2019, arXiv:1912
 Optimal regularization canmitigate double descent,2020, arXiv:2003
 Construction of a Hilbert curve onthe sphere with an isometric parameterization of area,2009, Office Note
 Random features for large-scale kernel machines,2008, In Advances inneural information processing systems
 Consistency of interpolation with laplace kernels is a high-dimensional phenomenon,2019, In Conference on Learning Theory
 On the Convergence of Adam and Beyond,2018, InInternational Conference on Learning Representations
 A simple method to derive bounds on the size and totrain multilayer neural networks,1991, IEEE transactions on neural networks
 Randomness in neural networks: An overview,2017, WileyInterdisciplinary Reviews: Data Mining and Knowledge Discovery
 The No-Prop algorithm: Anew learning algorithm for multilayer neural networks,2013, Neural Networks
 Understandingdeep learning requires rethinking generalization,2016, arXiv:1611
 Jacotet al,2021, (2018)
