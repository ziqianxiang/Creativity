title,year,conference
 Emnist: an extension ofmnist to handwritten letters,2017, International Joint Conference on Neural Networks (IJCNN)
 Large scaledistributed deep networks,2012, Conference on Neural Information Processing Systems (NIPS)
 Improving data aug-mentation for medical image segmentation,2020, Conference on Medical Imaging with Deep Learning(MIDL)
 Model inversion attacks that exploit confi-dence information and basic countermeasures,2015, Proceedings of the 22nd ACM SIGSAC Conferenceon Computer and Communications Security
 Augmenting data with mixup for sentence classifi-cation: An empirical study,2019, CoRR
 Deep residual learning for image recog-nition,2016, Conference on Conference on Computer Vision and Pattern Recognition (CVPR)
 The non-iid data quagmire ofdecentralized machine learning,2020, International Conference on Machine Learning (ICML)
 Advances and openproblems in federated learning,2019, ArXiv
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 On the convergence offedavg on non-iid data,2020, International Conference on Learning Representations (ICLR)
 Learning differentially privaterecurrent language models,2018, International Conference on Learning Representations (ICLR)
 Very deep convolutional networks for large-scale imagerecognition,2015, International Conference on Learning Representations (ICLR)
 Federated multi-task learn-ing,2017, Conference on Neural Information Processing Systems (NIPS)
 Overcomingnoisy and irrelevant data in federated learning,2020, International Conference on Pattern Recognition(ICPR)
 Manifold mixup: Better representations by interpolatinghidden states,2018, International Conference on Machine Learning (ICML)
 Speech commands: A dataset for limited-vocabulary speech recognition,2018, ArXiv
 Aggregated residual trans-formations for deep neural networks,2017, Conference on Computer Vision and Pattern Recognition(CVPR)
 Federated con-tinual learning With adaptive parameter communication,2020, ArXiv
 Bayesian nonparametric federated learning of neural netWorks,2019, InternationalConference on Machine Learning (ICML)
 mixup: Beyond empiri-cal risk minimization,2018, International Conference on Learning Representations (ICLR)
 Federatedlearning With non-iid data,2018, arXiv preprint arXiv:1806
 Modified VGGnetis consisted of6 convolutional layers with 3 max pooling layers,2020, 3x3 conv layers are stacked and 2x2maxpool layer is stacked after every 2 conv layers
01 and learning decay rate per round0,2021,999
