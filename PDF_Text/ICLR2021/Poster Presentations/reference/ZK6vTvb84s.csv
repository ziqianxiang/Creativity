title,year,conference
 Neural machine translation by jointlylearning to align and translate,2015, In International Conference on Learning Representations (ICLR)
 Non-Local Means Denoising,2011, ImageProcessing On Line
 Biological sequence modeling with convolutionalkernel networks,2019, Bioinformatics
 Recurrent kernel networks,2019, In Advances in NeuralInformation Processing Systems (NeurIPS)
 Improving textual network embedding with globalattention via optimal transport,2019, In Proceedings of the Annual Meeting of the Association forComputational Linguistics (ACL)
 Sinkhorn distances: Lightspeed computation of optimal transport,2013, In Advances inNeural Information Processing Systems (NeurIPS)
 Fast computation of wasserstein barycenters,2013, In InternationalConference on Machine Learning (ICML)
 Transformer-xl:Attentive language models beyond a fixed-length context,2019, In Proceedings of the Annual Meetingof the Association for Computational Linguistics (ACL)
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the North AmericanChapter of the Association for Computational Linguistics (NAACL)
 Learning generative models with sinkhorn diver-gences,2018, In International Conference on Artificial Intelligence and Statistics (AISTATS)
 Deepsf: deep convolutional neural network for map-pingprotein sequences to folds,2019, Bioinformatics
 Reformer: The efficient transformer,2020, InInternational Conference on Learning Representations (ICLR)
 Sliced wasserstein kernels for probability distri-butions,2016, In Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)
 Wasserstein em-bedding for graph learning,2021, In International Conference on Learning Representations (ICLR)
 Mercer kernels for object recognition with local features,2004, In Conference on ComputerVision and Pattern Recognition (CVPR)
 Computational optimal transport,2019, Foundations and Trends inMachine Learning
 Wasserstein barycenter and its applica-tion to texture mixing,2011, In International Conference on Scale Space and Variational Methods inComputer Vision (SSVM)
 Fixed encoder self-attention patterns intransformer-based machine translation,2020, In arXiv preprint arXiv: 2002
 Biological structure and function emerge from scaling unsupervised learningto 250 million protein sequences,2019, In bioRxiv 622803
 Rep theset: Neural networks for learning set representations,2020, In International Conference on ArtificialIntelligence and Statistics (AISTATS)
 Recursive deep models for semantic compositionality over a sentimenttreebank,2013, In Conference on Empirical Methods in Natural Language Processing (EMNLP)
 To aggregate or not to aggregate: Selectivematch kernels for image search,2013, In Proceedings of the International Conference on ComputerVision (ICCV)
 Transformer dissection: A unified understanding of transformerâ€™s attention via thelens of kernel,2019, In Conference on Empirical Methods in Natural Language Processing (EMNLP)
 Attention is all you need,2017, In Advances in Neural InformationProcessing Systems (NeurIPS)
 Optimal Transport: Old and New,2008, SPringer
 Linformer: Self-attentionwith linear comPlexity,2020, In arXiv preprint arXiv: 2006
 Non-local neural networks,2017, InProceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)
 Hard-coded gaussian attention for neural machine trans-lation,2020, In Proceedings of the Annual Meeting of the Association for Computational Linguistics(ACL)
 Using the nystrom method to speed up kernelmachines,2001, In Advances in Neural Information Processing Systems (NeurIPS)
 Deep sets,2017, In Advances in Neural Information Processing Systems (NeurIPS)
 Improved nystrom low-rank approximation anderror analysis,2008, In International Conference on Machine Learning (ICML)
 Predicting effects of noncoding variants with deep learning-based sequence model,2015, Nature methods
 The hyperparameters for CKN and RKN can be found in Chen et al,2021, (2019b)
