title,year,conference
 Provable limitations of deep learning,2018, arXiv preprintarXiv:1812
 Nearly-tight vc-dimensionand pseudodimension bounds for piecewise linear neural networks,2019, J
 Spectral networks and locallyconnected networks on graphs,2013, arXiv preprint arXiv:1312
 Globally optimal gradient descent for a convnet with gaussianinputs,2017, arXiv preprint arXiv:1702
 Locally-connected and convolutional neural networks for small footprint speaker recog-nition,2015, In Sixteenth Annual Conference of the International Speech Communication Association
 Inductive bias of deep convolutional networks through poolinggeometry,2016, arXiv preprint arXiv:1605
 Analysisand design of convolutional networks via hierarchical tensor decompositions,2017, arXiv preprintarXiv:1705
 Learning parities with neural networks,2020, arXiv preprintarXiv:2002
 Gradient descent learnsone-hidden-layer cnn: Donâ€™t be afraid of spurious local minima,2018, In International Conference onMachine Learning
 Neural architecture search: A survey,2018, arXivpreprint arXiv:1808
 Convolution by evolution: Differentiable pattern produc-ing networks,2016, In Proceedings of the Genetic and Evolutionary Computation Conference 2016
 Quantifying translation-invariance in convolutional neural networks,2017, arXivpreprint arXiv:1801
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in neural information processing systems
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In Advances in Neural Information Processing Systems
 Lc-dnn:Local connection based deep neural network for indoor localization with csi,2020, IEEE Access
 A provably correct algorithm for deep learning that actuallyworks,2018, arXiv preprint arXiv:1803
 Deep learning and hierarchal generative models,2016, arXiv preprintarXiv:1612
 Towards learning convolutions from scratch,2020, arXiv preprint arXiv:2007
 Bayesian deep convolutional networks with manychannels are gaussian processes,2018, arXiv preprint arXiv:1810
 I-theory on depth vs width: hierarchicalfunction composition,2015, Technical report
 Weight sharing is crucial to succesfuloptimization,2017, arXiv preprint arXiv:1706
 Online learning and online convex optimization,2011, Foundations and trendsin Machine Learning
 Theoretical insights into the optimizationlandscape of over-parameterized shallow neural networks,2018, IEEE Transactions on InformationTheory
