title,year,conference
 Pre-trainingtasks for embedding-based large-scale retrieval,2020, arXiv preprint arXiv:2002
 BERT: Pre-training of deepbidirectional transformers for language understanding,2019, In Proc
 End-to-end retrieval in continuousspace,2018, arXiv preprint arXiv:1811
 Realm: Retrieval-augmented language model pre-training,2020, arXiv preprint arXiv:2002
 Poly-encoders: Trans-former architectures and pre-training strategies for fast and accurate multi-sentence scoring,2019, arXivpreprint arXiv:1905
 Billion-scale similarity search with gpus,2019, IEEETransactions on Big Data
 Triviaqa: A large scale distantlysupervised challenge dataset for reading comprehension,2017, In Proc
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 The NarrativeQA reading comprehension challenge,2018, TACL
 Pre-training via paraphrasing,2020, arXiv preprint arXiv:2006
 Retrieval-augmented gener-ation for knowledge-intensive nlp tasks,2020, arXiv preprint arXiv:2005
 Introduction to informationretrieval,2008, Cambridge university press
 An introduction to neural information retrieval,2018, Foundationsand TrendsÂ® in Information Retrieval
 Passage re-ranking with bert,2019, arXiv preprintarXiv:1901
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Okapi at TREC-3,1995, NIST Special Publication Sp
 Fever: a large-scaledataset for fact extraction and verification,2018, arXiv preprint arXiv:1803
