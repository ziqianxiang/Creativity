title,year,conference
 Layer normalization,2016, arXiv preprintarXiv:1607
 Neural machine translation by jointlylearning to align and translate,2015, In Yoshua Bengio and Yann LeCun (eds
 Diagnostic assessment of deep learning algorithms for detection oflymph node metastases in women with breast cancer,2017, Jama
 Long short-term memory-networks for machinereading,2016, arXiv preprint arXiv:1601
 Rotdcf: Decomposition ofconvolutional filters for rotation-equivariant deep networks,2018, arXiv preprint arXiv:1805
 Rethinking attentionwith performers,2020, arXiv preprint arXiv:2009
 Group equivariant convolutional networks,2016, In International conferenceon machine learning
 Spherical cnns,2018, CoRR
 A general theory of equivariant cnns on ho-mogeneous spaces,2019, In Advances in Neural Information Processing Systems
 Gauge equivariant convolu-tional networks and the icosahedral cnn,2019, arXiv preprint arXiv:1902
 Deepsphere: agraph-based spherical cnn,2020, In International Conference on Learning Representations
 Exploiting cyclic symmetry in convolu-tional neural networks,2016, arXiv preprint arXiv:1602
 Cross-domain3d equivariant image embeddings,2019, In International Conference on Machine Learning
 Generalizing convolutionalneural networks for equivariance to lie groups on arbitrary continuous data,2020, arXiv preprintarXiv:2002
 Dense steerable filter cnns for exploiting rotationalsymmetry in histology images,2020, arXiv preprint arXiv:2004
 Squeeze-and-excitation networks,2018, In Proceedings of the IEEEconference on computer vision and pattern recognition
 Lietransformer: Equivariant self-attention for lie groups,2020, arXiv preprint arXiv:2012
 On the generalization of equivariance and convolution in neuralnetworks to the action of compact groups,2018, arXiv preprint arXiv:1802
 An empiricalevaluation of deep architectures on problems with many factors of variation,2007, In Proceedings of the24th international conference on Machine learning
 Group equivariant capsule networks,2018, InAdvances in Neural Information Processing Systems
 Understanding the difficultyof training transformers,2020, arXiv PrePrint arXiv:2004
 Sgdr: Stochastic gradient descent with warm restarts,2016, arXivPrePrint arXiv:1608
 Scale equivariance in cnnswith vector fields,2018, arXiv PrePrint arXiv:1807
 Searching for activation functions,2017, arXiv PrePrintarXiv:1710
 Stand-alone self-attention in vision models,2019, arXiv PrePrint arXiv:1906
 Universal equivariant multilayer perceptrons,2020, arXiv PrePrint arXiv:2002
 Co-attentive equivariant neural networks: Focusingequivariance on transformations co-occurring in data,2019, arXiv PrePrint arXiv:1911
 Attentive groupequivariant convolutional networks,2020, arXiv PrePrint arXiv:2002
 Wavelet networks:Scale equivariant learning from raw waveforms,2020, arXiv PrePrint arXiv:2006
 Scale-equivariant steerable networks,2020, InInternational Conference on Learning RePresentations
 Equivariant transformer networks,2019, arXiv PrePrintarXiv:1901
 Tensor Field Networks: Rotation-and Translation-Equivariant Neural Networks for 3D PointClouds,2018, arXiv PrePrint arXiv:1802
 Attention is all you need,2017, In Advances in neural informationProcessing systems
 Building deep equivariantcapsule networks,2020, In International Conference on Learning Representations
 Linformer: Self-attention withlinear complexity,2020, arXiv preprint arXiv:2006
 Non-local neural networks,2018, InProceedings ofthe IEEE Conference on Computer Vision and Pattern Recognition
 3d steerable cnns:Learning rotationally equivariant features in volumetric data,2018, In Advances in Neural InformationProcessing Systems
 Learning steerable filters for rotationequivariant cnns,2018, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Cubenet: Equivariance to 3d rotation and translation,2018, InProceedings of the European Conference on Computer Vision (ECCV)
 Deep scale-spaces: Equivariance over scale,2019, arXiv preprintarXiv:1905
 Harmonicnetworks: Deep translation and rotation equivariance,2017, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Big bird: Transformers for longersequences,2020, arXiv preprint arXiv:2007
2 are only valid for unimodulargroups,2020, That is
 attention_dropout_rate andvalue_dropout_rate are both set to 0,2016,1
 attention_dropout_rate and value_dropout_rateare both set to 0,2016,1
 If the self-attention formulation provided in Eq,2021, 10
