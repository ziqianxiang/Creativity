title,year,conference
 Transferring inductive biases throughknowledge distillation,2020, arXiv preprint arXiv:2006
 Progressive reinforcement learn-ing with distillation for multi-skilled motion control,2018, In International Conference on LearningRepresentations
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 Phasic policy gradient,2020, arXiv preprintarXiv:2009
 Funnel-transformer: Filtering out sequen-tial redundancy for efficient language processing,2020, arXiv preprint arXiv:2006
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Long short-term memory,1997, Neural computation
 Meta reinforcement learning as task inference,2019, arXiv preprint arXiv:1905
 Population based train-ing of neural networks,2017, arXiv preprint arXiv:1711
 Scalable syntax-aware language models using knowledge distillation,2019, In Proceedings of the 57th Annual Meetingof the Association for Computational Linguistics
 Large memory layers with product keys,2019, In Advances in Neural Information ProcessingSystems
 Asynchronous methods for deep reinforcementlearning,2016, In International conference on machine learning
 Actor-mimic: Deep multitask andtransfer reinforcement learning,2015, In Proceedings of the International Conference on LearningRepresentations
 Stabilizingtransformers for reinforcement learning,2019, arXiv preprint arXiv:1910
 Asym-metric actor critic for image-based robot learning,2017, arXiv preprint arXiv:1710
 Languagemodels are unsupervised multitask learners,2019, 2019
 Hogwild: A lock-free approach toparallelizing stochastic gradient descent,2011, In Advances in neural information processing systems
 Proximal policyoptimization algorithms,2017, arXiv preprint arXiv:1707
 V-mpo: On-policy maximum a posteriori policy optimizationfor discrete and continuous control,2020, In International Conference on Learning Representations
 Reinforcement Learning: an Introduction,1998, MIT Press
 Efficient transformers: A survey,2020, arXivpreprint arXiv:2009
 Distral: Robust multitask reinforcement learning,2017, In Advances inNeural Information Processing Systems
 The roW of pixelsextends as far as the maze dimension,2021, If the agent has a Wall in its field of vieW (represented asblack pixels in Fig
