title,year,conference
 Spice: Semantic propositionalimage caption evaluation,2016, In ECCV
 Abductive commonsensereasoning,2019, arXiv preprint arXiv:1908
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Electra: Pre-training textencoders as discriminators rather than generators,2020, arXiv preprint arXiv:2003
 Commonsense reasoning and commonsense knowledge in artificialintelligence,2015, Communications of the ACM
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Unified language model pre-training for natural language understanding andgeneration,2019, In H
 Pytorch lightning,2019, GitHub
 Realm: Retrieval-augmented language model pre-training,2020, arXiv preprint arXiv:2002
 Robust disambiguation of namedentities in text,2011, In Proceedings of the 2011 Conference on Empirical Methods in Natural LanguageProcessing
 Albert: A lite bert for self-supervised learning of language representations,2019, arXiv preprintarXiv:1909
 Teaching pretrained models with commonsense reasoning:A preliminary kb-based approach,2019, arXiv preprint arXiv:1909
 Commongen: A constrained text generation challenge for generative commonsensereasoning,2020, Findings of EMNLP
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Learned in translation:Contextualized word vectors,2017, In Advances in Neural Information Processing Systems
 Distributed representationsof words and phrases and their compositionality,2013, In Advances in neural information processingsystems
 Bleu: a method for automaticevaluation of machine translation,2002, In Proceedings of the 40th Annual Meeting of the Association forComputational Linguistics
 Glove: Global vectors for wordrepresentation,2014, In Proceedings of the 2014 conference on empirical methods in natural languageprocessing (EMNLP)
 Deep contextualized word representations,2018, arXiv preprint arXiv:1802
 Knowledge enhanced contextual word representations,2019, arXiv preprintarXiv:1909
 Kilt: a benchmark forknowledge intensive language tasks,2020, arXiv preprint arXiv:2009
 Languagemodels are unsupervised multitask learners,2019, 2019
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Knowledge-aware language model pretraining,2020, arXiv preprint arXiv:2007
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Cider: Consensus-based imagedescription evaluation,2015, In Proceedings of the IEEE conference on computer vision and patternrecognition
 K-adapter: Infusing knowledge into pre-trained models with adapters,2020, arXivpreprint arXiv:2002
 Huggingfaceâ€™s transformers: State-of-the-artnatural language processing,2019, ArXiv
 Pretrained encyclopedia:Weakly supervised knowledge-pretrained language model,2020, In International Conference on LearningRepresentations
 Ernie: Enhancedlanguage representation with informative entities,2019, arXiv preprint arXiv:1905
