title,year,conference
 Analysis of sgd with biased gradient estimators,2020, arXivpreprint arXiv:2008
 The theory of probabilities,1946, Gastehizdat Publishing House
 The tradeoffs of large scale learning,2008, In Advances in neuralinformation processing systems
 Random forests,2001, Machine learning
 Born again trees,1996, University of California
 Information-theoretic under-standing of population risk improvement with model compression,2020, In AAAI
 Distilling knowledge from ensembles of neural networks forspeech recognition,2016, In Interspeech
 Knowledge distillation with feature mapsfor image classification,2018, In Asian Conference on Computer Vision
 On the efficacy of knowledge distillation,2019, In Proceedingsof the IEEE International Conference on Computer Vision
 Orthogonal statistical learning,2019, arXiv preprint arXiv:1901
 Ensemble distillation for neural machinetranslation,2017, arXiv preprint arXiv:1702
 White-to-black: Efficient distillationof black-box adversarial attacks,2019, arXiv preprint arXiv:1904
 Adversarially robust distillation,2020, InProceedings ofthe AAAI Conference on Artificial Intelligence
 Knowledge distillation: Asurvey,2020, arXiv preprint arXiv:2006
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Learning small-size dnn with output-distribution-based criteria,2014, In Fifteenth annual conference of the international speech communicationassociation
 Mimicking very efficient network for object detection,2017, InProceedings of the ieee conference on computer vision and pattern recognition
 Learning without forgetting,2017, IEEE transactions on pattern analysisand machine intelligence
 Teacher-student compression with generativeadversarial networks,2018, arXiv preprint arXiv:1812
 Data-free knowledge distillation for deepneural networks,2017, arXiv preprint arXiv:1710
 Unifying distillation andprivileged information,2015, arXiv preprint arXiv:1511
 Knowledge distillation for small-footprint highwaynetworks,2017, In 2017 IEEE International Conference on Acoustics
 Empirical bernstein bounds and sample variancepenalization,2009, arXiv preprint arXiv:0907
 Self-distillation amplifies regularizationin hilbert space,2020, arXiv preprint arXiv:2002
 Distilling word embeddings: An encodingapproach,2016, In Proceedings of the 25th ACM International on Conference on Information andKnowledge Management
 Parallel wavenet: Fast high-fidelityspeech synthesis,2018, In International conference on machine learning
 Semi-supervisedknowledge transfer for deep learning from private training data,2016, arXiv preprint arXiv:1610
 Distillation as adefense to adversarial perturbations against deep neural networks,2016, In 2016 IEEE Symposium onSecurity and Privacy (SP)
 Towards understanding knowledge distillation,2019, In InternationalConference on Machine Learning
 Improving the adversarial robustness and interpretabilityof deep neural networks by regularizing their input gradients,2017, arXiv preprint arXiv:1711
 Feature representation of short utterances basedon knowledge distillation for spoken language identification,2018, In Interspeech
 And the bit goesdown: Revisiting the quantization of neural networks,2020, 2020
 Learning global additiveexplanations for neural nets using model distillation,2018, arXiv preprint arXiv:1801
 Learning using privileged information: similarity control andknowledge transfer,2015, J
 High-Dimensional Statistics: A Non-Asymptotic Viewpoint,2019, CambridgeSeries in Statistical and Probabilistic Mathematics
 Model distillation with knowledge transfer fromface classification to alignment and verification,2017, arXiv preprint arXiv:1709
 Private modelcompression via knowledge distillation,2019, In Proceedings of the AAAI Conference on ArtificialIntelligence
 Student-teacher networklearning with enhanced features,2017, In 2017 IEEE International Conference on Acoustics
