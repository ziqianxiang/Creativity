title,year,conference
 Selfless sequential learning,2018, arXiv preprintarXiv:1806
 Why lottery ticket wins? a theoretical perspective of sample complexity on sparseneural networks,2021, In Submitted to International Conference on Learning Representations
 Il2m: Class incremental learning with dual memory,2019, In TheIEEE International Conference on Computer Vision (ICCV)
 Efficientlifelong learning with a-gem,2018, arXiv preprint arXiv:1812
 The lottery tickets hypothesis for supervised and self-supervised pre-trainingin computer vision models,2020, arXiv preprint arXiv:2012
 The lottery ticket hypothesis for pre-trained bert networks,2020, arXiv preprintarXiv:2007
 Uncertainty-guidedcontinual learning with bayesian neural networks,2019, arXiv preprint arXiv:1906
 Exploring the challenges towards lifelong fact learning,2018, In Asian Conferenceon Computer Vision
 The difficulty of training sparseneural networks,2019, arXiv preprint arXiv:1906
 The lottery tickethypothesis at scale,2019, arXiv preprint arXiv:1903
 The early phase of neural networktraining,2020, In International Conference on Learning Representations
 The state of sparsity in deep neural networks,2019, arXivpreprint arXiv:1902
 Continual learning via neural pruning,2019, arXivpreprint arXiv:1903
 Exemplar-supported generative repro-duction for class incremental learning,2018, In British Machine Vision Conference
 Channel pruning for accelerating very deep neural net-works,2017, In Proceedings of the IEEE International Conference on Computer Vision
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Revisiting distillation and incremental classifier learning,2018, InAsian Conference on Computer Vision
 Overcom-ing catastrophic forgetting in neural networks,2017, Proceedings of the national academy of sciences
 Learning multiple layers of features from tiny images,2009, Masterâ€™sthesis
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in neural information processing systems
 Optimal brain damage,1990, In D
 Optimal brain damage,1990, In Advances in neuralinformation processing systems
 Learning without forgetting,2017, IEEE transactions on pattern analysisand machine intelligence
 Learn-ing efficient convolutional networks through network slimming,2017, In Proceedings of the IEEEInternational Conference on Computer Vision
 Rethinking the value ofnetwork pruning,2019, In International Conference on Learning Representations
 Gradient episodic memory for continual learning,2017, InAdvances in neural information processing systems
 Goodstudents play big lottery better,2021, arXiv preprint arXiv:2101
 Sparse transfer learning via winning lottery tickets,2019, arXiv preprint arXiv:1905
 Continuallifelong learning with neural networks: A review,2019, Neural Networks
 icarl:Incremental classifier and representation learning,2017, In Proceedings of the IEEE conference onComputer Vision and Pattern Recognition
 Comparing rewinding and fine-tuning in neuralnetwork pruning,2020, In International Conference on Learning Representations
 Progressive neural networks,2016, arXiv preprintarXiv:1606
 Continual learning with deep generativereplay,2017, In Advances in Neural Information Processing Systems
 Spacenet: Make free spacefor continual learning,2020, arXiv preprint arXiv:2007
 Lifelong robot learning,1995, Robotics and autonomous systems
 Deep learning and the information bottleneck principle,2015, In2015 IEEE Information Theory Workshop (ITW)
 80 million tiny images: A large data set fornonparametric object and scene recognition,2008, IEEE transactions on pattern analysis and machineintelligence
 Picking winning tickets before training bypreserving gradient flow,2020, arXiv preprint arXiv:2002
 Growing a brain: Fine-tuning by increas-ing model capacity,2017, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Drawing early-bird tickets: Toward more efficient training ofdeep networks,2020, In International Conference on Learning Representations
 Playing the lottery with rewardsand multiple languages: lottery tickets in rl and nlp,2019, arXiv preprint arXiv:1906
 To balance or not to balance: ASimPle-yet-effective approach for learning with long-tailed distributions,2019, arXiv
 Class-incremental learning via deep model consolidation,2020, In The IEEE WinterConference on Applications of Computer Vision
 Less is more: Towards compact cnns,2016, In EuropeanConference on Computer Vision
