title,year,conference
 Large scale distributed neural network training through online distillation,2018, 2018
 Bayesian reasoning and machine learning,2012, Cambridge University Press
 Private query release assisted by public data,2020, In ICML
 Learning from mixtures of private and publicpopulations,2020, arXiv preprint arXiv:2008
 A theory of learning from different domains,2010, Machine learning
 Mixmatch: A holistic approach to semi-supervised learning,2019, In NeurIPS
 Towardsfederated learning at scale: System design,2019, arXiv preprint arXiv:1902
 Bagging predictors,1996, Machine learning
 Learning imbalanceddatasets with label-distribution-aware margin loss,2019, In Advances in Neural Information ProcessingSystems
 Online knowledge distillationwith diverse peers,2020, In AAAI
 Ensemble methods in machine learning,2000, In International workshop on multipleclassifier systems
 Essentially no barriersin neural network energy landscape,2018, In ICML
 Geodesic flow kernel for unsuperviseddomain adaptation,2012, In CVPR
 Learning kernels for unsupervised domain adaptationwith applications to visual object recognition,2014, IJCV
 Semi-supervised learning by entropy minimization,2005, In NIPS
 One-shot federated learning,2019, arXiv preprintarXiv:1902
 On calibration of modern neuralnetworks,2017, 2017
 On the convergence of local descent methods in federatedlearning,2019, arXiv preprint arXiv:1910
 Group knowledge transfer: Federatedlearning of large cnns at the edge,2020, Advances in Neural Information Processing Systems
 Deep residual learning for imagerecognition,2016, In CVPR
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, 2017
 The non-iid data quagmire ofdecentralized machine learning,2020, In ICML
 Measuring the effects of non-identical datadistribution for federated visual classification,2019, arXiv preprint arXiv:1909
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Scaffold: Stochastic controlled averaging for on-device federatedlearning,2020, In ICML
 Tighter theory for local Sgd on identical and heterogeneousdata,2020, In AISTATS
 Semi-supervisedlearning with deep generative models,2014, In NIPS
 Federated learning: Strategies for improving communication efficiency,2016, arXivpreprint arXiv:1610
 Learning multiple layers of features from tiny images,2009, 2009
 Tiny imagenet visual recognition challenge,2015, CS 231N
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Fedmd: Heterogenous federated learning via model distillation,2019, arXivpreprint arXiv:1910
 On the convergence offedavg on non-iid data,2020, In ICLR
 Ensemble distillation for robust modelfusion in federated learning,2020, arXiv preprint arXiv:2006
 Visualizing data using t-sne,2008, JMLR
 Asimple baseline for bayesian uncertainty in deep learning,2019, In NeurIPS
 Communication-efficientlearning of deep networks from decentralized data,2017, In AISTATS
 Semi-supervised knowledge transfer for deep learning from private training data,2017, In ICLR
 Adaptive federated optimization,2020, arXiv preprintarXiv:2003
 Maximum classifierdiscrepancy for unsupervised domain adaptation,2018, In CVPR
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Federated multi-tasklearning,2017, In NIPS
 Local sgd converges fast and communicates little,2019, In ICLR
 Mean teachers are better role models: Weight-averaged consis-tency targets improve semi-supervised deep learning results,2017, In Advances in neural informationprocessing systems
 Group normalization,2018, In Proceedings of the European conference oncomputer vision (ECCV)
 Federated machine learning: Concept andapplications,2019, ACM Transactions on Intelligent Systems and Technology (TIST)
 Federated learning withunbiased gradient aggregation and controllable meta updating,2019, arXiv preprint arXiv:1910
 Bayesian nonparametric federated learning of neural networks,2019, In ICML
 Benchmarking semi-supervised federated learning,2020, arXiv preprint arXiv:2008
 Federatedlearning with non-iid data,2018, arXiv preprint arXiv:1806
 On the convergence properties of a k-step averaging stochastic gradientdescent algorithm for nonconvex optimization,2017, arXiv preprint arXiv:1708
 Ensemble methods: foundations and algorithms,2012, CRC press
 Semi-supervised learning literature survey,2005, Technical report
 Parallelized stochastic gradientdescent,2010, In NIPS
