title,year,conference
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, arXiv preprint arXiv:1802
 Why do larger models generalize better? a theoretical perspectivevia the xor problem,2019, In International Conference on Machine Learning
 Rigging the lottery:Making all tickets winners,2019, arXiv preprint arXiv:1911
 The state of sparsity in deep neural networks,2019, arXivpreprint arXiv:1902
 Soft filter pruning for acceleratingdeep convolutional neural networks,2018, arXiv preprint arXiv:1808
 Filter pruning via geometric medianfor deep convolutional neural networks acceleration,2019, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Optimal brain damage,1990, In Advances in neuralinformation processing systems
 Snip: Single-shot network pruning basedon connection sensitivity,2019, In International Conference on Learning Representations
 Eagleeye: Fast sub-net evaluationfor efficient neural network pruning,2020, arXiv preprint arXiv:2007
 Pruning filters forefficient convnets,2016, arXiv preprint arXiv:1608
 Provable filter pruningfor efficient neural networks,2020, In International Conference on Learning Representations
 Dynamic model pruningwith feedback,2020, In International Conference on Learning Representations
 Rethinking the value ofnetwork pruning,2019, In International Conference on Learning Representations
 Thinet: A filter level pruning method for deep neuralnetwork compression,2017, In Proceedings of the IEEE international conference on computer vision
 Importance estimation forneural network pruning,2019, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Towardsunderstanding the role of over-parametrization in generalization of neural networks,2018, arXiv preprintarXiv:1805
 Sensitivity and generalization in neural networks: an empirical study,2018, arXiv preprintarXiv:1802
 Comparing rewinding and fine-tuning in neuralnetwork pruning,2020, arXiv preprint arXiv:2003
 Pruning neural networkswithout any data by iteratively conserving synaptic flow,2020, arXiv preprint arXiv:2006
 Picking winning tickets before training bypreserving gradient flow,2020, In International Conference on Learning Representations
 Learning structured sparsity in deepneural networks,2016, In Advances in neural information processing systems
 Discovering neural wirings,2019, In Advancesin Neural Information Processing Systems
 Rethinking the smaller-norm-less-informativeassumption in channel pruning of convolution layers,2018, arXiv preprint arXiv:1802
 Gate decorator: Global filterpruning method for accelerating deep convolutional neural networks,2019, In Advances in NeuralInformation Processing Systems
 Nisp: Pruning networks using neuron importance scorepropagation,2018, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Discrimination-aware channel pruning for deep neural networks,2018, In Advances inNeural Information Processing Systems
