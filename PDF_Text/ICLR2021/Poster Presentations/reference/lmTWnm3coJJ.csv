title,year,conference
 Unsupervisedlabel noise modeling and loss correction,2019, arXiv preprint arXiv:1904
 MiXmatch: A holistic approach to semi-supervised learning,2019, In Advances in NeuralInformation Processing Systems 32 (NeurIPS)
 Understanding and utilizingdeep neural netWorks trained With noisy labels,2019, arXiv preprint arXiv:1905
 Robust loss functions under label noise for deepneural netWorks,2017, arXiv preprint arXiv:1712
 Curriculumnet: Weakly supervised learning from large-scale Web images,2018, InProceedings of the European Conference on Computer Vision (ECCV)
 Co-teaching: Robust training of deep neural netWorks With eXtremely noisy labels,2018, InAdvances in neural information processing systems
 Deep residual learning for imagerecognition,2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Weakly supervised image classificationthrough noise regularization,2019, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 O2u-net: A simple noisy label detectionapproach for deep neural netWorks,2019, In Proceedings of the IEEE International Conference onComputer Vision
 Self-pacedcurriculum learning,2015, In AAAI
 Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels,2018, In International Conferenceon Machine Learning
 Beyond synthetic noise: Deep learning oncontrolled noisy labels,2020, ICML
 How do humans teach: On curriculum learningand teaching dimension,2011, In NeurIPS
 Self-paced learning for latent variablemodels,2010, In NeurIPS
 Temporal ensembling for semi-supervised learning,2016, arXiv preprintarXiv:1610
 Robust inference viagenerative classifiers for handling noisy labels,2019, arXiv preprint arXiv:1901
 Cleannet: Transfer learning for scalableimage classifier training with label noise,2018, In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition
 Learning fromnoisy labels with distillation,2017, In Proceedings of the IEEE International Conference on ComputerVision
 Dimensionality-driven learning with noisy labels,2018, arXiv preprintarXiv:1806
 Nor-malized loss functions for deep learning with noisy labels,2020, arXiv preprint arXiv:2006
" Decoupling"" when to update"" from"" how to update""",2017, InAdvances in Neural Information Processing Systems
 Makingdeep neural networks robust to label noise: A loss correction approach,2017, In Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition
 Regularizingneural networks by penalizing confident output distributions,2017, arXiv preprint arXiv:1701
 Training deep neural networks on noisy labels with bootstrapping,2014, arXiv preprintarXiv:1412
 Regularization with stochastic transfor-mations and perturbations for deep semi-supervised learning,2016, Advances in neural informationprocessing Systems
 Data parameters: A new family of parametersfor learning a differentiable curriculum,2019, In Advances in Neural Information Processing Systems
 Baby Steps: How “Less is More” in unsu-pervised dependency parsing,2009, In NeurIPS 2009 Workshop on Grammar Induction
 Self-paced learning for long-term tracking,2013, In CVPR
 Rethinkingthe inception architecture for computer vision,2016, In Proceedings of the IEEE conference on computervision and pattern recognition
 Joint optimization frameworkfor learning with noisy labels,2018, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Self-paced dictionary learning for image classification,2012, InMM
 Mean teachers are better role models: Weight-averaged consis-tency targets improve semi-supervised deep learning results,2017, In Advances in Neural InformationProcessing Systems 30 (NeurIPS)
 Learningfrom noisy large-scale datasets with minimal supervision,2017, In Proceedings of the IEEE conferenceon computer vision and pattern recognition
 Symmetric crossentropy for robust learning with noisy labels,2019, In Proceedings of the IEEE International Conferenceon Computer Vision
 Learning from massive noisylabeled data for image classification,2015, In Proceedings of the IEEE conference on computer visionand pattern recognition
 Probabilistic end-to-end noise correction for learning with noisy labels,2019, InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Understandingdeep learning requires rethinking generalization,2017, In ICLR
 mixup: Beyond empiricalrisk minimization,2018, In International Conference on Learning Representations
 Generalized cross entropy loss for training deep neural networkswith noisy labels,2018, In Advances in neural information processing systems
 Minimax curriculum learning: Machine teaching with desirabledifficulties and scheduled diversity,2018, In ICLR
 Time-consistent self-supervision for semi-supervisedlearning,2020, In International Conference on Machine Learning (ICML)
 Curriculum learning by optimizing learningdynamics,2021, In AISTATS
 Unpaired image-to-image translationusing cycle-consistent adversarial networks,2017, In Computer Vision (ICCV)
