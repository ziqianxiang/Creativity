title,year,conference
 Backward feature correction: How deep learning per-forms deep learning,2020, arXiv preprint arXiv:2001
 A convergence theory for deep learningvia over-parameterization,2019, In Proceedings of the International Conference on MachineLearning (ICML)
 On exact computation with an infinitely wide neural net,2019, In Advances in NeuralInformation Processing Systems (NeurIPS)
 Fine-grained analysisof optimization and generalization for overparameterized two-layer neural networks,2019, InProceedings of the International Conference on Machine Learning (ICML)
 Sharp analysis of low-rank kernel matrix approximations,2013, In Conference onLearning Theory (COLT)
 The convergence rate ofneural networks for learned functions of different frequencies,2019, In Advances in NeuralInformation Processing Systems (NeurIPS)
 Frequency bias in neural networks for input of non-uniform density,2020, In Proceedingsof the International Conference on Machine Learning (ICML)
 On the inductive bias of neural tangent kernels,2019, InAdvances in Neural Information Processing Systems (NeurIPS)
 Towards under-standing the spectral bias of deep learning,2019, arXiv preprint arXiv:1912
 Optimal rates for the regularized least-squaresalgorithm,2007, Foundations of Computational Mathematics
 On lazy training in differentiable pro-gramming,2019, In Advances in Neural Information Processing Systems (NeurIPS)
 Kernel methods for deep learning,2009, In Advances inNeural Information Processing Systems (NIPS)
 On the mathematical foundations of learning,2002, Bul letin ofthe American mathematical society
 Depth separation for neural networks,2017, In Conference on Learning Theory(COLT)
 Gradient descent findsglobal minima of deep neural networks,2019, In Proceedings of the International Conferenceon Machine Learning (ICML)
 Gradient descent provably opti-mizes over-parameterized neural networks,2019, In Proceedings of the International Conferenceon Learning Representations (ICLR)
 Spherical harmonics in p dimensions,2014, WorldScientific
 The spectrum of kernel random matrices,2010, The Annals of Statistics
 Spectra of the conjugate kernel and neural tangent kernelfor linear-width neural networks,2020, In Advances in Neural Information Processing Systems(NeurIPS)
 Analytic combinatorics,2009, Cambridge Universitypress
 Linearizedtwo-layers neural networks in high dimension,2019, arXiv preprint arXiv:1904
 Delving deep into rectifiers:Surpassing human-level performance on imagenet classification,2015, In Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Multilayer feedforward networksare universal approximators,1989, Neural networks
 Neural tangent kernel: Convergenceand generalization in neural networks,2018, In Advances in Neural Information ProcessingSystems (NIPS)
 Deep neural networks as gaussian processes,2018, In Proceedings of theInternational Conference on Learning Representations (ICLR)
 Generalized leverage scoresampling for neural networks,2020, In Advances in Neural Information Processing Systems(NeurIPS)
 Learning overparameterized neural networks via stochas-tic gradient descent on structured data,2018, In Advances in Neural Information ProcessingSystems (NeurIPS)
 A mean field view of the landscapeof two-layer neural networks,2018, Proceedings of the National Academy of Sciences
 Deep vs,2016, shallow networks: An approximationtheory perspective
 Bayesian learning for neural networks,1996, Springer
 Approximation theory of the mlp model in neural networks,1999, Acta numerica
 Random features for large-scale kernel machines,2007, InAdvances in Neural Information Processing Systems (NIPS)
 Generalization properties of learning with randomfeatures,2017, In Advances in Neural Information Processing Systems
 Risk bounds for multi-layer perceptrons through spectraof integral operators,2020, arXiv preprint arXiv:2002
 Nonparametric regression using deep neural networks withrelu activation function,2020, Annals of Statistics
 Regularization with dot-productkernels,2001, In Advances in Neural Information Processing Systems (NIPS)
 Quadratic suffices for over-parametrization via matrix chernoffbound,2019, arXiv preprint arXiv:1906
 Benefits of depth in neural networks,2016, In Conference on Learning Theory(COLT)
 A fine-grained spectral perspective on neural networks,2019, arXivpreprint arXiv:1907
 Stochastic gradient descentoptimizes over-parameterized deep relu networks,2019, Machine Learning
