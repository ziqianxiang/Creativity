title,year,conference
 Stronger generalization bounds fordeep nets via a compression approach,2018, In ICML
 A closer look atmemorization in deep networks,2017, arXiv preprint arXiv:1706
 Food-101 - mining discriminative compo-nents with random forests,2014, In ECCV
 Convex optimization,2004, Cambridgeuniversity press
 Convex optimization: Algorithms and complexity,2014, arXiv preprint arXiv:1405
 Generalization bounds of stochastic gradient descent for wide and deepneural networks,2019, In NeurIPS
 Understanding and utilizingdeep neural networks trained with noisy labels,2019, arXiv preprint arXiv:1905
 Learning with instance-dependent label noise: A sample sieve approach,2021, In ICLR
 Learning with boundedinstance-and label-dependent label noise,2020, In ICML
 Imagenet: A large-scalehierarchical image database,2009, In CVPR
 Co-teaching: Robust training of deep neural networks with extremely noisy labels,2018, InNeurIPS
 Deep residual learning for imagerecognition,2015, arXiv preprint arXiv:1512
 Using trusted data to traindeep networks on labels corrupted by severe noise,2018, In NeurIPS
 Simple and effective regularization methods for training onnoisily labeled data with generalization guarantee,2020, In ICLR
 MentorNet: Learning data-driven curriculum for very deep neural networks on corrupted labels,2018, In ICML
 Snip: Single-shot network pruningbased on connection sensitivity,2019, In ICLR
 Dividemix: Learning with noisy labels as semi-supervised learning,2020, In ICLR
 Gradient descent with early stopping isprovably robust to label noise for overparameterized neural networks,2020, In AISTATS
 Provably end-to-endlabel-noise learning without anchor points,2021, arXiv preprint arXiv:2102
 Learning fromnoisy labels with distillation,2017, In ICCV
 Early-learningregularization prevents memorization of noisy labels,2020, arXiv preprint arXiv:2007
 Curriculum loss: Robust learning and generalization against labelcorruption,2020, In ICLR
 Nor-malized loss functions for deep learning with noisy labels,2020, In ICML
 Self: Learning to filter noisy labels with self-ensembling,2020, InICLR
 Makingdeep neural networks robust to label noise: A loss correction approach,2017, In CVPR
 Identifying mislabeled datausing the area under the margin ranking,2020, arXiv preprint arXiv:2001
 Faster r-cnn: Towards real-time objectdetection with region proposal networks,2015, In NeurIPS
 Deep learning is robust to massivelabel noise,2017, arXiv preprint arXiv:1705
 Meta transition adaptation for robust deeplearning with noisy labels,2020, arXiv preprint arXiv:2006
 Joint optimization frameworkfor learning with noisy labels,2018, In CVPR
 Tackling instance-dependent label noise via a universal probabilistic model,2021, In AAAI
 Learning with group noise,2021, In AAAI
 Symmetric crossentropy for robust learning with noisy labels,2019, In ICCV
 Class2simi: A new perspective on learning with label noise,2020, arXiv preprintarXiv:2006
 Extended T: Learning with mixed closed-set and open-set noisy labels,2020, arXiv preprintarXiv:2012
 Part-dependent label noise: Towards instance-dependentlabel noise,2020, In NeurIPS
 Fashion-mnist: a novel image dataset for benchmarkingmachine learning algorithms,2017, arXiv preprint arXiv:1708
 Learning from massive noisylabeled data for image classification,2015, In CVPR
 L_dmi: A novel information-theoretic lossfunction for training deep nets robust to label noise,2019, In NeurIPS
 Searching to exploit memoriza-tion effect in learning with noisy labels,2020, In ICML
 Learning with biased complementarylabels,2018, In ECCV
 Understandingdeep learning requires rethinking generalization,2017, In ICLR
 Boosting with early stopping: Convergence and consistency,2005, The Annalsof Statistics
 Generalized cross entropy loss for training deep neural networkswith noisy labels,2018, In NeurIPS
 Error-bounded correction of noisy labels,2020, In ICML
 A second-order approach to learning with instance-dependent label noise,2021, In CVPR
 Gradient descent optimizes over-parameterized deep relu networks,2020, Machine Learning
