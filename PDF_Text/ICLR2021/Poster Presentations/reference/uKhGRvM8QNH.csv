title,year,conference
 Tkd: Temporal knowledge distillation for activeperception,2020, In The IEEE Winter Conference on Applications of Computer Vision
 Cascade r-cnn: High quality object detection and instance seg-mentation,2019, IEEE Transactions on Pattern Analysis and Machine Intelligence
 On the efficacy of knowledge distillation,2019, In The IEEEInternational Conference on Computer Vision (ICCV)
 Imagenet: A large-scalehierarchical image database,2009, In CVPR
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, In NAACL
 Low-resolution face recognition in the wildvia selective knowledge distillation,2018, IEEE Transactions on Image Processing
 Deep residual learning for image recog-nition,2016, In CVPR
 A com-prehensive overhaul of feature distillation,2019, In Proceedings of the IEEE International Conferenceon Computer Vision
 Distilling the knowledge in a neural network,2014, InNeurIPS
 Searching for mobilenetv3,2019, arXivpreprint arXiv:1905
 Relation networks for objectdetection,2018, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Microsoft coco: Common objects in context,2014, In Europeanconference on computer vision
 Focal loss for dense objectdetection,2017, In Proceedings of the IEEE international conference on computer vision
 Structuredknowledge distillation for semantic segmentation,2019, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Rethinking the value ofnetwork pruning,2018, arXiv preprint arXiv:1810
 Grid r-cnn,2019, In Proceedings of the IEEEConference on Computer Vision and Pattern Recognition
 Shufflenet v2: Practical guidelinesfor efficient cnn architecture design,2018, In Proceedings of the European Conference on ComputerVision (ECCV)
 Self-distillation amplifies regularizationin hilbert space,2020, arXiv preprint arXiv:2002
 Faster r-cnn: Towards real-time objectdetection with region proposal networks,2015, In Advances in neural information processing systems
 Fitnets: Hints for thin deep nets,2015, In ICLR
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Similarity-preserving knowledge distillation,2019, In Proceedings of theIEEE International Conference on Computer Vision
 Non-local neural networks,2018, InProceedings of the IEEE conference on computer vision and pattern recognition
 Bert-of-theseus: Compressingbert by progressive module replacing,2020, arXiv preprint arXiv:2002
 Revisit knowledge distillation: ateacher-free framework,2019, arXiv preprint arXiv:1909
 Paying more attention to attention: Improving the perfor-mance of convolutional neural networks via attention transfer,2017, In ICLR
 Dynamic R-CNN:Towards high quality object detection via dynamic training,2020, arXiv preprint arXiv:2004
 Scan:A scalable neural networks framework towards compact and efficient models,2019, In Advances inNeural Information Processing Systems 32
 Learning deepfeatures for discriminative localization,2016, In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition (CVPR)
 Feature selective anchor-free module for single-shot object detection,2019, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
