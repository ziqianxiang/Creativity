title,year,conference
 Deep variational information bottleneck,2017, InICLR
 Transfer fine-tuning: A bert case study,2019, In EMNLP
 Don’t takethe premise for granted: Mitigating artifacts in natural language inference,2019, In ACL
 Onadversarial removal of hypothesis-only bias in natural language inference,2019, In SEM
 A large annotatedcorpus for learning natural language inference,2015, In EMNLP
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In NAACL
 Automatically constructing a corpus of sentential paraphrases,2005, InIWP
 Adversarial removal of demographic attributes from text data,2018, InEMNLP
 Natural language inference over interaction space,2017, In ICLR
 Annotation artifacts in natural language inference data,2018, In NAACL
 Variational pretraining for semi-supervisedtext classification,2019, In ACL
 The information bottleneck revisited or how to choose a gooddistortion measure,2007, In ISIT
 Generalization in reinforcement learning with selective noise injection and informationbottleneck,2019, In NeurIPS
 How much reading does reading comprehension require? acritical investigation of popular benchmarks,2018, In EMNLP
 Auto-encoding variational bayes,2013, In ICLR
 Mixout: Effective regularization to finetunelarge-scale pretrained language models,2019, In ICLR
 End-to-end bias mitigation bymodelling biases in corpora,2020, In ACL
 A sick cure for the evaluation of compositional distributional semantic models,2014, In LREC
 Most “babies” are “little” and most “problems” are “huge”:Compositional entailment in adjective-nouns,2016, In ACL
 Framenet+: Fast paraphrastic tripling of framenet,2015, In ACL
 Sentence encoders on stilts: Supplementary trainingon intermediate labeled-data tasks,2018, arXiv:1811
 Language modelsare unsupervised multitask learners,2019, 2019
 Resolving complex cases of definite pronouns: the winograd schemachallenge,2012, In EMNPL
 Semantic proto-roles,2015, In TACL
 Learning and generalization with the informationbottleneck,2010, In TCS
 Dropout:a simple way to prevent neural networks from overfitting,2014, In JMLR
 The information bottleneck method,1999, In Allerton
 Attention is all you need,2017, In NeurIPS
 GLUE:A multi-task benchmark and analysis platform for natural language understanding,2019, In ICLR
 Bilateral multi-perspective matching for natural languagesentences,2017, In IJCAI
 Bottlesum: Unsupervised and self-supervisedsentence summarization using the information bottleneck principle,2019, In EMNLP
 A broad-coverage challenge corpus for sentenceunderstanding through inference,2018, In NAACL
 Xlnet:Generalized autoregressive pretraining for language understanding,2019, In NeurIPS
 Revisiting few-samplebert fine-tuning,2021, ICLR
