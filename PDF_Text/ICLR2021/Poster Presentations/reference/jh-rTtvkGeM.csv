title,year,conference
 Finding approxi-mate local minima faster than gradient descent,2017, In Proceedings of the 49th Annual ACM SIGACTSymposium on Theory of Computing
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 On the convergence of adamand adagrad,2020, arXiv preprint arXiv:2003
 Why momentum really works,2017, Distill
 Deep Learning,2016, MIT Press
 On the diffusion approximation of noncon-vex stochastic gradient descent,2017, arXiv preprint arXiv:1705
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In Proceedings of the 32nd International Conference on MachineLearning
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In Advances in neural information processing systems
 The asymptotic spectrum of the hessian ofdnn throughout training,2020, In International Conference on Learning Representations
 Three factors influencing minima in sgd,2017, arXiv preprintarXiv:1711
 On the relation betWeen the sharpest directions of DNN loss and the SGD step length,2019, InInternational Conference on Learning Representations
 The break-even point on optimization trajectories of deep neuralnetWorks,2020, In International Conference on Learning Representations
 Efficient backprop,1998, Lecture notes in computer science
 Automatic learning rate maximization byon-line estimation of the hessianâ€™s eigenvectors,1993, In Advances in Neural Information ProcessingSystems 5
 Wide neural netWorks of any depth evolve as linear modelsunder gradient descent,2019, In Advances in neural information processing systems
 The largelearning rate phase of deep learning: the catapult mechanism,2020, arXiv preprint arXiv:2003
 On the convergence of stochastic gradient descent With adaptivestepsizes,2019, In The 22nd International Conference on Artificial Intelligence and Statistics
 Hessian basedanalysis of sgd for deep nets: Dynamics and generalization,2020, In Proceedings of the 2020 SIAMInternational Conference on Data Mining
 Learning overparameterized neural netWorks via stochastic gradi-ent descent on structured data,2018, In Proceedings of the 32nd International Conference on NeuralInformation Processing Systems
 Reconciling modern deep learning With traditionaloptimization analyses: The intrinsic learning rate,2020, Advances in Neural Information ProcessingSystems
 SECOND-ORDER OPTIMIZATION FOR NEURAL NETWORKS,2016, PhD thesis
 Pointer sentinel mixturemodels,2016, In International Conference on Learning Representations
 The full spectrum of deepnet hessians at scale: Dynamics with sgd training andsample size,2018, arXiv preprint arXiv:1811
 Measurements of three-level hierarchical structure in the outliers in the spectrum ofdeepnet hessians,2019, arXiv preprint arXiv:1901
 Stochastic variancereduction for nonconvex optimization,2016, In International conference on machine learning
 Adaptive federated optimization,2021, In InternationalConference on Learning Representations
 Empirical analysis ofthe hessian of over-parametrized neural networks,2017, arXiv preprint arXiv:1706
 The impactof neural network overparameterization on gradient confusion and stochastic gradient descent,2019, InInternational Conference on Machine Learning
 On the importance of initial-ization and momentum in deep learning,2013, In Proceedings of the 30th International Conference onMachine Learning
 Adagrad stepsizes: sharp convergence over nonconvexlandscapes,2019, In International Conference on Machine Learning
 Linear convergence of adaptive stochastic gradientdescent,2020, In International Conference on Artificial Intelligence and Statistics
 A diffusion theory for deep learning dynamics:Stochastic gradient descent exponentially favors flat minima,2021, In International Conference onLearning Representations
 A walk with sgd,2018, arXiv preprintarXiv:1802
 Large batch optimization for deeplearning: Training bert in 76 minutes,2020, In International Conference on Learning Representations
 Adaptive meth-ods for nonconvex optimization,2018, In Advances in neural information processing systems
 On the convergence ofadaptive gradient methods for nonconvex optimization,2018, arXiv preprint arXiv:1808
 The anisotropic noise in stochas-tic gradient descent: Its behavior of escaping from sharp minima and regularization effects,2019, InProceedings of the 36th International Conference on Machine Learning
 By Theorem 2,2021,37 in Elaydi (2005)
