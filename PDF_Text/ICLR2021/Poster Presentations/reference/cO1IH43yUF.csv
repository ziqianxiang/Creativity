title,year,conference
 Thefifth pascal recognizing textual entailment challenge,2009, In TAC 2009 Workshop
 Adaptation of maximum entropy capitalizer: Little data can help alot,2004, In EMNLP
 ELECTRA: Pre-trainingtext encoders as discriminators rather than generators,2020, In ICLR
 Metainit: Initializing learning by learning to initialize,2019, InNeurIPS
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In AISTATS
 Realm: Retrieval-augmented language model pre-training,2020, arXiv preprint arXiv:2002
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In ICCV
 Designing and interpreting probes with control tasks,2019, In EMNLP
 A structural probe for finding syntax in word representa-tions,2019, In NAACL
 Parameter-efficient transfer learning forNLP,2019, In ICML
 Universal language model fine-tuning for text classification,2018, InACL
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Albert: A lite bert for self-supervised learning of language representations,2020, In ICLR
 Mixout: Effective regularization to finetunelarge-scale pretrained language models,2020, In ICLR
 Rifle: Backpropagation indepth for deep transfer learning through re-initializing the fully-connected layer,2020, In ICML
 Linguisticknowledge and transferability of contextual representations,2019, arXiv preprint arXiv:1903
 Multi-task deep neural networks fornatural language understanding,2019, In ACL
 The microsoft toolkit of multi-taskdeep neural networks for natural language understanding,2020, arXiv preprint arXiv:2002
 Fine-tune BERT for extractive summarization,2019, arXiv preprint arXiv:1903
 RoBERTa: A Robustly Optimized BERT PretrainingApproach,2019, arXiv preprint arXiv:1907
 Deepdouble descent: Where bigger models and more data hurt,2019, In ICLR
 To tune or not to tune? adapting pretrainedrepresentations to diverse tasks,2019, arXiv preprint arXiv:1903
 Sentence encoders on stilts: Supplementarytraining on intermediate labeled-data tasks,2018, arXiv preprint arXiv:1811
 Training tips for the transformer model,2018, The Prague Bulletin ofMathematical Linguistics
 Languagemodels are unsupervised multitask learners,2019, 2019
 Recursive deep models for semantic compositionality over a sentimenttreebank,2013, In EMNLP
 Bert and pals: Projected attention layers for efficientadaptation in multi-task learning,2019, In ICML
 Investigating transferability inpretrained language models,2020, In EMNLP
 Bert rediscovers the classical nlp pipeline,2019, In ACL
 What do you learn fromcontext? probing for sentence structure in contextualized word representations,2019, In ICLR
 Attention is all you need,2017, In NeurIPS
 Regularization of neuralnetworks using dropconnect,2013, In ICML
 Superglue: A stickier benchmark for general-purpose languageunderstanding systems,2019, In NeurIPS
 Glue: Amulti-task benchmark and analysis platform for natural language understanding,2019, In ICLR
 A broad-coverage challenge corpus forsentence understanding through inference,2018, In ACL
 Huggingface’stransformers: State-of-the-art natural language processing,2019, arXiv preprint arXiv:1910
 Simple applications of BERT for ad hoc documentretrieval,2019, arXiv preprint arXiv:1903
 Residual learning without normalization via betterinitialization,2019, In ICLR
 BERTScore:Evaluating Text Generation with BERT,2020, In ICLR
 to InitializationL2 Dist,2021, to InitializationL2 Dist
tsiD 2LSSSSUOIaZIIeIlπII 01 ,1250,lslα zjTransformer Block 9Transformer Block 10UOIaZIIeIlπII 01 
