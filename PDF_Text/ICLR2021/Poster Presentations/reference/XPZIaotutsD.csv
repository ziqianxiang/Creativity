title,year,conference
 Layer normalization,2016, arXiv preprintarXiv:1607
 The second PASCALrecognising textual entailment challenge,2006, In Proceedings of the Second PASCAL ChallengesWorkshop on Recognising Textual Entailment
 Thefifth pascal recognizing textual entailment challenge,2009, In In Proc Text Analysis Conference (TACâ€™09
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Natural-to formal-language generation using tensor product representations,2019, arXiv preprintarXiv:1910
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 ELECTRA: Pre-trainingtext encoders as discriminators rather than generators,2020, In ICLR
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Unified language model pre-training for natural language understandingand generation,2019, In Advances in Neural Information Processing Systems
 A hybrid neural network model forcommonsense reasoning,2019, arXiv preprint arXiv:1907
 X-sql: reinforce schema representa-tion with context,2019, arXiv preprint arXiv:1908
 Musictransformer: Generating music with long-term structure,2018, 2018
 SMART:Robust and efficient fine-tuning for pre-trained natural language models through principled regu-larized optimization,2020, In ACL
 Spanbert:Improving pre-training by representing and predicting spans,2020, Transactions of the Association forComputational Linguistics
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Reformer: The efficient transformer,2019, InInternational Conference on Learning Representations
 Race: Large-scale readingcomprehension dataset from examinations,2017, In Proceedings of the 2017 Conference on EmpiricalMethods in Natural Language Processing
 The winograd schema challenge,2012, InThirteenth International Conference on the Principles of Knowledge Representation and Reasoning
 The Winograd schema challenge,2011, In AAAISpring Symposium: Logical Formalizations of Commonsense Reasoning
 On the variance of the adaptive learning rate and beyond,2019, In International Conference onLearning Representations
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv PrePrint arXiv:1907
 Pointer sentinel mixturemodels,2016, arXiv
 Deep learning based text classification: A comprehensive review,2020, arXiv PrePrintarXiv:2004
 Virtual adversarial training: aregularization method for supervised and semi-supervised learning,2018, IEEE transactions on Patternanalysis and machine intelligence
 Wic: the word-in-context dataset forevaluating context-sensitive meaning representations,2019, In Proceedings of the 2019 Conference ofthe North American ChaPter of the Association for ComPutational Linguistics: Human LanguageTechnologies
 Languagemodels are unsupervised multitask learners,2019, OPenAI Blog
 Choice of plausible alternatives:An evaluation of commonsense causal reasoning,2011, In 2011 AAAI SPring SymPosium Series
 Enhancing the transformer with explicit relational encoding for math problem solving,2019, arXivPrePrint arXiv:1910
 Ex-ploiting structured knowledge in text via graph-guided representation learning,2020, arXiv PrePrintarXiv:2004
 Megatron-lm: Training multi-billion parameter language models using gpu modelparallelism,2019, arXiv preprint arXiv:1909
 Tensor product variable binding and the representation of symbolic structures inConnectionist systems,1990, Artificial intelligence
 Ernie: Enhanced representation through knowledge integration,2019, arXivpreprint arXiv:1904
 A simple method for commonsense reasoning,2018, arXiv preprintarXiv:1806
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Superglue: A stickier benchmark for general-purpose languageunderstanding systems,2019, In Advances in neural information processing systems
 Structbert: Incor-porating language structures into pre-training for deep language understanding,2019, arXiv preprintarXiv:1908
 Swag: A large-scale adversarialdataset for grounded commonsense inference,2018, In Proceedings of the 2018 Conference on EmpiricalMethods in Natural Language Processing
 Aligning books and movies: Towards story-like visual explanations by watchingmovies and reading books,2015, In Proceedings of the IEEE international conference on computervision
