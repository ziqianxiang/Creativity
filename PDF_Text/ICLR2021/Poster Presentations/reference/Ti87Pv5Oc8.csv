title,year,conference
 A convergence theory for deep learning viaover-parameterization,2019, volume 97 of Proceedings ofMachine Learning Research
 Learning to learn by gradient descent by gradientdescent,2016, In D
 Onexact computation with an infinitely wide neural net,2019, In Advances in Neural Information ProcessingSystems
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, In International Conference on MachineLearning
 Generalized pad6 approximations to the exponential function,1992, BITNumerical Mathematics
 On the convergence theory of gradient-basedmodel-agnostic meta-learning algorithms,2020, In International Conference on Artificial Intelligenceand Statistics
 Model-agnostic meta-learning for fast adaptation ofdeep networks,2017, In International Conference on Machine Learning
 Probabilistic model-agnostic meta-learning,2018, InAdvances in Neural Information Processing Systems
 Deep learning versus kernel learning: an empirical study of loss landscapegeometry and the time evolution of the neural tangent kernel,2020, In Advances in Neural InformationProcessing Systems
 Countering adversarialimages using input transformations,2018, In International Conference on Learning Representations
 Provable guarantees for gradient-basedmeta-learning,2019, In International Conference on Machine Learning
 Adaptive gradient-based meta-learningmethods,2019, In Advances in Neural Information Processing Systems
 Adam: A method for stochastic optimization,2015, In InternationalConference on Learning Representations
 A simple neural attentive meta-learner,2018, In International Conference on Learning Representations
 Bayesian deep convolutional networks with manychannels are gaussian processes,2019, In International Conference on Learning Representations
 Tadam: Task dependent adaptive metricfor improved few-shot learning,2018, In Advances in Neural Information Processing Systems
 Meta-learning with implicitgradients,2019, In Advances in Neural Information Processing Systems
 Amortized bayesian meta-learning,2019, In International Conference onLearning Representations
 Evolutionary principles in self-referential learning,1987, Diploma thesis
 Prototypical networks for few-shot learning,2017, InAdvances in Neural Information Processing Systems
 Meta-dataset: A dataset of datasets for learning to learn from few examples,2020, In International Conferenceon Learning Representations
 Hierarchically structured meta-learning,2019, InInternational Conference on Machine Learning
