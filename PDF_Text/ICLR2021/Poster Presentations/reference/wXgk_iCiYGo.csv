title,year,conference
 A closer look atmemorization in deep networks,2017, In International Conference on Machine Learning
 Entropy-sgd: Biasing gradient descentinto wide valleys,2017, In International Conference on Learning Representations
 Sharp minima can generalize fordeep nets,2017, In International Conference on Machine Learning
 Limit distributions for sumsof independent,1954, Am
 Gradient descent happens in a tiny subspace,2018, arXivpreprint arXiv:1812
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Simplifying neural nets by discovering flat minima,1995, InAdvances in neural information processing systems
 Flat minima,1997, Neural Computation
 On the diffusion approximation of nonconvexstochastic gradient descent,2019, Annals of Mathematical Sciences and Applications
 Three factors influencing minima in sgd,2017, arXiv preprint arXiv:1711
 On large-batch training for deep learning: Generalization gap and sharp minima,2017, InInternational Conference on Learning Representations
 BroWnian motion in a field of force and the diffusion model of chemicalreactions,1940, Physica
 One Weird trick for parallelizing convolutional neural netWorks,2014, arXiv preprintarXiv:1404
 Learning multiple layers of features from tiny images,2009, 2009
 Deep learning,2015, nature
 Visualizing the loss landscapeof neural nets,2018, In Advances in Neural Information Processing Systems
 Exploring general-ization in deep learning,2017, In Advances in Neural Information Processing Systems
 First exit time analysisof stochastic gradient descent under heavy-tailed gradient noise,2019, In Advances in Neural InformationProcessing Systems
 Non-gaussianity ofstochastic gradient noise,2019, arXiv preprint arXiv:1910
 In all likelihood: statistical modelling and inference using likelihood,2001, OxfordUniversity Press
 Empirical analysis of thehessian of over-parametrized neural networks,2017, arXiv preprint arXiv:1706
 Approximation analysis of stochastic gradient langevin dynamicsby using fokker-planck equation and ito process,2014, In International Conference on Machine Learning
 A tail-index analysis of stochastic gradientnoise in deep neural networks,2019, In International Conference on Machine Learning
 A bayesian perspective on generalization and stochastic gradientdescent,2018, In International Conference on Learning Representations
 Normalized flat minima: Exploring scaleinvariant definition of flat minima for neural networks using pac-bayesian analysis,2019, arXiv preprintarXiv:1901
 Towards understanding generalization of deep learning: Perspective ofloss landscapes,2017, arXiv preprint arXiv:1706
 Adai: Separating theeffects of adaptive learning rate and momentum inertia,2020, arXiv preprint arXiv:2006
 Global convergence of langevin dynamicsbased algorithms for nonconvex optimization,2018, In Advances in Neural Information ProcessingSystems
 Hessian-based analysisof large batch training and robustness to adversaries,2018, In Advances in Neural Information ProcessingSystems
 Understandingdeep learning requires rethinking generalization,2017, In International Conference on Machine Learning
 Which algorithmic choices matter at which batch sizes? insights froma noisy quadratic model,2019, In Advances in Neural Information Processing Systems
 A hitting time analysis of stochastic gradientlangevin dynamics,2017, In Conference on Learning Theory
 Rate theories for biologists,2010, Quarterly reviews of biophysics
 The anisotropic noise in stochasticgradient descent: Its behavior of escaping from sharp minima and regularization effects,2019, In ICML
