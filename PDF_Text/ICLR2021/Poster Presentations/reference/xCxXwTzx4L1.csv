title,year,conference
 Learning the number of neurons in deep networks,2016, InAdvances in Neural Information Processing Systems
 Compression-aware training of deep networks,2017, In Advancesin Neural Information Processing Systems
 Exploiting linear structurewithin convolutional networks for efficient evaluation,2014, ArXiv
 Centripetal sgd for pruning verydeep convolutional networks with complicated structure,2019, In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition
 Learning to prune deep neural networks via layer-wiseoptimal brain surgeon,2017, In Advances in Neural Information Processing Systems
 Compressing deep convolutional net-works using vector quantization,2014, arXiv preprint arXiv:1412
 Eliminating beta-continuation from heaviside projection anddensity filter algorithms,2011, Structural and Multidisciplinary Optimization
 Identity mappings in deep residualnetworks,2016, In European conference on computer vision
 Soft filter pruning for acceleratingdeep convolutional neural networks,2018, arXiv preprint arXiv:1808
 Amc: Automl for modelcompression and acceleration on mobile devices,2018, In Proceedings of the European Conference onComputer Vision (ECCV)
 Distilling the knowledge in a neural network,2015, ArXiv
 Reinforcementlearning for neural architecture search: A review,2019, Image and Vision Computing
 Learning multiple layers of features from tiny images,2009, 2009
 Optimal brain damage,1990, In Advances in neuralinformation processing systems
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Structured pruning of neural networks with budget-aware reg-ularization,2019, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Pruning filters forefficient convnets,2016, arXiv preprint arXiv:1608
 Compressing convolu-tional neural networks via factorized convolutional filters,2019, In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition
 Learn-ing efficient convolutional networks through network slimming,2017, In Proceedings of the IEEEInternational Conference on Computer Vision
 Rethinking the value ofnetwork pruning,2018, arXiv preprint arXiv:1810
 Learning sparse neural networks throughl_0 regularization,2017, arXiv preprint arXiv:1712
 Thinet: A filter level pruning method for deep neuralnetwork compression,2017, 2017 IEEE International Conference on Computer Vision (ICCV)
 Deephoyer: Learning sparser neural network with differen-tiable scale-invariant sparsity measures,2019, arXiv preprint arXiv:1908
 Wide residual networks,2016, arXiv preprintarXiv:1605
 â†‘	CIFAR-10 Param,2015, J	FLOPs J	Acc
