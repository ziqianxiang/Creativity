title,year,conference
 Online embedding compression fortext classification using low rank matrix factorization,2019, Proceedings of the AAAI Conference on ArtificialIntelligence
 K-svd: An algorithm for designing overcomplete dictionaries for sparserepresentation,1053, Trans
 Probabilistic FastText for multi-sense wordembeddings,2018, In ACL
 Towards learning sparsely used dictionaries with arbitrary supports,2018, In 2018IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS)
 Practical coreset constructions for machine learning,2017, 2017
 Clustering with bregman diver-gences,1532, J
 Adaptive importance sampling to accelerate training of a neural probabilisticlanguage model,1045, Trans
 Enriching word vectors with subwordinformation,2017, Transactions of the Association for Computational Linguistics
 The relaxation method of finding the common point of convex sets and its application to thesolution of problems in convex programming,0041, USSR Computational Mathematics and Mathematical Physics
 Truly nonparametric online variational inference for hierarchical dirichletprocesses,2012, Advances in Neural Information Processing Systems
 How large a vocabularydoes text classification need? a variational approach to vocabulary selection,2019, In Proceedings of the 2019Conference of the North American Chapter of the Association for Computational Linguistics: HumanLanguage Technologies
 Compressing neural language models by sparse wordrepresentations,2016, In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics(Volume 1: Long Papers)
 Bayesian compression for natural languageprocessing,2018, In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing
 Learning to prune deep neural networks via layer-wise optimal brainsurgeon,2017, In I
 Mixed dimension em-beddings with application to memory-efficient recommendation systems,2019, arXiv preprint arXiv:1909
 Compression of recurrent neural networksfor efficient language modeling,2019, CoRR
 Learning to hash with optimized anchor embedding for scalableretrieval,2017, IEEE Transactions on Image Processing
 Noise-contrastive estimation: A new estimation principle for unnormal-ized statistical models,2010, In Proceedings of the Thirteenth International Conference on Artificial Intelligenceand Statistics
 Texture features for image classification,1973, IEEE Transactions onSystems
 Long short-term memory,0899, Neural Comput
 Neural input search for large scalerecommendation models,2019, CoRR
 Exploring the limits oflanguage modeling,2016, arXiv preprint arXiv:1602
 Convolutional neural networks for sentence classification,2014, In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing (EMNLP)
 Adam: A method for stochastic optimization,2015, In Yoshua Bengio and YannLeCun (eds
 Nonparametric bayesian sparse factor models with application togene expression modeling,2011, Ann
 Gradient-based learning applied to documentrecognition,1998, In Proceedings of the IEEE
 Sparse convolutionalneural networks,2015, In CVPR
 Learning efficientconvolutional networks through network slimming,2017, In Proceedings of the IEEE International Conference onComputer Vision
 Effective approaches to attention-based neural machinetranslation,2015, In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing
 Building a large annotated corpusof english: The penn treebank,0891, Comput
 Pointer sentinel mixture models,2017, In5th International Conference on Learning Representations
 Regularizing and optimizing LSTM languagemodels,2018, In International Conference on Learning Representations
 Distributed representations of wordsand phrases and their compositionality,2013, In Advances in Neural Information Processing Systems 26
 Wordnet: A lexical database for english,1995, Commun
 A fast and simple algorithm for training neural probabilistic languagemodels,2012, In Proceedings of the 29th International Conference on Machine Learning
 Coresets and sketches,2016, CoRR
 Using the output embedding to improve language models,2017, In Proceedings of the15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2
 Em algorithms for pca and spca,1998, In Proceedings of the 1997 Conference on Advances in NeuralInformation Processing Systems 10
 A Bayesian nonparametric approach to marginal structuralmodels for point treatments and a continuous or survival outcome,2016, Biostatistics
 Small-variance asymptotics for hidden markov models,2013, InAdvances in Neural Information Processing Systems 26
 Word embedding based on low-rank doubly stochastic matrix decomposition,2018, InLong Cheng
 Compressing word embeddings via deep compositional code learning,2018, InInternational Conference on Learning Representations
 Cambridge Series in Statistical and Probabilistic Mathematics,2010, Cambridge University Press
 Collapsed variational inference for hdp,2007, In Proceedings ofthe 20th International Conference on Neural Information Processing Systems
 Learning structured sparsity in deep neuralnetworks,2016, In Advances in Neural Information Processing Systems 29
 Adaptive methods fornonconvex optimization,2018, In Advances in neural information processing systems
4	Word embedding dropout	0,2018,1	Input embedding dropout	0
 Larger values of Î»2 allows us to control for more sparse entriesin T at the expense of performance,2021, For experiments on dynamic mixtures
01	Learning rate decay	0,1024,5	Decay step size	100
