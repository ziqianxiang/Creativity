title,year,conference
 Deep Learning with Differential Privacy,2016, In Proceedings of the 2016 ACMSIGSAC Conference on Computer and Communications Security
 The security of machinelearning,0885, Mach
 Adversarial Examples Are Not Easily Detected: BypassingTen Detection Methods,2017, In Proceedings of the 10th ACM Workshop on Artificial Intelligence andSecurity
 ARotation and a Translation Suffice: Fooling CNNs with Simple Transformations,2017, ArXiv171202779Cs Stat
 Poisoning Attacks to Graph-BasedRecommender Systems,2018, In Proceedings of the 34th Annual Computer Security ApplicationsConference
 Model-Agnostic Meta-Learning for Fast Adaptationof Deep Networks,2017, ArXiv170303400 Cs
 Deep Residual Learning for ImageRecognition,2015, ArXiv151203385 Cs
 Onthe Effectiveness of Mitigating Data Poisoning Attacks with Gradient Shaping,2020, ArXiv200211497Cs
 Targeted Poisoning Attacks on SocialRecommender Systems,2019, In 2019 IEEE Global Communications Conference (GLOBECOM)
 MetaPoison:Practical General-purpose Clean-label Data Poisoning,2020, ArXiv200400225 Cs Stat
 Spatial TransformerNetworks,2015, In Advances in Neural Information Processing Systems 28
 Understanding Black-box Predictions via Influence Functions,2017, InInternational Conference on Machine Learning
 Stronger Data Poisoning Attacks Break DataSanitization Defenses,2018, ArXiv181100741 Cs Stat
 Imagenet classification with deep convolu-tional neural networks,2012, In Advances in Neural Information Processing Systems
 Deep learning,1476, Nature
 Data Poisoning Attacks onFactorization-Based Collaborative Filtering,2016, In Advances in Neural Information ProcessingSystems 29
 Delving into Transferable AdversarialExamples and Black-box Attacks,2017, ArXiv161102770 Cs
 Biometric Backdoors: A Poisoning AttackAgainst Unsupervised Template Updating,2019, ArXiv190509162 Cs
 Deep Neural Network Fingerprinting byConferrable Adversarial Examples,2020, ArXiv191200888 Cs Stat
 Data Poisoning against Differentially-Private Learners:Attacks and Defenses,2019, ArXiv190309860 Cs
 Towards Poisoning of Deep Learning Algorithms with Back-gradient Optimization,2017, In Proceedings of the 10th ACM Workshop on Artificial Intelligence andSecurity
 Numerical Optimization,2006, Springer Series in OperationsResearch
 A Marauderâ€™s Map of Security and Privacy in Machine Learning,2018, ArXiv181101134Cs
 Detection of AdversarialTraining Examples in Poisoning Attacks through Anomaly Detection,2018, ArXiv180203041 Cs Stat
 Label Sanitization Against LabelFlipping Poisoning Attacks,2019, In ECML PKDD 2018 Workshops
 Adversarial Robustness through LocalLinearization,2019, ArXiv190702610 Cs Stat
 ImageNetLarge Scale Visual Recognition Challenge,2015, IntJ Comput Vis
 Hidden Trigger BackdoorAttacks,2019, ArXiv191000033 Cs
 Mo-bileNetV2: Inverted Residuals and Linear Bottlenecks,2018, ArXiv180104381 Cs
 Fawkes:Protecting Personal Privacy against Unauthorized Deep Learning Models,2020, ArXiv200208327 CsStat
 Very Deep Convolutional Networks for Large-Scale ImageRecognition,2014, ArXiv14091556 Cs
 DeepFace: Closing the Gapto Human-Level Performance in Face Verification,2014, In 2014 IEEE Conference on ComputerVision andPattern Recognition
 Understandingdeep learning requires rethinking generalization,2016, ArXiv161103530 Cs
 Transferable Clean-Label Poisoning Attacks on Deep Neural Nets,2019, ArXiv190505897Cs Stat
 While using the Carlini-Wagner lossas a surrogate for cross entropy helped in Huang et al,2018, (2020)
