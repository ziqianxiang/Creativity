title,year,conference
 On the convergence rate of training recurrent neuralnetworks,2019, In Advances in Neural Information Processing Systems 32
 Exploiting thepast and the future in protein secondary structure prediction,1999, Bioinformatics
 Dynamic Programming,1957, Princeton University Press
 Statistical methods for data with long-range dependence,1992, Statistical Science
 Skip RNN:Learning to skip state updates in recurrent neural networks,2017, arXiv preprint arXiv:1708
 AntisymmetricRNN: A dynamical systemview on recurrent neural networks,2019, In International Conference on Learning Representations
 Approximations of continuous functionals by neural networks withapplication to dynamical systems,1993, IEEE Transactions on Neural Networks
 Learning phrase representations using RNN encoder-decoderfor statistical machine translation,2014, arXiv preprint arXiv:1406
 Modeling of continuous time dynamical systems with inputby recurrent neural networks,2000, IEEE Transactions on Circuits and Systems I: Fundamental Theoryand Applications
 TopicRNN: A recurrent neuralnetwork with long-range semantic dependency,2017, In 5th International Conference on LearningRepresentations
 Universality of fully connected recurrent neural networks,1993, Dept
 Approximation of dynamical systems by continuoustime recurrent neural networks,0893, Neural Networks
 Approximation bounds for randomneural networks and reservoir systems,2020, arXiv preprint arXiv:2002
 Generating sequences with recurrent neural networks,2013, arXiv preprintarXiv:1308
 Towards end-to-end speech recognition with recurrent neuralnetworks,2014, In International Conference on Machine Learning
 Offline handwriting recognition with multidimensional re-current neural networks,2009, In Advances in Neural Information Processing Systems
 Speech recognition with deep recur-rent neural networks,2013, In 2013 IEEE International Conference on Acoustics
 DRAW: A recurrentneural network for image generation,2015, volume 37 of Proceedings of Machine Learning Research
 Stable architectures for deep neural networks,2017, Inverse Problems
 Theoretical guarantees for learning condi-tional expectation using controlled ODE-RNN,2020, arXiv preprint arXiv:2006
 Long short-term memory,1997, Neural computation
 Gradient flow in re-current nets: the difficulty of learning long-term dePendencies,2001, In S
 Overcoming the vanishinggradient problem in plain recurrent networks,2018, arXiv preprint arXiv:1801
 Long-term storage capacity of reservoirs,1951, Transactions of the American Societyof Civil Engineers
 Adam: a method for stochastic optimization,2015, In Proceedings ofthe International Conference on Learning Representations (ICLR)
 Predictability: A problem partly solved,1996, In Proc
 Likelihood ratios and recurrent random neural networks in detection ofdenial of service attacks,2007, 2007
 DeepONet: Learning nonlinear operators for identifyingdifferential equations based on the universal approximation theorem of operators,2019, arXiv preprintarXiv:1910
 Computational aspects of feedback inneural circuits,2007, PLOS Computational Biology
 Approximating nonlinear fading-memory operators using neural networkmodels,1993, Circuits
 A deep learning based approach to reduced order modeling for turbulentflow control using lstm neural networks,2018, arXiv: Computational Physics
 Uber den approximationssatz Von Weierstrass,1914, In Mathematische AbhandlungenHermann Amandus Schwarz
 Recurrent neural networks in the eye ofdifferential equations,2019, arXiv preprint arXiv:1904
 Characterization of phase-type distributions,1990, Stochastic Models
 Wavenet: A generative model forraw audio,2016, arXiv preprint arXiv:1609
 On the difficulty of training recurrent neuralnetworks,2013, volume 28 of Proceedings of Machine Learning Research
 Latent ordinary differential equationsfor irregularly-sampled time series,2019, In Advances in Neural Information Processing Systems 32
 Learning representations by back-propagating errors,1986, Nature
 Long range dependence,2006, Found
 A differential equation for modeling Nesterovâ€™saccelerated gradient method: Theory and insights,2014, In Advances in Neural Information ProcessingSystems
 Improving performance of recurrent neural network with relunonlinearity,2015, arXiv preprint arXiv:1511
 Estimators for long-range dependence: An empir-ical study,1995, Fractals
 Universal approximation to nonlinear operators by neural networkswith arbitrary activation functions and its application to dynamical systems,1995, IEEE Transactionson Neural Networks
 Learning longer-term dependenciesin RNNs with auxiliary losses,2018, In ICML
 Verifying the long-range dependency of rnn lan-guage models,2016, 2016 International Conference on Asian Language Processing (IALP)
