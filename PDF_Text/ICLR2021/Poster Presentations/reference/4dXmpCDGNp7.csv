title,year,conference
 Sanitychecks for Saliency maps,2018, In Advances in Neural Information Processing Systems
 Towards robust interpretability with self-explaining neuralnetworks,2018, Advances in Neural Information Processing Systems
 A unified view of gradient-based attribution methods for deep neural networks,2018, International Conference on LearningRepresentations
 On pixel-wise explanations for non-linear classifier decisions by layer-wiserelevance propagation,2015, PloS one
 Rutgers L,1964, Rev
 Decision-based adversarial attacks: Reliableattacks against black-box machine learning models,2017, arXiv preprint arXiv:1712
 Towards evaluating the robustness of neural networks,2017, In 2017IEEE Symposium on Security and Privacy (SP)
 Explaining imageclassifiers by counterfactual generation,2019, In International Conference on Learning Representations
 Robust decision trees againstadversarial examples,2019, In ICML
 Query-efficienthard-label black-box attack: An optimization-based approach,2018, arXiv preprint arXiv:1807
 Real time image saliency for black box classifiers,2017, In Advances inNeural Information Processing Systems
 ImageNet: A Large-Scale HierarchicalImage Database,2009, In CVPR09
 Explanations based on the missing: Towards contrastive explanations with pertinentnegatives,2018, In Advances in Neural Information Processing Systems
 Learningexplainable models using attribution priors,2019, arXiv preprint arXiv:1906
 Interpretable explanations of black boxes by meaningful per-turbation,2017, 2017 IEEE International Conference on Computer Vision (ICCV)
 Ai2: Safety and robustness certification of neural networks with abstract interpretation,2018, In2018 IEEE Symposium on Security and Privacy (SP)
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Counterfactual visualexplanations,2019, In International Conference on Machine Learning
 Black box explanation bylearning image exemplars in the latent feature space,2019, In Joint European Conference on MachineLearning and Knowledge Discovery in Databases
 Grounding visual explana-tions,2018, In ECCV
 Reluplex: An efficientsmt solver for verifying deep neural networks,2017, In International Conference on Computer AidedVerification
 Understanding black-box predictions via influence functions,2017, InInternational Conference on Machine Learning
 Understanding neural networks through representationerasure,2016, CoRR
 A unified approach to interpreting model predictions,2017, In Advancesin Neural Information Processing Systems
 Visual explanation by interpretation: Improvingvisual feedback capabilities of deep neural networks,2019, In International Conference on LearningRepresentations
 Rise: Randomized input sampling for explanation ofblack-box models,2018, arXiv preprint arXiv:1806
 Model agnostic supervised local explana-tions,2018, In Advances in Neural Information Processing Systems
 Why should i trust you?: ExPlaining thePredictions of any classifier,2016, In Proceedings of the 22nd ACM SIGKDD International Conferenceon Knowledge Discovery and Data Mining
 A convex relaxationbarrier to tight robust verification of neural networks,2019, arXiv preprint arXiv:1902
 Evaluating the visualization of what a deep neural network has learned,2016, IEEE transactionson neural networks and learning systems
 Learning important features throughpropagating activation differences,2017, International Conference on Machine Learning
 Deep inside convolutional networks:Visualising image classification models and saliency maps,2013, arXiv preprint arXiv:1312
 Smoothgrad:removing noise by adding noise,2017, arXiv preprint arXiv:1706
 Visualizing the impact of feature attributionbaselines,2020, Distill
 The many shapley values for model explanation,2019, arXivpreprint arXiv:1908
 Axiomatic attribution for deep networks,2017, InInternational Conference on Machine Learning
 Efficient formal safetyanalysis of neural networks,2018, In Advances in Neural Information Processing Systems
 Towards fast computation of certified robustness for relu networks,2018, InInternational Conference on Machine Learning
 Provable defenses against adversarial examples via the convex outeradversarial polytope,2018, In International Conference on Machine Learning
 Structured adversarial attack: Towards general implementation and betterinterpretability,2018, arXiv preprint arXiv:1808
 Visualizing and understanding convolutional networks,2014, InEuropean conference on computer vision
 Efficient neural networkrobustness certification with general activation functions,2018, In Advances in neural informationprocessing systems
 Recurjac: An efficient recursive algorithm forbounding jacobian matrix of neural networks and its applications,2019, In Proceedings of the AAAIConference on Artificial Intelligence
 Character-level convolutional networks for textclassification,2015, In Advances in neural information processing systems
 Men also likeshopping: Reducing gender bias amplification using corpus-level constraints,2017, In Proceedings of the2017 Conference on Empirical Methods in Natural Language Processing
 Visualizing deep neuralnetwork decisions: Prediction difference analysis,2017, In 5th International Conference on LearningRepresentations
 Adversarial robustness has been extensivelystudied in the past few years,2018, The adversarial robustness of a machine learning model on a givensample can be defined as the shortest distance from the sample to the decision boundary
