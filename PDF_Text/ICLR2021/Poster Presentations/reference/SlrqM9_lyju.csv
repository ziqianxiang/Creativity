title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In Proceedings of the 36th International Conference on Machine Learning
 Hyperopt: A python library for optimizing thehyperparameters of machine learning algorithms,2013, In Proceedings of the 12th Python in scienceconference
 A statistical method for global optimization,1992, In Proceedings of the1992 IEEE International Conference on Systems
 Bayesian optimization meetsBayesian optimal stopping,2019, In International Conference on Machine Learning
 Speeding Up aUtomatic hyperparameteroptimization of deep neUral networks by extrapolation of learning cUrves,2015, In IJCAI
 BOHB: RobUst and efficient hyperparameter opti-mization at scale,2018, arXiv preprint arXiv:1807
 Forward and reversegradient-based hyperparameter optimization,1165, In Proceedings of the 34th International Conferenceon Machine Learning
 Deep Learning,2016, MIT press
 Identity maPPings in deeP residualnetworks,2016, In European conference on computer vision
 Sequential model-based oPtimizationfor general algorithm configuration,2011, In International conference on learning and intelligentoptimization
 Population based trainingof neural networks,2017, arXiv preprint arXiv:1711
 Non-stochastic best arm identification and hyperparameteroptimization,2016, In Artificial Intelligence and Statistics
 Three factors influencing minima in SGD,2017, arXiv preprint arXiv:1711
 Deep learning without poor local minima,2016, In Advances in neural informationprocessing systems
 Adam: A method for stochastic optimization,2015, In InternationalConference on Learning Representations
 Learning curve prediction withBayesian neural networks,2017, In International Conference on Learning Representations
 Learning multiple layers of features from tiny images,2009, Master’s thesis
 Visualizing the loss landscapeof neural nets,2018, In Advances in Neural Information Processing Systems
 A system for massively parallel hyperparameter tuning,2018, In Conference onMachine Learning and Systems
 SGDR: stochastic gradient descent with warm restarts,2017, InInternational Conference on Learning Representations
 Probabilistic line searches for stochastic optimization,2015, InAdvances in Neural Information Processing Systems
 Mixedprecision training,2018, In International Conference on Learning Representations
 Step size matters in deep learning,2018, In Advances in Neural InformationProcessing Systems
 Provably efficient online hyperparameteroptimization with population-based bandits,2020, Advances in Neural Information Processing Systems
 No more pesky learning rates,2013, In InternationalConference on Machine Learning
 Taking thehuman out of the loop: A review of Bayesian optimization,0018, Proceedings of the IEEE
 Very deep convolutional networks for large-scale imagerecognition,2015, In International Conference on Learning Representations
 Cyclical learning rates for training neural networks,2017, In 2017 IEEE Winter Conferenceon Applications of Computer Vision (WACV)
 Practical bayesian optimization of machinelearning algorithms,2012, In Advances in neural information processing systems
 Freeze-thaw bayesian optimization,2014, arXivpreprint arXiv:1406
 Lecture 6,2012,5—RmsProp: Divide the gradient by a running average of itsrecent magnitude
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Understanding short-horizon bias instochastic meta-optimization,2018, In International Conference on Learning Representations
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
 Towards automated deep learning:Efficient joint neural architecture and hyperparameter search,2018, In ICML 2018 AutoML Workshop
