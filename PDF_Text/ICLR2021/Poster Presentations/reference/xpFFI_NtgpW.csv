title,year,conference
 Massively Multilingual Neural Machine Transla-tion,2019, In Proceedings of NAACL 2019
 Towards better substitution-based word sense induction,2019, arXivpreprint arXiv:1905
 Massively Multilingual Sentence Embeddings for Zero-ShotCross-Lingual Transfer and Beyond,2019, Transactions of the ACL 2019
 On the Cross-lingual Transferability of Mono-lingual Representations,2020, In Proceedings of ACL 2020
 An unsupervised model for instance level subcat-egorization acquisition,2014, In Proceedings of the 2014 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP)
 Semantics derived automatically fromlanguage corpora contain human-like biases,2017, Science
 ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators,2020, In Proceedings of ICLR 2020
 XNLI: Evaluating cross-lingual sentence representations,2018, InProceedings ofEMNLP 2018
 Un-supervised cross-lingual representation learning at scale,2020, In Proceedings of the 58th AnnualMeeting of the Association for Computational Linguistics
 Emerging cross-lingUal strUctUre in pretrained langUage models,2020, In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics
 oLMpics - On what LangUage Model Pre-training CaptUres,2019, arXivpreprint arXiv:1912
 Donâ€™t Stop Pretraining: Adapt LangUage Models to Domains and Tasks,2020, InProceedings of ACL 2020
 Large-scale learning of wordrelatedness with constraints,2012, In Proceedings of the 18th ACM SIGKDD international conferenceon Knowledge discovery and data mining
 Distilling the Knowledge in a Neural Network,2015, arXivpreprint arXiv:1503
 Universal Language Model Fine-tuning for Text Classifica-tion,2018, In Proceedings of ACL 2018
 Tying Word Vectors and Word Classifiers:A Loss Framework for Language Modeling,2017, In Proceedings of ICLR 2017
 Cross-lingual ability of multilingualbert: An empirical study,2020, In International Conference on Learning Representations
 Association for Computational Linguistics,2012, doi:10
 ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,2020, InInternational Conference on Learning Representations
 LinguisticKnowledge and Transferability of Contextual Representations,2019, In Proceedings of NAACL 2019
 RoBERTa: A Robustly Optimized BERT Pre-training Approach,2019, arXiv preprint arXiv:1907
 Better word representations withrecursive neural networks for morphology,2013, In Proceedings of the Seventeenth Conference onComputational Natural Language Learning
 Mogrifier LSTM,2020, In Proceedings of ICLR 2020
 XtremeDistil : Multi-stage Distillation forMassive Multilingual Models,2020, In Proceedings of ACL 2020
 Universal de-pendencies 2,2018,2
 Cross-lingual name tagging and linking for 282 languages,2017, In Proceedings ofACL 2017
 MAD-X: An Adapter-based Frame-work for Multi-task Cross-lingual Transfer,2020, In Proceedings of EMNLP 2020
 English intermediate-task training improves zero-shot cross-lingual transfer too,2020, arXiv preprint arXiv:2005
 Using the output embedding to improve language models,2017, In Proceedingsof the 15th Conference of the European Chapter of the Association for Computational Linguistics:Volume 2
 Transfer learningin natural language processing,2019, In Proceedings of the 2019 Conference of the North AmericanChapter of the Association for Computational Linguistics: Tutorials
 Green AI,2019, arXiv preprintarXiv:1907
 Mesh-tensorflow: Deep learning for supercomput-ers,2018, In S
 Well-Read Students Learn Better:On the Importance of Pre-training Compact Models,2019, arXiv preprint arXiv:1908
 Attention is all you need,2017, In I
 HUggingFace's Trans-formers: State-of-the-art Natural Language Processing,2019, arXiv preprint arXiv:1910
 PAWS-X: A cross-lingual adversarialdataset for paraphrase identification,2019, In Proceedings of EMNLP 2019
 HULK: An Energy Ef-ficiency Benchmark Platform for Responsible Natural Language Processing,2020, arXiv preprintarXiv:2002
 Overview of the third bucc shared task:Spotting parallel sentences in comparable corpora,2018, In Proceedings of 11th Workshop on Buildingand Using Comparable Corpora
