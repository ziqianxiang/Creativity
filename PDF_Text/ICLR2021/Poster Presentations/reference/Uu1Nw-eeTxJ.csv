title,year,conference
 Massively multilingual sentence embeddings for zero-shotcross-lingual transfer and beyond,2019, Transactions of the Association for Computational Linguistics
 On the cross-lingual transferability of mono-lingual representations,2019, arXiv preprint arXiv:1910
 A simple framework forcontrastive learning of visual representations,2020, In Proceedings of Machine Learning and Systems2020
 Infoxlm: An information-theoretic framework for cross-lingual language model pre-training,2020, CoRR
 ELECTRA: pre-training text encoders as discriminators rather than generators,2020, In 8th International Conferenceon Learning Representations
 Cross-lingual language model pretraining,2019, InProc
 XNLI: Evaluating cross-lingual sentence representations,2018, InProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing
 Un-supervised cross-lingual representation learning at scale,2020, In Proceedings of the 58th AnnualMeeting of the Association for Computational Linguistics
 Unified language model pre-training for natural language understandingand generation,2019, In Advances in Neural Information Processing Systems 32
 Pre-trained language model representations forlanguage generation,2019, In Proceedings of the 2019 Conference of the North American Chapter ofthe Association for Computational Linguistics: Human Language Technologies
 FILTER: an enhanced fusionmethod for cross-lingual language understanding,2020, CoRR
 Language-agnostic BERT sentence embedding,2020, CoRR
 Incorporat-ing bert into parallel sequence decoding with adapters,2020, In H
 Deep residual learning for image recog-nition,2016, In 2016 IEEE Conference on Computer Vision and Pattern Recognition
 Momentum contrast forunsupervised visual representation learning,2019, CoRR
 A mutual information maximization perspective of language representation learning,2020, In8th International Conference on Learning Representations
 ALBERT: A lite BERT for self-supervised learning of language representations,2020, In8th International Conference on Learning Representations
 Very deep transformers for neural ma-chine translation,2020, arXiv preprint arXiv:2008
 Roberta: A robustly optimized BERT pretrainingapproach,2019, CoRR
 Multilingual denoising pre-training for neural machine translation,2020, CoRR
 VECO:variable encoder-decoder pre-training for cross-lingual understanding and generation,2020, CoRR
 Distributed rep-resentations of words and phrases and their compositionality,2013, In Advances in Neural InformationProcessing Systems 26
 Learning word embeddings efficiently with noise-contrastiveestimation,2013, In Advances in Neural Information Processing Systems 26
 Representation learning with contrastive predic-tive coding,2018, CoRR
 Cross-lingual name tagging and linking for 282 languages,2017, In Proceedings of the 55th Annual Meeting ofthe Association for Computational Linguistics (Volume 1: Long Papers)
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Recursive deep models for semantic compositionality over a sentimenttreebank,2013, In Proceedings of the 2013 Conference on Empirical Methods in Natural LanguageProcessing
 MASS: masked sequence to sequencepre-training for language generation,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Contrastive multiview coding,2020, In Andrea Vedaldi
 Multiscalecollaborative deep models for neural machine translation,2020, In Proceedings of the 58th AnnualMeeting of the Association for Computational Linguistics
 Acquiring knowledgefrom pre-trained model to neural machine translation,2020, In Proceedings of the AAAI Conference onArtificial Intelligence
 Unsupervised feature learning via non-parametric instance discrimination,2018, In 2018 IEEE Conference on Computer Vision and PatternRecognition
 Unsupervised embedding learning viainvariant and spreading instance feature,2019, In IEEE Conference on Computer Vision and PatternRecognition
 PAWS: Paraphrase adversaries from word scram-bling,2019, In Proceedings of the 2019 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies
 On learning language-invariant representations for uni-Versal machine translation,2020, In Hal DaUme In and Aarti Singh (eds
 Hardness-aware deep met-ric learning,2019, In IEEE Conference on Computer Vision and Pattern Recognition
 Extractivesummarization as text matching,2020, In Proceedings of the 58th Annual Meeting of the Associationfor Computational Linguistics
 Incorporating BERT into neural machine translation,2020, In 8th International Conference onLearning Representations
