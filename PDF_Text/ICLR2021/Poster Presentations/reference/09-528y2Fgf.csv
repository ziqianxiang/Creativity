title,year,conference
 Layer normalization,2016, arXiv preprintarXiv:1607
 Unilmv2: Pseudo-masked language models for unified languagemodel pre-training,2020, arXiv preprint arXiv:2002
 What does bert look at?an analysis of bertâ€™s attention,2019, ACL 2019
 Electra: Pre-trainingtext encoders as discriminators rather than generators,2019, In International Conference on LearningRepresentations
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, arXiv preprintarXiv:1901
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Efficient training of bertby progressively stacking,2019, In International Conference on Machine Learning
 Toeplitz and circulant matrices: A review,2006, now publishers inc
 Deberta: Decoding-enhanced bertwith disentangled attention,2020, arXiv preprint arXiv:2006
 zip: Compressing text classification models,2016, arXiv preprint arXiv:1612
 Adam: A method for stochastic optimization,2014, CoRR
 Constituency parsing with a self-attentive encoder,2018, arXiv preprintarXiv:1805
 An efficient algorithm for a class of fused lasso problems,2010, InProceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and datamining
 Learning to encode position fortransformer with continuous dynamical model,2020, arXiv preprint arXiv:2003
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Mixed precisiontraining,2017, arXiv preprint arXiv:1710
 Distributed representationsof words and phrases and their compositionality,2013, In Advances in neural information processingsystems
 Automatic differentiation inpytorch,2017, 2017
 Glove: Global vectors for wordrepresentation,2014, In Proceedings of the 2014 conference on empirical methods in natural languageprocessing (EMNLP)
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Neural machine translation of rare wordswith subword units,2015, CoRR
 Novel positional encodings to enable tree-based transformers,2019, InAdvances in Neural Information Processing Systems 
 Attention is all you need,2017, In Advances in Neural InformationProcessing Systems
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
 Aligning books and movies: Towards story-like visual explanations by watchingmovies and reading books,2015, In arXiv preprint arXiv:1506
