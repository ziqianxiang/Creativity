title,year,conference
 Layer normalization,2016, arXiv preprintarXiv:1607
 Learning to compute word embeddings on the fly,2017, arXiv preprint arXiv:1706
 Electra: Pre-trainingtext encoders as discriminators rather than generators,2019, In International Conference on LearningRepresentations
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Parsimonious morpheme segmentationwith an application to enriching word embeddings,2019, In 2019 IEEE International Conference on BigData (Big Data)
 En-tities as experts: Sparse memory access with entity supervision,2020, arXiv preprint arXiv:2004
 Representation degenerationproblem in training natural language generation models,2019, arXiv preprint arXiv:1907
 Frage: Frequency-agnosticword representation,2018, In Advances in neural information processing systems
 Efficient training of bertby progressively stacking,2019, In International Conference on Machine Learning
 Realm: Retrieval-augmented language model pre-training,2020, arXiv preprint arXiv:2002
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Generalizationthrough memorization: Nearest neighbor language models,2019, arXiv preprint arXiv:1911
 Enrichingrare word representations in neural language models by embedding matrix augmentation,2019, arXivpreprint arXiv:1904
 Character-aware neural languagemodels,2016, In Thirtieth AAAI conference on artificial intelligence
 Adam: A method for stochastic optimization,2014, CoRR
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Better word representations withrecursive neural networks for morphology,2013, In Proceedings of the Seventeenth Conference onComputational Natural Language Learning
 Mixed precisiontraining,2017, arXiv preprint arXiv:1710
 Analyzing uncertainty in neuralmachine translation,2018, arXiv preprint arXiv:1803
 Automatic differentiation inpytorch,2017, 2017
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Meta-learning with memory-augmented neural networks,2016, In International conference on machinelearning
 Rare words: A major problem for contextualized embeddings andhow to fix it by attentive mimicking,2020, In AAAI
 Neural machine translation of rare wordswith subword units,2015, CoRR
 Energy and policy considerations for deeplearning in nlp,2019, arXiv preprint arXiv:1906
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
 Aligning books and movies: Towards story-like visual explanations by watchingmovies and reading books,2015, In arXiv preprint arXiv:1506
