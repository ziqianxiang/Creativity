title,year,conference
 More is different,1972, Science
 A convergence analysis of gradientdescent for deep linear neural networks,2018, arXiv preprint arXiv:1810
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, arXiv preprint arXiv:1802
 Theoretical analysis of auto rate-tuning by batchnormalization,2018, arXiv preprint arXiv:1812
 Onexact computation with an infinitely wide neural net,2019, In Advances in Neural Information ProcessingSystems
 Layer normalization,2016, arXiv preprintarXiv:1607
 On pixel-wise explanations for non-linear classifier decisions by layer-wiserelevance propagation,2015, PloS one
 Neural networks and principal component analysis: Learning fromexamples without local minima,1989, Neural networks
 Implicit gradient regularization,2020, arXiv preprintarXiv:2009
 Online normalization for training neural networks,2019, InAdvances in Neural Information Processing Systems
 Riemannian approach to batch normalization,2017, In Advances inNeural Information Processing Systems
 Algorithmic regularization in learning deep homogeneousmodels: Layers are automatically balanced,2018, In Advances in Neural Information Processing Systems
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
 Deep learning versus kernel learning: an empirical study of loss landscapegeometry and the time evolution of the neural tangent kernel,2020, Adv
 Dynamicsof stochastic gradient descent for two-layer neural networks in the teacher-student setup,2019, InAdvances in Neural Information Processing Systems
 The role of symmetry in fundamental physics,1996, Proceedings of the National Academyof Sciences
 Norm matters: efficient and accuratenormalization schemes in deep networks,2018, In Advances in Neural Information Processing Systems
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Three factors influencing minima in sgd,2017, arXiv preprint arXiv:1711
 Deep learning without poor local minima,2016, In Advances in neural informationprocessing systems
 Analysis of momentum methods,2019, arXiv preprintarXiv:1906
 Loss landscapes ofregularized linear autoencoders,2019, arXiv preprint arXiv:1901
 An analytic theory of generalization dynamics and transferlearning in deep linear networks,2018, In International Conference on Learning Representations (ICLR)
 Wide neural networks of any depth evolve as linear modelsunder gradient descent,2019, In Advances in neural information processing systems
 Stochastic modified equations and adaptive stochasticgradient algorithms,2017, In International Conference on Machine Learning
 An exponential learning rate schedule for deep learning,2019, arXivpreprint arXiv:1910
 Reconciling modern deep learning with traditionaloptimization analyses: The intrinsic learning rate,2020, Advances in Neural Information ProcessingSystems
 A mean field view of the landscape of two-layer neural networks,2018, Proceedings of the National Academy of Sciences
 Path-sgd: Path-normalized optimizationin deep neural networks,2015, In Advances in Neural Information Processing Systems
 Learning representations byback-propagating errors,1986, nature
 Dynamics of on-line gradient descent learning for multilayer neuralnetworks,1995, Advances in neural information processing systems
 Exact solutions to the nonlinear dynamicsof learning in deep linear neural networks,2013, arXiv preprint arXiv:1312
 A mathematical theory of semanticdevelopment in deep neural networks,2019, Proc
 A bayesian perspective on generalization and stochastic gradientdescent,2017, arXiv preprint arXiv:1710
 A differential equation for modeling nesterovâ€™saccelerated gradient method: Theory and insights,2014, In Advances in neural information processingsystems
 Pruning neural networkswithout any data by iteratively conserving synaptic flow,2020, arXiv preprint arXiv:2006
 Fluctuation-dissipation relations for stochastic gradient descent,2018, arXiv preprintarXiv:1810
 The anisotropic noise in stochasticgradient descent: Its behavior of escaping from sharp minima and regularization effects,2018, arXivpreprint arXiv:1803
