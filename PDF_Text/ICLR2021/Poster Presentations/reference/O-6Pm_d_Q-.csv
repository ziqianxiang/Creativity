title,year,conference
 Optimization Algorithms on Matrix Manifolds,2009, PrincetonUniversity Press
 Backward feature correction: How deep learning performs deeplearning,2020, arXiv preprint arXiv:2001
 A convergence theory for deep learning via Over-Parameterization,2019, In Proceedings of the 36th International Conference on Machine Learning
 Fine-Grained analysis of opti-mization and generalization for overparameterized Two-Layer neural networks,2019, In Proceedings ofthe 36th International Conference on Machine Learning
 Concentration inequalities for order statistics,2012, ElectronicCommunications in Probability
 Concentration Inequalities: A NonaSymP-totic Theory of Independence,2013, OUP Oxford
 Invariant scattering convolution networks,2013, IEEE Trans
 Generalization bounds of stochastic gradient descent for wide anddeep neural networks,2019, In Advances in Neural Information Processing Systems
 Implicit bias of gradient descent for wide two-layer neural networkstrained with the logistic loss,1305, In Proceedings of Thirty Third Conference on Learning Theory
 Kernel methods for deep learning,2009, In Advances in neuralinformation processing systems
 Group equivariant convolutional networks,2016, In Proceedings of The33rd International Conference on Machine Learning
 Measure Theory,2013, Birkhauser
 Imaging Vis,2005,
 Gradient descent provably optimizesover-parameterized neural networks,2019, In International Conference on Learning Representations
 General-isation error in learning with random features and the hidden manifold model,2020, In Proceedings ofthe 37th International Conference on Machine Learning
 Linearized two-layersneural networks in high dimension,2019, CoRR
 Modeling the influence ofdata structure on learning in neural networks: The hidden manifold model,2020, Phys
 Finite depth and width corrections to the neural tangent kernel,2020, InInternational Conference on Learning Representations
 A Basis Theory Primer: Expanded Edition,2011, Birkhauser Boston
 Topics in matrix analysis,1994, Cambridgeuniversity press
 Polylogarithmic width suffices for gradient descent to achieve arbi-trarily small test error with shallow ReLU networks,2020, In International Conference on LearningRepresentations
 Introduction to Riemannian Manifolds,2018, Springer
 Learning Over-Parametrized Two-Layer neuralnetworks beyond NTK,2020, In Proceedings of Thirty Third Conference on Learning Theory
 Group invariant scattering,2012, Commun
 A mean field view of the landscape oftwo-layer neural networks,2018, Proc
 Spectral normalizationfor generative adversarial networks,2018, In International Conference on Learning RePresentations
 The interpolation phase transition in neural networks: Mem-orization and generalization under lazy training,2020, arXiv PrePrint arXiv:2007
 AsPects of Multivariate Statistical Theory,1982, Wiley Series in Probability andStatistics
 Optimal rates for averaged stochastic gradient descent underneural tangent kernel regime,2021, In International Conference on Learning RePresentations
 Generalization guaran-tees for neural networks via harnessing the low-rank structure of the jacobian,2019, arXiv PrePrintarXiv:1906
 Non-asymptotic theory of random matrices: Extreme sin-gular values,2011, In Proceedings of the International Congress of Mathematicians 2010 (ICM 2010)
 Plug-and-Play methods provably converge with properly trained denoisers,2019, In Proceedings of the 36thInternational Conference on Machine Learning
 Block coordinate regularization by denoising,2020, IEEE Transactionson Computational Imaging
 Generalization bound of globally optimal non-convex neural network training: Trans-portation map estimation by infinite dimensional langevin dynamics,2020, In Advances in NeuralInformation Processing Systems
 A mean-field theory of lazy training in two-layer neu-ral nets: entropic regularization and controlled McKean-Vlasov dynamics,2020, arXiv preprintarXiv:2002
 The multiscalestructure of non-differentiable image manifolds,2005, In Wavelets XI
 Multi-Manifold Modeling in Non-Euclideanspaces,2015, In Proceedings of the Eighteenth International Conference on Artificial Intelligence andStatistics
 Sharp asymptotic and finite-sample rates of convergence of em-pirical measures in wasserstein distance,2019, Bernoulli
 The comparison geometry of ricci curvature,1997, In Comparison Geometry
 The proof is an application of Lemma B,2021,7
 â–¡Lemma B,2021,7 (Nominal to Finite)
 Following the calculations ofLemmas C,2021,10 and C
 Using lemma D,2021,15
1 Core Supporting ResultsLemma E,0000,6
25 denote the cutoff function defined in Lemma E,2021,31
 In the notation of Lemma E,0000,13
 Invoking then Lemma G,2013,9
 By Lemma E,2021,17
