title,year,conference
 Post training 4-bit quantization of convolutionalnetworks for rapid-deployment,2019, In NeurIPS
 Scalable methods for 8-bit training ofneural networks,2018, In NeurIPS
 Nice: Noise injection and clamping estimation for neuralnetwork quantization,2018, arXiv preprint arXiv:1810
 signsgd:compressed optimisation for non-convex problems,2018, In ICML
 High-dimensional norms,2015, In Mathematical Statistics and LimitTheorems
 Shifted and squeezed 8-bit floating point format for low-precision training of deep neuralnetworks,2020, In ICLR
 Imagenet: A large-scalehierarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Post-training piecewise linear quantization for deep neural networks,2020, In ECCV
 Fast distributed training of deep neural networks: Dynamiccommunication thresholding for model and data parallelism,2020, ArXiv
 Deep residual learning for imagerecognition,2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Leveraging the bfloat16 artificialintelligence datatype for higher-precision computations,2019, In 2019 IEEE 26th Symposium onComputer Arithmetic (ARITH)
 Algorithm 65: Find,0001, Commun
 Trained quantization thresholds foraccurate and efficient fixed-point inference of deep neural networks,2020, In Machine Learning andSystems
 A study of bfloat16 for deep learning training,2019, arXiv preprint arXiv:1905
 Learning sparse neural networks throughl0 regularization,2018, In ICLR
 Loss aware post-training quantization,2019, arXiv preprint arXiv:1911
 Distributed equivalent substitution training for large-scalerecommender systems,2020, Proceedings of the 43rd International ACM SIGIR Conference on Researchand Development in Information Retrieval
 Very deep convolutional networks for large-scale imagerecognition,2015, CoRR
 meprop: Sparsified back propagationfor accelerated deep learning with reduced overfitting,2017, In ICML
 Attention is all you need,2017, In NeurIPS
 Training deepneural networks with 8-bit floating point numbers,2018, In NeurIPS
 Training and inference with integers in deepneural networks,2018, In ICLR
 Accelerating cnntraining by pruning activation gradients,2019, arXiv preprint arXiv:1908
