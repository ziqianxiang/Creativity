title,year,conference
 Weighted transformer network for machinetranslation,2017, arXiv preprint arXiv:1711
 Neural headline generation with minimum risktraining,2016, arXiv preprint arXiv:1604
 Controlling computation versus quality forneural sequence models,2020, arXiv preprint arXiv:2002
 A parallel corpus of python functions anddocumentation strings for automated code documentation and code generation,2017, arXiv preprintarXiv:1707
 Curriculum learning,2009, InProceedings ofthe 26th annual international conference on machine learning
 Graph transformer for graph-to-sequence learning,2019, arXiv preprintarXiv:1911
 End-to-end object detection with transformers,2020, arXiv preprint arXiv:2005
 Active bias: Training moreaccurate neural networks by emphasizing high variance samples,2017, In Advances in Neural InformationProcessing Systems
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Depth-adaptive transformer,2020, InInternational Conference on Learning Representations
 Reducing transformer depth on demand withstructured dropout,2019, In International Conference on Learning Representations
 Learning to teach,2018, In InternationalConference on Learning Representations
 English gigaword,2003, Linguistic DataConsortium
 Achieving human parityon automatic chinese to english news translation,2018, arXiv preprint arXiv:1803
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Identity mappings in deep residualnetworks,2016, In European conference on computer vision
 Categorical reparameterization with gumbel-softmax,2016, arXivpreprint arXiv:1611
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Reformer: The efficient transformer,2020, InInternational Conference on Learning Representations
 Cross-lingual language model pretraining,2019, arXiv preprintarXiv:1901
 Set trans-former: A framework for attention-based permutation-invariant neural networks,2019, In InternationalConference on Machine Learning
 Ensure the correctness of the summary:Incorporate entailment knowledge into abstractive sentence summarization,2018, In Proceedings of the27th International Conference on Computational Linguistics
 Rouge: A packagefor automatic evaluation of summaries,2004, In ProceedingsofWorkshopon Text Summarization Branches Out
 Global encoding for abstractive summarization,2018, InProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume2: Short Papers)
 Fastbert: a self-distillingbert with adaptive inference time,2020, arXiv preprint arXiv:2004
 Scaling neural machine translation,2018, InProceedings of the Third Conference on Machine Translation: Research Papers
 Improving transformer models by reordering theirsublayers,2019, arXiv preprint arXiv:1911
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 A neural attention model forsentence summarization,2017, In ACLWeb
 The righttool for the job: Matching model and instance complexities,2020, arXiv preprint arXiv:2004
 Neural machine translation of rare words withsubword units,2015, arXiv preprint arXiv:1508
 Edinburgh neural machine translation systemsfor wmt 16,2016, arXiv preprint arXiv:1606
 Outrageously large neural networks: The sparsely-gated mixture-of-experts layer,2017, arXivpreprint arXiv:1701
 The evolved transformer,2019, In International Conference onMachine Learning
 Rethinkingthe inception architecture for computer vision,2016, In Proceedings of the IEEE conference on computervision and pattern recognition
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Improving neural language modeling via adversarialtraining,2019, In International Conference on Machine Learning
 A reinforced topic-awareconvolutional sequence-to-sequence model for abstractive text summarization,2018, In Proceedings ofthe 27th International Joint Conference on Artificial Intelligence
 Multi-agent dual learning,2019, In 7th International Conference on Learning Representations
 Code generation as a dual task of code summariza-tion,2019, In Advances in Neural Information Processing Systems
