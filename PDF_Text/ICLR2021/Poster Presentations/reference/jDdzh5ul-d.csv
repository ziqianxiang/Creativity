title,year,conference
 Semi-cyclicstochastic gradient descent,2019, arXiv preprint arXiv:1904
 Measuring the effects of non-identical datadistribution for federated visual classification,2019, arXiv preprint arXiv:1909
 Loadaboost: Loss-basedadaboost federated machine learning on medical data,2018, arXiv preprint arXiv:1811
 Advancesand open problems in federated learning,2019, arXiv preprint arXiv:1912
 Scaffold: Stochastic controlled averaging for on-device federatedlearning,2019, arXiv preprint arXiv:1910
 Better communication complexity forlocal sgd,2019, arXiv preprint arXiv:1909
 First analysis of local gd on heteroge-neous data,2019, arXiv preprint arXiv:1909
 Learning multiple layers of features from tiny images,2009, 2009
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 On the convergence offedavg on non-iid data,2019, arXiv preprint arXiv:1907
 Communication-efficientlearning of deep networks from decentralized data,2016, arXiv preprint arXiv:1602
 Adaptive federated optimization,2020, arXiv preprintarXiv:2003
 Robust andcommunication-efficient federated learning from non-iid data,2019, IEEE transactions on neuralnetworks and learning systems
 Local sgd converges fast and communicates little,2018, arXiv preprint arXiv:1805
 The error-feedback framework: Better rates for sgdwith delayed gradients and compressed communication,2019, arXiv preprint arXiv:1909
 Sparsified sgd with memory,2018, InAdvances in Neural Information Processing Systems
 Cooperative sgd: A unified framework for the design and analysis ofcommunication-efficient sgd algorithms,2018, arXiv preprint arXiv:1808
 Slowmo: Improvingcommunication-efficient distributed sgd with slow momentum,2019, arXiv preprint arXiv:1910
 On the linear speedup analysis of communication efficientmomentum sgd for distributed non-convex optimization,2019, arXiv preprint arXiv:1905
 Parallel restarted sgd with faster convergence and lesscommunication: Demystifying why model averaging works for deep learning,2019, In Proceedings ofthe AAAI Conference on Artificial Intelligence
 Federatedlearning with non-iid data,2018, arXiv preprint arXiv:1806
 On the convergence properties of a k-step averaging stochastic gradientdescent algorithm for nonconvex optimization,2017, arXiv preprint arXiv:1708
 Thisproblem is more prominent for highly non-i,2021,i
