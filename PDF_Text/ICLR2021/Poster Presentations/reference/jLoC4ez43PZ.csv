title,year,conference
 Learning to represent programswith graphs,2018, In International Conference on Learning Representations
 code2seq: Generating sequences fromstructured representations of code,2018, arXiv preprint arXiv:1808
 Generativecode modeling with graphs,2018, arXiv preprint arXiv:1805
 Exploring software natural-ness throughneural language models,2020, arXiv preprint arXiv:2006
 Tree-to-tree neural networks for program translation,2018, InAdvances in neural information processing systems
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Codebert: A pre-trained model for programming and naturallanguages,2020, arXiv preprint arXiv:2002
 Coupling retrieval and meta-learning forcontext-dependent semantic parsing,2019, ArXiv
 Code-searchnet challenge: Evaluating the state of semantic code search,2019, arXiv preprint arXiv:1909
 Deckard: Scalable andaccurate tree-based detection of code clones,2007, In 29th International Conference on SoftwareEngineering (ICSEâ€™07)
 Phrase-based statistical translation ofprogramming languages,2014, In Proceedings of the 2014 ACM International Symposium on New Ideas
 Code prediction by feeding trees totransformers,2020, arXiv preprint arXiv:2003
 Statistical phrase-based translation,2003, Technicalreport
 Cross-lingual language model pretraining,2019, arXiv preprintarXiv:1901
 Code completion with neural attention andpointer networks,2017, arXiv preprint arXiv:1711
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Lexical statistical machine translationfor language migration,2013, In Proceedings of the 2013 9th Joint Meeting on Foundations of SoftwareEngineering
 Tree-structured attention withhierarchical accumulation,2019, In International Conference on Learning Representations
 Deep contextualized word representations,2018, arXiv preprint arXiv:1802
 Abstract syntax networks for code generationand semantic parsing,2017, arXiv preprint arXiv:1704
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Intellicode compose:Code generation using transformer,2020, arXiv preprint arXiv:2005
 An empirical study on learning bug-fixing patches in the wild via neural machinetranslation,2019, ACM Transactions on Software Engineering and Methodology (TOSEM)
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Detecting code clones with graph neuralnetworkand flow-augmented abstract syntax tree,2020, arXiv preprint arXiv:2002
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
