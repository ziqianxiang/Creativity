title,year,conference
 Natural gradient works efficiently in learning,1998, Neural computation
 Implicit gradient regularization,2021, 9th International Conference onLearning Representations
 Stochastic gradient descent tricks,2012, In Neural networks: Tricks of the trade
 Batch normalization biases residual blocks towards the identity functionin deep networks,2020, Advances in Neural Information Processing Systems
 Three Factors Influencing Minima in SGD,2018, In Artificial Neural Networksand Machine Learning
 The break-even point on optimization trajectories of deep neuralnetworks,2020, arXiv preprint arXiv:2002
 On large-batch training for deep learning: Generalization gap and sharp minima,2017, InInternational Conference on Learning Representations
 Efficient backprop,2012, InNeural networks: Tricks of the trade
 The largelearning rate phase of deep learning: the catapult mechanism,2020, arXiv preprint arXiv:2003
 Towards explaining the regularization effect of initial largelearning rate in training neural networks,2019, In Advances in Neural Information Processing Systems
 An empirical model oflarge-batch training,2018, arXiv preprint arXiv:1812
 The effect of network width onstochastic gradient descent and generalization: an empirical study,2019, In International Conferenceon Machine Learning
 SGD implicitly regularizes generalization error,2018, In Integration of Deep LearningTheories Workshop
 Measuring the effects of data parallelism on neural network training,2018, arXivpreprint arXiv:1811
 A tail-index analysis of stochastic gradientnoise in deep neural networks,2019, In International Conference on Machine Learning
 A bayesian perspective on generalization and stochastic gradientdescent,2018, In International Conference on Learning Representations
 On the generalization benefit of noise in stochasticgradient descent,2020, In International Conference on Machine Learning
 Fashion-MNIST: a Novel Image Dataset for Bench-marking Machine Learning Algorithms,2017, arXiv preprint arXiv:1708
 Fluctuation-dissipation relations for stochastic gradient descent,2019, In 7th InternationalConference on Learning Representations
 Wide residual networks,2016, In Proceedings of the BritishMachine Vision Conference (BMVC)
 Which Algorithmic Choices Matter at Which BatchSizes? Insights From a Noisy Quadratic Model,2019, In Advances in Neural Information ProcessingSystems
