title,year,conference
 The fifth pascal recognizingtextual entailment challenge,2009, In TAC
 What does bert lookat? an analysis of bertâ€™s attention,2019, arXiv preprint arXiv:1906
 Xnli: Evaluating cross-lingual sentence representations,2018, arXivpreprint arXiv:1809
 Un-supervised cross-lingual representation learning at scale,2019, arXiv preprint arXiv:1911
 Bert: Pre-training of deepbidirectional transformers for langUage Understanding,2018, arXiv preprint arXiv:1810
 Teaching machines to read and comprehend,2015, In Advances inneural information processing systems
 Smart:RobUst and efficient fine-tUning for pre-trained natUral langUage models throUgh principled regU-larized optimization,2019, arXiv preprint arXiv:1911
 Abstractive sUmmarization of reddit postswith mUlti-level memory networks,2018, arXiv preprint arXiv:1811
 Roberta: A robUstly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Spectral normalizationfor generative adversarial networks,2018, arXiv preprint arXiv:1802
 Revisiting natural gradient for deep networks,2013, arXiv preprintarXiv:1301
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 Trust regionpolicy optimization,2015, In International conference on machine learning
 Trust regionpolicy optimization,2015, In International conference on machine learning
 On mutualinformation maximization for representation learning,2019, arXiv preprint arXiv:1907
 Prophetnet: Predicting future n-gram for sequence-to-sequence pre-training,2020, arXivpreprint arXiv:2001
 Pegasus: Pre-training with extractedgap-sentences for abstractive summarization,2019, arXiv preprint arXiv:1912
 Revisiting few-sample bert fine-tuning,2020, arXiv preprint arXiv:2006
 Freelb: Enhancedadversarial training for natural language understanding,2019, In International Conference on LearningRepresentations
