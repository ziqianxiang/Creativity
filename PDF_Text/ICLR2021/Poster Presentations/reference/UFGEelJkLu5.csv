title,year,conference
 UniLMv2: Pseudo-masked Language Models for UnifiedLanguage Model Pre-training,2020, arXiv preprint arXiv:2002
 The Fifth PASCAL RecognizingTextual Entailment Challenge,2009, TAC
 ReMixMatch: Semi-Supervised Learning with Distribution Alignment and Aug-mentation Anchoring,2019, International Conference on Learning Representations
 ELECTRA: Pre-trainingText Encoders as Discriminators Rather than Generators,2020, arXiv preprint arXiv:2003
 The PASCAL Recognising Textual EntailmentChallenge,2005, Machine Learning Challenges Workshop
 BERT: Pre-training of DeepBidirectional Transformers for Language Understanding,2018, arXiv preprint arXiv:1810
 Automatically Constructing a Corpus of Sentential Para-phrases,2005, International Workshop on Paraphrasing
 Augmenting Data with Mixup for Sentence Clas-sification: An Empirical Study,2019, arXiv preprint arXiv:1905
 The Second PASCAL Recognising Textual Entailment Challenge,2006, PASCAL ChallengesWorkshop on Recognising Textual Entailment
 Distilling the Knowledge in a Neural Network,2015, arXivpreprint arXiv:1503
 TinyBERT: Distilling BERT for Natural Language Understanding,2019, arXiv preprintarXiv:1909
 ImageNet Classification with Deep Con-volutional Neural Networks,2012, Neural Information Processing Systems
 Adversarial Vertex Mixup: Toward Better Adver-sarially Robust Generalization,2020, Computer Vision and Pattern Recognition
 Attentive Student MeetsMulti-task Teacher: Improved Knowledge Distillation for Pretrained Models,2019, arXiv preprintarXiv:1911
 Multi-task Deep Neural Networksfor Natural Language Understanding,2019, arXiv preprint arXiv:1901
 RoBERTa: A Robustly Optimized BERT Pre-training Approach,2019, arXiv preprint arXiv:1907
 Improved Knowledge Distillation via Teacher Assistant,2019, arXiv preprintarXiv:1902
 Distilling Transformers into Simple NeuralNetworks with Unlabeled Transfer Data,2019, arXiv preprint arXiv:1910
 CoDA:Contrast-enhanced and Diversity-promoting Data Augmentation for Natural Language Under-standing,2020, arXiv preprint arXiv:2010
 A Simple but Tough-to-Beat Data Augmentation Approach for Natural Language Understanding and Generation,2020, arXivpreprint arXiv:2009
 Transformation Invariancein Pattern Recognition-Tangent Distance and Tangent Propagation,1998, Neural Networks: Tricks ofthe Trade
 Best Practices for Convolutional Neural Networks Appliedto Visual Document Analysis,2003, International Conference on Document Analysis and Recognition
 Recursive Deep Models for Semantic Compositionality over a SentimentTreebank,2013, Empirical Methods in Natural Language Processing
 Patient Knowledge Distillation for BERT ModelCompression,2019, arXiv preprint arXiv:1908
 ERNIE: Enhanced Representation through Knowledge Integration,2019, arXivpreprint arXiv:1904
 Distilling Task-specific Knowledge from BERT into Simple Neural Networks,2019, arXiv preprint arXiv:1903
 Well-read Students Learn Better:On the Importance of Pre-training Compact Models,2019, arXiv preprint arXiv:1908
 GraphMix:Improved Training of GNNs for Semi-Supervised Learning,2019, arXiv preprint arXiv:1909
 Neural Networks Are More Pro-ductive Teachers Than Human Raters: Active Mixup for Data-Efficient Knowledge Distillationfrom a Blackbox Model,2020, Computer Vision and Pattern Recognition
 EDA: Easy Data Augmentation Techniques for Boosting Performance onText Classification Tasks,2019, arXiv preprint arXiv:1901
 A Broad-coverage Challenge Corpusfor Sentence Understanding through Inference,2018, North American Chapter of the Association forComputational Linguistics: Human Language Technologies
 Conditional BERT Contex-tual Augmentation,2019, International Conference on Computational Science
 Unsupervised DataAugmentation for Consistency Training,2019, arXiv preprint arXiv:1904
 QANet: Combining Local Convolution with Global Self-attention for ReadingComprehension,2018, arXiv preprint arXiv:1804
 mixup: Beyond Empiri-cal Risk Minimization,2018, International Conference on Learning Representations
 Extreme Language Model Compres-sion with Optimal Subwords and Shared Projections,2019, arXiv preprint arXiv:1909
