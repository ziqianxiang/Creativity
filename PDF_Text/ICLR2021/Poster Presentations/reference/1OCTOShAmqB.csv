title,year,conference
 Neural Machine Translation by JointlyLearning to Align and Translate,2014, In International Conference on Learning Representations (ICLR)
 On Identifiability in Transformers,2019, In International Conference on Learning Representations(ICLR)
 What Does BERT Lookat? An Analysis of BERTâ€™s Attention,2019, arXiv preprint arXiv:1906
 Hamiltonian Neural Networks,2019, In NeuralInformation Processing Systems (NIPS)
 Theoretical Limitations of Self-Attention in Neural Sequence Models,2020, Transactionsof the Association for Computational Linguistics
 Infinite attention : NNGP and NTK fordeep attention networks,2020, In International Conference on Machine Learning (ICML)
 Deep neural networks as gaussian processes,2018, In International Conference onLearning Representations (ICLR)
 Effective approaches to attention-basedneural machine translation,2015, In Empirical Methods in Natural Language Processing (EMNLP)
 Automatic differentiation inpytorch,2017, In Neural Information Processing Systems (NIPS)
 Exact solutions to the nonlineardynamics of learning in deep linear neural networks,2014, In International Conference on LearningRepresentations (ICLR)
 Parsing With Compositional Vector Grammars,2013, In Empirical Methods in NaturalLanguage Processing (EMNLP)
 Rethinking theInception Architecture for Computer Vision,2016, In IEEE Computer Society Conference on ComputerVision and Pattern Recognition (CVPR)
 Attention Inter-pretability Across NLP Tasks,2020, In International Conference on Learning Representations (ICLR)
 Attention is all you need,2017, In Neural Information Processing Systems(NIPS)
 Tensor Programs I : Wide Feedforward or Recurrent Neural Networks of Any Architectureare Gaussian Processes,2019, In Neural Information Processing Systems (NIPS)
