title,year,conference
 Understanding double descent requires a fine-grained bias-variance decomposition,2020, arXiv preprint arXiv:2011
 A continuous-time view of early stopping for leastsquares,2019, In International Conference on Artificial Intelligence and Statistics
 Neural learning in structured parameter spaces-natural riemannian gradient,1997, InAdvances in neural information processing Systems
 Natural gradient works efficiently in learning,1998, Neural computation
 Adaptive method of realizing natural gra-dient learning for multilayer perceptrons,2000, Neural computation
 Implicit regularization in deep matrixfactorization,2019, In Advances in Neural Information Processing Systems
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, arXiv preprintarXiv:1901
 Generalization oftwo-layer neural networks: An asymptotic viewpoint,2020, International Conference on LearningRepresentations
 Limit of the smallest eigenvalue of a large dimensional samplecovariance matrix,2008, In Advances In Statistics
 Benign overfitting in linearregression,2019, arXiv preprint arXiv:1906
 Reconciling modern machine learningand the bias-variance trade-off,2018, arXiv preprint arXiv:1812
 Overfitting or perfect fitting? risk bounds forclassification and regression rules that interpolate,2018, In Advances in neural information processingsystems
 On the inductive bias of neural tangent kernels,2019, In Advances inNeural Information Processing Systems
 Implicit bias of gradient descent for wide two-layer neural networkstrained with the logistic loss,2020, arXiv preprint arXiv:2002
 On the mathematical foundations of learning,2002, Bulletin of theAmerican mathematical society
 Can implicit bias explain generalization?stochastic convex optimization as a case study,2020, arXiv preprint arXiv:2003
 A model of double descent for high-dimensional binary linear classification,2019, arXiv preprint arXiv:1911
 Metric-free naturalgradient for joint-training of boltzmann machines,2013, arXiv preprint arXiv:1301
 Sharp minima can generalizefor deep nets,2017, arXiv preprint arXiv:1703
 High-dimensional asymptotics of prediction: Ridge regressionand classification,2018, The Annals of Statistics
 Gener-alisation error in learning With random features and the hidden manifold model,2020, arXiv preprintarXiv:2002
 Limitations of lazytraining of tWo-layers neural netWorks,2019, arXiv preprint arXiv:1906
 Implicit regularization of discrete gradientdynamics in deep linear neural netWorks,2019, arXiv preprint arXiv:1904
 Characterizing implicit bias interms of optimization geometry,2018, arXiv preprint arXiv:1802
 Surprises in high-dimensional ridgeless least squares interpolation,2019, arXiv preprint arXiv:1903
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In Advances in neural information processing systems
 Gradient descent aligns the layers of deep linear networks,2018, arXivpreprint arXiv:1810
 The implicit bias of gradient descent on nonseparable data,2019, InConference on Learning Theory
 Universal statistics of fisher information indeep neural networks: Mean field approach,2018, arXiv preprint arXiv:1806
 Asymptotic behavior of unregularized and ridge-regularized high-dimensional robust regression estimators: rigorous results,2013, arXiv preprint arXiv:1311
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Limitations of the empirical fisher approxi-mation,2019, arXiv preprint arXiv:1905
 Efficient backprop,2012, InNeural networks: Tricks of the trade
 Necessary and sufficient geometries for gradient methods,2019, InAdvances in Neural Information Processing Systems
 Gradient descent with early stop-ping is provably robust to label noise for overparameterized neural networks,2019, arXiv preprintarXiv:1903
 Algorithmic regularization in over-parameterizedmatrix sensing and neural networks with quadratic activations,2017, arXiv preprint arXiv:1712
 Just interpolate: Kernel” ridgeless” regression can gener-alize,2018, arXiv preprint arXiv:1808
 The dynamics of learning: a random matrix approach,2018, arXivpreprint arXiv:1805
 Deep learning via hessian-free optimization,2010, In ICML
 New insights and perspectives on the natural gradient method,2014, arXiv preprintarXiv:1412
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine learning
 Training deep and recurrent networks with hessian-free opti-mization,2012, In Neural networks: Tricks of the trade
 The generalization error of random features regression: Preciseasymptotics and double descent curve,2019, arXiv preprint arXiv:1908
 The generalization error of max-margin linear classifiers: High-dimensional asymptotics in the overparametrized regime,2019, arXivpreprint arXiv:1911
 Gradient descent in rkhs with importance labeling,2020, arXiv preprintarXiv:2006
 Deepdouble descent: Where bigger models and more data hurt,2019, arXiv preprint arXiv:1912
 Revisiting natural gradient for deep networks,2013, arXiv preprintarXiv:1301
 The implicit bias of adagrad on separable data,2019, In Advances inNeural Information Processing Systems
 Implicit regularization in deep learning may not be explainable bynorms,2020, arXiv preprint arXiv:2005
 Falkon: An optimal large scale kernelmethod,2017, In Advances in neural information processing SyStemS
 Exact solutions to the nonlinear dynam-ics of learning in deep linear neural networks,2013, arXiv preprint arXiv:1312
 Optimal rates for regularized least squares regres-sion,2009, In COLT
 On learning over-parameterized neural networks: A functional approx-imation perspective,2019, In Advances in Neural Information Processing Systems
 Connecting optimization and regulariza-tion paths,2018, In Advances in Neural Information Processing Systems
 Generalization bound of globally optimal non-convex neural network training:Transportation map estimation by infinite dimensional langevin dynamics,2020, arXiv preprintarXiv:2007
 Perturbation theory for pseudo-inverses,1973, BIT Numerical Mathematics
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
 Kernel and rich regimes in overparametrized models,2020, arXivpreprint arXiv:2002
 Diverse neural network learns true target functions,2016, arXivpreprint arXiv:1611
 Second-order optimization for non-convex ma-chine learning: An empirical study,2020, In Proceedings of the 2020 SIAM International Conferenceon Data Mining
 Feature learning in infinite-width neural networks,2020, arXiv preprintarXiv:2011
 Which algorithmic choices matter at which batch sizes? insightsfrom a noisy quadratic model,2019, In Advances in Neural Information Processing Systems
 Fast convergence of natural gradient descentfor over-parameterized neural networks,2019, In Advances in Neural Information Processing Systems
 NGD ,2014,
01 that is exponentially decayed every 1k updateswith the parameter value 0,1952,999
