title,year,conference
 Byzantine stochastic gradient descent,2018, InAdvances in Neural Information Processing Systems 31: Annual Conference on NeuralInformation Processing Systems 2018
 A little is enough: Circumventing defensesfor distributed learning,2019, In Advances in Neural Information Processing Systems 32: AnnualConference on Neural Information Processing Systems 2019
 signsgdwith majority vote is communication efficient and fault tolerant,2019, In 7th InternationalConference on Learning Representations
 Machinelearning with adversaries: Byzantine tolerant gradient descent,2017, In Advances in NeuralInformation Processing Systems 30: Annual Conference on Neural Information ProcessingSystems 2017
 DRACO:byzantine-resilient distributed training via redundant gradients,2018, In Proceedings of the35th International Conference on Machine Learning
 Distributed statistical machine learning in adversarialsettings: Byzantine gradient descent,2017, CoRR
 Momentum-based variance reduction in non-convex SGD,2019, In Hanna M
 The hidden vulnerabilityof distributed learning in byzantium,2018, In Proceedings of the 35th International Conferenceon Machine Learning
 Genuinely distributed byzantine machine learning,2020, In Yuval Emek and ChristianCachin (eds
 Why momentum really works,2017, Distill
 Scaling distributed machine learningwith the parameter server,2014, In 11th USENIX Symposium on Operating Systems Design andImplementation
 Deep gradient compression: Reduc-ing the communication bandwidth for distributed training,2018, In International Conference onLearning Representations
 Byzantine-robust federated machinelearning through adaptive model averaging,2019, arXiv preprint arXiv:1909
 Some methods of speeding up the convergence of iteration methods,1964, USSRComputational Mathematics and Mathematical Physics
 Detox: Aredundancy-based framework for faster and more robust gradient aggregation,2019, NeuralInformation Processing Systems
 Implementing fault-tolerant services using the state machine approach: Atutorial,1990, ACM Computing Surveys (CSUR)
 Aggregationrules based on stochastic gradient descent in byzantine consensus,2019, In 2019 IEEE 8th JointInternational Information Technology and Artificial Intelligence Conference (ITAIC)
 Fashion-mnist: a novel image dataset forbenchmarking machine learning algorithms,2017, arXiv preprint arXiv:1708
 Phocas: dimensional byzantine-resilientstochastic gradient descent,2018, CoRR
 Zeno: Distributed stochastic gradient descentwith suspicion-based fault-tolerance,2019, In Proceedings of the 36th International Conferenceon Machine Learning
 Bridge: Byzantine-resilient decentralized gradientdescent,2019, arXiv preprint arXiv:1908
 Byrdie: Byzantine-resilient distributed coordinatedescent for decentralized learning,2019, IEEE Transactions on Signal and Information Processingover Networks
 Adversary-resilient inference andmachine learning: From distributed to decentralized,2019, arXiv preprint arXiv:1908
 Python 3,1080,7
