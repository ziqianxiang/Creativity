title,year,conference
 A convergence analysis of gradientdescent for deep linear neural networks,2019, In International Conference on Learning Representations(ICLR)
 Implicit bias of gradient descent for wide two-layer neural networkstrained with the logistic loss,2020, In Conference on Computational Learning Theory (COLT)
 On lazy training in supervised differentiableprogramming,2019, In Conference on Neural Information Processing Systems (NeurIPS)
 Stochastic subgradientmethod converges on tame functions,2020, Foundations of computational mathematics
 Characterizing implicit bias interms of optimization geometry,2018, In International Conference on Machine Learing (ICML)
 Implicit bias of gradient de-scent on linear convolutional networks,2018, In Conference on Neural Information Processing Systems(NeurIPS)
 Gradient descent aligns the layers of deep linear networks,2019, InInternational Conference on Learning Representations (ICLR)
 The implicit bias of gradient descent on nonseparable data,2019, InConference on Computational Learning Theory (COLT)
 Directional convergence and alignment in deep learning,2020, InarXiv:2006
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In Conference on Neural Information Processing Systems (NeurIPS)
 Gradient descent quantizes ReLU networkfeatures,2018, In arXiv:1803
 Convergence of gradient descent on separable data,2019, In Conference onUncertainty in Artificial Intelligence (AISTATS)
 Understanding machine learning: From theory to algo-rithms,2014, Cambridge university press
