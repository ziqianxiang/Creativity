title,year,conference
 Neural network learning: Theoretical foundations,2009, cambridgeuniversity press
 Stronger generalization bounds fordeep nets via a compression approach,2018, arXiv preprint arXiv:1802
 Fine-grained analysis of op-timization and generalization for overparameterized two-layer neural networks,2019, In InternationalConference on Machine Learning
 Almost linear vc dimension bounds for piecewisepolynomial networks,1999, In Advances in neural information processing systems
 Spectrally-normalized margin bounds forneural networks,2017, In Advances in Neural Information Processing Systems
 Overfitting or perfect fitting? risk bounds forclassification and regression rules that interpolate,2018, In Advances in neural information processingsystems
 Algorithmic stability and generalization performance,2001, InAdvances in Neural Information Processing Systems
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Introduction to online optimization,2011, Lecture Notes
 Generative pretraining from pixels,2020, In Proceedings of the 37th InternationalConference on Machine Learning
 Implicit bias of gradient descent for wide two-layer neural networkstrained with the logistic loss,2020, arXiv preprint arXiv:2002
 Improved regularization of convolutional neural networkswith cutout,2017, arXiv preprint arXiv:1708
 Nas-bench-201: Extending the scope of reproducible neural architec-ture search,2020, In International Conference on Learning Representations
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 In search of robust measures of generaliza-tion,2020, Advances in Neural Information Processing Systems
 Bootstrap methods: Another look at the jackknife,1979, Ann
 An introduction to the bootstrap,1994, CRC press
 Threedworld: A platform forinteractive multi-modal physical simulation,2020, arXiv preprint arXiv:2007
 Online learning with non-convex losses and non-stationary regret,2018, In International Conference on Artificial Intelligence and Statistics
 Size-independent sample complexity ofneural networks,2018, In Conference On Learning Theory
 Affinity and diversity:Quantifying mechanisms of data augmentation,2020, arXiv preprint arXiv:2002
 Characterizing implicit bias interms of optimization geometry,2018, arXiv preprint arXiv:1802
 Implicit bias of gradient descenton linear convolutional networks,2018, In Advances in Neural Information Processing Systems
 Nearly-tight vc-dimension bounds for piece-wise linear neural networks,2017, In Conference on Learning Theory
 Introduction to online convex optimization,2019, arXiv preprint arXiv:1909
 Deep residual learning for image recog-nition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Identity mappings in deep residualnetworks,2016, In European conference on computer vision
 Denoising diffusion probabilistic models,2020, arXiv preprintarxiv:2006
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Evaluation of neural architectures trained with square loss vs cross-entropy in classification tasks,2020, arXiv preprint arXiv:2006
 Non-convex optimization for machine learning,2017, arXiv preprintarXiv:1712
 The implicit bias of gradient descent on nonseparable data,2019, InConference on Learning Theory
 Fantasticgeneralization measures and where to find them,2019, arXiv preprint arXiv:1912
 How to escapesaddle points efficiently,2017, arXiv preprint arXiv:1703
 Scaling laws for neural languagemodels,2020, arXiv preprint arXiv:2001
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Large scale learning of general visual representations for transfer,2019, arXivpreprint arXiv:1912
 Learning multiple layers of features from tiny images,2009, Masterâ€™s thesis
 Imagenet classification with deep convolu-tional neural networks,2012, In Advances in neural information processing systems
 Gradient descent onlyconverges to minimizers,2016, In Conference on learning theory
 DARTS: Differentiable architecture search,2019, InInternational Conference on Learning Representations
 Data structures for statistical computing in python,2010, In Proceedings of the 9thPython in Science Conference
 Towards learning convolutions from scratch,2020, arXiv preprint arXiv:2007
 In search of the real inductive bias: On therole of implicit regularization in deep learning,2015, In ICLR (Workshop)
 Norm-based capacity control in neuralnetworks,2015, In Conference on Learning Theory
 A pac-bayesian approach tospectrally-normalized margin bounds for neural networks,2017, arXiv preprint arXiv:1707
 To-wards understanding the role of over-parametrization in generalization of neural networks,2018, arXivpreprint arXiv:1805
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Understanding machine learning: From theory to algo-rithms,2014, Cambridge university press
 Online learning and online convex optimization,2011, Foundations and trendsin Machine Learning
 Very deep convolutional networks for large-scale imagerecognition,2015, In ICLR
 Revisiting unreasonable ef-fectiveness of data in deep learning era,2017, In Proceedings of the IEEE international conference oncomputer vision
 On the uniform convergence of relative frequencies of eventsto their probabilities,1971, Theory of Probability and its Applications
 Improved sample complexities for deep neural networks and robustclassification via an all-layer margin,2019, In International Conference on Learning Representations
 HUggingface's transformers:State-of-the-art natural language processing,2019, ArXiv
 Aggregated residual trans-formations for deep neural networks,2017, In Proceedings of the IEEE conference on computer visionand pattern recognition
 An optimalalgorithm for online non-convex learning,2018, Proceedings of the ACM on Measurement and Analysisof Computing Systems
 Wide residual networks,2016, arXiv preprintarXiv:1605
 Understandingdeep learning requires rethinking generalization,2017, In ICLR
