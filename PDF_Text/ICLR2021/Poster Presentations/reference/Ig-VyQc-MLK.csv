title,year,conference
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Espn: Extremely sparse pruned networks,2020, arXivpreprint arXiv:2006
 Progressive skeletonization: Trimming more fat from a network at initialization,2020, arXivpreprint arXiv:2006
 Sparse networks from scratch: Faster training without losingperformance,2019, arXiv preprint arXiv:1907
 The difficulty of training sparseneural networks,2019, arXiv preprint arXiv:1906
 Rigging the lottery:Making all tickets winners,2020, In International Conference on Machine Learning
 Linear modeconnectivity and the lottery ticket hypothesis,2020, In International Conference on Machine Learning
 The state of sparsity in deep neural networks,2019, arXivpreprint arXiv:1902
 Gradient descent happens in a tiny subspace,2018, arXivpreprint arXiv:1812
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Amc: Automl for modelcompression and acceleration on mobile devices,2018, In Proceedings of the European Conference onComputer Vision (ECCV)
 Pruning versus clipping in neural networks,1989, Physical Review A
 The break-even point on optimization trajectories of deep neuralnetworks,2020, In International Conference on Learning Representations
 Optimal brain damage,1990, In Advances in neuralinformation processing systems
 SNIP: Single-shot network pruning basedon connection sensitivity,2019, In International Conference on Learning Representations
 Pruning filters forefficient convnets,2017, In International Conference on Learning Representations
 Finding trainable sparse networks through neural tangent trans-fer,2020, In International Conference on Machine Learning
 Scalable training of artificial neural networks with adaptive sparse connec-tivity inspired by network science,2018, Nature communications
 Norm-based capacity control in neuralnetworks,2015, In Conference on Learning Theory
 Pruning algorithms-a survey,1993, IEEE transactions on Neural Networks
 Comparing rewinding and fine-tuning in neuralnetwork pruning,2020, In International Conference on Learning Representations
 Energy and policy considerations for deeplearning in nlp,2019, In Proceedings of the 57th Annual Meeting of the Association for ComputationalLinguistics
 Pruning neural networkswithout any data by iteratively conserving synaptic flow,2020, arXiv preprint arXiv:2006
 Pruning via iterative ranking of sensitivity Statis-tics,2020, arXiv preprint arXiv:2006
 Picking winning tickets before training bypreserving gradient flow,2020, In International Conference on Learning Representations
 Drawing early-bird tickets: Toward more efficient trainingof deep networks,2020, In International Conference on Learning Representations
 Wide residual networks,2016, arXiv preprintarXiv:1605
 The unpruned networks implemented by Lee et al,2016, (2019) appear poorlytuned such that they do not achieve standard performance levels
