title,year,conference
 Towards a human-like open-domain chatbot,2020, arXivpreprint arXiv:2001
 A closer look at memorization in deepnetworks,2017, In International Conference on Machine Learning
 Neural machine translation by jointly learning toalign and translate,2014, arXiv preprint arXiv:1409
 Jump to better conclusions:SCAN both left and right,2018, In EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networksfor NLP
 The description length of deep learning models,2018, In NeurIPS
 Optimization methods for large-scale machine learning,2018, SiamReview
 Word-orderbiases in deep-agent emergent communication,2019, arXiv preprint arXiv:1905
 Language modeling with gated convolutionalnetworks,2017, In ICML
 Cnns found to jump around more skillfully than rnns: Compositionalgeneralization in seq2seq convolutional networks,2019, In ACL
 Cognitive science in the era of artificial intelligence: A roadmap for reverse-engineeringthe infant language-learner,2018, Cognition
 Hierarchical neural story generation,2018, In ACL
 Learning inductive biases with simple neural networks,2018, arXiv preprintarXiv:1802
 Convolutional sequence tosequence learning,2017, In ICML
 Deep learning,2016, MIT press
 Long short-term memory,1997, Neural computation
 Adam: A method for stochastic optimization,2014, arXiv preprint arXiv:1412
 Generalization without systematicity: On the compositional skills ofsequence-to-sequence recurrent networks,2017, arXiv preprint arXiv:1711
 Human few-shot learning of compositional instructions,2019, arXivpreprint arXiv:1901
 Handwritten digit recognition with a back-propagation network,1990, In NeurIPS
 Rearranging the familiar: Testing compositional generalizationin recurrent networks,2018, arXiv preprint arXiv:1807
 The learnability of abstract syntactic principles,2011, Cognition
 Language models areunsupervised multitask learners,2019, OpenAI blog
 Modeling by shortest data description,1978, Automatica
 Cognitive psychology for deep neuralnetworks: A shape bias case study,2017, In ICML
 A formal theory of inductive inference,1964, part i
 Sequence to sequence learning with neural networks,2014, In NeurIPS
 Lstm networks can performdynamic counting,2019, arXiv preprint arXiv:1906
 Attention is all you need,2017, In NeurIPS
 Identity crisis: Memorizationand generalization under extreme overparameterization,2019, arXiv preprint arXiv:1902
