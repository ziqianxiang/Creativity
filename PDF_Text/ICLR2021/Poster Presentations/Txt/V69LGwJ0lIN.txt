Published as a conference paper at ICLR 2021
OPAL: Offline Primitive Discovery for Accel-
erating Offline Reinforcement Learning
Anurag Ajay* 1, Aviral Kumar3, Pulkit Agrawal1, Sergey Levine2,3, Ofir Nachum2
1MIT, 2GoogleResearch, 3UCBerkeley
Ab stract
Reinforcement learning (RL) has achieved impressive performance in a variety of
online settings in which an agent’s ability to query the environment for transitions
and rewards is effectively unlimited. However, in many practical applications, the
situation is reversed: an agent may have access to large amounts of undirected
offline experience data, while access to the online environment is severely lim-
ited. In this work, we focus on this offline setting. Our main insight is that, when
presented with offline data composed of a variety of behaviors, an effective way
to leverage this data is to extract a continuous space of recurring and temporally
extended primitive behaviors before using these primitives for downstream task
learning. Primitives extracted in this way serve two purposes: they delineate the
behaviors that are supported by the data from those that are not, making them use-
ful for avoiding distributional shift in offline RL; and they provide a degree of tem-
poral abstraction, which reduces the effective horizon yielding better learning in
theory, and improved offline RL in practice. In addition to benefiting offline policy
optimization, we show that performing offline primitive learning in this way can
also be leveraged for improving few-shot imitation learning as well as exploration
and transfer in online RL on a variety of benchmark domains. Visualizations and
code are available at https://sites.google.com/view/opal-iclr
1	Introduction
Reinforcement Learning (RL) systems have achieved impressive performance in a variety of on-
line settings such as games (Silver et al., 2016; Tesauro, 1995; Brown & Sandholm, 2019) and
robotics (Levine et al., 2016; Dasari et al., 2019; Peters et al., 2010; Parmas et al., 2019; Pinto &
Gupta, 2016; Nachum et al., 2019a), where the agent can act in the environment and sample as
many transitions and rewards as needed. However, in many practical applications the agent’s ability
to continuously act in the environment may be severely limited due to practical concerns (Dulac-
Arnold et al., 2019). For example, a robot learning through trial and error in the real world requires
costly human supervision, safety checks, and resets (Atkeson et al., 2015), rendering many stan-
dard online RL algorithms inapplicable (Matsushima et al., 2020). However, in such settings we
might instead have access to large amounts of previously logged data, which could be logged from
a baseline hand-engineered policy or even from other related tasks. For example, in self-driving
applications, one may have access to large amounts of human driving behavior; in robotic applica-
tions, one might have data of either humans or robots performing similar tasks. While these offline
datasets are often undirected (generic human driving data on various routes in various cities may not
be directly relevant to navigation of a specific route within a specific city) and unlabelled (generic
human driving data is often not labelled with the human’s intended route or destination), this data is
still useful in that it can inform the algorithm about what is possible to do in the real world, without
the need for active exploration.
In this paper, we study how, in this offline setting, an effective strategy to leveraging unlabeled and
undirected past data is to utilize unsupervised learning to extract potentially useful and temporally
extended primitive skills to learn what types of behaviors are possible. For example, consider a
dataset of an agent performing undirected navigation in a maze environment (Figure 1). While the
dataset does not provide demonstrations of exclusively one specific point-to-point navigation task,
* Work done during an internship at Google Brain
1
Published as a conference paper at ICLR 2021
Figure 1: Visualization of (a subset of) diverse datasets for (a) antmaze medium and (c) antmaze
large, along with trajectories sampled from CQL+OPAL trained on diverse datasets of (b) antmaze
medium and (d) antmaze large.
it nevertheless presents clear indications of which temporally extended behaviors are useful and
natural in this environment (e.g., moving forward, left, right, and backward), and our unsupervised
learning objective aims to distill these behaviors into temporally extended primitives. Once these
locomotive primitive behaviors are extracted, we can use them as a compact constrained temporally-
extended action space for learning a task policy with offline RL, which only needs to focus on task
relevant navigation, thereby making task learning easier. For example, once a specific point-to-point
navigation is commanded, the agent can leverage the learned primitives for locomotion and only
focus on the task of navigation, as opposed to learning locomotion and navigation from scratch.
We refer to our proposed unsupervised learning method as Offline Primitives for Accelerating of-
fline reinforcement Learning (OPAL), and apply this basic paradigm to offline RL, where the agent
is given a single offline dataset to use for both the initial unsupervised learning phase and then a
subsequent task-directed offline policy optimization phase. Despite the fact that no additional data
is used, we find that our proposed unsupervised learning technique can dramatically improve offline
policy optimization compared to performing offline policy optimization on the raw dataset directly.
To the best of our knowledge, ours is the first work to theoretically justify and experimentally verify
the benefits of primitive learning in offline RL settings, showing that hierarchies can provide tempo-
ral abstraction that allows us to reduce the effect of compounding errors issue in offline RL. These
theoretical and empirical results are notably in contrast to previous related work in online hierarchi-
cal RL (Nachum et al., 2019b), which found that improved exploration is the main benefit afforded
by hierarchically learned primitives. We instead show significant benefits in the offline RL setting,
where exploration is irrelevant.
Beyond offline RL, and although this isn’t the main focus of the work, we also show the applicability
of our method for accelerating RL by incorporating OPAL as a preprocessing step to standard online
RL, few-shot imitation learning, and multi-task transfer learning. In all settings, we demonstrate that
the use of OPAL can improve the speed and quality of downstream task learning.
2	Related Work
Offline RL. Offline RL presents the problem of learning a policy from a fixed prior dataset of
transitions and rewards. Recent works in offline RL (Kumar et al., 2019; Levine et al., 2020; Wu
et al., 2019; Ghasemipour et al., 2020; Jaques et al., 2019; Fujimoto et al., 2018) constrain the
policy to be close to the data distribution to avoid the use of out-of-distribution actions (Kumar
et al., 2019; Levine et al., 2020). To constrain the policy, some methods use distributional penalties,
as measured by KL divergence (Levine et al., 2020; Jaques et al., 2019), MMD (Kumar et al., 2019),
or Wasserstein distance (Wu et al., 2019). Other methods first sample actions from the behavior
policy and then either clip the maximum deviation from those actions (Fujimoto et al., 2018) or just
use those actions (Ghasemipour et al., 2020) during the value backup to stay within the support of
the offline data. In contrast to these works, OPAL uses an offline dataset for unsupervised learning
of a continuous space of primitives. The use of these primitives for downstream tasks implicitly
constrains a learned primitive-directing policy to stay close to the offline data distribution. As we
demonstrate in our experiments, the use of OPAL in conjunction with an off-the-shelf offline RL
algorithm in this way can yield significant improvement compared to applying offline RL to the
dataset directly.
2
Published as a conference paper at ICLR 2021
Online skill discovery. There are a number of recent works (Eysenbach et al., 2018; Nachum et al.,
2018a; Sharma et al., 2019) which use unsupervised objectives to discover skills and use the discov-
ered skills for planning (Sharma et al., 2019), few-shot imitation learning, or online RL (Eysenbach
et al., 2018; Nachum et al., 2018a). However, these works focus on online settings and assume
access to the environment. In contrast, OPAL focuses on settings where a large dataset of diverse
behaviors is provided but access to the environment is restricted. It leverages these static offline
datasets to discover primitive skills with better state coverage and avoids the exploration issue of
learning primitives from scratch.
Hierarchical policy learning. Hierarchical policy learning involves learning a hierarchy of policies
where a low-level policy acts as primitive skills and a high-level policy directs the low-level policy
to solve a task. While some works (Bacon et al., 2017; Stolle & Precup, 2002; Peng et al., 2019)
learn a discrete set of lower-level policies, each behaving as a primitive skill, other works (Vezhn-
evets et al., 2017; Nachum et al., 2018b; 2019a; Hausman et al., 2018) learn a continuous space of
primitive skills representing the lower-level policy. These methods have mostly been applied in on-
line settings. However, there have been some recent variants of the above works (Lynch et al., 2020;
Shankar & Gupta, 2020; Krishnan et al., 2017; Merel et al., 2018) which extract skills from a prior
dataset and using it for either performing tasks directly (Lynch et al., 2020) or learning downstream
tasks (Shankar & Gupta, 2020; Krishnan et al., 2017; Merel et al., 2018) with online RL. While
OPAL is related to these works, we mainly focus on leveraging the learned primitives for asymptot-
ically improving the performance of offline RL; i.e., both the primitive learning and the downstream
task must be solved using a single static dataset. Furthermore, we provide performance bounds for
OPAL and enumerate the specific properties an offline dataset should possess to guarantee improved
downstream task learning, while such theoretical guarantees are largely absent from existing work.
3	Preliminaries
We consider the standard Markov decision process (MDP) setting (Puterman, 1994), specified by
a tuple M = (S, A, P, μ, r, Y) where S represents the state space, A represents the action space,
P(s0∣s, a) represents the transition probability, μ(s) represents the initial state distribution, r(s, a) ∈
(-Rmax, Rmax) represents the reward function, and γ ∈ (0, 1) represents the discount factor. A
policy π in this MDP corresponds to a function S → ∆(A), where ∆(A) is the simplex over A.
It induces a discounted future state distribution dπ, defined by dπ(s) = (1 - γ) Pt∞=0 γtP(st =
s|n), where P(St = s∣∏) is the probability of reaching the state S at time t by running ∏ on M.
For a positive integer k, We use d∏(S) = (1 - γk) Pt= YtkP(Stk = s∣∏) to denote the every-
k-step state distribution of π. The return of policy π in MDP M is defined as JRL(π, M) =
1-1γEs〜d∏,a〜∏(a∣s) [r(s, a)]. We represent the reward- and discount-agnostic environment as a tuple
E =(S, A, P,μ).
We aim to use a large, unlabeled, and undirected experience dataset D := {τir := (St, at)tc=- 01}iN=1
associated with E to extract primitives and improve offline RL for downstream task learning. To
account for the fact that the dataset D may be generated by a mixture of diverse policies start-
ing at diverse initial states, we assume D is generated by first sampling a behavior policy ∏ 〜Π
along with an initial state S 〜κ, where Π, K represent some (unknown) distributions over poli-
cies and states, respectively, and then running π on E for c time steps starting at S0 = S.
We define the probability of a sub-trajectory τ := (S0, a0, . . . , Sc-1, ac-1) in D under a pol-
icy π as π(τ) = κ(S0) Q；]： P(St∣St-ι, at-ι) QC-1 π(at∣St), and the conditional probability as
π(τ|s) = 1[s = so] QC-1 P(St∣St_1, at-1) Q；-： π(at∣St). Inthis work, we will show how to apply
unsupervised learning techniques to D to extract a continuous space of primitives ∏θ (a∣S,z), where
Z ∈ Z, the latent space inferred by unsupervised learning. We intend touse the learned ∏θ (a∣S, z) to
asymptotically improve the performance of offline RL for downstream task learning. For offline RL,
we assume the existence of a dataset Dr := {τir := (St, at, rt)t；=-01}iN=1, corresponding to the same
sub-trajectories in D labelled with MDP rewards. Additionally, we can use the extracted primitives
for other applications such as few-shot imitation learning, online RL, and online multi-task transfer
learning. We review the additional assumptions for these applications in Appendix A.
3
Published as a conference paper at ICLR 2021
Forward .........A	Autoencoding loss + KL constraint 3 -	∙ Fine-tuning with BC ∙4 - A	Offline RL
a
(1) Offline unsupervised primitive learning
with OPAL
Primitive Policy
∏θ(a∣s, Z)
(2) Offline training of task policy
z
every step
zTask Policy
πψ(ZIs) ∖ once per C steps
(3) Test-time policy execution
s
Figure 2: Overview of offline RL with OPAL. OPAL is trained on unlabelled data D using autoen-
coding objective. For offline RL, the encoder first labels the reward-labelled data Dr with latents,
and divides it into Dhri and Dlro . The task policy πψ is trained on Dhri using offline RL while the
primitive policy πθ is finetuned on Dlo using behavioral cloning (BC).
4 Offline RL with OPAL
In this section, we elaborate on OPAL, our proposed method for extracting primitives from D and
then leveraging these primitives to learn downstream tasks with offline RL. We begin by describ-
ing our unsupervised objective, which distills D into a continuous space of latent-conditioned and
temporally-extended primitive policies ∏(a|s, z). For learning downstream tasks with offline RL,
We first label Dr with appropriate latents using the OPAL encoder qφ(z∖τ) and then learn a policy
∏ψ(z∖s) which is trained to sample an appropriate primitive every C steps to optimize a specific
task, using any off-the-shelf offline RL algorithm. A graphical overview of offline RL with OPAL
is shown in Figure 2. While we mainly focus on offline RL, we briefly discuss how to use the
learned primitives for few-shot imitation learning, online RL, and multi-task online transfer learning
in section 5 and provide more details in Appendix A.
4.1 Extracting temporally-extended primitives from data
We would like to extract a continuous space of temporally-extended primitives πθ(a∖s, z) from D
which we can later use as an action space for learning downstream tasks with offline RL. This
would reduce our effective task horizon, thereby making the downstream learning easier, as well as
allow the downstream policy to stay close to the offline data distribution, thereby bringing stability
to the downstream learning. We propose the following objective for learning πθ, incorporating an
auto-encoding loss function with a KL constraint to encourage better generalization:
∕∖
min J(θ,φ,ω) = ET〜D,z〜qφ(z∣τ)
θ,φ,ω
c-1
-	log πθ(at∖st, z)
t=0
(1)
∕∖
s.t. ET〜D [DκL(qφ(z∖τ)∖∖ρω (z∖sθ))] ≤ ∈KL	(2)
∕∖
where E indicates empirical expectation. The learned components of this objective may be inter-
preted as encoder, decoder, and prior:
Encoder: qφ(z∖τ) encodes the trajectory τ of state-action pairs into a distribution in latent space
and gives out parameters of that distribution. In our case, we represent qφ with a bidirectional GRU
which takes in τ and gives out parameters of a Gaussian distribution (μenc, σfnc).
Decoder (aka Primitive Policy): πθ(a∖s, z) is the latent-conditioned policy. It maximizes the con-
ditional log-likelihood of actions in τ given the state and the latent vector. In our implementation,
we parameterize it as a feed-forward neural network which takes in current state and latent vector
and gives out parameters of a Gaussian distribution for the action (μa, σa).
Prior/Primitive Predictor: ρω(z∖s0) tries to predict the encoded distribution of the sub-trajectory
τ from its initial state. Our implementation uses a feed-forward neural network which takes in the
initial state and gives out parameters of a Gaussian distribution (μpr, σpr).
4
Published as a conference paper at ICLR 2021
KL-constraint (Equation 2). As an additional component of the algorithm, we enforce consistency
in the latent variables predicted by the encoder qφ(z∖τ) and the prior ρω(z|so). Since our goal
is to obtain a primitive z that captures a temporal sequence of actions for a given sub-trajectory
T = (so, ao,…，Sc-ι,ac-ι) (as defined in Section 3), We utilize a regularization that enforces the
distribution, qφ(z∖τ) to be close to just predicting the primitive or the latent variable z given the start
state of this sub-trajectory, s0 (i.e. ρω(z∖s0)). This conditioning on the initial state regularizes the
distribution qφ(z∖τ) to not overfit to the the complete sub-trajectory τ as the same z should also be
predictable only given s0. The above form of KL constraint is inspired from past Works (Lynch et al.,
2020; Kumar et al., 2020a). In particular Lynch et al. (2020) add a KL-constraint (Equation 2, “Plan
prior matching” in Lynch et al. (2020)) that constrains the distribution over latent variables computed
only given the initial state and the goal state to the distribution over latent variables computed using
the entire trajectory. Our form in Equation 2 is similar to this prior except that We do not operate in
a goal-conditioned RL setting and hence only condition ρω on the initial state s0 .
In practice, rather than solving the constrained optimization directly, We implement the KL con-
straint as a penalty, Weighted by an appropriately chosen coefficient β. Thus, one may interpret our
unsupervised objective as using a sequential β-VAE (Higgins et al., 2016). HoWever, as mentioned
above, our prior is conditioned on s0 and learned as part of the optimization because the set of prim-
itives active in D depends on s0. If β = 1, OPAL is equivalent to a conditional VAE maximizing
log probability ofτ conditioned on its initial state s0; see Appendix D for more details. Despite the
similarities betWeen our proposed objective and VAEs, our presentation of OPAL as a constrained
auto-encoding objective is deliberate. As We Will shoW in Section 4.3, our theoretical guarantees
depend on a Well-optimized auto-encoding loss to provide benefits of using learned primitives πθ
for doWnstream tasks. In contrast, a VAE loss, Which simply maximizes the likelihood of observed
data, may not necessarily provide a benefit for doWnstream tasks. For example, if the data can be
generated by a single stationary policy, a VAE-optimal policy πθ can simply ignore the latent z, thus
producing a degenerate space of primitives. In contrast, When the KL constraint in our objective is
Weak (i.e., KL 0 or β < 1), the auto-encoding loss is encouraged to find a unique z for distinct
τ to optimize reconstruction loss.
4.2	Offline RL with primitives for downstream tasks
After distilling learned primitives from D in terms of an encoder qφ(z∖τ), a latent primitive policy
(or decoder) πθ(a∖s, z), and a prior ρω(z∖s0), OPAL then applies these learned models to improve
offline RL for doWnstream tasks.
As shoWn in Figure 2, our goal is to use a dataset With reWard labeled sub-trajectories Dr = {τi :=
(sit,ait,rti)tc=-01}iN=1 to learn a behavior policy π that maximizes cumulative reWard. With OPAL, We
use the learned primitives πθ(a∖s, z) as loW-level controllers, and then learn a high-level controller
πψ(z∖s). To do so, We relabel the dataset Dr in terms of temporally extended transitions using
the learned encoder qφ(z∖τ). Specifically, We create a dataset Dhri = {(si0, zi, Ptc=-01 γtrti, sic)}iN=1,
where Zi 〜qφ(∙∖τi). Given D<i, any off-the-shelf offline RL algorithm can be used to learn ∏ψ (z∖s)
(in our experiments We use CQL (Kumar et al., 2020b)). As aWay to ensure that the c-step transitions
τi := (sit, ait, rti)tc=-01 remain consistent with the labelled latent action zi, we finetune πθ(a∖s, z) on
Dlro = {((sit,ati)tc=-01, zi)}iN=1 with a simple latent-conditioned behavioral cloning loss:
c-1
mθin E(τ,z)〜Dr) 一 £ log ∏θ (at∖st,z) .	(3)
t=0
4.3	Suboptimality and performance bounds for OPAL
Now, we will analyze OPAL and derive performance bounds for it in the context of offline RL,
formally examining the benefit of the temporal abstraction afforded by OPAL as well as studying
what properties D should possess so that OPAL can improve downstream task performance.
As explained above, when applying OPAL to offline RL, we first learn the primitives πθ (a∖s, z)
using D, and then learn a high-level task policy ∏ψ (z∖s) in the space of the primitives. Let ∏ψ* (z∖s)
be the optimal task policy. Thus the low-level ∏ and high-level ∏ψ* together comprise a hierarchical
policy, which we denote as πθ,ψ* . To quantify the performance of policies obtained from OPAL,
5
Published as a conference paper at ICLR 2021
We define the notion of suboptimality of the learned primitives ∏θ(a|s, z) in an MDP M with an
associated optimal policy ∏* as
SubOPt(θ):= ∣Jrl(∏*,M)- Jrl(∏θ,ψ*,M)].
(4)
To relate SubOPt(θ) with some notion of divergence between ∏* and ∏θ,ψ*, we introduce the fol-
lowing performance difference lemma.
Lemma 4.0.1. If π1 and π2 are two policies in M, then
| Jrl(∏1, M)- Jrl(∏2, M)| ≤
2
(I- Yc)(I- Y)
RmaxEsyi [DTV(πl(τ|S)||n2(T|S))],
(5)
where Dτv(∏ι(τ ∣s)∣∣∏2(τ |s)) denotes the TV divergence over C -length sub-trajectories T sampled
from π1 vs. π2 (see section 3). Furthermore,
SubOPt(θ) ≤
2
(I- Yc)(I- Y)
RmaxEs〜d∏* [Dtv(∏*(t∣s)∣∣∏θ,ψ* (τ[s))].
(6)
The proof of the above lemma and all the following results are provided in Appendix B.1.
Through above lemma, we showed that the suboptimality of the learned primitives can be bounded
by the total variation divergence between the optimal policy ∏* in M and the optimal policy acting
through the learned primitives ∏θ,ψ*. We now continue to bound the divergence between ∏* and
∏θ,ψ* in terms of how representative D is of ∏* and how optimal the primitives ∏θ are with respect
to the auto-encoding objective (equation 1). We begin with a definition of how often an arbitrary
policy appears in Π, the distribution generating D:
Definition 1. We say a policy π in M is ζ-common in Π if En〜口” K [Dtv(∏(t ∣s)∣∣π(τ |s))] ≤ Z.
Theorem 4.1.	Let θ, φ, ω be the outputs of solving equation 1, such that J(θ, φ, ω) = c. Then, with
high probability 1 一 δ, for any π that is ζ-common in Π, there exists a distribution H over Z such
that for πH (τ |s) := Ez 〜H [∏θ (T |z, s)],
Es〜κ[Dτv(∏(τ∣s)∣∣∏H(τ|s))] ≤ ζ +
(7)
∖
where Hc = En〜∏,τ〜∏,so〜κ[Pc-1 log ∏(at∣st)] (i.e. a constant and property of D) and SJ is a
positive constant incurred due to sampling error in J(θ, φ, ω) and depends on concentration prop-
erties of ∏θ(a|s, z) and qφ(z∖τ).
Corollary 4.1.1. Ifthe optimal policy π* of M is ζ-common in Π, and ∣∣ dc^ ∣∣	≤ ξ, then, with
high probability 1 - δ,
SUbOPt(θ) ≤ (1 - γjj1 - Y) Rmax (ζ + t 2 (ec + JSJ + Hc)j .
(8)
As we can see, SUbOPt(θ) will reduce as D gets closer to ∏ (i.e. ζ approaches 0) and better
primitives are learned (i.e. ec decreases). While it might be tempting to increase c (i.e. the length of
sub-trajectories) to reduce the suboptimality, a larger c will inevitably make it practically harder to
control the autoencoding loss ec, thereby leading to an increase in overall suboptimality and inducing
a trade-off in determining the best value of c. In our experiments we treat c as a hyperparameter and
set it to c = 10, although more sophisticated ways to determine c can be an interesting avenue for
future work.
Till now, we have argued that there exists some near-optimal task policy πψ* if θ is sufficiently
learned and ∏* is sufficiently well-represented in D. Now, we will show how primitive learning can
improve downstream learning, by considering the benefits of using OPAL with offline RL. Building
on the policy performance analysis from Kumar et al. (2020b), we now present theoretical results
bounding the performance of the policy obtained when offline RL is performed with OPAL.
6
Published as a conference paper at ICLR 2021
Environment	BC	BEAR	EMAQ	CQL	CQL+OPAL (ours)
antmaze medium (diverse)	0~0XΓ	8.0	00	53.7 ± 6.1	81.1 ± 3.1
antmaze large (diverse)	-00-	0.0	00	14.9 ± 3.2	70.3 ± 2.9
kitchen mixed	^55~	47.2	70.8 ± 2.3	52.4 ± 2.5	69.3 ± 2.7
kitchen partial	~338~	13.1	74.6 ± 06	50.1 ± 1.0	80.2 ± 2.4
Table 1: Average success rate (%) (over 4 seeds) of offline RL methods: BC, BEAR (Kumar et al.,
2019), EMAQ (Ghasemipour et al., 2020), CQL (Kumar et al., 2020b) and CQL+OPAL (ours).
Theorem 4.2.	Let ∏ψ*(z∣s) be the policy obtained by CQL and let ∏ψ*,θ(a|s) refer to the policy
when ∏ψ*(z∣s) is used together with ∏θ (a|s, z). Let ∏β ≡ {∏ ； ∏ 〜∏} refer to the policy generating
DrinMDPM andZ 〜∏H(z|s) ≡ τ 〜∏β,so=s,z 〜qψ(z∖τ). Then, J(∏ψ*,θ, M) ≥ J(∏β, M)-κ
with high probability 1 - δ where
κ = O ((1- γc)(1- Y) Es 7北,θ (S) hq∖Z∖(DCQL(πΨ* ,πH )(s) + 1) i )	⑼
-ταcEs〜d"£ (S) [DCQL(πΨ*,θ,πH)(S) * * * *] , (IO)
1 - γ	MH
where DCQL is a measure of the divergence between two policies; see the appendix for a formal
statement.
The precise bound along with a proof is described in Appendix B.1. Intuitively, this bound suggests
that the worst-case deterioration over the learned policy depends on the divergence between the
learned latent-space policy DCQL and the actual primitive distribution, which is controlled via any
conservative offline RL algorithm (Kumar et al. (2020b) in our experiments) and the size of the
latent space ∖Z∖. Crucially, note that comparing Equation 9 to the performance bound for CQL
(Equation 6 in Kumar et al. (2020b)) reveals several benefits pertaining to (1) temporal abstraction
-a reduction in the factor of horizon by virtue of γc, and (2) reduction in the amount of worst-case
error propagation due to a reduced action space ∖Z∖ vs. ∖A∖. Thus, as evident from the above bound,
the total error induced due to a combination of distributional shift and sampling is significantly
reduced when OPAL is used as compared to the standard RL counterpart of this bound which is
affected by the size of the entire action space for each and every timestep in the horizon. This
formalizes our intuition that OPAL helps to partly mitigate distributional shift and sampling error.
One downside of using a latent space policy is that we incur unsupervised learning error while
learning primitives. However, empirically, this unsupervised learning error gets dominated by other
error terms pertaining to offline RL. That is, itis much easier to control unsupervised loss than errors
arising in offline RL.
5 Evaluation
In this section, we will empirically show that OPAL improves learning of downstream tasks with
offline RL, and then briefly show the same with few-shot imitation learning, online RL, and online
multi-task transfer learning. Unless otherwise stated, we use c = 10 and dim(Z) = 8. See Ap-
pendix C for further implementation and experimental details. Visualizations and code are available
at https://sites.google.com/view/opal-iclr
5.1 Offline RL with OPAL
Description: We use environments and datasets provided in D4RL (Fu et al., 2020). Since the aim
of our method is specifically to perform offline RL in settings where the offline data comprises varied
and undirected multi-task behavior, we focus on Antmaze medium (diverse dataset), Antmaze large
(diverse dataset), and Franka kitchen (mixed and partial datasets). The Antmaze datasets involve a
simulated ant robot performing undirected navigation in a maze. The task is to use this undirected
dataset to solve a specific point-to-point navigation problem, traversing the maze from one corner
to the opposite corner, with only sparse 0-1 completion reward for reaching the goal. The kitchen
datasets involves a franka robot manipulating multiple objects (microwave, kettle, etc.) either in an
7
Published as a conference paper at ICLR 2021
(a) medium (CQL) (b) medium (CQL+OPAL) (c) large (CQL)
(d) large (CQL+OPAL)
Figure 3: State visitation heatmaps for antmaze medium policies learned using (1) CQL and (2)
CQL+OPAL, and antmaze large policies learned using (3) CQL and (4) CQL+OPAL.
Environment	BC	BC+OPAL (ours)	BC+SVAE
antmaze medium (diverse)	30.1 ± 3.2	81.5 ± 2.7	72.8 ± 2.3
antmaze large (diverse)	9.2 ± 2.5	63.5 ± 2.3	49.4 ± 2.2
Table 2: Average success rate (%) (over 4 seeds) of few-shot IL methods: BC, BC+OPAL, and
BC+SVAE (Wang et al., 2017).
undirected manner (mixed dataset) or in a partially task directed manner (partial dataset). The task
is to use the datasets to arrange objects in a desired configuration, with only sparse 0-1 completion
reward for every object that attains the target configuration.
Baseline: We use Behavior cloning (BC), BEAR (Kumar et al., 2019), EMAQ (Ghasemipour et al.,
2020), and CQL (Kumar et al., 2020b) as baselines. We compare it to CQL+OPAL, which first uses
OPAL to distill primitives from the offline dataset before applying CQL to learn a primitive-directing
high-level policy.
Results: As shown in Table 1, CQL+OPAL outperforms nearly all the baselines on antmaze (see
Figure 1 and Figure 3 for visualization) and kitchen tasks, with the exception of EMAQ having simi-
lar performance on kitchen mixed. To ensure fair comparison with EMAQ, we use an autoregressive
primitive policy. With the exception of EMAQ on kitchen mixed, we are not aware of any existing
offline RL algorithms that achieves similarly good performance on these tasks; moreover, we are not
aware of any existing online RL algorithms which solve these tasks (see Table 3 for some compar-
isons), highlighting the benefit of using offline datasets to circumvent exploration challenges. There
are two potential reasons for OPAL’s success. First, temporally-extended primitives could make
the reward propagation learning problem easier. Second, the primitives may provide a better latent
action space than the atomic actions of the environment. To understand the relative importance of
these factors, we experimented with an ablation of CQL+OPAL that uses c = 1 to remove temporal
abstraction. In this case, we find the method’s performance to be similar to standard CQL. This
implies that the temporal abstraction provided by OPAL is one of the main contributing factors to
its good performance. This observation also agrees with our theoretical analysis. See Appendix E
for detailed discussion.
5.2	Few-shot imitation learning with OPAL
Description: Previously, we assumed that we have access to a task reward function, but only undi-
rected data that performs other tasks. Now, we will study the opposite case, where we are not
provided with a reward function for the new task either, but instead receive a small number of task-
specific demonstrations that illustrate optimal behavior. Simply imitating these few demonstrations
is insufficient to obtain a good policy, and our experiments evaluate whether OPAL can effectively
incorporate the prior data to enable few-shot adaptation in this setting. We use the Antmaze envi-
ronments (diverse datasets) to evaluate our method and use an expert policy for these environments
to sample n = 10 successful trajectories.
Baseline and Results: For baselines, we use Behavior cloning (BC) and the model from Wang
et al. (2017), which prescribes using a sequential VAE (SVAE) over state trajectories in conjunction
with imitation learning. As shown in Table 2, BC+OPAL clearly outperforms other baselines, show-
8
Published as a conference paper at ICLR 2021
Environment	HIRO	SAC+BC	SAC+OPAL(ours)	DDQN+DDCO
antmaze medium sparse (diverse)	0.0	0.0	81.6 ± 3.7	0.0
antmaze large sparse (diverse)	0.0	0.0	00	0.0
antmaze medium dense (diverse)	0.0	0.0	81.3 ± 3.3	0.0
antmaze large dense (diverse)	12	0.0	81.5 ± 3.9	0.0	—
Table 3: Average success rate (%) (over 4 seeds) of online RL methods: HIRO (Nachum et al.,
2018a), SAC+BC, SAC+OPAL, and DDQN+DDCO (Krishnan et al., 2017). These methods were
ran for 2.5e6 steps for antmaze medium environments and 17.5e6 steps for antmaze large environ-
ments.
Models	MT10	MT50
PPO	15.2 ± 4.8	5.1 ± 2.2
PPO+OPAL(ours)	70.1 ± 4.3	45.3 ± 3.1
SAC	39.5	28.8
Table 4: Due to improved exploration, PPO+OPAL outperforms PPO and SAC on MT10 and MT50
in terms of average success rate (%) (over 4 seeds).
ing the importance of temporal abstraction and ascertaining the quality of learned primitives. See
Appendix A for detailed discussion.
5.3	Online RL and multi-task transfer with OPAL
Description: For online RL and multi-task transfer learning, we learn a task policy in space of
primitives ∏θ(a|s, Z) while keeping it fixed. For multi-task transfer, the task policy also takes in
the task id and we use c = 5 and Z = 8. Since the primitives need to transfer to a different state
distribution for multi-task transfer, it only learns the action sub-trajectory distribution and doesn’t
take in the state feedback. See Appendix A for a detailed description of models. For online RL, we
use the Antmaze environments (diverse datasets) with sparse and dense rewards for evaluating our
method. For online multi-task transfer learning, we learn primitives with expert data from pick-and-
place task and then use it to learn multi-task policy for MT10 and MT50 (from metaworld (Yu et al.,
2020)), containing 10 and 50 robotic manipulation tasks which needs to be solved simultaneously.
Baseline and Results: For online RL, we use HIRO (Nachum et al., 2018b), a state-of-the-art
hierarchical RL method, SAC (Haarnoja et al., 2018) with Behavior cloning (BC) pre-training on
D, and Discovery of Continuous Options (DDCO) (Krishnan et al., 2017) which uses D to learn a
discrete set of primitives and then learns a task policy in space of those primitives with online RL
(Double DQN (DDQN) (Van Hasselt et al., 2015)). For online multi-task transfer learning, we use
PPO (Schulman et al., 2017) and SAC (Haarnoja et al., 2018) as baselines. As shown in Table 3 and
Table 4, OPAL uses temporal abstraction to improve exploration and thus accelerate online RL and
multi-task transfer learning. See Appendix A for detailed discussion.
6	Discussion
We proposed Offline Primitives for Accelerating offline RL (OPAL) as a preproccesing step for ex-
tracting recurring primitive behaviors from undirected and unlabelled dataset of diverse behaviors.
We derived theoretical statements which describe under what conditions OPAL can improve learning
of downstream offline RL tasks and showed how these improvements manifest in practice, leading to
significant improvements in complex manipulation tasks. We further showed empirical demonstra-
tions of OPAL’s application to few-shot imitation learning, online RL, and online multi-task transfer
learning. In this work, we focused on simple auto-encoding models for representing OPAL, and an
interesting avenue for future work is scaling up this basic paradigm to image-based tasks.
7	Acknowledgements
We would like to thank Ben Eysenbach and Kamyar Ghasemipour for valuable discussions at dif-
ferent points over the course of this work. This work was supported by Google, DARPA Machine
Common Sense grant and MIT-IBM grant.
9
Published as a conference paper at ICLR 2021
References
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 22-31.
JMLR. org, 2017.
Christopher G Atkeson, Benzun P Wisely Babu, Nandan Banerjee, Dmitry Berenson, Christoper P
Bove, Xiongyi Cui, Mathew DeDonato, Ruixiang Du, Siyuan Feng, Perry Franklin, et al. No
falls, no resets: Reliable humanoid behavior in the darpa robotics challenge. In 2015 IEEE-RAS
15th International Conference on Humanoid Robots (Humanoids), pp. 623-630. IEEE, 2015.
Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Thirty-First AAAI
Conference on Artificial Intelligence, 2017.
Noam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. Science, 365(6456):
885-890, 2019. ISSN 0036-8075. doi: 10.1126/science.aay2400. URL https://science.
sciencemag.org/content/365/6456/885.
Sudeep Dasari, Frederik Ebert, Stephen Tian, Suraj Nair, Bernadette Bucher, Karl Schmeckpeper,
Siddharth Singh, Sergey Levine, and Chelsea Finn. Robonet: Large-scale multi-robot learning.
arXiv preprint arXiv:1910.11215, 2019.
Gabriel Dulac-Arnold, Daniel Mankowitz, and Todd Hester. Challenges of real-world reinforcement
learning. arXiv preprint arXiv:1904.12901, 2019.
Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need:
Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018.
J.	Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine. D4rl: Datasets for deep data-driven rein-
forcement learning. In arXiv, 2020. URL https://arxiv.org/pdf/2004.07219.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. arXiv preprint arXiv:1812.02900, 2018.
Seyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu. Emaq: Expected-
max q-learning operator for simple yet effective offline and online rl. arXiv preprint
arXiv:2007.11091, 2020.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-
policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint
arXiv:1801.01290, 2018.
K.	Hausman, J. T. Springenberg, Z. Wang, N. Heess, and M. Riedmiller. Learning an embedding
space for transferable robot skills. In International Conference on Learning Representations
(ICLR), 2018.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. 2016.
Allan Jabri, Kyle Hsu, Abhishek Gupta, Ben Eysenbach, Sergey Levine, and Chelsea Finn. Un-
supervised curricula for visual meta-reinforcement learning. In Advances in Neural Information
Processing Systems, pp. 10519-10531, 2019.
Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah
Jones, Shixiang Gu, and Rosalind Picard. Way off-policy batch deep reinforcement learning of
implicit human preferences in dialog. arXiv preprint arXiv:1907.00456, 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Sanjay Krishnan, Roy Fox, Ion Stoica, and Ken Goldberg. Ddco: Discovery of deep continuous
options for robot learning from demonstrations. arXiv preprint arXiv:1710.05421, 2017.
10
Published as a conference paper at ICLR 2021
Ashish Kumar, Saurabh Gupta, and Jitendra Malik. Learning navigation subroutines from egocentric
videos. In Conference on Robot Learning, pp. 617-626. PMLR, 2020a.
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-
learning via bootstrapping error reduction. In Neural Information Processing Systems (NeurIPS),
2019.
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline
reinforcement learning. arXiv preprint arXiv:2006.04779, 2020b.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuo-
motor policies. The Journal of Machine Learning Research, 17(1):1334-1373, 2016.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, and
Pierre Sermanet. Learning latent plans from play. In Conference on Robot Learning, pp. 1113-
1132, 2020.
Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Ofir Nachum, and Shixiang Gu. Deployment-
efficient reinforcement learning via model-based offline optimization. arXiv preprint
arXiv:2006.03647, 2020.
Josh Merel, Leonard Hasenclever, Alexandre Galashov, Arun Ahuja, Vu Pham, Greg Wayne,
Yee Whye Teh, and Nicolas Heess. Neural probabilistic motor primitives for humanoid control.
arXiv preprint arXiv:1811.11711, 2018.
Ofir Nachum, Shixiang Gu, Honglak Lee, and Sergey Levine. Near-optimal representation learning
for hierarchical reinforcement learning. arXiv preprint arXiv:1810.01257, 2018a.
Ofir Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey Levine. Data-efficient hierarchical
reinforcement learning. In Advances in Neural Information Processing Systems, pp. 3303-3313,
2018b.
Ofir Nachum, Michael Ahn, Hugo Ponte, Shixiang Gu, and Vikash Kumar. Multi-agent manipula-
tion via locomotion using hierarchical sim2real. arXiv preprint arXiv:1908.05224, 2019a.
Ofir Nachum, Haoran Tang, Xingyu Lu, Shixiang Gu, Honglak Lee, and Sergey Levine. Why does
hierarchy (sometimes) work so well in reinforcement learning? arXiv preprint arXiv:1909.10618,
2019b.
Paavo Parmas, Carl Edward Rasmussen, Jan Peters, and Kenji Doya. Pipps: Flexible model-based
policy search robust to the curse of chaos. arXiv preprint arXiv:1902.01240, 2019.
Xue Bin Peng, Michael Chang, Grace Zhang, Pieter Abbeel, and Sergey Levine. Mcp: Learn-
ing composable hierarchical control with multiplicative compositional policies. In Advances in
Neural Information Processing Systems, pp. 3686-3697, 2019.
Jan Peters, Katharina Mulling, and Yasemin Altun. Relative entropy policy search. In Twenty-Fourth
AAAI Conference on Artificial Intelligence, 2010.
Lerrel Pinto and Abhinav Gupta. Supersizing self-supervision: Learning to grasp from 50k tries and
700 robot hours. In 2016 IEEE international conference on robotics and automation (ICRA), pp.
3406-3413. IEEE, 2016.
Martin L Puterman. Markov decision processes: Discrete stochastic dynamic programming. 1994.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Tanmay Shankar and Abhinav Gupta. Learning robot skills with temporal variational inference.
arXiv preprint arXiv:2006.16232, 2020.
11
Published as a conference paper at ICLR 2021
Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics-aware
unsupervised discovery of skills. arXiv preprint arXiv:1907.01657, 2019.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484-489, 2016.
Martin Stolle and Doina Precup. Learning options in reinforcement learning. volume 2371, pp.
212-223, 08 2002. doi: 10.1007/3-540-45622-8.16.
G. Tesauro. Temporal difference learning and td-gammon. J. Int. Comput. Games Assoc., 18:88,
1995.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. arXiv preprint arXiv:1509.06461, 2015.
Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David
Silver, and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning. arXiv
preprint arXiv:1703.01161, 2017.
Ziyu Wang, Josh S Merel, Scott E Reed, Nando de Freitas, Gregory Wayne, and Nicolas Heess.
Robust imitation of diverse behaviors. In Advances in Neural Information Processing Systems,
pp. 5320-5329, 2017.
Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.
arXiv preprint arXiv:1911.11361, 2019.
Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey
Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.
In Conference on Robot Learning, pp. 1094-1100, 2020.
12
Published as a conference paper at ICLR 2021
Appendices
A Other Applications of OPAL
A. 1 Few- shot imitation learning with OPAL
Additional Assumption: In addition to D, we assume access to a small number of expert demon-
strations Dexp = {τi := (st, at)tT=-01}in=1 where n	N.
How to use with OPAL? In imitation learning, the aim is to recover an expert policy given a small
number of stochastically sampled expert demonstrations Dexp = {τi := (st, at)tT=-01}in=1. As in the
offline RL setting, We use the primitives ∏θ(a|s, Z) learned by OPAL as a low-level controller and
learn a high-level policy ∏ψ (z|s). We first partition the expert demonstrations into sub-trajectories
Dpexapr = {τi,k := (sk+t, ak+t)tc=-0 for k = 0, . . . , T - c}in=1 of length c. We then use the learned
encoder qψ(z∖τ) to label these sub-trajectories with latent actions zi,k 〜qψ(z∖τi,k) and thus create a
dataset Dhexi p = {(sik+t, zi,k) for k = 0, . . . , T - c}in=1. We use Dhexi p to learn the high-level policy
πψ(z∖s) using behavioral cloning. As in the offline RL setting, we also finetune πθ(a∖s, z) with
latent-conditioned behavioral cloning to ensure consistency of the labelled latents.
Evaluation Description: We receive a small number of task-specific demonstrations that illustrate
optimal behavior. Simply imitating these few demonstrations is insufficient to obtain a good policy,
and our experiments evaluate whether OPAL can effectively incorporate the prior data to enable few-
shot adaptation in this setting. We use the Antmaze environments (diverse datasets) to evaluate our
method and use an expert policy for these environments to sample n = 10 successful trajectories.
Baseline: We evaluate two baselines. First, we test a simple behavioral cloning (BC) baseline,
which trains using a max-likelihood loss on the expert data. In order to make the comparison fair
to OPAL (which uses the offline dataset D in addition to the expert dataset), we pretrain the BC
agent on the undirected dataset using the same max-likelihood loss. As a second baseline and to test
the quality of OPAL-extracted primitives, we experiment with an alternative unsupervised objective
from Wang et al. (2017), which prescribes using a sequential VAE (SVAE) over state trajectories in
conjunction with imitation learning.
Results: As shown in Table 2, BC+OPAL clearly outperforms other baselines, showing the impor-
tance of temporal abstraction and ascertaining the quality of learned primitives. SVAE’s slightly
worse performance suggests that decoding the state trajectory directly is more difficult than simply
predicting the actions, as OPAL does, and that this degrades downstream task learning.
A.2 Online RL with OPAL
Additional Assumptions: We assume online access to M; i.e., access to Monte Carlo samples of
episodes from M given an arbitrary policy π.
How to use with OPAL? To apply OPAL to standard online RL, we fix the learned primi-
tives πθ(a∖s, z) and learn a high-level policy πψ(z∖s) in an online fashion using the latents z
as temporally-extended actions. Specifically, when interacting the environment, πψ(z∖s) chooses
an appropriate primitive every c steps, and this primitive acts on the environment directly for c
timesteps. Any off-the-shelf online RL algorithm can be used to learn ψ. In our experiments, we use
SAC (Haarnoja et al., 2018). To ensure that πψ(z∖s) stays close to the data distribution and avoid
generalization issues associated with the fixed πθ(a∖s, z), we add an additional KL penalty to the
reward of the form DKL(πψ(z∖s)∖∖ρω(z∖s0)).
Evaluation Description: We use Antmaze medium (diverse dataset) and Antmaze large (diverse
dataset) from the D4RL task suite (Fu et al., 2020) to evaluate our method. We evaluate using both a
dense distance-based reward -kg - antxyk and a sparse success-based reward 1[kg - antxyk ≤ 0.5]
(the typical default for this task), where antxy is the 2d position of the ant in the maze.
Baseline: To solve these tasks through online RL, we need both (i) hierarchy (i.e. learning a policy
on top of primitives) which improves exploration (Nachum et al., 2019b) and (ii) unlabelled (i.e. no
task reward) offline dataset which allows us to bootstrap the primitives. This informed our choice
13
Published as a conference paper at ICLR 2021
of these three baselines. First, to test the role of D in exploration, we use HIRO (Nachum et al.,
2018b), a state-of-the-art hierarchical RL method, as a baseline. Second, to test the role of temporal
abstraction, we pre-train a flat policy on D using behavioral cloning (BC) and then finetune the pol-
icy on downstream tasks with SAC. Third, to test the quality of extracted primitives for online RL,
we extract a discrete set of primitives with Deep Discovery of Continuous Options (DDCO) (Krish-
nan et al., 2017) and use Double DQN (DDQN) (Van Hasselt et al., 2015) to learn a task policy in
the space of learned discrete primitives.
Results: As shown in Table 3, SAC+OPAL outperforms all the baselines, showing (1) the impor-
tance of D in exploration, (2) the role of temporal abstraction, and (3) the good quality of learned
primitives. Except for HIRO on Antmaze large with dense rewards, all other baselines fail to make
any progress at all. In contrast, SAC+OPAL only fails to make progress on Antmaze large with
sparse rewards.
A.3 Online Multi-task transfer learning with OPAL
Additional Assumption: We assume the existence of M additional MDPs {Mi =
(Si , A, Pi , ri , γ)}iM=1 where the action space A, and the discount factor γ are same as those of
M.
How to use it with OPAL? In the multi-task setting, we aim to learn near-optimal behavior policies
on M MDPs {Mi = (Si, A, Pi, ri, γ)}iM=1. As in the previous applications of OPAL, we learn a
set of high-level policies ∏ψ (z∣s,i) which will direct pretrained primitives ∏(a| s,z) to maximize
cumulative rewards. Since the state space in the M MDPs is potentially distinct from that in the
offline dataset D, we cannot transfer the state distribution and can only hope to transfer the action
sub-trajectory distribution. Therefore, during the unsupervised training phase for learning πθ, we
make the encoder and the decoder blind to the states in sub-trajectory. Specifically, the encoder be-
comes (μZnc,σfnc) = qφ(zt∖st, {at+i}C-1) and is represented by a bidirectional GRU. The decoder
becomes πθ({at, . . . , at+c-1}|zt) which decodes the entire action sub-trajectory from the latent
vector and is represented by a GRU. With these state-agnostic primitives in hand, we then learn a
policy πψ(z∖s, i) using any off-the-shelf online RL method. In our experiments, we use Proximal
Policy Optimization (PPO) (Schulman et al., 2017).
Evaluation Description: We use the Metaworld task suite (Yu et al., 2020) to evaluate our method.
The dataset D for learning primitives consists of trajectories generated by an expert policy for a
goal-conditioned pick-and-place task. The pick-and-place task is suitable for unsupervised primitive
learning because it contains all the basic operations (eg: move, grasp, place) required for performing
more complex manipulation tasks in the Metaworld. Once we have learned the primitives, we learn
a policy πψ(z∖s, i) for the MT10 and MT50 benchmarks, where MT10 and MT50 contain 10 and
50 robotic manipulation tasks, respectively, which we need to be solved simultaneously. In these
experiments we use c = 5 and dim(Z) = 8.
Baseline: We use SAC (Haarnoja et al., 2018) and PPO (Schulman et al., 2017) as baselines.
Results: As shown in Table 4, PPO+OPAL clearly outperforms both PPO and SAC, showing the
importance of temporal abstraction in online multi-task transfer.
B	Proof of Theorems
B.1 Bounding the suboptimality of the learned primitives
We will begin by proving the following lemma which bounds the sampling error incurred by
J(φ, θ, ω).
Lemma B.0.1. With high probability 1 - δ,
Γ CT	] S
J (θ,φ,ω)- En 〜Π,τ 〜Π,z 〜qφ(z∣τ) — Elog ∏θ (at∖st,z)	≤ ↑∣ -δ-	(11)
t=0	δ
where SJ is a constant dependent on concentration properties of πθ (a∖s, z) and qφ(z∖τ).
14
Published as a conference paper at ICLR 2021
Proof. To be concise, let us denote the sampling error in J(φ, θ, ω) by
∆J
J(θ, φ, ω) - En〜Π,τr7∏,z~qφ(z∖τ)
c-1
-Elog∏θ(at∣st,z)
t=0
(12)
Applying Chebyshev’s inequality to ∆J, we get that, with high probability 1 - δ,
A J	/Var∏ 〜∏,τ 〜π,z∙
∆J ≤
eqφ (z∣τ)(-Pc-I log πθ (at|st,Z))
(13)
δ
□
Therefore, combining all the equations, we have
En〜Π,τ^π,z^qφ (z∣τ)
c-1
-Elog∏θ(at∣st,z)
t=0
≤ J(θ, φ, ω) +
(14)
We present a general performance difference lemma that will help in our proof of Lemma 4.0.1.
Lemma B.0.2. If π1 and π2 are two policies in M, then
2
∣Jrl(∏i, M)- Jrl(∏2, M)| ≤ (1 - Y)2 RmaXEs〜d∏ι [Dtv(∏1(a∣s)∣∣∏2(a|s))].	(15)
Proof. Following the derivations in Achiam et al. (2017) and Nachum et al. (2018a), we express the
performance of a policy π in M in terms of linear opterators:
Jrl(∏, M) = (1 - Y)-1R>(I - γ∏∏ P )-1∏∏ μ,	(16)
where R is a vector representation of the rewards of M, Ππ is a linear operator mapping state
distributions to state-action distributions according to ∏, and μ is a vector representation of the
initial state distribution of M. Accordingly, we express the performance difference ofπ1, π2 as,
IJRL(∏1, M)- Jrl(∏2, M)| = ∣R>((I - Y∏1P)-1∏iμ - (I - Y∏2P)-1∏2μ)∣	(17)
≤ RmaxI (I - Y∏1P)-1∏ιμ - (I - Y∏2P)-1∏2μ∣.	(18)
By the triangle inequality, we may bound equation 18 by
RmaX(I(I - Y∏ιP)-1∏ιμ - (I - Y∏2P)-1∏ιμ∣ +
I (I - γ∏2P )-1∏1μ - (I - γ ∏2P )-1∏2μ∣). (19)
We begin by approaching the first term inside the parentheses of equation 19. That first term may be
expressed as
I (I - Y∏2P )-1(I - Y∏2P - (I - Y∏1P ))(I - Y ∏1 P )-1 ∏ιμ∣	(20)
=Iy (I - γ∏2P )-1(∏1 - ∏2)P (I - γ ∏1P )-1∏1μI
≤ ɪI(∏ι - ∏2)P(I - γ∏1P)-1∏1μI
1-Y
2Y
=(1 _ )2 Es〜(1-γ)P(I-γ∏ιP)τ∏ι”[DTV(n1(a』)kn2 (aJs))].
(21)
(22)
(23)
Now we continue to the second term inside the parentheses of equation 19. This second term may
be expressed as
I (I - γ∏2P )-1(∏1 - ∏2)μI ≤ — I(∏ι - ∏2)μI
1-Y
2
=[	Es 〜μ[DTV(π1(aIs)kπ2(aIs))].
1-Y
(24)
(25)
15
Published as a conference paper at ICLR 2021
To combine equations 23 and 25, we note that
dπ1 = Y ∙ (1 一 Y)P(I 一 γ∏ιP)-1∏ιμ +(1 - Y) ∙ μ.	(26)
Thus, we have
2
(1-Y2
|JRL(π1, M)- JRL(π2, M)I ≤
RmaxEs〜d∏ι [Dtv(∏1(a∣s)k∏2(a∣s))],
(27)
as desired.
□
Lemma 4.0.1. If π1 and π2 are two policies in M, then
2
lJRL(π1, M)- JRL(π2, M)I ≤ ∩---------Cvl------VRmaxEs〜d* [DTV(n1(TIs) ||n2(TIs))],	(28)
(I- YC)(I- Y)	C
where DTV (π1 (T Is)IIπ2 (T Is)) denotes the TV divergence over c-length sub-trajectories T sampled
from π1 vs. π2 (see section 3). Furthermore,
2
SUbOpt(θ) ≤ (1 - YC)(I- Y)RmaxEs〜d∏* [Dtv(π (T∣s)∣∣∏θ,ψ*(τ|s))].	(29)
Proof. We focus on proving equation 28, as the subsequent derivation of equation 29 is straightfor-
ward by definition of SUbOpt.
To derive equation 28, we may simply consider π1 and π2 acting in an “every-c-steps” version of
M, where the action space is now T and reward are accumulated over c steps using Y-discounting.
Note that in this abstracted version of M, the max reward is I-YrRmax and the MDP discount is
Yc. Plugging this into the result of Lemma B.0.2 immediately yields the desired claim.	□
Theorem 4.1.	Let θ, φ, ω be the outputs of solving equation 1, such that J(θ, φ, ω) = C. Then, with
high probability 1 一 δ, for any π that is Z-Common in Π, there exists a distribution H over Z such
that for πH (τ |s) := Ez 〜H [∏θ (τ ∣z, s)],
Es〜κ[Dτv(∏(τ∣s)∣∣∏H(τ∣s))] ≤ Z +
(30)
∖
where HC = En 〜∏,s 〜κ,τ 〜∏ [PC-1 log ∏(at∣st)] (i.e. a constant and property of D) and SJ isa PoS-
itive constant incurred due to sampling error in J(θ, φ, ω) and depends on concentration properties
ofπθ(aIs, z) and qφ(zIT).
Proof. We start with application of the triangle inequality:
DTV(π(τ∣s)∣∣∏H(τIs)) ≤ DTV(π(τ∣s)∣∣∏(τ∣s)) + DTV(∏(τ∣s)∣∣∏H(T|s)).	(31)
Taking expectation with respect to ∏ 〜Π, s 〜K on both the sides, We get
Es〜κ[Dτv(π(τ)II∏H(τ))] ≤ En〜Π,s〜κ[Dτv(∏(τ)II∏(τ))] + En〜∏,s〜μ[Dτv(∏(τ)II∏H(T))]
≤ En〜Π,s〜k[Dtv(∏(t)II∏(t))]
+ En 〜Π,s 〜”[j1DKL (n(T )IInH (T ))]
≤ Z + 4 2 En 〜Π
≤ Z + JlEn〜Π
,τ〜π,s 〜κ[lθg ∏(τ ) - log Ez 〜H [∏θ (τ Iz)]]
,s-κ,τ ^n,z^H [log ∏(τ ) - log ∏θ (τ Iz)].
(32)
(33)
(34)
(35)
(36)
16
Published as a conference paper at ICLR 2021
The last two inequality used Jensen's inequality. Let H(Z) = En〜∏,τ〜∏[qφ(z∣τ)]. Cancelling out
the dynamics and using equation 14 we get,
Es 〜K [Dτv(∏(τ )∣∣∏H (T))]
□
Corollary 4.1.1. Ifthe optimal policy ∏ of M is Z-Common in Π ,and ∣∣ dc^ ∣∣	≤ ξ, then, with
high probability 1 - δ,
SUbOPt⑻ ≤(1 - γ2)ξι - Y) Rmax (Z+t 2 (C+rSJ+H) j
Proof. Expanding lemma 4.0.1 using the above assumption, we have
SubOPt(θ) ≤ |Jrl(π*, M)- Jrl(∏H, M)|
2
≤ 门 _~cw-∣ __7RmaXEs〜厅 [Dτv(π*(τ ∣s)∣∣∏H (τ |s))]
(I- YC)(I- Y)	C
*
≤ ∩-C5∩-TRmaX d-	Es~κ [DTV(∏*(T ∣s)∣∣∏H (T |s))]
(1 - YC)(1 - Y)	∣ κ ∣∞	θ
≤ ∩——2ξ-——-RmaXEs 〜κ [DTV (π*(τ |s)||nH (T |s))]
(1 - YC)(1 - Y)	θ
Now, we can use theorem 4.1 to prove the corollary.
(37)
(38)
(39)
(40)
(41)
□
B.2 Performance bounds for OPAL
Theorem 4.2.	Let πψ* (z|s) be the policy obtained by CQL and let πψ*,θ(a|s) refer to the policy
when πψ* (z|s) is used together with ∏θ (a|s, Z). Let ∏β ≡ {π ； π 〜∏} referto the policy generating
DTinMDP M andZ 〜∏H(z|s) ≡ T 〜∏β,so=s, z 〜qφ(z∣τ). Then, J(πψ*,θ, M) ≥ J(∏β, M)-κ
with high probability 1 - δ where
K = O((I-YC)(I-Y)Es〜⅛ς,θ(s) hq∣z∣(DCQL(πψ*,∏H)(s) + 1)i)	(42)
-ταcEsy* (s) [DCQL(πΨ*,θ,πH)(s)]	(43)
1 - Y MH
Proof. We assume that the variational posterior qφ(z∣τ) obtained after learning OPAL from D
is same (or nearly same) as the true posterior p(z∣τ). qφ can be used to define ∏H(z|s) as
z 〜∏H ≡ τ 〜∏β, z 〜qφ(z∣τ). This induces an MDP MH = (S, Z, Pz,瞑,YC) where Z is the
inferred latent space for choosing primitives, Pz and rz are the latent dynamics and reward function
Suchthat st+C ^ Pz(st+C|st, zt) ≡ st+i+1 〜P(St+i+ι 际+” at+i), at+i 〜n(at+i|st+i, Zt) ∀i ∈
{0, 1, . . . , c - 1} and rz(st, zt) = PiC=-01 Yir(st+i, at+i), and YC is the new discount factor ef-
fectively reducing the task horizon by a factor of c. π(a∣s, z) is the primitive induced by q$ and
∏β. Since q@ captures the true posterior, π(a∣s, z) is the optimal primitive you can learn and its
17
Published as a conference paper at ICLR 2021
autoencoding loss, under true expectation, is e* = En〜∏,τ^∏,z^qφ(z∖τ) [- Pc-I log∏(atIst, z)].
Therefore, T 〜∏β ≡ Z 〜∏H, T 〜∏(“∙, z). ∏β is used to collect the data Dr which induces an
ʌ ʌ
empirical MDP. We refer to the empirical MDP induced by Dr as M = (S, A, P, μ, r, Y) where
P(svIS,^) = P(S,£：： 1[1==a=⅛=s,] and μ(S)=二2囚二=闵.We use qφ to get Dri from
Dr which induces another empirical MDP MH. Using these definitions, We will try to bound
IJ(∏ψ*,θ, M) - J(∏β, M)I.
Let,s break IJ(∏ψ*,θ, M) - J(∏β, M)I into
IJ (πψ*,θ, M)- J (πβ, M)I ≤ IJ (πψ*,θ, M)- J (πψ* , MH ) |
+ IJ (πψ* , MH - J (πβ , MH)I
+ IJ(πβH,MH) -J(πβ,M)I
(44)
(45)
(46)
Since q@ captures the true variational posterior, T 〜∏β ≡ Z 〜∏βH,τ 〜∏(∙I∙, Z) and therefore,
IJ (πβH, MH) - J(πβ, M)I = 0. For bounding, IJ (πψ* , MH - J(πβH, MH)I, we use theorem 3.6
from Kumar et al. (2020b) and apply it to MH to get
IJ(πψ*,MH)-J(πβH,MH)I	(47)
Cr,δ	γcRmaxCP,δ	IZI	H
≤2(l+(i-γc)(i-γ))Es-dχ,θ(S)WEDCQLm*,πβ)(s)+ι]	(48)
-I -Yc EsfH (s) [DCQL (πψ*,θ, πH )(s)] = κ2	(49)
Now, we will try to bound IJ(πψ*,θ, M) - J (πψ* , MH)I. The only difference between the two is
that the primitive πθ(aIs, Z) is used in M and the primitive π(aIs, Z) is used in MH. Therefore,
We can write the above bound as IJ(∏ψ*,θ, M) - J(∏ψ*,∏(∙∣∙,z), M)I. Let's first bound their value
function at a particular state s. Using Lemma 4.0.1, we get
i J(πψ*,θ, M)- J(πψ*,n(∙∖∙,z), M) i
≤ (1 -2Rm1x- Y)Es〜dπΨ*,∏(∙ι∙,z) [Dτv(πψ*,∏(∙∣∙,Z)(TIs)IIn3*,e(TIs))]
2Rmax
一(i - γc)(i - γ) s~dCψ*,π(,"Z)
JIDKL (πψ*,∏(∙∣∙,z)(T is)Hπψ*,θ (T IS))
Now, we will try to bound DKL(∏ψ*,π(∙∣∙,z)(tIs)II∏ψ*,θ(TIs)). We have
:〜Πψ* (z∣s),τ〜π(τ|s,z)
log πψ*(ZIS) Qc-I P(StISt-1, at-1)Qc-I n(atIst, Z)
πψ* (ZIs) Qtc=-11 P(stIst-1, at-1) Qtc=-01 πθ(atIst, Z)
c-1
EZ〜nψ* (z| s),τ〜π(τ| s,z)〉: log π (at i st, z) - log πθ (at i st, Z)
t=0
πHMS)) Xlog∏(at∣st,Z) - log∏Θ(at∣st,Z)
πβ (ZIs)	t=0
≤
∏ψ* (z∣s)
πH (ZIS)
c-1
log π(at Ist, Z) - log πθ(atIst, Z)
t=0
≤ I πψ* (ZIS)
_ ∣πH(ZIS)
∞
EZ 〜∏β (z∣s),τ 〜π(τ | s,z)
∞
(50)
(51)
(52)
(53)
(54)
(55)
(56)
(57)
18
Published as a conference paper at ICLR 2021
The last equation comes from above definition of e* and equation 14. We will now try to bound
∏ψ* (z|s) I
πH (ZIs) l∞
using Dcql(∏ψ* , ∏H)(s). Using definition of Dcql(∏ψ* , ∏H)(s), We have
DCQL(πψ*, πβH)(S) =	πψ* (zIS)
z
⇒ DCQL (πψ* ,πH )(s) + 1 = EnH (HS)
z
|s)-1
|s) -1
|s))2≤∣π⅛" ∏H(中)
IS) /	InH (HS)L
where W = arg max.
(πψ* (ZIs) ʌ To be concise let
πH (z|s) . To be concise, let
∆
(58)
(59)
(60)
Combining above equations, we have
i
DKL(πψ* (zIS)π(τ IS, z)IIπψ* (zIS)πθ(τIS,z)) ≤ ∆c
DCQL(πψ* ,πH)(s) + 1
(61)
Using this to bound the returns, we get
IJ (πψ*,θ, M)- J (πψ*,∏(∙∣∙,z), M)I
“ 2Rmax	DCQL (πΨ* ,πH )(s) + l∖4 PTT
-(1 - γc)(1 - γ) S〜dcψ ,π(∙"z) [I	∏β(z∣s)	2 V 2 C
= κ1
(62)
(63)
(64)
We get κ = κ1 + κ2. We apply O to get the notation in the theorem.
□
C Experiment Details
C.1 OPAL experiment details
Encoder The encoder qφ(zIτ) takes in state-action trajectory τ of length c. It first passes the indi-
vidual states through a fully connected network with 2 hidden layers of size H and ReLU activation.
Then it concatenates the proccessed states with actions and passes it through a bidirectional GRU
with hidden dimension of H and 4 GRU layers. It projects the output of GRU to mean and log
standard deviation of the latent vector through linear layers.
Prior The prior ρω(zIS) takes in the current state S and passes it through a fully connected network
with 2 hidden layers of size H and ReLU activation. It then projects the output of the hidden layers
to mean and log standard deviation of the latent vector through linear layers.
Primitive Policy The primitive (i.e. decoder) πθ(aIS,z) has same architecture as the Prior but it
takes in state and latent vector and produces mean and log standard deviation of the action. For
kitchen environments, we use an autoregressive primitive policy with same architecture as used by
EMAQ (Ghasemipour et al., 2020).
We use H = 200 for antmaze environments and H = 256 for kitchen environments. In both cases,
OPAL was trained for 100 epochs with a fixed learning rate of 1e - 3, β = 0.1 (Lynch et al., 2020),
Adam optimizer (Kingma & Ba, 2014) and a batch size of 50.
C.2 Task Policy architecture
In all environments, for task policy, we used a fully connected network with 3 hidden layers of size
256 and ReLU activation. It then projects the output of the hidden layers to mean and log standard
deviation of the latent vector through linear layers.
19
Published as a conference paper at ICLR 2021
Environment	dim(Z) = 4	dim(Z) = 8	dim(Z) = 16
antmaze medium (diverse)	68.7 ± 2.3 ∙	81.1 ± 3.1	81.3 ± 1.8 一
Table 5: Average success rate (%) (over 4 seeds) of CQL+OPAL for different values of dim(Z). We
fix c = 10.
C.3 SAC Hyperparameters
We used SAC (Haarnoja et al., 2018) for online RL experiments in learning a task policy either
in action space A or latent space Z. For the discrete primitives extracted from DDCO (Krishnan
et al., 2017), we used Double DQN Van Hasselt et al. (2015). We used the standard hyperparameters
for SAC and Double DQN as provided in rlkit code base (https://github.com/vitchyr/
rlkit) with both policy learning rate and q value learning rate as 3e - 4.
C.4 CQL Hyperparameters
We used CQL (Kumar et al., 2020b) for offline RL experiments in learning a task policy either in
action space A or latent space Z. We used the standard hyperparameters, as mentioned in Kumar
et al. (2020b)., with minor differences. We used policy learning rate of 3e - 5, q value learning rate
of 3e - 4, and primitive learning rate of 3e - 4. For antmaze tasks, we used CQL(H) variant with
τ = 5 and learned α. For kitchen tasks we used CQL(ρ) variant with fixed α = 10. In both cases,
we ensured α never dropped below 0.001.
D Connection between OPAL and VAE objectives
We are given an undirected, unlabelled and diverse dataset D of sub-trajectories of length c. We
would like to fit a sequential VAE model to D which maximizes
max ET 〜D [log Pθ (T | s 0)]	(65)
θ
where sO is initial state of the sub-trajectory. Let’s consider
logpθ(TIsO) = log	pθ(T, zIsO)dz = log
Pθ(T,z|sO)qφ(ZIT)
qφ(Hτ)
dz
(66)
(using Jensen’s inequality)
≥ / qφ(z∣τ)[logPθ(τ,z∣so) - log qφ(z∣τ)]dz = Ez〜勺力3二) logPθ(T∣z,so) - log
qφ(Hτ)
pθ(zIsO)
(67)
Using the above equation, we have the following lower-bound for our objective function
max ET 〜D [log Pθ (T ISO)] ≥ max ET ^D ,z^qφ(z∣τ)
logpθ(τ|z, so) - log
qφ(Hτ)
Pθ(z∣so)一
(68)
maxET〜D,z〜qφ(z∣τ)[logPθ(τIz,so)] - DκL(qφ(z∣τ)∣∣PΘ(z∣so))
θ,φ
(69)
We separate the parameters of decoder from prior and hence write pθ(z∣so) = ρω(z∣so). We can
expand logpθ(TIz, sO) = Ptc=-11 logP(stIst-1, at-1) + Ptc=-O1 logπθ(atIst, z). Since P is fixed it
can be removed from the objective function. Therefore, we can write the objective function as
max ET〜D,z〜qφ(z∣τ)
θ,φ
c-1
Elog ∏θ (at∣st,z)
t=0
-βDKL(qφ(Hτ)llPω (z|s0))
(70)
where β = 1. This is similar to the autoencoding loss function we described in section 4.
E Ablation Studies
As shown in Table 5, we experimented with different choices of dim(Z) on antmaze-medium
(diverse). Using the hyperparameters from Nachum et al. (2018a), we fixed c = 10. While
20
Published as a conference paper at ICLR 2021
dim(Z) = 8, 16 gave similar performances, dim(Z) = 4 performed slightly worse. Therefore,
we selected dim(Z) = 8 for our final model as it was simpler.
Temporal abstraction actually helps: To empirically verify that the gain in performance was due
to temporal abstraction and not better action space learned through latent space, we tried c = 1
(dim(Z) = 8) and found the performance to be similar to that of CQL (i.e. 55.3 ± 3.8) thereby
empirically supporting the theoretical benefits of temporal abstraction.
We found dim(Z) = 8 and c = 10 to work well with other environments as well. However, we
acknowledge that the performance of CQL+OPAL can be further improved by carefully choosing
better hyperparameters for each environment or by using other offline hyperparameter selection
methods for offline RL, which is a subject of future work.
F Alternative methods for extracting primitives from offline
DATA
We describe alternative methods for extracting a primitive policy from offline data. These methods
are offline variants of CARML (Jabri et al., 2019) and DADS (Sharma et al., 2019). We tried these
techniques in an early phase of our project and used the environment antmaze-medium (diverse) to
evaluate these methods.
Let’s consider an offline undirected, unlabelled and diverse dataset D = {(sit,ati)tc=-01}iN=1. Let
τ = (st)tc=-01 represent the state trajectory. To extract primitives, we first cluster the trajectories by
maximizing the mutual information between the state trajectory τ and latent variable z (indicating
cluster index) with respect to the parameters of joint distribution pψ,ω (τ, Z) = pω (z)pφ(τ|z). For
now, weconsiderpω(z) = Cat(p1, . . . ,pk) (i.e. discrete latent variables sampled from a Categorical
distribution) and represent z as one-hot vector of dimension k. The choice of pω (z) is consistent
with the choices made in Jabri et al. (2019) and Sharma et al. (2019). Since z is discrete, we can use
Bayes rule to calculatepψ,ω(z∣τ) as
Pφ,ω (ZIT)
Pω (Z)Pφ(τ Iz)
Pk=I Pω (Zi)Pφ(τ Izi)
(71)
Our objective function becomes
max I(τ; z) = max ET〜D,z〜石力川(z∣τ)
φ,ω	φ,ω	,
log pφ^:
(72)
Offline CARML and offline DADS differ only in how they modelpφ(τIZ):
•	Offline CARML We model pφ(τIZ) = Qtc=-01 pφ(stIZ) and hence, logpφ(τIZ) =
Ptc=-01 logpφ(stIZ).
•	Offline DADS We modelpφ(τIZ) = p(s0) Qtc=-11 pφ(stIst-1, Z) and hence, logpφ(τIZ) =
logp(s0) + Ptc=-01 logpφ(stIst-1, Z). Here, we only modelpφ(stIst-1, Z) and not p(s0).
Since, log is additive in nature, p(s0) will be ignored while calculating gradient.
To optimize equation 72, we use Algorithm 2 from Jabri et al. (2019). Once we have clustered the
state trajectories τ with labels Z by maximizing I(τ; Z), we can use behavioral cloning (BC) to learn
πθ(aIs, Z).
Finally, we use pφ,ω(ZIτ) to label the reward-labelled data Dr = {(sit, ait, rti)tc=-01}iN=1 with latents,
and transform it into Dhri = {(si0, Zi, Ptc=-01 γirti, sc)iN=1}. The task policy πψ is trained on Dhri using
Conservative Q Learning (CQL) (Kumar et al., 2020b). Since the primitive policy πθ is trained after
pφ,ω(ZIτ) is fully trained, it doesn’t need any additional finetuning.
F.1 Results
Using the hyperparameters from Nachum et al. (2018a), we used c = 10. We experimented with
different values of k = 5, 10, 20 and found that k = 10, 20 works the best (see Table 6 for more
21
Published as a conference paper at ICLR 2021
Models	k = 5	k = 10	k = 20
CQL+Ofline DADS	31.4 ± 5.7	59.1 ± 3.1	59.6 ± 2.9
CQL+Offline CARML	13.3 ± 4.7	15.1 ± 2.6	14.9 ± 3.8
Table 6: Average success rate on antmaze medium (diverse) (%) (over 4 seeds) of CQL combined
with offline DADS and offline CARML for different values of k.
Environment	CQL	CQL+OPAL	CQL+ DADS	CQL+CARML
success rate	53.7 ± 6.1	81.1 ± 3.1	59.1 ± 3.1	15.1 ± 2.6
cumulative dense reward	-12138.6 ± 720.3	-7795.7 ± 535.4	-11184.7 ± 610.1	-13387.3 ± 710
cumulative dense reward (last 5 steps)	-45.1 ± 9.2	-7.8 ± 4.6	-33.1 ± 8.1	-51.6 ± 5.6
Table 7: Average success rate (%), cumulative dense reward, and cumulative dense reward (last 5
steps) (over 4 seeds) of CQL combined with different offline skill discovery methods on antmaze
medium (diverse). For CQL + (Offline) DADS and CQL + (Offline) CARML, we use k = 10. Note
that CQL+OPAL outperforms both other methods for unsupervised skill discovery on all of these
different evaluation metrics.
details). We went with k = 10 as our final model since it’s simpler. Offline CARML effectively
uses only 6 skills as the other 4 skills had pω (z) = 0. Offline DADS uses all the skills. The results
are described in Table 7. In addition to calculating the average success rate, we also calculate the
average cumulative dense rewards for entire trajectory and the average cumulative dense rewards
for the last 5 time steps. Here, the dense reward is negative l2 distance to the goal. The resulting
trajectory clusters (using a subset of the dataset) from discrete skills are also visualized in Figure 4
where different colors represent different clusters.
Since offline CARML treats the states in the trajectory conditionally independent of each other
given z, the clustering mainly focuses on the spatial location. Therefore, offline CARML isn’t able
to separate out different control modes starting around the same spatial locations which explains its
poor performance when combined with CQL. As we can see from Figure 5, offline CARML is able
to make progress towards the goal, but gets stuck along the way due to poor separation of control
modes. On the other hand, offline DADS treats the state transitions in the trajectory conditionally in-
dependent of each other given z and thus clusters trajectories with similar state transitions together.
This allows it to more effectively separate out the control modes. Therefore, CQL+offline DADS
slightly improves upon CQL but is still limited by discrete number of skills. Furthermore, increasing
the number of skills from 10 to 20 gives similar performance. Moreover, in these methods, it’s in-
tractable to use continuous skill space since We use Bayes rule to calculate pφ,ω (Z ∣τ). Therefore, We
decided to switch to learning a β-VAE (Higgins et al., 2016) style generative model with continuous
skill space i.e. OPAL.
(a) Offline DADS	(b) Offline CARML
Figure 4: Visualization of(a subset of) dataset trajectories colored according to their assigned cluster
using (a) Offline DADS and (b) Offline CARML. We use k = 10.
22
Published as a conference paper at ICLR 2021
(a) CQL	(b) CQL+OPAL (c) CQL+Offline DADS (d) CQL+Offline CARML
Figure 5: State visitation heatmaps for antmaze medium policies learned using (1) CQL, (2)
CQL+OPAL, (2) CQL+Offline DADS and (4) CQL+Offline CARML. Note that while offline
CARML and offline DADS get stuck at various corners of the maze, OPAL is able to find its path
through the maze to the goal location on the top right.
F.2 Training Details
For clustering by optimizing equation 72, both offline CARML and offline DADS only considers
the global x-y pose of the ant and ignores other dimensions of the state space. These methods fail to
work when considering the full state space.
Offline CARML pφ(s∣z) takes in the latent one-hot vector Z and passes it through a fully connected
network with 2 hidden layers of size H = 200 and ReLU activation. It then projects the output of
the hidden layers to mean and log standard deviation of the reduced state s (only global x-y pose
considered) through linear layers.
Offline DADSpφ(st∣st-ι, Z) has the same architecture aspφ(s∣z) but also takes in the reduced state
from the previous timestep.
Primitive Policy The primitive policy ∏θ(a|s, Z) takes in the current state S and latent one-hot vector
Z and passes it through a fully connected network with 2 hidden layers of size H = 200 and ReLU
activation. It then projects the output of the hidden layers to mean and log standard deviation of
action through linear layers.
We perform the clustering for 25 epochs with a fixed learning rate of 1e - 3, Adam opti-
mizer (Kingma & Ba, 2014) and a batch size of 50 using the Algorithm 2 from Jabri et al. (2019).
Task Policy For task policy πψ(s), we used a fully connected network with 3 hidden layers of size
256 and ReLU activation. It then projects the output of the hidden layers to the logits (corresponding
to the components of discrete latent space) through linear layers.
CQL Hyperparameters We used the standard hyperparameters for CQL(H) with discrete action
space, as mentioned in Kumar et al. (2020b).
23