Published as a conference paper at ICLR 2021
Fantastic Four: Differentiable Bounds on
Singular Values of Convolution Layers
Sahil Singla & Soheil Feizi
Department of Computer Science
University of Maryland
College Park, MD 20740, USA
{ssingla,sfeizi}@umd.edu
Ab stract
In deep neural networks, the spectral norm of the Jacobian of a layer bounds the
factor by which the norm of a signal changes during forward/backward propagation.
Spectral norm regularizations have been shown to improve generalization, robust-
ness and optimization of deep learning methods. Existing methods to compute the
spectral norm of convolution layers either rely on heuristics that are efficient in
computation but lack guarantees or are theoretically-sound but computationally
expensive. In this work, we obtain the best of both worlds by deriving four provable
upper bounds on the spectral norm of a standard 2D multi-channel convolution
layer. These bounds are differentiable and can be computed efficiently during
training with negligible overhead. One of these bounds is in fact the popular heuris-
tic method of Miyato et al. (2018) (multiplied by a constant factor depending on
filter sizes). Each of these four bounds can achieve the tightest gap depending on
convolution filters. Thus, we propose to use the minimum of these four bounds as a
tight, differentiable and efficient upper bound on the spectral norm of convolution
layers. We show that our spectral bound is an effective regularizer and can be
used to bound either the lipschitz constant or curvature values (eigenvalues of the
Hessian) of neural networks. Through experiments on MNIST and CIFAR-10, we
demonstrate the effectiveness of our spectral bound in improving generalization
and provable robustness of deep networks.
1	Introduction
Bounding singular values of different layers of a neural network is a way to control the complexity of
the model and has been used in different problems including robustness, generalization, optimization,
generative modeling, etc. In particular, the spectral norm (the maximum singular value) of a layer
bounds the factor by which the norm of the signal increases or decreases during both forward and
backward propagation within that layer. If all singular values are all close to one, then the gradients
neither explode nor vanish (Hochreiter, 1991; Hochreiter et al., 2001; Klambauer et al., 2017; Xiao
et al., 2018). Spectral norm regularizations/bounds have been used in improving the generalization
(Bartlett et al., 2017; Long & Sedghi, 2020), in training deep generative models (Arjovsky et al.,
2017; Gulrajani et al., 2017; Tolstikhin et al., 2018; Miyato et al., 2018; Hoogeboom et al., 2020) and
in robustifying models against adversarial attacks (Singla & Feizi, 2020; Szegedy et al., 2014; Peck
et al., 2017; Zhang et al., 2018; Anil et al., 2018; Hein & Andriushchenko, 2017; Cisse et al., 2017).
These applications have resulted in multiple works to regularize neural networks by penalizing the
spectral norm of the network layers (Drucker & Le Cun, 1992; Yoshida & Miyato, 2017; Miyato
et al., 2018; 2017; Sedghi et al., 2019; Singla & Feizi, 2020).
For a fully connected layer with weights W and bias b, the lipschitz constant is given by the spectral
norm of the weight matrix i.e, ∣∣ W ∣∣ 2, which can be computed efficiently using the power iteration
method (Golub & Van Loan, 1996). In particular, if the matrix W is of size p × q, the computational
complexity of power iteration (assuming convergence in constant number of steps) is O(pq).
Convolution layers (Lecun et al., 1998) are one of the key components of modern neural networks,
particularly in computer vision (Krizhevsky et al., 2012). Consider a convolution filter L of size
1
Published as a conference paper at ICLR 2021
cout × cin × h × w where cout , cin , h and w denote the number of output channels, input channels,
height and width of the filter respectively; and a square input sample of size cin × n × n where n is its
height and width. A naive representation of the Jacobian of this layer will result in a matrix of size
n2cout × n2cin. For a typical convolution layer with the filter size 64 × 3 × 7 × 7 and an ImageNet
sized input 3 × 224 × 224 (Krizhevsky et al., 2012), the corresponding jacobian matrix has a very
large size: 802816 × 150528. This makes an explicit computation of the jacobian infeasible. Ryu
et al. (2019) provide a way to compute the spectral norm of convolution layers using convolution
and transposed convolution operations in power iteration, thereby avoiding this explicit computation.
This leads to an improved running time especially when the number of input/output channels is small
(Table 1).
However, in addition to the running time, there is an additional difficulty in the approach proposed in
Ryu et al. (2019) (and other existing approaches described later) regarding the computation of the
spectral norm gradient (often used as a regularization during the training). The gradient of the largest
singular value with respect to the jacobian can be naively computed by taking the outer product of
corresponding singular vectors. However, due to the special structure of the convolution operation,
the jacobian will be a sparse matrix with repeated elements (see Appendix Section D for details). The
naive computation of the gradient will result in non-zero gradient values at elements that should be in
fact zeros throughout training and also will assign different gradient values at elements that should
always be identical. These issues make the gradient computation of the spectral norm with respect to
the convolution filter weights using the technique of Ryu et al. (2019) difficult.
Recently, Sedghi et al. (2019) provided a principled approach for exactly computing the singular
values of convolution layers. They construct n2 matrices each of size cout × cin by taking the Fourier
transform of the convolution filter (details in Appendix Section B). The set of singular values of the
jacobian equals the union of singular values of these n2 matrices. However, this method can have
high computational complexity since it requires SVD of n2 matrices. Although this method can be
adapted to compute the spectral norm of n2 matrices using power iteration (in parallel with a GPU
implementation) instead of full SVD, the intrinsic computational complexity (discussed in Table 2)
can make it difficult to use this approach for very deep networks and large input sizes especially when
computational resources are limited. Moreover, computing the gradient of the spectral norm using
this method is not straightforward since each of these n2 matrices contain complex numbers. Thus,
Sedghi et al. (2019) suggests to clip the singular values if they are above a certain threshold to bound
the spectral norm of the layer. In order to reduce the training overhead, they clip the singular values
only after every 100 iterations. The resulting method reduces the training overhead but is still costly
for large input sizes and very deep networks. We report the running time of this method in Table 1
and its training time for one epoch (using 1 GPU implementation) in Table 4c.
Because of the aforementioned issues, efficient methods to control the spectral norm of convolution
layers have resorted to heuristics (Yoshida & Miyato, 2017; Miyato et al., 2018; Gouk et al., 2018).
Typically, these methods reshape the convolution filter of dimensions cout × cin × h × w to construct
a matrix of dimensions cout × hwcin , and use the spectral norm of this matrix as an estimate of the
spectral norm of the convolution layer. To regularize during training, they use the outer product of
the corresponding singular vectors as the gradient of the largest singular value with respect to the
reshaped matrix. Since the weights do not change significantly during each training step, they use
only one iteration of power method during each step to update the singular values and vectors (using
the singular vectors computed in the previous step). These methods result in negligible overhead
during the training. However, due to lack of theoretical justifications (which we resolve in this work),
they are not guaranteed to work for all different shapes and weights of the convolution filter. Previous
studies have observed under estimation of the spectral norm using these heuristics (Jiang et al., 2019).
On one hand, there are computationally efficient but heuristic ways of computing and bounding
the spectral norm of convolutional layers (Miyato et al., 2017; 2018). On the other hand, the exact
computation of the spectral norm of convolutional layers proposed by Sedghi et al. (2019); Ryu
et al. (2019) can be expensive for commonly used architectures especially with large inputs such
as ImageNet samples. Moreover, the difficulty in computing the gradient of the spectral norm with
respect to the jacobian under these methods make their use as regularization during the training
process challenging.
In this paper, we resolve these issues by deriving a differentiable and efficient upper bound on the
spectral norm of convolutional layers. Our bound is provable and not based on heuristics. Our
2
Published as a conference paper at ICLR 2021
Filter shape	Spectral norm bounds						Running time (secs)		
	hhw ×				Bound (Ours)	Exact (Sedghi /Ryu)	Ours	Sed- ghi	Ryu
	I∣R∣∣2	l∣s∣∣2	l∣τ∣∣2	l∣u∣∣2					
64 × 3 × 7 × 7	47.29	46.37	28.89	77.31	28.89	15.92	0.004	0.94	0.03
64 × 64 × 3 × 3	9.54	9.61	9.33	10.51	9.33	6.01	0.033	8.46	0.04
64 × 64 × 3 × 3	6.57	6.30	6.49	7.86	6.30	5.34	0.033	9.19	0.04
64 × 64 × 3 × 3	8.92	8.71	8.80	10.97	8.71	7.00	0.033	9.16	0.04
64 × 64 × 3 × 3	5.40	5.52	5.81	6.30	5.40	3.82	0.033	9.25	0.03
128 × 64 × 3 × 3	6.99	6.92	6.01	9.12	6.01	4.71	0.033	3.08	0.05
128 × 128 × 3 × 3	7.45	7.38	7.21	8.51	7.21	5.72	0.033	8.98	0.08
128 × 128 × 3 × 3	6.78	7.18	6.85	8.78	6.78	4.41	0.032	9.02	0.08
128 × 128 × 3 × 3	7.57	7.72	8.89	7.87	7.57	4.88	0.033	9.00	0.08
256 × 128 × 3 × 3	8.58	8.45	8.67	8.77	8.45	7.39	0.032	3.92	0.14
256 × 256 × 3 × 3	8.19	8.07	8.04	8.99	8.04	6.58	0.033	13.1	0.26
256 × 256 × 3 × 3	8.06	8.13	7.57	9.26	7.57	6.36	0.034	11.2	0.26
256 × 256 × 3 × 3	9.91	9.76	10.97	9.17	9.17	7.68	0.033	11.2	0.26
512 × 256 × 3 × 3	11.24	11.00	11.50	11.07	11.00	9.99	0.034	5.26	0.51
512 × 512 × 3 × 3	10.98	10.65	11.91	10.45	10.45	9.09	0.033	15.7	1.04
512 × 512 × 3 × 3	20.22	19.90	21.91	18.37	18.37	17.60	0.033	15.7	1.04
512 × 512 × 3 × 3	7.83	7.99	8.26	7.60	7.60	7.48	0.034	15.8	1.04
Table 1: Comparison between the exact spectral norm of the jacobian of convolution layers (computed
using Sedghi et al. (2019); Ryu et al. (2019)) and our proposed bound for the a Resnet-18 network
pre-trained on ImageNet. Our bound is within 1.5 times the exact spectral norm (except the first layer)
while being significantly faster to compute compared to the exact methods. All running times were
computed on GPU using tensorflow. For Sedghi et al. (2019)’s method, to have a fair comparison,
we only compute the largest singular value (and not all singular values) for all individual matrices in
parallel using power iteration (on 1 GPU) and take the maximum. We observe that for different filters,
different components of our bound give the minimum value. Also, the values of the four bounds can
be very different for different convolution filters (example: filter in the first layer).
computational complexity is similar to that of heuristics (Miyato et al., 2017; 2018) allowing our
bound to be used as a regularizer for efficiently training deep convolutional networks. In this way, our
proposed approach combines the benefits of the speed of the heuristics and the theoretical rigor of
Sedghi et al. (2019). Table 2 summarizes the differences between previous works and our approach.
In Table 1, we empirically observe that our bound can be computed in a time significantly faster than
Sedghi et al. (2019); Ryu et al. (2019), while providing a guaranteed upper bound on the spectral
norm. Moreover, we empirically observe that our upper bound and the exact value are close to each
other (Section 3.1).
Below, we briefly explain our main result. Consider a convolution filter L of dimensions cout × cin ×
h × w and input of size cin × n × n. The corresponding jacobian matrix J is of size n2cout × n2cin.
We show that the largest singular value of thejacobian (i.e. J∣∣2) is bounded as:
J∣∣2 ≤ √hWmin (IIRil2,∣网2,∣∣T∣∣2 JUll2),
where R, S, T and U are matrices of sizes hcout ×wcin, wcout ×hcin, cout × hwcin and hwcout ×cin
respectively, and can be computed by appropriately reshaping the filter L (details in Section 3). This
upper bound is independent of the input width and height (n). Formal results are stated in Theorem 1
and proved in the appendix. Remarkably, ITI2 is the heuristic suggested by Miyato et al. (2018).
To the best of our knowledge, this is the first work that derives a provable bound on the spectral
norm of convolution filter as a constant factor (dependant on filter sizes, but not filter weights) times
the heuristic of Miyato et al. (2018). In Tables 1 and 3, we show that the other 3 bounds (using
3
Published as a conference paper at ICLR 2021
IlRll 2, Il S Il2, IlU Il2)Canbe significantly smaller than ∖∕hW∣∣T∣∣2 for different convolution filters. Thus,
we take the minimum of these 4 quantities to bound the spectral norm of a convolution filter.
In Section 4, we show that our bound can be used to improve the generalization and robustness
properties of neural networks. Specifically, we show that using our bound as a regularizer during
training, we can achieve improvement in accuracy on par with exact method (Sedghi et al., 2019)
while being significantly faster to train (Table 4). We also achieve significantly higher robustness
certificates against adversarial attacks than CNN-Cert (Boopathy et al., 2018) on a single layer CNN
(Table 5). These results demonstrate potentials for practical uses of our results. Code is available at
the github repository: https://github.com/singlasahil14/fantastic-four.
	Exact (Sedghi et al., 2019)	Exact (Ryu et al., 2019)	Upper Bound (Ours)
Computation	Norm of n2 matrices, each of Size: coutcin	Norm of one matrix of Size: n'coutcin	Norm of four matrices, each of Size: coutcin hw
Time complexity (O)	~Q~.	' n cout cin	n2 hw cout cin	h W Cout cin
Guaranteed bound	✓	✓	✓
Easy gradient computation	X	X	✓
Table 2: Comparison of various methods used for computing the norm of convolution layers. n is the
height and width for a square input, cin is the number of input channels, cout is the number of output
channels, h and w are the height and width of the convolution filter. For Sedghi et al. (2019), we only
compute the largest singular value using power iteration (i.e not all singular values).
2	Notation
For a vector v, We use Vj to denote the element in the jth position of the vector. We use Aj,: and
A:,k to denote the jth row and kth column of the matrix A respectively. We assume both Aj,:, A:,k
to be column vectors (thus Aj,: is the transpose of jth roW of A). Aj,k denotes the element in jth
row and kth column of A. The same rules can be directly extended to higher order tensors. For a
matrix A ∈ Rq×r and a tensor B ∈ Rp×q×r, vec(A) denotes the vector constructed by stacking the
rows of A and vec(B) denotes the vector constructed by stacking the vectors vec(Bj,:,:), j ∈ [p- 1]:
Vec(A)T = [AT,: , AT,,…，A"[	Vec(B)T = [Bk , Bk ,…，B",:」
We use the following notation for a convolutional neural network. L denotes the number of layers and
φ is the activation function. For an input x, we use z(I)(x) ∈ RNI and a(I) (x) ∈ RNI to denote the
raw (before applying φ) and activated (after applying φ) neurons in the Ith hidden layer respectively.
a(0) denotes the input image x. To simplify notation and when no confusion arises, we make the
dependency of Z(I) and a(I) to X implicit. φ (Z(I)) and φ' (Z(I)) denotes the elementwise first and
second derivatives of φ at z(1). W(I) denotes the weights for the Ith layer i.e W(I) will be a tensor
for a convolution layer and a matrix for a fully connected layer. J(I) denotes the jacobian matrix
of Vec(Z(1)) with respect to the input X. θ denotes the neural network parameters. fθ(X) denotes
the softmax probabilities output by the network for an input X. For an input X and label y, the cross
entropy loss is denoted by ` (fθ(X), y).
3	Main Results
Consider a convolution filter L of size cout × cin × h × w applied to an input X of size cin × n × n.
The filter L takes an input patch of size cin × h × w from the input and outputs a vector of size cout
for every such patch. The same operation is applied across all such patches in the image. To apply
convolution at the edges of the image, modern convolution layers either not compute the outputs
4
Published as a conference paper at ICLR 2021
thus reducing the size of the generated feature map, or pad the input with zeros to preserve its size.
When we pad the image with zeroes, the corresponding jacobian becomes a toeplitz matrix. Another
version of convolution treats the input as if it were a torus; when the convolution operation calls for
a pixel off the right end of the image, the layer “wraps around” to take it from the left edge, and
similarly for the other edges. For this version of convolution, the jacobian is a circulant matrix. The
quality of approximations between toeplitz and circulant colvolutions has been analyzed in the case
of 1D (Gray, 2005) and 2D signals (Zhu & Wakin, 2017). For the 2D case (similar to the 1D case),
O(1/p) bound is obtained on the error, where p ×p is the size of both (topelitz and circulant) matrices.
Consequently, theoretical analysis of convolutions that wrap around the input (i.e using circulant
matrices) has been become standard. This is the case that we analyze in this work. Furthermore, we
assume that the stride size to be 1 in both horizontal and vertical directions.
The output Y produced from applying the filter L to an input X is of size cout × n × n. The
corresponding jacobian (J) will be a matrix of size n2cout × n2cin satisfying:
vec(Y) = Jvec(X).
Our goal is to bound the norm of jacobian of the convolution operation i.e, Jg. Sedghi et al.
(2019) also derive an expression for the exact singular values of the jacobian of convolution layers.
However, their method requires the computation of the spectral norm of n2 matrices (each matrix
of size cout × cin) for every convolution layer. We extend their result to derive a differentiable and
easy-to-compute upper bound on singular values stated in the following theorem:
Theorem 1. Consider a convolution filter L of size cout × cin × h × w that applied to input X of size
cin × n × n gives output Y of size cout × n × n. The jacobian of Y with respect to X (i.e. J) will be a
matrix of size n2cout × n2cin. The spectral norm of J is bounded by:
J∣∣2 ≤ √hw min (IlRil2,∣ 网 2,∣∣T∣∣2,∣∣U∣∣2),
where the matrices R, S, T and U are defined as follows:
R=	L： ： 0 0 L；：0 ⋮	L：,：,0,1	… L：,：,1,1	… ⋮	L：,：,0,w-1 L：,：,1,w-1 ⋮	,	S =	-L：,：,0,0 L：,：,0,1 ⋮	L：,：,1,0	… L：,：,1,1	… ⋮	・	»:-;;] ∙,:,〃 1,1 ⋮
	♦ N,"h-1,0	L：,：,h-1,1	…	♦ L：,：,h-1,w-1_			. .L：,：,0,w-1	L：,：,1,w-1	…	. L：,：,h-1,w-1_
T=	A0 A1	…	Ah-1] ,	where Al =	[	：,：,l,0	L：,：,l,1	…	L：,：,l,w-1]	
UT =	[B0T	B1T	… BTh-1] ,	where BlT		[L：T,：,l,0	L：T,：,l,1		…	L：,：,l,w-1]	
Proof of Theorem 1 is in Appendix E. The matrices R, S, T and U are of dimensions couth ×
cinw, coutw × cinh, cout × cinhw and couthw × cin respectively. In the current literature (Miyato
et al., 2018), the heuristic used for estimating the spectral norm involves combining the dimensions
of sizes h, w, cin in the filter L to create the matrix T of dimensions cout × hwcin . The norm of
resulting matrix is used as a heuristic estimate of the spectral norm of the jacobian of convolution
operator. However, in Theorem 1 We show that the norm of this matrix multiplied with a factor of
hhw gives a provable upper bound on the singular values of the jacobian. In Tables 1 and 3, we show
that for different convolution filters, there can be significant differences between the four bounds and
any of these four bounds can be the minimum.
3.1	Tightness analysis
In Appendix F, we show that the bound is exact for a convolution filter with h = w = 1:
Lemma 1. For h = 1, w = 1, the bounds in Theorem 1 are exact i.e:
IJI2 = IRI2 = ISI2 = ITI2 = IUI2
In Table 3, we analyze the tightness between our bound and the exact largest singular value computed
by Sedghi et al. (2019) for different filter shapes. Each convolution filter was constructed by sampling
from a standard normal distribution N(0, 1). We observe that the bound is tight for small filter
sizes but the ratio (Bound/Exact) becomes large for large h and w. We also observe that the values
computed by the four bounds can be significantly different and that we can get a significantly
improved bound by taking the minimum of the four quantities. In Appendix Section G, Figure 1, we
empirically observe that the gap between our upper bound and the exact value can become very small
by adding our bound as a regularizer during training.
5
Published as a conference paper at ICLR 2021
Filter shape	hhw ×				Bound (Ours)	Exact (Sedghi)	Bound/ Exact
	∣∣R∣∣2	∣∣s∣∣2	∣∣t∣∣2	∣U∣2			
64 × 64 × 2 × 2	44.19	46.11	46.76	47.88	44.19	31.96	1.38
64 × 64 × 3 × 3	84.03	82.71	95.27	97.36	82.71	48.77	1.70
64 × 64 × 5 × 5	179.09	177.31	237.22	238.30	177.31	82.39	2.15
64 × 64 × 7 × 7	297.02	296.18	444.69	451.52	296.18	116.05	2.55
64 × 16 × 2 × 2	32.27	32.45	31.18	38.20	31.18	23.25	1.34
64 × 16 × 3 × 3	60.74	62.42	59.67	82.17	59.67	37.09	1.61
64 × 16 × 5 × 5	130.07	132.62	136.16	216.22	130.07	61.01	2.13
64 × 16 × 7 × 7	220.25	220.20	248.50	415.14	220.20	86.54	2.54
Table 3: Tightness analysis between our proposed bound and exact method by Sedghi et al. (2019).
3.2	Gradient Computation
Since the matrix R can be directly computed by reshaping the filter weights L (equation 8), We can
compute the derivative of our bound ∖∕hW∣∣R∣∣2 (or IlSIl2 JTgJUg) with respect to filter weights
L by first computing the derivative of ∖∕hW∣∣R∣∣2 with respect to R and then appropriately reshaping
the obtained derivative.
Let u and v be the singular vectors corresponding to the largest singular value, i.e. ∣R∣2 . Then, the
derivative of our upper bound \/hw ∣∣ R∣∣ 2 with respect to R can be computed as follows:
VR√hw∣R∣2 = √hwuvT where ∣R∣2 = uT Rv.
Moreover, since the weights do not change significantly during the training, we can use one iteration
of power method to update u, v and ∣R∣2 during each training step (similar to Miyato et al. (2018;
2017)). This allows us to use our bound efficiently as a regularizer during the training process.
4	Experiments
All experiments were conducted using a single NVIDIA GeForce RTX 2080 Ti GPU.
4.1	Comparison with existing methods
In Table 1, we show a comparison between the exact spectral norms (computed using Sedghi et al.
(2019), Ryu et al. (2019)) and our upper bound, i.e. √hwmin(∣∣R∣∣2, ∣∣S∣∣2, ∣∣T∣∣2, ∣∣U∣∣2) on a pre-
trained Resnet-18 network (He et al., 2015). Except for the first layer, we observe that our bound
is within 1.5 times the value of the exact spectral norm while being significantly faster to compute.
Similar results can be observed in Table 3 for a standard gaussian filter. Thus, by taking the minimum
of the four bounds, we can get a significant gain.
4.2	Effects on generalization
In Table 4a, we study the effect of using our proposed bound as a training regularizer on the
generalization error. We use a Resnet-32 neural network architecture and the CIFAR-10 dataset
(Krizhevsky, 2009) for training. For regularization, we use the sum of spectral norms of all layers of
the network during training. Thus, our regularized objective function is given as follows1:
min	E(x,y) ['(fθ(x),y)] +β∑U(I)	(1)
where β is the regularization coefficient, (x, y)’s are the input-label pairs in the training data, u(I)
denotes the bound for the Ith convolution or fully connected layer. For the convolution layers, u(I)
1 We do not use the sum of log of spectral norm values since that would make the filter-size dependant factor of
hWυr irrelevant for gradient computation.
6
Published as a conference paper at ICLR 2021
is computed as ∖∕hwmin(∣∣R∣∣2 JSg JTll2 JUg) using Theorem 1. For fully connected layers,
we can compute u(I) using power iteration (Miyato et al., 2018).
β	Test Accuracy						
				Clipping threshold	Test Accuracy		
	No weight decay	weight decay = 10-4					
					No weight decay	weight decay = 10-4	
^0	91.26%	92.53%					
				None	91.26%	92.53%	
0.0008	91.62%	92.37%					
				~n	90.67%	91.87%	
0.0009	91.74%	92.95%					
				^05	92.31%	93.30%	
0.0010	91.79%	93.13%					
				~0δ	91.92%	92.86%	
0.0011	91.87%	92.75%					
			(b) Singular value clipping (Sedghi et al., 2019)				
0.0012	91.81%	92.53%					
0.0013	92.17%	92.73%		Method	Running time (s)	Increase (%)	
0.0014	92.23%	92.61%					
0.0015	91.87%	93.15%		Standard	49.58	-	
0.0016	91.92%	93.26%		Ours	54.39	9.7%	
0.0017	91.70%	92.92%		Clipping (Sedghi et al., 2019)	156.46	215.6%	
0.0018	91.52%	91.89%					
0.0019	92.00%	92.47%					
0.0020	91.82%	92.56%	(C) Running time for 1 epoch on a dataset with				
(a) Our proposed regularizer			10,000 images of size 224x224x3				
Table 4: Comparison between our proposed regularizer (a) and singular value clipping by Sedghi
et al. (2019) (b) on test accuracy. Results are on CIFAR-10 dataset using a Resnet-32 network.
Since weight decay (Krogh & Hertz, 1991) indirectly minimizes the Frobenius norm squared which is
equal to the sum of squares of singular values, it implicitly forces the largest singular values for each
layer (i.e the spectral norm) to be small. Therefore, to measure the effect of our regularizer on the test
set accuracy, we compare the effect of adding our regularizer both with and without weight decay.
The weight decay coefficient was selected using grid search using 20 values between [0, 2 × 10-3]
using a held-out validation set of 5000 samples.
Our experimental results are reported in Table 4a. For the case of no weight decay, we observe
an improvement of 0.97% over the case when β = 0. When we include a weight decay of 10-4 in
training, there is an improvement of 0.73% over the baseline. Using the method of Sedghi et al.
(2019) (i.e. clipping the singular values above 0.5 and 1) results in gain of 0.77% (with weight decay)
and 1.05% (without weight decay) which is similar to the results mentioned in their paper. In addition
to obtaining on par performance gains with the exact method of Sedghi et al. (2019), a key advantage
of our approach is its very efficient running time allowing its use for very large input sizes and deep
networks. We report the training time of these methods in Table 4(c) for a dataset with large input
sizes. The increase in the training time using our method compared to the standard training is just
9.7% while that for Sedghi et al. (2019) is around 215.6%.
4.3	Effects on provable adversarial robustness
In this part, we show the usefulness of our spectral bound in enhancing provable robustness of
convolutional classifiers against adversarial attacks (Szegedy et al., 2014). A robustness certificate is
a lower bound on the minimum distance of a given input to the decision boundary of the classifier.
For any perturbation of the input with a magnitude smaller than the robustness certificate value, the
classification output will provably remain the same. However, computing exact robustness certificates
requires solving a non-convex optimization, making the computation difficult for deep classifiers. In
the last couple of years, several certifiable defenses against adversarial attacks have been proposed
(e.g. Boopathy et al. (2018); Mirman et al. (2018); Zhang et al. (2018); Weng et al. (2018); Singla
& Feizi (2020); Virmaux & Scaman (2018); Tsuzuku et al. (2018); Levine & Feizi (2020); Cohen
7
Published as a conference paper at ICLR 2021
et al. (2019); Levine & Feizi (2020).) In particular, to show the usefulness of our spectral bound in
this application, we examine the method proposed in Singla & Feizi (2020) that uses a bound on the
lipschitz constant of network gradients (i.e. the curvature values of the network). Their robustness
certification method in fact depends on the spectral norm of the Jacobian of different network layers;
making a perfect case study for the use of our spectral bound (details in Appendix Section C).
Due to the difficulty of computing J(I)II 2 When the I th layer is a convolution, Singla & Feizi
(2020) restrict their analysis to fully connected networks where J(I) simply equals the Ith layer
Weight matrix. HoWever, using our results in Theorem 1, We can further bound J(I) and run similar
experiments for the convolution layers. Thus, for a 2 layer convolution netWork, our regularized
objective function is given as folloWs:
min E(x,y)[' (fθ (x),y)] +Yb max (Wy2) - w(,j)∣)(u ⑴)2
Where γ is the regularization coefficient, bis a bound on the second-derivative of the activation function
(∣σ (.)∣ ≤ b), (x, y)’s are the input-label pairs in the training data, and U(I) denotes the bound on the
spectral norm for the I th linear (ConVoIUtion/fully connected) layer. For the convolutional layer, U ⑴
is again computed as \/hwmin(∣∣R∣∣2, ∣∣S∣∣2, ∣∣T∣∣2, ∣∣U∣∣2) using Theorem 1.
In Table 5, We study the effect of the regularization coefficient γ on provable robustness When the
netWork is trained With curvature regularization (Singla & Feizi, 2020). We use a 2 layer convolutional
neural netWork With the tanh (Dugas et al., 2000) activation function and 5 filters in the convolution
layer. We observe that the method in Singla & Feizi (2020) coupled With our bound gives significantly
higher robustness certificate than CNN-Cert, the previous state of the art (Boopathy et al., 2018).
In Appendix Table 7, We study the effect of the adversarial training method in Singla & Feizi (2020)
coupled With our bound on certified robust accuracy. We achieve certified accuracy of 91.25% on
MNIST dataset and 29.26% on CIFAR-10 dataset. These results further highlight the usefulness
of our spectral bound on convolution layers in improving the provable robustness of the netWorks
against adversarial attacks.
Y	MNIST				
	Standard Accuracy	Certified Robust Accuracy	CNN-Cert	CRT + Our bound	Certificate Improvement (Percentage %)
-o	98.35%	0.0%	0.1503	0.1770	17.76%
0.01	94.85%	75.26%	0.2135	0.8427	294.70%
0.02	93.18%	74.42%	0.2378	0.9048	280.49%
0.03	91.97%	72.89%	0.2547	0.9162	259.71%	—
Table 5: Comparison betWeen robustness certificates computed using CNN-Cert (Boopathy et al.,
2018) and the method proposed in Singla & Feizi (2020) coupled With our spectral bound, for
different values of γ for a single hidden layer convolutional neural netWork With tanh activation
function. Certified Robust Accuracy is computed as the fraction of correctly classified samples With
robustness-certificate (computed using Singla & Feizi (2020)) greater than 0.5.
5	Conclusion
In this paper, We derive four efficient and differentiable bounds on the spectral norm of convolution
layer and take their minimum as our single tight spectral bound. This bound significantly improves
over the popular heuristic method of Miyato et al. (2017; 2018), for Which We provide the first
provable guarantee. Compared to the exact methods of Sedghi et al. (2019) and Ryu et al. (2019),
our bound is significantly more efficient to compute, making it amendable to be used in large-scale
problems. Over various filter sizes, We empirically observe that the gap betWeen our bound and
the true spectral norm is small. Using experiments on MNIST and CIFAR-10, We demonstrate
the usefulness of our spectral bound in enhancing generalization as Well as provable adversarial
robustness of convolutional classifiers.
8
Published as a conference paper at ICLR 2021
6	Acknowledgements
This project was supported in part by NSF CAREER AWARD 1942230, HR001119S0026,
HR00112090132, NIST 60NANB20D134 and Simons Fellowship on “Foundations of Deep Learn-
ing.”
References
Cem Anil, James Lucas, and Roger B. Grosse. Sorting out lipschitz function approximation. In
ICML, 2018.
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference
on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 214-223,
International Convention Centre, Sydney, Australia, 06-11 Aug 2017. PMLR.
Peter L. Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Proceedings of the 31st International Conference on Neural Information
Processing Systems, NIPS’17, pp. 6241-6250, USA, 2017. Curran Associates Inc. ISBN 978-1-
5108-6096-4.
Akhilan Boopathy, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu, and Luca Daniel. Cnn-cert: An efficient
framework for certifying robustness of convolutional neural networks, 2018.
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval
networks: Improving robustness to adversarial examples. In Proceedings of the 34th International
Conference on Machine Learning - Volume 70, ICML’17, pp. 854-863. JMLR.org, 2017.
Jeremy M. Cohen, Elan Rosenfeld, and J. Zico Kolter. Certified adversarial robustness via randomized
smoothing. In ICML, 2019.
H. Drucker and Y. Le Cun. Improving generalization performance using double backpropagation.
Trans. Neur. Netw., 3(6):991-997, November 1992. ISSN 1045-9227. doi: 10.1109/72.165600.
Charles Dugas, Yoshua Bengio, Francois Belisle, Claude Nadeau, and Rene Garcia. Incorporating
second-order functional knowledge for better option pricing. In Proceedings of the 13th Interna-
tional Conference on Neural Information Processing Systems, NIPS’00, pp. 451-457, Cambridge,
MA, USA, 2000. MIT Press.
Gene H. Golub and Charles F. Van Loan. Matrix Computations (3rd Ed.). Johns Hopkins University
Press, Baltimore, MD, USA, 1996. ISBN 0-8018-5414-8.
Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael Cree. Regularisation of neural networks
by enforcing lipschitz continuity, 2018.
Robert M. Gray. Toeplitz and circulant matrices: A review. Commun. Inf. Theory, 2(3):155-239,
August 2005. ISSN 1567-2190. doi: 10.1561/0100000006.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.
Improved training of wasserstein gans. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing
Systems, volume 30. Curran Associates, Inc., 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
770-778, 2015.
Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classifier
against adversarial manipulation. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30,
pp. 2266-2276. 2017.
9
Published as a conference paper at ICLR 2021
S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber. Gradient flow in recurrent nets: the
difficulty of learning long-term dependencies. In S. C. Kremer and J. F. Kolen (eds.), A Field
Guide to Dynamical Recurrent Neural Networks. IEEE Press, 2001.
Sepp Hochreiter. Untersuchungen zu dynamischen neuronalen netzen. 1991.
Emiel Hoogeboom, Victor Garcia Satorras, Jakub M. Tomczak, and Max Welling. The convolution
exponential and generalized sylvester flows. In Advances in Neural Information Processing Systems
33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual, 2020.
Haoming Jiang, Zhehui Chen, Minshuo Chen, Feng Liu, Dingding Wang, and Tuo Zhao. On
computation and generalization of generative adversarial networks under spectrum control. In
International Conference on Learning Representations, 2019.
Gunter Klambauer, Thomas Unterthiner, Andreas Mayr, and SePP Hochreiter. Self-normalizing
neural networks. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran
Associates, Inc., 2017.
Alex Krizhevsky. Learning multiPle layers of features from tiny images. Technical rePort, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deeP convo-
lutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.),
Advances in Neural Information Processing Systems, volume 25. Curran Associates, Inc., 2012.
Anders Krogh and John A. Hertz. A simPle weight decay can imProve generalization. In NIPS, 1991.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning aPPlied to document
recognition. Proceedings ofthe IEEE,86(11):2278-2324,1998. doi: 10.1109/5.726791.
Alexander Levine and Soheil Feizi. (de)randomized smoothing for certifiable defense against
Patch attacks. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.),
Advances in Neural Information Processing Systems, volume 33, pp. 6465-6475. Curran As-
sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
47ce0875420b2dbacfc5535f94e68433-Paper.pdf.
Philip M. Long and Hanie Sedghi. Generalization bounds for deep convolutional neural networks. In
International Conference on Learning Representations, 2020.
Matthew Mirman, Timon Gehr, and Martin T. Vechev. Differentiable abstract interpretation for
provably robust neural networks. In ICML, 2018.
Takeru Miyato, Shin ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: A
regularization method for supervised and semi-supervised learning. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 41:1979-1993, 2017.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. In International Conference on Learning Representations, 2018.
Jonathan Peck, Joris Roels, Bart Goossens, and Yvan Saeys. Lower bounds on the robustness to
adversarial perturbations. In NIPS, 2017.
Ernest Ryu, Jialin Liu, Sicheng Wang, Xiaohan Chen, Zhangyang Wang, and Wotao Yin. Plug-
and-play methods provably converge with properly trained denoisers. In Kamalika Chaudhuri
and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine
Learning, volume 97 of Proceedings of Machine Learning Research, pp. 5546-5557, Long Beach,
California, USA, 09-15 JUn 2019. PMLR.
Hanie Sedghi, Vineet Gupta, and Philip M. Long. The singular values of convolutional layers. In
International Conference on Learning Representations, 2019.
10
Published as a conference paper at ICLR 2021
Sahil Singla and Soheil Feizi. Second-order provable defenses against adversarial attacks. In
Hal DaUme In and Aarti Singh (eds.), Proceedings of the 37th International Conference on
Machine Learning, volume 119 of Proceedings OfMachine Learning Research,pp. 8981-8991.
PMLR,13-18Jul 2020.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning
Representations, 2014.
Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein auto-encoders.
In International Conference on Learning Representations, 2018.
Yusuke Tsuzuku, I. Sato, and Masashi Sugiyama. Lipschitz-margin training: Scalable certification of
perturbation invariance for deep neural networks. In NeurIPS, 2018.
Aladin Virmaux and Kevin Scaman. Lipschitz regularity of deep neural networks: analysis and
efficient estimation. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran
Associates, Inc., 2018.
Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Duane Boning, and Inderjit
S. Dhillon ANDLuca Daniel. Towards fast computation of certified robustness for relu networks.
In International Conference on Machine Learning (ICML), july 2018.
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, and Jeffrey Pennington.
Dynamical isometry and a mean field theory of CNNs: How to train 10,000-layer vanilla con-
volutional neural networks. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th
International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning
Research, pp. 5393-5402, Stockholmsmassan, Stockholm Sweden, 10-15 JUl 2018. PMLR.
Yuichi Yoshida and Takeru Miyato. Spectral norm regularization for improving the generalizability
of deep learning. ArXiv, abs/1705.10941, 2017.
Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural network
robustness certification with general activation functions. In S. Bengio, H. Wallach, H. Larochelle,
K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing
Systems 31, pp. 4939-4948. Curran Associates, Inc., 2018.
Z. Zhu and M. B. Wakin. On the asymptotic equivalence of circulant and toeplitz matrices. IEEE Trans-
actions on Information Theory, 63(5):2975-2992, May 2017. doi: 10.1109/TIT.2017.2676808.
Appendix
A Notation
For a vector v, we use vj∙ to denote the element in the jth position of the vector. We use Aj； to
denote the jth row of the matrix A, A:,k to denote the kth column of the matrix A. We assume both
Aj,: and A:,k to be column vectors (thus Aj,: is constructed by taking the transpose of jth row of
A). Aj,k denotes the element in jth row and kth column of A. Aj,:k and Aj∙,k denote the vectors
containing the first k elements of the jth row and first j elements of kth column, respectively. Aj,:k
denotes the matrix containing the first j rows and k columns of A:
Aj,k
Γ A0,k
LL A1,k
Aj,:k
LΓ	A0,0
LL	A1,0
A0,1
A1,1
LAj-1,k」
Aj-1,1
A0,k-1 一
A1,k-1
⋮
Aj-i,k-i」
The same rules can be directly extended to higher order tensors.
For n ∈ N, we use [n] to denote the set {0, . . . , n} and [p, q] (p < q) to denote the set {p,p+ 1 . . . , q}.
We will index the rows and columns of matrices using elements of [n], i.e. numbering from 0.
Addition of row and column indices will be done mod n unless otherwise indicated. For a matrix
11
Published as a conference paper at ICLR 2021
A ∈ Rq×r and a tensor B ∈ Rp×q×r, vec(A) denotes the vector constructed by stacking the rows of A
and Vec(B) denotes the vector constructed by stacking the vectors Vec(Bj,:,:), j ∈ [p - 1]:
一 A0，: 一
Vec(A) =	1,：
一 Aq-1,: 一
一 Vec(B0,：,:)
Vec(B) =Vec(BG
一 Vec(Bp-1,：,J
For a given vector v ∈ Rn, circ(v) denotes the n × n circulant matrix constructed from v i.e rows
of circ(v) are circular shifts of v. For a matrix A ∈ Rn×n, circ(A) denotes the n2 × n2 doubly
block circulant matrix constructed from A, i.e each n × n block of circ(A) is a circulant matrix
constructed from the rows Aj,:, j ∈ [n - 1]:
	「V0	v1	•	Vn-1"		"circ(Ao,：)	∙	• circ(An-i,)
circ(v)=	vn-1 ⋮	v0	.	Vn-2 ⋮ ♦ .	,	circ(A) =	circ(An-1,:) ⋮	•	Cbrc(An-2,、 ⋮ ♦ .
	, vι	v2	- V0 .		_ circ(Aι,:)	∙	•	circ(Ao,:) _
We use F to denote the Fourier matrix of dimensions n × n, i.e Fj,k = ωjk, ω = e-2π"n,i2 = -1.
For a matrix A, σ(A) denotes the set of singular values of A. σmax(A) and σmin(A) denotes the
largest and smallest singular values of A respectively. A Θ B denotes the kronecker product of A
and B. We use A Θ B to denote the hadamard product between two matrices (or vectors) of the same
size. We use In to denote the identity matrix of dimensions n × n.
We use the following notation for a convolutional neural network. L denotes the number of layers and
φ denotes the activation φ. For an input x, we use z(I)(x) ∈ RNI and a(I) (x) ∈ RNI to denote the
raw (before applying the activation φ) and activated (after applying the activation function) neurons
in the Ith hidden layer of the network, respectively. Thus a(0) denotes the input image x. To simplify
notation and when no confusion arises, we make the dependency of z(I) and a(I) to x implicit.
φ (Z(I)) and φ' (Z(I)) denotes the elementwise derivative and double derivative of φ at z(1). W(I)
denotes the weights for the Ith layer i.e W(I) will be a tensor for a convolution layer and a matrix
for a fully connected layer. J(I) denotes the jacobian matrix of Vec(Z(1)) with respect to the input
x. θ denotes the neural network parameters. fθ(x) denotes the softmax probabilities output by the
network for an input x. For an input x and label y, the cross entropy loss is denoted by ` (fθ(x), y).
B Sedghi et al’s method
Consider an input X ∈ Rcin×n×n and a convolution filter L ∈ Rcout×cin ×h×w to X such that n >
max(h, w). Using L, we construct a new filter K ∈ Rcout×cin×n×n by padding with zeroes:
Lc,d,k,l
Kc,d,k,l = { 0,
k∈ [h-1], l∈ [w-1]
otherwise }
The Jacobian matrix J of dimensions coutn2 × cin n2 is given as follows:
-B(0,0)	B(0,1)	…	B(0,cin-i)
B(1,0)	B(1,1)	…	B(1,cin-1)
J =
:	:	∙.	⋮
B(Cout-1,0)	B(Cout-1,1)	...	B(Cout-1,Cin-1)
where B(c,d) is given as follows:
B(C,d) = circ(KC,d,:,: )
By inspection we can see that:
Vec(Y) = JVec(X)
From Sedghi et al. (2019), we know that singular values of J are given by:
σ(J) =	U	σ(Gj,k))
j∈[n-1],k∈[n-1]
12
Published as a conference paper at ICLR 2021
where each G(j,k) is a matrix of dimensions cout × cin and each element of G(j,k) is given by:
Gcjdk) = (FTKc,d,：,：F)j,k,	C ∈ [cout - 1], d ∈ [cin - 1]
The largest singular value of J i.e ∣ J ∣∣ 2 can be computed as follows:
J∣∣2 = σmaχ(J) = max	σmaχ(G(j,k))
j∈[n-1],k∈[n-1]
C Second-order robustness
C.1 Robustness certification
Given an input x(0) ∈ RD and a binary classifier f (here f : RD → R is differentiable everywhere),
our goal is to find a lower bound to the decision boundary f(x) = 0. The primal is given as follows:
* 一 ♦_ ____
Pcert = min max
xη
1IX - X(O嘴 + ηf(X)
The dual of the above optimization is given as follows:
dc*ert = max min
ηx
1
# - X(O) B + ηf(X)
2
The inner minimization can be solved exactly when the function 1/2IIX - X(O) ∣∣ 2 + ηf (x) is strictly
convex, i.e has a positive-definite hessian. The same condition can be written as:
1
▽X WlX-χ(叫2+ ηf(X) = I+ ηvX”0
2
It is easy to verify that if the eigenvalues of the hessian of f i.e ▽2xf are bounded between m and M
for all X ∈ RD, i.e:
ml & vXf & MI,	∀ x ∈ RD
The hessian I + ηvXf N 0 (i.e is positive definite) for -1/M < η < -1/m. We refer an interested
reader to Singla & Feizi (2020) for more details and a formal proof of the above theorem.
Thus the inner minimization can be solved exactly for these set of values resulting in the following
optimization:
qcert
max min
-1/M ≤η≤-1∕m x
1 ||x - X(O) l2 + ηf (X)
Note that qc*ert can be solved exactly using convex optimization. Thus, we get the following chain of
inequalities:
qcert ≤ dcert ≤ pcert
Thus solving qc*ert gives a robustness certificate. The resulting certificate is called Curvature-based
Robustness Certificate.
C.2 Adversarial training with curvature regularization
Let K(θ, y, t) denote the upper bound on the magnitude of curvature values for label y and target t
(computed using Singla & Feizi (2020)).
The objective function for training with curvature regularization is given by:
min E(x,y)['(fθ(χ),y)] + γK(θ,y,t)
θ
The objective function for adversarial training with curvature regularization is given by:
min E(x,y) [' (fθ (X(attack)) ,y)] + γK (θ,y,t)
θ
X(attack) is computed by doing an adversarial attack on the input X either using l2 bounded PGD
or Curvature-based Attack Optimization defined in Singla & Feizi (2020). When Curvature-based
Attack Optimization is used for adversarial attack, the resulting attack is called Curvature-based
Robust Training (CRT).
13
Published as a conference paper at ICLR 2021
D Difficulty of computing the gradient of convolution
Consider a 1D convolution filter of size 3 given by ([1, 2, -1]) applied to an array of size 5. The
corresponding jacobian matrix is given by:
	Γ1	2	-1	0	01
	0	1	2	-1	0
J=	0	0	1	2	-1
	-1	0	0	1	2
	2	-1	0	0	1
The largest singular value (JIl 2) , the left singular vector (U) and the right singular vector (V) is
given by:
IJI2 = 2.76008,
u=
Γ-0.556141
0.11457
0.62695
0.27290
-0.45829
v=
r-0.63245i
-0.19544
0∙51167
0∙51167
1-0.19544」
The gradient (Vj ∣∣ J∣∣2) is given as follows:
▽J 1|J|12 =
-0.35174
ΓΓ-0.07246
ΓΓ-0.39652
ΓΓ-0.17260
0.28984
0.10870
0.02239
-0.12253
-0.05333
0.08957
-0.28456
0.05862
0.32079
0.13964
-0.23449
-0.28456
0.05862
0.32079
0.13964
-0.23449
0.10870-
-0.02239
-0.12253
-0.05334
0.08957
Clearly the gradient contains non-zero elements in elements of J that are always zero and unequal
gradient values at elements of J that are always equal.
E Proof of Theorem 1
Proof. Consider an input X ∈ Rcin×n×n and a convolution filter L ∈ Rcout×cin×h×w to X such that
n > max(h, w). Using L, we construct a new filter K ∈ Rcout×cin×n×n by padding with zeroes:
Lc,d,k,l,	k ∈ [h - 1], l ∈ [w - 1]
c,d,k,l 0,	otherwise
(2)
The output Y ∈ Rcout×n×n of the convolution operation is given by:
Yc,r,s
cin -1 n-1 n-1
∑ ∑ ∑ Xd,r+k,s+lKc,d,k,l
d=0 k=0 l=0
We construct a matrix J of dimensions coutn2 × cinn2 as follows:
-B(O,O)	B(Oj)	…	B(0,cin-I)
B(Ie)	B(1,1)	…	B(I,cin-I)
J =	:
Γ
B(Cout-ι,o) B(Cout-ι,1)…B(Cout-ι,cin-1)
where B(c,d) is given as follows:
B(C,d) = Circ(KC,d,：,：)
By inspection we can see that:
vec(Y) = Jvec(X)
This directly implies that J is the jacobian of vec(Y) with respect to vec(X) and our goal is to find
a differentiable upper bound on the largest singular value of J.
From Sedghi et al. (2019), we know that singular values of J are given by:
σ(J) =	U	σ(G(j,k))	⑶
j∈[n-1],k∈[n-1]
14
Published as a conference paper at ICLR 2021
where each G(j,k) is a matrix of dimensions cout × cin and each element of G(j,k) is given by:
Gcjdk)= (FTKc,d,：,：F)j,k,	C ∈ [cout - 1], d ∈ [cin - 1]	(4)
Using equation 3, we can directly observe that a differentiable upper bound over the largest singular
value of G(j,k) that is independent ofj and k will give an desired upper bound. We will next derive
the same.
Using equation 4, we can rewrite G(c,jd,k) as:
G(cj,d,k) = (FTKc,d,：,：F)j,k = (F：,j)TKc,d,：,：F：,k	(5)
Using equation 2, we know that Kc,d,：,： is a sparse matrix of size n × n with only the top-left block
of size h × w that is non-zero. We take the first h rows and w columns of Kc,d,：,： i.e Kc,d,：h,：w, first
h elements of F：,j i.e F：h,j and first w elements of F：,k i.e F：w,k. We have the following set of
equalities:
Gc(,jd,k) = (F：,j)TKc,d,：,：F：,k
= (F：h,j ) Kc,d,：h,：wF：w,k
= (F：h,j)TLc,d,：,：F：w,k	(6)
Thus, G(j,k) can be written as follows:
h-1 w-1
Gc,d, = ∑ ∑ Fl,j Lc,d,l,m Fm,k
l=0 m=0
h-1 w-1
G(j,k) = ∑ ∑ Fl,jL：,：,l,mFm,k	(7)
l=0 m=0
Now consider a block matrix R of dimensions cout h × cinw given by:
R =	L： ： 0 0 L：,：,1,0 ⋮	L：,：,0,1 L：,：,1,1	,	L：,：,0,w-1 L：,：,1,w-1 ⋮ ♦ .	(8)
	」：,：,h-1,0	L：,：,h-1,1	L L：,：,h-1,w-1」	
Thus the block in lth row and mth column of R is the cout × cin matrix L：,：,l,m.
Consider two matrices: (F：hj)T 0 Icout (of dimensions Cout × Couth) and F：w,k 0 Icin (of dimensions
cinw × cin).
Using equation 7 and equation 8, we can see that:
G(j,k) = ((F：h,j)T0Icout)R(F：w,k0Icin)	(9)
Taking the spectral norm of both sides:
||G(j,k几=Il((F：h,j)T 0Icout) R(F：w,k 0 Icin儿
Il G(j,k) II2 ≤ ∣∣( (F：h,j )T 0 Icout)II2	I(F：w,k 0 Icin )12
Using IIA 0 B12 = ∣∣A∣∣2∣∣B∣∣2 and since both IIIcouJ 2 and IIIciJ12 are 1:
∣∣G(j,k)∣∣2 ≤ I1F：h,jI∣2 I∣R∣∣2 IEw,k∣∣2
Further note that since Fj,k = ωjk, we have ||F：h,j ∣∣2 = ∖∕h and ||F：w,k||2 = √w.
UGak)∣∣2 ≤ √hw ∣∣r∣∣2
Alternative inequality (1): Note that equation 6 can also be written by taking the transpose of the
scalar (F：h,j)TL：,：,c,dF：w,k:
Gc,d, = (F：h,j) Lc,d,：,：F：w,k = (F：w,k) (Lc,d,：,：) F：h,j
15
Published as a conference paper at ICLR 2021
Thus, G(j,k) can alternatively be written as follows:
w-1 h-1
Gc,d,	= ∑ ∑ Fl,k Lc,d,m,l Fm,j
l=0 m=0
w-1 h-1
G(j,k) = ∑ ∑ Fl,kL：,:,m,lFm,j
l=0 m=0
(10)
Now consider a block matrix S of dimensions coutw × cin h given by:
S =	L： ： 0 0 L⅛∣	L：,：,1,0 L：,：,1,1	,	L： ： h-1 o L：：,,：：,,hh--11,,01 ⋮ ♦ .	(11)
	工,：,0,w-1	L：,：,1,w-1	L L：,：,h-1,w-1」	
For S the block in lth row and mth column of R is the Cout × Cin matrix L：,：,m,i.
Consider two matrices:
(F：w,k)T屯Icout (of dimensions Cout × Coutw) and F：h,j 屯Icin
Cinh × Cin).
Using equation 10 and equation 11, we again have:
G(j,k) = ((F：w,k)T 屯 Icout) S (F：h,j 屯 Icin)
(of dimensions
(12)
Taking the spectral norm of both sides and using the same procedure that we used for R, we get the
following inequality:
W2 ≤ √hW∣∣S∣∣2
Alternative inequality (2): Using equation 7, G(j,k) can alternatively be written as follows:
h-1 w-1
Gc,d, = ∑ ∑ Lc,d,l,mFl,j Fm,k
l=0 m=0
h-1 w-1
G(j,k) = ∑ ∑ L：,：,l,mFl,jFm,k	(13)
l=0 m=0
Now consider a block matrix T of dimensions Cout × Cin hw given by:
T = [Aο Al	…Ah-ι]	(14)
where each block Al is a matrix of dimensions Cout × Cinw given as follows:
Al = [L：,：,l,0 L：,：,l,1 … L：,：,l,w-1]	(15)
For T, the block in lth column is the Cout × Cinw matrix Al. For Al, the block in the mth column is
the Cout × Cin matrix L：,：,l,m .
Consider the matrix: F：w,k 0 F：h,j 0 Icin (of dimensions Cinhw × Cin).
Using equation 13, equation 14 and equation 15, we again have:
G(j,k) = T (F：h,j 0 F：w,k 0 Icin)	(16)
Taking the spectral norm of both sides and using the same procedure that we used for R and S, we
get the following inequality:
IIGak) ∣∣2 ≤ √hW ∣∣τ∣∣2
Alternative inequality (3): Using equation 7, G(j,k) can alternatively be written as follows:
h-1 w-1
Gc,d, = ∑ ∑ Fl,j Fm,k Lc,d,l,m
l=0 m=0
h-1 w-1
G(j,k) = ∑ ∑ Fl,jFm,kL：,：,l,m	(17)
l=0 m=0
16
Published as a conference paper at ICLR 2021
Now consider a block matrix U of dimensions cout hw × cin given by:
Γ Bo I
U = 1bB 1]	(18)
LD h-1 J
where each block Bl is a matrix of dimensions coutw × cin given as follows:
L：,：,1,0
Bl =	L"⋮,1'1	(19)
L /	1
L ：,：,l,w-1J
For U, the block in lth row is the cout w × cin matrix Bl. For Bl, the block in the mth row is the
cout × cin matrix L：,：,l,m.
Consider the matrix: ((F：h,j)T 0 (F：w,k)T 艺 Icout) (of dimensions Cout × Couthw).
Using equation 17, equation 18 and equation 19, we again have:
G(j,k) = ((F：h,j)T 0 (F：w,k)T 0 Icout)U	(20)
Taking the spectral norm of both sides and using the same procedure that we used for R, S and T,
we get the following inequality:
IIGj叫2 ≤ √hWIIUIl2
Taking the minimum of the four bounds i.e ∖J~hw min(∣∣ R∣∣ 2, ∣∣ S ∣∣ 2, IITII2, ∣∣ U ∣∣ 2 ),we have the stated
result.	□
F	Proof of Lemma 1
Proof. When h = 1 and w = 1, note that in equations 9, 12, 16 and 20, we have:
F：h,j = 1,	F：w,k = 1
This directly implies:
∣J∣2 = ∣R∣2 = ∣S∣2 = ∣T∣2 = ∣U∣2
□
G EFFECT OF INCREASING β ON SINGULAR VALUES
In Figure 1, we plot the effect of increasing β on the sum of true singular values of the network and
sum of our upper bound. We observe that the gap between the two decreases as we increase β .
In Table 6, we show the effect of increasing β on the bound on the largest singular value of each
layer.
H Effect on certified robust accuracy
17
Published as a conference paper at ICLR 2021
Filter shape	β values							
	0	0.0006	0.0008	0.001	0.0012	0.0014	0.0016	0.0018
16 × 3 × 3 × 3	37.96	11.05	9.13	8.2	7.28	8.66	7.06	5.57
16 × 16 × 3 × 3	22.23	5.89	5.08	5.26	5.19	6.17	3.41	3.27
16 × 16 × 3 × 3	25.33	5.69	4.87	4.9	4.75	5.80	3.26	3.27
16 × 16 × 3 × 3	25.29	5.28	4.74	4.08	2.89	6.60	3.72	2.32
16 × 16 × 3 × 3	21.66	4.99	4.36	3.64	2.64	5.81	3.61	2.25
16 × 16 × 3 × 3	20.15	4.92	3.89	4.61	2.53	3.13	0.07	2.4
16 × 16 × 3 × 3	16.36	4.37	3.52	4.08	2.34	2.60	0.07	2.14
16 × 16 × 3 × 3	15.39	5.39	5.65	3.15	41	4.72	2.31	1.69
16 × 16 × 3 × 3	14.67	4.38	4.61	2.32	3.52	3.66	2.37	1.57
16 × 16 × 3 × 3	19.26	7.41	6.28	4.96	3.57	0.13	5.38	3.19
16 × 16 × 3 × 3	14.60	5.77	4.84	4.01	2.68	0.09	4.31	2.64
32 × 16 × 3 × 3	23.24	8.53	8.09	7.06	6.24	6.30	6.4	6.61
32 × 32 × 3 × 3	22.19	9.65	9.57	7.99	7.27	7.12	7.72	7.82
32 × 32 × 3 × 3	17.97	7.61	7.52	6.32	4.72	5.25	5.06	2.57
32 × 32 × 3 × 3	16.76	6.83	6.29	5.38	4.12	4.48	4.48	2.22
32 × 32 × 3 × 3	17.62	8.31	62-	6.73	4.06	5.23	5.65	3.21
32 × 32 × 3 × 3	15.7	7.24	5.23	5.5	3.42	4.45	4.73	2.8
32 × 32 × 3 × 3	15.82	8.34	5.42	5.29	4.86	5.41	5.03	4.11
32 × 32 × 3 × 3	15.83	6.99	4.48	4.47	4.02	4.62	4.24	3.56
32 × 32 × 3 × 3	18.46	81	6.03	5.31	5.91	5.64	4.83	6.24
32 × 32 × 3 × 3	17.75	7.08	5.17	4.53	4.98	4.61	4.15	5.31
64 × 32 × 3 × 3	23.57	12.21	10.97	10.41	10.84	10.15	9.7	10.31
64 × 64 × 3 × 3	22.97	13.2	12.23	11.72	12.55	11.36	11.14	12.39
64 × 64 × 3 × 3	22.35	12.18	11.71	11.13	10.53	10.57	10.2	10.43
64 × 64 × 3 × 3	22.02	11.28	10.95	10.43	9.94	9.94	9.52	9.55
64 × 64 × 3 × 3	20.91	12.09	11.7	11.07	10.47	10.85	9.87	9.81
64 × 64 × 3 × 3	21.77	10.85	10.51	9.81	96-	9.62	8.56	8.43
64 × 64 × 3 × 3	19.63	11.97	11.56	10.28	10.98	10.02	9.32	8.69
64 × 64 × 3 × 3	17.61	10-	10.03	8.81	9.63	8.26	7.57	7.35
64 × 64 × 3 × 3	16.67	10.34	11.61	10.26	10.93	8.64	8.05	8.56
64 × 64 × 3 × 3	17.81	8.16	8.73	7.68	8.18	6.61	6.07	6.49
10 × 64	12.43	10.79	10.55	10.53	10.11	10.14	9.64	9.39
Table 6: Effect of increasing β on the bounds (∖∕hW∣∣R∣∣2) of each layer
18
Published as a conference paper at ICLR 2021
Figure 1: We plot the effect of increasing β on the sum of the bounds (computed using our method
i.e ∖∕hW∣∣R∣∣2) and sum of true spectral norms (J(I) ∣∣2 computed using Sedghi et al. (2019)) for a
Resnet-32 neural network trained on CIFAR-10 dataset (without any weight decay). We observe that
the gap between the two decreases as we increase β . In Table 6, we show the effect of increasing β
on the bounds of individual layers of the same network.)
Y	MNIST			CIFAR-10		
	Standard Accuracy	Empirical Robust Accuracy	Certified Robust Accuracy	Standard Accuracy	Empirical Robust Accuracy	Certified Robust Accuracy
^0	98.68%	87.81%	0.00%	56.22%	14.88%	0.00%
0.01	97.08%	92.92%	91.25%	53.52%	31.82%	17.39%
0.02	96.36%	90.98%	89.58%	49.55%	31.80%	25.93%
0.03	95.54%	89.99%	88.75%	46.56%	31.98%	29.26%
Table 7: Comparison between certified robust accuracy for different values of the regularization
parameter γ for a single hidden layer convolutional neural network with softplus activation function.
Certified Robust Accuracy is computed as the fraction of correctly classified samples with CRC
(Curvature-based Robustness Certificate as defined in Singla & Feizi (2020)) greater than 0.5.
Empirical robust accuracy is computed by running l2 bounded PGD (200 steps, step size 0.01). We
use convolution layer with 64 filters and stride of size 2.
19