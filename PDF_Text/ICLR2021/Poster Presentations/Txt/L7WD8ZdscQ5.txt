Published as a conference paper at ICLR 2021
The Role of Momentum Parameters in the
Optimal Convergence of Adaptive Polyak’s
Heavy-ball Methods
Wei Tao *
Sheng Long *
Institute of Evaluation and Assessment Research Department of Information Engineering
Academy of Military Science
Beijing, China
wtao_plaust@163.com
Army Academy of Artillery and Air Defense
Hefei, China
ls15186322349@163.com
Gaowei Wu
Institute of Automation CAS
School of Artificial Intelligence
University of CAS
Beijing, China
gaowei.wu@ia.ac.cn
Qing Tao t
Department of Information Engineering
Army Academy of Artillery and Air Defense
Hefei, China
qing.tao@ia.ac.cn
Ab stract
The adaptive stochastic gradient descent (SGD) with momentum has been widely
adopted in deep learning as well as convex optimization. In practice, the last
iterate is commonly used as the final solution. However, the available regret anal-
ysis and the setting of constant momentum parameters only guarantee the optimal
convergence of the averaged solution. In this paper, we fill this theory-practice
gap by investigating the convergence of the last iterate (referred to as individual
convergence), which is a more difficult task than convergence analysis of the av-
eraged solution. Specifically, in the constrained convex cases, we prove that the
adaptive Polyak’s Heavy-ball (HB) method, in which the step size is only updated
using the exponential moving average strategy, attains an individual convergence
rate of O(√1t), as opposed to that of O(Iogtt) of SGD, where t is the number of
iterations. Our new analysis not only shows how the HB momentum and its time-
varying weight help us to achieve the acceleration in convex optimization but also
gives valuable hints how the momentum parameters should be scheduled in deep
learning. Empirical results validate the correctness of our convergence analysis
in optimizing convex functions and demonstrate the improved performance of the
adaptive HB methods in training deep networks.
1	Introduction
One of the most popular optimization algorithms in deep learning is the momentum method
(Krizhevsky et al., 2012). The first momentum can be traced back to the pioneering work of Polyak’s
heavy-ball (HB) method (Polyak, 1964), which helps accelerate stochastic gradient descent (SGD)
in the relevant direction and dampens oscillations (Ruder, 2016). Recent studies also find that the
HB momentum has the potential to escape from the local minimum and saddle points (Ochs et al.,
2014; Sun et al., 2019a). From the perspective of theoretical analysis, HB enjoys a smaller conver-
gence factor than SGD when the objective function is twice continuously differentiable and strongly
convex (Ghadimi et al., 2015). In nonsmooth convex cases, with suitably chosen step size, HB
attains an optimal convergence rate of O(√1t) in terms of the averaged output (Yang et al., 2016),
where t is the number of iterations.
* Equal contribution
,Corresponding author
1
Published as a conference paper at ICLR 2021
To overcome the data-independent limitation of predetermined step size rules, some adaptive gra-
dient methods have been proposed to exploit the geometry of historical data. The first algorithm in
this line is AdaGrad (Duchi et al., 2011). The intuition behind AdaGrad is that the seldom-updated
weights should be updated with a larger step size than the frequently-updated weights. Typically,
AdaGrad rescales each coordinate and estimates the predetermined step size by a sum of squared
past gradient values. As a result, AdaGrad has the same convergence rate as vanilla SGD but enjoys
a smaller factor especially in sparse learning problems. The detailed analysis of AdaGrad (Mukka-
mala & Hein, 2017) implies that one can derive similar convergence rates for the adaptive variants
of the predetermined step size methods without additional difficulties.
Unfortunately, experimental results illustrate that AdaGrad under-performed when applied to train-
ing deep neural newtworks (Wilson et al., 2017). Practical experience has led to the development of
adaptive methods that is able to emphasize the more recent gradients. Specifically, an exponential
moving average (EMA) strategy was proposed in RMSProp to replace the cumulative sum operation
(Tieleman & Hinton, 2012). Adam (Kingma & Ba, 2014), which remains one of the most popular
optimization algorithms in deep learning till today, built upon RMSProp together with updating the
search directions via the HB momentum. Generally speaking, the gradient-based momentum algo-
rithms that simultaneously update the search directions and learning rates using the past gradients
are referred to as the Adam-type methods (Chen et al., 2019). These kinds of methods have achieved
several state-of-the-art results on various learning tasks (Sutskever et al., 2013).
Compared with HB and AdaGrad, the main novelty of Adam lies in applying EMA to gradient
estimate (first-order) and to element-wise square-of-gradients (second-order), with the momentum
parameter β1t and step size parameter β2t (see (6)) (Alacaoglu et al., 2020). However, the use of
EMA causes a lot of complexities to the convergence analysis. For example, in the online setting,
(Kingma & Ba, 2014) offered a proof that Adam would converge to the optimum. Despite its re-
markable practicality, Adam suffers from the non-convergence issue. To overcome its advantages,
several variants such as AMSGrad and AdamNC were proposed (Reddi et al., 2018). Unfortunately,
the regret bound of AMSGrad in (Reddi et al., 2018) is O(√log tʌ/t) for nonsmooth convex prob-
lems, as opposed to that of O(√t) of SGD. On the other hand, EMA uses the current step size in
exponential moving averaging while the original HB can use the previous information (Zou et al.,
2018). This will lead the update to stagnate when β1t is very close to 1. Fortunately, such a dilemma
will not appear in Polyak’s HB method and a simple proof on the convergence of this kind of Adams
in smooth cases has been provided (DefOSSez et 1 2O20).
In this paper, we will focus on the adaptive Polyak’s HB method, in which the step size is only
updated using EMA. Despite various reported practical performance for the Adam-type methods,
there still exist some gaps between theoretical guarantees and empirical success.
•	First of all, some important regret bounds have been established to guarantee the perfor-
mance of online Adam-type algorithms. Nevertheless, the online-to-batch conversion can
inevitably lead the solution of the induced stochastic algorithm to take the form of averag-
ing of all the past iterates. In practice, the last iterate is popularly used as the final solution,
which has the advantage of readily enforcing the learning structure (Chen et al., 2012). For
SGD, the convergence of the last iterate, which is referred to as individual convergence
in (Tao et al., 2020b), was posed as an open problem (Shamir, 2012). Only recently, its
optimal individual convergence rate is proved to be O(lo√t) and O(Iogt) for general and
strongly convex problems respectively (Harvey et al., 2019; Jain et al., 2019). Despite
enjoying the optimal averaging convergence (Yang et al., 2016), as far as we know, the
individual convergence about the adaptive HB has not been discussed.
•	Secondly, the momentum technique is often claimed as an accelerated strategy in machine
learning community. However, almost all the theoretical analysis is only limited to the Nes-
terov’s accelerated gradient (NAG) (Nesterov, 1983) method especially in smooth cases
(HU et al., 2009; Liu & Belkin, 2020), which accelerates the rate of SGD from O(t) to
O(t2). While the individual convergence of HB is also concerned in some papers (Seb-
bouh et al., 2020; Sun et al., 2019b), the considered problem is limited to smooth and the
derived rate is not optimal in convex cases. It is discovered that NAG is capable of ac-
celerating the rate of individual convergence of SGD from O(l√gtt) to O(√t) (Tao et al.,
2
Published as a conference paper at ICLR 2021
2020a) in nonsmooth convex cases. Nevertheless, there is still a lack of the report about the
acceleration of the adaptive HB.
•	Finally, in practice, almost all the momentum and Adam-type algorithms are often used
with a constant momentum parameter β1t (typically between 0.9 and 0.99). In theory, re-
gret guarantees in the online Adam require a rapidly decaying β1t → 0 schedule, which
is also considered in (Sutskever et al., 2013; Orvieto et al., 2019). This gap is recently
bridged by getting the same regret bounds as that in (Reddi et al., 2018) with a constant β1t
(Alacaoglu et al., 2020). In each state-of-the-art deep learning library (e.g. TensorFlow,
PyTorch, and Keras), HB is named as SGD with momentum and β1t is empirically set to
0.9 (Ruder, 2016). Despite its intuition in controlling the number of forgotten past gradi-
ents and guarantee in optimal averaging convergence (Yang et al., 2016), how β1t affects
individual convergence has not been discussed (Gitman et al., 2019).
The goal of this paper is to close a theory-practice gap when using HB to train the deep neural
networks as well as optimize the convex objective functions. Specifically,
•	By setting βιt = £, We prove that the adaptive HB attains an individual convergence
rate of O(√) (Theorem 5), as opposed to that of O(l0√t) of SGD. Our proof is different
from all the existing analysis of averaging convergence. It not only provides a theoretical
guarantee for the acceleration ofHB but also clarifies hoW the momentum and its parameter
β1t help us to achieve the optimal individual convergence.
•	If 0 ≤ β1t ≡ β < 1, We prove that the adaptive HB attains optimal averaging convergence
(Theorem 6). To guarantee the optimal individual convergence, Theorem 5 suggests that
time-varying βιt can be adopted. Note βιt = 意 → 1, thus our new convergence analysis
not only offers an interesting explanation Why We usually restrict β1t → 1 but also gives
valuable hints how the momentum parameters should be scheduled in deep learning.
We mainly focus on the proof of individual convergence of HB (Theorem 3, Appendix A.1). The
analysis of averaging convergence (Theorem 4) is simpler. Their extensions to adaptive cases are
slightly more complex (Theorem 5 and 6), but it is similar to the proof of AdaGrad (Mukkamala &
Hein, 2017) and the details can be found in the supplementary material.
2	Problem Statement and Related Work
Consider the following optimization problem,
minf(w), s.t. w ∈ Q.
(1)
where Q ⊆ Rd is a closed convex set and f (W) is a convex function. Denote that W is an optimal
solution and P is the projection operator on Q. Generally, the averaging convergence is defined as
f (Wt) - f(w*) ≤ e(t),
(2)
where Wt = t Pt=1 Wi and e(t) is the convergence bound about t. By contrast, the individual
convergence is described as
f (wt) — f(w*) ≤ e(t).
(3)
Throughout this paper, we use g(Wt) to denote the subgradient of f at Wt. Projected subgradient
descent (PSG) is one of the most fundamental algorithms for solving problem (1) (Dimitri P. et al.,
2003), and the iteration of which is
Wt+1 = P [Wt - αtg(Wt)],
where αt > 0 is the step size. To analyze the convergence, we need the following assumption.
Assumption 1. Assume that there exists a number M > 0 such that
kg(W)k ≤ M, ∀W ∈ Q.
It is known that the optimal bound for the nonsmooth convex problem (1) is O(√1t) (Nemirovsky &
Yudin, 1983). PSG can attain this optimal convergence rate in terms of the averaged output while its
optimal individual rate is only O(l√t) (Harvey et al., 2019; Jain et al., 2019).
t
3
Published as a conference paper at ICLR 2021
When Q = RN, the regular HB for solving the unconstrained problem (1) is
wt+1 = wt - αtg(wt) + βt(wt - wt-1).	(4)
If 0 ≤ βt ≡ β < 1, the key property of HB is that it can be reformulated as (Ghadimi et al., 2015)
wt+1 + Pt+1 = Wt + Pt - ιαtβg(wt), where Pt = I-J(Wt - wt-ι).	(5)
Thus its convergence analysis makes almost no difference to that of PSG. Especially, if at ≡ √T, its
averaging convergence rate is O( √T) (Yang et al., 2016), where T is the total number of iterations.
Simply speaking, the regular Adam (Kingma & Ba, 2014) takes the form of
Wt+1 = Wt - √atVt-1 gt,
where g(wt) is a unbiased estimation of g(wt) and
gt = β1tgt-1 + (1- βιt)g(wt), Vt = β2tVt-1 + (1- β2t)diag [g(wt)g(wt)>] .	(6)
3	Individual Convergence of HB
To solve the constrained problem (1), HB can be naturally reformulated as
wt+1 = PQ [wt - αtg(wt) + βt(wt - wt-1)].	(7)
We first prove a key lemma, which extends (5) to the constrained and time-varying cases.
Lemma 1. (Dimitri P. et al., 2003) For w ∈ Rd, w0 ∈ Q,
hw - w0 , u - w0 i ≤ 0,
for all u ∈ Q if and only if w0 = P (w).
Lemma 2. Let {wt}t∞=1 be generated by HB (7). Let
tα
pt =t(Wt-Wt-1),βt = G, αt = (tΓ2√T
Then HB (7) can be reformulated as
α
Wt+1 + Pt+1 = PQwt + Pt ——广 g(wt)].	(8)
t
Proof. The projection operation can be rewritten as an optimization problem (Duchi, 2018), i.e.,
wt+1 = PQ[wt -αtg(wt) + βt(wt - wt-1)] is equivalent to
wt+ι = arg min{athg(wt), Wi + 1∣∣w - Wt - βt(wt - Wt-I)Il2}.	(9)
Then, ∀w ∈ Q, we have
hwt+1 - wt - βt(wt - wt-1) + αtg(wt), wt+1 - wi ≤ 0.
This is
α
hwt+ι + Pt+1 - (wt + Pt) + 7g(wt), wt+1 - wi≤ 0.	(10)
t
Specifically,
α
hwt+1 + Pt+1 - (wt + Pt) + Fg(Wt), wt+1 - Wti ≤ O.	(11)
t
From (10) and (11),
α
hwt+1 + Pt+1 - (wt + Pt) + √tg(wt), Wt+1 - W + (t + 1)(wt+1 - Wt)i ≤ 0.
i.e.,
α
hwt+1 + Pt+1 - (wt + Pt) + Fg(Wt), Wt+1 + Pt+1 - wi≤ 0.
t
Using Lemma 1, Lemma 2 is proved.
Due to the non-expansive property of PQ (Dimitri P. et al., 2003), Lemma 2 implies that the conver-
gence analysis for unconstrained problems can be applied to analyze the constrained problems.
4
Published as a conference paper at ICLR 2021
Theorem 3. Assume that Q is bounded. Let {wt}t∞=1 be generated by HB (7). Set
βt = t⅛ and αt = (t+¾√T
Then
f(Wt) - f (w*) ≤ o(√t)
Proof. According to Lemma 2,
∣∣W* - (Wt+1 + Pt+1)k2 ≤ ∣∣w* - (wt + Pt) + √g(wt)k2.
Ilw* - (Wt + Pt) + √g(Wt)k2
=llw* - (Wt + Pt)k2 + k√tg(Wt)k2 + 2
Note
Wt), Wt-1 - Wti
Wt) w* - Wti +2
hg(wt), w*	- wti	≤ f(w*) -	f(wt),	hg(wt), wt-1	- wti	≤	f(wt-1)	- f(wt).
Then
(t +1)[f (wt) - f (w*)]
≤ t[f (Wt-I) - f (W*)] + y-kw* - (wt + Pt)Il2 - y-kw* - (wt+ι + pt+ι)k2 + ^√kg(Wt)Il2.
2α	2α	2 t
Summing this inequality from k = 1 to t, we obtain
(t+ 1)[f(wt) -f(w*)]
t	t √k
≤ f(WO) - f(W*) +E √kft kg(Wk )k2 + ΣS [ 2ɑ (kw* - (Wk + Pk )k2 -|w* - (Wk+1 + pk + l)k2)]∙
k=1 2 k	k=1 α
Note
t 1
^2√k 恒(Wk )k2 ≤ √tM2.
and
，√k
E [^2^(kw* - (Wk + Pk)|2 - llw* - (Wk+ 1 + Pk+1)∣2)] ∙
k=1
≤ 2 llw* - (WI + Pl)k2 - ɪ llw* - (Wt+1 + Pt+1)∣2 + X(¥ - "2 1 )kw* - (Wk + Pk )k2.
k=2
Since Q is a bounded set, there exists a positive number M0 > 0 such that
lw* - (wt+1 + Pt+1)l2 ≤ M0,∀t ≥ 0.
Therefore
(t + 1)[f(wt) - f(w*)] ≤ f (wo) - f (w*) + α√tM2 + WM0.
2α
This completes the proof of Theorem 3.
It is necessary to give some remarks about Theorem 3.
•	In nonsmooth convex cases, Theorem 3 shows that the individual convergence rate of SGD
can be accelerated from O(l√gtt) to O( √) via the HB momentum. The proof here clarifies
how the HB-type momentum wt - wt-1 and its time-varying weight βt help us to derive
the optimal individual convergence.
5
Published as a conference paper at ICLR 2021
•	The convergence analysis in Theorem 3 is obviously different from the regret analysis
in all the available papers, this is because the connection between f (Wt) - f (w*) and
f (Wt-I) — f (w*) should be established here. It can be seen that seeking an optimal
individual convergence is more difficult than the analysis of averaging convergence in many
papers such as (Zinkevich, 2003) and (Yang et al., 2016).
•	We can get a stochastic HB by replacing the subgradient g(wt) in (7) with its unbiased
estimation g(wt). Such substitution will not influence our convergence analysis. This
means that We can get E[f (Wt) - f (w*)] ≤ O(√1t) under the same assumptions.
If βt remains a constant, we can get the averaging convergence rate, in which the proof of the first
part is similar to Lemma 2 and that of the second part is similar to online PSG (Zinkevich, 2003).
Theorem 4.	Assume that Q is bounded and 0 ≤ βt ≡ β < 1. Let {wt}t∞=1 be generated by HB (7).
Set
Pt = 1—J (Wt - wt-i) and at = √
Then we have
wt+1 + pt+1 = PQ [wt + pt -
αt
g(Wt)], f (t X Wk) - f (W) ≤ O(√t)
1 - β
If Q is not bounded, the boundness of sequence ∣∣w* - (wt+ι + Pt+ι)k can not be ensured,
which may lead to the failure of Theorem 4. Fortunately, like that in (Yang et al., 2016),
E[f(T PT=I Wk) - f (w*)] ≤ O(√T) still holds, but we need to set a ≡ √.
4 Extension to Adaptive Cases
It is easy to find that HB (8) is in fact a gradient-based algorithm with predetermined step size √t.
Thus its adaptive variant with EMA can be naturally formulated as
wt+ι = PQ[wt - αβ1tVt-1 g(wt) + βιt(wt - wt-ι)].
tt
(12)
where
βιt =厂£, Vt = β2tVt-1 + (1 - β2t)diag [g(wt)g(wt)>].
The detailed steps of the adaptive HB are shown in Algorithm 1.
Algorithm 1 Adaptive HB
Input: momentum parameters β1t, β2t, constant δ > 0, the total number of iterations T
1:
2:
3:
4:
5:
6:
Initialize w0 = 0, V0 = 0d×d
repeat
gt (wt) = Vft(Wt),
Vt = β2tVt-1 + (1- β2t)diag(gt(wt)gt(wt)>),
Vt = VtI + 卡 Id，
Wt+1 = PQ[wt - α√tVt 1g(wt) + βιt(wt - Wt-ι)],
7:	until t = T
Output: wT
Theorem 5.	Assume that Q is a bounded set. Let {wt}t∞=1 be generated by the adaptive HB (Algo-
rithm 1). Denote Pt = t(wt — Wt-ι). Suppose that βιt = t++2 and 1 一 1 ≤ β2t ≤ 1 一 γγ for some
0<γ ≤ 1. Then
Wt+1 + Pt+1 = PQ[wt + Pt ——元 Vt g(wt)]	(13)
E[f (wt) - f (w*)] ≤
O(左).
6
Published as a conference paper at ICLR 2021
The proof of (13) is identical to that of Lemma 2. It is easy to find that (13) is an adaptive variant
of (8). This implies that the proof of the second part is similar to that of AdaGrad (Mukkamala &
Hein, 2017). When 0 ≤ β1t ≡ β < 1, the adaptive variant of HB (7) is
wt+ι = Pq[wt - √αtVt-1 g(wt) + β(wt - wt-ι)].
(14)
where
Vt = β2tVt-1 + (1 - β2t)diag(g(wt)g(wt)>).
Similar to the proof of Theorem 5, we can get the following averaging convergence.
Theorem 6.	Assume that Q is bounded and 0 ≤ β1t ≡ β < 1 in Algorithm 1. Let {wt}t∞=1 be
generated by the adaptive HB (Algorithm 1). Suppose that 1 一 1 ≤ β2t ≤ 1 一 t for some 0<γ ≤ 1.
Denote Pt = ɪ-e (Wt — wt-ι). Then
wt+1 + pt+1 = PQ [wt + pt -
α	1	1t	1
a-β)√tVt	g(wt)], E[f(t Xwk)-f(w)] ≤ O√t)
It is necessary to give some remarks about Theorem 5 and Theorem 6.
•	The adaptive HB is usually used with a constant β1t in deep learning. However, accord-
ing to Theorem 6, the constant β1t only guarantees the optimal data-dependent averaging
convergence. The convergence property of the last iterate still remains unknown.
•	In order to assure the optimal individual convergence, according to Theorem 5, β1t has to
be time-varying. βιt = £ can explain Why We usually restrict βιt → 1 in practice. It
also offers a new schedule about the selection of momentum parameters in deep learning.
5	Experiments
In this section, We present some empirical results. The first tWo experiments are to validate the
correctness of our convergence analysis and investigate the performance of the suggested parameters
schedule. For fair comparison, We independently repeat the experiments five times and report the
averaged results. The last experiment (Appendix A.4) is to shoW the effective acceleration of HB
over GD in terms of the individual convergence.
5.1	Experiments on Optimizing General Convex Functions
This experiment is to optimize hinge loss With the l1-ball constraints. Let τ denotes the radius of the
l1-ball. For implementation of the l1 projection operation, We use SLEP package1.
min f (w), s.t. w ∈ {w : kwk1 ≤ τ}.	(15)
Datasets: A9a, W8a, Covtype, Ijcnn1, Rcv1, Realsim (available at LibSVM2 Website).
Algorithms: PSG (αt = √), HB (αt = (+√t, βt =意),NAG (Tao et al., 2020a) and adaptive
HB (12)(βιt = S).
The relative function value f (Wt) - f (w*) v.s. epoch is illustrated in Figure 1. As expected, the
individual convergence of the adaptive HB has almost the same behavior as the averaging output of
PSG, and the individual output ofHB and NAG. Since the three stochastic methods have the optimal
convergence for general convex problems, We conclude that the stochastic adaptive HB attains the
optimal individual convergence.
1http://yelabs.net/software/SLEP/
2http://www.csie.ntu.edu.tw/~cjlin∕libsvmtools∕datasets∕
7
Published as a conference paper at ICLR 2021
REEE□ B9->> EBRgnW
(a) Covtype (τ = 50)
REEE□ B9->> ERSe gow
(b) Realsim (τ = 60)
NAG
REEE□ B9->> EBRgnW
(c) A9a (τ = 20)
E-EfQ s->> ss>-£
(d) W8a (τ = 30)
E-EfQ s->> B_KE£ ss>-£
(e) Ijcnn1 (τ = 10)
Figure 1: Convergence on different LibSVM datasets
E-EfQ s->> B>s>-5-
(f) Rcv1 (τ = 80)
⅛
5.2	Training Deep Neural Networks
These experiments are conducted on 4-layer CNN and ResNet-18 using a server with 2 NVIDIA
2080Ti GPUs.
Datasets: MNIST (60000 training samples, 10000 test samples), CIFAR10 (50000 training samples,
10000 test samples), and CIFAR100 (50000 training samples, 10000 test samples).
Algorithms: Adam (α,	β1t	≡	0.9,	β2t	≡ 0.999,	=	10-8)	(Kingma & Ba, 2014), SGD (αt ≡	α),
SGD-momentum (αt ≡ α, βt ≡ 0.9), AdaGrad (αt ≡ α) (Duchi et al., 2011), RMSprop (αt ≡ α,
β2t ≡ 0.9, = 10-8) (Tieleman & Hinton, 2012). For our adaptive HB, γ = 0.1 and δ = 10-8.
Different from the existing methods, We set βιt = £ and β2t = 1 - Y in Algorithm 1. Within
each epoch, β1t and β2t remain unchanged.
Note that all methods have only one adjustable parameter α, We choose α from the set of {0.1, 0.01,
0.001, 0.0001} for all experiments. FolloWing (Mukkamala & Hein, 2017) and (Wang et al., 2020),
We design a simple 4-layer CNN architecture that consists tWo convolutional layers (32 filters of
size 3 × 3), one max-pooling layer (2 × 2 WindoW and 0.25 dropout) and one fully connected layer
(128 hidden units and 0.5 dropout). We also use Weight decay and batch normalization to reduce
over-fitting. The optimal rate is alWays chosen for each algorithm separately so that one achieves
either best training objective or best test performance after a fixed number of epochs.
The loss function is the cross-entropy. The training loss results are illustrated in Figure 2 and 4,
and the test accuracy results are presented in Figure 3 and 5. As can be seen, the adaptive HB
achieves the improved training loss. Moreover, this improvement also leads to good performance
on test accuracy. The experimental results shoW that our suggested schedule about the momentum
parameters could gain improved practical performance even in deep learning tasks.
6	Conclusion
In this paper, We prove that the adaptive HB method attains an optimal data-dependent individual
convergence rate in the constrained convex cases, Which bridges a theory-practice gap in using
momentum methods to train the deep neural netWorks as Well as optimize the convex functions.
Our new analysis not only clarifies how the HB momentum and its time-varying weight βιt = S
help us to achieve the acceleration but also gives valuable hints hoW its momentum parameters
should be scheduled in deep learning. Empirical results on optimizing convex functions validate the
8
Published as a conference paper at ICLR 2021
Figure 2: Training loss v.s. number of epochs on 4-layer CNN: CIFAR10, CIFAR100, MNIST
Figure 3: Test accuracy v.s. number of epochs on 4-layer CNN: CIFAR10, CIFAR100, MNIST
Figure 4: Training loss v.s. number of epochs on ResNet-18: CIFAR10, CIFAR100, MNIST
Figure 5: Test accuracy v.s. number of epochs on ResNet-18: CIFAR10, CIFAR100, MNIST
correctness of our convergence analysis and several typical deep learning experiments demonstrate
the improved performance of the adaptive HB.
7	Acknowledgements
This work was supported in part by National Natural Science Foundation of China under Grants
(62076252, 61673394, 61976213) and in part by Beijing Advanced Discipline Fund.
9
Published as a conference paper at ICLR 2021
References
Ahmet Alacaoglu, Yura Malitsky, Panayotis Mertikopoulos, and Volkan Cevher. A new regret analy-
sis for adam-type algorithms. In Proceedings of the International Conference on Machine Learn-
ing, 2020.
Xi Chen, Qihang Lin, and Javier Pena. Optimal regularized dual averaging methods for stochastic
optimization. In Advances in Neural Information Processing Systems, pp. 395-403, 2012.
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence ofa class of adam-type
algorithms for non-convex optimization. In International Conference on Learning Representa-
tions, 2019.
Alexandre Defossez, L. Bottou, Francis R. Bach, and Nicolas Usunier. On the convergence of adam
and adagrad. ArXiv, abs/2003.02395, 2020.
Bertsekas Dimitri P., Nedic Angelia., and Ozdaglar Asuman E. Convex analysis and optimization.
Athena Scientific, 2003.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(7), 2011.
John C Duchi. Introductory lectures on stochastic optimization. The mathematics of data, 25:99,
2018.
Euhanna Ghadimi, Hamid Reza Feyzmahdavian, and Mikael Johansson. Global convergence of the
heavy-ball method for convex optimization. In 2015 European Control Conference (ECC), pp.
310-315. IEEE, 2015.
Igor Gitman, Hunter Lang, Pengchuan Zhang, and Lin Xiao. Understanding the role of momentum
in stochastic gradient methods. In Advances in Neural Information Processing Systems, pp. 9630-
9640, 2019.
Nicholas JA Harvey, Christopher Liaw, Yaniv Plan, and Sikander Randhawa. Tight analyses for non-
smooth stochastic gradient descent. In Annual Conference on Learning Theory, pp. 1579-1613,
2019.
Chonghai Hu, Weike Pan, and James T Kwok. Accelerated gradient methods for stochastic optimiza-
tion and online learning. In Advances in Neural Information Processing Systems, pp. 781-789,
2009.
Prateek Jain, Dheeraj Nagaraj, and Praneeth Netrapalli. Making the last iterate of sgd information
theoretically optimal. In Annual Conference on Learning Theory, pp. 1752-1755, 2019.
Diederik P Kingma and Jimmy Ba. The unusual effectiveness of averaging in gan training. In
International Conference on Learning Representations, 2014.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097-1105,
2012.
Chaoyue Liu and Mikhail Belkin. Accelerating sgd with momentum for over-parameterized learn-
ing. In International Conference on Learning Representations, 2020.
Mahesh Chandra Mukkamala and Matthias Hein. Variants of rmsprop and adagrad with logarithmic
regret bounds. In International Conference on Machine Learning, 2017.
Arkadi Semenovich Nemirovsky and David Borisovich Yudin. Problem complexity and method
efficiency in optimization. John Wiley & Sons, 1983.
Yu Nesterov. A method of solving a convex programming problem with convergence rate o(1/k2).
Soviet Mathematics Doklady, 27(2):372-376, 1983.
Peter Ochs, Yunjin Chen, Thomas Brox, and Thomas Pock. ipiano: Inertial proximal algorithm for
nonconvex optimization. SIAM Journal on Imaging Sciences, 7(2):1388-1419, 2014.
10
Published as a conference paper at ICLR 2021
Antonio Orvieto, Jonas Kohler, and A. Lucchi. The role of memory in stochastic optimization.
ArXiv, abs/1907.01678, 2019.
Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR Com-
Putational Mathematics and Mathematical Physics, 4(5):1-17,1964.
Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for
strongly convex stochastic optimization. arXiv preprint arXiv:1109.5647, 2011.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In
International Conference on Learning Representations, 2018.
Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint
arXiv:1609.04747, 2016.
Othmane Sebbouh, Robert Mansel Gower, and Aaron Defazio. On the convergence of the stochastic
heavy ball method. ArXiv, abs/2006.07867, 2020.
Ohad Shamir. Open problem: Is averaging needed for strongly convex stochastic gradient descent?
In Anual Conference on Learning Theory, pp. 1-3, 2012.
Tao Sun, Dongsheng Li, Zhe Quan, Hao Jiang, Shengguo Li, and Yong Dou. Heavy-ball algorithms
always escape saddle points. arXiv preprint arXiv:1907.09697, 2019a.
Tao Sun, Penghang Yin, Dongsheng Li, Chun Huang, L. Guan, and Hao Jiang. Non-ergodic con-
vergence analysis of heavy-ball algorithms. In AAAI, 2019b.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initial-
ization and momentum in deep learning. In International Conference on Machine Learning, pp.
1139-1147, 2013.
Wei Tao, Zhisong Pan, Gaowei Wu, and Qing Tao. The strength of nesterov’s extrapolation in the
individual convergence of nonsmooth optimization. IEEE Transactions on Neural Networks and
Learning Systems, 31(7):2557-2568, 2020a.
Wei Tao, Zhisong Pan, Gaowei Wu, and Qing Tao. Primal averaging: A new gradient evaluation step
to attain the optimal individual convergence. IEEE Transactions on Cybernetics, 50(2):835-845,
2020b.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop, coursera: Neural networks for machine
learning. University of Toronto, Technical Report, 2012.
Guanghui Wang, Shiyin Lu, Weiwei Tu, and Lijun Zhang. Sadam: A variant of adam for strongly
convex functions. In International Conference on Learning Representations, 2020.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In Advances in neural information pro-
cessing systems, pp. 4148-4158, 2017.
Tianbao Yang, Qihang Lin, and Zhe Li. Unified convergence analysis of stochastic momentum
methods for convex and non-convex optimization. arXiv preprint arXiv:1604.03257, 2016.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In
Proceedings of the International Conference on Machine Learning, pp. 928-936, 2003.
Fangyu Zou, Li Shen, Zequn Jie, Ju Sun, and Wei Liu. Weighted adagrad with unified momentum.
arXiv preprint arXiv:1808.03408, 2018.
11
Published as a conference paper at ICLR 2021
A Supplementary Material
A.1 PROOF FOR THEOREM 4
Let {wt}∞=ι be generated by HB (7). Set
Pt = -β-z(Wt - Wt-1) and α = ʌ.
1 - β	√t
Then, ∀w ∈ Q, according to Lemma 1, We have
hwt+1 - Wt - β(wt - Wt-1) + αtg(wt), Wt+1 - wi≤ 0.
This is
,	1	,	、	Qt /	、	∖
h 1-β(wt+1 - Wt) - Pt + γ-βg(wt), Wt+1 - wi≤ 0.
i.e.,
Qt
〈Wt+1 + Pt+1 - (wt + Pt) + -——Wg(Wt) Wt+1 - wi≤ 0	(16)
1-β
Specifically,
〈Wt+1 + Pt+1 - (wt + Pt) + 7Qt万g(wt), β(wt+1 R Wt)〉≤ 0	(17)
1-β	1-β
From (16) and (17),
Qt
〈Wt+1 + Pt+1 - (Wt + Pt) + -——^g(Wt), Wt+1 + Pt+1 - w〉≤ 0.
1-β
Using Lemma 1, we have
Qt
Wt+1 + Pt+1 = PQ[wt + Pt - 1--βg(wt)].
Then
I∣w* - (wt+1 + Pt+1)ll2
≤∣∣w* - (wt + Pt) + -αt-zg(wt)k2
1-β
= ∣∣w* - (Wt + Pt)∣∣2 + Il-αt-zg(wt)∣∣2 +2〈-αt-zg(wt),w* - wti
1-β	1-β
+2〈叫∖2 g(wt), wt-1 - wti
(1 - β)
Note
Then
〈g(wt), W* - Wt〉≤ f (w*) - f (wt), 〈g(wt), Wt-1 - Wt〉 ≤ f (wt-1) - f (wt).
Ilw* - (wt+1 + Pt+1)∣∣2
≤kw* - (Wt + Pt)Il2 +QtA、2 Ilg(Wt)k2
(1 - β)
+ J [f(w*)- f (Wt)] +
1-β
(i2Ztβ)2 [f (Wt-I) - f (Wt)]∙
Rearrange the inequality, we have
τ2α⅛[f(Wt) - f(w*)] ≤∕Qtβ2 [f(wt-I) - f(wt)] + ∣∣w* - (Wt+ Pt)II2
1 - β	(1 - β)
-∣∣W* - (Wt+1 + Pt+1)∣∣2 + ∩-tm2 Ig(Wt)H2.
(1 - β)
12
Published as a conference paper at ICLR 2021
i.e.,
f (wt) - f (w*) ≤ τjβ-∑ [f (Wt-I) - f (wt)] + 1-β [kw* - (Wt + pt)k2
1 - β	2αt
-kw* - (Wt+1 + pt+1)k2] + 2(1 - β) kg(Wt)k2.
Summing this inequality from k = 1 to t, we obtain
t
X[f(Wk) - f(w*)]
k=1
≤ι-ββ[f (w0) - f (wt)l + 12ατkW* - (WI + Pι)k2
-∖αB ∣∣w* -(Wt+1 + pt+ι)k2 + X 2(1C- β) kg(wk)k2
+ X kw* - (Wk + Pk)k2(m - L).
k=2	2αk	2αk-1
i.e.,
∑[f(Wk) - f(w*)]
k=1
≤ι-β[f(WO) - f (wt)] + 1-Cβkw* - (WI + Pι)k2
+ X kw* - (Wk + Pk)k2((1⅜β^k - (I-"7)
2α	2α
k=2
t
+ X % 1 Qm 々kg(Wk)k2.
k=1 2(1 - β) k
Note
t 1
E τ√=kg(wk)k2 ≤ √tM2.
k=1 2 k
and since Q is a bounded set, there exists a positive number M0 > 0 such that
kw* - (wt+ι + pt+ι)k2 ≤ Mo,∀t ≥ 0.
From (18)(19)(20) we have,
t
X[f (Wk) - f(w*)] ≤
k=1
Iee [f(w0) - f (wt)] +
(1 - β)√tM0 , a√tM2
2C	+ 1 - β
(18)
(19)
(20)
By convexity of f (w), we obtain
1t
f(t EWk) - f(w*) ≤
k=1
β 「“	、	”	(1- β)Mo , αM2
(TF[f (WO) - f (Wt)]+	2α√t + (1 - β)√t.
This completes the proof of Theorem 4.
A.2 Proof for Theorem 5
Notation. For a positive definite matrix H ∈ Rd×d, the weighted '2-norm is defined by kxkH =
x>Hx. The H-weighted projection PQH (x) of x onto Q is defined by PQH (x) = arg miny∈Q ky -
XkH. We use g(wk) to denote the subgradient of fk(∙) at Wk. For the diagonal matrix sequence
{Mk }tk=1 , we use mk,i to denote the i-th element in the diagonal ofMk. We introduce the notation,
g1:k,i = (g1,i, g2,i, .., gk,i)>, where gk,i is the i-th elementofg(wk).
13
Published as a conference paper at ICLR 2021
Lemma 7. (Mukkamala & Hein, 2017) Suppose that 1 一 t ≤ β2t ≤ 1 一 + for some 0<γ ≤ 1, and
t ≥ 1, then
dt
XX
i=1 k=1
≤ XX 2(2L) IE+δ).
i=1 γ
g2,i
∙√kvk,i + δ
Proof for Theorem 5. Without loss of generality, we only prove Theorem 5 in the full gradient
setting. It can be extended to stochastic cases using the regular technique in (Rakhlin et al., 2011).
Note that the projection operation can be rewritten as an optimization problem (Duchi, 2018), i.e.,
1
wt+1 = PQ[wt 一 αtVt-1g(wt)+ β1t(wt 一 wt-1)] is equivalent to
Wt+1 = arg min{αth匕Tg(Wt), Wi + 1∣∣w — Wt — βιt(wt — Wt-I)Il2}.	(21)
Then, ∀u ∈ Q, we have
hwt+1 — Wt — βt(wt — wt-i) + αtVt-1g(wt), wt+1 一 w≤ 0.
This is	α
hwt+1 + pt+ι — (wt + Pt) + 7%Tg(Wt), wt+1 — wi≤ 0.	(22)
t
Specifically,
hwt+1 + Pt+1 — (wt + Pt) + -αV∕t-1g(wt), wt+1 — Wti ≤ O.	(23)
t
From (22) and (23),
hwt+1 + pt+1 — (wt + Pt) + -αV∕t-1g(wt), wt+1 — Wt + (t + 1)(wt+1 — wt)i ≤ O.
t
i.e.,
hwt+1 + Pt+1 — (wt + Pt) + 7Vt-1g(wt), Wt+1 + Pt+1 — Wti ≤ O.
t
Using Lemma 1, we have
Wt+1 + Pt+1 = PQ [wt + Pt - -OtVtTg(Wt)].
Then
I∣w* — (Wt+1+ Pt+1)∣Vt ≤ l∣w* — (wt+ Pt) + √tvt-1g(Wt)IIVt
=l∣w* — (Wt+ Pt)I 岛 + ll√αt g(Wt)I 岛
+ 2h-αg(wt), w* — Wti + 2h-αtg(wt), Wt-1 — Wti.
tt
Note
hg(wt), w* — wti ≤ f(w*) — f(wt), hg(wt), wt-1 — wti ≤ f(wt-1) — f(wt).
Then
(t + 1)[f (Wt) — f (w*)] ≤tf (Wt-I) — f(w*)] + Skw* — (wt + Pt)IIV
2α	Vt
—20 ι∣w*— (Wt+1+Pt+I)IIVt+ 2√t 恒(Wt)nV-1.
Summing this inequality from k= 1 to t, we obtain
t
(t +1)[f (Wt)- f(W*)] ≤ f (WO)- f (W*) + X τ√=Ig(wk)IIV-1
k=1 2 k	Vk
，√k
+ ΣS [ ^2O (Iw* — (Wk + Pk )"Vk — Iw* — (Wk+1 + Pk + 1)l∣Vk )] ∙
k=1
14
Published as a conference paper at ICLR 2021
Using Lemma 7, we have
t
X 熹 kg(Wk )kVk-ι ≤ X
i=1
Note
二 √k
X [石(kw* -(Wk + Pk )kVk
k=1
d
-llw* - (Wk+1 + Pk+1 )kVk)]
X V1αi (w； -(W1,i + p1,i))2- X
i=1
i=1
①;;(W； - (Wt+1,i + pt+1,i))2
(24)
d
d
: G+δ).
d t ]
+XX 2^(√kVk,i - √k - 1Vk-i,i)(wt - (wk,i + Pk,i))2.
i=1 k=2 α
Since Q is a bounded set, there exists a positive number M1 > 0 such that
and vk,i = β2kvk
(w* - (wt+i,i + Pt+i,i))2 ≤ Mi,∀t ≥ 0,i = 1, 2,...,d.
-i,i + (1 - β2k)g2,i as well as β2k ≥ 1 — 1 which implies kβ2k ≥ k — 1, we get
√kVk,i = VzkvkJ + δ
=	kβ2kvk-1,i + k(1 - β2k )gk2,i + δ
≥ J(k - 1)vk-ι,i + δ
=VZk - 1 Vk-ι,i.
Therefore
J √k
E [ 2α (kw* - (Wk + Pk )kVk - kw* - (Wk+1 + Pk + 1)k2^k )]
k=1
d	dt
≤ X Va Mi+ XX
2α (√kvk,i - √k - 1Vk-1,i)MI
i=1
X Vi,iMi
2a	2a
i=1
i=1 k=2
d
+X
i=1
√⅞t,iMι
2α
X Vi,iMi
2	2α
i=1
(25)
—
=21 X(Ptv,i+δ).
i=1
Since √tvt~i = kgi：t,ik，therefore
(t +1)[f (Wt)- f(w*)l ≤f(w0) - f (w*) + M X(Pv7
i=1
+ δ)+ X α2-^(k + δ)
i=1 γ
=f (WO) - f (W*) + (ʒɑɪ +	Y)) X(IlgI:t,ik + δ)
2α	γ i=1
This proves
f(wt) - f(w*) ≤
A.3 Proof for Theorem 6
Let {wt}t∞=1 be generated by the adaptive HB (Algorithm 1). Set
Pt = γ-β-z(wt - Wt-1) and 3 = -α.
1-β	t
15
Published as a conference paper at ICLR 2021
Then, ∀u ∈ Q, according to Lemma 1, We have
hwt+1 - Wt - β(wt - Wt-1) + αt%-1g(wt), Wt+1 - wi≤ 0.
This is
/ 1 /	、	, Qt 匕 / 、	∖ / c
h 1-β (wt+ι - Wt) - pt + ι - β g(wt), wt+ι - w>≤ 0.
i.e.,
一 TV-1
hwt+1 + Pt+1 - (Wt + Pt) + t t g(wt), wt+1 - w〉≤ 0	(26)
1 - P
Specifically,
-1
〈Wt+1 + Pt+1 - (wt + Pt) + t ∖j g(wt),-------t+1-z_-〉≤ 0	(27)
1 — p	1 — p
From (26) and (27),
-1
/	.	/	,	， Qt Vt / 、	,	∖ / c
(Wt+1 + Pt+1 - (Wt + Pt) + -——万-g(Wt), Wt+1 + Pt+1 - W〉≤ 0.
1 - P
Using Lemma 1, We have
nV「	QtDT
Wt+1 + Pt+1 = PQ [wt + Pt - 1 -p g(wt)].
According to Lemma 2,
Ilw* -(Wt+1 + Pt+1)∣良
^-1-1
≤∣∣w* - (Wt + Pt) +	t g(wt)略
1 — p	t
=∣∣w* - (Wt + Pt)略 + Il丁%g(wt)略
t 1 — p	t
+2〈Γq⅛g(wt), w* - wt〉+ 2〈 a QtP、2 g(wt), Wt-1 - W
1 - P	(1 - P)
Note
	〈g(wt), W* - Wti ≤ f (w*) - f (wt), hg(wt), Wt-1 - Wti ≤ f (wt-1) - f (wt).
Then	Ilw* - (Wt+1 + Pt+1)∣E ≤∣∣w* - (Wt + Pt)∣∣^ + 门(QtR、2 恒(Wt)kV-1 t (1 - P)	t + 12Qtp [f (W*) - f (Wt)] + (12QtP)2 [f (Wt-I) - f (Wt)].
Rearrange the inequality, We have
i.e.,	t2q⅛ [f (Wt) - f (w* )] 1-β ≤ ：(QP2 [f(wt-I) - f(wt)] + llw* - (Wt+ Pt)略 (1 - P) t *	2	Qt2	2 -∣∣w - (Wt+1 + pt+1)∣∣必 + (1 - p)2 Ilg(Wt)IlV-ι. f (wt) - f (w*) ≤ι B	[f(Wt-I) - f(wt)] + s' '[∣lw* - (wt+ Pt)I岛 -	Qt -∣∣w* - (Wt+1+ pt+1) I岛]+ 2门;R、Ilg(Wt)IIV-ι. 2(I - P)	t
16
Published as a conference paper at ICLR 2021
Summing this inequality from k = 1 to t, we obtain
t
X[f(wk) - f(w*)]
k=1
≤J [f (WO) - f (Wt)] + ∖ 'kw* - (WI + p1 )kV
-	α1
-j-αβ~ kw* - (wt+i + pt+1 )kVt + X 2(1αk β) kg(wk)kV-1
dt
+ X X (wi* - (wk,i + pk,i))2 (
i=1 k=2
(I - βyvk-∖,i)
20,k-i
(I - βyvk,j
2αk
i.e.,
t
X[f(wk) - f(w*)]
k=1
≤ 1 - β [f (WO) - f (Wt)] + ~2O^kw* - (WI + p1)kVι
d t	―	(28)
+ ££(w* — (wk,i + Pk,i))2-ɪ-(√kvk,i — √k - 1 Vk-i,i)
i=1 k=2	α
t
+ X 2(T-¾)√kkg(Wk )kVk-1.
Using Lemma 7, we have
∑α	ao	α(2 — Y)	α(2 — Y)
-2√k(1 - β) kg(Wk)kVτ ≤ X γ(1- β) (Ptv^ + δ)= γ(1 - β) X(kg1:t,ik + δ). (29)
k=1	i=1	i=1
and since Q is a bounded set, there exists a positive number MO > 0 such that
kw* - (wt+1 + pt+1)k2 ≤ MO,∀t ≥ 0.	(30)
From (28)(29)(30) we have,
t
X[f(wk) - f(w*)]
k=1
β 「乙 、 乙 、]	∖d^ (1 — β)ViiMo	a(2 — γ) xd^
≤γ-β[f(wo) - f(wt)] + Σ	2Oj — + Y(T-历〉^(kg1：t,ik + δ)
d
+X
i=1
(1 - β)vt,i√tM0
2ɑ
d
-X
i=1
(1 一 β)Vι,iMo
2ɑ
i.e.,
t
X[f(wk) - f(w*)]
k=1
≤ E [f(wo)—f(wt)]+α(f÷γ)
X(IlgI=t,i∣∣+δ)+Q ；]M(O X(kgLt,ik+δ).
i=1	α	i=1
By convexity of f(wk), we obtain
L β [门、a ∖1∣ α(2 - Y) y^^∕∣∣ H λx (1 - β)M0 ∖"^∕∣∣ Il . λ∖
≤ (1 - β)t [f (wθ)-f (wt)] + γ(1 - β)t ɪʧ(kgl:t,ik+*)+	2αt---->j(kgLt,ik+δ).
This completes the proof of Theorem 6.
17
Published as a conference paper at ICLR 2021
A.4 Experiments on Optimizing a Synthetic Convex Function
A constrained convex optimization problem was constructed in (Harvey et al., 2019) to show that
the optimal individual convergence rate of SGD is O( l√ggtt). We will use example to illustrate the
acceleration of HB.
Let Q be unit ball in RT . For i ∈ [T] and c ≥ 1, define the positive scalar parameters
ai
1
8c(T - i +1)
√
2c√T
Define f : Q → R and hi ∈ RT for i ∈ [T + 1] by
f(w) = max hi>w	where
aj,
hi,j =	-bj,
I 0,
1≤j<i
i=j<T
i<j≤T
Figure 6: Convergence of the function value when T = 1000 and T = 5000
Obviously, the minimum value of f on the unit ball is non-positive because f (0) = 0. It can be
proved f (WT) ≥ 3og√τ. Set C = 2, the function value f (Wt) v.s. iteration is illustrated in Figure 6,
where the step size of GD is √ct and the parameters of the constrained HB (7) (α = 8) and AdaHB
(12) (α = 0.08, γ = 0.9, δ = 10-8) are selected according to Theorem 3 and Theorem 5. As
expected, the individual convergence of HB is much faster than that of PSG. We thus conclude that
HB is an effective acceleration of GD in terms of the individual convergence.
18