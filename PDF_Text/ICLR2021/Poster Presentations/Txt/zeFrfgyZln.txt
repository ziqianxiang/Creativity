Published as a conference paper at ICLR 2021
Approximate Nearest Neighbor Negative Con-
trastive Learning for Dense Text Retrieval
Lee Xiong； Chenyan Xiong； Ye Li, KWok-Fung Tang, Jialin Liu,
Paul Bennett, Junaid Ahmed, Arnold Overwijk
Microsoft Corporation.
lexion, chenyan.xiong, yeli1, kwokfung.tang, jialliu,
paul.n.bennett, jahmed, arnold.overwijk@microsoft.com
Ab stract
Conducting text retrieval in a learned dense representation space has many intrigu-
ing advantages. Yet dense retrieval (DR) often underperforms word-based sparse
retrieval. In this paper, we first theoretically show the bottleneck of dense retrieval
is the domination of uninformative negatives sampled in mini-batch training, which
yield diminishing gradient norms, large gradient variances, and slow convergence.
We then propose Approximate nearest neighbor Negative Contrastive Learning
(ANCE), which selects hard training negatives globally from the entire corpus. Our
experiments demonstrate the effectiveness of ANCE on web search, question an-
swering, and in a commercial search engine, showing ANCE dot-product retrieval
nearly matches the accuracy of BERT-based cascade IR pipeline. We also empiri-
cally validate our theory that negative sampling with ANCE better approximates
the oracle importance sampling procedure and improves learning convergence.
1	Introduction
Many language systems rely on text retrieval as their first step to find relevant information. For
example, search ranking (Nogueira & Cho, 2019), open domain question answering (OpenQA) (Chen
et al., 2017), and fact verification (Thorne et al., 2018) all first retrieve relevant documents for their
later stage reranking, machine reading, and reasoning models. All these later-stage models enjoy the
advancements of deep learning techniques (Rajpurkar et al., 2016; Wang et al., 2019), while, the first
stage retrieval still mainly relies on matching discrete bag-of-words, e.g., BM25, which has become
the pain point of many systems (Nogueira & Cho, 2019; Luan et al., 2020; Zhao et al., 2020).
Dense Retrieval (DR) aims to overcome the sparse retrieval bottleneck by matching in a continuous
representation space learned via neural networks (Lee et al., 2019; Karpukhin et al., 2020; Luan
et al., 2020). It has many desired properties: fully learnable representation, easy integration with
pretraining, and efficiency support from approximate nearest neighbor (ANN) search (Johnson et al.,
2017). These grant dense retrieval an intriguing potential to fundamentally overcome some intrinsic
limitations of sparse retrieval, for example, vocabulary mismatch (Croft et al., 2009).
One challenge in dense retrieval is to construct proper negative instances when learning the represen-
tation space (Karpukhin et al., 2020). Unlike in reranking (Liu, 2009) where the training and testing
negatives are both irrelevant documents from previous retrieval stages, in first stage retrieval, DR
models need to distinguish all irrelevant ones in a corpus with millions or billions of documents. As
illustrated in Fig. 1, these negatives are quite different from those retrieved by sparse models.
Recent research explored various ways to construct negative training instances for dense re-
trieval (Karpukhin et al., 2020), e.g., using contrastive learning (Oord et al., 2018; He et al., 2020;
Chen et al., 2020a) to select hard negatives in current or recent mini-batches. However, as observed in
recent research (Karpukhin et al., 2020), the in-batch local negatives, though effective in learning word
or visual representations, are not significantly better than spare-retrieved negatives in representation
learning for dense retrieval. In addition, the accuracy of dense retrieval models often underperform
BM25, especially on documents (Gao et al., 2020b; Luan et al., 2020).
*Lee and Chenyan contributed equally.
1
Published as a conference paper at ICLR 2021
In this paper, we first theoretically analyze the convergence
of dense retrieval training with negative sampling. Us-
ing the variance reduction framework (Alain et al., 2015;
Katharopoulos & Fleuret, 2018), we show that, under con-
ditions commonly met in dense retrieval, local in-batch
negatives lead to diminishing gradient norms, resulted in
high stochastic gradient variances and slow training con-
vergence — the local negative sampling is the bottleneck
of dense retrieval’s effectiveness.
Based on our analysis, we propose Approximate near-
est neighbor Negative Contrastive Estimation (ANCE),
a new contrastive representation learning mechanism for
dense retrieval. Instead of random or in-batch local neg-
atives, ANCE constructs global negatives using the being-
optimized DR model to retrieve from the entire corpus.
This fundamentally aligns the distribution of negative sam-
ples in training and of irrelevant documents to separate
in testing. From the variance reduction perspective, these
★ Query
+ Relevant
“	▼ DRNeg
v +fX ▼	.	< BM25 Neg
V V ★ +	∙ Rand Neg
Figure 1: T-SNE (Maaten & Hinton,
2008) representations of query, relevant
documents, negative training instances
from BM25 (BM25 Neg) or randomly
sampled (Rand Neg), and testing nega-
tives (DR Neg) in dense retrieval.
ANCE negatives lift the upper bound of per instance gradient norm, reduce the variance of the
stochastic gradient estimation, and lead to faster learning convergence.
We implement ANCE using an asynchronously updated ANN index of the corpus representation.
Similar to Guu et al. (2020), we maintain an Inferencer that parallelly computes the document
encodings with a recent checkpoint from the being optimized DR model, and refresh the ANN index
used for negative sampling once it finishes, to keep up with the model training. Our experiments
demonstrate the advantage of ANCE in three text retrieval scenarios: standard web search (Craswell
et al., 2020), OpenQA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019), and in a commercial search
engine’s retrieval system. We also empirically validate our theory that the gradient norms on ANCE
sampled negatives are much bigger than local negatives, thus improving the convergence of dense
retrieval models.1
2	Preliminaries
In this section, we discuss the preliminaries of dense retrieval and its representation learning.
Task Definition: Given a query q and a corpus C, the first stage retrieval is to find a set of documents
relevant to the query D+ = {d1, ..., di, ..., dn} from C (|D+ |	|C|), which then serve as input to
later more complex models (Croft et al., 2009). Instead of using sparse term matches and inverted
index, Dense Retrieval calculates the retrieval score f () using similarities in a learned embedding
space (Lee et al., 2019; Luan et al., 2020; Karpukhin et al., 2020):
f(q, d) = sim(g(q; θ), g(d; θ)),	(1)
where g() is the representation model that encodes the query or document to dense embeddings.
The encoder parameter θ provides the main capacity. The similarity function (sim()) is often simply
cosine or dot product to leverage efficient ANN retrieval (Johnson et al., 2017; Guo et al., 2020).
BERT-Siamese Model: A standard instantiation of Eqn. 1 is to use the BERT-Siamese/two-
tower/dual-encoder model (Lee et al., 2019; Karpukhin et al., 2020; Luan et al., 2020):
f(q, d) = BERT(q) ∙ BERT(d) = MLP([CLS]q) ∙ MLP([CL⅛).	(2)
It encodes the query and document separately with BERT as the encoder g(), using their last
layer,s [CLS] token representation, and applied dot product (∙) on them. This enables offline Pre-
computing of the document encodings and efficient first-stage retrieval. In comparison, the BERT
reranker (Nogueira et al., 2019) applies BERT on the concatenation of each to-rerank query-document
pair: BERT(q ◦ d), which has explicit access to term level interactions between query-document with
transformer attentions, but is often infeasible in first stage retrieval as enumerating all documents in
the corpus for each query is too costly.
1Our code and trained models are available at http://aka.ms/ance.
2
Published as a conference paper at ICLR 2021
Learning with Negative Sampling: The effectiveness of DR resides in learning a good representa-
tion space that maps query and relevant documents together, while separating irrelevant ones. The
learning of this representation often follows standard learning to rank (Liu, 2009): Given a query q, a
set of its relevant document D+ and irrelevant ones D-, find the best θ* that:
θ* = argminθ X X X l(f(q,d+), f (q,d-)).	⑶
q d+∈Dq+ d-∈Dq-
The loss l() can be binary cross entropy (BCE), hinge loss, or negative log likelihood (NLL).
A unique challenge in dense retrieval, targeting first stage retrieval, is that the irrelevant documents
to separate are from the entire corpus (Dq- = C \ Dq+). This often leads to millions of negative
instances, which have to be sampled in training:
θ* = argmi□θ X X X l(f(q,d∖,f (q,d-)).	(4)
q d+∈D+ d-∈D-
Here we start to omit the subscript q in Dq. All D+ and D- are query dependent. A natural choice
is to sample negatives D - from top documents retrieved by BM25. However, they may bias the DR
model to merely mimic sparse retrieval (Luan et al., 2020). Another way is to sample negatives in
local mini-batches, e.g., as in contrastive learning (Oord et al., 2018), however, these local negatives
do not significantly outperform BM25 negatives (Karpukhin et al., 2020; Luan et al., 2020).
3	Analyses on The Convergence of Dense Retrieval Training
In this section, we theoretically analyze the convergence of dense retrieval training. We first show
the connections between learning convergence and gradient norms (Sec. 3.1), then we discuss how
non-informative negatives in dense retrieval yield less optimal convergence (Sec. 3.2).
3.1	Oracle Negative Sampling According to Per-Instance Gradient-Norm
Let l(d+, d-) = l(f (q, d+), f(q, d-) be the loss function on the training triple (q, d+, d-), PD- the
negative sampling distribution for the given (q, d+), and pd- the sampling probability of negative
instance d-, a stochastic gradient decent (SGD) step with importance sampling (Alain et al., 2015) is:
θt+ι = θt - ηM----Vθtl(d+, d-),	(5)
Npd- t
with θt the parameter at t-th step, θt+1 the one after, and N the total number of negatives. The scaling
factor Np _ ensures Eqn. 5 is an unbiased estimator of the non-stochastic gradient on the full data.
Then we can characterize the converge rate of this SGD step as the movement to the optimal θ*.
Following derivations in variance reduction (Katharopoulos & Fleuret, 2018; Johnson & Guestrin,
2018), let gd- = NI _ Vθt l(d+,d-) the weighted gradient, the convergence rate is:
pd-
E∆t = I∣θt - θ*∣∣2 - Epd- (∣∣θt+ι - θ*∣∣2)	(6)
=l∣θt∣l2 - 2θTθ* -Epd- (l∣θt - ηgd-Il2) + 2Θ*tEpd- (θt - ηgd-)	⑺
=-η2EpD- (∣∣gd- ∣∣2) + 2ηθTEpd- (gd-) - 2ηθ*TEPD- (g&-)	(8)
=2ηEpD-(gd- )t(θt - θ*) - η2EpD- (∣∣gd- ∣∣2)	(9)
=2ηEpD- (gd- )t(θt - θ*) - η2EpD- (gd- )TEpD- (gd-) - η2Tr(Vpd- (gd-)).	(10)
This shows we can obtain better convergence rate by sampling from a distribution PD- that minimizes
the variance of the stochastic gradient estimator Ep - (||gd- ||2), or Tr(Vp - (gd- )) as the estimator
is unbiased. The variance reflects how good the stochastic gradient from negative sampling represents
the full gradient on all negatives—the latter is ideal but infeasible. Intuitively, we prefer the stochastic
estimator to be stable and have smaller variances.
A well known result in importance sampling (Alain et al., 2015; Johnson & Guestrin, 2018) is that
there exists an optimal distribution that:
Pd- = argminpd-Tr(VPD- (gd-)) H ∣∣VθJ(d+,d-)∣∣2.	(11)
3
Published as a conference paper at ICLR 2021
Figure 2: ANCE Asynchronous Training. The Trainer learns the representation using negatives from
the ANN index. The Inferencer uses a recent checkpoint to update the representation of documents in
the corpus and, once finished, refreshes the ANN index with most up-to-date encodings.
To prove this, one can apply Jensen’s inequality on the gradient variance and verify that Eqn. 11
achieves the minimum. The detailed derivations can be find in Johnson & Guestrin (2018).
Eqn. 11 shows that the convergence rate can be improved by sampling negatives proportional to their
per-instance gradient norms (though too expensive to calculate). Intuitively, an negative instance with
larger gradient norm is more likely to reduce the non-stochastic training loss, thus should be sampled
more frequently than those with diminishing gradients. The correlation of larger gradient norm and
better training convergence is also observed in BERT fine-tuning (Mosbach et al., 2021).
3.2	Uninformative In-Batch Negatives and Their Diminishing Gradients
Diminishing Gradients of Uninformative Negatives: Though the close form of gradient norms
often does not exist, Katharopoulos & Fleuret (2018) derives the following upper bound:
∣∣Vθtl(d+,d-)∣∣2 ≤ Lρ∣∣VφLl(d+,d-)∣∣2,	(12)
where L is the number of layers, ρ is composed by pre-activation weights and gradients in interme-
diate layers, and ||V@ll(d+,d-)∣∣2 is the gradient on the last layer. The derivation of this upper
bound is on multi-layer perception with any depths and any activation function that is Lipschitz
continuous (Katharopoulos & Fleuret, 2018). On complicated neural networks, the intermediate
layers are regulated by various normalization and this upper bound often holds empirically (Sec. 6.3).
In addition, for many loss functions, for example, BCE loss and pairwise hinge loss, we can verify
that when the loss goes to zero the gradient norm of the last layer also goes to zero: l(d+, d-) →
0 ⇒ ∣∣VφLl(d+, d-)∣∣2 → 0 (Katharopoulos &Fleuret, 2018).
Putting everything together, using uninformative negative samples with near zero loss results in the
following chain of undesirable properties:
l∣VφL l(d+,d-川 2 → 0 ⇒ ∣∣Vθt l(d+,d-)∣∣2 → 0 ⇒ Tr(VPD-(gd-)) ↑⇒	E∆t ； .	(13)
{z}	{z}	D{}	^~~{^~}/
low upper bound	diminishing gradient norm large scholastic variance slow convergence
The uninformative negative samples yield diminishing gradient norms, larger variances of the
scholastic gradient estimator, and less optimal learning convergence.
Inefficacy of Local In-Batch Negatives: We argue that, when training DR models, the in-batch
local negatives are unlikely to provide informative samples due to two properties of text retrieval.
Let D- be the set of informative negatives that are hard to distinguish from D+, and b be the batch
size, We have (1) b《|C|, the batch size is far smaller than the corpus size; (2) ∣D-*∣《|C|, that
only a few negatives are informative and the majority of corpus is trivially unrelated.
Both conditions hold in most dense retrieval scenarios. The tWo together make the probability that a
random mini-batch includes meaningful negatives (p = ,CJ) close to zero. Selecting negatives
from local training batches unlikely provides optimal training signals for dense retrieval.
4
Published as a conference paper at ICLR 2021
4	Approximate Nearest Neighbor Noise Contrastive Estimation
Our analyses show the importance, if not necessity, to construct negatives globally from the corpus
to avoid uninformative negatives for better learning convergence. In this section, we propose
Approximate nearest neighbor Negative Contrastive Estimation (ANCE), which selects negatives
from the entire corpus using an asynchronously updated ANN index.
ANCE samples negatives from the top retrieved documents via the DR model from the ANN index:
θ*
argminθ	l(f (q, d+), f(q, d-)),
(14)
q d+∈D+ d-∈DA-NCE
with DA-NCE = ANNf (q,d) \ D+ and ANNf(q,d) the top retrieved documents by f() from the ANN
index. By definition, Dance are the hardest negatives for the current DR model: Dance ≈ D-*. In
theory, these more informative negatives have higher training loss, elevate the upper bound on the
gradient norms (first component of Eqn 13), and prevent the slow convergence indicated in Eqn 13.
ANcE can pair with many DR models. For simplicity, we use BERT-Siamese (Eqn. 2), with shared
encoder weights between q and d and negative log likelihood (NLL) loss (Luan et al., 2020).
Asynchronous Index Refresh: During stochastic training, the DR model f () is updated each mini-
batch. Maintaining an update-to-date ANN index to select fresh ANcE negatives is challenging, as
the index update requires two operations: 1) Inference: refresh the representations of all documents in
the corpus with an updated DR model; 2) Index: rebuild the ANN index using updated representations.
Although Index is efficient (Johnson et al., 2017), Inference is too expensive to compute per batch as
it requires a forward pass on a corpus much bigger than a training batch.
Thus we implement an asynchronous index refresh similar to Guu et al. (2020), and update the ANN
index once every m batches, i.e., with checkpoint fk . As illustrated in Fig. 2, besides the Trainer,
we run an Inferencer that takes the latest checkpoint (e.g., fk) and recomputes the encodings of the
entire corpus. In parallel, the Trainer continues its stochastic learning using Df- from ANNfk-1 .
Once the corpus is re-encoded, the Inferencer updates the index (ANNfk ) and feed it to the Trainer,
e.g., through a shared file system. In this process, the ANcE negatives (DA-NcE) are asynchronously
updated to “catch up” with the stochastic training, with an async-gap determined by the computing
resources allocated to the Inferencer. Our experiment in Sec 6.4 studies the influence of this async-gap
in learning convergence.
5	Experimental Methodologies
This section describes our experimental setups. More details can be found in Appendix A.1 and A.2.
Benchmarks: The web search experiments use the TREc 2019 Deep Learning (DL) Track (craswell
et al., 2020). It is a standard ad hoc retrieval benchmark: given web queries from Bing, to retrieval
passages or documents from the MS MARcO corpus (Bajaj et al., 2016). We use the official setting
and focus on the first stage retrieval, but also show results when reranking top 100 BM25 candidates.
The OpenQA experiments use the Natural Questions (NQ) (Kwiatkowski et al., 2019) and TriviaQA
(TQA) (Joshi et al., 2017), following the exact settings from Karpukhin et al. (2020). The metrics are
coverage@20/100, which evaluate whether the Top-20/100 retrieved passages include the answer.
We also evaluate whether ANcE’s better retrieval can propagate to better answer accuracy, by running
the state-of-the-art systems’ readers on top of ANcE retrieval. The readers are RAG-Token (Lewis
et al., 2020b) on NQ and DPR Reader on TQA, using their suggested settings.
We also study the effectiveness of ANcE in the first stage retrieval of a commercial search engine’s
production system. We change the training of a production-quality DR model to ANcE, and evaluate
the offline gains in various corpus sizes, encoding dimensions, and exact/approximate search.
Baselines: In TREc DL, we include best runs in relevant categories and refer to craswell et al.
(2020) for more baseline scores. We implement various DR baselines using the same BERT-Siamese
(Eqn. 2), but with different training negative construction: random sampling in batch (Rand Neg),
random sampling from BM25 top 100 (BM25 Neg) (Lee et al., 2019; Gao et al., 2020b), and the 1:1
combination of BM25 and Random negatives (BM25 + Rand Neg) (Karpukhin et al., 2020; Luan
5
Published as a conference paper at ICLR 2021
Table 1: Results in TREC 2019 Deep Learning Track. Results not available are marked as “n.a.”,
not applicable are marked as “-”. Best results in each category are marked bold. Dense Retrieval
baselines use the same BERT-Siamese but different training strategies.
	MARCODev		TREC DL Passage		TREC DL Document	
	Passage Retrieval		NDCG@10		NDCG@10	
	MRR@10	Recall@1k	Rerank	Retrieval	Rerank	Retrieval
Sparse & Cascade IR						
BM25	0.240	0.814	-	0.506	-	0.519
Best DeepCT	0.243	n.a.	-	n.a.	-	0.554
Best TREC Trad Retrieval	0.240	n.a.	-	0.554	-	0.549
BERT Reranker	-	-	0.742	-	0.646	-
Dense Retrieval						
Rand Neg	0.261	0.949	0.605	0.552	0.615	0.543
NCE Neg	0.256	0.943	0.602	0.539	0.618	0.542
BM25 Neg	0.299	0.928	0.664	0.591	0.626	0.529
DPR (BM25 + Rand Neg)	0.311	0.952	0.653	0.600	0.629	0.557
BM25 → Rand	0.280	0.948	0.609	0.576	0.637	0.566
BM25 → NCE Neg	0.279	0.942	0.608	0.571	0.638	0.564
BM25 → BM25 + Rand	0.306	0.939	0.648	0.591	0.626	0.540
ANCE (FirstP)	-0.330~~	0959	0.677^^	0.648	0.641 ^^	0.615
ANCE (MaxP)	-	-	-	-	0.671	0.628
Table 2: Retrieval results (Answer Coverage at Top-20/100)
on Natural Questions (NQ) and Trivial QA (TQA) in the
setting from Karpukhin et al. (2020).
	Single Task		Multi Task	
	NQ	TQA	NQ	TQA
Retriever	Top-20/100	Top-20/100 一	Top-20/100	Top-20/100
BM25	59.1/73.7	66.9/76.7	不	-/-
DPR	78.4/85.4	79.4/85.0	79.4/86.0	78.8/84.7
BM25+DPR	76.6/83.8	79.8/84.5	78.0/83.9	79.9/84.4
ANCE	81.9/87.5	80.3/85.3 一	82.1/87.9	80.3/85.2
Table 3: Relative gains in the
first stage retrieval of a commercial
search engine. The gains are from
changing the training of a produc-
tion DR model to ANCE.
Corpus Size	Dim	Search	Gain
250 Million	768	KNN	+18.4%
8 Billion	64	KNN	+14.2%
8 Billion	64	ANN	+15.5%
et al., 2020). We also compare with contrastive learning/Noise Contrastive Estimation, which uses
hardest negatives in batch (NCE Neg) (Gutmann & Hyvarinen, 2010; Oord et al., 2018; Chen et al.,
2020a). In OpenQA, we compare with DPR, BM25, and their combinations (Karpukhin et al., 2020).
Implementation Details: In TREC DL, recent research found MARCO passage training labels
cleaner (Yan et al., 2019) and BM25 negatives can help train dense retrieval (Karpukhin et al., 2020;
Luan et al., 2020). Thus, We include a “BM25 Warm Up” setting (BM25 → *), where the DR
models are first trained using MARCO official BM25 Negatives. ANCE is also warmed up by BM25
negatives. All DR models in TREC DL are fine-tuned from RoBERTa base uncased (Liu et al., 2019).
In OpenQA, we warm up ANCE using the released DPR checkpoints (Karpukhin et al., 2020).
To fit long documents in BERT-Siamese, ANCE uses the two settings from Dai & Callan (2019b),
FirstP which uses the first 512 tokens of the document, and MaxP, where the document is split to
512-token passages (maximum 4) and the passage level scores are max-pooled. The max-pooling
operation is natively supported in ANN. The ANN search uses the Faiss IndexFlatIP Index (Johnson
et al., 2017). We use batch size 8 and gradient accumulation step 2 on 4 V100 32GB GPUs. For each
positive, we uniformly sample one negative from ANN top 200 (weighted sample and/or from top
100 also work well). We measured ANCE efficiency using one 32GB V100 GPU, Intel(R) Xeon(R)
Platinum 8168 CPU and 650GB of RAM memory.
In asynchronous training, we allocate equal amounts of GPUs to the Trainer and the Inferencer, often
four or eight each. The Trainer produces a model checkpoint every 5k or 10k training batches. The
Inferencer loads the recent model checkpoint and calculates the embeddings of the corpus in parallel.
Once the embedding calculation finishes, a new ANN index is built and the Trainer switches to it for
negative construction. All their communications are through a shared file system. On MS MARCO,
the ANN negative index is refreshed about every 10K training steps.
6
Published as a conference paper at ICLR 2021
Table 4: OpenQA Test Scores in Single Task
Setting. ANCE+Reader switches the retrieve
of the OpenQA systems from DPR to ANCE
and keeps their QA components, which is
RAG-Token on Natural Questions (NQ) and
DPR Reader on Trivia QA (TQA). T5 results
are "closed-book". The others are open-book.
Model
T5-11B (CloSed)(RobertS et al.,2020)
T5-11B + SSM (Closed) (Roberts et al., 2020)
REALM (GUU et al.,2020)
DPR (KarPukhin et al.,2020)
DPR + BM25 (KarpUkhin et al., 2020)
RAG-Token (LewiS et al., 2020b)
RAG-SeqUence (LewiS et al., 2020b)
Table 5: Efficiency of ANCE Serving and Training.
Operation	Offline	Online
BM25 Index Build	3T^	-
BM25 Retrieval	-	37ms
BERT Rerank	-	1.15s
Sparse IR Total (BM25 + BERT)	-	1.42s
ANCE Inference		
Encoding of CorpuS/Per doc	10h∕4.5ms	-
Query Encoding	-	2.6ms
ANN Retrieval (batched q)	-	9ms
DenSe Retrieval Total	-	11.6ms
ANCE Training		
Encoding of CorpuS/Per doc	10h∕4.5ms	-
ANN Index Build	10s	-
Neg ConStruction Per Batch	72ms	-
Back Propagation Per Batch	19ms	-
NQ	TQA
34.5	-
36.6	-
40.4	-
41.5	56.8
39.0	57.0
44.1	55.2
44.5	56.1
46.0	57.5
ANCE + Reader
(a) ANCE FirStP (100%)
(b) NCE Neg (0%)
(d) BM25+Rand (7%)
FigUre 3: The toP DR ScoreS for ten random TREC DL teSting qUerieS. The x-axeS are their ranking
order. The y-axeS are their retrieval ScoreS minUS corPUS average. All modelS are warmed UP by BM25
Neg. The PercentageS are the overlaPS between the teSting and training negativeS near convergence.
6	Evaluation Results
In thiS Section, we firSt evalUate the effectiveneSS and efficiency of ANCE. Then we emPirically StUdy
the convergence of ANCE training and the inflUence of the aSynchronoUS gaP. More comPariSonS of
denSe and SParSe retrieval, hyPerParameter StUdy, and caSe StUdy are in APPendix.
6.1	Effectiveness
In web Search (Table 1), ANCE Significantly oUtPerformS all SParSe retrieval, inclUding the BERT-
baSed DeePCT (Dai et al., 2019). Among DR modelS with different training StrategieS, ANCE iS the
only one robUStly exceeding SParSe methodS in docUment retrieval. In OPenQA, ANCE oUtPerformS
DPR and itS fUSion with BM25 (DPR+BM25) in retrieval accUracy (Table 2). It alSo imProveS
end-to-end QA accUracy, USing the Same readerS with PrevioUS State-of-the-artS bUt ANCE retriever
(Table 4). ANCE’S effectiveneSS iS even more obServed in real ProdUction (Table 3).
Among all DR modelS, ANCE haS the SmalleSt gaP between itS retrieval and reranking accUracy,
Showing the imPortance of global negativeS in training retrieval modelS. ANCE retrieval nearly
matcheS the accUracy of the caScade IR with interaction-baSed BERT Reranker (NogUeira & Cho,
2019), even thoUgh BERT-SiameSe doeS not exPlicitly caPtUre term-level interactionS. With ANCE,
we can learn a representation space that effectively captures the finesse of search relevance.
6.2	Efficiency
The efficiency of ANCE (FirStP) in TREC DL doc iS Shown in Table 5. In serving, we meaSUre the
online latency to retrieve/rank 100 docUmentS Per qUery, with qUery batched. DR iS 100x faSter than
BERT Rerank, a natUral benefit of BERT-SiameSe where the docUment encodingS are calcUlated
offline and SeParately with the qUery. In comPariSon, the interaction-baSed BERT Reranker rUnS
BERT once Per qUery and candidate docUment Pair. The bUlk of training comPUting iS in calcUlating
the encoding of the corPUS for ANCE negative conStrUction, which iS mitigated by making the index
refreSh aSynchronoUS.
7
Published as a conference paper at ICLR 2021
Ok 5k 10kl5k20k25k30k Ok 5k 10kl5k20k25k30k Ok 5k 10kl5k20k25k30k
(a) Training Loss (b) Grad Norm (Bottom) (c) Grad Norm (Middle)
81---------------------------
---ANCE(FirstP)
BM25->NCE Neg
6 ʌ ------- BM25->Rand
rv⅜,---- BM25->BM25+Rand
0」...................
Ok 5k 10kl5k20k25k30k
(d) Grad Norm (Top)
(a) 10k Batch; 4:4; 1e-5
(b) 20k Batch; 8:4; 1e-6
Figure 5: Training loss and testing NDCG of ANCE (FirstP) on documents.The sub captions list the
ANN index refreshing rate (e.g., per 10k Batch), Trainer:Inferencer GPU allocation (e.g., 4:4), and
learning rate (e.g., 1e-5). The x-axes are the training steps.
Figure 4: The loss and gradient norms during DR training (after BM25 warm up). The gradient norms
are the per-layer average of the bottom (1-4), middle (5-8), and top (9-12) transformer layers. Black
dotted lines are the grad norm of the last layer in ANCE (FirstP). The x-axes are training steps.
0.6
Ok
____________⅛.4
IOOk 200k
IOOk 200k
(C) 5k Batch; 4:8; 1e-6
。留
(d) 10k Batch; 4:4; 5e-6

6.3	Empirical Analyses on Training Convergence
We first show the long tail distribution of search relevance in dense retrieval. As plotted in Fig. 3,
there are a few instances per query with significant higher retrieval scores, while the majority form
a long tail. In retrieval/ranking, the key challenge is to distinguish the relevant ones among those
highest scored ones; the rest is trivially irrelevant. We also empirically measure the probability of
local in-batch negatives including informative negatives (D-*), by their overlap with top 100 highest
scored negatives. This probability, either using NCE Neg or Rand Neg, is zero, the same as our theory
shows. In comparison, the overlap between BM25 Neg with top dense retrieved negatives is 15%,
while that of ANCE negatives starts at 63% and converges to 100% by design.
Then we empirically validate our theory that local negatives lead to lower loss, bounded gradient
norm, non-ideal importance sampling, and thus slow convergence (Eqn. 13). The training loss and
pre-clip gradient norms during DR training are plotted in Fig. 4. As expected, the uninformative
local negatives resulted in near-zero gradient norms, while ANCE global negatives maintain a higher
gradient norm. The gradient norm of the last layer in the BERT-Siamese model during ANCE
training (black dotted lines in Fig. 4) is consistently bigger than the other layers, which empirically
aligns with the upper bound in Eqn. 12. Also as our theory suggests, the gradient norms of local
negatives are bounded close to zero, while those of ANCE global negatives are bigger by orders of
magnitude. This confirms that ANCE better approximates the oracle importance sampling distribution
(Pd- 8 ∣Ut l (d+ ,d-) || 2) and improves learning convergence.
6.4	Impact of Asynchronous Gap
The efficiency constraints enforce an asynchronous gap (async-gap) in ANCE training: The negatives
are selected using the encodings from an earlier stage of the being optimized DR model. The
async-gap is determined by the target index refreshing rate, which is determined by the allocation of
computing resource on the Trainer versus the Inferencer, as well as the learning rate. This experiment
studies the impact of this async-gap. The training curves and testing NDCG of different configurations
are plotted in Fig. 5.
8
Published as a conference paper at ICLR 2021
A too large async-gap, either from large learning rate (Fig. 5(a)) or low refreshing rate (Fig. 5(b)),
makes the training unstable, perhaps because the refreshed index changes too dramatically, as
indicated by the peaks in training loss and dips of testing NDCG. The async-gap is not significant
when we allocate an equal amount of GPUs to the index refreshing and to the training (Fig. 5(d)).
Further reducing the gap (Fig. 5(c)) does not improve learning convergence.
In many scenarios, using a same amount of extra GPUs for ANCE as a one-time training cost is a
good return of investment. The efficiency bottleneck in production is often in inference and serving.
7	Related Work
In early research on neural information retrieval (Neu-IR) (Mitra & Craswell, 2018), a common
belief was that the interaction models, those that explicitly handle term level matches, are more
effective though more expensive (Guo et al., 2016; Xiong et al., 2017; Nogueira & Cho, 2019).
Many techniques are developed to reduce their cost, for example, distillation (Gao et al., 2020a) and
caching (Humeau et al., 2020; Khattab & Zaharia, 2020; MacAvaney et al., 2020). ANCE shows that
a properly trained representation-based BERT-Siamese can be effective as well. This finding will
motivate many new research explorations in Neu-IR.
Deep learning has been used to improve various components of sparse retrieval, for example, term
weighting (Dai & Callan, 2019b), query expansion (Zheng et al., 2020), and document expan-
sion (Nogueira et al., 2019). Dense Retrieval chooses a different path and conducts retrieval purely in
the embedding space via ANN search (Lee et al., 2019; Chang et al., 2020; Karpukhin et al., 2020;
Luan et al., 2020). This work demonstrates that a simple dense retrieval system can achieve SOTA
accuracy, while also behaves dramatically different from sparse retrieval. The recent advancement in
dense retrieval may raise a new generation of search systems.
Recent research in contrastive representation learning also shows the benefits of sampling negatives
from a larger candidate pool. In computer vision, He et al. (2020) decouple the negative sampling
pool size with training batch size, by maintaining a negative candidate pool of recent batches and
updating their representation with momentum. This enlarged negative pool significantly improves
unsupervised visual representation learning (Chen et al., 2020b). A parallel work (Xiong et al.,
2021) improves DPR by sampling negatives from a memory bank (Wu et al., 2018) — in which
the representations of negative candidates are frozen so more candidates can be stored. Instead of
a bigger local pool, ANCE goes all the way along this trajectory and constructs negatives globally
from the entire corpus, using an asynchronously updated ANN index.
Besides being a real world application itself, dense retrieval is also a core component in many other
language systems, for example, to retrieve relevant information for grounded language models (Khan-
delwal et al., 2020; Guu et al., 2020), extractive/generative QA (Karpukhin et al., 2020; Lewis et al.,
2020b), and fact verification (Xiong et al., 2021), or to find paraphrase pairs for pretraining (Lewis
et al., 2020a). There dense retrieval models are either frozen or optimized indirectly by signals from
their end tasks. ANCE is orthogonal to those lines of research and focuses on the representation
learning for dense retrieval. Its better retrieval accuracy can benefit many other language systems.
8	Conclusion
In this paper, we first provide theoretical analyses on the convergence of representation learning in
dense retrieval. We show that under common conditions in text retrieval, the local negatives used
in DR training are uninformative, yield low gradient norms, and contribute little to the learning
convergence. We then propose ANCE to eliminate this bottleneck by constructing training negatives
globally from the entire corpus. Our experiments demonstrate the advantage of ANCE in web search,
OpenQA, and the production environment of a commercial search engine. Our studies empirically
validate our theory that ANCE negatives have much bigger gradient norms, reduce the stochastic
gradient variance, and improve training convergence.
9	acknowledgments
We thank Di He for discussions on learning theories and Safoora Yousefi for feedback in writing.
9
Published as a conference paper at ICLR 2021
References
Guillaume Alain, Alex Lamb, Chinnadhurai Sankar, Aaron Courville, and Yoshua Bengio. Variance reduction in
SGD by distributed importance sampling. arXiv preprint arXiv:1511.06481, 2015.
Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew
McNamara, Bhaskar Mitra, Tri Nguyen, et al. MS MARCO: A human generated MAchine Reading
COmprehension dataset. arXiv preprint arXiv:1611.09268, 2016.
Wei-Cheng Chang, X Yu Felix, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar. Pre-training tasks for
embedding-based large-scale retrieval. In Proceedings of the 8th International Conference on Learning
Representations, ICLR 2020, Virtual Event, April 26-30. OpenReview.net, 2020.
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer open-oomain
questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL
2017, Vancouver, Canada, July30-August4, 2017,pp. 1870-1879. ACL, 2017.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive
learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning,
ICML 2020, 13-18 July 2020, Virtual Event, pp. 1597-1607. PMLR, 2020a.
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive
learning. arXiv preprint arXiv:2003.04297, 2020b.
Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M. Voorhees. Overview of the TREC
2019 deep learning track. In Proceedings of the Text REtrieval Conference (TREC). TREC, 2020.
W Bruce Croft, Donald Metzler, and Trevor Strohman. Search Engines: Information Retrieval in Practice.
Pearson Education, 2009. ISBN 978-0-13-136489-9.
Zhuyun Dai and Jamie Callan. Context-aware sentence/passage term importance estimation for first stage
retrieval. arXiv preprint arXiv:1910.10687, 2019a.
Zhuyun Dai and Jamie Callan. Deeper text understanding for IR with contextual neural language modeling. In
Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information
Retrieval, SIGIR 2019, Paris, France, July 21-25, pp. 985-988. ACM, 2019b.
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and Ruslan Salakhutdinov.
Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the 57th
Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2,
pp. 2978-2988. ACL, 2019.
Luyu Gao, Zhuyun Dai, and Jamie Callan. Understanding BERT rankers under distillation. In Proceedings of
ICTIR ’20: The 2020 ACM SIGIR International Conference on the Theory of Information Retrieval, Virtual
Event, Norway, September 14-17, pp. 149-152. ACM, 2020a.
Luyu Gao, Zhuyun Dai, Zhen Fan, and Jamie Callan. Complementing lexical retrieval with semantic residual
embedding. arXiv preprint arXiv:2004.13969, 2020b.
Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. A deep relevance matching model for ad-hoc retrieval.
In Proceedings of the 25th ACM International Conference on Information and Knowledge Management,
CIKM 2016, Indianapolis, IN, USA, October 24-28, pp. 55-64. ACM, 2016.
Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. Accelerating
large-scale inference with anisotropic vector quantization. arXiv preprint arXiv:1908.10396, 2020.
Michael GUtmann and AaPo Hyvarinen. Noise-contrastive estimation: A new estimation principle for Unnormal-
ized statistical models. In Proceedings of the Thirteenth International Conference on Artificial Intelligence
and Statistics, AISTATS 2010, Sardinia, Italy, May 13-15, JMLR Proceedings, pp. 297-304. JMLR.org, 2010.
Kelvin GUU, Kenton Lee, Zora TUng, PanUpong PasUpat, and Ming-Wei Chang. REALM: Retrieval-aUgmented
langUage model pre-training. arXiv preprint arXiv:2002.08909, 2020.
Kaiming He, Haoqi Fan, YUxin WU, Saining Xie, and Ross Girshick. MomentUm contrast for UnsUpervised
visUal representation learning. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition,
CVPR 2020, Seattle, WA, USA, June 13-19, pp. 9726-9735. IEEE, 2020.
SamUel HUmeaU, KUrt ShUster, Marie-Anne LachaUx, and Jason Weston. Poly-encoders: ArchitectUres and
pre-training strategies for fast and accUrate mUlti-sentence scoring. In Proceddings of 8th International
Conference on Learning Representations, ICLR 2020, Virtual Event, April 26-30. OpenReview.net, 2020.
10
Published as a conference paper at ICLR 2021
Jeff Johnson, Matthijs Douze, and HervC J6gou. Billion-scale similarity search With GPUs. arXiv preprint
arXiv:1702.08734, 2017.
Tyler B Johnson and Carlos Guestrin. Training deep models faster With robust, approximate importance sampling.
In Proceedins of the Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018,
Montreal, Canada,December 3-8, pp. 7276-7286, 2018.
Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised
challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association
for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, pp. 1601-1611. ACL,
2017.
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.
Dense passage retrieval for open-domain question ansWering. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, pp. 6769-6781.
ACL, 2020.
Angelos Katharopoulos and FrangoiS Fleuret. Not all samples are created equal: Deep learning with impor-
tance sampling. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018,
Stockholmsmdssan, Stockholm, Sweden, July 10-15, pp. 2530-2539. PMLR, 2018.
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through
memorization: Nearest neighbor language models. In Proceedings of the 8th International Conference on
Learning Representations, ICLR 2020, Virtual Event, April 26-30. OpenReview.net, 2020.
Omar Khattab and Matei Zaharia. ColBERT: Efficient and effective passage search via contextualized late
interaction over BERT. In Proceedings of the 43rd International ACM SIGIR conference on research and
development in Information Retrieval, SIGIR 2020, Virtual Event, July 25-30, pp. 39-48. ACM, 2020.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle
Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural Questions: A benchmark for question
answering research. Transactions of the Association for Computational Linguistics, 7:453-466, 2019.
Victor Lavrenko and W Bruce Croft. Relevance-based language models. In ACM Special Interest Group on
Information Retrieval (SIGIR) Forum, volume 51, pp. 260-267. ACM, 2017.
Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain
question answering. In Proceedings of the 57th Conference of the Association for Computational Linguistics,
ACL 2019, Florence, Italy, July 28- August 2, pp. 6086-6096. ACL, 2019.
Mike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida Wang, and Luke Zettlemoyer.
Pre-training via paraphrasing. In Proceedings of the Annual Conference on Neural Information Processing
Systems 2020, NeurIPS 2020, virtual, December 6-12, 2020a.
Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich
Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrieval-augmented generation for knowledge-
intensive nlp tasks. arXiv preprint arXiv:2005.11401, 2020b.
Tie-Yan Liu. Learning to rank for information retrieval. Foundations and Trends® in Information Retrieval, 3
(3):225-331, 2009.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke
Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv
preprint arXiv:1907.11692, 2019.
Yi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. Sparse, dense, and attentional representa-
tions for text retrieval. arXiv preprint arXiv:2005.00181, 2020.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine Learning
Research, 9(Nov):2579-2605, 2008.
Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Nazli Goharian, and Ophir Frieder.
Efficient document re-ranking for transformers by precomputing term representations. In Proceedings of the
43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR
2020, Virtual Event, China, July 25-30, 2020, pp. 49-58. ACM, 2020.
Bhaskar Mitra and Nick Craswell. An introduction to neural information retrieval. Foundations and Trends® in
Information Retrieval, 13(1):1-126, 2018.
11
Published as a conference paper at ICLR 2021
Marius Mosbach, Maksym Andriushchenko, and Dietrich Klakow. On the stability of fine-tuning BERT:
Misconceptions, explanations, and strong baselines. In Proceedings of the 9th International Conference on
Learning Representations, ICLR 2021, Virtual Event, May 3-7. OpenReview.net, 2021.
Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint arXiv:1901.04085,
2019.
Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. Document expansion by query prediction. arXiv
preprint arXiv:1904.08375, 2019.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding.
arXiv preprint arXiv:1807.03748, 2018.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine
comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language
Processing, EMNLP 2016, Austin, Texas, USA, November 1-4,pp. 2383-2392. ACL, 2016.
Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of
a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing, EMNLP 2020, Online, November 16-20, pp. 5418-5426. ACL, 2020.
James Thorne, Andreas Vlachos, Oana Cocarascu, Christos Christodoulopoulos, and Arpit Mittal. The fact
extraction and verification (FEVER) shared task. In Proceedings of the 1st Workshop on Fact Extraction and
VERification (FEVER), pp. 1-9, 2018.
Ellen M Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. Information
Processing & Management, 36(5):697-716, 2000.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. GLUE: A multi-
task benchmark and analysis platform for natural language understanding. In 7th International Conference on
Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9. OpenReview.net, 2019.
Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric in-
stance discrimination. In Proceedings of 2018 IEEE Conference on Computer Vision and Pattern Recognition,
CVPR 2018, Salt Lake City, UT, USA, June 18-22, pp. 3733-3742. IEEE, 2018.
Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. End-to-end neural ad-hoc ranking
with kernel pooling. In Proceedings of the 40th International ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR 2017, Shinjuku, Tokyo, Japan, August 7-11, pp. 55-64. ACM,
2017.
Wenhan Xiong, Xiang Lorraine Li, Srini Iyer, Jingfei Du, Patrick Lewis, William Yang Wang, Yashar Mehdad,
Wen-tau Yih, Sebastian Riedel, DouWe Kiela, and Barlas Oguz. Answering complex open-domain ques-
tions with multi-hop dense retrieval. In Proceedings of the 9th International Conference on Learning
Representations, ICLR 2021, Virtual Event, May 3-7, 2021.
Ming Yan, Chenliang Li, Chen Wu, Bin Bi, Wei Wang, Jiangnan Xia, and Luo Si. IDST at TREC 2019 deep
learning track: Deep cascade ranking with generation-based document expansion and pre-trained language
modeling. In Proceedings of Text REtrieval Conference. TREC, 2019.
Chen Zhao, Chenyan Xiong, Corby Rosset, Xia Song, Paul Bennett, and Saurabh Tiwary. Transformer-XH:
Multi-evidence reasoning with extra hop attention. In Proceedings of the 8th International Conference on
Learning Representations, ICLR 2020, Virtual Event, April 26-30. OpenReview.net, 2020.
Zhi Zheng, Kai Hui, Ben He, Xianpei Han, Le Sun, and Andrew Yates. BERT-QE: Contextualized query
expansion for document re-ranking. pp. 4718-4728, 2020.
12
Published as a conference paper at ICLR 2021
A	Appendix
A.1 More Experimental Details
More Details on TREC DL Benchmarks: There are two tasks in the TREC DL 2019 Track: document retrieval
and passage retrieval. The training and development sets are from MS MARCO, which includes passage level
relevance labels for one million Bing queries (Bajaj et al., 2016). The document corpus was post-constructed by
back-filling the body texts of the passage’s URLs and their labels were inherited from its passages (Craswell
et al., 2020). The testing sets are labeled by NIST accessors on the top 10 ranked results from past Track
participants (Craswell et al., 2020).
TREC DL official metrics include NDCG@10 on test and MRR@10 on MARCO Passage Dev. MARCO
Document Dev is noisy and the recall on the DL Track testing is less meaningful due to low label coverage on
DR results. There is a two-year gap between the construction of the passage training data and the back-filling of
their full document content. Some original documents were no longer available. There is also a decent amount
of content changes in those documents during the two-year gap, and many no longer contain the passages. This
back-filling perhaps is the reason why many Track participants found the passage training data is more effective
than the inherited document labels. Note that the TREC testing labels are not influenced as the annotators were
provided the same document contents when judging.
All the TREC DL runs are trained using these training data. Their inference results on the testing queries of the
document and the passage retrieval tasks were evaluated by NIST assessors in the standard TREC-style pooling
technique (Voorhees, 2000). The pooling depth is set to 10, that is, the top 10 ranked results from all participated
runs are evaluated, and these evaluated labels are released as the official TREC DL benchmarks for passage and
document retrieval tasks.
More Details on OpenQA Experiments: All the DPR related experimental settings, baseline systems, and
DPR Reader are based on their open source libarary2. The RAG-Token reader uses their open-source release in
huggingface3. The RAG-Seq release in huggingface is not yet stable by the time we did our experiment, thus we
choose the RAG-Token in our OpenQA experiment. RAG only releases the NQ models thus we use DPR reader
on TriviaQA. We feed top 20 passages from ANCE to RAG-Token on NQ and top 100 passages to DPR’s BERT
Reader, following the guideline in their open-source codes.
More Details on Baselines: The most representative sparse retrieval baselines in TREC DL include the standard
BM25 (“bm25base” or “bm25base_p”), Best TREC Sparse Retrieval (“bm25tuned_rm3” or “bm25tuned_prf_p”)
with tuned query expansion (Lavrenko & Croft, 2017), and Best DeepCT (“dct_tp_bm25e2”, doc only), which
uses BERT to estimate the term importance for BM25 (Dai & Callan, 2019a). These three runs represent
the standard sparse retrieval, best classical sparse retrieval, and the recent progress of using BERT to im-
prove sparse retrieval. We also include the standard cascade retrieval-and-reranking systems BERT Reranker
(“bm25exp_marcomb” or “p_exp_rm3_bert”), which is the best run using standard BERT on top of query/doc
expansion, from the groups with multiple top MARCO runs (Nogueira & Cho, 2019; Nogueira et al., 2019).
BERT-Siamese Configurations: We follow the network configurations in Luan et al. (2020) in all Dense
Retrieval methods, which we found provides the most stable results. More specifically, we initialize the BERT-
Siamese model with RoBERTa base (Liu et al., 2019) and add a 768 × 768 projection layer on top of the last
layer’s “[CLS]” token, followed by a layer norm.
We use BERT-Siamese, NLL loss, and dot product to be consistent with recent research. We have obtained better
accuracy with more vectors per document, BCE loss, and cosine similarity, but that is not the focus of this paper.
Implementation Details: The training often takes about 1-2 hours per ANCE epoch, which is whenever new
ANCE negative is ready, it immediately replaces existing negatives in training, without waiting. It converges
in about 10 epochs, similar to other DR baselines. The optimization uses LAMB optimizer, learning rate 5e-6
for document and 1e-6 for passage retrieval, and linear warm-up and decay after 5000 steps. More detailed
hyperparameter settings can be found in our code release.
A.2 Overlap with Sparse Retrieval in TREC 2019 DL Track
As a nature of TREC-style pooling evaluation, only those ranked in the top 10 by the 2019 TREC participating
systems were labeled. As a result, documents not in the pool and thus not labeled are all considered irrelevant,
even though there may be relevant ones among them. When reusing TREC style relevance labels, it is very
important to keep track of the “hole rate” on the evaluated systems, i.e., the fraction of the top K ranked results
without TREC labels (not in the pool). A larger hole rate shows that the evaluated methods are very different
2https://github.com/facebookresearch/DPR.
3https://huggingface.co/transformers/master/modeldoc/rag.html
13
Published as a conference paper at ICLR 2021
Table 6: Coverage of TREC 2019 DL Track labels on Dense Retrieval methods. Overlap with BM25
is calculated on top 100 retrieved documents.
	TREC DL Passage			TREC DL Document		
Method	Recall@1K	Hole@10	Overlap w. BM25	Recall@100	Hole@10	Overlap w. BM25
BM25	0.685	5.9%	100%	0387-	0.2%	100%
BM25 Neg	0.569-	25.8%	119%	0.217-	28.1%	17.9%
BM25 + Rand Neg	0.662	20.2%	16.4%	0.240	21.4%	21.0%
ANCE (FirstP)	0.661	14.8%	17.4%	0266-	13.3%	24.4%
ANCE (MaxP)	-	-	-	0.286	11.9%	24.9%
Table 7: Results of different hyperparameter configurations. “Top K Neg” lists the top k dense
retrieved candidates from which we sampled the ANCE negatives from.
	Learning rate	Hyperparameter Top K Neg	Refresh (step)	MARCO Dev Passage Retrieval MRR@10	TREC DL Document Retrieval NDCG@10
Passage ANCE	1e-6	200	10k	0.33	-
	1e-6	500	10k	0.31	-
	2e-6	200	10k	0.29	-
	2e-7	500	20k	0.303	-
	2e-7	1000	20k		0.302	-
Document ANCE	1e-5	100	10k	-	0.58
	1e-6	100	20k	-	0.59
	1e-6	100	5k	-	0.60
	5e-6	200	10k	-	0.614
	1e-6	200	10k	-		0.61	
from those systems that participated in the Track and contributed to the pool, thus the evaluation results are not
perfect. Note that the hole rate does not necessarily reflect the accuracy of the system, only the difference of it.
In TREC 2019 Deep Learning Track, all the participating systems are based on sparse retrieval. Dense retrieval
methods often differ considerably from sparse retrievals and in general will retrieve many new documents. This
is confirmed in Table 6. All DR methods have very low overlap with the official BM25 in their top 100 retrieved
documents. At most, only 25% of documents retrieved by DR are also retrieved by BM25. This makes the hole
rate quite high and the recall metric not very informative. It also suggests that DR methods might benefit more
in this year’s TREC 2020 Deep Learning Track if participants are contributing DR based systems.
The MS MARCO ranking labels were not constructed based on pooling the sparse retrieval results. They were
from Bing (Bajaj et al., 2016), which uses many signals beyond term overlap. This makes the recall metric in
MS MARCO more robust as it reflects how a single model can recover a complex online system.
A.3 Hyperparameter Studies
We show the results of some hyperparameter configurations in Table 7. The cost of training with BERT makes
it difficult to conduct a lot of hyperparameter explorations. Often a failed configuration leads to divergence
early in training. We barely explore other configurations due to the time-consuming nature of working with
pretrained language models. Our DR model architecture is kept consistent with recent parallel work and the
learning configurations in Table 7 are about all the explorations we did. Most of the hyperparameter choices are
decided solely using the training loss curve and otherwise by the loss in the MARCO Dev set. We found the
training loss, validation NDCG, and testing performance align well in our (limited) hyperparameter explorations.
A.4 Case Studies
In this section, we show Win/Loss case studies between ANCE and BM25. Among the 43 TREC 2019 DL Track
evaluation queries in the document task, ANCE outperforms BM25 on 29 queries, loses on 13 queries, and
ties on the rest 1 query. The winning examples are shown in Table 8 and the losing ones are in Table 9. Their
corresponding ANCE-learned (FirstP) representations are illustrated by t-SNE in Fig. 6 and Fig. 7.
In general, we found ANCE better captures the semantics in the documents and their relevance to the query. The
winning cases show the intrinsic limitations of sparse retrieval. For example, BM25 exact matches the “most
popular food” in the query “what is the most popular food in Switzerland” but the document is about Mexico.
The term “Switzerland” does match with the document but it is in the related question section.
The losing cases in Table 9 are also quite interesting. Many times we found that it is not that DR fails completely
and retrieves documents not related to the query’s information needs at all, which was a big concern when we
started research in DR. The errors ANCE made include retrieving documents that are related just not exactly
14
Published as a conference paper at ICLR 2021
Table 8: Queries in the TREC 2019 DL Track Document Ranking Tasks where ANCE performs better
than BM25. Snippets are manually extracted. The documents at the first disagreed ranking positions
are shown. ANCE won on all of them. The NDCG@10 of ANCE and BM25 in the corresponding
query is listed.
	ANCE	-	BM25	一
Query:	qid (104861): Cost of interior concrete flooring	
Title:	Concrete network: Concrete Floor Cost	Pinterest: Types of Flooring
DocNo:	D293855	—	D2692315	—
Snippet:	For a concrete floor with a basic finish, you can expect to pay $2 to $12 per square foot...	Know About Hardwood Flooring And Its Types White Oak Floors Oak Floor- ing Laminate Flooring In Bathroom . . .
Ranking Position:	1	-	1	-
TREC Label:	3 (Very Relevant)	0 (Irrelevant)
NDCG@10:	0.86	—	0.15	—
Query:	qid (833860): What is the most popular food in Switzerland	
Title:	Wikipedia: Swiss cuisine	Answers.com: Most popular traditional food dishes of Mexico
DocNo:	D1927155	—	D3192888	—
Snippet:	Swiss cuisine bears witness to many re- gional influences, . . . Switzerland was historically a country of farmers, so tra- ditional Swiss dishes tend not to be. . .	One of the most popular traditional Mex- ican deserts is a spongy cake . . . (in the related questions section) What is the most popular food dish in Switzer- land?. . .
Ranking Position:	1	-	1	-
TREC Label:	3 (Very Relevant)	0 (Irrelevant)
NDCG@10:	0.90	—	0.14	—
Query:	qid (1106007): Define visceral	
Title:	Vocabulary.com: Visceral	Quizlet.com: A&P EX3 autonomic 9-10
DocNo:	D542828	—	D830758	—
Snippet:	When something,s visceral, you feel it in your guts. A visceral feeling is in- tuitive — there might not be a rational explanation, but you feel that you know what,s best...	Acetylcholine A neurotransmitter liber- ated by many peripheral nervous system neurons and some central nervous sys- tem neurons. . .
Ranking Position:	1	-	1	-
TREC Label:	3 (Very Relevant)	0 (Irrelevant)
NDCG@10:	0.80	—	0.14	—
(a) 104861: interior flooring cost. (b) 833860: popular Swiss food (c) 1106007: define visceral
Figure 6: t-SNE Plots for Winning Cases in Table 8. Qids and queries are listed in the sub-captions.
relevant to the query, for example, “yoga pose” for “bow in yoga”. In other cases, ANCE retrieved wrong
documents due to the lack of the domain knowledge: the pretrained language model may not know “active
margin” is a geographical terminology, not a financial one (which we did not know neither and took some time
to figure out when conducting this case study). There are also some cases where the dense retrieved documents
make sense to us but were labeled irrelevant.
The t-SNE plots in Fig. 6 and Fig. 7 show many interesting patterns of the learned representation space. The
ANCE winning cases often correspond to clear separations of different document groups. For losing cases the
representation space is more mixed, or there is too few relevant documents which may cause the variances in
15
Published as a conference paper at ICLR 2021
Table 9: Queries in the TREC 2019 DL Track Document Ranking Tasks where ANCE performs
worse than BM25. Snippets are manually extracted. The documents at the first positions where BM25
wins are shown. The NDCG@10 of ANCE and BM25 on the corresponding query is listed. Typos in
the query are from the realist web search queries in TREC.
	ANCE	-	BM25	一
Query:	qid (182539): Example of monotonic function	
Title:	Wikipedia: Monotonic function	Explain Extended: Things SQL needs: sargability of monotonic functions
DocNo:	D510209	—	D175960	—
Snippet:	In mathematics, a monotonic function (or monotone function) is a function be- tween ordered sets that preserves or re- verses the given order... For example, if y=g(x) is strictly monotonic on the range [a,b]...	I’m going to write a series of articles about the things SQL needs to work faster and more efficienly. . .
Ranking Position:	1	-	1	-
TREC Label:	0 (Irrelevant)	2 (Relevant)
NDCG@10:	0.25	—	0.61	—
Query:	qid (1117099): What is a active margin	
Title:	Wikipedia: Margin (finance)	Yahoo Answer: What is the difference between passive and active continental margins
DocNo:	D166625	-	D2907204	—
Snippet:	In finance, margin is collateral that the holder of a financial instrument . . .	An active continental margin is found on the leading edge of the continent where ...
Ranking Position:	2	-	2	-
TREC Label:	0 (Irrelevant)	3 (Very Relevant)
NDCG@10:	0.44	—	0.74	—
Query:	qid (1132213): How long to hold bow in yoga	
Title:	Yahoo Answer: How long should you hold a yoga pose for	yogaoutlet.com: How to do bow pose in yoga
DocNo:	D3043610	—	D3378723	—
Snippet:	so i,ve been doing yoga for a few weeks now and already notice that my flexi- ablity has increased drastically	 That depends on the posture itself ...	Bow Pose is an intermediate yoga back- bend that deeply opens the chest and the front of the body. . . Hold for up to 30 seconds . . .
Ranking Position:	1	1
TREC Label:	0 (Irrelevant)	3 (Very Relevant)
NDCG@10:	0.66	—	0.74	—
★ Query
+ Relevant
▼ ANCE Neg
BM25 Neg
Rand Neg
(a) 182539: monotonic function
(b) 1117099: active margin
Figure 7: t-SNE Plots for Losing Cases in Table 9. Qids and queries are listed in the sub-captions.
(c) 1132213: yoga bow
model performances. There are also many different interesting patterns in the ANCE-learned representation
space. We include the t-SNE plots for all 43 TREC DL Track queries in our open-source repo. More future
analyses of the learned patterns in the representation space may help provide more insights on dense retrieval.
16