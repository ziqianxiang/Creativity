Published as a conference paper at ICLR 2021
Domain-Robust Visual Imitation Learning
with Mutual Information Constraints
Edoardo Cetin & Oya Celiktutan
Centre for Robotics Research
Department of Engineering, King’s College London
{edoardo.cetin,oya.celiktutan}@kcl.ac.uk
Ab stract
Human beings are able to understand objectives and learn by simply observing
others perform a task. Imitation learning methods aim to replicate such capa-
bilities, however, they generally depend on access to a full set of optimal states
and actions taken with the agent’s actuators and from the agent’s point of view.
In this paper, We introduce a new algorithm - called Disentangling Generative
Adversarial Imitation Learning (DisentanGAIL) - with the purpose of bypassing
such constraints. Our algorithm enables autonomous agents to learn directly from
high dimensional observations of an expert performing a task, by making use of
adversarial learning with a latent representation inside the discriminator network.
Such latent representation is regularized through mutual information constraints
to incentivize learning only features that encode information about the completion
levels of the task being demonstrated. This allows to obtain a shared feature space
to successfully perform imitation while disregarding the differences between the
expert’s and the agent’s domains. Empirically, our algorithm is able to efficiently
imitate in a diverse range of control problems including balancing, manipulation
and locomotive tasks, while being robust to various domain differences in terms
of both environment appearance and agent embodiment.
1	Introduction
Recent advances demonstrated the strengths of combining reinforcement learning (RL) with power-
ful function approximators to obtain effective behavior for high dimensional control tasks (Lillicrap
et al., 2015; Schulman et al., 2017; Haarnoja et al., 2018a). However, RL’s reliance on a reward
function introduces a fundamental limitation as reward specification and instrumentation can bring
about a great design burden to potential users aiming to train an agent for a novel problem.
An alternative approach for addressing this limitation is to recover a learning signal through expert
demonstrations. Most of the past work exploring this area focused on the problem setting where
demonstrations are provided directly from the agent’s point of view and through the agent’s actu-
ators, which we refer to as agent-centric imitation. However, applying agent-centric imitation for
real-world robot learning would demand users to provide a diverse range of kinesthetic or teleoper-
ated demonstrations to a robotic platform, leading to an unnatural user-agent interaction process.
In this paper, we focus instead on learning effective policies solely from a set of external, high
dimensional observations of a different expert agent executing a task. We refer to this problem
formulation as observational imitation. Solving this requires disentangling the expert’s intentions
from the observations’ context, which has been a challenging problem for prior research, and often
relied on additional assumptions about the environment and expert data (Torabi et al., 2019).
We propose a novel algorithm, called Disentangling Generative Adversarial Imitation Learning
(DisentanGAIL), to acquire effective agent behavior without such limitations. Our technique is
based on the framework of inverse reinforcement learning, yet, it enables an agent to learn with
only access to observations collected by watching a structurally different expert. DisentanGAIL
utilizes an off-policy learner alongside a novel discriminator with a latent representation bottleneck,
regularized to represent a domain invariant space over the agent’s and expert’s sets of observations.
1
Published as a conference paper at ICLR 2021
This is achieved by enforcing two constraints on the estimated mutual information between the latent
representation and the origin of collected observations.
In particular, the contribution of this work for solving observational imitation is threefold:
•	We propose a discriminator making use of novel mutual information constraints, and provide
techniques to adaptively and consistently ensure their enforcement.
•	We identify the problem of domain information disguising when estimating mutual information
and propose structural modifications to our models for its prevention.
•	We show that, unlike prior work, our algorithm can scale to high dimensional tasks while being
robust to domain differences in both environment appearance and agent embodiment, by testing
on a novel diverse set of tasks with varying difficulty.
2	Related work
Agent-centric imitation has been a long-studied problem setting. Behavior cloning (Pomerleau,
1989; 1991; Ross et al., 2011) was first proposed to approach imitation from a supervised learning
perspective. Particularly, an agent is trained to maximize the likelihood of executing a set of recorded
optimal actions from the states encountered by the expert. Inverse reinforcement learning (IRL) (Ng
et al., 2000; Abbeel & Ng, 2004; Ratliff et al., 2006) was more recently proposed as an alternative
two-step solution to imitation and has been often shown to be more effective. First, IRL aims to
recover a reward function by parameterizing a discriminator, trained to be representative of the
objective portrayed by the expert demonstrations. Second, it tries to learn behavior to accomplish
such objective, utilizing RL. To effectively understand and represent the expert’s intentions, modern
instantiations of IRL combined the maximum entropy problem formulation (Ziebart et al., 2008)
with deep learning (Wulfmeier et al., 2015; Finn et al., 2016b), and proposed a direct connection
with adversarial learning (Ho & Ermon, 2016; Finn et al., 2016a; Kostrikov et al., 2018). This
allowed for successful imitation in complex control tasks with few expert demonstrations. Related
to our algorithm, Peng et al. (2018) implemented a variational bottleneck to limit information flow
in the discriminator, for tackling the discriminator saturation problem (Arjovsky & Bottou, 2017).
Additionally, Zolna et al. (2019) proposed to optimize the discriminator to be maximally uncertain
about uninformative sets of data as a form of regularization to disregard irrelevant features.
A different line of research instead considered imitating from observations ofan expert performing a
task, in problem settings resembling observational imitation. Earlier methods proposed to use hand-
engineered mappings to domain invariant features (Gioioso et al., 2012; Gupta et al., 2016), while
more recent works proposed to learn such mappings, relying on specific prior data obtained under
both the agent and expert perspectives. These techniques include using time-aligned demonstrations
(Gupta et al., 2017; Sermanet et al., 2018; Liu et al., 2018; Sharma et al., 2019), or multiple tasks
where the agent and the expert already achieved expertise (Smith et al., 2019; Kim et al., 2019).
While effective, these methods for observational imitation make considerable assumptions about the
task structure and the available data. Therefore, they have limited applicability for arbitrary prob-
lems, where environment instrumentation and prior knowledge are minimal. The work most related
to ours is by Stadie et al. (2017), where a domain invariant representation is learned through utilizing
a domain confusion loss that requires two different expert policies for sampling failure and success
demonstrations in the expert domain. While this approach was also adopted in recent works (Oku-
mura et al., 2020; Choi et al., 2020), empirically, it yielded successful imitation results only when
working in low dimensional control tasks and with the agent and expert domains differing solely in
their appearance. On the contrary, our algorithm only requires a limited set of expert demonstrations
and allows for successful imitation in both low and high dimensional control tasks, with the expert’s
and agent’s domains differing in both environment appearance and agent embodiment.
3	Background and preliminaries
3.1	Adversarial imitation learning
In imitation learning, the agent is provided a set of expert demonstrations BE = {τ1, τ2, ..., τN},
where each τ = (s0, a0, s1, a1, ..., sT) represents a trajectory collected with an expert policy from
2
Published as a conference paper at ICLR 2021
the agent’s point of view. These demonstrations are used to provide the learning signal for the agent
to improve its policy π . In Generative Adversarial Imitation Learning (GAIL) (Ho & Ermon, 2016)
this learning signal is obtained through a pseudo-reward function RD, derived from a discriminator
network D trained to discern between ‘expert’ and ‘agent’ state-action-next state triplets:
arg max EBE log(D(si, ai, si+1)) +Epπ(τ) log(1 - D(si, ai, si+1)),	(1)
D
where pπ (τ) is the distribution of trajectories encountered by the agent, stemming from both the
environment’s dynamics and its own current policy π. Reinforcement learning methods are then
applied for π to adversarially maximize the sum of encountered pseudo-rewards:
arg maxEpπ(τ)
π
T-1
RD(st, at, st+1)
t=0
(2)
where RD (si, ai, si+1) = log(D(si, ai, si+1)) -log(1-D(si, ai, si+1)). Ho & Ermon (2016) pro-
posed to iteratively execute the adversarial optimization steps described in Eqs. 1 and 2, optimizing
π through the Trust Region Policy Optimization algorithm (Schulman et al., 2015).
3.2	Observational IMITATION LEARNING
In the problem setting of observational imitation, we are concerned with learning without knowl-
edge of the states visited and the actions taken by the expert. We purely rely on observations,
o ∈ O, provided in a set of expert demonstrations BE = {τ10 , τ20 , ..., τN0 } containing visual tra-
jectories τi0 = oi0, oi1, ..., oiT . Each visual trajectory τi0 is a sequence of observations obtained
by watching the expert act in its domain. Here, the discriminator D will be optimized to discern
between the expert demonstrations in BE and ‘agent’ observations in Bπ, a set of visual trajec-
tories collected by watching the agent act according to π. We consider each observation to be an
RGB image of the environment at the current time-step, hence, containing only partial and highly-
entangled information about the true state. We define the expert’s and agent’s domains as distinct
Partially Observed Markov Decision Processes (POMDPs), ME = (SE , AE , OE , P, po , R) and
MA = (SA , AA , OA , P, po, R), respectively. The main challenge in observational imitation is that
the states s ∈ SE , actions a ∈ AE and observations o ∈ OE in the expert’s POMDP do not neces-
sarily match the states s ∈ SA , actions a ∈ AA and observations o ∈ OA in the agent’s POMDP.
3.3	Mutual information
To overcome the differences between the expert’s and agent’s POMDPs, our algorithm relies on esti-
mating the mutual information between different sets of variables. Mutual information is a statistical
measure that represents the level of dependency between two random variables and quantifies how
much information each random variable is expected to contain about the other (Kinney & Atwal,
2014). In other words, the mutual information between X and Z measures the difference between
the entropy of X and the conditional entropy of X given Z :
I(X, Z) =H(X) - H(X|Z) = H(Z) - H(Z|X).	(3)
Belghazi et al. (2018) showed that the mutual information can be effectively estimated through the
Mutual Information Neural Estimator (MINE). This consists in lower bounding the mutual informa-
tion through the Donskher-Varadhan dual representation of the KL divergence between two random
variables distributions (Donsker & Varadhan, 1975), by searching for a parameterized function Tφ :
I(X, Z) ≥ sup EP(X,Z) [Tφ(x, z)] - log EP (X),P (Z) heTφ(x,z)i .	(4)
φ∈Φ
4	DisentanGAIL
DisentanGAIL utilizes several algorithmic components to address the problem of observational im-
itation. Particularly, its discriminator, D, is regularized by the enforcement of two mutual infor-
mation constraints between the domain origin and a specific latent representation of the collected
observations. This enables the pseudo-rewards, RD, to disregard the domain differences between the
3
Published as a conference paper at ICLR 2021
Figure 1: Simplified discriminator optimization structure: for each time-step the four most recent
observations ot:t-3 are processed independently by the preprocessor Pθ1, outputting the correspond-
ing latent representations zt:t-3. The latent representations are then concatenated and fed jointly
into the invariant discriminator Sθ2, and fed individually into the statistics network Tφ, outputting
respectively the GAIL objective JG, and the penalty loss Lβ or Lλ .
expert’s and agent’s domain and provide a meaningful learning signal to improve the agent’s policy,
π. Together with the visual trajectories in BE and Bπ, DisentanGAIL also exploits sets of prior data
collected in the expert’s and agent’s domains, denoted by BP.E and BP.π respectively. Such data is
obtained by recording observations from the expert’s and agent’s domains, while neither expert nor
agent is attempting to perform the target task, e.g., while they are acting randomly.
4.1	Discriminator components
Our algorithm for observational imitation utilizes a convolutional neural network discriminator Dθ ,
optimized for outputting the probability that an input sequence of observations occurred as a conse-
quence of expert behavior. As illustrated in Fig. 1, our discriminator can be divided into two distinct
sub-models, namely, the preprocessor Pθ1 and the invariant discriminator Sθ2 :
Dθ = Sθ2 ◦ Pθ1.	(5)
We define the preprocessor as a parameterized multivariate Gaussian distribution with diagonal co-
variance Pθ1 = {μθ1, Σθ1}, from which a latent representation is sampled for each input observation
Zi 〜N(μθ1 (oi), Σθ1 (oi)). The preprocessor's objective is to project each observation into a latent
space containing information about the achievement state of the goal, disregarding the irrelevant
information about the inherent differences between the expert’s and agent’s domains. The Gaussian
representation ensures that, for any input, the support over the possible latent representations Z is
infinite, moreover, it allows the model to directly reduce the information in any of the independent
dimensions of Z by increasing the corresponding variance in Σθ1 (oi).
Based on these latent representations, the invariant discriminator Sθ2 is tasked to output the dis-
criminator score for the observed behavior. To classify behavior at any time-step, Sθ2 takes as
input the concatenated sequence of the latent representations of the four most recent observations,
Zt = ConCat(Zt, Zt-1, zt-2, zt-3). Feeding a concatenation of the latent representations over mul-
tiple time-steps serves the purpose of facilitating the recovery of information regarding the true
unobserved state of the POMDP from the observations. It also allows the discriminator to reason di-
rectly with goal-completion progress throughout different consecutive observations. To understand
the necessity of this practice, consider a navigation problem where the agent’s task is to reach a
target position. Only given access to information about multiple visual observations showing the
location of the agent at different time-steps, the discriminator will be able to assess if the agent is
approaching the target and retrieve higher-order state information about its motion.
Both the preprocessor and invariant discriminator are trained end-to-end through the reparameteri-
zation trick (Kingma & Welling, 2013), to optimize the GAIL objective JG to discern behavior from
the set of expert demonstrations BE against behavior from the set of recent agent observations Bπ :
argmaX Jg(Θ,Be,B∏) = arg maxEBE,Pθ1 log(Sθ2 (Zi))+ EBn,Pθ1 log(1 - Se? (Zi)).	(6)
θ	θ1	1
4.2	Mutual information constraints
To obtain an invariant latent space from the preprocessor’s output, we propose to enforce two
different constraints on the mutual information between the observation’s latent representations
Zi 〜Pθ1 (oi) and a corresponding set of domain labels. Each domain label di is a binary vari-
able representing whether the associated observation oi was collected in the expert POMDP, i.e.,
4
Published as a conference paper at ICLR 2021
di = 1oi∈BE∪BP.E. To estimate the mutual information, we make use of the MINE estimator and
utilize a statistics network Tφ optimized to maximize the objective in Eq. 4 between the latent
representations and the domain labels for the observations in BE and Bπ :
argmaxIφ(zi,di∖BE∪B∏) = argmaxEP@),p34)[Tφ(zi,di)]-log (EP@),p(Zi) [eTφ(Zimi)D .(7)
Expert demonstrations constraint. The first mutual information constraint is for the latent repre-
sentations of the observations from the union of BE with Bπ. We propose to constraint the estimated
mutual information of these latent representations with the corresponding domain labels to be less
than 1 bit: Iφ(zi, di|BE ∪ Bπ) < 1. We define two kinds of information that the preprocessor
Pθ1 can encode into the latent representations to aid the invariant discriminator Sθ2 in discerning
transitions ot:t-3 from BE and Bπ : (i) domain information, from the visual differences of the two
environments (labeled by di), or (ii) goal-completion information, from the expected progress shown
in the observations towards achieving the goal demonstrated by the expert in BE (represented by the
variable ci). By constraining the mutual information of the latent representations with the domain
labels di to be less than 1 bit, we prevent the invariant discriminator to exclusively rely on informa-
tion inherent to the domain origin to make its classification decision. Therefore, we force it to seek
goal-completion information about ci to fully optimize its objective from Eq. 6. We empirically
evaluate this constraint against tighter constraints in Appendix D.
Prior data constraint. An additional mutual information constraint is for the latent representations
of the observations from the union of prior data sets collected independently in both agent and
expert domains, BP.E and BP.π. The observations collected in these sets are expected to come from
observing both expert’s and agent’s domains, while neither expert or agent are attempting to perform
the target task. Hence, by assuming that the goal-completion levels observed in these two sets
approximately match, we can constraint the mutual information of the relative latent representations
with the domain labels to be near 0, namely Iφ(zi, di|BP.E ∪ BP.π) ≈ 0. This mutual information
constraint implicitly optimizes for a mapping which makes the distributions of latent representations
generated from the observations in the two prior sets of data equivalent. Hence, it allows for the
utilization of great amounts of cheaply collected unsupervised data to provide an additional learning
signal regarding the information which should be discarded by the preprocessor.
Comparison with prior efforts. Stadie et al. (2017) proposed to constraint the mutual information
between the encoded observations and the domain labels in BE∪Bπ to be 0. We argue that enforcing
such constraint would unnecessarily limit the information in the latent representations and impair
learning. This constraint assumes that some observable factor determining ci is independent of the
domain labels di, otherwise, no information about ci can be encoded in the latent representations.
However, given that in BE ∪ Bπ we have oi ∈ BE ⇔ di = 1, such assumption seldom holds, as it
requires the distributions of some goal-completion information about ci present in the observations
in BE and Bπ to exactly match. However, unlike this work, the algorithm proposed by Stadie et al.
(2017) does not attempt to enforce such constraint precisely. Instead, it simply penalizes a measure
proportional to the mutual information via a domain confusion loss with a fixed weight coefficient.
We further discuss the implications of this practice and provide a toy example where truly enforcing
such constraint would prevent any learning in Appendix A.
4.3	Off-policy learning
To learn effective behaviour, we combine our regularized DisentanGAIL discriminator with the off-
policy Soft-Actor Critic (SAC) algorithm by Haarnoja et al. (2018b). To optimize a parameterized
agent policy, πω , SAC maximizes the expected sum of entropy regularized pseudo-rewards:
arg max J(ω) = arg max Epπ (τ)
ω	ωω
T
ERD(θt, 0t-i,0t-2,0t-3) - α∏ω(at∣st)
t=0
(8)
5	Implementation
5.1	Enforcing the mutual information constraints
We implement two different techniques to enforce the mutual information constraints proposed in
Section 4.2, given a set of observations B with the corresponding domain labels. Both techniques
5
Published as a conference paper at ICLR 2021
make use of a single hyper-parameter Imax, which represents the upper limit on the estimated infor-
mation about the domain labels that we allow the latent representations to retain.
Adaptive penalty Lβ. The first technique is having a supplementary loss function for the preproces-
sor Pθ1, penalizing it proportionally to the estimated mutual information in the latent representations.
We use an adaptive parameter β to ensure the mutual information is within the desired range:
Lβ(θ1, B) = βIφ(zi, di|B).	(9)
We design our updates of β to follow a similar pattern to the adaptive penalty coefficient proposed
by Schulman et al. (2017), utilizing Imax and updating:
• β J β X 1.5, if Iφ(Zi,di∣B) >Imaχ	• β J β ÷ 1.5, if Iφ(Zi,di∣B) <Imaχ ÷ 2.
Dual penalty Lλ. The second technique consists in a different supplementary loss function penal-
izing the preprocessor Pθ1 proportionally to the violation of the upper limit. In this case, we ensure
constraint enforcement through the introduction of a non-negative Lagrange multiplier variable λ:
Lλ(θι, B) = λ (Iφ (Zi, di|B) - Imax)	(10)
where λ is updated to maximize Lλ, approximating dual gradient descent (Boyd et al., 2004):
λ J max (0, λ + α (Iφ (zi, di|B) - Imax)) .	(11)
In practice, the dual penalty enforces precisely the mutual information constraints, but the dual
variable λ stabilizes in more iterations than the adaptive parameter β . Our final implementation
makes use of the adaptive penalty when enforcing the expert demonstrations constraint with Imax =
0.99 and the dual penalty when enforcing the prior data constraint with Imax = 0.001. Hence, our
penalized discriminator objective augments the original discriminator objective in Eq. 6 as:
arg max JG(θ, BE ∪ Bπ) - Lβ(θ1, BE ∪ Bπ) - Lλ(θ1, BP.E ∪ BP.π).	(12)
θ
5.2	Domain information disguising
The discriminator Dθ is updated to maximize Eq. 12, given a mutual information estimate from
a fixed-sized statistics network Tφ . Hence, if the optimization of Tφ temporarily converges to a
sub-optimal local minimum, Dθ could encode domain information into the latent representations
z without the statistics network detecting it. We refer to this phenomenon as domain information
disguising, and we utilize two further techniques to prevent this issue.
Double statistics network. The first technique takes inspiration from the work of Van Hasselt et al.
(2016) and consists of learning independently two statistics neural networks, namely Tφ1 and Tφ2 .
Thus, the mutual information is estimated through taking the maximum prediction of the indepen-
dent models over the same set of observations, Iφ(zi, di|B) = max(Iφ1 (zi, di|B), Iφ2 (zi, di|B)).
This change makes it impractical for the discriminator to disguise domain information, as the gra-
dient from each of the regularization losses can only contain information about a single statistics
network at a time (from the max operation). Therefore, this gives a statistics network reaching a
sub-optimal local minimum the chance to recover, without affecting the mutual information esti-
mates to a great extent. This practice has also the benefit of providing a better prediction of the
current mutual information, counteracting the effects of epistemic uncertainty on the optimization.
Invariant discriminator regularization. The second technique comprises regularizing the invariant
discriminator Sθ2 to be approximately 1-Lipschitz. A great part of the GAN literature (Arjovsky
et al., 2017; Gulrajani et al., 2017; Miyato et al., 2018) showed the effectiveness of this practice
when regularizing discriminator networks. In our specific problem setting, it has the further benefit
of restricting the expressivity of the invariant discriminator, preventing it from capturing domain
information not captured by our mutual information estimator. To enforce this, we utilize spectral
normalization, a regularization technique proposed by Miyato et al. (2018).
DisentanGAIL trains all the models end-to-end, in three main learning steps: (i) Discriminator learn-
ing, where the discriminator’s parameters θ are updated to maximize Eq. 12; (ii) Mutual information
learning, where the statistics network’s parameters φ are updated to maximize Eq. 7; (iii) Agent
learning, where the learner’s parameters ω are updated to maximize Eq. 8 with SAC. We provide
further implementation details and a formal summary of the algorithm in Appendix B.
6
Published as a conference paper at ICLR 2021
Figure 2: Performance curves for the Inverted Pendulum (Top) and Reacher (Bottom) realms. Dis-
entanGAIL is the only algorithm consistently achieving a performance close to the ‘expert’ agent’s.
Table 1: Results summary for the Inverted Pendulum and Reacher environment realms
Algorithms evaluated:
No differences
DiferenCes between the agent and the expert domains
Embodiment
Appearance
Embodiment and appearance
DisentanGAIL
Reacher Inverted Pendulum Reacher Inverted Pendulum Reacher Inverted Pendulum Reacher Inverted Pendulum
0.973 ± 0.074
1.021 ± 0.023
DisentanGAIL (No prior) 1.004 ± 0.012 1.015 ± 0.023
TPIL	0.251 ± 0.111 0.812 ± 0.162
0.941 ± 0.045 0.954 ± 0.081
TPIL (×5 experience)
DisentanGAIL (DCL)
No regularization
0.683 ± 0.158
0.894 ± 0.134
0.988 ± 0.042
1.024 ± 0.025
1.024 ± 0.025
1.018 ± 0.038
0.847 ± 0.064
0.185 ± 0.079
0.493 ± 0.195
0.867 ± 0.071
0.290 ± 0.187
0.914 ± 0.134
0.218 ± 0.191
0.331 ± 0.279
0.889 ± 0.159
0.635 ± 0.230
0.885 ± 0.064 0.894 ± 0.231
0.586 ± 0.143
0.278 ± 0.217
0.585 ± 0.256
0.550 ± 0.146
0.200 ± 0.176
0.794 ± 0.234
0.309 ± 0.122
0.519 ± 0.281
0.826 ± 0.194
0.677 ± 0.178
0.860 ± 0.081 0.918 ± 0.115
0.578 ± 0.160
0.235 ± 0.154
0.626 ± 0.282
0.523 ± 0.177
0.186 ± 0.136
0.887 ± 0.195
0.254 ± 0.199
0.313 ± 0.266
0.786 ± 0.288
0.682 ± 0.182
6 Experiments
To evaluate our algorithm, we design six different environment realms, simulated with MujoCo
(Todorov et al., 2012), extending the environments from BroCkman et al. (2016): Inverted Pendu-
lum, Reacher, Hopper, Half-Cheetah, 7DOF-Pusher and 7DOF-Striker. We define an environment
realm as a set of environments with a shared semantiC goal but with signifiCant differenCes in terms
of appearanCe and agent embodiment. For eaCh of the experiments, we seleCt a source environment
and a target environment within one environment realm. Thus, we train an ‘expert’ agent and ColleCt
a set of visual trajeCtories in the source environment. An ‘observer’ agent will then use these visual
trajeCtories to perform imitation in the target environment, without aCCess to the reward funCtion. In
all our experiments, eaCh epoCh Corresponds to the ‘observer’ agent ColleCting 1000 time-steps of
experienCe in the target environment. We report at eaCh epoCh the mean and standard error over five
experiments of the maximum expeCted Cumulative reward reCorded so far. We obtain the expeCted
Cumulative reward by averaging the performanCe of the ‘observer’ agent over five trajeCtories. We
sCale the Cumulative rewards suCh that 0 represents the performanCe from random behavior, and 1
represents the performanCe obtained by the ‘expert’ agent. We provide a detailed desCription of the
different environment realms in Appendix C.
Can DisentanGAIL efficiently solve the problem of observational imitation with both appear-
ance and embodiment mismatches? We first evaluate the performanCe of our algorithm on the
7
Published as a conference paper at ICLR 2021
Figure 3: Performance curves for the Hopper (Top-left), Half-Cheetah (Bottom-left), 7DOF-Pusher
(Top-right) and 7DOF-Striker (Bottom-right) environment realms.
Inverted Pendulum and Reacher realms. We test eight different combinations of source and target
environments for each of these realms. We allow the ‘observer’ agent to train for a maximum of
20 epochs. To evaluate the effectiveness of our proposed constraints and techniques in solving the
problem of observational imitation, we compare the performance of the following algorithms:
•	DisentanGAIL: The full proposed algorithm, as described in Section 5.
•	DisentanGAIL without prior data (No prior): DisentanGAIL without the prior data constraint.
•	TPIL: The original implementation of the algorithm from Stadie et al. (2017).
•	DisentanGAIL with domain confusion loss (DCL): Re-implementation of the domain confusion
loss by Stadie et al. (2017) applied to DisentanGAIL, substituting the proposed constraints.
•	No latent representation regularization (No regularization): DisentanGAIL model without any
loss or constraint to prevent encoding domain information in its latent representations.
We present the performance curves in Fig. 2 and a summary of the results in Table 1. Particularly,
the full DisentanGAIL algorithm outperforms all other algorithms when considering any domain
difference and consistently achieves a performance close to the ‘expert’ agent. In comparison, TPIL
severely under-performs, even when evaluated given five times the amount of experience. Applying
the domain confusion loss to DisentanGAIL significantly and consistently deteriorates the perfor-
mance, validating the effectiveness of our proposed constraints. However, this version of Disen-
tanGAIL still vastly outperforms the original TPIL implementation, underlying the superiority of
our proposed model and optimization. DisentanGAIL with no prior data, performs well in most
experiments, but under-performs when faced with drastic changes in domain appearance, indicat-
ing that the utilization of sets of prior data is important when strong visual cues about environment
correspondences are missing. We report additional ablation studies for our model in Appendix D.
Can DisentanGAIL scale to more challenging, high dimensional control tasks? We evaluate
our algorithm on the four remaining realms, which consist of substantially harder problems, nar-
rowing the evaluation gap with agent-centric imitation algorithms. We refer to the environments in
these realms as ‘high dimensional’ since their state and action spaces are significantly larger than
the state and action spaces of the environments explored in prior work making use of the domain
confusion loss (Stadie et al., 2017; Okumura et al., 2020; Choi et al., 2020). Namely, we explore
two locomotion realms, Hopper and Half-Cheetah, and two manipulation realms, 7DOF-Pusher and
8
Published as a conference paper at ICLR 2021
Table 2: Results summary for the ‘high dimensional’ environment realms
Environment realms
Algorithms evaluated:	Hopper	Half-Cheetah	7DOF-Pusher	7DOF-Striker
DisentanGAIL	0.749 ± 0.026	0.712 ± 0.036	0.901 ± 0.044	0.921 ± 0.061
DisentanGAIL (No prior)	0.622 ± 0.051	0.660 ± 0.231	0.677 ± 0.072	0.707 ± 0.150
TPIL	0.362 ± 0.057	0.066 ± 0.020	-0.020 ± 0.068	0.081 ± 0.065
DisentanGAIL (DCL)	0.619 ± 0.036	0.486 ± 0.058	0.747 ± 0.054	0.554 ± 0.134
No regularization	0.543 ± 0.039	0.247 ± 0.052	0.657 ± 0.080	0.504 ± 0.069
No regularization (source)	0.866 ± 0.100	0.914 ± 0.041	0.901 ± 0.044	0.837 ± 0.038
7DOF-Striker. We use DisentanGAIL to perform observational imitation with the ‘source’ and ‘tar-
get’ environments differing greatly both in terms of appearance and agent embodiment, as detailed
in Appendix C. We compare DisentanGAIL with the previously-introduced baselines. To provide
an upper bound on the expected performance of DisentanGAIL, we additionally evaluate the No la-
tent representation regularization baseline with the ‘observer’ agent learning in the original ‘source’
environment, i.e., imitating with no domain differences.
We present the performance curves in Fig. 3 and a summary of the results in Table 2. Remarkably,
DisentanGAIL is able to recover close to the expert’s performance in both manipulation realms,
with at least the same efficiency as the No latent representation regularization baseline learning in
the ‘source’ environment. In the locomotion realms, the performance appears to converge more
slowly to a similar but lower value than the expert. We hypothesize this is because the main objec-
tives in the locomotion realms are based on the agents continuously executing a particular stream
of actions rather than reaching a target state. Thus, the analyzed shifts in agent’s embodiment, even
modifying the action-spaces dimensionality, strongly increase the discriminator ambiguity on re-
warding the best possible way to solve the tasks. The performance gap of DisentanGAIL with the
rest of the baselines is considerably greater than in the previous set of experiments. In particular,
TPIL and the No latent representation regularization baseline fail to recover meaningful behavior in
any experiment. Similarly, applying the domain confusion loss to DisentanGAIL degrades consid-
erably the performance across all problems. Additionally, removing the prior data constraint from
DisentanGAIL also appears to degrade the performance. Yet, DisentanGAIL with no prior data still
outperforms all other baselines in three environment realms and is able to almost match the full
DisentanGAIL performance in the Half-Cheetah realm. These results highlight the complexity of
performing observational imitation in high dimensional environments and show the effectiveness of
our proposed constraints and optimization.
7 Conclusion
We proposed DisentanGAIL - a novel algorithm to effectively solve the problem of observational
imitation. Our method makes use of two mutual information constraints for a latent representa-
tion inside the discriminator network to encode goal-completion information and discard domain
information about the observations. Unlike prior work, our experiments show DisentanGAIL’s
effectiveness at dealing with various domain differences, both in terms of environment appear-
ance and agent embodiment, and at scaling to more complex high dimensional tasks. We believe
our work might have strong implications for future real-world imitation learning, as it could al-
low users to teach agents new tasks by simply being observed, leading to natural human-robot
interactions. To facilitate future efforts, we share the code for our algorithms and environments:
https://github.com/Aladoro/domain-robust-visual-il.
Acknowledgments
Edoardo Cetin would like to acknowledge the support from the Engineering and Physical Sciences
Research Council [EP/R513064/1].
9
Published as a conference paper at ICLR 2021
References
Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In
Proceedings of the twenty-first international conference on Machine learning, pp. 1. ACM, 2004.
Martin Arjovsky and Leon Bottou. Towards principled methods for training generative adversarial
networks. arxiv e-prints, art. arXiv preprint arXiv:1701.04862, 2017.
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint
arXiv:1701.07875, 2017.
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron
Courville, and Devon Hjelm. Mutual information neural estimation. In International Conference
on Machine Learning,pp. 531-540, 2018.
Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge uni-
versity press, 2004.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Sungho Choi, Seungyul Han, Woojun Kim, and Youngchul Sung. Cross-domain imitation learning
with a dual structure. arXiv preprint arXiv:2006.01494, 2020.
Monroe D Donsker and SR Srinivasa Varadhan. Asymptotic evaluation of certain markov process
expectations for large time, i. Communications on Pure and Applied Mathematics, 28(1):1-47,
1975.
Chelsea Finn, Paul Christiano, Pieter Abbeel, and Sergey Levine. A connection between generative
adversarial networks, inverse reinforcement learning, and energy-based models. arXiv preprint
arXiv:1611.03852, 2016a.
Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control
via policy optimization. In International Conference on Machine Learning, pp. 49-58, 2016b.
Guido Gioioso, Gionata Salvietti, Monica Malvezzi, and Daniele Prattichizzo. An object-based
approach to map human hand synergies onto robotic hands with dissimilar kinematics. Robotics:
Science and Systems VIII, pp. 97-104, 2012.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of wasserstein gans. In Advances in neural information processing systems, pp.
5767-5777, 2017.
Abhishek Gupta, Clemens Eppner, Sergey Levine, and Pieter Abbeel. Learning dexterous manip-
ulation for a soft robotic hand from human demonstrations. In 2016 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS), pp. 3786-3793. IEEE, 2016.
Abhishek Gupta, Coline Devin, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Learning invariant
feature spaces to transfer skills with reinforcement learning. arXiv preprint arXiv:1703.02949,
2017.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-
policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint
arXiv:1801.01290, 2018a.
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and appli-
cations. arXiv preprint arXiv:1812.05905, 2018b.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in neural
information processing systems, pp. 4565-4573, 2016.
Kun Ho Kim, Yihong Gu, Jiaming Song, Shengjia Zhao, and Stefano Ermon. Cross domain imita-
tion learning. arXiv preprint arXiv:1910.00105, 2019.
10
Published as a conference paper at ICLR 2021
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Justin B Kinney and Gurinder S Atwal. Equitability, mutual information, and the maximal informa-
tion coefficient. Proceedings of the National Academy of Sciences ,111(9):3354-3359, 2014.
Ilya Kostrikov, Kumar Krishna Agrawal, Debidatta Dwibedi, Sergey Levine, and Jonathan Tomp-
son. Discriminator-actor-critic: Addressing sample inefficiency and reward bias in adversarial
imitation learning. 2018.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
YuXuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Imitation from observation: Learn-
ing to imitate behaviors from raw video via context translation. In 2018 IEEE International Con-
ference on Robotics and Automation (ICRA), pp. 1118-1125. IEEE, 2018.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforcement learning. In Icml,
volume 1, pp. 2, 2000.
Ryo Okumura, Masashi Okada, and Tadahiro Taniguchi. Domain-adversarial and-conditional state
space model for imitation learning. arXiv preprint arXiv:2001.11628, 2020.
Xue Bin Peng, Angjoo Kanazawa, Sam Toyer, Pieter Abbeel, and Sergey Levine. Variational dis-
criminator bottleneck: Improving imitation learning, inverse rl, and gans by constraining infor-
mation flow. arXiv preprint arXiv:1810.00821, 2018.
Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. In Advances in neural
information processing systems, pp. 305-313, 1989.
Dean A Pomerleau. Efficient training of artificial neural networks for autonomous navigation. Neu-
ral computation, 3(1):88-97, 1991.
Nathan D Ratliff, J Andrew Bagnell, and Martin A Zinkevich. Maximum margin planning. In
Proceedings ofthe 23rd international conference on Machine learning, pp. 729-736. ACM, 2006.
StePhane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and StrUc-
tured prediction to no-regret online learning. In Proceedings of the fourteenth international con-
ference on artificial intelligence and statistics, PP. 627-635, 2011.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and PhiliPP Moritz. Trust region
Policy oPtimization. In International conference on machine learning, PP. 1889-1897, 2015.
John Schulman, FiliP Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
Proximal Policy oPtimization algorithms.	CoRR, abs/1707.06347, 2017. URL
http://arxiv.org/abs/1707.06347.
Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, Sergey
Levine, and Google Brain. Time-contrastive networks: Self-suPervised learning from video. In
2018 IEEE International Conference on Robotics and Automation (ICRA), PP. 1134-1141. IEEE,
2018.
Pratyusha Sharma, DeePak Pathak, and Abhinav GuPta. Third-Person visual imitation learning via
decouPled hierarchical controller. In Advances in Neural Information Processing Systems, PP.
2593-2603, 2019.
11
Published as a conference paper at ICLR 2021
Laura Smith, Nikita Dhawan, Marvin Zhang, Pieter Abbeel, and Sergey Levine. Avid: Learning
multi-stage tasks via pixel-level translation of human videos. arXiv preprint arXiv:1912.04443,
2019.
Bradly C Stadie, Pieter Abbeel, and Ilya Sutskever. Third-person imitation learning. arXiv preprint
arXiv:1703.01703, 2017.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033.
IEEE, 2012.
Faraz Torabi, Garrett Warnell, and Peter Stone. Recent advances in imitation learning from obser-
vation. arXiv preprint arXiv:1905.13566, 2019.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In Thirtieth AAAI conference on artificial intelligence, 2016.
Markus Wulfmeier, Peter Ondruska, and Ingmar Posner. Maximum entropy deep inverse reinforce-
ment learning. arXiv preprint arXiv:1507.04888, 2015.
Brian D Ziebart, Andrew Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse
reinforcement learning. 2008.
Konrad Zolna, Scott Reed, Alexander Novikov, Sergio Gomez Colmenarej, David Budden, Serkan
Cabi, Misha Denil, Nando de Freitas, and Ziyu Wang. Task-relevant adversarial imitation learn-
ing. arXiv preprint arXiv:1910.01077, 2019.
12
Published as a conference paper at ICLR 2021
A Alternative expert demonstrations constraint
Table 3: Example visual trajectories collected in the sample task
visual trajectories	{x0y0, x1y1, x2y2}	
BE:	{10,11,11} {10, 11, 11} {10, 11, 11} {10,11,11}
Bπ:	{00,00,00} {00, 01, 00} {00, 00, 01} {00, 00, 00}
Previous work (Stadie et al., 2017) proposed to optimize the GAIL objective described in section 6,
subject to constraining the mutual information to be 0:
arg max JG(θ, BE ∪ Bπ) s.t. I(zi, di|BE ∪ Bπ) = 0.
θ
In practice, however, the algorithm proposed by Stadie et al. (2017) enforces such constraint very
loosely via a domain confusion loss. This is achieved by introducing a second classifier, Cφ, on top
of the preprocessor Pθ1 , to predict the domain labels di from the latent representations zi .
JDCL (φ, θ1, BE, Bπ) = EBE,Pθ1 log(Cφ(zi)) + EBπ,Pθ1 log(1 - Cφ(zi)).
The GAIL objective JG is then augmented by the domain confusion loss, JDCL, where the prepro-
cessor is adversarially trained to minimize the information about the domain labels di useful for Cφ .
This optimization is then regulated by a fixed weight coefficient λ:
arg max arg min JG(θ, BE ∪ Bπ) - λJDCL(φ, θ1, BE, Bπ).
θφ
As a consequence, in practice the domain confusion loss acts more as a heuristic to minimize the
domain labels information, contained in the single latent representations, rather than attempting to
enforce a precise constraint. Below, we provide a toy example where truly enforcing I(zi , di) = 0
would prevent the preprocessor from encoding any useful information about the observations.
Consider a simple task where the objective is for an agent to reach and remain in a target state. In
this setting, we let the agent and expert POMDPs differ in their observation spaces. Specifically, we
define the observations collected to be composed of two binary variables, oi = xiyi . The value of
the first variable xi is 1 for any observation in the expert POMDP, and it is 0 for any observation
in the agent POMDP. The value of the second variable yi is 1 if the visited state is a target state,
and it is 0 otherwise. Therefore, for any observation oi the first binary variable xi contains domain
information about di which should be discarded (as xi = di), while the second binary variable yi
contains useful goal-completion information about ci which should be encoded in zi .
Consider an instance of this problem with a task horizon of 3 and with four visual trajectories in BE
and Bπ described in Table 3. In this example, unlike the visual trajectories collected by the agent
in Bπ, the expert demonstrations in BE accomplish the goal of this task as they show successfully
reaching and remaining in a target state. Therefore, the distribution of goal states encountered in
the observations from BE and Bπ are different, and consequently, yi is not statistically independent
from xi . We show this by computing the conditional probabilities:
p(xi = 1|yi = 1) = 4/5 6= p(xi = 1|yi = 0) = 2/7 ⇒ p(xi = 1|yi) 6= p(xi = 1) = 1/2.
Thus, the observable goal completion information about ci , present in yi , are not statistically inde-
pendent from the domain labels di .
13
Published as a conference paper at ICLR 2021
For simplicity, consider a deterministic preprocessor encoding the latent representations as zi =
P (oi) (as proposed by Stadie et al. (2017)). Enforcing I(zi, di) = 0 means that the value of zi
must be independent of di, and since di = xi, we must have zi = P(oi) = P (xyi)∀x = Py(yi).
However, we claim that this also implies that I(zi , ci) = 0. We can easily show this by contra-
diction, assume that I(zi , ci) > 0, since yi is the only observable source of information about ci
then Py(yi = 0) 6= Py(yi = 1). Therefore, Py must be invertible or in other words there exists a
function such that Py-1(Py(yi)) = yi∀yi. However, we then have that p(di|zi) = p(di|Py-1(zi)) =
p(di|yi) = p(xi|yi) 6= p(xi) = p(di), therefore zi is not independent of di and we must have
I(zi, di) 6= 0.
B	Algorithm details
B.1	Prior data
The prior data sets utilized to enforce the prior data constraint are collected by executing random
behavior in both ‘source’ and ‘target’ environments. To add diversity to the discriminator’s learn-
ing signal, we also use samples from both prior sets as additional negative examples for JG . The
baselines making use of the domain confusion loss also make use of prior data, analogously to how
‘failure’ data is used in the original TPIL algorithm (Stadie et al., 2017). On the other hand, the
baselines DisentanGAIL with no prior data and No latent representation regularization are evalu-
ated without access to prior data.
B.2	Discriminator
Given an observation o%, to sample the latent representation Zi 〜 N 皿、(θi), ∑θγ (θi)) We flatten
the output of the preprocessor and split the resulting K-dimensional vector in two halves. We utilize
the first half of the variables to obtain the latent representation’s mean, While We apply a Tanh
nonlinearity, exponentiate and scale the second half to obtain the latent representation’s covariance:
σθ1 (oi) = Pθ1 (oi)1:K/2,
Σθ1(oi) = diag(exp(tanh(Pθ1 (oi)K/2:K)) ÷ 2)).
To obtain a non-stochastic learning signal for the policy, When calculating the pseudo-reWard
RD(oi, oi-1, oi-2, oi-3) = log(Dθ (oi, oi-1, oi-2, oi-3)) - log(1 - Dθ(oi, oi-1, oi-2, oi-3)) We
set the Gaussian noise to zero, equivalently substituting Dθ(oi, oi-1, oi-2, oi-3) With:
Dθdet(oi, oi-1, oi-2, oi-3) = Sθ2 (concat(σθ1 (oi), σθ1 (oi-1), σθ1 (oi-2), σθ1 (oi-3))).
B.3	Training specifications
The discriminator loss from Eq. 12 and the MINE estimator objective from Eq. 7 are approximated
utilizing batches of transitions of observations b, sampled uniformly from the corresponding sets of
visual trajectories B. Specifically, for all optimizations, We set the batch size |b| = 128. We utilize
a fixed size agent set of visual trajectories Bπ and evict old transitions When reaching full capacity,
thus, effectively acting as a replay buffer (similarly to Kostrikov et al. (2018)). Throughout all the ex-
periments, We utilize the same 2 hidden-layer fully-connected policy and Q-netWorks With 256 units
and ReLU nonlinearities. We keep other model architectures structurally consistent, With a fully-
convolutional preprocessor Pθ1 , a fully-connected invariant discriminator Sθ2 and fully-connected
statistics netWorks Tφi. We only vary the depth of the models and the number of filters/units in each
layer depending on the environment realm. To avoid having a biased pseudo-reWard Which could
provide a learning signal to the agent even Without any meaningful discriminator (Kostrikov et al.,
2018), We modify the environments by removing terminal states. Thus, each collected visual trajec-
tory has a fixed length, equal to the task-horizon ∣τ|. We provide the utilized environment-specific
hyper-parameters in Table 4, Where We specify the buffer sizes in terms of total/maximum number
of observations.
We train each model through the Adam optimizer (Kingma & Ba, 2014) With a unique learning
rate α = 0.001 and momentum parameters β1 = 0.9, β2 = 0.999. We alternate the collection
of a single episode using the current policy πω , With repeating (i) Discriminator learning and (ii)
14
Published as a conference paper at ICLR 2021
Table 4: Environment-realm specific hyper-parameters
Realm	Pθ1	Sθ2	Tφ	|BE|	|BP.x|	∣B∏ I	|T |
Inverted Pendulum	2 × {16 × (3, 3)-conv, Tanh, (2, 2)-MaxPool} {1 × (3, 3)-conv, (2, 2)-MaxPool}	2 × {32-FC, ReLU} {1-FC, Sigmoid}	2 × {32-FC, Tanh} {1-FC}	10000	10000	10000	50
Reacher	2 × {16 × (3, 3)-conv, Tanh, (2, 2)-MaxPool} {1 × (3, 3)-conv, (2, 2)-MaxPool}	2 × {32-FC, ReLU} {1-FC, Sigmoid}	2 × {32-FC, Tanh} {1-FC}	10000	10000	10000	50
Hopper	{16 × (3, 3)-conv, Tanh, (2, 2)-MaxPool} {24 × (3, 3)-conv, Tanh, (2, 2)-MaxPool} {32 × (3, 3)-conv, Tanh, (2, 2)-MaxPool} {48 × (3, 3)-conv, (2, 2)-MaxPool}	2 × {100-FC, ReLU} {1-FC, Sigmoid}	2 × {128-FC, Tanh} {1-FC}	20000	20000	100000	200
Half-Cheetah	{16 × (3, 3)-conv, Tanh, (2, 2)-MaxPool} {24 × (3, 3)-conv, Tanh, (2, 2)-MaxPool} {32 × (3, 3)-conv, Tanh, (2, 2)-MaxPool} {48 × (3, 3)-conv, (2, 2)-MaxPool}	2 × {100-FC, ReLU} {1-FC, Sigmoid}	2 × {128-FC, Tanh} {1-FC}	20000	20000	100000	200
7DOF-Pusher	{24 × (3, 3)-conv, Tanh, (2, 2)-MaxPool} {32 × (3, 3)-conv, Tanh, (2, 2)-MaxPool} {40 × (3, 3)-conv, Tanh, (2, 2)-MaxPool} {64 × (3, 3)-conv, (2, 2)-MaxPool}	2 × {100-FC, ReLU} {1-FC, Sigmoid}	2 × {128-FC, Tanh} {1-FC}	10000	10000	100000	200
7DOF-Striker	{24 × (3, 3)-conv, Tanh, (2, 2)-MaxPool} {32 × (3, 3)-conv, Tanh, (2, 2)-MaxPool} {40 × (3, 3)-conv, Tanh, (2, 2)-MaxPool} {64 × (3, 3)-conv, (2, 2)-MaxPool}	2 × {100-FC, ReLU} {1-FC, Sigmoid}	2 × {128-FC, Tanh} {1-FC}	10000	10000	100000	200
Mutual information learning for as many iterations as the number of time-steps collected. Then,
by averaging the mutual information estimates accumulated from performing (ii), we update the
coefficients β and λ. Finally, we also perform (iii) Agent learning for as many iterations as the
number of time-steps collected. We utilize the same agent set of visual trajectories Bπ in all different
learning steps. A formal summary of DisentanGAIL is reported below in Algorithm 1.
Algorithm 1 DisentanGAIL
1 2	Input: expert demonstrations BE, prior expert and agent observations Bp.e, Bp.∏ Initialize Bn JQ
3	: for i = 1, 2, ..., do
4	:	Sample τ = (ot, at, st)tT=1 with πω
5	:	Bπ J Bπ ∪ τ
6	for j = 1,2,…,∣τ| do
7	:	Sample bE, bπ, bP.E, bP.π ∈ BE, Bπ, BP.E, BP.π	. Discriminator learning
8	:	LD = -JG(θ, bE, bπ) + Lβ(θ1, bE ∪ bπ) + Lλ(θ1, bP.E ∪ bP.π)
9	:	Update θ with Adam
10	:	for n = 1, 2 do	. Mutual information learning
11	:	Sample bE, bπ , bP.E, bP.π ∈ BE, Bπ , BP.E, BP.π
12	:	In = Iφn(zi, di|bE ∪ bπ)
13	1P = 1φn (Zi,di|bP.E ∪ bP.π)
14	:	LI = -In - InP
15	:	Update φn with Adam
16	:	Ijest = max(I1, I2)
17	:	IjP.est = max(I1P, I2P)
18	Update β using ∣1∣ PjT=I IjSt
19	Update λ using 煮 Pj= 1 Ip∙est
20	Update ∏ω with SAC, sampling from Bn for ∣τ| steps	. Agent learning
C Environments description
We evaluate the algorithms on six different environment realms designed to test the proposed meth-
ods for a diverse range of task difficulties and domain difference, extending the environments in
OpenAI Gym (Brockman et al., 2016):
15
Published as a conference paper at ICLR 2021
Table 5: Description of the implemented environment realms
Realm	Environment	Characteristics dim(A)	dim(O)	Semantic goal
Inverted Pendulum 1-Linked Inverted Pendulum 2-Linked Inverted Pendulum 1-Linked Colored Inverted Pendulum 2-Linked Colored Inverted Pendulum		1 link, standard coloring	1 2 links, standard coloring	1 1 link, alternative coloring	1 2 link, alternative coloring	1	32 × 32 × 3	Keeping the links vertically balanced above the moving cart.
Reacher	2-Linked Reacher 3-Linked Reacher 2-Linked Tilted Reacher 3-Linked Tilted Reacher	2 links, standard camera	2 3 links, standard camera	3 2 links, camera tilted 14.1 degrees	2 3 links, camera tilted 14.1 degrees	3	48 × 48 × 3 Approaching and fixating on a target goal with the end effector.
Hopper	Hopper Flexible Hopper	all movable joints, standard coloring 3 additional thigh joint, alternative thigh coloring 4	64 × 64 × 3 Hopping forward without falling.
Half-Cheetah	Half-Cheetah Immobilized feet Half-Cheetah	all movable joints, standard coloring 6 fixed feet joints, alternative feet coloring 4	64 × 64 × 3	Maximizing forward distance covered.
7DOF-Pusher	7DOF-Pusher Demonstrator 7DOF-Pusher	standard link sizes, standard coloring, 3-linked hand 7 alternative link sizes, alternative coloring, 4-linked hand 7	48 × 48 × 3 Grabbing and pushing an item to a target goal.
7DOF-Striker	7DOF-Striker Demonstrator 7DOF-Striker	standard link sizes, standard coloring 7 alternative link sizes, alternative coloring 7	48 × 48 × 3	Striking a ball into a target goal.
•	Inverted Pendulum: This environment realm consists of four variations of the original balancing
task. The variations explore changing the color of the agent and adding a second link to be
balanced on top of the moving cart.
•	Reacher: This environment realm consists of four variations of the original 2-D reaching task. The
variations explore augmenting the number of joints and changing the observer’s camera recording
angle.
•	Hopper: This environment realm consists of two environments, including the original Hopper
environment and an alternative version, in which the agent has an additional joint splitting its
thigh link in two, with the new link also appearing in a different color.
•	Half-Cheetah: This environment realm consists of two environments, including the original Half-
Cheetah environment and an alternative version, in which the agent has immobilized feet joints
and the connected links appearing in a different color.
•	7DOF-Pusher/7DOF-Striker: Each of these environment realms consists of two environments,
including the original Pusher/Striker environments and an alternative version, in which the agent’s
model is modified in its appearance and structural configuration, to make it resemble a very sim-
plified human operator.
To collect observations, we render the environments with Mujoco and down-scale the renderings
to different dimensions with the purpose of having efficient representations but still preserving the
relevant details about the observations. We provide further environment-specific descriptive details
in Table 5.
D Supplementary results
D. 1 Latent representations coupling
After performing the reported experiments, we utilize the learnt models to understand what features
are encoded into our constrained latent representations zi . Particularly, we use the output of the
trained preprocessors to map observations between the ‘expert’ agent’s and the ‘observer’ agent’s
sets of visual trajectories. We achieve this by taking four different observations from Bπ and com-
puting their latent representations. Then, we match these observations with the four observations
in BE having the closest latent representations, computed by taking the relative L1-distances. We
repeat this process, matching four observations in BP.π with four observations in BP.E. We show
the produced couplings for six different experiments with DisentanGAIL in Fig. 4. From the re-
sults, it can be inferred that the mutual information constraints successfully guide the preprocessor
to encode features which are agnostic to the agent’s embodiment and the environment’s appearance,
yet preserving information about the goal-completion levels displayed the observations. For the
Inverted Pendulum realm, the preprocessor appears to be encoding information about the relative
16
Published as a conference paper at ICLR 2021
Bp e	2-Linked Inverted Pendulum	Be
Bpe	3-Linked Reacher	Be
Bpe	Hopper	BE
Bp.e	Half-Cheetah	BE
Figure 4: Couplings produced by matching the observations collected in the ‘observer’ agent’s
environment with the observations collected in the ‘expert’ agent’s environment, to minimize the
L1-distance between the latent representations produced by DisentanGAIL. On the right we show
the results between the agent set of visual trajectories Bπ and the set of expert demonstrations BE,
and on the left we show the results between the prior set of agent observations BP.π and the prior
set of expert observations BP.E. From top to bottom, we show the results in the Inverted Pendulum,
Reacher, Hopper, Half-Cheetah, 7DOF-Pusher and 7DOF-Striker environment realms. We produce
the couplings in six sample observational imitation problems considering domain differences in
terms of both environment appearance and agent embodiment.
17
Published as a conference paper at ICLR 2021
Figure 5: Couplings produced by matching the observations collected in the ‘observer’ agent’s
environment with the observations collected in the ‘expert’ agent’s environment, to minimize the
L1-distance between the latent representations produced by DisentanGAIL with domain confusion
loss. This visualization is analogous to Fig. 4
18
Published as a conference paper at ICLR 2021
Figure 6: Average mutual information between the latent representations Z and the default rewards
rtrue throughout performing observational imitation, for differents value of the expert demonstra-
tion hyper-parameter Imax . We evaluate this measure for the experiments in the Reacher and In-
verted Pendulum environment realms considering domain differences in both appearance and em-
bodiment.
angle between the tip of the links and the moving cart, together with some information about the
cart’s position. For the Reacher realm, the preprocessor appears to be encoding information about
the position of the tip of the reacher, ignoring the orientation of the individual links. For the Hop-
per and Half-Cheetah realms, the preprocessor appears to be encoding the orientation of the joints
movable in both domains, together with the current height and position of the agent with respect to
the floor tiles, allowing the discriminator to assess whether the agent is hopping/advancing. For the
7DOF-Pusher and 7DOF-Striker realms, the preprocessor appears to be encoding the location of
the item/ball, together with the location and orientation of the agent’s hand actuator.
We also show the produced couplings for six different experiments with DisentanGAIL with domain
confusion loss in Fig. 5. From the results, it can be inferred that the features encoded by the domain
confusion loss are similar to the ones encoded by the mutual information constraints, yet not as
consistently interpretable. This is especially evident in the more challenging high dimensional envi-
ronments. Particularly, for the Hopper and Half-Cheetah realms, the preprocessor does not appear
to be always encoding the agent’s relative position to the floor tiles, but rather focusing either on the
angle of particular joints or the agent’s overall appearance. Additionally, for the 7DOF-Pusher and
7DOF-Striker realms, the preprocessor does not appear to be consistently encoding the location of
the item/ball, but rather focusing on some less interpretable feature about the agent’s appearance.
D.2 Expert demonstrations constraint
We evaluate the effects of enforcing tighter expert demonstration constraints on DisentanGAIL in
the low-dimensional environments. This is achieved by running our algorithm with lower values for
the hyper-parameter regulating the upper-limit on the estimated mutual information, Imax , in the
adaptive penalty loss Lβ (described in Section 5.1).
First, we analyze the effects that tighter constraints have on the amount of goal-completion in-
formation encoded in our latent representations Z. Particularly, for different values of I max, We
utilize MINE to estimate the mutual information between Z and default environment rewards『^.:
I(Z, rtrue). We argue that this measure is a good heuristic about the goal-completion information
contained in Z since rtrue can be effectively used to recover a policy to solve the task in all environ-
ments. Specifically, in Fig. 6, we show the average and standard deviation of the mutual information
collected throughout different experiments considering domain differences in both appearance and
embodiment. This data shows that there is a positive correlation between Imax and I(Z, rtrue), indi-
cating that tighter mutual information constraints are detrimental. This is explained since, especially
at the beginning of training, ci and di are highly dependent. Hence, looser constraint allow greater
amounts of information about Ci to be encoded within Z. These findings are also consistent with our
arguments from Appendix A.
19
Published as a conference paper at ICLR 2021
Figure 7: Performance curves from enforcing tighter expert demonstration constraints in the Inverted
Pendulum (Top) and Reacher (Bottom) environment realms.
20
Published as a conference paper at ICLR 2021
Table 6:	Results summary for DisentanGAIL with tighter expert demonstration constraints
DiferenceS between the agent and the expert domains
No differences	Embodiment	Appearance	Embodiment and appearance
Algorithms evaluated:	ReaCher Inverted Pendulum ReaCher Inverted Pendulum ReaCher Inverted Pendulum ReaCher Inverted Pendulum
DisentanGAIL-Imax = 0.99 0.973 ± 0.074 1.021 ± 0.023 0.941 ± 0.045 0.954 ± 0.081 0.885 ± 0.064^^0.894 ± 0.231 ^^0.860 ± 0.081 0.918 ± 0.115
DisentanGAIL-Imax = 0.75 0.983 ±	0.067	1.019 ± 0.021	0.841 ± 0.113	0.892 ± 0.131	0.903	±	0.064	0.887 ±	0.180	0.897 ± 0.056	0.772 ± 0.200
DisentanGAIL-Imax = 0.500.987 ±	0.045	1.013 ± 0.016	0.885 ± 0.104	0.853 ± 0.208	0.861	±	0.077	0.942 ±	0.131	0.837 ± 0.071	0.790 ± 0.234
DisentanGAIL-Imax = 0.25 0.975 ±	0.028	1.020 ± 0.025	0.927 ± 0.052	0.882 ± 0.173	0.861	±	0.088	0.930 ±	0.156	0.848 ± 0.055	0.720 ± 0.251
DisentanGAIL-Imax = 0.010.921 ±	0.094	0.992 ± 0.041	0.905 ± 0.057	0.790 ± 0.201	0.652 ±	0.188	0.589 ±	0.224	0.576 ± 0.141	0.630 ± 0.345
Table 7:	Results summary for DisentanGAIL with missing domain information disguising preven-
tion techniques
Differences between the agent and the expert domains
No differences	Embodiment	Appearance	Embodiment and appearance
Algorithms evaluated:	ReaCher Inverted Pendulum ReaCher Inverted Pendulum ReaCher Inverted Pendulum ReaCher Inverted Pendulum
DisentanGAIL	0.973 ± 0.074	1.021	± 0.023	0.941	± 0.045	0.954 ± 0.081	0.885 ± 0.064	0.894	± 0.231	0.860	±	0.081	0.918 ± 0.115
DisentanGAIL (No SN)	0.957 ± 0.084	1.018	± 0.024	0.844	±	0.105	0.937 ± 0.094	0.853 ± 0.092	0.871	± 0.153	0.861	±	0.049	0.864 ± 0.184
DisentanGAIL (No 2St) 0.982 ± 0.075	1.020	± 0.023	0.907	± 0.109	0.900 ± 0.156	0.857 ± 0.127	0.799	± 0.199	0.858	±	0.044	0.789 ± 0.182
DisentanGAIL (No Prev) 0.949 ± 0.105	1.002	± 0.022	0.866	± 0.056	0.969 ± 0.085	0.768 ± 0.115	0.778	± 0.235	0.763 ±	0.100	0.750 ± 0.225
Second, we analyze directly the effects that tighter constraints have on the performance of Disen-
tanGAIL. The performance curves for different values of Imax are shown in Fig. 7 and a summary
of the results is given in Table 6. Overall, DisentanGAIL appears to be quite robust to all settings
tested, excluding the extreme Imax = 0.01. In general, however, a lower mutual information upper-
limit appears to have a negative effect on the performance in most experiments, especially when the
‘expert’ agent’s embodiment and the ‘observer’ agent’s embodiment differ. This is likely because a
tighter constraint does not permit the discriminator to utilize enough information about single ob-
servations, thus, providing a less informative learning signal to finetune the agent’s behavior. The
effects of varying Imax appear to be less accentuated in the experiments performed in the ReaCher
realm. This is likely because the exploratory policy in this environment covers a greater range of
states than in the Inverted Pendulum realm, with a more diverse range of goal-Completion levels.
Thus, encoding features carrying goal-Completion information necessitates to carry less information
about the domain labels.
D.3 Domain information disguising
We also perform an ablation study to understand the effects of the techniques proposed to counteract
domain information disguising, described in Section 5.2. We compare the proposed DisentanGAIL
algorithm with alternative versions: (i) with no speCtral normalization (No SN) (ii) with no dou-
ble statistiCs network (No 2St) (iii) with no domain information disguising prevention (No Prev -
making use of neither spectral normalization or double statistics network). The performance curves
are shown in Fig. 8 and a summary of the results is given in Table 7. Overall, both techniques
contribute positively to the final performance. Particularly, the double statistics network appears to
have a slightly greater positive effect. This is especially evident in the Inverted Pendulum realm.
Additionally, removing spectral normalization from the invariant discriminator’s layers makes the
agent initially learn slightly faster, indicating that there might be a trade-off between convergence
speed and training stability.
D.4 Robustness to background differences
We examine whether DisentanGAIL’s performance is affected by larger visual domain differences,
solely concerning the background appearance. Particularly, most of the tested domain differences in
our environment realms involved changing the appearance and morphology of the agents themselves.
Hence, we test DisentanGAIL on two alternative target environments in the Hopper and 7DOF-
Pusher environment realms, with very distinct backgrounds from the relative source environments.
Particularly, the target environment of the Hopper realm has a much darker floor, where the tiles are
difficult to discern. Additionally, the target environment in the 7DOF-Pusher realm includes a green
table and a white floor, both of which differ greatly in appearance to the grey table and black floor
of the source environment. We compare the performance of DisentanGAIL performing imitation in
these two alternative target environments with the performance in the original target environments
to evaluate its robustness.
21
Published as a conference paper at ICLR 2021
—∙- DisentanGAIL
—e- DisentanGAIL with no spectral normalization
—DisentanGAIL with no double statistics network
-a- DisentanGAIL with no domain information disguising prevention
Figure 8: Performance curves for the domain information disguising ablation performed in the In-
verted Pendulum (Top) and Reacher (Bottom) environment realms.
—∙- DisentanGAIL
T- DisentanGAIL in original target environment
Figure 9: Performance curves for the Hopper (Left) and 7DOF-Pusher (Right) environment realms
in the alternative target environments.
22
Published as a conference paper at ICLR 2021
Table 8:	Results summary for the experiments considering further background domain differences
in the ‘target’ environments
Environment realms
Algorithms evaluated:
Hopper
7DOF-Pusher
DisentanGAIL
0.709 ± 0.078
0.835 ± 0.039
DisentanGAIL (original target) 0.749 ± 0.026	0.901 ± 0.044
We present the performance curves in Fig. 9 and a summary of the results in Table 8. The ob-
tained results show that background domain differences have a limited effect on DisentanGAIL’s
final performance. However, they have a more prominent effect on DisentanGAIL’s efficiency, mak-
ing it converge in an increased number of epochs. This is particularly noticeable in the Hopper
realm’s results. We hypothesize this is because in the new target environments there is a greater
amount of domain information that requires to be ‘disentangled’ from the useful goal-completion
information. For example, in the locomotion realms, the pre-processor encodes goal-completion
information about the relative position of the agent with respect to the floor tiles in the two envi-
ronments (as empirically suggested in Section D.1). In the new target environment of the Hopper
realm, this information needs to be also disentangled from domain information regarding the tiles’
appearance.
23