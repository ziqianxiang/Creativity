Published as a conference paper at ICLR 2021
Isometric Transformation Invariant and
Equivariant Graph Convolutional Networks
Masanobu Horie
University of Tsukuba,
Research Institute for Computational Science Co. Ltd.
horie@ricos.co.jp
Toshiaki Hishinuma & Yu Ihara
Research Institute for Computational Science Co. Ltd.
{hishinuma,ihara}@ricos.co.jp
Naoki Morita
University of Tsukuba,
Research Institute for Computational Science Co. Ltd.
morita@ricos.co.jp
Naoto Mitsume
University of Tsukuba
mitsume@kz.tsukuba.ac.jp
Ab stract
Graphs are one of the most important data structures for representing pairwise re-
lations between objects. Specifically, a graph embedded in a Euclidean space is
essential to solving real problems, such as physical simulations. A crucial require-
ment for applying graphs in Euclidean spaces to physical simulations is learning
and inferring the isometric transformation invariant and equivariant features in
a computationally efficient manner. In this paper, we propose a set of transfor-
mation invariant and equivariant models based on graph convolutional networks,
called IsoGCNs. We demonstrate that the proposed model has a competitive per-
formance compared to state-of-the-art methods on tasks related to geometrical and
physical simulation data. Moreover, the proposed model can scale up to graphs
with 1M vertices and conduct an inference faster than a conventional finite ele-
ment analysis, which the existing equivariant models cannot achieve.
1	Introduction
Graph-structured data embedded in Euclidean spaces can be utilized in many different fields such
as object detection, structural chemistry analysis, and physical simulations. Graph neural networks
(GNNs) have been introduced to deal with such data. The crucial properties of GNNs include
permutation invariance and equivariance. Besides permutations, isometric transformation invariance
and equivariance must be addressed when considering graphs in Euclidean spaces because many
properties of objects in the Euclidean space do not change under translation and rotation. Due to
such invariance and equivariance, 1) the interpretation of the model is facilitated; 2) the output of
the model is stabilized and predictable; and 3) the training is rendered efficient by eliminating the
necessity of data augmentation as discussed in the literature (Thomas et al., 2018; Weiler et al.,
2018; Fuchs et al., 2020).
Isometric transformation invariance and equivariance are inevitable, especially when applied to
physical simulations, because every physical quantity and physical law is either invariant or equiv-
ariant to such a transformation. Another essential requirement for such applications is computational
efficiency because the primary objective of learning a physical simulation is to replace a computa-
tionally expensive simulation method with a faster machine learning model.
In the present paper, we propose IsoGCNs, a set of simple yet powerful models that provide
computationally-efficient isometric transformation invariance and equivariance based on graph con-
volutional networks (GCNs) (Kipf & Welling, 2017). Specifically, by simply tweaking the definition
ofan adjacency matrix, the proposed model can realize isometric transformation invariance. Because
the proposed approach relies on graphs, it can deal with the complex shapes that are usually pre-
sented using mesh or point cloud data structures. Besides, a specific form of the IsoGCN layer can be
regarded as a spatial differential operator that is essential for describing physical laws. In addition,
we have shown that the proposed approach is computationally efficient in terms of processing graphs
1
Published as a conference paper at ICLR 2021
with up to 1M vertices that are often presented in real physical simulations. Moreover, the proposed
model exhibited faster inference compared to a conventional finite element analysis approach at the
same level of accuracy. Therefore, an IsoGCN can suitably replace physical simulations regarding
its power to express physical laws and faster, scalable computation. The corresponding implemen-
tation and the dataset are available online1.
The main contributions of the present paper can be summarized as follows:
•	We construct isometric invariant and equivariant GCNs, called IsoGCNs for the specified
input and output tensor ranks.
•	We demonstrate that an IsoGCN model enjoys competitive performance against state-of-
the-art baseline models on the considered tasks related to physical simulations.
•	We confirm that IsoGCNs are scalable to graphs with 1M vertices and achieve inference
considerably faster than conventional finite element analysis.
2	Related work
Graph neural networks. The concept of a GNN was first proposed by Baskin et al. (1997); Sper-
duti & Starita (1997) and then improved by (Gori et al., 2005; Scarselli et al., 2008). Although many
variants of GNNs have been proposed, these models have been unified under the concept of mes-
sage passing neural networks (Gilmer et al., 2017). Generally, message passing is computed with
nonlinear neural networks, which can incur a tremendous computational cost. In contrast, the GCN
developed by Kipf & Welling (2017) is a considerable simplification of a GNN, that uses a linear
message passing scheme expressed as
Hout = σ(AHinW),	(1)
where Hin (Hout) is an input (output) feature of the lth layer, A is a renormalized adjacency matrix
with self-loops, and W is a trainable weight. A GCN, among the variants of GNNs, is essential to
the present study because the proposed model is based on GCNs for computational efficiency.
Invariant and equivariant neural networks. A function f : X → Y is said to be equivariant
to a group G when f (g ∙ X)= g ∙ f (x), for all g ∈ G and X ∈ X, assuming that group G acts
on both X and Y. In particular, when f (g ∙ x) = f (x), f is said to be invariant to the group G.
Group equivariant convolutional neural networks were first proposed by Cohen & Welling (2016) for
discrete groups. Subsequent studies have categorized such networks into continuous groups (Cohen
et al., 2018), three-dimensional data (Weiler et al., 2018), and general manifolds (Cohen et al., 2019).
These methods are based on CNNs; thus, they cannot handle mesh or point cloud data structures as
is. Specifically, 3D steerable CNNs (Weiler et al., 2018) uses voxels (regular grids), which though
relatively easy to handle, are not efficient because they represent both occupied and non-occupied
parts ofan object (Ahmed et al., 2018). In addition, a voxelized object tends to lose the smoothness
of its shape, which can lead to drastically different behavior in a physical simulation, as typically
observed in structural analysis and computational fluid dynamics.
Thomas et al. (2018); Kondor (2018) discussed how to provide rotation equivariance to point clouds.
Specifically, the tensor field network (TFN) (Thomas et al., 2018) is a point cloud based rotation and
translation equivariant neural network the layer of which can be written as
H out i=Wll H(n)i+XX w lk (Xj-Xi) H (kj,	⑵
out,i	n,i	n,j
k≥0 j 6=i
k+l	J
Wlk(X) = X φlJk(kXk) X YJm(X/kXk)QlJkm,	(3)
J =|k-l|	m=-J
whereH (n),i(H oUt,i) is a type-l input (output) feature at the ith vertex, φlJk : R≥0 → R is a trainable
function, YJm is the mth component of the Jth spherical harmonics, and QlJkm is the Clebsch-
Cordan coefficient. The SE(3)-Transformer (Fuchs et al., 2020) is a variant of the TFN with self-
attention. These models achieve high expressibility based on spherical harmonics and message pass-
ing with nonlinear neural networks. However, for this reason, considerable computational resources
1 https://github.com/yellowshippo/isogcn- iclr2021
2
Published as a conference paper at ICLR 2021
are required. In contrast, the present study allows a significant reduction in the computational costs
because it eliminates spherical harmonics and nonlinear message passing. From this perspective,
IsoGCNs are also regarded as a simplification of the TFN, as seen in equation 14.
Physical simulations using GNNs. Several related studies, including those by Sanchez-Gonzalez
et al. (2018; 2019); Alet et al. (2019); Chang & Cheng (2020) focused on applying GNNs to learn
physical simulations. These approaches allowed the physical information to be introduced to GNNs;
however, addressing isometric transformation equivariance was out of the scope of their research.
In the present study, we incorporate isometric transformation invariance and equivariance into
GCNs, thereby, ensuring the stability of the training and inference under isometric transformation.
Moreover, the proposed approach is efficient in processing large graphs with up to 1M vertices that
have a sufficient number of degrees of freedom to express complex shapes.
3	Isometric Transformation invariant and equivariant graph
CONVOLUTIONAL LAYERS
In this section, we discuss how to construct IsoGCN layers that correspond to the isometric invariant
and equivariant GCN layers. To formulate a model, we assume that: 1) only attributes associated
with vertices and not edges; and 2) graphs do not contain self-loops. Here, G = (V, E) denotes a
graph and d denotes the dimension of a Euclidean space. In this paper, we refer to tensor as geomet-
ric tensors, and we consider a (discrete) rank-p tensor field H(P) ∈ RlVl×f×dp, where |V| denotes
the number of vertices and f ∈ Z+ (Z+ denotes the positive integers). Here, f denotes the number
of features (channels) of H(p), as shown in Figure 1 (a). With the indices, we denote Hi(;pg);k k ...k ,
where i permutes under the permutation of vertices and k1 , . . . , kp refers to the Euclidean repre-
Sentation∙ τhus, under the Permutation, π : Hi⅛1k2...kp → HΠ(i)w,kιk2...kp , and under orthogonal
transformation, U : Hi(;pg);k1k2...kp 7→ Pl1,l2,...,lp Uk1l1Uk2l2 ...UkplpHi(;pg);l1l2...lp.
3.1	Construction of an isometric adjacency matrix
Before constructing an IsoGCN, an isometric
adjacency matrix (IsoAM), which is at the
core of the IsoGCN concePt must be defined.
The Proof of each ProPosition can be found in
APPendix B.
An IsoAM G ∈ RlVl×lVl×d is defined as:
Rd 3 Gij;;: := X	Tijkl(xk - xl), (4)
k,l∈V,k6=l
Figure 1: Schematic diagrams of (a) rank-1 ten-
sor field H(I) with the number of features equal-
ing 2 and (b) the simplest case of Gj；；： =
δilδjk AijI(Xk ― Xl) = Aij (Xj - xi ).
where Gij；；： is a slice in the spatial index of G,
Xi ∈ Rd is the position of the ith vertex (rank-
1 tensor), and Tijkl ∈ Rd×d is an untrainable
transformation invariant and orthogonal trans-
formation equivariant rank-2 tensor. Note that
we denote Gij；；k to be consistent with the no-
tation of H(pg)k1k2 k because i and j permutes under the vertex permutation and k represents the
spatial index while the number of features is always 1. The IsoAM can be viewed as a weighted ad-
jacency matrix for each direction and reflects spatial information while the usual weighted adjacency
matrix cannot because a graph has only one adjacency matrix. If the size of the set {Gj；；： = 0}j
is greater than or equal to d, then it can be deemed to be a frame, which is a generalization of a
basis. For the simplest case, one can define Tijkl = δilδjkAijI (Figure 1 (b)), where δj is the Kro-
necker delta, A is the adjacency matrix of the graph, and I is the identity matrix that is the simplest
rank-2 tensor. In another case, Tijkl can be determined from the geometry of a graph, as defined
in equation 16. Nevertheless, in the bulk of this section, we retain Tijkl abstract to cover various
forms of interaction, such as position-aware GNNs (You et al., 2019). Here, G is composed of only
untrainable parameters and thus can be determined before training.
3
Published as a conference paper at ICLR 2021
Proposition 3.1. IsoAM defined in equation 4 is both translation invariant and orthogonal trans-
formation equivariant, i.e., for any isometric transformation ∀t ∈ R3, U ∈ O(d), T : x 7→ Ux +t,
T : Gij;;k 7→	Ukl Gij;;l.	(5)
l
Based on the definition of the GCN layer in the equation 1, let G * H(O) ∈ RlVl×f ×d denote the
convolution between G and the rank-0 tensor field H(O) ∈ RlVl×f (f ∈ Z+)as follows:
(G * H(0))i;g;k :=XGij;;kHj(;0g);.	(6)
j
With a rank-1 tensor field H⑴ ∈ RlVl×f ×d, let G Θ H⑴ ∈ RlVl×f and G Θ G ∈ RlVl×lVl denote
the contractions which are defined as follows:
(G Θ H(1))i;g;:= Xj,k Gij;;kHj(;1g);k, (G Θ G)il;;:= Xj,k Gij;;kGjl;k.	(7)
The contraction of IsoAMs G Θ G can be interpreted as the inner product of each component in the
IsoAMs. Thus, the subsequent proposition follows.
Proposition 3.2. The contraction of IsoAMs G Θ G is isometric transformation invariant, i.e., for
any isometric transformation ∀t ∈ R3, U ∈ O(d), T : x 7→ Ux + t, G Θ G 7→ G Θ G.
With a rank-p tensor field H(P) ∈ RlVl×f ×dp, let GXH(P) ∈ RlVl×f ×d1+p. and GXG ∈ RlVl×lVl×d2
denote the tensor products defined as follows:
(G X H(P))i;g;km1m2...mp
j
(P)
j;g;m1m2...mp
(8)
(G X G)il;;k1k2 :=	Gij;;k1 Gjl;;k2.	(9)
j
The tensor product of IsoAMs G X G can be interpreted as the tensor product of each of the IsoAMs
components. Thus, the subsequent proposition follows:
Proposition 3.3. The tensor product of the IsoAMs G X G is isometric transformation equivariant
in terms of the rank-2 tensor, i.e., for any isometric transformation ∀t ∈ R3, U ∈ O(d), T : x 7→
Ux +t, and∀i,j ∈ 1, . . . , |V|, (G X G)ij;;k1k2 7→ Uk1l1Uk2l2(G X G)ij;;l1l2.
This proposition is easily generalized to the tensors of higher ranks by defining the pth tensor power
of G as follows: NO G = 1, N1 G = G, and NP G = NP-1 G X G. Namely, NP G is isometric
transformation equivariant in terms of rank-p tensor. Therefore, one can see that (NP G) X H(q) =
(NP-1 G)
X (G X H(q)). Moreover, the convolution can be generalized for NP G and the rank-0
tensor field H(O) ∈ RlVl×f as follows:
The contraction can be generalized for NP G and the rank-q tensor field H⑷ ∈ RlVl×f×dq (p ≥ q)
as specified below:
(10)
= X	OG	Hj(;qg);m1m2...
i;g;k1 k2 ...kp-q j,m1 ,m2,...,mq	ij;;k1k2 ...kp-q m1 m2 ...mq
(11)
For the case p < q, the contraction can be defined similarly.
3.2	Construction of IsoGCN
Using the operations defined above, we can construct IsoGCN layers, which take the tensor field of
any rank as input, and output the tensor field of any rank, which can differ from those of the input.
In addition, one can show that these layers are also equivariant under the vertex permutation, as
discussed in Maron et al. (2018).
4
Published as a conference paper at ICLR 2021
3.2.1	Isometric transformation invariant layer
As can be seen in Proposition 3.1, the contraction of IsoAMs is isometric transformation invari-
ant. Therefore, for the isometric transformation invariant layer with a rank-0 input tensor field
f : RlVl×fin 3 H(n0) → Hout ∈ RlVl×fout (fin,fout ∈ Z+), the activation function σ, and the
trainable parameter matrix W ∈ Rfin ×fout can be constructed as Ho(0u)t = σ (G G) Hi(n0)W .
By defining L := G Θ G ∈ RlVl×lVl, it can be simplified as Hout = σ (LH(10) W), which has the
same form as a GCN (equation 1), with the exception that A is replaced with L.
An isometric transformation invariant layer with the rank-p input tensor field H(P) ∈ RlVl×fin×dp
can be formulated as Ho(0u)t = Fp→0 (H i(np)) = σ Np G Θ Hi(np) W
. If p = 1, such approaches
utilize the inner products of the vectors in Rd, these operations correspond to the extractions of a
relative distance and an angle of each pair of vertices, which are employed in Klicpera et al. (2020).
3.2.2	Isometric transformation equivariant layer
To construct an isometric transformation equivariant layer, one can use linear transformation, con-
volution and tensor product to the input tensors. If both the input and the output tensor ranks are
greater than 0, one can apply neither nonlinear activation nor bias addition because these opera-
tions will cause an inappropriate distortion of the isometry because isometric transformation does
not commute with them in general. However, a conversion that uses only a linear transformation,
convolution, and tensor product does not have nonlinearity, which limits the predictive performance
of the model. To add nonlinearity to such a conversion, we can first convert the input tensors to
rank-0 ones, apply nonlinear activations, and then multiply them to the higher rank tensors.
The nonlinear isometric transformation equivariant layer with the rank-m input tensor field Hi(nm) ∈
RlVl×fin×dm and the rank-l (m ≤ l) output tensor Hout ∈ RlVl×fout×dl can be defined as:
l-m
Holut = Fm→0 (Hw)X Fm→ι (hZ) , Fm→ι (Hw)= O G 乳 HzWml,	(12)
where × denotes multiplication with broadcasting and Wml ∈ Rfin ×fout are trainable weight ma-
trices multiplied in the feature direction. If m = 0, We regard G 0 H(0) as G * H ⑼.If m = l, one
can add the residual connection (He et al., 2016) in equation 12. If m > l,
m-l
H(olu)t = Fm→0	Hi(nm)	× Fm→l	Hi(nm)	,	Fm→l	Hi(nm)	= OG Θ	Hi(nm)Wml.	(13)
In general, the nonlinear isometric transformation equivariant layer with the rank-0 to rank-M input
tensor field {Hi(nm)}mM=0 and the rank-l output tensor field H(olu)t can be defined as:
H(olu)t = Hi(nl)W + XM fgather nFk→0(Hi(nk))okM=0 × Fm→l Hi(nm)),	(14)
m=0
where fgather denotes a function such as summation, product and concatenation in the feature di-
rection. One can see that this layer is similar to that in the TFN (equation 2), while there are no
spherical harmonics and trainable message passing.
To be exact, the output of the layer defined above is translation invariant. To output translation
equivariant variables such as the vertex positions after deformation (which change accordingly with
the translation of the input graph), one can first define the reference vertex position xref for each
graph, then compute the translation invariant output using equation 14, and finally, add xref to the
output. For more detailed information on IsoGCN modeling, see Appendix D.
3.3	Example of IsoAM
The IsoGCN G is defined in a general form for the propositions to work with various classes
of graph. In this section, we concretize the concept of the IsoAM to apply an IsoGCN
5
Published as a conference paper at ICLR 2021
to mesh structured data. Here, a mesh is regarded as a graph regarding the points in the
mesh as vertices of the graph and assuming two vertices are connected when they share
the same cell. A concrete instance of IsoAM D, D ∈ RlVl×lVl×d is defined as follows:
Dij；；k = Dij；；k - δij EDil；；k,	(15) l	Table 1: Correspondence between the differential operators and the expres-
Dij；；： = Mi 1 kχj - χik2 wij Aij (m)，	(16) xl - xi	xl - xi Mi = E η	∏ 区 η	∏wiiAil(m), (17) l kxl - xik kxl - xik	sions using the IsoAM D. Differential op. Expression Gradient	D * H(O) Divergence	D Θ H ⑴
where RlVl×lVl 3 A(m) := min (Pk= ι Ak, 1) is an adjacency matrix up to m hops and wij ∈ R is an un- trainable weight between the ith and jth vertices that	Laplacian	D Θ	DH(O) Jacobian	D %	H(I) Hessian	D %	D * H(O)
is determined depending on the tasks2. By regarding
Tijkl = δiiδjkMiTwijAij(m)/kXj — Xi∣∣2 in equation 4, one can see that D is qualified as an
∙-v
IsoAM. Because a linear combination of IsoAMs is also an IsoAM, D is an IsoAM. Thus, they
∙-v
provide both translation invariance and orthogonal transformation equivariance. D can be obtained
only from the mesh geometry information, thus can be computed in the preprocessing step.
∙-v
Here, D is designed such that it corresponds to the gradient operator model used in physical sim-
ulations (Tamai & Koshizuka, 2014; Swartz & Wendroff, 1969). As presented in Table 1 and Ap-
∙-v
pendix C, D is closely related to many differential operators, such as the gradient, divergence, Lapla-
cian, Jacobian, and Hessian. Therefore, the considered IsoAM plays an essential role in constructing
neural network models that are capable of learning differential equations.
4	Experiment
To test the applicability of the proposed model, we composed the following two datasets: 1) a
differential operator dataset of grid meshes; and 2) an anisotropic nonlinear heat equation dataset
of meshes generated from CAD data. In this section, we discuss our machine learning model, the
definition of the problem, and the results for each dataset.
∙-v
Using D defined in Section 3.3, we constructed a neural network model considering an encode-
process-decode configuration (Battaglia et al., 2018). The encoder and decoder were comprised of
component-wise MLPs and tensor operations. For each task, we tested m = 2, 5 in equation 16 to
investigate the effect of the number of hops considered. In addition to the GCN (Kipf & Welling,
2017), we chose GIN (Xu et al., 2018), SGCN (Wu et al., 2019), Cluster-GCN (Chiang et al., 2019),
and GCNII (Chen et al., 2020) as GCN variant baseline models. For the equivariant models, we
chose the TFN (Thomas et al., 2018) and SE(3)-Transformer (Fuchs et al., 2020) as the baseline.
We implemented these models using PyTorch 1.6.0 (Paszke et al., 2019) and PyTorch Geometric
1.6.1 (Fey & Lenssen, 2019). For both the TFN and SE(3)-Transformer, we used implementation
of Fuchs et al. (2020) 3 because the computation of the TFN is considerably faster than the original
implementation, as claimed in Fuchs et al. (2020). For each experiment, we minimized the mean
squared loss using the Adam optimizer (Kingma & Ba, 2014). The corresponding implementa-
tion and the dataset will be made available online. The details of the experiments can be found in
Appendix E and F.
4.1	Differential operator dataset
To demonstrate the expressive power of IsoGCNs, we created a dataset to learn the differential
operators. We first generated a pseudo-2D grid mesh randomly with only one cell in the Z direction
and 10 to 100 cells in the X and Y directions. We then generated scalar fields on the grid meshes
2Mi is invertible when the number of independent vectors in {xl - xi }l is greater than or equal to the
space dimension d, which is true for common meshes, e.g., a solid mesh in 3D Euclidean space.
3https://github.com/FabianFuchsML/se3-transformer-public
6
Published as a conference paper at ICLR 2021
」%n±=u6 Dlλ∣4—Uφ-pcλl6lφouφ-φt=-p
Figure 2: (Top) the gradient field and (bottom) the error vector between the prediction and the ground
truth of a test data sample. The error vectors are exaggerated by a factor of 2 for clear visualization.
Table 2: Summary of the test losses (mean squared error ± the standard error of the mean in the
original scale) of the differential operator dataset: 0 → 1 (the scalar field to the gradient field),
0 → 2 (the scalar field to the Hessian field), 1 → 0 (the gradient field to the Laplacian field), and
1 → 2 (the gradient field to the Hessian field). Here, if “x” is “Yes”, x is also in the input feature.
We show only the best setting for each method except for the equivariant models. For a full table,
see Appendix E. OOM denotes the out-of-memory on the applied GPU (32 GiB).
Method	# hops	x	Loss of 0 → 1 ×10-5	Loss of 0 → 2 ×10-6	Loss of 1 → 0 ×10-6	Loss of 1 → 2 ×10-6
GIN	=	5	Yes	147.07 ± 0.5T=	47.35 ± 0.31=	404.92 ± 1.h	46.18 ± 0.39Z=
GCNII	5	Yes	151.13 ± 0.53	31.87 ± 0.22	280.61 ± 1.30	39.38 ± 0.34
SGCN	5	Yes	151.16 ± 0.53	55.08 ± 0.42	127.21 ± 0.63	56.97 ± 0.44
GCN	5	Yes	151.14 ± 0.53	48.50 ± 0.35	542.30 ± 2.14	25.37 ± 0.28
Cluster-GCN	5	Yes	146.91 ± 0.51	26.60 ± 0.19	185.21 ± 0.99	18.18 ± 0.20
TFN	2	No	2.47 ± 0.02	OOM	26.69 ± 0.24	OOM
	5	No	OOM	OOM	OOM	OOM
SE(3)-Trans.	2	No	1.79 ± 0.02	3.50 ± 0.04	2.52 ± 0.02	OOM
	5	No	2.12 ± 0.02	OOM	7.66 ± 0.05	OOM
IsoGCN (Ours)	2	No	2.67 ± 0.02	6.37 ± 0.07	7.18 ± 0.06	1.44 ± 0.02
	5	No	14.19 ± 0.10	21.72 ± 0.25	34.09 ± 0.19	8.32 ± 0.09
and analytically calculated the gradient, Laplacian, and Hessian fields. We generated 100 samples
for each train, validation, and test dataset. For simplicity, we set wij = 1 in equation 16 for all
(i, j ) ∈ E . To compare the performance with the GCN models, we simply replaced an IsoGCN
layer with a GCN or its variant layers while keeping the number of hops m the same to enable a fair
comparison. We adjusted the hyperparameters for the equivariant models to ensure that the number
of parameters in each was almost the same as that in the IsoGCN model. For more details regarding
the model architecture, see Appendix E. We conducted the experiments using the following settings:
1) inputting the scalar field and predicting the gradient field (rank-0 → rank-1 tensor); 2) inputting
the scalar field and predicting the Hessian field (rank-0 → rank-2 tensor); 3) inputting the gradient
field and predicting the Laplacian field (rank-1 → rank-0 tensor); and 4) inputting the gradient field
and predicting the Hessian field (rank-1 → rank-2 tensor).
Figure 2 and Table 2 present a visualization and comparison of predictive performance, respectively.
The results show that an IsoGCN outperforms other GCN models for all settings. This is because
the IsoGCN model has information on the relative position of the adjacency vertices, and thus un-
derstands the direction of the gradient, whereas the other GCN models cannot distinguish where the
adjacencies are, making it nearly impossible to predict the gradient directions. Adding the vertex po-
sitions to the input feature to other GCN models exhibited a performance improvement, however as
the vertex position is not a translation invariant feature, it could degrade the predictive performance
of the models. Thus, we did not input x as a vertex feature to the IsoGCN model or other equiv-
ariant models to retain their isometric transformation invariant and equivariant natures. IsoGCNs
perform competitively against other equivariant models with shorter inference time as shown in Ta-
7
Published as a conference paper at ICLR 2021
Figure 3: (Top) the temperature field of the ground truth and inference results and (bottom) the error
between the prediction and the ground truth of a test data sample. The error is exaggerated by a
factor of 2 for clear visualization.
ble 7. As mentioned in Section 3.3, D corresponds to the gradient operator, which is now confirmed
in practice.
4.2 Anisotropic nonlinear heat equation dataset
To apply the proposed model to a real problem, we adopted the anisotropic nonlinear heat equation.
We considered the task of predicting the time evolution of the temperature field based on the ini-
tial temperature field, material property, and mesh geometry information as inputs. We randomly
selected 82 CAD shapes from the first 200 shapes of the ABC dataset (Koch et al., 2019), generate
first-order tetrahedral meshes using a mesh generator program, Gmsh (Geuzaine & Remacle, 2009),
randomly set the initial temperature and anisotropic thermal conductivity, and finally conducted a
finite element analysis (FEA) using the FEA program FrontISTR4 (Morita et al., 2016; Ihara et al.,
2017).
For this task, we set wij = Vjeffective /Vieffective , where Vieffective denotes the effective volume of
the ith vertex (equation 46.) Similarly to the differential operator dataset, we tested the number of
hops m = 2, 5. However because we put four IsoAM operations in one model, the number of hops
visible from the model is 8 (m = 2) or 20 (m = 5). As is the case with the differential operator
dataset, we replaced an IsoGCN layer accordingly for GCN or its variant models. In the case of
k = 2, we reduced the number of parameters for each of the equivariant models to fewer than the
IsoGCN model because they exceeded the memory of the GPU (NVIDIA Tesla V100 with 32 GiB
memory) with the same number of parameters. In the case of k = 5, neither the TFN nor the SE(3)-
Transformer fits into the memory of the GPU even with the number of parameters equal to 10. For
more details about the dataset and the model, see Appendix F.
Figure 3 and Table 3 present the results of the qualitative and quantitative comparisons for the test
dataset. The IsoGCN demonstrably outperforms all other baseline models. Moreover, owing to the
computationally efficient isometric transformation invariant nature of IsoGCNs, it also achieved a
high prediction performance for the meshes that had a significantly larger graph than those con-
sidered in the training dataset. The IsoGCN can scale up to 1M vertices, which is practical and is
considerably greater than that reported in Sanchez-Gonzalez et al. (2020). Therefore, we conclude
that IsoGCN models can be trained on relatively smaller meshes5 to save the training time and then
used to apply the inference to larger meshes without observing significant performance deterioration.
Table 4 reports the preprocessing and inference computation time using the equivariant models with
m = 2 as the number of hops and FEA using FrontISTR 5.0.0. We varied the time step (∆t =
4https://github.com/FrontISTR/FrontISTR. We applied a private update to FrontISTR to
deal with the anisotropic heat problem, which will be also made available online.
5 However, it should also be sufficiently large to express sample shapes and fields.
8
Published as a conference paper at ICLR 2021
1.0, 0.5) for the FEA computation to compute the t = 1.0 time evolution thus, resulting in different
computation times and errors compared to an FEA with ∆t = 0.01, which was considered as the
ground truth. Clearly, the IsoGCN is 3- to 5- times faster than the FEA with the same level of
accuracy, while other equivariant models have almost the same speed as FrontISTR with ∆t = 0.5.
5 Conclusion
In this study, we proposed the GCN-
based isometric transformation in-
variant and equivariant models called
IsoGCN. We discussed an exam-
ple of an isometric adjacency ma-
trix (IsoAM) that was closely re-
lated to the essential differential op-
erators. The experiment results con-
firmed that the proposed model lever-
aged the spatial structures and can
deal with large-scale graphs. The
computation time of the IsoGCN
model is significantly shorter than the
FEA, which other equivariant models
cannot achieve. Therefore, IsoGCN
must be the first choice to learn phys-
ical simulations because of its com-
putational efficiency as well as iso-
metric transformation invariance and
equivariance. Our demonstrations
were conducted on the mesh struc-
tured dataset based on the FEA re-
sults. However, we expect IsoGCNs
to be applied to various domains,
Table 3: Summary of the test losses (mean squared error ±
the standard error of the mean in the original scale) of the
anisotropic nonlinear heat dataset. Here, if “x” is “Yes”, x
is also in the input feature. We show only the best setting for
each method except for the equivariant models. For the full
table, see Appendix E. OOM denotes the out-of-memory on
the applied GPU (32 GiB).
			Loss
Method	# hops	x	×10-3
GIN	2	^No-	16.921 ± 0.04O=
GCN	2	No	10.427 ± 0.028
GCNII	5	No	8.377 ± 0.024
Gluster-GCN	2	No	7.266 ± 0.021
SGCN	5	No	6.426 ± 0.018
TFN	2~~	No	15.661 ± 0.019
	5	No	OOM
SE(3)-Trans.	2	No	14.164 ± 0.018
	5	No	OOM
IsoGCN (Ours)	2 5	No No	4.674 ± 0.014 2.470 ± 0.008
such as object detection, molecular property prediction, and physical simulations using particles.
Acknowledgments
The authors gratefully acknowledge Takanori Maehara for his helpful advice and NVIDIA for hard-
ware donations. This work was supported by JSPS KAKENHI Grant Number 19H01098.
Table 4: Comparison of computation time. To generate the test data, we sampled CAD data from
the test dataset and then generated the mesh for the graph to expand while retaining the element
volume at almost the same size. The initial temperature field and the material properties are set
randomly using the same methodology as the dataset sample generation. For a fair comparison,
each computation was run on the same CPU (Intel Xeon E5-2695 v2@2.40GHz) using one core,
and we excluded file I/O time from the measured time. OOM denotes the out-of-memory (500
GiB).
Method	|V| = Loss ×10-4	21, 289 Time [s]	|V| = Loss ×10-4	155, 019 Time [s]	|V|=1, Loss ×10-4	011, 301 Time [s]
FrontISTR (∆t = 1.0)	10.9	16.7	6.1	181.7	2.9	1656.5
FrontISTR (∆t = 0.5)	0.8	30.5	0.4	288.0	0.2	2884.2
TFN	77.9	46.1	30.1	400.9	OOM	OOM
SE(3)-Transformer	111.4	31.2	80.3	271.1	OOM	OOM
IsoGCN (Ours)	8.1	7.4	4.9	84.1	3.9	648.4
9
Published as a conference paper at ICLR 2021
References
Eman Ahmed, Alexandre Saint, Abd El Rahman Shabayek, Kseniya Cherenkova, Rig Das, Gleb
Gusev, Djamila Aouada, and Bjorn Ottersten. A survey on deep learning advances on different
3d data representations. arXiv preprint arXiv:1808.01462, 2018.
Ferran Alet, Adarsh Keshav Jeewajee, Maria Bauza Villalonga, Alberto Rodriguez, Tomas Lozano-
Perez, and Leslie Kaelbling. Graph element networks: adaptive, structured computation and
memory. In ICML, 2019.
Igor I Baskin, Vladimir A Palyulin, and Nikolai S Zefirov. A neural device for searching direct
correlations between structures and properties of chemical compounds. Journal of chemical in-
formation and computer sciences, 37(4):715-721,1997.
Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi,
Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al.
Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261,
2018.
Kai-Hung Chang and Chin-Yi Cheng. Learning to simulate and design for structural engineering.
arXiv preprint arXiv:2003.09103, 2020.
Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph
convolutional networks. arXiv preprint arXiv:2007.02133, 2020.
Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-gcn: An
efficient algorithm for training deep and large graph convolutional networks. In Proceedings of
the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.
257-266, 2019.
Taco Cohen and Max Welling. Group equivariant convolutional networks. In International confer-
ence on machine learning, pp. 2990-2999, 2016.
Taco S Cohen, Mario Geiger, Jonas Kohler, and Max Welling. Spherical cnns. In ICLR, 2018.
Taco S Cohen, Maurice Weiler, Berkay Kicanaoglu, and Max Welling. Gauge equivariant convolu-
tional networks and the icosahedral cnn. ICML, 2019.
Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In
ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.
Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. Se (3)-transformers: 3d roto-
translation equivariant attention networks. Advances in Neural Information Processing Systems,
33, 2020.
Christophe Geuzaine and Jean-Francois Remacle. Gmsh: a three-dimensional finite element mesh
generator with built-in pre- and post-processing facilities. International Journal for Numerical
Methods in Engineering, 79(11):1309-1331, 2009.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pp. 1263-1272. JMLR. org, 2017.
Marco Gori, Gabriele Monfardini, and Franco Scarselli. Anew model for learning in graph domains.
In Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., volume 2,
pp. 729-734. IEEE, 2005.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Yu Ihara, Gaku Hashimoto, and Hiroshi Okuda. Web-based integrated cloud cae platform for large-
scale finite element analysis. Mechanical Engineering Letters, 3:17-00520, 2017.
10
Published as a conference paper at ICLR 2021
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In ICLR, 2017.
Johannes Klicpera, Janek Groβ, and StePhan Gunnemann. Directional message passing for molec-
ular graphs. In ICLR, 2020.
Sebastian Koch, Albert Matveev, Zhongshi Jiang, Francis Williams, Alexey Artemov, Evgeny Bur-
naev, Marc Alexa, Denis Zorin, and Daniele Panozzo. Abc: A big cad model dataset for geometric
deep learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Risi Kondor. N-body networks: a covariant hierarchical neural network architecture for learning
atomic potentials. arXiv preprint arXiv:1803.01588, 2018.
Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph
networks. arXiv preprint arXiv:1812.09902, 2018.
Naoki Morita, Kazuo Yonekura, Ichiro Yasuzumi, Mitsuyoshi Tsunori, Gaku Hashimoto, and Hi-
roshi Okuda. Development of 3× 3 dof blocking structural elements to enhance the computational
intensity of iterative linear solver. Mechanical Engineering Letters, 2:16-00082, 2016.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In
Proceedings of the 27th international conference on machine learning (ICML-10), pp. 807-814,
2010.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024-8035. Curran
Associates, Inc., 2019.
Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Tobias Springenberg, Josh Merel, Martin Riedmiller,
Raia Hadsell, and Peter Battaglia. Graph networks as learnable physics engines for inference and
control. arXiv preprint arXiv:1806.01242, 2018.
Alvaro Sanchez-Gonzalez, Victor Bapst, Kyle Cranmer, and Peter Battaglia. Hamiltonian graph
networks with ode integrators. arXiv preprint arXiv:1909.12790, 2019.
Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Pe-
ter W Battaglia. Learning to simulate complex physics with graph networks. arXiv preprint
arXiv:2002.09405, 2020.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2008.
Alessandro Sperduti and Antonina Starita. Supervised neural networks for the classification of
structures. IEEE Transactions on Neural Networks, 8(3):714-735, 1997.
Blair Swartz and Burton Wendroff. Generalized finite-difference schemes. Mathematics of Compu-
tation, 23(105):37-49, 1969.
Tasuku Tamai and Seiichi Koshizuka. Least squares moving particle semi-implicit method. Com-
putational Particle Mechanics, 1(3):277-305, 2014.
Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick
Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point
clouds. arXiv preprint arXiv:1802.08219, 2018.
11
Published as a conference paper at ICLR 2021
Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco S Cohen. 3d steerable
cnns: Learning rotationally equivariant features in volumetric data. In NeurIPS ,pp.10381-10392,
2018.
Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simpli-
fying graph convolutional networks. In ICML, pp. 6861-6871. PMLR, 2019.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? arXiv preprint arXiv:1810.00826, 2018.
Jiaxuan You, Rex Ying, and Jure Leskovec. Position-aware graph neural networks. arXiv preprint
arXiv:1906.04817, 2019.
A Notation
G V |V| E Z+ d	A graph A vertex set The number of vertices An edge set The positive integers The dimension of the Euclidean space
xi	The position of the ith vertex
xik G ∈ RlVl×lVl×d Gij;;: ∈ Rd Gij;;k ∈ R H(P) ∈ RlVl×f×dp H(p) i;g;k1 k2 ...kp	Element k of xi The isometric adjacency matrix (IsoAM) (equation 4) Slice of G in the spatial index (equation 4) Element (i, j, k) of G A rank-p tensor field tensor (f, p ∈ Z+) Element (i; g; k1, k2, . . . , kp) of H(p). i refers to the permutation rep- resentation, k1, . . . kp refer to the Euclidean representation, and g de-
p O G * H(O)	notes the feature index (See section 3). Convolution of the pth power of G and rank-0 tensor field H(0) (equa-
p OG	H(q)	tion 6, equation 10) Contraction of the pth power of G and rank-q tensor fields (equation 7,
p O G㊈H⑷ H(p) in H(p) out σ W A ∈ RlVl×lVl δij V effective V mean D ∈ RlVl×lVl	equation 11) Tensor product of the pth power of G and rank-q tensor fields H(q) (equation 8) The rank-p input tensor field of the considered layer The rank-p output tensor field of the considered layer The activation function The trainable parameter matrix An adjacency matrix The Kronecker delta The effective volume of the ith vertex (equation 46) The mean volume of the ith vertex (equation 47) A concrete instance of IsoAM (equation 15)
12
Published as a conference paper at ICLR 2021
B	Proofs of propositions
In this section, we present the proofs of the propositions described in Section 3. Let
R3 3 g(xl, xk) = (xk - xl). Note that G is expressed using g(xi, xj) as Gij;;: =
k,l∈V,k6=l Tijklg(xl , xk).
B.1 Proof of Proposition 3.1
Proof. First, we demonstrate the invariance with respect to the translation with ∀t ∈ Rd. g(xi , xj )
is transformed invariantly as follows under translation:
g(xi + t, xj + t) = [xj + t - (xi + t)]
= (xj - xi)
= g(xi, xj).	(18)
By definition, Tijkl is also translation invariant. Thus,
Tijklg(xl + t, xk + t) =	Tijklg(xl , xk)
k,l∈V,k6=l	k,l∈V,k6=l
= Gij;;:.	(19)
We then show an equivariance regarding the orthogonal transformation with ∀U ∈ O(d). g(xi , xj )
is transformed as follows by orthogonal transformation:
g(Uxi, Uxj) = Uxj - Uxi
= U(xj - xi)
= Ug(xi, xj).	(20)
By definition, Tijkl is transformed to UTijklU-1 by orthogonal transformation. Thus,
X	UTijklU-1g(Uxl,Uxk) = X	UTijklU-1Ug(xl,xk)
k,l∈V,k6=l	k,l∈V,k6=l
= UGij;;:.	(21)
Therefore, G is both translation invariant and an orthogonal transformation equivariant.	□
B.2 Proof of Proposition 3.2
Proof. Here, G G is translation invariant because G is translation invariant. We prove rotation
invariance under an orthogonal transformation ∀U ∈ O(n). In addition, GG is transformed under
U as follows:
j,k
Gij;;k Gjl;;k 7→	Ukm Gij;;mUkn Gjl;;n
j,k,m,n
UkmUknGij;;mGjl
j,k,m,n
UmkUknGij;;mGjl
j,k,m,n
δmn Gij;;m Gj l;;n
j,m,n
Gij;;mGjl;;m
j
Gij;;kGjl;;k.
j,k
;;n
;;n
(∙.∙ property of the orthogonal matrix)
(∙.∙ Change the dummy index m → k) (22)
Therefore, G G is isometric transformation invariant.
□
13
Published as a conference paper at ICLR 2021
B.3 Proof of Proposition 3.3
Proof. G 0 G is transformed under ∀U ∈ O(n) as follows:
Gij;;kGjl;;m 7→	Ukn Gij;;n Umo Gj l;;o
j
n,o
Ukn Gij;;n Gjl;;oUoTm .
n,o
(23)
By regarding Gij;;n Gjl;;o as one matrix Hno, it follows the coordinate transformation of rank-2
tensor UHUT for each i, j, and l.	□
C PHYSICAL INTUITION OF D
∙-v
In this section, we discuss the connection between the concrete IsoAM example D and the dif-
ferential operators such as the gradient, divergence, the Laplacian, the Jacobian, and the Hessian
operators.
Let φi ∈ R denote a rank-0 tensor (scalar) at the ith vertex. Let us assume a partial derivative model
of a rank-0 tensor φ at the ith vertex regarding the kth axis (∂φ∕∂xk)i ∈ R (k ∈ {1,..., d}), that
is based on the gradient model in the least squares moving particle semi-implicit method (Tamai &
Koshizuka, 2014).
j :=M-1 X j φ∖ Xjk-Xik WjAj(m) Ji	j Ilxj - Xikkxj- Xik =	Dijk (φj- φi), j xl - xi	xl - xi Mi =E τ	∏- 0 τ	nwiiAii(m). l kxl -xik	kxl -xik	(24) (25) (26)
Although one could define wij as a function of the distance kxj - xi k, wij was kept constant with
respect to the distance required to maintain the simplicity of the model with fewer hyperparameters.
C.1 Gradient
D can be viewed as a Laplacian matrix based on D; however, D * H(O) can be interpreted as the
gradient within the Euclidean space. Let V H(O) ∈ RlVl×f ×d be an approximation of the gradient
of H(0). Using equation 25, the gradient model can be expressed as follows:
(27)
(28)
14
Published as a conference paper at ICLR 2021
Using this gradient model, We can confirm that (D * H(0))i；g;k = (V H(0))i;gik because
(D * H(O))」X Dj,kHS；	(29)
j
=Xj (Dij;;k -δijXl Dil;;k)Hj(;0g);
=X Dij；；kHSL X δijDii；；kh(0g);
= Xj Dij;;kHj(;0g); - Xl Dil;;kHi(;0g);
=X Dij；；kH(0). - X Dij；；kH(0);	(√ Change the dummy index l → j)
=XDij;;k(Hj(;0g); -Hi(;0g);)
j
= V H(0)	.	(30)
i;g;k
∙-v
Therefore, D* can be interpreted as the gradient operator Within a Euclidean space.
C.2 Divergence
We show that D Θ H(I) corresponds to the divergence. Using D, the divergence model V ∙ H(I) ∈
RlVl×f is expressed as follows:
i;g;
X Dij;;k (Hj(;g);k - Hi(;g);k).
j,k
(31)
(32)
Then, D Θ H ⑴ is
(D Θ H(1))i,g;= X Dij..kH黑k
j,k
= Xj,k	Dij;;k -δijXl D	Hi(;1g);k
=XDij;;kH(j1;g);k -XDil;;kHi(;1g);k
j,k	l,k
=X Dij..k(Hj1g.k - H(;g);k)	(√ Change the dummy index l → j)
j,k
= (V∙ H(1))i.g;.	(33)
C.3 Laplacian operator
∙-v	∙-v
We prove that D Θ D corresponds to the Laplacian operator within a Euclidean space.
15
Published as a conference paper at ICLR 2021
Using equation 25, the Laplacian model V2 H(O) ∈ RlVl×f can be expressed as follows:
Xj,k Dij;;k	Xl	Djl;;k(Hl(;0g);-Hj(;0g);)-Xl	Dil;;k(Hl(;0g);-Hi(;0g);)
XDij;;k(Djl;;k-Dil;;k)(Hl(;0g); -Hj(;0g);).	(34)
j,k,l
Then, (D Θ D)H(O) is
((D Θ D)H ⑼)i；g； = X Dij；；k Djl；；k HJg)
j,k,l
Dij;;k - δij
j,k,l
Dim;;k
m
Djl;;k - δjl	Djn;;k
n
X Dij;;k Djl;;kHl(;Og); - X Dij;;k Djn;;kHj(;Og);
j,k,l j,k,n
-	Dim;;kDil;;kHl(;Og);+	Dim;;kDin;;kHi(;Og);
k,l,m	k,m,n
X Dij;;k Djl;;kHl(;Og); - X Dij;;k Djn;;kHj(;Og);
j,k,l
j,k,n
- X Dij;;kDil;;kHl;g; + X Dij;;kDin;;kHi;g;
k,l,j
k,j,n
(∙.∙ Change the dummy index m → j for the third and fourth terms)
X Dij;;k(Djl;;k - Dil;;k)(Hl(;Og); - Hj(;Og);)
j,k,l
(∙.∙ Change the dummy index n → l for the second and fourth terms)
V2 H(O)
i;g;
(35)
C.4 Jacobian and Hessian operators
Considering a similar discussion, we can show the following dependencies. For the Jacobian model,
J[H(1)] ∈ R
∣V∣×f ×d×d
J[H(1)]
i;g;kl
(∂ H(I))
IK儿k
XDij;;l (Hj(;1g);k-Hi(;1g);k)
j
(D ㊈ H(I) )i；g；,k.
(36)
(37)
(38)
16
Published as a conference paper at ICLR 2021
For the Hessian model, Hess[H(0)] ∈ RlVl×f ×d×d,
(Hesw])ij (∂Xk卷 H()g;	(39)
= X Dij;;k[Djm;;l(Hm(0;)g; - Hl(;0g);) - Dim;;l(Hm(0;)g; - Hi(;0g);)]	(40)
j,m
=h(D ㊈ D)* H(O)i	.	(41)
i;g;kl
D IsoGCN modeling details
To achieve isometric transformation invariance and equivariance, there are several rules to follow.
Here, we describe the desired focus when constructing an IsoGCN model. In this section, a rank-p
tensor denotes a tensor the rank of which is p ≥ 1 and σ denotes a nonlinear activation function. W
is a trainable weight matrix and b is a trainable bias.
D. 1 Activation and bias
As the nonlinear activation function is not isometric transformation equivariant, nonlinear activation
to rank-p tensors cannot be applied, while one can apply any activation to rank-0 tensors. In addition,
adding bias is also not isometric transformation equivariant, one cannot add bias when performing
an affine transformation to rank-p tensors. Again, one can add bias to rank-0 tensors.
Thus, for instance, if one converts from rank-0 tensors H(0) to rank-1 tensors using IsoAM G,
G*σ(H(0)W +b) and (G*σ(H(0)))W are isometric equivariant functions, however (G*H(0))W +b
and σ (G * σ(H(0)))W are not due to the bias and the nonlinear activation, respectively. Like-
wise, regarding a conversion from rank-1 tensors H(1) to rank-0 tensors, σ (G H(1))W + b and
σG(H(1)W)are isometric transformation invariant functions; however, G (H(1)W + b)
and (G σ(H(1)))W + b are not.
To convert rank-p tensors to rank-q tensors (q ≥ 1), one can apply neither bias nor nonlinear ac-
tivation. To add nonlinearity to such a conversion, we can multiply the converted rank-0 tensors
σ((Np G H(p))W + b) with the input tensors H(p) or the output tensors H(q).
D.2 Preprocessing of input feature
Similarly to the discussion regarding the biases, we have to take care of the preprocessing of
rank-p tensors to retain isometric transformation invariance because adding a constant array and
component-wise scaling could distort the tensors, resulting in broken isometric transformation
equivariance.
For instance, H(p)/Stdall H(p) is a valid transformation to retain isometric transformation equiv-
ariance, assuming Stdall H(p) ∈ R is a standard deviation of all components of H(p). However,
conversions such as H(p) /Stdcomponent H(p) and H(p) - Mean H(p) are not isometric trans-
formation equivariant, assuming that Stdcomponent H(p) ∈ Rdp is a component-wise standard
deviation.
D.3 Scaling
∙-v
Because the concrete instance of IsoAM D corresponds to the differential operator, the scale of
〜 ~
the output after operations regarding D can be huge. Thus, we rescale D using the scaling factor
17
Published as a conference paper at ICLR 2021
[Meansampie,i(Z)IIi；；i + D2i；；2 + D2^3)i / , Where Meansampie,i denotes the mean over the samples
and vertices.
D.4 Implementation
Because an adjacency matrix A is usually a sparse matrix for a regular mesh, A(m) in equation 16
is also a sparse matrix for a sufficiently small m. Thus, We can leverage sparse matrix multiplica-
tion in the IsoGCN computation. This is one major reason Why IsoGCNs can compute rapidly. If
the multiplication (tensor product or contraction) of IsoAMs must be computed multiple times the
associative property of the IsoAM can be utilized.
For instance, it is apparent that [Nk g] *H(0) = G0(G0... (G*H(0))). Assuming that the number
of nonzero elements in A(m) equals n and H(O) ∈ RlVl×f, then the computational complexity of
the right-hand side is O(n|V |f dk). This is an exponential order regarding d. HoWever, d and k
are usually small numbers (typically d = 3 and k ≤ 4). Therefore one can compute an IsoGCN
layer With a realistic spatial dimension d and tensor rank k fast and memory efficiently. In our
implementation, both a sparse matrix operation and associative property are utilized to realize fast
computation.
E Experiment details: differential operator dataset
E.1 Model architectures
(G
(b)
(C)
(d)
Encoder	Propagation	Decoder
Figure 4: The ISOGCN model used for (a) the scalar field to the gradient field, (b) the scalar field to
the Hessian field, (c) the gradient field to the Laplacian field, (d) the gradient field to the Hessian field
of the gradient operator dataset. The numbers in each box denote the number of units. Below the
unit numbers, the activation function used for each layer is also shown. 0 denotes the multiplication
in the feature direction.
18
Published as a conference paper at ICLR 2021
Table 5: Summary of the hyperparameter setting for both the TFN and SE(3)-Transformer. For
the parameters not in the table, we used the default setting in the implementation of https://
github.com/FabianFuchsML/se3-transformer-public.
	0 → 1	0 → 2	1 → 0	1 → 2
# hidden layers	1	1	1	1
# NL layers in the self-interaction	1	1	1	1
# channels	24	20	24	24
# maximum rank of the hidden layers	1	2	1	2
# nodes in the radial function	16	8	16	22
Figure 4 represents the IsoGCN model used for the differential operator dataset. We used the tanh
activation function as a nonlinear activation function because we expect the target temperature field
to be smooth. Therefore, we avoid using non-differentiable activation functions such as the rectified
linear unit (ReLU) (Nair & Hinton, 2010). For GCN and its variants, we simply replaced the IsoGCN
layers with the corresponding ones. We stacked m (= 2, 5) layers for GCN, GIN, GCNII, and
Cluster-GCN. We used an m hop adjacency matrix for SGCN.
For the TFN and SE(3)-Transformer, we set the hyperparameters to have almost the same number
of parameters as in the IsoGCN model. The settings of the hyperparameters are shown in Table 5.
E.2 Result details
Table 6 represents the detailed comparison of training results. The results show that an IsoGCN
outperforms other GCN models for all settings. Compared to other equivariant models, IsoGCN has
competitive performance compared to equivariant models with shorter inference time as shown in
Table 7. Therefore, it can be found out the proposed model has a strong expressive power to ex-
press differential regarding space with less computation resources compared to the TFN and SE(3)-
Transformer.
F Experiments details: anis otropic nonlinear heat equation
DATASET
F.1 Dataset
The purpose of the experiment was to solve the anisotropic nonlinear heat diffusion under an adia-
batic boundary condition. The governing equation is defined as follows:
Ω ⊂ R3,	(42)
∂T (x, t)
∂t
▽ • C(T(x,t))VT(x,t), in Ω,
(43)
T(x,t = 0) = T0,0(x), in Ω,
VT(x,t)∣χ=χb ∙ n(xb) = 0, on ∂Ω,
(44)
(45)
where T is the temperature field, T0.0 is the initial temperature field, C ∈ Rd×d is an anisotropic
diffusion tensor and n(xb) is the normal vector at Xb ∈ ∂Ω. Here, C depends on temperature
thus the equation is nonlinear. We randomly generate C(T = -1) for it to be a positive semidef-
inite symmetric tensor with eigenvalues varying from 0.0 to 0.02. Then, we defined the linear
temperature dependency the slope of which is -C(T = -1)/4. The function of the anisotropic
diffusion tensor is uniform for each sample. The task is defined to predict the temperature field at
t = 0.2, 0.4, 0.6, 1.0 (T0.2, T0.4, T0.6, T0.8, T1.0) from the given initial temperature field, material
property, and mesh geometry. However, the performance is evaluated only with T1.0 to focus on the
predictive performance. We inserted other output features to stabilize the trainings. Accordingly,
the diffusion number of this problem is C∆t/(∆x)2 ` 10.04 assuming ∆x ` 10.0-3.
Figure 5 represents the process of generating the dataset. We generated up to 9 FEA results for each
CAD shape. To avoid data leakage in terms of the CAD shapes, we first split them into training,
validation, and test datasets, and then applied the following process.
19
Published as a conference paper at ICLR 2021
Table 6: Summary of the test losses (mean squared error ± the standard error of the mean in the
original scale) of the differential operator dataset: 0 → 1 (the scalar field to the gradient field),
0 → 2 (the scalar field to the Hessian field), 1 → 0 (the gradient field to the Laplacian field), and
1 → 2 (the gradient field to the Hessian field). Here, if “x” is “Yes”, x is also in the input feature.
Method	# hops	x	Loss of 0 → 1 ×10-5	Loss of 0 → 2 ×10-6	Loss of 1 → 0 ×10-6	Loss of 1 → 2 ×10-6
	2	No	151.19 ± 0.53	49.10 ± 0.36	542.52 ± 2.14	59.65 ± 0.46
GIN	2	Yes	147.10 ± 0.51	47.56 ± 0.35	463.79 ± 2.08	50.73 ± 0.40
	5	No	151.18 ± 0.53	48.99 ± 0.36	542.54 ± 2.14	59.64 ± 0.46
	5	Yes	147.07 ± 0.51	47.35 ± 0.35	404.92 ± 1.74	46.18 ± 0.39
	2	No	151.18 ± 0.53	43.08 ± 0.31	542.74 ± 2.14	59.65 ± 0.46
GCNII	2	Yes	151.14 ± 0.53	40.72 ± 0.29	194.65 ± 1.00	45.43 ± 0.36
	5	No	151.11 ± 0.53	32.85 ± 0.23	542.65 ± 2.14	59.66 ± 0.46
	5	Yes	151.13 ± 0.53	31.87 ± 0.22	280.61 ± 1.30	39.38 ± 0.34
	2	No	151.17 ± 0.53	50.26 ± 0.38	542.90 ± 2.14	59.65 ± 0.46
SGCN	2	Yes	151.12 ± 0.53	49.96 ± 0.37	353.29 ± 1.49	59.61 ± 0.46
	5	No	151.12 ± 0.53	55.02 ± 0.42	542.73 ± 2.14	59.64 ± 0.46
	5	Yes	151.16 ± 0.53	55.08 ± 0.42	127.21 ± 0.63	56.97 ± 0.44
	2	No	151.23 ± 0.53	49.59 ± 0.37	542.54 ± 2.14	59.64 ± 0.46
GCN	2	Yes	151.14 ± 0.53	47.91 ± 0.35	542.68 ± 2.14	59.60 ± 0.46
	5	No	151.18 ± 0.53	50.58 ± 0.38	542.53 ± 2.14	59.64 ± 0.46
	5	Yes	151.14 ± 0.53	48.50 ± 0.35	542.30 ± 2.14	25.37 ± 0.28
	2	No	151.19 ± 0.53	33.39 ± 0.24	542.54 ± 2.14	59.66 ± 0.46
Cluster-GCN	2	Yes	147.23 ± 0.51	32.29 ± 0.24	167.73 ± 0.83	17.72 ± 0.17
	5	No	151.15 ± 0.53	28.79 ± 0.21	542.51 ± 2.14	59.66 ± 0.46
	5	Yes	146.91 ± 0.51	26.60 ± 0.19	185.21 ± 0.99	18.18 ± 0.20
TFN	2	No	2.47 ± 0.02	OOM	26.69 ± 0.24	OOM
	5	No	OOM	OOM	OOM	OOM
SE(3)-Trans.	2	No	1.79 ± 0.02	-3.50 ± 0.04-	2.52 ± 0.02	OOM
	5	No	2.12 ± 0.02	OOM	7.66 ± 0.05	OOM
IsoGCN (Ours)	2	No	2.67 ± 0.02	-6.37 ± 0.07-	7.18 ± 0.06	1.44 ± 0.02
	5	No	14.19 ± 0.10	21.72 ± 0.25	34.09 ± 0.19	8.32 ± 0.09
Table 7: Summary of the inference time on the test dataset. 0 → 1 corresponds to the scalar field to
the gradient field, and 0 → 2 corresponds to the scalar field to the Hessian field. Each computation
was run on the same GPU (NVIDIA Tesla V100 with 32 GiB memory). OOM denotes the out-of-
memory of the GPU.
0→1
0→2
Method	# parameters	Inference time [s]	# parameters	Inference time [s]
TFN	5264 =	3.8	5220 =	OOM
SE(3)-Trans.	5392	4.0	5265	9.2
IsoGCN (Ours)	4816	0.4	4816	0.7
Using one CAD shape, we generated up to three meshes using clscale (a control parameter of the
mesh characteristic lengths) = 0.20, 0.25, and 0.30. To facilitate the training process, we scaled the
meshes to fit into a cube with an edge length equal to 1.
Using one mesh, we generated three initial conditions randomly using a Fourier series of the 2nd
to 10th orders. We then applied an FEA to each initial condition and material property determined
randomly as described above. We applied an implicit method to solve time evolutions and a direct
method to solve the linear equations. The FEA time step ∆t was set to 0.01.
During this process, some of the meshes or FEA results may not have been available due to excessive
computation time or non-convergence. Therefore, the size of the dataset was not exactly equal to
the number multiplied by 9. Finally, we obtained 439 FEA results for the training dataset, 143 FEA
results for the validation dataset, and 140 FEA results for the test dataset.
20
Published as a conference paper at ICLR 2021
Figure 5: The process of generating the dataset. A smaller clscale parameter generates smaller
meshes.
F.2 Input features
To express the geometry information, we extracted the effective volume of the ith vertex Vieffective
and the mean volume of the ith vertex Vimean , which are defined as follows:
V effective
eXe 1 Ve，
V mean
Σe∈Ni≡ 嵋
INeI
(46)
(47)
where Nie is the set of elements, including the ith vertex.
For GCN or its variant models, we tested several combinations of input vertex features T0.0, C,
Veffective, Vmean, and x (Table 9). For the IsoGCN model, inputs were T0.0, C, V effective, and
Vmean.
F.3 Model architectures
Figure 6 represents the IsoGCN model used for the anisotropic nonlinear heat equation dataset. We
used the tanh activation function as a nonlinear activation function because we expect the target tem-
perature field to be smooth. Therefore, we avoid using non-differentiable activation functions such as
the rectified linear unit (ReLU) (Nair & Hinton, 2010). Although the model looks complicated, one
propagation block corresponds to the first-order Taylor expansion T(t + ∆t) ` VC Θ VT(t) + T(t)
because the propagation block is expressed as D Θ C Θ MLP(T)D * T + T, where T denotes the
21
Published as a conference paper at ICLR 2021
Figure 6: The IsoGCN model used for the anisotropic nonlinear heat equation dataset. The numbers
in each box denote the number of units. Below the unit numbers, the activation function used for
each layer is also shown. 0 denotes multiplication in the feature direction, Θ denotes the contraction,
and ㊉ denotes the addition in the feature direction.
Table 8: Summary of the hyperparameter setting for both the TFN and SE(3)-Transformer. For the
parameters not written in the table, We used the default setting in the implementation of https：
//github.com/FabianFuchsML/se3-transformer-public.
#	hidden layers	1
#	NL layers in the self-interaction	1
#	channels	16
#	maximum rank of the hidden layers 2
#	nodes in the radial function	32
rank-0 tensor input to the propagation block. By stacking this propagation block P times, We can
approximate the pth order Taylor expansion of the anisotropic nonlinear heat equation.
For GCN and its variants, We simply replaced the IsoGCN layers with the corresponding ones. We
stacked m (= 2, 5) layers for GCN, GIN, GCNII, and Cluster-GCN. We used an m hop adjacency
matrix for SgCN.
For the TFN and SE(3)-Transformer, we set the hyperparameters to as many parameters as possible
that would fit on the GPU because the TFN and SE(3)-Transformer with almost the same number
of parameters as in IsoGCN did not fit on the GPU we used (NVIDIA Tesla V100 with 32 GiB
memory). The settings of the hyperparameters are shown in Table 8.
F.4 Result details
Table 9 shows a detailed comparison of the training results. The inclusion of x in the input features
of the baseline models did not improve the performance. In addition, if x is included in the input
features, a loss of the generalization capacity for larger shapes compared to the training dataset
may result as it extrapolates. The proposed model achieved the best performance compared to the
baseline models considered. Therefore, we concluded that the essential features regarding the mesh
shapes are included in D. Besides, IsoGCN can scale up to meshes with 1M vertices as shown in
Figure 7.
22
Published as a conference paper at ICLR 2021
Table 9: Summary of the test losses (mean squared error ± the standard error of the mean in the
original scale) of the anisotropic nonlinear heat dataset. Here, if “x” is “Yes”, x is also in the input
feature. OOM denotes the out-of-memory on the applied GPU (32 GiB).
Method	# hops	x	Loss ×10-3
	2~~	No	16.921 ± 0.040
CTNr GIN	2	Yes	18.483 ± 0.025
	5	No	22.961 ± 0.056
	5	Yes	17.637 ± 0.046
	2~~	No	10.427 ± 0.028
Cr''NΓ GCN	2	Yes	11.610 ± 0.032
	5	No	12.139 ± 0.031
	5	Yes	11.404 ± 0.032
	2~~	No	9.595 ± 0.026
πc,nγtt GCNII	2	Yes	9.789 ± 0.028
	5	No	8.377 ± 0.024
	5	Yes	9.172 ± 0.028
	2~~	No	7.266 ± 0.021
	2	Yes	8.532 ± 0.023
Cluster-GCN			
	5	No	8.680 ± 0.024
	5	Yes	10.712 ± 0.030
	2~~	No	7.317 ± 0.021
SCrVNT SGCN	2	Yes	9.083 ± 0.026
	5	No	6.426 ± 0.018
	5	Yes	6.519 ± 0.020
TPNT TFN	2~~	No	15.661 ± 0.019
	5	No	OOM
	2	No	14.164 ± 0.018
SE(3)-Trans.	5	No	OOM
	2~~	No	4.674 ± 0.014
IsoGCN (Ours)	5	No	2.470 ± 0.008
23
Published as a conference paper at ICLR 2021
Figure 7: Comparison between (left) samples in the training dataset, (center) ground truth computed
through FEA, and (right) ISoGCN inference result. For both the ground truth and inference result,
|V| = 1,011, 301. One can see that IsoGCN can predict the temperature field for a mesh, which is
much larger than these in the training dataset.
山 ɑ∩l4ɑ 山 d∣Λ□ι
24