Published as a conference paper at ICLR 2021
Self-supervised Representation Learning with
Relative Predictive Coding
Yao-Hung Hubert Tsai1, Martin Q. Ma1, Muqiao Yang1,
Han Zhao23, Louis-Philippe Morency1, Ruslan Salakhutdinov1
1Carnegie Mellon University, 2D.E. Shaw & Co., 3 University of Illinois at Urbana-Champaign
Ab stract
This paper introduces Relative Predictive Coding (RPC), a new contrastive repre-
sentation learning objective that maintains a good balance among training stabil-
ity, minibatch size sensitivity, and downstream task performance. The key to the
success of RPC is two-fold. First, RPC introduces the relative parameters to reg-
ularize the objective for boundedness and low variance. Second, RPC contains no
logarithm and exponential score functions, which are the main cause of training
instability in prior contrastive objectives. We empirically verify the effectiveness
of RPC on benchmark vision and speech self-supervised learning tasks. Lastly, we
relate RPC with mutual information (MI) estimation, showing RPC can be used
to estimate MI with low variance 1.
1	Introduction
Unsupervised learning has drawn tremendous attention recently because it can extract rich repre-
sentations without label supervision. Self-supervised learning, a subset of unsupervised learning,
learns representations by allowing the data to provide supervision (Devlin et al., 2018). Among
its mainstream strategies, self-supervised contrastive learning has been successful in visual object
recognition (He et al., 2020; Tian et al., 2019; Chen et al., 2020c), speech recognition (Oord et al.,
2018; RiViere et al., 2020), language modeling (Kong et al., 2019), graph representation learning
(Velickovic et al., 2019) and reinforcement learning (Kipf et al., 2019). The idea of self-supervised
contrastiVe learning is to learn latent representations such that related instances (e.g., patches from
the same image; defined as positive pairs) will haVe representations within close distance, while
unrelated instances (e.g., patches from two different images; defined as negative pairs) will haVe
distant representations (Arora et al., 2019).
Prior work has formulated the contrastiVe learning objectiVes as maximizing the diVergence between
the distribution of related and unrelated instances. In this regard, different diVergence measurement
often leads to different loss function design. For example, Variational mutual information (MI) esti-
mation (Poole et al., 2019) inspires ContrastiVe PredictiVe Coding (CPC) (Oord et al., 2018). Note
that MI is also the KL-diVergence between the distributions of related and unrelated instances (CoVer
& Thomas, 2012). While the choices of the contrastiVe learning objectiVes are abundant (Hjelm
et al., 2018; Poole et al., 2019; Ozair et al., 2019), we point out that there are three challenges faced
by existing methods.
The first challenge is the training stability, where an unstable training process with high Variance
may be problematic. For example, Hjelm et al. (2018); Tschannen et al. (2019); Tsai et al. (2020b)
show that the contrastiVe objectiVes with large Variance cause numerical issues and haVe a poor
downstream performance with their learned representations. The second challenge is the sensitiVity
to minibatch size, where the objectiVes requiring a huge minibatch size may restrict their practical
usage. For instance, SimCLRV2 (Chen et al., 2020c) utilizes CPC as its contrastiVe objectiVe and
reaches state-of-the-art performances on multiple self-superVised and semi-superVised benchmarks.
Nonetheless, the objectiVe is trained with a minibatch size of 8, 192, and this scale of training re-
quires enormous computational power. The third challenge is the downstream task performance,
which is the one that we would like to emphasize the most. For this reason, in most cases, CPC
1Project page: https://github.com/martinmamql/relative_predictive_coding
1
Published as a conference paper at ICLR 2021
Table 1: Different contrastive learning objectives, grouped by measurements of distribution divergence. PXY
represents the distribution of related samples (positively-paired), and PXPY represents the distribution of un-
related samples (negatively-paired). f (x, y) ∈ F for F being any class of functions f : X × Y → R.
f: Compared to JCPC and JRPC, We empirically find JWPC performs worse on complex real-world image
datasets spanning CIFAR-10/-100 (Krizhevsky et al., 2009) and ImageNet (Russakovsky et al., 2015).
Good Training 	ObjeCtiVe	StabiIity	Lower Minibatch Size Sensitivity	Good Downstream Performance
relating to KL-divergence between PXY and PXPY : JDV (Donsker & Varadhan, 1975), JNWJ (Nguyen et al., 2010), and JCPC (Oord et al., 2018)		
JDV(X,Y) ：= suPf∈FEPXY [f(χ,y')] - log(EPχPγ [ef (XMD	X	✓	X
JNWJ(X,Y) =suPf∈F EPXY [f (χ,y)↑ - EPχPγ [ef(x,y)T]	X	✓	X
JCPC(X Y) ：= suPf∈FE(χ,yι/PXY,{yj}N=2〜PY h log ʤ1 /含 Pj=I "("加)]	✓	X	✓
relating to JS-divergence between PXY and PXPY : JJS (Nowozin et al., 2016)		
JJS(X, Y) =supf∈F EPXY [-log(1 + e-f(x,y))] - EPXPY [log(1+ ef(x，y))]	/	✓	X
relating to Wasserstein-divergence between PXY and PXPY : JWPC (Ozair et al., 2019), with FL denoting the space of 1-Lipschitz functions		
JWPC(X,Y) ：= SUPf∈FLE(χ,yι)〜PXY,{%也〜PYhlog (ef(x,y1)/N PN=Ief(X,yj))i	✓	✓	Xt
relating to χ2-divergence between PXY and PXPY : JRPC (ours)		
JRPC(X,Y) ：= suPf ∈F EPXY [f(x,y)] — αEPXPY [f (x,y)] — β2 ePxy f2(X,y)] - YEPXPY f2(X,y)]	✓	✓	✓
is the objective that we would adopt for contrastive representation learning, due to its favorable
performance in downstream tasks (Tschannen et al., 2019; Baevski et al., 2020).
This paper presents a new contrastive representation learning objective: the Relative Predictive Cod-
ing (RPC), which attempts to achieve a good balance among these three challenges: training stabil-
ity, sensitivity to minibatch size, and downstream task performance. At the core of RPC is the
relative parameters, which are used to regularize RPC for its boundedness and low variance. From
a modeling perspective, the relative parameters act as a `2 regularization for RPC. From a statis-
tical perspective, the relative parameters prevent RPC from growing to extreme values, as well as
upper bound its variance. In addition to the relative parameters, RPC contains no logarithm and
exponential, which are the main cause of the training instability for prior contrastive learning objec-
tives (Song & Ermon, 2019).
To empirically verify the effectiveness of RPC, we consider benchmark self-supervised represen-
tation learning tasks, including visual object classification on CIFAR-10/-100 (Krizhevsky et al.,
2009), STL-10 (Coates et al., 2011), and ImageNet (Russakovsky et al., 2015) and speech recogni-
tion on LibriSpeech (Panayotov et al., 2015). Comparing RPC to prior contrastive learning objec-
tives, we observe a lower variance during training, a lower minibatch size sensitivity, and consistent
performance improvement. Lastly, we also relate RPC with MI estimation, empirically showing that
RPC can estimate MI with low variance.
2	Proposed Method
This paper presents a new contrastive representation learning objective - the Relative Predictive
Coding (RPC). At a high level, RPC 1) introduces the relative parameters to regularize the objective
for boundedness and low variance; and 2) achieves a good balance among the three challenges in
the contrastive representation learning objectives: training stability, sensitivity to minibatch size,
and downstream task performance. We begin by describing prior contrastive objectives along with
their limitations on the three challenges in Section 2.1. Then, we detail our presented objective
and its modeling benefits in Section 2.2. An overview of different contrastive learning objectives is
provided in Table 1. We defer all the proofs in Appendix.
Notation We use an uppercase letter to denote a random variable (e.g., X), a lower case letter to
denote the outcome of this random variable (e.g., x), and a calligraphy letter to denote the sample
space of this random variable (e.g., X). Next, if the samples (x, y) are related (or positively-paired),
we refer (x, y)〜PXY with PXY being the joint distribution of X X Y. If the samples (x, y) are
unrelated (negatively-paired), we refer (x, y)〜 PχPY with PXPγ being the product of marginal
distributions over X × Y. Last, we define f ∈ F forF being any class of functions f : X × Y → R.
2.1	Preliminary
Contrastive representation learning encourages the contrastiveness between the positive and the neg-
ative pairs of the representations from the related data X and Y. Specifically, when sampling a pair
2
Published as a conference paper at ICLR 2021
of representations (x, y) from their joint distribution ((x, y)〜 PXY), this pair is defined as a pos-
itive pair; when sampling from the product of marginals ((x, y)〜PXPγ), this pair is defined as
a negative pair. Then, Tsai et al. (2020b) formalizes this idea such that the contrastiveness of the
representations can be measured by the divergence between PXY and PXPY, where higher diver-
gence suggests better contrastiveness. To better understand prior contrastive learning objectives, we
categorize them in terms of different divergence measurements between PXY and PXPY, with their
detailed objectives presented in Table 1.
We instantiate the discussion using Contrastive Predictive Coding (Oord et al., 2018, JCPC), which
is a lower bound of DKL(PXY k PXPY) with DKL referring to the KL-divergence:
ef (x,y1)
JCPC(X,Y) := SUF E(X,yI)〜PXY Nyj }N=2 〜PY [ log . PN	于俳y).	(I)
f	N j=1=↑. e
Then, Oord et al. (2018) presents to maximize JCPC (X, Y), so that the learned representations X
and Y have high contrastiveness. We note that JCPC has been commonly used in many recent
self-supervised representation learning frameworks (He et al., 2020; Chen et al., 2020b), where they
constrain the function to be f (x,y) = Cosine(x,y) with Cosine(∙) being cosine similarity. Under
this function design, maximizing JCPC leads the representations of related pairs to be close and
representations of unrelated pairs to be distant.
The category of modeling DKL (PXY k PXPY) also includes the Donsker-Varadhan objective
(JDV (Donsker & Varadhan, 1975; Belghazi et al., 2018)) and the Nguyen-Wainright-Jordan ob-
jective (JNWJ (Nguyen et al., 2010; Belghazi et al., 2018)), where Belghazi et al. (2018); Tsai
et al. (2020b) show that JDV (X, Y) = JNWJ (X, Y) = DKL (PXY k PXPY). The other diver-
gence measurements considered in prior work are DJS (PXY k PXPY) (with DJS referring to the
Jenson-Shannon divergence) and DWass(PXY k PXPY) (with DWass referring to the Wasserstein-
divergence). The instance of modeling DJS(PXY k PXPY) is the Jensen-Shannon f-GAN objective
(JJS (Nowozin et al., 2016; HjeIm et al., 2018)), where Jjs(X,Y) = 2(DJS(PXY ∣∣ PXPY)-
log 2 .2 The instance of modeling DWass(PXY k PXPY) is the Wasserstein Predictive Coding
JWPC (Ozair et al., 2019) , where JWPC(X, Y) modifies JCPC (X, Y) objective (equation 1) by
searching the function from F to FL. FL denotes any class of 1-Lipschitz continuous functions
from (X × Y) to R, and thus FL ⊂ F. Ozair et al. (2019) shows that JWPC (X, Y) is the lower
bound of both DKL(PXY ∣ PXPY) and DWass(PXY ∣ PXPY). See Table 1 for all the equations. To
conclude, the contrastive representation learning objectives are unsupervised representation learning
methods that maximize the distribution divergence between PXY and PXPY. The learned represen-
tations cause high contrastiveness, and recent work (Arora et al., 2019; Tsai et al., 2020a) theoret-
ically show that highly-contrastive representations could improve the performance on downstream
tasks.
After discussing prior contrastive representation learning objectives, we point out three challenges in
their practical deployments: training stability, sensitivity to minibatch training size, and downstream
task performance. In particular, the three challenges can hardly be handled well at the same time,
where we highlight the conclusions in Table 1. Training Stability: The training stability highly
relates to the variance of the objectives, where Song & Ermon (2019) shows that JDV and JNWJ
exhibit inevitable high variance due to their inclusion of exponential function. As pointed out by Tsai
et al. (2020b), JCPC, JWPC, and JJS have better training stability because JCPC and JWPC can
be realized as a multi-class classification task and JJS can be realized as a binary classification
task. The cross-entropy loss adopted in JCPC, JWPC, and JJS is highly-optimized and stable in
existing optimization package (Abadi et al., 2016; Paszke et al., 2019). Sensitivity to minibatch
training size: Among all the prior contrastive representation learning methods, JCPC is known to
be sensitive to the minibatch training size (Ozair et al., 2019). Taking a closer look at equation 1,
JCPC deploys an instance selection such that y1 should be selected from {y1, y2,…，yN}, with
(x, yι)〜PXY, (x, y7->ι)〜PXPγ with N being the minibatch size. Previous work (Poole et al.,
2019; Song & Ermon, 2019; Chen et al., 2020b; Caron et al., 2020) showed that a large N results in
a more challenging instance selection and forces JCPC to have a better contrastiveness ofy1 (related
instance for x) against {yj }jN=2 (unrelated instance for x). JDV, JNWJ, and JJS do not consider
2Jjs (X,Y) achieves its supreme value when f * (x,y) = log(p(x, y)/p(x)p(y)) (Tsai et al., 2020b). Plug-
in f *(x,y) into Jjs(X,Y), we can conclude Jjs(X,Y) = 2(DJS(PXY ∣∣ PXPY) - log2).
3
Published as a conference paper at ICLR 2021
the instance selection, and JWPC reduces the minibatch training size sensitivity by enforcing 1-
Lipschitz constraint. Downstream Task Performance: The downstream task performance is what
we care the most among all the three challenges. JCPC has been the most popular objective as
it manifests superior performance over the other alternatives (Tschannen et al., 2019; Tsai et al.,
2020b;a). We note that although JWPC shows better performance on Omniglot (Lake et al., 2015)
and CelebA (Liu et al., 2015) datasets, we empirically find it not generalizing well to CIFAR-10/-
100 (Krizhevsky et al., 2009) and ImageNet (Russakovsky et al., 2015).
2.2 Relative Predictive Coding
In this paper, we present Relative Predictive Coding (RPC), which achieves a good balance among
the three challenges mentioned above:
JRPC(X, Y) =SUp Epχγ[f(x,y)]-αEpχ Pγ[f(x,y)]-β∙ Epχγ [f2 (x, y)] - Y EPX Pγ [f2 (x, y)]
f∈F	2	2
(2)
where α > 0, β > 0, γ > 0 are hyper-parameters and we define them as relative parameters.
Intuitively, JRPC contains no logarithm or exponential, potentially preventing unstable training due
to numerical issues. Now, we discuss the roles of α,β, γ. At a first glance, α acts to discourage the
scores of PXY and PχPγ from being close, and β∕γ acts as a '2 regularization coefficient to stop
f from becoming large. For a deeper analysis, the relative parameters act to regularize our objective
for boundedness and low variance. To show this claim, we first present the following lemma:
Lemma 1 (Optimal Solution for JRPC) Let r(x,y) = Ppxxpyy) be the density ratio. JRPC has the
optimal solution f *(x, y)= βr(xy-+γ := rα,β,γ (x,y) with - Y ≤ rα,β,γ ≤ 1.
Lemma 1 suggests that JRPC achieves its supreme value at the ratio rα,β,γ(X, y) indexed by the
relative parameters α, β, γ (i.e., we term rα,β,γ(X, y) as the relative density ratio). We note that
rα,β,γ (X, y) is an increasing function w.r.t. r(X, y) and is nicely bounded even when r(X, y) is
large. We will now show that the bounded rα,β,γ suggests the empirical estimation of JRPC has
boundeness and low variance. In particular, let {Xi, yi}in=1 be n samples drawn uniformly at random
from PXY and {X0j, yj0 }jm=1 be m samples drawn uniformly at random from PXPY. Then, we use
neural networks to empirically estimate JRPC as JmC:
Definition 1 (JmC, empirical estimation of JRPC) We parametrize f via a family of neural net-
works Fθ ：= {fθ : θ ∈ Θ ⊆ Rd} where d ∈ N and Θ is compact. Then, JmC =
supfθ ∈Fθ n Pn=1 fθ (xi,yi)- m Pm=I αfθ (Xj, yj )-n Pn=I 2 fθ2(xi,yi)- m Pm=I γ f2 (Xj,yj)
Proposition 1 (Boundedness of JRPC, informal) 0 ≤ JRPC ≤ * + 022. Then, with probability at
least 1 一 δ, | Jrpc — JmCI = O(Jd+lon,-)), where n0 = min {n, m}.
Proposition 2 (Variance of JRPC, informal) There exist universal constants ci and c2 that depend
only on α, β, γ, such that Var[ JRpC] = O (^1 + m).
From the two propositions, when m and n are large, i.e.,the sample sizes are large, JmC is bounded,
and its variance vanishes to 0. First, the boundedness of JmnC suggests JmnC will not grow to ex-
tremely large or small values. Prior contrastive learning objectives with good training stability (e.g.,
JCPC/JJS/JWPC) also have the boundedness of their objective values. For instance, the empirical
estimation of JCPC is less than log N (equation 1) (Poole et al., 2019). Nevertheless, JCPC often
performs the best only when minibatch size is large, and empirical performances of JJS and JWPC
are not as competitive as JCPC . Second, the upper bound of the variance implies the training of
JmC Can be stable, and in practice we observe a much smaller value than the stated upper bound.
On the contrary, Song & Ermon (2019) shows that the empirical estimations of JDV and JNWJ
exhibit inevitable variances that grow exponentially with the true DKL(PXY kPX PY ).
Lastly, similar to prior contrastive learning objective that are related to distribution diver-
gence measurement, we associate JRPC with the Chi-square divergence Dχ2 (PXY k PXPY ) =
4
Published as a conference paper at ICLR 2021
EPX PY [r2 (x, y)] - 1 (Nielsen & Nock, 2013). The derivations are provided in Appendix. By hav-
ing P0 = β+γPXY + β++γPXPY as the mixture distribution of PXY and PXPY, We can rewrite
JRPC(X,Y) as JRPC(X,Y) = β++γEpo[rα,β,γ(x,y)]. Hence, JRPC Can be regarded as a gener-
alization of Dχ2 with the relative parameters α, β, γ, where Dχ2 can be recovered from JRPC by
SPecializing α = 0, β = 0 and Y = 1 (e.g., Dχ = 2 Jrpc ∣α=β=0,γ=1 - 1). Note that JRPC may
not be a formal divergence measure with arbitrary α, β, γ .
3	Experiments
We Provide an overview of the exPerimental section. First, we conduct benchmark self-suPervised
rePresentation learning tasks sPanning visual object classification and sPeech recognition. This set
of exPeriments are designed to discuss the three challenges of the contrastive rePresentation learning
objectives: downstream task Performance (Section 3.1), training stability (Section 3.2), and mini-
batch size sensitivity (Section 3.3). We also Provide an ablation study on the choices of the relative
Parameters in JRPC (Section 3.4). On these exPeriments we found that JRPC achieves a lower
variance during training, a lower batch size insensitivity, and consistent Performance imProvement.
Second, we relate JRPC with mutual information (MI) estimation (Section 3.5). The connection
is that MI is an average statistic of the density ratio, and we have shown that the oPtimal solution
of JRPC is the relative density ratio (see Lemma 1). Thus we could estimate MI using the density
ratio transformed from the oPtimal solution of JRPC . On these two sets of exPeriments, we fairly
comPare JRPC with other contrastive learning objectives. Particularly, across different objectives,
we fix the network, learning rate, oPtimizer, and batch size (we use the default configurations sug-
gested by the original implementations from Chen et al.(2020c), RiViere et al. (2020) and Tsai et al.
(2020b).) The only difference will be the objective itself. In what follows, we Perform the first set
of experiments. We defer experimental details in the Appendix.
Datasets. For the Visual objectiVe classification, we consider CIFAR-10/-100 (KrizheVsky et al.,
2009), STL-10 (Coates et al., 2011), and ImageNet (RussakoVsky et al., 2015). CIFAR-10/-100
and ImageNet contain labeled images only, while STL-10 contains labeled and unlabeled images.
For the speech recognition, we consider LibriSpeech-100h (PanayotoV et al., 2015) dataset, which
contains 100 hours of 16kHz English speech from 251 speakers with 41 types of phonemes.
Training and Evaluation Details. For the Vision experiments, we follow the setup from Sim-
CLRV2 (Chen et al., 2020c), which considers Visual object recognition as its downstream task. For
the speech experiments, we follow the setup from prior work (Oord et al., 2018; RiViere et al., 2020),
which consider phoneme classification and speaker identification as the downstream tasks. Then, we
briefly discuss the training and eValuation details into three modules: 1) related and unrelated data
construction, 2) pre-training, and 3) fine-tuning and eValuation. For more details, please refer to
Appendix or the original implementations.
. Related and Unrelated Data Construction. In the vision experiment, we construct the related im-
ages by applying different augmentations on the same image. Hence, when (x, y)〜PXY, X and y
are the same image with different augmentations. The unrelated images are two randomly selected
samples. In the speech experiment, we define the current latent feature (feature at time t) and the
future samples (samples at time > t) as related data. In other words, the feature in the latent space
should contain information that can be used to infer future time steps. A latent feature and randomly
selected samples would be considered as unrelated data.
. Pre-training. The pre-training stage refers to the self-supervised training by a contrastive learn-
ing objective. Our training objective is defined in Definition 1, where we use neural networks to
parametrize the function using the constructed related and unrelated data. Convolutional neural net-
works are used for vision experiments. Transformers (Vaswani et al., 2017) and LSTMs (Hochreiter
& Schmidhuber, 1997) are used for speech experiments.
. Fine-tuning and Evaluation. After the pre-training stage, we fix the parameters in the pre-trained
networks and add a small fine-tuning network on top of them. Then, we fine-tune this small network
with the downstream labels in the data’s training split. For the fine-tuning network, both vision
and speech experiments consider multi-layer perceptrons. Last, we evaluate the fine-tuned repre-
sentations on the data’s test split. We would like to point out that we do not normalize the hidden
representations encoded by the pre-training neural network for loss calculation. This hidden nor-
5
Published as a conference paper at ICLR 2021
Table 2: Top-1 accuracy (%) for visual object recognition results. JDV and JNWJ are not reported on ImageNet
due to numerical instability. ResNet depth, width and Selective Kernel (SK) configuration for each setting are
provided in ResNet depth+width+SK column. A slight drop of JCPC performance compared to Chen et al.
(2020c) is because we only train for 100 epochs rather than 800 due to the fact that running 800 epochs
uninterruptedly on cloud TPU is very expensive. Also, we did not employ a memory buffer (He et al., 2020)
to store negative samples. We and we did not employ a memory buffer. We also provide the results from fully
supervised models as a comparison (Chen et al., 2020b;c). Fully supervised training performs worse on STL-10
because it does not employ the unlabeled samples in the dataset (LoWe et al., 2019).
Dataset	ResNet Depth+Width+SK	JDV	JNWJ	Self-su JJS	ervised JWPC	JCPC	JRPC	Supervised
CIFAR-10	18 + 1× +No SK	91.10	90.54	83.55	80.02	91.12	91.46	93.12
CIFAR-10	50+ 1× +No SK	92.23	92.67	87.34	85.93	93.42	93.57	95.70
CIFAR-100	18 + 1× +No SK	77.10	77.27	74.02	72.16	77.36	77.98	79.11
CIFAR-100	50+ 1× +No SK	79.02	78.52	75.31	73.23	79.31	79.89	81.20
STL-10	50+ 1× +No SK	82.25	81.17	79.07	76.50	83.40	84.10	71.40
ImageNet	50+1× +SK	-	-	66.21	62.10	73.48	74.43	78.50
ImageNet	152+2× +SK	-	-	71.12	69.51	77.80	78.40	80.40
Table 3: Accuracy (%) for LibriSpeech-100h phoneme and speaker classification results. We also provide the
results from fully supervised model as a comparison (Oord et al., 2018).
Task Name	Self-supervised JCPC	JDV	JNWJ	JRPC	Supervised
Phoneme classification Speaker classification	64.6	61.27	62.09	69.39 97.4	95.36	95.89	97.68	74.6 98.5
malization technique is Widely applied (Tian et al., 2019; Chen et al., 2020b;c) to stabilize training
and increase performance for prior objectives, but We find it unnecessary in JRPC .
3.1	Downstream Task Performances on Vision and Speech
For the doWnstream task performance in the vision domain, We test the proposed JRPC and other
contrastive learning objectives on CIFAR-10/-100 (Krizhevsky et al., 2009), STL-10 (Coates et al.,
2011), and ImageNet ILSVRC-2012 (Russakovsky et al., 2015). Here We report the best perfor-
mances JRPC can get on each dataset (We include experimental details in A.7.) Table 2 shoWs that
the proposed JRPC outperforms other objectives on all datasets. Using JRPC on the largest netWork
(ResNet With depth of 152, channel Width of 2 and selective kernels), the performance jumps from
77.80% of JCPC to 78.40% of JRPC.
Regarding speech representation learning, the doWnstream performance for phoneme and speaker
classification are shoWn in Table 3 (We defer experimental details in Appendix A.9.) Compared to
JCPC, JRPC improves the phoneme classification results With 4.8 percent and the speaker classifi-
cation results With 0.3 percent, Which is closer to the fully supervised model. Overall, the proposed
JRPC performs better than other unsupervised learning objectives on both phoneme classification
and speaker classification tasks.
3.2	Training Stability
We provide empirical training stability comparisons on JDV, JNWJ , JCPC and JRPC by plotting the
values of the objectives as the training step increases. We apply the four objectives to the SimCLRv2
frameWork and train on the CIFAR-10 dataset. All setups of training are exactly the same except the
objectives. From our experiments, JDV and JNWJ soon explode to NaN and disrupt training (shoWn
as early stopping in Figure 1a; extremely large values are not plotted due to scale constraints). On
the other hand, JRPC and JCPC has loW variance, and both enjoy stable training. As a result,
performances using the representation learned from unstable JDV and JNWJ suffer in doWnstream
task, While representation learned by JRPC and JCPC Work much better.
6
Published as a conference paper at ICLR 2021
Figure 1: (a) Empirical values of JDV, JNWJ, JCPC and JRPC performing visual object recognition on
CIFAR-10. JDV and JNWJ soon explode to NaN values and stop the training (shown as early stopping in the
figure), while JCPC and JRPC are more stable. Performance comparison of JCPC and JRPC on (b) CIFAR-10
and (c) LibriSpeech-100h with different minibatch sizes, showing that the performance of JRPC is less sensitive
to minibatch size change compared to JCPC .
3.3	Minibatch Size Sensitivity
We then provide the analysis on the effect of minibatch size on JRPC and JCPC, since JCPC is
known to be sensitive to minibatch size (Poole et al., 2019). We train SimCLRv2 (Chen et al., 2020c)
on CIFAR-10 and the model from RiViere et al. (2020) on LibriSpeech-IOOh using JRPC and JCPC
with different minibatch sizes. The settings of relative parameters are the same as Section 3.2. From
Figure 1b and 1c, we can obserVe that both JRPC and JCPC achieVe their optimal performance at a
large minibatch size. HoweVer, when the minibatch size decreases, the performance of JCPC shows
higher sensitiVity and suffers more when the number of minibatch samples is small. The result
suggests that the proposed method might be less sensitiVe to the change of minibatch size compared
to JCPC giVen the same training settings.
3.4	Effect of Relative Parameters
We study the effect of different combinations of relatiVe parameters in JRPC by comparing down-
stream performances on Visual object recognition. We train SimCLRV2 on CIFAR-10 with dif-
ferent combinations of α, β and γ in JRPC and fix all other experimental settings. We choose
α ∈ {0, 0.001, 1.0}, β ∈ {0, 0.001, 1.0}, γ ∈ {0, 0.001, 1.0} and we report the best performances
under each combination of α, β, and γ. From Figure 2, we first obserVe that α > 0 has better
downstream performance than α = 0 when β and γ are fixed. This obserVation is as expected, since
α > 0 encourages representations of related and unrelated samples to be pushed away. Then, we
find that a small but nonzero β (β = 0.001) and a large γ (γ = 1.0) giVe the best performance
compared to other combinations. Since β and γ serVe as the coefficients of `2 regularization, the
results imply that the regularization is a strong and sensitiVe factor that will influence the perfor-
mance. The results here are not as competitiVe as Table 2 because the CIFAR-10 result reported in
Table 2 is using a set of relatiVe parameters (α = 1.0, β = 0.005, γ = 1.0) that is different from the
combinations in this subsection. Also, we use quite different ranges of γ on ImageNet (see A.7 for
details.) In conclusion, we find empirically that a non-zero α, a small β and a large γ will lead to
the optimal representation for the downstream task on CIFAR-10.
3.5	Relation to Mutual Information Estimation
The presented approach also closely relates to mutual information estimation. For random Variables
X and Y with joint distribution PXY and product of marginals PXPY , the mutual information is
defined as I(X; Y) = DKL(Pxy∣∣PχPY). Lemma 1 states that given optimal solution f *(x, y) of
Jrpc, We Can get the density ratio r(x, y) := p(x, y)∕p(x)p(y) as r(x, y) = 1-βf+,y) — Y.We
can empirically estimate r(x, y) from the estimated f (x,y) Via this transformation, and use r(x, y)
to estimate mutual information ( Ii et al., 2020 ). Specifically, I(X; Y) ≈ 1 Pn=I log r(xi, yi)
with (xi, yi)〜PXrY, where PXrY is the uniformly sampled empirical distribution of Px,y.
7
Published as a conference paper at ICLR 2021
Figure 2: Heatmaps of downstream task performance on CIFAR-10, using different α, β and γ in the JRPC.
We conclude that a nonzero α, a small β (β = 0.001) and a large γ(γ = 1.0) are crucial for better performance.
q
i-I
10	41.8	71.95
10	60.05	73.52
85.97	90.76	83.29
0.0	0.001	1.0
10	61.22	73.27 I
38.76	87.5	76.45
81.49	91.3	85.23
Figure 3: Mutual information estimation performed on 20-d correlated Gaussian distribution, with the corre-
lation increasing each 4K steps. JRPC exhibits smaller variance than SMILE and DoE, and smaller bias than
JCPC.
We follow prior work (Poole et al., 2019; Song & Ermon, 2019; Tsai et al., 2020b) for the experi-
ments. We consider X and Y as two 20-dimensional Gaussians with correlation ρ, and our goal is
to estimate the mutual information I(X; Y ). Then, we perform a cubic transformation on y so that
y 7→ y3 . The first task is referred to as Gaussian task and the second is referred to as Cubic task,
where both have the ground truth I(X; Y ) = -10log (1 - ρ2). The models are trained on 20, 000
steps with I(X; Y ) starting at 2 and increased by 2 per 4, 000 steps. Our method is compared with
baseline methods JCPC (Oord et al., 2018), JNWJ (Nguyen et al., 2010), JJS (Nowozin et al., 2016),
SMILE (Song & Ermon, 2019) and Difference of Entropies (DoE) (McAllester & Stratos, 2020).
All approaches use the same network design, learning rate, optimizer and minibatch size for a fair
comparison. First, we observe JCPC (Oord et al., 2018) has the smallest variance, while it exhibits a
large bias (the estimated mutual information from JCPC has an upper bound log(batch size)). Sec-
ond, JNWJ (Nguyen et al., 2010) and JJSD (Poole et al., 2019) have large variances, especially in
the Cubic task. Song & Ermon (2019) pointed out the limitations of JCPC, JNWJ, and JJSD, and
developed the SMILE method, which clips the value of the estimated density function to reduce the
variance of the estimators. DoE (McAllester & Stratos, 2020) is neither a lower bound nor a upper
bound of mutual information, but can achieve accurate estimates when underlying mutual informa-
tion is large. JRPC exhibits comparable bias and lower variance compared to the SMILE method,
and is more stable than the DoE method. We would like to highlight our method’s low-variance
property, where we neither clip the values of the estimated density ratio nor impose an upper bound
of our estimated mutual information.
4	Related Work
As a subset of unsupervised representation learning, self-supervised representation learning (SSL)
adopts self-defined signals as supervision and uses the learned representation for downstream tasks,
such as object detection and image captioning (Liu et al., 2020). We categorize SSL work into two
groups: when the signal is the input’s hidden property or the corresponding view of the input. For
the first group, for example, Jigsaw puzzle (Noroozi & Favaro, 2016) shuffles the image patches and
defines the SSL task for predicting the shuffled positions of the image patches. Other instances are
Predicting Rotations (Gidaris et al., 2018) and Shuffle & Learn (Misra et al., 2016). For the second
group, the SSL task aims at modeling the co-occurrence of multiple views of data, via the contrastive
or the predictive learning objectives (Tsai et al., 2020a). The predictive objectives encourage recon-
struction from one view of the data to the other, such as predicting the lower part of an image from
8
Published as a conference paper at ICLR 2021
its upper part (ImageGPT by Chen et al. (2020a)). Comparing the contrastive with predictive learn-
ing approaches, Tsai et al. (2020a) points out that the former requires less computational resources
for a good performance but suffers more from the over-fitting problem.
Theoretical analysis (Arora et al., 2019; Tsai et al., 2020a; Tosh et al., 2020) suggests the con-
trastively learned representations can lead to a good downstream performance. Beyond the theory,
Tian et al. (2020) shows what matters more for the performance are 1) the choice of the contrastive
learning objective; and 2) the creation of the positive and negative data pairs in the contrastive
objective. Recent work (Khosla et al., 2020) extends the usage of contrastive learning from the self-
supervised setting to the supervised setting. The supervised setting defines the positive pairs as the
data from the same class in the contrastive objective, while the self-supervised setting defines the
positive pairs as the data with different augmentations.
Our work also closely rates to the skewed divergence measurement between distributions (Lee, 1999;
2001; Nielsen, 2010; Yamada et al., 2013). Recall that the usage of the relative parameters plays a
crucial role to regularize our objective for its boundness and low variance. This idea is similar to
the skewed divergence measurement, that when calculating the divergence between distributions P
and Q, instead of considering D(P k Q), these approaches consider D(P k αP + (1 - α)Q) with
D representing the divergence and 0 < α < 1. A natural example is that the Jensen-Shannon
divergence is a symmetric skewed KL divergence: DJS (P k Q) = 0.5DKL(P k 0.5P + 0.5Q) +
0.5DKL(Q k 0.5P + 0.5Q). Compared to the non-skewed counterpart, the skewed divergence has
shown to have a more robust estimation for its value (Lee, 1999; 2001; Yamada et al., 2013). Dif-
ferent from these works that focus on estimating the values of distribution divergence, we focus on
learning self-supervised representations.
5	Conclusion
In this work, we present RPC, the Relative Predictive Coding, that achieves a good balance among
the three challenges when modeling a contrastive learning objective: training stability, sensitivity
to minibatch size, and downstream task performance. We believe this work brings an appealing
option for training self-supervised models and inspires future work to design objectives for balancing
the aforementioned three challenges. In the future, we are interested in applying RPC in other
application domains and developing more principled approaches for better representation learning.
Acknowledgement
This work was supported in part by the NSF IIS1763562, NSF Awards #1750439 #1722822, Na-
tional Institutes of Health, IARPA D17PC00340, ONR Grant N000141812861, and Facebook PhD
Fellowship. We would also like to acknowledge NVIDIA’s GPU support and Cloud TPU support
from Google’s TensorFlow Research Cloud (TFRC).
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, MatthieU
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-
scale machine learning. In 12th {USENIX} Symposium on Operating Systems Design and Imple-
mentation ({OSDI} 16),pp. 265-283, 2016.
Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. cambridge
university press, 2009.
Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saun-
shi. A theoretical analysis of contrastive unsupervised representation learning. arXiv preprint
arXiv:1902.09229, 2019.
Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A frame-
work for self-supervised learning of speech representations. arXiv preprint arXiv:2006.11477,
2020.
9
Published as a conference paper at ICLR 2021
Peter L Bartlett. The sample complexity of pattern classification with neural networks: the size of
the weights is more important than the size of the network. IEEE transactions on Information
Theory, 44(2):525-536,1998.
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron
Courville, and R Devon Hjelm. Mine: mutual information neural estimation. arXiv preprint
arXiv:1801.04062, 2018.
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint
arXiv:2006.09882, 2020.
Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan,
and Ilya Sutskever. Generative pretraining from pixels. In Proceedings of the 37th International
Conference on Machine Learning, 2020a.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020b.
Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big self-
supervised models are strong semi-supervised learners. arXiv preprint arXiv:2006.10029, 2020c.
Ching-Yao Chuang, Joshua Robinson, Lin Yen-Chen, Antonio Torralba, and Stefanie Jegelka. De-
biased contrastive learning. arXiv preprint arXiv:2007.00224, 2020.
Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised
feature learning. In Proceedings of the fourteenth international conference on artificial intelli-
gence and statistics, pp. 215-223, 2011.
Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Monroe D Donsker and SR Srinivasa Varadhan. Asymptotic evaluation of certain markov process
expectations for large time, i. Communications on Pure and Applied Mathematics, 28(1):1-47,
1975.
Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by
predicting image rotations. arXiv preprint arXiv:1803.07728, 2018.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of wasserstein gans. In Advances in neural information processing systems, pp.
5767-5777, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 9729-9738, 2020.
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. arXiv preprint arXiv:1808.06670, 2018.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
K Hornik, M Stinchcombe, and H White. Multilayer feedforward networks are universal aPProxi-
mators. Neural Networks, 2(5):359-366, 1989.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deeP network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
10
Published as a conference paper at ICLR 2021
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron
Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. arXiv preprint
arXiv:2004.11362, 2020.
Thomas Kipf, Elise van der Pol, and Max Welling. Contrastive learning of structured world models.
arXiv preprint arXiv:1911.12247, 2019.
Lingpeng Kong, Cyprien de Masson d’Autume, Wang Ling, Lei Yu, Zihang Dai, and Dani Yo-
gatama. A mutual information maximization perspective of language representation learning.
arXiv preprint arXiv:1910.08350, 2019.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning
through probabilistic program induction. Science, 350(6266):1332-1338, 2015.
Lillian Lee. Measures of distributional similarity. In Proceedings of the 37th Annual Meeting
of the Association for Computational Linguistics, pp. 25-32, College Park, Maryland, USA, June
1999. Association for Computational Linguistics. doi: 10.3115/1034678.1034693. URL https:
//www.aclweb.org/anthology/P99-1004.
Lillian Lee. On the effectiveness of the skew divergence for statistical language analysis. In AIS-
TATS. Citeseer, 2001.
Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selective kernel networks. In Proceedings of
the IEEE conference on computer vision and pattern recognition, pp. 510-519, 2019.
Xiao Liu, Fanjin Zhang, Zhenyu Hou, Zhaoyu Wang, Li Mian, Jing Zhang, and Jie Tang. Self-
supervised learning: Generative or contrastive. arXiv e-prints, pp. arXiv-2006, 2020.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of the IEEE international conference on computer vision, pp. 3730-3738, 2015.
Sindy Lowe, Peter O'Connor, and Bastiaan Veeling. Putting an end to end-to-end: Gradient-isolated
learning of representations. In Advances in Neural Information Processing Systems, pp. 3039-
3051, 2019.
David McAllester and Karl Stratos. Formal limitations on the measurement of mutual information.
In International Conference on Artificial Intelligence and Statistics, pp. 875-884, 2020.
Ishan Misra, C Lawrence Zitnick, and Martial Hebert. Shuffle and learn: unsupervised learning
using temporal order verification. In European Conference on Computer Vision, pp. 527-544.
Springer, 2016.
XuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence functionals
and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory,
56(11):5847-5861, 2010.
Frank Nielsen. A family of statistical symmetric divergences based on jensen’s inequality. arXiv
preprint arXiv:1009.4004, 2010.
Frank Nielsen and Richard Nock. On the chi square and higher-order chi distances for approximating
f-divergences. IEEE Signal Processing Letters, 21(1):10-13, 2013.
Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw
puzzles. In European Conference on Computer Vision, pp. 69-84. Springer, 2016.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers
using variational divergence minimization. In Advances in neural information processing systems,
pp. 271-279, 2016.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018.
11
Published as a conference paper at ICLR 2021
Sherjil Ozair, Corey Lynch, Yoshua Bengio, Aaron Van den Oord, Sergey Levine, and Pierre Ser-
manet. Wasserstein dependency measure for representation learning. In Advances in Neural
Information Processing Systems,pp. 15604-15614, 2019.
Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus
based on public domain audio books. In 2015 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pp. 5206-5210. IEEE, 2015.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-
performance deep learning library. In Advances in Neural Information Processing Systems, pp.
8024-8035, 2019.
Ben Poole, Sherjil Ozair, Aaron van den Oord, Alexander A Alemi, and George Tucker. On varia-
tional bounds of mutual information. arXiv preprint arXiv:1905.06922, 2019.
Morgane Riviere, Armand Joulin, Pierre-Emmanuel Mazare, and Emmanuel Dupoux. Unsupervised
pretraining transfers well across languages. In ICASSP 2020-2020 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP), pp. 7414-7418. IEEE, 2020.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International journal of computer vision, 115(3):211-252, 2015.
Jiaming Song and Stefano Ermon. Understanding the limitations of variational mutual information
estimators. arXiv preprint arXiv:1910.06222, 2019.
Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. arXiv preprint
arXiv:1906.05849, 2019.
Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What
makes for good views for contrastive learning. arXiv preprint arXiv:2005.10243, 2020.
Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive learning, multi-view redun-
dancy, and linear models. arXiv preprint arXiv:2008.10150, 2020.
Yao-Hung Hubert Tsai, Yue Wu, Ruslan Salakhutdinov, and Louis-Philippe Morency. De-
mystifying self-supervised learning: An information-theoretical framework. arXiv preprint
arXiv:2006.05576, 2020a.
Yao-Hung Hubert Tsai, Han Zhao, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhut-
dinov. Neural methods for point-wise dependency estimation. arXiv preprint arXiv:2006.05553,
2020b.
Michael Tschannen, Josip Djolonga, Paul K Rubenstein, Sylvain Gelly, and Mario Lucic. On mutual
information maximization for representation learning. arXiv preprint arXiv:1907.13625, 2019.
Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Petar Velickovic, William Fedus, William L Hamilton, Pietro Lio, Yoshua Bengio, and R Devon
Hjelm. Deep graph infomax. In ICLR (Poster), 2019.
Makoto Yamada, Taiji Suzuki, Takafumi Kanamori, Hirotaka Hachiya, and Masashi Sugiyama. Rel-
ative density-ratio estimation for robust distribution comparison. Neural computation, 25(5):
1324-1370, 2013.
Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks. arXiv
preprint arXiv:1708.03888, 2017.
12
Published as a conference paper at ICLR 2021
A Appendix
A.1 Proof of Lemma 1 in the Main Text
Lemma 2 (Optimal Solution for JRPC, restating Lemma 1 in the main text) Let
JRPC(X,Y):= sup EPXY [f(x,y)]-αEpχPY [f(x,y)]-IeEpχγ [f2(x,y)] - YEPXPγ [f2(x,y)]
f∈F	2	2
and r(x,y) = Ppxxpyy) be the density ratio. JRPC has the optimal solution
f*(X，y)= β⅞⅛⅛ ：=『"MY(X，y) with - Y ≤『"MY ≤ 1 .
Proof: The second-order functional derivative of the objective is
-βdPX,Y - YdPX PY ,
Which is alWays negative. The negative second-order functional derivative implies the objective has
a supreme value. Then, take the first-order functional derivative djRpc and set it to zero:
dPχ,γ - α ∙ dPχPY - β ∙ f(x, y) ∙ dPχ,γ - Y ∙ f (x, y) ∙ dPχPY = 0.
We then get
f* (x, y) =
dPχ,γ - α ∙ dPχPY _ p(x,y) - αp(x)p(y) _ r(x,y) - α
β ∙ dPχ,γ + Y ∙ dPχPY	βp(χ, y) + Yp(X)P(y)	βr(χ, y) + γ.
Since0 ≤ r(x, y) ≤ ∞,We have-Y ≤ ⅛⅛+γ ≤1. Hence，
∀β = 0,γ = 0, f* (χ, y) ：= rα,β,γ(χ, y) with -
α
1
Y ≤ ra，e，Y ≤ β.

A.2 RELATION BETWEEN JRPC AND Dχ2
In this subsection, We aim to shoW the folloWing: 1) Dχ2 (PXY k PXPY) = EPXPY [『2 (X, y)] - 1;
and 2) JRPC(X,Y) = β++γ Epo [r" ,β,γ (x,y)] by having P0 = β+γ PXY + 七 PX PY as themixture
distribution of PXY and PXPY .
Lemma 3 Dχ2 (PXY k PXPY) = EPX PY [『2 (X, y)] - 1
Proof: By definition (Nielsen & Nock, 2013),
Dχ2 (PXYk PXPY) = Z ⅛⅛ -1 = /(dPP )2 dPXPY-1
=Z ( PpXxly、)2dPXPY - 1= / r2(x,y)dPXPY - 1
p(X)p(y)
= EPXPY [『2(X, y)] - 1.
Lemma 4 Defining P0 = e+γPXY + j+γPXPY as a mixture distribution of PXY and PXPY,
JRPC(X,Y) = β+γEpo [r":(X, y)].
13
Published as a conference paper at ICLR 2021
Proof: Plug in the optimal solution f * (x, y) = ；：p；-αd⅞Pτ (See Lemma 2) into Jrpc：
JRPC = EPχγf*(x,y)] - aEpxPγ[f*(x,y)] - 2EPXYhf*1 2(x,y)i - γEPXPYhf*2(x,y)i
—
/f*(x,y) ∙ (dPχγ - α ∙ dPχPY) - 2f*2(x,y) ∙ (β ∙ dPχγ + Y ∙ dPχPY)
d dPχ,Y - α ∙ dPχPγ f	∖	1 dPχ,γ - α ∙ dPχPγ
β β∙ dPχ,γ + Y ∙ dPχ PY WXY - α ∙ X PY) — 2 Ie ∙ dPχ,γ + Y ∙ dPχ PY
1	dPχ,Y - α ∙ dPχ PY λ2 (
2	∙ dPχ,γ + Y ∙ dPχPγ ) Ie ∙ dPXY + Y ∙ dPXPY)
β	+ Y	dPχ,γ -α ∙ dPXPY ∖2( e dP , Y dP p λ
F-	∙dPχ,γ + Y∙dPχPY) Ie + Y ∙d XY + β+Y ∙d XPY)
2
(β∙dPχγ+Y
∙ dPχ PY
Since We define raβ = AdPX,Y WdpX P耍 and P0 = -ɪPXY + -ɪPχPγ,
ɑ,β,γ	β∙dPχ,γ+γ∙dPχ Pγ	β+γ Xγ I β+γ X γ
JRPC = e+y EP 0 [rα ,β,γ (x,y)].

A.3 Proof of Proposition 1 in the Main Text
The proof contains two parts: showing 0 ≤ JRPC ≤ ɪ + α2 (See Section A.3.1) and JRP^C is a
consistent estimator for JRPC (see Section A.3.2).
A.3. 1 BOUNDNESS OF JRPC
Lemma 5 (Boundness of JRPC) 0 ≤ JRPC ≤ 21β + α
Proof: Lemma 4 suggests JRPC(X, Y) = β+γEp0 [r22出)(x,y)] with P0 = β+γPXY + β++γPχPγ
as the mixture distribution of PχY and PχPY. Hence, it is obvious JRPC (X, Y) ≥ 0.
We leverage the intermediate results in the proof of Lemma 4:
JRPC(X, Y)
1
2
dPχ,γ - α ∙ dPχPY )2
,∙ dPχ,γ + Y ∙ dPχ PY)
e ∙ dPχY + Y ∙ dPχPY)

1	dPχ,Y - α ∙ dPχ PY	α	dPχ,Y - α ∙ dPχ PY
2 J χ,γ ^β ∙ dPχ,γ + Y ∙ dPχPY)	2 J χ Y(e ∙ dPχ,γ + Y ∙ dPχPγ
1α
2 EPxY [rɑ,β,γ (X,y)] - 2 EPx Py ∖rɑ,β,γ (x, y)].
Since -α ≤ ra,β,γ ≤ β, JRPC(X,Y) ≤ 21β + 22γ.
A.3.2 Consistency
We first recall the definition of the estimation of JRPC :
Definition 2 (*JmC, empirical estimation of Jrpc, restating Definition 1 in the main text) We
parametrize f via a family of neural networks FΘ := {fθ : θ ∈ Θ ⊆ Rd} where d ∈ N and Θ is
compact. Let {xi, yi}in=1 be n samples drawn uniformly at random from PχY and {x0j, yj0 }jm=1 be
m samples drawn uniformly at random from PχPY. Then,
1n
JmPC = fSUFθ n Xfθ (xi,yi) -
1m	1ne	1
m Xαfθ (xj ,yj) - n X 2 fθ (xi, yi) - m
j=1
i=1
14
Published as a conference paper at ICLR 2021
Our goal is to show that J1RPC is a consistent estimator for Jrpc. We begin with the following
definition:
1n	1m	1nβ	1m
JRPc,θ :=	n X fθ(χi,yi) -	m X αfθ(xj,yj)	- n X 2f (Xi,y)-	m X	2 f (Xj，引	⑶
i=1	j=1	i=1	j=1
and
E [Jrpc,θi ：= Epxy [fθ(x, y)]-αEpχ Pγ fθ (x, y)] - 2Epχγ [f2 (x, y)] - YEpχPγ [f<2(x, y)]. (4)
Then, we follow the steps:
•	The first part is about estimation. We show that, with high probability, JRPC θ is close to
E [∙JRPC,θ], for any given θ.
•	The second part is about approximation. We will apply the universal approximation lemma
of neural networks (Hornik et al., 1989) to show that there exists a network θ* such that
E [Jrpc,θ*] is close to Jrpc.
Part I - Estimation: With high probability, JRPC,θ is close to E Jrpc,θ], for any given θ.
Throughout the analysis on the uniform convergence, we need the assumptions on the boundness
and smoothness of the function fθ . Since we show the optimal function f is bounded in JRpC,
we can use the same bounded values for fθ without losing too much precision. The smoothness of
the function suggests that the output of the network should only change slightly when only slightly
perturbing the parameters. Specifically, the two assumptions are as follows:
Assumption 1 (boundness of fθ) There exist universal constants such that ∀fθ ∈ FΘ, CL ≤ fθ ≤
CU. For notations simplicity, we let M = CU - CL be the range of fθ and U = max {|CU |, |CL|}
be the maximal absolute value of fθ. In the paper, we Can choose to constrain that CL = 一 Y and
CU = β since the optimal function f * has 一 Y ≤ f * ≤ β.
Assumption 2 (smoothness of fθ) There exists constant ρ > 0 such that ∀(X, y) ∈ (X × Y) and
θ1,θ2 ∈ Θ, f (χ,y) - fθ2 (x,y)| ≤ ρ∣θι -。21.
Now, we can bound the rate of uniform convergence of a function class in terms of covering num-
ber (Bartlett, 1998):
Lemma 6 (Estimation) Let > 0 and N (Θ, ) be the covering number ofΘ with radius . Then,
Pr (Sup IJRPC,θ - EhJRPC,θi I ≥ e)
n2	m2	n2	m2
≤2N(θ, 4ρ(l + α + 2(β + Y)U) "exp( 一 32M2)十呻(一 32MV) +exp( 一 32u2β2) +exp( 一 32W
Proof: For notation simplicity, we define the operators
•	P(f) = Epχγ[f(x,y)] and Pn(f) = 1 Pn=I f(χi,yi)
•	Q(f) = EPXPy[f(x,y)] andQm(f) = . Pm=I f(χj,yj)
Hence,
IJRPC" EhJRPC,θ i∣
=R(fθ) - P (fθ) 一 αQm(fθ) + αQ(fθ) 一 βPn(f2) + βP(f) - γQm(f2) + γQ(f2)∣
≤ ∣Pn(fθ) 一 P(fθ)| + α ∣Qm(fθ) - Q(fθ)∣ + β∣Pn(fθi) - P(f2)∣+ γ∖Qm(f2) - Q(fθ)∣
15
Published as a conference paper at ICLR 2021
Let e0=	巳	and T := N (Θ,e0).LetC = {fθl, fθ2,…，fθτ} with {&也，…,θτ}
4ρ(1+α+2(β+γ)U
be such that B∞(θι, e0),…，B∞(θτ, e0) are e0 cover. Hence, for any fθ ∈ Fθ, there is an fθk ∈ C
such that ∣∣θ - θk∣∣∞ ≤ e0.
Then, for any fθk ∈ C:
∣∙JRPC,θ - EhJRPC,θU
≤ ∖Pn(fθ ) - P (fθ )| + α ∖Qm(fθ) - Q(fθ )| + β ∣ Pn(fθ2) - Pf I + Y ∣Qm(fθ) - Qf ∣
≤ ∣Pn(fθk ) - P(fθk )| + ∣Pn(fθ ) - Pn(fθk )| + ∣P(fθ )- P (fθk )|
+	α(〔Qm(fθk ) - Q(fθk )| + IQm(fθ) - Qm (fθk )| + IQ(fθ ) - Q(fθfc » )
+	β ( I Pn(fθk) - P(fθk )1 + I Pn(f2) - Pn(fθk )| + | P (fθ) - P (fθk ) | )
+	γ( I Qmfk ) - Q(fθk )1 + I Qm(f2) - Qm(fθθk )| + | Q(fθ) - Q(fθk ) | )
≤ ∖Pn(fθk) - P(fθk)| + ρ∣θ - θkIl + ρ∣θ - θkIl
+	α(〔Qm(fθk) - Q(fθk)| + p∣lθ - θkIl + p∣lθ - θkII)
+	β( | Pnf)- P (fθk )1 + 2ρU∣θ - θk∣ + 2ρU∣θ - θk∣∣)
+ γ( | Qmfk ) - Q(fθk )1 + 2PU∣∣θ - θk k + 2PU∣∣θ - θk II)
=|Pn(fθk ) - P (fθk )| + α IQm(fθk ) - Q(fθk )| + β | Pn(fθk) - P (fθk )| + Y | Qm(fθk ) - Qf ) |
+ 2ρ(1+ α + 2(β + Y)U)I∣θ - θkl
≤ ∖Pn(fθk ) - P(fθk )| + a IQm (fθk ) - Qfθk )| + β | Pn(fθk ) - P (fθk )| + Y | Qm(fθk ) - Qfθk )| + ],
where
•	∖Pn(fθ) - Pn(fθk)| ≤ ρ∣∣θ - θkIl due to Assumption 2, and the result also applies for
|P(fθ) - P(fθk)\, IQm(fθ) - Qm(fθk)|, and |Q(fθ) - Q(fθk)|.
•	| Pn (fθ) - Pn(fθk) | ≤ 2∣∣fθ ∣∣∞ ρ∣∣θ-θk∣∣ ≤ 2ρU Iθ-θk Il due to Assumptions 1 and 2. The
result also applies for | P (f ) - P fk ) | , | Qmf ) - Qmfk) |,and | Qf ) - Qfk) |.
Hence,
Pr (Sup| JmC,θ - EhJRPC,θ U ≥ e)
≤ Pr (然∈C |Pn (fθk )	- P (fθk )| + α	IQm(fθk ) - Q(fθk )| + β | Pn f) - P (fθ2k )1+ Y | Qmf)	- Qf )∣	+ e ≥ e)
=Pr (然∈C |Pn (fθk)	- P (fθk )| + α	IQm(fθk) - Q(fθk )| + β | Pn f) - P (fθk )1+ Y | Qmfk)	- Qfk) |	≥ e)
τ
≤ X Pr QPn(fθk ) - P (fθk )| + α IQm(fθk ) - Q(fθk )| + β | Pnfk ) - Pfk )|+ Y | Qmfk ) - Q(fθk ) | ≥ 5)
k = 1
τ
≤ X Pr (∖Pn(fθk) - P (fθk )| ≥ e) +Pr (α Cm(fθk) - Q(fθ% )| ≥ ∣)
k = 1
+ Pr 3 PnlfQ- P(fθk) | ≥ e) + Pr (Y | Qm(fθk) - Q(fθk) | ≥ e) ∙
With Hoeffding,s inequality,
16
Published as a conference paper at ICLR 2021
•	Pr(IPnfθk ) - P (fθk )| ≥ 8) ≤ 2eχp( - 3nMM2)
•	Pr(α ∣Qm(fθk) - Qf )| ≥ 8) ≤ 2exp(
-32M2a2
• Pr (β ∣pn(f2k)- P(fθ2k)∣ ≥ 8) ≤ 2eχp(- 32U2β2)
me2
32U 2γ2
≤2N(θ, 4ρ(l + α +Zβ + Y)U)) (exp( - 3⅛) +呻(-
222
m2	n2	m2
32MV) +exp (- 32U^) +exp (- 32u2γ2

Part II - Approximation: Neural Network Universal Approximation. We leverage the univer-
sal function approximation lemma of neural network
Lemma 7 (Approximation (Hornik et al., 1989)) Let	> 0. There exists d ∈ N and a fam-
ily of neural networks FΘ := {fθ : θ ∈ Θ ⊆ Rd} where Θ is compact, such that
inf IEhJRPc,θi - Jrpc ∣ ≤ e.
fθ ∈FΘ	RP ,	RP
Part III - Bringing everything together. Now, we are ready to bring the estimation and approxi-
mation together to show that there exists a neural network θ* such that, with high probability, JRPC θ
can approximate JRPC with n0 = min {n, m} at a rate of O(1/√n7):
Proposition 3 With probability at IeastI - δ, ∃θ* ∈ Θ, ∣Jrpc - JRPnC θ∣ = O( Jd+lona/Z1),
where n0 = min {n, m}.
Proof: The proof follows by combining Lemma 6 and 7.
First, Lemma 7 suggests, ∃θ* ∈ Θ,
∣E[Jrpc,θ*] — Jrpc∣ ≤ 2∙
Next, we perform analysis on the estimation error, aiming to find n, m and the corresponding prob-
ability, such that
Applying Lemma 6 with the covering number of the neural network:	N(Θ, e)
O 卜xp(d log(1∕e))) (Anthony & Bartlett, 2009)) and let n0 = min{n,m}:
Pr fsuFθJmC,θ-E
≤2N(θ, 8ρ(l + α + 2(β + Y)U)) ,p( -
222	2
n	m	n	m
128M2) +exp(- 128M缜)+exp(-示调炉)+exp(-透U芽))
=O 卜xp(d log (1/e) — n0e2)
17
Published as a conference paper at ICLR 2021
where the big-O notation absorbs all the constants that do not require in the following derivation.
Since we want to bound the probability with 1 - δ, we solve the such that
exp(d log(1∕e) — n0e1 2) ≤ δ.
With log (x) ≤ x - 1,
n0e2 + d(e — 1) ≥ n0e2 + dlog e ≥ log (1∕δ),
where this inequality holds when
d + log(1∕δ)
n0

A.4 Proof of Proposition 2 in the Main Text - From an Asymptotic Viewpoint
Here, We provide the variance analysis on JmC via an asymptotic viewpoint. First, assum-
ing the network is correctly specified, and hence there exists a network parameter θ* satisfying
f *(x, y) = fθ* (x, y) = rα,β,γ(x, y). Then we recall that JmC is a consistent estimator of Jrpc
(see Proposition 3), and under regular conditions, the estimated network parameter θ in JmC sat-
isfying the asymptotic normality in the large sample limit (see Theorem 5.23 in (Van der Vaart,
200 )). We recall the definition of JmC θ in equation 3 and let n0 = min{n, m}, the asymptotic
expansion of JmC has
ʌ... 一	^... --	^...	ʌ	A
JmC,θ* = JmC,^+JmC,θ(θ* - θ)+。(田-。。)
=JmC,^+JmC,θ(θ*- θ)+op( √√no)	(5)
=JmC,^ + op( √=7),
where Jmn ^ = 0 since θ is the estimation from JRmpC = SuP JmC θ .
,	fθ ∈FΘ
Next, we recall the definition in equation 4:
E[JRPC,θ] = epxy[fθ(x,y)] -αEPχpγ[f^(χ,y)] - 2epxy[f2(χ,y)] - γEPXPy[f2(χ,y)].
Likewise, the asymptotic expansion of E[JRpCθ] has
- - , . ʌ , . .. ʌ ,...
E[Jrpc,θ] = E[Jrpc,θ*] + E[Jrpc,θ*](Θ - θ*) + o(∣∣θ - θ*k)
1
=EJRPC,θ*] + E[JRPC,θ*](θ - θ ) + op( √~~o )
=EJRPC,θ*] + op ( /= ),
(6)
where E[Jrpc,θ*] = 0 since E[Jrpc,θ*] = JRPC and θ* satisfying f*(x, y) = fθ* (x, y).
18
Published as a conference paper at ICLR 2021
Combining equations 5 and 6:
jRPC,θ - E[JRPc,θ] =JRPC,θ*- JRPC + op
1n
n^2fθ(xi,yi) -
i=1
α
1 m	β1 n	1 m
m X fθ (Xj,yj) — 2 n X fθ* (XMy)- 2 m X fθ* (Xj,y)
j=1	i=1	j=1
- ePxy [f*(X,y)]+ αEPχPγ [f *(X,y)] + 2EPXYhf *2(x, y)i +2EPXPYhf *2(x, y)i + op(√n)
1 n	1	m	β1 n	Y1	m
n X rα,β,γ(XiJyi)	-	a —	Xi	ra，e，Y (X j,	yj )	-	2~	^X rα,β,γ (Xi, yi)	-	2~	^X	rα,β,γ (Xj,yj)
n—	n	—
i=1	j=1	i=1	j=1
β2	Y	2
- EPXY [rα,β,γ (X,y)] + aEPχ Pγ [rα,β,γ (X,9)] + 2EPXY K,β,γ (X,9)] + 2EPX PY K,β,γ (X,9)]
+op( √
11
1
-------
n
X
i=1
1
rα,β,γ(Xi,yi) - 2rα,β,γ(Xi,yi) - EPXY
m
rα,β,γ(X,y) - 2rα,β,γ(X,y)
X 卜rα,β,γ (Xj ,yj) + Yrα,β,γ(Xj ,yj) - EPX Pγ
αrα,β,γ (X,y) + Y rα ,β,γ (X,y)
+ op
Therefore, the asymptotic Variance of JRPnC is
VarJRPC] = IVarPXY [rα,β,γ (x, y) - βrα ,β,γ (x, y)] + ；IVarPX Py [arα,β,γ (x, y + Yra ,β,γ (x, V)] + o(J )
n	2—	2	n
2aγ + βa2∖2( 1 ∖21
—2Y2 一),(刀 ʃ.
First, we look at VarPXY [ra,β,γ (x,y) - 导 Tra ,β,γ (x,y)]. Since β > 0 and - Y ≤ ra,β,γ ≤ 1 ,simple
calculation gives us - -雪。≤ rα,β,γ(x, y) - Ie以,β,γ(x, V) ≤ 泰.Hence,
β2
VarPXY [ra,β,γ(χ,y) - 2ra,β,γ(x,y)] ≤ max
Next, we look at VarPX Pγ[αrα,β,γ (x,y) + 2 Tra ,β,γ (x,y)]. Since a ≥ 0,γ> 0 and - Y ≤ raβγl ≤
1, simple calculation gives us -aγ ≤ αrα,β,γ(x,y) + γTra,∣,γ(x,V) ≤ 2穿Y. Hence,
VarPX Pγ [ara,β,γ(χ,y) + YTa,β,γ (χ,y)] ≤ max
Combining everything together, we restate the Proposition 2 in the main text:
{(/:(2aβ芦)2}.
Proposition 4 (Asymptotic Variance of JRPnC)
VarJRPC] = IVarPXY [rα,β,γ(x, V) - βra,β,γ(x, V)] + ；IVarPXPγ [ara,β,γ(x, V) + Yra,β,γ(x, V)] + o(g)
n	2—	2	n
≤ 1max[ (2aY +2βa2
_ n	2y2
2aβ + YvI 1
~2β^) /+ o(/)
> + j1-max
—
19
Published as a conference paper at ICLR 2021
A.5 PROOF OF PROPOSITION 2 IN THE MAIN TEXT - FROM BOUNDNESS OF fθ
As discussed in Assumption 1, for the estimation JmC, We can bound the function fθ in FΘ within
[-Y, 1 ] without losing precision. Then, re-arranging JmC:
sup
fθ ∈FΘ
1n
-∑fθ(χi,y)-
n
=1
1 m	1 n β	1 mγ
m X afθ (xj ,yj)- n X 2 fθ (χi,yi)- m X 2 fθ (Xj,y)
1n	β2
SUp 一 Σ2(fθ (Xi,yi) - fθfθiixiiVi
fθ ∈FΘ ni=1	2 θ
n
+ m X (αfθ I(XjRj) + 2 fθ(χj ,yj)
j=m
Then, since - Y ≤ fθ(∙, ∙) ≤ ɪ, basic calculations give US
-2αγ2+2βα ≤ fθ(χi,yi)-2fθ(χi,yi) ≤ 2β and -αγ ≤ αfθ(χj,yj)+Yfθ(χj,yj) ≤ 2α2β+ Y.
The resulting variances have
β2
Var[fθ(Xi,yi) - 2fθ(xi,Vi')∖ ≤ max
and
var[αfθ(χj, yj) + 2f2(χj,yj)∖ ≤ max {(αγ) , (2ɑ2β+ γ) }.
Taking the mean of m, n independent random variables gives the result:
{(「Y, (2⅛I}
Proposition 5 (Variance of JmC)
VarJmC∖ ≤ nmax{('A')2, (2βI} +
1
—max
m
{((OY )2, (七)
2
A.6 Implementation of Experiments
For visual representation learning, we follow the implementation in https://github.com/
google-research/simclr. For speech representation learning, we follow the imple-
mentation in https://github.com/facebookresearch/CPC_audio. For MI estima-
tion, we follow the implementation in https://github.com/yaohungt/Pointwise_
Dependency_Neural_Estimation/tree/master/MI_Est_and_CrossModal..
A.7 Relative Predictive Coding on Vision
The whole pipeline of pretraining contains the following steps: First, a stochastic data augmentation
will transform one image sample xk to two different but correlated augmented views, x02k-1 and
x2k. Then a base encoder f (∙) implemented using ResNet (He et al., 2016) will extract representa-
tions from augmented views, creating representations h2k-1 and h2k. Later a small neural network
g(∙) called projection head will map h2k-1 and h2k to z2k-1 and z2k in a different latent space. For
each minibatch of N samples, there will be 2N views generated. For each image xk there will be
one positive pair x02k-1 and x02k and 2(N -1) negative samples. The RPC loss between a pair of
positive views, x0i and x0j (augmented from the same image) , can be calculated by the substitution
fθ (xi, Xj) = (zi ∙ Zj )∕τ = Sij(T is a hyperparameter) to the definition of RPC:
2N	2N
CPC = -(Sij - 2(N- 1) X 1[k=i] si,k - 2s2,j - 2 ∙ 2(N - 1) X 1[k=i]s2,k)	⑺
For losses other than RPC, a hidden normalization of Si,j is often required by replacing Zi ∙ zj with
(Zi ∙ Zj )∕∣Zi∣∣Zj |. CPC and WPC adopt this, while other objectives needs itto help stabilize training
variance. RPC does not need this normalization.
20
Published as a conference paper at ICLR 2021
Confidence Interval of JRPC and JCPC			
Objective	CIFAR 10	CIFAR 100	ImageNet
JCPC	(91.09%, 91.13%)-	(77.11%, 77.36%)-	(73.39%, 73.48%)-
JRPC		(91.16%, 91.47%)	(77.41%, 77.98%)	(73.92%, 74.43%)
Table 4: Confidence Intervals of performances of JRPC and JCPC on CIFAR-10/-100 and ImageNet.
A.8 CIFAR- 1 0/- 1 00 and ImageNet Experiments Details
ImageNet Following the settings in (Chen et al., 2020b;c), we train the model on Cloud TPU with
128 cores, with a batch size of4, 096 and global batch normalization 3 (Ioffe & Szegedy, 2015). Here
we refer to the term batch size as the number of images (or utterances in the speech experiments)
we use per GPU, while the term minibatch size refers to the number of negative samples used to
calculate the objective, such as CPC or our proposed RPC. The largest model we train is a 152-layer
ResNet with selective kernels (SK) (Li et al., 2019) and 2× wider channels. We use the LARS
optimizer (You et al., 2017) with momentum 0.9. The learning rate linearly increases for the first
20 epochs, reaching a maximum of 6.4, then decayed with cosine decay schedule. The weight
decay is 10-4. A MLP projection head g(∙) with three layers is used on top of the ResNet encoder.
Unlike Chen et al. (2020c), we do not use a memory buffer, and train the model for only 100 epochs
rather than 800 epochs due to computational constraints. These two options slightly reduce CPC’s
performance benchmark for about 2% with the exact same setting. The unsupervised pre-training is
followed by a supervised fine-tuning. Following SimCLRv2 (Chen et al., 2020b;c), we fine-tune the
3-layer g(∙) for the downstream tasks. We use learning rates 0.16 and 0.064 for standard 50-layer
ResNet and larger 152-layer ResNet respectively, and weight decay and learning rate warmup are
removed. Different from Chen et al. (2020c), we use a batch size of 4, 096, and we do not use
global batch normalization for fine-tuning. For JRPC we disable hidden normalization and use a
temperature τ = 32. For all other objectives, we use hidden normalization and τ = 0.1 following
previous work (Chen et al., 2020c). For relative parameters, we use α = 0.3, β = 0.001, γ = 0.1
and α = 0.3, β = 0.001, γ = 0.005 for ResNet-50 and ResNet-152 respectively.
CIFAR-10/-100 Following the settings in (Chen et al., 2020b), we train the model on a single
GPU, with a batch size of 512 and global batch normalization (Ioffe & Szegedy, 2015). We use
ResNet (He et al., 2016) of depth 18 and depth 50, and does not use Selective Kernel (Li et al., 2019)
or a multiplied width size. We use the LARS optimizer (You et al., 2017) with momentum 0.9. The
learning rate linearly increases for the first 20 epochs, reaching a maximum of 6.4, then decayed with
cosine decay schedule. The weight decay is 10-4. A MLP projection head g(∙) with three layers
is used on top of the ResNet encoder. Unlike Chen et al. (2020c), we do not use a memory buffer.
We train the model for 1000 epochs. The unsupervised pre-training is followed by a supervised
fine-tuning. Following SimCLRv2 (Chen et al., 2020b;c), We fine-tune the 3-layer g(∙) for the
downstream tasks. We use learning rates 0.16 for standard 50-layer ResNet , and weight decay and
learning rate warmup are removed. For JRPC we disable hidden normalization and use a temperature
τ = 128. For all other objectives, we use hidden normalization and τ = 0.5 following previous work
(Chen et al., 2020c). For relative parameters, we use α = 1.0, β = 0.005, and γ = 1.0.
STL-10 We also perform the pre-training and fine-tuning on STL-10 (Coates et al., 2011) using
the model proposed in Chuang et al. (2020). Chuang et al. (2020) proposed to indirectly approximate
the distribution of negative samples so that the objective is debiased. However, their implementation
of contrastive learning is consistent with Chen et al. (2020b). We use a ResNet with depth 50 as an
encoder for pre-training, with Adam optimizer, learning rate 0.001 and weight decay 10-6. The
temperature τ is set to 0.5 for all objectives other than JRPC, which disables hidden normalization
and use τ = 128. The downstream task performance increases from 83.4% of JCPC to 84.1% of
JRPC.
Confidence Interval We also provide the confidence interval of JRPC and JCPC on CIFAR-10,
CIFAR-100 and ImageNet, using ResNet-18, ResNet-18 and ResNet-50 respectively (95% confi-
3 For WPC (Ozair et al., 2019), the global batch normalization during pretraining is disabled since we enforce
1-Lipschitz by gradient penalty (Gulrajani et al., 2017).
21
Published as a conference paper at ICLR 2021
dence level is chosen) in Table 4. Both CPC and RPC use the same experimental settings throughout
this paper. Here we use the relative parameters (α = 1.0, β = 0.005, γ = 1.0) in JRPC which gives
the best performance on CIFAR-10. The confidence intervals of CPC do not overlap with the con-
fidence intervals of RPC, which means the difference of the downstream task performance between
RPC and CPC is statistically significant.
A.9 Relative Predictive Coding on Speech
For speech representation learning, we adopt the general architecture from Oord et al. (2018). Given
an input signal x1:T with T time steps, we first pass it through an encoder φθ parametrized by
θ to produce a sequence of hidden representations {h1:T } where ht = φθ(xt). After that, we
obtain the contextual representation ct at time step t with a sequential model ψρ parametrized by ρ:
ct = ψρ(h1, . . . , ht), where ct contains context information before time step t. For unsupervised
pre-training, we use a multi-layer convolutional network as the encoder φθ, and an LSTM with
hidden dimension 256 as the sequential model ψρ . Here, the contrastiveness is between the positive
pair (ht+k, ct) where k is the number of time steps ahead, and the negative pairs (hi, ct), where hi
is randomly sampled from N , a batch of hidden representation of signals assumed to be unrelated to
ct. The scoring function f based on Equation 2 at step t and look-ahead k will be fk = fk(h, ct) =
exp((h)>Wkct), where Wk is a learnable linear transformation defined separately for each k ∈
{1, ..., K} and K is predetermined as 12 time steps. The loss in Equation 2 will then be formulated
as:
α
网
Y
隔
RPC
`t,k
-(fk (ht+k, ct) -
X fk (hi，Ct)- 2 fk(ht+k, Ct)-
hi∈N
fk2 (hi, Ct))
hi∈N
(8)
We use the following relative parameters: α = 1, β = 0.25, and γ = 1, and we use the temperature
τ = 16 for JRPC. For JCPC we follow the original implementation which sets τ = 1. We fix all
other experimental setups, including architecture, learning rate, and optimizer. As shown in Table
3, JRPC has better downstream task performance, and is closer to the performance from a fully
supervised model.
A.10 Empirical Observations on Variance and Minibatch Size
Variance Experiment Setup We perform the variance comparison of JDV , JNWJ and the pro-
posed JRPC . The empirical experiments are performed using SimCLRv2 (Chen et al., 2020c) on
CIFAR-10 dataset. We use a ResNet of depth 18, with batch size of 512. We train each objective
with 30K training steps and record their value. In Figure 1, we use a temperature τ = 128 for all
objectives. Unlike other experiments, where hidden normalization is applied to other objectives, we
remove hidden normarlization for all objectives due to the reality that objectives after normalization
does not reflect their original values. From Figure 1, JRPC enjoys lower variance and more stable
training compared to JDV and JNWJ .
Minibatch Size Experimental Setup We perform experiments on the effect of batch size on
downstream performances for different objective. The experiments are performed using SimCLRv2
(Chen et al., 2020c) on CIFAR-10 dataset, as well as the model from RiViere et al. (2020) on
LibriSpeech-100h dataset (Panayotov et al., 2015). For vision task, we use the default tempera-
ture τ = 0.5 from Chen et al. (2020c) and hidden normalization mentioned in Section 3 for JCPC.
For JRPC in Vision and speech tasks we use a temperature ofτ = 128 and τ = 16 respectiVely, both
without hidden normalization.
A.11 Mutual Information Estimation
Our method is compared with baseline methods CPC (Oord et al., 2018), NWJ (Nguyen et al., 2010),
JSD (Nowozin et al., 2016), and SMILE (Song & Ermon, 2019). All the approaches consider the
same design of f(x, y), which is a 3-layer neural network taking concatenated (x, y) as the input.
We also fix the learning rate, the optimizer, and the minibatch size across all the estimators for a fair
comparison.
We present results of mutual information by RelatiVe PredictiVe Coding using different sets of rela-
tiVe parameters in Figure 4. In the first row, we set β = 10-3, γ = 1, and experiment with different
22
Published as a conference paper at ICLR 2021
Figure 4: Mutual information estimation by RPC performed on 20-d correlated Gaussian distribu-
tion, with different sets of relative parameters.
Figure 5: Mutual information estimation by DoE performed on 20-d correlated Gaussian distribu-
tion. The figure on the left shows parametrization under Gaussian (correctly specified), and the
figure on the right shows parametrization under Logistic (mis-specified).
α values. In the second row, we set α = 1, γ = 1 and in the last row we set α = 1, β = 10-3.
From the figure, a small β around 10-3 and a large γ around 1.0 is crucial for an estimation that is
relatively low bias and low variance. This conclusion is consistent with Section 3 in the main text.
We also performed comparison between JRPC and Difference of Entropies (DoE) (McAllester &
Stratos, 2020). We performed two sets of experiments: in the first set of experiments we compare
JRPC and DoE when MI is large (> 100 nats), while in the second set of experiments we compare
JRPC and DoE using the setup in this section (MI < 12 nats and MI increases by 2 per 4k training
steps). On the one hand, when MI is large (> 100 nats), we acknowledge that DoE is performing
well on MI estimation, compared to JRPC which only estimates the MI around 20. This analysis
is based on the code from https://github.com/karlstratos/doe. On the other hand,
when the true MI is small, the DoE method is more unstable than JRPC, as shown in Figure 5. Figure
5 illustrates the results of the DoE method when the distribution is isotropic Gaussian (correctly
specified) or Logistic (mis-specified). Figure 3 only shows the results using Gaussian.
23