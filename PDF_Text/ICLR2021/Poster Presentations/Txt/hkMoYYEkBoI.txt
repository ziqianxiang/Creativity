Published as a conference paper at ICLR 2021
Computational Separation Between Convolu-
tional and Fully-Connected Networks
Eran Malach
School of Computer Science
Hebrew University
Jerusalem, Israel
eran.malach@mail.huji.ac.il
Shai Shalev-Shwartz
School of Computer Science
Hebrew University
Jerusalem, Israel
shais@cs.huji.ac.il
Abstract
Convolutional neural networks (CNN) exhibit unmatched performance in a mul-
titude of computer vision tasks. However, the advantage of using convolutional
networks over fully-connected networks is not understood from a theoretical per-
spective. In this work, we show how convolutional networks can leverage locality
in the data, and thus achieve a computational advantage over fully-connected net-
works. Specifically, we show a class of problems that can be efficiently solved
using convolutional networks trained with gradient-descent, but at the same time
is hard to learn using a polynomial-size fully-connected network.
1 Introduction
Convolutional neural networks (LeCun et al., 1998; Krizhevsky et al., 2012) achieve state-of-the-art
performance on every possible task in computer vision. However, while the empirical success of
convolutional networks is indisputable, the advantage of using them is not well understood from a
theoretical perspective. Specifically, we consider the following fundamental question:
Why do convolutional networks (CNNs) perform better than fully-connected networks (FCNs)?
Clearly, when considering expressive power, FCNs have a big advantage. Since convolution is a
linear operation, any CNN can be expressed using a FCN, whereas FCNs can express a strictly
larger family of functions. So, any advantage of CNNs due to expressivity can be leveraged by
FCNs as well. Therefore, expressive power does not explain the superiority of CNNs over FCNs.
There are several possible explanations to the
superiority of CNNs over FCNs: parameter ef-
ficiency (and hence lower sample complexity),
weight sharing, and locality prior. The main re-
sult of this paper is arguing that locality is a
key factor by proving a computational separa-
tion between CNNs and FCNs based on local-
ity. But, before that, let’s discuss the other pos-
sible explanations.
First, we observe that CNNs seem to be much
more efficient in utilizing their parameters. A
FCN needs to use a greater number of parame-
ters compared to an equivalent CNN: each neu-
ron of a CNN is limited to a small receptive
field, and moreover, many of the parameters of
			
	♦ ♦ ♦		. ♦ CNN-2 CNTNT Λ
			C CNN-4 ♦ CNN-6 ▲ FCN-2
	. ,▲▲▲ ▲	▲ ▲	▲ FCN-6 ▲ FCN-4
0	0.5	1	1.5
# params	∙107
90
80
70
60
Figure 1: Comparison between CNN and FCN of
various depths (2/4/6) and widths, trained for 125
epochs with RMSprop optimizer.
the CNN are shared. From classical results in
learning theory, using a large number of param-
eters may result in inferior generalization. So, can the advantage of CNNs be explained simply by
counting parameters?
1
Published as a conference paper at ICLR 2021
To answer this question, we observe the performance of CNN and FCN based architecture of various
widths and depths trained on the CIFAR-10 dataset. For each architecture, we observe the final test
accuracy versus the number of trainable parameters. The results are shown in Figure 1. As can be
seen, CNNs have a clear advantage over FCNs, regardless of the number of parameters used. As
is often observed, a large number of parameters does not hurt the performance of neural networks,
and so parameter efficiency cannot explain the advantage of CNNs. This is in line with various
theoretical works on optimization of neural networks, which show that over-parameterization is
beneficial for convergence of gradient-descent (e.g., Du et al. (2018); Soltanolkotabi et al. (2018);
Li & Liang (2018)).
The superiority of CNNs can be also attributed to the extensive weight sharing between the different
convolutional filters. Indeed, it has been previously shown that weight sharing is important for the
optimization of neural networks (Shalev-Shwartz et al., 2017b). Moreover, the translation-invariant
nature of CNNs, which relies on weight sharing, is often observed to be beneficial in various signal
processing tasks (Kauderer-Abrams, 2017; Kayhan & Gemert, 2020). So, how much does the weight
sharing contribute to the superiority of CNNs over FCNs?
To understand the effect of weight sharing on the behavior of CNNs, it is useful to study locally-
connected network (LCN) architectures, which are similar to CNNs, but have no weight sharing
between the kernels of the network. While CNNs are far more popular in practice (also due to the
fact that they are much more efficient in terms of model size), LCNs have also been used in different
contexts (e.g., Bruna et al. (2013); Chen et al. (2015); Liu et al. (2020)). It has been recently observed
that in some cases, the performance of LCNs is on par with CNNs (Neyshabur, 2020). So, even if
weight sharing explains some of the advantage of CNNs, it clearly doesn’t tell the whole story.
Finally, a key property of CNN architectures is their strong utilization of locality in the data. Each
neuron in a CNN is limited to a local receptive field of the input, hence encoding a strong locality
bias. In this work we demonstrate how CNNs can leverage the local structure of the input, giving
them a clear advantage in terms of computational complexity. Our results hint that locality is the
principal property that explains the advantage of using CNNs.
Our main result is a computational separation result between CNNs and FCNs. To show this result,
we introduce a family of functions that have a very strong local structure, which we call k-patterns.
A k-pattern is a function that is determined by k consecutive bits of the input. We show that for
inputs of n bits, when the target function is a (log n)-pattern, training a CNN of polynomial size
with gradient-descent achieves small error in polynomial time. However, gradient-descent will fail
to learn (log n)-patterns, when training a FCN of polynomial-size.
1.1 Related Work
It has been empirically observed that CNN architectures perform much better than FCNs on com-
puter vision tasks, such as digit recognition and image classification (e.g., Urban et al. (2017); Driss
et al. (2017)). While some works have applied various techniques to improve the performance of
FCNs (Lin et al. (2015); Fernando et al. (2016); Neyshabur (2020)), there is still a gap between
performance of CNNs and FCNs, where the former give very good performance “out-of-the-box”.
The focus of this work is to understand, from a theoretical perspective, why CNNs give superior
performance when trained on input with strong local structure.
Various theoretical works show the advantage of architectures that leverage local and hierarchical
structure. The work of Poggio et al. (2015) shows the advantage of using deep hierarchical models
over wide and shallow functions. These results are extended in Poggio et al. (2017), showing an
exponential gap between deep and shallow networks, when approximating locally compositional
functions. The works of Mossel (2016); Malach & Shalev-Shwartz (2018) study learnability of
deep hierarchical models. The work of Cohen et al. (2017) analyzes the expressive efficiency of
convolutional networks via hierarchical tensor decomposition. While all these works show that
indeed CNNs powerful due to their hierarchical nature and the efficiency of utilizing local structure,
they do not explain why these models are superior to fully-connected models.
There are a few works that provide a theoretical analysis of CNN optimization. The works of
Brutzkus & Globerson (2017); Du et al. (2018) show that gradient-descent can learn a shallow
CNN with a single filter, under various distributional assumptions. The work of Zhang et al. (2017)
2
Published as a conference paper at ICLR 2021
I +1 I +1 I -1 I -1 I +11 -1 I -1 I +1 I ... I -1 I
Figure 2: Example of a k-pattern with k =5.
shows learnability of a convex relaxation of convolutional networks. While these works focus on
computational properties of learning CNNs, as we do in this work, they do not compare CNNs to
FCNs, but focus only on the behavior of CNNs. The works of Cohen & Shashua (2016); Novak
et al. (2018) study the implicit bias of simplified CNN models. However, these result are focused on
generalization properties of CNNs, and not on computational efficiency of the optimization.
2 Definitions and Notations
Let X = {±1}n be our instance space, and let Y = {±1} be the label space. Throughout the paper,
we focus on learning a binary classification problem using the hinge-loss: '(y, y) = max{1-yy, 0}.
Given some distribution D over X, some target function f : X→Yand some hypothesis h : X→
Y, we define the loss of h with respect to f on the distribution D by:
Lf,D (h)= EJ'(h(x),f(x))]
The goal of a supervised learning algorithm is, given access to examples sampled from D and
labeled by f, to find a hypothesis h that minimizes Lf,D (h). We focus on the gradient-descent
(GD) algorithm: given some parametric hypothesis class H = {hw : w ∈ Rq}, gradient-descent
starts with some (randomly initialized) hypothesis hw(0) and, for some learning rate η>0, updates:
W⑶=W(I)- ηVwLf,D(hw(t-ι))
We compare the behavior of gradient-descent, when learning two possible neural network architec-
tures: a convolutional network (CNN) and a fully-connected network (FCN).
Definition 1. A convolutional network hu,W,b is defined as follows:
n-k
hu,W,b(x) = X Du(j),σ(Wxj...j+k-1 +
j=1
for activation function σ, with kernel W ∈ Rq×k, bias b ∈ Rq and readout layer u(1),...,u(n) ∈
Rq. Note that this is a standard depth-2 CNN with kernel k, stride 1 and q filters.
Definition 2. A fully-connected network hu,w,b is defined as follows:
hu,w,b
q
(X)=X uiσ (Dw(i)
i=1
XE + bi)
for activation function σ, first layer W(1),...,W(q) ∈ Rn, bias b ∈ Rq and second layer u ∈ Rq.
We demonstrate the advantage of CNNs over FCNs by observing a problem that can be learned
using CNNs, but is hard to learn using FCNs. We call this problem the k-pattern problem:
Definition 3. Afunction f : X → Y is a k-pattern, iffor some g : {±1}k → Y and index j*:
f (X) = g(Xj*...j*+k-I)
Namely, a k-pattern is a function that depends only on a small pattern of consecutive bits of the
input. The k-pattern problem is the problem of learning k-patterns: for some k-pattern f and some
distribution D over X, given access to D labeled by f, find a hypothesis h with LfD(h) ≤ e. We
note that a similar problem has been studied in Golovnev et al. (2017), providing results on PAC
learnability of a related target class.
3
Published as a conference paper at ICLR 2021
3 CNNS EFFICIENTLY LEARN (log n)-PATTERNS
The main result in this section shows that gradient-descent can learn k-patterns when training con-
volutional networks forpoly(2k,n) iterations, and when the network has poly(2k, n) neurons:
Theorem 4. Assume we uniformly initialize W(0) 〜{±1∕k}q×k, b = 1/k 一 1 and u(0,j) = 0 for
every j. Assume the activation σ satisfies ∣σ∣ ≤ c, ∣σ0∣ ≤ 1, for some constant c. Fix some δ > 0,
some k-pattern f and some distribution D over X. Then, if q > 2k+3 log(2k∕δ), with probability
at least 1 一 δ over the initialization, when training a convolutional network hu,W,b using gradient
descent with η = n^w we have:
qT
1 Sr ,7	、	2cn2k22k	2(2kk)2	c2n1-5√q
T XLf,D(hu(t),w(t),b) ≤ -^r- + " +
Before we prove the theorem, observe that the above immediately implies that when k = O(log n),
gradient-descent can efficiently learn to solve the k-pattern problem, when training a CNN:
Corollary 5. Let k = O (log n). Then, running GD on a CNN with q = O(e-2n3 log2 n) neurons
for T = O(e-2n3 log n) iterations, using a sample S 〜 D of size O(e-2nkq log(nkq∕δ)), learns
the k-pattern problem up to accuracy e w.p. ≥ 1 一 δ.
/、''、
Proof. Sample S 〜 D, and let D be the uniform distribution over S. Then, from Theorem 4 and the
choice ofq andT there exists t ∈ [T] with L Db(hu(t),W(t),b) ≤ e/2, i.e. GD finds a hypothesis with
train loss at most e/2. Now, using the fact the VC dimension of depth-2 ReLU networks with W
weights is O(W log W) (SeeBartlettetaL (2019)), we can bound the generalization gap by e/2. □
To prove Theorem 4, we show that, for a large enough CNN, the k-pattern problem becomes linearly
separable, after applying the first layer of the randomly initialized CNN:
Lemma 6. Assume we uniformly initialize W 〜 {±1∕k}q×k and b = 1/k — 1. Fix some δ > 0.
Then if q > 2k+3 log(2k∕δ), w.p. ≥ 1 一 δ over the choice of W, for every k-pattern f there exist
u*(1),..., u*(n-k) ∈ Rq with ∣∣u*(j*)∣∣ ≤ 2√k and ∣∣u*(j)∣∣ = 0for j = j*, s.t. hu*,w,b = f(x).
Proof. Fix some Z ∈ {±1}k, then for every w(i) 〜{±1∕k}k, we have: P [sign(w(i)) = z] = 2-k.
Denote by Jz ⊆ [q] the subset of indexes satisfying sign w(i) = z, for every i ∈ Jz, and note that
EW |Jz| = q2-k. From Chernoff bound:
PIjJZI ≤ q2-k/2] ≤ e-q2-k/8 ≤ δ2-k
by choosing q > 2k+3 log(2k∕δ). So, using the union bound, w.p. at least 1 -δ, for every Z ∈ {±1}k
We have ∣ JJ ≥ q2-k-1. By the choice of b we have σ(<w(i), z〉+ b) = (1∕k)1{sign w(i) = z}.
Now, fix some k-pattern f, where f (x) = g(xj*,…,j*+k-ι). For every i ∈ Jz we choose u*(j ) =
∣k∣g(z) and u*(j) = 0 for every j = j*. Therefore, we get:
n-k
hu*,w,b(x) = X (u*(j)
j=1
σ(Wχj...j+k-ι+b)E = x	u*(j*)σ (Dw(i)
z∈{±1}k
i∈Jz
xj* ...j * +k-1
E+bi)
X 1{z = xj*...j*+k-1}g(z) = g(xj*...j*+k-1) = f(x)
z∈{±1}k
Notethatbydefinitionof u*(j*) we have ∣∣u*(j*)∣∣2 = Pz∈{±1}k Pi∈Jz 舟 ≤ 4(2⅛2.	□
Comment 7. Admittedly, the initialization assumed above is non-standard, but is favorable for the
analysis. A similar result can be shown for more natural initialization (e.g., normal distribution),
using known results from random features analysis (for example, Bresler & Nagaraj (2020)).
4
Published as a conference paper at ICLR 2021
From Lemma 6 and known results on learning linear classifiers with gradient-descent, solving the
k-pattern problem can be achieved by optimizing the second layer of a randomly initialized CNN.
However, since in gradient-descent we optimize both layers of the network, we need a more re-
fined analysis to show that full gradient-descent learns to solve the problem. We follow the scheme
introduced in Daniely (2017), adapting it our setting.
We start by showing that the first layer of the network does not deviate from the initialization during
the training:
Lemma 8. We have Iu(T叫∣ ≤ ηT√q forall j ∈ [n 一 k], and ∣∣ W(0) 一 W(T[ ≤ cη2T2n√qk
We can now bound the difference in the loss when the weights of the first layer change during the
training process:
Lemma 9. For every u* we have:
n-k
ILf,D (hu*,W(T Mb)- Lf,D (hu*,W (0),b)l ≤ cη2T 2nk√q X I∣u*(j)II
j=1
The proofs of Lemma 8 and Lemma 9 are shown in the appendix.
Finally, we use the following result on the convergence of online gradient-descent to show that
gradient-descent converges to a good solution. The proof of the Theorem is given in Shalev-Shwartz
et al. (2011), with an adaptation to a similar setting in Daniely & Malach (2020).
Theorem 10. (Online Gradient Descent) Fix some η, and let f1,...,fT be some sequence of convex
functions. Fix some θι, and update θt+ι = θt — ηVft(θt). Thenfor every θ* the following holds:
1T	1T	1	1T	1T
T Xft(θt) ≤ T X ft(θ*) + 2ηT kθ*k2 + kθιk T X kVft(θt)k + ηT X kVft(θt)k2
Proof of Theorem 4. From Lemma 6, with probability at least 1 一 δ over the initialization, there
exist u* ⑴,..., u*(n-k) ∈ Rq With ∣∣u*(1)∣∣ ≤ 2√k and ∣∣u*(j)∣∣ = 0 for j > 1 SUCh that
hu*,W(0),b(x) = f(x), and so Lf,D (hu*,W(0),b)=0. Using Theorem 10, since Lf,D (hu,W,b) is
Convex With respeCt to u, We have:
1T
T XLf,D(hu(t) ,W(t),b)
t=1
1T
≤ T X Lf,D (hu*,W (t),b) +
t=1
1T
≤ T X Lf,D (hu*,W (t),b) +
t=1
1
η T
X	∂U LfD (Ju
2(2kk)2	2
———+ c ηnq = (*)
qηT
(t),W(t),b)
t=1
T
2
Using Lemma 9 We have:
(*) ≤ T X LfD (hu* ,w(0),b)+cη2T2 nk√q X ∣∣u*(j)∣∣ + 2∣kM
t=1	j=1	qη
≤ 2cη2T2nk22k + '2 >)——+ c2ηnq
qηT
+ c2 ηnq
Now, choosing η = -ɪn We get the required.
qT
□
3.1	Analysis of Locally-Connected Networks
The above result shows that polynomial-size CNNs can learn (log n)-patterns in polynomial time.
As discussed in the introduction, the success of CNNs can be attributed to either the weight sharing
5
Published as a conference paper at ICLR 2021
or the locality-bias of the architecture. While weight sharing may contribute to the success of CNNs
in some cases, we note that it gives no benefit when learning k-patterns. Indeed, we can show a
similar positive result for locally-connected networks (LCN), which have no weight sharing.
Observe the following definition of a LCN with one hidden-layer:
Definition 11. A locally-connected network hu,w,b is defined as follows:
n-k
hu,W,b(x)=XDu(j),σ(W(j)xj...j+k-1+b(j))E
j=1
for some activation function σ, with W(1),...,W(q) ∈ Rq×k, bias b(1),...,b(q) ∈ Rq and readout
layer u(1),...,u(n) ∈ Rq.
Note that the only difference from Definition 1 is the fact that the weights of the first layer are not
shared. It is easy to verify that Theorem 4 can be modified in order to show a similar positive result
for LCN architectures. Specifically, we note that in Lemma 6, which is the core of the Theorem, we
do not use the fact that the weights in the first layer are shared. So, LCNs are “as good as” CNNs for
solving the k-pattern problem. This of course does not resolve the question of comparing between
LCN and CNN architectures, which we leave for future work.
4	LEARNING (logn)-PATTERNS WITH FCN
In the previous section we showed that patterns of size log n are efficiently learnable, when using
CNNs trained with gradient-descent. In this section we show that, in contrast, gradient-descent fails
to learn (log n)-patterns using fully-connected networks, unless the size of the network is super-
polynomial (namely, unless the network is of size nQ(log n)). For this, We will show an instance of
the k-pattern problem that is hard for fully connected networks.
We take D to be the uniform distribution over X, and let f(x)=Q ∈ xi, where I is some set of
k consecutive bits. Specifically, we take I = {1,. ..,k}, although the same proof holds for any
choice of I. In this case, we show that the initial gradient of the network is very small, when a
fully-connected network is initialized from a permutation invariant distribution.
Theorem 12. Assume ∣σ∣ ≤ c, ∣σ0∣ ≤ 1. Let W be some permutation invariant distribution over
Rn, and assume we initialize w(1),..., w(q)〜W and initialize U such that |ui| ≤ 1 and for all X
we have hu,w (x) ∈ [-1, 1]. Then, the following holds:
•	Ew 〜Wll ∂W Lf,D (hu,w,b)∣∣2 ≤ qn ∙ min {(71)-1, (k-1)-1}
•	Ew~W Il ∂uLf,D(hu,w,b升2 ≤ c2q(k) 1
From the above result, if k = Ω(logn) then the average norm of initial gradient is qn-Q(logn).
Therefore, unless q = nQ(log n), we get that with overwhelming probability over the randomness
of the initialization, the gradient is extremely small. In fact, if we run GD on a finite-precision
machine, the true population gradient is effectively zero. A formal argument relating such bound
on the gradient norm to the failure of gradient-based algorithms has been shown in various previous
works (e.g. Shamir (2018); Abbe & Sandon (2018); Malach & Shalev-Shwartz (2020)).
The key for proving Theorem 12 is the following observation: since the first layer of the FCN is
initialized from a symmetric distribution, we observe that if learning some function that relies on
k bits of the input is hard, then learning any function that relies on k bits is hard. Using Fourier
analysis (e.g., Blum et al. (1994); Kearns (1998); Shalev-Shwartz et al. (2017a)), we can show that
learning k-parities (functions of the form X 7→ Q ∈ xi) using gradient-descent is hard. Since an
arbitrary k-parity is hard, then any k-parity, and specifically a parity of k consecutive bits, is also
hard. That is, since the first layer is initialized symmetrically, training a FCN on the original input
is equivalent to training a FCN on an input where all the input bits are randomly permuted. So, for a
FCN, learning a function that depends on consecutive bits is just as hard as learning a function that
depends on arbitrary bits (a task that is known to be hard).
6
Published as a conference paper at ICLR 2021
Proof of Theorem 12. Denote χI0 = Q ∈ 0 xi, so f(x) = χI with I = {1,...,k}. We begin by
(i)
calculating the gradient w.r.p. to w :
∂
7~(i)LfD (hu,w,b) = E
∂w	D
∂
~~(i) '(hu,w,b(X),f (X))
∂w
-E Ixjuiσ0 ((w(i), XE + bi) Xi(x)]
Fix some permutation π :[n] → [n]. For some vector X ∈ Rn we denote π(X) =
(xπ(1),..., xπ(n)), for some subset I ⊆ [n] we denote π(I) = ∪j∈I{π(j)}. Notice that we have
for all x,z ∈ Rn: χι(∏(x)) = χ∏(i)and h∏(x), Zi =(x,∏-1(z)). Denote ∏(hu,w,b)(x)=
P『i uiσ((π(w"), X + bi). Denote π(D) the distribution of π(x) where X 〜D. Notice that
since D is the uniform distribution, we have π(D) = D. From all the above, for every permutation
π with π(j)=j we have:
∂
苞LXn(I),D(hu,w,b) = XED hxjuiσ0 (Dw(i), xE + bi) Xn(I)(X)]
----
∂w
j
X 〜e(d)hxjuiσ0 (Dw(i),π-1(X)E+bi) XI (X)I
XED hxj uiσ0 (Dn(W⑴),xE + bi) XI (x)] = - ^diy LXI ,D (π(hu,w,b))
Fix some I ⊆ [n] with |I| = k andj ∈ [n]. Now, let Sj be a set of permutations satisfying:
1.	For all π1 ,π2 ∈ Sj with π1 6= π2 we have π1 (I) 6= π2 (I).
2.	For all π ∈ Sj we have π(j) = j.
Note that if j / I then the maximal size of such Sj is (n-1), and if j ∈ I then the maximal
size is (n-1). Denote gj-(x) = xjuiσ0((w(i), x) + bi). We denote the inner-product hψ, 0)。=
Ex〜D [ψ(x)φ(x)] and the induced norm ∣∣ψ∣∣D = P(ψ, Ψ)d. Since {χIo}Io⊆[n] is an orthonormal
basis w.r.p. to h∙, ∙)d from Parseval,s equality We have:
X ( ^~(i) LχI,d(n(hu,w,b)))	= X ( ~~(iyLXn(I),D(hu,w,b) )
π∈Sj	∂wj	π∈S ∂wj
= X(gj,Xπ(I))2D ≤ X hgj ,XI0iD = kgj kD ≤ 1
π∈S	I0⊆[n]
So, from the above we get that, taking Sj of maximal size:
E
π〜Sj
2
∂
n (i) LXI,d(n(hu,w,b)) I
∂wj
≤ |Sj|-1 ≤ min
Now, for some permutation invariant distribution of weights W we have:
E ( — —(i)LXI,D(hu,w,b)! = E E (~~(iy LXI,D(π(hu,w,b))! ≤ ISj | 1
W〜W ∖∂w(i)	W〜W ∏〜Sj ∂Wy
Summing over all neurons we get:
E
W〜W
2
2
∂
∂WLXI ,d (hu,w,b)
≤ qn ∙ min
We can use a similar argument to bound the gradient of u. We leave the details to the appendix. □
7
Published as a conference paper at ICLR 2021
n=19


Figure 3:	Top: Performance of different architectures on a size-n MNIST sequences, where the label
is determined by the parity of the central 3 digits. Bottom: MNIST sequences of varying length.
5	Neural Architecture Search
So far, we showed that while the (log n)-pattern problem can be solved efficiently using a CNN,
this problem is hard for a FCN to solve. Since the CNN architecture is designed for processing
consecutive patterns of the inputs, it can easily find the pattern that determines the label. The FCN,
however, disregards the order of the input bits, and so it cannot enjoy from the fact that the bits
which determine the label are consecutive. In other words, the FCN architecture needs to learn the
order of the bits, while the CNN already encodes this order in the architecture.
So, a FCN fails to recover the k-pattern since it does not assume anything about the order of the
input bits. But, is it be possible to recover the order of the bits prior to training the network? Can
we apply some algorithm that searches for an optimal architecture to solve the k-pattern problem?
Such motivation stands behind the thriving research field of Neural Architecture Search algorithms
(see Elsken et al. (2018) for a survey).
Unfortunately, we claim that if the order of the bits is not known to the learner, no architecture
search algorithm can help in solving the k-pattern problem. To see this, it is enough to observe that
when the order of the bits is unknown, the k-pattern problem is equivalent to the k-Junta problem:
learning a function that depends on an arbitrary (not necessarily consecutive) set of k bits from the
input. Learning k-Juntas is a well-studied problem in the literature of learning theory (e.g., Mossel
et al. (2003)). The best algorithm for solving the (log n)-Junta problem runs in time nO(logn), and
no poly-time algorithm is known for solving this problem. Moreover, if we consider statistical-query
algorithms (a wide family of algorithms, that only have access to estimations of query function on
the distribution, e.g. Blum et al. (2003)), then existing lower bounds show that the (log n)-Junta
problem cannot be solved in polynomial time (Blum et al., 1994).
6	Experiments
In the previous sections we showed a simplistic learning problem that can be solved using CNNs and
LCNs, but is hard to solve using FCNs. In this problem, the label is determined by a few consecutive
bits of the input. In this section we show some experiments that validate our theoretical results. In
these experiments, the input to the network is a sequence of n MNIST digits, where each digit is
scaled and cropped to a size of 24 × 8. We then train three different network architectures: FCN,
CNN and LCN. The CNN and LCN architectures have kernels of size 24 × 24, so that 3 MNIST
digits fit in a single kernel. In all the architectures we use a single hidden-layer with 1024 neurons,
and ReLU activation. The networks are trained with AdaDelta optimizer for 30 epochs 1.
1 In each epoch we randomly shuffle the sequence of the digits.
8
Published as a conference paper at ICLR 2021
5
=
n
19
=1
n
ycarucca
76
..
00
5
.
0
ycarucca
30
20ch
o
10ep
30
20ch
o
10ep
0
0
Figure 4:	n-sequence MNIST with non-consecutive parity.
In the first experiment, the label of the example is set to be the parity of the sum of the 3 consecutive
digits located in the middle of the sequence. So, as in our theoretical analysis, the label is determined
by a small area of consecutive bits of the input. Figure 3 shows the results of this experiment. As can
be clearly seen, the CNN and LCN architectures achieve good performance regardless of the choice
of n, where the performance of the FCN architectures critically degrades for larger n, achieving
only chance-level performance when n = 19. We also observe that LCN has a clear advantage over
CNN in this task. As noted, our primary focus is on demonstrating the superiority of locality-based
architectures, such as CNN and LCN, and we leave the comparison between the two to future work.
Our second experiment is very similar to the first, but instead of taking the label to be the parity of
3 consecutive digits, we calculate the label based on 3 digits that are far apart. Namely, we take the
parity of the first, middle and last digits of the sequence. The results of this experiment are shown
in Figure 4. As can be seen, for small n, FCN performs much better than CNN and LCN. This
demonstrates that when we break the local structure, the advantage of CNN and LCN disappears,
and using FCN becomes a better choice. However, for large n, all architectures perform poorly.
Acknowledgements: This research is supported by the European Research Council (TheoryDL
project). We thank Tomaso Poggio for raising the main question tackled in this paper and for valu-
able discussion and comments
9
Published as a conference paper at ICLR 2021
References
Emmanuel Abbe and Colin Sandon. Provable limitations of deep learning. arXiv preprint
arXiv:1812.06369, 2018.
Peter L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension
and pseudodimension bounds for piecewise linear neural networks. J. Mach. Learn. Res., 20:
63-1,2019.
Avrim Blum, Merrick Furst, Jeffrey Jackson, Michael Kearns, Yishay Mansour, and Steven Rudich.
Weakly learning dnf and characterizing statistical query learning using fourier analysis. In Pro-
ceedings of the twenty-sixth annual ACM symposium on Theory of computing, pp. 253-262, 1994.
Avrim Blum, Adam Kalai, and Hal Wasserman. Noise-tolerant learning, the parity problem, and the
statistical query model. Journal of the ACM (JACM), 50(4):506-519, 2003.
Guy Bresler and Dheeraj Nagaraj. A corrective view of neural networks: Representation, memo-
rization and learning. arXiv preprint arXiv:2002.00274, 2020.
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally
connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian
inputs. arXiv preprint arXiv:1702.07966, 2017.
Yu-hsin Chen, Ignacio Lopez-Moreno, Tara N Sainath, Mirko Visontai, Raziel Alvarez, and Carolina
Parada. Locally-connected and convolutional neural networks for small footprint speaker recog-
nition. In Sixteenth Annual Conference of the International Speech Communication Association,
2015.
Nadav Cohen and Amnon Shashua. Inductive bias of deep convolutional networks through pooling
geometry. arXiv preprint arXiv:1605.06743, 2016.
Nadav Cohen, Or Sharir, Yoav Levine, Ronen Tamari, David Yakira, and Amnon Shashua. Analysis
and design of convolutional networks via hierarchical tensor decompositions. arXiv preprint
arXiv:1705.02302, 2017.
Amit Daniely. Sgd learns the conjugate kernel class of the network. In Advances in Neural Infor-
mation Processing Systems, pp. 2422-2430, 2017.
Amit Daniely and Eran Malach. Learning parities with neural networks. arXiv preprint
arXiv:2002.07400, 2020.
S Ben Driss, Mahmoud Soua, Rostom Kachouri, and Mohamed Akil. A comparison study between
mlp and convolutional neural network models for character recognition. In Real-Time Image
and Video Processing 2017, volume 10223, pp. 1022306. International Society for Optics and
Photonics, 2017.
Simon Du, Jason Lee, Yuandong Tian, Aarti Singh, and Barnabas Poczos. Gradient descent learns
one-hidden-layer cnn: Don’t be afraid of spurious local minima. In International Conference on
Machine Learning, pp. 1339-1348, 2018.
Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. arXiv
preprint arXiv:1808.05377, 2018.
Chrisantha Fernando, Dylan Banarse, Malcolm Reynolds, Frederic Besse, David Pfau, Max Jader-
berg, Marc Lanctot, and Daan Wierstra. Convolution by evolution: Differentiable pattern produc-
ing networks. In Proceedings of the Genetic and Evolutionary Computation Conference 2016,
pp. 109-116, 2016.
Alexander Golovnev, Mika Goos, Daniel Reichman, and Igor Shinkar. String matching: Communi-
cation, circuits, and learning. arXiv preprint arXiv:1709.02034, 2017.
Eric Kauderer-Abrams. Quantifying translation-invariance in convolutional neural networks. arXiv
preprint arXiv:1801.01450, 2017.
10
Published as a conference paper at ICLR 2021
Osman Semih Kayhan and Jan C van Gemert. On translation invariance in cnns: Convolutional
layers can exploit absolute spatial location. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 14274-14285, 2020.
Michael Kearns. Efficient noise-tolerant learning from statistical queries. Journal of the ACM
(JACM), 45(6):983-1006, 1998.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems, pp. 8157-
8166, 2018.
Zhouhan Lin, Roland Memisevic, and Kishore Konda. How far can we go without convolution:
Improving fully-connected networks. arXiv preprint arXiv:1511.02580, 2015.
Wen Liu, Hong Chen, Zhongliang Deng, Xinyu Zheng, Xiao Fu, and Qianqian Cheng. Lc-dnn:
Local connection based deep neural network for indoor localization with csi. IEEE Access, 8:
108720-108730, 2020.
Eran Malach and Shai Shalev-Shwartz. A provably correct algorithm for deep learning that actually
works. arXiv preprint arXiv:1803.09522, 2018.
Eran Malach and Shai Shalev-Shwartz. When hardness of approximation meets hardness of learning.
arXiv preprint arXiv:2008.08059, 2020.
Elchanan Mossel. Deep learning and hierarchal generative models. arXiv preprint
arXiv:1612.09057, 2016.
Elchanan Mossel, Ryan O’Donnell, and Rocco P Servedio. Learning juntas. In Proceedings of the
thirty-fifth annual ACM symposium on Theory of computing, pp. 206-212, 2003.
Behnam Neyshabur. Towards learning convolutions from scratch. arXiv preprint arXiv:2007.13657,
2020.
Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A Abolafia,
Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with many
channels are gaussian processes. arXiv preprint arXiv:1810.05148, 2018.
Tomaso Poggio, Fabio Anselmi, and Lorenzo Rosasco. I-theory on depth vs width: hierarchical
function composition. Technical report, Center for Brains, Minds and Machines (CBMM), 2015.
Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. Why
and when can deep-but not shallow-networks avoid the curse of dimensionality: a review. Inter-
national Journal of Automation and Computing, 14(5):503-519, 2017.
Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah. Failures of gradient-based deep learning.
arXiv preprint arXiv:1703.07950, 2017a.
Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah. Weight sharing is crucial to succesful
optimization. arXiv preprint arXiv:1706.00687, 2017b.
Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and trends
in Machine Learning, 4(2):107-194, 2011.
Ohad Shamir. Distribution-specific hardness of learning neural networks. The Journal of Machine
Learning Research, 19(1):1135-1163, 2018.
11
Published as a conference paper at ICLR 2021
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization
landscape of over-parameterized shallow neural networks. IEEE Transactions on Information
Theory, 65(2):742-769, 2018.
Gregor Urban, Krzysztof J Geras, Samira Ebrahimi Kahou, Ozlem Aslan, Shengjie Wang, Rich
Caruana, Abdelrahman Mohamed, Matthai Philipose, and Matt Richardson. Do deep convolu-
tional nets really need to be deep and convolutional? International Conference on Learning
Representations, 2017.
Yuchen Zhang, Percy Liang, and Martin J Wainwright. Convexified convolutional neural networks.
In International Conference on Machine Learning, pp. 4044-4053. PMLR, 2017.
12