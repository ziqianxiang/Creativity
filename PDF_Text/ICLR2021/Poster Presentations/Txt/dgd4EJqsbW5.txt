Published as a conference paper at ICLR 2021
Control-Aware Representations for Model-
based Reinforcement Learning
Brandon Cui
Facebook AI Research
bcui@fb.com
Yinlam Chow
Google Research
yinlamchow@google.com
Mohammad Ghavamzadeh
Google Research
ghavamza@google.com
Ab stract
A major challenge in modern reinforcement learning (RL) is efficient control
of dynamical systems from high-dimensional sensory observations. Learning
controllable embedding (LCE) is a promising approach that addresses this challenge
by embedding the observations into a lower-dimensional latent space, estimating
the latent dynamics, and utilizing it to perform control in the latent space. Two
important questions in this area are how to learn a representation that is amenable
to the control problem at hand, and how to achieve an end-to-end framework for
representation learning and control. In this paper, we take a few steps towards
addressing these questions. We first formulate a LCE model to learn representations
that are suitable to be used by a policy iteration style algorithm in the latent space.
We call this model control-aware representation learning (CARL). We derive a loss
function and three implementations for CARL. In the offline implementation, we
replace the locally-linear control algorithm (e.g., iLQR) used by the existing LCE
methods with a RL algorithm, namely model-based soft actor-critic, and show that
it results in significant improvement. In online CARL, we interleave representation
learning and control, and demonstrate further gain in performance. Finally, we
propose value-guided CARL, a variation in which we optimize a weighted version
of the CARL loss function, where the weights depend on the TD-error of the
current policy. We evaluate the proposed algorithms by extensive experiments on
benchmark tasks and compare them with several LCE baselines.
1	Introduction
Control of non-linear dynamical systems is a key problem in control theory. Many methods have been
developed with different levels of success in different classes of such problems. The majority of these
methods assume that a model of the system is known and its underlying state is low-dimensional and
observable. These requirements limit the usage of these techniques in controlling dynamical systems
from high-dimensional raw sensory data (e.g., image), where the system dynamics is unknown, a
scenario often seen in modern reinforcement learning (RL).
Recent years have witnessed a rapid development of a large arsenal of model-free RL algorithms,
such as DQN (Mnih et al., 2013), TRPO (Schulman et al., 2015), PPO (Schulman et al., 2017),
and SAC (Haarnoja et al., 2018), with impressive success in solving high-dimensional control
problems. However, most of this success has been limited to simulated environments (e.g., computer
games), mainly due to the fact that these algorithms often require a large number of samples from the
environment. This restricts their applicability in real-world physical systems, for which data collection
is often a difficult process. On the other hand, model-based RL algorithms, such as PILCO (Deisenroth
& Rasmussen, 2011), MBPO (Janner et al., 2019), and Visual Foresight (Ebert et al., 2018), despite
their success, still face difficulties in learning a model (dynamics) in a high-dimensional (pixel) space.
To address the problems faced by model-free and model-based RL algorithms in solving high-
dimensional control problems, a class of algorithms have been developed, whose main idea is to first
learn a low-dimensional latent (embedding) space and a latent model (dynamics), and then use this
model to control the system in the latent space. This class has been referred to as learning controllable
embedding (LCE) and includes algorithms, such as E2C (Watter et al., 2015), RCE (Banijamali et al.,
2018), SOLAR (Zhang et al., 2019), PCC (Levine et al., 2020), Dreamer (Hafner et al., 2020a;b),
PC3 (Shu et al., 2020), and SLAC (Lee et al., 2020). The following two properties are extremely
important in designing LCE models and algorithms. First, to learn a representation that is the most
suitable for the control problem at hand. This suggests incorporating the control algorithm in the
1
Published as a conference paper at ICLR 2021
process of learning representation. This view of learning control-aware representations is aligned with
the value-aware and policy-aware model learning, VAML (Farahmand, 2018) and PAML (Abachi
et al., 2020), frameworks that have been recently proposed in model-based RL. Second, to interleave
the representation learning and control, and to update them both, using a unifying objective function.
This allows to have an end-to-end framework for representation learning and control.
LCE methods, such as SOLAR, Dreamer, and SLAC, have taken steps towards the second objective
by performing representation learning and control in an online fashion. This is in contrast to offline
methods like E2C, RCE, PCC, and PC3 that learn a representation once and then use it in the entire
control process. On the other hand, methods like PCC and PC3 address the first objective by adding
a term to their representation learning loss function that accounts for the curvature of the latent
dynamics. This term regularizes the representation towards smoother latent dynamics, which are
suitable for the locally-linear controllers, e.g., iLQR (Li & Todorov, 2004), used by these methods.
In this paper, we take a few steps towards the above two objectives. We first formulate a LCE model
to learn representations that are suitable to be used by a policy iteration (PI) style algorithm in the
latent space. We call this model control-aware representation learning (CARL) and derive a loss
function for it that exhibits a close connection to the prediction, consistency, and curvature (PCC)
principle for representation learning (Levine et al., 2020). We derive three implementations of CARL:
offline, online, and value-guided. Similar to offline LCE methods, such as E2C, RCE, PCC, and PC3,
in offline CARL, we first learn a representation and then use it in the entire control process. However,
in offline CARL, we replace the locally-linear control algorithm (e.g., iLQR) used by these LCE
methods with a PI-style (actor-critic) RL algorithm. Our choice of RL algorithm is the model-based
implementation of soft actor-critic (SAC) (Haarnoja et al., 2018). Our experiments show significant
performance improvement by replacing iLQR with SAC. Online CARL is an iterative algorithm in
which at each iteration, we first learn a latent representation by minimizing the CARL loss, and then
perform several policy updates using SAC in this latent space. Our experiments with online CARL
show further performance gain over its offline version. Finally, in value-guided CARL (V-CARL),
we optimize a weighted version of the CARL loss function, in which the weights depend on the
TD-error of the current policy. This would help to further incorporate the control algorithm in the
representation learning process. We evaluate the proposed algorithms by extensive experiments on
benchmark tasks and compare them with several LCE baselines: PCC, SOLAR, and Dreamer.
2	Problem Formulation
We are interested in learning control policies for non-linear dynamical systems, where the states
s ∈ S ⊆ Rns are not fully observed and we only have access to their high-dimensional observations
x ∈ X ⊆ Rnx , nx ns. This scenario captures many practical applications in which we interact
with a system only through high-dimensional sensory signals, such as image and audio. We assume
that the observations x have been selected such that we can model the system in the observation space
using a Markov decision process (MDP)1 MX = hX , A, r, P, γi, where X and A are observation
and action spaces; r : X × A → R is the reward function with maximum value Rmax , defined by the
designer of the system to achieve the control objective;2 P : X ×A → P(X) is the unknown transition
kernel; and γ ∈ (0, 1) is the discount factor. Our goal is to find a mapping from observations to control
signals, μ : X → P(A), with maximum expected return, i.e., J(μ) = E[P∞=o Ytr(xt, at) | P,μ].
Since the observations x are high-dimensional and the observation dynamics P is unknown, solving
the control problem in the observation space may not be efficient. As discussed in Section 1, the class
of learning controllable embedding (LCE) algorithms addresses this by learning a low-dimensional
latent (embedding) space Z ⊆ Rnz , nz nx, together with a latent dynamics, and controlling
the system there. The main idea behind LCE is to learn an encoder E : X → P(Z), a latent space
dynamics F : Z × A → P(Z), and a decoder D : Z → P(X),3 such that a good or optimal
controller (policy) in Z performs well in the observation space X . This means that if we model
the control problem in Z as a MDP MZ = hZ, A, r, F, Yi and solve it using a model-based RL
algorithm to obtain a policy π : Z → P(A), the image of π back in the observation space, i.e.,
1A method to ensure observations are Markovian is to buffer them for several time steps (Mnih et al., 2013).
2For example, in a goal tracking problem in which the agent (robot) aims at finding the shortest path to reach
the observation goal xg (the observation corresponding to the goal state sg), we may define the reward for each
observation x as the negative of its distance to xg, i.e., -kx - xg k2 .
3Some recent LCE models, such as PC3 (Shu et al., 2020), are advocating latent models without a decoder.
Although we are aware of the merits of such approach, we use a decoder in the models proposed in this paper.
2
Published as a conference paper at ICLR 2021
Algorithm 1 Latent Space Learning with Policy Iteration (LSLPI)
1:	Inputs: E⑼,F⑼，D(0);
2:	Initialization: μ(0) = random policy; D J samples generated from μ(0);
3:	for i = 0, 1, . . . do
4:	COmPUte π(i) as the projection of μ(i) in the latent space w.r.t. DKL(∏ ◦ E || μ); # μ(i) ≈ π(i) ◦ E(i)
5:Compute the value function of π(i) and set V (i) = Vπ(i) ;#	policy evaluation (critic)
6:Compute the greedy policy w.r.t.	V (i) and set π+(i) = G[V (i)];#	policy improvement (actor)
7:	Set μ(i+1) = π(i) ◦ E(i);	P project the improved policy π(i) back into the observation space
8:	LeanI (E(i+1), F(i+1) ,D(i+1) ,r(i+1)) from D, ∏(i), and π(i);	# representation learning
9:	GeneraaeSamPles D(i+1) = {(xt,at,rt,xt+1)}n=1 from μ(i+1); D J D∪ D(i+1);
10:	end for
(π ◦ E)(a∣x) = Jz dE(z∣x)π(a∣z), should have high expected return. Thus, the loss function to learn
Z and (E, F, D) from observations {(xt, at, rt, xt+1)} should be designed to comply with this goal.
This is why in this paper, we propose a LCE framework that tries to incorporate the control algorithm
used in the latent space in the representation learning process. We call this model, control-aware rep-
resentation learning (CARL). In CARL, we set the class of control (RL) algorithms used in the latent
space to approximate policy iteration (PI), and more specifically to soft actor-critic (SAC) (Haarnoja
et al., 2018). Before describing CARL in details in the following sections, we present a number of
useful definitions and notations here.
For any policy μ in X, we define its value function U* and Bellman operator Tμ as
Uμ(x) = E[X Y trμ(xt) | Pμ,X0 = x],	Tμ[U](x) = Eχ0 〜P-(∙∣ x) [∙μ (x)+ YU(X0)],	(1)
t=0
for all X ∈ X and U : X →R, where r*(x) = Ja dμ(a∣x)r(x, a) and Pμ(x0|x) = Ja dμ(a∣x)P(x0∣x, a)
are the reward function and dynamics induced by μ. Similarly, for any policy π in Z, we de-
fine its induced reward function and dynamics as r∏ (Z) = Ja d∏(a∣z)尸(z,a) and Fn (ZIz) =
Ja d∏(a∣z)F(z0∣z, a). We also define its value function V∏ and Bellman operator Tn as
∞
Vπ (Z)= EX Y tr∏(zt) | Fn ,z0 = z],	Tn [V ](z) = Ez，〜Fn (∙∣z)[rn (z) + YV (z0)]∙⑵
t=0
For any policy π and value function V in the latent space Z, we denote by π ◦ E and V ◦ E, their
image in the observation space X, given encoder E, and define them as
(π ◦ E)(a∣x) = I dE(z∣x)π(a∣z),
z
(V ◦ E)(x) =	dE(z|x)V (z).
z
(3)
3 CARL Model: A Control Perspective
In this section, we formulate our LCE model, which we refer to as control-aware representation
learning (CARL). As described in Section 2, CARL is a model for learning a low-dimensional latent
space Z and the latent dynamics, from data generated in the observation space X, such that this
representation is suitable to be used by a policy iteration (PI) style algorithm in Z. In order to derive
the loss function used by CARL to learn Z and its dynamics, i.e., (E, F, D, r), we first describe how
the representation learning can be interleaved with PI in Z. Algorithm 1 contains the pseudo-code of
the resulting algorithm, which we refer to as latent space learning policy iteration (LSLPI).
Each iteration i of LSLPI starts with a policy μ(i) in the observation space X, which is the mapping
of the improved policy in Z in iteration i - 1, i.e., π+(i-1), back in X through the encoder E(i-1)
(Lines 6 and 7). We then compute n(i), the current policy in Z, as the image of μ(i) in Z through
the encoder E(i) (Line 4). Note that E(i) is the encoder learned at the end of iteration i - 1 (Line 8).
We then use the latent space dynamics F(i) learned at the end of iteration i - 1 (Line 8), and
first compute the value function of π(i) in the policy evaluation or critic step, i.e., V (i) = Vn(i)
(Line 5), and then use V (i) to compute the improved policy π+(i), as the greedy policy w.r.t. V (i),
3
Published as a conference paper at ICLR 2021
i.e., π(i+1) = G[V (i)], in the policy improvement or actor step (Line 6). Using the samples in the
buffer D, together with the current policies in Z, i.e., π(i) and π+(i) , we learn the new representation
(E(i+1), F(i+1), D(i+1), r(i+1)) (Line 8). Finally, We generate samples D(i+1) by following μ(i+1),
the image of the improved policy π+(i) back in X using the old encoder E(i) (Line 7), and add it to the
buffer D (Line 9), and the algorithm iterates. It is important to note that both critic and actor operate
in the low-dimensional latent space Z .
LSLPI is a PI algorithm in Z. However, what is desired is that it also acts as a PI algorithm in X ,
i.e., it results in (monotonic) policy improvement in X, i.e., Uμ(i+i) ≥ U“. Therefore, we define
the representation learning loss function for CARL, such that it ensures LSLPI also results in policy
improvement in X . The following theorem, whose proof is reported in Appendix A, shows the
relationship between the value functions of two consecutive polices generated by LSLPI in X.
Theorem 1. Let μ, μ+, π, ∏+, and (E, F, D, f) be the policies μ(i, μ(i+1), π(i), π(i), and the
learned latent representation (E(i+1), F(Z+1), D(Z+1), f(i+1)) at iteration i Ofthe LSLPI algorithm
(Algorithm 1). Then, the following holds for the value functions of μ and μ+:
Uμ+ (x) ≥ Uμ(x)-(i——	X	Edn◦E [∆(E,F,D,r,π, ∙)∣x0 = x]
γ πe∈{π,π+ }
+ √2γRmax ∙ Ed'E [J。KL ((∏ ◦ E )(∙0∣∙) || μ(∙0∣∙)) ∣X0 = x]),
Lreg(E,μ,π,∙)
(4)
for all X ∈ X, where d∏°E(x0∣x°) = (1 - Y) ∙ E∞=0 Y'P(x' = x0 |x0 ; π ◦ E) is the γ-stationary
distribution induced by policy π ◦ E, and the error term ∆ for a policy π is given by
(I)=Led(E,D,x)
Z	—N	{
∆(E,F,D,r,π,x) = RmaxJ-I Z dE(z∣x)log D(x∣z)
1- γ 2 z
(II) = Lr (E,r,∏,χ)
z----------------------------
+ 2∣r∏°E(x) - / dE(z∣x)f∏(z)∣
(5)
{
+ √YRmax ʌ ( qKL(Pn「(∙∣x) ||(D oF∏ ◦ E)(∙∣x)) + qKL((E ◦ Pn。E)(∙∣x) ∣∣(F∏ ◦ E)(∙∣x))).
21- - γ) -------------------V----------------} 、-----------------V-----------------}
(III)=Lp(E,F,D,n,x)	(IV)
It is easy to see that LSLPI guarantees (policy) improvement in X, if the terms in the parentheses
on the RHS of (4) are zero. We now describe these terms. The last term on the RHS of (4) is the
KL between π(i) ◦ E and μ(i) = π(i) ◦ E(i). This term can be seen as a regularizer to keep the new
encoder E close to the old one E(Z). The four terms in (5) are: (I) The encoding-decoding error to
ensure x ≈ (D ◦ E)(x); (II) The error that measures the mismatch between the reward of taking
action according to policy π ◦ E at x ∈ X, and the reward of taking action according to policy π at
the image of x in Z under E ; (III) The error in predicting the next observation through paths in X
and Z. This is the error between x0 and X0 shown in Fig. 1(a); and (IV) The error in predicting the
next latent state through paths in X and Z. This is the error between Z0 and Z0 shown in Fig. 1(b).
Figure 1: (a) Paths from the current observation X to the next one, (left) in X and (right) through
Z . (b) Paths from the current observation X to the next latent state, (left) through X followed by
encoding and (right) starting with encoding and then through Z .
Representation Learning in CARL Theorem 1 provides us with a recipe (loss function) to learn
the latent space Z and (E, F, D, r). In CARL, we propose to learn a representation for which
the terms in the parentheses on the RHS of (4) are small. As mentioned earlier, the second term,
4
Published as a conference paper at ICLR 2021
Lreg (E, μ,∏,X), can be considered as a regularizer to keep the new encoder E close to the old one E-,
when the policy μ is given by π ◦ E-. Term (I) minimizes the reconstruction error between encoder
and decoder, which is standard for training auto-encoders (Kingma & Welling, 2013). Term (II) that
measures the mismatch between rewards can be kept small, or even zero, if the designer of the system
selects the rewards in a compatible way4. Although CARL allows us to learn a reward function in the
latent space, similar to several other LCE works (Watter et al., 2015; Banijamali et al., 2018; Levine
et al., 2020; Shu et al., 2020), in this paper, we assume that a compatible latent reward function is
given. Terms (III) and (IV) are the equivalent of the prediction and consistency terms in PCC (Levine
et al., 2020) for a particular latent space policy π. Since PCC has been designed for an offline
setting (i.e., one-shot representation learning and control), its prediction and consistency terms are
independent of a particular policy and are defined for state-action pairs. While CARL is designed for
an online setting (i.e., interleaving representation learning and control), and thus, its loss function at
each iteration depends on the current latent space policies π and π+. As we will see in Section 4, in our
offline implementation of CARL, these two terms are similar to prediction and consistency terms in
PCC. Note that (IV) is slightly different than the consistency term in PCC. However, ifwe upper-bound
it using Jensen inequality: (IV) ≤ Lc(E,F,∏,χ) := Rxo∈χ dP∏°E(X∖X) ∙ Dkl(E(∙∣x0) || (F∏ ◦ E)(∙∣χ)),
the resulted loss, Lc(E, F, π, x), would be similar to the consistency term in PCC. Similar to PCC,
we also add a curvature loss to the loss function of CARL to encourage having a smoother latent
space dynamics Fπ . Putting all these terms together, we obtain the following loss function for CARL:
min	λedLed(E, D, x) + λpLp(E, F, D, π, x) + λcLc(E, F, π, x)
E，F，D M	(6)
+ λcurLcur(F, ∏, x) + λregLreg(E, μ, ∏, x),
where (λed, λp, λc, λcur, λreg) are hyper-parameters5 of the algorithm, (Led, Lp) are the encoding-
decoding and prediction losses defined in (5), Lc is the consistency loss defined above, Lcur =
Eχ,u[Ee [fz(Z + “,U + Cu) — fz(z,u) — (Vzfz(z,u) Yz + VufZ(z,u) ∙ Cu)k2∖ ∖ E] is the curvature loss
that regulates the 2nd derivative of fZ, the mean of latent dynamics F, in which z , u are standard
Gaussian noise, and Lreg is the regularizer that ensures the new encoder remains close to the old one.
4	Different Implementations of CARL
The CARL loss function in (6) introduces an optimization problem that takes a policy π in Z as
input and learns a representation suitable for its evaluation and improvement. To optimize this loss
in practice, similar to the PCC model (Levine et al., 2020), we define P = D ◦ Fπ ◦ E as a latent
variable model that is factorized as P(xt+ι, zt, zt+ι∖xt,π) = P(zt∣xt)P(zt+ι∣zt, ∏)P(xt+1∣^t+1),
and use a variational approximation to the interactable negative log-likelihood of the loss terms in (6).
The variational bounds for these terms can be obtained similar to Eqs. 6 and 7 in Levine et al. (2020).
Below we describe three instantiations of the CARL model in practice. Implementation details can be
found in Algorithm 2 in Appendix D. Although CARL is compatible with most PI-style (actor-critic)
RL algorithms, we choose soft actor-critic (SAC) (Haarnoja et al., 2018) as its control algorithm.
Since most actor-critic algorithms are based on first-order gradient updates, as discussed in Section 3,
we regularize the curvature of the latent dynamics F (see Eqs. 8 and 9 in Levine et al. 2020) in CARL
to improve its empirical stability and performance in policy learning.
1.	Offline CARL We first implement CARL in an offline setting, where we generate a (relatively)
large batch of observation samples {(xt, at, rt, xt+1)}tN=1 using an exploratory (e.g., random) policy.
We then use this batch to optimize the CARL’s loss function (6) via the variational approximation
scheme described above, and learn a latent representation Z and (E, F, D). Finally, we solve the
decision problem in Z using a model-based RL algorithm, which in our case is model-based SAC6.
The learned policy ∏* in Z is then used to control the system from observations as at 〜(∏* oE)(∙∖xt).
This is the setting that has been used in several recent LCE works, such as E2C (Watter et al., 2015),
RCE (Banijamali et al., 2018), PCC (Levine et al., 2020), and PC3 (Shu et al., 2020). Our offline
4For example, in goal-based RL problems, a compatible reward function can be the one that measures the
negative distance between a latent state and the image of the goal in the latent space.
5 Theorem 1 provides a high-level guideline for selecting the hyper-parameters of the loss function: λed =
2Rmax∕(l - γ)2, λc = λp = √2γRmaχ/(1 一 γ)2, and λreg = √2γRmax/(1 一 γ).
6By model-based SAC, we refer to learning a latent policy with SAC using synthetic trajectories generated
by unrolling the learned latent dynamics model F, similar to the MBPO algorithm (Janner et al., 2019).
5
Published as a conference paper at ICLR 2021
implementation is different than those in which 1) we replace their locally-linear control algorithm,
namely iterative LQR (iLQR) (Li & Todorov, 2004), with model-based SAC, which results in
significant performance improvement, as shown in Section 5, and 2) we optimize the CARL loss
function, that despite close connection, is still different than the one used by PCC.
The CARL loss function presented in Section 3 has been designed for an online setting in which
at each iteration, it takes a policy as input and learns a representation that is suitable for evaluating
and improving this policy. However, in the offline setting, the learned representation should be good
for any policy generated in the course of running the PI-style control algorithm. Therefore, we
marginalize out the policy from the (online) CARL’s loss function and use the RHS of the following
corollary (proof in Appendix B) to construct the CARL loss function used in our offline experiments.
Corollary 2. Let μ and μ+ be two consecutive policies in X generated by a PI-Style control algorithm
in the latent space constructed by (E,F,D,尸).Then, thefoUowing holdsfor the value functions of μ
and μ+, where ∆ is defined by (5) (in modulo replacing sampled action a 〜π◦E with action a):
2
Uμ+ (x) ≥ Uμ(x)-----------∙ max	∆(E,F,D,尸,a,x), ∀x ∈ X.	(7)
+	1 - γ x,∈X,a∈A
2.	Online CARL In the online implementation of CARL, at each iteration i, the current policy π(i)
is the improved policy of the last iteration, π+(i-1). We first generate a relatively (to offline CARL)
small batch of samples using the image of the current policy in X, i.e., μ(i) = ∏(i) ◦ E(i-1), and then
learn a representation (E(i),F(i),D(i)) suitable for evaluating and improving the image of μ(i) in Z
under the new encoder E(i). This means that with the new representation, the current policy that was
the image of μ(i) in Z under E(i-1), should be replaced by its image ∏(i) under the new encoder,
i.e., π(i) ◦ E(i) ≈ μ(i). In online CARL, we address this by the following policy distillation step in
which we minimize the following loss:7
π(i) ∈ arg min ^X DKL ((∏ ◦ E(i))(∙∣x) || (∏(i-1) ◦ E(i-1))(∙∣x)).	(8)
π	X〜D
After the current policy π(i) is set, we perform multiple steps of (model-based) SAC in Z using the
current model, (F(i),r(i)), and then send the resulting policy ∏+) to the next iteration.
3.	Value-Guided CARL (V-CARL) While Theorem 1 shows that minimizing the loss in (6) guar-
antees performance improvement, this loss does not contain any information about the performance
of the current policy μ, and thus, the LCE model trained with this loss may have low accuracy in
regions of the latent space that are crucial for learning good RL policies. In V-CARL, we tackle this
issue by modifying the loss function in a way that the resulted LCE model has more accuracy in
regions with higher anticipated future returns.
To derive the V-CARL’s loss function, we use the variational model-based policy optimization
(VMBPO) framework by Chow et al. (2020) in which the optimal dynamics for model-based
RL can be expressed in closed-form as P*(χ0∣χ,a) = P(x0∣x,a) ∙ exp (Y(r(x,a) + γUμ(x0)-
WW1μ(x,a))), where	Uμ(x)	:=	1 log E [exp	(T P= γtrμ,t)	∣Pμ,xo = x]	and	Wμ(x,a)	:=	r(x,a)	+
Y log Ex，〜P(∙∣x,a) [eχp(τUμ(χ0))] are the optimistic value and action-value functions8 of policy μ,
and τ > 0 is a temperature parameter. Note that in the VMBPO framework, the optimal dynamics
P * is value-aware, because it re-weighs P with an exponential-twisting weight exp( Y w(χ,a,χ0)),
where w(x, a, χ0) := r(x, a) + γUμ(χ0) - WW1u(χ, a) is the temporal difference (TD) error.
In V-CARL, we use the VMBPO framework to modify the CARL’s prediction loss Lp (E, F, D, π, x).
Since the regularizer loss Lreg(E, μ, ∏, x) in CARL forces policies ∏◦ E and μ to be close to each other,
we may replace the transition dynamics PnoE with Pμ in Lp. This makes minimizing Lp equivalent to
maximizing the log-likelihood Rx0 dPμ(χ0∣χ) ∙ log(D ◦ F∏ ◦ E)(XIx). Finally, we replace Pμ with Pμ
in this log-likelihood and obtain Ra dμ(a∣x) Rx，dP(x0∣x,a) ∙ exp(Y ∙ w(x,a,x0)) ∙ log(D ◦ F∏ ◦ E)(X0∣x),
which is a weighted (by the exponential TD w(x, a, x0)) log-likelihood function (w.r.t. P). Note
7Our experiments reported in Appendix F.1 show that adding distillation improves the performance in online
CARL. Thus, all our results for online CARL and V-CARL, unless mentioned, are with policy distillation.
8We refer to Uμ as the optimistic value function (RUSZCZynSki & Shapiro, 2006), because it models the right
tail of the return via the exponential utility pτ (U (∙)∣x,a) = 1 log Ex，〜P (∙∣x,a)[exp(τ ∙ U (x0))].
6
Published as a conference paper at ICLR 2021
.ι . .ι ∙	∙ 1 . 1	1	.ι	.∙ ∙ . ∙	1 r∙ . ∙	τ~τ	ι ^rττ TyTj	∕∖ ∙	ιι
that this weight depends on the optimistic value functions U* and Wμ. When τ > 0 is small
(see Appendix C for more details), these value functions can be approximated by their standard
counterparts, i.e., Uμ(χ) ≈Uμ(χ) and Wu(χ,a) ≈ Wμ(χ,a):=r(χ,a)+ Rx0dP(x0∣x,a)Uμ(x0), which can
be further approximated by their latent-space counterparts, i.e., Uμ(χ) ≈ (V∏ ◦ E)(X) and Wμ(x, a) ≈
(Qn ◦ E)(χ, a), according to Lemma 5 in Appendix A.1. Since the latent reward function r is defined
such that r(x, a) ≈ (r ◦ E)(x, a), We may write the TD-error w(x, a, χ0) in terms of the encoder E and
the latent value functions as	记(χ,a,χ0)	:= R	0 dE(z∣x) ∙ dE(z0∣x0)	∙	(r(z,a)	-	Qn(z,a)	+ γV∏ (z0)).
z,z
5 Experimental Results
In this section, we experiment with the following continuous control domains: (i) Planar System,
(ii) Inverted Pendulum (Swingup), (iii) Cartpole, (iv) Three-link Manipulator (3-Pole), and compare
the performance of our CARL algorithms with three LCE baselines: PCC (Levine et al., 2020),
SOLAR (Zhang et al., 2019), SLAC (Lee et al., 2020), and two implementations of Dreamer (Hafner
et al., 2020a) (described below).9 These tasks have underlying start and goal states that are “not”
observable, instead, the algorithms only have access to the start and goal observations. We report
the detailed setup of the experiments in Appendix E, in particular, the description of the domains in
Appendix E.1 and the implementation of the algorithms in Appendix E.3.
To evaluate the performance of the algorithms, similar to Levine et al. (2020), we report the %-
time spent in the goal. The initial policy that is used for data generation is uniformly random (see
Appendix E.2 for more details). To measure performance reproducibility for each experiment, we
(i) train 25 models, and (ii) perform 10 control tasks for each model. For SOLAR, due to its high
computation cost, we only train and evaluate 10 different models. Besides the average results, we
also report the results from the best LCE models, averaged over the 10 control tasks.
General Results Table 1 shows the means and standard errors of %-time spent in goal, averaged
over all models and control tasks, and averaged over all control tasks for the best model. To compare
data efficiency, we also report the number of samples required to train the latent space and controller
in each algorithm. We also show the training curves (performance vs. number of samples) of the
algorithms in Fig. 2. We report more experiments and ablation studies in Appendix F.
Below summarizes our main observations of the experiments. First, offline CARL that uses model-
based SAC as its control algorithm achieves significantly better performance than PCC that uses
iLQR in all tasks. This can be attributed to the advantage that SAC is more robust and effective in non-
(locally)-linear environments. We report more detailed comparison between PCC and offline CARL
in Appendix F.3, where we explicitly compare their control performance and latent representation
maps. Second, in all tasks, online CARL is more data-efficient than its offline counterpart, i.e., it
achieves similar or better performance with fewer samples. In particular, online CARL is notably
superior in Planar, Cartpole, and Swingup, in which it achieves similar performance to offline CARL
with 2, 2.5, and 4 times less samples, respectively (see Fig. 2). In Appendix F.3, we show how the
latent representation of online CARL progressively improves through the iterations of the algorithm
(in particular, see Fig. 11). Third, in the simpler tasks (Planar, Swingup, Cartpole), V-CARL
performs even better than online CARL. This corroborates our hypothesis that CARL can achieve
extra improvement when its LCE model is more accurate in the regions of the latent space with higher
temporal difference (regions with higher anticipated future return). In 3-pole, the performance of
V-CARL is worse than online CARL. This is likely due to the instability in representation learning
resulted from sample variance amplification by the exponential-TD weight. Fourth, SOLAR requires
significantly more samples to learn a reasonable latent space for control, and with limited data it
fails to converge to a good policy. Even with the fine-tuned latent space from Zhang et al. (2019), its
performance is incomparable to those of CARL variants and Dreamer. We report more experiments
with SOLAR in Appendix F.5, in which we show that SOLAR can perform better, especially in Planar
when we fix the start and goal locations. However, the improved performance is still incomparable
with those of CARL and Dreamer. Fifth, we include an ablation study in Appendix F.2 to demonstrate
how each term of the CARL’s loss function impacts policy learning. It shows the importance of the
prediction and consistency terms, without which the resulting algorithms struggle, and the (relatively)
minor role of the curvature and encoder-decoder terms in the performance of the algorithms.
9We did not include E2C and RCE in our experiments, because Levine et al. (2020) has previously shown
that PCC outperforms them.
7
Published as a conference paper at ICLR 2021
Dreamer As described in Section 2, most LCE algorithms, including E2C, PCC, and CARL
variants, assume the observation space X is selected such that the system is Markovian there. In
contrast, Dreamer does not make this assumption and has been designed for more general class of
control problems that can be modeled as POMDPs. Thus, it is expected that it performs inferior
(requires more samples to achieve the same performance) to CARL when the system is Markov in
the observation space. Moreover, CARL and other LCE methods define the reward as the negative
distance to the goal in the latent space. This cannot be done in Dreamer, where the encoder is an
RNN that takes an entire observation trajectory as input. To address this, we propose two methods
to train the Dreamer’s reward function in the latent space, which we refer to as Dreamer Pixel and
Dreamer Oracle. While Dreamer Pixel uses the negative distance to the goal in the observation
space X as the signal to train the reward function, Dreamer Oracle uses the negative distance in the
(unobserved) underlying state space S . Thus, it is more fair to compare the CARL algorithms with
Dreamer Pixel than Dreamer Oracle that has the advantage of having access to the underlying state
space (see Appendix F.6 for more details). As it was expected, our results show that although both
Dreamer’s implementations learn reasonably-performing policies for most tasks (except Planar), they
require twice to 100-times more samples to achieve the same performance as the CARL algorithms.
We report longer (more samples) experiments with Dreamer on all tasks in Appendix F.6 (Fig. 12).
(c) Cartpole
(d) Three-pole
(a) Planar	(b) Swingup
Figure 2:	Training curves of offline CARL, online CARL, V-CARL, and two implementations of
Dreamer. The shaded region represents mean ± standard error.
Environment	Algorithm	Number of Samples	Avg %-Goal	Best %-Goal
Planar	^ΓCC	^3000	38.85 ± 2.45	62.5 ± 10.42
Planar	Offline CARL	^3000	63.43 ± 2.78	79.51 ± 0.38
Planar	Online CARL	^3072	68.03 ± 1.69	79.02 ± 0.38
Planar	V-CARL	-3200	71.05 ± 1.46	79.51 ± 0.38
Planar	SOLAR	5000 (VAE) + 16000 (ContrQlT	5.82 ± 2.50	9.13 ± 3.54
SWingUP	PCC	5000	86.60 ± 1.0寸	97.40 ± 0.6广
SWingUP	Oflme CARL	^3000	88.43 ± 2.02	98.50 ± 0.0
Swingup	Online CARL		95.04 ± 0.96	98.50 ± 0.0
Swingup	V-CARL		96.50 ± 0.25	98.50 ± 0.0
Swingup	SOLAR	5200 (VAE) + 40000 (Control)	16.1 ± 0.69	22.45 ± 1.96
Swingup	Dreamer Pixel	180895	70.35 ± 0.62	98.5 ± 0.0
Swingup	Dreamer Oracle	183084	94.65 ± 0.20~~	98.25 ± 0.0—
CartPole	PCC	10000	83.64 ± 0.6十	100.0 ± 0.0 =
CartPole	Offline CARL	10000	91.11 ± 1.50	100.0 ± 0.0
CartPole	Online CARL	^3I20	95.34 ± 1.17	100.0 ± 0.0
CartPole	V-CARL	^3I20	95.79 ± 1.06	100.0 ± 0.0
Cartpole	SOLAR	5000 (VAE) + 40000 (Control)	10.61 ± 2.58	12.33 ± 2.96
Cartpole	Dreamer Pixel	96941	95.59 ± 3.77	100.0 ± 0.0
Cartpole	Dreamer Oracle	14474	97.77 ± 1.525-	100.0 ± 0.0 —
Three-pole	PCC	4096	4.41 ± 0.75=	36.20 ± 7.06"
Three-pole	Oflme CARL	^4096	63.20 ± 1.77	88.55 ± 0.0
Three-pole	Online CARL	^^944	62.17 ± 2.28	90.05 ± 0.0
Three-pole	V-CARL	^^8Γ6	55.06 ± 2.42	89.05 ± 0.0
Three-Pole	SOLAR	2000 (VAE) + 20000 (Control)	0 ± 0	0 ± 0
Three-PoIe	Dreamer Pixel	-6245	61.93 ± 2.30	90.00 ± 0.0
Three-PoIe	Dreamer Oracle	6245	71.07 ± 2.45~~	88.40 ± 0.0—
Table 1: Mean ± standard error results (%-goal) and samples used for different LCE algorithms.
Results with Environment-biased Sampling In the previous experiments, all the online LCE
algorithms are warm-started with data collected by a uniformly random policy over the entire
8
Published as a conference paper at ICLR 2021
environment. With sufficient data the latent dynamics is accurate enough on most parts of the state
space for control, therefore we do not observe a significant difference between online CARL and V-
CARL. To further illustrate the advantage of V-CARL over online CARL, we modify the experimental
setting by gathering initial samples only from a specific region of the environment (see Appendix E.1
for more details). Fig. 3 shows the learning curves of online CARL and V-CARL in this case. As
expected, with biased data, both algorithms experience a certain level of performance degradation,
yet, V-CARL clearly outperforms online CARL — this verifies our conjecture that control-aware
LCE models are more robust to initial data distribution and superior in policy optimization.
1000	1500	2000	2500
Number of Environment Samples
Number of Environment Samples
Online CARL
√alue-Guided CARL
.000 1500 2000 2500 300∣
Number of Environment Samples
(a) Planar	(b) Swingup	(c) Cartpole	(d) Three-pole
Figure 3:	Training curves of Online CARL and V-CARL with environment-biased initial samples.
6 Conclusions
In this paper, we argued for incorporating control in the representation learning process and for the
interaction between control and representation learning in learning controllable embedding (LCE)
algorithms. We proposed a LCE model called control-aware representation learning (CARL) that
learns representations suitable for policy iteration (PI) style control algorithms. We proposed three
implementations of CARL that combine representation learning with model-based soft actor-critic
(SAC), as the controller, in offline and online fashions. In the third implementation, called value-
guided CARL, we further included the control process in representation learning by optimizing a
weighted version of the CARL loss function, in which the weights depend on the TD-error of the
current policy. We evaluated the proposed algorithms on benchmark tasks and compared them with
several LCE baselines. The experiments show the importance of SAC as the controller and of the
online implementation. Future directions include 1) investigating other PI-style algorithms in place
of SAC, 2) developing LCE models suitable for value iteration style algorithms, and 3) identifying
other forms of bias for learning an effective embedding and latent dynamics.
References
R. Abachi, M. Ghavamzadeh, and A. Farahmand. Policy-aware model learning for policy gradient
methods. preprint arXiv:2003.00030, 2020.
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. arXiv
preprint arXiv:1705.10528, 2017.
E. Banijamali, R. Shu, M. Ghavamzadeh, H. Bui, and A. Ghodsi. Robust locally-linear controllable
embedding. In Proceedings of the Twenty First International Conference on Artificial Intelligence
and Statistics, pp. 1751-1759, 2018.
V. Borkar. Q-learning for risk-sensitive control. Mathematics of operations research, 27(2):294-311,
2002.
M. Breivik and T. Fossen. Principles of guidance-based path following in 2D and 3D. In Proceedings
of the 44th IEEE Conference on Decision and Control, pp. 627-634, 2005.
Y. Chow, B. Cui, M. Ryu, and M. Ghavamzadeh. Variational model-based policy optimization. In
arXiv, 2020.
M. Deisenroth and C. Rasmussen. PILCO: A model-based and data-efficient approach to policy
search. In Proceedings of the 28th International Conference on Machine Learning, pp. 465-472,
2011.
F. Ebert, C. Finn, S. Dasari, A. Xie, A. Lee, and S. Levine. Visual Foresight: Model-based deep
reinforcement learning for vision-based robotic control. preprint arXiv:1812.00568, 2018.
9
Published as a conference paper at ICLR 2021
A. Farahmand. Iterative value-aware model learning. In Advances in Neural Information Processing
Systems 31 ,pp. 9072-9083, 2018.
A. Farahmand, A. Barreto, and D. Nikovski. Value-aware loss function for model-based reinforcement
learning. In Artificial Intelligence and Statistics, pp. 1486-1494, 2017.
K. Furuta, M. Yamakita, and S. Kobayashi. Swing up control of inverted pendulum. In Proceedings of
International Conference on Industrial Electronics, Control and Instrumentation, pp. 2193-2198,
1991.
S. Geva and J. Sitte. A cartpole experiment benchmark for trainable controllers. IEEE Control
Systems Magazine, 13(5):40-51, 1993.
T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep
reinforcement learning with a stochastic actor. In Proceedings of the 35th International Conference
on Machine Learning, pp. 1861-1870, 2018.
D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi. Dream to control: Learning behaviors by latent
imagination. In International Conference on Learning Representations, 2020a.
D. Hafner, T. Lillicrap, M. Norouzi, and J. Ba. Mastering atari with discrete world models. arXiv
preprint arXiv:2010.02193, 2020b.
M. Janner, J. Fu, M. Zhang, and S. Levine. When to trust your model: Model-based policy optimiza-
tion. In Advances in Neural Information Processing Systems 32, pp. 12519-12530. 2019.
D. Kingma and J. Ba. Adam: A method for stochastic optimization, 2014.
D. Kingma and M. Welling. Auto-encoding variational bayes, 2013.
X. Lai, A. Zhang, M. Wu, and J. She. Singularity-avoiding swing-up control for underactuated
three-link gymnast robot using virtual coupling between control torques. International Journal of
Robust and Nonlinear Control, 25(2):207-221, 2015.
A. Lee, A. Nagabandi, P. Abbeel, and S. Levine. Stochastic latent actor-critic: Deep reinforcement
learning with a latent variable model. Advances in Neural Information Processing Systems, 33,
2020.
N. Levine, Y. Chow, R. Shu, A. Li, M. Ghavamzadeh, and H. Bui. Prediction, consistency, curva-
ture: Representation learning for locally-linear control. In Proceedings of the 8th International
Conference on Learning Representations, 2020.
W. Li and E. Todorov. Iterative linear quadratic regulator design for nonlinear biological movement
systems. In ICINCO (1), pp. 222-229, 2004.
V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller.
Playing Atari with deep reinforcement learning. preprint arXiv:1312.5602, 2013.
A. RUszczynski and A. Shapiro. Optimization of convex risk functions. Mathematics ofoperations
research, 31(3):433-452, 2006.
J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In
Proceedings of the 32nd International Conference on Machine Learning, pp. 1889-1897, 2015.
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization
algorithms. preprint arXiv:1707.06347, 2017.
R. Shu, T. Nguyen, Y. Chow, T. Pham, K. Than, M. Ghavamzadeh, S. Ermon, and H. Bui. Predictive
coding for locally-linear control. preprint arXiv:2003.01086, 2020.
M. Spong. The swing up control problem for the acrobot. IEEE Control Systems Magazine, 15(1):
49-55, 1995.
M. Watter, J. Springenberg, J. Boedecker, and M. Riedmiller. Embed to control: A locally linear
latent dynamics model for control from raw images. In Advances in Neural Information Processing
Systems 28, pp. 2746-2754. 2015.
M. Zhang, S. Vikram, L. Smith, P. Abbeel, M. Johnson, and S. Levine. SOLAR: Deep structured
representations for model-based reinforcement learning. In Proceedings of the 36th International
Conference on Machine Learning, pp. 7444-7453, 2019.
10