Published as a conference paper at ICLR 2021
One Network Fits All? Modular versus
Monolithic Task Formulations in Neural
Networks
Atish Agarwala & Abhimanyu Das
Google Research
{thetish,abhidas}@google.com
Vatsal Sharan
MrTt
vsharan@mit.edu
Brendan Juba	Rina Panigrahy
Washington U. St. Louis*	Google Research
bjuba@wustl.edu rinap@google.com
Xin Wang & Qiuyi Zhang
Google Research
{wanxin,qiuyiz}@google.com
Ab stract
Can deep learning solve multiple tasks simultaneously, even when they are unre-
lated and very different? We investigate how the representations of the underlying
tasks affect the ability of a single neural network to learn them jointly. We present
theoretical and empirical findings that a single neural network is capable of si-
multaneously learning multiple tasks from a combined data set, for a variety
of methods for representing tasks—for example, when the distinct tasks are en-
coded by well-separated clusters or decision trees over certain task-code attributes.
More concretely, we present a novel analysis that shows that families of simple
programming-like constructs for the codes encoding the tasks are learnable by
two-layer neural networks with standard training. We study more generally how
the complexity of learning such combined tasks grows with the complexity of the
task codes; we find that combining many tasks may incur a sample complexity
penalty, even though the individual tasks are easy to learn. We provide empirical
support for the usefulness of the learning bounds by training networks on clusters,
decision trees, and SQL-style aggregation.
1 Introduction
Standard practice in machine learning has long been to only address carefully circumscribed, often
very related tasks. For example, we might train a single classifier to label an image as containing
objects from a certain predefined set, or to label the words of a sentence with their semantic roles.
rndeed, when working with relatively simple classes of functions like linear classifiers, it would be
unreasonable to expect to train a classifier that handles more than such a carefully scoped task (or
related tasks in standard multitask learning). As techniques for learning with relatively rich classes
such as neural networks have been developed, it is natural to ask whether or not such scoping of tasks
is inherently necessary. rndeed, many recent works (see Section 1.2) have proposed eschewing this
careful scoping of tasks, and instead training a single, “monolithic” function spanning many tasks.
Large, deep neural networks can, in principle, represent multiple classifiers in such a monolithic
learned function (Hornik, 1991), giving rise to the field of multitask learning. This combined function
might be learned by combining all of the training data for all of the tasks into one large batch-see
Section 1.2 for some examples. Taken to an extreme, we could consider seeking to learn a universal
circuit—that is, a circuit that interprets arbitrary programs in a programming language which can
encode various tasks. But, the ability to represent such a monolithic combined function does not
necessarily entail that such a function can be efficiently learned by existing methods. Cryptographic
hardness theorems (Kearns & Valiant, 1994) establish that this is not possible in general by any
method, let alone the specific training methods used in practice. Nevertheless, we still can ask how
* Work performed in part while visiting Google.
^ Work performed in part while affiliated with Stanford, and in part while interning at Google.
1
Published as a conference paper at ICLR 2021
-Class: Person -
string name
Address address
int income
—Class: Address—
string street
string city
string zip_code
Input: Positions Xi of k bodies
each with mass r∏ι
Output: Gravitational force Fi
-Table: zip_map—
Table mapping
zip_code to
(latitude, longitude)
boolean in_same_zip_codeCPθrson A, Person B):
-evaluate if A & B live in same zip code
float get.straight.line.distanceCPθf,son A, Person B):
-lookup (latitude, longitude) of Person A & B from table zipumap
-evaluate Euclidean distance between A & B based on (latitude, longitude)
float avg_income_zip_code(string zip_code):
-construct SQL table T with income, zip_code for all instances of Person
-evaluate query rtSELECT AVGCLnCome5 FROM T WHERE ZIP_CODE=zip_codeM
Figure 1: Our framework shows that it is possible to learn analytic functions such as the gravitational
force law, decision trees with different functions at the leaf nodes, and programming constructs such
as those on the right, all using a non-modular monolithic architecture.
rich a family of tasks can be learned by these standard methods. In this work, we study the extent to
which backpropagation with stochastic gradient descent (SGD) can learn such monolithic functions
on diverse, unrelated tasks. There might still be some inherent benefit to an architecture in which tasks
are partitioned into sub-tasks of such small scope, and the training data is correspondingly partitioned
prior to learning. For example, in the early work on multitask learning, Caruana (1997) observed that
training a network to solve unrelated tasks simultaneously seemed to harm the overall performance.
Similarly, the seminal work of Jacobs et al. (1991) begins by stating that “If backpropagation is used
to train a single, multilayer network to perform different subtasks on different occasions, there will
generally be strong interference effects that lead to slow learning and poor generalization”. We
therefore ask if, for an unfortunate choice of tasks in our model, learning by standard methods might
be fundamentally impaired.
As a point of reference from neuroscience, the classical view is that distinct tasks are handled in the
brain by distinct patches of the cortex. While it is a subject of debate whether modularity exists for
higher level tasks (Samuels, 2006), it is accepted that there are dedicated modules for low-level tasks
such as vision and audio processing. Thus, it seems that the brain produces a modular architecture,
in which different tasks are handled by different regions of the cortex. Conceivably, this division
into task-specific regions might be driven by fundamental considerations of learnability: A single,
monolithic neural circuit might simply be too difficult to learn because the different tasks might
interfere with one another. Others have taken neural networks trained by backpropagation as a model
of learning in the cortex (Musslick et al., 2017); to the extent that this is reasonable, our work has
some bearing on these questions as well.
1.1	Our results
We find, perhaps surprisingly, that combining multiple tasks into one cannot fundamentally impair
learning with standard training methods. We demonstrate this for a broad family of methods for
combining individual tasks into a single monolithic task. For example, inputs for each individual
tasks may come from a disjoint region (for example, a disjoint ball) in a common input space, and
each individual task could then involve applying some arbitrary simple function (e.g., a separate
linear classifier for each region). Alternately there may be an explicit “task code” attribute (e.g., a
one-hot code), together with the usual input attributes and output label(s), where examples with the
same task code are examples for the same learning task. Complementing our results that combining
multiple tasks does not impair learning, we also find that some task coding schemes do incur a sample
complexity penalty.
A vast variety of task coding schemes may be used. As a concrete example, when the data points
for each task are well-separated into distinct clusters, and the tasks are linear classification tasks, we
show that a two-layer architecture trained with SGD successfully learns the combined, monolithic
function; the required amount of data simply scales as the sum of the amount required to learn each
2
Published as a conference paper at ICLR 2021
task individually (Theorem 2). Meanwhile, if the tasks are determined by a balanced decision tree of
height h on d code attributes (as in Fig. 1, left), we find that the training time and amount of data
needed scales as 〜dh——quasipolynomial in the 2h leaves (distinct tasks) when d is of similar size to
h, and thus when the coding is efficient (Theorem 3). We also prove a corresponding lower bound,
which shows that this bound is in fact asymptotically tight (Theorem 3). More generally, for task
codings based on decision trees using linear splits with a margin of at least γ (when the data has unit
'2 norm), the training time and required data are asymptotically bounded by 〜eO(h/Y2), which for
constant γ is polynomial in the 2h functions (Theorem 4).
We generalize from these cluster-based and decision-tree based task codings to more complex codes
that are actually simple programs. For instance, we show that SQL-style aggregation queries over a
fixed database, written as a functions of the parameters of the query, can also be learned this way.
More generally, simple programming constructs (such as in Fig. 1, right), built by operations such
as compositions, aggregation, concatenation, and branching on a small number of such learnable
functions, are also learnable (Theorem 5). In general, we can learn a low-depth formula (circuit with
fan-out 1) in which each gate is not merely a switch (as in a decision tree), but can be any analytic
function on the inputs, including arithmetic operations. Again, our key technical contribution is that
we show that all of these functions are efficiently learned by SGD. This is non-trival since, although
universal approximation theorems show that such functions can be expressed by (sufficiently wide)
two-layer neural networks, under standard assumptions some expressible functions are not learnable
Klivans & Sherstov (2009). We supplement the theoretical bounds with experiments on clusters,
decision trees, and SQL-style aggregation showing that such functions are indeed learned in practice.
We note that the learning of such combined functions could have been engineered by hand: for
example, there exist efficient algorithms for learning clusterings or such decision trees, and it is easy
to learn the linear classifiers given the partitioned data. Likewise, these classes of functions are all
known to be learnable by other methods, given an appropriate transformation of the input features.
The key point is that the two-layer neural network can jointly learn the task coding scheme and the
task-specific functions without special engineering of the architecture. That is, it is unnecessary to
engineer a way of partitioning of the data into separate tasks prior to learning. Relatedly, the time
and sample requirements of learning multiple tasks on a single network in general is insufficient to
explain the modularity observed in biological neural networks if their learning dynamics are similar
to SGD ——i.e., we cannot explain the presence of modularity from such general considerations.
All our theoretical results are based upon a fundamental theorem that shows that analytic functions
can be efficiently learnt by wide (but finite-width) two-layer neural networks with standard activation
functions (such as ReLU), using SGD from a random initialization. Specifically, we derive novel
generalization bounds for multivariate analytic functions (Theorems 1 and 8) by relating wide
networks to kernel learning with a specific network-induced kernel (Jacot et al., 2018; Du et al.,
2019; Allen-Zhu et al., 2019; Arora et al., 2019a; Lee et al., 2019), known as the neural tangent
kernel (NTK) (Jacot et al., 2018). We further develop a calculus of bounds showing that the sum,
product, ratio, and composition of analytic functions is also learnable, with bounds constructed
using the familiar product and chain rules of univariate calculus (Corollaries 1, 2). These above
learnability results may be of independent interest; for example, they can be used to show that natural
physical laws like the gravitational force equations (shown in Fig. 1) can be efficiently learnt by neural
networks (Section B.1). Furthermore, our bounds imply that the NTK kernel for ReLU activation has
theoretical learning guarantees that are superior to the Gaussian kernel (Section A.2), which we also
demonstrate empirically with experiments on learning the gravitational force law (Section B.2).
1.2	Related work
Most related to our work are a number of works in application areas that have sought to learn a single
network that can perform many different tasks. In natural language processing, Tsai et al. (2019) show
that a single model can solve machine translation across more than 50 languages. Many other works
in NLP similarly seek to use one model for multiple languages, or even multiple tasks (Johnson et al.,
2017; Aharoni et al., 2019; Bapna et al., 2019; Devlin et al., 2018). Monolithic models have also been
successfully trained for tasks in very different domains, such as speech and language (Kaiser et al.,
2017). Finally, there is also work on training extremely large neural networks which have the capacity
to learn multiple tasks (Shazeer et al., 2017; Raffel et al., 2019). These works provide empirical clues
3
Published as a conference paper at ICLR 2021
that suggest that a single network can successfully be trained to perform a wide variety of tasks. But,
they do not provide a systematic theoretical investigation of the extent of this ability as we do here.
Caruana (1997) proposed multitask learning in which a single network is trained to solve multiple
tasks on the same input simultaneously, as a vector of outputs. He observed that average generalization
error for the multiple tasks may be much better than when the tasks are trained separately, and this
observation initiated an active area of machine learning research (Zhang & Yang, 2017). Multitask
learning is obviously related to our monolithic architectures. The difference is that whereas in
multitask learning all of the tasks are computed simultaneously and output on separate gates, here
all of the tasks share a common set of outputs, and the task code inputs switch between the various
tasks. Furthermore, contrary to the main focus of multitask learning, we are primarily interested in
the extent to which different tasks may interfere, rather than how much similar ones may benefit.
Our work is also related to studies of neural models of multitasking in cognitive science. In particular,
Musslick et al. (2017) consider a similar two-layer architecture in which there is a set of task code
attributes. But, as in multitask learning, they are interested in how many of these tasks can be
performed simultaneously, on distinct outputs. They analyze the tradeoff between improved sample
complexity and interference of the tasks with a handcrafted “gating” scheme, in which the parts of
activity are zeroed out depending on the input (as opposed to the usual nonlinearities); in this model,
they find out that the speedup from multitask learning comes at the penalty of limiting the number of
tasks that can be correctly computed as the similarity of inputs varies. Thus, in contrast to our model
where the single model is computing distinct tasks sequentially, they do find that the distinct tasks
can interfere with each other when we seek to solve them simultaneously.
2 Technical Overview
We now give a more detailed overview of our theoretical techniques and results, with informal
statements of our main theorems. For full formal statements and proofs, please see the Appendix.
2.1	Learning Analytic Functions
Our technical starting point is to generalize the analysis of Arora et al. (2019b) in order to show that
two-layer neural networks with standard activation, trained by SGD from random initialization, can
learn analytic functions on the unit sphere. We then obtain our results by demonstrating how our
representations of interest can be captured by analytic functions with power series representations of
appropriately bounded norms. Formal statements and proofs for this section appear in Appendix A.2.
Let Sd denote the unit sphere in d dimensions.
Theorem 1. (Informal) Given an analyticfunction g(y), thefunction g(β ∙ x), for fixed β ∈ Rd (With
β d=f ∣∣β∣∣2) and inputs X ∈ Sd is learnable to error E with n = O((βgf (β) + g(0))2∕e2) examples
using a single-hidden-layer, finite width neural network of width poly(n) trained with SGD, with
∞
g(y) = E |ak |yk
(1)
k=0
where the ak are the power series coefficients of g(y).
We will refer to g0(1) as the norm of the function g——this captures the Rademacher complexity of
learning g, and hence the required sample complexity. We also show that the g function in fact tightly
captures the Rademacher complexity of learning g, i.e. there is a lower bound on the Rademacher
complexity based on the coefficients of g for certain input distributions (see Corollary 5 in Section C
in the appendix).
We also note that we can prove a much more general version for multivariate analytic functions g(x),
with a modified norm function g(y) constructed from the multivariate power series representation
of g(x) (Theorem 8 in Appendix A.2). The theorems can also be extended to develop a “calculus
of bounds” which lets us compute new bounds for functions created via combinations of learnable
functions. In particular, we have a product rule and a chain rule:
4
Published as a conference paper at ICLR 2021
Figure 2: Some of the task codings which fit in our framework. On the left, we show a task coding
via clusters. Here, c(i) is the code for the ith cluster. On the right, we show a task coding based on
low-depth decision trees. Here, ci is the ith coordinate of the code c of the input datapoint.
Corollary 1 (Product rule). Let g(x) and h(x) meet the conditions of Theorem 1. Then the product
g(x)h(x) is efficiently learnable as well, with O(Mg∙h∕e2) samples where
√Mg! = g0(1)h(1) + g(1)h0(1) + g(0)h(0).	(2)
Corollary 2 (Chain rule). Let g(y) be an analytic function and h(x) be efficiently learnable, with
auxiliary functions g(y) and h(y) respectively. Then the composition g(h(x)) is efficiently learnable
as well with O(Mgoh/e2) samples where
√Mg0h = g0(h(1))h0(1) + g(h(0)),	(3)
provided that h(0) and h(1) are in the radius of convergence ofg.
The calculus of bounds enables us to prove learning bounds on increasingly expressive functions, and
we can prove results that may be of independent interest. As an example, we show in Appendix B.1
that forces on k bodies interacting via Newtonian gravitation, as shown in Figure 1, can be learned to
error e using only kO(ln(k/)) examples (even though the function 1/x has a singularity at 0).
2.2	Task coding via clusters
Our analysis of learning analytic functions allows us to prove that a single network with standard
training can learn multiple tasks. We formalize the problem of learning multiple tasks as follows. In
general, these networks take pairs of inputs (c, x) where c is a task code and x is the input (vector)
for the chosen task represented by c. We assume both c and x have fixed dimensionality. These
pairs are then encoded by the concatenation of the two vectors, which we denote by c; x. Given k
tasks, corresponding to evaluation of functions f1, . . . , fk respectively on the input x, the ith task
has a corresponding code c(i). Now, we wish to learn a function g such that g(c(i); x) = fi(x) for
examples of the form (c(i); x, fi(x)). This g is a “monolithic” function combining the k tasks. More
generally, there may be some noise (bounded within a small ball around c(i)) in the task codes which
would require learning the monolithic function g(c, x) = fj (x) where j = argmini kc - c(i) k2 .
Alternately the task-codes are not given explicitly but are inferred by checking which ball-center c(i)
(unique per task) is closest to the input x (see Fig. 2 (left) for an example). Note that these are all
generalizations of a simple one-hot coding.
We assume throughout that the fi are analytic, with bounded-norm multinomial Taylor series rep-
resentations. Our technical tool is the following Lemma (proved in Appendix A.2) which shows
that the univariate step function 1(x ≥ 0) can be approximated with error e and margin γ using a
low-degree polynomial which can be learnt using SGD.
Lemma 1. Given a scalar x, let
Φ(x, γ,e) = (1/2)(1 + erf (Cxplog(1∕e)∕γ))
5
Published as a conference paper at ICLR 2021
where erf is the Gauss error function and C is a constant. Let Φ0 (x, γ, ) be the function Φ(x, γ, )
with its Taylor series truncated at degree O(log(1∕e)/γ). Then,
o，f O O O(E) X ≤ -γ∕2,
φ (XV4- O(E) X ≥ γ∕2.
Also, Φ0(x, γ, E) can be learnt using SGD with at most eo((Iog(I/'"γ2" examples.
Using this lemma, we show that indicator functions for detecting membership in a ball near a
prototype c(i) can also be sufficiently well approximated by functions with such a Taylor series
representation. Specifically, we use the truncated representation of the erf function to indicate that
kc - c(i) k is small. As long as the centers are sufficiently well-separated, we can find a low-degree,
low-norm function this way using Lemma 1. For example, to check if c is within distance r of center
c(i) we can use 1(kc - c(i) k2 ≤ r2), which can be approximated using the φ0 function in Lemma 1.
Then given such approximate representations for the task indicators I1(c), . . . , Ik(c), the function
g(c; x) = I1(c)f1(x) +------+ Ik (c)fk (x) has norm linear in the complexities of the task functions,
so that they are learnable by Theorem 1 (we scale to inputs to lie within the unit ball as required by
Theorem 1). We state the result below, for the formal statement and proof see Appendix A.3.
Theorem 2. (Informal) Given k analytic functions having Taylor series representations with norm
at most poly(k∕E) and degree at most O(log(k∕E)), a two-layer neural network trained with SGD
can learn the following functions g(c; x) on the unit sphere to accuracy E with sample complexity
poly(k∕E) times the sum of the sample complexities for learning each of the individual functions:
• for Ω(1)-separated codes c(1),..., c(k), f ∣∣c 一 c(i)∣∣2 ≤ O ⑴,then g(c; x) = fi(x).
2.3	Task coding via low-depth decision trees
Theorem 2 can be viewed as performing a single k-way branching choice of which task function
to evaluate. Alternatively, we can consider a sequence of such choices, and obtain a decision tree
in which the leaves indicate which task function is to be applied to the input. We first consider
the simple case of a decision tree when c is a {±1}-valued vector. We can check that the values
c1 , . . . , ch match the fixed assignment c(1i) , . . . , c(hi) that reaches a given leaf of the tree using the
c +c(i)
function Ic(i) (c) = Qj=ι % ：j (or similarly for any subset of up to h of the indices). Then
g(c; x) = Ic(i)(c)fι(x) +---+ Ic(k)(c)fk(x) represents our decision tree coding of the tasks (see
Fig. 2 (right) for an example). For the theorem, we again scale the inputs to lie within the unit ball:
Theorem 3.	(Informal) Two-layer neural networks trained with SGD can learn such a decision tree
with depth h within error E with sample complexity O(dh∕E2) times the sum of the sample complexity
for learning each of the individual functions at the leaves. Furthermore, conditioned on the hardness
oflearning parity with noise, dQ(h) examples are in fact necessary to learn a decision tree of depth h.
We can generalize the previous decision tree to allow a threshold based decision at every internal
node, instead of just looking at a coordinate. Assume that the input data lies in the unit ball and
that each decision is based on a margin of at least γ . We can then use a product of our truncated erf
polynomials to represent branches of the tree. We thus show:
Theorem 4.	(Informal) If we have a decision tree of depth h where each decision is based on a
margin of at least γ, then we can learn such a such a function within error E with sample complexity
eO(h Iog(I∕e"γ' times the SamPle complexity oflearning each of the leaffunctions.
For the formal statements and proofs, see Appendix A.4. Note that by Theorem 3, the exponential
dependence on the depth in these theorems is necessary.
2.4 Simple programming constructs
So far, we have discussed jointly learning k functions with task codings represented by clusters
and decision trees. We now move to a more general setup, where we allow simple programming
constructs such as compositions, aggregation, concatenation, and branching on different functions. At
this stage, the distinction between “task codes” and “inputs” becomes somewhat arbitrary. Therefore,
6
Published as a conference paper at ICLR 2021
we will generally drop the task codes c from the inputs. The class of programming constructs we can
learn is a generalization of the decision tree and we refer to it as a generalized decision program.
Definition 1. We define a generalized decision program to be a circuit with fan-out 1 (i.e., a tree
topology). Each gate in the circuit computes a function of the outputs of its children, and the root
(top) node computes the final output. All gates, including the leaf gates, have access to the input x.
We can learn generalized decision programs where each node evaluates one among a large family of
operations, first described informally below, and then followed by a formal definition.
Arithmetic/analytic formulas As discussed in Section 2.1, learnability of analytic functions not
only allows us to learn functions with bounded Taylor series, but also sums, products, and ratios of
such functions. Thus, we can learn constant-depth arithmetic formulas with bounded outputs and
analytic functions (with appropriately bounded Taylor series) applied to such learnable functions.
Aggregation We observe that the sum of k functions with bounded Taylor representations yields a
function of the same degree and norm that is at most k times greater; the average of these k functions,
meanwhile does not increase the magnitude of the norm. Thus, these standard aggregation operations
are represented very efficiently. These enable us to learn functions that answer a family of SQL-style
queries against a fixed database as follows: suppose I(x, r) is an indicator function for whether or
not the record r satisfies the predicate with parameters x. Then a sum of the m entries of a database
that satisfy the predicate given by X is represented by I(x, r(I))r(I) +-+ I(x, r(m))r(m). Thus,
as long as the predicate function I and records r(i) have bounded norms, the function mapping the
parameters X to the result of the query is learnable. We remark that max aggregation can also be
represented as a sum of appropriately scaled threshold indicators, provided that there is a sufficient
gap between the maximum value and other values.
Structured data We note that our networks already receive vectors of inputs and may produce
vectors of outputs. Thus, one may trivially structured inputs and outputs such as those in Fig. 1 (right)
using these vectors. We now formalize this by defining the class of functions we allow.
Definition 2. We support the following operations at any gate in the generalized decision program.
Let every gate have at most k children. Let g be the output of some gate and {f1, . . . , fk} be the
outputs of the children of that gate.
1.	Any analytic function of the child gates which can be approximated by a polynomial of degree at
most p, including sum g = Pik=1 fi and product of p terms g = Πip=1fi.
2.	Margin-based switch (decision) gate with children {f1, f2} and some constant margin γ, i.e.,
g = fι if hβ, Xi- α ≤ — γ∕2, and g = f2 if hβ, Xi — α ≥ γ/2,for a vector β and ConStant a.
3.	Cluster-based switch gate with k centers {c(1), . . . , c(k)}, with separation r (for some constant
r), i.e. the output iS fi if kX — c(i) k ≤ r∕3. A Special caSe of thiS iS a look-up table which returnS
value vi ifX = c(i), and 0 ifX doeS not match any of the centerS.
4.	CompoSition of two functionS, g(X) = f1(f2(X)).
5.	Create a tuple out of Separate fieldS by concatenation: given inputS {f1, . . . , fk} g outputS a
tuple [f1, . . . , fk], which createS a Single data Structure out of the children. Or, extract a field out
ofa tuple: for a fixed field i, given the tuple [f1, . . . , fk], g returnS fi.
6.	For a fixed table T with k entrieS {r1, . . . , rk}, a Boolean-valued function b, and an analytic
function f, SQL querieS of the form SELECT SUM f(r_i), WHERE b(r_i, x) for the
input X, i.e., g computeS	i:b(r ,x)=1 f(ri). (We aSSume that f takeS bounded valueS and b can
be approximated by an analytic function of degree at moSt p.) For an example, See the function
avg_income_zip_code() in Fig. 1 (right).
As an example of a simple program we can support, refer to Fig. 1 (right) which involves table
lookups, decision nodes, analytic functions such as Euclidean distance, and SQL queries. Theorem 5
is our learning guarantee for generalized decision programs. See Section A.5 in the Appendix for
proofs, formal statements, and a detailed description of the program in Fig. 1 (right).
Theorem 5.	(Informal) Any generalized deciSion program of conStant depth h uSing the above
operationS with p ≤ O(log(k∕)) can be learnt within error with Sample complexity kpoly(log(k/)).
For the Specific caSe of the program in Fig. 1 (right), it can be learnt uSing (k∕)O(log(1/)) exampleS,
where k iS the number of individualS in the databaSe.
7
Published as a conference paper at ICLR 2021
>u2⊃uura4js①-L
50
IO1
5 O 5 O 5 Q 5
8 8 7 7 6 6 5
AUeJmUett①h
Number of examples per cluster
(a) Random linear classifier for each cluster.
45 - ...
IO2
IO3	IO4
Number of examples per cluster
(b) Random teacher network for each cluster.
Figure 3: Binary classification on multiple clusters, results are an average over 3 trials. A single neural
network does well even when there are multiple clusters. The error does not increase substantially on
increasing the number of clusters k
3	Experiments
We next empirically explore the learnability of multiple functions by a two layer neural network when
the tasks are coded by well-separated clusters or decision trees, and more generally the learnability
of SQL-style aggregation for a fixed database. We find good agreement between the empirical
performance and the bounds of Section 2. See Appendix D for more details of the experimental setup.
Learning binary classification for well-separated clusters data We demonstrate through ex-
periments on synthetic data that a single neural network can learn multiple tasks if the tasks are
well-separated into clusters, as we discussed in Section 2.2. Here the data is drawn from a mixture
of k well-separated Gaussians in d = 50 dimensions. Within each Gaussian, the data points are
marked with either of two labels. For the label generation, we consider two cases, first when the
labels within each cluster are determined by a simple linear classifier, and second when the labels
are given by a random teacher neural network with one hidden layer of 10 hidden units. Fig. 3
shows the performance of a single two-layer neural network with 50k hidden units on this task. The
performance of the neural network changes only slightly on increasing the number of clusters (k),
suggesting that a single neural network can learn across all clusters.
Learning polynomial functions on leaves of a decision tree We consider the problem of learning
polynomial functions selected by a decision tree. The data generation process is as follows. We
first fix parameters: tree depth h, decision variable threshold margin γ, number of variables k,
and degree p for leaf functions. Then we specify a full binary decision tree of depth h with a
random polynomial function on each leaf. To do this, we first generate thresholds t1 , t2, ..., th
from the uniform distribution on [0, 1] and 2h leaf functions which are homogeneous polynomials
of k variables and degree p, with uniformly distributed random coefficients in [0, 1]. A train/test
example (x, y) where x = (x1, ..., xh, xh+1, ..., xh+p) is generated by first randomly sampling the
xi’s from the uniform distribution on [0, 1], selecting the corresponding leaf based on x1, ..., xh (that
is, go left at the first branch if x1 ≤ t1 , otherwise go right, etc), and computing y by evaluating
the leaf function at (xh+1, ..., xh+p). The data is generated with the guarantee that each leaf has
the same number of data points. Fig. 4 shows the performance of a two-layer neural network with
32 × 2h hidden units, measured in the R-squared metric. Here the R-squared metric is defined as
1 - Pi(yi - yi)2 / Pi(yi - y)2, and is the fraction of the underlying variance explained by the model.
Note that for a model that outputs the mean y for any input, the R-Squared metric would be zero. We
observed for a fixed number of training samples, accuracy increases as threshold margin increases,
and the dependence of sample complexity on test error agrees with the bound in Theorem 4.
Learning SQL-style aggregation queries We demonstrate the learnability of SQL-style
aggregation queries, which are functions of the form SELECT SUM/MIN/MAX f(x)
WHERE p(x) from DATABASE. The train and test datasets are generated from the Penn World
8
Published as a conference paper at ICLR 2021
(a) Fixed threshold margin γ = 0.1.
Figure 4: Learning random homogeneous polynomials of 4 variables and degree 4 on the leaves of a
decision tree, the results are averaged over 7 trials. (a) Sample complexity scales as eO(h IOg(I/'"γ’
with error , where error is measured by (1-Test R-squared). (b) For fixed tree depth, accuracy
increases with increasing margin.
Decision variable threshold margin
(b) Fixed tree depth h = 10.
Table dataset (Feenstra et al., 2015), which contains 11830 rows of economic data. The WHERE
clause takes the form of (xi1 ≥ ti1) AND . . .AND (xik ≥ tik), where xi1 , . . . , xik are k randomly
selected columns and ti1 , . . . , tik are randomly selected values from the columns. The query target
function is randomly selected from SUM, MAX, and MIN and is over a fixed column (pl_x in the
table, which stands for price level for imports). The R-squared metric for a two-layer neural network
with 40k hidden units is summarized in Table 1. We observe that a neural network learns to do
SQL-style aggregation over dozens of data points, and for a fixed database, the test error only varies
slightly for different numbers of columns in the WHERE clause.
Table 1: R-Squared for SQL-style aggregation. A single network with one hidden layer gets high
R-Squared values, and the error does not increase substantially if the complexity of the aggregation is
increased by increasing the number of columns in the WHERE clause.
# columns in WHERE clause	1	2	3	4	5
Median # data points	21	12	9	4	3
Test R-Squared	(93.31 ± 0.11) %	(93.01 ± 2.7)%	(91.86 ± 2.59) %	(94.84 ± 1.86) %	(92.51 ± 2.2) %
4 Conclusion and Future Work
Our results indicate that even using a single neural network, we can still learn tasks across multiple,
diverse domains. However, modular architectures may still have benefits over monolithic ones: they
might use less energy and computation, as only a portion of the total network needs to evaluate
any data point. They may also be more interpretable, as it is clearer what role each part of the
network is performing. It is an open question if any of these benefits of modularity can be extended
to monolothic networks. For instance, is it necessary for a monolithic network to have modular parts
which perform identifiable simple computations? And if so, can we efficiently identify these from the
larger network? This could help in interpreting and understanding large neural networks.
Our work also begins to establish how neural networks can learn functions which are represented as
simple programs. This perspective raises the question, how rich can these programs be? Can we learn
programs from a full-featured language? In particular, supposing that they combine simpler programs
using other basic operations such as composition, can such libraries of tasks be learned as well, i.e.,
can these learned programs be reused? We view this as a compelling direction for future work.
9
Published as a conference paper at ICLR 2021
Acknowledgements
Brendan Juba was partially supported by NSF Awards CCF-1718380, IIS-1908287, and IIS-1939677,
and was visiting Google during a portion of this work. Vatsal Sharan was supported in part by NSF
award 1704417.
References
Roee Aharoni, Melvin Johnson, and Orhan Firat. Massively multilingual neural machine translation.
arXiv preprint arXiv:1903.00089, 2019.
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and Generalization in Overparameterized
Neural Networks, Going Beyond Two Layers. In Advances in Neural Information Processing
Systems 32, pp. 6155-6166. Curran Associates, Inc., 2019.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-Grained Analysis
of Optimization and Generalization for Overparameterized Two-Layer Neural Networks. In
International Conference on Machine Learning, pp. 322-332, May 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584, 2019b.
Ankur Bapna, Colin Andrew Cherry, Dmitry Dima Lepikhin, George Foster, Maxim Krikun, Melvin
Johnson, Mia Chen, Naveen Ari, Orhan Firat, Wolfgang Macherey, et al. Massively multilingual
neural machine translation in the wild: Findings and challenges. 2019.
Avrim Blum, Adam Kalai, and Hal Wasserman. Noise-tolerant learning, the parity problem, and the
statistical query model. Journal of the ACM (JACM), 50(4):506-519, 2003.
Rich Caruana. Multitask learning. Machine learning, 28(1):41-75, 1997.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Simon S. Du, XiyU Zhai, BarnabgS P6czos, and Aarti Singh. Gradient Descent Provably Optimizes
Over-parameterized Neural Networks. In 7th International Conference on Learning Representa-
tions, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.
Robert C Feenstra, Robert Inklaar, and Marcel P Timmer. The next generation of the penn world
table. American economic review, 105(10):3150-82, 2015.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4(2):
251-257, 1991.
Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of
local experts. Neural computation, 3(1):79-87, 1991.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural Tangent Kernel: Convergence and
Generalization in Neural Networks. In Advances in Neural Information Processing Systems 31, pp.
8571-8580. Curran Associates, Inc., 2018.
Melvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil
Thorat, Fernanda Viegas, Martin Wattenberg, Greg Corrado, et al. Google,s multilingual neural
machine translation system: Enabling zero-shot translation. Transactions of the Association for
Computational Linguistics, 5:339-351, 2017.
Lukasz Kaiser, Aidan N Gomez, Noam Shazeer, Ashish Vaswani, Niki Parmar, Llion Jones, and
Jakob Uszkoreit. One model to learn them all. arXiv preprint arXiv:1706.05137, 2017.
Michael Kearns. Efficient noise-tolerant learning from statistical queries. Journal of the ACM (JACM),
45(6):983-1006, 1998.
10
Published as a conference paper at ICLR 2021
Michael Kearns and Leslie Valiant. Cryptographic limitations on learning boolean formulae and
finite automata. Journal ofthe ACM (JACM),41(1):67-95,1994.
Adam R Klivans and Alexander A Sherstov. Cryptographic hardness for learning intersections of
halfspaces. Journal ofComputer and System Sciences, 75(1):2-12, 2009.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide Neural Networks of Any Depth Evolve as Linear Models
Under Gradient Descent. In Advances in Neural Information Processing Systems 32, pp. 8570-8581.
Curran Associates, Inc., 2019.
Sebastian Musslick, Andrew Saxe, Kayhan Ozcimder, Biswadip Dey, Greg Henselman, and
Jonathan D Cohen. Multitasking capability versus learning efficiency in neural network ar-
chitectures. In CogSci, pp. 829-834, 2017.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the Limits of Transfer Learning with a Unified
Text-to-Text Transformer. arXiv:1910.10683 [cs, stat], October 2019.
Oded Regev. On lattices, learning with errors, random linear codes, and cryptography. Journal of the
ACM (JACM), 56(6):1-40, 2009.
Richard Samuels. Is the mind massively modular? 2006.
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and
Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv
preprint arXiv:1701.06538, 2017.
Le Song, Santosh Vempala, John Wilmes, and Bo Xie. On the complexity of learning neural networks.
In Advances in neural information processing SyStemS, pp. 5514—5522, 2017.
Michel Talagrand. Sharper bounds for gaussian and empirical processes. The Annals of Probability,
pp. 28-76, 1994.
Henry Tsai, Jason Riesa, Melvin Johnson, Naveen Arivazhagan, Xin Li, and Amelia Archer. Small
and practical bert models for sequence labeling. arXiv preprint arXiv:1909.00100, 2019.
Gregory Valiant. Finding correlations in subquadratic time, with applications to learning parities and
the closest pair problem. Journal ofthe ACM (JACM), 62(2):1-45, 2015.
Yu Zhang and Qiang Yang. A survey on multi-task learning. arXiv preprint arXiv:1707.08114, 2017.
11
Published as a conference paper at ICLR 2021
A Theoretical Results
A.1 Kernel learning bounds
In this section, we develop the theory of learning analytic functions. For a given function g, we define
a parameter Mg related to the sample complexity of learning g with small error with respect to a
given loss function:
Definition 3. Fix a learning algorithm, and a 1-Lipschitz loss function L. For a function g over a
distribution of inputs D, a given error scale , anda confidence parameter δ, let the sample complexity
ng,D (, δ) be the smallest integer such that when the algorithm is given ng,D (, δ) i.i.d. examples
of g on D, with probability greater than 1 一 δ ,it produces a trained model g with generalization
error Ex〜D[L(g(x), g(x))] less than 匕 Fix a constant C > 0. We say g is efficiently learned by
the algorithm (w.r.t. C) if there exists a constant Mg (depending on g) such that for all , δ, and
distributions D on the inputs of g, n§D 亿 δ) ≤ C ([Mg + log(δ-1)]∕e2).
For example, itis known (Talagrand (1994)) that there exists a suitable choice of C such that empirical
risk minimization for a class of functions efficiently learns those functions with Mg at most the
VC-dimension of that class.
Previous work focused on computing Mg , for functions defined on the unit sphere, for wide neural
networks trained with SGD. We extend the bounds derived in Arora et al. (2019a) to analytic functions,
and show that they apply to kernel learning methods as well as neural networks.
The analysis in Arora et al. (2019a) focused on the case of training the hidden layers of wide networks
with SGD. We first show that these bounds are more general and in particular apply to the case where
only the final layer weights are trained (corresponding to the NNGP kernel in Lee et al. (2019)), and
therefore our results will apply to general kernel learning as well. The proof strategy consists of
showing that finite-width networks have a sensible infinite-width limit, and showing that training
causes only a small change in parameters of the network.
Let m be the number of hidden units, and n be the number of data points. Let y be the n × 1
dimensional vector of training outputs. Let h be a n × m random matrix denoting the activations of
the hidden layer (as a function of the weights of the lower layer) for all n data points. We will first
show the following:
Theorem 6.	For sufficiently large m, a function g can be learned efficiently in the sense of Definition
3 by training the final layer weights only with SGD, where the constant Mg given by
Mg ≤ yT(H∞)-1y	(4)
where we define H∞ as
H∞ = E[hhT]	(5)
which is the NNGP kernel from Lee et al. (2019).
We require some technical lemmas in order to prove the theorem. We first need to show that H∞ is,
with high probability, invertible. If K(x, x0), the kernel function which generates H∞ is given by a
infinite Taylor series in X ∙ x0 it can be argued that H∞ has full rank for most real world distributions.
For example, the ReLU activation this holds as long as no two data points are co-linear (see Definition
5.1 in Arora et al. (2019a)). We can prove this more explicitly in the following lemma:
Lemma 2. If all the n data points X are distinct and the Taylor SerieS of K(x, x0) in X ∙ X has
positive coefficients everywhere then H∞ is not singular.
Proof. First consider the case where the input x is a scalar. Since the Taylor series corresponding
to K(x, x0) consists of monomials of all degrees of xx0, we can view it as some inner product in
a kernel space induced by the function φ(x) = (1, x, x2, . . .), where the inner product is diagonal
(but with potentially different weights) in this basis. For any distinct set of inputs {x1, .., xn} the
set of vectors φ(xi) are linearly independent. The first n columns produce the Vandermonde matrix
obtained by stacking rows 1, x, x, ..., xn-1 for n different values of x, which is well known to be
non-singular (since a zero eigenvector would correspond to a degree n 一 1 polynomial with n distinct
roots {x1, .., xn}).
12
Published as a conference paper at ICLR 2021
This extends to the case of multidimensional x if the values, projected along some dimension, are
distinct. In this case, the kernel space corresponds to the direct sum of copies ofφ applied elementwise
to each coordinate xi . If all the points are distinct and and far apart from each other, the probability
that a given pair coincides under random projection is negligible. From a union bound, the probability
that a given pair coincide is also bounded - so there must be directions such that projections along
that direction are distinct. Therefore, H∞ can be considered to be invertible in general.	□
As m → ∞, hhT concentrates to its expected value. More precisely, (hhT)-1 approaches (H∞)-1
for large m ifwe assume that the smallest eigenvalue λmin(H∞) ≥ λ0, which from the above lemma
we know to be true for fixed n. (For the ReLU NTK the difference becomes negligible with high
probability for m = poly(n∕λo) Arora et al.(2019a).) This allows us to replace hhτ with H∞ in
any bounds involving the former.
We can get learning bounds in terms of hhT by studying the upper layer weights w of the network
after training. After training, We have y = W ∙ h. If hhτ is invertible (which the above arguments
show is true with high probability for large m), the following lemma holds:
Lemma 3. Ifwe initialize a random lower layer and train the weights of the upper layer then there
exists a solution W with norm，yT(hhT)-1y.
Proof. The minimum norm solution to y = WTh is
w* = (hT h)-1hτy.	(6)
The norm squared (w*)tw* of this solution is given by yτh(hτh)-2hτy.
We claim that h(hTh)-2hT = (hhT)-1. To show this, consider the SVD decomposition h =
USVT . Expanding we have
h(hTh)-2hT = USVT(VS2VT)-2VSUT.	(7)
Evaluating the right hand side gets us US-2UT = (hhT)-1.
Therefore, the norm of the minimum norm solution is yτ(hhτ)-1y.	□
We can now complete the proof of Theorem 6.
Proof of Theorem 6. For large m, the squared norm of the weights approaches yT(H∞)-1y. Since
the lower layer is fixed, the optimization problem is linear and therefore convex in the trained weights
w. Therefore SGD with small learning rate will reach this optimal solution. The Rademacher
complexity of this function class is at most ,yτ(H∞)Ty which we at most by y∕Mg where Mg is
an upper bound on yT(H∞)-1y. The optimal solution has 0 train error based on the assumption that
H∞
is full rank and the generalization error will be no more than
O(
yT(H∞)-1y
2n
which is at most
E if we use at least n = Ω(Mg/e2) training samples - note that this is identical to the previous results
for training the hidden layer only Arora et al. (2019a); Du et al. (2019).
)
□
A.2 Learning analytic functions
Now, we derive our generalization bounds for single variate functions. We use Theorem 6 to prove
the following corollary, a more general version of Corollary 6.2 proven in Arora et al. (2019a) for
wide ReLU networks with trainable hidden layer only:
Corollary 3. Consider the function g : Rd → R given by:
g(x) = X ak(βkTx)k	(8)
k
Then, if g is restricted to ||x|| = 1, and the NTK or NNGP kernel can be written as H(x, x0) =
kbk bk(x ∙ x0)k, thefunction can be learned efficiently with a wide one-hidden-layer network in the
sense of Definition 3 with
PMg = X b-1/2|ak∣∣∣βkIlk	(9)
k
13
Published as a conference paper at ICLR 2021
up to g-independent constants of O(1), where βk ≡ ∣∣βk ∣∣2. In the particular case ofa ReLU network,
the bound is	_____
p/Mg = £k|ak|||ek ||k	(IO)
k
The original corollary applied only to networks with trained hidden layer, and the bound on the ReLu
network excluded odd monomials of power greater than 1.
Proof. The extension to NNGP follows from Theorem 6, which allows for the application of the
arguments used to prove Corollary 6.2 from Arora et al. (2019a) (particularly those found in Appendix
E).
The extension of the ReLu bound to odd powers can be acheived with the following modification.
consider appending a constant component to the input X so that the new input to the network is
(x/√2,1 /√2). The kernel then becomes:
口 0、 X ∙ X0 + 1 (	XX ∙ X0 + 1、、	zin
K(x, X ) = 一4∏- I π - arccos I ---------%---- 1 卜	(11)
Re-writing the power series as an expansion around X ∙ x0 = 0, we have terms of all powers. An
asymptotic analysis of the coefficients using known results shows that coefficients bk are asymp-
totically O(k-3/k) - meaning in Equation 10 applies to these kernels, without restriction to even
k.	□
Equation 9 suggests that kernels with slowly decaying (but still convergent) bk will give the best
bounds for learning polynomials. Many popular kernels do not meet this criteria. For example, for
inputs on the sphere of radius r, the Gaussian kernel K(X, X0) = e-||x-x0||2/k can be written as
K(x, x0) = e-r2eχ∙χ0. This has b-1/2 = er2/√k!, which increases rapidly with k. This provides
theoretical justification for the empirically inferior performance of the Gaussian kernel which we will
present in Section B.2.
Guided by this theory, we focus on kernels where bk-1/k ≤ O(k), for all k (or, bk ≥ O(k-k )). The
modified ReLu meets this criterion, as well as hand-crafted kernels of the form
K(X，N) = X k-s(X ∙ X0)k	(12)
with s ∈ (1, 2] is a valid slowly decaying kernel on the sphere. We call these slowly decaying kernels.
We note that by Lemma 3, the results of Corollary 3 apply to networks with output layer training
only, as well as kernel learning (which can be implemented by training wide networks).
Using the extension of Corollary 3 to odd powers, we first show that analytic functions with appropri-
ately bounded norms can be learnt.
Theorem 7.	Let g(y) be a function analytic around 0, with radius of convergence Rg. Define the
auxiliary function g(y) by the power SerieS
∞
g(y) = X |ak |yk	(13)
k=0
where the ak are the power SerieS coefficients of g(y). Then the function g(β ∙ x) ,for some fixed
vector β ∈ Rd with ||X|| = 1 is efficiently learnable in the sense of Definition 3 using a model with a
slowly decaying kernel K with
/Mg = βg0(β) + g(0)	(14)
ifthe norm β ≡ ∣∣β∣∣/ is less than Rg.
Proof. We first note that the radius of convergence of the power series of g(y) is also Rg since g(y)
is analytic. Applying Equation 10, pulling out the 0th order term, and factoring out β , we get
∞
/Mg = |ao| + β X k∣ak∣β k = βg0(β) + g(0)	(15)
k=1
since β < Rg .
□
14
Published as a conference paper at ICLR 2021
The tilde function is the notion of complexity which measures how many samples we need to learn
a given function. Informally, the tilde function makes all coefficients in the Taylor series positive.
The sample complexity is given by the value of the function at 1 (in other words, the L1 norm of the
coefficients in the Taylor series). For a multivariate function g(x), We define its tilde function g(y)
by substituting any inner product term hα, xi by a univariate y. The above theorem can then also be
generalized to multivariate analytic functions:
Lemma 4. Given a collection of P vectors βi in Rd, thefunCtion f (x) = ∏P=ι βi ∙ X is efficiently
learnable with
PMf = P Y βi	(16)
where βi ≡ ∣∣β∕∣2.
Proof. The proof of Corollary 6.2 in Arora et al. (2019a) relied on the folloWing statement: given
positive semi-definite matrices A and B, With A B, We have:
PBA-1PB	B+	(17)
Where + is the Moore-Penrose pseudoinverse, and P is the projection operator.
We can use this result, along With the Taylor expansion of the kernel and a particular decomposition
of a multivariate monomial in the folloWing Way. Let the matrix X to be the training data, such that
the αth column Xi is a unit vector in Rd. Given K ≡ XTX, the matrix of inner products, the Gram
matrix H∞ of the kernel can be Written as
∞
H∞ = X bkK°k	(18)
k=0
Where ◦ is the Hadamard (elementWise) product. Consider the problem of learning the function
f (x) = QP=ι βi ∙ x. Note that We can write:
f (X) = (Xok)t 泌 =1 βi.	(19)
Here 0 is the tensor product, which for vectors takes an nι-dimensional vector and an n dimensional
vector as inputs vectors and returns a n1 n2 dimensional vector:
/ WlVl ∖
w1 v2
• ∙ ∙
W 0 V =	w1Vn2	.	(20)
w2V1
•••
wn1 vn2
The operator is the Khatri-Rao product, which takes an n1 × n3 matrix A = (a1, • • • , an3) and a
n2 0 n3 matrix B = (b1, • • • , bn3 ) and returns the n1n2 × n3 dimensional matrix
A Θ B = (aι 0 bi,…，an3 0 bn3).	(21)
ForP = 2, this form of f(X) can be proved explicitly:
(Xo2)Tβ1 0 β2 = (X1 0 X1, • • • , XP 0 XP)T β1 0 β2.	(22)
The αth element of the matrix product is
(Xα 0 Xα) • (β1 0 β2) = (β1 • Xα)(β2 • Xα)	(23)
which is exactly f(Xα). The formula can be proved forP > 2 by finite induction.
With this form of f (X), we can follow the steps of the proof in Appendix E of Arora et al. (2019a),
which was written for the case where the βi were identical:
yT(H∞)-1y = (0ip=1βi)TXop(H∞)-1(Xop)T 0ip=1 βi.	(24)
15
Published as a conference paper at ICLR 2021
Using Equation 17, applied to K°p, We have:
yT(H∞)-1y ≤	(25)
b-1H=ιβi)TX°pPκoP(K°p)+PK。P(X0p)T 砥 =1 βi
Since the X0p are eigenvectors of Pk。P with eigenvalue 1, and X0p(K°p)+ (Xop)T = Pχ0p, We
have:
yτ(H∞)-1y ≤ b-1(0P=iei)TPx&p 唠 =1 βi	(26)
p
yτ(H∞)-1y ≤ b-1∏ βi∙ βi.	(27)
For the slowly decaying kernels, bp ≥ p-2. Therefore, we have ∙∖∕yτ(H∞)-1y ≤ JMf for
PMf = P Y βi	(28)
where βi ≡ ∣∣βi∣∣2, as desired.	口
This leads to the following generalization of Theorem 7:
Theorem 8.	Let g(x) be a function with multivariate power series representation:
k
g(x) = XX av Y(βv,i∙ x)	(29)
k v∈Vk	i=1
where the elements of Vk index the kth order terms ofthe power series. We define g(y) = Ek Gkyk
with coefficients
k
aGk = X |av| Y βv,i.	(30)
v∈Vk	i=1
If the power series of gG(y) converges at y = 1 then with high probability g(x) can be learned
efficiently in the sense of Definition 3 with /Mg = g0(1) + g(0).
Proof. Follow the construction in Theorem 7, using Lemma 4 to get bounds on the individual terms.
Then sum and evaluate the power series of g0 ⑴ to arrive at the bound.	口
Remark 1. Note that the gG function defined above for multivariate functions depends on the repre-
sentation, i.e. choice of the vectors β. Therefore to be fully formal gG(y) should instead be gGβ (y). For
clarity, we drop β from the expression gGβ (y) and it is implicit in the gG notation.
Remark 2. If g(x) can be approximated by some function gapp such that |g(x) - gapp | ≤ 0 for all x
in the unit ball, then Theorem 8 can be used to learn g(x) within error 0 + with sample complexity
O(Mgapp/2).
To verify Remark 2, note that we are doing regression on the upper layer of the neural network,
where the lower layer is random. So based on gapp there exists a low-norm solution for the regression
coefficients for the upper layer weights which gets error at most 0. Ifwe solve the regression under
the appropriate norm ball, then we get training error at most 0 , and the generalization error will be at
most with O(Mgapp /2) samples.
We can also derive the equivalent of the product and chain rule for function composition.
Proof of Corollary 1. Consider the power series of g(x)h(x), which exists and is convergent since
each individual series exists and is convergent. Let the elements of Vj,g and Vk,h index the jth order
terms of g and the kth order terms of h respectively. The individual terms in the series look like:
jk
avbw	ɪɪ	(βv,j0	∙ x) ∏ (βw,k0	∙	x)	for V ∈	Vj,g, W ∈	Vk,h	(31)
j0=1	k0=1
16
Published as a conference paper at ICLR 2021
with bound
jk
(j+k)|av||bw|	βv,j0	βw,k0 for v ∈ Vj,g, w ∈ Vk,h	(32)
j0=1	k0=1
1'	11 .	∙ .1 ∙ . 7 . ∕∖	1 ~∕C∖7 ∕C∖ Γ∙ .1 .	∙ .) ∙	7	Γ∖
for all terms With j + k > 0 and g(0)h(0) for the term With j = k = 0.
Distribute the j + k product, and first focus on the j term only. Summing over all the Vk,h for all k,
We get
jk
∑w∑hj|av||bw|	βv,j0	βw,k0
j
|av| ∏βv,j,h(1).
j0=1
(33)
Now summing over the j and Vj,g we get g0(1)h(1). If we do the same for the k term, after summing
we get g(1)h0(1). These bounds add and we get the desired formula for y∕Mgh, which, up to the
additional g(0)h(0) term looks is the product rule applied to g and h.	□
One immediate application for this corollary is the product of many univariate analytic functions. If
we define
G(X) = ∏gi(βi∙ x)	(34)
i
where each of the corresponding gi(y) have the appropriate convergence properties, then G is
efficiently learnable with bound MG given by
d Y gi(βiy)
+ ∏gi(0).
y=1	i
(35)
Proof of Corollary 2. Writing out g(h(x)) as a power series in h(x), we have:
∞
g(h(x)) = X ak(h(x))k.	(36)
k=0
We can bound each term individually, and use the k-wise product rule to bound each term of (h(x))k.
Doing this, we have:
∞∞
PMg◦h = Xk∣ak∣h0(1)h(1)k-1 + X |ak∣h(0)k.	(37)
k=1	k=0
Factoring out h0(1) from the first term and then evaluating each of the series gets us the desired
result.	□
The following corollary considers the case where the function g(x) is low-degree and directly follows
from Theorem 8.
Fact 1. The following facts about the tilde function will be useful in our analysis—
1.	Given a multivariate analytic function g(x) of degree p for x in the d-dimensional unit ball,
there is afunCtion g(y) as defined in Theorem 8 such that g(x) is learnable to error E with
O(pg(1)∕c2) samples.
2.	The tilde of a sum of two functions is at most the sum of the tilde of each of the functions, i.e.
if f = g + h then f(y) ≤ g(y) + h(y) for y ≥ 0.
3.	The tilde of a product of two functions is at most the product of the tilde of each of the
functions, ι.e. if f = g ∙ h then f (y) ≤ g(y)h(y) for y ≥ 0.
17
Published as a conference paper at ICLR 2021
.,. -， . ..,. ≈, . _
4.	If g(x) = f(αx), then g(y) ≤ f(αy) for y ≥ 0.
5.	If g(x) = f (x + C) for some ∣∣ck ≤ 1, then g(y) ≤ f (y + 1) for y ≥ 0. By combining this
with the previous fact, f g(x) = f (α(x - C)) for some ∣∣c∣ ≤ 1, then g(1) ≤ f (2ɑ).
To verify the last part, note that in the definition of g We replace hβ, x with y. Therefore, We will
have an additional hβ, Ci term when we compute the tilde function for g(x) = f(x+ C). As ∣C∣ ≤ 1,
the additional term is at most 1.
The following lemma shows how we can approximate the indicator 1(x > α) with a low-degree
polynomial if X is at least γ∕2 far away from a. We will use this primitive several times to construct
low-degree analytic approximations of indicator functions. The result is based on the following
simple fact.
Fact 2. If the Taylor series of g(x) is exponentially decreasing, then we can truncate it at degree
O(log(1/)) to get error. We will use this fact to construct low-degree approximations of functions.
Lemma 5. Given a scalar x, let the function
Φ(x, γ,e,α) = (1/2)(1 + erf ((X - α)cPlog(1∕e)∕γ))
for some constant c. Let Φ0(x, γ, , α) be the function Φ(x, γ, , α) with its Taylor series truncated at
degree O(log(1∕e)∕γ). Thenfor |a| < 1,
Φ0(X, γ, , α) = 1 -
X ≤ α - γ∕2,
X ≥ α + γ∕2.
Also, Mφo is at most eo((Iog(I/'"γ2".
Proof. Note that Φ(X, γ, , α) is the cumulative distribution function (cdf) of a normal distribution
with mean α and standard deviation O(γ∕/log(1∕c)). Note that at most e/100 of the probability
mass of a Gaussian distribution lies more than O(,lοg(1∕c)) standard deviations away from the
mean. Therefore,
∕100	X ≤ α - γ∕2,
Φ(X,γ,,α)= 1 - ∕100 X≥α+γ∕2,.
Note that
erf(X)
ɪ广
√π Jo
e-t2 dt
g(-1)ix2i+1
巩2"1)
Therefore, the coefficients in the Taylor series expansion of erf((x - a)c/lοg(1∕c)∕γ)) in terms
of (X - α) are smaller than for i > O(log(1∕)∕γ2) and are geometrically decreasing hence-
forth. Therefore, we can truncate the Taylor series at degree O(log(1∕)∕γ2) and still have an O()
approximation. Note that for f(X) = erf(X),
~,.
f(y) ≤
I= yV et2 dt ≤
π Jo	-
≤ eO(y2 )
After shifting by a and scaling by O(Plog(1∕c)∕γ), we get Φ0(y) = eO((y+α)2 Iog(I/"γ2). For
x = 1, this is at most eO(log(1∕e)∕γ2). Hence the result now follows by Fact 1.
□
A.3 Learnability of cluster based decision node
In the informal version of the result for learning cluster based decisions we assumed that the task-codes
C are prefixed to the input datapoints, which we refer to as xinp. For the formal version of the theorem,
we use a small variation. The task code and the input c, XinP gets mapped to X = C + XinP ∙ (r∕3) for
18
Published as a conference paper at ICLR 2021
some constant r < 1/6. Since xinp resides on the unit sphere, x will be distance at most (r/3) from
the center it gets mapped to. Note that the overall function f can be written as follows,
k
f (X) = X 1 (kχ -Cjk2 ≤ (r/2产)fj((X -Cj)/(r/3))
j=1
where fj is the function corresponding to the center Cj . The main idea will be to show that the
indicator function can be expressed as an analytic function.
Theorem 9. (formal version of Theorem 2) Assume that d ≥ 10 log k (otherwise we can pad by extra
coordinates to increase the dimensionality). Then we can find k centers in the unit ball which are at
least r apart, for some constant r. Let
k
f (X) = X 1 (kχ -Cj Il2 ≤(r/2产)fj((X -Cj)/(r/3))
j=1
where fj is the function corresponding to the center Cj. Then if each fj is a degree p polynomial,
Mf Ofthefunction f is p ∙ poly(k/e) P fj(6/r) ≤ P ∙ poly(k/e)(6/r)p P fj (1).
Proof. Let
k
fapp(X) = X φ0 (IlX -Cj ∣∣2, (r/2)2 Wh (r/4)2) fj ((x -Cj )/(r/3))
j=1
where Φ0 is defined in Lemma 5. Let
Ij(X) = Φ0(IX - CjI2, (r/2)2, /k, (r/4)2).
The indicator Ij (X) checks if IX - Cj I is a constant fraction less than r/2, or a constant fraction more
than r/2. Note that if X is from a different cluster, then IX - Cj I is at least some constant, and hence
Ij (X) is at most /k. The contribution from k such clusters would be at most . If IX - Cj I < /k,
then the indicator is at least 1 - O(/k). Hence as fapp is an O()-approximation to f, by Remark 2
it suffices to show learnability of fapp.
If y = hX, Cj i and assuming X and the centers Cj are all on unit sphere,
Ij(y) = Φ0(2 + 2y, r/3, e/k, r/3) ≤ eO(log(k/e) = poly(k/e).
By Fact 1,
f(y) ≤ poly(k∕e) Efj(6/r).
j
As fj are at most degree p,
f(y) ≤ Poly(k∕e) X fj(6/r) ≤ p ∙poly(k∕e)(6∕r)p X f⑴.
j
□
Corollary 4. The previous theorem implies that we can also learn f where f is a lookup table with
Mf = poly(k/), as long as the keys ci are well separated. Note that as long as the keys ci are
distinct (for example, names) we can hash them to random vectors on a sphere so that they are all
well-separated.
Note that the indicator function for the informal version of Theorem 9 stated in the main body is
the same as that for the lookup table in Corollary 4. Therefore, the informal version of Theorem 9
follows as a Corollary of Theorem 9.
19
Published as a conference paper at ICLR 2021
A.4 Learnability of functions defined on leaves of a decision tree
We consider decision trees on inputs drawn from {-1, 1}d. We show that such a decision tree g can
be learnt with Mg ≤ O(dh). From this section onwards, we view the combined input c, x as x.
The decision tree g can be written as follows,
g(x) =	Ij(x)vj,
j
where the summation runs over all the leaves, Ij(x) is the indicator function for leaf j, and vj ∈
[-1,1] is the constant value on the leaf j. We scale the inputs by √d to make them lie on the unit
sphere, and hence each coordinate of X is either ±1/ √d.
Let the total number of leaves in the decision tree be B . The decision tree indicator function of the
j -th leaf can be written as the product over the path of all internal decision nodes. Let jl be variable
at the l-th decision node on the path used by the j-th leaf. We can write,
Ij (X) =	(ajl xjl + bjl) ,
l
where each Xjl ∈ {-1∕√d, 1∕√d} and a九 ∈ {-√d∕2, √d∕2} and %匕 ∈ {-1/2,1/2}. Note that
the values of ajl and bj,l are chosen depending on whether the path for the j-th leaf choses the left
child or the right child at the l-th decision variable. For ease of exposition, the following theorem is
stated for the case where the leaf functions are constant functions, and the case where there are some
analytic functions at the leaves also follows in the same way.
Theorem 10. If a function is given by g(X) = PjB=1 Ij (X)vj, where Ij (X) is a leaf indicator function
in the above form, with tree depth h, then Mg is at most O(dh).
Proof. Note that
g(y) ≤ X Ij ⑻ IvjI
≤ XY(√dy∕2 + 1∕2)
l
=⇒ 贸1) ≤ 2h(√d∕2 + 1∕2)h ≤ dh.
As the degree of g is at most h, therefore Mg ≤ hg(1) ≤ hdh.	□
Remark 3. Note that by Theorem 10 we need O (log k)log k-2 samples to learn a lookup table
based on a decision tree. On the other hand, by Corollary 4 we need poly(k∕) samples to learn a
lookup table using cluster based decision nodes. This shows that using a hash function to obtain a
random O(log k) bit encoding of the indexes for the k lookups is more efficient than using a fixed
log k length encoding for the k lookups.
We also prove a corresponding lower bound in Theorem 14 which shows that dQ(h) samples are
necessary to learn decision trees of depth h.
We will now consider decision trees where the branching is based on the inner product of X with
some direction βj,l. Assume that there is a constant gap for each decision split, then the decision tree
indicator function can be written as,
Ij(X) = Y 1(hX, βj,l i > αj,l).
l
Theorem 11. (formal version of Theorem 4) A decision tree of depth h where every node partitions
in a certain direction with margin γ can be written as g(X) = PjB=1 Ij (X)fj (X), then the final
Mg = eO(hIog(I/"γ%p + hlog1∕e) X f (1),
where p is the maximum degree of fj.
20
Published as a conference paper at ICLR 2021
Proof. Define gapp,
B
gapp(x) =	ΠlΦ0(hx, βj,li, γ, /h, αj,l)fj (x)
j=1
where Φ0 is as defined in Lemma 5. Note that for all y = 1,
Φ0(1,γ,e∕h,αj,ι) ≤ e。(IOg(I/"γ2L
Therefore,
B
gapp(1) ≤ X∏ιΦp,Y,e∕h,αj,ι)fj⑴，
j=1
≤ eO(log(1/e)/72) X fj ⑴.
Note that the degree of gapp is at most O(P + h log(1∕e)∕γ2). Therefore,
MgaBP ≤ eO(hIOg(I/"γ%p + hlog(1∕"γ2) X f ⑴.
By Remark 2, learnability of g follows from the learnability of its analytic approximation gapp.	□
A.5 Generalized Decision Program
In this section, instead a decision tree, we will consider a circuit with fan-out 1, where each gate
(node) evaluates some function of the values returned by its children and the input x. A decision tree
is a special case of such circuits in which the gates are all switches.
So far, the function outputs were univariate but we will now generalize and allow multivariate (vector)
outputs as well. Hence the functions can now evaluate and return data structures, represented by
vectors. We assume that each output is at most d dimensional and lies in the unit ball.
Definition 4. For a multivariate output function f, we define f (y) as the sum of fi (y) for each of
the output coordinates fi.
Remark 4. Theorem 9 , 10 and 11 extend to the multivariate output case. Note that if each of the
individual functions has degree at most p, then the sample complexity for learning the multivariate
output f is at most O(pf (1)∕2)) (where the multivariate tilde function is defined in Definition 4).
We now define a generalized decision program and the class of functions that we support.
Definition 5. We define a generalized decision program to be a circuit with fan-out 1 (i.e., a tree
topology) where each gate evaluates a function of the values returned by its children and the input x,
and the root node evaluates the final output. All gates, including those at the leaves, have access to
the input x. We support the following gate operations. Let h be the output of a gate, let each gate
have at most k children, and let {f1, . . . , fk} be the outputs of its children.
1.	Any analytic function of the child gates of degree at most p, including sum h = Pik=1 fi
and product of p terms h = Πip=1 fi.
2.	Margin based switch (decision) gate with children {f1, f2}, some constant margin γ, vector
β and constant α,
h= f1	if hβ, xi - α ≤ -γ∕2,
f2	if hβ, xi - α ≥ γ∕2.
3.	Cluster based switch gate with k centers {c(1), . . . , c(k)}, with separation r for some
constant r, and the output is fi if kx - c(i) k ≤ r∕3. A special case of this is a look-up table
which returns value vi if x = c(i), and 0 if x does not match any of the centers.
4.	Create a data structure out of separate fields by concatenation such as constructing a tuple
[f1, . . . , fk] which creates a single data structure out of its children, or extract a field out of
a data structure.
21
Published as a conference paper at ICLR 2021
(1(xi > j) > 1/2) ,
5.	Given a table T with k entries {r1, . . . , rk}, a Boolean-valued function p and an analytic
function f, SQL queries of the form SELECT SUM f(r_i), WHERE p(r_i, x).
Here, we assume that f has bounded value and p can be approximated by an analytic
function of degree at most p.
6.	Compositions of functions, h(x) = f(g(x)).
First, we note that all of the above operators can be approximated by low-degree polynomials.
Claim 1. Ifp≤ O(log(k/)), each of the above operators in the generalized decision program can
be expressed as a polynomial of degree at most O(log(k/)), where k is maximum out-degree of any
of the nodes.
Remark 5. Note that for the SQL query, we can also approximate other aggregation operators apart
from SUM, such as MAX or MIN. For example, to approximate MAX of x1, . . . , xk up to where the
input lies between [0, 1] we can first write it as
MAX(x1 , . . . , xk) =	1
j
and then approximate the indicators by analytic functions.
Lemma 6 shows how we can compute the tilde function of the generalized decision program.
Lemma 6. The tilde function for a generalized decision program can be computed recursively with
the following steps:
1.	For a SUm gate h = f + g, h(y) = f (y) + g(y).
2.	For a product gate, h = f.g, h(y) = f(y) ∙ g(y).
3.	For a margin based decision gate (switch) with children f and g, h = Ileftf + (1 - Ilef t)g
and h(y) = Ileft(f (y) + g(y)) + g(y). Here Ileft is the indicator for the case where the
left child is chosen.
4.	For cluster based decision gate (switch) with children {f1, ..., fk}, h(y) ≤	i Iifi(6y/r).
Here Ii is the indicator for the cluster corresponding to the i-th child.
5.	Fora look-up table with k key-values, h(y) ≤ kI(y) as long as the `1 norm of each key-value
is at most 1.
6.	Creating a data structure out of separate fields can be done by concatenation, and h for the
result is at most sum of the original tilde functions. Extracting a field out of a data structure
can also be done in the same way.
7.	Given an analytic function f and a Boolean function p, for a SQL
operator h over a table T with k entries {r1 , . . . , rk} representing
SELECT SUM f(r_i), WHERE p(r_i, x), or in other words h =
Pi f (ri)p(ri,x), h(y) ≤ P/p,% (y), where Ip,∏ is the indicator for p(ri,x). For
example, x here can denote some threshold value to be applied to a column of the table, or
selecting some subset of entries (in Fig. 1, x is the zip-code).
8.	For h(x) = f (g(x)), h(y) ≤ f(g(y)).
All except for the last part of the above Lemma directly follow from the results in the previous
sub-section. Below, we prove the result for the last part regarding function compositions.
Lemma 7. Assume that all functions have input and output dimension at most d. If f and g are
two functions with degree at most p1 and p2, then h(x) = f (g(x)) has degree at most p1p2 and
~ , ~ , ..
h(y) ≤ f(g(y)).
Proof. Note that this follows if f and g are both scalar outputs and inputs. Let g(x) =
(g1(x), ..., gd(x)). Let us begin with the case where f = hβ, xi, where kβk = 1. Then
22
Published as a conference paper at ICLR 2021
h(y) = Pi lβi∣gi(y) ≤ Pi gi(y) ≤ g(y). When f = Πp= ιhβi, xi, h(y) ≤ g(y)p1 ≤ f(g(y)).
The same argument works when we take a linear combination, and also for a multivariate function f
(as f for a multivariate f is the summation of individual fi, by definition).	口
We now present our result for learning generalized decision programs.
Theorem 12. Let the in-degree of any gate be at most k. The sample complexity for learning the
following classes of generalized decision programs is as follows:
1.	If every gate is either a decision node with margin γ, a sum gate, or a lookup of size at most
k ,then Mg ≤ eO(h b虱1/。/Ybk。⑺.
2.	For some constant C, if there are at most C product gates with degree at most C, and every
other gate is a decision gate with margin γ or a sum gate with constant functions at the
leaves, then Mg ≤ eO(h Iog(I/"γ’.
3.	Given a function f and a Boolean function p which can be approximated by a polynomial of
degree at most O(log(k/)), fora SQL operator g over a table T with k entries {r1, . . . , rk}
representing SELECT SUM f(r_i), WHERE p(r_i, x), Mg ≤ Pi Ip,r<1).
4.	Let the function at every gate be an analytic function f of degree at most p and the sum of the
coefficients of f is upper bounded by cp for some constant c. Then note that f(y) ≤ (cy)p
for y ≥ 1. Therefore, the final function g(y) ≤ (Cky)P and hence Mg ≤ (ck)p .
Proof. The first three claims can be obtained using Lemma 6.
For the final claim, consider the final polynomial obtained by expanding the function at each gate
in a bottom-up way. We will upper bound g(y) for the overall function g corresponding to the
generalized decision program. g(y) can be upper bounded by starting with f (y) for the leaf nodes f.
For any internal gate i, let gi(x) = fi(fj1 (x), . . . , fjp (x)) where fjt are the outputs of the children
of the gate i. We recursively compute gi(y) = fi(£ι fjι (y)). Therefore, for a gate with k children
gi(y) ≤ (C Pl gjι(y))p. Therefore, for the root gate go, go(y) ≤ (Ckyyh.	口
Remark 6. Note that the dependence on h is doubly exponential. We show a corresponding lower
bound in Theorem 15 that this is necessary.
Theorem 12 implies that we can learn programs such as the following formal version of Fig. 1
(right)—which involves analytic functions, SQL queries, data structures, and table look-up.
Example 1. Consider the following program:
class Person{
string name;
Address address;
int income;
public string get_zip_code(){
return address.zip_code;
}
init(input_name, input_address, input_income){
name = input_name;
address = input_address;
income = input_income;
}
}
class Address{
int street_number;
string street_name;
string city;
string state;
string zip_code;
23
Published as a conference paper at ICLR 2021
public string get_zip_code(){
return zip_code;
}
init(...){
... # function to create new object with input values
}
}
dictionary name_to_address_table;
dictionary zip_code_to_lat_long; #maps zip_code to tuple of (latitute, longitude)
boolean in_same_zip_code(Person A, Person B){
return A.get_zip_code() == B.get_zip_code();
}
float get_straight_line_distance(Person A, Person B){
lat_longA = zip_code_to_lat_long[A.get_zip_code()];
lat_longB = zip_code_to_lat_long[B.get_zip_code()];
return euclidean_distance(lat_longA, lat_longB);
}
float avg_income_zip_code(string zip_code){
construct SQL table T with income, zip_code from name_to_address_table;
return output of SQL query "SELECT AVG(INCOME) FROM T WHERE ZIP_CODE=zip_code"
}
The following claim follows from Theorem 12.
Claim 2. The above classes and functions can be implemented and learnt using (k/)O(log(1/))
samples, where the tables are of size at most k.
Proof. We begin with the in_same_zip_code() function. Note that this is a special case of the
cluster based functions. As in Corollary 4 all attributes such as zip-code are appropriately hashed
such that they are well-separated. We can now test equality by doing an indicator function for a
ball around the zip-code of Person A. The indicator function for a ball can be approximated by a
low-degree polynomial as in the cluster-based branching results in Theorem 9. As the total number
of individuals is at most k, therefore by Theorem 9 the sample complexity is at most poly(k/).
For the avg_income_zip_code() function, we use the SQL query result in Theorem 12. Note
that the indicators are testing equality in the case of our program, and hence as in the previous
case we can use the cluster-based branching result in Theorem 9 to approximate these indicators by
polynomial functions, to obtain a sample complexity of poly(k/).
Finally, we argue that we can learn the get_straight_line_distance() function. Here,
we are composing two functions f and (g1, g2) where f is the distance function and (g1, g2) are
the lookups for the latitude and longitude for Person A and B. By Corollary 4, the lookups have
gi (1) ≤ poly(k∕e). By part 6 of Lemma 6, the tilde for the concatenation is the sum of the tilde for
the individual functions. For computing the Euclidean distance ,P(χi - yi)2, note that the square
root function does not have a Taylor series defined at 0. However, we can use the same analysis as in
the proof for learning the 1/x function in the gravitational law (see Appendix B.1) to get a polynomial
of degree at most O(log(1∕e)), and hence f(y) ≤ (O(y))log(1/e). Thus using the composition rule
in Lemma 6, the sample complexity is (k/)O(log(1/)).
□
24
Published as a conference paper at ICLR 2021
B Learning dynamical systems
B.1 Gravitational force law
We can use the product and chain rules to show that many functions important in scientific applications
can be efficiently learnable. This is true even when the function has a singularity. As an example
demonstrating both, we prove the following bound on learning Newton’s law of gravitation:
Theorem 13. Consider a system of k bodies with positions xi ∈ R3 and masses mi, interacting via
the force:
Fi = X m3mj (Xj-Xi)	(38)
j6=i rij
where rij ≡ ||Xi - Xj ||. We assume that R = rmax/rmin, the ratio between the largest and smallest
pairwise distance between any two bodies, is constant. Suppose the mi have been rescaled to be
between 0 and 1. Then the force law is efficiently learnable in the sense of Definition 3 using the
modified ReLU kernel to generalization error less than using kO(ln(k/)) samples.
Proof. We will prove learning bounds for each component of F separately, showing efficient learning
with probability greater than 1 一 δ∕3k. Then, using the union bound, the probability of simultaneously
learning all the components efficiently will be 1 - δ.
There are two levels of approximation: first, we will construct a function which is within /2 of the
original force law, but more learnable. Secondly, we will prove bounds on learning that function to
within error /2.
We first rescale the vector of collective {Xi} so that their collective length is at most 1. In these new
units, this gives Us *0方 ≤ 2. The first component of the force on xι can be written as:
(F1)1
(Xjk -(XI)I)
r1j
(39)
If we find a bound VZMf for an individual contribution f to the force, we can get a bound on the total
√Mf = (k 一 1) JMf. Consider an individual force term in the sum. The force has a singularity at
r1j = 0. In addition, the function r1j itself is non-analytic due to the branch cut at 0.
We instead will approximate the force law with a finite power series in r12j , and get bounds on
learning said power series. The power series representation of (1 一 x)-3/2 is Pn∞=0 (2(；+)1，!! χn. If
we approximate the function with d terms, the error can be bounded using Taylor’s theorem. The
Lagrange form of the error gives us the bound
1	χX (2n + 1)!! n / √∏d∣x∣d+1
(1 一 x)3∕2 一 N (2n)!! X ≤ (1 一 ∣χ∣)5∕2+d
(40)
where we use (2n+)1J! ≈ √πn for large n. We can use the above expansion by rewriting
(41)
for some shift a. Approximation with fd(r12j), the first d terms of the power series in (1 一 r12j/a2)
gives us the error:
∣fd%f3∣≤ .3:|1一黑"
(42)
which we want to be small over the range rmin ≤ r1j ≤ rmax.
The bound is optimized when it takes the same value at rmin and rmax, so we set a2 = (rm2 in +
rm2 ax)/2. In the limit that rmax	rmin , where learning is most difficult, the bound becomes
Ifd(r2j) τ-3∣ ≤ √38πd(R2/2)5/2+d e-2(d+1)/R2
rmax
(43)
25
Published as a conference paper at ICLR 2021
where R = rmax/rmin, which is constant by assumption.
In order to estimate an individual contribution to the force force to error /2k (so the total error is
/2), we must have:
mιmjrmax∣fd(rij) - r-31 ≤ 不	(44)
This allows us to choose the smallest d which gives us this error. Taking the logarithm of both sides,
we have:
1 ln(d) - (5/2 + d)ln(2/R2) - 2(d + 1)/R2 ≤ ln(e/k2).	(45)
where we use that rm2 ax ≤ 2/k after rescaling. The choice d ≥ R2 ln(k2/) ensures error less than
/2k per term.
Using this approximation, we can use the product and chain rules to get learning bounds on the force
law. We can write the approximation
F(x) =	m1mjfd(hj(x))kj(x)
j6=1
(46)
where hj(x) = ||x1 - xj || and kj(x) = (x1)1 - (xj)j The number of samples needed for efficient
learning is bounded by PMF = "√8k AFe, for
rmax
AFe= fd (h(1))h,(1)k(1)+ fd(h(1))k, (1)
with	〜〜〜
Ky) = √2y, h(y) = 6y2, fd(y) = √πd(1 + y/a2)d.
Evaluating, we have
AFe = √2∏d (1 + ɪ )d + √∏d3 (1 + ɪ )d-1
rmax	rmax
which, after using rm2 ax ≤ 2/k and d = R2 ln(k2 /) gives us the bound
PMFe ≤ k-1/2 (R2 ln(k2/e))3/2 (24k)R2 ln(fc2/e)
The asymptotic behavior is
since R is bounded.
(47)
(48)
(49)
(50)
(51)
PMFe=k0°Mk∕e))
We can therefore learn an /2-approximation of one component of F1, with probability at least
1 - δ/3k and error e/2 with O(4(MFe + log(3k/6))/c2) samples. Therefore, we can learn Fi to
error with the same number of samples. Using a union bound, with probability at least 1 - δ we can
simultaneously learn all components of all {Fi} with that number of samples.	□
We note that since the cutoff of the power series at d() = O(R2 ln(k2 /)) dominates the bound,
we can easily compute learning bounds for other power-series kernels as well. If the dth power
series coefficient of the kernel is bd, then the bound on VZMFe is increased by (d(€)2bd(e))-1/2. For
example, for the Gaussian kernel, since b-1/2 = √d!, the bound becomes
PMFe =(R2 ln(k2/e)k)O(ln(k/e))
(52)
which increases the exponent of k by a factor of ln(R2 ln(k2 /)).
B.2 Empirical confirmation of learning bounds
We empirically validated our analytical learning bounds by training models to learn the gravitational
force function for k bodies (with k ranging from 5 to 400) in a 3-dimensional space. We created
synthetic datasets by randomly drawing k points from [0, 1]3 corresponding to the location of k
bodies, and compute the gravitational force (according to Figure 1) on a target body also drawn
randomly from [0, 1]3. To avoid singularities, we ensured a minimum distance of 0.1 between the
target body and the other bodies (corresponding to the choice R = 10). As predicted by the theory,
26
Published as a conference paper at ICLR 2021
none of the models learn well if R is not fixed. We randomly drew the masses corresponding to
the k + 1 bodies from [0, 10]. We generated 5 million such examples - each example with 4(k + 1)
features corresponding to the location and mass of each of the bodies, and a single label corresponding
to the gravitational force F on the target body along the x-axis. We held out 10% of the dataset as
test data to compute the root mean square error (RMSE) in prediction. We trained three different
neural networks on this data, corresponding to various kernels we analyzed in the previous section:
1.	A wide one hidden-layer ReLU network (corresponding to the ReLU NTK kernel).
2.	A wide one hidden-layer ReLU network with a constant bias feature added to the input
(corresponding to the NTK kernel).
3.	A wide one hidden-layer network with exponential activation function, where only the top
layer of the network is trained (corresponding to the Gaussian kernel).
We used a hidden layer of width 1000 for all the networks, as we observed that increasing the network
width further did not improve results significantly. All the hidden layer weights were initialized
randomly.
In Figure 5 we show the normalized RMSE (RMSE/[Fmax - Fmin]) for each of the neural networks
for different values of the number of bodies k.
Generalization error vs. k
山SWB pφz=BlwON
0	50 100 150 200 250 300 350 400
k (number of bodies)
Figure 5: RMSE vs number of bodies k for learning gravitational force law for different kernels.
Normalized by the range Fmax - Fmin of the forces. Gaussian kernels learn worse than ReLU at
large k.
All three networks are able to learn the gravitational force equation with small normalized RMSE
for hundreds of bodies. Both the ReLU network and ReLU with bias outperform the network
corresponding to the Gaussian kernel (in terms of RMSE) as k increases. In particular, the Gaussian
kernel learning seems to quickly degrade at around 400 bodies, with a normalized RMSE exceeding
50%. This is consistent with the learning bounds for these kernels in Section A.2, and suggests that
those bounds may in fact be useful to compare the performances of different networks in practice.
We did not, however, observe much difference in the performance of the ReLU network when adding
a bias to the input, which suggests that the inability to get an analytical bound due to only even powers
in the ReLU NTK kernel might be a shortcoming of the proof technique, rather than a property which
fundamentally limits the model.
C Lower Bounds
First, we show an exponential dependence on the depth h is necessary for learning decision trees.
The result depends on the hardness of solving parity with noise.
Conjecture 1. (hardness of parity with noise) Let a, x ∈ {0, 1}d be d-dimensional Boolean vectors.
In the parity with noise problem, we are given noisy inner products modulo 2 of the unknown vector
x with the examples ai, i.e. bi = hai , xi + ηi mod 2 where ηi is a Binomial random variable which
〜6小
is 1 with probability 0.1. Then any algorithm for finding X needs at least 20(d) time or examples
27
Published as a conference paper at ICLR 2021
(where Ω hides poly-loganthmιcfactors in d). Similarly, f X is given to be S-sparse for S《d, then
any algorithmforfinding X needs at least dQ(s) time or examples.
Note that the hardness of learning parity with noise is a standard assumption in computational learning
theory and forms the basis of many cryptographic protocols (Regev, 2009). The best known algorithm
for solving parity needs 2O(d/ log d) time and examples (Blum et al., 2003). Learning parities is
also known to Provably require 2ω5) samples for the class of algorithm known as statistical query
algorithms—these are algorithms are only allowed to obtain estimates of statistical properties of the
examples but cannot see the examples themselves (Kearns, 1998). Note that the usual stochastic
algorithms for training neural networks such as SGD can be implemented in the statistical query
model (Song et al., 2017). Similar hardness result are conjectured for the problem of learning sparse
parity with noise, and the best known algorithm runs in time Qω(S) (Valiant, 2015).
Based on the hardness of parity with noise, we show that exponential dependence on the depth for
learning decision trees is necessary.
Theorem 14.	Conditioned on the hardness of the sparse parity with noise problem, any algorithm
for learning decision trees of depth h needs at least dQ(h) time or examples.
Proof. Note that we can represent a parity with noise problem where the answer is h-sparse by a
decision tree of depth h where the leaves represent the solutions to the parity problem. The result
then follows by the hardness of the sparse parity with noise problem.	□
We also show that the doubly exponential dependence on the depth for learning generalized decision
programs is necessary.
Theorem 15.	Learning a generalized decision program which is a binary tree of depth h using
stochastic gradient descent requires at least 22ςu'hh examples. Conditioned on the hardness of
learning noisy parities, any algorithm for learning a generalized program of depth h needs at least
22c(h)time or examples (where Ω hides poly-logarithmicfactors in h).
Proof. Note that a generalized decision program of depth h can encode a parity function over D = 2h
bits. Any statistical query algorithm to learn a parity over D bits needs at least 2Q(D) samples. As
stochastic gradient descent can be implemented in the statistical query model, hence the bound for
stochastic gradient descent follows.
To prove the general lower bound, note that a generalized decision program of depth h can also
encode a noisy parity function over D = 2hbits. Conditioned on the hardness of parity with noise,
^Γ>f
any algorithm for learning noisy parities needs at least 2Q(D) samples. Hence the bound for general
algorithms also follows.	□
In our framework, we assume that all the underlying functions that we learn are analytic, or have
an analytic approximation. It is natural to ask if such an assumption is necessary. Next, we show
that learning even simple compositions of functions such as their sum is not possible without some
assumptions on the individual functions.
Lemma 8. There exists function classes F1 and F2 which can be learnt efficiently but for every
f1 ∈ F1 there exists f2 ∈ F2 such that f1 + f2 is hard to learn (conditioned on the hardness of
learning parity with noise)
Proof. Both f1 and f2 are modifications of the parity with noise problem. The input in both cases is
X ∈ {0, 1}d . Let β be the solution to the noisy parity problem. The output for the function class F1
is [β, y], where y is the value of the noisy parity for the input. The output for the function class F2 is
[-β, y], where y is again the value of the noisy parity for the input. Note that F1 and F2 are trivial to
learn, as the solution β to the noisy parity problem is already a part of the output. For any f1 ∈ F1,
choose f2 ∈ F2 to be the function with the same vector β . Note that conditioned on the hardness of
learning parity with noise, fι + f2 is hard to learn.	□
28
Published as a conference paper at ICLR 2021
C.1 Lower bounds for learning any analytic function
In this section, We show that there is a lower bound on the Rademacher complexity yTHTy based
on the coefficients in the polynomial expansion of the g function. Hence the g function characterizes
the complexity of learning g.
For any J = (J1, . . . , Jn) ∈ Nn, write a monomial XJ = x1J1 . . . xJnn. Define |J| = Pk Jk. For a
polynomial p(x) = PJ aJxJ, where aJ ∈ C, its degree deg(p) = maxaJ 6=0 |J|. The following fact
shows that monomials form an orthogonal basis over the unit circle in the complex plane.
Fact 3. hXj, XJ，,Cn = 1 if J = J0 and 0 otherwise (here, 〈., )cn denotes the inner product over
the unit circle in the complex plane).
Note that according to Theorem 7 the sample complexity for learning g(x) depends on g0(1)=
Pj j |aj |, and hence is the `1 norm of the derivative. The following Lemma shows that this is tight in
the sense that Ω(Pj jaj) samples or the '2 norm of the derivative are necessary for learning g(χ).
For any variable X let X denote the complex conjugate of x. Let xι, x2,..., Xn denote the training
examples. Let Q denote the kernel polynomial so that K(xi, Xj) = Q(XiTXj). Let Q(t) = Pi qiti.
For simplicity, let us look at the case where the power series and the kernel polynomial are univariate
polynomials of a bounded degree deg(q). We will assume that we have enough samples that Fact 3
hold when averaging over all samples. Let qJ be the coefficient of TJ in the polynomial expansion of
Q(t1 + …+ tn)∙
Lemma 9. For a univariate polynomial y = p(x) , yTH-1y = Ej aj/qj∙ asymptotically in the
sample size, where aj are the coefficients of the polynomial p. For a multivariate polynomial,
yTH-1y = Ej aJ/qj asymptotically in the sample size. Here, H-1 denotes the PSeUdOinverSe of
H.
Proof. We will begin with the univariate case. Let {(X1, y1), (X2, y2, . . . , (Xn, yn)} denote the
training examples and their labels. Let y be the vector of all the labels {yi}. Let d =
max{deg(p), deg(q)} (where we assume that deg(q) is bounded for simplicity). Now consider
the matrix G with n rows and d columns where the (i,j)-th entry is xj. Note that GT trans-
forms y from the standard basis to the monomial basis, i.e. the expected value of (1∕n)GTy
is (aι,...,ad) (by Fact 3). Therefore, (1∕n)GTy = (aι,...,ad) asymptotically in the sample
size n. We claim that H = GDGT where D is the diagonal matrix where Dk,k = qk. To
verify this, let G(i)denote that i-th row of G and observe that the (i,j)-th entry G(i)DGj)=
Pk Xkqk Xjk = qk (XiXj )k = K (Xi, Xj) = Hi,j. Now given the orthonormality of the monomial
basis, (1∕n)GTG = I. Therefore since H = GDGT is the SVD of H, HT = (1∕n2)GDTGT.
Hence yT HTy = ((1∕n)GT y)T DT((I∕n)GT y) = Pj (1∕qj∙ )aj.
For the multivariate case, instead of having d columns for G, we will have one column for every
possible value of J of degree at most d. In the diagonal entry DJ,J we put qJ, where qJ is the
coefficient of TJ in the polynomial expansion of Q(tι + + tn).	□
Corollary 5. For the ReLU activation qj∙ = Ω(1∕j), and hence yTH-1y ≥ Ω(Ej∙ jaj) asymptoti-
cally in the sample size.
Note that in Theorem 7, the upper bound for the sample complexity was O(Pj j|aj|), hence
Theorem 7 is tight up to the distinction between the '1 and '2 norm (which can differ by at most
VZdeg(p)).
D	Additional Details for Experiments
D. 1 Setup details
All the experiments are done in TensorFlow, trained with a GPU accelerator. We use the default
TensorFlow values for all hyper parameters involved in the training of the neural networks. All the
experiment results averaged over 3 runs. The number of training epochs for each experiment and
29
Published as a conference paper at ICLR 2021
(a) An instance of the problem with multiple (b) Test accuracy vs. number of points per
clusters, each cluster is indicated by a red cluster
circle.
Figure 6: Experiment where data is clustered into tasks with a separate linear function for each task.
A single neural network does well even when there are multiple clusters.
average runtime (for one run) are summarized in Table 2. For cluster experiments, number of training
examples per cluster varies 1000 to 100000, average runtime varies from 2 minutes to 100 minutes.
For the decision tree experiments, number of training examples per leaf node varies from 64 to 512,
avarage runtime varies from 14 minutes to 42 minutes. For the SQL-style aggregation experiment, the
train dataset contains 16384 examples, and test dataset contains 4096 examples, average runtime is
50 minutes. The source for the Penn World Table dataset Feenstra et al. (2015) used in the SQL query
experiment is https://www.rug.nl/ggdc/productivity/pwt/ and it is also available
at https://www.kaggle.com/jboysen/penn- world- table.
Table 2: Number of epochs and average runtime
Experiment name	Number of epochs	Average runtime
Cluster	100	2 - 100 minutes
Decision Tree	200	14 - 42 minutes
SQL-style aggregation	6400	50 minutes
D.2 Additional details for learning clusters of linear functions
We provide a more detailed setup of the experiment reported in Fig. 3a where the task codes are
given by clusters, and there is a separate linear function for every cluster. In this experiment, the data
is drawn from k clusters, and from a mixture of two well-separated Gaussians in each cluster. Data
points from the two Gaussians within each cluster are assigned two different labels, for 2k labels
in total. Fig. 6a below shows an instance of this task in two dimensions, the red circles represent
the clusters, and there are two classes drawn from well-separated Gaussians from each cluster. In
high dimensions, the clusters are very well-separated, and doing a k-means clustering to identify
the k cluster centers and then learning a simple linear classifier within each cluster gets near perfect
classification accuracy. Fig. 6b shows the performance of a single neural network trained on this
task (same as Fig. 3a in the main body). We can see that a single neural network still gets good
performance with a modest increase in the required number of samples.
30