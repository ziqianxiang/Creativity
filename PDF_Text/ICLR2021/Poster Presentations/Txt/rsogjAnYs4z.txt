Published as a conference paper at ICLR 2021
Understanding the Effects of Data Parallel-
ism and Sparsity on Neural Network Training
Namhoon Lee
University of Oxford
namhoon@robots.ox.ac.uk
Thalaiyasingam Ajanthan
Australian National University
thalaiyasingama.ajanthan@anu.edu.au
Philip H. S. Torr
University of Oxford
phst@robots.ox.ac.uk
Martin Jaggi
EPFL
martin.jaggi@epfl.ch
Ab stract
We study two factors in neural network training: data parallelism and sparsity;
here, data parallelism means processing training data in parallel using distributed
systems (or equivalently increasing batch size), so that training can be accelerated;
for sparsity, we refer to pruning parameters in a neural network model, so as to re-
duce computational and memory cost. Despite their promising benefits, however,
understanding of their effects on neural network training remains elusive. In this
work, we first measure these effects rigorously by conducting extensive experi-
ments while tuning all metaparameters involved in the optimization. As a result,
we find across various workloads of data set, network model, and optimization
algorithm that there exists a general scaling trend between batch size and number
of training steps to convergence for the effect of data parallelism, and further, dif-
ficulty of training under sparsity. Then, we develop a theoretical analysis based
on the convergence properties of stochastic gradient methods and smoothness of
the optimization landscape, which illustrates the observed phenomena precisely
and generally, establishing a better account of the effects of data parallelism and
sparsity on neural network training.
1	Introduction
Data parallelism is a straightforward and common approach to accelerate neural network training by
processing training data in parallel using distributed systems. Being model-agnostic, it is applicable
to training any neural networks, and the degree of parallelism equates to the size of mini-batch for
synchronized settings, in contrast to other forms of parallelism such as task or model parallelism.
While its utility has attracted much attention in recent years, however, distributing and updating
large network models at distributed communication rounds still remains a bottleneck (Dean et al.,
2012; Hoffer et al., 2017; Goyal et al., 2017; Smith et al., 2018; Shallue et al., 2019; Lin et al., 2020).
Meanwhile, diverse approaches to compress such large network models have been developed, and
network pruning - the SParsification process that zeros out many parameters of a network to reduce
computations and memory associated with these zero values - has been widely employed (Reed,
1993; Han et al., 2015). In fact, recent studies discovered that pruning can be done at initialization
prior to training (Lee et al., 2019; Wang et al., 2020), and by separating the training process from
pruning entirely, it not only saves tremendous time and effort in finding trainable sparse networks,
but also facilitates the analysis of pruned sparse networks in isolation. Nevertheless, there has been
little study concerning the subsequent training of these sparse networks, and various aspects of the
optimization of sparse networks remain rather unknown as of yet.
In this work, we focus on studying data parallelism and sparsity1, and provide clear explanations for
their effects on neural network training. Despite a surge of recent interest in their complementary
1For the purpose of this work, we equate data parallelism and sparsity to increasing batch size and pruning
model parameters, respectively; we explain these more in detail in Appendix E.
1
Published as a conference paper at ICLR 2021
benefits in modern deep learning, there is a lack of fundamental understanding of their effects.
For example, Shallue et al. (2019) provide comprehensive yet empirical evaluations on the effect
of data parallelism, while Zhang et al. (2019) use a simple noisy quadratic model to describe the
effect; for sparsity, Lee et al. (2020) approach the difficulty of training under sparsity solely from
the perspective of initialization.
In this regard, we first accurately measure their effects by performing extensive metaparameter
search independently for each and every study case of batch size and sparsity level. As a result,
we find a general scaling trend as the effect of data parallelism in training sparse neural networks,
across varying sparsity levels and workloads of data set, model and optimization algorithm. Also,
the critical batch size turns out to be no less with sparse networks, despite the general difficulty of
training sparse networks. We formalize our observation and theoretically prove the effect of data
parallelism based on the convergence properties of generalized stochastic gradient methods irre-
spective of sparsity levels. We take this result further to understand the effect of sparsity based
on Lipschitz smoothness analysis, and find that pruning results in a sparse network whose gradient
changes relatively too quickly. Notably, this result is developed under standard assumptions used
in the optimization literature and generally applied to training using any stochastic gradient method
with nonconvex objective and learning rate schedule. Being precise and general, our results could
help understand the effects of data parallelism and sparsity on neural network training.
2	Setup
We follow closely the experiment settings used in Shallue et al. (2019). We describe more details
including the scale of our experiments in Appendix B, and provide additional results in Appendix D.
The code can be found here: https://github.com/namhoonlee/effect-dps-public
Experiment protocol. For a given workload (data set, network model, optimization algorithm)
and study (batch size, sparsity level) setting, we measure the number of training steps required to
reach a predefined goal error. We repeat this process for a budget of runs while searching for the
best metaparameters involved in the optimization (e.g., learning rate, momentum), so as to record
the lowest number of steps, namely steps-to-result, as our primary quantity of interest. To this end,
we regularly evaluate intermediate models on the entire validation set for each training run.
Workload and study. We consider the workloads as the combinations of the followings: (data
set) MNIST, Fashion-MNIST, CIFAR-10; (network model) Simple-CNN, ResNet-8; (optimization
algorithm) SGD, Momentum, Nesterov with either a fixed or decaying learning rate schedule. For
the study setting, we consider a batch size from 2 up to 16384 and a sparsity level from 0% to 90%.
Metaparameter search. We perform a quasi-random search to tune metaparameters efficiently.
More precisely, we first generate Sobol low-discrepancy sequences in a unit hypercube and convert
them into metaparameters of interest, while taking into account a predefined search space for each
metaparameter. The generated values for each metaparameter is in length of the budget of trials, and
the search space is designed based on preliminary experimental results.
Pruning. Sparse networks can be obtained by many different ways, and yet, for the purpose of this
work, they must not undergo any training beforehand so as to measure the effects of data parallelism
while training from scratch. Recent pruning-at-initialization approaches satisfy this requirement,
and we adopt the connection sensitivity criterion in Lee et al. (2019) to obtain sparse networks.
3	Experimental results
3.1	Measuring the effect of data parallelism
First of all, we observe in each and every sparsity level across different workloads a general scaling
trend in the relationship between batch size and steps-to-result for the effects of data parallelism (see
the 1st and 2nd columns in Figure 1): Initially, we observe a period of linear scaling where doubling
the batch size reduces the steps to achieve the goal error by half (i.e., it aligns closely with the dashed
line), followed by a region of diminishing returns where the reduction in the required number of
steps by increasing the batch size is less than the inverse proportional amount (i.e., it starts to digress
from the linear scaling region), which eventually arrives at a maximal data parallelism (i.e., it hits a
2
Published as a conference paper at ICLR 2021
(a) MNIST, Simple-CNN, SGD
(b) MNIST, Simple-CNN, Momentum
All sparsity (normalized)
(c) Fashion-MNIST, Simple-CNN, Momentum
(d) CIFAR-10, ResNet-8, Nesterov
Figure 1: Effects of data parallelism and sparsity on neural network training for various workloads
with a fixed (a-c) or decaying learning rate (d). Across all workloads and sparsity levels, the same
scaling pattern is observed for the relationship between batch size and steps-to-result: it starts with
the initial phase of linear scaling, followed by the region of diminishing returns, and eventually
reaches to maximal data parallelism. Also, the effect of data parallelism in training sparse networks
is no worse than that of the dense counterpart, despite the general difficulty of training the former.
When training using a momentum based SGD, the breakdown of the linear scaling regime often
occurs much later at larger batch sizes for a network with higher sparsity. For example, in the
case of workload {MNIST, Simple-CNN, Momentum}, the critical batch size for the sparsity 90%
network is around 211 whereas it is 29 for the sparsity 0% network (see the 4th column in row (b)).
This potentially indicates that one can exploit large batch sizes more effectively when training sparse
networks than densely parameterized networks. We supply more results in Appendix D.
plateau) where increasing the batch size no longer decreases the steps to reach a goal error. The same
trend is observed across various workloads of data set, network model, and optimization algorithm
as well as different goal errors (see Appendix D). We note that our observation is consistent with the
results of regular network training presented in Shallue et al. (2019); Zhang et al. (2019).
3
Published as a conference paper at ICLR 2021
Figure 2: Comparing different optimization algorithms for the effects of data parallelism and spar-
sity on the workload {MNIST, Simple-CNN, SGD/Momentum/Nesterov} and study {batch size
(2-16384), sparsity levels (0, 50, 70, 90%)} settings; there is no normalization or averaging. Across
all sparsity levels, momentum optimizers (i.e., Momentum, Nesterov) record lower steps-to-result
in a large batch regime and have much bigger critical batch sizes than SGD without momentum.
Identifying such patterns is crucial especially when training in resource constrained environments,
as practitioners can potentially benefit from reducing the training time by deciding a critical batch
size properly, while utilizing resources more effectively.
When we put the results for all sparsity levels together, we observe that training a sparser network
takes a longer time; a data parallelism curve for higher sparsity usually lies above than that for
lower sparsity (see the 3rd column in Figure 1). For example, compared to the case of sparsity
0% (i.e., the dense, over-parameterized network), 90% sparse network takes about 2 - 4 times
longer training time (or the number of training steps), consistently across different batch sizes (see
Figure 12 in Appendix D.1 for more precise comparisons). Recall that we tune all metaparameters
independently for each and every study case of batch size and sparsity level without relying on a
single predefined training rule in order to find the best steps-to-result. Therefore, this result on the
one hand corroborates the general difficulty of training sparse neural networks against the ease of
training overly parameterized neural networks.
On the other hand, when we normalize the y-axis of each plot by dividing by the number of steps
at the first batch size, we can see the phase transitions more clearly. As a result, we find that the
regions of diminishing returns and maximal data parallelism appear no earlier when training sparse
networks than the dense network (see the 4th column in Figure 1). This is quite surprising in that
one could have easily guessed that the general optimization difficulty incurred by sparsification may
influence the data parallelism too, at least to some degree; however, it turns out that the effects of
data parallelism on sparse network training remain no worse than the dense case. Moreover, notice
that in many cases the breakdown of linear scaling regime occurs even much later at larger batch
sizes for a higher sparsity case; this is especially evident for Momentum and Nesterov optimizers
(e.g., compare training 90% sparse network using Momentum against 0% dense network). In other
words, for sparse networks, a critical batch size can be larger, and hence, when it comes to training
sparse neural networks, one can increase the batch size (or design a parallel computing system for
distributed optimization) more effectively, while better exploiting given resources. We find this
result particularly promising since SGD with momentum is often the method of choice in practice.
We further show that momentum optimizers being capable of exploiting large batch sizes hold the
same across different sparsity levels by displaying all plots together in Figure 2. Overall, we believe
that it is important to confirm the robustness of the data parallelism in sparse neural network training,
which has been unknown thus far and difficult to estimate a priori.
3.2	Analyzing metaparameter search
In this section, we analyze the metaparameter search used to measure the effect of data parallelism.
We specifically investigate the workload {MNIST, Simple-CNN, Momentum} where there are two
metaparameters to tune (i.e., learning rate and momentum), to visualize all metaparameters easily
in a 2D figure (see Appendix D.2 for other results). The results are presented in Figure 3, and we
summarize our key findings below:
• Our quasi-random search samples metaparameters efficiently, so that they are distributed evenly
(without being cluttered in a log-space) and flexibly (rather than sitting in a grid with fixed spac-
4
Published as a conference paper at ICLR 2021
S: 0%, B: 1024	,n0 S: 0%, B: 8192
EnlU3E0≡二
S: 0%, B: 16
10-5 IOT IQ-3 IQ-2 IoT IOO ItJ1
Learning rate
♦ S: 90%, B: 16
EnlU3E0≡二
S: 0%, B: 128
)-5 lQ-*> IQ-3 IQ-2 IOT ItJ0 ItJ1
Learning rate
S: 90%, B 128
EnlU3E0≡二
)-5 lQ-*> IQ-3 IQ-2 IOT ItJ0 ItJ1
Learning rate
S: 90%, B 1024
EnlU3E0≡二
)-5 IOT IQ-3 IQ-2 IoT IOO ItJ1
Learning rate
S: 90%, B: 8192
EnIU3E0≡二
10-5 IOT IQ-3 IQ-2 IoT IOO ItJ1
Learning rate
EnIU3E0≡二
)-5 lQ-*> IQ-3 IQ-2 IOT ItJ0 ItJ1
Learning rate
EnIU3E0≡二
)-5 IQ-O IQ-3 IQ-2 IOT ItJ0 ItJ1
Learning rate
EnIU3E0≡二
)-5 IOT 1Q-3 IQ-2 IoT 100 IO1
Learning rate
Figure 3: Metaparameter search results (100 samples in total) for Simple-CNN on MNIST trained
using Momentum optimizer. Sparsity level (S) and batch size (B) are denoted at the top of each
plot. The best trial that records the lowest steps to reach the goal (i.e., steps-to-result) is marked by
gold star (?). Complete/incomplete refer to the trials of goal reached/not reached given a maximum
training step budget, while infeasible refers to the trial of divergence during training.
ing) within the search spaces. Also, the best metaparameters to yield lowest steps (marked by gold
star ?) are located in the middle of the search ranges rather than sitting at the search boundaries
across different batch sizes and sparsity levels. This means that our experiments are designed
reasonably well, and the results are reliable.
• There are two distinguished regions (i.e., complete () and incomplete (N)) being separated by
a seemingly linear boundary as per the relationship between learning rate and momentum. This
indicates that the optimization is being done by an interplay between these two metaparameters; if
one metaparameter is not chosen carefully with respect to the other (e.g., increase learning rate for
fixed momentum), the optimizer may be stuck in a region spending time oscillating and eventually
results in incomplete runs. This highlights the importance of performing metaparameter search,
although itis expensive, rather than relying on predetermined heuristic training strategies, in order
to accurately measure the effect of data parallelism and avoid potentially suboptimal results.
• The successful region (filled with blue circles ∙) becomes larger as with increasing batch size,
showing that large batch training reaches a given goal error in less number of training iterations
than small batch training, and hence, yields more complete runs. Notably, the best learning rate
tends to increase as with increasing batch size too. This aligns well with the classic result in
learning theory that large batch training allows using bigger learning rates (Robbins & Monro,
1951; Bottou, 1998; Krizhevsky, 2014).
4	Understanding the effects of data parallelism and sparsity
So far, we have focused on measuring the effects of data parallelism and sparsity on neural network
training, and as a result found two distinctive global phenomena across various workloads: scaling
trend between batch size and steps-to-result, and training difficulty under sparsity. While our find-
ings align well with previous observations (Shallue et al., 2019; Zhang et al., 2019; Lee et al., 2020),
it remains unclear as to why it occurs, and whether it will generalize. To this end, we establish
theoretical results that precisely account for such phenomena based on convergence properties of
generalized stochastic gradient methods in this section.
4.1	Convergence analysis for the general effects of data parallelism
Let us begin with reviewing the convergence properties of stochastic gradient methods as the choice
of numerical algorithms for solving optimization problems. Consider a generic optimization prob-
lem where the objective is to minimize empirical risk with the objective function f : Rm → R, a
5
Published as a conference paper at ICLR 2021
prediction function h : Rdx × Rm → Rdy , and a loss function l : Rdy × Rdy → R which yields
the loss l(h(x; w), y) given an input-output pair (x, y), where w ∈ Rm is the parameters of the
prediction model h, and dx and dy denote the dimensions of input x and output y, respectively. A
generalized stochastic gradient method to solve this problem can be of the following form:
wk+1 := wk - ηkg(wk, ξk) ,
(1)
where ηk is a scalar learning rate, g(wk, ξk) ∈ Rm is a stochastic vector (e.g., unbiased estimate of
the gradient Vf) with ξk denoting a random variable to realize data samples, either a single sample
as in the prototypical stochastic gradient method (Robbins & Monro, 1951) or a set of samples as in
the mini-batch version (Bottou, 1991). Given an initial iterate w1, it finds a solution by performing
the above update iteratively until convergence.
Under the assumptions2 of Lipschitz smoothness of f and bounded variance of g, the convergence
rate result states that for such generic problem with nonconvex objective and optimization method
with a fixed3 learning rate ηk = n for all k satisfying 0 < n ≤ LM^, the expected average squared
norm of gradients of the objective function is guaranteed to satisfy the following inequality for all
K ∈ N (Bottou et al., 2018):
E
1K
K x
k=1
kVf(Wk)k2 ≤ ηLM + 2(f(WI)- f∞)
Kμη
(2)
μ
Here, f(w1), f∞, Vf(wk) refer to the objective function’s value at w1, lower bound, gradient at
Wk, respectively. Also, L is the Lipschitz constant of Vf, and μ, M, MG denote scalar bounds in
the assumption on the second moment of g(wk, ξk). Note here that M is linked to batch size B as
M a 1/B. In addition, if g(wk, ξk) is an unbiased estimate of Vf (Wk), which is the case for ξk
being i.i.d. samples as in our experiments, then simply μ = 1 (Bottou et al., 2018). In essence,
this result shows that the average squared gradient norm on the left-hand side is bounded above by
asymptotically decreasing quantity as per K, indicating a sublinear convergence rate of the method.
We note further that the convergence rate for the mini-batch stochastic optimization of nonconvex
loss functions is studied previously (Ghadimi et al., 2016; Wang & Srebro, 2017), and yet, here we
reconsider it to analyze the effects of data parallelism.
We now reformulate this result, such that it is translated into a form that matches our experiment set-
tings and reveals the relationship between batch size and steps-to-result. We start by recognizing that
the quantity on the left-hand side, the expected average squared norm of Vf(Wk) during the first K
iterations, indicates the degree of convergence; for example, it gets smaller as training proceeds
with increasing K. Thus, this quantity is directly related to a goal error to reach in our experiments,
which is set to be fixed across different batch sizes for a given workload. This effectively means that
training has stopped, and K will no longer contribute to decrease the bound of the quantity. Also,
recall that We select the optimal learning rate η?, out of extensive metaparameter search, to record
the lowest number of steps to reach the given goal error, i.e., steps-to-result K?. Next, notice that
the only factors that constitute the inequality in Eq. (2) are the Lipschitz constant L and the variance
bound M, and if they are assumed to be tight in the worst case, the inequality becomes tight. Now
we are ready to provide the relationship between batch size (B) and steps-to-result (K?) as follows:
Proposition 4.1. Let ε = E[ K? PK=I ∣∣Vf (Wk )|图 denote a degree of convergence achieved after
the first K? iterations and η? denote the optimal learning rate used to yield the lowest number of
steps K? to reach ε. Then,
? c1	∆Lβ
K? ≈ B + c2,	where ci = μ^ and c2
∆
η*με '
(3)
where ∆ = 2(f (w1) -f∞), β is the initial variance bound at batch size B = 1 for a given workload.
Proof. This result is obtained by recasting Eq. (2) as outlined above. The proof is in Appendix A.
□
2(i) f is differentiable and satisfies ∣∣Vf (w) — ▽/(w)∣∣2 ≤ LkW - W∣∣2, ∀{w, W} ⊂ Rm, and (ii) there
exist scalars M ≥ 0, MG ≥ μ2 > 0 such that Eξk [∣g(wk, ξk)∣∣2] ≤ M + MGkVf(Wk)∣∣2∙
3We also consider the general decaying learning rate case and prove the same result in Appendix A.2.
6
Published as a conference paper at ICLR 2021
SimpIe-CNN
z-lPSdrl
O 5	10 15 20 25 30 35 40
Steps (×103)
----Sparsity： 0%
Sparsity： 50%
——Sparsity： 70%
——Sparsity： 90%
ResN et-8
10
z-lPSdrl
0	20	40	60	80	100
Steps (×103)
Figure 4: LiPschitz constant of Vf measured locally over the course of training for networks with
different sparsity levels. The more a network is pruned, the higher the Lipschitz constant becomes;
e.g., for 0, 50, 70, 90% sParsity levels, the average LiPschitz constants are 0.57, 0.72, 0.81, 1.76
for SimPle-CNN and 3.87, 8.74, 9.54, 11.18 for ResNet-8, resPectively. This indicates that Pruning
results in a network whose gradients are less smooth during training. We further Provide additional
training logs and exPlain how smoothness is measured in APPendix C.
This result Precisely illustrates the relationshiP between batch size and stePs-to-result. For examPle,
when B is small, K? ≈ B, fitting the linear scaling regime (e.g., B → 2B makes K? → (1/2)K?),
whereas when B is large and asymPtotically dominates the right-hand side, K? ≈ c2, indicating the
maximal data Parallelism as K? remains constant. In general, for moderate batch sizes, scaling
B → 2rB results in K? → 击K? + (1 一 强)c2 (rather than /K?), indicating diminishing returns.
Moreover, we Prove the same relationshiP between B and K? (with different constant terms) for the
general decaying learning rate case (see APPendix A.2). Therefore, this result not only well accounts
for the scaling trend observed in the exPeriments, but also describes it more Precisely and generally.
Notably, the effect of data Parallelism, which has only been addressed emPirically and thus remained
as debatable, is now theoretically verified and aPPlicable to general nonconvex objectives. We will
further relate our result to sParse networks via smoothness analysis in Section 4.2.
4.2	Lipschitz smoothness for the difficulty of training sparse networks
Another distinct Phenomenon observed in our exPeriments is that the number of stePs required to
reach the same goal error for sParse networks is consistently higher than that for dense networks re-
gardless of batch size (i.e., a whole data Parallelism curve shifts uPwards when introducing sParsity).
This indicates the general difficulty of training sParse neural networks, and that sParsity degrades
the training sPeed. In this section, we investigate what may cause this difficulty, and find a Potential
source of the Problem by insPecting our theory of the effect of data Parallelism to this end.
Let us begin with our result for the effect of data Parallelism in ProPosition 4.1. Notice that it is
the coefficient ci (= ∆Lβ∕μ2ε2) that can shift a whole data parallelism curve vertically, by the
same factor across different batch sizes. Taking a closer look, we realize that it is the LiPschitz
constant L that can vary quite significantly by introducing sparsity and hence affect ci; ε and μ are
fixed, and ∆ and β can change by sparsity in a relatively minor manner (we explain this in detail in
Appendix C). Specifically, L refers to the bound on the rate of change in Vf and is by definition
a function of f . Also, sparsity introduced by pruning changes the prediction function h which is
linked to f via the loss function l. Therefore, we posit that a sparse neural network obtained by
pruning will be less smooth (with a higher Lipschitz constant) than the non-pruned dense network.
To verify our hypothesis, we empirically measure the Lipschitz constant for networks with different
sparsity levels over the course of the entire training process. The results are presented in Figure 4.
As we can see, it turns out that the Lipschitz constant increases as with increasing sparsity level, and
further, is consistently higher for sparse networks than for the dense network throughout training.
This means that pruning results in sparse networks whose gradient changes relatively too quickly
compared to the dense network; in other words, the prediction function h becomes less smooth after
pruning. This is potentially what hinders training progress, and as a result, sparse networks require
more time (i.e., steps-to-result) to reach the same goal error.
7
Published as a conference paper at ICLR 2021
Learning rate	Learning rate	Learning rate	Learning rate	Learning rate
Figure 5: Metaparameter search results for the workload of {MNIST, Simple-CNN, SGD}, where
the metaparameter being tuned is the learning rate η. The blue circles (∙) denote successful runs (i.e.,
it reached the goal error), and the best trial that records the steps-to-result is marked by gold star (?);
also, the grey triangles (N) and red crosses (×) refer to incomplete and infeasible runs, respectively.
The range of η shrinks as the sparsity level increases from 0% to 90%, indicating increased L based
on the learning rate condition from the convergence properties satisfying 0 < η ≤ 1/L.
Evidence of increased Lipschitz constant for sparse networks can be found further in metaparameter
search results presented in Figure 5. Notice that for each batch size, the size of the range for suc-
cessful learning rate η decreases when switching from 0 to 90% sparsity level. This is because the
learning rate bound satisfying the convergence rate theory becomes 0 < η ≤ 1/L for a fixed batch
size, and increased L due to sparsity shrinks the range of fj.
We note that our findings of increased Lipschitz constant for sparse networks are consistent with
the literature on over-parameterized networks such as Li & Liang (2018), which can be seen as the
opposite of sparsity. The more input weights a neuron has, the less likely it is that a single parame-
ter significantly changes the resulting activation pattern, and that wide layers exhibit convexity-like
properties in the optimization landscape Du et al. (2019). This even extends to non-smooth networks
with ReLU activations, which are still shown to exhibit pseudo-smoothness in the overparameterized
regime Li & Liang (2018). We further show that our theory precisely explains the difficulty of train-
ing sparse networks due to decreased smoothness based on a quantitative analysis in Appendix C.
In addition, we provide in Figure 6 the training logs of the networks used for the Lipschitz smooth-
ness analysis, in order to show the correlation between the Lipschitz smoothness of a network and
its training performance; i.e., sparsity incurs low smoothness of gradients (high L; see Figure 4) and
hence the poor training performance.
0.200
0.175
0.150
0.125
ω
8 0.100
-J
0.075
0.050
0.025
0000 一	_  _________________ _ _..
0	5 10 15 20 25 30 35 40
Steps (×103)
Steps (×103)
Figure 6: Training logs of Simple-CNN and ResNet-8 used for the smoothness analysis. The sparse
networks that recorded high Lipschitz constants show worse training performance, indicating that
low smoothness may be the potential cause of hampering the training of sparse neural networks.
8
Published as a conference paper at ICLR 2021
5	Discussion
Data parallelism with sparsity could have promising complementary benefits, and yet, little has been
studied about their effects on neural network training thus far. In this work, we accurately measured
their effects, and established theoretical results that precisely account for the general characteristics
of data parallelism and sparsity based on the convergence properties of stochastic gradient methods
and Lipschitz smoothness analysis. We believe our results are significant, in that these phenomena,
which have only been addressed partially and empirically, are now theoretically verified with more
accurate descriptions and applied to general nonconvex settings.
While our findings render positive impacts to practitioners and theorists alike, there are remaining
challenges. First, our experiments are bounded by available computing resources, and the cost of
experiments increases critically for more complex workloads. Also, the lack of convergence guar-
antees for existing momentum schemes in nonconvex and stochastic settings hinders a further theo-
retical analysis. We hypothesize that ultimate understanding of the effect of data parallelism should
be accompanied by a study of the generalization capability of optimization methods. Nonetheless,
these are beyond the scope of this work, and we intend to explore these directions as future work.
Acknowledgments
This work was supported by the ERC grant ERC-2012-AdG 321162-HELIOS, EPSRC grant See-
bibyte EP/M013774/1, EPSRC/MURI grant EP/N019474/1, the Australian Research Council Centre
of Excellence for Robotic Vision (project number CE140100016), and the Institute of Information &
communications Technology Planning & Evaluation (IITP) grant funded by the Korea government
(MSIT) (No.2020-0-01336, Artificial Intelligence Graduate School Program (UNIST)). We would
also like to acknowledge the Royal Academy of Engineering and FiveAI.
References
Mardn Abadi et al. Tensorflow: A system for large-scale machine learning. In 12th USENIX
symposium on operating systems design and implementation (OSD116), pp. 265-283, 2016.
Leon Bottou. Stochastic gradient learning in neural networks. Neuro Nimes, 1991.
Leon Bottou. Online learning and stochastic approximations. On-line learning in neural networks,
1998.
Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. Siam Review, 60(2):223-311, 2018.
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc’aurelio
Ranzato, Andrew Senior, Paul Tucker, Ke Yang, et al. Large scale distributed deep networks.
NeurIPS, 2012.
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. ICLR, 2019.
Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation meth-
ods for nonconvex stochastic composite optimization. Mathematical Programming, 155(1-2):
267-305, 2016.
Priya Goyal, Piotr Doll狂 Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for
efficient neural network. NeurIPS, 2015.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the general-
ization gap in large batch training of neural networks. NeurIPS, 2017.
Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv preprint
arXiv:1404.5997, 2014.
9
Published as a conference paper at ICLR 2021
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. SNIP: Single-shot network pruning
based on connection sensitivity. ICLR, 2019.
Namhoon Lee, Thalaiyasingam Ajanthan, Stephen Gould, and Philip H. S. Torr. A signal propaga-
tion perspective for pruning neural networks at initialization. ICLR, 2020.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. NeurIPS, pp. 8157-8166, 2018.
Tao Lin, Sebastian U Stich, Kumar Kshitij Patel, and Martin Jaggi. Don’t use large mini-batches,
use local sgd. ICLR, 2020.
Russell Reed. Pruning algorithms-a survey. Neural Networks, 1993.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathemati-
cal statistics, pp. 400-407, 1951.
Christopher J Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, and
George E Dahl. Measuring the effects of data parallelism on neural network training. JMLR,
2019.
Samuel L Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V Le. Don’t decay the learning rate,
increase the batch size. ICLR, 2018.
Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by
preserving gradient flow. ICLR, 2020.
Weiran Wang and Nathan Srebro. Stochastic nonconvex optimization with large minibatches. arXiv
preprint arXiv:1709.08728, 2017.
Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George Dahl, Chris
Shallue, and Roger B Grosse. Which algorithmic choices matter at which batch sizes? insights
from a noisy quadratic model. NeurIPS, 2019.
Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates
training: A theoretical justification for adaptivity. ICLR, 2020.
A Proof of the general effect of data parallelism
This section provides the missing proofs in Section 4. The goal is to derive the relationship between
batch size B and steps-to-result K? from the convergence rates of generalized stochastic gradient
methods for both fixed and decaying learning rate cases. The result serves to account for the effect
of data parallelism as a general phenomenon that must appear naturally at neural network training.
A. 1 Fixed learning rate case
We start from the convergence rate result in Eq. (2). We first recognize that the expected average
squared gradient norm on the left-hand side indicates the degree of convergence. Then, this quantity
is directly related to the concept of goal error to reach in our experiments, and hence, reduces to be a
small constant as soon as it is implemented as a pre-defined goal error for a given workload. Thus,
it follows that
ηLM [ 2(f(wι)- f∞)
μ	Kμη
(4)
Notice that by fixing E = E[六 PK=Ill Vf (Wk)k2], it effectively means that the training process
has stopped, and therefore, K on the right-hand side will no longer contribute to decrease the bound
of the quantity for a particular learning rate η and batch size B.
Also, We select the optimal4 learning rate η?, out of extensive metaparameter search, to record
the lowest number of steps to reach the given goal error, which we denote as steps-to-result K? .
4Here, optimal simply refers to the sense of yielding the lowest steps-to-result.
10
Published as a conference paper at ICLR 2021
Plugging in these yields the following:
e ≤ η*LM + 2(f (wι)- f∞)
μ μ	K *μη
(5)
Next, notice that the only factors that constitute the inequality in Eq. (2) come from the assumptions
made to derive the convergence rate result, which are on the Lipschitz smoothness L and the vari-
ance bound M, and if they are assumed to be tight in the worst case, the inequality becomes tight.
Then, after making algebraic manipulation and taking the first-order Taylor approximation while
substituting M = β∕B since the variance bound M is related to batch size B as M 8 1/B (BottoU
et al., 2018), we obtain the following result:
K ? ≈
∆
可μe - (η*)2LM
∆	∆LM
≈ —	+	2^2^
η*μc	μ2e2
_ ∆Lβ ∆
μ2e2B	η*μc
c1	∆Lβ
=—+ Cl, where ci = -ɪɪ and Cl
B	μ2 2
(6)
∆
η*μc
Here, ∆ = 2(f(w1) -f∞), β is the initial variance bound at batch size B = 1 for a given workload.
Notice that e, ∆, L, β, μ η* all are constant or become fixed for a given workload. Also, the degree
of metaparameter search quality is assumed to be the same across different batch sizes, and hence,
the result can be reliably used to interpret the relationshp between batch size and steps-to-result.
A.2 Decaying learning rate case
We could extend the convergence rate for a fixed learning rate ηk = n in Eq. (2) to any sequence of
decaying learning rates ηk satisfying Pk∞=1 ηk = ∞ and Pk∞=1 ηkl < ∞ based on Theorem 4.10 in
Bottou et al. (2018) as follows:
e" K XX ηk Rf(WkH ≤ ≡ Xn+2(E[f (Ki∞).	⑺
k=1	k=1
The requirements on learning rate ηk are the classical conditions (Robbins & Monro, 1951) that
are assumed for convergence of any (sub)gradient methods, and cover nearly all standard and/or
heuristical learning rate schedules employed in practice.
Now, the relationship between batch size and steps-to-result can be derived similarly as before.
First, applying N = E[PK=I ηk∣Nf (Wk)∣∣2] for the degree of convergence, and replacing with
∆ = 2(E[f(w1)] - f∞) for simplicity, Eq. (7) can be rewritten as follows:
n ≤ KM
K
X	ηkl +
k=1
∆
Kμ
(8)
Plugging H = PkK=1 ηkl for a finite constant from decaying learning rate, and further, H ? and K
for selected H by metaparameter search and steps-to-result, respectively, it becomes:
〜LMH 大	∆
E ― K 大 μ + K 大μ
(9)
Finally, using the worst case tightness on L and M, substituting M = β∕B, and rearranging the
terms, the effect of data parallelism for decaying learning rate case can be written as follows:
K ? ≈
LMH 大	∆
μ + μ
LH 大β	∆
μeB + μ
(10)
C1
B + c2,
where CN1
LH 大β
μe
and 办=∆
μ
11
Published as a conference paper at ICLR 2021
Here, e, ∆, L, H?, β, μ all are constant or become fixed for a given workload.
This result, along with the case of fixed learning rate, establishes the theoretical account for the
effect of data parallelism, by precisely and generally describing the relationship between batch size
B and steps-to-result K?. Further analysis of these results is referred to the main paper.
B Scale of our experiments
For a given workload of {data set, network model, optimization algorithm} and fora study setting of
{batch size, sparsity level}, we execute 100 training runs with different metaparameters to measure
steps-to-result. At each run, we evaluate the intermediate models at every 16 (for MNIST) or 32 (for
CIFAR-10) iterations, on the entire validation set, to check if it reached a goal error. This means
that, in order to plot the results for the workload of {MNIST, Simple-CNN, SGD} for example,
one would need to perform, 14 (batch sizes; 21 to 214) × 4 (sparsity levels; 0, 50, 70, 90%) × 100
(runs) × 40, 000 (max training iteration preset) / 16 (evaluation interval) = 14, 000, 000 number
of evaluations. Assuming that evaluating the Simple-CNN model on the entire MNIST validation
set takes only a second on a modern GPU, it will take 14, 000, 000 (evaluations) × 1 (second per
evaluation) / 3600 (second per hour) ≈ 3888 hours or 162 days.
Of course, there are multiple ways to reduce this cost; for instance, we may decide to stop as soon as
the run hits the goal error without running until the max training iteration limit. Or, simply reducing
any factor listed above that contributes to increasing the experiment cost (e.g., number of batch
sizes) can help to reduce the time, however, in exchange for the quality of experiments. We should
also point out that this is only for one workload where we assumed that the evaluation takes only a
second. The cost can increase quite drastically if the workload becomes more complex and requires
more time for evaluation and training (e.g., CIFAR-10). Not to mention, we have tested for multiple
workloads besides the above example, in order to confirm the generality of the findings in this work.
C More on Lipschitz smoothness analysis
We measure the local LiPschitz constant of Vf based on a Hessian-free method as used in Zhang
et al. (2020). Precisely, the local smoothness (or Lipschitz constant of the gradient) at iteration k is
estimated as in the following:
L(Wk)= max	kVf(Wk+ Yd)-Vf(Wk)k2 ,	(11)
v k γ∈{δ,2δ,..,1}	IlYdIl2	,	v J
where d = Wk+1 - Wk and δ ∈ (0, 1) for which we set to be δ = 0.1. The exPected gradient
Vf is computed on the entire training set, and We measure L(Wk) at every 100 iterations through-
out training. This method searches the maximum bound on the smoothness along the direction
between Vf (Wk+1) and Vf(Wk) based on the intuition that the degree of deviation of the linearly
approximated objective function is bounded by the variation of gradient between Wk+1 and Wk.
Furthermore, while ReLU networks (e.g., Simple-CNN, ResNet-8) can only be piecewise smooth,
the smoothness can still be measured for the same reason that we measure gradient (i.e., it only
requires differentiability).
We also empirically measure the changes in ∆ and β by introducing sparsity. Recall that these are the
other elements in c1 that can be affected by sparsity along with Lipschitz constant L. When we mea-
sure these quantities for Simple-CNN, we obtain the following results: ∆s∕∆d ≈ 4.68/4.66 ≈ 1.00,
and βs∕βd ≈ 107.39/197.06 ≈ 0.54; more precisely, ∆ does not change much since neither f (wι)
or f (w∞) changes much, and βs∕βd can be measured by the ratio of the variances of gradients
between sparse and dense networks at B = 1. We have already provided Ls, Ld in Figure 4, which
makes Ls/Ld ≈ 1.76/0.57 ≈ 3.09. Here, s and d denote sparse (90%) and dense, respectively. No-
tice that ifwe combine all the changes in ∆, β, L due to sparsity, and compute c1,s/c1,d, it becomes
1.00 × 0.54 × 3.09 ≈ 1.67. Importantly, c1,s/c1,d > 1 means that c1 has increased by sparsity, and
since the increase in Lipschitz constant L played a major role therein, these results indicate that the
general difficulty of training sparse networks is indeed caused by reduced smoothness. We further
note that this degree of change fits roughly the range of ks?/kd? as shown in Figure 12.
12
Published as a conference paper at ICLR 2021
D Additional results
In this section, we provide additional experiemental results that are not included in the main paper. In
Section D.1, we supplement more results for the effects of data parallelism and sparsity in Figures 7,
8, 9, 10, 11. In Figure 12 we present the difference in ratio between sparse (90%) and dense networks
across different batch sizes for all workloads presented in this work. This result shows how much
increase in steps-to-result is induced by introducing sparsity, and therefore, is used to study the
general difficulty of training sparse neural networks. In Section D.2, we provide metaparameter
search results for a subset of workloads studied in this work.
D. 1 Effects of data parallelism and sparsity
Sparsity： 0%
Batch size
(b) Momentum
All sparsity (normalized)
Sparsity： 0%
is 27	29 2n 233
Batch size
(c) Nesterov
Figure 7: Results for the effects of data parallelism for the workloads of {MNIST, Simple-CNN,
SGD/Momentum/Nesterov} with a constant learning rate.
arsIty
m
sm
τm
・ SPirW:
→- SMmy：
5wrs«v：
Batch size
Batch size

13
Published as a conference paper at ICLR 2021
Sparslty: 70%
(a) SGD
* 2u 2加
Batch size
(b) Momentum
(c) Nesterov
Figure 8: Results for the effects of data parallelism for the workloads of {Fashion-MNIST, Simple-
CNN, SGD/Momentum/Nesterov} with a constant learning rate and the goal error of 0.12.
(a) SGD
All sparsity (normalized)
(b) Momentum
(c) Nesterov
Figure 9: Results for the effects of data parallelism for the workloads of {Fashion-MNIST, Simple-
CNN, SGD/Momentum/Nesterov} with a constant learning rate and the goal error of 0.14.
14
Published as a conference paper at ICLR 2021
All sparsity (normalized)
(b) Momentum
(c) Nesterov
Figure 10: Results for the effects of data parallelism for the workloads of {CIFAR-10, ResNet-8,
SGD/Momentum/Nesterov} with a linear learning rate decay.
AJI sparsity (normalized)
2-z
22~3
2-ι
2-*
2-S
2-«
ZlO 2 ιz 21*
Batch size
(a) SGD
(b) Momentum
Figure 11: Results for the effects of data parallelism for the workloads of {CIFAR-10, ResNet-8,
SGD/Momentum} with a constant learning rate.
15
Published as a conference paper at ICLR 2021
SGD	Momentum	Nesterov
(©SUSP 0*j©sued-0seκ
23 25 27 29 211 2*
Batch size
(©SUSP 0*j©sued-0=eκ
21 23 25 27 29 211 2«
Batch size
(a)	MNIST, Simple-CNN, SGD/Momentum/Nesterov with a constant learning rate
SGD	Momentum	Nesterov
(©SUSP 2 ©sued-0seκ
2»	28 210 212 2w
Batch size
(©SUSP 2 ©sued-0seκ
(©SUSP 2 ©sued-0seκ
2» 2s 21° 212 2w
Batch size
(b)	Fashion-MNIST, Simple-CNN, SGD/Momentum/Nesterov with a constant learning rate
2∑	25	2«	2« 2«~Jia~J14
Batch size
(©SUSP 0*j©sued-0-sκ
3.50
5 0 5 0 5 0 5
ZQ 7 5 ZQ 7
L
(©SUSP 0©sued0-
Momentum
1.50
^2β	2® jɪŋ
Nesterov
3.4
132
Φ 3.0
P
2 2∙8
Φ
e 2.6
(0
0 2.4
° 2.2
+u
ra 一一
t£. 2.0
Batch size
22 2λ 2e 29	210 212 214
Batch size
(c)	CIFAR-10, ResNet-8, SGD/Momentum/Nesterov with a linear learning rate decay
SGD	Momentum
(©SUSP 0*j©sued-04eκ
(©SUSP 0*j©sued-04eκ
26 2e 210 2j2 214
Batch size
(d)	CIFAR-10, ResNet-8, SGD/Momentum with a constant learning rate
Figure 12: Differences in ratio between (90%) sparse network’s steps-to-result to dense network’s,
across different batch sizes for all workloads presented in this work. The difference ranges between
(1.5, 4.5) overall. Note that the ratio difference > 1 indicates that it requires more number of
training iterations (i.e., steps-to-result) for sparse network compared to dense network. Also, the
difference seems to decrease as batch size increases, especially for Momentum based optimizers.
This potentially indicates that sparse neural networks can benefit from large batch training, despite
the general difficulty therein.
16
Published as a conference paper at ICLR 2021
D.2 Metaparameter search results
Learning rate
Learning rate
IM 5: 0%, B: 256
4.0
3.5
3.0
0.0
2.5
鼠。
S1.5
1.0
0.5
10-4	10→	U)7 IO-I ιcβ
Learning rate
ιβ4 S: 0%, B: 512
3.5
3.0
2.5
普2.0
sL5
1.0
0.5
0.0
ι<r* ιo-3 107 IoT ιo0
Learning rate
Learning rate
ie4 S: 0%, B: 16384
4.0
3.5
3.0
a25
B 2.。
S 1.5
1.0
0.5
0.0
w∙ UΓ, U>7 IoT IflO
Learning rate
ιe-2 S: 90%. B: 1	ie4 S: 90%. B: 2	ιe4 S: 90%. B: 4	ιe4 S: 90%. B: 8	ιe4 S: 90%, B: 16
Learning rate	Learning rate	Learning rate	Learning rate	Learning rate
Learning rate
Figure 13: Meataparameter search results for the workloads of {MNIST, Simple-CNN, SGD} with
a constant learning rate.
17
Published as a conference paper at ICLR 2021
3.5
S: 0%, B: 4
,⅛4
3.0
2.5
g,20
1.5
1.0
0.5
2.5
2.0
1.0
0.5
*
Learning rate
1.5
3.0
IM S: 0%, B: 32
lc4 S: 0%, B: 256
1e3 S：。％, B: 2048
10→	U)7	1(Γ1 IO0
Learning rate
S: 90%, B: 4
,1⅜4
3.5
3.0
2.5
2.0
1.5
1(Γ*	U>7	K)7 lθ-ɪ 10«
Learning rate
2
2

S: 90%, B: 32
.5
.0
B 1.5
1.0
0.5
,⅛4
3.0
urj
Learning rate
10-∙
S: 0%, B: 4
K3 S： 0%，B: 16384
• I	vrm,nm
•	∙ A ,4⅛L	£
Learning rate
1
•	& ZyWyffn"∏"ff		
	*	
•_ ,		
⅜<⅛		aΞ,
7
7
1.2
6
6
1.C
5
5
OT 0.S
S0-6
0.4
0.2
3.0
5
0
J*l∙5
1.0
0.5
d4
Q.4
2
2
1
5: 90%, B: 256
2.0
IgI.5
1.0
0.5
1(Γ3 IO-N IOT 10°
Learning rate
")T
0.0	l
IoT U>7	K)7 Io-I 13
Learning rate
°L..... .....一，，，，，，T-3L
L0~*	10~3	1(Γ2	1(Γ1	100
Learning rate
0.0
2
2
0.0
ιe4 S: 90%, B: 2048
3.0

10", 10^3 IOT ι(r1 10°
Learning rate
2.5
W S: 9。％, B: 16384
	
3.0 2.5 2.0 Q. « L5 Ul 1.0 0.5 0.0	βχ ∕v⅛∖∕wv⅞¾φγχ⅞¾e>j¾)⅞a⅛ -S—— 区二 ∙g⅞a⅛⅛.....
ur,
10-3 IO-S 10^,	10«
Learning rate
ie4
.lc4 S: 0%, B: 256
.lc4 S: 0%, B: 32
.K3 s： 0%, B: 2048
3.5 ∙mrwτmww⅜vm∙wv;Wy邛7W 3.0	 2.5	 _	3.( 2.. 2.C		1.2 1.0 tn 0.8		.	a>⅞y>ι⅛f⅝zro *		7 6 5	l⅞√yy⅜wy⅜'	⅜y⅜⅝WΛ∕yw	ZQ八	7 6 5
2.0	 • •	Q. £1.- S		垓。6		* .	*	d4 SS S3	• _	•			•— ,	d4 SS S3
1.5	 10 - • °4	....>⅛ 1 ʌ-4	iʌ-3	iʌ-2	，，	1.( 0.. 0.( —1	♦	— »—4	1Λ-3	1 ʌ-2	，，	0.4 0.2 0.0 1—1	rɪ. 1—4 ，，	1-3	izɪ	≡ —2	，，	2 1 。 —1	l/v	• • -4	♦，	—3	izɪ	0® —2	1	2 1 .	0 -1	,，
Momentum
Momentum
Momentum
Momentum
S: Q%, B: 16384
ie3
√vwyw∕w∖√vyvΛ∕y⅜>λ z 心 a T *		
			∙ Λ—
・ ∙			——∙ M
		
I(TS
ur,
Momentum
w,
S: 90%. B: 4
ie4
3.5
3.0
2.0
1.5
B 2.5
1(Γ,
UrZ
Momentum
3.1
ie4
S: 90%, B: 32
10 ^*
1<Γ,
1<Γ*
ι.0	——工一
2.5
1.0
0.5
a20
豺5
10-3
U>7
Momentum
IOT
,ιe⅜ S: 9。％, B: 16384
3.0
.ie⅜ S: 9。％, B: 2048
3-0
S: 90%, B: 256
5
0
a L5
1.0
0.5
le4
3.0
	
	• •
	
••	• •
10-3
ur,
Momentum
2
2
o.o _
1(Γ,
1<Γ,
0.0
ur,
2.5
2.0
1.0
0.5
区L5
a⅛a¾⅛⅛⅜y¾o⅞⅛⅞WΛ∕∕¾x λ∕> λ		
■		*
		® •
	*	%
		
IOT
U>7
Momentum
IO-I
0.0
1(Γ*
2.5
2.0
B 1.5
1.0
0.5
g⅛ft⅞y⅛⅛⅜⅞a⅞⅛⅛⅜VΛ∕yw ∕w>⅞ &
士 W
• • •

10-3
ur,
Momentum
IOT
Figure 14: Meataparameter search results for the workloads of {MNIST, Simple-CNN, Momentum}
with a constant learning rate.
ie4 S: 0%, B: 4
2.00
1.75
1.50
扣
cnιoo
0.75
0.50
10^* IOT I(FN UΓ,
Learning rate
Learning rate
ie4 S: 0%, B: 4	ιβ4 S: 0%, B: 32	⅛4 S: 0%, B: 256
Momentum
Momentum
Momentum
	ιc4 S:。%, B: 2。48	, ⅝⅜WfΛΛWy⅝⅝WΛ∕r⅜∖ Λ⅛Λ Z
1.2	
1.0	
a08	
»6	
0.4	~¼*:
0.2 0.0 10	T	l(Γs	UTs	UΓ,
Momentum
K3 S: 0%, B: 16384
7
6
5
a4
a
S3
2
1
° . . . .
1(Γ*	UΓ,	1(Γ2	10^,
Momentum
3.0
2.5
2.0
Q.
£1.5
in
1.0
0.5
0.0
1O^* IOT IOT 1CΓ,
ιe4 S: 90%, B: 2048
3.0
2.5
2.0
a
JJ 1.5
in
1.0
0.5
0.0
1(Γ*	10-3 ι<r2 IoT
ιc4 S: 9。%, B: 16384
Momentum
Momentum
Figure 15: Meataparameter search results for the workloads of {MNIST, Simple-CNN, Nesterov}
with a constant learning rate.
18
Published as a conference paper at ICLR 2021
U4 S: 90%, B: 4	ιe5 S: 90%, B: 32	u5 S: 90%, B: 256	ιβ4 S: 90%, B: 2048
Learning rate	Learning rate	Learning rate	Learning rate
Figure 16: Meataparameter search results for the workloads of {CIFAR-10, ResNet-8, SGD} with a
constant learning rate.
U4 S: 0%, B: 256
3.0
2.5
2.0
a
B 1.5
Ul
1.0
0.5


1Q7 iq-J IL MJ9
ιe4 S: 0%, B: 2048
ie4 S: 90%. B: 32
W*
IOT 10-s	10^,
Learning rate
10。
Learning rate
3.0
2.5
2.0
a
£15
in
1.0
0.5
00 . . 一
10^4	IOT	IOT	IoT 10。
Learning rate
ie4 S: 90%, B: 2048
6
5
u14
冒3
Ul
2
1
0................................
w∙ UΓ, 1(Γ2	10^,	10°
Learning rate
⅛⅛⅛
tn 0.6
B
SO.4
ιc4 S: 0%, B: 16384
3.0
2.5
2.0
Q.
£1.5
in
1.0
0.5
00 . . 一
ur*	ur,	ι<r2	ιo^1 ιoβ
Learning rate

,ι⅜5 s： 9。％, B: 16384	,
i∙<>∣-e ∕⅝∕wvΛ∕⅜γ^∕w¾ac⅝a⅞i5
0.8
0.2
0.0
1CΓ* 1(Γ, UΓ, UΓ, IO9
Learning rate
,ιc4	S: 0%, B: 32	,
. Ws::aa ,a 3∕”,占
5----Λ---
W S: 0%, B: 256
4
h
in
2
1
ur,
1(Γ3	1(Γ2 IOT
Momentum
Momentum
,1C4
9.25 ` ;
9.00 -
8.75 -
a850
£ 8.25 -
S______
8.00 -
7.75 -
7.50 -
7.25卜.
1(Γ*
S: 90%, B: 4
IO-3 10-z	1(Γ,
Momentum
X S: 9。％, B: 32
ie4 S: 90%, B: 256
：,√'∙⅛I<⅛'∙∙ √⅛∙SiS⅛⅛⅛??
?
6
5
3
2
Momentum
1(Γ3	1(Γ2	1(Γ1
Momentum
⅛⅛ S: 90%, B: 2048
5-----
a4
S3
Ul
2
1
0 . . . .
w∙	UΓ,	1(Γ2	1O^,
1.0
0.8
a06
Si
S 0.4
0.2
0.0
IoT 1(Γ*	1(Γ2	1(Γ1
Momentum
1c5 S: 9。%, B: 16384
Momentum
Figure 17: Meataparameter search results for the workloads of {CIFAR-10, ResNet-8, Momentum}
with a constant learning rate.
19
Published as a conference paper at ICLR 2021
S: 90⅜, B: 2048
S: 0%, B: 4
5.0
4.5
3.0
2.5
2.0
2.40
郎3.5
ie4
5.5
		
•—		
%			ΞZ
⅛	P ∙ J «	
UΓ* IOT IOT IOT 10。
Learning rate
ie4
3.5 -
2
2.2
B
S: 0%, B: 32
3.0
.5
.0
1.5
1.0
0.5
	
二 * L— * ∙⅛ ∙ ∙ ..…M⅞嗯…	
UΓ* UΓj UΓ, 10^,	10°
Learning rate
ιc4 S: 0%, B: 256
3.δ'
2

3.0
5
0
1.0
0.5
0.0
久5
IoT 10^3 W2 10^,	10»
Learning rate

W S: 9。%, B: 4
les S: 90%, B: 32
1.0
9
0.8
a06
0.4
0.2
10→	U)7 IO-I ιcβ
Learning rate
,⅛4 S: 9。％, B: 256
5
4
自3
2
1

		■	
4
a
in
2
1
0

UΓ* uγj uγ, 10^,
10。
Learning rate
2.5 2.0 a is B S 1.0 0.5 0.0 10	,ie⅜ •		5: 0%, Δ	B: 2048	, v∕wxιβw¾ws⅛	
				
				
	T Il	⅛ -3 K	* 7 IC	Γ,	IO0
Learning rate
3.5 3.0 2.5 S. 2.0 B S 1.5 1.0 0.5 0.0	∣⅛4 •	 „ O	:。％, B: 1638	4 l∙.	
	悬		
UΓ* UΓj U>7 IOT 10°
Learning rate
5
5.0
4.5
3.0
2.0
3 4。
和5
W*
3.0 3.5 4.0 4.5 5.0 5.5 «.0
Decay steps le4
^⅛
IoT	1(Γ3	1(Γ2	1(Γ1 Learning rats	IO0
,1⅜4	S: 0%, B: 4	
		

2.5
			
*		*	
⅜∙β	.		■
3.0 3.5 4.0 4.5 5.0 5.5 «.0
3.0 3.5 4.0 4.5 5.0 5.5 «.0
10-3 UΓ, UF1 IO9
Learning rate
6F 5 4 h S 2 1 0	K S: 90%, B: 204						U / 6 - 5 - a4 瑟3	■4 5： 90%, B: 16384 "ΛΛ WWAWVWWWSM		
	β "£i	L	 yx≡			4 骁,,,,,,,			•— ——*			
w∙ W3 W2 W, W0 IoT W3 IoT 10-1 Learning rate	Learning rate ⅛4 S: 0%, B: 2048	⅛4 S: 0%, B: 16384										S9
2. 2.C ⅛1.. Si wιt 0.. 0.(	⅛. - -	/a	≡<				3. 3. 2. S.2. 久 ° 0.	5≥ - 5 - 5		亚 O
			• ∙							
									dɪ'	
								■ •		
3.0 3.5 4.0 4.5 5.0 5.5 6.0	3.0 3.5 4.0 4.5 5.0 5.5 6.0
Decay steps le4	Decay steps lβ4
Decay steps ɪ*4
Decay steps le4
W S: 0%, B: 4	ie4 S: 0%, B: 32	ιe4 S: 0%, B: 256	ie4 S: 0%, B: 2048	ie4 S: 0%, B: 16384
Decay factor
Decay factor
Decay factor
Decay factor
Decay factor
Figure 18: Meataparameter search results for the workloads of {CIFAR-10, ResNet-8, Nesterov}
with a linear learning rate decay.
20
Published as a conference paper at ICLR 2021
E	Implementation details
Data parallelism and sparsity. By data parallelism, we refer to utilizing a parallel computing
system where the training data is distributed to multiple processors for gradient computations, so
that the training process can be accelerated. For the purpose of this work, we consider the simplest
setting of synchronized distributed systems, in which the degree of parallelism equates to the size
of mini-batch used for training on a regular single-node system. This effectively means that the
effect of data parallelism can be measured by increasing batch size. By sparsity, we refer to pruning
parameters in a neural network model, such that the remaining parameters are distributed sparsely
on the network. For the purpose of this work, we employ a recent pruning-at-initialization method
to obtain sparse networks, since they must not undergo any training beforehand so as to measure the
effects of data parallelism while training from scratch.
Software and hardware. We used TensorFlow libraries (Abadi et al., 2016) and a compute cluster
with multiple nodes of CPUs (Intel Xeon Gold 5120 CPU @ 2.20GHz with 28 cores; 4 in total) and
GPUs (Tesla P100 and V100; 16GB; 28 in total).
21