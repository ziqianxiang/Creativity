Published as a conference paper at ICLR 2021
Differentiable Trust Region Layers for Deep
Reinforcement Learning
Fabian Otto*
Bosch Center for Artificial Intelligence
University of Tubingen
Philipp Becker
Karlsruhe Institute of Technology
Ngo Anh Vien & Hanna Carolin Ziesche
Bosch Center for Artificial Intelligence
Gerhard Neumann
Karlsruhe Institute of Technology
Ab stract
Trust region methods are a popular tool in reinforcement learning as they yield ro-
bust policy updates in continuous and discrete action spaces. However, enforcing
such trust regions in deep reinforcement learning is difficult. Hence, many ap-
proaches, such as Trust Region Policy Optimization (TRPO) and Proximal Policy
Optimization (PPO), are based on approximations. Due to those approximations,
they violate the constraints or fail to find the optimal solution within the trust re-
gion. Moreover, they are difficult to implement, often lack sufficient exploration,
and have been shown to depend on seemingly unrelated implementation choices.
In this work, we propose differentiable neural network layers to enforce trust re-
gions for deep Gaussian policies via closed-form projections. Unlike existing
methods, those layers formalize trust regions for each state individually and can
complement existing reinforcement learning algorithms. We derive trust region
projections based on the Kullback-Leibler divergence, the Wasserstein L2 dis-
tance, and the Frobenius norm for Gaussian distributions. We empirically demon-
strate that those projection layers achieve similar or better results than existing
methods while being almost agnostic to specific implementation choices. The
code is available at https://git.io/Jthb0.
1	Introduction
Deep reinforcement learning has shown considerable advances in recent years with prominent ap-
plication areas such as games (Mnih et al., 2015; Silver et al., 2017), robotics (Levine et al., 2015),
and control (Duan et al., 2016). In policy search, policy gradient (PG) methods have been highly
successful and have gained, among others, great popularity (Peters & Schaal, 2008). However, often
it is difficult to tune learning rates for vanilla PG methods, because they tend to reduce the entropy
of the policy too quickly. This results in a lack of exploration and, as a consequence, in premature or
slow convergence. A common practice to mitigate these limitations is to impose a constraint on the
allowed change between two successive policies. Kakade & Langford (2002) provided a theoretical
justification for this in the approximate policy iteration setting. Two of the arguably most favored
policy search algorithms, Trust Region Policy Optimization (TRPO) (Schulman et al., 2015a) and
Proximal Policy Optimization (PPO) (Schulman et al., 2017), follow this idea using the Kullback-
Leibler divergence (KL) between successive policies as a constraint.
We propose closed-form projections for Gaussian policies, realized as differentiable neural network
layers. These layers constrain the change in successive policies by projecting the updated policy onto
trust regions. First, this approach is more stable with respect to what Engstrom et al. (2020) refer
to as code-level optimizations than other approaches. Second, it comes with the benefit of imposing
constraints for individual states, allowing for the possibility of state-dependent trust regions. This
allows us to constrain the state-wise maximum change of successive policies. In this we differ from
previous works, that constrain only the expected change and thus cannot rely on exact guarantees
of monotonic improvement. Furthermore, we propose three different similarity measures, the KL
* Correspondence to fabian.otto@bosch.com
1
Published as a conference paper at ICLR 2021
divergence, the Wasserstein L2 distance, and the Frobenius norm, to base our trust region approach
on. The last layer of the projected policy is now the the trust region layer which relies on the old
policy as input. This would result in a ever-growing stack of policies, rendering this approach clearly
infeasible. To circumvent this issue we introduce a penalty term into the reinforcement learning
objective to ensure the input and output of the projection stay close together. While this still results
in an approximation of the trust region update, we show that the trust regions are properly enforced.
We also extend our approach to allow for a controlled evolution of the entropy of the policy, which
has been shown to increase the performance in difficult exploration problems (Pajarinen et al., 2019;
Akrour et al., 2019).
We compare and discuss the effect of the different similarity measures as well as the entropy control
on the optimization process. Additionally, we benchmark our algorithm against existing methods
and demonstrate that we achieve similar or better performance.
2	Related Work
Approximate Trust Regions. Bounding the size of the policy update in policy search is a com-
mon approach. While Kakade & Langford (2002) originally focused on a method based on mixing
policies, nowadays most approaches use KL trust regions to bound the updates. Peters et al. (2010)
proposed a first approach to such trust regions by formulating the problem as a constraint optimiza-
tion and provided a solution based on the dual of that optimization problem. Still, this approach is
not straightforwardly extendable to highly non-linear policies, such as neural networks. In an at-
tempt to transfer those ideas to deep learning, TRPO (Schulman et al., 2015a) approximates the KL
constraint using the Fisher information matrix and natural policy gradient updates (Peters & Schaal,
2008; Kakade, 2001), along with a backtracking line search to enforce a hard KL constraint. Yet,
the resulting algorithm scales poorly. Thus, Schulman et al. (2017) introduced PPO, which does not
directly enforce the KL trust region, but clips the probability ratio in the importance sampling objec-
tive. This allows using efficient first-order optimization methods while maintaining robust training.
However, Engstrom et al. (2020) and Andrychowicz et al. (2020) recently showed that implementa-
tion choices are essential for achieving state-of-the-art results with PPO. Code-level optimizations,
such as reward scaling as well as value function, observation, reward, and gradient clipping, can even
compensate for removing core parts of the algorithm, e. g. the clipping of the probability ratio. Ad-
ditionally, PPO heavily relies on its exploration behavior and might get stuck in local optima (Wang
et al., 2019). Tangkaratt et al. (2018) use a closed-form solution for the constraint optimization
based on the method of Lagrangian multipliers. They, however, require a quadratic parametrization
of the Q-Function, which can limit the performance. Pajarinen et al. (2019) introduced an approach
based on compatible value function approximations to realize KL trust regions. Based on the rein-
forcement learning as inference paradigm (Levine, 2018), Abdolmaleki et al. (2018) introduced an
actor-critic approach using an Expectation-Maximization based optimization with KL trust regions
in both the E-step and M-step. Song et al. (2020) proposed an on-policy version of this approach
using a similar optimization scheme and constraints.
Projections for Trust Regions. Akrour et al. (2019) proposed Projected Approximate Policy It-
eration (PAPI), a projection-based solution to implement KL trust regions. Their method projects
an intermediate policy, that already satisfies the trust region constraint, onto the constraint bounds.
This maximizes the size of the update step. However, PAPI relies on other trust region methods
to generate this intermediary policy and cannot operate in a stand-alone setting. Additionally, the
projection is not directly part of the policy optimization but applied afterwards, which can result in
sub-optimal policies. In context of computational complexity, both, TRPO and PAPI, simplify the
constraint by leveraging the expected KL divergence. Opposed to that, we implement the projec-
tions as fully differentiable network layers and directly include them in the optimization process.
Additionally, our projections enforce the constraints per state. This allows for better control of the
change between subsequent policies and for state-dependent trust regions.
For the KL-based projection layer we need to resort to numerical optimization and implicit gradients
for convex optimizations (Amos & Kolter, 2017; Agrawal et al., 2019). Thus, we investigate two
alternative projections based on the Wasserstein L2 and Frobenius norm, which allow for closed
form solutions. Both, Wasserstein and Frobenius norm, have found only limited applications in
reinforcement learning. Pacchiano et al. (2020) use the Wasserstein distance to score behaviors of
2
Published as a conference paper at ICLR 2021
agents. Richemond & Maginnis (2017) proposed an alternative algorithm for bandits with Wasser-
stein based trust regions. Song & Zhao (2020) focus on solving the trust region problem for distri-
butional policies using both KL and Wasserstein based trust regions for discrete action spaces. Our
projections are applicable independently of the underlying algorithm and only assume a Gaussian
policy, a common assumption for continuous action spaces.
Several authors (Dalal et al., 2018; Chow et al., 2019; Yang et al., 2020) used projections as network
layers to enforce limitations in the action or state space given environmental restrictions, such as
robotic joint limits.
Entropy Control. Abdolmaleki et al. (2015) introduced the idea of explicitly controlling the de-
crease in entropy during the optimization process, which later was extended to deep reinforcement
learning by Pajarinen et al. (2019) and Akrour et al. (2019). They use either an exponential or linear
decay of the entropy during policy optimization to control the exploration process and escape local
optima. To leverage those benefits, we embed this entropy control mechanism in our differentiable
trust region layers.
3	Preliminaries and Problem Statement
We consider the general problem of a policy search in a Markov Decision Process (MDP) defined
by the tuple (S, A, T, R, P0, γ). We assume the state space S and action space A are continuous
and the transition probabilities T : S × A × S → [0, 1] describe the probability transitioning to
state st+1 ∈ S given the current state st ∈ S and action at ∈ A. We denote the initial state
distributions as P0 : S → [0, 1]. The reward returned by the environment is given by a function
R : S × A → R and γ ∈ [0, 1) describes the discount factor. Our goal is to maximize the
expected accumulated discounted reward Rγ = ET,P0,π [Pt∞=0 γtR(st, at)]. To find the optimal
policy, traditional PG methods often make use of the likelihood ratio gradient and an importance
sampling estimator. Moreover, instead of directly optimizing the returns, it has been shown to be
more effective to optimize the advantage function as this results in an unbiased estimator of the
gradient with less variance
ʌ ___________________
max J (πθ, πθoId) = max E(s,a)〜∏θ I.
θ	θ	old
三Aπθold (s"
(1)
where Aπ (s, a) = E [Rγ |s0 = s, a0 = a; π] - E [Rγ |s0 = s; π] describes the advantage function,
and the expectation is w.r.t ∏θ0h, i.e. s0 〜T(∙∣s, a) a 〜∏θoh(∙∣s), so 〜 P0(s0), S 〜ρ∏θoλ. where
ρπθold is a stationary distribution of policy πθold . The advantage function is commonly estimated
bygeneralized advantage estimation (GAE) (Schulman et al., 2015b). Trust region methods use
additional constraints for the given objective. Using a constraint on the maximum KL over the
states has been shown to guarantee monotonic improvement of the policy (Schulman et al., 2015a).
However, since all current approaches do not use a maximum KL constraint but an expected KL
constraint, the guarantee of monotonic improvement does not hold exactly either. We are not aware
of such results for the W2 distance or the Frobenius norm.
For our projections we assume Gaussian policies ∏θ0h(at∣st) = N(at∣μ0id(st), ∑oid(St)) and
∏θ(at∣st) = N(at∣μ(st), Σ(st)) represent the old as well as the current policy, respectively. We
expiore three trust regions on top of Equation 1 that empioy different simiiarity measures between
old and new distributions, more specifically the frequently used reverse KL divergence, the Wasser-
stein L2 distance, and the Frobenius norm.
Reverse KL Divergence. The KL divergence between two Gaussian distributions with means μι
and μ2 and covariances ∑ι and ∑2 can generally be written as
KL({μ1,夕1} k {μ2,夕2}) = 5 (μ2 - μI)T夕-1(仙2 - μI) +log ττ=rr + 帆夕—1夕1}— d ,
2 L	|*i|	」
where d is the dimensionality of μ1,μ2. The KL uses the Mahalanobis distance to measure the
similarity between the two mean vectors. The difference of the covariances is measured by the
difference in shape, i.e., the difference in scale, given by the log ratio of the determinants, plus the
difference in rotation, given by the trace term. Given the KL is non-symmetric, it is clearly not a
distance, yet still a frequently used divergence between distributions. We will use the more common
reverse KL for our trust region, where the first argument is the new policy and the second is the old
policy.
3
Published as a conference paper at ICLR 2021
Wasserstein Distance. The Wasserstein distance is a distance measure based on an optimal trans-
port formulation, for more details see Villani (2008). The Wasserstein-2 distance for two Gaussian
distributions can generally be written as
W2 ({μι, ∑ι} , {μ2, ∑2}) = ∣μι - 〃2『+ tr (∑1 + ∑2 - 2 (∑272∑ι∑2/2) 1/2
A key difference to the KL divergence is that the Wasserstein distance is a symmetric distance
measure, i. e., W2(q, p) = W2(p, q). Our experiments also revealed that it is beneficial to measure
the W2 distance in a metric space defined by the covariance of the old policy distribution, denoted
here as Σ2, as the distance measure is then more sensitive to the data-generating distribution. The
W2 distance in this metric space reads
W2,∑2 ({μι, ∑1}, {μ2,夕2}) =3 - μI)Tς-13 - μI)
+ tr (∑-1∑ι + I - 2Σ-1 (∑2∕2∑ι∑2∕2)1/2
Frobenius Norm. The Frobenius norm is a matrix norm and can directly be applied to the differ-
ence of the covariance matrices of the Gaussian distributions. To measure the distance of the mean
vectors, we will, similar to the KL divergence, employ the Mahalanobis distance as this empirically
leads to an improved performance in comparison to just taking the squared distance. Hence, we will
denote the following metric as Frobenius norm between two Gaussian distributions
F({μι,夕1}, {μ2,匕}) = (μ2 - μI)Tς2 1(μ2 - μI) + tr(N2 - ςI)T(夕2 - ςI)) .
The Frobenius norm also constitutes a symmetric distance measure.
4	Differentiable Trust-Region Layers for Gaus s ian Policies
We present projections based on the three similarity measures, i. e., Frobenius norm, Wasserstein
L2 distance, and KL divergence. These projections realize state-wise trust regions and can directly
be integrated in the optimization process as differentiable neural network layers. Additionally, we
extend the trust region layers to include an entropy constraint to gain control over the evolution of
the policy entropy during optimization. The trust regions are defined by a distance or divergence
d(π(∙∣s), ∏oid(∙∣s)) between probability distributions. Complementing Equation 1 with the trust re-
gion constraint leads to
max J(∏θ,∏θold) s.t. d(∏θold(∙∣s)),∏θ(∙∣s)) ≤ e ∀s ∈ S.	(2)
θ
While, in principle, we want to enforce the constraint for every possible state, in practice, we can
only enforce them for states sampled from rollouts of the current policy.
To solve the problem in Equation 2, a standard neural network will output the parameters μ, Σ of a
Gaussian distribution πθ, ignoring the trust region bounds. These parameters are provided to the trust
region layers, together with the mean and covariance of the old policy and a parameter specifying the
size of the trust region . The new policy is then given by the output of the trust region layer. Since
the old policy distribution is fixed, all distances or divergences used in this paper can be decomposed
into a mean and a covariance dependent part. This enables us to use separate trust regions as well
as bounds for mean and covariance, allowing for more flexibility in the algorithm. The trust region
∙-v
layers aim to project ∏θ into the trust region by finding parameters μ and Σ that are closest to the
original parameters μ and Σ while satisfying the trust region constraints. The projection is based on
the same distance or divergence which was used to define the respective trust region. Formally, this
corresponds to the following optimization problems for each s
arg min dmean (μs,μ(s)) , s.t. dmean (μs, μold(s)) ≤ eμ,	and	(3)
μs
argmindɑov (∑s, ∑(s)) , s.t. dɑov (∑s, ∑old(s)) ≤ e∑,	(4)
∙-v
where "§ and ∑s are the optimization variables for state s. Here, dmean is the mean dependent part
and dcov is the covariance dependent part of the employed distance or divergence. For brevity of
notation we will neglect all dependencies on the state in the following. We denote the projected
∙-v
policy as π(a∣s) = N(a∣μ, Σ).
4
Published as a conference paper at ICLR 2021
4.1	Projection of the Mean
For all three trust region objectives we make use of the same distance measure for the mean, the
Mahalanobis distance. Hence, the optimization problem for the mean is given by
argmin (μ - μ)	ςoi1	(μ - μ)	s.t.	(μoid	-	μ)	%11	(μoid	- μ)	≤	∈μ∙	(5)
μ
By making use of the method of Lagrangian multipliers (see Appendix B.2), we can formulate the
dual and solve it for the projected mean μ as
μ= μ + ωμold with ω = J(μold - μ)T ς-1 (μold - μ) - 1.	(6)
1 + ω	V	Eμ
This equation can directly be used as mean for the Gaussian policy, while it easily allows to compute
gradients. Note, that for the mean part of the KL we would need to use the Σ-1 instead of Σo-ld1 in
the objective of Equation 5. Yet, this objective still results in a valid trust region problem which is
much easier to optimize.
4.2 Projection of the Covariance
Frobenius Projection. The Frobenius projection formalizes the trust region for the covariance
with the squared Frobenius norm of the matrix difference, which yields
argmintr ((夕-ς)t(夕-fŋ) , S.t. tr(Nold - ς)tNold -刈)≤ e∑∙
∙-v
We again use the method of Lagrangian multipliers (see Appendix B.3) and get the covariance Σ as
Σ + ηΣold
Σ =---------- with η
1+η
where η is the corresponding Lagrangian multiplier.
V-
tr((∑old- Σ)T(Σold - Σ)) - ι
(7)
Σ
(8)
Wasserstein Projection. Deriving the Wasserstein projection follows the same procedure. We
obtain the following optimization problem
arg∑mintr 卜-1∑ + ∑-d∑ - 2∑-1 (∑^∑∑“2) 1/2
s.t. tr (I +ς-1ς — 2ς-1 (瑞双/d) 1/2) ≤ %
where I is the identity matrix. A closed form solution to this optimization problem can be found
by using the methods outlined in Takatsu (2011). However, we found the resulting solution for
the projected covariance matrices to be numerically unstable. Therefore, we made the simplifying
∙-v
assumption that both the current Σ and the old covariance Σold commute with Σ. Under the common
premise of diagonal covariances, this commutativity assumption always holds. For the more general
case of arbitrary covariance matrices, we would need to ensure the matrices are sufficiently close
together, which is effectively ensured by Equation 8. Again, we introduce Lagrange multipliers
and solve the dual problem to obtain the optimal primal and dual variables (see Appendix B.4).
Note however, that here we chose the square root of the covariance matrix1 as primal variable. The
corresponding projection for the square root covariance Σ 1/2 is then
ς 1/2=ς⅛Withn=t
tr (I+∑-1∑ - 2∑-d∕2∑1∕2) - 1
(9)
Σ
where η is the corresponding Lagrangian multiplier. We see the same pattern emerging as for the
Frobenius projection. The chosen similarity measure reappears in the expression for the Lagrangian
multiplier and the primal variables are weighted averages of the corresponding parameters of the old
and the predicted Gaussian.
1We assume the true matrix square root Σ = ∑ 1/2 ∑ 1/2 and not a Cholesky factor Σ = LLT since it
naturally appears in the expressions for the projected covariance from the original Wasserstein formulation.
5
Published as a conference paper at ICLR 2021
(a) Frobenius
(b) Wasserstein
(c) KL
Figure 1: (a), (b), and (c): Interpolated covariances for the different projections for various values of
η. For Frobenius and Wasserstein the intermediate distributions clearly have a larger entropy, while
for the KL projection the intermediate entropy is smaller. (d): Entropy of the interpolated distribu-
tions. In this example π and πold have the same entropy. It can be seen that the entropy increases
for the Frobenius and Wasserstein projections when transitioning between the distributions, while it
decreases for the KL. A more general statement regarding this can be found in Theorem 1.
(d) Entropy
KL Projection. Identically to the previous two projections, we reformulate Equation 4 as
argmintr (∑-1Σ) + log |B|,	s.t. tr (∑-1∑) - d + log l⅛dl ≤ e∑,	(10)
∑	'	1 IςI	v i	IςI
where d is the dimensionality of the action space. It is impossible to acquire a fully closed form
solution for this problem. However, following Abdolmaleki et al. (2015), we can obtain the projected
precision Λ = Σ-1 by interpolation between the precision matrices of the old policy πold and the
current policy π
"Aid + λ	*
Λ = --	, η = arg min g(η), s.t. η ≥ 0,	(11)
η +1	η
where η is the corresponding Lagrangian multiplier and g(η) the dual function. While this dual
cannot be solved in closed form, an efficient solution exists using a standard numerical optimizer,
such as BFGS, since itis a 1-D convex optimization. Regardless, we want a differentiable projection
and thus also need to backpropagate the gradients through the numerical optimization. To this end,
we follow Amos & Kolter (2017) and compute those gradients by taking the differentials of the KKT
conditions of the dual. We refer to Appendix B.5 for more details and derivations.
Entropy Control. Previous works (Akrour et al., 2019; Abdolmaleki et al., 2015) have shown the
benefits of introducing an entropy constraint H(πθ) ≥ β in addition to the trust region constraints.
Such a constraint allows for more control over the exploration behavior of the policy. In order to
endow our algorithm with this improved exploration behavior, we make use of the results from
Akrour et al. (2019) and scale the standard deviation of the Gaussian distribution with a scalar factor
exp {(β - H(πθ)) /d}, which can also be individually computed per state.
4.3	Analysis of the Projections
It is instructive to compare the three projections. The covariance update is an interpolation for all
three projections, but the quantities that are interpolated differ. For the Frobenius projection we
directly interpolate between the old and current covariances (Equation 7), for the W2 projection
between their respective matrix square roots (Equation 9), and for the KL projection between their
inverses (Equation 11). In other words, each projection suggests which parametrization to use for
the covariance matrix. The different interpolations also have an interesting effect on the entropy of
the resulting covariances which can be observed in Figure 1. Further, we can prove the following
theorem about the entropy of the projected distributions
Theorem 1 Let πθ and πθold be Gaussian and η ≥ 0, then for the entropy of the projected distribution
H(Π) it holds that H(∏) ≥ minimum(H(∏θ), H(∏θold)) for the FrObeniUs (Equation 7) and the
Wasserstein projection (Equation 9), as well as, H(∏) ≤ maximum(H(∏θ), H(∏θoid)) for the KL
projection (Equation 11).
6
Published as a conference paper at ICLR 2021
Table 1: Mean return with 95% confidence interval of 20 epochs after completing 20% of the total
training and for the last 20 epochs. We trained 40 different seeds for each experiment and com-
puted five evaluation rollouts per epoch. The projections with (-E) and without entropy control are
considered separately, therefore, each column may have up to two best runs (bold).
	Hopper-v2		Walker2d-v2		Halfcheetah-v2		Ant-v2		Humanoid-v2	
	20%	final	20%	final	20%	final	20%	final	20%	final
FROB	1646 ± 19	2578 ± 17	2142±23	3443 ± 19	2525 ± 11	3552 ± 14	1265±11	3035 ± 26	2176 ± 65	5202 ± 23
W2	1586 ± 31	2490 ± 19	2284 ± 20	3390 ± 17	2586 ± 9	3692 ± 15	1362±12	3086 ± 28	2502 ± 87	5057 ± 25
KL	1584 ± 20	2476 ± 10	2071 ± 36	3583 ± 14	2369 ± 9	4255 ± 17	1460±26	3335 ± 21	2923 ±53	5510±27
PAPI	1378 ± 20	2549 ± 12	1663 ± 21	3232 ± 20	1875 ± 5	2380 ± 6	645 ± 5	3198 ± 17	1824 ± 74	5367 ± 22
PPO-M	1030 ± 23	2321 ± 19	1994 ±18	2771 ± 40	1922 ± 15	3272 ± 18	1494 ± 9	2783 ± 32	604 ± 10	5172 ± 23
PPO	1881±30	2515 ± 15	2490 ± 33	3447 ± 17	2048 ± 8	2880 ± 8	1657±10	2852 ± 25	1723 ± 67	4969 ±18
FROB-E	1587 ± 30	2478 ± 11	2037 ± 21	3370 ± 27	2762±10	4568 ± 14	730 ± 9	3475 ± 26	3662 ± 44	5807 ±18
W2-E	1518 ± 36	2437 ± 13	2174 ± 19	3303 ± 25	2266 ± 8	4213 ± 13	855 ± 27	3361 ± 30	3658 ± 56	5844 ± 8
KL-E	1502 ±18	2497 ± 16	2215 ± 31	3171 ± 28	2611± 12	4584 ± 18	955 ± 16	3437 ± 21	3801±42	5430±16
The proof is based on the multiplicative version of the Brunn-Minkowski inequality and can be
found in Appendix B.1. Intuitively, this implies that the Frobenius and Wasserstein projections
act more aggressively, i. e., they rather yield a higher entropy, while the KL projection acts more
conservatively, i. e., it rather yields a smaller entropy. This could also explain why many KL based
trust region methods lose entropy too quickly and converge prematurely. By introducing an explicit
entropy control, those effects can be mitigated.
4.4	Successive Policy Updates
The above projections can directly be implemented for training the current policy. Note, however,
that at each epoch i the policy πi predicted by the network before the projection layer does not
respect the constraints and thus relies on calling this layer. The policy of the projection layer ∏
not only depends on the parameters of ∏ but also on the old policy network ∏i,old = ∏i-ι. This
would result in an ever-growing stack of policy networks becoming increasingly costly to evaluate.
In other words, ∏ is computed using all stored networks of ∏i, ∏i-ι, ...,∏o. We now discuss the
parametrization of ∏ via amortized optimization.
We need to encode the information of the projection layer into the parameters θ of the next policy, i.e.
π(a∣s; θ) = P◦ ∏θ(a|s) is a composition function in whichP denotes the projection layer. The output
∙-v
of ∏θ is (μ, Σ), and P computes (μ, Σ) according Equations 6, 7, 9, or 11. Formally, We aim to find
a set of parameters θ* = argmin& Es 〜。亓阖[d (π(∙∣s), ∏θ (∙∣s))], where 夕冗湎 is the state distribution
of the old policy and d is the similarity measure used for the projection, such that we minimize the
expected distance or divergence between the projection and the current policy prediction.
The most intuitive way to solve this problem is to use the existing samples for additional regression
steps after the policy optimization. Still, this adds a computational overhead. Therefore, we propose
to concurrently optimize both objectives during training by penalizing the main objective, i. e.,
arg min E(s,a)〜∏θ ld
θ
π(a∣s; θ)
一πθold (a| S)
Aπold (s, a)
-αEs〜p∏old [d (n(1s； θ),πθ (Is))].
(12)
Note that the importance sampling ratio is computed based on a Gaussian distribution generated by
the trust region layer and not directly from the network output. Furthermore, the gradient for the
regression penalty does not flow through the projection, it is solely acting as supervised learning
signal. As appropriate similarity measures d for the penalty, we resort to the measures used in each
projection. For a detailed algorithmic view see Appendix A.
5 Experiments
Mujoco Benchmarks We evaluate the performance of our trust region layers regarding sample
complexity and final reward in comparison to PAPI and PPO on the OpenAI gym benchmark suite
(Brockman et al., 2016). We explicitly did not include TRPO in the evaluation, as Engstrom et al.
(2020) showed that it can can achieve similar performance to PPO. For our experiments, the PAPI
projection and its conservative PPO version are executed in the setting sent to us by the author. The
hyperparameters for all three projections and PPO have been selected with Optuna (Akiba et al.,
2019). See Appendix D for a full listing of all hyperparameters. We use a shared set of hyper-
parameters for all environments except for the Humanoid, which we optimized separately. Next
7
Published as a conference paper at ICLR 2021
Figure 2: (Left): Mean KL divergence for Ant-V2 as a standardizing measure to compare the policy
changes among all methods. (Center): Mahalanobis distance between the mean values of the unpro-
jected and old policy when using different α in comparison to a full regression. The mean bound
for the W2 projection is set to 0.03 (dotted black line). (Right): Mean cumulative reward with 95%
confidence interval based on 40 seeds for the semi-sparse 5-link Reacher task. For each method,
besides PAPI, We train policies with (dashed) and without (solid) contextual covariances.
to the standard PPO implementation with all code-level optimizations we further evaluate PPO-M,
which only leverages the core PPO algorithm. Our projections and PPO-M solely use the observa-
tion normalization, network architecture, and initialization from the original PPO implementation.
All algorithms parametrize the covariance as a non-contextual diagonal matrix. We refer to the
Frobenius projection as FROB, the Wasserstein projection as W2, and the KL projection as KL.
Table 1 gives an overview of the final performance and convergence speed on the Mujoco bench-
marks, Figure 4 in the appendix displays the full learning curves. After each epoch, we evaluate
five episodes without applying exploration noise to obtain the return values. Note that we initially
do not include the entropy projection to provide a fair comparison to PPO. The results show that
our trust region layers are able to perform similarly or better than PPO and PAPI across all tasks.
While the performance on the Hopper-v2 is comparable, the projections significantly outperform
all baselines on the HalfCheetah-v2. The KL projection even demonstrates the best performance
on the remaining three environments. Besides that, the experiments present a relatively balanced
performance between projections, PPO, and PAPI. The differences are more apparent when compar-
ing the projections to PPO-M, which uses the same implementation details as our projections. The
asymptotic performance of PPO-M is on par for the Humanoid-v2, but it convergences much slower
and is noticeably weaker on the remaining tasks. Consequently, the approximate trust region of PPO
alone is not sufficient for good performance, only paired with certain implementation choices. Still,
the original PPO cannot fully replace a mathematically sound trust region as ours, although it does
not exhibit a strong performance difference. For this, Figure 2 visualizes the mean KL divergence
at the end of each epoch for all methods. Despite the fact that neither W2 nor Frobenius projection
use the KL, we leverage it here as a standardizing measure to compare the change in the policy
distributions. All projections are characterized by an almost constant change, whereas for PPO-M
the changes are highly inconsistent. The code-level optimizations of PPO can mitigate this to some
extend but cannot properly enforce the desired constant change in the policy distribution. In partic-
ular, we have found that primarily the learning rate decay contributes to the relatively good behavior
of PPO. Albeit, PAPI provides a similar principled trust region projection as we do, it still has some
inconsistency by approaching the bound iteratively.
Entropy Control To demonstrate the effect of combining our projections with entropy control, as
described in Section 4.2, we evaluate all Mujoco tasks again for this extended setting. The target
entropy in each iteration i is computed by exponentially decaying the initial entropy Ho to K with
temperature T as K + (Ho - κ)τN, where N is the total number of training steps. The bottom of
Table 1 shows the results for our projections with entropy control. Especially on the more complex
tasks with more exploration, all three projections significantly benefit from the entropy control.
Their asymptotic performance for the HalfCheetah-v2, Ant-v2, and Humanoid-v2 increases and
yields a much faster convergence in the latter. For the other Mujoco tasks the performance remains
largely constant since the complexity of these tasks is insufficient to benefit from an explicit entropy
control, as also noted by Pajarinen et al. (2019) and Abdolmaleki et al. (2015).
8
Published as a conference paper at ICLR 2021
Contextual Covariances. To emphasize the advantage of state-wise trust regions we consider the
case of policies with state-dependent covariances. Existing methods, such as PPO and TRPO, are
rarely used in this setting. In addition, PAPI cannot project the covariance in the contextual case.
Further, Andrychowicz et al. (2020) demonstrated that for the standard Mujoco benchmarks, con-
textual covariances are not beneficial in an on-policy setting. Therefore, we choose to evaluate on
a task motivated from optimal control which benefits from a contextual covariance. We extend the
Mujoco Reacher-v2 to a 5-link planar robot, the distance penalty to the target is only provided in the
last time step, t = 200, and the observation space also contains the current time step t. This semi-
sparse reward specification imposes a significantly harder exploration problem as the agent is only
provided with a feedback at the last time step. We again tuned all hyperparameters using Optuna
Akiba et al. (2019) and did not include the entropy projection. All feasible approaches are compared
with and without contextual covariances, the results therefor are presented in Figure 2 (right). All
three projections significantly outperform the baseline methods with the non-contextual covariance.
Additionally, both the W2 and KL projection improve their results in the contextual case. In con-
trast, all baselines decrease in performance and are not able to leverage the advantage of contextual
information. This poor performance mainly originates from incorrect exploitation. PPO reduces the
covariance too quickly, whereas PAPI reduces it too slowly, leading to a suboptimal performance
for both. The Frobenius projection, however, does not benefit from contextual covariances either,
since numerical instabilities arise from too small covariance values close to convergence. Those
issues can be mitigated using a smaller covariance bound, but they cannot be entirely avoided. The
KL projection, while yielding the best results throughout all experiments, relies on a numerical op-
timization. Generally, this is computationally expensive, however, by leveraging an efficient C++
implementation this problem can be negated (see Appendix B.5). As a bonus, the KL projection has
all properties of existing KL-based trust region methods that have monotonic improvement guar-
antees. Nevertheless, for quick benchmarks, the W2 is preferred, given it is slightly less prone to
hyperparameter choices and does not require a dedicated custom implementation.
Trust Region Regression Loss. Lastly, we investigate the main approximation of our approach,
the trust region regression loss (Equation 12). In the following ablation, we evaluate how different
choices of the regression weight α affect the constraint satisfaction. Figure 2 (center) shows the
Mahalanobis distance between the unprojected and the old policy means for different α values. In
addition, for one run we choose α = 0 and execute the trust region regression separately after
each epoch for several iterations. One key observation is that decreasing the penalty up to a certain
threshold leads to larger changes in the policy and pushes the mean closer to its maximum bound.
Intuitively, this can be explained by the construction of the bound. As the penalty is added only to the
loss when the bound is violated, larger changes in the policy are punished while smaller steps do not
directly affect the loss negatively. By selecting a larger α, this behavior is reinforced. Furthermore,
we can see that some smaller values of α yield a behavior which is similar to the full regression
setting. Consequently, it is justified to use a computationally simpler penalty instead of performing
a full regression after each epoch.
6 Discussion and Future Work
In this work we proposed differentiable projection layers to enforce trust region constraints for
Gaussian policies in deep reinforcement learning. While being more stable than existing methods,
they also offer the benefit of imposing the constraints on a state level. Unlike previous approaches
that only constrain the expected change between successive policies and for whom monotonic im-
provement guarantees thus only hold approximately, we can constrain the maximum change. Our
results illustrate that trust regions are an effective tool in policy search for a wide range of different
similarity measures. Apart from the commonly used reverse KL, we also leverage the Wasserstein
distance and Frobenius norm. We demonstrated the subtle but important differences between those
three different types of trust regions and showed our benchmark performance is on par or better than
existing methods that use more code-level optimizations. For future work, we plan to continue our
research with more exploration-heavy environments, in particular with contextual covariances. Ad-
ditionally, more sophisticated heuristics or learning methods could be used to adapt the trust region
bounds for better performance. Lastly, we are interested in using our trust region layers for other
deep reinforcement learning approaches, such as actor-critic methods.
9
Published as a conference paper at ICLR 2021
References
A. Abdolmaleki, R. Lioutikov, J Peters, N. Lau, L. Reis, and G. Neumann. Model-based rel-
ative entropy stochastic search. In Advances in Neural Information Processing Systems 28,
pp. 3537-3545, 2015. URL http://www.ausy.tu-darmstadt.de/uploads/Team/
GerhardNeumann/Abdolmaleki_NIPS2015.pdf.
Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Mar-
tin Riedmiller. Maximum a posteriori policy optimisation. In International Conference on Learn-
ing Representations, 2018. URL https://openreview.net/forum?id=S1ANxQW0b.
Akshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, and Zico Kolter.
Differentiable Convex Optimization Layers. Advances in Neural Information Processing Systems,
32, oct 2019. URL http://arxiv.org/abs/1910.12430.
Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A
Next-generation Hyperparameter Optimization Framework. In Proceedings of the ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, pp. 2623-2631, 2019. URL
http://arxiv.org/abs/1907.10902.
Riad Akrour, Joni Pajarinen, Jan Peters, and Gerhard Neumann. Projections for approximate policy
iteration algorithms. In Proceedings of Machine Learning Research, pp. 181-190, 2019.
Brandon Amos and J. Zico Kolter. OptNet: Differentiable Optimization as a Layer in Neural
Networks. In 34th International Conference on Machine Learning, pp. 179-191, 2017. URL
http://arxiv.org/abs/1703.00443.
Marcin Andrychowicz, Anton Raichuk, Piotr Stanczyk, Manu Orsini, Sertan Girgin, Raphael
Marinier, Leonard Hussenot, MatthieU Geist, Olivier Pietquin, Marcin Michalski, Sylvain Gelly,
and Olivier Bachem. What Matters In On-Policy Reinforcement Learning? A Large-Scale Em-
pirical Study. In arXiv preprint, 2020. URL http://arxiv.org/abs/2006.05990.
Oleg Arenz, Mingjun Zhong, and Gerhard Neumann. Efficient Gradient-Free Variational Inference
using Policy Search. In Proceedings of Machine Learning Research, pp. 234-243, 2018. URL
http://proceedings.mlr.press/v80/arenz18a.html.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. OpenAI Gym. In arXiv preprint, 2016. URL http://arxiv.org/abs/
1606.01540.
Yinlam Chow, Ofir Nachum, Aleksandra Faust, Mohammad Ghavamzadeh, and Edgar Duenez-
Guzman. Lyapunov-based Safe Policy Optimization for Continuous Control. In ICML Workshop
RL4RealLife Submission, 2019.
Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Paduraru, and Yu-
val Tassa. Safe Exploration in Continuous Action Spaces. In arXiv preprint, 2018. ISBN
1801.08757v1. URL https://arxiv.org/abs/1801.08757.
Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking Deep
Reinforcement Learning for Continuous Control. 33rd International Conference on Machine
Learning, pp. 2001-2014, 2016. URL http://arxiv.org/abs/1604.06778.
Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry
Rudolph, and Aleksander Madry. Implementation Matters in Deep Policy Gradients: A Case
Study on PPO and TRPO. In International Conference on Learning Representations, 2020. URL
http://arxiv.org/abs/2005.12729.
Sham Kakade. A natural policy gradient. In Proceedings of the 14th International Conference
on Neural Information Processing Systems: Natural and Synthetic, NIPS’01, pp. 1531-1538,
Cambridge, MA, USA, 2001. MIT Press.
Sham M. Kakade and John C. Langford. Approximately Optimal Approximate Reinforcement
Learning. In Proceedings of the Nineteenth International Conference on Machine Learning, pp.
267-274, 2002. URL https://dl.acm.org/doi/10.5555/645531.656005.
10
Published as a conference paper at ICLR 2021
Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review.
CoRR, abs/1805.00909, 2018. URL http://arxiv.org/abs/1805.00909.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-End Training of Deep
Visuomotor Policies. In The Journal ofMachine Learning Research, volume 17, pp. 1334-1373,
2015. URL http://arxiv.org/abs/1504.00702.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learn-
ing. Nature, 518(7540):529-533, 2015. URL http://www.nature.com/articles/
nature14236.
Aldo Pacchiano, Jack Parker-Holder, Yunhao Tang, Anna Choromanska, Krzysztof Choroman-
ski, and Michael I Jordan. Learning to Score Behaviors for Guided Policy Optimization.
In Proceedings of the International Conference on Machine Learning, 2020. URL https:
//arxiv.org/abs/1906.04349.
Joni Pajarinen, Hong Linh Thai, Riad Akrour, Jan Peters, and Gerhard Neumann. Compatible
Natural Gradient Policy Search. Machine Learning, 108(8-9):1443-1466, 2019. URL http:
//arxiv.org/abs/1902.02823.
J.	Peters, K. Muelling, and Y. Altun. Relative entropy policy search. In Proceedings of the Twenty-
Fourth National Conference on Artificial Intelligence (AAAI), Physically Grounded AI Track,
2010. URL http://www.ias.informatik.tu-darmstadt.de/uploads/Team/
JanPeters/Peters2010_REPS.pdf.
Jan Peters and Stefan Schaal. Reinforcement learning of motor skills with policy gradi-
ents. Neural Networks, 21(4):682-697, 2008. URL https://www.ias.informatik.
tu-darmstadt.de/uploads/Team/JanPeters/Peters2010_REPS.pdf.
K.	B. Petersen and M. S. Pedersen. The matrix cookbook, 2012. URL http://www2.compute.
dtu.dk/pubdb/pubs/3274-full.html. Version 20121115.
Pierre H. Richemond and Brendan Maginnis. On Wasserstein Reinforcement Learning and the
Fokker-Planck equation. In arXiv preprint, 2017. URL http://arxiv.org/abs/1712.
07185.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust Region
Policy Optimization. In Proceedings of Machine Learning Research, pp. 1889-1897, 2015a. URL
http://proceedings.mlr.press/v37/schulman15.html.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
Dimensional Continuous Control Using Generalized Advantage Estimation. In International
Conference on Learning Representations, 2015b. URL http://arxiv.org/abs/1506.
02438.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy
Optimization Algorithms. In arXiv preprint, 2017. URL http://arxiv.org/abs/1707.
06347.
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur
Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap,
Karen Simonyan, and Demis Hassabis. Mastering Chess and Shogi by Self-Play with a Gen-
eral Reinforcement Learning Algorithm. In arXiv preprint, pp. 1-19, 2017. URL http:
//arxiv.org/abs/1712.01815.
H. Francis Song, Abbas Abdolmaleki, Jost Tobias Springenberg, Aidan Clark, Hubert Soyer, Jack W.
Rae, Seb Noury, Arun Ahuja, Siqi Liu, Dhruva Tirumala, Nicolas Heess, Dan Belov, Martin Ried-
miller, and Matthew M. Botvinick. V-mpo: On-policy maximum a posteriori policy optimization
for discrete and continuous control. In International Conference on Learning Representations,
2020. URL https://openreview.net/forum?id=SylOlp4FvH.
11
Published as a conference paper at ICLR 2021
Jun Song and Chaoyue Zhao. Optimistic Distributionally Robust Policy Optimization. In arXiv
preprint, pp. 15872-15882, 2020. URL https://arxiv.org/abs/2006.07815.
Asuka Takatsu. Wasserstein geometry of Gaussian measures. Osaka Journal of Mathematics, 48:
1005-1026, 2011.
Voot Tangkaratt, Abbas Abdolmaleki, and Masashi Sugiyama. Guide actor-critic for continuous
control. In Proceedings of the International Conference on Learning Representations, 2018.
URL https://arxiv.org/abs/1705.07606v2.
Cedric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,
2008.
Yuhui Wang, Hao He, Xiaoyang Tan, and Yaozhong Gan. Trust region-guided proximal policy
optimization. In Advances in Neural Information Processing Systems 32, pp. 626-636, 2019.
URL https://arxiv.org/abs/1901.10314.
Tsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, and Peter J Ramadge. Projection-Based
Constrained Policy Optimization. In International Conference on Learning Representations,
2020. URL https://sites.google.com/view/iclr2020-pcpo.
12
Published as a conference paper at ICLR 2021
A Algorithm
Algorithm 1 Differentiable Trust Region Layer. The trust region layer acts as final layer after
predicting a Gaussian distribution. It projects this predicted Gaussian onto the trust region in case it
is violating the specified bounds. As output it generates a projected mean and covariance that satisfy
the respective trust region bound. The entropy control in the last step can be disabled.
Initialize bounds tμ,e∑, temperature T as well as target K and initial entropy Ho.
1: procedure TRUSTREGIONLAYER(μ, Σ, μdd, Σ0ld)
2：	if dmean(μ, μoId) > Eμ then
3:	Compute μ with Equation 6
4:	else
5:	μ = μ
6:	ifdcov(Σ,Σold) > Σ then
∙-v
7:	Compute Σ with Equations 7, 9, or 11
8:	else
∙-v
9:	Σ = Σ ιo,
10:	β = κ + (Ho - κ)τN	. (Optional) entropy control as described in Section 4.2
11:	if H(Σ) < β then
12:	c = exp {(β - H(Σ)) /dim(a)}
∙-v	∙-v
13:	Σ = cΣ
∙-v
14:	return μ, Σ
Algorithm 2 Algorithmic view of the proposed Trust Region Projections. The trust region projec-
tions itself do not require approximations, the old policy update in the last step is the only point
where we introduce an approximation. This update would normally require additional supervised
regression steps that minimize the distance between the network output and the projection. How-
ever, by leveraging the regression penalty during policy optimization this optimization step can be
omitted. Both approaches yield a policy, which is independent of the old policy distribution, i. e.
it can act without the projection while maintaining the trust region. However, the penalty does not
require additional computation and the policy can directly generate new trajectories, equivalently to
other trust region methods, such as PPO.
1:	Initialize policy θo,o
2:	for i = 0, 1, . . . , N do	. epoch
3:	Collect set of trajectories Di = {τk} with π(θi,o)
ʌ
4:	Compute advantage estimates At with GAE
5:	for j = 0, 1, . . . , M do
6:	Use ∏(θi,j) to predict Gaussian action distributions N(μi,j, ∑i,j) for Di
7:	∏ = TRUSTREGIONLAYER(μi,j, ∑i,j", ∑i,o)	'	'
8:	Update policy with Adam using the following policy gradient:
θi,j+ι J Adam
n(a;%,o)At- αEs~p∏(θi,0) [d (π(∙ιs; θ),π(-∖s; θτ∣θ=θj
9:	Successive policy update: θi+1,o J θi,M
B Derivations
B.1 Proof of Theorem 1
This section provides a proof for Theorem 1. We mainly used the multiplicative version of the
Brunn-Minkowski inequality
log(α∣∑ι∣ + β∣∑2∣) ≥ log(∣∑1∣)αlog(∣∑2∣)β
where Σ1, Σ2 are p.s.d, α, β are positive, and α + β = 1.
13
Published as a conference paper at ICLR 2021
Frobenius Projection
H(Π) = 0.5 log ∣2πeΣ|
=0.5 log
≥ 0.5 log
2πe (Sς+Wh %d)
(2πeΣ)η+r det (2πe∑old)η+1
1	一^ 一 η	„
η+10.5log ∣2πe∑∣ + ^^O^log ∣2πe∑°ld∣
-ɪH(πθ) + -ɪH(πθold) ≥ minimum (H(πθ), H(πθold))
η + 1 η + 1
Wasserstein Projection Let k denote the dimensionality of the distributions under consideration.
H(π) = 0.5log(2πe)k∣∑ ∣
= 0∙5log(2πe”( ητι∑°5 + η¾ ∑0ldi )「
= 0.5log(2πe)k +log L∑0∙5 + ɪ ∑0lF I + log -4γ∑0.5 + ɪ ∑0l5
η + 1 η + 1	∣ η + 1 η + 1
≥ 0.5log(2πe)k + log ∣ Σ0.5 ∣ n+1 ∣ ∑0lji ∣ n+τ + log ∣ Σ0.5 ∣ n+1 ∣ ∑0lji ∣ n+τ
=0.5log(2πe)k + log ∣Σ∣n+τ ∣∑old∣n+τ
=0.5log (∣ (2πe∑∣n+τ ∣2πe∑old∣n+τ)
1η
=η+10.5log ∣2πe∑∣ + η-+j0.5log ∣2πe∑dd∣
1η
=η+ιH(πθ) + η+ιH(πθold) ≥ minimum (H(π), H(∏old))
KL Projection
∙-v
H(π) = 0.5 log ∣2πe∑∣
=0.5 log
η⅛(2πe∑)-1 + η⅛(2Ed)T
-1
= -0∙5log +(23-1 + η¾ (2πe∑old)-1
≤ -0.5log (∣(2πe∑)-1 ∣ 木 ∣ (2πe∑°ld)-1 ∣ 辞T)
=0.5 log (∣2πe∑∣ n+τ ∣2πe∑old∣n+τ)	(use the fact that: det(A-1) = 1/det(A))
1η
=η+70.5log ∣2πe∑∣ + η-+j0.5log ∣2πe∑°ld∣
1η
= ——H(∏θ) +——H(∏θold) ≤ maximum (H(∏θ), H(∏old))
η + 1 η + 1
B.2 MEAN PROJECTION
First, we consider only the mean objective
min (M - μ)T ∑-1 (M -谪
μ
s∙t∙	(Mold - a) ∑old (μold - ”) ≤ eμ,
which give us the following dual
L(μ, ω) = (μ -“尸 ∑0l1 (M - μ) + ω ((μold -“尸 ∑0ld (μold -a)-
(13)
14
Published as a conference paper at ICLR 2021
Differentiating w.r.t. μ yields
∂£(", ω)
-∂μ-
2Σ-1 (μ - μ) - 2ωΣ京(μ - μold).
Setting the derivative to 0 and solving for μ gives
~* μ + ωμoid
μ =^+ω-
Inserting the optimal mean μ* in Equation 13 results in
L(ω)
μ + ωμoid
1 + ω
-μ) ς-1
μ + ωμoid
1 + ω
((μ + ωμold	∖T °-i (μ + ωμ0ld	∖
+ ω	-μold	Uh (-μold -e^
ω2 (μ - MoId) ςo11 (μ - Mold)
(μ - μold)	(μ - μold)
ω
+
(1 + ω)2
Thus, differentiating w.r.t ω yields
(1 + ω)2
dL(ω) _ (μ - Mold)，ςoi1 (μ - μoid)
∂ω
(1+ ω)2
-eμ∙
Now solving 写/=0 for ω, We arrive at
(μ - μOtd) %1 (μ - μoId) - 1
B.3 Frobenius Covariance Projection
We consider the following objective for the covariance part
∙-v
mΣn 产-%
Σ
*
ω
1
2
	
with the corresponding Lagrangian
s.t.	∣∣ς -＜出|[ ≤ e∑
L(Σ ,η)=∣∣∑ - Σ∣∣: + η (∣∣∑ - ∑oid∣∣2-e∑
(14)
∙-v
Differentiating w.r.t. Σ yields
∙-v
dL∂∣^ = 2((∑ - ∑) + η (∑oid - ∑)).
∙-v
We can again solve for Σ by setting the derivative to 0, i.e.,
∑ * = ∑ + η∑oid
1 + η .
∙-v
Inserting Σ* into Equation 14 yields the dual function
g(η)
∑ + η∑oid
1 + η
F+η (II ∑≡d-∑oid
F
Differentiating w.r.t. η results in
Hence,嗡) = 0 yields
∂L(η) _ ∣∣∑ - ∑oid∣∣F
---二---- -----:-----~^7..-
∂η (1 + η)2
η* = k∑ - ∑oid∣∣F - 1
√e∑
15
Published as a conference paper at ICLR 2021
B.4 Wasserstein Covariance Projection
As described in the main text, the Gaussian distributions to have been rescaled by Σo-ld1 to measure
the distance in the metric space that is defined by the variance of the data. For notational simplicity,
we show the derivation of the covariance projection only for the unscaled scenario. The scaled
version can be obtained by a simple redefinition of the covariance matrices. For our covariance
projection we are interested in solving the following optimization problem
min tr (∑ + Σ - 2 (∑”2ΣΣ1∕2)")
s.t.	tr (∑ + ∑oid — 2 (∑0/d∑∑0/:) 1/] ≤ %
which leads to the following Lagrangian function
L(Σ, η) = tr (∑ + Σ - 2 (∑1∕2ΣΣ1/2) 1/2)
+ η (tr (∑ + ∑oid - 2(∑0^∑∑0/;)1/) -e∑) .	(15)
∙-v
Assuming that Σ commutes with Σ as well as Σold, Equation 15 simplifies to
L(Σ, η) = tr (∑ + Σ - 2Σ 1∕2Σ1∕2) + η (tr (∑ + Σ°kl - 2Σ**：/；) - e)
=tr(S2 + ∑ - 2SΣ1/2) + η (tr 卜2 + ∑oid - 2S∑0/， - e) ,	(16)
where S is the unique positive semi-definite root of the positive semi-definite matrix Σ, i.e.
S = Σ1/2. Instead of optimizing the objective w.r.t Σ, We optimize w.r.t S in order, which greatly
simplifies the calculation. That is, we solve
∂ LS,η
∂S
(1 + η)2S - 2(∑1∕2 + n£：/d) = 0
for S, which leads us to
S* = ∑1/2 + n£：/d ς*	= ∑ + η2∑old + 2η∑1∕2∑o^
1 + η ,	(1 + η)2
Inserting this into Equation 16 yields the dual function
η (tr (∑ + ∑old - 2∑1∕2∑o∕d))
g(η) = —— --------；—-------------- — η∑
1+η
The derivative of the dual w.r.t. η is given by
∂L(η) _ tr 0 + £old- 2ς 1/£/d)
~~∂^Γ =	(1 + λ)2	'”
Now solving dLn) = 0 for η, we arrive at
tr (∑ + ∑old- 2∑1∕2∑o∕d
η* = ∖--------------------------
Σ
-1
B.5 KL-Divergence Projection
We derive the KL-Divergence projection in its general form, i.e., simultaneous projection of mean
and covariance under an additional entropy constraint
∏* = arg min KL (Π∣∣∏θ) s.t. KL (∏∣∣∏θ°ld) ≤ 3	H (∏) ≥ β∙
16
Published as a conference paper at ICLR 2021
Instead of working with this minimization problem we consider the equivalent maximization prob-
lem
∏* = argmax -KL (Π∣∣∏θ)	s.t. KL (∏∣∣∏θ0h) ≤ e, H (∏) ≥ β,	(17)
π
which is similar to the one considered in Model Based Relative Entropy Stochastic Search (MORE)
(Abdolmaleki et al., 2015), with a few distinctions. To see those distinctions let η and ω denote the
Lagrangian multipliers corresponding to the KL and entropy constraint respectively and consider
the Lagrangian corresponding to the optimization problem in Equation 17
L = -KL(n|n) + η(e - KL(n||n&Old)) + ω (Hm)- β)
=En [log∏θ] + η (e - KL(π∣∣∏θ0ld)) + (ω + 1)H(π) - ωβ.
Opposed to Abdolmaleki et al. (2015) we are not working with an unknown reward but using the
log density of the target distribution π instead. Thus we do not need to fit a surrogate and can
directly read off the parameters of the squared reward. They are given by the natural parameters of
∏, i.e, A = Σ-1 and q = Σ-1μ. Additionally, We need to add a constant 1 to ω to account for the
additional entropy term in the original objective, similar to (Arenz et al., 2018).
FolloWing the derivations from Abdolmaleki et al. (2015) and Arenz et al. (2018) We can obtain a
closed form solution for the natural parameters of ∏, given the Lagrangian multipliers η and ω
A =必普+A and q=%—.	(18)
η+1+ω	η+1+ω
To obtain the optimal Lagrangian multipliers We can solve the folloWing convex dual function using
gradient descent
g(η, ω) =ηe - ωβ + η (-2qTdA-jqold + 1 log det(A) - 2 log(2π))
+ (η + 1 + ω) (2qrATq - 2 log det (A) + 2 log(2π)) + const,
5 = e - KL(∏H∏θold) and " = H(∏) - β∙
Given the optimal Lagrangian multipliers, η* and ω* we obtain the parameters of the optimal distri-
bution π* using Equation 18.
Forward Pass For the forward pass we compute the natural parameters of π, solve the optimiza-
tion problem and compute mean and covariance of ∏* from the optimal natural parameters. The
corresponding compute graph is given in Figure 3.
Figure 3: Compute graph of the KL projection layer. The layer first computes the natural parameters
of π from the mean and covariance. Then it numerically optimizes the dual to obtain the optimal
Lagrangian multipliers which are used to get the optimal natural parameters. Ultimately, the optimal
mean and covariance are computed from the optimal natural parameters. We omit the dependency
on constants, i.e., the bound e and β as well as the parameters of πold for clarity of the visualization.
17
Published as a conference paper at ICLR 2021
Backward Pass Given the computational graph in Figure 3 gradients can be propagated back
though the layer using standard back-propagation. All gradients for the analytical computations
(black arrows in Figure 3) are straight forward and can be found in (Petersen & Pedersen, 2012).
For the gradients of the numerical optimization of the dual (red arrows in Figure 3) we follow Amos
& Kolter (2017) and differentiate the KKT conditions around the optimal Lagrangian multipliers
computed during the forward pass. The KKT Conditions of the dual are given by
Vg(η*,ω*) + mTV (H) = (-HK⅛(∏-lβθ-)m-,m1) =0, (Stationarity)
mι(一η*) = 0 and m2(-ω*) = 0 (Complementary Slackness)
here m = (m1, m2)T denotes the Lagrangian multipliers for the box constraints of the dual (η and
ω need to be non-negative). Taking the differentials of those conditions yields the equation system
∂ ∂KL (Π*∣∣∏θoid)
∂∣J
∂ H(π*)
∂η*
-m1
0
∂KL (Π*∣∣∏θoid)
∂ω*
∂H(Π*)
∂ω*
0
-m2
-1
0
~ *
-η
0
-1
0
-ω*
dη
dω
dm1
dm2
∕∂KL (Π*∣∣∏θoid)
dq +
∂ KL (Π*∣∣∏θoid) dΛ
∂H(Π*),
一λ—dq -
∂q
0
0
∂Λ
∂ H(Π*)
∂ Λ (
dΛ
q
	
∖
/
which is (analytically) solved to obtain the desired partial derivatives
∂η ∂η ∂ω
, and
∂ω
∂Λ ,
Implementation We implemented the whole layer using C++, Armadillo, and OpenMP for par-
allelization. The implementation saves all necessary quantities for the backward pass and thus a
numerical optimization is only necessary during the forward pass. Before we perform a numerical
optimization We check whether it is actually necessary. If the target distribution ∏ is within the trust
region we immediately can set ∏* = ∏θ, i.e., the forward and backward pass become the identity
mapping. This check yield significant speed-ups, especially in early iterations, if the target is still
close to the old distribution. If the projection is necessary we use the L-BFGS to optimize the 2D
convex dual, which is still fast. For example, for a 17-dimensional action space and a batch size of
512, such as in the Humanoid-v2 experiments, the layer takes roughly 170ms for the forward pass
and 3.5ms for the backward pass if the all 512 Gaussians are actually projected2. If none of the
Gaussians needs to be projected its less than 1ms for forward and backward pass.
Simplifications If only diagonal covariances are considered the implementation simplifies sig-
nificantly, as computationally heavy operations (matrix inversions and cholesky decompositions)
simplify to pointwise operations (divisions and square roots). If only the covariance part of the KL
is projected, we set μ0ld = μ = μ* and dμ = 0 which is again a simplification for both the deriva-
tions and implementation. If an entropy equality constraint, instead of an inequality constraint, it is
sufficient to remove the ω > 0 constraint in the dual optimization.
2On a 8 Core Intel Core i7-9700K CPU @ 3.60GHz
18
Published as a conference paper at ICLR 2021
Hopper-v2
Walker2d-v2
C Additional Results
Figure 4 shows the training curves for all Mujoco environments with a 95% confidence interval.
Besides the projections we also show the performance for PAPI and PPO. In Figure 5 the projections
also leverage the Entropy control based on the results from from Akrour et al. (2019).
Ant-v2
——FROB
——KL
PPO-M
draweR
draweR
HalfCheetah-v2
0	0.5	1	1.5	2	2.5	3
Samples	.产
Humanoid-v2
draweR
0	0.5	1	1.5	2	2.5	3
Samples	.106
0	0.2	0.4	0.6	0.8	1
Samples	.107
≡
Figure 4:	Training curves for the projection layer as well as PPO and PAPI on the test environment.
We trained 40 agents with different seeds for each environment using five evaluation episodes for
every data point. The plot shows the total mean reward with 95% confidence interval.
Hopper-v2
Samples
0.8	1
∙106
PPO
Walker2d-v2
1.5	2	2.5	3
Samples	.产
0	0.5	1
HalfCheetah-v2
0	0.5	1	1.5	2	2.5	3
Samples	.产
4,000
3,000
2,000
1,000
Ant-v2
0	0.5	1	1.5	2	2.5	3
Samples	.产
6,000
4,000
Humanoid-v2
2,000
0	0.2	0.4	0.6	0.8	1
Samples	107
0
Figure 5:	Training curves for the projection layer with entropy control (-E) as well as PPO and PAPI
on the test environment. We trained 40 agents with different seeds for each environment using five
evaluation episodes for every data point. The plot shows the total mean reward with 95% confidence
interval.
19
Published as a conference paper at ICLR 2021
D Hyperparameters
Tables 2 and 3 show the hyperparameters used for the experiments in Table 1. Target entropy,
temperature, and entropy equality are only required when the entropy projection is included in the
layer, otherwise those values are ignored.
Table 2: Hyperparameters for all three projections as well as PAPI, PPO. and PPO-M on the Mujoco
benchmarks from Table 1
Frobenius W2 KL PAPI PPO PPO-M				
rollouts		2048		
GAE λ		0.95		
discount factor		0.99		
Eμ∕f	0.03	0.015	n.a.	
Σ	0.001		n.a.	
target entropy	0		n.a.	
temperature	0.5		n.a.	
entropy equality	False	False	n.a.	
optimizer		adam		
epochs vf		10		
epochs	20		10	
lr	5e-5		3e-4	
lr vf	4.5e-4		2.5e-4	
minibatch size	32		64	
trust region loss weight α	8.0		n.a.	
entropy loss penalty		0		
normalized observations		True		
normalized rewards	False	True		False
observation clip	n.a.	10		n.a.
reward clip	n.a.	10		n.a.
vf clip	n.a.	0.2		n.a.
importance ratio clip	n.a.		0.2	
contextual std		False		
hidden layers		[64, 64]		
hidden layers vf		[64, 64]		
hidden activation		tanh		
20
Published as a conference paper at ICLR 2021
Table 3: Hyperparameters for all three projection as well as PAPI and PPO on the Humanoid-v2
from Table 1.
	FrobeniUs W2	KL PAPI PPO	PPO-M
rollouts		16384	
GAE λ		0.95	
discount factor		0.99	
Eμ∕f	0.05	0.015	n.a.
Σ	1e-4	0.01	1e-4	n.a.	
target entropy	0	n.a.	
temperature	0.2	n.a.	
entropy equality	TrUe	False	n.a.
optimizer		adam	
epochs vf		10	
epochs		10	
lr	1e-4	1e-4	
lr vf	4.5e-4	1e-4	
minibatch size	256	512	
entropy loss penalty		0	
trust region loss weight α	8.0	n.a.	
normalized observations		TrUe	
normalized rewards	False	TrUe	False
observation clip	n.a.	10	n.a.
reward clip	n.a.	10	n.a.
vf clip	n.a.	0.2	n.a.
importance ratio clip	n.a.	0.2	
contextual std		False	
hidden layers		[64, 64]	
hidden layers vf		[64, 64]	
hidden activation		tanh	
21
Published as a conference paper at ICLR 2021
Table 4: Hyperparameters for all three projection as well as PAPI and PPO on our ReacherSparse-
v0 task from Figure 2. The second value for Σ of the KL projection is the bound when using a
contextual covariance.
	Frobenius	W2	KL	PAPI PPO	PPO-M
rollouts			16384		
GAE λ			0.95		
discount factor			0.99		
Cμ∣e		0.03		0.03	n.a.
Σ	5e-5	1e-3	5e-5∣1e-3	n.a.	
target entropy		n.a.		n.a.	
temperature		n.a.		n.a.	
entropy equality		n.a.		False	n.a.
optimizer			adam		
epochs vf			10		
epochs			20		
lr		3e-4		1e-4	
lr vf		4.5e-4		1e-4	
minibatch size		256		512	
entropy loss penalty			0		
trust region penalty α		8.0		n.a.	
normalized observations			True		
normalized rewards		False		True	False
observation clip		n.a.		10	n.a.
reward clip		n.a.		10	n.a.
vf clip		n.a.		0.2	n.a.
importance ratio clip		n.a.		0.2	
contextual std
hidden layers
hidden layers vf
hidden activation
False
[64, 64]
[64, 64]
tanh
22