Published as a conference paper at ICLR 2021
CcGAN: Continuous Conditional Generative
Adversarial Networks for Image Generation
Xin Ding∖ YongWei Wang*, Zuheng Xu, William J. Welch, Z. Jane Wang,
The University of British Columbia
{xin.ding@stat, yongweiw@ece, zuheng.xu@stat, will@stat,
zjanew@ece}.ubc.ca
Ab stract
This work proposes the continuous conditional generative adversarial network
(CcGAN), the first generative model for image generation conditional on contin-
uous, scalar conditions (termed regression labels). Existing conditional GANs
(cGANs) are mainly designed for categorical conditions (e.g., class labels); condi-
tioning on regression labels is mathematically distinct and raises two fundamental
problems: (P1) Since there may be very few (even zero) real images for some
regression labels, minimizing existing empirical versions of cGAN losses (a.k.a.
empirical cGAN losses) often fails in practice; (P2) Since regression labels are
scalar and infinitely many, conventional label input methods (e.g., combining a
hidden map of the generator/discriminator with a one-hot encoded label) are not
applicable. The proposed CcGAN solves the above problems, respectively, by (S1)
reformulating existing empirical cGAN losses to be appropriate for the continuous
scenario; and (S2) proposing a novel method to incorporate regression labels into
the generator and the discriminator. The reformulation in (S1) leads to two novel
empirical discriminator losses, termed the hard vicinal discriminator loss (HVDL)
and the soft vicinal discriminator loss (SVDL) respectively, and a novel empirical
generator loss. The error bounds of a discriminator trained with HVDL and SVDL
are derived under mild assumptions in this work. A new benchmark dataset, RC-49,
is also proposed for generative image modeling conditional on regression labels.
Our experiments on the Circular 2-D Gaussians, RC-49, and UTKFace datasets
show that CcGAN is able to generate diverse, high-quality samples from the image
distribution conditional on a given regression label. Moreover, in these experiments,
CcGAN substantially outperforms cGAN both visually and quantitatively.
1 Introduction
Conditional generative adversarial networks (cGANs), first proposed in (Mirza & Osindero, 2014),
aim to estimate the distribution of images conditioning on some auxiliary information, especially
class labels. Subsequent studies (Odena et al., 2017; Miyato & Koyama, 2018; Brock et al., 2019;
Zhang et al., 2019) confirm the feasibility of generating diverse, high-quality (even photo-realistic),
and class-label consistent fake images from class-conditional GANs. Unfortunately, these cGANs do
not work well for image generation with continuous, scalar conditions, termed regression labels, due
to two problems:
(P1) cGANs are often trained to minimize the empirical versions of their losses (a.k.a. the empirical
cGAN losses) on some training data, a principle also known as the empirical risk minimization
(ERM) (Vapnik, 2000). The success of ERM relies on a large sample size for each distinct condition.
Unfortunately, we usually have only a few real images for some regression labels. Moreover, since
regression labels are continuous, some values may not even appear in the training set. Consequently,
a cGAN cannot accurately estimate the image distribution conditional on such missing labels.
(P2) In class-conditional image generation, class labels are often encoded by one-hot vectors or label
embedding and then fed into the generator and discriminator by hidden concatenation (Mirza &
* Equal contribution
1
Published as a conference paper at ICLR 2021
Osindero, 2014), an auxiliary classifier (Odena et al., 2017) or label projection (Miyato & Koyama,
2018). A precondition for such label encoding is that the number of distinct labels (e.g., the number
of classes) is finite and known. Unfortunately, in the continuous scenario, we may have infinite
distinct regression labels.
A naive approach to solve (P1)-(P2) is to “bin” the regression labels into a series of disjoint intervals
and still train a cGAN in the class-conditional manner (these interval are treated as independent
classes) (Olmschenk, 2019). However, this approach has four shortcomings: (1) our experiments
in Section 4 show that this approach often makes cGANs collapse; (2) we can only estimate the
image distribution conditional on membership in an interval and not on the target label; (3) a large
interval width leads to high label inconsistency; (4) inter-class correlation is not considered (images
in successive intervals have similar distributions).
In machine learning, vicinal risk minimization (VRM) (Vapnik, 2000; Chapelle et al., 2001) is an
alternative rule to ERM. VRM assumes that a sample point shares the same label with other samples
in its vicinity. Motivated by VRM, in generative modeling conditional on regression labels where we
estimate a conditional distribution p(x|y) (x is an image and y is a regression label), it is natural to
assume that a small perturbation to y results in a negligible change to p(x|y). This assumption is
consistent with our perception of the world. For example, the image distribution of facial features for
a population of 15-year-old teenagers should be close to that of 16-year olds.
We therefore introduce the continuous conditional GAN (CcGAN) to tackle (P1) and (P2). To our
best knowledge, this is the first generative model for image generation conditional on regression
labels. It is noted that Rezagholizadeh et al. (2018) and Rezagholiradeh & Haidar (2018) train
GANs in an unsupervised manner and synthesize unlabeled fake images for a subsequent image
regression task. Olmschenk et al. (2019) proposes a semi-supervised GAN for dense crowd counting.
CcGAN is fundamentally different from these works since they do not estimate the conditional image
distribution. Our contributions can be summarized as follows:
•	We propose in Section 2 the CcGAN to address (P1) and (P2), which consists of two novel
empirical discriminator losses, termed the hard vicinal discriminator loss (HVDL) and the soft
vicinal discriminator loss (SVDL), a novel empirical generator loss, and a novel label input
method. We take the vanilla cGAN loss as an example to show how to derive HVDL, SVDL, and
the novel empirical generator loss by reformulating existing empirical cGAN losses.
•	We derive in Section 3 the error bounds of a discriminator trained with HVDL and SVDL.
•	In Section 4, we propose a new benchmark dataset, RC-49, for the generative image modeling
conditional on regression labels, since very few benchmark datasets are suitable for the studied
continuous scenario. We conduct experiments on several datasets, and our experiments show
that CcGAN not only generates diverse, high-quality, and label consistent images, but also
substantially outperforms cGAN both visually and quantitatively.
2	FROM CGAN TO CCGAN
In this section, we provide the solutions (S1)-(S2) to (P1)-(P2) in a one-to-one manner by introducing
the continuous conditional GAN (CcGAN). Please note that theoretically cGAN losses (e.g., the
vanilla cGAN loss (Mirza & Osindero, 2014), the Wasserstein loss (Arjovsky et al., 2017), and the
hinge loss (Miyato et al., 2018)) are suitable for both class labels and regression labels; however,
their empirical versions fail in the continuous scenario (i.e., (P1)). Our first solution (S1) focuses
on reformulating these empirical cGAN losses to fit into the continuous scenario. Without loss
of generality, we only take the vanilla cGAN loss as an example to show such reformulation (the
empirical versions of the Wasserstein loss and the hinge loss can be reformulated similarly).
The vanilla discriminator loss and generator loss (Mirza & Osindero, 2014) are defined as:
L(D) = -Ey~pr(y) [Ex~pr(x|y) [log (D(X,y))]] - Ey~Pg(y) [Ex~pg(x|y) [log (1 - D(X,y))]]
log(D(X, y))pr(X, y)dXdy -
log(1 - D(X, y))pg (X, y)dXdy,
(1)
2
Published as a conference paper at ICLR 2021
L(G) = -Ey~pg(y) [Ez~q(z) [log (D(G(z,y),y))]] = - J log(D(G(z,y),y))q(z)pg(y)dzdy,
(2)
where x ∈ X is an image of size d × d, y ∈ Y is a label, pr (y) and pg (y) are respectively the
true and fake label marginal distributions, pr(x|y) and pg(x|y) are respectively the true and fake
image distributions conditional on y, pr(x, y) andpg(x, y) are respectively the true and fake joint
distributions of x and y, and q(z) is the probability density function ofN(0, I).
Since the distributions in the losses of Eqs. (1) and (2) are unknown, for class-conditional image
generation, Mirza & Osindero (2014) follows ERM and minimizes the empirical losses:
1 C Ncr	1 C Ncg
Lδ(D) = - Nr XXlog(D(Xr,j,c))-而 XXlog(1 — D(Xg,j,c)),	⑶
1 C Ncg
Lδ (G) = — N XX log(D(G(zc,j ,c),c)),	(4)
N c=1 j=1
where C is the number of classes, N r and N g are respectively the number of real and fake images,
Ncr and Ncg are respectively the number of real and fake images with label c, Xcr,j and Xcg,j are
respectively the j-th real image and the j-th fake image with label c, and the zc,j are independently
and identically sampled from q(z). Eq. (3) implies we estimate pr(X, y) and pg(X, y) by their
empirical probability density functions as follows:
1 C Ncr	1 C Ncg
Pr(χ,y) =	NrXXδ(X-xc,j)δ(y-C),	Pg(x,y) =而XXδ(X-xg,j)δ(y-C),⑸
where δ(∙) is a Dirac delta mass centered at 0. However, p:(x, y) and Pg(x, y) in Eq. (5) are not
good estimates in the continuous scenario because of (P1).
To overcome (P1), we propose a novel estimate for each ofPr(X, y) andPg(X, y), termed the hard
vicinal estimate (HVE). We also provide an intuitive alternative to HVE, named the soft vicinal
estimate (SVE). The HVEs of Pr(X, y) and Pg(X, y) are:
PHVE(X,y) = Ci ∙ N1r Xeχp 卜 (y2σyj)
PHVE(X,y) = C2 ∙ NXeχp (-⅛r
1	Nr
NT~ El{∣y-yr∣≤κ}δ(X - Xi)
y,κ i=1
Ng
X Uy-yg∣≤κ}δ(X -Xg)
i=1
(6)
where Xi and Xig are respectively real image i and fake image i, yi and yigare respectively the labels
of Xir and Xig, κ and σare two positive hyper-parameters, C1 and C2 are two constants making these
two estimates valid probability density functions, Nyr,κ is the number of the yir satisfying |y -yir| ≤ κ,
Ny,κ is the number of the yf satisfying |y - y? | ≤ κ, and Iisan indicator function with support in
the subscript. The terms in the first square brackets of PHVE and PHVE imply we estimate the marginal
label distributions Pr(y) andPg(y) by kernel density estimates (KDEs) (Silverman, 1986). The terms
in the second square brackets are designed based on the assumption that a small perturbation to y
results in negligible changes to Pr(X|y) andPg(X|y). If this assumption holds, we can use images
with labels in a small vicinity of y to estimate Pr(X|y) and Pg(X|y). The SVEs of Pr(X, y) and
Pg (X, y) are:
PrVE(X,y) = C3
PgVE(X,y) = C4
1 X fSXn (_(y - yr)2! 1 "PNIWr(yr,y~)δ(X - Xr)
Nr j=1 P I-F2^	PNI Wr (yr ,y)一一
1 Xexn (_(y-yg)2∖∖	"PNgIwg(yg,y^Q-Xg)
Ng j=1	I	2σ2	PNIWg(yi,y)
(7)
3
Published as a conference paper at ICLR 2021
where C3 and C4 are two constants making these two estimates valid probability density functions,
wr(yir, y) = e-ν(yir-y)2 and wg(yig, y) = e-ν(yig-y)2 ,	(8)
and the hyper-parameter ν > 0. In Eq. (7), similar to the HVEs, we estimate pr (y) and pg (y) by
KDEs. Instead of using samples in a hard vicinity, the SVEs use all respective samples to estimate
pr(x|y) and pg(x|y) but each sample is assigned with a weight based on the distance of its label
from y . Two diagrams in Fig. 1 visualize the process of using hard/soft vicinal samples to estimate
p(x|y), i.e., a univariate Gaussian distribution conditional on its mean y.
Figure 1: HVE (Eq. (6)) and SVE (Eq. (7)) estimate p(x|y) (a univariate Gaussian conditional on y)
using two samples in hard and soft vicinities, respectively, of y. To estimate p(x|y) (the red Gaussian
curve) only from samples drawn from p(x|y1) and p(x|y2) (the blue Gaussian curves), estimation is
based on the samples (red dots) in a hard vicinity (defined by y ± κ) or a soft vicinity (defined by the
weight decay curve) around y. The histograms in blue are samples in the hard or soft vicinity. The
labels y1 , y, and y2 on the x-axis denote the means of x conditional on y1 , y, and y2 , respectively.
By plugging Eq. (6) and (7) into Eq. (1), we derive the hard vicinal discriminator loss (HVDL) and
the soft vicinal discriminator loss (SVDL) as follows:
LbHVDL (D)=- Nr X X Eer 〜N (0,σ2) " "1N「yr'} 3。(吗,% + J ))
j=1 i=1	yjr +r,κ
j	(9)
Ng Ng	Γ -ri	v 7
-	Ng XX Eeg 〜N (0,σ2)	j- "-D(xg ,yg + Cg)),
j=1 i=1	yjg +eg,κ
MDLC	C XXF	"	Wr(yr,yj + J)
L	(D) = - Nr g HEer~N(0,σ2) IPNl Wr(yr,yr + er)
C8 XXE	「Wg(yi,yj + eg)
-	Ng j=1 ⅛Ef (0,σ2) [pNgιWg(yg,yj + eg)
log(D(xir,yjr+Cr))
log(1 - D(xig, yjg + eg)) ,
(10)
where Cr , y - yjr, Cg , y - yjg, and C5, C6 , C7, and C8 are some constants.
Generator training: The generator of CcGAN is trained by minimizing Eq. (11),
Ng
1
Le(G) = -而 EEeg〜N(0,σ2) log(D(G(Zi,yg + eg),yg + eg)).	(11)
How do HVDL, SVDL, and Eq. (11) overcome (P1)? The solution (S1) includes:
(i)	Given a label y as the condition, we use images in a hard/soft vicinity of y to train the discriminator
instead of just using images with label y. It enables us to estimate pr(x|y) when there are not enough
real images with label y .
(ii)	From Eqs. (9) and (10), we can see that the KDEs in Eqs. (6) and (7) are adjusted by adding
Gaussian noise to the labels. Moreover, in Eq. (11), we add Gaussian noise to seen labels (assume
4
Published as a conference paper at ICLR 2021
yig’s are seen) to train the generator to generate images at unseen labels. This enables estimation of
pr(x|y0) when y0 is not in the training set.
How is (P2) solved? We propose a novel label input method. For G, we add the label y element-
wisely to the output of its first linear layer. For D, an extra linear layer is trained together with D to
embed y in a latent space. We then incorporate the embedded label into D by the label projection
(Miyato & Koyama, 2018). Please refer to Supp. S.3 for more details.
Remark 1. An algorithm is proposed in Supp. S.2 for training CcGAN in practice. Moreover,
CcGAN does not require any specific network architecture, therefore it can also use the state-of-art
architectures in practice such as SNGAN (Miyato et al., 2018) and BigGAN (Brock et al., 2019).
3 Error bounds
In this section, we derive the error bounds of a discriminator trained with LbHVDL and LbSVDL under the
theoretical loss L. First, without loss of generality, we assume y ∈ [0, 1]. Then, we introduce some
notations. Let D stand for the Hypothesis Space of D. Let p^KDE(y) and PKDE(y) stand for the KDEs
of Pr (y) and Pg (y) respectively. Let Pw (y0∣y)，Wr (Wy)胪川,Pw (y0∣y)，Wg(Wy", Wr (y)，
W wr (y0, y)Pr (y0)dy0 and Wg(y)，/ wg (y0, y)Pg(y0)dy0. Denote by D* the optimal discriminator
(Goodfellow et al., 2014) which minimizes L but may not be in D. Let D , arg minD∈DL(D). Let
Db HVDL , arg minD∈DLbHVDL(D); similarly, we define Db SVDL.
Definition 1. (Holder Class) Define the Holder class of functions
Σ(L) , {P : ∀t1, t2 ∈ Y, ∃L > 0, s.t.|P0(t1) -P0(t2)| ≤ L|t1 - t2|} .	(12)
Please see Supp. S.5.1 for more details of these notations. Moreover, we will also work with the
following assumptions: (A1) All D’s in D are measurable and uniformly bounded by U. Let U ,
max{supD∈D [- log D] , supD∈D [- log(1 - D)]} and U < ∞; (A2) For ∀x ∈ X and y, y0 ∈ Y,
∃gr (x) > 0 and Mr > 0, s.t. |Pr(x|y0) - Pr (x|y)| ≤ gr (x)|y0 - y| with R gr (x)dx = Mr; (A3)
For ∀x ∈ X and y, y0 ∈ Y, ∃gg(x) > 0 and Mg > 0, s.t. |Pg(x|y0) - Pg (x|y)| ≤ gg (x)|y0 - y|
with R gg(x)dx = Mg; (A4) Pr (y) ∈ Σ(Lr) andPg(y) ∈ Σ(Lg).
Theorem 1.	Assume that (A1)-(A4) hold, then ∀δ ∈ (0, 1), with probability at least 1 - δ,
L(DbHVDL) - L(D*)
≤2u(SCKκNσN + L σ2) +2u(S
+2U j1log (δ) (Ey~pKDE⑺ Nrκ~
for some constants C1K,Dδ E, C2K,DδE depending on δ.
C2K,DδE log Ng
Ngσ
+ Lgσ2	+ κU(Mr + Mg)
+ Ey~P KDE (y)
+ L(De) - L(D*),
(13)
TNo
Theorem 2.	Assume that (A1)-(A4) hold, then ∀δ ∈ (0, 1), with probability at least 1 - δ,
L(DbSVDL) - L(D*)
三汨尸丁 + L σ2) +2U(SC≡ + Lg。^
TT /T一/16、( 1 m	Γ 1	1	1 m	Γ 1 lʌ (14)
+ 2UV2log(τ) (√NrEy~pKDE⑺[wr^)J + √NgEy~pKDE⑺[而不∖)
+ U (MrEy^pK,DE(y) [Ey0~pw(y0∣y) |y0 - y|] + MgEy~pKDE(y) [Ey0~pW(y0∣y) |y0 - y|])
+ L(De) - L(D*),
for some constant C1K,DδE, C2K,DδE depending on δ.
5
Published as a conference paper at ICLR 2021
Remark 2. The error bounds in both theorems reflect the distance of DbHVDL and DbSVDL from D*.
Enlightened by the two upper bounds, when implementing CcGAN, we should (1) avoid letting D
output extreme values (close to 0 or 1) so that U is kept at a moderate level; (2) avoid using a too
small or a too large κ or ν to keep the third and fourth terms moderate in Eqs. (13) and (14). Please
see Supp. S.5.2.5 for a more detailed interpretation and Supp. S.5.2 for the proofs.
4	Experiment
In this section, we study the effectiveness of CcGAN on three datasets where cGAN (Mirza &
Osindero, 2014) cannot generate realistic samples. For a fair comparison, cGAN and CcGAN use the
same network architecture (a customized architecture for Circular 2-D Gaussians and the SNGAN
(Miyato et al., 2018) architecture for RC-49 and UTKFace) except for the label input modules. For
stability, image labels are normalized to [0, 1] in the RC-49 and UTKFace datasets during training.
4.1	Circular 2-D Gaussians
We first test on the synthetic data generated from 120 2-D Gaussians with different means.
Experimental setup: The means of the 120 Gaussians are evenly arranged on a unit circle centered
at the origin O of a 2-D space. The Gaussians share a common covariance matrix σ2I2×2, where
σ = 0.02. We generate 10 samples from each Gaussian for training. Fig. 2a shows 1,200 training
samples (blue dots) from these Gaussians with their means (red dots) on a unit circle. The unit circle
can be seen as a clock where we take the mean at 12 o’clock (point A) as the baseline point. Given
another mean on the circle (point B), the label y for samples generated from the Gaussian with mean
B is defined as the clockwise angle (in radians) between line segments OA and OB. E.g., the label
for samples from the Gaussian at A is 0. Both cGAN and our proposed CcGAN are trained on this
training set. When implementing cGAN, angles are treated as class labels (each Gaussian is treated
as a class); while when implementing CcGAN, angles are treated as real numbers. The network
architectures of cGAN and CcGAN are shown in Supp. S.6.1. Both cGAN and CcGAN are trained
for 6,000 iterations. We use the rule of thumb formulae in Supp. S.4 to select the hyper-parameters
of HVDL and SVDL, i.e., σ ≈ 0.074, κ ≈ 0.017 and ν = 3600 (see Supp. S.6.2 for details).
For testing, we choose 360 points evenly distributed on the unit circle as the means of 360 Gaussians.
For each Gaussian, we generate 100 samples, yielding a test set with 36,000 samples. It should be
noted that, among these 360 Gaussians, at least 240 are not used at the training. In other words, there
are at least 240 labels in the testing set which do not appear in the training set. For each test angle,
we generate 100 fake samples from each trained GAN, yielding 36,000 fake samples from each GAN
in total. The quality of these fake samples is evaluated. We repeat the whole experiment three times
and report in Table 1 the average quality over three repetitions.
Evaluation metrics and quantitative results: In the label-conditional scenario, each fake sample x
with label y is compared with the mean (sin(y), cos(y)) ofa Gaussian on the unit circle with label
y. A fake sample is defined as ”high-quality” if its Euclidean distance from x to (sin(y), cos(y)) is
smaller than 4σ = 0.08. A mode (i.e., a Gaussian) is said to be recovered if at least one high-quality
sample is assigned to it. We also measure the quality of fake samples with label y by computing
the 2-Wasserstein Distance (W2) (Peyre et al., 2019) betweenPr (x|y) = N([sin(y), cos(y)]1, σI)
and Pg(x|y) = N(μg, Σg), where we assume Pg(x|y) is Gaussian and its mean and covariance are
estimated by the sample mean and sample covariance of 100 fake samples with label y. In Table 1, we
report the average percentage of high-quality fake samples and the average percentage of recovered
modes over 3 repetitions. We also report the average W2 over 360 testing angles. We can see CcGAN
substantially outperforms cGAN.
Visual results: We select 12 angles which do not appear in the training set. We then use cGAN and
CcGAN to generate 100 samples for each unobserved angle. Fig. 2 visually confirms the obervation
from the numerical metrics: the fake samples from the two CcGAN methods are more realistic.
4.2	RC-49
Since most benchmark datasets in the GAN literature do not have continuous, scalar regression labels,
we propose a new benchmark dataset—RC-49, a synthetic dataset created by rendering 49 3-D chair
6
Published as a conference paper at ICLR 2021
Table 1: Average quality of 36,000 fake samples from cGAN and CcGAN over three repetitions with
standard deviations after the “士” SymboL “3”(“T")indicates lower (higher) values are preferred.
Method	% High Quality ↑	% Recovered Modes ↑ 2-Wasserstein Dist. 3
CGAN (120 classes) 68.8 ± 4.8	81.8 ± 3.9	3.32 X 10-2 ± 3.13 X 10-2
CcGAN (HVDL)	99.3	± 0.4	100.0	± 0.0	3.03 ×	10-4	± 5.05	× 10-5
CcGAN (SVDL)	99.6	± 0.1	100.0	± 0.0	2.56 ×	10-4	± 8.95	× 10-6
-10	-0∙5	Q。	0∙5	1.0	-1.0	-0.5	0.0	0.5	1.0	-1.0	-0.5	0.0	0.5	1.0	-1.0	-0.5	0.0	0.5
(a) 1200 training samples (b) cGAN	(c) CcGAN (HVDL) (d) CcGAN (SVDL)
and 120 means (red dots).
Figure 2: Visual results for the Circular 2-D Gaussians simulation. (a) shows 1,200 training samples
from 120 Gaussians, with 10 samples per Gaussian. In (b) to (d), each GAN generates 100 fake
samples at each of 12 means not appearing in the training set, where green and blue dots stand for
fake and real samples respectively.
models at different yaw angles. Each of 49 chair models is rendered at 899 yaw angles ranging from
0.1 to 89.9 with step size 0.1. Therefore, RC-49 consists of 44,051 64 X 64 rendered RGB images
and 899 distinct angles. Please see Supp. S.7 for more details of the data generation. Some example
images are shown in Fig. 3.
Experimental setup: Not all images are used for the cGAN and CcGAN training. A yaw angle
is selected for training if its last digit is odd. Moreover, at each selected angle, only 25 images are
randomly chosen for training. Thus, the training set includes 11250 images and 450 distinct angles.
The remaining images are held out for evaluation.
When training cGAN, we divide [0.1, 89.9] into 150 equal intervals where each interval is treated as
a class. When training CcGAN, we use the rule of thumb formulae in Supp. S.4 to select the three
hyper-parameters of HVDL and SVDL, i.e., σ ≈ 0.047, κ ≈ 0.004 and ν = 50625. Both cGAN and
CcGAN are trained for 30,000 iterations with batch size 256. Afterwards, we evaluate the trained
GANs on all 899 angles by generating 200 fake images for each angle. Please see Supp. S.7 for the
network architectures and more details about the training/testing setup.
Quantitative and visual results: To evaluate (1) the visual quality, (2) the intra-label diversity, and
(3) the label consistency (whether assigned labels of fake images are consistent with their true labels)
of fake images, we study an overall metric and three separate metrics here. (i) Intra-FID (Miyato &
Koyama, 2018) is utilized as the overall metric. It computes the FreChet inception distance (FID)
(Heusel et al., 2017) separately at each of the 899 evaluation angles and reports the average FID score.
(ii) Naturalness Image Quality Evaluator (NIQE) (Mittal et al., 2012) measures the visual quality
only. (iii) Diversity is the average entropy of predicted chair types of fake images over evaluation
angles. (iv) Label Score is the average absolute error between assigned labels and predicted labels.
Please see Supp. S.7.5 for details of these metrics.
We report in Table 2 the performances of each GAN. The example fake images in Fig. 3 and line
graphs in Fig. 5 support the quantitative results. cGAN often generates unrealistic, identical images
for a target angle (i.e., low visual quality and low intra-label diversity). “Binning” [0.1, 89.9] into
other number of classes (e.g., 90 classes and 210 classes) is also tried but does not improve cGAN’s
performance. In contrast, strikingly better visual quality and higher intra-label diversity of both
CcGAN methods are visually evident. Please note that CcGAN is designed to sacrifice some (not too
much) label consistency for better visual quality and higher diversity, and this explains why CcGAN
does not outperform cGAN in terms of the label score in Table 2.
7
Published as a conference paper at ICLR 2021
Table 2: Average quality of 179,800 fake RC-49 images from cGAN and CcGAN with standard
deviations after the “土” SymboL "]”(“？”)indicates lower (higher) values are preferred.
Method	Intra-FID J	NIQE J	Diversity ↑	Label Score J
cGAN (150 classes)	1.720 ± 0.384	2.731 ± 0.162	0.779 ± 0.199	4.815 ± 5.152
CcGAN (HVDL)	0.612 ± 0.145	1.869 ± 0.181	2.353 ± 0.121	5.617 ± 4.452
CcGAN (SVDL)	0.515 ± 0.181	1.853 ± 0.159	2.610 ± 0.113	4.982 ± 4.439
4.3	UTKFACE
In this section, we compare CcGAN and cGAN on UTKFace (Zhang et al., 2017), a dataset consisting
of RGB images of human faces which are labeled by age.
Experimental setup: In this experiment, we only use images with age in [1, 60]. Some images with
bad visual quality and watermarks are also discarded. After the preprocessing, 14,760 images are left.
The number of images for each age ranges from 50 to 1051. We resize all selected images to 64 × 64.
Some example UTKFace images are shown in the first image array in Fig.4.
When implementing cGAN, each age is treated as a class. For CcGAN we use the rule of thumb
formulae in Supp. S.4 to select the three hyper-parameters of HVDL and SVDL, i.e., σ ≈ 0.041,
κ ≈ 0.017 and ν = 3600. Both cGAN and CcGAN are trained for 40,000 iterations with batch size
512. In testing, we generate 1,000 fake images from each trained GAN for each age. Please see Supp.
S.8 for more details of data preprocessing, network architectures and training/testing setup.
Quantitative and visual results: Similar to the RC-49 experiment, we evaluate the quality of fake
images by Intra-FID, NIQE, Diversity (entropy of predicted races), and Label Score. We report
in Table 3 the average quality of 60,000 fake images. We also show in Fig. 4 some example fake
images from cGAN and CcGAN and line graphs of FID/NIQE versus ages in Fig. 5. Analogous to
the quantitative comparisons, we can see that CcGAN performs much better than cGAN.
Table 3: Average quality of 60,000 fake UTKFace images from cGAN and CcGAN with standard
deviations after the “土” SymboL "]”(“？”)indicates lower (higher) values are preferred.
Method	Intra-FID J	NIQE J	Diversity ↑	Label Score J
cGAN (60 classes)	4.516 ± 0.965	2.315 ± 0.306	0.254 ± 0.353	11.087 ± 8.119
CcGAN (HVDL)	0.572 ± 0.167	1.739 ± 0.145	1.338 ± 0.178	9.782 ± 7.166
CcGAN (SVDL)	0.547 ± 0.181	1.753 ± 0.196	1.326 ± 0.198	10.739 ± 8.340
Figure 4: Three UTKFace example images for
each of 10 ages: real images and example fake
images from cGAN and two proposed CcGANs,
respectively. CcGANs produce face images with
higher visual quality and more diversity.
Figure 3: Three RC-49 example images for each
of 10 angles: real images and example fake im-
ages from cGAN and two proposed CcGANs,
respectively. CcGANs produce chair images
with higher visual quality and more diversity.
8
Published as a conference paper at ICLR 2021
(a) RC-49: FID vs Angle (b) RC-49: NIQE vs Angle (c) UTKFace: FID vs Age (d) UTKFace: NIQE vs Age
Figure 5: Line graphs of FID/NIQE versus regression labels on RC-49 and UTKFace. Figs. 5(a) to
5(d) show that two CcGANs consistently outperform cGAN across all regression labels. The graphs
of CcGANs also appear smoother than those of cGAN because of HVDL and SVDL.
一⅛M<
-WlItCajMI(HWl)
-wnɪɪ »；«< Kra.)
3C
*⅛e
5	Conclusion
As the first generative model, we propose the CcGAN in this paper for image generation conditional
on regression labels. In CcGAN, two novel empirical discriminator losses (HVDL and SVDL), a
novel empirical generator loss and a novel label input method are proposed to overcome the two
problems of existing cGANs. The error bounds of a discriminator trained under HVDL and SVDL
are studied in this work. A new benchmark dataset, RC-49, is also proposed for the continuous
scenario. Finally we demonstrate the superiority of the proposed CcGAN to cGAN on the Circular
2-D Gaussians, RC-49, and UTKFace datasets.
Acknowledgments
This work was supported by the Natural Sciences and Engineering Research Council of Canada
(NSERC) under Grants CRDPJ 476594-14, RGPIN-2019-05019, and RGPAS2017-507965.
References
Zeynep Akata, Florent Perronnin, Zaid Harchaoui, and Cordelia Schmid. Label-embedding for image
classification. IEEE transactions on pattern analysis and machine intelligence, 38(7):1425-1438,
2015.
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
volume 70 of Proceedings of Machine Learning Research, pp. 214-223, International Convention
Centre, Sydney, Australia, 2017. PMLR.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity
natural image synthesis. In International Conference on Learning Representations, 2019.
Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li,
Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d
model repository. arXiv preprint arXiv:1512.03012, 2015.
Olivier Chapelle, Jason Weston, Leon Bottou, and Vladimir Vapnik. Vicinal risk minimization. In
Advances in neural information processing systems, pp. 416-422, 2001.
Harm De Vries, Florian Strub, Jeremie Mary, Hugo Larochelle, Olivier Pietquin, and Aaron C
Courville. Modulating early visual processing by language. In Advances in Neural Information
Processing Systems, pp. 6594-6604, 2017.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa-
tion processing systems, pp. 2672-2680, 2014.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In Advances
in Neural Information Processing Systems, pp. 6626-6637, 2017.
9
Published as a conference paper at ICLR 2021
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing
and improving the image quality of styleGAN. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 8110-8119, 2020.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio
and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint
arXiv:1411.1784, 2014.
Anish Mittal, Rajiv Soundararajan, and Alan C Bovik. Making a “completely blind” image quality
analyzer. IEEE Signal processing letters, 20(3):209-212, 2012.
Takeru Miyato and Masanori Koyama. cGANs with projection discriminator. In International
Conference on Learning Representations, 2018.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxiliary
classifier GANs. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pp. 2642-2651, 2017.
Greg Olmschenk. Semi-supervised Regression with Generative Adversarial Networks Using Minimal
Labeled Data. PhD thesis, 2019.
Greg Olmschenk, Jin Chen, Hao Tang, and Zhigang Zhu. Dense crowd counting convolutional neural
networks with minimal data using semi-supervised dual-goal generative adversarial networks. In
IEEE Conference on Computer Vision and Pattern Recognition: Learning with Imperfect Data
Workshop, 2019.
Gabriel Peyre, Marco Cuturi, et al. Computational optimal transport. Foundations and Trends® in
Machine Learning, 11(5-6):355-607, 2019.
Mehdi Rezagholiradeh and Md Akmal Haidar. Reg-GAN: Semi-supervised learning based on gener-
ative adversarial networks for regression. In 2018 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pp. 2806-2810. IEEE, 2018.
Mehdi Rezagholizadeh, Md Akmal Haidar, and Dalei Wu. Semi-supervised regression with generative
adversarial networks, November 22 2018. US Patent App. 15/789,518.
Bernard W Silverman. Density estimation for statistics and data analysis, volume 26. CRC press,
1986.
Vladimir Vapnik. The nature of statistical learning theory. Springer, 2nd edition, 2000.
Larry Wasserman. Density estimation @ONLINE. http://www.stat.cmu.edu/~larry/
=sml/densityestimation.pdf.
Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative
adversarial networks. In Proceedings of the 36th International Conference on Machine Learning,
volume 97, pp. 7354-7363, 2019.
Zhifei Zhang, Yang Song, and Hairong Qi. Age progression/regression by conditional adversarial
autoencoder. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 5810-5818, 2017.
Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han. Differentiable augmentation for
data-efficient gan training. In Advances in neural information processing systems, 2020.
10
Published as a conference paper at ICLR 2021
Supplementary Material
S.1	GitHub repository
Please find the codes for this paper at Github:
https://github.com/UBCDingXin/improved_CcGAN
S.2	Algorithms for CcGAN training
Algorithm 1: An algorithm for CcGAN training with the proposed HVDL.
Data: Nr real image-label pairs Ωr = {(xr,y1),..., (xNr, yN r)}, NUy ordered distinct labels
Υ
{y[r1] , . .
. , y[rNury] }
in the dataset, preset σ and κ, number of iterations K, the discriminator
batch size md, and the generator batch size mg .
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
Result: Trained generator G.
for k = 1 to K do
Train D;
Draw md labels Y d with replacement from Υ;
Create a set of target labels Y d, = {yi + |yi ∈ Y d, ∈ N (0, σ2), i = 1, . . . , md} (D training is
conditional on these labels) ;
Initialize Ωd = φ, Ωf = φ;
for i = 1 to md do
Randomly choose an image-label pair (x, y) ∈ Ωr satisfying |y — yi — e| ≤ K where
yi + e ∈ Yd,e and let Ωd = Ωd ∪ (x,yi + e).;
Randomly draw a label y0 from U (yi + — κ, yi + + κ) and generate a fake image x0 by
evaluating G(z, y0), where Z 〜N(0, I). Let Ωfi = Ωf ∪ (x0, yi + e).;
end
Update D with samples in set Ωd and Ωf via gradient-based optimizers based on Eq.(6);
Train G;
Draw mg labels Yg with replacement from Υ;
Create another set of target labels Yg, = {yi + |yi ∈ Yg, ∈ N(0, σ2), i = 1, . . . , mg} (G
training is conditional on these labels) ;
Generate mg fake images conditional on Yg,e and put these image-label pairs in Ωf ;
Update G with samples in Ωf via gradient-based optimizers based on Eq.(11);
end
Remark S.3. It should be noted that, for computational efficiency, the normalizing constants
Nyrjr+r,κ, Nygg+g,κ, PiN=1 wr(yir, yjr + r), and PiN=g1 wg(yig, yjg + g) in Eq. (9) and (10) are
excluded from the training and only used for theoretical analysis.
S.3	More details of the proposed label input method in Section 2
We propose a novel way to input labels to the conditional generative adversarial networks. For the
generator, we add a regression label element-wise to the feature map of the first linear layer. For the
discriminator, labels are first projected to a latent space learned by an extra linear layer. Then, we
incorporate the embedded labels into the discriminator by the label projection (Miyato & Koyama,
2018). Figs. S.3.6 and S.3.7 visualizes our proposed label input method. Please refer to our codes for
more details.
S.4	A rule of thumb for hyper-parameter selection
In our experiments, we normalize labels to real numbers in [0, 1] and the hyper-parameter selection is
conducted based on the normalized labels. To be more specific, the hyper-parameter σ is computed
based on a rule-of-thumb formula for the bandwidth selection of KDE (Silverman, 1986), i.e.,
11
Published as a conference paper at ICLR 2021
Algorithm 2: An algorithm for CcGAN training with the proposed SVDL.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
Data: Nr real image-label pairs Ωr = {(xr,y1),..., (xNr, yN r)}, NUy ordered distinct labels
Υ = {y[r1] , . . . , y[rNr] } in the dataset, preset σ and ν, number of iterations K, the discriminator
batch size md, and the generator batch size mg .
Result: Trained generator G.
for k = 1 to K do
Train D;
Draw md labels Y d with replacement from Υ;
Create a set of target labels Y d, = {yi + |yi ∈ Y d, ∈ N (0, σ2), i = 1, . . . , md} (D training is
conditional on these labels) ;
Initialize Ωd = φ, Ωf = φ;
for i = 1 to md do
Randomly choose an image-label pair (x,y) ∈ Ωr satisfying e-ν(y-yi-e)2 > 10-3 where
yi + e ∈ Yd,e and let Ωd = Ωd ∪ (x,yi + e). This step is used to exclude real images with too
small weights. ;
Compute wir (y, yi + ) = e-ν(yi+-y)2 ;
Randomly draw a label y0 from U (yi +
e -√- logν-3 ,yi + e +
log 10-3
ν
) and generate a
fake image x0 by evaluating G(z, y0), where Z 〜N(0, I). Let Ωf = Ωfi ∪ (x0, yi + e).;
end
Compute wig (y0, yi + ) = e-ν(yi+-y0)2 ;
-
Update D with samples in set Ωd and Ωf via gradient-based optimizers based on Eq.(7);
Train G;
Draw mg labels Yg with replacement from Y;
Create another set of target labels Yg,e = {yi + e|yi ∈ Yg, e ∈ N (0, σ2), i = 1,..., mg } (G
training is conditional on these labels);
Generate mg fake images conditional on Yg,e and put these image-label pairs in Ωf ;
Update G with samples in Ωf via gradient-based optimizers based on Eq.(11);
end
noise	label	Image	label
Figure S.3.6:	The label input method for the
generator in CcGAN.
Figure S.3.7:	The label input method for the
discriminator in CcGAN.
σ =(43；「/3Nr) 1/5, where ^^yr is the sample standard deviation of normalized labels in the training
set. Let Kbase = max (y^ - y[1] ,y[3] - yQ ,…，y"r] - y；Nr -1]), where y^ is the l-th smallest
12
Published as a conference paper at ICLR 2021
normalized distinct real label and Nury is the number of normalized distinct labels in the training set.
The κ is set as a multiple of κbase (i.e., κ = mκκbase) where the multiplier mκ stands for 50% of the
minimum number of neighboring labels used for estimating pr(x|y) given a label y. For example,
mκ = 1 implies using 2 neighboring labels (one on the left while the other one on the right). In our
experiments, mκ is generally set as 1 or 2. In some extreme case when many distinct labels have
too few real samples, We may consider increasing m%. We also found V = 1∕κ2 works well in our
experiments.
S.5	More details of Theorems S.4 and S.5
S.5.1	Some necessary definitions and notations
•	The hypothesis space D is a set of functions that can be represented by D (a neural network
with determined architecture).
•	In the HVDL case, denote by pry,κ(x) , Ryy-+κκ pr(x|y0)pr(y0)dy0 the marginal distribution
of real images with labels in [y - κ, y + κ] and similarly to pgy,κ(x) of fake images.
•	In the SVDL case, given y and weight functions (E.q.	(8)), if the number of
real and fake images are infinite, the empirical density converges to pry,wr (x) ,
Rpr(χ∣y0)Wr(Wy犷。)dy0 and pg,wg(χ) , RPg(x|y0)Wg(W,g⅞⑶dy0 respectively,
where Wr(y) , Rwr(y0,y)pr(y0)dy0 and Wg(y) , R wg(y0, y)pg(y0)dy0.
•	LetPW(y0∣y)，Wr(Wy)：；(J) andPW(y0∣y)，Wg(Wy)yg⑺.
•	The Holder Class defined in Definition 1isa set of functions with bounded second derivatives,
which controls the variation of the function when parameter changes. (A4) implies the two
probability density functions Pr (y) and Pg (y) are assumed in the Holder Class.
•	Given a G, the optimal discriminator which minimizes L is in the form of
D*(	) =	Pr(X,y)
(,y) = Pr(χ,y)+ Pg(x,y).
(S.15)
However, D* may not be covered by the hypothesis space D. The D is the minimizer of
L in the hypothesis space D. Thus, L(D) - L(D*) should be a non-negative constant. In
CcGAN, we minimize LbHVDL(D) or LbHVDL (D) with respect to D ∈ D, so we are more
interested in the distance of DbHVDL and DbSVDL from D*, i.e., L(DHVDL) - L(D*) and
L(D SVDL)-L(D*).
S.5.2	Proofs of Theorems 1 and 2
S.5.2.1 Technical lemmas
Before we move to the proofs of Theorems 1 and 2, we provide several technical lemmas used in the
later proof.
Recall notations and assumptions in Sections 3 and S.5.1, then we derive the following lemmas.
Lemma S.1. Suppose that (A1)-(A2) and (A4) hold, then ∀δ ∈ (0, 1), with probability at least 1 - δ,
sup
D∈D
l{∣y-yrI≤κ} [- log D(Xr ,y)] - Eχ~pr(x|y)[- log D(x,y)]
+
KUM r
-2-
(S.16)
for a given y.
13
Published as a conference paper at ICLR 2021
Proof. Triangle inequality yields
∖ 1	Nr	∖
SUp ∖Nr~ ∑1{∣y-yr l≤κ}[— log D(Xr,y)] - Eχ~pr (χ∣y) [- log D(x,y)]
D∈D ∖ Nyr,κ i=1 i	∖
∖∖ 1	Nr
≤ sup ∖n^- EI{∣y-yrι≤κ}[— logD(Xr,y)] — Eχ~py,κ(χ)[— logD(x,y)]
D∈D ∖ Ny,κ i=1
+ SUp ∖Eχ~pr∙κ(x) [— log D(χ,y)] — Eχ~pr(x∣y) [— log D(χ, y)] ∖
D∈D
We then bound the two terms of the RHS separately as follows:
1.	Real images with labels in [y- κ,k + κ] can be seen as independent samples from pry,κ(x).
Then the first term can be bounded by applying Hoeffding’s inequality as follows: ∀δ ∈
(0, 1), with at least probability 1 - δ,
∖ 1	Nr
DUDwJI {ly-yrl≤κ}
≤ uSk log (δ).
U - log D(XKy)-
-Ex~py,κ(x)
2. For the second term, by the definition of pry,κ(x) and defining pκ (y0)
we have
U - log D(χ,y)
(S.17)
1{∣y0-y∣≤κ}P(yO)
R 1{∣y0-y∣≤κ}P(y0)dy0 ,
U
U
SUp ∖Eχ~py,κ(x) [— log D(x, y)] — Eχ~pr(x|y) [— log D(x, y)] ∖
D∈D
(by the definition of total variation and the boundness of — log D)
≤ U2 / ∖py,κ(x) — Pr(χ∖y)∖ dx.
Then, focusing on ∖pry,κ(x) — pr(x∖y)∖,
(S.18)
∖pry,κ (x) — pr(x∖y)∖
p(x∖y')pκ(y')dy' — p(x∖y)
≤	|p(x|y0) - p(x|y)| pκ(y0)dy0
(by (A2))
≤ /gr(χ)∖y' - y∣Pκ(y0)dy0
≤ κgr (x).
Thus, Eq. (S.18) is upper bounded as follows,
SUp ∣Eχ~py"(x) [— logD(x,y)] — Eχ~pr(x∣y) [— logD(χ,y)]∖
D∈D
≤	κgr (x)dx
(by (A2))
= κMr.
(S.19)
By combining Eq. (S.17) and (S.19), We can get Eq. (S.16), which finishes the proof.	口
Similarly, we apply identical proof strategy to the fake images xg and generator distributionpg(x∖y).
14
Published as a conference paper at ICLR 2021
Lemma S.2. Suppose that (A1), (A3) and (A4) hold, then ∀δ ∈ (0, 1), with probability at least 1 - δ,
1 Ng
SuP I	Zg	ɪs	l{∣y-yg∣≤κ}	[—	log(1 - D(xi,	y))] -	Ex~pg(x∣y)	[—	log(1 - D(x, y))]
D∈D Ny,κ i=1	i
≤ Uf2⅛ log (2) +
κUMg
(S.20)
for a given y.
Proof. This proof is omitted because it is almost identical to the one for Lemma S.1.
The following two lemmas provide the bounds for SVDL.
Lemma S.3. Suppose that (A1), (A2) and (A4) hold, then ∀δ ∈ (0, 1), with probability at least 1 - δ,
sup
D∈D
N PNIWr (yr ,y)[- log D(Xr ,y)]
-Eχ~pr(χ∣y) [- log D(X, y)]
≤ Wr(y) V 2Nr
4 UMr	0
log ( δ )+	2~Ey0~pw(y0∣y)[|y - y|]，
(S.21)
U
2
□
for a given y.
Proof. For brevity, denote by f(x, y) = - log D(x, y) and F = - log D. Then,
sup
D∈D
N PNIWr (yr ,y)[- log D(Xr,y)]
-Eχ~Pr(χ∣y) [- log D(x, y)]
sup
f∈F
-Eχ~pr (χ∣y) [f(X, y)]
(S.22)
≤ sup
f∈F
Eχ~py,wr (x) [f(x, y)]
+ SuP |Ex-pr，wr (x) [f (X, y)] -
Ex~Pr (x|y) [f (X, y)]||
f∈F	r
where the inequality is by triangular inequality. We then derive bounds for both two terms of the last
line.
1. For the first term, we can further split it into two parts,
Nr PNPNn ,y,- ExSwr (χ) [f(x，y)]
≤
+
--------ΓVΣ-------------
N pi=ιWr (yr ,y)
N PNIWr (yr ,y)f (Xi,y)
Wr(y)
(S.23)
Wr(y)
Eχ~py,wr (x) [f (x, y)]
Focusing on the first part of RHS of Eq.(S.23). By (A1),
—
Wr(y)
≤U
Wr(y)
15
Published as a conference paper at ICLR 2021
Note that ∀y,y0,wr(y0,y) = e-My-y— ≤ 1 and hence given y, Wr (y0,y) is a random
variable bounded by 1. Apply Hoeffding’s inequality to the numerator of above, yielding
that with probability at least 1 - δ0,
N PNIWr(yr,y)f (Xr,y) _ N PNIWr (y ,y)f (XrM	U I 1 l_72λ
N PNI Wr(yr,y)	W^	≤ EV2 * * * *Nr og Vδ77.
(S.24)
Then, consider the second part of RHS of Eq.(S.23).
Rpr(χ∣y0) Wr(Wyy心0 dy7.τhus,
Recall that pry,wr (X)
N PNIWr (yr,y)f (Xr,y)
Wr (y)
- Ex-py,wr (x) [f(X, y)]
1
Wr (y)
1 Nr
NEWr(yr,y)f(xi,y) - E(x,y，)~pr(x,y)[Wr(y,y)f(xr,y)]
i=1
wherepr(X, y7) = pr(X|y7)pr(y7) denotes the joint distribution of real image and its label.
Again, since Wr(y7, y)f(Xir, y) is uniformly bounded by U under (A1), we can apply
Hoeffding’s inequality. This implies that with probability at least 1 - δ7, the above can be
upper bounded by
U
Wo
(S.25)
Combining Eq. (S.24) and (S.25) and by setting δ7 = 2, we have with probability at least
1 - δ ,
N PNIWr (yr ,y)f (χr,y)
N PNIWr (yr ,y)
Eχ~py,wr (x) [f (X, y)]
≤ WUUy) V2iNrlog (4
Since this holds for ∀f ∈ F, taking supremum over f, we have
sup
f∈F
Eχ~py,wr (x) [f (X, y)]
≤ WUy){2Nlog (4).
(S.26)
2. For the second term on the RHS of Eq.(S.22). By (A1) that |f| < U,
sup lEx~py,wr (x) [f(X, y)] -
Ex~Pr (x|y) [f (X, y)]ll
≤ Ukpry,wr(X) -pr(X|y)kTV
=UU J ∣py,wr (X)- Pr (X∣y)∣dX.
Note that by the definition of Pyw(x)，Rpr(X∣y7)W (Wy(y)(y )dy7 and PW (y7∣y)，
wr(y0,y)pr(y0)
'W r [y) ，, We have
r
|pry,w (X) - pr(X|y)
pr(X|y7)prw (y7|y) dy7 -pr(X|y)
≤	|pr(X|y7) -pr(X|y)|prw (y7|y) dy7.
By (A.2) and y ∈ [0,1], the above is upper bounded by gr (X)Ey0~?2(yo|y)[|y 一 y7∣]. Thus,
SuplEx~py,wr (x) [f(X, y)] -
Ex~Pr (x|y) [f (X, y)]ll
≤ ɪ Jg (X)EyO~pw(y0∣y)[|y - y|] dχ
UMr	7
=-2— Ey0~pw(y0∣y) [|y 一 y|].
(S.27)
16
Published as a conference paper at ICLR 2021
Therefore, combining both Eq.(S.26) and (S.27), with probability at least 1 - δ,
sup
D∈D
-Ex〜Pr(x|y) [- log D(x, y)]
UMr
—2 — Ey0~pW (y0∣y)
≤
[|y0-y|].
This finishes the proof.
□
Lemma S.4. Suppose that (A1), (A3) and (A4) hold, then ∀δ ∈ (0, 1), with probability at least 1 - δ,
N PNI Wgly,y)[-IOg(I - D(Xg,y))]
D∈D	忐 PNI Wg (源,y
≤ WU⅛S^2Nlog (4) + UM^Ey^pwQiy)
for a given y.
-Ex〜pg(x∣y) [— log(1 - D(x, y))]
[|y0-y|],
(S.28)
Proof. This proof is omitted because it is almost identical to the one for Lemma S.21.	□
As introduced in Section 2, we use KDE for the marginal label distribution with Gaussian kernel.
The next theorem characterizes the difference between a pr(y), pg(y) and their KDE using n i.i.d.
samples.
Theorem S.3. Let p)KDE(y) andPgDE(y) StandfortheKDEof Pr (y) and Pg (y) respectively. Under
condition (A4), if the KDEs are based on n i.i.d. samples from pr /pg and a bandwidth σ, for all
δ ∈ (0, 1), with probability at leaSt 1 - δ,
sup IPKDE(y) - Pr (y)∣ ≤ ∖l°δ,δ ɪog n + Lrσ2,
t r	nσ
SuP IPKDE(y) - Pg(y)∣ ≤ SC2/n；og n + Lgσ2,
for Some conStantS C1K,Dδ E, C2K,Dδ E depending on δ.
(S.29)
(S.30)
Proof. By ((Wasserman); P.12), for any P(t) ∈ ∑(L) (the Holder Class, see Definition 1), with
probability at least 1 - δ,
SuPIPKDE(t) -P(t)∣ ≤《Cδ Jogn + cσ2,	(S.31)
for some constants CδKDE and c, where C depends on δ and c = L K(s)|s|2ds. Since in this work,
K is chosen as Gaussian kernel, C = L ʃ K(s)∣s∣2ds = L.	□
S.5.2.2 Error bounds for HVDL and SVDL
Based on the lemmas and theorems in Supp. S.5.2.1, we derive the error bounds of HVDL and SVDL,
which will be used in the proofs of Theorems 1 and 2.
Theorem S.4. ASSume that (A1)-(A4) hold, then ∀δ ∈ (0, 1), with probability at leaSt 1 - δ,
SuP IILbHVDL(D) - L(D)II
D∈D
≤ u(「r + L”) +(
C2K,DδElog Ng
Ngσ
+ Lgσ2	+
κU (Mr+ Mg)
(S.32)
,y~P KDE (y)
TNgKj
2
17
Published as a conference paper at ICLR 2021
for some constants C1K,Dδ E, C2K,DδE depending on δ.
Proof. We first decompose supD∈D IILbHVDL(D) - L(D)II as follows
sup
D∈D
≤ sup
D∈D
+ sup
D∈D
LbHVDL(D) - L(D)
/ /[-logD(χ,y)]Pr(χ∣y)dχ (pr(y) -PKDE(y))dy
I/]/ [-iog(i - D(χ,y))]Pg(χ∣y)dχ (pg(y) -PKDE(y))dy
+ sup
D∈D
+ sup
D∈D
Nr	I
E1{∣y-yrl≤κ} [-logD(Xr,y)] - Eχ~p,(x|y) [-logD(x,y)] p^KDE(y)dy
i=1	I
Nr	I
X l{∣y-yg∣≤κ} [-log(1 - D(Xg,y))] - Ex~pg(x∣y) [-log(I- D(x,y))] PKDE⑻dy ∙
i=1	I
These four terms in the RHS can be bounded separately as follows
1. The first term can be bounded by using Theorem S.3 and the boundness of D and y ∈ [0, 1].
For the first term, ∀δ1 ∈ (0, 1), with at least probability 1 - δ1,
sup
D∈D
/U
[- log D(X, y)] Pr(X|y)dX
(Pr(y) - PKDE(y))dy
≤U (∖r + Lrσ2
(S.33)
for some constants C1K,Dδ E depending on δ1.
2.	The second term can be bounded by using Theorem S.3 and the boundness of D and
y ∈ [0, 1]. For the first term, ∀δ2 ∈ (0, 1), with at least probability 1 - δ2,
sup
D∈D
/ /[-iog(i-D(χ,y))]Pg(χ∣y)dχ (Pg(y) -PKDE(y))dy
≤U (FDN手 + Lgσ2
(S.34)
for some constants C2K,Dδ E depending on δ2 .
3.	The third term can be bounded by using Lemma S.1 and S.2. For the third term, ∀δ3 ∈ (0, 1),
with at least probability 1 - δ3,
sup
D∈D
≤ sup II
D∈DI
≤Z "Us
Nr
El{∣y-yr∣≤κ} [-logD(Xr,y)] - Ex〜pr(x∣y) [-logD(χ,y)] PKDE(y)dy
i=1
1 Nr	II
Njr~ E1{∣y-yr∣≤κ} [-log D(Xr ,y)] - Ex〜Pr(x|y) [-log D(x,y)] PrE)E(y)dy
Ny,κ i=1	I
1	2 κUMr
Flog lδ3j 十丁
PKDE(y)dy
18
Published as a conference paper at ICLR 2021
Note that Nr,κ = PN=I l{∣y-yr ∣}, which is a random variable of y0s. The above can be
expressed as
Ex~Pr (x|y) [- log D(x, y)]
PKDE(y)dy
(S.35)
4. Similarly, for the fourth term, ∀δ4 ∈ (0, 1), with at least probability 1 - δ4,
sup
D∈D
1 Nr
Ng~ ∑S1{∣y-ygl≤κ}[-log(I - D(xg，y))]
Ny,κ i=1
-Ex~pg(x∣y) [- log(1 - D(X, y))]
dχ 卜 KDE (y)dy
(S.36)
κUMg
≤---
-2
+U
Ey~PKDE(y)
With δι = δ2 = δ3 = δ4 = 4, combining Eq. (S.33) - (S.36) leads to the upper bound in Theorem
S.4.	口
Theorem S.5. Assume that (A1)-(A4) hold, then ∀δ ∈ (0, 1), with probability at least 1 - δ,
sup LbSVDL(D) - L(D)
D∈D
≤u(SCHΞ + Lr σ)+u(SCI^ + L σ∖
∖	)	∖	)	(S.37)
+ U j2log (1δ6) (√NrEy~prDE(y) _W⅛_ + √⅛Ey~pKKDSy _W1y)_)
+ UU (MrEy^prDE(y) [Ey0~pw(y0∣y) ly - y∖∖ + MgEy~PKKE(y) [Ey0~pW(y0∣y) U - y|])
for some constant C1K,KδE, C2K,KδE depending on δ.
Proof. Similar to the decomposition for Theorem S.4, we can decompose
supD∈D LbSVDL(D) - L(D) into four terms which can be bounded by using Theorem S.3,
the boundness of D, Lemma S.3, and Lemma S.4. The detail is omitted because it is almost identical
to the one of Theorem S.4.	口
S.5.2.3 Proof of Theorem 1
Based on Theorem S.4, we derive Theorem 1.
19
Published as a conference paper at ICLR 2021
Proof. We first decompose L(DHVDL) - L(D*) as follows
L(D HVDL)-L(D*)
=L(DHVDL) - L(DHVDL) + L(DHVDL) - L(D) + L(D) - L(D) + L(D) - l(d*)
(byLb(DbHVDL) - Lb(De) ≤ 0)
≤2 SUp ILHVDL(D) - L(D)I + L(D) - L(Dt)
D∈D
(by Theorem S.4)
+ κU(Mr+ Mg)
≤2U (∖r + Lrσ2)
+ 2U (-g + Lgσ2)
总)+L(D)-L(D*).
(S.38)
□
S.5.2.4	Proof of Theorem 2
Based on Theorem S.5, we derive Theorem 2.
Proof. The detail is omitted because it is almost identical to the one of Theorem 1 in Supp. S.5.2.3.
□
S.5.2.5	Interpretation of Theorems 1 and 2
Both theorems imply HVDL and SVDL perform well if the output of D is not too close to 0 or 1 (i.e.,
favor small U). The first two terms in both upper bounds control the quality of KDE, which implies
KDE works better if we have larger Nr and Ng and a smaller σ. The rest terms of the two bounds
are different. In the HVDL case, we favor smaller κ, Mr, and Mg. However, we should avoid setting
κ for a too small value because we prefer larger Nyr,κ and Nyg,κ. In the SVDL case, we prefer small
Mrand Mgbut large Wr(y) and Wg(y). Large Wr(y) and Wg(y) imply that the weight function
decays slowly (i.e., small ν; similar to large Nyr,κ and Nyg,κ in Eq.(S.32)). However, we should avoid
setting V too small because a small V leads to large Ey，~pr(yo\y)∣y0 - y| and Ey，~pw(y0|y)∣y0 - y|
(i.e., y0’s which are far away from y have large weights). In our experiments, we use some rule-of-
thumb formulae to select κ and V. As a future work, a refined hyper-parameter selection method
should be proposed.
S.6	More details of the simulation in Section 4.1
S.6.1	Network architectures
Please refer to Table S.6.1 and Table S.6.2 for the network architectures we adopted for cGAN and
CcGAN in our Simulation experiments.
S.6.2	Training setups
The cGAN and CcGAN are trained for 6000 iterations on the training set with the Adam (Kingma &
Ba, 2015) optimizer (with β1 = 0.5 and β2 = 0.999), a constant learning rate 5 × 10-5 and batch
size 128. The rule of thumb formulae in Section S.4 are used to select the hyper-parameters for
HVDL and SVDL, where we let mκ = 2. Thus, the three hyper-parameters in this experiments are
set as follows: σ = 0.074, κ = 0.017, V = 3600.
20
Published as a conference paper at ICLR 2021
Table S.6.1: Network architectures for the generator and discriminator of cGAN in the simulation.
“fc” denotes a fully-connected layer. “BN” stands for batch normalization. The label y is treated as
a class label and encoded by label-embedding (Akata et al., 2015) so its dimension equals to the
number of distinct angles in the training set (i.e., y ∈ R120).
(a) Generator	(b) Discriminator
Z ∈ R2 ~ N(0,I); y ∈ R120	A sample X ∈ R2
Concat(z, y) ∈ R122	fc→ 100; ReLU
fc→ 100; BN; ReLU	fc→ 100; ReLU
fc→ 100; BN; ReLU	fc→ 100; ReLU
fc→ 100; BN; ReLU	fc→ 100; ReLU	~~~~
fc→ 100; BN; ReLU	ConCat(output of PrevioUS layer, y) ∈ R220,
fc→ 100; BN; ReLU	where y ∈ R120 is the label of x.
fc→ 100; BN; ReLU	fc→ 100; ReLU
fc→2	fc→ 1; Sigmoid
Table S.6.2: Network architectures for the generator and discriminator of our proposed CcGAN in the
simulation. The label y is treated as a real scalar so its dimension is 1. We do not directly input y into
the generator and discriminator. We first convert each y into the coordinate of the mean represented
by this y, i.e., (sin(y), cos(y)). Then we insert this coordinate into the networks.
(a) Generator	(b) Discriminator
z ∈ R2 ~ N(0,I); y ∈ R	A sample X ∈ R2 with label y ∈ R
ConCat(z, sin(y), cοs(y)) ∈ R4	ConCat(x, sin(y), cοs(y)) ∈ R4
fc→ 100; BN; ReLU	fc→ 100; ReLU
fc→ 100; BN; ReLU	fc→ 100; ReLU
fc→ 100; BN; ReLU	fc→ 100; ReLU
fc→ 100; BN; ReLU	fc→ 100; ReLU
fc→ 100; BN; ReLU	fc→ 100; ReLU
fc→ 100; BN; ReLU	fc→ 1; Sigmoid
fc→ 2
S.6.3	Testing setups
When evaluating the trained cGAN, if a test label y0 is unseen in the training set, we first find its
closest, seen label y. Then, we generate samples from the trained cGAN at y instead of at y0 . On the
contrary, generating samples from CcGAN at unseen labels is well-defined.
S.6.4	Extra Experiments
S.6.4.1	Varying Number of Gaussians for Training Data Generation
In this section, we study the influence of the number of Gaussians used for training data generation on
the performance of cGAN and CcGAN. We vary the number of Gaussians from 120 to 10 with step
size 10 but keep other settings in Section 4.1 unchanged and plot the line graphs of 2-Wasserstein
Distance (log scale) versus the number of Gaussians in Fig. S.6.1. Reducing the number of Gaussians
for training implies a larger gap between any two consecutive distinct angles in the training set.
As the number of Gaussians decreases, the continuous scenario gradually degenerates to the
categorical scenario, therefore the assumption that a small perturbation to y results in a
negligible change to p(x|y) is no longer satisfied. Consequently, the 2-Wasserstein distances of the
proposed two CcGAN methods gradually increase and eventually surpass the 2-Wasserstein distance
of cGAN when the number of Gaussians is small (e.g., less than 40). Note that reducing the number
of Gaussians in the training data generation will not improve the performance of cGAN in the testing
21
Published as a conference paper at ICLR 2021
because many angles seen in the testing stage (we evaluate each method on 360 angles) do not appear
in the training set.
φ-e∑> S CTO- U-)①。USSQuφtn.l ① SSeMA
Number of Gaussians for Training Data Generation
Figure S.6.1: Line graphs of 2-Wasserstein Distance (log scale) versus the number of Gaussians for
training data generation. As the number of Gaussians decreases, the continuous scenario gradually
degenerates to the categorical scenario, therefore the assumption that a small perturbation to y results
in a negligible change to p(x|y) is no longer satisfied. Consequently, the 2-Wasserstein distances of
two CcGAN methods gradually increase and eventually surpass the 2-Wasserstein distance of cGAN
when the number of Gaussians is small (e.g., less than 40).
S.7	More details of the experiment on RC-49 in Section 4.2
S.7.1	Description of RC-49
To generate RC-49, firstly we randomly select 49 3-D chair object models from the “Chair” category
provided by ShapeNet (Chang et al., 2015). Then we use Blender v2.79 1 to render these 3-D models.
Specifically, during the rendering, we rotate each chair model along with the yaw axis for a degree
between 0.1° and 89.9° (angle resolution as 0.1°) where We use the scene image mode to compose
our dataset. The rendered images are converted from the RGBA to RGB color model. In total, the
RC-49 dataset consists of 44051 images of image size 64×64 in the PNG format.
S.7.2	Network architectures
The RC-49 dataset is a more sophisticated dataset compared with the simulation, thus it requires
networks with deeper layers. We employ the SNGAN architecture (Miyato et al., 2018) in both cGAN
and CcGAN consisting of residual blocks for the generator and the discriminator. Moreover, for the
generator in cGAN, the regression labels are input into the network by the label embedding (Akata
et al., 2015) and the conditional batch normalization (De Vries et al., 2017). For the discriminator in
cGAN, the regression labels are fed into the network by the label embedding and the label projection
(Miyato & Koyama, 2018). For CcGAN, the regression labels are fed into networks by our proposed
label input method in Section 2. Please refer to our codes for more details about the network
specifications of cGAN and CcGAN.
S.7.3	Training setups
The cGAN and CcGAN are trained for 30,000 iterations on the training set with the Adam (Kingma
& Ba, 2015) optimizer (with β1 = 0.5 and β2 = 0.999), a constant learning rate 10-4 and batch size
256. The rule of thumb formulae in Section S.4 are used to select the hyper-parameters for HVDL
and SVDL, where we let mκ = 2. Thus, the three hyper-parameters in this experiments are set as
follows: σ = 0.0473, κ = 0.004, ν = 50625.
1 https://www.blender.org/download/releases/2-79/
22
Published as a conference paper at ICLR 2021
S.7.4	Testing setups
The RC-49 dataset consists of 899 distinct yaw angles and at each angle there are 49 images
(corresponding to 49 types of chairs). At the test stage, we ask the trained cGAN or CcGAN to
generate 200 fake images at each of these 899 yaw angles. Please note that, among these 899 yaw
angles, only 450 of them are seen at the training stage so real images at the rest 449 angles are not
used in the training.
We evaluate the quality of the fake images from three perspectives, i.e., visual quality, intra-label
diversity, and label consistency. One overall metric (Intra-FID) and three separate metrics (NIQE,
Diversity, and Label Score) are used. Their details are shown in Supp. S.7.5.
S.7.5	Performance measures
Before we conduct the evaluation in terms of the four metrics, we first train an autoencoder (AE) , a
regression CNN and a classification CNN on all real images in RC-49. The bottleneck dimension of
the AE is 512 and the AE is trained to reconstruct the real images in RC-49 with MSE as the loss
function. The regression CNN is trained to predict the yaw angle of a given image. The classification
CNN is trained to predict the chair type of a given image. The autoencoder and both two CNNs are
trained for 200 epochs with a batch size 256.
•	Intra-FID (Miyato & Koyama, 2018): We take Intra-FID as the overall score to evaluate
the quality of fake images and we prefer the small Intra-FID score. At each evaluation angle,
we compute the FID (Heusel et al., 2017) between 49 real images and 200 fake images in
terms of the bottleneck feature of the pre-trained AE. The Intra-FID score is the average FID
over all 899 evaluation angles. Please note that we also try to use the classification CNN to
compute the Intra-FID but the Intra-FID scores vary in a very wide range and sometimes
obviously contradict with the three separate metrics.
•	NIQE (Mittal et al., 2012): NIQE is used to evaluate the visual quality of fake images with
the real images as the reference and we prefer the small NIQE score. We train one NIQE
model with the 49 real images at each of the 899 angles so we have 899 NIQE models.
During evaluation, a NIQE score is computed for each evaluation angle based on the NIQE
model at that angle. Finally, we report the average and standard deviations of the 899 NIQE
scores over the 899 yaw angels. Note that the NIQE is implemented by the NIQE module in
MATLAB.
•	Diversity: Diversity is used to evaluate the intra-label diversity and the larger the better.
In RC-49, there are 49 chair types. At each evaluation angle, we ask the pre-trained
classification to predict the chair types of the 200 fake images and an entropy is computed
based on these predicted chair types. The diversity reported in Table 2 is the average of the
899 entropies over all evaluation angles.
•	Label Score: Label Score is used to evaluate the label consistency and the smaller the better.
We ask the pre-trained regression CNN to predict the yaw angles of all fake images and the
predicted angles are then compared with the assigned angles. The Label Score is defined as
the average absolute distance between the predicted angles and assigned angles over all fake
images, which is equivalent to the Mean Absolute Error (MAE).
S.7.6	Extra experiments
S.7.6	. 1 More line graphs
S.7.6.2 Interpolation
In Fig. S.7.3, we present some interpolation results of the two CcGAN methods (i.e., HVDL and
SVDL). For an input pair (z, y), we fix the noise z but perform label-wise interpolations, i.e., varying
label y from 4.5 to 85.5. Clearly, all generated images are visually realistic and we can see the
chair distribution smoothly changes over continuous angles. Please note that, Fig. S.7.3 is meant to
show the smooth change of the chair distribution instead of one single chair so the chair type may
change over angles. This confirms CcGAN is capable of capturing the underlying conditional image
distribution rather than simply memorizing training data.
23
Published as a conference paper at ICLR 2021
(b) NIQE Vs Angle
Figure S.7.2:	Line graphs ofFID/NIQEzDiVerSity versus yaw angles on RC-49. Figs. S.7.2(a) to
S.7.2(c) show that two CcGANs consistently outperform cGAN across all angles. The graphs of
CcGANs also appear smoother than those of cGAN because of HVDL and SVDL.
HVDL
SVDL
4.5	13.5 22.5 31.5 40.5 49.5 58.5 67.5 76.5 85.5
中用用t∏ H网
Figure S.7.3:	Some example RC-49 fake images from the two CcGAN methods. We fix the noise Z
but vary the label y.
S.7.6.3	Degenerated CcGAN
In this experiment, we consider the extreme case of the proposed CcGAN (degenerated CcGAN), i.e.,
σ → 0 and K → 0 or V → +∞. Then We train the degenerated CcGAN with the same experimental
setting as for CcGANs. Some examples from degenerated CcGANs are shown in Fig. S.7.4. Since, at
each angle, the degenerated CcGAN only uses the images at this angle, it leads to the mode collapse
problem (e.g, the row in the yellow rectangle) and bad visual quality (e.g., images in the red rectangle)
at some angles.
Note that the degenerated CcGAN is still different from cGAN, since we still treat y as a continuous
scalar instead of a class label here and we use the proposed label input method to incorporate y into
the generator and the discriminator.
S.7.6.4	cGAN: different number of classes
In this experiment, we show that cGAN still fails even though we bin [0.1, 89.9] into other number
of classes. We experimented with three different bin setting - grouping labels into 90 classes, 150
classes and 210 classes, respectively. Experimental results are shown in Fig. S.7.5 and we observe
that all three cGANs fail.
S.7.6.5	Varying Sample Size for Each Distinct Angle
To test cGAN and CcGAN under more challenging scenarios, we vary the sample size for each
distinct angle in the training set from 45 to 5. We visualize the line graphs of Intra-FID versus the
sample size for each distinct training angle in Fig. S.7.6. From this figure, we can see the two CcGAN
methods substantially outperform cGAN no matter what is the sample size for each distinct angle
in the training set. The overall trend in this figure also shows that smaller sample size reduces the
performance of both cGAN and CcGAN.
S.7.6.6	Varying the strength of the correlation between the image and its
LABEL
To study how the strength of the correlation between the image x and its label y (i.e., the label
power) influences the performance of cGAN and CcGAN, in this study, we randomly add Gaussian
noises with a preset standard deviation to the raw regression labels in the training set. The strength
of the correlation is controlled by the standard deviation of the Gaussian noises which varies from
24
Published as a conference paper at ICLR 2021
Figure S.7.5: Example RC-49 fake images from
Figure S.7.4: Some example RC-49 fake images cGAN when we bin the yaw angle range into
from a degenerated CcGAN.	different number of classes.
Figure S.7.6: Line graphs of Intra-FID versus the sample size for each distinct training angle. The
grey vertical dashed line stands for the sample size used in the main study of the RC-49 experiment
in Section 4.2. Two CcGAN methods substantially outperform cGAN no matter what the sample size
for each distinct angle in the training set. The overall trend in this figure shows that a smaller sample
size deteriorates the performance of both cGAN and CcGAN.
0 (no Gaussian noise) to 25 (the unit is degree). A large standard deviation corresponds to a weak
correlation. The training setup is consistent with the main study in Section 4.2 except that we replicate
the training set five times and randomly add Gaussian noises to the raw regression labels in the
replicated training set. Therefore, each training sample has five noisy labels. We plot the line graphs
of Intra-FID versus the standard deviation of Gaussian noise in Fig. S.7.7. From Fig. S.7.7, we can
see that the performance of two CcGAN methods deteriorates as the standard deviation increases;
however, the line graph of the performance of cGAN does not have a clear increasing or decreasing
trend and the Intra-FID of cGAN always stays at a high level.
25
Published as a conference paper at ICLR 2021

Figure S.7.7: Line graphs of Intra-FID versus the standard deviation of Gaussian noise. The overall
trend in the figure shows that the performance of two CcGAN methods deteriorate as the standard
deviation increases.
S.8	More details of the experiment on the UTKFace dataset in
Section 4.3
S.8.1 Description of the UTKFace dataset
The UTKFace dataset is an age regression dataset (Zhang et al., 2017), with human face images
collected in the wild. We use the preprocessed version (cropped and aligned), with ages spanning
from 1 to 60. After data cleaning (i.e., removing images of very low quality or with clearly wrong
labels), the overall number of images is 14760. Images are resized to 64 × 64. The histogram of
UTKFace dataset w.r.t. ages 1-60 is shown in S.8.8.
From Fig. S.8.8, we can see UTKFace dataset is very imbalanced so the samples from the minority
age groups are unlikely to be chosen at each iteration during the GAN training. Consequently, cGAN
and CcGAN may not be well-trained at these minority age groups. To increase the chance of drawing
these minority samples during training, we randomly replicate samples in the minority age groups to
ensure that the sample size of each age is more than 200.
Figure S.8.8: The histogram of UTKFace dataset with ages range from 1 to 60.
26
Published as a conference paper at ICLR 2021
S.8.2	Network architectures
The network architectures used in this experiment is similar to those in the RC-49 experiment. Please
refer to our codes for more details about the network specifications.
S.8.3	Training setups
The cGAN and CcGAN are trained for 40,000 iterations on the training set with the Adam (Kingma
& Ba, 2015) optimizer (with β1 = 0.5 and β2 = 0.999), a constant learning rate 10-4 and batch size
512. The rule of thumb formulae in Section S.4 are used to select the hyper-parameters for HVDL
and SVDL, where we let mκ = 1.
S.8.4	Performance measures
Similar to the RC-49 experiment, we evaluate the quality of fake images by Intra-FID, NIQE,
Diversity, and Label Score. We also train an AE (bottleneck dimension is 512), a classification CNN,
and a regression CNN on all images. Please note that, the UTKFace dataset consists of face images
from 5 races based on which we train the classification CNN. The AE and both two CNNs are trained
for 200 epochs with a batch size 256.
S.8.5	Extra experiments
S.8.5.1	More line graphs
(a) FID vs Age
Figure S.8.9: Line graphs of FID/NIQE/Diversity versus ages on UTKFace. Figs. S.8.9(a) to S.8.9(c)
show that two CcGANs consistently outperform cGAN across almost all ages. The graphs of CcGANs
also appear smoother than those of cGAN because of HVDL and SVDL.
S.8.5.2	Interpolation
To perform label interpolation experiments, we keep the noise vector z fixed and vary label from age
3 to age 57 for CcGANs with HVDL and SVDL losses. The interpolation results are illustrated in
S.8.10. As age y increases, we observe the synthetic face gradually becomes older in appearance.
This observation convincingly shows that both HVDL and SVDL based CcGANs do not simply
memorize or overfit to the training set. Indeed, our CcGANs demonstrate continuous control over
synthetic images with respect to ages.
Figure S.8.10: Some examples of generated UTKFace images from CcGAN when the discriminator
is trained with HVDL and SVDL. We fix the noise z but vary the label y from 3 to 57.
27
Published as a conference paper at ICLR 2021
S.8.5.3	Degenerated CcGAN
We consider the extreme cases of the proposed CcGANs on the UTKFace dataset. As shown in
Fig. S.8.11, the degenerated CcGANs fails to generate facial images at some ages (e.g., 51 and 57)
because of too small sample sizes.
S.8.5.4	cGAN: different number of classes
In the last experiment, we bin samples into different number of classes based on ground-truth labels,
in order to increase the number of training samples at each class. Then we train cGAN using samples
from the binned classes. We experimented with two different bin setting, i.e., binning image samples
into 60 classes and 40 classes, respectively. The results are reported in Fig.S.8.12. The results
demonstrate cGANs consistently fail to generate diverse synthetic images with labels aligned with
their conditional information. Moreover, the image quality is much worse than those from the
proposed CcGANs. In conclusion, compared with existing cGANs, our proposed CcGANs have
substantially better performance in terms of the image quality and diversity.
Figure S.8.11:	Some example UTKFace fake
images from a degenerated CcGAN.
Figure S.8.12:	Example UTKFace fake images
from cGAN when we bin the age range into
different number of classes.
S.8.5.5	Training on images for odd ages only and testing on even-numbered
AGES
In this section, we train cGAN and CcGAN on images for odd ages only and test them on even-
numbered ages. Since the training set in this experiment is half of the one in the main study in Section
4.3, we reduce the number of iterations for the GAN training from 40,000 to 20,000 while other
settings are unchanged. The quantitative results for cGAN and two CcGAN methods are summarized
in Table S.8.3. From Table S.8.3, we can see two CcGAN methods are still much better than cGAN
in terms of all metrics except Label Score, since CcGAN is designed to sacrifice some (not too much)
label consistency for much better visual quality and diversity.
28
Published as a conference paper at ICLR 2021
Table S.8.3: Training cGAN and CcGAN on images for odd ages only and testing them on even-
numbered ages.
Method	Intra-FID J	NIQE J	Diversity ↑	Label Score J
cGAN (30 classes)	4.724 ± 1.339	2.763 ± 0.384	0.299 ± 0.349	9.114 ± 7.398
CcGAN (HVDL)	0.724 ± 0.161	1.795 ± 0.230	1.133 ± 0.257	10.341 ± 3.931
CcGAN (SVDL)	0.777 ± 0.248	1.803 ± 0.214	1.257 ± 0.112	13.141 ± 5.862
S.8.5.6	Training with smaller sample sizes
The histogram in Fig. S.8.8 shows that the UTKFace dataset is highly imbalanced. To balance the
training data and also test the performance of cGAN and CcGAN under smaller sample sizes, we
vary the maximum sample size for each distinct age in the training from 200 to 50. Note that, in
the main study in Section 4.3, we do not restrict the maximum sample size. Since we have a much
smaller sample size, we reduce the number of iterations for the GAN training from 40,000 to 20,000
and slightly increase mκ in Supp. S.4 from 1 to 2 (we therefore use a wider hard/soft vicinity). We
visualize the line graphs of Intra-FID versus the maximum sample size for each age of cGAN and
CcGAN in Fig. S.8.13. From the figure, we can clearly see that a smaller sample size worsens the
performance of both cGAN and CcGAN. Moreover, the Intra-FID scores of cGAN always stay at a
high level and are much larger than those of two CcGAN methods.
Figure S.8.13: Line graphs of Intra-FID versus the maximum sample size for each distinct angle in
the training set.
S.8.5.7	DiffAugment cannot save cGAN in the continuous scenario
DiffAugment (Zhao et al., 2020) is a concurrent work which proposes differentiable transformations
(i.e., translation, random brightness, contrast, saturation, and cutout) on both real and fake images
during the GAN training. This method shows promising results in improving the performance of un-
conditional GANs (e.g., StyleGAN2 (Karras et al., 2020)) and class-conditional GANs (e.g., BigGAN
(Brock et al., 2019)) when training samples are limited. However, DiffAugment is fundamentally
different from CcGAN since it is designed for the unconditional and class-conditional scenarios
rather than the continuous scenario. Even though incorporating DiffAugment into the cGAN training
in the continuous scenario, the two problems (P1) and (P2) discussed in Section 1 are still unsolved.
First, (P1) is still unsolved because DiffAugment does not provide a solution better than binning
the regression labels into a series of disjoint intervals to tackle the problem that some regression
labels do not exist in the training set. Second, since DiffAugment is designed for the unconditional
and class-conditional scenarios where the number of distinct conditions is always finite and known,
DiffAugment doesn’t provide a solution to (P2). Besides these two unsolved problems, another
concern of DiffAugment in the continuous scenario is that the ordinal information in the regression
labels is not utilized while our CcGAN implicitly uses this ordinal information to construct the
soft/hard vicinity.
29
Published as a conference paper at ICLR 2021
To support our arguments, we incorporate DiffAugment into the cGAN training in the UTKFace ex-
periment while other settings are kept constant. When implementing DiffAugment, we use the official
codes from the GitHub repository of DiffAugment 2. The strongest transformation combination (Color
+ Translation + Cutout) is used in the cGAN training. Quantitative results from cGAN+DiffAugment
are summarized in Table S.8.4 and some example images from cGAN+DiffAugment are shown in
Fig. S.8.14. The quantitative results show that DiffAugment substantially improves the visual quality
and diversity of the baseline cGAN; however, the performance of cGAN+DiffAugment is still much
worse than that of the proposed two CcGAN methods. The visual results also support the quantitative
evaluations. Therefore, cGAN+DiffAugment still does not solve the two fundamental problems in
the continuous scenario, since it is not designed for this purpose.
Table S.8.4: Average quality of 60,000 fake UTKFace images from cGAN and CcGAN with standard
deviations after the “土” SymboL "J”(“f”)indicates lower (higher) values are preferred.
Method	Intra-FID J	NIQE J	Diversity ↑	Label Score J
cGAN (60 classes)	4.516 ± 0.965	2.315 ± 0.306	0.254 ± 0.353	11.087 ± 8.119
cGAN (60 classes) + DiffAugment	1.328 ± 0.156	2.077 ± 0.245	1.102 ± 0.183	11.212 ± 8.329
CcGAN (HVDL)	0.572 ± 0.167	1.739 ± 0.145	1.338 ± 0.178	9.782 ± 7.166
CcGAN (SVDL)	0.547 ± 0.181	1.753 ± 0.196	1.326 ± 0.198	10.739 ± 8.340
Figure S.8.14: Some example UTKFace fake images from cGAN+DiffAugment. Even with the help
of DiffAugment, cGAN still has poor visual quality in the continuous scenario.
S.9	Potential Applications and Impacts of CcGANs
Generally, there are three label scenarios where we can apply CcGANs: Scenario I, mathematically
continuous labels (e.g., angles); Scenario II, discrete but ordinal labels (e.g., ages); and Scenario III,
discrete, categorical labels but sharing close relationships among different label categories (e.g., fine-
grained bird image generation). CcGANs can have potential applications in all three scenarios. For
example, in Scenario I, CcGANs could have potential impacts on autonomous driving which involves
predicting the steering angle ( a continuous scalar) to have better controllability over autonomous
cars. In Scenario II, the proposed methods are potentially meaningful in some medical applications.
E.g., in medical experiments, an important task is cell counting, where the cell counting regression
needs to predict the number of cells (i.e., ordinal integers) from a microscopic image. Even with
limited microscopic cell images, the proposed CcGAN can generate visually synthetic and diverse
microscopic images for the regression model training. In this way, CcGAN may help save tedious
efforts of medical reseachers in gathering microscopic images. In Scenario III, as suggested by
AnonReviewer 5 (Q3), CcGAN could be used on some fine-grained image classification datasets, e.g.,
on the bird dataset where birds of different categories may share close similarities. The generated
bird images can be used to enhance the fine-grained bird image classifiers, and potentially help us
better recognize birds and protect the environment. More generally, CcGANs can be potentially used
for image generation in regression datasets (associated with scalar labels y). In summary, CcGANs
can cover a wide range of tasks and applications which could potentially benefit the society.
2https://github.com/mit-han-lab/data-efficient-gans
30