Published as a conference paper at ICLR 2021
Byzantine-Resilient Non-Convex Stochastic
Gradient Descent* * * §
Zeyuan Allen-Zhuj Faeze Ebrahimian^ Jerry Li§ Dan Alistarh,
Ab stract
We study adversary-resilient stochastic distributed optimization, in which m
machines can independently compute stochastic gradients, and cooperate to
jointly optimize over their local objective functions. However, an α-fraction of
the machines are Byzantine, in that they may behave in arbitrary, adversarial
ways. We consider a variant of this procedure in the challenging non-convex case.
Our main result is a new algorithm SafeguardSGD which can provably escape
saddle points and find approximate local minima of the non-convex objective. The
algorithm is based on a new concentration filtering technique, and its sample and
time complexity bounds match the best known theoretical bounds in the stochastic,
distributed setting when no Byzantine machines are present.
Our algorithm is very practical: it improves upon the performance of all prior
methods when training deep neural networks, it is relatively lightweight, and it is
the first method to withstand two recently-proposed Byzantine attacks.
1	Introduction
Motivated by the pervasiveness of large-scale distributed machine learning, there has recently been
significant interest in providing distributed optimization algorithms with strong fault-tolerance
guarantees. In this context, the strongest, most stringent fault model is that of Byzantine
faults (Lamport et al., 1982): given m machines, each having access to private data, at most an α
fraction of the machines can behave in arbitrary, possibly adversarial ways, with the goal of breaking
or slowing down the algorithm. Although extremely harsh, this fault model is the “gold standard” in
distributed computing (Lynch, 1996; Lamport et al., 1982; Castro et al., 1999), as algorithms proven
to be correct in this setting are guaranteed to converge under arbitrary system behaviour.
A setting of particular interest in this context has been that of distributed stochastic optimization.
Here, the task is to minimize some stochastic function f (x) = E§~d [fs(x)] over a distribution D,
where fs(∙) can be viewed as the loss function for sample S ~ D. We assume there are m machines
(workers) and an honest master, and α < 1/2 fraction of the workers may be Byzantine. In each
iteration t, each worker has access to a version of the global iterate xt , which is maintained by the
master. The worker can independently sample S ~ D, compute Vfs(xt), and then synchronously
send this stochastic gradient to the master. The master aggregates the workers’ messages, and sends
an updated iterate xt+1 to all the workers. Eventually, the master has to output an approximate
minimizer of f. Clearly, the above description only applies to honest workers; Byzantine workers
may deviate arbitrarily and return adversarial “gradient” vectors to the master in every iteration.
This distributed framework is quite general and well studied. One of the first references in this setting
studied distributed PCA and regression (Feng et al., 2014). Other early approaches (Blanchard et al.,
2017; Chen et al., 2017; su & Vaidya, 2016a;b; Xie et al., 2018a) relied on defining generalizations
of the geometric median. These approaches can withstand up to half of the nodes being malicious,
but can have relatively high local computational cost Ω(m2d) (Blanchard et al., 2017; Chen et al.,
2017), where m is the number of nodes and d is the problem dimension, and usually have sub-
optimal sample and iteration complexities.
Follow-up work resolved this last issue when the objective f (∙) is convex, leading to tight sample
*The full and future editions of this paper can be found on https://arxiv.org/abs/2012.143 68.
,Microsoft Research Redmond, zeyuan@csail.mit.edu
^University of Waterloo, faezeeb75@gmail.com
§Microsoft Research Redmond, jerrl@microsoft.com
TIST Austria, dan.alistarh@ist.ac.at
1
Published as a conference paper at ICLR 2021
complexity bounds. Specifically, Yin et al. (2018) provided bounds for gradient descent-type
algorithms, and showed that the bounds are tight when the dimension is constant. Alistarh et al.
(2018) provided a stochastic gradient descent (SGD) type algorithm and showed that its sample and
time complexities are asymptotically optimal even when the dimension is large.
Non-convex Byzantine-resilient stochastic optimization. In this paper, we focus on the more
challenging non-convex setting, and shoot for the strong goal of finding approximate local minima
(a.k.a. second-order critical points). In a nutshell, our main result is the following. Fix d to denote
the dimension, and let the objective f : Rd → R be Lipschitz smooth and second-order smooth.
We have m worker machines, each having access to unbiased, bounded estimators of the gradient
of f. Given an initial point x0, the SafeguardSGD algorithm ensures that, even if at most α < 1/2
fraction of the machines are Byzantine, after
T = O ((α2 + ml) d(f(x0)-εmm f(X)))	parallel iterations,
for at least a constant fraction of the indices t ∈ [T], the following hold:
INf (Xt)II ≤ ε and V2f (xt) Z-C
If the goal is simply ∣∣Vf (xt)k ≤ ε, then T = O( (α2 + ml) f (x0)-minf(X))) iterations suffice.
Here, the O notation serves to hide logarithmic factors for readability. We spell out these factors in
the detailed analysis.
•	When α < 1 / √m, our sample complexity (= mT) matches the best known result in the non-
Byzantine case (Jin et al., 2019) without additional assumptions, and enjoys linear parallel speed-
up: with m workers of which < √m are Byzantine, the parallel speedup is Ω(m).1
•	For α ∈ [1∕√m, 1/2), our parallel time complexity is O(ɑ2) times that needed when no
parallelism is used. This still gives parallel speedup. This α2 factor appears in convex Byzantine
distributed optimization, where it is tight (Yin et al., 2018; Alistarh et al., 2018).
•	The Lipschitz and second-order smoothness assumptions are the minimal assumptions needed to
derive convergence rates for finding second-order critical points (Jin et al., 2019).
Comparison with prior bounds. The closest known bounds are by Yin et al. (2019), who derived
three gradient descent-type of algorithms (based on median, mean, and iterative filtering) to find
a weaker type of approximate local minima. Since it relies on full gradients, their algorithm is
arguably less practical, and their time complexities are generally higher than ours (see Section 2.1).
Other prior works consider a weaker goal: to find approximate stationary points IVf (x)I ≤ ε only:
Bulusu et al. (2020) additionally assumed there is a guaranteed good (i.e. non-Byzantine) worker
known by the master, Xie et al. (2018b) gave a practical algorithm when the Byzantine attackers
have no information about the loss function or its gradient, Yang et al. (2019); Xie et al. (2018a);
Blanchard et al. (2017) derived eventual convergence without an explicit complexity bound, and the
non-convex result obtained in Yin et al. (2018) is subsumed by Yin et al. (2019), discussed above.
Our algorithm and techniques. The structure of our algorithm is deceptively simple. The master
node keeps track of the sum of gradients produced by each worker across time. It labels (allegedly)
good workers as those whose sum of gradients “concentrate” well with respect to a surrogate of
the median vector, and labels bad workers otherwise. Once a worker is labelled bad, it is removed
from consideration forever. The master then performs the vanilla SGD, by moving in the negative
direction of the average gradients produced by those workers currently labelled as good.
We call our algorithm SafeguardSGD, since it behaves like having a safe guard to filter away bad
workers. Its processing overhead at the master is O(md), negligible compared to standard SGD.
As the astute reader may have guessed, the key non-trivial technical ingredient is to identify the
right quantity to check for concentration, and make it compatible with the task of non-convex
optimization. In particular, we manage to construct such quantities so that (1) good non-Byzantine
workers never get mislabelled as bad ones; (2) Byzantine workers may be labelled as good ones
(which is inevitable) but when they do, the convergence rates are not impacted significantly; and (3)
the notion does not require additional assumptions or running time overhead.
The idea of using concentration (for each worker across time) to filter out Byzantine machines
1By parallel speedup we mean the reduction in wall-clock time due to sampling gradients in parallel among
the m nodes. In each time step, the algorithm generates m new gradients, although some may be corrupted.
2
Published as a conference paper at ICLR 2021
traces back to the convex setting (Alistarh et al., 2018). However, the quantities used in (Alistarh
et al., 2018) to check for concentration are necessarily different from this paper, and our analysis is
completely new, as deriving non-convex rates is known to be much more delicate and challenging.
Recently, Bulusu et al. (2020) used similar concentration filters to Alistarh et al. (2018) in the non-
convex setting, but under stronger assumptions, and for the simpler task of finding stationary points.
Many other algorithms do not rely on concentration filters. In each iteration, they ask each worker
to compute a batch of stochastic gradients, and then use coordinate-wise median or mean over the
batch average (e.g. Yin et al. (2018; 2019); Yang et al. (2019)) or iterative filtering (e.g. Su & Xu
(2018); Yin et al. (2019)) by the master to derive a “robust mean.” These works fundamentally
rely on each iteration to calculate an almost precise full gradient, so that they can apply a surrogate
of full gradient descent. Such algorithms can introduce higher sample and time complexities (see
Section 2), are less practical than stochastic gradient schemes, require additional restrictions on the
resilience factor α, e.g. α < 1/4 (Su & Xu, 2018), and, critically, have been shown to be vulnerable
to recent attacks (Baruch et al., 2019; Xie et al., 2020).
Attack resilience and experimental validation. There is a growing literature on customized
attacks against Byzantine-resilient algorithms, showing that many defenses can be entirely
circumvented in real-world scenarios (Baruch et al., 2019; Xie et al., 2020). Our algorithm is
provably correct against these attacks, a fact we also validate experimentally. We implemented
SafeguardSGD to examine its practical performance against a range of prior works (Xie et al.,
2018b; Blanchard et al., 2017; Chen et al., 2017; Yin et al., 2018; 2019), and against recent attacks
on the distributed task of training deep neural networks. Our experiments show that SafeguardSGD
generally outperforms previous methods in convergence speed and final accuracy, sometimes by a
wide accuracy margin. This is true not only against known Byzantine attacks, but also against attack
variants we fine-crafted to specifically slow down our algorithm, and against transient node failures.
2	Statement of Our Theoretical Result
We denote by ∣∣ ∙ ∣∣ the Euclidean norm and [n] := {1,2,..., n}. Given symmetric matrices A, B,
we let kAk2 denote the spectral norm of A. We use to denote Loewner ordering, i.e. A B if
A - B is positive semi-definite. We denote by λmin(A) the minimum eigenvalue of matrix A.
We consider arbitrary d-dimensional non-convex functions f : Rd → R satisfying the following:
•	f (x) is L-LiPschitz smooth: meaning ∣∣Vf (x) — Vf (y)∣ ≤ Lkx 一 y∣ for any x,y ∈ Rd;
•	f (x) is L2-second-order smooth: ∣V2f (x) — V2f (y)∣2 ≤ L2 ∙ ∣∣x 一 y∣ for any x,y ∈ Rd;
For notational simPlicity of the Proofs, we assume L = L2 = V = 1.2 Note that we have also
assumed the domain of f is the entire sPace Rd . If instead there is a comPact domain X ⊂ Rd,
then one can use Projected SGD and re-derive similar results of this PaPer. We choose to Present our
result in the simPlest setting to convey our main ideas.
Byzantine non-convex stochastic distributed optimization. We let m be the number of worker
machines and assume at most an α fraction of them are Byzantine for α ∈ [0,1). We denote by
good ⊆ [m] the set of good (i.e. non-Byzantine) machines, and the algorithm does not know good.
Assumption 2.1. In each iteration t, the algorithm (on the master) is allowed to specify a point xt
and query m machines. Each machine i ∈ [m] gives back a vector Vt,i ∈ Rd satisfying
•	If i ∈ good, the stochastic gradient Vt,i satisfies E[Vt,i] = Vf (Xt) and ∣Vf (xt) — Vt,i∣ ≤ V.3
•	Ifi ∈ [m] \ good, then Vt,i can be arbitrary (w.l.o.g. we assume ∣Vf (xt) 一 Vt,i∣ ≤ V).4
Remark 2.2. For each t and i 6∈ good, the vector Vt,i can be adversarially chosen and may dePend
2In the literature of convergence analysis for non-convex oPtimization, the final comPlexity bounds naturally
and polynomially dePend on these Parameters L, L2 , V, and the way the dePendence goes is tyPically
unique (Allen-Zhu, 2018a;b; Fang et al., 2018; Jin et al., 2019). This is why it suffices to ignore their aPPearance
and only comPare the Polynomial dePendence on ε and d.
3One can instead assume Pr[kVf (Xt) - Vt,ik > t] ≤ 2exp(-12/2V2) and the results of this paper
continue to hold uP to logarithmic factors. To Present the simPlest theory, we do not include that version in this
paper. We refer interested readers to Jin et al. (2019) for how to deal with such probabilistic assumption (when
there is no Byzantine worker).
4This requirement kVf (xt)-Vt,ik ≤ V is “without loss of generality” because itis trivial for the algorithm
to catch bad machines if they output Vt,i more than 2V away from the majorities.
3
Published as a conference paper at ICLR 2021
Algorithm 1 SafeguardSGD: perturbed SGD with double safe guard
Input: point x0 ∈ Rd, rate η > 0, lengths T ≥ T1 ≥ T0 ≥ 1, threshold T1 > T0 > 0;
1:	goodo - [m];
2:	for t _ 0 to T 一 1 do
3:	lastι	J max{tι	∈	[t]:	tι	is a multiple of TJ;
4:	lasto	J max{to	∈	[t]:	to	is a multiple of To}
5:	for each i ∈ goodt do
6:	receive Vt,i ∈ Rd from machine i;
7:	"i，- k=la=tastg ∣goodk∣ and Bi
J- PPk=Iasto ∣goodk| ;
8:	Amed J Ai where i ∈ goodt is any machine s.t.	{j ∈ goodt : kAj 一 Aik	≤	T1}	>	m/2.
9:	Bmed J Bi where i ∈ goodt is any machine s.t.	{j ∈ goodt : kBj 一 Bi k	≤	To}	>	m/2.
10:	goodt+1 J {i ∈ goodt ： kAi- Amedk ≤ 2T1 V	IBi- BmedIl ≤ 2T0};
11:	Xt+1 = Xt	- η	(ξt	+	∣gθ0d/	Pi∈gθθdt	Vtj	;	◊ Gaussian noise	ξt	-N (0,ν2I)
on {Vt0,i}t0≤t,i∈[m]. In particular, the Byzantine machines can even collude during an iteration.
2.1 Our Algorithm and Theorem
Our algorithm is based on arguably the simplest possible method for achieving this goal, (perturbed)
stochastic gradient descent (SGD) (Ge et al., 2015). Our techniques more broadly apply to more
complicated methods (e.g. at least to Allen-Zhu (2018a;b)), but we choose to analyze the simplest
variant of SGD, since it is the most widely applied method in modern non-convex machine learning.
As illustrated in Algorithm 1, in each iteration t = 0, 1, . . . , T - 1, we maintain a set of (allegedly)
good machines goodt ⊆ [m]. We begin with goodo = [m] and start to detect malicious machines
and remove them from the set. We choose a learning rate η > 0, and perform the SGD update
xt+1 = xt + ξt - η ∣go0dt∣ Pi∈goodt Vt,i
where ξt 〜N(0, V2I) is a random Gaussian perturbation that is added for theoretical purpose.
For each machine i ∈ [m], we keep track of the history of its stochastic gradients up to two windows.
Namely, Ai	J	Pk=IaStI	∣⅛T and Bi	J Pk=IaSt0	, for WindoWS sizes To	≤	TI	≤	T.	We
compare among remaining machines in goodt, and kick out those ones whose Ai or Bi deviate
“more than usual” to construct goodt+1. Conceptually, We vieW these tWo as safe guards.
Our theory makes sure that, When the “WindoW sizes” and the thresholds for “more than usual”
are defined properly, then goodt shall alWays include good, and the algorithm shall proceed to find
approximate local minima. Formally, We have (letting the O notion to hide polylogarithmic factors)
Theorem 2.3. Let C3 = α2 + m. Suppose we choose V2 = Θ(C3), η = Θ(dC^), To = Θ(1),
Ti = Θ(-⅛), To = Θ(√T0), and Ti = Θ(√TY), then after
ηε
T = O (f(x0)-minf(x))d(ɑ2 + m))
iterations, with high probability, for at least constant fraction of the indices t ∈ [T], they satisfy
∣∣Vf(xt)k ≤ ε and V2f(xt)占-√εI .
Remark 2.4. If one only Wishes to achieve a significantly simpler goal — finding first-order critical
points IVf(xt)I ≤ ε — the analysis becomes much easier (see Section 3.1). In particular, having
one safe guard Without perturbation (i.e. V = 0) suffices, and the iteration complexity reduces to
T = O(f(χ0)-mInf(X) (α2 + ml)). BUlUSU et al. (2020) achieves this easier goal but requires an
additional assumption: there is one guaranteed good Worker knoWn by the master.
Our contribution. We reiterate our theoretical contributions from three perspectives. 1) When
α < 1∕√m, our algorithm requires mT = O( f(x0)-mm f(X))d) stochastic gradient computations.
This matches the best knoWn result (Jin et al., 2019) under our minimal assumptions of the non-
convex objective. (There exist other Works in the stochastic setting that break the ε-4 barrier
4
Published as a conference paper at ICLR 2021
and get rid of the dimension dependence d under stronger assumptions.)5. 2) When α < 1∕√m,
our algorithm enjoys linear parallel speed-up: the parallel time complexity reduces by a factor of
Θ(m). When α ∈ [1∕√m, 1/2), our parallel time complexity is O(ɑ2) times that needed when no
parallelism is used, still giving noticeable speedup. The α2 factor also appeared in convex Byzantine
distributed optimization (and is known to be tight there) (Yin et al., 2018; Alistarh et al., 2018).
Comparison to (Yin et al., 2019). Yin et al. (2019) derived three gradient descent-type
algorithms to find points with a weaker (and less standard) guarantee: ∣∣Vf(x)k ≤ ε and
V2f (x) 占 一(ε2d)V5I. Despite practical differences (namely, gradient descent may be less
favorable comparing to stochastic gradient descent especially in deep learning applications), the
parallel time complexities derived from their result are also generally larger than ours.
Their paper focuses on bounding the number of sampled stochastic functions, as opposed to the
number of stochastic gradient evaluations like We do. When translated to our language, each of the
workers in their setting needs to evaluate T stochastic gradients, where (1) T = O( ɑ2d + ε4m + 妥)
if using coordinate-wise median, (2) T = O( α2d2 + εdm) if using trimmed mean, and (3)
T = O( εα + εdm) if using iterative filtering. The complexities (1) and (2) are larger than ours (also
with a weaker guarantee); the complexity (3) seems incomparable to ours, but when translating to
r2	r3
the more standard (ε, √ε) guarantee, becomes T = O(Od- + ε⅛) so is also larger than ours. It is
worth noting that (3) requires α < 1/4 so cannot withstand half of the machines being Byzantine.
Resilience against practical attacks. Our algorithm’s filtering is based upon tracking Bi (resp.
Ai), the stochastic gradients of each machine i averaged over a window of T0 (resp. T1) iterations.
This is a departure from previous defenses, most of which are history-less, and enables us to be
provably Byzantine-resilient against state-of-the-art attacks (Baruch et al., 2019; Xie et al., 2020).
In Baruch et al. (2019), Byzantine workers collude to shift the gradient mean by a factor β times the
standard deviation of the (true stochastic) gradient, while staying within population variance. They
noticed β can be quite large especially in neural network training. Their attack circumvent existing
defenses because those defense algorithms are “historyless”, while their attack is statistically
indistinguishable from an honest execution in any single iteration. However, our algorithm can
provably defend against this attack since it has memory: Byzantine workers following their strategy
will progressively diverge from the (honest) “median” Bmed (by an amount proportional to Ω(T) in
T iterations as opposed to TT), and be marked as malicious by our algorithm. (See Figure 2(a).) In
Xie et al. (2020), Byzantine workers deviate in the negative direction of the gradient. However, to
avoid being caught by our algorithm, the maximum “magnitude” of this attack has to stay within our
thresholds. We implemented both attacks and showed our algorithm’s robustness experimentally.
Finally, we note that prior “historyless” schemes, such as Krum or median-based schemes, could
be thought of as providing stronger guarantees, as they in theory allow Byzantine nodes to change
IDs during the computation: such schemes only require an upper bound on the number of Byzantine
agents in each round. However, the attack of Baruch et al. (2019) essentially shows that all such
schemes are vulnerable to variance attacks, and that such attacks are eminently plausible in practice.
Thus, this suggests that the use of historical information, which requires that Byzantine nodes cannot
change their IDs during the execution, may be necessary for Byzantine resilience.
Tolerating transient failures and node ID relabeling. Our algorithm can also withstand transient
node failures and some degrees ofID relabeling, by resetting the set of good nodes goodt to include
all nodes every T1 steps. The algorithm then proceeds as usual. The key observation behind this
relaxation is the fact that our analysis only requires that the attack conditions hold inside the current
window. (Please see the Theorem B.1 for details.) We validate this experimentally in Section 5.
3	Warmup: Single Safe Guard
As a warmup, let us first analyze the behavior of perturbed SGD with a single safe guard. Consider
Algorithm 2, where we start with a point w0, a set good0 ⊇ good, and perform T steps of perturbed
SGD. (We use the wt sequence instead of the xt sequence to emphasize that we are in Algorithm 2.)
5Works such as (Allen-Zhu, 2018a; Lei et al., 2017; Tripuraneni et al., 2017; Allen-Zhu, 2018b; Fang et al.,
2018; Nguyen et al., 2017) require f(x) = Es〜D[fs(x)] where each fs(x) is second-order smooth and/or
Lipschitz smooth. This requirement may be too strong for certain practical applications.
5
Published as a conference paper at ICLR 2021
Algorithm 2 Perturbed SGD with single safe guard (for analysis purpose only)
Input: point w0 ∈ Rd, set good0 ⊇ good, rate η > 0, length T ≥ 1, threshold T > 0;
1:	for t — 0 to T 一 1 do
2:	for each i ∈ goodt do
3:	receive Vt,i ∈ Rd from machine i;
4:	B —Pt S,i ；
4:	Bi L k=0=g ∣goodk∣ ;
5:	Bmed L Bi where i ∈ goodt is any machine s.t. ∣{j ∈ goodt ： kBj — BiIl ≤ T}∣ > m/2.
6:	goodt+l L {i ∈ goodt : kBi- Bmedk ≤ 2T}；
7:	Wt+1 = Wt 一 η (ξt + ∣gθ0dt I Pi∈goodt Vt,);	◊ Gaussian noise ξt ZN(0,ν2I)
Definition 3.1. We make the following definition to simplify notations: let Ξt := σt+	∆t where
•	σt := IgoodtT Pi∈good "t,i 一 Vf (Wt))
•	At : = ∣goodt∣ Pi∈goodt∖good (Vt,i - Vf (wt))
Therefore, we can re-write the SGD update as Wt+1 = Wt 一 η(Vf (Wt) + ξt + Ξt) .
The following lemma is fairly immediate to prove:
Lemma 3.2 (single safe guard). In Algorithm 2, suppose we choose T = 8，T log(16mT∕p).
Then, with probability at least 1 一 p/4, for every t = 0, . . . , T 一 1,
•	goodt ⊇ good.
•	kσtk2 ≤ O(log(m/p)) and ∣∣σ° + …+ σt-i∣2 ≤ O(T吗Tm)
•	k∆tk2 ≤ α2 and ∣∣∆o +-----+ ∆t-ik2 ≤ O(α2Tlog(mT∕p))
•	IhVf (wt),ξti∣ ≤ ∣∣Vf (wt)k ∙ O(νPlog(T∕p)),
•	kξtk2 ≤ O(ν2dlog(T∕P)), ∣ξo +-----+ ξt-ik2 ≤ O(ν2dTlog(T∕P))
We call this probabilistic event EventsTingle(W0 ) and Pr[EventsTingle(W0 )] ≥ 1 一 p∕4.
(The third property above is ensured by our choice ofTand the use of safe guard, and the rest of the
properties follow from simple martingale concentration arguments. Details are in Appendix A.1.)
3.1 Core Technical Lemma 1: Objective Decrease
Our first main technical lemma is the following:
Lemma 3.3. Suppose we choose T as in Lemma 3.2. Denote by C1 = log(T ∕P) and C2 =
a2 log mT + 'ogm/P). Suppose η ≤ 0.01 min{1,看}, T = ιooη(i+√c^) and we start from wo and
apply Algorithm 2. Under event EventsTingle(W0 ), it satisfies
f(wo) - f(wτ) ≥ 0.7ηPT-1 (kVf(Wt)k2 - η ∙ O(C2+ (C2)1.5) - O(CIV2η(d + √C2)))
Lemma 3.3 Says after T ≈ ɪ steps of perturbed SGD, the objective value decreases by, UP to some
small additive error and up to logarithmic factors, f(W0 ) 一 f(WT) ≥ 0.7η PtT=-01 (∣Vf (Wt)∣2 一
2
ηC2). This immediately implies, if We choose η ≈ C, then by repeating this analysis for
O(C) = O(α +1/m) iterations, we can find approximate critical point X with ∣∣Vf (χ)∣ ≤ ε.
Proof sketch of Lemma 3.3. The full proof is in Appendix A.2 but we illustrate the main idea and
difficulties below. After simple manipulations, it is not hard to derive that
f(wo) - f(wτ) ' 0.9ηPTo1 (kVf(wt)k2 - η) + ηpT=o1hVf(wt),Ξ
'--------V---------}
remainder terms
where recall that Ξt = σt+∆t. When there are no Byzantine machines, we have E[Ξt] = E[σt] = 0
so the remainder terms must be small by martingale concentration. Therefore, the main technical
difficulty arises to deal with those Byzantine machines, who can adversarially design their Vt (even
by collusion) so as to negatively correlate with Vf(Wt) to “maximally destroy” the above inequality.
6
Published as a conference paper at ICLR 2021
Our main idea is to use second-order smoothness to write Vf (Wt) ≈ Vf (wo)+V2f (wo)∙(wt-wo).
To illustrate our idea, let us ignore the constant vector and assume that the Hessian is the identity:
that is, imagine as if Vf(wt) ≈ wt - w0. Using wt - w0 = - Pk<t Ξt + ξt, we immediately have
-hVf(wt),Ξti ≈ -hwt -w0,Ξti = Pk<thΞk, Ξti + Pk<thξk, Ξti	(3.1)
For the first partial sum hPk<t Ξk, Ξti in (3.1), itis easy to bound its magnitude using our safeguard.
Indeed, we have PthPk<t Ξk,Ξti ≤ k Pt Ξtk2+Pt kΞtk2 so we can apply Lemma 3.2. For the
second partial sum Pt Pk<t hξk , Ξti, we can apply the concentration Proposition 3.4 below.
Proposition 3.4. Fix the dimension parameter d ≥ 1. Suppose ξ0, . . . , ξT-1 ∈ Rd are i.i.d.
drawn from N (0, I), and that ∆1, . . . , ∆T-1 are arbitrary vectors in Rd. Here, each vector ∆t with
t = 1, . . . , T - 1 can depend on ξ0, . . . , ξt-1 but not on ξt, . . . , ξT-1. Suppose that these vectors
satisfy ∣∣∆ι + …+ ∆t ∣∣2 ≤ Tfor every t = 1,...,T — L Then, with probability at least 1 — P,
∣PT=ι1 hξo + …+ ξt-ι, ∆tiI ≤ O(PdTTIog(T7P)).
3.2 Core Technical Lemma 2: Randomness Coupling
Our next technical lemma studies that, if run Algorithm 2 from a point w0 so that the Hessian
V2f(w0) has a eigenvalue which is less than -δ (think of w0 as a saddle point), then with good
probability, after sufficiently many iterations, the sequence w1, w2, . . . , wT shall escape from w0 to
distance at least R for some parameter R ≈ δ. To prove this, motivated by Jin et al. (2017), we study
two executions of Algorithm 2 where their randomness are coupled. We then argue that at least one
of them has to escape from w0. For any vector v, let [v]i denote the i-th coordinate of v.
Lemma 3.5. Suppose we choose T as in Lemma 3.2 and C1, C2 as in Lemma 3.3. Suppose
w0 ∈ Rd satisfies λmin (V2f(w0)) = -δ for some δ ≥ 0. Without loss of generality let e1 be
the eigenvector of V2 f (w0) with smallest eigenvalue. Consider now two executions of Algorithm 2,
both starting from w0a = w0b = w0, and suppose their randomness {ξta}t and {ξtb}t are coupled so
that [ξta]1 = -[ξtb]1 but [ξta]i = [ξtb]i for i > 1. In words, the randomness is the same orthogonal to
eι, but along eι, the two have opposite signs. Now, suppose we perform T = Θ(赤 log Rδ) steps
of perturbed SGD from w0a, w0b respectively using Algorithm 2. Suppose
R ≤ O(√^-i—Λ2a,—K) and V2 ≥ Ω(C2 log Rδ).
√C] lοg(R2δ∕ην2)2 DnVJ
Then, under events EventsTingle(w0a ) and EventsTingle(w0b), with probability at least 0.98, either
∣wta - w0∣ > R or ∣wtb - w0∣ > R for some t ∈ [T].
Proof details in Appendix A.4. The main proof difficulty is to analyze a noisy version of the power
method, where the noise comes from (1) Gaussian perturbation (which is the good noise), (2)
stochastic gradients (which has zero mean), and (3) Byzantine workers (which can be adversarial).
4	From Warmup to Final Theorem with Double Safe Guard s
At a high level, Lemma 3.3 ensures that if we keep encountering points with large gradient
∣Vf(wt)∣, then the objective should sufficiently decrease; in contrast, Lemma 3.5 says that if
we keep encountering points with negative Hessian directions (i.e., λmin(V2f(wt)) < -δ), then
the points must move a lot (i.e., by more than R in T iterations, which can also lead to sufficient
objective decrease, see Lemma B.4). Therefore, ata high level, when the two lemmas are combined,
they tell that we must not encounter points with ∣Vf (x)∣ being large, or λmin(V2f(x)) being very
negative, for too many iterations. Therefore, the algorithm can find approximate local minima.
The reason we need two safe guards, is because the number of rounds T for Lemma 3.3 and
Lemma 3.5 differ by a factor. We need two safe guards with different window sizes to ensure the two
lemmas simultaneously hold. We encourage the reader to examine the full analysis in Appendix B.
5	Experimental Validation
We evaluate the convergence of SafeguardSGD to examine its practical performance against prior
works. We perform the non-convex task of training a residual network ResNet-20 (He et al., 2016)
on the CIFAR-10/100 datasets (Krizhevsky et al., 2014). More details are given in Appendix C.
7
Published as a conference paper at ICLR 2021
20
40
100
120
140
60	80
epochs
0 L
0
6 60
2 40
(a) variance attack
(d) delayed-gradient attack
------Single safeguard
------double safeguar
------coord median
------geo median
------Krum
------KrUm (3 bad nodes)
------7enc
100
(e) safeguard(x0.6) attack
(f) safeguard(x0.7) attack
Figure 1: Convergence comparison (CIFAR-10 test accuracy) under different attacks. (In Appendix C.2, one
can find additional CIFAR-100 experiments, more discussions, and bigger plots.)
We instantiate m = 10 workers and one master node executing data-parallel SGD for 140 passes (i.e.
epochs) over the training dataset. The results for higher number of workers and epochs are similar,
and therefore omitted. We compare against Geometric Median (Chen et al., 2017), Coordinate-
wise Median (Yin et al., 2018; 2019), Krum (Blanchard et al., 2017), and Zeno (Xie et al., 2018b).
Overall, our experimental setup is very similar to Zeno (Xie et al., 2018b) but with additional attacks.
We implemented the approach of Yang et al. (2019), but found it very sensitive to hyper-parameter
values and were unable to make it converge across all attacks even after significant tuning of its
γ parameter. We also implemented the convex algorithm of Alistarh et al. (2018), and executed it
in our non-convex setting. We found their algorithm can be easily attacked on our ResNet training
tasks. There exists a simple attack, described in Appendix C.4 which causes their algorithm to either
mislabel most good workers as Byzantine, or diverge, or converge to very poor solutions. This is not
surprising, since their algorithm is designed for, and only guaranteed to work in, the convex setting.
To make the comparison stronger, when implementing SafeguardSGD, we have chosen fixed
window sizes T0 = 1 epoch and T1 = 6 epochs across all experiments, and adopted an automated
process to select T0, T1. Determining these thresholds requires being able to pre-run the task on
an honest worker. We have also implemented a single safeguard variant of SafeguardSGD, with
window size T = 3 epochs.
Attacks. We set α = 0.4, which means that there are 4 Byzantine workers. (This exceeds the
fault-tolerance of Krum, and so we also tested Krum with only 3 Byzantine workers.)
•	Label-flipping attack: each Byzantine worker computes its gradient based on the cross-
entropy loss with flipped labels: for CIFAR-10, label ` ∈ {0, ..., 9} is flipped to 9 - `.
•	DELAYED-GRADIENT ATTACK: each Byzantine worker sends an old gradient to master. In our
experiments, the delay is of D = 1000 iterations.
•	Variance attack (Baruch et al., 2019): Byzantine workers measure the mean and the
standard-deviation of gradients at each round, and collude to move the mean by the largest value
which still operates within population variance. (For our parameter settings, this is 0.3 times the
standard deviation. We discuss results for additional parameter values in the Appendix.)
•	Sign-flipping attack: each Byzantine worker sends the negative gradient to the master.
•	S afeguard attack: each Byzantine workers sends a negative but re-scaled gradient to the
master. We use re-scale factors 0.6 and 0.7 in our experiments. The re-scale factor 0.6 avoids
triggering the safe-guard conditions at the master, and the re-scale factor 0.7 occasionally triggers
the safe-guard conditions. This attack is an instantiation of the inner-product attack (Xie et al.,
2020), customized specifically to maximally affect our SafeguardSGD algorithm.
Main experimental results. The ideal test accuracy is 91.7%, which corresponds to applying SGD
using only the stochastic gradients from the honest workers. Figure 1 compares the performances
8
Published as a conference paper at ICLR 2021
Comparison of Statistic Values
(a) ∣∣Bi — Bmedk between a good node (blue), and a (b) Convergence for our safeguard algorithms under
bad node (red) which pretends to be honest and then the variance attack, after periodically resetting the
starts to apply the variance attack.	set of good nodes.
Figure 2
in test accuracy. Below we summarize our main findings for the experiments, and we defer detailed
discussions (and additional experiments for CIFAR-100) to Appendix C.
•	SafeguardSGD generally outperforms all the previous methods in test accuracy. The test
accuracy difference can be “90% vs. < 40%” between our algorithm and the best prior work.
•	The variance attack is indeed very strong, in that it severely affects the accuracy of all prior
works (test accuracy < 35%). This is because thesese defenses are “historyless.” By contrast,
our algorithm not only provably but also empirically defends against it.
•	Our safeguard attack (especially with re-scale factor 0.7) is as strong as the variance attack, and
even stronger on the CIFAR-100 dataset; please see the results in Appendix C.2.5.
•	The label-flipping attack is rather weak: although some defenses, such as Zeno, did not
determine which of the workers are malicious, they still converge well under this attack.
•	The sign-flipping and delayed-gradient attacks are moderate: the best prior works can achieve
accuracy 60% 〜70%. It is worth noting that the sign-flipping attack can already nullify the
Zeno defence (test accuracy 20%). The issue seems to be that it can be very hard for Zeno to use
relatively few samples to determine if the gradient direction is flipped to negative.
•	SafeguardSGD can easily catch all the bad workers under sign-flipping and variance attacks,
and thus leads to gives ideal performance. It cannot catch any bad worker for label-flipping and
delayed-gradient attacks, but there is no performance loss anyways if we use such bad gradients.
•	The safeguard attacks, designed to maximally impact the performance of our SafeguardSGD,
can indeed affect our performance. Specifically, under re-scale factor 0.6, the test accuracy drops
from 91.7% to 89.3% because SafeguardSGD cannot catch any bad worker; however, under re-
scale factor 0.7, the test accuracy no longer drops because SafeguardSGD can begin to catch
some bad workers (it can catch between 0 and 4 bad workers depending on the randomness.)
•	In most cases, the single-safeguard algorithm is close to double-safeguard, except for the
safeguard(x0.7) attack, in which using double-safeguard one can more easily catch bad workers.
(This is more apparent in the CIFAR-100 experiment, see Appendix C.2.5.)
We conclude that SafeguardSGD can be practical, and outperforms previous approaches.
A deeper dive: how the algorithm works. Let us explain the inner workings of our algorithm
in the context of a “delayed” attack, where the Byzantine nodes collude to execute an attack only
after a specific, given point in the execution (in this case, the first half-epoch). Figure 2(a) presents
the results from the perspective of the value of kBi - Bmedk registered at the master server, for two
nodes, an honest one, and a Byzantine one. The value of kBi - Bmed k increases for all the nodes (at
a rate of roughly ʌ/t at step t); but, once the attack starts, the statistic for the Byzantine node grows
linearly in t, leading to fast detection.
Transient attacks and node ID relabeling. Finally, in Figure 2(b) we analyze the behaviour of
our algorithm when it periodically (every 3 epochs for single safeguard and 6 epochs for double
safeguard) resets the set of good nodes to include all nodes, restarting the detection process from
scratch. Our theoretical result still applies after this relaxation. This relaxation has two benefits.
First, it benefits from bad workers that under transient failures (e.g., the node fails for 10 epochs
but resumes to work correctly after a while), and thus benefits from the data stored on this worker.
Second, it can defend against certain degree of node ID relabeling: it supports the case when good
and bad workers exchange their IDs every 6 epochs. In Figure 2(b), we see even under the (very
strong) variance attack, relaxed safeguard maintains good performance.
9
Published as a conference paper at ICLR 2021
Acknowledgments
F. E. and D. A. were supported by the European Research Council (ERC) under the European
Union’s Horizon 2020 research and innovation programme (grant agreement No 805223 ScaleML).
References
Dan Alistarh, Zeyuan Allen-Zhu, and Jerry Li. Byzantine stochastic gradient descent. In Advances in Neural
Information Processing Systems, pp. 4613-4623, 2018.
Zeyuan Allen-Zhu. Natasha 2: Faster Non-Convex Optimization Than SGD. In NeurIPS, 2018a. Full version
available at http://arxiv.org/abs/1708.08694.
Zeyuan Allen-Zhu. How To Make the Gradients Small Stochastically. In NeurIPS, 2018b. Full version available
at http://arxiv.org/abs/1801.02982.
Zeyuan Allen-Zhu and Yuanzhi Li. Feature purification: How adversarial training performs robust deep
learning. arXiv preprint arXiv:2005.10190, 2020.
Gilad Baruch, Moran Baruch, and Yoav Goldberg. A little is enough: Circumventing defenses for distributed
learning. In Advances in Neural Information Processing Systems, pp. 8635-8645, 2019.
Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. Machine learning with
adversaries: Byzantine tolerant gradient descent. In NIPS, pp. 118-128, 2017.
Saikiran Bulusu, Prashant Khanduri, Pranay Sharma, and Pramod K Varshney. On distributed stochastic
gradient descent for nonconvex functions in the presence of byzantines. In ICASSP 2020-2020 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 3137-3141. IEEE,
2020.
Miguel Castro, Barbara Liskov, et al. Practical byzantine fault tolerance. In OSDI, 1999.
Yudong Chen, Lili Su, and Jiaming Xu. Distributed statistical machine learning in adversarial settings:
Byzantine gradient descent. Proceedings of the ACM on Measurement and Analysis of Computing Systems,
1(2):1-25, 2017.
Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex optimization
via stochastic path-integrated differential estimator. In Advances in Neural Information Processing Systems,
pp. 689-699, 2018.
Jiashi Feng, Huan Xu, and Shie Mannor. Distributed robust learning. arXiv preprint arXiv:1409.5937, 2014.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points—online stochastic gradient
for tensor decomposition. In Proceedings of the 28th Annual Conference on Learning Theory, COLT 2015,
2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry.
Adversarial examples are not bugs, they are features. In Advances in Neural Information Processing Systems,
pp. 125-136, 2019.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle points
efficiently. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 1724-
1732. JMLR. org, 2017.
Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M Kakade, and Michael I. Jordan. On nonconvex optimization
for machine learning: Gradients, stochasticity, and saddle points. arXiv preprint arXiv:1902.04811, 2019.
Jakub Konecny, H Brendan McMahan, Felix X Yu, Peter Richtdrik, Ananda Theertha Suresh, and Dave Bacon.
Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492,
2016.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The CIFAR-10/100 dataset. https://www.cs.
toronto.edu/ ~kriz∕cifar.html, 55, 2014.
Leslie Lamport, Robert Shostak, and Marshall Pease. The byzantine generals problem. ACM Transactions on
Programming Languages and Systems (TOPLAS), 4(3):382-401, 1982.
Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Nonconvex Finite-Sum Optimization Via SCSG
Methods. In NIPS, 2017.
Nancy A Lynch. Distributed algorithms. Elsevier, 1996.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep
learning models resistant to adversarial attacks. In ICLR. arXiv preprint arXiv:1706.06083, 2018.
Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Takac. Sarah: A novel method for machine learning
problems using stochastic recursive gradient. In Proceedings of the 34th International Conference on
10
Published as a conference paper at ICLR 2021
Machine Learning-Volume 70, pp. 2613-2621. JMLR. org, 2017.
Iosif Pinelis. Optimum bounds for the distributions of martingales in banach spaces. The Annals of Probability,
pp. 1679-1706, 1994.
Lili Su and Nitin H Vaidya. Fault-tolerant multi-agent optimization: optimal iterative distributed algorithms.
In PODC, pp. 425-434. ACM, 2016a.
Lili Su and Nitin H Vaidya. Defending non-bayesian learning against adversarial attacks. ISDC, 2016b.
Lili Su and Jiaming Xu. Securing distributed machine learning in high dimensions. arXiv preprint
arXiv:1804.10140, 2018.
Nilesh Tripuraneni, Mitchell Stern, Chi Jin, Jeffrey Regier, and Michael I Jordan. Stochastic Cubic
Regularization for Fast Nonconvex Optimization. ArXiv e-prints, abs/1711.02838, November 2017.
Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Generalized Byzantine-tolerant SGD. arXiv preprint
arXiv:1802.10116, 2018a.
Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Zeno: Byzantine-suspicious stochastic gradient descent.
arXiv preprint arXiv:1805.10032, 2018b.
Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Fall of empires: Breaking byzantine-tolerant SGD by
inner product manipulation. In Uncertainty in Artificial Intelligence, pp. 261-270. PMLR, 2020.
Haibo Yang, Xin Zhang, Minghong Fang, and Jia Liu. Byzantine-resilient stochastic gradient descent
for distributed learning: A lipschitz-inspired coordinate-wise median approach. arXiv preprint
arXiv:1909.04532, 2019.
Dong Yin, Yudong Chen, Kanna Ramchandran, and Peter Bartlett. Byzantine-robust distributed learning:
Towards optimal statistical rates. arXiv preprint arXiv:1803.01498, 2018.
Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Defending against saddle point attack in
byzantine-robust distributed learning. In International Conference on Machine Learning, pp. 7074-7084,
2019.
11
Published as a conference paper at ICLR 2021
Appendix
A Missing Proofs for Section 3
A.1 Proof of Lemma 3.2
Recall the following, useful inequality.
Lemma A.1 (Pinelis’ 1994 inequality (Pinelis, 1994)). Let X1 , . . . , XT ∈ Rd be a random process satisfying
E[Xt∣Xι,…，Xt-ι] = 0 and ∣∣Xt∣∣ ≤ M. Then, Pr [∣∣Xι + …+ XT∣∣2 > 2log(2∕δ)M2T] ≤ δ.
Lemma 3.2 is in fact a direct corollary of the following claim, whose proof is quite classical. Denote by
C = log(16mT /p). Denote by
B⑴：= %,i + ... + Vt-1，i	and B(t)：= RfW + ...+▽/(Wt-I)
i	∣g∞d0l +	+ ∣goodt-ιl	?	∣g∞d0l +	+ ∣goodt-ιl
Recall at iteration t - 1, Algorithm 2 computes {B1(t) , . . . , Bm(t) } as well as some Bm(te)d = Bi(t) where i is any
machine in goodt-ι such that at least half of j ∈ [m] satisfies ∣∣Bjt) — B(t)∣ ≤ 8√tC∕m.
Claim A.2. Let C = log(16mT /p). Then, with probability at least 1 - p/4, we have
(a)	for all i ∈ good and t ∈ [T], ∣B(t) — B?t) ∣ ≤ 4√tC∕m.
(b)	for all t∈ [T], each i ∈ good is a valid choice for Bm(te)d = Bi(t).
(c)	for all i ∈ good and t ∈ [T], ∣B(t) — Bmtedk ≤ 16√tC∕m and ||B?t) — Bmtedk ≤ 12√tC∕m
(d)	for all i ∈ good and t ∈ [T], we also have i ∈ goodt+1.
(e)	Il Pi∈good (Bp- BMll ≤ O(，tlog(T∕p)∕√m).
Proof of Claim A.2. We prove by induction. Suppose the statements hold for t — 1	and	we now move	to t.
(a)	For each i ∈ good, note E[Vt,i] = Vt and ∣Vt,i -Vtk	≤ 1. Let	Xt	= "gO-T,	So that
∣∣Xt∣ ≤ ∣goθd古। ≤(「a)m ≤ m. We can thus apply Lemma A.1 to the Xt and then take a union
bound over all i ∈ good. Thus, with probability at least 1 — 8T We have ∣∣B(t) — B?t) ∣∣ ≤ 4√tC/m for
all i ∈ good. The result follows from a further union bound over t	∈ [T].
(b)	Claim A.2a implies for every i, j ∈ good we have ∣∣B(t) — Bjt) ∣∣	≤ 8√tC/m. Therefore each i	∈ good
is a valid choice for setting Bm(te)d = Bi(t) .
(c)	This is a consequence of the previous items and the definition of Bm(te)d .
(d)	This is a consequence of the previous item.
(e)	We can apply Lemma A.1 with {X1,X2,…，Xt∣go"∣ } = { Vk；敢工，k) }k∈[t],i∈good. It holds with
probability at least1 — 8T that ∣l Pi∈good (Bitt- B?t))|| ≤ O(Pt IOg(T7P)∕√m)
Proof of Lemma 3.2. The property goodt ⊇ good is from Claim A.2d.
The property ∣∣σt∣2 ≤ O(log?P)) is by standard concentration inequalities for sums of bounded random
vectors.
The property ∣∣σo + …+ σt-ι ∣∣2 ≤ O(T '°fp) is from Claim A.2e.
The property ∣∆t∣ ≤ α is obvious as we have at most α fraction of the bad machines.
The bound on ∣∣∆o + •…+ ∆t-ι∣2 can be derived as follows. For every i ∈ [m] \ good, let t be the last
iteration i satisfies i ∈ goodt . Then, by the triangle inequality,
|出产1)— BfSii ≤ -2- + ∣B(t) — B?”
On the other hand, t ∈ goodt implies ∣∣B(t) — Briedk ≤ 16√tC∕m by the algorithm; combining this with
||B?t) — Bmmted k ≤ 12√tC/m, and summing up over all such bad machines i finishes the proof.
The final two properties follow from standard facts about Gaussian random vectors.
12
Published as a conference paper at ICLR 2021
A.2 PROOF OF LEMMA 3.3
Proof ofLemma 3.3. Using the Lipschitz smoothness of f (∙), We have
f (wt) - f(wt+ι) ≥ (Vf (wt),wt - wt+ι} - 2 Ilwt - wt+ι∣∣2
=ηkVf(wt)k2 + η(Vf (wt), Ξti- 2 IlWt- wt+ιk2 + η(Vf (wt),ξti
We first show:
∣Wt - Wt+ιk2 = η2kVf(wt) + Ξt - ξtk2 ≤ 3η2(kVf(wt)k2 + ∣∣Ξt∣∣2 + ∣∣ξt∣∣2)
T-1
I X η〈Vf(Wt),ξt>l ≤ ηt
t=0
T-1
T-1
E IVf(Wt)I2 ∙ O(ν√C1) ≤	0.05η E IIVf(Wt)∣∣2 + O(CIV2η)
t=0
t = 0
The first follows since (a + b + c)2 ≤ 3(a2 + b2 + c2) for any a, b, c ∈ R, and the second follows from
Lemma 3.2. Combining them, and also using that ∣∣Ξt∣∣2 ≤ O(C2), ∣∣ξt∣∣2 ≤ O(dν2C1), and η ≤ 0.01, we
have
T-1	T-1
f (W0) - f (WT) ≥ 0.9η X (∣Vf (Wt)I2 - O(ηC2)) + η X(Vf(Wt), Ξt)- O(ηTν2C1(ηd + T))
t = 0
t = 0
(A.1)
For the inner product on the right hand of (A.1), we have that
T-1
η X (Vf(Wt), 3)= T
t=0
T-1
T-1
T-1 T-1
X DVf(Wq), X ΞtE + T X X〈Vf (Wt)- Vf(Wq), Ξti
(A.2)
q=0	t=0	q=0 t=0
----------V-------------' 1	-
♦-----------------------圣
For the first term 务 we have
T-1
T-1
T-1
T-1
I*∣≤ T X IDVf(Wq), X mt)| ≤ T X kVf(Wq)k ∙ Il X 引
q=0
t=0
t=0
q=0
≤ 0.iηTd IVf(Wq)∣2 + 空
q=0
T-1
T-1	T-1	C
XllX Ξt∣∣
q=0	t=0
≤ 0.iη X IVf(wq)∣2 + O(ηC2)
q=0
where the last inequality follows from Lemma 3.2.
For the second term 品 we have
T-1 T-1
T-1 T-1
周 ≤ T X| X(Vf(Wt) - Vf(Wq), Ξt)∣ ≤ T X∣ X (V2f (W0)(Wt - Wq), Ξt>∣
q=0 t=0
q=0	t=0
1-------------------
♦
✓
✓
T-1 T-1
+ T X X (IWt- w0i + llWq - W0I)IWt - Wq iiξM
q=0 t=0
|	、,	,
{z
D
✓
Using ∣∣Wt - WqI ≤ ∣∣Wt - W0∣ + ∣∣Wq - W0∣, one can derive
T-1 T-1
°≤ TXX(IWt-W0I + ∣∣Wq - W0I)2 ∙ O(√C2)
q=0 t=0
T-1
≤ η X ∣∣Wt - W0I2 ∙ O(√C2)
t=0
T-1
≤ η3 ^X IVf(W0) + ∙∙∙ + Vf (Wt-1) + Ξ0 + ∙∙∙ + Ξt-1 + ξ0 + ∙∙∙ + ξt-112 ∙ O(√C2)
t=0
T-1
≤ O(√Cη3T2) X IVf(Wt)I2 + O(√¾C2η3T2) + O(η3ν2T2dC1√C2)
t=0
13
Published as a conference paper at ICLR 2021
As for ◊,
T-1	T-1	q-1
I E (V2f (wo)(wt - Wq), Ξti I ≤ I E (V2 f (wo)(wt - Wq), Ξt) | + | Eyf(Wʤ - Wq), ≡t) |
t = 0	t = q+1	t = 0
For the first term (and the second term is analogous), We have
T-1
I E (v2f (W0)(Wt- wq), Ξti I
t = q+1
T-1
=η I E (V2f (W0)(Vf (wq) +------Vf(Wt-I) + ≡q +----≡t-1 + ξq +----+ Et-I) , ≡ti 1
t = q+1
T-1
≤ ηI E (V2f(W0)(ξq + ∙∙∙ + ξt-1), Ξti I+
t = q+1
T-1	T-1
η I E (V2f (w0)(Vf (wq) + ∙∙∙Vf (wt-1)), Ξti I+ η ∖ E (V2f(w° )(Ξq + ∙∙∙Ξt-1), Ξ ∖
t=q+1	t=q+1
①
≤ η ∙ O(,dV2TCI ∙ √TC2)+
T-2
η I E (V2f (w0)Vf (wt) , Ξt+1 + ∙∙∙ + Ξt-11 + 2〈V2 f (w0)(Ξq + ∙∙∙ Ξt-1), (Ξq + ∙∙∙ Ξt-1)〉
t=q
________ T-2
≤ η ∙ O(√dν2TC1 ∙ √TC2) + η E kVf(wt)∣∣∣∣Ξt+1 + ∙∙∙ + Ξt-1∣∣ + 2 ∣∣Ξq + ∙∙∙ Ξt-1∣∣2
t=q
②	T-1
≤ O(η√TC2) ∙ E IIVf(Wt)Il + O(TηC2 + Tην2dC1).
t=0
Above, inequality ① USeS ∣∣Ξ0 +-Ξt ∣ ≤ O(√TC2) for C2 = α2 log mpτ + "吗"，(see Lemma 3.2) and
a delicate application of Azuma,s inequality that we state at the end of this subsection (see Proposition 3.4);
Inequality ② uses Young,s inequality and Lemma 3.2.
Putting this back to the formula of ◊, we have
τ-1
◊ ≤ O(η2√TC2) ∙ E l∣Vf(Wt)I + O(Tη2C2 + Tη2ν2dC1)
t=0
T-1
≤ 0.1η E ∣Vf(wt)∣2 + O(η3T2C2 + Tη2C2 + Tη2ν2dC1)
t=0
Finally, putting ◊ and 8 back to 帛，and putting 帛 and ⅛ back to (A.2) and (A.1), we have
T-1	T-1
f(W0) - f(WT) ≥ 0.8η E IVf(Wt)I2 - O(√Cη3T2) E ∣Vf(wt)∣2
t=0	t=0
-C2 ∙ O(η + η2T + η3T2 + √Cη3T2) - C1 ∙ O(Tη2ν2d + T2η3ν2√C2 + ηTν2(ηd + T))
together with T = 菽市+^^)and η ≤ 0.01 min{1,卷}, we have
τ-1
f (W0) - f (wτ) ≥ 0.7η E IVf(Wt)I2 - C2 ∙ O(η + η2T + η3T2 + √Cη3T2) - C1 ∙ O(Tην2η(d + √C2))
t=0
T-1 /	1	、
=0.7η E ( IVf(wt)I2 - C2 ∙ O(- + η + η2T + Eη2T) - C1 ∙ O(ηTν2η(d + √C2)))
t=0	T
T-1
≥ 0.7η E (IVf(Wt)I2 - η ∙ O(C2 + (C?)1，5) - O(C1ν2η(d + √C2))).
t=0
□
14
Published as a conference paper at ICLR 2021
A.3 Proof of Proposition 3.4
Proposition 3.4. Fix the dimension parameter d ≥ 1. Suppose ξ0, . . . , ξT-1 ∈ Rd are i.i.d. drawn from
N (0, I), and that ∆1 , . . . , ∆T-1 are arbitrary vectors in Rd. Here, each vector ∆t with t = 1, . . . , T - 1 can
depend on ξo,..., ξt-ι but not on ξt,..., ξτ-ι. Suppose that these vectors satisfy ∣∣∆ι + •…+ ∆t k2 ≤ Tfor
every t = 1, . . . , T - 1. Then, with probability at least 1 - p,
∣PT=ι1hξo + …+ ξt-ι, ∆ti∣ ≤ O(PdTTlog(T∕p)).
Proof of Proposition 3.4. Using the identity formula6
PT-L1hξ0 + …+ ξt-ι, ∆ti =(PT:02 ξt)(PT:11 ∆t) - PT=ι2hξt, ∆1 + …+ ∆ti
we have
IPT-L1〈& +	+ ξt-1,δ方)∣ ≤ IPT-2ξt∣∣ ∙ IIPT-L1 δ*∣∣ +1PT-L2hξt,δi +	^til .
≤ O(PdTTlog(T∕p)) + ∣PT=12hξt, ∆1 + …+ ∆ti∣ .
where the last inequality uses ∣∣ξo + …+ ξT-2∣ ≤ O(∖JdTlog(1∕p)) With probability at least 1 — p/2.
Furthermore, we note that ξt is independent of ξ0 , . . . , ξt-L , ∆L , . . . , ∆t and E[ξt] = 0. Therefore, letting
St = hξt, ∆1 +-+ ∆ti, we have E[St∣ξo,..., ξt-1] = 0; furthermore, with probability at least 1 — p/2, it
satisfies |St| ≤ O(ʌ/dT log(T∕p)) for every t. Finally, by Azuma's inequality, we have
∣PT=12hξt, ∆1 + …+ ∆ti∣ ≤ O(PdTTlog(T∕p)) .	口
A.4 Proof of Lemma 3.5
Proof ofLemma3.5. Let us denote by rt = [ξ2]1 = — [ξ2]1 and we know rt -N (0, ν2). We can write
wa+1 — wb+1 = ηrte1 + Wa — Wb — η(Vf (wa) — Nf(Wt)) — η(Ξa — Ξb)
Using the second-order smoothness, we have
Vf(Wta) — Vf(Wtb)
Z1
τ=0
V2f (Wa + T(Wt — Wa))(Wa — wb)dτ
V2f(wo) ∙ (Wa — Wb) + θt
for some vector ∣∣θt∣ ≤ max{∣Wa — Wa∣, ∣Wb — Wb∣∣}∙∣∣Wa — Wb∣. Therefore, we have
Wa+1 — Wb+1 = ηrte1 + (I — ηV2f (Wo))(Wa — Wb) — η(Ξa — Ξb + θt)
Now, giving ψo = ψ0 = 0, imagine two sequences
•	Ψt+1 = ηre1 + (I — ηV2f (Wo))ψt and
•	Ψt+1 = ηrte1 + (I — ηV2f (Wo))Ψt — η(Ξt — Ξb + θt) = Wa+1 — Wb+1
We will inductively prove ∣∣ψt — ψ方∣ ≤ 2 ∣∣ψt∣∣.On one hand, it is easy to see that ψt is zero except in the
t-1	2ν 2
first coordinate, in which it behaves as a Gaussian with zero mean and variance Ek=O(I + ηδ)2k ∙ η-^~ =
Θ ((1+ηδ)2t ∙ η2ν2). By Gaussian tail bounds, we know that
•	with probability at least 0.99, it satisfies ∣∣ψt ∣∣ ≤ O(√ηC1√(1+ηδ)t) for every t
•	with probability at least 0.99, it satisfies ∣∣ψT ∣∣ ≥ 1^ (√ην(√+ηδ) )
In the rest of the proof, we condition on this event happens. We prove towards contradiction by assuming
∣Wta — W0a ∣ ≤ R and ∣Wtb — W0b ∣ ≤ R for all t ∈ [T].
We will inductively prove that ∣∣ψt — ψ方 ∣ ≤ 21而(√ην√+ηδ) ). We calculate the difference
t-1
ψt — Ψt = η X (I — ηV2f (Wo))t-1-i(Ξa — Ξb + θi)
i=0
Let g = 1渭一连北,then we can inner-product the above equation by vector g, which gives
t-1
l∣ψt — Ψt∣ = η X(ma — Ξb + θi, (I — ηV2f(Wo))t-1-ig)
i=0
6We thank an anonymous reviewer on openreview who pointed out this simpler proof to us.
15
Published as a conference paper at ICLR 2021
① t-1
≤η
i=0
(〈ma - Ξb, (I - Nf(Woy)-IgE + R ∙O( (√Cv√+ ηδ)i) ∙(1 + ηδ)t-1-i)
≤ ηX〈ma - Ξb, (I- Nf(Wo)-Ig) + O(RnT√Cν(ι + ηδ)t)
i=0
where the inequality ① uses ∣∣θi∣∣ ≤ R ∙∣∣ψi∣∣ ≤ R ∙ (∣∣ψi∣∣ + kψi - Ψik), ∣∣(I - nV2f(wo))t-1-ig∣∣ ≤
(1 + nδ)t-1-i, and the inductive assumption. Let Us call M = (I — nV2f (wo)), and focus on
X〈ma, (I - nV2f(wo)) j-igE
i=o
t-2
=hΞ0 + …+ ma-i ,gi + X(mo + …+ ma, Mt-1-ig - Mt-2-ig)
i=o
t-2
≤ kmo + …+ ma-ι∣H∣gk + X kmo + …+ ma∣∏∣Mt-1-ig- Mt-2-igk
i=o
≤ O(√TC2 (kgk + X ∣∣Mt-1-ig - Mt-2-igk)	(using Lemma 3.2)
≤ O(√TC2 ∙ (1 + nδ)t-1)
Together, we have
kψt - Ψtk ≤ o(n√TC2) ∙ (1 + nδ)t + O(RnT^nCν(ι + nδ)t)
δ
Under our assumption, we have ∣∣ψt — Ψtk < 2⅛0 (√ην√+η" ) and therefore ∣∣ψτ ∣∣ ≥ ∣∣ψτ ∣-∣ψτ - ψτ ∣∣ ≥
200θ0 (√ην√+ηδ)t). Thus, within T iterations, We have ∣∣ψt∣ > R and this gives a contradiction.	口
B Final: Double Safe Guard
We now come to our final Algorithm 3 which is our perturbed SGD algorithm with two safeguards. The two
safeguard algorithm naturally divides itself into epochs, each consisting of T1 iterations. We will demonstrate
that within most epochs, we make good progress. Thus, consider some iterate xmT1 , for some m < T/T1 .
Our goal will be to argue that we make good function value progress by iterate x(m+1)T1 , and that we do not
settle into any saddle points. To slightly simplify notation, let Wo = xmT1 , and let the sequence of iterates be
Wo, . . . , WT1-1, so that WT1-1 = x(m+1)T1 -1. For completeness’ sake we rewrite this as Algorithm 1.
Algorithm 3 Perturbed SGD with double safe guard (for analysis purpose)
Input: w0 ∈ Rd, set good0 ⊇ good, rate η > 0, lengths T1 ≥ T0 ≥ 1, threshold T1 > T0 > 0;
1:	for t — 0 to Ti 一 1 do
2:	last J max{to ∈ [t]: to is a multiple of Tq}
3:	for each i ∈ goodt do
4:	receive Vt,i ∈ Rd from machine i;
5:	Ai J Pk=0 TgoodiJ and Bi J SPk=IaSt TgoodiJ;
6:	Amed J Ai where i ∈ goodt is any machine s.t.	{j ∈ goodt :	kAj 一 Aik	≤	T1}	> m/2.
7:	Bmed J Bi where i ∈ goodt is any machine s.t.	{j ∈ goodt :	kBj 一 Bi k	≤	Tq}	> m/2.
8:	goodt+i J {i ∈ goodt ： kAi 一 Amedk ≤ 2Ti V	IBi- BmedIl	≤ 2T0};
9:	wt+i = Wt — η (ξt + |go：dt| Pi∈goodt Vt,i);
Our main result is the following theorem.
Theorem B.1. Let C3 = α2 + -m. Suppose we pick parameters p,δ ∈ (0,1), n ≤ 0(CC3), V = Θ(C3),
To = θ(鼻),Ti = θ( η1δ) ≥ To, Ti = Θ (√T1), and To = Θ (√T0). Then, starting from Wo,
16
Published as a conference paper at ICLR 2021
(a)	with probability at least 1 - p we have
T1 -1
f(wo) - f(wτ) ≥ 0.7η X (lNf(wt)k2 - O(ηC3d).
t=0
δ
(b)	As long as ∣∣ wt — wo k ≥ R for some t ∈ {1, 2,...,Tι} and for R = Θ(δ) ≤ 多 then With probability at
least 1 - p we have then
T1 -1
f (wo) — f(wτι) ≥ 0.5η X (—O(ηC3d)) + Ω(δ3)
t=o
(c)矿λmin(V2f (wo)) ≤ —δ, we also have with probability at least 0.45,
T1 -1
f (wo) — f (wτι) ≥ 0.5η X (—O(ηC3d)) + Ω(δ3)
t=o
B.1 Why Theorem B.1 Implies Theorem 2.3
Using the parameter choice η = Θ(C^d) from Theorem 2.3, We know O(ηC3d) ≤ 0.1ε2. We claim two
things:
•	For at least 90% of the epochs, they must satisfy (denoting by wo and wT1 the beginning and ending points
of this epoch)
f (wo) - f(wτι) ≤ 20 f (XO) -∕min f (X) ≤ ε1.5
T/T1
The last inequality uses our choice of T and δ = Θ(√ε).
The reason for this is by way of contradiction. Suppose for at least 10% of the epochs it satisfies
f (wo) — f (WTI) > 20 f (x0)—minf(x), then, for the remainder of the epochs, they must at least satisfy
f (wo) — f (wτι) ≥ — 0.7ηT1 ∙ 0.1ε2. Summing over all the epochs, we shall obtain f (xo) — f (xt) >
f(Xo) — min f(X) but this gives a contradiction.
•	For at least 40% of the epochs, they must satisfy the three properties from Theorem B.1.
In particular, for at least 30% of the epochs, they must satisfy both. Since ε1.5 is so small that
T1-1
ε1.5 ≥ f (wo) — f (wτ1) ≥ 0.5η X (—O(ηC3d)) + Ω(δ3) ≥ Ω(δ3) — 0.05ηTιε2
t=o
would give a contradiction (for instance, one can choose δ to be slightly larger than √ε by some log factors),
this means, for those 30% of the epochs, they must satisfy:
•	ε1.5 ≥ 0.7ηPT=-I (∣∣Vf(wt)∣2 — 0.1ε2),
•	∣∣wt — wo∣ ≤ 2 for every t = 1, 2,...,T1, and
•	V2f (wo)占—δI.
The latter two properties together implies V2f (Wt)占—21 for every t = 1, 2,...,T1 (by the second-order
smoothness). The first property implies for at least 90% of the iterations t in this epoch, they must satisfy
∣Vf (X)∣ ≤ ε. This finishes the proof of Theorem 2.3.
B.2 Proof of Theorem B.1
We first have the following lemma
Lemma B.2 (double safe guard).	In Algorithm 3, suppose Ti = 8ʌ/ɪɪ log(16mT1∕p) and To =
8 ʌ/ɪo log(16mTι∕p). Then, with probability at least 1 — p/2 ,for every t = 0,...,Tι — 1,
•	goodt ⊇ good.
•	kσtk2 ≤ O(log(m/p1), k∆tk2 ≤ α2, kξtk2 ≤ O(ν2dlog(Tι∕p)),
•	∣σo + ∙∙∙+ σt-ι∣2 ≤ O(TI lo*1∕p)), ∣σiast + …+ σt-ι∣2 ≤ O( T R1/P))
•	∣∣∆o +---+ ∆t-i∣2 ≤ O(α2Ti log(mTi/P)) and ∣∆iast +--------+ ∆t-ι∣2 ≤ O(α2To log(mTι/P))
•	∣ξo + …+ ξt-ik2 ≤ O(ν2dTi log(Tι∕p)) and ||5§$ + …+ ξt-ι∣2 ≤ O(ν2dTolog(Tι∕p)).
We call this probabilistic event EventdTo1u,bTle0 (wo) and Pr[EventdTo1u,bTle0 (wo)] ≥ 1 — P/2.
The proof is a direct corollary of Lemma 3.2, by combining events EventsTingle (wo), EventsTingle (wo),
17
Published as a conference paper at ICLR 2021
EventsTingle (wT0), EventsTingle (w2T0) and so on. The next lemma is a simple corollary by repeatedly applying
Lemma 3.3. It proves Theorem B.1a.
Lemma B.3 (modified from Lemma 3.3). Denote by Ci = log(T1∕p) and C2 = α2 log mT1 + IOg(TI/p).
Suppose η ≤ 0.01min{1, 吉}, To = ^。贝)，。2)and T ≥ To. Westartfrom wo and apply Algorithm 3.
Under event EventdTo1u,bTle0 (w0), it satisfies
T1-1
f(wo) - f(wτι) ≥ 0.7η X (kVf(wt)k2 - η ∙0(C2 + (C2)1.5) - o(Ciν2η(d + √C2)))
t=o
The next lemma can be easily derived from Lemma 3.5.
Lemma B.4 (modified from Lemma 3.5). Suppose
δ	δ3
R = Θ(  ___________________) and ν2 = Θ(C2 log ——-)
(√C1 iog(δ3/ηC2))	( 2 g ηC2)
Suppose η ≤ 0.01min{1, C }, To = 100η(i+√c2) and T = Θ(得 log 急)≥ To. Let wo ∈ Rd be any
point in the space and suppose λmin (V2 f (wo)) ≤ -δ for some δ ≥ 0. Given two coupled sequences defined
as before, under events EventdTo1u,bTle0 (woa) and EventdTo1u,bTle0 (wob), we have with probability at least 0.98
max {f(wa) - f(wTι),f(wb) - f(wTJ}
T1 — 1	卉3
≥ 0.5η X (-η ∙ 0(C2 + (C2)1.5) - O(CIlV2η(d + √C2))] + ω(	δ3 )
=v	C	Ci log3 η⅛
Lemma B.4 directly proves the second half of Theorem B.1c, because given two coupled sequences with the
same marginal distribution, we have
Pr[f(wa) - f(wwT1) ≥ X] ≥ 2 Pr[max {f (wa) - f(wTι),f(wb) - f(wTj} ≥ X]
Proof of Lemma B.4. Our choice on r and R satisfy the requirement of Lemma 3.5. Suppose without loss of
generality that the wta sequence leaves wo by more than R. Let Tia be the first iteration t ≤ Ti in which
kwta - woa k ≥ R.
kwTa - Wak2 = η2kVf(wa) +-------+ Vf(WTa-1) + Ξo +----+ ΞTa-ι + ξa +-----+ ξτa-ι||2
T1a-i
≤ O(η2T1) X |Vf(wta)|2 + O(C2η2T1) + O(C1η2ν2T1d)
t=o
Combining this with Lemma B.3, we have
f(woa) - f (wTa 1a ) ≥
T1a -1	|wa	wa |2
0.5η X (kvf(wa)k2- η ∙O(C2 + (C2)1.5) - O(C1V2η(d + √C2))) + U T0OnTʃ
T1	/	∖	R2
≥ 0.5n X (kVf(wa)k2 - n ∙ O(C2 + (C2)1.5) - O(C1V2n(d + √C2))) + ɪʒ^
Combining this with Lemma B.3 again but for the remainder iterations, we have
T1-1 /	∖	R2
f(wo) - f(WT1) ≥ 0.5n E (kVf(wα)k2 -n∙0(C2 + (C2)1.5) - O(CIVn(d + √C2))) + ——
1	100ηT1
In fact, the above same proof of Lemma B.4 also implies Theorem B.1b. These together finish the proof of
Theorem B.1.
C More on Experiments
We conduct experiments on training a residual network ResNet-20 He et al. (2016) on the CIFAR-10/100 image
classification tasks Krizhevsky et al. (2014).
18
Published as a conference paper at ICLR 2021
C.1 Setting and Implemented Methods
In all of our experiments, we use 10 workers and mini-batch size 10 per worker. Given any attacker and any
defender algorithm, we run SGD three times for 140 epochs, each time with a different initial learning rate
η ∈ {0.1, 0.2, 0.4}.7 We let the learning rate decrease by a factor of 10 on epochs 80 and 110, and present
present the best testing accuracies in the three runs (each corresponding to a different initial learning rate).
We use standard data augmentation (random crops, random flips, and channel normalization).
We compare against Geometric Median Chen et al. (2017), Coordinate-wise Median Yin et al. (2018; 2019),
Krum Blanchard et al. (2017), and Zeno Xie et al. (2018b) with attacks. We set α = 0.4 so there are 4
Byzantine workers. (This exceeds the fault-tolerance of Krum, and so we also tested Krum with only 3
Byzantine workers.) We formally define those prior works as follows.
Definition C.1 (GeoMed Chen et al. (2017)). The geometric median of {y1, ..., ym}, denoted by
geo_med{yi,…,ym}, is
geοmed{yι,…,ym) := argminy∈Rd Pm=IIly - yi∣∣
In our experiments, we choose the geometric median from set {y1 , ..., ym }.
Definition C.2 (coordinate-wise median Yin et al. (2018; 2019)). Coordinate-wise median g =
med{y1 , ..., ym } is defined as a vector with its k-th coordinate being g[k] = med{y1 [k], ..., ym [k]} for each
k ∈ [d], where med is the usual (one-dimensional) median.
Definition C.3 (Krum Blanchard et al. (2017)).
KR{y1, ..., ym} := yk where k = arg min Iyi - yj I2
and i → j is the indices of the m- b- 2 nearest neighbours of yi in {y1 , ..., ym } \ {yi } by Euclidean distances.
Note that Krum requires 2b + 2 < m. So, we have also repeated the experiments for Krum with 3 Byzantine
workers (out of 10 workers) to be more fair.
Definition C.4 (Zeno Xie et al. (2018b)).
1 m-b
Zenob{yι,…，ym} = -----y £ y(i)
m-b i=1
where {ye(i) : i ∈ [m]} are the gradient estimators with the m - b highest “scores”, and the so-called
stochastic descendant score for any gradient estimator u, based on the current parameter x, learning rate η,
and a constant weight ρ > 0, is defined as:
Scoreη,ρ(u, x) = fr(x)	- fr(x - ηu) - ρkuk2
fr (x) - fr (x - ηu) is the estimated descendant of the loss function and ρkuk2 is the magnitude of the update.
In our experiments, we let fr (x) be the estimated objective over a mini-batch of size nr = 10 (so the time to
perform this estimation is on the same magnitude as the gradient evaluation for each individual worker). We
also chose ρ = 0.0005 (and this value does not affect our experimental results by much).
Safeguard SGD. Our Algorithm 1 is stated in a way to make our theoretical proofs as clean as possible. Here,
we discuss how we actually implement it in practice.
First of all, as common in the literature, we omit the Gaussian noise ξt that is added for theoretical purpose,
and instead rely on the natural noise in the training process to escape saddle points.
Also, we make universal choices for our safeguard window sizes (across all attackers): for our algorithm with
a single safeguard we have used a universal window size T = 3 epochs, and for our algorithm with double
safeguards we have used window sizes T0 = 1 epoch and T1 = 6 epochs.
We also provide an automatic empirical process to select safeguard thresholds and eliminate bad workers.8 The
process to determine Amed (and likewise for Bmed ) is described as follows. In each iteration, for every worker
i ∈ [m], we sort kAi - Aj k j∈[m] and pick the smallest dm/2 + 1e-th entry, and let this number be the
“score” for worker i. We select the worker with the smallest “score” as Amed and call its “score” S. Then, we
use 1.5 min{S, 5} as the safeguard threshold for this iteration. Namely, we declare any worker j satisfying
7Recall a typical suggested initial learning rate is 0.1 for training ResNet with SGD+momentum; since we
are using SGD without momentum, the initial learning rate can be appropriately enlarged.
8In our first version of the paper, we pre-run the algorithm for 20 epochs to determine safeguard thresholds;
in the newer version, we have avoided the pre-run.
19
Published as a conference paper at ICLR 2021
kAj - Amed k ≥ 1.5 max{S, 5} as a bad worker.9
C.2 Experiment Results by Attacks
The ideal accuracies are 91.7% / 68.0% for CIFAR-10/100, which correspond to applying SGD using only the
stochastic gradients from the honest workers. Below we discuss about the experimental results one attack at a
time.
C.2. 1 Variance Attack
We begin by looking at the hardest proposed attack from prior works. The Variance attack follows the
strategy prescribed by Baruch et al. (2019), by which Byzantine workers collude in order to shift the mean
among all gradients by a factor β times the standard deviation of the gradient, while staying within population
variance. More precisely, the maximal change to the mean that can be applied by an attacker without the
fear of being detected, by using the properties of the Normal distribution, specifically the cumulative standard
normal function, and compute the maximal possible shift so that the attackers’s values stay within population
variance. (See (Baruch et al., 2019, Algorithm 3) for a precise description. Our β is zmax in their notation.) We
implement this strategy coordinate-wise, the same way as they did. Their work observes that the shift β can be
non-trivial in practice, since stochastic gradients tend to have large variance in neural network training (which
we also observed in our setup). Critically, the attack cannot be defended against by historyless algorithms, as
the attacker’s values are statistically indistinguishable from a regular execution in a single iteration.
In our setting, for 10 total nodes and α = 0.4, β is upper bounded by 0.3 (following the classic tables for the
cumulative normal). We also ran the same attack in the setup from their paper (50 nodes total, of which 24 are
Byzantine, which allows β 〜1.5) and observed a similar outcome. Results for this experiment are given in
Figure 3.
epochs
(a) CIFAR-10
Figure 3: Performance comparison under the variance attack.
epochs
(b) CIFAR-100
As shown by the results, our algorithm provably circumvents this attack, and recovers full accuracy. This is
explained by the fact that the algorithm has memory: in particular, Byzantine nodes following this strategy will
progressively diverge from the (honest) “median” Amed (ata “linear” rate, recall Figure 2(a)), and therefore will
eventually exceed the threshold and be marked as malicious by the algorithm.
Specifically, both variants of the algorithm successfully catch all the bad nodes after at most 150 iterations.
Indeed, at the 100-th iteration, the pair-wise distances kAi - Aj k among good workers i, j ∈ good are between
5.3 and 6.3, but the pair-wise distance between a good and a bad worker is at least 12.5.
C.2.2	Sign-Flipping Attack
We next move onto the sign-flipping attack. Recall that, in a sign-flipping attack, each Byzantine worker sends
the negative gradient to the master. This is still a strong attack since if one does not avoid any bad workers, the
test accuracy will suffer from a significant drop. The results are in Figure 4.
From the plots, one can see that again our single and double safe-guard algorithms both outperform prior
works. They also successfully catch all the bad workers within 150 iterations. (For instance, at iteration 150
for CIFAR-10 training, the distance kAmed - Aj k for a good worker j ∈ good is at most 6.9, but for a bad
9This process appears a little different from Algorithm 1, buta similar proof also holds for this new empirical
process. The constant factor 1.5 requires no tuning, and the constant threshold 5 is chosen so that the stochastic
gradient of batch size 500 at random initialization has Euclidean norm no more than 5.
20
Published as a conference paper at ICLR 2021
Figure 4: Performance comparison under the sign-flipping attack.
worker j 6∈ good it can be more than 11.)
In contrast, prior work Zeno completely fails because locally at a training step, using merely nr = 10 samples
to evaluate the objective, it is statistically not possible to even distinguish if the sign of the stochastic gradient
is flipped or not. For prior works Krum and GeoMedian, although they appear to have some non-negligible
performances, but they are actually no better than simply applying SGD with the naive mean of gradients from
all the workers (including those from bad workers).10 Therefore, we conclude that prior works all fail to be
Byzantine fault tolerant under this attack.
C.2.3 Delayed-Gradient Attack
Recall that, in a delayed-gradient attack, each Byzantine worker sends an old gradient to the master. In our
experiments, the delay is of D = 1000 iterations (= 2 epochs). We believe this is not a very strong attack,
because delayed gradients are not sufficiently malicious: they are still “correct” to certain extent albeit being
delayed. The results are shown in Figure 5.
epochs	epochs
(a) CIFAR-10	(b) CIFAR-100
Figure 5: Performance comparison under the delayed-gradient attack.
From the plots, one can see that our single and double safe-guard algorithms again both match the ideal
accuracies. All the prior works suffer from a significant performance loss under this attack.
Itis worth noting that our single and double safe-guard algorithms do not catch any bad worker under this attack,
so they simply use the “naive mean” of gradients from all the workers (including those delayed gradients from
bad workers). However, there is no performance loss even if we use those delayed gradients. That is why we
believe the delayed-gradient attack is not very strong, as the gradients are not sufficiently malicious.
Prior work Zeno suffers from some performance loss, because it only uses 6 workers out of 10, in which
statistically only 6 X 0.6 ≈ 3 〜4 gradients are correct.11 Other prior works suffer from performance loss,
10We did not include this “naive mean” algorithm in the plots for cleanness, but under the sign-flipping
attack, it gives 81.4% test accuracy on CIFAR-10 and 38.3% on CIFAR-100. (This should not be surprising,
since using (1 - α)m = 6 positive gradients plus αm = 4 negative gradients still gives non-negligible
information about the true gradient.)
11In fact, we observed Zeno slightly favors delayed gradients, where each delay gradient is chosen with
probability 63%, comparing to true stochastic gradients each chosen with probability 58%.
21
Published as a conference paper at ICLR 2021
because they only pick one single stochastic gradient from the 10 workers, and it is sometimes even from the
bad worker.
C.2.4 Label-Flipping Attack
Recall that, in the label-flipping attack, each Byzantine worker computes its gradient based on the cross-entropy
loss with flipped labels: for CIFAR-10, label ` ∈ {0, ..., 9} is flipped to 9 - `, and for CIFAR-100, label ` is
flipped to 99 - `. The results are shown in Figure 6.
Figure 6: Performance comparison under the label-flipping attack.
From the plots, one can see that our single and double safe-guard algorithms even outperform the “ideal
accuracies.” (92.4% accuracy vs “ideal accuracy” 91.7% under CIFAR-10; 69.4% accuracy vs “ideal accuracy”
68.0 under CIFAR-100.) In addition, we have found out that the safeguard algorithms did not catch any
bad worker. This should not be surprising, since label-flipping (a.k.a. label smoothing) is known to be a
regularization technique to actually improve test accuracy, as opposed to hurt performance.
Zeno also performs well under this attack (but it does not outperform the ideal accuracy). We have investigated
into Zeno, and found out that it cannot distinguish good workers from bad workers under label-flipping attack;
and therefore Zeno effectively always runs under 6 random workers as opposed to using the full power of the
m = 10 workers (recall Zeno picks 6 workers with the topmost scores, see Definition C.4). This explains its
(minor) under-performance comparing to safeguard.
Other prior works perform significantly worse, and this should be alarming since label-flipping is one type of
smoothing technique to improve test accuracy, as opposed to an actual “attack” to hurt performance.
C.2.5 S afeguard Attacks
Finally, in the safeguard attack that we design, Byzantine workers send negative but re-scaled gradient to the
master. We choose the re-scale factor so that it hardly triggers the safe-guard conditions at the master. From our
experiment, choosing the re-scale factor as 0.6 in all the cases do not trigger the safe-guard conditions, while
choosing a re-scale factor as 0.7 enables the algorithm to catch Byzantine workers occasionally. Our results are
shown in Figure 7 (for re-scale factor 0.6) and Figure 8 (for re-scale factor 0.7).
Figure 7: Performance comparison under the safeguard attack with re-scale factor 0.6. (Recall this attack is
designed to maximally impact the performance of our algorithm.)
22
Published as a conference paper at ICLR 2021
Re-scale factor 0.6. In Figure 7, the performance of our (single and double) safeguard algorithms indeed
get hurt a bit. Recall in Figure 7 the re-scale factor 0.6 is chosen to maximally impact our algorithm. The test
accuracy drops from 91.7% to 89.3% under CIFAR-10; and drops from 68.0% to 60.0% under CIFAR-100 (for
both single and double safeguards). In these cases, we confirm that both versions of the safeguard algorithms
did not catch any bad worker. However, this still significantly outperforms all prior works.
Figure 8: Performance comparison under the safeguard attack with re-scale factor 0.7. (In this case, our our
algorithm can catch some bad workers, and thus perform nearly optimally.)
Re-scale factor 0.7. In Figure 8, we present the scenario when the re-scale factor is 0.7, so that the safeguard
algorithms can occasionally catch some bad workers (depending on the randomness and learning rate). We
confirm that in the three runs of single safeguard, it catches 1, 2, 3 bad workers for CIFAR-10, and 1, 0, 0 bad
workers for CIFAR-100 respectively; in the three runs of double safeguard, it catches 1, 2, 4 bad workers for
CIFAR-10, and 2, 2, 2 bad workers for CIFAR-100 respectively.
Since there is a significant performance gain when our safeguard algorithms catch bad workers, this explains
why safeguard algorithms in Figure 8 outperform their counterparts in Figure 7 with rescale factor 0.6. At the
same time, we notice that the double safeguard algorithm has the ability to catch bad workers more easily. This
is why double safeguard significantly outperforms single sageguard in Figure 8.
In contrast, all other prior algorithms perform extremely bad under this attack. To some extent, safeguard attack
is even stronger than the previously proposed variance attack, since it can drag the 100-class test accuracy on
CIFAR-100 for all prior defense algorithms to nearly 1%, while variance attack can only drag them down to
around 10%.
C.3 Full Comparison Table
We also include the full test accuracy comparison table in Table 1.
	single SafegUa rd	double SafegUa rd	coord median	geo median	KrUm	Krum(3 faulty nodes)	7 Zeno
	 variance attack	92.02	91.75	2L43	22∙01		 21.47	22.4		 33.42
sign-flipping attack	91.93	92.08	22	59.65	57.03	70.87	22∙4
label-flipping attack	92.33	9244	3L93	83.07	83.18	83.52	91.66
delayed-gradient attack	91.58	91.42	29.43	74.36	61.81	79.29	77.27
SafegUard(X0.6) attack	89.01	89.26	21.44	23.12	12.48	28.66	74.46
SafegUard(X0.7) attack	91.24	92.08	21.61	19.95	15.17	24.52	33.36
	single SafegUard	double SafegUard	coord median	geo median	KrUm	KrUm (3 faulty nodes)	Zeno
variance attack	68.27	67.95	6.6	5.81	5.05	6∙1	10.87
sign-flipping attack	68.02	68.08	2.13	10.19	10.93	28.34	2.59
label-flipping attack	69.43	68.8	5.34	51.85	52.13	51.66	67.86
delayed-gradient attack	67.03	66.42	4.04	36.34	31.43	44.85	36.6
SafegUard(X0.6) attack	59.87	60	2.01	1.9	1.25	1.72	5.02
SafegUard(X0.7) attack	59.84	64.91	2.07	1.97	1.32	1.55	3.31
Table 1: Table of test accuracy performances for CIFAR-10 (above) and CIFAR-100 (below).
23
Published as a conference paper at ICLR 2021
C.4 Attack Against the Convex Algorithm of Alistarh et al. (2018)
We now briefly describe an attack against this algorithm. The attack specifically leverages the fact that the
algorithm does not use sliding windows.
One can first run the vanilla SGD to compute “the maximum deviation per good worker” for the accumulation
vector used by the algorithm PT=° Vt. This maximum deviation is therefore a lower bound for the threshold
used in their algorithm. Next, we design an attacker who evenly distributes this total allowed deviation to e.g.
5 consecutive epochs, and behaves honestly for the remaining epochs. Such an attacker cannot be identified by
this algorithm, because its total deviation across all the iterations is identical to that ofa good worker. However,
this leads the algorithm to divergence.
Specifically, suppose 4 Byzantine workers all maliciously report their stochastic gradients multiplied by the
scalar -5, and the remaining 6 good workers report their true stochastic gradients. One can verify numerically
that this attacker can run for 5 consecutive epochs (say, epochs a, a + 1, a + 2, a + 3, a + 4) without being
caught by the algorithm. Now,
•	if a ≤ 75, within just 1 epoch of attack, the neural net weights diverge (value NAN).
•	if 80 ≤ a ≤ 115, this attack is applied after the first learning rate decay. Within just 1 epoch of the attack,
the objective explodes and accuracy becomes 10% (random), and within 3 epochs the algorithm diverges
completely.
•	if 120 ≤ a ≤ 155, this attack is after the second learning rate decay. Within just 2 epochs of attack, the
accuracy drops to 11%. Later, the accuracy never recovers above 40%.
24