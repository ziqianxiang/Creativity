Published as a conference paper at ICLR 2021
Generalized Energy Based Models
Michael Arbelt Liang Zhou & Arthur Gretton
Gatsby Computational Neuroscience Unit, University College London
Ab stract
We introduce the Generalized Energy Based Model (GEBM) for generative mod-
elling. These models combine two trained components: a base distribution (generally
an implicit model), which can learn the support of data with low intrinsic dimension in
a high dimensional space; and an energy function, to refine the probability mass on the
learned support. Both the energy function and base jointly constitute the final model,
unlike GANs, which retain only the base distribution (the "generator"). GEBMs are
trained by alternating between learning the energy and the base. We show that both
training stages are well-defined: the energy is learned by maximising a generalized
likelihood, and the resulting energy-based loss provides informative gradients for
learning the base. Samples from the posterior on the latent space of the trained model
can be obtained via MCMC, thus finding regions in this space that produce better qual-
ity samples. Empirically, the GEBM samples on image-generation tasks are of much
better quality than those from the learned generator alone, indicating that all else
being equal, the GEBM will outperform a GAN of the same complexity. When using
normalizing flows as base measures, GEBMs succeed on density modelling tasks, re-
turning comparable performance to direct maximum likelihood of the same networks.
1	Introduction
Energy-based models (EBMs) have a long history in physics, statistics and machine learning (LeCun
et al., 2006). They belong to the class of explicit models, and can be described by a family of energies
E which define probability distributions with density proportional to exp(-E). Those models are
often known up to a normalizing constant Z(E), also called the partition function. The learning task
consists of finding an optimal function that best describes a given system or target distribution P.
This can be achieved using maximum likelihood estimation (MLE), however the intractability of
the normalizing partition function makes this learning task challenging. Thus, various methods have
been proposed to address this (Hinton, 2002; Hyvarinen, 2005; Gutmann and Hyvarinen, 2012; Dai
et al., 2019a;b). All these methods estimate EBMs that are supported over the whole space. In many
applications, however, Pis believed to be supported on an unknown lower dimensional manifold. This
happens in particular when there are strong dependencies between variables in the data (Thiry et al.,
2021), and suggests incorporating a low-dimensionality hypothesis in the model .
Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) are a particular way to enforce low
dimensional structure in a model. They rely on an implicit model, the generator, to produce samples
supported on a low-dimensional manifold by mapping a pre-defined latent noise to the sample space
using a trained function. GANs have been very successful in generating high-quality samples on
various tasks, especially for unsupervised image generation (Brock et al., 2018). The generator is
trained adversarially against a discriminator network whose goal is to distinguish samples produced by
the generator from the target data. This has inspired further research to extend the training procedure to
more general losses (Nowozin et al., 2016; Arjovsky et al., 2017; Li et al., 2017; BinkOWSki et al., 2018;
Arbel et al., 2018) and to improve its stability (Miyato et al., 2018; Gulrajani et al., 2017; Nagarajan
and Kolter, 2017; Kodali et al., 2017). While the generator ofaGANhas effectively a low-dimensional
support, it remains challenging to refine the distribution of mass on that support using pre-defined
latent noise. For instance, as shown by Cornish et al. (2020) for normalizing flows, when the latent
distribution is unimodal and the target distribution possesses multiple disconnected low-dimensional
components, the generator, as a continuous map, compensates for this mismatch using steeper slopes.
In practice, this implies the need for more complicated generators.
* Correspondence: michael.n.arbel@gmail.com.
1
Published as a conference paper at ICLR 2021
In the present work, we propose a new class of models, called Generalized Energy Based Models
(GEBMs), which can represent distributions supported on low-dimensional manifolds, while offering
more flexibility in refining the mass on those manifolds. GEBMs combine the strength of both implicit
and explicit models in two separate components: a base distribution (often chosen to be an implicit
model) which learns the low-dimensional support of the data, and an energy function that can refine
the probability mass on that learned support. We propose to train the GEBM by alternating between
learning the energy and the base, analogous to f -GAN training (Goodfellow et al., 2014; Nowozin
et al., 2016). The energy is learned by maximizing a generalized notion of likelihood which we relate to
the Donsker-Varadhan lower-bound (Donsker and Varadhan, 1975) and Fenchel duality, as in (Nguyen
et al., 2010; Nowozin et al., 2016). Although the partition function is intractable in general, we propose
a method to learn it in an amortized fashion without introducing additional surrogate models, as done
in variational inference (Kingma and Welling, 2014; Rezende et al., 2014) orby Dai et al. (2019a;b).
The resulting maximum likelihood estimate, the KL Approximate Lower-bound Estimate (KALE),
is then used as a loss for training the base. When the class of energies is rich and smooth enough, we
show that KALE leads to a meaningful criterion for measuring weak convergence of probabilities.
Following recent work by Chu et al. (2020); Sanjabi et al. (2018), we show that KALE possesses well
defined gradients w.r.t. the parameters of the base, ensuring well-behaved training. We also provide
convergence rates for the empirical estimator of KALE when the variational family is sufficiently
well behaved, which may be of independent interest.
The main advantage of GEBMs becomes clear when sampling from these models: the posterior over
the latents of the base distribution incorporates the learned energy, putting greater mass on regions
in this latent space that lead to better quality samples. Sampling from the GEBM can thus be achieved
by first sampling from the posterior distribution of the latents via MCMC in the low-dimensional
latent space, then mapping those latents to the input space using the implicit map of the base. This is in
contrast to standard GANs, where the latents of the base have a fixed distribution. We focus on a class
of samplers that exploit gradient information, and show that these samplers enjoy fast convergence
properties by leveraging the recent work of Eberle et al. (2017). While there has been recent interest
in using the discriminator to improve the quality of the generator during sampling (Azadi et al., 2019;
Turner et al., 2019; Neklyudov et al., 2019; Grover et al., 2019; Tanaka, 2019; Wu et al., 2019b), our
approach emerges naturally from the model we consider.
We begin in Section 2 by introducing the GEBM model. In Section 3, we describe the learning
procedure using KALE, then derive a method for sampling from the learned model in Section 4. In
Section 5 we discuss related work. Finally, experimental results are presented in Section 6 with code
available at https://github.com/MichaelArbel/GeneralizedEBM.
2	Generalized Energy-Based Models
Figure 1: Data generating distribution supported on a line and with higher density at the extremities.
Models are learned using either a GAN, GEBM, or EBM. More details are provided in Appendix G.3.
In this section, we introduce generalized energy based models (GEBM), that combine the strengths of
both energy-based models and implicit generative models, and admit the first of these as a special case.
An energy-based model (EBM) is defined by a setE of real valued functions called energies, where
each E∈E specifies a probability density over the data space X ⊂Rd up to a normalizing constant,
Q(dx) = exp(-E (x) - A)dx,
A=log	exp(-E(x))dx .
(1)
While EBMs have been shown recently to be powerful models for representing complex high dimen-
sional data distributions, they still unavoidably lead to a blurred model whenever data are concentrated
on a lower-dimensional manifold. This is the case in Figure 1(a), where the ground truth distribution is
2
Published as a conference paper at ICLR 2021
supported on a 1-D line and embedded in a 2-D space. The EBM in Figure 1(d) learns to give higher den-
sity to a halo surrounding the data, and thus provides a blurred representation. That is a consequence of
EBM having a density defined over the whole space, and can result in blurred samples for image models.
An implicit generative model (IGM) is a family of probability distributions Gθ parametrized by a
learnable generator function G : Z 7→ X that maps latent samples z from a fixed latent distribution
η to the data space X. The latent distribution η is required to have a density over the latent space Z
and is often easy to sample from. Thus, Sampling from G is simply achieved by first sampling z from
η then applying G,
X 〜G Q⇒	X = G(z), Z 〜η.
(2)
GANs are popular instances of these models, and are trained adversarially (Goodfellow et al., 2014).
When the latent space Z has a smaller dimension than the input space X, the IGM will be supported
on a lower dimensional manifold of X, and thus will not possess a Lebesgue density on X (Bottou
et al., 2017). IGMs are therefore good candidates for modelling low dimensional distributions. While
GANs can accurately learn the low-dimensional support of the data, they can have limited power for
representing the distribution of mass on the support. This is illustrated in Figure 1(b).
A generalized energy-based model (GEBM) Q is defined by a combination of a base G and an
energy E defined over a subset X ofRd. The base component can typically be chosen to be an IGM
as in (2). The generalized energy component can refine the mass on the support defined by the base.
It belongs to a class E of real valued functions defined on the input space X, and represents the negative
log-density ofa sample from the GEBM with respect to the base G,
Q(dX) = exp(-E (X) - AG,E)G(dX),	AG,E = log	exp(-E(X))G(dX) ,	(3)
where AG,E is the logarithm of the normalizing constant of the model w.r.t. G. Thus, a GEBM Q
re-weights samples from the base according to the un-normalized importance weights exp(-E(X)).
Using the latent structure of the base G, this importance weight can be pulled-back to the latent space
to define a posterior latent distribution ν,
V(Z) := η(z)exp(-E(G(Z))-Ag,e).
(4)
Hence, the posterior latent ν can be used instead of the latent noise η for sampling from Q, as
summarized by Proposition 1:
Proposition 1. Sampling from Q requires sampling a latent Z from ν (4) then applying the map G,
X 〜Q Q⇒ X = G(z), z 〜V.
(5)
In order to hold, Proposition 1 does not need the generator G to be invertible. We provide a proof in
Appendix C.1 which relies on a characterization of probability distribution using generalized moments.
We will see later in Section 4 how equation (5) can be used to provide practical sampling algorithms
from the GEBM. Next we discuss the advantages of GEBMs.
Advantages of Generalized Energy Based Models. The GEBM defined by (3) can be related to
exponential tilting (re-weighting) (Siegmund, 1976; Xie et al., 2016) of the base G. The important
difference over classical EBMs is that the base G is allowed to change its support and shape in space.
By learning the base G, GEBMs can accurately learn the low-dimensional support of data, just like
IGMs do. They also benefit from the flexibility of EBMs for representing densities using an energy E
to refine distribution of mass on the support defined by G, as seen in Figure 1(c).
Compared to EBMs, that put mass on the whole space by construction (positive density), GEBMs
have the additional flexibility to concentrate the probability mass on a low-dimensional support learned
by the base G, provided that the dimension of the latent space Z is smaller than the dimension of the
ambient space X: see Figure 1(c) vs Figure 1(d). In the particular case when the dimension of Z is
equal to the ambient dimension and Gis invertible, the base G becomes supported over the whole space
X, and GEBM recover usual EBMs. The next proposition further shows that any EBM can be viewed
as a particular cases of GEBMs, as proved in Appendix C.1.
Proposition 2. Any EBM with energy E (as in (1)) can be expressed as a GEBM with base Ggivenasa
normalizing flow with density eXp(-r (X)) and a generalized energy E (X) = E(X) -r(X). In this partic-
ular case, the dimension of the latent is necessarily equal to the data dimension, i.e. dim(Z) = dim(X).
3
Published as a conference paper at ICLR 2021
Compared to IGMs, that rely on a fixed pre-determined latent noise distribution η, GEBMs offer the
additional flexibility of learning a richer latent noise distribution. This is particularly useful when the
data is multimodal. In IGMs, such a GANs, the latent noise η is usually unimodal thus requiring a
more sophisticated generator to distort a unimodal noise distribution into a distribution with multiple
modes, as shown by Cornish et al. (2020). Instead, GEBMs allow to sample from a posterior ν over the
latent noise defined in (4). This posterior noise can be multimodal in latent space (by incorporating
information from the energy) and thus can put more or less mass in specific regions of the manifold
defined by the base G. This allows GEBMs to capture multimodality in data, provided the support of
the base is broad enough to subsume the data support Figure 1(c). The base can be simpler, compared
to GANs, as it doesn’t need to distort the input noise too much to produce multimodal samples (see
Figure 8 in Appendix G.4). This additional flexibility comes atno additional training cost compared to
GANs. Indeed, GANs still require another model during training, the discriminator network, but do not
use it for sampling. Instead, GEBMs avoid this waist since the base and energy can be trained jointly,
with no other additional model, and then both are used for sampling.
3 Learning GEBMs
In this section we describe a general procedure for learning GEBMs. We decompose the learning
procedure into two steps: an energy learning step and a base learning step. The overall learning
procedure alternates between these two steps, as done in GAN training (Goodfellow et al., 2014).
3.1	Energy learning
When the base G is fixed, varying the energy E leads to a family of models that all admit a density
exp(-E - AG,E) w.r.t. G. When the base G admits a density exp(-r) defined over the whole space,
itis possible to learn the energy E by maximizing the likelihood of the model - (E+r)dP-AG,E.
However, in general G is supported on a lower-dimensional manifold so that r is ill-defined and the
usual notion of likelihood cannot be used. Instead, we introduce a generalized notion of likelihood
which does not require a well defined density exp(-r) for G:
Definition 1 (Generalized Likelihood). The expected G-log-likelihood under a target distribution
P ofa GEBM model Q with base G and energy E is defined as
LP,G(E):
E(x)dP(x)-AG,E.
(6)
To provide intuitions about the generalized likelihood in Definition 1, we start by discussing the
particular case where KL(P||G) < +∞. We then present the training method in the general case
where P and G might not share the same support, i.e. KL(P||G) =+∞.
Special case of finite KL(P||G). When the Kullback-Leibler divergence between P and G is well
defined, (6) corresponds to the Donsker-Varadhan (DV) lower bound on the KL (Donsker and Varadhan,
1975), meaning that KL(P||G) ≥ LP,G (E) for all E. Moreover, the following proposition holds:
Proposition 3. Assume that KL(P||G) < +∞ and0 ∈E. If, in addition, E? maximizes (6), then:
KL(P∣∣Q) ≤KL(P∣∣G).	(7)
In addition, we have that KL(P||Q) =0 when E? is the negative log-density ratio ofP w.r.t. G.
We refer to Appendix C.1 for a proof. According to (7), the GEBM systematically improves over the
IGM defined by G, with no further improvement possible in the limit case whenG=P. Hence as long
as there is an error in mass on the common support of P and G, the GEBM improves over the base G.
Estimating the likelihood in the General setting. Definition 1 can be used to learn a maximum
likelihood energy E? by maximizing LP,G (E) w.r.t. E even when the KL(P||G) is infinite and when P
andG don’t necessarily share the same support. Such an optimal solution is well defined whenever the
set of energies is suitably constrained. This is the case if the energies are parametrized by a compact
set Ψ with ψ7→Eψ continuous over Ψ. Estimating the likelihood is then achieved using i.i.d. samples
(Xn)1:N,(Ym)1:M from P andG (Tsuboi et al., 2009; Sugiyama et al., 2012; Liu et al., 2017):
1N	1M
LP,G(E) = - NEE(Xn)-Iog (M EeXP(-E(Ym))J .	⑻
4
Published as a conference paper at ICLR 2021
In the context of mini-batch stochastic gradient methods, however, M typically ranges from 10to 1000,
which can lead to a poor estimate for the log-partition function AG,E . Moreover, (8) doesn’t exploit
estimates of AG,E from previous gradient iterations. Instead, we propose an estimator which introduces
a variational parameter A ∈ R meant to estimate AG,E in an amortized fashion. The key idea is to
exploit the convexity of the exponential which directly implies -AG,E ≥ -A-exp(-A+AG,E)+1
for any A ∈R, with equality only when A=AG,E. Therefore, (6) admits a lower-bound of the form
LP,G
(E)≥- (E+A)dP-
exp(-(E + A))dG + 1 := FP,G(E+A),
where we introduced the functional FP,G for concision. Maximizing FP,G(E+A) over A recovers
the likelihood LP,G(E). Moreover, jointly maximizing over E and A yields the maximum likelihood
energy E? and its corresponding log-partition function A? =AG,E?. This optimization is well-suited
for stochastic gradient methods using the following estimator Kanamori et al. (2011):
1N	1M
FP,g (E+A)=-N E(E(Xn)+A) - M EeXP(TE (Ym)+A))+1.	⑼
n=1	m=1
3.2	Base learning
Unlike in Section 3.1, varying the base G does not need to preserve the same support. Thus, it is
generally not possible to use maximum likelihood methods for learning G. Instead, we propose to
use the generalized likelihood (6) evaluated at the optimal energy E? as a meaningful loss for learning
G, and refer to itas the KL Approximate Lower-bound Estimate (KALE),
KALE(P||G) =	sup	FP,G(E+A).	(10)
(E,A)∈E×R
From Section 3.1, KALE(P||G) is always a lower bound on KL(P,G). The bound becomes tight
whenever the negative log density of P w.r.t. G is well-defined and belongs to E (Appendix A).
Moreover, Proposition 4 shows that KALE is a reliable criterion for measuring convergence, and is
a consequence of (Zhang et al., 2017, Theorem B.1), with a proof in Appendix C.2.1:
Proposition 4. Assume all energies in E are L-Lipschitz and that any continuous function can be
well approximated by linear combinations of energies in E (Assumptions (A) and (B) of Appendix C.2),
then KALE(P||G) ≥0 with equality only ifP=G andKALE(P||Gn) →0 iff Gn →P in distribution.
The universal approximation assumption holds in particular when E contains feedforward networks.
In fact networks with a single neuron are enough, as shown in (Zhang et al., 2017, Theorem 2.3). The
Lipschitz assumption holds when additional regularization of the energy is enforced during training
by methods such as spectral normalization (Miyato et al., 2018) or additional regularization I(ψ)
on the energy Eψ such as the gradient penalty (Gulrajani et al., 2017) as done in Section 6.
Estimating KALE. According to Arora et al. (2017), accurate finite sample estimates of divergences
that result from an optimization procedures (such as in (10)) depend on the richness of the class E; and
richer energy classes can result in slower convergence. Unlike divergences such as Jensen-Shannon,
KL and the Wasserstein distance, which result from optimizing over a non-parametric and rich class of
functions, KALE is restricted to a class of parametric energies Eψ. Thus, (Arora et al., 2017, Theorem
3.1) applies, and guarantees good finite sample estimates, provided optimization is solved accurately.
In Appendix B, we provide an analysis for the more general case where energies are not necessarily
parametric but satisfy some further smoothness properties; we emphasize that our rates do not require
the strong assumption that the density ratio is bounded above and below as in (Nguyen et al., 2010).
Smoothness of KALE. Learning the base is achieved by minimizing K(θ) :=KALE(P∣∣Gθ) over the
set of parameters Θ of the generator Gθ using first order methods (Duchi et al., 2011; Kingma and Ba,
2014; Arbel et al., 2019). This requires K(θ) tobe smooth enough so that gradient methods converge to
local minima and avoid instabilities during training (Chu et al., 2020). Ensuring smoothness of losses
that result from an optimization procedure, as in (10), can be challenging. Results for the regularized
Wasserstein are provided by Sanjabi et al. (2018), while more general losses are considered by Chu
et al. (2020), albeit under stronger conditions than for our setting.Theorem 5 shows that when E, Gθ
and their gradients are all Lipschitz then K(θ) is smooth enough.We provide a proof for Theorem 5 in
Appendix C.2.1.
5
Published as a conference paper at ICLR 2021
Theorem 5. Under Assumptions (I) to (III) of Appendix C.2, sub-gradient methods on K converge
to local optima. Moreover, K is Lipschitz and differentiable for almost all θ ∈ Θ with:
VK(θ)=exp(-AGθ,E?)/VxE ?(G© (z))Vθ Gθ (z)exp(-E*(Gθ (z)))η(z)dz.	(11)
Estimating the gradient in (11) is
achieved by first optimizing over Eψ
and A using (9), with additional regular-
ization I(ψ). The resulting estimators
E? and A? are plugged in (12) to esti-
mate VK(θ) using samples (Zm)LM
from η . Unlike for learning the en-
ergy E?, which benefits from using the
amortized estimator of the log-partition
function, we found that using the em-
pirical log-partition for learning the
base was more stable. We summarize
the training procedure in Algorithm 1,
which alternates between learning the
energy and the base in a similar fashion
to adversarial training.
Algorithm 1 Training GEBM
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
Input P, N,M, nb, ne
Output Trained generator Gθ and energy Eψ.
Initialize θ , ψ and A.
for k= 1,...,nb do
forj= 1,...,ne do
Sample {Xn} 1：N ~ P and {Yn} 1：N ~ Gθ
gψ4---vψFP,Gθ (Eψ + A)+ I(ψ)
A—log( MM Pm=IeXp(-Eψ (Ym)))
gA — exp(A - A) - 1
Update ψ andA using gψ andgA.
end for
Set E? — Eψ and A? — A.
一 一 . . 一
Update θ using VK(θ) from (12)
end for
-----~7
VK(θ) =
exp(-A?)	ʌ	ʌ
PM ) EVxE*(Gθ(Zm))VθGθ(Zm)exp(-E?(G©(Zm))).
m=1
(12)
4	Sampling from GEBMs
A simple estimate of the empirical distribution of observations under the GEBM is via importance sam-
pling (IS). This consists in first sampling multiple points from the base G, and then re-weighting the sam-
ples according to the energy E . Although straightforward, this approach can lead to highly unreliable es-
timates, a well known problem in the Sequential Monte Carlo (SMC) literature which employs IS exten-
sively (Doucet et al., 2001; Del Moral et al., 2006). Other methods such as rejection sampling are known
to be inefficient in high dimensions Haugh (2017). Instead, we propose to sample from the posterior ν us-
ing MCMC. Recall from (5) that a sample x from Q is oftheformx=G(z) with z sampled from the pos-
terior latent ν of (4) instead of the prior η. While sampling from η is often straightforward (for instance if
ηis a Gaussian), sampling from ν is generally harder, due to dependence of its density on complex func-
tions E and G. Itis still possible to use MCMC methods to sample from ν, however, since we have access
to its density up to a normalizing constant (4). In particular, we are interested in methods that exploit
the gradientofν, and consider two classes of samplers: Overdamped samplers and Kinetic samplers.
Overdamped samplers are obtained as a time-discretization of the Overdamped Langevin dynamics:
dzt = (Vzlogn(Zt)-Vz E(G(zt))) + √2dwt,	(13)
where wt is a standard Brownian motion. The simplest sampler arising from (13) is the Unadjusted
Langevin Algorithm (ULA):
Zk+1=Zk+λ(Vz ιogn(Zk )-Vz e (G(Zk)))+√2λWk+ι,	Zο 〜n,
where (Wk)k≥0 are i.i.d. standard Gaussians and λ is the step-size. For large k, Zk is an approximate
sample from ν (Raginsky et al., 2017, Proposition 3.3). Hence, setting X=G(Zk) for a large enough
k provides an approximate sample from the GEBM Q, as summarized in Algorithm 2 of Appendix F.
Kinetic samplers arise from the Kinetic Langevin dynamics which introduce a momentum variable:
dzt = vtdt,	dvt = - Yvtdt+u(Vlogn( Zt )-VE(G(zt)))dt+ P2γudwt.	(14)
with friction coefficient γ ≥ 0, inverse mass u≥0, momentum vector vt and standard Brownian motion
wt. When the mass u-1 becomes negligible compared to the friction coefficient γ, i.e. uγ-2 ≈ 0,
standard results show that (14) recovers the Overdamped dynamics (13). Discretization in time of (14)
6
Published as a conference paper at ICLR 2021
leads to Kinetic samplers similar to Hamiltonian Monte Carlo (Cheng et al., 2017; Sachs et al., 2017).
We consider a particular algorithm from Sachs et al. (2017) which we call Kinetic Langevin Algorithm
(KLA) (see Algorithm 3 in Appendix F). Kinetic samplers were shown to better explore the modes of
the invariant distribution ν compared to Overdamped ones (see (Neal, 2010; Betancourt et al., 2017)
for empirical results and (Cheng et al., 2017) for theory), as also confirmed empirically in Appendix D
for image generation tasks using GEBMs. Next, we provide the following convergence result:
Proposition 6. Assume that logη(z) is strongly concave and has a Lipschitz gradient, that E, G and
their gradients are all L-Lipschitz. Set xt = G(zt), where zt is given by (14) and call Pt the probability
distribution ofxt. Then Pt converges to Q in the Wasserstein sense,
W2(Pt,Q)≤LCe-cγt,
where candC are positive constants independent of t, with c = O(exp(-dim(Z))).
Proposition 6 is proved in Appendix C.1 using (Eberle et al., 2017, Corollary 2.6), and implies that
(xt)t≥0 converges at the same speed as (zt)t≥0. When the dimension q of Z is orders of magnitude
smaller than the input space dimension d, the process (xt)t≥0 converges faster than typical sampling
methods on X, for which the exponent controlling the convergence rate is of order O(exp(-d)).
5	Related work
Energy based models. Usually, energy based models are required to have a density w.r.t. to a
Lebesgue measure, and do not use a learnable base measure; in other words, models are supported on
the whole space. Various methods have been proposed in the literature to learn EBMs. Contrastive
Divergence (Hinton, 2002) approximates the gradient of the log-likelihood by sampling from the
energy model with MCMC. More recently, (Belanger and McCallum, 2016; Xie et al., 2016; 2017;
2018c; 2019; Tu and Gimpel, 2018; Du and Mordatch, 2019; Deng et al., 2020) extend the idea using
more sophisticated models and MCMC sampling strategies that lead to higher quality estimators.
Score Matching (Hyvarinen, 2005) calculates an alternative objective (the score) to the log-likelihood
which is independent of the partition function, and was recently used in the context non-parametric
energy functions to provide estimators of the energy that are provably consistent (Sriperumbudur et al.,
2017; Sutherland et al., 2018; Arbel and Gretton, 2018; Wenliang et al., 2019). In Noise-Contrastive
Estimation (Gutmann and Hyvarinen, 2012), a classifier is trained to distinguish between samples
from a fixed proposal distribution and the target P. This provides an estimate for the density ratio
between the optimal energy model and the proposal distribution. In a similar spirit, Cranmer et al.
(2016) uses a classifier to learn likelihood ratios. Conversely, Grathwohl et al. (2020) interprets
the logits of a classifier as an energy model obtained after marginalization over the classes. The
resulting model is then trained using Contrastive Divergence. In more recent work, Dai et al. (2019a;b)
exploit a dual formulation of the logarithm of the partition function as a supremum over the set of
all probability distributions of some functional objective. Yu et al. (2020) explore methods for using
general f-divergences, such as Jensen-Shannon, to train EBMs.
Generative Adversarial Networks. Recent work proposes using the discriminator of a trained
GAN to improve the generator quality. Rejection sampling (Azadi et al., 2019) and Metropolis-
Hastings correction (Turner et al., 2019; Neklyudov et al., 2019) perform sampling directly on the
high-dimensional input space without using gradient information provided by the discriminator.
Moreover, the data distribution is assumed to admit a density w.r.t. the generator. Ding et al. (2019)
perform sampling on the feature space of some auxiliary pre-trained network; while Lawson et al.
(2019) treat the sampling procedure as a model on its own, learned by maximizing the ELBO. In our
case, no auxiliary model is needed. In the present work, sampling doesn’t interfere with training, in
contrast to recently considered methods to optimize over the latent space during training Wu et al.
(2019b;a). In Tanaka (2019), the discriminator is viewed as an optimal transport map between the
generator and the data distribution and is used to compute optimized samples from latent space. This is
in contrast to the diffusion-based sampling that we consider. In (Xie et al., 2018b;a), two independent
models, a full support EBM and a generator network, are trained cooperatively using MCMC. By
contrast, in the present work, the energy and base are part of the same model, and the model support
is lower-dimensional than the target space X. While we do not address the mode collapse problem,
Xu et al. (2018); Nguyen et al. (2017) showed that KL-based losses are resilient to it thanks to the
zero-avoiding property of the KL, a good sign for KALE which is derived from KL by Fenchel duality.
7
Published as a conference paper at ICLR 2021
The closest related approach appears in a study concurrent to the present work (Che et al., 2020),
where the authors propose to use Langevin dynamics on the latent space ofa GAN generator, but with
a different discriminator to ours (derived from the Jensen-Shannon divergence or a Wasserstein-based
divergence). Our theory results showing the existence of the loss gradient (Theorem 5), establishing
weak convergence of distributions under KALE (Proposition 4), and demonstrating consistency of
the KALE estimator (Appendix B) should transfer to the JS and Wasserstein criteria used in that
work. Subsequent to the present work, an alternative approach has been recently proposed, based
on normalising flows, to learn both the low-dimensional support of the data and the density on this
support (Brehmer and Cranmer, 2020). This approach maximises the explicit likelihood of a data
projection onto a learned manifold, and may be considered complementary to our approach.
6	Experiments
Figure 2: Samples at different
iterations of the MCMC chain of
Algorithm 3 (left to right).
6.1 Image generation.
Experimental setting. We train a GEBM on unsupervised image
generation tasks, and compare the quality of generated samples with
other methods using the FID score (Heusel et al., 2017) computed on
5 × 104 generated samples. We consider CIFAR-10 (Krizhevsky,
2009), LSUN (Yu et al., 2015), CelebA (Liu et al., 2015) and ImageNet
(Russakovsky et al., 2014) all downsampled to 32x32 resolution to
reduce computational cost. We consider two network architectures for
each of the base and energy, a smaller one (SNGAN ConvNet) and
a larger one (SNGAN ResNet), both of which are from Miyato et al.
(2018). For the base we used the SNGAN generator networks from
Miyato et al. (2018) with a 100-dimensional Gaussian for the latent
noise η. For the energy we used the SNGAN discriminator networks
from Miyato et al. (2018). (Details of the networks in Appendix G.1).
We train the models for 150000 generator iterations using Algorithm 1. After training is completed,
we rescale the energy by β= 100 to get a colder version of the GEBM and sample from it using either
Algorithm 2 (ULA) or Algorithm 3 (KLA) with parameters (γ= 100,u = 1). This colder temperature
leads to an improved FID score, and needs relatively few MCMC iterations, as shown in Figure 6 of
Appendix D. Sampler convergence to visually plausible modes at low tempteratures is demonstrated in
Figure 2. We perform 1000 MCMC iterations with initial step-size of λ= 10-4 decreased by 10 every
200 iterations. As a baseline we consider samples generated from the base of the GEBM only (without
using information from the energy) and call this KALE-GAN. More details are given in Appendix G.
Results: Table 1 shows that GEBM outperforms both KALE and standard GANs when using the same
networks for the base/generator and energy/critic. Moreover, KALE-GAN matches the performance
ofa standard GAN (with Jensen-Shannon critic), showing that the improvement of GEBM cannot be
explained by the switch from Jensen-Shannon to a KALE-based critic. Rather, the improvement is
largely due to incorporating the energy function into the model, and sampling using Algorithm 3.
This finding experimentally validates our claim that incorporating the energy improves the model, and
that all else being equal, a GEBM outperforms a GAN with the same generator and critic architecture.
Indeed, if the critic is not zero at convergence, then by definition it contains information on the
remaining mismatch between the generator (base) and data mass, which the GEBM incorporates, but
the GAN does not. The GEBM also outperforms an EBM even when the latter was trained using a
larger network (ResNet) with supervision (S) on ImageNet, which is an easier task ( Chen et al. (2019)).
More comparisons on Cifar10 and ImageNet are provided in Table 4 of Appendix D.
	SNGAN (ConVNet)	∣			SNGAN (ReSNet)			
	GEBM	KALE-GAN	GAN I	GEBM	KALE-GAN	GAN	EBM
Cifar10	23.02	32.03	29.9	19.31	20.19	21.7	38.2
ImageNet	13.94	19.37	20.66	20.33	21.00	20.50	14.31 (S)
Table 1: FID scores for two versions of SNGAN from (Miyato et al., 2018) on Cifar10 and ImageNet. GEBM:
training using Algorithm 1 and sampling using Algorithm 3. KALE-GAN: Only the base ofa GEBM is retained
for sampling. GAN: training as in (Miyato et al., 2018) with q= 128 for the latent dimension as it worked best.
EBM: results from Du and Mordatch (2019) with supervised training on ImageNet (S).
8
Published as a conference paper at ICLR 2021
Table 2 shows different sampling meth-
ods using the same trained networks
(generator and critic), with KALE-GAN
as a baseline. All energy-exploiting
methods outperform the unmodified
KALE-GAN with the same architecture.
That said, our method (both ULA and
KLA) outperforms both (IHM) (Turner
et al., 2019) and (DOT) (Tanaka, 2019),
which both use the energy information.
	Cifar10	LSUN	CelebA	ImageNet
KALE-GAN	32.03	21.67	6.91	19.37
IHM	30.47	20.63	6.39	18.15
DOT	26.35	20.41	5.93	16.21
GEBM (ULA)	23.02	16.23	5.21	14.00
GEBM (KLA)	24.29	15.25	5.38	13.94
Table 2: FID scores for different sampling methods using the same
trained SNGAN (ConvNet): KALE-GAN as a baseline w/o critic
information.
In Table 2, KLA was used in the high friction regime γ = 100 and thus behaves like ULA. This allows to
obtain sharper samples concentrated around the modes of the GEBM thus improving the FID score. If,
instead, the goal is to encourage more exploration of the modes of the GEBM, then KLA with a smaller
γ is a better alternative than ULA, as the former can explore multiple modes/images within the same
MCMC chain, unlike (ULA): see Figures 3 to 5 of Appendix D. Moving from one mode to another
results in an increased FID score while between modes, however, which can be avoided by decreasing λ.
6.2 Density Estimation
Motivation. We next consider the particular setting where the likelihood of the model is well-defined,
and admits a closed form expression. This is intended principally as a sanity check that our proposed
training method in Algorithm 1 succeeds in learning maximum likelihood solutions. Outside of this
setting, closed form expressions of the normalizing constant are not available for generic GEBMs.
While this is not an issue (since the proposed method doesn’t require a closed form expression for the
normalizing constant), in this experiment only, we want to have access to closed form expressions, as
they enable a direct comparison with other density estimation methods.
Experimental setting. To have a closed-form likelihood, we consider the case where the dimension of
the latent space is equal to data-dimension, and choose the base G of the GEBM to be a Real NVP (Ding
et al. (2019) ) with density exp(-r(x)) and energy E(x) = h(x) -r(x). Thus, in this particular case,
the GEBM has a well defined likelihood over the whole space, and we are precisely in the setting of
Proposition 2, which shows that this GEBM is equal to an EBM with density proportional to exp(-h).
We further require the EBM to be a second Real NVP so that its density has a closed form expression.
We consider 5 UCI datasets (Dheeru and Taniskidou, 2017) for which we use the same pre-processing as
in (Wenliang et al., 2019). For comparison, we train the EBM by direct maximum likelihood (ML) and
contrastive divergence (CD). To train the GEBM, we use Algorithm 1, which doesn’t directly exploit
the closed-form expression of the likelihood (unlike direct ML). We thus use either (8) (KALE-DV) or
(9) (KALE-F) to estimate the normalizing constant. More details are given in Appendix G.2.
Results. Table 3 reports the Negative Log-Likelihood (NLL) evaluated on the test set and corresponding
to the best performance on the validation set. Training the GEBM using Algorithm 1 leads to comparable
performance to (CD) and (ML). As shown in Figure 7 of Appendix E, (KALE-DV) and (KALE-
F) maintain a small error gap between the training and test NLL and, as discussed in Section 3.1
and Appendix F, (KALE-F) leads to more accurate estimates of the log-partition function, with a
relative error of order 0.1% compared to 10% for (KALE-DV).
	RedWine d =11,N 〜103	Whitewine d =11,N 〜103	Parkinsons d =15,N 〜103	Hepmass d = 22,N 〜105	Miniboone d = 43,N 〜104
NVPwML	11.98	13.05	14.5	24.89	42.28
NVPwCD	11.88	13.01	14.06	22.89	39.36
NVP w KALE (DV)	11.6	12.77	13.26	26.56	46.48
NVP w KALE (F)	11.19	12.66	13.26	24.66	38.35
Table 3: UCI datasets: Negative log-likelihood computed on the test set and corresponding to the best
performance on the validation set. Best method in boldface.
7	Acknowledgments
We thank Mihaela Rosca for insightful discussions and Song Liu, Bo Dai and Hanjun Dai for pointing
us to important related work.
9
Published as a conference paper at ICLR 2021
References
Arbel, M. and Gretton, A. (2018). Kernel Conditional Exponential Family. In International Conference
OnArtificial Intelligence and Statistics, pages 1337-1346.
Arbel, M., Gretton, A., Li, W., and Montufar, G. (2019). Kernelized Wasserstein Natural Gradient.
Arbel, M., Sutherland, D., Binkowski, M., and Gretton, A. (2018). On gradient regularizers for mmd
gans. In Advances in Neural Information Processing Systems 31. Curran Associates, Inc.
Arjovsky, M., Chintala, S., and Bottou, L. (2017). Wasserstein generative adversarial networks. In
Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings
of Machine Learning Research, International Convention Centre, Sydney, Australia. PMLR.
Arora, S., Ge, R., Liang, Y., Ma, T., and Zhang, Y. (2017). Generalization and equilibrium in generative
adversarial nets (GANs). In Proceedings of the 34th International Conference on Machine Learning,
volume 70 of Proceedings of Machine Learning Research, pages 224-232. PMLR.
Azadi, S., Olsson, C., Darrell, T., Goodfellow, I., and Odena, A. (2019). Discriminator rejection
sampling. In International Conference on Learning Representations.
Belanger, D. and McCallum, A. (2016). Structured prediction energy networks. In International
Conference on Machine Learning, pages 983-992.
Betancourt, M., Byrne, S., Livingstone, S., and Girolami, M. (2017). The geometric foundations of
Hamiltonian Monte Carlo. Bernoulli, 23(4A):2257-2298.
BinkoWski, M., Sutherland, D.J., ArbeL M., and Gretton, A. (2018). Demystifying MMD GANs.
In International Conference on Learning Representations.
Bottou, L., Arjovsky, M., Lopez-Paz, D., and Oquab, M. (2017). Geometrical insights for implicit
generative modeling. In Braverman Readings in Machine Learning.
Brehmer, J. and Cranmer, K. (2020). FloWs for simultaneous manifold learning and density estimation.
arXiv preprint arXiv:2003.13913.
Brock, A., Donahue, J., and Simonyan, K. (2018). Large scale gan training for high fidelity natural
image synthesis. arXiv preprint arXiv:1809.11096.
Che, T., Zhang, R., Sohl-Dickstein, J., Larochelle, H., Paull, L., Cao, Y., and Bengio, Y. (2020). Your
GAN is secretly an energy-based model and you should use discriminator driven latent sampling.
Chen, T., Zhai, X., Ritter, M., Lucic, M., and Houlsby, N. (2019). Self-supervised gans via auxiliary
rotation loss. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pages 12154-12163.
Cheng, X., Chatterji, N. S., Bartlett, P. L., and Jordan, M. I. (2017). Underdamped langevin mcmc:
A non-asymptotic analysis. arXiv preprint arXiv:1707.03663.
Chu, C., Minami, K., and Fukumizu, K. (2020). Smoothness and stability in gans. In International
Conference on Learning Representations.
Cornish, R., Caterini, A. L., Deligiannidis, G., and Doucet, A. (2020). Relaxing bijectivity constraints
With continuously indexed normalising floWs.
Cranmer, K., Pavez, J., and Louppe, G. (2016). Approximating likelihood ratios With calibrated
discriminative classifiers.
Dai, B., Dai, H., Gretton, A., Song, L., Schuurmans, D., and He, N. (2019a). Kernel exponential family
estimation via doubly dual embedding. In Proceedings of Machine Learning Research, volume 89
of Proceedings of Machine Learning Research, pages 2321-2330. PMLR.
Dai, B., Liu, Z., Dai, H., He, N., Gretton, A., Song, L., and Schuurmans, D. (2019b). Exponential Family
Estimation via Adversarial Dynamics Embedding. arXiv:1904.12083 [cs, stat]. arXiv: 1904.12083.
10
Published as a conference paper at ICLR 2021
Davis, D. and Drusvyatskiy, D. (2018). Stochastic subgradient method converges at the rate
$O(kA{-1/4})$onweakIyconvexfUnctions. arXiv:1802.02988[cs, math].
Del Moral, P., Doucet, A., and Jasra, A. (2006). Sequential monte carlo samplers. Journal of the Royal
Statistical Society: Series B (Statistical Methodology), 68(3):411-436.
Deng, Y., Bakhtin, A., Ott, M., Szlam, A., and Ranzato, M. (2020). ResidUal energy-based models
for text generation. arXiv preprint arXiv:2004.11714.
DheerU, D. and TaniskidoU, E. K. (2017). Uci machine learning repository.
Ding, X., Wang, Z. J., and Welch, W. J. (2019). SUbsampling Generative Adversarial Networks:
Density Ratio Estimation in FeatUre Space with SoftplUs Loss.
Dinh, L., Sohl-Dickstein, J., and Bengio, S. (2016). Density estimation Using real nvp.
DonahUe, J. and Simonyan, K. (2019). Large Scale Adversarial Representation Learning.
arXiv:1907.02544 [cs, stat]. arXiv: 1907.02544.
Donsker, M. D. and Varadhan, S. R. S. (1975). Asymptotic evalUation of cer-
tain markov process expectations for large time, i. 28(1):1-47. _eprint:
https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpa.3160280102.
DoUcet, A., Freitas, N. d., and Gordon, N. (2001). Sequential Monte Carlo Methods in Practice.
Information Science and Statistics. Springer-Verlag, New York.
DU, Y. and Mordatch, I. (2019). Implicit generation and modeling with energy based models. In
Advances in Neural Information Processing Systems 32, pages 3608-3618. CUrran Associates, Inc.
DUchi, J., Hazan, E., and Singer, Y. (2011). Adaptive SUbgradient Methods for Online Learning and
Stochastic Optimization. Journal of Machine Learning Research, 12(JUl):2121-2159.
Eberle, A., GUillin, A., and Zimmer, R. (2017). CoUplings and qUantitative contraction rates for
Langevin dynamics. The Annals of Probability.
Ekeland, I. and T6mam, R. (1999). ConvexAnalysis and Variational Problems. Classics in Applied
Mathematics. Society for IndUstrial and Applied Mathematics.
Feydy, J., SejoUma T., Vialard, F.-X., Amari, S.-i., TroUva A., and Peyra G. (2019). Interpolating
between optimal transport and mmd Using sinkhorn divergences. In The 22nd International
Conference on Artificial Intelligence and Statistics, pages 2681-2690.
Goodfellow, I., PoUget-Abadie, J., Mirza, M., XU, B., Warde-Farley, D., Ozair, S., CoUrville, A., and
Bengio, Y. (2014). Generative adversarial nets. In Advances in Neural Information Processing
Systems 27, pages 2672-2680. CUrran Associates, Inc.
Grathwohl, W., Wang, K.-C., Jacobsen, J.-H., DUvenaUd, D., NoroUzi, M., and Swersky, K. (2020).
YoUr classifier is secretly an energy based model and yoU shoUld treat it like one.
Grover, A., Song, J., Kapoor, A., Tran, K., Agarwal, A., Horvitz, E. J., and Ermon, S. (2019). Bias
correction of learned generative models Using likelihood-free importance weighting. In Advances
in Neural Information Processing Systems 32. CUrran Associates, Inc.
GUlrajani, I., Ahmed, F., Arjovsky, M., DUmoUlin, V., and CoUrville, A. (2017). Improved training
of wasserstein gans. In Proceedings of the 31st International Conference on Neural Information
Processing Systems, Red Hook, NY, USA. CUrran Associates Inc.
Gutmann, M. U. and Hyvarinen, A. (2012). Noise-contrastive estimation of unnormalized statistical
models, with applications to natUral image statistics. The Journal of Machine Learning Research,
13(null):307-361.
Haugh, M. (2017). Mcmc and bayesian modeling. IEOR E4703 Monte-Carlo Simulation, Columbia
University.
11
Published as a conference paper at ICLR 2021
Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. (2017). Gans trained by a
two time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information
Processing Systems 30, pages 6626-6637. Curran Associates, Inc.
Hinton, G. E. (2002). Training products of experts by minimizing contrastive divergence. Neural
Computation, 14(8):1771-1800.
Ho, J. and Ermon, S. (2016). Generative adversarial imitation learning. In Advances in neural
information processing systems, pages 4565-4573.
Hyvarinen, A. (2005). Estimation of Non-Normalized Statistical Models by Score Matching. The
Journal of Machine Learning Research, 6:695-709.
Kanamori, T., Suzuki, T., and Sugiyama, M. (2011). f -divergence estimation and two-sample
homogeneity test under semiparametric density-ratio models. IEEE Transactions on Information
Theory, 58(2):708-720.
Kingma, D. P. and Ba, J. (2014). Adam: A Method for Stochastic Optimization. arXiv:1412.6980
[cs]. arXiv: 1412.6980.
Kingma, D. P. and Welling, M. (2014). Auto-encoding variational Bayes. ICLR.
Klenke, A. (2008). Probability Theory: A Comprehensive Course. World Publishing Corporation.
Kodali, N., Abernethy, J., Hays, J., and Kira, Z. (2017). On Convergence and Stability of GANs.
arXiv:1705.07215 [cs]. arXiv: 1705.07215.
Krizhevsky, A. (2009). Learning multiple layers of features from tiny images. Technical report,
University of Toronto.
Lawson, J., Tucker, G., Dai, B., and Ranganath, R. (2019). Energy-inspired models: Learning with
sampler-induced distributions. In Advances in Neural Information Processing Systems 32, pages
8501-8513. Curran Associates, Inc.
LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., and Huang, F.-J. (2006). Predicting Structured Data,
chapter A Tutorial on Energy-Based Learning. MIT Press.
Li, C.-L., Chang, W.-C., Cheng, Y., Yang, Y., and Poczos, B. (2017). Mmd gan: Towards deeper
understanding of moment matching network. In Advances in Neural Information Processing
Systems 30, pages 2203-2213. Curran Associates, Inc.
Liu, S., Bousquet, O., and Chaudhuri, K. (2017). Approximation and Convergence Properties of
Generative Adversarial Learning.
Liu, Z., Luo, P., Wang, X., and Tang, X. (2015). Deep learning face attributes in the wild.
Milgrom, P. and Segal, I. (2002). Envelope Theorems for Arbitrary Choice Sets. Econometrica, 70.
Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y. (2018). Spectral normalization for generative
adversarial networks. In International Conference on Learning Representations.
Nagarajan, V. and Kolter, J. Z. (2017). Gradient descent gan optimization is locally stable.
Neal, R. M. (2010). Mcmc using hamiltonian dynamics. Handbook of Markov Chain Monte Carlo.
Neklyudov, K., Egorov, E., and Vetrov, D. (2019). The implicit metropolis-hastings algorithm.
Nguyen, T., Le, T., Vu, H., and Phung, D. (2017). Dual discriminator generative adversarial nets. In
Advances in Neural Information Processing Systems, pages 2670-2680.
Nguyen, X., Wainwright, M. J., and Jordan, M. I. (2010). Estimating divergence functionals and
the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory,
56(11):5847-5861.
12
Published as a conference paper at ICLR 2021
Nowozin, S., Cseke, B., and Tomioka, R. (2016). f-gan: Training generative neural samplers using
variational divergence minimization. In Advances in Neural Information Processing Systems 29,
pages 271-279. Curran Associates, Inc.
Oord, A. v. d., Kalchbrenner, N., and Kavukcuoglu, K. (2016). Pixel recurrent neural networks. arXiv
preprint arXiv:1601.06759.
Ostrovski, G., Dabney, W., and Munos, R. (2018). Autoregressive quantile networks for generative
modeling. arXiv preprint arXiv:1806.05575.
Papamakarios, G., Pavlakou, T., and Murray, I. (2017). Masked autoregressive flow for density
estimation. NIPS.
Radford, A., Metz, L., and Chintala, S. (2015). Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434.
Raginsky, M., Rakhlin, A., and Telgarsky, M. (2017). Non-convex learning via stochastic gradient
langevin dynamics: a nonasymptotic analysis.
Retherford, J. R. (1978). Review: J. diestel and j. j. uhl, jr., vector measures. Bull. Amer. Math. Soc.,
84(4):681-685.
Rezende, D. J. and Mohamed, S. (2015). Variational inference with normalizing flows. In Proceedings
of the 32nd International Conference on International Conference on Machine Learning - Volume
37, ICML'15,pages 1530-1538.JMLR.org.
Rezende, D. J., Mohamed, S., and Wierstra, D. (2014). Stochastic backpropagation and approximate
inference in deep generative models. In ICML, pages 1278-1286.
Rockafellar, R. T. (1970). Convex analysis. Princeton Mathematical Series. Princeton University
Press, Princeton, N. J.
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla,
A., Bernstein, M., Berg, A. C., and Fei-Fei, L. (2014). ImageNet Large Scale Visual Recognition
Challenge. arXiv:1409.0575 [cs]. arXiv: 1409.0575.
Sachs, M., Leimkuhler, B., and Danos, V. (2017). Langevin Dynamics with Variable Coefficients
and Nonconservative Forces: From Stationary States to Numerical Methods. Entropy, 19.
Sanjabi, M., Ba, J., Razaviyayn, M., and Lee, J. D. (2018). On the convergence and robustness of
training gans with regularized optimal transport. In Advances in Neural Information Processing
Systems 3l, pages 7θ91-7101. Curran Associates, Inc.
Siegmund, D. (1976). Importance sampling in the monte carlo study of sequential tests. The Annals
OfStatistics, pages 673-684.
Simon-Gabriel, C.-J. and Scholkopf, B. (2018). Kernel distribution embeddings: Universal kernels,
characteristic kernels and kernel metrics on distributions. Journal of Machine Learning Research,
19(44):1-29.
Simsekli, U., Zhu, L., Teh, Y. W., and Gurbuzbalaban, M. (2020). Fractional Underdamped
Langevin Dynamics: Retargeting SGD with Momentum under Heavy-Tailed Gradient Noise.
arXiv:2002.05685 [cs, stat]. arXiv: 2002.05685.
Sriperumbudur, B., Fukumizu, K., Kumar, R., Gretton, A., and Hyvarinen, A. (2017). Density
estimation in infinite dimensional exponential families. Journal of Machine Learning Research.
Sugiyama, M., Suzuki, T., and Kanamori, T. (2012). Density ratio estimation in machine learning.
Cambridge University Press.
Sutherland, D., Strathmann, H., Arbel, M., and Gretton, A. (2018). Efficient and principled score
estimation with Nystrom kernel exponential families. In International Conference on Artificial
Intelligence and Statistics, pages 652-660.
13
Published as a conference paper at ICLR 2021
Tanaka, A. (2019). Discriminator optimal transport. In Advances in Neural Information Processing
Systems 32. Curran Associates, Inc.
Thekumparampil, K. K., Jain, P., Netrapalli, P., and Oh, S. (2019). Efficient algorithms for smooth
minimax optimization. In Advances in Neural Information Processing Systems 32, pages
12680-1269LCUrran Associates, Inc.
Thiry, L., Arbel, M., Belilovsky, E., and Oyallon, E. (2021). The unreasonable effectiveness of patches
in deep convolUtional kernels methods. In International Conference on Learning Representations.
TsUboi, Y., Kashima, H., Hido, S., Bickel, S., and SUgiyama, M. (2009). Direct density ratio estimation
for large-scale covariate shift adaptation. Journal of Information Processing, 17:138-155.
TU, L. and Gimpel, K. (2018). Learning approximate inference networks for strUctUred prediction.
arXiv preprint arXiv:1803.03376.
TUrner, R., HUng, J., Frank, E., Saatchi, Y., and Yosinski, J. (2019). Metropolis-Hastings generative
adversarial networks. In Proceedings of the 36th International Conference on Machine Learning,
volUme 97 of Proceedings of Machine Learning Research, pages 6345-6353, Long Beach,
California, USA. PMLR.
Villani, C. (2009). Optimal transport: Old and new. Technical report.
Wenliang, L., SUtherland, D., Strathmann, H., and Gretton, A. (2019). Learning deep kernels for
exponential family densities. In International Conference on Machine Learning, pages 6737-6746.
WU, Y., DonahUe, J., BaldUzzi, D., Simonyan, K., and Lillicrap, T. (2019a). LOGAN: Latent
Optimisation for Generative Adversarial Networks. arXiv:1912.00953 [cs, stat]. arXiv: 1912.00953.
WU, Y., Rosca, M., and Lillicrap, T. (2019b). Deep compressed sensing. In Proceedings of the 36th
International Conference on Machine Learning, volUme 97 of Proceedings of Machine Learning
Research, pages 6850-6860, Long Beach, California, USA. PMLR.
Xie, J., LU, Y., Gao, R., and WU, Y. N. (2018a). Cooperative learning of energy-based model and latent
variable model via mcmc teaching. In AAAI, volUme 1, page 7.
Xie, J., LU, Y., Gao, R., ZhU, S.-C., and WU, Y. N. (2018b). Cooperative training of descriptor and
generator networks. IEEE transactions on pattern analysis and machine intelligence, 42(1):27-45.
Xie, J., LU, Y., ZhU, S.-C., and WU, Y. (2016). A theory of generative convnet. In International
Conference on Machine Learning, pages 2635-2644.
Xie, J., Zheng, Z., Gao, R., Wang, W., ZhU, S.-C., and Nian WU, Y. (2018c). Learning descriptor
networks for 3d shape synthesis and analysis. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 8629-8638.
Xie, J., ZhU, S.-C., and Nian WU, Y. (2017). Synthesizing dynamic patterns by spatial-temporal
generative convnet. In Proceedings of the ieee conference on computer vision and pattern
recognition, pages 7093-7101.
Xie, J., ZhU, S.-C., and WU, Y. N. (2019). Learning energy-based spatial-temporal generative convnets
for dynamic patterns. IEEE transactions on pattern analysis and machine intelligence.
XU, K., DU, C., Li, C., ZhU, J., and Zhang, B. (2018). Learning implicit generative models by teaching
density estimators. arXiv preprint arXiv:1807.03870.
YU, F., Seff, A., Zhang, Y., Song, S., FUnkhoUser, T., and Xiao, J. (2015). LSUN: ConstrUction of
a large-scale image dataset Using deep learning with hUmans in the loop.
YU, L., Song, Y., Song, J., and Ermon, S. (2020). Training deep energy-based models with f-divergence
minimization.
Zenke, F., Poole, B., and GangUli, S. (2017). ContinUal learning throUgh synaptic intelligence.
Proceedings of machine learning research, 70:3987.
Zhang, P., LiU, Q., ZhoU, D., XU, T., and He, X. (2017). On the Discrimination-Generalization Tradeoff
in GANs. arXiv:1711.02771 [cs, stat]. arXiv: 1711.02771.
14
Published as a conference paper at ICLR 2021
A KL Approximate Lower-b ound Estimate
We discuss the relation between KALE (10) and the Kullback-Leibler divergence via Fenchel duality.
Recall that a distribution P is said to admit a density w.r.t. G if there exists a real-valued measurable
function r0 that is integrable w.r.t. G and satisfies dP = r0dG. Such a density is also called the
Radon-Nikodym derivative ofP w.r.t. G. In this case, we have:
KL(P||G)= r0log(r0)dG.
(15)
Nguyen et al. (2010); Nowozin et al. (2016) derived a variational formulation for the KL using Fenchel
duality. By the duality theorem (Rockafellar, 1970), the convex and lower semi-continuous function
ζ : u 7→ ulog(u) that appears in (15) can be expressed as the supremum of a concave function:
Z (U)=SuPuv-Z *(v).
v
The function Z? is called the Fenchel dual and is defined as Z?(v) =suPuuv -Z(u). By convention,
the value of the objective is set to -∞ whenever u is outside of the domain of definition of
Z?. When Z(u) = u log(u), the Fenchel dual Z?(v) admits a closed form expression of the form
Z?(v) = exP(v - 1). Using the expression ofZ in terms of its Fenchel dual Z?, itis possible to express
KL(P||G) as the supremum of the variational objective (16) over all measurable functions h.
F(h):
- hdP- exP(-h)dG + 1.
(16)
Nguyen et al. (2010) provided the variational formulation for the reverse KL using a different choice
for Z: (Z(u) = -log(u)). We refer to (Nowozin et al., 2016) for general f -divergences. Choosing a
smaller set of functions H in the variational objective (16) will lead to a lower bound on the KL. This
is the KL Approximate Lower-bound Estimate (KALE):
KALE(P||G) = suPF(h)
h∈H
(17)
In general, KL(P||G) ≥ KALE(P||G). The bound is tight whenever the negative log-density
h0 = - logr0 belongs to H; however, we do not require r0 to be well-defined in general. Equation
(17) has the advantage that it can be estimated using samples from P and G. Given i.i.d. samples
(X1 , ..., XN ) and (Y1 , ..., YM ) from P and G, we denote by P and G the corresponding empirical
distributions. A simple approach to estimate KALE(P||G) is to use an M -estimator. This is achieved
by optimizing the penalized objective
h := argmaxF(h) — —12(h),
h∈H	2
(18)
where F“is an empirical version ofF and I2 (h) is a penalty term that prevents overfitting due to finite
samples. The penalty I2 (h) acts as a regularizer favoring smoother solutions while the parameter
λ determines the strength of the smoothing and is chosen to decrease as the sample size N and M
increase. The M -estimator of KALE(P||G) is obtained simply by plugging in h into the empirical
objective F (h):
_---~~~ ____ ^ ^ .
KALE(P||G):=F(h).	(19)
We defer the consistency analysis of (19) to Appendix B where we provide convergence rates in
a setting where the set of functions H is a Reproducing Kernel Hilbert Space and under weaker
assumptions that were not covered by the framework of Nguyen et al. (2010).
B	Convergence rates of KALE
In this section, we provide a convergence rate for the estimator in (19) when H is an RKHS. The theory
remains the same whether H contains constants or not. With this choice, the Representer Theorem
allows us to reduce the potentially infinite-dimensional optimization problem in (18) to a convex
finite-dimensional one. We further restrict ourselves to the well-specified case where the density r0
of P w.r.t. G is well-defined and belongs to H, so that KALE matches the KL. While Nguyen et al.
15
Published as a conference paper at ICLR 2021
(2010) (Theorem 3) provides a convergence rate of 1/√N for a related M-estimator, this requires
the density r0 to be lower-bounded by 0 as well as (generally) upper-bounded. This can be quite
restrictive if, for instance, r0 is the density ratio of two gaussians. In Theorem 7, we provide a similar
convergence rate for the estimator defined in (19) without requiring r0 to be bounded. We start by
briefly introducing some notations, the working assumptions and the statement of the convergence
result in Appendix B.1 and provide the proofs in Appendix B.2.
B.1	Statement of the result
We recall that an RKHS H of functions defined on a domain X ⊂ Rd and with kernel k is a Hilbert
space with dot product h.,.i, such that y 7→ k(x,y) belongs to H for any x ∈ X, and
k(x,y) = hk(x,.),k(y,.)i,	∀x,y∈X.
Any function h in H satisfies the reproducing property f(x) = hf,k(x,.)i for any x ∈ X.
Recall that KALE(P||G) is obtained as an optimization problem
KALE(P||G) = sup F (h)	(20)
h∈H
where F is given by:
F(h):=- hdP- exp(-h)dG+1.
Since the negative log density ratio h0 is assumed to belong to H, this directly implies that the
supremum of F is achieved at h0 and F(h0) = KALE(P||G). We are interested in estimating
,一，，一.、	.. ... ^ . ^.
KALE(P||G) using the empirical distributions P and G,
1N	1N
P= N XδXn，	G：= N XδYn，
n=1
n=1
where (Xn)1≤n≤N and (Yn)1≤n≤N are i.i.d. samples from P and G. For this purpose we introduce
the empirical objective functional,
F“(h):
—J hdP — J exp(-h)dG+1.
The proposed estimator is obtained by solving a regularized empirical problem,
hUH “㈤-λ khk2,
(21)
with a corresponding population version,
hUH F ㈤-λ khk2.
(22)
Finally, we introduce D(h,δ) and Γ(h,δ):
D(h,δ) = δexp(-h)dG- δdP,
Γ(h,δ) = -
(1 -t)δ2exp(-(h+tδ))dG.
EI	∙ ∙ 1	∙	i' TTA / 7 C-∖ F T-∖ / 7 C∖	F	, F ʌ / 7 C∖ F T^∖ / 7 C∖ ɪ ,	∙ 11 F
The empirical versions of D(h,δ) and Γ(h,δ) are denoted D(h, δ) and Γ(h,δ). Later, we will show
that D(h,δ) D(h,δ) are in fact the gradients ofF(h) and F(h) along the direction δ.
We state now the working assumptions:
(i)	The supremum ofFoverH is attained at h0.
16
Published as a conference paper at ICLR 2021
(ii)	The following quantities are finite for some positive :
/万荷 dP(x),
/ »k(x,x)exp((khok +e)
Jk(x,x)exp((∣hok +e)»k(x,x)) dG(x).
(iii)	For any h∈H, if D(h,δ) =0 for all δ then h= h0.
Theorem 7. Fix any 1 >η> 0. Under Assumptions (i) to (iii), and provided that λ = √=, it holds
with probability at least 1 - 2η that
∣F(h)-F (ho)∣≤
M 0(η,h0)
√N
fora constant M 0(η,h0) that depends only on η and h0.
The assumptions in Theorem 7 essentially state that the kernel associated to the RKHS H needs to satisfy
some integrability requirements. That is to guarantee that the gradient δ → VF (h)(δ) and its empirical
version are well-defined and continuous. In addition, the optimality condition VF (h)=0 is assumed
to characterize the global solution h0. This will be the case if the kernel is characteristic Simon-Gabriel
and Scholkopf (2018). The proof of Theorem 7, in Appendix B.2, takes advantage of the Hilbert
structure of the set H, the convexity of the functional F and the optimality condition VF (h) =λh of
the regularized problem, all of which turn out to be sufficient for controlling the error of (19).
B.2 Proofs
We state now the proof of Theorem 7 with subsequent lemmas and propositions.
Proof of Theorem 7. We begin with the following inequalities:
λ
ɔ(khk2-khok2) ≤F(h)-F(ho) ≤hVF(ho),h-h。).
2
The first inequality is by definition of h while the second is obtained by concavity of F. For
simplicity we write B= kh-h0k andC= kVF(h0) -L(h0)k. Using Cauchy-Schwarz and triangular
inequalities, itis easy to see that
-2(B2 + 2Bkhok) ≤F(h)-F(ho)≤CB.
Moreover, by triangular inequality, it holds that
.. .. ..^ ..
B≤khλ - h0k + kh-hλk .
Lemma 11 ensures that A(λ) = khλ -h0k converges to 0 as λ→0. Furthermore, by Proposition 12,
we have ∣∣h 一 hλk≤ 1D where D(λ) = ∣∣VF(hλ) -VL(hλ)∣∣. Now choosing λ = √= and applying
Chebychev inequality in Lemma 8, it follows that for any 1 >η >0, we have with probability greater
than 1 - 2η that both
D(λ) ≤
c(kh0kη)
-√N
C≤ C (kh0k,η)
_ -√N-
where C(∣h0 ∣, η) is defined in Lemma 8. This allows to conclude that for any η > 0, it holds with
probability at least 1 一 2η that |F(h) -F(h。)∣≤ M√,h0) where M0(η,ho) depends only on η and h。.
□
We proceed using the following lemma, which provides an expression for D(h,δ) and D(h,δ) along
with a probabilistic bound:
17
Published as a conference paper at ICLR 2021
Lemma 8. Under Assumptions (i) and (ii), for any h∈H such that khk ≤ kh0k +, there exists D(h)
in H satisfying
D(h,δ)=hδ,D(h)i,
and for any h ∈ H, there exists D(h) satisfying
-ʌ . . -ʌ...
D(h,δ) = hδ,D(h)i.
Moreover, for any 0 < η < 1 and any h ∈ H such that khk ≤ kh0k + := M, it holds with probability
greater than 1 -η that
-ʌ
kD(h)-D(h)k≤
C(MM
√N
where C(M,η) depends only on M and η.
Proof. First, we show that δ 7→ D(h,δ) is a bounded linear operator. Indeed, Assumption (ii) ensures
that k(x,.) and k(x,.)exp(-h(x)) are Bochner integrable w.r.t. P and G (Retherford (1978)), hence
D(h,δ) is obtained as
D(h,δ) := <δ,μeχp(-h)G-μpi,
where μexp(-h)G = Rk(x,.)exp(-h(x))dG and μp = Rk(x,.)dP. Defining D(h) to be = μeχp(-h)G -
μp leads to the desired result. D(h) is simply obtained by taking the empirical version of D(h).
Finally, the probabilistic inequality is a simple consequence of Chebychev's inequality.	□
The next lemma states that F (h) and F (h) are Frechet differentiable.
Lemma 9. Under Assumptions (i) and (ii) , h 7→ F(h) is Frechet differentiable on the open ball of
radius kh0k + while h 7→ F(h) is Frechet differentiable on H. Their gradients are given by D(h)
and D(h) as defined in Lemma 8,
VF (h) = D(h),	VF(h)= D(h)
Proof. The empirical functional F(h) is differentiable since it is a finite sum of differentiable
functions, and its gradient is simply given by D(h). For the population functional, we use second
order Taylor expansion of exp with integral remainder, which gives
F(h+δ) =F(h) -D(h,δ)+Γ(h,δ).
By Assumption (ii) We know that +：)converges to 0 as soon as ∣∣δk → 0. This allows to directly
conclude that F is Frechet differentiable, with differential given by δ 7→ D(h,δ). By Lemma 8, we
conclude the existence of a gradient VF (h) which is in fact given by VF (h) = D(h).
□
1-1	∙11 1	,1	, , ∙ X-Z k / 7 ∖ F X-Z 7 ∖ , Q ,1	T , ，十 / 1 、 F
From now on, we will only use the notation VF (h) and VF (h) to refer to the gradients ofF(h) and
F (h). The following lemma states that (21) and (22) have a unique global optimum, and gives a first
order optimality condition.
Lemma 10. The problems (21) and (22) admit unique global solutions h and hλ in H. Moreover, the
following first order optimality conditions hold:
^	-ʌ . ^ .
λh = VF(h),	λhλ = VF (hλ).
Proof. For (21), existence and uniqueness of a minimizer h is a simple consequence of continuity and
strong concavity of the regularized objective. We now show the existence result for (22). Let’s introduce
Gλ(h) = -F (h) + 2 ∣∣h∣2 for simplicity. Uniqueness is a consequence of the strong convexity of Gλ.
For the existence, consider a sequence of elements fk ∈ H such that Gλ(fk) → infh∈HGλ(h). If h0
18
Published as a conference paper at ICLR 2021
is not the global solution, then it must hold for k large enough that Gλ(fk) ≤ Gλ(h0). We also know
that F(fk) ≤ F(h0), hence, it is easy to see that kfkk ≤ kh0 k for k large enough. This implies that
fk is a bounded sequence, therefore it admits a weakly convergent sub-sequence by weak compactness.
Without loss of generality we assume that fk weakly converges to some element hλ ∈ H and that
kfkk ≤ kh0 k. Hence, khλk ≤ liminf k kfk k ≤ kh0 k. Recall now that by definition of weak convergence,
we have fk(x) →k hλ(x) for all x ∈ X. By Assumption (ii), we can apply the dominated convergence
theorem to ensure thatF(fk) →F(hλ). Taking the limit ofGλfk, the following inequality holds:
supGλ(h)=limsupGλ(fk)≤Gλ(hλ).
h∈H	k
Finally, by Lemma 9 We know that F is Frechet differentiable, hence We can use Ekeland and Temam
(1999)(Proposition 2.1) to conclude that VF(hλ) = λhλ. We use exactly the same arguments for
(21).	□
Next, We shoW that hλ converges toWards h0 in H.
Lemma 11. Under Assumptions (i) to (iii) it holds that:
A(λ) := khλ - h0 k → 0.
Proof. We Will first prove that hλ converges Weakly toWards h0, and then conclude that it must also
converge strongly. We start With the folloWing inequalities:
0≥F(hλ)-F(ho) ≥ 2(khλk2-khok2).
These are simple consequences of the definitions of hλ and h0 as optimal solutions to (20) and (21).
This implies that khλk is alWays bounded by kh0k. Consider noW an arbitrary sequence (λm)m≥0
converging to 0. Since khλm k is bounded by kh0k, it folloWs by Weak-compactness of balls in H that
hλm admits a Weakly convergent sub-sequence. Without loss of generality We can assume that hλm
is itself weakly converging towards an element h*. We will show now that h must be equal to ho.
Indeed, by optimality of hλm, it must hold that
λmhλm =VF(hm).
This implies that VF(hm) converges weakly to 0. On the other hand, by Assumption (ii), we can
conclude that VF(hm) must also converge weakly towards VF(h*), hence VF(h*)=0. Finally
by Assumption (iii) we know that ho is the unique solution to the equation VF (h)=0, hence h = ho.
We have shown so far that any subsequence of hλm that converges weakly, must converge weakly
towards ho. This allows to conclude that hλm actually converges weakly towards ho. Moreover, we
also have by definition of weak convergence that:
khok≤lim inf khλmk.
m→∞
Recalling now that khλm k ≤ khok it follows that khλm k converges towards khok. Hence, we have
the following two properties:
•	hλm converges weakly towards ho,
•	khλm k converges towards khok.
This allows to directly conclude that k hλm — ho k converges to 0.	□
Proposition 12. We have that:
1
kh-hλk≤ 二 kVF(hλ) -VF (hλ)k
λ
Proof. By definition of h and hλ the following optimality conditions hold:
^ -ʌ. ^ . .
λh = VF(h),	λhλ = VF (hλ).
19
Published as a conference paper at ICLR 2021
We can then simply write:
.^ -ʌ. ^ . -ʌ. . . -ʌ,
λ(h-hλ)-(VF(h)-VF(hλ)) = VF(hλ )-VF (hλ).
Now introducing δ:= h-hλ and E := VF (h) -VF(hλ) for simplicity and taking the squared norm
of the above equation, it follows that
λ2 kδk2 + kEk2 - 2λhδ,Ei = kV“(hi)-VF (hλ)k2.
Byconcavity of F on H we know that -hh-hλ,Ei ≥ 0. Therefore:
λ2Ilh-hλk2 ≤kV-(hλ)-VF(hλ)k2.
□
C Latent noise sampling and Smoothness of KALE
C.1 Latent space sampling
Here we prove Proposition 6 for which we make the assumptions more precise:
Assumption 1. We make the following assumption:
•	logη is strongly concave and admits a Lipschitz gradient.
•	There exists a non-negative constant L such that for any x,x0 ∈ X and z,z0 ∈Z:
∣E(x)-E(χ0)∣≤ ∣∣χ-χ0k,	IlVxE(X)-VxE(χ0)k ≤ ∣∣χ-χ0∣∣
∣G(z)-G(z0)∣≤ kz-z0k,	kVzG(Z)-VzG(z0)k ≤ ∣∣z-z0k
Throughout this section, we introduce U(z) := -log(η(z))+E(G(z)) for simplicity.
Proof of Proposition 1 . To sample from QG,E, we first need to identify the posterior latent distribution
νG,E used to produce those samples. We rely on (23) which holds by definition of QG,E for any test
function h on X :
h(x)dQ(x) = h(G(z))f (G(z))η(z)dz,
(23)
Hence, the posterior latent distribution is given by ν(z) = η(z)f(G(z)), and samples from GEBM
are produced by first sampling from νG,E, then applying the implicit map G,
X 〜Q o	X = G(Z), Z 〜ν.
□
Proof of Proposition 2. the base distribution G admits a density on the whole space denoted by
exp(-r(x)) and the energy E is of the form E(x) = E(x) - r(x) for some parametric function E,
itis easy to see that Q has a density proportional to exp(-E) and is therefore equivalent to a standard
EBM with energy E.
The converse holds as well, meaning that for any EBM with energy E, it is possible to construct a
GEBM using an importance weighting strategy. This is achieved by first choosing a base G, which
is required to have an explicit density exp(-r) up to a normalizing constant, then defining the energy
of the GEBM tobe E(x) = E(x) -r(x) so that:
、一，、 ，二，、、、_，、 ，一，、、、 .. ..
dQ(x) H exp(-E(x))dGθ (x) H exp(-E(x))dx	(24)
Equation (24) effectively depends only on E(x) and not on G since the factor exp(r) exactly
compensates for the density ofG. The requirement that the base also admits a tractable implicit map
G can be met by choosing G to be a normalizing flow (Rezende and Mohamed, 2015) and does not
restrict the class of possible EBMs that can be expressed as GEBMs.	□
20
Published as a conference paper at ICLR 2021
Proof of Proposition 6. Let πt be the probability distribution of (zt,vt) at time t of the diffusion in
(14), which we recall that
dzt = Vtdt,	dvt = -(Yvt +uVU (Zt))+ λ∕2λudwt,
We call π∞ its corresponding invariant distribution given by
∏∞(z,v) Y exp (-U (z)-2 Ilvk2)
By Lemma 13 we know that U is dissipative, bounded from below, and has a Lipschitz gradient. This
allows to directly apply (Eberle et al., 2017)(Corollary 2.6.) which implies that
W2 (πt,π∞) ≤ C exp(-tc),
where c is a positive constant and C only depends on π∞ and the initial distribution π0. Moreover,
the constant c is given explicitly in (Eberle et al., 2017, Theorem 2.3) and is of order 0(e-q) where
q is the dimension of the latent space Z .
We now consider an optimal coupling Πt between πt and π0. Given joints samples ((zt,vt),(z,v))
from Πt, we consider the following samples in input space (xt,x) := (G(zt),G(z)). Since zt and z
have marginals ∏t and ∏∞, it is easy to see that Xt 〜Pt and X 〜Q. Therefore, by definition of the
W2 distance, we have the following bound:
W22(Pt,Q)≤EIXt-XI2
≤ IG(zt)-G(z)I2dΠt(zt,z)
≤L2 Izt-zI2dΠt(zt,z)
≤ L2W22 (πt,π∞) ≤ C 2 L2 exp(-2tc).
The second line uses the definition of (Xt,X) as joint samples obtained by mapping (zt,z). The third
line uses the assumption that B is L-Lipschitz. Finally, the last line uses that Πt is an optimal coupling
between πt and π∞ .
Lemma 13. UnderAssumption 1, there exists A>0 andλ∈ (0,1 ] such that
1	z>tVU(z) ≥λ(u(z) + Jkzk2) —A,	∀z∈Z,
2	4u
□
(25)
where γ and u are the coefficients appearing in (14). Moreover, U is bounded bellow and has a
Lipschitz gradient.
Proof. For simplicity, let’s call u(z) = - log η(z), w(z) = E? ◦ Bθ? (z), and denote by M an
upper-bound on the Lipschitz constant of w and Vw which is guaranteed to be finite by assumption.
Hence U(z) =u(z)+w(z). Equation (25) is equivalent to having
2
z> Vu(z) — 2λu(z)-----IlzIl 2 ≥ 2λw(z) — z> Vw(z) -2A.
2u
(26)
Using that w is Lipschitz, we have that w(z) ≤ w(0) + MIzI and -z>Vw(z) ≤ MIz I. Hence,
2λw(z) -z>Vw(z) -2A≤ 2λw(0)+(2λ+1)M IzI-2A. Therefore, a sufficient condition for (26)
to hold is
2
z>Vu(z)-2λu(z)-1 ∣∣zH2 ≥ +(2λ+1)M∣z∣-2A+2λw(0).
2u
(27)
We will now rely on the strong convexity ofu, which holds by assumption, and implies the existence
of a positive constant m>0 such that
一u(z) ≥-u(0)-z›Vu(z) + mm∣∣z∣2,
z>Vu(z)≥-IzIIVu(0)I+mIzI2.
21
Published as a conference paper at ICLR 2021
This allows to write the following inequality,
z>Vu(z) — 2λu(z) — Y~ ≥ (1 — 2λ)z>Vu(z) + λ(m + ɪ-)kzk2 — 2λu(0)
2u	2u
2
≥ (1 — λ(m + ɪ ))kzk2 — (1 —2λ)kzkkVu(0)k-2λu(0).
2u
Combining the previous inequality with (27) and denoting M0 = kVu(0)k , it is sufficient to find A
and λ satisfying
(1 — λ(m+ γ2 ) )kz∣∣2 — (M+M0 + 2λ(M — M 0)) kz||—2λ(u(0)+w(0)) + 2A ≥ 0.
The l.h.s. in the above equation is a quadratic function in kzk and admits a global minimum when
λ < (m+ γ2)	. The global minimum is always positive provided that A is large enough.
To see that U is bounded below, it suffice to note, by Lipschitzness ofw, that w(z) ≥ w(0) —Mkzk
and by strong convexity ofu that
u(z) ≥u(0) + M0kzk + m2kzk2.
Hence, U is lower-bounded by a quadratic function in ∣∣zk with positive leading coefficient mm, hence
it must be lower-bounded by a constant. Finally, by assumption, u and w have Lipschitz gradients,
which directly implies that U has a Lipschitz gradient.	□
Proof of Proposition 3. By assumption KL(P||G) < +∞, this implies that P admits a density w.r.t.
G which we call r(x). As a result P admits also a density w.r.t. Q given by:
Zexp(E?(x))r(x).
We can then compute the KL(P||Q) explicitly:
KL(P||Q) = EP[E]+log(Z)+EP[log(r)]
=—LP,G(E?)+KL(P||G).
Since 0 belongs to E and by optimality ofE?, we know that LP,G(E?) ≥ LP,G(0) =0. The result then
follows directly.	□
C.2 Topological and smoothness properties of KALE
Topological properties of KALE. Denseness and smoothness of the energy class E are the key to
guarantee that KALE is a reliable criterion for measuring convergence. We thus make the following
assumptions on E:
(A)	For all E ∈ E, —E ∈ E and there is CE > 0 such that cE ∈ E for 0 ≤ c ≤ CE. For any
continuous function g, any compact support K in X and any precision > 0, there exists
a finite linear combination of energies G= Pir=1aiEi such that supx∈K |f (x) —G(x)| ≤ .
(B)	All energies E in E are Lipschitz in their input with the same Lipschitz constant L>0.
Assumption (A) holds in particular when E contains feedforward networks with a given number of
parameters. In fact networks with a single neuron are enough, as shown in (Zhang et al., 2017, Theorem
2.3). Assumption (B) holds when additional regularization of the energy is enforced during training
by methods such as spectral normalization Miyato et al. (2018) or gradient penalty Gulrajani
et al. (2017) as done in Section 6. Proposition 4 states the topological properties of KALE ensuring
that it can be used as a criterion for weak convergence. A proof is given in Appendix C.2.1 and is a
consequence of (Zhang et al., 2017, Theorem B.1).
Proposition 14. Under Assumptions (A) and (B) it holds that:
1.	KALE(P||G) ≥0 with KALE(P||G) =0 if and only ifP=G.
2.	KALE(P||Gn) →0 if and only ifGn →P under the weak topology.
22
Published as a conference paper at ICLR 2021
C.2.1 Topological properties of KALE
In this section we prove Proposition 4. We first start by recalling the required assumptions and make
them more precise:
Assumption 2. Assume the following holds:
•	The set X is compact.
•	For all E ∈ E, -E ∈ E and there is CE > 0 such that cE ∈ E for 0 ≤ c ≤ CE. For any
continuous function g, any compact support K in X and any precision > 0, there exists
a finite linear combination of energies G= Pir=1aiEi such that |f (x) -G(x)| ≤ on K.
•	All energies E in E are Lipschitz in their input with the same Lipschitz constant L > 0.
For simplicity we consider the set H = E+R, i.e.: H is the set of functions h of the form h = E+c
where E ∈ E and c ∈ R. In all what follows P1 is the set of probability distributions with finite first
order moments. We consider the notion of weak convergence on P1 as defined in (Villani, 2009,
Definition 6.8) which is equivalent to convergence in the Wasserstein-1 distance W1.
Proof of Proposition 4 . We proceed by proving the separation properties (1st statement), then the
metrization of the weak topology (2nd statement).
Separation. We have by Assumption 2 that 0 ∈ E, hence by definition KALE(P P ||G) ≥ FP,G (0) =0.
On the other hand, whenever P = G, it holds that:
FP,G
(h)=-
(exp(-h) + h - 1)dP,
∀h∈H.
Moreover, by convexity of the exponential, we know that exp(-x) +x- 1 ≥ 0 for all x ∈ R. Hence,
FP,G(h) ≤ FP,G(0) = 0 for all h ∈ H. This directly implies that KALE(P|G) = 0. For the converse,
we will use the same argument as in the proof of (Zhang et al., 2017, Theorem B.1). Assume that
KALE(P|G) =0 and let hbeinH. By Assumption 2, there exists Ch > 0 suchthatch∈H and we have:
F (ch) ≤ KALE(P||G) = 0.
Now dividing by c and taking the limit to 0, it is easy to see that - hdP+ hdG ≤ 0. Again, by Assump-
tion 2, we also know that -h ∈ H, hence, hdP- hdG ≤ 0. This necessarily implies that hdP-
hdG=0forall h ∈H. By the density of H in the set continuous functions on compact sets, we can con-
clude that the equality holds for any continuous and bounded function, which in turn implies thatP=G.
Metrization of the weak topology. We first show that for any P and G with finite first moment, it
holds that KALE(P|G) ≤ LW1 (P,G), where W1(P,G) is the Wasserstein-1 distance between P and
G. For any h ∈ H the following holds:
F(h)=- hdP-
exp(-h)dG + 1
h(x)dG(x)-h(x0)dP(x0)
- (exp(-h) + h - 1)dG
J |------{------}
≥0
≤
h(x)dG(x)-h(x0)dP(x0)≤LW1(P,G)
The first inequality results from the convexity of the exponential while the last one is a consequence
of h being L-Lipschitz. This allows to conclude that KALE(P||G) ≤ LW1 (P, G) after taking the
supremum over all h ∈H. Moreover, since W1 metrizes the weak convergence on P1 (Villani, 2009,
Theorem 6.9), it holds that whenever a sequence Gn converges weakly towards P in P1 we also have
W1(P,Gn) →0 and thus KALE(P||Gn) →0. The converse is a direct consequence of (Liu et al., 2017,
Theorem 10) since by assumption X is compact.
23
Published as a conference paper at ICLR 2021
Well-defined learning. Assume that for any > 0 and any h and h0 in E there exists f in 2E such
that kh+h0-fk∞ ≤ then there exists a constant C such that:
KALE(P,Q) ≤ C KALE(P,G)
This means that the proposed learning procedure which first finds the optimal energy E? given a
base G by maximum likelihood then minimizes KALE(P,G) ensures ends up minimizing the distance
between the data end the generalized energy-based model Q.
KALE(P,Q) = supLP,QG (h)
h∈E
=-KALE(P,G)+supLP,G(h+E?)
h∈E
Let’s choose = KALE(P,G) and let h ∈ 2E such that kh+E? - fk∞ ≤ . We have by concavity
of the function (α,β )→Lp,o(α(h+E? —f )+βf) We have that:
Lp,G(h+E?) ≤2Lp,g(2f)-Lp,G(h+E? — f)
By assumption, WehaVethatkh+E? — f k∞ ≤ e, thus |Lp,G(h+E? — f )| ≤ 2e. Moreover, we have
that Lp,g( 1 f) ≤KALE(P,G) since 1 f ∈E. This ensures that:
LP,G(h+E?) ≤ 3KALE(P,G).
Finally, We have shoWn that:
KALE(P,Q) ≤ 2KALE(P,G).
Hence, minimizing KALE(P,G) directly minimizes KALE(P,Q).
□
C.2.2 Smoothness properties of KALE
We Will noW prove Theorem 5. We begin by stating the assumptions that Will be used in this section:
(I)	E is parametrized by a compact set of parameters Ψ.
(II)	Functions in E are jointly continuous W.r.t. (ψ,x) and are L-lipschitz and L-smooth W.r.t.
the input x:
IlEψ(X)- Eψ (XO)Il ≤ Lellx-χ0∣∣,
I∣Vxeψ (x) -VχEψ (x0)k ≤ Lekx — x0k.
(III)	(θ,z) 7→ Gθ (z) is jointly continuous in θ and z, With z7→Gθ(z) uniformly Lipschitz W.r.t. z:
IGθ (z) — Gθ (z0)I ≤ Lb Iz —z0 I,	∀z,z0 ∈ Z,θ ∈ Θ.
There exists non-negative functions a and b defined from Z to R such that θ 7→ Gθ(z) are
a-Lipschitz and b-smooth in the folloWing sense:
kGθ (z)-Gθ0 (z)k≤ a(z)kθ — θ0k,
∣Vθ Gθ (z) — Vg Gθo (z)k≤ b(z)∣θ — θ0k∙
Moreover, a and b are integrable in the folloWing sense:
a(z)2exp(2LeLbIzI)dη(z)<∞,
exp(LeLbIzI)dη(z) <∞,
b(z)exp(LeLbIzI)dη(z) < ∞.
24
Published as a conference paper at ICLR 2021
To simplify notation, we will denote by Lθ(f) the expected Gθ log-likelihood under P. In other words,
Lθ (E)= Lp,g。(E) = - / EdP-log / exp(-E)dGθ.
We also denote by pE,θ the density of the model w.r.t. Gθ,
Pe,θ = ~p----',	ZG ,e = e exp(-E)dGθ.
ZGθ,E
We write K(θ) :=KALE(P∣∣Gθ) to emphasize the dependence on θ.
Proof of Theorem 5. To show that sub-gradient methods converge to local optima, we only need to
show that K is Lipschitz continuous and weakly convex. This directly implies convergence to local
optima for sub-gradient methods, according to Davis and Drusvyatskiy (2018); Thekumparampil et al.
(2019). Lipschitz continuity ensures that K is differentiable for almost all θ ∈ Θ, and weak convexity
simply means that there exits some positive constant C ≥ 0 such that θ 7→ K(θ) +Ckθk2 is convex.
We now proceed to show these two properties.
We will first prove that θ 7→ K(θ) is weakly convex in θ. By Lemma 15, we know that for any E ∈ E,
the function θ 7→ Lθ (E) is M -smooth for the same positive constant M. This directly implies that
it is also weakly convex and the following inequality holds:
Lθt (E) ≤ tLθ (E) + (1-t)Lθ0 (E) + MM t(1-t)kθ - Θ0k2.
Taking the supremum w.r.t. E, it follows that
K(θt) ≤tK(θ) + (1-t)K(θ0) + MMt(1-t)kθ-θ0k2.
This means precisely that K is weakly convex in θ.
To prove that K is Lipschitz, we will also use Lemma 15, which states that Lθ(E) is Lipschitz in θ
uniformly on E . Hence, the following holds:
Lθ(E) ≤Lθ(E)+LCkθ-θ0k.
Again, taking the supremum over E, it follows directly that
K(θ) ≤K(θ0)+LCkθ-θ0k.
We conclude that K is Lipschitz by exchanging the roles of θ and θ0 to get the other side of the
inequality. Hence, by the Rademacher theorem, K is differentiable for almost all θ.
We will now provide an expression for the gradient ofK. By Lemma 16 we know that ψ7→Lθ(Eψ) is
continuous and by Assumption (I) Ψ is compact. Therefore, the supremum supE∈ELθ (E) is achieved
for some function Eθ?. Moreover, we know by Lemma 15 that Lθ(E) is smooth uniformly on E,
therefore the family (∂θLθ(E))E∈E is equi-differentiable. We are in position to apply Milgrom and
Segal (2002)(Theorem 3) which ensures that K(θ) admits left and right partial derivatives given by
∂+K(θ) = tim ∂θLθ (Eθ+te)>e,
t→0
∂-K(θ) = tim ∂θLθ (E?+te)>e,
(28)
t→0
where e is a given direction in Rr. Moreover, the theorem also states that K(θ) is differentiable iff
t → E?+ te is continuous at t = 0. Now, recalling that K (θ) is actually differentiable for almost all θ, it
must hold that E?+te →t→o E? and ∂+ K(θ) = ∂-K(θ) for almost all θ. This implies that the two limits
in (28) are actually equal to ∂θ Lθ (E?)>e. The gradient of K, whenever defined, in therefore given by
Vθ K(θ) = Z-θ1,E? /VχE}(Gθ (z))Vθ Gθ (z)exp(-E? (Gθ (z)))η(z)dz.
□
25
Published as a conference paper at ICLR 2021
Lemma 15. Under Assumptions (I) to (III), the functional Lθ (E) is Lipschitz and smooth in θ
uniformly on E :
∣Lθ (E)-Lθo(E)∣≤ LC kθ-θ0k,
k∂θLθ (E)-∂θLθo(E))k≤ 2CL(1+L)kθ-θ0∣∣.
Proof. By Lemma 16, We have that Le (E) is differentiable, and that
∂e Le (E)= / (VxE ◦Ge Ne Gθ(pe,θ ◦Ge )dη
Lemma 16 ensures that k∂eLe(E)k is bounded by some positive constant C that is independent from
E and θ. This implies in particular that Le (E) is Lipschitz With a constant C. We Will noW shoW that
itis also smooth. For this, We need to control the difference
D ：= k∂e Le (E)-deLe，(E)∣∣.
We have by triangular inequality:
D ≤ / IlVxE OGe-VxE ◦Ge0 IIllVe Ge Il(PE,e ◦Ge )dη
|
}
z
I
+ ∕kVχE oGθ kkVθ Gθ-Vθ Gθo k(pE,θ oGθ )dη
|
{z
II
}
+ ∕kVχE oGθ Vθ Gθ ∣∣∣pe,θ oGθ- per，oGθo∣dη.
|
}
"^^^^^^^^^^^^^^^^^^^^^^^^{^^^^^^^^^^^^^^^^^^^^^^^^""^
III
The first term can be upper-bounded using Le-smoothness ofE and the fact that Gθ is Lipschitz in θ:
I ≤ Lekθ-θ0k ∕∣a∣2(PE,θ ◦ Gθ )dη
≤LeCkθ-θ0k.
The last inequality was obtained by Lemma 17. Similarly, using that VθGθ is Lipschitz, it follows
by Lemma 17 that
II ≤ Lekθ — θ0k ∕∣b∣(PE,θ oGθ )dη
≤LeCkθ-θ0k.
Finally, for the last term III, we first consider a path θt =tθ+(1 -t)θ0 fort∈ [0,1], and introduce the
function s(t) :=PE^t ◦ Gθt. WeWillnoW control the difference pe,θ ◦ Gθ —pe@ ◦ Ge，, also equal to
s(1) -s(0). Using the fact that st is absolutely continuous we have that s(1) -s(0) = R01s0(t)dt. The
derivative s0(t) is simply given by s0(t) = (θ 一θ0)>(Mt - Mt)s(t) where Mt = (VxE◦ BeJVeGθt
and Mt = /MtPE— ◦Get邮.Hence,
S⑴一s(0)=(θ-θ0)> Z (Mt-Mt)s(t)dt.
0
We also knoW that Mt is upper-bounded by La(z), Which implies
III≤L2kθ-θ'k∕o (/|a(Z)|2S⑴(Z)dη(Z)+(∕a(Z)S⑴(Z)dη(Z)))
≤Le2(C+C2)kθ-θ0k,
Where the last inequality is obtained using Lemma 17. This alloWs us to conclude that Le(E) is smooth
for any E ∈ E and θ ∈ Θ.
□
26
Published as a conference paper at ICLR 2021
Lemma 16. Under Assumptions (II) and (III), it holds that ψ 7→ Lθ(Eψ) is continuous, and that
θ7→Lθ(Eψ) is differentiable in θ with gradient given by
∂θLθ(E):=
(VxE oGθ )Vθ Gθ (pe,θ oGθ )dη.
Moreover, the gradient is bounded uniformly in θ and E:
kVθLθ(E)k≤Le
exp(-LeLbkzk)dη(z)
)Z a(z)eXp(LeLbkzk)dη(z).
Proof. To show that ψ7→Lθ(Eψ) is continuous, we will use the dominated convergence theorem. We
fix ψ0 in the interior of Ψ and consider a compact neighborhood W ofψ0. By assumption, we have that
(ψ,χ) → Eψ(x) and (ψ,z) → Eψ(Gθ(z)) arejointly continuous. Hence, ∣Eψ(0)| and ∣Eψ(Gθ(0))|
are bounded on W by some constant C. Moreover, by Lipschitz continuity ofx7→Eψ, we have
IEψ (X)I ≤ IEψ (O)∣+ Lekxk ≤ C +Lekxll,
eXp(-E(Gθ(z))) ≤ eXp(-E(Gθ(0)))eXp(LeLbkzk) ≤ eXp(C)eXp(LeLbkzk).
Recalling that P admits a first order moment and that by Assumption (III), eXp(LeLbkzk) is integrable
w.r.t. η, it follows by the dominated convergence theorem and by composition of continuous functions
that ψ7→Lθ(Eψ) is continuous in ψ0.
To show that θ 7→ Lθ (Eψ ) is differentiable in θ, we will use the differentiation lemma in (Klenke,
2008, Theorem 6.28). We first fix θ0 in the interior of Θ, and consider a compact neighborhood V
of θ0. Since θ 7→ IE(Gθ(0))I is continuous on the compact neighborhood V it admits a maximum
value C; hence we have using Assumptions (II) and (III) that
eXp(-E(Gθ(z))) ≤ eXp(-E(Gθ(0)))eXp(LeLbkzk) ≤ eXp(C)eXp(LeLbkzk).
Along with the integrability assumption in Assumption (III), this ensures that z 7→ eXp(-E (Gθ (z)))
is integrable w.r.t η for all θ in V . We also have thateXp(-E(Gθ(z))) is differentiable, with gradient
given by
VθeXp(-E(Gθ(z)))=VxE(Gθ(z))VθGθ(z)eXp(-E(Gθ(z))).
Using that E is Lipschitz in its inputs and Gθ(z) is Lipschitz in θ, and combining with the previous
inequality, it follows that
kVθeXp(-E(Gθ(z)))k ≤eXp(C)Lea(z)eXp(LeLbkzk),
where a(z) is the location dependent Lipschitz constant introduced in Assumption (III). The r.h.s.
of the above inequality is integrable by Assumption (III) and is independent ofθ on the neighborhood
V . Thus (Klenke, 2008, Theorem 6.28) applies, and it follows that
Vθ
eXp(-E(Gθ0 (z)))dη(z) =
VxE(Gθ0(z))VθGθ0(z)eXp(-E(Gθ0(z)))dη(z).
We can now directly compute the gradient of Lθ(E),
VθLθ(E)=
eXp(-E (Gθ0))dη
VxE(Gθ0 )Vθ Gθ0 eXp(-E (Gθ0 ))dη.
Since E and Gθ are Lipschitz in x and θ respectively, it follows that kVxE(Gθ0 (z))k ≤ Le and
kVθGθ0(z)k ≤ a(z). Hence, we have
/ a(z)(PE,θ ◦ Gθ (z))dη(z).
kVθLθ(E)k≤Le
Finally, Lemma 17 allows us to conclude that kVθLθ(E)k is bounded by a positive constant C
independently from θ and E.	□
Lemma 17. Under Assumptions (II) and (III), there exists a constant C independent from θ andE
such that
/ai(z)(PE,θoGθ(z))dη(z) <C,
(29)
/b(z)(pE,θoGθ(z))dη(z) <C,
fori∈ 1,2.
27
Published as a conference paper at ICLR 2021
Model	FID
Cifar10 Unsupervised	
PixelCNN Oord et al. (2016)	65.93
PixelIQN Ostrovski et al. (2018)	49.46
EBM Radford et al. (2015)	38.2
WGAN-GP Gulrajani et al. (2017)	36.4
NCSN Ho and Ermon (2016)	25.32
SNGAN Miyato et al. (2018)	21.7
GEBM (ours)	19.31
Cifar10 Supervised	
BigGAN Donahue and Simonyan (2019)	14.73
SAGAN Zenke et al. (2017)	13.4
ImageNet Conditional	
PixelCNN	33.27
PixelIQN	22.99
EBM	14.31
ImageNet Supervised	
SNGAN	20.50
GEBM (ours)	13.94
Table 4: FID scores on ImageNet and CIFAR-10.
Proof. By Lipschitzness of E and Gθ, we have exp(-LeLbkzk) ≤ exp(E(Gθ(0)) - E(Gθ(z)) ≤
exp(LeLbkzk), thus introducing the factor exp(E(Bθ0 (0)) in (29) we get
/ai(z)(PE,θoGθ(z))dη(e) ≤Le
exp(-LeLbkzk)dη(z)
)Z a(z)iexp(LeLbkzk)dη(z),
/b(z)(pE,θoGθ(ZXdn(Z) ≤ Le
exp(-LeLbkzk)dη(z)
) Zb(Z)exp(LeLbkZk)dη(Z).
The r.h.s. of both inequalities is independent ofθ and E, and finite by the integrability assumptions
in Assumption (III).	□
D	Image Generation
Figures 3 and4 show sample trajectories using Algorithm 3 with no friction γ=0 for the 4 datasets.
It is clear that along the same MCMC chain, several image modes are explored. We also notice the
transition from a mode to another happens almost at the same time for all chains and corresponds to
the gray images. This is unlike Langevin or when the friction coefficient γ is large as in Figure 5. In
that case each chain remains within the same mode.
Table 4 shows further comparisons with other methods on Cifar10 and ImageNet 32x32.
E Density Estimation
Figure Figure 7 (left) shows the error in the estimation of the log-partition function using both methods
(KALE-DV and KALE-F). KALE-DV estimates the negative log-likelihood on each batch of size
100 and therefore has much more variance than KALE-F which maintains the amortized estimator
of the log-partition function.
Figure Figure 7 (right) shows the evolution of the negative log-likelihood (NLL) on both training and
test sets per epochs for RedWine and Whitewine datasets. The error decreases steadily in the case
of KALE-DV and KALE-F while the error gap between the training and test set remains controlled.
28
Published as a conference paper at ICLR 2021
Figure 3: Samples from the GEBM at different stages of sampling using Algorithm 3 and inverse
temperature β = 1, on CelebA (Left), Imagenet (Right). Each row represents a sampling trajectory
from early stages (leftmost images) to later stages (rightmost images).
Larger gaps are observed for both direct maximum likelihood estimation and Contrastive divergence
although the training NLL tends to decrease faster than for KALE.
F Algorithms
Estimating the variational parameter. Optimizing (9) exactly over A yields (8), with the optimal
A equal to A = log(吉 PMM=Iexp(-E(Y‰,))). However, to maintain an amortized estimator of the
log-partition we propose to optimize (9) iteratively using second order updates:
Ak+1 = Ak -Nexp(Ak-Ak+I)-I),	AO = AO	(30)
where λ is a learning rate and Ak+1 is the empirical log-partition function estimated from a batch
of new samples. By leveraging updates from previous iterations, A can yield much more accurate
estimates of the log-partition function as confirmed empirically in Figure 7 of Appendix E.
Tempered GEBM. It can be preferable to sample from a tempered version of the model by rescaling the
energy E by an inverse temperature parameter β, thus effectively sampling from Q. High temperature
regimes (β→0) recover the base model G while low temperature regimes (β→∞) essentially sample
from minima of the energy E. As shown in Section 6, low temperatures tend to produce better sample
quality for natural image generation tasks.
Training In Algorithm 1, we describe the general algorithm for training a GEBM which alternates
between gradient steps on the energy and the generator. An additional regularization, denoted by I(ψ)
is used to ensure conditions of Proposition 4 and Theorem 5 hold. I(ψ) can include L2 regularization
over the parameters ψ, a gradient penalty as in Gulrajani et al. (2017) or Spectral normalization Miyato
et al. (2018). The energy can be trained either using the estimator in (8) (KALE-DV) or the one in
(9) (KALE-F) depending on the variable C .
29
Published as a conference paper at ICLR 2021
Figure 4: Samples from the GEBM at different stages of sampling using Algorithm 3 and inverse
temperature β = 1, on Cifar10 and LSUN (Right). Each row represents a sampling trajectory from
early stages (leftmost images) to later stages (rightmost images).

Sampling In Algorithm 3, we describe the MCMC sampler proposed in Sachs et al. (2017) which
is a time discretization of (14).
Algorithm 2 Overdamped Langevin Algorithm
1:	Input λ, γ, u,η,E,G
2:	Ouput XT
3:	Zo 〜η // Sample Initial latent from η.
4:	for t = 0,...,T do
5：	Yt+1 -□logη(Zt)-VzE◦B(Zt) //Evaluating Vzlog(ν(Zt+ι)) Using (4).
6：	Wt+ι ~N(0,I) // Sample standard Gaussian noise
7:	Zt+1 - Zt + λYt+ι + √2λWt+ι
8:	end for
9:	XT — G(ZT)
30
Published as a conference paper at ICLR 2021
Figure 5: Samples from the tempered GEBM at different stages of sampling using langevin and inverse
temperature β= 100, on Cifar10 (Left), Imagenet (Middle-left), CelebA (Middle-Right) and LSUN
(Right). Each row represents a sampling trajectory from early stages (leftmost images) to later stages
(rightmost images).
Algorithm 3 Kinetic Langevin Algorithm
1
2
3
4
5
6
7
8
9
10
11
12
13
Input λ, γ, u,η,E,G
Ouput XT
Zo 〜η // Sample Initial latent from η.
for t = 0,...,T do
Zt+ι J Zt+2 Vt
Yt+1 JVzlogη(Zt+ι)-VzE◦ B(Zt+ι) //Evaluating Vzlog(ν(Zt+ι)) using (4).
Vt+1 J Vt + u2λ Yt+1.
Wt+ι 〜N(0,I) // Sample standard Gaussian noise
Vt+1 J exp(一γλ)Vt+1 + P u(1 — exp(-2γλ))W+ι
Vt+1J Vt+1 + u2λ Yt+1
Zt+1 J Zt+1 + 2 Vt+1
end for
XT JG(ZT)
3t
Published as a conference paper at ICLR 2021
100%
Effect of temperature
Ooo
9 8 7
①一Ous OE①>一篇一①a
10-2 IO0	IO2
Temperature β~1
FID score vs MCMC Itearations
0	250	500	750	1000
MCMC iterations
Figure 6: Relative FID score: ratio between FID score of the GEBM QG,E and its base G. (Left)
Evolution of the ratio for increasing temperature on the4 datasets after 1000 iterations of (14). (Right)
Evolution of the same ratio during MCMC iteration using (14).
Log partition estimation
18
10_：
----donsker
----kale
①①≥4eφα:
^16
o
z⊂
a
A
6
3
ω 12
ro
6
z 10
Negative Log Likelιhood
10-4	8
0	500	1000	1500	2000	0
Epochs
Log partition estimation
2 3
- -
O O
1 1
①①≥43①a
6
0	500	1000	1500	2000	0
Epochs
500	1000	1500	2000
Epochs
Negative Log Likelihood
-6 4 2 0 8
- 1 1 1
POOU=*∑i6o^lφ>4ra69N
Figure 7: (Left): Relative error ∣Cc-cJ∣ on the estimation of the ground truth log-partition function
C by c using either KALE-DV or KALE-F Vs training Epochs on RedWine (Top) and WhiteWine
(Bottom) datasets. (Right): Negative log likelihood vs training epochs on both training and test set
for 4 different learning methods (KALE-DV,KALE-F, CD and ML) on RedWine dataset.
500	1000	1500	2000
Epochs
G	Experimental details
In all experiments, we use regularization which is a combination of L2 norm and a variant of the
gradient penalty Gulrajani et al. (2017). For the image generation tasks, we also employ spectral
normalization Miyato et al. (2018). This is to ensure that the conditions in Proposition 4 and Theorem 5
hold. We pre-condition the gradient as proposed in Simsekli et al. (2020) to stabilize training, and
to avoid taking large noisy gradient steps due to the exponential terms in (8) and (9). We also use the
second-order updates in (30) for the variational constant c whenever it is learned.
G.1 Image generation
32
Published as a conference paper at ICLR 2021
Z ∈ R100 〜N(0,I)
dense → Mg X Mg X 512
4 × 4, Stride= 2 deconv. BN 256 ReLU
4x4, Stride= 2 deconv. BN 128 ReLU
4 x 4, Stride= 2 deconv. BN 64 ReLU
3x3, stride= 1 conv. 3 Tanh
Table 5:	BaSe/Generator of SNGAN
ConvNet: Mg = 4.
RGB image X ∈ RM ×M ×3—
3x3, stride= 1 conv 64 lReLU
4x4, stride= 2 conv 64 lReLU
3x3, stride= 1 conv 128lReLU
4 x 4, Stride= 2 conv 128 lReLU
3x3, stride= 1 conv256lReLU
4×4, stride= 2conv256lReLU
3x3, stride= 1 conv512lReLU
dense → 1.
Table 6:	Energy/Discriminator of
SNGAN ConvNet: M=32.
RGB image x ∈ RM ×M ×3
ReSBlockdoWn 128
ReSBlockdoWn 128
ResBlock 128
ReSBlock 128
ReLu
Global sum pooling
dense → 1
Table 7:	Energy/DiScriminator
of SNGAN ReSNet.
Z ∈ R100 〜N (0,I)
dense, 4× 4 X 256
ResBlock UP 256
ResBlock up 256
ReSBlock UP 256
BN, ReLu, 3x3conv, Tanh
Table 8:	Base/Generator of
SNGAN ResNet.
Network Architecture Table 5 and Table 6 shoW the netWork architectures used for the GEBM
in the case of SNGAN ConvNet. Table 5 and Table 6 shoW the netWork architectures used for
the GEBM in the case of SNGAN ResNet. The residual connections of each residual block
consists of tWo convolutional layers proceeded by a BatchNormalization and ReLU activation:
BN+ReLU+Conv+BN+ReLU+Conv as in (Miyato et al., 2018, Figure 8).
Training: We train both base and energy by alternating 5 gradient steps to learn the energy vs
1 gradient step to learn the base. For the first tWo gradient iterations and after every 500 gradient
iterations on base, We train the energy for 100 gradient steps instead of 5. We then train the model
up to 150000 gradient iterations on the base using a batch-size of 128 and Adam optimizer Kingma
and Ba (2014) With initial learning rate of 10-4 and parameters (0.5,.999) for both energy and base.
Scheduler: We decrease the learning rate using a scheduler that monitors the FID score in a similar
way as in BinkoWSki et al. (2018); Arbel et al. (2018). More precisely, every 2000 gradient iterations
on the base, We evaluate the FID score on the training set using 50000 generated samples from the
base and check if the current score is larger than the score 20000 iterations before. The learning rate
is decreased by a factor of 0.8 if the FID score fails to decrease for 3 consecutive times.
Sampling: For (DOT) Tanaka (2019), We use the folloWing objective:
z →kz — Zy + e∣∣ +  -E οG(z)	(31)
keff
Where Zy is sampled from a standard Gaussian, e is a perturbation meant to stabilize sampling and
keff is the estimated Lipschitz constant of Eο B. Note that (31) uses a flipped sign for the EοB
compared to Tanaka (2019). This is because E plays the role of -D Where D is the discriminator in
Tanaka (2019). Introducing the minus sign in (31) leads to a degradation in performance. We perform
1000 gradient iterations With a step-size of 0.0001 Which is also decreased by a factor of 10 every 200
iterations as done for the proposed method. As suggested by the authors of Tanaka (2019) We perform
the folloWing projection for the gradient before applying it:
,	(g>z)
g J g----Lz
√q
33
Published as a conference paper at ICLR 2021
We set the perturbation to 0.001 and keff to 1 which was also shown in Tanaka (2019) to perform well.
In fact, We found that estimating the LiPschitz constant by taking the maximum value of kVEoG(Z)Il
over 1000 latent samples according to η lead to higher values for keff: ( Cifar10: 9.4, CelebA : 7.2,
ImageNet: 4.9, Lsun: 3.8). HoWever, those higher values did not Perform as Well as setting keff = 1.
For (IHM) Turner et al. (2019) We simPly run the MCMC chain for 1000 iterations.
G.2 Density estimation
Pre-processing We use code and Pre-Processing stePs from Wenliang et al. (2019) Which We
describe here for comPleteness. For RedWine and WhiteWine, We added uniform noise With suPPort
equal to the median distances betWeen tWo adjacent values. That is to avoid instabilities due to the
quantization of the datasets. For HePmass and MiniBoone, We removed ill-conditioned dimensions
as also done in PaPamakarios et al. (2017). We sPlit all datasets, excePt HePMass into three sPlits.
The test sPlit consists of 10% of the total data. For the validation set, We use 10% of the remaining
data With an uPPer limit of 1000 to reduce the cost of validation at each iteration. For HePMass, We
used the samPle sPlitting as done in PaPamakarios et al. (2017). Finally, the data is Whitened before
fitting and the Whitening matrix Was comPuted on at most 10000 data Points.
Regularization: We set the regularization Parameter to 0.1 and use a combination ofL2 norm and
a variant of the gradient Penalty Gulrajani et al. (2017):
I (ψ)2 = d1 kΨk2 + E[kVχfψ 区)『]
dψ
Network Architecture. For both base and energy, We used an NVP Dinh et al. (2016) With 5 NVP
layers each consisting of a shifting and scaling layer With tWo hidden layers of 100 neurons. We do
not use Batch-normalization.
Training: In all cases We use Adam oPtimizer With learning rate of 0.001 and momentum Parameters
(0.5, 0.9). For both KALE-DV and KALE-F, We used a batch-size of 100 data samPles vs 2000
generated samPles from the base in order to reduce the variance of the estimation of the energy. We
alternate 50 gradient stePs on the energy vs 1 steP on the base and further Perform 50 additional stePs
on the energy for the first tWo gradient iterations and after every 500 gradient iterations on base. For
Contrastive divergence, each training steP is Performed by first Producing 100 samPles from the model
using 100 Langevin iterations With a steP-size of 10-2 and starting from a batch of 100 data-samPles.
The resulting samPles are then used to estimate the gradient of the of the loss.
For (CD), We used 100 Langevin iterations for each learning steP to samPle from the EBM. This
translates into an imProved Performance at the exPense of increased comPutational cost comPared to
the other methods. All methods are trained for 2000 ePochs With batch-size of 100 (1000 on HePmass
and Miniboone datasets) and fixed learning rate 0.001, Which Was sufficient for convergence.
G.3 Illustrative example in Figure 1
We consider Parametric functions G(θ1) and G(θ2) from R to R of the form:
Gθ1)(x) = sin(8πWx)/(1+4πBx),	G^ (x) = 4πW 0 x+b
With θ= (W,B,W 0,b). We also call θ? = (1,1,1,0). In addition, We consider a sigmoid like function
h from [0,1] to [0,1] of the form:
e= tan(π(z - 2)，	h⑶=1 z+ +1+exP(-9≡)).
Data generation : To generate a data Point X = (X1, X2), We consider the folloWing simPle
generative model:
•	SamPle a uniform r.v. Z from [0,1].
•	APPly the distortion function hto get a latent samPle Y = h(Z).
34
Published as a conference paper at ICLR 2021
•	Generate point X using X1 = G(θ1?) (Y ) and X2 = G(θ2?) (Y ).
Hence, the data are supported on the 1-d line defined by the equation X2 = G(θ2?) (X1).
GAN For the generator we sample Z uniformly from [0, 1] then generate a sample
(X1,X2) = (G(θ1) (Z),G(θ2)(Z)). The goal is to learn θ.
For the discriminator, we used an MLP with 6 layers and 10 hidden units.
GEBM For the base we use the same generator as in the GAN model. For the energy we use the
same MLP as discriminator of the GAN model.
EBM To ensure tractability of the likelihood, we use the following model:
χ2X1 〜N(Gθ2) (XI) ,σθ)
Xi 〜MoG((μ1,σ1),(μ2,σ2))
MoG((μ1,σ1),(μ2,σ2)) refers to a Mixture of two gaussians with mean and variances μ% and σ%. We
learn each of the parameters (θ,σ0 ,μ1,σ1,μ2 ,σ2) by maximizing the likelihood.
Both GAN and GEBM have the capacity to recover the the exact support by finding the optimal
parameter θ?. For the EBM, when θ = θ?, the mean Gθ? (X1) of the conditional gaussian X2 |X1
draws a line which matches the data support exactly, i.e.: X2 = G(θ2?) (X1).
G.4 Base/Generator complexity
To investigate the effect of model complexity of the performance gap between GANs and GEBMs,
we performed additional experiments using the setting of Figure 1. Now we allow the generator/base
network to better model the hidden transformation h that produces the first coordinate X1 given
the latent noise. We choose G(θ1) to be either a one hidden layer network or an MLP with 3 hidden
layers both with leaky ReLU activation, instead of a simple linear transform as previously done in
Appendix G.3. The network has universal approximation capability that depends on the number of units.
This provides a direct control over the complexity of the generator/base. We then varied the number
of hidden units from 1 to 5 * 104 units for the one hidden layer network and from 10 to 5 * 103 units
per layer for the MLP Note that the MLP with 5 * 103 units per layer stores a matrix of size 2.5 * 107
and thus contains 2 orders of magnitudes more parameters than the widest shallow network with 5*104
units. We then compared the performance of the GAN and GEBM using the Sinkhorn divergence
Feydy et al. (2019) between each model and the data distribution. In all experiments, we used the
same discriminator/energy network described in Appendix G.3. Results are provided in Figure 8.
Figure 8: Sinkhorn divergence between data generating distribution and the trained model (either
GAN or GEBM) vs number of hidden units in G(θ1). The left figure represents the one hidden layer
network and right one is for the MLP. Each data point represents the average over 20 independent
runs for each choice of number of hidden units.
35
Published as a conference paper at ICLR 2021
Estimating the Sinkhorn divergence. The Sinkhorn is computed using 6000 samples from the data
and the model, with squared euclidean distance as a ground cost and using a regularization = 1e-3.
We then repeat the procedure 5 times and average the result to get the final estimate of the Sinkhorn
distance for a given run.
Training. Each run optimizes the parameters of the model using Adam optimizer (β1 = .5,β2 = .99),
learning rate lr= 1e-4 for the energy/discriminator and lr= 1e-5 for the base/generator and weight
decay of 1e- 2 for the base/generator. Training is performed using KALE for 2000 epochs using a
batch size of 5000 and 10 gradient iterations for the energy/discriminator per base/generator iteration.
We use the gradient penalty for the energy/discriminator with a penalty parameter of 0.01. We then
perform early stopping and retain the best performing model on a validation set.
Observations We make the following observations from Figure 8: the GAN generator indeed
improves when we increase the number of hidden units. The performance of the GEBM remains
stable as the number of hidden units increases. The performance of the GEBM is always better than
the GAN, although we can see the GAN converging towards the GEBM. GEBM with a simpler base
already outperforms the GAN with more powerful generators. The gap between the GEBM and the
GAN reduces as the GAN becomes more expressive. Using a deeper network further reduces the gap
compared to a shallow network.
These observations support the prior discussion that the energy witnesses a remaining difference
between the generator and training samples, as long as it is not flat. This information allows the
GEBM to perform better than a GAN that ignores it. The performance gap between the GEBM and
the GAN reduces as the generator becomes more powerful and forces the energy to be more flat. This
is consistent with the result in Proposition 3.
36