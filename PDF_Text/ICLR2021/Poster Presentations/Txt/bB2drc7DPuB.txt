Published as a conference paper at ICLR 2021
Global optimality of softmax policy gradient
WITH SINGLE HIDDEN LAYER NEURAL NETWORKS IN THE
mean-field regime
Andrea Agazzi,
Department of Mathematics,
Duke University,
agazzi@math.duke.edu
Jianfeng Lu
Department of Mathematics,
Department of Physics and Department of Chemistry,
Duke University
jianfeng@math.duke.edu
Ab stract
We study the problem of policy optimization for infinite-horizon discounted Markov Decision
Processes with softmax policy and nonlinear function approximation trained with policy
gradient algorithms. We concentrate on the training dynamics in the mean-field regime,
modeling e.g., the behavior of wide single hidden layer neural networks, when exploration
is encouraged through entropy regularization. The dynamics of these models is established
as a Wasserstein gradient flow of distributions in parameter space. We further prove global
optimality of the fixed points of this dynamics under mild conditions on their initialization.
1	Introduction
In recent years, deep reinforcement learning has revolutionized the world of Artificial Intelligence by outperform-
ing humans in a multitude of highly complex tasks and achieving breakthroughs that were deemed unthinkable
at least for the next decade. Spectacular examples of such revolutionary potential have appeared over the last
few years, with reinforcement learning algorithms mastering games and tasks of increasing complexity, from
learning to walk to the games of Go and Starcraft (Mnih et al., 2013; 2015; Silver et al., 2016; 2017; 2018;
Haarnoja et al., 2018a; Vinyals et al., 2019).
In most cases, the main workhorse allowing artificial intelligence to pass such unprecedented milestones was
a variation of a fundamental method to train reinforcement learning models: policy gradient (PG) algorithms
(Sutton et al., 2000). This algorithm has a disarmingly simple approach to the optimization problem at hand:
given a parametrization of the policy, it updates the parameters in the direction of steepest ascent of the associated
integrated value function. Impressive progress has been made recently in the understanding of the convergence
and optimization properties of this class of algorithms in the tabular setting (Agarwal et al., 2019; Cen et al.,
2020; Bhandari & Russo, 2019), in particular leveraging the natural tradeoff between exploration and exploitation
offered for entropy-regularized rewards by softmax policies (Haarnoja et al., 2018b; Mei et al., 2020). However,
this simple algorithm alone is not sufficient to explain the multitude of recent breakthroughs in this field: in
application domains such as Starcraft, robotics or movement planning, the space of possible states and actions
are exceedingly large - or even continuous - and can therefore not be represented efficiently by tabular policies
(Haarnoja et al., 2018a). Consequently, the recent impressive successes of artificial intelligence would be
impossible without the natural choice of neural networks to approximate value functions and /or policy functions
in reinforcement learning algorithms (Mnih et al., 2015; Sutton et al., 2000).
While neural networks, in particular deep neural networks, provide a powerful and versatile tool to approximate
high dimensional functions on continuous spaces (Cybenko, 1989; Hornik, 1991; Barron, 1993), their intrinsic
nonlinearity poses significant obstacles to the theoretical understanding of their training and optimization
properties. For instance, it is known that the optimization landscape of these models is highly nonconvex,
preventing the use of most theoretical tools from classical optimization theory. For this reason, the unprecedented
success of neural networks in artificial intelligence stands in contrast with the poor understanding of these
methods from a theoretical perspective. Indeed, even in the supervised setting, which can be viewed as a special
case of reinforcement learning, deep neural networks are still far from being understood despite having been an
important and fashionable research focus in recent years. Only recently, a theory of neural network learning
has started to emerge, including recent works on mean-field point of view of training dynamics (Mei et al.,
1
Published as a conference paper at ICLR 2021
2018; Rotskoff & Vanden-Eijnden, 2018; Rotskoff et al., 2019; Wei et al., 2018; Chizat & Bach, 2018) and on
linearized dynamics in the over-parametrized regime (Jacot et al., 2018; Allen-Zhu et al., 2018; Du et al., 2018;
2019; Zou et al., 2018; Allen-Zhu et al., 2019; Chizat et al., 2019; Oymak & Soltanolkotabi, 2020; Ghorbani
et al., 2019; Lee et al., 2019). More specifically to the context of reinforcement learning, some works focusing
on value-based learning (Agazzi & Lu, 2019; Cai et al., 2019; Zhang et al., 2020), and others exploring the
dynamics of policy gradient algorithms (Zhang et al., 2019) have recently appeared. Despite this progress, the
theoretical understanding of deep reinforcement learning still poses a significant challenge to the theoretical
machine learning community, and it is of crucial importance to understand the convergence and optimization
properties of such algorithms to bridge the gap between theory and practice.
Contributions.
The main goal of this work is to investigate entropy-regularized policy gradient dynamics for wide, single hidden
layer neural networks. In particular, we give the following contributions:
•	We give a mean-field formulation of policy gradient dynamics in parameter space, describing the
evolution of neural network parameters in the form of a transport partial differential equation (pde).
We prove convergence of the particle dynamics to their mean-field counterpart. We further explore the
structure of this problem by showing that such pde is a gradient flow in the Wasserstein space for the
appropriate energy functional.
•	We investigate the convergence properties of the above dynamics in the space of measures. In particular,
we prove that under some mild assumptions on the initialization of the neural network parameters and
on the approximating power of the nonlinearity, all fixed points of the dynamics are global optima, i.e.,
the approximate policy learned by the neural network is optimal,
Related Works .
Recent progress in the understanding of the parametric dynamics of simple neural networks trained with gradient
descent in the supervised setting has been made in (Mei et al., 2018; Rotskoff & Vanden-Eijnden, 2018; Wei et al.,
2018; Chizat, 2019; Chizat & Bach, 2020). These results have further been extended to the multilayer setting in
(Nguyen & Pham, 2020). In particular, the paper (Chizat & Bach, 2018) proves optimality of fixed points for
wide single layer neural networks leveraging a Wasserstein gradient flow structure and the strong convexity of
the loss functional wrt the predictor. We extend these results to the reinforcement learning framework, where
the convexity that is heavily leveraged in (Chizat & Bach, 2018) is lost. We bypass this issue by requiring a
sufficient expressivity of the used nonlinear representation, allowing to characterize global minimizer as optimal
approximators.
The convergence and optimality of policy gradient algorithms (including in the entropy-regularized setting) is
investigated in the recent papers (Bhandari & Russo, 2019; Mei et al., 2020; Cen et al., 2020; Agarwal et al.,
2019). These references establish convergence estimates through gradient domination bounds. In (Mei et al.,
2020; Cen et al., 2020) such results are limited to the tabular case, while (Agarwal et al., 2019; 2020) also discuss
neural softmax policy classes, but under a different algorithmic update and assuming certain well-conditioning
assumptions along training. Furthermore, all these results heavily leverage the finiteness of action space. In
contrast, this paper focuses on the continuous space and action setting with nonlinear function approximation.
Further recent works discussing convergence properties of reinforcement learning algorithms with function
approximation via neural networks include (Zhang et al., 2019; Cai et al., 2019). These results only hold for
finite action spaces, and are obtained in the regime where the network behaves essentially like a linear model
(known as the neural or lazy training regime), in contrast to the results of this paper, which considers training
in a nonlinear regime. We also note the work (Wang et al., 2019) where the action space is continuous but the
training is again in an approximately linear regime. 2 * * S
2 Markov Decision Processes and policy gradients
We denote a Markov Decision Process (MDP) by the 5-tuple (S, A, P, r, γ), where S is the state space, A
is the action space, P = P (s, a, s0)s,s0∈S,a∈A a Markov transition kernel, r(s, a, s0)s,s0∈S,a∈A is the real-
valued, bounded and continuous immediate reward function and γ ∈ (0, 1) is a discount factor. We will
consider a probabilistic policy, mapping a state to a probability distribution on the action space, so that
π : S → M1+(A) , where M1+(A) denotes the space of probability measures on A, and denote for any
S ∈ S the corresponding density ∏(s, ∙) : A → R+. The policy defines a State-to-state transition operator
2
Published as a conference paper at ICLR 2021
Pπ(s, ds0) = A P(s, a, ds0)π(s, da), and we assume that Pπ is Lipschitz continuous as an operator M+1 (S) →
M+1 (S) wrt the policy. We further encourage exploration by defining some (relative) entropy-regularized rewards
(Williams & Peng, 1991)
Rτ(s, a, s0) = r(s, a, s) - tDkl(π(s, ∙); π( ∙)),
where DKL denotes the relative entropy, ∏ is a reference measure and T indicates the strength of regularization.
Throughout, we choose ∏ to be the Lebesgue measure on A, which we assume, like S, to be a compact subset
of the Euclidean space. This regularization encourages exploration and absolute continuity of the policy wrt
Lebesgue measure. Consequently, with some abuse of notation, we use throughout the same notation for a
distribution and its density in phase space. Note that the original, unregularized MDP can be recovered in the
limit τ → 0.
In this context, given a policy π the associated value function Vπ : S → R maps each state to the infinite-horizon
expected discounted reward obtained by following the policy π and the Markov process defined by P :
∞
Vπ (s) = Eπ X γ Rτ (st, at, st+1 )s0 = s	(1)
t=0
∞
=En Xγt(r(st,at,st+ι) - τDκL(π(st, ∙);∏( ∙ )))∣so = s ,
t=0
where En [ ∙ |so = s] denotes the expectation of the stochastic process St starting at s° = S and following
the (stochastic) dynamics defined recursively by the transition operator Pπ (s, ds0) = P (s, a, ds0)π(s, da).
Correspondingly, we define the Q-function Qn : S × A → R as
∞
Qn(S, a) = En r(S0, a0, S1) + XγtRτ(St, at, St+1)∣∣S0 = S, a0 = a
t=1
=r(s,a) + YEn IVn(s1)∣s0 = s,a0 = a] ,	(2)
where r(s, a) = E[r(s, a, s0)] is the average reward from (s, a). Conversely, from the definition, we have the
identity for Vπ(s) = E∏ [Q∏(so, ao)∣so = s] - tDkl(π(s, ∙); ∏( ∙)).
We are interested in learning the optimal policy ∏ of a given MDP (S, A, P, r, γ), which satisfies for all S ∈ S
V∏* (s) =	max	V∏ (s).	(3)
n : S→M1+ (A)
More specifically we would like to estimate this function through a family of approximators πw : S → M1+(A)
parametrized by a vector w ∈ W := Rp. Note that since we consider entropy-regularized rewards, the optimal
policy will be a probabilistic policy (given as a Boltzmann distribution) instead ofa deterministic one.
A popular algorithm to solve this problem is given by policy gradient algorithm (Sutton & Barto, 2018). Starting
from an initial condition w(0) ∈ W, this algorithm updates the parameters w of the predictor in the direction of
steepest ascent of the average reward
w(t + 1) ：= w(t) + βtVwES〜％。V∏(s),	(4)
for a fixed absolutely continuous initial distribution of initial states %0 ∈ M1+(S) and sequence of time steps
{βt}t. Here E[∙] denotes an approximation of the expected value operator.
This work investigates the regime of asymptotically small constant step-sizes βt → 0. In this adiabatic limit, the
stochastic component of the dynamics is averaged before the parameters of the model can change significantly.
This allows to consider the parametric update as a deterministic dynamical system emerging from the averaging
of the underlying stochastic algorithm corresponding to the limit of infinite sample sizes. This is known as
the ODE method (Borkar, 2009) for analyzing stochastic approximation. We focus on the analysis of this
deterministic system to highlight the core dynamical properties of policy gradients with nonlinear function
approximation. The averaged, deterministic dynamics is given by the set of odes
djtw(t) = Es〜％0 [Vw V∏(s)] = Es〜％∏,a〜∏w [Vw log ∏w (s,a) (Q∏ (s,a) - T log(∏w (s,a)))] ,	(5)
3
Published as a conference paper at ICLR 2021
where in the second equality we have applied the policy gradient theorem (Sutton et al., 2000; Sutton & Barto,
2018), defining for a fixed %0 ∈ M1+ (S)
∞
%n(s0, s) :=	γtPnt(s0, s) ,	%n(s) =	%n(s0, s)%0(ds0) ,
t=0	S
(6)
as the (improper) discounted empirical measure. For completeness, we include a derivation of (5) in Appendix A.
Softmax policies in the mean-field regime
We choose to represent our policy as a softmax policy:
e e =	exp(fw (s,a))
πw(S,a)= RA exp(fw(s,a))da
and parametrize the energy f as a two-layer neural network in the mean-field regime, i.e.,
1N
fw (S,a) = Nf ψ(S,a, Wi)
for a fixed, usually nonlinear function ψ : S ×A× Ω → R, where we have separated W ∈ W into N identical
components Wi ∈ Ω so that W = Ωn.
We can rewrite the above expression in terms of an empirical measure:
1N
fν(N) (s,α) ：=	ψ(s,a; ω)ν(N)(dω)	where	V(N)(dω) = — ^δwω (dω) ∈M;(Ω).	⑺
Jn	N i=l
This empirical measure representation removes the symmetry of the approximating functions under permutations
of parameters Wi. It also facilitates the limit N → ∞, when ν(N) → ν weakly, so that fν(N) → fν. Then, for a
general distribution V ∈ M+(Ω) the softmax mean-field policy reads:
πν (s, a) =
exp(Jω Ψ(s, a； ω)ν(dω))
(8)
Note that by our choice of softmax policy and mean-field parametrization (7) we have
Vwi log πν(N) (S, a) = Vwi fν(N)(S, a) - Vwi log	expfν(N)(S, a)da
A
ι ʌ	RA VWieχp [N1 PiN=1 ψ(S, a； Wi ) da
=VWi N 工 ψ(s,a; Wi)	Ra exp fν(N) (s,α)dα
=N (VWiΨ(s, a； Wi) — / Vwiψ(s, a； WilnV(N) (s,
Thus the training dynamics (5), after an appropriate rescaling of time (t 7→ t/N, which is due to the mean-field
parametrization for fw), can be rewritten as
ddtwi(t) = !s /(Vwiψ(s,a; Wi)- EnV(N) Vwaψ(S, •； Wi)DX
× Qπν(N) (s, a) - τlogπν(N)(s, a) πν(N)(s,da)%πν(N)
The training dynamics can be more compactly represented by the evolution of the measure ν ∈
parameter space, given by a mean-field transport partial differential equation of the Vlasov type as
d∣νt(ω)=div &t(ω)( 乂 C∏ν [Vωψ(s, ∙； ω), Q∏ν - T log ∏ν](s)%∏ν (ds)),
(ds).	(9)
M+(Ω) in
(10)
where ω ∈ Ω and We have introduced the shorthand Cn [f, g](s) to denote the covariance operator WRT the
probability measure π(s, da). Note that the above partial differential equation also captures the dynamics of the
finite-width system, i.e., of the empirical measure ν(N) where each wi follows (9).
4
Published as a conference paper at ICLR 2021
We further note that the dynamics introduced above have a gradient flow structure in the probability space
M+(Ω): defining the expected value function
E [ν ]= Es0~%0 [V∏ν(S0)]	(11)
the dynamics (10) is a gradient flow for E in the Wasserstein space (see e.g., (Santambrogio, 2017) for an
introduction), as we prove in the appendix:
Proposition 2.1. For a fixed initial distribution %0 ∈ M1+ (S), the dynamics (10) is the Wasserstein gradient
flow of the energy functional (11).
Analogous dynamics equation for evolution of parameter space measure in the supervised learning case has been
derived in (Mei et al., 2018; Rotskoff & Vanden-Eijnden, 2018; Chizat & Bach, 2018) and in the td learning
case in (Agazzi & Lu, 2019; Zhang et al., 2020). In particular, in the case of supervised learning, the resulting
dynamics is a Wasserstein gradient flow, the structure of which is used to obtain the convergence of the particle
system to the mean-field dynamics. In our case, however, the energy functional is not convex wrt the policy and
moreover the softmax parametrization destroys the convexity of the approximator of the policy with respect to
νt . Thus showing convergence of the dynamics becomes much more challenging.
3 Simplified setting: the bandit problem
We now introduce our results in the simple bandit setting, where state space S is one point (and will be henceforth
suppressed in the notation) and without loss of generality action space A is continuous. In this case, for a reward
function r and a softmax policy
() =	exp(fν (a))
”""	RA exp(fν(a))da
we have that the value function for the regularized problem reads (we denote Vν = Vπν to simplify notation)
Vν
(r(a) - τ log πν (a))πν (da) ,
while the Q function is simply Q(a) = r(a). We further note that the optimal policy in the regularized case
reads:
π*(a) = Z-1 exp(τ-1r(a)),
exp(τ
A
-1r(a))da
Z
Recalling the definition of the covariance operator Cπ[f, g](s) from (10), the expression for the policy gradient
vector field in the latter case simplifies to
∂tωt
Ft(ωt; νt) = VωDnVDVV = C∏νVωψ(a; ω),r - Tlog(∏ν)]
Vωψ(a; ω) - ʃ Vωψ(a03∏ν(da0)) (r(a) - TfV(a)) ∏(da),
(12)
where DnV, DVV denote the Frechet derivative of ∏ν and VV WRT V and n respectively. Note that by the structure
of the covariance operator Cn, adding a constant to the function f (∙) does not affect the dynamics. This reflects
the fact that the softmax policy is normalized by definition.
3.1	Global optimality of softmax policy gradient
We now sketch the main steps in proving that the mean-field policy gradient dynamics converge, under appropriate
assumptions, to global optimizers. The proof in this simpler setting is much more transparent than the general
case to be discussed in the next section, and will provide some intuition for the latter. The first part of the
proof concerns the properties of fixed points of the dynamics (10), while the second part concerns the training
dynamics.
Statics
We first informally prove global optimality of any fixed point ν* of the transport equation 奈Vt = -div(VtFt)
with Ft from (12) such that
a)	ν* has full support in Ω,
5
Published as a conference paper at ICLR 2021
b)	the nonlinearity is 1-homogeneous in the first component of its parameters, i.e., that writing ω = (ω0, ω) ∈
R X Θ one has ψ(a; ω) := ω0φ(a; ω) for a regular enough φ : AX Θ → R,
c)	the span of {φ(a; ω)}ω∈θ is dense in L2(A),
so that i.e., ∏ν* (a) = ∏*(a) = Z-1 exp [τ-1r(a)]. Weaker assumptions and the general statement are given in
the next section, while the general proof appears in the appendix.
First, We note that by assumption a), div(ν*F(∙; V*)) = 0 directly implies that for almost all ω ∈ Ω
F(ω; v*)
I Vωψ(a; ω) (r(a)
A
- τ fν* (a) - Vν* ) πν* (da) = 0 .
In particular, by homogeneity assumption b), the first component of the above vector field must vanish on Θ.
A
φ(a; ω) (r(a)
- τfν* (a) - Vν* ) πν* (da) = 0 .
By assumption c) that span of φ is dense in L2(A) the above implies that
r(a) - τfν* (a) - Vν* = 0 πν* -a.e. in A.	(13)
Finally, recalling that by the softmax parametrization and by the boundedness of φ, πν* (a) > 0, we must have
fν* (a) = τ -1r(a) + C
which directly implies the optimality of the policy.
Dynamics
While it is clear that assumption b) and c) about the structure and approximating power of the nonlinearity ψ
hold independently of t, we want to show that assumption a) also holds uniformly in time. In this sense, the
continuity of the vector field (12) will preserve the full support properties of the measure νt for all t > 0, as we
will prove in a more general framework in Lemma C.2. Consequently, any measure ν respecting assumption
a) at initialization will do so for any finite positive time t > 0. However, the question remains of whether this
property still holds at t = ∞. This is the object of Lemma C.3, where we prove that whenever the gradient
approaches a fixed point in parameter space, if this fixed point is not a global minimizer, it must be avoided by
the dynamics, and thus, the only possible fixed points of the dynamics are global minimizers.
4 Results in the general setting
We now come back to the general mdp framework introduced in Section 2.
4.1	Assumptions
To state the main result of this section, the optimality of fixed points of (10) we need the following
Assumption 1. Assume that ω = (ω0, ω) ∈ R X Θ for Θ = RmT and ψ(s, a; ω) = ω0φ(s, a; ω) with
a)	Regularity of φ: φ is bounded, differentiable and Dφω is Lipschitz. Also, for all f ∈ L2 (S X A) the
regular values ofthe map ω → gf (ω) := f f (s, a)φ(s, a; ω) are dense in its range, and gf (rω) converges
in C 1({ω ∈ Θ : kω∣∣2 = 1}) as r → ∞ to a map gf (ω) whose regular values are dense in its range.
b)	Universal approximation: the span of {φ(∙, ω) : ω ∈ Θ} is dense in L2(S X A);
c)	Support of the measure: There exists r > 0 s.t. the support of the initial condition ν0 is contained in
Qr := [-r, r] X Θ and separates {-r} X Θ from {r} X Θ, i.e., any continuous path connecting {-r} X Θ
to {r} X Θ intersects the support ofν0.
Assumption 1 a) is a common, technical regularity assumption ensuring that (10) is well behaved and controlling
the growth, variation and regularity of φ. Alternative assumptions on the case Θ 6= Rm-1 are given in the
appendix. Assumption 1 b) speaks to the approximating power of the nonlinearity, assumed to be expressive
enough to approximate any function in L2 (S X A). This condition replaces the convexity assumption from
Chizat & Bach (2018), as the lack of convex structure in our setting prevents us from identifying the local
and global minimization properties of a fixed point. Indeed, despite the one-point convexity of E% [Vπ(s)] as a
6
Published as a conference paper at ICLR 2021
functional of π (Kakade & Langford, 2002) which can be leveraged in the tabular case, such property will be
lost, in general, when restricting to policies through nonlinear function approximation. We bypass this issue by
requiring sufficient expressivity of the approximating function class, guaranteeing that the optimal policy can be
represented with arbitrary precision. Similar assumption on approximability of neural network representation
was made in recent analysis of natural policy gradient algorithm (Agarwal et al., 2019). We note that this
assumption is easily satisfied by widely used nonlinearities by the universal approximation theorem (Cybenko,
1989; Barron, 1993). Examples of activation functions satisfying Assumption 1 a)-b) include sigmoid, tanh
and Gaussian radial function nonlinearities. Extension to analogous results in the ReLU case was discussed in
Wojtowytsch (2020) for the supervised learning. Finally, Assumption 1 c) guarantees that the initial condition
is such that the expressivity from b) can actually be exploited. This condition is satisfied for example by the
product of a uniform distribution on any bounded set A ⊂ R with the normal distribution on Θ or, if Θ is
compact, with the uniform distribution on Θ.
4.2	Convergence of the many-particle limit
Before discussing the optimality properties of the dynamics (10), we show that this pde accurately describes the
policy gradient dynamics of a sufficiently wide, single layer neural network. To this aim, We let P2 (Ω) be the
space of probability distributions on Ω with finite second moment.
Theorem 4.1. LetAssumption 1 hold and let W(N) be a solution of(5) with initial condition w0N) ∈ W = Ωn.
If VON) converges to νo ∈ P2(Ω) in Wasserstein distance W2 then VtN) converges, for every t > 0, to the unique
solution νt of (10).
We note that by the law of large numbers for empirical distributions, the condition of convergence of V0(N) to V0
is e.g., satisfied when w0(i) are drawn independently at random from V0.
The proof of this result is largely standard, under the given assumptions, and is provided in the appendix for
completeness. The idea of the proof is a canonical propagation of chaos argument (Sznitman, 1991). In a
nutshell the first step of the argument establishes sufficient regularity of the gradient dynamics, allowing to
guarantee existence and uniqueness of the solution to (10). Then, one bounds the difference in differential
updates for the particle system and the mean-field dynamics by comparing them with the evolution of the particle
system according to a linear, time-inhomogeneous PDE using the drift term of the mean-field model. The proof is
finally concluded by application of Gronwall inequality. The main difficulty wrt similar results in the literature
is to establish the needed Lipschitz continuity of the vector field driving the transport PDE: while this is an
immediate consequence of assumptions on the activation functions and on the risk functional in the supervised
setting, proving this type of regularity requires more effort in the rl setting given the involved dependence of
the vector field on the measure Vt.
4.3	Optimality
After discussing the connection between particle dynamics and mean-field equations, we present the main
convergence result of this paper:
Theorem 4.2. LetAssumption 1 hold and Vt given by (10) converge to V *, then ∏ν* = ∏* ,the optimal policy
for (3).
Thus if the policy gradient dynamics (10) converges to a stationary point, that point must be a global minimizer.
Again, we emphasize that in our regularized setting π * is given by a probability distribution, and can thus
be represented as a softmax policy. We prove this result in three steps. First, we connect the optimality of a
stationary point with the support of the underlying measure in parameter space. More specifically, we show in
Lemma C.1 that by the expressivity of φ, the transport vector field of suboptimal fixed points of the dynamics
(10) cannot vanish everywhere in parameter space. This implies that a measure with sufficient support cannot
correspond to a suboptimal fixed point.
We then show in Lemma C.2 that such sufficient notion of support (Assumption 1 c) ) is preserved by the
mean-field policy gradient dynamics (10) throughout training. For any finite time, this is true by topological
arguments: the separation property of the measure cannot be altered by the action of a continuous vector field
such as (10). We note in particular that we do not prove that assumption a) in Section 3 holds in this case.
Finally, in Lemma C.3 we combine the above results and prove that spurious fixed points are avoided by the
policy gradient dynamics (10) when initialized properly. To establish this we argue by contradiction: assuming
7
Published as a conference paper at ICLR 2021
(a)
(b)
Figure 1: Evolution of E(V*) - E(Vt) as a function of training time, for experiments (a) and (b) as described
in the main text. Different lines correspond to different random initializations of reward function and learning
model. We see that the training error decreases monotonically (but with different rates) during trainig.
that we are approaching such a spurious fixed point V at time t°, we show in Lemma C.5 that the velocity
field will change little for any t > t0. In particular, it follows that in this regime the dynamics of (10) can be
approximated by the gradient descent dynamics (in particle space) of an approximately fixed potential. On the
other hand, by Assumption 1 c) and by the homogeneity of ψ, we are able to show that by Lemma C.2 a positive
amount of measure VV will fall in a forward invariant region where its ω0 component will grow linearly in t
(which exists by Lemma C.1), thereby eventually contradicting the assumption that VV is a fixed point of (10).
There are two main conceptual differences between the proof outlined above and the one carried out in the
supervised learning setting. On one hand, a necessary step in our proof is to establish Lipschitz continuity of
the vector field defining the transport equation (10), also needed for convergence of the particle dynamics as
discussed above. On the other hand, the landscape of the objective function for policy gradients does not enjoy
the convexity (wrt the predictor) typically assumed in the supervised case. To exclude the existence of local
minima we assume sufficient expressivity of the activation functions, absent in the supervised analysis. This
assumption is key to deduce optimality of fixed points of (10) in Theorem 4.2 in our less regular setting.
5	Numerical examples
To test our theoretical results in a simple setting we train a wide, single hidden layer neural network with policy
gradients to learn the optimal softmax policy (8) for entropy-regularized rewards with parametrization (7) and
regularization parameter τ = 0.2. We do so in two separate settings:
(a)	S = {0}, A = [0, 1]. This setting corresponds to bandits framework discussed in Section 3.
(b)	S × A is a grid of size 100 × 100 in the set [0, 1]2. In this case, we have chosen a discount factor of
γ = 0.7 and a transition process given by P(s, a, s0) = 0.9δ(s0 - a) + 0.1/100 (i.e., an action a leads to
the corresponding state s0 = a with probability 0.9 and is uniformly distributed with probability 0.1). At
each iteration we have computed the exact distribution %π by computing the resolvent of the (weighted)
transition matrix.
In both cases, we defined the optimal Q function as Q*(s, a) = Tfw(s, a), where fW(s, a) is given by a single
hidden layer neural network of width n = 5, ReLU nonlinearites and weights w drawn independently and
identically distributed from a centered, normal distribution with variance σ2 = 4, i.e.,
Q*(s,a) = Tfw(s,a)	for Wi 〜N(0,4).
We learn the optimal policy for the problem defined above using a N = 800-neurons wide single hidden layer
neural network with ReLU nonlinearities in the mean-field regime (7) used as energy for a softmax policy
(8). The initialization of the student network is as follows: first-layer weights are initialized at random drawn
8
Published as a conference paper at ICLR 2021
independently from a centered, normal distribution with variance σ2 = 4, while output weights are initialized at
0. The model is trained according to (4) with fixed step-size βt = 10-3. We report the results of this training
procedure in Fig. 1, where We notice that all the paths monotonically decrease the error E(V*) - E(νt), as
predicted by our results. Note that the convergence rate of the model varies across experiments, consistently
with the purely qualitative nature of the convergence result we proved.
6	Conclusions and future work
This work addresses the problem of optimality of policy gradient algorithms, a workhorse of deep reinforcement
learning, when combined with mean-field models such as neural networks. More specifically, we provide a
mean-field formulation of the parametric dynamics of policy gradient algorithms for entropy-regularized mdps
and prove that, under mild assumptions, all fixed points of such dynamics are optimal. This extends similar
results obtained in the “neural” or “lazy” regime to the mean-field one, which is known to be much more
expressive (E et al., 2019; Ghorbani et al., 2020), but also highly nonlinear. The latter feature prevents, at present,
from obtaining convergence results of these models, except in very specific settings (Chizat, 2019; Javanmard
et al., 2019).
Interesting avenues or future research include the relaxing the adiabaticity assumption, i.e., considering the
stochastic approximation problem resulting from the finite number of samples and the finite gradient step-size, as
well as establishing quantitative bounds for models with a large, but finite, number of parameters. Probably the
most important open question, however, concerns establishing quantitative convergence of mean-field dynamics
of neural networks: even in the supervised setting, despite recent results in specific settings (Chizat, 2019;
Javanmard et al., 2019), these guarantees remain mainly out of reach.
Acknowledgments.
AA acknowledges the support of the Swiss National Science Foundation through the grant P2GEP2-17501 and
by the NSF grant DMS-1613337. The work of JL is in part supported by the US National Science Foundation
via grants CCF-1910571 (Duke TRIPODS) and DMS-2012286.
References
Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. Optimality and approximation with policy
gradient methods in markov decision processes. arXiv preprint arXiv:1908.00261, 2019.
Alekh Agarwal, Mikael Henaff, Sham Kakade, and Wen Sun. Pc-pg: Policy cover directed exploration for
provable policy gradient learning. arXiv preprint arXiv:2007.08459, 2020.
Andrea Agazzi and Jianfeng Lu. Temporal-difference learning for nonlinear value function approximation in the
lazy training regime. arXiv preprint arXiv:1905.10917, 2019.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. arXiv preprint arXiv:1811.03962, 2018.
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neural
networks, going beyond two layers. In Advances in neural information processing systems, pp. 6155-6166,
2019.
LUigi Ambrosio, Nicola Gigli, and Giuseppe Savare. Gradient flows: in metric spaces and in the space of
probability measures. Springer Science & Business Media, 2008.
A. R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions
on Information Theory, 39(3):930-945, 1993.
Jalaj Bhandari and Daniel Russo. Global optimality guarantees for policy gradient methods. arXiv preprint
arXiv:1906.01786, 2019.
Vivek S Borkar. Stochastic approximation: a dynamical systems viewpoint, volume 48. Springer, 2009.
Qi Cai, Zhuoran Yang, Jason D Lee, and Zhaoran Wang. Neural temporal-difference learning converges to
global optima. In Advances in Neural Information Processing Systems, pp. 11315-11326, 2019.
9
Published as a conference paper at ICLR 2021
Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi. Fast global convergence of natural policy
gradient methods with entropy regularization. arXiv preprint arXiv:2007.06558, 2020.
Lenaic Chizat. Sparse optimization on measures with over-parameterized gradient descent. arXiv preprint
arXiv:1907.10300, 2019.
LenaIc Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized models
using optimal transport. In Proceedings of the 32Nd International Conference on Neural Information
Processing Systems, NIPS'18,pp. 3040-3050, 2018.
Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks trained
with the logistic loss. arXiv preprint arXiv:2002.04486, 2020.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. In Advances
in Neural Information Processing Systems, pp. 2933-2943, 2019.
G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and
Systems, 2(4):303-314, Dec 1989.
S. S. Du, X. Zhai, B. Poczos, and A. Singh. Gradient descent provably optimizes over-parameterized neural
networks, 2018. preprint, arXiv:1810.02054.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep
neural networks. In International Conference on Machine Learning, pp. 1675-1685, 2019.
Weinan E, Chao Ma, and Lei Wu. Barron spaces and the compositional function spaces for neural network
models. arXiv preprint arXiv:1906.08039, 2019.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized two-layers neural
networks in high dimension, 2019. arXiv preprint arXiv:1904.12191.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural networks
outperform kernel methods? arXiv preprint arXiv:2006.13409, 2020.
Tuomas Haarnoja, Sehoon Ha, Aurick Zhou, Jie Tan, George Tucker, and Sergey Levine. Learning to walk via
deep reinforcement learning. arXiv preprint arXiv:1812.11103, 2018a.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum
entropy deep reinforcement learning with a stochastic actor. In International Conference on Machine Learning,
pp. 1861-1870, 2018b.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural Networks, 4(2):251 - 257,
1991. ISSN 0893-6080.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in
neural networks. In Advances in neural information processing systems, pp. 8571-8580, 2018.
Adel Javanmard, Marco Mondelli, and Andrea Montanari. Analysis of a two-layer neural network via displace-
ment convexity. arXiv preprint arXiv:1901.01375, 2019.
Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In Proceedings
of the Nineteenth International Conference on Machine Learning, pp. 267-274. Morgan Kaufmann Publishers
Inc., 2002.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and
Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. In
Advances in neural information processing systems, pp. 8570-8581, 2019.
Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence rates of
softmax policy gradient methods. arXiv preprint arXiv:2005.06392, 2020.
10
Published as a conference paper at ICLR 2021
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layer
neural networks. Proceedings of the National A,cαdemy of Sciences, 115(33):E7665-E7671, 2018. doi:
10.1073/pnas.1806579115.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and
Martin Riedmiller. Playing Atari with deep reinforcement learning. In NIPS Deep Learning Workshop. 2013.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex
Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik,
Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.
Human-level control through deep reinforcement learning. Nature, (7540):529-533, 02 2015.
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between value
and policy based reinforcement learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30.
Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
facf9f743b083008a894eee7baa16469-Paper.pdf.
Phan-Minh Nguyen and Huy Tuan Pham. A rigorous framework for the mean field limit of multilayer neural
networks. arXiv preprint arXiv:2001.11443, 2020.
Samet Oymak and Mahdi Soltanolkotabi. Towards moderate overparameterization: global convergence guaran-
tees for training shallow neural networks. IEEE Journal on Selected Areas in Information Theory, 2020.
Grant Rotskoff and Eric Vanden-Eijnden. Parameters as interacting particles: long time convergence and
asymptotic error scaling of neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 7146-7155.
Curran Associates, Inc., 2018.
Grant Rotskoff, Samy Jelassi, Joan Bruna, and Eric Vanden-Eijnden. Neuron birth-death dynamics accelerates
gradient descent and converges asymptotically. In International Conference on Machine Learning, pp.
5508-5517, 2019.
Filippo Santambrogio. {Euclidean, metric, and Wasserstein} gradient flows: an overview. Bulletin of Mathemat-
ical Sciences, 7(1):87-154, 2017.
David Silver, Aja Huang, Christopher J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian
Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe,
John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu,
Thore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search.
2016.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas
Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre,
George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of Go without human
knowledge. Nature, 550:354, October 2017.
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc
Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis
Hassabis. A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play.
Science, 362(6419):1140-1144, 2018.
R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, second
edition, 2018.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for
reinforcement learning with function approximation. In Advances in neural information processing systems,
pp. 1057-1063, 2000.
Alain-Sol Sznitman. Topics in propagation of chaos. In Ecole d'ete de probabilit´s de Saint-FlOurXIX—1989,
pp. 165-251. Springer, 1991.
11
Published as a conference paper at ICLR 2021
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, JUnyoUng Chung,
David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using
multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.
Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods: Global optimality
and rates of convergence. arXiv preprint arXiv:1909.01150, 2019.
C.	Wei, J. D. Lee, Q. Liu, and T. Ma. On the margin theory of feedforward neural networks, 2018. preprint,
arXiv:1810.05369.
Ronald J Williams and Jing Peng. Function optimization using connectionist reinforcement learning algorithms.
Connection Science, 3(3):241-268, 1991.
Stephan Wojtowytsch. On the convergence of gradient descent training for two-layer relu-networks in the mean
field regime. arXiv preprint arXiv:2005.13530, 2020.
Kaiqing Zhang, Zhuoran Yang, and Tamer Bayar. Multi-agent reinforcement learning: A selective overview of
theories and algorithms. arXiv preprint arXiv:1911.10635, 2019.
Yufeng Zhang, Qi Cai, Zhuoran Yang, Yongxin Chen, and Zhaoran Wang. Can temporal-difference and
q-learning learn representation? a mean-field theory. arXiv preprint arXiv:2006.04761, 2020.
D.	Zou, Y. Cao, D. Zhou, and Q. Gu. Stochastic gradient descent optimizes over-parametererized deep ReLU
networks, 2018. preprint, arXiv:1811.08888.
12
Published as a conference paper at ICLR 2021
A Derivation of softmax policy gradient dynamics
Lemma A.1. The gradient of the entropy-regularized value function can be written as
Es 〜%o [VwVn(S)]
=Es〜%π ,a〜πw [Vw log πw(S, a) (Qn (S, a) - τlogπw(S,a))] ,
and thus the policy gradient dynamics (5).
Proof. We choose throughout ∏ as the LebesgUe measure, and use that ∏w is absolutely continuous Wrt ∏.
Taking the gradient of (1) using a parametric policy πw we obtain
∞
VwVnw (S) = VwEnw EYt (r(st,at, st+ι) - tDkl(∏w(st, ∙); ∏( ∙)))卜0 = S
t=0
Vw
S×A
r(s,a1,s1) 一 T log 开：(：；1)+ YVnw (sι)) P(s,aι, dsι)∏w (s, daι)
/
S×A
πw(S, a1)
r(s,a1,s1) ― T log—；~~+- + YVnw (si) P (s,aι, dsι)Vw πw (s, daι)
.	π(aι)	J
+	-τVw log
S×A
Now, since S P(s, a, ds0) = 1 and A πw(s, da0) =
the last line
⅛⅛? + YVw咚w(SI)) P(S, a1,ds1)πwGda1(A.1)
1 for all s, a, w we have for the first term in brackets in
/	Vw	log	“：((：；1，P(s,aι, dsι)∏w(s, dai)	= /	Vwπw(s,ai)dai	=	Vw /	πw(s,ai)dai	= 0
On the other hand, we can rewrite the second term in brackets as Enw [YVVnw (s1)|s0 = s], and recognize the
LHS of (A.1) evaluated at the next state s1 in the expectation. Therefore, we can sequentially repeat the same
computation as above, and recalling the definition of %πw (∙) and Q∏ (s, a) in (6) and (2) we obtain
VwVnw (s) = (X Yt (r(st)at,st+ι) — T log 穴；：；)+ YVnw (st+i)) Vwπw(st, dat)1 S
∞
Xγt
t=0
(st,at) 一 τ log πw∏Sa,[t)) Vw logπw(st,at)卜0 = S
π (s a)
(st,at) 一 τ log ∏(a)	V Vw logπw(s,a) % w (ds)πw(s, da),
/
S×A
where in the second line we have used that, if ∏w > 0 on A, ∏w (st, dat)Vw log ∏w (st, at) = Rw∏w (st, dat).
□
Proposition 2.1. For a fixed initial distribution %0, the dynamics (10) is the Wasserstein gradient flow of the
energy functional
E [ν ] = ES0 〜%0 [V∏ν (S0)].
Proof. We find the potential of the gradient flow by functional differentiation of E :
-y-E[ν](ω) = j g(s,a)^∏ν(s, a; ω)dsda
δν	A δπ δν
and consider the two terms in the integral separately, starting from the second:
δπν	δ	eτ R ψ(s，a;ω)ν(dω)
方沙(s,a; ω) = δν R人 eτRψ(s,aM)V(dωMa
(A.2)
1
δ
eτ R ψ(S,a；s)v(ds)
13
Published as a conference paper at ICLR 2021
—
eτ R ψ(s,aMν(d5	/ JLeT R ψ(s,aMν(dω)da
(R4eτR3(s,a；s)v(ds)da)2 JA δν
= τ ψ(s, a; ω) —	ψ(s, a0; ω)πν (s, da0) πν (s, a)
For the first term in the integrand of (A.2), we use π(s, a) as a density and obtain
(A.3)
δE (s,a) = δ-
δπ	δπ
/	(r(s, a) — T log π(s, a))(%∏(ds)π(s, da)) (s, a)
S×A
L k r"(s,a)-T (Iog π(s, a)+I)) %π(s0,S)
+ I	(r(s",α'r) — τ log π(s00,a00))
S ×A
δ%π(s0,s00)
δπ
(s, a)π(s00, da00)ds00
%0
(A.4)
(ds0),(A.5)
where in the last line We have used that δ∏[∏ log ∏](s, a) = logπ(s, a) + 1.
We evaluate the variational derivative of %π as
γ
∞
XγtPπt(s000,s00
t=0
S0, S00) (S, a)
)+P∏ (s0,s000 ) δ∏X Yt δ∏p∏(
s000, s00)ds000
δ(s, s0)P(s0, a, s000)%π(s000, s00) + Pπ(s0, s000)
S
t=0
δ%π *s'(s,Z
δπ
We further recognize the same derivative on the rhs of the above expression, allowing to write
δ%π(s0,s00)
δπ
∞
(S,a)=X γtPπt(S0,S)	P(
S, a, S000)%π(S000, S00)dS000
Finally, we notice that the last term in (A.4) is constant in a and therefore vanishes when integrated against
(A.3), so we only consider
δE (s,a) — %π (s)= I	(r(s"
δπ	S2 ×A
a00) — T log π(S00, a00)) ×
X (δs,s00δα,a00 %π (s0,s) + ∏(s00,a00) '%	) (s,a)) %o(ds0)ds00da00
∞
/ X YtP∏(s0,s)(尸(s00, a00) — Tlog∏(s00,a00)) ×
S2×At=0
×	δs,s00 δa,a00 + Y P(S, a, S000)%π(S000, S00)dS000 %0(dS0)dS00da00
∞
XYt	Pπt (S0, S) (Qπ(S, a) — Tlogπ(S, a)) %0(dS0)
=(Qπ (s,a) — T log ∏(s,a)) %π(s)	(A.6)
We conclude by noting that combining (A.3) and (A.6) we obtain C∏ [ψ(ω), Q∏ —τ log ∏](s), and the Wasserstein
gradient flow corresponding to this potential is (10).	□
B	Proofs of the many-particle limit
Theorem 4.1. LetAssumption 1 hold and let W(N) be a solution of(5) with initial condition w0N) ∈ W = Ωn.
If VON) converges to ν° ∈ P2(Ω) in Wasserstein distance W2 then VtN) converges, for every t > 0, to the unique
solution νt of (10).
14
Published as a conference paper at ICLR 2021
Proof. As anticipated in the main text, the proof is divided in two parts:
1.	We prove sufficient regularity of the dynamics (10), allowing to establish existence and uniqueness of
its solution.
2.	We leverage the regularity proven above to establish a propagation of chaos result, showing that the
system of interacting particles behaves asymptotically as its mean-field limit.
While carrying out this proof is needed in our context since the dependence of E (ν) on ν is more involved than
in e.g., Mei et al. (2018); Rotskoff & Vanden-Eijnden (2018); Chizat & Bach (2018), the steps of this derivation
are mainly standard, see e.g., (Sznitman, 1991).
B.1 Regularity
We prove existence and uniqueness of the gradient flow dynamics (10) through standard arguments from the
optimal transportation literature (see e.g., (Ambrosio et al., 2008)). More specifically, recalling that π = πν we
leverage the Lipschitz continuity of the vector field
Ft(ω,ν) = / C∏ [Vωψ(s, ∙； ω),Q∏(s, ∙)
-T logπ(s, •)] %∏(ds)
with respect to ν. To prove such regularity result, decompose
E(ν)
for	R(f) = S ◦ π(f) .
(B.1)
and S : (S→M+(A)) → R maps μ → Eμ [Vμ(s)∣s 〜％o] and ∏ : L2(SxA) → (S → M：(A)) is the
softmax policy parametrization (8) of its argument. Recalling the definition of Qr from Assumption 1 and
denoting Fr = { ψν : supp ν ∈ Qr}, we further define the norms and constants needed in the following
proof as:
kDψkr,∞ = sup kDψω k
kDRkr,∞ =	sup kDRψk
ψ(∙M : ω∈Qr
LDψ = sup
ω,ω0∈Qr
LDR = sup
ψ,ψ0∈Fr
∣∣Dψω - Dψω0 k
∣∣ω - ω0k2
∣∣drψ - DRψ0 k
kψ — ψ0k2
where k ∙ k denotes the operator norm. While for any r > 0 the boundedness of kDψkr,∞, L0ψ results
directly from Assumption 1, more work is needed to prove that ∣DR∣r,∞ < ∞, LDR < ∞. We prove this in
Lemma C.5 below, and proceed with the proof of convergence of the particle dynamics.
For any r and corresponding Qr from Assumption 1 we define the set of localized functionals
E(r) (ν) = E(ν)
∞
if supp (ν) ⊂ Qr
else
Furthermore, We say that a coupling Y ∈ M∣(Ω × Ω) is an admissible transport plan if both its marginals
have support in Qr and finite second moments. To every admissible transport plan, forp ≥ 1 we associate a
transportation cost Cp(Y) = (JQ2 ∣ω — ω0∣pdγ(ω, ω0))Vp.
We prove the following results: for every r > 0 we have
1.	There exists λr > 0 such that for any admissible transport plan Y, defining the interpolation map
VY := (t∏o + (1 - t)ni)#Y, the function t → E(VY) is differentiable with Lipschitz continuous
derivative with constant λrC22 (Y).
2.	Let V0 have support in Qr . Then for any given transport plan Y with first marginal given by V0, a
velocity field F satisfies
E((m)#Y) ≥
E(V0) +
F(U) ∙ (u —
u0)dY(u, u0) + o(C2 (Y))
(B.2)
if and only if F(u) is in the subdifferential of DEν (u) := Cπ[ψ, Qπ - log π](u) for g ∈ L2(S × A)
(projected in the interior of Qr when U ∈ ∂Qr ) for νo almost every U ∈ Ω.
15
Published as a conference paper at ICLR 2021
The proof of the two points above corresponds to (Chizat & Bach, 2018, Lemma B.2). We sketch the proof of
these two points below, referring to the original reference for the details.
1.	By the LiPschitz continuity of ψ : Ω → L2 (S × A) and R0(f) = DRf = Cn [∙, Qn - T log π] in Qr,
the energy E(r) (νtγ) transported along an interpolating path νtγ is differentiable and we can write its
derivative as
djt E (r)(VY ) = Z R(Z ψνtγ )/Dψ(1-t)ω+tω0 (ω0 - ω)dY(ω,ω0)
Then, again by the LiPschitz continuity of Dψ and DR we have for 0 ≤ t0 < t00 < 1
ddtEE(VY) - djtE(r)(νYo) (R(Z ψνY) - R(Z ψνYo))/ Dψ(i-t0)ω+t0ω0Q - ω)dγ(ω,ω0)
+ R0( ψνtγ00)	(Dψ(1-t0)ω+t0ω0 - Dψ(1-t00)ω+t00ω0)(ω0 - ω)dγ(ω, ω0)
≤ λr |t00 - t0 |
for a λr large enough, where in the last inequality we have used the uniform bounds on DR in Qr , that
∣Dψ(i-to)ω+toω0 - Dψ(i-to0)ω+t00ω01 ≤ (t0 - t00)LDψ ∣ω - ω0∣ and we applied Holder,s inequality to
bound C12 (γ) ≤ C22 (γ).
2.	The proof of this result leverages an expansion of the functionals R, ψ to the second order in their
arguments:
ψ(ω0) = ψ(ω) + Dψω (ω0 - ω) + Rψ(ω, ω0)
R(g) =R(f)+DRf(g-f)+RR(f,g)
Recalling the Lipschitz bounds on the remainders Rψ(ω,ω0) < 2L0ψ∣ω - ω0∣2, Rκ(f,g) <
2LDRkf - gk2 and combining the two expansions above We have, for a transport plan Y with
marginals νo = (口。)#)，νι = (口。#Y
E(r) (ν1) = E(r) (ν0) + R0(	ψν0)Dψu(u0 - u)dγ(u, u0)dsda + R
for a remainder term R. We can bound such remainder term, again by the Lipschitz regularity of
Dψ and DR and by the boundedness of Dψ, DR in Qr, by C2(Y)2 and C1(Y)2 ≤ C2(Y)2, thereby
obtaining that
E(r)(ν1)
E(r) (ν0) + R0(	ψν0)Dψu(u0
- u) dsda dY(u, u0) + o(C2 (Y))
Noting that the integrand against the coupling is the gradient flow vector field, the above uniquely
characterizes the velocity field satisfying (B.2).
We note that point 1) above immediately implies that E(∙) is λr-semiconvex along geodesics, while by 2) E(r) (∙)
admits strong Wasserstein subdifferentials on its domain (Ambrosio et al., 2008, Definition 10.3.1). Combining
the two results one obtains existence and uniqueness of the solutions of the Wasserstein gradient flow through
(Ambrosio et al., 2008, Theorem 11.2.1).
B.2 Propagation of chaos
By the Lipschitz continuity of the transport field in (10) in ν0 with supp ν0 ⊂ Qr , there exists a time tr > 0
such that, supp νs(N) ⊂ Qr for all s ∈ [0,tr], N ∈ N. Consider now two times 0 ≤ t1 < t2 ≤ tr. To prove the
existence of the limiting curve (νt)t, we show that the curves νt(N) are uniformly in N equicontinuous in W2
and as such, possess a converging subsequence by Arzela-Ascoli theorem. To show equicontinuity, we bound
the the W2 distance between distributions by coupling positions of the same particles at different times and
using Cauchy-Schwartz inequality:
N	N	t2
(N)	(N)	2	1	(i)	(i)	2	t2	- t1	d	(i)	2
W2(νtι ,νt2 ) ≤ N UwtI - wt2 k2 ≤	X	k dSw()k2ds
i=1	i=1 t1	s
16
Published as a conference paper at ICLR 2021
Combining the above with the identity
NN
d E((N))ʌ — 1 X∕v E(小N八 d,“g — 1 Xlld,z,(i)∣∣2
dtE(Vt	) = NXEwE(Vt	),dtwt i = NLkHtwtk2
i=1	i=1
we have
W2(ν(N ),ν(N)) ≤	√t2	- tι JZ	d- E (VsN ))ds	≤	√t2	- tι ( sup	E (V)- inf	E (V))
1	2	t1 ds	supp ν∈Qr	supp ν∈Qr
where We recall that Fr = {v ∈ M∣(Ω) : SuPP V ⊂ Qr}. In particular, the above continuity bound is
independent on N, proving equicontinuity of V(N) in W2.
We now prove that the limiting point of the converging subsequence whose existence was identified above must
solve (10). To do so we compare both the differential for the mean-field and particle dynamics to the one of a
linear inhomogneous PDE: for any bounded and continuous f : R × Rm → Rm, denoting by E = VtFtdt and
EN = Vt(N)Ft(N)dt where Ft, Ft(N)are the vector fields of the mean-field and particle system respectively, we
write
f(ω)d(E-EN)	≤ kfk∞	Ft(N)-Ft dVt(N)dt+	fFtd(Vt(N)-Vt)dt.
By boundedness of fFt over Qr, the second term converges by our choice of subsequence. For the first term,
denoting throughout by k∙k bl the bounded Lipschitz norm, We leverage the Lipschitz continuity of the vector
field F WRT the underlying parametric measure:
kFt(N)-Ftk2 ≤CrkVt-Vt(N)kBL
for Cr > 0 large enough, again obtaining convergence by our choice of subsequence. This proves convergence
of the particle model to the mean-field equation (10) on [0, tr]. To extend the time interval on Which We prove
convergence, We use that E(V) (and thus R) decays along trajectories of (10). Consequently, by the boundedness
of the differential DR on sublevel sets of R the Lipschitz constant of E(Vt) is uniformly bounded. Using again
the Lipschitz continuity of F We can shoW that suPu∈Qr kF k < A + Br, i.e., that particle velocities can groW
at most linearly in r, and an application of GronWall alloWs to find, for every T > 0 that there exists r > 0 such
that SuPP Vt ∈ Qr for all t ∈ [0, T] and propagation of chaos follows.	口
C	Proofs of optimality
Theorem 4.2. LetAssumption 1 hold and Vt given by (10) converge to V * ,then ∏ν* = ∏* SX A -a.e.
Before proceeding to prove Theorem 4.2, we state the alternative form of Assumption 1 a) in the case where
Θ 6= Rm-1 . Our proof of the theorem above can be easily generalized to the setting of Assumption 2.
Assumption 2. Assume that ω = (ω0, ω) ∈ R × Θ for Θ ⊂ RmT which is the closure of a bounded open
convex set. Furthermore ψ(s, a; ω) = ω0φ(s, a; ω) where φ is bounded, differentiable and Dφω is Lipschitz.
Also, for all f ∈ L2 (S × A) the regular values of the map ω → gf (ω) := f f (s, a)φ(s, a; ω)dads are dense
in its range and gf (ω) satisfies Neumann boundary conditions (i.e.,for all ω ∈ ∂Θ we have dgf (ω)(nω) = 0
where nω ∈ RmTiS the normal of ∂Θ at ω).
We prove Theorem 4.2 as sketched in Section 4 by first connecting the optimality and the support of stationary
measures in parametric space through Lemma C.1, and then investigating how the dynamics preserves full
support property for any t > 0 in Lemma C.2 and avoids spurious minima in Lemma C.3.
Before starting this program we introduce the equivalent of greedy policies in the entropy-regularized setting.
For a given Q(s, a), the associated Boltzmannpolicy ∏b with respect to a reference measure ∏ is given by
∏B (s, a)：= exp[(Q(s,a) - VQ(S))/τ ]	for	VQ(s) := T log Εa~∏ [exp [Q(s, a)∕τ ]],
and satisfies
∏B(s, ∙) = argmax∏∈M+(A) (Ea~∏ [Q(s,a)] — TDKL(∏;∏)(s))
One can then define the Boltzmann backup or soft Bellman backup operator Tτ that, for a given Q and the
associated Boltzmann policy πB, gives the action-value function Tτ Q associated to πB:
TTQ(s,a) = r(s, a) + YTE∏ [logEa-∏ [exp Q(s1,a1)∕τ] |so = s]	(C.1)
17
Published as a conference paper at ICLR 2021
It is known (Haarnoja et al., 2018b, Theorem 1) that the fixed points of the above operator are optimal, i.e., they
correspond to the optimal policy ∏B = π* of the entropy-regularized MDP.
To state the first partial result towards the proof of Theorem 4.2, we observe that the ω0-component of the
transport vector field in (10) can be written as
C C∏ν [Vω ψ(s, ∙; ω),Q∏ν(s, ∙) - T log n“] %∏ν(ds)) = / /“ [φ(s, ∙； ω),Q∏ν (s, ∙) - T log n“] %∏ν(ds),
S	0S
(C.2)
where we recall thatCπ[f, g] is the covariance operator WRT the probability measure π(s, da) introduced below
(10). We note in particular that the above expression only depends on ω.
With the above information at hand we now proceed to prove Lemma C.1 relating the value of (C.2) and the
optimality of fixed points of (10):
Lemma C.1. Let Assumption 1 hold and let ν satisfy
/	φ(ω; s,a)(QnV (s,a) - T log∏(s,a) - VnV (S)) ∏ν(s, da)%∏ν (ds) = 0 ,
S×A
(C.3)
ω-almost everywhere in Θ. Then we have that QnV = Q∏* holds π%∏-a.e. in S ×A.
ProofofLemma C.1. Assuming that (C.3) holds Lebesgue-a.e. in Θ, by the assumed continuity of φ in ω
combined with the expressivity of φ Assumption 1 b) we must have that
QnV (s, a) - Tlogπν(s,a) - VnV (s) = 0	πν %nV - a.e. .
We then rewrite the above condition in compact notation as the fixed point equation
TτQnV(s,a) = QnV (s, a)
for the soft Q learning or Boltzmann backup operator Tτ defined in (C.1). Since all fixed points of Tτ for γ < 1
are optimal (Nachum et al., 2017, Theorem 3), We must have that ∏ν = ∏b [Q*] = ∏ for ∏ν%∏ν -almost every
(s, a) ∈ S ×A. The result follows by equivalence of ∏ν and ∏.	□
Consequently, suboptimal fixed points of the dynamics (10) cannot satisfy (C.3) Lebesgue-a.e. in Θ.
C.1 Proof of Theorem 4.2
We prove below that spurious local minima that do not satisfy (C.3) Θ-a.e. are avoided by the dynamics. We do
so by leveraging the approximate gradient structure of the policy gradient vector field when νt is close to one of
such stationary points, as discussed in the main text. Combining this fact with the assumed convergence to V*
proves Theorem 4.2.
We note that by the assumed homogeneity of ψ in its first component, if ν, ν0 are such that
J ω0ν(dωo, dω) = J ωoν0(dωo, dω) a.e.
(C.4)
then
fν ()=/
ωoφ( ∙; ω)ν(dωo, dω)
/ ωoφ(∙; ω)ν0(dωo, dω)
fν0(∙),
so that in turn we have πν = πν0 a.e.. In other words, the homogeneity of the chosen class of approximators
results in a degeneracy of the map ψ : M∣(Ω) → L2 (S × A). To remove this degeneracy in our analysis, we
identify all the distributions ν, ν0 that are equivalent under (C.4) by defining the signed measure
hV(dω) := / ωoν(dωo, dω)
(C.5)
Leveraging this definition, we prove the desired result Theorem 4.2 in two key steps: we show that
1.	the solution to (10) does not lose (projected) support for any finite time, thereby preserving the property
from Assumption 1 c),
2.	stationary points V with QnV = Q∏* 一 which by Lemma C.1 cannot have full projected support in Θ 一
are avoided by the dynamics.
18
Published as a conference paper at ICLR 2021
These facts are respectively summarized in the following lemmas:
Lemma C.2. Let Assumption 1 a) hold and let ν0 satisfy Assumption 1 c), then for every t > 0, νt solving (10)
with initial condition ν0 also satisfies Assumption 1 c).
Throughout, Weletk ∙ ∣∣bl denote the bounded LiPschitz norm.
Lemma C.3. LetAssumPtion 1 hold and let V be a fixed point of(10) such that (C.3) does not hold a.e.. Then
there exists ε > 0 such thatif ∣∣h- 一 媪打 ∣∣bl < ε fora tι > 0 there exists t2 > tι such that ∣hV 一 h^ ∣∣bl > ε.
Proof of Lemma C.2. Analogously to (Chizat & Bach, 2018, Lemma C.13), We aim to shoW that the seParation
ProPerty AssumPtion 1 c) is Preserved by the evolution of ν0 along the characteristic curves X(t, u) solving
∂tX(t, u) = Ft (X (t, u); νt),	(C.6)
Where Ft is the transPort field in (10). To reach this conclusion, the analogous result in Chizat & Bach (2018)
only relies on the continuity of the maP u 7→ X(t, u), established in (Chizat & Bach, 2018, Lemma B.4) under
Assumption 1 a). Hence, it is sufficient for our purposes to establish continuity of the map X (t, ∙) from (C.6).
This ProPerty, hoWever results immediately from the one-sided LiPschitz continuity of the vector field Ft on
Qr = [-r, r] X Θ uniformly on compact time intervals, which is in turn guaranteed by the Lipschitz continuity
and Lipschitz smoothness of ψ from Assumption 1 and boundedness of r.	□
To simplify the notation in the following proof, we denote throughout
δ(ν) := Qπν 一 τlogπν 一 Vπν
and
hf, giπ
/
S×A
f(s, a)g(s, a)π(s, da)%π(ds) .
ProofofLemma C.3. We first claim that by Lemma C.1, for any spurious fixed point (such that QnV =
Q∏*), there must exist a subset of Θ with positive Lebesgue measure where V loses support and such that
hVψ, δ(V)i∏ = 0. This is easily proven by contradiction: if hVψ, δ(V)i∏ = 0 a.e. then by Lemma C.1 we have
that QnV = Q∏ *. This implies that the quantity
gν(ω) ：= h∂ωoψ(∙;ω),δ(V)i∏ν = hΨ(∙"i,ω)),δ(V)i∏ν = hφ(∙;ω),δ(ν)i∏ν	(C.7)
cannot vanish a.e. on Θ. Then, by Assumption 1 on the regularity of g, there exists a nonzero regular value -n of
gν(ω) Assuming without loss of generality that this regular value is negative, so that n > 0 (else invert the signs
of ωo in the remainder of the proof), we define the nonempty sublevel set G := {(ω0, ω) ∈ Ω : gν(ω) < -n}
and
G+ = {(ω0,ω) ∈ G : ω0 > 0} .	(C.8)
Further denoting by G ⊆ Θ the projection of G onto Θ, we have by definition that the gradient field of gν (ω) is
orthogonal to the level set ∂G, the latter being an orientable manifold of dimension m 一 2. Denoting by nω the
normal unit vector to ∂G in the outward direction, by continuity of Vgp(ω) when G is compact1 we can bound
the scalar product between the two away from 0, i.e., there exists
β ：= min nω ∙ Vωgp(ω) > 0 .
ω ∈∂G
We now prove that the stationarity assumption in a ε-neighborhood of a spurious fixed point
I∣hp 一 hVt∣BL < ε for all t > tι,	(C.9)
leads to a contradiction for ε small enough. To do so, by Lemma C.5 we set ε(α, η, β) small enough so that
for all Vt such that (C.9) holds we have gνt (ω) < -n/2 on G and nω ∙ Vωgνt > β∕2 on ∂G. Then, the
two inequalities above combined with ∂ω0 ψ(ωo, ω) = ψ(1,ω) imply that the set G+ defined above is forward
invariant and therefore that ∂tνt(G+) ≥ 0 as long as (C.9) holds. Furthermore, by similar arguments we notice
that characteristic trajectories cannot enter the set G \ G+ after t1.
Now, we consider two cases: either (i) a positive amount of mass is present at t1 in the forward invariant set G+
(νt1 (G+) > 0) or (ii) νt1 (G+) = 0. We discuss these two cases separately, along the lines of (Chizat & Bach,
2018, Lemmas C.4, C.18), respectively.
1if Q is not compact we choose η to also be a regular value of the function on {ω ∈ Rm-1 : ∣∣ω∣∣2 = 1} to which g
converges as ω goes to infinity.
19
Published as a conference paper at ICLR 2021
(i)	Assume that νt1 (G+) > 0. We note that under our assumptions the first component of the velocity field in
G is lower bounded by η∕2, so that ω0(t) = ω0(0) + tn/2 bounds from below the ωo-component of the
trajectory of a test mass with initial condition with ω(0) ∈ G, as long as ω(t) ∈ G. Combining this bound
with the forward invariance of G+, we see that if ω(0) ∈ G+ then ω0(t) > tn/2. Consequently, assuming
that supp(νt) ⊂ (-M, M) × Θ for every t > t1 we have
hVt(G) ≥ n∕2(t - t1)νt1 (G+) + min{0, (t - t1)n∕2 - M}% (G \ G+).
This implies linear growth of h3<G) for t > tι + 2M∕n, contradicting the original assumption that
∣∣hp - hVtl kBL < ε for all t > t1.
(ii)	Consider now the complementary case νt1 (G+) = 0. We proceed to show that there exists t2 > t1 such
that %2 (G+) > 0, thereby reducing this case at time t? to part (i). To do so, we consider ω* ∈ SUPP(Vt1)
such that ω* ∈ G is a local minimum of gν, i.e., for which Vgp = 0 (which exists^by the preservation of
the support property Assumption 1 c)). Then, choosing ε such that Bε(ω*) ⊂ G, and setting M large
enough that supp (νt1 ) ⊆ [-M, M] × Θ, we prove below in Lemma C.4 that there exists t2 > t1 for
which the image at t2 of ω(t1) := ω* under the characteristic flow (C.6) is contained in G+ . By continuity
of the flow map X(∙,t), this conclusion extends to a neighborhood of ω*, with positive mass under ν门.
□
We denote throughout by ∣∣ ∙ ∣∣ci the maximum of the supremum norm of a function and the supremum norm of
its gradient and recall the structure of the policy gradient vector field
Ft(ω,νt) = -Vhωoφ(ω),δ(νt)i∏ν = -V(ω° gνt (ω))	(C.10)
where g is defined in (C.7) and νt solves (10).
With these definitions, we proceed to prove that case (ii) in the analysis above will ultimately reduce to case (i)
for t large enough.
Lemma C.4. Let V ∈ M∣(Ω) and ω* satisfy |Vgp(ω*)| = 0, gν(ω*) < —n < 0 for some n > 0. Thenfor
every ε, M > 0 there exists t2, ε > 0 such that iffor all t ∈ (0,t2) we have ∣∣gp 一gνt kci < ε andω0 ∈ [—M, 0],
then the point ω* is mapped, under the flow of the policy gradient vector field (C.10) at time t2 to a subset of
Bε((1,ω *)).
Proof of Lemma C.4. By homogeneity of the approximator, we can bound the first component of the velocity of
a particle (ωo(t),ω(t)) under (C.10) with initial condition ω(0) = ω* as
dtω0(t) = -gνt (ω(t)) ≥ -gν(ω*) - IgP@(t)) - gν(ω*)| - IgVt a(t)) - gP@(t))|
In the other directions, defining q(t) := ∣∣ω(t) 一 ω*∣, we have
dtq(t) ≤ ∣ωo(t)∣∣Vω gνt (ω(t))∣
≤ ∣ωo(t)∣ [∣Vωgν(ω*)∣ + ∣Vωgp(ω(t)) - Vωgν(ω*)∣ + ∣Vωgνt(ω(t)) - Vωgp(ω(t))∣]
for all t ∈ [0, τ] where T := inf{t : ω0(t) ∈ [-M, 1]}. Moreover, Lipschitz continuity of the potential gν( ∙)
and its Lipschitz smoothness imply the existence of a L > 0 such that max{∣gp(ω) — gν(ω*)∣, ∣Vgp(ω) 一
Vgν(ω*)∣} ≤ L∣ω - ω*∣. Combining this with the assumed convergence of Vt to ν, which implies ∣∣gp -
gνt IIci < ε, we can bound the evolution of (ω0(t), q(t)) for t ∈ [0, τ] in the perturbative regime of interest as
follows:
dtωo(t) ≥ n - ε - Lq(t)
dtq(t) ≤ lωo(t)1 [ε + Lq(t)]
(C.11)
(C.12)
We now show that, choosing both ε and a neighborhood around ω* = (ω*,ω*) to be small enough, the forward
dynamics of ω* will reach the set {ω0 > 0} before q(t) can increase too much. More precisely, by possibly
increasing the value of L such that n∕4L < ε, and defining Tq = inf{t : q(t) > n∕4L} we prove that there
20
Published as a conference paper at ICLR 2021
exists ε ∈ (0, η∕4) such that Tq > τ,i.e., that the trajectory ω* reaches G+ before q(t) > ε. Note that as long as
t ∈ [0, Tq] and ε ∈ (0, η∕4) the negative terms on the RHS (C.11) can be bounded from below, and We have
ω0(t) ≥ ω0(O) + 2t
so that ωo(t) > ω0(0) ≥ -M. Consequently, for all t ∈ [0,T ∧ Tq] we bound the rhs of (C.12) as 奈q(t) <
Mε + LMq(t). Using that q(0) = 0 and Gronwall inequality we can bound the total excursion in the ω
component q(t) ≤ εMt exp [LMt]. Finally, setting to := 2(M + 1)∕η ≥ -2(ω0(0) - 1)∕η > T so that
ωo(τo) > 1 we are still free to set ε small enough such that Tq > t。> T. Indeed, by monotonicity of the
upper-bound on q(t) we have
q(T0) ≤ 2ε(M + 1)M∕ηexp [2LM (M + 1))∕η] ≤ η∕4L ,
so that setting ε ∈ (0,η∕4) concludes the proof.	□
We now proceed to show the needed regularity of the potential gν from (C.7) in terms of the signed measure h1ν
defined in (C.5). In doing so, we also prove Lipschitz smoothness of the operator R defined in (B.1):
Lemma C.5. For any r > 0, the operator R on Fr = { ψν : supp ν ∈ Qr} is Lipschitz smooth, and DRf
is bounded in the supremum norm. Furthermore, for all C0 > 0 there exists α > 0 and ε > 0 such that for all
ν, ν0 satisfying khν1 kBL, kh1ν0kBL < C0, kh1ν - h1ν0 kBL < ε, one has
kgν - gν0 kC1 ≤ αkhν1 - hν10 kBL .	(C.13)
To prove the above lemma, we first bound some relevant quantities. Throughout, by slight abuse of notation, we
denote for any function f : S × A → R
kfk2
sup	f(s, a)2da
s∈S A
Lemma C.6. For all f, f0 ∈ Fr there exists ε > 0, C0, C00, C000 > 0 such that if kf - f0k2 < ε one has
kπν-πν0k2 ≤ C0kf - f0k2	(C.14)
k%ν-%ν0k1 ≤ C00kf - f0k2	(C.15)
kQπν - Qπν0 k2 ≤ C000kf - f0k2	(C.16)
Proof of Lemma C.6. Throughout this proof, for simplicity of notation, we will write π = πν and π0 = πν0 .
Furthermore, we use that for f ∈ Fr there exists C0 > 0 so that
e-C0kφkC1 ≤ exp[f (s, a)] ≤ eC0kφkC1 ,	(C.17)
implying, together with the assumed absolute continuity of %0 that kQπk∞, kπk∞, k%π k∞ < ∞. Setting
throughout T = 1 to simplify the notation and combining the above with the pointwise upper bound ex <
1 + Kr |x| for |x| < eC0kφkC1 we obtain
kπ - π0k2 ≤
≤
exp[f (s, a)]	exp[f0(s, a)]
-T；---: :----------- - —7；----： :-----------
exp[f (s, a0)]da0	exp[f 0 (s, a0)]da0 2
exp[f(s,a)]	-exp[f'(s, a)]	+ 2	exp[f'(s, a)]	-	exp[f'(s,a)]
exp[f (s, a0)]da0	exp[f (s, a0)]da0		exp[f (s, a0)]da0	R exp[f0(s, a0)]da0 2
≤
exp[f'(s, a)]	k1 - exp[f (s, a) - f0(s, a)]k2 + ∞	exp[f (s, a0)]da0 —τ;	二 :	-		1 ∕exp[f 0(s,a0)]da0	2
exp[f (s, a0)]da0		
e2C0kφkC1	e2C0kφkC1
≤ A	Krkf (s, a) - f0(s, a)k2 + A
|A|	|A|
/ (exp[f (s, a0) - f(s, a')] - l)da0
≤ e2c0AφkC1 Kr(1 + e4C0kφkc1 )kf - f0k2 =: C0kf - f0k2,
|A|
(C.18)
21
Published as a conference paper at ICLR 2021
where we have denoted by |A| the Lebesgue measure of the action space A. We now proceed to establish
the second bound in the statement of the lemma. In this case, denoting the t-steps transition probability as
Pπt(s,dst) = St-1 Pπ(s,ds1)Pπ(s1,ds2) . . . Pπ(st-1,dst) we have
k%π - %π0 k1
∞
X Yt Js %0(ds0) (P∏(so, ds) - P∏0 (so, ds))
∞ t-1
≤XYtX%0Pπj(Pπ-Pπ0)Pπt-0j-11 .
t=1	j=0
Observing that for any smooth % ∈ M1+(S) for the operator norm of the difference in the above sum we have
k%(Pπ - Pπ0)k1 =	%(ds)	P(s, a, ds0) (π(s,da) - π0(s, da))
(1 - Y)2
≤k%kιkPkιk∏ -∏0k2 ≤ (-YLC00kf - f0k2
Y
for large enough C00, where we used (C.18) in the last line and the Lipschitz continuity of P in its second
argument, and kPk1 is the operator norm of the transition operator A P(s, a, ds0)π00(s, da) : M1+(A) →
M1+(A), which is equal to 1. From this we conclude
(1	Y)2 ∞
k%π - %π0kl ≤ (-YL X tγtC 00kf - f0k2 = C00 kf - f0k2
Y	t=1
Finally, defining for notational convenience RT := r - tDkl(π0, π) We write:
kQπ - Qπ0 k2
P (s, a, ds0) (Vπ (s0) - Vπ0 (s0))
P(s, a, ds	Rτ(s00, a0)%π (s0, ds00)π(da0) - R0τ(s00, a0)%π0 (s0, ds00)π0(da0)
γ
P (s, a, ds0) Rτ (s00, a0)%π(s0, ds00)π(da0) - R0τ (s00, a0)%π0 (s0, ds00)π0(da0)
(Rτ - R0τ) (s00, a0)%π(s0, ds00)π(da0)
R0τ (s00, a0)(%π - %π0)(s0, ds00)π(da0)
R0τ (s00, a0)%π0 (s0, ds00)(π - π0)(da0)
(C.19)
and bound each term separately letting C1000 , C2000 , C3000 > 0 be large enough constants. For the first, we have:
(Rτ - R0τ) (s00, a0)%π (s0, ds00)π(s00, da0)
where we have bounded the log term as follows:
R ef(s,a)da
k log R ef 0(s,α)da k2 = k log
R ef (s,a)
1
≤-----
—1 - Y
1
≤-----
—1 - Y
kπk2k log π - log π0k2
R ef(s,a)da
k∏k2 kf -fig + klog RfF k2
≤C1000kf-f0k2
- ef0(s,a)da
R ef0(s,a)da
+1k2
(C.20)
γ
/
Z
+
∞
1
1
2
+
∞
∞
∞
R ef 0 (s,a) e|f (s,a)-f 0 (s,a)| - 1 da
≤ k log------------ R ef0(s,α)da-------------+ 1k2
22
Published as a conference paper at ICLR 2021
≤ k log ke--k∣A∣∣∣e k∞ / (ef(s,a)-f0(SaI- 1)da +1)k2
≤ k log (ke-f ”小 k∞ Kr Z ∣f(s,a)- f0(s,a)∣da + l) ∣∣2
≤ klogke-f0k∞kef0k∞Krkf(s,a)-f0(s,a)k2+1 k2
≤ ke-f0k∞kef0k∞Krkf-f0k2	(C.21)
For the second term in (C.19), using the boundedness of kRk2 < CR and that %π - %π0 = γ (%π - %π0) for a
certain, %0 depending on s0 we write
R0τ(s00,a0)(%π - %π0)(s0, ds00)π(s00, da0)
≤ kπk2kR0τ (s00, a0)k2k%π -%π0k1
∞
≤C2000kf-f0k2.
(C.22)
We finally bound the third term by writing
R0τ(s00
a0)%π0 (s0, ds00)(π - π0)(s00, da0)	≤ k%π0 k1kR0τ (s00, a0)k2kπ - π0k2
∞
≤C3000kf-f0k2
(C.23)
and obtain (C.16) by combining (C.20)-(C.23).
Proof of Lemma C.5. We first establish the desired properties of the functional R. To do so we differentiate (8)
atf ∈ L2(S ×A)
δπf _ δ eτ R f (s,a)ν(dω) _	1 δ τf
δf s，a δf Ra eτf (s,a)da	RA eτf da δfe
eτf
(RA eτf da)2
δ ；eTfda
A δν
τ f -	fπf(s,da0) πf (s, a) .
(C.24)
Then, combining the above with (A.6) and Holder inequality We obtain
kDRf k∞,r
sup	(DRf (s, a))2dsda
f∈Fr
<∞
where we have used that all the terms appearing in DRf are bounded for every choice of r > 0.
To establish Lipschitz smoothness of R, letting f, f0 ∈ Fr and denoting, to simplify notation, π = πf and
π0 = πf0, we proceed to bound the operator norm by splitting the RHS as
∣∣DRf - DRf 0 k	≤	sup	∣C∏ [', Qn - T log π]	- C∏0 [', Q∏0	- T log π0]|
'∈L2(S×A) : k'k = 1
≤	sup	[(I) + (II) + (III)]
'∈L2(S×A) : ∣∣'k = 1
and considering the resulting terms separately.
First of all, defining throughout by slight abuse of notation δ(π) := Qπ - T logπ, we have
(I) := |h
`(s, a)π0(s, da) -
`(s, a)π(s, da), δ(π0)iπ0 |
≤	`(s, a)(π(s, a) - π0(s, a))
δ(π0)π0
%π0 (s)ds
□
≤k'k2k∏ - ∏0k2kδ(π0)π0k2k%∏0kι
≤ c k'k2 kf - f 0k2,
(C.25)
23
Published as a conference paper at ICLR 2021
where the last line was obtained using (C.14) and boundedness from above and below of π, Qπ in Fr. We
further write, for a K < ∞ large enough
(II) :
lh'(s,a) -
`(s, a0)π(s, da0), δ(π0)iπ0 -
h`(s, a) -
`(s, a0)π(s, da0), δ(π0)iπ |
≤l
δ δ(π0)'(s, a)%π(s)(π — ∏0)(s, da)ds∣ +
`(s, a)δ (π0)(%π — %π0 )(s)π0(s, da)ds|
|
≤kδ(∏0)∣∣2∣∣%∏0∣∣ιk'∣∣2(∣∣∏ — ∏0k2 + k%∏ — %∏0 kι)
≤ Kk'k2kf — f0k2,
(C.26)
where we have used Cauchy-Schwartz inequality together with (C.14) and (C.15). Finally, using (C.16) and
(C.21), we bound for K < ∞ possibly larger than above
(III) :
lh'(s,a) —
/ '(s, a0)π(s, da0), δ(π) — δ(π0)i∏ |
≤k%∏ ∏k∞k'k2kδ(∏) — δ(π0)k2
≤ k%π klk∏k2k'k2 (kQπ - Q∏0k2 + kV∏ — V∏0 k2 + T k log π0 k2)
≤ k%∏k1k∏k2k'k2 ((1 + k∏0k∞)kQ∏ - Q∏0k2 + kQ∏k2k∏ - Π0k2 + τklogj0k2)
≤ K k'k2kf - f0k2.
(C.27)
Combining (C.25), (C.26) and (C.27) we obtain that
LDR = sup
f,f 0∈Fr
kDRf - DRf ok
kf — f 0k2
<∞
proving the Lipschitz smoothness claim.
Proceeding to the proof of (C.13), combining the Lipschitz smoothness of R on bounded sets, the identity
ψ(s, a; (1,cω)) = φ(s, a; ω) and the boundedness of the set {/ ψν : V ∈ P2(Ω), |hV| < C0} We obtain
kfν — fν0 k2
ψ(∙; ω)(ν — ν0)(dω)
(C.28)
2
Φ(∙; ω)(hV — hVo)(dω)
≤
sup
'∈L2(S×A),k'k≤1
J J '(s,a)φ(s,a; ω)dsda(hV — hVo)(dω)
2
≤ kφkC1 khν1 — hν10 kBL ,
which combined with the ∣∣φkcι -Lipschitz continuity of the map J '(s, a)φ(s, a; ω)dsda and with the LiPschitz
smoothness of R concludes the proof.	口
24