Published as a conference paper at ICLR 2021
Estimating Lipschitz constants of monotone
DEEP EQUILIBRIUM MODELS
Chirag Pabbaraju" & Ezra Winston*
Carnegie Mellon University
{cpabbara, ewinston}@cs.cmu.edu
J. Zico Kolter
Carnegie Mellon University
Bosch Center for Artificial Intelligence
zkolter@cs.cmu.edu
Ab stract
Several methods have been proposed in recent years to provide bounds on the
Lipschitz constants of deep networks, which can be used to provide robustness
guarantees, generalization bounds, and characterize the smoothness of decision
boundaries. However, existing bounds get substantially weaker with increasing
depth of the network, which makes it unclear how to apply such bounds to re-
cently proposed models such as the deep equilibrium (DEQ) model, which can
be viewed as representing an infinitely-deep network. In this paper, we show that
monotone DEQs, a recently-proposed subclass of DEQs, have Lipschitz constants
that can be bounded as a simple function of the strong monotonicity parameter of
the network. We derive simple-yet-tight bounds on both the input-output mapping
and the weight-output mapping defined by these networks, and demonstrate that
they are small relative to those for comparable standard DNNs. We show that
one can use these bounds to design monotone DEQ models, even with e.g. multi-
scale convolutional structure, that still have constraints on the Lipschitz constant.
We also highlight how to use these bounds to develop PAC-Bayes generalization
bounds that do not depend on any depth of the network, and which avoid the ex-
ponential depth-dependence of comparable DNN bounds.
1	Introduction
Measuring the sensitivity of deep neural networks (DNNs) to changes in their inputs or weights is
important in a wide range of applications. A standard way of measuring the sensitivity of a function
f is the Lipschitz constant of f, the smallest constant L ∈ R+ such that kf (x)-f (y)k2 ≤ Lkx-yk2
for all inputs x and y. While exact computation of the Lipschitz constant of DNNs is NP-hard
(Virmaux & Scaman, 2018), bounds or estimates can be used to certify a network’s robustness to
adversarial input perturbations (Weng et al., 2018), encourage robustness during training (Tsuzuku
et al., 2018), or as a complexity measure of the DNN (Bartlett et al., 2017), among other applications.
An analogous Lipschitz constant that bounds the sensitivity of f to changes in its weights can be
used to derive generalization bounds for DNNs (Neyshabur et al., 2018). A growing number of
methods for computing bounds on the Lipschitz constant of DNNs have been proposed in recent
works, primarily based on semidefinite programs (Fazlyab et al., 2019; Raghunathan et al., 2018)
or polynomial programs (Latorre et al., 2019). However, as the depth of the network increases,
these bounds become either very loose or prohibitively expensive to compute. Additionally, they are
typically not applicable to structured DNNs such as convolutional networks which are common in
everyday use.
The deep equilibrium model (DEQ) (Bai et al., 2019) is an implicit-depth model which directly
solves for the fixed point of an “infinitely-deep”, weight-tied network. DEQs have been shown
to perform as well as DNNs in domains such as computer vision (Bai et al., 2020) and sequence
modelling (Bai et al., 2019), while avoiding the large memory footprint required by DNN training in
order to backpropagate through a long computation chain. Given that DEQs represent infinite-depth
networks, however, their Lipschitz constants clearly cannot be bounded by existing methods, which
are very loose even on networks of depth 10 or less.
* Equal contribution.
1
Published as a conference paper at ICLR 2021
In this paper we take up the question of how to bound the Lipschitz constant of DEQs. In particular,
we focus on monotone DEQs (monDEQ) (Winston & Kolter, 2020), a recently-proposed class of
DEQs which parameterizes the DEQ model in a way that guarantees existence of a unique fixed-
point, which can be computed efficiently as the solution to a monotone operator splitting problem.
We show that monDEQs, despite representing infinite-depth networks, have Lipschitz constants
which can be bounded by a simple function of the strong-monotonicity parameter, the choice of
which therefore directly influences the bound. We also derive a bound on the Lipschtiz constant w.r.t.
the weights of the monDEQ, with which we derive a deterministic PAC-Bayes generalization bound
for the monDEQ by adapting the technique of (Neyshabur et al., 2018). While such generalization
bounds for DNNs are plagued by exponential dependence on network depth, the corresponding
monDEQ bound does not involve any depth-like term.
Empirically, we demonstrate that our Lipschitz bounds on fully-connected monDEQs trained on
MNIST are small relative to comparable DNNs, even for DNNs of depth only 4. We show a similar
trend on single- and multi-convolutional monDEQs as compared to the bounds on traditional CNNs
computed by AutoLip and SeqLip (Virmaux & Scaman, 2018), the only existing methods for (even
approximately) bounding CNN Lipshitz constants. Further, our monDEQ generalization bounds are
comparable with bounds on DNNs of around depth 5, and avoid the exponential dependence on
depth of those bounds. Finally, we also validate the significance of the small Lipschitz bounds for
monDEQs by empirically demonstrating strong adversarial robustness on MNIST and CIFAR-10.
2	Background and Related Work
Lipschitz constants of DNNs Lipschitz constants of DNNs were proposed as early as Szegedy
et al. (2014) as a potential means of controlling adversarial robustness. The bound proposed in
that work was the product of the spectral norms of the layers, which in practice is extremely loose.
Virmaux & Scaman (2018) derive a tighter bound via a convex maximization problem; however the
bound is typically intractable and can only be approximated. Combettes & Pesquet (2019) bound the
Lipschitz constant of DNNs by noting that the common nonlinearities employed as activation func-
tions are averaged, nonexpansive operators; however, their method scales exponentially with depth
of the network. (Zou et al., 2019) propose linear-program-based bounds specific to convolutional
networks, which in practice are several orders of magnitude larger than empirical lower bounds.
Upper bounds based on semidefinite programs which relax the quadratic constraints imposed by the
nonlinearities are studied by Fazlyab et al. (2019); Raghunathan et al. (2018); Jin & Lavaei (2018).
The bounds can be tight in practice but expensive to compute for deep networks; as such, Fazlyab
et al. (2019) propose a sequence of SDPs which trade off computational complexity and accuracy.
This allows us to compare our monDEQ bounds to their SDP bounds for networks of increasing
depth (see Section 5). Latorre et al. (2019) show that the complexity of the optimization problems
can be reduced by taking advantage of the typical sparsity of connections common to DNNs, but the
resulting methods are still prohibitively expensive for deep networks.
DEQs and monotone DEQs An emerging focus of deep learning research is on implicit-depth
models, typified by Neural ODEs (Chen et al., 2018) and deep equilibrium models (DEQs) (Bai
et al., 2019; 2020). Unlike traditional deep networks which compute their output by sequential,
layer-wise computation, implicit-depth models simulate “infinite-depth” networks by specifying,
and directly solving for, some analytical conditions satisfied by their output. The DEQ model di-
rectly solves for the fixed-point of an infinitely-deep, weight-tied and input-injected network, which
would consist of the iteration zi+1 = g(zi , x) where, g represents a nonlinear layer computation
which is applied repeatedly, zi is the activation at “layer” i, and x is the network input, which is
injected at each layer. Instead of iteratively applying the function g (which indeed may not con-
verge), the infinite-depth fixed-point z* = g(z*,χ) can be solved using a root-finding method. A
key advantage of DEQs is that backpropagation through the fixed-point can be performed analyti-
cally using the implicit function theorem, and DEQ training therefore requires much less memory
than DNNs, which need to store the intermediate layer activations for backpropagation.
In standard DEQs, existence of a unique fixed point is not guaranteed, nor is stable convergence to
a fixed-point easy to obtain in practice. Monotone DEQs (monDEQs) (Winston & Kolter, 2020)
improve upon this aspect by parameterizing the DEQ in a manner that guarantees the existence of a
stable fixed point. Monotone operator theory provides a class of operator splitting methods which
are guaranteed to converge linearly to the fixed point (see Ryu & Boyd (2016) for a primer). The
2
Published as a conference paper at ICLR 2021
monDEQ considers a weight-tied, input-injected network with iterations of the form
z(k+1) = σ(Wz(k) +Ux+b)	(1)
where x ∈ Rn is the input, U ∈ Rh×n the input-injection weights, z(i) ∈ Rh the hidden unit
activations at “layer” i, and W ∈ Rh×h the hidden-unit weights, and b ∈ Rh a bias term, and
σ : Rh → Rh an elementwise nonlinearity. The output of the monDEQ is defined as the fixed point
of the iteration, a z* such that
z* = σ(Wz* + Ux + b).	(2)
Just as for DEQs, forward iteration of this system need not converge to z*; instead, the fixed point is
found as the solution to a particular operator splitting problem. Various operator splitting methods
can be employed here, for example forward-backward iteration, which results in a damped version
of the forward iteration
z(k+1) = σ(z(k) - α((I - W)z(k) - (Ux + b))) = σ((I - α(I - W))z(k) +α(Ux+b)). (3)
The operatorI-α(I-W) appearing in this iteration is contractive for any 0 < α ≤ 2m/L2, and this
iteration is guaranteed to converge so long as the operator I - W is Lipschitz and strongly monotone
with parameters L (which is in fact the spectral norm kI - Wk2) and m (Ryu & Boyd, 2016). In
Section 3, we will see how unrolling this iteration leads directly to a bound on the Lipschitz constant
of the monDEQ. To ensure the strong monotonicity condition, that I - W mI, the monDEQ
parameterizes W as
W = (1 -m)I-ATA+B-BT.
The strong-monotonicity parameter m will in fact figure in directly to the Lipschitz constant of the
monDEQ.
Lipschitz constants for implicit-depth models A few prior works have proposed methods for
bounding the Lipschitz constants of other classes of implicit depth network. Ghaoui et al. (2020)
define restrictive conditions for well-posedness ofan implicit network which are different from those
of the monDEQ. In particular, they require the weight matrix W to be such that forward iteration
is stable (as opposed to the stability of the operator splitting methods required by monDEQ). They
derive Lipschitz constants and robustness guarantees under these conditions; for example when
kWk∞ < 1, then a Lipschitz bound can be derived by simply manipulating the fixed-point equation
as they demonstrate in equation 4.3. Herrera et al. (2020) propose a framework for implicit depth
models which incorporate the Neural ODE (Chen et al., 2018) (which is the solution ofan ODE at a
given time T) but not the monDEQ (which can be cast as finding the equilibrium point ofan ODE).
They derive bounds on the Lipschitz constant w.r.t. network weights, but their framework cannot be
applied to bound the Lipschitz constant of the monDEQ.
3	Lipschitz constants for monotone DEQs
We now present our main methodological contributions, easily-computable bounds on the Lipschitz
constants of monDEQs. We first derive the Lipschitz bound on the input-output mapping defined by
the monDEQ, followed by that for the weight-output mapping. As we describe below, both bounds
turn out to depend inversely on the strong-monotonicity parameter m of the monDEQ. Since m is
chosen for the monDEQ at design time, this implies an analytical handle on its Lipschitz constant.
3.1	Lipschitz constants with respect to input
The naive way of computing L for feedforward deep networks is by multiplying the spectral norms
of the weight matrices. As stated above, just employing forward iterations does not lead to con-
vergence of the monDEQ. Analogously, if we were to adopt the naive method and simply unroll
the forward iterations of the monDEQ as described in equation 1, we would end up with an infinite
product of spectral norms, which would not converge unless W itself is contractive. Here again, we
consider unrolling the averaged operator T := I - α(I - W) employed in the forward-backward
iterations, which ensures that the monDEQ converges, and will also lead to a finite bound on the
Lipschitz constant. Notice that T appears in the forward iterations in equation 3. In the sequel, let
L[A] denote the Lipschitz constant of a function or operator A. The following proposition, which
we prove in Appendix A, bounds the Lipschitz constant L[T].
3
Published as a conference paper at ICLR 2021
Proposition 1. L[T] ≤，1 一 2am + α2L[I - W]2
This implies that for α ∈(0, L∣-W产),L[T] < 1. In our subsequent analysis, We only consider
values of α in this range. We are now ready to state our bound for the Lipschitz constant of the
monDEQ:
Theorem 1 (Lipschitz constant of monDEQ). Let f (x) = z* denote the output of the monDEQ on
input x, as in equation 2. Consider any x, y ∈ Rn. Then, we have that
kf(χ)-f(y)k2 ≤≡2kχ-yk2.
m
In other words, L[f] ≤ kUk2.
Proof. Let fk(x) = z(k) denote the kth iterate of the forWard-backWard iterations as described in
equation 3 (We begin With f0(x) = 0). We Will try and unroll these iterations in the folloWing:
kfk(x) - fk(y)k2 = kσ(Tfk-1(x) + αUx + αb) - σ(T fk-1(y) + αUy + αb)k2
≤ kT fk-1(x) + αUx + αb - T fk-1 (y) - αUy - αb)k2 (σ =ReLU is 1-Lipschitz)
= kT (fk-1 (x) - fk-1(y)) + αU(x - y)k2 ≤ kT (fk-1(x) - fk-1(y))k2 + αkU (x - y)k2
≤ L[T]kfk-1(x) - fk-1(y)k2 + αL[U]kx - yk2
k-1
≤ L[T]kkfo(x) - fo(y)k2 + αkU∣2kx - yk2 ∙ X(L[T])i (unrolling k times)
i=0
k-1
=α∣∣U∣2kx - yk2 ∙ X(L[T])i (since fo(x) = fo(y) = 0)
i=0
Since the above inequality holds for all k, We can take the limit on both sides as k → ∞, keeping α
fixed. But notice that since the forWard-backWard iterations converge to the true f (Which does not
depend on α), We have that limk→∞ fk = f. That is, the dependence on α disappears on the LHS
once We take the limit on k. Thus, by using the continuity of the l2 norm, We have
kf(X) - f(y)k2 = Il lim fk(X)-Jim fk(y)	≤ αkuk2kx - yk2 ∙ X(L[TDi
k→∞	k→∞
2	i=0
=OkUTVrkkx-yk2 (since 〃？〕< 1)
1 - L[T]
≤ -——h	OakUk2 2 E Wl2 kx - yk2 (from PrOPoSitiOn 1)
1 -	1 - 2αm + α2L[I - W]2
NoW, since the above result holds for any α in the range considered, taking α → 0, We have that
akUk2
L[f] ≤ lim I___________________________
α→0 1 -，1 — 2am + a2L[I — W]
2
kU k2
m
(applying L’Hopital’s rule)
□
We observe here that the Lipschitz constant of the monDEQ With respect to its inputs indeed depends
on only tWo quantities, namely kUk2 and m, and doesn’t depend at all on the Weight matrix W.
Furthermore, because m is a hyperparameter chosen by the user, this illustrates that monDEQs have
the notable property that one can essentially control the Lipschitz parameter of the netWork (insofar
as the influence of W is concerned) by appropriately choosing m, and not require any additional
structure or regularization on W. This is in stark contrast to most existing DNN architectures,
Where enforcing Lipschitz bounds requires substantial additional effort.
4
Published as a conference paper at ICLR 2021
3.2 Lipschitz constants with respect to weights
We now turn to the question of bounding the change in the output of the monDEQ when the weights
are perturbed but the input remains fixed. This calculation has several important use cases, one of
which is in the derivation of generalization bounds for the monDEQ. Given a bound on the change
in the output on perturbing the weights of the monDEQ, we can derive bounds on the generalization
error in a straight-forward manner, as detailed in Section 4 below. The following theorem establishes
a perturbation bound for the monDEQ.
Theorem 2	(Perturbation bound for monDEQ). Let I 一 W 占 ml and I 一 W 占 m I .The Change in
the output ofthe monDEQ on perturbing the weights and biasesfrom W, U, b to W, U, b is bounded
as follows:
kf (W, U, b) - f (W, U, b)k2 ≤ kW-Wk2kux + bk2 + k(U- U罔2 + kb- bk2
mm	m
The proof steps for Theorem 2 parallel closely those involved in the derivation of the Lipschitz
constant with respect to the inputs, and are outlined in Appendix B. We highlight here again that
the bound depends inversely on m, a design parameter in our control. Further, when compared
to a similar perturbation bound derived in Neyshabur et al. (2018), we note that our perturbation
bound for the monDEQ does not involve a depth-dependent product of spectral norms of weights.
In addition, although we state the theorem in terms of a perturbation of W (which can thus lead to a
different strong monotonicity parameter m), the bound can also be adapted to perturbations on A and
B in the typical monDEQ parameterization, which leads to a perturbed network that will necessarily
still have the same monotonicity parameter m as the original (indeed, we take this approach in the
next section, when deriving the generalization bound).
4 Generalization bound for monDEQ
In this section, we demonstrate how the perturbation bound derived in Section 3.2 leads directly
to a deterministic PAC-Bayes, margin-based bound on the monDEQ generalization error, following
the analysis for DNNs of Neyshabur et al. (2018). A key difference from our work, however, is
that the perturbation bound they derive involves the product of spectral norms of all the weight
matrices in the DNN. Thus, as the network gets deeper, their bound grows exponentially looser. As
in Neyshabur et al. (2018), our generalization bound is based on two key ingredients. The first is
their deterministic PAC-Bayes margin bound (Lemma 1 in the Appendix C), which adapts traditional
PAC-Bayes bounds to bound the the expected risk of a parameterized, deterministic classifier in
terms of its empirical margin loss. The second is the perturbation bound on monDEQ with respect to
weights as derived in Section 3.2 above. Crucially, since our perturbation bound does not explicitly
involve a product of spectral norms of weights (which in the case of the monDEQ, would be an
infinite product), our final generalization bound does not either.
The monDEQ model we consider here consists of a fully connected layer at the end that maps f
to the output, so that fo(x) = Wof(x) + bo, where Wo and bo are the weights and bias in the
output layer; these parameters are important to include here since they contribute directly to the
perturbation bound. We also restrict the input x to the monDEQ to lie in an l2 norm ball of radius B.
Let h denote the hidden dimension of the monDEQ, and M the size of the training set, and define
β := max{kUk2, kAk2, kbk2, kWok2}. Let Lγ(fo) denote the expected margin loss at margin γ of
the monDEQ on the data distribution D, where
LY (fO) = P(x,y)〜D
fo(x)y ≤ γ + maxfo(x)j
j6=y
∙1?/，、∙1	. . 1	f	∙ ∙ 1	∙ 1	. 1 . ∙ ∙	1 .	5 T
and Lγ (fo) denote the corresponding empirical margin loss on the training dataset. We are now
ready to state our generalization bound for the monDEQ:
Theorem 3	(Generalization bound for monDEQ). Let
E kW∙kF = kAkF + kBkF + kU kF + kbkF + kWokF + MkF
For any δ, γ > 0, with probability at least 1 一 δ over the training set of size M, we have that
L0(fo) ≤ LY(fo) + O ( √ hln(h)[β2B(YY+m4)MmβB + m2]2 X kW∙kF + ∖
5
Published as a conference paper at ICLR 2021
lu-suoɔ ZEqɔsd"
2
W
6 4
O O
1 1
lu-suoɔ zl-ɔsd"
0	5	10	15	20
strong-monotonicity param m
(b) monDEQ Lipschitz bounds vs m
4	6	8	10	12	14
depth
(a) DNN Lipschitz bounds vs Depth
CNN greedy SeqLiP Ub
CNN annealing lb /
■ - Single-Conv monDEQ, m=20
-uplsuoɔ ZEqOSd"
0	5	10	15	20
strong-monotonicity param m
(d) Convolutional monDEQ bounds vs m
4	6	8	10
depth
(c) CNN bounds vs Depth
Figure 1:	MNIST results: Lipschitz bounds as a function of depth and strong monotonicity parame-
ter. lb: lower bound; ub: upper bound.
Note that our bound above does not involve any depth-like term that scales exponentially, like the
term that involves the product of spectral norms of the weight matrices in NeyshabUr et al. (2018)
(while still having the same dependence on h, which is √h ln h). To the best of our knowledge, this
is the first generalization bound for an implicit-layer model having effectively infinite depth. The
proof of Theorem 3 is given in Appendix C.
5 Experimental Results1
5.1	Lipschitz constants
In this section, we empirically verify the tightness of the Lipschitz constant of the monDEQ with re-
spect to inputs. We conduct all our experiments on MNIST and CIFAR-10, for which several bench-
marks exist for computing the Lipschitz constant. We conduct experiments for different monDEQ ar-
chitectures (fully connected/convolutional) with varying parameters (strong-monotonicity parameter
m and width h), which we compare to DNNs with different depths and widths. We compute empir-
ical lower bounds by maximizing the norm of the gradient at 10k randomly sampled points. A naive
upper bound can be computed as Qid=1 kWik2. We include these bounds wherever applicable.
MNIST Here, we train DNNs for various depths from d = 3, 4, . . . , 14 for a fixed hidden layer
width h = 40, and plot (Figure 1a) the bound on the Lipschitz constant given by the SDP-based
method of Fazlyab et al. (2019) on these DNNs. We can observe that all estimates of the Lipschitz
constant increase exponentially with depth. For comparison, in Figure 1b we plot our Lipschitz
constant bounds for monDEQs with fixed h = 40, 60, fora range of strong-monotonicity parameters
m. We note that the DNNs all have test error of around 3%, while the monDEQ test error ranges from
1Experimental code available at https://github.com/locuslab/lipschitz_mondeq.
6
Published as a conference paper at ICLR 2021
IuUlSUoɔ ZEqOSd"
----- -^==
7
O
O
monDEQs ----- CNN med
—— CNN sm —— CNN lg
1O0
0	5	10	15	20
strong-monotonicity param m
34
8
2
JoJJə-səl .
26 -
0	5	10	15	20
strong-monotonicity param m
(a) Lipschitz bounds for monDEQs vs m and for three
CNN models. Solid lines: upper bounds; dashed lines: (b) Test error for CNNs and monDEQs vs m.
empirical lower bounds.
Figure 2:	CIFAR-10 results: Lipschitz bounds and test accuracy for monDEQs as a function of
strong monotonicity parameter and for CNNs. See text for description of models.
2.4%-4.3%, increasing with m (see Figure 5 in Appendix F for details). We see that the Lipschitz
constant of the monDEQ is much smaller, and that on increasing m, the Lipschitz constant of the
monDEQ decreases, outlining how we can exercise control on the Lipschitz constant.
We also compare the Lipschitz constants of monDEQs and DNNs having the same width, for a fixed
depth d = 5. The results are shown in Figure 6 in Appendix F. The DNN numbers are derived from
Figure 2(a) in (Fazlyab et al., 2019). We observe that the Lipschitz constant of the monDEQ for the
same width (and essentially infinite depth) is much lower than the bounds for regular DNNs.
Next, using the bound derived in Section 3.1, we compute the Lipschitz constant of convolutional
monDEQ architectures, namely single convolutional and multi-tier convolutional monDEQs. We
compare to the numbers in Figure 5 in Virmaux & Scaman (2018), which reports the Lipschitz con-
stants computed by various methods for different CNNs with increasing depth. For our estimate on
the single convolutional monDEQ, we use a single convolutional layer with 128 channels, whereas
for the multi-tier convolutional monDEQ, we use 3 convolutional layers with 32, 64 and 128 chan-
nels. In Figure 1c, we can observe that as for DNNs, the CNN Lipschitz constants estimated by
existing methods also suffer with depth. However, we can observe in Figure 1d that the Lipschitz
bounds for convolutional monDEQs are much smaller. Also, on increasing m, we can control the
Lipschitz constant of both single as well as multi-tier convolutional monDEQs. The test error for
the convolutional monDEQs is 0.65%-3.22%, increasing with m (see Figure 5 in Appendix F), but
is not reported for the CNNs in Virmaux & Scaman (2018).
CIFAR-10 To demonstrate that the Lipschitz bounds scale to larger datasets, we run similar ex-
periments on CIFAR-10. Figure 2a shows our bound (solid blue lines) for single-convolutional
monDEQs (128 channels) with a range of m values, together with empirical lower bounds (dashed
blue lines). Also shown are upper bounds (solid lines) and empirical lower bounds (dashed lines)
for three standard CNN models (CNN sm, med, and lg, having 2, 4, and 6 convolutions respectively,
detailed in Appendix F). The upper bounds are computed using the Greedy SeqLip method of Vir-
maux & Scaman (2018). We see that a) the monDEQ Lipschitz bounds decrease with m, and b) the
upper bounds (and gap between upper and lower bounds) for the medium and large CNNs are large
by comparison. In Figure 2b we see that test error of the monDEQ increases with m from 26% to
33%, and that, despite their much higher Lipschitz bounds, the three CNNs have similar test error.
Finally, on CIFAR-10 with data augmentation, we also trained a larger multi-tier convolutional mon-
DEQ with three convolutional layers with 64, 128, and 128 channels. This model obtains 10.25%
test error and has a Lipschitz upper bound of 1996.86, which is on par with the upper bound of the
medium CNN (plotted in brown; with L =1554.01, test error =30.6%).
Unrolling monDEQs In this experiment, we study if unrolling the monDEQ with m = 1, h = 40
up to a finite depth and constructing an equivalent DNN with this depth leads to a tight estimate of
7
Published as a conference paper at ICLR 2021
PUnoq JoJJə .uəg
5 4
O O
1 1
5
W
PUnOq JOJJə .uəg
0	5	10	15	20
strong-monotonicity param m
(b)	monDEQ generalization bounds vs m
4	6	8	10	12
depth
(a)	DNN generalization bound vs Depth
Figure 3:	Generalization bounds for DNNs and monDEQs as a function of depth and m.
the Lipschitz constant of the monDEQ. Concretely, we do this for two operator splitting methods in
the monDEQ: Forward-backward (FB) iterations and Peaceman-Rachford (PR) iterations. For each
value of α in a range, we calculate the number of iterations (FB or PR) required to converge within
a tolerance 1e-3, and construct the equivalent DNN with this depth. Note that these unrolled DNNs
compute the same function as the monDEQ (up to tolerance), and therefore, must have the same
Lipschitz constant (which is around 10). We compute naive upper bounds on the Lipschitz constants
of these DNNs (we cannot use the SDP-based bound of Fazlyab et al. (2019) due to technicalities in
the construction of the unrolled DNN; refer to Appendix D). We can observe (Figure 7 in Appendix
F) that the upper bounds corresponding to both PR and FB iterations are in the range 105 to 1013,
suggesting that unrolling the monDEQ and employing standard techniques on the unrolled monDEQ
is not a viable way to bound the Lipschitz constant. More details about the construction of these
equivalent DNNs for both FB and PR iterations are provided in Appendix D.
5.2	Generalization bounds
A key advantage of the monDEQ generalization bounds derived in Section 4 is the lack of any depth
analog that can cause the bounds to grow exponentially. To assess this aspect experimentally, we
first compute the DNN generalization bound following the protocol of Nagarajan & Kolter (2018).
We train DNNs (width = 40) of varying depth of 3 to 14 layers, and compare to similar monDEQs
with various m values. Each model is trained on a sample of 4096 MNIST examples until the
margin error at margin γ = 10 reaches below 10% which serves to standardize the experiments
across choice of batch size and learning rate. As widely reported, we see that DNN bounds increase
exponentially with depth, ranging numerically from 104 for depth 3 networks to 108 (see Figure 3a).
For monDEQs of width = 40, the bound decreases monotonically with m, and is confined to the
range 104 to 106, as seen in Figure 3b (note the difference in scale). In contrast, the true test error of
the DNNs increases only slightly with depth, and that of the monDEQs increases only slightly with
m. Note that the DNNs and monDEQ s have comparable test error (see Figure 8 in Appendix G).
Finally, as done for Lipschitz bounds above, we compare our generalization bound to what we obtain
by unrolling the monDEQ into a DNN, and then computing the Neyshabur et al. (2018) bound more-
or-less directly (see Appendix E). We do this only for FB iterations, as the inverted operators of PR
iterations complicate the analysis. As seen in Figure 8c, the resulting bounds are quite high, though
the difference with our bound is not as great as was seen for the unrolled Lipschitz bounds above.
We attribute this to the fact that our generalization bound technique is a minimal modification to that
of Neyshabur et al. (2018); we expect that it can be tightened with more refined analysis.
5.3	Adversarial robustness of monDEQs
In this section, we empirically demonstrate an important use case for the tight Lipschitz constants of
the monDEQ: robustness to adversarial examples. We experiment with both certified adversarial `2
robustness as well as empirical robustness to adversarial PGD '2 -bounded attacks. Here We describe
our results on MNIST, and report similar experiments on CIFAR-10 in Appendix H.
8
Published as a conference paper at ICLR 2021
DNN d=3 L=14.15	DNN d=7 L=199.19
DNN d=4 L=23.46	— DNN d=8 L=557.46
DNN d=5 L=51.59	monDEQ m=0.1, L=47.38
——DNN d=6 L=82.41	monDEQ m=20, L=3.83
£64 2
Oooo
Aualnuuf Seαd
DNN d=3	——DNN d=14
DNN d=6	---' monDEQ m=0.1
DNN d=10 monDEQ m=20
(a)	Certified Adversarial Robustness
(b)	Adversarial robustness to PGD attacks
Figure 4:	Superior adversarial robustness of monDEQs as compared to DNNs
Certified adversarial robustness Consider any point x0 within an l2 ball of radius around x.
Then, we have that
kf(x)-f(x0)k∞≤kf(x)-f(x0)k2≤Lkx-x0k2≤L
Define margin(x) = f (x)y - maxi6=y f(x)i, where f (x)y is the logit corresponding to the label y
of the input x. Then if L ≤ 2margin(x), We are certified robust to any perturbed input x0 within an
l2-ball of radius around x. Thus, we can empirically compare DNNs and monDEQs with regards
to this certificate on MNIST. For a range of values, we compute the (certified) robust test accuracy
(fraction of points in test set for which the aforementioned condition holds) for trained DNNs as
well as monDEQs. We note here that our choice of values corresponds to inputs normalized in
the range [0, 1]. For DNNs, just as in Figure 1a, we vary the depth for fixed h = 40, and use the
L values computed by using the method in Fazlyab et al. (2019). For monDEQs, we set width
h = 40, m = 0.1, 20 and substitute our upper bound for L. Note that since the L values for
monDEQs observed in Section 5.1 were significantly smaller than those for the DNNs, one would
expect the condition for the certificate to hold more easily for monDEQs. Indeed, we verify this
in our experiments. In Figure 4a, we can observe that the robust test accuracy for the monDEQ
with m = 20 at = 0.2 is 51%, while that for the best DNN (d = 3), is just 4%. This illustrates
that monDEQs allow for better certificates to adversarial robustness, owing to their small Lipschitz
constants, and the ability to control it by setting m.
Empirical robustness We also assess the empirical robustness of monDEQs to l2-bounded Pro-
jected Gradient Descent attacks (implemented as part of the Foolbox toolbox (Rauber et al., 2017;
2020)) on both, trained monDEQs and DNNs on MNIST, and compute the accuracy on these adver-
sarially perturbed test examples. Figure 4b shows the results: in general, over a range of values,
the robust test accuracy of the monDEQ with m = 20 is larger than that of the DNNs.
6 Conclusion
In this paper, we derived Lipschitz bounds for monotone DEQs, a recently proposed class of impicit-
layer networks, and showed that they depend in a straighforward manner on the strong monotonicity
parameter m of these networks. Having derived a Lipschitz bound with respect to perturbation in
the weights, we were able to derive a PAC-Bayesian generalization bound for the monotone DEQ,
which does not depend exponentially on depth. We showed empirically that our bounds are sensible,
can be controlled by choosing m suitably, and do not suffer with increasing depth of the network.
As future work, we aim to analyze the vacuousness of the derived generalization bound. As such,
since our bound does not suffer exponentially with depth, we hope to be able to make the analysis
tighter and derive a non-vacuous generalization bound.
9
Published as a conference paper at ICLR 2021
References
Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In Advances in Neural
Information Processing Systems,pp. 690-701, 2019.
Shaojie Bai, Vladlen Koltun, and J Zico Kolter. Multiscale deep equilibrium models. arXiv preprint
arXiv:2006.08656, 2020.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6240-6249, 2017.
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differential equations. In Advances in neural information processing systems, pp. 6571-6583,
2018.
Patrick L Combettes and Jean-Christophe Pesquet. Lipschitz certificates for neural network struc-
tures driven by averaged activation operators. arXiv preprint arXiv:1903.01014, 2019.
Mahyar Fazlyab, Alexander Robey, Hamed Hassani, Manfred Morari, and George Pappas. Efficient
and accurate estimation of lipschitz constants for deep neural networks. In Advances in Neural
Information Processing Systems, pp. 11427-11438, 2019.
Laurent El Ghaoui, Fangda Gu, Bertrand Travacca, Armin Askari, and Alicia Y. Tsai. Implicit deep
learning, 2020.
Calypso Herrera, Florian Krach, and Josef Teichmann. Estimating full lipschitz constants of deep
neural networks, 2020.
Ming Jin and Javad Lavaei. Stability-certified reinforcement learning: A control-theoretic perspec-
tive. arXiv preprint arXiv:1810.11505, 2018.
Fabian Latorre, Paul Rolland, and Volkan Cevher. Lipschitz constant estimation of neural networks
via sparse polynomial optimization. In International Conference on Learning Representations,
2019.
Vaishnavh Nagarajan and Zico Kolter. Deterministic pac-bayesian generalization bounds for deep
networks via generalizing noise-resilience. In International Conference on Learning Representa-
tions, 2018.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to
spectrally-normalized margin bounds for neural networks. In International Conference on Learn-
ing Representations, 2018.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial exam-
ples. In International Conference on Learning Representations, 2018.
Jonas Rauber, Wieland Brendel, and Matthias Bethge. Foolbox: A python toolbox to benchmark
the robustness of machine learning models. In Reliable Machine Learning in the Wild Workshop,
34th International Conference on Machine Learning, 2017. URL http://arxiv.org/abs/
1707.04131.
Jonas Rauber, Roland Zimmermann, Matthias Bethge, and Wieland Brendel. Foolbox native: Fast
adversarial attacks to benchmark the robustness of machine learning models in pytorch, tensor-
flow, and jax. Journal of Open Source Software, 5(53):2607, 2020. doi: 10.21105/joss.02607.
URL https://doi.org/10.21105/joss.02607.
Ernest K Ryu and Stephen Boyd. Primer on monotone operator methods. Appl. Comput. Math, 15
(1):3-43, 2016.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna Estrach, Dumitru Erhan, Ian
Goodfellow, and Robert Fergus. Intriguing properties of neural networks. In 2nd International
Conference on Learning Representations, ICLR 2014, 2014.
Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. Lipschitz-margin training: Scalable certifi-
cation of perturbation invariance for deep neural networks. In Advances in neural information
processing systems, pp. 6541-6550, 2018.
10
Published as a conference paper at ICLR 2021
Aladin Virmaux and Kevin Scaman. Lipschitz regularity of deep neural networks: analysis and
efficient estimation. In Advances in Neural Information Processing Systems, pp. 3835-3844,
2018.
Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning,
and Inderjit Dhillon. Towards fast computation of certified robustness for relu networks. In
International Conference on Machine Learning, pp. 5276-5285, 2018.
Ezra Winston and J Zico Kolter. Monotone operator equilibrium networks. arXiv preprint
arXiv:2006.08591, 2020.
Dongmian Zou, Radu Balan, and Maneesh Singh. On lipschitz bounds of general convolutional
neural networks. IEEE Transactions on Information Theory, 66(3):1738-1759, 2019.
11
Published as a conference paper at ICLR 2021
A Proof of Proposition 1
Proof.
kTx - Tyk22 = k((1 - α)I + αW)x - ((1 - α)I + αW)yk22
= kx - y - α(I - W)(x - y)k22
= kx	- yk22	-	2α(x	-	y)T (I	-	W)(x - y) +	α2k(I -	W)(x - y)k22
≤ kx	- yk22	-	2α(x	-	y)T (I	-	W)(x - y) +	α2L[I -	W]2 kx - yk22
Now, note that by the strong monotonicity of the monDEQ,
I-W mI,
which implies that (x - y)T (I - W)(x - y) ≥ mkx - yk22. Substituting this bound above, we have
that
kTx - Tyk22 ≤ kx - yk22 - 2αmkx - yk22 + α2L[I - W]2 kx - yk22
= (1 - 2αm + α2L[I - W]2)kx - yk22
Thus, We have that L[T] ≤，1 一 2ɑm + α2L[I - W]2	□
B	Proof of Theorem 2
In order to derive the perturbation bound in Theorem 2, We first state the folloWing proposition,
Which bounds the norm of the output after k forWard-backWard iterations.
Proposition 2. Let fk (W, U, b) denote the kth iterate of the forward-backward iterations of the
monDEQ parameterized by W, U, b on a fixed arbitrary input x. Further, let T(W) = (1 一 α)I +
αW. Then, we have that
kfk (W, U, b)k2 ≤
α∣∣Ux + b∣∣2
1 一 L[T(W)]
Proof of Proposition 2.
kfk (W, U, b)k2 = kσ(T fk-1(W, U, b) + α(U x + b))k2
≤ kT fk-1(W, U, b) + α(Ux + b)k2 (ReLU is 1-Lipschitz)
≤ kTk2kfk-1(W,U,b)k2+αkUx+bk2
= L[T]kfk-1(W, U, b)k2 + αkU x + bk2
k-1
≤ L[T]kkf0(W,U,b)k2+αkUx+bk2XL[T]i
i=0
k-1
=αkUx+bk2XL[T]i (since f0(W, U, b) =0)
i=0
∞
≤ αkUx+bk2XL[T]i
i=0
=半勺柴(since L[T]< 1)
1 一 L[T]
□
We are noW ready to prove Theorem 2.
ProofofTheorem 2. Denote fk = fk(W ,U ,b) and fk = fk (W,U,b). Further, denote ∆k =
Ilfk — fk∣∣2. For α ∈ (0,min (LI-W2, l[i-W]2)), we have from Proposition 1 that both
12
Published as a conference paper at ICLR 2021
kT(W)k2,kT(W)k2 < 1. Thus,
∆k = kσ[T(W成-i + α(UX + b)] - σ[T(W)fk-i + α(Ux + b)]∣∣2
≤ kT(W成-1 - T(W)fk-i + α(U - U)x + α(b - b)k2
≤ kT(W)(fk-1 - fk-i) + (T(W)- T(W))fk-i + α(U - U)x + α(b - b)∣∣2
≤ IIT(W)(fk-1 - fk-1) + α(W - W)fk-1 + α(U - U)X + α(b - b)k2
≤ kT(W)∣∣2∆k-1 + α∣∣W - W)∣∣2Ilfk-1k2 + αk(U - U)x∣∣2 + α∣∣b - b∣∣2
≤ kT(W)k2∆k-1 + α2kW - W)器UX + bk2 + αk(U - U)x∣2 + αkb - b∣2	(Proposition 2)
1 - kT (W)k2
≤ (°2"I- WIHUX + bk2 + αk(U - U)xk2 + α∣∣b - b∣∣2) X kT(W)k2
1 - kT (W)k2	i=0
Notice here again that the above inequality holds for all k . Taking the limit as k → ∞ similar to the
step in the proof of Theorem 1, we have
kf(W,U, b)- f (W,U,b)k2 = Iim ∆k
k→∞
≤ (α2kWI- WIHUX + bk2 + αk(U - U)xk2 + αkb - bk2) X kT(W)k2
1 - kT (W)k2	i=0
_	α2kW - W)∣∣2∣∣Ux + bk2	k(U - U)x∣2 + kb - b∣∣2
= (i-kT (W )k2)(i-kT (W)k2)	α 1-kT (W)k2
Finally, taking α → 0, we have that
kf(W Ub) - f (W U b)k2 ≤ lim ( —MkW - W)k2kUX + bk2-----+
kf (	, , ) f ( , ,)k2- α→0 1(1-kT (W )k2)(l-kT (W)k2) +
— — .
k(U - U )x∣2 + kb- bk2、
1-kT (W)k2	)
αα
=kW - W)k2kUX + bk2 lim ；~~∖∖T(W∖∖∖ ∙ limi_II7Ymll
α→0 1 - kT (W)k2 α→0 1 - kT (W)k2
α
+ (k(U - U)x∣∣2 + kb - bk2)lim ——“ / 一、”
"" y  .............. α→0 1 - kT(W)k2
kW	- W k2kUX +	bk2 ɪ k(U	- U)x∣∣2	+ kb-bk2	/ 一 「口 .… I、
=----------------—+ ------------------—	(applying L,Hopital,s rule)
mm	m
□
C Proof of Theorem 3
Proof. We first state Lemma 1 from Neyshabur et al. (2018).
Lemma 1 (Lemma 1 from Neyshabur et al. (2018)). Let fw be any predictor with parameters w,
and let P denote any distribution on the parameters that is independent of the training data. Then,
for any δ, γ > 0, with probability ≥ 1 - δ over the training data of size M, for any w, and any
random perturbation U such that P[maxχ kfw+u(∕) — fw (x)∣∞ < 4 ] ≥ 2, we have
L0(fw) ≤ L Y (fw )+4SKLyI
M-1
Now, we derive a perturbation bound for the monDEQ when we incorporate the a fully connected
layer at the end, as mentioned in Section 4. That is, we consider fo (X) = Wof(X) + b0 where
f is the output of the fixed-point iterations of the monDEQ. Next, we consider perturbations
∆A, ∆B, ∆U, ∆b, ∆Wo, ∆bo for A, B, U, b, Wo, bo respectively. The entries in the perturbation ma-
trices are each drawn independently from a Gaussian N(0, σ2). Let f to denote the function at the
13
Published as a conference paper at ICLR 2021
perturbed values of the weights. Then, we have that
kfo(x) - fo(x)k2 = kWrof(x) + bo - Wof(x) - bok2
≤ kWo(f(x) - f(x)) + (Wo - W0)f(x)k2 + kbo - bok2
≤kWok2∆+ kwo-wok2kux + bk2 + kbo - bok2
m
where ∆ is the bound from Theorem 2.
Now, let β = max(kUk2, kAk2, kWok2, kbk2). Just as in (Neyshabur et al., 2018), since we cannot
use β in determining the parameters of the prior distribution P in Lemma 1, we will consider pre-
determined values β on a grid, and then do a union bound. For now, we fix β, and consider all the
values β such that ∣β - β∣ < cιβ for some constant ci < 1.
Since the entries in the perturbations are drawn fromN(0, σ2), we have the following bound on the
l2 norms of these perturbations:
P△,〜N(0a2i)[|Ak2 > t] ≤ 2he-t /2hσ
where △. is a placeholder for each of the perturbation matrices. Thus, We have that with probability
≥ 1/2, all of the ∣∣∆∙ k2 are bounded above by σp2h ln(24h) := ω.
Now, we bound the perturbation in kW k2 when A and B are perturbed. We have that
∣∣δw ι∣2 = IlAT δa + δAA+δAδa + δb - δB ι∣2
≤ 2∣∣a∣∣2∣∣∆a∣∣2 + ∣∆a∣2
≤ 2ω(β + ω) (with probability 1/2)
Substituting this above, we have that for all x, with probability at least 1/2,
llz^	Je 2ω(β + ω)(B +1)	2ωβ(B + 1)
kfo(x)- fo(x)∣∣2 ≤----------m2---------+ —m—+ω
Here, let c2 > 0 be some constant such that ω ≤ c2β. Thus, we have that
kfo(x) - fo(x)k2 ≤ ω (2β3(1 + c22)(B+ 1) + 2β≡^ +1)
m2	m
≤ ω (2β3(1 + c2)(B + 1) + 2β(B + 1) + 1!
一 [ m2(1 - ci)3	m(1 - ci)	I
σ √2h ln(24h)
2yβ3(1 + c2)(B + 1) + 2mβ(B + 1)(1 - ci)2 + m2(1 - ci)3
m2(1 — ci)3
Setting σ
γm2(i-c1)3
4√2h ln(24h)(2β3(i+c2)(B + i)+2mβ(B + i)(i-cι)2+m2(i-cι)3)
need ω ≤ c2 β we can take the smallest c2 such that
makes this ≤ 4. Since we
(1 + ci)σ，2h ln(24h)
c2 ≥ -------J---------
一	β
σ，2h ln(24h) ω
≥ -----------=一
一 β	β
Taking c2 = (i+∣)γ suffices. We will later plug in the value
(1 + ci)γ
4(1 - ci)β
≥ c2
to bound c2 .
Then,
KL(W∙+∆w,∣P) ≤
P kW∙kF
2σ2
16h ln(24h)(2β3(1 + c2)(B + 1) + 2mβ(B + 1)(1 — ci)2 + m2(1 — ci)3)2
Y 2m4(1 — ci)6
EkW∙∣F
≤
16h ln(24h)(2β3(1 + ci )3(1 + c3 )(B + 1) + 2mβ(1 + ci)(B + 1)(1 — ci )2 + m2(1 — ci)3)2
γ2m4(1 — ci)6
w∙ kF
14
Published as a conference paper at ICLR 2021
Then, by Lemma 1, we have that with probability 1 - δ ,
_ , . . ʌ ,..
LOfO) ≤ L Y (fO) +
4
16h ln(24h)(2β3 (1 + c1)3(1 + c3)(B + 1) + 2mβ(1 + c1)(B + 1)(1 - c1)2 + m2(1 - c1)3)2
γ2m4(1 - c1)6(M - 1)
X kW∙kF + M≡
Now we need to take a union bound over β so that the above result holds for all β. Observe that we
only need to consider β in the range
Ym	γm√M
2(B + 1) ≤ β ≤ 2(B + 1).
If β ≤ 2(BmI) then |f(x)| ≤ β(B+1) ≤ 2 so LY(f) > 1. If β ≥ Ym√* then the second term in
the first term in the numerator is greater than 1 so the theorem holds trivially.
Ym
So ∣β - β∣ ≤ c1 2(B+1) guarantees ∣β - β| ≤ c1β m this range. So We can use a cover of Size
√M∕2c1: This amounts to replacing ln6M∕δ with ln3M3/2/c16.
(1+cι)γ
4(1-cι)β
Finally, substituting c3
, we get the theorem statement. We can also effectively optimize
over c1 to remove it from the final bound.
□
D Unrolling Forward-Backward and Peaceman-Rachford
ITERATIONS
In this section, we derive the form of the equivalent feedforward DNN that computes the same quan-
tity as running k iterations of either Forward-Backward or Peaceman-Rachford iterations, which are
operator-splitting methods for computing the fixed point of equation 1 (refer Winston & Kolter
(2020)).
D. 1 Unrolling Forward-Backward Iterations
One step in the Forward-Backward iterations computes
z(i+1) = σ((1 - α)I + αW)z(i) + α(Ux + b))
To simulate k steps of these computations as a depth k-feedforward network, let us construct the
following weight matrices:
αU
W1 =	I
(1 - α)I + αW
0
αU
I
Then, we can observe that
Here, σ applies only to the top h coordinates corresponding to z(i). Finally, we multiply the output
of the monDEQ after k iterations with the output weights, to obtain fo(x) as
fo (x) = Wf zx + bo
Thus, the equivalent depth-k DNN which we construct would have weight matrices
W1, [Wu]ik=-11, Wf. In this DNN, the ReLUs in the intermediate layers would only apply to the
top coordinates corresponding to z (this is the technical reason why we can’t use the SDP bound
given by (Fazlyab et al., 2019) for computing the Lipschitz constants of these unrolled networks,
since they require the same nonlinearity to apply pointwise at all coordinates).
15
Published as a conference paper at ICLR 2021
D.2 Unrolling Peaceman-Rachford Iterations
Define V = (I + α(I - W))-1. One step in the Peaceman-Rachford iterations computes
u(i+1) = u(i) - 2z(i) + 4V z(i) - 2V u(i) +2αVUx+2αVb
z(i+1) = σ(ui+1)
To simulate k steps of these computations as a depth k-feedforward network, let us construct the
following weight matrices:
Then, we can observe that
2αV U
2αV U
I
I-2V
I-2V
0
4V - 2I
4V - 2I
0
2αVU
2αVU
I
[0
2αV b
2αV b
1)
z(1) = σ	W1x +
2αVb
2αVb
+
x
Here, σ applies only to the top h coordinates corresponding to z(i) . Finally, we multiply the output
of the monDEQ after k iterations with the output weights, to obtain fo(x) as
f0(x) = Wf
u(k)
z(k)
+ bo
x
Thus, the equivalent depth-k DNN which we construct would have weight matrices
W1, [Wu]ik=-11 , Wf, and the ReLUs in the intermediate layers would only apply to the top coordi-
nates corresponding to z.
E	Generalization b ound for unrolled monDEQ
In this section, we derive a generalization bound for an unrolled monDEQ after unrolling the
forward-backward iterations d times and constructing the equivalent depth-d network. The anal-
ysis follows the derivation of the generalization bound for DNNs by (Neyshabur et al., 2018), but
we have to be careful since the weight matrices across the hidden layers are all the same and are of
a certain parameterized form. Since the analysis in (Neyshabur et al., 2018) does not include biases
in the DNNs, here, we consider unrolling monDEQs which do not have the bias term b.
Take each layer weights to be
I-α(I-W) αU
0I
where the network is now a function of
z(i)
x
and the ReLU applies only to the z(i) component. And
Wo
0
16
Published as a conference paper at ICLR 2021
Our prior distribution will now be over ∆wo, Δa, Δb, ∆u 〜 N(0, σ2I). Similar to the corre-
sponding step in C above, we have that with probability ≥ 2, the perturbations are each bounded by
ω := σ,2hln(16h). Also
∣∣∆w∣∣2 = l∣AT A — AT A + B — B — B T + BT ∣∣2
≤ k∆Ak22+2k∆Ak2kAk2+2k∆Bk2
And so
∣∆Wu∣2=	α∆0W
α∆U
0
2
≤ α (心412 + 2IδA∣2 IIAk2 + 2IδB l∣2 + IIδU l∣2)
一 一	............ ... ..	____ ..	,	一 ，:	一， r 一 一一
We take β = max{∣∣A∣∣2, ∣∣B∣∣2, ∣ Wu∣∣2, ∣∣W0∣∣2}. We assume that ∣β 一 β| ≤ dβ and d ≥ 2, so we
have β < eβ. Then applying Lemma 2 in (Neyshabur et al., 2018), we have
|fWu +∆Wu 一 fWu |2 ≤ eBβd-1((d 一 1)I∆Wu I2 + I∆Wo I2)
≤ eBβd7((d-1)α (∣∣Δa∣∣2+ 2∣∣Δa∣∣2∣∣A∣∣2 + 2∣Δb∣∣2 + ∣∣∆u|切 + ∣∣∆w.∣∣2)
≤ eBβd-1((d 一 1)α(ω2 + 2ωβ + 3ω) + ω)
≤ eBβd-1((d 一 1)a(1 β + 2β + 3) + 1)ω
≤ e2Bβd-1((d - 1)a(eβ + 2eβ + 3) + 1)σ,2hln(16h) ≤ Y
if we choose
_	Y
σ —	—	~	~	,	.
4e2Bβd-1((d - 1)a( d β + 2eβ + 3) + 1)√2h ln(16h)
Let
EkW∙kF = ∣a∣F + ∣B∣F + ku kF + IWokF
Then
KL(A+ΔA,B+ΔB,U+ΔU,Wo+ΔWo)kP) ≤
P ∣A,B,U,Wo∣F
2σ2
16e4B2β2d-2 ((d - 1)a( d β + 2eβ + 3) + 1)22h ln(16h)
Y2
EkW∙kF
≤
16e2B2β2d-2((d - 1)a(dβ + 2β + 3) + 1)22hln(16h)
Y2
EkW∙kF
Now, instantiating Lemma 1 above, we have that with probability at least 1 一 δ,
Lo(f) ≤ LY(f) + 4 J
16e2B2β2d-2((d - 1)α(dβ + 2β + 3) + 1)22hln(16h) P ∣∣W∙kF
Y2 (M 一 1)
+ ln(第
+ M - 1
Following Neyshabur et al. (2018), we need to take a union bound over β so that the above result
holds for all β . Observe that we only need to consider β in the range
≤β≤
(γ√M
(2B
1/d
If βd ≤ γm then |f (x)| ≤ βdB ≤ γ2 so LY(f) > 1. If βd ≥ γ√M then the second term in the
first term in the numerator is greater than 1 so the result holds trivially. So ∣β 一 β∣ ≤ d (2B)1/d
guarantees ∣β 一 β∣ ≤ dβ in this range. So we can use a cover of size dM1/2d, just as in Neyshabur
et al. (2018).
17
Published as a conference paper at ICLR 2021
F Additional monDEQ Lipschitz bound results
0	5	10	15	20
strong-monotonicity param. m
32
rorre tset %
0	5	10	15	20
strong-monotonicity param. m
(a)	Width 40 monDEQs shown in Figure 1b
(b)	Convolutional monDEQs shown in Figure 1d
Figure 5:	Test error for the monDEQs from Section 5.1
lu-suoɔ ZEqOSd"
20	40	60	80	100
width
lu-suoɔ ZEqOSd"
12.5 -
10.0 -
7.5 -
5.0 -
20	40	60	80	100
width
(a)	DNN bounds vs Width
(b)	monDEQ bounds vs Width
Figure 6:	Evaluating Lipschitz bounds as a function of width. lb: lower bound; ub: upper bound.
(a) Forward-Backward unrolled Lipschitz bounds (b) Peaceman-Rachford unrolled Lipschitz bounds
Figure 7:	Lipschitz bounds for monDEQ by unrolling forward-backward or Peaceman-Rachford,
for a range of α.
18
Published as a conference paper at ICLR 2021
MNIST test error In Figure 5 we plot the test error of the width 40 monDEQs for which the
Lipschitz bounds are given in Figure 1b. We see that it increases from 2.4% for m = 0.5 to 4.2%
for m = 20. For comparison, the DNNs shown in Figure 1a have test error between 2.8% and 3.2%,
with no trend w.r.t. depth.
MNIST width experiments Figure 6 shows the lower and upper bounds for DNNs and monDEQs
of varying widths, as described in Section 5.1. Figure 6a is transcribed from Fazlyab et al. (2019)
figure 2(a).
CIFAR-10 CNN models The details of the CNN sm, CNN med, and CNN lg models used in
Section 5.1 are as follows:
CNN sm:
Layer #	Layer	# channels out	stride
1	Conv2D	32 ɪɪ	1
2	Conv2D	32	2
3	Linear	100	
4	Linear	10	
CNN med:
Layer #	Layer	# channels out	stride
1	Conv2D	32	1
2	Conv2D	32	2
3	Conv2D	64	1
4	Conv2D	64	2
5	Linear	100	
6	Linear	10	
CNN lg:
Layer #	Layer	# channels out	stride
1	Conv2D	32	1
2	Conv2D	32	2
3	Conv2D	64	1
4	Conv2D	64	2
5	Conv2D	128	1
6	Conv2D	128	2
7	Linear	100	
8	Linear	10	
All models use ReLU activations and kernels of size 3.
Unrolling monDEQs The approximate upper bounds on monDEQ Lipschitz constants obtained
by unrolling the operator splitting methods are shown in Figure 7.
19
Published as a conference paper at ICLR 2021
G Additional monDEQ generalization bound results
We validate that both the monDEQ and DNN models considered in Section 5.2 for generalization
bound comparisons indeed achieve comparable accuracy as well. Figures 8a, 8b illustrate this: we
can observe that the various models considered all achieve test error in the same range.
Next, we compute generalization bounds for unrolled monDEQs (Section E) for a variety of α
values, to get a sense of the quality of our generalization bound for the monDEQ. In Figure 8c, we
can observe that the generalization bounds for these unrolled networks are larger (on the order of
106) as compared to the generalization bound for the monDEQ (on the order 105). This shows that
our bound is tighter.
.5 .0
77
rre tset
• DNNs
---monDEQ, m=20
6.5 -
4	6	8	10	12
depth
(a)	DNN Test error vs Depth
O 5
7 6
jjə-səl
6.0 -
0	5	10	15	20
strong-monotonicity param m
(b)	monDEQ Test error vs m
(c) Forward-Backward unrolled generalization bound
Figure 8: Test error for DNNs and monDEQs, and monDEQ generalization bounds by unrolling
forward-backward iterations.
20
Published as a conference paper at ICLR 2021
H Additional adversarial robustness results
——CNN d=2, L=42.41, 34.46%
CNN d=4, L=1554.01, 30.6%
-CNN d=6, L=5.79 X 107, 26.94%
——monDEQ m=0.1, L=806.56, 26.65%
-monDEQ m=0.5, L=250.64, 26.3%
——monDEQ m=l, L=159.96, 26.44%
monDEQ m=2, L=109.23, 27.29%
-monDEQ m=10, L=78.90, 30.46%
monDEQ m=20, L=51.19, 33.34%
(a) Certified Adversarial Robustness - In the legend,
we state the Lipschitz constant of each model, and its
(clean) test error
CNN d=2	  monDEQ	m=l
CNN d=4	monDEQ m=2
CNN d=6	---- monDEQ m=10
---monDEQ m=0.1	monDEQ m=20
---monDEQ m=0.5
(b) Adversarial robustness to attacks
Figure 9: Adversarial robustness of monDEQs as compared to CNNs
We run the same experiments that we ran in Section 5.3 on the CIFAR-10 dataset. We train single
convolution monDEQs (128 channels) with a range of m values, as well as 3 CNN models: CNN
sm, CNN med and CNN lg whose architectures are described in Appendix F above.
As in the case of the MNIST experiments, we see that varying m gives a range of trade offs between
clean accuracy and adversarial robustness. In terms of certified robustness, we see in Figure 9a that
all of the monDEQ s besides m = 20 have both better clean and better robust accuracy than all the
CNNs (with the exception of the CNN with d = 2, which has much lower clean accuracy). In Figure
9b, in terms of empirical robustness, we see that m parameterizes a similar trade-off between robust
and clean accuracy. In fact, it is possible to choose m (e.g. m = 1 here), such that the monDEQ
outperforms all CNNs in terms of clean accuracy and at all attack sizes.
21