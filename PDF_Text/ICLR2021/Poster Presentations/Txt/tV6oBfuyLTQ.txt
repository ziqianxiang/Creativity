Published as a conference paper at ICLR 2021
Parameter-Based Value Functions
Francesco Faccio, Louis Kirsch & Jurgen Schmidhuber
The Swiss AI Lab IDSIA, USI, SUPSI
{francesco,louis,juergen}@idsia.ch
Ab stract
Traditional off-policy actor-critic Reinforcement Learning (RL) algorithms learn
value functions of a single target policy. However, when value functions are up-
dated to track the learned policy, they forget potentially useful information about
old policies. We introduce a class of value functions called Parameter-Based Value
Functions (PBVFs) whose inputs include the policy parameters. They can gener-
alize across different policies. PBVFs can evaluate the performance of any policy
given a state, a state-action pair, or a distribution over the RL agent’s initial states.
First we show how PBVFs yield novel off-policy policy gradient theorems. Then
we derive off-policy actor-critic algorithms based on PBVFs trained by Monte
Carlo or Temporal Difference methods. We show how learned PBVFs can zero-
shot learn new policies that outperform any policy seen during training. Finally
our algorithms are evaluated on a selection of discrete and continuous control tasks
using shallow policies and deep neural networks. Their performance is compara-
ble to state-of-the-art methods.
1	Introduction
Value functions are central to Reinforcement Learning (RL). For a given policy, they estimate the
value of being in a specific state (or of choosing a particular action in a given state). Many RL
breakthroughs were achieved through improved estimates of such values, which can be used to find
optimal policies (Tesauro, 1995; Mnih et al., 2015). However, learning value functions of arbitrary
policies without observing their behavior in the environment is not trivial. Such off-policy learning
requires to correct the mismatch between the distribution of updates induced by the behavioral policy
and the one we want to learn. Common techniques include Importance Sampling (IS) (Hesterberg,
1988) and deterministic policy gradient methods (DPG) (Silver et al., 2014), which adopt the actor-
critic architecture (Sutton, 1984; Konda & Tsitsiklis, 2001; Peters & Schaal, 2008).
Unfortunately, these approaches have limitations. IS suffers from large variance (Cortes et al., 2010;
Metelli et al., 2018; Wang et al., 2016) while traditional off-policy actor-critic methods introduce
off-policy objectives whose gradients are difficult to follow since they involve the gradient of the
action-value function with respect to the policy parameters VθQπθ (s, a) (Degris et al., 2012; Silver
et al., 2014). This term is usually ignored, resulting in biased gradients for the off-policy objective.
Furthermore, off-policy actor-critic algorithms learn value functions of a single target policy. When
value functions are updated to track the learned policy, the information about old policies is lost.
We address the problem of generalization across many value functions in the off-policy setting by
introducing a class of parameter-based value functions (PBVFs) defined for any policy. PBVFs are
value functions whose inputs include the policy parameters, the PSSVF V (θ), PSVF V (s, θ), and
PAVF Q(s, a, θ). PBVFs can be learned using Monte Carlo (MC) (Metropolis & Ulam, 1949) or
Temporal Difference (TD) (Sutton, 1988) methods. The PAVF Q(s, a, θ) leads to a novel stochastic
and deterministic off-policy policy gradient theorem and, unlike previous approaches, can directly
compute VθQπθ (s, a). Based on these results, we develop off-policy actor-critic methods and com-
pare our algorithms to two strong baselines, ARS and DDPG (Mania et al., 2018; Lillicrap et al.,
2015), outperforming them in some environments.
We make theoretical, algorithmic, and experimental contributions: Section 2 introduces the stan-
dard MDP setting; Section 3 formally presents PBVFs and derive algorithms for V (θ), V (s, θ) and
Q(s, a, θ); Section 4 describes the experimental evaluation using shallow and deep policies; Sec-
tions 5 and 6 discuss related and future work. Proofs and derivations can be found in Appendix A.2.
1
Published as a conference paper at ICLR 2021
2	Background
We consider a Markov Decision Process (MDP) (Stratonovich, 1960; Puterman, 2014) M =
(S, A, P, R,γ,μo) where at each step an agent observes a state S ∈ S, chooses action a ∈ A,
transitions into state s0 with probability P(s0|s, a) and receives a reward R(s, a). The agent starts
from an initial state, chosen with probability μo(s). It is represented by a parametrized stochastic
policy πθ : S → ∆(A), which provides the probability of performing action a in state s. Θ is
the space of policy parameters. The policy is deterministic if for each state s there exists an ac-
tion a such that ∏θ(a|s) = 1. The return Rt is defined as the cumulative discounted reward from
time step t: Rt = PkT=-0t-1 γkR(st+k+1, at+k+1), where T denotes the time horizon and γ a real-
valued discount factor. The performance of the agent is measured by the cumulative discounted
expected reward (expected return), defined as J(πθ) = Eπθ [R0]. Given a policy πθ, the state-value
function Vπθ (s) = Eπθ [Rt|st = s] is defined as the expected return for being in a state s and
following policy πθ . By integrating over the state space S, we can express the maximization of
the expected cumulative reward in terms of the state-value function J(∏θ) = JS μo(s)Vπθ(S) ds.
The action-value function Qπθ (s, a), which is defined as the expected return for performing action
a in state S, and following the policy πθ, is Qπθ (S, a) = Eπθ [Rt|St = S, at = a], and it is re-
lated to the state-value function by Vπθ (S) = JA ∏θ(a∣s)Qπθ (s,a)da. We define as dπθ (s0) the
discounted weighting of states encountered starting at so 〜μo(s) and following the policy ∏θ:
dπθ (s0) = Rs P∞=i Yt-1μo(S)P(S → s0,t,∏θ) ds, where P(S → s0,t,∏θ) is the probability of
transitioning to S0 after t time steps, starting from s and following policy πθ. Sutton et al. (1999)
showed that, for stochastic policies, the gradient of J(πθ) does not involve the derivative of dπθ (S)
and can be expressed in a simple form:
Vθ
J(πθ) =	dπθ (S)
Vθ∏θ(a∣S)Qπθ (s, a) dadS.
Similarly, for deterministic policies Silver et al. (2014) obtained the following:
Vθ
J(πθ) = S
dπθ (s)Vθ∏θ(S)VaQπθ (S, a)∣a=∏θ(s) dS.
(1)
(2)
Off-policy RL In off-policy policy optimization, we seek to find the parameters of the policy
maximizing a performance index Jb(πθ) using data collected from a behavioral policy πb. Here the
objective function Jb(πθ) is typically modified to be the value function of the target policy, integrated
over dπ∞b (S) = limt→∞ P(St = S|S0, πb), the limiting distribution of states under πb (assuming it
exists) (Degris et al., 2012; Imani et al., 2018; Wang et al., 2016). Throughout the paper we assume
that the support of d∞∞ includes the support of μo so that the optimal solution for Jb is also optimal
for J. Formally, we want to find:
Jb(∏θ*)
max
θ
dπ∞b(S)Vπθ(S)dS
max
θ
dπ∞b(S)
∏θ(a∣S)Qπθ (s, a) dadS.
(3)
Unfortunately, in the off-policy setting, the states are obtained from dπ∞b and not from dπ∞θ, hence the
gradients suffer from a distribution shift (Liu et al., 2019; Nachum et al., 2019). Moreover, since
we have no access to dπ∞θ, a term in the policy gradient theorem corresponding to the gradient of
the action value function with respect to the policy parameters needs to be estimated. This term is
usually ignored in traditional off-policy policy gradient theorems1 . In particular, when the policy is
stochastic, Degris et al. (2012) showed that:
S
vθ JbE )= JS d∞∞(S) JA πb(alS)年
(Qπθ (s, a)Vθ log∏θ(a∣S) + VθQπθ (s, a)) dadS
(4)
≈ ZS d∞∞(S) ZA πb(alS) πθ≡(Qπθ Ga)Vθlog *S)) da dS.
(5)
Analogously, Silver et al. (2014) provided the following approximation for deterministic policies 2:
Vθ Jb(∏θ ) = [d∞∞ (s) (Vθ ∏θ (S)VaQπθ (S,a)∣a=∏θ (s) + Vθ Qπθ (S,a)∣α=∏θ (S)) dS (6)
≈ (d∞∞(s) (Vθ∏θ(S)VaQπθ(S,a)∣ɑ=∏θ(s)) dS.
(7)
1With tabular policies, dropping this term still results in a convergent algorithm (Degris et al., 2012).
2In the original formulation of Silver et al. (2014) dπ∞b (s) is replaced by dπb (s).
2
Published as a conference paper at ICLR 2021
Although the term VθQπθ (s, a) is dropped, there might be advantages in using the approximate
gradient of Jb in order to find the maximum of the original RL objective J. Indeed, if we were
on-policy, the approximated off-policy policy gradients by Degris et al. (2012); Silver et al. (2014)
would revert to the on-policy policy gradients, while an exact gradient for Jb would necessarily
introduce a bias. However, when we are off-policy, it is not clear whether this would be better than
using the exact gradient of Jb in order to maximize J. In this work, we assume that Jb can be
considered a good objective for off-policy RL and we derive an exact gradient for it.
3	Parameter-based Value Functions
In this section, we introduce our parameter-based value functions, the PSSVF V (θ), PSVF V (s, θ),
and PAVF Q(s, a, θ) and their corresponding learning algorithms. First, we augment the state and
action-value functions, allowing them to receive as an input also the weights of a parametric pol-
icy. The parameter-based state-value function (PSVF) V (s, θ) = E[Rt|st = s, θ] is defined as
the expected return for being in state s and following policy parameterized by θ. Similarly, the
parameter-based action-value function (PAVF) Q(s, a, θ) = E[Rt|st = s, at = a, θ] is defined as
the expected return for being in state s, taking action a and following policy parameterized by θ. Us-
ing PBVFs, the RL objective becomes: JE) = JS μo(s)Vπ(s, θ) ds. Maximizing this objective
leads to on-policy policy gradient theorems that are analogous to the traditional ones (Sutton et al.,
1999; Silver et al., 2014):
Theorem 3.1.	Let πθ be stochastic. For any Markov Decision Process, the following holds:
Vθ J(∏θ) = Es〜d∏θ(s),a〜∏θ(.∣s) [(Q(s,a,θ)Vθ log∏(a|s))].	(8)
Theorem 3.2.	Let πθ be deterministic. Under standard regularity assumptions (Silver et al., 2014),
for any Markov Decision Process, the following holds:
VθJ(∏θ) = Es〜d∏θ(s) [VaQ(s,α,θ)∣a=∏θ(s)Vθ∏θ(s)] .	(9)
Parameter-based value functions allow us also to learn a function of the policy parameters that
directly approximates J(πθ). In particular, the parameter-based start-state-value function (PSSVF)
is defined as:
V(θ) := Es〜μo(s)[V(s, θ)] = μ μo(S)V(s,θ)ds = J(πθ).
S
(10)
Off-policy RL
Jb(∏θ*)
In the off-policy setting, the objective to be maximized becomes:
max	dπ∞b (s)V (s, θ) ds = max	dπ∞b (s)πθ (a|s)Q(s, a, θ) da ds.	(11)
By taking the gradient of the performance Jb with respect to the policy parameters θ we obtain novel
policy gradient theorems. Since θ is continuous, we need to use function approximators Vw(θ) ≈
V (θ), Vw(s, θ) ≈ V(s, θ) and Qw(s, a, θ) ≈ Q(s, a, θ). Compatible function approximations can
be derived to ensure that the approximated value function is following the true gradient. Like in
previous approaches, this would result in linearity conditions. However, here we consider nonlinear
function approximation and we leave the convergence analysis of linear PBVFs as future work. In
episodic settings, we do not have access to dπ∞b, so in the algorithm derivations and in the experiments
we approximate it by sampling trajectories generated by the behavioral policy. In all cases, the
policy improvement step can be very expensive, due to the computation of the arg max over a
continuous space Θ. Actor-critic methods can be derived to solve this optimization problem, where
the critic (PBVFs) can be learned using TD or MC methods, while the actor is updated following the
gradient with respect to the critic. Although our algorithms on PSSVF and PSVF can be used with
both stochastic and deterministic policies, removing the stochasticity of the action-selection process
might facilitate learning the value function. All our algorithms make use of a replay buffer.
3.1	PARAMETER-BASED START-STATE-VALUE FUNCTION V(θ)
We first derive the PSSVF V (θ). Given the original performance index J, and taking the gradient
with respect to θ, we obtain:
Vθ
J(πθ) =
μo(s)VθV(s,θ) ds
Es~μo(s) [VθV(s, θ)]
VθV(θ).
(12)
3
Published as a conference paper at ICLR 2021
In Algorithm 1, the critic Vw(θ) is learned using MC to estimate the value of any policy θ. The
actor is then updated following the direction of improvement suggested by the critic. Since the main
application of PSSVF is in episodic tasks3 , we optimize for the undiscounted objective.
Algorithm 1 Actor-critic with Monte Carlo prediction for V (θ)
Input: Differentiable critic Vw : Θ → R with parameters w; deterministic or stochastic actor
πθ with parameters θ; empty replay buffer D
Output: Learned Vw ≈ V(θ)∀θ, learned ∏θ ≈ ∏θ*
Initialize critic and actor weights w, θ
repeat:
Generate an episode s0, a0, r1, s1, a1, r2, . . . , sT-1, aT-1, rT with policy πθ
Compute return r = PkT=1 rk
Store (θ, r) in the replay buffer D
for many steps do:
Sample a batch B = {(r, θ)} from D
Update critic by stochastic gradient descent: RW E(r,θ)∈B [r - Vw(θ)]2
end for
for many steps do:
Update actor by gradient ascent: RθVw(θ)
end for
until convergence
3.2	PARAMETER-BASED STATE-VALUE FUNCTION V (s, θ)
Learning the value function using MC approaches can be difficult due to the high variance of the
estimate. Furthermore, episode-based algorithms like Algorithm 1 are unable to credit good actions
in bad episodes. Gradient methods based on TD updates provide a biased estimate of V (s, θ) with
much lower variance and can credit actions at each time step. Taking the gradient of Jb(πθ) in the
PSVF formulation4, we obtain:
Vθ Jb(∏θ) = JS d∞(SRθV(s,θ) ds = Es〜d∞bRV©V(s,θ)].	(13)
Algorithm 2 (Appendix) uses the actor-critic architecture, where the critic is learned via TD5.
3.3	PARAMETER-BASED ACTION-VALUE FUNCTION Q(s, a, θ)
The introduction of the PAVF Q(s, a, θ) allows us to derive new policy gradients theorems when
using a stochastic or deterministic policy.
Stochastic policy gradients We want to use data collected from some stochastic behavioral policy
πb in order to learn the action-value of a target policy πθ . Traditional off-policy actor-critic algo-
rithms only approximate the gradient of Jb, since they do not estimate the gradient of the action-
value function with respect to the policy parameters VθQπθ (s, a) (Degris et al., 2012; Silver et al.,
2014). With PBVFs, we can directly compute this contribution to the gradient. This yields an exact
policy gradient theorem for Jb :
Theorem 3.3.	For any Markov Decision Process, the following holds:
vθ Jb(πθ ) = Es 〜d∞b (s),a 〜∏b(.∣s) ∏θ(als) (Q(s,α, θ)vθ log πθ ⑷S) + vθ Q(s,α,θ)) .	(14)
Algorithm 3 (Appendix) uses an actor-critic architecture and can be seen as an extension of Off-
PAC (Degris et al., 2012) to PAVF.
3Alternatives include regenerative method for MC estimation (Rubinstein & Kroese, 2016).
4Compared to standard methods based on the state-value function, we can directly optimize the policy
following the performance gradient of the PSVF, obtaining a policy improvement step in a model-free way.
5 Note that the differentiability of the policy πθ is never required in PSSVF and PSVF.
4
Published as a conference paper at ICLR 2021
Deterministic policy gradients Estimating Q(s, a, θ) is in general a difficult problem due to the
stochasticity of the policy. Deterministic policies of the form π : S → A can help improving
the efficiency in learning value functions, since the expectation over the action space is no longer
required. Using PBVFs, we can write the performance of a policy πθ as:
Jb(πθ) =	dπ∞b (s)V (s, θ) ds =	dπ∞b (s)Q(s, πθ (s), θ) ds.
(15)
Taking the gradient with respect to θ we obtain a deterministic policy gradient theorem:
Theorem 3.4.	Under standard regularity assumptions (Silver et al., 2014), for any Markov Decision
Process, the following holds:
VθJb(πθ) = Es〜d∞b (S) [VaQ(s, a, θXa=∏θ(s)Vθπθ (S) + VθQ(S, a,刀 |a=n0(s)] .	(16)
Algorithm 4 (Appendix) uses an actor-critic architecture and can be seen as an extension of
DPG (Silver et al., 2014) to PAVF. Despite the novel formulation of algorithm 3, we decided to
avoid the stochasticity of the policy and to implement and analyze only the deterministic PAVF.
4	Experiments6
Applying algorithms 1, 2 and 4 directly can lead to convergence to local optima, due to the lack
of exploration. In practice, like in standard deterministic actor-critic algorithms, we use a noisy
version of the current learned policy in order to act in the environment and collect data to encourage
exploration. More precisely, at each episode We use ∏ with θ= θ + e, e 〜N(0, σ2I) instead of ∏
∙-v
and then store θ in the replay buffer. In our experiments, we report both for our methods as well as
the baselines the performance of the policy without parameter noise.
4.1	Visualizing PBVFs using LQRs
We start with an illustrative example that allows us to visualize how PBVFs are learning to estimate
the expected return over the parameter space. For this purpose, we use an instance of the 1D Linear
Quadratic Regulator (LQR) problem and a linear deterministic policy with bias. In figure 1, we plot
the episodic J (θ), the cumulative return that an agent would obtain by acting in the environment
using policy πθ for a single episode, and the cumulative return predicted by the PSSVF V (θ) for
two different times during learning. At the beginning of the learning process, the PSSVF is able
to provide just a local estimation of the performance of the agent, since only few data have been
observed. However, after 1000 episodes, itis able to provide a more accurate global estimate over the
parameter space. Appendix A.4.1 contains a similar visualization for PSVF and PAVF, environment
details and hyperparameters used.
4.2	Main results
Given the similarities between our PAVF and DPG, Deep Deterministic Policy Gradients (DDPG)
is a natural choice for the baseline. Additionally, the PSSVF V (θ) resembles evolutionary methods
as the critic can be interpreted as a global fitness function. Therefore, we decided to include in the
comparison Augmented Random Search (ARS) which is known for its state-of-the-art performance
using only linear policies in continuous control tasks. For the policy, we use a 2-layer MLP (64,64)
with tanh activations and a linear policy followed by a tanh nonlinearity. Figure 2 shows results
for deterministic policies with both architectures. In all the tasks the PSSVF is able to achieve at
least the same performance compared to ARS, often outperforming it. In the Inverted Pendulum
environment, PSVF and PAVF with deep policy are very slow to converge, but they excel in the
Swimmer task and MountainCarContinuous. In Reacher, all PBVFs fail to learn the task, while
DDPG converges quickly to the optimal policy. We conjecture that for this task it is difficult to
perform a search in parameter space. On the other hand, in MountainCarContinuous, the reward
is more sparse and DDPG only rarely observes positive reward when exploring in action space.
In Appendix A.4 we include additional results for PSSVF and PSVF with stochastic policies and
hyperparameters. We analyze the sensitivity of the algorithms on the choice of hyperparameters in
Appendix A.4.4.
6Code is available at: https://github.com/FF93/Parameter- based- Value- Functions
5
Published as a conference paper at ICLR 2021
S'
Optimization after 60 episodes
Optimization after 1000 episodes
J(θ)	VW(θ)
352
-32
-965
-2857
-4893
-6929
-8964
-11000
-5	-3	-11	3	-5	-3	-11	3
θι	θι
Figure 1: True episodic return J (θ) and PSSVF estimation V (θ) as a function of the policy pa-
rameters at two different stages in training. The red arrows represent an optimization trajectory in
parameter space. The blue dots represent the perturbed policies used to train V (θ).
4.3	Zero-shot learning
In order to test whether PBVFs are generalizing across the policy space, we perform the following
experiment with shallow deterministic policies: while learning using algorithm 1, we stop training
and randomly initialize 5 policies. Then, without interacting with the environment, we train these
policies offline, in a zero-shot manner, following only the direction of improvement suggested by
VθVw(θ), whose weights W remain frozen. We observe that shallow policies can be effectively
trained from scratch. Results for PSSVFs in Swimmer-v3 are displayed in figure 3. In particular,
we compare the performance of the policy learned, the best perturbed policy for exploration seen
during training and five policies learned from scratch at three different stages in training. We note
that after the PSSVF has been trained for 100,000 time steps interactions with the environment (first
snapshot), these policies are already able to outperform both the current policy and any policy seen
while training the PSSVF. They achieve an average return of 297, while the best observed return was
225. We include additional results for PSVF and PAVF in different environments, using shallow and
deep policies in Appendix A.4.2. When using deep policies, we obtain similar results only for the
simplest environments. For this task, we use the same hyperparameters as in figure 2.
4.4	Offline learning with fragmented behaviors
In our last experiment, we investigate how PSVFs are able to learn in a completely offline setting.
The goal is to learn a good policy in Swimmer-v3 given a fixed dataset containing 100,000 tran-
sitions, without additional environment interactions. Furthermore, the policy generating the data
is perturbed every 200 time steps, for a total of 5 policies per episode. Observing only incom-
plete trajectories for each policy parameter makes TD bootstrapping harder: In order to learn, the
PSVF needs to generalize across both the state and the parameter space. Given the fixed dataset, we
first train the PSVF, minimizing the TD error. Then, at different stages during learning, we train 5
new shallow deterministic policies. Figure 4 describes this process. We note that at the beginning
of training, when the PSVF V (s, θ) has a larger TD error, these policies have poor performance.
However, after 7000 gradient updates, they are able to achieve a reward of 237, before eventually
degrading to 167. They outperform the best policy in the dataset used to train the PSVF, whose
return is only of 58.
6
Published as a conference paper at ICLR 2021
HcIPPer-V3
0‰M	0.2OM	0ΛJM 0βlM 0«0M 1XK)M
,me steps
Shallow policies
Reacħer-v2
time steps
time steps
CartPole-vl
ars
PSvf
PSSvf
Cidpg
ρavf
0.8M	0X)2M	OMM OXXM OgM 0Δ0M
,me steps
DeeP policies
ReaCher-V2
HopPer-V3
2500
2000
0.02M	0.04M	0.0βM	0.08M	0.10M
irrκ
-40
OOTM
0X)2M	OMM OXXM OgM 0Δ0M
tinesteps
0X)2M	OMM OXXM OgM 0Δ0M
tine steps
Figure 2: Average return of shallow and deep deterministic policies as a function of the number of
time steps used for learning (across 20 runs, one standard deviation), for different environments and
algorithms. We use the best hyperparameters found when maximizing the average return.
5	Related work
There are two main classes of similar algorithms performing search in policy parameter space.
Evolutionary algorithms (Wierstra et al., 2014; Salimans et al., 2017; Mania et al., 2018) iteratively
estimate a fitness function evaluating the performance of a population of policies and then perform
gradient ascent in parameter space, often estimating the gradient using finite difference approxima-
tion. By replacing the performance of a population through a likelihood estimation, evolutionary
algorithms become a form of Parameter Exploring Policy Gradients (Sehnke et al., 2008; 2010).
Our methods are similar to evolution since our value function can be seen as a fitness. Unlike
evolution, however, our approach allows for obtaining the fitness gradient directly and is more
suitable for reusing past data. While direct V (θ) optimization is strongly related to evolution, our
more informed algorithms optimize V (s, θ) and Q(s, a, θ). That is, ours both perform a search in
policy parameter space AND train the value function and the policy online, without having to wait
for the ends of trials or episodes.
The second related class of methods involves surrogate functions (Box & Wilson, 1951;
Booker et al., 1998; Moore & Schneider, 1996). They often use local optimizers for generalizing
across fitness functions. In particular, Bayesian Optimization (BO) (Snoek et al., 2012; 2015) uses
a surrogate function to evaluate the performance of a model over a set of hyperparameters and
follows the uncertainty on the surrogate to query the new data to sample. Unlike BO, we do not
build a probabilistic model and we use the gradient of the value function instead of a sample from
the posterior to decide which policy parameters to use next in the policy improvement step.
The possibility of augmenting the value functions with auxiliary parameters was already
7
Published as a conference paper at ICLR 2021
Figure 3: Policies learned from scratch during training. The plot in the center represents the return
of the agent learning while interacting with the environment using Algorithm 1. We compare the
best noisy policy ∏ used for exploration to the policy ∏θ learned through the critic. The learning
curves in the small plots represent the return obtained by policies trained from scratch following the
fixed critic Vw(θ) after different time steps of training. The return of the closest policy (L2 distance)
in the replay buffer with respect to the policy learned from scratch is depicted in green.
rorre DT derauqs naeM
0
50	100
episodes
n buffer
MOt learned
0X 200	400	600	800 1k
×v gradient steps	/
0	200	400	600	800 1k
gradient steps
0	200	400	600	800 1k
gradient steps




I

Figure 4: Offline learning of PSVF. We plot the mean squared TD error of a PSVF trained using data
coming from a set of noisy policies. In the small plots, we compare the return obtained by policies
trained from scratch following the fixed critic Vw(s, θ) after different time steps of value function
training and the return of the best noisy policy used to train V.
considered in work on General Value Functions (Sutton et al., 2011), where the return is defined
with respect to an arbitrary reward function. Universal Value Function Approximators (Schaul et al.,
2015) extended this approach to learn a single value function V πθ (s, g), representing the value,
given possible agent goals g. In particular, they learn different embeddings for states and goals,
exploiting their common structure, and they show generalization to new unseen goals. Similarly,
our PSVF V (s, θ) is able to generalize to unseen policies, observing data for only a few (s, θ) pairs.
General and Universal Value Functions have not been applied to learn a single value function for
every possible policy.
Policy Evaluation Networks (PENs) (Harb et al., 2020) are closely related to our work and
share the same motivation. PENs focus on the simplest PSSVF V (θ) trained without an actor-critic
architecture. Like in some of our experiments, the authors show how following the direction of
improvement suggested by V (θ) leads to an increase in policy performance. They also suggest to
explore in future work a more complex setting where a PSVF V (s, θ) is learned using an actor-critic
8
Published as a conference paper at ICLR 2021
architecture. Our work directly introduces the PSVF V (s, θ) and PAVF Q(s, a, θ) and presents
novel policy gradient theorems for PAVFs when stochastic or deterministic policies are used. There
are many differences between our approach to learning V (θ) and theirs. For example, we do not use
a fingerprint mechanism (Harb et al., 2020) for embedding the weights of complex policies. Instead,
we simply parse all the policy weights as inputs to the value function, even in the nonlinear case.
Fingerprinting may be important for representing nonlinear policies without losing information
about their structure and for saving memory required to store the weights. Harb et al. (2020) focus
on the offline setting. They first use randomly initialized policies to perform rollouts and collect
reward from the environment. Then, once V (θ) is trained using the data collected, many gradient
ascent steps through V yield new, unseen, randomly initialized policies in a zero-shot manner,
exhibiting improved performance. They train their value function using small nonlinear policies
of one hidden layer and 30 neurons on Swimmer-v3. They evaluate 2000 deterministic policies on
500 episodes each (1 million policy evaluations), achieving a final expected return of ≈ 180 on new
policies trained from scratch through V. On the other hand, in our zero-shot learning experiment
using a linear PSSVF, after only 100 policy evaluations, we obtain a return of 297. In our main
experiments, we showed that a fingerprint mechanism is not necessary for the tasks we analyzed:
even when using a much bigger 2-layers MLP policy, we are able to outperform the results in PEN.
Although Harb et al. (2020) use Swimmer-v3 “to scale up their experiments”, our results suggest
that Swimmer-v3 does not conclusively demonstrate possible benefits of their policy embedding.
Gradient Temporal Difference (Sutton et al., 2009a;b; Maei et al., 2009; 2010; Maei, 2011)
and Emphatic Temporal Difference methods (Sutton et al., 2016) were developed to address
convergence under on-policy and off-policy (Precup et al., 2001) learning with function ap-
proximation. The first attempt to obtain a stable off-policy actor-critic algorithm under linear
function approximation was called Off-PAC (Degris et al., 2012), where the critic is updated using
GTD(λ) (Maei, 2011) to estimate the state-value function. This algorithm converges when using
tabular policies. However, in general, the actor does not follow the true gradient direction for
Jb. A paper on DPG (Silver et al., 2014) extended the Off-PAC policy gradient theorem (Degris
et al., 2012) to deterministic policies. This was coupled with a deep neural network to solve
continuous control tasks through Deep Deterministic Policy Gradients (Lillicrap et al., 2015). Imani
et al. (2018) used emphatic weights to derive an exact off-policy policy gradient theorem for Jb .
Differently from Off-PAC, they do not ignore the gradient of the action-value function with respect
to the policy, which is incorporated in the emphatic weighting: a vector that needs to be estimated.
Our off-policy policy gradients provide an alternative approach that does not need emphatic weights.
The widely used off-policy objective function Jb suffers the distribution shift problem. Liu
et al. (2019) provided an off-policy policy gradient theorem which is unbiased for the true RL
objective J(πθ), introducing a term dπ∞θ /dπ∞b that corrects the mismatch between the states distri-
butions. Despite their sound off-policy formulation, estimating the state weighting ratio remains
challenging. All our algorithms are based on the off-policy actor-critic architecture. The two
algorithms based on Q(s, a, θ) can be viewed as analogous to Off-PAC and DPG where the critic is
defined for all policies and the actor is updated following the true gradient with respect to the critic.
6	Limitations and future work
We introduced PBVFs, a novel class of value functions which receive as input the parameters of a
policy and can be used for off-policy learning. We showed that PBVFs are competitive to ARS and
DDPG (Mania et al., 2018; Lillicrap et al., 2015) while generalizing across policies and allowing for
zero-shot training in an offline setting. Despite their positive results on shallow and deep policies,
PBVFs suffer the curse of dimensionality when the number of policy parameters is high. Embed-
dings similar to those used in PENs (Harb et al., 2020) may be useful not only for saving memory
and computational time, but also for facilitating search in parameter space. We intend to evaluate the
benefits of such embeddings and other dimensionality reduction techniques. We derived off-policy
policy gradient theorems, showing how PBVFs follow the true gradient of the performance Jb . With
these results, we plan to analyze the convergence of our algorithms using stochastic approximation
techniques (Borkar, 2009) and test them on environments where traditional methods are known to
diverge (Baird, 1995). Finally, we want to investigate how PBVFs applied to supervised learning
tasks or POMDPs, can avoid BPTT by mapping the weights of an RNN to its loss.
9
Published as a conference paper at ICLR 2021
Acknowledgments
We thank PaUlo RaUber,Imanol Schlag, Miroslav StrUPL Robert Csordas, Aleksandar Stanic, Anand
Gopalakrishnan, Sjoerd Van Steenkiste and Julius Kunze for their feedback. This work was sup-
Ported by the ERC Advanced Grant (no: 742870). We also thank NVIDIA CorPoration for donating
a DGX-1 as Part of the Pioneers of AI Research Award and to IBM for donating a Minsky machine.
References
JoshUa Achiam. SPinning UP in DeeP Reinforcement Learning. 2018.
Leemon Baird. ResidUal algorithms: Reinforcement learning with fUnction aPProximation. In
Machine Learning Proceedings 1995, pp. 30-37. ElSevier, 1995.
Andrew J. Booker, J. E. Dennis, PaUl D. Frank, David B. Serafini, and Virginia Torczon. Optimiza-
tion Using Surrogate Objectives on a Helicopter Test Example, pp. 49-58. BirkhauSer Boston,
Boston, MA, 1998. ISBN 978-1-4612-1780-0. doi: 10.1007/978-1-4612-1780-0.3.
Vivek S Borkar. Stochastic approximation: a dynamical systems viewpoint, volUme 48. Springer,
2009.
G. E. P. Box and K. B. Wilson. On the experimental attainment of optimUm conditions. Journal of
the Royal Statistical Society. Series B (Methodological), 13(1):1-45, 1951. ISSN 00359246.
Corinna Cortes, Yishay MansoUr, and Mehryar Mohri. Learning boUnds for importance weighting.
In Advances in neural information processing systems, pp. 442-450, 2010.
Thomas Degris, Martha White, and Richard S. SUtton. Off-policy actor-critic. In Proceedings of the
29th International Coference on International Conference on Machine Learning, ICML’12, pp.
179-186, USA, 2012. Omnipress. ISBN 978-1-4503-1285-1.
Jean Harb, Tom SchaUl, Doina PrecUp, and Pierre-LUc Bacon. Policy evalUation networks. arXiv
preprint arXiv:2002.11833, 2020.
Timothy Classen Hesterberg. Advances in importance sampling. PhD thesis, Stanford University,
1988.
Ehsan Imani, Eric Graves, and Martha White. An off-policy policy gradient theorem Using emphatic
weightings. In Advances in Neural Information Processing Systems, pp. 96-106, 2018.
Max Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, David
Silver, and Koray KavUkcUoglU. DecoUpled neUral interfaces Using synthetic gradients. In Pro-
ceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 1627-1635.
JMLR. org, 2017.
Vijay Konda and John Tsitsiklis. Actor-critic algorithms. Society for Industrial and Applied Math-
ematics, 42, 04 2001.
Timothy P Lillicrap, Jonathan J HUnt, Alexander Pritzel, Nicolas Heess, Tom Erez, YUval Tassa,
David Silver, and Daan Wierstra. ContinUoUs control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Yao LiU, Adith Swaminathan, Alekh Agarwal, and Emma BrUnskill. Off-policy policy gradient with
state distribUtion correction. arXiv preprint arXiv:1904.08473, 2019.
Hamid R. Maei, Csaba Szepesvari, Shalabh Bhatnagar, Doina Precup, David Silver, and Richard S.
SUtton. Convergent temporal-difference learning with arbitrary smooth fUnction approxima-
tion. In Proceedings of the 22nd International Conference on Neural Information Processing
Systems, NIPS’09, pp. 1204-1212, Red Hook, NY, USA, 2009. CUrran Associates Inc. ISBN
9781615679119.
Hamid Reza Maei. Gradient temporal-difference learning algorithms. PhD thesis, University of
Alberta, 2011.
10
Published as a conference paper at ICLR 2021
Hamid Reza Maei, Csaba Szepesvari, Shalabh Bhatnagar, and Richard S. Sutton. Toward off-policy
learning control with function approximation. In Proceedings of the 27th International Confer-
ence on International Conference on Machine Learning, ICML’10, pp. 719-726, Madison, WL
USA, 2010. Omnipress. ISBN 9781605589077.
Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search of static linear policies is
competitive for reinforcement learning. In Advances in Neural Information Processing Systems,
pp. 1800-1809, 2018.
Alberto Maria Metelli, Matteo Papini, Francesco Faccio, and Marcello Restelli. Policy optimization
via importance sampling. In Advances in Neural Information Processing Systems, pp. 5442-5454,
2018.
N. Metropolis and S. Ulam. The monte carlo method. J. Am. Stat. Assoc., 44:335, 1949.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
Andrew W. Moore and Jeff G. Schneider. Memory-based stochastic optimization. In D. S. Touretzky,
M. C. Mozer, and M. E. Hasselmo (eds.), Advances in Neural Information Processing Systems 8,
pp. 1066-1072. MIT Press, 1996.
Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice:
Policy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019.
Jan Peters and Stefan Schaal. Natural actor-critic. Neurocomput., 71(7-9):1180-1190, March 2008.
ISSN 0925-2312. doi: 10.1016/j.neucom.2007.11.026.
Doina Precup, Richard S. Sutton, and Sanjoy Dasgupta. Off-policy temporal difference learning
with function approximation. In ICML, 2001.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John
Wiley & Sons, 2014.
Reuven Y. Rubinstein and Dirk P. Kroese. Simulation and the Monte Carlo Method. Wiley Publish-
ing, 3rd edition, 2016. ISBN 1118632168.
Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a
scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.
Tom Schaul, Dan Horgan, Karol Gregor, and David Silver. Universal value function approximators.
In Proceedings of the 32Nd International Conference on International Conference on Machine
Learning - Volume 37, ICML’15, pp. 1312-1320. JMLR.org, 2015.
Jurgen Schmidhuber. Networks adjusting networks. In Proceedings of” DistributedAdaPtive Neural
Information Processing”, pp. 197-208, 1990.
Frank Sehnke, Christian Osendorfer, Thomas RUckStieβ, Alex Graves, Jan Peters, and Jurgen
Schmidhuber. Policy gradients with parameter-based exploration for control. In Vera Kurkova,
Roman Neruda, and Jan Koutn´k (eds.), Artificial Neural Networks - ICANN 2008, pp. 387-396,
Berlin, Heidelberg, 2008. Springer Berlin Heidelberg. ISBN 978-3-540-87536-9.
Frank Sehnke, Christian Osendorfer, Thomas Ruckstieβ, Alex Graves, Jan Peters, and Jurgen
Schmidhuber. Parameter-exploring policy gradients. Neural Networks, 23(4):551-559, May
2010. ISSN 08936080. doi: 10.1016/j.neunet.2009.12.004.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In Proceedings of the 31st International Conference
on International Conference on Machine Learning - Volume 32, ICML’14, pp. I-387-I-395.
JMLR.org, 2014.
Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine
learning algorithms. In Advances in neural information Processing systems, pp. 2951-2959, 2012.
11
Published as a conference paper at ICLR 2021
Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram,
Md. Mostofa Ali Patwary, Prabhat Prabhat, and Ryan P. Adams. Scalable bayesian optimization
using deep neural networks. In Proceedings of the 32nd International Conference on International
Conference on Machine Learning - Volume 37,ICML'15, pp. 2171-2180.JMLR.org, 2015.
RL Stratonovich. Conditional Markov processes. Theory of Probability And Its Applications, 5(2):
156-178, 1960.
Richard S Sutton. Temporal Credit Assignment in Reinforcement Learning. PhD thesis, University
of Massachusetts Amherst, 1984.
Richard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3
(1):9-44, 1988.
Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods
for reinforcement learning with function approximation. In Proceedings of the 12th International
Conference on Neural Information Processing Systems, NIPS’99, pp. 1057-1063, Cambridge,
MA, USA, 1999. MIT Press.
Richard S Sutton, Hamid R Maei, and Csaba Szepesvari. A convergent o(n) temporal-difference
algorithm for off-policy learning with linear function approximation. In Advances in neural in-
formation processing systems, pp. 1609-1616, 2009a.
Richard S. Sutton, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba
Szepesvari, and Eric Wiewiora. Fast gradient-descent methods for temporal-difference learning
with linear function approximation. In Proceedings of the 26th Annual International Conference
on Machine Learning, ICML ’09, pp. 993-1000, New York, NY, USA, 2009b. Association for
Computing Machinery. ISBN 9781605585161. doi: 10.1145/1553374.1553501.
Richard S. Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M. Pilarski, Adam White,
and Doina Precup. Horde: A scalable real-time architecture for learning knowledge from unsu-
pervised sensorimotor interaction. In The 10th International Conference on Autonomous Agents
and Multiagent Systems - Volume 2, AAMAS ’11, pp. 761-768, Richland, SC, 2011. Interna-
tional Foundation for Autonomous Agents and Multiagent Systems. ISBN 0-9826571-6-1, 978-
0-9826571-6-4.
Richard S Sutton, A Rupam Mahmood, and Martha White. An emphatic approach to the problem
of off-policy temporal-difference learning. The Journal of Machine Learning Research, 17(1):
2603-2631, 2016.
Gerald Tesauro. Temporal difference learning and td-gammon. Commun. ACM, 38(3):58-68, March
1995. ISSN 0001-0782. doi: 10.1145/203330.203343.
Thomas Unterthiner, Daniel Keysers, Sylvain Gelly, Olivier Bousquet, and Ilya Tolstikhin. Predict-
ing neural network accuracy from weights. arXiv preprint arXiv:2002.11448, 2020.
Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu,
and Nando de Freitas. Sample efficient actor-critic with experience replay. arXiv preprint
arXiv:1611.01224, 2016.
Paul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the
IEEE, 78(10):1550-1560, 1990.
Daan Wierstra, Tom SChaUL Tobias Glasmachers, Yi Sun, Jan Peters, and Jurgen Schmidhuber.
Natural evolution strategies. The Journal of Machine Learning Research, 15(1):949-980, 2014.
12
Published as a conference paper at ICLR 2021
A Appendix
Index of the appendix
In the following, we briefly recap the contents of the appendix.
•	Appendix A.1 contains additional related works
•	Appendix A.2 reports all proofs and derivations.
•	Appendix A.3 illustrates implementation details and pseudocode.
•	Appendix A.4 provides the hyperparameters used in the experiments and further results.
A. 1 Additional related works
Recent work (Unterthiner et al., 2020) shows how to map the weights of a trained Convolutional
Neural Network to its accuracy. Experiments show how these predictions allow for performance
rankings of neural networks on new unseen tasks. These maps are either learned by taking the
flattened weights as input or using simple statistics. However, these predictions do not guide the
training process of CNNs.
In 1990, adaptive critics trained by TD were used to predict the gradients of an RNN from
its activations (Schmidhuber, 1990), avoiding backpropagation through time (BPTT) (Werbos,
1990). This idea was later used to update the weights of a neural network asynchronously (Jaderberg
et al., 2017). In our work, the critic is predicting errors instead of gradients. If applied to POMDPs,
or supervised learning tasks involving long time lags between relevant events, the PSSVF could
avoid BPTT by viewing the parameters of an RNN as a static object and mapping them to their loss
(negative reward).
Additional differences between our work and Policy Evaluation Networks (PENs) (Harb
et al., 2020) concern the optimization problem: we do not predict a bucket index for discretized
reward, but perform a regression task. Therefore our loss is simply the mean squared error between
the prediction of V (θ) and the reward obtained by πθ , while their loss (Harb et al., 2020) is
the KL divergence between the predicted and target distributions. Both approaches optimize the
undiscounted objective when learning V (θ).
A.2 Proofs and derivations
Theorem 3.1.	Let πθ be stochastic. For any Markov Decision Process, the following holds:
Vθ J (∏θ)=
Es〜dπθ (s),a-∏θ (. | s) [(Q(s,a,θ)Vθ log∏θ(a|s))].
(8)
Proof. The proof follows the standard approach by Sutton et al. (1999) and we report it for com-
pleteness. We start by deriving an expression for VθV (s, θ):
13
Published as a conference paper at ICLR 2021
VθV(s,θ) = Vθ / ∏θ(a∣s)Q(s, a,θ)da = / Vθ∏θ(a∣s)Q(s, a,θ) + ∏(a∣s)VθQ(s, a,θ) da
/ Vθ∏θ(a∣s)Q(s, a,
A
I Vθ∏θ(a∣s)Q(s, a,
A
I Vθ∏θ(a∣s)Q(s, a,
A
θ) + ∏θ (a∣s)Vθ(R(s,a) + Y / P (s0∣s,a)V (s0,θ)ds0) da
θ) + ∏θ(a∣s)γ J P(Sls,a)VθV(s0,θ)ds0da
θ) + ∏θ(a∣s)γ JP(s0∣s,a)×
X V Vθ∏θ(a0∣s0)Q(s0,a0,θ) + ∏(a0∣s0)γ P P(s ∣s0,a0)VθV(s ,θ)ds da0ds0da
AS
Z X γtP (s → s0, t, πθ) Z Vθ πθ (a|s0)Q(s0, a, θ) dads0.
S t=0	A
Taking the expectation with respect to so 〜μo(s) We have:
Vθ J(θ)= Vθ [ μo(s)V(s,θ)ds = f μo(s)VθV(s,θ)ds
SS
∞
=J μo (s) J ɪ2 YtP(s → s0,t,∏θ) J Vθ∏θ (a∣s)Q(s, a, θ) ds0 da ds
=	dπθ (s)	Vθ πθ (a|s)Q(s, a, θ) dads
=Es~dπθ (s),a~∏θ(.|s) [(Q(s,a,θ)Vθ log ∏θ(a|s))].
□
Theorem 3.2.	Let πθ be deterministic. Under standard regularity assumptions (Silver et al., 2014),
for any Markov Decision Process, the following holds:
Vθ J (∏θ ) = Es 〜d∏θ (S) [VɑQ(s,a,θ)∣ɑ=∏θ (s)Vθ ∏θ (s)] .	(9)
Proof. The proof follows the standard approach by Silver et al. (2014) and we report it for com-
pleteness. We start by deriving an expression for VθV(s, θ):
VθV(s, θ) =VθQ(s,πθ(s),θ) =Vθ R(s, πθ(s)) + Y	P (s0|s, πθ(s))V (s0, θ) ds0
=vΘ πθ (S)VaR(S,a)la=∏θ(s) +
+ Y/ P(s0∣s,∏θ(s))VθV(s0,θ) + Vθ∏θ(S)VaP(S0∣s,a)∣ɑ=∏θ(s) ds0
=Vθ ∏θ (s)Va(R(s, a)+ γ Js P (s0ls, a)V(s0,θ) ds0) ∣a=∏θ(s) +
+Y	P (s0|s, πθ(s))VθV (s0, θ) ds0
=Vθ∏θ(S)VaQ(s,a,θ)∣α=∏θ(s) + Y/ P(Sls,∏θ(s))VθV(s0,θ)ds0
=Vθ ∏θ (S)VaQ(S,a,θ)∣α=∏θ (s) +
+ YJ P (s0∣s,∏θ (s))Vθ ∏θ (s0)VaQ(s0,a, Θ)∣a=∏θ (so) ds0 +
+Y	P(S0|S, πθ(S))Y	P(S00 |S0, πθ(S0))VθV(S00, θ) dS00 dS0
SS
∞
X X YtP(s → s0,t,∏θ)Vθ∏θ(s0)VaQ(s0,a,θ)∣a=∏θ(so) ds0
S t=0
14
Published as a conference paper at ICLR 2021
Taking the expectation with respect to so 〜μo(s) We have:
VθJ(θ) = Vθ ∕μo(s)V(s,θ)ds = f μo(s)VθV(s,θ)ds
SS
=/ μo(s) / X YP(S → s0,t,∏θ)Vθ∏θ(s0)VaQ(s0,a,θ)∣α=∏θ(sθ) ds0ds
=d dπθ (s)Vθ ∏θ (S)VaQ(s,a,θ)∣ɑ=∏θ (S) ds
S
=Es 〜d∏θ (s) [Vθ ∏θ (S)VaQ(s, a, Θ)∣a=∏θ (s)]
□
Theorem 3.3.	For any Markov Decision Process, the following holds:
vΘ Jb(πθ ) = Es 〜d：b (s), a 〜πb (.|s) πθ (a||s) (Q(S,a,0)V6 log πθ ⑷S) + vΘ Q(S,a,θ)) .	(14)
8 … 八 I / L∏b(a∣S)	J
Proof.
VθJb(πθ) = Vθ dπ∞b (S)V (S, θ) dS
Vθ dπ∞b (S)
Zdπ∞b(S)Z [Q(
πθ (a|S)Q(S, a, θ) dadS
S, a, θ)Vθ∏θ(a∣S) + ∏θ(a∣S)VθQ(s, a, θ)] dadS
ZS d∞(S) ZA ∏b≡ πθ(a|s)[Q(s, a,θ)vθ log πθ(a1s)+vθ q(s, a,θ)]da ds
∏θ (a∣S)
.∏b(a∣S)
(Q(s, a,θ)Vθ log ∏θ(a|s) + VθQ(s, a, θ))
(17)
(18)
(19)
(20)
(21)
□
Theorem 3.4.	Under standard regularity assumptions (Silver et al., 2014), for any Markov Decision
Process, the following holds:
vθ Jb(πθ ) = Es 〜d∞b (s) RaQ(S, a, θ)la=∏θ(s)vθ πθ (S) + vθQ(S, a, θ)la=∏θ(s)] .	(16)
Proof.
VθJb(πθ) =	dπ∞b(S)VθQ(S,πθ(S),θ)dS
S
=/ d∞(s) [VaQ(s,a,θ)∣a=∏θ(s)Vθ∏θ(s) + VθQ(s,a,θ)∣a=∏θ(s)] ds
S
=Es 〜d∞b (s) [VaQ(S, a, θ)la=∏θ (s)vθ πθ (s) + vθ Q(S, a,θ)la=∏θ (s)]
(22)
(23)
(24)
□
A.3 Implementation details
A.3.1
In this appendix, we report the implementation details for PSSVF, PSVF, PAVF and the baselines.
We specify for each hyperparameter, which algorithms and tasks are sharing them.
Shared hyperparameters:
15
Published as a conference paper at ICLR 2021
•	Deterministic policy architecture (continuous control tasks): We use three different deter-
ministic policies: a linear mapping between states and actions; a single-layer MLP with
32 neurons and tanh activation; a 2-layers MLP (64,64) with tanh activations. All policies
contain a bias term and are followed by a tanh nonlinearity in order to bound the action.
•	Deterministic policy architecture (discrete control tasks): We use three different determin-
istic policies: a linear mapping between states and a probability distribution over actions;
a single-layer MLP with 32 neurons and tanh activation; a 2-layers MLP (64,64) with tanh
activations. The deterministic action a is obtained choosing a = arg max ∏θ (a|s). All
policies contain a bias term.
•	Stochastic policy architecture (continuous control tasks): We use three different stochastic
policies: a linear mapping; a single-layer MLP with 32 neurons and tanh activation; a 2-
layers MLP (64,64) with tanh activations all mapping from states to the mean of a Normal
distribution. The variance is state-independent and parametrized as c2ω with diagonal Ω.
All policies contain a bias term. Actions sampled are given as input to a tanh nonlinearity
in order to bound them in the action space.
•	Stochastic policy architecture (discrete control tasks): We use three different deterministic
policies: a linear mapping between states and a probability distribution over actions; a
single-layer MLP with 32 neurons and tanh activation; a 2-layers MLP (64,64) with tanh
activations. All policies contain a bias term.
•	Policy initialization: all weights and biases are initialized using the default Pytorch initial-
ization for PBVFs and DDPG and are set to zero for ARS.
•	Critic architecture: 2-layers MLP (512,512) with bias and ReLU activation functions for
PSVF, PAVF; 2-layers MLP (256,256) with bias and ReLU activation functions for DDPG.
•	Critic initialization: all weights and biases are initialized using the default Pytorch initial-
ization for PBVFs and DDPG.
•	Batch size: 128 for DDPG, PSVF, PAVF; 16 for PSSVF.
•	Actor’s frequency of updates: every episode for PSSVF; every batch of episodes for ARS;
every 50 time steps for DDPG, PSVF, PAVF.
•	Critic’s frequency of updates: every episode for PSSVF; every 50 time steps for DDPG,
PSVF, PAVF.
•	Replay buffer: the size is 100k; data are sampled uniformly.
•	Optimizer: Adam for PBVFs and DDPG.
Tuned hyperparameters:
•	Number of directions and elite directions for ARS ([directions, elite directions]): tuned
with values in [[1, 1], [4, 1], [4, 4], [16, 1], [16, 4], [16, 16]].
•	Policy’s learning rate: tuned with values in [1e - 2, 1e - 3, 1e - 4].
•	Critic’s learning rate: tuned with values in [1e - 2, 1e - 3, 1e - 4].
•	Noise for exploration: the perturbation for the action (DDPG) or the parameter is sampled
from N (0, σI) with σ tuned with values in [1, 1e - 1] for PSSVF, PSVF, PAVF; [1e -
1, 1e - 2] for DDPG; [1, 1e - 1, 1e - 2, 1e - 3] for ARS. For stochastic PSSVF and PSVF
we include also the value σ = 0, although it almost never results optimal.
Environment hyperparameters:
•	Environment interactions: 1M time steps for Swimmer-v3 and Hopper-v3; 100k time steps
for all other environments.
•	Discount factor for TD algorithms: 0.999 for Swimmer; 0.99 for all other environments.
•	Survival reward in Hopper: True for DDPG, PSVF, PAVF; False for ARS, PSSVF.
Algorithm-specific hyperparameters:
•	Critic’s number of updates: 50 for DDPG, 5 for PSVF and PAVF; 10 for PSSVF.
16
Published as a conference paper at ICLR 2021
•	Actor’s number of updates: 50 for DDPG, 1 for PSVF and PAVF; 10 for PSSVF.
•	Observation normalization: False for DDPG; True for all other algorithms.
•	Starting steps in DDPG (random actions and no training): first 1%.
•	Polyak parameter in DDPG: 0.995.
PAVF VθQ(s, a,θ) ablation We investigate the effect of the term VθQ(s, a,θ) in the off-policy
policy gradient theorem for deterministic PAVF. We follow the same methodology as in our main
experiments to find the optimal hyperparameters when updating using the now biased gradient:
Vθ Jb(∏θ) ≈ Es〜d∞b(S) [VaQ(s, a, Θ)∣a=∏θ(s)Vθ∏θ(s)] ,	(25)
which corresponds to the gradient that DDPG is following. Figure 5 reports the results for Hopper
and Swimmer using shallow and deep policies. We observe a significant drop in performance in
Swimmer when removing part of the gradient. In Hopper the loss of performance is less significant,
possibly because both algorithms tend to converge to the same sub-optimal behavior.
Figure 5: Performance of PAVF and biased PAVF (PAVF without the gradient of the action-value
function with respect to the policy parameters) using deterministic policies. We use the hyperparam-
eters maximizing average return and report the best hyperparameters found for the biased version in
Table 1. Learning curves are averaged over 20 seeds.
Table 1: Table of best hyperparameters for biased PAVFs
Learning rate policy	Policy:	[]	[64,64]
	Metric:	avg	avg
Swimmer-v3		1e-3	1e-4
Hopper-v3		1e-4	1e-4
Learning rate critic			
Swimmer-v3		1e-4	1e-4
Hopper-v3		1e-3	1e-3
Noise for exploration			
Swimmer-v3		1.0	1.0
Hopper-v3		0.1	0.1
ARS For ARS, we used the official implementation provided by the authors and we modified it
in order to use nonlinear policies. More precisely, we used the implementation of ARSv2-t (Mania
et al., 2018), which uses observation normalization, elite directions and an adaptive learning rate
based on the standard deviation of the return collected. To avoid divisions by zero, which may
happen if all data sampled have the same return, we perform the standardization only in case the
standard deviation is not zero. In the original implementation of ARS (Mania et al., 2018), the
survival bonus for the reward in the Hopper environment is removed to avoid local minima. Since
we wanted our PSSVF to be close to their setting, we also applied this modification. We did not
remove the survival bonus from all TD algorithms and we did not investigate how this could affect
their performance. We provide a comparison of the performance of PSSVF with and without the
bonus in figure 6 using deterministic policies.
17
Published as a conference paper at ICLR 2021
2500
Hopper-v3 []
0.0	0.2	0.4	0.6	0.8	1.0
time steps	ie6
H0pper-v3 [64, 64]
-----survival bonus
-----no survival bonus
0.0	0.2	0.4	0.6	0.8	1.0
time steps	ie6

Figure 6:	Performance of PSSVF with and without the survival bonus for the reward in Hopper-v3
when using the hyperparameters maximizing the average return. Learning curves are averaged over
5 seeds.
DDPG For DDPG, we used the Spinning Up implementation provided by OpenAI (Achiam,
2018), which includes target networks for the actor and the critic and no learning for a fixed set
of time steps, called starting steps. We did not include target networks and starting steps in our
PBVFs, although they could potentially help stabilizing training. The implementation of DDPG that
we used (Achiam, 2018) does not use observation normalization. In preliminary experiments we
observed that it failed to significantly increase or decrease performance, hence we did not use it.
Another difference between our TD algorithms and DDPG consists in the number of updates of the
actor and the critic. Since DDPG’s critic needs to keep track of the current policy, the critic and the
actor are updated in a nested form, with the first’s update depending on the latter and vice versa. Our
PSVF and PAVF do not need to track the policy learned, hence, when it is time to update, we need
only to train once the critic for many gradient steps and then train the actor for many gradient steps.
This requires less compute. On the other hand, when using nonlinear policies, our PBVFs suffer
the curse of dimensionality. For this reason, we profited from using a bigger critic. In preliminary
experiments, we observed that DDPG’s performance did not change significantly through a bigger
critic. We show differences in performance for our methods when removing observation normaliza-
tion and when using a smaller critic (MLP(256,256)) in figure 7. We observe that the performance is
decreasing if observation normalization is removed. However, only for shallow policies in Swimmer
and deep policies in Hopper there seems to be a significant benefit. Future work will assess when
bigger critics help.
Swimmer-v3 []	Swimmer-v3 [64, 64]	Hopper-v3 []	Hθpper-v3 [64, 64]
400
300
——PSVF
---Smallercrltlc
--- no obs norm
2000
1500
1000
500
0
0.0 02 04 0.6 OB IQ 0.0 02 04 0.6 OB IQ
time steps le6	time steps le6

Swimmer-v3 [64, 64]	Hopper-v3 []
Swimmer-v3 []
1∞
0.2
04 0.6
time steps
1.0
le6
300
250
200
150
100
50
0
-50
1750
1500
1250
1000
750
2000
1500
1000
Hopper-v3 [64, 64]
——PAVF
----Smallercrltlc
---no obs norm
0-0
Figure 7:	Learning curves for PSVF and PAVF for different environments and policies removing
observation normalization and using a smaller critic. We use the hyperparameters maximizing the
average return. Learning curves are averaged over 5 seeds. For this ablation we use deterministic
policies.
18
Published as a conference paper at ICLR 2021
Discounting in Swimmer For TD algorithms, we chose a fixed discount factor γ = 0.99 for all
environments but Swimmer-v3. This environment is known to be challenging for TD based algo-
rithms because discounting causes the agents to become too short-sighted. We observed that, with
the standard discounting, DDPG, PSVF and PAVF were not able to learn the task. However, making
the algorithms more far-sighted greatly improved their performance. In figure 8 we report the return
obtained by DDPG, PSVF and PAVF for different values of the discount factor in Swimmer when
using deterministic policies.

E 150
S
IOQ
28 ■
250
300-
PSVF-Swimmer-v3 []
20Q
250
300
350
PAVF-Swimmer-v3 []
150
250
DDPG-SWimmer-V3 []
0.0	0.2	0.4	0.6	0.8	1.0	0.0	0.2	0.4	0.6	0.β	1.0
time steps	le6	time steps	le6
0.0	0.2	0.4	06 O.a 1.0	oa 0.2	0.4	Q.6	Q.8	1.0	0.0	0.2	0.4	06	0.8	1.0
time steps	le6	time steps	le6	time steps	le6
Figure 8: Effect of different choices of the discount factor in Swimmer-v3 for PSVF, PAVF and
DDPG, with shallow and deep deterministic policies. We use the hyperparameters maximizing the
average return. Learning curves are averaged over 5 seeds
0.0	0.2 0Λ 0.6	0.8	1.0
time steps	le6
DDPG-Swimmer-v3 [64, 64]
400
300
IQO
0
B200
S


A.3.2 Pseudocode
Algorithm 2 Actor-critic with TD prediction for V (s, θ)
Input: Differentiable critic Vw : SX Θ → R with parameters w; deterministic or stochastic
actor πθ with parameters θ; empty replay buffer D
Output: Learned Vw ≈ V(s, θ), learned ∏ ≈ ∏θ*
Initialize critic and actor weights w, θ
repeat:
Observe state s, take action a = πθ(s), observe reward r and next state s0
Store (s, θ, r, s0) in the replay buffer D
if it’s time to update then:
for many steps do:
Sample a batch B1 = {(s, θ, r, s0)} from D
Update critic by stochastic gradient descent:
I	∕~∙j	∕~∙j
NW|B1| E(s,0,r,s0)∈Bι [Vw(s, θ) - (r + YVw(SZ,°))]
end for
for many steps do:
Sample a batch B2 = {(s)} from D
Update actor by stochastic gradient ascent: Vθ ^1^ Es∈b? [Vw(s, θ)]
end for
end if
until convergence
19
Published as a conference paper at ICLR 2021
Algorithm 3 Stochastic actor-critic with TD prediction for Q(s, a, θ)
Input: Differentiable critic Qw : S ×A× Θ → R with parameters w; stochastic differentiable
actor πθ with parameters θ; empty replay buffer D
Output: Learned Qw ≈ Q(s, a, θ), learned ∏ ≈ ∏θ*
Initialize critic and actor weights w, θ
repeat:
Observe state s, take action a = πθ(s), observe reward r and next state s0
Store (s, a, θ, r, s0) in the replay buffer D
if it’s time to update then:
for many steps do:
∙-v
Sample a batch B1 = {(s, a, θ, r, s0)} from D
Update critic by stochastic gradient descent:
l	∙-v	∙-v
Vw∣B11∣ E(s,a,e,r,s0)∈B1 [Qw(S, a,θ) - (r + YQw(S0, a 〜福(SO),θ))]2
end for
for many steps do:
∙-v
Sample a batch B2 = {(S, a, θ)} from D
Update actor by stochastic gradient ascent:
∣⅛ E(s,a& ∈B2 h πfS⅛ (Q(s,a,θ)vθ log πθ ⑷S) + vθ Q(s,a, θ))i
end for
end if
until convergence
Algorithm 4 Deterministic actor-critic with TD prediction for Q(S, a, θ)
Input: Differentiable critic Qw : S ×A× Θ → R With parameters w; differentiable determin-
istic actor πθ with parameters θ; empty replay buffer D
Output : Learned Qw ≈ Q(S, a, θ), learned πθ ≈ πθ*
Initialize critic and actor weights w, θ
repeat:
Observe state s, take action a = πθ(S), observe reward r and next state S0
Store (S, a, θ, r, S0) in the replay buffer D
if it’s time to update then:
for many steps do:
∙-v
Sample a batch B1 = {(S, a, θ, r, S0)} from D
Update critic by stochastic gradient descent:
j	∙-v	∙-v
vw∣B∣ E(s,a,d,r,s0)∈Bι [Qw(S，a，θ) - (r + YQw(S' ^θls'), θ))]
end for
for many steps do:
Sample a batch B2 = {(S)} from D
Update actor by stochastic gradient ascent:
∣B^ Es∈B2 [vθ πθ (S)VaQw(S, a, θ)la=πθ(s) + Ne Qw(S, a, θ)∣a=∏θ (s)]
end for
end if
until convergence
A.4 Experimental details
A.4.1 LQR
For our visualization experiment, we employ an instance of the Linear Quadratic Regulator. Here,
the agent observes a 1-D state, corresponding to its position and chooses a 1-D action. The transi-
tions are S0 = S + a and there is a quadratic negative term for the reward: R(S, a) = -S2 - a2 .
The agent starts in state S0 = 1 and acts in the environment for 50 time steps. The state space is
bounded in [-2,2]. The goal of the agent is to reach and remain in the origin. The agent is expected
to perform small steps towards the origin when it uses the optimal policy. For this task, we use a
20
Published as a conference paper at ICLR 2021
deterministic policy without tanh nonlinearity and we do not use observation normalization. Below
additional details and plots for different algorithms.
PSSVF We use a learning rate of 1e - 3 for the policy and 1e - 2 for the PSSVF. Weights are
perturbed every episode using σ = 0.5. The policy is initialized with weight 3.2 and bias -3.5. All
the other hyperparameters are set to their default. The true episodic J (θ) is computed by running
10,000 policies in the environment with parameters in [-5, 5] × [-5, 5]. Vw(θ) is computed by
measuring the output of the PSSVF on the same set of policies. Each red arrow in figure 1 represents
200 update steps of the policy.
PSVF ad PAVF Using the exact same setting, we run PSVF and PAVF in LQR environment and
we compare learned V (s0, θ) and Q(s0, πθ(s0), θ) with the true PSVF and PAVF over the parameter
space. Computing the value of the true PSVF and PAVF requires computing the infinite sum of
discounted reward obtained by the policy. Here we approximate it by running 10,000 policies in
the environment with parameters in [-5, -5] × [-5, 5] for 500 time steps. This, setting γ = 0.99,
provides a good approximation of their true values, since further steps in the environment result in
almost zero discounted reward from s0. We use a learning rate of 1e - 2 for the policy and 1e - 1
for the PSVF and PAVF. Weights are perturbed every episode using σ = 0.5. The policy is updated
every 10 time steps using 2 gradient steps; the PSVF and PAVF are updated every 10 time steps
using 10 gradient updates. The critic is a 1-layer MLP with 64 neurons and tanh nonlinearity.
In Figures 9 and 10 we report J (θ), the cumulative discounted reward that an agent would
obtain by acting in the environment for infinite time steps using policy πθ and the cumulative
return predicted by the PSVF and PAVF for two different times during learning. Like in the
PSSVF experiment, the critic is able improve its predictions over the parameter space. Since in the
plots V (s, θ) and Q(s, πθ(s), θ) are evaluated only in s0, the results show that PBVFs are able to
effectively bootstrap the values of future states. Each red arrow in Figures 9 and 10 represents 50
update steps of the policy.
Optimization after 15 episodes
Figure 9: True cumulative discounted reward J(θ) and PSVF estimation Vw(s0, θ) as a function of
the policy parameters at two different stages in training. The red arrows represent an optimization
trajectory in parameter space. The blue dots represent the perturbed policies used to train Vw(s0, θ).
Optimization after 100 episodes
VW(so,θ)
-489
-141
--1600
--4471
-7343
-10214
-13086
θι
θι
21
Published as a conference paper at ICLR 2021
Optimization after 15 episodes
Optimization after 100 episodes
-489
-141
--1600
--4471
-7343
-10214
-13086
-489
-141
--1600
--4471
-7343
-10214
-13086
Figure 10: True cumulative discounted reward J(θ) and PAVF estimation Qw (s0, πθ (s0), θ) as a
function of the policy parameters at two different stages in training. The red arrows represent an
optimization trajectory in parameter space. The blue dots represent the perturbed policies used to
train Qw(s0,πθ(s0), θ).
A.4.2 Offline experiments
Zero-shot learning We evaluate the performance of the policies learned from scratch evaluating
them with 5 test trajectories every 5 gradient steps. In addition to the results in the main paper,
we report in Figures 11 and 12 a comparison of zero-shot performance between PSSVF, PSVF
and PAVF in three different environments using deterministic shallow and deep policies (2-layers
MLP(64,64)). In this task we use the same hyperparameters found in tables 4, 6 and 8. One
additional hyperparameter needs to be considered: the learning rate of the policies trained from
scratch. In Figure 3 of the main paper, we use a tuned learning rate of 0.02 that we found working
particularly well for PSSVF in the Swimmer environment. In the additional experiments in
Figures 11 and 12, we use a learning rate of 0.05 that we found working well across all policies,
environments and algorithms when learning zero-shot.
We observe that, using shallow policies, PBVFs can effectively zero-shot learn policies with
performance comparable to the policy learned in the environment without additional tuning for
the learning rate. We note the regular presence of a spike in performance followed by a decline
due to the policy going to regions of the parameter space never observed. This suggests that there
is a trade-off between exploiting the generalization of the critic and remaining in the part of the
parameter space where the critic is accurate. Measuring the width of these spikes can be useful
for determining the number of offline gradient steps to perform in the general algorithm. When
using deep policies the results become much worse and zero-shot learned policies can recover the
performance of the main policy being learned only in simple environments and at beginning of
training (eg. MountainCarContinuous). We observe that, when the critic is trained (last column),
the replay buffer contains policies that are very distant to policies randomly initialized. This might
explain why the zero-shot performance is better sometimes at the beginning of training (eg. second
column). However, since PBVFs in practice perform mostly local off-policy evaluation around the
learned policy, this problem is less prone to arise in our main experiments.
Offline learning with fragmented behaviors In this task, data are generated by perturbing a
randomly initialized deterministic policy every 200 time steps and using itto act in the environment.
22
Published as a conference paper at ICLR 2021
SWlmmer-V3
350
*",ssojso-a-soF
u∙u∙∙∙a-∙∙1∙∙"∙∙∙∙a-∙∙-∙∙F-1∙∙O-1,•-»•
u∙u∙∙∙a-∙∙1∙∙,∙∙∙∙∙a-∙∙-∙∙F-1∙∙O-1,•-»•
Swlmmer-VS-SOOOOO steps
« i«a 2βα 38 xαα saa βaa τaa
Mt>uπtalπCa>Coπtlπu<>us40-50000 steps
,""","aF∞,∙,∙∙∙a-j∙->,-'∙-∞O-∞M
-aw  1-------1------1-----1------1-----1—	—c 1 1 1 1 1 1 1 1	-c 1 1 1 1 1 1 1 1	~∙rw	∙ ∙ ∙ ∙ ∙ ∙ ∙
OOOM 0.02M 0.04M 0.06M	0.0βM O-IOM	0 ɪθð 2β0 38 4β° SM CM TM	0 18 240 38 4C0 58 CM 78	0 IM 200 38 4ββ 58 CM 78
.....	arMer* ≠≡ρs	h—ert,teρs	w⅜dertsteps
time steps
Figure 11: Shallow policies learned from scratch during training. The plots in the left column repre-
sent the return of agents learning while interacting with the environment using different algorithms.
The learning curves in the other plots represent the return obtained by policies trained from scratch
following the fixed critics after different time steps of training. Zero-shot learning curves are aver-
aged over 5 seeds.



We use σ = 0.5 for the perturbations. After the dataset is collected, the PSVF is trained using a
learning rate of 1e - 3 with a batch size of 128. When the policy is learned, we use a learning rate
of 0.02. All other hyperparameters are set to default values.
A.4.3 Full experimental results
Methodology In order to ensure a fair comparison of our methods and the baselines, we adopt
the following procedure. For each hyperparameter configuration, for each environment and policy
architecture, we run 5 instances of the learning algorithm using different seeds. We measure the
learning progress by running 100 evaluations while learning the deterministic policy (without action
or parameter noise) using 10 test trajectories. We use two metrics to determine the best hyperparam-
eters: the average return over policy evaluations during the whole training process and the average
return over policy evaluations during the last 20% time steps. For each algorithm, environment and
policy architecture, we choose the two hyperparameter configurations maximizing the performance
of the two metrics and test them on 20 new seeds, reporting average and final performance in table 2
and 3 respectively.
Figures 13 and 14 report all the learning curves from the main paper and for a small non linear
policy with 32 hidden neurons.
Stochastic policies We include some results for stochastic policies when using PSSVF and PSVF.
Figures 15 and 16 show a comparison with the baselines when using shallow and deep policies re-
spectively. We observe results sometimes comparable, but often inferior with respect to deterministic
policies. In particular, when using shallow policies, PBVFs are able to outperform the baselines in
the MountainCar environment, while obtaining comparable performance in CartPole and Inverted-
Pendulum. Like in previous experiments, PBVFs fail to learn a good policy in Reacher. When using
deep policies, the results are slightly different: PBVFs outperform ARS and DDPG in Swimmer,
but fail to learn InvertedPendulum. Although the use of stochastic policies can help smoothing the
23
Published as a conference paper at ICLR 2021
SWlmmer-V3
350
Swlinnier-VS-ICOOOO steps
a ιaa 2a0 aaa 4aa saa βaa τaa
Mou πtal πCar<⅛ πtlπ uous-⅛fl-10000 steps
Swlmmer-VS-SOOOOO steps
a iw 2aa 3M <aa sm βaa τaa
y⅜dw*s⅛ρs
5Wlmmer-Vs-IOOOOOO steps
—psβvf
PeVf
—pavr
*"'ssojso-js-sof
0.0OM	0.02M	0.04M	0.06M	0.0βM OlOM
tiπιestef>s
Acrobot-Vl
« s« 10a iso 2aa 250 aaa
AciQbot-Vl-ICooo steps
« S« 1® ISO 2βa 2M 3M
Acrobot-Vl-SOOOO steps
LM 2M 38 4C0 58 CM 78
yadert steps
πtal πCar<⅛ πtlπ uous-vfl-100000 steps
0.0OM	0.02M	0.04M	0.06M	0.0βM OlOM
time steps
0 1® 2βa 38 xaa saa βaa 2
O
« S« l«a U« 2«« 2M 3M
yadert steps
AciQbot-Vl-ICOOOO steps
0 ι∞ saa 3aa xaa saa βaa 2
yadert steps
0 1® 2βa 38 4aa SM βaa τaa





Figure 12: Deep policies learned from scratch during training. The plots in the left column represent
the return of agents learning while interacting with the environment using different algorithms. The
learning curves in the other plots represent the return obtained by policies trained from scratch fol-
lowing the fixed critics after different time steps of training. Zero-shot learning curves are averaged
over 5 seeds.
objective function and allows the agent exploring in action space, we believe that the lower variance
provided by deterministic policies can facilitate learning PBVFs.
A.4.4 Sensitivity analysis
In the following, we report the sensitivity plots for all algorithms, for all deterministic policy ar-
chitectures and environments. In particular, figure 17, 18, 19, 20 and 21 show the performance
of each algorithm given different hyperparameters tried during training. We observe that in gen-
eral deep policies are more sensitive and, apart for DDPG, achieve often a better performance than
smaller policies. The higher sensitivity displayed by ARS is in part caused by the higher number of
hyperparameters we tried when tuning the algorithm.
A.4.5 Table of best hyperparameters
We report for each algorithm, environment, and policy architecture the best hyperparameters found
when optimizing for average return or final return in tables 4, 5, 6, 7, 8 and 9.
24
Published as a conference paper at ICLR 2021
Table 2: Average return with standard deviation (across 20 seeds) for hypermarameters optimizing
the average return during training using deterministic policies. Square brackets represent the number
of neurons per layer of the policy. [] represents a linear policy.
Policy: []	MountainCar Continuous-v0	Inverted Pendulum-v2	Reacher -v2	Swimmer -v3	Hopper -v3
ARS	63 ± 6	886 ± 72	-9.2 ± 0.3	228 ± 89	1184 ± 345
PSSVF	85 ± 4	944 ± 33	-11.7 ± 0.9	259 ± 47	1392 ± 287
DDPG	0±0	612 ± 169	-8.6 ± 0.9	95 ± 112	629 ± 145
PSVF	84 ± 20	926 ± 34	-19.7 ± 6.0	188±71	917 ± 249
PAVF	82 ± 21	913±40	-17.0 ± 7.7	231 ± 56	814 ± 223
Policy:[32]					
ARS	37 ± 11	851 ± 46	-9.6 ± 0.3	139±78	1003 ± 66
PSSVF	60 ± 33	701 ± 138	10.4 ± 0.5	189±35	707 ± 668
DDPG	0±0	816±36	-5.7 ± 0.3	61 ± 32	1384 ± 125
PSVF	71 ± 25	529 ± 281	-11.9 ± 1.2	226 ± 33	864 ± 272
PAVF	71 ± 27	563 ± 228	-10.9 ± 1.1	222 ± 28	793 ± 322
Policy: [64,64]
ARS	28 ± 8	812 ± 239	-9.8 ± 0.3	129±68	964 ± 47
PSSVF	72 ± 22	850 ± 93	-10.7 ± 0.2	158±59	922 ± 568
DDPG	0±0	834 ± 36	-5.5 ± 0.4	92 ± 117	767 ± 627
PSVF	80 ± 9	580 ± 107	-10.7 ± 0.6	137±38	843 ± 282
PAVF	73 ± 10	399 ± 219	-10.7 ± 0.5	142±26	875 ± 136
Policy: []	Acrobot-v1	CartPole-v1			
ARS	-161 ± 23	476 ± 13			
PSSVF	-137 ± 14	443 ± 105			
PSVF	-148 ± 25	459 ± 28			
Policy:[32]					
ARS	-296 ± 38	395 ± 141			
PSSVF	-251 ± 80	463 ± 18			
PSVF	-270± 113	413±61			
Policy: [64,64]					
ARS	-335 ± 35	416 ± 105			
PSSVF	-281 ± 117	452 ± 34			
PSVF	-397 ± 71	394 ± 71			
25
Published as a conference paper at ICLR 2021
Table 3: Final return with standard deviation (across 20 seeds) for hypermarameters optimizing the
final return during training using deterministic policies.
Policy: []	MountainCar Continuous-v0	Inverted Pendulum-v2	Reacher -v2	Swimmer -v3	Hopper -v3
ARS	73 ± 5	657 ± 477	-8.6 ± 0.5	334 ± 34	1443 ± 713
PSSVF	84 ± 28	970 ± 126	-10.0 ± 1.0	350 ± 8	1560 ± 911
DDPG	0±1	777 ± 320	-7.3 ± 0.4	146 ± 152	704 ± 234
PSVF	76 ± 36	906 ± 289	-16.5 ± 1.6	238 ± 107	1067 ± 340
PAVF	68 ± 42	950 ± 223	-17.2 ± 15.4	298 ± 40	720 ± 281
Policy:[32]					
ARS	54 ± 20	936 ± 146	-9.2 ± 0.4	239 ± 117	1048 ± 68
PSSVF	89 ± 22	816 ± 234	-10.2 ± 1.0	294 ± 41	1204 ± 615
DDPG	0±0	703 ± 283	-4.6 ± 0.6	179 ± 150	1290 ± 348
PSVF	84 ± 31	493 ± 462	-11.3 ± 0.8	290 ± 70	1003 ± 572
PAVF	92 ± 7	854 ± 295	-10.1 ± 0.9	307 ± 34	967 ± 411
Policy: [64,64]
ARS	11 ± 30	976 ± 83	-9.4 ± 0.4	157±54	1006 ± 47
PSSVF	91 ± 16	898 ± 227	-10.7 ± 0.6	224 ± 99	1412 ± 691
DDPG	0±0	943 ± 73	-4.4 ± 0.4	196 ± 151	1437 ± 752
PSVF	93 ± 1	1000±0	-10.6 ± 1.0	257 ± 26	1247 ± 344
PAVF	93 ± 2	827 ± 267	-10.6 ± 0.4	232 ± 42	1005 ± 155
Policy: []	Acrobot-v1	CartPole-v1			
ARS	-126 ± 26	499 ± 2			
PSSVF	-97±6	482 ± 53			
PSVF	-100 ± 18	500 ± 0			
Policy:[32]					
ARS	-215 ± 97	471 ± 110			
PSSVF	-116 ± 33	500 ± 0			
PSVF	-244 ± 151	488 ± 36			
Policy: [64,64]					
ARS	-182 ± 45	492 ± 18			
PSSVF	-233 ± 139	500 ± 0			
PSVF	-406 ± 51	499 ± 2			
26
Published as a conference paper at ICLR 2021
Hopper-v3-[64,64]
time steps
Deterministic policies
Hcpper-v3-[32]

Hopper-v3-Q
saw	---- are
time steps

0.2OM	0.4OM	0.8M	O.SOM 1.00M
,me steps


^10‰M	0X>2M OXMM OMM OgM 0Δ0M
timesteps
^1‰M	0.02M	0.04M	0.0βM	0.08M	0.10M
,me steps
timesteps
M0ιntainC⅛rContinucxjs-vO-[64, 64]
ιαo
Imestsps
Ac∙01Kt-vl-∣32]

time ⅛ps
⅛SX)lX)t-vl-∏
time steps
,me steps
time ⅛ps
Figure 13: Learning curves representing the average return as a function of the number of time steps
in the environment (across 20 runs) with different environments and deterministic policy architec-
tures. We use the best hyperparameters found while maximizing the average reward for each
task. For each subplot, the square brackets represent the number of neurons per policy layer. []
represents a linear policy.
time steps
⅛aχ>lX)t-vl-[64,641
27
Published as a conference paper at ICLR 2021
3000
Hopper-v3-[64,64]
time steps
⅝acħer-v2-[64, 641

Deterministic policies
Hcpper-v3-[32]
3000	3000
Hopper-v3-[]
^1‰M	0X>2M OXMM OMM OgM 0Δ0M
time steps
ReaCher-V2-口
020M 0ΛJM OeJM	0«JM	IXHM
time steps
-----ars
----PSVf
----PSSVf
----dttρg
-----PeVf
^10‰M	0X>2M OXMM OMM OgM 0Δ0M
time steps

^1‰M	0.02M	0.04M	0.0βM	0.08M	0.10M
,me steps
InvertedPendu Iu∣n-v2-∏
0X)2M	OMM OXXM OgM 0Δ0M
time steps
SWlmmer-V3■归4, 641
Swιmmer-v3-[321
Swιmmer-v3-l
0.8M	0.2OM	0.4OM	0.8M	0.80M	1.00M	0.8M	020M	040M	040M 。和 M 1XK)M
f me steps	Smesteps

OXHM 020M OΛJM OeJM	0«JM	1XK)M
timesteps
Mo UntamCarContm UCXj s-vθ-[321
^18Sbm
ι∞
SO
(j
,me steps
Imesteps
ACrOtlOt-VI-口2]
Figure 14:	Learning curves representing the average return as a function of the number of time steps
in the environment (across 20 runs) with different environments and deterministic policy architec-
tures. We use the best hyperparameters found while maximizing the final reward for each task.
For each subplot, the square brackets represent the number of neurons per policy layer. [] represents
a linear policy.
Stochastic shallow policies
ReaCher-V2	Inverted Penι⅛lunι-v2
O	M∞ J—ct-
3000
2500
Hopper-v3
800
ars
p Svf
p SSvf
dt⅛g
ρavf



Figure 15:	Learning curves representing the average return as a function of the number of time steps
in the environment (across 20 runs) with different environments using stochastic shallow policies.
We use the best hyperparameters found while maximizing the average reward for each task.
28
Published as a conference paper at ICLR 2021
HopPer-V3
Stochastic deep policies
ReaCher-V2
o
IngrtedpendUlum-V2
2500
2000
ime steps
CartPole-vl
ars
p Svf
p ssvf
dt⅛g
ρavf
tine steps
OflSM 0Δ0M
OXHM	0X)2M	OXMM OMM OgM 0Δ0M
tine steps

Figure 16:	Learning curves representing the average return as a function of the number of time
steps in the environment (across 20 runs) with different environments using stochastic deep policies
([64,64]). We use the best hyperparameters found while maximizing the average reward for
each task.
,me steps
‘me ^PS
90-100
70-S0
30-70
30
median
MamtainCarContJ rυo us-vO-口
IOo
M CXinta in C⅛rContin UCXJS-VO-[32]
CartpoIe-V1-口
CartPole-vl-[32]
CartPo te-vl-[64, 64]
5004005200100
ReaCher-V2-口
Reacher-v2-[32l
InvertedPendulu∣n-v2-[64, 64]
Reacher-v2-[64, 64]
0.O4M	0.0βM
,me steps
SWimmer-V3-口
540M	O.βUM
,me steps
Hopper-v3-Π
OBOM
300200lco
O?IOM
OXHM
020M	0.4OM	0.8M O.SOM 1.00M
,me steps
020M OΛJM OeJM	0«JM	1XK)M
ime steps
020M OΛJM 0βlM 0«0M
time steps
OaZM OXMM OWM OflSM
time steps
Swimmer-v3-[64, 64]
020M OΛJM 0βlM 0«0M
time steps
Hopper-v3-∣64,64]
OaZM 0Λ4M OΛβM OflSM
time steps
90-100
70-S0
30-70
30
median
90-100
70-S0
30-70
30
median
90-100
70-S0
30-70
30
median
90-100
70-S0
30-70
30
median
90-100
70-S0
30-70
30
median

Figure 17:	Sensitivity of PSSVFs using deterministic policies to the choice of the hyperparameter.
Performance is shown by percentile using all the learning curves obtained during hyperparameter
tuning. The median performance is depicted as a dark line. For each subplot, the numbers in the
square brackets represent the number of neurons per layer of the policy. [] represents a linear policy.
29
Published as a conference paper at ICLR 2021
Acrobot-vl-Q
ACrotlOt-VI-P2]
Acrobot-vl-[64, 64]
^s‰M 0X>2M	0.04M	0.0βM	0.08M	0.10M
,me steps
^5‰M OBZM 0Λ4M OΛβM OXJSM
time steps
^58⅛OM OdM OXMM OXMM (M)SM (HOM
ime steps
IoO
50
P
M0∣ntam(⅛rContmuα∣s-vO-[32]
MoUntamCarContmuous-vO-[64, 64]
itjRrzzz
90-100
70-S0
30-70
H 8。
---median
■ 90-100
70-S0
30-70
3。
----median
^10‰M	0X>2M	0.04M	0.0βM	0.08M	0.10M
,me steps
OaZM	0.04M	0.0βM O.OSM
,me steps
0.10M
ιαo ajr
0%0M	0X)2M	OXMM OMM OgM 0Δ0M
ime steps
^10‰M	0X>2M OXMM OMM OgM 0Δ0M
ime steps
0X)2M	OMM OXXM OgM 0Δ0M
time steps
CartPo le-vl-[64, 64]
90-100
70-S0
30-70
M3 ^30
----median
0%0M	0X)2M	OXMM OXHM OgM 0Δ0M
time steps
IngrtedPendUlUm-V2-口
InvertetiPeritiu lum-v2-∣32]
IOOO
400
200
0Λ2M
OMM OXXM OflSM 0Δ0M
ime steps
time steps
Reacher-v2-[64,64]
ReaCher-V2-[32]
70-S0
30-70
M3 ^30
----median
3M 0X>2M	0.04M	0.0βM	0.08M	0.10M
,me steps
Swiτnmer-ιβ-Q
0X)2M	OMM OXXM OgM 0Δ0M
imε steps
Swimmer-v3-∣32J
^1‰M	0X)2M	OXMM OMM OgM 0Δ0M
time steps
300200■•Mo
300r^∙Mo
3000
2000
IenO
5 40M
O.βUM
30UU
ι∞o
,me steps
,me steps
Hopper-v3-Π
Hopper-v3-[32]
MOO
020M	040M	OeJM	0«JM	1XK)M
‘me
Swimmer-v3-[64, 64]
Hopper-v3-[64,64]






Figure 18:	Sensitivity of PSVFs using deterministic policies to the choice of the hyperparameter.
Performance is shown by percentile using all the learning curves obtained during hyperparameter
tuning. The median performance is depicted as a dark line. For each subplot, the numbers in the
square brackets represent the number of neurons per layer of the policy. [] represents a linear policy.
30
Published as a conference paper at ICLR 2021
MamtainCarContJ rυo us-vO-口
ιαo
75
50
≈ι
0∙
-25
MoUntamCarContmuous-vO-[64, 64]
90-100
70-S0
30-70
30
median
IenO
800
βoo
400
200
0¾0kΓ
0X>2M
一
90-100
70-S0
30-70
30
median
90-100
70-S0
30-70
30
median
Imesteps
Inverted PentiUlUm-V2-口
ime steps
InvertedPendu kjm-v2-[32]
`afr ι:
π _ _ _	CL	-	_ _ —
time steps
InvertedPendulu∣n-v2-[64, 64]
0.04M	0.M
,me steps
Reacħer-v2-∏
)XMM OMM
ime steps
Reacher-v2-[32]
OflSM
time steps
⅝acher-v2-[64,64]
^1‰M	0X>2M	0.04M	0.0βM	0.08M	0.10M
,me steps
3500
Hoρρer-v3-g
3000
2500
2000
15∞
,me steps
08M	OXUM 08M OOBM OJOM
imε steps
Swimmer-v3-∣32J
020M OΛJM OeJM	0«JM	1XK)M
imε steps
Hcpper-v3-[32]
020M OΛJM OeJM	0«JM	1XK)M
irrκ ^eps
OXMM
0∙06M
time steps
Swimmer-v3-[64, 64]
time ⅛ps
90-100
70-S0
30-70
30
median
90-100
70-S0
30-70
30
median
time steps
H0ppβr-v3-[64, 641




Figure 19:	Sensitivity of PAVFs using deterministic policies to the choice of the hyperparameter.
Performance is shown by percentile using all the learning curves obtained during hyperparameter
tuning. The median performance is depicted as a dark line. For each subplot, the numbers in the
square brackets represent the number of neurons per layer of the policy.
31
Published as a conference paper at ICLR 2021
MamtalnCarCOntJrWoUS-V0-口	Mcχjntam(⅛rContmucχj9-vO-[32j
MoUntamCarContmuous-v0-[64, 64]
90-100
70-S0
30-70
3。
----median
-18¾0M	0X>2M	0.04M	0.0βM	0.08M	0.10M	-1‰M	0X>2M OXMM OMM OgM 0Δ0M
f me steps	(me steps
-1‰M	0X)2M	OXMM OMM OgM 0Δ0M
time steps
^1‰M	0X>2M	0.04M	0.0βM	0.08M	0.10M	^10%OM	0X)2M	OXMM OMM OgM 0Δ0M
ime steps	(to StePS
Swiτnmer-ιβ-Q	Swimmer-v3-∣32J
350
^1‰M	0X)2M	OXMM OMM OgM 0Δ0M
time steps
300
1XK)M
^‰M	020M 0ΛJM 0βlM 0«0M
^‰M	020M	0.4OM	0.8M	O.SOM 1.00M
,me steps
HOPPer-V3∙口
^‰M	020M 0ΛJM OeJM	0«JM	1XK)M
ime steps
HoPPer-V3-β2]
time steps
Hopper-v3-[64,64]
3500
3500
3500
Figure 20: Sensitivity of DDPG to the choice of the hyperparameter. Performance is shown by
percentile using all the learning curves obtained during hyperparameter tuning. The median perfor-
mance is depicted as a dark line. For each subplot, the numbers in the square brackets represent the
number of neurons per layer of the policy.



32
Published as a conference paper at ICLR 2021
ιαo
MamtainCareOntinuoUS∙v0-[]
50
0-
-18¾0M	0X)2M	0.04M	0.0βM	0.08M	0.10M
,me steps
-18¾0M	0X>2M	0.04M	0.0βM	0.08M	0.10M
,me steps
SWlmmer-V3-口
OXHM
ACrotlOt-VI-P2]
-50a*--------------------A -AA-~  --------------*~ΛΛA
Ti⅛0M 0Λ2M OXMM OaeM OoSM …
0Λ2M
0Λ4M
OX*M
OJOM
1∞
SO
imε steps
MountainC⅛rContinuou9-vO-∣32J
ιαo
so
0
MountainCarCon tinuou9-vO-[64,64]
90-100
70-S0
30-70
<^30
----median
^18⅛om
OXBM
OXMM
ime steps
OMM
OflSM
0Δ0M
-1‰M	0X>2M OXMM OXHM OgM 0Δ0M
time steps
ime steps
Reacher-v2-[32]
500
400
300 Λ∕r'
0‰M
CartPo le-vl-[64, 64]
90-100
70-S0
30-70
M3 ^30
----median
^1‰M
0X>2M
OXMM
ime steps
OMM
OflSM
300200loo
OXBM
OXMM
OMM
time steps
1000
800
600
400
200
0Δ0M
InvertedPendulu∣n-v2-[64, 641
time steps
⅝acher-v2-[64,64]
time steps
Swimmer-v3-[64, 64]
300
200
IOo
■ 90-100
■ 70-S0
■ 30-70
3。
----median
■ 90-100
■ 70-S0
■ 30-70
3。
----median
90-100
70-S0
30-70
M3 8。
----median
0
020M
0.4OM
0.8M
O-SOM
1.00M
04OM
OelM
OΛJM
OelM
3000
2000
2000
IenO
,me steps
HOPPer-V3∙口
irrκ ^PS
Hopper-v3-[32]
time steps
Hopper-v3-[64, 641
ICCC
fmestεps
‘me ^PS
time ⅛ps
300Q
■ 90-100
■ 70-S0
■ 30-70
30
median

0
0
0
Figure 21: Sensitivity of ARS to the choice of the hyperparameter. Performance is shown by
percentile using all the learning curves obtained during hyperparameter tuning. The median perfor-
mance is depicted as a dark line. For each subplot, the numbers in the square brackets represent the
number of neurons per layer of the policy.
33
Published as a conference paper at ICLR 2021
Table 4: Table of best hyperparameters for PSSVFs using deterministic policies
Learning rate policy	Policy: Metric:	[]		[32]		[64,64]	
		avg	last	avg	last	avg	last
Acrobot-v1		1e-2	1e-3	1e-4	1e-4	1e-4	1e-4
MountainCarContinuous-v0		1e-2	1e-3	1e-4	1e-4	1e-4	1e-4
CartPole-v1		1e-3	1e-3	1e-3	1e-3	1e-4	1e-4
Swimmer-v3		1e-3	1e-3	1e-3	1e-3	1e-2	1e-4
InvertedPendulum-v2		1e-3	1e-3	1e-3	1e-3	1e-4	1e-4
Reacher-v2		1e-4	1e-4	1e-4	1e-4	1e-4	1e-4
Hopper-v3		1e-4	1e-4	1e-4	1e-3	1e-4	1e-4
Learning rate critic							
Acrobot-v1		1e-2	1e-3	1e-2	1e-2	1e-2	1e-2
MountainCarContinuous-v0		1e-3	1e-2	1e-3	1e-2	1e-2	1e-2
CartPole-v1		1e-2	1e-2	1e-3	1e-3	1e-2	1e-2
Swimmer-v3		1e-3	1e-3	1e-2	1e-2	1e-3	1e-2
InvertedPendulum-v2		1e-2	1e-2	1e-3	1e-2	1e-3	1e-3
Reacher-v2		1e-3	1e-3	1e-3	1e-3	1e-4	1e-4
Hopper-v3		1e-3	1e-3	1e-2	1e-2	1e-2	1e-2
Noise for exploration							
Acrobot-v1		1.0	1.0	1e-1	1e-1	1e-1	1e-1
MountainCarContinuous-v0		1.0	1.0	1e-1	1e-1	1e-1	1e-1
CartPole-v1		1.0	1.0	1.0	1.0	1e-1	1e-1
Swimmer-v3		1.0	1.0	1.0	1.0	1.0	1e-1
InvertedPendulum-v2		1.0	1.0	1.0	1.0	1e-1	1e-1
Reacher-v2		1e-1	1e-1	1e-1	1e-1	1e-1	1e-1
Hopper-v3		1.0	1.0	1e-1	1.0	1e-1	1e-1
Table 5: Table of best hyperparameters for ARS
Learning rate policy	Policy: Metric:	[]		[32]		[64,64]	
		avg	last	avg	last	avg	last
Acrobot-v1		-^1e-2^^	1e-3	1e-2	1e-2	1e-2	1e-2
MountainCarContinuous-v0		1e-2	1e-2	1e-2	1e-2	1e-2	1e-2
CartPole-v1		1e-2	1e-2	1e-2	1e-2	1e-2	1e-2
Swimmer-v3		1e-2	1e-2	1e-2	1e-2	1e-2	1e-2
InvertedPendulum-v2		1e-2	1e-2	1e-2	1e-2	1e-2	1e-2
Reacher-v2		1e-2	1e-2	1e-3	1e-2	1e-3	1e-3
Hopper-v3		1e-2	1e-2	1e-2	1e-2	1e-2	1e-2
Number of directions and elite directions							
Acrobot-v1		(4,4)	(4,4)	(1,1)	(1,1)	(1,1)	(1,1)
MountainCarContinuous-v0		(1,1)	(1,1)	(1,1)	(16,4)	(1,1)	(1,1)
CartPole-v1		(4,4)	(4,4)	(1,1)	(1,1)	(4,1)	(4,1)
Swimmer-v3		(1,1)	(1,1)	(1,1)	(4,1)	(1,1)	(1,1)
InvertedPendulum-v2		(4,4)	(4,4)	(1,1)	(4,4)	(4,1)	(16,1)
Reacher-v2		(16,16)	(16,16)	(1,1)	(16,4)	(1,1)	(1,1)
Hopper-v3		(4,1)	(4,1)	(1,1)	(1,1)	(1,1)	(1,1)
Noise for exploration							
Acrobot-v1		1e-2	1e-3	1e-1	1e-1	1e-1	1e-1
MountainCarContinuous-v0		1e-1	1e-1	1e-1	1e-1	1e-1	1e-1
CartPole-v1		1e-2	1e-2	1e-1	1e-1	1e-2	1e-2
Swimmer-v3		1e-1	1e-1	1e-2	1e-1	1e-1	1e-1
InvertedPendulum-v2		1e-2	1e-2	1e-2	1e-2	1e-2	1e-2
Reacher-v2		1e-2	1e-2	1e-2	1e-2	1e-2	1e-2
Hopper-v3		1e-1	1e-1	1e-1	1e-1	1e-1	1e-1
34
Published as a conference paper at ICLR 2021
Table 6: Table of best hyperparameters for PSVFs using deterministic policies
Learning rate policy	Policy: Metric:	[]		[32]		[64,64]	
		avg	last	avg	last	avg	last
Acrobot-v1		1e-2	1e-2	1e-4	1e-4	1e-4	1e-2
MountainCarContinuous-v0		1e-2	1e-3	1e-2	1e-4	1e-3	1e-4
CartPole-v1		1e-2	1e-2	1e-2	1e-4	1e-3	1e-4
Swimmer-v3		1e-3	1e-3	1e-3	1e-3	1e-3	1e-3
InvertedPendulum-v2		1e-2	1e-3	1e-4	1e-4	1e-4	1e-4
Reacher-v2		1e-3	1e-2	1e-4	1e-4	1e-4	1e-4
Hopper-v3		1e-3	1e-3	1e-4	1e-4	1e-4	1e-3
Learning rate critic							
Acrobot-v1		1e-3	1e-4	1e-2	1e-2	1e-3	1e-2
MountainCarContinuous-v0		1e-4	1e-3	1e-2	1e-4	1e-3	1e-3
CartPole-v1		1e-2	1e-2	1e-2	1e-3	1e-2	1e-4
Swimmer-v3		1e-4	1e-4	1e-4	1e-4	1e-4	1e-4
InvertedPendulum-v2		1e-3	1e-2	1e-3	1e-4	1e-4	1e-3
Reacher-v2		1e-2	1e-2	1e-3	1e-3	1e-4	1e-4
Hopper-v3		1e-2	1e-2	1e-4	1e-4	1e-2	1e-4
Noise for exploration							
Acrobot-v1		1.0	1.0	1e-1	1e-1	1e-1	1e-1
MountainCarContinuous-v0		1.0	1e-1	1e-1	1.0	1e-1	1e-1
CartPole-v1		1.0	1.0	1.0	1e-1	1e-1	1e-1
Swimmer-v3		1.0	1.0	1.0	1.0	1.0	1.0
InvertedPendulum-v2		1.0	1.0	1e-1	1e-1	1e-1	1e-1
Reacher-v2		1.0	1.0	1.0	1.0	1e-1	1e-1
Hopper-v3		1.0	1.0	1e-1	1e-1	1e-1	1.0
Table 7: Table of best hyperparameters for PSSVFs and PSVFs using stochastic policies
	Algo:	PSSVF		PSVF	
Learning rate policy	Policy:	[]	[64,64]	[]	[64,64]
	Metric:	avg	avg	avg	avg
Acrobot-v1		1e-2	1e-2	1e-2	1e-3
MountainCarContinuous-v0		1e-2	1e-3	1e-2	1e-3
CartPole-v1		1e-3	1e-4	1e-2	1e-3
Swimmer-v3		1e-2	1e-4	1e-3	1e-4
InvertedPendulum-v2		1e-3	1e-4	1e-2	1e-3
Reacher-v2		1e-4	1e-3	1e-2	1e-2
Hopper-v3		1e-4	1e-4	1e-3	1e-4
Learning rate critic					
Acrobot-v1		1e-2	1e-4	1e-4	1e-2
MountainCarContinuous-v0		1e-2	1e-2	1e-3	1e-3
CartPole-v1		1e-2	1e-3	1e-2	1e-2
Swimmer-v3		1e-2	1e-3	1e-3	1e-4
InvertedPendulum-v2		1e-3	1e-3	1e-3	1e-2
Reacher-v2		1e-3	1e-3	1e-3	1e-3
Hopper-v3		1e-3	1e-2	1e-2	1e-4
Noise for exploration					
Acrobot-v1		1.0	1.0	1.0	1.0
MountainCarContinuous-v0		1.0	1e-1	1.0	1e-1
CartPole-v1		1.0	1.0	1.0	1e-1
Swimmer-v3		1.0	1e-1	1.0	1e-1
InvertedPendulum-v2		1.0	1.0	1.0	1e-1
Reacher-v2		1e-1	0.0	1.0	0.0
Hopper-v3		1.0	1e-1	1.0	1e-1
35
Published as a conference paper at ICLR 2021
Table 8: Table of best hyperparameters for PAVFs using deterministic policies
Learning rate policy	Policy: Metric:	[]		[32]		[64,64]	
		avg	last	avg	last	avg	last
MountainCarContinuous-v0		1e-2	1e-3	1e-3	1e-4	1e-4	1e-4
Swimmer-v3		1e-3	1e-3	1e-3	1e-3	1e-3	1e-3
InvertedPendulum-v2		1e-2	1e-3	1e-3	1e-4	1e-4	1e-4
Reacher-v2		1e-3	1e-3	1e-4	1e-4	1e-4	1e-4
Hopper-v3		1e-3	1e-4	1e-4	1e-4	1e-4	1e-3
Learning rate critic							
MountainCarContinuous-v0		1e-4	1e-4	1e-4	1e-3	1e-4	1e-3
Swimmer-v3		1e-4	1e-4	1e-4	1e-4	1e-4	1e-4
InvertedPendulum-v2		1e-3	1e-2	1e-2	1e-4	1e-2	1e-3
Reacher-v2		1e-3	1e-3	1e-3	1e-2	1e-3	1e-3
Hopper-v3		1e-4	1e-3	1e-3	1e-2	1e-4	1e-3
Noise for exploration							
MountainCarContinuous-v0		1.0	1e-1	1e-1	1e-1	1e-1	1e-1
Swimmer-v3		1.0	1.0	1.0	1.0	1.0	1.0
InvertedPendulum-v2		1.0	1.0	1e-1	1e-1	1e-1	1e-1
Reacher-v2		1e-1	1e-1	1e-1	1.0	1.0	1.0
Hopper-v3		1.0	1.0	1e-1	1e-1	1e-1	1.0
Table 9: Table of best hyperparameters for DDPG
Learning rate policy	Policy: Metric:	[]		[32]		[64,64]	
		avg	last	avg	last	avg	last
MountainCarContinuous-v0		1e-2	1e-2	1e-2	1e-4	1e-3	1e-3
Swimmer-v3		1e-3	1e-3	1e-2	1e-2	1e-2	1e-2
InvertedPendulum-v2		1e-4	1e-4	1e-3	1e-3	1e-3	1e-4
Reacher-v2		1e-4	1e-3	1e-2	1e-2	1e-3	1e-3
Hopper-v3		1e-2	1e-2	1e-2	1e-4	1e-2	1e-2
Learning rate critic							
MountainCarContinuous-v0		1e-4	1e-4	1e-4	1e-3	1e-3	1e-3
Swimmer-v3		1e-3	1e-3	1e-3	1e-3	1e-2	1e-3
InvertedPendulum-v2		1e-3	1e-3	1e-3	1e-4	1e-3	1e-3
Reacher-v2		1e-3	1e-3	1e-3	1e-3	1e-3	1e-3
Hopper-v3		1e-3	1e-3	1e-4	1e-4	1e-4	1e-4
Noise for exploration							
MountainCarContinuous-v0		1e-2	1e-2	1e-2	1e-1	1e-1	1e-1
Swimmer-v3		1e-1	1e-1	1e-2	1e-2	1e-2	1e-1
InvertedPendulum-v2		1e-1	1e-1	1e-2	1e-2	1e-2	1e-2
Reacher-v2		1e-1	1e-2	1e-1	1e-1	1e-1	1e-1
Hopper-v3		1e-1	1e-1	1e-1	1e-2	1e-1	1e-2
36