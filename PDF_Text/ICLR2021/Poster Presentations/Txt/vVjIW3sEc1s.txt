Published as a conference paper at ICLR 2021
A Mathematical Exploration of Why Lan-
guage Models Help Solve Downstream Tasks
Nikunj Saunshi, Sadhika Malladi & Sanjeev Arora
Princeton University
{nsaunshi,smalladi,arora}@cs.princeton.edu
Ab stract
Autoregressive language models, pretrained using large text corpora to do well
on next word prediction, have been successful at solving many downstream tasks,
even with zero-shot usage. However, there is little theoretical understanding of
this success. This paper initiates a mathematical study of this phenomenon for
the downstream task of text classification by considering the following questions:
(1) What is the intuitive connection between the pretraining task of next word
prediction and text classification? (2) How can we mathematically formalize this
connection and quantify the benefit of language modeling? For (1), we hypoth-
esize, and verify empirically, that classification tasks of interest can be reformu-
lated as sentence completion tasks, thus making language modeling a meaningful
pretraining task. With a mathematical formalization of this hypothesis, we make
progress towards (2) and show that language models that are -optimal in cross-
entropy (log-perplexity) learn features that can linearly Solve such classification
tasks with O(√e) error, thus demonstrating that doing well on language modeling
can be beneficial for downstream tasks. We experimentally verify various assump-
tions and theoretical findings, and also use insights from the analysis to design a
new objective function that performs well on some classification tasks.
1	Introduction
The construction of increasingly powerful language models has revolutionized natural language
processing (NLP). Using gigantic text corpora and a cross-entropy objective, language models are
trained to predict a distribution over the next word to follow a given context (piece of text). Pretrained
language models are useful for many downstream NLP tasks, either as initializations (Ramachandran
et al., 2017; Howard & Ruder, 2018) or as a source of contextual word embeddings (McCann et al.,
2017; Peters et al., 2018). Recent models (Radford et al., 2019; Brown et al., 2020) have even
bypassed the need for careful fine-tuning and have demonstrated strong performance on downstream
tasks without fine-tuning. This work aims to understand this incredible success of language models.
Since next word prediction is a powerful test of language understanding, at an intuitive level it is
believable that doing well on language modeling can help with many diverse NLP tasks. At the same
time, it is quite intriguing how improvements in the test perplexity of language models translate
to better downstream performance. Attempting to understand this phenomenon naturally raises the
following questions: (a) why should training on the next-word prediction task, with the cross-entropy
objective, result in useful features for downstream tasks? (b) what role do inductive biases of the
model architecture and training algorithms play in this empirical success? Given the nascency of
deep learning theory, it is very challenging to say anything mathematically precise about (b) for
deep networks. Given these difficulties, this paper focusses on the mathematical study of (a) by
exploring if and how quantitative improvements on downstream NLP tasks can be mathematically
guaranteed for language models that do well on the cross-entropy objective. As a first cut analysis,
we restrict attention to text classification tasks and the striking observation that they can be solved
fairly well with linear classifiers on top of fixed language models features, i.e. without finetuning
(Table 1). Although we treat models as black boxes, just first-order optimality conditions of the
cross-entropy objective reveal interesting properties of learned features, leading to an understanding
of their success on classification tasks. Insights from the analysis help us construct a simple objective
1
Published as a conference paper at ICLR 2021
(Quad), that provably learns useful features for classification tasks, as also verified empirically. We
summarize our contributions along with an overview of the paper below.
In Section 2, we set up notation and formally describe language modeling and the ubiquitous low-
dimensional softmax parametrization, along with a description of the cross-entropy objective and
properties of its optimal solutions. We then describe the observation, in Section 3.1, that text clas-
sification tasks of interest can be reformulated as sentence completion tasks. Amenability to such a
reformulation is mathematically formalized (Section 3.2) as the classification task being a natural
task: tasks that can be solved linearly using conditional distribution over words following an input
text. Section 4 presents our main results, theorems 4.1 and 4.2, that use the above formalization to
mathematically quantify the utility of language model features on natural tasks: e-optimal language
model (in cross-entropy) will do O(√e)-well on such tasks. Theorem 4.2 shows a stronger result
for low-dimensional softmax models by leveraging a new tool, conditional mean features (Defini-
tion 4.1), which we show (Section 6) to be effective in practice. The usefulness of the language
model features themselves is demonstrated by arguing a weak linear relationship between them and
conditional mean features. In Section 5.2, we present a new mathematically motivated objective
(Quad) that has formal guarantees. Experiments in Section 6 verify the sentence completion refor-
mulation idea and the good performance of conditional mean features on standard benchmarks.
1.1	Related work
Text embedding methods: Prior to language models, large text corpora like Wikipedia (Merity
et al., 2016) were used to learn low-dimensional embeddings for words (Mikolov et al., 2013b;a;
Pennington et al., 2014) and subsequently for sentences (Kiros et al., 2015; Arora et al., 2017;
Pagliardini et al., 2018; Logeswaran & Lee, 2018) for downstream task usage. These methods were
inspired by the distributional hypothesis (Firth, 1957; Harris, 1954), which posits that meaning of
text is determined in part by the surrounding context. Recent methods like BERT (Devlin et al.,
2018) and variants (Lan et al., 2019; Yang et al., 2019; Liu et al., 2019) learn models from auxiliary
tasks, such as sentence completion, and are among the top performers on downstream tasks. In this
work we consider autoregressive models and make a distinction from masked language models like
BERT; Table 2 shows that language model and BERT features have comparable performances.
Language models for downstream tasks: We are interested in language models (Chen & Good-
man, 1999), especially those that use neural networks to compute low-dimensional features for con-
texts and parametrize the next word distribution using softmax (Xu & Rudnicky, 2000; Bengio
et al., 2003). Language models have shown to be useful for downstream tasks as initializations (Ra-
machandran et al., 2017; Howard & Ruder, 2018) or as learned feature maps (Radford et al., 2017;
McCann et al., 2017; Peters et al., 2018). The idea of phrasing classification tasks as sentence com-
pletion problems to use language models is motivated by recent works (Radford et al., 2019; Puri
& Catanzaro, 2019; Schick & Schutze, 2020) that show that many downstream tasks can be solved
by next word prediction for an appropriately conditioned language model. This idea also shares
similarities with work that phrase a suite of downstream tasks as question-answering tasks (McCann
et al., 2018) or text-to-text tasks (Raffel et al., 2019) and symbolic reasoning as fill-in-the-blank
tasks (Talmor et al., 2019). Our work exploits this prevalent idea of task rephrasing to theoretically
analyze why language models succeed on downstream tasks.
Relevant theory: Since the success of early word embedding algorithms like word2vec (Mikolov
et al., 2013a) and GloVe (Pennington et al., 2014), there have been attempts to understand them
theoretically. Levy & Goldberg (2014) argue that word2vec algorithm implicitly factorizes the
PMI matrix. Noise Contrastive Estimation (NCE) theory is used to understand word embeddings
(Dyer, 2014) and to show parameter recovery for negative sampling based conditional models (Ma
& Collins, 2018). A latent variable model (Arora et al., 2016) is used to explain and unify various
word embedding algorithms. Theoretical justification is provided for sentence embedding meth-
ods either by using a latent variable model (Arora et al., 2017) or through the lens of compressed
sensing (Arora et al., 2018). Also relevant is recent work on theory for contrastive learning (Arora
et al., 2019; Tosh et al., 2020b;a; Wang & Isola, 2020) and reconstruction-based methods (Lee et al.,
2020), which analyze the utility of self-supervised representations learned for downstream tasks.
Our work is the first to analyze the efficacy of language model features on downstream tasks.
2
Published as a conference paper at ICLR 2021
2	Language modeling and optimal solutions
We use S to denote the discrete set of all contexts, i.e. complete or partial sentences (prefixes), W
to denote the vocabulary of words, with V = |W | being the vocabulary size. For a discrete set A,
let ∆A denote the set of distributions on A. We use p, pL ∈ ∆S to denote probability distributions
over S, and p,∣s,p*∣s ∈ ∆w to denote conditional distributions, where p.∣s(w) is the predicted
probability of word W following context S and p；§(w) denotes the true conditional probability.
Boldface p.∣s,p：§ ∈ RV denote vectors of probabilities for p.∣s,p*∣s ∈ ∆w. For V ∈ RV, v(w)
indexes the coordinate for W ∈ W; p.∣ s (w) is the probability of W according to p.∣ s. We use φw ∈ Rd
to denote a d-dimensional embedding for word w; word embeddings are stacked into the columns
Φ ∈ Rd×V. We use f : S → Rd for a feature map from contexts to d-dimensional embeddings, e.g.
f(s) can be the output of a Transformer model for input context s ∈ S. For embeddings {θs}s∈S
with θs ∈ RD (any D), we use {θs} to denote g : S → RD such that g(s) = θs.
2.1	Language modeling using cross-entropy
Language model aims to learn the true distribution ofa text corpus and a popular approach to do so
is through next word prediction. Given a context (e.g., a sentence s ∈ S), it predicts a distribution
p∙∣s over the word to follow, e.g. for the context “The food was "，the model could place high prob-
abilities on words “delicious”, “expensive”, “bland”, etc. We use pL to denote the true distribution
over the context set S in the language modeling corpus. A standard approach is to minimize the ex-
pected cross-entropy loss between the true distribution p；§ and the model prediction p.∣s. We define
the cross-entropy loss for a language model with output vector of probabilities {p,∣s}s∈s as
'xent({p∙∣s}) = E E [-log(P∙∣S(W))] = E ['xent,S(P∙∣S)]
S〜PL W〜P*∣s	S〜PL
(1)
To understand what language models learn, we look at the optimal solution of the cross-entropy
objective. While one cannot practically hope to learn the optimal solution due to optimization,
statistical and expressivity limitations, the optimal solution at least tells us the best that language
modeling can hope to do. A well-known property of cross-entropy objective is that its optimal
solution is p*^, which can be proved by noting that 'xent,S(p.|S)= DKL(p*∣s,P∙∣s) + C.
Proposition 2.1 (Cross-entropy recovers p；§). The unique minimizer of 'Xent({p∙∣s}) is P∙∣s = p；S
for every s ∈ support(pL).
2.2	Softmax parametrized language modeling
Unlike traditional language models like n-gram models, neural language models parametrize the
conditional distribution p.|§ as a softmax computed using low dimensional embeddings. For an
embedding θ ∈ Rd, the softmax distribution over W using word embeddings Φ ∈ Rd×V is
Pθ,φ(w) = eθ>φw/Zθ, where Zθ = Pw,∈w eθ>φw0 is the partition function. While pθ,φ depends
on Φ, we will use pθ instead whenever Φ is clear from context. Just like p；§, we can interpret
Pθ ∈ RV as a vector of probabilities for the distribution pθ .
We now describe the abstraction for softmax models that is applicable to most neural models. A
language model first embeds a context s into f (s) ∈ Rd using a feature map f : S → Rd that
is parametrized by an architecture of choice (e.g. Transformer (Vaswani et al., 2017)). The output
conditional distribution is set to be the softmax distribution induced by the context embedding f(s)
and word embeddings Φ, i.e. p,|§ = p〃s). The cross-entropy in its familiar form is presented below
'xent(f, Φ)= E E [- lθg(Pf (S)(w))] = E E [-f (s)>φw] + lθg(Zf (S))	⑵
S〜PL W〜P*|s	S〜PL W〜p*∣s
We rewrite it as 'xent(f, Φ) = E ['xent,S (f(s), Φ)], where 'xent,S(θ, Φ) = 'xent,S(Pθ,φ) is the cross-
S〜PL
entropy loss for a context s that uses embedding θ. Analogous to Proposition 2.1, we would like to
know the optimal d-dimensional feature map f； and the induced conditional distribution Pf*(S)1.
1A finite minimizer may not always exist. This is handled in Section 4 that deals with -optimal solutions.
3
Published as a conference paper at ICLR 2021
Proposition 2.2 (Softmax models recover p； S on a subspace). Fix a fixed Φ ,if f * ∈
argminf:s→Rd 'Xent(f, Φ) exists, then Φpf*(s) = Φp*s for every S ∈ SuPPort(PL).
Unlike Proposition 2.1, p『*(S) ∈ RV is only guaranteed to be equal to P*s ∈ RV on the d-
dimensional subspace spanned by rows of Φ ∈ Rd × V. We may not learn p*§ exactly when
d < V, but this result at least guarantees learning p*§ on a linear subspace determined by word
embeddings Φ. This forms the basis for our main results later and is proved by using the first-
order optimality condition, i.e. Vθ'xent,S(f *(s)) = 0, ∀s ∈ S. The gradient of cross-entropy is
Vθ'xent,s (θ) = -Φp*s + vθzθ∕zθ = -Φp*^ + Φpθ. Setting it to 0 completes the proof. We use the
properties of optimal solutions to understand why language models help with classification tasks.
3	Using language models for classification tasks
Sections 2.1 and 2.2 suggest that language models aim to learn p*§,ora low-dimensional projection
Φp* s. Thus to understand why language models help with downstream tasks, a natural starting point
is to understand how access to p:§ can help with downstream tasks. In a thought experiment, we
use oracle access to p*§ for any S and demonstrate that sentence classification task can be solved by
reformulating it as a sentence completion problem and using p;§ to get completions to predict the
label. This sentence completion reformulation is mathematically formalized as natural tasks.
3.1	Sentence completion reformulation
For exposition, we consider the sentence classification task of sentiment analysis, where the inputs
are movie reviews (subset ofS) and labels belongs to {±1}, denoting positive and negative reviews.
Classification task as sentence completion: Can we predict the label for a movie review S by using
p;∣s? One way is to use p*§ to compare probabilities of “:)” and “:(" following a movie review and
to predict sentiment based on which is higher. This seems like a reasonable strategy, since “:)” is
likelier than “:(” to follow a positive movie review. One issue, however, is that p*§ will place much
higher probability on words that start sentences, like “The”, rather than discriminative words useful
for the task. To allow a larger set of grammatically correct completions, we can append a prompt like
“This movie is ” at the end of all movie reviews and query probabilities of indicative adjectives like
good, bad, interesting, boring etc. that are better indicators of sentiment. This approach of adding
a prompt can also work for other classification tasks. For the AG news dataset (Zhang et al., 2015)
containing news articles from 4 categories (world, science/tech., sports, business), a prompt like
“This article is about ” can help solve the task. The theoretical and practical relevance of prompts
is discussed in Theorem 4.1, and Section 6 respectively. We note that the choice of prompts and
completion words is less important than the underlying idea of sentence completion reformulation
and its formalization.
Solving tasks using a linear function of p*§: The above process is actually a sub-case of using
a linear classifier on top of p;§ ∈ RV. For sentiment analysis, if w+ = “:)” and W- = ":("，
then the sign of p*§(w+) - p*s(w-) can predict the sentiment. This strategy can be expressed
as v>p*s, where the linear classifier V ∈ RV has v(w+) = 1, v(w-) = -1 and v(w0) = 0 for
w0 ∈ W\{w+, w-}. Similarly with the prompt, we can assign positive weights in v to adjectives
like “good” and negative weights to adjectives like “boring”. Strength of sentiment in different ad-
jectives (e.g., “good” vs “amazing”) can be captured through different weights. This equivalence
between sentence completion reformulation and linear classifier on p:§ is further explored in Sec-
tion D.1. Other tasks can be similarly solved with a different set of words for each class. We verify
experimentally that SST and AG news tasks can be solved by a linear function of probabilities of
just a small subset of words in Section 6 and for many other classification tasks in Section F.1, thus
lending credibility to the sentence completion view.
4
Published as a conference paper at ICLR 2021
3.2	Natural classification tasks
We now translate the above sentence completion reformulation into a reasonable mathematical char-
acterization for classification tasks of interest. Firstly we formally define text classification tasks and
the standard metric for performance of linear classification on fixed features. A binary classification
task2 T is characterized by a distribution pT over S × {±1}, where the input s is a piece of text
from S and the label y is in {±1}. Given a feature map g : S → RD (arbitrary D), T is solved by
fitting a linear classifier v ∈ RD on top of g(s) and the metric of classification loss is
't(g, V)= E(s,y)〜PT	['(v>g(s),y)]	；	'tS)=	inf	't(g,	V)
v∈RD
(3)
where ' is a I-LiPschitz surrogate to the 0-1 loss, like the hinge loss '(y, y) = (1 - yy)+ or the
logistic loss '(y,y) = log(1 + e-yy). For given embeddings {θs}s∈s, the classification loss is
written as't({θs}, V) = E(s,y)〜PT['(v>θs,y)].
We now formalize classification tasks amenable to sentence comPletion reformulation, from Sec-
tion 3.1), as (τ, B)-natural tasks, i.e. tasks that achieve a small classification loss of τ by using a
linear classifier with '∞-norm bounded3 by B on top of features p：§ ∈ RV.
Definition 3.1. A classification task T is (τ, B)-natural if	min	't({p%}, V) ≤ τ.
v∈RV ,kvk∞≤B	|
While We motivated this formalization of linear classification over p；S in Section 3.1, We provide a
mathematical justification in Section D.1, along with interpretations for τ and B that relate them to
the Bayes optimal predictor and probability mass of indicative words respectively. Low dimensional
softmax models, however, only learn p：§ in the subspace of Φ, per Proposition 2.2. Thus We are
also interested in subset of tasks that this subspace can solve.
Definition 3.2. TaskTis (τ, B)-naturalw.r.t. Φ ∈ Rd×V if	min	'T ({p；| }, V) ≤ τ.
v∈row-span(Φ),kvk∞ ≤B
Note that every (τ, B)-natural task w.r.t. Φ is trivially (τ, B)-natural, though the converse may not
hold. However it can be argued that if Φ has some “nice properties”, then (τ, B)-natural tasks of
interest will roughly also be (τ, B)-natural w.r.t. Φ. Capturing the synonym structure of words can
be such a nice property, as discussed in Section D.2. A better understanding of these properties of
word embeddings Φ can potentially enable better performance of language models on downstream
tasks. In fact, Section 5.2 describes a carefully designed objective that can learn word embeddings
with desirable properties like synonyms having similar embeddings. In the subsequent sections, we
use the above formalization to show guarantees for language models on natural tasks.
4	Guarantees for language models on natural tasks
We now show guarantees for features from language models on natural tasks in two cases: 1) for an
arbitrary language model {p.∣s} where we use V-dimensional features p.∣s ∈ RV for downstream
tasks and 2) for softmax language model (f, Φ) where we use new d-dimensional features Φpf(s) ∈
Rd. Since we cannot practically hope to learn the optimal solutions described in propositions 2.1 and
2.2, we only assume that the language models are -optimal in cross-entropy. We first define 'x；ent to
be the minimum achievable cross-entropy and 'xJent(Φ) to be the minimum achievable cross-entropy
by a d-dimensional softmax language model using Φ; clearly 'x；ent ≤ 'Xent(Φ).
'Xent = 'xent({p*∣s}), '；ent&) = E Linf∕xent,s(θ, Φ)l	(4)
S 〜PL 1θ∈Rd
We first present the results for arbitrary language models with a proof sketch that describes the main
ideas, following which we present our main results for softmax language models.
4.1 Arbitary language models
We show guarantees for a language model that is E-OPtimal, i.e. 'xent({p∙∣s}) - 'Xent ≤ a on (τ,B)-
natural tasks. An important consideration is that the language model distribution pL of contexts is
2EXtending to k-way tasks is straightforward.
3'∞ makes sense since ∣∣p*∣s kι = 1 & ∣∣∙∣∣∞ is dual norm of ∣∣ ∙∣∣ι.
5
Published as a conference paper at ICLR 2021
often a diverse superset of the downstream distribution pT (defined in Section 2.2) over sentences,
thus requiring Us to show how guarantees of p∙∣s ≈ p；§ on average over the distribution S 〜PL
transfer to guarantees on a subset PT. In the worst case, all of the E error in cross-entropy by {p.∣ s} is
incurred on sentences from the subset pT, leading to pessimistic bounds4. In practice, however, the
errors might be more evenly distributed across PL, thus bypassing this worst case bound. As a first
step, we present the worst case bound here; stronger guarantees are in Section 5.1. The worst-case
coefficient γ(PT), defined below, captures that PT is a γ (PT)-fraction of PL.
γ(PT) = sup{γ ∈ (0, 1] : PL (s) ≥ γPT(s) ∀s ∈ S}	(5)
We now present our results that applies to any language model, regardless of the parametrization
(e.g., n-gram models, softmax models). The result suggests that small test cross-entropy (hence test
perplexity) is desirable to guarantee good classification performance, thus formalizing the intuition
that better language models will be more useful for downstream tasks.
Theorem 4.1. Let {p.∣s} be a language model that is E-optimal, i.e. 'χent({p.∣s}) — 'Xent ≤ G for
some E > 0. For a classification task T that is (τ, B)-natural, we have
't ({p∙∣s}) ≤ T + ,2B2E(Y(PT))T
This upper bounds classification loss on task T for V-dimensional features {p∙∣s} from an E-OPtimaI
language model. We discuss factors that lead to small upper bound and corresponding intuitions.
•	E is small: learned language model has smaller cross-entropy (log-perplexity)
•	τ is small: task can be solved well through a sentence completion reformulation with a set of
indicative words as completions, as in Section 3.1, and has small Bayes error (cf. Section D.1)
•	B is small: set of indicative words has high probability mass in PXs (cf. Section D.1). This could
potentially explain the superior performance when prompts are added (Section 6).
•	γ(PT) is large: PT is closer toPL; note that γ(PT) ≤ 1 with equality if and only ifPT = PL
Thus the bound captures meaningful intuitions about good performance of language models on
downstream tasks. We provide a detailed proof sketch in Section E.1 and a strengthened version
of this (Theorem B.1) is presented in Section E.6. Proving this result requires connecting the clas-
sification loss with language modeling cross-entropy loss and dealing with distribution mismatch;
we present a rough outline to do so below. Since T is (τ, B)-natural, let vX be the classifier with
IlvXI∣∞ ≤ B and't({pχs}, VX) ≤ T. The result follows from the following 3 inequalities:
't ({p∙∣s}, V*) — 't({pχs}, vX) ≤ ʌE E [(vx>(P∙∣s — pχs))2]
V s〜PT	1
E [(v*>(P∙∣s - P*s))2] ≤ Y(PT)T E [(v*>(P∙∣s- P*s))2]
s 〜PT	I	s 〜PL	I
∀v ∈ RV, (v>(p∙∣s - P*s))2 ≤ 2kvk∞('xent,s(p∙∣s) - 'xent,s(p*∣s))
... Lipschitzness + Jensen’s
... Transfer PT to PL
... Pinsker’s inequality
The first and third inequalities (Lemma E.8 and Lemma E.3) connect the classification loss to the
cross-entropy loss in language modeling, while the second inequality deals with distribution mis-
match between PL and PT. We now present a stronger result for softmax models.
4.2	Softmax language model with conditional mean features
We now consider a softmax language model with feature map f that satisfies 'xent(f, Φ) 一 'Xent(Φ) ≤
e; suboptimality is measured w.r.t. the best d-dimensional model, unlike Theorem 4.1,. Note that
Theorem 4.1 can be invoked here to give a bound of't({Pf(s)}) ≤ T + O(B,e + Eφ) on (τ, B)-
natural tasks, where Eφ = 'Xent(Φ) — 'Xent is the suboptimality of the best d-dimensional model. The
fixed error of O(B∕Eφ) (even when E = 0), however, is undesirable. We improve on this by proving
a stronger result specifically for softmax models. Inspired by Proposition 2.2, our guarantees are for
features Φpf(s) ∈ Rd called conditional mean features.
Definition 4.1 (Conditional Mean Features). For a feature map f : S → Rd and Φ ∈ Rd×V, we
define conditional mean features ΦPf : S → Rd, where ΦPf (s) = Φpf (s), where pf (s) ∈ RV.
4For instance if PT is 0.001 fraction of pL, {p.∣s} could have 1000e error on PT and 0 error on rest of pL.
6
Published as a conference paper at ICLR 2021
We now present the result for softmax language models that has similar implications as Theorem 4.1,
but with above-mentioned subtle differences.
Theorem 4.2. For a fixed Φ, let f be features from an -optimal d-dimensional softmax language
model, i.e. 'Xent(f, Φ) — 'Xent(Φ) ≤ e. For a classification task T that is (τ, B)-natural w.r.t. Φ,
't(φPf) ≤ T + J2B2e (Y(PT))T
This result guarantees good performance of conditional mean features Φpf on some natural tasks,
thereby suggesting a novel way to extract features for downstream tasks. We empirically verify the
good performance of Φpf(s) on classifications tasks (Section 6) and also find a O(√e)-like behavior
(Section F.5). The proof (Section E.3) is similar to that of Theorem 4.1, the main difference being the
use of the following inequality, proved using a softmax variant of Pinsker’s inequality (Lemma E.4).
∀v ∈ row-span阐，(VT(Pf(S)- p*|S))2 ≤ 2kvk∞%ent,S(Pf(S))-	inf ∕xent,S(Pf*(S)))
f * (S)∈Rd
The more general result (Theorem 5.1) replaces γ(pT) with a more refined coefficient (Section 5.1).
While guarantees are only for natural tasks w.r.t. Φ, Section D.2 discusses why this might be enough
for tasks of interest if word embeddings Φ satisfy nice properties.
4.3	Φpf (s) IS A LINEAR FUNCTION OF f(s)
Theorem 4.2 shows that Φpf is useful for linear classification. However, using feature mapf directly
is more standard and performs better in practice (Section 6). Here we argue that there is a linear
relation between f and Φpf if word embeddings Φ satisfy a certain Gaussian-like property, which
we show implies that tasks solvable linearly with Φpf are also solvable linearly using f.
Assumption 4.1. There exists a symmetric positive semidefinite matrix A ∈ Rd×d, a vector b ∈ Rd
and a constant C ∈ R such that log(Zθ) = 2θτ Aθ + θ>b + Cfor any θ ∈ Rd.
If word embeddings were distributed as Gaussians, i.e. V columns of Φ are sampled from N(μ, Σ)
independently, it is not hard to show (Lemma E.1) that log(Zθ) ≈ 11 θ>Σθ + θ>μ + log(V). While
some papers (Arora et al., 2016; Mu & Viswanath, 2018) have noted that word embeddings are
fairly random-like in the bulk to argue that the log partition function is constant for kθk2 = 1, our
quadratic assumption is a bit stronger. However, empirically we find the fit to be very good, as
evident in Figure 1. Under the above assumption, we can show a linear relation between f and Φpf.
Lemma 4.3. Under Assumption 4.1, feature map f satisfies Φpf (s) = Af (s) + b, ∀s ∈ S.
Corollary 4.1. Under same setting as Lemma 4.3 and Theorem 4.2, 't (f) ≤ T + O(B√e).
This shows that f itself is good for natural classification tasks. However, in practice, the linearity
between f and Φpf only weakly holds on features from pretrained GPT-2 (Radford et al., 2018).
E kΦpf (S)-Af (S)-bk2
The fractional residual norm of the best linear fit, i.e. r = ——E kφpj(S^2-------, measured for
S〜P
different distributions (r = 0 is perfect fit) are 0.28 for SST, 0.39 for AG News, and 0.18 for IMDb
contexts. This non-trivial linear relationship, although surprising, might not completely explain the
success off, which usually performs better than Φpf; we leave exploring this to future work.
5	Extensions
5.1	Better handling of distributional shift
The bounds in the previous section use the coefficient γ(pT) to transfer guarantees from pL to pT
and we define a more refined notion of transferability here. The coefficient γ(pT) is independent of
the learned model and assumes a worst case distribution of errors. For the refined coefficient, we first
define the error made in predicted probabilities by a softmax language model f as ∆{pf(S)} (s) =
Pf (s) — P*,. For any distribution P ∈ ∆s, we define uncentered covariance of a function g : S →
RD as Σp(g) = ES〜P [g(s)g(s)>]. The refined transferability coefficient is then defined as
Y(P;中Pf) = (肉L (φ∆{pf(s)}厂1 ςp(φδ{p"s)})ςPL (皿功3}厂21)
7
Published as a conference paper at ICLR 2021
Figure 1: Learned quadratic function v/s log partition function on various datasets for features com-
puted from pre-trained GPT-2 to verify Assumption 4.1. We also plot the y = x line for reference.
We state the refined result for softmax language models; detailed results are deferred to Section B.
Theorem 5.1 (Simplified). In the same setting as Theorem 4.2, 't(Φpf) ≤ T +

2B2e
γ(pτ ；》Pf)
It is easy show that γ(pT; Φpf) ≥ γ(pT), so this is indeed a stronger bound. The coefficient
γ(pT; Φpf) measures how average error on f on pL can propagate to pT. This can potentially
be much smaller than γ (pT) due to some inductive biases of f . For instance, if errors made by
the model are random-like, i.e. ∆{p,(s)}(s) 〜 ρ, independently of s, then Σjjl (Φ∆{p,(s)}) ≈
Σp(Φ∆{Pf(s)}) ≈ En〜p[nn>], making Y(p; φPf) ≈ L Independence prevents accumulation of
language modeling error on contexts from pT, bypassing the worst case transfer of γ(pT).
5.2	Quad: A new objective function
In Definition 3.2 we discuss how low dimensional softmax language models learn a linear projection
of P*,, only solving tasks that lie in the row span of word embeddings Φ. Although Φ defines tasks
that language model features can solve, the standard cross-entropy objective does not lend a simple
closed form expression for optimal Φ. This motivates the construction of our Quad objective, that
has two nice properties:⑴ the optimal feature map f * is a linear function of p*∣s and thus can solve
some natural tasks, and (2) the optimal Φ* has an intuitively meaningful closed-form solution.
'quad(f, Φ) = E
E [-f(s)>φw ] + 1 kΦ>f(s)k2
W 〜P*∣s	2
(6)
The Quad objective is very similar to the cross-entropy objective from Equation (2), with the log
partition function replaced by a quadratic function, inspired in part by Assumption 4.1. We can
derive the optimal solution Φ* that depends on the eigen-decomposition of a substitutability matrix.
Definition 5.1. The substitutability matrix is defined to be Ω*
:=E	[p*∣s P*s>i ∈ RVXV. If
S〜PL
Ω* = USU > is the eigendecomposition, then Ud ∈ RV ×d is matrix of top d eigenvectors of Ω*.
The matrix Ω* captures substitutability between pairs of words. Words W and w0 are substitutable
if they have identical conditional probabilities for every context s ∈ S and thus can replace occur-
rences of each other while still providing meaningful completions. By definition, these words satisfy
Ω*[w] = Ω*[w0]. Such pairs of words were called “free variants” in the work on distributional se-
mantics (Harris, 1954), and capture the notion of synonyms; more in Section D.2.
Theorem 5.2. Let f*, Φ* = arg minf,φ 'quad(f, Φ). Then Φ* = BU>, for full rank B ∈ Rd×d.
Also, for a classification task T that is (τ, B)-natural w.r.t. Φ*, we have't(f *) ≤ τ.
Thus f * excels on natural tasks w.r.t. Φ*, which in turn, is the best d-dimensional projection of Ω*.
Thus words w, w0 ∈ W that are synonyms (hence substitutable) will satisfy φ*w = φ*w0 , fulfilling the
desired property for word embeddings discussed in Definition 3.2.
We train using the Quad objective and compare its performance to a similarly trained GPT-2 lan-
guage model. The results in Table 3 suggest that Quad performs comparably to Φpf from the
cross-entropy objective, which fits our theory since both are linear functions of p:§. Section F.3 has
more details and experiments. The goal of testing Quad is to demonstrate that theoretical insights
can aid the design of provably effective algorithms. Refer to Section C for more details on Quad.
8
Published as a conference paper at ICLR 2021
Table 1: Accuracy (%) on k-way linear classification using fixed GPT-2 features. Good performance
of features f (s), conditional mean features Φpf (s) and meaningful subset of ≤ 30 (and ≤ 2k)
coordinates of pf (s) verify the sentence completion reformulation and main results. The numbers
right below the features denote dimensionality of the features. An asterisk indicates that we added
a task-specific prompt. Other baselines are fine-tuning (FT, Section F.2) and random projection of
pf(s) (rand. proj.). Sentence version of SST (train/test: 6.9K/1.8K) is used.
Task	k	f(s) 768	Φpf (s) 768	pf(s) (subset) ≤ 30	pf (s) (class words) ≤ 2k	Pf (S) (rand. proj.) 768	FT
SST	2	87.5	83.3	82.6	78.7	67.5	91.4
SST*	2	89.4	87.3	85.4	79.1	76.4	92.3
SST fine	5	49.2	43.5	44.0	39.2	23.1	50.2
SST fine*	5	49.4	48.6	47.6	40.3	28.8	53.5
AG	4	90.7	84.6	83.8	75.4	58.5	94.5
AG*	4	91.1	88.2	86.1	75.1	63.7	94.4
6	Experiments
We use experiments to verify (1) linear classification on fixed language model features does compa-
rably to fine-tuning the features, (2) sentence completion reformulation (Section 3.1), i.e. tasks can
be solved using probabilities for indicative words, (3) conditional mean features are effective.
Tasks using linear function of p；§: We validate our claims from Section 3 that classification tasks
can be solved by linear functions of p：§. Since p：§ is never available, We instead use the output
features f (S) and probabilities p∙∣s := Pf(S) from a small pretrained GPT-2 model (Radford et al.,
2019). Table 1 demonstrates that on binary and fine-grained Stanford Sentiment Treebank (SST)
(Socher et al., 2013) and AG NeWs (Zhang et al., 2015) tasks, probabilities pf(s) of just 30 or
so task-relevant tokens (see Section F.1) can solve the tasks. Even just one/tWo token per class
(“class Words”) yields non-trivial performance. Furthermore, We validate the sentence completion
reformulation in Section 3.1 by using the probabilities pf(s) after adding a task specific prompt and
consistently observing improved performance, including for fine-tuning (FT) With small datasets.
Φpf and f are good features: We first note that linear classification over fixed features f(s) from
the pretrained model performs comparably to the FT baseline. We further validate Theorem 4.2 by
verifying that the conditional mean features Φpf (s) also linearly solve doWnstream tasks fairly Well.
This performance is comparable to, but alWays Worse than f (s), as seen in columns 3 and 4 of Table
1. We again find that adding a prompt improves performance. Note that a random projection ofpf(s)
to same dimensions as Φpf (s) has very poor performance. Section E.5 has results for a Wider range
of classification tasks. Evidence for Assumption 4.1 is provided by learning a quadratic function to
fit the log partition function of features from pretrained GPT-2 model (see Section F.4). Figure 1
demonstrates that the fit holds for its training and unseen data (e.g., WebText (Radford et al., 2019)).
7	Conclusions and future work
We provide intuitive and mathematical explanations for the success of language model features on
classification tasks by reformulating them as sentence completion problems. This reformulation
is formalized as natural tasks: those that can be solved linearly using the conditional probability
distribution p；§. Insights from our analysis help design the Quad objective that provably learns
good features for these natural tasks. We hope our analysis Will inspire other mathematical insights
into language models. While Section 4.3 argues linearity betWeen conditional mean features Φpf
and f, it is insufficient to explain the observed superiority of f over Φpf. We leave exploring this
limitation of our analysis to future Work. Guarantees for softmax models are for natural tasks W.r.t.
Φ, thus knowing the optimal d-dimensional word embeddings Φ* for 'xent(f, Φ) is also important.
Other meaningful directions include providing guarantees for other successful models like BERT
(Devlin et al., 2018) and more diverse downstream tasks. Although we would like to show stronger
guarantees by exploiting model and algorithmic inductive biases, as well as study the setting of fine-
tuning language model features, lack of a good theory of deep learning is the current bottleneck.
Acknowledgments: Sanjeev Arora, Sadhika Malladi and Nikunj Saunshi are supported by NSF,
ONR, Simons Foundation, Amazon Research, DARPA and SRC.
9
Published as a conference paper at ICLR 2021
References
Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. A latent variable model
approach to pmi-based word embeddings. Transactions of the Association for Computational
Linguistics, 2016.
Sanjeev Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat baseline for sentence
embeddings. In Proceedings of the International Conference on Learning Representations, 2017.
Sanjeev Arora, Mikhail Khodak, Nikunj Saunshi, and Kiran Vodrahalli. A compressed sensing view
of unsupervised text embeddings, bag-of-n-grams, and LSTMs. In Proceedings of the Interna-
tional Conference on Learning Representations, 2018.
Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saunshi.
A theoretical analysis of contrastive unsupervised representation learning. In Proceedings of the
36th International Conference on Machine Learning, 2019.
Soren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives.
Dbpedia: A nucleus for a web of open data. In Proceedings of the 6th International The Semantic
Web and 2nd Asian Conference on Asian Semantic Web Conference, 2007.
Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic
language model. Journal of machine learning research, 2003.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
Stanley F Chen and Joshua Goodman. An empirical study of smoothing techniques for language
modeling. Computer Speech & Language, 13, 1999.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Chris Dyer. Notes on noise contrastive estimation and negative sampling. arXiv preprint
arXiv:1410.8251, 2014.
John R Firth. A synopsis of linguistic theory, 1930-1955. Studies in linguistic analysis, 1957.
Zellig Harris. Distributional structure. Word, 1954.
Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification.
arXiv preprint arXiv:1801.06146, 2018.
Minqing Hu and Bing Liu. Mining and summarizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2004.
Mikhail Khodak, Nikunj Saunshi, Yingyu Liang, Tengyu Ma, Brandon Stewart, and Sanjeev Arora.
A la carte embedding: Cheap but effective induction of semantic feature vectors. In Proceedings
of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), 2018.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Ryan Kiros, Yukun Zhu, Russ R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. Skip-thought vectors. In Advances in neural information processing systems,
2015.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-
cut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint
arXiv:1909.11942, 2019.
Jason D. Lee, Qi Lei, Nikunj Saunshi, and Jiacheng Zhuo. Predicting what you already know helps:
provable self-supervised learning. arXiv preprint arXiv:2008.01064, 2020.
10
Published as a conference paper at ICLR 2021
Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In Ad-
vances in neural information processing systems, 2014.
Xin Li and Dan Roth. Learning question classifiers. In Proceedings of the 19th international
conference on Computational linguistics-Volume 1, 2002.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019.
Lajanugen Logeswaran and Honglak Lee. An efficient framework for learning sentence representa-
tions. In Proceedings of the International Conference on Learning Representations, 2018.
Zhuang Ma and Michael Collins. Noise contrastive estimation and negative sampling for conditional
models: Consistency and statistical efficiency. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, 2018.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher
Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting
of the ACL: Human Language Technologies, 2011.
Julian J. McAuley, Rahul Pandey, and Jure Leskovec. Inferring networks of substitutable and com-
plementary products. CoRR, 2015.
Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation:
Contextualized word vectors. In Advances in Neural Information Processing Systems, 2017.
Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language
decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. arXiv preprint arXiv:1609.07843, 2016.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word represen-
tations in vector space. arXiv preprint arXiv:1301.3781, 2013a.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed represen-
tations of words and phrases and their compositionality. In Advances in neural information pro-
cessing systems, 2013b.
Jiaqi Mu and Pramod Viswanath. All-but-the-top: Simple and effective postprocessing for word
representations. In Proceedings of the International Conference on Learning Representations,
2018.
Matteo Pagliardini, Prakhar Gupta, and Martin Jaggi. Unsupervised learning of sentence embed-
dings using compositional n-gram features. Proceedings of the North American Chapter of the
ACL: Human Language Technologies, 2018.
Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word
representation. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), 2014.
Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and
Luke Zettlemoyer. Deep contextualized word representations. arXiv preprint arXiv:1802.05365,
2018.
Raul Puri and Bryan Catanzaro. Zero-shot text classification with generative language models. arXiv
prepring arXiv:1912.10165, 2019.
Alec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to generate reviews and discovering
sentiment. arXiv preprint arXiv:1704.01444, 2017.
11
Published as a conference paper at ICLR 2021
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language
understanding by generative pre-training. 2018. URL https://s3-us-west-2.
amazonaws.com/openai-assets/researchcovers/languageunsupervised/
languageunderstandingpaper.pdf.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. OpenAI Blog, 2019.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer. arXiv preprint arXiv:1910.10683, 2019.
Prajit Ramachandran, Peter Liu, and Quoc Le. Unsupervised pretraining for sequence to sequence
learning. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language
Processing, 2017.
Timo Schick and Hinrich Schutze. It's not just size that matters: Small language models are also
few-shot learners. arXiv preprint arXiv:2009.07118, 2020.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 conference on empirical methods in natural language pro-
cessing, 2013.
Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. olmpics-on what language model
pre-training captures. arXiv preprint arXiv:1912.13283, 2019.
Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive learning, multi-view redun-
dancy, and linear models. arXiv preprint arXiv:2008.10150, 2020a.
Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive estimation reveals topic
posterior information to linear models. arXiv preprint arXiv:2003.02234, 2020b.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, 2017.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding. arXiv
preprint arXiv:1804.07461, 2018.
Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through align-
ment and uniformity on the hypersphere. arXiv preprint arXiv:2005.10242, 2020.
Theresa Wilson and Janyce Wiebe. Annotating opinions in the world press. In Proceedings of the
Fourth SIGdial Workshop of Discourse and Dialogue, 2003.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, and Jamie Brew. HUggingface's trans-
formers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.
Wei Xu and Alex Rudnicky. Can artificial neural networks learn language models? In Sixth inter-
national conference on spoken language processing, 2000.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le.
Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in neural
information processing systems, 2019.
Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text clas-
sification. In Advances in Neural Information Processing Systems 28. 2015.
12
Published as a conference paper at ICLR 2021
A	Overview
Section B is a more detailed version of Section 5.1 and Section C is a detailed version of Section 5.2.
Section D.1 has a discussion about why natural tasks are a reasonable formalization for the sentence
completion reformulation and also interpretations for τ and B in the definition of natural tasks.
Section D.2 discusses desirable properties of word embeddings Φ like capturing synonym structure
in words. Section E contains proofs for all results, including proof sketches for the main results in
Section E.1. Lemma E.4 is the softmax variant of Pinsker’s inequality that we prove and use for our
main results.
Section F contains many more experimental findings that consolidate many of our theoretical results.
Section F.1 provides the information about subsets of words used for results in Table 1 and also
additional experiments to test the performance of pretrained language model embeddings f on more
downstream tasks and also verifying that conditional mean embeddings Φpf do well on these tasks.
In Section F.3, we present additional results for Quad objective trained on a larger corpus and tested
on SST. Section F.4 provides additional details on how A, b and c from Assumption 4.1 are learned
and also further verification of the assumption on more datasets. Finally, Section F.5 experimentally
verifies the O(√e) dependence from Theorem 4.2.
B B etter handling of distributional shift
While the bounds above used γ(pT ) to transfer from the distribution pL to pT , we define a more
refined notion of transferability here. While γ(pT ) only depends on pL and pT , the more re-
fined notions depend also on the learned language model, thus potentially exploiting some in-
ductive biases. We first define the notion of error made in the predicted probabilities by any
predictor p∙∣s as ∆{p.∣s}(S) = p∙∣s - p：§. Thus for any softmax language model f We have
∆{pf(s)}(s) = Pf(s) — P∙∣s. For any distribution P ∈ ∆s, we define the covariance* 5 * of a func-
tion g : S → RD as Σp(g) = E g(s)g(s)> . We define 3 coefficients for the results to folloW
S〜P
Definition B.1. For any distribution p ∈ ∆S, we define the following
γ(p; {p∙∣s}) := (IM(δ{p∙∣s})- 1Σ(δ{p∙∣s})ςpl(δ{p∙∣s})- 1∣∣2)T	⑺
Yφ(p; {p∙∣s}) := (∣∣ςpl (φδ{p∙∣s})-2ςp(φδ{p∙∣s})ςPL (φδ{p∙∣s})- 1∣∣2)T	(8)
γ(p; Φpf) := γΦ(p; {pf(s)})	(9)
We notice that ςp(δ{p∙∣s})	=	SEPh(P∙∣s- P*∣s )(p∙∣s- P*∣s)>i,	ςp(φδ{P∙∣s })	=
ΦΣp(∆{p∙ls})Φ>. We are now ready to state the most general results.
Theorem B.1 (Strengthened Theorem 4.1). Let {p.∣s} be a language model that is e-optimal, i.e.
'Xent({p∙∣s}) — '*ent ≤ E for Some e > 0. For a classification task T that is (τ, B)-natural, we have
't (SR ≤ τ + SY(PTB{P∙∣s})
For a classification task T that is (τ, B)-natural w.r.t. Φ, we have
，	、	，	、	/	2B2E
't ({p∙∣s}) ≤'t ({φp∙∣s}) ≤ T + J----------J—
Y yφ(pt; {p∙∣s})
Theorem 5.1 (Strengthened Theorem 4.2). For a fixed Φ, let f be features from an E-optimal d-
dimensional softmax language model, i.e. 'xent(f, Φ) — '*ent(Φ) ≤ G where 'Xent(Φ) is defined in
Equation (4). For a classification task T that is (τ, B)-natural w.r.t. Φ, we have
、、一	、	I^^2B2E
't ({pf(s) }) ≤ 't (φpf ) ≤ τ + Y dpf )
5This is not exactly the covariance since the mean is not subtracted, all results hold even for the usual
covariance.
13
Published as a conference paper at ICLR 2021
Discussions: It is not hard to show that the coefficients satisfy yφ(pt; {p∙∣s}) ≥ γ(pτ； {P∙∣s}) ≥
γ(pT) and γ(pT; Φpf) ≥ γ(pT), thus showing that these results are strictly stronger than the ones
from the previous section. The transferability coefficient is a measure of how guarantees on pL
using a language model can be transferred to another distribution of contexts and it only depends
on the distribution of contexts and not the labels. Unlike γ(pT), the coefficients in Definition B.1
depend on the learned models, either {p.∣s} or {p〃s)}, and can be potentially much smaller due to
the inductive bias of the learned models. For instance, if errors made by the model are random-like,
i.e. ∆{p∙ls}(s)〜ρ, independently of s, then ΣpL (∆gls}) ≈ Σp(∆gls}) ≈ En〜ρ[ηη>], making
Y(p; {p∙∣s}) ≈ 1. Independence prevents language modeling error from accumulating on contexts
from pT, bypassing the worst case transfer of γ(pT).
C Quad: A new objective function
In Definition 3.2 we discuss how low dimensional softmax language models learn a linear projection
of p*s, only solving tasks that lie in the row span of word embeddings Φ. Although Φ defines tasks
that language model features can solve, the standard cross-entropy objective does not lend a simple
closed form expression for optimal Φ. This motivates the construction of our Quad objective, that
has two nice properties: (1) the optimal feature map f * is a linear function of p； S and thus can solve
some natural tasks, and (2) the optimal Φ* has an intuitively meaningful closed-form solution.
'quad,s(θ, Φ)=	E [-θ>Φw] + 1 kΦ>θk2 = -θ>Φp*s + 1 I^”k2
W 〜p*∣s22
'quad(f, φ) = E ['quad,s(f (s), φ)]
(10)
(11)
The Quad objective is very similar to the cross-entropy objective from Equation (2), with the log
partition function replaced by a quadratic function, inspired in part by Assumption 4.1. We can
derive the optimal solution Φ* that depends on the eigen-decomposition of a substitutability matrix.
Definition 5.1. The substitutability matrix is defined to be Ω* := E [p*|§ P*§>] ∈ RV×v∙ If
Ω* = USU > is the eigendeComPosition,then Ud ∈ RV ×d is matrix oftop d eigenvectors of Ω*.
The matrix Ω* captures substitutability between pairs of words. Words W and w0 are substitutable
if they have identical conditional probabilities for every context s ∈ S and thus can replace occur-
rences of each other while still providing meaningful completions. By definition, these words satisfy
Ω*[w] = Ω* [w0]. Such pairs of words were called “free variants” in the work on distributional se-
mantics (Harris, 1954), and capture the notion of synonyms in the distributional hypothesis. We now
derive expressions for the optimal solution of the Quad objective described in Equation (11). The
proof of all results from this section are in Section E.5.
Theorem C.1. The optimal solution f *, Φ* = arg minf,φ 'quad(f, Φ) satisfies
Φ* = BUd>, for full rank B ∈ Rd×d
f *(s) = (Φ*Φ*>)-U2Φ*p*∣s = CU>p*∣s,forfullrank C ∈ Rd×d
If Φ is fixed, then the optimal solution is f *(s) = (ΦΦ>)-1/2 Φp*s.
Theorem 5.2. Let f *, Φ* = arg minf,φ 'quad(f, Φ). Then Φ* = BUd, for full rank B ∈ Rd×d.
Also, for a classification task T that is (τ, B)-natural w.r.t. Φ*, we have't(f *) ≤ T.
Thus f * excels on natural tasks w.r.t. Φ*, which in turn, is the best d-dimensional projection of Ω*.
Thus words w, w0 ∈ W that are synonyms (hence substitutable) will satisfy φ*w = φ*w0, fulfilling the
desired property for word embeddings discussed in Definition 3.2. We train using the Quad objective
and compare its performance to a similarly trained language model, finding Quad to be reasonably
effective. The goal of testing Quad is not to obtain state-of-the-art results, but to demonstrate that
theoretical insights can aid the design of provably effective algorithms.
14
Published as a conference paper at ICLR 2021
D	More on natural tasks
The discussions in this section may not be formal and precise in places, they are meant to provide
more intuition for some of the definitions and results.
D.1	SENTENCE COMPLETION REFORMULATION ≡ NATURAL TASK
We provide informal justification for why the sentence completion reformulation can be formalized
as being able to solve using a linear classifier over p：§ ∈ RV. The analysis will also end UP
providing some intuitions for τ and B in Definition 3.1 and Theorem 4.1. In particular, we will
show that a task that is amenable to the sentence comPletion reformulation will be (τ, B)-natural,
with τ = O(Bayes-Error(T)), i.e. τ is small if the Bayes error for the task error, and B =
O(α(Windicative)-1) is inversely ProPortional to the Probability mass of the set of indicative words
for the task. This is formalized in ProPosition D.2.
Linear classifier over p；§
Consider a binary classification task T and that can be solved with a sentence comPletion refor-
mulation after adding a PromPt as in Section 3.1, for e.g. sentiment classification can be solved
by adding a PromPt “This movie is” at the end of every movie review and use the comPletions to
solve the task. Recall that pT is the distribution over S × {±1} for the task T. We abuse notation
and use pT to denote the distribution over inPuts where a PromPt is added to each to inPut, for e.g.
“I loved the movie." is transformed to “I loved the movie. This movie is”. For any S 〜PT, let
pT(y = 1|s) and pT(y = -1|s) denote the conditional Probabilities of the sentiment of review s
(with an added PromPt) being Positive and negative resPectively. By law of total Probability we can
write this conditional Probability as
PT(y = 1|S)= X Pr(y = 1|(s, w)) Pr(w∣s) = X Pr(y =1∣(s,w)) p*s(w)	(12)
w∈W	w∈W
For any task T we can roughly Partition the vocabulary set W into the following
Indicative words Windicative : w can be an indicative completion for the task, like “good”, “boring”,
“trash” etc, after a movie review like S =“I loved the movie. This movie is”. In this case the
sentence comPletion reformulation can be interPreted as the following: the comPletion w after a
review S is sufficient to determine the sentiment of the review, i.e. we do not need to know the
content of the review S to Predict the label if we know the comPletion w. This can be formalized as
Pr(y = 1|(S, w)) ≈ P(y = 1|w) for some fixed distribution P for indicative comPletions w.
Irrelevant words Wirrelevant : w can be an irrelevant completion for the task, like “a”, “very”, “not”.
In this case the comPletions, on the other hand, do not reveal anything more about the sentiment for
the review than S itself, i.e. Pr(y = 1|(S, w)) ≈ PT(y = 1|S) for irrelevant comPletions w.
Thus from Equation (12) we get
PT(y = 1|S)= X	Pr(y = 1∣(s,w)) P*∣s(w) + X	Pr(y =1∣(s,w)) P*∣s(w)
w∈Windicative	w∈Wirrelevant
≈ X	P (y = ι∣w) p*s(w) + X	PT (y = ι∣s) p*∣s(w)
= w∈Xj(W) P*|S(W)+ PT (y = 1ls)w∈Xljs (W)
=v1 p*s + PT(y = 1l S)P*|s (WirreIeVant)
where v1 ∈ RV is defined as v1(W) = P(y = 1|W) for W ∈ Windicative and v1 (W) = 0 for W ∈
Wirrelevant. Similarly we can define v-1 ∈ RV with v-1(W) = P(y = -1|W) for W ∈ Windicative,
v-1(W) = 0 for W ∈ Wirrelevant. From the earlier calculation, and a similar one for y = -1, we get
PT(y = b1S) ≈ 1- P*∣s(W…)v>p∣s = P*|s(W，e) v>p*∣s' for b ∈ {±1}
15
Published as a conference paper at ICLR 2021
If We assume p：§ (Windicative) ≈ α(Windicative) is roUghly the same for all s, i.e. probability mass of
indicative words following a modified review is approximately the same, then we get
PT (y = 1|s) - PT (y = -1|s) ≈ v>p*∣s ,where VT =。(⑷：.)电-VT)	(13)
Thus We can approximately express the difference in conditional probabilities of the 2 classes as a
linear function of p：§. While it is intuitively clear why knowing PT(y = 1|s) - PT(y = -1|s) is
useful for solving the task, we show precisely why in the next part.
Interpretation for T and B
Based on the above discussed, we will show that the task T from earlier is (τ, B)-natural according
to the Definition 3.1 and will also give us an interpretation for τ and B. First we show that the
following predictor from Equation (13) is effective for task T
gT(S) = PT(y = ι∣s) -PT(y = -i∣s) ≈ v;p*∣s	(N)
We reuse the notation from Equation (3) and define the task loss for any predictor g : S → R as
't(g) = E(s,y)〜PT ['(g(S), y)]	(15)
Furthermore let Bayes-Error(T) := infg=s→R E(s,y)〜PT[1{g(s) = y}] denote the Bayes error of
the task T, i.e. the optimal 0 - 1 error achievable on the task.
Proposition D.1. For any task T and for the hinge loss ','t (gT) ≤ 4 Bayes-Error (T), where
gT (S) = PT(y = 1|S) - PT(y = -1|S).
Thus if a task is easily solvable, i.e. has small Bayes error, then it will be solvable by the predictor
gT (S). Since we argued above that sentence reformulation implies that gT (S) is a linear function of
p；s, we can now show that T is a natural task as formalized in Definition 3.1.
Proposition D.2 (Informal). Task T that can be reformulated as a sentence completion task (de-
scribed above) is a (τ, B)-natural task w.r.t. the hinge loss, with the follow parameters
τ ≤ 4 Bayes-Error(T) and B = α(Windicative)-1
Here Bayes-Error(T) is the Bayes error of task T and α(Windicative) is the total mass of the indica-
tive words for the task.
If the task T can be reformulated as sentence completion, then T is (τ, B)-natural where
•	τ is small if the task is unambiguous, i.e. it has small Bayes error
•	B is small if the probability mass of the set of indicative words Windicative is large, i.e. the
task depends on a large set of frequent words
Thus the upper bound in Theorem 4.1 is smaller if the task can be reformulated as sentence comple-
tion task with a large and frequent set of completions, and we can ever hope to solve it well (Bayes
error is small). The proofs for the above propositions are in Section D.1.
D.2 NICE PROPERTIES OF WORD EMBEDDINGS Φ
We argue here that if the word embeddings Φ satisfy certain nice properties, then (τ, B)-natural
tasks of interest will be (τ0, B0)-natural w.r.t. Φ, where we will provide informal quantifications
for the nice properties and tasks of interest that lead to a small value for τ0 and B0 . The nice
property will be related to Φ capturing the semantic meaning (synonym structure) of words and
tasks of interest will be those that try to distinguish word completion (in the sentence completion
reformulation) with very different meanings, i.e. tries to distinguish more coarse-grained semantic
notions rather than very fine-grained ones. Note that the results here are informal and qualitative,
rather than quantitative.
Consider a task T that is (τ, B)-natural task and let v* ∈ RV be the classifier such that
't({p*s}, v*) ≤ τ and ∣∣v*∣∣∞ ≤ B. We want to find properties of Φ and v* that will make
T to be (τ0, B0)-natural w.r.t. Φ such that τ0 and B0 are not too large.6
6 Note that the converse is trivially true, i.e. a (τ, B)-natural task w.r.t. Φ is also (τ, B)-natural.
16
Published as a conference paper at ICLR 2021
We will show that T is (τ0, B0)-natural w.r.t. Φ by finding a classifier v such that v = Φ>λ ∈ R1 * * * V ,
∣∣v∣∣∞ ≤ B0 and't({p*^}, V) ≤ T0. First We define Pφ := Φ，Φ ∈ RV×v to be the projection
matrix for the row-span of Φ and PΦ⊥ := IV - PΦ to be orthogonal projection matrix. We will show
that the classifier V = Pφv* suffices for our case, under some intuitive conditions on v* and Φ.
To compute B0, we first look at the '∞ norm of V = Pφv*
B0= kvk∞ = kPΦv*k∞ = kv*-PΦ⊥v*k∞ ≤ kv*k∞ + kPΦ⊥v*k∞ ≤ B + kPΦ⊥v*k2
To find the upper bound τ0, we upper bound the classification loss ofv = PΦv*. We first define the
substitutability matrix Ωp = E [p*∣sP*∣s>], similar to the one in Definition 5.1. Then
'T ({P*∣s}, v)=	E	['(v>p*s,y)] = E	['((PφV*)>p*s,y)]
(s,y)〜PT L	」	(s,y)〜PT L	」
≤⑺	E	['(v*>p*∣s,y)] + E [|(v* - Pφv*)>p*s∣]
(s,y)〜PT L	」	S〜PT
='T ({p*s}, v*)+ JEJ∣v*>P⊥p*s∣i
≤⑹ τ + ʌʌ舄[(v*>P⊥P*ls)2i = T + ʌʌ舄[v*>P⊥P*lsP*s>P⊥v*i
(C) T + Jv*>P⊥ΩPtP⊥v* ≤⑷ T + kP⊥v*k2 J∣∣P⊥ΩPtP⊥∣∣2
where (a) follows from 1-Lipschitz property of ', (b) from Jensen’s inequality and that
't({p*s}, v*) ≤ τ, (C) from the definition of substitutability matrix Cp『and (d) by definition
of spectral norm of a symmetric PSD matrix.
Thus we have shown that T is (T0, B0)-natural w.r.t. Φ, where
T0 = T + kP⊥v*k2y∣∣P⊥ΩpTP⊥∣∣2, B0 = B + kP⊥v*k2	(16)
We will now show that if Φ captures the notion of synonyms, then ∣∣P⊥ΩpTP⊥∣∣2 will be small
leading to T0 being small. Furthermore we also shed some light on what it means for kPΦ⊥v* k2 to
be small, which will in turn make B0 small and T0 smaller. We do so with the following arguments,
1) ΩpT captures semantic meaning of words and thus its top eigen-directions will capture more
dominant semantic concepts, 2) if Φ captures the “top-d” directions of meaning, i.e. the top-d
eigen-directions of Ωpτ, then ∣∣P⊥Ωp丁Pφ ∣∣? = O(1∕d), 3) if additionally v* cares about the “top-
d" directions of meaning, i.e. top-d eigen-directions of Ωp丁 then ∣∣P⊥v*k2 will be small. We
expand on these points below
1. Substitutability matrix (Cp『)captures semantic meaning: We use a similar argument to
the one in Section 5.2 right after Definition 5.1 that is based on distributional semantics (Harris,
1954). Harris (1954) posits that meaning for elements (words) can be derived from the environments
(contexts) in which they occur. Thus Harris (1954) argues that words that occur in almost identical
set of contexts have the same meaning, i.e. are synonyms. On the other hand, if two words share
some contexts but not all, then they have different meanings and the amount of difference in meaning
roughly corresponds to amount of difference in contexts. In our setting, the similarity of words w and
w0 can then be determined by the probabilities assigned to them by different contexts s. In particular,
if p*s(w) = p*s(w0) for all or most s ∈ supp(pT), then w and w0 have essentially the same meaning
w.r.t. the distribution of contexts PT and the closer [p*∣s(w)]s∈supp(pT) and [p*∣s(w0)]s∈supp(pT) are,
the closer the meaning of W and w0 are. For the substitutability matrix Ωp = E [p*, p*, >] ∈
S〜PT
RV×V,itis not hard to show that Ω∖t(W) = Ω}t(w0) is equivalent top*∣s(w) = P*∣s(w0) ∀s 〜PT,
where Ω}t (w) is the row of Ω}t corresponding to word w. To show this, we can define βw ∈
R|supP(PT)| to be an embedding of W that looks like βw = [p*∣s(w) ,Pt(s)]s∈supp(pt). It is easy to
see that β>ιβw? = E [p*∣s(w1)p*∣s(w2)] = ΩPt(w1,w2). Thus βw = βw0 =⇒ CP『(w)=
17
Published as a conference paper at ICLR 2021
Ωpτ(w0) is straightforward to see. For the converse,
ωPT(W) = CpT(WO) =⇒ ωPT(W, W) = CpT(Wlw) = ωPT(w,w') = ωPT(WlwO)	(17)
=⇒	βw>βw	=	βw>βw0	=βw>0βw0	=⇒	βw	=βw0	(18)
Thus ΩpT indeed does capture the synonyms structure between words, and the top eigen-directions
of it capture the most significant “semantic meaning” directions.
2.	Φ has nice properties: if Φ roughly respects this synonym structure by aligning with the top-d
eigen-directions of ΩpTτ, We have
1 d+1
∣∣P⊥ΩpTP⊥∣∣2 ≤ λd+ι(ΩpT) ≤ d+1 ]Γλi(ΩpT) ≤
i=1
≤ d + 1 SJpTtr(p*sp*s>) ≤ d + 1
d+1 tr(°pT)
(19)
(20)
From Equation (16), We then have T' ≤ T + kP⊥d k2
3.	Tasks of interest: It is more likely for a classifier v* to separate words with big differences
in meaning rather than small differences. For e.g., it is more likely for a task to separate word
completions “good” and “bad” rather than “good” and “nice”. Since top eigen-directions of ΩpT
capture more dominant semantic meanings, this could correspond to v* aligning with the top eigen-
directions of ΩpT. In combination with the above property about Φ, this could suggest that ∣∣P⊥v* k2
is small, thus leading to T' and B' being small.
Note that they above arguments are informal and qualitative, and we leave exploring desirable prop-
erties of Φ more formally to future work.
D.3 Proofs for Section D.1
Proposition D.1. Let pb(s) = pT (y = b|s) for b ∈ {±1}, pmin(s) = minb∈{±1} pb(s), pmax(s) =
maxb∈{±1} pb(s) and g* (s) = arg maxb∈{±1} pb(s) denote the Bayes optimal predictor. We first
notice that there is a simple well-known closed form expression for the Bayes risk
Bayes-Error(T) = E	[1 {g* (s) 6= y}]
(s,y)JpT
= E 1 arg maxpb(s) 6= y = E [pmin (s)]
(s,y)JpT	b∈{±1}	sJpT
□
We now analyze the hinge loss of the predictor gpT defined in Equation (14). Note that since
gpT(s) ≤ 1, the hinge loss '(g”(s),y) = (1 - yg”(s))+ = 1 - yg”(s) for every s, y. Thus the
total loss is
gpT (s) =	E [(1 - ygpT (s))+] = E [(1 - ygpT (s))]
(s,y)JpT	(s,y)JpT
=(a)	E [p1(s) (1 - gpT (s)) + p-1(s) (1 + gpT (s))] = E [1 - (p1(s) - p-1(s))gpT (s)]
sJpT	sJpT
=(b) E 1 - (p1(s) - p-1(s))2 = E	(p1 (s) +p-1 (s))2 - (p1(s) - p-1(s))2
sJpT	sJpT
= E [4p1(s)p-1(s)] = 4 E [pmin(s)pmax(s)]
sJpT	sJpT
≤(c) 4 E [pmin(s)] = 4 Bayes-Error(T)
sJpT
where (a) follows by splitting the expectation over y|s, (b) follows from the definition of gpT (s) in
Equation (14) and (c) follows from pmax(s) ≤ 1. This completes the proof.
18
Published as a conference paper at ICLR 2021
Proposition D.2. Let B = α(Windicative)-1. We first note the following using the definition of v
from Equation (13).
kvT k∞ = α(Windicative)-1 max |v1 (w) - v-1(w)| = B max |P(y = 1|w) - P(y = -1|w)| ≤ B
w∈W	w∈W
(21)
To find the value of τ that makes the task (τ, B)-natural (Definition 3.1), we observe the following
minll∕ 't({p*s}, V)=(a) 't({p*s}, VT) = E	['(v>p∙∣s,y)]
v∈RV ,IWk≤B	(s,y)〜PT
=(b)	E	['(gτ(S), y)] = 't (gτ)
(s,y) 〜PT
≤(c) 4 Bayes-Error(T)
where (a) follows from the calculation in Equation (21), (b) follows from Equation (13) and (c)
follows from Proposition D.1.	□
E Proofs
E.1 Proof sketch
We first present a sketch of the arguments that help us show our main results, theorems 4.1 and 4.2.
The subsections after the next one contain the full proofs for strengthened versions of these results.
E.1.1 Proof sketch for arbitrary language models: Theorem 4.1
Here We want to show guarantees for features {p.∣s} on a (τ, B)-natural task T. From the definition
of natural tasks, we know
∃v* ∈ Rv, kv*k∞ ≤ B s.t. 't({pt∣s}, v*) ≤ T	(22)
We wish to upper bound the classification error't({p.∣s}) and do so using the following sequence
of inequalities.
't({p∙∣s}) - T = JnfV't({p∙∣s}, V)- T ≤ 't({p∙∣s}, V*) -'t({p*∣s}, v*)
't({p∙∣s}, V*) - 'T({p*s}, V* )
r E [(v*>(p∙∣s - p*s))2]
V S〜PT	1
't (M}, V*)- 'T ({p*∣s}, V* )
∕v*>ςPT (δ{p.∣s})v*
'∙^^^^^^^^^^^^^^^^{^^^^^^^^^^^^^^^^^
α1(V*)
Classification loss → error covariance on PT
Use Lipschitzness of ` and Jensen’s inequality
E [(v*>(P∙∣s -P*∣s))2],----------------------------
二 \*>--------------Y E [(v*>(P∙∣s- P*∣s))2 ]
E [(v* 1 (P∙∣s - P*s))2]	N s〜PL	∣s
S 〜PL	1
v*>ςPT 0{p∙∣s})v*
v*>ςPL O{p∙∣s})v*
一 ■ __ 一	/
'∙^^^^^^^^^^^{^^^^^^^^^^^^1
α2(v*)
Error covariance from PT → PL
Use transferability coefficient
∙∖ / E [(v*>(p∙∣s- p*s))2] (23)
V S 〜PL	1
I-------------------------}
α3(V*)
Error covariance → cross-entropy loss
Use (modified) Pinsker’s inequality
∖
I
}
J
where ΣP (g) := E [g(s)g(s)>] is the uncentered covariance of g w.r.t. distribution p ∈
S〜P
∆s, as defined in Section 5.1. We upper bound't({p.∣s}) - T by upper bounding each of
α1(V*), α2(V*), α3(V*) as follows
• Classification loss → prediction error covariance: α1(V*) is upper bounded by using
Lipschitzness of the loss ' used in the definition of 'T, e.g. hinge loss or logistic loss, and
then followed by an application of Jensen’s inequality
Lemma E.8 =⇒ α1(V) ≤ 1 for all V ∈ RV
• Error covariance from pT → pL: α2(V*) handles the mismatch in distributions pT and
pL over which the classification loss and cross-entropy losses are measured respectively. It
is upper bounded by the transferability coefficient
19
Published as a conference paper at ICLR 2021
Lemma E.10 and Lemma E.9 =⇒ a2(v) ≤ ,γ(PT)-1 for all V ∈ RV
•	Error covariance → cross-entropy loss (arbitrary language models): This is arguably
the most important step that connects the error in prediction to the cross-entropy loss. For
the arbitrary language model case, this is proved using Pinsker’s inequality and taking
expectation over the distribution pL .
LemmaE.3 =⇒ a3(v) ≤ q2∣∣vk∞('χent({p.∣s}) - 'xent(p*∣S)) for all V ∈ RV
E.1.2 Proof sketch for softmax language models: Theorem 4.2
Here we want to show guarantees for features Φpf = {Φpf(s)} on a (τ, B)-natural task T w.r.t Φ.
From the definition of natural tasks w.r.t. Φ, we know
∃v* = Φ>λ ∈ Rv, kv*k∞ ≤ B s.t. 't({p*∣s}, v*) ≤ T	(24)
Note that the difference here is that v* is in the span of Φ rather than an arbitrary vector in RV. We
wish to upper bound the classification error't({Φpf(S)}) and do so using the following sequence
of inequalities.
't({ΦPf(s)}) - τ = ʌinfd't({Φpf(s)},λ) - τ
= inf v'tHPf(S)}, V) - T
v=Φ>λ∈RV
≤'T ({Pf(s)}, V*)-'T ({p*s}, v*)
≤ αι(v*) ∙ ɑ2(v*) ∙ ɑ3(v*)	(25)
where the first inequality follows because V* is in the span of Φ and second inequality follows from
Equation (23). The bounds for α1(V*) and α2(V*) are the same as arbitrary language models. The
main difference is the bound on α3(V*) which will be a stronger bound for softmax models.
•	Error covariance → cross-entropy loss (softmax language models): For softmax lan-
guage models, we need to prove a modified version of Pinsker’s inequality specifically for
softmax models. This version will show a bound that only works when V * is in the span of
Φ and if the evaluated model Pf(S) computes softmax using Φ as well.
LemmaE.4 =⇒ α3(v) ≤ ,2kvk∞('xent({Pf(s)}) - inf({pf*(s)})) ∀v = Φ>λ ∈ RV
Thus we suffer the suboptimality of the language model {Pf(S)} w.r.t. the best softmax model
{Pf*(s)} rather than the absolute best language model {p*§}. This is done using the softmax variant
of Pinsker’s inequality in Lemma E.4. We now present the detailed proofs for all results.
E.2 Proofs for arbitrary language models
Theorem B.1 (Strengthened Theorem 4.1). Let {p.∣s} be a language model that is e-optimal, i.e.
'χent({P∙∣s}) — '*ent ≤ e for Some e > 0. Fora classification task T that is (τ, B)-natural, we have
…	…	I^^2B2e-
't ({p∙∣s}) ≤ T + ∖	——7~τ?
y(ptXp∙∣s})
For a classification task T that is (T, B)-natural w.r.t. Φ, we have
，	、	，	、	/	2B2e
't ({p∙∣s}) ≤'t ({φp∙∣s}) ≤ T + J--——ʃ——
YΦ(pτ; {p∙∣s})
Proof. The proof has two main steps that we summarize by the following two lemmas. The first one
upper bounds the downstream performance on natural tasks with the covariance of errors.
20
Published as a conference paper at ICLR 2021
Lemma E.2. For a language model {p.∣s}, if T is (τ, B)-natural,
v vv	v> .	.	IVTςPL (A{p.|s})v
't({P∙∣s}) ≤ T +	SUp	∖ —7——T-
v∈RV,kvk∞≤B V	Y(PT,{P∙∣sD
IfT is (T, B)-natural w.r.t. Φ ∈ Rd×V,
v v ʃ ɪ ι∖“	IVT ςPL (δ{P∙∣s})v
't({Φp∙∣s}) ≤ T + SUp	ʌ/ ——-.——7~
v=Φ>λ∈RV, V Yφ(Pτ； {P∙∣s})
kvk∞≤B
where γ(∙) and Yφ(∙) are from Definition B.1.
The second lemma upper bounds the covariance of error with the suboptimality of the language
model.
Lemma E.6. For a language model {p.∣s} and classifier V ∈ RV,
vt∑pl (∆{p∙∣s})V ≤ 2kVk∞ ('Xem ({py}) — 'Xen)
where ΣpL (∆{p,∣s}) = E 卜，.|§ — PXs)(P∙∣s — P：s)T] as defined in Section B.
We prove both the above lemmas in Section E.6. We first use these to prove the main result.
Combining the two lemmas, we get the following inequality
't({p∙∣s}) ≤(a) T + SUp
v∈RV ,kvk∞≤B
≤(b) τ + sup
v∈RV ,kvk∞≤B
VT夕PL (∆{P•|s})V
γ(pτ; {p∙∣s})
2kvk∞ ('xent({p∙∣s})-'Xent)
s
γ(pτ; {p∙∣s})
≤(c)τ+
2B2
γ(pτ; {p∙∣s})
where (a) uses first part OfLemmaE.2, (b) uses LemmaE.6 and (c) uses the E-OPtimality of {p.∣s}.
This proves the first part of the result. The second part can also be proved similarly.
((ʃ ɪ I、j(a) I	IVT ςPL (A{p.|s})V
'T({φP∙∣s}) ≤( ) T +	SUp	∖ ——7——ʃ	n
v=Φ>λ∈RV, V Yφ(pT,{p∙∣s})
kvk∞≤B
≤(b) T +	SUp
v=Φ> λ∈RV,
kvk∞≤B
≤ T + SUp
v∈RV,kvk∞≤B
2kVk∞ ('xent({p∙∣s}) — 'Xent)
YΦ(pτ; {p∙∣s})
2∣∣V∣∣∞ ('xent({p∙∣s}) — 'Xent)	+ [
γΦ(pτ; {p∙∣s})	- T y
2B2
yφ (pτ; {p∙∣s})
where (a) uses second part of Lemma E.2, (b) uses Lemma E.6 and (c) uses the -optimality of
{p∙∣s}. The proof of the lemmas can be found in Section E.6.
□
Theorem 4.1. Let {p.∣s} be a language model that is E-optimal, i.e. 'xent({p∙∣s}) — 'Xent ≤ G for
some > 0. For a classification task T that is (τ, B)-natural, we have
't (MD ≤τ + VY2Be)
21
Published as a conference paper at ICLR 2021
Proof. This follows from the first part of Theorem B.1 if we can also show that Y(PT; {p∙∣s})-1 ≤
γ(pT)-1. For that we use the following lemma that we prove in Section E.6.
LemmaE.9. For any g : S → RD andPT ∈ ∆s, we have k∑pL(g)-2Στ)τ(g)∑pL(g)-11∣2 ≤
γ(pT)-1
Instantiating this for g = ∆{p,∣s} and using Equation (7), we get Y(PT; {p∙∣s})-1 ≤ Y(PT)-1,
which completes the proof.	□
E.3 Proofs for softmax language models
Theorem 5.1 (Strengthened Theorem 4.2). For a fixed Φ, let f be features from an -optimal d-
dimensional Sofmax language model, i.e. 'Xent(f, Φ) — ':en(Φ) ≤ G where 'Xent(Φ) is defined in
Equation (4). For a classification task T that is (τ, B)-natural w.r.t. Φ, we have
2 2,	、、一 、	I^^2B2^
't ({Pf(s)}) ≤'t(φPf) ≤ T + ʌ/ʒ——ɪʒ
Y(PT; ΦPf)
Proof. Instantiating Lemma E.2 for p∙∣s = Pf (s), we get
't({ΦPf(s)}) ≤ T + SUp
v=Φ> λ∈RV
kvk∞≤B
IvT ςPL (A{Pf(s)}yv
V	YΦ(pτ; {Pf(s)})
uu	sUp	λ>ΦΣpL (∆{pf(s)})Φ>λ
(a) T + , kφ>λk∞≤B_____________________
∖	Y (PT； φPf)
,,	sUp	λ>ΣpL (Φ∆{pf(s)})λ
T +	kΦ>λk∞≤B____________________
∖	Y (PT； φPf)
where (a) follows from Equation (9) that says Y(PT; ΦPf) = YΦ (PT; {pf(s)}). We now prove a
similar result for the second term in the following lemma that we prove in Section E.6.
Lemma E.7. For a fixed Φ and a softmax language model with features f and λ ∈ Rd,
λ>∑PL (Φ∆{pf(s)})λ ≤ 2kΦ>λk∞ ('Xent(f, Φ) — 'Xent(Φ))
where ςPL Q{pf (S) }) = s 既
[(φPf(s) - φPx∣s)(φPf(s) - φP*s)>] as defined in Section B.
Using Lemma E.7 directly gives us't (Φp∕ ) = 't ({Φpf (s)}) ≤ T + JB"'：：((PT)-Pfe)nt(φ)) ,and the
E-OPtimaIity almost completes the proof. The only thing remaining to show is that't({Pf(s)}) ≤
'T(ΦPf) which follows from the following sequence.
'T({pf(s)}) =	inf	'T({pf(s)},v) ≤ inf	'T({pf(s)}, (Φ>λ, b))
v∈RV,b∈R	Φ>λ∈RV,b∈R
= inf	'T({Φpf(s)}, (λ, b)) = 'T(ΦPf)
λ∈Rd,b∈R
□
Theorem 4.2. For a fiXed Φ, let f be features from an E-optimal d-dimensional softmaX language
model, i.e. 'Xent(f, Φ) — 'XXent(Φ) ≤ E, where 'XXent(Φ) is defined in Equation (4). For a classification
task T that is (T, B)-natural w.r.t. Φ, we have
't ({Pf(s)}) ≤'t(φPf) ≤ T +
22
Published as a conference paper at ICLR 2021
Proof. This result follows directly from Theorem 5.1, if we can also show that γ(pT ; Φpf)-1 ≤
γ(pT )-1 just like in the proof of Theorem 4.1. For that we again use Lemma E.9 with g =
Φ∆{pf(s)} and Equation (9) and this completes the proof.	□
E.4 Proofs for Section 4.3
We first show why Assumption 4.1 is approximately true when word embeddings are gaussian like.
Lemma E.1. Suppose word embeddings φw are independent SamPksfrom the distribution N(μ, Σ).
Thenfor any θ ∈ Rd such that λ2 = θ>Σθ = O⑴ we have that | log(Zθ) 一 1 θ>∑θ 一 θ>μ 一
log(V )| ≤ E with probability 1 — δ for E = O ) and δ = 1 — exp(-Ω(log2(V))).
Proof. We first note that log(Zθ) = log (Pw eθ>φw) = θ>μ + log (Pw eθ>(φw-μ)j, thus We
can simply deal with the case where φw are sampled from N(0, Σ). Furthermore the only random
variable of interest is Xw = θ>φw Which is a gaussian variable N (0, θ> Σθ) = N(0, λ2). Thus the
problem reduces to showing that for V samples of Xw 〜N(0, λ2), log(Z) is concentrated around
λ2 + log(V) Where Z = Pw exp(Xw). This can be proved similarly to the proof of Lemma 2.1
in Arora et al. (2016). It is easy to see that E	[exp(Xw)] = eλ2 . However the variable
Xw 〜N (0,λ2)
exp(Xw) is neither sub-gaussian nor sub-exponential and thus standard inequalities cannot be used
directly. We use the same technique as Arora et al. (2016) to first observe that E[Z] = Ve1 λ2 and
Var[Z] ≤ E[exp(2Xw)] = Ve2λ2. After conditioning on the event that Xw ≤ 1 λlog(V) and
applying Berstein,s inequality just like in Arora et al. (2016) completes the proof.	□
We next prove Lemma 4.3 that establishes a linear relationship between Φpf and f (under Assump-
tion 4.1) and also the guarantees for f on natural tasks.
Lemma 4.3. Under Assumption 4.1, any feature map f : S → Rd satisfies Φpf (s) = Af (s) + b,
for all s ∈ S.
Proof. Assumption 4.1 gives us that log(Zθ) = 11 θ>Aθ + θ>b + c. We prove this lemma by
matching the gradients oflog(Zθ) and the quadratic function on the R.H.S.
v7 1 /7、	NeZθ	Pw∈w eφwθφw	ɪ
Ne Iog(Zθ) =	=-----------------= V Pθ(W)φw = φPθ
Zθ	Zθ	w∈W
Whereas the gradient of the quadratic part is Ne [ 1 θ>Aθ + θ>b + c] = Aθ + b. Matching the two
for θ = f(s) gives us Φpf (s) = Φpf(s) = Af (S) + b.	□
Corollary 4.1. Using Lemma 4.3, for any E-optimal f, as defined in Theorem 4.2, for classification
tasks that are (τ, B)-natural w.r.t. Φ we have't(f) ≤ T + O(√E).
Proof. The main idea is that Lemma 4.3 gives us that Φpf (s) = Af (s) + b and thus any linear
function of Φpf will also be a linear function off(s). From Theorem 5.1 (or Theorem 4.2), we also
know that Φpf will do well on T, i.e.'t(Φpf) ≤ T + O(B√E). We formalize7 the intuition as
't (Φpf )= inf 't (Φpf, (λ,b))= inf 't (Af + b,(λ,b))= inf 't (f,(A>λ,b + λ>b))
λ∈Rd,b	λ∈Rd,b	λ∈Rd,b
≥ inf 'T(f,(v,b0))='T(f)
v∈Rd,b0
This shows that't(f) ≤'t(Φpf) ≤ T + O(B√E) and completes the proof.	□
7 Note that here we assume that we learn both a linear classifier and an intercept for a downstream classifica-
tion task. All results in the paper essentially remain the same with an intercept in the definition of classification
loss.
23
Published as a conference paper at ICLR 2021
E.5 PROOFS FOR SECTION C
Theorem C.1. The optimal solution f *, Φ* = argmi∏∕,φ 'quad(f, Φ) satisfies
Φ* = BUd[,forfull rank B ∈ Rd×d
f *(s) = (Φ*Φ*τ)-1∕2Φ*p*s = CUjp^s,forfullrank C ∈ Rd×d
If Φ is fixed, then the optimal solution is f *(s) = (ΦΦτ)-1/2 Φp*s ∙
Proof. From Equations (10) and (11) We know that, 'quad,s(θ, Φ) = -θτΦp*s + 2 ∣∣Φτθk2 and
'quad (f, Φ) = E ['quad,s(f (s), Φ)]∙ FOrafixed Φ, we define fφ (S) = argmin^^d 'quad,SW Φ).
S〜PL
We use the first-order optimality condition to get fφ(s), by using the fact that ▽6'quad,s(θ, Φ)=
-Φp* s + ΦΦτθ. Setting the gradient to zero, we get fφ(S) = (ΦΦτ)-1Φp* s8. To get
the optimal Φ* for this objective, we plug in this expression for fφ in 'quad and find Φ* =
argminφ 'quad(fΦ, Φ).
'quad(持,Φ) = E ['quad,s(fφ (s), Φ)] = E
S〜P*	S〜p*
-fΦ (S)TΦp*∣ S + 2 ∣ΦτfΦ (s)∣2
E
s^p*
一((ΦΦτ)-1Φp*∣ S)TΦp*∣ S + 2 ∣Φτ(ΦΦτ)-1Φp*∣ s∣2
SE* -P*∣ sτΦτ(ΦΦτ)-1Φp*j S + 1 p*| sτφτ(ΦΦτ)-1ΦΦτ(ΦΦτ)-1Φp*∣ S
sE* -2p*∣ sτΦτ(ΦΦτ)-1Φp*∣ S = -1 SEJtr (p*∣ sτΦτ(φφτ)-1φP*∣ s)]
-1 tr (φT(φφT)Tφs与[p∣ spSTi)
1∕φτ(ΦΦτ)-1Φ, E [p"*∣STi) = -1 (Φτ(ΦΦτ)-1Φ, Ω*>
2 ∖	S〜p* Ll 1 J 2	2
where Ω* is the substitutability matrix defined in Definition 5.1. Let Φ = NTVT be the SVD. Then
the above objective reduces to 'quad(fΦ, Φ) = -2( VVτ, Ω*) And hence learning the optimal Φ*
reduces to learning an optimal V * such that
V * = arg min -〈VV τ,Ω*i
V ∈Rv ×d,V T V=Id
We will now show that the best such matrix is the matrix of top d eigenvectors of Ω*, i.e. V * = Ud
(cf. Definition 5.1). Here we will assume that the eigenvalues of Ω* are all distinct for simplicity of
presentation. First we note that〈VVτ,Ω*i = ∣∣ VVτΩ*1 IlF, where Ω*2 = US2Uτ, with U,
Ud and S define in Definition 5.1. This can be shown by the following sequence of steps
〈VVτ, Ω*i = tr(VVτΩ*) = tr(VVτVVTΩ*) = tr(VVτΩ*VVτ)
=tr(VV τUSU τVV τ) = tr(VV τUS 2 U τUS 2 U τVV τ)
=tr(VVτΩ*1Ω*2 VVτ) = 〈VVTΩ*1, VVτΩ*2〉
=IlVV τΩ*1 IlF
Furthermore, we notice that ∣ VVτΩ*2 ∣F = ∣∣Ω* 1∣F - ∣Ω*2 - VVτΩ*1 IlF as shown below
∣∣Ω*1 - VVτΩ*1 ∣∣F = ∣∣Ω*1IIF + IIVVτΩ*1 IlF - 2tr(Ω*2 VVτΩ*1)
8It will be clear later that the optimal solution will have as high a rank as possible Φ. All inverses can be
replaced by pseudo-inverses for low-rank matrices.
24
Published as a conference paper at ICLR 2021
∣∣Ω*2 ∣∣F + ∣∣VVτΩ*2IlF - 2tr(Ω*1VVτVV>Ω*1)
∣∣Ω*11lF + IlVVτΩ*1 kF - 2∣VVτΩ*2 kF
∣Ω*1 kF - ∣VVτΩ*2kF
Thus we get arg min	-hVVτ,Ω*> =	arg min	∣∣Ω* 1 — VVτΩ* 1 kF.
V ∈Rv ×d,V T V=Id	V ∈Rv ×d,V >V=Id
Note that VVτΩ*2 has columns that are columns of Ω*1 projected on the space spanned by
columns V. Itis folklore that the best such subspace V * is the subspace spanned by the top d eigen-
vectors of Ω*2, which is the same as top d eigenvectors of Ω*, thus giving us V*V*τ = UdU；.
Thus We get V * = UdM for M = Ud V *.
This tells us that the optimal solution Φ* will have SVD of the form Φ* = N*T* V*τ, thus
giving us Φ* = BU； for matrix B = N * T *MT ∈ Rd×d. This directly gives f * = fφ* =
(φ*φ*τ)-iφ*p*ls = N*TTV*τp*s = CUTP*∖s for C = N*T*-1Mτ.
□
E.6 PROOFS FOR SUPPORTING LEMMAS
Lemma E.2. For a language model {p.∖ §}, if T is (τ, B)-natural,
I	SvT∑Pl (δ{p∣s} )v
QT({p∙ ∖ s}) ≤ T + sup ʌ/-----------7-----p----
v∈Rv,∣∣v∣∣∞≤B V	Y(PTNp∙∖ s})
If T is (t, B)-natural w.r.t. Φ ∈ RdXV,
v ”而 n , ɪ	∣Pτ∑PL O{P.∣s})v
qT({φp∙ ∖ s}) ≤ T +	suP	∖ -----(~~.ʃe ∖
v=φτλ∈Rv, V γφ(pT,{p∙ ∖ s})
∣∣vk∞≤B
where γ(∙) and Yφ(∙) are from Definition B.1.
Proof. We note the following upper bounds on '丁({p.∣ s}) and't({Φp.∖ s}).
't({p.∖ s}) =Jnfv {'t({p∙∖ s}, v)} ≤ inV {'T({p∙∖ s}, v)}
(26)
∣∣v∣∞≤B
qT({φp∙∖ s}) =	inf rov {qT({p∙ ∖ s}, v)} ≤	TinfV	{qT({p∙ ∖ s}, v)}	(27)
v=Φτλ∈RV	v=Φτλ∈Rv ,b∈R,
∣∣v∣∞≤B
When T is (t, B)-natural, by Definition 3.1 we know that inf	QT({p*∖ s}, V) ≤ T. We now
v∈rv L 「	」
∣∣v∣∞≤B
upper bound QT({p-∖ s}, V) using Lemma E.8. Taking infimum w.r.t. V ∈ RV, ∣Wk∞ ≤ B from the
inequality in Lemma E.8.
't({p.∖ s},V) ≤'t({p*∖ s},V) + 'vτ∑pτ(△{「7})V
举'T ({p∙ ∖s},V) ≤
IWk∞≤B
inf 't({p*∖ s}, v) +
cmV	1
v∈R
suP	√vτ∑pt (δ{p.∣s})V
v∈Rv,∣W∣∣∞≤B
IWk∞≤B
This, combined with Equation (26), gives us
't({p∙∖ s}) ≤ T +	sup
v∈Rv,∣∣v∣∣∞≤B
JVT ∑PT (δ{r∣s"v
(28)
25
Published as a conference paper at ICLR 2021
Using Lemma E.10 and the definition of γ(pT； {p.∣s}) in Equation (7), We get that
v>ςPT (∆{p.|s})v ≤ I∣ςPL O{p∙∣s" 2 ςPT (Ap.|s}RPL O{p∙∣s" 2 I (v>ςPL (A{p.|s}
=.>'PL (Ap.|s})V
Y(PT; {p∙∣s})
(29)
We have thus successfully transferred the bound from the distribution pT to pL . Combining this
with Equation (28) completes the proof of the first part of the lemma.
We now prove the second part of the lemma where we only assume that T is (τ, B)-natural w.r.t. Φ.
Here we instead take the infimum over classifiers in the span of Φ in Lemma E.8 to get
TinfV	{'T ({p∙∣s}, V)} ≤ TinfV	{'T ({p*∣s}, V)O +
v=Φ>λ∈RV,b∈R,	v=Φ>λ∈RV,b∈R,
kvk∞ ≤B	kvk∞≤B
sup	v>Σ
pT Q{p.|s})V
v=Φ>λ∈RV,
kvk∞≤B
This, combined with definition of (τ, B)-natural task w.r.t. Φ and Equation (27) gives us
't({φP∙∣s}) ≤ T +	SUp	,v>ςPT Ap.|s})v
v=Φ>λ∈RV,
kvk∞≤B
For the last term, for any v = Φ>λ, λ ∈ Rd we notice that
V>∑PT (∆{P∙∣s})V = λ>ΦΣpτ (∆{p∙∣s})Φ>λ = λ>Σpτ (Φ∆{p∙∣s} )λ
≤叫∑PL (Φ∆{p∙∣s})- 1 ∑PT (Φ∆{p∙∣s})∑pL (Φ∆{p∙∣s})- 1∣l2(λ>∑PL Qg
=∙>ςPL aAgisD" = v>ςPL 0{p.|s})v
yφ (pt ; {p∙∣s})	yφ(pt; {p∙∣s})
This combined with Equation (31), we get
s
(30)
(31)
})λ
v>ςpl (A{p.|s})v
'T({φp∙ls}) ≤ τ + v=Φinλf∈RV, V	YΦ(PT； {P.sY)
kvk∞≤B
□
Lemma E.3 (Pinsker's inequality). For discrete distributions q,q* ∈ ∆v, let q, q* ∈ RV be the
corresponding vector of probabilities. Then we have
m maχ ∣v>(q - q*)| ≤ ,2DκL(q*,q)
kvk∞≤1
Proof. This basically follows from Pinsker’s inequality which upper bounds the total variation dis-
tance between distributions by their KL-divergence
ll max ∣v>(q - q*)| = ∣∣q - q*kι = 2 TV(q*, q) ≤ ,2DκL(q*,q)
kvk∞≤1
□
We remind the reader that for an embedding matrix Φ ∈ Rd×V, pθ,Φ := softmax(Φ>θ)
Lemma E.4 (Softmax variant of Pinsker’s inequality). Consider a matrix Φ ∈ Rd×V with d ≤ V .
For any discrete distribution q* ∈ ∆v and softmax distribution pθ,φ = softmax(Φ>θ) ∈ ∆v for
θ ∈ Rd, let q*, pθ,Φ ∈ RV be the corresponding vector of probabilities. Then we have
v=Φ> λ, lv>(pθ,φ - q*)| ≤ v2 (DKL(Pθ,φH)-θinR d Ds*,"	(32)
kvk∞≤1
26
Published as a conference paper at ICLR 2021
Pinsker’s inequality (Lemma E.3), on the other hand, gives
max ∣v>(pθ,φ - q*)| ≤ ∖∕2Dkl(pθ,φ,7*)
kvk∞≤1
Proof. Define the loss ρ(θ) := Dkl(pθ,φ, q*). The statement in Equation (32) to prove reduces to
max	∣λ>(Φpθ,φ - Φq*)| ≤
kΦ>λk∞≤1
inf ρ(θ*)
θ*∈Rd
(33)
To prove this, we compute the gradient and hessian of ρ(θ) w.r.t. θ. We can simplify ρ(θ) as follows
ρ(θ) = Dkl(pθ,φ, q*) = E [- log(pθ,Φ(w))] = E
W〜q*	W〜q*
-θ>Φq* + log
-θ>Φq* +log(Zθ)
-log 0)1
The gradient is
Vρ(θ) = V [-θ>Φq* + log(Zθ)] = -Φq* + 孕
Zθ
= -Φq* + V Pw" =-Φq*+ PW e7W φ
Zθ	Zθ
=-Φq* + Φpθ,Φ
Similarly the Hessian can be computed
e φw
V2ρ(θ) = V(Vρ(θ)) = V[-Φq* + Φpθ,φ] = VE pθ,φ(w)Φw = E V--φw
W∈W	W∈W	Zθ
-θ φw	-θ φw
E -~z—φw φW —Zr- φw
W∈W	θ	θ
E	[φW φW] - ( E	[φW ]) ( E [φw])	= COvW 〜Pθ Φ [φw]
W 〜Pθ,Φ	∖w 〜Pθ,Φ	W ∖w 〜Pθ,Φ	)	,
Where CovW〜？@,Φ[Φw ] denotes the covariance of the word embeddings Φw when measured w.r.t. the
distribution pθ,Φ. This directly gives us that V2ρ(θ) < 0, since the covariance is always psd, and
thus ρ is convex in θ.
We return to the statement in Equation (33) that we need to prove. With the expression for gradient
of ρ at hand, we can rewrite Equation (33) as trying to prove
∣λ>Vρ(θ)∣ ≤ kΦ>λk∞
inf ρ(θ*)
θ* ∈Rd
(34)
Furthermore, using the definition of the Hessian, it is not hard to see for some λ,石 ∈ Rd
that λ>V2ρ(石)λ = CovW〜Pj φ[λ>≠W] ≤ E [(λ>0W)2] ≤ ∣∣Φ>λk∞. Thus we can evoke
,	W 〜Pθ,Φ
Lemma E.5 with ` = ρ and L = ∣Φ>λ∣2∞ to prove Equation (34) and thus completing the proof.
Intuitively Lemma E.5 exploits the smoothness of the function to argue that small suboptimality (i.e.
being close to optimal solution in function value) is sufficient to guarantee small norm of the gradi-
ent, a property that is well-known in the optimization literature. We now present this lemma □
Lemma E.5. If a function ' : Rd → R and λ ∈ Rd satisfy λ>V2'(θ)λ ≤ L, ∀京 ∈ Rd (L-
smoothness in the direction of λ) and if' = inf θ∈Rd '(θ) ,then ∣λ>V'(θ)∣2 ≤ 2L('(Θ) - '*)
27
Published as a conference paper at ICLR 2021
Proof. This is a variant of a classical result used in optimization and We prove it here for complete-
ness. For any η ∈ R we have
'(θ) - '* ≥⑺ '(θ) - '(θ - ηλ)
≥⑹ '(θ) - θ(θ) + ",(θ), -ηλ> + η2λ>V2'(θ)λ
/ ` 丁	η2 L
≥(°) η(λ>V'(θ)) - η2-
where (a) follows from the definition of infimum and (b) follows from Taylor,s expansion for some
θ ∈ [θ-ηλ, θ] and (C) follows from the smoothness condition in the statement of the lemma. Picking
η = λ ?⑻ gives us '(θ) - '* ≥ £ ∣λ> V'(θ) ∣2,thus completing the proof.	□
Lemma E.6. FOr a language model {p.∣s} and classifier v ∈ RV,
v>ςPL O{p∙∣s})v ≤ 2kvk∞ ('Xent ({p.|s}) - 'Xent)
where ∑pl (g) = E [g(s)g(s)τ] and ∆{p }(s) = p,∣s — p*∣s are defined in Section B
S 〜PL	, s	1
Proof. We first note that
'xent(M}) - "{p*∣s}) = S我焉 JIOg (U )# = sJPl [DKL(P*∣s, B∣s)]	(35)
We bound vt∑pl (∆g∣s})v below
v>∑pl Ap∙∣s})v = sE ](VTg∣ s- p*∣ S))]
≤(α) kvk∞ E [2Dkl(p*∣ S,P∙∣ s)i
S〜PL
=⑹ 2∣∣v∣∣∞ ('χent({P∙ ∣ S})-'xent({p*∣ s}))
where (a) follows from Lemma E.3 (Pinsker,s inequality), (b) uses Equation (35).
Lemma E.7. For a fixed Φ, a softmax language model WithfeatUreS f and λ ∈ Rd,
λτ∑pL (Φ∆{pf⑸})λ ≤ 2∣∣Φτλ∣∣∞ ('Xent(f, Φ) - 'Xent(Φ))
where ∑PL (M{Pf(s)}) =SJPJ&Pf(S)- φP*∣ S)&Pf (S)- φP*∣ S)T] as defined in Section B.
□
Proof. We start by nothing that
λτΣPL (Φ∆Μ)})λ = λτ sJP J(ΦPf(S) - Φp*∣ S)(ΦPf(S) - Φp*∣ s)t] λ
=E [∣λτ(φPf(S) - φP*∣ s)∣2]= E [∣(φ>λ)T(Pf(S)- P*∣ S)|2]
SJPL	I	SJPL
We will use the variant of Pinsker,s inequality from Lemma E.4 to bound each term on the right
hand side. Notice that 45nt(f, Φ) - 'Xent(Φ) = E ['xent,S(f (s), Φ) - inf 'xent,S(θ, Φ)].
SJPL	θ∈Rd
λτ∑pL (Φ∆{pf ⑸})λ = E [∣(φτλ)τ(Pf (S) - P*∣ S )∣2]
SJPL
≤(O) 2kφτλk∞ E	DKL(PX S, Pf(S),φ) - /f DKL(P*∣ S, PΘ,φ)
SJPL |_	θ∈Rd	1
≤ 2∣∣Φτλ∣∣∞ E	'xent,S(f (s), Φ) - inf 'xent,S(θ, Φ)
SJPL |_	θ∈Rd
≤ 2∣∣Φτλ∣∣∞ ('xent(f, Φ)-Gent(Φ))
where (a) follows from Lemma E.4. This completes the proof.	□
28
Published as a conference paper at ICLR 2021
E.6. 1 Classification loss to covariance of error
Lemma E.8. For any task T and classifier V ∈ RV and predicted probabilities {p.∣s}
't({p∙∣s}, V) ≤'t({p*s}, V) + dsEτ h(v>(P∙∣s -p*∣s))2i
='t ({P*s}, V) + Jv>ςPT (δ{p∙∣sDv
where ∑1Tτ(g) = E [g(s)g(s)>] and ∆{p }(s) = p∙∣s — p：§ are defined in Section B.
S 〜PT	∙1 s	I
Proof. The following sequence of inequalities proves it
't({p∙∣s}, V)=	E	['(v>P∙∣s,y)] ≤(a)	E	['(v>p*∣s,y) + ∣V>(p*∣s — P∙∣s)∣
(s,y)〜PT	(s,y)〜PT L
'T({P*∣s}, V) +
≤⑹ E	'(V>p*s,y)
(s,y)〜PT L
E	[(P*s - P∙∣S)(P*∣s- P∙∣s)>
S〜PT L 1	1
V
='t({P*s}, V) + Jv>ςPTO{p∙∣s})v
where (a) follows from I-Iipschitzness of ', (b) follows from Jensen's inequality.	□
E.6.2 Handling distribution shift
LemmaE.9. For any g : S → RD andPT ∈ ∆s, we have ∣∣∑pl(g)-2∑pt(g)∑PL(g)-11∣2 ≤
γ(pT)-1
Proof. By definition of γ(pT), we have that
ΣPL (g) = E [g(s)g(s)>] =	pL(s)g(s)g(s)>
s~PL	s∈S
< γ(pT)	pT (s)g(s)g(s)> = γ(pT) E [g(s)g(s)>] = γ (pT)ΣPT (g)
s∈S	S 〜PT
ThUS Y(PT)ςpl (g)	<	ςpt(g) and hence Y(PTy ςpl (g)-2 ςpl (gRpL (g)-1	<
∑PL (g)-2 Σpτ(g)∑PL (g)-1, which is equivalent to Y(PT)ID < ∑pl (g)-2 ∑pt(g)∑PL (g)-2. ThiS
finishes the proof.	□
Lemma E.10. For matrices X , Y ∈ RD×D s.t. X , Y < 0 and Y is full rank, we have that
max a>Xa = kY-2XY-1 k2 for any norm k ∙ k.
a∈RD ,0<kak≤λ a>Ya 11	2 J J	11	11
>
Proof. Note that a>χa is independent of the scaling of a. The following sequence of inequalities
completes the proof
a>Xa
a>Xa
a>Xa
max	丁— = max 丁— = max-------------1------1——
a∈RD,0<kak≤λ a 1 Ya a∈RD a 1 Ya	a∈RD (Y 1 a)> (Y 1 a)
= max	a> Xa = max (Y -1 b)>X (Y - 2 b)
a∈RD ,kY1 ak2 = l	bERDJIbk2 = 1
= max	b>Y - 2 XY -1 b = ∣∣Y -1XY -11∣2
b∈RD,kbk2=1
□
29
Published as a conference paper at ICLR 2021
F	Experiment Details
For all experiments9, we use the 117M parameter “small” GPT-2 model proposed in Radford et al.
(2019) and implemented in HuggingFace (Wolf et al., 2019). Linear classification experiments
(except for fine-tuning baseline in Table 1) are performed on fixed output features from GPT-2.
We note that the binary SST-2 dataset used in all experiments is comprised of complete sentences,
and there are 6,920 train examples and 1,821 test examples. In particular, this dataset is smaller than
the version included with the GLUE benchmark (Wang et al., 2018). This smaller version of SST-2
better fits the sentence completion hypothesis we propose.
F.1 S OLVING DOWNSTREAM TASKS USING f AND Φpf
The features f from GPT-2 for any input sequence (w1, . . . , wN) is the output embedding of the
final token wN at the final layer, where N is the input length and can be different for different
inputs. This is also the embedding that is directly multiplied by the word embeddings to get the
softmax distribution for language modeling, as in the theoretical setting. To use a prompt, the same
prompt is added at the end of all inputs and the features are extracted for this modified input.
We use the LogisticRegressionCV class from the scikit-learn package to fit linear classifiers
to all fixed features (i.e., no finetuning). We use the liblinear solver and one-vs-rest loss function
unless it catastrophically fails (e.g., close to random performance) on a particular multi-class task.
In that case, we use the stochastic average gradient (SAG) algorithm with multinomial loss. We
use 5-fold cross validation for all experiments and test values for the regularization parameter C
between 1e-6 and 1e4 for small datasets (i.e., fewer than 10K examples) and between 1e-3 and
1e3 for larger datasets.
Details about word subsets: For all of the results presented in Table 1, we use a pre-trained GPT-
2 model. For SST, we use the prompt “This movie is ” when indicated. For AG News, we use the
prompt “This article is about ” when indicated.
We compute the conditional probability of selecting a subset of words to complete the sentence.
For AG News, this subset is: ’world’, ’politics’, ’sports’, ’business’, ’science’, ’financial’, ’market’,
’foreign’, ’technology’, ’international’, ’stock’, ’company’, ’tech’, ’technologies’. For SST, this
subset is: ’:)’, ’:(’, ’great’, ’charming’, ’flawed’, ’classic’, ’interesting’, ’boring’, ’sad’, ’happy’,
’terrible’, ’fantastic’, ’exciting’, ’strong’. For AG News, the class words we use are: ’foreign’,
’sports’, ’financial’, ’scientific’. For SST, the class words we use are ‘:)’ and ‘:(’.
We account for BPE tokenization by using the encoding of the word directly and the encoding of the
word with a space prepended. We then filter to use only words that encode to a single BPE token.
Tests on additional datasets: We also test the performance of pre-trained GPT-2 embeddings f
and the conditional mean embeddings Φpf on the DBPedia (Auer et al., 2007), Yahoo Answers
(Zhang et al., 2015), TREC (Li & Roth, 2002), IMDb (Maas et al., 2011), Customer Review (CR)
(Hu & Liu, 2004), and MPQA polarity (Wilson & Wiebe, 2003) datasets in Table 2. We limited the
training set size to 250K for larger datasets (i.e., DBPedia and Yahoo Answers). For CR and MPQA,
we follow Zhang et al. (2015) and average the performance across 10 random 90-10 train-test splits
of the dataset.
We find that Φpf consistently has comparable performance to f across non-sentiment and sentiment
downstream classification tasks. We include baseline results of bag of n-grams (BonG) for most
tasks and the mLSTM model (Radford et al., 2017) for sentiment tasks. BonG performs quite well
on the larger datasets, but not as well on smaller datasets, due to the high dimensionality of features.
For sentiment tasks, adding a prompt almost always boosts performance. We also demonstrate that
much of the performance can be recovered by only looking at “positive” and “negative” or “:)”
and “:(” as class words. Using these 2-dimensional features is even more sample-efficient than the
standard 768-dimensional ones.
9Link to code: https://github.com/sadhikamalladi/mathematical- exploration- downstream- tasks.
30
Published as a conference paper at ICLR 2021
Table 2: GPT-2 performance without fine-tuning on downstream task test sets with k classes. We
provide the performance of bag of n-grams (BonG) as an approximate baseline for these tasks. AG
News, DBPedia and Yahoo performances were reported in Zhang et al. (2015), and the other tasks
were reported in Khodak et al. (2018). We also include results from mLSTM (Sentiment Neuron)
(Radford et al., 2017) for the sentiment-related classification tasks (SST, IMDb, CR, and MPQA)
with numbers reported from Khodak et al. (2018). Furthermore, we include results for BERT (Devlin
et al., 2018) features without fine-tuning, where we use the output features for the first position of
an input for linear classification. An asterisk indicates we add a standard sentiment prompt “The
sentiment is” to each input, but for AG News we used the prompt “This article is about”. We also
tested the performance of the conditional probability distribution over “positive” and “negative” as
well as “:)” and “:(” on the sentiment-related tasks with and without the prompt.
Task	k f (S)		Φpf(s) p.|s： pos,neg p.|§: :),:(	BonG mLSTM		BERT
Non-sentiment					
AG News	4	90.7	84.6	-	-	92.4 (n = 5)	-	88.9
AG News*	4	91.1	88.2			89.9
DBPedia	14	97.2	88.2	-	-	98.6(n=5)	-	98.7
Yahoo	10	69.2	56.7	-	-	68.5 (n = 5)	-	65.0
TREC	6	93.6	87.8	-	-	89.8 (n = 3)	-	90.6
Sentiment
SST
SST*
SST fine
SST fine*
IMDb
IMDb*
CR
CR*
MPQA
MPQA*
22552-2-2
87.5	83.3	74.9	78.7	80.9 (n = 2)	91.8	85.8
89.4	87.3	80.8	79.1	-	-	84.1
49.2	43.5	37.5	39.2	42.3 (n = 3)	52.9	43.5
49.4	48.0	41.5	40.2	-	-	43.3
88.1	82.7	73.8	76.2	89.8 (n = 3)	92.3	82.2
88.4	85.3	81.8	80.9	-	-	84.0
86.8	84.6	74.9	80.0	78.3 (n = 3)	91.4	85.5
87.9	87.1	82.5	79.4	-	-	84.6
86.0	79.2	75.6	70.7	85.6 (n = 3)	88.5	87.3
87.8	86.1	80.3	71.4	-	-	88.1
We also include results using the pre-trained BERT base cased model (Devlin et al., 2018; Wolf
et al., 2019), using the embedding at the first token as input to the downstream task. We also tried
using the mean embedding and last token embedding and found that the first token embedding is
often the best. Moreover, the first token embedding is what is extracted in the traditional usage of
BERT on downstream tasks, though we note that it is rare to use BERT without fine-tuning.
F.2 Finetuning Experiments
As a strong baseline, we finetune the GPT-2 features along with learning a linear classifier for the
SST and AG News classification tasks and report accuracy numbers in Table 1. We use a maximum
sequence length of 128 BPE tokens for downstream inputs of SST-2 and a maximum length of 400
BPE tokens for AG News inputs. We use the end of sentence token as the padding token. The
datasets are described below.
1.	AG News has 108K train examples, 12K dev examples, 7600 test examples. We split the
train set for AG News into train and dev (90-10) and use the same test set as the non-
finetuning experiments.
2.	The sentence version of SST-2 has 6,920 train examples (same as non-finetuning), and 810
examples for dev and test each (split the original test set in half).
3.	Fine-grained SST-2 has 8,544 train examples (same as non-finetuning), and 1,105 examples
each for the dev and test data (split the original test set in half).
To select the best hyperparameter configuration, we run a grid search over learning rate and batch
size. We train each model for 10 epochs. For all datasets, we test learning rates 5e-5, 1e-4, and
31
Published as a conference paper at ICLR 2021
Table 3: Comparing Quad features to cross-entropy features for GPT-2 trained on the IMDb un-
labeled corpus (Maas et al., 2011). In this experiment we fix Φ to be the word embeddings from
prertained GPT-2 model for the cross-entropy objective. For the Quad objective, we initialize Φ to
be the SVD of the pre-trained embeddings. An asterisk indicates that we added the prompt “This
movie is ” to each input.
Task	f(s) (xent)	Φpf (s) (xent)	f(s) (Quad)
SST	82.1%	79.9%	77.3%
SST*	83.1%	81.1%	80.7%
Table 4: Comparing Quad features to cross-entropy features for GPT-2 trained on the Amazon
corpus. An asterisk indicates that we added the prompt “This movie is ” to each input. Note that the
validation loss was still decreasing at the time of measurement.
Task	f (s) (xent)	Φpf (s) (xent)	f(s) (Quad, learned Φ)
SST	89.4%	89.7%	79.2%
SST*	89.7%	89.2%	84.3%
3e-4. For both version of SST-2, we try batch sizes 8, 16, and 32, and for AG News, we try batch
sizes 8, 12, and 16. We note that the longer sequence length of AG News inputs required us to
use parallelization across multiple GPUs to simulate larger batch sizes, which made batch size 32
prohibitively expensive to test.
We take the hyperparameter configuration that achieves the best performance on the dev set and then
perform fine-tuning using those settings with three different random seeds: 8, 33, and 42. We then
report the average performance on the test set in Table 1.
We perform the hyperparameter grid search over the standard datasets and then perform fine-tuning
using the best settings on the dataset with task-specific prompts added. For SST-2, we use the prompt
“This movie is ”, and for AG News we use “This article is about ”.
F.3 Testing Quad objective
We test two models with the same parametrizations, one trained using our Quad objective and an-
other trained with the standard cross-entropy objective using the unlabeled IMDb corpus (Maas
et al., 2011) and the Amazon product review corpus (McAuley et al., 2015). We slightly modify the
standard architecture of GPT-2 to generate Tables 3 and 4. First we add a single linear layer (that is
trained) on top of the output features of the standard Transformer architecture. Furthermore, instead
of tying the input and output word (token) embeddings, we learn them separately so that f and Φ are
independent functions; this is more in line with out theoretical setup. We fix the input embeddings
and the positional embeddings to be the parameters from the pre-trained GPT-2.
For Quad, we initialize Φ, the output embeddings, using the singular vectors of the pre-trained
word embeddings Φ. For the cross-entropy models, we initialize Φ to be the full pre-trained word
embeddings Φ, because we found that initializing with the singular vectors harmed performance.
Given our parameterization, initializing with the singular vectors is as expressive as initializing
with the pretrained embeddings Φ themselves; however it potentially lends a better optimization
landscape and speeds up training for our new objective Quad. As described in Section 5.2, we
minimize the following objective
'quad(f, Φ)= E [-f(s)>Φw + 1 kΦ> f (s)k2
(s,w)	2
(36)
where (s, w) are sampled from the text corpus. The implementation of the Quad loss is the same
as the standard cross-entropy loss, the main difference being the second term: it is 1 ∣∣Φ>f (s)k2 for
Quad instead of the log-partition function log Pw0 ef(s)>φw0 in the cross-entropy objective.
32
Published as a conference paper at ICLR 2021
True log partition versus learned quadratic function
DBPedia	IMDb	,Vħhoo
-200	-100	0
TREC
-100	0	100
CR
-120 -100 -80	-60
True log partition
-200 -100	0	100
MPQA
-100	-50	0	-100 -50	0	50
True log partition	True log partition
Figure 2: Fit of the learned quadratic function to the log partition function on various datasets for
features computed by the full, pre-trained GPT-2. We also plot the y = x line for reference. These
plots are meant to verify Assumption 4.1.
Because IMDb is a smaller dataset, we fix Φ at its initialization and only train f to generate Table
3. When training on the Amazon dataset, we initialized Φ the same way as we did for the IMDb
dataset, but we allowed f and Φ to both be trained, since more data was available. To train the
models, we use the standard learning rate schedule as in in Radford et al. (2019). To learn a model
on IMDb, we use a context size of 512 BPE tokens, and for the Amazon reviews dataset (McAuley
et al., 2015), we use the standard context length of 1,024 BPE tokens.
We observe that training using Quad, in both cases, yields comparable performance to the language
model on the SST task, but always slightly worse. According to the theory, features f (s) from
Quad should learn p；§ on a subspace, just like Φpf from cross-entropy models, thus making the
comparison between these two important. Furthermore, adding a prompt consistently improves
performance for both objectives. While Quad did not beat the cross-entropy in either case, its good
performs at least demonstrates that insights from the theoretical analysis can translate to practical
algorithms. We leave exploring the gap in performance between Quad and cross-entropy and a more
extensive evaluation of Quad for future work.
F.4 Learning the quadratic approximation of the log-partition function
In Assumption 4.1, we assert that there is a quadratic fit for the log partition function, which allows
us to show in Lemma 4.3 that a linear relation holds between f and Φpf. We validate these theoret-
ical findings by fitting a quadratic function to the log partition function for a subset of embeddings
from the IMDb, SST, and AG News datasets (Figure 1). Here, we describe how we learned A, b
and c. To ensure A is symmetric and positive semi-definite as required, we parametrize A = UUT .
>	θ φw0
As defined earlier, the partition function Z§ = £仅,eθ QPw and Φpθ = £仅,e^—。W，for any
θ ∈ Rd. We minimize the following objective function:
33
Published as a conference paper at ICLR 2021
(a) Trained on IMDb (Maas et al., 2011)
(b) Trained on Amazon (McAuley et al., 2015)
Figure 3: Logistic loss of conditional mean features on the SST-2 task for various checkpoints of a
GPT-2 architecture trained on IMDb and Amazon. The reported cross-entropy is measured on the
validation set. The red trend shows the fit of a square-root function, which is what the upper bound
in Theorem 4.2 looks like.
L(U,b,c)	= E	λι	(log(Zθ)	- 2θ>UU>θ	-	θ>b	- C)	+ λ2 ∣∣Φpθ	- UU>θ	- b∣∣2	(37)
In practice, we train only on the regression loss (i.e., λ1 = 0, λ2 = 1) for the most promising results.
Note that the regression term is trying to learn a linear relationship between between θ and Φpθ that
Lemma 4.3 aims to prove. This ends up learning a matrix A = UU> and vector b that also satisfy
the quadratic form of log(Zθ) from Assumption 4.1.
We use 20,000 examples from a mix of IMDb, SST, and AG News embeddings as the training set.
Thus we sample θ by sampling s from the aforementioned datasets and set θ = f (s), f being
the feature map from pretrained GPT-2. We use the Adam (Kingma & Ba, 2014) optimizer with
learning rate 1e-3 for U and learning rate 1e-4 for b and c. We decay the learning rate every 50
steps by a factor of 0.1. We use the U obtained after 8 epochs of training. We further demonstrate
the quality of the learned fit by plotting the true log partition and estimated log partition function for
embeddings from other datasets in Figure 2.
F.5 Experimentally Checking Theorem 4.2
Theorem 4.2 can be informally summarized as stating that an e SUboPtimanty in the cross-entropy
of a d-dimensional language model propagates toa √e increase in the logistic loss. We note that the
τ, B, and γ(pT ) factors are fixed for a given pre-training corpus and downstream task, so we can
empirically test if this square root relationship holds in practice. In particular, Theorem 4.2 says
't(Φpf) ≤ T + ,2B2 (Y(PT))T ('xent(f, Φ) - 'Xent)	(38)
Of these, τ, B, γ(pT-)-1 and 'Xent are independent of the language model (f, Φ) and only depend
on the task T and language modeling distribution. Thus We can rewrite this as't(Φpf) ≤ C +
avz'xent(f, Φ) - b for suitable constants a,b,c ∈ R. The left hand side, 't(Φpf), is the logistic
loss of conditional mean features from language model (f, Φ) on task T and 'xent(f, Φ) is the cross-
entropy loss of the language model, both of which can be measured in practice.
We train a 117M parameter GPT-2 model from scratch on the IMDb and Amazon corpora, described
in Section F.3. We maintain checkpoints during training, and for each checkpoint, we measure the
cross-entropy of the model on the validation set as well as the performance of the conditional mean
features Φpf on SST-2. Plotting these values together yields Figure 3.
We furthermore fit a square root trend, shown in red, to these points. We learn a, b, C such that
y ≈ a√x - b + c, where y = 't(Φpf) is the logistic loss and X = 'xent(f, Φ) is the cross-entropy
loss. For this, we perform a grid search over 100 evenly spaced valid values of b, and for each b,
34
Published as a conference paper at ICLR 2021
We perform linear regression on X- - b to find a and c. We choose the a, b, C that maximizes the
r-value of the regression. While Theorem 4.2 only provides an upper bound on the logistic loss, this
experiment shoWs that some square-root trend is observable in practice.
35