Published as a conference paper at ICLR 2021
Learning perturbation sets for robust ma-
CHINE LEARNING
Eric Wong
Computer Science and Artificial
Intelligence Laboratory
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
wongeric@mit.edu
J. Zico Kolter
Computer Science Department
Carnegie Mellon University and
Bosch Center for Artificial Intelligence
Pittsburgh, PA 15213, USA
zkolter@cs.cmu.edu
Ab stract
Although much progress has been made towards robust deep learning, a significant
gap in robustness remains between real-world perturbations and more narrowly
defined sets typically studied in adversarial defenses. In this paper, we aim to bridge
this gap by learning perturbation sets from data, in order to characterize real-world
effects for robust training and evaluation. Specifically, we use a conditional genera-
tor that defines the perturbation set over a constrained region of the latent space.
We formulate desirable properties that measure the quality of a learned perturbation
set, and theoretically prove that a conditional variational autoencoder naturally
satisfies these criteria. Using this framework, our approach can generate a variety
of perturbations at different complexities and scales, ranging from baseline spatial
transformations, through common image corruptions, to lighting variations. We
measure the quality of our learned perturbation sets both quantitatively and qualita-
tively, finding that our models are capable of producing a diverse set of meaningful
perturbations beyond the limited data seen during training. Finally, we leverage
our learned perturbation sets to train models which are empirically and certifiably
robust to adversarial image corruptions and adversarial lighting variations, while
improving generalization on non-adversarial data. All code and configuration files
for reproducing the experiments as well as pretrained model weights can be found
at https://github.com/locuslab/perturbation_learning.
1	Introduction
Within the last decade, adversarial learning has become a core research area for studying robustness
and machine learning. Adversarial attacks have expanded well beyond the original setting of
imperceptible noise to more general notions of robustness, and can broadly be described as capturing
sets of perturbations that humans are naturally invariant to. These invariants, such as facial recognition
should be robust to adversarial glasses (Sharif et al., 2019) or traffic sign classification should be
robust to adversarial graffiti (Eykholt et al., 2018), form the motivation behind many real world
adversarial attacks. However, human invariants can also include notions which are not inherently
adversarial, for example image classifiers should be robust to common image corruptions (Hendrycks
& Dietterich, 2019) as well as changes in weather patterns (Michaelis et al., 2019).
On the other hand, although there has been much success in defending against small adversarial
perturbations, most successful and principled methods for learning robust models are limited to
human invariants that can be characterized using mathematically defined perturbation sets, for
example perturbations bounded in `p norm. After all, established guidelines for evaluating adversarial
robustness (Carlini et al., 2019) have emphasized the importance of the perturbation set (or the threat
model) as a necessary component for performing proper, scientific evaluations of adversarial defense
proposals. However, this requirement makes it difficult to learn models which are robust to human
invariants beyond these mathematical sets, where real world attacks and general notions of robustness
can often be virtually impossible to write down as a formal set of equations. This incompatibility
between existing methods for learning robust models and real-world, human invariants raises a
fundamental question for the field of robust machine learning:
1
Published as a conference paper at ICLR 2021
How can we learn models that are robust to perturbations without a predefined perturbation set?
In the absence of a mathematical definition, in this work we present a general framework for learning
perturbation sets from perturbed data. More concretely, given pairs of examples where one is a
perturbed version of the other, we propose learning generative models that can “perturb” an example
by varying a fixed region of the underlying latent space. The resulting perturbation sets are well-
defined and can naturally be used in robust training and evaluation tasks. The approach is widely
applicable to a range of robustness settings, as we make no assumptions on the type of perturbation
being learned: the only requirement is to collect pairs of perturbed examples.
Given the susceptibility of deep learning to adversarial examples, such a perturbation set will
undoubtedly come under intense scrutiny, especially if it is to be used as a threat model for adversarial
attacks. In this paper, we begin our theoretical contributions with a broad discussion of perturbation
sets and formulate deterministic and probabilistic properties that a learned perturbation set should
have in order to be a meaningful proxy for the true underlying perturbation set. The necessary
subset property ensures that the set captures real perturbations, properly motivating its usage as an
adversarial threat model. The sufficient likelihood property ensures that real perturbations have high
probability, which motivates sampling from a perturbation set as a form of data augmentation. We
then prove the main theoretical result, that a learned perturbation set defined by the decoder and prior
of a conditional variational autoencoder (CVAE) (Sohn et al., 2015) implies both of these properties,
providing a theoretically grounded framework for learning perturbation sets. The resulting CVAE
perturbation sets are well motivated, can leverage standard architectures, and are computationally
efficient with little tuning required.
We highlight the versatility of our approach using CVAEs with an array of experiments, where we vary
the complexity and scale of the datasets, perturbations, and downstream tasks. We first demonstrate
how the approach can learn basic '∞ and rotation-translation-skew (RTS) perturbations (Jaderberg
et al., 2015) in the MNIST setting. Since these sets can be mathematically defined, our goal is
simply to measure exactly how well the learned perturbation set captures the target perturbation set
on baseline tasks where the ground truth is known. We next look at a more difficult setting which
can not be mathematically defined, and learn a perturbation set for common image corruptions on
CIFAR10 (Hendrycks & Dietterich, 2019). The resulting perturbation set can interpolate between
common corruptions, produce diverse samples, and be used in adversarial training and randomized
smoothing frameworks. The adversarially trained models have improved generalization performance
to both in- and out-of-distribution corruptions and better robustness to adversarial corruptions. In
our final setting, we learn a perturbation set that captures real-world variations in lighting using a
multi-illumination dataset of scenes captured “in the wild” (Murmann et al., 2019). The perturbation
set generates meaningful lighting samples and interpolations while generalizing to unseen scenes, and
can be used to learn image segmentation models that are empirically and certifiably robust to lighting
changes. All code and configuration files for reproducing the experiments as well as pretrained model
weights for both the learned perturbation sets as well as the downstream robust classifiers are at
https://github.com/locuslab/perturbation_learning.
2	Background and related work
Perturbation sets for adversarial threat models Adversarial examples were initially defined
as imperceptible examples with small 'ι, '2 and '∞ norm (Biggio et al., 2013; Szegedy et al.,
2013; Goodfellow et al., 2014), forming the earliest known, well-defined perturbation sets that
were eventually generalized to the union of multiple 'p perturbations (Tramer & Boneh, 2019;
Maini et al., 2019; Croce & Hein, 2019; Stutz et al., 2019). Alternative perturbation sets to the
`p setting that remain well-defined incorporate more structure and semantic meaning, such as
rotations and translations (Engstrom et al., 2017), Wasserstein balls (Wong et al., 2019), functional
perturbations (Laidlaw & Feizi, 2019), distributional shifts (Sinha et al., 2017; Sagawa et al., 2019),
word embeddings (Miyato et al., 2016), and word substitutions (Alzantot et al., 2018; Jia et al., 2019).
Other work has studied perturbation sets that are not necessarily mathematically formulated but well-
defined from a human perspective such as spatial transformations (Xiao et al., 2018b). Real-world
adversarial attacks tend to try to remain either inconspicuous to the viewer or meddle with features
that humans would naturally ignore, such as textures on 3D printed objects (Athalye et al., 2017),
graffiti on traffic signs (Eykholt et al., 2018), shapes of objects to avoid LiDAR detection (Cao et al.,
2
Published as a conference paper at ICLR 2021
2019), irrelevant background noise for audio (Li et al., 2019a), or barely noticeable films on cameras
(Li et al., 2019b). Although not necessarily adversarial, Hendrycks & Dietterich (2019) propose the
set of common image corruptions as a measure of robustness to informal shifts in distribution.
Generative modeling and adversarial robustness Relevant to our work is that which combines
aspects of generative modeling with adversarial examples. While our work aims to learn real-world
perturbation sets from data, most work in this space differs in that they either aim to generate synthetic
adversarial `p perturbations (Xiao et al., 2018a), run user studies to define the perturbation set (Sharif
et al., 2019), or simply do not restrict the adversary at all (Song et al., 2018; Bhattad et al., 2020).
Gowal et al. (2019) trained a StyleGAN to disentangle real-world perturbations when no perturbation
information is known in advance. However the resulting perturbation set relies on a stochastic
approximation, and it is not immediately obvious what this set will ultimately capture. Most similar
is the concurrent work of Robey et al. (2020), which uses a GAN architecture from image-to-image
translation to model simple perturbations between datasets. In contrast to both of these works, our
setting requires the collection of paired data to directly learn how to perturb from perturbed pairs
without needing to disentangle any features or translate datasets, allowing us to learn more targeted
and complex perturbation sets. Furthermore, we formulate desirable properties of perturbation sets for
downstream robustness tasks, and formally prove that a conditional variational autoencoder approach
satisfies these properties. This results in a principled framework for learning perturbation sets that is
quite distinct from these GAN-based approaches in both setting and motivation.
Adversarial defenses and data augmentation Successful approaches for learning adversarially
robust networks include methods which are both empirically robust via adversarial training (Goodfel-
low et al., 2014; Kurakin et al., 2016; Madry et al., 2017) and also certifiably robust via provable
bounds (Wong & Kolter, 2017; Wong et al., 2018; Raghunathan et al., 2018; Gowal et al., 2018;
Zhang et al., 2019) and randomized smoothing (Cohen et al., 2019; Yang et al., 2020). Critically,
these defenses require mathematically-defined perturbation sets, which has limited these approaches
from learning robustness to more general, real-world perturbations. We directly build upon these
approaches by learning perturbation sets that can be naturally and directly incorporated into robust
training, greatly expanding the scope of adversarial defenses to new contexts. Our work also relates
to using non-adversarial perturbations via data augmentation to reduce generalization error (Zhang
et al., 2017; DeVries & Taylor, 2017; Cubuk et al., 2019), which can occasionally also improve
robustness to unrelated image corruptions (Geirhos et al., 2018; Hendrycks et al., 2019; Rusak et al.,
2020). Our work differs in that rather than aggregating or proposing generic data augmentations, our
perturbation sets can provide data augmentation that is targeted for a particular robustness setting.
3	Perturbation sets learned from data
For an example x ∈ Rm, a perturbation set S(x) ⊆ Rm is defined informally as the set of examples
which are considered to be equivalent to x, and hence can be viewed as “perturbations” of x. This set
is often used when finding an adversarial example, which is typically cast as an optimization problem
to maximize the loss of a model over the perturbation set in order to break the model. For example,
for a classifier h, loss function `, and label y, an adversarial attack tries to solve the following:
maximize '(h(x0),y).	(1)
x0∈S(x)
A common choice for S(x) is an `p ball around the unperturbed example, defined as S(x) = {x + δ :
kδ kp ≤ } for some norm p and radius . This type of perturbation captures unstructured random
noise, and is typically taken with respect to `p norms for p ∈ {0, 1, 2, ∞}, though more general
distance metrics can also be used.
Although defining the perturbation set is critical for developing adversarial defenses, in some scenar-
ios, the true perturbation set may be difficult to mathematically describe. In these settings, it may still
be possible to collect observations of (non-adversarial) perturbations, e.g. pairs of examples (x, X)
where X is the perturbed data. In other words, X is a perturbed version of x, from which we can learn
an approximation of the true perturbation set. While there are numerous possible approaches one
can take to learn S(x) from examples (x, X), in this work We take a generative modeling perspective,
where examples are perturbed via an underlying latent space. Specifically, let g : Rk × Rm → Rm
3
Published as a conference paper at ICLR 2021
be a generator that takes a k-dimensional latent vector and an input, and outputs a perturbed version
of the input. Then, we can define a learned perturbation set as follows:
S(x) = {g(z,x) : kzk ≤ }	(2)
In other words, we have taken a well-defined norm-bounded ball in the latent space and mapped it to
a set of perturbations with a generator g, which perturbs X into X via a latent code z. Alternatively,
we can define a perturbation set from a probabilistic modeling perspective, and use a distribution over
the latent space to parameterize a distribution over examples. Then, S(X) is now a random variable
defined by a probability distribution p(z) over the latent space as follows:
S(x)〜pθ such that θ = g(z,x),	Z 〜Pe	(3)
where p has support {z : kzk ≤ } and pθ is a distribution parameterized by θ = g(z, X).
3.1	General measures of quality for perturbation sets
A perturbation set defined by a generative model that is learned from data lacks the mathematical
rigor of previous sets, so care must be taken to properly evaluate how well the model captures real
perturbations. In this section we formally define two properties relating a perturbation set to data,
which capture natural qualities of a perturbation set that are useful for adversarial robustness and
data augmentation. We note that all quantities discussed in this paper can be calculated on both
the training and testing sets, which allow us to concretely measure how well the perturbation set
generalizes to unseen datapoints. For this section, let d : Rm × Rm → R be an distance metric (e.g.
mean squared error) and let x,X ∈ Rm be a perturbed pair, where X is a perturbed version of x.
To be a reasonable threat model for adversarial examples, one desirable expectation is that a perturba-
tion set should at least contain close approximations of the perturbed data. In other words, the set of
perturbed data should be (approximately) a necessary subset of the perturbation set. This notion of
containment can be described more formally as follows:
Definition 1. A perturbation set S(X) satisfies the necessary subset property at approximation error
at most δ for a perturbed pair (x, X) if there exists an x0 ∈ S (x) such that d(x0, X) ≤ δ.
For a perturbation set defined by the generative model from Equation (2), this amounts to finding a
latent vector Z which best approximates the perturbed example X by solving the following problem:
min d(g(z, x), X).	(4)
kzk≤e
This approximation error can be upper bounded with point estimates or can be solved more accurately
with projected gradient descent. Note that mathematically-defined perturbation sets such as `p balls
around clean datapoints contain all possible observations and naturally have zero approximation error.
Our second desirable property is specific to the probabilistic view from Equation (3), where we would
expect perturbed data to have a high probability of occurring under a probabilistic perturbation set. In
other words, a perturbation set should assign sufficient likelihood to perturbed data, described more
formally in the following definition:
Definition 2. A probabilistic perturbation set S(X) satisfies the sufficient likelihood property at
likelihood at least δ for a perturbed pair (x, X) if Epe(Z) [pθ (X)] ≥ δ where θ = g(z, x).
A model that assigns high likelihood to perturbed observations is likely to generate meaningful
samples, which can then be used as a form of data augmentation in settings that care more about
average-case over worst-case robustness. To measure this property, the likelihood can be approximated
with a standard Monte Carlo estimate by sampling from the priorpe.
4	Variational autoencoders for learning perturbations sets
In this section we will focus on one possible approach using conditional variational autoencoders
(CVAEs) to learn the perturbation set (Sohn et al., 2015). We shift notation here to be consistent with
the CVAE literature and consider a standard CVAE trained to generate X ∈ Rm from a latent space
Z ∈ Rk conditioned on some auxiliary variable y, which is traditionally taken to be a label. In our
4
Published as a conference paper at ICLR 2021
setting, the auxiliary variable y is instead another datapoint such that x is a perturbed version of y,
but the theory we present is agnostic to the choice in auxiliary variable. Let the posterior distribution
q(z|x, y), prior distribution p(z|y), and likelihood function p(x|z, y) be the following multivariate
normal distributions with diagonal variance:
q(z∖x,y) ^N(μ(x,y),σ2(χ,y)), P(ZIy)〜N(μ(y),σ2(y)), P(XEy)〜N(g(z,y'),I) ⑸
where μ(x, y), σ2(x, y), μ(y), σ2(y), and g(z, y) are arbitrary functions representing the respective
encoder, prior, and decoder networks. CVAEs are trained by maximizing a likelihood lower bound
logP(x∖y) ≥ Eq(z|x,y)[log P(x∖z, y)] - KL(q(z∖x, y)kP(z∖y))	(6)
also known as the SGVB estimator, where KL(∙∣∣∙) is the KL divergence. The CVAE framework
lends to a natural perturbation set by simply restricting the latent space to an `2 ball that is scaled and
shifted by the prior network. For convenience, we will define the perturbation set in the latent space
before the reparameterization trick, so the latent perturbation set for all examples is a standard `2 ball
{u : ∣∣u∣∣2 ≤ e} where Z = U ∙ σ(y) + μ(y). Similarly, a probabilistic perturbation set can be defined
by simply truncating the prior distribution at radius (also before the reparameterization trick).
4.1 Theoretical motivation of using CVAEs to learn perturbation sets
Our theoretical results prove that optimizing the CVAE objective naturally results in both the necessary
subset and sufficient likelihood properties outlined in Section 3.1, which motivates why the CVAE is
a reasonable framework for learning perturbation sets. Note that these results are not immediately
obvious, since the likelihood of the CVAE objective is taken over the full posterior while the
perturbation set is defined over a constrained latent subspace determined by the prior. The proofs
rely heavily on the multivariate normal parameterizations, with requiring several supporting results
which relate the posterior and prior distributions. We give a concise, informal presentation of the
main theoretical results in this section, deferring the full details, proofs, and supporting results to
Appendix A. Our results are based on the minimal assumption that the CVAE objective has been
trained to some threshold as described in Assumption 1.
Assumption 1. The CVAE objective has been trained to some thresholds R, Ki as follows
1k
Eq(z∣χ,y)[logP(XEy)] ≥ r, KLg(Z∖x,y)∣p(ZIy)) ≤ 2 工国
where each Ki bounds the KL-divergence of the ith dimension.
Our first theorem, Theorem 1, states that the approximation error of a perturbed example is bounded
by the components of the CVAE objective. The implication here is that with enough representational
capacity to optimize the objective, one can satisfy the necessary subset property by training a CVAE,
effectively capturing perturbed data at low approximation error in the resulting perturbation set.
Theorem 1. Let r be the Mahalanobis distance which captures 1 - α of the probability mass for a
k-dimensional standard multivariate normal for some 0 < α < 1. Then, there exists a Z such that
Ilz-σμy(r)∣∣2 ≤ eandkg(Zy) -χk2 ≤ δfor
= Br+	Ki,	δ
J一 (2R + m log(2π))
1 - a
—
where B is a constant dependent on Ki. Moreover, as R → 一 1 m log(2π) and Ki → 0 (the
theoretical limits of these bounds1), then → r and δ → 0.
Our second theorem, Theorem 2, states that the expected approximation error over the truncated prior
can also be bounded by components of the CVAE objective. Since the generator g parameterizes a
multivariate normal with identity covariance, an upper bound on the expected reconstruction error
implies a lower bound on the likelihood. This implies that one can also satisfy the sufficient likelihood
property by training a CVAE, effectively learning a probabilistic perturbation set that assigns high
likelihood to perturbed data.
1In practice, VAE architectures in general have a non-trivial gap from the approximating posterior which
may make these theoretical limits unattainable.
5
Published as a conference paper at ICLR 2021
Table 1: Condensed evaluation of CVAE perturbation sets trained to produce rotation, translation,
ans skew transformations on MNIST (MNIST-RTS), CIFAR10 common corruptions (CIFAR10-C)
and multi-illumination perturbations (MI). The approximation error measures the necessary subset
property, and the expected approximation error measures the sufficient likelihood property.
Setting	Approx. error	Expected approx. error	CVAE Recon. error	KL
MNIST-RTS	0.11	0.54	0.04	22.2
CIFAR10-C	0.005	0.029	0.001	69.3
MI	0.006	0.049	0.004	65.8
Theorem 2. Let r be the Mahalanobis distance which captures 1 - α of the probability mass for
a k-dimensional standard multivariate normal for some 0 < α < 1. Then, the truncated expected
approximation error can be bounded with
Epr(u) [kg(U ∙ σ(y) + μ(y),y) - xk2] ≤ -ι~1α(2R + mIog(Zn))H
where pr (u) is a multivariate normal that has been truncated to radius r and H is a constant that
depends exponentially on Ki and r.
The main takeaway from these two theorems is that optimizing the CVAE objective naturally results
in a learned perturbation set which satisfies the necessary subset and sufficient likelihood properties.
The learned perturbation set is consequently useful for adversarial robustness since the necessary
subset property implies that the perturbation set does not “miss” perturbed data. It is also useful for
data augmentation since the sufficient likelihood property ensures that perturbed data occurs with
high probability. We leave further discussion of these two theorems to Appendix A.3.
5	Experiments
Finally, we present a variety of experiments to showcase the generality and effectiveness of our
perturbation sets learned with a CVAE. Our experiments for each dataset can be broken into two
distinct types: the generative modeling problem of learning and evaluating a perturbation set, and the
robust optimization problem of learning an adversarially robust classifier to this perturbation. We
note that our approach is broadly applicable, has no specific requirements for the encoder, decoder,
and prior networks, and avoids the unstable training dynamics found in GANs. Furthermore, we do
not have the blurriness typically associated with VAEs since we are modeling perturbations and not
the underlying image.
In all settings, we first train perturbation sets and evaluate them with a number of metrics averaged
over the test set. We present a condensed version of these results in Table 1, which establishes a
quantitative baseline for learning real-world perturbation sets in three benchmark settings that future
work can improve upon. Specifically, the approximation error measures the necessary subset property,
the expected approximation error measures the sufficient likelihood property, and the reconstruction
error and KL divergence are standard CVAE metrics. The full evaluation is described in Appendix B,
and a complete tabulation of our results on evaluating perturbation sets can be found in Tables 4, 7,
and 10 in the appendix for each setting.
We then leverage our learned perturbation sets into new downstream robustness tasks by simply using
standard, well-vetted techniques for `2 robustness directly on the latent space, namely adversarial
training with an `2 PGD adversary (Madry et al., 2017), a certified defense with `2 randomized
smoothing (Cohen et al., 2019), as well as an additional data-augmentation baseline via sampling
from the CVAE truncated prior. Further discussion on the meaning of the randomized smoothing
certification can be found in Appendix B, and the corresponding pseudo-code for all of these
approaches can be found in Appendix C. We defer the experiments on MNIST to Appendix D,
and spend the remainder of this section highlighting the main empirical results for the CIFAR10
and Multi-Illumination settings. Additional details and supplementary experiments can be found in
Appendix E for CIFAR10 common corruptions, and Appendix F for the multi-illumination dataset.
6
Published as a conference paper at ICLR 2021
Figure 1: Visualization of a learned perturbation set trained on CIFAR10 common corruptions. (top
row) Interpolations from fog, through defocus blur, to pixelate corruptions. (middle row) Random
corruption samples for three examples. (bottom row) Adversarial corruptions that misclassify an
adversarially trained classifier at = 10.2.
Table 2: Adversarial robustness to CIFAR10 common corruptions with a CVAE perturbation set.
Method	Test set accuracy (%)			Test set robust accuracy (%)		
	Clean	Perturbed	OOD	=2.7	e = 3.9	e= 10.2
CIFAR10-C data augmentation	90.6	87.7	85.0	42.4	37.2	17.8
CVAE data augmentation	94.5	90.5	89.6	68.6	63.3	43.4
CVAE adversarial training	94.6	90.3	89.9	72.1	66.1	55.6
Standard training	95.2	67.0	68.1	20.1	17.8	10.1
Fast AutoAugment (Lim et al., 2019)	93.9	63.7	79.3	22.6	20.8	10.9
AugMix (Hendrycks et al., 2019)	92.0	68.8	82.9	39.5	34.4	16.8
`2 robust (Engstrom et al., 2019)	90.8	74.4	82.8	58.4	48.1	20.6
'∞ robust (Carmon et al., 2019)	89.7	71.2	80.3	60.2	50.7	23.6
Computational cost The cost of using our learned perturbation sets in downstream tasks is pro-
portional to the cost of a forward pass through the generator. For example, training with data
augmentation increases the cost by a single pass through the generator per example, while adversarial
training with T steps increases the cost by T passes through the generator. Although generative
models can often be quite large in size, we find that only modestly sized networks are sufficient for
modeling perturbations. In the following experiments, we use small residual (He et al., 2016) and
UNet (Ronneberger et al., 2015) architectures for the generator. These models are only a fraction
of the size of the downstream classifier, which dominates the computational cost of training. An
experiment exploring the scalability of the CVAE perturbation set to higher resolutions (e.g. up to
500 × 750) is discussed in Appendix F.1.
5.1	CIFAR 1 0 common corruptions
In this section, we first learn a perturbation set which captures common image corruptions for
CIFAR10 (Hendrycks & Dietterich, 2019).2 We focus on the highest severity level of the blur,
weather, and digital categories, resulting in 12 different corruptions which capture more “natural”
corruptions that are unlike random `p noise (a complete description of the setting is in Appendix
E). We find that the resulting perturbation set accurately captures common corruptions with a mean
approximation error of 0.005 as seen in Table 1. A more in-depth quantitative evaluation as well as
architecture and training details are in Appendix E.1.
We qualitatively evaluate our perturbation set in Figure 1, which depicts interpolations, random
samples, and adversarial examples from the perturbation set. For additional analysis of the perturba-
tion set, we refer the reader to Appendix E.2 for a study on using different pairing strategies during
training and Appendix E.3 which finds semantic latent structure and visualizes additional examples.
2We note that this is not the original intended use of the dataset, which was proposed as a general measure
for evaluating robustness. Instead, we are using the dataset for a different setting of learning perturbation sets.
7
Published as a conference paper at ICLR 2021
Figure 2: Visualizations from a learned perturbation set trained on the multi-illumination dataset. (top
row) Interpolations between different lighting angles. (bottom row) Random lighting perturbations
for two scenes.
Figure 3: Pairs of MI scenes (left) and their adversarial lighting perturbations (right).
Robustness to corruptions We next employ the perturbation set in adversarial training and random-
ized smoothing to learn models which are robust against worst-case CIFAR10 common corruptions.
We report results at three radius thresholds {2.7, 3.9, 10.2} which correspond to the 25th, 50th, and
75th percentiles of latent encodings as described in Appendix E.3. We compare to two data augmen-
tation baselines of training on perturbed data or samples drawn from the learned perturbation set, and
also evaluate performance on three extra out-of-distribution corruptions (one for each weather, blur,
and digital category denoted OOD) that are not present during training.
We highlight some empirical results in Table 2, where we first find that training with the CVAE
perturbation set can improve generalization. Specifically, using the CVAE perturbation set during
training achieves 3 - 5% improved accuracy over training directly on the common corruptions (data
augmentation) across all non-adversarial metrics. These gains motivate learning perturbation sets
beyond the setting of worst-case robustness as a way to improve standard generalization. Additionally,
the CVAE perturbation set improves worst-case performance, with the adversarially trained model
being the most robust at 66% robust accuracy for = 3.9 whereas pure data augmentation only
achieves 17.8% robust accuracy. Finally, we include a comparison to models trained with standard
training, AugMiX data augmentation, '∞ adversarial training, and '2 adversarial training, none which
can perform as well as our CVAE approach. We note that this is not too surprising, since these
approaches have different goals and data assumptions. Nonetheless, we include these results for the
curious reader, with additional details and discussion in AppendiX E.5. For certifiably robust models
with randomized smoothing, we defer the results and discussion to AppendiX E.6.
5.2	Multi-illumination
Our last set of eXperiments looks at learning a perturbation set that captures multiple lighting
conditions using the Multi-Illumination (MI) dataset (Murmann et al., 2019). These consist of a
thousand scenes captured in the wild under 25 different lighting variations, and our goal is to learn a
perturbation set which captures real-world lighting conditions. Architecture and training specifics can
be found in AppendiX F.1, and our perturbation set accurately captures real-world changes in lighting
that generalizes to new, unseen scenes with a low approXimation error of 0.006 as seen in Table 1.
Qualitatively, the perturbation set can produce interpolations between lighting angles as well as new
lighting variations from random samples as shown in Figure 2. This allows us to capture a continuous
range of lighting changes, a significantly larger set than the discrete angles manually collected in the
dataset. Additional samples and interpolations can be found in AppendiX F.2.
Robustness to lighting perturbations The MI dataset contains material map annotations for each
scene such as wood, plastic, and fabric, and so a natural downstream task is to predict materials in each
scene. Specifically, for each piXel, we predict one of 40 different material types to generate a material
segmentation map. Our goal is to generate material segmentation maps which are adversarially robust
to lighting perturbations, using our CVAE perturbation set. Several adversarial eXamples can be
8
Published as a conference paper at ICLR 2021
Table 3: Learning image segmentation models that are robust to real-world changes in lighting with a
CVAE perturbation set.
Method	Test set accuracy (%)	Test set robust accuracy (%)		
	Perturbed	=7.35	E = 8.81	E=17
Fixed lighting angle	37.2	26.3	24.2	14.9
MI data augmentation	45.2	38.0	36.5	27.1
CVAE data augmentation	41.5	35.5	33.9	24.7
CVAE adversarial training	41.7	39.4	38.8	35.4
found in Figure 3, which remain reasonable lighting perturbations despite being adversarial, and
additional examples as well as their corresponding segmentation maps can be found in Appendix F.3.
Table 3 summarizes the material segmentation results for adversarial lighting perturbations. We
report robustness at thresholds ∈ {7.35, 8.81, 17} which correspond to the 50th, 75th, and 100th
percentiles of the latent encodings for real data as described in Appendix F.1. We find that adversarial
training can improve robustness to worst-case lighting perturbations over directly training on the
perturbed examples, increasing robust accuracy from 27.1% to 35.4% at the maximum radius = 17.
However, training with the learned perturbation set does reduce performance on the perturbed data in
this setting. This can be seen as the cost of adversarial robustness, which is a common tradeoff when
training adversarially robust models. However, this reduction in perturbed accuracy can also be seen
as reduced overfitting to the discrete camera angles in the perturbed dataset, since the perturbation
set captures a continuous spectrum of lighting changes that are not explicitly collected. Additional
results on certifiably robust models with randomized smoothing can be found in Appendix F.3.
6	Conclusion
In this paper, we presented a general framework for learning perturbation sets from data when the
perturbation cannot be mathematically-defined. We outlined deterministic and probabilistic properties
that measure how well a perturbation set fits perturbed data, and formally proved that a perturbation
set based upon the CVAE framework satisfies these properties. This work establishes a principled
baseline for learning perturbation sets with quantitative metrics which future work can potentially
improve upon. Different approaches may result in better perturbation sets, for example by using more
powerful generative models such as autoregressive models and generative adversarial networks, or by
improving the training procedure.
Our framework of learning perturbation sets opens up a diverse range of new downstream robustness
tasks that more accurately reflect real-world changes. Although in this work we focused primarily
on adversarial and certifiable robustness to common image corruptions and lighting perturbations
for vision tasks, our framework is not specific to these tasks and so new perturbation sets could
potentially be learned and leveraged in robustness problems found in other domains. Our work may
also have implications beyond the adversarial regime, as we found that a learned perturbation set
can sometimes improve non-adversarial performance to natural perturbations, though this is not
necessarily guaranteed. Overall, our work opens a pathway for practitioners to learn machine learning
models that are robust to targeted, real-world perturbations that can be collected as data.
References
Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, and Kai-Wei
Chang. Generating natural language adversarial examples. arXiv preprint arXiv:1804.07998, 2018.
Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial
examples. arXiv preprint arXiv:1707.07397, 2017.
Anand Bhattad, Min Jin Chong, Kaizhao Liang, Bo Li, and David Forsyth. Unrestricted adversarial
examples via semantic manipulation. In International Conference on Learning Representations,
2020.
9
Published as a conference paper at ICLR 2021
Battista Biggio,Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic, Pavel Laskov, Giorgio
Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Joint European
conference on machine learning and knowledge discovery in databases, pp. 387-402. Springer,
2013.
Yulong Cao, Chaowei Xiao, Dawei Yang, Jing Fang, Ruigang Yang, Mingyan Liu, and Bo Li. Adver-
sarial objects against lidar-based autonomous driving systems. arXiv preprint arXiv:1907.05418,
2019.
Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras,
Ian Goodfellow, Aleksander Madry, and Alexey Kurakin. On evaluating adversarial robustness.
arXiv preprint arXiv:1902.06705, 2019.
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Unlabeled
data improves adversarial robustness. In Advances in Neural Information Processing Systems, pp.
11192-11203, 2019.
Jeremy M Cohen, Elan Rosenfeld, and J Zico Kolter. Certified adversarial robustness via randomized
smoothing. arXiv preprint arXiv:1902.02918, 2019.
Francesco Croce and Matthias Hein. Provable robustness against all adversarial Lp-perturbations for
p ≥ 1. arXiv preprint arXiv:1905.11213, 2019.
Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:
Learning augmentation strategies from data. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 113-123, 2019.
Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks
with cutout. arXiv preprint arXiv:1708.04552, 2017.
Logan Engstrom, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. A rotation and a
translation suffice: Fooling cnns with simple transformations. arXiv preprint arXiv:1712.02779, 1
(2):3, 2017.
Logan Engstrom, Andrew Ilyas, Hadi Salman, Shibani Santurkar, and Dimitris Tsipras. Robustness
(python library), 2019. URL https://github.com/MadryLab/robustness.
Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul
Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world attacks on deep learning
visual classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 1625-1634, 2018.
Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and
Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves
accuracy and robustness. arXiv preprint arXiv:1811.12231, 2018.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan
Uesato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval
bound propagation for training verifiably robust models. arXiv preprint arXiv:1810.12715, 2018.
Sven Gowal, Chongli Qin, Po-Sen Huang, Taylan Cemgil, Krishnamurthy Dvijotham, Timothy Mann,
and Pushmeet Kohli. Achieving robustness in the wild via adversarial mixing with disentangled
representations. arXiv preprint arXiv:1912.03192, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision, pp. 630-645. Springer, 2016.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common
corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019.
10
Published as a conference paper at ICLR 2021
Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshmi-
narayanan. Augmix: A simple data processing method to improve robustness and uncertainty.
arXiv preprint arXiv:1912.02781, 2019.
Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In
Advances in neural information processing systems, pp. 2017-2025, 2015.
Robin Jia, Aditi RaghUnathan, Kerem GokseL and Percy Liang. Certified robustness to adversarial
word substitutions. arXiv preprint arXiv:1909.00986, 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv
preprint arXiv:1611.01236, 2016.
Cassidy Laidlaw and Soheil Feizi. Functional adversarial attacks. In Advances in Neural Information
Processing Systems, pp. 10408-10418, 2019.
Juncheng Li, Shuhui Qu, Xinjian Li, Joseph Szurley, J Zico Kolter, and Florian Metze. Adversarial
music: Real world audio adversary against wake-word detection system. In Advances in Neural
Information Processing Systems, pp. 11908-11918, 2019a.
Juncheng B Li, Frank R Schmidt, and J Zico Kolter. Adversarial camera stickers: A physical camera
attack on deep learning classifier. arXiv preprint arXiv:1904.00759, 2019b.
Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment. In
Advances in Neural Information Processing Systems (NeurIPS), 2019.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Pratyush Maini, Eric Wong, and J Zico Kolter. Adversarial robustness against the union of multiple
perturbation models. arXiv preprint arXiv:1909.04068, 2019.
Claudio Michaelis, Benjamin Mitzkus, Robert Geirhos, Evgenia Rusak, Oliver Bringmann, Alexan-
der S Ecker, Matthias Bethge, and Wieland Brendel. Benchmarking robustness in object detection:
Autonomous driving when winter is coming. arXiv preprint arXiv:1907.07484, 2019.
Takeru Miyato, Andrew M Dai, and Ian Goodfellow. Adversarial training methods for semi-supervised
text classification. arXiv preprint arXiv:1605.07725, 2016.
Lukas Murmann, Michael Gharbi, Miika Aittala, and Fredo Durand. A dataset of multi-illumination
images in the wild. In Proceedings of the IEEE International Conference on Computer Vision, pp.
4080-4089, 2019.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial
examples. arXiv preprint arXiv:1801.09344, 2018.
Leslie Rice, Eric Wong, and J Zico Kolter. Overfitting in adversarially robust deep learning. arXiv
preprint arXiv:2002.11569, 2020.
Alexander Robey, Hamed Hassani, and George J Pappas. Model-based robust deep learning. arXiv
preprint arXiv:2005.10247, 2020.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical
image segmentation. In International Conference on Medical image computing and computer-
assisted intervention, pp. 234-241. Springer, 2015.
Evgenia Rusak, Lukas Schott, Roland Zimmermann, Julian Bitterwolf, Oliver Bringmann, Matthias
Bethge, and Wieland Brendel. Increasing the robustness of dnns against image corruptions by
playing the game of noise. arXiv preprint arXiv:2001.06057, 2020.
11
Published as a conference paper at ICLR 2021
Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust
neural networks for group shifts: On the importance of regularization for worst-case generalization.
arXiv preprint arXiv:1911.08731, 2019.
Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter. A general framework for
adversarial examples with objectives. ACM Transactions on Privacy and Security (TOPS), 22(3):
1-30, 2019.
Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness with
principled adversarial training. arXiv preprint arXiv:1710.10571, 2017.
Leslie N Smith. Cyclical learning rates for training neural networks. In 2017 IEEE Winter Conference
on Applications of Computer Vision (WACV), pp. 464-472. IEEE, 2017.
Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using
deep conditional generative models. In Advances in neural information processing systems, pp.
3483-3491, 2015.
Yang Song, Rui Shu, Nate Kushman, and Stefano Ermon. Constructing unrestricted adversarial
examples with generative models. In Advances in Neural Information Processing Systems, pp.
8312-8323, 2018.
David Stutz, Matthias Hein, and Bernt Schiele. Confidence-calibrated adversarial training and
detection: More robust models generalizing beyond the attack used during training. arXiv preprint
arXiv:1910.06259, 2019.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Florian Tramer and Dan Boneh. Adversarial training and robustness for multiple perturbations. In
Advances in Neural Information Processing Systems, pp. 5858-5868, 2019.
Eric Wong and J Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. arXiv preprint arXiv:1711.00851, 2017.
Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and J Zico Kolter. Scaling provable adversarial
defenses. In Advances in Neural Information Processing Systems, pp. 8400-8409, 2018.
Eric Wong, Frank R Schmidt, and J Zico Kolter. Wasserstein adversarial examples via projected
sinkhorn iterations. arXiv preprint arXiv:1902.07906, 2019.
Chaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, Mingyan Liu, and Dawn Song. Generating adversarial
examples with adversarial networks. arXiv preprint arXiv:1801.02610, 2018a.
Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, and Dawn Song. Spatially transformed
adversarial examples. arXiv preprint arXiv:1801.02612, 2018b.
Greg Yang, Tony Duan, Edward Hu, Hadi Salman, Ilya Razenshteyn, and Jerry Li. Randomized
smoothing of all shapes and sizes. arXiv preprint arXiv:2002.08118, 2020.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,
2016.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. arXiv preprint arXiv:1710.09412, 2017.
Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Duane Boning, and Cho-Jui Hsieh. Towards
stable and efficient training of verifiably robust neural networks. arXiv preprint arXiv:1906.06316,
2019.
12
Published as a conference paper at ICLR 2021
A Theoretical results
In this section, we present the theoretical results in their full detail and exposition. Both of the main
theorems presented in this work require a number of preceding results in order to formally link the
prior and the posterior distribution based on their KL divergence. We will present and prove these
supporting results before proving each main theorem.
A.1 Proof of Theorem 1
Theorem 1 connects the CVAE objective to the necessary subset property. In order to prove this, we
first prove three supporting lemmas. Lemma 1 states that if the expected value of a function over a
normal distribution is low, then for any fixed radius there must exist a point within the radius with
proportionally low function value. This leverages the fact that the majority of the probability mass is
concentrated around the mean and can be characterized by the Mahalanobis distance from the mean
of the normal distribution. Specifically, for a given 0 ≤ α ≤ 1, r is the Mahalanobis distance which
captures 1 - α of the probability mass ofNk(0, I) if kuk ≤r p(u)du = 1 - α.
Lemma 1. Let f (u) : Rk → R+ be a non-negative integrable function, and let Nk (0, I) be a
k-dimensional standard multivariate normal random variable with zero mean and identity covariance.
Suppose Eu〜N(o,i)[f (u)] ≤ δ for some δ > 0. Then,, for any pair (r, α) where r is the Mahalanobis
distance which captures 1 - α of the probability mass ofNk(0, I), there exists a u such that kuk2 ≤ r
and f (U) ≤ 1-δα.
Proof. We will prove this by contradiction. Assume for sake of contradiction that for all u such that
IluIl2 ≤ r, We have f (U) > ι-δα. We divide the expectation into two integrals over the inside and
outside of the Mahalanobis ball:
Eu〜N(0,I) [f (u)]= /	f (u)p(u)du + /	f (u)p(u)du
kuk2 ≤r	kuk2 >r
(7)
Using the assumption on the first integral and non-negativity of f in the second integrand, we can
conclude
Eu〜N(0,I) [f (u)] > /	1 δ p(u)du = δ
kuk2≤r 1 - α
(8)
where the equality holds by definition of the Mahalanobis distance, which contradicts the initial
assumption that Eu〜N(0,1)[f (u)] ≤ δ. Thus, we have proven by contradiction that there exists a u
such that ku∣2 ≤ r and f (u) ≤ #a.	□
Our second lemma, Lemma 2, is an important result that comes from the algebraic form of the KL
divergence. It is needed to connect the bound on the KL divergence to the actual variances of the
prior and the posterior distribution, and uses the LambertW function to do so.
Lemma 2. Let Wk be the LambertW function with branch k. Let x > 0, and suppose x - log x ≤ y.
Then, x ∈ [a, b] where a = -W0 (-e-y) and b = -W-1 (-e-y)}. Additionally, these bounds
coincide at x = 1 when y = 1.
Proof. The LambertW function Wk(y) is defined as the inverse function of y = xex, where the path
to multiple solutions is determined by the branch k. We can then write the inverse of y = x - logx
as one of the following two solutions:
x∈ {-W0(-e-y),-W-1(-e-y)}	(9)
Since x - logx is convex with minimum at x = 1, and since the two solutions surround x = 1,
the set of points which satisfy x - logx ≤ y is precisely the interval [-W0(-e-y), -W-1(-e-y)].
Evaluating the bound at y = 1 completes the proof.	□
Lemma 3 is the last lemma needed to prove Theorem 1, which explicitly bounds terms involving the
mean and variance of the prior and posterior distributions by their KL distance, leveraging Lemma 2
to bound the ratio of the variances. The two quantities bounded in this lemma will be used in the main
theorem to bound the distance of a point with low reconstruction error from the prior distribution.
13
Published as a conference paper at ICLR 2021
Lemma 3. Suppose the KL distance between two normals is bounded, so
KL(N (μι,σ2)∣∣N (μ2,σ2)) ≤ K for some constant K. Then,
(μι - M2)2-2 ≤ K
σ2
and also σσ2 ∈ [a, b] where
a = -W0(-e-(K+1)),
b= -W-1(-e-(K+1))
Proof. By definition of KL divergence, we have
—2 + (μι - M2)2—2 - 1 - log _22 ≤ K
σ2	σ2	σ2
(10)
Since X - log X ≥ 1 for X ≥ 0, We apply this for X = σ2 to prove the first bound on the squared
σ2
distance as
(μi - M2)2-2 ≤ K	(II)
σ1
Next, since (μ2 — μ2)2 去 ≥ 0, We can bound the remainder as the following
σ12	σ12
—2 - log —2 ≤ K + 1
σ22	σ22
(12)
σ2
and using Lemma 2, we can bound T ∈ [a, b] where
σ2
a= -W0(-e-(K+1)),	b= -W-1(-e-(K+1))
□
With these three results, we can now prove the first main theorem, which we is presented below in its
complete form, allowing us to formally tie the CVAE objective to the existence of a nearby point
with low reconstruction error.
Theorem 1.	Consider the likelihood lower bound for X ∈ Rn from the conditional VAE in k-
dimensional latent space conditioned on some other input y. Let the posterior, prior, and decoder
distributions be standard multivariate normals with diagonal covariance as follows
q(z|x,y) 〜N(〃(X,y),σ2(X,y)), P(ZIy) 〜N(μ(y),σ2(y)), P(X|z,y) 〜N(g(z,y),I)
resulting in the following likelihood lower bound:
logP(X|y) ≥ Eq(z|x,y)[log P(X|z, y)] - KL(q(z|X, y)kP(z|y))	(13)
Suppose we have trained the lower bound to some thresholds R, Ki
k
Eq(z|x,y)[log P(X|z, y)] ≥ R,	KL(q(z|X, y)kP(z|y)) ≤	Ki
i=1
where Ki bounds the KL-divergence of the ith dimension. Let r be the Mahalanobis distance which
captures 1 - α of the probability mass for a k-dimensional standard multivariate normal for some
0 < α < 1. Then, there exists a Z such that ∣∣ z-μ(? ∣∣2 ≤ E and ∣∣g(z, y) — X∣∣2 ≤ δ for
C = Br + χK Ki, δ = - ι - & (2R + m log(2π))
where B is a constant dependent on Ki. Moreover, as R → — 11 m log(2π) and Ki → 0 (the
theoretical limits of these bounds, e.g. by training), then C → r and δ → 0.
14
Published as a conference paper at ICLR 2021
Proof. The high level strategy for this proof will consist of two main steps. First, we will show
that there exists a point near the encoding distribution which has low reconstruction error, where
we leverage the Mahalanobis distance to capture nearby points with Lemma 1. Then, we apply the
triangle inequality to bound its distance under the prior encoding. Finally, we will use Lemma 3 to
bound remaining quantities relating the distances between the prior and encoding distributions to
complete the proof.
Let z(u) = u∙σ(x, y)+μ(x, y) be the parameterization trick of the encoding distribution. Rearranging
the log probability in the assumption and using the reparameterization trick, we get
Eu kx - g(z(u), y)k22 ≤ -2R - m log(2π) = δ(1 - α)	(14)
Applying Lemma 1, there must exist a u such that kuk ≤ r and kx - g(z(u), y)k22 ≤ δ, and so z(u)
satisfies the reconstruction criteria for δ. We will now show that z(u) fulfills the remaining criteria
for : calculating its `2 norm under the prior distribution and applying the triangle inequality, we get
Z(U) - μ(y)
σ(y)	2
U ∙ σ(x,y) + μ(x,y) - μ(y)
σ(y)	2
U ∙ σ(x,y)	+
σ(y)	2
μ(X,y) - μ5
σ(y)
2
(15)
We can use Lemma 3 on the KL assumption to bound the following quantities
(μi(y) - μi(x,y))2儿 ≤ Ki, σσxyy) ∈ [ai,bi]
(16)
where [ai, bi] are as defined from Lemma 3.
Let B = maxi √bi, so σj(Xyy) ≤ B for all i. Plugging this in along with the previous bounds We get
the following bound on the norm of z(U) before the prior reparameterization:
Z(U) 一 μ(y)
σ(y)
≤ Br +	Ki =
(17)
Thus, the norm (before the prior reparameterization) and reconstruction error of Z(U) can be bounded
by and δ.
To conclude the proof, we note that from Lemma 2, Ki → 0 for all i implies B → 1, and so → r.
Similarly by inspection, R → -2m log(2π) implies that δ → 0, which concludes the proof.
A.2 Proof of Theorem 2
In this section, we move on to prove the second main result of this paper, Theorem 2, which connects
the CVAE objective to the sufficient likelihood property. The proof for this theorem is not immediate,
because since the generator is an arbitrary function, two normal distributions which have a difference
in means have an exponentially growing ratio in their tail distributions, and so truncating the normal
distributions is crucial. This truncation is leveraged in Lemma 4, which bounds the ratio of two
normal distributions constrained within an `2 ball, and is what allows us to connect the expectation
over the prior with the expectation over the posterior.
Lemma 4. LetP 〜N(0,1) and q 〜N(-μ∕σ2,1∕σ2). Then,
q7z)1(lσz + μl ≤ r) ≤ h(r,μ,σ).	(18)
p(Z)
Furthermore if μ2 ≤ K and σ ∈ [a, b] ,then h(r, μ, σ) ≤ bemax(Cl,C2)where
Ci =	(b2	- 1)r2 - K,	C2 = ɪ	((1 -	a2)r2	+	2r√K +	K)).
Proof. The proof here is almost purely algebraic in nature. By definition of q and p, we have
q(Z) = σe- 1((σz+μ)2-z2) = σe ⅛((i-σ2)z2-2σμz-μ2)
p(Z)
(19)
15
Published as a conference paper at ICLR 2021
We will focus on bounding the exponent, (1 - σ2)z2 - 2σμz - μ2. We can bound this by considering
two cases. First, suppose that 1 - σ2 < 0 and so the exponent is a concave quadratic. Then, the
maximum value of the quadratic is at its root:
2(1 — σ2)z — 2σμ = 0 ⇒ Z =----巴
1 - σ2
Further assume that this z is within the interval of the indicator function, so
(20)
Γ2μ + μ∣ = ∣ι-M≤r.
Then, plugging in the maximum value into the quadratic results in the following bound f1 for this
case:
22
(1 -	σ2)z2-	2σμz - μ2	≤ -(1 -	σ2)(Γ-τψ - μ2	≤	(σ2-1)r2-	μ2	≡	fι(r,μ,σ)	(21)
Consider the other case, so either 1 - σ2 ≥ 0, or the optimal value for z* in the previous case when
1 - σ2 < 0 is not within the interval ∣σz + μ∣ ≤ r. Then, the maximum value of this quadratic must
occur at ∣σz + μ∣ = r, so Z = ±r-μ. Plugging this into the quadratic for positive r, this results in
(1 - σ2)z2 - 2σμz - μ2∣σz+μ=r =后((I - σ^r2 - 2rμ + μ2)	(22)
and for negative r , this is
(I - σ2)z2	-	2σμz - μ2	lσz+μ=-r	=	σ12	((1 - σ2)r2+	2rμ	+ μ2)	(23)
and so we can bound this case with the following function f2 :
(1 - σ2)z2	-	2σμz - μ2	≤	J2	((1 - σ2)r2 + 2∣rμ∣	+	μ2)	≡ f2(r,μ,σ)	(24)
Plugging in the maximum over both cases forms our final bound on the ratio of distributions.
q(z)1(∣σz + μ∣ ≤ r) ≤ σemax(f1 (r,μ,σ),f2(r,μ,σ)) = h(r,μ,σ)	(25)
p(Z)
To finish the proof, assume we have the corresponding bounds on μ and σ. Then, the first case can be
bounded with C1 defined as
fι(r,μ,σ) ≤ (b2 - 1)r2 - K = Ci
The second case can be bounded with C2 defined as
f2(r,μ,σ) ≤ J ((1 - a2)r2 + 2r√K + K)) = C2
And thus we can bound h(r, μ, σ) as
h(r,μ,σ) ≤ bemax(Cl，C2)= H
□
Lemma 4 can be directly applied to the setting of Theorem 2 for the case of a non-conditional VAE
with a standard normal distribution for the prior. However, since we are using conditional VAEs
instead, the prior distribution has its own mean and variance and so the posterior distribution needs to
be unparameterized from the posterior and reparameterized with the prior. Corollary 1 establishes
this formally, extending Lemma 4 to the conditional setting.
Corollary 1. LetP 〜N(0,1) and q 〜N (μ2~μ1, σ∣). Then,
q(Z)	( σiz + μι - μ2『、/* ʌ	SG
T^1 ----------- ≤ r ≤ h(r,μ,σ)	(26)
p(Z)	∣	σ2	∣
where μ = σ1- (μι — μ2) and σ = 公.Furthermore, if KL(N (μι, σ2 )∣∣N (μ2, σ2)) ≤ K for some
constant K, then h(r, μ, σ) ≤ H where H is a constant which depends on K.
16
Published as a conference paper at ICLR 2021
Proof. First, note that Lemma 3 implies that μ2 ≤ K and σ ∈ [a, b]. Observe that q 〜N(- μ,表)
and σ1z+μ1-μ2 = Qz + μ, and so the proof reduces to an application of Lemma 4 to this particular μ
and σ.	□
These results allow us to bound the ratio of the prior and posterior distributions within a fixed radius,
which will allow us to bound the expectation over the prior with the expectation over the posterior.
We can now finally prove Theorem 2, presented in its complete form below.
Theorem 2.	Consider the likelihood lower bound for x ∈ Rn from the conditional VAE in k-
dimensional latent space conditioned on some other input y. Let the posterior, prior, and decoder
distributions be standard multivariate normals with diagonal covariance as follows
q(zlx,y) 〜N(μ(x,y),σ2(χ,y)), P(ZIy)〜N(μ(y),σ2(y)), P(XEy)〜N(g(z,y),I)
resulting in the following likelihood lower bound:
logP(x|y) ≥ Eq(z|x,y)[log P(x|z, y)] - KL(q(z|x, y)kP(z|y))	(27)
Suppose we have trained the lower bound to some thresholds R, Ki
k
Eq(z|x,y)[log P(x|z, y)] ≥ R,	KL(q(z|x, y)kP(z|y)) ≤	Ki
i=1
where Ki bounds the KL-divergence of the ith dimension. Let r be the Mahalanobis distance which
captures 1 - α of the probability mass for a k-dimensional standard multivariate normal for some
0 < α < 1. Then, the truncated expected reconstruction error can be bounded with
Epr (u) [kg(u ∙ σ(y) + μ(y),y) - χk2] ≤-占(2R + mlog(2π))H
where Pr (u) is a multivariate normal that has been truncated to radius r and H is a constant that
depends exponentially on Ki.
Proof. The overall strategy for this proof will be as follows. First, we will rewrite the expectation
over a truncated normal into an expectation over a standard normal, using an indicator function to
control the radius. Second, we will do an “unparameterization” of the standard normal to match the
parameterized objective in the assumption. Finally, we will bound the ratio of the unparameterized
density over the normal prior, which allows us to bound the expectation over the prior with the
assumption. This last step to bound the ratio of densities is made possible by the truncation, which
would otherwise grow exponentially with the tails of the distribution.
For notational simplicity, let f (∙) = ∣∣g(∙, y) - x∣∣2. The quantity We wish to bound can be rewritten
using the Mahalanobis distance as
EPa [f(U ∙ σ(y) + μ(y))]
1-α / f(U ∙ σ(y) + μ(y))ι(kuk ≤ r)p(u)dz
(28)
where we used the fact that Pr (U) = ɪ-a 1(∣u∣ ≤ r)p(u), which simply rewrites the density of a
truncated normal using a scaled standard normal density and an indicator function. We can do a
parameterization trick z = U ∙ σ(y) + μ(y) to rewrite this as
EPr[f (U	∙	σ(y)	+ 〃(y))] =	ι-^α ZZf (Z)Inl z	Q(；)y)	≤ r)pζ∣y(Z)dz	(29)
followed by a reverse parameterization trick with V = z-μ(χ,y) to get the following equivalent
expression
1-α / f(v	∙	σ(x,y)	+	μ(X,y))1 QV，σ(x,y)	；(：,y)―μ(y)	≤	r)	Pv(V)dz	(30)
wherePv 〜N (μ(X/溜：用),您：；)).For convenience, we can let
Pv (V) = 1 (| V F(XC μ(y l≤ r) Pv (V)
17
Published as a conference paper at ICLR 2021
which can be interpreted as a truncated version of pv , and so the expectation can be represented more
succinctly as
Epr [f (u ∙ σ(y) + μ(y))]
ι-1α If(V ∙ σ(x,y)
+ μ(x,y))pv (v)dz.
(31)
We will now bound Pv (V) with the standard normal distribution p(v) so that We can apply our
assumption. Because f is non-negative, the '2 constraint in Pv (V) can be relaxed to an element-wise
'∞ constraint to get
k
Pv(V) ≤ p(v) Y 1
i=1
V ∙ σ(x,y) + μ(x,y) - μ(y)
σ(y)
Pv(Vi
≤r5
(32)
where we also used the fact that P is a diagonal normal, so Qik=1 P(Vi) = P(V). Each term in this
product can be bounded by Lemma 4 to get
k
Pv(v) ≤ p(v) Y Hi= p(v) ∙ H	(33)
i=1
where Hi is as defined in Corollary 1 for each i using the corresponding Ki, and we let H = Qik=1 Hi.
Thus we can now bound the truncated expected value by plugging in our bound for Pv (v) into Equation
(31) to get
H
EPr [f (U ∙ σ(y) + μ(y))] ≤ γ--α ∙ Eq(z∣χ,y) [f (z)]	(34)
Using our lower bound on the expected log likelihood from the assumption, we can bound the
remaining integral with
Eq(z|x,y) [f(z)] ≤ -2R - m log(2π)	(35)
and so combining this into our previous bound from Equation (34) results in the final bound
Epr [f(u)] ≤ -γ^-(2R + mlog(2∏))H	(36)
r	1-α
□
Finally, while Theorem 2 implies a bound on the the expected reconstruction error, the sufficient
likelihood property is defined with respect to the likelihood. For a multivariate Gaussian, reconstruc-
tion error and likelihood are closely linked, and so the upper bound on reconstruction error directly
implies a lower bound on the likelihood as described in Corollary 2
Corollary 2. Suppose the truncated reconstruction error ofa perturbed pair (x, x) is upper bounded
by U:
EPr (U) [IE" ∙ σ(x) + W(X), X)- Xk2] ≤ U	(37)
Then, the likelihood of X conditioned of X is lower bounded by L:
EPr (U) [Pμ,σ (X)] ≥ L	(38)
where L = exp (—mm log(2π) — U2).
Proof. The results follows by first applying Jensen’s inequality and expanding the density function
EPr (u) [Pμ,σ (X)] ≥ exp (EPr (u) [logPμ,σ (X)])	(39)
m1 =exp ( -^2 log(2∏) - 2EPr(u) [kg(u ∙ σ(X) + 〃(x), x) - x∣∣2])	(40)
mU ≥ eχp I-y log(2π) - 2 )	(41)
(42)
□
18
Published as a conference paper at ICLR 2021
δα(z)
Figure 4: A simple example demonstrating how the expected value of a function can tend to zero
while the maximum tends to infinity as a → ∞.
A.3 Discussion of theoretical results
We first note that the bound on the expected reconstruction error in Theorem 2 is larger than the
corresponding bound from Theorem 1 by an exponential factor. This gap is to some extent desired,
since Theorem 1 characterizes the existence of a highly accurate approximation whereas Theorem
2 characterizes the average case, and so if this gap were too small, the average case of allowable
perturbations would be constrained in reconstruction error, which is not necessarily desired in all
settings.
This relates to the overapproximation error, or the maximum reconstruction error within the per-
turbation set. Initially one may think that having low overapproximation error to be a desirable
property of perturbation sets, in order to constrain the perturbation set from deviating by “too much.”
However, perturbation sets such as the rotation-translation-skew perturbations for MNIST will always
have high overapproximation error, and so reducing this is not necessary desired. Furthermore, it
not theoretically guaranteed for the CVAE to minimize the overapproximation error without further
assumptions. This is because it is possible for a function to be arbitrarily small in expectation but
be arbitrarily high at some nearby point, so optimizing the CVAE objective does not imply low
overapproximation error. A simple concrete example demonstrating this is the following piecewise
linear function
(aɜz + a for 一 a2 ≤ z < 0
—a3z + a for 0 ≤ z < O2
0	otherwise
for a > 0 as seen in Figure 4. Note that max∣z∣≤e δa(z') = a which is unbounded as a → ∞
for all e ≥ 0, while at the same time EN(o,i)[δa(z)] ≤ ɪ → 0. Thus, explicitly minimizing
overapproximation error is not necessarily desired nor is it guaranteed by the CVAE. Nonetheless, we
report it anyways to give the reader a sense of how much the CVAE can deviate within the learned
perturbation set.
B	Evaluating a perturbation set
In this section we describe in more detail the process of selecting a radius for a learned perturbation
set and formally describe all the details regarding the evaluation metrics for a perturbation set learned
with a CVAE. As a reminder, let μ(x, x),σ(x, X) be the posterior encoding, μ(x), σ(x) be the prior
encoding, and g(x, z) be the decoder for the CVAE. We will use u to represent the latent space before
the parameterization trick. Then the perturbation set defined by our generative process is
S(x) = {g(z, x) : Z = u ∙ σ(x) + μ(x), ∣∣uk ≤ e}
and the probabilistic perturbation set is defined by the truncated normal distribution N before the
parameterization trick as follows,
S(x)〜NOrmal(g(z, x), I), Z = U ∙ σ(x) + μ(x), U 〜M(0, I)
19
Published as a conference paper at ICLR 2021
Selecting a radius To select a radius, we take the following conservative estimate calculated on a
held-out validation set, which computes the smallest `2 ball under the prior which contains all the
mean encodings under the posterior:
= max
i
μ(xi,Xi) - μ(xi)
σ(xi)
2
The benefits of such a selection is that by taking the maximum, we are selecting a learned perturbation
set that includes every point with approximation error as low as the posterior encoder. To some extent,
this will also capture additional types of perturbations beyond the perturbed data, which can be both
beneficial and unwanted depending on what is captured. However, in the context of adversarial
attacks, using a perturbation set which is too large is generally more desirable than one which is too
small, in order to not underspecify the threat model.
Evaluation metrics We present each evaluation metric in detail for a single perturbed pair (x, X).
These evaluation metrics can then be averaged over the test dataset to produce the evaluations
presented in this paper.
1.	Encoder approximation error (Enc. AE) We can get a fast upper bound of approximation
error by taking the posterior mean, unparameterizing it with respect to the prior, and
projecting it to the ball as follows:
Enc. AE(x, X) ≡ ∣∣X — g(z, x)k2
where Z = U ∙ σ(x) + μ(x),
U = PrOje ("(X，：；— μ(X))
2.	PGD approximation error (PGD AE) We can refine the upper bound by solving the
following problem with projected gradient descent:
PGDAE(X, X) ≡ min ∣∣X — g(z,x)∣2
kuk≤e
where Z = U ∙ σ(∕) + μ(∕)
In practice, we implement this by warm starting the procedure with the solution from the
encoder approximation error, and run 50 iterations of PGD at step size /20.
3.	Expected approximation error (EAE) We can compute this by drawing N samples Ui 〜
Ne(0, I, ) and calculating the following Monte Carlo estimate:
1N
EAE(x,X) ≡ N EkX - g(zi, X)k2
i=1
where Zi = Ui ∙ σ(∕) + μ(∕)
In practice, we find that N = 5 is sufficient for reporting means over the dataset with
near-zero standard deviation.
4.	Over approximation error (OAE) We can compute this by performing a typical `2 PGD
adversarial attack:
OAE(X,X) ≡ max ∣∣X — g(z,x)∣2
kuk≤e
where Z = U ∙ σ(X) + μ(∕)
In practice, we implement this by doing a random initialization and run 50 iterations of PGD
at step size /20.
5.	Reconstruction error (Recon. err) This is the typical reconstruction error of a variational
autoencoder, which is a Monte Carlo estimate over the full posterior with one sample:
Recon. err(X, X) ≡ -^— — g(z, x)∣2
m
where Z = U ∙ σ(X, X) + μ(X, X)
U 〜N(0,I)
20
Published as a conference paper at ICLR 2021
Note that we report the average over all pixels to be consistent with the other metrics in
this paper, however it is typical to implement this during training as a sum of squared error
instead of a mean.
6.	KL divergence (KL) This is the standard KL divergence between the posterior and the
prior distributions
KL(x,X) ≡ KL(N(μ(x, X), σ(x, X)∣∣N(μ(x),σ(x)))
C Adversarial training, randomized smoothing, and data
AUGMENTATION WITH LEARNED PERTURBATION SETS
In this section, we describe how various standard, successful techniques in robust training can applied
to learned perturbation sets. We note that each method is virtually unchanged, with the only difference
being the application of the method to the latent space of the generator instead of directly in the input
space.
Algorithm 1 Given a dataset D, perform an epoch of adversarial training with a learned perturbation
set given by a generator g and a radius with step size γ, using a PGD adversary with T steps and
step size α
for (x, y) ∈ D do
δ := 0 // Initialize perturbation
for t ∈ T do
g := Vδ'(h(g(z + δ, x)), y) //Calculate gradient
δ := δ + α ∙ g∕∣∣g∣∣2 //Take a gradient step
if kδ k2 > then
δ := e ∙ δ∕kδ∣∣2 //Project onto E ball
θ := θ - γ Vθ `(h(g (z + δ, x)), y) // Optimize model weights
C.1 Adversarial training
Adversarial training, or training on adversarial examples generated by an adversary, is a leading
empirical defenses for learning models which are robust to adversarial attacks. The adversarial attack
is typically generated with a PGD adversary and is used in the context of `p adversarial examples.
We follow closely the adversarial training formulation from (Madry et al., 2017). In order to do
adversarial training on learned perturbation sets, it suffices to simply run the PGD adversarial attack
in the latent space of the generator, as shown in Algorithm 1.
C.2 Randomized smoothing
Randomized smoothing is a method for learning models robust to adversarial examples which come
with certificates that can prove (with high probability) the non-existance of adversarial examples.
We follow closely the randomized smoothing procedure from (Cohen et al., 2019), which trains and
certifies a network by augmented the inputs with large Gaussian noise. In order to do randomized
smoothing on the learned perturbation sets, it suffices to simply train, predict, and certify with
augmented noise in the latent space of the generator, as shown in Algorithm 2 for prediction and
certification.
The algorithms are almost identical to that propose by Cohen et al. (2019), with the exception that
the noise is passed to the generator before going through the classifier. Note that LowerConfi-
DENCEBOUND(k, n, 1 - α) returns a one-sided 1 - α confidence interval for the Binomial parameter
P given a sample k 〜BINOMIAL(n,p), and BINOMPVALUE(a, a + b,p) returns the P value of a
two-sided hypothesis test that a 〜BINOMIAL(a + b,p).
On the meaning of “certified” robustness to a learned perturbation set Note that the training
and certification procedure with randomized smoothing operates solely on the learned perturbation
set. Strictly speaking, this implies that the meaning of this certification is only as good as the quality
of the learned perturbation set.
21
Published as a conference paper at ICLR 2021
Algorithm 2 Given a datapoint x, pseudocode for certification and prediction for a classifier which
has been smoothed over a learned perturbation set given by a generator g and a radius using a noise
level σ with probability at least 1 - α.
function SAMPLEUNDERNOISE(h, g, x, n, σ)
ci := 0 for i ∈ [k] // Initialize counts
for t ∈ [n] do
// Draw random samples from the generator and count classes for prediction
Z := N(0,σ2)
j := arg maxi h(g(z, x))
cj := cj + 1
return c
function PREDICT(h, g, x, n, σ, α)
c := SAMPLEUNDERNOISE(h, g, x, n, σ)
i, j := top two indices in c
if BINOMPVALUE(ci, ci + cj, 0.5) ≤ α then
return prediction i
else
return ABSTAIN
function CERTIFY(h, g, x, n0 , n, σ, α)
c0 := SAMPLEUNDERNOISE(h, g, x, n0, σ)
k := arg maxi(c0)i
c := SAMPLEUNDERNOISE(h, g, x, n, σ)
pa := LOWERCONFIDENCEBOUND(c0k , n, 1 - α)
if pa > 0.5 then
return prediction k with radius σΨ-1(pa)
else
return ABSTAIN
22
Published as a conference paper at ICLR 2021
For example, if a learned perturbation set contains exactly the real perturbed examples, then certified
accuracy according to this perturbation set is precisely an upper bound on the worst case adversarially
perturbed example. In practice, the generative model produces inexact approximations, and so
this is an upper bound on the performance to the synthetically generated perturbations. As the
generative model becomes more accurate, the certified accuracy becomes more reflective of real
world performance. Careful evaluation of these generative models is necessary if learned perturbation
sets are to be used as proxies for measuring performance in the real world.
D MNIST
In this section, we discuss some benchmark tasks on MNIST, where the perturbation set is
mathematically-defined. Since an exact perturbation set will always be better than a learned ap-
proximation, the goal here is simply to explore the data and architectural requirements for learning
accurate perturbations sets with CVAEs, and establish a baseline where the ground truth is known.
We consider learning the classic '∞ perturbation and rotation-translation-skew (RTS) perturbations
(Jaderberg et al., 2015).
We use the standard MNIST dataset consisting of 60,000 training examples with 1,000 examples
randomly set aside for validation purposes, and 10,000 examples in the test set. These experiments
were run on a single GeForce RTX 2080 Ti graphics card, with the longest perturbation set taking 1
hour to train.
D.1 Experimental details
'∞ details For the '∞ setting, perturbations with radius E = 0.3. Our fully connected network
encoders have one hidden layer with 784 hidden units, then two linear modules with 784 outputs each
for computing the mean and log variance of the latent space distributions. The generator network has
the same structure, one hidden layer with 784 hidden units. The posterior and generator networks
simply concatenate their inputs into a single vector.
The encoders for the convolutional network for the '∞ setting uses two hidden convolutional layers
with 32 and 64 channels, each followed by ReLU and max pooling. To generate the mean and log
variance, there are two final convolution with 128 channels. The posterior network concatenates its
inputs via the channel dimension. The generator has two analogous hidden layers using convolutions
with 64 and 32 channels followed by ReLU and nearest neighbor upsampling. One final convolution
reduces the number of channels to 1, and the conditioned input x is downsampled with 1 × 1
convolutions and concatenated with the feature map before each convolution. All convolutions other
than the downsampling convolutions use a 3 × 3 kernel with one padding.
Both networks are trained for 20 epochs, with step size following a piece-wise linear schedule of
[0, 0.001, 0.0005, 0.0001] over epochs [0, 10, 15, 20] with the Adam optimizer using batch size 128.
The KL divergence is weighted by β which follows a piece-wise linear schedule from [0, 0.001, 0.01]
over epochs [0, 5, 20].
RTS details For the RTS setting, we follow the formulation studied by Jaderberg et al. (2015),
which consists of a random rotation between [-45, 45] degrees, random scaling factor between
[0.7, 1.3], and random placement in a 42 × 42 image. The architecture uses a typical pre-activation
convolution block, which consists of 2 convolutions with reflection padding which are pre-activated
with batchnorm and ReLU. The encoders uses a single block followed by two linear modules to 128
units to generate the mean and log variance, where the posterior network concatenates its inputs via
the channel dimension. The generator has a linear layer to map the latent vector back to a 42 × 42
feature map, followed by five parallel spatial transformers which use the latent vector to transform the
conditioned input. Multiple spatial transformers here are used simply because training just one can
be inconsistent. The outputs of the spatial transformers are concatenated via the channel dimension,
followed by a convolutional block and one final convolution to reduce it back to one channel.
All networks are trained for 100 epochs with cyclic learning rate peaking at 0.0008 on the 40th epoch
with the Adam optimizer using batch size 128. The KL divergence is weighted by β which follows a
piece-wise linear schedule from [0, 0.01, 1, 1] over epochs [0, 10, 50, 100].
23
Published as a conference paper at ICLR 2021
Figure 5: Samples of '∞ (top) and RTS (bottom) perturbations for MNIST from the convolutional
'∞ and RTS-1 models.
Table 4: Evaluation of a perturbation set learned from MNIST examples perturbed by '∞ noise with
= 0.3 and RTS perturbations. The standard deviation is less than 0.01 for all metrics except KL
divergence.
Test set quality metrics	Test set CVAE metrics
Model		Enc. AE	PGD AE	EAE	OAE	Recon. err	KL
Fully connected '∞	28	0.31	0.25	0.32	0.65	0.27	585.5 ± 0.29
Convolutional '∞	29	0.30	0.27	0.32	0.35	0.27	610.3 ± 0.10
RTS	14	0.29	0.11	0.54	1.03	0.04	22.2 ± 0.02
RTS-1	14	0.28	0.10	0.61	1.34	0.05	22.2 ± 0.03
RTS-5	14	0.28	0.10	0.53	0.86	0.05	23.7 ± 0.02
D.2 Samples and visualizations
Figure 5 plots samples from the convolutional model producing '∞ perturbations and the RTS model
trained with only one perturbed datapoint per example. We note that the final β value for the '∞
perturbation was tuned to produce reasonable samples in order to balance the KL divergence and
reconstruction error, likely due to the inherent difficulty of this setting.
D.3 Evaluating the perturbation set
Table 4 contains the full tabulated results evaluating the MNIST perturbation sets. The KL divergence
for the '∞ perturbation sets are much higher due to the KL divergence being weighted by 0.01 in order
to produce reasonable samples. We note that the convolutional network and fully connected network
evaluate to about the same, except that the convolutional network has a much lower overapproximation
error and is likely restraining the size of the learned perturbation set.
The RTS-1 results demonstrate how fixing the number of perturbations seen during training to one per
datapoint is still enough to learn a perturbation set with as much approximation error as a perturbation
set with an infinite number of samples, denoted RTS. Increasing the number of perturbations to five
per datapoint allows the remaining metrics to match the RTS model, and so this suggests that not
many samples are needed to learn a perturbation set in this setting.
E CIFAR10 common corruptions
In this section, we describe the CIFAR10 common corruptions setting and experiments in greater
detail. The dataset comes with 15 common corruptions covering noise, blurs, weather, and digital
24
Published as a conference paper at ICLR 2021
Table 5: Prior and encoder architecture for learning CIFAR10 common corruptions		Table 6: Decoder architecture for learning CIFAR10 common corruptions
Input(3 × 32 × 32)	Input(k × 32 × 32)	InPut(500)
Concat(3, k)		FC(1 × 32 × 32) InPut(3 × 32 × 32)
Conv(64)		Concat(1, 3)
4 X Residual(64)		Conv(64)
Conv(16)		4 × Residual(64)
FC(512)	FC(512)	Conv(3)
corruptions. We omit the three noise corruptions due to similarity to unstructured `p noise, leaving us
with the following 12 corruptions for each example:
1.	Blurs: defocus blur, glass blur, motion blur, zoom blur
2.	Weather: snow, frost, fog
3.	Digital: brightness, contrast, elastic, pixelate, jpeg
The dataset comes with different corruption levels, so we focus on the highest severity, corruption
level five, so the total training set has 600,000 perturbations (12 for each of 50,000 examples) and the
test set has 120,000 perturbations. The dataset also comes with several additional corruptions meant
to be used as a validation set, however for our purposes will serve as a way to measure performance
on out-of-distribution corruptions. These are Gaussian blur, spatter, and saturate which correspond to
the blur, weather, and digital categories respectively. We generate a validation set from the training set
by randomly setting aside 1/50 of the CIFAR10 training set and all of their corresponding corrupted
variants. These experiments were run on a single GeForce RTX 2080 Ti graphics card, taking 22
hours to train the CVAE and 28 hours to run adversarial training.
E.1 Perturbation model architecture and training specifics
We use standard preactivation residual blocks (He et al., 2016), with a convolutional bottleneck which
reduces the number of channels rather than downsampling the feature space. This aids in learning to
produce CIFAR10 common corruptions since the corruptions are reflective of local rather than global
changes, and so there is not so much benefit from compressing the perturbation information into a
smaller feature map. Specifically, our residual blocks uses convolutions that go from 64 → 16 → 64
channels denoted as Residual(64). Then, our encoder and prior networks are as shown in Table 5,
where k = 0 for the prior network and k = 3 for the encoder network, and the decoder network is as
shown in Table 6.
Stabilizing the exponential function Note that the CVAE encoders output the log variance of the
prior and posterior distributions, which need to be exponentiated in order to calculate the KL distance.
This runs the risk of of numerical overflow and exploding gradients, which can suddenly cause normal
training to fail. To stabilize the training procedure, it is sufficient to use a scaled Tanh activation
function before predicting the log variance, which is a Tanh activation which has been scaled to
output a log variance betwen [ln(10-3), ln(10)]. This has the effect of preventing the variance from
being outside the range of [10-3, 10], and stops overflow from happening. In practice, the prior and
posterior converge to a variance within this range, and so this doesn’t seem to adversely effect the
resulting representative power of the CVAE while stabilizing the exponential calculation.
Training is done for 1000 epochs using a cyclic learning rate (Smith, 2017), peaking at 0.001 on the
400th epoch using the Adam optimizer (Kingma & Ba, 2014) with momentum 0.9 and batch size
128. The hyperparameter β is also scheduled to increase linearly from 0 to 10-2 over the first 400
epochs. We use standard CIFAR10 data augmentation with random cropping and flipping.
25
Published as a conference paper at ICLR 2021
Table 7: Measuring and comparing quality metrics for a CVAE perturbation set trained on CIFAR10
common corruptions with different pairing strategies depending on the available information.
Method	Test set quality metrics	Test set CVAE metrics E Enc. AE PGD AE EAE OAE Recon. err	KL
Centered at original Original + perturbed Perturbed only	28	0.005	0.006	0.029	0.17	5.73	∙	10-4	69.31 34	0.019	0.010	0.035	0.20	8.33	∙	10-4	160.60 28	0.019	0.007	0.038	0.17	8.53	∙	10-4	182.85
Table 8: CIFAR10 common corruptions validation set statistics for the `2 norm of the latent space
encodings.
Method	β	Mean	Std	25%	50%	75%	Max
Centered at original	0.01	6.53	5.22	2.69	3.85	10.16	26.61
Original + perturbed	0.01	11.50	4.52	7.81	10.95	14.48	31.30
Perturbed only	0.01	11.04	3.81	7.93	10.82	13.46	29.57
E.2 Pairing strategies
Here, we quantitatively evaluate the effect of the different pairing strategies for the CIFAR10 common
corruptions setting when training a perturbation set with a CVAE in Table 7. We first note that the
approach using random pairs of both original and perturbed data has a larger radius than the others,
likely since the generator needs to produce not just the perturbed data but also the original as well.
We next see that all metrics across the board are substantially lower for the approach which centers
the prior by always conditioning on the unperturbed example, and so if such an unperturbed example
is known, we recommend conditioning on said example to learn a better perturbation set.
E.3 Properties of the CVAE latent space
We next study closely the properties of the underlying latent space which has been trained to generate
CIFAR10 common corruptions. We summarize a number of statistics in Table 8, namely the mean,
standard deviation, and some percentile thresholds where we can clearly see that the latent vectors
for the CVAE centered around the original are much smaller in `2 norm and thus learns the most
compact perturbation set. We remind the reader that maximum `2 norm reported here is calculated on
the test set and is not necessarily the same as the radius reported in Table 7 since the radius is selected
based on a validation set.
Distribution of `2 distances in latent space We plot the distribution of the `2 norm of the latent
space encodings in Figure 6, and further find the centered CVAE has semantic latent structure.
Specifically, corruptions can be ordered by their `2 norm in the latent space where each corruption
inhabits a particular range of distance, so reducing the radius directly constrains the type of corruptions
in the perturbation set. For example, restricting the perturbation set to radius = 5 corresponds to
leaving out frost fog, snow, elastic, and glass blur corruptions, while containing all defocus blur, jpeg
compression, and motion blur corruptions. A larger version of Figure 6a with a complete legend
listing all the corruptions is in Figure 7.
Additional samples and interpolations from the CVAE To further illustrate what corruptions
are represented in the CVAE latent space, we plot additional samples and interpolations from the
CVAE. In Figure 8 we draw four random samples for eight different examples, which show a
range of corruptions and provide qualitative evidence that the perturbation set generates reasonable
samples. In Figure 9 we see a number of examples being interpolated between weather, blur, and
digital corruptions, which also demonstrate how the perturbation set not only includes the corrupted
datapoints, but also variations and interpolations in between corruptions.
26
Published as a conference paper at ICLR 2021
(a) CVAE centered at original
Figure 6: Distribution of `2 norms for latent encodings of CIFAR10 common corruptions on the test
set for each pairing strategy.
(b) CVAE original + perturbed
(c) CVAE perturbed only
2.5 -
2.0 -
1.5 -
1.0 -
0.5 -
0.0 -
0	5	10	15	20	25	30
Figure 7: A larger version of the density of the `2 norms of latent encodings of CIFAR10 common
corruptions, broken down by type of corruption.
E.4 Adversarial training
For learning a robust CIFAR10 classifier, we use a standard wide residual network (Zagoruyko &
Komodakis, 2016) with depth 28 and width factor 10. All models are trained with the Adam optimizer
with momentum 0.9 for 100 epochs with batch size 128 and cyclic learning rate schedule which peaks
at 0.2 at epoch 40. This cyclic learning rate was tuned to optimize the validation performance for the
data augmentation baseline, and kept fixed as-is for all other approaches. We use validation-based
early stopping for all approaches to combat robust overfitting (Rice et al., 2020). However, we find
that in this setting and also likely due to the cyclic learning rate, we do not observe much overfitting.
Consequently, the final model at the end of training ends up being selected for all methods regardless.
Adversarial training is done with a PGD adversary at the full radius of = 28 with step size /5 = 5.6
and 7 iterations. At evaluation time, for a given radius we use a PGD adversary with 50 steps of
size /20. Additional examples of adversarial attacks under this adversary are in Figure 10, which
still appear to be reasonable corruptions despite being adversarial.
27
Published as a conference paper at ICLR 2021
Figure 8: Random perturbations from the CVAE prior, showing four random samples for each
example.
Figure 9: Interpolations between fog (left), defocus blur (middle), and pixelate (right) corruptions as
representative distinct types of corruptions from the weather, blur, and digital corruption categories.
E.5 Comparisons to other baselines
One may be curious as to how these results compare to other methods for learning models which
are robust to different threat models. In this section, we elaborate more on the comparison to other
baselines. We preface this discussion with the following disclaimers:
1.	It not necessarily expected for robustness to one threat model to generalize to another threat
model.
2.	It is expected that training against a given threat model gives the best results to that threat
model, in comparison to training against a different threat model.
3.	These alternative baselines have different data assumptions (no access to corrupted data)
or solve for a different type of robustness than what is being considered in this paper (`p
robustness)
Consequently, the fact that these baselines perform universally worse than the CVAE approaches is
unsurprising and confirms our expectations. Nonetheless, we provide this comparison to satiate the
readers curiosity.
As alternative baselines, we compared to standard training, methods in general data augmentation
(AugMix), '2 adversarial robustness, and '∞ adversarial robustness for CIFAR10. For '2 and
'∞ adversarially robust models, We download pretrained state-of-the-art models ranked by the
AutoAttack3 leaderboard. For AugMix, there is no released CIFAR10 model, so we retrained an
3https://github.com/fra31/auto-attack
28
Published as a conference paper at ICLR 2021
Figure 10: Adversarial examples that cause misclassification for an adversarially trained classifier.
Table 9: Certified robustness to CIFAR10 common corruptions with a CVAE perturbation set.
Noise level		Test set accuracy (%)			Test set certified accuracy (%)			
		Clean Perturbed		OOD	e=2.7	e = 3.9	e	10.2
σ	0.84	93.8	92.5	84.2	34.7	00.0		00.0
σ	1.22	94.0	92.6	83.9	66.1	15.3		00.0
σ	3.19	86.7	84.7	65.0	86.7	39.4		00.0
AugMix model using the official AugMix implementation4 using the parameters recommended by the
authors. We note that the AugMix results for CIFAR10 in this paper are worse than what is reported
by Hendrycks et al. (2019), which is a known issue in the repository that others have encountered
(see https://github.com/google-research/augmix/issues/15).
E.6 Randomized smoothing
Randomized smoothing is done in its most basic form, with normal Gaussian data augmentation
at the specified noise level σ. We predict labels with n0 = 100 samples and do certification with
n = 10, 000 samples at confidence α = 0.001, so each example has a 0.1% chance of being
incorrectly certified. Unlike in the typical setting, we have a known maximum radius for the
perturbation set. This allows us to backwards engineer a noise level to have a specific maximum
certified radius as follows: let ` be the maximum possible lower bound obtained from a confidence
interval at level 1 - α with n classifications that are in unanimous agreement. Then, the maximum
certifiable radius is r = σΦ-1(') where Φ-1 is the inverse cumulative distribution function of a
standard normal distribution. By setting r = e,we can solve for the desired noise level σ = ①一：⑷ to
naturally select a noise level with maximum certified radius . We do this for the ∈ {2.7, 3.9, 10.2}
which are the 25th, 50th, and 75th percentiles of the perturbation set, and get σ ∈ {0.84, 1.22, 3.19}
respectively.
The results of randomized smoothing at these thresholds are presented in Table 9. Note that by
construction, each noise level cannot certify a radius larger than what is calculated for. We find that
clean accuracy isn’t significantly harmed when smoothing at the lower noise levels, and actually
improves perturbed accuracy over the adversarial training approach from Table 2 but with lower
out-of-distribution accuracy than adversarial training. For example, smoothing with a noise level
of σ = 0.84 can achieve 92.5% perturbed accuracy, which is 2% higher than the best empirical
approach from Table 2. Furthermore, all noise levels are able to achieve non-trivial degrees of
certified robustness, with the highest noise level being the most robust but also taking a hit to standard
test set accuracy metrics. Most notably, due to the latent structure present in the perturbation set as
discussed in Appendix E.3, even the ability to certify small radii translates to meaningful provable
4https://github.com/google-research/augmix
29
Published as a conference paper at ICLR 2021
Figure 11: Certified accuracy against CIFAR10 common corruptions using randomized smoothing
at various noise levels. The horizontal axis plots the certified radius, and the vertical axis plots the
fraction of examples certified at that radius.
guarantees against certain types of corruptions, like defocus blur and jpeg compression which are
largely captured at radius = 3.9.
We note that although the largest noise level can theoretically reach the 75th percentile of the
perturbation set at radius = 10.2, none of the examples can be certified to that extent. This is
possibly a limitation of the certification procedure, since although a noise level of σ = 3.19 can
theoretically certify a radius of up to = 10.2, the `2 norm of the random samples needed to perform
randomized smoothing is approximately 23, which is well beyond the 75th percentile and towards
the boundary of the perturbation set. The complete robustness curves for all three noise levels are
plotted in Figure 11.
F	Multi-illumination
In this final section we present the multi-illumination experiments in greater detail with an expanded
discussion. We use the test set provided by Murmann et al. (2019) which consists of 30 held out
scenes and hold out 25 additional “drylab” scenes for validation. Unlike in the CIFAR10 common
corruptions setting, there is no such thing as an “unperturbed” example in this dataset so we train on
random pairs selected from the 25 different lighting variations for each scene. These experiments
were run on a single Quadro RTX 8000 graphics card, taking 16 hours to train the CVAE and 12
hours to run adversarial training.
F.1 Architecture and training specifics
We convert a generic UNet architecture (Ronneberger et al., 2015) to use as a CVAE, by inserting
the variational latent space in between the skip connections of similar resolutions. At a high level,
the encoder and prior networks will be based on the downsampling half of a UNet architecture, the
conditional generator will be based on the full UNet architecture, and the components will be linked
via the latent space of the VAE. Specifically, our networks have [64, 128, 256, 512, 512] channels
when downsampling, and twice the number of channels when upsampling due to the concatenation
from the skip connection. Each skip connection passes through a 1 × 1 convolution that reduces the
number of channels to 16, and an adaptive average pooling layer that reduces the feature maps to
height and width [(127, 187), (62, 93), (31, 46), (15, 23), (7, 11)] respectively. The adaptive average
pooling layer has the effect of a null operator for the MIP5 resolution, but allows higher resolutions
to use the same architecture. These fixed-size feature maps are then passed through a fully connected
layer to output a mean and log variance for the latent space with dimension [128, 64, 32, 16, 16] for
each respective feature map. The concatenation of all the latent vectors from each skip connection
30
Published as a conference paper at ICLR 2021
Table 10: Measuring and comparing quality metrics for a multi-illumination CVAE at different
resolutions.
Test set quality metrics	Test set CVAE metrics
Resolution		Encoder AE	PGD AE	OAE	EAE	Recon. err	KL
MIP5 (125 × 187)	17	0.019	0.006	0.049	0.13	0.0040	65.8
MIP4 (250 × 375)	25	0.034	0.008	0.049	0.27	0.0042	146.6
MIP3 (500 × 750)	21	0.060	0.009	0.048	0.33	0.0055	106.1
Table 11: Multi-illumination validation set statistics for the `2 norm of the latent space encodings.
Resolution	β	Mean	Std	25%	50%	75%	Max
MIP5 (125 × 187)	1	7.42	2.14	5.93	7.35	8.81	16.63
MIP4 (250 × 375)	1	10.88	3.18	8.69	10.75	12.95	24.69
MIP3 (500 × 750)	10	9.11	2.72	7.14	9.00	10.97	20.65
forms the full, 256 dimensional latent space vector for the CVAE. Similar to the CIFAR10 setting,
we use a scaled Tanh activation to stabilize the log variance calculation.
The generator of the CVAE UNet is implemented as a typical UNet that takes as input the conditioned
example, where intermediate feature maps are concatenated with extra feature maps from the latent
space of the CVAE. Specifically, each latent space sub-vector is mapped with a fully connected layer
back to a feature map with size [(127, 187), (62, 93), (31, 46), (15, 23), (7, 11)] with one channel.
It is then interpolated to the actual feature map size of the UNet (which is a no-op for the MIP5
resolution) and concatenated to the standard UNet feature map before upsampling.
We train the model for 1000 epochs with batch size 64, using the Adam optimizer with 0.9 momentum,
weight decay 5 ∙ 10-7, and a cyclic learning rate from [0.0001,0.001,0] over [0,400,1000] epochs.
Same as in the CIFAR10 setting, the learning rate and weight decay were chosen to optimize the
validation performance of the baseline data augmentation approach and kept fixed as-is for all other
methods. We weight the KL divergence with a β hyperparameter which is scheduled from [0, 1, 1]
over [0, 400, 1000], using random flip and crop with padding of 10 for data augmentation. We then
fine tune the model for higher resolutions: for MIP4 we fine tune for 100 epochs with the same
learning rate schedule scaled down by a factor of 4, and for MIP3 we fine tune for 25 epochs with
the learning rate schedule scaled down by 40. In order to keep the samples looking reasonable, for
MIP3 we also increase the β weight on the KL divergence to 10. Random cropping augmentation is
proportionately increased with the size of the image, using padding of 20 and 40 for MIP4 and MIP3
respectively.
Learning and evaluating an illumination perturbation set at multiple scales The results of this
fine tuning procedure to learn a perturbation at higher resolutions are summarized in Tables 10 and 11.
As expected, the quality metrics of the perturbation set get slightly worse at higher resolutions which
is counteracted to some degree by the increase in weighting for the KL divergence at MIP3. Despite
using the same architecture size, the perturbation set is able to reasonably scale to images with 16
times more pixels and generate reasonable samples while keeping relatively similar quality metrics.
However, to keep computation requirements at a reasonable threshold, we focus our experiments at
the MIP5 resolution, which is sufficient for our robustness tasks.
F.2 Additional samples and interpolations from the CVAE
We present additional samples and interpolations of lighting changes learned by the CVAE perturba-
tion set. Figure 12 shows interpolations between three randomly chosen lighting perturbations for
four different scenes, while Figure 13 shows 24 additional random samples from the perturbation set,
showing a variety of lighting conditions. These demonstrate qualitatively that the perturbation set
contains a reasonable set of lighting changes.
31
Published as a conference paper at ICLR 2021
Figure 12: Additional interpolations between three (left, middle, right) randomly chosen lighting
perturbations in each row.
Figure 13: Additional random samples from the CVAE prior showing variety in lighting perturbations
in various different scenes.
F.3 Adversarial training and randomized smoothing
In this final task, we leverage our learned perturbation set to learn a model which is robust to
lighting perturbations. Specifically the multi-illumination dataset comes with annotated material
maps that label each pixel with the type of material (e.g. fabric, plastic, and paper), so a natural task
to perform in this setting is to learn material segmentation maps. We report robustness results at
∈ {7.35, 8.81, 17} which correspond to the the 50th, 75th, and 100th percentiles. We select a fixed
lighting angle which appears to be neutral and train a material segmentation model to generate a
baseline, which achieves 37.2% accuracy on the test set but only 14.9% robust accuracy under the
full perturbation model.
We evaluate several methods for improving robustness of the segmentation maps to lighting pertur-
bations, namely data augmentation with the perturbed data, data augmentation with the CVAE, and
adversarial training with the CVAE. The results are tabulated in Table 3. The CVAE data augmentation
approach is not as effective in this setting at improving robust accuracy, as the pure data augmentation
approach does reasonably well. However, the adversarial training approach unsurprisingly has the
most robust accuracy, maintaining 35.4% robust accuracy under the full perturbation set at = 17
and outperforming the data augmentation approaches. We plot adversarial perturbations at = 7.35
and the resulting changed segmentation maps in Figure 14 for a model trained with pure data aug-
mentation and in Figure 15 for a model trained to be adversarially robust, the latter of which has, on
average, less pixels that are effected by an adversarial lighting perturbation. Adversarial examples at
the full radius of = 17 are shown in Figure 16, where we see that the perturbation set is beginning
to cast dark shadows over regions of the image to force the model to fail.
32
Published as a conference paper at ICLR 2021
Wood
Piasti Plastic - opaque
Fabric/cloth
Painted
Metal
Plastic - clear
Other
Tile
Figure 14: Adversarial examples that can cause on average 9.9% more pixels in the shown segmenta-
tion maps to be incorrect for a model trained with data augmentation. The first two rows are a benign
lighting perturbation and its corresponding predicted material segmentation, and the next two rows
are an adversarial lighting perturbation and its corresponding predicted material segmentation. For
reference, the final row contains the true material segmentation.
Figure 15: Adversarial examples that can cause on average 3% more pixels in the shown segmentation
maps to be incorrect for an adversarially trained model.
Finally, we train a certifiably robust material segmentation model using the perturbation set. We
train using a noise level of σ = 6.90 which can certify a radius of at most = 17, or the limit of
the perturbation set. The resulting robustness curve is plotted in Figure 17. The model achieves
30.7% perturbed accuracy and is able to get 12.4% certified accuracy at the 50th percentile of radius
= 7.35. The key takeaway is that we can now certify real-world perturbations to some degree, in
this case certifying robustness to 50% of lighting perturbations with non-trivial guarantees.
33
Published as a conference paper at ICLR 2021
Figure 16: Adversarial examples at the full radius of = 17 that for an adversarially trained model,
which are starting to cast dark shadows to obscure the objects in the image.
Figure 17: Certified accuracy for material segmentation model using randomized smoothing. The
horizontal axis denotes the certified radius, and the vertical axis denotes the fraction of pixels that
are certifiably correct at that radius. Note that a radius of = 17 is the maximum radius of the
perturbation set.
34