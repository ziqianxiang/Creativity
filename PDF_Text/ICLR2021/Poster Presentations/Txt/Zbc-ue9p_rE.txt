Published as a conference paper at ICLR 2021
Refining Deep Generative Models via
Discriminator Gradient Flow
Abdul Fatir Ansari, Ming Liang Ang & Harold Soh
Department of Computer Science, School of Computing
National University of Singapore
{abdulfatir, angmingliang}@u.nus.edu, harold@comp.nus.edu.sg
Ab stract
Deep generative modeling has seen impressive advances in recent years, to the
point where it is now commonplace to see simulated samples (e.g., images) that
closely resemble real-world data. However, generation quality is generally in-
consistent for any given model and can vary dramatically between samples. We
introduce Discriminator Gradient flow (DGf low), a new technique that improves
generated samples via the gradient flow of entropy-regularized f -divergences be-
tween the real and the generated data distributions. The gradient flow takes the
form of a non-linear Fokker-Plank equation, which can be easily simulated by
sampling from the equivalent McKean-Vlasov process. By refining inferior sam-
ples, our technique avoids wasteful sample rejection used by previous methods
(DRS & MH-GAN). Compared to existing works that focus on specific GAN vari-
ants, we show our refinement approach can be applied to GANs with vector-valued
critics and even other deep generative models such as VAEs and Normalizing
Flows. Empirical results on multiple synthetic, image, and text datasets demon-
strate that DGf low leads to significant improvement in the quality of generated
samples for a variety of generative models, outperforming the state-of-the-art Dis-
criminator Optimal Transport (DOT) and Discriminator Driven Latent Sampling
(DDLS) methods.
1	Introduction
Deep generative models (DGMs) have excelled at numerous tasks, from generating realistic im-
ages (Brock et al., 2019) to learning policies in reinforcement learning (Ho & Ermon, 2016). Among
the variety of proposed DGMs, Generative Adversarial Networks (GANs) (Goodfellow et al., 2014)
have received widespread popularity for their ability to generate high quality samples that resem-
ble real data. Unlike Variational Autoencoders (VAEs) (Kingma & Welling, 2014) and Normal-
izing Flows (Rezende & Mohamed, 2015; Kingma & Dhariwal, 2018), GANs are likelihood-free
methods; training is formulated as a minimax optimization problem involving a generator and a dis-
criminator. The generator seeks to generate samples that are similar to the real data by minimizing
a measure of discrepancy (between the generated samples and real samples) furnished by the dis-
criminator. The discriminator is trained to distinguish the generated samples from the real samples.
Once trained, the generator is used to simulate samples and the discriminator has traditionally been
discarded.
However, recent work has shown that discarding the discriminator is wasteful — it actually contains
useful information about the underlying data distribution. This insight has led to sample improve-
ment techniques that use this information to improve the quality of generated samples (Azadi et al.,
2019; Turner et al., 2019; Tanaka, 2019; Che et al., 2020). Unfortunately, current methods either
rely on wasteful rejection operations in the data space (Azadi et al., 2019; Turner et al., 2019), or
require a sensitive diffusion term to ensure sample diversity (Che et al., 2020). Prior work has
also focused on improving GANs with scalar-valued discriminators, which excludes a large family
of GANs with vector-valued critics, e.g., MMDGAN (Li et al., 2017; BinkoWski et al., 2018) and
OCFGAN (Ansari et al., 2020), and likelihood-based generative models.
1
Published as a conference paper at ICLR 2021
In this work, we propose Discriminator Gradient flow
(DGflow) which formulates sample improvement as re-
fining inferior samples using the gradient flow of f-
divergences between the generator and the real data dis-
tributions (Fig. 1). DGflow avoids wasteful rejec-
tion operations and can be used in a deterministic set-
ting without a diffusion term. Existing state-of-the-art
methods — specifically, Discriminator Optimal Transport
(DOT) (Tanaka, 2019) and Discriminator Driven Latent
Sampling (DDLS) (Che et al., 2020) — can be viewed as
special cases of DGflow. Similar to DDLS, DGflow re-
covers the real data distribution when the gradient flow is
simulated exactly.
We further present a generalized framework that employs
existing pre-trained discriminators to refine samples from
a variety of deep generative models: we demonstrate
our method can be applied to GANs with vector-valued
critics, and even likelihood-based models such as VAEs
and Normalizing Flows. Empirical results on synthetic
datasets, and benchmark image (CIFAR10, STL10) and
text (Billion Words) datasets demonstrate that our gradi-
ent flow-based approach outperforms DOT and DDLS on
multiple quantitative evaluation metrics.
In summary, this paper’s key contributions are:
•	DGflow, a method to refine deep generative
models using the gradient flow of f -divergences;
Po	Pi	P2	・・・ Pn-1 'Pn
Figure 1: An illustration of refinement us-
ing DGflow, with the gradient flow in the
2-Wasserstein space P2 (top) and the corre-
sponding discretized SDE in the latent space
Z (bottom). The image samples from the
densities along the gradient flow are shown
in the middle.
•	a framework that extends DGflow to GANs with vector-valued critics, VAEs, and Nor-
malizing Flows;
•	experiments on a variety of generative models trained on synthetic, image (CIFAR10 &
STL10), and text (Billion Words) datasets demonstrating that DGflow is effective in im-
proving samples from generative models.
2	Background: Gradient Flows
The following gives a brief introduction to gradient flows; we refer readers to the excellent overview
by Santambrogio (2017) for a more thorough introduction.
Let (X, k ∙ ∣∣2) be a Euclidean space and F : X → R be a smooth energy function. The gradient
flow ofF is the smooth curve {xt}t∈R+ that follows the direction of steepest descent, i.e.,
X (t) = -VF (x(t)).	(1)
The value of the energy F is minimized along this curve. This idea of steepest descent curves can be
characterized in arbitrary metric spaces via the minimizing movement scheme (Jordan et al., 1998).
Of particular interest is the metric space of probability measures that is endowed with the Wasser-
stein distance (Wp ); the Wasserstein distance is a metric and the Wp topology satisfies weak con-
vergence of probability measures (Villani, 2008, Theorem 6.9). Gradient flows in the 2-Wasserstein
space (P2(Ω), W2)——i.e., the space of probability measures with finite second moments and the
2-Wasserstein metric — have been studied extensively. Let {ρt}t∈R+ be the gradient flow of a
functional F in the 2-Wasserstein space, where ρt is absolutely continuous with respect to the
Lebesgue measure. The curve {ρt }t∈R+ satisfies the continuity equation (Ambrosio et al., 2008,
Theorem 8.3.1),
∂tPt + V ∙ (ρtVt) = 0.	(2)
The velocity field vt in Eq. (2) is given by
δF
Vt(X) = -Vx δρ (P),
(3)
2
Published as a conference paper at ICLR 2021
where * denotes the first variation of the functional F.
Since the seminal work of Jordan et al. (1998) that showed that the Fokker-Plank equation is the
gradient flow of a particular functional in the Wasserstein space, gradient flows in the Wasserstein
metric have been a popular tool in the analysis of partial differential equations (PDEs). For ex-
ample, they have been applied to the study of the porous-medium equation (Otto, 2001), crowd
modeling (Maury et al., 2010; 2011), and mean-field games (Almulla et al., 2017). More recently,
gradient flows of various distances used in deep generative modeling literature have been proposed,
notably that of the sliced Wasserstein distance (Liutkus et al., 2019), the maximum mean discrep-
ancy (Arbel et al., 2019), the Stein discrepancy (Liu, 2017), and the Sobolev discrepancy (Mroueh
et al., 2019). Gradient flows have also been used for learning non-parametric and parametric implicit
generative models (Liutkus et al., 2019; Gao et al., 2019; 2020). As an example of the latter, Varia-
tional Gradient Flow (Gao et al., 2019) learns a mapping between latent vectors and samples evolved
using the gradient flow of f -divergences. In this work, we present a method using gradient flows
of entropy-regularized f -divergences for refining samples from deep generative models employing
existing discriminators as density-ratio estimators.
3 Generator Refinement via Dis criminator Gradient Flow
This section describes our main contribution: Discriminator Gradient flow (DGf low). As an
overview, we begin with the construction of the gradient flow of entropy-regularized f -divergences
and describe its application to sample refinement. We then discuss how to simulate the gradient
flow in the latent space of the generator — a procedure more suitable for high-dimensional datasets.
Finally, we present a simple technique that extends our method to generative models that have not
yet been studied in the context of refinement. Due to space constraints, we focus on conveying the
key concepts and relegate details (e.g., proofs) to the appendix.
The entropy-regularized f -divergence functional is defined as
Ff(P) , Df (μkρ) - γH(P),
(4)
where the f-divergence term Df (μkρ) ensures that the “distance” between the probability density
ρ and the target density μ decreases along the gradient flow. The differential entropy term H(P)
improves diversity and expressiveness when the gradient flow is simulated for finite time-steps. We
now construct the gradient flow of Ff.
Lemma 3.1. Define thefunCtional Ff : P (Ω) → R as
Ff (P)，/
I.
f (P(X"μ(X)) μ(X)dχ + Y
P(x) log P(x)
dX,
(5)
f-divergence
negative entropy
where f is a twice-differentiable convex function with f(1) = 0. The gradient flow of the functional
Ff (p) in the Wasserstein space (P2(Ω), W2) is given by the following PDE,
∂tPt(x) - Vx ∙ (Pt(X)Vxf0 (Pt(X)/μ(x))) - γ∆χχPt(x) = 0,	⑹
where Vx ∙ and ∆xx denote the divergence and the Laplace operators respectively.
The proof is given in Appendix A.1. The PDE in Eq. (6) is a type of Fokker-Plank equation (FPE).
FPEs have been studied extensively in the literature of stochastic processes and have a Stochastic
Differential Equation (SDE) counterpart (Risken, 1996). In the case of Eq. (6), the equivalent SDE
is given by
dXt
-Vxf0 (Pt∕μ) (Xt)dt + √2γdwt,
(7)
|
} |---------------{z---------/
z
drift
diffusion
where dwt denotes the standard Wiener process. Eq. (7) defines the evolution of a particle Xt under
the influence of drift and diffusion. Specifically, it is a McKean-Vlasov process (Braun & Hepp,
1977) which is a type of non-linear stochastic process as the drift term at any time t depends on the
distribution Pt of the particle Xt . Eqs. (6) and (7) are equivalent in the sense that the distribution
of the particle Xt in Eq. (7) solves the PDE in Eq. (6). Consequently, samples from the density Pt
along the gradient flow can be obtained by first drawing samples Xo 〜 Po and then simulating the
3
Published as a conference paper at ICLR 2021
SDE in Eq. (7). The SDE can be approximately simulated via the stochastic Euler scheme (also
known as the Euler-Maruyama method) (Beyn & Kruse, 2011) given by
xτn+1 = xτn -Nxf (PTn /μ) (xTn ) + √2γηξτn ,	⑻
where ξτn 〜N(0, I), the time interval [0, T] is partitioned into equal intervals of size η and τo <
τι < •…<tn denote the discretized time-steps.
Eq. (8) provides a non-parametric procedure to refine samples from a generator gθ where We let μ
be the density of real samples and ρτ0 the density of samples generated from gθ obtained by first
sampling from the prior latent distribution Z 〜PZ (Z) and then feeding Z into gθ. We first generate
particles x0 〜 夕「。and then update the particles using Eq. (8) for N time steps.
Given a binary classifier (discriminator) D that has been trained to distinguish between samples
from μ and ?丁。, the density-ratio ?「。(x)∕μ(x) can be estimated via the well-known density-ratio
trick (Sugiyama et al., 2012),
Pτo (χ"μ(χ) = 1 DDy (= =∣X)x) = eχp(-d(χ)b	⑼
where D(y = 1|x) denotes the conditional probability of the sample X being from μ and d(x)
denotes the logit output of the classifier D. We term this procedure where samples are refined via
gradient flow of f -divergences as Discriminator Gradient flow (DGf low).
3.1	Refinement in the latent space
Eq. (8) requires a running estimate of the density-ratio ρτn (x)∕μ(x), which can be approximated
using the stale estimate ρτn (x)∕μ(x) ≈ ?「。(x)∕μ(x) for η → 0 and small N, where the density
ρτn will be close to ρτ0 . However, our initial image experiments showed that refining directly in
high-dimensional data-spaces with the stale estimate is problematic; error is accumulated at each
time-step leading to a visible degradation in the quality of data samples (e.g., appearance of artifacts
in images).
To tackle this problem, we propose refining the latent vectors before mapping them to samples in
data-space using gθ . We describe a procedure analogous to Eq. (8) but in the latent space for
generators gθ that take a latent vector Z ∈ Z as input and generate a sample x ∈ X. We first show
in Lemma 3.2 that the density-ratio in the latent space between two distributions can be estimated
via the density-ratio of corresponding distributions in the data space.
Lemma 3.2. Let g : Z → X be a sufficiently well-behaved injective function where Z ⊆ Rn and
X ⊂ Rm with m > n. Let PZ (z), PZ (Z) be probability densities on Z and qχ (x), qχ (x) be the den-
ʌ
sities ofthe pushforward measures g]Z, g]Z respectively. Assume that PZ(z) andPZ (Z) have same
support, and the Jacobian matrix Jg hasfull column rank. Then, the density-ratio PZ (u)∕pz (u) at
the point u ∈ Z is given by
PZ (u) — qχ (g(u))
---:~~~= :—:——
PZ (u)	qX (g(u))
(10)
The proof is in Appendix A.2. We let
PZ (Z) be the density of the “correct” latent
space distribution induced by a generator
gθ, i.e., PZ(Z) is the density of a probabil-
ity measure whose pushforward under gθ
approximately equals the target data den-
sity μ. The density-ratio of the prior latent
distribution PZ (Z) and PZ (Z) can now be
computed by combining Lemma 3.2 with
Eq. (9),
PZ (u)	ρτ0 (gθ(u))
---:--——:—:———
p^ (u)	μ(gθ (u))
eχp(-d(gθ(u))).
(11)
Algorithm 1 Refinement in the Latent Space using
DGflow.__________________________________________
Require: First derivative of f (f0), generator (gθ), dis-
criminator (dφ), number of update steps (N), step-
size (η), noise factor (γ ).
1:	Zo 〜pz (Z)	. Sample from the prior.
2:	for i - 0, N do
3:	ξi 〜N(0,I)
4:	Zi+1 = Zi- ηVzif0(e-dφ(gθ(zi))) + √2ηγξi
5:	end for
6:	return gθ(Zn)	. The refined sample.
4
Published as a conference paper at ICLR 2021
Although a generator gθ parameterized by a neural network may not satisfy the conditions of in-
jectivity and full column rank Jacobian matrix Jgθ , Eq. (11) provides an approximation that works
well in practice as shown by our experiments. Combining Eq. (11) with Eq. (8) provides us with an
update rule for refining samples in the latent space,
uτn+1 = uτn - ηVuf0 (PuTnlPZ) (UTn ) + P2γηξTn ,	(12)
where u「o 〜 PZ (Z) and the density-ratio puτn /pz is approximated using the stale estimate
PuTO /PZ = exp(-d(gθ(u))). We summarize the complete algorithm in Algorithm 1.
3.2	Refinement for All
Thus far, prior work (Azadi et al., 2019; Turner et al., 2019; Tanaka, 2019; Che et al., 2020) has
focused on improving samples for GANs with scalar-valued discriminators, which comprises the
canonical GAN as well as recent variants, e.g., WGAN (Gulrajani et al., 2017), and SNGAN (Miyato
et al., 2018). Here, we propose a technique that extends our approach to refine samples from a larger
class of DGMs including GANs with vector-valued critics, VAEs, and Normalizing Flows.
Let pθ be the density of the samples generated by a generator gθ and μ be the density of the real data
distribution. We are interested in refining samples from gθ ; however, a corresponding density-ratio
estimator for pθ/μ is unavailable, as is the case with the aforementioned generative models.
Let Dφ be a discriminator that has been trained on the same dataset but for a different generative
model gφ (e.g., let gφ and Dφ be the generator and discriminator of SNGAN respectively). Dφ can
be used to compute the density ratio Pφ∕μ. A straightforward technique would be to use the crude
approximation pθ ∕μ ≈ pφ∕μ, which could work provided pθ and pφ are not too far from each other.
Our experiments show that this simple approximation works to a limited extent (see appendix E).
To improve upon the crude approximation above, we propose to correct the density-ratio estimate.
Specifically, a discriminator Dλ is initialized with the weights from Dφ and is fine-tuned on samples
from gφ and gθ. Dφ and Dλ are then used to approximate the density-ratio pθ ∕μ,
Pθ(x)	Pφ(x) Pθ(x)
—:————:—：-----:--
μ(χ)	μ(χ) Pφ(χ)
exp(-dφ(x)) ∙ exp(-dλ(x)),
(13)
where dφ and dλ are logits output from Dφ and Dλ, respectively. We term the network Dλ the
density ratio corrector, which experiments show produces higher quality samples than using pθ∕μ ≈
Pφ∕μ. The estimate in Eq. (13) is similar to telescoping density-ratio estimation (TRE), a technique
proposed in very recent independent work (Rhodes et al., 2020). In brief, Rhodes et al. (2020)
show that classifier-based density ratio estimators perform poorly when distributions are “too far
apart”; the classifier can easily distinguish between the distributions, even with a poor estimate of the
density ratio. TRE expands the standard density ratio into a telescoping product of more difficult-
to-distinguish intermediate density ratios. Likewise, in Eq. (13), we treat Pφ as an intermediate
distribution and estimate the final density-ratio as a product of two density-ratios.
4	Related Work
Azadi et al. (2019) first proposed the idea of improving samples from a GAN’s generator by discrim-
inator rejection sampling (DRS), making use of the density-ratio provided by the discriminator to
estimate the acceptance probability. Metropolis-Hastings GAN (MH-GAN) (Turner et al., 2019) im-
proved upon the costly rejection sampling procedure via the Metropolis-Hastings algorithm. Unlike
DGf low, both of these methods reject inferior samples instead of refining them.
Our method is closely related to recent state-of-the-art sample refinement techniques, specifically
Discriminator-Driven Latent Sampling (DDLS) (Che et al., 2020) and Discriminator Optimal Trans-
port (DOT) (Tanaka, 2019). In fact, both these methods can be seen as special cases of DGf low.
DDLS treats a GAN as an energy-based model and uses Langevin dynamics to sample from the
energy-based latent distribution Pt(Z) 8 PZ (z) exp(d(gθ (Z))) induced by performing rejection sam-
pling in the latent space. This distribution is the same as PZ(Z), which can be seen by rearranging
terms in Eq. (11). If we use the KL-divergence by setting f = r log r , DGflow is equivalent to
DDLS. However, there are practical differences that make DGf low more appealing. DDLS requires
estimation of the score function Vz{logPZ(Z) + d(gθ(Z))} to perform the update which becomes
5
Published as a conference paper at ICLR 2021
undefined if z escapes the support ofpZ(z), e.g., in the case of the uniform prior distribution com-
monly used in GANs; handling such cases would require techniques such as projected gradient
descent. This problem does not arise in the case of DGf low since it only uses the density-ratio
that is implicitly defined by the discriminator. Moreover, DDLS uses Langevin dynamics which
requires the sensitive diffusion term to ensure diversity and to prevent points from collapsing to the
maximum-likelihood point. In DGf low, the sample diversity is ensured by the density-ratio term
and the diffusion term serves as an enhancement. Note that DGf low performs well even without
the diffusion term (i.e., with γ = 0, see Tables 13 & 14 in the appendix). This deterministic variant
of DGf low is a practical alternative with one less hyperparameter to tune.
DOT refines samples by constructing an Optimal Transport (OT) map induced by the WGAN dis-
criminator. The OT map is realized by means of a deterministic optimization problem in the vicinity
of the generated samples. If we further analyze the case of DGf low with γ = 0 and solve the
resulting ordinary differential equation (ODE) using the backward Euler method,
uTn+1 =arg∈min ∣f 0 3uτJPZ)(U)+ 2λ ku - UTnk2	(14)
DOT emerges as a special case when we consider a single update step of Eq. (14) using gradient
descent and set f 0(t) = log(t)1 with λ = 1. This connection of DGflow to DOT, an optimal
transport technique, is perhaps unsurprising given the relationship between gradient flows and the
dynamical Benamou-Brenier formulation of optimal transport (Santambrogio, 2017).
Recent work has also sought to improve generative models via sample evolution in the train-
ing/generation process. In energy-based generative models (Arbel et al., 2021; Deng et al., 2020),
the energy functions can be viewed as a component that improves some base generator. For example,
the Generalized Energy-Based Model (GEBM) (Arbel et al., 2021) jointly trains a base generator
by minimizing a lower bound of the KL divergence along with an energy function in an alternating
fashion. Once trained, the energy function is used to refine samples from the base generator using
Langevin dynamics and serves a similar purpose to the discriminator in DDLS and DGf low. The
Noise Conditional Score Network (NCSN) (Song & Ermon, 2019; 2020) — a score-based genera-
tive model — can be seen as a gradient flow that refines a sample right from noise to data. Latent
Optimization GAN (LOGAN) (Wu et al., 2020) optimizes a latent vector via natural gradient de-
scent as part of the GAN training process. In contrast to these works, we primarily focus on refining
samples from pretrained generative models using the gradient flow of f -divergences.2
5	Experiments
In this section, we present empirical results on various deep generative models trained on multiple
synthetic and real world datasets. Our primary goals were to determine if (a) D Gflow is effective
in improving the quality of samples from generative models, (b) the proposed extension to other
generative models improves their sample quality, and (c) DGf low is generalizable to different types
of data and metrics. Note that we did not seek to achieve state-of-the-art results for the datasets
studied but to demonstrate that D Gflow is able to significantly improve samples from the bare
generators for different models.
We experimented with three f -divergences, namely the Kullback-Leibler (KL) divergence, the
Jensen-Shannon (JS) divergence, and the log D divergence (Gao et al., 2019). The specific forms
of the functions f and corresponding derivatives are tabulated in Table 7 (appendix). We com-
pare D Gflow with two state-of-the-art competing methods: DOT and DDLS. In this section we
discuss the main results and relegate details to the appendix. Our code is available online at
https://github.com/clear-nus/DGflow.
5.1	2D Datasets
We first tested D Gflow on two synthetic datasets, 25Gaussians and 2DSwissroll, to visually inspect
the improvement in the quality of generated samples. We generated 5000 samples from a trained
WGAN-GP generator and refined them using DOT, DDLS, and DGf low. We performed refinement
in the latent space for DDLS and directly in the data-space for DOT and DGf low. Fig. 2 shows
1This implies that f(t) = t log t - t + 1, which is a twice-differentiable convex function with f(0) = 1.
2For further discussion about these techniques, please refer to appendix C.
6
Published as a conference paper at ICLR 2021
Real Samples
Real Samples	GAN	DOT	DDLS	DGflOW
d)	⅜	ð	∂	∂
Figure 2: Qualitative comparison of DGf low(KL) with DOT and DDLS on synthetic 2D datasets.
the samples generated from the WGAN-GP generator (blue) and the refined samples using different
techniques (red) against the real samples from the training dataset (brown). Although the WGAN-
GP generator learned the overall structure of the dataset, it also learned a number of spurious modes.
DOT is able to refine the spurious samples but to a limited degree. In contrast, DDLS and DGf low
are able to correct almost all spurious samples and are able to recover the correct structure of the
data. Visualizations for DGf low with different f -divergences can be found in the appendix (Fig.
4).
We also compared the different methods quan-
titatively on two metrics: % high quality
samples and kernel density estimate (KDE)
score. A sample is classified as a high qual-
ity sample if it lies within 4 standard de-
viations of its nearest Gaussian. The KDE
score is computed by first estimating the KDE
using generated samples and then comput-
ing the log-likelihood of the training sam-
ples under the KDE estimate. We computed
both the metrics 10 times using 5000 sam-
Table 1: Quantitative comparison on the 25Gaussians
dataset. Higher scores are better.
	% High Quality	KDE Score
GAN	26.5 ± .8	-7037 ± 64
DOT	69.8 ± .7	-4149 ± 39
DDLS	89.3 ± .6	-2997 ± 17
DGf low(KL)	89.5 ± .4	-2893 ± 07
DGflow(JS)	82.6 ± .4	-3118 ± 19
DGflow(log D)	84.5 ± .3	-3036 ± 14
ples and report the mean in Table 1. The quantitative metrics reinforce the qualitative anal-
ysis and show that DDLS and DGf low significantly improve the samples from the gen-
erator, with DGf low performing slightly better than DDLS in
terms of the KDE score.
5.2	Image Experiments
We conducted experiments on the
CIFAR10 and STL10 datasets to
demonstrate the efficacy of DGf low
in the real-world setting. We fol-
lowed the setup of Tanaka (2019)
for our image experiments. We
used the Frechet Inception Distance
(FID) (Heusel et al., 2017) and In-
ception Score (IS) (Salimans et al.,
2016) metrics to evaluate the quality
of generated samples before and after
refinement. A high value of IS and a
low value of FID corresponds to high
quality samples, respectively.
We first applied D Gflow to GANs
with scalar-valued discriminators
(e.g., WGAN-GP, SNGAN) trained
on the CIFAR10 and the STL10
datasets. Table 2 shows that DGf low
significantly improves the quality of
(a) CIFAR10	(b) STL10
Figure 3: Improvement in the quality of samples generated from
the base model (leftmost columns) over the steps of DGf low for
SN-ResNet-GAN and SN-DCGAN on the CIFAR10 and STL10
datasets respectively.
7
Published as a conference paper at ICLR 2021
Table 2: Comparison of different variants of DGf low with DOT on the CIFAR10 and STL10 datasets. For
SN-DCGAN, (hi) denotes the hinge loss and (ns) denotes the non-saturating loss. Lower scores are better.
DGf low’s results have been averaged over 5 random runs with the standard deviation in parentheses.
Model		FreChet Inception Distance				
		Base Model	DOT	DGfloW(KL)	DGfloW(JS)	DGfloW(log D)
	WGAN-GP	28.37 (.08)	24.14	24.68 (.09)	23.15 (.07)	24.53 (.11)
	SN-DCGAN (hi)	20.70 (.05)	17.12	15.68 (.07)	16.45 (.06)	17.36 (.05)
	SN-DCGAN (ns)	20.90 (.11)	15.78	15.30 (.08)	15.90 (.11)	16.42 (.05)
	SN-ResNet-GAN	14.10 (.06)	—	9.62 (.03)	9.79 (.02)	9.73 (.05)
	WGAN-GP	51.50 (.15)	44.45	39.07 (.07)	50.83 (.06)	39.71 (.29)
	SN-DCGAN (hi)	40.54 (.17)	34.85	34.95 (.06)	36.37 (.12)	36.56 (.08)
	SN-DCGAN (ns)	41.86 (.12)	34.84	34.60 (.11)	35.37 (.12)	37.07 (.14)
Table 3: Inception scores of different generative models, DRS, MH-GAN, DDLS, and DGf low on the CI-
FAR10 dataset. Higher scores are better.
Model	Inception Score
WGAN-GP (Gulrajani et al., 2017)	7.86 (.07)
ProgressiveGAN (Karras et al., 2017)	8.80 (.05)
SN-ResNet-GAN (Miyato et al., 2018)	8.22 (.05)
NCSN (Song & Ermon, 2019)	8.87 (.12)
DCGAN	2.88
DCGAN + DRS (cal) (Azadi et al., 2019)	3.07
DCGAN +MH (cal) (Turner et al., 2019)	3.38
SN-ResNet-GAN (our evaluation)	8.38 (.03)
SN-ResNet-GAN + DDLS (cal) (Che et al., 2020)	9.09 (.10)
SN-ResNet-GAN + DGfloW(KL)	9.35 (.03)
BigGAN	9.22
the samples in terms of the FID score and outperforms DOT on multiple models. The corresponding
values of the Inception score can be found in the Appendix (Table 11), which shows DGf low
outperforms DOT on all models. In Table 3, we reproduce previously reported IS results for
generative models and other sample improvement methods (DRS, MH-GAN, and DDLS) for
completeness. DGf low performs the best in terms of relative improvement from the base score
and even outperforms the state-of-the-art BigGAN (Brock et al., 2019), a conditional generative
model, without the need for additional labels. Qualitatively, DGf low improves the vibrance of the
samples and corrects deformations in the foreground object. Fig. 3 shows the change in the quality
of samples when using DGf low where the leftmost columns show the image generated form the
base models and the successive columns show the refined sample using DGf low over increments
of 5 update steps.
We then evaluated the ability of DGf low to refine samples from generative models without cor-
responding discriminators, namely MMDGAN, OCFGAN-GP, VAEs, and Normalizing Flows
(Glow). We used the SN-DCGAN (ns) as the surrogate discriminator Dφ for these models and
fine-tuned density ratio correctors Dλ for each model as described in section 3.2. Table 4 shows the
FID scores achieved by these models without and with refinement using DGf low. We obtain a clear
improvement in quality of samples when these generative models are combined with DGf low.
5.3	Character-Level Language Modeling
Finally, we conducted an experiment on the character-level language modeling task proposed
by Gulrajani et al. (2017) to show that DGf low works on different types of data. We trained a
character-level GAN language model on the Billion Words Dataset (Chelba et al., 2013), which was
pre-processed into 32-character long strings. We evaluated the generated samples using the JS-4 and
JS-6 scores which compute the Jensen-Shannon divergence between the 4-gram and 6-gram proba-
bilities of the data generated by the model and the real data. Table 5 (a) shows that DGflow leads to
8
Published as a conference paper at ICLR 2021
Table 4: Comparison of different variants of DGf low applied to MMDGAN, OCFGAN-GP, VAE, and Glow
models. Lower scores are better. Results have been averaged over 5 random runs with the standard deviation in
parentheses.
Model		FreChet Inception Distance			
		Base Model	DGfloW(KL)	DGflOW(JS)	DGfloW(log D)
	MMDGAN	41.98 (.12)	36.75(.09)	38.06 (.14)	37.75 (.10)
	OCFGAN-GP	31.98 (.12)	26.89 (.06)	28.20 (.06)	27.82 (.09)
	VAE	129.5 (.13)	116.0 (.21)	128.9 (.13)	115.2 (.06)
	Glow	100.5 (.52)	79.02 (.23)	94.61 (.34)	81.12 (.35)
	MMDGAN	47.20 (.07)	43.21 (.06)	46.74 (.05)	43.06 (.05)
	OCFGAN-GP	36.55 (.08)	31.12 (.13)	36.05 (.11)	30.61 (.14)
	VAE	150.5 (.09)	130.1 (.18)	149.9 (.08)	132.5 (.28)
Table 5: Results of DGf low on a character-level GAN language model.
(a) JS-4 and JS-6 scores. Lower scores are better.
Model	JS-4	JS-6
WGAN-GP	0.224 (.0009)	0.574 (.0015)
DGf loW(KL)	0.212 (.0008)	0.512 (.0012)
DGf loW(JS)	0.186 (.0007)	0.508 (.0011)
DGfloW(logD)	0.209 (.0005)	0.506 (.0008)
(b) Examples of text samples refined by DGflow.
Generated by WGAN-GP	Refined by DGf low
In Ruoduce that fhance would pol I said thowe toot lind talker . Now their rarning injurer hows Police report in B0sbu does off We gine jaid 121 , one bub like In years in 19mbisuch said he h	In product that chance could rol I said this stood line talked 10 Now their warning injurer shows Police report inturner will befe We gave wall said left out like In years in 1900b such said he h
an improvement in the JS-4 and JS-6 scores. Table 5 (b) shows example sentences where DGf low
visibly improves the quality of generated text.
6 Conclusion
In this paper, we proposed a technique to improve samples from deep generative models by refining
them using gradient flow of f -divergences between the real and the generator data distributions. We
also presented a simple framework that extends the proposed technique to commonly used deep gen-
erative models: GANs, VAEs, and Normalizing Flows. Experimental results indicate that gradient
flows provide an excellent alternative methodology to refine generative models. Moving forward,
we are considering several technical enhancements to improve D Gf low’s performance. At present,
DGf low uses a stale estimate of the density-ratio, which could adversely affect sample evolution
when the gradient flow is simulated for larger number of steps; how we can efficiently update this
estimate is an open question. Another related question is when the evolution of the samples should
be stopped; running chains for too long may modify characteristics of the original sample (e.g., ori-
entation and color) which may be undesirable. This issue does not just affect DGf low; a method
for automatically stopping sample evolution could improve results across refinement techniques.
Acknowledgements
This research is supported by the National Research Foundation Singapore under its AI Singapore
Programme (Award Number: AISG-RP-2019-011) to H. Soh. Thank you to J. Scarlett for his
comments regarding the proofs.
9
Published as a conference paper at ICLR 2021
References
Noha Almulla, Rita Ferreira, and Diogo Gomes. Two numerical approaches to stationary mean-field
games. Dynamic Games and Applications, 7(4), 2017.
LUigi Ambrosio, Nicola Gigli, and GiUsePPe Savare. Gradient flows: in metric spaces and in the
space of probability measures. Springer Science & Business Media, 2008.
AbdUl Fatir Ansari, Jonathan Scarlett, and Harold Soh. A characteristic fUnction aPProach to deeP
imPlicit generative modeling. In CVPR, 2020.
Michael Arbel, Anna Korba, Adil Salim, and ArthUr Gretton. MaximUm mean discrePancy gradient
flow. In NeurIPS, 2019.
Michael Arbel, Liang ZhoU, and ArthUr Gretton. Generalized energy based models. In ICLR, 2021.
Samaneh Azadi, Catherine Olsson, Trevor Darrell, Ian Goodfellow, and AUgUstUs Odena. Discrim-
inator rejection samPling. In ICLR, 2019.
Adi Ben-Israel. The change-of-variables formUla Using matrix volUme. SIAM Journal on Matrix
Analysis and Applications, 21(1), 1999.
Wolf-JUrgen Beyn and RaPhael KrUse. NUmerical methods for stochastic Processes. Lecture Book
(in preparation), 2011.
Mikolaj BinkoWski, Dougal J Sutherland, Michael ArbeL and Arthur Gretton. Demystifying MMD
GANs. In ICLR, 2018.
Werner Braun and K HePP. The Vlasov dynamics and its fluctuations in the 1/n limit of interacting
classical Particles. Communications in mathematical physics, 56(2), 1977.
AndreW Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity
natural image synthesis. In ICLR, 2019.
Tong Che, Ruixiang Zhang, Jascha Sohl-Dickstein, Hugo Larochelle, Liam Paull, Yuan Cao, and
Yoshua Bengio. Your GAN is secretly an energy-based model and you should use discriminator
driven latent samPling. In NeurIPS, 2020.
CiPrian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, PhilliPP Koehn, and Tony
Robinson. One billion Word benchmark for measuring Progress in statistical language modeling.
arXiv preprint arXiv:1312.3005, 2013.
Adam Coates, AndreW Ng, and Honglak Lee. An analysis of single-layer netWorks in unsuPervised
feature learning. In AISTATS, 2011.
Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and Marc’Aurelio Ranzato. Residual
energy-based models for text generation. In ICLR, 2020.
Yuan Gao, Yuling Jiao, Yang Wang, Yao Wang, Can Yang, and Shunkang Zhang. DeeP generative
learning via variational gradient floW. In ICML, 2019.
Yuan Gao, Jian Huang, Yuling Jiao, and Jin Liu. Learning imPlicit generative models With theoretical
guarantees. arXiv preprint arXiv:2002.02862, 2020.
Mevlana C Gemici, Danilo Rezende, and Shakir Mohamed. Normalizing floWs on Riemannian
manifolds. arXiv preprint arXiv:1611.02304, 2016.
Ian GoodfelloW, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014.
Aditya Grover, Ramki Gummadi, Miguel Lazaro-Gredilla, Dale Schuurmans, and Stefano Ermon.
Variational rejection samPling. In AISTATS, 2018.
Aditya Grover, Jiaming Song, Ashish KaPoor, Kenneth Tran, Alekh AgarWal, Eric J Horvitz, and
Stefano Ermon. Bias correction of learned generative models using likelihood-free imPortance
Weighting. In NeurIPS, 2019.
10
Published as a conference paper at ICLR 2021
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of Wasserstein gans. In NeurIPS, 2017.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to a local nash equilibrium. In NeurIPS,
2017.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In NeurIPS, 2016.
Richard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the Fokker-
Planck equation. SIAM journal on mathematical analysis, 29(1), 1998.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for
improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.
Diederik P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2014.
Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In
NeurIPS, 2018.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabas Poczos. MMDGAN:
Towards deeper understanding of moment matching network. In NeurIPS, 2017.
Qiang Liu. Stein variational gradient descent as gradient flow. In NeurIPS, 2017.
Antoine Liutkus, UmUt Simsekli, Szymon Majewski, Alain Durmus, and Fabian-Robert Stoter.
Sliced-Wasserstein flows: Nonparametric generative modeling via optimal transport and diffu-
sions. In ICML, 2019.
Bertrand Maury, Aude Roudneff-Chupin, and Filippo Santambrogio. A macroscopic crowd motion
model of gradient flow type. Mathematical Models and Methods in Applied Sciences, 20(10),
2010.
Bertrand Maury, Aude Roudneff-Chupin, Filippo Santambrogio, and Juliette Venel. Handling con-
gestion in crowd motion modeling. arXiv preprint arXiv:1101.4102, 2011.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. In ICLR, 2018.
Youssef Mroueh, Tom Sercu, and Anant Raj. Sobolev descent. In AISTATS, 2019.
Felix Otto. The geometry of dissipative evolution equations: the porous medium equation. 2001.
Danilo Jimenez Rezende and S. Mohamed. Variational inference with normalizing flows. In ICML,
2015.
Benjamin Rhodes, Kai Xu, and Michael U. Gutmann. Telescoping density-ratio estimation. In
NeurIPS, 2020.
Hannes Risken. Fokker-Planck equation. Springer, 1996.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training GANs. In NeurIPS, 2016.
Filippo Santambrogio. {Euclidean, metric, and Wasserstein} gradient flows: an overview. Bulletin
of Mathematical Sciences, 7(1), 2017.
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
In NeurIPS, 2019.
Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. In
NeurIPS, 2020.
11
Published as a conference paper at ICLR 2021
Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density ratio estimation in machine
learning. Cambridge University Press, 2012.
Akinori Tanaka. Discriminator optimal transport. In NeurIPS, 2019.
Ryan Turner, Jane Hung, Eric Frank, Yunus Saatchi, and Jason Yosinski. Metropolis-Hastings
generative adversarial networks. In ICML, 2019.
Cedric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,
2008.
Yan Wu, Jeff Donahue, David Balduzzi, Karen Simonyan, and Timothy Lillicrap. LOGAN: Latent
optimisation for generative adversarial networks. arXiv preprint arXiv:1912.00953, 2020.
12
Published as a conference paper at ICLR 2021
A	Proofs
A.1 Lemma 3.1
Proof. Gradient flows in the Wasserstein space are of the form of the continuity equation (see Am-
brosio et al. (2008), page 281), i.e,
∂tpt + ▽• (ρtv) = 0.	(15)
The velocity field v in Eq. (15) is given by
V(X) = -Vx δδF (ρ),	(16)
where * (P) denotes the first variation of the functional F. The first variation is defined as
dd F (P + εX)	= / δF (ρ)χ,	(17)
dε	ε=0 δ ρ
where X = V 一 P for some V ∈P(Ω).
Let’s derive an expression for the the first variation of F. In the following, we drop the notation for
dependence on X for clarity,

ddε F (ρ+εχ)
ε=0
(P+εχ)log(P+εχ)
ε=0
P P + εχ
I μ
χ+γ
(log(P + εχ) + 1)χ
ε=0
f0
+ γ log(P) + γ
χ.
(18)
(19)
(20)
μ + Y
Substituting 蓊(ρ) in Eq. (16) we get,
V(X) = -Vx
+ γ log(P) + γ
-Vxf0 (μ) -Y vxρ.
Substituting V in Eq. (15) we get the gradient flow,
dtpt - vx ∙ (PtVxf 0 (μ)+pt ρ VxP)=0
∂tρt(χ) -Vx ∙	PtVxf
Y∆xxPt(X) = 0,
(21)
(22)
(23)
(24)
where ∆xx and Vx ∙ denote the Laplace and the divergence operators respectively.	□
A.2 Lemma 3.2
Proof. Let f be an integrable function on X . If Jg has full column rank and g is an injective
function, then we have the following change-of-variables equation (Ben-Israel, 1999; Gemici et al.,
2016),
L f (x)dx = L (f ◦ g)(z)Jdet J>Jg (Z) dz.
(25)
13
Published as a conference paper at ICLR 2021
This implies that the infinitesimal volumes dx and dz are related as dx = det Jg>Jg(z)dz and
the densities PZ (Z) and qχ (x) are related as PZ (Z) = qχ (g(z)) Jdet J> Jg (z). Similarly, PZ (Z)=
qχ(g(Z)) Jdet J>Jg(Z). Finally, the density-ratio PZ(u)/pz(U) at the point U ∈ Z is given by
PZ (U) qX(g (U))Jdet J> Jg(U) qχ (g(u))
---:—-=-----------,	== :—:——.
PZ (U)	qx (g(U)),det J>Jg (U)	qX(g(U))
(26)
□
B A DISCUSSION ON DGfLOW FOR WGAN
We apply DGf low to WGAN models by treating the output from their critics as the logit for the
estimation of density-ratio. However, it is well-known that WGAN critics are not density-ratio
estimators as they are trained to maximize the 1-Wasserstein distance with an unconstrained out-
put. In this section, we provide theoretical justification for the good performance of DGflow on
WGAN models. We show that DGflow is related to the gradient flow of the entropy-regularized
I-WaSSerStein functional FW : P2(Ω) → R,
FW (P), kdSUP≤ι
|
J d (x) μ(x)dx — J d (x) ρ(x)dx + YJ P(X) log ρ(x)dx,
(27)
{^^^^^^^^^
1-Wasserstein distance
}	|------------{------------}
negative entropy
where μ denotes the target density, kd∣∣Lip denotes the Lipschitz constant of the function d.
Let d* be the function that achieves the supremum in Eq. (27). This results in the functional,
FW (P) = / d*(x) μ(χ)dx - / d*(x) P(X)dx + TZ P(X) log P(X)dχ.
(28)
Following a similar derivation as in Appendix A.1, the gradient flow of FW (P) is given by the
following PDE,
∂tPt(x) + Vχ ∙ (PtVχd*(x)) - γ∆χχPt(x) = 0.	(29)
If d* is approximated using the critic (dφ) of WGAN, we get the following gradient flow,
∂tPt(x) + Vχ ∙ (PtVχdφ(x)) - γ∆χχPt(x) = 0,	(30)
which is same as the gradient flow of entropy-regularized f -divergence with f = r logr (i.e., the KL
divergence) when dφ is treated as a density-ratio estimator. The gradient flow of entropy-regularized
f -divergence with f = r log r is simplified below,
∂tPt(x) -	Vχ	∙	(PtVχf 0 (exp(-dφ(x)))) - Y∆χχPt(x) = 0	(31)
∂tPt(x) -	Vχ	∙	(PtVχ (log(exp(-dφ(x))) + 1)) - Y∆χχPt(X)=	0	(32)
∂tPt(x) +	Vχ	∙	(PtVχdφ(x)) - γ∆χχPt(X)= 0.	(33)
The equality of Eq. (30) and Eq. (33) implies that DGflow approximates the gradient flow of the
1-Wasserstein distance when the critic of WGAN is used for density-ratio estimation.
C	Further Discussion on Related Work
Energy-based & Score-based Generative Models DGflow is related to recently proposed
energy-based generative models (Arbel et al., 2021; Deng et al., 2020) — one can view the en-
ergy functions used in these methods as a component that improves some base model. For example,
the Generalized Energy-Based Model (GEBM) (Arbel et al., 2021) jointly trains an implicit gener-
ative model with an energy function and uses Langevin dynamics to sample from the combination
14
Published as a conference paper at ICLR 2021
of the two. Similarly, in Deng et al. (2020), a discriminator that estimates the energy function is
combined with a language model to train an energy-based text-generation model.
Score-based generative modeling (SBGM) (Song & Ermon, 2019; 2020) is another active area of
research closely-related to energy-based models. Noise Conditional Score Network (NCSN) (Song
& Ermon, 2019; 2020), a SBGM, trains a neural network to estimate the score function of a proba-
bility density at various noise levels. Once trained, this score network is used to evolve samples from
noise to the data distribution using Langevin dynamics. NCSN can be viewed as a gradient flow that
refines a sample right from noise to data; however, unlike D Gf low, NCSN is a complete genera-
tive models in itself and not a sample refinement technique that can be applied to other generative
models.
Other Related Work Monte Carlo techniques have been used for improving various components
in generative models, e.g., Grover et al. (2018) proposed Variational Rejection Sampling which per-
forms rejection sampling in the latent space of VAEs to improve the variational posterior and Grover
et al. (2019) used likelihood-free importance sampling for bias correction in generative models . Wu
et al. (2020) proposed Latent Optimization GAN (LOGAN) which optimizes the latent vector as
part of the training process unlike DGf low that refines the latent vector post training.
D	Implementation Details
D.1 2D Datasets
Datasets The 25 Gaussians dataset was constructed by generating 100000 samples from a mixture
of 25 equally likely 2D isotropic Gaussians with means {-4, -2,0, 2,4} X {-4, -2,0,2,4} ⊂ R2
and standard deviation 0.05. Once generated, the data-points were normalized by 2√2 following
Tanaka (2019). The 2DSwissroll dataset was constructed by first generating 100000 samples of the
3D swissroll dataset using make_swiss_roll from ScikitTearn with noise=0.25 and then only
keeping dimensions {0, 2}. The generated samples were normalized by 7.5.
Base Models We trained a WGAN-GP model for both the datasets. The generator was a fully-
connected network with ReLU non-linearities that mapped Z 〜N(0, I2×2) to X ∈ R2. Similarly,
the discriminator was a fully-connected network with ReLU non-linearities that mapped x ∈ R2
to R. We refer the reader to Gulrajani et al. (2017) for the exact network structures. The gradient
penalty factor was set to 10. The models were trained for 10K generator iterations with a batch size
of 256 using the Adam optimizer with a learning rate of 10-4, β1 = 0.5, and β1 = 0.9. We updated
the discriminator 5 times for each generator iteration.
Hyperparameters We ran DOT for 100 steps and performed gradient descent using the Adam
optimizer with a learning rate of 0.01 and β = (0., 0.9) as suggested by Tanaka (2019). DDLS was
run for 50 iterations with a step-size of 0.01 and the Gaussian noise was scaled by a factor of 0.1 as
suggested by Che et al. (2020). For DGf low, we set the step-size η = 0.01, the number of steps
n = 100, and the noise regularizer γ = 0.01. We used the output from the WGAN-GP discriminator
directly as a logit for estimating the density ratio for DDLS and DGf low.
Metrics We compared the different methods quantitatively on two metrics: % high quality samples
and kernel density estimate (KDE) score. A sample is classified as a high quality sample if it lies
within 4 standard deviations of its nearest Gaussian. The KDE score is computed by first estimating
the KDE using generated samples and then computing the log-likelihood of the training samples
under the KDE estimate. KDE was performed using sklearn.neighbors.KernelDensity with a
Gaussian kernel and a kernel bandwidth of 0.1. The quantitative metrics were averaged over 10 runs
with 5000 samples from each method.
D.2 Image Experiments
Datasets CIFAR10 (Krizhevsky et al., 2009) is a dataset of 60K natural RGB images of size
32 × 32 from 10 classes. STL10 is a dataset of 100K natural RGB images of size 96 × 96 from 10
15
Published as a conference paper at ICLR 2021
Table 6: Network architectures used for MMDGAN and VAE models.
(a) Generator or Decoder	(b) Discriminator or Encoder
Input Shape: (b, d, 1, 1)	InputShape: (b, 3, 32, 32)
UPconv(256)	Conv(64)
BatchNorm	LeakyReLU(0.2)
ReLU	Conv(128)
UPconv(128)	BatchNorm
BatchNorm	LeakyReLU(0.2)
ReLU	Conv(256)
UPconv(64)	BatchNorm
BatchNorm	LeakyReLU(0.2)
ReLU	Conv(m)
UPconv(3) Tanh	Output Shape: (b, m, 1, 1)
Output Shape: (b, 3, 32, 32)
classes. We resized the STL10 (Coates et al., 2011) dataset to 48 × 48 for SNGAN and WGAN-
GP, and to 32 × 32 for MMDGAN, OCFGAN-GP, and VAE since the respective base models were
trained on these sizes.
Base Models for CIFAR10 We used the publicly available pre-trained models for WGAN-GP,
SN-DCGAN (hi), and SN-DCGAN (ns). We refer the reader to Tanaka (2019) for exact details
about these models. For SN-ResNet-GAN and OCFGAN-GP we used the pre-trained models from
Miyato et al. (2018) and Ansari et al. (2020) respectively. We used the respective discriminators of
SN-DCGAN (ns), SN-DCGAN (hi), and WGAN-GP for density-ratio estimation when refining their
generators. For the SN-ResNet-GAN (hi) generator, we used SN-DCGAN (ns) discriminator as the
non-saturating loss provides a better density-ratio estimation than a discriminator trained using the
hinge loss.
We trained our own models for MMDGAN, VAE, and Glow. We used the generator and dis-
criminator architectures shown in Table 6 for MMDGAN with d = 32. VAE used the same
architecture with d = 64. Our Glow model was trained using the code available at https:
//github.com/y0ast/Glow-PyTorch with a batch size of 56 for 150 epochs. The density ratio
correctors, Dλ (see section 3.2), were initialized with the weights from the SN-DCGAN (ns) re-
leased by Tanaka (2019). Dλ was then fine-tuned on images from SN-DCGAN (ns)’s generator and
the generator being improved (e.g., MMDGAN and OCFGAN-GP) using SGD with a learning rate
of 10-4 and momentum of 0.9. We fine-tuned Dλ for 10000 iterations with a batch size of 64.
Base Models for STL10 We used the publicly available pre-trained models (Tanaka, 2019; Ansari
et al., 2020) for WGAN-GP, SN-DCGAN (hi), SN-DCGAN (ns), and OCFGAN-GP. We trained our
own models for MMDGAN and VAE with the same architecture and training details as CIFAR10.
We fine-tuned the density ratio correctors for STL10 for 5000 iterations with other details being the
same as CIFAR10.
Hyperparameters We performed 25 updates of DGf low for CIFAR10 and STL10 with a step
size of 0.1 for models that do not require density ratio corrections. For STL10 models that require
a density ratio correction, we performed 15 updates with a step size of 0.05. The noise regularizer
(γ), whenever used, was set to 0.01.
Metrics We used the Frechet Inception Distance (FID) (HeUsel et al., 2017) and Inception Score
(IS) (Salimans et al., 2016) metrics to evaluate the quality of generated samples before and after re-
finement. The IS denotes the confidence in classification of the generated samples using a pretrained
InCePtionV3 network whereas the FID is the FreChet distance between multivariate Gaussians fitted
to the 2048 dimensional feature vectors extracted from the InceptionV3 network for real and gen-
erated data. Both the metrics were computed using 50K samples for all the models, except Glow
16
Published as a conference paper at ICLR 2021
Table 7: f -divergences and their derivatives.
f -divergence	f	f0	f00
KL	r log r	log r + 1	1
JS	r log r — (r + 1) log r++1	log r2+r1	r 1
log D	(r + 1) log(r + 1) - 2 log 2	log(r + 1) + 1	r2 +r 1 r + 1
where we used 10K samples. Following Tanaka (2019), we used the entire training and test set (60K
images) for CIFAR10 and the entire unlabeled set (100K images) for STL10 as the set of real images
used to compute FID.
D.3 Character Level Language Modeling
Dataset We used the Billion Words dataset (Chelba et al., 2013) which was pre-processed into
32-character long strings.
Base Model Our generator was a 1D CNN which followed the architecture used by Gulrajani et al.
(2017).
Hyperparameters We performed 50 updates of DGf low with a step size of 0.1 and noise factor
γ = 0.
Metrics The JS-4 and JS-6 scores were computed using the code provided by Gulrajani et al.
(2017) at https://github.com/igul222/improved_wgan_training. We used 10000 samples
from the models to compute the JS-4 score.
E Additional Results
GAN
GAN	DGflOW(KL)	DGHOW(IOgD)	DGHOWQS)
Figure 4: Qualitative comparison of DGflow with different f -divergences on the 25Gaussians and 2DSwissroll
datasets.
Fig. 4 shows the samples generated by WGAN-GP (leftmost, blue) and refined samples gener-
ated using DGflow with different f -divergences (red). Fig. 5 shows the deterministic compo-
nent, -Vxf(P0∕μ)(x0), of the velocity for different f-divergences on the 2D datasets. Fig. 6
(right) shows the latent space distribution recovered by DGf low when applied in the latent space
for the 2D datasets. This latent space is same as the one derived by Che et al. (2020), i.e.,
pt(z) 8 PZ(Z) exp(d(gθ(Z))) which is shown in Fig. 6 (left) for both datasets.
Table 11 shows the comparison of DGflow with DOT in terms of the inception score for the CI-
FAR10 and STL10 datasets. DGf low outperforms DOT significantly for all the base GAN mod-
els on both the datasets. Table 12 compares different variants of D Gf low applied to MMDGAN,
OCFGAN-GP, VAE, and Glow generators in terms of the inception score. DGflow leads to a
17
Published as a conference paper at ICLR 2021
KL
Figure 5: A vector plot showing the deterministic component of the velocity, i.e., the drift -Vxf 0(ρ0/μ)(x0),
for different f -divergences on the 25Gaussians and 2DSwissroll dataset.
Table 8: Runtime comparison of DOT, DDLS, and DGflow(KL) on the 25Gaussians dataset. The runtime is
averaged over 100 runs with standard deviation reported in parentheses.
Method	Runtime (s) per 5K samples
DOT	2.24 (0.18)
DDLS	2.23 (0.14)
DGflow	2.22 (0.15)
significant improvement in the quality of samples for all the models. Tables 13 and 14 compare
the deterministic variant of DGflow (γ = 0) against DOT and DDLS. These results show that
the diffusion term only serves as an enhancement for DGflow, not a necessity, and it outper-
forms competing methods even without added noise. Table 15 shows the results of DGflow on
MMDGAN, OCFGAN-GP, and VAE models when the SN-DCGAN (ns) discriminator is directly
used as a density-ratio estimator without an additional density-ratio corrector. Figures 7, 8, 9, and 10
show the samples generated by the base model (left) and the refined samples (right) using DGflow
for the CIFAR10 and STL10 datasets.
Runtime DGflow performs a backward pass through
dφ ◦ gθ to compute the gradient of the density-ratio with
respect to the latent vector. This results in the same
runtime complexity as that of DOT and DDLS. Table 8
shows a comparison of the runtimes of DOT, DDLS, and,
DGflow on the 25Gaussians dataset under same condi-
tions. As expected, these refinement methods have sim-
ilar runtimes in practice. The wall-clock time required
for DGflow(KL) to refine 100 samples from different base
models on the CIFAR10 and STL10 datasets is reported
in tables 9 and 10.
(a) 25 Gaussians
(b) 2D Swissroll
Figure 6: Latent space recovered by
DGflow (right) for the 2D datasets is same
as the one derived by Che et al. (2020) (left).
18
Published as a conference paper at ICLR 2021
Table 9: Runtime of DGf low(KL) for models that do not require density-ratio correction on a single GeForce
RTX 2080 Ti GPU. The runtime is averaged over 100 runs with standard deviation reported in parentheses.
02≤u 一 OEJS
Model	Runtime (s) per 100 samples
WGAN-GP	0.897(0.017)
SN-DCGAN (hi)	0.952 (0.008)
SN-DCGAN (ns)	0.952 (0.007)
SN-ResNet-GAN	1.982 (0.013)
WGAN-GP	1.376(0.025)
SN-DCGAN (hi)	1.413 (0.015)
SN-DCGAN (ns)	1.415 (0.013)
Table 10: Runtime of DGf low(KL) for models that require density-ratio correction on a single GeForce RTX
2080 Ti GPU. The runtime is averaged over 100 runs With standard deviation reported in parentheses.
Model	Runtime (s) per 100 samples
MMDGAN	1.192 (0.007)
OCFGAN-GP	1.186 (0.011)
VAE	1.186 (0.012)
MMDGAN	1.036 (0.004)
OCFGAN-GP	1.029 (0.010)
VAE	1.028 (0.011)
Table 11: Comparison of different variants of DGf loW With DOT on the CIFAR10 and STL10 datasets. Higher
scores are better.
02≤o - 0-UH∞
Inception Score
Base Model DOT DGfloW(KL)	DGfloW(JS)	DGfloWaog D)
Model
WGAN-GP	6.51 (.02)	7.45	7.99 (.02)	7.71 (.02)	7.11 (.03)
SN-DCGAN (hi)	7.35 (.03)	8.02	8.13 (.02)	7.98 (.01)	7.85 (.02)
SN-DCGAN (ns)	7.38 (.03)	7.97	8.14 (.03)	7.98 (.04)	7.94 (.01)
SN-ResNet-GAN	8.38 (.03)	—	9.35 (.03)	9.13 (.04)	9.05 (.03)
WGAN-GP	8.72 (.02)	9.31	10.41 (.02)	8.85 (.06)	9.80 (.03)
SN-DCGAN (hi)	8.77 (.03)	9.35	9.74 (.04)	9.50 (.05)	9.41 (.07)
SN-DCGAN (ns)	8.61 (.04)	9.45	9.66 (.01)	9.49 (.03)	9.18 (.03)
Table 12: Comparison of different variants of DGf loW applied to MMDGAN, OCFGAN-GP, VAE, and GloW
models. Higher scores are better.
Inception Score
Model	__________________________p_______________________________
Base Model	DGf low(KL)	DGf low(JS)	DGf low(log D)
02≤o - 02≤o
MMDGAN	5.74 (.02)	6.27 (.05)	5.99 (.03)	6.02 (.01)
OCFGAN-GP	6.52 (.02)	7.21 (.05)	6.93 (.03)	6.92 (.03)
VAE	3.20 (.01)	3.85 (.01)	3.21 (.02)	3.57 (.02)
GloW	3.64 (.02)	4.57 (.02)	3.91 (.03)	4.47 (.03)
MMDGAN	6.07 (.02)	6.16 (.01)	6.12 (.03)	6.12 (.03)
OCFGAN-GP	7.09 (.01)	7.46 (.04)	7.10 (.03)	7.33 (.02)
VAE	3.25 (.01)	3.72 (.04)	3.27 (.01)	3.65 (.03)
19
Published as a conference paper at ICLR 2021
Table 13: Comparison of different variants of DGf loW Without diffusion (i.e., γ = 0) on the CIFAR10 and
STL10 datasets. LoWer scores are better.
≡dvho 一≡TLS
Model
Frechet Inception Distance
Base Model DOT	DGfloW(KL)	DGfloW(JS)	DGf low(iog D)
WGAN-GP	28.34 (.11)	24.14	24.64 (.13)	23.30 (.11)	24.42 (.19)
SN-DCGAN (hi)	20.67 (.09)	17.12	15.79 (.07)	16.79 (.09)	17.79 (.05)
SN-DCGAN (ns)	20.94 (.12)	15.78	15.47 (.11)	16.32 (.11)	16.97 (.08)
WGAN-GP	51.34 (.21)	44.45	38.96 (.08)	50.44 (.09)	39.35 (.12)
SN-DCGAN (hi)	40.82 (.16)	34.85	35.18 (.09)	36.53 (.13)	36.75 (.13)
SN-DCGAN (ns)	41.83 (.20)	34.84	34.81 (.08)	35.75 (.10)	37.68 (.08)
Table 14: Comparison of DDLS with DGf low (with and without diffusion) on the CIFAR10 dataset. Higher
scores are better.
Model	Inception Score
SN-ResNet-GAN (Miyato et al., 2018)	8.22 (.05)
SN-ResNet-GAN + DDLS (cal) (Che et al., 2020)	9.09 (.10)
SN-ReSNet-GAN (our evaluation)	8.38 (.03)
SN-ResNet-GAN + DGf loW(KL) (γ = 0)	9.35 (.04)
SN-ResNet-GAN + DGf loW(KL) (γ = 0.01)	9.35 (.03)
BigGAN	9.22
Table 15: Comparison of different variants of DGf loW applied to MMDGAN, OCFGAN-GP, and VAE models
Without density-ratio correction. LoWer scores are better.
Model	FreChet Inception Distance
Base Model KL	JS	log D
02≤u 一 OlTLS
MMDGAN	42.03 (.06)	39.06 (.08)	39.68 (.06)	39.47 (.07)
OCFGAN-GP	31.95 (.07)	27.92 (.08)	29.25 (.06)	28.82 (.10)
VAE	129.49 (.19)	127.50 (.15)	128.24 (.11)	128.3 (.14)
GloW	100.7 (.14)	93.47 (.09)	97.50 (.11)	97.78 (.14)
MMDGAN	47.22 (.04)	45.75(.10)	45.96 (.07)	46.26 (.13)
OCFGAN-GP	36.60 (.15)	34.17 (.18)	34.42 (.04)	34.99 (.07)
VAE	150.49 (.07)	151.76 (.01)	152.03 (.05)	151.88 (.11)
20
Published as a conference paper at ICLR 2021
(a) WGAN-GP	(b) WGAN-GP + DGf low
(c) SN-DCGAN (hi)
(d) SN-DCGAN (hi) + DGf low
(e) SN-DCGAN (ns)
(f) SN-DCGAN (ns) + DGflow
(g) SN-ResNet-GAN
(h) SN-ResNet-GAN + DGf low
Figure 7:	Samples from different models for the CIFAR10 dataset before (left) and after (right) refinement
using DGf low.
21
Published as a conference paper at ICLR 2021
(a) MMDGAN
(b) MMDGAN + DGf low
(c) OCFGAN-GP
(d) OCFGAN-GP + DGf low
(e) VAE
(g) Glow
Figure 8:	Samples from different models for the CIFAR10 dataset before (left) and after (right) refinement
using DGf low.
22
Published as a conference paper at ICLR 2021
(a) WGAN-GP
(b) WGAN-GP + DGf low
(c) SN-DCGAN (hi)
(d) SN-DCGAN (hi) + DGf low
(e) SN-DCGAN (ns)
(f) SN-DCGAN (ns) + DGf low
Figure 9:	Samples from different models for the STL10 dataset before (left) and after (right) refinement using
DGf low.
23
Published as a conference paper at ICLR 2021
(a) MMDGAN
(b) MMDGAN + DGf low
(c) OCFGAN-GP
(d) OCFGAN-GP + DGf low
(e) VAE
(f) VAE + DGf low
Figure 10:	Samples from different models for the STL10 dataset before (left) and after (right) refinement using
DGf low.
24