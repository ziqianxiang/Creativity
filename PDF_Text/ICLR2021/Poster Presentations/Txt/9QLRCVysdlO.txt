Published as a conference paper at ICLR 2021
BiPointNet: Binary Neural Network
for Point Clouds
Haotong Qin* 1,2, Zhongang Cai*3, Mingyuan Zhang*3, Yifu Ding1, Haiyu Zhao3,
Shuai Yi3, Xianglong LiuM, Hao Su4
1	State Key Lab of Software Development Environment, Beihang University
2	Shen Yuan Honors College, Beihang University 3 SenseTime Research
4University of California, San Diego
{qinhaotong,xlliu}@nlsde.buaa.edu.cn, zjdyf@buaa.edu.cn
{caizhongang, zhangmingyuan, zhaohaiyu, yishuai}@sensetime.com
haosu@eng.ucsd.edu
Ab stract
To alleviate the resource constraint for real-time point cloud applications that run
on edge devices, in this paper we present BiPointNet, the first model binarization
approach for efficient deep learning on point clouds. We discover that the im-
mense performance drop of binarized models for point clouds mainly stems from
two challenges: aggregation-induced feature homogenization that leads to a degra-
dation of information entropy, and scale distortion that hinders optimization and
invalidates scale-sensitive structures. With theoretical justifications and in-depth
analysis, our BiPointNet introduces Entropy-Maximizing Aggregation (EMA) to
modulate the distribution before aggregation for the maximum information en-
tropy, and Layer-wise Scale Recovery (LSR) to efficiently restore feature repre-
sentation capacity. Extensive experiments show that BiPointNet outperforms ex-
isting binarization methods by convincing margins, at the level even comparable
with the full precision counterpart. We highlight that our techniques are generic,
guaranteeing significant improvements on various fundamental tasks and main-
stream backbones. Moreover, BiPointNet gives an impressive 14.7× speedup and
18.9× storage saving on real-world resource-constrained devices.
1	Introduction
With the advent of deep neural networks that directly process raw point clouds (PointNet (Qi et al.,
2017a) as the pioneering work), great success has been achieved in learning on point clouds (Qi et al.,
2017b; Li et al., 2018; Wang et al., 2019a; Wuet al., 2019; Thomas et al., 2019; Liu et al., 2019b;
Zhang et al., 2019b). Point cloud applications, such as autonomous driving and augmented reality,
often require real-time interaction and fast response. However, computation for such applications is
usually deployed on resource-constrained edge devices. To address the challenge, novel algorithms,
such as Grid-GCN (Xu et al., 2020b), RandLA-Net (Hu et al., 2020), and PointVoxel (Liu et al.,
2019d), have been proposed to accelerate those point cloud processing networks. While significant
speedup and memory footprint reduction have been achieved, these works still rely on expensive
floating-point operations, leaving room for further optimization of the performance from the model
quantization perspective. Model binarization (Rastegari et al., 2016; Bulat & Tzimiropoulos, 2019;
Hubara et al., 2016; Wang et al., 2020; Zhu et al., 2019; Xu et al., 2019) emerged as one of the
most promising approaches to optimize neural networks for better computational and memory usage
efficiency. Binary Neural Networks (BNNs) leverage 1) compact binarized parameters that take
small memory space, and 2) highly efficient bitwise operations which are far less costly compared
to the floating-point counterparts.
Despite that in 2D vision tasks (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; Szegedy
et al., 2015; Girshick et al., 2014; Girshick, 2015; Russakovsky et al., 2015; Wang et al., 2019b;
* equal contributions
,corresponding author
1
Published as a conference paper at ICLR 2021
Entropy-Maximizmg Aggregation
Layer-wise Scale Recovery
aggregation
unit
—►
上 中
aggregated feature
2m"ωJ~ndu!
pωJu!q
IUXU
Θ
binarized weight
learnable layer-wise
scaling factor
original feature	output feature
input	BiMLPS	feature
BiMLPS
EMA-max
, transformation
unit
人，丁
input feature
output feature
Figure 1: Overview of our BiPointNet on PointNet base model, applying Entropy-Maximizing Ag-
gregation (EMA) and Layer-wise Scale Recovery (LSR). EMA consists of the transformation unit
and the aggregation unit for maximizing the information entropy of feature after binarization. LSR
with the learnable layer-wise scaling factor α is applied to address the scale distortion of bi-linear
layers (which form the BiMLPs), flexibly restore the distorted output to reasonable values
Zhang et al., 2021) has been studied extensively by the model binarization community, the methods
developed are not readily transferable for 3D point cloud networks due to the fundamental differ-
ences between 2D images and 3D point clouds. First, to gain efficiency in processing unordered 3D
points, many point cloud learning methods rely heavily on pooling layers with large receptive field
to aggregate point-wise features. As shown in PointNet (Qi et al., 2017b), global pooling provides
a strong recognition capability. However, this practice poses challenges for binarization. Our analy-
ses show that the degradation of feature diversity, a persistent problem with binarization (Liu et al.,
2019a; Qin et al., 2020b; Xie et al., 2017), is significantly amplified by the global aggregation func-
tion (Figure 2), leading to homogenization of global features with limited discriminability. Second,
the binarization causes immense scale distortion at the point-wise feature extraction stage, which
is detrimental to model performance in two ways: the saturation of forward-propagated features
and backward-propagated gradients hinders optimization, and the disruption of the scale-sensitive
structures (Figure 3) results in the invalidation of their designated functionality.
In this paper, we provide theoretical formulations of the above-mentioned phenomenons and obtain
insights through in-depth analysis. Such understanding allows us to propose a method that turns full-
precision point cloud networks into extremely efficient yet strong binarized models (see the overview
in Figure 1). To tackle the homogenization of the binarized features after passing the aggregation
function, we study the correlation between the information entropy of binarization features and the
performance of point cloud aggregation functions. We thus propose Entropy-Maximizing Aggrega-
tion (EMA) that shifts the feature distribution towards the statistical optimum, effectively improving
expression capability of the global features. Moreover, given maximized information entropy, we
further develop Layer-wise Scale Recovery (LSR) to efficiently restore the output scale that enhances
optimization, which allows scale-sensitive structures to function properly. LSR uses only one learn-
able parameter per layer, leading to negligible storage increment and computation overhead.
Our BiPointNet is the first binarization approaches to deep learning on point clouds, and it outper-
forms existing binarization algorithms for 2D vision by convincing margins. It is even almost on
par (within 〜1-2%) with the full-precision counterpart. Although We conduct most analysis on the
PointNet baseline, we show that our methods are generic and can be readily extendable to other pop-
ular backbones, such as PointNet++ (Qi et al., 2017b), PointCNN (Li et al., 2018), DGCNN (Wang
et al., 2019a), and PointConv (Wu et al., 2019), which are the representatives of mainstream cate-
gories of point cloud feature extractors. Moreover, extensive experiments on multiple fundamental
tasks on the point cloud, such as classification, part segmentation, and semantic segmentation, high-
light that our BiPointNet is task-agnostic. Besides, we highlight that our EMA and LSR are efficient
and easy to implement in practice: in the actual test on popular edge devices, BiPointNet achieves
14.7× speedup and 18.9× storage savings compared to the full-precision PointNet. Our code is
released at https://github.com/htqin/BiPointNet.
2	Related Work
Network Binarization. Recently, various quantization methods for neural networks have emerged,
such as uniform quantization (Gong et al., 2019; Zhu et al., 2020), mixed-precision quantization (Wu
2
Published as a conference paper at ICLR 2021
et al., 2018; Yu et al., 2020), and binarization. Among these methods, binarization enjoys compact
binarized parameters and highly efficient bitwise operations for extreme compression and accelera-
tion (Rastegari et al., 2016; Qin et al., 2020a). In general, the forward and backward propagation of
binarized models in the training process can be formulated as:
Forward : b= sign(x) = +1, ifx ≥ 0 Backward : gx = gb, ifx ∈ (-1, 1) (1)
-1, otherwise	0,	otherwise
where x denotes the element in floating-point weights and activations, b denotes the element in
binarized weights BW and activations Ba. gχ, and gb donate the gradient ⅞f and 喙,respectively,
where C is the cost function for the minibatch. In forward propagation, sign function is directly
applied to obtain the binary parameters. In backward propagation, the Straight-Through Estimator
(STE) (Bengio et al., 2013) is used to obtain the derivative of the sign function, avoiding getting all
zero gradients. The existing binarization methods are designed to obtain accurate binarized networks
by minimizing the quantization error (Rastegari et al., 2016; Zhou et al., 2016; Lin et al., 2017),
improving loss function (Ding et al., 2019; Hou et al., 2017), reducing the gradient error (Liu et al.,
2018; 2020), and designing novel structures and pipelines (Martinez et al., 2020). Unfortunately, we
show in Sec 3 that these methods, designed for 2D vision tasks, are not readily transferable to 3D
point clouds.
Deep Learning on Point Clouds. PointNet (Qi et al., 2017a) is the first deep learning model that
processes raw point clouds directly. The basic building blocks proposed by PointNet such as MLP
for point-wise feature extraction and max pooling for global aggregation (Guo et al., 2020) have
become the popular design choices for various categories of newer backbones: 1) the pointwise
MLP-based such as PointNet++ (Qi et al., 2017b); 2) the graph-based such as DGCNN (Xu et al.,
2020b); 3) the convolution-based such as PointCNN (Li et al., 2018), PointConv (Wu et al., 2019)
RS-CNN (Liu et al., 2019c) and KP-Conv (Thomas et al., 2019). Recently, methods are proposed
for efficient deep learning on point clouds through novel data structuring (Xu et al., 2020b), faster
sampling (Hu et al., 2020), adaptive filters (Xu et al., 2020a), efficient representation (Liu et al.,
2019d) or convolution operation (Zhang et al., 2019b) . However, they still use expensive floating-
point parameters and operations, which can be improved by binarization.
3	Methods
Binarized models operate on efficient binary parameters, but often suffer large performance drop.
Moreover, the unique characteristics of point clouds pose even more challenges. We observe there
are two main problems: first, aggregation of a large number of points leads to a severe loss of
feature diversity; second, binarization induces an immense scale distortion, that undermines the
functionality of scale-sensitive structures. In this section, we discuss our observations, and propose
our BiPointNet with theoretical justifications.
3.1	B inarization Framework
We first give a brief introduction to our framework that binarizes a floating-point network. For
example, deep learning models on point clouds typically contain multi-layer perceptrons (MLPs)
for feature extraction. In contrast, the binarized models contain binary MLPs (BiMLPs), which
are composed of binarized linear (bi-linear) layers. Bi-linear layers perform the extremely efficient
bitwise operations (XNOR and Bitcount) on the lightweight binary weight/activation. Specifically,
the activation of the bi-linear layer is binarized to Ba, and is computed with the binarized weight
BW to obtain the output Z:
Z=BaBW,	(2)
where denotes the inner product for vectors with bitwise operations XNOR and Bitcount. When
Bw and Ba denote the random variables in BW and Ba, we represent their probability mass function
as pBw (bw), andpBa(ba).
Moreover, we divide the BiPointNet into units for detailed discussions. In BiPointNet, the original
data or feature X ∈ Rn×c first enters the symmetric function Ω, which represents a composite
function built by stacking several permutation equivariant and permutation invariant layers (e.g.,
nonlinear layer, bi-linear layer, max pooling). And then, the output Y ∈ Rn×k is binarized to obtain
3
Published as a conference paper at ICLR 2021
(a) POmt-WiSe features to be aggregated.
(b) FUll-PreciSiOn features aggregated
with max pooling.
(c) Bmanzed features aggregated with
max pooling.
* I IIll
mil
(d) Binanzed features aggregated with
EMA.
Figure 2: Aggregation-induced feature homogenization. (a) shows the activation of each test sample
in a batch of ModelNet40. In (b)-(d), the single feature vectors pooled from all points are mapped to
colors. The diversity of colors represents the diversity of pooled features. The original aggregation
design is incompatible with binarization, leading to the homogenization of output features in (c),
whereas our proposed EMA retains high information entropy, shown in (d)
the binary feature B ∈ {-1, 1}i×k, where i takes n when the feature is modeled independently and
takes 1 when the feature is aggregated globally. The single unit is thus represented as
B = Sign(Y) = sign(Ω(X)).	(3)
Similarly, when B, Y and X denote the random variables sampled from B, Y and X, we represent
their probability mass function as pB (b), pY (y) and pX (x).
3.2	Entropy-maximizing Aggregation
Unlike images pixels that are arranged in regular lattices, point clouds are sets of points without
any specific order. Hence, features are usually processed in a point-wise manner and aggregated
explicitly through pooling layers. Our study shows that the aggregation function is a performance
bottleneck of the binarized model, due to severe homogenization as shown in Figure 2.
We apply information theory (Section 3.2.1) to quantify the effect of the loss of feature diversity,
and find that global feature aggregation leads to a catastrophic loss of information entropy. In Sec-
tion 3.2.2, we propose the concept of Entropy-Maximizing Aggregation (EMA) that gives the sta-
tistically maximum information entropy to effectively tackle the feature homogenization problem.
3.2.1	Aggregation-induced Feature Homogenization
Ideally, the binarized tensor B should reflect the information in the original tensor Y as much as
possible. From the perspective of information, maximizing mutual information can maximize the
information flow from the full-precision to the binarized parameters. Hence, our goal is equivalent
to maximizing the mutual information I(Y ; B) of the random variables Y and B:
arg max I(Y;B) = H(B) -H(B | Y)	(4)
Y,B
where H(B) is the information entropy, and H(B | Y ) is the conditional entropy of B given Y .
H(B | Y ) = 0 as we use the deterministic sign function as the quantizer in binarization (see Section
A.1 for details). Hence, the original objective function Eq. (4) is equivalent to:
arg max HB(B) = -	pB(b) logpB(b),	(5)
B	b∈B
where B is the set of possible values of B . We then study the information properties of max pool-
ing, which is a common aggregation function used in popular point cloud learning models such as
PointNet. Let the max pooling be the last layer φ of the multi-layer stacked Ω, and the input of φ
is defined as Xφ. The data flow of Eq. (3) can be further expressed as B = Sign(φ(Xφ)), and the
information entropy HB of binarized feature B can be expressed as
HB(Xφ) = -	P pXφ(xφ)nlog P pXφ(xφ)n - 1 - P pXφ(xφ)n log 1 - P pXφ(xφ)n (6)
xφ≥0	xφ≥0	xφ≥0	xφ≥0
where n is the number of elements aggregated by the max pooling, and Xφ is the random variable
sampled from Xφ. The brief derivation ofEq (6) is shown in Appendix A.2. Theorem 1 shows the
information properties of max pooling with the normal distribution input on the binarized network
architecture.
4
Published as a conference paper at ICLR 2021
Theorem 1 For input Xφ of max pooling φ with arbitrary distribution, the information entropy of
the binarized output goes to zero as n goes to infinity, i.e., lim HB = 0. And there exists a
n→+∞
constant c, for any n1 and n2, if n1 > n2 > c, we have HB,n1 < HB,n2, where n is the number of
elements to be aggregated.
The proof of Theorem 1 is included in Appendix A.2, which explains the severe feature homoge-
nization after global feature pooling layers. As the number of points is typically large (e.g. 1024
points by convention in ModelNet40 classification task), it significantly reduces the information en-
tropy HB of binarized feature B, i.e., the information ofY is hardly retained in B, leading to highly
similar output features regardless of the input features to pooling layer as shown in Figure 2.
Furthermore, Theorem 1 provides a theoretical justification for the poor performance of existing
binarization methods, transferred from 2D vision tasks to point cloud applications. In 2D vision, the
aggregation functions are often used to gather local features with a small kernel size n (e.g. n = 4 in
ResNet (He et al., 2016; Liu et al., 2018) and VGG-Net (Simonyan & Zisserman, 2014) which use
2 × 2 pooling kernels). Hence, the feature homogenization problem on images is not as significant
as that on point clouds.
3.2.2	EMA for Maximum Information Entropy
Therefore, we need a class of aggregation functions that maximize the information entropy of B to
avoid the aggregation-induced feature homogenization.
We study the correlation between the information entropy HB of binary random variable B and
the distribution of the full-precision random variable Y . We notice that the sign function used in
binarization has a fixed threshold and decision levels, so we get Proposition 1 about information
entropy of B and the distribution of Y .
Proposition 1 When the distribution of the random variable Y satisfies	y<0 pY (y)
y≥0 pY (y) = 0.5, the information entropy HB is maximized.
The proof of Proposition 1 is shown in Appendix A.3. Therefore, theoretically, there is a distribution
of Y that can maximize the mutual information of Y and B by maximizing the information entropy
of the binary tensor B, so as to maximally retain the information of Y in B.
To maximize the information entropy HB, we propose the EMA for feature aggregation in BiPoint-
Net. The EMA is not one, but a class of binarization-friendly aggregation layers. Modifying the
aggregation function in the full-precision neural network to a EMA keeps the entropy maximized by
input transformation. The definition of EMA is
Y = EMA(Xφ) = HT (Xφ)),	(7)
where 夕 denotes the aggregation function (e.g. max pooling and average pooling) and T denotes the
transformation unit. Note that a standard normal distribution N(0, 1) is assumed for Xφ because
batch normalization layers are placed prior to the pooling layers by convention. T can take many
forms; we discover that a simple constant offset is already effective. The offset shifts the input so
that the output distribution satisfies Py<0 pY (y) = 0.5, to maximize the information entropy of
binary feature B. The transformation unit T in our BiPointNet can be defined as T(Xφ) = Xφ - δ*.
When max pooling is applied as 夕，We obtain the distribution offset δ* for the input Xφ that maxi-
mizes the information entropy HB by solving the objective function
arg max HB (δ) = - ( X -4= δ	2π xφ≥0	-(xΦ-δ)2、n	/ L	1	- (xΦ-δ)2、n e	2	)log( XeI √2∏e-	2	) xφ≥0 2	2	(8)
-(1-(X xφ≥0	1	(xΦ-δ)、n、,	(	1 „	1	(xφ-δ)、n、, √2∏e-	2	))log(1-(XcI √2∏e-	2	))， xφ≥0
where n denotes the number of elements in each batch. For each n, we can obtain an optimized
δmaχ for Eq. (8), we include the pseudo code in the Appendix A.5.
Moreover, we derive in the Appendix A.6 that when average pooling is used as 夕，the solution to
its objective function is expressed as δ = 0. We thus obtain δ*g = 0. This means the solution
5
Published as a conference paper at ICLR 2021
Figure 4: (a) Information entropy of the aggregated features. With EMA, our BiPointNet achieves a
higher information entropy. (b) Regularizer loss comparison. Our PointNet has a low loss, indicating
that the scale distortion is reduced and T-Net is not disrupted. (c) Ratio of zero-gradient activations
in back-propagation. LSR alleviates the scale distortion, enhancing the optimization process
is not related to n. Hence, average pooling can be regarded as a flexible alternative because its
performance is independent of the input number n.
In a nutshell, We provide two possible variants of g：first, We show that a simple shift is sufficient
to turn a max pooling layer into an EMA (EMA-max); second, average pooling can be directly used
(EMA-avg) without modification as a large number of points does not undermine its information
entropy, making it adaptive to the dynamically changing number of point input. Note that modi-
fying existing aggregation functions is only one way to achieve EMA; the theory also instructs the
development of new binarization-friendly aggregation functions in the future.
3.3	One-scale-fits-all: Layer-wise Scale Recovery
In this section, we show that binarization leads to feature scale distortion and study its cause. We
conclude that the distortion is directly related to the number of feature channels. More importantly,
we discuss the detriments of scale distortion from the perspectives of the functionality of scale-
sensitive structures and the optimization.
To address the severe scale distortion in feature due to binarization, we propose the Layer-wise Scale
Recovery (LSR). In LSR, each bi-linear layer is added only one learnable scaling factor to recover
the original scales of all binarized parameters, with negligible additional computational overhead
and memory usage.
3.3.1	Scale Distortion
The scale of parameters is defined as the standard deviation σ of
their distribution. As we mentioned in Section 3.2, balanced bina-
rized weights are used in the bi-linear layer aiming to maximize
the entropy of the output after binarization, i.e., pBw (1) = 0.5
and pBa (1) = 0.5.
Theorem 2 When we let pBw (1) = 0.5 and pBa (1) = 0.5
in bi-linear layer to maximize the mutual information, for the
binarized weight Bw ∈ {-1, +1}m×k and activation Ba ∈
{-1, +1}n×m, the probability mass function for the distribution
of output Z can be represented as pZ (2i - m) = 0.5mCmi , i ∈
{0, 1, 2, ..., m}. The output has approximately a normal distribu-
tion N (0, m).
The proof of Theorem 2 is found in Appendix A.4. Theorem 2
shows that given the maximized information entropy, the scale
of the output features is directly related to the number of feature
channels. Hence, scale distortion is pervasive as a large number
of channels is the design norm of deep learning neural networks
for effective feature extraction.
Λ
(a)	Input point
cloud
τ∖
(b)	PointNet
-.√
-1■二.'
—尹
(d) BNN
Figure 3: Scale Distortion. Fig-
ures (b)-(d) show the trans-
formed input. Compared with
the input (a), the scales of (b) in
full-precision PointNet and (c)
in our BiPointNet are normal,
while the scale of (d) in BNN is
significantly distorted
We discuss two major impacts of the scale distortion on the performance of binarized point cloud
learning models. First, the scale distortion invalidates structures designed for 3D deep learning that
6
Published as a conference paper at ICLR 2021
Table 1: Ablation study for our BiPointNet of various tasks on ModelNet40 (classification),
ShapeNet Parts (part segmentation), and S3DIS (semantic segmentation). EMA and LSR and com-
plementary to each other, and they are useful across all three applications
Method	Bit-width	Aggr.	ModelNet40	ShapeNet Parts	S3DIS	
			OA	mIoU	mIoU	OA
Full Prec.	32/32	MAX	88.2	84.3	54.4	83.5
	32/32	AVG	86.5	84.0	51.5	81.5
BNN	1/1	MAX	7.1	54.0	9.5	45.0
BNN-LSR	1/1	MAX	4.1	58.7	2.0	25.4
BNN-EMA	1/1	EMA-avg	11.3	53.0	9.9	46.8
	1/1	EMA-max	16.2	47.3	8.5	47.2
Ours	1/1	EMA-avg	82.5	80.3	40.9	74.9
	1/1	EMA-max	86.4	80.6	44.3	76.7
are sensitive to the scale of values. For example, the T-Net in PointNet is designed to predict an
orthogonal transformation matrix for canonicalization of input and intermediate features (Qi et al.,
2017a). The predicted matrix is regularized by minimizing the loss term Lreg = I - ZZT 2.
However, this regularization is ineffective for the Z with huge variance as shown in Figure 3.
Second, the scale distortion leads to a saturation of forward-propagated activations and backward-
propagated gradients (Ding et al., 2019). In the binary neural networks, some modules (such as
sign and Hardtanh) rely on the Straight-Through Estimator (STE) Bengio et al. (2013) for feature
binarization or feature balancing. When the scale of their input is amplified, the gradient is truncated
instead of increased proportionally. Such saturation, as shown in Fig 4(c), hinders learning and even
leads to divergence.
3.3.2	LSR for Output Scale Recovery
To recover the scale and adjustment ability of output, we propose the LSR for bi-linear layers in our
BiPointNet. We design a learnable layer-wise scaling factor α in our LSR. α is initialized by the
ratio of the standard deviations between the output of bi-linear and full-precision counterpart:
αo = σ(A 0 W)∕σ(Ba Θ Bw),	(9)
where σ denotes as the standard deviation. And the α is learnable during the training process. The
calculation and derivative process of the bi-linear layer with our LSR are as follows:
Forward : Z = α(Ba Θ Bw) Backward : gα = gZ (Ba Θ Bw),	(10)
where gɑ and gz denotes the gradient ∂∂α and ∂∂Z ,respectively. By applying the LSR in BiPointNet,
we mitigate the scale distortion of output caused by binarization.
Compared to existing methods, the advantages of LSR is summarized in two folds. First, LSR is
efficient. It not only abandons the adjustment of input activations to avoid expensive inference time
computation, but also recovers the scale of all weights parameters in a layer collectively instead of
expensive restoration in a channel-wise manner (Rastegari et al., 2016). Second, LSR serves the
purpose of scale recovery that we show is more effective than other adaptation such as minimizing
quantization errors (Qin et al., 2020b; Liu et al., 2018).
4	Experiments
In this section, we conduct extensive experiments to validate the effectiveness of our proposed Bi-
PointNet for efficient learning on point clouds. We first ablate our method and demonstrate the con-
tributions of EMA and LSR on three most fundamental tasks: classification on ModelNet40 (Wu
et al., 2015), part segmentation on ShapeNet (Chang et al., 2015), and semantic segmentation on
S3DIS (Armeni et al., 2016). Moreover, we compare BiPointNet with existing binarization methods
where our designs stand out. Besides, BiPointNet is put to the test on real-world devices with limited
computational power and achieve extremely high speedup (14.7×) and storage saving (18.9×). The
details of the datasets and the implementations are included in the Appendix E.
7
Published as a conference paper at ICLR 2021
Table 2: Comparison of binarization methods
on PointNet. EMA is critical; even if all meth-
ods are equipped with our EMA, our LSR out-
performs others with least number of scaling
factors. OA: Overall Accuracy
Method	Bit-width	Aggr.	# Factors	OA
Full Prec.	32/32	MAX	-	88.2
	32/32	AVG	-	86.5
	1/1	MAX	0	7.1
BNN	1/1	EMA-avg	0	11.3
	1/1	EMA-max	0	16.2
	1/1	MAX	10097	7.3
IR-Net	1/1	EMA-avg	10097	22.0
	1/1	EMA-max	10097	63.5
	1/1	MAX	10097	4.0
Bi-Real	1/1	EMA-avg	10097	77.0
	1/1	EMA-max	10097	77.5
	1/1	MAX	51	4.1
ABC-Net	1/1	EMA-avg	51	68.9
	1/1	EMA-max	51	77.8
	1/1	MAX	18	4.1
XNOR++	1/1	EMA-avg	18	73.8
	1/1	EMA-max	18	78.4
	1/1	MAX	28529	64.9
XNOR	1/1	EMA-avg	28529	78.2
	1/1	EMA-max	28529	81.9
	1/1	MAX	18	4.1
Ours	1/1	EMA-avg	18	82.5
	1/1	EMA-max	18	86.4
Table 3: Our methods on mainstream back- bones. We use XNOR as a strong baseline for comparison. The techniques in our BiPointNet are generic to point cloud learning. Hence, they are easily extendable to other backbones				
Base Model	Method	Bit-width	Aggr.	OA
PointNet (Vanilla)	Full Prec.	32/32	MAX	86.8
	XNOR	1/1	MAX	61.0
	Ours	1/1	EMA-max 85.6	
	Full Prec.	32/32	MAX	88.2
PointNet	XNOR	1/1	MAX	64.9
	Ours	1/1	EMA-max	86.4
	Full Prec.	32/32	MAX	90.0
PointNet++	XNOR	1/1	MAX	63.1
	Ours	1/1	EMA-max	87.8
	Full Prec.	32/32	AVG	90.0
PointCNN	XNOR	1/1	AVG	83.0
	Ours	1/1	EMA-avg	83.8
	Full Prec.	32/32	MAX	89.2
DGCNN	XNOR	1/1	MAX	51.5
	Ours	1/1	EMA-max	83.4
	Full Prec.	32/32	一	90.8
PointConv	XNOR	1/1	一	83.1
	Ours	1/1	一	87.9
4.1	Ablation Study
As shown in Table 1, the binarization model baseline suffers a catastrophic performance drop in
the classification task. EMA and LSR improve performance considerably when used alone, and
they further close the gap between the binarized model and the full-precision counterpart when used
together.
In Figure 4, we further validate the effectiveness of EMA and LSR. We show that BiPointNet
with EMA has its information entropy maximized during training, whereas the vanilla binarized
network with max pooling gives limited and highly fluctuating results. Also, we make use of the
regularization loss Lreg = I - ZZT F for the feature transformation matrix of T-Net in PointNet
as an indicator, the Lreg of the BiPointNet with LSR is much smaller than the vanilla binarized
network, demonstrating LSR’s ability to reduce the scale distortion caused by binarization, allowing
proper prediction of orthogonal transformation matrices.
Moreover, we also include the results of two challenging tasks, part segmentation, and semantic seg-
mentation, in Table 1. As we follow the original PointNet design for segmentation, which concate-
nates pointwise features with max pooled global feature, segmentation suffers from the information
loss caused by the aggregation function. EMA and LSR are proven to be effective: BiPointNet is
approaching the full precision counterpart with only 〜 4% mIoU difference on part segmentation
and 〜10.4% mIoU gap on semantic segmentation. The full results of segmentation are presented
in Appendix E.6.
4.2	Comparative Experiments
In Table 2, we show that our BiPointNet outperforms other binarization methods such as
BNN (Hubara et al., 2016), XNOR (Rastegari et al., 2016), Bi-Real (Liu et al., 2018), ABC-Net (Lin
8
Published as a conference paper at ICLR 2021
■ PointNet -BiPointNet
(b)
Time Cost (ms)
(c)
Figure 5: (a) Time cost comparison. Our BiPointNet achieves 14.7× speedup on ARM A72 CPU
device. (b) Storage usage comparison. Our BiPointNet enjoys 18.9× storage saving on all devices.
(c) Speed vs accuracy trade-off plot. We evaluate various binarization methods (with our EMA-max)
upon PointNet architecture on ARM A72 CPU device, our BiPointNet is the leading method in both
speed and accuracy
et al., 2017), XNOR++ (Bulat & Tzimiropoulos, 2019), and IR-Net (Qin et al., 2020b). Although
these methods have been proven effective in 2D vision, they are not readily transferable to point
clouds due to aggregation-induced feature homogenization.
Even if we equip these methods with our EMA to mitigate information loss, our BiPointNet still
performs better. We argue that existing approaches, albeit having many scaling factors, focus on
minimizing quantization errors instead of recovering feature scales, which is critical to effective
learning on point clouds. Hence, BiPointNet stands out with a negligible increase of parameters
that are designed to restore feature scales. The detailed analysis of the performance of XNOR is
found in Appendix C. Moreover, we highlight that our EMA and LSR are generic, and Table 3
shows improvements across several mainstream categories of point cloud deep learning models,
including PointNet (Qi et al., 2017a), PointNet++ (Qi et al., 2017b), PointCNN (Li et al., 2018),
DGCNN (Wang et al., 2019a), and PointConv (Wu et al., 2019).
4.3	Deployment Efficiency on Real-world Devices
To further validate the efficiency of BiPointNet when deployed into the real-world edge devices, we
further implement our BiPointNet on Raspberry Pi 4B with 1.5 GHz 64-bit quad-core ARM CPU
Cortex-A72 and Raspberry Pi 3B with 1.2 GHz 64-bit quad-core ARM CPU Cortex-A53.
We compare our BiPointNet with the PointNet in Figure 5(a) and Figure 5(b). We highlight that
BiPointNet achieves 14.7× inference speed increase and 18.9× storage reduction over PointNet,
which is recognized as a fast and lightweight model itself. Moreover, we implement various bina-
rization methods over PointNet architecture and report their real speed performance on ARM A72
CPU device. As Figure 5(c), our BiPointNet surpasses all existing binarization methods in both
speed and accuracy. Note that all binarization methods adopt our EMA and report their best accu-
racy, which is the important premise that they can be reasonably applied to binarize the PointNet.
5	Conclusion
We propose BiPointNet, the first binarization approach for efficient learning on point clouds. We
build a theoretical foundation to study the impact of binarization on point cloud learning models,
and proposed EMA and LSR in BiPointNet to improve the performance. BiPointNet outperforms
existing binarization methods, and it is easily extendable to a wide range of tasks and backbones,
giving an impressive 14.7× speedup and 18.9× storage saving on resource-constrained devices. Our
work demonstrates the great potential of binarization. We hope our work can provide directions for
future research.
Acknowledgement This work was supported by National Natural Science Foundation of China
(62022009, 61872021), Beijing Nova Program of Science and Technology (Z191100001119050),
and State Key Lab of Software Development Environment (SKLSDE-2020ZX-06).
9
Published as a conference paper at ICLR 2021
References
Iro Armeni, Ozan Sener, Amir R. Zamir, Helen Jiang, Ioannis Brilakis, Martin Fischer, and Silvio
Savarese. 3d semantic parsing of large-scale indoor spaces. In IEEE CVPR, 2016.
Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
Adrian Bulat and Georgios Tzimiropoulos. Xnor-net++: Improved binary neural networks. In
BMVC, 2019.
Angel X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Qixing Huang, Zimo Li, S. Savarese,
M. Savva, Shuran Song, H. Su, J. Xiao, L. Yi, and F. Yu. Shapenet: An information-rich 3d
model repository. CoRR, abs/1512.03012, 2015.
Ruizhou Ding, Ting-Wu Chin, Zeye Liu, and Diana Marculescu. Regularizing activation distribution
for training binarized deep networks. In IEEE CVPR, 2019.
Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric.
CoRR, abs/1903.02428, 2019.
Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accu-
rate object detection and semantic segmentation. In IEEE CVPR, 2014.
Ross B. Girshick. Fast r-cnn. IEEE ICCV, 2015.
Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei Yu, and
Junjie Yan. Differentiable soft quantization: Bridging full-precision and low-bit neural networks.
In IEEE ICCV, 2019.
Yulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu, and Mohammed Bennamoun. Deep
learning for 3d point clouds: A survey. IEEE TPAMI, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In IEEE CVPR, 2016.
Lu Hou, Quanming Yao, and James T. Kwok. Loss-aware binarization of deep networks. ICLR,
2017.
Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua Wang, Niki Trigoni, and
Andrew Markham. Randla-net: Efficient semantic segmentation of large-scale point clouds. 2020.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized
neural networks. In NeurIPS, 2016.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In NeurIPS, 2012.
Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, X. Di, and B. Chen. Pointcnn: Convolution on
x-transformed points. In NeurIPS, 2018.
Xiaofan Lin, Cong Zhao, and Wei Pan. Towards accurate binary convolutional neural network. In
NeurIPS, 2017.
Chunlei Liu, Wenrui Ding, Xin Xia, Baochang Zhang, Jiaxin Gu, Jianzhuang Liu, Rongrong Ji, and
David S. Doermann. Circulant binary convolutional networks: Enhancing the performance of
1-bit dcnns with circulant back propagation. In IEEE CVPR, 2019a.
Yongcheng Liu, Bin Fan, Shiming Xiang, and Chunhong Pan. Relation-shape convolutional neural
network for point cloud analysis. IEEE CVPR, 2019b.
Yongcheng Liu, Bin Fan, Shiming Xiang, and Chunhong Pan. Relation-shape convolutional neural
network for point cloud analysis. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 8895-8904, 2019c.
10
Published as a conference paper at ICLR 2021
Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, and Kwang-Ting Cheng. Bi-real net:
Enhancing the performance of 1-bit cnns with improved representational capability and advanced
training algorithm. In ECCV, 2018.
Zechun Liu, Zhiqiang Shen, Marios Savvides, and Kwang-Ting Cheng. Reactnet: Towards precise
binary neural network with generalized activation functions. In ECCV, 2020.
Zhijian Liu, Haotian Tang, Yujun Lin, and Song Han. Point-voxel cnn for efficient 3d deep learning.
In NeurIPS, 2019d.
Brais Martinez, Jing Yang, Adrian Bulat, and Georgios Tzimiropoulos. Training binary neural
networks with real-to-binary convolutions. In ICLR, 2020.
Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. Pointnet: Deep learning on point sets
for 3d classification and segmentation. IEEE CVPR, 2017a.
Charles R. Qi, Li Yi, Hao Su, and Leonidas J. Guibas. Pointnet++: Deep hierarchical feature
learning on point sets in a metric space. In NeurIPS, 2017b.
Haotong Qin, Ruihao Gong, Xianglong Liu, Xiao Bai, Jingkuan Song, and Nicu Sebe. Binary neural
networks: A survey. Pattern Recognition, 2020a.
Haotong Qin, Ruihao Gong, Xianglong Liu, Mingzhu Shen, Ziran Wei, Fengwei Yu, and Jingkuan
Song. Forward and backward information retention for accurate binary neural networks. In IEEE
CVPR, 2020b.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet
classification using binary convolutional neural networks. In ECCV, 2016.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael S Bernstein, et al. Imagenet large scale vi-
sual recognition challenge. IJCV, 2015.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
IEEE CVPR, 2015.
HUgUes Thomas, Charles R. Qi, Jean-EmmanUel Deschaud, Beatriz Marcotegui, Francois Goulette,
and Leonidas J. Guibas. Kpconv: Flexible and deformable convolution for point clouds. IEEE
ICCV, 2019.
Yiru Wang, Weihao Gan, Wei Wu, and Junjie Yan. Dynamic curriculum learning for imbalanced
data classification. In IEEE ICCV, 2019a.
Yunhe Wang, Chang Xu, Chao Xu, and Dacheng Tao. Packing convolutional neural networks in the
frequency domain. IEEE TPAMI, 2019b.
Ziwei Wang, Ziyi Wu, Jiwen Lu, and Jie Zhou. Bidet: An efficient binarized object detector. In
IEEE CVPR, 2020.
B. Wu, Y. Wang, P. Zhang, Yuandong Tian, P. Vajda, and K. Keutzer. Mixed precision quantization
of convnets via differentiable neural architecture search. CoRR, abs/1812.00090, 2018.
Wenxuan Wu, Zhongang Qi, and Fuxin Li. Pointconv: Deep convolutional networks on 3d point
clouds. IEEE CVPR, 2019.
Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong
Xiao. 3d shapenets: A deep representation for volumetric shapes. In IEEE CVPR, 2015.
Zizhao Wu, Ruyang Shou, Yunhai Wang, and Xinguo Liu. Interactive shape co-segmentation via
label propagation. Computers & Graphics, 38:248-254, 2014.
11
Published as a conference paper at ICLR 2021
Bo Xie, Yingyu Liang, and Le Song. Diverse neural network learns true target functions. In Artificial
Intelligence and Statistics, 2017.
Chenfeng Xu, Bichen Wu, Zining Wang, Wei Zhan, Peter Vajda, Kurt Keutzer, and Masayoshi
Tomizuka. Squeezesegv3: Spatially-adaptive convolution for efficient point-cloud segmentation.
In ECCV, 2020a.
Qiangeng Xu, Xudong Sun, Cho-Ying Wu, Panqu Wang, and U. Neumann. Grid-gcn for fast and
scalable point cloud learning. 2020b.
Yinghao Xu, Xin Dong, Yudian Li, and Hao Su. A main/subsidiary network framework for simpli-
fying binary neural networks. In IEEE CVPR, 2019.
Li Yi, Vladimir G Kim, Duygu Ceylan, I-Chao Shen, Mengyan Yan, Hao Su, Cewu Lu, Qixing
Huang, Alla Sheffer, and Leonidas Guibas. A scalable active framework for region annotation in
3d shape collections. ACM Transactions on Graphics (ToG), 35(6):1-12, 2016.
Haibao Yu, Q. Han, Jianbo Li, Jianping Shi, Guang-Liang Cheng, and Bin Fan. Search what you
want: Barrier panelty nas for mixed precision quantization. In ECCV, 2020.
Jianhao Zhang, Yingwei Pan, Ting Yao, He Zhao, and Tao Mei. dabnn: A super fast inference
framework for binary neural networks on ARM devices. In ACM MM, 2019a.
Xiangguo Zhang, Haotong Qin, Yifu Ding, Ruihao Gong, Qinghua Yan, Renshuai Tao, Yuhang Li,
Fengwei Yu, and Xianglong Liu. Diversifying sample generation for data-free quantization. In
IEEE CVPR, 2021.
Zhiyuan Zhang, Binh-Son Hua, and Sai-Kit Yeung. Shellnet: Efficient point cloud convolutional
neural networks using concentric shells statistics. In IEEE ICCV, 2019b.
Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Training
low bitwidth convolutional neural networks with low bitwidth gradients. arXiv, abs/1606.06160,
2016.
Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu, Yanfei Wang, Zhelong Li, Xiuqi Yang, and
Junjie Yan. Towards unified int8 training for convolutional neural network. In IEEE CVPR, 2020.
Shilin Zhu, Xin Dong, and Hao Su. Binary ensemble neural network: More bits per network or
more networks per bit? In IEEE CVPR, 2019.
12
Published as a conference paper at ICLR 2021
Appendix for BiPointNet
A Main Proofs and Discussion
A.1 Proof of Zero Conditional Entropy
In our BiPointNet, we hope that the binarized tensor B reflects the information in the original tensor
Y as much as possible. From the perspective of information, our goal is equivalent to maximizing
the mutual information I(Y ; B) of the random variables Y and B:
arg max I(Y ; B)
Y,B
p(Y,B)(y, b) log
y∈Y,b∈B
P(γ,B)(y,b
PY (y)PB (b)
X P(Y,B)(y,b) log P(Y,B)(y, b) - X P(γ,B)(y,b) logPB(b)
y∈Y,b∈B	pY (y)	y∈Y,b∈B
PY (y)PB|Y =y(b) log PB|Y =y(b) -	P(Y,B)(y, b) log PB(b)
y∈Y,b∈B	y∈Y,b∈B
XPY(y) X PB|Y =y(b) log PB|Y =y(b) - X X P(Y,B)(y, b) log PB (b)
y∈Y	b∈B	b∈B y
- X P(y)H(B |Y =y)-XPB(b)logPB(b)
y∈Y	b∈B
-H(B | Y) + H(B)
H(B) - H(B | Y ),
(11)
(12)
(13)
(14)
(15)
(16)
(17)
(18)
where P(Y,B) and PY, PB are the joint and marginal probability mass functions of these discrete
variables. H(B ) is the information entropy, and H(B |Y ) is the conditional entropy of B given Y .
According to the Eq. (15) and Eq. (18), the conditional entropy H(Y | X ) can be expressed as
H(B | Y) = XPY(y) X PB|Y =y (b) log PB|Y =y (b) .
y∈Y	b∈B
(19)
Since we use the deterministic sign function as the quantizer in binarization, the value of B fully
depends on the value of Y , PB|Y=y(b) = 0 or 1 in Eq. (4), i.e., every value y has a fixed mapping to
a binary value b. Then we have
H(B | Y) = EPY(y)(0 + 0 + …+ 0) = 0.
y∈Y
(20)
Hence, the original objective function is equivalent to maximizing the information entropy H(B):
arg max
B
HB(B) = -	PB(b) logPB (b).
b∈B
(21)
A.2 Proofs of Theorem 1
Theorem 1 For input Xφ of max pooling φ with arbitrary distribution, the information entropy of
the binarized output to zero as n to infinity, i.e., lim HB = 0. And there is a constant c, for any n1
n→+∞
and n2, if n1 > n2 > c, we have HB,n1 < HB,n2, where n is the number of aggregated elements.
13
Published as a conference paper at ICLR 2021
Proof. We obtain the correlation between the probability mass function of input Xφ and output
Y of max pooling, intuitively, all values are negative to give a negative maximum value:
XpY (y) =	X pXφ (xφ)n.	(22)
y<0	xφ<0
Since the sign function is applied as the quantizer, the HB (B) of binarized feature can be expressed
as Eq. (6).
(1)	When Xφ obeys a arbitrary distribution, the probability mass function pXφ (xφ) must satisfies
P pXφ (xφ) ≤ 1. According to Eq. (6), lett = P pXφ(xφ), we have
xφ<0	xφ<0
lim HB (Xφ) = lim -tn log tn - (1 - t)n log (1 - t)n	(23)
n→∞	n→∞
= - lim tn	log lim tn	- lim (1	- t)n	log lim	(1 - t)n	(24)
n→∞	n→∞	n→∞	n→∞
=-0 log 0- 1 log 1	(25)
=0	(26)
(2)	For any n ≥ 1, we can obtain the representation of the information entropy HB,n (Xφ):
HB,n(Xφ) = -	X pXφ (xφ) nlog X pXφ(xφ) n
xφ<0	xφ<0
- 1 - X pXφ(xφ) n log 1 - X pXφ(xφ) n ,
xφ<0	xφ<0
Let pn = Pxφ<0 pXφ (xφ) n, theHB,n(pn) can be expressed as
(27)
HB,n(pn) = -pn log pn - (1 - pn) log(1 -pn),	(28)
and the derivative of HB,n (pn) is
d HB,n (pn)	1 - pn
dpB (Pn)	- Ctglpn)
(29)
the HB,n(pn) is maximized when pn takes 0.5, and is positive correlation with pn when pn < 0.5
since the d/B,;(Pn) > 0 Whenpn < 0.5.
d pB (pn )	n
Therefore, When the constant c satisfies pc = Px <0 pXφ (xφ)	≥ 0.5, given the n1 > n2 > c,
We have pn1 < pn2 < pc, and HB,n1 (Xφ) < HB,n2(Xφ) < HB,c(Xφ).

A.3 Proofs of Proposition 1
Proposition 1 When the distribution of the random variable Y satisfies	y<0 pY (y)
y≥0 pY (y) = 0.5, the information entropy HB is maximized.
Proof. According to Eq (5), We have
HB (B) = -	pB (b) log pB (b)	(30)
b∈B
= -pB(-1) logpB(-1) -pB(1)logpB(1)	(31)
= -pB(-1) logpB(-1) - (1 -pB(-1)log(1 -pB(-1))) .	(32)
14
Published as a conference paper at ICLR 2021
Then we can get the derivative of HB (B) with respect to pB (-1)
I-E PB (-1) + PB⅛⅛2) + S-PB T)+ (T≡⅛⅛2)
(33)
-log Pb (-1) +log(1 - PB (T)) -	+
= log
(1 - PB (-1) ∖
V Pb(-1) J
(34)
(35)
When we let IdrHBB1)=° to maximize the HB (B),we have PB (-1) = 0.5. Sine the deterministic
sign function with the zero threshold is applied as the quantizer, the probability mass function of B
is represented as
I Σ py (y) dy, if b = -1
PB (b) =	yP<0
I	PY (y) dy, if b = 1,
y≥0
(36)
and when the information entropy is maximized, we have
PY (y) dy = 0.5.	(37)
y<0
A.4 Discussion and Proofs of Theorem 2
The bi-linear layers are widely used in our BiPointNet to model each point independently, and each
linear layer outputs an intermediate feature. The calculation of the bi-linear layer is represented as
Eq. (2). Since the random variable B is sampled from Bw or Ba obeying Bernoulli distribution, the
probability mass function of B can be represented as
P, if b = +1
PB(b) = 1-P, ifb=-1,	(38)
where P is the probability of taking the value +1. The distribution of output Z can be represented by
the probability mass function of Bw and Ba.
Proposition 2 In bi-linear layer, for the binarized weight Bw ∈ {-1, +1}m×k and activation
Ba ∈ {-1, +1}n×m with probability mass function PBw (1) = Pw and PBa (1) = Pa, the probability
mass function for the distribution of output Z can be represented as PZ (2i - m) = Cmi (1 - Pw -
Pa +2PwPa)i(Pw +Pa -2PwPa)m-i, i ∈ {0, 1,2, ..., m}.
Proof. To simplify the notation in the following statements, we define A = Ba and W = Bw .
Then, for each element Zi,j in output Z ∈ {-1, +1}n×k, we have
m
xi,j = Ai,k × Wk,j .	(39)
k=1
Observe that Ai,k is independent to Wk,j and the value of both variables are either -1 or +1.
Therefore, the discrete probability distribution of Ai,k × Wk,j can be defined as
{Pw Pa + (1 - Pw ) X (1 - Pa),	if X = 1
Pw × (1 -Pa) + (1 -Pw) × Pa, ifx = -1	(40)
0,	otherwise.
15
Published as a conference paper at ICLR 2021
Simplify the above equation
(1 - Pw - Pa + 2pwPa, if X = 1
pw +pa - 2pwpa,	ifx = -1
0,	otherwise.
(41)
Notice that xi,j can be parameterized as a binomial distribution. Then we have
Pr(xi,j = l - (m - l)) = Cm (1 - Pw - Pa + 2PwPa) (Pw + Pa - 2PwPa )	.	(42)
Observe that PZ obeys the same distribution as xi,j . Finally, we have
PZ (2i - m) = Cmi (1 -Pw -Pa + 2PwPa)i(Pw +Pa - 2PwPa)m-i, i ∈ {0, 1, 2, ..., m}.	(43)
Proposition 2 shows that the output distribution of the bi-linear layer depends on the probability
mass functions of binarized weight and activation. Then we present the proofs of Theorem 2.
Theorem 2 When we let PBw (1) = 0.5 and PBa (1) = 0.5 in bi-linear layer to maximize the mutual
information, for the binarized weight Bw ∈ {-1, +1}m×k and activation Ba ∈ {-1, +1}n×m,
the probability mass function for the distribution of output Z can be represented as PZ (2i - m) =
0.5m Cmi , i ∈ {0, 1, 2, ..., m}. The distribution of output is approximate normal distribution
N(0, m).
Proof. First, we prove that the distribution ofZ can be approximated as a normal distribution. For
bi-linear layers in our BiPointNet, all weights and activations are binarized, which can be represented
as Bw and Ba, respectively. And the value of an element z(i,j) in Z can be expressed as
m
z(i,j) =	bw(i,k) × ba(k,j) ,
k=1
and the value of the element bw (i,k) × ba (k,j) can be expressed as
bw(i,k) × ba(k,j)	=	1,1	iiff	bbw(i,k)	YY	bba(k,j)	= 1 1
-1,	if	bw	(i,k)	Y	ba (k,j)	= -1.
(44)
The bw(i,k) ×ba (k,j) only can take from two values and its value can be considered as the result of one
Bernoulli trial. Thus for the random variable Z sampled from the output tensor Z, the probability
mass function, PZ can be expressed as
PZ(2i-m) =Cmi Pek(1-Pe)n-k,	(45)
where Pe denotes the probability that the element bw(i,k) × ba(k,j) takes 1. Note that the Eq. (45) is
completely equivalent to the representation in Proposition 2. According to the De Moivre-Laplace
theorem, the normal distribution N(μ, σ2) can be used as an approximation of the binomial distri-
bution under certain conditions, and thePZ(2i - m) can be approximated as
PZ(2i-m) =Cmi Pek(1-Pe)n-k'
1	- (k-npe)2
——,	---B 2nPe (I-Pe)
√2πnpe(1 - pe)
(46)
and then, We can get the mean μ = 0 and variance σ = √m of the approximated distribution N
with the help of equivalent representation of PZ in Proposition 2. Now we give proof of this below.
According to Proposition 2, When Pw = Pa = 0.5, We can reWrite the equation as
16
Published as a conference paper at ICLR 2021
pZ(2i -m) = 0.5mCmi ,i ∈ {0, 1, 2,..., m}.	(47)
Then we move to calculate the mean and standard variation of this distribution. The mean of this
distribution is defined as
μ(pz) = £(2i — m)0.5mCm, i ∈ {0, 1, 2,…,m}.	(48)
By the virtue of binomial coefficient, we have
(2i -	m)0.5mCmi	+ (2(m - i) - m)0.5mCmm-i =	0.5m((2i -	m)Cmi	+ (m -	2i)Cmm-i)	(49)
=	0.5m((2i -	m)Cmi	+ (m -	2i)Cmi )	(50)
= 0.	(51)
Besides, when m is an even number, We have (2i - m)0.5mCm = 0,i = mm. These equations prove
the symmetry of function (2i - m)0.5mCmi . Finally, we have
μ(Pz) = £(2i — m)0.5mCm, i ∈ {0,1, 2,…,m}	(52)
m
=E((2i - m)0.5mcm + (2(m — i) — m)0.5mCm-i), i ∈ {0,1, 2,…,-}	(53)
= 0.	(54)
The standard variation of pZ is defined as
σ(pz) = J(X |2i - m∣20.5mCm)	(55)
=VZX (4i2 - 4im + m2)0.5mcm	(56)
=Jθ.5m(4 X i2cm - 4m X ICm + m2 X Cm) .	(57)
To calculate the standard variation of pZ, we use Binomial Theorem and have several identical
equations:
Cmi = (1+1)m =2m	(58)
X iCmi = m(1 + 1)m-1 = m2m-1	(59)
X i2Cmi = m(m + 1)(1 + 1)m-2 = m(m + 1)m2m-2 .	(60)
These identical equations help simplify Eq. (57):
σ(pZ) = 0.5m 4Xi2Cmi -4mXiCmi +m2XCmi	(61)
=P0.5m(4m(m + 1)2m-2 - 4m22m-1 + m22m)	(62)
=Pθ.5m((m2 + m)2m - 2m22m + m22m)	(63)
=P0.5m(m2m)	(64)
=√m.	(65)
Now we proved that, the distribution of output is approximate normal distribution N (0, m).

17
Published as a conference paper at ICLR 2021
A.5 DISCUSSION OF THE OPTIMAL δ FOR EMA-MAX
When the Xφ 〜 N(0,1), the objective function of EMA-max to obtain optimal δ* is represented
as Eq. (8). It is difficult to directly solve the objective function. To circumvent this issue, we use
Monte Carlo simulation to approximate the value of the optimal δma乂 as shown Algorithm 1.
Algorithm 1 Monte Carlo Simulation for EMA-max
Input: The number n of points to be aggregated; the number of simulations m (e.g. 10000)
Output: Estimated optimal δma乂 for EMA-max
1: Creating an empty list F (represents elements sampled form distribution of aggregated feature)
2: for i = 0 to m do
3:	Creating an empty list Ti (representing one channel of input feature)
4:	for j = 0 to n do
5:	Sampling an element eij from the distribution N(0, 1)
6:	Adding the sampled element eij to the list Ti
7:	end for
8:	Adding an element represents the aggregated feature MAX(Ti) to F
9:	end for
10:	Estimating the optimal δma乂 as δma乂 = Median(F) (follow Proposition 1)
A.6 DISCUSSION OF THE OPTIMAL δ FOR EMA-AVG
When the Xφ 〜 N(δ, 1), the Y 〜N(δ, n-1) and the objective function of EMA-avg for obtaining
optimal δ*vg can be represented as
arg max HB (δ) = - ( X —ɪ= δ	∖χ⅛ n-1 √2π	(XΦ-δ)2∖ 1	( L	1	(xΦ-δ)2∖ e-—)"g(xXo n-√∏e--
-(X -^-= 'xφ≥0 n-1√2π	2	2	(66) (xφ-δ) ∖、	( L	1	(XΦ-δ)、 e--加N …厂—).
The solution of Eq. (66) is expressed as δ = 0, We thus obtain δ£Vg = 0. This means the solution is
not related to n.
B Implementation of BiPointNet on ARM Devices
B.1	Overview
We further implement our BiPointNet on Raspberry Pi 4B with 1.5 GHz 64-bit quad-core ARM
Cortex-A72 and Raspberry Pi 3B with 1.2 GHz 64-bit quad-core ARM Cortex-A53, and test the
real speed that one can obtain in practice. Although the PointNet is a recognized high-efficiency
model, the inference speed of BiPointNet is much faster. Compared to PointNet, BiPointNet enjoys
up to 14.7× speedup and 18.9× storage saving.
We utilize the SIMD instruction SSHL on ARM NEON to make inference framework
daBNN (Zhang et al., 2019a) compatible with our BiPointNet and further optimize the implementa-
tion for more efficient inference.
B.2	Implementation Details
Figure 6 shows the detailed structures of six PointNet implementations. In Full-Precision version
(a), BN is merged into the later fully connected layer for speedup, which is widely chosen for
deployment in real-world applications. In Binarization version (b)(c)(d)(e), we have to keep BN
unmerged due to the binarization of later layers. Instead, we merge the scaling factor of LSR into
BN layers. The HardTanh function is removed because it does not affect the binarized value of
18
Published as a conference paper at ICLR 2021
input for the later layers. We test the quantization for the first layer and last layer in the variants
(b)(c)(d)(e). In the last variant(f), we drop the BN layers during training. The scaling factor is
ignored during deployment because it does not change the sign of the output.
I ] Full Precision FC [	∣ BinariZation FC with BN I I	Binarization FC w/o BN (	1 MaX Pooling
Input Points Linear 3x64	nχ3	nχ3	nχ3 H I	nχ3	nχ3	nχ3
Linear 64x128	⅛	⅛	⅛	⅛	⅛	⅛
Linear 128x1024	I	rɪn I	I	rɪn I	I	rɪn I
Max Pooling	φ	φ	φ	φ	φ	φ
Linear 1024x512						丁
Linear 512x256	小	小		小	小	十
Linear 256x40		十		十		十
Output	I Class I	I Class I	I Class I	I Class I	I Class I	I Class I
	(a)	(b)	(C)	(d)	(e)	(f)
Figure 6: Structures of different PointNet implementations. Three fully connected layers are used in
all six variants: Full Precision FC, Binarization FC with BN, Binarization FC w/o BN. Full Precision
FC contains a full precision fully connected layer and a ReLU layer. Original BN is merged into
the later layer. Binarization FC with BN also contains two layers: a quantized fully connected layer
and a batch normalization layer. Binarization FC w/o BN is formed by a single quantized fully
connected layer
B.3	Ablation analysis of time cost and quantization sensitivity
Setup	Bit-width	FL	LL	BN	OA	Storage & Saving Ratio	Time & Speedup Ratio	
							A72	A53
(a)	32/32	32/32	32/32	Merged	86.8	3.16MB / 1.0×	131ms / 1.0×	67ms / 1.0×
(b)	1/1	32/32	32/32	Not Merged	85.62	0.17MB / 18.9×	9.0ms / 14.7×	5.5ms / 12.1×
(c)	1/1	32/32	1/1	Not Merged	84.60	0.12MB / 26.3×	9.0ms / 14.7×	5.3ms / 12.6×
(d)	1/1	1/1	32/32	Not Merged	5.31	0.16MB / 19.7×	11.5ms / 11.4×	6.5ms / 10.3×
(e)	1/1	1/1	1/1	Not Merged	4.86	0.12MB / 26.3×	11.4ms / 11.5×	6.4ms / 10.4×
(f)	1/1	32/32	32/32	Not Used	85.13	0.15MB / 21.0×	8.1ms / 16.1×	4.8ms / 13.9×
Table 4: Comparison of different configurations in deployment on ARM devices. The storage-saving
ratio and speedup ratio are calculated according to the full precision model as the first row illustrates.
All the models use PointNet as the base model and EMA-max as the aggregation function. The
accuracy performance is reported on the point cloud classification task with the ModelNet40 dataset.
FL: First Layer; LL: Last Layer
Table 4 shows the detailed configuration including overall accuracy, storage usage, and time cost
of the above-mentioned six implementations. The result shows that binarization of the middle fully
connected layers can extremely speed up the original model. We achieve 18.9× storage saving,
14.7× speedup on A72, and 12.1× speed on A53. The quantization of the last layer further helps
save storage consumption and improves the speed with a slight performance drop. However, the
quantization of the first layer causes a drastic drop in accuracy without discernible computational
cost reduction. The variant (f) without BN achieves comparable performance with variant (b). It
suggests that our LSR method could be an ideal alternative to the original normalization layers to
achieve a fully quantized model except for the first layer.
19
Published as a conference paper at ICLR 2021
C Comparison between Layer-Wise S cale Recovery and other
Methods
In this section, we will analyze the difference between the LSR method with other model binarization
methods. Theorem 2 shows the significance of recovering scale in point cloud learning. However,
IRNet and BiReal only consider the scale of weight but ignore the scale of input features. Therefore,
these two methods cannot recover the scale of output due to scale distortion on the input feature.
A major difference between these two methods is that LSR opts for layer-wise scaling factor while
XNOR opts for point-wise one. Point-wise scale recovery needs dynamical computation during
inference while our proposed LSR only has a layer-wise global scaling factor, which is independent
of the input. As a result, our method can achieve higher speed in practice.
Ado-u 山 CO-%E^O^C-
IO0
10~1
10^2
10~3
IO-4
10-5
O 25	50	75 IOO 125	150	175	200
epoch
Figure 7: The information entropy of BNN, XNOR and our BiPointNet
Table 3 shows that XNOR can alleviate the aggregation-induced feature homogenization. The point-
wise scaling factor helps the model to achieve comparable adjustment capacity as full-precision
linear layers. Therefore, although XNOR suffers from feature homogenization at the beginning of
the training process, it can alleviate this problem with the progress of training and achieve acceptable
performance, as shown in Figure 7.
D Comparison with Other Efficient Learning Methods
We compare our computation speedup and storage savings with several recently proposed methods
to accelerate deep learning models on point clouds. Note that the comparison is for reference only;
tests are conducted on different hardware, and for different tasks. Hence, direct comparison cannot
give any meaningful conclusion. In Table 5, we show that BiPointNet achieves the most impressive
acceleration.
E	Experiments
E.1 Datasets
ModelNet40: ModelNet40 (Wu et al., 2015) for part segmentation. The ModelNet40 dataset is the
most frequently used dataset for shape classification. ModelNet is a popular benchmark for point
cloud classification. It contains 12,311 CAD models from 40 representative classes of objects.
20
Published as a conference paper at ICLR 2021
Table 5: Comparison between BiPointNet and other approaches to efficient learning on point clouds.
Grid-GCN (Xu et al., 2020b) leverages novel data structuring strategy; RAND-LA Net (Hu et al.,
2020) designs a faster sampling method; PointVoxel (Liu et al., 2019d) proposes an efficient repre-
sentation. These works, albeit achieving high performance, are not as effective as our binarization
method in terms of model acceleration. The asterisk indicates the vanilla version
Method	Hardware	Dataset	Base Model	Metric/ Performance	Speedup
BiPointNet	ARM Cortex-A72	ModelNet4	PointNet*	OA/85.6	12.1×
BiPointNet	ARM Cortex-A53	ModelNet40	PointNet*	OA/85.6	14.7×
Grid-GCN	RTX 2080 GPU	S3DIS	PointNet	mIoU/53.2	1.62×
RandLA-Net	RTX 2080Ti GPU	S3DIS	PointNet*	mIoU/70.0	1.04×
PointVoxel	GTX 1080Ti GPU	ShapeNet	PointNet	mIoU/46.9	2.46×
ShapeNet Parts: ShapeNet Parts (Chang et al., 2015) for part segmentation. ShapeNet contains
16,881 shapes from 16 categories, 2,048 points are sampled from each training shape. Each shape
is split into two to five parts depending on the category, making up to 50 parts in total.
S3DIS: S3DIS for semantic segmentation (Armeni et al., 2016). S3DIS includes 3D scan point
clouds for 6 indoor areas including 272 rooms in total, each point belongs to one of 13 semantic
categories. We follow the official code (Qi et al., 2017a) for training and testing.
E.2 Implementation Details of BiPointNet
We follow the popular PyTorch implementation of PointNet and the recent geometric deep learning
codebase (Fey & Lenssen, 2019) for the implementation of PointNet baselines. Our BiPointNet is
built by binarizing the full-precision PointNet. All linear layers in PointNet except the first and last
one are binarized to bi-linear layer, and we select Hardtanh as our activation function instead of
ReLU when we binarize the activation before the bi-linear layer. For the part segmentation task, we
follow the convention (Wu et al., 2014; Yi et al., 2016) to train a model for each of the 16 classes.
We also provide our PointNet baseline under this setting.
Following previous works, we train 200 epochs, 250 epochs, 128 epochs on point cloud classifica-
tion, part segmentation, semantic segmentation respectively. To stably train the binarized models,
we use a learning rate of 0.001 with Adam and Cosine Annealing learning rate decay for all binarized
models on all three tasks.
E.3 More Backbones
We also propose four other models: BiPointCNN, BiPointNet++, BiDGCCN, and BiPointConv,
which are binarized versions of PointCNN (Li et al., 2018), PointNet++ (Qi et al., 2017b),
DGCNN (Wang et al., 2019a), and PointConv (Wu et al., 2019), respectively. This is attributed
to the fact that all these variants have characteristics in common, such as linear layers for point-wise
feature extraction and global pooling layers for feature aggregation (except PointConv, which does
not have explicit aggregators). In PointNet++, DGCNN, and PointConv, we keep the first layer and
the last layer full-precision and binarize all the other layers. In PointCNN, we keep every first layer
of XConv full precision and keep the last layer of the classifier full precision.
E.4 B inarization Methods
For comparison, we implement various representative binarization methods for 2D vision, including
BNN (Hubara et al., 2016), XNOR-Net (Rastegari et al., 2016), Bi-Real Net (Liu et al., 2018),
XNOR++ (Bulat & Tzimiropoulos, 2019), ABC-Net (Lin et al., 2017), and IR-Net (Qin et al.,
2020b), to be applied on 3D point clouds. Note that the Case 1 version of XNOR++ is used in
our experiments for a fair comparison, which applies layerwise learnable scaling factors to mini-
mize the quantization error. These methods are implemented according to their open-source code
or the description in their papers, and we take reference of their 3x3 convolution design when im-
plementing the corresponding bi-linear layers. We follow their training process and hyperparameter
21
Published as a conference paper at ICLR 2021
settings, but note that the specific shortcut structure in Bi-Real and IR-Net is ignored since it only
applies to the ResNet architecture.
E.5 Training Details
Our BiPointNet is trained from scratch (random initialization) without leveraging any pre-trained
model. Amongst the experiments, we apply Adam as our optimizer and use the cosine annealing
learning rate scheduler to stably optimize the networks. To evaluate our BiPointNet on various
network architectures, we mostly follow the hyper-parameter settings of the original papers (Qi
et al., 2017a; Li et al., 2018; Qi et al., 2017b; Wang et al., 2019a).
E.6 Detailed Results of Segmentation
We present the detailed results of part segmentation on ShapeNet Part in Table 6 and semantic
segmentation on S3DIS in Table 7. The detailed results further prove the conclusion of Section 4.1
as EMA and LSR improve performance considerably in most of the categories (instead of huge
performance in only a few categories). This validates the effectiveness and robustness of our method.
22
PUbliShed as a ConferenCe PaPersICLR 2021
Table 6: Detailed results of our BiPointNet for part segmentation on ShapeNet Parts.
	aggr.		ear												mug	pistol	skate		
			mean	aero	bag	cap	car	chair	phone	guitar	knife	lamp	laptop	motor			rocket	board	table
	# shapes		2690	76	55	898	3758	69	787	392	1547	451	202	184	283	66	152	5271	
	FP	max	84.3	83.6	79.4	92.5	76.8	90.8	70.2	91.0	85.6	81.9	95.6	64.4	93.5	80.9	54.5	70.6	81.5
K	FP	avg	84.0	83.4	78.5	90.8	76.3	90.0	73.1	90.8	84.3	80.8	95.5	61.7	93.8	81.6	56.2	72.2	81.8
	BNN	max	54.0	35.1	48.1	65.5	26.5	55.8	57.1	48.8	62.2	48.6	90.1	23.1	68.3	57.5	31.3	43.7	66.8
	BNN	ema-avg	53.0	39.8	46.5	57.5	24.1	58.2	56.2	44.0	50.0	53.0	81.0	16.9	48.8	36.3	25.7	43.7	63.3
	BNN	ema-max	47.3	37.9	46.2	44.6	24.1	61.3	38.2	33.5	42.6	50.8	48.6	16.9	49.0	25.2	26.8	43.7	50.30
	LSR	max	58.7	41.5	46.2	80.2	39.2	75.3	46.0	47.8	75.5	50.0	93.8	25.4	51.0	60.2	36.2	43.7	61.4
	Ours	ema-avg	80.3	79.3	71.9	85.5	66.1	87.7	65.6	84.1	82.8	76.0	94.8	42.7	91.8	75.9	47.2	59.1	79.7
	Ours	ema-max	80.6	79.5	69.7	86.1	67.4	88.6	68.5	87.4	83.0	74.9	95.1	44.8	91.6	76.3	47.7	56.9	79.5
PUbliShed as a ConferenCe PaPersICLR 2021
Table 7: Detailed results of our BiPointNet for semantic segmentation on S3DIS.
method	aggr	overall overall		areal (mIoU/acc.)	area2 (mIoU/acc.)	area3 (mIoU/acc.)	area4 (mIoU/acc.)	area5 (mIoU/acc.)	area6 (mIoU/acc.)	ceiling floor wall beam column window door table chair sofa bookcase board clutter											
										IoU	IoU IoU	IoU	IoU	IoU	IoU	IoU	IoU	IoU	IoU	IoU	IoU
		mloU	acc.																		
FP	max	54.4	83.5	61.7/86.2	38.0/76.8	62.4/88.0	45.0/82.4	45.3/83.3	70.0/89.2	91.1	93.8 72.8 50.3		34.6	52.0	58.0 55.8 51.3 14.5				44.4	43.4	45.2
FP	avg	51.5	81.5	59.9/84.6	35.4/72.4	61.2/87.2	43.8/81.2	42.0/81.2	68.2/88.3	90.1	89.1 71.7 46.1		33.7	53.5	53.8 53.8 47.8			9.4	40.4	38.7	41.8
BNN	max	9.5	45.0	9.6/44.0	9.8/50.5	8.3/41.9	9.3/42.5	9.5/45.8	9.8/41.6	45.5	40.6 28.1	0	0	0	0	0	7.7	0	0	0	2.1
BNN	ema-avg	9.9	46.8	7.6/36.6	11.2/51.2	7.1/36.5	9.8/46.0	11.4/54.8	8.6/41.6	51.5	35.1 32.1	0	0	0	0	0.6	9.3	0	0	0	0.6
BNN	ema-max	8.5	47.2	7.7/44.0	10.1/54.4	7.1/46.8	7.8/39.7	7.6/49.2	7.2/45.3	50.8	43.5 15.9	0	0	0	0	0	0	0	0	0	0
LSR	max	2.0	25.4	2.0/26.0	2.1/27.0	2.0/25.7	1.8/22.8	2.0/25.8	1.9/24.5	25.4	0	0	0	0	0	0	0	0	0	0	0	0
Ours	ema-avg	40.9	74.9	47.1/75.8	29.1/68.3	48.0/79.9	34.2/73.2	34.7/76.1	53.3/79.8	84.6	84.6 60.5	32.0	19.0	39.6	43.0 43.5		39.2	5.8	30.5	18.5	31.3
Ours	ema-max	44.3	76.7	50.9/78.3	31.0/70.3	53.4/82.4	36.6/73.9	36.9/77.6	57.9/82.3	85.1	86.1 62.6 34.5		23.8	43.0	48.0 45.7 40.6			9.6	36.9	26.2	33.9
tjɔ