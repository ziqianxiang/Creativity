Published as a conference paper at ICLR 2021
Single-Timescale Actor-Critic Provably Finds
Globally Optimal Policy
Zuyue Fu
Northwestern University
zuyue.fu@u.northwestern.edu
Zhuoran Yang
Princeton University
zy6@princeton.edu
Zhaoran Wang
Northwestern University
zhaoranwang@gmail.com
Ab stract
We study the global convergence and global optimality of actor-critic, one of the
most popular families of reinforcement learning algorithms. While most exist-
ing works on actor-critic employ bi-level or two-timescale updates, we focus on
the more practical single-timescale setting, where the actor and critic are updated
simultaneously. Specifically, in each iteration, the critic update is obtained by ap-
plying the Bellman evaluation operator only once while the actor is updated in the
policy gradient direction computed using the critic. Moreover, we consider two
function approximation settings where both the actor and critic are represented by
linear or deep neural networks. For both cases, we prove that the actor sequence
converges to a globally optimal policy at a sublinear O(K-1/2) rate, where K
is the number of iterations. To the best of our knowledge, we establish the rate
of convergence and global optimality of single-timescale actor-critic with linear
function approximation for the first time. Moreover, under the broader scope of
policy optimization with nonlinear function approximation, we prove that actor-
critic with deep neural network finds the globally optimal policy at a sublinear rate
for the first time.
1	Introduction
In reinforcement learning (RL) (Sutton et al., 1998), the agent aims to make sequential decisions that
maximize the expected total reward through interacting with the environment and learning from the
experiences, where the environment is modeled as a Markov Decision Process (MDP) (Puterman,
2014). To learn a policy that achieves the highest possible total reward in expectation, the actor-critic
method (Konda and Tsitsiklis, 2000) is among the most commonly used algorithms. In actor-critic,
the actor refers to the policy and the critic corresponds to the value function that characterizes the
performance of the actor. This method directly optimizes the expected total return over the policy
class by iteratively improving the actor, where the update direction is determined by the critic. In
particular, recently, actor-critic combined with deep neural networks (LeCun et al., 2015) achieves
tremendous empirical successes in solving large-scale RL tasks, such as the game of Go (Silver
et al., 2017), StarCraft (Vinyals et al., 2019), Dota (OpenAI, 2018), Rubik’s cube (Agostinelli et al.,
2019; Akkaya et al., 2019), and autonomous driving (Sallab et al., 2017). See Li (2017) fora detailed
survey of the recent developments of deep reinforcement learning.
Despite these great empirical successes of actor-critic, there is still an evident chasm between theory
and practice. Specifically, to establish convergence guarantees for actor-critic, most existing works
either focus on the bi-level setting or the two-timescale setting, which are seldom adopted in practice.
In particular, under the bi-level setting (Yang et al., 2019a; Wang et al., 2019; Agarwal et al., 2019;
Fu et al., 2019; Liu et al., 2019; Abbasi-Yadkori et al., 2019a;b; Cai et al., 2019; Hao et al., 2020;
Mei et al., 2020; Bhandari and Russo, 2020), the actor is updated only after the critic solves the
policy evaluation sub-problem completely, which is equivalent to applying the Bellman evaluation
operator to the previous critic for infinite times. Consequently, actor-critic under the bi-level setting
1
Published as a conference paper at ICLR 2021
is a double-loop iterative algorithm where the inner loop is allocated for solving the policy evaluation
sub-problem of the critic. In terms of theoretical analysis, such a double-loop structure decouples
the analysis for the actor and critic. For the actor, the problem is essentially reduced to analyzing the
convergence of a variant of the policy gradient method (Sutton et al., 2000; Kakade, 2002) where
the error of the gradient estimate depends on the policy evaluation error of the critic. Besides, under
the two-timescale setting (Borkar and Konda, 1997; Konda and Tsitsiklis, 2000; Xu et al., 2020;
Wu et al., 2020; Hong et al., 2020), the actor and the critic are updated simultaneously, but with
disparate stepsizes. More concretely, the stepsize of the actor is set to be much smaller than that
of the critic, with the ratio between these stepsizes converging to zero. In an asymptotic sense,
such a separation between stepsizes ensures that the critic completely solves its policy evaluation
sub-problem asymptotically. In other words, such a two-timescale scheme results in a separation
between actor and critic in an asymptotic sense, which leads to asymptotically unbiased policy
gradient estimates. In sum, in terms of convergence analysis, the existing theory of actor-critic
hinges on decoupling the analysis for critic and actor, which is ensured via focusing on the bi-level
or two-timescale settings.
However, most practical implementations of actor-critic are under the single-timescale setting (Pe-
ters and Schaal, 2008a; Schulman et al., 2015; Mnih et al., 2016; Schulman et al., 2017; Haarnoja
et al., 2018), where the actor and critic are simultaneously updated, and particularly, the actor is
updated without the critic reaching an approximate solution to the policy evaluation sub-problem.
Meanwhile, in comparison with the two-timescale setting, the actor is equipped with a much larger
stepsize in the the single-timescale setting such that the asymptotic separation between the analysis
of actor and critic is no longer valid.
Furthermore, when it comes to function approximation, most existing works only analyze the con-
vergence of actor-critic with either linear function approximation (Xu et al., 2020; Wu et al., 2020;
Hong et al., 2020), or shallow-neural-network parameterization (Wang et al., 2019; Liu et al., 2019).
In contrast, practically used actor-critic methods such as asynchronous advantage actor-critic (Mnih
et al., 2016) and soft actor-critic (Haarnoja et al., 2018) oftentimes represent both the actor and critic
using deep neural networks.
Thus, the following question is left open:
Does single-timescale actor-critic provably find a globally optimal policy under the function
approximation setting, especially when deep neural networks are employed?
To answer such a question, we make the first attempt to investigate the convergence and global
optimality of single-timescale actor-critic with linear and neural network function approximation. In
particular, we focus on the family of energy-based policies and aim to find the optimal policy within
this class. Here we represent both the energy function and the critic as linear or deep neural network
functions. In our actor-critic algorithm, the actor update follows proximal policy optimization (PPO)
(Schulman et al., 2017) and the critic update is obtained by applying the Bellman evaluation operator
only once to the current critic iterate. As a result, the actor is updated before the critic solves the
policy evaluation sub-problem. Such a coupled updating structure persists even when the number
of iterations goes to infinity, which implies that the update direction of the actor is always biased
compared with the policy gradient direction. This brings an additional challenge that is absent in the
bi-level and the two-timescale settings, where the actor and critic are decoupled asymptotically.
To tackle such a challenge, our analysis captures the joint effect of actor and critic updates on the
objective function, dubbed as the “double contraction” phenomenon, which plays a pivotal role for
the success of single-timescale actor-critic. Specifically, thanks to the discount factor of the MDP,
the Bellman evaluation operator is contractive, which implies that, after each update, the critic makes
noticeable progress by moving towards the value function associated with the current actor. As a
result, although we use a biased estimate of the policy gradient, thanks to the contraction brought
by the discount factor, the accumulative effect of the biases is controlled. Such a phenomenon
enables us to characterize the progress of each iteration of joint actor and critic update, and thus
yields the convergence to the globally optimal policy. In particular, for both the linear and neural
settings, we prove that, single-timescale actor-critic finds a O(K-1/2)-globally optimal policy after
K iterations. To the best of our knowledge, we seem to establish the first theoretical guarantee of
global convergence and global optimality for actor-critic with function approximation in the single-
timescale setting. Moreover, under the broader scope of policy optimization with nonlinear function
2
Published as a conference paper at ICLR 2021
approximation, our work seems to prove convergence and optimality guarantees for actor-critic with
deep neural network for the first time.
Contribution. Our contribution is two-fold. First, in the single-timescale setting with linear function
approximation, we prove that, after K iterations of actor and critic updates, actor-critic returns a
policy that is at most O(K-1/2) inferior to the globally optimal policy. Second, when both the
actor and critic are represented by deep neural networks, we prove a similar O(K-1/2) rate of
convergence to the globally optimal policy when the architecture of the neural networks are properly
chosen.
Related Work. Our work extends the line of works on the convergence of actor-critic under the
function approximation setting. In particular, actor-critic is first introduced in Sutton et al. (2000);
Konda and Tsitsiklis (2000). Later, Kakade (2002); Peters and Schaal (2008b) propose the natural
actor-critic method which updates the policy via the natural gradient (Amari, 1998) direction. The
convergence of (natural) actor-critic with linear function approximation are studied in Bhatnagar
et al. (2008; 2009); Bhatnagar (2010); Castro and Meir (2010); Maei (2018). However, these works
only characterize the asymptotic convergence of actor-critic and their proofs all resort to tools from
stochastic approximation via ordinary differential equations (Borkar, 2008). As a result, these works
only show that actor-critic with linear function approximation converges to the set of stable equilibria
of a set of ordinary differential equations. Recently, Zhang et al. (2019) propose a variant of actor-
critic where Monte-Carlo sampling is used to ensure the critic and the policy gradient estimates
are unbiased. Although they incorporate nonlinear function approximation in the actor, they only
establish finite-time convergence result to a stationary point of the expected total reward. Moreover,
due to having an inner loop for solving the policy evaluation sub-problem, they focus on the bi-level
setting. Moreover, under the two-timescale setting, Wu et al. (2020); Xu et al. (2020) show that actor-
critic with linear function approximation finds an ε-stationary point with (e(ε-5/2) samples, where
ε measures the squared norm of the policy gradient. All of these results establish the convergence
of actor-critic, without characterizing the optimality of the policy obtained by actor-critic.
In terms of the global optimality of actor-critic, Fazel et al. (2018); Malik et al. (2018); Tu and
Recht (2018); Yang et al. (2019a); Bu et al. (2019); Fu et al. (2019) show that policy gradient and
bi-level actor-critic methods converge to the globally optimal policies under the linear-quadratic
setting, where the state transitions follow a linear dynamical system and the reward function is
quadratic. For general MDPs, Bhandari and Russo (2019) recently prove the global optimality of
vanilla policy gradient under the assumption that the families of policies and value functions are
both convex. In addition, our work is also related to Liu et al. (2019) and Wang et al. (2019),
where they establish the global optimality of proximal policy optimization and (natural) actor-critic,
respectively, where both the actor and critic are parameterized by two-layer neural networks. Our
work is also related to Agarwal et al. (2019); Abbasi-Yadkori et al. (2019a;b); Cai et al. (2019);
Hao et al. (2020); Mei et al. (2020); Bhandari and Russo (2020), which focus on characterizing the
optimality of natural policy gradient in tabular and/or linear settings. However, these aforementioned
works all focus on bi-level actor-critic, where the actor is updated only after the critic solves the
policy evaluation sub-problem to an approximate optimum. Besides, these works consider linear
or two-layer neural network function approximations whereas we focus on the setting with deep
neural networks. Furthermore, under the two-timescale setting, Xu et al. (2020); Hong et al. (2020)
4
prove that linear actor-critic requires a sample complexity of O(ε-4) for obtaining an ε-globally
optimal policy. In comparison, our O(K-1/2) convergence for single-timescale actor-critic can be
4
translated into a similar O(ε-4) sample complexity directly. Moreover, when reusing the data, our
2
result leads to an improved O(ε-2 ) sample complexity. In addition, our work is also related to
Geist et al. (2019), which proposes a variant of policy iteration algorithm with Bregman divergence
regularization. Without considering an explicit form of function approximation, their algorithm
is shown to converge to the globally optimal policy at a similar O(K-1/2) rate, where K is the
number of policy updates. In contrast, our method is single-timescale actor-critic with linear or
deep neural network function approximation, which enjoys both global convergence and global
optimality. Meanwhile, our proof is based on a finite-sample analysis, which involves dealing with
the algorithmic errors that track the performance of actor and critic updates as well as the statistical
error due to having finite data.
3
Published as a conference paper at ICLR 2021
Our work is also related to the literature on deep neural networks. Previous works (Daniely, 2017;
Jacot et al., 2018; Wu et al., 2018; Allen-Zhu et al., 2018a;b; Du et al., 2018; Zou et al., 2018; Chizat
and Bach, 2018; Jacot et al., 2018; Li and Liang, 2018; Cao and Gu, 2019a;b; Arora et al., 2019; Lee
et al., 2019; Gao et al., 2019) analyze the computational and statistical rates of supervised learning
methods with overparameterized neural networks. In contrast, our work employs overparameterized
deep neural networks in actor-critic for solving RL tasks, which is significantly more challenging
than supervised learning due to the interplay between the actor and the critic.
Notation. We denote by [n] the set {1, 2, . . . , n}. For any measure ν and 1 ≤ p ≤ ∞, we denote by
kf kν,p = (RX |f (x)∣pdν)1/p and ∣∣f kp = (RX |f (x)∣pdμ)“p, where μ is the Lebesguemeasure.
2	Background
In this section, we introduce the background on discounted Markov decision processes (MDPs) and
actor-critic methods.
2.1	Discounted MDP
A discounted MDP is defined by a tuple (S, A, P, ζ, r, γ). Here S and A are the state and action
spaces, respectively, P : S × S × A → [0, 1] is the Markov transition kernel, ζ : S → [0, 1] is the
initial state distribution, r : S × A → R is the deterministic reward function, and γ ∈ [0, 1) is the
discount factor. A policy π(a | s) measures the probability of taking the action a at the state s. We
focus on a family of parameterized policies defined as follows,
π = {πθ(∙ | S) ∈ P(A): s ∈ S},	(2.1)
where P(A) is the probability simplex on the action space A and θ is the parameter of the policy
πθ. For any state-action pair (s, a) ∈ S × A, we define the action-value function as follows,
∞
Qn (s,a) = (1 - Y) ∙ En [X Yt ∙ r(st,at)卜0
t=0
s, a0 = a ,
(2.2)
where st+ι 〜P(∙ | St, at) and at+ι 〜π(∙ | st+ι) for any t ≥ 0. We use En [∙] to denote that the
actions follow the policy π, which further affect the transition of the states. We aim to find an optimal
policy π* such that Qn* (s, a) ≥ Qn(s, a) for any policy π and state-action pair (s, a) ∈ S × A.
That is to say, such an optimal policy ∏* attains a higher expected total reward than any other
policy π, regardless of the initial state-action pair (s, a). For notational convenience, we denote by
Q*(s, a) = Qn*(s,a) for any (s, a) ∈ S × A hereafter.
Meanwhile, We denote by Vn (s) and Pn(s, a) = Vn(s) ∙ π(a | s) the stationary state distribution and
stationary state-action distribution of the policy π, respectively, for any (s, a) ∈ S × A. Correspond-
ingly, We denote by V*(s) and ρ*(s, a) the stationary state distribution and stationary state-action
distribution of the optimal policy ∏*, respectively, for any (s, a) ∈ S ×A. For ease of presentation,
given any functions g1 : S → R and g2 : S × A → R, we define two operators P and Pn as follows,
[Pg1](s, a) =	E[g1(s1)	| s0	= s, a0 =	a],	[Png2](s,	a)	=	En[g2(s1,	a1)	|	s0 = s,	a0	= a], (2.3)
where si 〜 P(∙ | s0,a0) and aι 〜 π(∙ | si). Intuitively, given the current state-action pair
(s0 , a0 ), the operator P pushes the agent to its next state s1 following the Markov transition kernel
P (∙ | so ,a。), while the operator Pn pushes the agent to its next state-action pair (si,ai) following
the Markov transition kernel P(∙ | so,ao) and policy ∏(∙ | si). These operators also relate to the
Bellman evaluation operator Tn, which is defined for any function g : S × A → R as follows,
Tng =(1- Y) ∙ r + Y ∙ Png.	(2.4)
The Bellman evaluation operator Tn is used to characterize the actor-critic method in the following
section. By the definition in (2.2), it is straightforward to verify that the action-value function Qn is
the fixed point of the Bellman evaluation operator Tn defined in (2.4), that is, Qn = TnQn for any
policy ∏. For notational convenience, we let P' denote the '-fold composition PP …P, where there
are ` operators P composed together. Such notation is also adopted for other linear operators such
as Pn and Tn .
4
Published as a conference paper at ICLR 2021
2.2	Actor-Critic Method
To obtain an optimal policy ∏*, the actor-critic method (Konda and Tsitsiklis, 2000) aims to maxi-
mize the expected total reward as a function of the policy, which is equivalent to solving the follow-
ing maximization problem,
maχ J(∏) = Es〜ζ,a〜∏(∙∣ s) [Qπ(s, a)],	(2.5)
π∈Π
where ζ is the initial state distribution, Qπ is the action-value function defined in (2.2), and the family
of parameterized polices Π is defined in (2.1). The actor-critic method solves the maximization
problem in (2.5) via first-order optimization using an estimator of the policy gradient Vθ J(∏). Here
θ is the parameter of the policy π . In detail, by the policy gradient theorem (Sutton et al., 2000), we
have
vθ J(π) = E(s,a)〜%∏ [Qπ (S, a) ∙ vθ log π(a | S)] .	(2.6)
Here %π is the state-action visitation measure of the policy π, which is defined as %π(s, a) =
(1 - Y) ∙ P∞=o γt ∙ Pr[st = s, at = a]. Based on the closed form of the policy gradient in (2.6),
the actor-critic method consists of the following two parts: (i) the critic update, where a policy eval-
uation algorithm is invoked to estimate the action-value function Qπ , e.g., by applying the Bellman
evaluation operator Tπ to the current estimator of Qπ , and (ii) the actor update, where a policy
improvement algorithm, e.g., the policy gradient method, is invoked using the updated estimator of
Qπ.
In this paper, we consider the following variant of the actor-critic method,
∏k+ι — argmaxEVnJhQk(s, ∙),∏(∙ I SFi- β ∙ KL(∏(∙ | S) k ∏k(∙ | s))],
π∈Π k
Qk+ι(s,a) . E∏k+ι [(1 — Y) ∙ r(so,ao) + Y ∙ Qk(sι,aι) ∣ so = s,ao = a],	(2.7)
for any (s, a) ∈ S × A, where si 〜 P(∙ ∣ so, ao), aι 〜 ∏k+ι(∙ ∣ si), and We write EVnkH =
Es〜VπJ∙] for notational convenience. Here Π is defined in (2.1) and KL(∏(∙ ∣ s) ∣∣ ∏k(∙ ∣ s)) is the
Kullback-Leibler (KL) divergence between ∏(∙ ∣ s) and ∏k(∙ ∣ s), which is defined for any S ∈ S as
follows, KL(π(∙ ∣ s) k ∏k(∙ ∣ s)) = Pa∈A log(π(a ∣ s)∕∏k(a ∣ s)) ∙ π(a ∣ s). In (2.7), the actor update
uses the proximal policy optimization (PPO) method (Schulman et al., 2017), while the critic update
applies the Bellman evaluation operator Tπk+1 defined in (2.4) to Qk only once, which is the current
estimator of the action-value function. Furthermore, we remark that the updates in (2.7) provide a
general framework in the following two aspects. First, the critic update can be extended to letting
Qk+i J (Tπk+1 )τQk for any fixed T ≥ 1, which corresponds to updating the value function via T-
step rollouts following πk+i. Here we only focus on the case with τ = 1 for simplicity. Our theory
can be easily modified for any fixed T . Moreover, the KL divergence used in the actor step can also
be replaced by other Bregman divergences between probability distributions over A. Second, the
actor and critic updates in (2.7) is a general template that admits both on- and off-policy evaluation
methods and various function approximators in the actor and critic. In the next section, we present an
incarnation of (2.7) with on-policy sampling and linear and neural network function approximation.
Furthermore, for analyzing the actor-critic method, most existing works (Yang et al., 2019a; Wang
et al., 2019; Agarwal et al., 2019; Fu et al., 2019; Liu et al., 2019) rely on (approximately) obtaining
Qπk+1 at each iteration, which is equivalent to applying the Bellman evaluation operator Tπk+1
infinite times to Qk. This is usually achieved by minimizing the mean-squared Bellman error kQ -
Tπk+1 Qk2ρn ,2 using stochastic semi-gradient descent, e.g., as in the temporal-difference method
(Sutton, 1988), to update the critic for sufficiently many iterations. The unique global minimizer
of the mean-squared Bellman error gives the action-value function Qπk+1 , which is used in the
actor update. Meanwhile, the two-timescale setting is also considered in existing works (Borkar
and Konda, 1997; Konda and Tsitsiklis, 2000; Xu et al., 2019; 2020; Wu et al., 2020; Hong et al.,
2020), which require the actor to be updated more slowly than the critic in an asymptotic sense.
Such a requirement is usually satisfied by forcing the ratio between the stepsizes of the actor and
critic updates to go to zero asymptotically.
In comparison with the setting with bi-level updates, we consider the single-timescale actor and
critic updates in (2.7), where the critic involves only one step of update, that is, applying the Bell-
man evaluation operator Tπ to Qk only once. Meanwhile, in comparison with the two-timescale
5
Published as a conference paper at ICLR 2021
setting, where the actor and critic are updated simultaneously but with the ratio between their step-
sizes asymptotically going to zero, the single-timescale setting is able to achieve a faster rate of
convergence by allowing the actor to be updated with a larger stepsize, while updating the critic
simultaneously. In particular, such a single-timescale setting better captures a broader range of prac-
tical algorithms (Peters and Schaal, 2008a; Schulman et al., 2015; Mnih et al., 2016; Schulman et al.,
2017; Haarnoja et al., 2018), where the stepsize of the actor is not asymptotically zero. In §3, we
discuss the implementation of the updates in (2.7) for different schemes of function approximation.
In §4, we compare the rates of convergence between the two-timescale and single-timescale settings.
3	Algorithms
We consider two settings, where the actor and critic are parameterized using linear functions and
deep neural networks (which is deferred to §A of the appendix), respectively. We consider the
energy-based policy ∏θ (a | S) 8 exp(τ-1fθ (s, a)), where the energy function fθ (s, a) is Parameter-
ized with the parameter θ. Also, for the (estimated) action-value function, we consider the parame-
terization Qω(s, a) for any (s, a) ∈ S × A, where ω is the parameter. For such parameterizations of
the actor and critic, the updates in (2.7) have the following forms.
Actor Update. The following proposition gives the closed form of πk+1 in (2.7).
Proposition 3.1. Let ∏θk (a | S) H exp(τ-1fθk (s,a)) be an energy-based policy and e^+ι =
argmaxπ EVk [hQωk (s, ∙),∏(∙ | s)i — β ∙ KL(∏(∙ | S) k ∏θk (∙ | s))]. Then ek+ι has the following
closed form: πek+1(a | S) H exp β-1Qωk (S, a) + τk-1fθk (S, a) , for any (S, a) ∈ S × A, where
νk = νπθ is the stationary state distribution of πθk .
See §G.1 for a detailed proof of Proposition 3.1. Motivated by Proposition 3.1, to implement the ac-
tor update in (2.7), we update the actor parameter θ by solving the following minimization problem,
Θk+1 - argminEρ% [(fθ(s,a) — τfc+ι ∙ (β-1Qωk (s, a) + τ-1fθ%(s,a)))2],	(3.1)
θ
where ρk = ρπθ is the stationary state-action distribution of πθk .
Critic Update. To implement the critic update in (2.7), we update the critic parameter ω by solving
the following minimization problem,
ωk+ι — argminEρk+ι [([Qω — (1 — Y) ∙ r — Y ∙ Pπθk+1 Qω%](s,a))2],	(3.2)
ω
where ρk+1 = ρπθ	is the stationary state-action distribution of πθk+1 and the operator Pπ is
defined in (2.3).
3.1	Linear Function Approximation
In this section, we consider linear function approximation. More specifically, we parameterize the
action-value function using Qω (s, a) = ω>夕(s, a) and the energy function of the energy-based
policy ∏θ using fθ(s, a) = θ>夕(s, a). Here 夕(s, a) ∈ Rd is the feature vector, where d > 0 is
the dimension. Without loss of generality, We assume that k 夕(s, a)k2 ≤ 1 for any (s, a) ∈ S ×A,
which can be achieved by normalization.
Actor Update. The minimization problem in (3.1) admits the following closed-form solution,
Θfc+1 = Tk+1 ∙ (β-1ωfc + τ-1θk),	(3.3)
which corresponds to a step of the natural policy gradient method (Kakade, 2002).
Critic Update. The minimization problem in (3.2) admits the following closed-form solution,
ωk+ι = (E9k+ι k(s, a⅛(s, 6>])-1%+1 [[(I- Y) ∙ r + γ ∙ Pπθk+1 Qωk](s, a) ∙ °(s, a)]. (3.4)
Since the closed-form solution ωek+1 in (3.4) involves the expectation over the stationary state-action
distribution ρk+1 of πθk+1 , we use data to approximate such an expectation. More specifically,
we sample {(s',1,a',1)}'∈[N] and {(s',2, a',2,r',2, s',2, a',2)}'∈[N] such that (s`,i,a`,i)〜ρk+ι,
6
Published as a conference paper at ICLR 2021
(S',2,a',2)~ Pk+1, r',2 = r(s',2,a',2), s',2 ~ P(∙ | S',2,a',2), and a',2 ~ ∏θk+ι (∙ | s',2), where N
is the sample size. We approximate ωek+1 using ωk+1, which is defined as follows,
ωk+ι = Γr {(X 2(s`,i,a`,i)P(s',1,a',1)>)	(3.5)
'=1
N
• X((I-Y) ∙ r',2 + Y ∙ Qωk (s',2,a',2)) ∙ 3(s',2,a',2)}.
'=1
Here ΓR is the projection operator, which projects the parameter onto the centered ball with radius
R in Rd . Such a projection operator stabilizes the algorithm (Konda and Tsitsiklis, 2000; Bhatnagar
et al., 2009). It is worth mentioning that one may also view the update in (3.5) as one step of the
least-squares temporal difference method (Bradtke and Barto, 1996), which can be modified for the
off-policy setting (Antos et al., 2007; Yu, 2010; Liu et al., 2018; Nachum et al., 2019; Xie et al.,
2019; Zhang et al., 2020; Uehara and Jiang, 2019; Nachum and Dai, 2020). Such a modification
allows the data points in (3.5) to be reused in the subsequent iterations, which further improves the
sample complexity. Specifically, let ρbhv ∈ P(S × A) be the stationary state-action distribution
induced by a behavioral policy πbhv. We replace the actor and critic updates in (3.1) and (3.2) by
Θk+1 - argmin EPbhv[(fθ (s, a) —「k+i • (β-1Qωk (s,a) + τ-1fθ% (s,a)))2],	(3.6)
θ
ωk+ι — argminEPbhv [([Qω — (1 — Y) • T- Y • Pπθk+1 QωJ(s, a))2],	(3.7)
ω
respectively. With linear function approximation, the actor update in (3.6) is reduced to (3.3), while
the critic update in (3.7) admits a closed form solution
ωk+ι = (EPbhv[以s,a⅛(s,a)>])T • EPbhv [[(I-Y) • r + γ • Pπθk+1 QωJ(s, a) ∙ φ(s,a)],
which can be well approximated using state-action pairs drawn from ρbhv. See §4 for a detailed
discussion. Finally, by assembling the updates in (3.3) and (3.5), we present the linear actor-critic
method in Algorithm 1, which is deferred to §B of the appendix.
4 Theoretical Results
In this section, we upper bound the regret of the linear actor-critic method. We defer the analysis of
the deep neural actor-critic method to §C of the appendix. Hereafter we assume that |r(s, a)| ≤ rmax
for any (s, a) ∈ S × A, where rmax is a positive absolute constant. First, we impose the following
assumptions. Recall that ρ* is the stationary state-action distribution of ∏*, while Pk is the stationary
state-action distribution of πθk . Moreover, let ρ ∈ P(S × A) be a state-action distribution with
respect to which we aim to characterize the performance of the actor-critic algorithm. Specifically,
after K + 1 actor updates, we are interested in upper bounding the following regret
KK
E[X(kQ* - Qπθk+1 kρ,ι)] = E[X(Q*(s,α) — Qπθk+1 (s,α))],	(4.1)
where the expectation is taken with respect to {θk}k∈[κ+i] and (s, a)〜ρ. Here we allow P to be
any fixed distribution for generality, which might be different from ρ*.
Assumption 4.1 (Concentrability Coefficient). The following statements hold.
(i)	There exists a positive absolute constant φ* such that φk ≤ φ* for any k ≥ 1, where
Φk = kdρ*∕dρkkρk,2.
(ii)	We assume that for any k ≥ 1 and a sequence of policies {πi}i≥1, the k-step future-state-
action distribution ρPπ1 •…Pnk is absolutely continuous with respect to ρ*, where P is the
same as the one in (4.1) Also, it holds for such P that Cρ,ρ* = (1 — Y)2 P∞=ι k2Yk • c(k) <
∞, where c(k) =suP{∏i}i∈[k] kd(PPπ1 ••• Pnk )∕dρ*kρ*,∞.
In Assumption 4.1, CP,P* is known as the discounted-average concentrability coefficient of the
future-state-action distributions. Such an assumption indeed measures the stochastic stability prop-
erties of the MDP, and the class of MDPS with such properties is quite large. See SzePeSVari and
7
Published as a conference paper at ICLR 2021
Munos (2005); Munos and Szepesvari (2008); Antos et al. (2008a;b); Scherrer (2013); Scherrer et al.
(2015); Farahmand et al. (2016); Yang et al. (2019b); Geist et al. (2019); Chen and Jiang (2019) for
more examples and discussion.
Assumption 4.2 (Zero Approximation Error). It holds for any ω, θ ∈ B(0, R) that
infω∈B(0,R) Eρ∏θ [([TπθQω — ω>^](s, a))2] = 0, where Tπθ is defined in (2.4).
Assumption 4.2 imposes a structural assumption of the MDP under the linear setting. Specifically
speaking, it assumes that the Bellman operator of each policy maps a linear value function to a
linear function. Therefore, the value function associated with each policy (which is the fixed point
of the corresponding Bellman operator) lies in the linear function class. Since the value functions are
linear here, the energy-based policy class approximately covers the optimal policy as the temperature
parameter τ goes to zero. In summary, our Assumption 4.2 ensures that the energy-based policy
class approximately captures the optimal policy and thus there is no approximation error. When
Assumption 4.2 does not hold, we only need to add an additional bias term to the regret upper bound
in our theorem without much change in the proof.
Assumption 4.3 (Well-Conditioned Feature). The minimum singular value of the matrix
Eρk [夕(s, a)夕(s, a)>] is UniformIy lower bounded by a positive absolute constant σ* for any k ≥ 1.
Assumption 4.3 ensures that the minimization problem in (3.2) admits a unique minimizer, which
is used in the critic update. Similar assumptions are commonly imposed in the literature (Bhandari
et al., 2018; Xu et al., 2019; Zou et al., 2019; Wu et al., 2020).
Under Assumptions 4.1, 4.2, and 4.3, we upper bound the regret of Algorithm 1 in the following
theorem.
Theorem 4.4. We assume that Assumptions 4.1, 4.2, and 4.3 hold. Let ρ be a state-action distri-
bution satisfying (ii) of Assumption 4.1. Also, for any sufficiently large K > 0, let β = K1/2,
N = Ω(KCp,ρ* ∙ (φ*∕σ*)2 ∙ log2 N), and the sequence of policy parameters {θk}k∈[κ+i] be gen-
erated by Algorithm 1. It holds that
K
E[X(Q*(s, a) — Qπθk+ι (s, a))] ≤(2(1 - γ)-3 ∙ log |A| + O(1)) ∙ K 1/2,	(4.2)
k=0
where the expectation is taken with respect to {θk}k∈[κ+1] and (s, a)〜ρ.
We sketch the proof in §D. See §E.1 for a detailed proof. Theorem 4.4 establishes an O(K1/2 )
regret of Algorithm 1, where K is the total number of iterations. Here O(∙) omits terms involving
(1 - γ)-1 and log |A|. To better understand Theorem 4.4, we consider the ideal setting, where we
have access to the action-value function Qπ of any policy π. In such an ideal setting, the critic
update is unnecessary. However, the natural policy gradient method, which only uses the actor
update, achieves the same O(K1/2) regret (Liu et al., 2019; Agarwal et al., 2019; Cai et al., 2019).
In other words, in terms of the iteration complexity, Theorem 4.4 shows that in the single-timescale
setting, using only one step of the critic update along with one step of the actor update is as efficient
as the natural policy gradient method in the ideal setting.
Furthermore, by the regret bound in (4.2), to obtain an ε-globally optimal policy, it suffices to set
K N (1 一 γ)-6 ∙ ε-2 ∙ log2 |A| in Algorithm 1 and output a randomized policy that is drawn from
{∏θk}K+11 uniformly. Plugging such a K into N = Ω(KCp ρ* (φ*∕σ*)2 ∙ log2 N), we obtain that
2
N = O(ε 2), where O(∙) omits the logarithmic terms. Thus, to achieve an ε-globally optimal
4
policy, the total sample complexity of Algorithm 1 is O(ε-4). This matches the sample complexity
results established in Xu et al. (2020); Hong et al. (2020) for two-timescale actor-critic methods.
Meanwhile, notice that here the critic updates are on-policy and we draw N new data points in each
critic update. As discussed in §3.1, under the off-policy setting, the critic updates given in (3.7)
can be implemented using a fixed dataset sampled from ρbhv, the stationary state-action distribution
induced by the behavioral policy. Under this scenario, the total number of data points used by the
algorithm is equal to N. Moreover, by imposing similar assumptions on ρbhv as in (i) of Assumption
4.1 and Assumption 4.3, we can establish a similar O(K1/2) regret as in (4.2) for the off-policy
setting. As a result, with data reuse, to obtain an ε-globally optimal policy, the sample complexity
of Algorithm 1 is essentially O(ε-2), which demonstrates the advantage of our single-timescale
8
Published as a conference paper at ICLR 2021
actor-critic method. Besides, only focusing on the convergence to an ε-stationary point, Wu et al.
(2020); XU et al. (2020) establish the sample complexity of Oe(ε-5/2) for two-timescale actor-critic,
where ε measures the squared Euclidean norm of the policy gradient. In contrast, by adopting the
natUral policy gradient (Kakade, 2002) in actor Updates, we achieve convergence to the globally
optimal policy. We remark that the idea of off-policy evalUation cannot be applied to typical two-
timescale setting (WU et al., 2020; XU et al., 2020), where the critic is Updated Using TD learning
(e.g. TD(0) and TD(λ)), since it is shown that off-policy TD method may diverge even with linear
fUnction approximation (Baird et al., 1995; SUtton et al., 2008). To the best of oUr knowledge,
we establish the rate of convergence and global optimality of the actor-critic method with fUnction
approximation in the single-timescale setting for the first time.
FUrthermore, as we will show in Theorem C.5 of §B, when both the actor and the critic are repre-
Sented using overparameterized deep neural networks, we establish a similar O((1 - Y)-3 ∙ log |A| ∙
K1/2 ) regret when the architectUre of the actor and critic neUral networks are properly chosen. To
our best knowledge, this seems the first theoretical guarantee for the actor-critic method with deep
neural network function approximation in terms of the rate of convergence and global optimality.
References
Abbasi-Yadkori, Y., Bartlett, P., Bhatia, K., Lazic, N., Szepesvari, C. and Weisz, G. (2019a). Poli-
tex: Regret bounds for policy iteration using expert prediction. In International Conference on
Machine Learning.
Abbasi-Yadkori, Y., Lazic, N., Szepesvari, C. and Weisz, G. (2019b). Exploration-enhanced politex.
arXiv preprint arXiv:1908.10479.
Agarwal, A., Kakade, S. M., Lee, J. D. and Mahajan, G. (2019). Optimality and approximation with
policy gradient methods in Markov decision processes. arXiv preprint arXiv:1908.00261.
Agostinelli, F., McAleer, S., Shmakov, A. and Baldi, P. (2019). Solving the Rubik’s cube with deep
reinforcement learning and search. Nature Machine Intelligence, 1 356-363.
Akkaya, I., Andrychowicz, M., Chociej, M., Litwin, M., McGrew, B., Petron, A., Paino, A.,
Plappert, M., Powell, G., Ribas, R. et al. (2019). Solving Raubik’s cube with a robot hand. arXiv
preprint arXiv:1910.07113.
Allen-Zhu, Z., Li, Y. and Liang, Y. (2018a). Learning and generalization in overparameterized neu-
ral networks, going beyond two layers. arXiv preprint arXiv:1811.04918.
Allen-Zhu, Z., Li, Y. and Song, Z. (2018b). A convergence theory for deep learning via over-
parameterization. arXiv preprint arXiv:1811.03962.
Amari, S.-I. (1998). Natural gradient works efficiently in learning. Neural Computation, 10 251-
276.
Antos, A., Szepesvari, C. and Munos, R. (2007). Value-iteration based fitted policy iteration: Learn-
ing with a single trajectory. In IEEE International Symposium on Approximate Dynamic Program-
ming and Reinforcement Learning.
Antos, A., Szepesvari, C. and Munos, R. (2008a). Fitted Q-iteration in continuous action-space
MDPs. In Advances in Neural Information Processing Systems.
Antos, A., Szepesvari, C. and Munos, R. (2008b). Learning near-optimal policies with bellman-
residual minimization based fitted policy iteration and a single sample path. Machine Learning,
71 89-129.
Arora, S., Du, S. S., Hu, W., Li, Z. and Wang, R. (2019). Fine-grained analysis of optimiza-
tion and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584.
Baird, L. et al. (1995). Residual algorithms: Reinforcement learning with function approximation.
In International Conference on Machine Learning. Elsevier.
9
Published as a conference paper at ICLR 2021
Bhandari, J. and Russo, D. (2019). Global optimality guarantees for policy gradient methods. arXiv
preprint arXiv:1906.01786.
Bhandari, J. and Russo, D. (2020). A note on the linear convergence of policy gradient methods.
arXiv preprint arXiv:2007.11120.
Bhandari, J., Russo, D. and Singal, R. (2018). A finite time analysis of temporal difference learning
with linear function approximation. arXiv preprint arXiv:1806.02450.
Bhatnagar, S. (2010). An actor-critic algorithm with function approximation for discounted cost
constrained Markov Decision Processes. Systems & Control Letters, 59 760-766.
Bhatnagar, S., Ghavamzadeh, M., Lee, M. and Sutton, R. S. (2008). Incremental natural actor-critic
algorithms. In Advances in Neural Information Processing Systems.
Bhatnagar, S., Sutton, R., Ghavamzadeh, M. and Lee, M. (2009). Natural actor-critic algorithms.
Automatica, 45 2471-2482.
Borkar, V. S. (2008). Stochastic Approximation: A Dynamical Systems Viewpoint. Cambridge Uni-
versity Press.
Borkar, V. S. and Konda, V. R. (1997). The actor-critic algorithm as multi-time-scale stochastic
approximation. Sadhana, 22 525-543.
Bradtke, S. J. and Barto, A. G. (1996). Linear least-squares algorithms for temporal difference learn-
ing. Machine Learning 33-57.
Bu, J., Mesbahi, A., Fazel, M. and Mesbahi, M. (2019). LQR through the lens of first order methods:
Discrete-time case. arXiv preprint arXiv:1907.08921.
Cai, Q., Yang, Z., Jin, C. and Wang, Z. (2019). Provably efficient exploration in policy optimization.
arXiv preprint arXiv:1912.05830.
Cao, Y. and Gu, Q. (2019a). Generalization bounds of stochastic gradient descent for wide and deep
neural networks. arXiv preprint arXiv:1905.13210.
Cao, Y. and Gu, Q. (2019b). A generalization theory of gradient descent for learning over-
parameterized deep ReLU networks. arXiv preprint arXiv:1902.01384.
Castro, D. D. and Meir, R. (2010). A convergent online single-time-scale actor-critic algorithm.
Journal of Machine Learning Research ,11 367T10.
Chen, J. and Jiang, N. (2019). Information-theoretic considerations in batch reinforcement learning.
arXiv preprint arXiv:1905.00360.
Chizat, L. and Bach, F. (2018). A note on lazy training in supervised differentiable programming.
arXiv preprint arXiv:1812.07956.
Daniely, A. (2017). SGD learns the conjugate kernel class of the network. In Advances in Neural
Information Processing Systems.
Du, S. S., Lee, J. D., Li, H., Wang, L. and Zhai, X. (2018). Gradient descent finds global minima of
deep neural networks. arXiv preprint arXiv:1811.03804.
Farahmand, A.-m., Ghavamzadeh, M., Szepesvari, C. and Mannor, S. (2016). Regularized policy
iteration with nonparametric function spaces. Journal of Machine Learning Research, 17 4809-
4874.
Farahmand, A.-m., Szepesvari, C. and Munos, R. (2010). Error propagation for approximate policy
and value iteration. In Advances in Neural Information Processing Systems.
Fazel, M., Ge, R., Kakade, S. M. and Mesbahi, M. (2018). Global convergence of policy gradient
methods for linearized control problems. arXiv preprint arXiv:1801.05039.
10
Published as a conference paper at ICLR 2021
Fu, Z., Yang, Z., Chen, Y. and Wang, Z. (2019). Actor-critic provably finds Nash equilibria of linear-
quadratic mean-field games. arXiv preprint arXiv:1910.07498.
Gao, R., Cai, T., Li, H., Wang, L., Hsieh, C.-J. and Lee, J. D. (2019). Convergence of adversarial
training in overparametrized networks. arXiv preprint arXiv:1906.07916.
Geist, M., Scherrer, B. and Pietquin, O. (2019). A theory of regularized markov decision processes.
arXiv preprint arXiv:1901.11275.
Haarnoja, T., Zhou, A., Abbeel, P. and Levine, S. (2018). Soft actor-critic: Off-policy maximum
entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290.
Hao, B., Lazic, N., Abbasi-Yadkori, Y., Joulani, P. and Szepesvari, C. (2020). Provably efficient
adaptive approximate policy iteration. arXiv preprint arXiv:2002.03069.
Hong, M., Wai, H.-T., Wang, Z. and Yang, Z. (2020). A two-timescale framework for bilevel opti-
mization: Complexity analysis and application to actor-critic. arXiv preprint arXiv:2007.05170.
Jacot, A., Gabriel, F. and Hongler, C. (2018). Neural tangent kernel: Convergence and generaliza-
tion in neural networks. In Advances in Neural Information Processing Systems.
Kakade, S. M. (2002). A natural policy gradient. In Advances in Neural Information Processing
Systems.
Konda, V. R. and Tsitsiklis, J. N. (2000). Actor-critic algorithms. In Advances in Neural Information
Processing Systems.
LeCun, Y., Bengio, Y. and Hinton, G. (2015). Deep learning. Nature, 521 436-444.
Lee, J., Xiao, L., Schoenholz, S. S., Bahri, Y., Sohl-Dickstein, J. and Pennington, J. (2019). Wide
neural networks of any depth evolve as linear models under gradient descent. arXiv preprint
arXiv:1902.06720.
Li, Y. (2017). Deep reinforcement learning: An overview. arXiv preprint arXiv:1701.07274.
Li, Y. and Liang, Y. (2018). Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems.
Liu, B., Cai, Q., Yang, Z. and Wang, Z. (2019). Neural proximal/trust region policy optimization
attains globally optimal policy. arXivpreprint arXiv:1906.10306 10564-10575.
Liu, Q., Li, L., Tang, Z. and Zhou, D. (2018). Breaking the curse of horizon: Infinite-horizon off-
policy estimation. In Advances in Neural Information Processing Systems.
Maei, H. R. (2018). Convergent actor-critic algorithms under off-policy training and function ap-
proximation. arXiv preprint arxiv:1802.07842.
Malik, D., Pananjady, A., Bhatia, K., Khamaru, K., Bartlett, P. L. and Wainwright, M. J. (2018).
Derivative-free methods for policy optimization: Guarantees for linear quadratic systems. arXiv
preprint arXiv:1812.08305.
Mei, J., Xiao, C., Szepesvari, C. and Schuurmans, D. (2020). On the global convergence rates of
softmax policy gradient methods. arXiv preprint arXiv:2005.06392.
Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D. and
Kavukcuoglu, K. (2016). Asynchronous methods for deep reinforcement learning. In In-
ternational Conference on Machine Learning.
Munos, R. and Szepesvari, C. (2008). Finite-time bounds for fitted value iteration. Journal of Ma-
chine Learning Research, 9 815-857.
Nachum, O., Chow, Y., Dai, B. and Li, L. (2019). Dualdice: Behavior-agnostic estimation of dis-
counted stationary distribution corrections. In Advances in Neural Information Processing Sys-
tems.
11
Published as a conference paper at ICLR 2021
Nachum, O. and Dai, B. (2020). Reinforcement learning via Fenchel-Rockafellar duality. arXiv
preprint arXiv:2001.01866.
OpenAI (2018). Openai five. https://blog.openai.com/openai-five/.
Peters, J. and Schaal, S. (2008a). Natural actor-critic. Neurocomputing,71 1180-1190.
Peters, J. and Schaal, S. (2008b). Reinforcement learning of motor skills with policy gradients.
Neural Networks, 21 682-697.
Puterman, M. L. (2014). Markov Decision Processes: Discrete Stochastic Dynamic Programming.
John Wiley & Sons.
Sallab, A. E., Abdou, M., Perot, E. and Yogamani, S. (2017). Deep reinforcement learning frame-
work for autonomous driving. Electronic Imaging, 2017 70-76.
Scherrer, B. (2013). On the performance bounds of some policy search dynamic programming
algorithms. arXiv preprint arXiv:1306.0539.
Scherrer, B., Ghavamzadeh, M., Gabillon, V., Lesner, B. and Geist, M. (2015). Approximate mod-
ified policy iteration and its application to the game of Tetris. Journal of Machine Learning
Research, 16 1629-1676.
Schulman, J., Levine, S., Abbeel, P., Jordan, M. and Moritz, P. (2015). Trust region policy optimiza-
tion. In International Conference on Machine Learning.
Schulman, J., Wolski, F., Dhariwal, P., Radford, A. and Klimov, O. (2017). Proximal policy opti-
mization algorithms. arXiv preprint arXiv:1707.06347.
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T.,
Baker, L., Lai, M., Bolton, A. et al. (2017). Mastering the game of Go without human knowl-
edge. Nature, 550 354-359.
Sutton, R. S. (1988). Learning to predict by the methods of temporal differences. Machine Learning,
3 9-44.
Sutton, R. S., Barto, A. G. et al. (1998). Introduction to Reinforcement Learning. MIT press.
Sutton, R. S., McAllester, D. A., Singh, S. P. and Mansour, Y. (2000). Policy gradient methods for
reinforcement learning with function approximation. In Advances in Neural Information Process-
ing Systems.
Sutton, R. S., Szepesvari, C. and Maei, H. R. (2008). A convergent o (n) algorithm for off-policy
temporal-difference learning with linear function approximation. Advances in neural information
processing systems, 21 1609-1616.
Szepesvari, C. and Munos, R. (2005). Finite time bounds for sampling based fitted value iteration.
In International Conference on Machine Learning. ACM.
Tosatto, S., Pirotta, M., D’Eramo, C. and Restelli, M. (2017). Boosted fitted Q-iteration. In Inter-
national Conference on Machine Learning.
Tropp, J. A. (2015). An introduction to matrix concentration inequalities. arXiv preprint
arXiv:1501.01571.
Tu, S. and Recht, B. (2018). The gap between model-based and model-free methods on the linear
quadratic regulator: An asymptotic viewpoint. arXiv preprint arXiv:1812.03565.
Uehara, M. and Jiang, N. (2019). Minimax weight and Q-function learning for off-policy evaluation.
arXiv preprint arXiv:1910.12809.
Vinyals, O., Babuschkin, I., Chung, J., Mathieu, M., Jaderberg, M., Czarnecki, W., Dudzik, A.,
Huang, A., Georgiev, P., Powell, R. et al. (2019). Alphastar: Mastering the Real-
Time Strategy Game StarCraft II. https://deepmind.com/blog/article/
alphastar-mastering-real-time-strategy-game-starcraft-ii/.
12
Published as a conference paper at ICLR 2021
Wang, L., Cai, Q., Yang, Z. and Wang, Z. (2019). Neural policy gradient methods: Global optimality
and rates of convergence. arXiv preprint arXiv:1909.01150.
Wu, L., Ma, C. and Weinan, E. (2018). How SGD selects the global minima in over-parameterized
learning: A dynamical stability perspective. In Advances in Neural Information Processing Sys-
tems.
Wu, Y., Zhang, W., Xu, P. and Gu, Q. (2020). A finite time analysis of two time-scale actor critic
methods. arXiv preprint arXiv:2005.01350.
Xie, T., Ma, Y. and Wang, Y.-X. (2019). Towards optimal off-policy evaluation for reinforcement
learning with marginalized importance sampling. In Advances in Neural Information Processing
Systems.
Xu, T., Wang, Z. and Liang, Y. (2020). Non-asymptotic convergence analysis of two time-scale
(natural) actor-critic algorithms. arXiv preprint arXiv:2005.03557.
Xu, T., Zou, S. and Liang, Y. (2019). Two time-scale off-policy TD learning: Non-asymptotic anal-
ysis over Markovian samples. In Advances in Neural Information Processing Systems.
Yang, Z., Chen, Y., Hong, M. and Wang, Z. (2019a). On the global convergence of actor-critic: A
case for linear quadratic regulator with ergodic cost. arXiv preprint arXiv:1907.06246.
Yang, Z., Xie, Y. and Wang, Z. (2019b). A theoretical analysis of deep Q-learning. arXiv preprint
arXiv:1901.00137.
Yu, H. (2010). Convergence of least squares temporal difference methods under general conditions.
In International Conference on Machine Learning.
Zhang, K., Koppel, A., Zhu, H. and Bayar, T. (2019). Global convergence of policy gradient methods
to (almost) locally optimal policies. arXiv preprint arXiv:1906.08383.
Zhang, R., Dai, B., Li, L. and Schuurmans, D. (2020). Gendice: Generalized offline estimation of
stationary values. arXiv preprint arXiv:2002.09072.
Zou, D., Cao, Y., Zhou, D. and Gu, Q. (2018). Stochastic gradient descent optimizes over-
parameterized deep ReLU networks. arXiv preprint arXiv:1811.08888.
Zou, S., Xu, T. and Liang, Y. (2019). Finite-sample analysis for SARSA with linear function ap-
proximation. In Advances in Neural Information Processing Systems.
13
Published as a conference paper at ICLR 2021
A Deep Neural Network Approximation
In this section, we consider deep neural network approximation. We first formally define deep neural
networks. Then we introduce the actor-critic method under such a parameterization.
A deep neural network (DNN) uθ(x) with the input x ∈ Rd, depth H, and width m is defined as
X(O) = χ, x(h) = √= ∙ σ(W>x(h-1)), for h ∈ [H], uθ(X)= b>x(H).	(A.1)
Here σ : Rm → Rm is the rectified linear unit (ReLU) activation function, which is define as σ(y) =
(max{0, y1}, . . . , max{0, ym})> for any y = (y1, . . . , ym)> ∈ Rm. Also, we have b ∈ {-1, 1}m,
W1 ∈ Rd×m, and Wh ∈ Rm×m for 2 ≤ h ≤ H. Meanwhile, we denote the parameter of the
DNN uθ as θ = (vec(W1)>, . . . , vec(WH)>)> ∈ Rmall with mall = md + (H - 1)m2. We call
{Wh}h∈[H] the weight matrices of θ. Without loss of generality, we normalize the input X such that
kXk2 = 1.
We initialize the DNN such that each entry ofWh follows the standard Gaussian distribution N (0, 1)
for any h ∈ [H], while each entry of b follows the uniform distribution Unif ({-1, 1}). Without loss
of generality, we fix b during training and only optimize {Wh}h∈[H] . We denote the initialization of
the parameter θ as θ0 = (vec(W10)> , . . . , vec(WH0 )> )> . Meanwhile, we restrict θ within the ball
B(θ0 , R) during training, which is defined as follows,
B(θo,R) = {θ ∈ Rmall: IlWh - W0∣∣f ≤ R, for h ∈ [H]}.	(A.2)
Here {Wh}h∈[H] and {Wh0}h∈[H] are the weight matrices of θ and θ0, respectively. By (A.2), we
have ∣∣θ 一 θo∣∣2 ≤ R√H for any θ ∈ B(θ0, R). Now, We define the family of DNNs as
U(m, H, R) = uθ : θ ∈ B(θ0, R)},	(A.3)
where uθ is a DNN with depth H and width m.
We parameterize the action-value function using Qω (s, a) ∈ U (mc, Hc, Rc) and the energy func-
tion of the energy-based policy πθ using fθ(s, a) ∈ U(ma, Ha, Ra). Here U(mc, Hc, Rc) and
U (ma, Ha , Ra) are the families of DNNs defined in (A.3). Hereafter we assume that the energy
function fθ and the action-value function Qω share the same architecture and initialization, i.e.,
ma = mc, Ha = Hc, Ra = Rc, and θ0 = ω0 . Such shared architecture and initialization of the
DNNs ensure that the parameterizations of the policy and the action-value function are approxi-
mately compatible. See Sutton et al. (2000); Konda and Tsitsiklis (2000); Kakade (2002); Peters
and Schaal (2008a); Wang et al. (2019) for a detailed discussion.
Actor Update. To solve (3.1), we use projected stochastic gradient descent, whose n-th iteration
has the following form,
θ(n + 1)
—rB(θo,Ra) (Wn)- α ∙ "θ(n)(S, O)- τk+1 ∙ (β-1Qωfc (S, a) + T-I fθk (S, a)) ) ∙ Vθ fθ(n) (S,a)).
Here ΓB(θ0,Ra) is the projection operator, which projects the parameter onto the ball B(θ0, Ra)
defined in (A.2). The state-action pair (S, a) is sampled from the stationary state-action distribution
ρk. We summarize the update in Algorithm 3, which is deferred to §B of the appendix.
Critic Update. To solve (3.2), we apply projected stochastic gradient descent. More specifically, at
the n-th iteration of projected stochastic gradient descent, we sample a tuple (S, a, r, S0,a0), where
(s, a)〜ρk+ι, r = r(s, a), s0 〜 P(∙ | s, a), and O 〜 ∏θk+ι (∙ | s0). We define the residual atthe n-th
iteration as δ(n) = Qω(n)(s, a) — (1 - Y) ∙ r - Y ∙ Qωk (s0, α0). Then the n-th iteration of projected
stochastic gradient descent has the following form,
ω(n + 1) - ΓB(ωo,Rc) (ω(n) - η ∙ δ(n) ∙VωQω(n)(s,α)).
Here ΓB(ω0,Rc) is the projection operator, which projects the parameter onto the ball B(ω0, Rc)
defined in (A.2). We summarize the update in Algorithm 4, which is deferred to §B of the appendix.
By assembling Algorithms 3 and 4, we present the deep neural actor-critic method in Algorithm 2,
which is deferred to §B of the appendix.
Finally, we remark that the off-policy actor and critic updates given in (3.6) and (3.7) can also
incorporate deep neural network approximation with a slight modification, which enables data reuse
in the algorithm.
14
Published as a conference paper at ICLR 2021
B	Details of Algorithms
In this section, we summarize the algorithms in §3. We first introduce the actor-critic method with
linear function approximation in Algorithm 1.
Algorithm 1 Linear Actor-Critic Method
Input: Number of iterations K, sample size N, temperature parameter β.
Initialization: Set τo J ∞, and randomly initialize the actor parameter θo and the critic param-
eter ω0 .
for k = 0, 1, 2, . . . , K do
Actor Update: Update θk+ι via (3.3) with τ-1 = (k + 1) ∙ β-1.
Critic Update: Sample {(s',ι,a',ι)}'∈[N] and {(s',2,a',2,r',2,s',2,a',2)}'∈[N] as specified
in §3.1. Update ωk+1 via (3.5).
end for
Output: {∏θk}k∈[κ+i], where ∏θk H exp(τ-1fθk).
We introduce the actor-critic method with DNN approximation in Algorithm 2, which relies on
Algorithms 3 and 4 for the actor and critic updates.
Algorithm 2 Deep Neural Actor-Critic Method
Input: Number of iterations K, Na, Nc, stepsizes α, η, and temperature parameter β.
Initialization: Set τ0 J ∞ and initialize DNNs fθ0 and Qω0 as specified in §A.
for k = 0, 1, 2, . . . , K do
Actor Update: Update θk+ι via Algorithm 3 with input ∏θk, θo, Qω⅛, α, β, τk+ι = (k + 1)-1∙
β, and Na .
Critic Update: Update ωk+1 via Algorithm 4 with input πθk+1, Qωk, ω0, η, and Nc.
end for
Output: {πθk}k∈[K+1], where πθk H exp(τk-1fθk).
Algorithm 3 Actor Update for Deep Neural Actor-Critic Method
Input: Policy πθ H exp(τ -1 fθ), initial actor parameter θ0, action-value function Qω, stepsize α,
temperature parameter β, temperature τe, and number of iterations Na .
Initialization: Set θ(0) J θ0.
for n = 0, 1, 2, . . . , Na - 1 do
Sample (s, a) as specified in §A.
Set θ(n + I) J rB(θo,Ra)Wn)- α ∙ (fθ(n)(s,a) - τ ∙ (β-1Qω (S,a) + τ-1fθ(S,aO)) ∙
Ne fθ(n)(S, a)).
end for _
Output: θ = 1/Na ∙ Pn= 1 θ(n).
Algorithm 4 Critic Update for Deep Neural Actor-Critic Method
Input: Policy ∏e, action-value function Qω, initial critic parameter ω0, stepsize η, and number of
iterations Nc .
Initialization: Set ω(0) J ω0.
for n = 0, 1, 2, . . . , Nc - 1 do
Sample (S, a, r, S0, a0) as specified in §A.
Set δ(n) J Qω(n)(s, a) — (1 — Y) ∙ r — Y ∙ Qω(s0, a0).
Set ω(n + 1) J rB(ωo,Rc)(ω(n) - η ∙ δ(n) ∙ VωQω(n)(S,a))∙
end for
Output: ω = 1/Nc ∙ PN= 1 ω(n).
15
Published as a conference paper at ICLR 2021
C	Convergence Results of Algorithm 2
In this section, we upper bound the regret of the deep neural actor-critic method. Hereafter we
assume that |r(s, a)| ≤ rmax for any (s, a) ∈ S × A, where rmax is a positive absolute constant.
First, We impose the following assumptions in parallel to Assumption 4.1. Recall that ρ* is the
stationary state-action distribution of π*, while Pk is the stationary state-action distribution of ∏θk.
Assumption C.1 (Concentrability Coefficient). The following statements hold.
(i)	There exists a positive absolute constant φ* such that φk ≤ φ* for any k ≥ 1, where
Φk = kdρ*∕dρkkρk,2.
(ii)	For the state-action distribution ρ used to define the regret in (4.1), we assume that for
any k ≥ 1 and a sequence of policies {πi}i≥1, the k-step future-state-action distribution
ρPπ1 ∙∙∙ Pnk is absolutely continuous with respect to ρ*. Also, it holds that
∞
Cρ,ρ* =(I-Y)2 X k3γk ∙ c(k) < ∞,
whereHk) = SUp{∏i}i∈[k] kd(PPnI …Pnk"dρ*kρ*,∞.
Meanwhile, we impose the following assumption in parallel to Assumption 4.2.
Assumption C.2 (Zero Approximation Error). For any Qω ∈ U(mc, Hc, Rc) and policy π, it holds
that TπQω ∈ U(mc, Hc, Rc), where Tπ is defined in (2.4).
Assumption C.2 states that U(mc, Hc, Rc) is closed under the Bellman evaluation operator Tπ,
which is commonly imposed in the literature (Munos and Szepesvari, 2008; Antos et al., 2008a;
Farahmand et al., 2010; 2016; Tosatto et al., 2017; Yang et al., 2019b; Liu et al., 2019).
We upper bound the regret of the deep neural actor-critic method in Algorithm 2 in the sequel. To
establish such an upper bound, we first establish the rates of convergence of Algorithms 3 and 4 as
follows.
Proposition C.3. For any sufficiently large Na	>	0, let ma =
Ω(d3/2R-1H-3J log(ma/2/Ra)3∕2), Ha = O(N1/4), and Ra = O(m1/2H-6(log ma)-3).
We denote by θ the output of Algorithm 3 with input ∏θ 8 exp(τ-1fθ), θo, Qω, α, β,
τ = (τT + β-1)-1, and Na. Also, let f = T ∙ (β-1Qω + T-1fθ). With probability at least
1 - exp(-C(Ra/3ma/3Ha)) over the random initialization θo, we have
E[(fθ(s, a) — f(s, a))2] = O(RaN-1/ + R^m-1"/ log ma).
Here the expectation is taken over the randomness of θ conditioning on the initialization θo and
(s, a)〜ρ∏θ, where ρ∏ is the stationary state-action distribution of ∏θ.
Proof. See §G.2 for a detailed proof.	□
Proposition C.4. For any sufficiently large Nc	>	0, let mc =
Ω(d312R-1H-3∕2 log(m1∕2∕Rc)3∕2), Hc = O(N1∖ and Rc = O(m112H-6(log mc)-3).
We denote by ω the output of Algorithm 4 with input ∏θ, Qω, ω0, η, and Nc. Also, let
Q = (1 - γ) ∙ r + γ ∙ Pπθ Qω. With probability at least 1 - exp(-Ω(R2∕3m213HC)) over the random
initialization ω0, we have
E[(Qω(s, a) - Q(s, a))2] = O(RCNL/ + Rcam-1^H logm°)∙
Here the expectation is taken over the randomness of ω conditioning on the initialization ωo and
(s, a)〜ρ∏θ, where ρ∏θ is the stationary state-action distribution of ∏θ.
Proof. See §G.3 for a detailed proof.
□
16
Published as a conference paper at ICLR 2021
Propositions C.3 and C.4 characterize the errors that arise from the actor and critic updates in Algo-
rithm 2, respectively. In particular, if the widths ma and mc of the DNNs fθ and Qω are sufficiently
large, the errors characterized in Propositions C.3 and C.4 decay to zero at the rates of O(Na-1/2)
and O(Nc-1/2), respectively. Propositions C.3 and C.4 act as the key ingredients to upper bounding
the regret of the deep neural actor-critic method.
Based on Propositions C.3 and C.4, we upper bound the regret of Algorithm 2 in the following
theorem, which is in parallel to Theorem 4.4.
Theorem C.5. We assume that Assumptions C.1 and C.2 hold. Let ρ be a state-action distri-
bution satisfying (ii) of Assumption C.1. Also, for any sufficiently large K > 0, let Na =
Ω(K2C4,ρ* (φ* + ψ* + 1)4R4), Nc = Ω(K2c4,ρ*φ*4R4), Ha = Hc = O(N1/4), Ra = Rc =
O(m1∕2H-6(log mc)-3), ma = mc = Ω(d3^K6Cρ,29* (φ*+ψ* + 1)12R16Hc42 log(m1/2/Rc)3/2),
β = K1/2, and the sequence {θk}k∈[K] be generated by Algorithm 2. With probability at least
1 - 1/K over the random initialization θ0 and ω0 , it holds that
K
E [X Q*(s,a)- Qπθk+ι (s,a)] ≤(2(1 — γ)-3 log |A| +。⑴)∙ K1/2,
k=0
where the expectation is taken over the randomness of (s, a)〜P and {θk+ι}k∈[κ] conditioning on
the initialization θ0 and ω0 .
Proof. See §E.2 for a detailed proof.	口
When the architecture of the actor and critic neural networks are properly chosen, Theorem C.5
establishes an O(K1/2) regret of Algorithm 2, where K is the total number of iterations. Specifically
speaking, to establish such a regret upper bound, we need the widths ma and mc of the DNNs fθ
and Qω to be sufficiently large. Meanwhile, to control the errors of actor update and critic update in
Algorithm 2, we also run sufficiently large numbers of iterations in Algorithms 3 and 4.
In terms of the total sample complexity, to simplify our discussion, we omit constant and logarithmic
terms here. To obtain an ε-globally optimal policy, it suffices to set K ε-2 in Algorithm 2. By
plugging such a K into Na = Ω(K2C4,ρ* (φ* + ψ* + 1)4R4) and Nc = Ω(K2C4,ρ*φ*4R4) as
required in Theorem C.5, we have Na = Oe(ε-4) and Nc = Oe(ε-4). Thus, to achieve an ε-globally
optimal policy, the total sample complexity of Algorithm 2 is O(ε-6). With the modification to off-
policy setting as in §3.1, the total sample complexity of Algorithm 2 is O(ε-4). In comparison, Liu
et al. (2019) requires a total sample complexity of O(ε-8) to achieve an ε-globally optimal policy,
which is worse than our single-timescale algorithm. Meanwhile, since Liu et al. (2019) uses TD(0)
in the critic update, which is shown to diverge under off-policy setting even with linear function
approximation (Baird et al., 1995), the method of data reuse cannot be applied to Liu et al. (2019)
to eliminate the total sample complexity.
To the best of our knowledge, we establish the rate of convergence and global optimality of the
actor-critic method under single-timescale setting with DNN approximation for the first time.
D	Proof S ketch of Main Theorem 4.4
In this section, we sketch the proof of Theorem 4.4. Recall that ρ is a state-action distribution
satisfying (ii) of Assumption 4.1. We first upper bound PK=o(Q*(s, a) 一 Qπθk+1 (s, a)) for any
(s, a) ∈ S × A in part 1. Then by further taking the expectation over ρ in part 2, we conclude the
proof of Theorem 4.4. See §E.1 for a detailed proof.
Part 1.	In the sequel, We upper bound PK=o(Q*(s, a) 一 Qrπθk+1 (s, a)) for any (s, a) ∈ S × A. We
first decompose Q* — Qπθk+1 into the following three terms,
KK
X[Q* — Qπθk+ι](s,	a)	= X[(I	— γPπ*)-1(Aι,k	+	A2,k	+	A3,k)](s, a),	(D.1)
k=0	k=0
17
Published as a conference paper at ICLR 2021
the proof of which is deferred to (E.1) and (E.2) in §E.1 of the appendix. Here the operator Pπ* is
defined in (2.3), (I - γPπ* )-1 = P∞=0(γ Pπ* )i, and Aι,k, A2,k ,and A3,k are defined as follows,
*
A1,k(s,a)=[γ(Pπ - Pπθk+1)Qωk](s, a),	(D.2)
A2,k(s,a) = γ Pπ* (Qπθk+1 - Qωk)(s, a),	(D.3)
A3,k(s,a)=[Tπθk+1Qωk-Qπθk+1](s,a).	(D.4)
To understand the intuition behind A1,k, A2,k, and A3,k, we interpret them as follows.
Interpretation of A1,k. As defined in (D.2), A1,k arises from the actor update and measures the con-
vergence of the policy ∏θk+ι towards a globally optimal policy ∏*, which implies the convergence
of Pπθk+1 towards Pπ* .
Interpretation ofA3,k. Note that by (2.2) and (2.4), we have Qπθk+1 = Tπθk+1 Qπθk+1 and Tπθk+1
is a γ-contraction, which implies that applying the Bellman evaluation operator Tπθk+1 to any Q,
e.g., Qωk, infinite times yields Qπθk+1 . As defined in (D.4), A3,k measures the error of tracking
the action-value function Qπθk+1 of πθk+1 by applying the Bellman evaluation operator Tπθk+1 to
Qωk only once, which arises from the critic update. Also, as A3,k = Tπθk+1 (Qωk - Qπθk+1 ), A3,k
measures the difference between Qπθk , which is approximated by Qωk as discussed subsequently,
and Qπθk+1 . Such a difference can also be viewed as the difference between πθk and πθk+1 , which
arises from the actor update. Therefore, the convergence of A3,k to zero implies the contractions
of not only the critic update but also the actor update, which illustrates the “double contraction”
phenomenon. We establish the convergence of A3,k to zero in (D.10) subsequently.
Interpretation of A2,k. Assuming that A3,k-1 converges to zero, we have Tπθk Qωk-1 ≈ Qπθk .
Moreover, assuming that the number of data points N is sufficiently large and ignoring the pro-
jection in (3.5), we have Tπθk Qωk-1 = Qωek ≈ Qωk as ωek defined in (3.4) is an estimator of ωk.
Hence, we have Qπθk ≈ Qωk. Such an approximation error is characterized by ck defined in (D.5)
subsequently. Hence, A2,k measures the difference between πθk and πθk+1 through the difference
between Qπθk ≈ Qωk and Qπθk+1 , which relies on the convergence of A3,k-1 to zero.
In the sequel, we upper bound A1,k, A2,k, and A3,k, respectively. To establish such upper bounds,
we define the following quantities,
ck+1(s, a) = [Tπθk+1 Qωk - Qωk+1](s, a),	(D.5)
ek+1(s, a) = [Qωk - Tπθk+1 Qωk](s, a),	(D.6)
W (S) = KL(π* (∙ | S) k πθk (∙ | S)) - KL(π*(∙ | S) k πθk+ι (∙ | S)).	(D.7)
To understand the intuition behind 僦+「ek+ι, and IOk, We interpret them as follows.
Interpretation of ck+1. Recall that ωek+1 is defined in (3.4), which parameterizes Tπθk+1 Qωk (ig-
noring the projection in (3.5)). Here ck+1 arises from approximating ωek+1 using ωk+1 as an estima-
tor, which is constructed based on ωk and the N data points. In particular, ck+1 decreases to zero as
N → ∞, which is used in characterizing A2,k defined in (D.3).
Interpretation of ek+1. Assuming that A3,k-1 defined in (D.4) and ck defined in (D.5) converge
to zero, which implies Tπθk Qωk-1 ≈ Qπθk and Tπθk Qωk-1 ≈ Qωk, respectively, we have Qωk ≈
Qπθk . Therefore, as defined in (D.6), ek+1 = Qωk -k-T1πθk+1 Qkωk ≈ Qπθk - Tπθk+1 Qπθkk =
(Tπθk - Tπθk+1 )Qπθk measures the difference between πθk and πθk+1 , which implies the difference
between Tπθk and Tπθk+1 . We remark that ek+1 fully characterizes A3,k defined in (D.4) as shown
in (D.8) subsequently.
Interpretation of Ok. As defined in (D.7), Ok measures the difference between πθk and πθk+1 in
terms of their differences with ∏*, which are measured by the corresponding KL-divergences. In
particular, Ok is used in characterizing A1,k and A2,k defined in (D.2) and (D.3), respectively.
We remark that ck+1 measures the statistical error in the critic update, while Ok measures the opti-
mization error in the actor update. As discussed above, the convergence of A3,k to zero implies the
18
Published as a conference paper at ICLR 2021
Critic Update:
Actor Update:
Figure 1: Illustration of the relationship among Aι,k, A2,k, A3,k, ek+「ek+ι, and ®k. Here {θk,ωk}
and {θk+1, ωk+1} are two consecutive iterates of actor-critic. The red arrow from Qωk to Qωk+1
represents the critic update and the red arrow from Qπθk to Qπθk+1 represents the action-value
functions associated with the two policies in any actor update. Here Iok given in (D.7) quantifies the
difference between ∏θk and ∏θk+ι in terms of their KL distances to ∏*. In addition, the cyan arrows
represent quantities Aι,k, A2,k, and A3,k introduced in (D.2)-(D.4), which are intermediate terms
used for analyzing the error Q* - Qπk+1. Finally, the blue arrows represent εk+ι and ek+1 defined
in (D.5) and (D.6), respectively. Here εck+1 corresponds to the statistical error due to having finite
data whereas ek+1 essentially quantifies the difference between πθk and πθk+1 .
contraction of both the actor update and the critic update, which illustrates the “double contraction”
phenomenon. Meanwhile, since ek+1 fully characterizes A3,k as shown in (D.8) subsequently, ek+1
plays a key role in the “double contraction” phenomenon. In particular, the convergence of ek+1 to
zero is established in (D.9) subsequently. See Figure 1 for an illustration of these quantities.
With the quantities defined in (D.5), (D.6), and (D.7), we upper bound A1,k, A2,k, and A3,k as
follows,
Aι,k(s, a) ≤ γβ ∙ [P0k](s, a),
k-1
A2,k(s, a) ≤ [(γPπ* )k+1(Q* - Qωo)](s, a) + γβ ∙ X[(γPπ* )k-iPOi] (s, a)
i=0
k-1
+ X[(γPπ*)k-ieic+1(s, a),
i=0
A3,k(s,a) = [γPπθk+1 (I - γPπθk+1)-1ek+1(s, a),	(D.8)
the proof of which is deferred to Lemmas E.1, E.2, and E.3 in §E.1 of the appendix, respectively.
Meanwhile, by recursively expanding (D.5) and (D.6), we have
k	kk
ek+1(s, a) ≤	γk	Y Pπθs	e1+Xγk-i	Y	Pπθs	(I -	γPπθi)eic	(s, a),	(D.9)
s=1	i=1	s=i+1
19
Published as a conference paper at ICLR 2021
the proof of which is deferred to Lemma E.4 in §E.1 of the appendix. By plugging (D.9) into (D.8),
we have
A3,k(s,a) ≤ γPπθk+1 (I - γ Pπθk+1)-1 γkYPπθse1	(D.10)
s=1
kk
+Xγk-i Y Pπθs (I-γPπθi)ic	(s, a).
i=1	s=i+1
To better understand (D.10) and how it relates to the convergence of A3,k, A2,k, and A1,k to zero,
we discuss in the following two steps.
Step (i). We assume ic = 0, which corresponds to the number of data points N → ∞. Then (D.10)
yields A3,k = O(γk), which implies that A3,k defined in (D.4) converges to zero driven by the
discount factor γ. As discussed above, the convergence of A3,k to zero also implies the contraction
between πθk and πθk+1 of the actor update and the contraction between Qωk and Qπθk of the critic
update, which illustrates the “double contraction” phenomenon.
Step (ii). The convergence of A3,k to zero further ensures that A2,k converges to zero. To see
this, we further assume A3,k = 0, which together with the assumption that ck+1 = 0 implies
Qπθk+1 = Tπθk+1 Qωk = Qωk+1 by their definitions in (D.4) and (D.5), respectively. Then by
telescoping the sum of A2,k defined in (D.3), which cancels out Qωk+1 and Qπθk+1 , we obtain the
convergence of A2,k to zero. Meanwhile, telescoping the sum of A1,k defined in (D.2) and the sum
of its upper bound in (D.8) implies that A1,k converges to zero.
Now, by plugging (D.8) and (D.10) into (D.1), We establish an upper bound of PK=O(Q*(s, a)-
Qπθk+1 (s, a)) for any (s, a) ∈ S × A, which is deferred to (E.12) in §E.1 of the appendix. Hence,
we conclude the proof in part 1. See part 1 of §E.1 for details.
Part 2.	Recall that ρ is a state-action distribution satisfying (ii) of Assumption 4.1. In the sequel, we
take the expectation over ρ in (E.12) and upper bound each term. We first introduce the following
lemma, which upper bounds ck+1 defined in (D.5).
Lemma D.1. Under Assumptions 4.2 and 4.3, it holds for any k ≥ 1 that
E[ek+ι(s,a)2] = E[(Qω%+ι Ga)-[Tπ%QωJ(s, a))2] ≤ 16CrN； + R ∙bg(N + d)2,
where the expectation is taken with respect to randomness of ωk+ι and (s, a)〜ρk+ι.
Proof. See §H.1 for a detailed proof.	□
On the right-hand side of (E.12) in §E.1 of the appendix, for the terms not involving ck+1, i.e., M1,
M2, and M3 in (E.13), we take the expectation over P and establish their upper bounds in the '∞-
norm over (s, a) in Lemma E.5. On the other hand, for the terms involving ck+1, i.e., M4 and M5 in
(E.14), we take the expectation over ρ and then change the measure from ρ to ρk+1. By Assumption
4.1 and Lemma D.1, which relies on ρk+1, we establish the upper bounds in Lemma E.6. See part 2
of §E.1 for details.
Combining Lemmas E.5 and E.6 yields Theorem 4.4. See §E.1 for a detailed proof.
E	Proofs of Theorems
E.1 Proof of Theorem 4.4
Recall that ρ is a state-action distribution satisfying (ii) of Assumption 4.1. We first upper bound
PK=o(Q*(s, a) - Qπθk+1 (s, a)) for any (s, a) ∈ S × A in part 1. Then by further taking the
expectation over ρ and invoking Lemma D.1 in part 2, we conclude the proof of Theorem 4.4.
20
Published as a conference paper at ICLR 2021
Part 1. In the sequel, We upper bound PNO(Q*G a) — Qπθk+1 (s, a)) for any (s, a) ∈ S ×A. By
the definition of Q* in (2.2), it holds for any (s, a) ∈ S ×A that
[Q*- Qπθk+ι ](s,α)
∞
=X[(1 - γ) ∙ (γPπ* )'r](s, a) - Qπθk+1 (s, a)
'=0
∞
=X[(1 - Y) ∙ (γPπ* )'r + (γPπ* )'+1Qπθk+ι - (γPπ*)'+1Qπ%+1 ](s,a) - Qπ%+ι (s, a)
'=0
∞
=X[(1 - Y) ∙ (γPπ* )'r + (γPπ* )'+1Qπθk+ι - (γPπ*)'Qπθk+ι] (s, a)
'=0
∞
=X [(7Pπ* )'((1 - Y) ∙ r + Y ∙ Pπ*Qπθk+1 - Qπθk+1)] (s, a),	(E.1)
'=0
where Pπ* is defined in (2.3). We upper bound [(1 - γ) ∙ r+ γ ∙ Pπ*Qπ%+1 - Qπθk+1 ](s,a) onthe
RHS of (E.1) in the sequel. By calculation, we have
[(1 - γ) ∙ r + γ ∙ Pπ* Qπ%+1 - Qπ%+1] (s, a)
=[((1 -Y) ∙ r + Y ∙ Pπ*Qπθk+1 ) -((I-Y) ∙ r + γ ∙ Pπ*Qωfc)] (s, a)
+ [((1 - Y) ∙ r + Y ∙ Pπ* Qωk ) -((I- Y) ∙ r + Y ∙ Pπθfc+1 Qωk )](s,a)
+ [((1 - Y) ∙ r + Y ∙ Pπθk+1Qωk) - Qπθk+1](s,a)
=A1,k (s, a) + A2,k (s, a) + A3,k (s, a),	(E.2)
where Aι,k, A2,k, and A3,k are defined as follows,
A1,k (s,a)= [y(p** - Pπθk+1 )QωJ(s, a),
A2,k (s,a)= [γpπ* (Qπθk+1 - Qωfc )](s,a),
A3,k(s, a) = [Tπθk+1 Qωk - Qπθk+1 ](s, a).	(E.3)
Here Tπθk+1 is defined in (2.4). By the following three lemmas, we upper bound Aι,k, A2,k, and
A3,k on the RHS of (E.2), respectively.
Lemma E.1. It holds for any (s, a) ∈ S ×A that
Aι,k(s, a) = [γ(Pπ* - Pπ%+1 )Q" (s, a) ≤ [γβ ∙ P(% + ≡k+1)] (s, a),
where Iok and %〔 are defined as follows,
W(S) = kl(π*(∙∣ S)Il πθk(∙∣ S)) - KL(π*(∙∣ S)Il πθk+1 (∙∣ s)),	(E.4)
成+1(S) = (l0g(πθfc+1 (∙ I S)∕πθk (∙ I s)) - β-1 ∙ Qωk (s, ∙),π*(∙ I S)-πθk+1 (T s)〉.	(E.5)
Proof. See §H.2 for a detailed proof.	□
We remark that e∣+1 = 0 for any k in the linear actor-critic method. Meanwhile, such a term is
included in Lemma E.1 only aiming to generalize to the deep neural actor-critic method.
Lemma E.2. It holds for any (s, a) ∈ S ×A that
k-1
A2,k(S, a) ≤ [(γPπ* )k+1(Q* - Qωo )](s, a) + Ye ∙ X [(γPπ* )k-iP(必 + e2)](s, a)
i=0
k-1
+ X[(γPπ* )k-i*ι](S,a),
i=0
where 必 is defined in (E.4) of Lemma E.1, Cf+ is defined in (E.5) of Lemma E.1, and ∈C+ι is
defined as follows,
cc+1(S,a) = [Tπθi+1 Qωi - Q51](s, a)∙	(ES)
21
Published as a conference paper at ICLR 2021
Proof. See §H.3 for a detailed proof.
□
We remark that ak+1 = 0 for any k in the linear actor-critic method. Meanwhile, such a term is
included in Lemma E.2 only aiming to generalize to the deep neural actor-critic method.
Lemma E.3. It holds for any (s, a) ∈ S × A that
A3,k(s,a) = γPπθk+1 (I - γPπθk+1)-1ek+1(s, a),
where ek+1 is defined as follows,
ek+1(s, a) = [Qωk - Tπθk+1 Qωk](s, a).	(E.7)
Proof. See §H.4 for a detailed proof.	□
We upper bound ek+1 in (E.7) of Lemma E.3 using Lemma E.4 as follows.
Lemma E.4. It holds for any (s, a) ∈ S × A that
k	kk
ek+1 (s,a) ≤ Y k (Y Pπθs)eι + X Y k-i ( Y P"%) (YePeb+1 + (I - γPπθi H,a)
s=1	i=1	s=i+1
where eic(s, a) is defined in (E.6) of Lemma E.2 and eib+1(s) is defined as follows,
eb+1(S) = 0θg(πθi+1 (T s"πθi (T S)) - β-1 ∙ Qω"s, ∙),πθi (∙ | S)- πθi+ι (T S)〉.	(E.8)
Proof. See §H.5 for a detailed proof.	□
We remark that eib+1 = 0 for any i in the linear actor-critic method. Meanwhile, such a term is
included in Lemma E.4 only aiming to generalize to the deep neural actor-critic method.
Combining Lemmas E.3 and E.4, we obtain the following upper bound ofA3,k,
A3,k(S,a) = YPπθk+1 (I - YPπθk+1)-1ek+1(S, a)
≤ YPπθk+1(I-YPπθk+1)-1 YkYPπθse1	(E.9)
s=1
kk
+ X Yk-i( Y Pπθs) (βγPeb+ι + (I-γPπθi )eC)	(s,a).
i=1	s=i+1
Combining (E.1), (E.2), Lemma E.1 and Lemma E.2, it holds for any (S, a) ∈ S × A that
K
X[Q* - Qπθk+ι ](s,a)
k=0
Kk
≤ X [(I - γPπ* )-1 ((γPπ* )k+1(Q* - Qωo) + X(γPπ*)k-iYβP(必 + ea+ι)
k-1
+ X(YPπ*)k-ieic+1 + A3,k) (S, a)
i=0
K	Kk
=(I-YPπ* )-1 (X(YPπ* )k+1(Q*- Qωo) + XX(YP** )k-iγβPea+ι	(E.10)
K k-1	K	K k
+ X X(YPπ* )k-ieC+ι + X A3,k + X X(YPπ* )k-iYePM) (s,a),
k=0 i=0	k=0	k=0 i=0
22
Published as a conference paper at ICLR 2021
where 必，球十],前十],and ek+ι are defined in (E.4) of Lemma E.1, (E.5) of Lemma E.1, (E.6) of
Lemma E.2, and (E.7) of Lemma E.3, respectively. We upper bound the last term as follows,
γPπ*)
k-iγβP 必
Kk
(s, a) = XX
γβ(γPπ* )iPW-i (s,a)
k=0 i=0
KX i=0 KX =0i KX =i0 KX
===≤
K
γβ(γPπ* )iP X Ok-i (s,a)
k=i
K
γβ(γPπ* )iP X (KL(∏* k ∏θk-J - KL(∏* k ∏θk-i+j) (s,a)
k=i
Yβ(γPπ* )iP(KL(∏* k ∏θo) — KL(∏* k ∏θκ-i+ι)) (s, a)
*
γβ(γPπ )iPKL(π* k ∏θo) (s,a),	(E.11)
where we use the definition of Ok-i in (E.4) of Lemma E.1 and the non-negativity of the KL diver-
gence in the second equality and the last inequality, respectively. By plugging (E.9) and (E.11) into
(E.10), we have
K
X[Q* - Qπθk+ι](s,a)
k=0
K	Kk
≤ (I-γPπ*)-1 X(γPπ*)k+1(Q*-Qω0) + XX(γPπ*)k-iγβPea+ι	(E.12)
k=0	k=0 i=0
K k-1	K	k
+XX(γPπ*)k-iic+1 +Xγk+1Pπθk+1 (I - γPπθk+1)-1 Y Pπθs e1
k=0 i=0	k=0	s=1
K	kk
+ X Pπθk+1 (I-YPπθk+ι )-1 X Yk-'+1( Y Pπθs) (YePeb+1 + (I - γPπθ' )ec)J (s,a).
k=0	'=1	s='+1	人
K
+ X(γPπ* )iYβPKL(∏*k ∏θ0)
We remark that eia+1 = eib+1 = 0 for any i in the linear actor-critic method. Meanwhile, such terms is
included in (E.12) only aiming to generalize to the deep neural actor-critic method. This concludes
the proof in part 1.
Part 2. Recall that ρ is a state-action distribution satisfying (ii) of Assumption 4.1. In the sequel,
we take the expectation over ρ in (E.12) and upper bound each term. Recall that eia+1 = eib+1 = 0
for any i in the linear actor-critic method. Hence, we only need to consider terms in (E.12) that do
not involve eia+1 or eib+1. We first upper bound terms on the RHS of (E.12) that do not involve eic+1.
More specifically, for any measure ρ satisfying satisfying (ii) of Assumption 4.1, we upper bound
the following three terms,
K
Mi = Eρ [(I - γPπ*)-1 X(γPπ*)k+1(Q* - Qωo)],
k=0
M2 = Eρ h(I - YPπ*)-1 X Yk+1Pπθk+1 (I - YPπθk+1)-1 Y Pπθs e1i,
k=0	s=1
K
M3 = Eρ [(I - γPπ*)-i X(γPπ*)iγβPKL(π* k ∏θo)].	(E.13)
i=0
We upper bound M1 , M2 , and M3 in the following lemma.
23
Published as a conference paper at ICLR 2021
Lemma E.5. It holds that
|M1 | ≤ 4(1 - Y)-2 YrmaX + R),	|M2| ≤ (I - Y厂3 ∙ (2R + rmaX),
∣M3∣≤ (1 - γ)-2 ∙ log ∣A∣∙ K 1/2,
where M1, M2, and M3 are defined in (E.13).
Proof. See §H.6 for a detailed proof.	□
Now, we upper bound terms on the RHS of (E.12) that involve ic+1. More specifically, for any
measure ρ satisfying (ii) of Assumption 4.1, we upper bound the following two terms,
Kk
M4 = Eρ [(I - γPπ* )-1 X X(γPπ* )k-ieC+1],	(E.14)
k=0 i=0
K	kk
M5 = EP (I - γPπ* )-1 X Pπθk+ι (I - γPπθk+ι )-1 X γ k-'+1( Y Pπθs)(I - γPπθ' )ec .
k	k=0	'=1	s='+1	-
We upper bound M4 and M5 in the following lemma.
Lemma E.6. It holds that
∣M4∣≤ 3KCρ,ρ* ∙ εQ,	∣M5∣≤ KCρ,ρ* ∙ εQ.
where M4 and M5 are defined in (E.14).
Proof. See §H.7 for a detailed proof.	□
Now, by plugging Lemmas E.5 and E.6 into (E.12), we have
K
Eρ [X Q*(s,a)- Qπθk+ι (s,a)]
k=0
≤ 2(1 - Yr- ∙ log |A| ∙ K 1/2 + 4KCρ,ρ* ∙号Q + O(1).	(E.15)
Meanwhile, by changing measure from p to ρk+1, it holds for any k that
Eρ* [*1|] ≤ ,Eρk+ι[(ek+1 (s,a))2] ∙ Φk+1,	生⑹
where φk+1 is defined in Assumption 4.1. Also, by Lemma D.1, it holds that
qEPpk+1 [(ek+1(s,a))2] = θ(l∕(√Nσ*) ∙ log N).	(E.17)
Now, by plugging (E.17) into (E.16), combining the definition ofεQ = maxk Eρ* [|ck+1|], we have
εQ = θ(φ*∕(√Nσ*) ∙ log N).	(E.18)
Combining (E.15), (E.18), and the choices of parameters stated in the theorem that
N = Ω(KCp,p* (φ*∕σ*)2 ∙ log2 N),
we have
K
EPX Q*(s, a) - Qπθk+ι (s, a)] ≤(2(1 -Y)-3 log |A| + O(1)) ∙ K 1/2,
k=0
which concludes the proof of Theorem 4.4.
24
Published as a conference paper at ICLR 2021
E.2 Proof OF Theorem C.5
We follow the proof of Theorem 4.4 in §E.1. Following similar arguments when deriving (E.12) in
§E.1, We have
K
X[Q*- Qπθk+ι](s,α)
k=0
「	/ K	K k
≤ (I-γPπ*)-1∙ (X(Yk)k+1(Q*-Qωc,) + XX(Yk)k-i∙科得+1	(E.19)
K k-1	K
+ XX(Yk )k-iM+ι + X(Yk )i ∙ YeP ∙ KL(π* k ∏θo)
Kk
+ X Yk+1Pπθk+ι (I-YPnθk+ι )-1 (Y Pπθs 卜 1
K	k	k	∖ 1
+ X Pπθk+1 (I - YPπθk+ι )-1 X Yk-'+1( ∏ Pπθs) (βYP%ι-(I-YPπθ' )以)(s,α),
k=0	'=1	s='+1	)-
for any (s, a) ∈ S × A. Here	eb+ι, 6C+ι, and eι are defined in (E.5), (E.8), (E.6), and (E.7),
respectively.
Now, it remains to upper bound each term on the RHS of (E.19). We introduce the following error
propagation lemma.
Lemma E.7. Suppose that
EPk [(fθk+1 (s,ɑ) - τk + 1 ∙ (e-1Qωk (s,a) -τ-1fθk (s,a)))2]1∕2 ≤ εk+1,f .	(E.2O)
Then, we have
EV* [I*1(S)I] ≤ √2τk+1 ∙ εk+1,f ∙ (Φk + ψk) Eν* [|eb+i(s)|] ≤ √2τk+1 ∙ εk + 1,f ∙ (1 + ψk)
where ek+ι and e%i are defined in (E.5) and (E.8), respectively, φk and ψk are defined in Assump-
tion C.1.
Proof. See §H.8 for a detailed proof.
□
Following from Lemma F.4, with probability at least 1 - O(Hc) exp(-Ω(H-1mc)), We have
∣Qωo I ≤ 2. Also, from the fact that ∣r(s, α)∣ ≤ rmaχ, we know that |Qk| ≤ rmaχ. Therefore,
for any measure ρ, we have
K
∣Eρ [(I - yPπ* )-1 X(yPπ* )k+1(Qk - Qωo )]∣
k=0
K
≤ Eρ [(I - yPπ* )-1 X(yPπ* )k+1|Q* - Qωo |]
k=0
K
≤ rmax(1 - Y)T X Yk+1 ≤ rmax(1 - Y『.
k=0
(E.21)
25
Published as a conference paper at ICLR 2021
Also, by changing the index of summation, We have
K k
W [(I - γPπ* )-1 X X(γPπ* )k-iγβP输』
k=0 i=0
K k ∞
=I Eρ[XXX(γk)k-i+j γβ Pea+1] 1
k = 0 i = 0 j = 0
K k ∞
=I %[xxx(γPπ* )tγβP*ι]∣
k=0 i=0 t=k-i
K k ∞
≤ XX X I Eρ[(γPπ*)tYβP‰]∣,	(E.22)
k=0 i=0 t=k-i
where we expand (I — γPπ* )-1 into an infinite sum in the first equality. Further, by changing the
measure of the expectation on the RHS of (E.22), We have
K k ∞	K k ∞
XXX IE∕(γk)tγβPea+1] 1≤ XXXβγt+1c(t) ∙ Eν* [总1|],	(E.23)
k=0 i=0 t=k-i	k=0 i=0 t=k-i
where c(t) is defined in Assumption C.1. Further, by Lemma E.7 and interchanging the summation
on the RHS of (E.23), we have
Kk
I Eρ [(I - γPπ* )-1 X X(γPπ* )k-iγβPea" ∣
k=0 i=0
K ∞	k
≤ 2XX X	βγt+1c(t) ∙ Ti-1ιεf ⑻ + 此
k=0 t=0 i=max{0,k-1}
K ∞
≤ XX4ktγt+1 c(t) ∙ ε∕(φ*+ ψ*)
k=0 t=0
K
≤ Y X4Cρ,ρ* ∙ εf (φ* + ψ*) ≤ 2γKCρ,ρ*(φ* + ψ*) ∙ εf,	(E.24)
whereεf = maxi Eρz[(fθz+1 (s,a)-Ti+ι∙(β-1Qωl(s,⑺-7-1/%(s, a)))2]1/2, andCρ,ρ* is defined
in Assumption C.1. Here in the second inequality, we use the fact that T-I = (i + 1) ∙ β-1, and
φ↑ ≤ φ* and * ≤ ψ by Assumption C.1.
By similar arguments in the derivation of (E.24), we have
K k-1
I Eρ [(I - γPπ* )-1 XX(Yk)k iec+1]I ≤ 2(K + 1)Cρ,ρ* φ* ∙ εQ,	(E.25)
k=0 i=0
K
I Eρ [(I - γPπ* )-1 X(γPπ* )iγβPKL(π* k ∏θo)] I ≤ log ∣A∣∙ K 1/2(1 - γ)-2,
i=0
Kk
Eρ [(I - γPπ* )-1 X γk+1Pπ%+ι (I - γPπ%+ι )-1 (Y Pπθs 卜1] ≤ (2 + rmax) ∙ (1 - γ)-3,
k=0	s=1
where εQ = maxi Eρ* [|/+11]. And we use the fact that β = K1/2.
26
Published as a conference paper at ICLR 2021
Now, it remains to upper bound the last term on the RHS of (E.19). We first consider the terms
involving e∣+1. We have
「	K	k	k	-∣
EP (I-γPπ*)-1 XPπ%+ι (I-YPπ%+ι )-1 X Yl+1( Y Pπθs)β7Peb+1
-	k=0	'=1	s='+1	-
∞ ∞ Kk	ɪ-	k	-
=XXXXEP (γPπ*)j(γPπθk+ι )i+1γY Pπ%)βγP⅛l+1
j=0 i=0 k=0 '=1	L	s='+1	」
≤ βγ X X X X Yi+j+k-'+1 ∙ Eρ* [∣Peb+11] ∙ c(i + j + k - ' +1)
k=0'=1j=0 i=0
K k ∞	∞
≤ 2γ XXXX
Yi+j+k-'+1 ∙ (' +1)εf ∙ (1 + 媒)∙ c(i + j + k - 0 +1),	(E.26)
k=0'=1j=0 i=0
where we expand (I - γPπ* )-1 and (I - γPπθk+1 )-1 to infinite sums in the first equality, change
the measure of the expectation in the first inequality, and use Lemma E.7 in the last inequality. Now,
by changing the index of the summation, we have
K k ∞	∞
Y XXXX γi+j+k-'+1 ∙ (2 + 1)εf ∙ (1 + %)∙ c(i + j + k — ' + 1)
k=0'=1j=0 i=0
K k ∞	∞
=Y XXX X Yt∙(' +1)εf∙ (1 + 以)∙ c(t)
k=0 '=1 j=0 t=j + k-'+1
K ∞	∞	k
≤YXX X	X
Yt∙(4 +1)εf∙(1 + ψ*) ∙ c(t),	(E.27)
k=0 j=0 t=j + 1 '=max{0,j+k-1+1}
where we use the fact that ψ' ≤ ψ from Assumption C.1 in the last inequality. By further manipu-
lating the order of summations of the RHS of (E.27), we have
K ∞	∞	k
YXX X	X	Yt ∙ (' +1)εf(1 + ψ*) ∙ c(t)
k=0 j=0 t=j+1 '=max{0,j + k-1+1}
K ∞ j+k+1	∞
≤ YXX(X (t-j)(2k + j - k + 1) ∙ Ytc(t)+ X	k2 ∙ YtC(G ∙ εf(1 + ψ*)
k=0 j=0 t=j+1	t=j+k+2
K ∞	t-1
=Y X(X X	(t-j)(2k + j - k +1) ∙ Y tc(t)
k=0 t=1 j=max{0,t-k-1}
∞ t-k-2
+ X X k2 ∙ Y'c(t)) ∙ εf(1 + ψ*)
t=k+2 j=1
K ∞	∞
≤ 20Y X(X k2 ∙ tYtc(t) + Xk2 ∙ tY'c(t)) ∙ εf(1 + ψ*)
k=0 t=1	t=1
≤ 20yK ∙g,ρ* ∙ ε∕(1 + ψ*),	(E.28)
where we use the definition of CP ρ* from Assumption C.1 in the last inequality. Now, combining
(E.26), (E.27), and (E.28), we have
K
k
EP
(I - γPπ* )-1 E Pπθk+1 (I-YPnθk+ι )-1 EYk-'+1
k=0
2=1
k
(Y Pπθs) βγP%1
s='+1
≤ 20γK ∙Cρ,ρ* ∙ εf ∙ (1 + ψ*)∙
(E.29)
27
Published as a conference paper at ICLR 2021
Following from similar arguments when deriving (E.29), we have
K	kk
EP	(I-γPπ* )-1 XPπθk+1	(I	- γPπθk+1)-1 Xγk-'+1( Y	Pπθs)(I	- γPπθ' )e'
-	k=0	'=1	s='+1	-
≤ 20K ∙Cρ,ρ* φ* ∙ εQ,	(E.30)
Now, by plugging (E.21), (E.24), (E.25), (E.29), and (E.30) into (E.19), with probability at least
1 -O(Hc) exp(-Ω(H-1mc)), we have
K
EP [X Q*(s,a) - Qπθk+ι (s,a)]	(E.31)
k=0
≤ 2log |A| ∙ K 1/2(1 - γ)-3 + 60KCρ,ρ* (φ* + ψ* + 1) ∙ εf + 50KCρ,ρ*φ* ∙ εQ.
Meanwhile, following from Propositions C.3 and C.4, it holds with probability at least 1 - 1/K that
εf =O(RaN-1/4 + R4∕3m-1∕12H7∕2(log ma)1/2),
εQ =O(RCN-1/4 + R4/3m-1/12H7/2 (log mc )1/2).	(E.32)
Combining (E.31), (E.32), and the choices of parameters stated in the theorem, it holds with proba-
bility at least 1 - 1/K that
K
EPX Q*(s, a) - Qπθk+ι (s, a)] ≤(2(1 - γ)-3 log |A| + O(1)) ∙ K 1/2,
k=0
which concludes the proof of Theorem C.5.
F Supporting Results
In this section, we provide some supporting results in the proof of Theorems 4.4 and C.5. We
introduce Lemma F.1, which applies to both Algorithms 1 and 2. To introduce Lemma F.1, for any
policy π and action-value function Q, We define e(a | s) 8 exp(β-1Q(s, a)) ∙ π(a | s).
Lemma F.1. For any S ∈ S and ∏*, we have
β-1 ∙ hQ(S, ∙),πt (T S)- e(∙ | Ss) ≤ KL(πt(∙ | s) k π(∙ | S)) - KL(πt(∙ | s) k e(∙ | S))
+ Qog(e(∙ I s)∕∏(∙ | s)) - β-1 ∙ q(s, ∙),∏t(∙ | s) - e(∙ | s)〉.
Proof. By calculation, it suffices to show that
<log(e(∙ | s)∕π(∙ | S)), πt(∙ | s) - e(∙ | s)〉
≤ KL(πt(∙ | s) k π(∙ | s)) - KL(πt(∙ | s) k e(∙ |S)).
By the definition of the KL divergence, it holds for any s ∈ S that
KL(πt(∙ | s) k π(∙ | s)) - KL(πt(∙ | s) k e(∙ | s))
=0og(e(∙ I s)∕∏(∙ | S)),πt(∙ I s)〉.	(F.1)
Meanwhile, for the term on the RHS of (F.1), we have
0og(e(T s"πθk (∙ |S)),πt(T s)〉
=〈log(e(・| s"π(T S)),πt(T s) - e(∙ | s)〉
+αog(e(τS)/n(・|S)),e(τ s)〉
=0og(e(∙ |	s"π(∙	|	S)), πt(∙ |	s) -	e(∙ |	s)〉+	KL(e(∙	| s)	k π(∙	| s))
≥ Qog(e(∙∣	s)∕∏(∙	I	s)),∏t(∙ I	s) -	e(∙ I	s)〉.	(f.2)
Combining (F.1) and (F.2), we obtain that
<log(e(∙ i s)∕π(∙ i S)), πt(∙ i s) - e(∙ i s)〉
≤ KL(πt(∙ i s) k π(∙ i s)) - KL(πt(∙ i s) k e(∙ i S)),
which concludes the proof of Lemma F.1.	□
28
Published as a conference paper at ICLR 2021
F.1 Local Linearization of DNNs
In the proofs of Propositions C.3 and C.4 in §G.2 and §G.3, respectively, we utilize the linearization
of DNNs. We introduce some related auxiliary results here. First, We define the linearization U of
the DNN uθ ∈ U (w, H, R) as follows,
Uθ (∙) = Uθo (∙) + (θ - θθ )>Vθo Uθ (∙),
Where θ0 is the initialization of uθ . The folloWing lemmas characterize the linearization error.
Lemma F.2. Suppose that H = O(m1/12RT/6(logm)-1/2) and m = Q(d3/2RTH-3/2 ∙
log(m1/2/R)3/2). Then with probability at least 1 一 exp(-Ω(R2∕3m2∕3H)) over the random ini-
tialization θ0, it holds for any θ ∈ B(θ0, R) and any (s, a) ∈ S × A that
kVθUθ(s, a) - VθUθo(s, a)k2 = O(R1/3m-1/6H5/2(logm)1/2)
and
kVθuθ(s,a)k2=O(H).
Proof. See the proof of Lemma A.5 in Gao et al. (2019) for a detailed proof.	□
Lemma F.3. Suppose that H = O(m1/12R-1/6(logm)-1/2) and m = Ω(d3/2R-1H-3/2 ∙
log(m1∕2∕R)3∕2). Then with probability at least 1 — exp(—Ω(R2∕3m2∕3H)) over the random ini-
tialization θ0, it holds for any θ ∈ B(θ0, R) and any (s, a) ∈ S × A that
∣uθ(s, a) — Uθ(s, a)| = O^R4/3m-1/6H5/2(logm)1/2).
Proof. Recall that
Uθ(s, a) = uθo (s, a) + (θ - Θo)>Vθuj。(s, a).
By mean value theorem, there exists t ∈ [0, 1], which depends on θ and (s, a), such that
uθ(s,a) - Uθ(s,a) = (θ - θo)> (VjUθo+t(θ-θo)(s, a) -Vjuj。(s,a)).
Further by Lemma F.2, we have
|uj(s, a) - uj(s,a)∣ ≤ ∣∣θ - θ0∣∣2 ∙ ∣∣VjUθo+t∙(θ-θo)(s, a) - Vjuj。(s,a)∣∣2
=θ(R4∕3m-1/6 H5/2 (log m)”),
where we use Cauchy-Schwarz inequality in the first inequality. This concludes the proof of Lemma
F.3.	□
We denote by x(h) the output of the h-th layer of the DNN uj ∈ U(m, H, R), and x(h),0 the output
of the h-th layer of the DNN uj。 ∈ U(m, H, R). The following lemma upper bounds the distance
between x(h) and x(h),0.
Lemma F.4. With probability at least 1 - exp(-Ω(R2/3m2/3 H)) over the random initialization θ0,
for any θ ∈ B(θ0 , R) and any h ∈ [H], we have
∣∣x(h) - x(h),0∣2 =O(RH5∕2m-1∕2(logm)1/2).
Also, with probability at least 1 - O(H) exp(-Ω(H-1m)) over the random initialization θo, for
any θ ∈ B(θ0, R) and any h ∈ [H], it holds that
2/3 ≤ ∣x(h)∣2 ≤ 4/3.
Proof. The first inequality follows from Lemma A.5 in Gao et al. (2019), and the second inequality
follows from Lemma 7.1 in Allen-Zhu et al.(2018b).	□
29
Published as a conference paper at ICLR 2021
G Proofs of Propositions
G.1 Proof of Proposition 3.1
The proof follows the proof of Proposition 3.1 in Liu et al. (2019). First, we write the update
ek+1 J argmax∏ Eνk [hQωk (s, ∙), ∏(∙ | s)i - β ∙ KL(π(∙ | S) ∣∣ ∏θk (∙ | s))[ as a constrained OPtimiza-
tion problem in the following way,
max EVk [hπ(∙ | S), Qωk (Sci- β ∙ KL(π(∙ | S) k πθk (∙ | S))]
s.t. π(a | s) = 1,	for any s ∈ S.
a∈A
We consider the Lagrangian of the above Program,
( s"' 1 s),Qωk(S, .)〉-β ∙ KL(π(∙ | s) k πθk (∙ | S)))dvk (s) +/ S(X π(a 1 S)- 1)dX(S),
where λ(∙) is the dual parameter, which is a function on S. Now, by plugging in
πθk (a |S)=
eχp(τ-1fθk(S,a))
Pa0∈A exP(τ-1fθk (S,a0))，
we have the following optimality condition,
Qωk (S,a) + βτ-1fθk (S,a) — β ∙
log X exp(τk-1fθk(S,a0))
a0∈A
+ log π(a 1S) + 1)+ 黑=0,
for any (S, a) ∈ S × A. Note that log(Pa0∈A exp(τk-1fθk (S, a0))) is only a function of S. Thus, we
have
∏k+1 (a | S) H exp(β-1Qωk (s, a) + τ-1fθk (S,a))
for any (S, a) ∈ S × A, which concludes the proof of Proposition 3.1.
G.2 Proof of Proposition C.3
We define the local linearization of fθ as follows,
fθ = fθ0 +(θ - θθ)>Vθ°fθ.	(G.1)
Meanwhile, we denote by	
gn =	fθ(n)	— e ∙	(β-1Qω	+ T-1fθ)) ∙	Vθ fθ(n),	g；e	=	Eρ∏θ	[gn],	
gn =	(fθ(n)	— e ∙	(β-1Qω	+ T-1fθ)) ∙	Vθfθ0 ,	g；e	=	Eρ∏θ	®n],	
g* = (fθ* — e ∙ (β-1Qω + τ-1fθ)) ∙ Vθfθ*,	ge = Eρ∏θ [g*],	
g* = (fθ* — e ∙ (β-1Qω + τ-1fθ)) ∙ Vθfθo,	ge = Eρ∏θ [g*],	(G.2)
where θ* satisfies that	
θ* = rB(θo,Ra)(θ* — α ∙ ge).	(G.3)
By Algorithm 3, we know that	
θ(n +1) = rB(θo,Ra)(θ(n) — α ∙ gn).	(G.4)
By (G.3) and (G.4), we have	
Eρ∏θ [kθ(n +1)-θ*k2 I θ(n)]
=EP∏θ [krB(θo,Ra)(θ(n) — α ∙ gn) - rB(θo,Ra) (θ* - α ∙或32 | θ(n)]
≤ Eρ∏θ [k(θ(n) — α ∙ gn) — (θ* — α ∙ ge)k2 | θ(n)]
=kθ(n)	—	θ*k2 + 2α	∙hθ*	— θ(n), gη	—	祓i	+α2	∙	Eρ∏θ	[∣gn	— 阖|2 ∣ θ(n)],	(G.5)
'--------{Z-------}	'----------{-----------}
⑴	(ii)
30
Published as a conference paper at ICLR 2021
where we use the fact that ΓB(θ0,Ra) is a contraction mapping in the first inequality. We upper bound
term (i) and term (ii) on the RHS of (G.5) in the sequel.
Upper Bound of Term (i). By CaUchy-SchWarz inequality, it holds that
hθ* — θ(n), gn - gei = hθ* - θ(n), gn - gn i + hθ* - θ(n), gn - gei
≤ iiθ* - θ(n)k2 ∙ kgn - gn ι∣2 + hθ* - θ(n),gn - gei
≤ 2Ra ∙ kgn - gn ι∣2 + E- θ(n), gn - gei,	(GS)
where We use the fact that θ(n), θ* ∈ B(θo, Ra) in the last inequality. Further, by the definitions in
(G.2), it holds that
hθ* - θ(n),gn - gei = Eρ∏θ [(fθ(n) - fθ*) ∙ hθ* - θ(n), vθ fθoi]
=Eρ∏θ [(fθ(n) - fθ*) ∙ (fθ* - fθ(n))]
=-Eρ∏θ [(fθ(n) - fθ* )2],	(G.7)
where we use (G.1) in the second equality. Combining (G.6) and (G.7), we obtain the following
upper bound of term (i),
h& - θ(n), gn	- gei ≤ 2Ra ∙kgn - gn k2 - Eρ∏θ	[沅⑺ - fθ* )2 ].	(g.8)
Upper Bound	of Term (ii).	We now upper bound term (ii) on	the RHS of (G.5).	It	holds by
Cauchy-Schwarz inequality that
Eρ∏θ	[kgn	- gek2 I θ(n)] ≤	2Eρ∏θ [kgn - gnk2 I θ(n)] + 21成	- gk2
≤	2Eρ∏θ [kgn - gnk2 i θ(n)] +4 kgn	- gnk2 +4 kgn - gek2.	(g.9)
Vθ{z} X7} X7}
(ii)	.a	(ii).b	(ii).c
We upper bound term (ii).a, term (ii).b, and term (ii).c in the sequel.
Upper Bound of Term (ii).a. Note that
Eρπθ [kgn	-	gne k22	I θ(n)]	= Eρπθ	[kgnk22 -	kgne k22	I θ(n)]	≤ Eρπθ	[kgnk22	I θ(n)].	(G.10)
Meanwhile, by the definition of gn in (G.2), it holds that
kgnk2 = fθ(n) - T∙(β-1Qω + T -1fθ ))2 ∙∣∣Vθ fθ(n) k2∙	G")
We first upper bound fθ as follows,
fθ2 = x(Ha)>bb>x(Ha) = x(Ha)>x(Ha) = kx(Ha) k22,
where x(Ha) is the output of the Ha-th layer of the DNN fθ. Further combining Lemma F.4, it holds
with probability at least 1 - O(Ha) exp(-Ω(H-1ma)) that
Ifθ I ≤ 2.	(G.12)
Following from similar arguments, with probability at least 1 - O(Ha) exp(-Ω(H-1ma)), we have
IQωI ≤2,	Ifθ(n) I ≤2.	(G.13)
Combining Lemma F.2, (G.10), (G.11), (G.12), and (G.13), it holds with probability at least 1 -
exp(-C(R2/3ma/3Ha)) that
Eρπθ [kgn - gne k22 I θ(n)] = O(Ha2),	(G.14)
which establishes an upper bound of term (ii).a.
Upper Bound of Term (ii).b. It holds that
kgn - gn k2 = l∣Eρ∏θ [(fθ(n) - e ∙ (β-1Qω + T-1fθ)) ∙ Vθfθ(n)
- (fθ(n) - e ∙ (β-1Qω + τ-1fθ)) ∙ vθfθo] ||2
≤Eρπθ[kfθ(n)Vθfθ(n) - fgθ(n)Vθfθ0k2]
+ e ∙ Eρ∏θ [k(β-1Qω + T-1fθ) ∙ (Vθfθo - Vθfθ(n))k2]
≤Eρπθ[kfθ(n)Vθfθ0 -fgθ(n)Vθfθ0k2] + Eρπθ [kfθ(n)Vθfθ(n) - fθ(n)Vθfθ0k2]
(G.15)
+ Eρ∏θ [ke ∙ (β-1Qω + τ-1fθ) ∙ (vθfθo - vθfθ(n))k2].
31
Published as a conference paper at ICLR 2021
We upper bound the three terms on the RHS of (G.15) in the sequel, respectively.
For the term ∣∣fθ(n)Vθfθ0 - fθ(n)Vθfθ0 ∣∣2 on the RHS of (G.15), following from Lemmas F.2 and
F.3, it holds with probability at least 1 - exp(-C(Ra/3ma/3Ha)) that
kfθ(n)Vθ fθo - fθ(n)Vθ fθo ∣2 = O 1R4/m-1 H7/QGg 恒视)1/2).	(G.16)
For the term ∣fθ(n)Vθfθ(n) - fθ(n)Vθfθ0 ∣2 on the RHS of (G.15), following from (G.13) and
Lemma F.2, with probability at least 1 - exp(-Ω(R213m2/3Ha)) We have
kfθ(n)Vθfθ(n) - fθ(n)Vθfθ°∣∣2 = O(R^m-1/6抬/2(log馆&)1/2).	(G.17)
For the term ∣∣e ∙ (β-1Qω + TTfθ) ∙ (Vθfθ0 - Vθfθ(n))∣∣2 on the RHS of (G.15), We first upper
bound e ∙ (β-1Qω + τ-1fθ) as follows,
∣e∙ (β-1Qω + T-1fθ)| ≤ 2,
where we use (G.12), (G.13), and the fact that τe-1 = β-1 + τ-1. Further combining Lemma F.2, it
holds with probability at least 1 - exp(-Ω(R2∕3ma/3Ha)) that
∣e ∙ (β-1Qω + T-1fθ) ∙ (Vθfθo - Vθfθ(n))∣2 = O侬/m-1/HII(logma)1/2).	(G.18)
Now, combining (G.15), (G.16), (G.17), and (G.18), it holds with probability at least 1 -
exp(-C(R2/3ma/3Ha)) that
kgn -就k2 = O(R8/3m-1/3H7 log ma),	(G.19)
which establishes an upper bound of term (ii).b.
Upper Bound of Term (ii).c. It holds that
|腐-g"∣2 = ∣∣Eρ∏θ [(fθ(n) - fθ* )Vθ fθo ] ∣∣2 ≤ Eρ∏θ [(fθ(n) - fθ, )2 ∙∣Vθ fθo ∣2].
Further combining Lemma F.2, it holds with probability at least 1 - exp(-Ω(R2∕3ma/3Ha)) that
kgn - ge∣2 ≤ O(Ha) ∙ Eρ∏θ [(fθ(n) - k )2],	(G.20)
which establishes an upper bound of term (ii).c.
Now, combining (G.9), (G.14), (G.19), and (G.20), we have
Eρ∏θ [kgn -祓k2 I θ(n)]	≤ O(Ra/3m-1/3H7	log ma) +	O(Ha)	∙	Eρ∏°	[(fθ(n)	- 五)2],	(G21)
which is an upper bound of term (ii) on the RHS of (G.5).
By plugging the upper bound of term (i) in (G.8) and the upper bound of term (ii) in (G.21) into
(G.5), combining (G.19), with probability at least 1 - exp(-C(R2/3m2/3Ha)), we have
Ep∏θ [kθ(n +1)-θ*k2 I θ(n)]
≤ kθ(n) - θ*k2 + 2α ∙(O(R7/3m-1/6H7/2(logma)1/2) - Eρ∏° [(fθ(n)-五)2])
(G.22)
+ α2 ∙(O(R8/3m-1/3H7 log ma) + O(Ha) ∙ Eρ∏θ [(fθ(n) - fθ*)2]).
Rearranging terms in (G.22), it holds with probability at least 1 - exp(-C(R2/3m2/3Ha)) that
(2α - α2 ∙ O(Ha)) ∙ Eρ∏θ [(fθ(n) - fθ* )2]
≤ kθ(n)-θ*k2 - Eρ∏θ [kθ(n +1)-θ*k2 I θ(n)] + α ∙ O(Ra/3m-1/6H7 log mJ.
(G.23)
32
Published as a conference paper at ICLR 2021
By telescoping the sum and using Jensen’s inequality in (G.23), we have
1 Na-1
Eρ∏θ [(.fθ - fθ* )2] ≤ N ∙ X Eρ∏θ Kfθ(n) - fθJ2]
Na n=0
≤ 1/Na ∙ (2α - α2 ∙O(Ha))— 1 ∙ (1优 — θ*k2 + αN ∙O(R8/3m-1/6H7 logm®))
≤ N-1/2 ∙kθo -θ*k2 + O(R8/3m-1/6H7 log ma),
where the last line comes from the choices that α = Na-1/2 and Ha = O(Na1/4). Further combining
Lemma F.3 and using triangle inequality, we have
Eρ∏θ [(fθ — fθ* )2]=O(R2N-1∕2 + R8/3m-1/6H7 log m3).	(G.24)
By the definition of θ* in (G.3), We know that
hge, θ — θ*i≥ 0,	for any θ ∈ B(θ°, Ra)	(G.25)
By plugging the definition of gf_ into (G.25), We have
Eρ∏θ [h,fθ, — e∙(β-1Qω + τ-1fθ),fθt — fθ* i] ≥ 0, for any θ^ ∈B(θo,Ra),
which is equivalent to
θ* = argmin Eρ∏g [(fθt — e ∙ (β-1Qω + τ-1fθ))2].	(G.26)
θt∈B(θ0,Ra)
Meanwhile, by the fact that θ0 = ω0, we have
e ∙ (β-1Qω + τ-1fθ )= e ∙ (β —1 ∙ (Qωo + (ω — ωo)>Vω Qωo ) + T-1 ∙ f +(θ — θθ)>Vθ fθ0 ))
=fθo + (e ∙ (β-1ω + τ-1θ) — θo)> Vθfθo,
where the second line comes from e—1 = β-1 + T-1. Note that θ ∈ B(θo, Ra), ω ∈ B(ω0, Rc),
θo = ωo, and Ra = Rc, we know that e ∙ (β-1ω + τ-1θ) ∈ B(θ0, Ra). Therefore, with probability
at least 1 - exp(—C(R2/3ma/3Ha)) we have
Eρ∏θ [(fθ* — e∙(β-1Qω + T-1fθ ))2]
≤ Eρ∏θ [(e ∙ (β-1Qω + T-1fθ) — e ∙ (β-1Qω + τ -1fθ ))2 ]
≤ e2 ∙ β-2 ∙ Eρ∏θ [(Qω - Qω )2] + e2 ∙ T-2 ∙ Eρ∏θ [(fθ - fθ )2]
= O(Ra8/3ma-1/3Ha5logma),	(G.27)
where the first inequality comes from (G.26), and the last inequality comes from Lemma F.3 and the
fact that Rc = Ra, mc = ma, and Hc = Ha. Combining (G.24) and (G.27), by triangle inequality,
we have
Eρ∏θ [(fθ (S, a) — e ∙ (β-1Qω (s, a) + τ-1fθ (s, a)))2]=O(R2N-1∕2 + R8∕3m-1∕6H7 log ma),
which finishes the proof of Proposition C.3.
G.3 Proof of Proposition C.4
The proof is similar to that of Proposition C.3 in §G.2. For the completeness of the paper, we present
it here. We define the local linearization of Qω as follows,
Q ω = Qωo + (ω — ω0)> vωo Qω∙	(G.28)
We denote by
gn =	(Qω(n)(S0,a0 ) —	Y ∙ Qω (s1,a1) — (1 — Y) ∙ r0)∙ R ω Qω(n) (s0,a0),	g£ = E∏θ[gn],
gn =	(Q ω(n)(s0,。0 ) —	Y ∙ Qω (s1, a1) — (1 — Y) ∙ r°) ∙ Vω Qω0 ($0,。。)，	g^ = E∏θ[gn],
g* =	(Qω* (S0,aθ) — Y	∙ Qω (s1,a1) — (1 — γ) ∙ r。) ∙VωQω* (so, a。)，	ge = E∏θ[g*],
g* =	(Qω* (S0,aθ) — Y	∙ Qω (s1,a1) — (1 — y) ∙ r。) ∙ VωQωo (so, a。)，	ge = E∏θ[g*],
(G.29)
33
Published as a conference paper at ICLR 2021
where ω* satisfies that
ω* = rB(ωo,Rc)(ω* - α ∙ Ue).	(G.3O)
Here the expectation E∏θ [∙] is taken following (so, a°)〜ρ∏g (∙), si 〜P(∙ | so, a°), aι 〜∏θ(∙ | si),
and r0 = r(s0, a0). By Algorithm 4, we know that
ω(n + 1) = rB(ωo,Rc) (ω(n) - η ∙ gn).
Note that
E∏θ [kω(n + 1)- ω*∣∣2 | ω(n)]
=E∏θ [krB(ωo,Rc)(ω(n) - η ∙ gn) - rB(ω0,Rc)(ω* - T 祓)k2 | ω(n)]
≤ e∏θ [k(ω(n) - η ∙ gn) - (ω* - η ∙ ge)k2 |ω(n)]
=kω(n) - ω*k2 + 2η∙ hω* - ω(n),gn - gei+η2 ∙ E∏θ [kgn - ge∣∣2 | ω(n)]. (G.31)
'------------------------------{z----------}	'---------{z----------}
(iii)	(iv)
We upper bound term (iii) and term (iv) on the RHS of (G.31) in the sequel.
Upper Bound of Term (iii). By Holder,s inequality, it holds that
hω*- ω(n), gn - Uei
=hω*- ω(n),gn- gn i + hω*- ω(n),gn- gei
≤ ι∣ω*- ω(n)k2 ∙ kgn- gn ι∣2 + hω*- ω(n), Un- gei
≤ 2RcTlgn- gn ι∣2 + hω*- ω(n), Un- uei,	(G.32)
where We use the fact that ω(n),ω* ∈ B(ωo,Rc) in the last line. Further, by the definitions in
(G.29), it holds that
hω*- ω(n), gn -Uei
=E∏θ [(Qω(n)(so,ao) - Q ω* (so,。。))∙(ω* - ω(n), Vω Qωo (so, ao)i]
=E∏θ [(Qω(n)(s0, a0) - Qω* (sθ,aθ)) ∙ (Qω* (sθ,aθ) - Qω(n) (SO, a0))]
= -Eπθ [(QUω(n)(so, ao) -QUω*(so,ao))2] = -Eρπθ [(QUω(n) -QUω*)2],	(G.33)
where the second equality comes from (G.28), and the last equality comes from the fact that the
expectation is only taken to the state-action pair (so, ao). Combining (G.32) and (G.33), we obtain
the following upper bound of term (i),
hω* - ω(n),gn - gei ≤ 2Rc ∙ kgn - gn ∣∣2 - Eρ∏θ [(Qω(n) - Q ω* )2] .	(G34)
Upper Bound of Term (iv). We now upper bound term (iv) on the RHS of (G.31). It holds by
Cauchy-Schwarz inequality that
E∏θ [kgn - gek2 | ω(n)]
≤ 2E∏θ [kgn- gn k2 | ω(n)] + 2kgn- ge k2
≤ 2 e∏θ [kgn - gn ∣21 ω(n)] +4 kgn - Un ∣2 +4 kgn - ge∣2.	(G.35)
、----------{Z----------}	'----V----}	'----V-------}
(iv)	.a	(iv).b	(iv).c
We upper bound term (iv).a, term (iv).b, and term (iv).c in the sequel.
Upper Bound of Term (iv).a. We now upper bound term (iv).a on the RHS of (G.35). By expanding
the square, we have
E∏θ [kgn - gn k2 | ω(n)]	=	E∏θ	[kgn12 -	kgn k2 lω(n)]	≤	E∏θ	[kgn∣∣2	| ω(n)] ∙	(G∙36)
Meanwhile, by the definition of gn in (G.29), it holds that
kgn∣∣2 =	(Qω(n)(s0, a0)	- Y ∙	Qω (s1,	a1)	- (1 - Y)	∙ rθ)	∙ ||Vs Qω(n)(s0,	a0)k2.	(G.37)
34
Published as a conference paper at ICLR 2021
We first upper bound Qω as follows,
Q2ω = x(Hc)> bb> x(Hc) = x(Hc)> x(Hc) = kx(Hc) k22,
where x(Hc) is the output of the Hc-th layer of the DNN Qω . Further combining Lemma F.4, it
holds that
∣Qω| ≤ 2.	(G.38)
Similarly, we have
∣Qω(n)∣≤ 2.	(G.39)
Combining Lemma F.2, (G.36), (G.37), (G.38), and (G.39), we have
Eπθ kgn - gne k22 | ω(n) = O(Hc2).	(G.40)
Upper Bound of Term (iv).b. We now upper bound term (iv).b on the RHS of (G.35). It holds that
kgn - gn ι∣2
=∣∣E∏θ [ (Qω(n)(s0,aθ) — Y ∙ Qω(Sl,。1)- (1 - Y) ∙⑹∙ R ω Qω(n)(s0,。0)
- (Q ω(n) (S0, aO) - Y ∙ Qω (s1, a1) -(I - Y) ∙ rθ) ∙ N ω Qωo (s0, aO)| ||2
≤ E∏Θ [|| (γ ∙ Qω (s1, aI) + (1 - Y) ∙ rt) ∙ (Vω Qω0 (s0,aO)- R ω Qω(n)(s0, aO))b]
+	Eρ∏θ [kQω(n)VωQω(n) - Qω(n)NωQω0 |周
≤ E∏θ[∣∣(y ∙Qω(sι,aι) + (1 - Y) ∙ ro) 0Qω°(so,ao)-VωQω(n)(s0,a0))∣∣2] (G.41)
+	Eρ∏θ [II(Qω(n) - Qω(n)) ∙ Vs Qω0 |图 + Eρ∏g [∣∣Qω(n)，(Vω Qω(n) - Vs Qω° )g] .
We now upper bound the three terms on the RHS of (G.41) in the sequel, respectively.
For the term Epn@ [k(Qω(n) - Qω(n)) ∙ VωQω0 k2] on the RHS of(G.41), following from Lemmas
F.2 and F.3, it holds with probability at least 1 - exp(-Ω(R2∕3m2/3Hc)) that
Eρ∏θ [k(Qω(n) - Q ω(n) ) ∙ ^ω QuJ/ = O(R4/3m-1/6H7/2 (log mc)”).	(G.42)
For the term Eρ∏θ [kQω(n) ∙ (VωQω(n) - VQωο)k2] on the RHS of (G.41), following from (G.39)
and Lemma F.2, with probability at least 1 - exp(-Ω(R2∕3m2∕3Hc)), We have
Eρ∏θ [kQω(n) ∙ (Vω Qω(n) - ^ω Qω° )∣∣2] = O(R1/3m-1/6H5/2 (log mc)”).	(G.43)
For the term E∏θ[∣∣(y ∙ Qω(sι,aι) + (1 - Y) ∙ ro) ∙ (VωQω° (so,ao) -VωQω(n)(s0,a0))k2] onthe
RHS of (G.41), We first upper bound |y ∙ Qω (s1,a1 ) + (1 - Y) ∙ ro| as follows,
∣Y ∙ Qω (s1,a1) + (1 - y) ∙ ro| ≤ 2 +rmax,
where we use (G.38) and the fact that |r(s, a)| ≤ rmax for any (s, a) ∈ S × A. Further combining
Lemma F.2, with probability at least 1 - exp(-Ω(R2∕3m2/3Hc)), We have
E∏θ [H(Y ∙ Qω (s1 ,a1 ) + (1 - y) ∙ r0) ∙ (Vω Qω° (S O,aθ) - V ω Qω(n) (s0, a0)) 12]
=O(R1/3 m-1/6H5/2(log mc)1/2).	(G.44)
Now, combining (G.41), (G.42), (G.43), and (G.44), it holds with probability at least 1 -
exp(-Ω(R2∕3m2/3Hc)) that
kgn - gnk2 = O(R8/3m-1/3H7 logmc).	(G.45)
Upper Bound of Term (iv).c. We now upper bound term (iv).c on the RHS of (G.35). It holds that
崂 - 祓 k2 = ∣∣Eρ∏θ [(Qω(n) - Qω* Xω Qω0] ∣∣2 ≤ Eρ∏θ [(Qω(n) - Qω* )2 ∙ Ru Qω0 k2] ∙
35
Published as a conference paper at ICLR 2021
Further combining Lemma F.2, it holds that
E∏θ [kgn -祓k2 I ω(n)] ≤ OH) ∙ Eρ∏θ [(Qω(n) — Qω*)2]∙	(G.46)
Combining (G.35), (G.40), (G.45), and (G.46), we obtain the following upper bound for term (iv)
on the RHS of (G.31),
E∏θ [kgn -阖12 I ω(n)] ≤ O(R8/3m-1/3H7 log mc) + O(Hf) ∙ Eρ∏θ [(Qω(n) - Qω* 尸].
(G.47)
We continue upper bounding (G.31). By plugging (G.34) and (G.47) into (G.31), it holds with
probability at least 1 - exp(-Ω(R2∕3m2/3HC)) that
E∏θ[kω(n + 1) - ω*∣∣2 ∣ω(n)]
≤ kω(n) - ω*∣∣2 + 2η ∙(O(R7/3m-1/6H7/2(log馆0)1/2) - Eρ∏θ [(Qω(n) - Qω*)2])
+ η2 ∙(O(R8/3m-1/3H7 log m°) + O(Hf) ∙ Eρ∏θ [(Qω(n) - Qω*)2])∙	(G.48)
Rearranging terms in (G.48), it holds with probability at least 1 - exp(-Ω(R2∕3m2/3HC)) that
(2η - η2 ∙O(Hf)) ∙ Eρ∏θ [(Qω(n) - Qω* )2]
≤ l∣ω(n) - ω*k2 - ep∏θ [kω(n +1) - ω*k2 | ω(n)] + η ∙OHl/m-1/Hc logmc).
(G.49)
By telescoping the sum and using Jensen’s inequality in (G.49), we have
Eρ∏θ [(Qω - Qω*)2] ≤ N ∙ X Eρ∏θ [(Qω(n) - Qω* )2]
Nc n=0
≤ 1/Nc ∙ (2η - η2 ∙O(HC)Γ1 ∙ (∣M - ω*∣∣2 + ηNc ∙ O(R^3m-1^6HC log m°))
≤ N-1/2 ∙kθo - θ*k2 + O(R8/3m-1/6H7 logmc),
where the last line comes from the choices that η = Nc-1/2 and Hc = O(Nc1/4). Further combining
Lemma F.3 and using triangle inequality, we have
Eρ∏θ [(Qω - Qω* )2]=O(R2N-1∕2 + R8/3m-1/6H7 log m。).	(G.50)
To establish the upper bound of Eρπ^ [(Qω* - Q)2],weupperbound Eρπ^ [(Qω* - Q)2] inthe sequel.
By the definition of ω* in (G.30), following a similar argument to derive (G.26), we have
ω* =	argmin Eρ∏g [(Qωt (so, a。)一 Q(S0, ao))2].	(G.51)
j ∈B(ω0,Rc)
From the fact that Q ∈ U (mc , Hc , Rc ) by Assumption C.2, we know that Q = Qωe for some
ω ∈ B(ωo, Rc). Therefore, by (G.51), with probability at least 1 - exp(-C(R2/3m2/3Hc)), we
have
Eρ∏θ [(Qω* - Q)2] ≤ Eρ∏θ [(Qω - Q)2]=O(R8/3m-1/3H5 log m0),	(G.52)
where we use Lemma F.3 in the last inequality. Now, combining (G.50) and (G.52), by triangle
inequality, with probability at least 1 - exp(-C(R2/3m2/3Hc)), we have
Eρ∏θ [(Qω - Q)2] ≤ 2Eρ∏θ [(Qω - Qω* )2] + 2Eρ∏θ [(Qω* - Q)2]
= O(Rc2Nc-1/2 + Rc8/3mc-1/6Hc7 log mc),
which concludes the proof of Proposition C.4.
36
Published as a conference paper at ICLR 2021
H Proofs of Lemmas
H.1 Proof of Lemma D.1
W denote by^Q = Tπθk Qωfc. In the sequel, We upper bound Eρfc+ι [(Qωk+ι - Qωk+ι )2], where
ωk+ι =Γχ(ωk+ι) and ωk+ι isdefinedin(3.4). Notethatbythefactthat k 夕(s, a) k 2 ≤ 1 uniformly,
it suffices to upper bound kωk+1 - ωek+1 k2. By the definitions of ωk+1 and ωek+1 in (3.5) and (3.4),
respectively, we have
.. .. ^ _ .. _.. .. _ .. .. ^ _..
I∣ωk+1 - ωk+ιk2 ≤ ∣∣Φv - Φvk2 ≤ ∣∣Φ∣∣2 ∙ ∣∣v - v∣∣2 + ∣∣Φ - Φk2 ∙ ∣∣v∣∣2.	(H.1)
Here, we use the fact that the projection Γr(∙) is a contraction in the first inequality, and triangle
inequality in the second inequality. Also, for notational convenience, we denote by Φ, Φ, vb, and v
in (H.1) as follows,
φ = ( N X 以 S',ι,a',ι W(S',ι,a',ι)>)	, φ= (EPk+ι3(S,α⅛(S,a)>])T,
v '=1
1N
& = NE((I - Y)r',2 + γQωfc(S',2,a',2)) ∙φ(s',2,a',2),
'=1
V = EPk+1 [((I-Y)r + YPπθk+1 Qωfc )(s,a) ∙ ψ(s,a)∖.
BythefaCtthatk夕(s, a)k2 ≤ 1, ∣r(s,a)∣ ≤ rmax, and ∣∣ωk∣∣2 ≤ R we have
∣Φ∣2 ≤ 1∕σ*,	|同2 ≤ rmax + R.
(H.2)
Now, following from matrix Bernstein inequality (Tropp, 2015) and Assumption 4.3, we have
2
E[∣∣Φ - Φ∣2∖ ≤√=-2 ∙log(N + d),	(H.3)
√N (σ*)2
where σ* is defined in Assumption 4.3. Similarly, we have
E[∣bb - V∣2∖ ≤ 2(rmaχ + R)∕√N ∙log(N + d).	(H.4)
Now, combining (H.1), (H.2), (H.3), and (H.4), we have
E[∣ωk + 1 - ωk + 1∣2∖ ≤ 'rmax +、:) ∙ lOg(N + d).
√N (σ*)2
Therefore, it holds that
E[(Qωk+ι- Qωk+ι)2∖ ≤ 16(NIa； +2R)2 ∙ log2(N + d).	(H.5)
Meanwhile, by Assumption 4.2 and the definition of ω⅛+ι, we have
Cr 一
Q = Qωfc+ι .	(Hs)
Combining (H.5) and (H.6), we have
E[(Qωk + ι - Q)2∖ ≤ 16(NLa；*；4Μ∙ lθg2(N + d),
which concludes the proof of Lemma D.1.
H.2 Proof of Lemma E.1
Following from the definitions of Pπ and P in (2.3), we have
A1,k (s,a)= [γ(Pπ* - Pπθk+1 )Qωk∖(s,a)= [γPhQωfc ,π*- πθk+ιi∖(s,a).	(H.7)
By invoking Lemma F.1 and combining (H.7), it holds for any (S, a) ∈ S × A that
Aι,k(s,a)= [γ(Pπ* - Pπθk+ι )Qωfc∖(s,a) ≤ [γβ ∙ P(Hk + %ι)∖(s,a),
where Idk and ^+]are defined in (E.4) and (E.5) of Lemma E.1, respectively. We conclude the
proof of Lemma E.1.
37
Published as a conference paper at ICLR 2021
H.3 Proof of Lemma E.2
By the definition that Q* is the action-value function of an optimal policy π*, We know that
Q*(s,a) ≥ Qπ(s, a) for any policy π and state-action pair (s, a) ∈ S × A. Therefore, for any
(s, a) ∈ S ×A, we have
A2,k(s,a) = [γPπ* (Qπθk+ι - Qωk)](s,a) ≤ [γPπ* (Q* - Qωfc)](s,a).	(H.8)
In the sequel, we upper bound Q* (s, a) - Qωk (s, a) for any (s, a) ∈ S ×A. We define
≈	,	、	— L-	一
Qk + 1 = (1 - Y) ∙ r + Y ∙ P θfc+1 Qωk.
By its definition, we know that Qk+1 = T k+1 Qωk. It holds for any (s, a) ∈ S ×A that
Q* (S,a) - Qωk+ι (S,a)
XΛ*
=Q (s, a) - Qk+1(s, a)+ Qk+ 1(s, a) - Qωk+ι (s, a)
=[((1 -Y) ∙ T + Y ∙ pπ*Q*) -((I-Y) ∙ T + Y ∙ pπθk+1 Qωk)](s,a) + ek+ι(s,a)
=Y ∙ [Pπ* Q* - Pπθk+1 QωJ(s, a) + ek+ι(s, a)
=Y ∙ [Pπ* Q* - Pπ* Qωk](s, a)+ Y ∙ [Pπ*Qωk - Pπθk+1 Qωk](s, a) + ek+ι(s, a)
=Y ∙ [pπ*(Q* - Qωk )](s,a) + A1,k (s, a) + ek+1(s,a)
≤ Y ∙ [Pπ* (Q* - Qωfc)](s,a) + Ye ∙ [P(% + ek+ι)](s,a) + /+ι(s,a),	(H.9)
where 京+i and Aι,k are defined in (E.6) and (E.3), respectively. Here, we use Lemma E.1 to upper
bound Aι,k in the last line. We remark that (H.9) upper bounds Q* - Qω⅛+1 using Q* - Qωfc. By
recursively applying a similar argument as in (H.9), we have
Q*(s, a) - Qωk (s, a)
k-1
≤ [(YPπ*)k(Q* - Qωo)](s,a)+ Ye ∙ X[(YPπ*)k-i-1P(必 + V+ι)](s,a)	(H.10)
i=0
k-1
+ X[(YPπ* )k-i-1*ι](s,a).
i=0
Combining (H.8) and (H.10), it holds for any (s, a) ∈ S ×A that
A2,k(s,a) ≤ [yPπ* (Q* - Qωfc )](s,a)
k-1
≤ [(γPπ* )k+1(Q* - Qωo )](s, a) + Ye ∙ X [(γPπ* )k-iP(必 + M1)] (s, a)
i=0
k-1
+ X[(γPπ* )k-i*ι](s,a),
i=0
where 必，琮+1,and ∈C+ι are defined in (E.4) of Lemma E.1, (E.5) of Lemma E.1, and (E.6) of
Lemma E.2, respectively. We conclude the proof of Lemma E.2.
38
Published as a conference paper at ICLR 2021
H.4 Proof OF Lemma E.3
Note that for any (s, a) ∈ S × A, we have
A3,k(s,a) = [Tπθk+ι Qωk - Qπ%+ι](s,α)
=[((1 - Y) ∙ r + γPπθk+ι Qωk) - Qπ%+ι] (s, a)
∞
=[((1 -Y) ∙ r + γPπθk+ι Qωk) - X(1 - Y)(γPπθk+ι )tr] (s, a)
t=0
「∞	∞
= X((YPπθk+ι )tQωk-(γPπθk+ι )t+1Qωfc) - X(1-γ )(γPπθk+ι )tr (s,a)
Lt=ι	t=ι	-
∞
=X [(γPπθk+1 )t(Qωk - YPπθk+1 Qωfc - (1 - γ) ∙ r)] (s, a)
t=1
∞
=X [(YPπθk+1 )t(Qωfc - Tπθk+1 Qωfc)] (s, a)
t=1
∞
=X [(YPπθk+ι )tek+ι] (s, a) = [yPπθk+1 (I-YPπ%+ι )-1efc+1] (s, a),
t=1
where the term efc+1 in the last line is defined in (E.7). We conclude the proof of Lemma E.3.
H.5 Proof of Lemma E.4
We invoke Lemma F.1 in §F, which gives
βτ ∙ Qk(S, ∙),πθk(Ts) -πθk+ι (∙ IS)
≤ (l°g(πθk+l (∙ 1 s)∕πθk (∙ 1 S))- β-1 ∙ Qωk (s, ∙),πθk (∙ 1 s) - πθk+ι (T s)〉
-KL(πθfc (∙ 1 s)k πθk+l (T s))
≤ (l°g(πθk+l (∙ 1 S)∕πθk (∙ 1 s)) - β-1 ∙ Qωk (S, ∙),πθk (∙ 1 s) - πθk+ι (T s)〉= eb+1(S) ∙
(H.11)
Combining (H.11) and the definition of Pπ in (2.3), We have
[PπθkQωk - Pπθk+1 Qωk](s,a) ≤ β[Peb+ι](s).	(H.12)
By the definition of efc+1 in (E.7), We have
ek+1(s, a) = [Qωk - Y ∙ pπθk+1 Qωk - (1 - Y) ∙ r] (s, a)
≤ [Qωk - Y ∙ Pπθk Qωk - (1 - Y) ∙ r](s, a) + βY ∙ [Peb+ι](s, a)	(H.13)
=[Qk- Y ∙ Pπ% Qk-(I- Y) ∙ r] (s, a) + [βYPeb+1 - (I - yP"% )闱(s, a),
where We use (H.12) in the first inequality, and
≈ 、 ___________________________________________________ _
Qk = (1 - Y) ∙ r + y ∙ Pπ% Qωfc-ι.	(H.14)
For the first term on the RHS of (H.13), by (H.14), it holds that
~ ______________~ . .
Qk - γ ∙ P θk Qk - (1 - γ) ∙ r
=(1 - Y) ∙ r + γ ∙ Pπθk Qωk-1 - y(1 - γ) ∙ Pπθkr - (γPπ% )2Qωfc-ι - (1 - γ) ∙ r
=Y ∙ Pπθk(Qωk-1 - γPπθkQωk-1 -(1- γ)r) = Y ∙ P"θ% ek.	(H.15)
Combining (H.13) and (H.15), we have for any (s, a) ∈ S ×A that
ek+ι(s,a) ≤ [γpπθkek](s,a) + [βγpeb+ι -(I- γpπθk)熊](s,a).	(H.16)
By telescoping (H.16), it holds that
r k	k	k	-l
ek+ι(s,a) ≤ (Y γPπθf + X γk-i( Y P"θ') (βγPe3 - (I - γPπθi)/) (s,a).
L s=1	i=1	s=i⅛1	-
This finishes the proof of the lemma.
39
Published as a conference paper at ICLR 2021
H.6 Proof OF Lemma E.5
Note that ∣∣ω0∣∣2 ≤ R and |r(s, a)| ≤ rmax for any (S) a) ∈ S×A, which implies that ∣Qωo (S) a)| ≤
R and ∣Q*(s, a)∣ ≤ rmax by their definitions. Thus, for M1, We have
K
∣M1∣ ≤ Eρ [(I - γPπ* )-1 X(γPπ* )k+1∣Q* - Qωo ∣]
k=0
K
≤ 4(I- Y)I X Yk+1 ∙ (rmax + R) ≤ 4(I- Y) 2 YrmaX + R) ∙	(H.⑺
k=0
For M2, by the definition of e1 in (E.7), ∣ωk ∣ ≤ R, ∣φ(s, α)∣ ≤ 1, and ∣r(s, α)∣ ≤ rmax, We have
∣e1(s,α)∣ = I [Qωk - Tπθk十ιQωJ(S,a)|
=1 ω>φ(s,a) - Y ∙ ω>[Pπθk十1 φ](s,a) - (1 - Y) ∙ r(s,a)
≤ 2R + rmax	(H.18)
for any (s, a) ∈ S ×A. Therefore, we have
∣M2∣ ≤ (1 - y)-3 ∙ (2R + rmax).	(H.19)
Meanwhile, by the initialization τ0 = ∞ in Algorithm 1, the initial policy 开拆(,∣ S) is a uniform
distribution over A. Therefore, it holds for any S ∈ S that
KL(π*(∙∣ S)Il πθo(∙∣
*/ I、】	π*(a∣s) 1
π* (a ∣ s) log-7-Yda
πθo(a ∣ s)
=/ π*(a ∣ s) log π*(a ∣ s)da — / π*(a ∣ s) log ng (a ∣ s)da
JA	JA
≤ - / π*(a ∣ s) log ng。(a ∣ s)da
A
/ π*(a ∣ s) log ∣A∣da = log ∣A∣.
A
Therefore, by (H.20), we have
M3 ≤ (1 - y)-2 ∙ log ∣A∣ ∙ K1/2,
(H.20)
(H.21)
where we use β = K1/2. We see that (H.17), (H.19), and (H.21) upper bound M1, M2, and M3,
respectively. We conclude the proof of Lemma E.5.
H.7 Proof of Lemma E.6
For M4, by changing the index of summation, we have
K k ∞
∣M4∣ = I Eρ[XXX(YPπ* )k-i+居1]∣
k=0 i=0 j=0
K k ∞
=∣ Eρ[XXX (Yk )%c+1]∣
k=0 i=0 t=k-i
K k ∞
≤ XXX I Eρ[(γPπ*)t*1]∣ ,	(H.22)
k=0 i=0 t=k-i
where we expand (I - γPπ* )-1 into an infinite sum in the first equality. Further, by changing the
measure of the expectation from P to ρ* on the RHS of (H.22), we have
K k ∞	K k ∞
XX X ∣Eρ[(γPπ* )t*1]∣ ≤ XX X Ytc(t) ∙ Eρ* [%∣],	(H.23)
k=0 i=0 t=k-i	k=0 i=0 t=k-i
40
Published as a conference paper at ICLR 2021
where c(t) is defined in Assumption 4.1. Further, by changing the index of summation on the RHS
of (H.23), combining (H.22), we have
K∞	k
∣M4∣≤ XX	X	γtc(t) ∙ EQ
k=0 t=0 i=max{0,k-t}
K∞
≤XX2tγtc(t) ∙ Eq
k=0 t=0
K
≤ Y X 2Cρ,ρ* ∙ EQ ≤ 3KCρ,ρ* ∙ EQ,	(H.24)
where 号Q = maxi Eρ* [后+11], and Cρ,ρ* is defined in Assumption 4.1.
Now, for M5, by a similar argument as in the derivation of (H.24), we have
∞K∞ k
M5 ≤ XXXX
γi+j + k-'+1c(i + j + k - ' + 1) ∙ Eq
i=0k=0j=0'=1
∞ K ∞ i+j+k	K ∞
=XXX X Ytc(t) ∙EQ ≤ XXt2γtc(t) ・ EQ ≤ KCρ,ρ* ∙ EQ.	(H.25)
i=0 k=0 j=0 t=i+j+1	k=0 t=1
We see that (H.24) and (H.25) upper bound M4 and M5, respectively. We conclude the proof of
Lemma E.6.
H.8 Proof of Lemma E.7
Part 1.	We first show that the first inequality holds. Note that
πθk(a | S) = eχp(τ-1 fθk (s,α0"Zθk (S),	πθk+ι(a | S) = eχp(τk+ιfθk+ι (s,α0"Zθk+ι (S),
Here Zθk (s), Zθk+1 (s) ∈ R are normalization factors, which are defined as
Zθk (S) =	eχp(τk-1fθk(S,a0)),	Zθk+1(S) =	eχp(τk-+11fθk+1(S,a0)).
a0∈A	a0∈A
Thus, we have
hlog(πθk+ι (∙ | s)∕πθk (∙ | S))- β-1Qωk (s, ∙),π* (∙ | S)- πθk+ι (T s)i
=hτ-+1fθk+1 (S,∙)-(β-1Qωk (S, ∙)+ τ-1fθk (S, ∙)),∏*(∙ | S) - ∏θk (∙ | S)i,	(H.26)
where we use the fact that
hlogZθk+1 (S)- log Zθk(S), π*(∙∣ S) - Πθk+1 (∙∣ S)i
=(log Zθk+1 (S)- log Zθk (s)) ∙ X (π*(a0l S)-πθk+ι (a0| s)) = 0.
a0∈A
Thus, it remains to upper bound the right-hand side of (H.26). We have
hτk+lfθk+ι (SQ-(I3Sk (s, ∙) + τ-1fθk (S, ∙)), π*(∙ | s) - πθk+ι (∙ | s)i	(H.27)
_ / —1 F ( 、 /总―1C ( ʌ I — 1 ʃ (	、、	∕l∖(π (^ 1 s)	πθk+ι (∙ | s) ʌ ∖
=ξτk+1fθk+ι (S, j - (Bk Qωk (S, ^) + Tk fθk (S, ∙)),πθk (∙ | S> ("k (.∣ S)	∏θfc (∙ | s) ) /
41
Published as a conference paper at ICLR 2021
Taking expectation with respect to S 〜Vk on the both sides of (H.27) and using the Cauchy-Schwarz
inequality, we obatin
Eν* [|〈T/1 瓜+1 (S, ∙) - (β-1Qωk (S, ∙) + τ-1 fθk (S, ∙)),πk(∙ | S) - Πθk+1 (∙ | S)〉|] I
=JS (τ∕1fθk+1 (S, ∙) - (β-1Qωk (S, ∙) + Tu fθk (S, ∙)),
πθk (| S) ∙ Vk(S)∙
π*(∙ | S)	πθk+1
________ __ _____
πθk (∙ 1 S)	πθk (∙
|S
S)
个|ds
Vk(s) I
—
/	lτk+11 于Θk+1 (S, a) - (β-1Qωk (S, a) + T-1fθk (S, a)) I
S S×A
ρk (a | s) πθk+1 (a | s) ∙νk(s)
-----------------------------
dρk(s,o)
Pk(a | S)
Pk(a | S)
≤ EPk [(τ⅛fθk+1 (S,a) - (β-1Qωk (S,a) + τ-1fθ% (s, a)))2]1/2 ∙ E0% [1萼-d⅛^ 1]"
dPk	dPk
≤ √2τk+11 ∙ εk+1,f ∙ (φk + ψ"),
where in the last inequality we use the error bound in (E.20) and the definition of φk and ψk in
Assumption C.1. This finishes the proof of the first inequality.
Part 2.	The proof of the second inequality follows from a similar argument as above. We have
Qθg(πθk+1 (∙ 1 s)∕πθk (∙ | s)) - βTQωκ (s, ∙), πθk ( 1 s) - πθk + 1 ( 1 S))
=hτ∕ιfθk+1 (S, ∙) - (β-1Qωk (S, ∙) + τ-1fθk (S, ∙)),∏θk (∙ | S) -∏θk+1 (∙ | s)),	(H.28)
where we use the fact that
QOg Zθk+1 (s) - Iog Zθk (s), πθk (T s) - πθk+1 (∙ | s))
=(Iog Zθk+1 (s) - bg Zθk (s)) ∙ E (πθk (ɑ' 1 S)- πθk+1 (ɑ' | S))= 0∙
α0∈A
Thus, it remains to upper bound the right-hand side of (H.28). We have
hτ-1fθk+1 (S, ∙) - (β-1Qωk (S, ∙)+ τ-1fθk (S, ∙)),πθk (∙ | S) - ∏θk+1 (∙ | s))	(H.29)
=(τ∕1fθk+1 (s, ∙) - (β-1Qωk (S, ∙) + Tk-fθk (S, ∙)),∏θk (∙ | S)∙(1 - 2+(( S； ))∙
Taking expectation with respect to S ~ Vk on the both sides of (H.29) and using the Cauchy-Schwarz
inequality, we obatin
Eν* [l<τ#1fθk+1 (s, ∙) - (β-1Qωk (S, ∙) + τ-1 fθk (s, ∙)),∏θk (∙ | S) -∏θk+1 (∙ | s)〉|]
=L (τ^1fθk+1 (s, ∙) - (β-1Qωk (S, ∙) + τ-1 fθk (S, ∙)),∏θk (∙ | S) ∙ Vk(s)∙(1 - 7；+((IS)))；
l Vk(s)|
IVk(S)IS
πθk+ι (a 1 S) ”"(S)
/	l τk+1ι 于θk+ι(S,a) - (B-IQωk (s,α)+ τ-1fθk (S,α))∣∙ 1-
SSaA
≤ EPk [(τ-1fθk+1 (s, a) - (即1Qωk (s, a) + TrIfθk (s, a)))2] 1/2 ∙ EPk
Pk(a | s)
1	d(πθfc.
1  ------
dρk (s, a)
,+1 V *)|2]1/2
dPk
≤ √2τk + 1 , εk+1 ,f ∙ (1 + ψk)
where in the last inequality We use the error bound in (E.20) and the definition of ψk in Assumption
C.1. This finishes the proof of the second inequality.
42