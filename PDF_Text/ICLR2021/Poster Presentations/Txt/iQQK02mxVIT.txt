Published as a conference paper at ICLR 2021
Why resampling outperforms reweighting for
correcting sampling bias with stochastic gra-
DIENTS
Jing An, Lexing Ying and Yuhua Zhu
Stanford University
{jingan, lexing, yuhuazhu}@stanford.edu
Ab stract
A data set sampled from a certain population is biased if the subgroups of the pop-
ulation are sampled at proportions that are significantly different from their under-
lying proportions. Training machine learning models on biased data sets requires
correction techniques to compensate for the bias. We consider two commonly-
used techniques, resampling and reweighting, that rebalance the proportions of
the subgroups to maintain the desired objective function. Though statistically
equivalent, it has been observed that resampling outperforms reweighting when
combined with stochastic gradient algorithms. By analyzing illustrative exam-
ples, we explain the reason behind this phenomenon using tools from dynamical
stability and stochastic asymptotics. We also present experiments from regression,
classification, and off-policy prediction to demonstrate that this is a general phe-
nomenon. We argue that it is imperative to consider the objective function design
and the optimization algorithm together while addressing the sampling bias.
1	Introduction
A data set sampled from a certain population is called biased if the subgroups of the population are
sampled at proportions that are significantly different from their underlying population proportions.
Applying machine learning algorithms naively to biased training data can raise serious concerns and
lead to controversial results (Sweeney, 2013; Kay et al., 2015; Menon et al., 2020). In many domains
such as demographic surveys, fraud detection, identification of rare diseases, and natural disasters
prediction, a model trained from biased data tends to favor oversampled subgroups by achieving
high accuracy there while sacrificing the performance on undersampled subgroups. Although one
can improve by diversifying and balancing during the data collection process, it is often hard or
impossible to eliminate the sampling bias due to historical and operational issues.
In order to mitigate the biases and discriminations against the undersampled subgroups, a common
technique is to preprocess the data set by compensating the mismatch between population pro-
portion and the sampling proportion. Among various approaches, two commonly-used choices are
reweighting and resampling. In reweighting, one multiplies each sample with a ratio equal to its pop-
ulation proportion over its sampling proportion. In resampling, on the other hand, one corrects the
proportion mismatch by either generating new samples for the undersampled subgroups or selecting
a subset of samples for the oversampled subgroups. Both methods result in statistically equivalent
models in terms of the loss function (see details in Section 2). However, it has been observed in
practice that resampling often outperforms reweighting significantly, such as boosting algorithms in
classification (Galar et al., 2011; Seiffert et al., 2008), off-policy prediction in reinforcement learning
(Schlegel et al., 2019) and so on. The obvious question is why.
Main contributions. Our main contribution is to provide an answer to this question: resampling
outperforms reweighting because of the stochastic gradient-type algorithms used for training. To
the best of our knowledge, our explanation is the first theoretical quantitative analysis for this phe-
nomenon. With stochastic gradient descent (SGD) being the dominant method for model training,
our analysis is based on some recent developments for understanding SGD. We show via simple and
1
Published as a conference paper at ICLR 2021
explicitly analyzable examples why resampling generates expected results while reweighting per-
forms undesirably. Our theoretical analysis is based on two points of view, one from the dynamical
stability perspective and the other from stochastic asymptotics.
In addition to the theoretical analysis, we present experimental examples from three distinct cat-
egories (classification, regression, and off-policy prediction) to demonstrate that resampling out-
performs reweighting in practice. This empirical study illustrates that this is a quite general phe-
nomenon when models are trained using stochastic gradient type algorithms.
Our theoretical analysis and experiments show clearly that adjusting only the loss functions is not
sufficient for fixing the biased data problem. The output can be disastrous if one overlooks the
optimization algorithm used in the training. In fact, recent understanding has shown that objective
function design and optimization algorithm are closely related, for example optimization algorithms
such as SGD play a key role in the generalizability of deep neural networks. Therefore in order
to address the biased data issue, we advocate for considering data, model, and optimization as an
integrated system.
Related work. In a broader scope, resampling and reweighting can be considered as instances
of preprocessing the training data to tackle biases of machine learning algorithms. Though there
are many well-developed resampling (Mani & Zhang, 2003; He & Garcia, 2009; Maciejewski &
Stefanowski, 2011) and reweighting (Kumar et al., 2010; Malisiewicz et al., 2011; Chang et al.,
2017) techniques, we only focus on the reweighting approaches that do not change the optimization
problem. It has been well-known that training algorithms using disparate data can lead to algorithmic
discrimination (Bolukbasi et al., 2016; Caliskan et al., 2017), and over the years there have been
growing efforts to mitigate such biases, for example see (Amini et al., 2019; Kamiran & Calders,
2012; Calmon et al., 2017; Zhao et al., 2019; LoPez et al., 2013). We also refer to (GUo et al., 2017;
He & Ma, 2013; Krawczyk, 2016) for a comprehensive review of this growing research field.
OUr aPProaches for Understanding the dynamics of resamPling and reweighting Under SGD are based
on tools from nUmerical analysis for stochastic systems. Connections between nUmerical analysis
and stochastic algorithms have been raPidly develoPing in recent years. The dynamical stability
PersPective has been Used in (WU et al., 2018) to show the imPact of learning rate and batch size in
minima selection. The stochastic differential eqUations (SDE) aPProach for aPProximating stochas-
tic oPtimization methods can be traced in the line of work (Li et al., 2017; 2019; Rotskoff & Vanden-
Eijnden, 2018; Shi et al., 2019), jUst to mention a few.
2	Problem setup
Let Us consider a PoPUlation that is comPrised of two different groUPs, where a ProPortion a1 of
the PoPUlation belongs to the first groUP, and the rest with the ProPortion a2 = 1 - a1 belongs
to the second (i.e., a1, a2 > 0 and a1 + a2 = 1). In what follows, we shall call a1 and a2 the
population proportions. Consider an oPtimization Problem for this PoPUlation over a Parameter
θ. For simPlicity, we assUme that each individUal from the first groUP exPeriences a loss fUnction
V1(θ), while each individUal from the second groUP has a loss fUnction of tyPe V2(θ). Here the loss
fUnction V1 (θ) is assUmed to be identical across all members of the first groUP and the same for
V2 (θ) across the second groUP, however it is Possible to extend the formUlation to allow for loss
fUnction variation within each groUP. Based on this setUP, a minimization Problem over the whole
PoPUlation is to find
θ* = argmin V(θ),	where V(θ) ≡ aιV1(θ) + a2V2(θ).	(1)
θ
For a given set Ω of N individuals sampled uniformly from the population, the empirical minimiza-
tion Problem is
θ* = argminɪ X V√θ),	(2)
θ N riΩ
where ir ∈ {1, 2} denotes which group an individual r belongs to. When N grows, the empirical
loss in (2) is consistent with the population loss in (1) as there are approximately a1 fraction of
samples from the first group and a2 fraction of samples from the second.
2
Published as a conference paper at ICLR 2021
However, the sampling can be far from uniformly random in reality. Let n1 and n2 with n1 +n2 = N
denote the number of samples from the first and the second group, respectively. It is convenient to
define fi, i = 1, 2 as the sampling proportions for each group, i.e., f1 = n1/N and f2 = n2/N with
f1 + f2 = 1. The data set is biased when the sampling proportions f1 and f2 are different from the
population proportions a1 and a2. In such a case, the empirical loss is f1V1(θ) + f2V2(θ), which is
clearly wrong when compared with (1).
Let us consider two basic strategies to adjust the model: reweighting and resampling. In reweighting,
one assigns to each sample r ∈ Ω a weight a%r/fir and the reweighting loss function is
Vw (θ) ≡ N X fr Vir (θ) = a1 V1(θ) + a2V2(θ).
r∈Ω fir
(3)
In resampling, one either adds samples to the minority group (i.e., oversampling) or removing sam-
ples from the majority group (i.e., undersampling). Although the actual implementation of over-
sampling and undersampling could be quite sophisticated in order to avoid overfitting or loss of
information, mathematically we interpret the resampling as constructing a new set of samples of
size M, among which a1M samples are of the first group and a2M samples of the second. The
resampling loss function is
VS(θ) ≡ MM X Vis (θ) = aιV1(θ) + a2V2(θ).	(4)
s
Notice that both Vw(θ) and Vs(θ) are consistent with the population loss function V (θ). This means
that, under mild conditions on V1 (θ) and V2(θ), a deterministic gradient descent algorithm from a
generic initial condition converges to similar solutions for Vw(θ) and Vs(θ). Fora stochastic gradient
descent algorithm, the expectations of the stochastic gradients of Vw(θ) and Vs(θ) also agree at any
θ value. However, as we shall explain below, the training behavior can be drastically different for a
stochastic gradient algorithm. The key reason is that the variances experienced for Vw(θ) and Vs(θ)
can be drastically different: computing the variances of gradients for resampling and reweighting
reveals that
V [v½(θ)] = aιVV1(θ)Wι(θ)T + a2VV2(θ)VV2(θ)T - (E[V½(θ)])2,
V [vVw (θ)]
a1 VV1(θ)VVι(θ)T + aVV2(θ)VV2(θ)τ - (E[VVw(θ)])2.
f1	f2
(5)
These formulas indicate that, when f1/f2 is significantly misaligned with a1/a2, the variance of
reweighting can be much larger. Without knowing the optimal learning rates a priori, it is difficult
to select an efficient learning rate for reliable and stable performance for stiff problems, when only
reweighting is used. In comparison, resampling is more favorable especially when the choice of
learning rates is restrictive.
3 Stability analysis
Let us use a simple example to illustrate why resampling outperforms reweighting under SGD, from
the viewpoint of stability. Consider two loss functions V1 and V2 with disjoint supports,
V— + 1)2-1, θ≤0,
v2(θ) = {0∙,(θ- 1)2 - 1, θ≤0,
(6)
each of which is quadratic on its support. The population loss function is V (θ) = a1V1(θ)+a2V2(θ),
with two local minima at θ = -1 and θ = 1. The gradients for V1 and V2 are
VV1(θ)
θ+1,
0,
θ≤0
θ >0.
VV2(θ)
θ0,-1,
θ≤0
θ >0.
Suppose that the population proportions satisfy a2 > a1, then θ = 1 is the global minimizer and
it is desired that SGD should be stable near it. However, as shown in Figure 1, when the sampling
proportion f2 is significantly less than the population proportion a2, for reweighting θ = 1 can easily
become unstable: even if one starts near the global minimizer θ = 1, the trajectories for reweighting
3
Published as a conference paper at ICLR 2021
(1) Reweighting	(2) Resampling
Figure 1: Comparison of reweighting and resampling with a1/a2 = 0.4/0.6 and f1/f2 = 0.9/0.1
at the learning rate η = 0.5. The resampling strategy here is to randomly select the sub-population i
with the probability ai with replacement in each iteration. (1) For reweighting, the trajectory starting
from θ0 = 1.1 can end up at θ = -1 after a few iterations, but θ = -1 is not the global minimizer.
(2) For resampling, the trajectory starting from θ0 = 2.0 stays close to the desired minimizer θ = 1.
Hence resampling is more reliable than reweighting. We include more comparisons with various
learning rates in Appendix D to show that resampling is stable for a wider range of η.
always gear towards θ = -1 after a few steps (see Figure 1(1)). On the other hand, for resampling
θ = 1 is quite stable (see Figure 1(2)).
The expectations of the stochastic gradient are the same for both methods. It is the difference in the
second moment that explains why trajectories near the two minima exhibit different behaviors. Our
explanation is based on the stability analysis framework used in (Wu et al., 2018). By definition, a
stationary point θ* is Stochastically Stabie if there exists a uniform constant 0 < C ≤ 1 such that
E[∣∣θk 一 θ*k2] ≤ C∣∣θo 一 θ*k2, where θk is the k-th iterate of SGD. The stability conditions for
resampling and reweighting are stated in the following two lemmas, in which we use η to denote the
learning rate.
Lemma 1. For resampling, the conditions for the SGD to be stochastically stable around θ = -1
and θ = 1 are respectively
(1 - ηa1)2 + η2a1a2 ≤ 1,	(1 - ηa2)2 + η2a1a2 ≤ 1.
Lemma 2. For reweighting, the condition for the SGD to be stochastically stable around θ = -1
and θ = 1 are respectively
(I - ηaI)2 + η2f1f2 (f ) ≤ 1, (I - ηa2)2 + η2f1f2 (f ) ≤ 1.
Note that the stability conditions for resampling are independent of the sampling proportions
(f1, f2), while the ones for reweighting clearly depend on (f1, f2). We defer the detailed com-
putations to Appendix A.
Lemma 2 shows that reweighting can incur a more stringent stability criterion. Let us consider the
case aι = 11 一 e,a2 = 2 + e with a small constant e > 0 and f2∕f1《1. For reweighting, the
global minimum θ = 1 is stochastically stable only if η(1 + f1/f2) ≤ 4 + O(). This condition
becomes rather stringent in terms of the learning rate η since f1∕f2	1. On the other hand, the
local minimizer θ = -1 is stable ifη(1 + f2∕f1) ≤ 4+ O(e), which could be satisfied for a broader
range of η because f2∕f1	1. In other words, for a fixed learning rate η, when the ratio f2∕f1
between the sampling proportions is sufficiently small, the desired minimizer θ = 1 is no longer
statistically stable with respect to SGD.
4 SDE analysis
The stability analysis can only be carried for a learning rate η of a finite size. However, even for a
small learning rate η, one can show that the reweighting method is still unreliable from a different
perspective. This section applies stochastic differential equation analysis to demonstrate it.
4
Published as a conference paper at ICLR 2021
Let us again use a simple example to illustrate the main idea. Consider the following two loss
functions,
Vi (θ) = / +1l- 1，θ ≤ 0 ,	V2 (θ ) = {/」］θ ≤ 0 ,
eθ,	θ > 0	∣θ — 1| — 1, θ > 0
with 0 <	1. The population loss function is V(θ) = a1V1(θ) + a2V2(θ) with local minimizers
θ = —1 and θ = 1. Note that the O(6) terms are necessary. Without it, if the SGD starts in (-∞, 0),
all iterates will stay in this region because there is no drift from V2(θ). Similarly, if the SGD starts
in (0, ∞), no iterates will move to (-∞, 0). That means the result of SGD only depends on the
initialization when O() term is absent.
In Figure 2, we present numerical simulations of the resampling and reweighting methods for the
designed loss function V(θ). Ifa2 > a1, then the global minimizer ofV(θ) is θ = 1 (see the Figure
2(1)). Consider a setup with population proportions a1/a2 = 0.4/0.6 along sampling proportions
f1/f2 = 0.9/0.1, which are quite different. Figures 2(2) and (3) show the dynamics under the
reweighting and resampling methods, respectively. The plots show that, while the trajectory for
resampling is stable across time, the trajectory for reweighting quickly escapes to the (non-global)
local minimizer θ = —1 even when it starts near the global minimizer θ = 1.
Figure 2: Comparison of reweighting and resampling with learning rate η = 0.12. We set a1/a2 =
0.4/0.6, f1/f2 = 0.9/0.1 and = 0.1. Both experiments start at θ0 = 0.9. The resampling
strategy here is to randomly select the sub-population i with the probability ai with replacement in
each iteration. In (2) where reweighting is used, the trajectory skips to the local minimizer θ = —1
later. In (3) where resampling is used, it stabilizes at the global minimizer θ = 1 all the time. We
include more comparisons with various learning rates in Appendix D to show that resampling is
more reliable for a wider range of η .
When the learning rate is sufficiently small, one can approximate the SGD by an SDE, which in
this piece-wise linear loss example is approximately a Langevin dynamics with a piecewise constant
mobility. In particular when the dynamics reaches equilibrium, the stationary distribution of the
stochastic process is approximated by a Gibbs distribution, which gives the probability densities
at the stationary points. Let us denote ps (θ) and pw (θ) as the stationary distribution over θ under
resampling and reweighting, respectively. Following lemmas quantitatively summarize the results.
Lemma 3. When a2 > a1, V(1) < V (—1). The stationary distribution for resampling satisfies the
relationship
-pS(⅜ = exp (—ɪ(v⑴—V(-1))) + O ⑥ > 1.
ps(—1)	a1a2η
Lemma 4. With a2 > a1, V(1) < V(—1) < 0. Under the condition f ≤ 郎 JVV-II) for the
sampling proportions, the stationary distribution for reweighting satisfies the relationship
Pw (1)	al/f2” TJ 2f2∕f1	- 2f1∕f2	)
E)=而?exp「F V ⑴ + ▼V (-1)J+ O⑥ < 1.
The proofs of the above two lemmas can be found in Appendix B. Lemma 3 shows that for re-
sampling it is always more likely to find θ at the global minimizer 1 than at the local minimizer
—1. Lemma 4 states that for reweighting it is more likely to find θ at the local minimizer —1 when
f2	V	a2
f1	—	α1
) . Together, they explain the phenomenon shown in Figure 2.
5
Published as a conference paper at ICLR 2021
+ O(E).
To better understand the condition in Lemma 4, let Us consider the case aι = 2 - e, a2 = 2 + E
with a small constant > 0. Under this setup, V (-1)/V (1) ≈ 1. Whenever the ratio of the
sampling proportions f2/f1 is significantly less than the ratio of the population proportions a2/a1 ≈
1, reweighting will lead to the undesired behavior. The smaller the ratio f2/f1 is, the less likely the
global minimizer will be visited.
The reason for constructing the above piecewise linear loss function is to obtain an approximately
explicitly solvable SDE with a constant coefficient for the noise. One can further extend the re-
sults in 1D for piecewise strictly convex function with two local minima (See Lemmas 9 and 10
in Appendix B.3). Here we present the most general results in 1D, that is, piecewise strictly con-
vex function with finite number of local minima. One may consider the population loss function
V (θ) = Pik=1 aiVi(θ) with Vi (θ) = hi (θ) for θi-1 < θ ≤ θi and Vi (θ) = O(E) otherwise, where
hi (θ) are strictly convex functions and continuously differentiable, O(E) term is sufficiently small
and smooth. Here {θi}ik=-11 are k - 1 disjoint points, and θ0 = -∞, θk = ∞. We assume that V (θ)
has k localminimizers θ* for θ* ∈ (θi-ι,θi). Wepresent the following two lemmas with suitable
assumptions (See Appendix B.3 for details of assumptions, the proof and follow-up discussions).
Lemma 5. The stationary distribution for resampling at any two local minizers θp, θq with p > q
satisfies the relationship
Ps(θp) =eχ " 2 /θp ɪdθ (_J____________________M + Og = ∫ > 1, if ap >aq；
Ps(θq)	exp	[η √θ*	h0p(θ)	V -	ap	1 -	aqj∖	<	< < 1,	if ap	<	aq,
Lemma 6. The stationary distribution for reweighting at any two local minizers θp, θ* with p > q
satisfies the relationship
Pw (总=	"2 Cθp ɪ dθ (	fp____________fq
Pw(θq)	exp [η Λp hp(θ)	Iap(I- fp)	aq(1 - fq)
Multi-dimensional results. Let us now consider the minimization of V (θ) = a1V1(θ) + a2V2(θ)
for more general V1 , V2 and also θ in high dimensions. It is in fact not clear how to extend the above
stochastic analysis to more general functions V (θ). Instead we focus on the transition time from one
stationary point to another in order to understand the behavior of resampling and reweighting. For
this purpose, we again resort to the SDE approximation of the SGD in the continuous time limit.
Such a SDE approximation, first introduced in (Li et al., 2017), involves a data-dependent covariance
coefficient for the diffusion term and is justified in the weak sense with an error of order O(√η).
More specifically, the dynamics can be approximated by
dΘ = -VV (Θ)dt + √η∑(Θ)1^dB,	(7)
where Θ(t = kη) ≈ θk for the step k parameter θk, η is the learning rate, and Σ(Θ) is the covariance
of the stochastic gradient at location Θ. In the SDE theory, the drift term VV (∙) is usually assumed to
be Lipschitz. However, in machine learning (for example neural network training with non-smooth
activation functions), it is common to encounter non-Lipschitz gradients of loss functions (as in
the example presented in Section 3). To fill this gap, we provide in Appendix C a justification of
SDE approximation for the drift with jump discontinuities, based on the proof presented in (Muller-
Gronbach et al., 2020). The following two lemmas summarize the transition times between the two
local minimizers.
Lemma 7. Assume that there are only two local minimizers θɪ, θ2 for the objective function V (θ).
Let tθ*→θ* be the transition time for Θ(t) in (7) from the E-neighborhood of θ↑ (a closed ball
of radius E centered at θɪ) to the E-neighborhood of θ2 and tθ*→θ* be the transition time in the
opposite direction. Then
E[τθf ] = /det(V2L(θ2))	(2 (δV(⑹ _ δV(θ2) C +
E[τθ2→θ" = V det(V2L(θ;)) P [ηl∑0)	∑(θ力〃+ W'
Here det(V2L(θ^)) and det(V2L(θ2)) are the determinants of the Hessians at θ↑ and θ2, respec-
tively. δV(θ1) ≡ V(θ°) - V(θk) for k = 1, 2 where θ° is the saddle point between θj,θg.1
1The formal definition of θ : Let θ(t) be a path with θ(0) = θ1,θ(1) = θz, then θ(t)=
arg minθ(t) supt∈[0,1] V (θ(t)) is the path with minimal saddle point height among all continuous paths.
◦
θ = suPt∈(o,i) θ(t) is the saddle point of this path.
6
Published as a conference paper at ICLR 2021
This lemma is known in the diffusion process literature as the Eyring-Kramers formula; see, e.g.,
(Berglund, 2011; Bovier et al., 2004; 2005). Using the above lemma, we obtain the following result
for the transition times for resampling and reweighting.
Lemma 8. Assume that there are only two local minimizers θɪ, θ2 for the objective function V (θ).
Also assume that the loss function V1(∙) for the first group is O(E) in the e-neighborhood of θ2
and the loss function V2(∙) for the second group is O(E) in the E-neighborhood of θɪ. In addition,
assume that the determinants of the Hessian at two local minimizers are the same. Then the ratio of
the transition times between the two local minimizers for resampling is
E[τ⅞→θ2] _	f 2 (	δV(θ打	δV(θ2)
E[τθ…埒]=exp In <aιVH(θ^VH(θD> - a2VV2(θ^)w(θ2)>
and the ratio for reweighting is
E[τw-θ2]=ex。但(/ISV®)___________________f2δve2)	r+
E[τw→θ: ]	pin [a2VVι(θf)VVι(θf)>	a2V½(θ2)VV2(θ^)> ； + +
))+ O(√E)
See Appendix B for the proof. When the ratio is larger than 1, it means that θɪ is more stable
than θ2. This result shows that for reweighting the relative stability of the two minimizers highly
depends on the sampling proportions (f1 , f2). On the other hand, for resampling it is independent
of (f1 , f2), which is precisely the desired result for a bias correction procedure. To see how the
sampling proportions affect the behavior of reweighting, let US consider a simple case where θɪ is
the global minimizer, VV1(θ^)VV1(θ^)> = V½(θ2)VV2(θ^)>, aι = ɪ + e,a2 = ɪ — e, and
fι《 f2. This ensures that δV(θɪ) > δV(θ2) and the above ratio for resampling is larger than 1,
which is the desired result. However, fι《f2 implies that f《1, f》1, and the above ratio for
reweighting is much smaller than 1, which means that the local minimizer θ2 is more stable than the
global minimizer θɪ.
5	Experiments
This section examines the empirical performance of resampling and reweighting for problems from
classification, regression, and reinforcement learning. As mentioned in the previous sections, the
noise of stochastic gradient algorithms makes optimal learning rate selections much more restric-
tive for reweighting, when the data sampling is highly biased. In order to achieve good learning
efficiency and reasonable performance in a neural network training, adaptive stochastic gradient
methods such as Adam (Kingma & Ba, 2014) are applied in the first two experiments. We observe
that resampling consistently outperforms reweighting with various sampling ratios when combined
with these adaptive learning methods.
Classification. This experiment uses the Bank Marketing data set from (Moro et al., 2014) to
predict if a client will subscribe a term deposit. After preprocessing, the provided data distribu-
tion over the variable “y” that indicates the subscription, is highly skewed: the ratio of “yes” and
“no” is f1/f2 = 4640/36548 ≈ 1/7.88. We assume that the underlying population distribution is
a1/a2 = 1. We setup a 3-layer neural network with the binary cross-entropy loss function and train
with the default Adam optimizer. The training and testing data set is obtained using train_test_split
provided in sklearn2 . The training takes 5 epochs with the batch-size equal to 100. The performance
is compared among the baseline (i.e. trained without using either resampling or reweighting), re-
sampling (oversample the minority group uses the sample with replacement), and reweighting. We
run the experiments 10 times for each case, and then compute and plot results by averaging.
To estimate the performance, rather than using the classification accuracy that can be misleading
for biased data, we use the metric that computes the area under the receiver operating characteristic
curve (ROC-AUC) from the prediction scores. The ROC curves plots the true positive rate on the
y-axis versus the false positive rate on the x-axis. As a result, a larger area under the curve indicates
a better performance of a classifier. From both Table 1 and Figure 3, we see that the oversampling
has the best performance compared to others. We choose oversampling rather than undersampling
for the resampling method, because if we naively down sample the majority group, we throw away
many information that could be useful for the prediction.
2 https://scikit- learn.org/stable
7
Published as a conference paper at ICLR 2021
	Baseline	Resampling	Reweighting
training loss	0.3221	0.2602 二	0.2831	=
roc _auc .score	0.9277	0.9516	0.9312 —
Table 1: The loss takes the binary cross-entropy with a 3-layer neural network. We see that in
average of 10 trials, the resampling method (oversampling) achieves the lowest training loss and
highest ROC-AUC score over testing data among all tested cases.
Nonlinear Regression. This experiment uses
the California Housing Prices data set3 to pre-
dict the median house values. The target me-
dian house values, ranging from 15k to 500k,
are distributed quite non-uniformly. We select
subgroups with median house values > 400k
(1726 in total) and < 200k (11767 in total) and
combine them to make our dataset. In the pre-
processing step, we drop the “ocean proximity”
feature and randomly set 30% of the data to be
the test data. The remaining training data set
with 8 features is fed into a 3-layer neural net-
work. The population proportion of two sub-
groups is assumed to be a1/a2 ≈ 1, while re-
sampling and reweighting are tested with var-
ious sampling ratios f1/f2 near 11767/1726.
Their performance of is compared also with the
baseline. In each test, the mean squared error
(MSE) is chosen as the loss function and Adam
Figure 3: The ROC curve comparisons show
that the resampling has the largest area under the
curve.
is used as the optimizer in the model. The batch-size is 32 and the number of epochs is 400 for each
case. As shown in Table 2, resampling significantly outperforms reweighting for all sampling ratios
in terms of a lower averaged MSE, and its good stability is reflected in its lowest standard deviation
for multiple runs.
MSE	Baseline	RS	RW(f1∕f2 =正	RW (fι /f = 9∏	RW (fl ∕f2 = 12∏
mean	1.0386e+05 二	7.9679e+04	9.3567e+04	9.0436e+04	9.1949e+04
std	8.0371e+03 —	1.8620e+03	3.8044e+03~~	2.4692e+03~~	3.0341e+03
Table 2: Mean squared errors (MSE) for nonlinear regression problems. RS stands for resampling
and RW for reweighting. The weights used in reweighting are a1 and a2, respectively. For each
case, we run experiments for 10 times and compute the corresponding mean and standard deviation.
Resampling (oversampling the minor group) achieves the lowest mean and standard deviation of
MSE among all tested cases.
Off-policy prediction. In the off-policy prediction problem in reinforcement learning, the objec-
tive is to find the value function of policy π using the trajectory {(at, st, st+1)}tT=1 generated by a
behavior policy μ. To achieve this, the standard approach is to update the value function based on
the behavior policy’s temporal difference (TD) error δ(st) = R(st) + γV (st+1) - V (st) with an
importance weight En [δ∣st = s] = Pa∈A ](：|：) E[δ∣st = s, at = a]μ(a∣s), where the summation is
taken over the action space A. The resulting reweighting TD learning for policy π is
Vt+1 (St) = Vt(St) + ηπ(⅛4(R(St) + YV(st+ι) - Vt(St)),
μ(at∣st)
where η is the learning rate. This update rule is an example of reweighting. On the other hand,
the expected TD error can also be written in the resampling form, En [δ|st = s] = fα∈A E[δ|st =
3https://www.kaggle.com/camnugent/california-housing-prices
8
Published as a conference paper at ICLR 2021
s, at = a]π(a∣s) = Pa∈A P∏=aιlS)N E[δj |st = s, at = a], where N is the total number of samples
for st = s. This results to a resampling TD learning algorithm: at step t,
Vt+1 (st) = Vt(st) + η(R(sk) + γVt(sk+1) - Vt (sk)),
where (ak, sk, sk+1 ) is randomly chosen from the data set {(aj , sj, sj+1 )}sj =st with probability
π(ak |st).
Consider a simple example with discrete state space S = {i}in=-01, action space A = {±1}, discount
factor γ = 0.9 and transition dynamics st+1 = mod(st + at, n), where the operator mod (m, n)
gives the remainder of m divided by n. Figure 4 shows the results of the off-policy TD learning
by these two approaches, with the choice of n = 32 and r(s) = 1+ sin(2πs∕n) and learning rate
η = 0.1. The target policy is π(a∕s) = 2 while the behavior policy is μ(a∕s) = 2 + ca%. The
difference between the two policies becomes larger as the constant c ∈ [0, 1/2] increases. From the
previous analysis, if one group has much fewer samples as it should have, then the minimizer of
the reweighting method is highly affected by the sampling bias. This is verified in the plots: as c
becomes larger, the performance of reweighting deteriorates, while resampling is rather stable and
almost experiences no difference with the on-policy prediction in this example.
state
Figure 4: The left plot shows the approximate value function obtained by the two methods. The right
plot is the evolution of the relative error log(et), where the absolute error et = ∣∣Vt(s) - Vπ(s)k∣.
RW and RS in the upright corner represent reweighting and resampling, respectively. c determines
the behavior policy μ(ai | s) = ɪ + ca》The value function is trained on a trajectory with length 105
generated by the behavior policy. The value function obtained from resampling is fairly close to the
exact value function, while the results of reweighting gets worse as the behavior policy gets further
from the target policy.
-2------------1----------1-----------
0	2	4	6
iterates
8	10
x104
6 Discussions
This paper examines the different behaviors of reweighting and resampling for training on biasedly
sampled data with the stochastic gradient descent. From both the dynamical stability and stochastic
asymptotics viewpoints, we explain why resampling is numerically more stable and robust than
reweighting. Based on this theoretical understanding, we advocate for considering data, model, and
optimization as an integrated system, while addressing the bias.
An immediate direction for future work is to apply the analysis to more sophisticated stochastic
training algorithms and understand their impact on resampling and reweighting. Another direction
is to extend our analysis to unsupervised learning problems. For example, in the principal component
analysis one computes the dominant eigenvectors of the covariance matrix of a data set. When the
data set consists of multiple subgroups sampled with biases and a stochastic algorithm is applied
to compute the eigenvectors, then an interesting question is how resampling or reweighting would
affect the result.
Acknowledgements
The work of L.Y. and Y.Z. is partially supported by the U.S. Department of Energy via Scien-
tific Discovery through Advanced Computing (SciDAC) program and also by the National Science
Foundation under award DMS-1818449. J.A. is supported by Joe Oliger Fellowship from Stanford
University.
9
Published as a conference paper at ICLR 2021
References
Alexander Amini, Ava P Soleimany, Wilko Schwarting, Sangeeta N Bhatia, and Daniela Rus. Un-
covering and mitigating algorithmic bias through learned latent structure. In Proceedings of the
2019AAAI/ACM Conference onAI, Ethics, and Society, pp. 289-295, 2019.
Nils Berglund. Kramers’ law: Validity, derivations and generalisations. Markov Processes Relat.
Fields, 19(3):459-490, 2011.
Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. Man is
to computer programmer as woman is to homemaker? debiasing word embeddings. In Advances
in neural information processing systems, pp. 4349-4357, 2016.
Anton Bovier, Michael Eckhoff, VeroniqUe Gayrard, and Markus Klein. Metastability in reversible
diffusion processes i. sharp asymptotics for capcities and exit times. Journal of the European
Mathematical Society, 6(4):399-424, 2004.
Anton Bovier, Veronique Gayrard, and Markus Klein. Metastability in reversible diffusion processes
ii: Precise asymptotics for small eigenvalues. Journal of the European Mathematical Society, 7
(1):69-99, 2005.
Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. Semantics derived automatically from
language corpora contain human-like biases. Science, 356(6334):183-186, 2017.
Flavio Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Ramamurthy, and Kush R
Varshney. Optimized pre-processing for discrimination prevention. In Advances in Neural Infor-
mation Processing Systems, pp. 3992-4001, 2017.
Haw-Shiuan Chang, Erik Learned-Miller, and Andrew McCallum. Active bias: Training more accu-
rate neural networks by emphasizing high variance samples. In Advances in Neural Information
Processing Systems, pp. 1002-1012, 2017.
Mikel Galar, Alberto Fernandez, Edurne Barrenechea, Humberto Bustince, and Francisco Herrera.
A review on ensembles for the class imbalance problem: bagging-, boosting-, and hybrid-based
approaches. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and
Reviews), 42(4):463-484, 2011.
Haixiang Guo, Yijing Li, Jennifer Shang, Mingyun Gu, Yuanyue Huang, and Bing Gong. Learning
from class-imbalanced data: Review of methods and applications. Expert Systems with Applica-
tions, 73:220-239, 2017.
Haibo He and Edwardo A Garcia. Learning from imbalanced data. IEEE Transactions on knowledge
and data engineering, 21(9):1263-1284, 2009.
Haibo He and Yunqian Ma. Imbalanced learning: foundations, algorithms, and applications. John
Wiley & Sons, 2013.
Faisal Kamiran and Toon Calders. Data preprocessing techniques for classification without discrim-
ination. Knowledge and Information Systems, 33(1):1-33, 2012.
Matthew Kay, Cynthia Matuszek, and Sean A Munson. Unequal representation and gender stereo-
types in image search results for occupations. In Proceedings of the 33rd Annual ACM Conference
on Human Factors in Computing Systems, pp. 3819-3828, 2015.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Bartosz Krawczyk. Learning from imbalanced data: open challenges and future directions. Progress
in Artificial Intelligence, 5(4):221-232, 2016.
M Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable
models. In Advances in neural information processing systems, pp. 1189-1197, 2010.
Qianxiao Li, Cheng Tai, and E Weinan. Stochastic modified equations and adaptive stochastic
gradient algorithms. In International Conference on Machine Learning, pp. 2101-2110, 2017.
10
Published as a conference paper at ICLR 2021
Qianxiao Li, Cheng Tai, and E Weinan. Stochastic modified equations and dynamics of stochastic
gradient algorithms i: Mathematical foundations. J. Mach. Learn. Res., 20:40-1, 2019.
Victoria Lopez, AIberto Fernandez, Salvador Garcia, Vasile Palade, and Francisco Herrera. An
insight into classification with imbalanced data: Empirical results and current trends on using
data intrinsic characteristics. Information sciences, 250:113-141, 2013.
Tomasz Maciejewski and Jerzy Stefanowski. Local neighbourhood extension of smote for min-
ing imbalanced data. In 2011 IEEE symposium on computational intelligence and data mining
(CIDM), pp. 104-111. IEEE, 2011.
Tomasz Malisiewicz, Abhinav Gupta, and Alexei A Efros. Ensemble of exemplar-svms for object
detection and beyond. In 2011 International conference on computer vision, pp. 89-96. IEEE,
2011.
Inderjeet Mani and I Zhang. knn approach to unbalanced data distributions: a case study involv-
ing information extraction. In Proceedings of workshop on learning from imbalanced datasets,
volume 126, 2003.
Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. Pulse: Self-
supervised photo upsampling via latent space exploration of generative models. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2437-2445, 2020.
Sergio Moro, Paulo Cortez, and Paulo Rita. A data-driven approach to predict the success of bank
telemarketing. Decision Support Systems, 62:22-31, 2014.
Thomas Muller-Gronbach, Larisa Yaroslavtseva, et al. On the performance of the euler-maruyama
scheme for sdes with discontinuous drift coefficient. In Annales de l'Institut Henri Poincare,
Probabilites et Statistiques, volume 56, pp. 1162-1178. Institut Henri Poincare, 2020.
Grant Rotskoff and Eric Vanden-Eijnden. Parameters as interacting particles: long time convergence
and asymptotic error scaling of neural networks. In Advances in neural information processing
systems, pp. 7146-7155, 2018.
Matthew Schlegel, Wesley Chung, Daniel Graves, Jian Qian, and Martha White. Importance re-
sampling for off-policy prediction. In Advances in Neural Information Processing Systems, pp.
1799-1809, 2019.
Chris Seiffert, Taghi M Khoshgoftaar, Jason Van Hulse, and Amri Napolitano. Resampling or
reweighting: A comparison of boosting implementations. In 2008 20th IEEE International Con-
ference on Tools with Artificial Intelligence, volume 1, pp. 445-451. IEEE, 2008.
Bin Shi, Simon S Du, Weijie Su, and Michael I Jordan. Acceleration via symplectic discretization
of high-resolution differential equations. In Advances in Neural Information Processing Systems,
pp. 5744-5752, 2019.
Latanya Sweeney. Discrimination in online ad delivery. Queue, 11(3):10-29, 2013.
Lei Wu, Chao Ma, and Weinan E. How sgd selects the global minima in over-parameterized learning:
A dynamical stability perspective. In Advances in Neural Information Processing Systems, pp.
8279-8288, 2018.
Han Zhao, Amanda Coston, Tameem Adel, and Geoffrey J Gordon. Conditional learning of fair
representations. In International Conference on Learning Representations, 2019.
A Proofs in section 3
A.1 Proof of Lemma 1
Proof. In resampling, near θ = -1 the gradient is θ + 1 with probability a1 and 0 with probability
a2. Let us denote the random gradient at each step by W (θ + 1), where W is a Bernoulli random
11
Published as a conference paper at ICLR 2021
variable with mean E(W) = a1 and variance V(W) = a1a2. At the learning rate η, the iteration
can be written as
(θk+1 + 1) = (1 - ηW)(θk + 1).
The first and second moments of the iterates are
E[θk + 1] =(1-ηa1)k(θ0+1),	(8)
E[(θk + 1)2] = ((1 - ηa1)2 + η2a1a2)k(θ0 + 1)2.
According to the definition of the stochastic stability, SGD is stable around θ = -1 if the multi-
plicative factor of the second equation is bounded by 1, i.e.
(1 - ηa1)2 + η2a1a2 ≤ 1.	(9)
Consider now the stability around θ = 1, the iteration can be written as
(θk+1-1)=(1-ηW)(θk-1),
where W is again a Bernoulli random variable with E(W) = a2 and V(W) = a1a2. The same
computation shows that the second moment follows
E[(θk - 1)2] = ((1 - ηa2)2 + η2a1a2)k(θ0 - 1)2.
Therefore, the condition for the SGD to be stable around θ = 1 is
(1 - ηa2)2 + η2a1a2 ≤ 1.	(10)
□
A.2 Proof of Lemma 2
Proof. In reweighting, near θ = -1 the gradient is f1 (θ + 1) with probability fι and 0 with Prob-
ability f2. Let us denote the random gradient at each step by W (θ + 1), where W is a Bernoulli
random variable with E(W) = aι and V(W) = ff (f1) . At the learning rate η, the iteration
can be written as
(θk + 1 + I) - (I - ηw)(θk + I).
Hence the second moments of the iterates are given by
E[(θk + 1)2] = ((1 - ηaι)2 + η2fιf2(aι∕fι)2)k(θo + 1)2.
Therefore, the condition for the SGD to be stable around θ = -1 is
(1 - ηaι)2 + η2fιf2 (-1) ≤ 1.
Consider now the stability around θ = 1, the gradient is 0 with probability fι and a2 (θ - 1) with
probability f2. An analysis similar to the case θ = -1 shows that the condition for the SGD to be
stable around θ = 1 is
(1 - η-2)2 + η2fιf2 (f) ≤ 1.
□
B Proofs in section 4
B.1	Proof of Lemma 3
Proof. In resampling, with probability -1 the gradients over the four intervals (-∞, -1), (-1, 0),
(0, 1), and (1, ∞) are -1, 1, , and . With probability -2, they are -, -, -1, and 1 across
these four intervals. The variances of the gradients are -1-2(1 - )2, -1-2(1 + )2, -1-2(1 + )2,
12(1 - )2, respectively, across the same intervals.
12
Published as a conference paper at ICLR 2021
Since	1, the variance can be written as a1a2+O() across all intervals. Then the SGD dynamics
with learning rate η can be approximated by
θk + 1 - θk - η (V0(θk) + Pa1a2 + O(E)W),
where W 〜N(0,1) is a normal random variable. When η is small, one can approximate the
dynamics by a stochastic differential equation of form
dΘ = -V 0(Θ)dt + √ηp a1a2 + O(E) dB
by identifying θk ≈ Θ(t = kη) (see Appendix C for details). The stationary distribution of this
stochastic process is
Ps(θ) = BeXp --τ-------jrv N V⑹),
Z	(a1a2 + O(E))η
where Z is a normalization constant. Plugging in θ = -1, 1 results in
-2T¾ =exp (-7_ 2nt (V (V(1) - V(-1))) = exp (--—(V(1) - V(-1)) + O(E))
ps(-1)	(a1a2 + O(E))η	a1a2η
= exp (——(V(1) - V(-1))) + O(E).
a1a2η
Under the assumption that E 1, the last term is negligible. When a2 > a1, V(θ) is minimized at
θ = 1, which implies -(V(1) - V(-1)) > 0. Hence, this ratio is larger than 1.	□
B.2	Proof of Lemma 4
Proof. In reweighting, with probability f the gradients are - a1, a1, a1 e, and a1 E over the four in-
tervals (-∞, -1), (-1,0), (0,1), and (1, ∞), respectively. With probability f2, they are - f e,
-a2e, -a2, and a2. The variances of the gradients are f1f2(a1 - a2e)2, f1f2(a1 + a2e)2,
f1f2( f e + a2)2, and f1f2( f e - f )2, respectively, across the same intervals.
22
Since E < 1, the variance can be written as ff -2 + O(e) for θ < 0 and f f2 72 + O(e) for θ > 0.
f1	f2
With θk ≈ Θ(kη), the approximate SDE for θ < 0 is given by
dΘ = -V0(⑼ dt + √ηjf1f2 f + O(E) dB
while the one for θ > 0 is
dΘ
-V0(θ)dt + √η jf1f2 f2 + O(E)dB
(see Appendix C for the SDE derivations). The stationary distributions for θ < 0 and θ > 0 are,
respectively,
1
Zrexp
7----------------V V (θ)
(f1f2 f2 + O(E)) η
1
Zexp
7-------------V V (θ)
(f1f2 f + O(E))η
Plugging in θ = -1, 1 results in
Pw(I)
Pw (-1)
Z1
Z2exp
α——-----------ʌ- V(1) + τ——-----------V(-1)
(f1f2 f + O(E)) η	(f1f2 f + O(E)) η
(11)
Z1
Z2exp
(-甘V(1) + 2⅛f2V(-1) +O(E)).
a22η	a21η
13
Published as a conference paper at ICLR 2021
The next step is to figure out the relationship between Z1 and Z2. Consider an SDE with non-smooth
diffusion dΘ = -V0(Θ)dt + σdB. The Kolmogorov equation for the stationary distribution is
(12)
This suggests that σ2P is continuous at the discontinuity θ = 0. In our setting, since V(0) = 0, this
simplifies to
(f1f2 f + O(E)) η ∙ Z" = (f1f2 f + O(E)) η ∙ ZΓ.
This simplifies to
2
Zl = f1f2 f +O(E) = af
Z2	f1f2 f + O(E)	a2∕fl	.
f2
Inserting this into (11) results in
O(f exP (- ff V ⑴ + 2ff V (T) + O9
a2η	a1η
(-2ff V(1) + 2ff V (-1)) + O(e).
a22η	a21η
By the assumption f ≤ M 'V-)) and V⑴ < V(-1) < 0, one has 倡)(旨)≤ V(-y) < 1
and -f/f1 V⑴ ≤ -f/f2 V(-1). Hence the above ratio is less than 1.	□
a2	a1
B.3 EXTENDED RESULTS FOR 1-DIMENSION
Let us consider the population loss function V(θ) = a1V1(θ) + a2V2(θ) with,
V1(θ) =	hθ1(θ),
θ,
θ≤0
θ>0
V2(θ)=	h-2(θθ,),	θθ>≤00,
where h1, h2 are strictly convex functions and continuously differentiable. We assume V(θ) has
two local minimizers θ1 < 0, θ2 > 0 and the values are negative at local minima. Therefore, when
a2 > a1, θ2 should be the global minimizer. In addition, we assume that the geometries of h1, h2
at two local minimizers are similar, i.e., h1(θ1) = h2(θ2), h01(θ1) = h02(θ2); if we set gi (θ) to be
the anti-derivative of 1/h0i (θ), then g1(θ1) = g2(θ2). Moreover, we assume that the two disjoint
convex functions are smooth at the disjoint point, i.e., h01(0) = h02(0) and g1(0) = g2(0). The
following two lemmas extend Lemmas 3 and 4 to piecewise strictly convex function based on the
above assumptions.
Lemma 9. When a2 > a1, V(θ2) < V(θ1). The stationary distribution for resampling satisfies the
relationship
PS (&)
Ps(θ1)
+ O() > 1.
Proof. In resampling, with probability a1 the gradients in the two intervals (-∞, 0), (0, ∞) are
h01(θ), respectively; with probability a2 the gradients are -, h02(θ) respectively. Therefore, the
expectation of the gradients μ(θ) is aιh1(θ) + O(e) in (-∞, 0) and a2h2(θ) + O(e) in (0, ∞). The
variance of the gradients σ(θ) is a1a2h01(θ)2 + O() in (-∞, 0) and a1a2h02(θ)2 + O() in (0, ∞).
The p.d.fps(t, θ) satisfies
∂tPs = ∂θ (μps + 2∂θ (σps)),
therefore, the stationary distribution ps(θ) satisfies
μ + 2∂θσ) Ps + η2σ∂θps = 0, or equivalently,
+---ps + dθps = 0,
14
Published as a conference paper at ICLR 2021
which implies PS (θ) = 1 e-F(θ) with normalization constant Z = f∞∞ e-F(θ), where
F(θ)= /θ	2μ(ξ)	,	∂ξσ(ξ) fF1(θ)	-	Fι(-∞), θ ≤ 0,	(13)
()=J-∞	ησ(ξ)	+	σ(ξ)	ξ = "2(θ)-	F2(0) + F1(O) - Fι(-∞),	θ> 0.	(3)
By inserting μ, σ in different intervals, one has
F1(θ)
F2(θ)
T7 dθ + log(a1a2(h1)2) + O(E);
h01	1
-j-d dθ + log(a1a2(h2)2) + O(e).
h02	2
Hence, the ratio of the stationary probabiliy at two local minimizers θ1 < 0, θ2 > 0 is
禺=exp(-F(θl) + F(θ2)) = exp(-F≡) + F2(θ2) + (FI(O)- F2(O)))
exp
exp
-2gι(θΟ + 2g2(θ2) + iog ()) ∙
ηa2	ηa1	h01 (θ1 )2
ɪgι(0) - ɪg2(0)+log (需2)) + O(E),
ηa2	ηa1	h02 (O)2
where gi(θ) = / 齐dθ,i = 1, 2. By the assumption that g1(θ1) = g2(θ2) and h[(θ1) = h02(θ2),
g1 (0) = g2(0) and h01 (0) = h02(0) one has,
段=eχp(2(gi(o)-gi(°I)) &
W))+O(E),
—
Since a2 > aι > 0,焉 - 六 <	0.	Because of the strictly convexity	of hi,	hl(θ)	> 0 in (θι, 0),
therefore, one has gi(0) - gi(θi)	=	R0 h^dθ > 0. Therefore
Ptj=eχp (3ι(O)- g1(θI)) α
aɔ)+O(E) < 1,
—
□
Lemma 10. When	a2	>	aι,	V (θ2)	<	V (θι).	Under the condition	f1	> JaI,the stationary
distribution for resampling satisfies the relationship
Pw(θ2) _	(2( f2	fl ∖ 0	1	,,A n, ʌ 1
F=exp U (靠-西)Jθl E J+O(E) <
One sufficient condition such that f1 > ʌʌɑ1 is when fi, f2 is significantly different from ai, a? in
the sense that f1 > f2 when the actually population proportion a1 < a2 .
Proof. In reweighting, with probability f1 the gradients over the two intervals (-∞, 0), (0, ∞)
are f1 hl(θ), f1 E respectively; with probability f? the gradients are -a2e, a2h2(θ) respectively.
Therefore, the expectation ofthe gradients μ(θ) is aιhi(θ)+O(E) in (-∞, 0) and a2h2(θ)+O(E) in
(0, ∞). The variance of the gradients σ(θ) is f2 a2hl(θ)2 + O(E) in (-∞, 0) and f aghg(θ)2 + O(E)
in (0, ∞). From the similar analysis as in Lemma 9, the stationary distribution is Pw (θ)= 十e-F⑻
with the same F(θ) defined in equation 13, but F1, F2 are defined as follows
Fi(θ)=fi / hdθ+log (fi (h1)2)+O(E)；
F2(θ) = f / h2dθ + log (f (h2)2) +O(E).
15
Published as a conference paper at ICLR 2021
Hence, the ratio of the stationary probabiliy at two local minimizers θ1 < 0, θ2 > 0 is
PwD=eχpTM+F2 (初 + (FM- F2W)
exp
2f1	M、 f z. x 1
—7一gl(θl) + —7一g2(θ2) +lθg
ηf2a1	ηf1a2
(f1a2局(生产C
lf¾ hiW77
exp
1(0) -
~f~ g2(0)+log
ηf1a2
(fɑl 业 C
w2 a2 h2(Oy))
+ O(),
where gi(θ) = f f7dθ, i = 1,2. By the assumption that g1 (θ1) = g2(θ2) and h1(θ1) = h'2(θ2)
fi
1(0) = g2(0) and h01(0) = h02(0) one has,
Pwlj=exp G2(gι (O)- gι(θI))
+ O().
Because of the strictly convexity of h1, one has g1(0) - g1(θ1) > 0. By the assumption f > ʌ/a1,
then (f - f) > 0,WhiCh gives Ps(θ1) > L	□
Proof of Lemmas 5 and 6 We can further extend the results in 1D for a finite number of local
minima as presented in Lemmas 5 and 6. In the same way as in the two local minima case, we
assume that hi (θ) has a similar geometry at the minimizers and hi (θ), hi+1(θ) are smooth enough
at the disjoint point θi. In order to obtain the ratio of the stationary distribution at two arbitrary local
minimizes, we take an additional assumption thatgi(θi-1) =gi(θi) for all i, wheregi(θ) is the anti-
derivative of 1/h0i (θ). Intuitively, this assumption requires that each local minimum has an equal
barrier on both sides. To be more specific, the assumptions we mentioned above are the following:
at all the local minimizers, hi(θ*) = hj(θj) < 0, hi(θ*) = hj(θj), let gi(θ) = R ^1^dθ, then
gi(θ*) = gj(θ*) for any i = j; at all the disjoint points, hi(θi) = hi+1(θi), gi(θi-1) = gi(θi)=
i+1(θi) for all i. Lemmas 5 and 6 are under the above assumptions.
Proof of Lemma 5. For resampling, with probability ai, the gradient is h0i(θ) for θ ∈ (θi-1, θi), and
O(E) for θ ∈ (θi-ι, θi). Therefore, the expectation and variance in (θi-1, θi) are μ = &h； (θ)+O(e)
and σ = ai(1 - ai)h0i(θ)2 + O(). The stationary solution is
1	i-1
Ps(θ) = Ze-F⑻，with F(θ) = Fi(θ) - Fi(θi-ι) + £ Fj(θj) - Fj(θj--1), for θ ∈ Rf 仇),
j=1
where Z = R-∞∞ e-F(θ) is a normalization constant and
21
Fi(θ) = η J h0(θ) dθ + log (a；(1 - ai)hi(θ)2) + O(e).
Therefore, the ratio of the stationary probability at any two local minimizers θp, θq is
簧=exP ]-卜p(θP) - FP(θp-ι)+ XFj(θj) - Fjg-)
+ (Fq (θ;) - Fq (θq-1) + X Fj (θj )- Fj (θj-1) |
j=1
q-1
-Fp(θp) + Fq(θq) + X Fj(θj) - Fj+ι(θj)
exp
exp -
j=p
2 a— 2	∕θ*∖, l	a αq(I-aq)% (%)')
η(1- ap) gp(θp) + ηa-^q)gq 也)+ g I^aq (1 - ap)hp(θp)2J )-
exp (g η(⅛) gj ⑸)- ηθ-2aj+1) gj+M) + log ( aj+aj(1--aa+ 1hh jjθj )2 !i + OB.
16
Published as a conference paper at ICLR 2021
By the assumption that	gp(θp)	=	gq(θq), hp(θp)	=	h∖(θj)	and	gi(θi-ι)	=	gi(θi)
gi+1(θi), h0i(θi) = h0i+1(θi) for all i, then the above ratio can be simplified to
Ps (θp)
Ps (θq)
exp
+ O() =	<> 11
if ap > aq;
if ap < aq,
where the last inequality can be easily derived from that gp(θp )-gp(θp) = R'θP h^ dθ > 0 because
of the strictly convexity of hp.	□
ProofofLemma 6. For reweighting, with probability fi, the gradient is fihi(θ) for θ ∈ (θi-ι,θi),
and O(C) for θ ∈ (θ%-ι,θi). Therefore, the expectation and variance in (θi-ι, θi) are μ = a%hi(θ) +
O(E) and σ = (∖-fi)a hi(θ)2 + O(c). The stationary solution
1	i-1
Pw(θ) = Ze-F(θ),	with F(θ) = Fi(θ) - Fi(%-i) + £ Fj(θj) - Fj(θj-ι), for θ ∈ (%—i,%),
Z	j=1
where Z = R-∞∞ e-F(θ) is a normalization constant and
F" η⅛ Z h⅛ dθ + log (f2 hi(θ)2 ) + O(e)
Therefore, the ratio of the stationary probability at any two local minimizers θp, θq is
q-1
-Fp(θp) + Fq(θq) + X Fj(θj) - Fj+ι(θj)
j=p
e (	2fp (θ*) +	2fq (θ*) +1 f fp(1	- fq)a2hq(%)	ʌʌ
P I- ηap(1 -	fp) gp(θp) + ηaq(1 —	fq) gq圾) + g "(1	- fp)aphp(θp)2 J J	'
exp fχ ɪɪʒgj(θj) - (1,)gj+ιM) + log f f	f(1 - f*)Iθ)2!)
j=p η(1 - aj)	η(1 - aj+1)	fj+1 (1 - fj+1)aj+1hj+1(θj)
+ O(C)
By the assumption that	gp(θp)	=	gq(θq),	hp(θp)	=	h∖(θj)	and	gi(θi-ι)	=	gi(θi)=
gi+1(θi), h0i(θi) = h0i+1(θi) for all i, then the above ratio can be simplified to
森=exp [2 (gp(θp)- gp(θp)) (apffp)- aqffq))]+ O(e).
□
Follow-up discussions of Lemma 5 and 6
We first note that Jθ h1(θ) dθ > 0 due to the strictly convexity of hp Therefore, one can see from
Lemma 5 that for resampling, the stationary solution always has the highest probability at the global
minimizer. On the other hand, for the stationary solution of reweighting in Lemma 6, let us consider
the case when a? > aq. In this case, V(θp) < V(θq), therefore, one expects the above ratio larger
than 1, which implies that @(f-于)一 ɑ(f-于)> 0. Note that if fp = a?, fq = aq, then this
term is always larger than 0, but when fp , fq are significantly different from ap , aq in the sense that
fP < fq and fp < ap,fq > aq, then gp(f- fp) - ”, (f- f) < 0, WhiCh will lead to p^ < 1, i.e.,
higher probability of converging to θq, which is not desirable. To sum up, Lemma 6 shows that for
reweighting, the stationary solution won’t have the highest probability at the global minimizer if the
empirical proportion is significantly different fron the population proportion.
B.4 Proof of Lemma 8
Proof. By the variance of the gradients for resampling and reweighting in (5), and given that at
the stationary point E[VV(θɪ)] = E[VV(θ2)] = 0, one can omit the last term in the variance.
In addition, since VV1(θ^), V½(θj^)=O(C)《VVι(θ1), VV)(θ)) by assumption, all the higher
order terms are included in an O(√C) term. One can then derive Lemma 8 from Lemma 7.	□
17
Published as a conference paper at ICLR 2021
C A Justification of the SDE approximation
The stochastic differential equation approximation of SGD involving data-dependent covariance
coefficient Gaussian noise was first introduced in (Li et al., 2017) and justified in the weak sense.
Consider the SDE
dΘ = b(Θ)dt + σ(Θ)dB.	(14)
The Euler-Maruyama discretization with time step η results in
Θk+1 = Θk + ηb(Θk) + √ησ(Θk)Zk, Zk 〜N(0,1), Θo = θ°.	(15)
In our case, b(∙) = -V0(∙). When b satisfies LiPschitz continuity and some technical smoothness
conditions, according to (Li et al., 2017) for any function g from a smooth class M, there exists
C > 0 and a > 0 such that for all k = 0,1,2,…，N,
∣E[g(Θkn)] - E[g(θk)]| ≤ Cηα.
However, as the loss function considered in this PaPer has jumP discontinuous in the first derivative,
the classical aPProximation error results for SDE do not aPPly. In fact, the Problem V ∈/ C1(Rn)
is a common issue in machine learning and deeP neural networks, as many loss functions involves
non-smooth activation functions such as ReLU and leaky ReLU. In our case, we need to justify
the SDE aPProximation adoPted in Section 3. It turns out that strong aPProximation error can be
obtained if
•	the noise coefficient σ is LiPschitz continuous and non-degenerate, and
•	the drift coefficient b is Piece-wise LiPschitz continuous, in the sense that b has finitely
many discontinuity points -∞ = ξo < ξι < •…< ξm < ξm+ι = ∞ and in each interval
(ξi-1, ξi), b is LiPschitz continuous.
Under these conditions, the following approximation result holds: for all k = 0,1,2,…，N, there
exists C > 0 such that
E[∣Θkη - θk|] ≤ C√η.	(16)
Here Θ^ is the solution to SDE at time kη. The proof strategy closely follows from (Muller-
Gronbach et al., 2020). The key is to construct a bijective mapping G : R → R that transforms (14)
to SDE with Lipschitz continuous coefficients. With such a bijection G, one can define a stochastic
process Z : [0, T] X Ω → R by Zt = G(Θt) and the transformed SDE is
__ ~	,	—	,	_	_ -	-r	_	_ .	,	、
dZt = b(Zt)dt + σdBt, t ∈ [0,T], Zo = G(Θo),	(17)
with b = (G ∙ b + 1G00 ∙ σ2) ◦ GT and σ = (G- σ) ◦ G-1.	(18)
As the SGD updates can essentially be viewed as data from the Euler-Maruyama scheme, consider-
ing Zk as updates from Euler-Maruyama scheme leads to
E[∣θkn - θk ∣] ≤ c1E[\Zkn - G ◦ θk ∣] = c1E[\Zkn - Zk + Zk - G ◦ θk ∣]
≤ C2√η + cιE[∣Zk - G ◦ θk|].
To control the second item, we introduce
θt := θk + b(θk)(t — kη) + pt — kησ(θk)Zk,
where t ∈ [0, kη]. Then as shown in (Muller-Gronbach et al., 2020),
E[∣Zk — G ◦ θk |] ≤ c√η + CE
with B being the set of pairs (y1, y2) ∈ R2 where the joint Lipschitz estimate |b(y1) - b(y2)| does
not apply due to at least one discontinuity. In (Muller-Gronbach et al., 2020), it is estimated by
kη
1B(θt, θk)dt ,
0
E
Z kη 1B(θt, θk)dt
0
which leads us to (16).
18
Published as a conference paper at ICLR 2021
D Numerical comparisons with different learning rates
In this section, we present extensive numerical results to show the effect of learning rates in our toy
examples. The Figure 5 corresponds to the example in Section 3, and Figure 6 corresponds to the
example in Section 4.
1
0.8
0.6
、0.4
0.2
0
-0.2
-0.4
1
0.8
0.6
、0.4
0.2
0
-0.2
-0.4
1
0.8
0.6
叵0.4
> 0,2
0
-0.2
-0.4
(a) η = 0.3
(b) η = 0.4
1
0.8
0.6
0.4
0.2
0
-0.2
-0.4
(c) η = 0.5
(d) η = 0.6
Figure 5: A comparison of reweighting (upper row) and resampling (lower row) with a1/a2 =
0.4/0.6 and f1/f2 = 0.9/0.1 at various learning rates η. All experiments start at θ0 = 1.6. We can
see that unless the learning rate η < 0.4, resampling is more stable near the minimizer θ = 1.
θ	θ
(a)η=0.10	(b)η=0.11
(c) η = 0.12
(d) η = 0.13
Figure 6: A comparison of reweighting (upper row) and resampling (lower row) with a1 /a2 =
0.4/0.6, f1/f2 = 0.9/0.1 and = 0.1 at various learning rates η. All experiments start at θ0 = 0.9.
We can see that unless the learning rate η < 0.12, resampling is more reliable in the sense that its
trajectory stays around the desired minimizer.
19