Published as a conference paper at ICLR 2021
Effective Distributed Learning with Random
Features: Improved Bounds and Algorithms
Yong Liu1,2,*,t, Jiankun Liu3,*, Shuqiang Wang4
1	Gaoling School of Artificial Intelligence, Renmin University of China
2	Beijing Key Laboratory of Big Data Management and Analysis Methods
3	Institute of Information Engineering, Chinese Academy of Sciences
4	Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences
liuyonggsai@ruc.edu.cn, liujiankun@iie.ac.cn, sq.wang@siat.ac.cn
Ab stract
In this paper, we study the statistical properties of distributed kernel ridge regres-
sion together with random features (DKRR-RF), and obtain optimal generalization
bounds under the basic setting, which can substantially relax the restriction on
the number of local machines in the existing state-of-art bounds. Specifically,
we first show that the simple combination of divide-and-conquer technique and
random features can achieve the same statistical accuracy as the exact KRR in
expectation requiring only O(|D|) memory and O(|D|1.5) time. Then, beyond
the generalization bounds in expectation that demonstrate the average information
for multiple trails, we derive generalization bounds in probability to capture the
learning performance for a single trail. Finally, we propose an effective communi-
cation strategy to further improve the performance of DKRR-RF, and validate the
theoretical bounds via numerical experiments.
1	Introduction
Kernel ridge regression (KRR) is one of the most popular nonparametric learning methods (Vapnik,
2000). Despite the excellent theoretical guarantees, KRR does not scale well in large scale settings
because of high time and memory complexities (Liu et al., 2013; 2014; 2017; 2018; 2020b; Liu &
Liao, 2015; Li et al., 2018; 2019c). Distributed learning (Zhang et al., 2013; Hsieh et al., 2014; Chang
et al., 2017b; Li et al., 2019b; Lin et al., 2020), random features (Rahimi & Recht, 2007; Sutherland
& Schneider, 2015; Rudi & Rosasco, 2017; Rudi et al., 2018; Liu et al., 2020a; Avron et al., 2017a;
Yu et al., 2016; Jacot et al., 2020), and Nystrom methods (Drineas & Mahoney, 2005; Ding & Liao,
2012; Yang et al., 2012; Camoriano et al., 2016; Si et al., 2016; Musco & Musco, 2017; Kriukova
et al., 2017) are the most widely used large scale techniques to address the scalability issues. Recent
statistical learning works on KRR together with large scale approaches demonstrate that these large
scale approaches can not only obtain great computational gains but also can guarantee the optimal
theoretical properties, such as KRR with divide-and-conquer (Zhang et al., 2013; 2015; Chang et al.,
2017b;a; Guo et al., 2017; Lin et al., 2017; Li et al., 2019b;d; Lin et al., 2020), with random features
(Rudi & Rosasco, 2017; Li et al., 2019e; Carratino et al., 2018; Yang et al., 2012), and with Nystrom
methods (Bach, 2013; Alaoui & Mahoney, 2015; Rudi et al., 2015; 2017; Ding et al., 2020).
The combinations of distributed learning and other large scale approaches are very intuitive but
effective strategies to further improve the effectiveness, such as distributed learning with gradient
descent algorithms (Lin & Zhou, 2018; Richards et al., 2020), with multi-pass SGD (Lin & Rosasco,
2017; Lin & Cevher, 2018; 2020), with random features (Li et al., 2019b), and with Nystrom methods
(Yin et al., 2020). The optimal generalization performance of these combining approaches has been
studied, however, the main theoretical problem is that there is a strict restriction on the number of
local machines. For sample, in (Lin & Zhou, 2018; Li et al., 2019b; Yin et al., 2020), to guarantee the
optimal generalization performance in the basic setting, the upper bounds of the local machines are
restricted to be a constant, which is difficult to be satisfied in real applications.
*Both authors contributed equally to this work
,Corresponding author
1
Published as a conference paper at ICLR 2021
In this paper, we aim at enlarging the number of local machines by considering communications
among different local machines. This paper makes the following three main contributions. Firstly, we
improve the existing state-of-art results of the divide-and-conquer technique together with random
features. We prove that the optimal generalization performance can be guaranteed even the partitions
reach Ω( a∕∣D∣), which are limited to a constant Ω(1) for the existing bounds in the basic setting, |D|
is the size of the data sets. Secondly, to essentially reflect the generalization performance, beyond
the minimax optimal rates in expectation, we derive optimal learning rates in probability, which can
capture the learning performance for a single trail. Finally, we develop a communication strategy
to further improve the performance of our proposed method, and validate the effectiveness of the
proposed communications via both theoretical assessments and numerical experiments.
Related Work The most related work includes the statistical analysis of distributed learning and
random features.
Distributed learning. Optimal learning rates for divide-and-conquer KRR in expectation were
established in the seminal work (Zhang et al., 2013; 2015). An improved bound was derived in (Lin
et al., 2017) based on a novel tool of integral operator. Based on the proof techniques proposed in
(Zhang et al., 2013; 2015; Lin et al., 2017), optimal learning rates were established for distributed
spectral algorithms (Guo et al., 2017; Blanchard & Mucke, 2018; Lin & Cevher, 2020), distributed
gradient descent algorithms (Lin & Zhou, 2018; Richards et al., 2020), distributed semi-supervised
learning (Chang et al., 2017b), distributed local average regression (Chang et al., 2017a; Lin & Cevher,
2020), localized SVM (Meister & Steinwart, 2016), etc. Some other communication strategies for
distributed learning have been provided, see e.g. (Fan et al., 2019; Li et al., 2019a; Lin & Cevher,
2020; Li et al., 2020), and references therein. The theoretical analysis mentioned above shows that
the divide-and-conquer learning can achieve the same statistical accuracy as the exact KRR, however,
there is a strict restriction on the number of local machines. The optimal learning rates with a less
strict condition on the number of local machines for distributed stochastic gradient methods and
spectral algorithms were established in (Lin & Cevher, 2020). In (Lin et al., 2020), they considered the
communications among different local machines to enlarge the number of local machines. However,
the communication strategy proposed in (Lin et al., 2020) based on an operator representation, which
requires communicating the input data between each local machine. Thus, it is difficult to protect the
data privacy of each local machine. Furthermore, for each iteration, the communication complexity
of each local machine is O(|D|d), where d denotes the dimension, which is infeasible in practice for
large scale data sets.
Random Features. The generalization bound of random features was first proposed in (Rahimi
& Recht, 2008), which shows that O(IDI) random features are needed for O(1/∙∖∕∣D∣) learning
rate. Some works further studied its theoretical performance (Cortes et al., 2010; Yang et al., 2012;
Sutherland & Schneider, 2015; Sriperumbudur & Szabo, 2015; Bach, 2017; Avron et al., 2017b). By
applying the standard integral operator framework (Smale & Zhou, 2007; Caponnetto & Vito, 2007),
the optimal generalization bounds of KRR with random features were established in (Rudi & Rosasco,
2017), which requires only O(，|D| log(∣D∣)) random features. To decrease the size of random
features, an improved approach was proposed based on a novel leverage score sampling strategy
(Rudi et al., 2018). Sun et al. (2018) extended the result of (Rudi & Rosasco, 2017) to SVM. In (Li
et al., 2019e), they further devised a simple framework for the unified analysis of random Fourier
features, which can be applied to KRR, as well as SVM and logistic regression. To further improve
the effectiveness, recently, Li et al. (2019b) considered the simple combination of divide-and-conquer
and random features. However, to guarantee the optimal generalization performance, the number of
local machines should be restricted to a constant, degenerating it into a single random features-based
large scale KRR.
2	Background
In a standard framework of supervised learning, there is a probability space X × Y with an unknown
distribution ρ, where X is the input and Y is the output space. The sample setD = {(xi, yi)}in=1 of
size n is drawn i.i.d from X × Y with respect to ρ. Let K : X × X → R be a Mercer kernel, and
HK be its reproducing kernel Hilbert space (RKHS) (Steinwart & Christmann, 2008; Vapnik, 2000),
and assume that K(x, x0) ≤ κ, ∀x, x0 ∈ X. Throughout, we will denote the inner product in HK by
h∙, ∙iκ, and corresponding norm by ∣∣ ∙ ∣∣k.
2
Published as a conference paper at ICLR 2021
Kernel Ridge Regression (KRR)
KRR is one of the most popular nonparametric learning methods (Shawe-Taylor & Cristianini, 2000;
Vapnik, 2000), which can be stated as
1	|D|
fD,λ = argmin < |D| ɪ^(f (Xi ) - yi )2 + λkf kK
(1)
where λ > 0 is the regularization parameter, |D| is the size of D. Using the representation
theorem (Shawe-Taylor & Cristianini, 2000; Vapnik, 2000), fD,λ can be written as fD,λ (X) =
PiD11 αiK(xi, X) With α = (KD + λI)-1yD, where KD =由[Kd(xi, Xj)]iDj=1 is the |D| × |D|
kernel matrix and ND =由(yi,..., y∣D∣)τ.
Despite the excellent theoretical guarantees (Blanchard & Kramer, 2010; Caponnetto & Vito, 2007),
KRR requires O(|D|2) memory to store KD, and O(|D|3) time to solve inverse of KD + λI, which
is infeasible for large scale settings.
KRR with Random Features (KRR-RF)
Assuming the spectral measure has a density function %(∙), the corresponding shift-invariant k-
ernel can be written as K(x, x0) = JQ ψ(x, ω)ψ(x0, ω)%(ω)dω, where ψ : XX Ω → R is
a continuous and bounded function with respect to ω and x. The main idea behind random
Fourier features is to approximate the kernel function K(x, x0) by its Monte-Carlo estimation
(Rahimi & Recht, 2007): Km(x, x0) = M PM=I ψ(x, ωi)ψ(x0, ωj = (Φm(x), Φm(x0)i, where
Φm(x) = √M (ψ(x, ωι),..., ψ(x, ωM))T. The solution of KRR with random features can be
written as
fM,D,λ(x) = WM,D,λΦM(x) with wM,D,λ = (Φm,dΦM,D + λI)-1ΦM,DNd,	(2)
where Φm,d = √=d∣ (Φm (xi),…，Φm (x∣D∣)) and y = √^ (yi, ...,y∣D∣)T.
KRR-RF requires O(M |D|) to store ΦM,D, O(M3) and O(M 2|D|) time to solve the inverse of
(ΦM,DΦTM,D + λI) and the matrix multiplication ΦM,D ΦM,D, respectively. Thus, the total space
and time complexity of KRR-RF are O(M |D|) and O(M 2|D|), M |D|, respectively.
Distributed KRR with Random Features (DKRR-RF)
Let {Dj}jm=1 be m disjoint subsets with D = ∪jm=1Dj. The distributed KRR with random features
(DKRR-RF) is defined as
fM ,D,λ
m
X
j=1
|Dj I
|D|
(3)
where fM,D3,λ(x) = WM,Dj,λ0M(x) with WM,D,,λ = (ΦM,DjΦM,Dj + λI)-iΦM,DjNd,. The
space complexity, time complexity and communication complexity of DKRR-RF for each local
machine are O(M |Dj |), O(M3 + M2|Dj|) and O(M), respectively.
3 DKRR-RF with Communications (DRKK-RF-CM)
In this section, we will present an effective communication strategy to enlarge the number of
local machines. We first give the motivation of our communication strategy, and then propose a
communication-based method, called DRKK-RF-CM. The proposed communication strategy are
adaptations from (Lin et al., 2020) to avoid communicating local data among partition nodes.
Motivation. Let gM,D,λ : RM → RM be gM,D,λ(w) := [Φm,dΦM,d + λl] w — Φm,dNd. One
can see that 2gM,D,λ(w) is the gradient ofthe empirical risk of 6 P(Xa yi)∈d (wtΦm (Xi) — y,2 +
λkwk2 on w. From Eq.2, we know that for any w, the following equation holds:
WM,D,λ = w — [Φm,dΦM,d + λl]	[ [Φm,dΦM,d + λl] w — Φm,dNd]
=w— [ΦM,D ΦTM,D + λI] 1 gM,D,λ (w).
3
Published as a conference paper at ICLR 2021
Algorithm 1 Distributed KRR with Random Features and Communications (DKRR-RF-CM)
Initialize: WM d 入=0
for t = 1 to p do
Local machine: compute the local gradient gM,Dj,λ(Wt-D λ)，and communicate back to GM.
Global machine: get the global gradient gM,D,λ(WM,D,λ) = Pm=I 制gM,Dj,λ(WMD,λ) and
communicate to each local machine.
Local machine: compute βj-1 = [φM,Dj ΦM,D + λ∣	gM,D,λ(WM-D λ) and communi-
cate back to the global machine.
Global machine: compute WM d λ = Wt-D λ - lDf Pjm=I βj-1, and communicate to each
local machine.
end for
OUtPUt: WM,D,λ and fM,D,λ = hwM,D,λ, φM(∙)i
Define WM,D,λ = Pm=I 制wm,d3-,λ, it is easy to verify that
m |D |	-1
WM,D,λ = W - X -|D|- [φM,DjφM,Dj + λI]	gM,Dj,λ(W).
(5)
Comparing 4 and 5, and noting that the global gradient gM,D,λ(W) can be achieved via the commu-
nications of each local gradient gM,d”i(w), i.e., gM,d,i(w) = Pm=I 制gM,d”i(w), thus, We
consider the following Newton Raphson iteration-based communication strategy:
m |D |	-1
WM,D,λ = WM,D,λ - X -∣Dp [φM,Dj φM,Dj + λI]	gM,D,λ(WM,D,λ)
(6)
We propose an iterative procedure to implement the communication strategy Eq.6, Which can be
broken doWn into 4 steps. At first, each local machine computes the local gradient and communicates
back to the global machine. Then the global machine computes the global gradient based on the local
gradient, and communicates it to each local machine. In the third step, each local machine computes
βjt-1 , and communicates back to the global machine. Finally, the global machine obtains the solution
WM D λ. More details can be seen in Algorithm 1.
ComPlexity analysis. Space complexity: each local machine only needs to store ΦM,Dj and the local
gradient gM,Dj,λ, thus the space complexity of each local machine is O(M ∣Dj ∣ + M) = O(M ∣Dj ∣);
Time complexity: for each local machine, We only need to compute the matrix multiplication
ΦM,Dj ΦTM,D and the inverse of ΦM,Dj ΦTM,D + λI once. For each iteration, We need to compute
the local gradient gM,Dj,λ and βj for each local machine. Therefore, the total time complexity of
each local machine is O(M3 + M2∣Dj ∣ + pM ∣Dj ∣), Where p is the number of communication;
Communication complexity: for each iteration, We only communicate the local gradient gM,Dj,λ and
βj to the global machine, and receive the gradient gM,D,λ and WMD λ from the global machine, so
the total communication complexity is O(pM).
Remark 1. From the complexity analysis above, we can see that if the number of the communication
p satisfying p ≤ M orp ≤ ∣Dj ∣, then the time and space complexity of DKRR-RF-CM are the same
as DKRR-RF. Only the communication complexity is slightly increased from O(M) to O(pM).
4 Theoretical Analysis
In this section, We analyze the generalization performances of DKRR-RF and DKRR-RF-CM. The
performance of the algorithm is usually measured by the expected risk E(f ) = X×Y(f (x) -
y)2dρ(x, y). The optimal hypothesis fHK in HK is denoted by fHK = arg minf∈H E(f ), and We
assume fHK exists in the paper.
4
Published as a conference paper at ICLR 2021
4.1	Optimal learning rate for DKRR in Expectation
Theorem 1. Suppose that ∣ψ(x, ω)∣ ≤ T almost surely, T ∈ [1, ∞) and |y| ≤《.If λ = Ω(∣D∣- 1),
|D1 | = . . . |Dm |, the number of partitions m and the number of random features M respectively
correspond to m . a∕∣D∣ and M & ∙∖∕∣D∣, then, for every δ ∈ (0,1], With probability at least 1 一 δ,
We have E[Ef ,。,1)]一 E(∕‰) = O (∣D∣-2 log2(1∕δ)).
From Theorem 1, one can see that if m . a∕∣D∣ and M & a∕∣D∣, the learning rate of the general-
ization bound can reach O(1∕ a∕∣D∣), which is optimal in a minmax sense (Rudi & Rosasco, 2017;
Caponnetto & Vito, 2007). It means that, in this basic setting, as long as the number of partitions
and random features are in order Ω( a∕∣D∣), the corresponding ridge regression estimator has optimal
generalization properties. The assumption of ∣y ∣ ≤ ζ can be related to the Bernstein condition or
moment assumption (Blanchard & Kramer, 2016), but for simplicity, in this paper, we only consider
that Y is bounded.
Optimal learning rates for divide-and-conquer KRR in expectation have been established in (Zhang
et al., 2013; 2015; Lin et al., 2017), etc. However, there is a strict restriction on the number of local
machines m. Specifically, in (Lin et al., 2017), to reach the optimal rate, m should to restrict to a
constant m = Ω(1). In (Li et al., 2019b), the authors have studied the generalization performance
of the combination of divide-and-conquer technique and random features. Using the same setting
as Theorem 1 (that is r = 1∕2 and γ = 1 in Theorem 8 of Li et al. (2019b)), they prove that, if
M & p∣D∣ and m . Ω(1),then E[E(IfM,。,入)]-E(∕‰) = O(∣D∣-2 log2(1∕δ)). Itmeansthatto
guarantee the optimal generalization properties, the number of partitions should be restricted to a
constant, but for our result is Ω( a∕∣D∣). In (Li et al., 2019b), they also considered using the unlabeled
data to enlarge the number of partitions. They have proved that (see Corollary 12 of Li et al. (2019b)
for detail), if M & P^ and m . ∣D* ∣∕∣D∣, then E[E(fM ,。,1)]一 E(∕‰) = O(∣D∣-2 log2(1∕δ)),
where D* is the dataset includes both labeled and unlabeled data. Thus, if we want the number of
partitions m to reach Ω( a∕∣D∣) as the same as our Theorem 1, the size of ∣D* ∣ should be Ω( a∕∣D∣)
times of ∣D∣. In this case, the data size of each local machine is ∣D∣ = ∣D*2/a∕∣D∣, so the time and
space complexity are the same as KRR with a single random feature technique.
4.2	Optimal learning rates for DKRR in Probability
Note that E[E(fM,d,i)]-E(fHκ) = E[kfM,d,i-∕‰ kp] (Caponnetto & Vito, 2007), thus Theorem
1 proposes the optimal learning rate in expectation, which demonstrates the average information for
multiple trails, but may fail to capture the learning performance for a single trail. To essentially reflect
the generalization performance for a single trail, we derive the optimal learning rate in probability:
Theorem 2.	Under the same assumptions as Theorem 1. If λ = Ω(∣D∣-2), ∣Dι∣ = ... ∣Dm∣,
m . ∣D∣ 4 and M & ∣D∣ 1, then, for every δ ∈ (0,1], with probability at least 1 一 δ, we have
IIfM,D,λ- /‰，= o(∣D∣- 1 log2(1∕δ))
To guarantee the optimal generalization properties in probability, the number of partitions should
be restricted to Ω(∣D∣1/4), which is stricter than Ω(∣D∣1/2) in Theorem 1. This is because the
generalization error in expectation can be decomposed into approximation error, sample error and
distributed error (more details can be seen in Proposition 1 in Appendix), but the error decomposition
in probability is not easy to separate a distributed error in probability to control the number of local
machines. The derive the optimal learning rate, we provide a novel decomposition, please see details
in Proposition 9 in Appendix.
The following result demonstrates that the proposed communication strategy can enlarge the number
of partitions in probability.
Theorem 3.	Under the same assumptions as Theorem 1, If λ = Ω(∣D∣- 1), ∣Dι∣ = ... ∣Dm∣,
p + 1	1
m . ∣D∣ 2(p+2) and M & ∣D∣ 2, then, for every δ ∈ (0,1], with probability at least 1 — δ, we have
5
Published as a conference paper at ICLR 2021
Table 1: Computational complexity required by different algorithms for the optimal learning rate
O(1/∙∖∕∣D∣)) in the basic setting. Logarithmic terms are not showed.
Methods	Partitions m	Random M	Types	Space	Time	Communication
KRR (Caponnetto & Vito, 2007)	/	/	In probability	|D|2	|D|3	/
KRR-RF (Rudi & Rosasco, 2017)	/	∣D∣0.5	In probability	∣d∣1∙5	|D|2	/
KRR-Nystom Rudi et al. (2015)	/ ∣D∣0.5	|D|0.5	In probability	|D|1.5	|D|2	/ ∣D∣0.5
DKRR (Zhang et al., 2015; Chang et al., 2017b)		/	In expectation	|D|	|D|2	
DKRR (Lin et al., 2020)	∣D∣0.25	/	In probability	|D|1.5	∣D∣2∙25	∣D∣0.75
	p+1			p + 3	3(p+3)	
DKRR-CM (Lin et al., 2020)	|D| 2(P+ 2)	/ ∣D∣0.5	In probability	|D| P+2	|D| 2(P+2)	pd|D|
DKRR-RF (Li et al., 2019b)	Ω(1)		In expectation	|D|	|D|2	∣D∣0.5
DKRR-RF (Theorem 1)	|D|0.5	|D|0.5	In expectation	|D|	|D|1.5	|D|0.5
DKRR-RF (Theorem 2)	∣D∣0.25	∣D∣0.5	In probability	∣D∣1∙25	|D|1.75	∣D∣0.5
	p+1 .			2p + 5	3p+7	
DKRR-RF-CM (Theorem 3)	|D| 2(P+2)	∣D∣0.5	In probability	|D| 2p+4	|D| 2p+4	P∣D∣0.5
IfMD λ — ∕hk Il = O (∣D∣-1 logp+2(1∕δ)) , where fM D 入 is returned by Algorithm 1 under
p-th iterations.
Compared Theorem 3 with Theorem 2, it is clear that the proposed communication strategy can
relax the restriction on m from Ω (∣D∣1/4) to Ω (IDI(P+1)/(2(P+2))). Note that m is monotonically
increasing with the number of communications p, which can demonstrate the power of the proposed
communications. When P → ∞, the partitions can reach Ω(a∕∣d∣), which is the same as the
generalization bound in expectation.
Remark 2. In the main text of this paper, we only give the optimal rates of DKRR-RF in the basic
setting. The fast learning rates can be achieved under favorable conditions, see in Appendix.
Remark 3 (The Significance of Distributed Learning for RF). At first glance, it seems that the
bottleneck in learning with random Fourier features is not the size of the dataset but the number
of features. HoWeverfrom (Rudi & Rosasco, 2017), one can see that we only requiring O( ∖∕∖D∣)
random features to guarantee the optimal performance, so the total computational complexity is
O(M3 + M 2∣D∣) = O(∣D∣2). Thus, the computational bottleneck in learning is not only the size
of random features, but also the size of dataset. Ifwe don’t consider reducing the size ofD, the
computational complexity is ∣D∣2 in the basic setting, which is not suitable for large scale problems.
Distributed learning is one of the most popular methods to reduce the size of dataset. The distributed
learning bring the distributed error, but can decrease the variance of the model Zhang et al. (2013;
2015). Thus, how to choose an appropriate number of partitions to trade off the distributed error and
the variance to guarantee the optimal performance is a very interesting and significant direction.
4.3 Compared with the Related Work
Comparisons of the Time and Space Complexities
Table 1 reports the statistical and computational properties of the related approaches and our the-
oretical findings under the basic setting. We see that our DKRR-RF can guarantee the optimal
generalization performance in expectation only requiring ∣D∣ memory and ∣D∣1.5 time, which is more
effective than other methods. For DKRR-RF-CM, we can also see that it can guarantee optimal
generalization performance in probability requiring less complexity than the communication-based
method of DKRR-CM (Lin et al., 2020).
Remark 4. In (Rudi et al., 2017), the authors considered combining the NyStrOm method and
preconditioned conjugate gradient (PCG) (Cutajar et al., 2016) to scale up KRR. As far as we know,
it is the only existing work that can guarantee optimal statistical accuracy, only requiring ∣D∣ memory
and ∣D∣1.5 time for KRR. In this paper, we consider combining distributed learning and random
features, a completely different path from (Rudi et al., 2017). Note that in our proposed method, we
need to compute the inverse of ΦM,Dj ΦTM,D + λI, which requires ∣D∣1.5 time. Inspired by (Rudi
et al., 2017), we can also adopt PCG to avoid the inverse calculation, which can further speed up
our proposed DKRR-RF-CM. The combination of DKRR-RF-CM and PCG may open a path to reach
the linear time complexity for optimal learning rate.
6
Published as a conference paper at ICLR 2021
Novelty and Proof Techniques
The most related works of our paper are (Li et al., 2019b), (Lin et al., 2020) and (Rudi & Rosasco,
2017). We discuss the novel techniques adopted to derive the improved results compared with them.
Compared with (Li et al., 2019b). (a) To derive the learning bounds, QM,D := k(CM +
λI)-1/2(CM — CMD)(Cm + λI)-1∕2∣∣ is required to be estimated, where CM and CMD are
self-adjoint and positive operators defined in Definition 1 (see in Appendix). In (Li et al., 2019b), they
used a classical approach from (Chang et al., 2017b; Guo et al., 2017) to estimate QM,D (see Lemmas
21 and 22 in (Li et al., 2019b)), and obtain that Qm,d ≤ 'II(Cm 一 Cm,d)(Cm + λI)-1/21∣ =
θ(l∕λ∣D∣ + PN(λ)∕λ∣D∣), whereN(λ) is the effective dimension defined in Assumption 1 (see
in Appendix). However, in our paper, we directly estimate QM,D based on the concentration in-
equality for self-adjoint operators (Rudi & Rosasco, 2017; Lin & Cevher, 2020; 2018; Caponnetto &
Yao, 2006), and prove that Qm,d = θ(l∕(λ∣D∣) 十 ,1∕(λ∣D∣)) (see in Proposition 6). Thus, our
estimation of QM,D is y/N(λ) tighter than that in (Li et al., 2019b). This is the one of the key
reasons why we can substantially relax the restriction on the number of local machines compared
with (Li et al., 2019b); (b) We not only present the bounds in expectation, but also in probability. To
derive the tight bound in probability, we provide new decompositions OfkfM ° 入 一 ∕m,d,λ∣ and
∣wM D λ 一 WM,D,λk,please see Proposition 9 in detail. As far as we know, these decompositions are
novel; (c) We also consider the Newton Raphson iteration-based communication strategy. To derive
the improved high-probability bounds with communication, we introduce a novel decomposition of
kfM,D,λ — fM,D,λkρ (see in Proposition 10).
Compared with (Lin et al., 2020). (a) In (Lin et al., 2020), they also considered a communication
strategy to enlarge the number of local machines. At first it seems that it only need to communicate
the gradient information, but it should be noted that the gradient information is based on an operator
representation (see Eq.(7) in (Lin et al., 2020)), which is usually infeasible in practice. The authors
present a realization for the proposed strategy by communicating the data among each local machine,
see in Appendix B (page 34 in (Lin et al., 2020), step 1). Thus, the data privacy of each local
machine is difficult to be protected. Furthermore, since it requires communicating data Dj , j =
1, . . . , m, among each local machine, for each iteration, the communication complexity of each
local machine is O(|D|d), which is too high for large scale data sets. However, the communication
strategy proposed in this paper only requires communicating the gradient gM,Dj,λ(WM-D λ) and
the model parameters βjt-1, rather than the data, therefore our proposed strategy do better on
privacy protection. Moreover, the communication complexity is only O(M) for each local machine,
M |D|, which is suitable for large scale data sets; (b) At first it seems that the proof techniques
of (Lin et al., 2020) can be easily extended to our paper, but it is not true. If we use the same
proof technical of (Lin et al., 2020), we can only obtain that kfM ,D,λ 一 fM,λkρ = k(LM,D +
λI厂I(LM,D — LM)(fρ 一 fM,λ)k = O((1/(√λ∖D∖) + PN(1”1DI)kfρ 一 fM,λkρ/√λ), where
fM ,D,λ, fM,λ and fρ are defined in Definition 12. Combing with Proposition 2, one can only obtain
that kfM,D,λ 一 fM,λkρ = θ((l∕λ∖D∖ + PN (λ)∕λ∖D∖)kfρ 一 fM,λkρ). However, in our paper, we
introduce new decompositions of ∣∣fM -d,x 一 fM,λkρ and kfM,°,λ 一 fM,0,λkρ (See Propositions 2
and 3 in detail), and further obtain that kfM,D,λ 一 fM,λkρ = θ((l∕(√λ∖D∖) + PN(λ)∕∖D∖) ∣fρ —
fM,λkρ) (see in Proposition 4), which is Ω(1∕√λ) tighter than the directly use of the techniques of
(Lin et al., 2020). The novel decompositions ∣∣fM,D,λ - fM,λ∣ρ and ∣∣fM,D,λ 一 fM,D,λkρ are the
key reasons why we can guarantee the optimal performance even under m = Ω(P∖D∖). We only
give an example here, but the novel decompositions have also been embedded in Propositions 5, 9,
10, etc.
Compared with (Rudi & Rosasco, 2017). (a) We study the statistical properties of the combination
of distributed learning and random features, but in (Rudi & Rosasco, 2017), they only consider random
features. As mentioned above, to estimate the tight bound of the distributed error, we introduce
a novel decomposition OfkfM d λ - fM,D,λk to derive the tight bound in expectation, and novel
decompositions of kfM,D,λ - fM,D,λk and kfM,D,λ - fM,D,λk to derive tight bounds in probability;
(b) The combination not only brings distributed error, but also brings some other problems. If you
compare the proofs of (Rudi & Rosasco, 2017) with these of our paper in detail, you can find that the
decompositions of (Rudi & Rosasco, 2017) and ours are very different.
7
Published as a conference paper at ICLR 2021
(a) simulated data, q = 2, t = 6
Figure 1: The mean square error or error rate on the test set with different partitions on KRR,
DKRR-RF and our DKRR-RF-CM. # represents the number of communications.
(b) simulated data, q = 2, t = 5
Overall, we improve the existing state-of-art bounds in expectation, and provide novel communication-
based distributed bounds with RF in probability. Moreover, we introduce some novel techniques
and decompositions to substantially relax the restriction on the number of local machines, which are
non-trivial extensions of (Li et al., 2019b; Lin et al., 2020; Rudi & Rosasco, 2017).
5	Experiments
In this section, we validate our theoretical findings by performing experiments on both simulated and
real datasets.
Numerical Experiments. Inspired by numerical experiments in (Rudi & Rosasco, 2017; Li et al.,
2019e), we consider a spline kernel of order q: K2q(x, x0) = 1 + Pk∞=1 cos(2πk(x - x0))/(k2q). If
the marginal distribution of X is uniform on [0,1], then K2q(x, x0) = R01 ψ(x, ω)ψ(x, ω)%(ω)dω,
where ψ(x, ω) = Kq(x, ω) and %(ω) is also uniform on [0,1]. The random features of the spline
kernel are Φm(x) = (ψ(x, ωι),..., ψ(x, ωM))T∕√M. According to Theorem 1, 2 and 3, We set
the size of the random features to be M = a∕∣D∣, and fine tune λ around |D|-1/2 using 5-fold cross
validation1, the tuned set is {2-5,2-3,... 25}|D|-1/2. We let the target function f be a Gaussian
random variable with mean μ = Kt(x, 0) and variance σ2 = 0.01.
We generate 10000 samples for training and 10000 samples for testing. We use the exact KRR as a
baseline, which trains all samples in a batch. We compare our proposed DKRR-RF-CM (p = 2, 4, 8)
with KRR and DKRR-RF. We repeat the training 5 times and estimate the averaged error on testing
data. The mean square error on the test set with different partitions is given in Figure 1(a, b), which
can be summarized as follows: 1) When m is not too large, the distributed methods (DRKK-RF
and DRKK-RF-CM) are always comparable to original KRR. There exists an upper bound of m,
when larger than it, the error increases dramatically and is far from the original KRR. This verifies
the theoretical statement in Theorem 1, 2 and 3; 2) The upper bound m of DKRR-RF-CM is much
larger than DKRR-RF. This result is aligned with Theorem 3, which demonstrates that the proposed
communication strategy can enlarge the upper bound m; 3) The upper bound m of DKRR-RF-CM
monotonically increases with the number of communications, which verifies Theorem 3.
Real Data. In this experiment, we consider the performance on real data. We use 6 publicly available
datasets from LIBSVM Data2. The empirical evaluations with Gaussian kernel, exp(-∣∣x - x[∣2∕σ),
are given in Figure 2, where the optimal σ and λ are selected by 5-fold cross-validation, σ ∈ {2i, i =
-10, -8,..., 10}, {2-5,2-3,... 25}|D|-1/2, and the number of random features is 2P∣D∣.
1Because of the selection of the optimal λ using the 5-fold cross validation, the computational complexity
should be enlarged. However, it should be noted that even for the plain methods, tuning the optimal λ is also
required, which will enlarge the computational complexity as well.
2http:〃www.csie.ntu.edu.tw/〜CjlinZlibsvm.
8
Published as a conference paper at ICLR 2021
0.0210
mπist
0.0205
0.0200
0.0195
0.0190
0.0185
0
5
20
25
10	15
N Of Blocks
Figure 2: The mean square error or error rate on the test set with different partitions DKRR-RF and
our DKRR-RF-CM on minist, a8a, a6a, space-ca, cpusmall and abalone. # represents the number of
communications.
From Figure 2, one can find that: 1) our DKRR-RF-CM are better than the original DKRR-RF on all
data-sets; 2) the larger the iterations of the communication, the better the performance. The above
results demonstrate that our communication-based DKRR-RF is effective.
6	Conclusion
In this paper, we study the generalization properties of the combination of distributed learning and
random features for ridge regression. We first improve the existing results of divide-and-conquer KRR
with random features in expectation. Then, beyond the expectation, we derive generalization error
bound in probability. Finally, we propose a novel effective communication strategy to further improve
the learning performance of the combination method, and demonstrate the power of communications
via both theoretical assessments and numerical experiments. Our results may open several venues
for both theoretical and empirical work: (a) combine the approach with gradient algorithms such as
preconditioned conjugate gradient Avron et al. (2017a) and multi-pass SGD (Carratino et al., 2018;
Lin & Cevher, 2018; 2020); (b) replace synchronous distributed methods with asynchronous ones
(Suresh et al., 2017); (c) consider the loss functions other than quadratic loss (Li et al., 2019e).
Acknowledgment
This work was supported in part by the National Natural Science Foundation of China NO. 62076234
and the Beijing Outstanding Young Scientist Program NO. BJJWZYJH012019100020098.
9
Published as a conference paper at ICLR 2021
References
Ahmed Alaoui and Michael Mahoney. Fast randomized kernel ridge regression with statistical
guarantees. In Advances in Neural Information Processing Systems (NeurIPS), pp. 775-783, 2015.
Miguel Arcones. A Bernstein-type inequality for U-statistics and U-processes. Statistics & probability
letters, 22(3):239-247, 1995.
Haim Avron, Kenneth Clarkson, and David Woodruff. Faster kernel ridge regression using sketching
and preconditioning. SIAM Journal on Matrix Analysis and Applications, 38(4):1116-1138, 2017a.
Haim Avron, Michael Kapralov, Cameron Musco, Christopher Musco, Ameya Velingker, and Amir
Zandieh. Random fourier features for kernel ridge regression: Approximation bounds and statistical
guarantees. In Proceedings of the 34th International Conference on Machine Learning (ICML), pp.
253-262, 2017b.
Francis Bach. Sharp analysis of low-rank kernel matrix approximations. In Proceedings of the 26th
Annual Conference on Learning Theory Conference on Learning Theory (COLT), pp. 185-209,
2013.
Francis Bach. On the equivalence between kernel quadrature rules and random fekernel expansions.
Journal of Machine Learning Research, 21:1-38, 2017.
Gilles Blanchard and Nicole Kramer. Optimal learning rates for kernel conjugate gradient regression.
In Advances in Neural Information Processing Systems (NeurIPS), pp. 226-234, 2010.
Gilles Blanchard and Nicole Kramer. Convergence rates of kernel conjugate gradient for random
design regression. Analysis and Applications, 14(6):763-794, 2016.
Gilles Blanchard and Nicole MUcke. Parallelizing spectrally regularized kernel algorithms. Journal
of Machine Learning Research, 19(30):1-29, 2018.
Raffaello Camoriano, Tomas Angles, Alessandro Rudi, and Lorenzo Rosasco. Nytro: When sub-
sampling meets early stopping. In Proceedings of the 19th International Conference on Artificial
Intelligence and Statistics (AISTATS), pp. 1403-1411, 2016.
Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm.
Foundations of Computational Mathematics, 7(3):331-368, 2007.
Andrea Caponnetto and Yuan Yao. Adaptation for regularization operators in learning theory.
Technical report, Technical Report CBCL Paper# 265/AI Technical Report, 2006.
Luigi Carratino, Alessandro Rudi, and Lorenzo Rosasco. Learning with SGD and random features.
In Advances in Neural Information Processing Systems (NeurIPS), pp. 10192-10203, 2018.
Xiangyu Chang, Shao-Bo Lin, and Yao Wang. Divide and conquer local average regression. Electronic
Journal of Statistics, 11(1):1326-1350, 2017a.
Xiangyu Chang, Shao-Bo Lin, and Ding-Xuan Zhou. Distributed semi-supervised learning with
kernel ridge regression. The Journal of Machine Learning Research, 18(1):1493-1514, 2017b.
Corinna Cortes, Mehryar Mohri, and Ameet Talwalkar. On the impact of kernel approximation on
learning accuracy. In Proceedings of the 13th International Conference on Artificial Intelligence
and Statistics (AISTATS), pp. 113-120, 2010.
Kurt Cutajar, Michael Osborne, John Cunningham, and Maurizio Filippone. Preconditioning kernel
matrices. In Proceedings of the 33rd International Conference on Machine Learning (ICML), pp.
2529-2538, 2016.
Lizhong Ding and Shizhong Liao. NyStrOm approximate model selection for LSSVM. In Advances
in Knowledge Discovery and Data Mining - 16th Pacific-Asia Conference (PAKDD), pp. 282-293,
2012.
10
Published as a conference paper at ICLR 2021
Lizhong Ding, Shizhong Liao, Yong Liu, Li Liu, Fan Zhu, Yazhou Yao, Ling Shao, and Xin Gao.
Approximate kernel selection via matrix approximation. IEEE Transactions on Neural Networks
and Learning Systems, pp. Online, DOI: 10.1109/TNNLS.2019.2958922, 2020.
Petros Drineas and Michael W Mahoney. On the Nystrom method for approximating a Gram matrix
for improved kernel-based learning. Journal ofMachine Learning Research, 6:2153-2175, 2005.
Jianqing Fan, Yongyi Guo, and Kaizheng Wang. Communication-efficient accurate statistical
estimation. arXiv preprint arXiv:1906.04870, 2019.
Zheng-Chu Guo, Shao-Bo Lin, and Ding-Xuan Zhou. Learning theory of distributed spectral
algorithms. Inverse Problems, 33(7):074009, 2017.
Cho-Jui Hsieh, Si Si, and Inderjit Dhillon. A divide-and-conquer solver for kernel support vector
machines. In Proceedings of the 31st International Conference on Machine Learning (ICML), pp.
566-574, 2014.
Daniel Hsu, Sham M Kakade, and Tong Zhang. Random design analysis of ridge regression. In
Conference on learning theory, pp. 9-1, 2012.
Arthur Jacot, Berfin Simsek, Francesco Spadaro, Clement Hongler, and Franck Gabriel. Implicit
regularization of random feature models. In Proceedings of the 37th International Conference on
Machine Learning (ICML), 2020.
Galyna Kriukova, Sergiy Pereverzyev, and Pavlo Tkachenko. Nystrom type subsampling analyzed as
a regularized projection. Inverse Problems, 33(7):074001, 2017.
Boyue Li, Shicong Cen, Yuxin Chen, and Yuejie Chi. Communication-efficient distributed optimiza-
tion in networks with gradient tracking. arXiv preprint arXiv:1909.05844, 2019a.
Boyue Li, Shicong Cen, Yuxin Chen, and Yuejie Chi. Communication-efficient distributed optimiza-
tion in networks with gradient tracking and variance reduction. In International Conference on
Artificial Intelligence and Statistics (AISTATS), pp. 1662-1672, 2020.
Jian Li, Yong Liu, Rong Yin, Hua Zhang, Li zhong Ding, and Weiping Wang. Multi-Class learning:
From theory to algorithm. In Advances in Neural Information Processing Systems (NeurIPS), pp.
1593-1602, 2018.
Jian Li, Yong Liu, and Weiping Wang. Distributed learning with random features. arXiv preprint
arXiv:1906.03155, 2019b.
Jian Li, Yong Liu, Rong Yin, and Weiping Wang. Multi-class learning using unlabeled samples:
Theory and algorithm. In Proceedings of the 28th International Joint Conference on Artificial
Intelligence (IJCAI), pp. 2880-2886, 2019c.
Jian Li, Yong Liu, Rong Yin, and Weiping Wang. Approximate manifold regularization: scalable
algorithm and generalization analysis. In Proceedings of the 28th International Joint Conference
on Artificial Intelligence (IJCAI), pp. 2887-2893, 2019d.
Zhu Li, Jean-Franois Ton, Dino Oglic, and Dino Sejdinovic. Towards a unified analysis of random
fourier features. In Proceedings of the 36th International Conference on Machine Learning (ICML),
pp. 3905-3914, 2019e.
Junhong Lin and Volkan Cevher. Optimal distributed learning with multi-pass stochastic gradient
methods. In Proceedings of the 35th International Conference on Machine Learning (ICML),
number 3092-3101, 2018.
Junhong Lin and Volkan Cevher. Optimal convergence for distributed learning with stochastic
gradient methods and spectral algorithms. Journal of Machine Learning Research, 21(147):1-63,
2020.
Junhong Lin and Lorenzo Rosasco. Optimal rates for multi-pass stochastic gradient methods. The
Journal of Machine Learning Research, 18(1):3375-3421, 2017.
11
Published as a conference paper at ICLR 2021
Shao-Bo Lin and Ding-Xuan Zhou. Distributed kernel-based gradient descent algorithms. Construc-
tive Approximation, 47(2):249-276, 2018.
Shao-Bo Lin, Xin Guo, and Ding-Xuan Zhou. Distributed learning with regularized least squares.
The Journal of Machine Learning Research, 18(1):3202-3232, 2017.
Shao-Bo Lin, Di Wang, and Ding-Xuan Zhou. Distributed kernel ridge regression with communica-
tions. arXiv preprint arXiv:2003.12210, 2020.
Fanghui Liu, Xiaolin Huang, Yudong Chen, and Johan AK Suykens. Random features for kernel
approximation: A survey in algorithms, theory, and beyond. arXiv preprint arXiv:2004.11154,
2020a.
Yong Liu and Shizhong Liao. Eigenvalues ratio for kernel selection of kernel methods. In Proceedings
of the 29th AAAI Conference on Artificial Intelligence (AAAI 2015), pp. 2814-2820, 2015.
Yong Liu, Shali Jiang, and Shizhong Liao. Eigenvalues perturbation of integral operator for kernel se-
lection. In Proceedings of the 22nd ACM International Conference on Information and Knowledge
Management (CIKM), pp. 2189-2198, 2013.
Yong Liu, Shali Jiang, and Shizhong Liao. Efficient approximation of cross-Validation for kernel
methods using Bouligand influence function. In Proceedings of the 31st International Conference
on Machine Learning (ICML), pp. 324-332, 2014.
Yong Liu, Shizhong Liao, Hailun Lin, Yinliang Yue, and Weiping Wang. Infinite kernel learning:
Generalization bounds and algorithms. In Proceedings of the 31st AAAI Conference on Artificial
Intelligence (AAAI), pp. 2280-2286, 2017.
Yong Liu, Hailun Lin, Li-Zhong Ding, Weiping Wan, and Shizhong Liao. Fast cross-Validation.
In Proceedings of the 27th International Joint Conference on Artificial Intelligence (IJCAI), pp.
2497-2503, 2018.
Yong Liu, Shizhong Liao, Shali Jiang, Lizhong Ding, Hailun Lin, and Weiping Wang. Fast cross-
Validation for kernel-based algorithms. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 42(5):1083-1096, 2020b.
Mona Meister and Ingo Steinwart. Optimal learning rates for localized svms. The Journal of Machine
Learning Research, 17(1):6722-6765, 2016.
Cameron Musco and Christopher Musco. Recursive sampling for the Nystrom method. In Advances
in Neural Information Processing Systems (NeurIPS), pp. 3833-3845, 2017.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in
Neural Information Processing Systems (NeurIPS), pp. 1177-1184, 2007.
Ali Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: Replacing minimization
with randomization in learning. In Advances in Neural Information Processing Systems (NeurIPS),
pp. 1313-1320, 2008.
Dominic Richards, Patrick Rebeschini, and Lorenzo Rosasco. Decentralised learning with random
features and distributed gradient descent. In Proceedings of the 37th International Conference on
Machine Learning (ICML), 2020.
Lorenzo Rosasco, Mikhail Belkin, and Ernesto De Vito. On learning with integral operators. Journal
of Machine Learning Research, 11:905-934, 2010.
Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random features.
In Advances in Neural Information Processing Systems (NeurIPS), pp. 3215-3225, 2017.
Alessandro Rudi, Guillermo D Canas, and Lorenzo Rosasco. On the sample complexity of subspace
learning. In Advances in Neural Information Processing Systems, pp. 2067-2075, 2013.
Alessandro Rudi, Raffaello Camoriano, and Lorenzo Rosasco. Less is more: Nystrom computational
regularization. In Advances in Neural Information Processing Systems (NeurIPS), pp. 1648-1656,
2015.
12
Published as a conference paper at ICLR 2021
Alessandro Rudi, Luigi Carratino, and Lorenzo Rosasco. Falkon: An optimal large scale kernel
method. In Advances in Neural Information Processing Systems (NeurIPS),pp. 3888-3898, 2017.
Alessandro Rudi, Daniele Calandriello, Luigi Carratino, and Lorenzo Rosasco. On fast leverage score
sampling and optimal learning. In Advances in Neural Information Processing Systems (NeurIPS),
pp. 5672-5682, 2018.
John Shawe-Taylor and Nello Cristianini. An Introduction to Support Vector Machines and other
Kernel-based Learning Methods. Cambridge University Press, 2000. ISBN 0521780195.
Si Si, Cho-Jui Hsieh, and Inderjit Dhillon. Computationally efficient nystrom approximation using
fast transforms. In Proceedings of the 33rd International Conference on Machine Learning (ICML),
pp. 2655-2663, 2016.
Steve Smale and Ding-Xuan Zhou. Shannon sampling ii: connections to learning theory. Applied
and Computational Harmonic Analysis, 19(3):285-302, 2005.
Steve Smale and Ding-Xuan Zhou. Learning theory estimates via integral operators and their
approximations. Constructive Approximation, 26(2):153-172, 2007.
Bharath Sriperumbudur and Zoltan Szabo. Optimal rates for random fourier features. In Advances in
Neural Information Processing Systems, pp. 1144-1152, 2015.
Ingo Steinwart and Andreas Christmann. Support Vector Machines. Springer Verlag, New York,
2008.
Yitong Sun, Anna Gilbert, and Ambuj Tewari. But how does it work in theory? linear SVM
with random features. In Advances in Neural Information Processing Systems (NeurIPS), pp.
3379-3388, 2018.
Ananda Theertha Suresh, Felix Yu, Sanjiv Kumar, and Brendan McMahan. Distributed mean
estimation with limited communication. In Proceedings of the 34th International Conference on
Machine Learning (ICML), pp. 3329-3337, 2017.
Dougal J. Sutherland and Jeff Schneider. On the error of random fourier features. In Proceedings of
the Conference on Uncertainty in Artificial Intelligence (UAI), pp. 862871, 2015.
Vladimir Vapnik. The Nature of Statistical Learning Theory. Springer Verlag, 2000.
Tianbao Yang, Yu-Feng Li, Mehrdad Mahdavi, Rong Jin, and Zhi-Hua Zhou. Nystrom method
vs random Fourier features: A theoretical and empirical comparison. In Advances in Neural
Information Processing Systems (NeurIPS), pp. 476-484, 2012.
Rong Yin, Yong Liu, Lijing Lu, Weiping Wang, and Dan Meng. Divide-and-conquer learning with
Nystrom: optimal rate and algorithm. In Proceedings ofthe 34th AAAI Conference on Artificial
Intelligence (AAAI), pp. 6696-6703, 2020.
Felix Xinnan X Yu, Ananda Theertha Suresh, Krzysztof M Choromanski, Daniel N Holtmann-Rice,
and Sanjiv Kumar. Orthogonal random features. In Advances in Neural Information Processing
Systems (NeurIPS), pp. 1975-1983, 2016.
Yuchen Zhang, John Duchi, and Martin Wainwright. Divide and conquer kernel ridge regression. In
Proceedings of the 26th Annual Conference on Learning Theory (COLT), pp. 592-617, 2013.
Yuchen Zhang, John Duchi, and Martin Wainwright. Divide and conquer kernel ridge regression: A
distributed algorithm with minimax optimal rates. The Journal of Machine Learning Research, 16
(1):3299-3340, 2015.
13
Published as a conference paper at ICLR 2021
A Appendix: Fast Learning Rates
In the section, we will show that the fast learning rates can be achieved under favorable conditions.
Let
L2ρX = f : X → R	f2 (x)dρX ≤ ∞
be the square integrable space, k ∙ ∣∣p be its norm. Denote the integral operator (Smale & Zhou, 2007)
LK by
LKf
X K(x,
∙)f(x)dρχ, ∀f ∈
L2ρX.
Assumption 1. For λ > 0, N(λ) is the effective dimension of the integral operator LK defined as
N(λ) = Tr (LK + λI)-1 LK , where Tr is the trace. Assuming there exists a constant c ≥ 1, such
that
N(λ)≤cλ-γ,γ∈ [0, 1].
(7)
The effective dimension is a common assumption within the framework of learning theory (Caponnetto
& Vito, 2007; Smale & Zhou, 2005; Rudi & Rosasco, 2017), which is used to measure the complexity
of the hypothesis space. It is always satisfied for γ = 1 and c = κ. Equation 7 can control the
variance of the estimator and is equivalent to the classic entropy and covering number conditions.
In particular, it holds if the eigenvalues of integral operator LK decay as i-1/Y, which is satisfied
by the popular Gaussian and polynomial kernel functions. More details can be seen in (Steinwart &
Christmann, 2008; Caponnetto & Vito, 2007; Rudi & Rosasco, 2017).
Assumption 2. Let fρ(x) = JY ydρ(y∣x) be the regression function, ρ(y∣x) be the Conditional
distribution at X induced by P. For 2 ≤ r ≤ 1, assume there exists a g ∈ LPX such that
fρ(x) = LrK g(x),	(8)
where LrK is the rth power of LK.
Regression function fρ is the best function in L2ρ , which is the primary objective in regression
problem. Assumption 2 is used to measure the complexity of the regression function fρ, which
is commonly used in approximation theory (Caponnetto & Vito, 2007). Equation 8 can be used
to control the bias of the estimator, it requires the expansion of the regression function fρ having
coefficients that decay faster than the eigenvalues of integral operator LK. The larger the value of r,
the faster the coefficients decay. The case r = 1/2 means that fρ ∈ HK. More detail can be seen in
(Rudi & Rosasco, 2017; Smale & Zhou, 2007).
Theorem4.	Suppose ψ is continuous, such that ∣ψ(x, ω)∣ ≤ T almostsurely, T ∈ [1, ∞) and |y| ≤ Z.
Under Assumptions 1-2 with r ∈ [1/2,1], Y ∈ [0,1], if λ = Ω(∣D∣-2r+), ∣Dι | = ... |Dm|,
m. |D|
2r+γ-1
2r+γ
and M & |D|
1+γ(2r-1)
2r+γ
(9)
then, for every δ ∈ (0, 1], with probability at least 1 - δ, we have
E [kfM,D,λ - fρkp] = O (|D|-等Y log2 δ)
The bound above is the same as the original KRR estimator and is optimal in a minimax sense
(Caponnetto & Vito, 2007; Lin et al., 2017; Chang et al., 2017b). In the best case, when r = 1 and
γ = 0, the rate O(1∕∣D∣) can be achieved by Ω(a∕∣D∣) random features and Ω(a∕∣D∣) partitions. In
the worst case, that is r = 1/2 and γ = 1, which has been covered in Theorem 1.
Theorem 5.	Suppose ψ is continuous, such that ∣ψ(x, ω)∣ ≤ T almost surely, T ∈ [1, ∞) and ∣y∣ ≤ ζ.
Under Assumptions 1-2 with r ∈ [1/2,1], Y ∈ [0,1], if λ = Ω(∣D∣-2r+), ∣Dι | = ... |Dm|,
m
2r+γ-1
.∣d∣ 4r+2γ
and M & ∣D∣
1+γ(2r-1)
2r+γ
(10)
then, for every δ ∈ (0, 1], with probability at least 1 - δ, we have
kfM,D,λ-fρk2 = O (∣d∣-洋 log21).
14
Published as a conference paper at ICLR 2021
2r+γ — 1
One Can See that the upper bound of m is |D| 4r+2γ , which is stricter than that of Theorem 4. In the
best case, when r = 1 and Y = 0, the rate O(1/|D|) can be achieved considering Ω(a∕∣D∣) random
features and Ω(∣D∣1/4) partitions. For the basic setting, that is r = 1/2 and Y = 1, which has been
given in Theorem 2.
Theorem6.	Suppose ψ is continuous, such that ∣ψ(x, ω)∣ ≤ T almostsurely, T ∈ [1, ∞) and |y| ≤ C
Under Assumptions 1-2 with r ∈ [1/2,1], Y ∈ [0,1], if λ = Ω(∣D∣-2r+γ ), ∣Dι | = ... |Dm|,
(2r+γ-1)(p+1)	, 1+γ(2r-1)	1
m . |D| (2r+γ)(p+2)	and M & |D|	2r+γ	log -,
(11)
then, for any δ ∈ (0, 1], with probability at least 1 - δ,
kfM,D,λ - fpkp = o(∣d∣-2r+γ lOgp+2 1
One can see that the communication can relax the restriction on the number of partitions. As p → ∞,
2r+γ — 1
the partitions can reach Ω(∣D∣ 2r+γ ), which is the same as Theorem 4.
Remark 5. In this paper, we focus on enlarging the number of local machines. In (Rudi & Rosasco,
2017; Rudi et al., 2018), they have proved that when generating random features in a data-dependent
manner, fewer random features are required to obtain optimal learning. Thus, if we adopt the
data-dependent manner to generate random features, we can further improve the performance of
DKRR-RF-CM with fewer random features.
B Appendix: Notation and Preliminary
In this paper we denote the operator norm by k ∙ k, and the square integrable norm by ∣∣∙∣∣p.
Definition 1.
SM : RM → L2ρX, (SMw)(X)= hw, φM (X)i,
SM : LPX → RM, SMg = φ Φm(χ)g(χ)dρχ(χ),
X
SM,d : Lpx → RM, SM,dg
CM : RM → RM, CM
=焉 X φM (Xj )g(xj ),
|D| xj∈DX
φM (X)φM (X)T dρX (X),
X
CMD : RM → RM, CM,D = ^D^ X φM (Xj )φM (Xj )T.
|D| xj∈DX
Lemma 1. CM and CM,D are self-adjoint and positive operators, with spectrum is [0, T2]. Moreover
we have CM = SMSM and Cmd = Φm,dΦM,d = SM,dSM.
Proof. CM and CM,D are self-adjoint and positive operators, with spectrum is [0, T2], and
CM = SM SM, CMD = Φm,D ΦM,D
can be directly obtained from Caponnetto & Vito (2007); Smale & Zhou (2005; 2007); Rosasco et al.
(2010); Rudi & Rosasco (2017); Lin et al. (2020).
In the following, we prove that Cmd = SM D SM. From the definitions of SM, SM D and Cmd in
Definition 1, we have
∀β ∈ RM, (SMβ)(∙) = hβ, Φm(∙)i = Φm(∙)Tβ,
and thus we can obtain that,
SM,DSMβ = |D| X φM (Xj )φM (Xj )Te = CM,Dβ
|D| xj∈DX
So, the Equation Cmd = SM DSM holds.
□
15
Published as a conference paper at ICLR 2021
Definition 2.
fM,D,λ = wM,D,λΦM (∙), WM,D,λ = arg min ∖ ɪ X (wtΦm (Xi) - y，2 + λ∣∣wk2 J>;
w∈RM	|D| zi∈D
fM,D,λ = WM,D,λφM(∙), WM,D,λ = arg min ||DD| X (wTφM(Xi) - fP(Xi))2 + λkwk2}；
fM,λ = wM λΦM(∙), WM,λ = argmin{/ (wtΦm(x) - fρ(x))2 dρx(x) + λ∣∣wk2 ；;
w∈RM	X
fλ = arg min	(f (x) - fρ(x))2 dρX (x) + λkf k2K .
f∈HK	X
(12)
Remark 6 (From Caponnetto & Vito (2007); Smale & Zhou (2007); Li et al. (2019b); Lin et al.
(2020)).
fM,D,λ = SMWM,D,λ, WM,D,λ = (CM,D + λI)-1Φm,dND；
fM,D,λ = SMwM,D,λ, wM,D,λ = (CM,D + λI)TSM,D fP;	(13)
fM,λ = SMWM,λ, WM,λ = (CM + λI)-1SMfρ.
Definition 3. The maximum random feature dimension is denoted as
N∞(λ) = SUp k(Lκ + λI)-"2ψ(∙, ω)k2,λ > 0.
ω∈Ω
Remark 7 (From Rudi & Rosasco (2017)). N∞(λ) ≤ τ2λ-1 is always satisfied for every λ > 0.
C Appendix: Proof of Theorem 4
C.1 Appendix: Error decomposition for DKRR-RF in Expectation
Proposition 1. Let fM D 人,and fM D 人,fM,λ, fλ be defined by 3 and 12, respectively. Then, we
have
E [kfM,D,λ - fρkp] ≤ 3 [kfM,λ - fλ∣∣p] + 3 [llfλ - fρ∣∣p]
+ 3 X ʌDj∣2E [kfM,Dj,λ - fM,λkp] + 3 X ^Dj∙EhkfM,Dj,λ - fM,λk2i .
Proof. Let fMg λ, and /Md λ, fM,λ be defined by 3 and 12, respectively. According to the
Proposition 5 of (Chang et al., 2017b) or Lemma 20 of (Li et al., 2019b), we have
E [kfM,D,λ- fM,λkp]
≤ X ]dD∣2 E [kfM,Dj,λ - fM,λkp] + X ʌDj∙E [kfM,D,λ - fM,λkp].	()
Note that (a + b + c)2 ≤ 3a2 + 3b2 + 3c2, ∀a, b, c ≥ 0. Thus, we have
E [kfM,D,λ- fPkP] = E [kfM,D,λ- fM,λ + fM,λ - fλ + fλ - fρkp]
≤ 3E [kfM,D,λ - fM,λkp] + 3 [kfM,λ - fλkp] + 3 [kfλ - fρkp].
Combining the above inequality and Eq. 14, we can prove this proposition.	□
Proposition 2. The follows hold:
VλkWM,D,λ - wM ,D,λk ≤ JM,D (RM,D + KM,D )
and
kfM,D,λ - fM ,D,λ kP ≤ JM2 ,D (RM,D + KM,D).
where Rm,D ：= Il(CM + λI)-1∕2(Φm,dNd - SMfρ)∣∣, Jm,d ：= ∣∣(Cm,d + λI)-1/2(CM +
λI)1/2k, Km,d := ∣(Cm + λi)-1/2(SMfρ- SM,Dfρ)k.
16
Published as a conference paper at ICLR 2021
Proof. From 13, We know that WMDλ = (Cm,D + λI)-1Φm,dSD and WM”入=(CMD +
λI)-1SMD fρ, so we have
wM,D,λ - wM ,D,λ = (CM,d + λl )-1(φM,D yD — SM ,D fP)
=(Cm,d + λI)-1/2(Cm,d + λI)T/2(CM + λI)1/2(Cm + λI)-1∕2(Φm,dND - SMDfρ)∙
(15)
Note (Cm,d + λI)-1/2 is a self-adjoint and positive operator, so ∣∣(Cm,d + λI)-1∕2∣∣
thus, we can obtain that
≤
∣∣wM,D,λ - wM ,D,λl ≤ √λ jM,D Il(CM + λI 厂1/2®M,D yD - SM ,D fP)I
=√λ Jm,D Il(CM + λI厂1/2(^M,DyD - SM,dfP + SM,dfP - SM,dfP)
≤	D (Rm,D + Km,D).
1∕√λ,
(16)
Note that fM,D,λ - fM,D,λ = SM(wm,d,λ - WM,d,λ), by 15, we have
fM,D,λ - fM,D,λ = SM(WM,D,λ - wM,D,λ)
=SM (CM + λI)-1/2(CM + λI)1/2(CM,d + λI)-1/2(CM,d + λI)-1/2(CM + λI)1/2	(17)
(Cm + λI)T∕2(Φm,dNd - SM,dfP + SM,dfp - SM,dfP》
Note that
ISM (Cm + λI )T∕2∣∣ = Il(CM + λI )-1/2SM SM (Cm + λI 厂1/2|1/2
=I (Cm + λI )-1/2Cm (Cm + λI )-1/2|1/2 ≤ 1.
So, by Eq 17, we have
∣∣fM,D,λ - fM,D,λkP ≤ JMM,d (RM,d + kM,D).
□
Proposition 3. Thefollows hold:
√λkwM ,D,λ - wM,λ∣∣ ≤ ∣∣fM,λ - fPkP + jM,D ∣∣fM,λ - fPkP
and
kfM,D,λ - fM,λ∣∣ρ ≤ (JM,d + JM,d)kfM,λ - fρ∣∣ρ,
where jm,D := ∣∣(cM,D + λI厂1/2 (CM + λI)1/2||.
Proof. By Remark 6, we have
wM,D,λ - wM,λ = (cM,D + λI)-1SM,dfP - (CM + λI)-1SMfP
=(Cm,d + λI)-1[SM,dfρ - SMfρ] + [(Cm,d + λI)-1 - (Cm + λI)-1]SMfρ.
Note that for any self-adjoint and positive operators A and B,
AT - B-1 = A-1(B - A)B-1,A-1 - B-1 = B-1 (B - A)A-1,	(18)
so we have
wM,D,λ - wM,λ
= (CM,D + λI) 1[SM,dfP - SMfP] + (CM,D + λI) I(CM - cM,D)wM,λ
From Lemma 1, we know that CM = SMSM and Cm,d = Φm,dΦM D = SM DSM, thus we can
obtain that
wM,D,λ - wM,λ
=(Cm,d + λI)-1[SM,Dfρ - SMfρ] + (Cm,d + λI)-1(SMSMwm,Λ - SM,dSMwm,λ)
= (CM,D + λI) 1[SM,DfP - SM,dSMwM,λ] + (CM,D + λI) 1[SMSMwM,λ - SMfP] (19)
= (Cm,D + λI)-1[SM,Dfρ - SM,dfM,λ] + (Cm,d + λI)-1[SMfM,λ - SMfρ]
= (cM,D + λI)-1SM,d [fP - fM,λ] + (CM,d + λI)-1SM [fM,λ - fP].
17
Published as a conference paper at ICLR 2021
Thus, we have
kwM,D,λ - wM,λk ≤ (Il(CM,D + λI厂ISM,D k + Il(CM,D + λI厂ISM II) IlfM,λ - fρkρ. QO)
Note that
∣∣(Cm,d + λI)-1∕2SMDk ≤ ∣∣(Cm,d + λI)-1/2Cm,d(Cm,d + λI)-1/211/2 ≤ 1
and
∣(Cm,d + λi )-1∕2SM k =∣(Cm,d + λi )-1/2(CM + λi )1/2(CM + λi )-1/2SM k
≤JM,D Il(CM + λI 厂1/2SM Il ≤ JM,D,
since ∣∣(Cm + λI)-1/2SMk = ∣∣(Cm + λI)-1/2CM(CM + λI)-1/211/2 ≤ 1. Substituting the
above two inequalities into Eq. 20, we have
IIwM ,D,λ - wM,λ∣∣ ≤ √λ (1 + JM,D )kfM,λ - fρ∣∣ρ,
which prove the first result of Proposition 3.
In the following, we will prove the second result. By Eq. 19, we have
fM ,D,λ - fM,λ = SM (wM,D,λ - wM,λ)
=SM (CM,D + λI)-1SM,D [fρ - fM,λ] + SM (CM,D + λI)-1SM [fM,λ - fρ]
=SM (CM + λI)-1/2 (CM + λI)1/2 (CM,D + λI)-1/2 (CM,D + λI)-1/2SM,D [fρ - fM,λ]
+ Sm (Cm + λI )-1/2(CM + λI )1/2(Cm,d + λI 厂 1∕2(Cm,d + λI )-1/2 (CM + λI )1/2
(CM + λI)-1/2SM [fM,λ - fρ]
Note that IISM(CM + λI)-1/2 Il = Il (CM + λI)-1/2CM(CM + λI)-1/2111/2 ≤ 1, k (CM,D +
λI)-1/2SM,Dk	=	∣(Cm,d	+ λI)-1/2Cm,d(Cm,d	+ λI)-1/2k1/2	≤	1,	and	∣∣(Cm	+
λI )-1/2SM k = ∣(Cm + λI )-1/2 Cm (Cm + λI )-1/2k1/2 ≤ 1, sowehave
kfM ,D,λ - fM,λkρ ≤ (JM,D + JM2 ,D)kfM,λ - fρkρ.	(21)
□
Proposition 4. The follows hold:
Vλ∣WM,D,λ - WM,λk ≤ JM,D(Rm,D + Km,D) + (1 + JM,D)∣fM,λ - fρkρ,
and
kfM,D,λ - fM,λkρ ≤ JM2 ,D(RM,D + KM,D) + (JM,D + JM2 ,D)kfM,λ - fρkρ.
where RM,D := Il(CM + λI)-1∕2(φM,DyD - SMfρ)∣∣, JM,D := ||(CM,D + λI)-1/2(CM +
λi)1/2k, Km,d := k(CM + λi)-1/2(SMfρ - SM,Dfρ)k∙
Proof. Note that
kwM,D,λ - wM,λk ≤ kwM,D,λ - wM ,D,λ k + kwM ,D,λ - wM,λk
and
kfM,D,λ - fM,λkρ ≤ kfM,D,λ - fM ,D,λkρ + kfM ,D,λ - fM,λkρ.
Combining Propositions 2 and 3, We can prove this result.	□
18
Published as a conference paper at ICLR 2021
C.2 Proof of Theorem 4
Lemma 2 (Lemma 23 in (Li et al., 2019b), can be also seen in (Rudi & Rosasco, 2017)). For
δ ∈ (0, 1] and λ > 0, when
M ≥ Ω ((¥ J' 1 (∙N∞(λ)l°g 1J 2r∨ (N∞(λ))lθg ^ .
Then, with probability at least 1 - δ, we have
kfM,λ - fλ kρ2 ≤ cλ2r ,
where c is a constant.
Lemma 3 (Theorem 4 in (Smale & Zhou, 2005)). Under Assumption 2, for r ∈ [1/2, 1], we have
kfλ - fρk2ρ ≤ cλ2r,
where c is a constant.
Lemma 4 (Lemma 6 in (Rudi & Rosasco, 2017)). Forδ ∈ (0, 1], with probability at least 1 - δ, we
have
Rm,D = II(CM + λi)T∕2 (Φm,dDyD - SMfρ)∣∣ = O
ɪ INM(λ) V	1
+ V ∏DΓ) log δ
where NM (λ) := Tr (LM + λI)-1 LM , LM is the integral operator associated with the approxi-
mate kernel function KM, (LM f)(x) = X KM (x, x0)f (x0)dρX (x0).
Lemma 5 (Proposition 10 in (Rudi & Rosasco, 2017)). For any δ ∈ (0,1], M ≥ Ω (N∞(λ) log =)，
then with probability at least 1 - δ,
|NM (λ)-N(λ)∣≤ 1.55N (λ),
(22)
where NM(λ) := Tr ((LM + λI[\Lm).
Lemma 6 (Lemma E.2, (Blanchard & Kramer, 2010)). For any self-adjoint and positive Semidefinite
operators A and B, if there exists η > 0 such that the following inequality holds
k(A + λI)-1∕2(B - A)(A + λI)-1∕2k ≤ 1 - η,
then
k(A+ λI)1∕2(B+ λI)-1∕2k ≤
1
√η
Proposition 5. For δ ∈ (0, 1], with probability at least 1 - δ, we have
Km,d ：= k(CM + λI)T∕2(SMfρ- SMDfρ)k ≤ 2：；Iogj + 2Z[/NMP,
3|D| λ	|D|
where NM(λ) := Tr ((LM + λI)-1Lm).
The proof closely follows the proof of Lemma 6 in (Rudi & Rosasco, 2017).
Proof. Define μi = (CM + λI)1/2SMfρ -(CM + λI)1∕2Φm(xi)fρ(xi). Note that
1	|D|
(CM + λI)1/2 (SM fρ- SM,Dfρ)=回 X μi.
Since μι,..., μ∣D∣ are independent and identically distributed random vector, and
Eμi = / (CM + λI)1∕2Φm(x)fρ(x)dρχ - I (CM + λIW2Φm(Xi)fρ(xi)dρχ = 0.
19
Published as a conference paper at ICLR 2021
To apply the Bernstein inequality (Arcones, 1995; Rudi & Rosasco, 2017) for random vectors, we
need to bound their moments. Note that
Il(CM + λIW2φM(X)fρ(X)Il ≤ ~7=
and
Eμ2 ≤ 2
I(CM +λI)
X
1/20M (X) k2kfρ(X) kpdρχ (X)
≤ 2ζ2
X
∣∣(Cm + λI)1∕2Φm(X)k2dρχ(x) ≤ 2Z2Nm(λ).
Thus, using the Bernstein inequality (Arcones, 1995; Rudi & Rosasco, 2017), for any δ ∈ (0, 1], with
1 - δ, we have
k(CM + λI厂1/2(SMfρ- SM,dfρ)k ≤ ；∖θgδ +2qINM：，.
3|D| λ	|D|
□
Lemma 7 (Lemma 2 in (Smale & Zhou, 2007)). Let H be a Hilbert space and ξ be a random
variable on Z, ρ with values in H. Assume IξI ≤ M < ∞ almost surely. Denote σ(ξ) = E(ξ2 ). Let
{zi}in=1 be independent random drawers of ρ. For any 0 < δ < 1, with confidence 1 - δ,
≤ 2M log(2∕δ) J /
n
2kσ(ξ)k2 log(2∕δ)
n
Proposition 6. For any δ > 0, with probability at least 1 - δ, we have
Qm,D ：= II(CM + λI)-1/2(Cm 一 Cm,D)(Cm + λ)-"2k
≤
2log2(2∕δ)(N∞(λ) + 1)
D
/2iog(2∕δ)(N∞(λ) + l)
V D
and
LM,D ：= I(CM + λI)-1(CM - CM,D)I
≤
2log2(2∕δ)(N∞(λ) + 1)
D
/2log(2∕δ)(N∞(λ) + 1)
V D
where N∞(λ) = suPω∈Ω k(Lκ + λI)-1/2ψ(∙, ω)∣p, ci and c2 are two constants.
To prove Proposition 6, we first prove the following lemma (a similar technique can be found in (Hsu
et al., 2012; Rudi et al., 2013; Caponnetto & Yao, 2006; Rudi & Rosasco, 2017)).
The similar result for the matrix case was first proved in (Hsu et al., 2012), and later was extended to
the operator case in (Rudi et al., 2013; Rudi & Rosasco, 2017).
Lemma 8. Let ζ1 , . . . , ζn with n ≥ 1, be i.i.d random vectors on a separable Hilbert spaces
H such that H = EZ 0 Z is a trace class, and for any λ there exists N∞(λ) < ∞ such that
hZ,(H + λI)-1Zi ≤ N∞(λ). Denote Hn as n Pn=I Zi0 Zi . Then for any δ ≥ 0, with probability
at least 1 - 2δ, the following holds
II(H + λI)-1/2(H -Hn)(H + λ)-1∕2Il ≤ 2log2(20(N∞㈤ + 1)+ r2log(20(N∞(λ) + 1).
Proof. Let H = H + λI, η = H-1/2HH-1/2, ξi = η 一 H-1/2Zi 0 H；'1/Zi. One can see that
Eξi = 0. Note that
kη 一 H-1/2Zi 0 H-1/2Zik ≤ kηk + hZi,H-1^2Zi〉≤ 1+ N∞(λ),
20
Published as a conference paper at ICLR 2021
and
kE[ξ2]k = IIE hhZi,H-1/2ZiiH-1/2Zi 乳 H-1/2Zii - H-2H2∣∣
≤N∞(λ) ||e[h-1/2Zi 乳 H—1/2Zi]|| + ∣∣h-2h 2II
≤N∞(λ)∣∣H-"Hk + ∣∣H-2H 2∣∣
≤ N∞(λ) + 1.
Thus, substituting the above two inequalities to Lemma 7 (Lemma 2 in (Smale & Zhou, 2007)), which
finishes the proof.	口
Proof of Proposition 6. Since CM is self-adjoint operator, so we have
k(CM + λI)-1(Cm - CMD)k = k(CM + λITI2(Cm - CMDMCM + λI)-1/2||.
According to Lemma 8 with Ui = Φm (xi), We can obtain this result.	口
Proposition 7. If |D| ≥ 32log(2∕δ)(1+ N∞(λ)), thenforany δ > 0, with probability at least 1 一 δ,
we have
Jm,d ：= ∣∣(Cm,d + λI)t/2(Cm + λI)1/2k ≤ √2.
Proof. From Proposition 6, we know that if |D| ≥ 32log(2∕δ)(1+ N∞(λ)), then
Il(CM + λI )-1/2 (Cm,D 一 CM )(Cm + λ)T∕2k ≤ 2.
Combining the above inequality and Lemma 6, we can prove this result.
□
Proposition 8. If δ ∈ (0,1], and |D| ≥ Ω(N∞(λ)), then with 1 — δ, we have
kfM,D,λ 一 fM,λkρ = O (γM,D,λ log δ + kfM,λ 一 fλkρ + kfλ - fρkρ),
where γM,D,λ ：= √⅛+VZNDF.
Proof. From Proposition 4, we have
kfM,D,λ 一 fM,λ kρ ≤ JM2 ,D(RM,D + KM,D) + (JM,D + JM2 ,D)kfM,λ 一 fρkρ.
Thus, from Lemmas 4, 5, and Propositions 5, 7, we know that if |D| ≥ Ω(N∞(λ)), we can prove this
result.	口
Proof of Theorem 4. According to Proposition 1, we have
E [kfM,D,λ — fρkp] ≤ 3 [kfM,λ — fλkp] + 3 [kfλ — fρkp]
+ 3 X ʌDj2^E [kfM,Dj,λ - fM,λkp] + 3 X ^DjpEhkfM,Dj,λ - fM,λk2i .
Substituting Lemmas 2,3, Proposition 3, 7, 8 into the above inequality, one can see that if
M ≥ Ω (("；]?)N∞(λ)2-2r ∨N∞(λζj , and |Dj| ≥ Ω(N∞(λ)),
with confidence 1 一 δ, we have
E [kfM ,D,λ - fρkP] = O 卜r + X 震(yM,dq log2 ɪ) + X 1D λ),	(23)
21
Published as a conference paper at ICLR 2021
where YMDj,λ = √χ^ + JNλ)』setting |Di| = ... = ∖Dm∖, λ = Ω(∣D∣-2r+γ), we have
γMDj,λ = O (Nj+ + jλ) = O (√mlDl-鼻 + m1D1-).
(24)
/	2r+γ-1 ∖	1
Note that if m ≤ Ω (∣D∣ 2r+γ ) and ∣Dι ∣ = ... = ∣Dm∣, and λ = Ω(∣D∣-2r+γ), we have
∣巧∣ = E ≥ Ω (∣D∣$)=Ω(N∞(λ)).
Thus, substituting 24 into 23, one can see that if m ≤ Ω
(∣D∣ U )
and
M ≥ Ω ((NHy ) N∞(λ)2-2r ∨N∞(λ)) = Ω (∣D∣5F产),
with probability at least 1 - δ, we have
E [IIfM ,D,λ - ∕ρ∣∣p] = O (∣D∣-洋 log21 + ∣D∣-1 log21 + ∣D
=O(IDI-2⅛ log21).
2r
2r+γ
D Appendix： Proof of Theorem 5
D.1 Appendix： error Decomposition for DKRR-RF in Probability
Proposition 9. Thefollows hold:
□
II.fM ,D,λ
MM,D,λ - wM,D,λ
XID⅛,D (QM,D
+ QM,Dj)∣∣WM,Dj,λ - WM,λ∣∣ and
-fM,D,λk ≤
+ Qm,Dj)
(IlfM,Dj,λ - fM,λkρ + √λkwM,Dj,λ - wM,λ∣∣
where JM,D := ∣∣(CM,D + λI厂1/2(CM + λI)1/2|| and QM,D := Il(CM + λI厂1/2(CM -
Cm,d )(Cm + λ)-"∣∣∙
22
Published as a conference paper at ICLR 2021
Proof. Note that wm,d,x = (Cm,D + λI)-1Φm,dND, thus We have
W M ,D,λ — wM,D,λ
口 IDjI”	、一丁	“	、一丁
E曷 (CM,Dj + λI )-1⅛M,Dj YDj — (Cm,D + λI )-1Φm,D ND
j=1 i i
:X ʌDjɪ ((CM,Dj + λI)-1 — (cM,D + λI)-1) φM,DjyDj
X ηDjτ(Cm,d + λI)-1 (Cm,d — CMDj) (Cm,Dj + λI)-1ΦmdYDj
j=1 1	1
口 IDj I”	、一…	…	、
:ɪ2 -jDT(Cm,D + λI)	(Cm,D — Cm,Dj) Wm,Dj,λ
j=1 i i
X ηDjΓ-(Cm,D + λI)-1 (Cm,D — CM) WM,Dj,λ
j=1 i i
m
+X
j
m
X
j=1
DI (Cm,D + λI)-1 (CM — Cm,Dj ) WM,Dj,λ
IDj∙ J	、…	「、/	、
I^^ I-(Cm,d + λI)	(Cm,d—CM )(wM,Dj,λ — WM,λ)
m
+X
j=1
m
+X
j
m
X
j=1
|Dj I
|D|
IDj I
IDI
(Cm,D + λI )-1 (Cm,d —CM) WM,λ
(Cm,D + λI)-1 (Cm — Cm,dJ wm,dj,λ
ID I
I^^ I-(Cm,d + λI)- (Cm,d—CM )(wM,Dj,λ — WM,λ)
(25)
1
1
m IDj I	-1
+	^[dT (Cm,D + λI)	(CM — Cm,Dj ) (wM,Dj,λ — W M,λ).
j=1 i i
Note that
X四
M IDI
=X叫
=M IDI
and X IDj ।
and M IDI
=X叫
=M IDI
(CM,D + λI I)T(CM,D—cm )(wM,Dj,λ — wM,λ)
(Cm,D + λI )-1(Cm + λI )(Cm + λI )-1 (Cm,D — CM )(wM,Dj,λ — WM,λ)
(Cm,d + λI)-1 (Cm — Cm,Dj) (wM,Dj,λ — WM,λ)
(Cm,d + λI)-1(Cm + λI)(Cm + λI)-1 (Cm — Cm,Dj) (wm,dq — wm,λ)∙
Substituting the above equations into Eq. 25, We have
m IDj I
kwM,D,λ — wM,D,λ∣∣ ≤	IDpJM,D(LM,d
+ LM,Dj)∣∣wM,Dj,λ — wM,λ∣∣,
23
Published as a conference paper at ICLR 2021
where LM,D := k(CM + λI)-1(CM - CM,D)k. From Proposition 6, we know that LM,D = QM,D,
so we have
m
kwM,D,λ - wM,D,λ
j=1
DI
|D|
M,D + QM,Dj)kwM,Dj,λ - wM,λk,
which prove the first result of this proposition.
In the following, we will prove the second result of this proposition. Note that SM (WM,D,λ 一
wM,D,λ) = fM,D,λ — fM,D,λ. According to 25, we have
fM,D,λ - fM,D,λ = X ^DjpSM(CM,D + λI)-1 (CM,D - CM) (wM,Dj,λ - wM,λ)
m |Dj |
+ ∑jwSM(Cm,d + λI)-1 (Cm - Cm,Dj) (wM,Dj,λ - wM,λ) (26)
j=1 |D|
:=X 胃(~1+~2).
Note that
~1 =SM (CM + λI )-1/2 (CM + λI )1/2(CM,d + λI )-1/2(CM,d + λI )-1/2(CM + λI )1/2
(CM + λI厂1/2 (CM,D - CM) (CM + λI厂1/2	(27)
(CM + λI )-1/2(Cm + λI )(wM,Dj ,λ - wM,λ),
thus we have
k~1kρ ≤ JM D QM,D IlSM (CM + λI )-1/2Ilk(CM + λI )-1/2(CM + λI )(wM,Dj,λ - wM,λ)k
≤ JM D Qm,D ∣∣(Cm + λI )-1/2(Cm + λI )(wM,Dj ,λ - WM,λ )k.
(28)
Since ∣∣Sm(CM + λI)-1/2k = ∣(Cm + λI)-1/2Cm(CM + λI)-1/2k1/2 ≤ 1. So, we have
k~1kρ ≤ JM ,D Qm,D ∣∣(Cm + λI )-1/2(Cm + λI )(wM,Dj ,λ - wM,λ)k
=JM D Qm,D ∣∣(Cm + λI )-1/2(SM SM + λI )(wM,Dj ,λ - wM,λ)k
≤ JM D Qm,D ∣∣(Cm + λI )-1/2SM SM (wM,Dj,λ — wM,λ)k
+ λJM ,D Qm,D ∣∣(Cm + λI 厂“2(wM,Dj,λ - wM,λ)k
≤ JM ,D qM,D Il(CM + λI )-1/2SM kkfM,Dj ,λ - fM,D,λkρ
+ VλJMM ,D Qmd k(wM,Dj ,λ - wM,λ)k
≤ JM,DqM,D(kfM,Dj,λ - fM,D,λkρ + √λkwM,Dj,λ - wM,λk),
the last inequality uses the fact that
∣∣(Cm + λi)-1/2SMk = ∣(Cm + λI)-1/2SMSM(CM + λI)T∕2k1∕2 ≤ 1.
Similar as the above process, we can also obtain that
k~2kρ ≤ JM D QM,Dj (kfM,Dj ,λ - fM,D,λkρ - √λkwM,Dj ,λ - wM,λk).
Thus, using 26, we can prove this result.
□
D.2 Appendix: Proof of Theorem 5
Proof of Theorem 5. Combining Proposition 9 and Proposition 4, we have
kfM,D,λ - fM,D,λkρ ≤ X ^^j^JM,D (QM,D + QM,Dj)
(JM,Dj + JM2 ,Dj)(RM,Dj + KM,Dj) + (2JM,Dj + JM2 ,Dj + 1)kfM,λ - fρkρ .
(29)
24
Published as a conference paper at ICLR 2021
From Propositions 7 and 8, one can see that if |Dj| ≥ Ω(N∞(λ)), and λ ≤ ∣∣Lk∣∣, then for any
δ > 0, with probability at least 1 - δ,
IfM,D,λ - fM,D,λ∣∣ρ
=o (X ^DI (QM,d + QM,Dj) (γM,Dj,λ log δ + llfM,λ - fλ∣∣ρ + ∣∣fλ - fρ∣∣ρ)),
where Ym,dɪ,λ = √λD∖ + JNDλ)∙ Note that Qm,d ≤ Qm,d,∙, and by Lemmas 2 and 3, so we
have
II.fM ,D,λ - fM,D,λ∣∣ρ = O (X ʃjDIQM,DjγM,Dj,λ log $ + λr QM,Dj ).
Note that
II.fM,D,λ - fPkP = IlfM,D,λ - fM,D,λ + fM,D,λ - fM,λ + fM,λ - fλ + fλ - fρ∣∣ρ
≤kfM,D,λ - fM,D,λkρ + IlfM,D,λ - fM,λkρ + IlfM,λ - fλ∣∣ρ + ∣∣∕λ - f ∣∣ρ∙
(30)
(31)
Combining E.q 30, Lemmas 2, 3, Proposition 8 and E.q 31, one can see that if M ≥
Ω (("：；r-P)) N∞(λ)2-2r ∨N∞(λ)) , with probability 1 - δ, we have
IfM,D,λ - fρ!ρ = o (X ^DIQM,Dj γM,Dj,λ log δ + γM,D,λ log δ + λr QM,Dj + λr ).
(32)
-
|D(
Ω
=
λ
,
|
+ |D|-44++γ-1) = O (|D|一令),
(33)
/ I 	r	4r+2γ-1 ∖
YMQ,λ = O (而|。|-叩 + m|D|--r+2^τ)
2	2r+γ-1	__ 2r+γ-1
Qm,dj = O m|D|- 2r 十 γ + ʌ/m |D| - ^4r + 2Y
Thus, when m ≤ Ω
(|D|
2r+γ-1 ∖
"十2，	, We have
Ym,dqQm;Dj= O (|D|-2r+γ),
r	2r — 1+Y	r
Qm,d,λr = O (|D|-E |D|-ɪ+^) = O (|D|-E),
|D|	2r+γ + 1	1
|Dj| =* ≥ Ω (|D|ɪ+^) ≥ Ω (|D| W) = Ω(N∞(λ)).
Thus, we have fM,d,λ -加P = O (|D|-舁 log21) ,which prove this result.
□
E Appendix： Proof of Theorem 6
E.1 Appendix： error Decomposition for DKRR-RF-CM in Probability
Proposition 10.
kfM,D,λ - fM,D,λkρ
qM,D + 2JM,Dj QM,Dj
-fM,D,λ∣∣ρ +，X||WM,D,λ - wM,D,λ∣∣
where Jm,d ：= ∣∣(Cm,d + ʌʃS + λI)1/2|| and Qm,d ：= Il(CM + ʌʃS -
Cm,D )(Cm + λ)T∕2∣∣∙
25
Published as a conference paper at ICLR 2021
Proof. Note that
wM,D,λ = WM,D,λ - (CM,D + λI厂1 kCM,D + λI)wM-,D,λ - φM,Dy0],
DI
WM,D,λ = WM,D,λ - X ^∣D∣^(CM,Dj + λI)	[(CM,D + λI)wm,D6 - φM,Dy0]
Thus, We have
wM,D,λ - wM,D,λ
=WM,D,λ - (CM,D + λI)TkCM,D + λI)WM,D,λ - φM,DyD
-WM,D,λ + X ^Diɪ (CM,Dj + λI)TkCM,D + λI)WM,D6 - φM,DyD]
=X l∣Dj∣l [(CM,Dj + λI厂1 - (CM,D + λI厂 1] [(cM,D + λI)wM,D,λ - φM,DyD]
=X ʌDI(CM,Dj + λI厂1 [cm,D - cM,Dj] (CM,D + λI)-1 h(CM,D + λI)wM-,D,λ - φM,DyD]
=X ^DI (CM,Dj + λI厂1 [cM,D - CM,Dj ] [wM,D,λ - wM,D,λ]
=X ^D^p (CM,Dj + λI厂1 [CM,D - CM] [wM,D,λ - wM,D,λ]
+ X ^DI (CM,Dj + λI厂1 [cM - cM,Dj] [wM,D,λ - wM,D,λ]
:=X 博 N1 + n2.
& ∣D∣ 1	2
(34)
Note that
Sm N《=Sm (CM + λI L?(CM + λI >/2(Cm,d + λI )-1/2
(Cm,dj + λIL2(Cm + λI)1/2(Cm + λI)-1/2 [Cm,d - CM](Cm + λI)-1/2	(35)
(CM + λI)-1/2(CM + λI) (WM,D,λ - wM,D,λ).
Note that kSM(CM + λI)-1/2|| = Il(CM + λI)-1/2Cm(CM + λI)-1/2||1/2 ≤ 1, so, We have
IlSMN1∣∣ρ ≤ JM,dqM,D I(CM + λI厂1/2(CM + λI) (WM,D,λ - wM,D,λ) U .	(36)
Note that CM = SM SM, so
cM (WM,D,λ - wM,D,λ) = sMsM (WM,D,λ - wM,D,λ) = sM (fM,D,λ - fM,0,λ).
Substituting the above inequality into Eq. 36, We have
IIsMNIIlP ≤ JM,DqM,D U(CM + λI厂1/2SM (fM,D,λ - fM,D,λ) U
+ 入 JM,Dj qM,D U(CM + λI厂1/2 (WM,D,λ - wM,D,λ) U
≤ JM ,d∙Qm,d (∣∣∕M,D ,λ- fM,D,λ∣∣ρ+ √λ∣∣w M,D,λ- wm,d,λ ii),
26
Published as a conference paper at ICLR 2021
the last inequality use the fact that Il(CM + λI)-1/2SMIl = ∣∣(cm + λI)-1/2CM(cm +
λI)-1/2 Il1/2 ≤ 1. Using the same process, We can obtain that
IISMN2∣∣ρ ≤ JM,Dj QM,Dj (IlfM-D,λ - fM,D,λkρ + √λ∣∣wM,D,λ - wM,D,λ∣∣).
Thus, we have
IlfM,D,λ - -fM,D,λkρ =kSM(wM,D,λ - wM,D,λ)kρ ≤ X DIkSMNIkP + IlSM^2kρ
≤ X ^DI (Jf,Dj qM,D + jm,Dj QM,Dj )
(kfM-D,λ — fM,D,λ∣∣ρ + √λ∣∣w M,D,λ — wM,D,λ∣ι).
According to 34, we know that
wM,D,λ 一 wM,D,λ = X *(W + n2)
(37)
=X ʃ∣Djp (CM,Dj + λI厂1 [Cm,D - CM][WM,D,λ - wM,D,λ]
+ X ^DI (CM,Dj + λI厂1 [CM - CM,Dj ] [wM,D,λ - wM,D,λ]
=^X ʃ∣Djp(CM,Dj ÷ λI)-1(CM + λI)(CM + λI)-1 [CM,D — CM][wM,D,λ ― wM,D,λ]
+ ^X ʃ∣Djp (CM,Dj + λI)-1(CM + λI )(CM + λI)-1 [CM — CM,Dj ] IW M,D,λ ― wM,D,λ].
Thus, one can obtain that
m
∣∣wM,D,λ — W M,D,λk ≤ EJM ,Dj (LM,D + LM,Dj)∣∣w M,D,λ - wM,D,λ ∣∣,
j = 1
where Lm,D ：= ∣∣(cm + λI)-1(cm 一 cm,d) k. From Proposition 6, we know that Lm,d = Qm,d.
Thus, we have
m
∣∣wM,D,λ — W M,D,λk ≤ EJM ,Dj (QM,D + QM,Dj)∣∣w t7,D,λ — wM,D,λ∣∣.	(38)
j = 1
Combining 37 and 38, we have
IlfM,D,λ - fM,D,λkP + √λkwM,D,λ - WM,D,λk
≤ X ^DI JM ,Di qM,D + JM ,Dj QM,Dj) (IlfM-D ,λ — fM,D,λkρ + √λ∣∣w M,D,λ — wM,D,λ ιι)
+X* JM ,Dj (Qm,D + Qm,Dj ) √λ∣∣w M,D,λ — wM,D,λ∣∣
≤ X ^DI (2 JM ,Dj QM,Dj +2 jM ,Dj QM,Dj (Ilf-D,λ 一 fM,D,λ IlP + √λ∣∣w M,D,λ 一 wM,D,λ∣∣)
≤ (2 X ^DIJM ,Dj qM,D + JM ,Dj QM,Dj ) (IlfM ,D,λ — fM,D,λkρ + √λ∣∣w M ,D,λ — wM,D,λ ∣∣),
which prove the result.
□
27
Published as a conference paper at ICLR 2021
E.2 Appendix： Proof OF Theorem 6
Proof of Theorem 6. Substituting Propositions 4, 5, 7, 8 and Lemma 4 into Proposition 9, we have
∣fM,D,λ - fM,D,λ∣ρ + √λ∣∣wD,λ -WM,D,λ∣∣2
=O (X IDj (QM,D + QM,Dj) (RM,D + KM,D + ∣∣fM,λ - fρ∣IP))
=O (X ʃjDjp (QM,Dj + QMDj)(YM,Dj,λ + kfM,λ - fλ∣∣ + ∣∣fλ — fρ∣∣ρ) ) ∙
Combining the above inequality and Proposition 10, and note that Qm,d ≤ QM,Dj, We can obtain
that
∣fM,D,λ - fM,D,λ∣∣ρ
≤O ( (X ^DIQM,Dj ) (X ^DjɪQM,Dj (γM,Dj,λ + ∣∣fM,λ - fλ∣∣ + ∣∣fλ - fρ∣∣ρ) ) ) ∙
Note that
IfM,D,λ -	fρ∣ρ	= IIfM,D,λ -	fM,D,λ	+ fM,D,λ -	fM,λ +	fM,λ -	fλ	+	fλ	-	fρkρ	)
≤IfM,D,λ - fM,D,λ∣∣ρ + ∣∣fM,D,λ - fM,λ∣∣ρ + ∣∣fM,λ - fλ∣∣ρ + ∣∣fλ - f ∣∣ρ∙
Thus, by Lemmas 2, 3, one can see that if
M > Ω
衿 N∞(λ)2-2r vN∞(λ∕j,
we have
IfM,D,λ - fρkρ
=O ( (X 骼 Qm,d) (X 看 QM,Dj (γM,Dj,λ + λr)) + γM,D,λ log | + λr ) ∙
From 33, by setting ∣D11
∣Dm∣, λ = Ω(∣D∣-2r+γ), we know that
r I 	r	4r + 2γ-1
YM,Dj,λ = O (√m∣D∣-E + m∣D∣-τr+^
2	2r+γ-1
QMDj = O (m∣D∣― 2r+γ
__	_ 2r+γ-1
+ √m∣D∣- 4r+2γ
r	r	4r + 2γ-1
Ym,d,λ = O ∣D∣-W + ∣D∣-^r+^
(	(2r+γ-1)(t + 1)、	_	/	r 、
Thus, when m ≤ Ω (∣D∣ (2r+Y)(t+2) ), we have ∣∣fM d λ — fρkρ = O (∣D∣-2r+γ), which proves
the result.	口
F Appendix： Proof of Theorems 1, 2, 3
Proof. From (Smale & Zhou, 2007; Caponnetto & Vito, 2007), if r = 1/2, then fρ ∈ HK, Thus, in
this case, f∏κ exists and E(f∏κ) = E(fρ). Note that Assumption 1 is always satisfied for Y = 1
and C = T2. So, using Theorems 4, 5 and 6 with r = 1/2, Y = 1 and C = T2, Theorem 1, 2, and 3
can be proved.	口
28