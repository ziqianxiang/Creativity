Published as a conference paper at ICLR 2021
Gradient Descent on Neural Networks Typi-
cally Occurs at the Edge of Stability
Jeremy Cohen Simran Kaur Yuanzhi Li J. Zico Kolter1 and Ameet Talwalkar2
Carnegie Mellon University and: 1Bosch AI 2 Determined AI
Correspondence to: jeremycohen@cmu.edu
Ab stract
We empirically demonstrate that full-batch gradient descent on neural network
training objectives typically operates in a regime we call the Edge of Stability.
In this regime, the maximum eigenvalue of the training loss Hessian hovers just
above the value 2/(step size), and the training loss behaves non-monotonically
over short timescales, yet consistently decreases over long timescales. Since this
behavior is inconsistent with several widespread presumptions in the field of op-
timization, our findings raise questions as to whether these presumptions are rel-
evant to neural network training. We hope that our findings will inspire future
efforts aimed at rigorously understanding optimization at the Edge of Stability.
1	Introduction
Neural networks are almost never trained using (full-batch) gradient descent, even though gradient
descent is the conceptual basis for popular optimization algorithms such as SGD. In this paper, we
train neural networks using gradient descent, and find two surprises. First, while little is known
about the dynamics of neural network training in general, we find that in the special case of gradient
descent, there is a simple characterization that holds across a broad range of network architectures
and tasks. Second, this characterization is strongly at odds with prevailing beliefs in optimization.
In more detail, as we train neural networks using gradient descent with step size η, we measure the
evolution of the sharpness — the maximum eigenvalue of the training loss Hessian. Empirically,
the behavior of the sharpness is consistent across architectures and tasks: so long as the sharpness
is less than the value 2∕η, it tends to continually rise (§3.1). We call this phenomenon progressive
sharpening. The significance of the value 2/n is that gradient descent on quadratic objectives is
unstable if the sharpness exceeds this threshold (§2). Indeed, in neural network training, if the
sharpness ever crosses 2/n, gradient descent quickly becomes destabilized — that is, the iterates
start to oscillate with ever-increasing magnitude along the direction of greatest curvature. Yet once
ss-ush
Fully-connected net on CIFAR-IO 5k subset
ss-u"h
VGG on CIFAR-IO
ss-u"h
ResNet on CIFAR-IO
c 200 -
e-
S ιoo-
300
S 60 ^
40--
20-
6000
2000	4000
iteration
2000	4000	6000	8000
iteration
Figure 1: Gradient descent typically occurs at the Edge of Stability. On three separate architec-
tures, we run gradient descent at a range of step sizes η, and plot both the train loss (top row) and the
sharpness (bottom row). For each step size n, observe that the sharpness rises to 2/n (marked by the
horizontal dashed line of the appropriate color) and then hovers right at, or just above, this value.
1
Published as a conference paper at ICLR 2021
this happens, gradient descent does not diverge entirely or stall. Instead, it enters a new regime
we call the Edge of Stability1 (§3.2), in which (1) the sharpness hovers right at, or just above, the
value 2∕η; and (2) the train loss behaves non-monotonically, yet consistently decreases over long
timescales. In this regime, gradient descent is constantly “trying” to increase the sharpness, but is
constantly restrained from doing so. The net effect is that gradient descent continues to successfully
optimize the training objective, but in such a way as to avoid further increasing the sharpness.2
In principle, itis possible to run gradient descent at step sizes η so small that the sharpness never rises
to 2∕η. However, these step sizes are suboptimal from the point of view of training speed, sometimes
dramatically so. In particular, for standard architectures on the standard dataset CIFAR-10, such step
sizes are so small as to be completely unreasonable — at all reasonable step sizes, gradient descent
eventually enters the Edge of Stability (see §4). Thus, at least for standard networks on CIFAR-10,
the Edge of Stability regime should be viewed as the “rule,” not the “exception.”
As we describe in §5, the Edge of Stability regime is inconsistent with several pieces of conventional
wisdom in optimization theory: convergence analyses based on L-smoothness or monotone descent,
quadratic Taylor approximations as a model for local progress, and certain heuristics for step size
selection. We hope that our empirical findings will both nudge the optimization community away
from widespread presumptions that appear to be untrue in the case of neural network training, and
also point the way forward by identifying precise empirical phenomena suitable for further study.
Certain aspects of the Edge of Stability have been observed in previous empirical studies of full-
batch gradient descent (Xing et al., 2018; Wu et al., 2018); our paper provides a unified explanation
for these observations. Furthermore, JaStrzebSki et al. (2020) proposed a simplified model for the
evolution of the sharpness during stochastic gradient descent which matches our empirical observa-
tions in the special case of full-batch SGD (i.e. gradient descent). However, outside the full-batch
special case, there is no evidence that their model matches experiments with any degree of quanti-
tative precision, although their model does successfully predict the directional trend that large step
sizes and/or small batch sizes steer SGD into regions of low sharpness. We discuss SGD at greater
length in §6. To summarize, while the sharpness does not obey simple dynamics during SGD (as
it does during GD), there are indications that the “Edge of Stability” intuition might generalize
somehow to SGD, just in a way that does not center around the sharpness.
2	Background: S tab ility of Gradient Descent on Quadratics
In this section, we review the stability properties of gradient descent on quadratic functions. Later,
we will see that the stability of gradient descent on neural training objectives is partly well-modeled
by the stability of gradient descent on the quadratic Taylor approximation.
On a quadratic objective function f (x) = 1XT AX + bτX + c, gradient descent with step size η
will diverge if3 any eigenvalue of A exceeds the threshold 2∕η. To see why, consider first the one-
dimensional quadratic f (x) = 2ax2 + bx + c, with a > 0. This function has optimum x* = -b/a.
Consider running gradient descent with step size η starting from x0 . The update rule is xt+1 =
xt - η(axt + b), which means that the error xt - x* evolves as (xt+1 - x*) = (1 - ηa)(xt - x*).
Therefore, the error at step t is (xt - x*) = (1 - ηa)t(x0 - x*), and so the iterate at step t is
Xt = (1 - ηa)t (x0 — x*) + x*. If a > 2∕η, then (1 - ηa) < -1, so the sequence {xt} will oscillate
around x* with ever-increasing magnitude, and diverge.
Now consider the general d-dimensional case. Let (ai, qi) be the i-th largest eigenvalue/eigenvector
of A. As shown in Appendix B, when the gradient descent iterates {Xt} are expressed in the spe-
cial coordinate system whose axes are the eigenvectors of A, each coordinate evolves separately.
In particular, the coordinate for each eigenvector qi, namely hqi , Xti, evolves according to the dy-
namics of gradient descent on a one-dimensional quadratic objective with second derivative ai .
1This nomenclature was inspired by the title of Giladi et al. (2020).
2In the literature, the term “sharpness” has been used to refer to a variety of quantities, often connected to
generalization (e.g. Keskar et al. (2016)). In this paper, “sharpness” strictly means the maximum eigenvalue of
the training loss Hessian. We do not claim that this quantity has any connection to generalization.
3For convex quadratics, this is “if and only if.” However, if A has a negative eigenvalue, then gradient
descent with any (positive) step size will diverge along the corresponding eigenvector.
2
Published as a conference paper at ICLR 2021
Therefore, if ai > 2∕η, then the sequence {<qi, x∕}
will oscillate with ever-increasing magnitude; in this
case, we say that the iterates {xt} diverge along
the direction qi . To illustrate, Figure 2 shows a
quadratic function with eigenvalues a1 = 20 and
a2 = 1. In Figure 2(a), we run gradient descent
with step size η = 0.09; since 0 < a2 < ai < 2∕η,
gradient descent converges along both q1 and q2 .
In Figure 2(b), we use step size η = 0.11; since
0 < a2 < 2∕η < ai, gradient descent converges
along q2 yet diverges along q1 , so diverges overall.
Figure 2: Gradient descent on a quadratic
with eigenvalues ai = 20 and a2 = 1.
Polyak momentum (Polyak, 1964) and Nesterov momentum (Nesterov, 1983; Sutskever et al., 2013)
are notable variants of gradient descent which often improve the convergence speed. On quadratic
functions, these two algorithms also diverge if the sharpness exceeds a certain threshold, which we
call the “maximum stable sharpness,” or MSS. In particular, we prove in Appendix B that gradient
descent with step size η and momentum parameter β diverges if the sharpness exceeds:
MSSPolyakS, B) = 1(2 + 2β) ,	MSSNesteIUη,⑶=1 (1+1).
(1)
The Polyak result previously appeared in Goh (2017); the Nesterov one seems to be new. Note that
this discussion only applies to full-batch gradient descent. As we discuss in §6, several recent papers
have proposed stability analyses for SGD (WU et al., 2018; JaStrzebSki et al., 2020).
Neural network training objectives are not globally quadratic. However, the second-order Taylor
approximation around any point x0 in parameter space is a quadratic function whose “A” matrix is
the Hessian at x°. If any eigenvalue of this Hessian exceeds 2∕η, gradient descent with step size η
would diverge if run on this quadratic function — the iterates would oscillate with ever-increasing
magnitude along the corresponding eigenvector. Therefore, at any point x0 in parameter space where
the sharpness exceeds 2∕η, gradient descent with step size η would diverge if run on the quadratic
Taylor approximation to the training objective around x0 .
3	Gradient Descent on Neural Networks
In this section, we empirically characterize the behavior of gradient descent on neural network train-
ing objectives. Section 4 will show that this characterization holds broadly.
3.1	Progressive sharpening
When training neural networks, it seems to be a general rule that so long as the sharpness is small
enough for gradient descent to be stable (< 2∕η, for vanilla gradient descent), gradient descent
has an overwhelming tendency to continually increase the sharpness. We call this phenomenon
progressive sharpening. By “overwhelming tendency,” we mean that gradient descent can occasion-
ally decrease the sharpness (especially at the beginning of training), but these brief decreases always
seem be followed by a return to continual increase. JaStrzebSki et al. (2020) previously hypothesized
(in their Assumption 4) that a similar phenomenon may hold for SGD, but the evidence for, and the
precise scope of, this effect are both currently far clearer for gradient descent than for SGD.
(a)	Mean Squared Error (MSE) loss
(b)	Cross entropy loss
S 0-6
IΛ
—QA
"2 0.2
0.0
0	10000	20000
iteration
0	10000	20000
iteration
2.0
1.5-
1.0
0.5
0.0
0	2000	4000
iteration
250
S 200 -
Φ
C 150-
d
Jg IGO-
'S 50
0	2000	4000
iteration
Figure 3: So long as the sharpness is less than 2∕η, it tends to continually increase during
gradient descent. We train a network to completion (99% accuracy) using gradient descent with a
very small step size. We consider both MSE loss (left) and cross-entropy loss (right).
3
Published as a conference paper at ICLR 2021
Progressive sharpening is illustrated in Figure 3. Here, we use (full-batch) gradient descent to train
a network on a subset of 5,000 examples from CIFAR-10, and we monitor the evolution of the
sharpness during training. The network is a fully-connected architecture with two hidden layers
of width 200, and tanh activations. In Figure 3(a), we train using the mean squared error loss for
classification (Hui & Belkin, 2020), encoding the correct class with 1 and the other classes with
0. We use the small step size of η = 2/600, and stop when the training accuracy reaches 99%.
We plot both the train loss and the sharpness, with a horizontal dashed line marking the stability
threshold 2∕η. Observe that the sharpness continually rises during training (except for a brief dip at
the beginning). This is progressive sharpening. For this experiment, we intentionally chose a step
size η small enough that the sharpness remained beneath 2/n for the entire duration of training.
Cross-entropy. When training with cross-entropy loss, there is an exception to the rule that the
sharpness tends to continually increase: with cross-entropy loss, the sharpness typically drops at the
end of training. This behavior can be seen in Figure 3(b), where we train the same network using the
cross-entropy loss rather than MSE. This drop occurs because once most data points are classified
correctly, gradient descent tries to drive the cross-entropy loss to zero by scaling up the margins, as
detailed in Soudry et al. (2018). As we explain in Appendix C, this causes the sharpness to drop.
The effect of width. It is known that when networks parameterized in a certain way (the “NTK
parameterization”) are made infinitely wide, the Hessian moves a vanishingly small amount during
training (Jacot et al., 2018; Lee et al., 2019; Li & Liang, 2018), which implies that no progressive
sharpening occurs. In Appendix D, we experiment with networks of varying width, under both NTK
and standard parameterizations. We find that progressive sharpening occurs to a lesser degree as net-
works become increasingly wide. Nevertheless, our experiments in §4 demonstrate that progressive
sharpening occurs to a dramatic degree for standard architectures on the standard dataset CIFAR-10.
We do not know why progressive sharpening occurs, or whether “sharp” solutions differ in any
important way from “not sharp” solutions. These are important questions for future work. Note that
Mulayoff & Michaeli (2020) studied the latter question in the context of deep linear networks.
3.2	The Edge of Stability
In the preceding section, we ran gradient descent using step sizes η so small that the sharpness
never reached the stability threshold 2/n. In Figure 4(a), We start to train the same network at the
larger step size of n = 0.01, and pause training once the sharpness rises to 2/n = 200. Recall
from §2 that in any region where the sharpness exceeds 2/n, gradient descent with step size n
would be unstable if run on the quadratic Taylor approximation to the training objective — the
gradient descent iterates would oscillate with ever-increasing magnitude along the leading Hessian
eigenvector. Empirically, we find that gradient descent on the real neural training objective behaves
similarly — at first. Namely, let q1 be the leading Hessian eigenvector at the iteration where the
sharpness reaches 2/n. In Figure 4(b), we resume training the network, and we monitor both the
train loss and the quantity hq1, xti for the next 215 iterations. Observe that hq1, xti oscillates
with ever-increasing magnitude, similar to the divergent quadratic example in Figure 2(b). At first,
these oscillations are too small to affect the objective appreciably, and so the train loss continues to
monotonically decrease. But eventually, these oscillations grow big enough that the train loss spikes.
(a) train loss and sharpness
before sharpness crosses 2∕η
E o.5 -
0.0
steps Intotralnlng
(b} train loss and {qι,xt) for first
215 steps after sharpness crosses 2∕η
(c} train loss and (qι,xt} for first
1000 steps after sharpness crosses 2∣η
Figure 4: Once the sharpness crosses 2/n, gradient descent becomes destabilized. We run gradi-
ent descent at n = 0.01. (a) The sharpness eventually reaches 2/n. (b) Once the sharpness crosses
2/n, the iterates start to oscillate along q1 with ever-increasing magnitude. (c) Somehow, GD does
not diverge entirely; instead, the train loss continues to decrease, albeit non-monotonically.
4
Published as a conference paper at ICLR 2021
Once gradient descent becomes destabilized in this manner, classical optimization theory gives no
clues as to what will happen next. One might imagine that perhaps gradient descent might diverge
entirely, or that gradient descent might stall while failing to make progress, or that gradient descent
might jump to a flatter region and remain there. In reality, none of these outcomes occurs. In Figure
4(c), We plot both the train loss and hqι, Xti for 1000 iterations after the sharpness first crossed 2∕η.
Observe that gradient descent somehow avoids diverging entirely. Instead, after initially spiking
around iteration 215, the train loss continues to decrease, albeit non-monotonically.
This numerical example is representative. In general, after the sharpness initially crosses 2∕η, gradi-
ent descent enters a regime we call the Edge of Stability, in which (1) the sharpness hovers right at,
orjust above, the value 2/n; and (2) the train loss behaves non-monotonically over short timescales,
yet decreases consistently over long timescales. Indeed, in Figure 5, we run gradient descent at a
range of step sizes using both MSE and cross-entropy loss. The left plane plots the train loss curves,
with a vertical dotted line (of the appropriate color) marking the iteration where the sharpness first
crosses 2/n. Observe that the train loss decreases monotonically before this dotted line, but be-
haves non-monotonically afterwards. The middle plane plots the evolution of the sharpness, with a
horizontal dashed line (of the appropriate color) at the value 2/n. Observe that once the sharpness
reaches 2/n, it ceases to increase further, and instead hovers right at, or just above, the value 2/n
for the remainder of training. (The precise meaning of “just above” varies: in Figure 5, for MSE
loss, the sharpness hovers just a minuscule amount above 2/n, while for cross-entropy loss, the gap
between the sharpness and 2/n is small yet non-miniscule.)
At the Edge of Stability, gradient descent is “trying” to increase the sharpness further, but is being
restrained from doing so. To demonstrate this, in Figure 7, we train at step size 2/200 until reaching
the Edge of Stability, and then at iteration 6,000 (marked by the vertical black line), we drop the step
size to n = 2/300. Observe that after the learning rate drop, the sharpness immediately starts to in-
crease, and only stops increasing once gradient descent is back at the Edge of Stability. Appendix O
repeats this experiment on more architectures. Intuitively, gradient descent with fixed step sizes acts
like a constrained optimization algorithm: the use of step size n imposes an implicit 2/n constraint
on the sharpness (Nar & Sastry, 2018), and at the Edge of Stability this constraint is “active.”
Observe from Figure 5 that there do exist step sizes n (in purple) small enough that the sharpness
never rises to 2/n. We call such a step size stable. However, observe that with cross-entropy loss, it
takes 3700 iterations to train at the stable step size in purple, but only 1000 iterations to train at the
larger step size in blue. In general, we always observe that stable step sizes are suboptimal in terms
train loss
-64 2
Ooo
ssu
Wwo-sw
ιn 400-
M
(U
C
e-
2 200-
sharpness
SSgUReUS
400
200
sharpness (by time)
a = 2/500
η = 2/400
η = 2/300
η = 2/200
η = 2/100
Q 5 Q 5 C
2 1 1 O n
Sso- u
O IOOO 2000	3000
iteration
IOOO 2000	3000
iteration
20	40
time = iteration X η
60
Figure 5: After the sharpness reaches 2∕η, gradient descent enters the Edge of Stability. A
network is trained with gradient descent at a range of step sizes (see legend), using both MSE loss
(top row) and cross-entropy (bottom row). Left: the train loss curves, with a vertical dotted line at
the iteration where the sharpness first crosses 2∕η. Center: the sharpness, with a horizontal dashed
line at the value 2∕η. Right: sharpness plotted by time (= iteration X η) rather than iteration.
5
Published as a conference paper at ICLR 2021
O 2000	4000	6000	8∞0 IOOOO
iteration
Figure 6: Momentum. We run GD
with step size η = 0.01 and Polyak or
Nesterov momentum at various β . For
each algorithm, the horizontal dashed
line marks the MSS from Equation 1.
Figure 7: After a learning rate drop, progressive sharp-
ening resumes. We start training at η = 2/200 (orange)
and then after 6000 iterations (dotted vertical black line),
we cut the step size to η = 2/300 (green). Observe that as
soon as the step size is cut, the sharpness starts to rise.
of convergence speed. In fact, in §4 we will see that for standard networks on CIFAR-10, stable step
sizes are so suboptimally small that they are completely unreasonable.
The “Edge of Stability” effect generalizes to gradient descent with momentum. In Figure 6, we train
using gradient descent with step size η = 0.01, and varying amounts of either Polyak or Nesterov
momentum. Observe that in each case, the sharpness rises until reaching the MSS given by Equation
1, and then plateaus there. Appendix N has more momentum experiments.
In Appendix P, we briefly examine the evolution of the next few Hessian eigenvalues during gradient
descent. We find that each of these eigenvalues rises until PIateaUing near 2∕η.
Prior work. Aspects of the Edge of Stability have been observed previously in the literature. Wu
et al. (2018) noted that the sharPness at the solution reached by full-batch gradient descent was not
just less than 2∕η, as was expected due to stability considerations, but was mysteriously approx-
imately equal to 2∕η. In retrospect, We can attribute this observation to progressive sharpening.
Xing et al. (2018) observed that full-batch gradient descent eventually enters a regime (the Edge of
Stability) in which the training loss behaves non-monotonically, and the iterates oscillate along the
direction of largest curvature; however, they did not relate this regime to the sharpness. Lewkowycz
et al. (2020) found that in neural network training, if the sharpness at initialization is larger than
2∕η, then after becoming initially destabilized, gradient descent does not always diverge entirely (as
the quadratic Taylor approximation would suggest), but rather sometimes “catapults” into a flatter
region that is flat enough to stably accommodate the step size. It seems plausible that whichever
properties of neural training objectives permit this so-called “catapult” behavior may also be the
same properties that permit successful optimization at the Edge of Stability. Indeed, optimization
at the Edge of Stability can conceivably be viewed as a never-ending series of micro-catapults. As
we discuss at greater length in §6, several papers (JaStrzebSki et al., 2017; 2019) have observed that
large step sizes steer stochastic gradient descent into less sharp regions of the loss landscape, and
Jastrzebski et al. (2020) attributed this effect to the stability properties of SGD. Finally, our pre-
cise characterization of the behavior of the sharpness during full-batch gradient descent adds to a
growing body of work that empirically investigates the Hessian spectrum of neural networks (Sagun
et al., 2017; Ghorbani et al., 2019; Li et al., 2020a; Papyan, 2018; 2019; 2020).
3.3	The gradient flow trajectory
In the right pane of Figure 5, we plot the evolution of the sharpness during gradient descent, with
“time” = iteration × η, rather than iteration, on the x-axis. This allows us to directly compare the
sharpness after, say, 100 iterations at η = 0.01 to the sharpness after 50 iterations at η = 0.02;
both are time 1. Observe that when plotted by time, the sharpnesses for gradient descent at different
step sizes coincide until the time where each reaches 2∕η. This is because for this network, gradient
descent at η = 0.01 and gradient descent at η = 0.02 initially travel the same path (moving at a speed
proportional to η) until each reaches the point on that path where the sharpness hits 2∕η. This path
is the gradient flow trajectory. The gradient flow solution at time t is defined as the limit as η → 0
of the gradient descent iterate at iteration t/n (if this limit exists). The empirical finding of interest
is that for this particular network, gradient descent does not only track the gradient flow trajectory
in the limit of infinitesimally small step sizes, but for any step size that is less than 2/sharpness.
6
Published as a conference paper at ICLR 2021
We can numerically approximate gradient flow trajectories by using the Runge-Kutta RK4 algorithm
(Press et al., 1992) to numerically integrate the gradient flow ODE. Empirically, for many but not all
networks studied in this paper, we find that gradient descent at any step size η closely tracks the
RUnge-KUtta trajectory until reaching the point on that trajectory where the sharpness hits 2∕η.
(This sometimes occurs even for networks with ReLU activations or max-pooling, which give rise
to training objectives that are not continuously differentiable, which means that the gradient flow
trajectory is not necessarily guaranteed to exist.) For such networks, the gradient flow trajectory
provides a coherent framework for reasoning about which step sizes will eventually enter the Edge
of Stability. Let λ0 be the sharpness at initialization, and let λmax be the maximum sharpness along
the gradient flow trajectory. If η < 2∕λmaχ, then gradient descent will stably track the gradient flow
trajectory for the entire duration of training, and will never enter the Edge of Stability. On the other
hand, if η ∈ [2∕λmaχ, 2∕λ0], then gradient descent will stably track the gradient flow trajectory only
until reaching the point on that trajectory where the sharpness hits 2/n; shortly afterwards, gradient
descent will become destabilized, depart the gradient flow trajectory, and enter the Edge of Stability.
4	Further experiments
Section 3 focused for exposition on a single architecture and task. In this section, we show that our
characterization of gradient descent holds broadly across a wide range of architectures and tasks.
We detail several known caveats and qualifications in Appendix A.
Architectures. In Appendix J, we fix the task of training a5k subset of CIFAR-10, and we systemat-
ically vary the network architecture. We consider fully-connected networks, as well as convolutional
networks with both max-pooling and average pooling. For all of these architectures, we consider
tanh, ReLU, and ELU activations, and for fully-connected networks we moreover consider softplus
and hardtanh —- eleven networks in total. We train each network with both cross-entropy and MSE
loss. In each case, we successfully reproduce Figure 5.
Since batch normalization (Ioffe & Szegedy, 2015) is known to have unusual optimization properties
(Li & Arora, 2019), it is natural to wonder whether our findings still hold with batch normalization.
In Appendix K, we confirm that they do, and we reconcile this point with Santurkar et al. (2018).
Tasks. In Appendix L, we verify our findings on: (1) a Transformer trained on the WikiText-
2 language modeling task; (2) fully-connected tanh networks with one hidden layer, trained on a
synthetic one-dimensional toy regression task; and (3) deep linear networks trained on Gaussian
data. In each case, the sharpness rises until hovering right at, orjust above, the value 2/n.
Standard networks on CIFAR-10. In Appendix M, we verify our findings on three standard ar-
chitectures trained on the full CIFAR-10 dataset: a ResNet with BN, a VGG with BN, and a VGG
without BN. For all three architectures, we find that progressive sharpening occurs to a dramatic
degree, and, relatedly, that stable step sizes are dramatically suboptimal. For example, when we
train the VGG-BN to 99% accuracy using gradient flow / Runge-Kutta, we find that the sharpness
rises from 6.3 at initialization to a peak sharpness of 2227.6. Since this is an architecture for which
gradient descent closely hews to the gradient flow trajectory, we can conclude that any stable step
size for gradient descent would need to be less than 2/2227.6 = 0.000897. Training finishes at time
14.91, so gradient descent at any stable step size would require at least 14.91/0.000897 = 16, 622
iterations. Yet empirically, this network can be trained to completion at the larger, “Edge of Stabil-
ity” step size ofη = 0.16 in just 329 iterations. Therefore, training ata stable step size is suboptimal
by a factor of at least 16622/329 = 50.5. The situation is similar for the other two architectures
we consider. In short, for standard architectures on the standard dataset CIFAR-10, stable step sizes
are not just suboptimally small, they are so suboptimal as to be completely unreasonable. For these
networks, gradient descent at any reasonable step size eventually enters the Edge of Stability regime.
Tracking gradient flow. Recall that for some (but not all) architectures, gradient descent closely
hews to the gradient flow trajectory so long as the sharpness is less than 2/n. Among the archi-
tectures considered in Appendix J, we found this to be true for the architectures with continuously
differentiable components, as well as some, but not all, with ReLU, hardtanh, and max-pooling.
Among the architectures in Appendix L, we found this to be true for the tanh network, but not for
the deep linear network or the Transformer. Finally, we did find this to be true for the three standard
architectures in Appendix M, even though those architectures use ReLU.
7
Published as a conference paper at ICLR 2021
5	Discussion
We now explain why the behavior of gradient descent at the Edge of Stability contradicts several
pieces of conventional wisdom in optimization.
At reasonable step sizes, gradient descent cannot be analyzed using (even local) L-smoothness
Many convergence analyses of gradient descent assume a bound on the sharpness — either globally
or, at the very least, along the optimization trajectory. This condition, called L-smoothness (Nes-
terov, 1998), is intended to guarantee that each gradient step will decrease the training objective by
a certain amount; the weakest guarantee is that if the local sharpness is less than 2∕η, then a gradient
step with size η is guaranteed to decrease (rather than increase) the training objective. At a bare
minimum, any convergence analysis of gradient descent based on L-smoothness will require the
sharpness along the optimization trajectory to be less than 2∕η. Yet to the contrary, at the Edge of
Stability, the sharpness hovers just above 2∕η. Therefore, at any step size for which gradient descent
enters the Edge of Stability (which, on realistic architectures, includes any reasonable step size),
gradient descent cannot be analyzed using L-smoothness. Li et al. (2020b) previously argued that
convergence analyses based on L-smoothness do not apply to networks with both batch normaliza-
tion and weight decay; our paper empirically extends this to neural networks without either.
L-smoothness may be inappropriate when analyzing other optimization algorithms too It is
common for optimization papers seemingly motivated by deep learning to analyze algorithms un-
der the “non-convex but L-smooth”’ setting (Reddi et al., 2016; Agarwal et al., 2017; Zaheer et al.,
2018; Zhou et al., 2018; Chen et al., 2019; Li & Orabona, 2019; Ward et al., 2019; You et al., 2020;
Vaswani et al., 2019; Sankararaman et al., 2019; Reddi et al., 2021; DefOSSez et al., 2020; Xie et al.,
2020; Liu et al., 2020; Defazio, 2020). Since our experiments focus on gradient descent, it does not
necessarily follow that L-smoothness assumptions are unjustified when analyzing other optimiza-
tion algorithms. However, gradient descent is arguably the simplest optimization algorithm, so we
believe that the fact that (even local) L-smoothness fails even there should raise serious questions
about the suitability of the L-smoothness assumption in neural network optimization more generally.
In particular, the burden of proof should be on authors to empirically justify this assumption.
At reasonable step sizes, gradient descent does not monotonically decrease the training loss
In neural network training, SGD does not monotonically decrease the training objective, in part
due to minibatch randomness. However, it is often assumed that full-batch gradient descent would
monotonically decrease the training objective, were it used to train neural networks. For example,
Zhang et al. (2020) proposed a “relaxed L-smoothness” condition that is less restrictive than standard
L-smoothness, and proved a convergence guarantee for gradient descent under this condition which
asserted that the training objective will decrease monotonically. Likewise, some neural network
analyses such as Allen-Zhu et al. (2019) also assert that the training objective will monotonically
decrease. Yet, at the Edge of Stability, the training loss behaves non-monotonically over short
timescales even as it consistently decreases over long timescales. Therefore, convergence analyses
which assert monotone descent cannot possibly apply to gradient descent at reasonable step sizes.
The Edge of Stability is inherently non-quadratic Itis tempting to try to reason about the behav-
ior of gradient descent on neural network training objectives by analyzing, as a proxy, the behavior
of gradient descent on the local quadratic Taylor approximation (LeCun et al., 1998). However, at
the Edge of Stability, the behavior of gradient descent on the real neural training objective is irrec-
oncilably different from the behavior of gradient descent on the quadratic Taylor approximation: the
former makes consistent (if choppy) progress, whereas the latter would diverge (and this divergence
would happen quickly, as we demonstrate in Appendix E). Thus, the behavior of gradient descent at
the Edge of Stability is inherently non-quadratic.
Dogma for step size selection may be unjustified An influential piece of conventional wisdom
concerning step size selection has its roots in the quadratic Taylor approximation model of gradient
descent. This conventional wisdom (LeCun et al., 1993; 1998; Schaul et al., 2013) holds that if
the sharpness at step t is λt, then the current step size η must be set no greater than 2∕λt (in
order to prevent divergence); and furthermore, barring additional information about the objective
function, that η should optimally be set to 1∕λt. Our findings complicate this conventional wisdom.
8
Published as a conference paper at ICLR 2021
To start, it is nearly impossible to satisfy these prescriptions with a fixed step size: for any fixed
(and reasonable) step size ηt = η, progressive sharpening eventually drives gradient descent into
regions where the sharpness is just a bit greater than 2/n — which means that the step size η is
purportedly impermissible. Furthermore, in Appendix F, we try running gradient descent with the
purportedly optimal η = 1 /λt rule, and find that this algorithm is soundly outperformed by the
purportedly impermissible baseline of gradient descent with a fixed η = 1∕λo step size, where λo
is the sharpness at initialization. The η = 1∕λt rule continually anneals the step size, and in so
doing ensures that the training objective will decrease at each iteration, whereas the fixed η = 1∕λo
step size often increases the training objective. However, this non-monotonicity turns out to be a
worthwhile price to pay in return for the ability to take larger steps.
6	Stochastic gradient descent
Our precise characterization of the behavior of the sharpness only applies to full-batch gradient
descent. In contrast, during SGD, the sharpness does not always settle at any fixed value (Appendix
G), let alone one that can be numerically predicted from the hyperparameters. Nevertheless, prior
works (JaStrZebSki et al., 2017; 2019; 2020) have demonstrated that large step sizes do steer SGD
into regions of the landscape with lower sharpness; the 2∕η rule we have identified for full-batch
gradient descent is a special case of this observation. Furthermore, small batch sizes also steer SGD
into regions with lower sharpness (Keskar et al., 2016; JaStrZebSki et al., 2017), as we illustrate in
Appendix G. Jastrzebski et al. (2020) attributed this phenomenon to the stability properties of SGD.
Even though our findings only strictly hold for gradient descent, they may have relevance to SGD
as well. First, since gradient descent is a special case of SGD, any general characterization of the
dynamics of SGD must reduce to the Edge of Stability in the full-batch special case. Second, there
are indications that the Edge of Stability may have some analogue for SGD. One way to interpret our
main findings is that gradient descent “acclimates” to the step size in such a way that each update
sometimes increases and sometimes decrease the train loss, yet an update with a smaller step size
would consistently decrease the training loss. Along similar lines, in Appendix H we demonstrate
that SGD “acclimates” to the step size and batch size in such a way that each SGD update sometimes
increases and sometimes decreases the training loss in expectation, yet an SGD update with a smaller
step size or larger batch size would consistently decrease the training loss in expectation.
In extending these findings to SGD, the question arises of how to model “stability” of SGD. This is a
highly active area of research. Wu et al. (2018) proposed modeling stability in expectation, and gave
a sufficient (but not necessary) criterion for the stability of SGD in expectation. Building on this
framework, JaStrZebSki et al. (2020) argued that if the Hessian is aligned with the second moment
matrix of per-example gradients, then SGD is stable so long as a certain expression (involving the
sharpness) is below a certain threshold. In the special full-batch case, their criterion reduces to the
sharpness being beneath 2∕η — a constraint which we have shown is “tight” throughout training.
However, in the general SGD case, there is no evidence that their stability constraint is tight through-
out training. Giladi et al. (2020) showed that the generalization gap in asynchronous SGD can be
mostly ameliorated by setting the step size so as to ensure that stability properties in expectation
remain identical to those of a well-tuned implementation of synchronous SGD. Finally, a number
of papers have attempted to mathematically model the propensity of SGD to “escape from sharp
minima” (Hu et al., 2017; Zhu et al., 2019; Xie et al., 2021).
7	Conclusion
We have empirically demonstrated that the behavior of gradient decent on neural training objectives
is both surprisingly consistent across architectures and tasks, and surprisingly different from that
envisioned in the conventional wisdom. Our findings raise a number of questions. Why does pro-
gressive sharpening occur? At the Edge of Stability, by what mechanism does gradient descent avoid
diverging entirely? Since the conventional wisdom for step size selection is wrong, how should the
gradient descent step size be set during deep learning? Does the “Edge of Stability” effect generalize
in some way to optimization algorithms beyond gradient descent, such as SGD? We hope to inspire
future efforts aimed at addressing these questions.
9
Published as a conference paper at ICLR 2021
8	Acknowledgements
This work was supported in part by DARPA FA875017C0141, the National Science Foundation
grants IIS1705121 and IIS1838017, an Amazon Web Services Award, a JP Morgan A.I. Research
Faculty Award, a Carnegie Bosch Institute Research Award, a Facebook Faculty Research Award,
and a Block Center Grant. Any opinions, findings and conclusions or recommendations expressed
in this material are those of the author(s) and do not necessarily reflect the views of DARPA, the
National Science Foundation, or any other funding agency.
References
Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approxi-
mate local minima faster than gradient descent. In Proceedings of the 49th Annual ACM SIGACT
Symposium on Theory of Computing, pp.1195-1199, 2017.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252. PMLR, 2019.
David Barrett and Benoit Dherin. Implicit gradient regularization. In International Confer-
ence on Learning Representations, 2021. URL https://openreview.net/forum?id=
3q5IqUrkcF.
Leon Bottou, Frank E. Curtis, and Jorge NocedaL Optimization methods for large-scale machine
learning. Siam Reviews, 60(2):223-311, 2018. URL http://leon.bottou.org/papers/
bottou-curtis-nocedal-2018.
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence ofa class of adam-type
algorithms for non-convex optimization. In International Conference on Learning Representa-
tions, 2019. URL https://openreview.net/forum?id=H1x-x309tm.
Aaron Defazio. Understanding the role of momentum in non-convex optimization: Practical insights
from a lyapunov analysis, 2020.
Alexandre Defossez, Leon Bottou, Francis Bach, and Nicolas Usunier. On the convergence of adam
and adagrad. arXiv preprint arXiv:2003.02395, 2020.
Saber Elaydi. An introduction to difference equations. Springer Science & Business Media, 2005.
Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net optimization
via hessian eigenvalue density. volume 97 of Proceedings of Machine Learning Research, pp.
2232-2241, 2019.
Niv Giladi, Mor Shpigel Nacson, Elad Hoffer, and Daniel Soudry. At stability’s edge: How to adjust
hyperparameters to preserve minima selection in asynchronous training of neural networks? In
International Conference on Learning Representations, 2020. URL https://openreview.
net/forum?id=Bkeb7lHtvH.
Gabriel Goh. Why momentum really works. Distill, 2(4):e6, 2017.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http:
//www.deeplearningbook.org.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generaliza-
tion gap in large batch training of neural networks. In Advances in Neural Information Processing
Systems, pp. 1731-1741, 2017.
Wenqing Hu, Chris Junchi Li, Lei Li, and Jian-Guo Liu. On the diffusion approximation of noncon-
vex stochastic gradient descent. arXiv preprint arXiv:1705.07562, 2017.
Like Hui and Mikhail Belkin. Evaluation of neural architectures trained with square loss vs cross-
entropy in classification tasks, 2020.
10
Published as a conference paper at ICLR 2021
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine
Learning,pp. 448-456, 2015.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing systems, pp. 8571-
8580, 2018.
Arthur Jacot, Franck Gabriel, and Clement Hongler. The asymptotic spectrum of the hessian of
dnn throughout training. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=SkgscaNYPS.
StanisIaW Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua
Bengio, and Amos Storkey. Three factors influencing minima in sgd. arXiv preprint
arXiv:1711.04623, 2017.
StaniSIaW JaStrzebski, Zachary Kenton, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amost
Storkey. On the relation betWeen the sharpest directions of DNN loss and the SGD step length. In
International Conference on Learning Representations, 2019. URL https://openreview.
net/forum?id=SkgEaj05t7.
Stanislaw Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek Tabor, Kyunghyun
Cho*, and Krzysztof Geras*. The break-even point on optimization trajectories of deep neural
netWorks. In International Conference on Learning Representations, 2020. URL https://
openreview.net/forum?id=r1g87C4KwB.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima, 2016.
Y LeCun, L Bottou, GB Orr, and K-R Muller. Efficient backprop. Lecture notes in computer science,
pp. 9-50, 1998.
Yann LeCun, Patrice Y. Simard, and Barak Pearlmutter. Automatic learning rate maximization by
on-line estimation of the hessian’s eigenvectors. In Advances in Neural Information Processing
Systems 5. 1993.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural netWorks of any depth evolve as linear models
under gradient descent. In Advances in neural information processing systems, pp. 8572-8583,
2019.
Aitor LeWkoWycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large
learning rate phase of deep learning: the catapult mechanism. arXiv preprint arXiv:2003.02218,
2020.
Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent With adaptive
stepsizes. In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 983-
992, 2019.
Xinyan Li, Qilong Gu, Yingxue Zhou, Tiancong Chen, and Arindam Banerjee. Hessian based
analysis of sgd for deep nets: Dynamics and generalization. In Proceedings of the 2020 SIAM
International Conference on Data Mining, pp. 190-198. SIAM, 2020a.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural netWorks via stochastic gradi-
ent descent on structured data. In Proceedings of the 32nd International Conference on Neural
Information Processing Systems, pp. 8168-8177, 2018.
Zhiyuan Li and Sanjeev Arora. An exponential learning rate schedule for deep learning. In Interna-
tional Conference on Learning Representations, 2019.
Zhiyuan Li, Kaifeng Lyu, and Sanjeev Arora. Reconciling modern deep learning With traditional
optimization analyses: The intrinsic learning rate. Advances in Neural Information Processing
Systems, 33, 2020b.
11
Published as a conference paper at ICLR 2021
Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Toward a theory of optimization for over-
parameterized systems of non-linear equations: the lessons of deep learning, 2020.
James Martens. SECOND-ORDER OPTIMIZATION FOR NEURAL NETWORKS. PhD thesis, 2016.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. In International Conference on Learning Representations, 2016.
Rotem Mulayoff and Tomer Michaeli. Unique properties of flat minima in deep networks, 2020.
Kamil Nar and Shankar Sastry. Step size matters in deep learning. In Advances in Neural Informa-
tion Processing Systems, pp. 3436-3444, 2018.
Yurii Nesterov. Introductory lectures on convex programming volume i: Basic course. 1998.
Yurii E Nesterov. A method for solving the convex programming problem with convergence rate o
(1∕k^ 2).In Dokl. akad. nauk Sssr, volume 269, pp. 543-547,1983.
Vardan Papyan. The full spectrum of deepnet hessians at scale: Dynamics with sgd training and
sample size. arXiv preprint arXiv:1811.07062, 2018.
Vardan Papyan. Measurements of three-level hierarchical structure in the outliers in the spectrum of
deepnet hessians. arXiv preprint arXiv:1901.08244, 2019.
Vardan Papyan. Traces of class/cross-class structure pervade deep learning spectra, 2020.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, LUca Antiga, Alban Desmaison, Andreas Kopf, Ed-
ward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep
learning library, 2019.
B.T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computa-
tional Mathematics and Mathematical Physics, 1964.
William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. Numerical
Recipes in C (2nd Ed.): The Art of Scientific Computing. Cambridge University Press, USA,
1992. ISBN 0521431085.
Sashank J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola. Stochastic variance
reduction for nonconvex optimization. In International conference on machine learning, pp. 314-
323, 2016.
Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny,
Sanjiv Kumar, and Hugh Brendan McMahan. Adaptive federated optimization. In International
Conference on Learning Representations, 2021. URL https://openreview.net/forum?
id=LkFG3lB13U5.
Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of
the hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017.
Karthik A Sankararaman, Soham De, Zheng Xu, W Ronny Huang, and Tom Goldstein. The impact
of neural network overparameterization on gradient confusion and stochastic gradient descent. In
International Conference on Machine Learning, 2019.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. HoW does batch nor-
malization help optimization? In Advances in Neural Information Processing Systems, pp. 2483-
2493, 2018.
Tom Schaul, Sixin Zhang, and Yann LeCun. No more pesky learning rates. In International Con-
ference on Machine Learning, pp. 343-351, 2013.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The im-
plicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19
(1):2822-2878, 2018.
12
Published as a conference paper at ICLR 2021
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initial-
ization and momentum in deep learning. In Proceedings of the 30th International Conference on
Machine Learning, 2013.
Sharan Vaswani, Aaron Mishkin, Issam Laradji, Mark Schmidt, Gauthier Gidel, and Simon Lacoste-
Julien. Painless stochastic gradient: Interpolation, line-search, and convergence rates. In Ad-
Vances in Neural Information Processing Systems,pp. 3732-3745, 2019.
Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: sharp convergence over nonconvex
landscapes. In International Conference on Machine Learning, pp. 6677-6686, 2019.
Lei Wu, Chao Ma, and E Weinan. How sgd selects the global minima in over-parameterized learning:
A dynamical stability perspective. In Advances in Neural Information Processing Systems, pp.
8279-8288, 2018.
Yuege Xie, Xiaoxia Wu, and Rachel Ward. Linear convergence of adaptive stochastic gradient
descent. In International Conference on Artificial Intelligence and Statistics, pp. 1475-1485,
2020.
Zeke Xie, Issei Sato, and Masashi Sugiyama. A diffusion theory for deep learning dynamics:
Stochastic gradient descent exponentially favors flat minima. In International Conference on
Learning Representations, 2021. URL https://openreview.net/forum?id=wXgk_
iCiYGo.
Chen Xing, Devansh Arpit, Christos Tsirigotis, and Yoshua Bengio. A walk with sgd. arXiv preprint
arXiv:1802.08770, 2018.
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan
Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep
learning: Training bert in 76 minutes. In International Conference on Learning Representations,
2020. URL https://openreview.net/forum?id=Syx4wnEtvH.
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive meth-
ods for nonconvex optimization. In Advances in neural information processing systems, pp. 9793-
9803, 2018.
Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates
training: A theoretical justification for adaptivity. In International Conference on Learning Rep-
resentations, 2020. URL https://openreview.net/forum?id=BJgnXpVYwS.
Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu. On the convergence of
adaptive gradient methods for nonconvex optimization. arXiv preprint arXiv:1808.05671, 2018.
Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma. The anisotropic noise in stochas-
tic gradient descent: Its behavior of escaping from sharp minima and regularization effects. In
Proceedings of the 36th International Conference on Machine Learning. PMLR, 2019.
13
Published as a conference paper at ICLR 2021
A Caveats
In this appendix, we list several caveats to our generic characterization of the dynamics of full-batch
gradient descent on neural network training objectives.
1.	With cross-entropy loss, the sharpness often drops at the end of training As mentioned in
the main text, when neural networks are trained on classification tasks using the cross-entropy loss,
the sharpness frequently drops near the end of training, once the classification accuracy begins to
approach 1. We explain this effect in Appendix C.
2.	For shallow or wide networks, or on simple datasets, sharpness doesn’t rise that much
When the network is shallow (Figure 16 - 17) or wide (Figure 12 - 14), or when the dataset is “easy”
or small (Figure 18), the sharpness may rise only a small amount over the gradient flow trajectory.
For these optimization problems, “stable step sizes” (those for which gradient descent never enters
the Edge of Stability) may be quite reasonable, and the range of “Edge of Stability” step sizes may
be quite small.
3.	Sharpness sometimes drops at the beginning of training We sometimes observe that the
sharpness drops at the very beginning of training, as the network leaves its initialization. This was
more common when training with MSE loss than with cross-entropy loss. For most networks, this
drop was very slight. However, the combination of both batch normalization and MSE loss some-
times caused situations where the sharpness was considerably large at initialization, and dropped
precipitously as soon as training began. Figure 8 illustrates one such network.
train loss	sharpness	train accuracy
40
time
60
20	40	60
time
Figure 8: With batch norm and MSE loss, the sharpness sometimes drops precipitously: We use
gradient flow to train a CNN with ReLU activations and batch norm using the MSE loss. Observe
that the sharpness drops substantially at the beginning of training, and never recovers to its initial
value. This seems to occur due to the combination of both batch normalization and MSE loss.
0	20	40	60
time
4.	With batch normalization, need to look at sharpness between iterates As detailed in Ap-
pendix K, when we trained batch-normalized networks using very small step sizes η, we sometimes
observed that the sharpness at the gradient descent iterates themselves plateaued below 2∕η, even
though the sharpness in between the iterates PlateaUedjUSt above 2∕η,as expected.
5.	With non-differentiable components, instability sometimes begins when the sharpness
is a bit less than 2∕η When training networks with ReLU or hardtanh activations, We some-
times observed that the sharpness started to plateaU (and the training loss started to behave non-
monotonically) a bit before the instant when the sharpness crossed 2∕η. For example, see Figures
41, 43 or FigUres 45, 47. One potential explanation is that for sUch networks, the training objective
is not continuously differentiable, so the second-order Taylor approximation around a given iterate
may be a poor local model for the training objective at even an tiny distance away in weight space, if
that weight change causes some activations to switch “ReLU case” from ≤ 0 to > 0, or visa versa.
14
Published as a conference paper at ICLR 2021
B S tab ility of gradient descent on quadratic functions
This appendix describes the stability properties of gradient descent (and its momentum variants)
when optimizing the quadratic objective function
f (x) = 2 XT AX + bτ X + C	(2)
starting from the initialization x0 .
To review, vanilla gradient descent is defined by the iteration:
Xt+1 = Xt - ηVf (xt).
Meanwhile, gradient descent with Polyak (also called “heavy ball”) momentum (Polyak, 1964;
Sutskever et al., 2013; Goodfellow et al., 2016) is defined by the iteration:
vt+1 = βvt - ηVf (Xt)
Xt+1 = Xt + vt+1
where vt is a “velocity” vector and 0 ≤ β < 1 is the momentum coefficient. For β = 0 the algorithm
reduces to vanilla GD.
Finally, Nesterov momentum Sutskever et al. (2013); Goodfellow et al. (2016) is an adaptation of
Nesterov’s accelerated gradient (Nesterov, 1983) for deep learning defined by the iteration:
vt+1 = βvt - ηVf(Xt + βvt)
Xt+1 = Xt + vt+1
where vt is a “velocity” vector and 0 ≤ β < 1 is the momentum coefficient. For β = 0 the algorithm
reduces to vanilla GD.
All three of these algorithms share a special property: on quadratic functions, they act independently
along each Hessian eigenvector. That is, if we express the iterates in the Hessian eigenvector basis,
then in this basis the coordinates evolve independent from one another under gradient descent.
Proposition 1. Consider running vanilla gradient descent on the quadratic objective (2) starting
from X0. Let (q, a) be an eigenveCtor/eigenvalue pair of A. If a > 2∕η, then the SequenCe {qτ Xt}
will diverge.
Proof. The update rule for gradient descent on this quadratic function is:
Xt+1 = Xt - η(AXt + b)
= (I - ηA)Xt -ηb.
Therefore, the quantity qTXt evolves under gradient descent as:
qT Xt+1 = qT(I - ηA)Xt -ηb
= (1 - ηa)qTXt - ηqTb.	(qTA = aq)
Define Xt = qτXt + aqτb, and note that {qτxt} diverges if and only if {Xt} diverges.
The quantity Xt evolves under gradient descent according to the simple rule:
Xt+1 = (1 - ηa)Xt
Since η > 0, if a > 2∕η then (1 - ηa) < -1, so the sequence {Xt} will diverge.	口
We now prove analogous results for Nesterov and Polyak momentum.
15
Published as a conference paper at ICLR 2021
Theorem 1.	Consider running Nesterov momentum on the quadratic objective (2) starting from
any initialization. Let (q,a) be an eigenveCtor/eigenvalue pair of A. If a > 1 (2：2：), then the
sequence {qTxt} will diverge.
Proof. The update rules for Nesterov momentum on this quadratic function are:
vt+1 = β(I - ηA)vt - ηb - ηAxt
xt+1 = xt + vt+1.
Using the fact that vt = xt - xt-1, we can rewrite this as a recursion in xt alone:
xt+1 = xt + β(I - ηA)(xt - xt-1) - ηb - ηAxt
= (1 +β)(I - ηA)xt - β(I - ηA)xt-1 - ηb
Define Xt = qτXt + aqτb, and note that qτXt diverges iff Xt diverges. It can be seen that Xt
evolves as:
Xt+1 = (1+ β)(1 - ηa)Xt - β(1 - ηa)Xt-i
This is a linear homogenous second-order difference equation. By Theorem 2.37 in Elaydi (2005),
since η > 0 and β < 1, if a > ^ (∣++2β) then this recurrence diverges.
□
The following result previously appeared in Goh (2017).
Theorem 2.	Consider running Polyak momentum on the quadratiC objeCtive (2) starting from any
initialization. Let (q, a) be an eigenveCtor/eigenvalue pair of A. If a > 1 (2 + 2β), then the
sequenCe {qTXt} will diverge.
Proof. Using the fact that vt = Xt - Xt-1, we can re-write the Polyak momentum recursion as a
recursion in X alone:
χt+ι = Xt + βvt - ηVf (xt)
=Xt + (β(χt - χt-ι) - ηVf (Xt))
= (1 + β)Xt - βXt-ι - ηVf (Xt).
For the quadratic objective (2), this update rule amounts to:
Xt+1 = (1 + β)Xt -βXt-1 - η(AXt +b)
= (1 +β - ηA)Xt -βXt-1 -ηb.
Multiplying by qT, we obtain:
qT Xt+1 = (1 +β -ηa)qTXt - βqTXt-1 - ηqTb.
Now, define Xt = qτXt + 1 qτb. Note that qτXt diverges iff Xt diverges. It can be seen that Xt
evolves as:
Xt+1 = (1 + β - ηa)Xt - βXt-i.
This is a linear homogenous second-order difference equation. By Theorem 2.37 in Elaydi (2005),
since η > 0 and β < 1, if a > η1 (2 + 2β) then this recurrence diverges.	□
16
Published as a conference paper at ICLR 2021
C Cross-entropy loss
In this appendix, we explain why the sharpness decreases at the end of training when the cross-
entropy loss is used. Before considering the full multiclass case, let us first consider the simpler
case of binary classification with the logistic loss.
Binary classification with logistic loss We consider a dataset {xi, yi)}in=1 ⊂ Rd × {-1, 1},
where the examples are vectors in Rd and the labels are binary {-1, 1}. We consider a neural
network h : Rd × Rp → R which maps an input x ∈ Rd and a parameter vector θ ∈ Rp to a
prediction h(x; θ) ∈ R. Let ` : R × {-1, 1} → R be the logistic loss function:
`(z; y) = log(1 + exp(-zy))
=一logP where P = ɪ-^——；------------’.
The second derivative of this loss function w.r.t z is:
`00 (z; y) = P(1 一 P).
The full training objective is:
1n
f (θ) = - Efi(θ)	where fi(θ) = '(f(Xi； θ); y)
ni=1
The Hessian of this training objective is the average of per-example Hessians:
1n
v2 f (θ) = - Ev2fi(θ).
n
i=1
For any arbitrary loss function `, we have the so-called Gauss-Newton decomposition (Martens,
2016; Bottou et al., 2018) of the per-example Hessian v2fi(θ):
v2fi(θ) = '0(zi; yi) Vθh(xi, θ) Vθh(xi, θ)T + '(zi； yi) V2h(xi； θ) where Zi = h(xi； θ)
where vθh(Xi; θ) ∈ Rp is the gradient of the network output with respect to the weights, and `0
refers to the derivative of ` with respect to its first argument, the score.
Empirically, the first term in the decomposition (usually called the “Gauss-Newton matrix”) tends to
dominate the second, which implies the following “Gauss-Newton approximation” to the Hessian:
-n
v2f (θ) ≈ n N '0(Zi yi) Vθh(xi, θ) Vθh(xi, θ)T	(3)
In our experience, progressive sharpening affects Vθh(xi; θ) Vθh(xi; θ)T.	That is,
Vθh(xi; θ) Vθh(xi; θ)T tends to grow in scale continually during training. For the square loss
'(z; y) = 1 (z 一 y)2, the second derivative '00(z; y) = 1 is the identity, so the V2fi(θ) grows con-
tinually as well. In contrast, for the logistic loss, many of the '00(zi； yi) decrease at the very end of
training. Why is this? In Figure 9, we plot both the logistic loss `, and its second derivative `00, as a
function of the quantity yz, which is often called the “margin.”
Crucially, observe that both ` and `00 are decreasing in yz. Because the loss ` is decreasing in yz,
once an example i is classified correctly (i.e. yizi > 0), the training objective can be optimized fur-
ther by increasing the margin yizi. Because `00 is also decreasing in yz, if the margin yizi increases,
the term '00(zi； yi) drops. Near the end of training, once most examples are classified correctly,
gradient descent can easily increase the margins of all these examples by simply scaling up the final
layer weight matrix. This causes the 00(zi； yi) to drop. Therefore, even though progressive sharpen-
ing still applies to Vθh(xi； θ) Vθh(xi； θ)T, the decrease in the '00(Zi; yi)’ pulls down the leading
eigenvalue of the Gauss-Newton matrix in equation 3.
This effect is illustrated in Figure 10. Here, we train a network on the binary classification task of
CIFAR-10 airplane vs. automobile, using the logistic loss. In Figure 10(e), we plot the margin yizi
for 10 examples in the dataset. Notice that at the end of training, these margins all continually rise;
17
Published as a conference paper at ICLR 2021
Figure 9: Left: logistic loss function ` as a function of yz. Right: its second derivative `00 as a
function of yz .
this is because gradient descent “games” the objective by increasing the margins of successfully
classified examples. When the margin yizi rises, the second derivative '00(zi; G drops. This can be
seen from Figure 10(f), where WePiot '00(zi; y%) for these same 10 examples. Now, all the while, the
leading eigenvalue of the matrix -1 pn=1 Vθh(x%; θ)Vθh(x%; θ)T keeps rising, as can be seen from
Figure 10(d). However, because the '00s are dropping, the leading eigenvalue of the Gauss-Newton
matrix ɪ P1=ι '"(，i； y%) V h(xi; Θ)Vθ h(x%; θ)T starts to decrease at the end of training, as can be
seen from the green line in Figure 10(c). Finally, since the leading eigenvalue of the Gauss-Newton
matrix is an excellent approximation to the leading eigenvalue of the Hessian (i.e. the sharpness),
the sharpness also drops at the end of training, as can be seen from the orange line in Figure 10(c).
"n">u"65
(a) train loss
0	2000 4000 6000 8000
step
(b) train accuracy
ADeJnUUe u"h
(c) leading eigenvalue of Hessian
and Gauss-Newton matrix
----Hessian
----Gauss-Newton
IOOOO
5000
n
(d) leading eigenvalue of ⅛ ΣVeh(x；;0) θ)τ
__________________i=ι___________
7.5
0
0	2000 4000 6000 8000
step
0	2000 4000 6000 8000
step
(f) i" of 10 examples
0.2
0.1
0.0
O
800
蠹
O
200
Figure 10: We train a network using the logistic loss on the binary classification problem of CIFAR-
10 airplane vs. automobile. (a) The train loss. (b) The train accuracy. (c) The leading eigenvalue
of the Hessian and of the Gauss-Newton matrix; observe that the latter is a great approximation to
the former. (d) The leading eigenvalue of the matrix ɪ P1=1 Vθh(x%; θ)Vθh(x%; θ)T, which is the
Gauss-Newton matrix without the `00 terms; observe that this value constantly rises — it does not
dip at the end of training. (e) The margin yizi of 10 examples; observe that all the margins rise at
the end of training. (f) the value '0(zi; y%) for 10 examples; observe that all of these curves decline
at the end of training.
Multiclass classification with cross-entropy loss We consider a dataset {(xi , yi)}in=1 ⊂ Rd ×
{1, . . . , k}, where the examples are vectors in Rd and the labels are in {1, . . . , k}. We consider a
neural network h : Rd × Rp → Rk which maps an input x ∈ Rd and a parameter vector θ ∈ Rp to
a prediction h(x; θ) ∈ Rk. Let ` : Rk × {1, . . . , k} → R be the cross-entropy loss function:
`(z; y)
- log
exp(zy)
Pj=I eχp(zj)
- log py where p
eχp(z)
Pj exP(zj)
18
Published as a conference paper at ICLR 2021
The Hessian V2'(z; y) ∈ Rk×k of this loss function w.r.t the class scores Z is:
V2 '(z; y) = diag(p) — PPT
Now, for any loss function ` : Rk × {1, . . . , k} → R, we have the Gauss-Newton decomposition:
k
V2fi(θ) = JT [V2i'(zi; yi)] Ji + X [Vzi'(Zi； yi)]j V2hj(xi； θ)
j=1
where zi = hi(xi； θ) ∈ Rk are the logits for example i, Ji ∈ Rk×p is the network output-to-
weights Jacobian for example i, NZi'(zi； yi) ∈ Rk×k is the Hessian of ' w.r.t its input Zi, and
Vjhj (xi； θ) ∈ Rp×p is the Hessian matrix of the j-th output of the network h on the i-th example.
Dropping the second term yields the Gauss-Newton approximation:
V2fi(θ) ≈ JiT Vz2i '(zi； yi)] Ji.
As in the binary classification case discussed above: at the end of training, for many examples i, the
yi entry of Pi will tend toward 1 and the other entries of Pi will tend to 0. Once this occurs, the
matrix diag(Pi) - PiPiT will broadly decrease in scale: the diagonal entries of this matrix are of the
form p(1 - p), which goes to zero as p → 0 or p → 1; and the off-diagonal entries are of the form
-pq, which also goes to zero ifp → 0, q → 1 or p → 0, q → 0 or p → 1, q → 0.
This effect is illustrated in Figure 11. Here, we train a network on CIFAR-10 using the cross-entropy
loss. In Figure 11(e), for ten examples i in the dataset (with output scores zi = h(xi) ∈ Rk), we
plot the margin zi [yi] - maxj6=yi zi [j], which is the difference between the score of the correct
class yi and the score of the next-highest class. Observe that for all of these examples, this margin
rises at the end of training. In Figure 11(f), for those same ten examples, we plot the quantity
Pi [yi](1 - Pi [yi]).Observe that for all of these examples, this quantity continually decreases at
the end of training. Now, all the while, the leading eigenvalue of the matrix 1 PZi JT Ji keeps
rising, as can be seen from Figure 11(d). However, because V2'(zi； yi) is decreasing, the leading
eigenvalue of the Gauss-Newton matrix 1 PZi JT V2'(zi； yi) Ji starts to decrease at the end of
training, as can be seen from the green line in Figure 11(c). Finally, since the leading eigenvalue
of the Gauss-Newton matrix is an excellent approximation to the leading eigenvalue of the Hessian
(i.e. the sharpness), the sharpness also drops at the end of training, as can be seen from the orange
line in Figure 11(c).
19
Published as a conference paper at ICLR 2021
"n">u"65
(a) train loss
2000 4000 6000 8000 10000
step
0 O
O 5
1
n>u
(c) leading eigenvalue of Hessian
and Gauss-Newton matrix
150-
0	2000	4000	6000	8000 10000
step
(f) ltt of 10 examples
6000-
4000-
2000-
0-
n
(d) leading eigenvalue of ⅛ Σ∕⅛
(e) margin of 10 examples
Figure 11: We train a network using the cross-entropy loss on CIFAR-10. (a) The train loss. (b) The
train accuracy. (c) The leading eigenvalue of the Hessian and of the Gauss-Newton matrix; observe
that the latter is a great approximation to the former. (d) The leading eigenvalue of the matrix
* pn=1 JTJi, which is the GaUss-NeWton matrix except the V2' terms; observe that this value
constantly rises — it does not dip at the end of training. (e) The margin zi[yi] - maxj6=yi zi[j] of 10
examples; observe that all these margins rise at the end of training. (f) the value pi [yi](1 - pi [yi])
for 10 examples; observe that all of these curves decline at the end of training.
20
Published as a conference paper at ICLR 2021
D	Empirical study of progressive sharpening
In this appendix, we empirically study how problem parameters such as network width, network
depth, and dataset size affect the degree to which progressive sharpening occurs. To study progres-
sive sharpening on its own, without the confounding factor of instability, we train neural networks
using gradient flow (Runge-Kutta) rather than gradient descent. Informally speaking, gradient flow
does “what gradient descent would do if gradient descent didn’t have to worry about instability.”
We observe that progressive sharpening occurs to a greater degree: (1) for narrower networks than
for wider networks (which is consistent with infinite-width NTK theory), (2) for deeper networks
than for shallower networks, and (3) for larger datasets than for smaller datasets.
The effect of width When networks parameterized in a certain way (the “NTK parameterization”)
are made infinitely wide and trained using gradient flow, the Hessian moves a vanishingly small
amount during training, implying that no progressive sharpening occurs (Jacot et al., 2018; Lee
et al., 2019; Jacot et al., 2020; Li & Liang, 2018). Therefore, a natural hypothesis is that progressive
sharpening might attenuate as network width increases. We now run an experiment which supports
this hypothesis.
We consider fully-connected architectures with two hidden layers and tanh activations, with widths
{32, 64, 128, 256, 512, 1024}. We train on a size-5,000 subset of CIFAR-10 using the cross-entropy
loss. We train using gradient flow (details in §I.5). We consider both NTK parameterization and
standard parameterization (Lee et al., 2019).
In Figure 12, for each width, we train NTK-parameterized networks from five different random
initializations, and plot the evolution of the sharpness during gradient flow. Observe that the max-
imum sharpness along the gradient flow trajectory is larger for narrow networks, and smaller for
wide networks. (As elsewhere in this paper, note that the sharpness drops at the end of training
due to the cross-entropy loss.) In Figure 13, we plot summary statistics from these training runs.
Namely, define λmax as the maximum sharpness over the gradient flow trajectory, and define λ0
as the initial sharpness. In Figure 13(a), for each width we plot the mean and standard deviation
of the maximum sharpness λmax over the five different random initializations. Observe that λmax
becomes smaller, on average, as the width is made larger. In Figure 13(b), for each width we plot
the mean and standard deviation of the maximum sharpness gain λmaχ∕λo over the five different
random initializations. Observe that the maximum sharpness gain λmaχ∕λ0 also becomes smaller
as the width is made larger. NTK theory suggests that λmaχ∕λo should deterministically tend to 1
as the width → ∞, and Figure 13(b) is consistent with this prediction.
In Figures 14 and 15, we conduct similar experiments, but with standard parameterization rather
than NTK parameterization. Similar to NTK parameterization, we observe in Figure 14 that the
sharpness rises more for narrow networks than for wide networks.
21
Published as a conference paper at ICLR 2021
width 32
width 64
width 128
width 256
width 512
width 1024
Figure 12: NTK parameterization: evolution of the sharpness. We use gradient flow to train
NTK-parameterized networks, and we track the evolution of the sharpness during training. For
each width, we train from five different random initializations (different colors). Observe that the
sharpness rises more when training narrow networks than when training wide networks.
maximum sharpness along trajectory
maximum sharpness gain along trajectory
ɪ
工一
O 5
L
(PSueE)
XeUJF
ɪ
-I-
32	64	128	256	512	1024	32	64	128	256	512	1024
width	width
Figure 13: NTK parameterization: summary statistics. Left: For each network width, we plot
the mean and standard deviation (over the five different random initializations) of the maximum
sharpness λmax along the gradient flow trajectory. Observe that λmax decreases in expectation as
the width increases. Right: For each network width, we plot the mean and standard deviation (over
the five different random initializations) of the maximum sharpness gain λmaχ∕λo along the gradient
flow trajectory. Observe that λmaχ∕λo decreases in expectation as the width increases. Indeed, NTK
theory predicts that λmaχ∕λ0 should deterministically tend to 1 as width → ∞ and this plot is
consistent with that prediction.
Figure 14: Standard parameterization: evolution of the sharpness. We use gradient flow to train
standard-parameterized networks, and we track the evolution of the sharpness during training. For
each width, we train from five different random initializations (different colors). Observe that the
sharpness rises more when training narrow networks than when training wide networks.
0	10	20	30	40
time
Q 5	10	15	20	25	30	35	40
time
22
Published as a conference paper at ICLR 2021
(PS+lueωE)
ssωud∙Jeqs XraE
maximum sharpness along trajectory
ɪ
200 -	=>=
==—一
° 32	64	128	256	512 1024
c
'S
maximum sharpness gam along trajectory
(PS+lueωE)
-ssωud∙Jeqs)
5-
width
° 32	64	128	256	512	1024
width
ɪ ɪ ɪ ɪ
E
Figure 15:	Standard parameterization: summary statistics. Left: For each network width, we
plot the mean and standard deviation (over the five different random initializations) of the maximum
sharpness λmax along the gradient flow trajectory. Observe that λmax tends to decrease in expec-
tation as the width increases, though it is not clear whether this pattern still holds when moving
from width 512 to width 1024 — more samples are needed. Right: For each network width, we
plot the mean and standard deviation (over the five different random initializations) of the maximum
sharpness gain λmaχ∕λo along the gradient flow trajectory. Observe that λmaχ∕λo decreases in ex-
pectation as the width increases. It is not clear whether or not λmaχ∕λ0 is deterministically tending
to 1, but that does seem possible.
23
Published as a conference paper at ICLR 2021
Effect of depth We now explore the effect of network depth on progressive sharpening. We use
gradient flow to train fully-connected tanh architectures of width 200 and varying depths — ranging
from 1 hidden layer to 4 hidden layers. We train on a 5k subset of CIFAR-10 using both cross-
entropy loss (in Figure 16) and square loss (in Figure 17). For each depth, we train from five
different random initializations (different colors). Observe that progressive sharpening occurs to a
greater degree as network depth increases.
depth 1	depth 2	depth 3	depth 4
ssφud∙Jeus
Ooo
5 0 5
7 5 2
e
Io m
2 t
，40
e
,
O
Figure 16:	The effect of depth: cross-entropy. We use gradient flow to train networks of var-
ious depths, ranging from 1 hidden layer to 4 hidden layers, using cross-entropy loss. We train
each network from five different random initializations (different colors). Observe that progressive
sharpening occurs to a greater degree for deeper networks.
depth 1	depth 2	depth 3	depth 4
Ooo
5 0 5
7 5 2
ssud∙Jeus
Ooo
5 0 5
7 5 2
O O
5
2
，25
125t
5,0time
e
5,0tim
O
Figure 17:	The effect of depth: mean squared error. We use gradient flow to train networks of var-
ious depths, ranging from 1 hidden layer to 4 hidden layers, using MSE loss. We train each network
from five different random initializations (different colors). Observe that progressive sharpening
occurs to a greater degree for deeper networks.
Effect of dataset size We now explore the effect of dataset size on progressive sharpening. We
use gradient flow to train a network on different-sized subsets of CIFAR-10. The network is a 2-
hidden-layer, width-200 fully-connected tanh architecture, and we train using cross-entropy loss.
The results are shown in Figure 18. Observe that progressive sharpening occurs to a greater degree
as dataset size increases.
400-
ω 300-
W 200-
100-
dataset size Ik
500
10
time
0
0
Figure 18: The effect of dataset size. We use gradient flow to train a network on varying-sized
subsets of CIFAR-10. Observe that progressive sharpening occurs to a greater degree as the dataset
size increases.
24
Published as a conference paper at ICLR 2021
E S peed of Divergence on Quadratic Taylor Approximation
If the sharpness at some iterate is strictly greater than 2∕η, then gradient descent with step size η is
guaranteed to diverge if run on the quadratic Taylor approximation around that iterate. However, the
speed of this divergence could conceivably be slow — in particular, the train loss might continue to
decrease for many iterations before it starts to increase. In this appendix we empirically demonstrate,
to the contrary, that at the Edge of Stability, gradient descent diverges quickly if, at some iterate, we
start running gradient descent on the quadratic Taylor approximation around that iterate.
We consider the fully-connected tanh network from section 3, trained on a 5,000-sized subsample
of CIFAR-10 using both cross-entropy loss and MSE loss. At some timestep t0 during training,
we suddenly switch from running gradient descent on the real neural training objective, to running
gradient descent on the quadratic Taylor approximation around the iterate at step t0 . We do this
for three timesteps before gradient descent has entered the Edge of Stability, and three afterwards.
Figure 20 shows the results for cross-entropy loss, and Figure 21 shows the (similar) results for
MSE loss. Before entering the Edge of Stability (top row), gradient descent on the quadratic Taylor
approximation behaves similar to gradient descent on the real neural training objective — that is,
the orange line almost overlaps the blue line. Yet after entering the Edge of Stability (bottom row),
gradient descent on the quadratic Taylor approximation quickly diverges, whereas gradient descent
on the real neural training objective makes consistent (if choppy) progress.
In short, when gradient descent is not at the Edge of Stability, the quadratic Taylor approximation
serves as a good model for the local progress of gradient descent. But when gradient descent is at
the Edge of Stability, the quadratic Taylor approximation is an extremely poor model for the local
progress of gradient descent. It is conceivable that there exists some simple modification to the
quadratic Taylor model which would fix this issue (e.g. perhaps if one ignores a certain direction,
the quadratic Taylor model is accurate). Nevertheless, unless/until such a fix is discovered, it is
unclear why quadratic Taylor approximations should yield any insight into the local behavior of
gradient descent.
iteration	iteration
Figure 19: We train a neural network using cross-entropy loss (left) and MSE loss (right). In Figure
20 and 21, we show what happens when, at the iterations marked above by vertical dotted lines,
we switch from gradient descent on the real neural training objective to gradient descent on the
quadratic Taylor approximation.
25
Published as a conference paper at ICLR 2021
step 200
360 380 400 420 440
step
step 1000
step 800
Figure 20: Cross-entropy loss (η = 2/60). At six different iterations during the training of the
network from Figure 19 (marked by the vertical dotted black lines), we switch from running gradient
descent on the real neural training objective (for which the train loss is plotted in blue) to running
gradient descent on the quadratic Taylor approximation around the current iterate (for which the
train loss is plotted in orange). Top row are timesteps {200, 400, 600} before gradient descent has
entered the Edge of Stability; observe that the orange line (Taylor approximation) closely tracks the
blue line (real objective). Bottom row are timesteps {800, 1000, 1200} during the Edge of Stability;
observe that the orange line quickly diverges, whereas the blue line does not.
----real objective
quadratic Taylor
approximation
step
step
step
Figure 21: MSE loss (η = 2/200). At six different iterations during the training of the network
from Figure 19 (marked by the vertical dotted black lines), we switch from running gradient descent
on the real neural training objective (for which the train loss is plotted in blue) to running gradient
descent on the quadratic Taylor approximation around the current iterate (for which the train loss is
plotted in orange). Top row are timesteps (2000, 3000, 4000) before gradient descent has entered
the Edge of Stability; observe that the orange line (Taylor approximation) closely tracks the blue
line (real objective). Bottom row are timesteps (5000, 6000, 7000) during the Edge of Stability;
observe that the orange line quickly diverges, whereas the blue line does not.
26
Published as a conference paper at ICLR 2021
F “Optimal” step size selection
One heuristic for setting the step size of gradient descent is to set the step size at iteration t to ηt =
1∕λt , where λt is the sharpness at iteration t. While this heuristic is computationally impractical
due to the time required to compute the sharpness at each iteration, it is often regarded as an ideal,
for instance in LeCun et al. (1998) (Eq. 39), LeCun et al. (1993), and Schaul et al. (2013) (Eq 8).
The motivation for this heuristic is: if all that is known about the training objective is that the local
sharpness is λ, then a step size of 1∕λ maximizes the guaranteed decrease in the training objective
that would result from taking a step.
First, we demonstrate (on a single numerical example) that the dynamic step size ηt = 1∕λt is
outperformed by the baseline approach of gradient descent with a fixed step size ηt = 1∕λ0, where
λ0 is the sharpness at initialization. In Figure 22, we train the network from §3 using both the
dynamic η = 1∕λt step size heuristic as well as the baseline fixed step size of η = 1∕λ0. Observe
that the η = 1∕λ0 baseline outperforms the 1∕λt heuristic. Intuitively, because of progressive
sharpening, the ηt = 1∕λt heuristic anneals the step size, and therefore ends up taking steps that
are suboptimally small. In contrast, while the ηt = 1∕λ0 baseline quickly becomes unstable, this
instability is apparently a worthwhile “price to pay” in return for the benefit of taking larger steps.
---fixed ∕7=l.O"0
dynamic η - 1.0∕λt
train loss
cross-entropy loss
-4r
O n
Sso-U
MSE loss
sharpness
0	5000	10000	15000
step
» 20Q
8 0.010
tt 0.005
step size
0	5000	10000	15000
step
0	5000	10000	15000
step
Figure 22:	A dynamic step size ηt = 1∕λt underperforms a fixed step size of ηt = 1∕λ0.
Another natural idea is to dynamically set the step size at iteration t to ηt = 1.9∕λt. This step size
rule takes larger steps than the η = 1∕λt rule while still remaining stable. In Figure 23, we compare
this ηt = 1.9∕λt rule to a baseline approach of gradient descent with a fixed step size ηt = 1.9∕λ0,
where λ0 is the sharpness at initialization. Observe that the baseline of a fixed 1.9∕λ0 step size
outperforms the dynamic ηt = 1.9∕λt rule.
train loss
0
500	1000	1500	0
step
train loss
cross-entropy loss
sharpness
0.20
月 0.15
OT
&0-10
tt
0.05
0.00
500	1000	1500	0
step
MSE loss
sharpness
step size
500	1000	1500
step
step size
---fixed η=13∣λ0
dynamic η —1.9 禽
S 400
I
S 200
0	2000	4000	6000	8000
step
S 0∙02
OT
燃 0.01
0	2000	4000	6000	8000
step
0	2000	4000	6000	8000
step
Figure 23:	A dynamic step size n = 1.9∕λt underperforms a fixed step size of n = 1.9∕λo.
27
Published as a conference paper at ICLR 2021
G Evolution of sharpness during SGD
In this appendix, we briefly illustrate how the sharpness evolves during stochastic gradient descent.
In Figure 24, we train the tanh network from §3 using SGD with both cross-entropy loss (top) and
mean squared error (bottom). We train using a range of batch sizes (different colors). We observe
the following:
1.	During large-batch SGD, the sharpness behaves similar to full-batch gradient descent: it
rises to 2/n (marked by the black horizontal dashed line) and then hovers just above that
value.
2.	Consistent with prior reports, we find that the smaller the batch size, the lower the sharpness
(Keskar et al., 2016; JaStrzebSki et al., 2017; 2019; 2020).
3.	Notice that when training with cross-entropy loss at batch size 8 (the blue line), the sharp-
ness decreases throughout most of training. The train accuracy (not pictured) is only 66%
when the sharpness starts to decrease, which suggests that the cause of this decrease is
unrelated to the effect described in Appendix C, whereby the sharpness decreases at the
end of training. Figure 5(a) of JaStrzebSki et al. (2020) also depicts a network where the
sharpness decreases during SGD training.
•	BS = 8
•	BS = 16
•	BS = 32
•	BS = 64
•	BS = 128
•	BS = 256
100
focus on all batch sizes
5 0 5
7 5 2
Ss3ud∙lllS
0	10000	20000
iteration
cross-entropy
focus on smallest batch sizes
302010
Ss3ud∙l"llS
0	10000	20000
iteration
Ss3ud∙l"llS
IOO-
75-
50-
25-
focus on largest batch sizes
0	1000	2000
iteration
• BS = 2
• BS = 4
• BS = 8
• BS = 16
• BS = 32
• BS = 64
• BS= 128
• BS = 256
• BS = 512
iteration
mean squared error
80-
S 60-
S
ω
I 40-
ɪ
S
20-
0	20000	40000	60000
focus on smallest batch sizes
iteration
focus on largest batch sizes
Figure 24: Evolution of sharpness during SGD. We train a network using SGD at various batch
sizes, and plot the evolution of the sharpness. In the top row, we train with cross-entropy loss and
step size η = 0.02; in the bottom row, we train with mean squared error and step size η = 0.01.
The leftmost column shows all batch sizes; the center column focuses on just the smallest; and the
rightmost column focuses onjust the largest. The black dashed horizontal line marks 2∕η.
28
Published as a conference paper at ICLR 2021
H SGD acclimates to the hyperparameters
In this appendix, we conduct an experiment which suggests that some version of “Edge of Stability”
may hold for SGD.
One way to interpret our main findings is that gradient descent “acclimates” to the step size in such
a way that each training update sometimes increases, and sometimes decreases, the training loss, yet
an update with a smaller step size would always decrease the training loss. We now demonstrate
that this interpretation may generalize to SGD. In particular, we will demonstrate that SGD seems to
“acclimate” to the step size and batch size in such a way that an actual update sometimes increases
and sometimes decreases the loss in expectation, yet a update with a larger step size or smaller batch
size would almost always increase the loss in expectation, and a step with a smaller step size or a
larger batch size would almost always decrease the loss in expectation.
In Figure 25, we train the tanh network from §3 with MSE loss, using SGD with step size 0.01 and
batch size 32. We periodically compute the training loss (over the full dataset) and plot these on
the left pane of Figure 25. Observe that the training loss does not decrease monotonically, but of
course this is not surprising — SGD is a random algorithm. However, what may be more surprising
is that SGD is not even decreasing the training loss in expectation. On the right pane of Figure
25, every 500 steps during training, we use the Monte Carlo method to approximately compute the
expected change in training loss that would result from an SGD step (the expectation here is over
the randomness involved in selecting the minibatch). Observe that at many points during training,
an SGD step would decrease the loss (as desired) in expectation, but at other points, and SGD step
would increase the loss in expectation.
In Figure 26(a), while training that network, we compute the expected change in training loss that
would result from taking an SGD step with the same step size used during training (i.e. 0.01), but
half the batch size used during training (i.e. 16). We observe that an SGD step with half the batch
size would consistently cause an increase in the training loss in expectation. In Figure 26(b) we
repeat this experiment, but with twice the batch size used during training (i.e. 64). Notice that
an SGD step with twice the batch size would consistently cause a decrease in the training loss in
expectation. In Figure 26(c) and (d) repeat this experiment with the step size; we observe that an
SGD step with a larger step size (0.02) would consistently increase the training loss in expectation,
while an SGD step with a smaller step size (0.005) would consistently decrease the training loss in
expectation.
In Figure 27, as a “control” experiment, we both train and measure the expected loss change under
the following four hyperparameter settings: (step size 0.01, batch size 16), (step size 0.01, batch size
64), (step size 0.02, batch size 32), and (step size 0.005, batch size 32). In each case, we observe
that, after a brief period at the beginning of training, each SGD update sometime increases and
sometimes decreases the training loss in expectation.
Therefore, at least for this single network, we can conclude that no matter the hyperparameters,
SGD quickly navigates to, and then lingers in, regions of the loss landscape in which an SGD
update with those hyperparameters sometimes increases, and sometimes decreases the training loss
in expectation, yet an SGD update with a smaller step size or larger batch size would consistently
decrease the loss in expectation, and an SGD update with a larger step size or smaller batch size
would consistently increase the loss in expectation.
29
Published as a conference paper at ICLR 2021
training loss
Figure 25: SGD does not consistently decrease the training loss in expectation. We train the tanh
network from section 3 with MSE loss, using SGD with step size 0.01 and batch size 32. Periodically
during training, we compute (left) the full-batch training loss, and (right) the expected change in the
full-batch training loss that would result from taking an SGD step (where the expectation is over the
randomness in sampling the minibatch). Strikingly, note that after the very beginning of training,
the expected loss change is sometimes negative (as desired) but oftentimes positive. See Figure 26.
0	5000	10000	15000
SGD step
0 6u"5IP SSo- potjodxo
0.05-
o.oo-
-0.05-
expected loss change
5000	10000	15000
SGD step
Figure 26: An SGD step with a smaller learning rate or a larger batch size than the ones used
during training would consistently decrease the loss in expectation. At regular intervals during
the training run depicted in Figure 25 (with η = 0.01 and batch size 32), we measure the expected
change in the full-batch training loss that would result from an SGD step with a different step size
or batch size. Observe that taking an SGD step with a smaller step size or a larger batch size would
consistently have decreased the loss in expectation, while taking an SGD step with a larger step size
or a smaller batch size would have consistently increased the loss in expectation.
⅞ 0.025
train at q = 0.01, BS=32
「al at twice that step size (0.02)
⅞ 0.025
ð Q.QQQ
train at η = 0.01, BS=32
eval at half that step size (0.005) ev(
.050 -j--------------------------- ⅛ 0.050 -
ð -0.05Q
5。。。 IQQQQ 15000
SGD Step
9 -0.050
0	5000 10000 15000
SGD step
0.01, BS=64 train AND eval at n = 0.01, BS=16 train AND eval at n = 0.005, BS=32 train ANDevaI at n = 0.02, BS=32
B6uequ SSol PaPadXB
train AND eval
0	5000 10000 15000
SGD step
B6uequ SSol PaPadXB
0	10000	20000
SGD step
B6uetSSol PaPadXB
0	10000	20000	30000
SGD step
B6uetSSol PaPadXB
0	5000	10000
SGD step
I O
Figure 27: Control experiment. Above, in Figure 26(a), as we trained a network with step size 0.01
and batch size 32, we evaluated the expected change in training loss that would result from taking
an SGD step with step size 0.01 and batch size 64. Here, in Figure 27(a), as a “control experiment,”
we train the network with step size 0.01 and batch size 64, and evaluate the expected change in
training loss that would result from taking an SGD step with the same step size and batch size. We
observe that an SGD step using the same step size and batch size that are used during training would
sometimes increase and sometimes decrease the training loss in expectation. The other three panes
are analogous.
30
Published as a conference paper at ICLR 2021
I Experimental details
I.1	Varying architectures on 5k subset of CIFAR- 1 0
Dataset. The dataset consists of the first 5,000 examples from CIFAR-10. To preprocess the dataset,
we subtracted the mean from each channel, and then divided each channel by the standard deviation
(where both the mean and stddev were computed over the full CIFAR-10 dataset, not the 5k subset).
Architectures. We experimented with two architecture families: fully-connected and convolutional.
For each of these two families, we experimented with several different activation functions, and for
convolutional networks we experimented with both max pooling and average pooling.
The PyTorch code for e.g. the fully-connected ReLU network is as follows:
nn.Sequential(
nn.Flatten(),
nn.Linear(3072, 200, bias=True),
nn.ReLU(),
nn.Linear(200, 200, bias=True)
nn.ReLU(),
nn.Linear(200, 10, bias=True)
)
Networks with other activation functions would have nn.ReLU() replaced by nn.ELU(),
nn.Tanh(), nn.Softplus(), or nn.Hardtanh().
The PyTorch code for e.g. the convolutional ReLU network with max-pooling is as follows:
nn.Sequential(
nn.Conv2d(3, 32, bias=True, kernel_size=3, padding=1),
nn.ReLU(),
nn.MaxPool2d(2),
nn.Conv2d(32, 32, bias=True, kernel_size=3, padding=1),
nn.ReLU(),
nn.MaxPool2d(2),
nn.Flatten(),
nn.Linear(2048, 10, bias=True)
)
Networks with other activation functions would have nn.ReLU() replaced by nn.ELU() or
nn.Tanh(), and networks with average pooling would have nn.MaxPool2d(2) replaced by
nn.AvgPool2d(2).
For all of these networks, we use the default PyTorch initialization. That is, both fully-connected
layers and convolutional layers have the entries of their weight matrix and bias vector sampled i.i.d
fromUniform(-√⅛, √⅛).
Loss functions. For a k-class classification problem, if the network outputs are z ∈ Rk
and the correct class is i ∈ [k], then the mean squared error (MSE) loss is defined as
2 [(z[i] -1)2 + Pj=i z[j]2]
That is, we encode the correct class with a “1” and the other classes
with “0.” The cross-entropy loss is defined as - log
e eχp(ZM 、
lpj=iexP(Zjl)>
I.2	Standard architectures on CIFAR- 1 0
To preprocess the CIFAR-10 dataset, we subtracted the mean from each channel, and then divided
each channel by the standard deviation.
Since training with full-batch gradient descent is slow, we opted to experiment on relatively
shallow networks. The VGG networks (both with and without BN) are VGG-11’s, from the
implementation here: https://github.com/chengyangfu/pytorch-vgg-cifar10/
31
Published as a conference paper at ICLR 2021
blob/master/vgg.py, with the dropout layers removed. The ResNet is the (non-fixup) ResNet-
32 implemented here: https://github.com/hongyi- zhang/Fixup.
For the two networks with batch normalization, running gradient descent with full-dataset batch
normalization would not have been feasible under our GPU memory constraints. Therefore, we
instead used ghost batch normalization (Hoffer et al., 2017) with 50 ghost batches of size 1,000.
This means that we divided the 50,000 examples in CIFAR-10 into 50 fixed groups of size 1,000
each, and defined the overall objective function to be the average of 50 fixed batch-wise objectives.
To correctly compute the overall gradient of this training objective, we can just run backprop 50
times (once on each group) and average the resulting gradients.
To compute the sharpness over the full CIFAR-10 dataset would have been computationally expen-
sive. Therefore, in an approximation, we instead computed the sharpness over just the first 5,000
examples in the dataset (or, for the BN networks, over the first 5 batches out of 50).
I.3	Batch normalization experiments
We used the CNN architecture from §J (described above in §I.1), but with a BatchNorm2d()
layer inserted after each activation layer.
Since our GPUs did not have enough memory to run batch normalization with the full dataset of size
5,000, we used ghost batch normalization with five ghost batches of size 1,000 (see I.2 for details).
I.4	Transformer on WikiText-2
We used both the Transformer architecture and the preprocessing setup from the official Py-
Torch (Paszke et al., 2019) word-level language modeling tutorial: https://github.com/
pytorch/examples/tree/master/word_language_model. We used the settings
ninp=200, nhead=2, nhid=200, nlayers=2, dropout=0. We set bptt = 35,
which means that we divided the corpus into chunks of 35 tokens, and trained the network, us-
ing the negative log likelihood loss, to predict each token from the preceding tokens in the same
chunk. Since computing the sharpness over the full dataset would not have been computationally
practical, we computed the sharpness over a subset comprising 2500 training examples.
I.5	Runge- Kutta
We used the “RK4” fourth-order Runge-Kutta algorithm (Press et al., 1992) to numerically integrate
the gradient flow ODE. The Runge-Kutta algorithm requires a step size. Rather than use a sophisti-
cated algorithm for adaptive step size control, we decided to take advantage of the fact that we were
already periodically computing the sharpness: at each step, We set the step size to a∕λ, where α is
a tunable parameter and λ is the most recent value for the sharpness. We set α = 1 or α = 0.5.
I.6	Random projections
In order to ascertain whether gradient descent at step size η initially followed the gradient flow
trajectory (and, if so, for how long), we monitored the `2 distance in weight space between the
gradient flow solution at time t and the gradient descent iterate at step t∕η. One way to do would be
as follows: (a) when running gradient flow, save the weights after every ∆t units of time, for some
parameter ∆t; (b) when running gradient descent, save the weights at each (∆ηt)-th step; (c) plot the
difference between these two sequences. (Note that this approach requires ∆t to be divisible by η.)
We essentially used this approach, but with one modification: regularly saving the entire net-
work weight vector would have consumed a large amount of disk space, so we instead saved low-
dimensional random projections of the network weights. To be clear, let d be the number of net-
work weights, and let k be the number of random projections (a tunable parameter chosen such
that k d). Then we first generated a matrix M ∈ Rk×d by sampling each entry i.i.d from the
standard normal distribution. During training, rather than periodically save the whole weight vector
(a d-dimensional vector), we premultiplied this vector by the matrix M to obtain a k-dimensional
vector, and we periodically saved these vectors instead. Then we plotted the `2 distance between the
low-dimensional vectors from gradient flow, and the low-dimensional vectors from gradient descent.
32
Published as a conference paper at ICLR 2021
J Experiments: vary architectures
In this appendix, we fix the task as that of fitting a 5,000-sized subset of CIFAR-10, and we verify
our main findings across a broad range of architectures.
Procedure We consider fully-connected networks and convolutional networks, the latter with both
max-pooling and average pooling. For all of these, we consider tanh, ReLU, and ELU activations,
and for fully-connected networks we moreover consider softplus and hardtanh activations. We train
each network with both cross-entropy and MSE loss. See §I.1 for full experimental details.
In each case, we first use the Runge-Kutta method to numerically integrate the gradient flow ODE
(see §I.5 for details). For architectures that give rise to continuously differentiable training objec-
tives, the gradient flow ODE is guaranteed to have a unique solution (which we call the gradient flow
trajectory), and Runge-Kutta will return a numerical approximation to this solution. On the other
hand, for architectures with ReLU, hardtanh, or max-pooling, the training objective is not continu-
ously differentiable, so the gradient flow ODE does not necessarily have a unique solution, and there
are no guarantees a priori on what Runge-Kutta will return (more on this below under the “findings”
heading). Still, in both cases, since our implementation of Runge-Kutta automatically adjusts the
step size based on the local sharpness in order to remain stable, the Runge-Kutta trajectory can be
roughly viewed as “what gradient descent would do if instability was not an issue.”
We then run gradient descent at a range of step sizes. These step sizes η were chosen by hand so that
the quantity 2/n would be spaced uniformly between λo (the sharpness at initialization) and λmaχ
(the maximum sharpness along the Runge-Kutta trajectory).
Results During Runge-Kutta, we observe that the sharpness tends to continually increase dur-
ing training (progressive sharpening), with the exception that when cross-entropy loss is used, the
sharpness decreases at the very end of training, as explained in Appendix C.
During gradient descent with step size η, We observe that once the sharpness reaches 2/n, it ceases
to increase much further, and instead hovers right at, or just above, the value 2/n. For reasons
unknown, it tends to be true that for MSE loss, the sharpness hovers just a tiny bit above the value
2/n, while for cross-entropy loss the gap between the sharpness and the value 2/n is a bit larger.
For each step size, we monitor the distance between the gradient descent trajectory and the Runge-
Kutta trajectory — that is, we monitor the distance between the Runge-Kutta iterate at time t and
the gradient descent iterate at step t/n (see §I.6 for details). Empirically, for architectures that
give rise to continuously differentiable training objectives, we observe that this distance is nearly
zero before the sharpness hits 2/n, and it starts to climb immediately afterwards. This means that
gradient descent closely tracks the gradient flow trajectory so long as the sharpness remains less
than 2/n. Note that this finding was not a foregone conclusion: gradient descent is guaranteed
to track the gradient flow trajectory in the limit of infinitesimal step sizes (since gradient descent
is the forward Euler discretization of the gradient flow ODE), but for non-infinitesimal step sizes,
there is discretization error, which is studied in Barrett & Dherin (2021). Our empirical finding
is essentially that this discretization error is small compared to the difference between trajectories
caused by instability.
On the other hand, for architectures with non-differentiable components such as ReLU or max-
pooling, we sometimes observe that gradient descent tracks the Runge-Kutta trajectory so long as the
sharpness remains less than 2/n, but we also sometimes observe that the gradient descent trajectories
differ from one another (and from Runge-Kutta) from the beginning of training. In the former case,
we can infer that the gradient flow trajectory apparently does exist, and is returned by Runge-Kutta;
in the latter case, we can infer that either (a) the gradient flow trajectory does not exist, or (b) that
it does exist (and is returned by Runge-Kutta), but the step sizes we used for gradient descent were
too large to track it.
33
Published as a conference paper at ICLR 2021
J.1 Fully-connected tanh network
J.1.1 S quare loss
4
O
Sso- u
0	20	40	60	80
time
0	20	40	60	80
time
Figure 28: Gradient flow. We train the network to 99% accuracy with gradient flow, by using the
Runge-Kutta method to discretize the gradient flow ODE (details in §I.5). We plot the train loss
(left), sharpness (center), and train accuracy (right) over time. Observe that the sharpness tends to
continually increase (except for a slight decrease at initialization).
0	20	40	60	80
time
loss (iteration)
4ɔ
O C
Sso- u
0	5000	10000	15000
iteration
distance from gradient flow
sharpness (iteration)
tram accuracy (iteration)
Q∙8∙64
Iooo
Aoe-Inbe
40-
I 30-
(o
I 20-
10-
0	5000	10000	15000
iteration
sharpness (time)
0	20	40	60	80	100 120
time
0	5000	10000	15000
iteration
test accuracy (iteration)
-64
O O
Aoe-Inbe
0	5000	10000	15000
iteration
0	20	40	60	80
time
Figure 29: Gradient descent. We train the network to 99% accuracy using gradient descent at a
range of step sizes η. Top left: we plot the train loss curves, with a vertical line marking the iteration
where the sharpness first crosses 2∕η. Observe that the train loss monotonically decreases before this
point, but behaves non-monotonically afterwards. Top middle: we plot the sharpness (measured at
regular intervals during training). For each step size, the horizontal dashed line of the appropriate
color marks the maximum stable sharpness 2∕η. Observe that the sharpness tends to increase during
training until reaching the value 2∕η, and then hovers just a bit above that value. Bottom left: We
track the `2 distance between (random projections of) the gradient flow iterate at time t and the
gradient descent iterate at iteration t/n (details in §I.6). For each step size η, the vertical dotted
line marks the time when the sharpness first crosses 2/n. Observe that the distance is essentially
zero until this time, and starts to grow afterwards. From this, we can conclude that gradient descent
closely tracks the gradient flow trajectory (moving at a speed η) until the point on that trajectory
where the sharpness reaches 2/n. Bottom middle: to further visualize the previous point, we plot
the evolution of sharpness during gradient descent, but with time (= iteration × step size) on the
x-axis rather than iteration. We plot the gradient flow sharpness in black. Observe that the gradient
descent sharpness matches the gradient flow sharpness until reaching the value 2/n.
34
Published as a conference paper at ICLR 2021
J.1 .2 Cross-entropy loss
sharpness
Figure 30: Gradient flow. Refer to the Figure 28 caption for more information. Additionally, in this
figure the sharpness drops at the end of training due to the cross-entropy loss.
ss-ush
train loss (iteration)
0-
5-
0-
5-
sharpness (iteration)
〉 g
〉 5
ss3ud,ιeqs
O
O 500 IOOO 1500 2000 2500
iteration
so distance from gradient flow
40-
830-
e
⅞20-
10-
0	500	1000 1500 2000 2500
iteration
sharpness (time)
Aue∙lnbe
test accuracy (iteration)
150
S 100
U
e-
e
ω 50
8
6
4
2
Ausnbe
0	500	1000 1500 2000 2500
iteration
Figure 31: Gradient descent. Refer to the Figure 29 caption for more information.
35
Published as a conference paper at ICLR 2021
J.2 Fully-connected ELU network
J.2. 1 S quare loss
train loss
0.6-
4
O
Sso- u
SSgUReqS
600-
Sharpness
Uura u"h
train accuracy
ι.o
0	20	40	60	80
time
0	20	40	60	80
time
0	20	40	60	80
time
Figure 32:	Gradient flow. Refer to the Figure 28 caption for more information.
tram loss (iteration)
500-
S 400-
M
ω
∣300-
e
ɪ 200-
100-
Iteration
sharpness (time)
600-
g 400-
e-
e
S 200-
5000	10000 15000 20000
iteration
distance from gradient flow
traιn accuracy (iteration)
——η = 2/100
η = 2/200
——[ = 2∕300
——η = 2/400
——η = 2/500
5000	10000 15000 20000
iteration
test accuracy (iteration)
5000	10000 15000 20000
iteration
Figure 33:	Gradient descent. Refer to the Figure 29 caption for more information.
36
Published as a conference paper at ICLR 2021
J.2.2 Cross-entropy loss
Figure 34: Gradient flow. Refer to the Figure 28 caption for more information. Additionally, in this
figure the sharpness drops at the end of training due to the cross-entropy loss.
sharpness
ss-ush
tram loss (iteration)
0.0
0	500 1000 1500 2000 2500
iteration
distance from gradient flow
Aue∙lnbe
tram accuracy (iteration)
80-
O 60-
e
S4θ-
20-
0-
0	10	20	30	40
time
Figure 35: Gradient descent. Refer to the Figure 29 caption for more information.
Ausnbe
500	1000 1500 2000 2500
iteration
test accuracy (iteration)
500	1000 1500 2000 2500
iteration
37
Published as a conference paper at ICLR 2021
J.3 Fully-connected softplus network
J.3. 1 S quare loss
traιn accuracy
1.0-
0.8-
g 0.6-
S 0.4-
0.2-
SSgUReUS
0	50	100	150
time
0	50	100	150
time
0	50	100	150
time
Figure 36: Gradient flow. Refer to the Figure 28 caption for more information.
1.00-
w 0.75-
c 0.50-
E
“0.25-
0.00
traιn loss (iteration)

200
distance from gradient flow
800
600
400
200
0	10000 20000 30000 40000 50000
iteration
sharpness (iteration)
600
c 400
0	10000 20000 30000 40000 50000
iteration
sharpness (time)
ιooo
SSgUReUS
0	10000 20000 30000 40000 50000
iteration
0	10000 20000 30000 40000 50000
iteration
Figure 37: Gradient descent. Refer to the Figure 29 caption for more information.
tram accuracy (iteration)
ι.o--------------------------------------
test accuracy (iteration)
38
Published as a conference paper at ICLR 2021
J.3.2 Cross-entropy loss
Figure 38: Gradient flow. Refer to the Figure 28 caption for more information. Additionally, in this
figure the sharpness drops at the end of training due to the cross-entropy loss.
2.0
1.5
1.0
0.5
0.0
0	1000 2000 3000 4000 5000 6000
iteration
sharpness (iteration)
test accuracy (iteration)
1.0
distance from gradient flow
O
O IO 20	30	40	50	60
time
200-
S 150-
c
0 100-
U
IΛ
50-
sharpness (time)
0	20	40	60	80	100 120
time
Figure 39: Gradient descent. Refer to the Figure 29 caption for more information.
Ausnbe
0.2 -
0.0
0	1000 2000 3000 4000 5000 6000
iteration
39
Published as a conference paper at ICLR 2021
J.4 Fully-connected ReLU network
Note that since the ReLU activation function is not continuously differentiable, the training objective
is not continuously differentiable, and so a unique gradient flow trajectory is not guaranteed to exist.
J.4. 1 S quare loss
Sso- u5h
0	20	40	60
time
0	20	40	60
time
Figure 40: Runge-Kutta. We train the network to 99% accuracy using the Runge-Kutta algorithm.
(Since a unique gradient flow trajectory is not guaranted to exist, we hesitate to call this “gradient
flow”; Runge-Kutta should essentially be viewed as gradient descent with a very small step size.)
Observe that the sharpness tends to continually increase.
0	20	40	60
time
train loss (iteration)
sharpness (iteration)
0	1000 2000 3000 4000 5000
iteration
sharpness (time)
train accuracy (iteration)
0	1000 2000 3000 4000 5000
iteration
40-
(u
S
S 20-
0	1000 2000 3000 4000 5000
iteration
distance from gradient flow
time
150-
S
α)
售100-
e
■C
IΛ
50-
0	20	40	60	80	100 120
time
test accuracy (iteration)
4
0	1000 2000 3000 4000 5000
iteration

Figure 41: Gradient descent. All panes except bottom left: refer to the Figure 29 caption for
more information. Bottom left: We track the `2 distance between (random projections of) the
RUnge-KUtta iterate at time t and the gradient descent iterate at iteration t∕η. For each step size η,
the vertical dotted loss marks the time when the sharpness first crosses 2∕η. In contrast to Figure
29, here the distance between gradient descent and gradient flow starts to noticeably grow from the
beginning of training. From this we conclUde that for this architectUre, gradient descent does not
track the RUnge-KUtta trajectory at first. FUrthermore, for this architectUre, observe that the training
loss starts behaving non-monotonically (and the sharpness begins to plateaU) before the sharpness
hits 2∕η. As discussed in Appendix A ("Caveats"), We believe that this is also due to the fact that
ReLU is non-differentiable.
40
Published as a conference paper at ICLR 2021
J.4.2 Cross-entropy loss
Sso- ush
train loss
0	10	20	30	40
sharpness
0 5g 5
0 7 5 2
SSgUReqS
0	10	20	30	40
Uura u"h
train accuracy
0	10	20	30	40
time
time
time
Figure 42: Runge-Kutta. Refer to the Figure 40 caption for more information. Additionally, in this
figure the sharpness drops at the end of training due to the cross-entropy loss.
ss-u"h
tram loss (iteration)
Aue∙lnbe
tram accuracy (iteration)
3UU£-P
500	1000	1500
iteration
distance from gradient flow
10	20	30	40
time
Figure 43: Gradient descent. Refer to the Figure 41 caption for more information.
500	1000	1500
iteration
test accuracy (iteration)
Ausnbe
500	1000	1500
iteration
41
Published as a conference paper at ICLR 2021
J.5 Fully-connected hard tanh network
Note that since the hardtanh function is not continuously differentiable, the training objective is not
continuously differentiable, and so a unique gradient flow trajectory is not guaranteed to exist.
J.5. 1 S quare loss
Sso- u5h
tram loss
o
20
60
80
40
time
Figure 44: Runge-Kutta. Refer to the Figure 40 caption for more information.
4
O
Sso- u
tram loss (iteration)
800-
B 600-
ω
c.
⅛ 400-
£
200-
10000	20000	30000	0
iteration
sharpness (iteration)
10000	20000
iteration
30000
tram accuracy (iteration)
10000	20000	30000
iteration
Aue∙lnbe
distance from gradient flow
800
S 600
U
e-
2 400
200
sharpness (time)
0	20	40	60	80	100
time
Figure 45: Gradient descent. Refer to the Figure 41 caption for more information.
Ausnuue
test accuracy (iteration)
0	10000	20000	30000
iteration
42
Published as a conference paper at ICLR 2021
J.5.2 Cross-entropy loss
Figure 46: Runge-Kutta. Refer to the Figure 40 caption for more information. Additionally, in this
figure the sharpness drops at the end of training due to the cross-entropy loss.
train loss (iteration)
o-
5-
o-
5-
ss-ush
O
O 500 IOOO 1500 2000 2500 3000
iteration
150
sharpness (iteration)	C tram accuracy (iteration)
g 100
B
e
ɪ 50
Aue∙lnbe
O 500 IOOO 1500 2000 2500 3000
iteration
time
time
0.0
0
Figure 47: Gradient descent. Refer to the Figure 41 caption for more information.
Ausnbe
test accuracy (iteration)
500 1000 1500 2000 2500 3000
iteration
43
Published as a conference paper at ICLR 2021
J.6 Convolutional tanh network with max pooling
Note that since max-pooling is not continuously differentiable, the training objective is not continu-
ously differentiable, and so a unique gradient flow trajectory is not guaranteed to exist.
J.6. 1 S quare loss
0	50	100	150	200
time
sharpness
0	50	100	150	200	0
time
Figure 48: Runge-Kutta. We train the network to 99% accuracy using the Runge-Kutta algorithm.
(Since a unique gradient flow trajectory is not guaranted to exist, we hesitate to call this “gradient
flow.”) Observe that the sharpness tends to continually increase.
50	100	150	200
time
120
S ιoo
M
ω
e- so
e
ɪ 60
40
sharpness (iteration)
tram accuracy (iteration)
Aue∙lnbe
3UU£-P
O 5000 IOOOO 15000
iteration
sharpness (time)
ss3ud.IeqS
Aue∙lnbe
0	5000	10000	15000
iteration
test accuracy (iteration)

Figure 49: All panes except bottom left: refer to the Figure 29 caption for more information.
Bottom left: We track the `2 distance between (random projections of) the Runge-Kutta iterate at
time t and the gradient descent iterate at iteration t∕η. For each step size η, the vertical dotted
loss marks the time when the sharpness first crosses 2∕η. Observe that the distance is essentially
zero until this time, and starts to grow afterwards. From this, we can conclude that even though the
training objective is not differentiable, gradient descent does closely track the Runge-Kutta trajectory
(moving at a speed η) until the point on that trajectory where the sharpness reaches 2∕η.
44
Published as a conference paper at ICLR 2021
J.6.2 Cross-entropy loss
Figure 50: Runge-Kutta. Refer to the Figure 48 caption for more information. Additionally, in this
figure the sharpness drops at the end of training due to the cross-entropy loss.
sharpness
train loss (iteration)
sharpness (iteration)	C tram accuracy (iteration)
0	1000	2000	3000	4000	5000
iteration
sharpness (time)
0	1000	2000	3000	4000	5000
iteration
test accuracy (iteration)
o.o
0	1000	2000	3000	4000	5000
iteration
80
a 60
ω
B 40
■C
IΛ
20
O 25	50	75 IOO 125
time
Figure 51: Gradient descent. Refer to the Figure 49 caption for more information.
Ausnbe
1000	2000	3000	4000	5000
iteration
45
Published as a conference paper at ICLR 2021
J.7 Convolutional ELU network with max pooling
Note that since max-pooling is not continuously differentiable, the training objective is not continu-
ously differentiable, and so a unique gradient flow trajectory is not guaranteed to exist.
J.7. 1 S quare loss
traιn loss
Sso- u5h
6-
5-
4-
3-
2-
sharpness
50
QOQ
0 5 0
W 7 5
ss3ud.IeqS
ι.o-
0.8-
I 0.6-
h 0.4-
0.2-
traιn accuracy
Figure 52:	Runge-Kutta. Refer to the Figure 48 caption for more information.
train loss (iteration)
4 ɔ
O ∩
Sso- u
o.o
0	20000 40000 60000 80000100000
iteration
distance from gradient flow
20-
Qs-
e
⅛ ɪθ-
5-
0-
0	50	100	150	200
time
sharpness (iteration)	C train accuracy (iteration)
O 20000 40000 60000 80000100000
iteration
sharpness (time)
ιooo-
Aue∙lnbe
0	20000 40000 60000 80000100000
iteration
test accuracy (iteration)
Ausnuue
0	20000 40000 60000 80000100000
iteration
Figure 53:	Gradient descent. Refer to the Figure 49 caption for more information.
46
Published as a conference paper at ICLR 2021
J.7.2 Cross-entropy loss
Sharpness
Figure 54: Runge-Kutta. Refer to the Figure 48 caption for more information. Additionally, in this
figure the sharpness drops at the end of training due to the cross-entropy loss.
sharpness (iteration)	tram accuracy (iteration)
ss-ush
train loss (iteration)
0-
5-
0-
5-
Ss3ud」eus
0
0	5000 10000 15000 20000 25000
iteration
1000
0
0	5000 10000 15000 20000 25000
iteration
-64
O O
Aue∙lnbe
0.0
0	5000 10000 15000 20000 25000
iteration
50
40-
830-
e
⅞20-
10-
0-
0	10
distance from gradient flow
S 1000
IΛ
ω
U
e-
ɪ 500
0
20	30	40	50
time
sharpness (time)
test accuracy (iteration)
Q∙8∙64∙2Q
Iooooo
Aoe-Inbe
5000 10000 15000 20000 25000
iteration
Figure 55: Gradient descent. Refer to the Figure 49 caption for more information.
47
Published as a conference paper at ICLR 2021
J.8 Convolutional ReLU network with max pooling
Note that since ReLU and max-pooling are not continuously differentiable, the training objective is
not continuously differentiable, and so a unique gradient flow trajectory is not guaranteed to exist.
J.8. 1	S quare loss
Figure 56: Runge-Kutta. Refer to the Figure 48 caption for more information.
o.o
0	20000 40000 60000 80000 100000
iteration
tram accuracy (iteration)
Aue∙lnbe
20000 40000 60000 80000 100000
iteration
0	20000 40000 60000 80000 100000
so distance from gradient flow
sharpness (time)
iteration
0	50	100	150	200	0	50	100 150 200 250 300
time	time
Figure 57: Gradient descent. Refer to the Figure 49 caption for more information.
test accuracy (iteration)
Ausnuue
20000 40000 60000 80000 100000
iteration
48
Published as a conference paper at ICLR 2021
J.8.2 Cross-entropy loss
train loss
0	10	20	30	40	50
time
Figure 58: Runge-Kutta. Refer to the Figure 48 caption for more information. Additionally, in this
figure the sharpness drops at the end of training due to the cross-entropy loss.
sharpness
ss-ush
train loss (iteration)
0-
5-
0-
5-
ss3ud,ιeqs
sharpness (iteration)
5000	10000	15000	20000
iteration
0	5000	10000	15000	20000
iteration
sharpness (time)
50-
40-
S3O-
(0
I 20-
10-
0-'
0
distance from gradient flow
SSgUReUS
00
20
000°
Q∙8∙64∙2Q Q∙8∙64∙2Q
Iooooo Iooooo
Aoe.Inbe Aoe.Inbe
train accuracy (iteration)
0	5000	10000	15000 20000
iteration
test accuracy (iteration)
0	20	40	60
time
0	5000	10000	15000	20000
iteration
W

呷[∣l [	，
Figure 59: Gradient descent. Refer to the Figure 49 caption for more information.
49
Published as a conference paper at ICLR 2021
J.9 Convolutional tanh network with average pooling
J.9. 1 Cross-entropy loss
Figure 60: Gradient flow. Refer to the Figure 28 caption for more information. Additionally, in this
figure the sharpness drops at the end of training due to the cross-entropy loss.
ss-ush
train loss (iteration)
distance from gradient flow
1000	2000	3000	4000
iteration
sharpness (iteration)
tram accuracy (iteration)
50-
40-
83o-
e
S≡0-
10-
O-
O
50	100	150	200	250	300
time
sharpness (time)
Figure 61: Gradient descent. Refer to the Figure 29 caption for more information.
1000	2000	3000	4000
iteration
test accuracy (iteration)
1000	2000	3000	4000
iteration
50
Published as a conference paper at ICLR 2021
J.1 0 Convolutional ELU network with average pooling
J.10.1 Square loss
train accuracy
Figure 62: Gradient flow. Refer to the Figure 28 caption for more information.
train loss (iteration)
400-
S 300-
(u
C
g-200-
■C
IΛ
100-
sharpness (iteration)
train accuracy (iteration)
Ausnbe
O 50000 IOOOOO 150000 200000
iteration
sharpness (time)
ιooo -	/
0	50000 100000 150000 200000
iteration
test accuracy (iteration)
Figure 63: Gradient descent. Refer to the Figure 29 caption for more information.
Aue∙lnbe
o.o
0	50000 100000 150000 200000
iteration
51
Published as a conference paper at ICLR 2021
J.10.2 Cross-entropy loss
Figure 64: Gradient flow. Refer to the Figure 28 caption for more information. Additionally, in this
figure the sharpness drops at the end of training due to the cross-entropy loss.
ss-ush
train loss (iteration)
0-
5-
0-
5-
Ss3ud」eus
750
500
250
0
sharpness (iteration)
ιooo
0	10000	20000	30000	40000
iteration
sharpness (time)
tram accuracy (iteration)
o.
0	10000	20000	30000	40000
iteration
so distance from gradient flow
40-
830-
e
⅞20-
10-
0-
0	20
10000	20000	30000	40000
iteration
test accuracy (iteration)
time
Figure 65: Gradient descent. Refer to the Figure 29 caption for more information.
10000	20000	30000	40000
iteration
52
Published as a conference paper at ICLR 2021
J.1 1 Convolutional ReLU network with average pooling
Note that since ReLU is not continuously differentiable, the training objective is not continuously
differentiable, and so a unique gradient flow trajectory is not guaranteed to exist.
J.1 1.1 Square loss
Figure 66: Runge-Kutta. Refer to the Figure 48 caption for more information.
tram loss (iteration)	sharpness (iteration)
Aue∙lnbe
tram accuracy (iteration)
50000 100000150000200000250000
iteration
test accuracy (iteration)
Figure 67: Gradient descent. Refer to the Figure 49 caption for more information.
50000 100000150000200000250000
iteration
53
Published as a conference paper at ICLR 2021
J.1 1.2 Cross-entropy loss
Figure 68: Runge-Kutta. Refer to the Figure 48 caption for more information. Additionally, in this
figure the sharpness drops at the end of training due to the cross-entropy loss.
ss-ush
O.
O IOOOO 20000 30000 40000 50000
iteration
distance from gradient flow
20-
Qs-
e
⅛ ɪθ-
5-
0
0	20	40	60	80	100
time
0	10000 20000 30000 40000
iteration
sharpness (time)
S 2000
0
e-
2 woo
25	50	75	100	125
time
Aue∙lnbe
tram accuracy (iteration)
10000 20000 30000 40000 50000
iteration
test accuracy (iteration)
0.0
0	10000 20000 30000 40000 50000
iteration
Ausnbe
Figure 69: Gradient descent. Refer to the Figure 49 caption for more information.
54
Published as a conference paper at ICLR 2021
K	Batch Normalization experiments
In this appendix, we demonstrate that our findings hold for networks that are trained with batch
normalization (BN) (Ioffe & Szegedy, 2015). We experiment on a size-5,000 subset of CIFAR-
10, and we consider convolutional networks with three different activation functions: ELU (Figure
70-71, tanh (Figure 72-73), and ReLU (Figure 74-75). See §I.3 for experimental details.
Empirically, our findings hold for batch-normalized networks. The one catch is that when training
batch-normalized networks at very small step sizes, it is apparently inadequate to measure the sharp-
ness directly at the iterates themselves, as we do elsewhere in the paper. Namely, observe that in
Figure 71(e), when we run gradient descent at the red step size, the sharpness (measured directly at
the iterates) plateaus a bit beneath the value 2/n. At first, this might sound puzzling: after all, if the
sharpness is less than 2/n then gradient descent should be stable. The explanation is that the sharp-
ness in between successive iterates does in fact cross 2/n. In Figure 71(b), we track the maximum
sharpness on the path “in between” successive iterates. (To estimate the maximum sharpness be-
tween a pair of successive iterates, we compute the sharpness at a grid of eight points spaced evenly
between them, and then take the maximum of these values.) Observe that this quantity does rise to
2/n and hover there. We do not know why measuring the sharpness between iterates is necessary
for batch-normalized networks, whereas for non-BN networks it suffices to measure the sharpness
only at the iterates themselves.
In §K.1, we reconcile these findings with Santurkar et al. (2018).
SsO-U∙J-
Figure 70: We train a ELU CNN (+ BN) using gradient flow.
sssU∙J-
(b) sharpness between iterates (iteration)
0	5000	10000	15000
iteration
(c) sharpness between iterates (time)
0	5000	10000	15000
iteration
(d) train accuracy (iteration)
0	5000	10000	15000
iteration
(e) sharpness at iterate (iteration)
SSaUdJeUS
0	5000	10000	15000
iteration
O 5 IO 15	20
time
(f) distance from gradient flow
Figure 71: We train an ELU CNN (+ BN) using gradient decent at a range of step sizes. (a) we
plot the train loss, with a vertical dotted line marking the iteration where the sharpness on the path
first crosses 2∕η. The inset shows that the red curve is indeed behaving non-monotonically. (b) We
track the sharpness “between iterates.” This means that instead of computing the sharpness right
at the iterates themselves (as we do elsewhere in the paper, and in pane (e) here), we compute the
maximum sharpness on the line between between successive iterates. Observe that this quantity
rises to 2/n (marked by the horizontal dashed line) and then hovers right at, OrjUSt above that value.
(c) we plot the same quantity by “time” (= iteration × step size) rather than iteration. (e) we plot the
sharpness at the iterates themselves. Note that for the red step size, this quantity plateaus at a value
that is beneath 2/n. (f) distance from the gradient flow trajectory.
55
Published as a conference paper at ICLR 2021
train loss
Figure 72: We train a tanh CNN (+ BN) using gradient flow.
(a) train loss (iteration)
(b) sharpness between iterates (iteration)
(c) sharpness between iterates (time)
0	5000 10000 15000 20000
iteration
2°1°
8uels-p
Figure 73: We train a tanh CNN (+ BN) using gradient decent.
train loss
sssU∙J-
sharpness
1000-ι------------------------
Figure 74: We train a ReLU CNN (+ BN) using gradient flow.
train accuracy
1.00
Auejn8e
(a) train loss (iteration)
(b) sharpness between iterates (iteration)
BOO-----------r∙vvr∏-
600 -----------------
a。LTB-∙一
0	1000	2000	3000
iteration
BOO
600
400
200
(c) sharpness between iterates (time)
(e) sharpness at iterate (iteration)
B00
产
∣1400
200
iteration
^o i
0	2	4	6	8
time
(f) distance from gradient flow
Eels-P
Figure 75:	We train a ReLU CNN (+ BN) using gradient descent.
K. 1 Relation to Santurkar et al. (2018)
We have demonstrated that the sharpness hovers right at (or just above) the value 2/n when both
BN and non-BN networks are trained using gradient descent at reasonable step sizes. Therefore, at
least in the case of full-batch gradient descent, it cannot be said that batch normalization decreases
the sharpness (i.e. improves the local L-smoothness) along the optimization trajectory.
56
Published as a conference paper at ICLR 2021
Santurkar et al. (2018) argued that batch normalization improves the effective smoothness along
the optimization trajectory, where effective smoothness is defined as the Lipschitz constant of the
gradient in the update direction (i.e. the negative gradient direction, for full-batch GD). That is, given
an objective function f, an iterate θ, and a distance α, the effective smoothness of f at parameter θ
and distance α is defined in Santurkar et al. (2018) as
sup
γ∈[0,α]
kVf (θ) -Vf (θ - γVf (θ))k2
kγVf (θ)k2
where the sup can be numerically approximated by evaluating the given ratio at several values γ
spaced uniformly between 0 and α.
In Figure 76, we train two ReLU CNNs — one with BN, one without — at the same set of step sizes,
and we monitor both the sharpness (i.e. the L-smoothness) and the effective smoothness. When
computing the effective smoothness, we use a distance α = η. Observe that for both the BN and the
non-BN network, the effective smoothness initially hovers around zero, but once gradient descent
enters the Edge of Stability, the effective smoothness jumps to the value 2/n and then remains there.
Thus, at least for full-batch gradient descent on this particular architecture, batch normalization does
not improve the effective smoothness along the optimization trajectory. (Despite this, note that for
each step size, the BN network trains faster than the non-BN network, confirming that BN does
accelerate training.)
train loss
1000-
750-
500-
250-
0-
effective smoothness
0	500 1000 1500 2000 2500 3000
2.0
1.5
1.0
0.5
ReLU CNN without Batch Normalization
effective smoothness
Figure 76:	On a 5,000-size subset of CIFAR-10, we train a ReLU CNN both with BN (top row) and
without BN (bottom row) at the same grid of step sizes. We plot the sharpness/smoothness (center
column) as well as the effective smoothness (Santurkar et al., 2018) (right column). Observe that
for both networks, the effective smoothness hovers around zero initially, and jumps up to 2/n once
gradient descent enters the Edge of Stability.
Note that this finding is actually consistent with Figure 4(c) in Santurkar et al. (2018), which is meant
to show that BN improves effective smoothness when training a VGG network using SGD. Their
Figure 4(c) shows that during SGD with step size η = 0.1, the effective smoothness hovers around
the value 20 for both the BN and the non-BN network. Since 20 = 2/(0.1), this is fully consistent
with our findings (though they use SGD rather than full-batch GD). Figure 4(c) does show that the
effective smoothness behaves more regularly for the BN network than for the non-BN network. But
we disagree with their interpretation of this figure as demonstrating that BN improves the effective
smoothness during training.
The other piece of evidence in Santurkar et al. (2018) in support of the argument that batch normal-
ization improves the effective smoothness during training is their Figure 9(c). This figure shows that
a deep linear network (DLN) trained without BN has a much larger (i.e. worse) effective smooth-
ness during training than a DLN trained with BN. However, for this figure, the distance α used to
compute effective smoothness was larger than the training step size η by a factor of 30. The effec-
tive smoothness at distances larger than the step size does not affect training. We have verified that
57
Published as a conference paper at ICLR 2021
when effective smoothness is computed at a distance equal to the training step size (i.e. α = η), the
effective smoothness for the DLN with BN and for the DLN without BN both hover right at 2∕η.
Specifically, in Figure 77 and Figure 78, we train a DLN both with and without BN (respectively),
and we measure the effective smoothness at a distance α = 30η, as done in Figure 9(c) of Santurkar
et al. (2018). We use the same experimental setup and the same step size ofη = 1e-6 as they do, and
we repeat the experiment across four random seeds. Observe that when training the BN network,
the effective smoothness hovers right at 2/n (marked by the horizontal black line), whereas when
training the non-BN network, the effective smoothness is much larger. This is consistent with Figure
9(c) in Santurkar et al. (2018). However, in Figure 79 and 80, we measure the effective smoothness
at the actual step size α = η. When effective smoothness is computed in this way, we observe that
for both the network with BN and the network without BN, the effective smoothness hovers right at
2/n. Therefore, We conclude that there is no evidence that the use of batch normalization improves
either the smoothness or the effective smoothness along the optimization trajectory. (That said, this
experiment possibly explains why the batch-normalized network permits training with larger step
sizes.)
seed 0	seed 1	seed 2	seed 3
SSgUEOOES
Figure 77: When training a deep linear network without BN, we measure effective smoothness at a
distance α = 30η that is 30x larger than the step size.
seed 0
seed 1
seed 2
seed 3
ιo5 -li----------1---------∣j
0	5000	10000
iteration
IO5 j-ι---------1----------∣j
0	5000	10000
iteration
IO5 -l∣---------1----------Γl IO5 -l∣-------------1---------Γl
0	5000	10000	0	5000	10000
iteration	iteration
Figure 78: When training a deep linear network with BN, we measure effective smoothness at a
distance α = 30η that is 30x larger than the step size.
seed 0
seed 2
seed 3
IO13 -	——2/r)
IO9-
iteration
IO13 -
IO9-
seed 1
IO5 -1----------1----------∣j
0	5000	10000
iteration
IO13 -	-- 2∕∏
IO9-
IO13 -	-- 2∕∏
IO9-
IO5 j-ι---------1----------∣j
0	5000	10000
iteration
IO5 j-ι---------1----------∣j
0	5000	10000
iteration


SS3U£。。ES
Figure 79:	When training a deep linear network without BN, we measure effective smoothness at a
distance α = η that is equal to the step size.
58
Published as a conference paper at ICLR 2021
SSgUEOOES
seed O
3 9
ɪ0110
O
seed 1
1O1≡ -	- 2∕∏
W9-
1O5 L I ：
IOOOO O	5000	IOOOO
iteration
5000
iteration
seed 2
1O1≡ -	- 2/r)
IO9-
IO5 I」	：	：
0	5000	10000
iteration
seed 3
1O1≡ -	- 2∕∏
IO9-
IO5 L ：	：
0	5000	10000
iteration
Figure 80:	When training a deep linear network with BN, we measure effective smoothness at a
distance α = η that is equal to the step size.
59
Published as a conference paper at ICLR 2021
L Additional Tasks
So far, we have verified our findings on image classification and language modeling. In this ap-
pendix, we verify our findings on three additional tasks: training a Transformer on the WikiText-2
language modeling dataset (L.1), training a one-hidden-layer network on a one-dimensional toy
regression task (L.2), and training a deep linear network (L.3).
L.1 Transformer on WikiText- 2
We consider the problem of training a Transformer on the WikText-2 word-level language modeling
dataset (Merity et al., 2016). See §I.4 for full experimental details. In Figure 81, we train using
gradient flow (only partially, not to completion). Observe that the sharpness continually rises. In
Figure 82, we train using gradient descent at a range of step sizes. Consistent with our general
findings, for each step size η, We observe that the sharpness rises to 2/n and then hovers right at,
or just above, that value. However, for this Transformer, we do not observe that gradient descent
closely tracks the gradient floW trajectory at the beginning of training.
Sso- ush
Figure 81: Training a Transformer using gradient flow.
sharpness
50	100	150	200
time
train loss (iteration)
Sso- ≡eh
sharpness (iteration)
0	2500 5000 7500 10000 12500 15000
iteration
sharpness (time)
time
二二嬴赢W
_± -S-t_ ±一r
50	100	150	200
time
Figure 82: Training a Transformer using gradient descent. We train a Transformer for 15,000
iterations on the WikiText-2 language modeling dataset. (For this problem, training to completion
Would not be computationally practical.) Top left: We plot the train loss curves, With a vertical
dotted line marking the iteration where the sharpness first crosses 2/n. The train loss decreases
monotonically before this line, but behaves non-monotonically afterWards. Top right: We plot the
evolution of the sharpness, with a horizontal dashed line marking the value 2/n. Observe that for
each step size, the sharpness rises to 2/n and then hovers right at, or just above, that value. Bottom
left: for the initial phase of training, we plot the distance between the gradient descent trajectory and
the gradient flow trajectory, with a vertical dotted line marking the gradient descent iteration where
the sharpness first crosses 2/n. Observe that the distance begins to rise from the start of training,
indicating that for this architecture, gradient descent does not track the gradient flow trajectory
initially. Bottom right: for the initial phase of training, we plot the evolution of the sharpness by
“time” = iteration × n rather than iteration. The black dots are the sharpness during gradient flow.
60
Published as a conference paper at ICLR 2021
L.2 One-dimensional toy regression task
Task Our toy regression problem is to approximate a Chebyshev polynomial using a neural net-
work. To generate a toy dataset, we take 20 points spaced uniformly on the interval [-1, 1], and we
label them noiselessly using the Chebyshev polynomial of some degree k. Note that the Chebyshev
polynomial of degree k is a polynomial with k zeros that maps the domain [-1, 1] to the range
[-1, 1]. Figure 83 shows the Chebyshev datasets for degree 3, 4, and 5.
Chebyshev degree 5
Chebyshev degree 3	Chebyshev degree 4
ι.o-
0.5-
0.0-
-0.5-
-1.0-
-1.0	-0.5	0.0	0.5	1.0	-1.0	-0.5	0.0	0.5	1.0	-1.0	-0.5	0.0	0.5	1.0
Figure 83: The datasets for the toy one-dimensional regression task.
Network For the network, we use a tanh architecture with one hidden layer of h = 100 units,
initialized using Xavier initialization. We train using the MSE loss until the loss reaches 0.05.
Results In Figure 84, we fit the Chebyshev degree 3, 4, and 5 datasets using gradient flow. Em-
pirically, the higher the degree, the more the sharpness rises during the course of training: on the
degree 3 polynomial, the sharpness rises by a factor of 1.2; on the degree 4 polynomial, by a factor
of 3.2; and on the degree 5 polynomial, by a factor of 63.5.
In Figure 85 and Figure 86, we fit the degree 4 and 5 datasets using gradient descent at a range of
step sizes. We observe mostly the same Edge of Stability behavior as elsewhere in the paper. The
only difference is that for the degree 5 dataset, after the sharpness hits 2∕η,the training loss first Un-
dergoes a temporary period of non-monotonicity in which no progress is made, and then it decreases
monotonically Until training is finished (in contrast to oUr other experiments where we observe that
training loss behaves non-monotonically at the same time as it is consistently decreasing).
61
Published as a conference paper at ICLR 2021
train loss ChebySheV degree 3	sharpness
o.o
0	10
20	30	40	50
time
0-.	.
0	10
20	30
time
40	50
... Chebyshev degree 4	.
train loss y a	sharpness
0.0
0
SSo- u-e匕
100	200	300	400
time
W
SSeUdJES
100	200	300	400
time
sharpness
0- I
0
Chebyshev degree 5
train loss
Figure 84: Fitting Chebyshev polynomials using gradient flow. We use gradient flow to fit a
one-hidden-layer tanh network to the Chebyshev polynomials of degrees 3, 4, and 5. We observe
that the sharpness rises more when fitting a higher-degree polynomial, likely because the dataset is
more complex.
SSeUd-ES
200-
100-
0
0	2000	4000	6000
time
5
• ∩ = 2/6
• η = 2∕8
• η = 2/10
distance from gradient flow
ωuuewp
O
O 500 IOOO 1500 2000
iteration
sharpness (time)
Figure 85: Chebyshev degree 4 (gradient descent). We fit the Chebyshev polynomial of degree 4
using gradient descent at a range of step sizes (see legend).
62
Published as a conference paper at ICLR 2021
0.3-
0.2-
0.1-
0.0
0	200000 400000 600000 800000
iteration
sharpness (iteration)
2/50
2/100
2/150
2/200
distance from gradient flow
Figure 86: Chebyshev degree 5 (gradient descent). We fit the Chebyshev polynomial of degree 5
using gradient descent at a range of step sizes (see legend).
sharpness (time)
L.3 Deep linear network
Task The task is to map n inputs x1 , . . . , xn ⊆ Rd to n targets y1 , . . . , yn ⊆ Rd using a function
f : Rd → Rd. Error is measured using the square loss, i.e. the objective is 1 PZi kf (Xi) - Yik2∙
Let X ∈ Rn×d be the vertical stack of the inputs, and let Y ∈ Rn×d be the vertical stack of the
targets. We first generate X as a random whitened matrix (i.e. 1 XTX = I). To generate X as a
random whitened matrix, We sample a n X d matrix of standard Gaussians, and then set X to be √n
times the Q factor in the QR factorization of that matrix. We then generate Y via Y = XAT , where
A ∈ Rd×d is a random matrix whose entries are sampled i.i.d from the standard normal distribution.
We use n = 50 datapoints with a dimension of d = 50.
Network The function f : Rd → Rd is implemented as a L-layer deep linear network: f(X) =
WL ... W2 W ix, with w` ∈ Rd×d. We initialize all layers of the deep linear network using Xavier
initialization: all entries of each w` are drawn i.i.d from N(0, d).
We use a network with L = 20 layers.
Results In Figure 87, we train the network using gradient flow. (Since it is unclear whether the
network can be trained to zero loss, and how long this would take, we arbitrarily chose to stop
training at time 100.) In Figure 88, we train the network using gradient descent at a range of step
sizes. We observe mostly the same Edge of Stability behavior as elsewhere in the paper. The only
difference is that in Figure 88, the train loss does not really behave non-monotonically — for each
step size η, there is a brief blip at some point, but otherwise, the train loss decreases monotonically.
63
Published as a conference paper at ICLR 2021
sspush
tram loss	sharpness
SSo--e 上
Figure 87: Training a deep linear network using gradient flow. We use gradient flow to train a
deep linear network. Since it is unclear whether this network can be trained to zero loss (or how
long that would take), we arbitrarily chose to stop training at time 100.
train loss (iteration)
100
5 0 5
7 5 2
0 -j-∣----1-----1-----1-----1-----H
0	5000 10000150002000025000
iteration
ssωud,Jeus
• r∕ = 2∕1000
• η = 2/1500
• η = 2/2000
• η = 2∕2500
0	5000 10000150002000025000
iteration
distance from gradient flow
3000 -
T	0
1.0	0
ssωud,Jeus
2000
1000
sharpness (time)
5	10	15	20
time
Figure 88: Training a deep linear network using gradient descent. We train a deep linear network
using gradient descent at a range of step sizes (see legend). (Note that in the top left pane, the blue,
orange and green dotted lines are directly on top of one another.)
64
Published as a conference paper at ICLR 2021
M	Experiments: standard architectures on CIFAR- 1 0
In this appendix, we demonstrate that our findings hold for three standard architectures on the stan-
dard dataset CIFAR-10. The three architectures are: a VGG with batch normalization (Figures
89-90), a VGG without batch normalization (Figures 91-92), and a ResNet with batch normalization
(Figures 93 -94). See §I.2 for full experimental details.
For each of these three architectures, we confirm our main points: (1) so long as the sharpness is
less than 2∕η,the sharpness tends to increase; and (2) if the sharpness reaches 2∕η, gradient descent
enters a regime (the Edge of Stability) in which (a) the training loss behaves non-monotonically,
yet consistently decreases over long timescales, and (b) the sharpness hovers right at, or just above,
the value 2∕η. Moreover, We observe that even though these architectures use the ReLU activation
function (which is not continuously differentiable), gradient descent closely tracks the Runge-Kutta
trajectory until reaching the point on that trajectory where the sharpness hits 2∕η.
Furthermore, We observe that for these three standard architectures, the folloWing additional points
hold: (1) progressive sharpening occurs to a dramatic degree, and (2) stable step sizes are so small
as to be completely unreasonable.
Progressive sharpening occurs to a dramatic degree To assess the degree of progressive sharp-
ening, we train these networks using Runge-Kutta / gradient flow, which can be viewed as gradient
descent with an infinitesimally small step size. (In practice, the Runge-Kutta algorithm does have a
step size parameter, and throughout training, we periodically adjust this step size in order to ensure
that the algorithm remains stable.) Intuitively, training with gradient flow tell us how far the sharp-
ness would rise “if it didn’t have to worry about” instability caused by nonzero step sizes. In Figure
89, we train the VGG with BN to completion (99% training accuracy) using Runge-Kutta / gradient
flow, and find that the sharpness rises from its initial value of 6.38 to a peak value of 2227.6. For the
other two architectures, progressive sharpening occurs to such a degree that it is not computation-
ally feasible for to train using Runge-Kutta / gradient flow all the way to completion. (The reason
is that in regions where the sharpness is high, the Runge-Kutta step size must be made small, so
Runge-Kutta requires very many iterations.) Therefore, we instead train these two networks only
partially. In Figure 91, for the VGG without BN, we find that the sharpness rises from its initial
value of 0.64 to the value 2461.78 at 37.1% accuracy, when we stop training. In Figure 93, for the
ResNet, we find that the sharpness rises from its initial value of 1.07 to the value 760.6 at 43.2%
accuracy, when we stop training. Thus, even though we observed in Appendix D that progressive
sharpening attenuates as the width of fully-connected networks is made larger, it appears that either:
(1) this does not happen for modern families of architectures such as ResNet and VGG, or (2) this
does happen for modern families of architectures, but practical network widths lie on the narrow end
of the scale.
Stable step sizes are so small as to be unreasonable Recall from 3.3 that if λmax is the maximum
sharpness along the gradient flow trajectory, then any stable step size must be less than 2∕λmaχ.
Therefore, for these three architectures, because progressive sharpening occurs to a dramatic degree
(i.e. λmax is extraordinarily large), any stable step size must be extraordinarily small, which means
that training will require many iterations. Yet, at the same time, we find that these three networks
can be successfully trained in far fewer iterations by using larger step step size. This means that
training at a stable step size is extremely suboptimal. We now elaborate on this point:
VGG with BN. For this network, gradient flow terminates at time 15.66, and the maximum
sharpness along the gradient flow trajectory is 2227.6. Therefore, the largest stable step size is
2/2227.59 = 0.000897, and training to completion at this step size would take 14.91/0.000897 =
16622 iterations. Meanwhile, we empirically observe that the network can also be trained to com-
pletion at the much larger step size of η = 0.16 in just 329 iterations. Therefore, using a stable step
size is suboptimal by a factor of at least 16622/329 = 50.5.
For the other two architectures, since we are unable to train to completion using gradient flow, we
are unable to obtain a tight lower bound for the number of iterations required to run gradient descent
to completion at a stable step size. Therefore, by extension, we are unable to compute a tight lower
bound for the suboptimality factor of stable step sizes. As a substitute, we will instead compute
65
Published as a conference paper at ICLR 2021
both: (1) a tight lower bound on the suboptimality of training partially at a stable step size, and (2)
a very loose lower bound on the suboptimality of training to completion at a stable step size.
VGG without BN. For this network, gradient flow reaches 37.1% accuracy at time 8, and the maxi-
mum sharpness up through this point on the gradient flow trajectory is 2461.8. Therefore, the largest
stable step size is 2/2461.8 = 0.00081, and training to 37.1% accuracy at this step size would re-
quire 8/0.00081 = 9, 876 iterations. Meanwhile, we empirically observe that the network can also
be trained to 37.1% accuracy at the larger step size of η = 0.16 in just 355 iterations. Therefore,
when training this network to 37% accuracy, stable step sizes are suboptimal by a factor of at least
9876/355 = 27.8. This is a tight lower bound on the suboptimality of training to 37.1% accuracy at
a stable step size.
To obtain a loose lower bound on the suboptimality of training the VGG without BN to completion
at a stable step size, we note that (a) since the maximum sharpness up through time 8 is 2461.8,
the maximum sharpness along the entire gradient flow trajectory must be at least 2461.8; and (b)
since by time 8 gradient flow has only attained 37.1% training accuracy, the time to reach 99%
accuracy (i.e. completion) must be at least 8. (Note that both of these lower bounds are extremely
loose.) Therefore, training this network to completion at a stable step size would require at least
8/(2/2461.8) = 9, 876 iterations (which is the same number of iterations as training to 37.1%
accuracy at a stable step size). Meanwhile, we find that the network can be trained to completion at
the larger step size of η = 0.16 in just 1782 iterations. Therefore, training to completion at a stable
step size is suboptimal by a factor of at least 9, 876/1782 = 5.54.
ResNet. For this network, gradient flow reaches 43.2% accuracy at time 70, and the maximum
sharpness up through this point on the gradient flow trajectory is 760.6. Therefore, the largest sta-
ble step size is 2/760.6 = 0.0026, and training to 43.2% accuracy at this step size would require
70/0.0026 = 26, 923 iterations. Meanwhile, we empirically observe that the network can also be
trained to 43.2% accuracy at the larger step size of η = 2.0 in just 99 iterations. Therefore, when
training this network to 43.2% accuracy, stable step sizes are suboptimal by a factor of at least
26, 923/99 = 271.9. This is a tight lower bound on the suboptimality of training to 43.2% accuracy
at a stable step size. For the loose lower bound on the suboptimality of training to completion at a sta-
ble step size, note that (by similar reasoning as the VGG-without-BN above), training to completion
at a stable step size must require at least 26, 923 iterations. Meanwhile, we find that the network can
be trained to completion at the larger step size of η = 2.0 in just 807 iterations. Therefore, training
to completion at a stable step size is suboptimal by a factor of at least 26, 923/807 = 33.3.
66
Published as a conference paper at ICLR 2021
0.0	2.5	5.0	7.5 10.0 12.5 15.0
time
sharpness
0.0	2.5	5.0	7.5 10.0 12.5 15.0
time
Figure 89: We train a VGG with BN to completion using gradient flow (Runge-Kutta). Observe
that the sharpness rises dramatically from 6.38 at initialization to a peak of 2227.59.
0.0	2.5	5.0	7.5 10.0 12.5 15.0
time
train loss (iteration)
200 -
100-
sharpness (iteration)
0	250 500 750 1000 1250 1500
iteration
train accuracy (iteration)
0	500	1000	1500
iteration
test accuracy (iteration)
0	500	1000	1500
iteration
distance from gradient flow
0	1	2	3	4	5	6
time
200 -
150-
100-
sharpness (time)
0	500	1000	1500
iteration
Figure 90: We train a VGG with BN to completion using gradient descent at different step sizes
(see legend in the top right pane). Top left: we plot the train loss, with a vertical dotted line
marking the iteration where the sharpness first crosses 2∕η. Top center: We plot the evolution of
the sharpness, with a horizontal dashed line (of the appropriate color) marking the value 2∕η. Top
right: we plot the train accuracy. Bottom left: for the initial phase of training, we monitor the
distance between the gradient descent iterate at iteration t∕η, and the gradient flow solution at time
t, with a vertical dotted line (of the appropriate color) marking the time when the sharpness crosses
2∕η. Observe that the distance between gradient descent and gradient flow is almost zero before
this instant, but starts to rise shortly afterwards. Bottom center: we plot the sharpness by time (=
iteration ×η) rather than iteration. The black dots are the sharpness of the gradient flow trajectory,
which shoots up immediately. Bottom right: we plot the test accuracy.
time
67
Published as a conference paper at ICLR 2021
2.2
I20
1.8
train loss
0	2	4	6	8
time
ss"ud」eus
sharpness
0	2	4	6	8
time
accuracy
0	2	4	6	8
time
Figure 91: We train a VGG without BN to 37.1% accuracy using gradient flow (Runge-Kutta).
Observe that the sharpness rises dramatically from 0.64 at initialization to 2461.78 at 37.1% accu-
racy. We train this network only partway because training this network to completion would be too
computationally expensive: Runge-Kutta runs very slowly when the sharpness is high (because it
is forced to take small steps) and for this network the sharpness is extremely high when the train
accuracy is only 37% (which means that there is a long way to go).
train loss (iteration)
300 -
200 -
100-
sharpness (iteration)
train accuracy (iteration)
iteratioπ
distance from gradient flow
300
200—τ
100 =
0	2000	4000	6000	8000
iteration
sharpness (time)
Figure 92: We train a VGG without BN to completion using gradient descent at a range of step
sizes (see legend in the top right pane). Refer to the Figure 90 caption for more information.
2000	4000	6000	8000
iteration
test accuracy (iteration)
time	time	time
Figure 93: We train a ResNet to 43.2% accuracy using gradient flow (Runge-Kutta). Observe that
the sharpness rises dramatically from 1.07 at initialization to 760.63 at 43.2% accuracy. We train
this network only partway because training this network to completion would be too computationally
expensive: Runge-Kutta runs very slowly when the sharpness is high (because it is forced to take
small steps) and for this network the sharpness is extremely high when the train accuracy is only
42% (which means that there is a long way to go).
68
Published as a conference paper at ICLR 2021
sharpness (iteration)
tram loss (iteration)
100
.∖.,∙'*. -V.	.v-,-√
0	2000	4000	6000
>UE3uun
tram accuracy (iteration)
0	2000	4000	6000
iteration
QoO
6 4 2
8。UeaS-P
iteration
sharpness (time)
5 0 5 0
7 5 2
SsUdJeqS
test accuracy (iteration)
>UE3uun
o
0	2000	4000	6000
iteration
-
Figure 94:	We train a ResNet to completion using gradient descent at a range of step sizes (see
legend in the top right pane). Refer to the Figure 90 caption for more information.
69
Published as a conference paper at ICLR 2021
N Experiments: Momentum
This appendix contains systematic experiments for gradient descent with Polyak momentum and
Nesterov momentum. Our aim is to demonstrate that the sharpness rises until reaching the maxi-
mum stable sharpness (MSS) given by equation 1, and then either plateaus just above that value, or
oscillates around that value.
We experiment on a 5k-sized subset of CIFAR-10, using four architectures: a tanh fully-connected
network (section N.1), a ReLU fully-connected network (section N.2), a tanh convolutional network
(section N.3), and a ReLU convolutional network (section N.4). For each of these four architectures,
we experiment with both the square loss (for classification) and cross-entropy loss. For each archi-
tecture and loss function, we experiment with both Polyak momentum at β = 0.9, and gradient de-
scent with Nesterov momentum at β = 0.9. We run gradient descent at a range of several step sizes
which were chosen by hand so that the MSS’s are approximately spaced evenly. Note that for Polyak
momentum with step size η and momentum parameter β = 0.9, the MSS is 2+η2β = 等.For Nets-
terov momentum with step size η and momentum parameter β = 0.9, the MSS is n；+2^)
We run gradient descent until reaching 99% accuracy.
1.35714
η

We find that the sharpness rises until reaching the maximum stable sharpness (MSS) given by equa-
tion 1, and then either plateaus just above that value, or oscillates around that value. Sometimes
these oscillations are rapid (e.g. Figure 96), sometimes they are a bit slower (e.g. Figure 103), and
sometimes they are slow (e.g. Figure 109).
70
Published as a conference paper at ICLR 2021
N. 1 Fully-connected tanh network
In the leftmost plot, the vertical dotted line marks the iteration where the sharpness first crosses the
MSS. (Note that unlike vanilla gradient descent, momentum gradient descent can sometimes cause
the train loss to increase even when the algorithm is stable (Goh, 2017).) In the middle plot, the
horizontal dashed line marks the MSS.
N.1.1 Square loss
ss-usb
0.0
0	200	400	600	800
iteration
tram loss (iteration)
400-
S 300-
M
ω
g-200-
■C
in
100-
sharpness (iteration)
train accuracy (iteration)
0	200	400	600	800
iteration
0	200	400	600	800
iteration
Aue∙lnbe
Figure 95:	Gradient descent with Polyak momentum, β = 0.9.
tram loss (iteration)
0.6-
° 0.4-
£
E
i 0.2-
o.o
0	500	1000	1500	2000 2500
iteration
sharpness (iteration)
0	500	1000	1500	2000	2500
iteration
train accuracy (iteration)
Ausnbe
0	500	1000	1500	2000 2500
iteration
Figure 96:	Gradient descent with Nesterov momentum, β = 0.9.
N.1.2 Cross-entropy loss
train loss (iteration)
Sso- ush
sharpness (iteration)
0 5g 5
0 7 5 2
SSgUReUS
Aoe-Inbe
train accuracy (iteration)
1.0
0.8
0.6
0.4
0.2
0.0
0	50	100	150
iteration
0	50	100	150
iteration
train loss (iteration)
Figure 97: Gradient descent with Polyak momentum, β
Ausnbe
iteration
0	100	200	300	400
iteration
0.9.
train accuracy (iteration)
0	100	200	300	400
iteration
Figure 98: Gradient descent with Nesterov momentum, β
0.9.
71
Published as a conference paper at ICLR 2021
N.2 Fully-connected ReLU network
In the leftmost plot, the vertical dotted line marks the iteration where the sharpness first crosses the
MSS. (Note that unlike vanilla gradient descent, momentum gradient descent can sometimes cause
the train loss to increase even when the algorithm is stable (Goh, 2017).)
In the middle plot, the horizontal dashed line marks the MSS.
N.2.1 Square loss
sharpness (iteration)
0	50	100	150	200	250
iteration
Figure 99: Gradient descent with Polyak momentum, β = 0.9.
Aue∙lnbe
tram loss (iteration)
0.6-
80-4-
U
S
匕 0.2-
0.0-
0	50	100	150	200	250	300
iteration
tram loss (iteration)	sharpness (iteration)
Figure 100: Gradient descent with Nesterov momentum,
tram accuracy (iteration)
200	400	600
iteration
Ausnbe
β=0.9.
N.2.2 Cross-entropy loss
Sso- u5h
train loss (iteration)
ss-usb
0-
0
sharpness (iteration)
5 。 5
7 5 2
SSgUReUS
50	100	150	200
iteration
Figure 101: Gradient descent with Polyak momentum, β
Aue∙lnbe
0	50	100	150	200
iteration
0.9.
tram loss (iteration)
0-
5-
0-
5-
0-
0	100	200	300
iteration
sharpness (iteration)
0 5g 5
0 7 5 2
SSgUReUS
o-
0	50	100	150	200	250	300
iteration
Figure 102: Gradient descent with Nesterov momentum, β = 0.9.
Ausnbe
train accuracy (iteration)
0	100	200	300
iteration
72
Published as a conference paper at ICLR 2021
N.3 Convolutional tanh network
In the leftmost plot, the vertical dotted line marks the iteration where the sharpness first crosses the
MSS. (Note that unlike vanilla gradient descent, momentum gradient descent can sometimes cause
the train loss to increase even when the algorithm is stable (Goh, 2017).)
In the middle plot, the horizontal dashed line marks the MSS.
N.3.1 Square loss
125-
S ɪoɑ-
In
ω
I 75 ^
e
ω 50-
25-
0
sharpness (iteration)
Figure 103: Gradient descent with Polyak momentum, β
iteration
200	400	600	800
0.9.
Qooo
0 8 6 4
SSgUReUS
sharpness (iteration)
20-
0	500	1000	1500	2000
iteration
Figure 104: Gradient descent with Nesterov momentum, β = 0.9.
Ausnbe
ι
0
8
6
4
2
500	1000	1500	2000
iteration
Aue∙lnbe
N.3.2 Cross-entropy loss
train loss (iteration)
sharpness (iteration)
train accuracy (iteration)
ss-usb
o-
5-
o-
5-
o-
0	50	100 150 200 250 300
iteration
80
耨 60 -___Lr___________________________
U	. ∙	,. ɪ»∙ ∙ ∙ ∙ I...
" 40 一二二二-1 "S___________________二，
2	. .：•,	...
S 20- ∙',	•••***
0	50	100	150	200	250	300
iteration
Aue∙lnbe
----η = 0.1
----∩ = 0.0666
----η = 0.05
50	100 150 200 250 300
iteration
Figure 105: Gradient descent with Polyak momentum, β = 0.9.
Sso- u5h
train loss (iteration)
600
200	400
iteration
sharpness (iteration)
0	200	400	600
iteration
Figure 106: Gradient descent with Nesterov momentum, β = 0.9.
tram accuracy (iteration)
Ausnbe
0	200	400	600
iteration
73
Published as a conference paper at ICLR 2021
N.4 Convolutional ReLU network
In the leftmost plot, the vertical dotted line marks the iteration where the sharpness first crosses the
MSS. (Note that unlike vanilla gradient descent, momentum gradient descent can sometimes cause
the train loss to increase even when the algorithm is stable (Goh, 2017).) In the middle plot, the
horizontal dashed line marks the MSS.
N.4. 1 Square loss
train loss (iteration)
4
O
ssu
sharpness (iteration)
1000
Ooo
5 0 5
7 5 2
SSgUReUS
0	2000	4000	6000
iteration
Aue∙lnbe
2000	4000
iteration
6000
Figure 107: Gradient descent with Polyak momentum, β
0.9.
tram loss (iteration)
0.6-
g 0.4-
£
£ 0.2-
Sharpness (iteration)
0.0
0 0	5000 10000 15000 20000 25000
iteration
Ausnbe
0	5000 10000 15000 20000 25000
iteration
0.0
0	5000 10000 15000 20000 25000
iteration
Figure 108: Gradient descent with Nesterov momentum, β
N.4.2 Cross-entropy loss
0.9.
train loss (iteration)
4000
0.0
0	1000	2000	3000
iteration
1500
Looo
&
ɪ 500
1000	2000	3000	4000
iteration
sharpness (iteration)
--------；---------------------- io
0.0
0	1000	2000	3000	4000
iteration
Aue∙lnbe
Figure 109: Gradient descent with Polyak momentum, β = 0.9.
train loss (iteration)
Sso- u5h
sharpness (iteration)	train accuracy (iteration)
---------------------------------- ι.o--------------------------------------------
0-
5-
0-
5-
0-
0	2000	4000
iteration
6000
Figure 110: Gradient descent with Nesterov momentum, β
Ausnbe
2000	4000	6000
iteration
0.9.
74
Published as a conference paper at ICLR 2021
O	Experiments: Learning rate drop
In this appendix, we run gradient descent until reaching the Edge of Stability, and then we cut the
step size. We will see that the sharpness starts increasing as soon as the step size is cut, and only
stops increasing once gradient descent is back at the Edge of Stability (or training is finished). As a
consequence of this experiment, one can interpret the Edge of Stability as a regime in which gradient
descent is constantly “trying” to increase the sharpness beyond 2∕η, but is constantly being blocked
from doing so. Our experiments focus on image classification on a 5k-sized subset of CIFAR-10.
We study two architectures (a fully-connected tanh network and a convolutional ReLU network) and
two loss functions (squared loss and cross-entropy loss).
75
Published as a conference paper at ICLR 2021
O. 1 Fully-connected tanh network: square loss
Gradient descent with fixed step size η
Aue∙lnbe
5000	10000	15000
iteration
4
O
Sso- u
Drop η from 2/100 to 2/200 at iteration 2000
500
0
0	2000	4000	6000
iteration
Aue∙lnbe
4
O
ssu
0	2000 4000	6000	8000 10000
iteration
500
400-
300-
200-
100-
Drop η from 2/200 to 2/300 at iteration 6000
SSgUReUS
0
0	2000	4000	6000	8000
iteration
10000
Drop η from 2/300 to 2/400 at iteration 10000
4
O
Sso- u
0	2500 5000 7500 10000 12500	0	2500 5000 7500 10000 12500
iteration	iteration
Figure 111: Top row: we train a network using gradient descent at a range of step sizes η (see
the legend in the right pane). Bottom three rows: Once gradient descent has reached the Edge
of Stability, we cut the step size. The black vertical dotted line marks the iteration where the step
size is cut. This iteration was chosen by hand, but was not cherry-picked. We use different colors
(consistent with the legend) to plot the train loss, sharpness, and train accuracy before and after the
learning rate drop. In the middle sharpness plot, the two horizontal lines mark the maximum stable
sharpness 2/n for both the old and new step size. Takeaway: once We decrease the step size, the
sharpness immediately starts to increase until it reaches the maximum stable sharpness 2/n for the
new step size η (or training finishes).
Aue∙lnbe
0.0
0.0
Aue∙lnbe
0	2500 5000 7500 10000 12500
iteration
76
Published as a conference paper at ICLR 2021
O.2 Fully-connected tanh
network: cross-entropy loss
train loss	train loss	train loss	train loss	train loss
Drop η from 2/80 to 2/110 at iteration 1200
500	1000	1500	2000
iteration
2.0- ∖
1.5-
1.0-
0.5-
0
500
S ιoo-
In
ω
U
e-
2 50-
SSgUReUS
Gradient descent with fixed step size η
sharpness (iteration)
0	500	1000 1500 2000 2500
iteration
Drop η from 2/20 to 2/50 at iteration 300
50
° 0	200	400	600	800
iteration
150
0	500	1000	1500	2000
iteration
Drop η from 2/110 to 2/140 at iteration 2000
1000 1500 2000 2500 3000
iteration
Figure 112: Refer to the caption for Figure
Aue∙lnbe
Aue∙lnbe
Aue∙lnbe

500	1000 1500 2000 2500
iteration
iteration
500	1000	1500
iteration
iteration
Ausnbe
0	500 1000 1500 2000 2500 3000
iteration
111.
77
Published as a conference paper at ICLR 2021
O.3 Convolutional ReLU network： square loss
train loss	train loss	train loss	train loss
Gradient descent with fixed step size η
Drop η from 2/200 to 2/450 at iteration 10000
ιooo
ss3ud.IeUS
800
600
400
200
0
0	10000 20000 30000 40000 50000
iteration
Drop η from 2/450 to 2/700 at iteration 30000
Ooooo
Ooooo
18 6 4 2
SSgUReUS
o
0	20000	40000	60000	80000
iteration
Drop η from 2/700 to 2/900 at iteration 60000
ιooo
SSgUReUS
o
0	20000 40000 60000 80000 100000
iteration
0
8
6
4
2
Aue∙lnbe
train accuracy (iteration)
——∏ = 2/200
η = 2/450
——η = 2/700
20000 40000 60000 80000 100000
iteration
Aue∙lnbe
10000 20000 30000 40000 50000
iteration
Aue∙lnbe
20000	40000	60000	80000
iteration
Aue∙lnbe
Figure 113: Refer to the caption for Figure 111.
20000 40000 60000 80000 100000
iteration
78
Published as a conference paper at ICLR 2021
O.4 Convolutional ReLU network: cross-entropy loss
train loss	train loss	train loss	train loss
train loss (iteration)
2.o-
1.5-
1.0-
0.5-
0.0-
0	5000	10000	15000	20000
iteration
0	2500 5000 7500 10000 12500 15000
iteration
SSgUReqS
Gradient descent with fixed step size η
sharpness (iteration)
0	5000	10000	15000	20000
iteration
Drop η from 2/100 to 2/400 at iteration 1000
1500
SSgUReqS
Drop η from 2/400 to 2/700 at iteration 7500
0 0	2500 5000 7500 10000 12500 15000
iteration
Drop η from 2/700 to 2/1000 at iteration 7500
1500
Aue∙lnbe
1.0
Aue∙lnbe
2.0-
1.5-
1.0-
0.5-
0	5000	10000 15000 20000
iteration
train accuracy (iteration)
0	5000	10000	15000	20000
iteration
0.0
0	2500 5000 7500 10000 12500 15000
iteration
ω 1000
(u
U
e-
S 500
5000	10000 15000 20000
iteration
1.0

Figure 114: Refer to the caption for Figure 111.
0.0
0	5000	10000 15000 20000
iteration
79
Published as a conference paper at ICLR 2021
P	Other eigenvalues
Throughout most of this paper, we have studied the evolution of the maximum Hessian eigenvalue
during gradient descent. In this appendix, we examine the evolution of the top six eigenvalues.
While training the network from §3, we monitor the evolution of the top six Hessian eigenvalues.
For each of cross-entropy loss and MSE loss, we train at four different step sizes. The results are
shown in Figure 115. Observe that each of the top six eigenvalues rises and then plateaus. The
precise details differ between MSE loss and cross-entropy loss. For MSE loss, each eigenvalue rises
past 2∕η, and then plateaus just above that value. In contrast, for cross-entropy loss, some of the
lesser eigenvalues plateau below 2∕η.
150
step size 2/30
150
125
g 100
75
step size 2/60
125
100
75
50
~l司凝梆喇祠嗫部
500	1000
Iteration
1500
• elg 1
elg 2
elg 3
o elg 4
elg 5
• elg 6
25
0
0
200	400	600	B00
Iteration
Iteration
25
0
0
Iteration
Iteration
step size 2/400
500-∣-------------------------------
:::：N
200
wo-
θɪn-------1------1------1------1—
0	1000 2000 3000 4000
Iteration
« « 50
Figure 115: Evolution of top six eigenvalues. For both cross-entropy loss (top) and MSE loss
(bottom), we run gradient descent at four different step sizes (columns), and plot the evolution of
the top six eigenvalues (different colors; see legend).
80