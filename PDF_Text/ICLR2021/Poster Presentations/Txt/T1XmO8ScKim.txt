Probabilistic Numeric
Convolutional Neural Networks
Marc Finzi*
Qualcomm AI Research
New York University
maf820@nyu.edu
Roberto Bondesan & Max Welling
Qualcomm AI Researcht
{rbondesa, mwelling}@qti.qualcomm.com
Ab stract
Continuous input signals like images and time series that are irregularly sampled
or have missing values are challenging for existing deep learning methods. Co-
herently defined feature representations must depend on the values in unobserved
regions of the input. Drawing from the work in probabilistic numerics, we propose
Probabilistic Numeric Convolutional Neural Networks which represent features as
Gaussian processes (GPs), providing a probabilistic description of discretization
error. We then define a convolutional layer as the evolution of a PDE defined on
this GP, followed by a nonlinearity. This approach also naturally admits steerable
equivariant convolutions under e.g. the rotation group. In experiments we show
that our approach yields a 3× reduction of error from the previous state of the art
on the SuperPixel-MNIST dataset and competitive performance on the medical
time series dataset PhysioNet2012.
1	Introduction
Standard convolutional neural networks are defined on a regular input grid. For continuous signals
like time series and images, these elements correspond to regular samples of an underlying function
f defined on a continuous domain. In this case, the standard convolutional layer of a neural network
is a numerical approximation of a continuous convolution operator A.
Coherently defined networks on continuous functions should only depend on the input function f,
and not on spurious shortcut features (Geirhos et al., 2020) such as the sampling locations or sam-
pling density, which enable overfitting and reduce robustness to changes in the sampling procedure.
Each application of A in a standard neural network incurs some discretization error which is deter-
mined by the sampling resolution. In some sense, this error is unavoidable because the features f (`)
at the layers ` depend on the values of the input function f at regions that have not been observed.
For input signals which are sampled at a low resolution, or even sampled irregularly such as with
the sporadic measurements of patient vitals data in ICUs or dispersed sensors for measuring ocean
currents, this discretization error cannot be neglected. Simply filling in the missing data with zeros
or imputing the values is not sufficient since many different imputations are possible, each of which
can affect the outcomes of the network.
Probabilistic numerics is an emergent field that studies discretization errors in numerical algorithms
using probability theory (Cockayne et al., 2019). Here we build upon these ideas to quantify the
dependence of the network on the regions in the input which are unknown, and integrate this un-
certainty into the computation of the network. To do so, we replace the discretely evaluated feature
maps {f (`) (xi)}iN=1 with Gaussian processes: distributions over the continuous function f(`) that
track the most likely values as well as the uncertainty. On this Gaussian process feature represen-
tation, we need not resort to discretizing the convolution operator A as in a standard convnet, but
instead we can apply the continuous convolution operator directly. If a given feature is a Gaussian
process, then applying linear operators yields a new Gaussian process with transformed mean and
covariance functions. The dependence of Af on regions of f which are not known translates into
*Work done during internship at Qualcomm AI Research
^Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.
1
the uncertainty represented in the transformed covariance function, the analogue of the discretization
error in a CNN, which is now tracked explicitly. We call the resulting model Probabilistic Numeric
Convolutional Neural Network (PNCNN).
2	Related Work
Over the years there have been many successful convolutional approaches for ungridded data such as
GCN (Kipf and Welling, 2016), PointNet (Qi et al., 2017), Transformer (Vaswani et al., 2017), Deep
Sets (Zaheer et al., 2017), SplineCNN (Fey et al., 2018), PCNN (Atzmon et al., 2018), PointConv
(Wu et al., 2019), KPConv (Thomas et al., 2019) and many others (de Haan et al., 2020; Finzi et al.,
2020; Schutt et al., 2017; Wang et al., 2018). However, the target domains of sets, graphs, and
point clouds are intrinsically discrete and for continuous data each of these methods fail to take full
advantage of the assumption that the underlying signal is continuous. Furthermore, none of these
approaches reason about the underlying signal probabilistically.
In a separate line of work there are several approaches tackling irregularly spaced time series with
RNNs (Che et al., 2018), Neural ODEs (Rubanova et al., 2019), imputation to a regular grid (Li and
Marlin, 2016; Futoma et al., 2017; Shukla and Marlin, 2019; Fortuin et al., 2020), set functions (Horn
et al., 2019) and attention (Shukla and Marlin, 2020). Additionally there are several works exploring
reconstruction of images from incomplete observations for downstream classification (Huijben et al.,
2019; Li and Marlin, 2020).
Most similar to our method are the end-to-end Gaussian process adapter (Li and Marlin, 2016) and
the multi-task Gaussian process RNN classifier (Futoma et al., 2017). In these two works, a Gaussian
process is fit to an irregularly spaced time series and sampled imputations from this process are fed
into a separate RNN classifier. Unlike our approach where the classifier operates directly on a
continuous and probabilistic signal, in these works the classifier operates on a deterministic signal
on a regular grid and cannot reason probabilistically about discretization errors.
Finally, while superficially similar to Deep GPs (Damianou and Lawrence, 2013) or Deep Differen-
tial Gaussian Process Flows (Hegde et al., 2018), our PNCNNs tackle fundamentally different kinds
of problems like image classification1, and our GPs represent epistemic uncertainty over the values
of the feature maps rather than the parameters of the network.
3	Background
Probabilistic Numerics: We draw inspiration for our approach from the community of proba-
bilistic numerics where the error in numerical algorithms are modeled probabilistically, and typically
with a Gaussian process. In this framework, only a finite number of input function calls can be made,
and therefore the numerical algorithm can be viewed as an autonomous agent which has epistemic
uncertainty over the values of the input. A well known example is Bayesian Monte Carlo where a
Gaussian process is used to model the error in the numerical estimation ofan integral and optimally
select a rule for its computation (Minka, 2000; Rasmussen and Ghahramani, 2003). Probabilistic
numerics has been applied widely to numerical problems such as the inversion ofa matrix (Hennig,
2015), the solution of an ODE (Schober et al., 2019), a meshless solution to boundary value PDEs
(Cockayne et al., 2016), and other numerical problems (Cockayne et al., 2019). To our knowledge,
we are the first to construct a probabilistic numeric method for convolutional neural networks.
Gaussian Processes: We are interested in operating on the continuous function f (x) underlying
the input, but in practice we have access only to a collection of the values of that function sampled
on a finite number of points {xi}iN=1. Classical interpolation theory reconstructs f deterministically
by assuming a certain structure of the signal in the frequency domain. Gaussian processes give a
way of modeling our beliefs about values that have not been observed (Rasmussen et al., 2006), as
reviewed in appendix A. These beliefs are encoded into a prior covariance k of the GP f 〜 GP(0, k)
and updated upon seeing data with Bayesian inference. Explicitly, given a set of sampling locations
X = {χi}N=ι and noisy observations y = {yi}N=ι sampled yi 〜 N(f (xi), σ2), using Bayes rule
1While GPs can be applied directly to image classification, they are not well suited to this task even with
convolutional structure baked in, as shown in Kumar et al. (2018).
2
gτ
Input Data
gp
gp
gp
Probabilistic
ReLU
Prob K) Probabilistic W) K) Probabilistic "㈤ σ⑺
ReLU	ReLU
“(X)σ(x)
["Qp,∑p)
QT□0o
Figure 1: The PNCNN operating on SuperPixel-MNIST images shown on the left. The mean and
elementwise uncertainty of the Gaussian process feature maps are shown as they are transformed
through the network by the convolution layers. Observation points shown as green dots in σ(x).
one can compute the posterior distribution f|y, X 〜GP(μp,kp), which captures our epistemic
uncertainty about the values between observations. The posterior mean and covariance are given by
μp(x) = k(x)>[K + S]-1 y , kp(x,x0) = k(x,x0) — k(x)>[K + S]-1 k(x0),	(1)
where Kij = k(xi, xj), k(x)i = k(x, xi) and S = diag(σi2). Below we shall choose the RBF ker-
nel2 as prior covariance, due to its convenient analytical properties: kRBF (x, x0) = aN (x; x0, l2I) =
—d	1
a (2πl2) 2 exp(-圭||x — x0∣∣2). In typical applications of GPs to machine learning tasks such as
regression, the function f that we want to predict is already the regression model. In contrast, here
we use GPs as a way of representing our beliefs and epistemic uncertainty about the values of both
the input function and the intermediate feature maps of a neural network.
4	Probabilistic Numeric Convolutional Neural Networks
4.1	Overview
Given an input signal f : X → Rc , we define a network with layers that act directly on this
continuous input signal. We define our neural network recursively from the input f(0) = f, as a
series of L continuous convolutions A(') with pointwise ReLU nonlinearities and weight matrices
which mix only channels (known as 1 × 1 convolutions) M ∈ Rc×c :
f ('+1) = M ⑶ ReLU[A⑶ f ⑷],	⑵
and a final global average pooling layer P which acts channel-wise as natural generalization of the
discrete case: P(f(L))α = R fα(L) (x)dx for each α = 1, 2, . . . , c. Denoting the space of functions
on X with C channels by Hc, the convolution operators A(') are linear operators from Hce to Hc'+1.
Like in ordinary convolutional neural networks, the layers build up increasingly more expressive
spatial features and depend on the parameters in A(') and M('). Unlike ordinary convolutional
networks, these layers are well defined operations on the underlying continuous signal.
While it is clear that such a network can be defined abstractly, the exact values of the function f(L)
cannot be computed as the operators depend on unknown values of the input. However, by adapting
a probabilistic description we can formulate our ignorance of f(0) with a Gaussian process and see
how the uncertainties propagate through the layers of the network, yielding a probabilistic output.
Before delving into details, we outline the key components of equation 2 that make this possible.
Continuous Convolutional Layers: Crucially, we consider continuous convolution operators A
that can be applied to input Gaussian process f 〜GP (μp, kp) in closed form. The output is another
Gaussian process with a transformed mean and covariance Af 〜GP (Aμp, AkpA) where A0 acts
2For convenience, We include the additional scale factor (2πl2)d/2 relative to the usual definition.
3
to the left on the primed argument of kp (x, x0).3 In section 4.2 we show how to parametrize these
continuous convolutions in terms of the flow of a PDE and show how they can be applied to the RBF
kernel exactly in closed form.
Probabilistic ReLUs: Applying the ReLU nonlinearity to the GP yields a new non Gaussian
stochastic process h(`) = ReLU[A(')f (')], and We show in section 4.5 that the mean and covariance
of this process has a closed form solution which can be computed.
Channel Mixing and Central Limit Theorem: The activations h(`) are not Gaussian; however, for
a large number of weakly dependent channels we argue that f ('+1) = M(')h(') is approximately
distributed as a Gaussian Process in section 4.5.
Measurement and Projection to RBF Gaussian Process: While f ('+1) is approximately a GaUS-
sian process, the mean and covariance functions have a complicated form. Instead of using these
functions directly, we take measurements of the mean and variance of this process and feed them
in as noisy observations to a fresh RBF kernel GP, allowing us to repeat the process and build up
multiple layers without increasing complexity.
The Gaussian process feature maps in the final layer f(L) are aggregated spatially by the integral
pooling P which can also be applied in closed form (see appendix D), to yield a Gaussian out-
put. Assembling these components, we implement the end to end trainable Probabilistic Numeric
Convolutional Neural Network which integrates a probabilistic description of missing data and dis-
cretization error inherent to continuous signals. The layers of the network are shown in figure 1.
4.2	Continuous convolutional layers
On a discrete domain such as the lattice X = Zd, all translation equivariant linear operators A are
convolutions, a fact which we review in appendix B. In general, these convolutions can be written
in terms of a linear combination of powers of the generators of the translation group: the shift
operators τi, i = 1, . . . , d shift all elements by one unit along the i-th axis of the grid. For a one
dimensional grid, one can always write A = Pk Wkτk where the weight matrices Wk ∈ Rc×c
act only on the channels and the shift operator τ acts on functions on the lattice. In d dimensions,
A = Pkl kd Wki,…,kdτk1 ∙∙∙ Tdd for some set of integer coefficients kι,..., k√. For example
when d = 2, we can take k1, k2 ∈ {-1, 0, 1} to fill out a 3 × 3 neighborhood.
On the continuous domain X = Rd we similarly parametrize convolutions with A = Pk WkeDk,
where Dk is given by powers of the partial derivatives ∂i, i = 1, . . . , d which generate infinitesimal
translations along the i-th axes. Setting d = 1 for simplicity, we can indeed verify that the operator
exponential τa = ea∂x applied to a function g(x) is a translation: ea∂x g (x) = g(x) + ag0 (x) +
2a2g00(x)+----= g(x + a), which is the Taylor series expansion of g(x + a) around x. Exponentials
of operators can be defined similarly in terms of the formal Taylor series eD = Pk∞=0 Dk/k! or more
broadly as the solution to the PDE:
∂tg(t, x) = (Dg)(t, x) ,	g(0, x) = g(x)	(3)
at time t = 1: eDg (x) = g(t = 1, x).
Following the discussion in the discrete case, translation invariance of Dk imposes that it is ex-
pressed in terms of powers of the generators. Collecting the derivatives into the gradient V, we can
write the general form of Dk as ɑk + β>V + 2V>∑kV + ... for any constants ak, vectors βk,
matrices Σk etc. For simplicity, we truncate the series at second order to get
Dk = β> V + 2歹>夕左 V ,	(4)
where we omit the constants αk that can be absorbed into the definition of Wk. For this choice of
D, the PDE in equation 3 is nothing but the diffusion equation with drift βk and diffusion Σk. When
discussing rotational equivariance in section 4.4, we also consider a more general form of D.
The diffusion layer can also be viewed in another way as the infinitesimal generator of an Ito
diffusion (a stochastic process). Given an Ito process with constant drift and diffusion dXt =
3More generally neural networks have affine layers including both convolutions and biases. An affine
transformation Af + b of a Gaussian process is also a Gaussian process f 〜GP(Aμp + b, AkpA0), and we
include biases in our network but omit them from the derivations for simplicity.
4
βdt + Σ^2dBt where Bt is a d dimensional BroWnian motion, the time evolution operator can be
written via the Feynman-Kac formula as etDf (x) = E[f (Xt)] where X0 = x. In other words, the
operator layer A = etD is the expectation under a parametrized Neural Stochastic Differential equa-
tion (Li et al., 2020; Tzen and Raginsky, 2019) that is homogeneous and therefore shift invariant.
The flow of this SDE depends on the drift and diffusion parameters β and Σ.
To recap, we define our convolution operator through the general form A = Pk WkeDk where
the weight matrices Wk ∈ Rc×c mix only channels and eDk is the forward evolution by one unit
of time of the diffusion equation with drift βk and diffusion Σk containing learnable parameters
{(Wk, βk, Σk)}kK=1. The translation equivariance of A follows directly from the fact that the gen-
erators commute ∀k, i : [Dk, ▽/ = 0 and therefore [A, τj = 0 (the bracket [a, b] = ab - ba is the
commutator of the two operators). In appendix B we show that our definition of A reduces to the
usual one in the discrete case and is thus a principled generalization to the continuous domain.
4.3	Exact Application on RBF GPs
Although the application of the linear operator A = Pk WkeDk involves the time evolution of a
PDE, owing to properties of the RBF kernel we fortuitously can apply the operator to an input GP
in closed form! Gaussian processes are closed under linear transformations: given f 〜 GP(μp, kp),
we need only compute the action of A on the mean and covariance: Af 〜GP (Aμp, AkpA0),
where A0 is the adjoint w.r.t. the L2(X) inner product. The application of time evolution eDk is a
convolution with a Green,s function Gk, so Af = Pk WkeDk f = Pk WkGk * f. As we derive in
appendix C, the Green,s function for Dk = β> V + (1∕2)Vτ∑k V, is nothing but the multivariate
Gaussian density Gk (x) = N(x; -βk, Σk):
Af=XWkeDkf=XWkGk*f=XWkN(-βk,Σk) *f.	(5)
In order to apply etD to the posterior GP, we need only to be able to apply the operator to the
posterior mean and covariance. This posterior mean and covariance in equation 1 are expressed in
terms of kRBF = aN(x; χ0, '2I) and the computation boils down to a convolution of two Gaussians:
etDkRBF(x, x0) = N(x; —tβ, t∑) * aN(x; x0, '2I) = aN(x; XZ — tβ,'2I + tΣ)	(6)
etD1 kRBF(x, x0)etD2 = aN(x; XZ — t(βι — β2),'2I + t∑ι +1∑2).	(7)
The application of the channel mixing matrices Wk and summation is also straightforward through
matrix multiplication for the mean and covariance. To summarize, because of the closed form action
on the RBF kernel, the layer can be implemented efficiently and exactly with no discretization or
approximations.
We note that with the Green,s function above, the action ofA encompasses the ordinary convolution
operator on the 2d lattice as a special case. Given drift βk ∈ {—1, 0, 1}×2, k = 1, . . . , 9 filling out
the 9 elements of a 3 × 3 grid and as the diffusion Σk → 0, the Green,s function is a Dirac delta, so
that: Af (x) = Pk Wkδ(x — βk) * f(x) = Pi,j=-1,0,1 Wij f (x1 — i,x2 —j) = W *Z2 f (x).
4.4	General equivariance
The convolutional layers discussed so far are translation equivariant. We discuss how to extend the
continuous linear operator layers to more general symmetries such as rotations. Feature fields in
this more general case are described by tensor fields, where the symmetry group acts not only on
the input space X but also on the vector space attached to each point x ∈ X. A linear layer A is
equivariant if its action commutes with that of the symmetry. In appendix E we derive constraints
for general linear operators and symmetries, which generalize those appearing in the steerable-
CNN literature (Weiler and Cesa, 2019; Cohen et al., 2019). Then we show how equivariance
under continuous roto-translations in 2d constrains the form of a convolutional layer by solving the
equivariance constraint. Non-trivial solutions require that the operator D in the PDE of equation 3
has a non-trivial matrix structure.
5
4.5	Probabilistic Nonlinearities and Rectified Gaussian Processes
Gast and Roth (2018) derive the mean and variance for a univariate rectified Gaussian distribution
for use in a neural network. We generalize these results to the full covariance function (and higher
moments) of a rectified Gaussian process in appendix J and present the results here. For the input GP
A⑶f ⑶(X)〜 GP(μ(x), k(x, x0)), We denote σ(x) =，k(x, x), Σ the matrix with components
∑ij = k(xi, Xj) for i, j = 1,2 and μ = [μ(xι), μ(x2)]. We use notation Φ(z) for the univariate
standard normal CDF, and Φ(z; Σ) for (two dimensional) multivariate CDF ofN(0, Σ) at z. Σ1
and Σ2 are the column vectors of Σ. The first and second moments of h = ReLU[Af] are:
E[h(x)] = μ(x)Φ(μ(x)/σ(x)) + σ(x)Φ0(μ(x)/σ(x)),	(8)
E[h(x1)h(x2))] = (k(xι, x2) + μ(x1)μ(x2))Φ(μ; Σ)	(9)
+ (μ(xi)∑> + μ(x2)∑>)VΦ(μ; Σ) + Σ>VV>Φ(μ; Σ)∑2,
where VV> Φ denotes the Hessian of Φ with respect to the first argument. The first and higher order
derivatives of the Normal CDF are just the PDF and products of the PDF with Hermite polynomials.
Note that the mean and covariance interact through the nonlinearity.
4.6	Channel Mixing and Central Limit Theorem
After the non-linearity the process is no longer Gaussian. To overcome this issue we introduce
a channel mixing matrix M(') ∈ Rc'+1×c' and define the feature map in the following layer by
f ('+1) = M⑶h⑶,where h⑻=ReLU[A⑶f ⑶].So long as the channels of h⑻ are only weakly
dependent, we can apply the central limit theorem (CLT) to each function fg+1) = P% Mlehe
so that in the limit of large e`, the statistics of the fθα'+1)'s converge to a GP with first and second
moments given by:
E[f('+1)(x)] = ME[h⑶(x)], E[f('+1)(x)f('+1)(x0)>] = ME[h⑶(x)h⑶(x0)>]M> . (10)
The convergence to a Gaussian process here is reminiscent of the well known infinite width limit
of Bayesian neural networks Neal (1996); de G. Matthews et al. (2018); Yang (2019). However the
setting here is fundamentally different. Unlike the Bayesian case where the distribution of M is
given by a prior or posterior, in PNCNN M is a deterministic quantity and instead the uncertainty is
about the input. PNCNN is not a Bayesian method in the sense of representing uncertainty about the
parameters of the model, but instead it is Bayesian in representing and propagating the uncertainty in
the value of the inputs. We go through the central limit theorem argument in appendix I, and go over
some potential caveats such if the weights were to converge to 0 or the independence assumptions
are violated. In the following section, we evaluate the Gaussianity empirically.
4.7	Measurement and Projection to RBF Gaussian Process
As a last step we simplify the mean and covariance functions of the approximate GP f ('+1). While
we can readily compute the values of these functions, unlike in the RBF kernel case, we can-
not apply the convolution operator etD in closed form. In order to circumvent this challenge, we
model the (approximately) Gaussian process f ('+1) with an RBF Gaussian process as follows: we
evaluate the mean yi = E[f(`+1)(xi)] and variance σ2 = Var[f('+1)(xi)] of the approximate
Gaussian process f ('+1) at a collection of points {χi}N=ι using equations 8, 9 and 10. These
values yi are treated as measurements of the underlying signal with a heteroscedastic noise σi2
that varies from point to point. We can then compute the RBF-based posterior GP of this signal
fw1)∣{(χi,yi,σJ}i=ι 〜 GP(μp,kp) with posterior mean and covariance given by equation 1 for
the heteroschedastic noise model. The uncertainty in the input f(`) is propagated through to the
RBF posterior f(e+1')∣{(χi, yi m )}NN=ι via the measurement noise σ》 Crucially, this Gaussian pro-
cess mean and covariance functions are written in terms of the RBF kernel and we can therefore
continue applying convolutions in closed form in future layers.
As we describe in the following section, the RBF kernel in each layer is trained to maximize the
marginal likelihood of the data that it sees, and thereby minimize the discrepancy with the under-
lying generating distribution f('+1). While this measurement/projection approach is effective in
6
TeSt Pixels 14 29 58 118 167 239
layer
• 1
2
3
• 4
O(1∕√N)
■ Train Res
N = 75
Test Resolution (pixels)
S-4UEnσ(x)o∕Qσ,— (x)m
50
0
5
5
2
.0
0
N(0,1) Quantiles (offset)
Figure 2: Left: Qualitative convergence of the mean and uncertainty of the first 3 channels of the
feature maps is shown in RGB color as the input test resolution is increased. Middle: Median
predicted uncertainties over spatial locations as a function of the test resolution. Right: Using the
predictions of the highest resolution model as ground truth, the distribution of prediction residuals
is shown in a Q-Q plot for each layer (shifted horizontally for clarity) with the black lines showing
the theoretical relationship, and the overall distribution histogram is shown on the right.
many scenarios, in networks with many layers or a very large number of observations uncertainty
information can get attenuated as it passes through the layers, a phenomenon which we investigate
in appendix H.
With a network that is trained on a version of MNIST that is randomly subsampled to 75 pixels,
in figure 2 (left) we evaluate the mean and uncertainty of the internal feature maps as we vary the
number of pixels of the inputs at test time. As expected, the mean functions for the feature maps
slowly converge and the predicted uncertainties decrease in magnitude as the input resolution is
increased. In figure 2 (middle) We show that in early layers the uncertainties decrease at a similar rate
to the O(1∕√N) of discretization error that we would expect from a standard convolutional layer
which is discretized to a square grid.4 Despite the fact that these resolutions differ substantially from
those seen at training time and the fact that there are no explicit uncertainty targets for these internal
layers, the predictions are reasonably well calibrated as demonstrated in figure 2 (right). While the
prediction residuals have fatter tails than a standard Gaussian (as evidenced by the deviations from
the line in the QQ plot), the mean and standard deviation are close to the theoretically optimal 0 and
1 values across a range of resolutions.
4.8	Training procedure
Our neural network has two sets of parameters: the channel mixing and diffusion parame-
ters, {(M⑶,W⑶,β⑶,∑('))}'=1, as well as kernel hyperparameters of the Gaussian Processes
{(l('),a('))}L=ι. We train all parameters jointly on the loss Ltask + λLgp, where Ltask is the cross
entropy with logits given by the mean μp of the pooled features P(f (L))〜N(μp, Σp) and LGP
is the marginal log likelihoods of the GP feature maps:
1 LC`/	∖	i (`)
LGP (f ) = 2∑. Σ [T[K [Kxx + Sα]-1fα) +logdet [Kxx + Sα]+ N log2∏	, (11)
where for each layer ', fα = [fα(xι),…，fɑ(xN)] ∈ RN are the observed values for channel ɑ
at locations X = [x1,. . . , xN], Kxx is the covariance of the RBF kernel and Sα = diag(σα2 ) the
measurement noise for each channel α and spatial location. Notably the GP marginal likelihood is
independent of the class labels.
5 Experimental Results
We evaluate the Probabilistic Numeric CNN on two different problems which have incomplete and
irregular observations.
4A 2D discrete convolution layer using N = m2 points can be interpreted as a Riemann sum approximation
of the continuous integral and will therefore have an error of O(1∕m) = O(1/VN) which is the same rate as
would be achieved through Monte Carlo sampling.
7
0.0
CNN
0	100	200
PNCNN without σ
0	100	200
Test Resolution (pixels)
0	100	200
Train
Resolution
16
36
64
----100
—144
196
256
Figure 3: Zero shot generalization to other resolutions: Having trained on MNIST at a given training
resolution shown by the color, we evaluate the performance of an PNCNN, PNCNN without uncer-
tainty, and an ordinary CNN on varying test resolutions. Notably, PNCNN with uncertainty is the
most robust.
Superpixel MNIST is an adaptation of the MNIST dataset where the 784 pixels of the original
images are replaced by 75 salient superpixels that are non uniformly spread throughout the domain
and are different for each image (Monti et al., 2017). Despite the simplicity of the underlying images,
the lack of grid structure and high fraction of missing values make this a challenging task. Example
inputs are visualized at the left of figure 1. We compare to Monet (Monti et al., 2017), SplineCNN
(Fey et al., 2018), Graph Convolutional Gaussian Processes (GCGP) (Walker and Glocker, 2019),
and Graph Attention Networks (GAT) (Avelar et al., 2020). As shown in table 5, the probabilistic
numeric CNN greatly outperforms the competing methods reducing the classification error rate by
more than 3× over the previous state of the art.
MNIST Superpixel 75	Error Rate (1)	PhysIONet2012	AP⑴	AUROC (↑)
Monet	8.89			
SplineCNN	4.78	IP-Nets	51.86	86.24
GCGP	4.2	SEFT-ATTN	53.67	85.14
GAT	3.81	GRU-D	54.97	86.99
PNCNN	1.24±0.12	PNCNN	53.92±.17	86.13±.06
PNCNN w/o σ	3.03±0.10			
Table 1: Left: Classification Error on the 75-SuperPixel MNIST problem. Right: Average Precision
and Area Under ROC curve metrics for PhysioNET2012. Mean and standard deviation are computed
over 3 trials.
We conduct an ablation study where uncertainty propagation is removed: the probabilistic ReLU is
replaced with the deterministic one applied to the mean, and the uncertainties in each layer are set
to 0. This form of the network still makes use of the fact that the input function is continuous by
use of the GP interpolation of the means, but crucially it does not integrate the uncertainty in the
computation resulting from the missing data. While this variant (PNCNN w/o σ) with a 3.03% error
rate outperforms existing methods from the literature, it is substantially worse than the PNCNN
that integrates the uncertainty at 1.24% error. This validates both that the underlying architecture
(using the continuous convolution operators) has good inductive biases and that reasoning about
discretization errors probabilistically can improve performance directly.
In figure 3 we evaluate the performance on zero shot generalization to a different test resolution
for a variety of training resolutions. In order to compare to an ordinary CNN we sample MNIST
on a regular square grid: PNCNN with uncertainty is the most robust to this train test sampling
distribution shift, followed by PNCNN w/o uncertainty, and finally the ordinary CNN which is quite
sensitive to these changes.
Irregularly Spaced Time Series For the second task, we evaluate our model on the irregularly
spaced time series dataset PhysioNet2012 (Silva et al., 2012) for predicting mortality from ICU vi-
tals signs. This dataset is particularly challenging because different vital sign channels are observed
at different times, even within a single patient record. This means that we cannot compute the GP
inference formula of equation 1 efficiently for all channels simultaneously because the observation
8
points {xi } and hence the matrices K in that formula differ between the channels, increasing com-
putational complexity. To circumvent this difficulty, we employ a stochastic diagonal estimator to
compute the variances as described in appendix G. We compare against IP-Nets (Shukla and Mar-
lin, 2019), SEFT-ATTN (Horn et al., 2019), and GRU-D (Che et al., 2018) as reported in Horn
et al. (2019). PNCNN performs competitively, although not a breakout performance as in the image
dataset which we attribute to the use of the stochastic variance estimates over an exact calculation.
6 Conclusion
Based on the ideas of probabilistic numerics, we have introduced a new class of neural networks
which model missing values and discretization errors probabilistically within the layers of a CNN.
The layers of our network are a series of operators defined on continuous functions, removing de-
pendence on shortcut features like the sampling locations and distribution. On irregularly sampled
and incomplete spatial data we show improved generalization and robustness. We believe that gen-
erative modeling of the internal feature maps of a neural network for representing uncertainty from
incomplete observations is an underexplored research direction and has significant promise for other
modalities of data, which we leave to future work.
9
References
Matan Atzmon, Haggai Maron, and Yaron Lipman. Point convolutional neural networks by exten-
sion operators. arXiv preprint arXiv:1803.10091, 2018.
Pedro HC Avelar, Anderson R Tavares, Thiago LT da Silveira, ClgUdio R Jung, and LUis C Lamb.
Superpixel image classification with graph attention networks. arXiv preprint arXiv:2002.05544,
2020.
Costas Bekas, Effrosyni Kokiopoulou, and Yousef Saad. An estimator for the diagonal of a matrix.
Applied numerical mathematics, 57(11-12):1214-1229, 2007.
Erik J Bekkers. B-spline cnns on lie groups. arXiv preprint arXiv:1909.12057, 2019.
Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. Recurrent
neural networks for multivariate time series with missing values. Scientific reports, 8(1):1-12,
2018.
Jon Cockayne, Chris Oates, Tim Sullivan, and Mark Girolami. Probabilistic numerical methods for
partial differential equations and bayesian inverse problems. arXiv preprint arXiv:1605.07811,
2016.
Jon Cockayne, Chris J Oates, Timothy John Sullivan, and Mark Girolami. Bayesian probabilistic
numerical methods. SIAM Review, 61(4):756-789, 2019.
Taco Cohen and Max Welling. Group equivariant convolutional networks. In International confer-
ence on machine learning, pages 2990-2999, 2016a.
Taco S Cohen and Max Welling. Steerable cnns. arXiv preprint arXiv:1612.08498, 2016b.
Taco S Cohen, Mario Geiger, and Maurice Weiler. A general theory of equivariant cnns on ho-
mogeneous spaces. In Advances in Neural Information Processing Systems, pages 9145-9156,
2019.
Andreas Damianou and Neil Lawrence. Deep gaussian processes. In Artificial Intelligence and
Statistics, pages 207-215, 2013.
Alexander G. de G. Matthews, Mark Rowland, Jiri Hron, Richard E. Turner, and Zoubin Ghahra-
mani. Gaussian process behaviour in wide deep neural networks, 2018.
Pim de Haan, Maurice Weiler, Taco Cohen, and Max Welling. Gauge equivariant mesh cnns:
Anisotropic convolutions on geometric graphs. arXiv preprint arXiv:2003.05425, 2020.
Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In
ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.
Matthias Fey, Jan Eric Lenssen, Frank Weichert, and Heinrich Muller. Splinecnn: Fast geomet-
ric deep learning with continuous b-spline kernels. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 869-877, 2018.
Marc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Gordon Wilson. Generalizing convolu-
tional neural networks for equivariance to lie groups on arbitrary continuous data. arXiv preprint
arXiv:2002.12880, 2020.
Vincent Fortuin, Dmitry Baranchuk, Gunnar Ratsch, and Stephan Mandt. Gp-vae: Deep probabilis-
tic time series imputation. In International Conference on Artificial Intelligence and Statistics,
pages 1651-1661. PMLR, 2020.
Fabian B Fuchs, Daniel E Worrall, Volker Fischer, and Max Welling. Se (3)-transformers: 3d roto-
translation equivariant attention networks. arXiv preprint arXiv:2006.10503, 2020.
Joseph Futoma, Sanjay Hariharan, and Katherine Heller. Learning to detect sepsis with a multitask
gaussian process rnn classifier. arXiv preprint arXiv:1706.04152, 2017.
10
Jochen Gast and Stefan Roth. Lightweight probabilistic deep networks. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pages 3369-3378, 2018.
Robert Geirhos, Jorn-Henrik Jacobsen, ClaUdio Michaelis, Richard ZemeL Wieland Brendel,
Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. arXiv
preprint arXiv:2004.07780, 2020.
Pashupati Hegde, Markus Heinonen, Harri Lahdesmaki, and Samuel Kaski. Deep learning with
differential gaUssian process flows. arXiv preprint arXiv:1810.04066, 2018.
Philipp Hennig. Probabilistic interpretation of linear solvers. SIAM Journal on Optimization, 25(1):
234-260, 2015.
Max Horn, Michael Moor, Christian Bock, Bastian Rieck, and Karsten Borgwardt. Set functions for
time series. arXiv preprint arXiv:1909.12064, 2019.
Iris AM Huijben, Bastiaan S Veeling, and Ruud JG van Sloun. Deep probabilistic subsampling
for task-adaptive compressed sensing. In International Conference on Learning Representations,
2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. arXiv preprint arXiv:1609.02907, 2016.
Vinayak Kumar, Vaibhav Singh, PK Srijith, and Andreas Damianou. Deep gaussian processes with
convolutional kernels. arXiv preprint arXiv:1806.01655, 2018.
Steven Cheng-Xian Li and Benjamin M Marlin. A scalable end-to-end gaussian process adapter
for irregularly sampled time series classification. In Advances in neural information processing
systems, pages 1804-1812, 2016.
Steven Cheng-Xian Li and Benjamin M Marlin. Learning from irregularly-sampled time series: A
missing data perspective. arXiv preprint arXiv:2008.07599, 2020.
Xuechen Li, Ting-Kam Leonard Wong, Ricky TQ Chen, and David Duvenaud. Scalable gradients
for stochastic differential equations. arXiv preprint arXiv:2001.01328, 2020.
Thomas P Minka. Deriving quadrature rules from gaussian processes. Technical report, Technical
report, Statistics Department, Carnegie Mellon University, 2000.
Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M
Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5115-
5124, 2017.
Radford M Neal. Priors for infinite networks. In Bayesian Learning for Neural Networks, pages
29-53. Springer, 1996.
Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets
for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 652-660, 2017.
Carl Edward Rasmussen and Zoubin Ghahramani. Bayesian monte carlo. Advances in neural
information processing systems, pages 505-512, 2003.
C.E. Rasmussen, C.K.I. Williams, M.I.T. Press, F. Bach, and ProQuest (Firm). Gaussian Processes
for Machine Learning. Adaptive computation and machine learning. MIT Press, 2006. ISBN
9780262182539. URL https://books.google.nl/books?id=Tr34DwAAQBAJ.
Yulia Rubanova, Ricky TQ Chen, and David Duvenaud. Latent odes for irregularly-sampled time
series. arXiv preprint arXiv:1907.03907, 2019.
11
Michael Schober, Simo Sarkka, and PhiliPP Hennig. A probabilistic model for the numerical solu-
tion of initial value problems. Statistics and Computing, 29(1):99-122, 2019.
Kristof Schutt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre
Tkatchenko, and Klaus-Robert Muller. Schnet: A continuous-filter convolutional neural network
for modeling quantum interactions. In Advances in neural information processing systems, pages
991-1001, 2017.
Zhengyang Shen, Lingshen He, Zhouchen Lin, and Jinwen Ma. Pdo-econvs: Partial differential
operator based equivariant convolutions. arXiv preprint arXiv:2007.10408, 2020.
Satya Narayan Shukla and Benjamin Marlin. Multi-time attention networks for irregularly sampled
time series. 2020.
Satya Narayan Shukla and Benjamin M Marlin. Interpolation-prediction networks for irregularly
sampled time series. arXiv preprint arXiv:1909.07782, 2019.
Ikaro Silva, George Moody, Daniel J Scott, Leo A Celi, and Roger G Mark. Predicting in-hospital
mortality of icu patients: The physionet/computing in cardiology challenge 2012. In 2012 Com-
puting in Cardiology, pages 245-248. IEEE, 2012.
Bart Smets, Jim Portegies, Erik Bekkers, and Remco Duits. Pde-based group equivariant convolu-
tional neural networks. arXiv preprint arXiv:2001.09046, 2020.
Michael L Stein. Interpolation of spatial data: some theory for kriging. Springer Science & Business
Media, 2012.
Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, FrangOiS Goulette,
and Leonidas J Guibas. Kpconv: Flexible and deformable convolution for point clouds. In
Proceedings of the IEEE International Conference on Computer Vision, pages 6411-6420, 2019.
Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick
Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point
clouds. arXiv preprint arXiv:1802.08219, 2018.
Belinda Tzen and Maxim Raginsky. Neural stochastic differential equations: Deep latent gaussian
models in the diffusion limit. arXiv preprint arXiv:1905.09883, 2019.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pages 5998-6008, 2017.
Ian Walker and Ben Glocker. Graph convolutional gaussian processes. arXiv preprint
arXiv:1905.05739, 2019.
Shenlong Wang, Simon Suo, Wei-Chiu Ma, Andrei Pokrovsky, and Raquel Urtasun. Deep para-
metric continuous convolutional neural networks. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 2589-2597, 2018.
Maurice Weiler and Gabriele Cesa. General e (2)-equivariant steerable cnns. In Advances in Neural
Information Processing Systems, pages 14334-14345, 2019.
Maurice Weiler, Fred A Hamprecht, and Martin Storath. Learning steerable filters for rotation equiv-
ariant cnns. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pages 849-858, 2018.
Daniel E Worrall, Stephan J Garbin, Daniyar Turmukhambetov, and Gabriel J Brostow. Harmonic
networks: Deep translation and rotation equivariance. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 5028-5037, 2017.
Wenxuan Wu, Zhongang Qi, and Li Fuxin. Pointconv: Deep convolutional networks on 3d point
clouds. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pages 9621-9630, 2019.
12
Greg Yang. Wide feedforward or recurrent neural networks of any architecture are gaussian pro-
cesses. In H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlch6-Buc, E. Fox, and R. Gar-
nett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Asso-
ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
5e69fda38cda2060819766569fd93aa5- Paper.pdf.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and
Alexander J Smola. Deep sets. In Advances in neural information processing systems, pages
3391-3401,2017.
Yanzhao Zhou, Qixiang Ye, Qiang Qiu, and Jianbin Jiao. Oriented response networks. In Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 519-528,
2017.
13
A Review of Gaussian Processes
We briefly review here the main ideas of Gaussian Processes for machine learning, see Stein (2012);
Rasmussen et al. (2006) for more details. We start to explain how to use stochastic processes for
Bayesian inference. We see the stochastic process as prior over functions p(f) and as we are given
samples x = (x1, . . . , xN), y = (y1, . . . , yN), yi ≡ f(xi), we update our beliefs about the function
by constructing the posterior via Bayes rule: p(f |y, x) = p(y|f, x)p(f)/p(y |x). Here we need to
specify the likelihood of the data with our model p(y |f, x) = QiN=1 p(yi|f, xi) and the denominator,
called the evidence or marginal likelihood, follows: p(y∣x) = Ef〜p(f)[p(y∣f, x)]. The power of
this approach is that the value of the signal y at an unseen point x has an uncertainty which depends
on our knowledge of its neighbourhood: p(y|x, y, x) = Ef〜pf∣y,χ)[p(y∣f, x)] and allows Us to
reason probabilistically about the underlying signal.
A particular convenient class of random function is Gaussian processes (GPs) for which inference
can be done exactly. A stochastic process can be presented in terms of the finite distributions of the
random variables {f(xi)}iM=1 at points {xi}iM=1. For a GP these distributions are Gaussian and can
be defined uniquely by specifying means and covariances, and so a GP is specified entirely by its
mean function μ(x) and covariance kernel k(x, x0). We shall write f 〜GP(μ, k). Let us assume a
Gaussian likelihood model as well, i.e. p(yi|f, xi) = N (f(xi), σi2), where σn represents aleatoric
uncertainty on the measurement. (For simplicity we take here the function tobe scalar valued but the
reasoning can be easily generalized.) Then properties of the Gaussian distribution (see (Rasmussen
et al., 2006, Chap. 2) for a detailed derivation of the formulas) lead to the following posterior
distributions after seeing data y, x: p(f∣y, x) = GP(μp,kp), with
Mp(x) = k(x)T[K + S]-1y ,	kp(x, x0) = k(x, x0) — k(x)T[K + S]-1k(x0).	(12)
where Kij = k(xi, xj), k(x)i = k(x, xi) and S = diag(σi2).
B From discrete to continuous convolutional layers
We here show that the general formula
A =	WkeDk
k
(13)
with Dk a function of spatial derivatives, reduces in the case of discrete input space X to the usual
convolution we encounter in deep learning.
For simplicity we shall assume a 1d grid as input space X = {1, . . . , N}. Let us start by recalling
the form of the classical discrete convolution when C' = C'+ι = 1. We define a convolutional layer
as a linear map that commutes with the translation operator. To make the symmetry exact, we need
assume periodic boundaries. Then in the standard basis of RN, {ei}iN=1 of vectors localized at site
i, the translation operator τ acts as τei = ei+1 mod N . An N × N matrix B is translation invariant
iffτB = Bτ. Since τ is diagonal in Fourier space, the most general solution is B = F-1diag(b)F,
where Fjk = e-2Nijk is the discrete Fourier transform. Such matrices are called circulant and can
be written alternatively as B = PN-I bN-iτi, b = F-1b. Explicitly:
/ bo	bi
bN-i	bo
.
B =	.	bN-i
b2
b1	b2
,..bN-2	bN-i∖
bi	bN-2
bo	....
...	...	bi
. .	bN-i	bo
(14)
This shows that the most general convolutional layer is a circulant matrix. E.g. if bi = 0 unless
i = 0, 1, N — 1, B coincides with the matrix representing a periodic convolution of filter size 3. The
matrix B is invertible as long as bk = 0 for all k. In a convolutional network the parameters b are
random variables and the measure of the set where B is not invertible is zero. Thus the role ofeD is
replaced in the discrete case by the B .
14
The discrete analog of A is then:
A = ^X Wi 0 Bi.	(15)
i
Introducing the unit matrices Eα,β which have 1 at the row α and column β and 0 otherwise, we
can rewrite it as:
A= X Eα,β0τjWjα,β, Wjα,β = XWiα,βbi,N-j.	(16)
j,α,β	i
Since Eα,β 0τj is a linear basis of the space of convolutional layers, we see that equation 13 indeed
reduces to the usual one when discretizing the input domain and is a principled generalization to the
continuous domain.
C Greens Function
Given the operator D = β>V + 1 V>ΣV We can compute the action of etD in terms of con-
volutions. Using the d dimensional Fourier transforms F[h](k) = (2π)-d R h(x)e-ik>xdx and
F-1 = Ft, we can rewrite the derivative operator D in terms of elementwise multiplication in the
Fourier domain, Which diagonalizes D. Since V = F -1(ik)F,
D = F-1(iβ>k - 2k>Σk)F.
Using the series definition etD = P∞=o(tD)n∕n!, we have:
etD = F -%t(iβ>k-2 k> Σk) F
(17)
(18)
Applying this operator to a test function h(x) yields
etDh = FT[et(iβ>kTk>£k)F[h](k)] = F-1[F[Gt] ∙ F[h]] = Gt * h,	(19)
where the final step follows from the Fourier convolution theorem, and we define the function Gt =
F-1 [et(iβ>k-2 k>£k)]. Directly applying the Fourier integral yields a Gaussian integral
Gt(x) = (2π)-2 /eik>(X+te)-1 k>t£kdk = e-2(X+te)>M)T(X+tβ)det(2πtΣ)-V2. (20)
This function Gt (x) = N(x; -tβ, tΣ) is nothing but a multivariate heat kernel, the Greens function
(also known as the fundamental solution or time propagator) for the diffusion equation ∂tGt (x -
x0) = DGt (x - x0), and indeed limt→0 Gt (x - x0) = δ(x - x0).
D Integral Pooling
The integral pooling operator P[f] = Rd f (x)dx can be applied to the Gaussian process just like
any other linear operator. Given f(L)〜 GP(μ, k), we have that
P f(L)〜GP (P μ, P kP0) = N (P μ, P kP 0).	(21)
Again, computing the mean μp = Pμ and covariance matrix Σp = PkP0 we need just to be able
to apply P to the RBF kernel.
PkRBF(x0) =	kRBF(x, x0)dx = a
PkRBFP0 =	kRBF(x, x0)dxdx0 = ∞
(22)
(23)
For many applications such as image classification using the mean logit value, we require only the
predictive mean, so an unbounded covariance matrix ΣP is acceptable. We use this form for all of
our experiments.
15
However for some applications an output uncertainty can be useful, so we also provide a variant that
integrates over a finite region [0, 1]d, Pf = [0,1]d f (x)dx.
d
PkRBF(x0) =	kRBF(x,x0)dx = a Y [Φ(xi) - Φ(xi-1)]
[0,1]d	i=1
(24)
PkRBFP0 = /	kRBF(x,x0)dxdx0 = a['q∕2(e-1/2'2 - 1) + 2Φ(') - l]d	(25)
[0,1]d×[0,1]d
where Φ is again the univariate standard normal CDF.
E Equivariance
E.1	Related Work
We note that there has been considerable research effort in the development of equivariant CNNs
which we build on top of. The group equivariant CNN was introduced by Cohen and Welling (2016a)
for discrete groups on lattices. This work has been extended for continuous groups (Worrall et al.,
2017; Zhou et al., 2017) and with steerable equivariance (Cohen and Welling, 2016b; Weiler and
Cesa, 2019) where other group representations are used. There have also been group equivariant
networks designed for point clouds and other irregularly spaced data (Thomas et al., 2018; Finzi
et al., 2020; Fuchs et al., 2020; de Haan et al., 2020). In Shen et al. (2020), layers using finite
difference estimation of derivative operators are used for defining equivariant layers in an equivariant
CNN, effectively a change of basis.
Most closely related to our PDE operator approach to equivariance is work by Smets et al. (2020).
In this work, the authors define layers of their convolutional network through the time evolution of a
PDE which is a nonlinear generalization of the diffusion equation, and which additionally includes
pooling like behaviour. Smets et al. (2020) parametrize their equivariant PDEs in terms of left
invariant vector fields. These PDEs are applied to scalar functions on the group, unlike our work
where feature fields (of various representations) live on the base space Rn . These two approaches
reflect the two major approaches to equivariance in the literature: using regular representations and
using irreducible/tensor representations. Both have advantages and disadvantages and we focus on
the second approach in this work. It may be possible that there is a deeper relationship between the
solutions we find and the left invariant vector fields in Smets et al. (2020), but this is beyond the
scope of our work.
E.2 Translation Equivariance
A key factor in the generalization of convolutional neural networks is their translation equivariance.
Patterns in different parts of an input signal can be seen in the same way because convolution is
translation equivariant. Our learnable linear operators A are equivariant to continuous transforma-
tions. Two linear operators eC and eB commute [eC, eB] = 0 if and only if their generators commute:
[C , B] = 0. Since the generator of diffusions Di is a sum of derivative operators, and the generators
of translations are just V as mentioned in section 4.2, the two commute: [Dk, V] = 0. Therefore
[A,τa] = [Pk WkeDk ,τa] = Pk Wk [eDk, ea>v] = 0 and A is translation equivariant.
E.3 Steerable Equivariance for Linear Operators
For some tasks like medical segmentation, aerial imaging, and chemical property prediction there are
additional symmetries in the data it makes sense to exploit other than mere translation equivariance.
Below we show how to enforce equivariance of the Linear operator A to other symmetry groups G
such as the group of continuous rotations SO(d) inRd. Applying equivariance constraints separately
on each of the components of eDi on top of translation equivariance yields very restricted set of
operators. For example, enforcing equivariance to continuous rotations G = SO(d), the operator
must be an isotropic heat kernel: Dk = ckV>V. The reason for this apparent restriction is a result
of considering the different channels independently, as scalar fields.
The alternative is to use features fields which transform under more general representations of the
symmetry group, introduced in steerable-CNNs (Cohen and Welling, 2016b) and used in (Worrall
16
et al., 2017; Thomas et al., 2018; Weiler et al., 2018; Weiler and Cesa, 2019) and others. In this
way, the symmetry transformation acts not only on the spatial domain X, but also transforms the
channels. The way that the group acts on Rc (i.e. the channels) is formalized by a representation
matrix ρ(g) ∈ Rc×c for each element g ∈ G in the transformation group that satisfies ∀g, h ∈ G :
ρ(gh) = ρ(g)ρ(h). Choosing the type of each intermediate feature map is equivalent to choosing
their representations, and we describe a simple way of doing this with tensor representations in the
later section.
Operator Equivariance Constraint: Returning to linear operators, we derive the equivariance
constraint and show how to use constructs from the previous sections to implement steerable rotation
equivariance. Equivariance ofa linear operator A : (Rd → Rcin ) → (Rd → Rcout) requires that, for
any input function, transforming the input function first (both argument and channels) and applying
A is equivalent to first applying A and then transforming the output: Aρin(g)Lgf = ρout (g)LgAf
where Lgf(x) = f (g-1x). Rearranging the terms, one sees that the equivariance constraint on the
linear operator A is:
ρout(g)LgALg-1ρin(g-1) = A,	(26)
where the operators Lg and Lg-1 are understood not to act on the representation matrices ρ (although
implicitly a function of g). As shown in Appendix E.4, eq. 26 is a direct generalization of the
equivariance constraint for convolutions ∀x : ρout(g)K(g-1x)ρin(g-1) = K(x) described in the
literature (Weiler and Cesa, 2019; Cohen et al., 2019).
As shown in Appendix E.6, the equivariance constraint for continuous rotations applied to the dif-
fusion operators A = Pk WkeDk has only the trivial solutions of isotropic diffusion without any
drift. For this reason we instead consider a more general form of diffusion operator where the PDE
itself couples the different channels. For the coupled PDE:
f = X WkDkf	(27)
k
the time evolution contains the matrices Wk in the exponential A = e k WkDk. Like with the
example of translation above, this operator is equivariant if and only if the infinitesmal generator
Pk WkDk is equivariant.
Because equation 26 applies generally to linear operators and not just convolutions, we can com-
pute the equivariance constraint for these derivative operators. We can simplify the summation
Pk WkDk = Pk Wk (βTV + (1∕2)Vt∑k V) by writing it in terms of the collection of matrices
Bi = Pk Wkeki and Sij = (1/2) Pk Wk∑kj to express Aderiv = Pi Bi& + P,/ Sijdidj
where the indices i,j = 1, 2..., d enumerate the spatial dimensions of each vector βk and each ma-
trix Σk. As we derive in appendix E.5, the necessary and sufficient conditions for the equivariance
of Pk WkDk and therefore A is that ∀g ∈ G : [ρ°ut 0 PJn 0 P(i,o)](g)vec(B) = Vec(B) and
∀g ∈ G : [pout 0Pln 0P(2,o)](g)Vec(S) = Vec(S) where vec(∙) denotes flattening the elements into
a single vector and ρ(r,s) is the tensor representation with r covariant and s contravariant indices.
E.4 Generalization of Equivariance Constraint for Convolutions
This equivariance constraint is a direct generalization of the equivariance constraint for convolution
kernels as described in Weiler and Cesa (2019); Cohen et al. (2019). In fact, when Ais a convolution
operator, Af = K * f, the action of Lg by conjugation A is equivalent to transforming the argument
of the kernel K :
Lg (K *)Lg-ι f (x) = / K (g-1x — x0)f (gx0)dμ(x0)
=/ K(g-1(x — x00))f (x00)dμ(x") = (Lg[K]) * f.
Letting both sides of eq 26 act on the product of a constant unit vector ei and a delta function,
f = eiδ the expression ∀ei : Pout (g)Lg [K]Pin (g-1) * eiδ = K * eiδ can be rewritten as ∀x :
Pout(g)K(g-1x)Pin(g-1) = K(x) which is precisely the constraint for steerable equivariance for
convolution described in the literature. 5
5This assumes as is typically done that measure μ over which the convolution is performed is left invariant.
For the more general case, see the discussion in Bekkers (2019).
17
E.5 Equivariant Diffusions with Matrix Exponential
Below we solve for the necessary and sufficient conditions for the equivariance of the operator
Aderiv.
We will use tensor representations for their convenience, but the approach is general to allow
other kinds of representations. A rank (p, q) tensor t is an element of the vector space T(p,q) :=
V0p 0 (V *)0q where V is some underlying vector space, V * is its dual and (•产P is the tensor
product iterated p times. In common language T(0,0) are scalars, T(1,0) are vectors, and T(1,1) are
matrices. Given the action of a group G on the vector space V, the representation on T(p,q) is
P(p,q)(g) = g0p 0 (g->)0q where -> is inverse transpose and 0 on the matrices is the tensor
product (Kronecker product) of matrices. Composite representations can be formed by stacking
different tensor ranks together, such as a representation of 50 scalars, 25 vectors, 10 matrices and
5 higher order tensors: T(500)㊉ ,250)㊉ T(Ii01)㊉ T(12)，where ㊉ in this context is the same as
the Cartesian product. For a composite representation U = Li T(pi ,qi ) the group representation is
similarly PU(g) = Li p(pi,qi)(g) where ㊉ concatenates matrices as blocks on the diagonal.
Noting that the operator Lg that acts only on the argument and the matrix ρin(g) acts only on the
components, the two commute and we can rewrite the constraint for Aderiv as
ρout(g)Biρin(g-i )Lg∂iLg-1 +	ρout(g)Sij ρin(g-i )Lg ∂i∂j Lg-1 = Aderiv	(28)
i	ij
We can simplify the expression Lg∂iLg-1 by seeing how it acts on a function. For any differentiable
function ∂iLg- f(x)=忌[f (gx)] = Pj gji[∂jf](gx) = Lg-ι Pj gjidj f(x) where gj are the
components of the matrix g. Since this holds for any f, We find that LgPLg-I = gτ V and therefore
Lg VVT Lg-ι = Lg VLg-ι Lg VT Lg-ι = gτ VVT g.
Since equation 28 holds as an operator equation, it must be true separately for each component ∂i
and ∂i ∂j . This means that the constraint separates into a constraint for B and a constraint for S:
1.	∀g, i : Pj gij ρout (g)Bjρin (g- ) = Bi
2.	∀g,i,j : Pkl gi'gikρout(g)S'k Pin(g I) = Sij .
These relationships can be expressed more succinctly by flattening the elements of B and S
into vectors: [Pout (g) 0 Pin(g-T) 0 P(i,0) (g)]vec(B) = vec(B) and [Pout (g) 0 Pin(g-T) 0
P(2,0) (g)]vec(S) = vec(S).
E.6 Rotation Equivariance Constraint for Scalar Diffusions has only Trivial
S olutions
The diffusion operator A = Pk WkeDk leads to only trivial βk = 0 and ∑k α I if it satisfies the
continuous rotation equivariance constraint.
Proof:
The application of eDk is just a convolution with the Greens function
X WkeDkf = X Wk [e-1 (x+βk)>Σ-1(x+βk)det(2π∑k)-1/2] * f = X WkGk * f (29)
kk	k
where the Greens function is the multivariate Gaussian density: Gk (x) = N(x; -βk, Σk).
As shown in appendix E.4, for convolutions the operator constraint is equivalent to the kernel
equivariance constraint Pout(g)K(g-1x)Pin(g-1) = K(x) from (Weiler and Cesa, 2019). With
K(x) = Pk WkGk(x) this reads:
∀x ∈ Rd, g ∈ G :	X Pout(g)WkN (g-1x; -βk, Σk)Pin(g-1) = XWkN(x; -βk, Σk),
For rotations g ∈ SO(2) where we can parametrize gθ = eθJ in terms of the antisymmetric matrix
J = [[0,1], [-1,0]] ∈ R2×2 and the translation operator can be written Lg = e-θxTJT▽, We can
18
take derivatives with respect to θ to get (now with double sums implicit):
∀x ∈ Rd ： X [dρoutWkN(x; -βk, Σk)-WkN(x; -βk, Σk)dρin-Wk(XT JTVW(x; -βk, Σk)] = 0.
k
Here the Lie Algebra representation of J is dρ :=另ρ(gθ)∣θ=o. Factoring out the normal density:
∀x ∈ Rd : X [dρoutWk - Wk dρin - Wk (xTJTΣk- (x + βk))]N (x; -βk, Σk) = 0.
k
Without loss of generality we may assume that each of the Gaussians βk, Σk pairs are distinct since
if they were not then we could replace the collection with a single element. Since the (finite) sum
of distinct Gaussian densities is never a Gaussian density, and monomials of order > 0 multiplied
by a Gaussian density cannot be formed with sums of Gaussian densities or sums multiplied by
monomials of a different order and Gaussian densities are never 0, this constraint separates out into
several independent constraints.
1.	∀i : dρoutWk = Wkdρin
2.	∀i, x : Wk(xTJTΣk-1βk) = 0
3.	∀i, x : Wk(xTJTΣk-1x) = 0
We may assume w.l.o.g. that Wk is not 0 for all components of the matrix (otherwise we could have
deleted this element of k and continue). Therefore there is some component which is nonzero, and
the expressions in parentheses in equations 2 and 3 must be 0. Given that this holds for all x, eq 3
implies: JTΣk-1 = 0 or equivalently Σk-1J = 0 because Σk is symmetric, and since J = -JT this
can be expressed concisely as [Σk-1, J] = 0 for which the only symmetric solution is proportional
to the identity Σk = ckI. Since both Σk and J are invertible, equation 2 yields βk = 0. Therefore
there are no nontrivial solutions for β, Σ in A = Pk WkeDk for continuous rotation equivariance.
F	Dataset and Training Details
In this section we elaborate on some of the details regarding hyperparameters, network architecture,
and the datasets.
As described in the main text, the PNCNN is composed of a chain of convolutional blocks contain-
ing a convolution layer, a probabilistic ReLUs, and linear channel mixing layer (analogue of the
colloquial 1 × 1 convolution). In each of these convolutional blocks, the input is a collection of
points and feature mean value at those points along with the feature elementwise standard deviation
at those points: {(xi, μ(χi), σ(χi)}N=ι. These observations seed the GP layer, and the block is eval-
uated at the same collection of points for the output (although it can be evaluated elsewhere since it
is a continuous process, and we make use this fact to visualize the features in figures 1 and 2).
Hyperparameters: For the PNCNN on the Superpixel MNIST dataset, we use 4 PNCNN convo-
lution blocks with c = 128 channels and with K = 9 basis elements for the different drift and
diffusion parameters in PkK=1 WkeDk. We train for 20 epochs using the Adam optimizer (Kingma
and Ba, 2014) with lr = 310-3 with batch size 50.
For the PNCNN on the PhysioNet2012 dataset, we use the variant of the PNCNN convolution layer
that uses the stochastic diagonal estimator described in appendix G with P = 20 probes. In the
convolution blocks we use c = 96 channels, K = 5 basis elements and we train for 10 epochs using
the same optimizer settings above. For both datasets we tuned hyperparameters on a validation set
of size 10% before folding the validation set back into the training set for the final runs. Both models
take about 2 hours to train.
SuperPixel-MNIST We source the SuperPixel MNIST dataset (Monti et al., 2017) from Fey and
Lenssen (2019) consisting of 60k training examples and 10k test represented as collections of posi-
tions and grayscale values {(xi, f (xi))}i7=5 1 at the N = 75 super pixel centroids.
PhysioNet2012 We follow the data preprocessing from Horn et al. (2019) and the 10k-2k train test
split. The individual data points consist of 42 irregularly spaced vital sign time series signals as
19
well as 5 static variables: Gender, ICU Type, Age, Height, Weight. We use one hot embeddings for
the first two categoric variables, and we treat each of these static signals as fully observed constant
time series signals. As the binary classification task exhibits a strong label imbalance, 14% positive
signals, we apply an inverse frequency weighting of 1/.14 to the binary cross entropy loss.
G Stochastic Diagonal Estimation for PhysioNet20 1 2
In order to compute the mean and variance of the rectified Gaussian process, the activations of
the probabilistic ReLU, we need compute the diagonal of AkpA0(xn, xn) for the relevant points
{xn}nN=1.
In the usual case where each of the channels α = 1, 2, ..., c are observed at the same locations this
can be done efficiently. First one computes the application of eDi on the left and eDj0 on the right
onto the posterior kp :
Nij = (eDikpeDj0 )(xn, xn) = (eDikeDj0)(xn,xn) - (eDik>)(xn)[K + S]-1(keDj0)(xn)
where k is the RBF kernel and we have reused the notation from appendix A. Notably, this quantity
is the same for each of the channels, and the elementwise variance is just:
vα(xn) = (AkpA0)αα(xn,xn) = XWiαβNijWjαβ	(30)
i,j,β
where the α, β index the channels of each of the matrices Wi . Because N is the same for all
channels, we can compute this quantity efficiently with a reasonable memory cost and compute.
For the PhysioNet2012 dataset where the observation points differ between the channels we must
consider a different observation set {xβn}nN=1 for each channel β. This means that evaluated kernel
depends on the channel and we have the objects: kβ , Kβ and Sβ . As a result, we have an additional
index for Niβj and the desired computation is
vα(xnα)=(AkpA0)αα(xnα,xnα)= XWiαβNiβjWjαβ.	(31)
i,j,β
While each of the terms in the computation can be computed without much difficulty, performing
the summation explicitly requires an unreasonably large memory cost and also compute.
However, by the same approach we can consider the full covariance matrix B(αn)(βm) =
(AkpA0)αβ (xnα, xβm), and while it would not be feasible to compute this matrix directly we can
define matrix vector multiplies onto vectors of size RcN implicitly using the sequence of operations
that define it. Crucially, this sequence of operations has much more modest memory consumption
(and compute cost) over the direct expression in equation 31. These implicit matrix vector multiplies
can then be used to compute a stochastic diagonal estimator (Bekas et al., 2007) given by:
P
Va(Xa) = P X Zp Θ Bzp	(32)
p=1
with Gaussian probe vectors zp 〜N(0, I), and where Θ is elementwise multiplication (see Bekas
et al. (2007) for more details on this stochastic diagonal estimator). We use this estimator with
P = 20 probes for computing the variances for PhysioNet. We note that with P = 20 the variance
estimates are still quite noisy, however without the estimator cannot readily apply the PNCNN to
PhysioNet. We leave a better approach for handling this kind of data to future work.
H	Pathologies in Projection to RBF Gaus s ian Process
In section 4.7 describe an approach by which a Gaussian process with a complex mean and co-
variance function is projected down to the posterior of a (simpler) RBF kernel GP from a set of
observations. We know given the representation capacity of the RBF kernel that with the right set of
observations, a complex function can be well approximated in principle. However, the relationship
for uncertainty is less straightforward.
20
The properties of the input Gaussian process must be conveyed to the output Gaussian process by
only the (uncorrelated) noisy observations {(xi, μ(xi), σ(xi))}N=ι. As the uncertainty in original
GP increases, so do the measurement uncertainties in the transmission, and therefore the output
GP also has a higher uncertainty. However, the uncertainty in the input GP is in the form of a
full covariance kernel k(x, x0) and it seems that individual observations will not easily be able to
communicate the covariance of the values of the GP function at different spatial locations despite
the heterogeneous noise model.
Fundamentally, the problem is that the observation values are treated as independent, an incorrect
assumption which has other knock-on effects when the number of observations is large. With some
fixed measurement error no matter how high but a large enough set of independent observations, the
mean value can be pinned down precisely. If in contrast the observations are not independent, then
there may be a situation where the mean value cannot be known more precisely than some limiting
uncertainty. This effect leads the output GP to have less uncertainty and be more confident in the
values that it should be given the input GP.
If the observations are sparse, then the effective sample size of the estimator for the mean of the
GP at any given location is small, and then the amount by which uncertainty is underestimated is
small. However, if there are very many observations then this kind of observation transmission of
information with the independence assumption will attenuate the uncertainty. We would also expect
that over the course of many layers, this attenuation can accumulate. We believe that this is what
causes the poorer uncertainty calibration in layers 3 and 4 of the PNCNN shown in figure 2. We hope
that this problem can be resolved perhaps by removing the independence assumption or providing
an alternative projection method in future work.
I Central Limit Theorem for Stochastic Processes
In this section we show that each of the outputs features fα = β Mαβhβ are marginally distributed
as Gaussian processes in the limit as cin → ∞, provided that hβ are independent and have bounded
moments, and that the fraction of zeros in each row of M does approach 1. The result follows from
an application of the multivariate Lyapunov CLT to the joint distribution of the functions evaluated
at a finite collection of points.
Since we are only making the case about the marginal distribution of each of the output channels,
we can separate them out individually and apply a CLT argument on each one. For any given output
channel g, the output is a sum g = Pβ mβhβ for values mβ given by the particular row of M . For
each coefficient mβ that is nonzero (with index β = i), we can define an element hi = mi hi . The
output is the sum of such elements g = Pihi.
Now choose any finite collection of points x1 , x2 , . . . , xN . The random vector
[g(x1), g(x2), ..., g(xN )] =	i[hi(x1), hi(x2), ..., hi(xN )]. In the limit as the number of
channels cin goes to infinity, we can apply the Lyapunov central limit theorem to this sum of
random vectors, so long as the number of elements in the sum also goes to infinity. This means that
the vector [g(χ1),g(χ2), ...,g(χN)] converges multivariate Gaussian with mean μ having entries
μn = Ei E[hi(xn)] and covariance Σ with entries ∑nm = Ei E[hi(xn)hi(xm)]. Since We can
repeat the same argument for any collection of points x1, x2, . . . , xN , we must deduce that g is a
Gaussian process according to the definition. The mean and covariance of g(x) → GP(μ, k) follow
from the mean and covariance of these finite collections, μ(x) = Ei E[hi(x)] = Ei miE[hi(x)]
and k(x, x0) = Pi E[hi(x)hi(x0)] = Pi m2E[hi(x)hi(x0)].
Conceivably the CLT could be violated during training if a large enough fraction of Mαβ converged
to 0 during training, or also if the separate channels became more strongly correlated. We investigate
the non-Gaussianity of the features in section 4.7 and find that the features are mostly Gaussian but
have somewhat longer tails indicating deviation from the theoretical behavior.
21
J Moments of rectified Gaus sian random variables
Let f 〜N(μ, Σ) be a d dimensional random Gaussian vector. We compute here
E(ReLUfI)…ReLUfd)) = N f> >0 ddff1 …fd exp - 2 f-μ)T ςTfi)
N∑ = (2n)d/2 det(Σ)".
We use the generating function technique. Define
Z(b)
I	ddf exp[-2(f - μ)T∑-1 (f - μ) + bTf]
f>0
ebT μ Z	ddf exp[-2 f T Σ-1 f + bT f ].
Jf >-μ
Then
∂∂
E(ReLUfI)…ReLUfd)) = ∂b2 …西 Z ㈤
b=0
To compute Z(b) we proceed as in Gaussian case. We change variables to
f=Σb+g,
and define Z = μ + Σb to get:
Z(b) = ebT"+2bTΣbɪ ∣'	ddg exp[-2gτΣ-1g]
NΣ g<+z
=eS(b) φ(d) (z；0, ∑), S(b) = bTμ + 2 bT∑b .
Φ being the multivariate standard Normal CDF:
Φ(d)(z; μ, ∑)= Z	ddgψ(d)(g; μ, Σ),
g<+z
ψ(d)(g; μ, ς)=奈 eχp[-2 (g - μ)T £-1(g - μ)]∙
NΣ
Now we compute the first two derivatives. Note that in d =1, denoting σ2 = Σ:
d-Φ(1)(z, 0,σ2) = ψ⑴(z, 0,σ2).
∂z
In d = 2, we can use the conditional probability decomposition to get the required derivatives:
@(2)(g;0, Σ) = ψ(1)(gι; α1g2,β1) ∙ ψ(1)(g2:0, ∑22)
α2 = Σ22Σ2-22 , β2 = Σ22 - Σ22Σ2-22Σ22
dz2Φ(2)(z, 0, ∑) = @(1)(z2；0, ∑22) /1 dgιψ(1)(gι; α1z2,βι),
= ψ(2)(z2; 0, Σ22)Φ(2)(z2, α2z2,β2)
∂22Φ(2)(z, 0, ∑) = -∑^@(1)(Z2；0, ∑22)Φ⑴(z1,α1 z2,β1)
+ ψ(2)(z2; 0, Σ22)∂z2Φ(2)(z2, α2z2, β2)
∂zι ∂z2 Φ(2)(z, 0, Σ) = ψ(2)(zW, ∑).
(33)
(34)
(35)
(36)
(37)
(38)
(39)
(40)
(41)
(42)
(43)
(44)
(45)
(46)
(47)
(48)
(49)
(50)
So denoting ∂% = ∂L ,we get:
∂iZ(b)
(μi + E ∑ijbj)Z(b) + eS(b) E d〃Φ(d)(z, 0, Σ)Σ',i
j	、-----'--------------------}
^^{^^≡
mi(b)
∂k∂iZ(b) = ΣikZ(b) + (μi + £ Σijbj)∂kZ(b)
j
+ (〃k + X ∑kjbj)mi(b) + eS(b) X 32也.Φ(d)(z, 0, ∑)∑',i∑q,k .
j	',q
(51)
(52)
(53)
22
In particular, for the first moment d = 1 we have:
E(ReLU(f )) = μΦ⑴(〃;0,b2) + ψ⑴(μ, 0,σ2)σ2 ,	(54)
which coincides with equation 8. Note that ψ(1)(μ, 0, σ2) = σψ(μ∕σ; 0,1) because of the normal-
ization factor. For the second moments d = 2 we have:
E(ReLU(f1)ReLU(f2)) = ∑12Φ⑵(〃;0, Σ) + μ1μ2Φ⑵(〃;0, Σ) + μιm2(0) + μ2m1(0) (55)
+ X ∑',1∑q,2∂z'∂zqΦ⑵(z, 0, Σ)∣b=0 .	(56)
',q=1,2
which can be rewritten in the form of equation 9.
23