Published as a conference paper at ICLR 2021
On the Impossibility of Global Convergence
in Multi-Loss Optimization
Alistair Letcher
aletcher.github.io
Ab stract
Under mild regularity conditions, gradient-based methods converge globally to a
critical point in the single-loss setting. This is known to break down for vanilla
gradient descent when moving to multi-loss optimization, but can we hope to build
some algorithm with global guarantees? We negatively resolve this open problem
by proving that desirable convergence properties cannot simultaneously hold for
any algorithm. Our result has more to do with the existence of games with no
satisfactory outcomes, than with algorithms per se. More explicitly we construct
a two-player game with zero-sum interactions whose losses are both coercive and
analytic, but whose only simultaneous critical point is a strict maximum. Any
‘reasonable’ algorithm, defined to avoid strict maxima, will therefore fail to con-
verge. This is fundamentally different from single losses, where coercivity implies
existence of a global minimum. Moreover, we prove that a wide range of existing
gradient-based methods almost surely have bounded but non-convergent iterates
in a constructed zero-sum game for suitably small learning rates. It nonetheless
remains an open question whether such behavior can arise in high-dimensional
games of interest to ML practitioners, such as GANs or multi-agent RL.
1 Introduction
Problem Setting. As multi-agent architectures proliferate in machine learning, it is becoming in-
creasingly important to understand the dynamics of gradient-based methods when optimizing multi-
ple interacting goals, otherwise known as differentiable games. This framework encompasses GANs
(Goodfellow et al., 2014), intrinsic curiosity (Pathak et al., 2017), imaginative agents (Racaniere
et al., 2017), synthetic gradients (Jaderberg et al., 2017), hierarchical reinforcement learning (Wayne
& Abbott, 2014; Vezhnevets et al., 2017) and multi-agent RL in general (Busoniu et al., 2008). The
interactions between learning agents make for vastly more complex mechanics: naively applying
gradient descent on each loss simultaneously is known to diverge even in simple bilinear games.
Related Work. A large number of methods have recently been proposed to alleviate the failings
of simultaneous gradient descent: adaptations of single-loss algorithms such as Extragradient (EG)
(Azizian et al., 2019) and Optimistic Mirror Descent (OMD) (Daskalakis et al., 2018), Alternat-
ing Gradient Descent (AGD) for finite regret (Bailey et al., 2019), Consensus Optimization (CO)
for GAN training (Mescheder et al., 2017), Competitive Gradient Descent (CGD) based on solv-
ing a bilinear approximation of the loss functions (Schaefer & Anandkumar, 2019), Symplectic
Gradient Adjustment (SGA) based on a novel decomposition of game mechanics (Balduzzi et al.,
2018; Letcher et al., 2019a), and opponent-shaping algorithms including Learning with Opponent-
Learning Awareness (LOLA) (Foerster et al., 2018) and its convergent counterpart, Stable Opponent
Shaping (SOS) (Letcher et al., 2019b). Let A be this set of algorithms.
Each has shown promising theoretical implications and empirical results, but none offers insight into
global convergence in the non-convex setting, which includes the vast majority of machine learning
applications. One of the main roadblocks compared with single-loss optimization has been noted by
Schaefer & Anandkumar (2019): “a convergence proof in the nonconvex case analogue to Lee et al.
(2016) is still out of reach in the competitive setting. A major obstacle to this end is the identification
of a suitable measure of progress (which is given by the function value in the single agent setting),
since norms of gradients can not be expected to decay monotonously for competitive dynamics in
non-convex-concave games.”
1
Published as a conference paper at ICLR 2021
It has been established that Hamiltonian Gradient Descent converges in two-player zero-sum games
under a “sufficiently bilinear” condition by Abernethy et al. (2019), but this algorithm is unsuitable
for optimization as it cannot distinguish between minimization and maximization (Hsieh et al., 2020,
Appendix C.4). Global convergence has also been established for some algorithms in a few special
cases: potential and Hamiltonian games (Balduzzi et al., 2018), zero-sum games satisfying the two-
sided Polyak-Eojasiewicz condition (Yang et al., 2020), zero-sum linear quadratic games (Zhang
et al., 2019) and zero-sum games whose loss and first three derivatives are bounded (Mangoubi
& Vishnoi, 2020). These are significant contributions with several applications of interest, but do
not include any of the architectures mentioned above. Finally, Balduzzi et al. (2020) show that
GD dynamics are bounded under a ‘negative sentiment’ assumption in smooth markets, which do
include GANS - but this does not imply convergence, as We will show.
On the other hand, failure of global convergence has been shown for the Multiplicative Weights Up-
date method by Palaiopanos et al. (2017), for policy-gradient algorithms by Mazumdar et al. (2020),
and for simultaneous and alternating gradient descent (simGD and AGD) by Vlatakis-Gkaragkounis
et al. (2019); Bailey et al. (2019), with interesting connections to POinCare recurrence. Nonetheless,
nothing is claimed about other optimization methods. Farnia & Ozdaglar (2020) show that GANs
may have no Nash equilibria, but it does not follow that algorithms fail to converge since there may
be locally-attracting but non-Nash critical points (Mazumdar et al., 2019, Example 2).
Finally, Hsieh et al. (2020) uploaded a preprint just after the completion of this work with a similar
focus to ours. They prove that generalized Robbins-Monro schemes may converge with arbitrarily
high probability to spurious attractors. This includes simGD, AGD, stochastic EG, optimistic gradi-
ent and Kiefer-Wolfowitz. However, Hsieh et al. (2020) focus on the possible occurrence of undesir-
able convergence phenomena for stochastic algorithms. We instead prove that desirable convergence
properties cannot simultaneously hold for all algorithms (including deterministic). Moreover, their
results apply only to decreasing step-sizes whereas ours include constant step-sizes. These distinc-
tions are further highlighted by Hsieh et al. (2020) in the further related work section. Taken together,
our works give a fuller picture of the failure of global convergence in multi-loss optimization.
Contribution. We prove that global convergence in multi-loss optimization is fundamentally
incompatible with the ‘reasonable’ requirement that algorithms avoid strict maxima and converge
only to critical points. We construct a two-player game with zero-sum interactions whose losses are
coercive and analytic, but whose only critical point is a strict maximum (Theorem 1). Reasonable
algorithms must either diverge to infinite losses or cycle (bounded non-convergent iterates).
One might hope that global convergence could at least be guaranteed in games with strict minima and
no other critical points. On the contrary we show that strict minima can have arbitrarily small regions
of attraction, in the sense that reasonable algorithms will fail to converge there with arbitrarily high
probability for fixed initial parameter distribution (Theorem 2).
Finally, restricting the game class even further, we construct a zero-sum game in which all algorithms
in A (as defined in Appendix A) are proven to cycle (Theorem 3).
It may be that cycles do not arise in high-dimensional games of interest including GANs. Proving
or disproving this is an important avenue for further research, but requires that we recognise the
impossibility of global guarantees in the first place.
2	Background
2.1 Single losses: global convergence of gradient descent
Given a continuously differentiable function f : Rd → R, let
θk+ι = θk — αVf (θk)
be the iterates of gradient descent with learning rate α, initialised at θ0. Under standard regularity
conditions, gradient descent converges globally to critical points:
Proposition 1. Assume f ∈ C2 has compact sublevel sets and is either analytic or has isolated
critical points. For any θ0 ∈ Rd, define U0 = {f (θ) ≤ f(θ0)} and let L < ∞ be a Lipschitz
Constantfor Vf in Uo. Thenfor any 0 < ɑ < 2/L we have limk θk = θ for some CriticaIpoint G.
2
Published as a conference paper at ICLR 2021
The requirements for convergence are relatively mild:
1.	f has compact sublevel sets iff f is coercive, limkθk→∞ f(θ) = ∞, which mostly holds in
machine learning since f is a loss function.
2.	f has isolated critical points if it is a Morse function (nondegenerate Hessian at critical
points), which holds for almost all C2 functions. More precisely, Morse functions form an
open, dense subset of all functions f ∈ C2(Rd, R) in the Whitney C2-topology.
3.	Global Lipschitz continuity is not assumed, which would fail even for cubic polynomials.
The goal of this paper is to prove that similar (even weaker) guarantees cannot be obtained in the
multi-loss setting - not only for GD, but for any reasonable algorithm. This has to do with the more
complex nature of gradient vector fields arising from multiple losses.
2.2	Differentiable games
Following Balduzzi et al. (2018), we frame the problem of multi-loss optimization as a differentiable
game among cooperating and competing agents/players. These may simply be different internal
components of a single system, like the generator and discriminator in GANs.
Definition 1. A differentiable game is a set of n agents with parameters θ = (θ1, . . . , θn) ∈ Rd and
twice continuously differentiable losses Li : Rd → R, where θi ∈ Rdi for each i and Pi di = d.
Losses are not assumed to be convex/concave in any of the parameters. In practice, losses need only
be differentiable almost-everywhere: think of neural nets with rectified linear units.
If n = 1, the ‘game' is simply to minimise a given loss function. We write NiLk = VθiLk and
Vij Lk = Vθj Vθi Lk for any i, j, k, and define the simultaneous gradient of the game
ξ = (V1L1,..., VnLn)T ∈ Rd
as the concatenation of each player’s gradient. If each agent independently minimises their loss
using GD with learning rate a, the parameter update for all agents is given by θ J θ 一 ɑξ(θ).
We call this simultaneous gradient descent (SimGD), or GD for short. We call G a critical point if
ξ(θ) = 0. Now introduce the 'Hessian, (or Jacobian) of the game as the block matrix
(V11L1 …V 1nL1 ʌ
H = Vξ =	:	...	: I ∈ Rd×d.
VnILn …VnnLn)
Importantly note that H is not symmetric in general unless n = 1, in which case we recover the
usual Hessian H = V2L. However H can be decomposed into symmetric and anti-symmetric
components as H = S + A (Balduzzi et al., 2018). A second useful decomposition has appeared
recently in (Letcher et al., 2019b) and (Schaefer & Anandkumar, 2019): H = Hd + Ho where
Hd and Ho are the matrices of diagonal and off-diagonal blocks; formally, Hd = Li ViiLi . One
solution concept for differentiable games, analogous to the single-loss case, is defined as follows.
Definition 2. A critical point θG is a (strict, local) minimum if H (θG)	0.1
These were named (strict) stable fixed points by Balduzzi et al. (2018), but the term is usually
reserved in dynamical systems to the larger class defined by Hessian eigenvalues with positive real
parts, which is implied but not equivalent to H 0 for non-symmetric matrices.
In particular, strict minima are (differential) Nash equilibria as defined by Mazumdar et al. (2019),
since diagonal blocks must also be positive definite: ViiLi(θG)	0. The converse does not hold.
Algorithm class. This paper is concerned with any algorithm whose iterates are obtained by ini-
tialising θ0 and applying a function F to the previous iterates, namely θk+1 = F(θk, . . . , θ0). This
holds for all gradient-based methods (deterministic or stochastic); most of them are only functions
1For non-symmetric matrices, positive definiteness is defined as H 0 iff uT Hu > 0 for all non-zero
u ∈ Rd. This is equivalent to the symmetric part S of H being positive definite.
3
Published as a conference paper at ICLR 2021
of the current iterate θk, so that θk = Fk (θ0). All probabilistic statements in this paper assume
that θ0 is initialised following any bounded and continuous measure ν on Rd . Continuity is a weak
requirement and widely holds across machine learning, while boundedness mostly holds in practice
since the bounded region can be made large enough to accommodate required initial points.
For single-player games, the goal of such algorithms is for θk to converge to a local (perhaps global)
minimum as k → ∞. The goal is less clear for differentiable games, but is generally to reach a
minimum or a Nash equilibrium. In the case of GANs the goal might be to reach parameters that
produce realistic images, which is more challenging to define formally.
Throughout the text we use the term (limit) cycle to mean bounded but non-convergent iterates.
This terminology is used because bounded iterates are non-convergent if and only if they have at
least two accumulation points, between which they must ‘cycle’ infinitely often. This is not to be
taken literally: the set of accumulation points may not even be connected. Hsieh et al. (2020) provide
a more complete characterisation of these cycles.
Game class. Expecting global guarantees in all differentiable games is excessive, since every
continuous dynamical system arises as simultaneous GD on the loss functions of a differentiable
game (Balduzzi et al., 2020, Lemma 1). For this reason, the aforementioned authors have introduced
a vastly more tractable class of games called markets.
Definition 3. A (smooth) market is a differentiable game where interactions between players are
pairwise zero-sum, namely,
Li(θ) = Li(θi) + X gij(θi, θj)
j6=i
with gij (θi , θj ) + gji (θj , θi) = 0 for all i, j.
This generalises zero-sum games while remaining amenable to optimization and aggregation, mean-
ing that “we can draw conclusions about the gradient-based dynamics of the collective by summing
over properties of its members” (Balduzzi et al., 2020). Moreover, this class captures a large num-
ber of applications including GANs and related architectures, intrinsic curiosity modules, adversar-
ial training, task-suites and population self-play. One would modestly hope for some reasonable
algorithm to converge globally in markets. We will prove that even this is too much to ask.
2.3	Reasonable algorithms
We wish to prove that global convergence is at odds with weak, ‘reasonable’ desiderata. The first
requirement is that fixed points of an optimization algorithm F are critical points. Formally,
F(θ) =θ =⇒ ξ(θ) =0.	(R1)
If not, some agent i could strictly improve its losses by following the gradient -ViLi = 0. There is
no reason for a gradient-based algorithm to stop improving if its gradient is non-zero.
The second requirement is that algorithms avoid strict maxima. Analogous to strict minima, they are
defined for single losses by a negative-definite Hessian H Y 0. Converging to such a point G is the
opposite goal of any meaningful algorithm since moving anywhere away from G decreases the loss.
There are multiple ways of generalising this concept for multiple losses, but Proposition 2 below
justifies that H Y 0 is the weakest one.
Proposition 2. Write λ(A) = Re(Spec(A)) for real parts of the eigenvalues of a matrix A. We
have the following implications, and none of them are equivalences.
max λ(H) < 0 => min λ(H) < 0
HY0
min λ(S) < 0
max λ(Hd) < 0 => min λ(H) < 0
Definition 4. A critical point θG is a (strict, local) maximum if H (θG) Y 0.
4
Published as a conference paper at ICLR 2021
Imposing that algorithms avoid strict maxima is therefore the weakest possible requirement of its
kind. Note that the bottom-left implication Proposition 2 is equivalent to H Y 0 for all i,
so strict maxima are also strict maxima of each player’s individual loss function. Players can all
decrease their losses by moving anywhere away from them. It is exceedingly reasonable to ask that
optimization algorithms avoid these points almost surely. Formally, we require that for any strict
maximum G and bounded region U there are hyperparameters such that
μ ({θ° ∈ U | lim θk = G}) = 0.	(R2)
μ denotes Lebesgue measure. Hyperparameters may depend on the given game and the region U, as
is typical for learning rates in gradient-based methods.
Definition 5 (Reason). An algorithm is reasonable if it satisfies R1 and R2.
Reason is not equivalent to rationality or self-interest. Reason is much weaker, imposing only
that agents are well-behaved regarding strict maxima even if their individual behavior is not self-
interested. For instance, SGA agents do not behave out of self-interest (Balduzzi et al., 2018).
3 Global Convergence in Differentiable Games
3.1	Reasonable algorithms fail to converge globally
Our main contribution is to show that global guarantees do not exist for any reasonable algorithm.
First recall that global convergence should not be expected in all games, since there may be a di-
vergent direction with minimal loss (imagine minimising L = ex). It should however be asked that
algorithms have bounded iterates in coercive games, defined by coercive losses
lim Li (θ) = ∞
kθk→∞
for all i. Indeed, unbounded iterates in coercive games would lead to infinite losses for all agents,
the worst possible outcome. Given bounded iterates, convergence should hold if the Hessian is
nondegenerate at critical points (which must therefore be isolated, recall Proposition 1). We call
such a game nondegenerate. This condition can also be replaced by analyticity of the loss. In the
spirit of weakest assumptions, we ask for convergence when both conditions hold.
Definition 6 (Globality). An algorithm is global if, in a coercive, analytic and nondegenerate game,
for any fixed θ0, iterates θk are bounded and converge for suitable hyperparameters.	(G1)
Note that GD is global for single-player games by Proposition 1. Unfortunately, reason and globality
are fundamentally at odds as soon as we move to two-player markets.
Theorem 1. There is a coercive, nondegenerate, analytic two-player market M whose only critical
point is a strict maximum. In particular, algorithms only have four possible outcomes in M:
1.	Iterates are unbounded, and all players diverge to infinite loss. [Not global]
2.	Iterates are bounded and converge to the strict maximum. [Not reasonable]
3.	Iterates are bounded and converge to a non-critical point. [Not reasonable]
4.	Iterates are bounded but do not converge (cycle). [Not global]
Proof. Consider the analytic market M given by
L1(χ, y) = x6/6 - x2/2 + Xy + 1 (ι⅛ - 1+y2)
L2(x,y) = y6/6 - y2/2 - Xy - 1 ( y-2 - x~^).
4	1 + x2	1 + y2
We prove in Appendix D that M is coercive, nondegenerate, and has a unique critical point at the
origin, which is a strict maximum.	□
5
Published as a conference paper at ICLR 2021
Constructing an algorithm with global guarantees is therefore doomed to be unreasonable in that it
will converge to strict maxima or non-critical points in M.
None of the outcomes of M are satisfactory. The first three are highly objectionable, as already dis-
cussed. The fourth is less obvious, and may even have game-theoretic significance (Papadimitriou
& Piliouras, 2019), but is counter-intuitive from an optimization standpoint. Terminating the itera-
tion would lead to a non-critical point, much like the third outcome. Even if we let agents update
parameters continuously as they play a game or solve a task, they will have oscillatory behavior and
fail to produce consistent outcomes (e.g. when generating an image or playing Starcraft).
The hope for machine learning is that such predicaments do not arise in applications we care about,
such as GANs or intrinsic curiosity. This may well be the case, but proving or disproving global
convergence in these specific settings is beyond the scope of this paper.
Remark. Why can this approach not be used to disprove global convergence for single losses? One
reason is that we cannot construct a coercive loss with no critical points other than strict maxima:
coercive losses, unlike games, always have a global minimum.
3.2 What if there are strict minima
One might wonder if it is purely the absence of strict minima that causes non-convergence, since
strict minima are locally attracting under gradient dynamics. Can we guarantee global convergence
if we impose existence of a minimum, and more, the absence of any other critical points?
Unfortunately, strict minima may have an arbitrarily small region of attraction. Assuming parame-
ters are initialised following any bounded continuous measure ν on Rd , we can always modify M
by deforming a correspondingly small region around the origin, turning it into a minimum while
leaving the dynamics unchanged outside of this region.
For a fixed initial distribution, any reasonable algorithm can therefore enter a limit cycle or diverge
to infinite losses with arbitrarily high probability.
Theorem 2.	Given a reasonable algorithm with bounded continuous distribution on θ0 and a real
number > 0, there exists a coercive, nondegenerate, almost-everywhere analytic two-player mar-
ket Mσ with a strict minimum and no other critical points, such that θk either cycles or diverges to
infinite losses for both players with probability at least 1 - .
fσ (θ) =
Proof. Let 0 < σ < 0.1 and define
(x2 + y2 - σ2)∕2	if kθk ≥ σ
(y2 - 3x2)(x2 + y2 - σ2)∕(2σ2) otherwise,
where θ = (x, y) and ∣∣θk =，x2 + y2 is the standard L2-norm. Note that fσ is continuous since
kθkli→mσ+ fσ(x, y) = 0 = kθkli→mσ- fσ(x).
Now consider the two-player market Mσ given by
LI(X,y) = x6/6 - x2 + fσ(χ,y) + Xy + 1( 1 y 2 - x~^
4 1 + x2	1 + y2
L2(χ,y) = y6/6 - fσ(χ,y) - Xy - 1( 1 y 2 -	χ-2).
4 1 + X2	1 + y2
We prove in Appendix E that Mσ is a coercive, nondegenerate, almost-everywhere analytic game
whose only critical point is a strict minimum at the origin. We then prove that θk cycles or diverges
with probability at least 1 - e, and plot iterates for each algorithm in A.	□
3.3 How do existing algorithms b ehave ?
Any algorithm will either fail to be reasonable or global in M. Nonetheless, it would be interesting
to determine the specific failure that each algorithm in A exhibits. Each of them is defined in
6
Published as a conference paper at ICLR 2021
co
Figure 1: Algorithms in A fail to converge in M with α = γ = 0.01. Single run with standard
normal initialisation, 3000 iterations. The behavior of SGA is slightly different, explained by the
presence of a non-continuous parameter λ jumping between ±1 according to an alignment criterion.
Appendix A, writing α for the learning rate and γ for the Consensus Optimization hyperparameter.
We expect each algorithm to be reasonable and moreover to have bounded iterates in M for suitably
small hyperparameters. If this holds, they must cycle by Theorem 1.
This was witnessed experimentally across 1000 runs for α = γ = 0.01, with every run resulting
in cycles. A single such run is illustrated in Figure 1. Algorithms may follow one of the three
other outcomes for other hyperparameters, for instance diverging to infinite loss if α is too large
or converging to the strict maximum for CO if γ is too large. The point here is to characterise the
‘regular’ behavior which can be seen as that occurring for sufficiently small hyperparameters.
Instead of proving that algorithms must cycle in M, we construct a zero-sum game N with similar
properties as M and prove below that algorithms in A almost surely fail to converge there for small
α, γ. This is stronger than proving the analogous result for M, since N belongs to the even smaller
class of zero-sum games which one might have hoped was well-behaved.
In this light, one might wish to extend Theorem 1 to zero-sum games. However, zero-sum games
cannot be coercive since L1 → ∞ implies L2 → -∞. It is therefore unclear whether global
guarantees should be expected. Note however that N will be weakly-coercive in the sense that
lim
kθik→∞
Li(θi, θ-i)
∞
for all i and fixed θ-i .
Theorem 3.	There is a weakly-coercive, nondegenerate, analytic two-player zero-sum game N
whose only critical point is a strict maximum. Algorithms in A almost surely have bounded non-
convergent iterates in N for α, γ sufficiently small.
Proof. Consider the analytic zero-sum game N given by
L1 = xy - x2/2 + y2/2 + x4/4 - y4/4 = -L2 .
We prove in Appendix F that N is weakly-coercive, nondegenerate, and has a unique critical point
at the origin which is a strict maximum. We prove that algorithms in A have the origin as unique
fixed points, with negative-definite Jacobian for α, γ small, hence failing to converge almost surely.
We moreover prove that algorithms have bounded non-convergent iterates in N for α, γ sufficiently
small. Iterates are plotted for a single run of each algorithm in Figure 3 with α = Y = 0.01.	□
As in M, the behavior of each algorithm may differ for larger hyperparameters. All algorithms
may have unbounded iterates or converge to the strict maximum for large α, while EG and OMD
may even converge to a non-critical point (see proof). All such outcomes are unsatisfactory, though
unbounded iteration will not result in positive infinite losses for both players since L1 = -L2.
7
Published as a conference paper at ICLR 2021
3.4 Corollary: there are no suitable measures of progress
A crucial step in proving global convergence of GD on single losses is showing that the set of
accumulation points is a subset of critical points, using the function value as a ‘measure of progress’.
The fact that this fails for differentiable games implies that there can be no suitable measures of
progress for reasonable algorithms with bounded iterates. We formalise this below, answering the
question of Schaefer & Anandkumar (2019) quoted in the introduction.
Definition 7. A measure of progress for an algorithm given by θk+1 = F (θk) is a continuous map
M : Rd → R, bounded below, suchthatM(F(θ)) ≤ M(θ) andM(F(θ)) = M(θ) iff F (θ) = θ.
Measures of progress are very similar to descent functions, as defined by Luenberger & Ye (1984),
and somewhat akin to Lyapunov functions. The function value f is a measure of progress for single-
loss GD under the usual regularity conditions, while the gradient norm kξk is a measure of progress
for GD in strictly convex differentiable games:
kξ(θ-αξ)k2=kξk2-αξTHtξ+o(α)≤kξk2
for small α. Unfortunately, games like M prevent the existence of such measures in general.
Corollary 1. There are no measures of progress for reasonable algorithms which produce bounded
iterates in M or N .
Assuming the algorithm to be reasonable is necessary: any map is a measure of progress for the
unreasonable algorithm F (θ) = θ. Assuming the algorithm to have bounded iterates in M or N is
necessary: M(θ) = exp(- θ ∙ 1) is a measure of progress for the reasonable but always-divergent
algorithm F(θ) = θ + 1, where 1 is the constant vector of ones.
4 Conclusion
We have proven that global convergence is fundamentally at odds with weak, desirable requirements
in multi-loss optimization. Any reasonable algorithm can cycle or diverge to infinite losses, even in
two-player markets. This arises because coercive games, unlike losses, may have no critical points
other than strict maxima. However, this is not the only point of failure: strict minima may have
arbitrarily small regions of attraction, making convergence arbitrarily unlikely.
Limit cycles are not necessarily bad: they may even have game-theoretic significance (Papadimitriou
& Piliouras, 2019). This paper nonetheless shows that some games have no satisfactory outcome in
the usual sense, even in the class of two-player markets. Players should neither escape to infinite
losses, nor converge to strict maxima or non-critical points, so cycling may be the lesser evil. The
community is accustomed to optimization problems whose solutions are single points, but cycles
may have to be accepted as solutions in themselves.
The hope for machine learning practitioners is that local minima with large regions of attraction
prevent limit cycles from arising in applications of interest, including GANs. Proving or disproving
this is an interesting and important avenue for further research, with real implications on what to
expect when agents learn while interacting with others. Cycles may for instance be unacceptable in
self-driving cars, where oscillatory predictions may have life-threatening implications.
8
Published as a conference paper at ICLR 2021
References
Jacob Abernethy, Kevin A. Lai, and Andre Wibisono. Last-iterate convergence rates for min-max
optimization. ArXiv e-prints, 2019.
PA Absil, Robert Mahony, and Ben Andrews. Convergence of the iterates of descent methods for
analytic cost functions. SIAM Journal on Optimization, 16, 01 2005.
Walss Azizian, Ioannis Mitliagkas, Simon Lacoste-JUlien, and GaUthier GideL A tight and unified
analysis of extragradient for a whole spectrum of differentiable games. ArXiv e-prints, 2019.
James P. Bailey, GaUthier Gidel, and Georgios PilioUras. Finite regret and cycles with fixed step-size
via alternating gradient descent-ascent. ArXiv e-prints, 2019.
D. BaldUzzi, S. Racaniere, J. Martens, J. Foerster, K. TUyls, and T. Graepel. The Mechanics of
n-Player Differentiable Games. ICML, 2018.
David BaldUzzi, Wojciech M. Czarnecki, Tom Anthony, Ian Gemp, Edward HUghes, Joel Leibo,
Georgios PilioUras, and Thore Graepel. Smooth markets: A basic mechanism for organizing
gradient-based learners. In International Conference on Learning Representations, 2020.
Saugata Basu, Richard Pollack, and Marie-Francoise Roy. Algorithms in Real Algebraic Geometry.
Springer-Verlag Berlin Heidelberg, 2006.
L. Busoniu, R. Babuska, and B. De Schutter. A comprehensive survey of multiagent reinforce-
ment learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and
Reviews), 38(2):156-172, March 2008.
C. Daskalakis, A. Ilyas, V. Syrgkanis, and H. Zeng. Training GANs with Optimism. In International
Conference on Learning Representations, 2018.
Constantinos Daskalakis and Ioannis Panageas. The limit points of (optimistic) gradient descent in
min-max optimization. NIPS, 2018.
W. Decker, G.-M. Greuel, G. Pfister, and H. Schonemann. Singular 4-1-2 — A computer algebra
system for polynomial computations. http://www.singular.uni-kl.de, 2019.
Farzan Farnia and Asuman Ozdaglar. Do GANs always have Nash equilibria? ICML, 2020.
J.	N. Foerster, R. Y. Chen, M. Al-Shedivat, S. Whiteson, P. Abbeel, and I. Mordatch. Learning with
Opponent-Learning Awareness. AAMAS, 2018.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and
Y. Bengio. Generative Adversarial Networks. NIPS, 2014.
Ya-Ping Hsieh, Panayotis Mertikopoulos, and Volkan Cevher. The limits of min-max optimization
algorithms: convergence to spurious non-critical sets. ArXiv e-prints, 2020.
M. Jaderberg, W. M. Czarnecki, S. Osindero, O. Vinyals, A. Graves, D. Silver, and K. Kavukcuoglu.
Decoupled Neural Interfaces using Synthetic Gradients. ICML, 2017.
K.	Lange. Optimization. Springer Texts in Statistics. Springer New York, 2013.
J. D. Lee, M. Simchowitz, M. I. Jordan, and B. Recht. Gradient Descent Only Converges to Mini-
mizers. In 29th Annual Conference on Learning Theory, volume 49 of Proceedings of Machine
Learning Research, pp. 1246-1257, 2016.
Alistair Letcher, David Balduzzi, SebaStien Racaniere, James Martens, Jakob Foerster, Karl Tuyls,
and Thore Graepel. Differentiable game mechanics. Journal of Machine Learning Research,
2019a.
Alistair Letcher, Jakob Foerster, David Balduzzi, Tim Rocktaschel, and Shimon Whiteson. Stable
opponent shaping in differentiable games. In International Conference on Learning Representa-
tions, 2019b.
9
Published as a conference paper at ICLR 2021
David Luenberger and Yinyu Ye. Linear and Nonlinear Programming, volume 67. 01 1984.
Oren Mangoubi and Nisheeth K. Vishnoi. A second-order equilibrium in nonconvex-nonconcave
min-max optimization: Existence and algorithm. ArXiv e-prints, 2020.
Eric Mazumdar, Lillian J. Ratliff, Michael I. Jordan, and S. Shankar Sastry. Policy-gradient al-
gorithms have no guarantees of convergence in linear quadratic games. In Proceedings of the
19th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS ’20, pp.
860-868. International Foundation for Autonomous Agents and MUItiagent Systems, 2020.
Eric V. Mazumdar, Michael I. Jordan, and S. Shankar Sastry. On finding local nash equilibria (and
only local nash equilibria) in zero-sum games. ArXiv e-prints, 2019.
L.	Mescheder, S. Nowozin, and A. Geiger. The Numerics of GANs. NIPS, 2017.
G.S. Nelson. A User-Friendly Introduction to Lebesgue Measure and Integration. Student Mathe-
matical Library. American Mathematical Society, 2015.
Gerasimos Palaiopanos, Ioannis Panageas, and Georgios Piliouras. Multiplicative weights update
with constant step-size in congestion games: Convergence, limit cycles and chaos. NIPS, 2017.
I. Panageas and G. Piliouras. Gradient Descent Only Converges to Minimizers: Non-Isolated Critical
Points and Invariant Regions. In ITCS 2017, volume 67 of Leibniz International Proceedings in
Informatics, pp. 2:1-2:12, 2017.
Christos Papadimitriou and Georgios Piliouras. Game dynamics as the meaning ofa game. SIGecom
Exch., 16(2):53-63, May 2019.
D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiosity-driven Exploration by Self-supervised
Prediction. ICML, 2017.
S. Racaniere, T. Weber, D. P. Reichert, L. Buesing, A. Guez, D. Jimenez Rezende, A. Puigdomenech
Badia, O. Vinyals, N. Heess, Y. Li, R. Pascanu, P. Battaglia, D. Hassabis, D. Silver, and D. Wier-
stra. Imagination-Augmented Agents for Deep Reinforcement Learning. NIPS, 2017.
Florian Schaefer and Anima Anandkumar. Competitive gradient descent. NIPS, 2019.
M. Spivak. Calculus On Manifolds: A Modern Approach To Classical Theorems Of Advanced
Calculus. Avalon Publishing, 1971.
A. S. Vezhnevets, S. Osindero, T. Schaul, N. Heess, M. Jaderberg, D. Silver, and K. Kavukcuoglu.
FeUdal Networks for Hierarchical Reinforcement Learning. ICML, 2017.
Emmanouil-Vasileios Vlatakis-Gkaragkounis, Lampros Flokas, and Georgios Piliouras. Poincare
recurrence, cycles and spurious equilibria in gradient-descent-ascent for non-convex non-concave
zero-sum games. NIPS, 2019.
G. Wayne and L. F. Abbott. Hierarchical control using networks trained with higher-level forward
models. Neural Computation, 26(10):2163-2193, 2014.
Junchi Yang, Negar Kiyavash, and Niao He. Global convergence and variance-reduced optimization
for a class of nonconvex-nonconcave minimax problems. ArXiv e-prints, 2020.
Kaiqing Zhang, Zhuoran Yang, and Tamer Basar. Policy optimization provably converges to nash
equilibria in zero-sum linear quadratic games. NIPS, 2019.
10
Published as a conference paper at ICLR 2021
Appendix
A Algorithms and experiment hyperparameters
Each algorithm in A cited in the ‘Related Work’ section can be defined as F (θ) = θ - αG(θ) for
some continuous G : Rd → Rd . We have already seen that simultaneous GD is given by GGD = ξ .
The only examples in this paper are two-player games, for which AGD is given by
G	ξ1(θ1,θ2)
GAGD = ξ2(θ1 -αξ1,θ2)
The other algorithms are given by
GEG = ξ ◦ (id - αξ)
GSGA = (I + λAT)ξ
GCGD = (I + αHo)-1ξ
GLOLA = (I - aHo)ξ - α diag(HT▽L)
GOMD = 2ξ(θk) - ξ(θk-1)
GCO = (I+γHT)ξ
GLA= (I - αHo)ξ
GSOS = (I - aHo)ξ - Pa diag(HT VL).
For OMD, the previous iterate can be uniquely recovered as θk-1 = (id - αξ)-1 (θk) using the
proximal point algorithm if kHk ≤ L and α < 1/L, giving
GOMD = 2ξ - ξ ◦ (id - αξ)-1 .
In all experiments we initialise θ0 following a standard normal distribution and use a learning rate
α = 0.01, with γ = 0.01 for CO. Learning rates αi could be chosen to be different for each player
i, but we set them to be equal throughout this paper for simplicity. Claims regarding the behavior of
each algorithm for sufficiently small α mean that all αi should be sufficiently small. The λ parameter
for SGA is obtained by the alignment criterion introduced in the original paper,
λ = Sign (hξ,HtξihATξ,HTξ).
Similarly, thep parameter for SOS is given by a two-part criterion which need not be described here.
Accompanying code for all experiments can be found at https://github.com/aletcher/
impossibility-global-convergence.
B	Proof of Proposition 1
We first prove a lemma and state a standard optimization result.
Lemma 0. Let G ∈ C1(U, Rd) for an open set U. If G is L-Lipschitz then supθ∈U kVG(θ)k ≤ L.
The proof is an adaptation of (Panageas & Piliouras, 2017, Lemma 7) for non-convex sets.
Proof. Fix any θ ∈ U and > 0. Since U is open, the ball Br(θ) of radius r centered at θ is
contained in U for some r > 0. By Taylor expansion, for any unit vector θ0,
kG(θ + rθ0) - G(θ)k ≥ r kVG(θ)θ0k - o(r) ≥ r kVG(θ)θ0k -r
for r sufficiently small. Since G is L-Lipschitz, we obtain
r kVG(θ)θ0k ≤ kG(θ + rθ0) - G(θ)k +r≤ r(L+).
Since was arbitrary, kVG(θ)θ0k ≤ L for any unit θ0. By definition of the norm, we obtain
kVG(θ)k = sup kVG(θ)θ0k ≤L
kθ0k=1
for all θ ∈ U and hence supθ∈u kVG(θ)k ≤ L.	□
Proposition ((Lange, 2013, Prop. 12.4.4) and (Absil et al., 2005, Th. 4.1)). Assume f has L-
Lipschitz gradient and is either analytic or has isolated critical points. Then for any 0 < α < 2/L
and θ0 ∈ Rd we have
lim ∣∣θkk = ∞ or limθk = θ
for some critical point θ. If f moreover has compact Sublevel sets then the latter holds, limk θk = θ.
11
Published as a conference paper at ICLR 2021
We can now prove Proposition 1, which avoids requiring Lipschitz continuity by proving that iterates
are contained in the sublevel set given by θ0 for appropriate learning rate α.
Proposition 1. Assume f ∈ C2 has compact sublevel sets and is either analytic or has isolated
critical points. For any θ0 ∈ Rd, define U0 = {f (θ) ≤ f (θ0)} and let L < ∞ be a Lipschitz
Constantfor Vf in Uo. Thenfor any 0 < ɑ < 2/L we have limk θk = θ for some CriticaIpoint G.
Proof. Note that Vf ∈ C 1, so f has L-LiPschitz gradient inside any compact set U for some finite
L, and supθ∈U kV2f (θ)k ≤ L by Lemma 0. Now define Uα = {θ - tαVf (θ) | t ∈ [0, 1], θ ∈ U0}
and the continuous function L(α) = supθ∈Uα V2f (θ). Notice that U0 ⊂ Uα0 for all α. We prove
that αL(α) < 2 implies Uα = U0 and in particular, L(α) = L(0). By Taylor expansion,
t2α2
f(θ - tαVf) = f (θ) - α kVf (θ)k2 + FVf (θ)TV2f (θ - t0αVf )f (θ)
for some t0 ∈ [0, t] ⊂ [0, 1]. Since θ - t0αVf ∈ Uα, it follows that
f(θ — tαVf) ≤ f(θ) - α kVf (θ)k2 (1 - αL(α)∕2) ≤ f(θ)
for all αL(α) < 2. In particular, θ - tαVf ∈ U0 and hence Uα = U0. We conclude that αL(α) < 2
implies L(α) = L(0), implying in turn αL(0) < 2. We now claim the converse, namely that
αL(0) < 2 implies αL(α) < 2. For contradiction, assume otherwise that there exists α0L(0) < 2
with α0L(α0) ≥ 2. Since αL(α) is continuous and 0L(0) = 0 < 2, there exists αG ≤ α0 such that
αGL(0) < 2 and αGL(αG) = 2. This is in contradiction with continuity:
2 = αGL(αG) = lim αL(α) = lim αL(0) = αGL(0) .
α-α-	α→α-
Finally we conclude that Uα = U0 for all αL(0) < 2, and in particular, for all αL < 2. Finally,
θk ∈ U0 implies θk+1 ∈ Uα = U0 and hence θk ∈ U0 by induction. The result now follows by
applying the previous proposition to f |u0 .	□
C Proof of Proposition 2
Proposition 2. Write λ(A) = Re(Spec(A)) for real parts of the eigenvalues of a matrix A. We
have the following impliCations, and none of them are equivalenCes.
max λ(H) < 0	min λ(H) < 0
H Y 0
min λ(S) < 0
max λ(Hd) < 0 => min λ(H) < 0
The top row is dynamics-based, governed by the collective Hessian, while the bottom row is game-
theoretic whereby Hd = L ViiLi decomposes into agentwise Hessians. The left and right triangles
collide respectively to strict maxima and saddles for single losses, since H = S = Hd = V2L.
Proof. First note that H Y 0 ^⇒ S Y 0 ^⇒ max λ(S) < 0, so the leftmost term can be
replaced by max λ(S) < 0.
We begin with the leftmost implications. If max λ(S) < 0 then S Y 0 by symmetry of S, implying
both H Y 0 since uTHu = uTSu for all u ∈ Rd, and negative definite diagonal blocks V2Lii Y 0;
finally Hd Y 0. In particular this implies max λ(H) < 0 and max λ(Hd) Y 0 since real parts of
eigenvalues of a negative definite matrix are negative.
The rightmost implications follow as above by contraposition: if min λ(S) ≥ 0 then S	0, which
implies H 0 and Hd	0 and hence min λ(H) ≥ 0, min λ(Hd) ≥ 0.
The top and bottom implications are trivial.
12
Published as a conference paper at ICLR 2021
The diagonal implications hold by a trace argument:
Xλi(H) = Tr(H) = Tr(Hd) = Xλi(Hd),
i
i
hence max λ(H) < 0 implies the LHS is negative and thus	i λi (Hd) < 0. It follows that
λi(Hd) < 0 for some i and finally min λ(Hd) < 0. The other diagonal holds identically.
We now prove that no implication is an equivalence. For the leftmost implications,
H = -21	-21
has max λ(Hd) = -1 < 0 while max λ(S) = 3 > 0, and
H = -24 -44
has max λ(H) = -1 < 0 while max λ(S) = 2 > 0. This also proves the diagonal implications:
the first matrix has min λ(Hd) = -1 < 0 but max λ(H) = 3 > 0, and the second matrix has
min λ(H) = -1 < 0 but max λ(Hd) = 2 > 0.
For the rightmost implications, swap the sign of the diagonal elements for the two matrices above.
The top and bottom implications are trivially not equivalences:
H=Hd =	10	-01
has min λ(H) = min λ(Hd) = -1
□
D Proof of Theorem 1
The variable changes
(x0, y0) = (y, -x) ,
< 0 but max λ(H) = max λ(Hd) = 1 > 0.
(x0, y0) = (-y, x) ,	(x0, y0) = (-x, -y)
⑴
will be useful, taking the positive quadrant x, y ≥ 0 to the other three.
Theorem 1. There is a coercive, nondegenerate, analytic two-player market M whose only critical
point is a strict maximum. In particular, algorithms only have four possible outcomes in M:
1.	Iterates are unbounded, and all players diverge to infinite loss. [Not global]
2.	Iterates are bounded and converge to the strict maximum. [Not reasonable]
3.	Iterates are bounded and converge to a non-critical point. [Not reasonable]
4.	Iterates are bounded but do not converge (cycle). [Not global]
For intuition purposes, M was constructed by noticing that there is no necessary reason for the
local minima of two coercive losses to coincide: the gradients of each loss may only simultaneously
vanish at a local maximum in each player’s respective coordinate. The highest-order terms (first
and last) provide coercivity in both coordinates while still having zero-sum interactions. The -x2
and -y2 terms yield a strict local maximum at the origin, while the ±xy terms provide opposite
incentives around the origin, preventing any other simultaneous critical point to arise.
Proof. Write θ = (x, y) and consider the analytic market M given by
L1 = x6/6 — x2/2 + Xy + ；
L2 = y6/6 - y2/2 - xy - 4
y4
1 + x2
y4
1 + x2
X4 )
1 + y2 J
x4 )
1 + y2 J
13
Published as a conference paper at ICLR 2021
with simultaneous gradient
ξ
x5 - X + y - 2(l+X2)2
y - y- X - 2(2)2
We prove 'by hand' that the origin θ= 0 is the only critical point (solution to ξ = 0). See further
down for an easier approach based on Sturm’s theorem, computer-assisted though equally rigorous.
We can assume χ,y ≥ 0 since any other solution can be obtained by a quadrant variable change (f).
Now assume for contradiction that ξ = 0 with y 6= 0.
1.	We first show that y > 1. Indeed,
0 = ξ2= y5-y-X-，x4y 八,--<y5-y = y(y4 - 1)
ξ2	y y 2(1 + y2)2	1 + x2 y y y(y )
implies y > 1 since y ≥ 0.
2.	We now show that y < 1.5. First assume for contradiction that X ≥ y, then
43
ξι = y — x + χ5 — —--χ — ~	> > 1 — X + χ5  χ5∕8 — χ3∕2 := h(x).
2(1 + X2)2	1 + y2
Now
h0(x)=当X4 - χx2 - 1
82
has unique positive root
∕6÷2√79
Xo = NF^-
and h(X) → ∞ as X → ∞, hence h attains its minimum atX0 and plugging X0 yields a contradiction
ξ1>	h(X0) >	0 .
We conclude that X < y, but combining this with X ≥ 0 yields
ξ2>	-2y+y5 -y5/8-y3 = y(7y4/8 - y2 -2)>	7y4/8 - y2 -2>	0
for all y ≥ 1.5, since the rightmost polynomial is positive at y = 1.5 and has positive derivative
7y3 /2-2y=y(7y2/2-2) ≥ 7(1.5)2/2 - 2 >	0.
We must therefore have y < 1.5 as required.
3.	It remains only to show that ξ1>	0 for all 1 < y < 1.5. First notice that fx(y) = ξ1(X, y) is
concave in y for any fixed X ≥ 0 since
f 0 (y) = 1 - 2y3X + 2x3 -y—
fx(y) 1 (1+ X2)2 + 2x (1 + y2)2
and so
fxx(y) = - (⅛ψ
+ 2X3
1 + y2 - 4y2
(1 + y2)3
6y2χ	_ 2 3 3y2 - 1
(1 + X2)2 - X (1+ y2)3
≤0
for y> 1. It follows that fx attains its infimum on the boundary y ∈ {1, 1.5}, so it suffices to check
that ξ1(X, 1)>	0 and ξ1(X, 1.5) > 0 for all X ≥ 0. First notice that
g(X) := .1 : 2、2
2(1 + X2)2
satisfies
0 f 、	1 + X2 — 4x2	1 — 3x2
g (X) = 2(1+ X2)2 = 2(1+ X2)2 ,
14
Published as a conference paper at ICLR 2021
which has a unique positive root at xo = 1/√3. This critical point of g must be a maximum since
g(x) > 0 for x > 0 and g(x) → 0 as x → ∞. It follows that
g(X) ≤ g(x0) = 2√3(1 + 1∕3)2 = 3√3/32 .
We now obtain
ξι(x, 1) ≥ x5 — x3∕2 — x + 1 — 3√3∕32 := P(X)
and
ξ1(x, 1.5) ≥ X5 — 4x3/13 — X + 1.5 — (1.5)43√3∕32 := q(x).
Notice that
p0(x) = 5x4 — 3x2/2 — 1
has unique positive root
∕3÷√89
XO = YJr-
and p(x) → ∞ as x → ∞, hence p attains its minimum at x0 and plugging x0 yields
ξ1(X, 1) ≥ p(X0) > 0 .
Similarly for q we have
q0(X) = 5X4 — 12X2/13 — 1
has unique positive root
∕6 + √881
x0 = NF^-
and plugging X0 yields
ξ1(X, 1.5) ≥ q(X0) > 0 .
We conclude that
ξ1(X,y) ≥ min(ξ1(X, 1),ξ1(X, 1.5)) > 0
and the contradiction is complete, hence y = 0. Finally ξ2 = 0 = x, so 8 = 0 is the unique critical
point as required. Now the Hessian at G is
H(θG)=	——11	—11
which is negative definite since S(G) = —I Y 0, so G is a nondegenerate strict maximum and
M is nondegenerate. It remains only to prove coercivity of M, namely coercivity of L1 and L2 .
Coercivity of L1 follows by noticing that the dominant terms are X6/6 and y4/(1 + X2). Formally,
first note that ι+xy2 ≤ χ4, hence
L1 ≥ x6∕6 — χ4∕4 — χ2∕2 + χy + 1 (—y—2).
4	1 + X2
Now xy ≥ —|xy| ≥ —(2x2 + y2∕8) by Young’s inequality, hence
L1 ≥ X6∕6 — χ4∕4 — 5χ2∕2 — y2∕8 +1 (-y^2).
4	1 + x2
For any sequence kθk → ∞, either |X| → ∞ or |X| is bounded above by some k ∈ R and |y| → ∞.
In the latter case, we have
lim L1 ≥ lim —k4/4 — 5k2 /2 — y2/8 +
l∣θk→∞	∣y∣一∞
y4
------=∞
4(1 + k2)
since the leading term y4 is of even degree and has positive coefficient, so we are done. Otherwise,
for |X| → ∞, we pursue the previous inequality to obtain
L1 ≥ χ6∕6 — χ4∕4 — 5x2∕2 + % ( ]2y 2 — 1
15
Published as a conference paper at ICLR 2021
Now notice that y2 ≥ x2 ≥ 1 implies
L1 ≥ x6∕6-x4∕4- 5x2∕2 + x2 (⅞⅛
≥ x6/6 - x4/4 - 5x2/2 - x2/8 .
On the other hand, x2 ≥ y2 also implies
L1 ≥ x6∕6 - x4∕4 - 5x2∕2 - x2∕8
by discarding the first (positive) term in the brackets. Both cases lead to the same inequality and
hence, for any sequence with |x| → ∞,
lim L1 ≥ lim x6∕6 - x4∕4 - 5x2 ∕2 - x2∕8 = ∞
k θk→∞	∣x∣→∞
since the leading term x6 has even degree and positive coefficient. Hence L1 is coercive, and the
same argument holds for L2 by swapping x and y . As required we have constructed a coercive,
nondegenerate, analytic two-player market M whose only critical point is a strict maximum.
In particular, any algorithm either has unbounded iterates with infinite losses or bounded iterates. If
they are bounded, they either fail to converge or converge. If they converge, they either converge to
a non-critical point or a critical point, which can only be the strict maximum.
[For an alternative proof that θ = 0 is the only critical point, We may take advantage of computer
algebra systems to find the exact number of real roots using the resultant matrix and Sturm’s theorem.
Singular (Decker et al., 2019) is one such free and open-source system for polynomial computations,
backed by published computer algebra references. In particular, the rootsur library used beloW is
based on the book by Basu et al. (2006). First convert the equations into polynomials:
2(1	+ x2)2(1	+ y2)(x5	- x + y) - y4x(1	+ y2) -	2x3(1	+ x2)2	= 0
2(1	+ y2)2(1	+ x2)(y5	- y - x) - x4y(1	+x2) -	2y3 (1	+ y2)2	= 0.
We compute the resultant matrix determinant of the system With respect to y, a univariate polynomial
P in x Whose zeros are guaranteed to contain all solutions in x of the initial system. We then use the
Sturm sequence of P to find its exact number of real roots. This is implemented With the Singular
code beloW, Whose output is1.
LIB "solve.lib"; LIB "rootsur.lib";
ring r = (0,x),(y),dp;
poly pl = 2*(1+x^2)^2*(1+y^2)*(x^5-x+y)-y^4*x*(1+y^2)-2*x^3*(1+x^2)^2;
poly p2 = 2*(1+y^2)^2*(1+x^2)*(y^5-y-x)-x^4*y*(1+x^2)-2*y^3*(1+y^2)^2;
ideal i = p1,p2;
poly f = det(mp_res_mat(i));
ring s = 0,(x,y),dp; poly f = imap(r, f);
nrroots(f);
We know that θ = 0 is a real solution, so θ must be the unique critical point.]	口
E	Proof of Theorem 2
Theorem 2. Given a reasonable algorithm with bounded continuous distribution on θ0 and a real
number > 0, there exists a coercive, nondegenerate, almost-everywhere analytic two-player mar-
ket Mσ with a strict minimum and no other critical points, such that θk either cycles or diverges to
infinite losses for both players with probability at least 1 - .
Proof. We modify the construction from Theorem 1 by deforming a small region around the maxi-
mum to replace it with a minimum. First let 0 < σ < 0.1 and define
(x2 + y2 - σ2)∕2	if kθk ≥ σ
(y2 - 3x2)(x2 + y2 - σ2)∕(2σ2) otherwise,
where θ = (x, y) and ∣∣θk = ,x2 + y2 is the standard L2-norm. Note that fσ is continuous since
lim fσ (θ) = 0 = lim fσ (θ) .
kθk→σ+ σ	kθk→σ- σ
fσ (θ) =
16
Published as a conference paper at ICLR 2021
Now consider the two-player market Mσ given by
L1 = x6/6 - x2 + fσ + Xy + 4 (1+懑
X4 )
1 + y2 J
L2 = y6/6 - fσ - Xy - 4
y4	x4	)
1 + x2	1 + y2 J
The resulting losses are continuous but not differentiable; however, they are analytic (in particular
smooth) almost everywhere, namely, for all θ not on the circle of radius σ. This is sufficient for
the purposes of gradient-based optimization, noting that neural nets also fail to be everywhere-
differentiable in the presence of rectified linear units.
We claim that Mσ has a single critical point at the origin θ = 0. First note that
ξMσ = ξM0
χ5 - X + y - 2(ι+x2)2
y - y- X - 2(i⅛2)2
ξM
for all kθk ≥ σ, where M is the game from Theorem 1. It was proved there that the only real
solution to ξ = 0 is the origin, which does not satisfy kθk ≥ σ . Any critical point must therefore
satisfy kθk < σ, for which
/X5 + X + y - 2x(3x2 + y2"σ2 - 2(i+x2)2 - ι++y2、
ξ = ξMσ =	4	3	.
y5 + y- X - 2y(y2 - ^)42 - wy⅛ -
First note that θ = 0 is a critical point; We prove that there are no others. The continuous parameter
σ prevents us from using a formal verification system, so we must work ‘by hand’. Warning: the
proof is a long inelegant string of case-by-case inequalities.
Assume for contradiction that ξ = 0 With θ 6= 0. First note that kθk < σ implies |X|, |y| < σ, and
X = 0 or y = 0 implies X = y = 0 using ξ1 = 0 or ξ2 = 0 respectively. We can therefore assume
0 < |X|, |y| < σ. We can moreover assume that X > 0, the opposite case folloWing by the quadrant
change of variables (X0, y0) = (-X, -y).
1.	We begin with the case σ∕2 ≤ x < σ. First notice that
X + y — 2x(3x2 + y2)∕σ2 = x(1 — 6x2∕σ2) + y(1 — 2Xy∕σ2) ≤ x(1 — 3/2) + y(1 — y∕σ)
and the rightmost term attains its maximum value for y = σ∕2, hence
X + y - 2X(3X2 + y2)∕σ2 ≤ -X∕2 + σ∕4 ≤ 0 .
This implies
5	y4X	X3	5	X3	3	2	1	-X3y4
ξ1 ≤ X - 2(1+ x2)2 - 1+^2 <x - 1+^2 <x (1 - y - 1+^2) = T+y2 < 0
using X2 + y2 < 1, which is a contradiction to ξ = 0.
2.	We proceed with the case X < σ∕2 and |y| ≤ σ∕2. First, y < 0 implies the contradiction
ξ2 <y - 2y3∕σ2- 2(1；" - i⅛ </2-y (⅛ + ⅛) <y (1 - 25 - 22) < 0,
so we can assume y > 0. In particular we have (1 - 2y(y + X)∕σ2) > 0. Ify ≤ X, we also obtain
ξ2 <y5 + (y - x) (1 - 2y(y+ x"σ2) - f⅛ < y3 (y2 - ɪ^) < —⅛ < 0,
+X	+X	+X
so we can assume X < y. There are again two cases to distinguish. IfX < σ∕2 - bσ2 with b = 0.08,
X(1 - 6X2∕σ2) + y(1 - 2Xy∕σ2) > X(1 - 3(1∕2 - σb)) + X(1 - (1∕2 - σb)) > 4σbX
17
Published as a conference paper at ICLR 2021
which implies the contradiction
ξι > 4σbx-，y4x 八,-二 >σx f 4b -	-4)> σx(4b - ɪ - ɪ) > 0 .
ξ1	2(1 + x2)2	1+ y2	25	22	25	22
Finally assume X ≥ σ∕2 一 bσ2. Then We have
(y — x)(1 — 2y(x + y)∕σ2) < bσ2(1 — 4x2/σ2) < bσ2(1 — (1 — 2σb)2) = 4σ3b2(1 — σb) < 4σ3b2
and obtain
ξ <y5 +4σ3b2 -	< σ3 (σ2∕25 +4b2 - W).
1 + x2	1 + σ2∕4
We claim that the rightmost term is negative. Indeed, the quantity inside the brackets has derivative
σ∕24 + (1+ σ2/4)2 (3b(1 + σ2∕4) + σ(I∕2 - σb"2) > 0
and so its supremum across σ ∈ [0, 0.1] must be attained at σ = 0.1. We obtain the contradiction
ξ2 <^ S01/25 +4b2 - (+⅜⅛) < 0
for b = 0.08 and σ > 0, as required.
3.	Finally, consider the case x < σ∕2 and |y| > σ∕2. First, y < 0 implies the contradiction
ξ1 < x + y - 2x(3x2 + y2)∕σ2 < -2x(3x2 + y2) < 0
so We can assume y > 0. NoW assume y < σ - x(1 + σ2 ). Then
x(1 - 6x2∕σ2) + y(1 - 2xy∕σ2) > -x∕2 + y(1 - y∕σ) > -x∕2 + x(1 + σ2) > x(1∕2 + σ2) ,
Which yields the contradiction
ξ1 >x Q+ σ2 - 2(⅛ψ - 1⅛) > X(1/2 + σ2 - σ4 - σ2∕4) > X(I/2 - 1/4) > 0 .
We can therefore assume y ≥ σ - x(1 + σ2 ). We have
(y - x)(1 - 2y(y + x)∕σ2) < (y - x)(1 - (y + x)∕σ) ≤ (y - x)(1 - (1 - σx)) < σx(y - x)
Which attains its maximum in x at x = y∕2, hence
ξ2 < y5 -
y3	+ σy2 <
1 + X2	4
2
1 + σ2
+4
Finally We obtain the contradiction
ξ2 <
σy2 ( 5σ2 + 4σ4 — 1
4 1	1 + σ2
<0
for all σ < 0.1. All cases lead to contradictions, so we conclude that G is the only critical point, with
positive definite Hessian
H(θG)= (-11 11)	0,
hence θGis a strict minimum. Now notice that M0 has the same dominant terms as M from Theorem
1, so coercivity of M0 follows from the same argument. Since Mσ is identical to M0 outside the
σ-ball Bσ = {(X, y) ∈ R2 | kθk < σ}, coercivity ofM0 implies coercivity ofMσ for any σ.
Fix any reasonable algorithm F, any bounded continuous measure ν on Rd with initial region U,
and any > 0. We abuse notation somewhat and write Fσk(θ0) for the kth iterate of F in Mσ with
initial parameters θ0. We claim that there exists σ > 0 such that
Pν θ0 ∈ U and limFσk(θ0) = θG < .
18
Published as a conference paper at ICLR 2021
Figure 2: Algorithms in A fail to converge in Mσ with σ = α = γ = 0.01. Single run with standard
normal initialisation, 3000 iterations.
Since θ is the only critical point and Mσ is coercive, this implies bounded but non-convergent
iterates or divergent iterates with infinite losses with probability at least 1 - , proving the theorem.
To begin, μ(Bσ) → 0 as σ → 0 implies that We can pick σ0 > 0 such that PV (θo ∈ B。，)< e/2 by
continuity of ν with respect to Lebesgue measure.
Now let U be the closure of U and define D = U ∩{∣∣θ∣∣≥ σ0}. Note that D is compact since U is
compact and closed subsets of a compact set are compact. F is reasonable, D is bounded and θ= 0
is a strict maximum in M0, so there are hyperparameters such that the stable set
Z = {θ0 ∈ D | lim F0k(θ0) = 0}
k
has zero measure. We claim that
Zδ ：= {θο ∈ D i inf I制(θο)∣∣ <δ}
k∈N
has arbitrarily small measure as δ → 0. Assume for contradiction that there exists α > 0 such that
μ(Zδ) ≥ a for all δ > 0. Then Zδ ⊂ Zδ0 and μ(Zδ) ≤ μ(D) < ∞ for all δ < δ0 implies
“	∩z1
n∈N
lim μ
n→∞
(ZI)
≥α
by Nelson (2015, Exercise 1.19). On the other hand,
\ Zn = Z0
n∈N
yields the contradiction 0 = μ(Z0) ≥ α. We conclude that Zδ has arbitrarily small measure, hence
there exists δ > 0 such that
Pν (θ0 ∈ Zδ ) < e/2
by continuity of ν. Now let σ = min{σ0, δ} and notice that
θ0 ∈ D \ Zδ	=⇒	inf∣∣F(k(θο)∣∣ ≥ δ ≥ σ	=⇒	inf∣∣Fk(θο)∣∣ ≥ σ,
where the last implication holds since Mσ and M0 are indistinguishable in {kθk ≥ σ}, so the
algorithm must have identical iterates Fσk(θ0) = F0k(θ0) for all k. It follows by contraposition that
limk Fk(θ() = θ implies infk ∣∣Ffk(θ()∣∣ < σ and so θ( ∈ Zδ or θο ∈ D. Finally we obtain
PV (θ0 ∈ U and limF∕(θ()=在)=PV (θ( ∈ U ∩ Zδ or θ( ∈ U \ D)
≤Pν(θ0 ∈ U∩Zδ)+Pν(θ0 ∈ U\D)
≤ PV (θ0 ∈ Zδ ) + PV (θ0 ∈ Bσ0 )
< e/2 + e/2 = e
as required. We plot iterates for a single run of each algorithm in Figure 3 with α = Y = 0.01.	□
19
Published as a conference paper at ICLR 2021
F	Proof of Theorem 3
Theorem 3. There is a weakly-coercive, nondegenerate, analytic two-player zero-sum game N
whose only critical point is a strict maximum. Algorithms in A almost surely have bounded non-
convergent iterates in N for α, γ sufficiently small.
Proof. Consider the analytic zero-sum game N given by
L1 = xy - x2/2 + y2/2 + x4/4 - y4/4 = -L2
with simultaneous gradient
ξ = y - x +x3
-x - y + y3
and Hessian
H = -1-+13x2 -1+13y2 .
We show that the only solution to ξ = 0 is the origin. First we can assume x, y ≥ 0 since any
other solution can be obtained by a quadrant variable change (f). Now assume for contradiction that
y 6= 0, then
ξ2 = 0 = -x - y + y3 ≤ -y +y3 = y(y2 - 1)
implies y ≥ 1 and hence
ξ1 = 0 = y - x + x3 ≥ 1 - x + x3 = (x + 1)(x - 1)2 + x2 > 0
which is a contradiction. It follows that y = 0 and hence ξ2 = 0 = x as required. Now the origin
has invertible, negative-definite Hessian
H(0) = (-1	_11) Y 0
so the unique critical point is a strict maximum. The game is nondegenerate since the only critical
point has invertible Hessian. The game is weakly-coercive since L1(x, y) → ∞ for any fixed y by
domination of the x4 term; similarly for L2 (x, y) by domination of the y4 term.
Bounded iterates: strategy. We begin by showing that all algorithms have bounded iterates in N
for α, γ sufficiently small. For each algorithm F, our strategy is to show that there exists r > 0 such
that for any s > 0 we have kF (θ)k < kθk for all r < kθk < s and α, γ sufficiently small. This will
be enough to prove bounded iteration upon bounded initialisation. Denote by Br the ball of radius
r centered at the origin.
GD. We have
θTξ = x(y - x + x3) + y(-x - y + y3)
= x4 - x2 + y4 - y2
=(x2-1)2+(y2-1)2+x2+y2-2>1
for all kθk2 = x2 + y2 > 3. For any s > 0 we obtain
kF(θ)k2=kθ-αξk2=kθk2-2αθTξ+α2kξk2<kθk-α2-αkξk2 < kθk2
for all √3 < ∣∣θk < s and α sufficiently small, namely 0 < α < 2/ supθ∈Bs kξk2.
EG. For any s > 0 and √4 < ∣∣θ∣ < s We have
∣θ - αξ(θ)∣2 >4-2αθTξ >3
for α < 1/ supθ∈Bs 2θT ξ. Now using θTξ > 1 for all ∣θ∣2 > 3by the argument for GD above,
∣F (θ)∣2 = ∣θ∣2 - 2αθT ξ(θ - αξ(θ)) + α2 ∣ξ(θ - αξ(θ))∣2
= ∣θ∣2 - 2α(θ - αξ(θ))T ξ(θ - αξ(θ)) + O(α2)
<∣θ∣2-α(2-O(α))<∣θ∣2
for α sufficiently small.
20
Published as a conference paper at ICLR 2021
AGD. For any s > 0, notice by continuity of ξ that there exists δ > 0 such that
θT(ξ1,ξ2(θ1- αξ1,θ2)) >θTξ - 1/2
for all α < δ and θ ∈ Bs, since Bs is bounded and θ1 - αξ1 → θ1 as α → 0. It follows that
kF (θ)k2 = kθk2 - 2αθT (ξ1, ξ2(θ1 - αξ1, θ2)) + O(α2)
<	kθk2 - 2α(θT ξ - 1/2) + O(α2)
<	kθk2 - 2α(1 - 1/2) + O(α2)
<	kθk2-α(1-O(α))<kθk2
for all √3 < ∣∣θk < s and α < δ sufficiently small.
OMD. For any s > 0, notice by continuity of ξ that there exists δ > 0 such that
θT (ξ(θ) - ξ((id - αξ)-1(θ)) < 1/2
for all α < δ and θ ∈ Bs, since Bs is bounded and (id - αξ)-1(θ) → θ as α → 0. It follows that
∣F (θ)∣2 = ∣θ∣2 - 2αθT ξ - 2αθT (ξ(θ) - ξ((id - αξ)-1(θ)) + O(α2)
< ∣θ∣2-2α+α+O(α2)
=∣θ∣2-α(1-O(α))<∣θ∣2
for all √3 < ∣∣θ∣ < s and α < δ sufficiently small.
CO, CGD, LA, LOLA, SOS. Writing ν for γ if F = FCO and ν for α otherwise, for each
algorithm we have
F(θ) = θ - αξ + ανK
for some continuous function K : Rd → R. For instance, K = -HTξ for CO (see Appendix A).
We obtain
∣F (θ)∣2 = ∣θ - αξ+ ανK∣2
= ∣θ∣2 - 2αθTξ + 2ανθTK - 2α2νξTK + α2 ∣ξ∣2 + α2ν2 ∣K∣
= ∣θ∣2 -α 2θTξ - 2νθTK + 2ανξTK -α∣ξ∣2 -αν2 ∣K∣ .
Notice that every term in the brackets contains an α or ν except for the first. We have already shown
that θTξ > 1 for all ∣θ∣2 > 3 for GD above, hence for any s > 0 we have
∣F (θ)∣2 < ∣θ∣2 - α 2 - 2ν sup θTK + 2αν inf ξTK - α sup ∣ξ∣2 - α sup ν2 ∣K∣
θ∈Bs	θ∈Bs	θ∈Bs	θ∈Bs
=∣θ∣2-α(2-O(α,ν))<∣θ∣2
for all √3 < ∣∣θ∣2 < s and α, ν sufficiently small.
SGA. The situation differs from the above since parameter λ follows an alignment criterion,
namely λ = sign hξ, HTξihATξ, HTξi , which cannot be made small. First note that
θT GSGA = θtξ+ λθT (ATξ) = x4 +y4 - x2 - y2 + λ(x2 +y2 +x3y - xy3) .
Ifλ= -1,
θT GSGA = x4 + y4 - 2x2 - 2y2 - x3y + xy3
and splitting x4 + y4 in two yields
44
x-+y- - 2x2- 2y2 = 1 [(x2 - y2)2 + (x2 + y2)(x2 + y2 - 8)] > 1
for ∣θ∣2 = x2 +y2 > 9, while
44
x4ɪ -χ3y + xy3 = 1 [(-X2 + Xy + y2)2 + χ2y2] > 0
21
Published as a conference paper at ICLR 2021
for kθk > 0. Summing the two yields θT GSGA > 1 for kθk2 > 9 and λ = -1. If λ = 1,
θT GSGA	=	x4	+y4 +x3y	-	xy3
=	x4	+ y4 -	2x2	-	2y2 +	x3y	-	xy3	+ 2(x2	+ y2)
≥	x4	+ y4 -	2x2	-	2y2 +	x3y	-	xy3	> 1
for kθk2 > 9 by swapping	x	and y in the λ	=	-1 case above.	We conclude θT GSGA > 1 for
kθk2 > 9 regardless of λ. For any s > 0 we obtain
kF(θ)k2= kθk2 - 2αθT GSGA + α2 kGSGAk2 < kθk2-α2-αkGSGAk2 < kθk2
for all 3 < kθk < s and α < 2/ supθ∈Bs GSGA.
Bounded iterates: conclusion. Now assume as usual that θ0 is initalised in any bounded region
U. For each algorithm we have found r such that for any s > 0 we have kF (θ)k < kθk for all
r < kθk < s and α, γ sufficiently small. Now pick r0 ≥ r such that U ⊂ Br0. Define the bounded
region
V = {θ - tG(θ) | t ∈ [0, 1], θ ∈ Br0}.
and pick s ≥ r0 such that V ⊂ Bs . By the above we have kF (θ)k < kθk for all r < kθk < s
and α, γ sufficiently small. In particular, fix any α, γ < 1 satisfying this condition. We claim that
F(θ) ∈ Bs for all θ ∈ Bs. Indeed, either θ ∈ Br implies F(θ) = θ - αG(θ) ∈ V ⊂ Bs or θ ∈/ Br
implies kF (θ)k < kθk < s and so F (θ) ∈ Bs. We conclude that θ0 ∈ U ⊂ Bs implies bounded
iterates θk = F k(θ) ∈ Bs for all k.
Non-convergence: strategy. We show that all methods in A have the origin as unique fixed points
for α, γ sufficiently small. Fixed points of each gradient-based method are given by G = 0, where
G is given in Appendix A, and We moreover show that the Jacobian VG at the origin is negative-
definite. Non-convergence will follow from this for α sufficiently small.
GD. Fixed points of simultaneous GD correspond by definition to critical points:
The Jacobian of G at 0 is
GGD = ξ = 0 ^⇒ θ = 0 .
vξ = H =(-1 -11)Y 0.
AGD. We have
GAGD = 0 O {ξ1(=10- αξ1,θ2)=0	O {ξ1 =0
^⇒ ξ = 0 ^⇒ θ = 0 .
Now
ξ2(x - αξ1(x, y), y) = -(x - α(y - x + x3)) - y + y3
= x(-1 - α) + y(-1 + α) + αx + y
so the Jacobian at the origin is
with symmetric part
JAGD =	-1--1 α
-1+α
SAGD =(二/2
-a/2 ʌ
-1+α
which has negative trace for all α < 2 and positive determinant
-α2∕2 — α + 1 = -(α + 1)2/2 + 3/2 > -9/8 + 3/2 > 0
for all α < 1/2, which together imply negative eigenvalues and hence SAGD Y 0. Recall that a
matrix is negative-definite iff its symmetric part is, hence JAGD Y 0 for all α < 1/2.
22
Published as a conference paper at ICLR 2021
EG. We have
GEG = ξ ◦ (id — αξ) = 0 ^⇒ id — αξ = 0 ^⇒
(x - α(y - x + x3) = 0
y — α(-x — y + y3) =0.
We have shown that any bounded initialisation results in bounded iterates for EG for α sufficiently
small. Let U be this bounded region and assume for contradiction that id — αξ = 0 with x, y 6= 0
(noting that x = 0 implies y = 0 by the first equation and vice-versa). We can assume x, y > 0
since any other solution can be obtained by a quadrant change of variable ⑴. We first prove that
x, y < 1 for 0 < α < 1/ supθ∈U {y — x + x3}. Indeed we have
0 = ξ1 > x — α sup > x — 1
θ∈U
hence x < 1. A similar derivation holds for y, hence 0 < x, y < 1. But now x ≥ y implies
0 = ξ1 ≥ x — α(y — y + x3) = x(1 — αx2) ≥ x(1 — α) > 0
for α < 1 while x < y implies
0 = ξ ≥ y — α(-x — X + y3) = y(1 — αy2) ≥ y(1 — α) > 0
and the contradiction is complete, hence θ = 0 is the only fixed point of EG. Now
JEG = H(I — αH) =	——1
1	1 + α	—a	—1	1 + 2α∖
—1 a 1 + a) = [-1 — 2a	—1 )
with SEG = —I Y 0, hence JEG Y 0 for all a.
OMD. By Daskalakis & Panageas (2018, Remark 1.5), fixed points of OMD must satisfy ξ = 0
by viewing OMD as mapping pairs (θk, θk-1) to pairs (θk+1, θk), hence θ = 0. Now
JOMD = 2H — H(I — aH)-1 = 2 ——1
1 A_______1	(-1 — 2a	1 A
—1)	1 + 2a + 2a2	—1	—1 — 2a)
Now notice that
1√2+⅛ ≤ 1
and so
SOMD
_Q _|_	1+2α
2 + 1+2α+2α2	0	0
0	-2 +	+2α 2 Y 0
1+2α+2α2
for all a.
CO. We have
GCO=(I + YHT)ξ = 0 ^⇒ ξ = 0 ^⇒ θ = 0
for all γ since the matrix
(I + YH T )=(1 — Y 1—γγ)
is always invertible with determinant (1 — γ )2 + γ 2 > 0. Now
JCO=(I+YHT)H=1—YY	1——YY——11	—11	=	—1—+12Y	—1+12YY0
for all Y < 1/2.
23
Published as a conference paper at ICLR 2021
SGA. We have
GSGA = (I + λAT)ξ = 0 ^⇒ ξ = 0 ^⇒ θ = 0
since antisymmetric A with eigenvalues ia, a ∈ R implies that I + λAτ is always invertible with
eigenvalues 1 + iλa = 0. Now recall that λ is given by
λ = sign (hξ, HTξihAT, HTξ>) = sign (ξTHtξ ∙ ξτAHTξ).
We have
HT =(-1 + 3x2 -1 +BQY 0
and
AHT =(1-13χ2 -T) A 0
for all ∣∣θ∣∣ sufficiently small, hence ξτHtξ ≤ 0 and ξτAHtξ ≥ 0 and thus
λ = sign (hξ, Hτξ><Aτ, Hτξi) = sign (ξτHtξ ∙ ξτAHtξ) ≤ 0
around the origin. Now
JSGA = (I +λAT)H =QT) (-1 -1) = (-1-λ -1+λλ)γ 0
for all λ < 1, which holds in particular for λ ≤ 0.
CGD. Note that
Ho=C 0)=A
is antisymmetric, hence I + aHo is always invertible as for SGA and
GCGD = (I + aHo)-1ξ = 0 ^⇒ ξ = 0 ^⇒ θ = 0 .
Now
JCGD = (I + αHo)TH =T⅛ (α -10)(-1 -1)= i⅛ (-1- α -1+a) Y 0
for all α < 1.
LA. As above,
GLA = (I — αHo)ξ = 0 ^⇒ ξ = 0 ^⇒ θ = 0
since (I 一 αHo) is always invertible. Now
jla=(I-0Ho)H=(I-°A)H=(-1+α -11++α) Y0
for all α < 1.
LOLA. Notice that
diag (HT VL)=diag((0 -ŋ (： + X + j
3
-y + X - X
3
-x - y + y3
-x — y + y3λ _ q ξ
-y + x - x3 j o
and so
GLOLA = (I - αHo)ξ - α diag (HTVL) = (I - 20Ho)ξ ^⇒ ξ = 0 ^⇒ θ = 0
as for LA. Similarly, substituting 20 for α in the derivation for LA yields
Jlola = (I - 20Ho)H Y 0
for all 0 < 1/2.
24
Published as a conference paper at ICLR 2021
SOS. As for LOLA we have
GSOS = (I — αHo)ξ — Padiag (HTVL)= (I — α(1 + p)H0)ξ ^⇒ ξ = 0 ^⇒ θ = 0
for any α, p. Now p(θ) = 0 for fixed points G by Letcher et al. (2019b, Lemma D.7), hence
JSOS = Jla=(-1 + α —1二)Y 0
for all α < 1.
Non-convergence: conclusion. We conclude that all algorithms in A have the origin as unique
fixed points, with negative-definite Jacobian, for α, γ sufficiently small. If a method converges, it
must therefore converge to the origin. We show that this occurs with zero probability. One may
invoke the Stable Manifold Theorem from dynamical systems, but there is a more direct proof.
Take any algorithm F in A and let U be the initialisation region. We prove that the stable set
Z = {θ0 ∈ U | limFk(θ0) = 0}
k
has Lebesgue measure zero for α sufficiently small. First assume for contradiction that θk → 0 with
θk 6= 0 for all k. Then
G(θk) = G(0) + VG(0)θk + O(kθkk2) = VG(θG)(θk) + O(kθkk2)
since G(0) = 0, and we obtain
kθk+1 k = kθk — αG(θk)k
= kθk k — 2αθkTG(θk) + α2 kG(θk)k
≥ kθkk2 — 2αθkT VG(0)θk + O(kθkk3) > kθkk2
for all k sufficiently large, since VG(0) Y 0. This is a contradiction to θk → 0, so θk → 0 implies
θk = 0 for some k and so, writing FU : U → Rd for the restriction of F to U,
Z⊂∪k∞=0FU-k({0}).
We claim that FU is a C1 local diffeomorphism, and a diffeomorphism onto its image. Now GU is
C1 with bounded domain, hence L-Lipschitz for some finite L. By Lemma 0, the eigenvalues ofVG
in U satisfy ∣λ∣ ≤ ∣∣VG∣∣ ≤ L, hence VFU = I 一 OVGU has eigenvalues 1 一 αλ ≥ 1 一 a∣λ∣ ≥
1 — αL > 0. It follows that VFU is invertible everywhere, so FU is a local diffeomorphism by
the Inverse Function Theorem (Spivak, 1971, Th. 2.11). To prove that FU : U → F(U) is a
diffeomorphism, it is sufficient to show injectivity of FU. Assume for contradiction that FU (θ) =
FU (θ0 ) with θ 6= θ0 . Then by definition,
θ 一 θ0 = a(Gu(θ0) 一 GU(θ))
and so
kθ 一 θ0k = a IlGU(θ0) — GU(θ)k ≤ aL ∣∣θ — θ0∣∣ < ∣∣θ — θ0∣∣ ,
a contradiction. We conclude that FU is a diffeomorphism onto its image with continuously differ-
entiable inverse FU-1, hence FU-1 is locally Lipschitz and preserves measure zero sets. It follows by
induction that μ(F-k ({0})) = 0 for all k, and so
μ(z) ≤ μ (∪k=oF-k({0})) = 0
since countable unions of measure zero sets have zero measure. Since θ0 follows a continuous
distribution ν, we conclude
Pν limFk(θ0) =0 =0
as required. Since all algorithms were also shown to produce bounded iterates, they almost surely
have bounded non-convergent iterates for a, γ sufficiently small. The proof is complete; iterates are
plotted for a single run of each algorithm in Figure 3 with α = Y = 0.01.	□
25
Published as a conference paper at ICLR 2021
SOS
Figure 3: Algorithms in A fail to converge in N with α = γ = 0.01. Single run with standard
normal initialisation, 3000 iterations.
G Proof of Corollary 1
Corollary 1. There are no measures of progress for reasonable algorithms which produce bounded
iterates in M or N .
Proof. Assume for contradiction that a measure of progress M exists for some reasonable algorithm
F and consider the iterates θk produced in the game M orN. We prove that the set of accumulation
points of θk is a subset of critical points, following Lange (2013, Prop. 12.4.2). Consider any
accumulation point θ = limm→∞ θkm. The sequence M(θk) is monotonically decreasing and
bounded below, hence convergent. In particular,
limmM(F(θkm))=limmM(θkm+1)=limmM(θkm).
By continuity of M and F , we obtain
M (F W)) = M (lim F(θkm)) = lim M (F (θkm ))= lim M (θkm )= M ⑻
and hence F(θ) = θ. Since F isreasonable, θ must be a critical point. Now thegnly critical point of
M or N is the strict maximum8= 0,so any accumulation point of θk must be θ. The sequence θk is
assumed to be bounded, so it must have at least one accumulation point by Bolzano-Weierstrass. A
sequence with exactly one accumulation point is convergent, hence θk → θ. This is in contradiction
with the algorithm being reasonable.	□
26