Published as a conference paper at ICLR 2021
Coping with label shift via distributionally
ROBUST OPTIMISATION
Jingzhao Zhang
Massachusetts Institute of Technology
jzhzhang@mit.edu
Aditya Krishna Menon & Andreas Veit & Srinadh Bhojanapalli & Sanjiv Kumar
Google Research
{adityakmenon, aveit, bsrinadh, sanjivk}@mit.edu
Suvrit Sra
Massachusetts Institute of Technology
suvrit@mit.edu
Ab stract
The label shift problem refers to the supervised learning setting where the train and
test label distributions do not match. Existing work addressing label shift usually
assumes access to an unlabelled test sample. This sample may be used to estimate
the test label distribution, and to then train a suitably re-weighted classifier. While
approaches using this idea have proven effective, their scope is limited as it is not
always feasible to access the target domain; further, they require repeated retraining
if the model is to be deployed in multiple test environments. Can one instead learn
a single classifier that is robust to arbitrary label shifts from a broad family? In this
paper, we answer this question by proposing a model that minimises an objective
based on distributionally robust optimisation (DRO). We then design and analyse a
gradient descent-proximal mirror ascent algorithm tailored for large-scale problems
to optimise the proposed objective. Finally, through experiments on CIFAR-100
and ImageNet, we show that our technique can significantly improve performance
over a number of baselines in settings where label shift is present.
1	Introduction
Classical supervised learning involves learning a model from a training distribution that generalises
well on test samples drawn from the same distribution. While the assumption of identical train and test
distributions has given rise to useful methods, it is often violated in many practical settings (Kouw &
Loog, 2018). The label shift problem is one such important setting, wherein the training distribution
over the labels does not reflect what is observed during testing (Saerens et al., 2002). For example,
consider the problem of object detection in self-driving cars: a model trained in one city may see a
vastly different distribution of pedestrians and cars when deployed in a different city. Such shifts in
label distribution can significantly degrade model performance. As a concrete example, consider the
performance of a ResNet-50 model on ImageNet While the overall error rate is 〜24%, Figure 1
reveals that certain classes suffer an error as high as 〜80%. Consequently, a label shift that increases
the prevalence of the more erroneous classes in the test set can significantly degrade performance.
Most existing work on label shift operates in the setting where one has an unlabelled test sample
that can be used to estimate the shifted label probabilities (du Plessis & Sugiyama, 2014; Lipton
et al., 2018; Azizzadenesheli et al., 2019). Subsequently, one can retrain a classifier using these
probabilities in place of the training label probabilities. While such techniques have proven effective,
it is not always feasible to access an unlabelled set. Further, one may wish to deploy a learned model
in multiple test environments, each one of which has its own label distribution. For example, the
label distribution for a vehicle detection camera may change continuously while driving across the
city. Instead of simply deploying a separate model for each scenario, deploying a single model that is
1
Published as a conference paper at ICLR 2021
Figure 1: Distribution of per-class test errors of a ResNet-50 on ImageNet (left). While the average error rate
is 〜24%, some classes achieve an error as high as 〜80%. An adversary can thus significantly degrade test
performance (right) by choosing pte (y) with more weight on these classes.
robust to shifts may be more efficient and practical. Hence, we address the following question in this
work: can we learn a single classifier that is robust to a family of arbitrary shifts?
We answer the above question by modeling label shift via distributionally robust optimisation
(DRO) (Shapiro et al., 2014; Rahimian & Mehrotra, 2019). DRO offers a convenient way of coping
with distribution shift, and have lead to successful applications (e.g. Faury et al. (2020)). Intuitively,
by seeking a model that performs well on all label distributions that are “close” to the training data
label distribution, this task can be cast as a game between the learner and an adversary, with the latter
allowed to pick label distributions that maximise the learner’s loss. We remark that while adversarial
perspectives have informed popular paradigms such as GANs, these pursue fundamentally different
objectives from DRO (see Appendix A for details).
Although several previous works have explored DRO for tackling the problem of example shift (e.g.,
adversarial examples) (Namkoong & Duchi, 2016; 2017; Duchi & Namkoong, 2018), an application
of DRO to the label shift setting poses several challenges: (a) updating the adversary’s distribution
naively requires solving a nontrivial convex optimisation subproblem with limited tractability, and also
needs careful parameter tuning; and (b) naively estimating gradients under the adversarial distribution
on a randomly sampled minibatch can lead to unstable behaviour (see §3.1). We overcome these
challenges by proposing the first algorithm that successfully optimises a DRO objective for label
shift on a large scale dataset (i.e., ImageNet). Our objective encourages robustness to arbitrary label
distribution shifts within a KL-divergence ball of the empirical label distribution. Importantly, we
show that this choice of robustness set admits an efficient and stable update step.
Summary of contributions
(1)	We design a gradient descent-proximal mirror ascent algorithm tailored for optimising large-scale
problems with minimal computational overhead, and prove its theoretical convergence.
(2)	With the proposed algorithm, we implement a practical procedure to successfully optimise the
robust objective on ImageNet scale for the label shift application.
(3)	We show through experiments on ImageNet and CIFAR-100 that our technique significantly
improves over baselines when the label distribution is adversarially varied.
2 Background and problem formulation
In this section we formalise the label shift problem and motivate its formulation as an adversarial
optimisation problem. Consider a multiclass classification problem with distribution ptr over instances
X and labels Y = [L]. The goal is to learn a classifier hθ : X → Y parameterised by θ ∈ Θ, with
the aim of ensuring good predictive performance on future samples drawn from ptr . More formally,
the goal is to minimise the objective minθ E(>,y)〜?吐['(x, y, θ)], where ': X X Y X Θ → R+ is a
loss function. In practice, We only have access to a finite sample S = {(xi, yi)}n=ι 〜pt, which
motivates us to use the empirical distribution PemP(x, y) = ɪ P3 1(x = Xi,y = y%) in place of
ptr. Doing so, we arrive at the objective of minimising the empirical risk:
min EPemp ['(x,y,θ)] := n χn=1 '(xi,yi,θ).	(1)
The assumption underlying the above formulation is that test samples are drawn from the same
distribution ptr that is used during training. However, this assumption is violated in many practical
settings. The problem of learning from a training distribution ptr, while attempting to perform well
on a test distribution pte 6= ptr is referred to as domain adaptation (Ben-David et al., 2007). In
2
Published as a conference paper at ICLR 2021
Label distribution	Reference
Train distribution Specified a-priori (e.g., balanced) Estimated test distribution	Standard ERM (Elkan, 2001; Xie & Manski, 1989; Cao et al., 2019) (du Plessis & Sugiyama, 2014; Lipton et al., 2018; Azizzadenesheli et al., 2019; Garg et al., 2020; Combes et al., 2020)
Worst-performing class Worst k-performing classes	(Hashimoto et al., 2018; Mohri et al., 2019; Sagawa et al., 2020) (Fan et al., 2017; Williamson & Menon, 2019; Curi et al., 2019; Duchi et al., 2020)
Adversarial shifts within KL-divergence	This paper
Table 1: Summary of approaches to learning with a modified label distribution.
the special case of label shift, one posits that pte(x | y) = ptr(x | y), but the label distribution
pte(y) 6= ptr (y) (Saerens et al., 2002); i.e., the test distribution satisfies pte(x, y) = pte(y)ptr(x | y).
The label shift problem admits the following three distinct settings (see Table 1 for a summary):
(1)	Fixed label shift. Here, one assumes a-priori knowledge of pte (y). One may then adjust the
outputs of a probabilistic classifier post-hoc to improve test performance (Elkan, 2001). Even when
the precise distribution is unknown, it is common to posit a uniform pte (y). Minimising the resulting
balanced error has been the subject of a large body of work (He & Garcia, 2009), with recent
developments including Cui et al. (2019); Cao et al. (2019); Kang et al. (2020); Guo et al. (2020).
(2)	Estimated label shift. Here, we assume that pte (y) is unknown, but that we have access to
an unlabelled test sample. This sample may be used to estimate pte (y), e.g., via kernel mean-
matching (Zhang et al., 2013), minimisation of a suitable KL divergence (du Plessis & Sugiyama,
2014), or using black-box classifier outputs (Lipton et al., 2018; Azizzadenesheli et al., 2019; Garg
et al., 2020). One may then use these estimates to minimise a suitably re-weighted empirical risk.
(3)	Adversarial label shift. Here, we assume that pte (y) is unknown, and guard against a suitably
defined worst-case choice. Observe that an extreme case of label shift involves placing all probability
mass on a single y* ∈ Y. This choice can be problematic, as (1) may be rewritten as
min X PemP(y) ∙ {n1 X '(xi,yi,θ) k
y∈[L]	y i: yi=y
where ny is the number of training samples with label y. The empirical risk is thus a weighted average
of the per-class losses. Observe that if some y* ∈ Y has a large per-class loss, then an adversary could
degrade performance by choosing a Pte with pte(y*) being large. One means of guarding against
such adversarial label shifts is to minimise the minimax risk (Alaiz-Rodrlguez et al., 2007; Davenport
et al., 2010; Hashimoto et al., 2018; Mohri et al., 2019; Sagawa et al., 2020)
min maχ X π(y) ∙ ∖ — X '(xi,yi,θ) ∖,	(2)
θ π∈∆L	ny
y∈[L]	i: yi=y
where ∆L denotes the simplex. In (2), we combine the per-label risks according to the worst-case
label distribution. In practice, focusing on the worst-case label distribution may be overly pessimistic.
One may temper this by instead constraining the label distribution. A popular choice is to enforce
that ∣∣∏k∞ ≤ 1 for suitable k, which corresponds to minimising the average of the top-k largest
per-class losses for integer k (Williamson & Menon, 2019; Curi et al., 2019; Duchi et al., 2020).
We focus on the adversarial label shift setting, as it meets the desiderata of training a single model
that is robust to multiple label distributions, and not requiring access to test samples. Adversarial
robustness has been widely studied (see Appendix A for more related work), but its application to
label shift is much less explored. Amongst techniques in this area, Mohri et al. (2019); Sagawa et al.
(2020) are most closely related to our work. These works optimise the worst-case loss over subgroups
induced by the labels. However, both works consider settings with a relatively small (≤ 10) number
of subgroups; the resultant algorithms face many challenges when trained with many labels (see
Section 4). We now detail how a suitably constrained DRO formulation, coupled with optimisation
choices, can overcome this limitation.
3
Published as a conference paper at ICLR 2021
Algorithm 1 ADVSHIFT(θ0, γc, λ, NNOpt,pemp, η∏)
1:	Initialise adversary distribution as ∏ι = (L ,…，L).
2:	for t = 1, . . . , T do
3:	Sample mini-batch of b examples {(xi, yi)}ib=1.
4:	Evaluate stochastic gradient gθ = b P=I Pempyy)) ∙ ▽"'(xi,y ,θt)
5:	Update neural network parameters θt+1 = NNOpt(gθ)
6:	Update Lagrangian variable α = 0 if r > KL(πt,pemp), α = 2γcλ if r < KL(πt,pemp).
7:	Evaluate adversarial gradient g∏(i) = 1 Pj=I ；：？/；} ∙ Vπ'(xj, yj,θt+ι).
8:	Update adversarial distribution ∏t+ι = (∏t ∙ PomP) * 1 241+α) ∙ exp (η∏g∏ )/C
3	Algorithm: distributionally robust KL-divergence minimisation
To address the adversarial label shift problem, we propose to replace the empirical risk (1) with
min max En['(x,y,θ)],	P := {∏ ∈ ∆L | d(π,Pemp) ≤ r},	(3)
θ	π∈P
where P is an uncertainty set containing perturbations of the empirical distribution pemp . This is an
instance of distributionally robust optimisation (DRO) (Shapiro et al., 2014), a framework where one
minimises the worst-case expected loss over a family of distributions. In this work, we instantiate
DRO with P being a parameterised family of distributions with varying marginal label distributions
in KL-divergence, i.e., d(p, q) = Ey~q [- logp(y)∕q(y)]. (We use this divergence, as opposed to a
generic f -divergence, as it affords closed-form updates; see §3.3.) Solving (3) thus directly addresses
adversarial label shift, as it ensures our model performs well on arbitrary label distributions from P.
Observe further that the existing minimax risk (2) is a special case of (3) with r = +∞.
Having stated our learning objective, we now turn to the issue of how to optimise it. One natural
thought is to leverage strategies pursued in the literature on example-level DRO using f -divergences.
For example, Namkoong & Duchi (2016) propose an algorithm that alternately performs iterative
gradient updates for model parameters θ and adversarial distribution π, assuming access to projection
oracles, and the ability to sample from the adversarial distribution. However, there are challenges in
applying such techniques on large-scale problems (e.g., ImageNet):
(1) directly sampling from π is challenging in most data loading pipelines for ImageNet.
(2) projecting π onto the feasible set P requires solving a constrained convex optimization problem
at every iteration, which can incur non-trivial overhead (see Appendix E).
We now describe AdvS hift (Algorithm 1), our approach to solve these problems. In a nutshell,
we iteratively update model parameters θ and adversarial distributions π. In the former, we update
exactly as per ERM optimization (e.g., ADAM, SGD), which we denote as NNOpt (neural network
optimiser); in the latter, we introduce a Lagrange multiplier to avoid projection. Extra care is needed
to obtain unbiased gradients and speed up adversarial convergence, as we now detail.
3.1 Estimating the adversarial minibatch gradient
For a fixed ∏ ∈ ∆L, to estimate the parameter gradient En [Vθ'(χ, y, θ)] on a training sample
S = {(xi, yi)}in=1, we employ the importance weighting identity and write
En[vθ'(X,y,θ)]= EpemP pm1(y) ∙ vθ'(X,y,θ) = n X peπ⅛⅛ ∙ vθ'(Xi,yi,θ)∙
We may thus draw a minibatch as usual from S, and apply suitable weighting to obtain unbiased gradi-
ent estimates. A similar reweighting is necessary to compute the adversary gradients En [V∏'(χ, y, θ)],
Making the adversarial update efficient requires further effort, as we now discuss.
3.2 Removing constraints by Lagrangian duality
To efficiently update the adversary distribution π in (3), we would like to avoid the cost of projecting
onto P. To bypass this difficulty, we make the following observation based on Lagrangian duality.
4
Published as a conference paper at ICLR 2021
Proposition 1. Suppose ` is bounded, and pemp is not on the boundary of the simplex. Then, ∀r > 0,
∃γ* > 0 such thatfor every Yc ≥ γ* ,the Constrained objective is solvable in Unconstrainedform:
argmax	En ['(x,y,θ)] = argmax En ['(x,y,θ)] + min{0,γc(r — KL(π,Pemp))}.
π∈∆L, KL(π,pemp)≤r	π∈∆L
Motivated by this, we may thus transform the objective (3) into:
min max En['(x,y,θ)] +min{0,γc(r — KL(∏,Pemp))},	(4)
θ n∈∆L
where γc > 0 is a sufficiently large constant; in practice, this may be chosen by a bisection search.
The advantage of this formulation is that it admits an efficient update for π, as we now discuss.
3.3	Adversarial distribution updates
We now detail how we can employ proximal mirror descent to efficiently update π . Observe that
We may decompose the adversary's (negated) objective into two terms: f (θ, ∏) := -En ['(x, y, θ)]
and h(π) := max{0, γc(KL(π,pemp) — r)}, where h(π) is independent of the samples. Such
decomposable objectives suggest using proximal updates (Combettes & Pesquet, 2011):
∏t+ι = proXλh(∏t — λVnf (θt,∏t)) := argminh(π) + ；1(IInt- ∏k2 + 2入Nnf (θt,∏t),π>),	(5)
n∈∆L	2λ
where λ serves as the learning rate. The value of proximal descent relies on the ability to efficiently
solve the minimisation problem in (5). Unfortunately, this does not hold as-is for our choice of
h(π), essentially due to a mismatch between the use of KL-divergence in h, and Euclidean distance
Iπt — πI2 in (5). Motivated by the advantages of mirror descent over gradient descent on the
simplex (Bubeck, 2014), we propose to replace the Euclidean distance with KL-divergence:
∏t+ι = argmin h(π) + ((KL(∏,∏t ) + 2λ(gt,∏i),	(6)
where gt is an unbiased estimator of Vnf(θt, πt). We have the following closed-form update.
Lemma 2. Assume the optimal solution πt+1 to (6) satisfies KL(πt+1,pemp) 6= r, and that all the
classes appeared at least once in the empirical distribution, i.e. ∀i, piemp > 0. Let γ = γc if r <
KL(πt+1, pemp), and γ = 0 if r > KL(πt+1,pemp), then πt+1 permits a closed form solution
∏t+ι = (∏t ΘPamP)1/(1+a) exp(ηngt)∕C,	⑺
where η = (Y+1/2；)(1+a)，α = 2γλ, C = ∣∣(πt ΘPamP)141+a) exp (ηgt)kι projects ∏t+ι onto
the simplex, and a b is the element-wise product between two vectors a, b.
In Algorithm 1, we set γ = γc if r < KL(πt, PemP) and 0 otherwise to appoximate the true γ. Such
approximation works well when r — KL(πt,PemP) does not change sign frequently.
3.4	Convergence Analysis
We provide below a convergence analysis of our gradient descent-proximal mirror ascent
method for nonconvex-concave stochastic saddle point problems. For the composite objective
minθ maxn∈∆L f(θ, π) + h(π), and fixed learning rate ηθ, we abstract the Algorithm 1 update as:
θt+ι =	θt	—	ηθg(θt),	∏t+ι = argmax	h(π)	—	71(KL(π,	∏t)	+ 2λ(g(∏t), π>),	(8)
n	2λ
where g(π), g(θ) are stochastic gradients assumed to satisfy the following.
Assumption 1. The stochastic gradient g(θ) with respect to θ satisfies that for some σ > 0,
E[g(θ)] = Vθf(θ,π), and E[∣g(θ) — E[g(θ)]∣2] ≤ σ2.
Assumption 2. The stochastic gradient g(π) with respect to π satisfies that for some G > 0,
E[g(π)] = Vnf(θ, π), and E[∣g(π)∣2∞] ≤ G2 .
5
Published as a conference paper at ICLR 2021
We make the following assumptions about the objective, similar to Lin et al. (2019; 2020):
Assumption 3. f(θ, π) + h(π) is L-smooth and l-Lipschitz; f(θ, π) and h(π) are concave in π.
Assumption 4. Every adversarial distribution iterate πt satisfies KL(πt , pemp) ≤ R for some R > 0.
Assumption 3 and 4 may be enforced by adding a constant to the adversarial updates, which prevents
πt from approaching the boundary of the simplex. Assumption 2 in the label shift setting implies
that the loss is upper and lower bounded. Such an assumption may be enforced by clipping the loss
for computing the adversarial gradient, which can significantly speed up training (see Appendix ??).
Furthermore, this is a standard assumption for analyzing nonconvex-concave problems (Lin et al.,
2019). The assumption that the square L∞ norm is bounded is weaker than L2 norm being bounded;
such a relaxation results from using mirror rather than Euclidean update.
Given that the function F(θ) := maxπ∈∆ f(θ, π) + h(π) is nonconvex, our goal is to find a stationary
point instead of approximating global optimum. Yet, due to the minimax formulation, the function
F(θ) may not necessarily be differentiable. Hence, we define convergence following some recent
works (Davis & Drusvyatskiy, 2019; Lin et al., 2019; Thekumparampil et al., 2019) on nonconvex-
concave optimisation. First, Assumption 3 implies F (θ) is L-weakly convex and l-Lipschitz (Lin
et al., 2019, Lemma 4.7). Hence, we define stationarity in the language of weakly convex functions.
Definition 1. A point θ is an €-stationary point of a weakly convex function F if ∣∣VFi∕2l(Θ)∣∣ ≤ e,
where Fi∕2l(Θ) denotes the Moreau envelope Fi∕2l(Θ) = minw F(W) + Lkw - θ∣∣2.
With the above definition, we can establish convergence of the following update:
Theorem 3 (informal). Under Assumptions 1-4, the update in (8) finds a point θ with
E[kVF (θ)k] ≤ e in O(e-8) iterations.
For a precise description of the theorem, please see Appendix H. The above result matches the
best known rate in Lin et al. (2019) for optimising nonconvex-concave problem with stochastic
gradients. To our knowledge, this is the first result that studies convergence of composite objectives
with proximal methods under nonconvex-concave settings. By utilizing the proximal operator, it
solves the objective with an extra h(π) term without incurring additional complexity cost.
3.5	Clipping and regularising for faster convergence
In addition to the proposed algorithm, we apply two additional techniques. We explain them here
with motivations. First, we also observe that the adversarial’s update could be very sensitive to the
adversarial gradient gk, i.e. label-wise loss in each minibatch, because the gradient appears in the
exponential of the update. To avoid convergence degradation resulted from the noise in gk, we clip
the label-wise loss at value 2. Second, we notice that the KL divergence from any interior point of a
simplex to its boundary is infinity. Hence, updates near boundary can be highly unstable due to the
nonsmooth KL loss. To cope with this, we add a constant e term on the adversarial distribution to
avoid the adversarial distribution reaching any of the vertices on the simplex. The e term and clipping
is critical in both training and convergence analysis. We conduct an ablation of the sensitivity to these
parameters in Figures 5 and 6. Note that the experiments show that even without these tricks, our
proposed algorithm alone still outperform baselines.
3.6	Discussion and comparison to existing algorithms
A number of existing learning paradigms (e.g., fairness, adversarial training, and domain adaptation)
have connections to the problem of adversarial label shift; see Appendix A for details.
We comment on some key differences between AdvShift and related techniques in the literature.
For the problem of minimising the worst-case loss (2) — which is equivalent to setting the radius
r = +∞ in (3) — Sagawa et al. (2020) propose an algorithm that assumes the ability to sample
data from a given group in order to evaluate adversarial gradients. Such sampling is cumbersome to
implement in most ImageNet data loading pipelines. Mohri et al. (2019) propose a way to evaluate
gradients using importance sampling, and then apply projected gradient descent-ascent. This method
suffers from instability owing to sampling (upon which we improve with proximal updates), and
incurs a non-trivial computational overhead due to the projection step. We will illustrate these
6
Published as a conference paper at ICLR 2021
0	12	3	4
KLdivergence threshold
(b)	Ablation validation.
0	12	3	4
KLdivergence threshold
(a)	Ablation train.
0	12	3	4
KL divergence threshold
(c)	Comparison train.
0	12	3	4
KLdivergence threshold
---Baseline
Agncstic 0.05
Balanced
----Fixed 1
----Fixed 2
----Fixed 3
——AdvShift 0.01
----AdvShift 0.1
——AdvShift 1.0
(d)	Comparison validation.
Figure 2: Comparison of performance on ImageNet under adversarial label distributions. For each
method, we vary the KL divergence threshold τ , and for each τ report the maximal validation error
induced by the adversarial shift within the threshold. Subplots (a) (b) compare the performance
of ADVSHIFT trained with different DRO radius r against the default ERM training. We subtract
the baseline error of ERM from all values for easy visualization. Absolute values can be found in
Figure 8 in the Appendix. Combined with (c), (d), we see that AdvShift can reduce the adversarial
validation error by over 〜2.5% compared to the baseline method and is consistently superior to
the agnostic, balanced and fixed methods. Figure 3(c) illustrates adversarial distributions for
varying thresholds τ .
problems in our subsequent experiments (see results for AGNOSTIC in §4). Finally, for an uncertainty
set P based on the CVaR, Curi et al. (2019) provide an algorithm that updates weights using EXP3.
This approach relies on a determinantal point process, which has a poor dimension-dependence.
4	Experimental results
We now present a series of experiments to evaluate the performance of the proposed AdvShift
algorithm and how it compares to related approaches from the literature. We first explain our
experiment setups and evaluation methods. We then present the results on ImageNet dataset, and
show that under the adversarial validation setting, our proposed algorithm significantly outperforms
other methods discussed in Table 1. Similar results on CIFAR-100 are shown in the Appendix.
4.1	Experimental setup
To evaluate the proposed method, we use the standard image classification setup of training a ResNet-
50 on ImageNet using SGD with momentum as the neural network optimiser. All algorithms are run
for 90 epochs, and are found to take almost the same clock time. Note that ImageNet has a largely
balanced training label distributions, and perfectly balanced validation label distributions.
We assess the performance of models under adversarial label shift as follows. First, we train a model
on the training set and compute its error distribution on the validation set. Next, we pick a threshold
τ on the allowable KL divergence between the train and target distribution and find the adversarial
distribution within this threshold which achieves the worst-possible validation error. Finally, we
compute the validation performance under this distribution. Note that τ = 0 corresponds to the train
distribution, while τ = +∞ corresponds to the worst-case label distribution (see Figure 1).
We evaluate the following methods, each corresponding to one row in Table 1: (i) standard empirical
risk minimisation (baseline) (ii) balanced empirical risk minimisation (balanced) (iii) agnostic
federated learning algorithm of Mohri et al. (2019), which minimises the worst-case loss (agnostic)
(iv) our proposed KL-divergence based algorithm, for various choices of adversarial radius r (AD-
VSHIFT) (v) training with ADVSHIFT with a fixed adversarial distribution extracted from Figure 3(c)
(Fixed). This corresponds to the estimated test distribution row in Table 1 with an ideal estimator.
4.2	Results and discussion
Figure 2 shows the train and validation performance on ImageNet. Each curve represents the average
and standard deviation across 10 independent trials. To better illustrate the differences amongst
methods, we plot the difference in error to the baseline method. (See Figure 8 in the Appendix
for unnormalised plots.) Subfigures (a) and (b) compre the performance of AdvShift for various
choices of radius r to the ERM baseline; (c) and (d) compare ADVSHIFT to the remaining methods.
7
Published as a conference paper at ICLR 2021
」OU3 C-EH
(b) Validation.
(a) Train.
Figure 3: Subplots (a) (b) show violin plots of the distribution of errors for both the baseline and
our AdvShift methods over the course of training. On the training set, AdvShift significantly
reduces the worst-case error, evidenced by lower upper endpoints of the distribution. On the validation
set, the reduction is consistent, albeit less pronounced owing to a generalisation gap. Subplot (c)
illustrates adversarial distributions at KL distances of 1, 2 and 3 for model trained with baseline.
Even at τ = 1, the adversarial distribution is highly concentrated on only a few hard labels.
4LI6ωM -e∙ces,J3>p4
(c) Adversarial distributions.
Epoch O
IOOO
>0>0Q
Ooo
6 4 2
AUUBn balu-
0⅞005 O.OO1 0.0015 0.002
Adversarial weight
200
Epoch 18
200
Epoch 54
200
Epoch 90
Figure 4: Evolution of learned adversarial distribution (π) across training epochs. Starting off from a
uniform distribution over labels, the adversary quickly infers the relative difficulty of a small fraction
of labels, assigning nearly 2× the weight on them compared to the average. This distribution remains
largely stable in subsequent iterations, getting gradually more concentrated as training converges.
Hyperparameters for each method are separately tuned. Fixed 1, 2, 3 corresponds to training with
each of the three adversarial distributions in Figure 3(c). We see that:
•	the reduction offered by ADVSHIFT is consistently superior to that afforded by the AGNOSTIC,
Balanced and Fixed methods. On the training set, We observe significant (〜8%) reduction
in performance for large KL divergence thresholds. On the validation set, the gains are less
pronounced (〜2.5%), indicating some degradation due to a generalisation gap.
•	While ADVSHIFT consistently improves above the baseline across adversarial radii, We observe
best performance for r = 0.1. Smaller values of r lead to smaller improvements, While training
becomes increasingly unstable for larger radii. Please see the discussion in the last section.
•	during training, AGNOSTIC either learns the adversarial distribution too sloWly (such that it
behaves like ERM), or uses too large a learning rate for the adversary (such that the training fails).
This highlights the importance of the proximal mirror ascent updates in our algorithm.
Illustration of distributions at fixed KL thresholds. Figure 3(c) visualises the adversarial distribu-
tions corresponding to a feW values of the KL threshold τ . At a threshold of τ = 3, the adversarial
distribution is concentrated on only a feW hard labels. Consequently, the resulting performance on
such distributions is highly reflective of the Worst-case distribution that can happen in reality.
Training with a fixed adversarial distribution. Suppose We take the final adversarial distributions
shoWn in Figure 3(c), and then employ them as fixed distributions during training; this corresponds to
the specified a-priori and estimated validation distribution approaches in Table 1. Does the resulting
model similarly reduce the error on hard classes? Surprisingly, Figure 2(d) indicates this is not so,
and performance is in fact significantly Worse on the “easy” classes. Employing a fixed adversarial
distribution may thus lead to underfitting, Which has an intuitive explanation: the model must struggle
to fit difficult patterns from early stages of training. Similar issues With importance Weighting in
conjunction With neural netWorks have been reported in Byrd & Lipton (2019).
Evolution of error distributions. To dissect the evolution of performance during training, Figure 3
shoWs violin plots of the distribution of errors for both the baseline and our AdvShift methods
8
Published as a conference paper at ICLR 2021
UoznPaJJOJJa ura」.L
0	12	3	4
KL divergence threshold
(a)	ImageNet train.
Baseline
AdvShIft Clip 0.1
AdvShIft Clip 2
AdvShIft Clip 10
uo≈UnPaJ -JOjJa UoReP=e>
----Baseline
AdvShIft Clip 0.1
—— AdvShIft Clip 2
——AdvShIft Clip 10
(b)	ImageNet validation.
Figure 5:	Ablation of loss clipping threshold. We see that when the clipping threshold is either too
large or too small, validation performance of the model tends to suffer.
UofPnP3」」0J」3 C-EH
0	12	3	4
KL divergence threshold
(a) ImageNet train.
---Baseline
AdvShIft Eps 0.0
——AdvShIft Eps 0.001
—— AdvShIft Eps 0.01
——AdvShIft Eps 0.1
UonUnPaJ」OJJ3 Uo-⅛p=ra>
KL divergence threshold
(b) ImageNet validation.
---Baseline
AdvShIft Eps 0.0
——MvShIft Eps 0.001
——AdvShIft Eps 0.01
——MvShIft Eps 0.1
Figure 6:	Ablation of gradient stabilisation parameter , which is a constant added to the gradient
updates to prevent iterates from reaching the vertices of the simplex. We see that without any
gradient stabilisation, the model’s performance rapidly degrades as the adversarial radius increases.
Conversely, performance also suffers when the stablisation is too high.
after fixed training epochs. We observe that on the training set, AdvShift significantly reduces the
worst-case error, evidenced by the upper endpoints of the distribution being reduced. Note also that,
as expected, the adversarial algorithm is slower to reduce the error on the “easy” classes early in
training, evidenced by the lower endpoints of the distribution initially taking higher values. On the
validation set, the reduction is consistent, albeit less pronounced owing to a generalisation gap.
Evolution of learned adversarial weights. To understand the evolution of the adversarial distribu-
tion across training epochs, Figure 4 plots the histogram of adversary weights at fixed training epochs.
Starting off from a uniform distribution, the adversary is seen to quickly infer the relative difficulty of
a small fraction of labels, assigning 〜2 × the weight on them compared to the average. In subsequent
iterations the distribution becomes more concentrated, and gradually reduces the largest weights.
Ablation of clipping threshold and gradient stabiliser.
Figures 5 and 6 show an ablation of the choice of loss clipping threshold, and the gradient stabiliser
. We see that when the clipping threshold is either too large or too small, validation performance
of the model tends to suffer (albeit still better than the baseline). Similarly, we see that without any
gradient stabilisation, the model’s performance rapidly degrades as the adversarial radius increases.
Conversely, performance also suffers when the stablisation is too high.
In summary, our experiments show that our proposed DRO formulation can be effectively solved
with AdvShift, and results in a model that is robust to adversarial label shift.
5	Discussion and future work
We proposed AdvShift, an algorithm for coping with label shift based on distributionally robust
optimisation, and illustrated its effectiveness of real-world datasets. Despite this, our approach does
not solve the problem fully. First, Figure 2(a)(b) shows that the generalization gap increases as the
perturbation radius increases. Understanding why there is a correlation between hard examples and
bad generalization could improve robustness. Second, Figure 2(a) shows that even on the train set, the
algorithm threshold r does not translate to the model’s level of robustness. We conjecture this results
from the interplay of model expressivity and data distribution, whose future study is of interest.
9
Published as a conference paper at ICLR 2021
References
Roclo Alaiz-Rodrlguez, Alicia Guerrero-Curieses, and Jesus Cid-Sueiro. Minimax regret classifier
for imprecise class distributions. Journal of Machine Learning Research, 8:103-130, May 2007.
ISSN 1532-4435.
Martin Arjovsky, Leon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization,
2019.
Kamyar Azizzadenesheli, Anqi Liu, Fanny Yang, and Animashree Anandkumar. Regularized learning
for domain adaptation under label shifts. In International Conference on Learning Representations,
2019.
Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for
domain adaptation. In Advances in neural information processing systems, pp. 137-144, 2007.
Sebastien Bubeck. Convex optimization: Algorithms and complexity, 2014.
Jonathon Byrd and Zachary Chase Lipton. What is the effect of importance weighting in deep
learning? In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th Inter-
national Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California,
USA, volume 97 of Proceedings of Machine Learning Research, pp. 872-881. PMLR, 2019.
Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced
datasets with label-distribution-aware margin loss. In Advances in Neural Information Processing
Systems, 2019.
Remi Tachet des Combes, Han Zhao, Yu-Xiang Wang, and Geoff Gordon. Domain adaptation with
conditional distribution matching and generalized label shift. arXiv preprint arXiv:2003.04475,
2020.
Patrick L Combettes and Jean-Christophe Pesquet. Proximal splitting methods in signal processing.
In Fixed-point algorithms for inverse problems in science and engineering, pp. 185-212. Springer,
2011.
Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based on
effective number of samples. In CVPR, 2019.
Sebastian Curi, Kfir. Y. Levy, Stefanie Jegelka, and Andreas Krause. Adaptive sampling for stochastic
risk-averse learning, 2019.
M. A. Davenport, R. G. Baraniuk, and C. D. Scott. Tuning support vector machines for minimax and
neyman-pearson classification. IEEE Transactions on Pattern Analysis and Machine Intelligence,
32(10):1888-1898, 2010.
Damek Davis and Dmitriy Drusvyatskiy. Stochastic model-based minimization of weakly convex
functions. SIAM Journal on Optimization, 29(1):207-239, 2019.
Michele Donini, Luca Oneto, Shai Ben-David, John Shawe-Taylor, and Massimiliano Pontil. Em-
pirical risk minimization under fairness constraints. In Proceedings of the 32nd International
Conference on Neural Information Processing Systems, NIPS’18, pp. 2796-2806, Red Hook, NY,
USA, 2018. Curran Associates Inc.
Marthinus Christoffel du Plessis and Masashi Sugiyama. Semi-supervised learning of class balance
under class-prior change by distribution matching. Neural Networks, 50:110-119, 2014.
J. Duchi and H. Namkoong. Learning models with uniform performance via distributionally robust
optimization. arXiv preprint arXiv:1810.08750, 2018.
John Duchi, Tatsunori Hashimoto, and Hongseok Namkoong. Distributionally robust losses for latent
covariate mixtures, 2020.
Charles Elkan. The foundations of cost-sensitive learning. In Proceedings of the 17th International
Joint Conference on Artificial Intelligence - Volume 2, IJCAI’01, pp. 973-978, San Francisco, CA,
USA, 2001. Morgan Kaufmann Publishers Inc. ISBN 1558608125.
10
Published as a conference paper at ICLR 2021
Yanbo Fan, Siwei Lyu, Yiming Ying, and Baogang Hu. Learning with average top-k loss. In Advances
in neural information processing Systems, pp. 497-505, 2017.
Louis Faury, Ugo Tanielian, Elvis Dohmatob, Elena Smirnova, and Flavian Vasile. Distributionally
robust counterfactual risk minimization. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 34, pp. 3850-3857, 2020.
Saurabh Garg, Yifan Wu, Sivaraman Balakrishnan, and Zachary C. Lipton. A unified view of label
shift estimation, 2020.
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In Zoubin Ghahramani,
Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger (eds.), Advances in
Neural Information Processing Systems 27: Annual Conference on Neural Information Processing
Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 2672-2680, 2014.
Jiaxian Guo, Mingming Gong, Tongliang Liu, Kun Zhang, and Dacheng Tao. Ltf: A label transfor-
mation framework for correcting label shift. In International Conference on Machine Learning, pp.
3843-3853. PMLR, 2020.
Tatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness without
demographics in repeated loss minimization. In Jennifer Dy and Andreas Krause (eds.), Interna-
tional Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research,
pp. 1929-1938, Stockholmsmassan, Stockholm Sweden, 10-15 JUl 2018. PMLR.
Haibo He and Edwardo A. Garcia. Learning from imbalanced data. IEEE Transactions on Knowledge
and Data Engineering, 21(9):1263-1284, 2009.
WeihUa HU, Gang NiU, Issei Sato, and Masashi SUgiyama. Does distribUtionally robUst sUpervised
learning give robUst classifiers? In Jennifer G. Dy and Andreas KraUse (eds.), Proceedings
of the 35th International Conference on Machine Learning, ICML 2018, StOCkhOlmSmaSSan,
Stockholm, Sweden, July 10-15, 2018, volUme 80 of Proceedings of Machine Learning Research,
pp. 2034-2042. PMLR, 2018.
Bingyi Kang, Saining Xie, MarcUs Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yan-
nis Kalantidis. DecoUpling representation and classifier for long-tailed recognition. In Eighth
International Conference on Learning RepreSentationS (ICLR), 2020.
WoUter M. KoUw and Marco Loog. An introdUction to domain adaptation and transfer learning.
CoRR, abs/1812.11806, 2018. URL http://arxiv.org/abs/1812.11806.
Daniel Levy, Yair Carmon, John C DUchi, and Aaron Sidford. Large-scale methods for distribUtionally
robUst optimization. AdvanceS in Neural Information ProceSSing SyStemS, 33, 2020.
Tianyi Lin, Chi Jin, and Michael I. Jordan. On gradient descent ascent for nonconvex-concave
minimax problems, 2019.
Tianyi Lin, Chi Jin, and Michael. I. Jordan. Near-optimal algorithms for minimax optimization, 2020.
Zachary Lipton, YU-Xiang Wang, and Alexander Smola. Detecting and correcting for label shift
with black box predictors. In Jennifer Dy and Andreas KraUse (eds.), International Conference
on Machine Learning, volUme 80 of ProceedingS of Machine Learning ReSearch, pp. 3122-3130,
Stockholmsmassan, Stockholm Sweden, 10-15 Jul 2018. PMLR.
Mehryar Mohri, Gary Sivek, and Ananda Theertha SUresh. Agnostic federated learning. In Interna-
tional Conference on Machine Learning, 2019.
Krikamol Muandet, David Balduzzi, and Bernhard Scholkopf. Domain generalization via invariant
feature representation. In Sanjoy Dasgupta and David McAllester (eds.), International Conference
on Machine Learning, Proceedings of Machine Learning Research, pp. 10-18, Atlanta, Georgia,
USA, 17-19 Jun 2013. PMLR.
H. Namkoong and J. Duchi. Stochastic gradient methods for distributionally robust optimization with
f-divergences. In AdvanceS in Neural Information ProceSSing SyStemS (NeurIPS), 2016.
11
Published as a conference paper at ICLR 2021
Hongseok Namkoong and John C Duchi. Variance-based regularization with convex objectives. In
I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.),
Advances in Neural Information Processing Systems 30, pp. 2971-2980. Curran Associates, Inc.,
2017.
Hamed Rahimian and Sanjay Mehrotra. Distributionally robust optimization: A review, 2019.
Marco Saerens, Patrice Latinne, and Christine Decaestecker. Adjusting the outputs of a classifier to
new a priori probabilities: A simple procedure. Neural Computation, 14(1):21-41, 2002.
S.	Sagawa, P. W. Koh, T. B. Hashimoto, and P. Liang. Distributionally robust neural networks for
group shifts: On the importance of regularization for worst-case generalization. In International
Conference on Learning Representations (ICLR), 2020.
Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczynski. Lectures on Stochastic Program-
ming: Modeling and Theory, Second Edition. Society for Industrial and Applied Mathematics,
Philadelphia, PA, 2014. doi: 10.1137/1.9781611973433.
Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness with
principled adversarial training. arXiv preprint arXiv:1710.10571, 2017.
Aman Sinha, Hongseok Namkoong, and John Duchi. Certifiable distributional robustness with
principled adversarial training. In International Conference on Learning Representations, 2018.
Amos J Storkey and Masashi Sugiyama. Mixture regression for covariate shift. In B. Scholkopf,
J. C. Platt, and T. Hoffman (eds.), Advances in Neural Information Processing Systems 19, pp.
1337-1344. MIT Press, 2007.
Kiran Koshy Thekumparampil, Prateek Jain, Praneeth Netrapalli, and Sewoong Oh. Efficient
algorithms for smooth minimax optimization, 2019.
Robert C. Williamson and Aditya Krishna Menon. Fairness risk measures. In Proceedings of the
36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach,
California, USA, pp. 6786-6797, 2019.
Yu Xie and Charles F. Manski. The logit model and response-based samples. Sociological Methods
& Research, 17(3):283-302, 1989.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P. Gummadi. Fair-
ness beyond disparate treatment & disparate impact: Learning classification without disparate
mistreatment. In Proceedings of the 26th International Conference on World Wide Web, WWW
’17, pp. 1171-1180, Republic and Canton of Geneva, CHE, 2017. International World Wide Web
Conferences Steering Committee. ISBN 9781450349130.
Kun Zhang, Bernhard Scholkopf, Krikamol Muandet, and Zhikun Wang. Domain adaptation under
target and conditional shift. In Proceedings of the 30th International Conference on International
Conference on Machine Learning - Volume 28, ICML’13, pp. III-819-III-827. JMLR.org, 2013.
12
Published as a conference paper at ICLR 2021
A Related problems
Example-level DRO. Existing work on DRO has largely focussed on the setting where P encom-
passes shifts in the instance space (Namkoong & Duchi, 2016; 2017; Sinha et al., 2018; Duchi &
Namkoong, 2018; Levy et al., 2020). This notion of robustness has a natural link with adversarial
training (Sinha et al., 2017), and involves a more challenging problem, as it requires parameterising
the adversary’s distribution. Hu et al. (2018) illustrate the potential pitfalls of DRO, owing to a
mismatch between surrogate and 0-1 losses. They also propose to encode an uncertainty set based on
latent label distribution shift (Storkey & Sugiyama, 2007), which requires domain knowledge. The
techniques in example-level DRO are mostly designed for small scale dataset with SVM models, as
these techniques require sampling according to adversarial distribution, which can be very unstable if
implemented with importance sampling only. It also requires maintaining a vector proportional to the
number of labels and indexing each sample during training to match up the sample index, which is
not available in most dataloading pipelines.
Fairness. Adversarial label shift may be related to algorithmic fairness. Abstractly, this concerns
the mitigation of systematic bias in predictions on sensitive subgroups (e.g., country of origin). One
fairness criteria posits that the per-subgroup errors should be equal (Zafar et al., 2017; Donini et al.,
2018), an ideal that may be targetted by minimising the worst-subgroup error (Mohri et al., 2019;
Sagawa et al., 2020). When the subgroups correspond to labels, ensuring this notion of fairness is
tantamount to guarding against an adversary that can place all mass on the worst performing label.
GANs. GANs (Goodfellow et al., 2014) involve solving a min-max objective that bears some
similarity to the DRO formulation (3), but is fundamentally different in details: while DRO considers
reweighting of samples according to a fixed family, GANs involve a parameterised adversarial family,
with the training objective augmented with an additional penalty.
Domain adaptation. Label shift can be viewed as a special case of domain adaptation, where ptr and
pte can systematically differ. Typically, one assumes access to a small sample from pte , which may
be used to estimate importance weights (Combes et al., 2020), or samples from multiple domains,
which may be used to estimate a generic domain-agnostic representation (Muandet et al., 2013). In
causal inference, there has been interest in similar classes of models (Arjovsky et al., 2019).
B Algorithm implementation details
We introduce some additional details in our implementation of AdvShift. First, as observed in
Section 3.1, our algorithm requires knowing the empirical label distribution. As the exact value is not
always available, we estimate the empirical label distribution online for all the experiments presented
later in Section 4 using an exponential moving average, PemP = β ∙ PemP + (1 - β) ∙ PbatCh, where
pbatch is the label distribution in the minibatch. We set β = 0.999. The number is set such that the
exponential moving average has a half-life roughly equal to the number of iterations in one epoch of
ImageNet training using our setup.
In all the experiments, we set 2γcλ = 1 in Algorithm 1 for simplicity. For learning the adversarial
distribution, we only tune the adversarial learning rate ηπ .
C Additional experimental results
We present here additional experimental results, including:
•	for ImageNet, an illustration of the lack of correlation between a label’s frequency in the
training set, and its validation error. (Figure 7)
•	unnormalised versions of the results on ImageNet shown in the body, where we do not
subtract the baseline performance from each of the curves; this gives a sense of the absolute
performance numbers obtained by each method. (Figure 8)
•	an ablation of the loss clipping threshold and gradient stabiliser as introduced above.
(Figure 5,6)
•	results on CIFAR-100, to complement those for ImageNet. (Figure 9,10)
13
Published as a conference paper at ICLR 2021
0.0006 0.0007 0.0008 0.0009 0.0010
Label frequency
Figure 7: Illustration that training label frequency does not strongly correlate with test error. Observe
that several classes with a high error appear frequently in the training set.
——■ Baseline
Agnostic 0.05
Balanced
----Fixed 1
——AdvShiftO.1
——■ Baseline
Agnostic 0.05
Balanced
----Fixed 1
——AdvShift 0.1
(a)	ImageNet train.
(b)	ImageNet validation.
Figure 8:	Comparison of performance of various methods on ImageNet under adversarial label
distributions. For each plot, we vary a KL divergence threshold τ , and for a given τ construct the
label distribution which results in maximal test error for the baseline model. We then compute the
test error under this distribution. Note that the case τ = 0 corresponds to using the train distribution,
while τ = +∞ corresponds to using the worst-case label distribution, which is concentrated on
the worst-performing label. Our proposed AdvShift can reduce the adversarial test error by over
〜2.5% over the baseline method.
C.1 BALANCED LABELS =6⇒ BALANCED PERFORMANCE
Figure 7 shows that training label frequency does not strongly correlate with test error. Observe that
several classes with a high error appear frequently in the training set. Indeed, the three classes with
highest error - Casette player, maillot, and water jug - all appear an equal number of
times in the training set.
C.2 Unnormalised plots on ImageNet
Figure 8 presents plots of the unnormalised performance of the various methods compared in the
body. Here, rather than subtract the performance of the baseline, we show the absolute accuracy of
each method as the adversarial radius is varied. Evidently, the baseline and Agnostic models tend
to suffer in their validation error as the adversarial radius increases.
C.3 Results on CIFAR- 1 00
Figure 9 shows results on CIFAR-100, where we train various methods using a CIFAR-ResNet-18
as the underlying architecture, Here, we see a consistent and sizable improvement from AdvShift
over the baseline method. On this dataset, Agnostic fares better, and eventually matches the
performance of AdvShift with a large adversarial radius. This is in keeping with the intended
use-case of Agnostic, i.e., minimising the worst-case loss. Figure 10 supplements these plots with
unnormalised versions, to illustrate the absolute performance differences.
14
Published as a conference paper at ICLR 2021
5 。
1 2
- -
UOqUnPaI」0」」8 C-
-25
0	1	2	3	4	5
KLdivergence threshold
----Baseline
——■ Agnostic 0.001
——■ Agnostic 0.01
——■ Agnostic 0.1
——AdvShift 0.01
——AdvShiftO.I
AdvShift 0.3
UoLPnP①」」。」」8 Uo4up-ra>
2.5
0.0
-2.5
-5.0
-7.5
-10.0
KL divergence threshold
----Baseline
——■ Agnostic 0.001
——■ Agnostic 0.01
——■ Agnostic 0.1
——AdvShift 0.01
——AdvShiftO.I
AdvShift 0.3
(a) CIFAR-100 train.
(b) CIFAR-100 validation.
Figure 9:	Comparison of performance of various methods on CIFAR-100.
。 。 Q
5 4 3
」0」」8 C-2F
20
0	1	2	3	4	5
KL divergence threshold
----Baseline
——■ Agnostic 0.001
——■ Agnostic 0.01
——■ Agnostic 0.1
——AdvShift 0.01
——AdvShiftO.I
AdvShift 0.3
70
e
⅛ 60
I 50
40
0	1	2	3	4	5
KL divergence threshold
(b) CIFAR-100 Validation.
----Baseline
——■ Agnostic 0.001
——■ Agnostic 0.01
——■ Agnostic 0.1
——AdvShift 0.01
——AdvShift 0.1
AdvShift 0.3
(a) CIFAR-100 train.
Figure 10:	Comparison of performance of various methods on CIFAR-100 (unnormalised).
We remark here that the choice of a CIFAR-ResNet-18 results in an underparameterised model, which
does not perfectly fit the training data. In the overparameterised case, there are challenges with
employing DRO, as noted by Sagawa et al. (2020). Addressing these challenges in settings where the
training data is balanced remains an interesting open question.
D Constrained DRO does not permit a B oltzman s olution
We start with a simple example with three label classes {a, b, c} with class losses l = {1, 2, 4}
respectively. We assume an uniform empirical distribution, i.e. pemp = {1/3, 1/3, 1/3}. We
consider two different problems. The first is to find the optimal solution to regularised objective,
p = argminp p>l + γKL(p, pe
mp).
This problem is well known (e.g. see 2.7.2 of lecture ) to permit a solution of form p(x) =
P——exp lx/t “ /十 for some t.
x0∈{a,b,c} exp l0x/t
In contrast, we show that distributions of the above form does not solve the constrained version of the
problem. In particular, we consider the following optimisation problem:
max p> l
p
such that KL(p, pemp) ≤ r
If the solution is of form p(x) = P / ：xp lx/Xp * 八,then We know for lb = lc.
(log(pa) - log(pc))/(log(pb) - log(pc)) = (la - lc)/(lb - lc).
We solve the above problem with a convex optimizer using r = 0.01 and found pa = 0.283, pb
0.322, pc = 0.395.
(log(pa) - log(pc))/(log(pb) - log(pc)) = 1.64 6= (la - lc)/(lb - lc) = 1.5.
Note that the above example shows that not all solutions of the contrained problem can be written
a Boltzman distribution, i.e. p(x) =	——expIlx/—ττ-77. Yet, this does not contradict with results
x0∈{a,b,c} exp lx/t
15
Published as a conference paper at ICLR 2021
[e.g. Lemma 4, Faury et al. (2020)] that claim there is a Boltzman distribution whose function
value matches the optimal value of the constraint problem. Mathematically, we can have p 6= p0 but
Ep[l(x)] = Ep0 [l(x)].
E Projecting an adversarial distribution
The projection operator in our setting aims to project a distribution P into the set P = {q : KL(q, P) ≤
r} by solving the following problem:
min kq -Pk2
q
such that	qi log(qi/Pi) ≤ r
i
qi = 1,
i
∀i, qi ≥ 0,
where qi , Pi denotes the ith component of q, P and n denotes number of classes. Given that our
implementation is based on Tensorflow, we use the “trust-region constrained algorithm” provided by
SciPy for easy integration with our python-based training procedure. However, even after extensive
tuning, solving each problem up to 1% relative constraint error requires more than 1 minute when
n = 1000 (the number of labels in ImageNet). This means that if we train ResNet50 on ImageNet for
100k iterations, we need to spend 100k minutes on projection operation, which is not affordable.
F	Proof of Proposition 1
Proof. We only need to show that for large enough γc, any minimiser p* of the unconstrained problem
satisfies that KL(p*,pemp) ≤ r. Since the distance from boundary of the simplex to any interior
point is +∞, we can safely assume that the point lies within the relative interior of the simplex.
To prove the proposition, denote C = inf{kVpKL(p,pemp)k | P ∈ ∆L, s.t. KL(p,pemp) > r}.
By strict convexity of KL divergence and the fact that r > 0, we know c > 0. Denote the
upper bound of loss as M, then when γc > M/c, we know that KL(P, Pemp) > r =⇒ 0 6∈
∂∏ (En ['(x, y, θ)] +min{0, γc(r 一 KL(π,pemp))}). However, since P minimises the objective if and
only if 0 ∈ ∂∏(En ['(x,y,θ)] +min{0,γc(r 一 KL(π,pemp))}), we have that any minimiser P must
satisfy KL(π,Pemp)) ≤ r.	□
G Proof of Lemma 2
Recall that we want to find πk+1 that minimises the following objective,
∏k+ι = argmin h(∏) + 2λ (KL(π,∏k) + 2λ(gk,π>)
argmin max ∣0, ] ：； KL(∏,Pemp)} + ] :& (KL(∏,∏k) 一 ηhgk,∏i),
where η = 1∕(2γc + 1∕λ), a。 = 2γcλ. Denote v(i) as the ith component of vector v. Notice that
the simplex can be written as a constraint that Pi π(i) =1; ∀i, π(i) ≥ 0. Based on this constraint,
we first write (6)’s Lagrangian dual
L(a, b, π) =	(aiπ(i)) + b(	π(i) 一 1) + max 0,
ii
α
TT^ (KL(π,Pemp i
+ 1 +1α (KL(π,πk) 一 ηhgk,πi)
where π(i) denotes the ith component of π. If πk > 0 component-wise, then the optimal π cannot lie
on the boundary (i.e. ∀i, π(i) > 0), which results in KL(π, πk) = ∞. By Lagrangian duality and
16
Published as a conference paper at ICLR 2021
complementary slackness, we know that for if KL(π, pemp) > r,
∂α	1
0 = ∂∏(i)L(a, b,∏) = b + 1 + α log(∏(i)∕Pemp(i)) + 1 + α log(∏(i)∕∏k(i)) - ηgk(i) + L
On the other hand, if KL(π, pemp) < r
∂1
0 = ∂∏(i)L(a,b,π) = b + 1 + α log(π(i"πk(Z)) - ηgk(i) + 1.
We discuss the case when KL(π, pemp) < r, and the other case follows similarly. Rearrange the
optimality condition of Lagrangian multiplier, we get
α1
1+α lθg(∏(i)∕Pemp(i)) + 1 + α lθg(∏(i)∕∏k (i)) — ηgk (i) = -b — 1.
Since b is a constant for all coordinates,
π(i) (X (Pemp(i)α∏k(i))1/(1+ac)exp(^^)
The result follows by noting that Pi π(i) = 1.
H	Proof of Theorem 3
For completeness, we define several terms used in optimisation. A function f (θ) is l—Lipschitz if
for all θ, θ0,
If(θ) — f(θ0)l ≤ ikθ -θ0k.
A function f(θ) is L—smooth if for all θ, θ0,
kVf(θ)-Vf(θ0)k ≤ Lkθ -θ0k.
A function f (θ) is L-Weakly convex if f (θ) + ^L ∣∣θk2 is convex.
Then we can state the formal theorem below.
Theorem 4 (formal version of Theorem 3). Under Assumptions 1-4, the update in (8) generates a
sequence of points θ1, ..., θT with the following property:
T X E[kvF1∕2L(Ot) k2] ≤T1/4 (L (F1/2L(OO)- FT/2L) +2G + G2 + ɪ + (l2 +。2)1/2))
+ (h* - h(πO))/T
Proof. For convenience, denote Φ(O, π) = f(O, π) + h(π), F(O) = maxp Φ(O,p). We start by
following the standard SGD proof. Denote gθ as the stochastic gradient evaluated at step t - 1 with
respect to O. Denote O = proxF/2L(O) := arg minw {F (w) + 2L∣w - O∣2}. Conditioned on Ot-1,
we have
E[%t-ι - θtk2] =∣θt-ι — θt-1k2 + 2ηθE[hθt-ι - θt-ι,gθi]+ ηθE[∣∣gθ∣∣2]
≤∣∣θt-ι — θt-ik2 + 2ηθGt-I- Θt-1, VθΦ(θt-1, ∏t-ι)〉+ η2(L2 + σ2)	(9)
where the first equality follows by Ot = Ot-1 - ηθVθΦ(Ot-1, πt-1). Next, we observe that
L
hθt-ι - θt-ι, VθΦ(θt-i,∏t-i)i ≤ Φ(θt-i,∏t-i) - Φ(θt-i,∏t-i) + — ∣∣θt-ι - θt-1∣	(10)
L
≤ F(θt-ι) — Φ(θt-1,πt-1) + — ∣θt-ι - θt-1k2
≤ F(θt-ι) — Φ(θt-1,πt-1) — — ∣θt-ι - θt-1k2
17
Published as a conference paper at ICLR 2021
The first line follows by convexity and L-smoothness. The second line by definition of F. The third
line by definition of θ. Next, by definition of Moreau envelop,
Fι∕2L(θt) ≤ F(θt-ι) + Lkθt-1- θtk2
Take expectation on both sides and we get
E[Fι∕2L(θt)]	(11)
≤ F(θt-ι) + E[L∣∣θt-ι- θtk2]
=F(θt-ι) + L(Ilθt-ι - θt-ιk2 + 2ηθhθt-ι - θt-ι, Vθφ(θt-ι,πt-ι)i + η2(l2 + σ2))
≤ Fi∕2l(Θji) + 2Lηd(Φ(θt-i,∏t-i) - Φ(θt-i,∏t-i)-刍∣θt-i - θt-ik2) + L喻(l + σ2)
The second line substitutes in (9). The third line follows by convexity and L-smoothness. Denote
that ∆t := F(θt-1) - Φ(θt-1, πt-1) ≥ Φ(θt-1, πt-1) - Φ(θt-1, πt-1). We can sum over t and
take expectation recursively to get,
XE[∣VFι∕2L(θt)k2] = 2LXE[kθt-1 - θt-ιk2]	(12)
tt
2
≤ Lη^ (F1∕2L(θθ) - F1/2L) + 4[δ, + Tηθ (I + σ ))
where F；” = min& Fi∕2l(Θ). The first equality follows by the definition of Moreau envelope. The
second inequality follows by rearranging (11).
Next, we aim to bound the accumulated error Pt ∆t .
Recall that the update for the π is as follows for E[gπ] = Vπf (θ, π),
πk+1 := argminπ∈∆L {-2λhgπ, πi - 2λh(π) + KL(π, πk)}
Applying Lemma 5 with L(π) = -2λhgπ, πi + -2λh(π), we get
-h(∏ (θs)) - hg∏,∏*(θs)i +KL(∏*,∏k)
≥ -hg∏,∏k+ιi - h(∏k+ι) +KL(∏k+ι,∏k) +KL(π*,∏k+ι)
Rearrange and take expectation we get
2λ(E[h-g∏, ∏k - ∏*(θs)>] - E[h(∏k+ι)]+ E[h(π*(θs))])
≤ -E[KL(∏k+ι,∏k)]+ KL(π*,∏k+ι) - KL(π*,∏k) + 2λ(E[<g∏,∏k - ∏k+ιi])
≤ -E[KL(∏k,∏*)] + E[KL(∏*,∏k+ι)] - k∏* - ∏k∣2∕2 + 2λ2E[∣g∏k∞] + ∣∣π* - ∏|附2
The second inequality follows by the fact that KL-divergence is strongly convex with respect to L1
norm and Cauchy-Schwartz inequality. We further observe that
-EKgn,∏k - ∏*(θs))] = -hV∏f(θk,∏k),∏k - ∏*(θs)i ≥ -f(θk,∏k) + f(θk,∏*(θs))
=-f(θk ,∏k)+ f(θk ,∏*(θk)) - f(θk ,∏*(θk))+ f(θk ,∏*(θs))
≥ -f(θk ,∏k)+ f(θk ,∏*(θk)) - f(θk ,∏*(θk))+ f(θs,∏*(θk))
-f(θs,∏*(θs))+ f(θk ,∏*(θs))
≥ -f(θk ,πk ) + f (θk ,π*(θk )) - 2lkθs - θk k
The first inequality follows by concavity. The third line follows by f(θs, ∏;(θk)) ≤ f(θs, ∏;(θs)).
The last inequality follows by Lipschitzness. Similarly,
-h(θk, πk ) + h(θk , π*(θS)) ≥ -h(θk ,πk ) + h(θk, π*(θk )) - 2lkθs - θkk
We can take iterative expectation and get sum over k = s + 1, ..., s + B to get
s+B
X E[-f (θk, ∏k) + f(θk, ∏*(θk)) - h(∏k) + h(∏*(θk))]
k=s+1
s+B k
≤ -h(πs) + E[h(πs+B)] + 4l X X E[Iθj - θj+1 I] + λBG2
k=s+1 j=s
+ U(E[KL(∏*(θs),∏o)]- E[KL(∏*(θs),∏s)])
2λ
18
Published as a conference paper at ICLR 2021
Note that ∆ = -f(θk,∏k) + f(θk,∏*(θk))- h(∏k) + h(∏*(θk)), hence
s+B
X E[∆k] ≤ -h(πs) + E[h(πs+B)] + 2ηθlB2G + λBG2
k=s+1
+ /(E[KL(∏*(θs), ∏o)] - E[KL(π*(θs), ∏s)])
2λ
By further sum over all blocks and divide by total number of iterations T , we get
1T/B s+B
T XX E[∆k] ≤ (-h(∏o) + E[h(∏τ)])/t + 2ηθBG + λG2
b=1 k=bs+1
+ -1-(E[KL(π*(θs),∏o)] - E[KL(π*(θs),∏s)])
2λB
Substitute the above inequality into (12) and we get
12
T £ E[∣NF1∕2L(Ot)Il ] ≤TL— (F1/2L(θ0) - F1/2L) + 4(-h(πO) + E[h(πT)])/T + 2ηθθBG + λG
T t	TLηθ
+ 与(E[KL(∏*(θs),∏o)] - E[KL(π*(θτ),∏τ)])+ η(l2 + σ2)1∕2)
2λB
If we set ηθ = T-3/4, B = T1/2 , λ = T-1/4, then we see that
T X E[IVF1/2L(%)k2] ≤ T1∕4 (L (F1/2L(OO)- FT/2L) + 2G + G2 + ɪ + (l2 + σ2)1∕2))
+ (-h(πθ) + h*”T
□
Lemma 5. For any differentiable Convexfunction L, if x* = argminχ∈∆{L(x) + KL(x, x0)}, then
for any x0 ∈ ∆, we have
'(x0) + KL(x0, xo) ≥ '(x*) + KL(x*, xo) + KL(x0, x*).
Proof. This Lemma is well-known, but we include a proof for completeness. By optimality of x*
and convexity of ∆, we know that
hV'(x*) + Vφ(x*) — Vφ(xo),x — x*i ≥ 0,
where φ(x) = Pi xi log(xi), and the Bregman divergence defined according to φ is KL-divergence.
Then
'(x0) ≥ '(x*) + hV'(x*),x0 - x*i
≥ '(x*) + hVφ(xo) — Vφ(x*),x — x*i
='(x* ) - hVφ(xo), X* - Xoi + φ(x*) - φ(xo)
+ hVφ(xO), x0- xOi + φ(x0) - φ(xO) - hVφ(x*), x0- x*i + φ(x0) - φ(x*)
='(x*) + KL(x*, xo) — KL(x0, xo) + KL(x0, x*)
□
19