Published as a conference paper at ICLR 2021
Representation Balancing
Offline Model-based Reinforcement Learning
Byung-Jun Lee1,3, Jongmin Lee1 & Kee-Eung Kim1,2
1 School of Computing, KAIST, Daejeon, Republic of Korea
2Graduate School of AI, KAIST, Daejeon, Republic of Korea
3Gauss Labs Inc., Seoul, Republic of Korea
{bjlee,jmlee}@ai.kaist.ac.kr, kekim@kaist.ac.kr
Ab stract
One of the main challenges in offline and off-policy reinforcement learning is to
cope with the distribution shift that arises from the mismatch between the target
policy and the data collection policy. In this paper, we focus on a model-based
approach, particularly on learning the representation for a robust model of the
environment under the distribution shift, which has been first studied by Repre-
sentation Balancing MDP (RepBM). Although this prior work has shown promis-
ing results, there are a number of shortcomings that still hinder its applicability
to practical tasks. In particular, we address the curse of horizon exhibited by
RepBM, rejecting most of the pre-collected data in long-term tasks. We present a
new objective for model learning motivated by recent advances in the estimation
of stationary distribution corrections. This effectively overcomes the aforemen-
tioned limitation of RepBM, as well as naturally extending to continuous action
spaces and stochastic policies. We also present an offline model-based policy op-
timization using this new objective, yielding the state-of-the-art performance in a
representative set of benchmark offline RL tasks.
1	Introduction
Reinforcement learning (RL) has accomplished remarkable results in a wide range of domains, but
its successes were mostly based on a large number of online interactions with the environment.
However, in many real-world tasks, exploratory online interactions are either very expensive or
dangerous (e.g. robotics, autonomous driving, and healthcare), and applying a standard online RL
would be impractical. Consequently, the ability to optimize RL agents reliably without online inter-
actions has been considered as a key to practical deployment, which is the main goal of batch RL,
also known as offline RL (Fujimoto et al., 2019; Levine et al., 2020).
In an offline RL algorithm, accurate policy evaluation and reliable policy improvement are both
crucial for the successful training of the agent. Evaluating policies in offline RL is essentially an
off-policy evaluation (OPE) task, which aims to evaluate the target policy given the dataset collected
from the behavior policy. The difference between the target and the behavior policies causes a
distribution shift in the estimation, which needs to be adequately addressed for accurate policy
evaluation. OPE itself is one of the long-standing hard problems in RL (Sutton et al., 1998; 2009;
Thomas & Brunskill, 2016; Hallak & Mannor, 2017).
However, recent offline RL studies mainly focus on how to improve the policy conservatively while
using a common policy evaluation technique without much considerations for the distribution shift,
e.g. mean squared temporal difference error minimization or maximum-likelihood training of en-
vironment model (Fujimoto et al., 2019; Kumar et al., 2019; Yu et al., 2020). While conservative
policy improvement helps the policy evaluation by reducing the off-policyness, we hypothesize that
addressing the distribution shift explicitly during the policy evaluation can further improve the over-
all performance, since it can provide a better foundation for policy improvement.
To this end, we aim to explicitly address the distribution shift of the OPE estimator used in the
offline RL algorithm. In particular, we focus on the model-based approach, where we train an
1
Published as a conference paper at ICLR 2021
environment model robust to the distribution shift. One of the notable prior works is Representation
Balancing MDP (RepBM) (Liu et al., 2018b), which regularizes the representation learning of the
model to be invariant between the distributions. However, despite the promising results by RepBM,
its step-wise estimation of the distance between the distributions has a few drawbacks that limit the
algorithm from being practical: not only it assumes a discrete-action task where the target policy is
deterministic, but it also performs poorly in long-term tasks due to the curse of horizon of step-wise
importance sampling (IS) estimators (Liu et al., 2018a).
To address these limitations, we present the Representation Balancing with Stationary Distribution
Estimation (RepB-SDE) framework, where we aim to learn a balanced representation by regular-
izing, in the representation space, the distance between the data distribution and the discounted
stationary distribution induced by the target policy. Motivated by the recent advances in estimating
stationary distribution corrections, we present a new representation balancing objective to train a
model of the environment that no longer suffers from the curse of horizon. We empirically show
that the model trained by the RepB-SDE objective is robust to the distribution shift for the OPE task,
particularly when the difference between the target and the behavior is large. We also introduce a
model-based offline RL algorithm based on the RepB-SDE framework and report its performance on
the D4RL benchmark (Fu et al., 2020), showing the state-of-the-art performance in a representative
set of tasks.
2	Related work
Learning balanced representation Learning a representation invariant to specific aspects of data
is an established method for overcoming distribution shift that arises in unsupervised domain adap-
tation (Ben-David et al., 2007; Zemel et al., 2013) and in causal inference from observational
data (Shalit et al., 2017; Johansson et al., 2018). They have shown that imposing a bound on the
generalization error under the distribution shift leads to the objective that learns a balanced repre-
sentation such that the training and the test distributions look similar. RepBM (Liu et al., 2018b)
can be seen as a direct extension to the sequential case, which encourages the representation to be
invariant under the target and behavior policies in each timestep.
Stationary distribution correction estimation (DICE) Step-wise importance sampling (IS) es-
timators (Precup, 2000) compute importance weights by taking the product of per-step distribution
ratios. Consequently, these methods suffer from exponentially high variance in the lengths of trajec-
tories, which is a phenomenon called the curse of horizon (Liu et al., 2018a). Recently, techniques of
computing a stationary DIstribution Correction Estimation (DICE) have made remarkable progress
that effectively addresses the curse of horizon (Liu et al., 2018a; Nachum et al., 2019a; Tang et al.,
2020; Zhang et al., 2020; Mousavi et al., 2020). DICE has been also used to explicitly address the
distribution shift in online model-free RL, by directly applying IS on the policy and action-value
objectives (Liu et al., 2019; Gelada & Bellemare, 2019). We adopt one of the estimation techniques,
DualDICE (Nachum et al., 2019a), to measure the distance between the stationary distribution and
the data distribution in the representation space.
Offline reinforcement learning There are extensive studies on improving standard online model-
free RL algorithms (Mnih et al., 2015; Lillicrap et al., 2016; Haarnoja et al., 2018) for stable learning
in the offline setting. The main idea behind them is to conservatively improve policy by (1) quanti-
fying the uncertainty of value function estimate, e.g. using bootstrapped ensembles (Kumar et al.,
2019; Agarwal et al., 2020), or/and (2) constraining the optimized target policy to be close to the
behavior policy (i.e. behavior regularization approaches) (Fujimoto et al., 2019; Kumar et al., 2019;
Wu et al., 2019; Lee et al., 2020). A notable exception is AlgaeDICE (Nachum et al., 2019b), which
implicitly uses DICE to regularize the discounted stationary distribution induced by the target policy
to be kept inside of the data support, similar to this work.
On the other hand, Yu et al. (2020) argued that the model-based approach can be advantageous
due to its ability to generalize predictions on the states outside of the data support. They introduce
MOPO (Yu et al., 2020), which uses truncated rollouts and penalized rewards for conservative policy
improvement. MOReL (Kidambi et al., 2020) trains a state-action novelty detector and use it to pe-
nalize rewards in the data-sparse region. Matsushima et al. (2020), MOOSE (Swazinna et al., 2020)
2
Published as a conference paper at ICLR 2021
and MBOP (Argenson & Dulac-Arnold, 2020) guide their policy optimization using the behavior
policy, similar to the behavior regularization approaches.
Note that these aforementioned offline RL methods build on the standard approximate dynamic pro-
gramming algorithm for action-value estimation (model-free) or on a maximum-likelihood environ-
ment model (model-based), without explicitly addressing the distribution shift in the estimator. In
contrast, we augment the objective for model learning to obtain a robust model under the distribution
shift, which is the first attempt for offline RL to the best of our knowledge.
3	Preliminaries
A Markov Decision Process (MDP) is specified by a tuple M = hS, A, T, R, d0 , γi, consisting
of state space S, action space A, transition function T : S × A → ∆(S), reward function
R : S × A → ∆([0, rmax]), initial state distribution d0, and discount rate γ. In this paper, we
mainly focus on continuous state space S ⊆ Rds and conduct experiments on both discrete action
spaces A = {a0, ...ana } and continuous action spaces A ⊆ Rda . Given MDP M and policy π,
which is a (stochastic) mapping from state to action, the trajectory can be generated in the form of
so, ao, ro, si, aι, ri,..., where so 〜do and for each timestep t ≥ 0, at 〜π(st), r 〜R(st, at),
and st+i 〜T(St, at). The goal of RL is to optimize or evaluate a policy, based on the normalized
expected discounted return: Rπ , (1 - γ)EM,π [Pt∞=o γtrt].
A useful and important concept throughout the paper is the discounted stationary distribution, which
represents the long-term occupancy of states:
∞
dπ (s, a) , (1 - γ) X γt Pr(st = s, at = a|M, π).
t=o
From the definition, it can be observed that Rn can be obtained by Rn = E(s,a)〜d∏ [r(s, a)].
Offline RL and off-policy evaluation In this paper, we focus on the offline RL problem where
the agent can only access a static dataset D = {(si, ai, ri, s0i)}iN=i for the maximization of Rπ. We
consider a behavior-agnostic setting where we do not have any knowledge of the data collection
process. We denote the empirical distribution of the dataset by dD .
Before improving policy, we first aim to better evaluate Rπ given a target policy π anda static dataset
D, which corresponds to an off-policy evaluation (OPE) problem. We mainly focus on a model-
based approach where the algorithm first estimates the unknown dynamics T, R using the dataset
D. This defines an approximate MDP M = hS, A, T, R, do , γi, with the approximate expected
∞
discounted return Rπ , (1 -γ)EMc π [ t=o γtrt] obtained from M. In this paper, we are interested
in the MDP estimate M that can effectively reduce the error in the evaluation of policy ∏, | Rπ - Rπ |.
In order to do so, we need to learn a good representation of a model that results in a small OPE error.
We assume a bijective representation function φ : S × A → Z where Z ⊆ Rdz is the representation
space. We define the transition and the reward models in terms of the representation function φ, i.e.
T = Tz ◦ φ and R = Rz ◦ φ. In practice, where we use a neural network for T and R, z can be
chosen to be the output of an intermediate hidden layer, making φ represented by lower layers and
Tz, Rz by the remaining upper layers. We define dπφ(z) the discounted stationary distribution on Z
induced by dπ(s, a) under the representation function z = φ(s, a), and similarly for dφD(z).
4	Representation balancing offline model-based RL
4.1	Generalization error bound for model-based off-policy evaluation
We aim to construct a model Mc from the dataset D, which can accurately evaluate the policy π ,
by minimizing a good upper bound of policy evaluation error ∣Rπ - Rπ |. We define the following
point-wise model loss for notational convenience:
Eφ,bz,^bz (s,a) = CRDTV (R(r∣s,a)∣∣Rz(r∣Φ(s,a))) + CT DTV (T (s0∣s,a) ∣ ∣TbZ (s0∣φ(s,a))),
3
Published as a conference paper at ICLR 2021
where cR = 2(1 - γ) and cT = 2γrmax. Then, we start by restating the simulation lemma (Kearns
& Singh, 2002) to bound the policy evaluation error in terms of the point-wise model loss. The proof
is available in Appendix A.
Lemma 4.1. Given an MDP M and its estimate Mc with a bijective representation function φ, i.e.
(T, R) = hTz ◦ φ, Rz ◦ φi, the policy evaluation error of a policy π can be bounded by:
Rn- Rnl ≤ E(s,a)〜dπ hEφ,Rz,Tz(* s, a)i
(1)
The Lemma 4.1 has a natural interpretation: if the model error is small in the states frequently
visited by following the policy π, the resultant policy evaluation error will also be small. However,
minimizing the RHS of Eq. (1) in the off-policy evaluation (OPE) task is generally intractable since
the distribution dπ is not directly accessible. Therefore, the common practice has been to construct
a maximum-likelihood MDP using D while ignoring the distribution shift, but its OPE performance
is not guaranteed.
Instead, we will derive a tractable upper bound on the policy evaluation error by eliminating the
direct dependence on dπ in Eq. (1). To this end, we adopt the distance metric between two distri-
butions over representations dπφ and dφD that can bound their difference in expectations, which is the
Integral Probability Metric (IPM) (MuIIer,1997):
IPMG(p, q) = SuP IEz〜p[g(z)] - Ez〜q [g(z)]∣.
g∈G
(2)
where particular choices of G make the IPM equivalent to different well-known distances of distri-
butions, e.g. total variation distance or Wasserstein distance (Sriperumbudur et al., 2009).
Theorem 4.2.	Given an MDP M and its estimate Mc with a bijective representation function φ,
i.e. (T, R) = hTz ◦ φ, Rz ◦ φi, assume that there exists a constant Bφ > 0 and a function class
G，{g : Z → R} SUCh that 京 E0 R T (φ-1(∙)) ∈ G. Then, for any policy π,
∣Rπ - Rπ∣ ≤ E(s,a)〜dD hEφ,Rz,τz(s,a)i + BφIPMG(% 编)	⑶
This theorem is an adaptation of Lemma 1 of Shalit et al. (2017) to an infinite horizon model-based
policy evaluation and can be derived by the definition of IPMG(dπφ, dφD) since it serves as an upper
bound of the difference in the expectations of any function in G. The first term in Eq. (3) corresponds
to the fitness to the data following dD, while the second term serves as a regularizer. To see this,
minimizing the second term would yield a near-constant representation function, which would be
bad for the first term since it cannot distinguish states and actions well enough. It shows a natural
trade-off between optimizing the model that fits data better and learning the representation that is
invariant with respect to dπφ and dφD .
Nevertheless, RHS of Eq. (3) still cannot be evaluated naively due to its dependence on dπ in esti-
mating the IPM. We address this challenge via a change of variable, which is known as a DualDICE
trick (Nachum et al., 2019a). Define ν : Z → R as an arbitrary function of state-action pairs that
satisfies:
ν(φ(s, a)) , g(φ(s, a)) + YEs0〜T(s,a) [ν(φ(s0, a'))], ∀(s, a) ∈ S × A.
a0〜π(s0) L	」
Then we can rewrite the IPM as:
IPMG(d∏, dD) = sup ∣E(s,a)〜d∏ [g(φ(s, a))] - E(s,a)〜dD [g(φ(s, a))] ∣
suP	(I-Y)E S〜do	[ν (φ(s,a))	- E(s,a,s0)〜dD	ν(φ(s,a))	-YV(φ(s0,a0))]
ν∈F	a 〜π(S)	a0 〜π(s0)
(4)
where F
ν : ν(z) = ET,π
∞
Ytg(φ(st, at))
t=0
(s0 , a0 ) = φ-1(z )
,g∈G .
In other words, we are now taking a supremum over the new function class F, which captures a
function that returns the expected discounted sum of g(φ(st, at)) following the policy π in an MDP
4
Published as a conference paper at ICLR 2021
M given an initial representation z. While it is now difficult to choose F from G, Eq. (3) still can
be kept valid by using a sufficiently rich function class for F .
In this work, we choose F to be the family of functions in the unit ball in a reproducing kernel
Hilbert space (RKHS) Hk with the kernel k, which allows the following closed-form formula (see
Lemma A.3 in Appendix for details):
IPMG (dφ, dD )2 = ES0〜d0,a0〜∏(so ),(s,a,s0)〜dD,a0〜π(s0)	⑸
的〜do,丽〜π(g0 ),(s,a,s')〜dD ,a0^π(s0) L
k(φ(s, a), φ(s, α)) + (1 - γ)2k(φ(so, ao), φ(So,丽))+ γ2k(φ(s0, a0), φ(S, a0))
-2(1 - γ)k(φ(so, ao),φ(s, a)) - 2γk(φ(s0, a0), φ(s, a)) + 2γ(1 - γ)k(φ(s0, a0), φ(so, a。)).
This completes our derivation for the tractable upper bound of policy evaluation error (Eq. (3)),
whose direct dependence on dπ is eliminated by Eq. (5). Finally, we can train a model by minimizing
the upper bound that encourages us to learn balanced representation while improving data fitness,
where the trained model can readily provide a model-based OPE.
The IPMG (dπφ, dφD)2 in Eq. (5) can be estimated via finite random samples, and we denote its
sampled-based estimator as IdPM(dπφ, dφD)2 . We show in the following that, a valid upper bound
can be established based on the sample-based estimators instead of the exact terms in the RHS of
Eq. (3) under certain conditions.
Theorem 4.3.	Given an MDP M, its estimate Mc with a bijective representation func-
tion φ, i.e. (Tb, Rb) =	hTbz ◦ φ, Rbz	◦ φi, and an	RKHS	Hk ⊂	(Z → R)	in-
duced by a universal kernel	k such that	supz∈z k(z,z)	= k,	assume that fD r t (Z)	=
ET,π hPt∞=0γtEφ,Rbz,Tbz(st, at)(s0,a0) = φ-1(z)i ∈ Hk with Bφ = kfφ,Rbz,Tbz kHk and the loss
is bounded by Ek = sups∈S,a∈A ED Rb Tb (s, a). Let n be the number of data in D. With probability
1 - 2δ,	, z, z
∣Rπ-R[ ≤ n X ED,Rz,Tz(s,a)+BDipM(dD,dD)+∖∕2n log δ+BDʌ/1 (4+d8 log δ)∙
(s,a)∈D
This result can be proved by adapting the convergence results of the empirical estimate of the
MMD (Gretton et al., 2012) and Hoeffding’s inequality (Hoeffding, 1963). With the choice of
an RKHS Hk, we can now interpret BD as the RKHS norm kfD,Rb ,Tb kHk, which captures the mag-
nitude and the smoothness of the expected cumulative model loss f Rb Tb . In general, assuming
D,Rz ,Tz
smooth underlying dynamics, we can expect BD to be small when the model error is small. Further-
more, although k depends on the kernel function We use, We can always let k = 1 and subsume it
into BD as long as it is bounded, i.e. using BD，BD Vk. In the next section, we develop algorithms
based on practical approximations of Eq. (3).
Detailed comparison to RepBM As previously stated, RepBM (Liu et al., 2018b) is a model-
based finite-horizon OPE algorithm that trains the model to have balanced representation φ, which
is encouraged to be invariant under the target and behavior policies. Specifically, given the deter-
ministic target policy ∏ and the behavior policy μ, at each timestep t, it defines the factual distri-
bution on Z given that the actions until timestep t have been executed according to the policy π:
PFt(Z) = Pr(zt∣M,μ,ao/ = n(s0：t)) and the CoUnterfactUal distribution on Z given the same
condition except the action at timestep t: PCF(Z) = Pr(zt∣M, μ, ao：t—i = n(so：t—i), at = ∏(st)).
Then, RepBM bounds the OPE error as,1
∞
∣Rπ-R[ ≤ (1-γ) X Yt (Ep(StM,“,ao：t=n(s0：t))[ED,Rz,Tz(st,n(st))] + BD,tIPMGt (pF,t,PCF)) ∙
t=0
Although RepBM achieves performance improvement over other OPE algorithms, we found a num-
ber of practical challenges: from the definition of the IPMGt (PDF,t,PDC,Ft ), it requires a discrete-action
1We adapted their formulation to the infinite horizon discounted MDP setting.
5
Published as a conference paper at ICLR 2021
environment and a deterministic policy π, which cannot be met by many practical RL settings. In
addition, since the sample-based estimation of IPMGt (pφF,t,pφC,Ft ) requires samples consistent with
the policy ∏, i.e. ao：t-i = n(so:t-i), the algorithm would reject exponentially many samples with
respect to t, which is the curse of horizon (Liu et al., 2018a). When there is a large difference be-
tween the behavior and the target policies in long-term tasks, their implementation becomes close to
using the maximum likelihood objective, which can also be observed empirically in our experiments.
In contrast, our work is free from the abovementioned practical limitations by performing balancing
πD
between the discounted stationary distribution d and the data distribution d , leveraging the recent
advances in stationary distribution correction estimation (i.e. the DualDICE trick) to overcome the
difficulties pertinent to the expectation concerning dπ required to evaluate the IPM in the objective.
4.2 Representation Balancing with S tationary Distribution Estimation
In the following, we describe algorithms for OPE and offline RL based on the practical approxima-
tions to Eq. (3), which we call the RepB-SDE framework.
Objective for off-policy evaluation As we mentioned earlier, we aim to minimize the upper
bound of OPE error ∣Rπ - Rbπ | specified in Theorem 4.2. To make the RHS of Eq. (3) tractable for
optimization, we replace the intractable total variation distance with KL-divergence, which can be
easily minimized by maximizing the data log-likelihood. We also replace the IPM with its sample-
based estimator to obtain the learning objective:
min 1 X h — log Rz(r∣φ(s,a))	Tog Tz (s0∣φ(s, a)) ] + °m IPM(dφ, dD) (6)
φ,Rz,Tz n (S…∈D L '----------------------}	'------------------} J
DκL(R(r∣s,a)∣∣Rz(r∣Φ(s,a))) DKL(T(s0∣s,a)∣∣Tz (s0∣φ(s,a)))
The constant Bφ in Theorem 4.2 depends on the function classes and cannot be estimated, and thus,
we replace it with a tunable hyperparameter αM that balances between data fitness and representa-
tion invariance. Remark that αM = 0 recovers the simple maximum-likelihood objective.
By simulating the target policy π under the environment model (T , R) obtained by minimizing
Eq. (6), it is possible to perform a model-based OPE that approximately minimizes the upper bound
of OPE error.
Objectives for offline model-based RL By rearranging Eq. (3), we have,
Rn ≥ Rn - E(s,a)〜D [Eφ,Rz,τz(s,a)i - BφIPMG(d∏,蝠).	⑺
Then, we can maximize the RHS of Eq. (7) to get the model and the policy that maximizes the lower
bound of true return Rπ . Similar to the derivation of Eq. (6), we replace the total variation distance
by KL-divergence to obtain the following learning objectives:
LM(M,∏,αM) = EdDh-log Rz(r∣Φ(s,a)) - log Tz (s0∣Φ(s,a))] + OmIPMg (d∏, dD), (8)
∞
Jπ(π, Mc, απ) = EMc,π X γtrt - απIPMG(dπφ, dφD).	(9)
t=0
where the expectation in Eq. (9) can be optimized using various model-based RL algorithms, e.g.
with planning (Chua et al., 2018) or using a model-free learner (Janner et al., 2019). By iterating
between the minimization of LMc with respect to Mc and the maximization of Jπ with respect to π
by stochastic gradient method, it is possible to perform offline model-based RL that approximately
maximizes the lower bound of the true return Rπ .
Implementation details Following the recent practice (Chua et al., 2018; Janner et al., 2019; Yu
et al., 2020), we model the dynamics (T, R) using a bootstrap ensemble of neural networks. To op-
timize a policy based on the objective, we perform full rollouts (until reaching the terminal states or
maximum timesteps) using learned dynamics (T, R). During obtaining the rollouts, we pessimisti-
cally augment the estimated reward function using the penalty proportional to the bootstrapped
uncertainty, which helped the algorithm to perform robustly. We suspect the difficulty in calculating
6
Published as a conference paper at ICLR 2021
S	CartPoIe-VO
HIV simulator
ro
n
P
-————工—® — fK—分一 ∙≡-—— — — —
1.0：
0.5-
Acrobot-vl
H
-
-S
-
1.0
0.5-
0.1	0.3	0.5	0.7	0.9
Behavior off-poIicyness (ε)
@ RepBM @ RepB-SDE (ours)
-O- - - O _
Tl①—一 一。
S
-N .
∈ 0.5-
g θ'l 03
05 θ'7 θ'9
Behavior off-poIicyness (ɛ)
0.1	0.3	0.5	0.7	0.9
Behavior off-policyness (ε)
----Baseline

Figure 1: The OPE results of different model learning algorithms with varying off-policyness on the
x-axis. The y-axis plots the normalized individual MSE on test trajectories where the performance of
the baseline model is set to 1. The tasks used are infinite-horizon discounted environments (γ = 0.98
(HIV), γ = 0.99 (others)), where we truncated att = 1000. The experiments are repeated 200 times,
and the error bars indicate 95% confidence intervals.
accurate IPM estimation is what makes the additional pessimism beneficial. We store the generated
experiences to a separate dataset Db and update the policy
π with IPM-regularized soft actor-critic
(SAC) (Haarnoja et al., 2018) using samples from both datasets D ∪ D similar to MBPO (Janner
et al., 2019).
Since the presented model objective requires a policy π to perform a balancing, we initially
trained the model and the policy using αM = α∏ = 0: by Mo = arg mine LM(M) ∙, 0) and
π0 = arg minπ Lπ (π, M0, 0). Then, we retrained the model and the policy using π0: M1 =
arg minMc LMc(M, π0, αM) and π1 = arg minπ Lπ(π, M1, απ) for some non-negative αM and απ.
While it is desirable to repeat the optimization of the model and the policy until convergence, we
did not observe significant improvement after the first iteration and reported the performance of the
policy after the first iteration, π1.
5	Experiments
We demonstrate the effectiveness of the RepB-SDE framework, by comparing the OPE performance
of Eq. (6) to that of RepBM and evaluating the presented model-based offline RL algorithm on the
benchmarks. The code used to produce the results is available online.2 A detailed description of the
experiments can be found in Appendix B.
5.1	Model-based off-policy evaluation
For the sake of comparison with RepBM, we test our OPE algorithm on three continuous-state
discrete-action tasks where the goal is to evaluate a deterministic target policy. We trained a sub-
optimal deterministic target policy π and used an -greedy policy with various values of as the
data collection policy. In each experiment, we trained environment models for a fixed number of
epochs concerning three different objectives: simple maximum-likelihood baseline, step-wise rep-
resentation balancing objective used in RepBM (Liu et al., 2018b), and the presented OPE objective
of RepB-SDE (Eq. (6)). We measured the individual mean squared error (Liu et al., 2018b).
The normalized error of each algorithm, relative to the error of the baseline valued at 1, is presented
in Figure 1. We can observe that the presented objective of RepB-SDE can reduce the OPE error
from the baseline significantly, outperforming RepBM in most of the cases. As the off-policyness be-
tween the policies () increases, representation balancing algorithms should more benefit compared
to the maximum-likelihood baseline in principle. However, the result shows that the performance of
RepBM merely increases due to the increased sample rejection rate under large .
2https://github.com/dlqudwns/repb-sde
7
Published as a conference paper at ICLR 2021
Table 1: Normalized scores on D4RL MuJoCo benchmark datasets (Fu et al., 2020) where the score
of 0 corresponds to a random policy and 100 corresponds to a converged SAC policy. All results
(except MF, which is taken from Fu et al. (2020) and Kumar et al. (2020)) are averaged over 5 runs,
where ± denotes the standard error. The highest scores are highlighted with boldface.
Dataset type	Environment	RePB-SDE (ours)	RP	Base	MOPO	MF	BC
Random	Walker2d	21.1 ± 1.0	18.4	16.4	1.3	7.3 BEAR	0.0
Medium	Walker2d	72.1 ± 1.9	56.3	5.5	-0.1	81.1 BRAC	7.7
Med-Replay	Walker2d	49.8 ± 11.4	41.1	6.2	47.8	26.7 CQL	8.0
Med-Expert	Walker2d	88.8 ± 6.9	72.6	51.7	32.4	98.7 CQL	3.3
Random	Hopper	8.6± 1.0	8.3	8.3	9.1	12.2 BRAC	9.0
Medium	Hopper	34.0 ± 2.8	27.5	19.8	19.2	58.0 CQL	34.5
Med-Replay	Hopper	62.2 ± 6.7	49.8	32.9	80.8	48.6 CQL	13.2
Med-Expert	Hopper	82.6 ± 7.0	74.0	19.1	23.2	111.0 CQL	41.9
Random	HalfCheetah	32.9 ± 1.1	31.3	26.1	29.9	35.4 CQL	-2.4
Medium	HalfCheetah	49.1 ± 0.3	47.3	22.1	45.0	46.3 BRAC	31.0
Med-Replay	HalfCheetah	57.5 ± 0.8	53.6	54.0	53.8	47.7 BRAC	14.7
Med-Expert	HalfCheetah	55.4 ± 8.3	36.7	31.9	91.0	64.7 BCQ	29.5
5.2	Offline reinforcement learning with representation balanced model
We evaluate the offline model-based RL algorithm presented in Section 4.2 on a subset of datasets
in the D4RL benchmark (Fu et al., 2020): using four types of datasets (Random, Medium, Medium-
Replay, and Medium-Expert) from three different MuJoCo environments (HalfCheetah-v2, Hopper-
v2, and Walker2d-v2) (Todorov et al., 2012). Random dataset contains 106 experience tuples from
a random policy. Medium dataset contains 106 experience tuples from a policy trained to approx-
imately 1/3 the performance of the expert, which is an agent trained to completion with SAC.
Med-Replay dataset contains 105 (2 × 105 for Walker2d-v2) experience tuples, which are from the
replay buffer of a policy trained up to the performance of the medium agent. Med-Expert dataset
is a Medium dataset combined with 106 samples from the expert. This experimental setting exactly
follows that of (Yu et al., 2020; Argenson & Dulac-Arnold, 2020).
The normalized score of each algorithm is presented in Table 1. MF denotes the best score from
offline model-free algorithms (taken from Fu et al. (2020) and Kumar et al. (2020)), including
SAC (Haarnoja et al., 2018), BCQ (Fujimoto et al., 2019), BEAR (Kumar et al., 2019), BRAC (Wu
et al., 2019), AWR (Peng et al., 2019), cREM (Agarwal et al., 2020), AlgaeDICE (Nachum et al.,
2019b), and CQL (Kumar et al., 2020). The actual algorithm that achieves the reported score is
presented next to the numbers. Base shows the performance of the most naive baseline, which at-
tempts to maximize the estimated policy return under the maximum-likelihood model. RP denotes
the performance of Base equipped with the appropriate reward penalty using the bootstrapped un-
certainty of the model, which is equivalent to π0 described in Section 4.2. RepB-SDE denotes the
performance after a single iteration of our algorithm, corresponding to π1. We also provide BC,
the performance of direct behavior cloning from the data, and MOPO (Yu et al., 2020), an offline
model-based RL algorithm that optimizes policy based on truncated rollouts with the heuristic re-
ward penalty.
The significant gap between RepB-SDE and RP in the results shows the advantage brought by our
framework that encourages balanced representation. While our approach was less successful on
some of the datasets (mostly on the Hopper-v2 environment), we hypothesize that the conservative
training techniques: the behavior regularization approaches exploited in the model-free algorithms,
the rollout truncation technique in MOPO, and the pessimistic training based on the bootstrapped
uncertainty estimates adopted in our algorithm exhibit their strengths in different datasets. For ex-
ample, it may be the case that the ensemble models are overconfident especially in Hopper-v2, and
should be regularized with more explicit methods. Nevertheless, we emphasize that the presented
framework can be used jointly with any other conservative training technique to improve their per-
formance.
8
Published as a conference paper at ICLR 2021
6	Conclusion and Future Work
In this paper, we presented RepB-SDE, a framework for balancing the model representation with sta-
tionary distribution estimation, aiming at obtaining a model robust to the distribution shift that arises
in off-policy and offline RL. We started from the theoretical observation that the model-based policy
evaluation error can be upper-bounded by the data fitness and the distance between two distributions
in the representation space. Motivated by the bound, we presented a model learning objective for
off-policy evaluation and model-based offline policy optimization. RepB-SDE can be seen as an
extension of RepBM, which addresses the curse of horizon by leveraging the recent advances in
stationary distribution correction estimation (i.e. the DualDICE trick). Using stationary distribution
also frees us from other limitations of RepBM to be applied to more practical settings. To the best of
our knowledge, it is the first attempt to introduce an augmented objective for the learning of model
robust to a specific distribution shift in offline RL.
In the experiments, we empirically demonstrated that we can significantly reduce the OPE error
from the baseline, outperforming RepBM in most cases. We also showed that the robust model also
helps in the offline model-based policy optimization, yielding the state-of-the-art performance in a
representative set of D4RL benchmarks. We emphasize that our approach can be directly adopted in
many other model-based offline RL algorithms.
There are a number of promising directions for future work. Most importantly, we have not leveraged
the learned representation in the policy when optimizing the policy, yet it is very natural to do so.
We can easily incorporate the representation into the policy by assuming energy-based models, but
this would make the computation of entropy intractable in entropy-regularized policy optimization
algorithms. It would be also interesting to see if the proposed framework for learning balanced
representation can benefit off-policy (and offline) model-free methods.
Acknowledgments
This work was supported by the National Research Foundation (NRF) of Korea (NRF-
2019M3F2A1072238 and NRF-2019R1A2C1087634), and the Ministry of Science and Information
communication Technology (MSIT) of Korea (IITP No. 2019-0-00075, IITP No. 2020-0-00940 and
IITP No. 2017-0-01779 XAI).
References
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline
reinforcement learning. In International Conference on Machine Learning, 2020.
Arthur Argenson and Gabriel Dulac-Arnold. Model-based offline planning. arXiv preprint
arXiv:2008.05556, 2020.
Peter L Bartlett and Shahar Mendelson. Rademacher and Gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations
for domain adaptation. In Advances in Neural Information Processing Systems, 2007.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. OpenAI Gym. arXiv preprint arXiv:1606.01540, 2016.
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learn-
ing in a handful of trials using probabilistic dynamics models. In Advances in Neural Information
Processing Systems, 2018.
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: Datasets for deep
data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, 2019.
9
Published as a conference paper at ICLR 2021
Carles Gelada and Marc G Bellemare. Off-policy deep reinforcement learning by bootstrapping the
covariate shift. In AAAI Conference on Artificial Intelligence, 2019.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola.
A kernel two-sample test. Journal ofMachine Learning Research,13(Mar):723-773, 2012.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Confer-
ence on Machine Learning, 2018.
Assaf Hallak and Shie Mannor. Consistent on-line off-policy evaluation. In International Confer-
ence on Machine Learning, 2017.
Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the
American Statistical Association, 58(301):13-30, 1963.
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-
based policy optimization. In Advances in Neural Information Processing Systems, 2019.
Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning. In
International Conference on Machine Learning, 2016.
Fredrik D Johansson, Nathan Kallus, Uri Shalit, and David Sontag. Learning weighted representa-
tions for generalization across designs. arXiv preprint arXiv:1802.08598, 2018.
Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Ma-
chine learning, 49(2-3):209-232, 2002.
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. MOReL: Model-
based offline reinforcement learning. In Advances in Neural Information Processing Systems,
2020.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy
q-learning via bootstrapping error reduction. In Advances in Neural Information Processing Sys-
tems, 2019.
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative Q-learning for offline
reinforcement learning. In Advances in Neural Information Processing Systems, 2020.
Byung-Jun Lee, Jongmin Lee, Peter Vrancx, Dongho Kim, and Kee-Eung Kim. Batch reinforcement
learning with hyperparameter gradients. In International Conference on Machine Learning, 2020.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Inter-
national Conference on Learning Representations, 2016.
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: Infinite-
horizon off-policy estimation. In Advances in Neural Information Processing Systems, 2018a.
Yao Liu, Omer Gottesman, Aniruddh Raghu, Matthieu Komorowski, Aldo A Faisal, Finale Doshi-
Velez, and Emma Brunskill. Representation balancing MDPs for off-policy policy evaluation. In
Advances in Neural Information Processing Systems, 2018b.
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Off-policy policy gradient with
state distribution correction. arXiv preprint arXiv:1904.08473, 2019.
Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Ofir Nachum, and Shixiang Gu. Deployment-
efficient reinforcement learning via model-based offline optimization. arXiv preprint
arXiv:2006.03647, 2020.
10
Published as a conference paper at ICLR 2021
ColinMcDiarmid. On the method of bounded differences. Surveys in combinatorics, 141(1):148-
188, 1989.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
Ali Mousavi, Lihong Li, Qiang Liu, and Denny Zhou. Black-box off-policy estimation for infinite-
horizon reinforcement learning. In International Conference on Learning Representations, 2020.
Alfred Muller. Integral probability metrics and their generating classes of functions. Advances in
Applied Probability, 29(2):429-443, 1997.
Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. DualDICE: Behavior-agnostic estimation
of discounted stationary distribution corrections. In Advances in Neural Information Processing
Systems, 2019a.
Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. AlgaeDICE:
Policy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019b.
Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:
Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.
Doina Precup. Eligibility traces for off-policy policy evaluation. Computer Science Department
Faculty Publication Series, pp. 80, 2000.
Prajit Ramachandran, Barret Zoph, and Quoc V Le. Swish: a self-gated activation function. arXiv
preprint arXiv:1710.05941, 7, 2017.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Uri Shalit, Fredrik D Johansson, and David Sontag. Estimating individual treatment effect: gener-
alization bounds and algorithms. In International Conference on Machine Learning, 2017.
Bharath K Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Scholkopf, and Gert RG
Lanckriet. On integral probability metrics, φ-divergences and binary classification. arXiv preprint
arXiv:0901.2698, 2009.
Richard S Sutton, Andrew G Barto, et al. Introduction to reinforcement learning, volume 135. MIT
press Cambridge, 1998.
Richard S Sutton, Hamid R Maei, and Csaba Szepesvari. A convergent o(n) temporal-difference
algorithm for off-policy learning with linear function approximation. In Advances in neural in-
formation processing systems, 2009.
Phillip Swazinna, Steffen Udluft, and Thomas Runkler. Overcoming model bias for robust offline
deep reinforcement learning. arXiv preprint arXiv:2008.05533, 2020.
Ziyang Tang, Yihao Feng, Lihong Li, Dengyong Zhou, and Qiang Liu. Doubly robust bias reduction
in infinite horizon off-policy estimation. In International Conference on Learning Representa-
tions, 2020.
Philip Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement
learning. In International Conference on Machine Learning, 2016.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In International Conference on Intelligent Robots and Systems, 2012.
Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.
arXiv preprint arXiv:1911.11361, 2019.
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea Finn,
and Tengyu Ma. MOPO: Model-based offline policy optimization. In Advances in Neural Infor-
mation Processing Systems, 2020.
11
Published as a conference paper at ICLR 2021
Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations.
In International Conference on Machine Learning, 2013.
Ruiyi Zhang, Bo Dai, Lihong Li, and Dale Schuurmans. GenDICE: Generalized offline estimation
of stationary values. In International Conference on Learning Representations, 2020.
12
Published as a conference paper at ICLR 2021
A Proofs
Lemma 4.1. Given an MDP M and its estimate M with a bijective representation function φ, i.e.
(T, R) = hTz 0 φ, RZ 0 φ, the policy evaluation error ofa policy π can be bounded by:
Rn- R∏ I ≤ E(S,α)~dπ [Eφ,Rz,Tz(s, a)i
(1)
Proof. We define a value function Vπ (s) = Em,∏ [P∞=0 YtrtIS0 = s], the expected discounted
return starting from the state s, for the following proof. Due to its definition, We can write Rπ =
(1 - Y)ES〜a。[Vπ (s)]. The following recursive equation also holds:
Vπ (s) = Ea〜π(s) [r(s,α) + γEs0〜T(s,a)[V(s0)]],
and similarly with an approximate value function Vπ (s) = Ec ∏ [P∞=0 Ytrt ∣ s0 = s] given an MDP
estimate M. Then,
占 I Rn - Rπ ∣= Ed0 [vπ(S0)
-Vπ(S0)
π
(sι)T(s1∣S0,α0) - V (s1)T(s1∣s0,ɑ0) π(α0∣s0)ds1da0
, .一 , ,	^ _ , 一 , ,
(S1 )T(S1∣S0,α0) - V (sι)T(s1∣s0,α0)


+ Vπ(sι)T(sι∣S0, a0)- Vπ(sι)T(s1∣s0, ao) )π(a0∣s0)ds1da0
si) - Vπ(sι)) T(s1 ∣s0, ɑ0)π(α0∣s0)ds1dα0
一■――	,
+ YEd0
^^^β{^^^^^^^^~*r
This forms a recursive equation
s1∣s0, ɑ0) - T(s1∣s0, a。))π(α0∣s0)ds1dα0
^R(r∣s, a) — R(r∣s, a))dr + Y J Vπ(s0)(T(s0∣s, a) — T(s0∣s, a))ds0
R(r∣s, a) — R(r∣s, a) ∣ dr + Y J Vπ(s0) ∣ T(s0∣s, a) — T(s0∣s, a) ∣ ds0
≤ E(s,α)〜d∏ ]/I R(r∣s, a) - ]b(r∣s, a) ddr + Y / Im-Y ∣T(s0∣s,α)- T(SlS,α)卜s0
=2E(S,α)〜dπ [Dtv (R(r∣s,α)∣∣Rz(r∣φ(s,α)))]
+ ⅞-m^E(s,。)〜d∏ [Dtv(T(s0∣s,a)∣∣TZ(s0∣φ(s,a)))]
□
Theorem 4.2. Given an MDP M and its estimate M with a bijective representation function φ,
i.e. (T, R) = hTz 0 φ, RZ 0 φ, assume that there exists a constant Bφ > 0 and a function class
G , {g : ZT R} such that 卷E@ R T (φ-1(∙)) ∈ G. Then, for any policy π,
∣Rπ - Rπ I ≤ E(s,a)〜dD [Eφ,Rz,Tz(S,a)i + BφIPMg宿,蝠)
(3)
Proof. From Lemma 4.1, we directly have:
IRn- RnI ≤ E(S,α)〜dD [Eφ,Rz,Tz (S, a)
Eφ b T (s, a) (dπ(s, a) — dD(s, a)) dsda.
13
Published as a conference paper at ICLR 2021
Then,
/Eφ R Tb (s, a) (dπ(s, a) — dD(s, a)) dsda
=/ Eφ,Rbz,Tz (Φ-1 (z))(dπ(z)- dD (φT(φT(z))))∣d⅛^dz
=/ εφ,Rz,∙fz (φ-1(Z)) (dφ(Z)- dD (Z)) dz
≤ Bφ∖f Bl Eφ,Rz,fz(φT(zY)鸣⑶-dD(z)) dz	(BD >。)
≤ BDsupiʃgiz) (d∏t(Z)- dD(Z)) dz	(BED,Rz,fz(OT(Z)) ∈G)
=BDIPMg (d∏ ,dD)
□
We state some previous results, which are required for further proof.
Theorem A.1 (McDiarmid’s inequality (McDiarmid, 1989)). Let {Xi}in=1 be independent random
variables taking values in set X, and assume that f : Xn → R satisfies
sup	|f ({xi}n=l) — f(xi, ∙∙∙,Xi-1,X,Xi+1, ∙∙∙,Xn)| ≤ Ci∙
{Xi}n=1 ∈X n,X∈X
Then for every > 0,
Pr{f({Xi}in=1)-E{Xi}in=1[f({Xi}in=1)] ≥} ≤exp
(-P2⅛).
(10)
(11)
Lemma A.2 (Rademacher complexity of RKHS (Bartlett & Mendelson, 2002)). Let F be a unit ball
in a universal RKHS on the compact domain X, with kernel bounded according to 0 ≤ k(x, x0) ≤ k.
Let {xi}in=1 be an i.i.d. sample of size n drawn according to a probability measure p on X, and let
σi be i.i.d. and take values in {-1, 1} with equal probability. The Rademacher complexity, which is
defined as below, is upper bounded as:
Rn (F) , E{xi}in 1,σ sup
f∈F
1n
-Eσif (Xi)
n i=1
(12)
The upper bound is followed by Lemma 22 of (Bartlett & Mendelson, 2002).
Now we prove the following using the results above.
Lemma A.3. Let Hk be a RKHS associated with universal kernel k(∙, ∙). Let(•，)h be the inner
product of Hk, which satisfies the reproducing property V(Z) = (ν, k(∙, ZyiHk. When G is chosen
such that
G = g9 ∈ IZ → R) : g(Z) = V(Z)- YE30~f(φ-i(z))[ν(φ(s0,aO))], ν ∈ (Z → R), hν,νiHk ≤ 1 1,
1	a0~π(s0)	J
the IPMG (dπD, dDD) has the following closed form definition:
IPMG(dD, dD )2 = Eso~do ,a0~∏(s0),(s,a,s0)~dD,a0~∏(s0)
S0~d0 ,a0^π(s0),(s,α,s0)^dD,a0~π(30) L
k(φ(s, a), φ(s, α)) + (1 - γ)2k(φ(s0, ao), φ(So, ao)) + γ2k(φ(s0, a0), φ(S, a0))
-2(1 - Y)k(φ(so, ao), φ(s, ɑ)) - 2γk(φ(s0, a0), φ(s, α)) + 2γ(1 - γ)k(φ(s0, a0),φ(s0,丽)).
Furthermore, suppose that k，sup%∈z k(Z, z). The estimator IPM(dD, dD)2, which is the sample-
based estimation ofIPMG(dπD, dDD)2from n samples, satisfies with probability at least 1 - δ,
IPMG(dπD,dDD)-IdPM(dπD,dDD) ≤
14
-
∙(⅞.<0)M.l1e(福
U
I=IH2
.(⅞一⅝vMf :s2 . (⅝⅝)VM> "s<<Ost
- -4
一S6J+SZJ+S<dns U (筲号&3
Wd əə JqIUl Popwp
Oq ueə qqM CmnUIOJdnS £ uouyωp jəoue SjəAOɔəj uEɔ əM CJəpjo əsjəAəj uAωp A
((⅝SO⅜F0 -e's)0)n；I)S + (S:¾0 -?: 2S) 0 MZl
(svss)0 -(⅝-⅝)0)-⅛(f~l ɪ)z — ((S飞-Si 一£飞-≡'s)0)空L +
((⅝soS) 0 -(⅝⅝)0)靖(；I) + ((S0 -ss)0 -(≡二⅛0wmm "ZGP⅛)同I
G一号信I
eeuɪpsə pəSeq,əɪduws əs əugəp əm .>(≡'s)k ?£«%? (≡∖(⅞~(⅛~(⅝)k
〜(JCOP 〜(JS ssəɔɑɪd əaejəuəql uiojj I(()∕c()％ -Sd C(P)Se(JC(JS) SOIdmBS S s>δ
. 一((飞S) 0)、上∕αFW)国 H (A)S
C-(Ges) 0ɑp0s 国— U (A)ZJ(G 6)0w〜JOP〜(/L — I) U (4)IJ əjm
κw4
一s6J+sZJ+s≤dns U (^⅛)⅜vdl
工 Wd əəJqI£P)dl Dpwp əM Cm.joieuipsə əip JO PUnOq Jojjə ətp OAOJd əM MON
3^*∖ J-Ulay EUnn9 əip∙su∙2SSOJdXə UnOJ pəs-ɔ ətp əa-əp UEɔ əM
《(Z)7uH〈(吗•)+(*•)-qlJdoJd ωqls∩ AHrAJ-H-〈*、C-*dns pup spq
*x(w*3VlJN 工 XNWVlJWS
AA CIκ∕7 CpʊŋWbOE Wqɔɔnpoql o-ənɑ
(7)仁〜飞 ()ɑ〜
∙-(('5S) 0，)器— («S) 0，)冬 αlxQS)国— 一(«S) 0，)冬。了£(； I) = (.)*4
oJoqΛ∖
κw4
JM WdnS
-FX〈((飞S0，)应Λ; *x((e6)0 W * W 一 ɑɪ`sws 团 I
;
(S〜G )4
F((e Cs) 0 C∙)7C4OP〜S,国(L — I) ▼ dɪɪs
(7)仁〜飞 G)仁〜
一((飞<)0)∕7f-l (e6)0AαpΛxQS)国 I 一((0 .s)0A。了 SaI?— I)
一 hA
》dns
LS-JYsw f
-(('0<)0)4f~l(e~s)0)∕7lαpΛ'sesal-(e6)0)∕7op2saf~II) dns
∙iH jo X=SdOJd *。npo.ɪdəj
Sw 皆 Sn ∙WH SHxa.sΠBq.∙sun e WlVl旦4 W 二国 TN)出 ± uκSaUISaq EaqKPuB
(%).Ir 〜(S∙)ji 〜A
二((飞S)0)/71 (G 6)0WaPw)国—一((")0w。了£(： I) dnf VdI
H 葭MdI
-E Uu-PVdIoqlo=JM°J
unɔ əM^s∙bw£ SV •-P 捻P)sdl souəp2sdl CPUEWoqS∙sə-jm əM M-ωq ətp Ul .Joald
IZOZ xuOIaJOdEd əɔuəjəjus E SE poqs=qnd
Published as a conference paper at ICLR 2021
We can bound the error of sample-based estimator with individual errors as:
IPMG(dφ,dφ ) - IPM(dφ,dD)
泮历(V) + W) + f3(V )1 -泮 fG) + ∙MV)+ fbS
≤ sup
ν∈F
IIfI(V) + f2 (V) + f3 (V )∣ - f1(V) + f2 (V) + f3 (V )||
≤ suP I f1 (V) + f2 (V) + f3(V) - f1(V) - f2 (V) - f3 (V) |
ν∈F I	I
≤ sup 口 f1(V) - f1(V)∣ + 1 f2(v)-金(V)∣ + 1 f3(V) - f3(v) 1 i
≤ sup 1 f1(ν) - f1(ν) + sup 1 f2(ν) - f2(ν)1 + sup | f3(ν) - f3(ν)
ν∈F I	I ν∈F I	I ν∈F I
We then observe that
sup 1 fι(ν) - f1(ν) = (1 - γ)( ES〜d°,α〜π(s)[k(φ(s, a),φ(3 α))]
ν∈F
S〜 d0,无〜 π(s)
n n
-n X Ei0,◎〜π(s) Ik (φ 卜(i),a(i)) ,φ(5,a))]
1/2
which shows that changing s(i), a⑶ results in changes of sup^∈F ∣ fι(ν) - fι(ν) ∣ in magnitude
of at most 2(1 - Y泳 1/2/n where k = sup%∈z k(z, z). Therefore, by McDiarmid,s inequality
(Theorem A.1),
Pr {泮1 f1 (V)-
五(V) 1 - Es(i), a(i)
≡uFl f1(V)- f1 (V)
2
ne
≥ J ≤exp(-2(1-Y)2kb
Also,
ES(i),α(i) sup l fI(V) - fI (v )
1-γ
n	ES(i),α(i)
sup EEi) α(i)
ν∈F
1-γ
≤	ES(i) .a(i) .s(i) .a(i)
n ,	,	,
1-γ
n ES(i) ,a(i) ,s(i) ,a(i ,σ(i)
≤ 2(1 - γ)
sup
ν∈F
sup
ν∈F
n
XZ {ν(φ^,kQ)-ν((40i),aQ)}
i=1
where the last inequality is from Lemma A.2. Combining the results, we get
Pr {sup ∣ fι(ν) - f1(ν) l - 2(1 - Y)
exp I-刃fk 卜
2
ne2
Similarly, We derive bounds for f2 and f3 respectively:
Pr
卜 F ∣ f2(ν)- f2(ν)
≤ exp
Pr
s sup ∣ f3(ν)-
[ν∈F I
f3(ν)∣-
exp
(ne2
I-/
16
Published as a conference paper at ICLR 2021
By letting RHS of above bounds tobe δ∕3 and using union bound, We get, with probability 1-δ, We
get	______
IPMG(dφ,dD) - IPM(dφ,dD) ≤ ʌ/j (4+ʌʌlog3) .	(13)
□
The relationship betWeen F and G
G = [g ∈ (Z → R) : g(Z)= V(Z)- YEsO〜T(φ-1(z)) [ν(φ(s0, aO))],ν ∈ (Z → R), hν, ViHk ≤ 1
1	a0 〜π(s0)
show that when the conditional expectation Eso〜T库-乂.))®〜∏(so) [ν(φ(s0, a0))] : Z → R is a func-
tion in RKHS Hk, G also becomes a subset ofHk.
Then we can prove the following Theorem.
Theorem 4.3. Given an MDP M, its estimate Mc with a bijective representation func-
tion φ, i.e. (Tb, Rb)	= hTbz ◦ φ, Rbz	◦ φi, and an	RKHS	Hk ⊂	(Z → R)	in-
duced by a universal	kernel k such that	suPz∈z k(z,z)	= k,	assume that fφ r t (Z)	=
ET,π	Pt∞=0γtEφ,Rbz,Tbz(st, at)(s0, a0)	= φ-1(Z)	∈	Hk	with	Bφ	=	kfφ,Rbz,Tbz kHk	and the loss
is bounded by Ek = sups∈S,a∈A Eφ Rb Tb (s, a). Letn be the number of data in D. With probability
1 - 2δ,	, z, z
IRn-RnI ≤ n X 分寓且岱,。)+%IPM(dφ,dD)+J2n log δ+bφʌ/l (4+
(s,a)∈D
Proof. Applying the Hoeffding inequality (Hoeffding, 1963), with probability 1 - δ, we get:
L	「r ，	、IILr ，	、	Ak2"T
E(s,a)~dD [Eφ,Rz,TzGa)] ≤ n E eφ,Rz,Tz(s，a) +V2nlogδ∙
(s,a)〜D
By using an union bound with Eq. (13) and substituting terms in Eq.(3), we recover the result. □
17
Published as a conference paper at ICLR 2021
B Experiment details
B.1	Computing infrastructure
All experiments were conducted on the Google Cloud Platform. Specifically, we used compute-
optimized machines (c2-standard-4) that provide 4 vCPUs and 16 GB memory for the evaluation
experiment of Section 5.1, and we used high-memory machines (n1-highmem-4), which provide
4 vCPUs and 26GB memory, equipped with an Nvidia Tesla K80 GPU for the RL experiment of
Section 5.2.
B.2	Details of the OPE experiment
Task details We did not modify CartPole-v0 environment and Acrobot-v1 environment from the
original implementation of OpenAI Gym (Brockman et al., 2016) except for the maximum trajectory
length. We ran PPO (Schulman et al., 2017) to optimize policies to reach a certain performance level
and set them as the target policies for CartPole-v0 and Acrobot-v1. For the HIV simulator, we used
the code adapted by Liu et al. (2018b), which is originally from the implementation of RLPy3. We
modified the environment to have more randomness in the initial state (up to 10% perturbation from
the baseline initial state) and to use the reward function that gives the logarithm of original reward
values, as the original reward function scales up to 1010. We used a tree-based fitted q-iteration
algorithm implemented by Liu et al. (2018b) to optimize the target policy for the HIV simulator. All
the other details are shown in Table 2. We assume that the termination conditions of tasks are known
in prior.
Table 2: Task settings of OPE experiments.
CartPole-v0		Acrobot-v1	HIV simulator
state space dimension	4	6	6
# of actions	2	3	4
discount rate γ	0.99	0.99	0.98
# of trajectories	200	200	50
max length of training traj.	200	200	200
max length of rollouts for evaluation	1000	1000	1000
discounted return of target policy	89.4	-56.88	803.2
Model and algorithm details The model we learn is composed of a representation module and
a dynamics module. To be consistent with the experiment settings in Liu et al. (2018b), we use a
representation module of a single hidden layer feed-forward network that takes the state as input
and outputs representation. We squashed the representation between (-1, 1) using the tanh activa-
tion function. The dynamics module is also a single hidden layer feed-forward network that takes
representation as input and outputs state difference and reward prediction for each action. We use
the swish activation function (Ramachandran et al., 2017) for the hidden layers of two modules. As
a whole, the model can also be seen as a feed-forward network with three hidden layers of varying
activation functions, where the output of the second hidden layer is the representation we regularize.
For the purpose of comparison, we minimize the L2 distance between the model prediction and the
desired outcome from data, which corresponds to using a model of Gaussian predictive distribu-
tion with fixed variance. We standardized the inputs and outputs of the neural network and used
Adam (Kingma & Ba, 2014) with a learning rate of 3 × 10-4 for the optimization. When compared
to the similar experiments conducted in Liu et al. (2018b), we used a larger and more expressive
model with more optimization steps with a smaller learning rate for a more accurate comparison.
While the derivation of the RepB-SDE objective was based on the state-action representation func-
tion, we use state representation in this experiment for direct comparison with RepBM, which uses
state representation (it can be also understood as using action invariant kernel). We follow the choice
3https://github.com/rlpy/rlpy
18
Published as a conference paper at ICLR 2021
ofLiuetal. (2018b) and use dot product kernel k(φ(s), φ(s)) = φ(s)>φ⑸ for the OPE experiment,
which is not universal but allows us to avoid search of kernel hyperparameters, such as length-scales.
After the training, we generate another 200 trajectories (50 in case of HIV simulator), and rollout
in both true and simulated (based on learned model) environments to evaluate models. We measure
the individual MSE, which is
∞	∞2
IndividualMSE = Es°〜d°(Ec,n [X Ytrt∣so] - Em,∏ [X Ytrt∣so])
t=0	t=0
for measuring the performance of each model. Whole experiment, from sampling data to learning
and evaluating the model, is repeated 200 times with different random seeds.
Choice and effect of hyperparameter α For choosing
hyperparameter α for each algorithm, we searched over
α ∈ {0.001,0.01,0.1,1,10} for each off-policyness e
and for each environment. Chosen as were mainly α ∈
{0.001,0.01} for CartPole-v0, α ∈ {1,10} for Acrobot-
v1, and α ∈ {0.01,0.1} for HIV simulator for both al-
gorithms. In general, large α was beneficial when high
off-policyness (e) is present and/or the task is hard to gen-
eralize. On the right We show the example of effect of
varying α in CartPole-v0.
Comparison to other baselines In Figure 2, the OPE results with other model-free baselines
are presented. FQE: Fitted Q-evaluation, IS: step-wise importance sampling, DR: doubly robust
estimator based on step-wise importance sampling using the value function learned with fitted Q-
evaluation (Jiang & Li, 2016), DualDICE: stationary distribution correction algorithm (Nachum
et al., 2019a). We used the implementation provided by the authors in case of DualDICE. All results
are normalized to set the MSE of model-based baseline to be 1. Here, we used average MSEs instead
of individual MSEs since importance sampling estimators are not suitable in computing individual
MSEs. The results show that model-based methods are more robust to increasing off-policyness
when compared to FQE. The results of model-based methods on Acrobot is relatively worse due to
the difficult dynamics of Acrobot environment.
山 s≡a,6roJa,>4 pa,z=roULION
CartPole-vO
IO7 -
IO4 -
IO1-
0.1	0.3	0.5	0.7	0.9
Behavior ofT-policyness (ɛ)
--------Baseline φ Ours
1O3 -
ιo1 -
io-1 -
Acrobot-Vl
0.1	0.3	0.5	0.7	0.9
Behavior ofT-policyness (ε)
φ FQE IS	屯
IO6 -
HIV simulator
IO3 -
IO0
0.1	0.3	0.5	0.7	0.9
Behavior off-policyness (ɛ)
DR φ DuaIDICE
Figure 2: The OPE results compared to other model-free baselines. All experiments are repeated
200 times and the error bars denote 95% confidence interval.
B.3	Details of the offline RL experiments
Task details We used 12 datasets in D4RL (Fu et al., 2020) over four dataset types and three envi-
ronments as specified in the main text. In Table 1, normalized scores suggested by (Fu et al., 2020)
is used to report the result, where score of 0 corresponds to a random policy and 100 corresponds
to a converged SAC policy. In HalfCheetah-v2, score of 0 means the undiscounted return of -280,
and score of 1 means the undiscounted return of 12135. In Hopper-v2, score of 0 means the undis-
counted return of -20, and score of 1 means the undiscounted return of 3234. In Walker2d-v2 score
of 0 means the undiscounted return of 2, and score of 1 means the undiscounted return of 4592. We
assume that the termination conditions of tasks are known in prior.
19
Published as a conference paper at ICLR 2021
Representation balancing maximizing undiscounted return As we report the undiscounted sum
of rewards in the experiments, the maximization of lower bound of Rπ may result in an under-
utilization of experiences of later timesteps. One way to mitigate the mismatch is to optimize the
policy by maximizing the returns starting from the states in the dataset. It corresponds to maximizing
∞
Rn = (1 - Y)Eso〜dD,m,∏ [£t=o Ytrt] instead of Rn, where the expectation respect to the initial
state distribution d0 is altered with the data distribution dD .
Consequently, to bound the error of Rn, the representation should be balanced with another dis-
counted stationary distribution dπ(s, a)，(1 - γ) P∞=o γt Pr(St = s,at = a∣so 〜dD, T, π), the
distribution induced by the policy π where the initial state is sampled from the data distribution. The
derivations can be easily adapted by noting that:
IPMG (dφ, dD ) =	suP	(I-Y )Es 〜dD	hν(φ(S,a))] -	E(s,a,s0)〜dD	hν (φ(S,a))	-Yν(φ(Sla'))],
ν∈F	a 〜n(S)	。'〜∏(s0)
and changing the initial state sampling distributions to dD during the estimation of IPM.
Model and algorithm details Similar to the model used in the OPE experiment, the model we
learn is composed of a representation module and a dynamics module. A representation module is
a feed-forward network with two hidden layers that takes the state-action pair as input and outputs
representation through the tanh activation function. The dynamics module is a single hidden layer
network that takes representation as input and outputs parameters of diagonal Gaussian distribution
predicting state difference and reward. We use 200 hidden units for all intermediate layers including
the representation layer. Across all domains, we train an ensemble of 7 models and pick the best
5 models on their validation error on hold-out set of 1000 transitions in the dataset. The inputs
and outputs of the neural network is normalized. We present the pseudo-code of the presented
Representation Balancing Offline Model-based RL algorithm below.
Algorithm 1 Representation Balancing Offline Model-based RL
Input: Offline dataset D, previous policy π	Output: Optimized policy π
1:	Sample K independent datasets with replacement from D.
2:	Train bootstrapped ensemble of K models {Tbi, RR}K=0 minimizing Eq. (8) (adapted with dφ).
3:	for repeat = 0, 1, . . . do
4:	for rollout = 0, 1, . . . , B do
5:	Sample initial rollout state S0 from D.
6:	for t = 0, 1, . . . do
7:	Sample an action at 〜 ∏(s..
8:	Randomly pick (Ti, Ri) and sample (st+ι,rt)〜(Ti, Ri)(st, at).
9:	Compute rt = rt - γλ IpVK[μ(st,at)]∣∣ and store (st,at,rt,st+ι) in D.
10:	end for
11:	end for
12:	Draw samples from D to compute IPM(dφ, dD).
13:	Draw samples from D and Db to update critic Q.
14:	Maximize Es〜D∪D,a〜n(s)[Q(s, a) - T log∏(a∣s)] - α∏IPM(dφ, dD) to update π.
15:	end for
For the result of MOPO (Yu et al., 2020), we ran the code kindly provided by the authors4 without
any modification on the algorithm or the hyperparameters. All algorithms we experimented (Base,
RP, RepB-SDE) share all the hyperparameters in common except the ones associated with changing
objectives. We run SAC on the full rollouts from the trained ensemble models as shown in Algo-
rithm 1. The common hyperparameters shared among algorithms are shown in Table 3. We simply
tried the listed hyperparameters and not tuned them further. For RP and RepB-SDE, we penalized
the reward from simulated environments with the standard deviation of prediction means of neural
network ensembles. We used standardized output of all 7 neural networks to compute the reward
4https://github.com/tianheyu927/mopo
20
Published as a conference paper at ICLR 2021
Table 3: Common hyperparameters used in offline RL experiments.
Parameter
optimizer
learning rate
discount factor γ
number of samples per minibatch
target smoothing coefficient τ
[actor/critic] number of hidden layers
[actor/critic] number of hidden units per layer
[actor/critic] non-linearity
# of rollouts
max length of rollouts
rollout buffer size
Value
Adam (Kingma & Ba, 2014)
3 × 10-4
0.99
256
5 × 10-3
2
256
ReLU
104
103
5 × 107
Table 4: Normalized scores on D4RL MuJoCo benchmark datasets (Fu et al., 2020) with standard
errors fully specified.
Dataset type	Environment	RePB-SDE (ours)	RP	Base	MOPO
Random	Walker2d-v2	21.1 ± 1.0	18.4 ± 1.8	16.4 ± 2.2	1.3 ±0.5
Medium	Walker2d-v2	72.1 ± 1.9	56.3 ± 4.3	5.5 ± 0.4	-0.1 ± 0.0
Med-Replay	Walker2d-v2	49.8 ± 11.4	41.1 ± 10.3	6.2 ± 0.7	47.8 ± 4.8
Med-Expert	Walker2d-v2	88.8 ± 6.9	72.6 ± 5.1	51.7 ± 7.1	32.4 ± 8.8
Random	Hopper-v2	8.6± 1.0	8.3 ± 0.2	8.3 ± 0.2	9.1 ± 0.3
Medium	Hopper-v2	34.0 ± 2.8	27.5 ± 3.1	19.8 ± 0.8	19.2 ± 4.5
Med-Replay	Hopper-v2	62.2 ± 6.7	49.8 ± 8.7	32.9 ± 5.5	80.8 ± 2.9
Med-Expert	Hopper-v2	82.6 ± 7.0	74.0 ± 7.2	19.1 ± 1.1	23.2 ± 1.3
Random	HalfCheetah-v2	32.9 ± 1.1	31.3 ± 1.5	26.1 ± 6.2	29.9 ± 1.2
Medium	HalfCheetah-v2	49.1 ± 0.3	47.3 ± 0.3	22.1 ± 4.4	45.0 ± 0.4
Med-Replay	HalfCheetah-v2	57.5 ± 0.8	53.6 ± 0.8	54.0 ± 2.4	53.8 ± 1.1
Med-Expert	HalfCheetah-v2	55.4 ± 8.3	36.7 ± 12.0	31.9 ± 10.5	91.0 ± 2.5
penalty. We first search over the reward penalty coefficient λ ∈ {0, 2, 5, 7, 10, 15} that grants RP
the best performance. We shared same λ for RepB-SDE and searched over αM ∈ {0.01, 0.1, 1},
απ ∈ {0, 0.01, 0.1, 1, 10}. We ran the algorithms for 1.5 × 106 gradient updates, except for
HalfCheetah-Medium-Expert where we ran for 5.0 × 106. In Table 4, we present the standard
errors of results, which was omitted in the main text.
In addition to the performance after the final iteration presented in Table 1, we also present the
learning curves of the experiment in Figure 3, and the effect of the choice of αM and απ in Figure 4.
The Figure 4 shows that the presented regularization is robust to the choice of αM , απ except for
too large αM , consistently improving from RP, which corresponds to αM = 0, απ = 0.
21
Published as a conference paper at ICLR 2021
u」nattωσsω><
0.0 0.3 0.6 0.9 1.2 1.5
Ra ndom-Hopper-v2
0.0 0.3 0.6 0.9 1.2 1.5	0.0 0.3 0.6 0.9 1.2 1.5	0.0 0.3 0.6 0.9 1.2 1.5
400 f
≡ 300-
ω
θ 200 -
⅞ 100 -
0-
0.0 0.3 0.6 0.9 1.2 1.5
Medium-Hopper-v2
Medium-RepIay-Hopper-VZ
Medium-Expert-Hopper-v2
Random-Ha IfCheeta h-v2
Medium-HalfCheetah-v2
Medium-Replay-Ha lfCheetah-v2
Medium-Expert-Ha lfCheetah-v2
Updates (106)
Updates (106)
Updates (106)
Updates (106)
MOPO	Base	RP —— RP + MRB …… BC …… MF
Figure 3: The learning curves of the D4RL experiment.
Effect of varying αw and aπ (Mediun∩-Walker2d-v2)
BC
MF
RP (αM = 0.0,απ = 0.0)
(«m = 0.01, α7r = 0.0)
(aM = 0.1, απ = 0.0)
(«m = 1.0,απ = 0.0)
(ɑʌj = 10.0, σ7r = 0.0)
(aM = 0.1, απ = 0.01)
(aM = 0.1, σπ = 0.1)
(aM = 0.1, απ = 1.0)
(aM = 0.1,απ = 10.0)
Figure 4: The effect of varying hyperparameter in the D4RL experiment.
22