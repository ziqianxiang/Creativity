Published as a conference paper at ICLR 2021
Improved Estimation of Concentration Under
'p-Norm Distance Metrics Using HALF Spaces
Jack B. Prescott, Xiao Zhang and David Evans
Department of computer science
University of Virginia
{jbp2jn, shawn, evans}@virginia.edu
Ab stract
concentration of measure has been argued to be the fundamental cause of adversar-
ial vulnerability. Mahloujifar et al. (2019b) presented an empirical way to measure
the concentration of a data distribution using samples, and employed it to find lower
bounds on intrinsic robustness for several benchmark datasets. However, it remains
unclear whether these lower bounds are tight enough to provide a useful approxima-
tion for the intrinsic robustness of a dataset. To gain a deeper understanding of the
concentration of measure phenomenon, we first extend the Gaussian isoperimetric
inequality to non-spherical Gaussian measures and arbitrary `p-norms (p ≥ 2).
We leverage these theoretical insights to design a method that uses half-spaces to
estimate the concentration of any empirical dataset under `p-norm distance metrics.
Our proposed algorithm is more efficient than Mahloujifar et al. (2019b)’s, and
our experiments on synthetic datasets and image benchmarks demonstrate that it
is able to find much tighter intrinsic robustness bounds. These tighter estimates
provide further evidence that rules out intrinsic dataset concentration as a possible
explanation for the adversarial vulnerability of state-of-the-art classifiers.
1	Introduction
Despite achieving exceptional performance in benign settings, modern machine learning models have
been shown to be highly vulnerable to inputs, known as adversarial examples, crafted with targeted
but imperceptible perturbations (szegedy et al., 2014; Goodfellow et al., 2015). This discovery has
prompted a wave of research studies to propose defense mechanisms, including heuristic approaches
(PaPemot et al., 2016; Madry et al., 20l8; Zhang et al., 2019) and certifiable methods (Wong & Kolter,
2018; Gowal et al., 2019; cohen et al., 2019). Unfortunately, none of these methods can successfully
Produce adversarially-robust models, even for classification tasks on toy datasets such as ciFAR-10.
To exPlain the Prevalence of adversarial examPles, a line of theoretical works (Gilmer et al., 2018;
Fawzi et al., 2018; shafahi et al., 2019; Dohmatob, 2019; Bhagoji et al., 2019) have Proven uPPer
bounds on the maximum achievable adversarial robustness by imPosing different assumPtions on
the underlying metric Probability sPace. in Particular, Mahloujifar et al. (2019a) generalized the
Previous results showing that adversarial examPles are inevitable as long as the inPut distributions are
concentrated with resPect to the Perturbation metric. Thus, the question of whether or not natural
image distributions are concentrated is highly relevant, as if they are it would rule out any Possibility
of there being adversarially robust image classifiers.
Recently, Mahloujifar et al. (2019b) ProPosed an emPirical method to measure the concentration
of an arbitrary distribution using data samPles, then emPloyed it to estimate a lower bound on
intrinsic robustness (see Definition 2.2 for its formal definition) for several image benchmarks. By
demonstrating the gaP between the estimated bounds of intrinsic robustness and the robustness
Performance achieved by the best current models, they further concluded concentration of measure is
not the sole reason behind the adversarial vulnerability of existing classifiers for benchmark image
distributions. However, due to the heuristic nature of the ProPosed algorithm, it remains elusive
whether the estimates it Produces can serve as useful aPProximations of the underlying intrinsic
robustness limits, thus hindering understanding of how much of the actual adversarial risk can be
exPlained by the concentration of measure Phenomenon.
1
Published as a conference paper at ICLR 2021
In this work, we address this issue by first characterizing the optimum of the actual concentration
problem for general Gaussian spaces, then using our theoretical insights to develop an alternative
algorithm for measuring concentration empirically that significantly improves both the accuracy and
efficiency of estimates of intrinsic robustness. While we do not demonstrate a specific classifier which
achieves this robustness upper bound, our results rule out inherent image distribution concentration
as the reason for our current inability to find adversarially robust models.
Contributions. We generalize the Gaussian Isoperimetric Inequality to non-spherical Gaussian
distributions and 'p-norm distance metrics with P ≥ 2 (including '∞) (Theorem 3.3). Motivated by
the optimal concentration results for special Gaussian spaces (Remark 3.4), we develop a sample-
based algorithm to estimate the concentration of measure using half spaces that works for arbitrary
distribution and any `p -norm distance (Section 4). Compared with prior approaches, we empirically
demonstrate the significant increase in efficacy of our method under '∞-norm distance metric
(Section 6). Not only does the proposed method converge to its limit with an order of magnitude
fewer data (Section 6.2), it also finds a much tighter lower bound of intrinsic robustness for both
simulated datasets whose underlying concentration function is analytically derivable and various
benchmark image datasets (Section 6.1). In particular, we improve the best current estimated lower
bound of intrinsic robustness from approximately 82% to above 93% for CIFAR-10 under '∞-norm
bounded perturbations with = 8/255. These tighter concentration estimates produced by our
algorithm provide strong evidence that concentration of measure should not be considered as the main
cause of adversarial vulnerability, at least for the image benchmarks evaluated in our experiments.
Related Work. Several prior works have sought to empirically estimate lower bounds on intrinsic
robustness using data samples. The pioneering work of Gilmer et al. (2018) introduced the connection
between adversarial examples and the concentration phenomenon for uniform n-spheres, then
proposed a simple heuristic to find a half space that expands slowly under Euclidean distance for
the MNIST dataset. Our work can be seen as a strict generalization of Gilmer et al. (2018)’s, which
applies to arbitrary 'p-norm distance metrics (including '∞). By characterizing the optimal transport
cost between conditional distributions, Bhagoji et al. (2019) estimated a lower bound on the best
possible adversarial robustness for several image datasets. However, when applied to adversaries
beyond '2, such as '∞,the lower bound produced by their method is not informative (that is, it is close
to zero). The most relevant previous work is Mahloujifar et al. (2019b), which proposed a general
method for measuring concentration using special collections of subsets. Although the optimal value
of the considered empirical concentration problem is proven to asymptotically converge to the actual
concentration, there is no guarantee that the proposed searching algorithm for solving the empirical
problem finds the optimum. Our approach follows the framework introduced by Mahloujifar et al.
(2019b)’s, but considers a different collection of subsets for the empirical concentration problem. This
not only results in optimality for theoretical Gaussian distributions, but also significantly improves
the estimation performance for typical image benchmarks.
Another line of work attempts to provide estimates of intrinsic robustness upper bounds based on
generative assumptions. In order to justify the theoretically-derived impossibility results, Fawzi et al.
(2018) estimated the smoothness parameters of the state-of-the-art generative models on CIFAR-10
and SVHN datasets, which yield approximated upper bounds on adversarial robustness for any
classifiers. Zhang et al. (2020) generalized their results to non-smoothed data manifolds, such as
datasets that can be captured by a conditional generative model. However, these methods only work
for simulated generative distributions, which may deviate from the actual distributions they are
intended to understand.
Notation. For any n ∈ Z+ , denote by [n] the set {1, 2, . . . , n}. Lowercase boldface letters denote
vectors and uppercase boldface letters represent matrices. For any vector x and p ∈ [1, ∞), let
xj , kxkp and kxk∞ be the j-th element, the 'p-norm and the '∞-norm of x. For any matrix A,
B is said to be a square root of A if A = BB, and the induced matrix p-norm of A is defined as
kAkp = supx6=0 {kAxkp/kxkp}. Denote by N (θ, Σ) the Gaussian distribution with mean θ and
covariance matrix Σ. Let γn be the probability measure ofN(0, In), where In denotes the identity
matrix. Let Φ(∙) be the cumulative distribution function of N(0,1) and Φ-1(∙) be its inverse. For
any set A, let Pow(A) and 1a(∙) be all measurable subsets and the indicator function of A. Let
(X, μ, ∆) be a metric probability space, where △: X ×X → R≥o denotes a distance metric on X.
Define the empirical measure with respect to a sample set {xi}i∈m] as bm(A)=* Pi∈[m] 1a(xi),
2
Published as a conference paper at ICLR 2021
∀A ∈ pow(X). Let B(x, , ∆) = {x0 ∈ X : ∆(x0, x) ≤ } be the ball around x with radius.
Define the -expansion of A as A(∆) = {x ∈ X : ∃ x0 ∈ B(x, , ∆) ∩ A}.
2	Preliminaries
In this section, we introduce the problem of measuring concentration and its connection to adversarial
robustness. Consider a metric probability space of instances (X, μ, ∆). Given parameters E ≥ 0 and
α > 0, the concentration of measure problem1 can be cast as the following optimization problem:
minimize
E∈Pow(X)
〃(*)
subject to μ(E) ≥ α.
(2.1)
We focus on the case where ∆ is some 'p-norm distance metric (including '∞) in this work.
Concentration of measure has been shown to be closely related to adversarial examples (Gilmer et al.,
2018; Fawzi et al., 2018; Mahloujifar et al., 2019a). In particular, one can prove that for a given
robust learning problem, if the input distribution is concentrated with respect to the perturbation
metric, no adversarially robust model exists. The concentration parameter (which corresponds to the
optimal value of optimization problem (2.1)) determines an inherent upper bound on the maximum
adversarial robustness that any model can achieve for the given problem.
To explain the connection between concentration of measure and robust learning in a more formal way,
we lay out the definition of adversarial risk that we work with. We draw this definition from several
previous works, including Gilmer et al. (2018); Bubeck et al. (2019); Mahloujifar et al. (2019a;b).2
Definition 2.1 (Adversarial Risk). Let (X, μ, ∆) be the input metric probability space. Assume f *
is the underlying ground-truth classifier that gives labels to any input. Given classifier f and E ≥ 0,
the adversarial risk of f with respect to E-perturbations measured by ∆ is defined as:
AdvRisk (f) = Pr ∃ x0 ∈ B(x, E, ∆) s.t. f(x0) 6= f*(x0).
X〜μ
Correspondingly, we define the adversarial robustness of f as AdvRob (f) = 1 - AdvRisk(f).
When E = 0, adversarial risk degenerates to standard risk. In other words, it holds for any f that
AdvRisko(f) = Risk(f) := Prx〜μ[f (x) = f *(x)]. We remark that this definition assumes the
existence of an underlying ground-truth labeling function, which does not apply to the agnostic
setting where inputs can have non-deterministic labels.
Initially introduced in Mahloujifar et al. (2019b), intrinsic robustness captures the maximum adver-
sarial robustness that can be achieved by any imperfect classifier for a robust classification problem.
Definition 2.2 (Intrinsic Robustness). Consider the same setting as in Definition 2.1. For any α > 0,
let Fα = {f : Risk(f) ≥ α} be the set of imperfect classifiers whose risk is at least α. Then the
intrinsic robustness of the given robust classification problem with respect to Fα is defined as:
AdvRobe(Fa) = 1 — inf {AdvRiske(f)} = sup {AdvRobe(f)}.
f∈Fα	f∈Fα
It is worth noting that the value of AdvRobe(Fa) is only determined by the underlying input data
distribution, the perturbation set and the risk threshold parameter α, which is independent of the
model class one would choose for learning.
By relating the robustness of a classifier to the E-expansion of its induced error region, the following
lemma, proved in Mahloujifar et al. (2019a), establishes a fundamental connection between the
concentration of measure and the intrinsic robustness one can hope for a robust classification problem.
Lemma 2.3. Consider the same setting as in Definition 2.2. Let h^ʌ)(a, E) be the concentration
function that captures the optimal value of the concentration of measure problem (2.1):
hʌtʌ)(a, E) = inf {μ(£(△)) : E ∈ PoW(X) and μ(E) ≥ α}.
Then, AdvRobe(Fa) = 1 — h^)(a, E) holds for any α > 0 and E ≥ 0.
1The standard notion of concentration of measure (Talagrand, 1995) corresponds to the case where α = 0.5.
2Other related definitions, such as the one used in most empirical works for robustness evaluation, are
equivalent to this, as long as small perturbations preserve the ground truth. See Diochnos et al. (2018) for a
detailed comparison of these and other definitions of adversarial robustness.
3
Published as a conference paper at ICLR 2021
Lemma 2.3 suggests that one can characterize the intrinsic robustness limit for a robust classification
problem by measuring the concentration of the input data with respect to the perturbation metric.
In this paper, we aim to understand and empirically estimate the intrinsic robustness limit for typical
robust classification tasks by measuring concentration. Itis worth noting that solving the concentration
problem (2.1) itself only shows the existence of an error region E whose -expansion has certain
(small) measure. This further implies the possibility of existing an optimally robust classifier (with
risk at least α), whose robustness matches the intrinsic robustness limit AdvRobe (Fa). However,
actually finding such optimal classifier using a learning algorithm might be a much more challenging
task, which is beyond the scope of this work.
3	Generalizing the Gaussian Isoperimetric Inequality
Before proceeding to introduce the proposed methodology for solving the concentration of measure
problem, we first present our main theoretical results of generalizing the Gaussian Isoperimetric
Inequality. This theoretical result largely motivates our method.
To begin with, we introduce the Gaussian Ispoperimetric Inequality (Sudakov & Tsirelson, 1974;
Borell, 1975). It characterizes the optimum of the concentration problem (2.1) with respect to
standard Gaussian distribution and '2-distance, where half spaces are proven to be the optimal sets.
Definition 3.1 (Half Space). Let w ∈ Rn and b ∈ R. Without loss of generality, assume kwk2 = 1.
An n-dimensional half space with parameters w and b is defined as:
Hw,b = {z ∈ Rn : w>z + b ≤ 0}.
Lemma 3.2 (Gaussian Isoperimetric Inequality). Consider the standard Gaussian space (Rn, γn)
with '2-distance. Let E ∈ Pow(Rn) and H be a half space such that Yn(E) = Yn(H), then for any
≥ 0, it holds that
Yn(E('2)) ≥ Yn(HP) = Φ(Φ-1(Yn(E)) + e).
The proof of the Gaussian Isoperimetric Inequality can be found in Ledoux (1996).
Lemma 3.2 implies the concentration function of (Rn,Yn,k∙k2) is hY)(α,e) = Φ(Φ-1(α) + e),
but only applies when the underlying distribution is a spherical Gaussian and the metric function is '2-
distance. Thus, it only gives a concentration function for estimating the the intrinsic robustness limit
in a very restrictive setting. To understand the concentration of measure for more general problems,
we prove the following theorem that extends the standard Gaussian Isoperimetric Inequality (Lemma
3.2) to non-spherical Gaussian measure and general 'p-norm distance metrics for any p ≥ 2.
Theorem 3.3 (Generalized Gaussian Isoperimetric Inequality). Let ν be the probability measure of
N(θ, Σ), where θ ∈ Rn and Σ is a positive definite matrix in Rn×n. Consider the probability space
(Rn, ν) with 'p-norm distance, where p ≥ 2 (including '∞). For any E ∈ pow(Rn) and ≥ 0,
V(E('p)) ≥ Φ(Φ-1(ν(E)) + e∕k∑1∕2kp),	(3.1)
where Σ1/2 is the square root of Σ, and k∑1∕2kp denotes the induced matrix p-norm of Σ1/2.
Remark 3.4. Theorem 3.3 suggests that for general Gaussian distribution N(θ, Σ) and any 'p-
norm distance (p ≥ 2), the corresponding concentration function is lower bounded by Φ(Φ-1(α) +
∕kΣ1∕2 kp). Due to the NP-hardness of approximating the matrix p-norm (Hendrickx & Olshevsky,
2010), it is generally hard to infer whether the equality of (3.1) can be attained or not. However, for
specific special Gaussian spaces, we can derive optimal subsets that achieve the lower bound. In
particular, for the case where Σ = In and p > 2, the optimum is attained when E is a half space with
axis-aligned weight vector (that is, w = ej for some j ∈ [n]). For the case where Σ 6= In andp = 2,
the optimal solution is a half space Hv1,b, where v1 is the eigenvector with respect to the largest
eigenvalue of Σ. The proofs of these optimality results are provided in Appendix A.2.
4	Empirically Measuring Concentration using Half Spaces
Recall that the primary goal of this paper is to measure the concentration of an arbitrary distribution.
However, for typical classification problems, we might not know the density function of the underlying
4
Published as a conference paper at ICLR 2021
distribution μ, but instead We usually have access to a finite set of m instances {xi}i∈m] sampled
from μ. Following Mahloujifar et al. (2019b), we consider the following empirical counterpart of the
actual concentration problem (2.1):
minimize bm(£(△))	subject to bm(E) ≥ ɑ,
(4.1)
where bm is the empirical measure based on {xi}i∈[m] and G ⊆ Pow(X) denotes a particular collec-
tion of subsets. Mahloujifar et al. (2019b) proposed the complement of union of T hyperrectangles as
G for '∞ and the union of T balls for '2. They proved that if one increases the complexity parameter
T and the sample size m together in a careful way, the optimal value of the empirical concentration
problem (4.1) converges to the actual concentration asymptotically. However, it is unclear how
quickly it converges and how well the proposed heuristic algorithm in Mahloujifar et al. (2019b) finds
the optimum of (4.1).
In this work, we argue that the set of half spaces is a superior choice for G with respect to any `p-norm
distance. Apart from achieving the optimality for certain Gaussian spaces as discussed in Remark 3.4,
estimating concentration using half spaces has several other advantages including the closed-form
solution of 'p-distance to half-space (Lemma 4.1) and its small sample complexity requirement for
generalization (Theorem 4.2). To be more specific, we focus on the following optimization problem
based on the empirical measure bm and the collection of half spaces HS(n):
minimize bm,(E('p))	subject to bm(E) ≥ a,	(4.2)
E∈HS(n)
where HS(n) = {Hw,b : w ∈ Rn, b ∈ R, and kwk2 = 1} is the set of all half spaces in Rn.
The following lemma, proven in Appendix B.1, characterizes the closed-form solution of the `p-norm
distance between a point x and a half space. Such a formulation enables an exact computation of the
empirical measure with respect to the -expansion of any half space.
Lemma 4.1 ('p-Distance to Half Space). Let Hw,b ∈ HS(n) be an n-dimensional half space. For
any vector x ∈ Rn, the `p-norm distance (p ≥ 1) from x to Hw,b is:
dp(x, Hw,b) =	(0w, >x + b)/kwk
Here, q is a real number that satisfies 1/p + 1/q = 1.
w>x + b ≤ 0;
otherwise.
q,
Lemma 4.1 implies that the -expansion of any half space with respect to the `p-norm is still a half
space. Since the VC-dimensions of both the set of half spaces and its expansion are bounded, we
can thus apply the generalization theorem in Mahloujifar et al. (2019b), which yields the following
theorem, proved in Appendix B.2, that characterizes the generalization of concentration with respect
to the collection of half spaces.
Theorem 4.2 (Generalization of Concentration of Half Spaces). Consider the metric probability
space, (X, μ, k ∙ ∣∣p), where X ⊆ Rn andP ≥ 1. Let {xi}i∈[m] be a set of m instances sampled from
μ, and let bm be the corresponding empirical measure. Define the concentration functions regarding
the collection of half spaces HS(n) with respect to μ as:
WPp (α, 3 HSS)) = Weinf⑺){μ(E('p)): μ(E) ≥ α},
and let hb'p)(a, e, HS(n)) be its empirical counterpart with respect to bm. For any δ ∈ (0,1), there
exists constants co and ci such that with probability at least 1 一 co ∙ e-n logn,
hbɪ) (α 一 δ, e, HS(n)) — δ ≤ h& (α, e, HS(n)) ≤ hbp) (α + δ, e, HS(n)) 十 δ
holds, provided that the sample size m ≥ ci ∙ n log n∕δ2.
Remark 4.3. Theorem 4.2 suggests that for the concentration of measure problem with respect to
half spaces, in order to achieve δ estimation error with high probability, it requires Ω(nlog(n)∕δ2)
number of samples. Compared with Mahloujifar et al. (2019b), our method using half spaces
requires fewer samples in theory to achieve the same estimation error.3 For standard Gaussian
3The proposed estimators for '∞ and '2 in Mahloujifar et al. (2019b) require Ω(nTlog(n) log(T)∕δ2)
samples to achieve δ approximation, where T is a predefined number of hyperrectangles or balls.
5
Published as a conference paper at ICLR 2021
inputs, the empirical concentration with respect to half spaces is guaranteed to converge to the actual
concentration as in (2.1), i.e., limm→∞ h(b'p) (α, e, HS(n)) = Wp (α, e); whereas for distributions
that are not Gaussian, there might exist a gap. However, this gap of empirical and actual concentration
is shown to be uniformly small across various data distributions, as will be discussed in Section 6.2.
Based on Lemma 4.1, estimating the empirical concentration using half spaces as defined in (4.2) is
equivalent to solving the following constrained optimization problem:
minimize
w∈Rn,b∈R
subject to
1{w>xi +b ≤ kwkq}
i∈[m]
X SX 1{w>Xi + b ≤ 0} ≥ α and kw∣∣2 = 1.
m
i∈[m]
(4.3)
The optimal solution to (4.3) would be a half space Hw,b that satisfies the following two properties:
(1) approximately α-fraction of data is covered by Hw,b, and (2) most of the remaining data points
are at least -away from Hw,b under `p-norm distance metric.
Note that we can always set b to be the α-quantile of the projections {-w>xi : i ∈ [m]} to satisfy
the first property. In addition, to satisfy the second condition, inspired by the special case optimality
results in Remark 3.4, we propose to search for a weight vector W such that both the 'q-norm of W is
small and the variation of the given sample set along the direction of w is large. These searching
criteria guarantee that the given dataset {xi}i∈[m], when projected onto W then normalized by kWkq,
will have a large variance, which implies the second property.
We propose a heuristic algorithm to search for the desirable half space according to the aforementioned
criteria (for the pseudocode and a detailed analysis of the algorithm, see Appendix C). It first conducts
a principal component analysis with respect to the given empirical dataset, then iterates through all
the principal components raised to an arbitrary power with normalization as candidate choices of
the weight vector W. Finally, based on the optimal choice of b, the algorithm outputs the best Hw,b
that achieves the smallest empirical measure with respect to the -expansion of Hw,b. As we see
in the experiments in the next section, this algorithm is able to find near-optimal solutions to the
concentration problem for various datasets.
5 Error Analysis
The goal of measuring concentration is to provide an empirical estimate of concentration of measure
that minimizes the overall approximation error. Here, we describe the error components based on the
general empirical framework for measuring concentration, and then discuss its implications on how
to choose the collection of subsets G for the empirical concentration problem (4.1).
Error Decomposition. Let (X, μ, ∆) be the underlying input metric probability space and G be the
selected collection of subsets for the empirical concentration problem. Suppose an algorithm that
aims to solve the empirical concentration problem (4.1) returns E as a solution. For any α ∈ (0, 1)
and ≥ 0, the approximation error between the empirical estimate of concentration and the actual
concentration can be decomposed into three error terms:
h"-")} = W 3T" G )+hμ秋α,e, Gii，G)
approximation error
} 、------------------{---------------}
finite sample estimation error
+ hμ∆)(α,e, G)- bm(EfR .	(5.1)
X-----------------------------/
^^^^^{^^^^^≡
modeling error
{^^^^^^≡
optimization error
The modeling error denotes the difference between the actual concentration function and the con-
centration function with respect to the selected collection of subsets G ; the finite sample estimation
error represents the generalization gap between the empirical concentration function and its limit;
and the optimization error captures how well the algorithm approximates the empirical concentration
problem. Such an error decomposition applies to both the empirical method proposed in this work as
well as Mahloujifar et al. (2019b)’s, despite the different choices of G.
6
Published as a conference paper at ICLR 2021
The complexity of G and the complexity of its -expansion G = {E : E ∈ G} control the finite
sample estimation error. So, G should be selected such that the empirical concentration function
hʃbt) (α,e, G) generalizes. If either G or Ge is too complex (e.g., it has unbounded VC-dimension), it
will be difficult to control the generalization of the empirical concentration function (see Remark 3.4
in Mahloujifar et al. (2019b) for a detailed discussion).
There exist tradeoffs among the three error terms in (5.1), and it is unlikely that there is a uniformly
good choice for G that minimizes all these error terms. In particular, increasing the complexity of G
typically reduces the modeling error, since the feasible set of the concentration function with respect
to G becomes larger. However, according to the generalization of concentration, this will also increase
the finite sample estimation error. Therefore, we should consider the effect of all these error terms
when choosing G, including the hardness of the optimization problem with respect to the empirical
concentration. It is favorable that the distance to any set in G has a closed-form solution, which
enables exactly computing the empirical measure of the -expansion of any set in G . In addition,
it will be easier to control the optimization error (i.e., develop an algorithm that produces tight
estimates), if the empirical concentration problem is simpler. For instance, solving the empirical
concentration problem with respect to the set of half spaces should be easier than solving it based
on the union of hyperrectangles or balls, since there are more hyperparameters to optimize for the
latter problem. Such simplicity further contributes to a tighter empirical estimate produced by our
algorithm for the underlying concentration of measure problem.
Depending on the underlying distance metric Mahloujifar et al. (2019b) set G as a collection of the
union of complement of hyperrectangles or the union of balls, whereas we choose G as the set of
half spaces for any `p-norm distance. In this work, we show that the set of half spaces is a superior
choice of G for measuring the concentration of typical image benchmarks. Other choices of G may
be preferred for different settings, but the same error decomposition and criteria will apply.
6	Experiments
In this section, We evaluate our empirical method for estimating concentration under '∞-norm distance
and comparing its performance to that of the method proposed by Mahloujifar et al. (2019b). We first
demonstrate that the estimate produced by our algorithm is very close to the actual concentration for
a spherical Gaussian distribution, and that our method is able to find much tighter bounds on the best
possible adversarial risk for several image benchmarks. We then compare the convergence rates, and
shoW that our method converges With substantially less data. Note that While We only provide results
for the most widely-used '∞-norm perturbation metric adopted in the existing adversarial examples
literature, our algorithm and experiments can be applied to any other `p -norm.
6.1	Estimation Accuracy
First, we evaluate the performance of our algorithm under '∞-norm distance metric on a generated
synthetic dataset consisting of 30,000 samples from N (0, I784). Since the proposed method follows
from the analytical results of concentration of multivariate Gaussian distributions, we expect results
produced by our empirical method to closely approach the analytical concentration on this simulated
Gaussian dataset. We initially consider the case where = 1.0 and α = 0.5 for the actual concen-
tration problem, requiring that the feasible set contains at least half of the data samples, and the
adversary can perturb each entry by precisely the standard deviation of the underlying distribution.
Our algorithm is able to produce a half space whose -expansion has mean empirical measure 84.18%
over 5 repeated trials. According to Theorem 3.3 and Remark 3.4, the optimal value of the consid-
ered concentration problem is 84.13%. This implies that our method performs very well when the
underlying distribution is Gaussian, while in stark contrast, the method by Mahloujifar et al. (2019b)
is not able to find a region whose expansion has measure less than 1 on the same simulated set. In
addition, we consider another setting for this dataset where = 1.0 is set the same and α = 0.05 is
set to be much smaller. Similarly, we observe that our method significantly outperforms Mahloujifar
et al. (2019b)’s in terms of the estimation accuracy (see Table 1 for the detailed comparison results).
Next, we evaluate our method on several image benchmarks. We set the values of α and to be
the same as in Mahloujifar et al. (2019b) for the '∞ case. For example, we use α = 0.01, E ∈
7
Published as a conference paper at ICLR 2021
Table 1: Comparisons between our method of estimating concentration with '∞ -norm distance and
the method proposed by Mahloujifar et al. (2019b) for different settings. For N (0, I784) with α = 0.5
and = 1.0, the previous method is unable to produce nontrivial estimate. Results for the previous
method are taken directly from the original paper (except for the Gaussian results).
Dataset	α		Test Risk (%)		Test Adv. Risk (%)	
			Prev. Method	Our Method	Prev. Method	Our Method
N (0, I784)	0.05	1.0	6.21 ± 0.44	5.20 ± 0.16	89.75 ± 0.80	26.37 ± 0.17
	0.5	1.0	-	49.98 ± 0.25	-	84.18 ± 0.11
		0.1	1.23 ± 0.12	1.22 ± 0.05	3.64 ± 0.30	1.35 ± 0.06
MNIST	0.01	0.2	1.11 ± 0.10	1.24 ± 0.05	5.89 ± 0.44	1.52 ± 0.06
		0.3	1.15 ± 0.13	1.25 ± 0.04	7.24 ± 0.38	1.75 ± 0.05
		0.4	1.21 ± 0.09	1.27 ± 0.05	9.92 ± 0.60	1.98 ± 0.08
		2/255	5.72 ± 0.25	5.14 ± 0.13	8.13 ± 0.26	5.28 ± 0.12
CIFAR-10	0.05	4/255	6.05 ± 0.40	5.22 ± 0.20	13.66 ± 0.33	5.68 ± 0.21
		8/255	5.94 ± 0.34	5.22 ± 0.16	18.13 ± 0.30	6.28 ± 0.13
		16/255	5.28 ± 0.23	5.19 ± 0.08	28.83 ± 0.46	7.34 ± 0.15
		0.1	5.92 ± 0.85	5.33 ± 0.14	11.56 ± 0.84	6.04 ± 0.13
Fashion-MNIST	0.05	0.2	6.00 ± 1.02	5.34 ± 0.14	14.82 ± 0.71	6.82 ± 0.19
		0.3	6.13 ± 0.93	5.24 ± 0.10	17.46 ± 0.53	8.01 ± 0.19
SVHN	0.05	0.01	8.83 ± 0.30	5.23 ± 0.09	10.17 ± 0.29	5.56 ± 0.08
{0.1, 0.2, 0.3, 0.4} for MNIST, and α = 0.05, ∈ {2/255, 4/255, 8/255, 16/255} for CIFAR-10.
These α values were selected to roughly represent the standard error of the state-of-the-art classifiers.
Table 1 demonstrates the risk and adversarial risk with respect to the best produced subsets using both
methods, computed on a separate test dataset. In our context of measuring concentration, risk refers
to the empirical measure of the produced subset, while adversarial risk corresponds to the empirical
measure of its -expansion. We use a 50/50 train-test split over the whole dataset to perform our
evaluation, and determine the best exponent of each principal component based on a brute-force
search. Though our method is deterministic for a given pair of training and testing sets, we account
for the variance of our method over different train-test splits by repeating our experiments 5 times
and reporting the mean and standard deviation of the results for each (α, ). It is worth noting that
the randomness of the previous method is derived not only from the selection of the training and test
sets, but also from the inherent randomness of the employed k-means algorithm.
We observe from Table 1 that in every case, the estimated adversarial risk is significantly lower for
our method than for the one found by Mahloujifar et al. (2019b)’s. Since both methods restrict the
search space to some special collection of subsets, these estimates can be viewed as valid empirical
upper bounds of the actual concentration as defined in (2.1). Therefore, the fact that our results are
significantly lower indicates that our algorithm is able to produce estimates that are much closer to
the optimum of the targeted problem. In addition, when translated to adversarial robustness, these
tighter estimates prove the existence of a rather robust classifier4 that has risk at least α, which further
suggests that the underlying intrinsic robustness limit of each of these image benchmarks is actually
much higher than previously thought.
For example, the best classifier produced by Mahloujifar et al. (2019b) has 18.1% adversarial risk
under '∞-perturbations bounded by E = 8/255 on CIFAR-10. However, our results demonstrate
that the adversarial risk of the best possible robust classifier can be as low as 6.3% given the same
risk constraint, indicating the underlying intrinsic robustness to be above 93.7%. As the intrinsic
robustness limits are shown to be very close to the trivial upper bound 1 - α across all the settings,
our results reveal that the concentration of measure phenomenon is not an important factor that causes
the adversarial vulnerability of existing classifiers on these image benchmarks.
4Based on the ground-truth f * and the returned set E of our algorithm, this classifier can be simply constructed
by setting f (x) = f * (x) for x ∈ E and f (x) = f * (x) for X ∈ E. Without knowing the ground-truth, we note
that such classifier may or may not be learnable. The learnability of such f is beyond the scope of this paper.
8
Published as a conference paper at ICLR 2021
(a) N (0, I784)
(b) MNIST
(c) CIFAR-10
Figure 1: The convergence curves of the best possible adversarial risk estimated using our method
and the method proposed by Mahloujifar et al. (2019b) as the number of training samples grows.
6.2	Convergence
Figure 6.2 shows the convergence rate of our method compared with that of the method proposed by
MahloUjifar et al. (2019b) under '∞-distance for GaUSSian data from N(0, I784) (α = 0.05, e = 1),
as well as for MNIST (α = 0.01, = 0.1) and CIFAR-10 (α = 0.05, = 2/255). We observe
Similar trendS for other SettingS, thUS we defer theSe reSUltS to Appendix D. For each graph, the
horizontal x-axiS repreSentS the Size of the dataSet USed to train the eStimator, and the vertical y-axiS
ShowS the concentration boUndS eStimated for a Separate teSt Set, which iS of Size 30, 000 for each
caSe. We generate the meanS and Standard deviationS for theSe convergence cUrveS by repeating both
methodS 5 timeS for different randomly-Selected training and teSt teStS. For the propoSed algorithm in
MahloUjifar et al. (2019b), we tUne the nUmber of the hyperrectangleS T for the optimal performance
baSed on the empirically-obServed adverSarial riSk.
For the SimUlated GaUSSian dataSetS, we inclUde a horizontal line at y = 0.2595 to repreSent the trUe
concentration of the Underlying diStribUtion, derived from Theorem 3.3 and Remark 3.4. ThiS allowS
US to more accUrately aSSeSS the convergence of oUr method, aS it iS the only caSe for which we know
the optimal valUe that oUr empirical eStimateS ShoUld be converging to. We See that oUr eStimateS
approach thiS line very qUickly, coming within 0.01 of the trUe valUe given only aboUt 1,000 SampleS.
While we do not have SUch a theoretical limit for other dataSetS, the riSk threShold α can be viewed
aS a lower boUnd of the actUal concentration, which iS USefUl in viSUally aSSeSSing the convergence
performance of oUr method. We can See that for both MNIST and CIFAR-10, oUr eStimateS get
very cloSe to the horizontal line at y = α given Several thoUSand training SampleS. Since the actUal
concentration mUSt be no leSS than α and oUr eStimated Upper boUnd iS approaching α from the above,
We immediate infer that the actual concentration of these data distributions with '∞-norm distance
ShoUld be a valUe Sightly greater than α. TheSe reSUltS not only demonStrate the SUperiority of oUr
method over the method of Mahloujifar et al. (2019b) in estimating concentration, but also show that
concentration of measure is not the reason for our inability to find adversarially robust models for
these image benchmarks.
7	Conclusion
Our results advance understanding of the intrinsic limits of adversarial robustness, strengthening the
conclusion from Mahloujifar et al. (2019b) which asserted that concentration of measure is not the sole
cause of the vulnerability of existing classifiers to adversarial attacks. We generalized the standard
Gaussian Isoperimetric Inequality, and then leveraged theoretical insights from that to construct an
efficient method for empirically estimating the concentration of arbitrary data distribution using
samples. Our method is able to generalize to any `p-norm distance metric, and surpasses previous
approaches in both estimation accuracy and data-efficiency.
Availability
An implementation of our method, and code for reproducing our experiments, is available under an
open source license from https://github.com/jackbprescott/EMC_HalfSpaces.
9
Published as a conference paper at ICLR 2021
Acknowledgements
This work was partially funded by an award from the National Science Foundation SaTC program
(Center for Trustworthy Machine Learning, #1804603). We thank Saeed Mauloujifar and Mohammad
Mahmoody for valuable comments and discussions.
References
Arjun Nitin Bhagoji, Daniel Cullina, and Prateek Mittal. Lower bounds on adversarial robustness
from optimal transport. In NeurIPS, 2019.
Christer Borell. The Brunn-Minkowski inequality in Gauss space. Inventiones mathematicae, 30(2):
207-216,1975.
Sebastien Bubeck, Yin Tat Lee, Eric Price, and Ilya Razenshteyn. Adversarial examples from
computational constraints. In International Conference on Machine Learning, 2019.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In International Conference on Machine Learning, 2019.
Dimitrios Diochnos, Saeed Mahloujifar, and Mohammad Mahmoody. Adversarial risk and robustness:
General definitions and implications for the uniform distribution. In NeurIPS, 2018.
Elvis Dohmatob. Generalized no free lunch theorem for adversarial robustness. In International
Conference on Machine Learning, 2019.
Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Adversarial vulnerability for any classifier. In
NeurIPS, 2018.
Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S Schoenholz, Maithra Raghu, Martin Wattenberg,
and Ian Goodfellow. Adversarial spheres. arXiv:1801.02774, 2018.
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2015.
Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan
Uesato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. Scalable verified training for
provably robust image classification. In IEEE International Conference on Computer Vision, 2019.
Julien M Hendrickx and Alex Olshevsky. Matrix p-norms are NP-hard to approximate if p 6= 1, 2, ∞.
SIAM Journal on Matrix Analysis and Applications, 31(5), 2010.
Michel Ledoux. Isoperimetry and Gaussian analysis. In Lectures on Probability Theory and Statistics.
Springer, 1996.
Saeed Mahloujifar, Dimitrios Diochnos, and Mohammad Mahmoody. The curse of concentration in
robust learning: Evasion and poisoning attacks from concentration of measure. In AAAI Conference
on Artificial Intelligence, 2019a.
Saeed Mahloujifar, Xiao Zhang, Mohammad Mahmoody, and David Evans. Empirically measuring
concentration: Fundamental limits on intrinsic robustness. In NeurIPS, 2019b.
Aleksander Madry, Aleksandar Makelov, LUdWig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet TalWalkar. Foundations of Machine Learning.
MIT Press, 2018.
Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram SWami. Distillation as a
defense to adversarial perturbations against deep neural netWorks. In IEEE Symposium on Security
and Privacy, 2016.
10
Published as a conference paper at ICLR 2021
MustaPha RaIssouli and Iqbal H JebriL Various proofs for the decrease monotonicity of the Schatten's
power norm, various families of Rn-norms and some open problems. International Journal of
Open Problems in Computer Science and Mathematics, 3(2):164-174, 2010.
Ali Shafahi, W. Ronny Huang, Christoph Studer, Soheil Feizi, and Tom Goldstein. Are adversarial
examples inevitable? In International Conference on Learning Representations, 2019.
Vladimir N Sudakov and Boris S Tsirelson. Extremal properties of half-spaces for spherically
invariant measures. Zapiski Nauchnykh Seminarov Leningrad Otdel Mathematical Institute Steklov
(LOMI), 41:14-24, 1974.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning
Representations, 2014.
Michel Talagrand. Concentration of measure and isoperimetric inequalities in product spaces.
Publications Mathematiques de IfInstitut des Hautes Etudes Scientifiques, 81(1):73-205, 1995.
Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In International Conference on Machine Learning, 2018.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.
Theoretically principled trade-off between robustness and accuracy. In International Conference
on Machine Learning, 2019.
Xiao Zhang, Jinghui Chen, Quanquan Gu, and David Evans. Understanding the intrinsic robustness of
image distributions using conditional generative models. In International Conference on Artificial
Intelligence and Statistics (AISTATS), 2020.
A	Proofs of Main Results in Section 3
In this section, we provide the detailed proofs of our main theoretical results presented in Section 3.
A.1 Proof of Theorem 3.3
Before proving Theorem 3.3, we first lay out the following lemma regarding the monotonicity of
'p-norms. For a rigorous proof of this lemma, see Raissouli & Jebril (2010).
Lemma A.1 (Monotonicity of `p). For any vector x ∈ Rn , the mapping p → kxkp is monotonically
decreasing for any p ≥ 1 (including p = ∞). That said, kxkp ≤ kxkq holds for any p ≥ q ≥ 1.
Now, we are ready to prove Theorem 3.3. In particular, we first include a high-level proof sketch,
then present the complete proof after.
Proof Sketch of Theorem 3.3. We start with the spherical Gaussian distribution where ν = γn. More
specifically, we are going to prove that for any E ⊆ Rn and η ≥ 0,
Yn(En'P)) ≥ Φ(Φ-1(γn(E)) + η) holds forP ≥ 2.	(A.1)
Note that for any vector x ∈ Rn, the mapping p → kxkp is monotonically decreasing for any p ≥ 1,
thus We can show that Erq) ⊆ E#P) holds for any P ≥ q ≥ 1. Making use of the standard Gaussian
Isoperimetric Inequality (Lemma 3.2), we then immediately obtain
Yn(EH)) ≥ Yn(Ea)) ≥ Φ(Φ-1(γn(E)) + *,for any p ≥ 2.
Moreover, to prove the concentration bound for general case where ν is the probability measure
ofN(θ, Σ), we build connections with the spherical Gaussian case by constructing a subset A =
{Σ-1∕2(x - θ) : x ∈ E}. Based on the affine transformation of Gaussian measure, we then prove:
V(E) = Yn(A) and V(EfP)) ≥ Yn(An'p)),	whereη = e∕k∑1∕2∣∣p.	(A.2)
Finally, combining (A.1) and (A.2) completes the proof of Theorem 3.3.	□
11
Published as a conference paper at ICLR 2021
Complete Proof of Theorem 3.3. To begin with, we consider the special case where the underlying
distribution is standard Gaussian (ν = γn). Specifically, we are going to prove that for any E ⊆ Rn
and η ≥ 0,
Yn(Efp)) ≥ Φ(Φ-1(γn(E)) + η) holds for any p ≥ 2.	(A.3)
Letp ≥	q ≥	1. According to the definition of -expansion ofa subset and Lemma A.1, we	have
Esq)= {x ∈ Rn : ∃ x0 ∈ E s.t. kx0 一 Xkq ≤ η}
⊆ {x ∈ Rn : ∃ x0 ∈ E s.t. kx0 — Xkp ≤ η} = E#P)	(A.4)
where the inclusion	is due to the fact that kx0 一 xkp ≤ kx0 一 xkq holds for any	x0 and	x.	Therefore,
by setting q = 2 in (A.4), we further obtain that for any p ≥ 2,
Yn(EH) ≥ Yn(EfG) ≥ Φ(Φ-1 (γn(E)) + e),
where the second inequality is due to the standard Gaussian Isoperimetric Inequality (Lemma 3.2).
Thus, we have proven (A.3).
Now we turn to proving the concentration bound for the general Gaussian case. Let UΛU> be the
eigenvalue decomposition of Σ, where U ∈ Rn×n is an orthonormal matrix and Λ ∈ Rn×n is a
diagonal matrix consisting of all the eigenvalues. Since Σ is positive definite, the square root of Σ
can be expressed as Σ1/2 = UΛ1∕2U>. Let Σ-1/2 = UΛ-1∕2U> be the inverse matrix of Σ1/2.
Construct a subset A in Rn such that A = {Σ-1∕2(x — θ) : X ∈ E}. Based on the construction of
A, we can then prove the following results for any E ⊆ Rn and ≥ 0:
V(E) = Yn(A) and V(ECpP) ≥ Yn(An'p)),	whereη = e∕k∑1∕2kp.	(A.5)
First, we prove the equality ν(E) = Yn(A). Since ν is the probability measure ofN(θ, Σ), we have
V(E) = Pr [X ∈	E]	= Pr	[Σ-1∕2(X	— θ) ∈	A]	= Pr	[u	∈	A]	=	Yn(A),	(A.6)
X〜V	X〜V	U〜Yn
where the third inequality is due to the affine transformation of Gaussian random variables.
Next, we prove the remaining inequality in (A.5). By definition, for any U ∈ Ayp), there exists
u ∈ A such that ku0 — ukp ≤ η. Let X0 = θ + Σ1∕2u0 and X = θ + Σ1∕2u, then we have
∣∣x0 - X∣∣p = ∣∣∑1∕2(u0 - u)∣∣p ≤ k∑1∕2kp ∙ ku0 - Ukp ≤ ηk∑1∕2kp ≤ e, (A.7)
where the first inequality is due to the definition of induced matrix p-norm and the last inequality
holds because η = e∕kΣ1∕2kp. By the construction of A and the fact that u ∈ A, we have X ∈ E.
Combining (A.7), this further implies that for any U ∈ Ayp), θ + Σ1∕2u0 ∈ Efp) Thus, we have
ν(E('P)) ≥ V(θ + Σ1∕2 ./n`p))=居[∑T∕2(x — θ) ∈ /n`p)] = Yn(AFp)),	(A.8)
where θ + Σ1∕2 ∙ Ayp) denotes the transformed subset {θ + Σ1∕2u : U ∈ Ayp)}. Therefore, based
on (A.6) and (A.8), we prove the soundness of (A.5).
Finally, combining (A.3) and (A.5) completes the proof of Theorem 3.3.	□
A.2 Proof of the Optimality Results in Remark 3.4
Proof. First, we prove the optimality for the spherical Gaussian case, where V = Yn and p > 2.
Let H = Hw,b be a half space with axis-aligned weight vector, that said w = ej for some j ∈ [n].
Intuitively speaking, the e-expansion of H with respect to `p -norm will only happen along the j -th
dimension. More rigorously, we are going to prove the following results: for any e ≥ 0,
H'P) = H呼2) holds for any P ≥ 1.	(A.9)
By definition, H = {X ∈ Rn : xj + b ≤ 0}. For any X ∈∕ H, let Xb ∈ H be the closest point of X in
terms of `p-norm. Since the weight vector w of H is axis-aligned, thus Xb will only differ from X by
12
Published as a conference paper at ICLR 2021
the j-th element. That said, xbj0 = xj0 for any j0 6= j and xbj = -b. Thus for any p ≥ 1, we have
kx - xbkp = kx - xbk2 = xj + b. Based on this observation, we further obtain that for any p ≥ 1,
HFp) = {x ∈ Rn : Xj + b ≤ e} = H('2),
which proves (A.9). According to the Gaussian Isoperimetric Inequality (Lemma 3.2), we obtain
YnU = Ynn =Φ(Φ-1(γn (H))+ O
Therefore, combining this with Theorem 3.3, we prove the optimality for the spherical Gaussian case.
Now we turn to prove the non-spherical Gaussian case with p = 2. Based on Theorem 3.3, the lower
bound is Φ(Φ-1(ν(E) + E∕k∑1∕2k2) whenP = 2. In the following, we are going to prove: if we
choose E = Hv1,b, where v1 is the eigenvector with respect to the largest eigenvalue of Σ, this lower
bound is attained. Similarly to the proof of Theorem 3.3, we construct A = {Σ-1∕2(x - θ) : X ∈ E}.
Note that when E is a half space, the constructed set A is also a half space. In particular, for the
case where E = Hvι,b, for any U ∈ A, there exists an X ∈ Rn such that U = Σ-1∕2(x - θ) and
v>x + b ≤ 0. This implies that v>Σ1∕2u + v>θ + b ≤ 0 for any U ∈ A. Since v1 is the eigenvector
of Σ, We further have that A is a half space with weight vector Σ1∕2v1 = ∣∣Σ1/21∣2 ∙ vι.
Note that according to (A.2), as in the proof of Theorem 3.3, for any E ⊆ Rn, we have
V(E) = Yn(A) and V(E('2)) ≥ γn(黑吟,where η = e∕k∑1∕2∣2.
For E = Hvι,b, based on the explicit formulation of '2-distance to a half space, we can explicitly
compute the η-expansion of A as
/匕'2)= {u ∈ Rn : v>Σ1∕2u + v>θ + b ≤ η ∙ k∑1∕2∣∣2}∙
When we set η = ∕∣Σ1∕2∣2, it further implies that
Yn(A柠)) = Pr ∣^v>Σ1∕2u + v>θ + b ≤ e^∣ = Pr ∣^v;X + b ≤ e^∣ = v(E('2)).
/	U 〜Yn	X 〜V
Finally, according to the optimality of the standard Gaussian Isoperimetric Inequality (Lemma 3.2),
this completes the proof.	□
B Proofs of Theoretical Results in Section 4
In this section, we present the proofs to the theoretical results presented in Section 4.
B.1	Proof of Lemma 4.1
Proof of Lemma 4.1. We only consider the case when w>X + b > 0, because dp(X, Hw,b) is zero
trivially holds if w>x + b ≤ 0. The problem of finding the 'p-distance from a given point X to a half
space Hw,b can be formulated as the following constrained optimization problem:
min ∣z - X∣p,	subject to w>z + b ≤ 0∙	(B.1)
z∈Rn
Let ze = z - X, then optimization problem (B.1) is equivalent to
min ∣ze∣p,	subject to w>ze+ w>X + b ≤ 0∙	(B.2)
ze∈Rn
According to Holder,s Inequality, for any e ∈ Rn We have
-kwkq ∙ Ilekp ≤ w>e ≤ Ilwkq ∙ Help,
where 1∕p + 1∕q = 1. Therefore, for any ze that satisfies the constraint of (B.2), we have
w>X + b ≤ -w>e ≤ kwkq ∙ kekp∙	(B.3)
Since kwk2 = 1, we have kwkq > 0, thus (B.3) further suggests kzekp ≥ (w>X + b)∕kwkq∙
13
Published as a conference paper at ICLR 2021
Up till now, we have proven that the optimal value of (B.1) is lower bounded by (w>x + b)/kwkq.
The remaining task is to show this lower bound can be achieved. To this end, we construct zb as
zbj = xj
w>x + b ( Wj
TnT ∙ (ρ∈n
1/p
for any j ∈ [n],
—
where 1/p + 1/q = 1. We remark that for the extreme case where p = ∞, such choice of zb can
be simplified as b = X - (w>x + b) ∙ Sgn(W)/kWkq, where sgn(∙) denotes the sign function for
vectors. According to the construction, it can be verified that
W>zb + b = (W>x + b) -
W>x + b
kwkq
• Ewj ∙
j∈[n]
Wq	γ∕p
P* Wj)
0,
and kzb- xkp = (W>x + b)/kWkq.
□
B.2 Proof of Theorem 4.2
Proof of Theorem 4.2. We write HS as HS(n) for simplicity. Let S be a set of size m sampled from
μ and μs be the corresponding empirical measure. Note that the VC-dimension of HS(n) is n +1
(see Mohri et al. (2018)), thus according to the VC inequality, we have
Pr [ SUp IbS(E) - μ(E)1 ≥ δ] ≤ 8e(n+1)log(m+1)-mδ2/32.
S一μm E∈HS(n)
In addition, according to Lemma 4.1, the -expansion of any half space is still a half space. Therefore,
we can directly apply Theorem 3.3 in Mahloujifar et al. (2019b) to bound the generalization of
concentration with respect to half spaces: for any δ ∈ (0, 1), we have
SPrmhhbp)(α - δ, e, HS) - δ ≤ h£。)(α, e, HS) ≤ hg)(α + δ, e, HS) + δ]
≥ 1 - 32e(n+1) log(m+1)-mδ2 /32 .
Finally, assuming the sample size m ≥ co • n log n∕δ2 for some constant co large enough, then there
exists positive constant c1 such that
hbp∖α - δ, e, HS) - δ ≤ h^(a,e, HS) ≤ 限(a + δ, e, HS) 十 δ
holds with probability at least 1 - ci • e-n log n.	□
C Algorithm for Estimating Concentration
Algorithm 1: Heuristic Search for Robust Half Space under 'p-distance
Input : a set of samples {xi}i∈[m]; strength (in `p-norm); risk threshold α; #iterations S.
Q J compute the sample covariance matrix based on {xi}i∈[m];
V J obtain the set of principal components by eigenvalue decomposition on Q;
for v ∈ V do
for s = 1, 2, . . . , S do
W J select from {±pow(v, s)};	// pow() is defined according to (C.1)
b J α-quantile of the set {-W>xi : i ∈ [m]};
AdvRiSk (Hw,b) J Pim=1 1(W>xi + b ≤ kWkq)/m;
end
end
(Wb, b) J argmin(w,b) AdvRiSk (Hw,b);
Output : Hwb,bb
14
Published as a conference paper at ICLR 2021
(a) N(0, I784)
(b) MNIST
(c) CIFAR-10
Figure 2: The convergence curves of the best possible adversarial risk estimated using our method
under various settings as the sample size of the training dataset increases.
To solve the empirical concentration problem (4.3), Algorithm 1 searches for a desirable half space
based on the principal components of the empirical dataset and their rotations defined by a power
parameter. More specifically, the function pow() takes a vector v ∈ Rn and a positive integer s ∈ Z+,
and returns the normalized s-th power of v (with sign preserved):
pow(v, s) = sgn(v) ◦ [abs(v)]s/kvsk2 = v /kv k2,s	s	ifsisodd;	(C.1)
sgn(v) ◦ vs/kvsk2,	otherwise.
Note that all the functions used in (C.1) are element-wise operations for vectors, where sgn(v),
abs(v), vs represent the sign, absolute value and the s-th power of v respectively, and the operator ◦
denotes the Hardamard product of two vectors.
Connected with the theoretical optimum regarding Gaussian spaces in Remark 3.4, the top principal
component corresponds to the optimal choice of W if the perturbation metric is '2-distance, whereas
close-to-axis would be favourable for w when p > 2. In addition, as implied by the empirical
concentration problem (4.3) and the monotonicity of `p-mapping (Lemma A.1), the value of kwkq
will be more influential in affecting the e-expansion of half space as P grows larger. For example,
the '∞ -norm of W can be as large as √n for the worst case (n denotes the input dimension), while
kw k∞ = 1 if w aligns any axis. By searching through the region between each principal component
and the closest axis, the proposed algorithm aims to find the optimal balance between kW kq and
the variance of the given data along W that leads to the smallest e-expansion. Although there is no
theoretical guarantee that our algorithm will find the optimum to (4.3) for an arbitrary dataset, we
empirically show (in Section 6) its efficacy in estimating concentration across various datasets.
Moreover, our algorithm is efficient in terms of both time and space complexities. Precomputing
the principal components requires O(mn2 + n3) time and O(n2) space to store them, where m
denotes the samples size and n is the input dimension. For each iteration step, the time complexity
of computing W, b and AdvRisk (Hw,b) is O(mn), while the space complexity for saving the
intermediate variables and the best parameters is O(m + n). With n outer iterations and S inner
iterations, the total time complexity is O(n3 + mn2S). The total space complexity is O(n2 + mn),
where the extra O(mn) denotes the initial space requirement for saving all the input data. For
our experiments, we observe AdvRisk(Hw,b) is not sensitive to small increment of the exponent
parameter s, thus we choose to increase s in a more aggressive way, which further saves computation.
D Additional Experiments
Figure 2 shows the convergence performance of our algorithm under different experimental settings:
α = 0.5, e = 1 for the simulated Gaussian dataset, α = 0.01, e = 0.4 for MNIST, and α =
0.05, e = 16/255 for CIFAR-10. Under these additional settings, the algorithm proposed by
Mahloujifar et al. (2019b) either cannot provide meaningful estimates of concentration, or takes a
substantial amount of time to run. For instance, our algorithm takes around 2 days to generate the
convergence curve on CIFAR-10 (α = 0.05, e = 16/255), whereas the previous method is at least 5
times slower, due to the large number of rectangles T needed. Thus, we only report the convergence
curves of our method, where the standard deviations are calculated over 3 repeated trials.
15