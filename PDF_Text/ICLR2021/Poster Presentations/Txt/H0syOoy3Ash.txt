Published as a conference paper at ICLR 2021
Average-case Acceleration
Games and Normal Matrices
Carles Domingo-Enrich
Computer Science Department
Courant Institute of Mathematical Sciences
New York University
New York, NY 10012, USA
cd2754@nyu.edu
for Bilinear
Fabian Pedregosa
Google Research
pedregosa@google.com
Damien Scieur
Samsung SAIT AI Lab & Mila
Montreal, Canada
damien.scieur@gmail.com
Ab stract
Advances in generative modeling and adversarial learning have given rise to re-
newed interest in smooth games. However, the absence of symmetry in the ma-
trix of second derivatives poses challenges that are not present in the classical
minimization framework. While a rich theory of average-case analysis has been
developed for minimization problems, little is known in the context of smooth
games. In this work we take a first step towards closing this gap by develop-
ing average-case optimal first-order methods for a subset of smooth games. We
make the following three main contributions. First, we show that for zero-sum
bilinear games the average-case optimal method is the optimal method for the
minimization of the Hamiltonian. Second, we provide an explicit expression for
the optimal method corresponding to normal matrices, potentially non-symmetric.
Finally, we specialize it to matrices with eigenvalues located in a disk and show a
provable speed-up compared to worst-case optimal algorithms. We illustrate our
findings through numerical simulations with a varying degree of mismatch with
our assumptions.
1 Introduction
The traditional analysis of optimization algorithms is a worst-case analysis (Nemirovski, 1995; Nes-
terov, 2004). This type of analysis provides a complexity bound for any input from a function class,
no matter how unlikely. However, since hard-to-solve inputs might rarely occur in practice, the
worst-case complexity bounds might not be representative of the observed running time.
A more representative analysis is given by the average-case complexity, averaging the algorithm’s
complexity over all possible inputs. This analysis is standard for analyzing, e.g., sorting (Knuth,
1997) and cryptography algorithms (Katz & Lindell, 2014). Recently, a line of work (Berthier et al.,
2020; Pedregosa & Scieur, 2020; Lacotte & Pilanci, 2020; Paquette et al., 2020) focused on optimal
methods for the optimization of quadratics, specified by a symmetric matrix. While worst-case
analysis uses bounds on the matrix eigenvalues to yield upper and lower bounds on convergence,
average-case analysis relies on the expected distribution of eigenvalues and provides algorithms
with sharp optimal convergence rates. While the algorithms developed in this context have been
shown to be efficient for minimization problems, these have not been extended to smooth games.
A different line of work considers algorithms for smooth games but studies worst-case optimal
methods (Azizian et al., 2020). In this work, we combine average-case analysis with smooth games,
and develop novel average-case optimal algorithms for finding the root of a linear system determined
by a (potentially non-symmetric) normal matrix. We make the following main contributions:
1
Published as a conference paper at ICLR 2021
1.	Inspired by the problem of finding equilibria in smooth games, we develop average-case opti-
mal algorithms for finding the root of a non-symmetric affine operator, both under a normality
assumption (Thm. 4.1), and under the extra assumption that eigenvalues of the operator are sup-
ported in a disk (Thm. 4.2). The proposed method shows a polynomial speedup compared to the
worst-case optimal method, verified by numerical simulations.
2.	We make a novel connection between average-case optimal methods for optimization, and
average-case optimal methods for bilinear games. In particular, we show that solving the Hamil-
tonian using an average-case optimal method is optimal (Theorem 3.1) for bilinear games. This
result complements (Azizian et al., 2020), who proved that Polyak Heavy Ball algorithm on the
Hamiltonian is asymptotically worst-case optimal for bilinear games.
2 Average-case analysis for normal matrices
In this paper we consider the following class of problems.
Definition 1. Let A ∈ Rd×d be a real matrix and x? ∈ Rd a vector. The non-symmetric (affine)
operator (NSO) problem is defined as:
Find x : F (x) d=ef A(x-x?) = 0 .	(NSO)
This problem generalizes that of minimization of a convex quadratic function f, since we can cast the
latter in this framework by setting the operator F = Vf. The set of solutions is an affine subspace
that we will denote X ? . We will find convenient to consider the distance to this set, defined as
dist(x, X?) d=ef min kx - vk2, with X? = {x ∈ Rd | A(x - x?) = 0} .	(1)
In this paper we will develop average-case optimal methods. For this, we consider A and x? to
be random vectors, and a random initialization x0 . This induces a probability distribution over
NSO problems, and we seek to find methods that have an optimal expected suboptimality w.r.t.
this distribution. Denoting E(A,x?,x0) the expectation over these random problems, we have that
average-case optimal methods they verify the following property at each iteration t
minE(A,χ*,χo) dist(xt, X?) s.t. Xi ∈ xo + span({F(Xj)}j=0), ∀i ∈ [1 : t].	(2)
Xt
The last condition on xt stems from restricting the class of algorithms to first-order methods. The
class of first-order methods encompasses many known schemes such as gradient descent with mo-
mentum, or full-matrix AdaGrad. However, methods such as Adam (Kingma & Ba, 2015) or diag-
onal AdaGrad (Duchi et al., 2011) are not in this class, as the diagonal re-scaling creates iterates Xt
outside the span of previous gradients. Although we will focus on the distance to the solution, the
results can be extended to other convergence criteria such as kF(Xt)k2.
Finally, note that the expectations in this paper are on the problem instance and not on the random-
ness of the algorithm.
2.1	Orthogonal residual polynomials and first-order methods
The analysis of first-order methods simplifies through the use of polynomials. This section provides
the tools required to leverage this connection.
Definition 2. A residual polynomial is a polynomial P that satisfies P (0) = 1.
Proposition 2.1. (Hestenes et al., 1952) If the sequence (Xt)t∈Z+ is generated by a first-order
method, then there exist residual polynomials Pt, each one of degree at most t, verifying
Xt - X? = Pt(A)(X0 - X?).	(3)
As we will see, optimal average-case method are strongly related to orthogonal polynomials. We
first define the inner product between polynomials, where We use z* for the complex conjugate of
z∈C.
2
Published as a conference paper at ICLR 2021
Definition 3. For P, Q ∈ R[X ], we define the inner product(∙, •)* for a measure μ over C as
hP,Qiμ
d=ef
C
P(λ)Q(λ)* dμ(λ).
(4)
Definition 4. A sequence ofpolynomials {Pi} is orthogonal (resp. orthonormal) w.r.t,(•，)* if
(Pi, Piiμ > 0 (resp. = 1)；	(Pi, Pjiμ = 0 if i = j.
2.2	Expected Spectral Distribution
Following (Pedregosa & Scieur, 2020), we make the following assumption on the problem family.
Assumption 1. xo 一 x? is independent of A, and E(Xo, χ*)[(xo — x?)(xo — x?)>] = R Id.
We will also require the following definitions to characterize difficulty of a problem class. Let
{λ1 , . . . , λd} be the eigenvalues of a matrix A ∈ Rd×d. We define the empirical spectral distri-
bution of A as the probability measure
μA(λ) d=f d Pd=ιδλi (λ),	(5)
where δλi is the Dirac delta, a distribution equal to zero everywhere except at λi and whose integral
over the entire real line is equal to one. Note that with this definition, JD d∕^A(λ) corresponds to the
proportion of eigenvalues in D.
When A is a matrix-valued random variable, μA is a measure-valued random variable. As such, We
can define its expected spectral distribution
μA =f E a[∕^a] ,	(6)
which by the Riesz representation theorem is the measure that verifies Jf dμ = E a[∕ f dμA] for
all measureable f. Surprisingly, the expected spectral distribution is the only required characteristic
to design optimal algorithms in the average-case.
2.3	Expected error of first-order methods
In this section we provide an expression for the expected convergence in terms of the residual poly-
nomial and the expected spectral distribution introduced in the previous section. To go further in the
analysis, we have to assume that A is a normal matrix.
Assumption 2. The (real) random matrix A is normal, that is, it verifies AA> = A>A.
Normality is equivalent to A having the spectral decomposition A = UΛU*, where U is unitary,
i.e., U*U = UU* = I. We now have everything to write the expected error of a first-order
algorithm applied to (NSO).
Theorem 2.1.	Consider the application of a first-order method associated to the sequence
of polynomials {Pt} (Proposition 2.1) on the problem (NSO). Let μ be the expected spectral
distribution of A. Under Assumptions 1 and 2, we have
E[dist(xt, X?)]
R2 [	|Pt|2 dμ,
C\{0}
(7)
Before designing optimal algorithms for certain specific distributions, we compare our setting with
the average-case accelerating for minimization problems of Pedregosa & Scieur (2020), who pro-
posed optimal optimization algorithms in the average-case.
2.4 Difficulties of First-Order Methods on Games and Related Work
This section compares our contribution with the existing framework of average-case optimal meth-
ods for quadratic minimization problems.
3
Published as a conference paper at ICLR 2021
Definition 5. Let H ∈ Rd×d be a random symmetric positive-definite matrix and x? ∈ Rd a
random vector. These elements determine the following random quadratic minimization problem
minχ∈Rd {f(x) d=f 2(x-x*)>H(x-x*)} .	(OPT)
As in our paper, Pedregosa & Scieur (2020) find deterministic optimal first-order algorithms in ex-
pectation w.r.t. the matrix H, the solution x?, and the initialization x0. Since they work with
problem (OPT), their problem is equivalent to (NSO) with the matrix A = H. However, they have
the stronger assumption that the matrix is symmetric, which implies being normal. The normality
assumption is restrictive in the case of game theory, as they do not always naturally fit such applica-
tions. However, this set is expressive enough to consider interesting cases, such as bilinear games,
and our experiments show that our findings are also consistent with non-normal matrices.
Using orthogonal residual polynomials and spectral distributions, they derive the explicit formula
of the expected error. Their result is similar to Theorem 2.1, but the major difference is the domain
of the integral, a real positive line in convex optimization, but a shape in the complex plane in our
case. This shape plays a crucial role in the rate of converge of first-order algorithms, as depicted in
the work of Azizian et al. (2020); Bollapragada et al. (2018).
In the case of optimization methods, they show that optimal schemes in the average-case follow a
simple three-term recurrence arising from the three-term recurrence for residual orthogonal poly-
nomials for the measure λμ(λ). Indeed, by Theorem 2.1 the optimal method corresponds to the
residual polynomials minimizinghP, P)*, and the following result holds:
Theorem 2.2.	(Fischer, 1996, §2.4) When μ is supported in the real line, the residual polynomial of
degree t minimizing hP, P)* is given by the degree t residual orthogonal polynomial w.r.t. λμ(λ).
However, the analogous result does not hold for general measures in C, and hence our arguments
will make use of the following Theorem 2.3 instead, which links the residual polynomial of degree
at most t that minimizeshP, P)* to the sequence of orthonormal polynomials for μ.
Theorem 2.3. [Theorem 1.4 OfASSche (1997)] Let μ be a positive Borel measure in the complex
plane. The minimum of the integral JC |P (λ)∣2 dμ(λ) over residual polynomials P of degree lower
or equal than t is uniquely attained by the polynomial
P？(λ) = Ek=0 "k(')"k(0) , with optimal value Z |P*(λ)∣2 dμ(λ) = —ɪ----------------, (8)
Ptk= ∣φk (0)l2	C	Ptk=腐(0)l2
where (φk )k is the orthonormal SeqUence ofpolynomials with respect to the inner product(•，•〉兴.
In the next sections we consider cases where the optimal scheme is identifiable.
3 Average-case Optimal Methods for B ilinear Games
We consider the problem of finding a Nash equilibrium of the zero-sum minimax game given by
minmax'® θ?) =f (θι - θf)>M包-θ?).
θ1 θ2
(9)
Let θ1, θ1? ∈ Rd1 , θ2, θ2? ∈ Rd2, M ∈ Rd1×d2 and d d=ef d1 + d2. The vector field of the game
(Balduzzi et al., 2018) is defined as F(x) = A(x - x?), where
F(θ1, θ2)
.Vθι'(θι, θ2)
-Vo?'(θι, θ2)
{z^
=A
0M
-M>	0
=x	=x
θ12?	= A(x - x?) .
(10)
|
}
As before, X? denotes the set of points x such that F (x) = 0, which is equivalent to the set of
Nash equilibrium. If M is sampled independently from x0, x? and x0 一 x? has covariance Rd-Id,
Assumption 1 is fulfilled. Since A is skew-symmetric, it is in particular normal and Assumption 2
is also satisfied.
4
Published as a conference paper at ICLR 2021
We now show that the optimal average-case algorithm to solve bilinear problems is Hamiltonian
gradient descent with momentum, described below in its general form. Contrary to the methods in
Azizian et al. (2020), the method we propose is anytime (and not only asymptotically) average-case
optimal.
Optimal average-case algorithm for bilinear games.
Initialization. x-1 = x0 = θ1,0, θ2,0 , sequence {ht, mt} given by Theorem 3.1.
Main loop. For t ≥ 0,
gt = F(Xt- F(Xt))- F(Xt)	(= 2VkF(xt)k2 by (12))
xt+1 = xt - ht+1gt + mt+1 (xt-1 - xt)
(11)
The quantity 2 ∣∣F (x) k2 is commonly known as the Hamiltonian of the game (Balduzzi et al., 2018),
hence the name Hamiltonian gradient descent. Indeed, gt = V (2∣∣F(x)∣∣2) When F is afine:
F(X - F (X)) - F(X) = A(X - A(X - X?) - X?) - A(X - X?) = -A(A(X - X?))
A>(A(x — x?)) = V (2∣A(x - x?)k2
(12)
The following theorem shows that (11) is indeeed the optimal average-case method associated to the
minimization problem min∣ (2l∣F(x)∣2), as the following theorem shows.
Theorem 3.1. Suppose that Assumption 1 holds and that the expected spectral distribution of
MM> is absolutely continuous with respect to the Lebesgue measure. Then, the method (11)
is average-case optimal for bilinear games when ht, mt are chosen to be the coefficients of the
average-case optimal minimization of 2 IlF(x)∣2.
HoW to find optimal coefficients? Since 2IlF(x)k2 is a quadratic problem, the coefficients
{ht , mt } can be found using the average-case framework for quadratic minimization problems of
(Pedregosa & Scieur, 2020, Theorem 3.1).
Proof sketch. When computing the optimal polynomial Xt = Pt(A)(X0 - X?), we have that the
residual orthogonal polynomial Pt behaves differently if t is even or odd.
•	Case 1: t is even. In this case, we observe that the polynomial Pt (A) can be expressed as
Qt/2(-A2), where (Qt)t≥0 is the sequence of orthogonal polynomials w.r.t. the expected spectral
density of -A2, whose eigenvalues are real and positive. This gives the recursion in (11).
•	Case 2: t is odd. There is no residual orthogonal polynomial of degree t for t odd. Instead, odd
iterations do correspond to the intermediate computation ofgt in (11), but not to an actual iterate.
3.1 PARTICULAR CASE: M WITH I.I.D. COMPONENTS
We now show the optimal method when the entries of M are i.i.d. sampled. For simplicity, we
order the players such that d1 ≤ d2.
Assumption 3. Assume that each component of M is sampled iid from a distribution of mean 0 and
variance σ2, and we take d1,d2 → ∞ with d1 → r < 1.
In such case, the spectral distribution of 女 MM> tends to the Marchenko-Pastur law, supported in
[`, L] and with density:
PMP(λ) =f VZ(L ?(： ' , where L =f σ2(1 + √r)2,' =f σ2(1 — √r)2.	(13)
2πσ 2 rλ
5
Published as a conference paper at ICLR 2021
Proposition 3.1. When M satisfies Assumption 3, the optimal parameter of scheme (11) are
ht = - σ2√, mt = 1 + Pδt, where P = 1√r, δt = (-p - δt-1)-1, δ0 = 0.	(14)
Proof. By Theorem 3.1, the problem reduces to finding the optimal average-case algorithm for the
problem minx 2∣∣F(x)k2. Since the expected spectral distribution of aMM> is the Marchenko-
Pastur law, We can use the optimal algorithm from (Pedregosa & Scieur, 2020, Section 5).	□
4 General average-case optimal method for normal operators
In this section we derive general average-case optimal first-order methods for normal operators.
First, we need to assume the existence of a three-term recurrence for residual orthogonal polynomials
(Assumption 4). As mentioned in subsection 2.4, for general measures in the complex plane, the
existence ofa three-term recurrence of orthogonal polynomials is not ensured. In Proposition B.3 in
Appendix B we give a sufficient condition for its existence, and in the next subsection we will show
specific examples where the residual orthogonal polynomials satisfy the three-term recurrence.
Assumption 4 (Simplifying assumption). The sequence of residual polynomials {ψt}t≥0 orthogo-
nal w.r.t. the measure μ, defined on the complex plane, admits the three-term recurrence
ψ-1 =	0,	ψ0	= 1,	ψt(λ)	=	(at +	btλ)ψt-1	(λ)	+ (1 - at)ψt-2(λ).	(15)
Under Assumption 4, Theorem 4.1 shows that the optimal algorithm can also be written as an aver-
age of iterates following a simple three-terms recurrence.
Theorem 4.1.	Under Assumption 4 and the assumptions of Theorem 2.1, the following algo-
rithm is optimal in the average case, with y-1 = y0 = x0:
yt = atyt-1 + (1 - at)yt-2 + btF(yt-1)
Xt = R	XtT	+	&	yt,	βt = φ2(0),	Bt	=	Bt-I + βt-ι,	Bo	= 0 .	(16)
Bt + βt	Bt + βt
where (φk (0))k≥0 can be computed using the three-term recurrence (upon normalization).
Moreover, E (A,x?,x0) dist(Xt, X?) converges to zero at rate 1/Bt.
Remark. Notice that it is not immediate that (16) fulfills the definition of first-order algorithms
stated in (2), as yt is clearly a first-order method but Xt is an average of the iterates yt . Using that
F is an affine function we see that Xt indeed fulfills (2).
Remark. Assumption 4 is needed for the sequence (yt)t≥0 to be computable using a three-term
recurrence. However, for some distribution, the associated sequence of orthogonal polynomials may
admit another recurrence that may not satisfy Assumption 4.
4.1	Circular spectral distributions
In random matrix theory, the circular law states that ifA is an n × n matrix with i.i.d. entries of mean
C and variance R2 /n, as n → ∞ the spectral distribution of A tends to the uniform distribution
on DC,R. In this subsection we apply Theorem 4.1 to a class of spectral distributions specified by
Assumption 5, which includes the uniform distribution on DC,R. Even though the random matrices
with i.i.d entries are not normal, we will see in section 6 that the empirical results for such matrices
are consistent with our theoretical results under the normality assumption.
Assumption 5. Assume that the expected spectral distribution μA is supported in the complex plane
on the disk DC,R of center C ∈ R, C > 0 and radius R < C. Moreover, assume that the spectral
density is CirCUIarIy symmetric, i.e. there exists a probability measure μR SUPPOrted on [0, R] such
for all f measurable and r ∈ [0, R], dμA(C + reiθ) = 2∏ dθ dμκ(r).
Proposition 4.1. If μ satisfies Assumption 5, the SeqUenCe of orthonormal polynomials is (φt)t≥o,
φt(λ) = (λZ- C) , where Kt,R = qRR r2t d〃R(r) .	(17)
Kt,R
6
Published as a conference paper at ICLR 2021
Example. The uniform distribution in Dc,r is to dμR = R dr, and Kt,R = Rt/√t +1.
From Proposition 4.1, the sequence of residual polynomials is given by φt(λ)∕φt(0) =(1 - C)t,
which implies that Assumption 4 is fulfilled with at = 1, bt = — C. Thus, by Theorem 4.1 we have
Theorem 4.2.	Given an initialization x0(y0 = x0), if Assumption 5 is fulfilled with R < C
and the assumptions of Theorem 2.1 hold, then the average-case optimal first-order method is
yt =	yt-ι	-	cCF(ytT),	βt	=	CC2/Klr,	Bt	= Bt-I	+ βt-1,
Bt	βt
xt =瓦可XT +瓦可yt.
Moreover, E (A,x?,x0) dist(xt, X?) converges to zero at rate 1/Bt.
(18)
We now compare Theorem 4.2 with worst-case methods studied in Azizian et al. (2020). They give
a worst-case convergence lower bound of (R/C)2t on the quantity dist(zt, X?) for first-order meth-
ods (zt)t≥0 on matrices with eigenvalues in the disk DC,R. By the classical analysis of first-order
methods, this rate is achievable by gradient descent with stepsize 1/C, i.e. the iterates yt defined in
(18). However, by equation (79) in Proposition D.3 we have that under slight additional assumptions
(those of Proposition 5.2), limt→∞ E [dist(xt, X?)]/E [dist(yt, X?)] = 1 - C holds. That is, the
average-case optimal algorithm outperforms gradient descent by a constant factor depending on the
conditioning R/C.
5 Asymptotic behavior
The recurrence coefficients of the average-case optimal method typically converges to limiting val-
ues when t → ∞, which gives an ”average-case asymptotically optimal first-order method” with
constant coefficients. For the case of symmetric operators with spectrum in [`, L], Scieur & Pe-
dregosa (2020) show that under mild conditions, the asymptotically optimal algorithm is the Polyak
momentum method with coefficients depending only on ` and L. For bilinear games, since the
average-case optimal algorithm is the average-case optimal algorithm of an optimization algorithm,
we can make use of their framework to obtain the asymptotic algorithm (see Theorem 3 of Scieur &
Pedregosa (2020)).
Proposition 5.1. Assume that the expected SPectral density Mmm> of MM> is SuPPorted in [', L]
for 0 < ` < L, and strictly positive in this interval. Then, the asymptotically optimal algorithm for
bilinear games is the following version of Polyak momentum:
gt = F(xt - F(xt)) - F(xt)
xt+1 = xt + ( √L^)2 (XtT- Xt)- ( √L+√')2 gt
(19)
Notice that the algorithm in (19) is the worst-case optimal algorithm from Proposition 4 of Azizian
et al. (2020). For the case of circularly symmetric spectral densities with support on disks, we can
also compute the asymptotically optimal algorithm.
Proposition 5.2. Suppose that the assumptions of Theorem 4.2 hold with μR ∈ P([0, R]) fulfilling
μR([r, R]) = Ω((R — r)κ) for r in [ro, R] for some r° ∈ [0, R) and for some K ∈ Z. Then, the
average-case asymptotically optimal algorithm is, with y0 = X0:
yt = yt-1 - CF(yt-1),
(20)
Moreover, the convergence rate for this algorithm is asymptotically the same one as for the optimal
algorithm in Theorem 4.2. Namely, limt→∞ E [dist(Xt, X?)]Bt = 1.
The condition on μR simply rules out cases in which the spectral density has exponentially small
mass around 1. It is remarkable that in algorithm (20) the averaging coefficients can be expressed so
simply in terms of the quantity R/C. Notice also that while the convergence rate of the algorithm
7
Published as a conference paper at ICLR 2021
is slower than the convergence rate for the optimal algorithm by definition, both rates match in the
limit, meaning that the asymptotically optimal algorithm also outperforms gradient descent by a
constant factor 1 - R in the limit t → ∞.
6	Experiments
We compare some of the proposed methods on settings with varying degrees of mismatch with our
assumptions.
Bilinear Games. We consider min-max bilinear problems of the form (10), where the entries of
M are generated i.i.d. from a standard Gaussian distribution. We vary the ratio r = d/n parameter
for d = 1000 and compare the average-case optimal method of Theorems 3.1 and 5.1, the asymp-
totic worst-case optimal method of (Azizian et al., 2020) and extragradient (Korpelevich, 1976). In
all cases, we use the convergence-rate optimal step-size assuming knowledge of the edges of the
spectral distribution.
The spectral density for these problems is displayed in the first row of Figure 1 and the benchmark
results on the second row. Average-case optimal methods always outperform other methods, and the
largest gain is in the ill-conditioned regime (r ≈ 1).
Circular Distribution. For our second experiment we choose A as a matrix with iid Gaussian
random entries, therefore the support of the distribution of its eigenvalue is a disk. Note that A
does not satisfy the normality assumption of Assumption 2. Figure 1 (third row) compares the
average-case optimal methods from Theorems 4.2 and 5.2 on two datasets with different levels of
conditioning. Note that the methods converge despite the violation of Assumption 2, suggesting a
broader applicability than the one proven in this paper. We leave this investigation for future work.
7	Discussion and Future Research Directions
In this paper, we presented a general framework for the design of optimal algorithms in the average-
case for affine operators F, whose underlying matrix is possibly non-symmetric. However, our
approach presents some limitations, the major one being the restriction to normal matrices. Fortu-
nately, the numerical experiments above suggests that this assumption can be relaxed. Developing a
theory without that assumption is left for future work. Another avenue for future work is to analyze
the nonlinear-case in which the non-symmetric operator A is non-linear, as well as the case in which
itis accessed through a stochastic estimator (as done by (Loizou et al., 2020) for the worst-case anal-
ysis).
Acknowledgements
C. Domingo-Enrich has been partially funded by “la Caixa” Foundation (ID 100010434), under
agreement LCF/BQ/AA18/11680094, and partially funded by the NYU Computer Science Depart-
ment.
8
Published as a conference paper at ICLR 2021
Bilinear Problems
r=0.9	r=0.95
r=1.0	r=1.2
宜suəp ənmuəM-ə
5
-2
・°-
5
-2
5
-2
5
-2
-A- Average-case Acceleration (Th. 3.2)	—Azizian et al. 2020	Extragradient
Problems with Circular Law
Suboptimality
EigenValUe DiStTibUtiOn
耳 rge gap = WeII-Cclnditiclned
0.0	0.5	1.0
Real(λ)
iterations
EigenValUe DiStribUtiOn
5mall gap = irCOnditiClned
0.0	0.5	1.0
Real(λ)
-λ- Average-case Acceleration (Th. 4.2)	-λ- Average-case Acceleration (Th. 5.2)	→- ExtraGradient —Gradient Descent
5
2
O
5
2
υ
5
2
υ
5
2
Figure 1: Benchmarks and spectral density for different games. Top row: spectral density asso-
ciated with bilinear games for varying values of the ratio parameter r = n/d (the x-axis represents
the imaginary line). Second row: Benchmarks. Average-case optimal methods always outperform
other methods, and the largest gain is in the ill-conditioned regime (r ≈ 1). Third row. Bench-
marks (columns 1 and 3) and eigenvalue distribution of a design matrix generated with iid entries
for two different degrees of conditioning. Depite the normality assumption not being satisfied, we
still observe an improvement of average-case optimal methods vs worst-case optimal ones.
9
Published as a conference paper at ICLR 2021
References
Walter Van Assche. Orthogonal polynomials in the complex plane and on the real line. In Fields
Institute Communications, volume 14, pp. 211-245, 1997.
Walss Azizian, Damien Scieur, Ioannis Mitliagkas, Simon Lacoste-JUlien, and GaUthier GideL Ac-
celerating smooth games by manipulating spectral shapes. In Proceedings of Machine Learning
Research, 2020.
David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, and Thore Grae-
pel. The mechanics of n-player differentiable games. In Proceedings of the International Con-
ference on Machine Learning, 2018.
Raphael Berthier, Francis Bach, and Pierre Gaillard. Accelerated gossip in networks of given di-
mension using Jacobi polynomial iterations. SIAM Journal on Mathematics of Data Science, 2
(1):24-47, 2020.
Raghu Bollapragada, Damien Scieur, and Alexandre d’Aspremont. Nonlinear acceleration of mo-
mentum and primal-dual algorithms. arXiv preprint arXiv:1810.04539, 2018.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12:2121-2159, 2011.
Bernd Fischer. Polynomial Based Iteration Methods for Symmetric Linear Systems.
Vieweg+Teubner Verlag, 1996.
Magnus R Hestenes, Eduard Stiefel, et al. Methods of conjugate gradients for solving linear systems.
Journal of research of the National Bureau of Standards, 1952.
Jonathan Katz and Yehuda Lindell. Introduction to modern cryptography. CRC press, 2014.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2015.
Donald Knuth. The art of computer programming, volume 3. Pearson Education, 1997.
GM Korpelevich. The extragradient method for finding saddle points and other problems. Matecon,
12, 1976.
Jonathan Lacotte and Mert Pilanci. Optimal randomized first-order methods for least-squares prob-
lems. Proceedings of the 37th International Conference on Machine Learning, 2020.
Nicolas Loizou, Hugo Berard, Alexia Jolicoeur-Martineau, Pascal Vincent, Simon Lacoste-Julien,
and Ioannis Mitliagkas. Stochastic Hamiltonian gradient methods for smooth games. arXiv
preprint arXiv:2007.04202, 2020.
Arkadi Nemirovski. Information-based complexity of convex programming. Lecture Notes, 1995.
Yurii Nesterov. Introductory Lectures on Convex Optimization. Springer, 2004.
Courtney Paquette, Bart van Merrienboer, and Fabian Pedregosa. Halting time is pre-
dictable for large models: A universality property and average-case analysis. arXiv preprint
arXiv:2006.04299, 2020.
Fabian Pedregosa and Damien Scieur. Average-case acceleration through spectral density estima-
tion. In Proceedings of the 37th International Conference on Machine Learning, 2020.
Damien Scieur and Fabian Pedregosa. Universal average-case optimality of Polyak momentum. In
Proceedings of the 37th International Conference on Machine Learning, 2020.
10
Published as a conference paper at ICLR 2021
A	Proof of Theorem 2.1
A.1 Preliminaries
Before proving Theorem 2.1, we quickly analyze the distance function (1), recalled below,
dist(x, X?) d=ef min kx - vk2.
The definition of the distance function is not practical for the theoretical analysis. Fortunately, it
is possible to find a simple expression that uses the orthogonal projection matrix Π to the kernel
Ker(A). Since Π is an orthogonal projection matrix to the kernel of a linear transformation, it
satisfies
Π = ΠT,	Π2 = Π, and AΠ = 0.	(21)
The normality assumption on A implies also that
ΠA = 0.	(22)
Indeed, the spectral decomposition of A is
A = [U1U2] λ 0 [U1∣U2]*,
and then Π = UU. The next proposition uses Π to derive the explicit solution of the (1).
Proposition A.1. We have that
dist(y, X?) = k(I - Π)(y - x?)k2 ∀x? ∈ X?.
Proof. We first parametrize the set of solution X?. By definition we have
X? = {x : A(x - x?) = 0}.
Which can be written in terms of the kernel of A as
X? = {x? + Πw : w ∈ Rd}.
From this, we can rewrite the distance function (1) as
dist(y, X?) = min ky - (x? + Πw)k2.
w∈Rd
The minimum can be attained at different points, but in particular at w = -(y - x?), which proves
the statement.	□
We now simplifies further the result of the previous proposition in the case where xt is generated by
a first order method.
Proposition A.2. For every iterate xt of a first-order methods, i.e., xt satisfies
xt - x? = Pt(A)(x0 - x?),	deg(Pt) ≤ t, P(0) = I,
we have that
dist(xt, X?) = kxt - x?k2 - kΠ(x0 - x?)k2.
Proof. We start with the result of Proposition A.1,
dist(xt, X?) = k(I - Π)(xt - x?)k2.
The norm can be split into
k(I- Π)(xt-x*)k2 = kxt-x*k2 + k ∣∏Z} (xt-x*)k2- 2k∏(xt-x?)k2
= kxt - x?k2 - kΠ(xt - x?)k2.
Since xt is generated by a first order method, we have
xt - x? = Pt(A)(x0 - x?),	Pt(0) = 1.
11
Published as a conference paper at ICLR 2021
Since P(0) = 1, the polynomial can be factorized as P(A) = I + AQt-1 (A), Qt-1 being a
polynomial of degree t - 1. Therefore, kΠ(xt - x?)k2 reads
kΠ(xt - x?)k2 = kΠ (I + AQt-1(A)) (x0 - x?)k2
=k∏(xo - x?)+ ΠA Qt-1 (A)(xo - x?)k2
=0 by (22)
= kΠ(x0 - x?)k2,
which prove the statement.
□
A.2 Proof of the theorem
We are now ready to prove the main result.
Theorem 2.1. Consider the application of a first-order method associated to the sequence of poly-
nomials {Pt} (Proposition 2.1) on the problem (NSO). Let μ be the expected spectral distribution of
A. Under Assumptions 1 and 2, we have
E[dist(xt, X?)] = R2 [	|Pt|2 dμ,
C\{0}
(7)
Proof. We start with the result of Proposition A.2,
dist(xt, X?)= kxt - x?k2 - kΠ(x0 - x?)k2.
We now write the expectation of the distance function,
E[dist(xt, X?)] = E hkxt - x?k2 - kΠ(x0 - x?)k2i
= E hkPt(A)(x0 - x?)k2 - kΠ(x0 - x?)k2i
= E htrPt(A)Pt(A)T (x0 - x?) (x0 - x?)T - tr Π2 (x0 - x?)(x0 - x?)Ti
= EA htrPt(A)Pt(A)TE h(x0 - x?) (x0 - x?)T |Ai - trΠE (x0 - x?)(x0 - x?)T|Ai
=REA trPt(A)Pt(A)T -trΠ
RE
RE
d
X |P(λi)∣2 - tr∏
i=1
/	|P(λ)∣2δλi(λ) + ∣P(0)∣2 ∙ [# zero eigenvalues] - trΠ
C\{0}
However, |P (0)|2 = 1 and trΠ corresponds to the number of zero eigenvalues of A, therefore,
E[dist(xt, X?)] = RE
I	|P (λ)∣2δλi (λ)
C\{0}
R /	P (λ)μ(λ).
□
B Proofs of Theorem 3.1 and Proposition 3.1
Proposition B.1. [Block determinant formula] If A, B, C, D are (not necessarily square) matrices,
det
AB
CD
det(D)det(A - BD-1C),
(23)
if D is invertible.
12
Published as a conference paper at ICLR 2021
Definition 6 (Pushforward of a measure). Recall that the pushforward f μ of a measure μ by a
function f is defined as the measure such that for all measurable g,
/ g(λ)d(f*μ)(λ) = / g(f(λ))dμ(λ).
(24)
Equivalently, if X is a random variable with distribution μ, then f (X) has distribution f*μ.
Proposition B.2. Assume that the dimensions ofM ∈ Rdx×dy fulfill dx ≤ dy and let r = dx/dy.
Let μMM> be the expected SPectral distribution of the random matrix MM> ∈ Rdx×dx, and
assume that it is absolutely continuous with respect to the Lebesgue measure. The expected spectral
distribution of A is contained in the imaginary line and is given by
μA(iλ)
一… 2lλl	」、
δ0(λ) +	1 μMM> (λ ) .
1 + r
(25)
1 - τ⅛
r
for λ ∈ R. If dχ ≥ dy, then (25) holds with μ.>m in place of Mmm> and 1/r in place of r.
Proof. By the block determinant formula, we have that for s 6= 0,
det(sId1+d2 -A) = MsId>1 -sIM =det(sId2)det(sId1 +Ms-1Id2M>)
= sd2-d1det(s2Id1 + MM>)
(26)
Thus, for every eigenvalue -λ ≤ 0 of -MM>, both i√λ and -i√λ are eigenvalues of A. Since
rank(MM>) = rank(M), we have rank(A) = 2rank(M). Thus, the rest of the eigenvalues of A
are 0 and there is a total of d - 2d1 = d2 - d1 of them. Notice that
di	11
di + d2 = d1+d2 = 1 + 1
d1	r
(27)
Let f+(λ) = i√λ, f-(λ) = -i√λ, and let (f+)≠μMM> (resp., (f-)*MM>) be the pushforward
measure of Mmm> by the function f+ (resp., f-). Thus, by the definition of the pushforward
measure (Definition 6),
μA(iλ) =	( 1	- II) δ0(λ)	+ ι	i (f+)*μMM> (λ)	+ ι i (f-)*μMM>	(λ)	(28)
∖	1	+ r J	1 +	r	1	+ r
We compute the pushforwards (f+)*μMM>, (f-)*μMM> performing the change of variables y =
±i√λ under the assumption that Mmm> (λ) = PMM> (λ)dλ:
L g(±i√λ) d〃MM > (λ) = ɪ g(±i√λ) PMM >(λ)dλ = /册 g (y) PMM > (|y|2)2|y| d|y|,
(29)
which means that the density of (f+)*μMM> at y ∈ iR≥o is 2∣y∣ρMM> (|y|2) and the density of
(f-)*μMM> at y ∈ -iR≥0 is also 2|y|PMM> (IyF).	□
Proposition B.3. The condition
∀P, Q polynomials hP (λ), λQ(λ)i = 0 =⇒ hλP (λ), Q(λ)i = 0	(30)
is sufficient for any sequence (Pk)k≥0 of orthogonal polynomials of increasing degrees to satisfy a
three-term recurrence of the form
γkPk(λ) = (λ-αk)Pk-i(λ) - βkPk-2(λ),	(31)
where
hλPk-i(λ),Pk (λ)i _	hλPk-i(λ),Pk-i(λ)i	Q	_	hλPk-i(λ),Pk-2 (λ)i
hPk(λ),Pk(λ)i ,	αk =	hPk-1(λ),Pk-1(λ)i ,	βk	= hPk-2(λ),Pk-2(λ)i
13
Published as a conference paper at ICLR 2021
Proof. Since λPk-1(λ) is a polynomial of degree k, and (Pj)0≤j≤k is a basis of the polynomials of
degree up to k, we can write
λPk-ι(λ)=XX hλp- PPj i Pj (λ)
(33)
Now, remark that for all j < k - 2, hPk-1, λPji = 0 because the inner product of Pk-1 with
a polynomial of degree at most k - 2. If we make use of the condition (30), this implies that
□
hλPk-1, Pji = 0 for all j < k - 2. Plugging this into (33), we obtain (31).
Proposition B.4. Let ΠtR be the set of polynomials with real coefficients and degree at most t. For
t ≥ 0 even, the minimum of the problem
Pt∈∏m%=∕∖{0严 R2内ρMM>( 内 2)d 内
is attained by an even polynomial with real coefficients.
(34)
Proof. Since dμ(iλ) =f ∣λ∣ρMM> (∣λ∣2)d∣λ∣ is supported in the imaginary axis and is symmetric
with respect to 0, for all polynomials P,Q,
hλP(λ),Q(λ)i =/ λP(λ)Q(λ)*dμ(λ) = -/ P(λ)λ*Q(λ)*dμ(λ) = -hP(λ),λQ(λ)i.
iR	iR	(35)
Hence, hP(λ),λQ(λ)= 0 implies hλP(λ),Q(λ)= 0. By Proposition B.3, a three-term recur-
rence (31) and (32) for the orthonormal sequence (φt )t≥0 of polynomials holds.
By Proposition B.5, the orthonormal polynomials (φt )t≥0 of even (resp. odd) degree are even (resp.
odd) and have real coefficients. Hence, for all t ≥ 0 even
Pk=0 φk (λ)φk (0)* = Pk=O Φ2k (λ)φ2k (0)*
Ptk=O ∣Φk (0)∣2	Pk=O ∣Φ2k(0)∣2
is an even polynomial with real coefficients. By Theorem 2.3, this polynomial attains the minimum
of the problem
Pt∈ΠmtC,Pint(O)=1 iR\{O} ∣Pt(λ)∣2∣λ∣ρMM>(∣λ∣2)d∣λ∣
(37)
and, a fortiori, the minimum of the problem in (34), in which the minimization is restricted polyno-
mials with real coefficients instead of complex coefficients.	□
Proposition B.5. The polynomials (Φt)t≥O of the orthonormal sequence corresponding to the mea-
sure μ(iλ) = ∣λ∣ρMM>(∣λ∣2)d∣λ∣ have real Coefficients and are ^ven (resp. odd) for even (resp.
odd) k.
Proof. The proof is by induction. The base case follows from the choice ΦO = 1. Assuming that
Φk-1 ∈ R[X] by the induction hypothesis, we show that αk = 0 (where αk is the coefficient from
(31) and (32)):
hλΦk-1(λ), Φk-1(λ)i =	λ∣Φk-1(λ)∣2∣λ∣ρMM> (∣λ∣2)d∣λ∣
iR	(38)
=	iλ(∣Φk-1 (iλ)∣2 - ∣Φk-1 (-iλ)∣2)λρM M > (λ2)dλ = 0
R≥0
The last equality follows from ∣φk-1(iλ)∣2 = ∣φk-1(-iλ)∣2, which holds because φk-ι(iλ)* =
Φk-1 (-iλ), and in turn this is true because Φk-1 ∈ R[X] by the induction hypothesis.
Once we have seen that αk = 0, it is straightforward to apply the induction hypothesis once again
to show that φk also satisfies the even/odd property. Namely, for k even (resp. odd), γkPk =
λPk-1 - βkPk-2, and the two polynomials in the right-hand side have even (resp. odd) degrees.
14
Published as a conference paper at ICLR 2021
Finally, φk must have real coefficients because φk-1 and φk-2 have real coefficients by the induction
hypothesis, and the recurrence coefficient βk is real, as
hλPk-1(λ),Pk-2(λ)i = ∕λφk-i(λ)φk-2(λ)*∣λ∣ρMM >(∣λ∣2 )d∣λ∣
=I	iλ(φk-1(iλ)φk-2(iλ)* -φk-1(iλ)*φk-2(iλ))λρMM>(λ2)dλ
R≥0
=-[2λIm(φk-1(iλ)φk-2(iλ)*)λρMM>(λ2)dλ ∈ R.
R≥0
(39)
□
Proposition B.6. Let t ≥ 0 even. Assume that on R>0, the expected spectral density Mmm> has
Radon-Nikodym derivative ρMM > with respect to the Lebesgue measure. If
Qt?/2 d=ef arg min
Pt∕2∈∏R∕2,
Pt/2(0)=1
P	pt∕2(λ)2 dμ-A2 (λ),
R>0
(40)
and
Pt =f argmin^⑼ ∣Pt(λ)∣2∣λ∣ρMM>(∣λ∣2)d∣λ∣,
(41)
Pt(0)=1
then Pj(λ) = Q?/2(-X2).
Proof. First, remark that the equalities in (40) and (41) are well defined because the arg min are
unique by Theorem 2.3. Without loss of generality, assume that dx ≤ dy (otherwise switch the
players), and let r d=ef dx/dy < 1. Since,
A2	MM>	0
-A =	0	M>M
(42)
each eigenvalue of MM> ∈ Rdx ×dx is an eigenvalue of -A2 with doubled duplicity, and the rest
of eigenvalues are zero. Hence, We have μ-A =(1 - 2/(1 + 11)) δo + 2*mm>/(1 + ɪ). Thus,
for all t ≥ 0,
Q? = argmin / Pt(λ)2 dμτ2(λ) = argmin / Pt(λ)2ρMM>(λ)dλ	(43)
Pt(0)=1
Pt(0)=1
By Proposition B.4, for an even t ≥ 0 the minimum in (41) is attained by an even polynomial With
real coefficients. Hence,
min
Pt∈ΠtR,
Pt(0)=1
I	∣Pt(λ)∣2∣λ∣ρMM>(∣λ∣2 )d∣λ∣
iR\{0}
min
Pt/2 ∈πR∕2,
Pt/2(0)=1
/
iR\{0}
|Pt/2(X2)|2|NpMM> (|X|2) dlλ
2	minR	[	lpt∕2((iλ)2)l2λPMM> (λ2)dλ = 2	minR	[	Pt∕2(λ2)2λPMM>	(λ2)dλ
Pt∕2∈πt∕2, R>0	Pt∕2∈πt∕2, R>0
Pt/2 (0)=1	Pt/2 (0)=1
min	/	Pt∕2(λ)2ρMM> (λ) dλ
Pt∕2∈πt∕2, R>o
Pt/2(0)=1
(44)
Moreover, for any polynomial Qt∕2 that attains the minimum on the right-most term, the polynomial
Ρt(λ) = Qt∕2(-λ2) attains the minimum on the left-most term. In particular, using (43), Pt (λ) =f
Q:/2 (-λ2) attains the minimum on the left-most term.	□
15
Published as a conference paper at ICLR 2021
Theorem 3.1. Suppose that Assumption 1 holds and that the expected spectral distribution of
MM> is absolutely continuous with respect to the Lebesgue measure. Then, the method (11)
is average-case optimal for bilinear games when ht , mt are chosen to be the coefficients of the
average-case optimal minimization of 2 ∣∣F(x)k2.
Proof. Making use of Theorem 2.1 and Proposition B.2, we obtain that for any first-order method
using the vector field F,
E[dist(xt, X?)] = R2 /	∣Pt(λ)∣2 dμA(λ)
C\{0}
2R2
1 + 1
r
I	∣Pt(λ)∣2∣λ∣ρMM> (∣λ∣2)d∣λ∣
iR\{0}
(45)
Let Qt?/2, Pt? be as defined in (41) and (40). For t ≥ 0 even the iteration t of the average-case
optimal method for the bilinear game must satisfy
xt - PX? (x0) = Pt? (A)(x0 - PX? (x0)) = Qt?/2 (-A2 )(x0 - PX? (x0))	(46)
On the other hand, the first-order methods for the minimization of the function 1 ∣∣F (x)∣2 make use
of the vector field V (1 ∣∣F(x)∣∣2) = A>(Ax + b) = -A2(x 一 x?). Let μ-A2 be the spectral
density of -A2. By Theorem 2.1, the average-case optimal first-order method for the minimization
problem is the one for which the residual polynomial Pt (Proposition 2.1) minimizes the functional
JR Pt dμ-A2. That is, the residual polynomial is Q?. From (46), We see that the t-th iterate of the
average-case optimal method for F is equal to the t/2-th iterator of the average-case optimal method
for V (2IlF(x)k2).	□
C Proofs of Theorem 4.1 and Theorem 4.2
Theorem 4.1.	Under Assumption 4 and the assumptions of Theorem 2.1, the following algorithm is
optimal in the average case, with y-1 = y0 = x0:
yt = atyt-1 + (1 - at)yt-2 + btF(yt-1)
Xt = R	xt-1 + a	yt,	βt	= φ2(0),	Bt	=	Bt-I	+ βt-ι,	Bo	= 0 .	(16)
Bt + βt	Bt + βt
where (φk (0))k≥0 can be computed using the three-term recurrence (upon normalization). More-
over, E (A,x?,x0) dist(xt, X?) converges to zero at rate 1/Bt.
Proof. We prove by induction that
xt - x
Pk=O φk(A)φk(0)*
Pk= Φk (0)2
(47)
The base step t = 0 holds trivially because φ0 = 1. Assume that (47) holds for t - 1. Subtracting
x? from (16), We have
	xt 一 x?	= Pk=O φk(0)2 (xt-ι-x?)+	*0)2	(yt-x?) Pk=O Φk(0)2	Ptk=O Φk (0)2	(48)
If			
		φt(0)2 (yt 一 x?) = φt(0)φt (A)(xo 一 x?),	(49)
by the induction hypothesis for t - 1 and (48), We have
Ek=O φt(O)φt(A)	一	φt(0)φt(A)
Pk=0 Φk(0)2	)	Pk=o Φk(0)2
xt - x
Pk=O Φt(0)Φt(A)
Pk= Φk(0)2
(xo 一 x*),
(50)
16
Published as a conference paper at ICLR 2021
which concludes the proof of (47). The only thing left is to show (49), again by induction. The base
case follows readily from y0 = x0 in (16). Dividing by φt(0)2, we rewrite (49) as
yt - x?=φt(A)(X0-x?)=ψt (A)(X0- x?),
(51)
where ψt is the t-th orthogonal residual polynomial of sequence. By Assumption 4, ψt must satisfy
the recurrence in (15). If we subtract x* from the second line of (16), we apply the induction
hypothesis and then the recurrence in (15), we obtain
yt- x?	=	at(yt-1 -	x?) +	(1- at)(yt-2 -	x?) + btF(yt-1)
=	at(yt-1 -	x?) +	(1- at)(yt-2 -	x?) + btA(yt-1 - x*)	(52)
= atψt-1 (A)(x0- x?) + (1- at)ψt-2 (A)(x0- x?) + btAψt-1 (A)(x0- x?)
= ψt(A)(x0 - x?),
thus concluding the proof of (49).
□
Proposition C.1. Suppose that Assumption 5 holds with C = 0, that is, the circular Support of μ is
centered at 0. Then, the basis of orthonormal polynomials for the scalar product
hP,Qi = JD	P(λ)Q(λ)* dμ(λ) is φk(λ)
λk
Dk,R，
∀k ≥ 0,
(53)
where Kk,R = ∖∣2∏ RR r2kdμκ(r).
Proof. First, we will show that if μ satisfies Assumption 5 with C = 0, then〈》, λj) = 0 if j, k ≥ 0
With j 6= k (Without loss of generality, suppose that j > k).
hλj,λki = /	λj(λ*)k dμ(λ) = /	λj-k ∣λ∣2k dμ(λ)
DR,0	DR,0
=/R ɪ /(reiθ)j-kr2k dθ dμR(r) = ɪ / eiθ(Ak) dθ ZR rj+k dμR(r) (54)
0 2π 0	2π 0	0
_ ei2π - 1
2πi(j — k)
And for all k ≥ 0,
hλk,λki= Z
DR,
Z rj+k dμκ(r) = 0
0
R	2π	2π
Jk12dμ(λ)=/ 方/ 产dθdμR(r)=/ r2kdμR(r).	(55)
□
Proposition 4.1. If μ satisfies Assumption 5, the Sequence of orthonormal polynomials is (φt)t≥o,
φt(λ)
(λ- C)t, where Kt,R = J/f r2t dμκ(r).
Kt,R
(17)
Proof. The result follows from Proposition C.1 using the change of variables z → z + C. To
compute the measure μR for the uniform measure on Dc,r, We perform a change of variables to
circular coordinates:
Z	f (λ)dμ(λ)	=	-12	ZR ∕2"	f(C	+ reiθ)r dθdr	=	ZR	/" f(C + reiθ)dθd〃R(r).
DC,R	πR 0	0	0	0
r
=⇒ d〃R(r) = ∏R2 dr
(56)
And
Z r2t dμκ(r)
0
R	ι R2t
r2t+1dr = ^=⇒ Kt,R = WE (57)
□
17
Published as a conference paper at ICLR 2021
Theorem 4.2.	Given an initialization x0(y0 = x0), if Assumption 5 is fulfilled with R < C and the
assumptions of Theorem 2.1 hold, then the average-case optimal first-order method is
yt = yt-ι - C1F(yt-ι),	βt = C2t∕κ2,R,
Bt	βt
Xt = B+βt XT + B+βtyt.
Bt = Bt-1 + βt-1,
(18)
Moreover, E (A,x?,x0) dist(Xt, X?) converges to zero at rate 1∕Bt.
Proof. By Proposition 4.1, the sequence of residual orthogonal polynomials is given by ψt(λ) =
φt(λ)∕φt(0) =(1 - λ) . Hence, Assumption 4 is fulfilled with at = 1,b = -C1, as ψt(λ)=
ψt-ι(λ) - λψt-ι(λ). We apply Theorem 4.1 and make use of the fact that φk(0)2 = ~Ck. See
Proposition D.3 for the rate on dist(xt, X?).	□
D Proof of Proposition 5.2
Proposition D.1. Suppose that the assumptions of Theorem 4.2 hold with the probability measure
μR fulfilling μn(∖r, R]) = Ω((R — r)κ) for r in [ro, R] for some r° ∈ [0, R) and for some K ∈ Z.
Then,
lim
t→∞
C 2t
t	C2k
k=0 KR
(58)
Proof. Given > 0, let c ∈ Z
1
≥0 be the minimum such that
Pc=0 (R)
Define Qt,R def κR2^ .Then,
C2
k2R
i ≤ (1 + )
(1+
(59)
Pk=o
C2k
Rt Qt,R
QKR
Kk,R
Pk=0 R2k Qk,R	Pk=0 (R )	Qk,R
(60)
Σ
1
Now, on one hand, using that Qt,R is an increasing sequence on t,
Qt,R
Ptk=0 (C2)t-k
On the other hand, for t ≥ c,
C - Lt R R2 ∖i-k — L∞ R R2 ∖ k
Qk,R	k=0=O V2μ)	k=0=O Vc2)
1-R2
(61)
Qt,R
Ptk=0
(C)	Qk,r	Pk
Qt,R
=t -ce (C2 )	Qk,R	Pk = t-ce (C2 )
Qt,R
t — k / „
1
1
(62)
Thus, We want to upper-bound RktddsQs,r ds. First, notice that
d Q
dSQs，R
2s dμR(r)!
(ROR (R)2s d〃R(r))2
(63)
By concavity of the logarithm function we obtain log(rR ) ≤ R - 1 for r ∈ [r0, R]. Choose r° close
enough to R so that R - 1 ≤ e∕ce. We obtain that
/R (R 广
log
dμR(r) ≤
r0	r 2s
Jo⑴
log
dμR(r) +
Z： (R )2s
(r^ - 1) dμR(r).
(64)
18
Published as a conference paper at ICLR 2021
Thus,
td
Jk dsQsRds ≤
Zt
k
R；0 (R)2s log (R) d〃R(r)	[t RrR (R)2s (R-1) dμRr
(RiR (R)2s d〃R(『))2	Jk	(RiR (R)2s d〃R(『))2	'
Using that log x ≤ x, for k ∈ [t - c, t] we can bound the first term of (65) as
∕t RJ (R)2s log (R) dμR(()ds ≤ ∕t / (R)2s-1 dμR(『)ds
Jk	(RoR (R)2s dμR(r))2	7 (RoR (R)2s dμR(r))2
≤ (t-k)
(R )2k-1
(RR (R)2tdμR(『))2
2(t-c)-1
Qt,R
(66)
≤ Ce ((RI ⅛ ⑵ +1)2κ
t→∞
---→ 0.
In the last inequality we use that by Proposition D.2, for t large enough, Qt,R
1)k/c1. For k ∈ [t - ce, t], the second term of (65) can be bounded as
ft RrR (R)2, dμR((
Jk(ROl (R)2s dμR(r))2
R2t
K2R
(2t +
(67)
≤ c
≤
From (65), (66) and (67), we obtain that for t large enough, for k ∈ [t - c, t],
td
J dsQs,R ds ≤ 2eQt,R∙
Hence, we can bound the right-hand side of (62):
(68)
(69)
The last inequality follows from the definition of c in (59). Since is arbitrary, by the sandwich
theorem applied on (60), (61) and (69),
C2t
1.	K2R	r	R2
lim -----,---= 1 --
t→∞Pt	_C2L	1 C2.
k=k = ° K2,R
(70)
□
Proposition D.2. Under the assumptions of Theorem 4.2, we have that there exists c1 > 0 such that
for t large enough,
Kt2,R ≥ c1R2t(2t+ 1)-κ.
(71)
19
Published as a conference paper at ICLR 2021
Proof. By the assumption on μR, there exist r0, cι,κ > 0 such that
Kt2,R d=ef 2π Z R r2t dμκ(r) = 2π[苫 dμR (r) + 2π∕Rr2t dμR(r)
≥ 2πc1 Z r2t (R - r)κ-1 dr = -2πc1 Z r2t (R - r)κ-1 dr + 2πc1 Z r2t (R - r)κ-1 dr
r0	0	0
≥ -2πc1Rr02t + 2πc1R2t+κB(2t + 1, κ).
(72)
where the beta function B(x, y) is defined as
1
B(x, y) d=ef
0
rx+1 (1 - r)y+1 dr.
(73)
Using the link between the beta function and the gamma function B(x, y) = Γ(x)Γ(y)∕Γ(x + y),
and Stirling’s approximation, we obtain that for fixed y and large x,
B(x,y)〜Γ(y)x-y.
(74)
Hence, for t large enough, B(2t + 1,κ)〜Γ(κ)(2t + 1)-κ = (κ - 1)!(2t + 1)-κ. Hence, from (72)
we obtain that there exist c01 depending only on κ and r0 such that for t large enough
Kt2,R ≥ -2πc1Rr02t + 2πc1R2t+κ(k - 1)!(2t + 1)-κ ≥ c01R2t(2t + 1)-κ.	(75)
□
Proposition 5.2. Suppose that the assumptions of Theorem 4.2 hold with μR ∈ P([0, R]) fulfilling
μR([r,R]) = Ω((R — r)κ) for r in [ro, R] for some r° ∈ [0, R) and for some K ∈ Z. Then, the
average-case asymptotically optimal algorithm is, with y0 = x0:
yt = yt-ι - CF(yt-ι),
(20)
Moreover, the convergence rate for this algorithm is asymptotically the same one as for the optimal
algorithm in Theorem 4.2. Namely, limt→∞ E [dist(xt, X?)]Bt = 1.
Proof. The proof follows directly from Theorem 4.2 and Proposition D.1. See (77) and (79) in
Proposition D.3 for the statement regarding the convergence rate.	□
Proposition D.3. For the average-case optimal algorithm (18),
E dist(xt, X ?) = ξopt(t) = Pt 1 C2k	(76)
k=0=K K2R
For the average-case asymptotically optimal algorithm (20),
E dist(xt, X ?)= ξaymp(t) =f (l - (C Y)2 XX ⅛R (C yt. + (R](77)
k=1
For the iterates yt in (18), i.e. gradient descent with stepsize 1/C, we have
E dist(yt,X ?) = ZGD ⑴=f kc⅛	(78)
Moreover, for all t ≥ 0, we have ξopt(t) ≤ ξasymp(t), and under the assumptions of (5.1),
lim ∕t(?、= 1, lim Ip(4 = ξasymp(t) = 1 - 1C
t→∞ ξasymp(t)	t→∞ ξGD(t) ξGD(t)	C
(79)
20
Published as a conference paper at ICLR 2021
Proof. To show (76), (77), (78), we use the expression xt -x? = Pt(A)(x0 -x?) (Proposition 2.1)
and then evaluate IlPtl∣μ = JC`. |Pt|2 dμ (Theorem 2.1).
For (76), the value of IlPtI∣μ follows directly from Theorem 2.3, which states that the value for the
optimal residual polynomial Pt is
1	_	1
Pk=血(0)∣2 = Pk= KkR
(80)
A simple proof by induction shows that for the asymptotically optimal algorithm (20), the following
expression holds for all t ≥ 0:
Thus,
xt - x
R。
t
X
k=1
2(t-k)
(x0 - x?)
(81)
2(t-k)
Pt(λ)
R T φ0(λ)+(TR )2) XX H φk (λ) (J
(82)
which concludes the proof of (77), as
By equation (52),
(83)
(84)
Thus, for the yt iterates, kPt ∣μ = K^, and (78) follows.
Now, ξopt(t) ≤ ξasymp(t), ∀t ≥ 0 is a consequence of ξopt(t) being the rate of the optimal algorithm.
And
lim ξopt≡
t→∞ ξGD(t)
C2t
lim —K2R—
t→∞ Pt
2=0 = K K2,R
1 - r2
(85)
follows from Proposition D.1. To show limt→∞ ξopt(t) = 1 - C, which concludes the proof, We
rewrite
ξasymp (t)
2 t 1 R 2(t-k)	R 2t
0=1 QR⑺	+⑴
(86)
using that by definition, Qk,R = R2k/Kk2,R. Now, let c ∈ Z≥0 such that
≤ .
(87)
Using the same argument as in Proposition D.1 (see (68)), fort large enough and k ∈ [t - c,t],
Q 丁Qs,R ds ≤ 2^Qt,R.
k ds
(88)
21
Published as a conference paper at ICLR 2021
Hence, for t large enough,
(89)
which can be made arbitrarily close to (1 - (R)2) QI^ by taking e > 0 small enough. Plugging
this into (86), We obtain that We can make ξasymp(t) arbitrarily close to (1 - (R)2) (R广 Q^ =
^Q i,.R
(1 - (R) ) ξGD(t) by taking t large enough.	□
22