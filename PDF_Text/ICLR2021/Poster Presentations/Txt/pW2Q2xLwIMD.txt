Published as a conference paper at ICLR 2021
Few- S hot Learning via Learning the
Representation, Provably
Simon S. Du* 1*, Wei Hu2*, Sham M. Kakade1,3*, Jason D. Lee2*, and Qi Lei2*
1University of Washington 2Princeton University 3Microsoft Research
{ssdu,sham}@cs.washington.edu, {huwei@cs.,
jasonlee@,qilei@}princeton.edu
Ab stract
This paper studies few-shot learning via representation learning, where one uses T
source tasks with n1 data per task to learn a representation in order to reduce the
sample complexity of a target task for which there is only n2( n1) data. Specifi-
cally, we focus on the setting where there exists a good common representation
between source and target, and our goal is to understand how much a sample size
reduction is possible. First, we study the setting where this common representation
is low-dimensional and provide a risk bound of O(ndT + 卷)on the target task for
the linear representation class; here d is the ambient input dimension and k( d)
is the dimension of the representation. This result bypasses the Ω( T) barrier under
the i.i.d. task assumption, and can capture the desired property that all n1T samples
from source tasks can be pooled together for representation learning. We further
extend this result to handle a general representation function class and obtain a
similar result. Next, we consider the setting where the common representation
may be high-dimensional but is capacity-constrained (say in norm); here, we again
demonstrate the advantage of representation learning in both high-dimensional
linear regression and neural networks, and show that representation learning can
fully utilize all n1T samples from source tasks.
1	Introduction
A popular scheme for few-shot learning, i.e., learning in a data-scarce environment, is representation
learning, where one first learns a feature extractor, or representation, e.g., the last layer of a con-
volutional neural network, from different but related source tasks, and then uses a simple predictor
(usually a linear function) on top of this representation in the target task. The hope is that the learned
representation captures the common structure across tasks, which makes a linear predictor sufficient
for the target task. If the learned representation is good enough, it is possible that a few samples
are sufficient for learning the target task, which can be much smaller than the number of samples
required to learn the target task from scratch.
While representation learning has achieved tremendous success in a variety of applications (Bengio
et al., 2013), its theoretical studies are limited. In existing theoretical work, the most natural algorithm
is to explicitly look for the optimal representation given source data, which when combined with a
(different) linear predictor on top for each task can achieve the smallest cumulative training error on
the source tasks. Of course, it is not guaranteed that the representation found will be useful for the
target task unless one makes some assumptions to characterize the connections between different
tasks. Existing work often imposes a probabilistic assumption about the connection between tasks:
each task is sampled i.i.d. from an underlying distribution. Under this assumption, Maurer et al.
(2016) showed an O(方 + √=) risk bound on the target task, where T is the number of source
tasks, n1is the number of samples per source task, and n2 is the number of samples from the target
task.1 Unsatisfactorily, this bound necessarily requires the number of tasks T to be large, and it
* Alphabetical Order.
1We only focus on the dependence on T, n1 and n2 in this paragraph. Note that Maurer et al. (2016) only
considered n1 = n2, but their approach does not give a better result even if n1 > n2.
1
Published as a conference paper at ICLR 2021
does not improve when the number of samples per source task, n1, increases. Intuitively, one should
expect more data to help, and therefore an ideal bound would be √=r + √√= (or n1τ + * in the
realizable case), because n1T is the total number of training data points from source tasks, which can
be potentially pooled to learn the representation.
Unfortunately, as pointed out by Maurer et al. (2016), there exists an example that satisfies the i.i.d.
task assumption for which Ω(√T) is unavoidable (or Ω( 1) in the realizable setting). This means
that the i.i.d. assumption alone is not sufficient if we want to take advantage of a large amount of
samples per task. Therefore, a natural question is:
What connections between tasks enable representation learning to utilize all source data?
In this paper, we obtain the first set of results that fully utilize the n1T data from source tasks. We
replace the i.i.d. assumption over tasks with natural structural conditions on the input distributions
and linear predictors. These conditions depict that the target task can be in some sense “covered” by
the source tasks, which will further give rise to the desirable guarantees.
First, we study the setting where there exists a common well-specified low-dimensional representation
in source and target tasks, and obtain an O( ndT + 卷)risk bound on the target task where d is the
ambient input dimension, k( d) is the dimension of the representation, and n2 is the number of
data from the target task. Note that this improves the 舟 rate ofjust learning the target task without
using representation learning. The term ^^ indicates that we can fully exploit all nιT data in the
source tasks to learn the representation. We further extend this result to handle general representation
function class and obtain an O( C(φ) + 卷)risk bound on the target task, where Φ is the representation
function class and C (Φ) is a certain complexity measure of Φ.
Second, we study the setting where there exists a common linear high-dimensional representation for
O W
+
source and target tasks, and obtain an
rate where R is a normalized nuclear
norm control over linear predictors, and Σ is the covariance matrix of the raw feature. This also
improves over the baseline rate for the case without using representation learning. We further extend
this result to two-layer neural networks with ReLU activation. Again, our results indicate that we can
fully exploit n1T source data.
A technical insight coming out of our analysis is that any capacity-controlled method that gets low
test error on the source tasks must also get low test error on the target task by virtue of being forced to
learn a good representation. Our result on high-dimensional representations shows that the capacity
control for representation learning does not have to be through explicit low dimensionality.
Organization. The rest of the paper is organized as follows. We review related work in Section 2. In
Section 3, we formally describe the setting we consider. In Section 4, we present our main result for
low-dimensional linear representation learning. A generalization to nonlinear representation classes
is demonstrated in Section 5. In Section 6, we present our main result for high-dimensional linear
representation learning. In Section 7, we present our result for representation learning in neural
networks. We conclude in Section 8 and leave most of the proofs to appendices.
2	Related Work
The idea of multitask representation learning at least dates back to Caruana (1997); Thrun and Pratt
(1998); Baxter (2000). Empirically, representation learning has shown its great power in various
domains; see Bengio et al. (2013) for a survey. In particular, representation learning is widely adopted
for few-shot learning tasks (Sun et al., 2017; Goyal et al., 2019). Representation learning is also
closely connected to meta-learning (Schaul and Schmidhuber, 2010). Recent work Raghu et al. (2019)
empirically suggested that the effectiveness of the popular meta-learning algorithm Model Agnostic
Meta-Learning (MAML) is due to its ability to learn a useful representation. The scheme we analyze
in this paper is closely related to Lee et al. (2019); Bertinetto et al. (2018) for meta-learning.
On the theoretical side, Baxter (2000) performed the first theoretical analysis and gave sample
complexity bounds using covering numbers. Maurer et al. (2016) and follow-up work gave analyses
on the benefit of representation learning for reducing the sample complexity of the target task. They
assumed every task is i.i.d. drawn from an underlying distribution and can obtain an O(√T + √=)
2
Published as a conference paper at ICLR 2021
rate. As pointed out in Maurer et al. (2016), the √t dependence is not improvable even if n1 → ∞
because 力 is the rate of concentration for the distribution over tasks.
The concurrent work of Tripuraneni et al. (2020) studies low-dimensional linear representation
learning and obtains a similar result as ours in this case, but they assume isotropic inputs for all tasks,
which is a special case of our result. Furthermore, we also provide results for high-dimensional linear
representations, general non-linear representations, and two-layer neural networks. Tripuraneni et al.
(2020) also give a computationally efficient algorithm for standard Gaussian inputs and a lower bound
for subspace recovery in the low-dimensional linear setting.
Another recent line of theoretical work analyzed gradient-based meta-learning methods (Denevi et al.,
2019; Finn et al., 2019; Khodak et al., 2019) and showed guarantees for convex losses by using tools
from online convex optimization. Lastly, we remark that there are analyses for other representation
learning schemes (Arora et al., 2019; McNamara and Balcan, 2017; Galanti et al., 2016; Alquier
et al., 2016; Denevi et al., 2018).
3	Notation and Setup
Notation. Let [n] = {1, 2,..., n}. We use ∣∣∙k or ∣∣∙k2 to denote the '2 norm of a vector or the
spectral norm of a matrix. Denote by ∣∣∙∣f and ∣∣∙∣* the Frobenius norm and the nuclear norm of a
matrix, respectively. Let〈•，•〉be the Euclidean inner product between vectors or matrices. Denote by
I the identity matrix. Let N(μ, σ2)/"(μ, Σ) be the one-dimensional/multi-dimensional Gaussian
distribution, and χ2(m) the chi-squared distribution with m degrees of freedom.
For a matrix A ∈ Rm×n, let σi(A) be its i-th largest singular value. Let span(A) be the subspace of
Rm spanned by the columns of A, i.e., SPan(A) = {Av | V ∈ Rn}. Denote PA = A(A>A)*A> ∈
Rm×m, which is the projection matrix onto SPan(A). Here * stands for the Moore-Penrose pseudo-
inverse. Note that 0 PA I and PA2 = PA . We also define PA⊥ = I - PA , which is the projection
matrix onto SPan(A)⊥, the orthogonal complement of SPan(A) in Rm. For a positive semidefinite
(psd) matrix B, denote by λmax(B) and λmin(B) its largest and smallest eigenvalues, respectively;
let B1/2 be the psd matrix such that (B1/2)2 = B.
We use the standard O(∙), Ω(∙) and Θ(∙) notation to hide universal constant factors. We also use
a . b or b & a to indicate a = O(b), and use a》b or b《a to mean that a ≥ C ∙ b fora sufficiently
large universal constant C > 0.
Problem Setup. Suppose that there are T source tasks. Each task t ∈ [T] is associated with a
distribution μt over the joint data space X × Y, where X is the input space and Y is the output space.
In this paper we consider X ⊆ Rd and Y ⊆ R. For each source task t ∈ [T] we have access to n1
i.i.d. samples (xt,1, yt,1), . . . , (xt,n1, yt,n1) from μt. For convenience, we express these n1 samples
collectively as an input matrix Xt ∈ Rn1 ×d and an output vector yt ∈ Rn1 .
Multitask learning tries to learn prediction functions for all the T source tasks simultaneously in the
hope of discovering some underlying common property of these tasks. The common property we
consider in this paper is a representation, which is a function φ : X → Z that maps an input to some
feature space Z ⊆ Rk . We restrict the representation function to be in some function class Φ, e.g.,
neural networks. We try to use different linear predictors on top of a common representation function
φ to model the input-output relations in different source tasks. Namely, for each task t ∈ [T], we
set the prediction function to be x 7→ hwt, φ(x)i (wt ∈ Rk). Therefore, using the training samples
from T tasks, we can solve the following optimization problem to learn the representation:2
1 T n1
minimize .........-	(yti — <w% φ(xti)i)2 .
φ∈Φ,wι,...,wτ∈Rk 2nιT t=1 ± (yt,i	h t,φ( t,i)i)
(1)
We overload the notation to allow φ to apply to all the samples in a data matrix simultaneously, i.e.,
φ(Xt) = [φ(xt,1), . . . , φ(xt,n1 )]> ∈ Rn1 ×k. Then (1) can be rewritten as
1T
Φ∈Φnwniπz∈Rk 而it Xkyt- φ(Xt)wtk2.
(2)
2We use the `2 loss throughout this paper.
3
Published as a conference paper at ICLR 2021
Let φ ∈ Φ be the representation function obtained by solving (2). Now we retain this representation
and apply it to future (target) tasks. For a target task specified by a distribution μτ+ι over X X Y,
suppose we receive n2 i.i.d. samples XT+1 ∈ Rn2×d, yT+1 ∈ Rn2. We further train a linear
predictor on top of φ for this task:
mwτ+im∈iZke2n2^yτ+1- φ(XT+1)wT+1u.	⑶
Let WT +1 be the returned solution. We are interested in whether our learned predictor X →
(Wτ +1,φ(x)i works well on average for the target task, i.e., we want the population loss
12
Lμτ+1 (φ, WT +1 ) = E(x,y)~μτ+1 2(y - hwT +1, φ(X)))
to be small. In particular, we are interested in the few-shot learning setting, where the number of
samples n from the target task is small - much smaller than the number of samples required for
learning the target task from scratch.
Data assumption. In order for the above learning procedure to make sense, we assume that there
is a ground-truth optimal representation function φ* ∈ Φ and specializations wɪ,..., WT +1 ∈ Rk
for all the tasks such that for each task t ∈ [T + 1], we have E(x,y)〜*=[y|x] = (wjc, φ* (x)). More
specifically, we assume (x, y)〜μt can be generated by
y = (w*,φ*(x)i + z, x 〜pt,z 〜N(0,σ2),
(4)
where X and z are independent. Our goal is to bound the excess risk of our learned model on the
target task, i.e., how much our learned model (φ, WT +1) performs worse than the optimal model
(φ*, WT +1) on the target task:
「n / 7 入 ∖ T /7 ʌ ∖ T /∕* *∖
ER(φ, WT +1) = Lμτ+1 (φ, WT +1) - Lμτ+1 (φ , WT +1)
1	2	(5)
=2 Ex〜PT +1[((Wt +1,φ(x)i-(wT +1,φ*(x)i)2].
Here we have used the relation (4). Oftentimes we are interested in the average performance on
a random target task (i.e., WT +1 is random). In such case we look at the expected excess risk
EwT+1 [ER(B, WT +1)].
4	Low-Dimensional Linear Representations
In this section, we consider the case where the representation is a linear map from the original input
space Rd to a low-dimensional space Rk (k d). Namely, we let the representation function class
be Φ = {X 7→ B>X | B ∈ Rd×k}. Then the optimization problem (2) for learning the representation
can be written as:
1T
(B,W) -	argmin	5~^£kyt - XtBWt『.	(6)
B∈Rd×k	2n1T t=1
W =[w1 ,...,wτ]∈Rk×τ	t=1
The inputs from T source tasks, X1, . . . , XT ∈ Rn1 ×d, can be written in the form of a linear operator
X : Rd×T → Rn1 ×T, where
X(Θ) = [X1θ1,. ..,XTθT],	∀Θ = [θ1,.. .,θT] ∈ Rd×T.
With this notation, (6) can be rewritten as
(B,W) J arg min	IIy — X (BW )kF , where Y = [y1,..., yτ ] ∈ Rn1×τ.⑺
B∈Rd×k,W ∈Rk×τ 2n1T
With the learned representation B from (7), for the target task, we further find a linear function on
top of the representation:
WT +1 J argminɪ ∣∣yτ +1 - XT +1Bw∣∣ .	(8)
w∈Rk 2n2
4
Published as a conference paper at ICLR 2021
As described in Section 3, we assume that all T + 1 tasks share a common ground-truth representation
specified by a matrix B* ∈ Rd×k such that a sample (x,y) 〜μt satisfies X 〜Pt and y =
(wJc)>(B*)>x + Z where Z 〜 N(0,σ2) is independent of x. Here Wt ∈ Rk, and We assume
kwt* k = Θ(1) for all t ∈ [T + 1]. Denote W* = [w1*, . . . , wT* ] ∈ Rk×T. Then we can write
Y = X (B*W *) + Z, where the noise matrix Z has i.i.d. N(0, σ2) entries.
Assume Ex〜pt [x] = 0 and let ∑t = Ex〜pt [xx>] for all t ∈ [T + 1]. Note that a sample X 〜Pt
can be generated from X = Σ1∕2X for X 〜Pt such that Ex〜p/x] = 0 and Ex〜p」XX>] = I.
(Pt is called the whitening of pQ In this section we make the following assumptions on the input
distributions P1 , . . . , PT+1 .
Assumption 4.1 (subgaussian input). There exists ρ > 0 such that, for all t ∈ [T + 1], the random
vector X 〜Pt is ρ2 -subgaussian.3
Assumption 4.2 (covariance dominance). There exists C > 0 such that ∑t 占 c∙ ∑t +ι for all t ∈ [T].4
Assumption 4.1 is a standard assumption in statistical learning to obtain probabilistic tail bounds used
in our proof. It may be replaced with other moment or boundedness conditions if we adopt different
tail bounds in the analysis.
Assumption 4.2 says that every direction spanned by ΣT+1 should also be spanned by Σt (t ∈ [T]),
and the parameter c quantifies how “easy” it is for Σt to cover ΣT+1. Intuitively, the larger c is, the
easier it is to cover the target domain using source domains, and we will indeed see that the risk will
be proportional to C. We remark that we do not necessarily need ∑t 占 C ∙ ∑t +ι for all t ∈ [T]; as
long as this holds for a constant fraction of t’s, our result is valid.
We also make the following assumption that characterizes the diversity of the source tasks.
Assumption 4.3 (diverse source tasks). The matrix W* = [w1*, . . . , wT* ] ∈ Rk×T satisfies
σ2(W*) ≥ Ω(T))
Recall that kwt* k = Θ(1), which implies Pjk=1 σj2(W*) = kW* k2F = Θ(T). Thus, Assumption 4.3
is equivalent to saying that ；1(W*) = O(1). Roughly speaking, this means that {w* }t∈[T] can cover
all directions in Rk. As an example, Assumption 4.3 is satisfied with high probability when wt*’s are
sampled i.i.d. from N(0, Σ) with ：max(j1) = O(1).
Finally, we make the following assumption on the distribution of the target task.
Assumption 4.4 (distribution of target task). Assume that wT* +1 follows a distribution ν such that
IlEw〜V[ww>]∣∣ ≤ O (1).
Since we assume ∣∣wT+/| = Θ(1), the assumption ∣∣Ew〜V[ww>]∣∣ ≤ O( 1) means that the
distribution of wT* +1 does not align with any direction significantly more than average. It is useful to
think of the uniform distribution on the unit sphere as an example, though we can allow a much more
general class of distributions. This is also compatible with Assumption 4.3 which says that wt*’s
cover all the directions.
Assumption 4.4 can be removed at the cost of a slightly worse risk bound. See Remark 4.1. Our main
result in this section is the following theorem.
Theorem 4.1 (main theorem for linear representations). Fix a failure probability δ ∈ (0, 1). Under
Assumptions 4.1, 4.2, 4.3 and 4.4, we further assume 2k ≤ min{d, T} and that the sample sizes in
source and target tasks satisfy nι》ρ4(d + log 手),n》ρ4(k + log 1), and cnι ≥ n. Define
K = max；：；] λmax(∑;). Then with probability at least 1 一 δ over the samples, the expected excess
risk ofthe learned predictor X → W>+ιB X on the target task satisfies
EwT…v[ER(B, WT +1)] . σ2 (%”D + k⅛i) .	(9)
T +1	∖	CnII	n?	)
3A random vector x is called ρ2-subgaussian if for any fixed unit vector v of the same dimension, the random
variable v>x is ρ2-subgaussian, i.e., E[es∙v>(X-E[x])] ≤ es2ρ2/2 (∀s ∈ R).
4 Note that Assumption 4.2 is a significant generalization of the identically distributed isotropic assumption
used in concurrent work Tripuraneni et al. (2020): they require ∑ι = ∑2 =…=∑t+ι = I.
5
Published as a conference paper at ICLR 2021
The proof of Theorem 4.1 is in Appendix A. Theorem 4.1 shows that it is possible to learn the target
task using only O(k) samples via learning a good representation from the source tasks, which is better
than the baseline O(d) sample complexity for linear regression, thus demonstrating the benefit of
representation learning. It also shows that all n1T samples from source tasks can be pooled together,
bypassing the Ω(T) barrier under the i.i.d. tasks assumption.
Remark 4.1 (deterministic target task). We can drop Assumption 4.4 and easily obtain the following
excess risk boundfor any deterministic WT+1by slightly modifying theProofofTheorem 4.1:
ER(B, WT +1) . σ2 (k⅛* + 巴 + k⅛1
cn1 T	cn1	n2
which is only at most k times larger than the bound in (9).
5	General Low-Dimensional Representations
Now we return to the general case described in Section 3 where we allow a general representation
function class Φ. We still assume that the representation is of low dimension k like in Section 4, and
We assume that inputs from all the tasks follow the same distribution, i.e., pi = •…=PT +1 = p,
but each task t still has its own specialization function Wt (c.f. (4)). We overload the notation
from Section 4 and use X to represent the collection of all the training inputs from T source tasks
X1 , . . . , XT ∈ Rn1 ×d. We can think of X as a third-order tensor of dimension n1 × d × T .
To characterize the complexity of the representation function class Φ, we need the standard definition
of Gaussian width.
Definition 5.1 (Gaussian width). Given a set K ⊂ Rm, the Gaussian width of K is defined as:
G (K) ：= Ez〜N(0,i) suPv∈κhv, z).
We will measure the complexity of Φ using the Gaussian width of the following set that depends on
the input data X :
FX (Φ) = A = [a1, . . . , aT] ∈ Rn1×T : kAkF = 1,
∃φ, φ ∈ Φ s.t. at ∈ span([φ(Xt),φ0(Xt)]), ∀t ∈ [T]}.
(10)
We also need the following definition.
Definition 5.2 (covariance between two representations). Given a distribution q over Rd and two
representation functions φ, φ0 ∈ Φ, define the covariance between φ and φ0 with respect to q to be
∑q (Φ, Φ0) = Ex 〜q [Φ (X) Φ0 (X)> ] ∈ Rk×k .
Also define the symmetric covariance as
Λq(φ, φ0) =
Σq (φ, φ) Σq (φ, φ0)
Σq (φ0, φ) Σq (φ0, φ0)
∈ R2k×2k .
It is easy to verify Λq(φ, φ0)	0 for any φ, φ0 and q, as shown in the proof of Lemma B.2.
We make the following assumptions on the input distribution p, which ensure concentration properties
of the representation covariances.
Assumption 5.1 (point-wise concentration of covariance). For δ ∈ (0, 1), there exists a number
Npoint (Φ, p, δ) such that if n ≥ Npoint (Φ, p, δ), then for any given φ, φ0 ∈ Φ, n i.i.d. samples ofp
will with probability at least 1 - δ satisfy
0.9Λp(φ, φ0) W Λp(φ, φ0) W 1.1Λp(φ, φ0),
where P is the empirical distribution over the n samples.
Assumption 5.2 (uniform concentration of covariance). For δ ∈ (0, 1), there exists a number
Nunif (Φ, P, δ) such that ifn ≥ Nunif (Φ, P, δ), then n i.i.d. samples ofP will with probability at least
1 - δ satisfy
0.9Λp(φ, φ0) W Λp(φ, φ0) W 1.1Λp(φ, φ0),	∀φ, φ0 ∈ Φ,
where P is the empirical distribution over the n samples.
6
Published as a conference paper at ICLR 2021
Assumptions 5.1 and 5.2 are conditions on the representation function class Φ and the input distribu-
tion p that ensure concentration of empirical covariances to their population counterparts. Typically,
we expect Nunif (Φ, p, δ) Npoint (Φ, p, δ) since uniform concentration is a stronger requirement.
In Section 4, we have essentially shown that for linear representations and subgaussian input distribu-
tions, Nunif (Φ, p, δ) = O (d) and Npoint (Φ, p, δ) = O (k) (see Claims A.1 and A.2).
Our main theorem in this section is the following:
Theorem 5.1 (main theorem for general representations). Fix a failure probability δ ∈ (0, 1).
Suppose nι ≥ NUnif (Φ,P, 3T) and n ≥ NPoint (Φ,P, δ)∙ Under Assumptions 4.3 and 4.4, with
probability at least 1 - δ over the samples, the expected excess risk of the learned predictor x 7→
W>+1Φ(x) on the target task satisfies
κ	γprz↑ ʌ V 2 (G(FX (φ))2 +log 1 , k + log 1∖	zln
EwT+1 〜ν[ER(φ,WT +1)] . σ ------------------------δ +--------δ .	(II)
T+1	n1T	n2
Theorem 5.1 is very similar to Theorem 4.1 in terms of the result and the assumptions made. In the
bound (11), the complexity of Φ is captured by the Gaussian width of the data-dependent set FX (Φ)
defined in (10). Data-dependent complexity measures are ubiquitous in generalization theory, one of
the most notable examples being Rademacher complexity. Similar complexity measure also appeared
in existing representation learning theory (Maurer et al., 2016). Usually, for specific examples,
we can apply concentration bounds to get rid of the data dependency, such as our result for linear
representations (Theorem 4.1).
Our assumptions on the linear specification functions w}s are the same as in Theorem 4.1. The
probabilistic assumption on WT+1can also be removed at the cost of an additional factor of k in the
bound - see Remark 4.1. We defer the full proof of Theorem 5.1 to Appendix B.
6	High-Dimensional Linear Representations
In this section, we consider the case where the representation is a general linear map without an
explicit dimensionality constraint, and we will prove a norm-based result by exploiting the intrinsic
dimension of the representation. Such a generalization is desirable since in many applications the
representation dimension is not restricted.
Without loss of generality, we let the representation function class be Φ = {x 7→ B>x | B ∈ Rd×T}.
We note that a dimension-T representation is sufficient for learning T source tasks and any choice of
dimension greater than T will not change our argument. We use the same notation from Section 4
unless otherwise specified.
In this section we additionally assume that all tasks have the same input covariance:
Assumption 6.1. The input distributions in all tasks satisfy ∑ι = •…=∑t +1 = ∑.
Note that each task t still has its own specialization function Wt (c.f. (4)). We remark that there are
many interesting and nontrivial scenarios under Assumption 6.1 - for example, consider the case
where the inputs in each task are all images from ImageNet and each task asks whether the image is
from a specific class.
Since we do not have a dimensionality constraint, we modify (7) by adding norm constraints:
(B,W) -	argmin ɪIY -X(BW)kF + "kF + λ怛匕 (12)
B∈Rd×T,W ∈RT ×T 2n1	2	2
For the target task, we also modify (8) by adding a norm constraint:
W T +1 - arg min 1— IlXT +ιB W — yτ +ι∣∣2.	(13)
kwk≤r 2n2
We will specify the choices of regularization, i.e., λ and r in Theorem 6.1.
Similar to Section 4, the source task data relation is denoted as Y = X(Θt) + Z, where Θt ∈ Rd×T
is the ground truth and Z has i.i.d. N(0, σ2) entries. Suppose that the target task data satisfy
yT+1 = XT+1θTt +1 + zT+1 ∈ Rn2. Similar to the setting in Section 4, we assume the target task
data is subgaussian as in Assumption 4.1.
7
Published as a conference paper at ICLR 2021
Theorem 6.1 (main theorem for high-dimensional representations). Fix a failure probability δ ∈
(0,1). Under Assumptions 4.1 and 6.1, Wefurtherassume nι ≥ n, R = ∣∣Θk*. Let r = 2 ,R/T,
R = R∕√T and proper λ specified in Lemma C.2. Let the target task model θT +1be coherent with
the source task models Θ* in the sense that θT+]〜V = N(0, Θ*(Θ*)>/T). Then with probability
at least 1 一 δ over the samples, the expected excess riskofthe learned predictor X → W>+jB >x on
the target task satisfies:
EθT+1”[ER(B, Wτ +ι)] ≤ σR ∙ O (P√lp + P√∑k2! + Zn1,n2,	(14)
where Zn1,n2 := ρ4R2O (Tn∑) + k∑k) is lower-order terms due to randomness ofthe input data.
Here O hides logarithmicfactors.
The proof of Theorem 6.1 is given in Appendix C. Note that ∣Θ*∣f = √T when each θt is of unit
norm. Thus R = ∣∣Θ*∣* /√T should generally be regarded as O⑴ for a well-behaved Θ* that is
nearly low-dimensional. In this regime, Theorem 6.1 indicates that we are able to exploit all n1T
samples from the source tasks, similar to Theorem 4.1.
With a good representation, the sample complexity on the target task can also improve over learning
the target task from scratch. Consider the baseline of regular ridge regression directly applied to the
target task data:
12
θ — argmin -一∣∣Xt +ιθ — yτ +ι∣ .
∣∣θ∣∣≤∣∣θT"I 2n2
(15)
Its standard excess risk bound in fixed design is ER(θλ) . σ J
∣∣θT +ι∣∣2Tr(Σ)
n2
. (See e.g. Hsu et al.
(2012).) Taking expectation over θT十]〜V = N(0, Θ*(Θ*)>∕T), we obtain
EθT+"ER”. σ ≡>∖T≡.
T+1	T	n2
kΘ*k2
Compared with (16), our bound (14) is an improvement as long as lrι*l *《
kΘ kF
(16)
Tr∑k). The left hand
.1 kΘ*k2 .	1	,1	I	1	TTT	.1	.
side 〜 * IS always no more than the rank of Θ*, and we call It the intrinsic rank. Hence we see that
kΘ* k2F
we can gain from representation learning if the source predictors are intrinsically low dimensional.
To intuitively understand how this is achieved, we note that a representation B is reweighing linear
combinations of the features according to their “importance” on the T source tasks. We make an
analogy with a simple case of feature selection. Suppose we have learned a representation vector
b where bi scales with the importance of the i-th feature, i.e., the representation is φ(x) = x b
(entry-wise product). Then ridge regression on the target task data (X, y), minimize∣w∣≤r 2^ ∣∣X ∙
diag(b) ∙ w — y∣2, is equivalent to minimize∣diag(b)-iv∣≤r 焉 ∣∣X V — y∣2. From the above equation,
we see that the features with large |bi | (those that were useful on the source tasks) will be more
heavily used than the ones with small |bi | due to the reweighed `2 constraint. Thus the important
features are learned from the source tasks, and the coefficients are learned from the target task.
Remark 6.1 (The non-convex landscape). Although the optimization problem (12) is non-convex, its
structure allows us to apply existing landscape analysis of matrix factorization problems (Haeffele
et al., 2014) and to show that it has the nice properties of no strict saddles and no bad local minima.
Therefore, randomly initialized gradient descent or perturbed gradient descent are guaranteed to
converge to a global minimum of (12) (Ge et al., 2015; Lee et al., 2016; Jin et al., 2017).
Remark 6.2 (Multi-class problems). When both source and target have multi-class labels instead of
independent tasks, using quadratic loss on the one-hot labels, our results apply similarly and will
attain an excess risk of the form σRO (√T(S) + √~n^^^ plus lower-order terms (see e.g. Lee
et al. (2020)). Notice the result is independent of the number of classes.
7 Neural Networks
In this section, we show that we can provably learn good representations in a neural network.
8
Published as a conference paper at ICLR 2021
Consider a two-layer ReLU neural network fB,w (x) = w> (B>x)+, where w ∈ Rd, B ∈ Rd0×d
and X ∈ Rd0. Here (•)+ is the ReLU activation (z)+ = max{0, z} defined element-wise. Namely,
we let the representation function class be Φ = {x → (B>x)+ |B ∈ Rd0×d}. On the source tasks
we use the square loss with weight decay regularizer:5
1T	λ	λ
(B,W) -	argmin	2~T Ekyt-(XtB)+wtk2 + 2∣∣B∣∣F + 2∣∣WkF.
B∈Rd0×d,W =[wι ,∙∙∙wτ ]∈Rd×T 2n1T t=ι	2	2
(17)
On the target task, we simply re-train the output layer while fixing the hidden layer weights:
WT +1 J arg min 1— IlyT +1 - (XT +ιB)+w∣∣2.	(18)
kwk≤r 2n2
Assumption 7.1. All tasks share the same input distribution: pi = •…=PT +ι = P. We redefine
Σ to be the covariance operator of the feature induced by ReLU, i.e., it is a kernel defined by
Σ(u, V) = Ex 〜p[(u>x) + (v>x)+] ,for u, V on the unit sphere Sd0-1 ⊂ Rd0.
Assumption 7.2 (teacher network). Assume for the source tasks that yt = (XtB*)+wJ= + Zt is
generated by a teacher network with parameters BJ ∈ Rd0×d, WJ = [wj,∙∙∙ , WT ] ∈ Rd×T, and
noise term Zt 〜N(0, σ2I). A Standard lifting of the neural network is: fat = hαt, φ(x)i where
φ(x) : Sd0-1 → R, φ(x)b = (b>x)+ is thefeaturemap, i.e.,foreach task, at(bi∕∣bik) = Wi,t∣bik
and is zero elsewhere. We assume αT+1 that describes the target function to follow a Gaussian
process μ with covariance function K (b, b0) = PT=I αt(b)ɑt(b0).
Theorem 7.1. Fix a failure probability δ ∈ (0, 1). Under Assumptions 4.1, 7.1 and 7.2, let n1 ≥ n2,
R= (2∣∣B*kF + 2∣∣WJkF)∕√T. Let the target task model fατ+ι = haT +i,φ(x)i be coherent
with the source task models in the sense that αTJ +1 〜ν. Set r2 = (IIBJkF + ∣WJkF∖/TThen
with probability at least 1 - δ over the samples, the expected excess risk of the learned predictor
x → W>+ι(B>x)+ on the target task satisfies:
EaT +1 〜ν[ER(fB,Wτ+1)] ≤ σR ∙ O ^P√≡ + P^S-) + Zn,n,	(19)
where Znm := ρ4R2O(T(∑) + k∑k) is lower-order term due to randomness ofthe input data.
To highlight the advantage of representation learning, we compare to training a neural network with
weight decay directly on the target task:
1n
(B, W)= arg min — Ekyt+1-(XT +iB)+w∣2.	(20)
B,w,kBwk≤R 2n i=1
The error of the baseline method in fixed-design is
叫ER(ZB,w)]. σRR ;n).	(21)
We see that Equation (19) is always smaller than Equation (21) since n1T ≥ n2. See Appendix D for
the proof of Theorem 7.1 and the calculation of (21).
8	Conclusion
We gave the first statistical analysis showing that representation learning can fully exploit all data
points from source tasks to enable few-shot learning on a target task. This type of results were shown
for both low-dimensional and high-dimensional representation function classes.
There are many important directions to pursue in representation learning and few-shot learning. Our
results in Sections 6 and 7 indicate that explicit low dimensionality is not necessary, and norm-based
capacity control also forces the classifier to learn good representations. Further questions include
whether this is a general phenomenon in all deep learning models, whether other capacity control can
be applied, and how to optimize to attain good representations.
5Wei et al. (2019) show that (17) can be minimized in polynomial iteration complexity using perturbed
gradient descent, though potentially exponential width is required.
9
Published as a conference paper at ICLR 2021
Acknowledgments
SSD acknowledges support of National Science Foundation (Grant No. DMS-1638352) and the
Infosys Membership. JDL acknowledges support of the ARO under MURI Award W911NF-11-1-
0303, the Sloan Research Fellowship, and NSF CCF 2002272. WH is supported by NSF, ONR,
Simons Foundation, Schmidt Foundation, Amazon Research, DARPA and SRC. QL is supported by
NSF #2030859 and the Computing Research Association for the CIFellows Project. The authors also
acknowledge the generous support of the Institute for Advanced Study on the Theoretical Machine
Learning program, where SSD, WH, JDL, and QL were participants.
References
Pierre Alquier, The Tien Mai, and Massimiliano Pontil. Regret bounds for lifelong learning. arXiv
preprint arXiv:1610.08628, 2016.
Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saunshi. A
theoretical analysis of contrastive unsupervised representation learning. In Proceedings of the 36th
International Conference on Machine Learning, 2019.
Jonathan Baxter. A model of inductive bias learning. J. Artif. Int. Res., 2000.
Yoshua Bengio, Nicolas L Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte. Convex
neural networks. In Advances in neural information processing systems, pages 123-130, 2006.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828,
2013.
Luca Bertinetto, Joao F Henriques, Philip HS Torr, and Andrea Vedaldi. Meta-learning with differen-
tiable closed-form solvers. arXiv preprint arXiv:1805.08136, 2018.
Rich Caruana. Multitask learning. Machine Learning, 28(1):41-75, Jul 1997. ISSN 1573-0565. doi:
10.1023/A:1007379606734. URL https://doi.org/10.1023/A:1007379606734.
Giulia Denevi, Carlo Ciliberto, Dimitris Stamos, and Massimiliano Pontil. Incremental learning-to-
learn with statistical guarantees. arXiv preprint arXiv:1803.08089, 2018.
Giulia Denevi, Carlo Ciliberto, Riccardo Grazzi, and Massimiliano Pontil. Learning-to-learn stochas-
tic gradient descent with biased regularization. In Proceedings of the 36th International Conference
on Machine Learning, 2019.
Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online meta-learning. In
Proceedings of the 36th International Conference on Machine Learning, 2019.
Tomer Galanti, Lior Wolf, and Tamir Hazan. A theoretical framework for deep transfer learning.
Information and Inference: A Journal of the IMA, 5(2):159-209, 2016.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points - online stochastic
gradient for tensor decomposition. In Proceedings of The 28th Conference on Learning Theory,
pages 797-842, 2015.
Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. Scaling and benchmarking self-
supervised visual representation learning. In Proceedings of the IEEE International Conference on
Computer Vision, pages 6391-6400, 2019.
Benjamin Haeffele, Eric Young, and Rene Vidal. Structured low-rank matrix factorization: Optimality,
algorithm, and applications to image processing. In International conference on machine learning,
pages 2007-2015, 2014.
Daniel Hsu, Sham M Kakade, and Tong Zhang. Random design analysis of ridge regression. In
Conference on learning theory, pages 9-1, 2012.
10
Published as a conference paper at ICLR 2021
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, and Michael I. Jordan. How to escape
saddle points efficiently. In Proceedings of the 34th International Conference on Machine Learning,
pages 1724-1732, 2017.
Mikhail Khodak, Maria-Florina Balcan, and Ameet Talwalkar. Adaptive gradient-based meta-learning
methods. arXiv preprint arXiv:1906.02717, 2019.
Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only
converges to minimizers. In Conference on Learning Theory, pages 1246-1257, 2016.
Jason D Lee, Qi Lei, Nikunj Saunshi, and Jiacheng Zhuo. Predicting what you already know helps:
Provable self-supervised learning. arXiv preprint arXiv:2008.01064, 2020.
Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with
differentiable convex optimization. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 10657-10665, 2019.
Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The benefit of multitask
representation learning. The Journal of Machine Learning Research, 17(1):2853-2884, 2016.
Daniel McNamara and Maria-Florina Balcan. Risk bounds for transferring representations with and
without fine-tuning. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70, pages 2373-2381. JMLR. org, 2017.
Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature reuse?
towards understanding the effectiveness of maml. arXiv preprint arXiv:1909.09157, 2019.
Saharon Rosset, Grzegorz Swirszcz, Nathan Srebro, and Ji Zhu. l1 regularization in infinite dimen-
sional feature spaces. In International Conference on Computational Learning Theory, pages
544-558. Springer, 2007.
Tom SchaUl and Jurgen Schmidhuber. Metalearning. Scholarpedia, 5(6):4650, 2010.
Nathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm. In International Conference on
Computational Learning Theory, pages 545-560. Springer, 2005.
Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable
effectiveness of data in deep learning era. In Proceedings of the IEEE international conference on
computer vision, pages 843-852, 2017.
Sebastian Thrun and Lorien Pratt. Learning to Learn: Introduction and Overview, pages 3-17.
Springer US, Boston, MA, 1998. ISBN 978-1-4615-5529-2. doi: 10.1007/978-1-4615-5529-2_1.
URL https://doi.org/10.1007/978-1-4615-5529-2_1.
Nilesh Tripuraneni, Chi Jin, and Michael I Jordan. Provable meta-learning of linear representations.
arXiv preprint arXiv:2002.11684, 2020.
Joel A Tropp et al. An introduction to matrix concentration inequalities. Foundations and Trends® in
Machine Learning, 8(1-2):1-230, 2015.
Roman Vershynin. Four lectures on probabilistic methods for data science. https://arxiv.
org/pdf/1612.06661.pdf, 2017.
Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and
optimization of neural nets vs their induced kernel. In Advances in Neural Information Processing
Systems, pages 9709-9721, 2019.
11
Published as a conference paper at ICLR 2021
A Proof of Theorem 4.1
We first prove several claims and then combine them to finish the proof of Theorem 4.1. We will use
technical lemmas proved in Section A.1.
Claim A.1 (covariance concentration of source tasks). Suppose n1 ρ4 (d + log(T /δ)) for δ ∈
(0,1). Then with probability at least 1 一 磊 over the inputs Xi,..., XT in the source tasks, we have
0.9∑t W ɪX>Xt W 1.1∑t,	∀t ∈ [T].
n1 t
(22)
Proof. According to our assumption on pt, We can write Xt = Xt∑1/2, where Xt ∈ Rn1 ×d and the
rows of Xt hold i.i.d. samples of pt. Since pt satisfies the conditions in Lemma A.6, from Lemma A.6
we know that with probability at least 1 一 备,
0.9I W ɪX>Xt W 1.1I,
n1 t
which implies
0.9∑t W L∑"X>XMt1/ = ɪX>Xt W 1.1∑t.
n1	n1
The proof is finished by taking a union bound over all t ∈ [T].	□
ClaimA.2 (covariance concentration of target task). Suppose n/》ρ4(k + log(1∕δ)) for δ ∈ (0,1).
Thenfor any given matrix B ∈ Rd×2k that is independent of XT +ι, with probability at least 1 一 今
over XT+1 we have
0.9B>∑t +iB W —B>X>+]Xt +iB W 1.1B>∑t +ιB.	(23)
Proof. According to our assumption on PT +i, we can write XT +i = XT +1∑T+1, where XT +i ∈
Rn2 ×d and the rows of XT +i hold i.i.d. samples of PT +i. We take the SVD of ∑T+ιB: ∑T+ιB =
UDV >, where U ∈ Rd×2k has orthonormal columns. Now we look at the matrix XT +iU ∈ Rn2×2k.
It is easy to see that the rows of XT +iU are i.i.d. 2k-dimensional random vectors with zero mean,
identity covariance, and are ρ2-subgaussian. Therefore, applying Lemma A.6, with probability at
least 1 一 i0 We have
0.9I W ɪU>X>+iXT +iU W 1.1I,
n2	T+i
which implies
0.9VDDV> W ɪVDU>X>+iXT +iUDV> W 1.1VDDV>.
n2	T+i
Since n2VDU>X>+1Xτ +iUDV> =击B>∑T+iX>+iXT +i∑T+iB = n⅛B>X>+iXt +iB
andVDDV> = VDU>UDV> = B>ΣT+iB, the above inequality becomes
0.9BτΣτ +iB W ɪB>X>+iXT +iB W 1.1B>∑τ +iB.	□
n2	+
Claim A.3 (guarantee on source training data). Under the setting of Theorem 4.1, with probability at
least 1 - 5 we have
kX(BW - B*W*)kF . σ2 (kT + kdlog(κni) + log(1∕δ)).	(24)
Proof. We assume that (22) is true, which happens with probability at least 1 一 余 according to
Claim A.1.
Let ΘΘ = BW and Θ* = B W*. From the optimality of B and W for (7) we have ∣∣Y — X(ΘΘ)kF ≤
∣∣Y 一 X(Θ*)∣F. Plugging in Y = X(Θ*) + Z, this becomes
∣X(Θ — Θ*)∣∣F ≤ 2〈Z, X(Θ 一 Θ*)i.	(25)
12
Published as a conference paper at ICLR 2021
Let ∆ = ΘΘ 一 Θ*. Since rank(∆) ≤ 2k, We can write ∆ = VR = [Vri, ∙∙∙ , Vrτ] where
V ∈ Od,2k and R = [ri,…，rτ] ∈ R2k×T. Here Odi& (di ≥ d2) is the set of orthonormal
d1 × d2 matrices (i.e., the columns are orthonormal). For each t ∈ [T] we further write XtV = UtQt
where Ut ∈ On1,2k and Qt ∈ R2k×2k. Then we have
T
hZ,X(∆)i=Xzt>XtVrt
t=i
T
= X zt>UtQtrt
t=i
T
≤ X∣∣U>Zt∣∣∙kQtrtk
t=i
T
≤ \，2『\
T
= tXUUtF2 ∙t
T
= tXUuft
T
X∣∣U>zt∣∣2 ∙kΧ(∆)kF .	(26)
t=i
T
X kQtrtk2
t=i
T
XkUtQtrtk2
t=i
T
X kXtVrtk2
t=i
∖
Next we give a high-probability upper bound on PtT=i ∣∣Ut> zt ∣∣ using the randomness in Z. Since
Ut ’s depend on V which depends on Z, we will need an -net argument to cover all possible
V ∈ Od,2k. First, for any fixed V ∈ Od,2k, we let XtV = UtQt where Ut ∈ On,2k. The
Ufs defined in this way are independent of Z. Since Z has i.i.d. N(0,σ2) entries, we know
that σ-2 PT=1 ∣∣U>zt∣∣2 is distributed as χ2(2kT). Using the standard tail bound for X random
variables, we know that with probability at least 1 一 δ0 over Z,
σ-2 X∣∣U>Zt∣∣2. kT + log(1∕δ0).
t=i
Therefore, using the same argument in (26) we know that with probability at least 1 一 δ0,
hZ, X(VRyi . σ√kT + log(1∕δ0) ∣∣X(VR)∣∣F .
Now, from Lemma A.5 we know that there exists an -net N of Od,2k in Frobenius norm such that
N ⊂ Od,2k and |N| ≤ (6^2k)2kd. Applying a union bound over N, we know that with probability
at least 1 一 δ0∣N∣,
hZ, X(VR) . σ√kT + log(1∕δ0) ∣∣X(VR)∣∣f , ∀V ∈N.	(27)
Choosing δ0 = 20(6击)2, We know that (27) holds with probability at least 1 一 击.
We will use (22), (25) and (27) to complete the proof of the claim. This is done in the following steps:
1.	Upper bounding kZ kF .
Since σ-2 ∣∣Z∣∣F 〜χ2(n1T), we know that with probability at least 1 一 余,
kZk2F . σ2(niT + log(1∕δ)).	(28)
13
Published as a conference paper at ICLR 2021
2.	Upper bounding k∆kF.
From (25) we have ∣∣X (∆)kF ≤ 2 ∣∣Z kF ∣∣X (∆)kF, which implies ∣∣X (∆)kF ≤ 2 ∣∣Z kF
σ,nιT + log(1∕δ). On the other hand, letting the t-th column of ∆ be δt, We have
T
∣X(∆)∣2F=X∣Xtδt∣2
t=1
T
X δt>Xt>Xtδt
t=1
T
≥ 0.9n1 X δt>Σtδt
t=1
T
≥ 0.9n1 X λmin(Σt) ∣δt∣2
t=1
≥ 0.9nιλ ∣∆∣F ,
where λ = mint∈[τ] λmin(∑t). Hence we obtain
(using (22))
∣∆∣2F .
1^0)底 . σ2(nιT + log(1∕δ))
nι λ ~	nιλ
3.	Applying the -net N .
Let V ∈ N such that ∖∖V 一 叫IF ≤ e. Then we have
WX(VR - VR)WF
T
=XWXt(V 一 v)rt∖∖2
t=1
T
≤ X ∣Xt∣2WV - VW2IIrtII2
t=1
T
≤ X1.1n1λmax(Σt)2∣rt∣2
t=1
≤ 1.1nιλe2 IlRIlF
(using (22))
1.1n1λe2 ∣∆∣F
(λ = max λmax (Σt))
t∈[T]
(k∆kF = kVRkF = kRkF)
.niʌe2 ∙
σ2(n1T + log(1∕δ))
nιλ
κ2σ2(n1T + log(1∕δ)).
(29)
4. Finishing the proof.
We have the following chain of inequalities:
2 ∣X(∆)∣F
≤ hZ, X (∆)i	(using (25))
=hZ, X(VR)i + hZ, X(VR — VR)i
.σ√kT + log(1∕δ0) WX(VR)WF + IlZkF WX(VR - VR)WF	(using (27))
≤ σ√kT + log(1∕δ0) (IIX(VR)IlF + WX(VR - VR)WF)
+ σ√n1T + log(1∕δ) WX(VR - VR)WF	(using (28))
14
Published as a conference paper at ICLR 2021
.σ√kT + log(1∕δ0) ∣∣X(VR)∣∣f + σ√mT + log(1∕δ0) IlX(VR - VR)IlF
(using k < n1 and δ0 < δ)
.σ√kT + log(1∕δ0) ∣∣X(Δ)∣∣f + ZnT + log(1∕δ0) ∙ ,κe2σ2(nιT + log(1∕δ))
(using (29))
≤ σ√kT + log(1∕δ0) ∣∣X(Δ)∣∣f + eσ2√K(nιT + log(1∕δ0)).
Finally, We let E = √KL-, and recall δ0 = 20(6提产. Then the above inequality implies
kX(∆)kF
. max 卜√kT + log(1∕δ0), q∕eσ2√K(nιT + log(1∕δ0))
=max {σ√kT + log(1∕δ0), σ4^-(nιT + log(1∕δ0))}
≤ max {σ√kT + log(1∕δ0), σ√kT + log(1∕δ0))}
=σ√kT + log(1∕δ0)
√~^^^^TT^^k^^^^^T
kT + kd log - + log -
Eδ
≤ σ4kT + kdlog(κnι) + log ；.
(using k< n1)
The high-probability events We have used in the proof are (22), (27) and (28). By a union bound, the
failure probability is at most 磊 + 东 + 东=δ. Therefore the proof is completed.	□
Claim A.4 (Guarantee on target training data). Under the setting of Theorem 4.1, with probability at
least 1 一 学,we have
1	IIP⊥ X BJI2 . σ2 (kT + kdlog(κnι) + log1)
n2 1PXT+1BT +1B L .	cnι ∙ σ2(W*)	^
Proof. We suppose that the high-probability events in Claims A.1, A.2 and A.3 happen, Which holds
with probability at least 1 ― 25δ. Here we instantiate Claim A.2 using B = [B, B*] ∈ Rd×2k.
From the optimality of B and W in (6) we know XtBWt = PXtByt = PXtB (XtB*w* + Zt) for
each t ∈ [T]. Then we have
σ2 (kT + kd log(κn1) + log(1∕δ))
& kx(B W - b*w *)kF
T
=XIIXtBWt - XtB*w*II
t=1
T2
=X IIPXtB (XtB*W* + Zt) - XtB*W* II
t=1
T
=X II-PXtBXtB*w* + PXtBZtII
t=1
T
=Xq-PXtBXtB*w*n +nPXtB ztB ;
T
≥ x IIPXtB XtB*w*ii
t=1
(from (24))
(the cross term is 0)
15
Published as a conference paper at ICLR 2021
T2
≥0.9nι X ∣∣P⊥ι/2B^∑y2B*wR
t=1 t
≥0.9cn1 X p∑t∕2ib^∑T+i
T+1
t=1
B*w*
(using (22) and Lemma A.7)
(using Assumption 4.2 and Lemma A.7)
T
2
= 0.9cn1
≥ 0.9cn1
∣2
p⅛1 b^ ∑T+ib*w *
T+1	F
∣2
P⊥ι∕2 b^∑T+iB*	∙σ2(W*).
ΣT+1B	∣F
Next, We write B = [B, B*] ]I[ =： BA and B* = [B, B*] ]；[ =： BC. Recall that We have
nlB>X>+]Xτ +ιB W 1.1B>∑t +ιB from Claim A.2. Then using Lemma A.7 We can obtain
2
F
1.1 p^bA∑T+21bc
≥ n ∣∣P⊥τ+iba XT+1BC ∣∣f
i.e.,
∣	∣2	1∣	∣2
1.1 P⅛B^∑T+1B* f≥ m∣P⊥τ +1B^XT +1B*∣∣F
Therefore We get
σ2 (kT + kdlog(κn1) + log(1∕δ)) &
0≡r ∣∣PXT+ιBXT+1B*∣∣F ∙ σ2(W *)，
completing the proof.
□
Proof of Theorem 4.1. We Will use all the high-probability events in Claims A.1, A.2, A.3 and A.4.
Here we instantiate Claim A.2 using B = [B, B*] ∈ Rd×2k. The success probability is at least
1 - 45δ.
For the target task, the excess risk of our learned linear predictor X → (BwT +1)1 X is
ER(B, Wt +1) = 2 Ex〜PT +J(x>(BWt +1 - B*wT +1))2
1
2(BwT +1 - B*wT +1)>∑T +1(BwT+1 - B*wT +1).
Applying Claim A.2 with B = [B, B*], we have
0.9B>∑t +1B W ɪB>X>+1Xt +1B,
n2	T +1
which implies 0.9v>B>∑τ +1Bv ≤ n1-vB>X>+1Xτ +1Bv for V = WT +1 . This becomes
,ʌ	_ ,	,	. Γ —	, ʌ	_ ,	,	、
(BWT +1 - B wT +1) ∑T +1(BWT +1 - B wT +1)
≤0 9n (BWτ +1 - b*wT +1)>χ>+1χτ +1(BWT+1 - b*wT +1).
Therefore we have
ER(B, WT +1) ≤ γ-5—(BWT +1 - b*wT +1)>X>+1Xτ +1(BWτ +1 - b*wT +1)
1.8n2
= r⅛∣∣XT+1(B WT+1- B*wτ+1)∣∣2.
16
Published as a conference paper at ICLR 2021
From the optimality of WT +ι in (8) We know XT +ιBτwτ +ι	= PX	Byτ +ι
Pχτ+1B (Xt +1 B*wT +1 + ZT +ι). Itfollows that
ER(B, WT +1) . ɪ ∣∣pXτ+ιB (XT +ιB*wT +1 + ZT +1)- XT +1B*wT +J]
=n12 ll-P⊥τ+iBXT +WWT +1 + PXτ+iBzT +1||f
=* ∣∣PXT+ιB XT+1B*wT+1∣∣F+ * ∣∣PXT+1B zT+1∣∣f .
Recall that WT +1 〜 V and ∣∣Ew〜ν[ww>]∣∣ ≤ O(ɪ). Taking expectation over WT +1 〜 V and
denoting Σ = Ew〜V[ww>], We obtain
EwT+ι 〜V [ER(B, WT +1)]
n EwT+i llp⊥τ+ibXT+1b*wt+JI/+*llPXT+1BzT+1∣∣F
-T EwT+1~V ]Tr pXt+iBXT+1b*wT +1wT+1 (PXT+1BXT +1B*)	+ n- ||PXt+iBzτ +1
n2	n2
2
F
n12 Tr 卜XT+ιB XT +1b*ς (PXt+iB XT +1 B*)	+ n12 11 pXt+1B zτ +1IIf
=n ∣∣P⊥τ+ib XT+1b*ς1∕2∣∣f+n iiPXT+1B zT+1∣∣F
≤ n2 llp⊥τ H XT+miFp1/2 Il2+* IlPXT+： zT+1∣∣F
.n⅛ ||pXt+1B XT+1b*IIf + n12 ∣lPXT+1B zτ+1Hf
.1 . σ2 (kT + kdlog(κn1) + log(1∕δ)) + ɪ ∣∣P ʌ ∣∣2
〜k	cn1 ∙ σ*(W*)	n2 Il XT+1BZT +1∣∣F
σ2 (kT+ kd log(κn1) + log(1∕δ))	1 ∣	∣2
.-----------CnT------------+ n; IlPXT+1B zτ+1I∣F.
(Using 冈.1)
(using Claim A.4)
(using σ2(W*) & τ)
2
F
For the second term above, notice that σ2
ιB zT+1
〜χ2(k), and thus with probability at
2
F
least 1 -
5 we have σ2
1B zT +1
.k + log 1. Therefore we obtain the final bound
，T+i[ER(B, WT +1)] . σ2 (kT + kdIog(Kn1) + log(1∕δ)) + σ2(k + log I)
T+1	cn1T	n2
kd log(κn1)
Cn1T
σ
2
k log δδ k + log1
布 + 而T +
σ
2
kd log(κn1)
Cn1T
where the last inequality is due to cn1 ≥ n2 .
□
A.1 Technical Lemmas
LemmaA.5. Let Odi@ = {V ∈ Rd1 ×d2 | V>V = I} (d1 ≥ d2), and E ∈ (0,1). Then there exists
a subset N ⊂ θd`d that is an E-net of Od1,d2 in Frobenius norm such that |N| ≤ (6^d2)d1d2,i.e.,
for any V ∈ Od1,d2, there exists V0 ∈ N such that kV - V0kF ≤ E.
Proof. For any V ∈ Od1,d2, each column of V has unit `2 norm. It is well known that there exists
an τ√≡--net (in '2 norm) of the unit sphere in Rd1 with size (6^d2)d1. Using this net to cover
2 d2
17
Published as a conference paper at ICLR 2021
all the columns, We obtain a set N0 ⊂ Rd1 ×d2 that is an j-net of Odi@
in Frobenius norm and
|N0| ≤ (守)dιd2
Finally, We need to transform N0 into an -net N that is a subset of Od1,d2 . This can be done
by projecting each point in N0 onto Odid. Namely, for each V ∈ N0, let P(V) be its closest
point in Odid (in Frobenium norm); then define N = {P(V) | V ∈ N0}. Then we have
|N| ≤ |N0| ≤ (6√√d2)d1d2 and N is an e-net of Odid, because for any V ∈ Odid, there
exists V ∈ N0 such that ∣∣V — VIIF ≤ j, which implies P(V) ∈ N and ∣∣V — P(V)IIF ≤
IlV - v∣∣f + ∣∣v - P(V)UF ≤ ∣∣v - v∣∣f + ∣∣v - v∣∣f = 2 ∣∣v - v∣∣f ≤ e.	□
Lemma A.6. Leta1, . . . , an be i.i.d. d-dimensional random vectors such thatE[ai] = 0, E[aiai>] =
I, and a% is PI-Subgaussian. For δ ∈ (0,1), suppose n》ρ4(d + log(1∕δ)). Then with probability
at least 1 - δ we have
1n
0.9I W — 0a aia> W 1.1I.
i=1
Proof. Let A = * PZi aia> 一 I. Then it suffices to show k A∣∣ ≤ 0.1 with probability at least
1 - δ.
We use a standard e-net argument for the unit sphere Sd-i = {v ∈ Rd : kvk = 1}. First, consider
any fixed V ∈ Sd-1. We have v>Av = * Pn=ι[(vτai)2 一 1]. From our assumptions on a%
we know that v>ai has mean 0 and variance 1 and is ρ2 -subgaussian. (Note that we must have
ρ≥ 1.) Therefore (vτai)2 - 1 is zero-mean and 16ρ2-sub-exponential. By Bernstein inequality for
sub-exponential random variables, we have for any e > 0,
pr [|vTAvI >.] ≤ 2eχp (-2min{(16ρψ，1⅛}).
Next, take a 1 -net N ⊂ Sd-1 of Sd-1 with size ∣N∣ ≤ eo⑷.By a union bound over all v ∈ N, we
have
pr [m∈aNIvTAvI >e] ≤2|NIexp (-2min{wy, ⅛})
≤exp (O(d)- nmin {(1⅛, 1⅛}).
Plugging in e = 20 and noticing ρ > 1, the above inequality becomes
pr ]maN IvTAvI > 2θ ] ≤exp (O(d) - 2 ∙ (w0ρ) ≤δ,
where the last inequality is due ton》ρ4 (d +log(1∕δ)).
Therefore, with probability at least 1 一 δ we have maXv∈N IvTAvI ≤ 击.Suppose this indeed
happens. Next, for any U ∈ Sd-1, there exists U ∈ N such that ku 一 u0k ≤ 5. Then we have
∣∣uTAu∣∣ ≤ ∣∣(u0)T Au0 ∣∣ + 2 ∣∣(u - u0)TAu0 ∣∣ + ∣∣(u - u0)TA(u - u0)∣∣
≤ 20+2 ku - u0∣H∣A∣H∣u0k + ku - u0k2∙kAk
≤20+2∙5∙kAk∙1+(1) ∙kAk
≤ 2θ + 2 kAk .
Taking a supreme over u ∈ Sd-1, we obtain kAk ≤ 20 + 2 kAk,i.e., kAk ≤ ι1o .	□
Lemma A.7. If two matrices A1 and A2 (with the same number of columns) satisfy A1TA1	A2TA2,
then for any matrix B (of compatible dimensions), we have
A1TPA⊥iBA1	A2TPA⊥2BA2 .
18
Published as a conference paper at ICLR 2021
As a consequence, for any matrices B and B0 (of compatible dimensions), we have
PA⊥1BA1B02F ≥ PA⊥2BA2B02F.
Proof. For the first part of the lemma, it suffices to show the following for any vector v:
v>A1>PA⊥1BA1v ≥ v>A2>PA⊥2BA2v,
which is equivalent to
min kA1Bw - A1vk22 ≥ min kA2Bw - A2vk22 .
ww
Let w* ∈ argmi□w kA1Bw 一 Aιv∣∣2. Then we have
min kA1Bw — Aιv∣∣2 = kA1Bw* — Aiv∣∣2
w
= (Bw* — v)>A1>A1 (Bw* — v)
≥ (Bw* — v)>A2>A2 (Bw* — v)
= kA2Bw* — A2vk22
≥ min kA2Bw — A2vk22 ,
w
finishing the proof of the first part.
For the second part, from A1>PA⊥ BA1	A2>PA⊥ BA2 we know
(B0)>A1>PA⊥1BA1B0	(B0)>A2>PA⊥2BA2B0.
Taking trace on both sides, we obtain
PA⊥1BA1B02F ≥ PA⊥2BA2B02F,
which finishes the proof.	□
B	Proof of Theorem 5.1
Here we first prove an important intermediate result on the in-sample risk, which explains how the
Gaussian width of FX (Φ) arises.
Claim B.1 (analogue of Claim A.3). Let φ and W i,..., WT be the optimal solution to (2). Then
with probability at least 1 — δ we have
T
XM(Xt)Wt — φ*(Xt)w*∣∣ . σ2 (g(Fx(Φ))2 +log 1).
t=1
>Λ	∕' ɪʌ .1	. ∙ 1 ∙ .	i' 7	1 ʌ	ʌ i' ∕C∖	1
Proof. By the OPtlmaIity of φ and W i,..., WT for (2), we know
TT
X ∣∣yt — Φ(Xt)Wt∣∣ ≤ X kyt — Φ*(Xt)W*k2.
Plugging in yt = φ*(Xt)W* + Zt (Zt 〜N(0, I) is independent of Xt), We get
X ∣∣φ*(Xt)W* + Zt - Φ(Xt)Wt∣∣2 ≤ X kztk2,
t=1	t=1
which gives
T
χ∣∣φ(Xt)Wt- Φ*(Xt)w;∣∣
t=1
≤ 2XhZt, Φ(Xt)Wt - Φ*(Xt)w*i∙
t=1
19
Published as a conference paper at ICLR 2021
Denote Z = [zι,…,ZT] ∈ Rn1 ×T and A = [aι,…,aτ] ∈ Rn1 ×t where at = φ(Xt)Wt 一
φ* (Xt)wt. Then the above inequality reads IlAkF ≤ 2(Z,Ai. Notice that IAl ∈ Fχ(Φ) (c.f. (10)).
F
It follows that
∣A∣f ≤ 2 ZZ, A— ≤ ≤ 2 SUp hZ,Ai.	(30)
∖	kAkF /	Α∈Fχ (Φ)
By definition, We have EZ 卜upA∈Fχ(①)hσ-1Z, A)] = G(FX(Φ)). Furthermore, since the function
Z → supΑ∈Fx (φ) hZ, Ai is 1-Lipschitz in Frobenius norm, by the standard Gaussian concentration
inequality, we have with probability at least 1 一 δ,
SUp hσ-1Z, √4) ≤ E sup hσ-1Z, √4)
A∈Fx (Φ)	[a∈Fx (Φ)
Then the proof is completed using (30).	□
+ ʌ/logɪ = G (FX (Φ)) + jog δ.
The proof is conditioned on several high-probability events, each happening with probability at least
1 一 Ω(δ). By a union bound at the end, the final success probability is also at least 1 一 Ω(δ). We can
always rescale δ by a constant factor such that the final probability is at least 1 一 δ. Therefore, we
will not carefully track the constants before δ in the proof. All the δ's should be understood as Ω(δ).
We use the following notion of representation divergence.
Definition B.1 (divergence between two representations). Given a distribution q over Rd and two
representation functions φ, φ0 ∈ Φ, the divergence between φ and φ0 with respect to q is defined as
Dq (Φ, Φ0) = ∑q (O', Φ0) - ∑q (Φ', Φ)(∑q (Φ, Φ)) ∑q (Φ, Φ0) ∈ R1 ∙
It is easy to verify Dq(φ, φ0)	0, Dq (φ, φ) = 0 for any φ, φ0 and q. See Lemma B.2’s proof.
The next lemma shows a relation between (symmetric) covariance and divergence.
Lemma B.2. Suppose that two representation functions φ, φ0 ∈ Φ and two distributions q, q0 over Rd
satisfy Λq (φ, φ0)占 α ∙ Λq0 (φ, φ0) forsome α > 0. Then it must hold that Dq (φ, φ0)占 ɑ ∙ Dqo (φ, φ0).
Proof. Fix any V ∈ Rk. We will prove v>Dq(φ, φ0)v ≥ ɑ ∙ v>Dqo(φ, φ0)v, which will complete
the proof of the lemma.
We define a quadratic function f : Rk → R as f (w) = [w>, -v>]Λq(φ, φ0) -W ∙ According to
Definition 5.2, we can write
f(w) = w>Σq (φ, φ)w 一 2w>Σq(φ, φ0)v + v>Σq(φ0, φ0)v
=Ex〜q [(w>φ(x) — v>φ0(x))2i .
Therefore we have f(w) ≥ 0 for any w ∈ Rk.6 This means that f must have a global minimizer
in Rk. Since f is convex, taking its gradient Vf (w) = 2Σq(φ, φ)w 一 2Σq(φ, φ0)v and setting the
gradient to 0, we obtain a global minimzer w* = (Σq(φ, φ))土∑q(φ, φ0)v. Plugging this into the
definition of f , we obtain7
min f(w) = f(w*) = v>Dq(φ, φ0)v.	(31)
w∈Rk
w
-V
, we have
Similarly, letting g(w) = [w>, -v>]Λqo (φ, φ0)
min g(w) = v>Dq0 (φ, φ0)v.
w∈Rk
6 Note that we have proved Λq (φ, φ0)	0.
7 Note that (31) implies Dq (φ, φ0)	0.
20
Published as a conference paper at ICLR 2021
From Λq(φ, φ0)占 a ∙ Λq0 (φ, φ0) We know f (W) ≥ αg(w) for any W ∈ Rk. Recall that w* ∈
arg minw∈Rk f (w). We have
αv>Dq0 (φ, φ0)v = α min g(W) ≤ αg(W*) ≤ f (W*)
w∈Rk
= min f(W) = v>Dq(φ, φ0)v.
w∈Rk
This finishes the proof.	□
Claim B.3 (analogue of Claim A.4). Under the setting of Theorem 5.1, with probability at least 1 - δ
we have
_1||P⊥	φ*(X	)∣∣2 . σ2 (G(FX&))2 + logδ)
n2 ∣∣Pφ(Xτ+ι)φ ( T +1)||F .	nισ2(W*)	^
Proof. We continue to use the notation from Claim B.1 and its proof.
LetPt be the empirical distribution over the samples in Xt (t ∈ [T +1]). According to Assumptions 5.1
and 5.2 as well as the setting in Theorem 5.1, we know that the followings are satisfied with probability
at least 1 - δ:
0.9Λp(φ,φ0) √ Λpt(φ,φ0) W 1.1Λp(φ,φ0), ∀φ,φ0 ∈ Φ,∀t ∈ [T],
, ʌ . , , , ʌ , , , , ʌ ,,.
0.9Λp(φ, φ*) W Λpτ+ι (φ, φ*) W 1.1Λp(φ, φ*).
(32)
Tk T . ∙	.1	.	7	1 I ⅛	∙	1	1	. f .1	1	,'	,1	, ,	1	、"丁	/ T	Λ ∖ ∙
Notice that φ and φ* are independent of the samples from the target task, so n ≥ NPoint (Φ,p, 3) is
sufficient for the second inequality above to hold with high probability. Using Lemma B.2, we know
that (32) implies
0.9Dp(φ,φ0) W Dpt(φ,φ0) W 1.1Dp(φ,φ0), ∀φ,φ0 ∈ Φ,∀t ∈ [T],
_ , ʌ . , , _ , ʌ . , , _ , ʌ .,.
0.9Dp(φ,φ*) W Dpτ+ι (φ,φ*) W 1.1Dp(φ, φ*).
(33)
1 ʌ	.1	. ∙	1 ∙ .	Cl	1 ʌ	ʌ	i'	∕C∖	1	7 / TT- ∖ ʌ	I >
By the optimality of φ and w^ι,..., WT for (2), we know φ(Xt)wt = P^(χt)yt
P^(xt )(φ*(Xt)w* + zt). Then we have the following chain of inequalities:
σ2 (G(FX(Φ))2 +log 1)
T
& X∣∣Φ(Xt)w t - Φ*(Xt)w*∣∣
t=1
T2
=X ∣∣pΦ(χt )(φ* (Xt)W*+Zt)- φ*(χt)w*∣∣
t=1
T
=χ||-pΦ(χt)φ*(χt)w*+pΦ(χMt∣∣
t=1
T
=χ(∣∣p⊥(χt)φ*(Xt)w*∣∣ +∣∣pΦ(χt)Zt∣∣)
T
≥ X∣∣P⊥(χt )φ*(Xt )w;∣∣
t=1
T	/	t
=X(w*)>Φ*(Xt)>	I - Φ(Xt) (φ(Xt)>Φ(Xt))
t=1
(Claim B.1)
(cross term is 0)
Φ(Xt)>	Φ*(Xt)w:
T
n1 X(w*)>Dpt (φ, φ*)w*
t=1
21
Published as a conference paper at ICLR 2021
((33))
T
≥ 0∙9nι X(WJi)TDP(φ, φ*)w*
t=1
≥0∙9n1 (DP(B,φ*)) 1/2 j2(W*)
= 0.9n1Tr [Dp(φ, φ*)] σ2(W*)
≥ 01pTr [Dpτ+1 (φ,φ*)] σ2(W*)
= 0⅛k⊥(Xτ+ι)φ*(XT +1”： σ2(W *)，
((33))
completing the proof.
Now We can finish the proof of Theorem 5.1.
ProofofTheorem 5.1. The excess risk is bounded as
ER(φ, WT +ι)
(W>+ιΦ(x) - (wT +i)tΦj
WT +ι
-wT +1
T
,ʌ ..
Λp(φ,φ*)
WT +ι
-wT +1
WT +1
-wT +1
T
, ʌ ,.
λPt+1 (φ,φ )
W T +1
-wT +1
((32))
n~ ∣∣<^ (XT+1)WT+1- φ (XT+1)wT+111
V II-p⊥Xt+ι)φ (XT +1) WT +1 + pΦ(Xt+1)NT +1ll
1
n2
(∣∣p⊥xt+ι)φ* (Xt+1)WT+11∣+1P(XT+1)NT+1∣∣)
1
n2
P(XT+1)φ* (XT +1) WT+1∣∣2 +
σ2(k + log *)
n2
(using χ2 tail bound)
Taking expectation over WT +1 〜V, we get
EWT+1 〜V [ER(B，WT +1)]
p⊥(xt+1 )φ (XT+0 ∣ l FIIEW ~ν[ww>] Ii+ σ (k +2log δ)
1
kn2
p⊥(Xt+1)φ (XT +1) H f +
σ2(k + log ^)
n
1
2
□
L σ2 (G(Fχ(Φ))2 +log 1) + σ2(k + log δ)
k	n1σk (W *)	n2
< σ (G(FX(φ))2 + log R + b2(k + log ^)
r∙-y	rπ	~>	，
H11	n2
(Claim B.3)
(σk(W*) > T)
finishing the proof.
□
22
Published as a conference paper at ICLR 2021
C Proof of Theorem 6.1
C.1 Proof Sketch of Theorem 6.1
Let R = ∣∣Θ*k*. Recall B and W are derived from Eqn. (12) and let Θ := BW. We first note
that the constraint set {∣∣w∣∣2 ≤ R/T, ∣∣B∣∣F ≤ R} ensures ∣∣ W kF ≤ R and ∣∣WBk* ≤ R at global
minimum. On the other hand, our constraint for W, B is also expressive enough to attain any Θ that
satisfies ∣Θ∣* ≤ R. See reference e.g. Srebro and Shraibman (2005). Therefore at global minimum
∣W kF ≤ √R, kB ∣F ≤ RR and ∣Θ ∣* ≤ R.
For the ease of proof, we introduce the following auxiliary functions and parameters. Write
Lι(W) =1 ∣∣∑1/2Θ* - £1/2BW∣∣2 ,
Lλ(W )=Lι(W) + 2 ∣W kF,
L2(w)=2∣Σ1/2θT +ι - Σ1∕2Bw∣∣2,
L I(W =. ∣∣阳θ*- BW "2,
L λ(W )=L i(w ) + 2 ∣w kF
L2(w)=2n^ ∣∣χτ+1 θt+1 - XT+ιB w∣∣2
W)— argmin{L}(W)},
W
wλλ — arg min{L2(w) + λ∕2∣∣wk2},
w
W) . argmin{L }(W)},
W
w2 — arg min{L2(w)}.
w≤r
We define terms ic,1 and ic,2 that will be used to bound intrinsic dimension concentration error in
the input signal. Namely with high PrObability, k∑1∕2Θ∣∣- J1∕nι PT=I ∣∣X∕tk2 ≤ Gc,ι ∣∣Θ∣∣*, and
similarly k∑1∕2Bv∣∣ - J+kxBvk2 ≤ Qc,2 kvk2. Additionally we use ee,i, i ∈ {1, 2} to bound
the estimation error (for fixed design) incurred when using noisy label yT+1 and Y .
The choice of ee,i, and ic,i are respectively justified in Lemma C.5, Claim C.4, Lemma C.10 and
Claim C.11, along with some more detailed descriptions.
Proof of Theorem 6.1.
Eθ*〜V ER(B, WT +1)
=Eθ*〜V L(WT +ι)
.Eθ*〜V L2 (WT +1) + E2c,2r2
.Eθ*〜V L2 (W2 ) + e2e,2r + e2c,2r2
≤ Eθ*〜V L2 (W)) + 曦2r + e2c,2r2
.Eθ*〜V L2 (w)) + 曦2r + e2c,2r2
=TLI(W)) + ele,2r + e2c,2r2
丁 + ele,2r + e2c,2r2
VeJR + e2c,1R2	2	.	2	2
—'T-'— + 屋e,2r + eic,2r .
(Claim C.11)
(Lemma C.4)
(Definition of W 2)
(Claim C.11)
(Claim C.3)
(Lemma C.2)
(Choices of λ)
Each step is with high probability 1 一 δ∕10 over the randomness of X or XT +1. Therefore overall
by union bound, with probability 1 - δ, by plugging in the values of ic,i and ee,i we have:
Eθ〜ER(B, WT +1) ≤√RO fp√Tr≡ + p√3
√T ∖ √Tn1	√n2
+半O (空+吗
T	n1	n2
Notice a term kΣk∕n1 is absorbed by kΣk∕n2 since we assume n1 ≥ n2 .
□
23
Published as a conference paper at ICLR 2021
Claim C.1 (guarantee with source regularization).
一∣∣X(Θ* - Θ)kF + λkΘk* ≤ 3λ∣∣Θ*k* ≤ 3λR,
and IlBIlF ≤ 3R, ∣∣W∣∣F ≤ 3Rforany λ ≥ 2∣∣X*(Z)∣∣2∙
Here X* is the adjoint operator of X such that X*(Z) = PT=I X>zte>.
Proof. With the optimality of Θ We have:
2n1
Let ∆ = Θ 一 Θ*. Therefore
1-∣X(Θ - Θ*) - ZkF + λ∣Θk* ≤ 1-∣ZkF + λ∣θ*k*
2n1
1- kx(∆)kF
2n1
1
≤λ(∣Θ*k* -∣θk*) + -h∆,X*(Z)i
n1
≤λ∣θ*k* + -1 kθ*k* ∙∣x*(Z)k + -1 kΘk* ∙∣x*(Z)k- λ∣Θk*
n1	n1
≤λkΘ*k* + λ∕2kΘ*k* + λ∕2∣∣Θk* - λkΘk*
(Let λ ≥ —kX* (Z) k)
=2 λkΘ*k*- 1 λkΘ k*.
Therefore *∣∣X(∆)kF + 2∣∣<Θk* ≤ 2λ∣∣Θ*k*, and clearly both terms satisfy 看∣∣X(∆)kF ≤
3λkΘ*k* and kΘk* ≤ 3kΘ*k*.	ɪ
Lemma C.2 (source task concentration). For a fixed δ > 0, let λ = e2e,1 + i2c,1R, we have
Lλ(Wλ) .λR
kwλkF .√R.
with probability 1 - δ∕10.
Proof of Lemma C.2.
2
kWλkF <=Lλ(Wλ)
λ
2
≤ 二 Lλ(W)
λ
=2 {1 k∑^2(θ*- Θ)kF + λ ∣WkF
λ2	2
(Definition of W1λ)
≤ 2 {2( £ kX(Θ* - Θ)∣f + O(q°,l )R)2 + 2 ∣WkF}
≤2 {n; kX (θ* - BWλ)kF+2 kWλkF+O 虑JR」
≤- {6λR + O(e2c,ιR2)}
λ
2
=γ O(XR)
λ
=O(R).
(from Claim C.1)
Thus both results have been shoWn.
24
Published as a conference paper at ICLR 2021
Claim C.3 (Source and Target Connections).
Eθ*j L2(wλ) =L1(Wλ)
Proof of Claim C.3.
wλ = (B >ςb + λi )-1 B >ςθt+1 =： SλθT+1,
where Sλ := (B>ΣB + λI)-1 B>Σ.
Eθ*j L2(wλ) = Eθ*j k∑2(I — Sλ)θT +1k2
=T k∑2(i- Sλ)θT+1k2
=T L1(Wλ).
□
Lemma C.4 (Estimation Error for Target Task).
ʌ , . . ʌ 、
L2(w) - L2 (W)
R	C ,C	,——	C
≤√T=σ(^og 1∕δ产2 log(n2)Pk^k =: e2e,2r∙
ProofofLemma C.4. With the definition of W We write the basic inequality:
11
T— kXτ +1(BW - θ*) - ZT +1kF ≤ — kXτ +1(BW - θ*) - ZT+1kF,
2n2	2n2
Therefore by rearranging we get:
k—kXτ+1B(W - W)IIF ≤—hw - W, B >χ>+1zτ+1i
2n2	n2	+
≤ 'R∕τkB>χ> zτ+1kF
n2	+
R	c/O...........Z—
.-7^σ log2"(l∕δ)log(n2) √i∑k	(ClaIm C.6)
Tn2
=O(re2e,2 )
□
C.2 Technical Lemmas
This section includes the technical details for several parts: bounding the noise term from basic
inequality; and intrinsic dimension concentration for both source and target tasks.
Lemma C.5 (Regularizer Estimation). For X ∈ Rn×d drawn from distribution p with covariance
matrix Σ, and noise Z 〜N(0, σ2In), with high probability 1 一 δ, we have
曦 1：= nkX>Zk2 ≤ √nσ (logδ)	Iog(T+n)PTiNk+Tr㈤.
Proof. We use matrix Bernstein with intrinsic dimension to bound λ (See Theorem 7.3.1 in Tropp
et al. (2015)).
Write A = √X>Z = √ PT=1 X>zte> =： PT=1 St
T1
EX,Z [AA>] = Ex ɪ2 —X> EZ [ztz>]X
t=1 n1	t
=σ2TΣ
25
Published as a conference paper at ICLR 2021
T1
Ex,z[A>A] = E 而e Ex,z [z>XX>zte>]
t=1 n
T1
=E — Ex,z[z> XX >zt]ete>
t=1 n
=σ2Tr(Σ)In.
Therefore the matrix variance statistic of the sum v(A) satisfies: v(A) = σ2 max{T kΣk, Tr(Σ)}.
Denote V = diag([T Σ, Tr(Σ)I]) and its intrinsic dimension dΣ = tr(V )∕kV k. Tr(V ) = σ2(T +
n)Tr(Σ), and kV k2 ≥ σ2Tr(Σ). Therefore dΣ ≤ T + n.
Finally from Hanson-Wright inequality, the uPPer bound on each term is kSt k2 ≤ kXzt k2 ≤
σ2Tr(Σ) + σ2k∑k log δ + σ2k∑∣∣Fʌ/logɪ with probability 1 - δ. Thus using ∣∣∑∣∣f ≤ Tr(Σ),
kStk ≤σ
logδ再⑶ + 间 log 1=: L
Then from intrinsic matrix bernstein (Theorem 7.3.1 in TroPP et al. (2015)), with probability 1 - δ
WehaVe, ∣∣Ak ≤ O(σ Jlog 1V log(d∑) + σ log 1L log(d∑)), which gives
kAk≤σ
log 1Tk∑k log(T + n) + log 1Tr(Σ)log(T + n) + log 1 σLlog(T + n)
δδδ
3/2
log(T + n)√T k∑k +Tr(Σ).
σ
□
Claim C.6 (target noise concentration). For a fixed δ > 0, with probability 1 - δ∕10, e22e2 :=
√n2kB>X>+1zk2 ≤ O(log2∕3(1∕δ)log(n2)qTr(B>∑B)) ≤ O(√WR).
Proof. The first inequality directly follows from Lemma C.5. Meanwhile Tr(B> ΣB) =
h∑,BB>i ≤ k∑k2∣∣BB>k* . ∣∣∑k2R. This finishes the proof.	□
Definition C.7. The sub-gaussian norm of some vector y is defined as:
∣y∣ψ2 := sup ∣hy, xi∣ψ2,	(34)
x∈Sn-1
where Sn-1 denotes the unit Euclidean sphere in Rn.
Definition C.8. Let T ⊂ Rd be a bounded set, and g be a standard normal random vector in Rd,
i.e., g 〜N(0, Id).Then the quantities
w(T) := E suphg, xi, andγ(T) := E sup |hg, xi|	(35)
x∈T	x∈T
are called the Gaussian width of T and the Gaussian complexity of T, respectively.
Theorem C.9 (Restated Matrix deviation inequality from Vershynin (2017)). Let A be an m × n
matrix whose rows ai are independent, isotropic and sub-gaussian random vectors in Rn. Let
T ⊂ Rn be a fixed bounded set. Then
Esup ∣∣Ax∣2 - √m∣x∣2∣ ≤ Cp2γ(T),	(36)
x∈T
where K = maxi ∣Ai∣ψ2 is the maximal sub-gaussian norm of the rows of A. A high-probability
version states as follows. With probability 1 - δ,
SUp l∣Ax∣2 - √m∣x∣2∣ ≤ Cp2[γ(T) + √log(2∕δ)r(T)],	(37)
x∈T
where the radius r(T) := supx∈T ∣x∣2.
26
Published as a conference paper at ICLR 2021
Lemma C.10 (intrinsic dimension concentration). Let X, Xt, t ∈ [T] be n × d matrix whose rows x
are independent, isotropic and sub-gaussian random vectors in Rd that satisfy Assumption 4.1, and
the whitening distribution is with sub-gaussian norm C1ρ, where E[x] = 0 and E[xx>] = Σ. For a
fixed δ > 0, and any v ∈ Rd, we have
k∑"v∣∣2 ≤ √nkXvk2 + C√p2 (PTr(∑) + Plog(2∕δ)k∑k) kvk2.
For any Θ ∈ Rd×T, we further have
k∑"θ∣∣F ≤
1
√n t
T
X kXtθtk2 + Gc,ιkθ∣∣*
t=1
(38)
where Gc,ι := 2√√P- (pTr(∑) + plog(2∕δ)∣∑k), with probability 1 一 δ.
Proof. WeUseTheorem C.9. Let T = {v : ∣∑-^2v∣2 ≤ 1}. Let X = ∑"z,X = ZΣ1/2. Then
Y(T) = pTr(Σ), r(T) = ∣∣∑k1/2. We note with probability 1 一 δ,
SUP ɪ ∣XV∣2 -k∑"v∣2
kvk=1	n
= SUP ； kZ vk2 一 I∣v∣∣2
v∈τ √n
≤C√ρ2 (γ(T) + Pl0g(27δ)r(T))
n
=C√ρ2 (PTr(∑)+piοg(2∕δ)k∑k).
n
TherefOre ∣√nkXvk2 一 |同1/2切川 ≤ √ (PTrw + Plog(27δ)p∣) ,∀kvk = 1∙ Then by
homogeneity of v, for arbitrary v, we have
√n kXV∣2 -k∑1∕2vk2
≤ kvk2 C⅛ (PTr(∑) + piοg(2∕δ)∣∑k).
n
'----------------------------}
{z∕f^^
term I
Notice when n》C2ρ4(Tr(Σ) + k∑k log1∕δ), term I ≤ 0.1√λ. Therefore ∣k∑1∕2v∣∣2 —
* kX Vk2∣≤ 0.1√λkvk.
Write Θ = UDV>, where D = diag(σι, σ2,…，στ).
1T	1T
—X kXtθtk2 = — X σ2kXtutk2
nn
t=1	t=1
T	C 2	2
≥ X σ (k∑1∕2utk2 — kutk2 Cn (PT(∑)+piog(2∕δ)k∑k))
> Xσ (k∑1∕2utk2 - 2|同1/2Utkkutk2CP2 (PTr(∑)+ Plog(2∕δ)k∑k))
= k∑1∕2ΘkF - 2√n2 (PT(∑) + Plog(2∕δ)k∑k) Xσt(σtk∑1∕2utk2)
≥k∑"θ∣∣F - 2√√P2 (PT(∑) + Plog(2∕δ)k∑k) Xσt(maxσtk∑1∕2ut∣∣2)
≥k∑"θ∣∣F - 2√ρ2 (PT(∑)+ Plog(2∕δ)k∑k) kθk*k∑1∕2θkF∙
n
27
Published as a conference paper at ICLR 2021
Therefore k∑V2Θ∣∣F
+ 2√ρ2 (E+ pog(2E) kθk*.
X------------------}
^^{^^≡
term II
□
Claim C.11. Let X be n2 × d matrix whose rows x are independent, isotropic and sub-gaussian
random vectors in Rd that satisfy Assumption 4.1, where E[x] = 0 and E[xx>] = Σ. Let ΣB =
B >ΣB for some matrix B that satisfies ∣∣BB>k* . R. Then for a fixed δ > 0, and any V ∈ Rd
we have： k∑∕vk ≤ n⅛kxBvk + l¾,2kvk, where <⅛,2 ：=隽pRk∑k log(1∕δ), and C is a
universal constant.
ɪʌ C mi ♦	1, 1 ∙	, 1	τ	z-5τc1	1	PT-I TT- iɔ ʌ τ . ∙	.1	1	♦
Proof. This result directly uses Lemma C.10 when replacing X by XB. Notice now the subgaussian
norm for the whitening distribution for B>x remains the same as Cιρ. Therefore ∣∣Σ1/2!?v∣∣2 ≤
√n2kXBv∣2 + Skvk2 ≤ k∑"Bv∣2 ≤ √n2∣Xv∣2 + Skv∣. Here S = Cρ2∕√n2(PT(∑B) +
Plog(2∕δ)k∑Bk) ≤ C0ρ2∕√n2√k∑kRlog(1∕δ) =: S
□
D Proof of Theorem 7.1
First, we describe a standard lifting of neural networks to infinite dimension linear regression Wei et al.
(2019); Rosset et al. (2007); Bengio et al. (2006). Define the infinite feature vector with coordinates
φ(x)b = (b>x)+ for every b ∈ Sd0-1. Let αt be a signed measure on Sd0-1. The inner product
notation denotes integration: α>φ(x) , Sd0-1 φ(x)bdα(b). The tth output of the infinite-width
neural network is fαt (x) = hαt, φ(x)i. Consider the least-squares problem
min
ɑι,...,αt"supp(at)∣≤d
kαk2,1 ≤r
2n X(yit - α>φ(Xi))2,
i,t
(39)
where α(u) = [αι(u),..., ɑτ(u)],and ∣∣α∣∣2,1 = f^-ι ∣∣α(δ)k2d(b). Theregularizer corresponds
to a group `1 regularizer on the vector measure α.
Proposition D.1. Let γd be the value of Equation (17) when the network has d neurons and γd? be
the value of Equation (39). Then
γd = γd? .
(40)
Proof. Let B, W be solutions to Equation (17). Let B = BD-1 and De be a diagonal matrix whose
entries are βj = ∣∣B>ejk2. The network ∕b,w(x) = W>Dβ(B>x)+ and it satisfies
kβk2 = kBkF.
We first show that Yd ≤ γd. Define at( jj) = Wtjej. We verify that
dd
α>φ(X) = X αt(bj )φ(X)% = X Wtjej 0>x)+ = w>(B>x)+ = fB,wt(x).
j=1	j=1
Due to the regularizer, and using the AM-GM inequality, at optimality ej = kWej k2. Next, we
verify that the two regularizer values are the same. Let Wj- be the j-th row vector of W. We have
d
ka∣2,1 = X β kwj Il
j=1
d
≤ X ej2∕2 + kWjk2∕2
j=1
28
Published as a conference paper at ICLR 2021
=1 kβk2 + 1 kWkF ≤ R.
Thus the network given by αt>φ(x) has the same network outputs and regularizer values. Thus
γ? ≤ γd .
Finally, We show that Yd ≤ γ?. Let bj for j ∈ [d] be the support of the optimal measure of (39).
Define βj = Jka(δj)∣2, B = BDe where B is a matrix whose rows are bj, and W such that
Wjt = αt(bj )∕Jkα(bj )k.
We verify that the network values agree
e>fB,w (x) = e>W >Dβ (B >x)+ = X Wjtej (b> x)+ = α>φ(x).
j
Finally by our construction βj = kWej k, so the regularizer values agree. Thus γd = γd? .
□
Finally, we note that the regularizer can be expressed in a variational form as8
kαk2,1
min
b,W ：at(b) = e(b)wt(b)
kβk22 + kWk2F,
where ∣∣βk2 = R β(b)2d(b) and ∣∣ W∣∣F = PtR wt(b)2d(b). With these in place, we note that
Equation (39) can be expressed as Equation (12) with B constrained to be a diagonal operator and
xit as the lifted features φ(xit).
Proof of Theorem 7.1. The global minimizer of Equation (39) with d = ∞ may have infinite support,
so the corresponding value may not be achieved by minimizing (17). However, Theorem 6.1 only
requires that the we obtain a learner network with regularized loss less than the regularized loss of
the teacher network. Since the teacher network has d neurons, this value is attainable by (17). Thus
the finite-size network does not need to attain the global minimum of (39) for Claim C.1 to apply.
Since Theorem 6.1 has no dependence (even in the logarithmic terms) on the input dimension of the
data, it can be applied when the input features the infinite-dimensional feature vector φ(x). The only
part of the proof of Theorem 6.1 specific to the nuclear norm is that the dual norm is the operator
norm. In Lemma C.5 we had an upper bound on 1 ∣∣X>Z∣∣2. Since we use the ∣∣ ∙ ∣∣2,1 norm, we must
upper bound 1 ∣∣X>Z∣∣2,∞ , the dual of the (2,1)-norm. Note that ∣∣A∣∣2,∞ ≤ ∣∣A∣∣2, so the upper
bound in Lemma C.5 still applies. Thus, Theorem 7.1 follows from Theorem 6.1.	□
Proof of (21). The test error of (20) is given by
E[ER(fB,w)] . σ2√n(∣BT +ι∣2 + ∣wT +ι∣2) E Xi吟，[∣Φ(X)>z∣∞],	⑷)
Z	Z 〜N(0,σ2I)
via the basic inequality (c.f. proof of Claim C.2 and C.4). By the matrix Bernstein inequality (c.f.
Lemma C.5 orWei et al. (2019)), Ex 吧d? Z〜N(0 ∕)[∣∣Φ(X)>z∣∞] . ptr(∑). When BT +1, WT +1
are sampled from the same distribution as the source tasks, then 1 (∣BT +1∣2 + ∣∣wT +∕∣2) ≥ 号.
Thus we conclude
E[ERfB,w)].σ √t Stnp.
□
8Informally if α ∈ Rd×t with D potentially infinite, kα∣∣2,1 = min@=diag(b)w 11 ∣∣b∣∣2 + 2IlWIIF.
29