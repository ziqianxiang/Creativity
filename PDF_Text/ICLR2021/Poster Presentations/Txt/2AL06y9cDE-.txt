Published as a conference paper at ICLR 2021
Towards Robust Neural Networks via Close-
loop Control
Zhuotong Chen1§, Qianxiao Li2,3§, Zheng Zhang* 1
1Department of Electrical & Computer Engineering, University of California, Santa Barbara, CA 93106
2Department of Mathematics, National University of Singapore, Singapore
3Institute of High Performance Computing, A*STAR, Singapore
ztchen@ucsb.edu, qianxiao@nus.edu.sg, zhengzhang@ece.ucsb.edu
Ab stract
Despite their success in massive engineering applications, deep neural networks
are vulnerable to various perturbations due to their black-box nature. Recent study
has shown that a deep neural network can misclassify the data even if the input
data is perturbed by an imperceptible amount. In this paper, we address the ro-
bustness issue of neural networks by a novel close-loop control method from the
perspective of dynamic systems. Instead of modifying the parameters in a fixed
neural network architecture, a close-loop control process is added to generate con-
trol signals adaptively for the perturbed or corrupted data. We connect the robust-
ness of neural networks with optimal control using the geometrical information
of underlying data to design the control objective. The detailed analysis shows
how the embedding manifolds of state trajectory affect error estimation of the
proposed method. Our approach can simultaneously maintain the performance on
clean data and improve the robustness against many types of data perturbations.
It can also further improve the performance of robustly trained neural networks
against different perturbations. To the best of our knowledge, this is the first work
that improves the robustness of neural networks with close-loop control 1.
1	Introduction
Due to the increasing data and computing power, deep neural networks have achieved state-of-the-
art performance in many applications such as computer vision, natural language processing and
recommendation systems. However, many deep neural networks are vulnerable to various malicious
perturbations due to their black-box nature: a small (even imperceptible) perturbation of input data
may lead to completely wrong predictions (Szegedy et al., 2013; Nguyen et al., 2015). This has been
a major concern in some safety-critical applications such as autonomous driving (Grigorescu et al.,
2020) and medical image analysis (Lundervold & Lundervold, 2019). Various perturbations have
been reported, including the `p norm based attack (Madry et al., 2017; Moosavi-Dezfooli et al., 2016;
Carlini & Wagner, 2017), semantic perturbation (Engstrom et al., 2017) etc. On the other side, some
algorithms to improve the robustness against those perturbations have shown great success (Madry
et al., 2017). However, most robustly trained models are tailored for certain types of perturbations,
and they do not work well for other types of perturbations. Khoury & Hadfield-Menell (2018)
showed the non-existence of optimal decision boundary for any `p-norm perturbation.
Recent works (E, 2017; Haber & Ruthotto, 2017) have shown the connection between dynamical
systems and neural networks. This dynamic system perspective provides some interesting theoretical
insights about the robustness issue. Given a set of data x0 ∈ Rd and its labels y ∈ Rl with a joint
distribution D, training a neural network can be considered as following
min E	[Φ(xT , y)], s.t. xt+1 = f (xt, θt),
θ (χo,y)〜D
§ Equal contributing authors.
1A Pytorch implementation can be found in:https://github.com/zhuotongchen/
Towards-Robust-Neural-Networks-via-Close-loop-Control.git
1
Published as a conference paper at ICLR 2021
Figure 1: The structures of feed-forward neural network (black) and the proposed method (blue).
where θ are the unknown parameters to train, and f, Φ represent the forward propagation rule and
loss function (e.g. cross-entropy) respectively. The dynamical system perspective interprets the
vulnerability of neural networks as a system instability issue, which addresses the state trajectory
variation under small perturbations applied on initial conditions. The optimal control theory focuses
on developing a control model to adjust the system state trajectory in an optimal manner. The first
work that links and extends the classical back-propagation algorithm using optimal control theory
was presented in Li et al. (2017), where the direct relationship between the Pontryagin’s Maximum
Principle (Kirk, 1970) and the gradient based network training was established. Ye et al. (2019)
used control theory to adjust the hyperparameters in the adversarial training algorithm. Han et al.
(2018) established the mathematical basis of the optimal control viewpoint of deep learning. These
existing works on algorithm development are open-loop control methods since they commonly treat
the network weights θ as control parameters and keep them fixed once the training is done. The fixed
control parameters θ operate optimally for data sampled from the data distribution D. However,
various perturbation methods cause data distributions to deviate from the true distribution D (Song
et al., 2017) and cause poor performance with the fixed open-loop control parameters.
1.1	Paper Contributions
To address the limitation of using open-loop control methods, we propose the Close- Loop Control
Neural Network (CLC-NN), the first close-loop control method to improve the robustness of neural
networks. As shown in Fig. 1, our method adds additional blocks to a given T -layer neural network:
embedding functions Et , which induce running losses in all layers that measure the discrepancies
between true features and observed features under input perturbation, then control processes gen-
erate control variables ut to minimize the total running loss under various data perturbations. The
original neural network can be designed by either standard training or robust training. In the lat-
ter case, our CLC-NN framework can achieve extra robustness against different perturbations. The
forward propagation rule is thus modified with an extra control parameter ut ∈ Rd0
xt+1 = f(xt, θt, ut).
Fig. 1 should not be misunderstood as an open-loop control. From the perspective of dynamic
systems, x0 is an initial condition, and the excitation input signal is ut (which is 0 in a standard
feed-forward network). Therefore, the forward signal path is from ut to the internal states xt and
then to the output label y. The path from xt to the embedding function Et(xt) and then to the
excitation signal ut forms a feedback and closes the whole loop.
The technical contributions of this paper are summarized below:
•	The proposed method relies on the well accepted assumption that the data and hidden state man-
ifolds are low dimensional compared to the ambient dimension (Fefferman et al., 2016). We
study the geometrical information of the data and hidden layers to define the objective function
for control. Given a trained T -layer neural network, a set of embedding functions Et are trained
off-line by minimizing the reconstruction loss kE(xt) - xt k over some clean data from D only.
The embedding functions support defining a running loss required in our control method.
•	We define the control problem by dynamic programming and implement the online iterative solver
based on the Pontryagin’s Maximum Principle to avoid the curse of dimensionality. The proposed
close-loop control formulation does not require prior information of the perturbation.
•	We provide a theoretical error bound of the controlled system for the simplified case with linear
activation functions and linear embedding. This error bound reveals how the close-loop control
improves neural network robustness in the simplest setting.
2
Published as a conference paper at ICLR 2021
2	Related Works
Many techniques have been reported to improve the robustness of neural networks, such as data
augmentation (Shorten & Khoshgoftaar, 2019), gradient masking (Liu et al., 2018), etc. We review
adversarial training and reactive defense which are most relevant to this work.
Adversarial Training. Adversarial training is (possibly) the most popular robust training method,
and it solves a min-max robust optimization problem to minimize the worse-case loss with perturbed
data. Adversarial training effectively regularizes the network’s local Lipschitz constants of the loss
surface around the data manifold (Liu et al., 2018). Zhang et al. (2019) formulated the robustness
training using the Pontryagon’s Maximum Principle, such open-loop control methods result in a
set of fixed parameters that operates optimally on the considered perturbation. Liu et al. (2020a;b)
considered a close-loop formulation from the differential dynamic programming perspective, this
algorithm is categorized as a open-loop control method because it utilizes the state feedback infor-
mation to boost the training convergence and results in a set of fixed controls for any unseen data.
On the contrary, the proposed CLC-NN formulation adaptively targets on the inputs with different
control parameters and is capable of distinguishing clean data by generating no control.
Reactive Defense. A reactive defense method tries to reject or pre-process the input data that may
cause mis-classifications. Metzen et al. (2017) rejected perturbed data by using adversarial detectors
that are trained with adversarial data to detect abnormal data during forward propagation. Song et al.
(2017) estimated the input data distribution D with a generative model (Oord et al., 2016) to detect
data that does not belong to D, it applies a greedy method to search the local neighbour of input
data for a more statistically plausible counterpart. This purification process has shown improved
accuracy with adversarial data contaminated by various types of perturbations. Purification can be
considered as a one-step method to solve the optimal control problem that has the objective function
defined over the initial condition only. On the contrary, the proposed CLC-NN solves the control
problem by the dynamic programming principle and its objective function is defined over the entire
state trajectory, which guarantees the optimality for the resulted controls.
3	The Close-loop Control Framework for Neural Networks
Now we present a close-loop optimal control formulation to address the robustness issue of deep
learning. Consider a neural network consisting of model parameters θ equipped with external con-
trol policy π, where π ∈ Π is a collection of functions Rd → Rd0 acting on the state and outputting
the control signal. The feed-forward propagation in a T -layer neural network can be represented as
xt+1 = f(xt, θt,∏t(xt)), t = 0,…，T - 1.
(1)
Given a trained network, we solve the following optimization problem
T-1
minE(xo,y)〜D [J(xo, y, π)] := minE(x0,y)〜D Φ(xτ, y) + E^Lg, ∏s(Xs)) , St Eq. (1),
π	∏	, “
s=0
(2)
where π collects the control policies ∏o,…，∏t-ι for all layers. Note that (2) differs from the
open-loop control used in standard training. An open-loop control that treats the network parameters
as control variables seeks for a set of fixed parameters θ to match the output with true label y by
minimizing the terminal loss Φ, and the running loss L defines a regularization for θ. However,
the terminal and running losses play different roles when our goal is to improve the robustness of a
neural network by generating some adaptive controls for different inputs.
Challenge of Close-loop Control for Neural Networks. Optimal control has been well studied
in the control community for trajectory optimization, where one defines the running loss as the
error between the actual state Xt and a reference state Xt,ref over time interval [0, T]. The resulting
control policy adjusts Xt and makes it approach Xt,ref. In this paper, we apply the idea of trajectory
optimization to improve the robustness of a neural network via adjusting the undesired state of Xt .
However, the formulation is more challenging in neural networks: we do not have a “reference”
state during the inference process, therefore it is unclear how to define the running loss L.
In the following, we investigate manifold embedding of the state trajectory to precisely define the
loss functions Φ and L of Eq. (2) required for the control objective function of a neural network.
3
Published as a conference paper at ICLR 2021
3.1	Manifold Learning for State Trajectories
State Manifold. Our controller design is based on the “manifold hypothesis”: real-world high
dimensional data can often be embedded in a lower dimensional manifold M (Fefferman et al.,
2016). Indeed, neural networks extract the embedded features from M. To fool a well-trained neural
network, the perturbed data often stays away from the data manifold M (Khoury & Hadfield-Menell,
2018). We consider the data space Z (X ∈ Z, ∀x 〜D) as: Z = Zk L Z⊥, where Zk contains the
embedded manifold M and Z⊥ is the orthogonal complement of Zk . During forward propagation,
the state manifold embedded in Zk varies at different layers due to both the nonlinear activation
function f and state dimensionality variation. Therefore, we denote Zt = Zkt L Z⊥t as the state
space decomposition at layer t and Mt ∈ Zkt . Once an input data is perturbed, the main effects
of causing misclassifications are in Z⊥ . Therefore, it is important to measure how far the possibly
perturbed state Xt deviates from the state manifold Mt .
Embedding Function. Given an embedding function Et that encodes Xt onto the lower-
dimensional manifold Mt and decodes the result back to the full state space Zt, the reconstruction
loss kEt(Xt) - Xtk measures the deviation of the possibly perturbed state Xt from the manifold Mt.
The reconstruction loss is nonzero as long as Xt has components in Z⊥t . The embedding functions
are constructed offline by minimizing the total reconstruction losses over a clean training data set.
•	Linear Case: Et(∙) can be considered as Vr(Vr)T where Vr forms an orthonormal basis for
Zkt. Specifically one can first perform a principle component analysis over a collection of hidden
states at layer t, then Vtr can be obtained as the first r columns of the resulting eigenvectors.
•	Nonlinear Case: we choose a convolutional auto-encoder (detailed in Appendix B) to obtain
a representative manifold embedding function Et due to its ease of implementation. Based on
the assumption that most perturbations are in the Z⊥ subspace, the embeddings are effective to
detect the perturbations as long as the target manifold is of a low dimension. Alternative manifold
learning methods such as Izenman (2012) may also be employed.
3.2	Formulation for the Close-Loop Control of Neural Networks
Control Objectives. The above embedding function allows us to define a running loss L:
L(xt,∏t(xt), Et(∙)) = ∣∣Et(Xt)- Xtk2 + (∏t(xt))TRnt(Xt).	⑶
Here the matrix R defines a regularization term promoting controls of small magnitudes. In practical
implementations, using a diagonal matrix R with small elements often helps to improve the perfor-
mance. Now we are ready to design the control objective function of CLC-NN. Different from a
standard open-loop control, this work sets the terminal loss Φ as zero because no true label is given
during inference. Consequently, the close-loop control formulation in Eq. (2) becomes
T -1
minE(xo,y)〜D [J(xo, y, ∏)] ：= minE(χ°,y)〜DE [L(Xt，∏t(xt), Et(∙))], s∙t∙ Eq. (1).	(4)
π	∏
t=0
Assume that the input data is perturbed by a bounded and small amount, i.e.,
xe,0 = xo + 〜z,
where z can be either random or adversarial. The proposed CLC-NN adjusts the perturbed state tra-
jectory x,t such that it stays at a minimum distance from the desired manifold Mt while promoting
small magnitudes of controls.
Intuition. We use an intuitive example to show how CLC-NN controls the state trajectory of un-
seen data samples. We create a synthetic binary classification data set with 1500 samples. We train
a residual neural network with one hidden layer of dimension 2, and adopt the fast gradient sign
method (Goodfellow et al., 2014) to generate adversarial data. Fig. 2 (a) and (b) show the states of
clean data (red and blue) and of perturbed data (black and gray) at t = 0 and t = 1, respectively. The
CLC-NN adjusts the state trajectory to reduce the reconstruction loss as shown in Fig. 2 (c) and (d),
where lighter background color represents lower reconstruction loss. Comparing Fig. 2 (a) with (c),
and Fig. 2 (b) with (d), we see that the perturbed states in Fig. 2 (a) and (b) deviate from the desired
state manifold (light green region) and has a high reconstruction loss. Running 1000 iterations of
Alg. 1 adjusts the perturbed states and improves the classification accuracy from 86% to 100%.
4
Published as a conference paper at ICLR 2021
(a) x0, x,0
(b) xι, xe,ι	(C) Controlled xe,o	(d) Controlled xe,ι
Figure 2: (a) and (b) show the states of clean data (red and blue) and of perturbed data (black and
gray) at the initial and hidden layers respeCtively. The yellow and green baCkgrounds represent the
two Classes prediCted by the network. Some of the perturbed data are mis-Classified. (C) and (d)
show the adjusted states after the proposed Close-loop Control. The baCkground lightness from light
to dark represent the inCreasing reConstruCtion loss kEt(xt) - xtk22.
4	Implementation via the Pontryagin’s Maximum Principle
Dynamic Programming for Close-Loop Control (4). The Control problem in Eq. (4) Can be
solved by the dynamiCal programming prinCiple (Bellman, 1952). For simpliCity we Consider one
input data sample, and define a value funCtion V : T × Rd → R (where T := {0, 1, . . . , T - 1}).
Here V (t, x) represents the optimal Cost-to-go funCtion of Eq. (4) inCurred from time t at state x.
One Can show that V (t, x) satisfies the dynamiC programming prinCiple
V(t, X)= inf [V(t + 1, X + f(x, θt, ∏(x))) + L(x, ∏(x), Et(•))].	(5)
π∈Π
Eq. (5) gives a neCessary and suffiCient Condition for optimality of Eq. (4), and it is often solved
baCkward in time by disCretizing the entire state spaCe. The state dimension of a modern neural
network is at the order of thousands or even higher, therefore, disCretizing the state spaCe and direCtly
solving Eq. (5) is intraCtable for real-world appliCations due to the Curse of dimensionality.
Solving (5) via the Pontryagin’s Maximum Principle. To overCome the Computational Chal-
lenge, the Pontryagin’s Maximum PrinCiple (Kirk, 1970) Converts the intraCtable dynamiCal pro-
gramming into two ordinary differential equations and a maximization Condition. Instead of Com-
puting the Control poliCy π of Eq. (5), the Pontryagin’s Maximum PrinCiple provides a neCessary
condition for the optimality with a set of control parameters [u0, • • • , UT]. The mean-field Pontrya-
gin’s Maximum PrinCiple Can be Considered when the initial Condition is a batCh of i.i.d. samples
drawn from D. Specifically, we trade the intractable computational complexity with processing time
for solving the Hamilton equations and its maximization condition for every newly observed data.
To begin with, we define the Hamiltonian H : T × Rd × Rd × Rl × Rm → R as
H(t, Xt, pt+1 , θt, ut)	:=	ptT+1	•	f(Xt,	θt,	ut)	- L(Xt,	ut, Et (•)).	(6)
Let x* denote the corresponding optimally controlled state trajectory. There exists a co-state process
p* : [0, T] → Rd such that the Hamilton,s equations
x*+ι = VpH(t, x*, p*, θt, u*),	(X*,y) 〜D,	(7)
p* = vxH(t, Xt, p*+ι, θt, u*),	PT = 0,	(8)
are satisfied. The terminal co-state pT = 0, since we do not consider the terminal loss Φ(XT, y).
Moreover, we have the Hamiltonian maximization condition
H(t,Xt*,pt*,θt,ut*) ≥H(t,Xt*,pt*,θt,ut),∀u∈Rd0and∀t∈T.	(9)
Instead of solving Eq. (5) for the optimal control policy π* (Xt), for a given initial condition, the
Pontryagin’s Maximum Principle seeks for a open-loop optimal solution such that the global op-
timum of Eq. (5) is satisfied. The limitation of using the maximum principle is that the control
parameters ut* need to be solved for every unseen data to achieve the optimal solution.
Algorithm Flow. The numerical implementation of CLC-NN is summarized in Alg. 1. Given
a trained network (either from standard or adversarial training) and a set of embedding functions,
the controls are initialized as ut = 0, ∀t ∈ T, because adding random initialization weakens the
5
Published as a conference paper at ICLR 2021
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Algorithm 1: CLC-NN with the Pontryagin’s Maximum Principle.
Input : Possibly perturbed data Xe, a trained neural network, embedding functions
[Ei, ∙∙∙ , ET-ι], maxItr (maximum number of iterations).
Output: A set of optimal control parameters uɔ, ∙∙∙ , UT_「
for k = 0 to maxItr do
Jk = 0,
for t = 0 to T - 1 do
xt+ι,k = f (xt,k, θt, ut,k), where x0,k = xe,	. Forward propagation Eq. (7),
Jk = Jk + L(xt,k, ut,k, Et(xt,k)),	. Objective function Eq. (4),
end for
for t = T to 1 do
pt,k = pt+1 ∙ ^xt f (Xt,k, θt, ut,k) - Vχt L(Xt,k, ut,k , Et(Xt,k)),
where pτ,k = 0,	. Backward propagation Eq. (8)
end for
for t = 0 to T - 1 do
Ut,k+1 = Ut,k + (PTH,k ∙ Vutf(Xt,k, θt, Ut,k) - NutL(Xt,k, Ut,k, Et(xt,k))),
.Maximization of Hamiltonian Eq. (9) based on Eq. (6) and gradient ascent.
end for
end for
robustness performance in general, and clean trajectory often does not result in any running loss for
the gradient update on the control parameters. In every iteration, a given input X0 is propagated
forwardly with Eq. (7) to obtain all the intermediate hidden states Xt for all t and to accumulate cost
J. Eq. (8) backward propagates the co-state Pt and Eq. (9) maximizes the tth Hamiltonian with
current Xt and Pt to compute the optimal control parameters Uz
5	Error Analysis for S implified Linear Cases
For the ease of analysis, we consider a simplified neural network with linear activation functions:
Xt+1 = θt(Xt + Ut),
and reveal why our proposed method can improve robustness in the simplest setting. Given a per-
turbed data sample xe,Q, We denote its perturbation-free counterpart as xq so that Z = xe,Q 一 xq. We
consider a general perturbation where z is the direct sum of two orthogonal contributions: zk, which
is a perturbation within the data manifold (subspace), and z⊥, which is a perturbation in the orthog-
onal complement of the data manifold. This case is general: ifwe consider adversarial attacks, then
the perturbation along the orthogonal complement dominates. In contrast, if we consider random
perturbations, then the two perturbations are on the same scale. Our formulation covers both such
extreme scenarios, together with intermediate cases.
We use an orthogonal projection as the embedding function such that Et = Vtr (Vtr)T, where Vtr
is the first r columns of the eigenvectors computed by the Principle Component Analysis on a
collection of states xt. The proposed CLC-NN minimizes ∣∣xe,t 一 Xt k2 by reducing the components
of xe,t that lie in the the orthogonal complement of Zkt . The following theorem provides an error
estimation between xe,t and Xt.
Theorem 1. For t ≥ 1, we have the error estimation
kxe,t-xtk2 ≤ kθt-1 …θQk2 ∙(α2tkz⊥k2 + kzkk2 +γt∣∣z∣∣2(γtα2(1-αtT)2+2(α-at))), (IO)
where Yt := max(1 + κ(θs)2) ∣∣I — θTθs∣2, and α = ɪ^-, C represents the control regularization.
s≤t	1+c
In particular, the equality
kxe,t -xtk2 = α2tkz⊥k2 + I∣zkk2,	(II)
holds when all θt are orthogonal.
The detailed derivation is presented in Appendix A. Let us summarize the insights from Theorem 1.
6
Published as a conference paper at ICLR 2021
Table 1: Experimental results on ResNet-20 from standard training.
Dataset	€	Accuracy: original model without CLC / CLC-NN + Linear / CLC-NN + Nonlinear				
		None	Type of input perturbations			CW
			Manifold	FGSM	PGD	
CIFAR- 10	2		-^24/79/ 82^^	21 / 56/56	0/50/50	8 / 75 / 79
	4	92/88/ 89	5 / 78 / 81	11/40/30	0/31/19	0 / 75 / 79
	8		1 /78/81	8 / 20 / 12	0/11/2	0 / 76 / 79
CIFAR- 100	2		9/51/52~~	9 / 25 / 23	0 / 17 / 22	4 / 47 / 49
	4	69/60/ 58	3/50/52	5/15/9	0/6/4	1 / 47 / 49
	8		2/50/52	4/9/5	0/1/0	0 / 47 / 49
•	The above error estimation is general for any input perturbation. It shows the working principle
behind the proposed CLC-NN on controlling the perturbation that lies in the orthogonal comple-
ment of input subspace (z⊥).
•	The above error estimation improves as the control regularization c goes to 0 (so α → 0). It is not
the sharpest possible as it relies on a greedily optimal control at each layer. The globally optimal
control defined by the Ricatti equation may achieve a lower loss when c 6= 0.
•	When the dimension of embedding subspace r decreases, our control becomes more effective in
reducing ∣∣xe,t - xtk2. This means that the control approach works the best when the data is
constrained on a low dimensional manifold, which is consistent with the manifold hypothesis. In
particular, observe that as r → 0, ∣zk ∣22 → 0
•	The obtained upper bound is tight: the estimated upper bound becomes the actual error if all the
forward propagation layers are orthogonal matrices.
6	Numerical Experiments
We test our proposed CLC-NN framework under various input data perturbations. Here we briefly
summarize our experimental settings, and we refer readers to Appendix B for the details.
•	Original Networks without Close-Loop Control. We choose residual neural networks (He et al.,
2016) with ReLU activation functions as our target for close-loop control. In order to show that
CLC-NN can improve the robustness in various settings, we consider networks from both standard
and adversarial trainings. We consider multiple adversarial training methods: fast gradient sign
method (FGSM) (Goodfellow et al., 2014), project gradient descent (PGD) (Madry et al., 2017),
and the Label smoothing training (Label Smooth) (Hazan et al., 2017).
•	Input Perturbations. In order to test our CLC-NN framework, we perturb the input data within
a radius of with = 2, 4 and 8 respectively. We consider various perturbations, including non-
adversarial perturbations with the manifold-based attack (Jalal et al., 2017) (Manifold), as well as
some adversarial attacks such as FGSM, PGD and the CW methods (Carlini & Wagner, 2017).
•	CLC-NN Implementations. We consider both linear and nonlinear embedding in our close-
loop control. Specifically, we employ a principal component analysis with a 1% truncation er-
ror for linear embedding, and convolutional auto-encoders for nonlinear embedding. We use
Adam (Kingma & Ba, 2014) to maximize the Hamiltonian function (9) and keep the same hyper-
parameters (learning rate, maximum iterations) for each model against all perturbations.
Result Summary: Table 1 and Table 2 show the results for both CIFAR-10 and CIFAR-100
datasets on some neural networks from both standard training and adversarial training respectively.
•	CLC-NN significantly improves the robustness of neural networks from standard training.
Table 1 shows that the baseline network trained on a clean data set becomes completely vulnerable
(with almost 0% accuracy) under PGD and CW attacks. Our CLC-NN improves its accuracy
to nearly 40% and 80% under PGD and CW attacks respectively. The accuracy under FGSM
attacks has almost been doubled by our CLC-NN method. The accuracy on clean data is slightly
decreased because the lower-dimensional embedding functions cannot exactly capture Zk or M.
•	CLC-NN further improves the robustness of adversarially trained networks. Table 2 shows
that while an adversarially trained network is inherently robust against certain types of pertur-
bations, CLC-NN strengthens its robustness significantly against various perturbations. For in-
7
Published as a conference paper at ICLR 2021
Table 2: Experimental results on robustly trained networks.
			Accuracy: Robustly trained models / CLC-NN + Linear Embedding
Dataset	Training	€	Type of input perturbations
	methods		None	Manifold FGSM	PGD	CW
		2	43/82	90/87	Γ760	14781
	FGSM	4	92/90	17/81	95/90	0/36	2/81
		8	3/80	96/93	0/ 10	0/81
	Label	2	42/78	57/69	10760	29/76
CIFAR-10	V tγi /ʌ zʌ f h Smooth	4	93/89	20/77	51 /61	1 /49	11 /76
		8	6/77	40/47	0/26	2/76
		2	80/78	75/73	74/73	75/75
	PGD	4	83 /80	78/ 76	66 /66	63/ 65	66 / 71
		8	74/73	49/51	40/46	48/64
		2	21755	59/54	1727	9757
	FGSM	4	69/66	8/54	66/55	0/9	2/57
		8	3/53	67/56	0/ 1	0/57
		2	10743	16728	1719	5746
CIFAR-100	Label q TVizAzAfrl Smooth	4	69/62	3/42	12/ 19	0/9	1 /46
		8	2/42	8/11	0/2	0/46
		2	52/50	46/43	45/43	45/47
	PGD	4	58 /52	49/ 48	36 /36	34/ 35	35 / 43
		8		43/45	22/24	16/21	21 /40	
Table 3: Accuracy comparision of CLC-NN and reactive defense in Eq. (12), with attack = 2 / 4 / 8.
Here “+” (or “-”) indicates how much CLC-NN outperforms (or underperforms) reactive defense.
Method	Type of input perturbations				
	None	Manifold	FGSM	PGD	CW
CIFAR-10	--3^^	+47 / +63 / +66	+27/+20/+13	+43/+35/+25	+66/+76/+77
CIFAR-100	+1	+34/+37/+38	+22/0/+9~~	+44/+30/+11	+37/+30/+16
stance, CLC-NN improves the accuracy of an FGSM trained network under PGD and CW attacks
by a maximum of 59% and 81%, respectively.
•	The robustness improvement of adversarially trained networks is less significant. This is
expected because the trajectory of perturbed data lies on the embedding subspace Zk if that data
sample has been used in adversarial training. However, our experiments show that applying CLC-
NN to adversarially trained networks can achieve the best performance under most attacks.
Comparison with PixelDefend (Song et al., 2017). Our method achieves similar performance
on CIFAR-10 with slightly different experimental setting. Specifically, PixelDefend improved the
robustness of a normally trained 62-layer ResNet from 0% to 78% against CW attack. Our proposed
CLC-NN improves the robustness of a 20-layer ResNet from 0% to 81% against CW attacks. Fur-
thermore, we show that CLC-NN is robust against the manifold-based attack. No result was reported
for CIFAR-100 in Song et al. (2017).
Comparison with Reactive Defense Reactive defenses can be understood as only applying a
control at the initial condition of a dynamical system. Specifically, reactive defense equipped with
linear embedding admits the following dynamics:
Xt+1 = f (Xt, θt), s.t. X0 = Vr(Vr)TXe,0∙	(12)
By contrast, CLC-NN controls all hidden states and results in a decreasing error as the number of
layers T increases (c.f. Theorem 1). To quantitatively compare CLC-NN with reactive defense,
we implement them with the same linear embedding functions and against all perturbations. In
Table 3, CLC-NN outperforms reactive defense in almost all cases except that their performances
are case-dependent on clean data.
7	Conclusion
We have proposed a close-loop control formulation to improve the robustness of neural networks.
We have studied the embedding of state trajectory during forward propagation to define the optimal
control objective function. The numerical experiments have shown that our method can improve
8
Published as a conference paper at ICLR 2021
the robustness of a trained neural network against various perturbations. We have provided an error
estimation for the proposed method in the linear case. Our current implementation uses the Pontrya-
gin’s Maximum Principle and an online iterative algorithm to overcome the intractability of solving
a dynamical programming. This online process adds extra inference time. In the future, we plan to
show the theoretical analysis for the nonlinear embedding case.
Acknowledgement Zhuotong Chen and Zheng Zhang are supported by NSF CAREER Award No.
1846476 and NSF CCF No. 1817037. Qianxiao Li is supported by the start-up grant under the NUS
PYP programme.
9
Published as a conference paper at ICLR 2021
References
Richard Bellman. On the theory of dynamic programming. Proceedings of the National Academy
of Sciences of the United States of America, 38(8):716, 1952.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
ieee symposium on security and privacy (sp), pp. 39-57. IEEE, 2017.
E. A proposal on machine learning via dynamical systems. Communications in Mathematics and
Statistics, 5(1):1-11, 2017.
Logan Engstrom, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. A rotation and a
translation suffice: Fooling cnns with simple transformations. arXiv preprint arXiv:1712.02779,
1(2):3, 2017.
Charles Fefferman, Sanjoy Mitter, and Hariharan Narayanan. Testing the manifold hypothesis.
Journal of the American Mathematical Society, 29(4):983-1049, 2016.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Sorin Grigorescu, Bogdan Trasnea, Tiberiu Cocias, and Gigel Macesanu. A survey of deep learning
techniques for autonomous driving. Journal of Field Robotics, 37(3):362-386, 2020.
Eldad Haber and Lars Ruthotto. Stable architectures for deep neural networks. Inverse Problems,
34(1):014004, 2017.
Jiequn Han, Qianxiao Li, et al. A mean-field optimal control formulation of deep learning. arXiv
preprint arXiv:1807.01083, 2018.
Tamir Hazan, George Papandreou, and Daniel Tarlow. Adversarial perturbations of deep neural
networks. 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Alan Julian Izenman. Introduction to manifold learning. Wiley Interdisciplinary Reviews: Compu-
tational Statistics, 4(5):439-446, 2012.
Ajil Jalal, Andrew Ilyas, Constantinos Daskalakis, and Alexandros G Dimakis. The robust manifold
defense: Adversarial training using generative models. arXiv preprint arXiv:1712.09196, 2017.
Marc Khoury and Dylan Hadfield-Menell. On the geometry of adversarial examples. arXiv preprint
arXiv:1811.00525, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Donald E Kirk. Optimal control theory: an introduction. Springer, 1970.
Qianxiao Li, Long Chen, Cheng Tai, and E Weinan. Maximum principle based algorithms for deep
learning. The Journal of Machine Learning Research, 18(1):5998-6026, 2017.
Guan-Horng Liu, Tianrong Chen, and Evangelos A Theodorou. Differential dynamic programming
neural optimizer. arXiv preprint arXiv:2002.08809, 2020a.
Guan-Horng Liu, Tianrong Chen, and Evangelos A Theodorou. A differential game theoretic neural
optimizer for training residual networks. arXiv preprint arXiv:2007.08880, 2020b.
Xuanqing Liu, Minhao Cheng, Huan Zhang, and Cho-Jui Hsieh. Towards robust neural networks via
random self-ensemble. In Proceedings of the European Conference on Computer Vision (ECCV),
pp. 369-385, 2018.
Alexander Selvikvag LUndervold and Arvid LUndervoid. An overview of deep learning in medical
imaging focusing on mri. ZeitschriftfurMedizinische Physik, 29(2):102-127, 2019.
10
Published as a conference paper at ICLR 2021
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Jan Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischoff. On detecting adversarial
perturbations. arXiv preprint arXiv:1702.04267, 2017.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and
accurate method to fool deep neural networks. In Proceedings of the IEEE conference on com-
Puter vision and pattern recognition, pp. 2574-2582, 2016.
Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confi-
dence predictions for unrecognizable images. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 427-436, 2015.
Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks.
arXiv preprint arXiv:1601.06759, 2016.
Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-gan: Protecting classifiers against
adversarial attacks using generative models. arXiv preprint arXiv:1805.06605, 2018.
Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning.
Journal of Big Data, 6(1):60, 2019.
Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman. Pixeldefend:
Leveraging generative models to understand and defend against adversarial examples. arXiv
preprint arXiv:1710.10766, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Nanyang Ye, Qianxiao Li, and Zhanxing Zhu. Amata: An annealing mechanism for adversarial
training acceleration. 2019.
Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong. You only propagate
once: Accelerating adversarial training via maximal principle. In Advances in Neural Information
Processing Systems, pp. 227-238, 2019.
A Appendix A Error Estimation for the Proposed CLC-NN
Preliminaries We define the performance index at time t as
1c
J(Xt, Ut) = 2 kQt(Xt + Ut)k2 + 2 Ilutk2,	(13)
where Qt = I - Vtr (Vtr)T, Vtr is the linear projection matrix at time t with only its first r principle
components corresponding to the largest r eigenvalues. The optimal feedback control is defined as
u； (Xt) = arg min J(xt, Ut),
ut
due to the linear system and quadratic performance index, the optimal feedback control admits an
analytic solution by taking the gradient of performance index (Eq. (13)) and setting it to 0.
VuJ (xt, Ut)
▽uQ kQt(Xt + ut )k2 + 2 kutk2),
QT QtXt + QT QtUt + C ∙ ut,
which leads to the analytic solution of Ut； (Xt) as
u；(Xt) = -(C ∙ I + QTQt)-1QTQtXt.
(14)
The above analytic control solution Ut； optimizes the performance index instantly at time step t, the
error measured by Eq. (13) for the dynamical programming solution Xe,t must be smaller or equal
11
Published as a conference paper at ICLR 2021
to the state trajectory equipped with Ut define by Eq. (14), which gives a guaranteed upper bound
for the error estimation of the dynamic programming solution.
We define the feedback gain matrix Kt = (C ∙ I + QTQt)TQTQt. Thus, the one-step optimal
feedback control can be represented as utt = -Ktxt.
The difference between the controlled system applied with perturbation at initial condition and the
uncontrolled system without perturbation is shown
xe,t+1 - xt+1 = θt(xe,t + Ut - Xt),
=θt(Xe,t - KtXj- Xt).	(15)
The control objective is to minimize the state components that span the orthogonal complement of
the data manifold (I - Vtr (Vtr)T), when the input data to feedback control only stays in the state
manifold, such that k(I - Vtr(Vtr)T)Xtk22 = 0, the feedback control KtXt = 0. The state difference
of Eq. (15) can be further shown by adding a 0 term of (θtKtXt)
xe,t+1 - xt+1 = θt(I - Kt)Xe,t — θ tXt + θtKtXt,
=θt(I - Kt)(Xe,t - Xt).	(16)
In the following, we show a transformation on the control dynamic term (I - Kt) based on its
definition.
Lemma 1. For t ≥ 0, we have
I — Kt = α ∙ I + (1 — α) ∙ Pt,
where Pt := Vr(Vr)T, which is the orthogonal projection onto Zt, and α := #C such that
α ∈ [0, 1].
Proof. Recall that Kt = (c ∙ I + QTQt)-1QTQt, and Qt = I - Vr (Vr )T, Qt can be diagonalized
as following
	-0	0 …0 0	
	0 0 …0 0	
Vt	... ....0 0	VT,
	0 0 …1 0	
	0 0 …0 1	
where the first r diagonal elements have common value of 0 and the last (d - r) diagonal elements
have common value of 1. Furthermore, the feedback gain matrix Kt can be diagonalized as
		0	0 …	0	0	
		0	0 …	0	0	
Kt	Vt	. . .	.. .. ..	0	0	VT,
		0 0	0 … 0 …	1 1+C 0	0 1 1+C.	
where the last (d - r) diagonal elements have common value of *.The control term (I - Kt)
thus can be represented as
	-1	0 …0	0 '	
	0 1 …0	0	
I-Kt=Vt	... ....	0	0	VT
	0	0	…1+C	0	
	_0	0	…0	1+C _	
where the first r diagonal elements have common value of 1 and the last (d - r) diagonal elements
have common value of 卡.By denoting the projection of first r columns as Vr and last (d - r)
columns as Vtr, it can be further shown as
i - Kt = Vr (Vr )T + 占(V r (V r )T),
1+c
Pt + α I - Pt ,
a ∙ I + (1 — α) ∙ Pt.
□
12
Published as a conference paper at ICLR 2021
Oblique Projections. Let P be a linear operator on Rd ,
•	We say that P is an projection if P2 = P.
•	P is an orthogonal projection if P = PT = P2 .
•	If P2 = P but P 6= PT, it is called an oblique projection.
Proposition 2. For a projection P,
1.	If P is an orthogonal projection, then kPk2 = 1.
2.	If P is an oblique projection, then kPk2 > 1.
3.	If P, Q are two projections such that range(P) = range(Q), then PQ = Q and QP = P.
4.	If P is a projection, then rank(P) = T r(P). Furthermore, if P is an orthogonal projection,
then rank(P) = kPk2F = T r(PPT).
Define for t ≥ 0
(Pt0 := Pt,
[p(s+1) := θ--s-ιPSθt-s-ι, S = 0,1,...,t - 1,
Lemma 3. Let Pt3 * s be definedas above for 0 ≤ s ≤ t. Then
1.	Pts is a projection.
2.	Pts is a projection onto Zkt-s, i.e. range(Pts) = Zkt-s.
3.
kPSkF ≤ κ(θt-i θt.2 ... θt-s)2 ∙ r,
where K(A) is the condition number of A, i.e. K(A) = ∣∣A∣∣2 ∙ ∣∣ A-lk2, and r = rank(Z0)=
rank(Zk1) = . . . = rank(Zkt).
Proof.
1.	We prove it by induction on s for each t. For s = 0, Pt0 = Pt, which is a projection by its
definition. Suppose it is true for s such that Pts = PtsPts, then for (s + 1),
(PS + 1)2 = (θ-1s-iPSθt-s-l)2,
=θ-1s-i(PS)2θt-s-1,
= θt--1s-1Ptsθt-s-1,
s+1
= Pt .
2.	We prove it by induction on s for each t. For s = 0, Pt0 = Pt , which is the orthogonal projection
onto Zkt . Suppose that it is true for s such that Pts is a projection onto Zkt-s, then for(s+1),
Pts+1 = θt--1s-1Ptsθt-s-1, which implies
range(Pts+1) = range(θt--1s-1Pts),
= {θt--1s-1x: x ∈ Zkt-s},
t-s-1
= Zk .
3. We use the inequalities ∣AB∣F ≤ ∣A∣2 ∣B∣F, and ∣AB∣F ≤ ∣A∣F ∣B∣2. By the definition
of Pts,
Ps = (θt-ιθt-2 …θt-s)-1P0(θt-ιθt-2 •…θt-s),
we have the following
kPskF ≤k(θt-1θt-2 …θt-s)-1k2 ∙k(θt-ι θt-2 …θt-s)k2 ∙∣P0kF,
≤ κ(θt-1θt-2 …θt-s)2 ∙ r,	Lemma 2(4).
13
Published as a conference paper at ICLR 2021
□
The following Lemma uses the concept of oblique projection to show a recursive relationship to
project any tth state space of Eq. (16) back to the input data space.
Lemma 4. Define for 0 ≤ s ≤ t,
Gs := α ∙ I +(1 - α)Ps.
Then, Eq. (16) can be written as
xe,t - Xt = (θt-1θt-2 …θO)(Gt-1Gt-2 …GO)(Xe,0 - XO), t ≥ 1.
Proof. We prove it by induction on t. For t = 1, by the definition of Gts and transformation from
Lemma 1,
xe,ι - Xi = θo(I - Ko)(Xe,o - Xo),	Eq. (16),
=θo(α ∙ I + (1 - α) ∙ P0)(Xe,0 - x0),	Lemma 1,
=θogO(x6,0 - xO).
Suppose that it is true for (Xe,t - Xt), by using Eq. (16) and Lemma 1, We have
Xe,t+i - Xt+i = θt(I - Kt)(Xe,t - Xt),	Eq. (16),
=θt(α ∙ I - (1 - α) ∙ Pt)(Xe,t - Xt),	Lemma 1,
=θtG0(θt-iθt-2 …θo)(Gt-iGt-2 …G0)(Xe,o - Xo).	(17)
Recall the definitions of P，+1) := θ-1s-ιPsθt-s-ι, and Gs := α ∙ I + (1 - α)Ps, we have the
folloWing
Gs+i = α ∙ I +(1- α) ∙ P(s+1),
=α ∙ i + (1 - α) ∙ θ-1s-ιpsθt-s-ι,
=θ-1s-i(α ∙ I +(1- α) ∙ Ps)θt-s-i,
= θt--is-iGtsθt-s-i,
which results in the equality for the oblique projections. Furthermore,
θt-s-iGt(s+i) = Gtsθt-s-i.
Applying the above to Eq. (17) results in
Xe,t+1 - Xt+1 = θtG0(θt-1θt-2 …θθ)(Gt-IGt-2 …GO)(Xe,0 - XO),
=(仇仇-131(凡-2仇-3 …θO)(Gt-IGt-2 …GO)(Xe,0 - xO),
=(仇仇一1仇一2)02 (θt-3θt-4 …θθ) (Gt-IGt-2 …GO)(Xe,0 - XO),
=(θtθt-1 …θθ)(GtGt-I …GO)(£,0 - xO).
□
Lemma 5. Let
Then,
(t-i) (t-2)	O
F t := Gt-I Gt-2 …gO, t ≥ 1.
t-i
Ft = αt ∙ I + (1 - α) X αsPs.
s=O
Proof. We prove it by induction on t. Recall the definition of Gs := α ∙ I + (1 - α) ∙ Ps. When
t=1,
Fi = GO = α ∙ I + (1 - α) ∙ PO.
Suppose that it is true for t such that
t-i
Ft = G(-;1)Gt-22) …GO = αt ∙ I + (1- α) X αsPs,
s=O
14
Published as a conference paper at ICLR 2021
for (t + 1),
Ft+1 = GttF (t),
=(α ∙ I + (1 - α) ∙ Pt)Ft,
=(α ∙ I + (1 - α) ∙ Pt)(αt ∙ I + (1 - α) X asP：),
s=0
t-1	t-1
=at+1 ∙ I + at(1 - a)Pt + (1 - a)2 X as ∙ PtPs + a(1 - a) X as ∙ Ps.
s=0	s=0
Recall Lemma 3, range(Ptt) = range(Pss ) = Zk0. According to Proposition 2 (3), PttPss = Pss .
Hence,
t-1
Ft+1 = at+1 ∙ I + at(1 - a) ∙ Pt + (1 - a) X as ∙ Ps,
s=0
=at+1∙I +(1 - a) X as ∙ Ps.
s=0
□
Lemma 6. Let V ∈ Rd×r be a matrix whose columns are an orthogonal basis for a subspace D,
and θ ∈ Rd×d be invertible. Let P = VVT be the orthogonal projection onto D. Denote by P the
orthogonal projection onto θD := {θx : x ∈ D}. Then
1.	θ-1Pθ is an oblique projection onto D.
2.	kθ-1Pθ - P∣∣2 ≤ (1 + κ(θ)2) ∙ kI - θTθ∣∣2.
In general, the last inequality shows that θ-1Pθ = P, if θ is orthogonal.
Proof.
1.	(θ-1Pθ)2 = θ-1P2θ = θ-1Pθ, therefore, θ-1Pθ is an projection.
2.	Since P is orthogonal projection onto the row space of θV, then
P = θv[(θV)T (θV)]-1(θV)T,
= θVVT θT θV-1VT θT.
θ-1P θ = V [VT θT θv]-1vτ θT θ.
Furthermore,
∣∣θ-1Pθ - P∣∣2 = ∣∣V[VTθTθV]-1VTθTθ - VVT∣∣2,
≤ kV[VTθTθV]-1VTθTθ-VVTθTθk2+kVVTθTθ-VVTk2,
≤ ∣V([VTθTθV]-1 - I)VTk2 ∙ kθTθ∣2 + kθTθ -1∣2,
≤ k[VTθTθV]-1∣2 ∙ ∣I- VTθTθV∣2 ∙ kθTθ∣2 + kθTθ -1∣2,
≤ k[VTΘTθV]-1∣2 ∙∣I - ΘTθ∣2 ∙∣ΘTθ∣2 + ∣ΘTΘ -1∣2.
15
Published as a conference paper at ICLR 2021
We further bound k[VT θT θV]-1 k2.
k[VT θT θV]-1k2 = (λmin (VT θT θV))-1 ,
=(inf XT VT θτ θVx)-1,
kxk2=1
≤ ( kxinf=1(XO)TθTθx0)T,
=(λmin (θτθ)) 1,
= k(θT θ)-1k2.
Hence, we have
kθ-1 Pθ - Pk2 ≤ (1 + kθTθk2 ∙k(θTθ)-lk2) ∙ kI - θTθk2,
=(1 + κ(θ)2) ∙kI- θτθk2.
□
Corollary 1. Let t ≥ 1. Thenfor each S = 0,1, ∙∙∙ , t, we have
kPS- P0k2 ≤ (1 + κ(θs)2) ∙kI- θτθsk2,
where
•	θ := θs-1 …θo, S ≥ 1,
•	θ := I, s = 0.
Observe that Ps = (θs)-1Psθs. Using Lemma 6, We arrive at the main theorem.
Theorem 1. For t ≥ 1, we have the error estimation
kxe,t -χt k2 ≤ kθt-1 …θ0k2 ∙ (α2tkz⊥k2 + kzkk2 + YtkZk2(Yt a2(1 -αtT)2 +2(a -at))).
where Yt := max(1 + κ(θs)2) ∣∣I 一 θθsk2, and α = ɪ+C, C represents the control regularization.
s≤t	1+c
In particular, the equality
kxe,t -χtk2 = α2tkz⊥k2 + llzkk2.
holds when all θt are orthogonal.
Proof. The input perturbation Z = Xe,0 一 xo can be written as Z = zk + ∙z⊥, where zk ∈ Zk and
z⊥ ∈ Z⊥, Where zk and z⊥ are vectors such that
•	zk ∙ z⊥ = 0 almost surely.
•	zk , z⊥ have uncorrelated components.
•	zk ∈ D, and z⊥ ∈ D⊥.
Since zk and z⊥ are orthogonal almost surely, recall Lemma 4,
Ilxe,t - xtk2 = k(θt-1θt-2 …θO)(Gt-I …GO)Zk2,
≤ kθt-1θt-2 …θok2 ∙k(Gt-1 …G0)zk2,	(18)
For the term ∣∣(Gt-1 …G0)zk2, recall Lemma 5,
t-1
k(Gt-1 …G0)zk2 = k (αt ∙ I +(1 - α) Xɑs ∙ Ps)Zk2,
s=O
= kαtz+(1-α)Xt-1αsPOz+(1-α)Xt-1αs(Pss - PO)zk22,
s=O	s=O
t-1
=kαtz+(1-αt)zk+(1-α)Xαs(Pss-PO)zk22,
s=O
16
Published as a conference paper at ICLR 2021
in the above, P0 is an orthogonal projection on t = 0 (input data space), therefore, P0z = zk .
Furthermore, when s = 0, Pss - P0 = 0. Thus,
k(Gt-1 …G0)zk2
t-1
=α2tkzk22+(1-αt)2kzkk22+(1-α)2 X αsαqzT(Pss-P0)T(Pqq-P0)z
s,q=1
t-1
+ 2αt(1 -αt)kzkk22 +2αt(1 -α)XαszT(Pss -P0)z
s=1
t-1
+ 2(1 - αt)(1 - α) X αs(zk)T (Pss - P0)z,
s=1
=α2t∣∣z⊥k2 + (α2t + 2αt(1 - αt) + (1 - αt)2)∣∣zk k2
t-1	t-1
+(1 -α)2 X αsαqzT (Pss -P0)T(Pqq - P0)z + 2αt(1 -α)XαszT(Pss - P0)z
s,q=1	s=1
t-1
+ 2(1 - αt)(1 - α) X αs(zk)T (Pss - P0)z,
s=1
t-1
= α2tkz⊥k22 + kzkk22 + (1 - α)2 X αsαqzT(Pss-P0)T(Pqq-P0)z
s,q=1
t-1	t-1
+ 2αt(1 - α)XαszT(Pss -P0)z+2(1 - αt)(1 -α)Xαs(zk)T(Pss - P0)z.
s=1	s=1
Using Corollary 1, we have
zT(Ps - Po)z ≤kzk2」PS — P0k2,
≤ γtkzk22.
ZT (Ps — Po)T (pq - Po)z ≤ kzk2 ∙kPs — P0k2 ∙ kpq — P0k2,
≤ γt2 kzk22 .
(zk )T (Ps - Pθ)z ≤ YtkZkk2 ∙kzk2,
≤ γtkzk22.
Thus, we have
k(Gt-1 …G0)zk2 ≤ α2tkz⊥k2 + kzk k2 + α2(1 - αt-1)2γ2kzk2 + 2αt+1(1 - αt-1 )γt∣∣z∣∣2
+ 2α(1 — αt)(1 — αt-1)γt kzk22,
=α2tkz⊥k2 + kzkk2 + YtkZk2 (γt α2(I- ɑtT)2 + 2(α - at)).
Recall the error estimation in Eq. (18),
Ilxe,t - xtk2 ≤ kθt-1θt-2 …θ0k2 ∙ k (Gt-I …GO)Zk 2,
≤ kθt-ι …θok2 ∙ (α2tkz⊥k2 + kzkk2 + YtkZk2(γtα2(1 - αtT)2 + 2(α - αt》).
17
Published as a conference paper at ICLR 2021
Table 4: ResNet for Both CIFAR-10 and CIFAR-100
Structure	Configuration
Initial Layer	Conv2d (input channel = 3, output channel = 16, kernel size = 3 X 3), BatchNorm2d(channel = 16), Relu()
Residual Block 1	{Conv2d (input channel = 16, output channel = 16, kernel size = 3 × 3), BatchNorm2d(channel = 16), Relu(), Shortcut。} ×6
Residual Block 2	Conv2d (input channel = 16, output channel = 32, kernel size = 3 × 3), BatchNorm2d(channel = 16), Relu(), Shortcut(), {Conv2d (input channel = 32, output channel = 32, kernel size = 3 × 3), BatchNorm2d(channel = 32), Relu(), Shortcut()} × 5
Residual Block 3	Conv2d (input channel = 32, output channel = 64, kernel size = 3 × 3), BatchNorm2d(channel = 64), Relu(), Shortcut(), {Conv2d (input channel = 64, output channel = 64, kernel size = 3 × 3), BatchNorm2d(channel = 64), Relu(), Shortcut()} ×5
Final Layer	Fully Connected (64,10)
In the specific case, when all θt are orthogonal,
Yt : = max(1 + κ(θs)2)kI - θTθs∣∣2
s≤t
= 0.
Thus,
kxe,t -χtk2 = α2tkz⊥k2 + ∣∣zkk2.
□
B Appendix B Details of Experimental Setting
B.1	Network Configurations
Since the proposed CLC-NN optimizes the entire state trajectory, it is important to have a relatively
smooth state trajectory, in which case, when the reconstruction loss ∣Et (χt) - χt ∣22 at layer t is
small, the reconstruction losses at its adjacent layers should be small. For this reason, we use
residual neural network (He et al., 2016) as network candidate to retain smoother dynamics. The
configuration of the residual neural network used for both CIFAR-10 and CIFAR-100 is shown in
Tab. 4.
Based on the configuration of residual neural network shown in Tab. 4, we construct 4 embedding
functions applied at input space, outputs of initial layer, residual block 1 and residual block 2. The
output of residual block 3 is embedded with a linear orthogonal projection. We randomly select
5000 clean training data to collect state trajectories at all 5 locations.
•	For the linear orthogonal projections: we apply the Principle Component Analysis on each of the
state collections. We retain the first r columns of the resulted basis, such that r = arg min{i :
λ1+.++λi ≥ 1 - δ}, where δ = 0.1.
18
Published as a conference paper at ICLR 2021
Table 5: Convolutional Auto-Encoders
Structure	Configuration				
Encoder	Conv2d (input channel = ci, output channel = c2, kernel size = 4 X 4, stride = 2 × 2, padding = 1 × 1 ), ELU(alpha=1), BatchNorm2d(channel = c2), Conv2d (input channel = c2, output channel = c3, kernel size = 4 × 4, stride = 2 × 2, padding = 1 × 1 ), ELU(alpha=1).				
Decoder	COnvTranSPose2d (input channel = c3, output channel = c2, kernel size = 4 × 4, stride = 2 × 2, padding = 1 × 1), ELU(alpha=1), ConvTranspose2d (input channel = c2, output channel = c1, kernel size = 4 × 4, stride = 2 × 2, padding = 1 × 1),				
Auto-encoder Index		0	1	2	3
Channel Dimensions [c1, c2, c3]		[3, 18, 36]	[16, 36, 72]	[16, 36, 72]	[32, 128, 256]
•	For the nonlinear embedding: we train 4 convolutional auto-encoders for the input space, outputs
of the initial layer and residual blocks 1, 2. All of the embedding functions are trained individually.
We adopt shallow convolutional auto-encoder structure to gain fast inference speed, in which case,
CLC-NN equipped with linear embedding often outperform the nonlinear embedding as shown
in Tab. 1. The configuration of all 4 convolutional auto-encoders are shown in Tab. 5.
B.2	Perturbations and Defensive Training
In this section, we show details about the perturbations and robust networks that have been consid-
ered in this work. For the adversarial training objective function,
min max E [(1 - λ) ∙ Φi(xe,τ, y, θ) + λ∙ Φi(xτ, y, θ)],
θ∈Θ Xe,0=∆(xo,e)(χ0,y)〜D
where ∆(x0, ) generates a perturbed data from given input x0 within the range of . λ balances
between standard accuracy and robustness. We choose λ = 0.5 in all adversarial training.
For robust networks, we consider both perturbation agnostic and non-agnostic methods. For the
perturbation agnostic adversarial training algorithms equipped ∆(x0, ), the resulted network that is
the most robust against the ∆(x0, ) perturbation. On the contrary, perturbation non-agnostic robust
training methods are often robust against many types of perturbations.
•	Adversarial training with the fast gradient sign method (FGSM) (Goodfellow et al., 2014) consid-
ers perturbed data as follows.
xe,0 = X0 + Signex0Φ(xτ, y)),	(x0,y)〜D,
where sign(∙) outputs the sign of the input. In which case, FGSM considers the worse case
within the range of E along the gradient Vxo Φ(xτ, y) increasing direction. Due to the worse case
consideration, it does not scale well for deep networks, for this reason, we adversarially train the
network with FGSM with E = 4, which is half of the maximum perturbation considered in this
paper.
•	The label smoothing training (Label Smooth) (Hazan et al., 2017) does not utilize any perturbation
information ∆(x0, E). It converts one-hot labels into soft targets by setting the correct class as
1 - e, while other classes have value of n—1, where E is a small constant and N is number of
classes. Specifically, we choose E = 0.9 in this paper.
19
Published as a conference paper at ICLR 2021
Table 6: Experimental results on DenseNet-40 from standard training.
Dataset	€	Accuracy: original without CLC / CLC-NN + Nonlinear Embedding			
		None	Manifolc	Type of input perturbations FGSM	PGD	CW
CIFAR-	2	19/67	27/57	0 / 52	5 / 79
10	4	92/89	4/56	20 / 48	0 / 37	0 / 79
	8	1/59	15/33	0/17	0 / 79
CIFAR-	2	8/25	8724	0 / 25	3 / 47
100	4	70/64	2/24	4/15	0/10	0 / 45
	8		1/24	3/7	0/5	0 / 45
•	Adversarial training with the project gradient descent (PGD) (Madry et al., 2017) generates adver-
sarial data by iteratively run FGSM with small step size, which results in stronger perturbations
compared with FGSM within the same range . We use 7-step of = 2 to generate adversarial
data for robust training.
For Perturbations, we consider the maximum range of = 2, 4, 8 to test the performance the
network robustness against both strong and weak perturbations. For this work, we test network
robustness with the manifold-based attack (Jalal et al., 2017), FGSM (Goodfellow et al., 2014),
20-step of PGD (Madry et al., 2017) and the CW attack (Carlini & Wagner, 2017).
B.3	Online Optimization
Optimization Methods. we use Adam (Kingma & Ba, 2014) to maximize the Hamiltonian Eq. (9)
with default setting. In which case, solving the PMP brings in extra computational cost for inference.
Each online iteration of solving the PMP requires a combination of forward propagation (Eq. (7)),
backward propagation (Eq. (8)) and a maximization w.r.t. the control parameters (Eq. (9)), which
has computational cost approximately the same as performing gradient descent on training a neural
network for one iteration. For the numerical results presented in the paper, we choose the maximum
iteration that gives the best performance from one of [5, 10, 20, 30, 50].
C More Numerical Experiments
The proposed CLC-NN is designed to be compatible with existing open-loop trained. We show extra
experiments by employing the proposed CLC-NN on two baseline models, DenseNet-40 (Table 6).
The layer-wise projection performs orthogonal projection on the hidden state. We define the local
cost function at the tth layer as follows
1c
J (xt, ut) = 2 kQt(xt + ut)k2 + 2 kutk2,
the layer-wise achieves the optimal solution at local time t,
u； (Xt) = arg min J(xt, ut).
ut
However, the layer-wise optimal control solution does not guarantee the optimum across all lay-
ers. In Table 7, we launch comparisons between the proposed CLC-NN with layer-wise projection.
Specifically, under all perturbations the proposed CLC-NN outperforms layer-wise projection.
D Robustness Against Manifold-based Attack
The manifold-based attack (Jalal et al., 2017) (denoted as Manifold) has shown great success on
breaking down the manifold-based defenses (Samangouei et al., 2018). The proposed CLC-NN
can successfully defend such specifically design adversarial attack for manifold-based defenses and
improves the robustness accuracy from 1% to 81% for the standard trained model in Cifar-10, and
2% to 52% in Cifar-100.
20
Published as a conference paper at ICLR 2021
Table 7: Comparison between CLC-NN and layer-wise projection.
Dataset	€	Accuracy: Layer-wise projection / CLC-NN + Linear embedding				
		None	Manifold	Type of input perturbations FGSM	PGD	CW
CIFAR- 10	2		68 / 79	29/56	4 / 50	51 /75
	4	92/88	64 / 78	15/40	0 / 31	50/75
	8		64 / 78	10/20	0/11	49 / 76
CIFAR- 100	2		45 / 51	T3725	2/17	35 / 47
	4	67/60	44 / 50	8/15	0/6	34 / 47
	8		44 / 50	5/9	0/1	34 / 47
We provide detailed explanation for the successful defense of the proposed CLC-NN against such
strong adversarial attack. Exsiting manifold-based defense (Samangouei et al., 2018) focuses on
detecting and de-noising the input components that do not lie within the underlying manifold. The
overpowered attack proposed in Jalal et al. (2017) searches adversarial attack with in the embedded
latent space, which is undetectable for the manifold-based defenses and caused complete failure
defense.
In the real implementation, the manifold-based attack (Jalal et al., 2017) is detectable and control-
lable under the proposed framework due to the following reason. The numerically generated man-
ifold embedding functions are not ideal. The error sources of non-ideal embedding functions are
mainly due to the algorithm that used to compute the manifold, the architecture of embedding func-
tion, and the distribution shift between training and testing data (embedding functions of training
data do not perfectly agree with testing data). In which case, even the perturbation is undetectable
and non-controllable at initial layer, as it is propagated into hidden layers, each layer amplifies such
perturbation, therefore, the perturbation becomes detectable and controllable in hidden layers.
We randomly select the batch of testing data to generate the manifold-based attack following the
same procedure proposed in Jalal et al. (2017). The proposed method improves the attacked accuracy
from 1% to 78%. More specifically, we compare the differences of all hidden states spanning the
orthogonal complement between a perturbed testing data and its unperturbed counterpart, kPt⊥x,t -
P⊥Xe,tk,where P⊥ is a projection onto the orthogonal complement. The difference is growing such
as 0, 0.016, 0.0438, 0.0107, 0.0552 for hidden states at layer 0, 1, 2, 3, 4 respectively. This validates
the argument for how the proposed method is able to detect such perturbation and controls the
perturbation in hidden layers.
Furthermore, we provide some insights about the reasons behind the success of such an adversarial
attack. This follows the same concept of the existence of adversarial attack in neural networks. The
highly nonlinear behaviour of neural networks preserves complex representative ability, meanwhile,
its powerful representation results in its vulnerability. For example, a constant function has 50%
chance to make a correct prediction in binary classification problem under any perturbation, but its
performance is limited. Therefore, we propose to use a linear embedding function that compensates
between the embedding accuracy and robustness.
E	Definition of threat model
Generally, an attacker should not have access to the hidden states during inference, in which case,
an attacker is not allowed to inject extra noise during inference. To define the threat model of
the proposed method, for the white-box setting, an attacker has access to both network and all
embedding functions. The condition that the perturbation e ∙ Z makes our method vulnerable is
defined as follows,
T-1
EkEt(Xe,t) — Xe,tk2 = O, Xe,0 = X0 + e ∙ Z.
t=0
In words, the perturbation e ∙ Z applied on the input data must result in 0 reconstruction losses across
all hidden layers, which means its corresponding state trajectory does not span any of all orthogonal
complements of all hidden state spaces. To obtain an effective attack satisfying the above equation,
conventional gradient-based attackers cannot guarantee to find an perfect attack. A possible way is
21
Published as a conference paper at ICLR 2021
to perform grid-search backward in layers to find such an adversarial attack satisfying the thread
model condition, which is extremely costly.
22