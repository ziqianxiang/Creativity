Published as a conference paper at ICLR 2021
A unifying view on implicit bias in training
LINEAR NEURAL NETWORKS
Chulhee Yun*
MIT
chulheey@mit.edu
Shankar Krishnan
Google Research
skrishnan@google.com
Hossein Mobahi
Google Research
hmobahi@google.com
Ab stract
We study the implicit bias of gradient flow (i.e., gradient descent with infinitesi-
mal step size) on linear neural network training. We propose a tensor formulation
of neural networks that includes fully-connected, diagonal, and convolutional net-
works as special cases, and investigate the linear version of the formulation called
linear tensor networks. With this formulation, we can characterize the conver-
gence direction of the network parameters as singular vectors of a tensor defined
by the network. For L-layer linear tensor networks that are orthogonally decom-
posable, we show that gradient flow on separable classification finds a stationary
point of the '2〃 max-margin problem in a “transformed” input space defined by
the network. For underdetermined regression, we prove that gradient flow finds a
global minimum which minimizes a norm-like function that interpolates between
weighted `1 and `2 norms in the transformed input space. Our theorems subsume
existing results in the literature while removing standard convergence assump-
tions. We also provide experiments that corroborate our analysis.
1	Introduction
Overparametrized neural networks have infinitely many solutions that achieve zero training error,
and such global minima have different generalization performance. Moreover, training a neural
network is a high-dimensional nonconvex problem, which is typically intractable to solve. However,
the success of deep learning indicates that first-order methods such as gradient descent or stochastic
gradient descent (GD/SGD) not only (a) succeed in finding global minima, but also (b) are biased
towards solutions that generalize well, which largely has remained a mystery in the literature.
To explain part (a) of the phenomenon, there is a growing literature studying the convergence of
GD/SGD on overparametrized neural networks (e.g., Du et al. (2018a;b); Allen-Zhu et al. (2018);
Zou et al. (2018); Jacot et al. (2018); Oymak & Soltanolkotabi (2020), and many more). There are
also convergence results that focus on linear networks, without nonlinear activations (Bartlett et al.,
2018; Arora et al., 2019a; Wu et al., 2019; Du & Hu, 2019; Hu et al., 2020). These results typically
focus on the convergence of loss, hence do not address which of the many global minima is reached.
Another line of results tackles part (b), by studying the implicit bias or regularization of gradient-
based methods on neural networks or related problems (Gunasekar et al., 2017; 2018a;b; Arora
et al., 2018; Soudry et al., 2018; Ji & Telgarsky, 2019a; Arora et al., 2019b; Woodworth et al., 2020;
Chizat & Bach, 2020; Gissin et al., 2020). These results have shown interesting progress that even
without explicit regularization terms in the training objective, algorithms such as GD applied on
neural networks have an implicit bias towards certain solutions among the many global minima.
However, in proving such results, many results rely on convergence assumptions such as global
convergence of loss to zero and/or directional convergence of parameters and gradients. Ideally,
such convergence assumptions should be removed because they cannot be tested a priori and there
are known examples where GD does not converge to global minima under certain initializations
(Bartlett et al., 2018; Arora et al., 2019a).
* Based on work performed during internship at Google Research
1
Published as a conference paper at ICLR 2021
Figure 1: Gradient descent trajectories of linear coefficients of linear fully-connected, diagonal, and
convolutional networks on a regression task, initialized with different initial scales α = 0.01, 1.
Networks are initialized at the same coefficients (circles on purple lines), but follow different trajec-
tories due to implicit biases of networks induced from their architecture. The figures show that our
theoretical predictions on limit points (circles on yellow line, the set of global minima) agree with
the solution found by GD. For details of the experimental setup, see Section 6.
1.1	Summary of our contributions
We study the implicit bias of gradient flow (GD with infinitesimal step size) on linear neural net-
works. Following recent progress on this topic, we consider classification and regression problems
that have multiple solutions with zero training error. Our analyses apply to a general class of net-
works, and prove both convergence and implicit bias, providing a more complete characterization
of the algorithm trajectory without relying on convergence assumptions.
•	We propose a general tensor formulation of nonlinear neural networks which includes many
network architectures considered in the literature. In this paper, we focus on the linear version
of this formulation (i.e., no nonlinear activations), called linear tensor networks.
•	For linearly separable classification, we prove that linear tensor network parameters converge
in direction to singular vectors of a tensor defined by the network. As a corollary, we show that
linear fully-connected networks converge to the `2 max-margin solution (Ji & Telgarsky, 2020).
•	For separable classification, we further show that if the linear tensor network is orthogonally
decomposable (Assumption 1), the gradient flow finds the '2/depth max-margin solution in the
singular value space, leading the parameters to converge to the top singular vectors of the tensor
when depth = 2. This theorem subsumes known results on linear convolutional networks and
diagonal networks proved in Gunasekar et al. (2018b), without using convergence assumptions.
•	For underdetermined linear regression, we study the limit points of gradient flow on orthogo-
nally decomposable networks (Assumption 1), and provide a full characterization of the limit
points. This theorem covers results on deep matrix sensing (Arora et al., 2019b) as a special
case, and extends a similar recent result (Woodworth et al., 2020) to a broader class of networks.
•	For underdetermined linear regression with deep linear fully-connected networks, we prove that
the network converges to the minimum `2 norm solutions as we scale the initialization to zero.
•	Lastly, we present simple experiments that corroborate our theoretical analysis. Figure 1 shows
that our predictions of limit points match with solutions found by GD.
2	Problem settings and related works
We first define notation used in the paper. Given a positive integer a, let [a] := {1, . . . , a}. We use
Id to denote the d × d identity matrix. Given a matrix A, we use vec(A) to denote its vectorization,
i.e., the concatenation of all columns of A. For two vectors a and b, let a ® b be their tensor product,
a b be their element-wise product, and ak be the element-wise k-th power ofa. Given an order-
L tensor A ∈ Rk1×…×kL, We USe [Aj,…,九 to denote the (j1,j2,…，jL )-th element of A, where
jι ∈ [kι] for all l ∈ [L]. In element indexing, We use ∙ to denote all indices in the corresponding
dimension, and a : b to denote all indices from a to b. For example, for a matrix A, [A] .46 denotes a
submatrix that consists of 4th-6th columns of A. The square bracket notation for indexing overloads
2
Published as a conference paper at ICLR 2021
with [a] when a ∈ N, but they will be distinguishable from the context. Since element indices start
from 1, We re-define the modulo operation a mod d := a 一 [a-1 Cd ∈ [d] for a > 0. We use ej
to denote the j-th stardard basis vector of the vector space Rk . Lastly, we define the multilinear
multiplication betWeen a tensor and linear maps, Which can be vieWed as a generalization of left-
and right-multiplication on a matrix. Given a tensor A ∈ Rk1 × × kL and linear maps Bl ∈ Rpl × kl
for l ∈ [L], We define the multilinear multiplication ◦ betWeen them as
A ◦ (BT, BT,…,BL ) = X，	, [Ajι,..jL (ej1 乳…乳 j) ◦ (BT,…,BL)
j1 ,...,jL
：=Xj,..jJAjι,..jL (Bιej1 乳…乳 BLejL) ∈ Rp1×…×pL .
2.1	Problem settings
We are given a dataset {(xi, yi)}in=1, Where xi ∈ Rd and yi ∈ R. We let X ∈ Rn×d and y ∈
Rn be the data matrix and the label vector, respectively. We study binary classification and linear
regression in this paper, focusing on the settings Where there exist many global solutions. For binary
classification, We assume yi ∈ {±1} and that the data is separable: there exists a unit vector z and a
constant γ > 0 such that yixiTz ≥ γ for all i ∈ [n]. For regression, We consider the underdetermined
case (n ≤ d) Where there are many parameters z ∈ Rd such that X z = y. Throughout the paper,
We assume that X has full roW rank.
We use f (∙; Θ) : Rd → R to denote a neural network parametrized by Θ. Given the network
and the dataset, we consider minimizing the training loss L(Θ) := Pn=1 '(f (xi； Θ), yi) over Θ.
Following previous results (e.g., Lyu & Li (2020); Ji & Telgarsky (2020)), we use the exponential
loss '(y, y) = exp(-yy) for classification problems. For regression, we use the squared error loss
'(y,y) = 2 (y-y)2. On the algorithm side, we minimize L using gradient flow, which can be viewed
as GD with infinitesimal step size. The gradient flow dynamics is defined as dΘ = -VθL(Θ).
2.2	Related works
Gradient flow/descent in separable classification. For linear models f(x; z) = xTz with sep-
arable data, Soudry et al. (2018) show that the GD run on L drives kz k to ∞, but z converges in
direction to the `2 max-margin classifier. The limit direction of z is aligned with the solution of
minimizez∈Rd kzk subject to yixiTz ≥ 1 for i ∈ [n],	(1)
where the norm in the cost is the `2 norm. Nacson et al. (2019b;c); Gunasekar et al. (2018a); Ji &
Telgarsky (2019b;c) extend these results to other (stochastic) algorithms and non-separable settings.
Gunasekar et al. (2018b) study the same problem on linear neural networks and show that GD
exhibits different implicit bias depending on the architecture. The authors show that the linear
coefficients of the network converges in direction to the solution of (1) with different norms: `2 norm
for linear fully-connected networks, '2∕L (quasi-)norm for diagonal networks, and DFT-domain
'2∕L (quasi-)norm for convolutional networks with full-length filters. Here, L denotes the depth.
We note that Gunasekar et al. (2018b) assume that GD globally minimizes the loss, and the network
parameters and the gradient with respect to the linear coefficients converge in direction. Subsequent
results (Ji & Telgarsky, 2019a; 2020) remove such assumptions for linear fully-connected networks.
A recent line of results (Nacson et al., 2019a; Lyu & Li, 2020; Ji & Telgarsky, 2020) studies general
homogeneous models and show divergence of parameters to infinity, monotone increase of smoothed
margin, directional convergence and alignment of parameters (see Section 4 for details). Lyu & Li
(2020) also characterize the limit direction of parameters as the KKT point of a nonconvex max-
margin problem similar to (1), but this characterization does not provide useful insights for the
functions f (∙; Θ) represented by specific architectures, because the formulation is in the parameter
space Θ. Also, these results require that gradient flow/descent has already reached 100% training
accuracy. Although we study a more restrictive set of networks (i.e., deep linear), we provide a
more complete characterization of the implicit bias for the functions f (∙; Θ), without assuming
100% training accuracy.
Gradient flow/descent in linear regression. It is known that for linear models f(x; z) = xTz,
GD converges to the global minimum that is closest in `2 distance to the initialization (see e.g.,
3
Published as a conference paper at ICLR 2021
Gunasekar et al. (2018a)). However, relatively less is known for deep networks, even for linear
networks. This is partly because the parameters do not diverge to infinity, hence making limit points
highly dependent on the initialization; this dependency renders analysis difficult. A related problem
of matrix sensing aims to minimize Pn=ι(yi -hA%, Wi …Wl〉)2 over Wι,..., WL ∈ Rd×d. It
is shown in Gunasekar et al. (2017); Arora et al. (2019b) that if the sensor matrices Ai commute
and we initialize all Wl ’s to αI, GD finds the minimum nuclear norm solution as α → 0.
Chizat et al. (2019) show that if a network is zero at initialization, and we scale the network output
by a factor of α → ∞, then the GD dynamics enters a “lazy regime” where the network behaves like
a first-order approximation at its initialization, as also seen in results studying kernel approximations
of neural networks and convergence of GD in the corresponding RKHS (e.g., Jacot et al. (2018)).
Woodworth et al. (2020) study linear regression with a diagonal network of the form
f(x; w+, w-) = xT (w+L - w-L), where w+ and w- are identically initialized w+(0) =
W-(O) = αW. The authors show that the global minimum reached by GD minimizes a norm-
like function which interpolates between (weighted) `1 norm (α → 0) and `2 norm (α → ∞). In
our paper, we consider a more general class of orthogonally decomposable networks, and obtain
similar results interpolating between weighted `1 and `2 norms. We also remark that our results
include the results in Arora et al. (2019b) as a special case, and we do not assume convergence to
global minima, as done in Gunasekar et al. (2017); Arora et al. (2019b); Woodworth et al. (2020).
3	Tensor formulation of neural networks
In this section, we present a general tensor formulation of neural networks. Given an input x ∈ Rd,
the network uses a linear map M that maps X to an order-L tensor M(X) ∈ Rk1 × × kL, where L ≥ 2.
Using parameters vl ∈ Rkl and activation φ, the network computes its layers as the following:
Hi(x) = φ(M(x) ◦ (v1,Ik2,..., IkL)) ∈ Rk2×…×kL,
Hl (x) = φ (Hl-I(X) ◦ (vι, Ik1+1,…，IkL)) ∈ Rkl+1 ×...,kL, for l = 2,...,L - 1,	(2)
f(X; Θ) = HL-i(X) ◦vL ∈ R.
We use Θ to denote the collection of all parameters (vi, . . . , vL). We call M(X) the data tensor.
Although this new formulation may look a bit odd in the first glance, it is general enough to capture
many network architectures considered in the literature, including fully-connected networks, diago-
nal networks, and circular convolutional networks. We formally define these architectures below.
Diagonal networks. An L-layer diagonal network is written as
fdiag(X； Θdiag) = φ(…φ(φ(x Θ Wl) Θ W2)•一。WL-i)T WL,
(3)
where Wl ∈ Rd for l ∈ [L]. The representation of fdiag as the tensor form (2) is straightforward. Let
Mdiag(X) ∈ Rd×…×d have [Mdiag(x)]j,j,…,j = [xj, while all the remaining entries of Mdiag(X) are
set to zero. We can set vl = Wl for all l, and M = Mdiag to verify that (2) and (3) are equivalent.
Circular convolutional networks. The tensor formulation (2) includes convolutional networks
fconv (X； Θconv ) = φ(…φ(φ(X ? Wl) ? W2)…? WL-i)T WL,
(4)
where Wl ∈ Rkl with kl ≤ d and kL = d, and? defines the circular convolution: for any a ∈ Rd and
b ∈ Rk (k ≤ d), we have a ? b ∈ Rd defined as [a ? b]i = Pjk=i [a](i+j-i) mod d[b]j, for i ∈ [d].
Define Mconv(X) ∈ R 1× × × LL as [Mconv(X)j1,j2,…,jL = [X](pL=i jι-L+1) mod d for jl ∈ [kl],
l ∈ [L]. Setting vl = Wl and M = Mconv, we can verify that (2) and (4) are identical.
Fully-connected networks. An L-layer fully-connected network is defined as
ffc(X； Θfc) = φ(…φ(φ(XTWI)W2)…Wl-i)wl,
(5)
where Wl ∈ Rdl ×dl+1 for l ∈ [L - 1] (we use di = d) and WL ∈ RdL . One can represent ffc
as the tensor form (2) by defining parameters vl = vec(Wl) for l ∈ [L - 1] and vL = WL, and
constructing the tensor Mfc(X) by a recursive “block diagonal” manner. For example, if L = 2, we
can define Mfc(X) ∈ Rd1d2×d2 to be the Kronecker product of Id2 and X. For deeper networks, we
defer the full description of Mfc (X) to Appendix B.
4
Published as a conference paper at ICLR 2021
Our focus: linear tensor networks. Throughout this section, we have used the activation φ to
motivate our tensor formulation (2) for neural networks with nonlinear activations. For the remaining
of the paper, we study the case whose activation is linear, i.e., φ(t) = t. In this case,
f(x; Θ) = M(X)O (vι, V2,..., Vl).	(6)
We will refer to (6) as linear tensor networks, where “linear” is to indicate that the activation is linear.
Note that as a function of parameters v1, . . . , vL, f(x; Θ) is in fact multilinear. We also remark that
when depth L = 2, the data tensor M(x) is a k1 × k2 matrix and the network formulation boils down
to f(x; Θ) = v1T M(x)v2 .
Since the data tensor M(x) is a linear function of x, the linear tensor network is also a linear function
ofx. Thus, the output of the network can also be written as f(x; Θ) = xT β(Θ), where β(Θ) ∈ Rd
denotes the linear coefficients computed as a function of the network parameters Θ. Since the linear
tensor network f(x; Θ) is linear in x, the expressive power of f is at best a linear model x 7→ xTz.
However, even though the models have the same expressive power, their architectural differences
lead to different implicit biases in training, which is the focus of our investigation in this paper.
Studying separable classification and underdetermined regression is useful for highlighting such
biases because there are infinitely many coefficients that perfectly classify or fit the dataset.
For our linear tensor network, the evolution of the parameters vl via gradient flow reads
Xn0
' (f(xi； Θ),yi)M(xi) ◦ (vi,..., vi-i,Ik, vι+ι,..., VL)
i=1
= M(-XTr) ◦ (V1, . . . , Vl-1, Ikl,Vl+1, . . . , VL), ∀l ∈ [L],
where We initialize vι(0) = αVι, for l ∈ [L]. We refer to α and Vl as the initial scale and initial
direction, respectively. We note that we do not restrict vl's to be unit vectors, in order to allow
different scaling (at initialization) over different layers. The vector r ∈ Rn is the residual vector,
and each component of r is defined as
[r]i= '0(f (Xi； Θ),yi) = {-(-χp)-f (Xi; θ))
for classification,
for regression.
(7)
4 Implicit bias of gradient flow in separable clas sification
In this section, we present our results on the implicit bias of gradient flow in binary classification
with linearly separable data. Recent papers (Lyu & Li, 2020; Ji & Telgarsky, 2020) on this separable
classification setup prove that after 100% training accuracy has been achieved by gradient flow
(along with other technical conditions), the parameters of L-homogeneous models diverge to infinity,
while converging in direction that aligns with the direction of the negative gradient. Mathematically,
lim kΘ(t)k = ∞,
t→∞
li θ⑴
t→m∞kθw
Θ∞,
lim
t→∞
Θ(t)T VθL(Θ(t))
∣∣Θ(t)k∣VθL(Θ(t))∣∣
-1.
Since the linear tensor network satisfies the technical assumptions in the prior works, we apply these
results to our setting and develop a new characterization of the limit directions of the parameters.
Here, we present theorems on separable classification with general linear tensor networks. Corollar-
ies for specific networks are deferred to Appendix A.
4.1	Limit directions of parameters are singular vectors
Consider the singular value decomposition (SVD) of a matrix A = Pj=I Sj (Uj 0 Vj), where m is
the rank of A. Note that the tuples (uj, Vj, sj) are solutions to the system of equations su = AV
and sV = ATu. Lim (2005) generalizes this definition of singular vectors and singular values
to higher-order tensors: given an order-L tensor A ∈ Rk1× ×kL, We define the singular vectors
u1, u2, . . . , uL and singular value s to be the solution of the following system of equations:
sul = A ◦ (u1, . . . ,ul-1, Ikl,ul+1, . . . , uL), for l ∈ [L].	(8)
Using the definition of the singular vectors of tensors, we can characterize the limit direction of
parameters after reaching 100% training accuracy. In Appendix C, we prove the following:
5
Published as a conference paper at ICLR 2021
Theorem 1. Assume that the gradient flow satisfies L(Θ(t0)) < 1 for some t0 ≥ 0 and XTr(t)
converges in direction, say u∞ := limt→∞ ∣∣χχτ芯上∙ Then vι,..., VL converge to the singular
vectors of M(-u∞).
For this theorem, we make some convergence assumptions, because the network is fully general;
this is the only result where we assume convergence. It fact, for the special case of linear fully-
connected networks, the directional convergence assumption is not required, and the linear coef-
ficients βfc (Θfc) converge in direction to the `2 max-margin classifier. We state this corollary in
Appendix A.1; this result also appears in Ji & Telgarsky (2020), but we provide an alternative proof.
4.2	Limit directions for orthogonally decomposable networks
Admittedly, Theorem 1 is not a full characterization of the limit directions, because there are usu-
ally multiple solutions that satisfy (8). For example, in case of L = 2, the data tensor M(-u∞)
is a matrix and the number of possible limit directions (up to scaling) of (v1, v2) is at least the
rank of M(-u∞). Singular vectors of high order tensors are much less understood than the ma-
trix counterparts, and are much harder to deal with. Although their existence is implied from the
variational formulation (Lim, 2005), they are intractable to compute. Testing if a given number is
a singular value, approximating the corresponding singular vectors, and computing the best rank-1
approximation are all NP-hard (Hillar & Lim, 2013); let alone orthogonal decompositions.
Given this intractability, it might be reasonable to make some assumptions on the “structure” of
the data tensor M(x), so that they are easier to handle. The following assumption defines a class
of orthogonally decomposable data tensors, which includes linear diagonal networks and linear
full-length convolutional networks as special cases (for the proof, see Appendix D.2 and D.3).
Assumption 1. For the data tensor M(X) ∈ Rk1×…×kL of a linear tensor network (6), there exist
a full column rank matrix S ∈ Cm×d (d ≤ m ≤ minl kl) and matrices U1 ∈ Ck1 ×m, . . . , UL ∈
CkL ×m such that UlH Ul = Im for all l ∈ [L], and the data tensor M(x) can be written as
M(X) = Xm=1[Sx]j ([U1]∙,j 乳 [U2]∙,j ③…乳 [UL]∙,j ).	⑼
In this assumption, we allow U1, . . . , UL and S to be complex matrices, although M(x) and param-
eters Vl stay real, as defined earlier. For a complex matrix A, We use A* to denote its entry-wise
complex conjugate, AT to denote its transpose (without conjugating), and AH to denote its conju-
gate transpose. In case of L = 2, Assumption 1 requires that the data tensor M(X) (now a matrix)
has singular value decomposition M(X) = U1 diag(SX)U2T; i.e., the left and right singular vectors
are independent of X, and the singular values are linear in X. Using Assumption 1, the following
theorem characterizes the limit directions.
Theorem 2. Suppose a linear tensor network satisfies Assumption 1. If there exists λ > 0 such that
the initial directions Vi,..., VL ofthe network parameters satisfy |[UT Vl j ∣2 一 |[UT Vl] j|2 ≥ λ for
all l ∈ [L - 1] and j ∈ [m], then β(Θ(t)) converges in a direction that aligns with S Tρ∞, where
ρ∞ ∈ Cm denotes a stationary point of the following optimization problem
minimizeρ∈cm	∣∣P∣∣2∕l subject to yiXTSTP ≥ 1, ∀i ∈ [n].
IfS is invertible, then β(Θ(t)) converges in a direction that aligns with a stationary point z∞ of
minimizez∈Rd	kS-Tzk2/L sub ject to yiXiTz ≥ 1, ∀i ∈ [n].
Theorem 2 shows that the gradient flow finds sparse ρ∞ that minimizes the '2〃 norm in the "sin-
gular value space,” where the data points Xi are transformed into vectors SXi consisting of singular
values of M(Xi). Also, the proof of Theorem 2 reveals that in case of L = 2, the parameters Vl (t) in
fact converge to the top singular vectors ofthe data tensor M(-XT r); thus, compared to Theorem 1,
we have a more complete characterization of “which” singular vectors to converge to.
The proof of Theorem 2 is in Appendix D. Since the orthogonal decomposition (Assumption 1) of
M(X) tells us that the singular vectors M(X) in U1, . . . , UL are independent ofX, we can transform
the network parameters Vl to UlT Vl and show that the network behaves similar to a linear diagonal
network. This observation comes in handy in the characterization of limit directions.
6
Published as a conference paper at ICLR 2021
Remark 1 (Necessity of initialization assumptions). In order to remove the assumption that the loss
converges to zero, at least some condition on initialization is necessary, because there are exam-
ples showing non-convergence of gradient flow for certain initializations (Bartlett et al., 2018; Arora
et al., 2019a). In our theorems, We Pose assumptions on initial directions vι that are sufficient Con-
ditions for the loss L(Θ(t)) to converge to zero. Although such sufficient conditions are “stronger”
than assuming L(Θ(t)) → 0, they are useful because they can be easily checked a priori, i.e., before
running gradient floW. We note an important fact that in Theorems 2 and onWards, the conditions
on initialization are used solely to prove convergence of the loss to zero, and our statements on
the implicit bias hold whenever the loss converges to zero, even for initializations that do not
satisfy our conditions. In addition, We argue that our assumptions are not too restrictive; λ can be
arbitrarily small, so the conditions are satisfied with probability 1 if we set VL = 0 and randomly
sample other vi,s. Setting one layer to zero to prove convergence is also studied in Wu et al. (2019).
Lastly, the condition that VL is “small” can be replaced with any layer; e.g., convergence still holds
if |[UTVι]j∣2 - IUTVι]j I2 ≥ λ for all l = 2,...,L and j ∈ [m].
Remark 2 (Comparison to existing results). Theorem 2 leads to corollaries (stated in Appendix A.2)
on linear diagonal and full-length convolutional networks, showing that diagonal (or convolutional)
networks converge to the stationary point of the max-margin problem with respect to the '2〃 norm
(or DFT-domain '2〃 norm). Theorem 2 recovers the results in Gunasekar et al. (2018b) without
relying on assumptions such as directional convergence of parameters and gradients.
Remark 3 (Implications to architecture design). Theorem 2 shows that the gradient flow finds a
solution that is sparse in a “transformed” input space where all data points are transformed with S.
This implies something interesting about architecture design: if the sparsity of the solution under a
certain linear transformation T is needed, one can design a network using Assumption 1 by setting
S = T . Training such a network will give us a solution that has the desired sparsity property.
Other than Assumption 1, there is another setting where we can prove a full characterization of limit
directions: when there is one data point (n = 1) and the network is 2-layer (L = 2). This “extremely
overparametrized” case is motivated by an experimental paper (Zhang et al., 2019) which studies
generalization performance of different architectures when there is only one training data point.
Theorem 3. Suppose we have a 2-layer linear tensor network (6) and a single data point (x, y).
Consider the compact SVD M(x) = U1 diag(s)U2T, where U1 ∈ Rk1 ×m, U2 ∈ Rk2×m, and
s ∈ Rm for m ≤ min{k1, k2}. Let ρ∞ ∈ Rm be a solution of the following optimization problem
minimizeρ∈Rm	kρk1 subject to ysTρ ≥ 1.
Assume that there exists λ > 0 such that the initial directions vι, rv2 of the network parameters
satisfy [Ut Vι]2 — [UT V2j ≥ λ for all j ∈ [m]. Then, vι and v2 converge in direction to Uιη∞
and U2η∞, where ∣η∞∣ = ∣η∞∣ = ∣ρ∞∣θ1/2, andsign(η∞) = sign(y) Θ sign(η∞).
The proof of Theorem 3 can be found in Appendix E. Since ρ∞ is the minimum '1 norm solution in
the singular value space, the parameters V1 and V2 converge in direction to the top singular vectors.
We would like to emphasize that this theorem can be applied to any network architecture that can
be represented as a linear tensor network. Recall that the previous result (Gunasekar et al., 2018b)
only considers full-length filters (k1 = d), hence providing limited insights on networks with small
filters, e.g., k1 = 2. In light of this, we present a corollary in Appendix A.3 showing that linear
coefficients of convolutional networks converge in direction to a “filtered” version of x.
5	Implicit bias of gradient flow in underdetermined regression
In Section 4, the limit directions of parameters we characterized do not depend on initialization.
This is due to the fact that the parameters diverge to infinity in separable classification problems, so
that the initialization becomes unimportant in the limit. This is not the case in regression setting,
because parameters do not diverge to infinity. As we show in this section, the limit points are closely
tied to initialization, and our analyses characterize the dependency between them.
5.1	Limit point characterization for orthogonally decomposable networks
For the orthogonally decomposable networks satisfying Assumption 1 with real S and Ul ’s, we
consider how limit points of gradient flow change according to initialization. We consider a specific
7
Published as a conference paper at ICLR 2021
initialization scheme that, in the special case of diagonal networks, corresponds to setting wl (0) =
aw for l ∈ [L 一 1] and wl(0) = 0. We use the following lemma on a relevant system of ODEs:
Lemma 4. Consider the system of ODEs, where p, q : R → R:
P = pL-2q,	q= pL-1, p(0) = 1,	q(0) = 0.
Then, the solutions pL (t) and qL (t) are continuous on their maximal interval of existence of the
form (—c,c) ⊂ R for some C ∈ (0, ∞]. Define hL(t) = PL(t)L-1qL(t); then, hL(t) is odd and
strictly increasing, satisfying limt↑c hL(t) = ∞ and limtψ-c hL(t) = 一∞.
Using the function hL(t) from Lemma 4, we can obtain the following theorem that characterizes the
limit points as the minimizer of a norm-like function QL,α,η among the global minima.
Theorem 5. Suppose a linear tensor network satisfies Assumption 1. Assume further that the matri-
ces U1 , . . . , UL and S from Assumption 1 are all real matrices. For some λ > 0, choose any vector
η ∈ Rm satisfying 忻 j ≥ λ for all j ∈ [m], and choose initial directions Vi = Uιη for l ∈ [L — 1]
and VL = 0. Then, the linear coefficients β(Θ(t)) converge to STρ∞, where ρ∞ is the solution of
minimizeρ∈Rm QL,a,η(ρ) := α2 Xm=1[η]2HL (αL∣pj∣l )	subject to XSTP = y,
where QL,α,n : Rm → R is a norm-like function defined using HLo := Rt h-1(τ )dτ. If S is
invertible, then β(Θ(t)) converges to the solution z∞ of
minimizez∈Rd	QL,α,n(S-TZ) subject to Xz = y.
The proofs of Lemma 4 and Theorem 5 are deferred to Appendix F.
Remark 4 (Interpolation between 'ι and '2). It can be checked that HL(t) grows like the absolute
value function if t is large, and grows like a quadratic function if t is close to zero. This means that
li	、、m	I [ρj I	li	、、m	[ρ]2
αi→0 QLa爪P) H U=I WF-2,	α→→∞ QLa爪P) H U=I WL-2,
so QL,α,n interpolates between the weighted '1 and weighted '2 norms of ρ. Also, the weights
in the norm are dependent on the initialization direction n unless L = 2 and a → 0. In general,
QL,a,n interpolates the standard '1 and '2 norms only if ∣[ηj | is the same for all j ∈ [m]. This
result is similar to the observations made in Woodworth et al. (2020) which considers a diagonal
network with a “differential” structure f(x; w+, w-) = xT (w+L 一 w-L). In contrast, our results
apply to a more general class of networks, without the need to have the differential structure. In
Appendix A.4, we state corollaries of Theorem 5 for linear diagonal networks and linear full-length
convolutional networks with even data points. There, we also show that deep matrix sensing with
commutative sensor matrices (Arora et al., 2019b) is a special case of our setting.
Next, we present the regression counterpart of Theorem 3, for 2-layer linear tensor networks with
a single data point. For this extremely overparametrized setup, we can fully characterize the limit
points as functions of initialization vι(0) = αV1 and V2(0) = αV2, for any linear tensor networks
including linear convolutional networks with filter size smaller than input dimension.
Theorem 6. Suppose we have a 2-layer linear tensor network (6) and a single data point (x, y).
Consider the compact SVD M(x) = U1 diag(s)U2T, where U1 ∈ Rk1 ×m, U2 ∈ Rk2×m, and
S ∈ Rm for m ≤ min{kι, k2}. Assume that there exists λ > 0 such that the initial directions Vι, V2
of the network parameters satisfy [UT VIj — [UT V)2]2 ≥ λ for all j ∈ [m]. Then, gradient flow
converges to a global minimizer of the loss L, and V1 (t) and V2(t) converge to the limit points:
v∞ = αU1 (UTVi Θ cosh (g-1 (奈)
+ UTV2 Θ Sinh (g-i (奈)s))+α(Ik] — UiUT)Vi,
v∞ = αU2 (UTVi Θ Sinh 0-1(—),)
+ UT V2 Θ CoSh
(g 1(α2) s))+α(Ik2- U2UT)v2,
where g-i is the inverse of the following strictly increasing function
g(ν ) = Xm=1[sj( UT vlj+uτ v2]2 sinh(2[sjν) + UT Vij [U2T V2j cosh(2[sj V)).
The proof can be found in Appendix G. We can observe that as α → 0, we have g-1 (&) → ∞,
which results in exponentially faster growth of the sinh(∙) and cosh(∙) for the top singular values.
As a result, the top singular vectors dominate the limit points Vi∞ and V2∞ as α → 0, and they do
not depend on the initial directions Vi, V2. Experiment results in Section 6 support this observation.
8
Published as a conference paper at ICLR 2021
5.2 IMPLICIT BIAS IN FULLY-CONNECTED NETWORKS: THE α → 0 LIMIT
We state our last theoretical element of this paper, which proves that the linear coefficients βfc(Θfc)
of deep linear fully-connected networks converge to the minimum `2 norm solution as α → 0. We
assume for simplicity that di = d2=…=dL = d in this section, but We can extend it for di ≥ d
without too much difficulty. Recall f/x; Θfc) = XT Wi ∙ ∙ ∙ Wl-iWl. We minimize the training
loss L with initialization Wl (0) = αW∣ for l ∈ [L 一 1] and wl(0) = αWL.
Theorem 7. Assume that initial directions Wi,..., Wl-i, WL satisfy (1) WlT Wι 占 W∣+ι Wl+i
for l ∈ [L-2], and (2) there exists λ > 0 such that WT-IWL-ι-WLWT 占 λId. Then, the gradient
flow converges to a global minimum, and limα→0 limt→∞ βfc (Θfc (t)) = XT(XXT)-iy.
The proof is presented in Appendix H. Theorem 7 shows that in the limit α → 0, linear fully-
connected networks have bias towards the minimum `2 norm solution, regardless of the depth.
This is consistent with the results shown for classification. We also note that the convergence to
a global minimum holds for any a > 0, and our sufficient conditions (WT Wι 占 Wi+i W+ and
WL-IWL-i 一 WLWT 占 λId) for global convergence is a generalization of the zero-asymmetric
initialization scheme (Wi = … = WL-1 = Id and WL = 0) proposed in WU et al. (2019).
6 Experiments
Regression. To fully visualize the trajectory of linear coefficients, we run simple experiments
with 2-layer linear fully-connected/diagonal/convolutional networks with a single 2-dimensional
data point (x, y) = ([1 2], 1). For this dataset, the minimum `2 norm solution (corresponding to
fully-connected networks) of the regression problem is [0.2 0.4], whereas the minimum `i norm
solution (corresponding to diagonal) is [0 0.5] and the minimum DFT-domain `i norm solution
(corresponding to convolutional) is [0.33 0.33]. We randomly pick four directions Zi,... Z4 ∈ R2,
and choose initial directions of the network parameters in a way that their linear coefficients at
initialization are exactly β(Θ(0)) = α2Zj. With varying initial scales α ∈ {0.01,0.5,1}, we run
GD with small step size η = 10-3 for large enough number of iterations T = 5 × 103. Figures 1 and
2 plot the trajectories of β(Θ) (appropriately clipped for visual clarity) as well as the predicted limit
points (Theorem 6). We observe that even though the networks start at the same linear coefficients
α2Zj, they evolve differently due to different architectures. Note that the prediction of limit points
is accurate, and the solution found by GD is less dependent on initial directions when α is small.
Classification. It is shown in the existing works as well as in Section 4 that the limit directions
of linear coefficients are independent of the initialization. Is this also true in practice? To see this,
we run a set of toy experiments on classification with two data points (xi, yi) = ([1 2], +1) and
(x2, y2) = ([0 一 3], 一1). One can check that the max-margin classifiers for this problem are in
the same directions to the corresponding min-norm solutions in the regression problem above. We
use the same networks as in regression, and the same set of initial directions
a2Zj. With initial scales α ∈ {0.01,0.5,1}, we run GD with step size η
satisfying β(Θ(0)) =
= 5 × 10-4 for T =
2 × 106 iterations. All experiments reached L(Θ) . 10-5 at the end. The trajectories are plotted
in Figure 2 in the Appendix. We find that, in contrast to our theoretical characterization, the actual
coefficients are quite dependent on initialization, because we do not train the network all the way
to zero loss. This observation is also consistent with a recent analysis (Moroshko et al., 2020) for
diagonal networks, and suggests that understanding the behavior of iterates after a finite number of
steps is an important future work.
7 Conclusion
This paper studies the implicit bias of gradient flow on training linear tensor networks. Under a
general tensor formulation of linear networks, we provide theorems characterizing how the network
architectures and initializations affect the limit directions/points of gradient flow. Our work provides
a unified framework that connects multiple existing results on implicit bias of gradient flow as special
cases.
9
Published as a conference paper at ICLR 2021
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. arXiv preprint arXiv:1811.03962, 2018.
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. In International Conference on Machine Learning, pp.
244-253, 2018.
Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient de-
scent for deep linear neural networks. In International Conference on Learning Representations,
2019a.
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. In Advances in Neural Information Processing Systems, pp. 7413-7424, 2019b.
Peter Bartlett, Dave Helmbold, and Philip Long. Gradient descent with identity initialization effi-
ciently learns positive definite linear transformations by deep residual networks. In International
Conference on Machine Learning, pp. 521-530, 2018.
Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks
trained with the logistic loss. arXiv preprint arXiv:2002.04486, 2020.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
In Advances in Neural Information Processing Systems, pp. 2937-2947, 2019.
Simon S Du and Wei Hu. Width provably matters in optimization for deep linear neural networks.
arXiv preprint arXiv:1901.08572, 2019.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018a.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018b.
Daniel Gissin, Shai Shalev-Shwartz, and Amit Daniely. The implicit bias of depth: How incremental
learning drives generalization. In International Conference on Learning Representations, 2020.
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro.
Implicit regularization in matrix factorization. In Advances in Neural Information Processing
Systems, pp. 6151-6159, 2017.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in
terms of optimization geometry. In International Conference on Machine Learning, pp. 1832-
1841, 2018a.
Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent
on linear convolutional networks. In Advances in Neural Information Processing Systems, pp.
9461-9471, 2018b.
Christopher J Hillar and Lek-Heng Lim. Most tensor problems are NP-hard. Journal of the ACM
(JACM), 60(6):1-39, 2013.
Wei Hu, Lechao Xiao, and Jeffrey Pennington. Provable benefit of orthogonal initialization in opti-
mizing deep linear networks. In International Conference on Learning Representations, 2020.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing systems, pp. 8571-
8580, 2018.
Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. In
International Conference on Learning Representations, 2019a.
Ziwei Ji and Matus Telgarsky. The implicit bias of gradient descent on nonseparable data. In
Conference on Learning Theory, pp. 1772-1798, 2019b.
10
Published as a conference paper at ICLR 2021
Ziwei Ji and Matus Telgarsky. A refined primal-dual analysis of the implicit bias. arXiv preprint
arXiv:1906.04540, 2019c.
Ziwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning. arXiv
preprint arXiv:2006.06657, 2020.
Lek-Heng Lim. Singular values and eigenvalues of tensors: a variational approach. In 1st IEEE
International Workshop on Computational Advances in Multi-Sensor Adaptive Processing, 2005.,
pp. 129-132. IEEE, 2005.
Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks.
In International Conference on Learning Representations, 2020.
Edward Moroshko, Suriya Gunasekar, Blake Woodworth, Jason D Lee, Nathan Srebro, and Daniel
Soudry. Implicit bias in deep linear classification: Initialization scale vs training accuracy. arXiv
preprint arXiv:2007.06738, 2020.
Mor Shpigel Nacson, Suriya Gunasekar, Jason Lee, Nathan Srebro, and Daniel Soudry. Lexico-
graphic and depth-sensitive margins in homogeneous and non-homogeneous deep models. In
International Conference on Machine Learning, pp. 4683-4692, 2019a.
Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique Pamplona Savarese, Nathan
Srebro, and Daniel Soudry. Convergence of gradient descent on separable data. In The 22nd
International Conference on Artificial Intelligence and Statistics, pp. 3420-3428. PMLR, 2019b.
Mor Shpigel Nacson, Nathan Srebro, and Daniel Soudry. Stochastic gradient descent on separable
data: Exact convergence with a fixed learning rate. In The 22nd International Conference on
Artificial Intelligence and Statistics, pp. 3051-3059. PMLR, 2019c.
Samet Oymak and Mahdi Soltanolkotabi. Towards moderate overparameterization: global con-
vergence guarantees for training shallow neural networks. IEEE Journal on Selected Areas in
Information Theory, 2020.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The im-
plicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19
(1):2822-2878, 2018.
Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan,
Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In
Conference On Learning Theory, 2020.
Lei Wu, Qingcan Wang, and Chao Ma. Global convergence of gradient descent for deep linear
residual networks. In Advances in Neural Information Processing Systems, pp. 13389-13398,
2019.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Michael C Mozer, and Yoram Singer. Identity
crisis: Memorization and generalization under extreme overparameterization. arXiv preprint
arXiv:1902.04698, 2019.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep ReLU networks. arXiv preprint arXiv:1811.08888, 2018.
11
Published as a conference paper at ICLR 2021
Figure 2: Gradient descent trajectories of linear coefficients of linear fully-connected, diagonal, and
convolutional networks on a regression task with initial scale α = 0.5 (top left), and networks on
a classification task with initial scales α = 0.01, 0.5, 1 (rest). Networks are initialized at the same
coefficients (circles on purple lines), but follow different trajectories due to different implicit biases
of networks induced from their architecture. The top left figure shows that our theoretical predictions
on limit points (circles on yellow line, the set of global minima) agree with the solution found by
GD. For details of the experimental setup, please refer to Section 6.
A Corollaries on specific network architectures
We present corollaries obtained by specializing the theorems in the main text to specific network
architectures. We briefly review the linear neural network architectures studied in this section.
Linear fully-connected networks. An L-layer linear fully-connected network is defined as
ffc(x; Θfc) = XTWi …WL-IwL,	(10)
where Wl ∈ Rdl×dl+1 forl ∈ [L - 1] (we use d1 = d) and wL ∈ RdL.
Linear diagonal networks. An L-layer linear diagonal network is written as
fdiag(X； Θdiag) =(X Θ wi。…。wL-i)T wL,	(11)
where wl ∈ Rd for l ∈ [L].
Linear (circular) convolutional networks. An L-layer linear convolutional network is written as
fconv(x; Θconv) = (• 一 ((X ? wi) ? w2)• 一 ? wL-i)T wL,	(12)
where wl ∈ Rkl with kl ≤ d and kL = d, and ? defines the circular convolution: for any a ∈ Rd
and b ∈ Rk (k ≤ d), we have a?b ∈ Rd defined as [a?b]i = Pjk=i[a](i+j-i) mod d[b]j, for i ∈ [d].
In case of kl = d for all l ∈ [L], we refer to this network as full-length convolutional networks.
Deep matrix sensing. The deep matrix sensing problem considered in Gunasekar et al. (2017);
Arora et al. (2019b) aims to minimize the following problem
minimize	Lms(Wi …WL) := Xi (y, -〈A,, Wi …WL〉)2,	(13)
W1,...,WL∈Rd×d	i=i
where the sensor matrices Ai, . . . , An ∈ Rd×d are symmetric. Following Gunasekar et al. (2017);
Arora et al. (2019b), we consider sensor matrices Ai, . . . , An ∈ Rd×d that commute. To make the
problem underdetermined, we assume that n ≤ d, and Ai ’s are linearly independent.
12
Published as a conference paper at ICLR 2021
A.1 Corollary of Theorem 1
Corollary 1. Consider an L-layer linear fully-connected network (10). If the training loss satisfies
L(Θfc(t0)) < 1 for some t0 ≥ 0, then βfc (Θfc(t)) converges in a direction that aligns with the
solution of the following optimization problem
minimizez∈Rd	kzk22	subject to	yixiTz ≥ 1, ∀i ∈ [n].
Corollary 1 shows that whenever the network separates the data correctly, the linear coefficients
βfc (Θfc) convergence in direction to the `2 max-margin classifier. Note that this corollary does
not require the directional convergence of XTr, which is different from Theorem 1. In fact, this
corollary also appears in Ji & Telgarsky (2020), but we provide an alternative proof based on our
tensor formulation. The proof of Corollary 1 can be found in Appendix C.
A.2 Corollaries of Theorem 2
Corollary 2. Consider an L-layer linear diagonal network (11). If there exists λ > 0 such that the
initial directions W1,..., WL ofthe network parameters satisfy [w(j — [w l]2 ≥ λ for all l ∈ [L 一 1]
andj ∈ [d], then βdiag (Θdiag (t)) converges in a direction that aligns with a stationary point z∞ of
minimizez∈Rd	kzk2/L	subject to yixiTz ≥ 1, ∀i ∈ [n].
For full-length convolutional networks, we define F ∈ Cd×d to be the matrix of discrete Fourier
transform basis [F j,k = % exp(— √-1∙2π j-1)、-1)). Note that F * = F-1, and both F and F *
are symmetric, but not Hermitian.
Corollary 3. Consider an L-layer linear full-length convolutional network (12). If there exists
λ > 0 such that the initial directions Wi,..., W L ofthe network parameters satisfy ∣[FWιj∣2 —
|[FW Lj ∣2 ≥ λ for all l ∈ [L — 1] and j ∈ [d], then β∞nv(Θc°nv(t)) converges in a direction that
aligns with a stationary point z∞ of
minimizez∈Rd	kF zk2/L	subject to yixiTz ≥ 1, ∀i ∈ [n].
Corollary 2 shows that in the limit, linear diagonal network finds a sparse solution z that is a sta-
tionary point of the '2〃 max-margin classification problem. Corollary 3 has a similar conclusion
except that the standard '2/L norm is replaced with DFT-domain '2/L norm. By specifying mild
conditions on initialization (see Remark 1), these corollaries remove the convergence assumptions
required in Gunasekar et al. (2018b). The proofs of Corollaries 2 and 3 are in Appendix D.
A.3 Corollary of Theorem 3
Recall that Theorem 3 can be applied to any 2-layer networks that can be represented as linear
tensor networks. Examples include the convolutional networks that are not full-length (i.e., filter
size k1 < d), which are not covered by the previous result (Gunasekar et al., 2018b). Here, we
present the characterization of convergence directions of βconv(Θconv(t)) for small-filter cases:
k1 = 1 and k1 = 2.
Corollary 4. Consider a 2-layer linear convolutional network (12) with k1 = 1 and a single data
point (x,y). Ifthere exists λ > 0 such that the initial directions Wi and W2 ofthe network parame-
ters satisfy ||x『V2 — (XTV2)2 ≥ ||x『λ, then βconv(Θgnv(t)) converges in direction that aligns
with yx.
Consider a 2-layer linear convolutional network (12) with k1 = 2 and a single data point (x, y).
Let t— := [[x]2	…	[x]d [x]i], and → :二 [[x]d [x]i …	[x]d-i]. Ifthere exists λ> 0
such that the initial directions Wi and W2 ofthe network parameters satisfy
([Vi]1 + [Vi]2)2 — (⅛+¾⅛ ≥λ, and ([Vi]1 — [Vi]2)2 — ¾-⅛¾l ≥λ,
then βconv (Θconv (t)) converges in a direction that aligns with a filtered version ofx:
lim	βconv(Θconv(t))	H [凝工 + Vt— + V→	if XT毋 >。，
t→∞ kβconv(Θ
Conv(t))∣∣2	(2yχ — V— — y→	if χτt < 0.
13
Published as a conference paper at ICLR 2021
Corollary 4 shows that if the filter size is k1 = 1, then the limit direction ofβconv(Θconv) is always
the `2 max-margin classifier. Note that this is quite different from the case k1 = d which converges
to the DFT-domain `1 max-margin classifier. However, for 1 < k1 < d, it is difficult to characterize
the limit direction as the max-margin classifier of some common norms. Rather, the limit directions
of βconv (Θconv) correspond to a “filtered” version of the data point, and the weights of the filter
depend on the data point x. For kι = 2, the filter is a low-pass filter if the autocorrelation XTt- of
x is positive, and high-pass if the autocorrelation is negative. For k1 > 2, the filter weights are more
complicated to characterize in terms of x, and the filter length increases as k1 increases. We prove
Corollary 4 in Appendix E.
A.4 Corollaries of Theorem 5
In this subsection, we apply Theorem 5 to linear diagonal networks, linear full-length convolutional
networks with even data, and deep matrix sensing. The proofs of the corollaries can be found in
Appendix F.
Corollary 5. Consider an L-layer linear diagonal network (11). For some λ > 0, choose any vector
W ∈ Rd satisfying [Wj ≥ λ for all j ∈ [d], and choose initial directions Wl = W for l ∈ [L — 1]
and WL = 0. Then, the linear coefficients βdiag(Θdiag(t)) converge to the solution z∞ of
minimizez∈Rd	Ql0,w (z) ：= α2 XJ=1[W]2HL (αL⅛⅜7F)	subject to Xz = y.
Recall that the original statement of Assumption 1 allows the matrices S, U1, . . . , UL to be com-
plex, but Theorem 5 poses another assumption that these matrices are real. In applying Theo-
rem 2 to convolutional networks to get Corollary 3, we used the fact that the data tensor Mconv (x)
of a linear full-length convolutional network satisfies Assumption 1 with S = d--^ F and
Ui =…=UL 二
[Fj,k = √d exp(-
F* , where F ∈ Cd×d is the matrix of discrete Fourier transform basis
√-Γ∙2πj-1)(k-1)
d
)and F * is the complex conjugate of F. Note that these
are complex matrices, so one cannot directly apply Theorem 5 to convolutional networks. However,
it turns out that if the data and initialization are even, we can derive a corollary for convolutional
networks.
We say that a vector is even when it satisfies the even symmetry, as in even functions. More con-
cretely, a vector X ∈ Rd is even if [x j+2 = [x]d-j for j = 0,..., [d-3C; i.e., the vector has the
even symmetry around its “origin” [x]1. From the definition of the matrix F ∈ Cd×d, it is straight-
forward to check that if x is real and even, then its DFT Fx is also real and even (see Appendix F.4
for details).
Corollary 6. Consider an L-layer linear full-length convolutional network (12). Assume that the
data points {xi}n=ι are all even. For some λ > 0, choose any even vector W satisfying [Fwj ≥ λ
for all j ∈ [d], and choose initial directions Wι = W for l ∈ [L — 1] and WL = 0. Then, the linear
coefficients βconv(Θconv(t)) converge to the solution z∞ of
minimize	Ql°,fw(Fz) ：= α2 X^ JFWjHL (αL[F⅛j∣l )	subject to Xz = y.
Corollaries 5 and 6 show that the interpolation between minimum weighted `1 and weighted `2 so-
lutions occurs for diagonal networks, and also for convolutional networks (in DFT domain, with the
restriction of even symmetry). The conclusion of Corollary 5 is similar to the results in Woodworth
et al. (2020), but the network architecture (11) considered in our corollary is a slightly different from
the “differential” network f(x; W+, W-) = xT (W+L — W-L) in Woodworth et al. (2020).
As mentioned in the main text, we can actually show that the matrix sensing result in Arora et al.
(2019b) is a special case of our Theorem 5. Given any symmetric matrix M ∈ Rd×d, let eig(M) ∈
Rd be the d-dimensional vector of eigenvalues of M .
Corollary 7. Consider the depth-L deep matrix sensing problem (13). Let Ai ’s be symmetric, and
assume A1, . . . , An commute. For α > 0, choose initialization Wl (0) = αId for l ∈ [L — 1] and
Wl(0) = 0. Then, the product Wl(∕) •…Wi(t) converge to the solution M∞ of
minimize	QL,α(eig(M))：=	α2	X	HL	([eig(M)j)	subject to	Lms(M)	=	0.
M ∈Rd×d , symmetric	j =1	α
14
Published as a conference paper at ICLR 2021
Under an additional assumption that Ai’s are positive semidefinite, Theorem 2 in Arora et al.
(2019b) studies the initialization Wl (0) = αId for all l ∈ [L], and shows that the limit point of
WL . . . W1 converges to the minimum nuclear norm solution as α → 0. We remove the assumption
of positive definiteness of Ai ’s and let WL (0) = 0, to show a complete characterization of the so-
lution found by gradient flow, which interpolates between the minimum nuclear norm (i.e., Schatten
1-norm) solution (when α → 0) and the minimum Frobenius norm (i.e., Schatten 2-norm) solution
(when α → ∞).
B Tensor representation of fully-connected networks
In Section 3, we only defined the data tensor Mfc (x) of fully-connected networks for L = 2. Here,
we describe an iterative procedure constructing the data tensor for deep fully-connected networks.
We start with T1(x) := x ∈ Rd1. Next, define a block diagonal matrix T2(x) ∈ Rd1d2×d2 where
the “diagonals” [T2(x)]d1(j-1)+1:d1j,j = T1 (x) for j ∈ [d2], while all the other entries are filled
with 0. We continue this “block diagonal” procedure, as the following. Having defined Tl-1 (x) ∈
RdId2 ×∙∙∙× dl - 2dl -1 × dl-1
1.	DefineTl(X) ∈ Rd1d2×…×dι-ιdι×dl.
2.	Set [Tl (X)L,…,∙,di-ι(j-1)+Ldi-ιj,j = Tl-I(X),∀j ∈ [dl].
3.	Set all the remaining entries of Tl (x) to zero.
We repeat this process for l = 2, . . . , L, and set Mfc(X) := TL(X). By defining the parameters
of the tensor formulation vl = vec(Wl) for l ∈ [L - 1] and vL = wL, and using the tensor
M(X) = Mfc(X), we can check the equivalence of (2) and (5).
C Proofs of Theorem 1 and Corollary 1
C.1 Proof of Theorem 1
The proof of Theorem 1 is outlined as follows. First, using the directional convergence and align-
ment results in Ji & Telgarsky (2020), we prove that each of our network parameters vl converges
in direction, and it aligns with its corresponding negative gradient -Nvl L. Then, We prove that the
directions of vl's are actually singular vectors of M(-u∞), where u∞ := limt→∞ 口备舄丁?.
Since a linear tensor network is an L-homogeneous polynomial of v1, . . . , vL, it satisfies the as-
sumptions required for Theorems 3.1 and 4.1 in Ji & Telgarsky (2020). These theorems imply that
if the gradient flow satisfies L(Θ(t0)) < 1 for some t0 ≥ 0, then Θ(t) converges in direction, and
the direction aligns with -NΘL(Θ(t)); that is,
lim ∣∣Θ(t)k2 = ∞, lim Ilf(I)I = Θ∞,	lim
t→∞	t→∞ ∣Θ(t)∣2	, t→∞
Θ(t)TNΘL(Θ(t))
kΘ(t)k2kNΘL(Θ(t))k2
-1.	(14)
For linear tensor networks (6), the parameter Θ is the concatenation of all parameter vectors
v1, . . . , vL, so (14) holds for Θ = v1T . . . vLTT.
Now, recall that by the definition of the linear tensor network, we have the following gradient flow
V l = M(-X T r) ◦ (vι,..., vl-ι, Ikl, vl+1,..., vl).
Note that we can apply this to calculate the rate of growth of ∣vl ∣22 :
dI∣vlk2 = 2vTVl = 2vTM(-XTr) ◦ (vι,…，vl-1, Ikl, vl+1,…，VL)
= 2M(-XTr) ◦ (v1, .. .,vl-1,vl,vl+1,. ..,vL)
=dIlvl0k2	forany l0 ∈ [L],
15
Published as a conference paper at ICLR 2021
so the rate at which kvl k22 grows over time is the same for all layers l ∈ [L]. By the definition of Θ
and (14), we have
L
kΘk22 = X kvlk22 → ∞,
l=1
which then implies
lim kvl(t)k2 → ∞,
t→∞
lim
t→∞
kΘ≡2
kv1(t)k2
∕kθ(t)k2
V kvι(t)k2
√L,
for all l ∈ [L]. Now, let Il be the set of indices that correspond to the components of vl in Θ. It
follows from (14) that
lim
t→∞
Vl⑴
kvi(t)k2
lim	kΘ≡2 = lim ≡k kΘ≡2 = √L[Θ∞]I ,
t→∞ kθ⑴k2 kvl (t)k2 t→∞ ∣∣θ(t)∣2 kvl (t)k2	”
thus showing the directional convergence of vl ’s.
Next, it follows from directional convergence of Θ and its alignment with -VθL(Θ) (14) that
VθL(Θ) also converges in direction, in the opposite direction of Θ. By comparing the components
in Il’s, we get that Vvl L(Θ) converges in the opposite direction ofvl.
For any l ∈ [L], now let v∞ := limt→∞ 葭品历.Also recall the assumption that XTr(t) converges
in direction; let the unit vector u∞ := limt→∞ 1^;t;)= be the limit direction. By the gradient
flow dynamics of vl , we have
v∞ a-V 仞 L(Θ∞) = M(-u∞) ◦ (v∞,..., v∞ι,Ikl, g,∙∙∙, v∞),
for all l ∈ [L]. Note that this equation has the same form as (8), the definition of singular vectors in
tensors. So this proves that (v1∞, . . . , vL∞) are singular vectors of M(-u∞).
C.2 Proof of Corollary 1
The proof proceeds as follows. First, we will show using the structure of the data tensor Mfc that the
limit direction of linear coefficients βfc(Θf∞c ) is proportional to cu∞, where c is a nonzero scalar
and u∞ is the limit direction of XTr . Then, through a closer look at u∞ and c, we will prove
that βfc(Θf∞c ) is in fact a conic combination of the support vectors (i.e., the data points with the
minimum margins). Finally, we will compare βfc (Θf∞c ) with the KKT conditions of the `2 max-
margin classification problem and conclude that βfc(Θf∞c ) must be in the same direction as the `2
max-margin classifier.
Due to the way how the data tensor Mfc is constructed for fully-connected networks (Appendix B),
we always have
-Vv1L(Θfc)
Mfc(-XTr) ◦ (Ik1 , v2 , . . . , vL) ∈ span
From Theorem 1, we have directional convergence ofv1 and its alignment with -Vv1L(Θfc). This
means that the limit direction v1∞, which is a fixed vector, must be also in the span of vectors written
above. This implies that XTr must also converge to some direction, say u∞ := limt→∞ |最；)；)1 .
Now recall the definition of v1 in case of the fully-connected network: v1 = vec(W1). So, by
reshaping v1∞ into its original d1 × d2 matrix form W1∞, we have
W∞ H u∞qτ,
for some q ∈ Rd2 . This implies that the linear coefficients βfc(Θfc) of the network converge in
direction to
βfc(Θf∞c) = W1∞ W2∞ . . . WL∞-1wL∞ H u∞qTW2∞ . . . WL∞-1wL∞ = cu∞,	(15)
16
Published as a conference paper at ICLR 2021
where c is some nonzero real number.
Let us now take a closer look at the vector u∞, the limit direction ofXTr. Recall from Section 2.1
that for any i ∈ [n],
[r]i = -yi exp(-yiffc(xi; Θfc)) = -yi exp(-yixiTβfc(Θfc)),
in case of classification. Recall that kβfc(Θfc(t))k2 → ∞ while converging to a certain direction
βfc(Θf∞c ). This means that if
yjxjTβfc(Θf∞c) > yixiTβfc(Θf∞c )
for any i, j ∈ [n], then
limexp(-yj XT βfc(Θfc(2=0.
t→∞ exp(-yixiTβfc (Θfc (t)))
(16)
Take i to be the index of any support vector, i.e., any i that attains the minimum yixiTβfc(Θf∞c )
among all data points. Using such an i, the observation (16) implies that limt→∞ [r(t)]j = 0 for any
xj that is not a support vector. Thus, by the argument above, u∞ can in fact be written as
u∞ = lim Pn=1 x*Ri = - XX ViyiXi,
t→∞ k∑n=ι g[r(t)]ik2	⅛
(17)
where νi ≥ 0 for all i ∈ [n], and νj = 0 for xj’s that are not support vectors. Combining (17) and
(15),
n
βfc(Θ∞) H -CEViyiXi.
i=1
(18)
Recall that we do not yet know whether c, introduced in (15), is positive or negative; we will now
show that c has to be negative. From Lyu & Li (2020), we know that L(Θfc (t)) → 0, which implies
that yixiTβfc(Θf∞c ) > 0 for all i ∈ [n]. However, if c > 0, then (18) implies that βfc(Θf∞c ) is inside
a cone K defined as
K:
γiyixi | γi ≤ 0, ∀i ∈ [n]	.
Note that the polar cone of K, denoted as K°, is
K° := {z | βτZ ≤ 0,∀β ∈ K} = {z | yiXTZ ≥ 0, ∀i ∈ [n]}.
It is known that K ∩ K° = {0} for any convex cone K and its polar cone K°. Therefore, having
c > 0 implies that βfc(Θ∞) ∈ K \ K°, which means that there exists some i ∈ [n] such that
yixiTβfc (Θf∞c ) < 0; this contradicts the fact that the loss goes to zero as t → ∞. Therefore, c in
(15) and (18) must be negative:
n
βfc (Θf∞c ) H	νiyixi,
i=1
for νi ≥ 0 for all i ∈ [n] and νj = 0 for all xj’s that are not suport vectors.
Finally, compare (19) with the KKT conditions of the following optimization problem:
minimize	kZ k22 sub ject to yixiTZ ≥ 1, ∀i ∈ [n].
The KKT conditions of this problem are
n
Z = Eμi"iXi, and μi ≥ 0, μi(1 - yiXTZ) = 0 for all i ∈ [n],
i=1
(19)
where μι,...,μn are the dual variables. Note that this is (UP to scaling) satisfied by βfc(Θ∞) (19),
if we replace μ∕s with %'s. This finishes the proof that βfc(Θ∞) is aligned with the '2 max-margin
classifier.
17
Published as a conference paper at ICLR 2021
D Proofs of Theorem 2 and Corollaries 2 & 3
D.1 Proof of Theorem 2
D.1.1 Convergence of loss to zero
Since Theorem 2 does not assume the existence of t0 ≥ 0 satisfying L(Θ(t0)) < 1, we need to first
show that given the conditions on initialization, the training loss L(Θ(t)) converges to zero. Recall
from Section 2.1 that
V ι = -Vvi L(Θ) = M(-X T r) ◦ (vι,..., Vl-1,1断,v1+1,..., vl).
Applying the structure (9) in Assumption 1, we get
Vl = M(-XTr) ◦ (vι,..., vl-ι,Ikι, vl+ι,..., VL)
m
=-X[SX T r]j (vT [U1]∙,j 乳…% vl-i[ul-1]∙,j 乳 [Ul]∙,j 乳 vl+l[Ul + 1]∙,j③…③vT [UL]∙,j)
j=1
=-X [sχ T M Yut Vk ]j) [ul ]∙,j.
j=1	k6=l
Left-multiplying UlH (the conjugate transpose of Ul) to both sides, we get
UHVI = -SXTr Θ Y；=l UTVk,	(20)
where Q denotes the product using entry-wise multiplication .
Now consider the rate of growth for the absolute value squared of the j-th component of UlT vl :
d I [UlTvl] j |2 = d [UlT Vl] j [UlT vl]* = d [UlTvl ]j[UlHvl]j
dt	dt	dt
=UlTv l ]j [UH vl]j + [UH vl]j[UlTVlj
= 2Re([UlH Vl ]j[UlTvl]j)
= 2Re -[SXTr]j YkL=1[UkTvk]j
=d | UlTvlo]j |2	for any l0 ∈ [L],
so for any j ∈ [m], the squared absolute value of the j-th components in UlT vl grow at the same
rate for each layer l ∈ [L]. This means that the gap between any two different layers stays constant
for all t ≥ 0. Combining this with our conditions on initial directions, we have
|[UlTvl(t)]j|2 - |[ULTvL(t)]j|2 = |[UlTvl(0)]j|2 - |[ULTvL(0)]j|2	21
=α2∣[UlTVl]j∣2 - α2∣[UΤVl]j∣2 ≥ ɑ2λ,	()
for any j ∈ [m], l ∈ [L - 1], and t ≥ 0. This inequality also implies
I[UlTvl(t)]jI2 ≥ |[UTVL(t)]jI2 + α2λ ≥ α2λ.	(22)
Let us now consider the time derivative of L(Θ(t)). We have the following chain of upper bounds
on the time derivative:
d L(Θ(t)) = Vθ L(Θ(t))T Θ(t) = -∣∣VθL(Θ(t))k2
≤-kVvL L(θ(t))k2 = -kvL(t)k2
≤) -kUHVL(t)k2 =) - ∣∣SXTr(t) Θ Y；=LUTVk(t)∣∣2
= -Xjm=1 I[SXT r(t)]jI2 Yk6=L I[UkT vk(t)]jI2
18
Published as a conference paper at ICLR 2021
(≤c) -α2L-2λL-1 Xjm=1 |[SXT r(t)]j|2
= -α2L-2λL-1kSX T r(t)k22
(d)
≤ -α2L-2λL-1smin(S)2kXT r(t)k22,
(23)
where(a)usedthefaCtthatkv L(t)k2 ≥ IlUL UH V L(t)k2 because it is a projection onto a subspace,
and IlULUHVL(t)k2 = IlUHVL(t)k2 because UHUL = IkL； (b) is due to (20); (C) is due to (22);
and (d) used the fact that S ∈ Cm×d is a matrix that has full column rank, so for any z ∈ Cd , we
can use ISzI2 ≥ smin(S)IzI2 where smin(S) is the minimum singular value of S.
We now prove a lower bound on the quantity IXT r(t)I22. Recall from Section 2.1 the definition of
[r(t)]i = -yi exp(-yif(xi; Θ(t))) for classification problems. Also, recall the assumption that the
dataset is linearly separable, which means that there exists a unit vector z ∈ Rd such that
yixiT z ≥ γ > 0
holds for all i ∈ [n], for some γ > 0. Using these,
kX Tr⑴k2 = kχn 1 Vixi exp(-yf(Xi ； θ(I)))k2
i=1
≥ [zT Xin=1 yixi exp(-yif(xi; Θ(t)))]2
≥ γ2 [Xn exp(-yif(xi; Θ(t)))]2 = γ2L(Θ(t))2.
i=1
Combining this with (23), we get
ddtL(Θ(t)) ≤ -α2L-2λLTsmin(S)2γ2L(Θ(t))2,
which implies
L(Θ(t)) ≤
L(Θ(0))
1 + α2L-2λLTsmm(S)2γ2t∙
Therefore, L(Θ(t)) → 0 as t → ∞.
D.1.2 Characterizing the limit direction
Since we proved that L(Θ(t)) → 0, the argument in the proof of Theorem 1 applies to this case,
and shows that the parameters Vl converge in direction and align with Vι = -Vvi L(Θ). Let v∞ :=
limt→∞ kvv(t)k2 be the limit direction of Vl.
The remaining steps of the proof are as follows. We first prove that SXT r(t) converges in direc-
tion u∞ . Using this u∞ , we derive a number of conditions that has to be satisfied by the limit
directions of the parameters. Finally, we compare these conditions with the KKT conditions of the
minimization problem, and finish the proof.
By Assumption 1, we have
mL
f(x; Θ) = M(x) ◦ (V1, . . . ,VL) = X[Sx]j Y[UlTVl]j
j=1	l=1
JX (Y [UlTVl ]j)[S Hx = XT ST (∏[ UlTVl) = XT ST ρ.
Here, we defined ρ := Ql∈[L] UlT Vl ∈ Cm. Since the linear coefficients must be real, we have
STρ ∈ Rd for any real Vl ’s. Since Vl ’s converge in direction, ρ also converges in direction, to
ρ∞ := Ql∈[L] UlTVl∞. So we can express the limit direction ofβ(Θ) as
β(Θ∞) U ST(∏[L] UlTV∞) = STρ∞.	(24)
19
Published as a conference paper at ICLR 2021
From (20) and alignment of Vl and Vι, We have
lim UHvl(t) = lim (UlTvl(t))* H 一 lim SXTr(t) Θ Yθ UTVk(t).
t→∞	t→∞	t→∞	k6=l
(25)
Since all vectors UlT Vl (t) converge in direction, the term SXT r(t) should also converge in direc-
tion. Let u∞ := limt→∞ ∣∣SSXTNtt))g.One can use the same argument as in Appendix C.2, more
specifically (16) and (17), to shoW that u∞ can be Written as
u∞
l∙	S Pn=ι Xi [r(t)]i	_ _S X
t-∞ kS Pn=1 Xi[r(t)]ik2 =之 IViyxi
(26)
Where νi ≥ 0 for all i ∈ [n], and νj = 0 for xj’s that are not support vectors, i.e., those satisfying
yj xjTSTρ∞ > mini∈[n] yixiTSTρ∞ .
Using u∞, We can reWrite (25) as
UHv∞ H -u∞ Θ Y：=l UTV∞,
for all l ∈ [L]. Element-Wise multiplying UlTVl∞ to both sides gives
UlTv∞ θ UHV∞ = |UlTv∞∣θ2 H -u∞ Θ Y[L] UTv∞ = -u∞ Θ ρ∞,	(27)
Where aθb denotes element-Wise b-th poWer of the vector a. Since the LHS of (27) is a positive real
number, We have
arg(|[UlTVl ]j|2) = 0 = arg([-u∞]j) + arg([ρ∞j),	(28)
so using this, (27) becomes
∣UlTv∞产 h∣u∞∣Θ∣P∞∣.	(29)
NoW element-Wise multiply (29) for all l ∈ [L], then We get
∣P∞∣θ2 h∣u∞∣θl Θ∣p∞∣θl.	(30)
A close look at (30) reveals that ifL ≥ 2, ρ∞ and u∞ must satisfy that
∣[ρ∞]jl=0 =⇒ ∣[u∞]j∣H∣[ρ∞]j∣⅛-1,	(31)
for all j ∈ [m]. There is another condition that has to be satisfied When L = 2:
∣[ρ∞]jI = 0, ∣[ρ∞]j01 = 0 =⇒ ∣[u∞]jI ≤ ∣[u∞]j01,	(32)
for any j, j0 ∈ [m]; let us prove Why. First, consider the time derivative of [ρ]j = [U1TV1]j [U2TV2]j.
d [ρ(t)]j = [UT Vl(t)]j d [UT V2(t)]j + [UT V2(t)]j d [UT Vl(t)]j
dt	dt	dt
=-[SX T r(t)]；(I[UT vι(t)]j I2 + |[UT V2(t)]j I2),	(33)
Where (a) used (20). NoW consider
Idt -	= I [SX T - IUT vι(t)j I2 + |[UT v2(t)]j∣2
kSX T r(t)k2I[ρ(t)]j I = kSX T r(t)k2	I[ρ(t)j I	.	( )
We Want to compare this quantity for different j, j0 ∈ [m]. Before We do that, We take a look at the
last term in the RHS of (34). Recall from (21) that
I[U1TV1(t)]jI2= I[U2TV2(t)]jI2+I[U1TV1(0)]jI2-I[U2TV2(0)]jI2.	(35)
For simplicity, let δj := I[U1TV1(0)]j I2 - I[U2TV2(0)]j I2, Which is a positive number due to our
assumption on initialization. Then, We can use (35) and I[ρ(t)]j I = I[U1TV1(t)]j II[U2TV2(t)]j I to
shoW that
I[UTv1(t)]jI2 + I[UTV2(t)jI2 =	2I[UTvUt)jI2 + δj	≥ 2
I[P(t)]j i	IUT v2(t)]j PI[UT V2(t)j I2 + δj - i
20
Published as a conference paper at ICLR 2021
—vi(j + FUT3 = 2 if IUTV(t)j∣ = ∞.
t→∞	I[ρ(t)jl	t→∞
Recall that we want to prove that (32) should necessarily hold. For the sake of contradiction, suppose
that there exists j ∈ [m] that satisfies I[ρ∞]jI = 0 but I[u∞]jI > I[u∞]j0 I, for some j0 ∈ [m]
satisfying I[ρ∞]j0 I 6= 0. Note that having I[ρ∞]jI = 0 and I[ρ∞]j0 I 6= 0 implies that I[ρ(t)]j0 I → ∞
and |[p((；)j| → 0. We now want to compare the ratio of (34) for j and j0. First, note that
|[SXTr(jksχTr(t)∣∣2 = ∣[u∞]j∣ > 1
t→∞ I[SXTr(t)]j0∣∕kSXTr(t)∣∣2 = I[u∞]j0∣	.
(36)
Next “sin。Umt)j1
Next, USing ∣[ρ(t)jo |
δ > 0, we have
→ 0 and the fact that X → 方^2塔 is a decreasing function of X ≥ 0 for any
(|[UT vι(t)]j∣2 + |[UT V2(t)]j ∣2)∕∣[ρ(t)]j∣	≥ 1
(I[UTvι(t)]j0∣2 +1[UTV2(t)]j0I2))∕∣[ρ(t)]j01 ≥ ,
(37)
for any t ≥ t0, when t0 is large enough. Combining (36) and (37) to compare the ratio of (34) for j
and j0, we get that there exists some t00 ≥ 0 such that for any t ≥ t00, we have
隆[ρ(t)j∣ /I[ρ(t)j∣ > I
隆[ρ(t)]j0∣∕∣[ρ(t)]j0∣	.
(38)
This implies that the ratio of the absolute value of time derivative of [ρ(t)]j to the absolute value
of current value of [ρ(t)]j is strictly bigger than that of [ρ(t)]j0. Moreover, we saw in (33) that the
phase of £ [ρ(t)j converges to that of -[u∞j. Since this holds for all t ≥ t0, (38) results in a
growth of I[ρ(t)]j I that is exponentially faster than that of I[ρ(t)]j0 I, so [ρ(t)]j becomes a dominant
component in ρ(t) as t → ∞. This contradicts that [ρ∞]j = 0, hence the condition (32) has to be
satisfied.
So far, we have characterized a number of conditions (26), (28), (31), (32) that have to be satisfied
by the limit directions u∞ and ρ∞ of XTr and ρ. We now consider the following optimization
problem and prove that these conditions are in fact the KKT conditions of the optimization problem.
Consider
minimize ∣∣P∣∣2∕l subject to yiXTSTP ≥ 1, ∀i ∈ [n].	(39)
The KKT conditions of this problem are
n
∂ ∣∣ρk2∕L 3 S * E〃iy，Xi, and 〃，≥ 0, μi(1 - yiXT ST P) = 0 for all i ∈ [n],
i=1
where μι,...,μn are the dual variables. The symbol ∂ ∣∣∙∣∣2∕l denotes the (local) subdifferential of
the '2∕L norm1, which can be written as
∂ I∣ρ∣1 = {u ∈ Cm I |[u]j| ≤ 1 for all j ∈ [m], and [ρj = 0 =⇒ [u j = exp(√-Γarg([ρj))},
if L = 2 (in this case ∂ ∣P∣1 is the global subdifferential), and
d l∣P∣2∕L = {u ∈ CmHPj= 0 =⇒ [uj = L2|[P]jiL2TeXp(√-Iarg([ρj”,
if L > 2. By replacing μ∕swith ν∕sdefinedin(26), we can check from (26), (28), (31), (32) that the
that P∞ and u∞ satisfy the KKT conditions up to scaling. Therefore, by (24), β(Θ(t)) converges in
direction aligned with STP∞, where P∞ is again aligned with a stationary point (global minimum
in case of L = 2) of the optimization problem (39).
If S is invertible, we can get S-Tβ(Θ∞) α ρ∞. Plugging this into the optimization problem (39)
gives the last statement of the theorem.
1the definition of subdifferentials used here is taken from Gunasekar et al. (2018b).
21
Published as a conference paper at ICLR 2021
D.2 Proof of Corollary 2
It suffices to prove that linear diagonal networks satisfy Assumption 1, with S = Id . The proof
is very straightforward, since Mdiag (x)∈ Rd×…×d has [Mdiag(x)]j,j,...,j = [x]j while all the
remaining entries are zero. It is straightforward to verify that Mdiag (x) satisfies Assumption 1 with
S = Ui = •一 = UL = Id. A direct substitution into Theorem 2 gives the corollary.
D.3 Proof of Corollary 3
For full-length convolutional networks (ki = … =kL = d), we will prove that they satisfy
Assumption 1 with S = d --^ F and Ui = … =UL = F *, where F ∈ Cd×d is the matrix
of discrete Fourier transform basis [F j,k = √d exp(- √-τ∙2π(j-1)(kτ)) and F * is the complex
conjugate of F.
For simplicity of notation, define ψ = exp(- V-1∙2π). With such matrices S and Ui,..., UL, we
can write M (x) as
d
M(X) = X[Sx]j ([U1]∙,j ③ [U2]∙,j ③…③[UL]∙,j)
j=i
dd
X dl-2 X[x]kψ(jT)(i)
j =i	k=i
0L
where a0L denotes the L-times tensor product of a. We will show that M(X) = Mconv(x).
For any ji , . . . , jL ∈ [d],
d
d
[M(X)]j1,...,jL
IS	a
d X X[χ]kψ
l=i
k=i
(l-i)(k-i)
ψ-(l-i)(PqL=1 jq -L)
dd
d X[x]k X ψ(lτ)(kτ-p3 jq+L)
Recall that
Xd ψ(l-i)(k-i-PqL=1jq+L) = (0d
l=i
Using this, we have
if k - 1 - PqL=i jq + L is a multiple of d,
otherwise.
d
d
[M(X)]j1,...,jL
l=i
1 jq +L)
k=i
[X]PqL=
1 jq-L+i mod d = [Mconv (X)]j1,...,jL.
Hence, linear full-length convolutional networks satisfy Assumption 1 with S = dL-1 F. A direct
substitution into Theorem 2 and then using the fact that |[Fz]j | = |[F* z]j | for any real vector
z ∈ Rd gives the corollary.
E Proofs of Theorem 3 and Corollary 4
E.1 Proof of Theorem 3
E.1.1 Convergence of loss to zero
Since Theorem 3 does not assume the existence oft0 ≥ 0 satisfying L(Θ(t0)) < 1, we need to first
show that given the conditions on initialization, the training loss L(Θ(t)) converges to zero. Since
22
Published as a conference paper at ICLR 2021
L = 2 and M(x) = U1 diag(s)U2T, we can write the gradient flow dynamics from Section 2.1 as
V1 = -M(XTr) ◦ (Iki, V2)= -rUι diag(s)UTv2,
V2 = -M(XTr) ◦ (vι, Ik?) = -rU2 diag(s)UTvι,
(40)
where r(t) = -y exp(-yf (x; Θ(t))) is the residual of the data point (x, y). From (40) we get
UlTV1 = —rs Θ UTV2, UTV2 = —rs Θ UTvι.	，
(41)
Now consider the rate of growth for the j -th component of U1T v1 squared:
ddt [UT Vι]2 = 2[UT Vι]j [PT V ιj = -2r[sj[UfVιj[UT V2 j = -dt [U2T V2j.	(42)
dt	dt
So for any j ∈ [m], [U1TV1]j2 and [U2TV2]j2 grow at the same rate. This means that the gap between
the two layers stays constant for all t ≥ 0. Combining this with our conditions on initial directions,
[U1T V1(t)]j2 - [U2T V2(t)]j2 = [U1T V1(0)]j2 - [U2T V2(0)]j2	43
=α2[UTVi]2-α2[UTV2]2 ≥ ɑ2λ,	( )
for any j ∈ [m] and t ≥ 0. This inequality implies
[U1TV1(t)]j2 ≥ [U2T V2(t)]j2 + α2λ ≥ α2λ.	(44)
Let us now consider the time derivative of L(Θ(t)). We have the following chain of upper bounds
on the time derivative:
d L(Θ(t)) = VθL(Θ(t))T Θ(t) = -kVθL(Θ(t))k2
≤-kVv2 L(θ(t))k2 = -kV 2(t)k2
≤) -kUT V 2(t)k2 = -r(t)2 ns θ UTVI ⑴ ∣∣2
= -r(t)2 Xjm=1[s]j2[U1TV1(t)]j2
(≤c) -α2λr(t)2 Xjm=1[s]j2
= -α2λksk22L(Θ(t))2,
where (a) used the fact that 心2(t)k2 ≥ ∣∣U2U^TV2(t)k2 because it is a projection onto a subspace,
and ∣∣U2U.TVL(t)∣2 = ∣UTV2(t)k∣ because UTU2 = Ik2; (b) is due to (41); (c) is due to (44).
From this, we get
L(Θ(t)) ≤
L(Θ(0))
1 + α2λ∣sk2t
Therefore, L(Θ(t)) → 0 as t → ∞.
E.1.2 Characterizing the limit direction
Since we proved that L(Θ(t)) → 0, the argument in the proof of Theorem 1 applies to this case,
and shows that the parameters Vl converge in direction and align with Vl = -Vvl L(Θ). Let v∞ :=
limt→∞ L')? be the limit direction of vl. As done in the proof of Theorem 2, define ρ(t) =
U1TV1(t) ΘU2TV2(t) and ρ∞ = U1T V1∞ ΘU2TV2∞.
It follows from r(t) = -y exp(-yf (x; Θ(t))) that we have sign(r(t)) = - sign(y). Using this,
(41), and alignment of vl and Vl, We have
UTv∞ H ys Θ UTv∞, UTv∞ H ys Θ UTv∞.	(45)
Element-wise multiplying UlT Vl∞ to both sides gives
(U1TV1∞)2 HysΘρ∞, (U2TV2∞)2 HysΘρ∞.	(46)
23
Published as a conference paper at ICLR 2021
Since the LHSs are positive and s is positive, the following equations have to be satisfied for all
j ∈ [m]:
sign(y) = sign([ρ∞]j).
Now, multiplying both sides of the two equations (46), we get
(ρ∞)θ2 (X sθ2 Θ (ρ∞)θ2.
From (48), ρ∞ must satisfy that
[ρ∞]j 6= 0, [ρ∞]j0 6=0 =⇒ I[s]jI=I[s]j0I,
(47)
(48)
(49)
for all j, j0 ∈ [m]. As in the proof of Theorem 2, there is another condition that has to be satisfied:
[ρ∞]j = 0,[ρ∞]j0 = 0 =⇒ ∣[s]j| ≤ ∣[s]j01,
(50)
for any j, j0 ∈ [m]; let us prove why. First, consider the time derivative of [ρ]j = [U1Tv1]j [U2Tv2]j.
d [ρ(t)]j = [UT vι(t)]j d [UT V2(t)]j + [UT V2(t)]j d [UT vι(t)]j
dt	dt	dt
(=a) -r(t)[s]j([U1Tv1(t)]j2+[U2Tv2(t)]j2),
where (a) used (41). Now consider
Ir⅛P()tj]j I = I[s]j i
[U1Tv1(t)]j2 + [U2Tv2(t)]j2
∣[ρ(t)j I
(51)
We want to compare this quantity for different j, j0 ∈ [m]. Before we do that, we take a look at the
last term in the RHS of (51). Recall from (43) that
[U1Tv1(t)]j2= [U2T v2(t)]j2 + [U1T v1(0)]j2 - [U2Tv2(0)]j2.
(52)
For simplicity, let δj := [U1Tv1(0)]j2 - [U2T v2 (0)]j2 , which is a positive number due to our assump-
tion on initialization. Then, we can use (52) and I[ρ(t)]j I = I[U1Tv1(t)]j II[U2T v2 (t)]j I to show
that
UT Vl(t)j + UT V2(t)j
2[UT V2(t)j + δ
I[ρ(t)]j I
/-------------- ≥ 2,
IUTV2(t)]j I √[UTV2(t)]2 + δj
lim
t→∞
[UT vι(t)j + [UT V2(t)j
I[ρ(t)]j∣
2 if lim I[U2Tv2(t)]j I = ∞.
t→∞	2
Recall that we want to prove that (50) should necessarily hold. For the sake of contradiction, suppose
that there exists j ∈ [m] that satisfies [ρ∞]j = 0 but I[s]j I > I[s]j0 I, for some j0 ∈ [m] satisfying
[ρ∞]j0 6= 0. Note that having [ρ∞]j = 0 and [ρ∞]j0 6= 0 implies that I[ρ(t)]j0 I → ∞ and
| [p(二)] j |	0	| [p(<)] j |
| [ρ(t)] T | → 0. We now Want to COUlpaIe Ihe IatiO of (51) for j and j . Using ∣[ρ(t)] .∣ |	→ 0 and Ihe
fact that X → χ2√2++δj is a decreasing function of X ≥ 0 for any δ > 0, we have
([UTvι(t)]2 + [U2TV2(t)]2)∕∣[ρ(t)]j I
([UTV1(t)j0 + [UTV2(t)]2o))∕∣[ρ(t)]j0∣ ≥ ,
(53)
for any t ≥ to, when to is large enough. Combining ![sjʌ > 1 and (53) to compare the ratio of (51)
for j and j0, there exists some t0 ≥ 0 such that for any t ≥ t0, we have
隆[ρ(t)j∣ ∕I[ρ(t)jI > 1
隆[ρ(t)]j0∣∕I[ρ(t)]jθ I .
(54)
This implies that the ratio of the absolute value of time derivative of [ρ(t)]j to the absolute value of
current value of [ρ(t)]j is strictly bigger than that of [ρ(t)]j0. Moreover, by the definition of r(t),
(t)]j does not change sign over time. Since this holds for all t ≥ t0 , (54) results in a growth of
)]j I that is exponentially faster than that of I[ρ(t)]j0 I, so [ρ(t)]j becomes a dominant component
in ρ(t) as t → ∞. This contradicts that [ρ∞]j = 0, hence the condition (50) has to be satisfied.
24
Published as a conference paper at ICLR 2021
So far, we have characterized some conditions (47), (49), (50) that have to be satisfied by the limit
direction ρ∞ of ρ. We now consider the following optimization problem and prove that these
conditions are in fact the KKT conditions of the optimization problem. Consider
minimize ∣ρ∣1 subject to	ysTρ ≥ 1.
The KKT condition of this problem is
(55)
∂ kρk1 3 ys,
where the global subdifferential ∂ ∣∣∙kι is defined as
∂ kρk1 = {u ∈ Rm | |[u]j | ≤ 1 for all j ∈ [m], and [ρ]j 6= 0 =⇒ [u]j = sign([ρ]j)}.
We can check from (47), (49), (50) that the that ρ∞ satisfies the KKT condition up to scaling.
Now, how do we characterize v1∞ and v2∞ in terms of ρ∞? Let η1∞ := U1T v1∞ and η2∞ := U2Tv2∞.
Then, vl∞ = Ulηl∞ = Ul UlT vl∞ holds because any component orthogonal to the column space of
Ul stays unchanged while the component in the column space of Ul diverges to infinity. By (42),
∣η∞∣ = ∣η∞∣ = ∣ρ∞∣θ1/2. By (45), we have sign(η∞) = sign(y) Θ sign(η∞).
E.2 Proof of Corollary 4
The proof of Corollary 4 boils down to characterizing the SVD of Mconv(x).
E.2. 1 THE k1 = 1 CASE
First, it is straightforward to check that for L = 2 and k1 = 1, we have
βconv(Θconv) = v1v2.
For kι = 1, the data tensor is simply Mconv(X) = XT. Thus, We have Ui = 1, U = kxp,
and s = ∣x∣2. Substituting U1 and U2 to the theorem gives the condition on initial directions in
Corollary 4. Also, the theorem implies us that the limit direction v∞ of v2 satisfies v∞ α yv∞x.
Using this, it is easy to check that
βconv(Θ∞nv) Y V∞V∞ Y yX.
E.2.2 THE k1 = 2 CASE
First, it is straightforward to check that for L = 2 and k1 = 2, we have
	[vi]l	0	0	…0	[vi]2^	
	[v1]2	[v1]1	0	…0	0	
	0	[v1]2	[vi]i	…0	0	
βconv (Θconv ) =	.	.	..	.	. .	.	...	.	V2
	.	.	.	..	. 0	0	0	…[vi ]i	0	
	0	0	0	…[vi ]2	[vi]i_	
(56)
For k1 = 2, by definition, the data tensor is
XT
Mconv(X)=I-T
and it is straightforward to check that the SVD of this matrix is
so
JkXk2 + XT 左
0
0
q∣X∣2 - XT⅛.
XT + ⅜-T	，
VkXk2 + XT 毋
q∣X∣2 - XT⅛.
25
Published as a conference paper at ICLR 2021
Substituting U1 and U2 to the theorem gives the conditions on initial directions. Also, note that the
maximum singular value depends on the sign of XT⅛-. Consider the optimization problem in the
theorem statement:
minimizeρ∈Rm	kρk1	subject to	ysTρ ≥ 1.
If xτ ⅛T > 0, then the solution ρ∞ to this problem is in the direction of [y 0]. Therefore, the limit
directions v1∞ and v2∞ will be of the form
v∞ H Cl [l] , v∞ H C2(X + t-),
where sign(c1) sign(c2) = sign(y). Using (56), it is straightforward to check that
100
1 1 0
0 1	1
βconv(Θ∞nv) H y I ...
01
0 0
0 0
(x + t-) = y(2x + I- + →).
000
000
10
11
Similarly, if XT⅛- < 0, then the solution ρ∞ is in the direction of [0 y]. Using (56), We have
	-1	0	0 …0	-1	
	-1	1	0 …0	0	
	0	-1 1 …0	0	
βconv(Θc∞onv) H y	.	. ..	.	. .	. ..	.	.	(X — ⅛T) = y(2x -⅛T- →)
	.	.	.	..	. 0	0	0	…1	0	
	0	0	0 …一1	1	
F	Proofs of Theorem 5, Corollaries 5, 6 & 7, and Lemma 4
F.1 Proof of Lemma 4
In this subsection, We restate Lemma 4 and prove it.
Lemma 4. Consider the system of ODEs, where p, q : R → R:
P = pL-2q,	q= PL-1, p(0) = 1, q(0) = 0.
Then, the solutions pL (t) and qL (t) are continuous on their maximal interval of existence of the
form (-c, c) ⊂ R for some c ∈ (0, ∞]. Define hL(t) = PL(t)L-1qL(t); then, hL (t) is odd and
strictly increasing, satisfying limt↑c hL(t) = ∞ and limt]-C hL(t) = -∞.
Proof First, continuity (and also continuous differentiability) of P(t) and q(t) is straightforWard
because the RHSS of the ODES are differentiable in P and q. Next, define p(t) = p(-t) and q(t)=
-q(-t). Then, one can show that P and q are also the solution of the ODE because
ddtP(t) = -∣p(-t) = -P(-t) = -p(-t)L-2q(-t) = P(t)L-2q(t),
dt dt
dtq(t) = - ~tq(-t) =q(-t) = P(T)LT = p(t)Lτ.
dt	dt
However, by the Picard-Lindelof theorem, the solution has to be unique; this means that p(t)=
Pq(t) = P(-t) and q(t) = qq(t) = -q(-t), which proves that P is even and q is odd and also implies
that the domain of P and q has to be of the form (-c, c) (i.e. symmetric around the origin) and
h = PL-1q is odd.
To show that h is strictly increasing, it suffices to show that P and q are both strictly increasing on
[0, c). To this end, we show that P(t) ≥ 1 for all t ∈ [0, c). First, due to the initial condition P(0) = 1
and continuity of P, there exists 1 > 0 such that P(t) > 0 for all t ∈ [0, 1) =: I1. This implies
that q(t) = p(t)L-1 > 0 for t ∈ Ii \ {0}, so q is strictly increasing on I、. Since q(0) = 0, we
have q(t) > 0 for t ∈ I、\ {0}, which then implies that p(t) = p(t)L-2q(t) > 0. Therefore, P is
26
Published as a conference paper at ICLR 2021
also strictly increasing on I1; this then means p(t) ≥ 1 for t ∈ [0, 1] because p(0) = 1. Now, due
to p(1) ≥ 1 and continuity of p, there exists 2 > 1 such that p(t) > 0 for all t ∈ [1 , 2) =: I2.
Using the argument above for I2 results in p(t) ≥ 1 for t ∈ [0, 2]. Repeating this until the end of
the domain, We can show that p(t) ≥ 1 holds for all t ∈ [0, c). By P ≥ 1, We have q = PLT ≥ 1
on [0, c), so q is strictly increasing on [0, c). Also, q(t) > 0 on (0, c), so P = pL-2q > 0 on (0, C)
and P is also strictly increasing on [0, c). This proves that h is strictly increasing on [0, c), and also
on (-c, c) by oddity of h.
Finally, it is left to show limt↑c h(t) = ∞ and limtψ-c h(t) = -∞. If c < ∞, then this together
With monotonicity implies that the limits hold. To see Why, suppose c < ∞ and limt↑c h(t) < ∞.
Then, P and q can be extended beyond t ≥ c, which contradicts the fact that (-c, c) is the maximal
interval of existence of the solution. Next, consider the case c = ∞. From P(t) ≥ 1, we have
q(t) ≥ 1 for t ≥ 0. This implies that q(t) ≥ t for t ≥ 0. Now, p(t) ≥ p(t)L-2q(t) ≥ t, which gives
p(t) ≥ t2 + 1 for t ≥ 0. Therefore, we have
lim h(t) = lim p(t)L-1q(t) ≥ lim [-- ■+11	t = ∞,
t→∞ t	t→∞	t→∞	2
hence finishing the proof.	□
F.2 Proof of Theorem 5
F.2. 1 Convergence of loss to zero
We first show that given the conditions on initialization, the training loss L(Θ(t)) converges to zero.
Recall from Section 2.1 that
V ι = -Vvi L(Θ) = M(-X T r) ◦ (vι,..., vι-ι,4, vι+ι,..., vl)∙
Applying the structure (9) in Assumption 1, we get
Vι = M(-XTr) ◦ (vι,..., vι-ι,Ikι, vι+ι,..., VL)
m
=-X[SX T r]j (VT U1]∙,j ③…% vT-1[Ul-1]∙,j 乳 Ul]∙,j 乳 vT+l[Ul+1]∙,j③…③vT [UL]∙,j)
j=1
=-X[SXTr]j( Y[UTVk]j) [U1 ]∙,j.
j=1	k6=l
Left-multiplying UlT to both sides, we get
UlTVI = -SXTr Θ YL UTVk,	(57)
where Q denotes the product using entry-wise multiplication .
Now consider the rate of growth for the second power of the j-th component of UlTvl :
d[UlTvlj =2[UlTVlj[UlTvlj = -2[SXTr]j Y 1[UTVkj = /∣[UlTvl，j|2
dt	k=1	dt
for any l0 ∈ [L]. Thus, for any j ∈ [m], the second power of the j-th components in UlTvl grow at
the same rate for each layer l ∈ [L]. This means that the gap between any two different layers stays
constant for all t ≥ 0. Combining this with our conditions on initial directions, we have
[UlTvl(t)]2 - [UL vL(t)]2 = [UlTvl(0)]2 - [UTVL(0)]2 = α2[η]2 ≥ ɑ2λ,
for any j ∈ [m], l ∈ [L - 1], and t ≥ 0. This inequality also implies
[UlTvl(t)]j2 ≥ [ULT vL(t)]j2 + α2λ ≥ α2λ.	(58)
27
Published as a conference paper at ICLR 2021
Let us now consider the time derivative of L(Θ(t)). We have the following chain of upper bounds
on the time derivative:
ddt L(Θ(t)) = VθL(Θ(t))T Θ(t) = -∣∣VθL(Θ(t))k2
≤-∣∣VvL L(θ(t))k2 = -kvL(t)k2
(≤) -kUTvL(t)k2 =) TSXTr(t) θ y；=L UTVk(t)∣∣2
= -Xjm=1[SXTr(t)]j2Yk6=L[UkTvk(t)]j2
(≤c) -α2L-2λL-1 Xm [SXT r(t)]j2
j=1
= -α2L-2λL-1kSXTr(t)k22
(d)
≤ -α2L-2λL-1smin(S)2smin(X)2kr(t)k22,
= -2α2L-2λL-1smin(S)2smin(X)2L(Θ(t)),	(59)
where (a) used the fact that k<v L(t)k2 ≥ IlULUT V L(t)k2 because it is a projection onto a subspace,
and ∣∣UlUTVL(t)k2 = IlUTVL(t)k2 because UTUL = IkL; (b) is due to (57); (c) is due to (58);
and (d) used the fact that S ∈ Rm×d and XT ∈ Rd×n are matrices that have full column rank, so for
any Z ∈ Cn, we can use IlSXTz∣∣2 ≥ Smin(S)Smin(X)∣∣z∣∣2 where Smin(∙) denotes the minimum
singular value of a matrix.
From (59), we get
L(Θ(t)) ≤ L(Θ(0))exp(-2α2L-2λL-1Smin(S)2Smin(X)2t),	(60)
so that L(Θ(t)) → 0 as t → ∞.
F.2.2 Characterizing the limit point
Now, we move on to characterize the limit points of the gradient flow. First, by defining a “trans-
formed” version of the parameters ηl (t) := UlTVl (t) and using (57), one can define an equivalent
system of ODEs:
η 1 = -SXtr θ Ylk=l ηk for l ∈ [L],
ηι(0) = αη for l ∈ [L — 1], nl(0) = 0.
(61)
Using Lemma 4, it is straightforward to verify that the solution to (61) has the following form. For
odd L, we have
for l ∈ [L - 1],
(62)
ηι(t) = 0η Θ PL (一αL-2 忻∣θl-2 Θ SXT / r
ηL(t) = α∣η∣ Θ qL (—aL-2|n|；L-2 Θ SXT Z r(τ)dτ
ηι(t) = αη ΘPL (—aL-2n；L-2 Θ SXTJ
ηL(t) = αη Θ qL (—aL-2n；L-2 Θ SXT /
Similarly, for even L, the solution for (61) satisfies
r(τ)dτ for l ∈ [L - 1],
tr(τ)dτ .	(63)
Now that we know how the solutions ηl look like, let us see how these relate to the linear coefficients
of the network. By Assumption 1, we have
mL
f(x; Θ) = M(x) ◦ (V1, . . . ,VL) = X[Sx]j Y[UlTVl]j
j=1	l=1
28
Published as a conference paper at ICLR 2021
= JX(Y[η]j)[S]j,∙ x = XTST ―阴)=XTSTP
Here, we defined ρ := Ql∈[L] ηl ∈ Rm . Therefore, the linear coefficients of the network can be
written as β(Θ(t)) = STP(t). From the solutions (62) and (63), we can write
L
P⑴=Y ηι(t) = aFnFL © hL
i=1
-αL-2 InI0L-2 © SXT Z r(τ)dτ),
where hL := pLL-1qL, defined in Lemma 4. By the convergence of the loss to zero (60), we have
limt→∞ Xβ(Θ(t)) = y. Therefore,
|
y.
(64)
Next, we will show that P∞ is in fact the solution of the following optimization problem
minimize QL,α,η(p) subject to XSTP = y,
(65)
where QL,α,η : Rm → R is a norm-like function defined using HL(t) := Rt h-1(τ)dτ:
QL,α,η(P) = α2 X[n]2HL (αLICjj IL ).
Note that the KKT conditions for (65) are
XS t p = y, VρQL,α,η(ρ) = SXT ν,
for some ν ∈ Rn. It is clear from (64) that P∞ satisfies the first condition (primal feasibility), so let
us check the other one. Through a straightforward calculation, we get
VρQL,α,η(ρ) = α2-L∣n∣02-L © h-1 (a-L|n|0(-L) © ρ).
Equating this with SXT ν gives
α2-L∣n∣02-L © h-1 b-L∣n∣0(-L) © P) = sx T V
⇔ h-1 (α-L∣n∣0(-L) © p) = αL-2∣n∣0L-2 © SXTV
⇔ P = αL∣n∣0L © hL (αL-2∣n∣0L-2 © SXTV).
Hence, by setting V = - 0∞ r(τ)dτ, P∞ satisfies this condition as well. Also, ifS is invertible, we
can substitute P = S-Tz to (65) to get the last statement of the theorem. This finishes the proof.
F.3 Proof of Corollary 5
The proof is a direct consequence of the fact that Assumption 1 holds with S = U1 = … =
UL = Id for linear diagonal networks. Hence, the proof is the same as Corollary 2, proved in
Appendix D.2.
F.4 Proof of Corollary 6
We start by showing the DFT of a real and even vector is also real and even. Suppose that X ∈ Rd
is real and even. First,
1d
[Fx]j = F£[x]k exP
d k=1
√-1 ∙ 2∏(j- 1)(k - 1)
—
d
29
Published as a conference paper at ICLR 2021
2∏(j — 1)(k — 1)
d
,√-1 X
+	[x]k sin
d k=1
2∏(j — 1)(k — 1)
d
2∏(j — 1)(k — 1)
d
∈ R,
for all j ∈ [d]. To prove that Fx is even, for j = 0,..., [ d-3 C, We have
[Fx]j+2 = 4= X[x]k cos
d k=1
1d
=FE[x]k Cos
d k=1
1d
=FE[x]k Cos
d k=1
1d
=FE[x]k Cos
d k=1
= [F x]d-j .
2π(j+1)(k-1)
—
-1)-
—
-j- 1)(k - 1)
d
2π(j+1)(k-1)
2π(d-j-1)(k-1)
d
d
d
It is proved in Appendix D.3 that linear full-length convolutional networks (kι =…=⅛l = d)
satisfy Assumption 1 with S = d --^ F and Ui = •一= UL = F *, where F ∈ Cd×d is the matrix
of discrete Fourier transform basis [F j,k = √ exp(- V-^2"。1'.-1)) and F* is the complex
conjugate of F .
The proof of convergence of loss to zero in Appendix F.2.1 is written for real matrices
S, U1, . . . , UL, but we can actually apply the same argument as in Appendix D.1.1 and prove that
the loss converges to zero, even in the case where S, U1, . . . , UL are complex.
Next, since Ul’s are complex, we can write the system of ODE as (see (20) for its derivation)
Fw ι
L-1
-dFFXTr Θ ɪɪH F*wk,
(66)
Since all data points xi and initialization wl (0) are real and even, we have that FXTr is real and
even, and F*wl (0) = Fwl (0)’s are real and even. By (66), we see that the time derivatives of Fwl
are also real and even. Thus, the parameters wl (t) are all real and even for all t ≥ 0. From this
observation, we can define ηι(t) := Fwι(t), η := FW, and S := d--^ Re(F), which are all real
by the even symmetry. Then, starting from (61), the proof goes through.
F.5 Proof of Corollary 7
Since the sensor matrices A1, . . . , An commute, they are simultaneously diagonalizable with a real
unitary matrix U ∈ Rd×d, i.e., UTAi U’s are diagonal matrices. From the deep matrix sensing
problem (13), we can compute Vwi Lms, which gives the gradient flow dynamics of Wι.
W1 = -VWlLms = -Wl-I …WT (Xn=Iri Ai)WLT ∙∙∙ WT+i,
where r =hA%, Wi ∙ ∙ ∙ WL)一 yi is the residual for the i-th sensor matrix. If we left-multiply UT
and right-multiply U to both sides, we get
UTWlU = -UTW-1U ∙ ∙ ∙ UTWTU(X： JiUTAiU)UTWTU ∙ ∙ ∙ UTWT+iU.	(67)
If UT WT U is a diagonal matrix for all k = l, then UT Wl U is also a diagonal matrix. Note also
that, since Wl(0) = αId = αUUT for l ∈ [L - 1], the product UTWlU is a diagonal matrix at
initialization. These observations imply that Wl(t)’s are all diagonalizable with U for all t ≥ 0.
30
Published as a conference paper at ICLR 2021
Now, define vl (t) = eig(Wl(t)), i.e., UTWlU = diag(vl). Also, let xi = eig(Ai). Then, (67)
can be written as
Vl = TXn=Irixi) θ Y；=*.
Therefore, this is equivalent to the regression problem with linear diagonal networks, initialized at
vl(0) = α1 for l ∈ [L - 1] and vL(0) = 0. Given this equivalence, Corollary 7 can be implied from
Corollary 5.
G Proof of Theorem 6
G.1 Convergence of loss to zero
We first show that given the conditions on initialization, the training loss L(Θ(t)) converges to zero.
Since L = 2 and M(x) = U1 diag(s)U2T, we can write the gradient flow dynamics from Section 2.1
as
V1 = -M(XTr) ◦ (Iki, V2) = -rUι diag(s)UTV2,
V2 = -M(XTr) ◦ (vι,几)=-rU2 diag(s)UTvι,
where r(t) = f(x; Θ(t)) - y is the residual of the data point (x, y). From (68) we get
UT V1 = —rs Θ UTV2, UTV2 = -rs Θ U1TV1.
(68)
(69)
Now consider the rate of growth for the j -th component of U1T V1 squared:
ddt [UTV1]2 = 2[UTV1j [UT V ιj = -2r[sj [UT Vι]j [U2T V2 j = d [U2T V2j.
dt	dt
So for any j ∈ [m], [U1TV1]j2 and [U2TV2]j2 grow at the same rate. This means that the gap between
the two layers stays constant for all t ≥ 0. Combining this with our conditions on initial directions,
[U1T V1(t)]j2 - [U2T V2(t)]j2 = [U1T V1(0)]j2 - [U2T V2(0)]j2
=α2[UTVι]2 -α2[UTV2]2 ≥ ɑ2λ,
for any j ∈ [m] and t ≥ 0. This inequality implies
[U1T V1 (t)]j2 ≥ [U2TV2(t)]j2 + α2λ ≥ α2λ.	(70)
Let us now consider the time derivative of L(Θ(t)). We have the following chain of upper bounds
on the time derivative:
-d L(Θ(t)) = VθL(Θ(t))T Θ(t) = -kVθL(Θ(t))k2
dt
≤-kVv2 L(θ(t))k2 = -kV2(t)k2
(≤)-kUTV2(t)k2=)-r(t)2∣∣s Θ UTV1(t)∣∣2
= -r(t)2 Xjm=1[s]j2[U1T V1(t)]j2
-2α2λksk22L(Θ(t)),
where (a) used the fact that ∣∣V2(t)k2 ≥ ∣∣U2U^TV2(t)k2 because it is a projection onto a subspace,
and ∣∣U2U^TVL(t)∣2 = ∣UTV2(t)∣2 because UTU2 = Ik2; (b) is due to (69); (c) is due to (70).
From this, we get
L(Θ(t)) ≤ L(Θ(0)) exp(-2α2λ∣s∣22t).	(71)
Therefore, L(Θ(t)) → 0 as t → ∞.
31
Published as a conference paper at ICLR 2021
G.2 Characterizing the limit point
Now, we move on to characterize the limit points of the gradient flow. First, note that any changes
made in vl over time are in the subspace spanned by the columns of Ul . Therefore, any component
in the initialization vι(0) = αVι that is orthogonal to the column space of Ul stays constant.
So, we can focus on the evolution of vl in the column space of Ul ; this can be done by defining
a “transformed” version of the parameters ηl (t) := UlT vl (t) and using (69), one can define an
equivalent system of ODEs:
力 1 = -rs 0 η2,力2 = -rs Θ ηι,
ηι(0) = αηι, η2(0) = αη2,
(72)
where ηι := UTVι, η2 := UTV2. It is straightforward to verify that the solution to (72) has the
following form.
ηι(t) = αηι 0 cosh (—s / r(τ)dτ) + αf∣2 0 Sinh (—s / r(τ)dτ
η2(t) = α∙η 1 0 sinh (—s / r(τ)dτ) + αf∣2 0 Cosh (—s / r(τ)dτ
(73)
By the convergence of the loss to zero (71), we have limt→∞ f(x; Θ(t)) = y. Note that f(x; Θ(t))
can be written as
f(x; Θ(t)) = M(x) ◦ (v1(t),v2(t)) = v1(t)TM(x)v2(t)
= v1(t)TU1 diag(s)U2T v2(t) = sT (η1 (t) 0 η2(t)).
Therefore,
lim f (x; Θ(t)) = lim sT (η1 (t) 0η2(t))
t→∞	t→∞
α2sT
η?2 + 西
m
α2 X[s]j
j=1
+ (ηι 0 η2) 0 Cosh
α2sτ (η?2 + η,2) 0 cosh
(τ )dτ 0 sinh
∞
r(τ )dτ + sinh
∞
r (τ )dτ
(τ )dτ
0 sinh (—2s / r(τ)dτ) +(ηι 0 η2) 0 cosh (—2s / r(τ)dτ
忻 1]2 + 忻2]2
sinh (2[s]jV) + 忻山年 j cosh (2[sjV)
2
2
y,
(74)
where we defined V := — 0∞ r(τ)dτ. Consider the function V 7→ a sinh(V) + bcosh(V). This is a
strictly increasing function if a > |b|. Note also that
j^ mi]"",
(75)
which holds with equality if and only if | [ηι] j | = | [η2]j |. However, recall from our assumptions on
initialization that [ηι]2 — [η2j ≥ λ > 0, so (75) can only hold with strict inequality. Therefore,
g(V) := X[s]j ([η1]j + [η2]j Sinh⑵s]jV) + [ηι]j 忻2匕 CoSh⑵s]jV))
is a strictly increasing (hence invertible) function because it is a sum of m strictly increasing func-
tion. Using this g(V), (74) can be written as α2g(V) = y, and by using the inverse ofg, we have
V = —	r(τ)dτ = g-1
0
(76)
32
Published as a conference paper at ICLR 2021
Plugging (76) into (73), we get
lim v1 (t)
t→∞
Ui lim ηι(t) + α(Ikι - UIUT)Vι
t→∞
αUι (ηι Θ Cosh (g-i (02) S) + η Θ Sinh (g-i (3)S) + + α(Ik1
lim v2(t)
t→∞
U2 lim η2(t)+ α(Ik2 - U2UT)V2
t→∞
0U2 (ηι Θ sinh (g-i O S) + η Θ Cosh (g-i (*) S)) + α(Ik2
-Ui UT )Vi,
-U2UT)V2.
This finishes the proof.
H Proof of Theorem 7
H.1 Convergence of loss to zero
We first show that given the conditions on initialization, the training loss L(Θ(t)) converges to zero.
Recall from (10) that the linear fully-connected network can be written as
ffc(X； Θfc) = XTWiW2 …WL-1WL.
From the definition of the training loss L, it is straightforward to check that the gradient flow dy-
namics read
WI = -Vwι L(Θfc) = -W-I …WTXTTwnLWL-I …W+i for l ∈ [L - 1],
Wl = -VwlL(Θfc) = -WL-I …WTXtr,
Wl(0) = αWl for l ∈ [L - 1],
wL (0) = αwL,
(77)
where r ∈ Rn is the residual vector satisfying [r]i = ffc(xi; Θfc) - yi, as defined in Section 2.1.
From (77), we have
WITWl = Wι+ιWι+i = -WT …WTXTTWLWL-I …Wι+i,
WTWι = Wι+iWι+i = -Wι+i ∙ ∙ ∙ Wl-iWlttXWi ∙ ∙ ∙ Wι,
for any l ∈ [L - 2]. From this, we have
J WIT Wι = J Wι+iWι+i,
dt	dt	+
and thus
Wι(t)TWι(t) -Wι+i(t)Wι+i(t)T= Wι(0)TWι(0) - Wι+i(0)Wι+i(0)T
=α2WTWι - α2Wι+iW+i,
for any l ∈ [L - 2]. Similarly, we have
WL-i(t)TWL-i(t) -wL(t)wL(t)T= WL-i(0)TWL-i(0) -wL(0)wL(0)T
=α2WL-IWl-i - α w LwrL.
(78)
(79)
Let us now consider the time derivative of L(Θfc(t)). We have the following chain of upper bounds
on the time derivative:
d L(Θfc(t)) = Vθfc L(Θfc(t))τΘfc(t) = -kVθfc L(Θfc(t))k2
≤-kVwL L(θfc(t))k2 = -kwL(t)k2
=-kWT-i …WT X T rk2.	(80)
33
Published as a conference paper at ICLR 2021
Note from (80) that if WT-ι ∙ ∙ ∙ WT is full-rank, its minimum singular value is positive, and one
can bound
kWLT-1 …WTXTrk2 ≥ σmin(WT-ι …WT)∣∣XT川[	(81)
We now prove that the matrix WT-1 … WT is full-rank, and its minimum singular value is bounded
from below by aL-1λ(LT)/2 for any t ≥ 0. To show this, it suffices to show that
WT-1 ∙∙∙ WT W1 ∙∙∙ Wl-i 占 α2L-2λL-1Id.	(82)
Now,
WL-I …WTWT WιW2 …Wl-i
(= WT-1 ∙ ∙ ∙ WT(W2 W2T + α2 WTW1 — α2 W2 WT)W2 .一 WL-I
(b)
占 WT-1 ∙ ∙ ∙ WT WT W2WT W2W3 ∙∙∙ WL-I
(= WT-I …WT(W3WT + α2 WTW2 - α2W3 WT)2W3 …WL-I
(b)
之 WT-I …WT(W3WT)2 W3 …WL-I
=•.士 (WL-IWL-I)L-1,
where equalities marked in (a) used (78), and inequalities marked in (b) used the initialization con-
ditions WIT W∣ 占 W∣+ι W屋 Next, it follows from (79) that
(WL-IWL-I)LT = (WLwL + α2WT-1WL-1- α2wLwT)LT
之 α2L-2(WT-WL-i- WLwT)LT
(c)
α2L-2λL-1Id.
where (c) used the assumption that WL-IWL-ι 一 wLwT 占 λId. This proves (82). Applying (82)
to (80) then gives
ddtL(Θfc(t)) ≤-kWL-ι …WLXTrk2
≤-σmin(WL-ι …WL)2kXTrk2
≤ -α2L-2λL-1kXTrk22
(d)
≤ -α2L-2λL-1σmin(X)2krk22
= -α2L-2λL-1σmin(X)2L(Θfc(t)),
where (d) used the fact that XT is a full column rank matrix to apply a bound similar to (81). From
this, we get
L(Θfc(t)) ≤ L(Θfc(0)) exp(-α2L-2λL-1σmin(X)2t),
hence proving L(Θfc (t)) → 0 as t → ∞.
H.2 CHARACTERIZING THE LIMIT POINT: α → 0 CASE
Now, we move on to characterize the limit points of the gradient flow, for the “active regime” case
α → 0. This part of the proof is motivated from the analysis in Ji & Telgarsky (2019a).
Let ul and vl be the top left and right singular vectors of Wl, for l ∈ [L - 1]. Note that since Wl
varies over time, the singular vectors and singular value also vary over time. Similarly, let sl be the
largest singular value of W?. We will show that the linear coefficients βfc(Θfc) = Wi ∙ ∙ ∙ WL-IwL
align with u1 as α → 0, and u1 is in the subspace of row(X) in the limit α → 0, hence proving
that βfc (Θfc) is the minimum `2 norm solution in the limit α → 0.
First, note from (78) and (79) that if we take trace of both sides, we get
kWιkF - kWi+ikF = α2(∣∣W1∣∣F -IIW∣+1∣∣F) for l ∈ [L - 2],
34
Published as a conference paper at ICLR 2021
kWL-lkF -kwLk2 = α2(∣∣WL-1∣∣F -kWLk2).
Summing the equations above for l, l + 1, . . . , L - 1, we get
kWιkF-kwL k2 = α2(∣∣wι∣∣F -kWLk2).
Next, consider the operator norms (i.e., the maximum singular values), denoted as ∣∣∙∣∣2,
matrices.
kWlk22 ≥ulT+1WlTWlul+1
=) UT+1W1+1W1+1U1+1 + α2uT+1(W(TWι - WmW)uι+ι
=kW(+ιk2 + α2Ua(WTW( - W:+ιW+1)u1+1
≥ kW(+ιk2 - α2kWTWι - Wι+ιW(+ιk2 for l ∈ [L - 2],
(83)
of the
kWL-ιk2 ≥ rw⅛WL-IWL-1 τwLr
L-1 2 kwLk2 L-1	L-1kwLk2
(f) wL	T wL	2 wL	T	T
=	WLWL	+ α	(WL-IWL-I- W LWL )
kwLk2	L kwLk2	kwLk2	L-1	L
≥ kWLk2 - α2kWT-IWL-1 - WLW T k2.
where (e) used (78) and (f) used (79). Summing the inequalities gives
L-1
kW(k2 ≥ kWLk2 - α2 X kWTWk - WmWT+1k2.
k=1
WL
kwLk2
(84)
From (83) and (84), we get a bound on the gap between the second powers of the Frobenius norm
(or the `2 norm of singular values) and operator norm (or the maximum singular value sl) of Wl:
L-1
kWι(t)kF - kWι(t)k2 ≤ α2(∣WJIF -kwLk2) + α2 X kWTWk - Wk+1 WT+1k2,	(85)
k=l
which holds for any t ≥ 0. The gap (85) implies that each Wl, for l ∈ [L - 1], can be written as
Wl(t) = sl(t)ul(t)vl(t)T + O(α2).	(86)
Next, we show that the “adjacent” singular vectors vl and ul+1 align with each other as α → 0. To
this end, we will get lower and upper bounds for a quantity vlT Wl+1WlT+1vl.
VTW(+1 W1+1V1 = VTWTWlvl- a2vTWtW(Vl + α2vf Wm W百通
≥ kWl k2 - α2∣∣WT W( - Wι+ιWf+ι∣∣2
=s2 - α2∣∣WtW( - W:+ιW(+ι∣∣2 ,	(87)
VlTWl+1WlT+1Vl = VlT(sl2+1ul+1ulT+1 + Wl+1WlT+1 - sl2+1ul+1ulT+1)Vl
= sl2+1(VlTul+1)2 +VlT(Wl+1WlT+1 - sl2+1ul+1ulT+1)Vl
≤ sl2+1(VlTul+1)2 + kWl+1k2F - kWl+1k22 .	(88)
Combining (87), (88), and (85), we get
s2 ≤ s2+1(vTul+1)2 + α2 ∣∣wtWl - Wl + 1W1+1∣∣2 + IIWl + lkF - kWl + 1k2
L-1
≤ s2+1(vTul+1)2 + α2(IIWl+1∣∣F -||WLk2)+ α2 X kWTWk - Wk + 1WL+1∣∣2.	(89)
k=l
Next, by a similar reasoning as (87), we have
s2 ≥ UEIWlTWlul+ι ≥ s2+ι - α2 ∣∣WtW( - WmW(+ι∣∣2 .	(90)
Combining (89) and (90) and dividing both sides by sl2+1, we get
(vl(t)Tul+1(t))2 ≥ 1 - α2 —Gl-2	(91)
sl+1(t)2
35
Published as a conference paper at ICLR 2021
for t ≥ 0, where
L-1
Gl = IlWTWI - Wι+ιWι+ι∣∣2 + (IlW∣+1IlF -kWLk2) + X kWTWk - WmWT+1k2.
k=l
By a similar argument, we can also get
(VL-I⑴T wL(t))2	2 GL-I
≥> 1 — α
∣∣WL (t)k2	-	∣∣WL(t)k2
where
(92)
GL-I= 2 Il WL-IWL-1 - WLwT Il2 .
From (91) and (92), we can note that as α → 0, the inner product between the adjacent singular
vectors converges to ±1, unless s2, . . . , sL-1, kwLk2 also diminish to zero. So it is left to show
that the singular values do not diminish to zero as α → 0. To this end, recall that we proved in the
previous subsection that
lim XW1(t)…WL-ι(t)wL(t) = y.
t→∞
A necessary condition for this to hold is that
kyk2
kx k2
L-1
≤ t→m∞ kW1 ⑴…WL-I⑴WL⑴k2 ≤ t→in Y Sl⑴ kwL⑴k2 .
l=1
This means that after converging to the global minimum solution of the problem (i.e., t → ∞),
the product of the singular values must be at least greater than some constant independent of α.
Moreover, we can see from (87) and (90) that the gap between singular values squared of adjacent
layers is bounded by O(α2), for all t≥	0; so the maximum singular values become closer and closer
to each other as α diminishes. This implies that
kyk1/L	kyk1/L
lim lim sl(t) >	2-π- for l ∈ [L 一 1], lim lim ∣∣WL(t)k2 >	2-π-.
α→0 t→∞	kX k1/L	α→0 t→∞	2	kX k1/L
Therefore, we have the alignment of singular vectors at convergence as α → 0:
lim lim (vl(t)Tul+1(t))2 = 1, for l ∈ [L — 2], lim lim (VL-I⑴ wL2")) = 1.	(93)
α→0 t→∞	α→0 t→∞	kWL (t)k22
So far, we saw from (86) that Wl (t)’s become rank-1 matrices as α → 0, and from (93) that the
top singular vectors align with each other as t → ∞ and α → 0. These imply that, as t → ∞ and
α → 0, βfc(Θfc) is a scalar multiple of the u1, the top left singular vector of W1:
lim lim βfc(Θfc(t)) = C ∙ lim lim uι(t),	(94)
α→0 t→∞ fc fc	α→0 t→∞ 1
for some c ∈ R.
In light of (94), it remains to take a close look atu1(t). Note from the gradient flow dynamics of W1
that W1 is always a rank-1 matrix whose columns are in the row space of X, since XT r ∈ row(X).
This implies that, if we decompose W1 into two orthogonal components W1⊥ and W1k so that the
columns in W1k are in row(X) and the columns in W1⊥ are in the orthogonal subspace row(X)⊥,
we have
W⊥ = 0, Wk = Wι.
That is, any component W1⊥(0) orthogonal to row(X) remains unchanged for all t≥ 0, while the
component W1k changes by the gradient flow. Since we have
IIW⊥(t)h = IIw⊥(0)IIf ≤ αIIWJIf,
the component in W1 that is orthogonal to row(X) diminishes to zero as α → 0. This means that
at the limit α → 0, the columns of W1 are entirely from row(X), which also means that
lim lim βfc(Θfc(t)) ∈ row(X).
α→0 t→∞
However, recall that there is only one unique global minimum of Xz = y in row(X): namely,
z = XT(XXT)-1y, the minimum `2 norm solution. This finishes the proof.
36