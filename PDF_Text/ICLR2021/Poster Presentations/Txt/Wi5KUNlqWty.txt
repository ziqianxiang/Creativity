Published as a conference paper at ICLR 2021
How to Find Your Friendly Neighborhood:
Graph Attention Design with Self-Supervision
Dongkwan Kim & Alice Oh
KAIST, Republic of Korea
dongkwan.kim@kaist.ac.kr, alice.oh@kaist.edu
Ab stract
Attention mechanism in graph neural networks is designed to assign larger
weights to important neighbor nodes for better representation. However, what
graph attention learns is not understood well, particularly when graphs are noisy.
In this paper, we propose a self-supervised graph attention network (SuperGAT),
an improved graph attention model for noisy graphs. Specifically, we exploit two
attention forms compatible with a self-supervised task to predict edges, whose
presence and absence contain the inherent information about the importance of
the relationships between nodes. By encoding edges, SuperGAT learns more
expressive attention in distinguishing mislinked neighbors. We find two graph
characteristics influence the effectiveness of attention forms and self-supervision:
homophily and average degree. Thus, our recipe provides guidance on which at-
tention design to use when those two graph characteristics are known. Our experi-
ment on 17 real-world datasets demonstrates that our recipe generalizes across 15
datasets of them, and our models designed by recipe show improved performance
over baselines.
1	Introduction
Graphs are widely used in various domains, such as social networks, biology, and chemistry. Since
their patterns are complex and irregular, learning to represent graphs is challenging (Bruna et al.,
2014; Henaff et al., 2015; Defferrard et al., 2016; Duvenaud et al., 2015; Atwood & Towsley,
2016). Recently, graph neural networks (GNNs) have shown a significant performance improve-
ment by generating features of the center node by aggregating those of its neighbors (Zhou et al.,
2018; Wu et al., 2020). However, real-world graphs are often noisy with connections between un-
related nodes, and this causes GNNs to learn suboptimal representations. Graph attention networks
(GATs) (Velickovic et al., 2018) adopt self-attention to alleviate this issue. Similar to attention in
sequential data (Luong et al., 2015; Bahdanau et al., 2015; Vaswani et al., 2017), graph attention
captures the relational importance of a graph, in other words, the degree of importance of each of
the neighbors to represent the center node. GATs have shown performance improvements in node
classification, but they are inconsistent in the degree of improvement across datasets, and there is
little understanding of what graph attention actually learns.
Hence, there is still room for graph attention to improve, and we start by assessing and learning the
relational importance for each graph via self-supervised attention. We leverage edges that explicitly
encode information about the importance of relations provided by a graph. If node i and j are
linked, they are more relevant to each other than others, and if node i and j are not linked, they are
not important to each other. Although conventional attention is trained without direct supervision,
if we have prior knowledge about what to attend, we can supervise attention using them (Knyazev
et al., 2019; Yu et al., 2017). Specifically, we exploit a self-supervised task, using the attention value
as input to predict the likelihood that an edge exists between nodes.
To encode edges in graph attention, we first analyze what graph attention learns and how it relates
to the presence of edges. In this analysis, we focus on two commonly used attention mechanisms,
GAT’s original single-layer neural network (GO) and dot-product (DP), as building blocks of our
proposed model, self-supervised graph attention network (SuperGAT). We observe that DP attention
shows better performance than GO attention in the task to predict link with attention value. On the
other hand, GO attention outperforms DP attention in capturing label-agreement between a target
1
Published as a conference paper at ICLR 2021
node and its neighbors. Based on our analysis, we propose two variants of SuperGAT, scaled dot-
product (SD) and mixed GO and DP (MX), to emphasize the strength of GO and DP.
Then, which graph attention models the relational importance best and produces the best node repre-
sentations? We find that it depends on the average degree and homophily of the graph. We generate
synthetic graph datasets with various degrees and homophily, and analyze how the choice of atten-
tion affects node classification performance. Based on this result, we propose a recipe to design
graph attention with edge self-supervision that works most effectively for given graph characteris-
tics. We conduct experiments on a total of 17 real-world datasets and demonstrate that our recipe
can be generalized across them. In addition, we show that models developed by our method improve
performance over baselines.
We present the following contributions. First, we present models with self-supervised attention using
edge information. Second, we analyze the classic attention forms GO and DP using label-agreement
and link prediction tasks, and this analysis reveals that GO is better at label agreement and DP at
link prediction. Third, we propose recipes to design graph attention concerning homophily and
average degree and confirm its validity through experiments on real-world datasets. We make our
code available for future research (https://github.com/dongkwan-kim/SuperGAT).
2	Related Work
Deep neural networks are actively studied in modeling graphs, for example the graph convolutional
networks (Kipf & Welling, 2017) which approximate spectral graph convolution (Bruna et al., 2014;
Defferrard et al., 2016). A representative work in a non-spectral way is the graph attention net-
works (GATs) (Velickovic et al., 2018) which model relations in graphs using self-attention mech-
anism (Vaswani et al., 2017). Similar to attention in sequence data (Bahdanau et al., 2015; Luong
et al., 2015; Vaswani et al., 2017), variants of attention in graph neural networks (Thekumpara-
mpil et al., 2018; Zhang et al., 2018; Wang et al., 2019a; Gao & Ji, 2019; Zhang et al., 2020; Hou
et al., 2020) are trained without direct supervision. Our work is motivated by studies that improve
attention’s expressive power by giving direct supervision (Knyazev et al., 2019; Yu et al., 2017).
Specifically, we employ a self-supervised task to predict edge presence from attention value. This is
in line with two branches of recent GNN research: self-supervision and graph structure learning.
Recent studies about self-supervised learning for GNNs propose tasks leveraging the inherent in-
formation in the graph structure: clustering, partitioning, context prediction after node masking,
and completion after attribute masking (Hu et al., 2020b; Hui et al., 2020; Sun et al., 2020; You
et al., 2020). To the best of our knowledge, ours is the first study to analyze self-supervised learn-
ing of graph attention with edge information. Our self-supervised task is similar to link predic-
tion (Liben-Nowell & Kleinberg, 2007), which is a well-studied problem and recently tackled by
neural networks (Zhang & Chen, 2017; 2018). Our DP attention to predict links is motivated by
graph autoencoder (GAE) (Kipf & Welling, 2016) and its extensions (Pan et al., 2018; Park et al.,
2019) reconstructing edges by applying a dot-product decoder to node representations.
Graph structure learning is an approach to learn the underlying graph structure while jointly learning
downstream tasks (Jiang et al., 2019; Franceschi et al., 2019; Klicpera et al., 2019; Stretcu et al.,
2019; Zheng et al., 2020). Since real-world graphs often have noisy edges, encoding structure
information contributes to learn better representation. However, recent models with graph structure
learning suffer from high memory and computational complexity. Some studies target all spaces
where edges can exist, so they require O(|V |2) space and computational complexity (Jiang et al.,
2019; Franceschi et al., 2019). Others using iterative training (or co-training) between the GNNs
and the structure learning model are time-intensive in training (Franceschi et al., 2019; Stretcu et al.,
2019). We moderate this problem using graph attention, which consists of parallelizable operations,
and our model is built on it without additional parameters. Our model learns attention values that
are predictive of edges, and this can be seen as a new paradigm of learning the graph structure.
3	Model
In this section, We review the original GAT (Velickovic et al., 2018) and then describe our self-
supervised GAT (SuperGAT) models.
2
Published as a conference paper at ICLR 2021
Figure 1: Overview of attention mechanism of SUPerGATs: GO, DP, MX, and SD. Blue circles (e4)
represent the unnormalized attention before Softmax and red diamonds (φj) indicate the probability
of edge between node i and j. The attention mechanism of the original GAT (Velickovic et al., 2018)
is in the dashed rectangle.
Softmax.
e -→Θ~~F
Φij ♦= Pw, i) ∈ E)
Notation For a graph G = (V, E), N is the number of nodes and Fl is the number of features at
layer l. Graph attention layer takes a set of features Hl = {hl1, . . . , hlN}, hli ∈ RFl as input and pro-
duces output features Hl+1 = {hl1+1, . . . , hlN+1}. To compute hli+1, the model multiplies the weight
matrix Wl+1 ∈ RF l+1 ×Fl to Hl, linearly combines the features of its first-order neighbors (includ-
ing itself) j ∈ Ni ∪ {i} by attention coefficients αli+j 1, and finally applies a non-linear activation ρ.
That is hli+1 =ρ Pj∈Ni∪{i} αli+j 1W l+1hlj
We can compute αli+j 1 = softmaxj (LReLU(eli+j 1))
by normalizing eli+j 1 = ae(Wl+1hli, Wl+1hlj) with softmax on Ni ∪ {i} under leaky ReLU activa-
tion (Maas et al., 2013), where ae is a function of the form RF l+1 × RF l+1 → R.
Graph Attention Forms Among two widely used attention mechanisms, the original GAT (GO)
computes the coefficients by single-layer feed-forward network parameterized by al+1 ∈ R2F l+1 .
The other is the dot-product (DP) attention, (Luong et al., 2015; Vaswani et al., 2017) motivated by
prior work on node representation learning, and it adopts the same mathematical expression for link
prediction score (Tang et al., 2015; Kipf & Welling, 2016),
eli+j,G1O = (al+1)> Wl+1hlikWl+1hlj	and eli+j,D1P = (Wl+1hli)> • Wl+1hlj.	(1)
From now on, we call GAT that uses GO and DP as GATGO and GATDP, respectively.
Self-supervised Graph Attention Network We propose SuperGAT with the idea of guiding at-
tention with the presence or absence of an edge between a node pair. We exploit the link prediction
task to self-supervise attention with labels from edges: for a pair i and j, 1 if an edge exists and 0
otherwise. We introduce aφ with sigmoid σ to infer the probability φij ofan edge between i andj.
aφ : RF × RF → R and φij = P ((j, i) ∈ E) = σ(aφ(Whi, Whj))	(2)
We employ four types (GO, DP, SD, and MX) of SuperGAT based on GO and DP attention. For
aφ, the form of which is the same as ae in GATGO and GATDP, we name them SuperGATGO and
SuperGATDP respectively. For more advanced versions, we describe SuperGATSD (Scaled Dot-
product) and SuperGATMX (Mixed GO and DP) by unnormalized attention eij and probability φij
that an edge exist between i and j .
SUPerGATsd： ej,SD = ej,DP/VF, φij,sD = σ(ej,sD).	(3)
SUPerGATmx： ej,MX = ej,GO ∙ σ(ej,Dp), φj,Mχ = σ(ej,Dp).	(4)
SuperGATSD divides the dot-product of nodes by a square root of dimension as Trans-
former (Vaswani et al., 2017). This prevents some large values to dominate the entire attention
after softmax. SuperGATMX multiplies GO and DP attention with sigmoid. The motivation of this
form comes from the gating mechanism of Gated Recurrent Units (Cho et al., 2014). Since DP
attention with the sigmoid represents the probability of an edge, it can softly drop neighbors that are
not likely linked while implicitly assigning importance to the remaining nodes.
3
Published as a conference paper at ICLR 2021
Training samples are a set of edges E and the complementary set Ec = (V × V ) \ E. However, if
the number of nodes is large, it is not efficient to use all possible negative cases in Ec . So, we use
negative sampling as in training word or graph embeddings (Mikolov et al., 2013; Tang et al., 2015;
Grover & Leskovec, 2016), arbitrarily choosing a total of Pn ∙ |E| negative samples E- from Ec
where the negative sampling ratio pn ∈ R+ is a hyperparameter. SuperGAT is capable of modeling
graphs that are sparse with a sufficiently large number of negative samples (i.e., |V × V |	|E|),
but this is generally not a problem because most real-world graphs are sparse (Chung, 2010).
We define the optimization objective of layer l as a binary cross-entropy loss LlE,
LlE
=- ∣E∪E-| P(j,i)∈E∪E- 1(j,i) = 1 ∙ log φij + 1(j,i)=0 ∙ log (1 - φj)，	⑸
where L is an indicator function. We use a subset of E ∪ E- sampled by probability Pe ∈ (0,1]
(also a hyperparameter) at each training iteration for a regularization effect from randomness.
Finally, we combine cross-entropy loss on node labels (LV ), self-supervised graph attention losses
for all L layers (LlE), and L2 regularization loss, with mixing coefficients λE and λ2 .
L = LV + λE ∙ PL=ILE + λ2 ∙kWk2.	(6)
We use the same form of multi-head attention in GAT and take the mean of each head’s atten-
tion value before the sigmoid to compute φij . Note that SuperGAT has equivalent time and space
complexity as GAT. To compute LlE for one head, we need additional operations in terms of
O(Fl ∙ |E ∪ E -|), and We do not need extra parameters.
4	Experiments
Our primary research objective is to design graph attentions that are effective with edge self-
supervision. To do this, we pose four specific research questions. We first analyze what basic graph
attentions (GO and DP) learn (RQ1 and 2) and how that can be improved with edge self-supervision
(RQ3 and 4). We describe each research question and the corresponding experiment design below.
RQ1. Does graph attention learn label-agreement? First, we evaluate what the graph attentions
of GATGO and GATDP learn without edge supervision. For this, we present ground-truth of relational
importance and a metric to assess graph attention with ground-truth. Wang et al. (2019a) showed
that node representations in the connected component converge to the same value in deep GATs.
If there is an edge between nodes with different labels, then it will be hard to distinguish the two
corresponding labels with GAT of sufficiently many layers; that is, ideal attention should give all
weights to label-agreed neighbors. In that sense, we choose label-agreement between nodes as
ground-truth of importance.
We compare label-agreement and graph attention based on Kullback-Leibler divergence of the nor-
malized attention αk = [αkk, αk1, . . . , αkJ] with label agreement distribution for the center node k
and its neighbors 1 to J. The label agreement distribution, 'k = ['kk ,'ki,..., 'kj] is defined by,
`kj = `kj/ s `ks,	`kj = 1 (if k andj have the same label) or 0 (otherwise).	(7)
We employ KL divergence in Eq. 8, whose value becomes small when attention captures well the
label-agreement between a node and its neighbors.
KLD(αk , `k) = Pj∈Nk∪{k} αkj log(αkj /`kj )	(8)
RQ2. Is graph attention predictive of edge presence? To evaluate how well edge information is
encoded in SuperGAT, we conduct link prediction experiments with SuperGATGO and SuperGATDP
using φij of the last layer as a predictor. We measure the performance by AUC over multiple runs.
Since link prediction performance depends on the mixing coefficient λE in Eq. 6, we adopt multiple
λE ∈ {10-3, 10-2, . . . , 103}. We train with an incomplete set of edges, and test with the missing
edges and the same number of negative samples. At the same time, node classification performance
is measured with the same settings to see how learning edge presence affects node classification.
4
Published as a conference paper at ICLR 2021
RQ3. Which graph attention should we use for given graphs? The above two research ques-
tions explore what different graph attention learns with or without supervision of edge presence.
Then, which graph attention is effective among them for given graphs? We hypothesize that differ-
ent graph attention will have different abilities to model graphs under various homophily and average
degree. We choose these two properties among various graph statistics because they determine the
quality and quantity of labels in our self-supervised task. From the perspective of supervised learn-
ing of graph attention with edge labels, the learning result depends on how noisy labels are (i.e., how
low the homophily is) and how many labels exist (i.e., how high the average degree is). So, we gen-
erate 144 synthetic graphs (Section 4.1) controlling 9 homoPhily (0.1 - 0.9) and 16 average degree
(1 - 100) and perform the node classification task in the transdUctive setting with GCN, GATgo,
SuperGATSD, and SuperGATMX.
In RQ3, there are also practical reasons to Use the average degree and homophily, oUt of many graph
properties (e.g., diameter, degree seqUence, degree distribUtion, average clUstering coefficient). First,
the graph property can be compUted efficiently even for large graphs. Second, there shoUld be an
algorithm that can generate graphs by controlling the property of interest only. Third, the property
shoUld be a scalar valUe becaUse if the synthetic graph space is too wide, it woUld be impossible to
condUct an experiment with sUfficient coverage. Average degree and homophily satisfy the above
conditions and are sUitable for oUr experiment, Unlike some of the other graph properties.
RQ4. Does design choice based on RQ3 generalize to real-world datasets? Experiments on
synthetic datasets provide an Understanding of graph attention models’ performance, bUt they are
oversimplified versions of real-world graphs. Can design choice from synthetic datasets be gener-
alized to real-world datasets, considering more complex strUctUres and rich featUres in real-world
graphs? To answer this qUestion, we condUct experiments on 17 real-world datasets with the varioUs
average degree (1.8 - 35.8) and homophily (0.16 - 0.91), and compare them with synthetic graph
experiments in RQ3.
4.1	Datasets
Real-world datasets We Use a total of 17 real-world datasets (Cora, CiteSeer, PUbMed, Cora-ML,
Cora-FUll, DBLP, ogbn-arxiv, CS, Physics, Photo, CompUters, Wiki-CS, FoUr-Univ, Chameleon,
Crocodile, Flickr, and PPI) in diverse domains (citation, co-aUthorship, co-pUrchase, web page, and
biology) and scales (2k - 169k nodes). We try to Use their original settings as mUch as possible. To
verify research qUestions 1 and 2, we choose foUr classic benchmarks: Cora, CiteSeer, PUbMed in
the transdUctive setting, and PPI in the indUctive setting. See appendix A.1 for detailed description,
splits, statistics (inclUding degree and homophily), and references.
Synthetic datasets We generate random partition graphs of n nodes per class and c classes (For-
tUnato, 2010), Using NetworkX library (Hagberg et al., 2008). A random partition graph is a graph
of commUnities controlled by two probabilities pin and pout. If the nodes have the same class labels,
they are connected with pin , and otherwise, they are connected with pout . To generate a graph with
an average degree of d°vg = n ∙ δ, we choose Pin and Pout by Pin + (c - 1) ∙ Pout = δ. The input
featUres of nodes are sampled from overlapping mUlti-GaUssian distribUtions (AbU-El-Haija et al.,
2019). We setn to 500, c to 10, and choose davg between 1 and 100, Pin from {0.1δ, 0.2δ, . . . , 0.9δ}.
We use 20 samples per class for training, 500 for validation and 1000 for test.
4.2	Experimental Set-up
We follow the experimental set-up of GAT with minor adjustments. All parameters are initialized
by Glorot initialization (Glorot & Bengio, 2010) and optimized by Adam (Kingma & Ba, 2014).
We apply L2 regularization, dropout (Srivastava et al., 2014) to features and attention coefficients,
and early stopping on validation loss and accuracy. We use ELU (Clevert et al., 2016) as a non-
linear activation ρ. Unless specified, we employ a two-layer SuperGAT with F = 8 features and
K = 8 attention heads (total 64 features). All models are implemented in PyTorch (Paszke et al.,
2019) and PyTorch Geometric (Fey & Lenssen, 2019). See appendix A.5 for detailed model and
hyperparameter configurations.
5
Published as a conference paper at ICLR 2021
Figure 2: Distribution of KL divergence between normalized attention and label-agreement on all
nodes and layers for Cora dataset (Left: two-layer GAT, Right: four-layer GAT).
Baselines For all datasets, we compare our model against representative graph neural models:
graph convolutional network (GCN) (Kipf & Welling, 2017), GraphSAGE (Hamilton et al., 2017),
and graph attention network (GAT) (VelickoVic et al., 2018). Furthermore, for Cora, CiteSeer, and
PubMed, we choose recent graph neural architectures that learn aggregation coefficients (or dis-
crete structures) oVer edges: constrained graph attention network (CGAT1) (Wang et al., 2019a),
graph learning-conVolutional network (GLCN) (Jiang et al., 2019), learning discrete structure
(LDS) (Franceschi et al., 2019), graph agreement model (GAM) (Stretcu et al., 2019), and Neu-
ralSparse (NS in short) (Zheng et al., 2020). For the PPI, we use CGAT as an additional baseline.
5	Results
This section describes the experimental results which answer the research questions in Section 4. We
include a qualitatiVe analysis of attention, quantitatiVe comparisons on node classification and link
prediction, and recipes of graph attention design. The results for sensitiVity analysis of important
hyper-parameters are in the appendix B.5.
Does graph attention learn label-agreement? GO learns label-agreement better than DP.
We draw box plots of KL diVergence between attention and label agreement distributions of two-
layer and four-layer GAT with GO and DP attention for Cora dataset in Figure 2. We see similar
patterns in other datasets and place their plots in the appendix B.3. At the rightmost of each sub-
figure, we draw the KLD distribution when uniform attention is giVen to all neighborhoods. Note
that the maximum Value of KLD of each node is different since the degree of nodes is different.
Also, the KLD distribution shows a long-tail shape like a degree distribution of real-world graphs.
There are three obserVations regarding distributions of KLD. First, we obserVe that the KLD dis-
tribution of GO attention shows a pattern similar to the uniform attention for all citation datasets.
This implies that trained GO attention is similar to the uniform distribution, which is in line with
preViously reported results in the case of entropy2 (Wang et al., 2019b). Second, KLD Values ofDP
attention tend to be larger than those ofGO attention for the last layer, resulting in bigger long-tails.
This mismatch between the learned distribution ofDP attention and the label agreement distribution
suggests that DP attention does not learn label-agreement in the neighborhood. Third, the deeper the
model (more than two), the larger the KLD Value of DP attention in the last layer. This is because
the Variance of DP attention increases as the layer gets deeper, as explained below in Proposition 1.
Proposition 1. For l + 1th GAT layer, if W and a are independent and identically drawn from
zero-mean uniform distribution with variance σw2 and σa2 respectively, assuming that parameters are
independent to input features hl and elements of hl are independent to each other,
%r[ei+Go] = 2Fl+1σWσ2E(∣∣hlk2) and Var[e jDP] ≥ Fl+" (5E (((hi)>hj)2) + %r((hi)>hj)) (9)
The proof is giVen in the appendix B.1. While the Variance of GO depends on the norm of features
only, the Variance ofDP depends on the expectation of the square of input’s dot-product and Variance
of input’s dot-product. Stacking GAT layers, the more features of i and j correlate with each other,
the larger the input’s dot-product will be. After DP attention is normalized by softmax, which
intensifies the larger Values among them, normalized DP attention attends to only a small portion of
the neighbors and learns a biased representation.
1Since CGAT uses node labels in the loss function, it is difficult to use it in semi-superVised learning. So,
we modify its auxiliary loss for SSL. See appendix A.6 for details.
2https://docs.dgl.ai/en/latest/tutorials/models/1_gnn/9_gat.html
6
Published as a conference paper at ICLR 2021
-2.5	0.0	2.5
Mixing Coeff. (Log)
-2.5	0.0	2.5
Mixing Coeff. (Log)
Figure 3: Test performance on node classification and link prediction for GO and DP attentions
against the mixing coefficient λE. We report accuracy (Cora, CiteSeer, PubMed) and micro f1-score
(PPI) for node classification, and AUC for link prediction.
-2.5	0.0	2.5
Mixing Coeff. (Log)
-2.5	0.0	2.5
Mixing Coeff. (Log)
GAT & Task
GO & Link
DP & Link
GO & Node
DP & Node
3%) Oo .«> .uu< ⅛ωF⅛
0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0	0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0
Avg. Degree (LoglO)	Avg. Degree (LoglO)	Avg. Degree (LoglO)	Avg. Degree (LoglO)
Figure 4: Mean test accuracy gains (of 5 runs) against GATGO on synthetic datasets, varying ho-
mophily and average degree of the input graph.
Is graph attention predictive for edge presence? DP predicts edge presence better than GO.
In Figure 3, we report the mean AUC over multiple runs (5 for PPI and 10 for others) for link
prediction (red lines) and node classification (gray lines). As the mixing coefficient λE increases,
the link prediction score increases in all datasets and attentions. This is a natural result considering
that λE is the weight factor of self-supervised graph attention loss (LE in Equation 6). For three
out of four datasets, DP attention outperforms GO for link prediction for all range of λE in our
experiment. Surprisingly, even for small λE, DP attention shows around 80 AUC, much higher than
the performance of GO attention. PPI is an exception where GO attention shows higher performance
for small λE than DP, but the difference is slight. The results of this experiment demonstrate that
DP attention is more suitable than GO attention in encoding edges.
This figure also includes node classification performance. For all datasets except PubMed, we ob-
serve a trade-off between node classification and link prediction; that is, node classification perfor-
mance decreases in SuperGATGO and SuperGATDP as λE increases and thus link prediction perfor-
mance increases. PubMed also shows a decrease in performance at the largest λE we have tested.
This implies that it is hard to learn the relational importance from edges by simply optimizing graph
attention for link prediction.
Which graph attention should we use for given graphs? It depends on homophily and average
degree of the graph.
In Figure 4, we draw the mean test accuracy gains (over 5 runs) against GATGO as the average
degree increases from 1 to 100, for different values of homophily, on 64 synthetic graphs with
GCN, SuperGATSD, SuperGATMX. See full results from appendix B.2. We define homophily h
as the average ratio of neighbors with the same label as the center node (Pei et al., 2020). That
is h = ∣vV∣ Pi∈v (Pj∈N L(i)=ι(j)/|Ni|), where l(i) is the label of node i. The expectation of
homoPhily for random partition graphs is analytically Ipin/δ, and We just adopt this value to label
the homophily of graphs in Figure 4.
We make the following observations from this figure. First, if the homophily is low (≤ 0.2),
SuperGATSD performs best among models because DP attention tends to focus on a small number
of neighbors. This result empirically confirms what we analytically found in Proposition 1. Second,
even when homophily is low, the performance gain of SuperGAT against GAT increases as the av-
erage degree increases to a certain level (around 10), meaning relation modeling can benefit from
self-supervision if there are sufficiently many edges providing supervision. This is in agreement
with prior study of label noise for deep neural networks where they find that the absolute amount of
data with correct labels affects the learning quality more than the ratio between data with noisy and
correct labels (Rolnick et al., 2017). Third, if the average degree and homophily are high enough,
7
Published as a conference paper at ICLR 2021
Table 1: Summary of classification accuracies of GCN, GraphSAGE, GAT, SuperGATSD , and
SuperGATMX for real-world datasets (30 runs for ogbn-arxiv and Flickr, and 100 runs for others).
Model	ogbn-arxiv	CS	Physics	Cora-ML	Cora-FUU	DBLP	Cham.	Four-Univ	Wiki-CS	Photo	Comp.	Flickr	Croco.
GCN	33.3±i.2	91.5±o.2	92.5±o.2	85.0±o.4	59.5±o.2	77.8±o.5	33.1±o.9	74.8±o.6	74.0±i.o	91.6±o.6	84.5±i.4	51.2±o.4	32.6±o.4
GraPhSAGE 54.6±o.3	9O.O±o.i	92.2±o.i	83.7±o.4	59.2±o.2	78.7±o.6	41.0±o.9	74.4±o.6	77.5 ±0.5	90.4±ι.ι	83.0±i.4	50.7±o.2	53.0±i.o
GAT	54.1±o.5	89.5±o.2	91.2±o.6	83.2±o.6	58.7±o.3	78.2±i.5	40.8±o.7	74.2±o.7	77.6±o.6	91.8±o.6	85.7±o.9	50.9±o.2	53.3±i.o
SuPerGATSD 54∙5±01.3~88.8±0.4	91.6±*o.5	84.5±o.4	55.81o.6	79.4±0.8	41.6±*o.7	76.2±01.8	77.9±o.7	86.8±2.5	82.2±o.g	45.1±-	53.3±o.9
SuPerGATMX 55.1±0.2	90.2±O.2	91.9±O.5	84.7±*o.4	59.6±0.2	80.7±O.7	42.0±O.8	75.3±*o.6	77.9±o.5	91.8±o.9	85.7±i.i	50.81o.2	53.3±o.9
Table 2: Summary of classification accuracies with 100 ran-
dom seeds for Cora, CiteSeer, and PubMed. We mark with
daggers (f) the reprinted results from the respective papers.
Model	Cora	CiteSeer	PubMed
GCNt	81.5	70.3	79.0
GraphSAGE	82.1 ± 0.6	71.9 ± 0.9	78.0 ± 0.7
CGAT	81.4 ± 1.1	70.1 ± 0.9	78.1 ± 1.0
GLCNt	85.5	72.0	78.3
LDSt	84.1	75	-
GCN + GAMt	86.2	73.5	86.0
GCN + NSt	83.7 ± 1.4	74.1 ± 1.4	-
GATt	83.0 ± 0.7	72.5 ± 0.7 一	79.0 ± 0.4 一
SuperGATSD	82.7±0.6	72.5±0,^^	** 81.3±0.5
SuperGATMX	84.3±0.6	72.6±o.8	81.7±0.5
Table 3: Summary of micro f1-
scores with 30 random seeds for
PPI.
Model	PPI
GCN GraphSAGE CGAT	61.5 ± 0.4 59.0 ± 1.2 68.3 ± 1.7
-GAT	72.2 ± 0.6
SUPerGATSD SUPerGATMX	74.4±0.4 67.2±i.2
**	p-value < .0001
*	p-value < .0005
]	Worse than GATGO
Color	Best graph attention (See Fig. 5)
there is no difference between all models, including GCNs. If there are more correct edges beyond
a certain amount, we can learn fine representation without self-supervision. Most importantly, if the
average degree is not too low or high and homophily is above 0.2, SuperGATMX performs better than
or similar to SuperGATSD. This implies that we can take advantage of both GO attention to learn
label-agreement and DP attention to learn edge presence by mixing GO and DP. Note that many of
the real-world graphs belong to this range of graph characteristics.
The results on synthetic graphs imply that understanding of graph domains should be preceded to
design graph attention. That is, by knowing the average degree and homophily of the graphs, we can
choose the optimal graph attention in our design space.
Does design choice based on RQ3 generalize to real-world datasets? It does for 15 of 17 real-
world datasets.
In Figure 5, we plot the best-performed graph attention for synthetic graphs with square points in the
plane of average degree and homophily. The size is the performance gain of SuperGAT against GAT,
and the color indicates the best model. If the difference is not statistically significant (p-value ≥ .05)
between GAT and SuperGAT, and between SuperGATMX and SuperGATSD, we mark as GAT-Any
and SuperGAT-Any, respectively. We call this plot a recipe since it introduces the optimal attention
to a specific region’s graph.
Now we map the results of 17 real-world datasets in Figure 5 according to their average degree and
homophily. Average degree and homophily can be found in the appendix A.1, and experimental
results of graph attention models are summarized in Tables 1, 2 and 3. We report the mean and
standard deviation of performance over multiple seeds (30 for graphs with more than 50k nodes and
100 for others). We put unpaired t-test results of SuperGAT with GATGO with asterisks.
We find that the graph attention recipe based on synthetic experiments can generalize across real-
world graphs. PPI and Four-Univ (N) are surrounded by squares of SuperGATSD () at the bottom
of the plane. Wiki-CS (N), located in the SuperGAT-Any region (), also show no difference in
performance between SuperGATs. Nine datasets (N), which SuperGATMX shows the highest per-
formance, are located in the MX regions () or within the margin of two squares. Note that there are
two MX regions: lower-middle average degree (2.5 - 7.5) and high homophily (0.8 - 0.9), and upper-
middle average degree (7.5 - 50) and lower-middle homophily (0.3 - 0.5). There are five datasets
with no significant performance change across graph attention (N). CiteSeer, Photo, and Computers
are within a margin of one square from the GAT-Any region (); however, Flickr and Crocodile are
8
Published as a conference paper at ICLR 2021
Figure 5: The best-performed graph attention design for synthetic and real-world graphs with various
average degree and homophily.
in the SuperGAT-Any region. To find out the cause of this irregularity, we examine the distribution
of degree and per-node homophily (appendix A.4). We observe a more complex mixture distribution
of homophily and average degree in Flickr and Crocodile, and this seems to be equivalent to mixing
graphs of different characteristics, resulting in inconsistent results with our attention design recipe.
Comparison with baselines For a total of 17 datasets, SuperGAT outperforms GCN for 13
datasets, GAT for 12 datasets, GraphSAGE for 16 datasets. Interestingly, for CS, Physics, Cora-
ML, and Flickr, in which our model performs worse than GCN, GAT also cannot surpass GCN.
It is not yet known when the degree-normalized aggregation of GCN outperforms the attention-
based aggregation, and more research is needed to figure out how to embed the degree information
into graph attention. Tables 2 and 3 show performance comparisons between SuperGAT and re-
cent GNNs for Cora, CiteSeer, PubMed, and PPI. Our model performs better for CiteSeer (0.6%p)
and PubMed (3.4%p) than GLCN, which gives regularization to all relations in a graph. GCN +
NS (NeuralSparse) performs better than our model for CiteSeer (1.5%p) but not for Cora (0.6%p).
CGAT modified for semi-supervised learning shows lower performance than GAT. Although LDS
and GAM which use iterative training show better performance except for LDS on Cora, these mod-
els require significantly more computation for the iterative training. For example, GCN + GAM
compared to our model needs more ×34 more training time for Cora, ×72 for CiteSeer, and ×82
for PubMed. See appendix A.7 and B.4 for the experimental set-up and the result of wall-clock time
analysis.
6 Conclusion
We proposed novel graph neural architecture designs to self-supervise graph attention following the
input graph’s characteristics. We first assessed what graph attention is learning and analyzed the
effect of edge self-supervision to link prediction and node classification performance. This analysis
showed two widely used attention mechanisms (original GAT and dot-product) have difficulty en-
coding label-agreement and edge presence simultaneously. To address this problem, we suggested
several graph attention forms that balance these two factors and argued that graph attention should be
designed depending on the input graph’s average degree and homophily. Our experiments demon-
strated that our graph attention recipe generalizes across various real-world datasets such that the
models designed according to the recipe outperform other baseline models.
Acknowledgments
This research was supported by the Engineering Research Center Program through the Na-
tional Research Foundation of Korea (NRF) funded by the Korean Government MSIT (NRF-
2018R1A5A1059921)
9
Published as a conference paper at ICLR 2021
References
Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew
Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath
Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah,
Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vin-
cent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Watten-
berg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning
on heterogeneous systems, 2015. URL http://tensorflow.org/. Software available from
tensorflow.org.
Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, Kristina Lerman, Hrayr
Harutyunyan, Greg Ver Steeg, and Aram Galstyan. Mixhop: Higher-order graph convolutional
architectures via sparsified neighborhood mixing. In International Conference on Machine Learn-
ing,pp.21-29, 2019.
James Atwood and Don Towsley. Diffusion-convolutional neural networks. In Advances in Neural
Information Processing Systems, pp. 1993-2001, 2016.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In International Conference on Learning Representations, 2015.
Aleksandar Bcjchevski and Stephan Gunnemann. Deep gaussian embedding of graphs: Unsuper-
vised inductive learning via ranking. In International Conference on Learning Representations,
2018.
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally
connected networks on graphs. In International Conference on Learning Representations, 2014.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-
decoder for statistical machine translation. In Proceedings of the 2014 Conference on Em-
pirical Methods in Natural Language Processing (EMNLP), pp. 1724-1734, Doha, Qatar, Oc-
tober 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1179. URL
https://www.aclweb.org/anthology/D14-1179.
Fan Chung. Graph theory in the information age. Notices of the AMS, 57(6):726-732, 2010.
Djork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network
learning by exponential linear units (elus). In International Conference on Learning Representa-
tions, 2016.
Mark Craven, Dan DiPasquo, Dayne Freitag, Andrew McCallum, Tom Mitchell, Kamal Nigam, and
Sean Slattery. Learning to extract symbolic knowledge from the world wide web. In Proceedings
of the Fifteenth National/Tenth Conference on Artificial Intelligence/Innovative Applications of
Artificial Intelligence, AAAI ’98/IAAI ’98, pp. 509-516, 1998.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks
on graphs with fast localized spectral filtering. In Advances in Neural Information Processing
Systems, pp. 3844-3852, 2016.
David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan
Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular
fingerprints. In Advances in Neural Information Processing Systems, pp. 2224-2232, 2015.
Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In
International Conference on Learning Representations Workshop on Representation Learning on
Graphs and Manifolds, 2019.
Santo Fortunato. Community detection in graphs. Physics Reports, 486(3-5):75-174, 2010.
10
Published as a conference paper at ICLR 2021
Luca Franceschi, Mathias Niepert, Massimiliano Pontil, and Xiao He. Learning discrete structures
for graph neural networks. In International Conference on Machine Learning, pp. 1972-1982,
2019.
Hongyang Gao and Shuiwang Ji. Graph representation learning via hard and channel-wise attention
networks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining, pp. 741-749, 2019.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence
and Statistics, pp. 249-256, 2010.
Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings
of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
pp. 855-864, 2016.
Aric Hagberg, Pieter Swart, and Daniel S Chult. Exploring network structure, dynamics, and func-
tion using networkx. Technical report, Los Alamos National Lab.(LANL), Los Alamos, NM
(United States), 2008.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Advances in Neural Information Processing Systems, pp. 1024-1034, 2017.
Mikael Henaff, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured
data. arXiv preprint arXiv:1506.05163, 2015.
Yifan Hou, Jian Zhang, James Cheng, Kaili Ma, Richard TB Ma, Hongzhi Chen, and Ming-Chang
Yang. Measuring and improving the use of graph information in graph neural networks. In
International Conference on Learning Representations, 2020.
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,
and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv
preprint arXiv:2005.00687, 2020a.
Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure
Leskovec. Strategies for pre-training graph neural networks. In International Conference on
Learning Representations, 2020b.
Binyuan Hui, Pengfei Zhu, and Qinghua Hu. Collaborative graph convolutional networks: Unsu-
pervised learning meets semi-supervised learning. In AAAI, pp. 4215-4222, 2020.
Bo Jiang, Ziyan Zhang, Doudou Lin, Jin Tang, and Bin Luo. Semi-supervised learning with graph
learning-convolutional networks. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 11313-11320, 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Thomas N Kipf and Max Welling. Variational graph auto-encoders. Advances in Neural Information
Processing Systems Workshop on Bayesian Deep Learning, 2016.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In International Conference on Learning Representations, 2017.
Johannes Klicpera, Stefan WeiBenberger, and StePhan Gunnemann. Diffusion improves graph learn-
ing. In Advances in Neural Information Processing Systems, pp. 13333-13345, 2019.
Boris Knyazev, Graham W Taylor, and Mohamed Amer. Understanding attention and generalization
in graph neural networks. In Advances in Neural Information Processing Systems, pp. 4204-4214,
2019.
David Liben-Nowell and Jon Kleinberg. The link-prediction problem for social networks. Journal
of the American Society for Information Science and Technology, 58(7):1019-1031, 2007.
11
Published as a conference paper at ICLR 2021
Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-based
neural machine translation. In Empirical Methods in Natural Language Processing, pp. 1412-
1421, 2015.
Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural
network acoustic models. In International conference on Machine learning Workshop on Deep
Learning for Audio, Speech and Language Processing, 2013.
Julian McAuley and Jure Leskovec. Image labeling on a network: using social-network metadata for
image classification. In European conference on computer vision, pp. 828-841. Springer, 2012.
Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. Image-based rec-
ommendations on styles and substitutes. In Proceedings of the 38th international ACM SIGIR
conference on research and development in information retrieval, pp. 43-52, 2015.
Peter Mernyei and Catalina Cangea. Wiki-cs: A WikiPedia-based benchmark for graph neural net-
works. arXiv preprint arXiv:2007.02901, 2020.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of Word represen-
tations in vector space. arXiv preprint arXiv:1301.3781, 2013.
S Pan, R Hu, G Long, J Jiang, L Yao, and C Zhang. Adversarially regularized graph autoencoder
for graph embedding. In International Joint Conference on Artificial Intelligence, 2018.
JiWoong Park, Minsik Lee, Hyung Jin Chang, KyueWang Lee, and Jin Young Choi. Symmetric
graph convolutional autoencoder for unsupervised graph representation learning. In Proceedings
of the IEEE International Conference on Computer Vision, pp. 6519-6528, 2019.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-
performance deep learning library. In Advances in neural information processing systems, pp.
8026-8037, 2019.
Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric
graph convolutional netWorks. In International Conference on Learning Representations, 2020.
David Rolnick, Andreas Veit, Serge Belongie, and Nir Shavit. Deep learning is robust to massive
label noise. arXiv preprint arXiv:1705.10694, 2017.
Benedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding. arXiv
preprint arXiv:1909.13021, 2019.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in netWork data. AI Magazine, 29(3):93-93, 2008.
Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Gunnemann. Pitfalls
of graph neural netWork evaluation. arXiv preprint arXiv:1811.05868, 2018.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple Way to prevent neural netWorks from overfitting. Journal of Machine Learning
Research, 15(1):1929-1958, 2014.
Otilia Stretcu, Krishnamurthy VisWanathan, Dana Movshovitz-Attias, Emmanouil Platanios, Sujith
Ravi, and AndreW Tomkins. Graph agreement models for semi-supervised learning. In Advances
in Neural Information Processing Systems, pp. 8710-8720, 2019.
Aravind Subramanian, Pablo Tamayo, Vamsi K Mootha, Sayan Mukherjee, Benjamin L Ebert,
Michael A Gillette, Amanda Paulovich, Scott L Pomeroy, Todd R Golub, Eric S Lander, et al.
Gene set enrichment analysis: a knoWledge-based approach for interpreting genome-Wide expres-
sion profiles. Proceedings of the National Academy of Sciences, 102(43):15545-15550, 2005.
Ke Sun, Zhouchen Lin, and Zhanxing Zhu. Multi-stage self-supervised learning for graph convolu-
tional netWorks on graphs With feW labeled nodes. In AAAI, pp. 5892-5899, 2020.
12
Published as a conference paper at ICLR 2021
Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-
scale information network embedding. In Proceedings of the 24th International Conference on
World Wide Web, pp. 1067-1077. International World Wide Web Conferences Steering Commit-
tee, 2015.
Kiran K Thekumparampil, Chong Wang, Sewoong Oh, and Li-Jia Li. Attention-based graph neural
network for semi-supervised learning. arXiv preprint arXiv:1803.03735, 2018.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems, pp. 5998-6008, 2017.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In International Conference on Learning Representations,
2018. URL https://openreview.net/forum?id=rJXMpikCZ.
Guangtao Wang, Rex Ying, Jing Huang, and Jure Leskovec. Improving graph attention networks
with large margin-based constraints. arXiv preprint arXiv:1910.11945, 2019a.
Kuansan Wang, Zhihong Shen, Chiyuan Huang, Chieh-Han Wu, Yuxiao Dong, and Anshul Kanakia.
Microsoft academic graph: When experts are not enough. Quantitative Science Studies, 1(1):396-
413, 2020.
Minjie Wang, Lingfan Yu, Da Zheng, Quan Gan, Yu Gai, Zihao Ye, Mufei Li, Jinjing Zhou,
Qi Huang, Chao Ma, Ziyue Huang, Qipeng Guo, Hao Zhang, Haibin Lin, Junbo Zhao, Jinyang Li,
Alexander J Smola, and Zheng Zhang. Deep graph library: Towards efficient and scalable deep
learning on graphs. International Conference on Learning Representations Workshop on Rep-
resentation Learning on Graphs and Manifolds, 2019b. URL https://arxiv.org/abs/
1909.01315.
Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A
comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and
Learning Systems, 2020.
Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning
with graph embeddings. In Proceedings of the 33rd International Conference on International
Conference on Machine Learning-Volume 48, pp. 40-48, 2016.
Yuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen. When does self-supervision help
graph convolutional networks? In International Conference on Machine Learning, 2020.
Youngjae Yu, Jongwook Choi, Yeonhwa Kim, Kyung Yoo, Sang-Hun Lee, and Gunhee Kim. Su-
pervising neural attention models for video captioning by human gaze data. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 490-498, 2017.
Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. Graph-
saint: Graph sampling based inductive learning method. In International Conference on Learning
Representations, 2020. URL https://openreview.net/forum?id=BJe8pkHFwS.
Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung. Gaan: Gated
attention networks for learning on large and spatiotemporal graphs. In Conference on Uncertainty
in Artificial Intelligence, 2018.
Kai Zhang, Yaokang Zhu, Jun Wang, and Jie Zhang. Adaptive structural fingerprints for graph
attention networks. In International Conference on Learning Representations, 2020.
Muhan Zhang and Yixin Chen. Weisfeiler-lehman neural machine for link prediction. In Proceed-
ings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, pp. 575-583, 2017.
Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. In Advances in
Neural Information Processing Systems, pp. 5165-5175, 2018.
13
Published as a conference paper at ICLR 2021
Cheng Zheng, Bo Zong, Wei Cheng, Dongjin Song, Jingchao Ni, Wenchao Yu, Haifeng Chen,
and Wei Wang. Robust graph representation learning via neural sparsification. In International
Conference on Machine Learning, 2020.
Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li,
and Maosong Sun. Graph neural networks: A review of methods and applications. arXiv preprint
arXiv:1812.08434, 2018.
Marinka Zitnik and Jure Leskovec. Predicting multicellular function through multi-layer tissue
networks. Bioinformatics, 33(14):i190-i198, 2017.
14
Published as a conference paper at ICLR 2021
A	Experimental Set-up
A. 1 Real-world Dataset
In this section, we describe details (including nodes, edges, features, labels, and splits) of real-world
datasets. We report statistics of real-world datasets in Tables 4 and 5. For multi-label graphs (PPI),
we extend the homophily to the average of the ratio of shared labels on neighbors over nodes, i.e.,
h = |vV1 Pi∈v (Pj∈N ∣Ci ∩ Cj ∣∕(∣N∕∙ |C|)), where Ci is a set of labels for node i and C is a set
of all labels.
A.1.1 Citation Network
We use a total of 7 citation network datasets. Nodes are documents, and edges are citations. The
task for all citation network datasets is to classify each paper’s topic.
Cora, CiteSeer, PubMed We use three benchmark datasets for semi-supervised node classifica-
tion tasks in the transductive setting (Sen et al., 2008; Yang et al., 2016). The features of the nodes
are bag-of-words representations of documents. We follow the train/validation/test split of previ-
ous work (Kipf & Welling, 2017). We use 20 samples per class for training, 500 samples for the
validation, and 1000 samples for the test.
Cora-ML, Cora-Full, DBLP These are other citation network datasets from Bojchevski &
Gunnemann (2018). Node features are bag-of-words representations of documents. For CoraFull
with features more than 5000, we reduce the dimension to 500 by performing PCA. We use the split
setting in Shchur et al. (2018): 20 samples per class for training, 30 samples per class for validation,
the rest for the test.
ogbn-arxiv The ogbn-arxiv is a recently proposed large-scale dataset of citation networks (Hu
et al., 2020a; Wang et al., 2020). Nodes represent arXiv papers, and edges indicate citations be-
tween papers, and node features are mean vectors of skip-gram word embeddings of their titles and
abstracts. We use the public split by publication dates provided by the original paper.
A.1.2 Co-author Network
CS, Physics The CS and Physics are co-author networks in each domain (Shchur et al., 2018).
Nodes are authors, and edges mean whether two authors co-authored a paper. Node features are
paper keywords from the author’s papers, and we reduce the original dimension (6805 and 8415) to
500 using PCA. The split is the 20-per-class/30-per-class/rest from Shchur et al. (2018). The goal
of this task is to classify each author’s respective field of study.
A.1.3 Amazon Co-purchase
Photo, Computers The Photo and Computers are parts of the Amazon co-purchase
graph (McAuley et al., 2015; Shchur et al., 2018). Nodes are goods, and edges indicate whether
two goods are frequently purchased together, and node features are a bag-of-words representation of
product reviews. The split is the 20-per-class/30-per-class/rest from Shchur et al. (2018). The task
is to classify the categories of goods.
A.1.4 Web Page Network
Wiki-CS The Wiki-CS dataset is computer science related page networks in Wikipedia (Mernyei
& Cangea, 2020). Nodes represent articles about computer science, and edges represent hyperlinks
between articles. The features of nodes are mean vectors of GloVe word embeddings of articles.
There are 20 standard splits, and we experiment with five random seeds for each split (total 100
runs). The task is to classify the main category of articles.
Chameleon, Crocodile These datasets are Wikipedia page networks about specific topics,
Chameleon and Crocodile (Rozemberczki et al., 2019). Nodes are articles, and edges are mutual
15
Published as a conference paper at ICLR 2021
Table 4: Average degree and homophily of real-world graphs.
Dataset	Degree	Homophily
Four-Univ	1.83 ± 1.71 ^^	0.16
PPI	28.0 ± 39.26	0.17
Chameleon	15.85 ± 18.20	0.21
Crocodile	15.48 ± 15.97	0.26
Flickr	10.08 ± 31.75	0.32
Cora-Full	6.41 ± 8.79	0.59
ogbn-arxiv	7.68 ± 9.05	0.63
Wiki-CS	26.40 ± 36.04	0.68
CiteSeer	2.78 ± 3.39	0.72
PubMed	4.50 ± 7.43	0.79
Cora-ML	5.45 ± 8.24	0.81
DBLP	5.97 ± 9.35	0.81
Computers	35.76 ± 70.31	0.81
Cora	3.90 ± 5.23	0.83
CS	8.93 ± 9.11	0.83
Photo	31.13 ± 47.27	0.85
Physics	14.38 ± 15.57	0.91
links between them. Node features are a bag-of-words representation with informative nouns in the
article. The number of features is 13183, but we use a reduced dimension of 500 by PCA. The split
is 20-per-class/30-per-class/rest from Shchur et al. (2018). The original dataset is for the regression
task to predict monthly traffic, but we group values into six bins and make it a classification problem.
Four-Univ The Four-Univ dataset is a web page networks from computer science departments
of diverse universities (Craven et al., 1998). Nodes are web pages, edges are hyperlinks between
them, and node features are TF-IDF vectors of web page’s contents. There are five graphs consists
of four universities (Cornell, Texas, Washington, and Wisconsin) and a miscellaneous graph from
other universities. As the original authors suggested3, we use three graphs of universities and a
miscellaneous graph for training, another one graph for validation (Cornell), and the other one graph
for the test (Texas). Classification labels are types of web pages (student, faculty, staff, department,
course, and project).
A.1.5 Flickr
The Flickr dataset is a graph of images from Flickr (McAuley & Leskovec, 2012; Zeng et al., 2020).
Nodes are images, and edges indicate whether two images share common properties such as geo-
graphic location, gallery, and users commented. Node features are a bag-of-words representation of
images. We use labels and split in in Zeng et al. (2020). For labels, they construct seven classes by
manually merging 81 image tags.
A.1.6 Protein-Protein Interaction
The protein-protein interaction (PPI) dataset (Zitnik & Leskovec, 2017; Hamilton et al., 2017; Sub-
ramanian et al., 2005) is a well-known benchmark in the inductive setting. A graph is given for
human tissue, the nodes are proteins, the node’s features are biological signatures like genes, and
the edges illustrate proteins’ interactions. The dataset consists of 20 training graphs, two validation
graphs, and two test graphs. This dataset has multi-labels of gene ontology sets.
A.2 Link Prediction
For link prediction, we split 5% and 10% of edges for validation and test set, respectively. We fix
the negative edges for the test set and sample negative edges for the training set at each iteration.
3http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/
16
Published as a conference paper at ICLR 2021
Table 5: Statistics of the real-world datasets.
Dataset	# Nodes	# Edges	# Features	# Classes	Split	# Training Nodes	# Val. Nodes	# Test Nodes
Four-Univ	4518	3426	2000	6	fixed	4014 (3 Gs)	248 (1 G)	256 (1 G)
PPI	56944	818716	50	121	fixed	44906 (20 Gs)	6514 (2 Gs)	5524 (2 Gs)
Chameleon	2277	36101	500	6	random	120	180	1977
Crocodile	11631	180020	500	6	random	120	180	11331
Flickr	89250	449878	500	7	fixed	44625	22312	22313
Cora-Full	19793	63421	500	70	random	1395	2049	16349
ogbn-arxiv	169343	1166243	128	40	fixed	90941	29799	48603
Wiki-CS	11701	297110	300	10	fixed	580	1769	5847
CiteSeer	3327	4732	3703	6	fixed	120	500	1000
PubMed	19717	44338	500	3	fixed	60	500	1000
Cora-ML	2995	8158	2879	7	random	140	210	2645
DBLP	17716	52867	1639	4	random	80	120	17516
Computers	13752	245861	767	10	random	200	300	13252
Cora	2708	5429	1433	7	fixed	140	500	1000
CS	18333	81894	500	15	random	300	450	17583
Photo	7650	119081	745	8	random	160	240	7250
Physics	34493	247962	500	5	random	100	150	34243
A.3 Synthetic Dataset
To the best of our knowledge, our synthetic datasets are not used in recent literature. Therefore,
we give some small examples of synthetic datasets to see qualitatively how the average degree and
homophily vary according to δ and pin . Specifically, we draw 2D t-SNE plot of node features and
edges in Figure 6. In this figure, We can observe that the average degree (davg = n ∙ δ) increases as
δ increases, and homoPhily (h = pin∕δ) increases as Pin increases. Note that these are raw input
features sampled from the 2D Gaussian distribution, not learned node representation. We use the
code from the prior work4 (Abu-El-Haija et al., 2019) and apply normalization by standard score.
We choose δ from {0.025, 0.2}, pin from {0.1δ, 0.5δ, 0.9δ}, and fix n = 100 and c = 5.
Figure 6: t-SNE plots of node features and edges for synthetic graph examples. Hyperparameters
are δ ∈ {0.025 (Top), 0.2 (Bottom)} and pin ∈ {0.1δ (Left), 0.5δ (Center), 0.9δ (Right)}.
A.4 Distribution of Degree and Homophily of Datasets
In Figure 7, we draw kernel density estimation plots of per-node homophily and degree of nodes
in real-world datasets. We define per-node homophily as the ratio of neighbors with the same label
as the center node, that is, hi = Pj∈N 1l(i)=l(j) |Ni|. Note that we define homophily as h =
由 Pi∈V hi in Section 5.
4https://github.com/samihaija/mixhop/blob/master/data/synthetic/make_x.
py
17
Published as a conference paper at ICLR 2021
Figure 7: Kernel density estimate plot of distribution of degree and per-node homophily in real-
world graphs.
We focus more on the part outside of where the degree is 1 (0 with log scale), and the per-node
homophily is 0. These are leaf nodes incorrectly connected and does not significantly affect the
learning overall graph representation. In most datasets, only the largest mode exists, or there are
some small modes around it. However, in Flickr and Crocodile, we can observe that the interval
between modes is wide. More specifically, Crocodile’s modes are in the area of (high degree, low
per-node homophily) and (low degree, high per-node homophily), and Flickr’s modes cover most
homophily at a specific degree. Note that we can regard a mixture of distribution as a mixture of
different sub-graphs. We argue that this is the reason our recipe does not fit for these two datasets.
A.5 Model & Hyperparameter Configurations
Model Since we experiment with numbers of datasets, we maintain almost the same configurations
across datasets. We do not use other methods such as residual connections, deeper layers, batch
normalization, edge augmentation, and more hidden features, although we have confirmed from
previous studies that these techniques contribute to performance improvement. For example, prior
18
Published as a conference paper at ICLR 2021
Table 6: Hyperparameters for experiments on real-world datasets.
Dataset	Model	λ2	λE	pe	pn
Cora	SuperGATSD	0.007829935945	10.88266937	0.8	0.5
	SuperGATMX	0.008228864973	11.34657453	0.8	0.5
CiteSeer	SuperGATSD	0.04823808657^^	0.09073992828	0.8	0.3
	SuperGATMX	0.04161321832	0.01308169273	0.8	0.5
PubMed	SuperGATSD	0.0002030927563	18.82560333	0.6	0.7
	SuperGATMX	2.19E-04	10.4520518	0.6	0.5
PPI	SuperGATSD	3.39E-07	0.001034351842	1	0.5
	SuperGATMX	1.00E-07	1.79E-06	1	0.5
Table 7: Summary of classification accuracies with 10 random seeds for Cora, CiteSeer, and PubMed
in the full-supervised setting. We mark with asterisks the reprinted results from the respective papers.
Model	Cora	CiteSeer	PubMed
GAT*	87.2 ± 0.3	77.3 ± 0.3	87.0 ± 0.3
CGAT*	88.2 ± 0.3	78.9 ± 0.2	87.4 ± 0.3
CGAT (Our Impl.: 2-layer w/ 64-features)	88.9 ± 0.3	78.9 ± 0.2	86.9 ± 0.2
SUPerGATMX (2-layer w/ 64-features)	88.7 ± 0.2	79.1 ± 0.2	87.0 ± 0.1
work has shown f1-score close to 100 for PPI. To clearly see the difference between the various
graph attention designs, we intentionally keep a simple model configuration.
Hyperparameter For real-world datasets, we tune two hyperparameters (mixing coefficients λ2
and λE) by Bayesian optimization for the mean performance of3 random seeds. We choose negative
sampling ratio pn from {0.3, 0.5, 0.7, 0.9}, and edge sampling ratio pe from {0.6, 0.8, 1.0}. We fix
dropout probability to 0.0 for PPI, 0.2 for ogbn-arxiv, 0.6 for others. We set learning rate to 0.05
(ogbn-arxiv), 0.01 (PubMed, PPI, Wiki-CS, Photo, Computers, CS, Physics, Crocodile, Cora-Full,
DBLP), 0.005 (Cora, CiteSeer, Cora-ML, Chameleon), 0.001 (Four-Univ). For ogbn-arxiv, we set
the number of features per head to 16 and the number of heads in the last layer to one; otherwise,
we use eight features per head and eight heads in the last layer.
For synthetic datasets, we choose λE from {10-5, 10-4, 10-3, 10-2, 10-1, 1, 10, 102} and λ2 from
{10-7, 10-5, 10-3}. We fix learning rate to 0.01, dropout probability to 0.2, pn to 0.5, and, pe to
0.8 for all synthetic graphs.
Table 6 describes hyperparameters for SuperGAT on four real-world datasets. For other datasets and
experiments, please see the code (./SuperGAT/args.yaml).
A.6 CGAT Implementation
CGAT (Wang et al., 2019a) has two auxiliary losses: graph structure based constraint Lg and class
boundary constraint Lb . We borrow their notation for this section: V as a set of nodes, Ni as a set of
one-hop neighbors, Ni+ as a set of neighbors that share labels, Ni- as a set of neighbors that do not
share labels, Z∙ as a margin between attention values, and φ (Vi ,vk) as unnormalized attention value.
Lg=	max (0, φ(vi, vk) + ζg - φ (vi, vj))	(10)
i∈V j三Ni∖N- k∈(V∖Ni)
Lb=	max(0,φ(vi,vk) + ζb - φ(vi,vj))	(11)
i∈V j∈Ni+ k∈Ni-
Since label information is included in these two losses, they are difficult to use in semi-supervised
settings that provide few labeled samples. In fact, in the CGAT paper, they conduct experiments in
full-supervised settings; that is, they use all nodes in training except validation and test nodes.
19
Published as a conference paper at ICLR 2021
So, we only use Lg modified for semi-supervised learning.
LgSSL=XX X
max(0,φ(vi,vk) + ζg - φ(vi,vj))	(12)
i∈V j∈Ni k∈(V∖Ni)
With Lc, the multi-class cross-entropy on node labels, CGAT’s optimization objective is
L=Lc+λgLg+λbLb,	(13)
and our modified CGAT’s loss is
L = Lc + λgLgSSL.	(14)
In addition to losses, CGAT proposes top-k softmax and node importance based negative sampling
(NINS). Top-k softmax picks up nodes with top-k attention values among neighbors. NINS adopts
importance sampling when choosing negative sample nodes.
Since the code for CGAT has not been released, we implement our own version. In all experiments
in our paper, we use only the modified losses (Equation 14) and top-k softmax due to the training and
implementation complexity of NINS. For PPI, even if we do not assume a semi-supervised setting,
we use the same loss because we could not accurately implement multi-label cases for Equation 10
and 11 with only CGAT’s description.
To verify the functionality of our implementation, we report the results of a full-supervised setting
with the original loss (Equation 13) like CGAT paper, in Table 7. Our implementation of CGAT
shows almost the same performance reported in the original paper. In addition, SuperGAT and
CGAT showed almost similar performance in a full-supervised setting. Note that the original paper
employs two hidden layers with hidden dimensions as 32 for Cora, 64 for CiteSeer, and three hidden
layers with hidden dimensions 32 for PubMed, where models in our experiments are all two-layer
with 64 features.
A.7 Wall-clock Time Experimental Set-up
To demonstrate our model’s efficiency, we measure the mean wall-clock time of the entire training
process of three runs using a single GPU (GeForce GTX 1080Ti). We compare our model with
GAT (Velickovic et al., 2018) and GAM (StretcU et al., 2019). GAT is the basic model using a
simpler attention mechanism than ours, and GAM is the state-of-the-art model using co-training
with the auxiliary model.
For GAT and SuperGAT, we use our implementation (including hyperparameter settings) in Py-
Torch (Paszke et al., 2019). For GAM, we adopt the code in TensorFlow (Abadi et al., 2015) from
the authors 5 and choose GCN + GAM model, which showed the best performance. We retain the
default settings in the code but use the hyperparameters reported in the paper, if possible. With this
setting, GCN + GAM on PubMed is not finished after 24 hours; therefore, we manually early-stop
the training at the best accuracy.
5https://github.com/tensorflow/neural-structured-learning/tree/master/
research/gam
20
Published as a conference paper at ICLR 2021
B	Results
B.1 Proof of Proposition
Proposition 2. For l + 1th GAT layer, if W and a are independent and identically drawn from
zero-mean uniform distribution with variance σw2 and σa2 respectively, assuming that parameters are
independent to input features hl and elements of hl are independent to each other,
Var[ejGO]=2Fl+1σWσ2E(∣∣hlk2) and Var[ejDP] ≥ Fl+1σW (4E (((hi)>hj)2) + Var((hi)>hj))
(15)
l+1
Proof. Let h0 = Whl, then h0i,k = Pr=1 Wkrhli
Note that,
eli+j,G1 O = a> [h0i kh0j] and eli+j,D1 P = h0i>h0j	(16)
First, we compute E(a2) and E(h02).	
E(a2) = Var(a) + E(a)2 = σa2	(17)
E(hk2) = E ((PF=+1 Wkrhl,r)2)	(18)
=E (PF=+1 Wkr(hl,r )2)	(19)
=E (WnE (PF=+1 (hl,r)2)	(20)
=σWE (khlk2)	(21)
For the variance of eli+j,G1O,	
Var(eli+j,G1O) = Var(a>[h0ikh0j])	(22)
= Var PrF=l+11 (ar h0i,r + ar+F l+1 h0j,r)	(23)
= 2Fl+1Var(ah0)	(24)
=2Fl+1(E (a2) E (h02) - E (a)2 E (h0)2)	(25)
=2Fl+1E (a2) E (h02)	(26)
=2Fl+1σ2σWE (Mlk2)	(27)
Now we compute E(h0ih0j ) and E(h0i2 h0j2 ),	
E(h0i,k h0j,k) = E ((Pr=1 Wkr hli,r ) (Pr=1 Wkr hlj,r ))	(28)
= E (Pr=1 Wk2r hli,r hlj,r )	(29)
=E(Wt)E (PF=+1 hi,rhj,r)	(30)
= σw2 E((hli)>hlj )	(31)
21
Published as a conference paper at ICLR 2021
E(hi2k HG	(32)
=E ((p31 Wkrhl,J2 (p31 Wkrhj,J2)	(33)
(Fl + 1	∖	(Fl + 1	∖
=E(W4,∙)E I X (% hj,r )21 + E(W2)2E I X (%%)2 +2(% %)(%%) I
(34)
E(W4,∙)E (X (hi,r hj,r )2) +E(W2)2E (χ(hi,r )2 £(%『-£(必,hj,r )2
∖ r=1	/	∖ r	r	r
(35)
+ 2E(Wk2,J2E ((X hi,, hjj	- XH,r hj,r)2
9	F l + 1
9，4E (X (hi,,hj,,)2
∖ r=1
F	Fl + 1
+ σWE Mik2khjk2 - 3 X (hi,rhj,r)2
∖	r=1
(36)
+ 2σWE (((hi)Thj)2)
(37)
/	6 Fl+1
σWE Fhik2khjk2 - ʒ E(hi,rhj,r)2 +2((hi)Thj)2
σWE	4((hi)τhj)2 +』X ((hi,shj,t + hi,thj,s)2 + 8(hi,shj,t)2)
∖	s=t
(38)
+ 鬲E (((hi)Thj)2)
(39)
Note that for the zero-mean uniform distribution U(-u, u) with variance σW,
E(W∙2∙) = Var(W.,.)+ E(W,)2 = σww	(40)
σW = *(U - (-u))2 = 1u	(41)
4
.	1 二 .	1 .	9 .
E(W∙4.) = - X(-u)ru4-r = -u4 = -σW	(42)
5	—	—
r=0
For the variance of ej D p，
Var 卜 jDP)	(43)
=Var (hiτhj)	(44)
=Var (PF=+1 hi,rhj,r)	(45)
=Fl+1Var (hihj)	(46)
=Fl+1 (E((hihj)2) - E(hihj)2)	(47)
=Fl+1σWE (-((hi)Thj)2 + -0 X ((hi,shj,t + hi,thj,s)2 + 8(hi,shj,t)2)|	(48)
+ Fl+1σW (E (((hi)τhj)2) - E((hi)τhj)2)	(49)
≥ Fl+1σW (4E (((hi)τhj)2) + Var((hi)Thj))	(50)
□
22
Published as a conference paper at ICLR 2021
Table 8: Summary of classification accuracies (of 5 runs) for synthetic datasets.
Avg. degree	HomoPhily
GCN	0.1	0.2	0.3	0.4	0.5	0.6	0.7	0.8	0.9
1	32.7 ± 1.1	35.1 ± 0.4	34.7 ± 0.4	40.8 ± 0.4	43.4 ± 0.3	46.2 ± 0.5	49.7 ± 0.5	54.8 ± 0.9	56.2 ± 0.3
1.5	28.7 ± 0.5	30.0 ± 0.6	34.6 ± 0.4	40.8 ± 0.4	41.7 ± 0.4	46.9 ± 0.5	52.1 ± 0.6	54.5 ± 0.5	63.6 ± 0.5
2.5	24.6 ± 0.2	27.9 ± 0.9	31.0 ± 0.3	39.7 ± 0.6	44.0 ± 0.6	51.4 ± 0.8	56.6 ± 0.5	65.2 ± 0.6	74.5 ± 0.4
3.5	25.8 ± 0.8	30.5 ± 0.8	31.9 ±0.7	38.4 ± 0.4	45.0 ± 1.0	49.8 ± 0.5	60.8 ± 0.7	70.3 ± 0.3	79.5 ± 0.3
5	25.0 ± 0.3	26.3 ± 0.5	33.0 ± 1.1	39.8 ± 0.7	50.2 ± 0.5	58.6 ± 1.1	68.8 ± 0.3	78.0 ± 1.9	88.5 ± 0.2
7.5	26.1 ± 0.5	29.8 ± 0.6	34.0 ± 0.6	43.8 ± 0.9	52.0 ± 0.4	70.7 ± 3.2	74.6 ± 2.4	88.6 ± 0.8	95.0 ± 0.2
10	25.4 ± 0.6	29.2 ± 0.5	37.1 ± 0.5	47.6 ± 1.0	58.1 ± 0.9	73.1 ± 2.8	85.3 ± 2.6	92.7 ± 1.1	97.9 ± 0.5
12.5	24.0 ± 0.4	27.6 ± 0.5	39.6 ± 0.8	48.9 ± 0.4	65.8 ± 3.1	77.8 ± 2.4	88.5 ± 1.5	95.0 ± 0.9	99.2 ± 0.0
15	22.1 ± 0.5	28.2 ± 0.6	44.0 ± 0.7	53.0 ± 0.5	69.9 ± 4.7	79.1 ± 2.0	92.2 ± 1.8	98.0 ± 0.4	99.6 ± 0.3
20	24.3 ± 0.6	30.9 ± 0.7	41.9 ± 0.9	58.1 ± 1.2	75.0 ± 2.9	86.7 ± 1.1	96.1 ± 0.3	98.9 ± 0.1	99.8 ± 0.1
25	26.6 ± 1.0	31.1 ± 0.3	44.9 ± 0.2	62.4 ± 0.7	80.1 ± 2.6	91.0 ± 1.5	97.5 ± 0.5	99.5 ± 0.2	100.0 ± 0.0
32.5	26.3 ± 0.7	33.8 ± 0.7	51.8 ± 0.8	67.5 ± 2.9	83.5 ± 1.9	95.7 ± 0.3	98.4 ± 0.2	100.0 ± 0.0	100.0 ± 0.0
40	23.7 ± 0.5	34.2 ± 0.5	53.4 ± 0.4	72.8 ± 1.5	87.6 ± 0.9	96.1 ± 0.6	99.7 ± 0.1	99.9 ± 0.0	100.0 ± 0.0
50	25.0 ± 1.1	36.4 ± 1.0	55.1 ± 0.5	82.7 ± 3.0	91.2 ± 0.7	98.6 ± 0.4	99.8 ± 0.0	99.9 ± 0.0	100.0 ± 0.0
75	25.9 ± 0.6	40.6 ± 0.6	68.0 ± 1.0	87.9 ± 2.6	97.1 ± 0.6	99.6 ± 0.1	100.0 ± 0.0	100.0 ± 0.0	100.0 ± 0.0
100	25.1 ± 0.5	42.0 ± 0.9	72.4 ± 1.4	92.6 ± 0.4	98.6 ± 0.2	100.0 ± 0.0	100.0 ± 0.0	100.0 ± 0.0	100.0 ± 0.0
GATGO	0.1	0.2	0.3	0.4	T5	0.6	0.7	0.8	0.9
1	33.7 ± 0.9	36.7 ± 0.8	35.2 ± 1.2	40.1 ± 0.7	43.0 ± 0.8	46.3 ± 0.6	49.4 ± 0.5	53.8 ± 1.4	56.1 ± 0.5
1.5	28.8 ± 1.0	33.0 ± 0.5	34.8 ± 0.9	40.7 ± 0.4	42.2 ± 0.9	46.7 ± 1.0	52.2 ± 0.8	53.6 ± 1.4	62.8 ± 0.5
2.5	28.8 ± 1.2	30.6 ± 0.6	32.3 ± 0.5	40.1 ± 0.8	44.0 ± 1.3	52.2 ± 0.6	55.5 ± 1.4	64.6 ± 1.3	73.1 ± 0.6
3.5	28.0 ± 0.4	33.4 ± 1.6	33.5 ± 0.6	40.1 ± 1.0	46.2 ± 0.6	50.8 ± 0.6	62.5 ± 0.6	71.7 ± 0.6	78.0 ± 0.5
5	28.0 ± 1.3	30.0 ± 0.8	36.9 ± 0.6	42.1 ± 0.4	53.6 ± 0.7	61.0 ± 1.1	70.4 ± 0.5	78.7 ± 1.2	87.5 ± 0.6
7.5	30.0 ± 0.9	31.6 ± 2.0	40.4 ± 0.5	45.0 ± 1.0	59.5 ± 0.6	72.3 ± 1.0	79.4 ± 0.6	90.4 ± 0.5	95.0 ± 0.5
10	31.0 ± 1.4	34.0 ± 1.2	43.1 ± 0.6	54.6 ± 0.5	66.1 ± 0.6	77.4 ± 1.5	90.6 ± 1.4	94.7 ± 0.7	98.4 ± 0.5
12.5	29.8 ± 1.4	35.5 ± 1.7	47.3 ± 0.9	58.6 ± 1.7	72.1 ± 1.4	86.3 ± 0.5	90.7 ± 0.7	96.2 ± 0.3	98.9 ± 0.2
15	31.9 ± 1.6	34.6 ± 1.6	49.5 ± 0.9	62.0 ± 0.7	77.2 ± 1.5	85.9 ± 0.3	94.4 ± 0.9	98.5 ± 0.5	99.4 ± 0.1
20	34.4 ± 1.8	38.3 ± 1.6	54.1 ± 1.9	70.0 ± 1.1	83.9 ± 1.0	93.7 ± 0.1	97.5 ± 0.3	99.0 ± 0.3	99.7 ± 0.0
25	35.8 ± 2.0	42.0 ± 2.4	57.7 ± 0.6	77.7 ± 0.9	87.9 ± 1.2	95.0 ± 0.7	98.7 ± 0.6	99.6 ± 0.3	99.9 ± 0.1
32.5	37.4 ± 1.2	44.7 ± 1.1	66.5 ± 1.9	79.9 ± 1.2	91.4 ± 1.4	98.0 ± 0.5	99.0 ± 0.3	99.8 ± 0.1	100.0 ± 0.0
40	37.5 ± 2.0	45.1 ± 0.9	66.5 ± 1.1	85.7 ± 1.5	93.5 ± 1.0	97.6 ± 0.6	99.5 ± 0.1	99.9 ± 0.1	99.9 ± 0.1
50	38.7 ± 1.8	49.5 ± 2.3	68.9 ± 2.5	89.5 ± 0.8	96.0 ± 0.8	99.0 ± 0.4	99.7 ± 0.2	99.8 ± 0.1	100.0 ± 0.0
75	39.6 ± 3.0	53.5 ± 1.7	77.6 ± 2.6	92.8 ± 2.3	98.0 ± 1.2	99.5 ± 0.3	99.8 ± 0.2	100.0 ± 0.0	100.0 ± 0.0
100	41.3 ± 1.2	56.6 ± 1.4	81.1 ± 3.5	95.9 ± 1.2	99.0 ± 0.4	99.8 ± 0.1	99.9 ± 0.1	100.0 ± 0.1	100.0 ± 0.0
SuPerGATSD	0.1	0.2	0.3	0.4	T5	0.6	0.7	0.8	0.9
1	38.5 ± 1.0	40.5 ± 1.5	39.5 ± 1.6	42.3 ± 0.9	44.0 ± 0.5	47.1 ± 0.5	50.1 ± 1.0	53.0 ± 0.3	55.1 ± 1.0
1.5	36.7 ± 1.2	37.8 ± 1.8	42.0 ± 0.4	42.9 ± 0.8	45.0 ± 0.7	47.8 ± 0.5	52.9 ± 0.7	54.7 ± 1.2	63.0 ± 0.4
2.5	35.7 ± 1.8	37.3 ± 1.6	39.2 ± 1.4	47.8 ± 1.3	49.0 ± 0.3	56.1 ± 0.5	59.4 ± 0.4	65.9 ± 0.8	70.6 ± 1.2
3.5	35.7 ± 1.0	39.2 ± 1.9	41.4 ± 0.5	44.0 ± 0.7	51.0 ± 0.3	55.0 ± 1.2	65.1 ± 1.1	73.5 ± 1.0	78.1 ± 0.6
5	37.0 ± 1.5	39.0 ± 1.5	45.0 ± 1.7	48.1 ± 1.0	57.5 ± 0.6	66.1 ± 0.9	74.4 ± 0.2	81.0 ± 1.0	88.3 ± 0.8
7.5	36.8 ± 1.5	38.7 ± 0.7	47.3 ± 0.5	51.1 ± 0.8	61.2 ± 0.6	75.5 ± 0.5	82.3 ± 0.9	91.0 ± 0.2	95.2 ± 0.3
10	39.4 ± 1.1	42.3 ± 0.7	50.5 ± 1.2	58.6 ± 0.3	70.0 ± 0.5	80.1 ± 0.5	90.8 ± 0.3	95.5 ± 0.6	98.7 ± 0.3
12.5	38.5 ± 1.0	42.0 ± 0.7	50.5 ± 0.6	62.4 ± 0.5	73.6 ± 0.4	86.8 ± 0.4	92.9 ± 0.4	98.0 ± 0.6	99.3 ± 0.3
15	40.4 ± 1.1	42.8 ± 0.7	53.6 ± 0.3	67.4 ± 0.4	79.7 ± 0.3	89.0 ± 1.0	96.3 ± 0.3	99.1 ± 0.2	99.6 ± 0.3
20	37.8 ± 0.8	42.1 ± 0.9	56.9 ± 0.9	70.0 ± 0.5	86.1 ± 0.8	95.6 ± 0.3	99.1 ± 0.1	99.6 ± 0.2	99.7 ± 0.1
25	40.0 ± 1.0	48.8 ± 1.2	59.5 ± 0.5	78.7 ± 0.2	90.7 ± 0.2	97.5 ± 0.1	99.4 ± 0.1	100.0 ± 0.0	99.9 ± 0.1
32.5	39.7 ± 1.1	48.5 ± 0.7	69.4 ± 0.4	82.7 ± 0.5	94.3 ± 0.2	99.3 ± 0.1	99.7 ± 0.1	99.9 ± 0.1	99.9 ± 0.0
40	44.2 ± 1.1	48.7 ± 1.3	69.2 ± 0.7	88.3 ± 0.4	97.1 ± 0.1	99.7 ± 0.1	99.9 ± 0.0	99.8 ± 0.2	100.0 ± 0.0
50	44.3 ± 0.7	53.2 ± 0.6	73.4 ± 1.2	91.2 ± 0.4	97.7 ± 0.2	100.0 ± 0.0	100.0 ± 0.0	100.0 ± 0.0	100.0 ± 0.0
75	44.8 ± 0.7	56.1 ± 0.9	82.7 ± 0.6	95.7 ± 0.2	99.8 ± 0.2	99.9 ± 0.1	100.0 ± 0.0	100.0 ± 0.0	100.0 ± 0.0
100	43.1 ± 1.3	60.7 ± 0.7	87.4 ± 0.3	98.3 ± 0.1	99.9 ± 0.0	100.0 ± 0.0	100.0 ± 0.0	100.0 ± 0.0	100.0 ± 0.0
SuPerGATMX	0.1	0.2	0.3	0.4	T5	0.6	0.7	0.8	0.9
1	33.4 ± 0.7	36.3 ± 0.8	35.4 ± 0.7	40.5 ± 0.3	43.4 ± 0.6	47.0 ± 0.8	50.0 ± 0.5	54.2 ± 1.1	56.4 ± 0.4
1.5	28.8 ± 0.7	31.5 ± 1.0	36.0 ± 0.8	41.1 ± 1.1	41.3 ± 0.6	47.5 ± 0.8	52.3 ± 0.7	54.8 ± 0.5	63.4 ± 0.3
2.5	26.6 ± 0.9	30.5 ± 1.3	33.6 ± 0.8	42.7 ± 1.5	44.7 ± 0.5	52.2 ± 0.5	56.5 ± 0.8	66.7 ± 0.8	74.5 ± 0.7
3.5	27.7 ± 0.9	33.9 ± 1.7	37.9 ± 0.7	42.4 ± 0.6	49.7 ± 0.8	54.2 ± 1.1	65.3 ± 1.1	73.2 ± 0.7	79.4 ± 1.5
5	28.4 ± 2.1	33.4 ± 0.8	40.9 ± 0.8	46.2 ± 0.4	56.4 ± 0.2	64.1 ± 1.8	74.6 ± 1.4	83.1 ± 1.0	90.6 ± 0.9
7.5	31.1 ± 0.7	35.0 ± 0.9	46.5 ± 1.1	51.9 ±0.7	64.3 ± 1.5	75.2 ± 1.6	82.1 ± 1.3	92.6 ± 1.8	95.8 ± 0.2
10	32.6 ± 1.7	41.5 ± 1.4	48.9 ± 1.2	61.7 ± 1.1	70.4 ± 1.3	81.3 ± 1.1	91.3 ± 0.7	95.1 ± 0.8	99.2 ± 0.3
12.5	34.2 ± 3.1	41.1 ± 0.8	53.3 ± 1.0	65.9 ± 1.1	77.2 ± 1.4	88.1 ± 0.7	92.6 ± 2.6	97.5 ± 0.9	99.3 ± 0.2
15	36.4 ± 1.3	39.9 ± 2.1	53.8 ± 1.5	68.5 ± 0.8	81.0 ± 0.4	90.2 ± 1.0	96.1 ± 0.9	99.2 ± 0.2	99.6 ± 0.1
20	35.2 ± 3.1	41.0 ± 2.1	62.3 ± 1.2	73.9 ± 0.9	86.3 ± 0.6	95.2 ± 1.3	98.6 ± 0.7	99.6 ± 0.1	99.9 ± 0.2
25	36.8 ± 1.9	46.6 ± 1.7	62.7 ± 1.1	81.9 ± 1.7	90.7 ± 0.8	96.8 ± 1.5	99.3 ± 0.3	99.8 ± 0.2	100.0 ± 0.0
32.5	36.7 ± 0.8	46.8 ± 0.5	68.9 ± 0.6	83.8 ± 0.7	93.7 ± 1.0	98.4 ± 0.7	99.8 ± 0.1	100.0 ± 0.0	100.0 ± 0.0
40	39.8 ± 2.4	48.8 ± 1.7	72.1 ± 0.8	87.5 ± 1.7	96.6 ± 0.8	98.9 ± 0.3	99.9 ± 0.1	100.0 ± 0.0	100.0 ± 0.0
50	42.1 ± 1.3	53.6 ± 2.3	75.1 ± 1.9	92.7 ± 1.0	97.1 ± 0.9	99.6 ± 0.2	100.0 ± 0.0	100.0 ± 0.0	100.0 ± 0.0
75	41.9 ± 1.1	55.9 ± 2.4	80.8 ± 1.2	95.0 ± 1.1	99.5 ± 0.2	99.9 ± 0.1	100.0 ± 0.0	100.0 ± 0.0	100.0 ± 0.0
100	39.6 ± 2.0	59.2 ± 1.8	84.2 ± 3.1	96.9 ± 0.7	99.7 ± 0.1	100.0 ± 0.0	100.0 ± 0.0	100.0 ± 0.0	100.0 ± 0.0
B.2	Full Result of Synthetic Graph Experiments
In Table 8, we rePort all results of synthetic graPh exPeriments. We exPeriment on a total of 144
synthetic graPhs controlling 9 homoPhily (0.1, 0.2, ..., 0.9) and 16 average degree (1, 1.5, 2.5, 3.5,
5, 7.5, 10, 12.5, 15, 20, 25, 32.5, 40, 50, 75, 100).
23
Published as a conference paper at ICLR 2021
GO-Iayer-I G0-layer-2 DP-Iayer-I DP-layer-2 Uniform
Attention Type (Cora)
GO-Iayer-I G0-layer-2 GO-Iayer-B G0-layer-4 DP-Iayer-I DP-layer-2 DP-layer-3 DP-layer-4 Uniform
Attention Type (Cora)
GO-Iayer-I G0-∣ayer-2 DP-Iayer-I DP-layer-2 Uniform
Attention Type (CiteSeer)
GO-Iayer-I G0-layer-2 G0-layer-3 G0-layer-4 DP-Iayer-I DP-layer-2 DP-layer-3 DP-Iayer-4 Uniform
Attention Type (CiteSeer)
sɪ5
510
3 5
O
GO-Iayer-I GO-Iayer-2 DP-Iayer-I DP-∣ayer-2 Uniform
Attention Type (PubMed)
GO-Iayer-I G0-layer-2 GO-Iayer-B GO-Iayer-4 DP-Iayer-I DP-layer-2 DP-layer-3 DP-layer-4 Uniform
Attention Type (PubMed)
20
0 15
EIo
m 5
O
sω< b⅛ A≡
Figure 8: Distribution of KL divergence between normalized attention and label-agreement on all
nodes and layers for Cora, CiteSeer, PubMed, and PPI (Left: two-layer GAT, Right: four-layer
GAT).
Table 9: Mean wall-clock time (seconds) of three runs of the training process on real-world datasets.

Model	Cora	CiteSeer	PubMed
GAT	11.3 ± 2.7	20.4 ± 6.7	21.1 ± 2.0
GCN + GAM	709.3 ± 235.9	1099.3 ± 812.5	6923.3 ± 7042.0
SuperGATMX	30.8 ± 0.5	19.3 ± 1.1	151.4 ± 11.4
SuperGATMX+ MPNS	20.8 ± 0.2	15.2 ± 0.1	84.6 ± 0.9
B.3	Label-agreement Study for Other Datasets and Deeper Models
In Figure 8, we draw box plots of KL divergence between attention distribution and label agreement
distribution for all nodes and layers of two-layer GATs and four-layer GATs. As shown in the paper,
we can see that DP attention does not capture label-agreement rather than GO attention. Also, the
degree of this phenomenon becomes stronger as the layer goes down.
B.4	Wall-clock Time Result
In Table 9, we report the mean wall-clock time (over three runs) of the training of GAT, GAM,
and SuperGATMX. In SuperGAT, we find that negative sampling of edges is the bottleneck of train-
ing. So, we additionally implement SuperGATMX+ MPNS, which employs multi-processing when
sampling negative edges. There are three observations in this experiment. GCN + GAM is highly
time-intensive in the training stage (×53.9 - ×328.1 versus GAT) for all datasets. Compared to
GAT, our model needs ×2.7 more training time for Cora and ×7.2 for PubMed, and we reduce the
time by applying multi-processing to negative sampling (×1.8 for Cora and ×4.0 for PubMed). For
CiteSeer, we can see that SuperGATMX ends faster than GAT because of faster convergence and
fewer epochs.
24
Published as a conference paper at ICLR 2021
Figure 9: Test performance on node classification against the mixing coefficient λE for
SuperGATMX (Cora, CiteSeer, PubMed) and SuperGATSD (PPI).
0.85
lθ.84
⅛
(υ
I-
0.83
0	2	4	0	2	4
Negative Sampling Ratio (Cora) Negative Sampling Ratio (CiteSeer)
082	-----------------------1-	0.750
.七"d⅛8J.
0.78
0	1	2	3	4	5	0	1	2
Negative Sampling Ratio (PubMed)	Negative Sampling Ratio (PPI)
Figure 11: Test performance on node classification against the edge sampling ratio pe for
SuperGATMX (Cora, CiteSeer, PubMed) and SuperGATSD (PPI).
Figure 10: Test performance on node classification against the negative sampling ratio pn for
SuperGATMX (Cora, CiteSeer, PubMed) and SuperGATSD (PPI).
B.5	Sensitivity Analysis of Hyper-parameters
We analyze sensitivity of mixing coefficient of losses λE , negative sampling ratio pn , and edge
sampling ratio pe . We plot mean node classification performance (over 5 runs) against each hyper-
parameter in Figure 9, 10, and 11 respectively. We use the best model for each dataset: SuperGATMX
for citation networks and SuperGATSD for PPI.
For λE , there is a specific range that maximizes test performance in all datasets. Performance on
PPI is the largest when λE is 10-3, but the difference is relatively small comparing to others. We
observe that there is an optimal level of the edge supervision for each dataset, and using too large
λE degrades node classification performance.
For pn , using too many negative samples has been shown to decrease performance. The optimal
number of negative samples is different for each dataset, and all are less than the number of positive
samples (pn < 1.0). Note that as pn increases, the required GPU memory also increases. When
pn = 5.0, the model and data for PPI could not be accommodated by one single GPU (GeForce
GTX 1080Ti).
When pe changes, the performance also changes, but the pattern is different by datasets. For Cora
and PubMed, the performance against pe shows the convex curve. Performance for CiteSeer gen-
erally decreases as pe increases, but there are intervals the performance change of which is nearly
zero. In the case of PPI, there are no noticeable changes against pe .
25