Published as a conference paper at ICLR 2021
TropEx: An Algorithm for Extracting Linear
Terms in Deep Neural Networks
Martin Trimmel* 1, Henning Petzka*1, Cristian Sminchisescu1,2
1 Lund University 2Google Research
{martin.trimmel, henning.petzka, cristian.sminchisescu}@math.lth.se
Ab stract
Deep neural networks with rectified linear (ReLU) activations are piecewise linear
functions, where hyperplanes partition the input space into an astronomically high
number of linear regions. Previous work focused on counting linear regions to
measure the network’s expressive power and on analyzing geometric properties
of the hyperplane configurations. In contrast, we aim to understand the impact of
the linear terms on network performance, by examining the information encoded
in their coefficients. To this end, we derive TropEx, a non-trivial tropical algebra-
inspired algorithm to systematically extract linear terms based on data. Applied to
convolutional and fully-connected networks, our algorithm uncovers significant
differences in how the different networks utilize linear regions for generalization.
This underlines the importance of systematic linear term exploration, to better
understand generalization in neural networks trained with complex data sets.
1	Introduction
Many of the most widely used neural network architectures, including VGG (Simonyan & Zisserman,
2015), GoogLeNet (Szegedy et al., 2015) and ResNet (He et al., 2016), make use of rectified linear
activations (ReLU, (Hahnloser et al., 2000; Glorot et al., 2011), i.e., σ(x) = max{x, 0}) and are
therefore piecewise linear functions. Despite the apparent simplicity of these functions, there is a
lack of theoretical understanding of the factors that contribute to the success of such architectures.
Previous attempts of understanding piecewise linear network functions have focused on estimating
the number of linear terms, which are the linear pieces (affine functions) that constitute the network
function. A linear region is being defined as a maximally connected subset of the input space on
which the network function is linear. Since computing the exact number of linear regions is intractable,
work has focused on obtaining upper and lower bounds for this number (Arora et al., 2016; Serra
et al., 2018; PascanU et al., 2013; RaghU et al., 2017; Montufar et al., 20l4; Montufar, 2017; Xiong
et al., 2020; Zhang et al., 2018). To our knowledge, the currently best upper and lower bounds were
calcUlated by Serra et al. (2018). RaghU et al. (2017) show these boUnds to be asymptotically tight.
All of the mentioned papers share the intUition that the nUmber of linear regions of neUral networks
measUres their expressivity. Since the boUnds grow linearly in width and exponentially in depth,
deep networks are interpreted to have greater representational power. However, these boUnds are
staggeringly high: the Upper boUnd on the nUmber of linear regions in (Serra et al., 2018) exceeds
10300 even for the smallest networks we experimented on. (There are approximately 1080 atoms
in the Universe.) For slightly larger networks, the Upper boUnd exceeds 1017000 whereas the lower
boUnd exceeds 1083 linear regions. The nUmber of training samples is generally mUch smaller than
the estimated nUmber of linear regions (≤ 106), so that almost none of the linear regions contains
training data. This raises the qUestion of how representative the nUmber of linear regions is for
network performance and how information extracted from training samples passes on to the many
linear regions free of data for sUccessfUl generalization to test data.
There are indications that a high nUmber of linear regions is not reqUired for good network perfor-
mance. Frankle & Carbin (2019) point oUt that smaller networks perform similarly well as large ones,
when a sUitable initialization of the smaller network can be foUnd from training the larger one. Hence,
* Denotes equal contribution.
1
Published as a conference paper at ICLR 2021
Figure 1: A ReLU network function before (left) and after (right) extraction. Left : Hyperplanes
separate the input space into linear regions. Most of them do not contain any data points. Each data
point occupies its own linear region. Right: After extraction, the function remains unchanged on the
linear regions of training samples. Test samples now fall into regions of training samples.
the expressivity of the large network is helpful to explore the parameter space, but the small, less
expressive network is sufficient to achieve high accuracy. Lee et al. (2019) and Croce et al. (2018)
modify the training loss to encourage larger linear regions with the goal of robustness to adversarial
attacks. Hanin & Rolnick (2019b;a) argue that in practice there are fewer linear regions than expected
from the bounds and empirically investigate this for the MNIST data set. All these observations
question the explanatory power of astronomically high bounds for the number of linear regions. More
recently, the focus of research on linear regions has been shifting away from pure counting towards
an understanding of the linear regions themselves. Zhang & Wu (2020) study geometric properties
of linear regions and notice that batch normalization and dropout, albeit leading to similar network
accuracies, produce differently looking linear regions.
Our approach to the understanding of linear regions differs in that it investigates the linear coefficients
of linear regions. To this end, we propose TropEx, a tropical algebra-based algorithm extracting
linear terms of the network function N (Figure 1) using a data set X. TropEx outputs an extracted
function N(X) containing only the linear terms corresponding to regions on which data lies.
As a result, N and N(X ) agree on neighbourhoods of all data points. This creates a tool for the study
of generalization from a new viewpoint, i.e., the perspective of linear regions and their coefficients.
Our contributions are as follows:
•	A new computational framework representing tropical functions (Definition B.4) as matrices to
efficiently perform tropical calculations appearing in networks with rectified linear activations.
•	This framework allows us to derive TropEx, an algorithm to systematically extract linear terms
from piecewise linear network functions.1
•	An application of TropEx to fully-connected (FCN) and convolutional networks (CNN) reveals
that (i) consistently all training and test samples fall into different linear regions; (ii) Simple tasks
(MNIST) can be solved with the few linear regions of training samples alone, while this does not
hold for more complex data sets. (iii) FCNs and CNNs differ in how they use linear regions free of
training data for their performance on test data: Several measures illustrate that CNNs, in contrast
to FCNs, tend to learn more diverse linear terms. (iv) We confirm that the number of linear regions
alone is not a good indicator for network performance and show that the coefficients of linear regions
contain information on architecture and classification performance.
2	Background and Overview
It was recently shown by Charisopoulos & Maragos (2018); Zhang et al. (2018) that ReLU neural
network functions are the same as tropical rational maps. Tropical rational maps are exactly those
functions where each entry in the output vector can be written as a difference of maxima
Ni(x) = max{ai+1(x), . . . ,ai+n(x)} - max{ai-1(x), . . . ,ai-m(x)},	(1)
1Link to open source implementation: https://github.com/martrim/tropex
2
Published as a conference paper at ICLR 2021
where each ai+j , ai-j : Rd → R is an affine function with only positive coefficients, taking the form
x 7→ Pj wjxj +w0 with all wj ∈ R≥0. Since the number of terms in (1) dwarfs the number of atoms
in the universe, it is impossible to obtain this expression in practice. Therefore, we only extract those
terms that correspond to linear regions of data points. For a fixed data point x ∈ X, the maximum of
the network outputs can be written as maxi Ni (x) = ax+(x) - ax-(x), where ax+, ax- are the affine
functions such that ax+(x) ≥ ai+j(x), ax-(x) ≥ ai-j (x) for all i, j. TropEx extracts ax+ and ax-. The
extracted terms can be used to construct a tropical map N(X) (x) = (N(X) (x),∙∙∙, NsX) (x)) With
maximally enlarged linear regions, given by
Ni(X)(x) = max{ax+k1 (x), . . . , ax+kD (x)} - max{ax-k1 (x), . . ., ax-kD (x)},	(2)
Where there are Di data points xk1 , . . . , xkD given label i by the original netWork. Being a tropical
rational map, the function N (X) is again a ReLU neural netWork function by Zhang et al. (2018). The
maximal entries of the tWo output vectors (hence also the assigned labels) of the extracted function
N(X) and the original netWork N agree in the neighbourhood of any data point x ∈ X.
We discuss the basics of tropical algebra in Appendix B.1 and refer to Maclagan & Sturmfels (2015)
for a detailed introduction. The relation of tropical geometry and ReLU netWorks is studied in Zhang
et al. (2018); Charisopoulos & Maragos (2018); Alfarra et al. (2021).
3	Method
3.1	Matrix Representation of Tropical Rational Maps
If one Were to represent tropical rational maps symbolically on a computer, computations Would be
too sloW. Therefore, We present tropical rational maps as multi-dimensional arrays.
Definition 3.1. Given an affine function a : Rd0 → R; x 7→ Pk wkxk + w0, We Will call the vector
(w0, w1, . . . , wd0 ) its coefficient vector, the scalar w0 its constant part and the vector (w1, . . . , wd0 )
its variable part.
We can represent functions Ni : Rd0 → R as in equation (1) in the folloWing Way: Let the
roWs of the matrix Ai+ ∈ Rn×d0 and the vector ai+ ∈ Rn×1 be the variable and the constant
parts of the affine functions ai+j , respectively. (Analogously for Ai- and ai- .) We can then define
(Ai+, ai+)(x) = max{Ai+x + ai+ }, Where the maximum is taken over the roWs of the resulting
column vector. IfWe define the formal quotient2 of matrix-vector pairs by (Ai+, ai+)/(Ai-, ai-)(x) =
max{Ai+x + ai+} - max{Ai-x + ai-}, thenNi(x) = (Ai+, ai+)/(Ai-, ai-)(x), giving us a matrix-
representation of the function Ni . An entire netWork function With s output dimensions can then be
represented by a list ((A+, a+)/(A-,a-))1≤i≤s.
The advantage of the proposed matrix representation of tropical rational maps are natural oper-
ations performing calculations that arise for (concatenations of) layers of neural netWorks (see
supplements). A dense layer ` : Rd1 → Rd2 With ReLU activation is represented as a list
(Ai+ , ai+ )/(Ai- , ai- ) 1≤i≤d . Denoting by Wpos and Wneg the positive and negative part of a
matrix W, respectively, i.e. wipjos = max{wij , 0} and winjeg = max{-wij, 0}, the matrix representa-
tion of a single neuron n^(x) = max{w ∙ X + b, 0} is given by
Ai+ =	wwnpeogs	,	ai+	=	bbnpeogs	;	Ai-	=	wneg,	ai-	=	bneg.
3.2	Extracting Linear Terms of a Classification Network
We noW consider a classification neural netWork N With s labels. We shoW that We can represent the
netWork N With a matrix-vector pair (A-, a-) in the denominator that is constant over all output
dimensions. The proof is given in Section C.2 of the supplementary material.
2This is in line With the tropical algebra notation: a tropical quotient is the same as a usual difference.
3
Published as a conference paper at ICLR 2021
Algorithm 3.1 TropEx: Extracting Linear Terms of a Neural Network
Inputs: Neural Network N
Data set X = {(xik , i)} with Di points of label i
Output: Extracted Function N(X) = ((Ai+, ai+)/(A-, a-))1≤i≤s
1:	W, b J weight matrix and bias vector of last layer '
2:	C-, C- J column sums of Wneg, bneg
3:	C+ JW+C-,c+ Jb+c-
4:	for i = 1 to s do
5:	A+ j rep(C+∙, Di), a+ J rep(c+, Di)	. Repetition Di times along the rows.
6:	A- J rep(C-, D), a- J rep(c-, D)	. D = total no of data points
7:	Amax J maxima of the columns of all Ai+ and A- stacked
8:	for last layer ` in Nnot yet used do
9:	(N(X) , Amax) J merge` (N(X) , Amax) according to Table 1
Lemma 3.2. Let N : Rd → Rs be the function of a ReLU neural network for classification with s
output neurons. Then there are affine functions ai+j , aj- such that
(max{a+i (X),...,a+nι (X)}∖
N(X)=	；	I — max{a-(x), ...,am(x)},	(3)
max{as+1(X), . . ., as+ns (X)}
where the maxima of the ai- (X) on the right is subtracted from each entry of the vector on
the left.3 In terms of our matrices the classification network N with s labels can be repre-
sented by a list ((Ai+, ai+)/(A-, a-))1≤i≤s of matrix-vector pairs. The label is then given by
argmax1≤i≤s (Ai+, ai+)(X).
How to get the extracted function N(X) of (2) from the network N. TropEx extracts, for each
data point Xk of label i, the affine functions ai+j and al- from the network representation in (3)
such that a+ (Xk) ≥ a+(xk),a-(xk) ≥ aj(xk) for all i, j, I. We start with the last layer of the
network and inductively merge new layers into the existing matrix pairs. The merge operations
depend on the type of the layer as shown in Table 1. Putting things together gives Algorithm 3.1,
which has the extracted function N(X) as its output. The run-time and storage complexities per data
point correspond to 3 forward passes through the network. Theorem 3.3 states that TropEx indeed
results in a selection of linear terms based on a data set of points and that the extracted tropical
function agrees with the network on neighbourhoods of all these points. Its proof and the complete,
non-trivial derivation of the algorithm are in the appendix, where we develop a framework that
enables calculations on tropical matrices that correspond to manipulations of the tropical functions
they represent. For illustrative purposes, we also present a worked-out example of applying TropEx
to a toy neural network there.
Theorem 3.3. Let N = (N1, . . . , Ns) : Rd → Rs be the function of a ReLU neural network
for classification into s classes. Let N(X) be the network obtained from Algorithm 3.1, applied
to N using a data set X = {(xkj , i)| 1 ≤ i ≤ s, 1 ≤ j ≤ Di} of Di data points xkj given
label i by N. (There are D points x1 , . . . , xD in total). Then, (1) for all labels i, Ni(X) (x) =
max{ai+1(x), . . . , ai+D (x)} - max{a1-(x), . . . , a-D (x)}, where the ai+j and al- are extracted from a
representation ofNi as in Equation 3; and (2) every data point xk has a neighbourhood Uk on which
the maximum of the extracted function agrees with the maximal network output:
(X)
max Ni	(x) = max Ni(x) for all x ∈ Uk.
1≤i≤s	1≤i≤s
In particular, N(X) andN classify all points in Uk by assigning the same label.
3This implies that every ReLU neural network classifier can be represented by a convex function.
4
Published as a conference paper at ICLR 2021
Type	Operation	Type	Operation
BNorm	γ, β, μ, σ, e J Batchnorm parameters S J— γ/√σ2 + e, t J— —μ ∙ S + β A J s A; a J a + At AmaX J- |S | AmaX		Maxpool	A, Amax J repeat to input shape of ' Ak j set 0 according to activations of '(xk)
Conv	F, b J filter, bias of ' K J COnvTranS(Amax, Fneg) Amax J K + COnvTranS(Amax, Fpos) a J a + Ab A J K + COnvTrans (A, F)		Dense	W, b J weights, bias of ' K J AmaxWneg Amax J K + AmaxWPoS a J a + Ab A J K + AW	
Flatten	A, Amax J reshape to input shape of '	L-ReLU	a J Leaky ReLU parameter Ak J α ∙ Ak if '(xk j ≤ 0
ReLU	Ak J 0 if '(xk j =0		
Table 1: Merge operations (N(X), Amax) 7→ (N(X), Amax), according to type of layer `. Ak denotes
the slice of A corresponding to data point k. ' denotes all of the network UP to and including '. Read
expressions like A — K + aW as A+ — K + A+W; A- - K + A-W for all i.
4	Experiments
TropEx extracts a function containing only linear terms corresponding to regions on which the given
data lies. The extracted function agrees with the network on this data. This allows us to compare
linear regions of train and test data, to separate the network structure from the information contained
in the linear coefficients, and to test how well the linear coefficients generalize to test data.
Setup We train neural networks on MNIST (LeCun et al., 2010) and CIFAR10 (Krizhevsky, 2009).
After training, we use training data points x(tr) to extract linear terms ax+(tr) (x) and ax-(tr) (x) and
construct an extracted function N (X) as in equation (2). For some experiments, we also extract linear
terms ax+(te) (x) and ax-(te) (x) corresponding to test data points x(te). Regarding the architecture, we
use fully-connected networks, AllCNN-C from Springenberg et al. (2015) and variations of VGG-B
from Simonyan & Zisserman (2015). Section D in the appendix summarizes all architectures we used
in our experiments. If not stated otherwise, we use architectures Conv for CNNs and FCN8 for FCNs
in our experiments. It is not our goal to train networks to state-of-the-art performance, but rather
to compare variations of simple networks which are composed of the layers shown in Table 1. All
layers have ReLU activations except for the last layer where we apply a softmax output function into
the ten respective classes.4 We train five networks of each architecture to ensure the consistency of
our results. Further details on the training setup can be found in the appendix.
Train and Test Linear Regions At first, we investigate how training and test samples are dis-
tributed over the linear regions of the neural networks. For each data point x, let ax = ax+ - ax-
be the function corresponding to the linear region on which x lies. Observing that ax 6= ax0 for all
training and test points x, x0, we see that all points lie in different linear regions.5 This is not a result
of overfitting during training: All data points also occupy different regions when we check the linear
regions after 1, 3, 5, 10, 15, 20, 30, 40, and 50 epochs of training on CIFAR10, and from epochs 1 to
20 on MNIST. Therefore we conclude that generalization capabilities of neural networks cannot be
explained by test samples falling into the same linear region as training samples (or, in other words,
by test samples inducing the same activation pattern as training samples).
Examining Function Coefficients With test samples falling into different regions than training
samples, it is conceivable that neighboring regions could still be “nearly identical” and test samples
would fall into such neighboring regions of training samples. To clarify, we ran experiments
to test the similarity of linear regions for test samples before and after reduction. We take a
test sample and extract the linear coefficient of its linear region in both the original network N
and in the extracted function N(X) , where X is the training data. For these linear coefficient
4We also experimented with replacing ReLU with Leaky ReLU. Our observations are in line with what we
describe for ReLU. More details can be found in the appendix, Section E.3.
5Except for the small 2-layer architecture, where the data points lie on 59,850 regions instead of 60,000.
5
Published as a conference paper at ICLR 2021
Name	N Vs N (X)	N vs true	N(X) vs true
FCN MNIST	97.7±0.2	98.1±0.2	97.6±0.2
CNN MNIST	95.7±0.3	99.2±0.1	95.8±0.2
FCN CIFAR10	52.5±3.5	49.2±0.7	38.2±1.5
CNN CIFAR10	30.8±1.3	71.1±0.5	30.3±1.4
Table 2: Results on test data after extraction of linear terms. Column 1: Agreement of network N
with extracted function N(X). Columns 2&3: Multi-class accuracy for N and N(X). All values are
averages over 5 runs and over each of the architectures in table 3 in section D of the appendix.
vectors, we calculate the (i) angle and (ii) Euclidean norm difference. Figure 2 shows both values
for all test samples, where we differentiate between those test points x(te) that get correctly
classified by N(X) (in blue) and those that get a wrong label (in red). We observe a clear
difference between the CNN and the FCN. For CNNs, the coefficient vectors of both training
and test affine functions are all close to orthogonal for correctly as well as incorrectly classified
points, For FCNs, the angle and distance of correctly classified points are smaller than for
incorrectly classified ones, but still far away from zero and therefore rule out the possibility of
test samples falling into very similar neighboring regions. Finally, instead of comparing linear
coefficients, we also tested the similarity in activation patterns before and after extraction. The
results in section E.5 of the appendix show that in each layer approx 80% of neuron activations
agree between test and training region, showing that also the activation patterns of test samples
deviate considerably and generalization cannot be simply explained by very similar activation patterns.
0
400
300
200
100
Convolutional Network
5001--------------1------------1——
S ①uuel∞α u(ŋ① P=Un 山
ττ7⅛-------;-------7T7⅛-
Angles
Fully Connected Network
Figure 2: Angle and distance between coefficients of the linear regions of test data and training data
used for correct (blue) and incorrect (red) classification by the extracted function. All angles for
CNNs (left) are close to orthogonal, while FCNs (right) shows clear correlations between angles,
distances and correctness of prediction.
Accuracy of the Extracted Functions As predicted by Theorem 3.3, the maximum of the extracted
function N(X) agrees with the maximum of the original network on all training points for each of our
networks. In particular, network and extracted function assign the same label to each training point.
To investigate how well the coefficients of these linear regions generalize to unseen data, we compare
test accuracy of network and extracted function in Table 2. We see a consistent difference between
CNNs and FCNs across both data sets and all architectures: There is a drastic drop in the test accuracy
of the CNNs, as opposed to a relatively small drop in the accuracy of the FCNs. Interestingly, for
MNIST, the extracted tropical function has almost the same test accuracy as the original network.
This is surprising as all known bounds on the number of linear regions of the original network suggest
numbers of the order of 1080 up to over 1017000 from which we only observe 60.000 after reduction.
Hence, for fully-connected networks on a simple task, the coefficients used on training data generalize
well to test data, but for complex data, the learned coefficients generalize worse. This is remarkable,
since previous studies (Hanin & Rolnick, 2019b;a; Zhang & Wu, 2020) of linear regions were forced
to base their experiments on small data sets (or small networks) for computational reasons, and it
seems that care must be taken when generalizing observations to more complex tasks. Moreover, the
results reveal another difference between FCNs and CNNs that we further investigate.
6
Published as a conference paper at ICLR 2021
Figure 3: Left: Comparison of average test accuracy (dotted red) and agreement with labels assigned
by the original network (blue) for networks Narrow and Wide while being transformed into tropical
functions. The letters D,M,C denote dense, maxpooling and convolutional layers, respectively. Right:
Average test accuracy for all CIFAR networks while being transformed into a tropical function.
Fully-connected: dotted blue lines, CNNs: full red lines. The curves show a significant difference
between the network types with FCN performance being more stable to extraction of linear regions.
Transformation Process Figure 3, left shows how the test accuracy and agreement with the original
network develop during the transformation from full network N to its extracted form N(X) . Starting
with the last layer, TropEx iteratively merges layers into an extracted function. The x-axis shows
which layers have been merged to an extracted tropical function at each step. The graph shows
accuracy values for passing test samples through the original network until the layer that represents
the input to the extracted tropical function and then applying this extracted function. At the right end
of the plots, all layers have been merged to the tropical function N(X). There is a clear difference
between fully-connected networks and CNNs, which is consistent over all networks (Figure 3, right).
Number of linear regions The fully-connected network Wide and the CNN Narrow have the same
number of nodes after each parameter layer. Since Narrow has only few connections between its
nodes and Wide is fully-connected, it is reasonable to assume that the number of linear regions of
Wide is greater than the one of Narrow as its theoretical upper bound is higher. Hence, it would be
expected that extraction of a fixed number of linear terms resulted in a smaller change of results for an
initially worse performing Narrow, but the drop in test accuracy for Narrow is almost 5 times the drop
for Wide (36.6% vs 8.1%). An estimate of the number of linear regions in practice (Appendix E.8)
further suggests that Narrow has more linear regions than Wide, both being astronomically high.
This all contradicts our intuition about how CNNs and FCNs work from the perspective of network
expressivity in terms of bounding the maximal number of linear regions.
Network Training We compare the performance of extracted functions N(X ) during the training
of the network N. Figure 4 displays the test accuracy (mean and standard deviation over 5 networks)
of the extracted function and the agreement of label assignments of extracted form with the original
network function for Narrow (CNN) and Wide (fully-connected) for several epochs. The difference
between fully-connected and convolutional networks is here even more striking. For the CNN, the
agreement between the extracted tropical function and the original network function falls rapidly after
only one epoch and only slightly reduces from there. For the FCN, the agreement decreases slowly
over the entire 50 training epochs and it never reaches a value as low as the CNN after its first epoch.
Information encoded in linear coefficients The extracted functions all share the same number of
linear terms, hence their difference in performance must be explained by the coefficient values. With
this in mind, we attempt an interpretation of the above results and hypothesize that the difference lies
in how FCN and CNN store important information for the classification task in the coefficients of
linear regions. An FCN has the full freedom to compose weights to tailored coefficients of linear
regions, whereas CNNs impose a structure on the weight space by filters and weight sharing, which
results in the incapability to compose tailored linear coefficients for correct label assignments. Instead,
the structural properties of convolutions play a significant role in generalization, which we remove by
extracting linear terms. This changes the outcome on test data as the coefficients of linear regions
7
Published as a conference paper at ICLR 2021
alone are limited in meaning. As training progresses, to achieve higher accuracy, the FCN reduces
the information stored in linear coefficients and also learns to use some structure, so that the removal
of this structure could explain the decrease in performance of the extracted function. An experiment,
where we visually inspect misclassified images (Appendix E.2) is in line with this interpretation
suggesting that the object shape is encoded in the linear coefficients of the FCNs, but for CNNs only
simple features such as background color are encoded in the linear coefficients of linear regions. We
visualize coefficients in E.7 to further support observed differences.
Figure 4: Mean and standard deviation of the performance of extracted tropical function during
training over 5 networks. The performance of the CNN network function suffers strongly from
extraction early in training, whereas the FCN shows a slow, gradual decline.
Re-training the network The observation
of smaller angles for FCNs in Figure 2 further
supports our interpretation that coefficient
values of linear regions play a larger role in
classification for FCNs, since smaller angles
together with small Euclidean distance are ex-
plained by a similarity of the coefficient val-
ues. This suggests to also compare the sim-
ilarity of linear coefficients after re-training
the network in order to further understand the
information encoded in the linear coefficients.
Again, we find that the coefficient vectors of
two separately trained CNNs are close to or-
thogonal, whereas both angles and distances
are considerably smaller for FCNs.6 Plots
are shown in Appendix E.6.
For each dimension, we additionally compute
the Pearson correlation between the linear co-
efficients of two separately trained networks
over all training samples. We reduce the re-
sulting vector to a single number by averag-
Figure 5: Pearson correlation between the linear coef-
ficients of two separately trained networks averaged
over all input dimensions. Black dots indicate a reduc-
tion of the learning rate by a factor of 10. Coefficients
are more correlated after re-training for FCNs than
for CNNs, suggesting that FCNs encode more infor-
mation in the coefficients of linear regions than CNNs
ing the correlation factors over the dimensions. We experiment with two pairs of FCNs and CNNs
trained on CIFAR10 on MNIST. Figure 5 shows the evolution of the correlation during training,
confirming that the similarity of coefficient values is also larger for FCNs than for CNNs if measured
by correlation. The correlation of linear coefficients after re-training and convergence for the FCNs is
significant. Interestingly, for the networks trained on CIFAR10, we notice jumps in the correlation
values precisely when the learning rates get decreased.
6We are comparing linear coefficients of the full network function instead of weight vectors. Whereas
symmetries in the parameterization of a network function make comparisons of weight vectors complicated, the
comparison of linear coefficients is well-defined.
8
Published as a conference paper at ICLR 2021
5	Conclusion
The function of a ReLU network is piecewise linear, with an astronomically high number of linear
regions. We introduced TropEx, an algorithm to systematically extract linear regions based on data
points. The derivation is based on a matrix representation of tropical functions that supports efficient
algorithmic development. TropEx enables investigations of the linear components of piecewise linear
network functions: By extracting the networks’ linear terms, the algorithm allows us to compare
training and test regions and to systematically analyze their linear coefficients. Applying TropEx to
fully-connected and convolutional architectures shows significant differences between linear regions
of CNNs and FCNs. Other possible use cases are outlined in Appendix G. Our findings indicate a
potential benefit of shifting focus from counting linear regions to an understanding of their interplay,
as differences between CNNs and FCNs may be found in the coefficients of the extracted linear terms.
Several measures of similarity indicate that the linear terms of CNNs are more diverse than those of
FCNs and suggest that CNNs efficiently exploit the structure imposed by their architecture, whereas
FCNs rely on encoding information in the values of linear coefficients.
Acknowledgements
This work was supported in part by the European Research Council Consolidator grant SEED,
CNCSUEFISCDI PN-III-PCCF-2016-0180, Swedish Foundation for Strategic Research (SSF) Smart
Systems Program, as well as the Wallenberg AI, Autonomous Systems and Software Program (WASP)
funded by the Knut and Alice Wallenberg Foundation.
References
Motasem Alfarra, Adel Bibi, Hasan Abed Al Kader Hammoud, Mohamed Gaafar, and Bernard
Ghanem. On the decision boundaries of neural networks: A tropical geometry perspective. arXiv
preprint arXiv:2002.08838v2, 2021.
Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep neural
networks with rectified linear units. In Proceedings of the 4th International Conference on Learning
Representations, 2016.
Mikhail Belkin, Daniel J Hsu, and Partha Mitra. Overfitting or perfect fitting? risk bounds for
classification and regression rules that interpolate. In Advances in Neural Information Processing
Systems, 2018.
Vasileios Charisopoulos and Petros Maragos. A tropical approach to neural networks with piecewise
linear activations. arXiv preprint arXiv:1805.08749, 2018.
Dan Claudiu Ciresan, Ueli Meier, Luca Maria Gambardella, and Juergen Schmidhuber. Deep big
simple neural nets excel on handwritten digit recognition. arXiv preprint arXiv:1003.0358, 2010.
Francesco Croce, Maksym Andriushchenko, and Matthias Hein. Provable robustness of relu networks
via maximization of linear regions. In Proceedings of the International Conference on Artificial
Intelligence and Statistics, 2018.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. In Proceedings of the 7th International Conference on Learning Representations, 2019.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In
Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, 2011.
Richard H. R. Hahnloser, Rahul Sarpeshkar, Misha A. Mahowald, Rodney J. Douglas, and H. Sebas-
tian Seung. Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit.
Nature volume 405, 2000.
Boris Hanin and David Rolnick. Complexity of linear regions in deep networks. In Proceedings of
the 36th International Conference on Machine Learning, 2019a.
Boris Hanin and David Rolnick. Deep relu networks have surprisingly few activation patterns. In
Advances in Neural Information Processing Systems, 2019b.
9
Published as a conference paper at ICLR 2021
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the 15th International
Conference on Computer Vision, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the 29th IEEE Conference on Computer Vision and Pattern
Recognition, 2016.
Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov.
Improving neural networks by preventing co-adaptation of feature detectors. In arXiv preprint
arXiv:1207.0580, 2012.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of
the 3rd International Conference on Learning Representations, 2015.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, University
of Toronto, 2009.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online].
Available: http://yann. lecun. com/exdb/mnist, 2010.
Guang-He Lee, David Alvarez-Melis, and Tommi S. Jaakkola. Towards robust, locally linear deep
networks. In Proceedings of the 7th International Conference on Learning Representations, 2019.
Diane Maclagan and Bernd Sturmfels. Introduction to Tropical Geometry. Graduate Studies in
Mathematics, vol. 161, AMS, 2015.
GUido Montufar. Notes on the number of linear regions of deep neural networks. Presented at
Mathematics of Deep Learning, Sampling Theory and Applications, 2017.
Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear
regions of deep neural networks. In Advances in Neural Information Processing Systems, 2014.
Razvan Pascanu, Guido Montufar, and Yoshua Bengio. On the number of response regions of deep
feed forward networks with piece-wise linear activations. arXiv preprint arXiv:1312.6098, 2013.
Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the
expressive power of deep neural networks. In Proceedings of the 34th International Conference on
Machine Learning, 2017.
Thiago Serra, Christian Tjandraatmadja, and Srikumar Ramalingam. Bounding and counting linear
regions of deep neural networks. Proceedings of the 35th International Conference on Machine
Learning, 2018.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In Proceedings of the 3rd International Conference on Learning Representations,
2015.
Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving
for simplicity: The all convolutional net. Workshop Track Proceedings of the 3rd International
Conference on Learning Representations, 2015.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the 28th IEEE Conference on Computer Vision and Pattern Recognition, 2015.
Huan Xiong, Lei Huang, Mengyang Yu, Li Liu, Fan Zhu, and Ling Shao. On the number of linear
regions of convolutional neural networks. In Proceedings of the 37th International Conference on
Machine Learning, 2020.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
learning requires rethinking generalization. In Proceedings of the 5th International Conference on
Learning Representations, 2017.
10
Published as a conference paper at ICLR 2021
Liwen Zhang, Gregory Naitzat, and Lek-Heng Lim. Tropical geometry of deep neural networks. In
Proceedings of the 35th International Conference on Machine Learning, 2018.
Xiao Zhang and Dongrui Wu. Empirical studies on the properties of linear regions in deep neural
networks. In Proceedings of the 8th International Conference on Learning Representations, 2020.
11