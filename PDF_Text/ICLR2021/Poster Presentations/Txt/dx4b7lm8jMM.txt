Published as a conference paper at ICLR 2021
Seq2Tens: An Efficient Representation of Se-
quences by Low-Rank Tensor Projections
Csaba Toth
Patric Bonnier
Harald Oberhauser?
?Mathematical Institute, University of Oxford
{toth, bonnier, oberhauser}@maths.ox.ac.uk
Ab stract
Sequential data such as time series, video, or text can be challenging to analyse
as the ordered structure gives rise to complex dependencies. At the heart of this
is non-commutativity, in the sense that reordering the elements of a sequence can
completely change its meaning. We use a classical mathematical object - the free
algebra - to capture this non-commutativity. To address the innate computational
complexity of this algebra, we use compositions of low-rank tensor projections.
This yields modular and scalable building blocks that give state-of-the-art per-
formance on standard benchmarks such as multivariate time series classification,
mortality prediction and generative models for video. Code and benchmarks are
publically available at https://github.com/tgcsaba/seq2tens.
1	Introduction
A central task of learning is to find representations of the underlying data that efficiently and faith-
fully capture their structure. In the case of sequential data, one data point consists of a sequence of
objects. This is a rich and non-homogeneous class of data and includes classical uni- or multi-variate
time series (sequences of scalars or vectors), video (sequences of images), and text (sequences of
letters). Particular challenges of sequential data are that each sequence entry can itself be a highly
structured object and that data sets typically include sequences of different length which makes naive
vectorization troublesome.
Contribution. Our main result is a generic method that takes a static feature map for a class of
objects (e.g. a feature map for vectors, images, or letters) as input and turns this into a feature map for
sequences of arbitrary length of such objects (e.g. a feature map for time series, video, or text). We
call this feature map for sequences Seq2Tens for reasons that will become clear; among its attractive
properties are that it (i) provides a structured, parsimonious description of sequences; generalizing
classical methods for strings, (ii) comes with theoretical guarantees such as universality, (iii) can be
turned into modular and flexible neural network (NN) layers for sequence data. The key ingredient
to our approach is to embed the feature space of the static feature map into a larger linear space that
forms an algebra (a vector space equipped with a multiplication). The product in this algebra is then
used to “stitch together” the static features of the individual sequence entries in a structured way.
The construction that allows to do all this is classical in mathematics, and known as the free algebra
(over the static feature space).
Outline. Section 2 formalizes the main ideas of Seq2Tens and introduces the free algebra T(V )
over a space V as well as the associated product, the so-called convolution tensor product. Section
3 shows how low rank (LR) constructions combined with sequence-to-sequence transforms allows
one to efficiently use this rich algebraic structure. Section 4 applies the results of Sections 2 and
3 to build modular and scalable NN layers for sequential data. Section 5 demonstrates the flexibil-
ity and modularity of this approach on both discriminative and generative benchmarks. Section 6
makes connections with previous work and summarizes this article. In the appendices we provide
mathematical background, extensions, and detailed proofs for our theoretical results.
1
Published as a conference paper at ICLR 2021
2	Capturing order by non-commutative multiplication
We denote the set of sequences of elements in a set X by
Seq(X) = {x = (xi)i=1,...,L : xi ∈ X, L ≥ 1}	(1)
where L ≥ 1 is some arbitrary length. Even if X itself is a linear space, e.g. X = R, Seq(X) is
never a linear space since there is no natural addition of two sequences of different length.
Seq2Tens in a nutshell. Given any vector space V we may construct the so-called free algebra
T(V ) over V . We describe the space T(V ) in detail below, but as for now the only thing that is
important is that T(V ) is also a vector space that includes V , and that it carries a non-commutative
product, which is, in a precise sense, “the most general product” on V .
The main idea of Seq2Tens is that any “static feature map” for elements in X
φ : X → V
can be used to construct a new feature map Φ : Seq(X) → T(V ) for sequences in X by using the
algebraic structure of T(V): the non-commutative product on T(V) makes it possible to “stitch to-
gether” the individual features φ(x1), . . . , φ(xL) ∈ V ⊂ T(V) of the sequence x in the larger space
T(V) by multiplication in T(V). With this we may define the feature map Φ(x) for a sequences
x = (x1, . . . , xL) ∈ Seq(X) as follows
(i)	lift the map φ : X → V to a map 夕：X → T(V),
(ii)	map Seq(X) → Seq(T(V)) by (xι,..., xl) → (P(xι),...,夕(xl)),
(iii)	map Seq(T(V)) → T(V) by multiplication (夕(xι),...,夕(XL)) → 夕(xι)… 夕(xl).
In a more concise form, we define Φ as
L
Φ: Seq(X) → T(V), Φ(x) = Y 以Xi)	⑵
i=1
where Q denotes multiplication in T(V). We refer to the resulting map Φ as the Seq2Tens map,
which stands short for Sequences-2-Tensors. Why is this construction a good idea? First note, that
step (i) is always possible since V ⊂ T(V) and we discuss the simplest such lift before Theorem 2.1
as well as other choices in Appendix B. Further, if φ, respectively 夕,provides a faithful represen-
tation of objects in X, then there is no loss of information in step (ii). Finally, since step (iii) uses
“the most general product" to multiply 夕(xι) •… 夕(XL) one expects that Φ(x) ∈ T(V) faithfully
represents the sequence x as an element of T(V).
Indeed in Theorem 2.1 below we show an even stronger statement, namely that if the static feature
map φ : X → V contains enough non-linearities so that non-linear functions from X to R can be
approximated as linear functions of the static feature map φ, then the above construction extends
this property to functions of sequences. Put differently, if φ is a universal feature map for X, then Φ
is a universal feature map for Seq(X); that is, any non-linear function f(x) ofa sequence x can be
approximated as a linear functional of Φ(x), f(x) ≈ h`, Φ(x)i. We also emphasize that the domain
of Φ is the space Seq(X) of sequences of arbitrary (finite) length. The remainder of this Section
gives more details about steps (i),(ii),(iii) for the construction of Φ.
The free algebra T(V) over a vector space V. Let V be a vector space. We denote by T(V) the
set of sequences of tensors indexed by their degree m,
T(V)：= {t = (tm)m≥0 | tm ∈ V即}	(3)
where by convention V00 = R. For example, if V = Rd and t = (tm)m≥o is some element of
T(Rd), then its degree m = 1 component is a d-dimensional vector t1, its degree m = 2 component
is a d × d matrix t2, and its degree m = 3 component is a degree 3 tensor t3 . By defining addition
and scalar multiplication as
S + t := (Sm + tm)m≥0,	C ∙ t = (Ctm)m≥0	(4)
2
Published as a conference paper at ICLR 2021
the set T(V ) becomes a linear space. By identifying v ∈ V as the element (0, v, 0, . . . , 0) ∈ T(V )
we see that V is a linear subspace of T(V ). Moreover, while V is only a linear space, T(V ) carries
a product that turns T(V ) into an algebra. This product is the so-called tensor convolution product,
and is defined for s, t ∈ T(V ) as
m
S ∙ t := (X Si 0 tm-i)m≥0 =(1, Si + tl, S2 + Sl X tl + 12,-.) ∈ T(V)	(5)
i=0
where 0 denotes the usual outer tensor product; e.g. for vectors u = (ui), v = (vi) ∈ Rd the outer
tensor product u 0 v is the d × d matrix (uivj)i,j=1,...,d. We emphasize that like the outer tensor
product 0, the tensor convolution product ∙ is non-commutative, i.e. S ∙ t = t ∙ s. Ina mathematically
precise sense, T(V) is the most general algebra that contains V; it is a “free construction”. Since
T(V) is realized as series of tensors of increasing degree, the free algebra T(V) is also known as the
tensor algebra in the literature. Appendix A contains background on tensors and further examples.
Lifting static feature maps. Step (i) in the construction of Φ requires turning a given feature map
φ : X → V into a map 夕:X → T(V). Throughout the rest of this article We use the lift
q(x) = (1,φ(x), 0,0 ...) ∈ T(V).	(6)
We discuss other choices in Appendix B, but attractive properties of the lift 6 are that (a) the evalu-
ation of Φ against loW rank tensors becomes a simple recursive formula (Proposition 3.3, (b) it is a
generalization of sequence sub-pattern matching as used in string kernels (Appendix B.3, (c) despite
its simplicity it performs exceedingly Well in practice (Section 4).
Extending to sequences of arbitrary length. Steps (i) and (ii) in the construction specify hoW
the map Φ : X → T(V) behaves on sequences of length-1, that is, single observations. Step (iii)
amounts to the requirement that for any tWo sequences x = (x1, . . . , xK), y = (y1, . . . , yL) ∈
Seq(V), their concatenation defined as z = (x1, . . . , xK, y1, . . . , yL) ∈ Seq(V) can be understood
in the feature space as (non-commutative) multiplication of their corresponding features
Φ(z) = Φ(x) ∙ Φ(y).	(7)
In other words, we inductively extend the lift 夕 to sequences of arbitrary length by starting from
sequences consisting of a single observation, Which is given in equation 2. Repeatedly applying the
definition of the tensor convolution product in equation 5 leads to the following explicit formula
Φm(x) =	X	Xii 0 …0 Xim ∈ V0m, Φ(x) = (Φm (x))m≥0,	(8)
1≤iι<∙∙∙<im≤L
where x = (x1, . . . , xL) ∈ Seq(V) and the summation is over non-contiguous subsequences ofx.
Some intuition: generalized pattern matching. Our derivation of the feature map Φ(x) =
(1, Φ1(x), Φ2(x), . . .) ∈ T(V) was guided by general algebraic principles, but equation 8 pro-
vides an intuitive interpretation. It shows that for each m ≥ 1, the entry Φm(x) ∈ V0m constructs
a summary of a long sequence x = (x1, . . . , xL) ∈ Seq(V) based on subsequences (xi1 , . . . , xim)
of X of length-m. It does this by taking the usual outer tensor product x” 0∙∙∙0 Xim ∈ V0m and
summing over all possible subsequences. This is completely analogous to how string kernels provide
a structured description of text by looking at non-contiguous substrings of length-m (indeed, Ap-
pendix B.3 makes this rigorous). However, the main difference is that the above construction works
for arbitrary sequences and not just sequences of discrete letters. Readers with less mathematical
background might simply take this as motivation and regard equation 8 as definition. However, the
algebraic background allows to prove that Φ is universal, see Theorem 2.1 below.
Universality. A function φ : X → V is said to be universal for X if all continuous functions on
X can be approximated as linear functions on the image of φ. One of the most powerful features of
neural nets is their universality (Hornik, 1991). A very attractive property of Φ is that it preserves
universality: ifφ : X → V is universal for X, then Φ : Seq(X) → T(V) is universal for Seq(X). To
make this precise, note that V0m is a linear space and therefore any ' = ('o, 'i,..., 'm, 0,0,...) ∈
3
Published as a conference paper at ICLR 2021
T(V) consisting of M tensors'm ∈ V0m, yields a linear functional on T(V); e.g. if V = Rd and
we identify `m in coordinates as `m = (`im1,...,im )i1,...,im∈{1,...,d} then
MM
h',ti := X h'm,tmi = X X	`m--,im tim,-,im.	(9)
m=0	m=0 i1,...,im∈{1,...,d}
Thus linear functionals of the feature map Φ, are real-valued functions of sequences. Theorem 2.1
below shows that any continuous function f : Seq(X) → R can by arbitrary well approximated by
a ' ∈ T(V),f(x) ≈ h', Φ(x)i.
Theorem 2.1. Let φ : X → V be a universal map with a lift that satisfies some mild constraints,
then the following map is universal:
Φ : Seq(X) → T(V), x 7→ Φ(x).	(10)
A detailed proof and the precise statement of Theorem 2.1 is given in Appendix B.
3	Approximation by low-rank linear functionals
The combinatorial explosion of tensor coordinates and what to do about it. The universal-
ity of Φ suggests the following approach to represent a function f : Seq(X ) → R of sequences:
First compute Φ(x) and then optimize over ` (and possibly also the hyperparameters of φ) such that
f (x) ≈ h', Φ(x)i = PM=oh'm, Φm(x)i. Unfortunately, tensors suffer from a combinatorial explo-
sion in complexity in the sense that evenjust storing Φm(x) ∈ V0m ⊂ T(V) requires O(dim(V)m)
real numbers. Below we resolve this computational bottleneck as follows: in Proposition 3.3 we
show that for a special class of low-rank elements ` ∈ T(V), the functional x 7→ h`, Φ(x)i can be
efficiently computed in both time and memory. This is somewhat analogous to a kernel trick since it
shows that h`, Φ(x)i can be cheaply computed without explicitly computing the feature map Φ(x).
However, Theorem 2.1 guarantees universality under no restriction on `, thus restriction to rank-1
functionals limits the class of functions f(x) that can be approximated. Nevertheless, by iterating
these “low-rank functional” constructions in the form of sequence-to-sequence transformations this
can be ameliorated. We give the details below but to gain intuition, we invite the reader to think
of this iteration analogous to stacking layers in a neural network: each layer is a relatively sim-
ple non-linearity (e.g. a sigmoid composed with an affine function) but by composing such layers,
complicated functions can be efficiently approximated.
Rank-1 functionals are computationally cheap. Degree m = 2 tensors are matrices and low-
rank (LR) approximations of matrices are widely used in practice (Udell & Townsend, 2019) to
address the quadratic complexity. The definition below generalizes the rank of matrices (tensors of
degree m = 2) to tensors of any degree m.
Definition 3.1. The rank (also called CP rank (Carroll & Chang, 1970)) of a degree-m tensor
tm ∈ V0m is the smallest number r ≥ 0 such that one may write
r
tm = X Vi 乳…乳 Vm,	V1,…，Vm ∈ V.	(11)
i=0
We say that t = (tm)m≥0 ∈ T(V) has rank-1 (and degree-M) if each tm ∈ V0m is a rank-1 tensor
and ti = 0 for i > M.
Remark 3.2. For x = (x1, . . . , xL) ∈ Seq(V), the rank rm ∈ N of Φm(x) satisfies rm ≤ mL ,
while the rank and degree r, d ∈ N of Φ(x) satisfy r ≤ (K) for K = |_L2_| and d ≤ L.
A direct calculation shows that if` is of rank-1, then h`, Φ(x)i can be computed very efficiently by
inner product evaluations in V .
Proposition 3.3. Let ' = ('m)m≥o ∈ T(V) be of rank-1 and degree-M. If φ is lifted to 夕 as in
equation 6, then
Mm
h`, Φ(x)i = X X	γ hvm,ΦXk )i	(12)
m=0 1≤iι<∙∙∙<im≤L k=1
where 'm = Vm 0∙∙∙0 Vm ∈ V0m, Vm ∈ V and m = 0,...,M.
4
Published as a conference paper at ICLR 2021
Note that the inner sum is taken over all non-contiguous subsequences ofx of length-m, analogously
to m-mers of strings and we make this connection precise in Appendix B.3; the proof of Proposition
3.3 is given in Appendix B.1.1. While equation 12 looks expensive, by casting it into a recursive
formulation over time, it Canbe computed in O(M2 ∙ L ∙ d) time and O(M2 ∙ (L + C)) memory, where
d is the inner product evaluation time on V , while c is the memory footprint of a v ∈ V . This can
further be reduced to O(M ∙ L ∙ d) time and O(M ∙ (L + C)) memory by an efficient parametrization
of the rank-1 element ` ∈ T(V ). We give further details in Appendices D.2, D.3, D.4.
Low-rank Seq2Tens maps. The composition of a linear map L : T(V ) → RN with Φ can be
computed cheaply in parallel using equation 12 when L is specified through a collection of N ∈ N
rank-1 elements '1,...,'N ∈ T(V) such that
Φ @(xi, ..., XL ) := L ◦ Φ(xi, ..., XL) = (h'j, Φ(xi, .. . , XL))N=I ∈ RN.	(13)
We call the resulting map Φ石:Seq(X) → RN a Low-rank Seq2Tens map of width-N and order-M,
where M ∈ N is the maximal degree of '1,...,'N such that 'j = 0 for i > M. The LS2T map is
parametrized by(1) the component vectors vj,m ∈ V ofthe rank-1 elements j = Vjm0∙ ∙ ∙0vmm,
(2) by any parameters θ that the static feature map φθ : X → V may depend on. We jointly denote
these parameters by θ = (Θ,'1,...,'N). In addition, by the subsequent composition of Φ^ with a
linear functional RN → R, we get the following function subspace as hypothesis class for the LS2T
N
H= {hX αj 'j, Φ(xι,..., XL)i∣αj ∈ R} ( H = {<', Φ(xι,..., XL) | ' ∈ T(V)}	(14)
j=1
Hence, we acquire an intuitive explanation of the (hyper)parameters: the width of the LS2T, N ∈ N
specifies the maximal rank of the low-rank linear functionals ofΦ that the LS2T can represent, while
the spanoftherank-1 elements, span('1,...,'N) determine an N-dimensional subspace of the dual
space of T(V) consisting ofat most rank-N functionals.
Recall now that without rank restrictions on the linear functionals of Seq2Tens features, Theo-
rem 2.1 would guarantee that any real-valued function f : Seq(X) → R could be approximated
by f(x) ≈ h`, Φ(x1, . . . , xL)i. As pointed out before, the restriction of the hypothesis class to low-
rank linear functionals of Φ(x1, . . . , xL) would limit the class of functions of sequences that can be
approximated. To ameliorate this, we use LS2T transforms in a sequence-to-sequence fashion that
allows us to stack such low-rank functionals, significantly recovering expressiveness.
Sequence-to-sequence transforms. We can use LS2T to build sequence-to-sequence transforma-
tions in the following way: fix the static map φθ : X → V parametrized by θ and rank-1 elements
such that θ = (θ,'1,..., 'N) and apply the resulting LS2T map Φ^ over expanding windows of x:
Seq(X) → Seq(RN),	X → (Φ 京(xι), Φ ^(xι, X2),..., Φ d(xι,..., xl)).	(15)
Note that the cost of computing the expanding window sequence-to-sequence transform in equa-
∙-v
tion 15 is no more expensive than computing Φ^(xj,..., XL) itself due to the recursive nature of
our algorithms, for further details see Appendices D.2, D.3, D.4.
Deep sequence-to-sequence transforms. Inspired by the empirical successes of deep RNNs
(Graves et al., 2013b;a; Sutskever et al., 2014), we iterate the transformation 15 D-times:
Seq(X) → Seq(RNI) → Seq(RN2) → ——→ Seq(RND).	(16)
Each of these mappings Seq(RNi) → Seq(RNi+1) is parametrized by the parameters θi of a static
feature map φθi and a linear map Li specified by Ni rank-1 elements of T(V); these parameters are
collectively denoted by 仄=(θi,'j,...,'Ni). Evaluating the final sequence in Seq(RND) at the
last observation-time t = L, we get the deep LS2T map with depth-D
∙-v
φΘi,...,Θd : Seq(X) → RnD.	(17)
Making precise how the stacking of such low-rank sequence-to-sequence transformations approx-
imates general functions requires more tools from algebra, and we provide a rigorous quantitative
statement in Appendix C. Here, we just appeal to the analogy made with adding depth in neural
networks mentioned earlier and empirically validate this in our experiments in Section 4.
5
Published as a conference paper at ICLR 2021
4	Building neural networks with LS2T layers
The Seq2Tens map Φ built from a static feature map φ is universal if φ is universal, Theorem 2.1.
NNs form a flexible class of universal feature maps with strong empirical success for data in X =
Rd , and thus make a natural choice for φ. Combined with standard deep learning constructions, the
framework of Sections 2 and 3 can build modular and expressive layers for sequence learning.
Neural LS2T layers. The simplest choice among many is to use as static feature map φ : X =
Rd → Rh a feedforward network With depth-P, φ = φp ◦•••◦ φι where φj (x) = σ (Wj X + bj) for
Wj ∈ Rh×d, bj- ∈ Rh. We can then lift this to a map 夕：Rd → T(Rh) as prescribed in equation 6.
∙-v
Hence, the resulting LS2T layer X → (Φ石(xι,..., Xi))i=ι,...,L is a SeqUence-to-sequence transform
Seq(Rd) → Seq(Rh) that is parametrized by θ = (Wι, bi,..., Wp, bp,'1,..., 'N1).
Bidirectional LS2T layers. The transformation in equation 15 is completely causal in the sense
that each step of the output sequence depends only on past information. For generative models, it can
behove us to make the output depend on both past and future information, see Graves et al. (2013a);
Baldi et al. (1999); Li & Mandt (2018). Similarly to bidirectional RNNs and LSTMs (Schuster &
Paliwal, 1997; Graves & Schmidhuber, 2005), we may achieve this by defining a bidirectional layer,
Φbθ1,θ2)(x) ： Seq(Rd) → Seq(RN+N0), X → (Φθ1 (xi,…，Xi), Φθ2(xi,…，XL))L=> (18)
The sequential nature is kept intact by making the distinction between what classifies as past (the
first N coordinates) and future (the last N0 coordinates) information. This amounts to having a form
of precognition in the model, and has been applied in e.g. dynamics generation (Li & Mandt, 2018),
machine translation (Sundermeyer et al., 2014), and speech processing (Graves et al., 2013a).
Convolutions and LS2T. We motivate to replace the time-distributed feedforward layers proposed
in the paragraph above by temporal convolutions (CNN) instead. Although theory only requires the
preprocessing layer of the LS2T to be a static feature map, we find that it is beneficial to capture some
of the sequential information in the preprocessing layer as well, e.g. using CNNs or RNNs. From a
mathematical point of view, CNNs are a straightforward extension since they can be interpreted as
time-distributed feedforward layers applied to the input sequence augmented with a p ∈ N number
of its lags for CNN kernel size p (see Appendix D.1 for further discussion).
In the following, we precede our deep LS2T blocks by one or more CNN layers. Intuitively, CNNs
and LS2Ts are similar in that both transformations operate on subsequences of their input sequence.
The main difference between the two lies in that CNNs operate on contiguous subsequences, and
therefore, capture local, short-range nonlinear interactions between timesteps; while LS2Ts (equa-
tion 12) use all non-contiguous subsequences, and hence, learn global, long-range interactions in
time. This observation motivates that the inductive biases of the two types of layers (local/global
time-interactions) are highly complementary in nature, and we suggest that the improvement in the
experiments on the models containing vanilla CNN blocks are due to this complementarity.
5	Experiments
We demonstrate the modularity and flexibility of the above LS2T and its variants by applying it to
(i) multivariate time series classification, (ii) mortality prediction in healthcare, (iii) generative mod-
elling of sequential data. In all cases, we take a strong baseline model (FCN and GP-VAE, as detailed
below) and upgrade it with LS2T layers. As Thm. 2.1 requires the Seq2Tens layers to be preceded
by at least a static feature map, we expect these layers to perform best as an add-on on top of other
models, which however can be quite simple, such as a CNN. The additional computation time is
negligible (in fact, for FCN it allows to reduce the number of parameters significantly, while retain-
ing performance), but it can yield substantial improvements. This is remarkable, since the original
models are already state-of-the-art on well-established (frequentist and Bayesian) benchmarks.
5.1	Multivariate time series classification
As the first task, we consider multivariate time series classification (TSC) on an archive of bench-
mark datasets collected by Baydogan (2015). Numerous previous publications report results on this
6
Published as a conference paper at ICLR 2021
Table 1: Posterior probabilities given by a Bayesian signed-rank test comparison of the proposed
methods against the baselines. {>}, {<}, {=} refer to the respective events that the row method is
better, the column method is better, or that they are equivalent.
Model	LS2T64			FCN64-LS2T364			FCN128-LS2T364		
	p(>)	p(=)	P(V)	p(>)	PU)	P(<)	P(>)	P(=)	P(<)
SMTS (Baydogan & Runger, 2015a)	0.180	0.000	0.820	0.010	0.000	0.990	0.008	0.000	0.992
LPS (Baydogan & Runger, 2015b)	0.191	0.002	0.807	0.012	0.001	0.987	0.006	0.001	0.993
mvARF (Tuncel & Baydogan, 2018)	0.011	0.140	0.849	0.000	0.126	0.874	0.000	0.088	0.912
DTW (Sakoe & Chiba, 1978)	0.033	0.000	0.967	0.001	0.000	0.999	0.000	0.000	1.000
ARKernel (Cuturi & Doucet, 2011)	0.100	0.097	0.803	0.000	0.021	0.979	0.000	0.015	0.985
gRSF (Karlsson et al., 2016)	0.481	0.011	0.508	0.028	0.013	0.960	0.022	0.013	0.965
MUSE (SCHAFER & Leser, 2017)	0.405	0.128	0.467	0.001	0.074	0.925	0.001	0.077	0.922
MLSTMFCN (Karim et al., 2019)	0.916	0.043	0.041	0.123	0.071	0.807	0.055	0.110	0.835
FCN128 (Wang et al., 2017)	0.998	0.002	0.000	0.363	0.186	0.451	0.169	0.011	0.820
ResNet (Wang et al., 2017)	0.998	0.002	0.001	0.056	0.240	0.704	0.016	0.048	0.935
LS2T364	-	-	-	0.000	0.001	0.999	0.000	0.001	0.999
FCN64-LS2T364	0.999	0.001	0.000	-	-	-	0.020	0.387	0.593
archive, which makes it possible to compare against several well-performing competitor methods
from the TSC community. These baselines are detailed in Appendix E.1. This archive was also
considered in a recent popular survey paper on DL for TSC (Ismail Fawaz et al., 2019), from where
we borrow the two best performing models as DL baselines: FCN and ResNet. The FCN is a
fully convolutional network which stacks 3 convolutional layers of kernel sizes (8, 5, 3) and filters
(128, 256, 128) followed by a global average pooling (GAP) layer, hence employing global param-
eter sharing. We refer to this model as FCN128. The ResNet is a residual network stacking 3 FCN
blocks of various widths with skip-connections in between (He et al., 2016) and a final GAP layer.
The FCN is an interesting model to upgrade with LS2T layers, since the LS2T also employs pa-
rameter sharing across the sequence length, and as noted previously, convolutions are only able
to learn local interactions in time, that in particular makes them ill-suited to picking up on long-
range autocorrelations, which is exactly where the LS2T can provide improvements. As our models,
we consider three simple architectures: (i) LS2T364 stacks 3 LS2T layers of order-2 and width-64;
(ii) FCN64-LS2T634 precedes the LS2T364 block by an FCN64 block; a downsized version of FCN128;
(iii) FCN128-LS2T634 uses the full FCN128 and follows it by a LS2T634 block as before. Also, both
FCN-LS2T models employ skip-connections from the input to the LS2T block and from the FCN
to the classification layer, allowing for the LS2T to directly see the input, and for the FCN to di-
rectly affect the final prediction. These hyperparameters were only subject to hand-tuning on a
subset of the datasets, and the values we considered were H, N ∈ {32, 64, 128}, M ∈ {2, 3, 4}
and D ∈ {1, 2, 3}, where H, N ∈ N is the FCN and LS2T width, resp., while M ∈ N is the LS2T
order and D ∈ N is the LS2T depth. We also employ techniques such as time-embeddings (Liu
et al., 2018a), sequence differencing and batch normalization, see Appendix D.1; Appendix E.1 for
further details on the experiment and Figure 2 in thereof for a visualization of the architectures.
Results. We trained the models, FCN128, ResNet, LS2T364, FCN64-LS2T364, FCN128-LS2T364 on
each of the 16 datasets 5 times while results for other methods were borrowed from the cited pub-
lications. In Appendix E.1, Figure 3 depicts the box-plot of distributions of accuracies and a CD
diagram using the Nemenyi test (Nemenyi, 1963), while Table 7 shows the full list of results. Since
mean-ranks based tests raise some paradoxical issues (Benavoli et al., 2016), it is customary to
conduct pairwise comparisons using frequentist (Demsar, 2006) or Bayesian (Benavoli et al., 2017)
hypothesis tests. We adopted the Bayesian signed-rank test from Benavoli et al. (2014), the poste-
rior probabilities of which are displayed in Table 1, while the Bayesian posteriors are visualized on
Figure 4 in App. E.1. The results of the signed-rank test can be summarized as follows: (1) LS2T364
already outperforms some classic TS classifiers with high probability (p ≥ 0.8), but it is not compet-
itive with other DL classifiers. This observation is not surprising since even theory requires at least
a static feature map to precede the LS2T. (2) FCN64-LS2T634 outperforms almost all models with
high probability (p ≥ 0.8), except for ResNet (which is stil outperformed by p ≥ 0.7), FCN128 and
FCN128-LS2T364. When compared with FCN128, the test is unable to decide between the two, which
upon inspection of the individual results in Table 7 can be explained by that on some datasets the
benefit of the added LS2T block is high enough that it outweighs the loss of flexibility incurred by
reducing the width of the FCN - arguably these are the datasets where long-range autocorrelations
7
Published as a conference paper at ICLR 2021
are present in the input time series, and picking up on these improve the performance - however, on
a few datasets the contrary is true. (3) Lastly, FCN128-LS2T364, outperforms all baseline methods
with high probability (p ≥ 0.8), and hence successfully improves on the FCN128 via its added abil-
ity to learn long-range time-interactions. We remark that FCN64 -LS2T364 has fewer parameters than
FCN128 by more than 50%, hence we managed to compress the FCN to a fraction of its original
size, while on average still slightly improving its performance, a nontrivial feat by its own accord.
5.2	Mortality prediction
We consider the Physionet20 1 2 challenge dataset (Goldberger et al., 2000) for mortality predic-
tion, which is a case of medical TSC as the task is to predict in-hospital mortality of patients after
their admission to the ICU. This is a difficult ML task due to missingness in the data, low signal-
to-noise ratio (SNR), and imbalanced class distributions with a prevalence ratio of around 14%. We
extend the experiments conducted in Horn et al. (2020), which we also use as very strong baselines.
Under the same experimental setting, we train two models: FCN-LS2T as ours and the FCN as
another baseline. For both models, we conduct a random search for all hyperparameters with 20
samples from a pre-specified search space, and the setting with best validation performance is used
for model evaluation on the test set over 5 independent model trains, exactly the same way as it was
done in Horn et al. (2020). We preprocess the data using the same method as in Che et al. (2018,
eq. (9)) and additionally handle static features by tiling them along the time axis and adding them
as extra coordinates. We additionally introduce in both models a SpatialDropout1D layer after
all CNN and LS2T layers with the same tunable dropout rate to mitigate the low SNR of the dataset.
Results. Table 2 compares the perfor- Table 2: Comparison of FCN-LS2T and FCN on PHY-
mance of FCN-LS2T with that of FCN SIONET20 12 with the results from Horn et al. (2020).
and the results from Horn et al. (2020) on 3 metrics: (1) accuracy, (2) area un-	Model	Accuracy	AUPRC	AUROC
	FCN-LS2T FCN	84.1 ± 1.6 80.7 ± 1.7	53.9 ± 0.5 52.8 ± 1.3	85.6 ± 0.5 85.6 ± 0.2
der the precision-recall curve (AUPRC),				
(3) area under the ROC curve (AUROC).	GRU-D	80.0 ± 2.9	53.7 ± 0.9	86.3 ± 0.3
We can observe that FCN-LS2T takes	GRU-Simple IP-Nets	82.2 ± 0.2 79.4 ± 0.3	42.2 ± 0.6 51.0 ± 0.6	80.8 ± 1.1 86 .0 ± 0 .2
on average first place according to both	Phased-LSTM	76.8 ± 5.2	38.7 ± 1.5	79.0 ± 1.0
Accuracy and AUPRC, outperforming	Transformer	83.7 ± 3.5	52.8 ± 2.2	86.3 ± 0.8
FCN and all SOTA methods, e.g. Trans -	Latent-ODE SeFT-Attn.	76.0 ± 0.1 75.3 ± 3.5	50.7 ± 1.7 52.4 ± 1.1	85.7 ± 0.6 85.1 ± 0.4
former (Vaswani et al., 2017), GRU-D
Che et al. (2018), SeFT (Horn et al., 2020), and also being competitive in terms of AUROC. This
is very promising, and it suggests that LS2T layers might be particularly well-suited to complex
and heterogenous datasets, such as medical time series, since the FCN-LS2T models significantly
improved accuracy on ECG as well, another medical dataset in the previous experiment.
5.3	Generating sequential data
Finally, we demonstrate on sequential data imputation for time series and video that LS2Ts do not
only provide good representations of sequences in discriminative, but also generative models.
The GP-VAE model. In this experiment, we take as base model the recent GP-VAE (Fortuin
et al., 2020), that provides state-of-the-art results for probabilistic sequential data imputation. The
GP-VAE is essentially based on the HI-VAE (Nazabal et al., 2018) for handling missing data in
variational autoencoders (VAEs) (Kingma & Welling, 2013) adapted to the handling of time series
data by the use of a Gaussian process (GP) prior (Williams & Rasmussen, 2006) across time in
the latent sequence space to capture temporal dynamics. Since the GP-VAE is a highly advanced
model, its in-depth description is deferred to Appendix E.3.We extend the experiments conducted in
Fortuin et al. (2020), and we make one simple change to the GP-VAE architecture without changing
any other hyperparameters or aspects: we introduce a single bidirectional LS2T layer (B-LS2T)
into the encoder network that is used in the amortized representation of the means and covariances
of the variational posterior. The B-LS2T layer is preceded by a time-embedding and differencing
block, and succeeded by channel flattening and layer normalization as depicted in Figure 5. The
idea behind this experiment is to see if we can improve the performance of a highly complicated
model that is composed of many interacting submodels, by the naive introduction of LS2T layers.
8
Published as a conference paper at ICLR 2021
Table 3: Performance comparison of GP-VAE (B-LS2T) with the baseline methods
Method	HMNIST			Sprites	Physionet
	NLL	MSE	AUROC	MSE	AUROC
Mean imputation	-	0.168 ± 0.000	0.938 ± 0.000	0.013 ± 0.000	0.703 ± 0.000
Forward imputation	-	0.177 ± 0.000	0.935 ± 0.000	0.028 ± 0.000	0.710 ± 0.000
VAE	0.599 ± 0.002	0.232 ± 0.000	0.922 ± 0.000	0.028 ± 0.000	0.677 ± 0.002
HI-VAE	0.372 ± 0.008	0.134 ± 0.003	0.962 ± 0.001	0.007 ± 0.000	0.686 ± 0.010
GP-VAE	0.350 ± 0.007	0.114 ± 0.002	0.960 ± 0.002	0.002 ± 0.000	0.730 ± 0.006
GP-VAE (B-LS2T)	0.251 ± 0.008	0.092 ± 0.003	0.962 ± 0.001	0.002 ± 0.000	0.743 ± 0.007
BRITS	-	-	-	-	0.742 ± 0.008
Results. To make the comparison, we ceteris paribus re-ran all experiments the authors originally
included in their paper (Fortuin et al., 2020), which are imputation of Healing MNIST, Sprites,
and Physionet 2012. The results are in Table 3, which report the same metrics as used in Fortuin
et al. (2020), i.e. negative log-likelihood (NLL, lower is better), mean squared error (MSE, lower
is better) on test sets, and downstream classification performance of a linear classifier (AUROC,
higher is better). For all other models beside our GP-VAE (B-LS2T), the results were borrowed
from Fortuin et al. (2020). We observe that simply adding the B-LS2T layer improved the result
in almost all cases, except for Sprites, where the GP-VAE already achieved a very low MSE score.
Additionally, when comparing GP-VAE to BRITS on Physionet, the authors argue that although the
BRITS achieves a higher AUROC score, the GP-VAE should not be disregarded as it fits a generative
model to the data that enjoys the usual Bayesian benefits of predicting distributions instead of point
predictions. The results display that by simply adding our layer into the architecture, we managed
to elevate the performance of GP-VAE to the same level while retaining these same benefits. We
believe the reason for the improvement is a tighter amortization gap in the variational approximation
(Cremer et al., 2018) achieved by increasing the expressiveness of the encoder by the LS2T allowing
it to pick up on long-range interactions in time. We provide further discussion in Appendix E.3.
6	Related work and Summary
Related Work. The literature on tensor models in ML is vast. Related to our approach we mention
pars-pro-toto Tensor Networks (Cichocki et al., 2016), that use classical LR decompositions, such
as CP (Carroll & Chang, 1970), Tucker (Tucker, 1966), tensor trains (Oseledets, 2011) and tensor
rings (Zhao et al., 2019); further, CNNs have been combined with LR tensor techniques (Cohen
et al., 2016; Kossaifi et al., 2017) and extended to RNNs (Khrulkov et al., 2019); Tensor Fusion
Networks (Zadeh et al., 2017) and its LR variants (Liu et al., 2018b; Liang et al., 2019; Hou et al.,
2019); tensor-based gait recognition (Tao et al., 2007). Our main contribution to this literature is the
use of the free algebra T(V) with its convolution product ∙, instead of V0m with the outer product
0 that is used in the above papers. While counter-intuitive to work in a larger space T(V), the
additional algebra structure of (T(V), ∙) is the main reason for the nice properties of Φ (universality,
making sequences of arbitrary length comparable, convergence in the continuous time limit; see
Appendix B) which we believe are in turn the main reason for the strong benchmark performance.
Stacked LR sequence transforms allow to exploit this rich algebraic structure with little computa-
tional overhead. Another related literature are path signatures in ML (Lyons, 2014; Chevyrev &
Kormilitzin, 2016; Graham, 2013; Bonnier et al., 2019; Toth & Oberhauser, 2020). These arise as
special case of Seq2Tens (Appendix B) and our main contribution to this literature is that Seq2Tens
resolves a well-known computational bottleneck in this literature since it never needs to compute
and store a signature, instead it directly and efficiently learns the functional of the signature.
Summary. We used a classical non-commutative structure to construct a feature map for se-
quences of arbitrary length. By stacking sequence transforms we turned this into scalable and mod-
ular NN layers for sequence data. The main novelty is the use of the free algebra T(V) constructed
from the static feature space V. While free algebras are classical in mathematics, their use in ML
seems novel and underexplored. We would like to re-emphasize that (T(V), ∙) is not a mysterious
abstract space: if you know the outer tensor product 0 then you can easily switch to the tensor
convolution product ∙ by taking sums of outer tensor products, as defined in equation 5. As our ex-
periments show, the benefits of this algebraic structure are not just theoretical but can significantly
elevate performance of already strong-performing models.
9
Published as a conference paper at ICLR 2021
References
Pierre Baldi, S0ren Brunak, Paolo Frasconi, Giovanni Soda, and Gianluca Pollastri. Exploiting the
past and the future in protein secondary structure prediction. Bioinformatics, 15(11):937-946,
1999.
Robert Bamler and Stephan Mandt. Dynamic word embeddings. In Proceedings of the 34th Inter-
national Conference on Machine Learning-Volume 70, pp. 380-389. JMLR. org, 2017.
Mustafa Baydogan. Multivariate time series classification datasets. http://
mustafabaydogan.com, 2015. [Accessed: 2020-06-11].
Mustafa Gokce Baydogan and George Runger. Learning a symbolic representation for multivariate
time series classification. Data Mining and Knowledge Discovery, 29(2):400-422, 2015a.
Mustafa Gokce Baydogan and George C. Runger. Time series representation and similarity based
on local autopatterns. Data Mining and Knowledge Discovery, 30:476-509, 2015b.
Alessio Benavoli, Giorgio Corani, Francesca Mangili, Marco Zaffalon, and Fabrizio Ruggeri. A
bayesian wilcoxon signed-rank test based on the dirichlet process. In International conference on
machine learning, pp. 1026-1034, 2014.
Alessio Benavoli, Giorgio Corani, and Francesca Mangili. Should we really use post-hoc tests based
on mean-ranks? The Journal of Machine Learning Research, 17(1):152-161, January 2016. ISSN
1532-4435.
Alessio Benavoli, Giorgio Corani, Janez Demsar, and Marco Zaffalon. Time for a change: a tutorial
for comparing multiple classifiers through bayesian analysis. The Journal of Machine Learning
Research, 18(1):2653-2688, 2017.
David M Blei and John D Lafferty. Dynamic topic models. In Proceedings of the 23rd international
conference on Machine learning, pp. 113-120, 2006.
David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisti-
cians. Journal of the American statistical Association, 112(518):859-877, 2017.
P Bonnier, C Liu, and H Oberhauser. Adapted topologies and higher rank signatures. arXiv preprint
arXiv:2005.08897, 2020.
Patric Bonnier, Patrick Kidger, Imanol Perez Arribas, Cristopher Salvi, and Terry Lyons. Deep
signature transforms. 33rd Conference on Neural Information Processing Systems, NeurIPS,
2019.
Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. Brits: Bidirectional recurrent
imputation for time series. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 6775-
6785. Curran Associates, Inc., 2018.
J Douglas Carroll and Jih-Jie Chang. Analysis of individual differences in multidimensional scaling
via an n-way generalization of “Eckart-young” decomposition. Psychometrika, 35(3):283-319,
1970.
Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. Recurrent
neural networks for multivariate time series with missing values. Scientific reports, 8(1):1-12,
2018.
K. T. Chen. Iterated integrals and exponential homomorphisms. Proc. London Math. Soc, 4, 502-
512, 1954.
K. T. Chen. Integration of paths, geometric invariants and a generalized Baker-Hausdorff formula.
Ann. of Math. (2), 65:163-178, 1957.
K. T. Chen. Integration of paths - a faithful representation of paths by non-commutative formal
power series. Trans. Amer. Math. Soc. 89 (1958), 395-407, 1958.
10
Published as a conference paper at ICLR 2021
I. Chevyrev and A. Kormilitzin. A primer on the signature method in machine learning. arXiv
preprint arXiv:1603.03788, 2016.
Andrzej Cichocki, Namgil Lee, Ivan Oseledets, Anh-Huy Phan, Qibin Zhao, and Danilo P Mandic.
Tensor networks for dimensionality reduction and large-scale optimization: Part 1 low-rank tensor
decompositions. Foundations and Trends® in Machine Learning, 9(4-5):249-429, 2016.
Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor
analysis. In Conference on learning theory, pp. 698-728, 2016.
Chris Cremer, Xuechen Li, and David Duvenaud. Inference suboptimality in variational autoen-
coders. In Proceedings of the 35th International Conference on Machine Learning, pp. 1078-
1086, 2018.
N Cristianini and J Shawe-Taylor. An Introduction to Support Vector Machines. Cambridge, 2000.
Marco Cuturi and Arnaud Doucet. Autoregressive Kernels For Time Series. arXiv e-prints, art.
arXiv:1101.0673, Jan 2011.
Janez Demsar. Statistical comparisons of classifiers over multiple data sets. Journal of Machine
learning research, 7(Jan):1-30, 2006.
J Diehl, K Ebrahimi-Fard, and N Tapia. Time-warping invariants of multidimensional time series.
arXiv preprint arXiv:1906.05823, 2019.
Garoe Dorta, Sara Vicente, Lourdes Agapito, Neill DF Campbell, and Ivor Simpson. Structured
uncertainty prediction networks. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 5477-5485, 2018.
K. Ebrahimi-Fard and F. Patras. Cumulants, free cumulants and half-shuffles. Proceedings of the
Royal Society, 2015.
Vincent Fortuin, Dmitry Baranchuk, Gunnar Ratsch, and StePhan Mandt. GP-VAE: Deep ProbabiliS-
tic time series imputation. In International Conference on Artificial Intelligence and Statistics,
pp. 1651-1661. PMLR, 2020.
Samuel J. Gershman and Noah D. Goodman. Amortized inference in probabilistic reasoning. Cog-
nitive Science, 36, 2014.
R Giles. A generalization of the strict topology. Transactions of the American Mathematical Society,
1971.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256, 2010.
AL Goldberger, LAN Amaral, L Glass, JM Hausdorff, P Ch Ivanov, RG Mark, JE Mietus,
GB Moody, CK Peng, and HE Stanley. Components of a new research resource for complex
physiologic signals. PhysioBank, PhysioToolkit, and Physionet, 2000.
Benjamin Graham. Sparse arrays of signatures for online character recognition. arXiv preprint
arXiv:1308.0371, 2013.
Alex Graves and Jurgen Schmidhuber. Framewise phoneme classification with bidirectional lstm
and other neural network architectures. Neural Networks, 18(5):602 - 610, 2005.
Alex Graves, Navdeep Jaitly, and Abdel-rahman Mohamed. Hybrid speech recognition with deep
bidirectional lstm. In 2013 IEEE workshop on automatic speech recognition and understanding,
pp. 273-278. IEEE, 2013a.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recur-
rent neural networks. In 2013 IEEE international conference on acoustics, speech and signal
processing, pp. 6645-6649. IEEE, 2013b.
11
Published as a conference paper at ICLR 2021
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
I. Higgins, Lolc Matthey, A. Pal, C. Burgess, Xavier Glorot, M. Botvinick, S. Mohamed, and
Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational
framework. In ICLR, 2017.
Max Horn, Michael Moor, Christian Bock, Bastian Rieck, and Karsten Borgwardt. Set functions for
time series. In ICML, 2020.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4
(2):251-257, 1991.
Ming Hou, Jiajia Tang, Jianhai Zhang, Wanzeng Kong, and Qibin Zhao. Deep multimodal multi-
linear fusion with high-order polynomial pooling. In Advances in Neural Information Processing
Systems, pp. 12136-12145, 2019.
Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and Pierre-Alain
Muller. Deep learning for time series classification: a review. Data Mining and Knowledge
Discovery, 33(4):917-963, Jul 2019. ISSN 1573-756X.
Fazle Karim, Somshubra Majumdar, Houshang Darabi, and Samuel Harford. Multivariate lstm-fcns
for time series classification. Neural Networks, 116:237 - 245, 2019. ISSN 0893-6080.
Isak Karlsson, Panagiotis Papapetrou, and Henrik Bostrom. Generalized random shapelet forests.
Data Min. Knowl. Discov., 30(5):1053-1085, September 2016. ISSN 1384-5810.
Nitish Shirish Keskar and Richard Socher. Improving generalization performance by switching from
adam to SGD. arXiv preprint arXiv:1712.07628, 2017.
Valentin Khrulkov, Oleksii Hrinchuk, and Ivan Oseledets. Generalized tensor models for recurrent
neural networks. arXiv preprint arXiv:1901.10801, 2019.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,
abs/1412.6980, 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Franz J KiraIy and Harald Oberhauser. Kernels for sequentially ordered data. Journal of Machine
Learning Research, 2019.
Jean Kossaifi, Zachary C Lipton, Aran Khanna, Tommaso Furlanello, and Anima Anandkumar.
Tensor regression networks. arXiv preprint arXiv:1707.08308, 2017.
Serge Lang. Algebra. Springer-Verlag New York, 2002.
C Leslie and R Kuang. Fast string kernels using inexact matching for protein sequences. Journal of
Machine Learning Research, 2004.
Yingzhen Li and Stephan Mandt. Disentangled sequential autoencoder, 2018.
Paul Pu Liang, Zhun Liu, Yao-Hung Hubert Tsai, Qibin Zhao, Ruslan Salakhutdinov, and Louis-
Philippe Morency. Learning representations from imperfect time series data via tensor rank reg-
ularization. arXiv preprint arXiv:1907.01011, 2019.
Rosanne Liu, Joel Lehman, Piero Molino, Felipe Petroski Such, Eric Frank, Alex Sergeev, and Jason
Yosinski. An intriguing failing of convolutional neural networks and the coordconv solution. In
Proceedings of the 32nd International Conference on Neural Information Processing Systems,
NIPS’18, pp. 9628-9639, Red Hook, NY, USA, 2018a. Curran Associates Inc.
Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshminarasimhan, Paul Pu Liang, Amir Zadeh, and
Louis-Philippe Morency. Efficient low-rank multimodal fusion with modality-specific factors.
arXiv preprint arXiv:1806.00064, 2018b.
12
Published as a conference paper at ICLR 2021
Terry Lyons. Rough paths, signatures and the modelling of functions on streams. arXiv preprint
arXiv:1405.4537, 2014.
Wesley J Maddox, Gregory Benton, and Andrew Gordon Wilson. Rethinking parameter counting in
deep models: Effective dimensionality revisited. arXiv preprint arXiv:2003.02139, 2020.
Dmytro Mishkin and Jiri Matas. All you need is a good init. arXiv preprint arXiv:1511.06422,
2015.
James Morrill, Adeline Fermanian, Patrick Kidger, and Terry Lyons. A generalised signature method
for time series. arXiv preprint arXiv:2006.00873, 2020.
Alfredo Nazabal, Pablo M Olmos, Zoubin Ghahramani, and Isabel Valera. Handling incomplete
heterogeneous data using vaes. arXiv preprint arXiv:1807.03653, 2018.
Daniel Neil, Michael Pfeiffer, and Shih-Chii Liu. Phased lstm: Accelerating recurrent network
training for long or event-based sequences. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29. Curran As-
sociates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/file/
5bce843dd76db8c939d5323dd3e54ec9- Paper.pdf.
P. Nemenyi. Distribution-free Multiple Comparisons. Princeton University, 1963. URL https:
//books.google.nl/books?id=nhDMtgAACAAJ.
Ivan V Oseledets. Tensor-train decomposition. SIAMJournal on Scientific Computing, 33(5):2295-
2317, 2011.
C Reutenauer. Free Lie Algebras. Clarendon press - Oxford, 1993.
Yulia Rubanova, Ricky T. Q. Chen, and David K Duvenaud. Latent ordinary differential equations
for irregularly-sampled time series. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-
Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, vol-
ume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/
paper/2019/file/42a6845a557bef704ad8ac9cb4461d43-Paper.pdf.
W. Rudin. Principles of Mathematical Analysis. Cambridge University Press, 1965.
H. Sakoe and S. Chiba. Dynamic programming algorithm optimization for spoken word recognition.
IEEE Transactions on Acoustics, Speech, and Signal Processing, 26(1):43-49, 1978.
Tim Sauer, James A Yorke, and Martin Casdagli. Embedology. Journal of statistical Physics, 65
(3-4):579-616, 1991.
Patrick Schafer and Ulf Leser. Multivariate time series classification with weasel+muse. ArXiv,
abs/1711.11343, 2017.
Mike Schuster and Kuldip K Paliwal. Bidirectional recurrent neural networks. IEEE transactions
on Signal Processing, 45(11):2673-2681, 1997.
Satya Narayan Shukla and Benjamin M Marlin. Interpolation-prediction networks for irregularly
sampled time series. arXiv preprint arXiv:1909.07782, 2019.
Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker, and Hermann Ney. Translation modeling
with bidirectional recurrent neural networks. In Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing (EMNLP), pp. 14-25, 2014.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with
neural networks. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and
K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27, pp.
3104-3112. Curran Associates, Inc., 2014. URL http://papers.nips.cc/paper/
5346-sequence-to-sequence-learning-with-neural-networks.pdf.
Floris Takens. Detecting strange attractors in turbulence. In Dynamical systems and turbulence,
Warwick 1980, pp. 366-381. Springer, 1981.
13
Published as a conference paper at ICLR 2021
Dacheng Tao, Xuelong Li, Xindong Wu, and Stephen J Maybank. General tensor discriminant anal-
ysis and gabor features for gait recognition. IEEE transactions on pattern analysis and machine
intelligence, 29(10):1700-1715, 2007.
C Toth and H Oberhauser. Bayesian learning from sequential data using gaussian processes with
signature covariances. ICML, 2020.
Ledyard R Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika, 31(3):
279-311, 1966.
Kerem Sinan Tuncel and Mustafa Gokce Baydogan. Autoregressive forests for multivariate time
series modeling. Pattern Recognition, 73:202-215, 2018.
Madeleine Udell and Alex Townsend. Why are big data matrices approximately low rank? SIAM
Journal on Mathematics of Data Science, 2019.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Z. Wang, W. Yan, and T. Oates. Time series classification from scratch with deep neural networks:
A strong baseline. In 2017 International Joint Conference on Neural Networks (IJCNN), pp.
1578-1585, 2017.
Christopher KI Williams and Carl Edward Rasmussen. Gaussian processes for machine learning,
volume 2. MIT press Cambridge, MA, 2006.
Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Tensor
fusion network for multimodal sentiment analysis. arXiv preprint arXiv:1707.07250, 2017.
Cheng Zhang, Judith BUtepage, Hedvig Kjellstrom, and Stephan Mandt. Advances in variational
inference. IEEE transactions on pattern analysis and machine intelligence, 41(8):2008-2026,
2018.
Qibin Zhao, Masashi Sugiyama, Longhao Yuan, and Andrzej Cichocki. Learning efficient tensor
representations with ring-structured networks. In ICASSP 2019-2019 IEEE International Confer-
ence on Acoustics, Speech and Signal Processing (ICASSP), pp. 8608-8612. IEEE, 2019.
14