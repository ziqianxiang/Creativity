Published as a conference paper at ICLR 2021
Faster Binary Embeddings for Preserving
Euclidean Distances
Jinjie Zhang & Rayan Saab *
Department of Mathematics, HahcIoglU Data Science Institute
University of California San Diego
{jiz003, rsaab}@ucsd.edu
Ab stract
We propose a fast, distance-preserving, binary embedding algorithm to transform
a high-dimensional dataset T ⊆ Rn into binary sequences in the cube {±1}m.
When T consists of well-spread (i.e., non-sparse) vectors, our embedding method
applies a stable noise-shaping quantization scheme to Ax where A ∈ Rm×n is
a sparse Gaussian random matrix. This contrasts with most binary embedding
methods, which usually use x 7→ sign(Ax) for the embedding. Moreover, we
show that Euclidean distances among the elements of T are approximated by the
`1 norm on the images of {±1}m under a fast linear transformation. This again
contrasts with standard methods, where the Hamming distance is used instead.
Our method is both fast and memory efficient, with time complexity O(m) and
space complexity O(m) on well-spread data. When the data is not well-spread,
we show that the approach still works provided that data is transformed via a
Walsh-Hadamard matrix, but now the cost is O(n log n) per data point. Further,
we prove that the method is accurate and its associated error is comparable to
that of a continuous valued Johnson-Lindenstrauss embedding plus a quantization
error that admits a polynomial decay as the embedding dimension m increases.
Thus the length of the binary codes required to achieve a desired accuracy is quite
small, and we show it can even be compressed further without compromising the
accuracy. To illustrate our results, we test the proposed method on natural images
and show that it achieves strong performance.
1	Introduction
Analyzing large data sets of high-dimensional raw data is usually computationally demanding and
memory intensive. As a result, it is often necessary as a preprocessing step to transform data into
a lower-dimensional space while approximately preserving important geometric properties, such as
pairwise `2 distances. As a critical result in dimensionality reduction, the Johnson-Lindenstrauss
(JL) lemma (Johnson & Lindenstrauss, 1984) guarantees that every finite set T ⊆ Rn can be (lin-
early) mapped to a m = O(-2 log(|T |)) dimensional space in such a way that all pairwise dis-
tances are preserved up to an -Lipschitz distortion. Additionally, there are many significant results
to speed up the JL transform by introducing fast embeddings, e.g. (Ailon & Chazelle, 2009; Ailon
& Liberty, 2013; Krahmer & Ward, 2011; Nelson et al., 2014), orby using sparse matrices (Kane &
Nelson, 2014; 2010; Clarkson & Woodruff, 2017). Such fast embeddings can usually be computed
in O(n log n) versus the O(mn) time complexity of JL transforms that rely on unstructured dense
matrices.
1.1	Related Work
To further reduce memory requirements, progress has been made in nonlinearly embedding high-
dimensional sets T ⊆ Rn to the binary cube {-1, 1}m with m n, a process known as binary
embedding. Provided that dι(∙, ∙) is a metric on Rn, a distace preserving binary embedding is a map
*The Python source code of our paper: https://github.com/jayzhang0727/Faster-Binary-Embeddings-for-
Preserving-Euclidean-Distances.git
1
Published as a conference paper at ICLR 2021
f : T → {-1,1}m and a function d2(∙, ∙) on {-1,1}m X {-1,1}m to approximate distances, i.e.,
|d2 (f (x), f(y)) - d1(x, y)| ≤ α,	for ∀x, y ∈ T.	(1)
The potential dimensionality reduction (m n) and 1-bit representation per dimension imply that
storage space can be considerably reduced and downstream applications like learning and retrieval
can happen directly using bitwise operations. Most existing nonlinear mappings f in (1) are gener-
ated using simple memory-less scalar quantization (MSQ). For example, given a set of unit vectors
T ⊆ Sn-1 with finite size |T|, consider the map
qx := f(x) = sign(Gx)	(2)
where G ∈ Rm×n is a standard Gaussian random matrix and sign(∙) returns the element-wise
sign of its argument. Let dι(x, y) = ∏1 arccos(kxk-1kyk-1hx, y))be the normalized angular
distance and d2(qχ, qy) = *∣∣qχ 一 qy ∣∣ι be the normalized Hamming distance. Then, Yi et al.
(2015) show that (1) holds with probability at least 1 一 η if m & α-2 log(∣T∣∕η), so one can
approximate geodesic distances with normalized Hamming distances. While this approach achieves
optimal bit complexity (up to constants) (Yi et al., 2015), it has been observed in practice that m
is usually around O(n) to guarantee reasonable accuracy (Gong et al., 2013; SanChez & Perronnin,
2011; Yu et al., 2014). Much like linear JL embedding techniques admit fast counterparts, fast
binary embedding algorithms have been developed to significantly reduce the runtime of binary
embeddings (Gong et al., 2012b; Liu et al., 2011; Gong et al., 2012a; 2013; Li et al., 2011; Raginsky
& Lazebnik, 2009). Indeed, fast JL transforms (FJLT) and Gaussian Toeplitz matrices (Yi et al.,
2015), structured hashed projections (Choromanska et al., 2016), iterative quantization (Gong et al.,
2012b), bilinear projection (Gong et al., 2013), circulant binary embedding (Yu et al., 2014; Dirksen
& Stollenwerk, 2018; 2017; Oymak et al., 2017; Kim et al., 2018), sparse projection (Xia et al.,
2015), and fast orthogonal projection (Zhang et al., 2015) have all been considered.
These methods can decrease time complexity to O(n log n) operations per embedding, but still suffer
from some important drawbacks. Notably, due to the sign function, these algorithms completely
discard all magnitude information, as sign(Ax) = sign(A(αx)) for all α > 0. So, all points in the
same direction embed to the same binary vector and cannot be distinguished. Even if one settles for
recovering geodesic distances, using the sign function in (2) is an instance of MSQ so the estimation
error α in (1) decays slowly as the number of bits m increases (Yi et al., 2015).
In addition to the above data independent approaches, there are data dependent embedding methods
for distance recovery, including product quantization (Jegou et al., 2010; Ge et al., 2013), LSH-
based methods (Andoni & Indyk, 2006; Shrivastava & Li, 2014; Datar et al., 2004) and iterative
quantization (Gong et al., 2012c). Their accuracy, which can be excellent, nevertheless depends
on the underlying distribution of the input dataset. Moreover, they may be associated with larger
time and space complexity for embedding the data. For example, product quantization performs
k-means clustering in each subspace to find potential centroids and stores associated lookup tables.
LSH-based methods need random shifts and dense random projections to quantize each input data
point.
Recently Huynh & Saab (2020) resolved these issues by replacing the simple sign function with a
Sigma-Delta (Σ∆) quantization scheme, or alternatively other noise-shaping schemes (see (Chou &
Gunturk, 2016)) whose properties will be discussed in Section 3. They use the binary embedding
qx := Q(DBx)	(3)
where Q is now a stable Σ∆ quantization scheme, D ∈ Rm×m is a diagonal matrix with random
signs, and B ∈ Rm×n are specific structured random matrices. To give an example of Σ∆ quan-
tization in this context, consider w := DBx. Then the simplest Σ∆ scheme computes qx via the
following iteration, run for i = 1, ..., m:
{uo = 0,
qx(i) = sign(wi + ui-1),	(4)
ui = ui-1 + wi 一 qi .
The choices of B in (Huynh & Saab, 2020) allow matrix vector multiplication to be implemented
using the fast Fourier transform. Then the original Euclidean distance ∣x 一 y∣2 can be recovered
via a pseudo-metric on the quantized vectors given by
dVe (qx, qy) := ∣Ve (qx 一 qy)∣2	(5)
2
Published as a conference paper at ICLR 2021
where V ∈ Rp×m is a “normalized condensation operator”, a sparse matrix that can be applied fast
(see Section 3). Regarding the complexity of applying (3) to a single x ∈ Rn, note that x 7→ DBx
has time complexity O(n log n) while the quantization map needs O(m) time and results in an m
bit representation. So when m ≤ n, the total time complexity for (3) is around O(n log n).
1.2	Methods and Contributions
We extend these results by replacing DB in (3) by a sparse Gaussian matrix A ∈ Rm×n so that now
qx := Q(Ax).	(6)
Given scaled high-dimensional data T ⊂ Rn contained in the `2 ball B2n (κ) with radius κ, we put
forward Algorithm 1 to generate binary sequences and Algorithm 2 to compute estimates of the
Euclidean distances between elements of T via an 'ι-norm rather than '2-norm. The contribution of
this work is threefold. First, we prove Theorem 1.1 quantifying the performance of our algorithms.
Algorithm 1: Fast Binary Embedding for Finite T
Input: T = {x(j)}jk=1 ⊆ B2n(κ)	. Data points in `2 ball
Generate A ∈ Rm×n as in Definition 2.2	. Sparse Gaussian matrix A
for j — 1 to k do
Zcj) - Axj)
_ q(j) = Q(Zj))	. Stable Σ∆ quantizer Q as in (4), or more generally (21).
Output: Binary sequences B = {qcj)}k=ι ⊆ {-1,1}m
	
Algorithm 2: `2 Norm Distance Recovery	
Input: qci) , qcj ) ∈ B y(i) — V q(i) y(j) — V q(j) Output: ∣y(i) - y(j) ∣1	. Binary sequences produced by Algorithm 1 . Condense the components of q . Approximation of ∣x(i) - x(j) ∣2
Theorem 1.1 (Main result). Let T ⊆ Rn be a finite, appropriately scaled set with elements satis-
fying ∣∣xk∞ = O(n-1/2kxk2) and ∣∣χk2 ≤ K < 1. If m & P := Ω(e-2 log(∣T∣2∕δ)) and r ≥ 1 is
the integer order of Q, then with probability 1 - 2δ on the draw of the sparse Gaussian matrix A,
the following holds uniformly over all x, y in T: Embedding x, y into {-1, 1}m using Algorithm 1,
and estimating the associated distance between them using Algorithm 2 yields the error bound
dVe (qx , qy ) - ∣x
-r+1/2
+ ∣x - y∣2
where c > 0 is a constant.
Theorem 1.1 yields an approximation error bounded by two components, one due to quantization and
another that resembles the error from a linear JL embedding into a p-dimensional space. The latter
part is essentially proportional top-1/2, while the quantization component decays polynomially fast
in m, and can be made harmless by increasing m. Moreover, the number of bits m & -2 log(|T |)
achieves the optimal bit complexity required by any oblivious random embedding that preserves Eu-
clidean or squared Euclidean distance, see Theorem 4.1 in (Dirksen & Stollenwerk, 2020). Theorem
4.2 is a more precise version of Theorem 1.1, with all quantifiers, and scaling parameters specified
explicitly, and with a potential modification to A that enables the result to hold for arbitrary (not
necessarily well-spread) finite T, at the cost of increasing the computational complexity of embed-
ding a point to O(n log n). We also note that if the data did not satisfy the scaling assumption of
Theorems 1.1 and 4.2, then one can replace {-1, 1} by {-C, C}, and the quantization error would
scale by C .
Second, due to the sparsity of A, (6) can be computed much faster than (3), when restricting our
results to “well-spread” vectors x, i.e., those that are not sparse. On the other hand, in Section 5,
we show that Algorithm 1 achieves O(m) time and space complexity in contrast with the common
O(n log n) runtime of fast binary embeddings, e.g., (Gong et al., 2013; Yi et al., 2015; Yu et al.,
3
Published as a conference paper at ICLR 2021
2014; Dirksen & Stollenwerk, 2018; 2017; Huynh & Saab, 2020) that rely on fast JL transforms or
circulant matrices. Meanwhile, Algorithm 2 requires only O(m) runtime.
Third, Definition 2.3 shows that V is sparse and essentially populated by integers bounded by
(m/p)r where r, m, p are as in Theorem 1.1. In Section 5, we note that each y(i) = Ve q(i) (and
the distance query), can be represented by O(p log2(m/p)) bits, instead of m bits, without affecting
the reconstruction accuracy. This is a consequence of using the '1 -norm in Algorithm 2. Had We
instead used an '2-norm, we would have required O(p(log2(m∕p))2) bits.
Finally, We remark that While the assumption that the vectors x are Well-spread (i.e. kxk∞ =
O(n-1/2 k
xk2)) may appear restrictive, there are important instances where it holds. Natural im-
ages seem to be one such case, as are random Fourier features (Rahimi & Recht, 2007). Sim-
ilarly, Gaussian (and other subgaussian) random vectors satisfy a slightly weakened kxk∞ =
O(log(n)n-1/2 kxk2 ) assumption with high probability, and one can modify our construction by
slightly reducing the sparsity of A (and slightly increasing the computational cost) to handle such
vectors. On the other hand, if the data simply does not satisfy such an assumption, one can still
apply Theorem 4.2 part (ii), but now the complexity of embedding a point is O(n log n).
2	Preliminaries
2.1	Notation and definitions
Throughout, f (n) = O(g(n)) and f (n) = Ω(g(n)) mean that |f (n)| is bounded above and below
respectively by a positive function g(n) up to constants asymptotically; that is, lim supn→∞ f (：)| <
∞. Similarly, we use f(n) = Θ(g(n)) to denote that f(n) is bounded both above and below by a
positive function g(n) up to constants asymptotically. We next define operator norms.
Definition 2.1. Let α, β ∈ [1, ∞] be integers. The (α, β) operator norm of K ∈ Rm×n is
kKl∣α,β = maxx=0 k£；卜.
We now introduce some notation and definitions that are relevant to our construction.
Definition 2.2 (Sparse Gaussian random matrix). Let A = (aij) ∈ Rm×n be a random matrix with
i.i.d. entries such that &j is 0 with probability 1 一 S and is drawn from N(0, ɪ) with probability s.
We adopt the definition of a condensation operator of Chou & GUntUrk (2016); Huynh & Saab
(2020).
Definition 2.3 (Condensation operator). Let p, r, λ be fixed positive integers such that λ = rλ-r+1
λ
for some integer λ. Let m = λp and v be a row vector in Rλ whose entry vj is the j -th coefficient
e
of the polynomial (1 + z + . . . + zλ-1)r. Define the condensation operator V ∈ Rp×m by
v
V = Ip 0 V =	...
v
λ
For example, when r = 1, λ = λ, and v ∈ Rλ is simply the vector of all ones. The normalized
condensation operator is given by
V
三V
Pkvk2
The fast JL transform was first studied by Ailon & Chazelle (2009). It admits many variants and
improvements, e.g. (Krahmer & Ward, 2011; Matousek, 2008). The idea is that given any X ∈ Rn
we use a fast “Fourier-like” transform, like the Walsh-Hadamard transform, to distribute the total
mass (i.e. ||x||2) of x relatively evenly to its coordinates.
Definition 2.4 (FJLT). The fast JL transform can be obtained by
Φ := AHD ∈ Rm×n.	(7)
4
Published as a conference paper at ICLR 2021
Here, A ∈ Rm×n is a sparse Gaussian random matrix, as in Definition 2.2, while H ∈ Rn×n is
a normalized Walsh-Hadamard matrix defined by Hij = n-1/2(-1)hi-1,j-1i where hi, ji is the
bitwise dot product of the binary representations of the numbers i and j . Finally, D ∈ Rn×n is
diagonal with diagonal entries drawn independently from {-1, 1} with probability 1/2 for each.
2.2	condensed Johnson-Lindenstrauss Transforms
Definition 2.5. When V is a condensation operator, and A is a sparse Gaussian, we refer to V A as
a condensed sparse JL transform (CSJLT). When A is replaced by Φ as in Definition 2.4 we refer
to V Φ as a condensed fast JL transform (CFJLT).
The definition above is justified by the following lemma (see Appendix B for the proof).
Lemma 2.6 (CJLT lemma). Let T be a finite subset of Rn, λ ∈ N, E ∈ (0,1), δ ∈ (0,1), P =
O(e-2 log(∣T∣2∕δ)) ∈ N and m = λp. Let V ∈ Rp×m be as in Definition 2.3, A ∈ Rm×n
be the sparse Gaussian matrix in Definition 2.2 with S = Θ(e-1η-1(kvk∞/∣∣v∣∣2)2) ≤ 1, and
Φ = AHD ∈ Rm×n be the FJLTin Definition 2.4 with S = Θ(e-1η-1(kvk∞∕kvk2)2 log n) ≤ 1.
IfT consists of well-spread vectors, that is, kxk∞ = O(n-1/2 kxk2) for all x ∈ T, then
kVe A(x - y)k1 - kx - yk2 ≤ Ekx - yk2	(8)
holds uniformly for all x, y ∈ T with probability at least 1 - δ. IfT is finite but arbitrary, then
kVe Φ(x - y)k1 - kx - yk2 ≤ Ekx - yk2	(9)
holds uniformly for all x, y ∈ T with probability at least 1 - δ.
So T ⊆ Rn is embedded into Rp with pairwise distances distorted at most E, where p =
O(E-2 log |T|) as one would expect from a JL embedding. This will be needed to guarantee the
accuracy associated with our embeddings algorithms. Note that the bound on p does not require
extra logarithmic factors, in contrast to the bound O(E-2 log |T| log4 n) in (Huynh & Saab, 2020).
3	Sigma-Delta quantization
An r-th order Σ∆ quantizer Q(r) : Rm → Am maps an input signal y = (yi)im=1 ∈ Rm to a
quantized sequence q = (qi)im=1 ∈ Am via a quantization rule ρ and the following iterations
{U0 = u-1 = ... = U1-r = 0,
qi = Q(ρ(yi , ui-1, . . . , ui-r )) for i = 1, 2, . . . , m,	(10)
Pru=y-q
where Q(y) = arg minv∈A |y - v| is the scalar quantizer related to alphabet A and P ∈ Rm×m is
the first order difference matrix defined by
if i = j,
ifi=j+1,
otherwise.
Note that (10) is amenable to an iterative update of the state variables ui as
Pr U = y — q Q⇒ Ui
ui-j + yi -
qi,
i = 1, 2, . . . , m.
(11)
Definition 3.1. A quantization scheme is Stable if there exists μ > 0 such that for each input with
∣∣yk∞ ≤ μ, the state vector U ∈ Rm satisfies ∣∣u∣∣∞ ≤ C. Crucially, μ and C do not depend on m.
Stability heavily depends on the choice of quantization rule and is difficult to guarantee for arbitrary
ρ in (10) when the alphabet is small, as is the case of 1-bit quantization where A = {±1}. When
r = 1 andA = {±1}, the simplest stable Σ∆ scheme Q(1) : Rm → Am is equipped with the greedy
quantization rule ρ(yi, ui-1) := ui-1+yi giving the simple iteration (4) from the introduction, albeit
with yi replacing wi . A description of the design and properties of stable Q(r) with r ≥ 2 can be
found in Appendix C.
5
Published as a conference paper at ICLR 2021
4	Main Results
The ingredients that make our construction work are a JL embedding followed by Σ∆ quantization.
Together these embed points into {±1}m, but it remains to define a pseudometric so that we may
approximate Euclidean distances by distances on the cube. We now define this pseudometric.
Definition 4.1. Let Am = {±1}m and let V ∈ Rp×m with p ≤ m. We define dV on Am × Am as
dV(q1,q2) = kV (q1 - q2)k1 ∀q1,q2 ∈ Am.
We now present our main result, a more technical version of Theorem 1.1, proved in Appendix D.
Theorem 4.2 (Main result). Let λ, r ∈ N, e ∈ (0,1), δ ∈ (0,1), β = Ω(log(∣T∣∕δ)) > 0,
μ ∈ (0,1), P = Ω(e-2 log(∣T∣2∕δ)) ∈ N, and m = λp. Let V ∈ Rp×m be as in Definition 2.3,
A ∈ Rm×n be the sparse Gaussian matrix in Definition 2.2 with S = Θ(e-1n-1(kvk∞∕kvk2)2) ≤
1, and Φ be the FJLT in Definition 2.4 with s = Θ(-1 n-1 (kv k∞ /kv k2)2 log n) ≤ 1.
Let T be a finite subset of B2n(κ) := {x ∈ Rn : kxk2 ≤ κ} and suppose that
/	μ
K ≤ ---,	.
― 2VZe + log(2m)
Defining the embedding maps f1 : T → {±1}m by f1 = Q(r) ◦ A and f2 : T → {±1}m by
f2 = Qm ◦ Φ, there exists a constant C (μ, r) such that the following are true:
(i)	If the elements ofT satisfy kxk∞ = O(n-1/2 kxk2), then the bound
∣dv(fι(χ),fι(y)) - kχ - yk2∣ ≤ C(μ,r)λ-r+1/2 + d∣χ - yk2	(12)
holds uniformly for all x, y ∈ T with probability exceeding 1 一 δ 一 ∣T∣e-β.
(ii)	On the other hand, for arbitrary T ⊂ B2n (κ)
∣dV(f2(x),f2(y)) - kx - yk2∣ ≤ C(μ,r)λ-r+1/2 + e∣∣x - y∣∣2	(13)
holds uniformly for any x, y ∈ T with probability exceeding 1 一 δ — 2∣T∣e-β.
Under the assumptions of Theorem 4.2, we have
log(∣T∣2∕δ)) . ɪ
p J 〜√p.
By (12), (13) and (14), we have that with high probability the inequality
∣dv(fi(χ),fi(y)) - kx -yk2∣ ≤ C(μ,r)(ʒp)+ / + dlχ - yk2
≤ C(μ,r)(m)-r72 + 2κe
(14)
≤ C(μ,r)
-r+1/2	μ
P + log(2m)
(15)
holds uniformly for x, y ∈ T. The first error term in (15) results from Σ∆ quantization while the
second error term is caused by the CJLT. So the term O((m∕p)-r+1/2) dominates when λ = m/p
is small. If m/p is sufficiently large, the second term O(1∕√p) becomes dominant.
5	Computational and Space Complexity
In this section, we assume that T = {x(j)}jk=1
⊆ Rn consists of well-spread vectors. Moreover, we
will focus on stable r-th order Σ∆ schemes Q(r) : Rm → Am with A = {-1, 1}. By Definition
2.3, when r = 1 we have v = (1, 1, . . . , 1) ∈ Rλ, while when r = 2, v = (1, 2, . . . , eλ - 1, λe, eλ -
6
Published as a conference paper at ICLR 2021
1,..	., 2,1) ∈ Rλ. Ingeneral, kv∣J∞∕kvk2 = O(λ-1/2) holds for all r ∈ N. We also assume
that S = Θ(e-1n-1(kvk∞∕kvk2)2) = Θ(e-1n-1λ-1) ≤ 1 as in Theorem 4.2. We consider
b-bit floating-point or fixed-point representations for numbers. Both entail the same computational
complexity for computing sums and products of two numbers. Addition and subtraction require O(b)
operations while multiplication and division require M(b) = O(b2) operations via “standard” long
multiplication and division. Multiplication and division can be done more efficiently, particularly
for large integers and the best known methods (and best possible up to constants) have complexity
M(b) = O(b log b) (Harvey & Van Der Hoeven, 2019). We also assume random access to the
coordinates of our data points.
Embedding complexity. For each data point x(j) ∈ T, one can use Algorithm 1 to quantize it.
Since A has sparsity constant s = Θ(-1n-1λ-1) and -1 = O(p1/2) by (14), and since λ = m/p,
computing Ax(j) needs O(snm) = O(λ-1-1m) = O(p3/2) time. Additionally, it takes O(m)
time to quantize Ax(j) based on (21). When p3/2 ≤ m, Algorithm 1 can be executed in O(m)
for each x(j) . Because A has O(snm) = O(m) nonzero entries, the space complexity is O(m)
bits per data point. Note that the big O notation here hides the space complexity dependence on the
bit-depth b of the fixed or floating point representation of the entries of A and x(j). This clearly has
no effect on the storage space needed for each q(j), which is exactly m bits.
Complexity of distance estimation. If one does not use embedding methods, storing T directly,
i.e., by representing the coefficients of each x(j) by b bits requires knb bits. Moreover, the resulting
computational complexity of estimating kx - yk22 where x, y ∈ T is O(nM(b)). On the other
hand, suppose we obtain binary sequences B = {q(j)}jk=1 ⊆ Am by performing Algorithm 1 on T.
Using our method with accuracy guaranteed by Theorem 4.2, high-dimensional data points T ⊆ Rn
are now transformed into short binary sequences, which only require km bits of storage instead
of knb bits. Algorithm 2 can be applied to recover the pairwise `2 distances. Note that V is the
normalization of an integer valued matrix V = Ip 0 V (by Definition 2.3) and q(i) ∈ Am is a
binary vector. So, by storing the normalization factor separately, we can ignore it when considering
runtime and space complexity. Thus we observe:
1.	The number of bits needed to represent each entry of v is at most log2(kvk∞) ≈ (r -
1) log2 λ = O(log2 λ) when r > 1 and O(1) when r = 1. So the computation of y(i) =
Ve q(i) ∈ Rp only involves m additions or subtractions of integers represented by O(log2 λ)
bits and thus the time complexity in computing y(i) is O(m log2 λ).
2.	Each of the p entries of y(i) is the sum of λ terms each bounded by λr-1. We can store
y(i) in O(p log2 λ) bits.
3.	Computing ky(i) - y(j) k1 needs O(p log2 λ) time and O(p log2 λ) bits.
So we use O (p log2 λ) bits to recover each pairwise distance kx(i) - x(j) k2 in O(m log2 λ) time.
	MethOd		Time	Space	Storage	Query Time
Gaussian Toeplitz (Yi et al., 2015)	O(n log n)	O(n)	O(m)	O(m)
Bilinear (Gong et al., 2013)	O(n√m)	O( ʌ/mn)	O(m)	O(m)
Circulant (Yu et al., 2014)	O(n log n)	-Ο(n)	O(m)	O(m)
BOE or PCE? (HUynh & Saab, 2020)	O(n log n)	O(n)	O(P log2 λ)	O(pM(log2 λ))
Our Algorithm? (on well-spread T)	O(m)	O(m)	O(P log2 λ)一	O(P log2 λ)
These algorithms recover Euclidean distances and others recover geodesic distances.
Table 1: Here “Time” is the time needed to embed a data point, while “Space” is the space needed to
store the embedding matrix. “Storage” contains the memory usage to store each encoded sequence.
“Query time” is the time complexity of pairwise distance estimation.
Comparisons with baselines. In Table 1, we compare our algorithm with various JL-based methods
from Section 1. Here n is the input dimension, m is the embedding dimension (and number of bits),
and P = m∕λ is the length of encoded sequences y = Vq. In our case, We use O(P log2 λ) to store
y = Vq. See Appendix E for a comparison With product quantization.
7
Published as a conference paper at ICLR 2021
(a) MAPE of Method 1 (r = 1)
(b) MAPE of Method 2 (r = 1)
(c) MAPE of Method 1 (r = 2)
(d) MAPE of Method 2 (r = 2)
Figure 1: Plots of `2 distance reconstruction error when r = 1, 2
6 Numerical Experiments
To illustrate the performance of our fast binary embedding (Algorithm 1) and `2 distance recovery
(Algorithm 2), we apply them to real-world datasets: Yelp open dataset1, ImageNet (Deng et al.,
2009), Flickr30k (Plummer et al., 2017), and CIFAR-10 (Krizhevsky et al., 2010). All images are
converted to grayscale and resampled using bicubic interpolation to size 128 × 128 for images from
Yelp, ImageNet, and Flickr30k and 32 × 32 for images from CIFAR-10. So, each can be represented
by a 16384-dimensional or 1024-dimensional vector. The results are reported here and in Appendix
A. We consider the two versions of our fast binary embedding algorithm from Theorem 4.2:
Method 1. We quantize FJLT embeddings Φx, and recover distances based on Algorithm 2.
Method 2. We quantize sparse JL embeddings Ax and recover distances by Algorithm 2.
In order to test the performance of our algorithm, we compute the mean absolute percentage error
(MAPE) of reconstructed `2 distances averaged over all pairwise data points, that is,
..， 、… .. ..
2 X kV(qx - qy)kl - kx - yk2
k (k -1) X ⅛T	kx - yk2	.
x,y∈
Experiments on the Yelp dataset. To give a numerical illustration of the relation among the length
m of the binary sequences, embedding dimension p, and order r, as compared to the upper bound
in (15), we use both Method 1 and Method 2 on the Yelp dataset. We randomly sample k = 1000
images and scale them by the same constant so all data points are contained in the `2 unit ball. The
scaled dataset is denoted by T. Based on Theorem 4.2, we set n = 16384 and s = 1650/n ≈ 0.1.
For each fixed p, we apply Algorithm 1 and Algorithm 2 for various m. We present our experimental
results for stable Σ∆ quantization schemes, given by (21), with r = 1 and r = 2 in Figure 1. For
r = 1, we observe that the curve with small p quickly reaches an error floor while with high p the
error decays like m-1/2 and eventually reach a lower floor. The reason is that the first error term
in (15) is dominant when m/p is relatively small but the second error term eventually dominates as
1Yelp open dataset: https://www.yelp.com/dataset
8
Published as a conference paper at ICLR 2021
(a) MAPE of Method 1 (p = 64)
(b) MAPE of Method 2 (p = 64)
(c) MAPE of Method 1 (p = p(m))
Figure 2: Plots of `2 distance reconstruction error with fixed p = 64 and optimal p = p(m)
(d) MAPE of Method 2 (p = p(m))
m becomes larger and larger. When r = 2 the error curves decay faster and eventually achieve the
same flat error because now the first term in (15) has power -3/2 while the second flat error term is
independent of r. Moreover, the performance of Method 2 is very similar to that of Method 1.
Next, we illustrate the relationship between the quantization order r and the number of measure-
ments m in Figure 2. The curves obtained directly from an unquantized CFJLT (resp. CSJLT) as
in Lemma 2.6, with m = 256, 512, 1024, 2048, 4096, and p = 64 are used for comparison against
the quantization methods. The first row of Figure 2 depicts the mean squared relative error when
p = 64 is fixed for all distinct methods. It shows that stable quantization schemes with order r > 1
outperform the first order greedy quantization method, particularly when m is large. Moreover,
both the r = 2 and r = 3 curves converge to the CFJLT/CSJLT result as m goes to 4096. Note
that by using a quarter of the original dimension, i.e. m = 4096, our construction achieves less
than 10% error. Furthermore, if we encode V q as discussed in Section 5, then we need at most
rp log2 λ = 64r log2(4096/64) = 384r bits per image, which is . 0.023 bits per pixel.
For our final experiment, we illustrate that the performance of the proposed approach can be further
improved. Note that the choice of p only affects the distance computation in Algorithm 2 and
does not appear in the embedding algorithm. In other words, one can vary p in Algorithm 2 to
improve performance. This can be done either analytically by viewing the right hand side of (15)
as a function of p and optimizing for p (up to constants). It can also be done empirically, as we
do here. Following this intuition, if we vary p as a function of m, and use the empirically optimal
p := p(m) in the construction of V , then we obtain the second row of Figure 2 where the choice
r = 3 exhibits lower error than other quantization methods. Note that the decay rate, as a function
of m, very closely resembles that of the unquantized JL embedding particularly for higher orders r
(as one can verify by optimizing the right hand side of (15)).
Acknowledgments
Our work was supported in part by NSF Grant DMS-2012546 and a UCSD senate research award.
The authors would like to thank Sjoerd Dirksen for inspiring discussions and suggestions.
9
Published as a conference paper at ICLR 2021
References
Nir Ailon and Bernard Chazelle. The fast johnson-lindenstrauss transform and approximate nearest
neighbors. SIAM Journal on computing, 39(1):302-322, 2009.
Nir Ailon and Edo Liberty. An almost optimal unrestricted fast johnson-lindenstrauss transform.
ACM Transactions on Algorithms (TALG), 9(3):1-12, 2013.
Alexandr Andoni and Piotr Indyk. Near-optimal hashing algorithms for approximate nearest neigh-
bor in high dimensions. In 2006 47th annual IEEE symposium on foundations of computer science
(FOCS’06), pp. 459-468. IEEE, 2006.
Anna Choromanska, Krzysztof Choromanski, Mariusz Bojarski, Tony Jebara, Sanjiv Kumar, and
Yann LeCun. Binary embeddings with structured hashed projections. In International Conference
on Machine Learning, pp. 344-353, 2016.
Evan Chou and C Sinan Gunturk. Distributed noise-shaping quantization: I. beta duals of finite
frames and near-optimal quantization of random measurements. Constructive Approximation, 44
(1):1-22, 2016.
Kenneth L Clarkson and David P Woodruff. Low-rank approximation and regression in input spar-
sity time. Journal of the ACM (JACM), 63(6):1-45, 2017.
Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S Mirrokni. Locality-sensitive hashing
scheme based on p-stable distributions. In Proceedings of the twentieth annual symposium on
Computational geometry, pp. 253-262, 2004.
Ingrid Daubechies and Ron DeVore. Approximating a bandlimited function using very coarsely
quantized data: A family of stable sigma-delta modulators of arbitrary order. Annals of mathe-
matics, 158(2):679-710, 2003.
Percy Deift, Felix Krahmer, and C Sinan Gunturk. An optimal family of exponentially accurate
one-bit sigma-delta quantization schemes. Communications on Pure and Applied Mathematics,
64(7):883-919, 2011.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
Sjoerd Dirksen and Alexander Stollenwerk. Fast binary embeddings with gaussian circulant ma-
trices. In 2017 International Conference on Sampling Theory and Applications (SampTA), pp.
231-235. IEEE, 2017.
Sjoerd Dirksen and Alexander Stollenwerk. Fast binary embeddings with gaussian circulant matri-
ces: improved bounds. Discrete & Computational Geometry, 60(3):599-626, 2018.
Sjoerd Dirksen and Alexander Stollenwerk. Binarized johnson-lindenstrauss embeddings. arXiv
preprint arXiv:2009.08320, 2020.
Tiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun. Optimized product quantization for approximate
nearest neighbor search. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 2946-2953, 2013.
Yunchao Gong, Sanjiv Kumar, Vishal Verma, and Svetlana Lazebnik. Angular quantization-based
binary codes for fast similarity search. In Advances in neural information processing systems, pp.
1196-1204, 2012a.
Yunchao Gong, Svetlana Lazebnik, Albert Gordo, and Florent Perronnin. Iterative quantization: A
procrustean approach to learning binary codes for large-scale image retrieval. IEEE transactions
on pattern analysis and machine intelligence, 35(12):2916-2929, 2012b.
Yunchao Gong, Svetlana Lazebnik, Albert Gordo, and Florent Perronnin. Iterative quantization: A
procrustean approach to learning binary codes for large-scale image retrieval. IEEE transactions
on pattern analysis and machine intelligence, 35(12):2916-2929, 2012c.
10
Published as a conference paper at ICLR 2021
Yunchao Gong, Sanjiv Kumar, Henry A Rowley, and Svetlana Lazebnik. Learning binary codes
for high-dimensional data using bilinear projections. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 484-491, 2013.
C Sinan Gunturk. One-bit sigma-delta quantization with exponential accuracy. Communications
on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical
Sciences, 56(11):1608-1630, 2003.
David Harvey and Joris Van Der Hoeven. Integer multiplication in time o (n log n). Preprint, 2019.
Thang Huynh and Rayan Saab. Fast binary embeddings and quantized compressed sensing with
structured matrices. Communications on Pure and Applied Mathematics, 73(1):110-149, 2020.
Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor
search. IEEE transactions on pattern analysis and machine intelligence, 33(1):117-128, 2010.
William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert space.
Contemporary mathematics, 26(189-206):1, 1984.
Daniel M Kane and Jelani Nelson. A derandomized sparse johnson-lindenstrauss transform. arXiv
preprint arXiv:1006.3585, 2010.
Daniel M Kane and Jelani Nelson. Sparser johnson-lindenstrauss transforms. Journal of the ACM
(JACM), 61(1):1-23, 2014.
Saehoon Kim, Jungtaek Kim, and Seungjin Choi. On the optimal bit complexity of circulant binary
embedding. In AAAI, pp. 3423-3430, 2018.
Felix Krahmer and Rachel Ward. New and improved johnson-lindenstrauss embeddings via the
restricted isometry property. SIAM Journal on Mathematical Analysis, 43(3):1269-1281, 2011.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced re-
search). URL http://www. cs. toronto. edu/kriz/cifar. html, 5, 2010.
Ping Li, Anshumali Shrivastava, Joshua L Moore, and Arnd C Konig. Hashing algorithms for large-
scale learning. In Advances in neural information processing systems, pp. 2672-2680, 2011.
Wei Liu, Jun Wang, Sanjiv Kumar, and Shih-Fu Chang. Hashing with graphs. In ICML, 2011.
Jiri Matousek. On variants of the johnson-lindenstrauss lemma. Random Structures & Algorithms,
33(2):142-156, 2008.
Jelani Nelson, Eric Price, and Mary Wootters. New constructions of rip matrices with fast multi-
plication and fewer rows. In Proceedings of the twenty-fifth annual ACM-SIAM symposium on
Discrete algorithms, pp. 1515-1528. SIAM, 2014.
Samet Oymak, Christos Thrampoulidis, and Babak Hassibi. Near-optimal sample complexity
bounds for circulant binary embedding. In 2017 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pp. 6359-6363. IEEE, 2017.
Bryan A. Plummer, Liwei Wang, Christopher M. Cervantes, Juan C. Caicedo, Julia Hockenmaier,
and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer
image-to-sentence models. IJCV, 123(1):74-93, 2017.
Maxim Raginsky and Svetlana Lazebnik. Locality-sensitive binary codes from shift-invariant ker-
nels. In Advances in neural information processing systems, pp. 1509-1517, 2009.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. Advances in
neural information processing systems, 20:1177-1184, 2007.
Jorge Sanchez and Florent Perronnin. High-dimensional signature compression for large-scale im-
age classification. In CVPR 2011, pp. 1665-1672. IEEE, 2011.
Anshumali Shrivastava and Ping Li. In defense of minhash over simhash. In Artificial Intelligence
and Statistics, pp. 886-894, 2014.
11
Published as a conference paper at ICLR 2021
Yan Xia, Kaiming He, Pushmeet Kohli, and Jian Sun. Sparse projections for high-dimensional
binary codes. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 3332-3339, 2015.
Xinyang Yi, Constantine Caramanis, and Eric Price. Binary embedding: Fundamental limits and
fast algorithm. In International Conference on Machine Learning, pp. 2162-2170, 2015.
Felix Yu, Sanjiv Kumar, Yunchao Gong, and Shih-Fu Chang. Circulant binary embedding. In
International conference on machine learning, pp. 946-954, 2014.
Xu Zhang, Felix X Yu, Ruiqi Guo, Sanjiv Kumar, Shengjin Wang, and Shi-Fu Chang. Fast orthogo-
nal projection based on kronecker product. In Proceedings of the IEEE International Conference
on Computer Vision, pp. 2929-2937, 2015.
A Comparisons on different datasets
Figure 3: Plot of MAPE of Method 2 on four datasets with fixed p = 64 and order r = 1, 2
Experiments on the Yelp dataset in Section 6 showed that Method 2 based on sparse JL embeddings
performs as well as Method 1 which usues an FJLT to enforce the well-spreadness assumption. Now,
we only focus on Method 2 and check its performance on all four different datasets: Yelp, ImageNet,
Flickr30k, and CIFAR-10.
Specifically, for each dataset we randomly sample k = 1000 images and scale them such that
all scaled data points are contained in the `2 unit ball. Then we apply Method 2 to each dataset
separately and compute the corresponding MAPE metric, see Figure 3, where we fix p = 64 and let
r = 1, 2. We can observe that curves with r = 1 fluctuate, but displays a clear downward trend, when
m ≤ 8192 and reach an error floor around 0.08. In contrast to the first order quantization scheme,
curves with r = 2 decays faster and eventually achieve a lower floor around 0.07. Additionally,
Method 2 performs well on all datasets and implies that assumption of well-spread input vectors is
not too restrictive on natural images.
12
Published as a conference paper at ICLR 2021
B Proof of Lemma 2.6
We will require the following lemmas, adapted from the literature, to prove the distance-preserving
properties of our condensed sparse Johnson-Lindenstrauss transform (CSJLT) and condensed fast
Johnson-Lindenstrauss transform (CFJLT) in Lemma 2.6.
Lemma B.1 (Theorem 5.1 in MatoUsek (2008)). Let n ∈ N, E ∈ (0,1), δ ∈ (0,1), α ∈ [√n, 1]
be parameters and set m = C-2 log(δ-1) ∈ N where C is a sufficiently large constant. Let
S = 2α2∕e ≤ 1, A ∈ Rm×n be as in Definition 2.2. Then
P((1 - E)kxk2 ≤ ^π/2kAxkι ≤ (1 + e)kxk2) ≥ 1 - δ	(16)
m
holds for all x ∈ Rn with kxk∞ ≤ αkxk2.
Lemma B.2 below is adapted from (Ailon & Chazelle, 2009, Lemma 1), and we present its proof for
completeness.
Lemma B.2. Let H ∈ Rn×n and D ∈ Rn×n be as in Definition 2.4. For any λ > 0 and x ∈ Rn
we have
P(kHDxk∞ ≤ λ∣∣x∣∣2) ≥ 1 - 2ne-nλ2/2.	(17)
Proof. WithoUt loss of generality, we can assUme kxk2 = 1. Let u = HDx = (u1, . . . , un). Fix
i ∈ {1,...,n}. Then Ui = Pn=I aj Xj with PQj = √n) = PQj = 一 √η) = 1 for all j.
Moreover, a1, a2 , . . . , an are independent and symmetric. So ui is also symmetric, that is, ui and
-ui share the same distribUtion. For any t ∈ R we have
E(etnui) = YY E[exp(tnajXj)] = Y exp(t√nxj)+exp(-t√nxj)
j=1	j=1	2
n
≤ Y exp(nt2xj2/2) = exp(nt2/2).
j=1
Since ui is symmetric, by Markov’s ineqUality and the above resUlt, we get
P(∣Ui∣ ≥ λ) = 2P(eλnui ≥ eλ2n) ≤ 2e-λ2nE(eλnui) = 2e-λ2n/2.
Inequality (17) follows by the union bound over all i ∈ {1,..., n}.	□
Lemma B.3. Let n,λ ∈ N, e ∈ (0, 2), δ ∈ (0,1), P = O(e-2 log(δ-1)) ∈ N and m = λp. Let
V ∈ Rp×m be as in Definition 2.3, A ∈ Rm×n be the sparse Gaussian matrix in Definition 2.2
with s = Θ(e-1n-1 (IlvIl∞∕∣∣v∣∣2)2) ≤ 1, and Φ = AHD ∈ Rm×n be the FJLT in Definition 2.4
with s = Θ(e-1n-1(kvk∞∕kvk2)2 logn) ≤ 1. Thenfor X ∈ Rn with ∣∣x∣∞ = O(n-1/2|xk2),
we have
P (1 - E)IxI2 ≤ IVeAxI1≤ (1 + E)IxI2 ≥ 1-δ,	(18)
and for arbitrary x ∈ Rn, we have
P((1 - E)IxI2 ≤ IVeΦxI1≤ (1 + E)IxI2 ≥ 1-δ.	(19)
Proof. Recall that V = Ip 0 v and Φ = AH D. Let y ∈ Rn and K := V A = (Ip 0 v)A ∈ Rp×n.
For 1 ≤ i ≤ p and 1 ≤ j ≤ n, we have
λ
Kij =	vka(i-1)λ+k,j .
k=1
Denote the row vectors of A by a1, a2 , . . . , am. It follows that
n	nλ	λ
(Ky)i =	Kij yj =	yj vka(i-1)λ+k,j =	vk hy, a(i-1)λ+ki = [B(vT 0 y)]i
j=1	j=1 k=1	k=1
13
Published as a conference paper at ICLR 2021
where
a1	a2
aλ+1	aλ+2
B :=	.	.
..
..
a(p-1)λ+1 a(p-1)λ+2
aλ
a2λ
.
.
.
apλ
∈ Rp
×λn
v1y
v2 y
and VT 0 y =	.	∈ Rλn.
.
.
vλy
Hence V Ay = Ky = B(vT 0 y) holds for all y ∈ Rn . Additionally, we get a reshaped sparse
Gaussian random matrix B by rearranging the rows of A.
For the first assertion in the theorem, note that X ∈ Rn satisfies ∣∣x∣∣∞ = O(kxk2∕√n). So, We
have V Ax = B(vT 0 x), kvT0xk2 = kvk2 kxk2 and kvT 0 xk∞ = kvk∞ kxk∞. Then (18) holds
by applying Lemma B.1 to random matrix B and vector v> 0 X with α = Θ(n-V2kvk∞∕kvk2).
For the second assertion, if x ∈ Rn is arbitrary, then by substituting HDx for y one can get
V ΦX = B(vT 0 (HDX)).
Note that
kvT 0 (H DX)k2 = kvk2kH DXk2 = kvk2kXk2 and kvT0 (HDX)k∞ = kvk∞kHDXk∞.
Inequality (19) follows immediately by using the above fact and applying Lemma B.1 and Lemma
B.2 to the random operator B and vector vT 0 (H DX) with α = Θ((n-1 log n)”∣∣v∣∣∞/∣∣v∣∣2).
□
Now we can embed a set of points in a high dimensional space into a space of much lower dimension
in such a way that distances between the points are nearly preserved. By substituting δ with 2δ∕∣T∣2
in Lemma B.3 and using the fact 1 一 (|T|) PT2 = 1 一 lτl(lTl-1) ∙ -^2 > 1 一 δ, Lemma 2.6 follows
from the union bound over all pairwise data points in T.
C Stable Sigma-Delta quantization and its properties
Although it is a non-trivial task to design a stable quantization rule ρ when r > 1, families of one-
bit Σ∆ quantization schemes that achieve this goal have been designed by Daubechies & DeVore
(2003); Gunturk (2003); Deift et al. (2011), and we now describe one such family. To start, note that
an r-th order Σ∆ quantization scheme may also arise from a more general difference equation of
the form
y 一 q = f * V	(20)
where * denotes convolution and the sequence f = Pr g with g ∈ '1. Then any (bounded) solution
V of (20) generates a (bounded) solution u of (11) via u = g * V. Thus (11) can be rewritten in
the form (20) by a change of variables. Defining h := δ(0) 一 f, where δ(0) denotes the Kronecker
delta sequence supported at 0, and choosing the quantization rule ρ in terms of the new variable as
(h*V)i + yi. Then (10) reads as
qi = Q((h * V)i + yi),
vi = (h * V)i + yi 一 qi .
(21)
By designing a proper filter h one can get a stable r-th order Σ∆ quantizer, as was done in Deift
et al. (2011); GUntUrk (2003), leading to the following result from GUntUrk (2003), which exploits
the above relationship between V and u to bound kuk∞ .
Proposition C.1. Fix an integer r, an integer σ ≥ 6 and let nj = σ(j 一 1)2 + 1 forj = 1, 2, . . . , r.
Let the filter h be of the form
r
h=Xdjδnj
j=1
where δnj is the Kronecker delta supported at n7∙ and dj = Ili=• n—n for j = 1, 2,...,r. There
exists a universal constant C > 0 such that the rth order Σ∆ scheme (21) with 1-bit alphabet
A = {一1, 1}, is stable, and
kyk∞ ≤ μ< 1 = kuk∞ ≤ Cc(μ)r rr,	(22)
where c(μ) > 0 is a Constant only depends on μ.
14
Published as a conference paper at ICLR 2021
Having introduced stable Σ∆ quantization, we now present a lemma controlling an operator norm
of V Pr. We will need this result in controlling the error in approximating distances associated with
our binary embedding.
Lemma C.2. For a stable r-th order Σ∆ quantization scheme,
kVPrk∞,ι ≤ P∏∕2(8r)r+1λ-r+1/2.
Proof. By the same method used in the proof of Lemma 4.6 in Huynh & Saab (2020), one can get
kVPrk∞,∞ ≤ r23r-1 and ∣∣v∣∣2 ≥ λr-"r-r.
It follows that
kVPrk∞,ι =言kVPrk∞,ι ≤ ^π/2kVPrk∞,∞ ≤ P∏∕2(8r)r+1λ-r+1/2.
pkv k2	kv k2
□
The following result guarantees that the linear part of our embedding generates a bounded vector,
and therefore allows us to later appeal to the stability property of Σ∆ quantizers. In other words, it
will allow us to use (22) to control the infinity norm of state vectors generated by Σ∆ quantization.
Lemma C.3 (Concentration inequality for ∣∣ ∙ ∣∣∞). Let β > 0, E ∈ (0,1), A ∈ Rm×n be the sparse
Gaussian matrix in Definition 2.2 with s = Θ(-1n-1) ≤ 1, and Φ = AHD ∈ Rm×n be the FJLT
in Definition 2.4 with s = Θ(E-1n-1 log n) ≤ 1. Suppose that
(23)
(24)
(25)
+ log(2m) ≤ μ ≤ √=.
Then
P(∣Ax∣∞ ≤ μkx∣∣2) ≥ 1 - e-β
holds for x ∈ Rn with ∣x∣∞ = O(n-1/2 ∣x∣2 ) and
P(∣Φx∣∞ ≤ μkx∣∣2) ≥ 1 — 2e-β
holds for x ∈ Rn.
Proof. Without loss of generality, we can assume that x is a unit vector with ∣x∣2 = 1. We start
with the proof of (25). By applying Lemma B.2 to X with λ = Θ(dlog n/n), We have
P∣HDx∣∞ ≤λ ≥ 1-e-β.	(26)
Let A be as in Definition 2.2 with S = 2λ2∕e = Θ(e-1n-1 log n) ≤ 1 and recall that Φ = AH D.
Suppose that y ∈ Rn with ∣∣y∣2 = 1 and ∣∣y∣∣∞ ≤ λ. Let Y = Ay. Then Yi := (Ay)=
Pn=I ajyj for 1 ≤ i ≤ m. Fort ≤ to := √2s∕λ = 2∕√E, we get t2y2∕2s ≤ 1 for all j. Since
ex ≤ 1+2x for all x ∈ [0, 1] and 1+x ≤ ex for all x ∈ R, set2yj2/2s+1 -s ≤ s(1+t2yj2/s)+1-s =
1 + t2yj2 ≤ et2yj2. It follows that
nn	n
E(etYi) = Y E(etaijyj) = Y(Set2y2/2s + 1 - s) ≤ Y et2y2 = et2
j=1	j=1	j=1
holds for all 1 ≤ i ≤ m and t ∈ [0, t0]. So for t ∈ [0, t0 ], by Markov inequality and above inequality
we have
P (Yi ≥ μ) = P 卜tYi ≥ etμ) ≤ e-tμE(etYi) ≤ e-tμ+t2.
According to (23) we can set t = μ∕2 ≤ t0 = 2∕√E, then P (匕 ≥ μ) ≤ e-μ2/4. By symmetry we
have P (一匕 ≥ μ) ≤ e-μ2/4. Consequently, for all 1 ≤ i ≤ m we have
P(|Yi| ≥ 0 ≤ 2e-μ2/4.	(27)
15
Published as a conference paper at ICLR 2021
By a union bound, (23), and (27)
P(l∣Ay∣∣∞ ≥ μ) = Pɑmaχ |匕| ≥ 〃) ≤ mP(|匕| ≥ μ)
=2me-"2/4 ≤ e-β.	(28)
It follows immediately from (26) and (28) with y = HDx that
P(∣∣Φx∣∣∞ ≤ μ) = P(IlAHDxk∞ ≤ μ)
≥ P(kAH Dxk∞ ≤ μ,kH Dx∣∞ ≤ λ)
=P(kAHDx∣∞ ≤ μ | ∣HDXh ≤ λ)P(∣∣HDXh ≤ λ)
≥ (1 - e-β)2
≥ 1 - 2e-β.
Furthermore, if We replace y by x in (28) and use A with s = Θ(e-1n-1), then inequality (24)
follows. The difference in the choice of S is due to the fact that for vectors in the unit ball with
∣∣x∣∣∞ = O(n-1∕2∣∣x∣∣2) we have that ∣∣x∣∣∞ ≤ n-1/2.	□
D	Proof of Theorem 4.2
Proof. Since the proofs of (12) and (13) are almost identical except for using different random
projections A and Φ, we shall only establish the result for (13) in detail. For any X ∈ T ⊆ Bn(K)
we have ∣∣x∣2 ≤ κ. By applying Lemma C.3 we get
P(∣Φχ∣∞ < μ) ≥ P(∣Φχ∣∞ < μ∣∣χ∣l2∕κ)
≥ P(∣Φx∣∞ < 2√β + log(2m)∣x∣2)
≥ 1 - 2e-β.
Since above inequality holds for arbitrary x ∈ T, by union bound one can get
P (max ∣Φx∣∞ < μ) ≥ 1 - 2|T|e-β.
Suppose that UX is the state vector of input signal Φx which is produced by stable r-th order Σ∆
scheme. Using Lemma C.2 and formula (22) to get
∣VPl∞,ι∣Uχk∞ ≤ Cc(μ)rrr(8r)r+1√∏∕2λ-r+1/2,	(29)
which holds uniformly for all X ∈ T with probability exceeding 1 - 2|T|e-β.
Furthermore, by Lemma 2.6 the probability that
∣∖∖VΦ(χ - y)kι - ∣χ - y∣2∣ ≤ d∣χ - y∣2	(30)
holds simultaneously for all x, y ∈ T is at least 1 - δ.
We deduce from triangle inequality and equations (29), (30) that
∣ dVe(f2(X),f2(y)) - ∣χ - y∣2 ∣
=∣VQ6(Φx) - VQs)(Φy)kι -∣X - y∣2 ∣
≤ ∣VQ6(Φx) - VQ(r)(Φy)kι -∣VΦ(x - y)kι + ∣∣VΦ(x - y)kι -∣x - y∣2 ∣
≤ ∣V(QS)(Φx) - Φx) - V(Q6(Φy) - Φy)kι + ∣VΦ(x - y)kι - ∣X - y∣2 ∣
≤ ∣VPrUXIII + IlVPrUyIII + ∣∣VΦ(x - y)kι -∣x - y∣2 ∣
≤ ∖vpr∖∞,ι(IIUX∖∞ + Iluyll∞)+ IkVφ(X - y)l∣ι - ∖x - y∣∣2 ∣
≤ 2Cc(μ)rrr(8r)r+1√∏∕2λ-r+1/2 + e∣∣x - y∣∣2
=√2πCc(μ)r rr (8r)r+1λ-r+1/2 + c∣∣x — y∣∣2
=C(μ,r)λ-r+1/2 + E∣∣x - y∣∣2
16
Published as a conference paper at ICLR 2021
holds uniformly for any x, y ∈ T with probability at least 1 - δ - 2∣T∣e-β. The bound (12) is
associated with a weaker condition on β due to the associated weaker condition in Lemma C.3. □
E	Comparis on with product quantization
Note that the distance preserving quality (as well as performance on retrieval and classification tasks)
of MSQ binary embeddings using bilinear projection (Gong et al., 2013) or circulant matrices (Yu
et al., 2014) has be shown to be at least as good as product quantization (Jegou et al., 2010), LSH
(Andoni & Indyk, 2006; Shrivastava & Li, 2014) and ITQ (Gong et al., 2012c). Our method uses
Sigma-Delta quantization, which
1.	gives provably better error rates than the MSQ design as shown in this paper, and in (Huynh
& Saab, 2020);
2.	is more efficient in terms of both memory and distance query computation as shown in
Section 5.
In order to more explicitly compare our algorithm with data dependent methods, as an example, we
now briefly analyze product quantization as presented in (Jegou et al., 2010). We then present a brief
analysis of optimal data-independent methods as well as data-independent product quantization, in
comparison with our method.
Data-dependent product quantization
The key idea here is to decompose the input vector space Rn into the Cartesian product of M
low-dimensional subspaces Rd with n = Md and quantize each subspace into k codewords, for
example by using the k-means algorithm. So the total number of centroids (codewords) in Rn
is k = (k*)M and the time complexity of learning all k centroids is O(nNk*t) where N is the
number of training data points and t is the number of iterations in the k-means algorithm. Moreover,
converting each input vector X ∈ Rn to the index of its codeword needs time O(MdkD = O(nk*)
and the length of binary codes is m = log2 k = M log2 k*. Since we have to store all k centroids
and M lookup tables, memory usage is O(M(dk* + (k*)2)) = O(nk* + M(k*)2). Moreover, the
query time, i.e. the time complexity of pairwise distance estimation is O(Mk*) using lookup tables.
As a result, we obtain Table 2, whose column headings are analogous to those in Table 1.
	MethOd		Time	Space	Storage	Query Time
Product Quantization	O(nk*)	O(nk* + M (k*)2)	O(M log2 k*)	O(Mk*) 一
Our Method (on well-spread T)	-O(m)-	O(m)	O(P log2 λ)	O(p log2 λ)
Table 2: Comparison between the proposed method and product quantization per data point
A direct comparison of the associated errors is not possible due to the fact that the error associated
with data-dependent product quantization is a function of the input data distribution, and the conver-
gence of the k-means algorithm. Nevertheless, one can note some tradeoffs from Table 2. Namely,
the embedding time and the space needed to store our embedding matrix are lower than those asso-
ciated with product quantization. On the other hand, the space needed to store the embedded data
points and the query time associated with product quantization depend on the parameter choices M
and k*, which also affect the resulting accuracy. Finally, we note that product quantization (using k-
means clustering) is associated with a pre-processing time O(nN k*t), which is significantly larger
than our method.
Data-independent product quantization and optimality of our method
If one were to just encode, in a data independent way, the `2 ball of Rn , so that the encoding
error is at most θ, then a simple volume argument shows that one needs at least θ-n codewords,
hence n l0g2(l∕θ) bits. This lower bound holds, independent of the encoding method, i.e., whether
one uses product quantization or any other technique. To reduce the number of bits below n, one
17
Published as a conference paper at ICLR 2021
approach is to capitalize on the finiteness of the data, and use a JL type embedding (such as random
sampling for well-spread data) to reduce the dimension to p ≈ log |T |/2 (up to log factors), and
therefore introduce a new embedding error of , on top of the encoding error. The advantage is that
one would then only need to encode an `2 ball in the p-dimensional space. Again, independently
of the encoding method, one would now need P log(l/θ) bits to get an encoding error of θ. If We
denote cx, cy, the encoding of x and y, then this gives the error estimate
kcx - cyk - kx - yk . θ+ kx - yk.
If we rewrite the error now in terms of the number of bits b = P log(1∕θ) ,we get
kcx-cyk-kx-yk .2-b/p+kx-yk.
Note that in all of this, no computational complexity was taken into account.
One can envision replacing k-means clustering in product quantization, with a data-independent
encoding. With a careful choice of parameters, this may be significantly more computationally
efficient than the above optimal encoding, albeit at the expense of a sub-optimal error bound.
On the other hand, consider that our computationally efficient scheme uses m bits, and that those m
bits can be compressed into b ≈ rP log(m/P) bits (see Section 5), then our error, by Theorem 4.2 is
kcx - cyk - kx - yk . c(m/P)-r+1/2 + kx - yk,
which in rate-distortion terms is
Ikcx - Cyk - kx - yk∣ . 2-br-r/2 + ekx - yk.
In other words, up to constants in the exponent, and possible logarithmic terms, our result is near-
optimal.
18