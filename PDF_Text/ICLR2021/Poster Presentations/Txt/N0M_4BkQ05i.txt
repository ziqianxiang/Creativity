Published as a conference paper at ICLR 2021
Selective Classification Can Magnify
Disparities Across Groups
Erik Jones； Shiori SagaWa； Pang Wei Koh； Ananya Kumar & Percy Liang
Department of Computer Science, Stanford University
{erjones,ssagawa,pangwei,ananya,pliang}@cs.stanford.edu
Ab stract
Selective classification, in which models can abstain on uncertain predictions, is
a natural approach to improving accuracy in settings where errors are costly but
abstentions are manageable. In this paper, we find that while selective classifi-
cation can improve average accuracies, it can simultaneously magnify existing
accuracy disparities between various groups within a population, especially in the
presence of spurious correlations. We observe this behavior consistently across
five vision and NLP datasets. Surprisingly, increasing abstentions can even de-
crease accuracies on some groups. To better understand this phenomenon, we
study the margin distribution, which captures the model’s confidences over all
predictions. For symmetric margin distributions, we prove that whether selective
classification monotonically improves or worsens accuracy is fully determined by
the accuracy at full coverage (i.e., without any abstentions) and whether the distri-
bution satisfies a property we call left-log-concavity. Our analysis also shows that
selective classification tends to magnify full-coverage accuracy disparities. Moti-
vated by our analysis, we train distributionally-robust models that achieve similar
full-coverage accuracies across groups and show that selective classification uni-
formly improves each group on these models. Altogether, our results suggest that
selective classification should be used with care and underscore the importance of
training models to perform equally well across groups at full coverage.
1	Introduction
Selective classification, in which models make predictions only when their confidence is above a
threshold, is a natural approach when errors are costly but abstentions are manageable. For exam-
ple, in medical and criminal justice applications, model mistakes can have serious consequences,
whereas abstentions can be handled by backing off to the appropriate human experts. Prior work has
shown that, across a broad array of applications, more confident predictions tend to be more accu-
rate (Hanczar & Dougherty, 2008; Yu et al., 2011; Toplak et al., 2014; Mozannar & Sontag, 2020;
Kamath et al., 2020). By varying the confidence threshold, we can select an appropriate trade-off
between the abstention rate and the (selective) accuracy of the predictions made.
In this paper, we report a cautionary finding: while selective classification improves average ac-
curacy, it can magnify existing accuracy disparities between various groups within a population,
especially in the presence of spurious correlations. We observe this behavior across five vision and
NLP datasets and two popular selective classification methods: softmax response (Cordella et al.,
1995; Geifman & El-Yaniv, 2017) and Monte Carlo dropout (Gal & Ghahramani, 2016). Surpris-
ingly, we find that increasing the abstention rate can even decrease accuracies on the groups that have
lower accuracies at full coverage: on those groups, the models are not only wrong more frequently,
but their confidence can actually be anticorrelated with whether they are correct. Even on datasets
where selective classification improves accuracies across all groups, we find that it preferentially
helps groups that already have high accuracies, further widening group disparities.
These group disparities are especially problematic in the same high-stakes areas where we might
want to deploy selective classification, like medicine and criminal justice; there, poor performance
on particular groups is already a significant issue (Chen et al., 2020; Hill, 2020). For example, we
study a variant of CheXpert (Irvin et al., 2019), where the task is to predict if a patient has pleural
* Equal contribution
1
Published as a conference paper at ICLR 2021
A-Suea
"am00®əMoə-əs
4
Average coverage
Figure 1: A selective classifier (y, c) makes a prediction y(x) on a point X if its confidence c(x) in
that prediction is larger than or equal to some threshold τ . We assume the data comprises different
groups each with their own data distribution, and that these group identities are not available to
the selective classifier. In this figure, we show a classifier with low accuracy on a particular group
(red), but high overall accuracy (blue). Left: The margin distributions overall (blue) and on the red
group. The margin is defined as ^(x) on correct predictions (y(x) = y) and -c(x) otherwise. For a
threshold τ, the selective classifier is thus incorrect on points with margin ≤ -τ; abstains on points
with margin between -τ and τ ; and is correct on points with margin ≥ τ . Right: By varying τ ,
we can plot the accuracy-coverage curve, where the coverage is the proportion of predicted points.
As coverage decreases, the average (selective) accuracy increases, but the worst-group accuracy
decreases. The black dots correspond to the threshold τ = 1, which is shaded on the left.
effusion (fluid around the lung) from a chest x-ray. As these are commonly treated with chest tubes,
models can latch onto this spurious correlation and fail on the group of patients with pleural effusion
but not chest tubes or other support devices. However, this group is the most clinically relevant as it
comprises potentially untreated and undiagnosed patients (Oakden-Rayner et al., 2020).
To better understand why selective classification can worsen accuracy and magnify disparities, we
analyze the margin distribution, which captures the model’s confidences across all predictions and
determines which examples it abstains on at each threshold (Figure 1). We prove that when the
margin distribution is symmetric, whether selective classification monotonically improves or wors-
ens accuracy is fully determined by the accuracy at full coverage (i.e., without any abstentions) and
whether the distribution satisfies a property we call left-log-concavity. To our knowledge, this is the
first work to characterize whether selective classification (monotonically) helps or hurts accuracy in
terms of the margin distribution, and to compare its relative effects on different groups.
Our analysis shows that selective classification tends to magnify accuracy disparities that are present
at full coverage. Motivated by our analysis, we find that selective classification on group DRO
models (Sagawa et al., 2020)—which achieve similar accuracies across groups at full coverage by
using group annotations during training—uniformly improves group accuracies at lower coverages,
substantially mitigating the disparities observed on standard models that are instead optimized for
average accuracy. This approach is not a silver bullet: it relies on knowing group identities during
training, which are not always available (Hashimoto et al., 2018). However, these results illustrate
that closing disparities at full coverage can also mitigate disparities due to selective classification.
2	Related work
Selective classification. Abstaining when the model is uncertain is a classic idea (Chow, 1957; Hell-
man, 1970), and uncertainty estimation is an active area of research, from the popular approach of us-
ing softmax probabilities (Geifman & El-Yaniv, 2017) to more sophisticated methods using dropout
(Gal & Ghahramani, 2016), ensembles (Lakshminarayanan et al., 2017), or training snapshots (Geif-
man et al., 2018). Others incorporate abstention into model training (Bartlett & Wegkamp, 2008;
Geifman & El-Yaniv, 2019; Feng et al., 2019) and learn to abstain on examples human experts are
more likely to get correct (Raghu et al., 2019; Mozannar & Sontag, 2020; De et al., 2020). Selec-
tive classification can also improve out-of-distribution accuracy (Pimentel et al., 2014; Hendrycks &
Gimpel, 2017; Liang et al., 2018; Ovadia et al., 2019; Kamath et al., 2020). On the theoretical side,
early work characterized optimal abstention rules given well-specified models (Chow, 1970; Hell-
man & Raviv, 1970), with more recent work on learning with perfect precision (El-Yaniv & Wiener,
2010; Khani et al., 2016) and guaranteed risk (Geifman & El-Yaniv, 2017). We build on this liter-
ature by establishing general conditions on the margin distribution for when selective classification
helps, and importantly, by showing that it can magnify group disparities.
2
Published as a conference paper at ICLR 2021
Group disparities. The problem of models performing poorly on some groups of data has been
widely reported (e.g., Hovy & S0gaard (2015); Blodgett et al. (2016); Corbett-Davies et al. (2017);
Tatman (2017); Hashimoto et al. (2018)). These disparities can arise when models latch onto spu-
rious correlations, e.g., demographics (Buolamwini & Gebru, 2018; Borkan et al., 2019), image
backgrounds (Ribeiro et al., 2016; Xiao et al., 2020), spurious clinical variables (Badgeley et al.,
2019; Oakden-Rayner et al., 2020), or linguistic artifacts (Gururangan et al., 2018; McCoy et al.,
2019). These disparities have implications for model robustness and equity, and mitigating them is
an important open challenge (Dwork et al., 2012; Hardt et al., 2016; Kleinberg et al., 2017; Duchi
et al., 2019; Sagawa et al., 2020). Our work shows that selective classification can exacerbate this
problem and must therefore be used with care.
3	Setup
A selective classifier takes in an input x ∈ X and either predicts a label y ∈ Y or abstains. We study
standard confidence-based selective classifiers (y, ^), where y : X → Y outputs a prediction and
c : X → R+ outputs the model,s confidence in that prediction. The selective classifier abstains on
X whenever its confidence C(X) is below some threshold T and predicts y(x) otherwise.
Data and training. We consider a data distribution D over X × Y × G, where G = {1, 2, . . . , k} cor-
responds to a group variable that is unobserved by the model. We study the common setting where
the model y is trained under full coverage (i.e., without taking into account any abstentions) and the
confidence function c is then derived from the trained model, as described in the next paragraph. We
will primarily consider models y trained by empirical risk minimization (i.e., to minimize the aver-
age training loss); in that setting, the group g ∈ G is never observed. In Section 7, we will consider
the group DRO training algorithm (Sagawa et al., 2020), which observes g at training time only. In
both cases, g is not observed at test time and the model thus does not take in g. This is a common
assumption: e.g., we might want to ensure that a face recognition model has equal accuracies across
genders, but the model only sees the photograph (X) and not the gender (g).
Confidence. We will primarily consider softmax response (SR) selective classifiers, which take
C(X) to be the normalized logit of the predicted class. Formally, we consider models that estimate
p^(y | x) (e.g., through a softmax) and predict y(x) = arg maxy∈γ p^(y | x), with the corresponding
probability estimate p^(y(χ) | x). For binary classifiers, we define the confidence c(x) as
C(X)TOg (πXj5χ¼) .	(1)
2 V - p(y(χ) | χ)√
This corresponds to a confidence of C(x) = 0 when p(y(χ) | χ) = 0.5, i.e., the classifier is com-
pletely unsure of its prediction. We generalize this notion to multi-class classifiers in Section A.1.
Softmax response is a popular technique applicable to neural networks and has been shown to im-
prove average accuracies on a range of applications (Geifman & El-Yaniv, 2017). Other methods
offer alternative ways of computing ^(x); in Appendix B.1, we also run experiments where C(x) is
obtained via Monte Carlo (MC) dropout (Gal & Ghahramani, 2016), with similar results.
Metrics. The performance of a selective classifier at a threshold τ is typically measured by its
average (selective) accuracy on its predicted points, P[y(x) = y | C(x) ≥ T], and its average
coverage, i.e., the fraction of predicted points P[C(x) ≥ T]. We call the average accuracy at threshold
0 the full-coverage accuracy, which corresponds to the standard notion of accuracy without any
abstentions. We always use the term accuracy w.r.t. some threshold T ≥ 0; where appropriate, we
emphasize this by calling it selective accuracy, but we use these terms interchangeably in this paper.
Following convention, we evaluate models by varying T and tracing out the accuracy-coverage curve
(El-Yaniv & Wiener, 2010). As Figure 1 illustrates, this curve is fully determined by the distribution
of the margin, which is C(x) on correct predictions (y(x) = y) and -C(x) otherwise. We are also
interested in evaluating performance on each group. For a group g ∈ G, we compute its group
(selective) accuracy by conditioning on the group, P[y(x) = y | g, C(x) ≥ T], and we define group
COVerage analogously as P[C(x) ≥ T | g]. We pay particular attention to the worst group under the
model, i.e., the group argmi、P[y(x) = y | g] with the lowest accuracy at full coverage. In our
setting, we only use group information to evaluate the model, which does not observe g at training or
test time; we relax this later when studying distributionally-robust models that use g during training.
3
Published as a conference paper at ICLR 2021
Dataset	Modality	# examples	Prediction task	Spurious attributes A
CelebA	Photos	202,599	Hair color	Gender
CivilComments	Text	448,000	Toxicity	Mention of Christianity
Waterbirds	Photos	11,788	Waterbird or landbird	Water or land background
CheXpert-device	X-rays	156,848	Pleural effusion	Has support device
MultiNLI	Sentence pairs	412,349	Entailment	Presence of negation words
Table 1: We study these datasets from Liu et al. (2015); Borkan et al. (2019); Sagawa et al. (2020);
Irvin et al. (2019); Williams et al. (2018) respectively. For each dataset, we form a group for each
combination of label y ∈ Y and spuriously-correlated attribute a ∈ A, and evaluate the accuracy of
selective classifiers on average and on each group. Dataset details in Appendix C.1.
Datasets. We consider five datasets (Table 1) on which prior work has shown that models latch onto
spurious correlations, thereby performing well on average but poorly on the groups of data where
the spurious correlation does not hold up. Following Sagawa et al. (2020), we define a set of labels
Y as well as a set of attributes A that are spuriously correlated with the labels, and then form one
group for each (y, a) ∈ Y × A. For example, in the pleural effusion example from Section 1, one
group would be patients with pleural effusion (y = 1) but no support devices (a = 0). Each dataset
has |Y| = |A| = 2, except MultiNLI, which has |Y| = 3. More dataset details are in Appendix C.1.
4 Evaluating selective classification on groups
We start by investigating how selective classification affects group (selective) accuracies across the
five datasets in Table 1. We train standard models with empirical risk minimization, i.e., to mini-
mize average training loss, using ResNet50 for CelebA and Waterbirds; DenseNet121 for CheXpert-
device; and BERT for CivilComments and MultiNLI. Details are in Appendix C. We focus on soft-
max response (SR) selective classifiers, but show similar results for MC-dropout in Appendix B.1.
Accuracy-coverage curves. Figure 2 shows group accuracy-coverage curves for each dataset, with
the average in blue, worst group in red, and other groups in gray. On all datasets, average accuracies
improve as coverage decreases. However, the worst-group curves fall into three categories:
1.	Decreasing. Strikingly, on CelebA, worst-group accuracy decreases with coverage: the more
confident the model is on worst-group points, the more likely it is incorrect.
2.	Mixed. On Waterbirds, CheXpert-device, and CivilComments, as coverage decreases, worst-
group accuracy sometimes increases (though not by much, except at noisy, low coverages) and
sometimes decreases.
3.	Slowly increasing. On MultiNLI, as coverage decreases, worst-group accuracy consistently im-
proves but more slowly than other groups: from full to 50% average coverage, worst-group accu-
racy goes from 65% to 75% while the second-to-worst group accuracy goes from 77% to 95%.
Figure 2: Accuracy (top) and coverage (bottom) for each group, as a function of the average cover-
age. Each average coverage corresponds to a threshold τ . The red lines represent the worst group.
At low coverages, accuracy estimates are noisy as only a few predictions are made.
4
Published as a conference paper at ICLR 2021
Figure 3: SR selective classifiers (solid line) substantially underperform their group-agnostic ref-
erences (dotted line) on the worst group, which are in turn far behind the best-case Robin Hood
references (dashed line). By construction, these share the same average accuracy-coverage curves
(blue line). Similar results for MC-dropout are in Figure 7.
Group-agnostic and Robin Hood references. The results above show that even when selective
classification is helping the worst group, it seems to help other groups more. We formalize this
notion by comparing the selective classifier to a matching group-agnostic reference that is derived
from it and that tries to abstain equally across groups. At each threshold, the group-agnostic ref-
erence makes the same numbers of correct and incorrect predictions as its corresponding selective
classifier, but distributes these predictions uniformly at random across points without regard to group
identities (Algorithm 1). By construction, it has an identical average accuracy-coverage curve as its
corresponding selective classifier, but can differ on the group accuracies. We show in Appendix A.2
that it satisfies equalized odds (Hardt et al., 2016) w.r.t. which points it predicts or abstains on.
The group-agnostic reference distributes abstentions equally across groups; from the perspective of
closing disparities between groups, this is the least that we might hope for. Ideally, selective clas-
sification would preferentially increase worst-group accuracy until it matches the other groups. We
can capture this optimistic scenario by constructing, for a given selective classifier, a corresponding
Robin Hood reference which, as above, also makes the same number of correct and incorrect pre-
dictions (see Algorithm 2 in Appendix A.3). However, unlike the group-agnostic reference, for the
Robin Hood reference, the correct predictions are not chosen uniformly at random; instead, we pri-
oritize picking them from the worst group, then the second worst group, etc. Likewise, we prioritize
picking the incorrect predictions from the best group, then the second best group, etc. This results
in worst-group accuracy rapidly increasing at the cost of the best group.
Both the group-agnostic and the Robin Hood references are not algorithms that we could implement
in practice without already knowing all of the groups and labels. They act instead as references:
selective classifiers that preferentially benefit the worst group would have worst-group accuracy-
coverage curves that lie between the group-agnostic and Robin Hood curves. Unfortunately, Figure 3
shows that SR selective classifiers substantially underperform even their group-agnostic counter-
parts: they disproportionately help groups that already have higher accuracies, further exacerbating
the disparities between groups. We show similar results for MC-dropout in Section B.1.
Algorithm 1: Group-agnostic reference for (y, ^ at threshold τ
Input: Selective classifier (y, ^), threshold T, test data D
Output: The sets of correct predictions Cτga ⊆ D and incorrect predictions Iτga ⊆ D that the
group-agnostic reference for (y, ^) makes at threshold T.
ι Let CT be the set of all examples that (y, ^ correctly predicts at threshold τ:
CT = {(χ,y,g) ∈ D | y(x) = y and C(X) ≥ T}.	(2)
Sample a subset CTa of size |C「| uniformly at random from Co, which is the set of all
examples that y would have predicted correctly at full coverage.
2	Let IT be the analogous set of incorrect predictions at τ :
It = {(χ,y,g) ∈ D | y(χ) = y and c(x) ≥ T}.	⑶
Sample a subset ITga of size |IT| uniformly at random fromI0.
3	Return CTga andITga. Since |CTga| = |CT| and |ITga| = |IT |, the group-agnostic reference makes
the same numbers of correct and incorrect predictions as (y, ^), but in a group-agnostic way.
5
Published as a conference paper at ICLR 2021
CelebA
Waterbirds CheXpert-devιce CiviIComments MuItiNLI
至 suωp 至 suωp
ω6eJω>4 dnαJ6JSJOM
ce□cce□□
CE CC CE □C Sl
-10 0	10
Margin
-10	0	10
Margin
-2.5 0.0 2.5
Margin
-2.5 0.0 2.5
Margin
-2.5 0.0	2.5
Margin
Figure 4: Margin distributions on average (top) and on the worst group (bottom). Positive (negative)
margins correspond to correct (incorrect) predictions, and we abstain on points with margins closest
to zero first. The worst groups have disproportionately many confident but incorrect examples.
5	Analysis: Margin distributions and accuracy-coverage curves
We saw in the previous section that while selective classification typically increases average (selec-
tive) accuracy, it can either increase or decrease worst-group accuracy. We now turn to a theoretical
analysis of this behavior. Specifically, we establish the conditions under which we can expect to see
its two extremes: when accuracy monotonically increases, or decreases, as a function of coverage.
While these extremes do not fully explain the empirical phenomena, e.g., why worst-group accuracy
sometimes increases and then decreases, our analysis broadly captures why accuracy monotonically
increases on average with decreasing coverage, but displays mixed behavior on the worst group.
Our central objects of study are the margin distributions for each group. Recall that the margin of
a selective classifier (y, ^ on a point (x, y) is its confidence c(x) ≥ 0 if the prediction is correct
(y(χ) = y) and -c(x) ≤ 0 otherwise. The selective accuracies on average and on the worst-
group are thus completely determined by their respective margin distributions, which we show for
our datasets in Figure 4. The worst-group and average distributions are very different: the worst-
group distributions are consistently shifted to the left with many confident but incorrect examples.
Our approach will be to characterize what properties of a general margin distribution F lead to
monotonically increasing or decreasing accuracy. Then, by letting F be the overall or worst-group
margin distribution, we can see how the differences in these distributions lead to differences in
accuracy as a function of coverage.
Setup. We consider distributions over margins that have a differentiable cumulative distribution
function (CDF) and a density, denoted by corresponding upper- and lowercase variables (e.g., F
and f, respectively). Each margin distribution F corresponds to a selective classifier over some
data distribution. We denote the corresponding (selective) accuracy of the classifier at threshold
T as AF(T) = (1 - F(T))/(F(-τ) + 1 - F(T)). Since increasing the threshold T monotonically
decreases coverage, we focus on studying accuracy as a function ofτ. All proofs are in Appendix D.
5.1	Symmetric margin distributions
We begin with symmetric distributions. We introduce a generalization of log-concavity, which we
call left-log-concavity; for symmetric distributions, left-log-concavity corresponds to monotonicity
of the accuracy-coverage curve, with the direction determined by the full-coverage accuracy.
Definition 1 (Left-log-concave distributions). A distribution is left-log-concave if its CDF is log-
concave on (一∞,μ], where μ is the mean of the distribution.
Left-log-concave distributions are a superset of the broad family of log-concave distributions (e.g.,
Gaussian, beta, uniform), which require log-concave densities (instead of CDFs) on their entire
support (Boyd & Vandenberghe, 2004). Notably, they can be multimodal: a symmetric mixture of
two Gaussians is left-log-concave but not generally log-concave (Lemma 1 in Appendix D).
Proposition 1 (Left-log-concavity and monotonicity). Let F be the CDF of a symmetric distri-
bution. If F is left-log-concave, then AF (T) is monotonically increasing in T if AF (0) ≥ 1/2
and monotonically decreasing otherwise. Conversely, if AFd (T) is monotonically increasing for all
translations Fd such that Fd(T) = F(T -d)forall T and AFd (0) ≥ 1/2, then F is left-log-concave.
6
Published as a conference paper at ICLR 2021
Proposition 1 is consistent with the observation that selective classification tends to improve average
accuracy but hurts worst-group accuracy. As an illustration, consider a margin distribution that is a
symmetric mixture of two Gaussians, each corresponding to a group, and where the average accuracy
is >50% at full coverage but the worst-group accuracy is <50%. As the overall and worst-group
margin distributions are both left-log-concave (Lemma 1), Proposition 1 implies that worst-group
accuracy will decrease monotonically with τ while the average accuracy improves monotonically.
Applied to CelebA (Figure 4), Proposition 1 is also consistent with how average accuracy improves
while worst-group accuracy, which is <50% at full coverage, worsens. Finally, Proposition 1 also
helps to explain why selective classification generally improves average accuracy in the literature,
as average accuracies are typically high at full coverage and margin distributions often resemble
Gaussians (Balasubramanian et al., 2011; Lakshminarayanan et al., 2017).
5.2	Skew-symmetric margin distributions
The results above apply only to symmetric margin distributions. As not all of the margin distri-
butions in Figure 4 are symmetric, we extend our analysis to asymmetric margin distributions by
building upon prior work on skew-symmetric distributions (Azzalini & Regoli, 2012).
Definition 2. A distribution with density fa,μ is skew-symmetric with skew a and center μ if
fα,μ(τ) = 2h(T - μ)G(α(τ - μ))	⑷
for all τ ∈ R, where h is the density of a distribution is symmetric about 0, and G is the CDF of a
potentially different distribution that is also symmetric about 0.
In other words, fα,μ is a skewed form of the symmetric density h, where higher α means more right
skew, and setting α = 0 yields a (translated) h. Skew-symmetric distributions are a broad family and
include, e.g., skew-normal distributions, as well as all symmetric distributions. In Appendix D.3,
we show some properties of skew-symmetric distributions as pertains to selective classification, e.g.,
margin distributions that are more right-skewed have higher accuracies (Proposition 6).
Our result is that skewing a symmetric distribution in the “same” direction preserves monotonicity:
if accuracy is monotone increasing, then right skew (which increases accuracy) preserves this.
Proposition 2 (Skew in the same direction preserves monotonicity). Let Fa,μ be the CDF ofa skew-
symmetric distribution. Ifaccuracy of its Symmetric version, Af0* (T), is monotonically increasing
in τ, then Apa* (T) is also monotonically increasing in T for any α > 0. Similarly, if Af0* (T) is
monotonically decreasing in T, then Apα,μ (T) is also monotonically decreasing in T for any α < 0.
5.3	Discussion
Proposition 1 relates the left-log-concavity of a symmetric margin distribution to the monotonicity
of selective accuracy, with the direction of monotonicity (increasing or decreasing) determined by
the full-coverage accuracy. Proposition 2 then states that ifwe have a symmetric margin distribution
with monotone accuracy, skewing it in the same direction preserves monotonicity. Combining both
propositions, We have that if Fo,μ is symmetric left-log-concave with full-coverage accuracy >50%,
then the accuracy AFa 仙(T)of any skewed Fα,μ(T) with α > 0 is monotone increasing in T.
Since accuracy-coverage curves are preserved under all odd, monotone transformations of margins
(Lemma 8 in Appendix D), these results also generalize to odd, monotone transformations of these
(skew-)symmetric distributions. As many margin distributions—in Figure 4 and in the broader lit-
erature (Balasubramanian et al., 2011; Lakshminarayanan et al., 2017)—resemble the distributions
studied above (e.g., Gaussians and skewed Gaussians), we thus expect selective classification to
improve average accuracy but worsen worst-group accuracy when worst-group accuracy at full cov-
erage is low to begin with.
An open question is how to characterize the properties of margin distributions that lead to non-
monotone behavior. For example, in Waterbirds and CheXpert-device, accuracies first increase and
then decrease with decreasing coverage (Figure 2). These two datasets have worst-group margin
distributions that have full-coverage accuracies >50% but that are left-skewed with skewness -0.30
and -0.33 respectively (Figure 4), so we cannot apply Proposition 2 to describe them.
7
Published as a conference paper at ICLR 2021
6	Analysis: Comparison to group-agnostic reference
Even if selective classification improves worst-group accuracy, it can still exacerbate group dispari-
ties, underperforming the group-agnostic reference on the worst group (Section 4). In this section,
we continue our analysis and show that while it is possible to outperform the group-agnostic refer-
ence, it is challenging to do so, especially when the accuracy disparity at full coverage is large.
Setup. Throughout this section, we decompose the margin distribution into two components F =
pFwg + (1 - p)Fothers, where Fwg and Fothers correspond to the margin distributions of the worst
group and of all other groups combined, respectively; p is the fraction of examples in the worst
group; and the worst group has strictly worse accuracy at full coverage than the other groups (i.e.,
AFwg (0) < AFothers (0)). Recall from Section 4 that for any selective classifier (i.e., any margin
distribution), its group-agnostic reference has the same average accuracy at each threshold τ but
potentially different group accuracies. We denote the worst-group accuracy of the group-agnostic
reference as AFwg(T), which can be written in terms of FWg, Fothers, and P (Appendix A.2). We
continue with notation from Section 5 otherwise, and all proofs are in Appendix E.
A selective classifier with margin distribution F is said to outperform the group-agnostic reference
on the worst group if AFwg(T) ≥ AFwg(T) for all T ≥ 0. To establish a necessary condition for out-
performing the reference, we study the neighborhood of τ = 0, which corresponds to full coverage:
Proposition 3 (Necessary condition for outperforming the group-agnostic reference). Assume that
1/2 < AFwg (0) < AFothers (0) < 1 and the worst-group density fwg(0) > 0. IfAFwg(T) ≤ AFwg (T)
for all T ≥ 0, then
fothers(0) ≤ 1 - AFothers (0)
fwg(0)	— 1 — AFwg(0)
(5)
The RHS is the ratio of full-coverage errors; the larger the disparity between the worst group and the
other groups at full coverage, the harder it is to satisfy this condition. In Appendix F, we simulate
mixtures of Gaussians and show that this condition is rarely fulfilled.
Motivated by the empirical margin distributions, we apply Proposition 3 to the setting where Fwg and
Fothers are both log-concave and are translated and scaled versions of each other. We show that the
worst group must have lower variance than the others to outperform the group-agnostic reference:
Corollary 1 (Outperforming the group-agnostic reference requires smaller scaling for log-concave
distributions). Assume that 1/2 < AFwg (0) < AFothers (0) < 1, Fwg is log-concave, and fothers (T) =
VfWg(V(T — μothers) + μwg) for all T ∈ R, where V is a Scalingfactor If AFwg(T) ≤ AFwg(T) forall
T ≥ 0, v < 1.
This is consistent with the empirical margin distributions on Waterbirds: the worst group has higher
variance, implying V > 1 as V is the ratio of the worst group’s standard deviation to the other group’s,
and it thus fails to satisfy the necessary condition for outperforming the group-agnostic reference.
A further special case is when Fwg and Fothers are log-concave and unscaled translations of each
other. Here, selective classification underperforms the group-agnostic reference at all thresholds T.
Proposition 4 (Translated log-concave distributions underperform the group-agnostic reference).
Assume Fwg and Fothers are log-concave and fothers (T) = fwg (T - d) for all T ∈ R. Then for all
T ≥ 0,
AFWg(T) ≤ AFWg(T).
(6)
This helps to explain our results on CheXpert-device, where the worst-group and average margin
distributions are approximately translations of each other, and selective classification significantly
underperforms the group-agnostic reference at all confidence thresholds.
7	Selective classification on group DRO models
Our above analysis suggests that selective classification tends to exacerbate group disparities, espe-
cially when the full-coverage disparities are large. This motivates a potential solution: by reducing
8
Published as a conference paper at ICLR 2021
Figure 5: When applied to group DRO models, which have more similar accuracies across groups
than standard models, SR selective classifiers improve average and worst-group accuracies. At low
coverages, accuracy estimates are noisy as only a few predictions are made.
Waterbirds CheXpert-device CiviIComments MuItiNLI
CeIebA
至 suωp X4-Suωp
ω6eJω>4 dnαJ6JSJOM
-2.5 0.0	2.5
Margin
」LAJLJL4
ZΛICS 一」UA
-10	1
Margin
-2	0	2
Margin
-2.5 0.0	2.5
Margin
-2.5	0.0	2.5
Margin
Figure 6: Density of margins for the average (top) and the worst-group (bottom) distributions over
margins for the group DRO model. Positive (negative) margins correspond to correct (incorrect)
predictions, and we abstain on margins closest to zero first. Unlike the selective classifiers trained
with ERM, we see similar average and worst-group distributions for group DRO.
group disparities at full coverage, models are more likely to satisfy the necessary condtion for out-
performing the group-agnostic reference defined in Proposition 3. In this section, we explore this
approach by training models using group distributionally robust optimization (group DRO) (Sagawa
et al., 2020), which minimizes the worst-group training loss LDRO(θ) = maxg∈G E [`(θ; (x, y)) | g].
Unlike standard training, group DRO uses group annotations at training time. As with prior work,
we found that group DRO models have much smaller full-coverage disparities (Figure 5). Moreover,
worst-group accuracies consistently improve as coverage decreases and at a rate that is comparable
to the group-agnostic reference, though small gaps remain on Waterbirds and CheXpert-device.
While our theoretical analysis motivates the above approach, the analysis ultimately depends on
the margin distributions of each group, not just on their full-coverage accuracies. Although group
DRO only optimizes for similar full-coverage accuracies across groups, we found that it also leads
to much more similar average and worst-group margin distributions compared to ERM (Figure 6),
explaining why selective classification behaves more uniformly over groups across all datasets.
Group DRO is not a silver bullet, as it relies on group annotations for training, which are not al-
ways available. Nevertheless, these results show that closing full-coverage accuracy disparities can
mitigate the downstream disparities caused by selective classification.
8	Discussion
We have shown that selective classification can magnify group disparities and should therefore be
applied with caution. This is an insidious failure mode, since selective classification generally im-
proves average accuracy and can appear to be working well if we do not look at group accuracies.
However, we also found that selective classification can still work well on models that have equal
full-coverage accuracies across groups. Training such models, especially without relying on too
much additional information at training time, remains an important research direction. On the theo-
retical side, we characterized the behavior of selective classification in terms of the margin distribu-
tions; an open question is how different margin distributions arise from different data distributions,
models, training procedures, and selective classification algorithms. Finally, in this paper we focused
on studying selective accuracy in isolation; accounting for the cost of abstention and the equity of
different coverages on different groups is an important direction for future work.
9
Published as a conference paper at ICLR 2021
Acknowledgments
We thank Emma Pierson, Jean Feng, Pranav Rajpurkar, and Tengyu Ma for helpful advice. This
work was supported by NSF Award Grant no. 1804222. SS was supported by the Herbert Kunzel
Stanford Graduate Fellowship and AK was supported by the Stanford Graduate Fellowship.
Reproducibility
All code, data, and experiments are available on CodaLab at https://worksheets.
codalab.org/worksheets/0x7ceb817d53b94b0c8294a7a22643bf5e. The code is
also available on GitHub at https://github.com/ejones313/worst- group- sc.
References
Adelchi Azzalini and Giuliana Regoli. Some properties of skew-symmetric distributions. Annals of
the Institute of Statistical Mathematics, 64(4):857-879, 2012.
Marcus A Badgeley, John R Zech, Luke Oakden-Rayner, Benjamin S Glicksberg, Manway Liu,
William Gale, Michael V McConnell, Bethany Percha, Thomas M Snyder, and Joel T Dudley.
Deep learning predicts hip fracture using confounding patient and healthcare variables. npj Digital
Medicine, 2, 2019.
Mark Bagnoli and Ted Bergstrom. Log-concave probability and its applications. Economic Theory,
26:445-469, 2005.
Krishnakumar Balasubramanian, Pinar Donmez, and Guy Lebanon. Unsupervised supervised
learning II: Margin-based classification without labels. Journal of Machine Learning Research
(JMLR), 12:3119-3145, 2011.
Peter L Bartlett and Marten H Wegkamp. Classification with a reject option using a hinge loss.
Journal of Machine Learning Research (JMLR), 9(0):1823-1840, 2008.
Su Lin Blodgett, Lisa Green, and Brendan O’Connor. Demographic dialectal variation in social
media: A case study of African-American English. In Empirical Methods in Natural Language
Processing (EMNLP), pp. 1119-1130, 2016.
Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Nuanced
metrics for measuring unintended bias with real data for text classification. In World Wide Web
(WWW), pp. 491-500, 2019.
Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in com-
mercial gender classification. In Conference on Fairness, Accountability and Transparency, pp.
77-91, 2018.
Irene Y Chen, Emma Pierson, Sherri Rose, Shalmali Joshi, Kadija Ferryman, and Marzyeh Ghas-
semi. Ethical machine learning in health. arXiv preprint arXiv:2009.10576, 2020.
C. K. Chow. An optimum character recognition system using decision functions. In IRE Transac-
tions on Electronic Computers, 1957.
Chao K Chow. On optimum recognition error and reject tradeoff. IEEE Transactions on Information
Theory, 16(1):41-46, 1970.
Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Algorithmic decision
making and the cost of fairness. In International Conference on Knowledge Discovery and Data
Mining (KDD), pp. 797-806, 2017.
Luigi Pietro Cordella, Claudio De Stefano, Francesco Tortorella, and Mario Vento. A method for
improving classification reliability of multilayer perceptrons. IEEE Transactions on Neural Net-
works, 6(5):1140-1147, 1995.
10
Published as a conference paper at ICLR 2021
Madeleine Cule, Richard Samworth, and Michael Stewart. Maximum likelihood estimation of a
multi-dimensional log-concave density. Journal of the Royal Statistical Society, 73:545-603,
2010.
Abir De, Paramita Koley, Niloy Ganguly, and Manuel Gomez-Rodriguez. Regression under human
assistance. In Association for the Advancement of Artificial Intelligence (AAAI), pp. 2611-2620,
2020.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Association for Computational Linguis-
tics (ACL), pp. 4171-4186, 2019.
Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Measuring and mit-
igating unintended bias in text classification. In Association for the Advancement of Artificial
Intelligence (AAAI), pp. 67-73, 2018.
John Duchi, Tatsunori Hashimoto, and Hongseok Namkoong. Distributionally robust losses
against mixture covariate shifts. https://cs.stanford.edu/~thashim/assets/
publications/condrisk.pdf, 2019.
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Rich Zemel. Fairness through
awareness. In Innovations in Theoretical Computer Science (ITCS), pp. 214-226, 2012.
Ran El-Yaniv and Yair Wiener. On the foundations of noise-free selective classification. Journal of
Machine Learning Research (JMLR), 11, 2010.
Jean Feng, Arjun Sondhi, Jessica Perry, and Noah Simon. Selective prediction-set models with
coverage guarantees. arXiv preprint arXiv:1906.05473, 2019.
Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model
uncertainty in deep learning. In International Conference on Machine Learning (ICML), 2016.
Yonatan Geifman and Ran El-Yaniv. Selective classification for deep neural networks. In Advances
in Neural Information Processing Systems (NeurIPS), 2017.
Yonatan Geifman and Ran El-Yaniv. Selectivenet: A deep neural network with an integrated reject
option. In International Conference on Machine Learning (ICML), 2019.
Yonatan Geifman, Guy Uziel, and Ran El-Yaniv. Bias-reduced uncertainty estimation for deep
neural classifiers. In International Conference on Learning Representations (ICLR), 2018.
Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and
Noah A Smith. Annotation artifacts in natural language inference data. In Association for Com-
putational Linguistics (ACL), pp. 107-112, 2018.
Blaise Hanczar and Edward R. Dougherty. Classification with reject option in gene expression data.
Bioinformatics, 2008.
Moritz Hardt, Eric Price, and Nathan Srebo. Equality of opportunity in supervised learning. In
Advances in Neural Information Processing Systems (NeurIPS), pp. 3315-3323, 2016.
Tatsunori B. Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness with-
out demographics in repeated loss minimization. In International Conference on Machine Learn-
ing (ICML), 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Computer Vision and Pattern Recognition (CVPR), 2016.
Martin Hellman and Josef Raviv. Probability of error, equivocation, and the chernoff bound. IEEE
Transactions on Information Theory, 16(4):368-372, 1970.
Martin E Hellman. The nearest neighbor classification rule with a reject option. IEEE Transactions
on Systems Science and Cybernetics, 6(3):179-185, 1970.
11
Published as a conference paper at ICLR 2021
Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution
examples in neural networks. In International Conference on Learning Representations (ICLR),
2017.
Kashmir Hill. Wrongfully accused by an algorithm. The New York Times, 2020. URL https://
www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.
html.
Dirk Hovy and Anders S0gaard. Tagging performance correlates with age. In Association for
Computational Linguistics (ACL),pp. 483-488, 2015.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 4700-4708, 2017.
Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik
Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: A large chest radio-
graph dataset with uncertainty labels and expert comparison. In Association for the Advancement
of Artificial Intelligence (AAAI), volume 33, pp. 590-597, 2019.
Shalmali Joshi, Oluwasanmi Koyejo, Been Kim, and Joydeep Ghosh. xGEMs: Generating exam-
plars to explain black-box models. arXiv preprint arXiv:1806.08867, 2018.
Amita Kamath, Robin Jia, and Percy Liang. Selective question answering under domain shift. In
Association for Computational Linguistics (ACL), 2020.
Fereshte Khani, Martin Rinard, and Percy Liang. Unanimous prediction for 100% precision with
application to learning semantic mappings. In Association for Computational Linguistics (ACL),
2016.
Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determi-
nation of risk scores. In Innovations in Theoretical Computer Science (ITCS), 2017.
Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsub-
ramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne
David, Ian Stavness, Wei Guo, Berton A. Earnshaw, Imran S. Haque, Sara Beery, Jure Leskovec,
Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. WILDS: A
benchmark of in-the-wild distribution shifts. arXiv, 2020.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. In Advances in Neural Information Processing
Systems (NeurIPS), 2017.
Shiyu Liang, Yixuan Li, and R. Srikant. Enhancing the reliability of out-of-distribution image
detection in neural networks. In International Conference on Learning Representations (ICLR),
2018.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of the IEEE International Conference on Computer Vision, pp. 3730-3738, 2015.
R Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic
heuristics in natural language inference. In Association for Computational Linguistics (ACL),
2019.
Hussein Mozannar and David Sontag. Consistent estimators for learning to defer to an expert. arXiv
preprint arXiv:2006.01862, 2020.
LUke Oakden-Rayner, Jared Dunnmon, Gustavo Carneiro, and Christopher Re. Hidden stratification
causes clinically meaningful failures in machine learning for medical imaging. In Proceedings of
the ACM Conference on Health, Inference, and Learning, pp. 151-159, 2020.
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D Sculley, Sebastian Nowozin, Joshua V. Dil-
lon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty? eval-
uating predictive uncertainty under dataset shift. In Advances in Neural Information Processing
Systems (NeurIPS), 2019.
12
Published as a conference paper at ICLR 2021
Ji Ho Park, Jamin Shin, and Pascale Fung. Reducing gender bias in abusive language detection. In
Empirical Methods in Natural Language Processing (EMNLP),pp. 2799-2804, 2018.
Marco AF Pimentel, David A Clifton, Lei Clifton, and Lionel Tarassenko. A review of novelty
detection. Signal Processing, 99:215-249, 2014.
Jose M PorceL Chest tube drainage of the pleural space: a concise review for pulmonologists.
Tuberculosis and Respiratory Diseases, 81(2):106-115, 2018.
Maithra Raghu, Katy Blumer, Rory Sayres, Ziad Obermeyer, Bobby Kleinberg, Sendhil Mul-
lainathan, and Jon Kleinberg. Direct uncertainty prediction for medical second opinions. In
International Conference on Machine Learning (ICML), pp. 5281-5290, 2019.
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ”Why Should I Trust You?”: Explaining
the predictions of any classifier. In International Conference on Knowledge Discovery and Data
Mining (KDD), 2016.
Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust
neural networks for group shifts: On the importance of regularization for worst-case generaliza-
tion. In International Conference on Learning Representations (ICLR), 2020.
Rachael Tatman. Gender and dialect bias in YouTube’s automatic captions. In Workshop on Ethics
in Natural Langauge Processing, volume 1, pp. 53-59, 2017.
Marko Toplak, Rok Mocnik, Matija Polajnar, Zoran Bosnic, Lars Carlsson, Catrin Hasselgren, Janez
Demsar, Scott Boyer, Blaz Zupan, and Jonna Stalring. Assessment of machine learning reliabil-
ity methods for quantifying the applicability domain of QSAR regression models. Journal of
Chemical Information and Modeling, 54, 2014.
C Wah, S Branson, P Welinder, P Perona, and S Belongie. The Caltech-UCSD Birds-200-2011
dataset. Technical report, California Institute of Technology, 2011.
Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Association for Computational Linguistics (ACL), pp.
1112-1122, 2018.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, R’emi Louf, Morgan Funtowicz, and Jamie Brew. HuggingFace’s
transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771,
2019.
Kai Xiao, Logan Engstrom, Andrew Ilyas, and Aleksander Madry. Noise or signal: The role of
image backgrounds in object recognition. arXiv preprint arXiv:2006.09994, 2020.
Dong Yu, Jinyu Li, and Li Deng. Calibration of confidence measures in speech recognition. Trans.
Audio, Speech and Lang. Proc., 19(8):2461-2473, 2011.
Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 mil-
lion image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 40(6):1452-1464, 2017.
13
Published as a conference paper at ICLR 2021
A Setup
A.1 Softmax response selective classifiers
In this section, we describe our implementation of softmax response (SR) selective classifiers (Geif-
man & El-Yaniv, 2017). Recall from Section 3 that a selective classifier is a pair (y, ^), where
y : X → Y outputs a prediction and c : X → R+ outputs the model's confidence, which is always
non-negative, in that prediction. SR classifiers are defined for neural networks (for classification),
which generally have a last softmax layer over the k possible classes. For an input point x, we denote
its maximum softmax probability, which corresponds to its predicted class y(x), as p^(y(χ) | x). We
defined the confidence C(X) for binary classifiers as
C(X)TOg( Jyx 1;)).	⑺
2 V - p(y(χ) | χ)√
Since the maximum softmax probability p^(y(x) | x) is at least 0.5 for binary classification, C(X) is
nonnegative for each x, and is thus a valid confidence. For k > 2 classes, however, p^(y(χ) | x)
can be less than 0.5, in which case c(x) would be negative. To ensure that confidence is always
nonnegative, we define c(x) for k classes to be
C(X) = 1 log (] p(y(x) [x))) + 1 log(k -1).	(8)
2	V - p(y(x) | x)J 2
With k classes, the maximum softmax probability p^(y(x) | x) ≥ 1/k, and therefore we can verify
that C(x) ≥ 0 as desired. Moreover, when k = 2, Equation (8) reduces to our original binary
confidence; we can therefore interpret the general form in Equation (8) as a normalized logit.
Note that C(x) is a monotone transformation of the maximum softmax probability p(y(x) | x).
Since the accuracy-coverage curve of a selective classifier only depends on the relative ranking of
C(x) across points, we could have equivalently set C(x) to be p(y(x) | x). However, following prior
work, we choose the logit-transformed version to make the corresponding distribution of confidences
easier to visualize (Balasubramanian et al., 2011; Lakshminarayanan et al., 2017).
Finally, we remark on one consequence of SR on the margin distribution for multi-class classifica-
tion. Recall that we define the margin of an example to be C(x) on correct predictions (y(x) = y)
and -C(x) otherwise, as described in Section 3. In Figure 4, we plot the margin distributions of SR
selective classifiers on all five datasets. We observe that on MultiNLI, which is the only multi-class
dataset (with k = 3), there is a gap (region of lower density) in the margin distribution around 0.
We attribute this gap in part to the comparative rarity of seeing a maximum softmax probability of
3 when k = 3 versus seeing 1 when k = 2; in the former, all three logits must be the same, while
for the latter only two logits must be the same.
A.2 Group-agnostic reference
Here, we describe the group-agnostic reference described in Section 4 in more detail. We elaborate
on the construction from the main text and then define the reference formally. Finally, we show that
the group-agnostic reference satisfies equalized odds.
A.2. 1 Definition
We begin by recalling on the construction described in the main text for a finite test set D. In
Algorithm 1 we relied on two important sets; the set of correctly classified points at threshold τ, Cτ:
CT = {(x,y,g) ∈ D | y(x) = y and C(x) ≥ T},	(9)
and similarly, the set of incorrectly classified points at threshold τ, Iτ :
ZT = {(x,y,g) ∈ D | y(x) = y and C(x) ≥ T}.	(10)
To compute the accuracy of the group-agnostic reference, we sample a subset CTa of size |C「| uni-
formly at random from C0, and similarly a subset ITga of size |IT| uniformly at random from I0. The
group-agnostic reference makes predictions when examples are in CTga ∪ ITga and abstains otherwise.
We compute group accuracies over this set of predicted examples.
14
Published as a conference paper at ICLR 2021
Note that the group accuracies, as defined, are randomized due to the sampling. For the remainder of
our analysis, we take the expectation over this randomness to compute the group accuracies. We now
generalize the above construction by considering data distributions D. We first define the number of
correctly and incorrectly classified points:
Definition 3 (Correctly and incorrectly classified points). Consider a selective classifier (y, C). For
each threshold τ, we define the fractions of points that are predicted (not abstained on), and correctly
or incorrectly classified, as
C(T) = P(y(χ) = y ∧ C(x) ≥ T),	(11)
I(τ) = p(y(χ) = y ∧ C(x) ≥ T).	(12)
We define analogous metrics for each group g as
Cg(τ) = p(y(χ) = y ∧ C(x) ≥ T | g),	(13)
Ig(τ) = p(y(χ) = y ∧ C(χ) ≥ T | g).	(14)
For each threshold T, we will make predictions on a C(T)/C(0) fraction of the C(0) total (proba-
bility mass of) correctly classified points. Since each group g has Cg (0) correctly classified points,
at threshold T, the group-agnostic reference will make predictions on Cg (0)C (T)/C (0) correctly
classified points in group g. We can reason similarly over the incorrectly classified points. Putting it
all together, we can define the group-agnostic reference as satisfying the following:
Definition 4 (Group-agnostic reference). Consider a SehctIve classifier (y, c) and let C, I, Cg, Ig
denote the analogous quantities to Definition 3 for its matching group-agnostic reference. For each
threshold T, these satisfy
~
C(T) = C(T)
~
I(T )= I (t ),
and for each threshold T and group g,
~ , .....
Cg (t ) = Cg (0)C (T )∕C(0)
~ , ....................................................
Ig (τ )= Ig (0)I (τ )∕I (0).
The group-agnostic reference thus has the following accuracy on group g:
〜
Ag (t )
~ , .
Cg (T)
~ ~
Cg(T)+ Ig (t )
Cg (O)C(T )∕C(0)
Cg(O)C(T)∕C(0) + Ig(O)I(τ)∕I(0).
(15)
(16)
(17)
(18)
(19)
(20)
A.2.2 Connection to equalized odds
We now show that the group-agnostic reference satisfies equalized odds with respect to which points
it predicts or abstains on. The goal of a selective classifier is to make predictions on points it would
get correct (i.e., y(x) = y) while abstaining on points that it would have gotten incorrect (i.e.,
y(χ) = y). We can view this as a meta classification problem, where a true positive is when the
selective classifier decides to make a prediction on a point X and gets it correct (y(x) = y), and a
false positive is when the selective classifier decides to make a prediction on a point x and gets it
incorrect (y(x) = y). As such, We can define the true positive rate RTP(T) and false positive rate
RFP(T) of a selective classifier:
Definition 5. The true positive rate of a selective classifier at threshold T is
RTP(T)
C(T)
C (O),
and the false positive rate at threshold T is
RFP(T)
I (T)
I(0).
(21)
(22)
15
Published as a conference paper at ICLR 2021
Analogously, the true positive and false positive rates on a group g are
RgP(T)=C⅛),	(23)
RgP(T)=*.	(24)
The group-agnostic reference satisfies equalized odds (Hardt et al., 2016) with respect to this defini-
tion:
Proposition 5. The group-agnostic reference defined in Definition 4 has equal true positive and
false positive rates for all groups g ∈ G and satisfies equalized odds.
Proof. By construction of the group-agnostic reference (Definition 4), we have that
~ . .....
Cg(T) = Cg(0)C(τ )/C (0)	(25)
~ , ~ , . . ~ ,
=Cg (0)C(T )∕C(0),	(26)
and therefore, for each group g, we can show that the true-positive rate of the group-agnostic refer-
ence RgTP on g is equal to its average true-positive rate RTP(T).
RTP(T)
~ , .
Cg (T)
~ , .
Cg (0)
~ ~
C(t )∕C(0)
RTP(T).
(27)
(28)
(29)
Each group thus has the same true positive rate with the group-agnostic reference. Using similar
reasoning, each group also has the same false positive rate. By the definition of equalized odds, the
group-agnostic reference thus satisfies equalized odds.	口
A.3 Robin Hood reference
In this section, we define the Robin Hood reference, which preferentially increases the worst-group
accuracy through abstentions until it matches the other groups. Like the group-agnostic reference,
we constrain the Robin Hood reference to make the same number of correct and incorrect predictions
as a given selective classifier.
We formalize the definition of the Robin Hood reference in Algorithm 2. The Robin Hood refer-
ence makes predictions on the subset of examples from D that has the smallest discrepency between
best-group and worst-group accuracies, while still matching the number of correct and incorrect
predictions ofa given selective classifier. Since enumerating over all possible subsets of the test data
Algorithm 2: Robin Hood reference at threshold T
Input: Selective classifier (y, ^), threshold T, test data D
Output: The set of points P ⊆ D that the Robin Hood reference for (y, ^) makes predictions
on at threshold T .
1	In Algorithm 1, we defined Cτ and Iτ to be the sets of correct and incorrect points predicted
on and abstained on at threshold T respectively. Define Q, the set of subsets ofD that have
the same number of correct and incorrect predictions as (y, ^) at threshold T as follows:
Q = {S⊆D∣∣P∩Co∣ = ∣Cτ∣,∣P∩Io∣ = Z |}.	(30)
2	Let accg (S) be the accuracy of y on points in S that belong to group g. We return the set P
that minimizes the difference between the best-group and worst-group accuracies.
P = argmin max accg (S) - minaccg(S) .	(31)
S∈Q g∈G	g∈G
16
Published as a conference paper at ICLR 2021
D is intractable, we compute the accuracies of the Robin Hood reference iteratively. Starting from
full coverage, we abstain on examples from lowest to highest confidence. Whenever we abstain
on an incorrect example, we assume it comes from the current lowest accuracy group, and simi-
larly whenever we abstain on a correctly classified example we assume it comes from the current
highest accuracy group. This reduces disparities to the maximum extent possible as the threshold τ
increases.
B S upplemental experiments
B.1	Monte-Carlo dropout
In the main text, we observed that SR selective classifiers monotonically improve average accuracy
as coverage decreases, but exacerbate accuracy disparities across groups on all five datasets. To
demonstrate that these observations are not specific to SR selective classifiers, we now present our
empirical results on another standard selective classification method: Monte-Carlo (MC) dropout
(Gal & Ghahramani, 2016; Geifman & El-Yaniv, 2017). We find that MC-dropout selective classi-
fiers exhibit similar empirical trends as SR selective classifiers.
MC-dropout selective classifiers. MC-dropout is an alternate way of assigning confidences to
points. Taking a model with a dropout layer, the selective classifier first predicts y(x) simply by
taking the model output without dropout. To estimate the confidence, it then samples n softmax
probabilities corresponding to the label y(x) over the randomness of the dropout layer. The con-
fidence is computed as C(X) = 1/s, where s2 is the variance of the sampled probabilities. We
implement MC-dropout by using the existing dropout layers for BERT and adding dropout to the
final fully-connected layer for ResNet and DenseNet, with a dropout probability 0.1. We present
empirical results for n = 10; Gal & Ghahramani (2016) observed that this was sufficient to produce
good confidence estimates.
Results. The MC-dropout selective classifiers exhibit similar trends to those that observed in Section
4, demonstrating that the observed empirical trends are not specific to SR response; even though the
average accuracy improves monotonically as coverage decreases across all five datasets, the worst-
group accuracy tends to decrease for CelebA, fails to increase consistently for CheXpert, Waterbirds,
and CivilComments, and increases consistently but slowly for MultiNLI. Comparing SR and MC-
dropout selective classifiers, we observe that the MC-dropout selective classifiers performs slightly
worse. For example, we see a more prominent drop in worst-group accuracy for Waterbirds and
much smaller improvements in worst-group accuracy for CheXpert.
Figure 7: Selective accuracy (top) and coverage (bottom) for each group, as a function of the average
coverage for the MC-dropout selective classifier. Each average coverage corresponds to a threshold
τ . The red lines represent the worst group.
17
Published as a conference paper at ICLR 2021
CeIebA
CiviIComrTients
Waterbirds
CheXpert-device
MuItiNLI
Figure 8: Selective accuracy (top) and coverage (bottom) for each group as a function of the average
coverage for the softmax response selective classifier with model optimized with group DRO. Each
average coverage corresponds to a specific threshold τ. The red lines represent the worst group (i.e.
the one with the lowest accuracy at full coverage,) and the gray lines represent the other groups.
B.2	Group DRO
We showed in Section 7 that SR selective classifiers trained with the group DRO objective success-
fully improve worst-group selective accuracies as coverage decreases, and perform comparably to
the group-agnostic reference. We now present additional empirical results for these selective classi-
fiers.
Accuracy-coverage curves. We first present the accuracy-coverage curves for all groups, along
with the group coverage trends, in Figure 8. Group selective accuracies tend to improve mono-
tonically, in stark contrast with our results on standard (ERM) selective classifiers. These trends
hold generally across groups and datasets, with a few exceptions; selective accuracies drop on two
groups in MultiNLI at roughly 20% average coverage, and selective accuracies improve slowly on
two groups in CivilComments. Below, we look into these anomalies and offer potential explanations.
For MultiNLI, we first note that the drops are observed at very low group coverages—lower than
1%—at which point the accuracies are computed based on very few examples and thus are noisy.
In addition, much of the anomaly can be explained by label noise; we manually inspect the 20
examples from the above two groups with the highest softmax confidences, and find that 17 of them
are labeled incorrectly. The observations on CivilComments can also be potentially attributed to
label noise. Removing examples with high inter-annotator disagreement (with the fraction of toxic
annotations between 0.5 and 0.6) yields an accuracy-coverage curve that fits the broader empirical
trends.
C Experiment details
C.1 Datasets
CelebA. Models have been shown to latch onto spurious correlations between labels and demo-
graphic attributes such as race and gender (Buolamwini & Gebru, 2018; Joshi et al., 2018), and we
study this on the CelebA dataset (Liu et al., 2015). Following Sagawa et al. (2020), we consider
the task of classifying hair color, which is spuriously correlated with the gender. Concretely, inputs
are celebrity face images, labels are hair color Y = {blond, non-blond}, and spurious attributes are
gender, A = {male, female}, with blondness associated with being female. Of the four groups,
blond males are the smallest group, with only 1,387 examples out of 162,770 training examples, and
they tend to be the worst group empirically. We use the official train-val-split of the dataset.
18
Published as a conference paper at ICLR 2021
Waterbirds. Object recognition models are prone to using image backgrounds as a proxy for the
label (Ribeiro et al., 2016; Xiao et al., 2020). We study this on the Waterbirds dataset (Sagawa
et al., 2020), constructed using images of birds from the Caltech-UCSD Birds dataset (Wah et al.,
2011) placed on backgrounds from the Places dataset (Zhou et al., 2017). The task is to classify a
photograph of a bird as one ofY = {waterbird, landbird}, and the label is spuriously correlated with
the background A = {water background, land background}. Of the four groups, waterbirds on land
backgrounds make up the smallest group with only 56 examples out of 4,795 training examples,
and they tend to be the worst group empirically. We use the train-val-split provided by Sagawa et al.
(2020), and also follow their protocol for computing average metrics; to compute average accuracies
and coverages, we first compute the metrics for each group and obtain a weighted average according
to group proportions in the training set, in order to account for the discrepancy in group proportions
across the splits.
CheXpert-device. Models can latch onto spurious correlations even in high-stakes applica-
tions such as medical imaging. When models are trained to classify whether a patient has
certain pathologies from chest X-rays, models have been shown to spuriously detect the pres-
ence of a support device, in particular a chest drain, instead (Oakden-Rayner et al., 2020).
We study this phenomenon in a modified version of the CheXpert dataset (Irvin et al., 2019),
which we call CheXpert-device. Concretely, the inputs are chest X-rays, labels are Y =
{pleural effusion, no pleural effusion}, and spurious attributes indicate the the presence of a support
device, A = {support device, no support device}. We note that chest drain is one type of support
device, and is used to treat suspected pleural effusion (Porcel, 2018).
CheXpert-device is a subsampled version of the full CheXpert dataset that manifests the spurious
correlation more strongly. To create CheXpert-device, we first create a new 80/10/10 train/val/test
split of examples from the publicly available CheXpert train and validation sets, randomly assigning
patients to splits so that all X-rays of the same patient fall in the same split. We then subsample the
training set; in particular, we enforce that in 90% of examples, the label of support device matches
the pleural effusion label. Of the four groups, cases of pleural effusion without a support device
make up the smallest group, with 5,467 examples out of 112,100 training examples, and they tend
to be the worst group empirically.
To compute the average accuracies and coverages, we weight groups according to group proportions
in the training set, similarly to Waterbirds. Another complication with CheXpert is that some patients
can have multiple X-rays from one visit. Following Irvin et al. (2019), we treat these images as
separate training examples at training time, but output one prediction for each patient-study pair at
evaluation time. Concretely, we predict pleural effusion if the model detects the condition in any
of the X-ray images belonging to the patient-study pair, as pathologies may only appear clearly in
some X-rays.
CivilComments. In toxicity comment detection, models have been shown to latch onto spurious
correlations between the toxicity and mention of certain demographic groups (Park et al., 2018;
Dixon et al., 2018). We study this in the CivilComments dataset (Borkan et al., 2019). The task
is to classify the toxicity of comments on online articles with labels Y = {toxic, non-toxic}.
As spurious attributes, we consider whether each comment mentions a Christian identity, A =
{mention of Christian identity, no mention of Christian identity}; non-toxicity is associated with
the mention of Christian identity, often resulting in high false negative rate on comments with such
mentions. Of the four groups, toxic comments with a mention of Christian identity make up the
smallest group, with only 2,446 examples out of 269,038 training examples, and they tend to be the
worst group empirically. and further split the development set into a training set and a validation set
by randomly splitting on articles and associating all comments with each article to either set. We
use the train, validation, and test set from the WILDS benchmark (Koh et al., 2020). The training,
validation, and test set all comprise comments from disjoint sets of articles. The original dataset
also contains many additional examples that have toxicity annotations but not identity annotations;
we do not use these in our experiments.
In the original CivilComments dataset, each comment is given a probabilistic labels for both the
toxicity and the mention of a Christian identity, where a probabilistic label is the average of binary
19
Published as a conference paper at ICLR 2021
labels across annotators. Following the associated Kaggle competition1, we use binarized labels
obtained by thresholding the probabilistic labels at 0.5.
MultiNLI. Lastly, we consider natural langage inference (NLI), where the task is to predict
whether a hypothesis is entailed, contradicted by, or neutral to an associated premise, Y =
{entailed, contradictory, neutral}. NLI models have been shown to exploit annotation artifacts, for
example predicting contradictory whenever negation words such as never or nobody are present (Gu-
rurangan et al., 2018). We study this on the MultiNLI dataset (Williams et al., 2018). To annotate
examples’ spurious attributes A = {negation words, no negation words}, we consider the following
negation words following Gururangan et al. (2018): “nobody”, “no”, “never”, and “nothing”. We
use the splits used in Sagawa et al. (2020), whose training set includes 206,175 examples with 1,521
examples from the smallest group (entailment with negations). The worst group for standard models
tends to be neutral examples with negation words.
C.2 Models
We train ResNet (He et al., 2016) for CelebA and Waterbirds (images), DenseNet (Huang et al.,
2017) for CheXpert (X-rays), and BERT (Devlin et al., 2019) for CivilComments and MultiNLI
(text). For tasks studied in Sagawa et al. (2020) (CelebA, Waterbirds, MultiNLI), we use the hyper-
parameters from Sagawa et al. (2020). For others (CivilComments and CheXpert-device), we test
the same number of hyperparameter sets for each of ERM and DRO, and report the best set below.
Across all image and X-ray tasks, inputs are downsampled to resolution 224 x 224.
CelebA. To train a model on CelebA, we initialize to pretrained ResNet-50. For ERM we optimize
with learning rate 1e-4, weight decay 1e-4, batch size 128, and train for 50 epochs. For DRO we
use learning rate 1e-5, weight decay 1e-1, and use generalization adjustment 1 (described in Sagawa
et al. (2020)). The batch size is 128, and we train for 50 epochs.
Waterbirds. For Waterbirds, as with CelebA, we use pretrained ResNet-50 as an initialization. For
ERM we use learning rate 1e-3, weight decay 1e-4, batch size 128, and train for 300 epochs. For
DRO we use learning rate 1e-5, weight decay 1, geneneralization adjustment 1, batch size 128, and
train for 300 epochs.
CheXpert-device. For CheXpert-device, we fine-tune pretrained DenseNet-121 for three epochs.
For ERM we use learning rate 1e-3, no weight decay, batch size 16, and choose the model (out of
the first three epochs) with highest average accuracy (epoch 2). For DRO, we use learning rate 1e-
4, weight decay 1e-1, and batch size 16, and choose the model with highest worst-group accuracy
(epoch 1).
CivilComments. To train a model for CivilComments, we fine-tune bert-base-uncased using the
implementation from Wolf et al. (2019). For both ERM and DRO we use learning rate 1e-5, weight
decay 1e-2, and batch size 16. We train the ERM models and DRO models for three epochs (early
stopping,) then choose the model with highest average accuracy for ERM (epoch 2), and highest
worst-group accuracy for DRO (epoch 1).
MultiNLI. For MultiNLI we again fine-tune bert-base-uncased using the implementation from Wolf
et al. (2019). For ERM, we fine-tune for three epochs with learning rate 2e-5, weight decay 0, and
batch size 32. For DRO, we also use learning rate 2e-5, and weight decay 0, and batch size 32, but
use generalization adjustment 1. For both ERM and DRO the model after the third epoch is best in
terms of average accuracy for ERM and worst-group accuracy for DRO.
D	Proofs: Margin distributions and accuracy-coverage curves
D. 1 Left-log-concavity
Recall the definition of left-log-concave from Section 5:
Definition 1 (Left-log-concave distributions). A distribution is left-log-concave if its CDF is log-
concave on (一∞,μ], where μ is the mean of the distribution.
1www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/
20
Published as a conference paper at ICLR 2021
We first prove that a symmetric mixture of Gaussians is left-log-concave, but not necessarily log-
concave.
D.1.1 Symmetric mixtures of Gaussians are left-log-concave
Lemma 1 (Symmetric mixtures of two gaussians are left-log-concave). Consider a symmetric mix-
ture of Gaussians with density f = 0.5fμ + 0.5f-μ, where fμ is the density of a N (μ, σ2) random
variable and likewise f-μ is the density of a N(一μ,σ2) random variable. Then the mixture is
left-log-concave for all values of μ ∈ R,σ > 0, but only log-concave if ∣μ∣ ≤ σ.
Proof. Without loss of generality, we can take σ = 1, since (left-)log-concavity is invariant to
scaling, and also assume that μ is positive. First consider the case where μ ≤ 1. Then the mixture is
log-concave, and therefore left-log-concave (Cule et al., 2010).
Now, consider the case where μ > 1. Cule et al. (2010) show the mixture is no longer log-concave.
However, we claim that it is still left-log-concave. We start by studying the gradient of log f,
f0(x) _ — exp(一(X-μ)2 )(x — μ) 一 exp(一(X+" )(χ + μ)
——~~— = ------------:--TT；--------: ： TT；-----
f (x)	exp(-(x-μ2) +exp(-(x+μ2)
-X + μ
1 一 exp(-2xμ)
1 + expj2xμ)
(33)
We claim that fx) has a local minimum at X = _a < 0, and as it is an odd function, a correspond-
ing local maximum at x = a > 0. To show this, we first differentiate to obtain
d f0(x)	]	4M2€乂「(一2/丛)
dx f(x)	+ (1+exp(—2xμ))2
Setting the derivative to 0 gives us the quadratic equation
	0	4μ2 exp(.2xμ) 二 一 1 + -.	；	一 C (1 + expj2xμ))2	(35)
^⇒	(1 + expj2xμ))2 二	=4μ2 exp(ι2xμ)	(36)
^⇒	[exp(—2xμ)]2 + (2 一 4μ2) expj2xμ) + 1 二	0	(37)
^⇒	expj2xμ)二	=2μ2 - 1 ± 2μPμ2 - 1.	(38)
Since μ > 1, there are two distinct roots of this quadratic, and two corresponding critical points
of f0∕f. Let v(μ) = 2μ2 一 1 + 2μ,μ2 — 1 be the larger root. Then v(μ) is a strictly increasing
function for μ ≥ 1, and since v(1) = 1, we have that v(μ) > 1 for all μ > 1. Let X = _a satisfy
exp(2aμ) = v(μ). Then we have that -a,the smaller of the critical points of f0∕f, is
一a
log v(μ)
	
2μ
log(2μ2 一 1 + 2μ∙∖∕μ2 - 1)
2μ
< 0.
To show that f 0 (a)∕f (a) is a local minimum, we take the second derivative
d2 f0(x)	d ]	4μ2exp(~2xμ)
dx2 f(x)	dx + (1 +expj2xμ))2
8μ3 exp(_2xμ)(exp(_2xμ) 一 1)
(exp(.2xμ) + 1)3	，
which at X = _a gives
d2 f0(x) ।	_ 8μ3v(μ)(v(μ) — 1)
dx2 f(x) x=-a	(v(μ) + 1)3
> 0,
(39)
(40)
(41)
(42)
(43)
(44)
(45)
21
Published as a conference paper at ICLR 2021
since v(μ) > 1. Since -a is the only critical point of f /f that is less than 0 and it is a local
minimum, f0/f must be decreasing on (-∞, -a], which in turn implies that f, and therefore F, is
log-concave on (-∞, -a].
It remains to show that F is also log-concave on [-a, 0]. We make use of two facts. First, since -a
is a local minimum and the only critical point less than 0, we have thatf-a) ≤ 需 ≤ 儒=0
for all x ∈ [-a, 0]. Second, since f(x) and F (x) are non-negative for all x, f (x)/F (x) is also
non-negative for all x. Thus, for all x ∈ [-a, 0]
d f(x)	F(x)f0(x) - f(x)2			(46)
dx F(x)	F (x)2		
_ f (X) =	:~~- F (x)	f f 0(x) V (x)	f (x) ʌ F (X)J	(47)
l-{z"}	v∣-{z-}	∣∙{z"}∕	
≥0	≤0	≥0	
≤ 0,			(48)
and therefore F is also log-concave on [-a, 0].			□
Remark 1. Note that if f is (left-)log-concave, then F is also (left-)log-concave (Bagnoli &
Bergstrom, 2005). However, the reverse direction does not hold.
D.2 Symmetric margin distributions
In this section we prove Proposition 1. We first prove the following helpful lemma:
Lemma 2 (Conditions for monotonicity of selective accuracy). AF (τ) is monotone increasing in τ
if and only if
f(-τ)、	f(T)
--:----≥ ----:——
F(-τ) — 1 - F(T)
(49)
for all T ≥ 0. Conversely, AF (T) is monotone decreasing in T if and only if the above inequality is
flipped for all T ≥ 0.
Proof. AF (τ) is monotone increasing in T if and only if dAF ≥ 0 for all T ≥ 0. We obtain dAAF by
differentiating AF and simplifying:
dAF 	= dT	d (	1 - F(T)	ʌ	(50) dτ V - F(t) + F(-T))	() [1 - F(τ) + F(-t)][-f (τ)] - [1 - F(τ)][-f (τ) - f (-T)] (1 - F(τ) + F(-τ))2	( ) -f(τ) + f(τ)F(T) - f(τ)F(-T) + f(τ) + f (-T) - f(τ)F(T) - f(-τ)F(T) (1- F(T) + F(-τ))2 f(-T) - f (T)F(-T) - f (-T)F(T)	(52) (1 - F(τ) + F(-τ))2	( ) f(-T)[1 - F(T)] - f (T)F(-T)	(53) (1 - F (τ)+ F (-τ ))2	.	( )
Since the denominator is always positive, we have that ddAF ≥ 0 if and only if the numerator
f (-T)[1 - F(T)] - f(T)F(-T) ≥ 0, which in turn is equivalent to
	f(-T)	f(T) -T——L ≥ _τ±2_	(54) F(-τ) ≥ 1 - F(τ),	()
as desired. The case for monotone decreasing AF(τ) is analogous.	口
In the next two lemmas, we prove the necessary and sufficient conditions for Proposition 1 respec-
tively.
22
Published as a conference paper at ICLR 2021
Lemma 3 (Left-log-concavity and symmetry imply monotonicity.). Let f be symmetric about μ
and let F be left-log-concave. If AF (0) ≥ 0.5, then AF (τ) is monotone increasing. Conversely, if
AF (0) ≤ 0.5, then AF (τ) is monotone decreasing.
Proof. Consider the case where AF (0) ≥ 0.5; the case where AF (0) ≤ 0.5 is analogous. From
Lemma 2 and the symmetry of f, we have that AF(τ) is monotone increasing if (and only if)
f(-τ) ≥ f (T) = f (2μ - T)	(55)
F(-τ) ≥ 1 - F(τ) = F(2μ - τ)	( )
holds for all T ≥ 0.
To show that this inequality holds for all T ≥ 0, we first note that since AF (0) ≥ 0.5, we have that
F(0) ≤ 0.5, which together with the symmetry of F implies that μ ≥ 0. Thus, -T ≤ 2μ - T for all
T ≥ 0.
Now, if τ ≥ μ, then 2μ - T ≤ μ, so the desired inequality (55) follows from the log-concavity of F
on (-∞, μ] (Remark 1 of Bagnoli & Bergstrom (2005).)
If instead 0 ≤ T ≤ μ,we have
f(-τ) > f(τ) ≥ F (-T) — F (T)	[by log-concavity of F on (-∞, μ]]	(56)
≥	f(τ) —1 - F(τ)	[since F(T) ≤ 0.5]	(57)
and thus (55) also holds.		□
Lemma 4 (Monotonicity and symmetry imply left-log-concavity.). Let f be symmetric with mean
0, and let fμ be a translated version of f that has mean μ. If Afμ is monotone increasing for all
μ ≥ 0, then f is left-log-concave.
Proof. First consider μ ≥ 0. Since Afμ is monotone increasing for all μ ≥ 0, from Lemma 2 and
the symmetry of fμ,
fμ(-T) ≥	fμ(T)	= fμ(2μ -T)
Fμ(-T) — 1 - Fμ (t )	Fμ(2μ - T)
must hold for all T ≥ 0. Since fμ(χ) = f (X - μ) by construction, we can equivalently write
f(一μ - T) ≥ f(μ - T)
F(-μ - T) — F(μ - T)，
which holds for all T ≥ 0 as well, and by assumption, for all μ ≥ 0 as well. By letting a
and b = μ - T, we can equivalently write the above inequality as
f(a) > f(b)
≥
F(a) ≥ F(b)
for all a, b ∈ R where a ≤ b, a + b ≤ 0.
By the definition of log-concavity, this means that F is log-concave on (-∞, 0].
(58)
(59)
(60)
-μ — T
□
D.2. 1 Left-log-concavity and monotonicity.
Proposition 1 (Left-log-concavity and monotonicity). Let F be the CDF of a symmetric distri-
bution. If F is left-log-concave, then AF (T) is monotonically increasing in T if AF (0) ≥ 1/2
and monotonically decreasing otherwise. Conversely, if AFd (T) is monotonically increasing for all
translations Fd such that Fd(T) = F(T -d)forall T and AFd (0) ≥ 1/2, then F is left-log-concave.
Proof. If F is left-log-concave, by Lemma 3 AF (t) is monotonically increasing in T if Af* (0) ≥ 2
and monotonically decreasing otherwise. Conversely, if AF is monotonically increasing for all
translations Fd such that Fd(T) = F(T - d) for all T by Lemma 4, F is log-concave, completing
the proof.	□
23
Published as a conference paper at ICLR 2021
D.3	Skew-symmetric margin distributions
We now turn to how skew affects selective classification. Recall in Section 5 we defined skew-
symmetric distributions as follows:
Definition 2. A distribution with density fα,μ is Skew-Symmetric with skew a and center μ if
fα,μ(τ) = 2h(T - μ)G(α(τ - μ))	⑷
for all τ ∈ R, where h is the density of a distribution is symmetric about 0, and G is the CDF of a
potentially different distribution that is also symmetric about 0.
When α = 0, f = h and there is no skew. Increasing α increases rightward skew, and decreasing
α increases leftward skew. Note that in general, the mean of f depends on α as well. We will also
consider the translated distribution:
fα,μ(x) = fα(x - μ)	(61)
The CDF of fa,μ is Fα,μ and the CDF of fa is Fa. We will use the following properties of these
distributions:
Lemma 5 (Skew-symmetry about μ). These properties hold when we flip the skew from a to -a:
fα,μ(X)= f-α,μ(2μ - X)	(62)
Fa,μ (X) = 1 - F-α,μ (2μ - X)	(63)
Proof.
fa,μ(x) 二	二 2h(x — μ)G(α(x — μ))	(64)
	二 2h(μ — x)[G((-α)(μ — x))]	(65)
	二 f-a,μ(2μ - x).	(66)
Fa,μ(x) 二	=J fa,μ(t)dt	(67)
	=X f-a,μ(2c - t)dt	(68)
	∞ f f	f-a,μ(t)dt	(69)
	2c-x 1 1 - F-a,μ(2c - x).	(70)
		□
Lemma 6 (Stochastic ordering with α [Proposition 4 from Azzalini & Regoli (2012)]). Let α1 ≤ α2.
Then,forany μ ∈ R, Faι,μ ≥ Fa2,μ∙
Proof. Since the ordering is invariant to translation, without loss of generality We can take μ = 0.
First consider X ≤ 0. Then
Fα1 (X) =	2h(t)G(α1t)dt
≥	2h(t)G(α2t)dt
Fα2 (X).
(71)
(72)
(73)
24
Published as a conference paper at ICLR 2021
Now consider x ≥ 0. We have			
Fα1 (x)	2h(t)G(α1t)dt		(74)
	x 2h(t)(1 - G(-α1t))dt	[by symmetry of G]	(75)
	2H(x) -	x 2h(t)G(-α1t)dt		(76)
	2H (x) -	2h(-t)G(-α1 t)dt	[by symmetry ofh]	(77)
	2H(x) -	∞ 2h(t)G(α1t)dt -x	[change of variables]	(78)
	2H (x) - 1 + Fα1 (-x),		(79)
which reduces to the case where x ≤ 0.			□
Lemma 7 (Log gradient ordering by skew). For all a ≥ 0, T ≥ 0, and μ ∈ R,
fα,μ(T) ≥ f0,μ(T) ≥ f-α,μ(T)
Fα,μ (T) — F0,μ(τ) — F-α,μ(T)
(80)
Proof. Since the ordering is invariant to translation, without loss of generality We can take μ = 0.
We have:
fα(T )	h(T )G(aT)	(81)
Fa (t)	R-∞ h(t)G(0t)dt	
	=	h(T)	(82)
	R- ∞ h(t) 制 dt	
	h	h(T)	- fo(T) ≥ -R-∞ h(t)dt	f0(T)	(83)
	≥	h(T) -RT	h(t) G(-at) dt	(84)
	J-∞ h(t) G(-aτ)dt	
	h(T )G(-α,T)	(85)
	R-∞ h(t)G(-αt)dt	
	=f-a(T) =F-a(T) .	(86)
To get the inequalities, note that when ɑ ≥ 0 and t ≤ T, we have a ≥ 0, G(at)/G(αT) ≤ 1
since G is an increasing function. Similarly, G(-at)∕G(-ɑT) ≥ 1. Since h is non-negative, the
inequalities then follow from the monotonicity of the integral.	□
We now prove the main results of this section.
D.3.1 Accuracy is monotone with skew
Proposition 6 (Accuracy is monotone with skew). Let Fa,μ be the CDF of a skew-symmetric dis-
tribution. For all T ≥ 0 and μ ∈ R, A%* (T) is monotonically increasing in a.
Proof. We use the skew-symmetry of fa,μ to write the selective accuracy as
1
Afa,μ (T )
1	Fa,μ(-T)
1 + 1-Fa,μ(τ)
1
(87)
1 I	Fa,μ(-T)
1 + Fα,μ(2μ-τ)
(88)
25
Published as a conference paper at ICLR 2021
We see that Afα,μ (T) is a monotone decreasing function of FFα,μ(-)T). From Lemma 6, the nu-
merator decreases with increasing α while the denominator increases. Thus, this fraction decreases,
Which in turn implies that Afα,μ (T) is monotone increasing with α as desired.	□
D.3.2 Skew in the same direction preserves monotonicity
Proposition 2 (Skew in the same direction preserves monotonicity). Let Fa,μ be the CDF ofa skew-
symmetric distribution. Ifaccuracy of its symmetric version, Af0* (T), is monotonically increasing
in τ, then Apa* (T) is also monotonically increasing in T for any α > 0. Similarly, if Af0* (T) is
monotonically decreasing in T, then Apα,μ (T) is also monotonically decreasing in T for any α < 0.
Proof. The idea is to use Lemma 7 to reduce the statement to the case where α = 0, so that we can
apply monotonicity. First consider the case where Af0* (t) is monotonically increasing in T, and
α > 0. We have
fα,μ(-τ ) Fα,μ(-τ )	≥ f0,μ(一τ) 一F0,μ(-τ)	[Lemma 7]	(89)
	=hμ(一τ) Hμ(-τ )	[definition of f]	(90)
	≥	hμ(T ) —1 — Hμ(τ)	[from monotonicity of A (Lemma 2)]	(91)
	_ h"(2μ - T) =Hμ(2μ - T)	[symmetry of h]	(92)
	=f0,μ(2μ - T) =Fθ,μ(2μ - T)	[definition of f]	(93)
	≥ f-α,μ(2μ - T) F F-α,μ(2μ - T)	[Lemma 7]	(94)
	≥	f a,μ (T) -1 - Fα,μ(τ).	[Lemma 5]	(95)
Applying Lemma 2 completes the proof. The case where a < 0 is analogous.	□
D.4 Monotone odd transformations preserve accuracy-coverage curves.
We now show that our results are unchanged by strictly monotone and odd transformations to the
density.
Lemma 8 (Odd and strictly monotonically increasing transformations preserve selective accuracy).
Let X be a real-valued random variable with CDF FX, T be a strictly monotonically increasing
function, and define random variable Y = T(X) with CDF FY . Then, AFX (T) = AFY (T(T)) for
each T.
Proof. For each T, since T is a strictly monotonically increasing function, we apply the change of
variables formula for transformations of univariate random variables to get the following:
FX(T) = FY (T(T))
FX(-T) = FY (T (-T)) = FY (-T (T)).
We now solve for selective accuracy:
A ( ) =	1 - FX (T)
FX ( )= 1 — FX (t )+ FX (-t )
=	1- FY(T(T))
1 — Fy (T (τ)) + FY (-T (τ))
= AFY (T (T)).
(96)
[T is odd]	(97)
(98)
(99)
(100)
□
26
Published as a conference paper at ICLR 2021
With Lemma 8, our results extend all random variables T (X) where X corresponds to a left-log-
concave distribution and T is odd and strictly monotonically increasing. In particular, X and T (X)
have the same accuracy-coverage curves.
E Proofs: Comparison to the group-agnostic reference
In this section, we present the proofs from Section 6, which outline conditions under which selective
classifiers outperform the group-agnostic reference.
E.1 Definitions
We first define certain metrics on selective classifiers and their group-agnostic reference in terms of
their margin distributions.
Definition 6. Consider a margin distribution with CDF
F =	pg Fg ,
g∈G
(101)
where pg is the mixture weight and Fg is the CDF for group g. We write the fraction of examples
that are correct/incorrect and predicted on at threshold τ from group g and on average as follows:
Cg(T) Ig(T) C(T)	1-Fg(T)	(102) Fg(-T)	(103) XPgCg(T)	(104) g
I(T)	XPgIg(T)	(105) g
We write the true-positive rate RTP(τ) and false-positive rate RFP(τ) for a given threshold τ as
	RTP (T) = τXJ,	(106) C(0) RFP (t ) = IT.	(107)
Finally, we write the accuracy of the group-agnostic reference on group g as
〜 AFg (T) =	AFg (0)RTP(T)	(108) -AFg(0)RTP(T) + (1 - AFg (0))RFP(T)	(	) Cg (0)C(T )∕C(0)	(109) -Cg (0)C(T )∕C(0) + Ig (0)I (t )∕I (0)	(	)
For convenience in our proofs below, we also define the fraction of each group g in correctly and
incorrectly classified predictions at each threshold τ .
Definition 7. We define the fraction of group g out of correctly and incorrectly classified predictions
at threshold τ, CFg(τ) and IFg(τ) respectively, as
CFg(τ)
Pg Cg(T)
C(T)
IFg(τ)
PgIg(T)
I(T)
(110)
(111)
E.2 General necessary conditions to outperform the group-agnostic
REFERENCE
We now present the proofs for Proposition 3 and Corollary 1, starting with the supporting lemmas.
We first write the accuracy of the group-agnostic reference in terms of IFwg(0) and CFwg(0).
27
Published as a conference paper at ICLR 2021
Lemma 9.	1 AFwg(T) = I I IFwg(0) ~W).	(112) 1 + CFwg (0) ∙ CM
Proof. We have:	九(T) =		A%(O)RTP(T)__	(113) Fwg( )	AFwg(O)RTP(T) + (1-AFwg(O))RFP(T)	(	) =	PCwg(O)RTP(T) PCwg(O)RTP(T)+ PIwg(O)RFP(T)	(	) =	PCwg(O)舞…	(115) PCwg(O) C(O) + PIwg(O)品 =	CFwg(O)C(T) CFwg (O)C (t )+ IFwg (O)I (t )	(	) =1 , IFwg(O) ~W) .	(117) 1 + CFwg(O) ∙ C(T)
□
Lemma 10 (Bounding the derivative of 1/CFwg (τ) at τ = 0). If AFothers (0) ≥ AFwg (0),
d (	1 M 〉( 1 - P) ( Fwg(O) A (fwg(O)Fothers(O)- fothers(O)Fwg(O))
dT ICFwg(T)儿=o ≥ I ~) I- Fwg(O)) I	Fwg(O)2	)
(118)
Proof.
d
dτ CFwg(τ)
(119)
τ=0
d (PCWg(T) + (1 - P)Cothers(τ)
dT
d
dτ (
1-P
P
1-P
P
1-P
1+
PCwg(T)
1 - P 1 - Fothers (T)
1 - Fwg (τ )
d ( 1 - FOthers(T)
dτ V 1 — FWg (τ)
1
τ=0
τ=0
τ=0
(1 - Fothers (τ)) - fothers (τ)(1 - Fwg (τ))
P 1 - Fwg(T)
1 - p 1
丁 1 - FWg(O)
(1 -Fwg(τ))2
(fwg(T) (⅛TΓ
(fwg(O) (⅛⅛fr
- fothers
τ=0
- fothers
τ=0
(120)
(121)
(122)
(123)
(124)
(125)
1
p
≥ T1-Fwg(O) (fwg(O) ¾OF- fothers(O))	[O < Fothers(O) ≤FWg(O) < 1]
(126)
1 - P 1	fwg(O)Fothers(O)- fOthers(O)Fwg(O)
P 1 - Fwg(O)	Fwg(O)
1 - P	Fwg(O)	fwg(O)Fothers(O)- fOthers(O)Fwg(O)
P 1 - Fwg(O)	Fwg(O)2
(127)
(128)
□
28
Published as a conference paper at ICLR 2021
Lemma 11. If AFothers (O) ≥ AFWg (O) > O.5, then
d IFWg(T)
dT CFWg(T)
for some positive constant C.
≥ C(fothers(O)FWg(O) - fWg(O)Fothers(O))
τ=0
(129)
Proof.
d IFWg(T)
dT CFWg(T)
=(dτ IFWg(T)
≥( dIFWg(T)
(130)
τ=0
τ=0
CFwg(O)
1
+ I FWg(O)(而 CFWg(T )
(131)
τ=0
(132)
(O)Fothers(O)- fothers(O)Fwg(O)
+ IFwg (0)
(133)
Fwg(O)2
τ=0 CFWg(O)
1-P A ( Fwg(O)
P J υ - FWg(O)
|
>0
}	>0
{z
>0
(
1
∖
1+
1—p I-Fothers(0)
1+
F I-Fwg(0)
1—P Fothers(0)
≥1 because A
P Fwg(O)
{z∕1^^^^^^^
Fothers (0)≥AFwg (0)
Fwg(O)
I- FWg (0)
'-------{-------}
<1 because AFwg (0)>0.5
= C(fothers(O)Fwg(O) - fwg(O)Fothers(O))
where (133) follows from Lemma 10.
(137)
□
E.2. 1 Necessary condition for outperforming the group-agnostic reference
Proposition 3 (Necessary condition for outperforming the group-agnostic reference). Assume that
1/2 < AFWg (O) < AFothers (O) < 1 and the worst-group density fWg(O) > O. If AFWg (T) ≤ AFWg (T)
for all T ≥ O, then
fothers(O) ≤ 1 - AFOthers (O)
fwg(O)	—	1 - AFWg(O)
(5)
Proof. Recall that
AFwg(T) = 1 I IFWg(T) ~W)
1 + CFwg (τ) ∙ C(T)
彳-________________1________
AFwg(T) = 1 + IFwg(O)	I(T).
1 + CFWg(O) ∙ C(T)
(138)
(139)
*
}
	
29
Published as a conference paper at ICLR 2021
If AFwg(T) ≥ AFwg(T) for all T ≥ 0, then
d IFwg (τ)
dT CFwg(T)
≤ 0.
τ=0
(140)
From Lemma 11,
C (fothers(0)Fwg (0) - fwg (O)Fothers(O)) ≤ 二;Fwg (T )	(⑷)
dT CFwg(T)
τ=0
for some positive constant C. Combined,
fothers (O)Fwg (O) - fwg (O)Fothers (O) ≤ O.
(142)
□
E.2.2 Outperforming the group-agnostic reference requires smaller scaling
for log-concave distributions
Corollary 1 (Outperforming the group-agnostic reference requires smaller scaling for log-concave
distributions). Assume that 1/2 < AFwg (O) < AFothers (O) < 1, Fwg is log-concave, and fothers (T) =
Vfwg (V(T — μothers) + μwg) Jor all T ∈ R, where V is a scaling factor If AFWg(T) ≤ AFwg(T) forall
T ≥ O, v < 1.
Proof. By Proposition 3, if AFWg(T) ≥ AFWg(T) for all T ≥ 0, then:
fothers (O)Fwg (O) — fwg (O)Fothers (O) ≤ O,
and equivalently,
fothers(O)	fwg(O)
-------；~- ≤ ——；~-
Fothers(O) — Fwg(O)
From the definition of fothers ,
V≤
fwg(O)/Fwg(O)
fwg (-μothersv + μwg ) /Fwg (-μothers V + μwg )
Because AFothers(0) > AFwg(0), —MothersV + μwg < 0. Applying log-concavity yields
V < 1.
Thus, when V > 1, there exists some threshold T ≥ 0 Where AFwg(T ) > AFwg(T).
(143)
(144)
(145)
(146)
□
E.3 Translated, log-concave distributions always underperform the
group-agnostic reference
We noW present a proof of Proposition 4. Assume fwg and fothers are log concave and symmetric,
the worst group has mean μwg, the combination of other group(s) has mean μ°thers, and fothers (x)=
fwg(χ — (μothers — μwg)), (the densities are translations of each other.) For convenience, define
d = Mothers — μwg. This implies:
FotherS(X) = Fwg(X — (μothers — μwg))	(147)
= Fwg(x —d).	(148)
We will first show that wg is indeed the worst group at full coverage; i.e. AFwg (0) < AFothers (0):
Lemma 12. If fwg is a PDF and fothers is obtained by translating fwg to the right (i.e. fothers(X) =
fwg(X — d) for d > 0), and Fwg and Fothers are their associated CDFs, AFwg (0) < AFothers (0).
Proof. We have:
AFwg (0) = 1 — Fwg(0)
≤ 1 — Fwg (—d)
= 1 — Fothers(0)
= AFothers (0).
(149)
(150)
(151)
(152)
□
30
Published as a conference paper at ICLR 2021
We next show that CFwg (τ) is monotonically decreasing in τ :
Lemma 13. If fwg, Fwg, fothers, Fothers, p, and CFwg are as described, CFwg is monotonically
decreasing in τ.
Proof. First, defining CFwg in terms of Fwg, we have:
CFwg(τ)
P(I - Fwg(TD
p(1 - Fwg (τ)) + (1 - p)(1 - Fothers (τ))
p(1- Fwg(T))
(153)
P(1 — Fwg (T)) + (1 — P)(1 — Fwg (T — d))
_	PFwg (2μwg — T )
PFwg (2μwg - T) + (1 - P)Fwg (2μwg - T + d)
_	1
1 + (I-P)FWg (2*wg-τ + d)
pFwg(2μwg-T )
Taking the derivative of CFwg, we have:
(154)
(155)
(156)
上CF _(t) = q ___________________1____________
dT CFwg(T)	dT I 1+ (1-p)Fwg(2μwg-τ+d)
∖	pFwg(2μwg-τ )
	
(157)
___________1___________ʌ (1 — P
1 _i_ (I-P)Fwg(2μwg-τ+d) Jl p
十	pFwg(2μwg-τ)	)
d FFwg(2μwg — τ + d)
dT y FWg (2μwg — T)
(158)
-C ( -fwg (2μwg - T + d)Fwg (2μwg - T) + fwg(2μwg - T )Fwg (2μwg - T + d)),
(159)
where c is a positive constant (since P and 1 — P are positive and all of the terms are squares.) Thus,
we can say that CFwg (T) is monotonically decreasing for all T if:
fwg (2μ
wg
-T + d) Fwg (2μwg - T)
≤ fwg(2μwg
T )Fwg(2μwg
— T + d)
(160)
fwg(2μwg - T + d) ≤ fwg(2μwg - T)
Fwg(2μwg - T + d)	Fwg(2μwg - T)
(161)
Now, since d > 0, this follows from the log concavity of fwg . Therefore, the fraction of correct
examples that Come from group 1 is a decreasing function of T.	□
Next, we show that the IFwg(T) is monotonically increasing in T:
Lemma 14. If fwg, Fwg, fothers, Fothers, P and IFwg are as described, IFwg is monotonically in-
creasing in T.
Proof. First, we note that:
IFW KT) =-----------PFWg(一τ)-------------
wg( )	PFwg(-T) + (1 — P)Fwg(-T — d)
__	1
1 I (I-P)FWg( — 丁-d) .
十	PFwg(-T)
(162)
(163)
We will show that the derivative of IFwg (T) with respect to T is positive. Doing so gives us:
aFwg(T )
d	1
dT 1 I (I-P)Fwg(一τ-d)
十	PFwg(-τ)
_1_ Y
1 I (I-P)Fwg(-T-d) J
十	PFwg(-τ)	/
1 — P ʌ d F Fwg (—T — d)
P JdTk Fwg(—T)
—c (—fwg(—T — d)Fwg (—T) + fwg (—T)Fwg (—T — d)) ,
(164)
(165)
(166)
31
Published as a conference paper at ICLR 2021
where c is positive since it is the product of squared terms and (1 - p)/p, which is also positive.
Thus, ddT IFwg (T) ≥ 0 is equivalent to:
	
(-fwg (-τ - d)Fwg (-τ) + fwg (-τ)Fwg (-τ - d)) ≥ 0	(167)
fwg (-τ - d)Fwg (-τ) ≥ fwg (-τ)Fwg (-τ - d)	(168)
fwg(-T - d ≥ fwg(-T)
Fwg(一τ - d	Fwg(-T)
(169)
Since fwg is log-concave and dis positive, the last inequality is true, so IFwg (T) is monotonically
increasing in T, as desired.	□
Lastly, We ShoW that Lemma 13 and Lemma 14 imply that the ratio CFwg (T)) is monotonically in-
creasing in T .
Lemma 15. CFwg(T)) is monotonically increasing in T
Proof. We simply folloW the quotient rule:
dd CFwg(TI = CFwg(T)IF0(t) - IFwg(T)CF1 (T)	(170)
dT CFwg (T )
≥ 0,	(171)
Where We note that CFwg (T), IFwg(T) ≥ 0, CF10(T) < 0 from Lemma 13, and IF10(T) > 0 from
Lemma 14.	□
We Will noW prove the main result of this section:
E.3.1 Translated, log-concave distributions always underperform the
group-agnostic reference
Proposition 4 (Translated log-concave distributions underperform the group-agnostic reference).
Assume Fwg and Fothers are log-concave and fothers (T) = fwg (T - d) for all T ∈ R. Then for all
T ≥ 0,
AFwg (T) ≤ AFwg(T).	⑹
Proof. We consider the case of underperforming the group-agnostic reference: the case of overper-
forming the group-agnostic reference is analogous. Assume the Worst group is the loWer accuracy
group (so dis positive.) Using the definition of selective accuracy, We have:
AF (t)= —Cwgg—
Fwg ()	Cwg(T ) + Iι (T)
=	CFwg(T )C(τ)
CFwg (τ )C (τ) + IFwg (τ )I (τ)
_	1
=I I IFwg(T) * I(T)
1 + CFwg(T) * CH
,	1
≤ 1 I IFwg(O) *~i(TT
1 + CFwg (0) * C(τ)
=	CFwg(0)C(τ)
=CFwg(0)C(τ)+ IFwg(O)I (T)
=AFwg (T ).
[by Lemma 15]
Thus, the Worst group underperforms the group-agnostic reference, as desired.
(172)
(173)
(174)
(175)
(176)
(177)
□
32
Published as a conference paper at ICLR 2021
F	Simulations
To demonstrate that it is possible but challenging to outperform the group-agnostic reference, we
present simulation results on margin distributions that are mixtures of two Gaussians. Following
the setup from Section 6, we consider a best-group margin distribution N (1, 1) and a worst-group
margin distribution N(μ, σ2) varying the parameters μ, σ. We set the fraction of mass from the
worst group, p to be 0.5. We evaluate whether selective classification outperforms the group-agnostic
reference, plotting the results in Figure 9. We observe that selective classification outperforms the
group-agnostic reference under some parameter settings, notably when the necessary condition in
Proposition 3 is met and when the variance is smaller as implied by Corollary 1, but it underperforms
the group-agnostic reference under most parameter settings.
1.50
0.00
O 0.75
⅛ 0.50
o
M 0.25
U 1.25
tŋ
(υ
E 1.00
Worst-group standard deviation
Outperforms the group-agnostic baseline
Underperforms the group-agnostic baseline
Satisfies the necessary condition
Best-group parameter
Figure 9: Simulation results on margin distributions that are mixtures of two Gaussians, where
We vary the mean and the variance of the worst-group margin distributions. For parameters corre-
sponding to the blue and orange regions, the worst group outperforms and underperforms the group-
agnostic reference, respectively. We do not consider parameters in the white region to maintain that
they yield full-coverage accuracies that are worse than the other group. We shade parameters that
satisfy the necessary condition in Proposition 3.
In addition to whether the worst group underperforms or outperforms the group-agnostic reference,
we study the magnitude of the difference in the worst-group accuracy with respect to the group-
agnostic reference through the same simulations (Figure 10). Concretely, we compute the difference
in worst-group accuracy with respect to the group-agnostic reference, taking the worst-case differ-
ence across thresholds: max「AFwg(T) - AFwg(T). We observe significant disparities between the
observed worst-group accuracy and the group-agnostic reference.
Difference in worst-group accuracy from the group-agnostic baseline
Worst-group standard deviation
Figure 10: Simulation results on margin distributions that are mixtures of two Gaussians, where we
vary the mean and the variance of the worst-group margin distribution, and keep the other group's
margin distribution fixed with mean and variance 1. We compute the difference in worst-group accu-
racy with respect to the group-agnostic reference, taking the worst-case difference across thresholds:
maxτ AFWg(T) - AFWg(T).
Lastly, we simulate the effects of varying the worst-group and other-group means while keeping
the variance fixed at σ2 = 1 for both groups, following the same simulation protocol otherwise.
33
Published as a conference paper at ICLR 2021
We similarly observe substantial gaps between the observed worst-group accuracy and the group-
agnostic reference (Figure 11).
Difference in worst-group accuracy
from the group-agnostic baseline
Other-group mean
Figure 11: Simulation results on margin distributions that are mixtures of two Gaussians, where we
vary the means of the two margin distributions, keeping their variances fixed at 1. We compute the
difference in worst-group accuracy with respect to the group-agnostic reference, taking the worst-
case difference across thresholds: max「AFwg(T) - AFwg(T).
VAFLT) IAFLn
34