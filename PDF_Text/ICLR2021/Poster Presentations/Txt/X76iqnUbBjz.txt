Published as a conference paper at ICLR 2021
A Unified Approach to Interpreting and Boost-
ing Adversarial Transferability
Xin Wanga*, Jie Ren”Shuyun Lina, Xiangming Zhua, Yisen Wangb, Quanshi Zhangat
a Shanghai Jiao Tong University
bKey Lab. of Machine Perception (MoE), School of EECS, Peking University, Beijing, China
Ab stract
In this paper, we use the interaction inside adversarial perturbations to explain
and boost the adversarial transferability. We discover and prove the negative cor-
relation between the adversarial transferability and the interaction inside adver-
sarial perturbations. The negative correlation is further verified through different
DNNs with various inputs. Moreover, this negative correlation can be regarded as
a unified perspective to understand current transferability-boosting methods. To
this end, we prove that some classic methods of enhancing the transferability es-
sentially decease interactions inside adversarial perturbations. Based on this, we
propose to directly penalize interactions during the attacking process, which sig-
nificantly improves the adversarial transferability. Our code is available online* 1.
1	Introduction
Adversarial examples of deep neural networks (DNNs) have attracted increasing attention in re-
cent years (Ma et al., 2018; Madry et al., 2018; Wang et al., 2019; Ilyas et al., 2019; Duan et al.,
2020; Wu et al., 2020b; Ma et al., 2021). Goodfellow et al. (2014) found the transferability of ad-
versarial perturbations, and used perturbations generated on a source DNN to attack other target
DNNs. Although many methods have been proposed to enhance the transferability of adversarial
perturbations (Dong et al., 2018; Wu et al., 2018; 2020a), the essence of the improvement of the
transferability is still unclear.
This paper considers the interaction inside adversarial perturbations as a new perspective to interpret
adversarial transferability. Interactions inside adversarial perturbations are defined using the Shapley
interaction index proposed in game theory (Michel & Marc, 1999; Shapley, 1953). Given an input
sample x ∈ Rn , the adversarial attack aims to fool the DNN by adding an imperceptible perturbation
δ ∈ Rn on x. Each unit in the perturbation map is termed a perturbation unit. Let φi denote the
importance of the i-th perturbation unit δi to attacking. φi is implemented as the Shapley value,
which will be explained later. The interaction between perturbation units δi , δj is defined as the
change of the i-th unit’s importance φi when the j-th unit is perturbed w.r.t the case when the j-th
unit is not perturbed. If the perturbation δj on the j -th unit increases the importance φi of the i-
th unit, then there is a positive interaction between δi and δj . If the perturbation δj decreases the
importance φi , it indicates a negative interaction.
In this paper, we discover and partially prove a clear negative correlation between the transferability
and the interaction between adversarial perturbation units, i.e. adversarial perturbations with lower
transferability tend to exhibit larger interactions between perturbation units. We verify such a cor-
relation based on both the theoretical proof and comparative studies. Furthermore, based on the
correlation, we propose to penalize interactions during attacking to improve the transferability.
* Equal contribution
,Correspondence. This study is conducted under the supervision of Dr. Quanshi Zhang.
zqs1022@sjtu.edu.cn. Quanshi Zhang is with the John Hopcroft Center and the MoE Key Lab of Arti-
ficial Intelligence, AI Institute, at the Shanghai Jiao Tong University, China.
1https://github.com/xherdan76/A-Unified-Approach-to-Interpreting-and-
Boosting-Adversarial-Transferability
1
Published as a conference paper at ICLR 2021
In fact, our research group led by Dr. Quanshi Zhang has proposed game-theoretic interactions,
including interactions of different orders (Zhang et al., 2020) and multivariate interactions (Zhang
et al., 2021c). As a basic metric, the interaction can be used to explain signal processing in trained
DNNs from different perspectives. For example, we have build up a tree structure to explain the
hierarchical interactions between words in NLP models (Zhang et al., 2021a). We have also used
interactions to explain the generalization power of DNNs (Zhang et al., 2021b). The interaction
can also explain the utility of adversarial training (Ren et al., 2021). As an extension of the system
of game-theoretic interactions, in this study, we explain the adversarial transferability based on
interactions.
In this paper, the background for us to investigate the correlation between adversarial transferability
and the interaction is as follows. First, we prove that multi-step attacking usually generates pertur-
bations with larger interactions than single-step attacking. Second, according to (Xie et al., 2019),
multi-step attacking tends to generate more over-fitted adversarial perturbations with lower trans-
ferability than single-step attacking. We consider that the more dedicated interaction reflects more
over-fitting towards the source DNN, which hurts adversarial transferability. In this way, we propose
the hypothesis that the transferability and the interaction are negatively correlated.
•	Comparative studies are conducted to verify this negative correlation through different DNNs.
•	Unified explanation. Such a negative correlation provides a unified view to understand current
transferability-boosting methods. We theoretically prove that some classic transferability-boosting
methods (Dong et al., 2018; Wu et al., 2018; 2020a) essentially decrease interactions between per-
turbation units, which also verifies the hypothesis of the negative correlation.
•	Boosting adversarial transferability. Based on above findings, we propose a loss to decrease
interactions between perturbation units during attacking, namely the interaction loss, in order to
enhance the adversarial transferability. The effectiveness of the interaction loss further proves the
negative correlation between the adversarial transferability and the interaction inside adversarial
perturbations. Furthermore, we also try to only use the interaction loss to generate perturbations
without the loss for the classification task. We find that such perturbations still exhibit moderate
adversarial transferability for attacking. Such perturbations may decrease interactions encoded by
the DNN, thereby damaging the inference patterns of the input.
Our contributions are summarized as follows. (1) We reveal the negative correlation between the
transferability and the interaction inside adversarial perturbations. (2) We provide a unified view to
understand current transferability-boosting methods. (3) We propose a new loss to penalize interac-
tions inside adversarial perturbations and enhance the adversarial transferability.
2 Related work
Adversarial transferability. Attacking methods can be roughly divided into two categories, i.e.
white-box attacks (Szegedy et al., 2013; Goodfellow et al., 2014; Papernot et al., 2016; Carlini &
Wagner, 2017; Kurakin et al., 2017; Su et al., 2017; Madry et al., 2018) and black-box attacks (Liu
et al., 2016; Papernot et al., 2017; Chen et al., 2017a; Bhagoji et al., 2018; Ilyas et al., 2018; Bai et al.,
2020). A specific type of the black-box attack is based on the adversarial transferability (Dong et al.,
2018; Wu et al., 2018; Xie et al., 2019; Wu et al., 2020a), which transfers adversarial perturbations
on a surrogate/source DNN to a target DNN.
Thus, some previous studies focused on the transferability of adversarial attacking. Liu et al. (2016)
demonstrated that non-targeted attacks were easy to transfer, while the targeted attacks were difficult
to transfer. Wu et al. (2018) and Demontis et al. (2019) explored factors influencing the transferabil-
ity, such as network architectures, model capacity, and gradient alignment. Several methods have
been proposed to enhance the transferability of adversarial perturbations. The momentum iterative
attack (MI Attack) (Dong et al., 2018) incorporated the momentum of gradients to boost the trans-
ferability. The variance-reduced attack (VR Attack) (Wu et al., 2018) used the smoothed gradients
to craft perturbations with high transferability. The diversity input attack (DI Attack) (Xie et al.,
2019) applied the adversarial attacking to randomly transformed input images, which included ran-
dom resizing and padding with a certain probability. The skip gradient method (SGM Attack) (Wu
et al., 2020a) used the gradients of the skip connection to improve the transferability. Dong et al.
(2019) proposed the translation-invariant attack (TI Attack) to evade robustly trained DNNs. Li et al.
2
Published as a conference paper at ICLR 2021
(2020) used the dropout erosion and the skip connection erosion to improve the transferability. In
comparison, we explain the transferability based on game theory, and discover the negative correla-
tion between the transferability and interactions as a unified explanation for some above methods.
Interaction. The interaction between input variables has been widely investigated. Michel & Marc
(1999) proposed the Shapley interaction index based on the Shapley value (Shapley, 1953) in game
theory. Daria Sorokina (2008) defined the interaction of K input variables of additive models.
Scott Lundberg (2017) quantified interactions between each pair of input variables for tree-ensemble
models. Some studies mainly focused on interactions to analyze DNNs. Tsang et al. (2018) mea-
sured statistical interactions based on DNN weights. Murdoch et al. (2018) proposed to extract
interactions in LSTMs by disambiguating information of different gates, and Singh et al. (2019)
extended this method to CNNs. Jin et al. (2020) quantified the contextual independence of words
to hierarchically explain the LSTMs. Janizek et al. (2020) extended the method of Integrated Gra-
dients (Sundararajan et al., 2017) to quantify pairwise interactions of input features based on the
Hessian matrix, which required the DNN to use the SoftPlus operation replace the ReLU operation.
Chen et al. (2020) extended the attribution in (Chen & Ji, 2020) to use the Shapley interaction index
to generate hierarchical explanations of NLP tasks. In comparison, in this study, we use the Shapley
interaction index to explain and improve the transferability of adversarial perturbations.
3 The relationship between transferability and interactions
Preliminaries: the Shapley value. The Shapley value was first proposed in game theory (Shapley,
1953). Considering multiple players in a game, each player aims to win a high reward. The Shapley
value is considered as a unique and unbiased approach to fairly allocating the total reward gained by
all players to each player (Weber, 1988). The Shapley value satisfies four desirable properties, i.e.
the linearity, dummy, symmetry, and efficiency (please see the Appendix A.1 for details). Let Ω =
{1,2,...,n} denote the set of all players, and let v(∙) denote the reward function. V(S) represents
the reward obtained by a set of players S ⊆ Ω. The Shapley value φ(i∣Ω) unbiasedly measures the
contribution of the i-th player to the total reward gained by all players in Ω, as follows.
Ei 0(i|a)= V(C)-V⑼,
Φ(i∣Ω) = Xsec |S|!(n IS|-1)! (V(S∪{i}) -V(S)).⑴
s⊆Ω∖{i}	n!
Adversarial attack. Given an input sample x ∈ [0, 1]n with the true label y ∈ {1, 2, . . . , C}, we use
h(x) ∈ RC to denote the output of the DNN before the softmax layer. To simplify the story, in this
study, we mainly focus on the untargeted adversarial attack. The goal of the untargeted adversarial
attack is to add a human-imperceptible perturbation δ ∈ Rn on the sample x, and make the DNN
classify the perturbed sample x0 = x + δ into an incorrect category, i.e. arg maxy0 hy0 (x0) 6= y. The
objective of adversarial attacking is usually formulated as follows.
maximize '(h(x + δ),y) s.t. ∣∣δ∣∣p≤ e, x + δ ∈ [0,1]n,	设)
δ
where '(h(χ + δ), y) is referred to the classification loss, and E is a constant of the norm constraint.
Please see Appendix C for technical details of solving Equation (2).
3.1	Theoretical understanding of the adversarial attack in game theory.
In adversarial attacking, given the perturbation δ ∈ Rn, We use Ω = {1,2,..., n} to denote all
units/dimensions in the perturbation. We use the Shapley value in Equation (1) to measure the
contribution of each perturbation unit i ∈ Ω to the attack. To this end, it requires us to define the
utility of a subset of perturbation units S ⊆ Ω for attacking, which can be formulated as V(S)=
maxyo=y hyo(x + δ(S)) - hy(x + δ(S)), according to Equation (2). hy(∙) is the value of the y-th
element of h(∙) ∈ RC. δ(S) ∈ Rn is the perturbation which only contains perturbation units in S,
i.e. ∀i ∈ S, δ(S) = δi; ∀i ∈ S, δ(S) = 0. In this way, v(Ω) = maxyo=y hy (x + δ) — hy(x + δ)
denotes the utility of all perturbation units, and v(0) = max#，=# hy (x) - hy (x) denotes the baseline
score without perturbations. Thus, the overall contribution of perturbation units can be measured as
v(Ω) - v(0). We apply the Shapley value in Equation (1) to assign the overall contribution to each
perturbation unit as Pi φ(i∣Ω) = v(Ω) - v(0), where φ(i∣Ω) denotes the contribution of the i-th
perturbation unit.
3
Published as a conference paper at ICLR 2021
Interactions. Perturbation units do not contribute to the adversarial utility independently. For ex-
ample, perturbation units may form a certain pattern, e.g. an edge in the image. Thus, perturbations
units in the edge must appear together. The absence of a few units in the pattern may invalidate
this pattern. Let us consider two perturbation units i, j . According to (Michel & Marc, 1999), the
Shapley interaction index between units i, j is defined as the additional contribution as follows.
Iij(δ) = φ(Sij∣Ω0) - [φ(i∣Ω \ {j}) + φ(j∣Ω \ {i})],	(3)
where φ(i∣Ω \ {j}) and φ(j ∣Ω \ {i}) represent the individual contributions of units i and j, respec-
tively, when the perturbation units i, j work individually. Note that φ(i∣Ω \ {j}) is computed in the
scenario of considering the unit j always absent. Pi φ(i∣Ω \ {j}) = v(Ω \ {j}) - v(0), due to the ab-
sence of perturbation unit j. φ(Sj ∣Ω0) denotes thejoint contribution of i,j, when perturbation units
i, j are regarded as a singleton unit Sij = {i, j}. In this case, units i, j are supposed to be always
perturbed or not perturbed simultaneously, and we can consider that there are only n - 1 players in
the game. Thus, the set of all perturbation units is considered as Ω0 = Ω \ {i,j} ∪ Sij. The joint
contribution of Sij is denoted by φ(Sj∣Ω0), s.t. Pio∈Ωo∖{s-力 Φ(i0∣Ω0) + φ(Sj∣Ω0) = v(Ω0) - v(0).
The interaction defined in Equation (3) is equivalent to the change of the i-th unit’s importance φi
when the unit j exists w.r.t the case when the unit j is absent. Please see Appendix D for details.
If Iij (δ) > 0, it means δi and δj cooperate with each other, i.e. the interaction is positive; if
Iij(δ) < 0, it means δi and δj conflict with each other, i.e. the interaction is negative. The absolute
value of |Iij (δ) | indicates the interaction strength. The interaction is symmetric that Iij (δ) = Iji (δ).
We are given an input sample X ∈ Rn and a DNN h(∙) trained for classification. With the definition
of interactions, in adversarial attacking, we have the following propositions:
Proposition 1. (Proof in Appendix E) The adversarial perturbation generated by the multi-step
attack via gradient descent is given as δ^加=α Ptm-I Vχ'(h(x + δ1nιuiti), y), where δttnUlti denotes
the perturbation after the t-th step of updating, andm is referred to as the total number of steps. The
adversarial perturbation generated by the single-step attack is g^ven as δSingle = αmVχ'(h(x), y).
Then, the expectation of interactions between perturbation units in δnmulti, Ea,b[Iab(δnmulti)], is larger
than Ea,b [Iab(δsingle)], i.e. Ea,b [Iab(δnmulti)] ≥ Ea,b [Iab(δsingle)].
Note that when we compare interactions inside different perturbations, magnitudes of these pertur-
bations should be similar. It is because the comparison of interactions between adversarial perturba-
tions of different magnitudes is not fair. Therefore, we use the step size αm in the single-step attack
to roughly (not accurately) balance the magnitude of perturbations. The fairness is further discussed
in Appendix E.1.
Proposition 1 shows that, in general, adversarial perturbations generated by the multi-step attack
tend to exhibit larger interactions than those generated by the single-step attack. In addition, Ap-
pendix E.4 shows that the multi-step attack usually generates perturbations with larger interactions
than noisy perturbations of the same magnitude. Besides, Xie et al. (2019) demonstrated that the
multi-step attack tends to over-fit the source DNN, which led to low transferability. Intuitively, large
interactions mean a strong cooperative relationship between perturbation units, which indicates the
significant over-fitting towards adversarial perturbations oriented to the source DNN. In this way,
we propose the hypothesis that the adversarial transferability and the interactions inside adversar-
ial perturbations are negatively correlated.
3.2	Empirical verification of the negative correlation
To verify the negative correlation between the transferability and interactions, we conduct experi-
ments to examine whether adversarial perturbations with low transferability tend to exhibit larger in-
teractions than those perturbations with high transferability. Given a source DNN and an input sam-
ple x, we generate the adversarial example x0 = x + δ. Then, given a target DNN h(t), we measure
the transfer utility ofδ as Transfer Utility = [maxy0 6=y h(yt0)(x+δ)-h(yt)(x+δ)]-[maxy06=y h(yt0) (x)-
h(yt)(x)] as mentioned in Section 3.1. The interaction is given as Interaction = Ei,j [Iij(δ)], which
is computed on the source DNN. Note that the computational cost of Iij (δ) is NP-hard. However,
we prove that we can simplify the computation of the average interaction over all pairs of units as
4
Published as a conference paper at ICLR 2021
L 6u=θω
RN34-DN121 Corr:-069	RN34-DN201 Corr: -0.87 DN121-RN34 Corr: -0.87
DN201-RN34
Corr: -0.69 DN201-RN152 Corr: -0.82
≈≡n」a5ue」.L
u <λ!5 0,20 0,25 0,30 0.35
Interaction
RN34-DN121 Corr: -0.91
2---n」a5ue」l
Mβo3ioaα
Interaction
DN201 Corr: -0.
≈---n ∙lat∙ue.ll
DN121-RN152 Corr: -0.94 RN152-DN121 Corr:-0.95
RN152-DN201 Corr:-0.94
>=≡n」a5ue」J_
i≡n」a5ue」l
Z 6u=θω
i=5∙lajsue.ll
0,n 0,W 0.89 0.9B
Interaction
>=≡n」a5ue」l
αβ4 0,72 O,BO O,Bβ 0,95
Interaction
uθ∙⅛o o.»o 1.00 no i.io α⅛θM9ftraoS7Mβ ,,vo.io o.so i,iɪo i.io i.io
Interaction	Interaction	Interaction
DN121-RN34 Corr: -0.87 DN121-RN152 Corr: -0.97 RN152-DN121 Corr: -0.94
≈≡n」a5ue」l
Interaction
Λl=-=LatUSl
,Ia-SUS-
Interaction	Interaction	Interaction
RN152-DN201 Corr: -0.97 DN201-RN34 Corr: -0.82 DN201-RN152 Corr
Interaction
1,20 1.3S LH 1,(S
Interaction
Λl=-=LatUSl
l.g« 1.21 1.3« 1.S1 1.N
Interaction
>=≡n」a5ue」l
Ml LW LB U*
Interaction
9,97 1,M 1,15 L24
Interaction


Figure 1: The negative correlation between the transfer utility and the interaction. The correla-
tion is computed as the Pearson correlation. The blue shade in each subfigure represents the 95%
confidence interval of the linear regression.
follows, which significantly reduces the computational cost. Please see Appendix F for the proof.
Ei,j [Iij(δ)]	=	n--ιEi	[v(Ω) - v(Ω \	{i})	-	v({i})	+	v(0)].	(4)
Using 50 images randomly sampled from the validation set of the ImageNet dataset (Russakovsky
et al., 2015), We generate adversarial perturbations on four types of DNNs, including ResNet-
34/152(RN-34/152) (He et al., 2016) and DenSeNet-121/201(DN-121/201) (Huang et al., 2017).
We transfer adversarial perturbations generated on each ResNet to DenseNets. Similarly, We
also transfer adversarial perturbations generated on each DenseNet to ResNets. Figure 1 shows
the negative correlation between the transfer utility and the interaction. Each subfigure corre-
sponds to a specific pair of source DNN and target DNN. In each subfigure, each point repre-
sents the average transfer utility and the average interaction of adversarial perturbations through
all testing images. Different points represent the average interaction and the average transfer
utility computed using different hyper-parameters. Given an input image x, adversarial pertur-
bations are generated by solving the relaxed form of Equation (2) via the gradient descent, i.e.
minδ -'(h(X + δ),y) + C ∙ ∣∣δkp s.t. X + δ ∈ [0,1]n, where C ∈ R is a scalar. In this way, We grad-
ually change the value of c and set different values of p2 as different hyper-parameters to generate
different adversarial perturbations, thereby drawing different points in each subfigure. Fair compar-
isons require adversarial perturbations generated with different hyper-parameters C to be comparable
with each other. Thus, we select a constant τ and take ∣δ∣2= τ as the stopping criteria of all adver-
sarial attacks. Please see Appendix G for more details.
4	Unified understanding of transferability-boosting attacks
In this section, we prove that some classical methods of improving the adversarial transferability
essentially decrease interactions between perturbation units, although these methods are not origi-
nally designed to decrease the interaction. Without loss of generality, let us be given an input sample
x ∈ Rn and a DNN h(∙) trained for classification.
•	VR Attack (Wu et al., 2018) smooths the classification loss with the Gaussian noise during
attacking. In the VR Attack, the gradient of the input sample is computed as follows. gt =
Eξ〜N(0,σ2i) [Vχ'(h(χ + δt + ξ),y)]. The following proposition proves that the VR Attack is prone
to decrease interactions inside perturbation units.
Proposition 2. (Proof in Appendix H) The adversarial perturbation generated by the multi-step
attack is given as δ黑出=α Pt=-I Vx'(h(x + δ1nιUlti), y). The adversarial perturbation generated
by the VR Attack is computed as δm = α Pm-I Vx'(h(x + δVr),y), where '(h(X + δVr), y)=
Eξ〜N(O/1) ['(h(X + δVr + ξ), y)]. Perturbation units of δvr tend to exhibit Smaller interactions
than δmulti, i.e. ExEa,b[Iab(δvmr )] ≤ ExEa,b[Iab(δmmulti)].
Besides the theoretical proof, we also conduct experiments to compare interactions of perturbation
units generated by the baseline multi-step attack (implemented as (Madry et al., 2018)) with those
2We set p = 2 as the setting 1, and p = 5 as the setting 2. To this end, the performance of adversarial
perturbations is not the key issue in the experiment. Instead, we just randomly set the p value to examine
the trustworthiness of the negative correlation under various attacking conditions (even in extreme attacking
conditions).
5
Published as a conference paper at ICLR 2021
of perturbation units generated by the VR Attack. Table 5 shows that the VR Attack exhibits lower
interactions between perturbation units than the baseline multi-step attack.
•	MI Attack (Dong et al., 2018) incorporates the momentum of gradients when updating the ad-
versarial perturbation. In the MI Attack, the gradient used in step t is computed as follows.
gt = μ ∙ gt-1 + Vx' (h (X + δt-1) ,y) /∣∣Vχ' (h (X + δt-1) ,y)∣∣1.
Note that the original MI Attack and the multi-step attack cannot be directly compared, since that
magnitudes of the generated perturbations cannot be fairly controlled. The values of interactions
are sensitive to the magnitude of perturbations. Comparing perturbations with different magnitudes
is not fair. Thus, We slightly revise the MI Attack as ∀t > 0, gmi = μgtt-1 + (1 — μ)Vχ'(h(x +
δmt-i 1),y);gm0i =0, where μ = (t - l)/t. We investigate the interaction of adversarial perturbations
generated by the original multi-step attack and the MI Attack. We prove the folloWing proposition,
which shows that the MI Attack decreases the interaction between perturbation units in most cases.
Proposition 3. (Proof in Appendix I) The adversarial perturbation generated by the multi-step
attackisgiven as δ黑出=α Pt=-I Vχ'(h(x+δIm Ulti),y). The adversarial perturbation generated by
the multi-step attack incorporating the momentum is computed as δmmi = α Ptm=-01 gmt i. Perturbation
units of δmmi exhibit smaller interactions than δmmulti, i.e. Ea,b[Iab(δmmi)] ≤ Ea,b[Iab(δmmulti)].
•	SGM Attack (Wu et al., 2020a) exploits the gradient information of the skip connection in ResNets
to improve the transferability of adversarial perturbations. The SGM Attack revises the gradient in
the backpropagation, which can be considered as to add a specific dropout operation in the back-
propagation. We notice that Zhang et al. (2021b) has proved that the dropout operation can decrease
the significance of interactions, so as to decrease the significance of the over-fitting of DNNs. Thus,
this also proves that the SGM Attack decreases interactions between perturbation units.
Besides the theoretical proof, we also conduct experiments to compare interactions of perturbation
units generated by the baseline multi-step attack (implemented as Madry et al. (2018)) with those
of perturbation units generated by the SGM Attack. Table 5 shows that the SGM Attack exhibits
lower interactions than the baseline multi-step attack.
5	The interaction loss for transferability enhancement
Interaction loss. Based on findings in previous sections, we propose a loss to directly penalize
interactions during attacking, in order to improve the transferability of adversarial perturbations.
Based on Equation (2), we jointly optimize the classification loss and the interaction loss to generate
adversarial perturbations. This method is termed the interaction-reduced attack (IR Attack).
max ['(h(x + δ), y) - λ'interaction],	'interaction = Ei,j [Iij (δ)] s.t. kδkp≤ E, x + δ ∈ [0, 1] , (5)
where 'interaction is the interaction loss, and λ is a constant weight for the interaction loss. Although
the computation of the interaction loss can be simplified according to Equation (4), the computa-
tional cost of the interaction loss is intolerable, when the dimension of images is high. Therefore, as
a trade-off between the accuracy and the computational cost, we divide the input image into 16 × 16
grids. We measure and penalize interactions at the grid level, instead of the pixel level. Moreover,
we apply an efficient sampling method to approximate the expectation operation during the compu-
tation of interactions in Equation (4). Figure 2 visualizes interactions between adjacent perturbation
units at the grid level generated with and without the interaction loss.
Experiments. For implementation, we generated adversarial perturbations on six different source
DNNs, including Alexnet (Krizhevsky et al., 2012), VGG-16 (Simonyan & Zisserman, 2015),
ResNet-34/152 (RN-34/152) (He et al., 2016) and DenseNet-121/201 (DN-121/201) (Huang et al.,
2017). For each source DNN, we tested the transferability of the generated perturbations on seven
target DNNs, including VGG-16, ResNet-152 (RN-152), DenseNet-201 (DN-201), SENet-154 (SE-
154) (Hu et al., 2018), InceptionV3 (IncV3) (Szegedy et al., 2016), InceptionV4 (IncV4) (Szegedy
et al., 2017), and Inception-ResNetV2 (IncResV2) (Szegedy et al., 2017). In addition, three state-
of-the-art DNNs, including the Dual-Path-Network (DPN-68) (Chen et al., 2017b), the NASNet-
LARGE (NASN-L) (Zoph et al., 2018), and the Progressive NASNet (PNASN) (Liu et al., 2018),
were used as target DNNs to evaluate the ensemble source model (will be introduced in the next
paragraph). Besides unsecured target DNNs mentioned above, we also used three secured target
6
Published as a conference paper at ICLR 2021
Image
w/o interaction loss
Interactions
with interaction loss
Image
Interactions
w/o interaction loss with interaction loss
Image
Interactions
interaction loss with interaction loss
Figure 2: Visualization of interactions between neighboring perturbation units generated with and
without the interaction loss. The color in the visualization is computed as color [i] 8 Ej∈Ni [Iij (δ)],
where Ni denotes the set of adjacent perturbation units of the perturbation unit i. Here, We ignore
interactions between non-adjacent units to simplify the visualization. It is because adjacent units
usually encode much more significant interactions than other units. The interaction loss forces the
perturbation to encode more negative interactions.
Table 1: The success rates of L∞ and L2 black-box attacks crafted on six source models, includ-
ing AlexNet, VGG16, RN-34/152, DN-121/201, against seven target models. Transferability of
adversarial perturbations can be enhanced by penalizing interactions.
Source	Method	VGG-16 RN152	DN-201	SE-154 IncV3	IncV4 IncResV2
AlexNet	PGD L∞ PGD L∞ +IR	67.0±1.6	27.8±1.1	32.3±0.4	28.2±0.7	29.1±1.5	23.0±0.4	18.6±1.5 78.7±1.0	42.0±1.5	50.3±0.4	41.2±0.6	43.7±0.5	36.4±1.5	29.0±1.0
VGG-16	PGD L∞ PGD L∞ +IR	-	43O±1.8 48.3±2.0 52.9±2.7 39.3±0.7 49.3±1.1 29.7±2.0 -	63.1±1.6 70.0±1.1 71.2±1.5 57.6±1.0 68.6±3.2 49.2±1.2
RN-34	PGD L∞ PGD L∞ +IR	65.4±2.9	59.2±2.7	63.5±3.3	33.1±2.9	27.4±3.6 23.9±1.7	21.1±1.1 84.0±0.5	84.7±2.3	88.5±0.9	64.4±1.6	56.9±3.1 59.3±4.3	49.2±1.1
RN-152	PGD L∞ PGD L∞ +IR	51.6±3.2	-	61.5±2.4	33.9±1.5	28.1±0.9	25.0±1.2	22.4±1.0 72.3±1.2	-	82.1±1.3	61.1±0.9	53.6±0.8	50.6±3.5	46.0±2.3
DN-121	PGD L∞ PGD L∞ +IR	68.6±1.1	63.6±3.2	86.9±1.5	46.1±1.5	37.3±1.6	37.1±2.1	28.9±2.8 85.0±0.3	84.8±0.4	95.1±0.2	70.3±1.7	61.1±2.5	62.1±2.0	53.5±0.3
DN-201	PGD L∞ PGD L∞ +IR	64.4±1.4	67.8±0.2	-	50.9±0.8	39.5±3.3	36.5±0.9	34.2±0.4 78.6±2.5	85.0±1.1	―	73.9±0.5	61.6±1.8	63.7±0.6	56.4±2.1
AlexNet	-PGD L2 PGD L2+IR	85.1±1.5 58.9±1.0 60.2±2.1 55.1±1.5 56.0±3.7 49.6±3.4 44.6±3.3 91.6±1.1 72.0±1.6 76.8±1.0 69.0±1.0 73.0±0.8 63.1±2.1 59.4±1.9
VGG-16	-PGD L2 PGD L2+IR	-	76.7±0.9 82.3±2.9 83.5±1.9 77.5±3.6 82.1±2.2 69.4±2.1 -	86.5±0.9 88.9±1.5 89.6±1.2 85.2±1.1 88.3±1.4 80.4±0.4
RN-34	-PGD L2 PGD L2+IR	88.2±1.4	86.2±0.4	89.6±1.3	66.9±1.1	64.2±2.9	60.0±1.9	55.2±1.8 95.2±0.2	95.4±0.1	96.7±0.6	86.7±1.2	84.3±0.6	81.8±1.9	80.4±1.9
DN-121	-PGD L2- PGD L2+IR	89.4±1.1	86.8±1.0	97.6±1.0	75.6±1.7	70.1±2.9	70.4±4.4	66.5±4.7 94.2±0.1	93.3±0.8	97.7±0.3	87.8±0.7	84.5±0.7	84.2±0.1	82.4±0.1
Table 2: The success rates of L∞ black-box attacks crafted on the ensemble model (RN-34+RN-
152+DN-121) against nine target models.
Source Method VGG-16 RN-152 DN-201	SE-154 IncV3 IncV4 IncResV2 DPN-68 NASN-L PNASN
Ensemble
PGD L∞	86.6±1.2 99.9±0.1 97.0±0.7	70.7±1.6 64.2±0.3	57.7±2.4	53.1±0.7	61.6±0.5 59.6±0.4	72.3±0.3
PGD L∞ +IR 91.5±0.1 92.4±1.6 92.1±1.7	86.1±0.3 81.6±0.9	79.9±1.7	78.4±1.3	82.5±1.0 82.3±1.6	85.6±0.5
models for testing, which were learned via ensemble adversarial training: IncV3ens3 (ensemble of
three IncV3 networks), IncV3ens4 (ensemble of four IncV3 networks), and IncResV2ens3 (ensemble
of three IncResV2 networks), which were released by Tramer et al. (2017).
Ensemble source model: Besides above adversarial transferring from a single-source model, we also
conducted the proposed IR Attack in the scenario of the ensemble-based attacking (Liu et al., 2016),
in order to generate adversarial perturbations on the ensemble of RN-34, RN-152, and DN-121.
Baselines. The first baseline method, the PGD Attack (Madry et al., 2018), directly solved the
Equation (2), which was widely used for adversarial attacks. Besides this baseline attack, the other
four baselines were the MI Attack (Dong et al., 2018), the VR Attack (Wu et al., 2018), the SGM
Attack (Wu et al., 2020a), and the TI Attack (Dong et al., 2019). Our method was implemented
according to Equation (5), namely the IR Attack. Because the SGM Attack was one of the top-ranked
methods of boosting the adversarial transferability, we further added the interaction loss 'interaction
to the SGM Attack as another implementation of our method (namely the SGM+IR Attack). We
also used the interaction loss to boost the performance of the MI Attack and the VR Attack (namely
MI+IR and VR+IR, respectively). Please see Appendix M.1 for details. Moreover, as Section 4
states, the MI Attack, VR Attack, and SGM Attack also decrease interactions during attacking.
Thus, we combined the IR Attack with all these interaction-reducing techniques together as a new
implementation of our method, namely the HybridIR Attack. All attacks were conducted with 100
7
Published as a conference paper at ICLR 2021
Table 3: Transferability against the secured models: the success rates of L∞ black-box attacks
crafted on RN-34 and DN-121 source models against three secured models.
Source	Method	IncV3ens3	IncV3ens4	IncResens3	Source	Method	IncV3ens3	IncV3ens4	IncResens3
RN-34	PGD L∞ PGD L∞+IR	-9.8±0.1 26.5±2.9	10.0±0.5 22.1±1.3	5.7±0.3 14.3±0.4	DN-121	PGD L∞ PGD L∞+IR	12.8±0.1 28.0±1.8	11.2±1.7 26.5±2.1	6.9±1.0 17.4±1.3
	TI3 4 TI4 + IR	21.4±0.8 33.6±0.4	20.9±0.9 33.2±0.3	14.9±1.4 24.0±0.5		TI4 TI4 + IR	26.8±1.3 38.0±2.5	26.1±1.5 42.2±7.7	19.4±1.6 29.0±1.4
Table 4: The success rates of L∞ black-box attacks crafted by different methods on four source
models (RN-34/152, DN-121/201) against seven target models. Transferability of adversarial per-
turbations can be enhanced by penalizing interactions.
Source
RN-34
RN-152
DN-121
DN-201
Method
MI
VR
SGM
SGM+IR
HybridIR
MI
VR
SGM
SGM+IR
HybridIR
MI
VR
SGM
SGM+IR
HybridIR
MI
VR
SGM
SGM+IR
VGG-16~RN152^^DN-201 ^^SE-154	IncV3
80.1±0.5
88.8±0.2
91.8±0.6
94.7±0.6
96.5±0.1
70.3±0.6
83.9±3.4
88.2±0.5
92.0±1.0
95.3±0.4
83.0±4.9
91.5±0.5
88.7±0.9
91.7±0.2
96.9±0.4
77.3±0.8
87.3±1.1
87.3±0.3
89.5±0.9
73.0±2.3
86.4±1.6
89.0±0.9
91.7±0.6
94.9±0.3
72.0±0.7
88.7±0.5
88.1±1.0
90.4±0.4
96.8±0.4
74.8±1.4
90.4±1.2
92.4±1.0
91.8±0.7
77.7±0.5
87.9±2.4
90.0±0.4
93.4±0.8
95.6±0.6
74.8±1.4
91.1±0.9
90.2±0.3
92.5±0.4
96.9±0.2
91.5±0.2
98.8±0.2
98.0±0.4
94.3±0.1
99.1±0.4
48.9±0.8
62.1±1.5
68.0±1.4
72.7±0.4
79.7±1.0
51.7±0.8
70.0±3.7
72.7±1.4
79.3±0.1
84.7±0.7
58.4±2.6
75.1±1.3
78.0±0.9
87.0±0.4
90.9±0.5
64.6±1.0
78.0±1.5
82.9±0.2
87.3±1.2
46.2±1.2
58.4±3.0
63.9±0.3
68.9±0.9
77.1±0.8
47.1±0.9
63.1	±0.9
63.2	±0.7
69.6±0.8
80.0±1.2
54.6±1.6
74.3±1.7
64.7±2.5
78.8±1.3
88.4±0.8
56.5±2.5
75.8±2.1
72.3±0.3
82.5±0.8
IncV4
39.9±0.5
56.3±2.3
58.2±1.1
64.1±1.3
73.8±0.1
40.5±1.6
58.8±0.1
59.1±1.5
66.2±1.0
77.5±0.8
49.2±2.4
75.6±3.0
65.4±2.3
79.5±0.2
87.8±0.8
51.1±2.1
75.8±1.3
71.3±0.6
80.3±0.3
IncResV2
34.8±2.5
49.7±0.9
54.6±1.2
61.3±1.0
70.2±0.5
36.8±2.7
56.2±1.3
58.1±1.2
63.6±0.9
75.6±0.6
43.9±1.5
69.8±1.3
59.7±1.7
75.8±2.7
87.1±0.4
47.8±1.9
71.3±1.2
68.8±0.5
81.5±0.5
HybridIR 94.4±0.1 96.9±0.5
91.7±0.2 89.6±0.6 88.3±0.3
87.3±0.7
steps3 on randomly selected 1000 images of the validation set in the ImageNet dataset. We set
E = 16/255 for the L∞ attack, and set E = 16∕255√n following the setting in (Dong et al., 2018)
for the L2 attack. The step size was set to 2/255 for all attacks. Considering the efficiency of signal
processing in DNNs with different depths, we set λ = 1 for the IR Attack, when the source DNN
was ResNet. We set λ = 2, for other source DNNs. To enable fair comparisons, the transferability
of each baseline was computed based on the best adversarial perturbation during the 100 steps via
the leave-one-out (LOO) validation. Please see the Appendix K for the motivation and the evidence
of the LOO evaluation of transferability. All attacks were conducted with three different random
samplings of grids or different initial perturbations.
Table 1 reports the success rates of the baseline attack (PGD (Madry et al., 2018)) and the IR Attack,
namely PGD L∞+IR of L∞ attacks and PGD L2+IR of L2 attacks. Compared with the baseline
attack, the transferability was significantly improved by the interaction loss on various source mod-
els against different target models. Let us focus on the L∞ attack. For most source models and
target models, the transferability enhancement brought by the interaction loss was more than 10%.
In particular, when the source DNN and the target DNN were DN-201 and IncV4, respectively, the
baseline attack achieved the transferability of 36.5%. With the interaction loss, the transferability
was improved to 63.7% (> 27% gain). As Table 2 shows, in most cases, the IR Attack on the
ensemble model generated more transferable perturbations than the PGD Attack. Besides, as Ta-
ble 3 shows, our interaction loss also improved the transferability against the secured target DNNs.
Such improvement further verified the negative correlation between transferability and interactions.
Note that we did not use the LOO in Table 3, in order to make experimental settings in this table
consistent with the evaluation used by Tramer et al. (2017). Table 4 shows the improvement of the
transferability obtained by the interaction loss on other attacking methods. The interaction loss could
further boost the transferability of state-of-the-art transfer attacks. Without the interaction loss, the
highest transferability made by the SGM Attack against the IncResV2 was 68.8% (when the source
is DN-201). When the interaction loss was added, the transferability was improved to 81.5% (>
3Previous studies usually set the number of steps to 10 or 20. Here, we set the number of steps to 100
together with the leave-one-out validation for fair comparisons of different attacks.
4The TI Attack was designed oriented to the secured DNNs which were robustly trained via adversarial
training. Thus, we applied the TI Attack to the secured models in Table 3.
8
Published as a conference paper at ICLR 2021
Source: RN34	Source: DN121
>-=qe」-sue」l
A--qe」-sue」l ⑻
A----ele,ɪ-sue.ɪl
Source: RN34
Source: DN121
4 3 2 1 0 ∖7
αφ
Figure 3: (a) The success rates of black-box attacks with the IR Attack using different values of λ.
The success rates increased, when the value of λ increased. (b) The transferability of adversarial
perturbations generated by only using the interaction loss (without the classification loss). Such
adversarial perturbations still exhibited moderate adversarial transferability. Points localized at the
last epoch represent the transferability of noise perturbations as the baseline.
12% gain). Moreover, the HybridIR Attack, which combined all methods of reducing interactions
together, improved success rates from the range of 54.6%〜98.8% to the range of 70.2%〜99.1%.
We can understand behaviors of the proposed interaction loss as follows. Different methods generate
adversarial perturbations in different manifolds, thereby exhibiting different transferability. Based
on the current perturbation, the interaction loss can point out the optimization direction towards
further decrease of interactions in a local manner due to its optimization power. Thus, the interaction
loss further boosts the transferability.
To further demonstrate the broad applicability of the interaction loss, besides untargeted attacks on
the ImageNet dataset, we also conducted targeted attacks on the CIFAR-10 dataset (Krizhevsky &
Hinton, 2009). Experimental results consistently showed that the adversarial transferability can be
enhanced by reducing interactions in targeted attacks. Please see Appendix. M.2 for details.
Effects of the interaction loss. We tested the transferability of perturbations generated by the IR
Attack with different weights of the interaction loss λ. In particular, the baseline attack (PGD) can
be considered as the IR Attack when λ = 0. We conducted attacks on two source DNNs (RN-34,
DN-121), and transferred adversarial perturbations to seven target DNNs (VGG16, RN-152, DN-
201, SE-154, IncV3, IncV4, IncResV2). The attacks were conducted with 100 steps3 on validation
images in ImageNet . Figure 3 (a) shows the black-box success rates with different values of λ. The
transferability of the IR Attack increased along with the increase of the weight λ.
Attack only with the interaction loss. To further understand the effects of the interaction loss, we
generated perturbations by exclusively using the interaction loss (without the classification loss). We
used the RN-34 and DN-121 as source DNNs and tested the transferability on seven target DNNs.
The attacks were conducted with 100 steps3 on ImageNet validation images. Figure 3 (b) shows the
curve of the transferability in different epochs. We compared such adversarial perturbations with
noise perturbations generated as e ∙ sign(noise), where noise 〜N(0, σ2I), and e = 16/255, which
was the same as the value used in the L∞ attack. We found that perturbations generated by only
using the interaction loss still exhibited moderate adversarial transferability. This phenomenon may
be explained as that such perturbations decrease most interactions in the DNN, thereby damaging
the inference patterns in the input image.
6 Conclusion
In this paper, we have analyzed the transferability of adversarial perturbations from the perspective
of interactions based on game theory. We have proved that the multi-step attack tends to gener-
ate adversarial perturbations with large interactions. We have discovered and partially proved the
negative correlation between the transferability and interactions inside adversarial perturbations.
I.e. adversarial perturbations with higher transferability usually exhibit more negative interactions.
We have proved that some classical methods of enhancing the transferability essentially decrease
interactions between perturbation units, which provides a unified view to understand the enhance-
ment of transferability. Moreover, we have proposed a new loss to directly penalize interactions
between perturbation units during attacking, which significantly improves the transferability of pre-
vious methods. Furthermore, we have found that adversarial perturbations generated only using the
interaction loss without the classification loss still exhibited moderate transferability, which provides
a new perspective to understand the transferability of adversarial perturbations.
9
Published as a conference paper at ICLR 2021
Acknowledgments
All members in Shanghai Jiao Tong university, including Xin Wang, Jie Ren, Shuyun Lin, Xiang-
ming Zhu, and Dr. Quanshi Zhang are supported by National Natural Science Foundation of China
(61906120 and U19B2043) and Huawei Technologies. Dr. Yisen Wang is partially supported by the
National Natural Science Foundation of China under Grant 62006153, and CCF-Baidu Open Fund
(OF2020002). Xin Wang is supported by Wu Wen Jun Honorary Doctoral Scholarship, AI Institute,
Shanghai Jiao Tong University.
References
Marco Ancona, Cengiz Oztireli, and Markus Gross. Explaining deep neural networks with a poly-
nomial time algorithm for shapley value approximation. In International Conference on Machine
Learning,pp. 272-281, 2019.
Yang Bai, Yuyuan Zeng, Yong Jiang, Yisen Wang, Shu-Tao Xia, and Weiwei Guo. Improving query
efficiency of black-box adversarial attack. In ECCV, 2020.
Arjun Nitin Bhagoji, Warren He, Bo Li, and Dawn Song. Practical black-box attacks on deep neural
networks using efficient query mechanisms. In European Conference on Computer Vision, pp.
158-174. Springer, 2018.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
ieee symposium on security and privacy (sp), pp. 39-57. IEEE, 2017.
Hanjie Chen and Yangfeng Ji. Learning variational word masks to improve the interpretability of
neural text classifiers. In EMNLP, 2020.
Hanjie Chen, Guangtao Zheng, and Yangfeng Ji. Generating hierarchical explanations on text clas-
sification via feature interaction detection. In ACL, 2020.
Jianbo Chen, Le Song, Martin J. Wainwright, and Michael I. Jordan. L-shapley and c-shapley:
Efficient model interpretation for structured data. In arXiv:1808.02610, 2018a.
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order opti-
mization based black-box attacks to deep neural networks without training substitute models. In
arXiv:1708.03999, 2017a.
Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh. Ead: Elastic-net attacks
to deep neural networks via adversarial examples. In AAAI, 2018b.
Yunpeng Chen, Jianan Li, Huaxin Xiao, Xiaojie Jin, Shuicheng Yan, and Jiashi Feng. Dual path
networks. In Advances in neural information processing systems, pp. 4467-4475, 2017b.
Mirek Riedewald Daria Sorokina, Rich Caruana. Detecting statistical interactions with additive
groves of trees. In ICML, 2008.
Ambra Demontis, Marco Melis, Maura Pintor, Matthew Jagielski, Battista Biggio, Alina Oprea,
Cristina Nita-Rotaru, and Fabio Roli. Why do adversarial attacks transfer? explaining transfer-
ability of evasion and poisoning attacks. In 28th USENIX Security Symposium USENIX Security
19), pp. 321-338, 2019.
Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Xiaolin Hu, Jianguo Li, , and Jun Zhu.
Boosting adversarial attacks with momentum. In CVPR, 2018.
Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Evading defenses to transferable adversarial
examples by translation-invariant attacks. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 4312-4321, 2019.
Ranjie Duan, Xingjun Ma, Yisen Wang, James Bailey, A Kai Qin, and Yun Yang. Adversarial
camouflage: Hiding physical-world attacks with natural styles. In CVPR, 2020.
10
Published as a conference paper at ICLR 2021
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, 2016.
Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 7132-7141, 2018.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700-4708, 2017.
Qian Huang, Isay Katsman, Horace He, Zeqi Gu, Serge Belongie, and Ser-Nam Lim. Enhancing
adversarial example transferability with an intermediate level attack. In Proceedings of the IEEE
International Conference on Computer Vision, pp. 4733-4742, 2019.
Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with
limited queries and information. In ICML, 2018.
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander
Madry. Adversarial examples are not bugs, they are features. In NeurIPS, 2019.
Nathan Inkawhich, Wei Wen, Hai Helen Li, and Yiran Chen. Feature space perturbations yield more
transferable adversarial examples. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 7066-7074, 2019.
Nathan Inkawhich, Kevin Liang, Lawrence Carin, and Yiran Chen. Transferable perturbations of
deep feature distributions. In International Conference on Learning Representations, 2020.
Joseph D Janizek, Pascal Sturmfels, and Su-In Lee. Explaining explanations: Axiomatic feature
interactions for deep networks. arXiv preprint arXiv:2002.04138, 2020.
Xisen Jin, Zhongyu Wei, Junyi Du, Xiangyang Xue, and Xiang Ren. Towards hierarchical im-
portance attribution: Explaining compositional semantics for neural sequence models. In ICLR,
2020.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, 2009.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. pp. 1097-1105, 2012.
Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial examples in the physical world.
In arXiv:1607.02533, 2017.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Yingwei Li, Song Bai, Yuyin Zhou, Cihang Xie, Zhishuai Zhang, and Alan Yuille. Learning trans-
ferable adversarial examples via ghost networks. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 34, 2020.
Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan
Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. In Proceed-
ings of the European Conference on Computer Vision (ECCV), pp. 19-34, 2018.
Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial exam-
ples and black-box attacks. ICLR, 2016.
11
Published as a conference paper at ICLR 2021
Xingjun Ma, Bo Li, Yisen Wang, Sarah M Erfani, Sudanthi Wijewickrema, Grant Schoenebeck,
Dawn Song, Michael E Houle, and James Bailey. Characterizing adversarial subspaces using
local intrinsic dimensionality. In ICLR, 2018.
Xingjun Ma, Yuhao Niu, Lin Gu, Yisen Wang, Yitian Zhao, James Bailey, and Feng Lu. Under-
standing adversarial attacks on deep learning based medical image analysis systems. Pattern
Recognition, 110:107332, 2021.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In ICLR, 2018.
Grabisch Michel and Roubens Marc. An axiomatic approach to the concept of interaction among
players in cooperative games. In International Journal of Game Theory, 1999.
W. James Murdoch, Peter J. Liu, and Bin Yu. Beyond word importance: Contextual decomposition
to extract interactions from lstms. In ICLR, 2018.
Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik, and Ananthram
Swami. The limitations of deep learning in adversarial settings. In IEEE European Symposium
on Security & Privacy, 2016.
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik, and Ananthram
Swami. Practical black-box attacks against machine learning. In arXiv:1602.02697, 2017.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox,
and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32, pp. 8026-
8037. Curran Associates, Inc., 2019.
Jie Ren, Die Zhang, Yisen Wang, Lu Chen, Zhanpeng Zhou, Xu Cheng, Xin Wang, Yiting Chen, Jie
Shi, and Quanshi Zhang. Game-theoretic understanding of adversarially learned features. arXiv
preprint arXiv:2103.07364, 2021.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
Imagenet large scale visual recognition challenge. In International Journal of Computer Vision,
115(3):211-252, 2015.
Su-In Lee Scott Lundberg. Consistent feature attribution for tree ensembles. In ICML WHI Work-
shop, 2017.
Lloyd S Shapley. A value for n-person games. In Contributions to the Theory of Games, 2(28):
307-317, 1953.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In ICLR, 2015.
Chandan Singh, W. James Murdoch, and Bin Yu. Hierarchical interpretations for neural network
predictions. In ICLR, 2019.
Jiawei Su, Danilo Vasconcellos Vargas, and Sakurai Kouichi. One pixel attack for fooling deep
neural networks. In arXiv:1710.08864, 2017.
Mukund Sundararajan and Amir Najmi. The many shapley values for model explanation. arXiv
preprint arXiv:1908.08474, 2019.
Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In
International Conference on Machine Learning, pp. 3319-3328, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
12
Published as a conference paper at ICLR 2021
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-
ing the inception architecture for computer vision. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 2818-2826, 2016.
Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4,
inception-resnet and the impact of residual connections on learning. In Thirty-first AAAI con-
ference on artificial intelligence, 2017.
Florian Tramer, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick Mc-
Daniel. Ensemble adversarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204,
2017.
Michael Tsang, Dehua Cheng, and Yan Liu. Detecting statistical interactions from neural network
weights. In ICLR, 2018.
Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quanquan Gu. On the
convergence and robustness of adversarial training. In ICML, 2019.
Robert J Weber. Probabilistic values for games. The Shapley Value. Essays in Honor of Lloyd S.
Shapley, pp. 101-119, 1988.
Dongxian Wu, Yisen Wang, Shu-Tao Xia, James Bailey, and Xingjun Ma. Skip connections matter:
On the transferability of adversarial examples generated with resnets. In International Conference
on Learning Representations, 2020a.
Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust gener-
alization. In NeurIPS, 2020b.
Lei Wu, Zhanxing Zhu, and Cheng Tai. Understanding and enhancing the transferability of adver-
sarial examples. arXiv preprint arXiv:1802.09707, 2018.
Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren, and Alan L Yuille.
Improving transferability of adversarial examples with input diversity. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 2730-2739, 2019.
Die Zhang, Huilin Zhou, Hao Zhang, Xiaoyi Bao, Da Huo, Ruizhao Chen, Xu Cheng, Mengyue
Wu, and Quanshi Zhang. Building interpretable interaction trees for deep nlp models. In AAAI,
2021a.
Hao Zhang, Xu Cheng, Yiting Chen, and Quanshi Zhang. Game-theoretic interactions of different
orders. arXiv preprint arXiv:2010.14978, 2020.
Hao Zhang, Sen Li, Yinchao Ma, Mingjie Li, Yichen Xie, and Quanshi Zhang. Interpreting and
boosting dropout from a game-theoretic view. In ICLR, 2021b.
Hao Zhang, Yichen Xie, Longjie Zheng, Die Zhang, and Quanshi Zhang. Interpreting multivariate
interactions in dnns. In AAAI, 2021c.
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures
for scalable image recognition. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 8697-8710, 2018.
13
Published as a conference paper at ICLR 2021
A	Motivations for using the Shapley interaction index
In this section, we discuss the motivations of using the Shapley interaction index to define the inter-
action.
A. 1 Four Properties of Shapley Values
Let Ω = {1,2,...,n} denote the set of all players, and the reward function is v. Without ambiguity,
We use φ(i∣Ω) to denote the Shapley value of the player i in the game with all players Ω and reward
function v, which is given as follows.
Φ(i∣Ω)= X IS |!(n -ST)!(v(S ∪{i})-v(S)).	(6)
n!
S⊆Ω∖{i}
The Shapley value satisfies the following four properties (Weber, 1988):
•	Linearity property: If there are two games and the corresponding reward functions are v and w, i.e.
V(S) and W(S) measure the reward obtained by players in S in these two games. Let φv(i∣Ω) and
φw (i∣Ω) denote the Shapley value of the player i in the game V and game w, respectively. If these two
games are combined into anew game, and the reward function becomes reward(S) = v(S) + w(S),
then the Shapley value comes to be φv+w (i∣Ω) = φv (i∣Ω) + φw (i∣Ω) for each player i in Ω.
•	Dummy property: A player i ∈ Ω is referred to as a dummy player if ∀S ⊆ Ω∖{i}, V(S ∪ {i})=
V(S) + v({i}). In this way, φ(i∣Ω) = v({i}) - v(0), which means that player i plays the game
independently.
•	Symmetry property: If ∀S ⊆ Ω \ {i, j}, V(S ∪ {i}) = V(S ∪ {j}), then Shapley values of player i and
j are equal, i.e. φ(i∣Ω) = φ(j∣Ω).
•	Efficiency property: The sum of each individual’s Shapley value is equal to the reward won by
the coalition N, i.e. Pi φ(i∣Ω) = v(Ω) - v(0). This property guarantees the overall reward can be
allocated to each player in the game.
A.2 Motivations
Theoretical rigor. We use the Shapley interaction index defined based on the Shapley value, be-
cause the Shapley value has a solid theoretical foundation in the game theory, which is the unique
attribution satisfying the above four desirable axioms.
Whether the metric depends on network architectures. Because adversarial transferability is a
general property for the attack, a convincing metric for adversarial transferability is supposed not
to be directly related to the network architecture. To this end, the computation of the interaction
defined on the Shapley value does not depend on the network architecture. In comparison, previous
definitions of the interaction are usually oriented to model architectures. For example, the interaction
proposed by Tsang et al. (2018) requires the DNN to be fully-connected. The two interaction metrics
proposed by Murdoch et al. (2018) and Jin et al. (2020) are designed for LSTMs. The Hessian-based
interaction (Janizek et al., 2020) requires the DNN to use the softPlus operation to replace the ReLU
operation.
Computational cost. The computational cost of the Shapley-based interaction-reduction loss is
relatively low. Because of the efficiency axiom of the Shapley value, we prove that the time cost
of computing the interaction loss 'interaction = n⅛ιEi[v(Ω) - v(Ω∖{i}) - v({i}) + v(0)] is linear,
i.e. O(n), where n is the dimension of features. The linear complexity makes it possible to apply
the interaction to high-dimensional data and deep neural networks. In contrast, the complexity of
computing all possible pairwise interactions defined in (Daria Sorokina, 2008) is O(n2).
B Comparisons between Interactions inside Perturbations of
Different Attacks
We have theoretically proved that some classical attacking methods of boosting the adversarial trans-
ferability essentially decrease interactions inside perturbations. Besides the theoretical proof in Ap-
14
Published as a conference paper at ICLR 2021
Table 5: The average interaction inside adversarial perturbations generated by different attacks.
Method	RN-34	RN-152	DN-121	DN-201
Baseline (PGD Attack)	0.422	0.926	0.909	0.784
SGM Attack	-0.012	0.037	0.395	0.308
VR Attack	0.097	0.270	0.242	0.137
pendix I and Appendix H, we also conduct experiments to compare interactions of perturbation
units when we generate adversarial perturbations with and without these attacking methods. Such
experiments further verify that these methods of boosting the transferability essentially decrease in-
teractions. We conduct attacks with the validation set in the ImageNet dataset on four DNNs, and
measure the average interaction inside perturbation units. As Table 5 shows, the SGM Attack and
the VR Attack decrease interactions inside perturbations.
C Adversarial Attack
In general, the objective of adversarial attacking can be formulated as the following optimization
problem.
maximize '(h(x + δ),y) s.t. ∣∣δ∣∣p≤ e, x + δ ∈ [0,1]n,	(7)
δ
where '(h(χ + δ), y) is the classification loss. There are many ways to solve the above optimization
problem under different norm constraints ∣∣∙kp (Goodfellow et al., 2014; Carlini & Wagner, 2017;
Kurakin et al., 2017; Madry et al., 2018; Chen et al., 2018b; Wang et al., 2019).
Optimization-based approach. One approach to approximately solving Equation (7) is to solve
the following relaxed form:
minimize {-'(h(x + δ),y) + C ∙∣∣δ∣p} s.t. X + δ ∈ [0,1]n,	(8)
δ
where c > 0 is a scalar constant to balance the classification loss and the norm constraint. Szegedy
et al. (2013); Carlini & Wagner (2017) have demonstrated the effectiveness of this method.
Projected gradient descent (PGD) (Madry et al., 2018). The PGD Attack is usually considered
as one of the simplest and the most widely used baseline for adversarial attacking. In this paper,
this method is called the Baseline. The PGD Attack directly optimizes the classification loss in
Equation (7). Considering the norm constraint, after each step of updating, the PGD Attack projects
the adversarial perturbation δ back to the -ball, if the perturbation goes beyond the ball.
PGD updates adversarial perturbations in each step with the following equation:
(Π(∞) (δt + α ∙ sign (V' (h(x + δt), y))), P = +∞
δt+1 =	Π π⑵(δt	+ α ∙ V'(h(χ+δt),y)	ʌ	p = 2	⑼
(U C	+ ɑ kV'(h(x+δt),y)k2	J	,	P = 2,
where δt denotes the perturbation of the t-th step. Π(∞) and Π(2) are projection operations, which
project the perturbation δ back to the -ball, if the perturbation goes beyond the ball. α is the step
size. Given δ ∈ Rn , we have:
∏(∞)(δ∙) = J E ∙ sign(δi), if∣δi∣>E	∏(2)(δ) = JE kδk2 , if kδk2 > €	(10)
"e	(δi)	= t	δi,	if∣δi∣≤ E	, U	(δ)=( X if	∣δ∣2	≤ E . (IO)
D Equivalent forms of the interaction
In Section 3.1, the interaction between units i, j is defined as the additional contribution as follows.
Iij(δ) = Φ(Sij∣Ω0) - [φ(i∣Ω \ {j}) + φ(j∣Ω \ {i})],	(11)
where φ(Sj∣Ω0) denotes the joint contribution of i,j, when perturbation units i,j are regarded as a
singleton unit Sij = {i, j }, as follows.
φ(Sij ∣Ω0)=	X	|S|!((： ] |S|-2)!(V(S ∪{i,j})-v(S)),
S⊆Ω∖{i,j}	l }
15
Published as a conference paper at ICLR 2021
where Sij = {i, j} represents the coalition of perturbation units i, j. In this game, because pertur-
bation units i, j are regarded as a singleton player, we can consider there are only n - 1 players in
the game, and consequently the set of players changes to Ω0 = Ω \ {i, j} ∪ Sij.
φ(i∣Ω ∖{j}) and φ(j ∣Ω \ {i}) represent the individual contributions of units i and j, respectively,
when the perturbation units i, j work individually. The individual contribution of perturbation unit
i, when perturbation unit j is absent, is given as follows.
Φ(i∣Ω \{j})=	X	|S]!((；-|S|-2)! (V(S ∪{i})-v(S)).
S⊆Ω∖{i,j}	l }
In this game, because the perturbation unit j is always absent, we can consider there are only n - 1
players in the game. Consequently the set of players changes to Ω ∖{j}.
Similarly, the individual contribution of perturbation unit j , when perturbation unit i is absent, is
given as follows.
Φ(j∣Ω \{i})=	X ISlHSl-2%v(S ∪{j})-v(S)).
S⊆Ω∖{i,j}	l '
In Section 1, the interaction between perturbation units δi, δj is defined as the change of the impor-
tance φi of the i-th unit when the j-th unit δj is perturbed w.r.t the case when the j-th unit δj is not
perturbed. If the perturbation δj on the j -th unit increases the importance φi of the i-th unit, then
there is a positive interaction between δi and δj . If the perturbation δj decreases the importance φi ,
it indicates a negative interaction. Mathematically, this definition can be written as follows.
Iij ⑷=φi,w∕j - φi,w∕oj,	(12)
where φi,w∕j represents the importance of δi, when δj∙ is always present; φi,w∕oj represents the
importance of δi , when δj is always absent. When perturbation unit j is always present, the contri-
bution of perturbation unit i is given as follows.
φi,w∕j =	X	lSl!(∑n! 2)!(v(S ∪ {i,j}) - V(S ∪{j})).
s⊆Ω∖{i,j}	l ―八
In this game, because the perturbation unit j is always present, we can consider there are only n - 1
players.
When perturbation unit j is always absent, the contribution of perturbation unit i is given as follows.
|S|! (n - |S|-2)!
φi,w∕oj = E ---------------( _ i)~~L(V(S ∪ {i}) - V(S)).
S⊆Ω∖{i,j}	⑺―八
In this game, because the perturbation unit j is always absent, we can consider there are only n - 1
players.
The interaction in Equation (11) is equal to the interaction in Equation (12), i.e.
Iij (δ) = Ii0j (δ)
E Proof of Proposition 1
To simplify the problem setting, we do not consider some tricks in adversarial attacking, such as gra-
dient normalization and the clip operation. In multi-step attacking, the final perturbation generated
after t steps is given as follows.
t-1
δmulti c=f α χ Vx '(h(x+δmulti),y),
t0=0
where α represents the step size, and '(h(x), y) is referred as the classification loss.
16
Published as a conference paper at ICLR 2021
To simplify the notation, We use g(x) to denote Vχ'(h(x), y), i.e. g(x) d=f Vχ'(h(x), y). FUrther-
more, we define the update of the perturbation with the multi-step attack at each step t as follows.
∆xmnultid=fα ∙ g(X + δm-U1i).	(13)
In this Way, the perturbation can be Written as folloWs.
δmulti = ∆xɪmulti + ∆x2nulti +	+ ∆xmll^ti.	(14)
Lemma 1. Given the sample X ∈ Rn and the adversarial perturbation δ ∈ Rn, we use Ω =
{1, 2, . . . , n} to denote the set of all perturbation units. The score function is denoted by v(S) =
L(X + δ(S) ), where δ(S) satisfies ∀i ∈ S, δi(S) = δi; ∀i ∈/ S, δi(S) = 0. The Shapley interaction
between perturbation units a, b can be written as Iab = δaHab(X)δb + R2 (δ), where Hab(X) =
∂L∂) represents the element ofthe Hessian matrix, and R2(δ) denotes terms with elements in δ of
higher than the second order.
Proof. The Shapley interaction betWeen perturbation units a, b is
Iab(δ) =	X 倒(n TS|-2)![v(S ∪{a,b}) - v(S ∪{b})-v(S ∪{a})+ V(S)],
(n- 1)!
S⊆Ω∖{a,b}	\	'
Where v(S) = L(X + δ(S) ). Here, the classification loss can be approximated as L(X + δ)
L(X) + gτ(x)δ + 2δτH(x)δ + R2(δ') using Taylor series. Thus, ∀S0 ⊆ Ω,
V(S0) = L(X) + X ga(x)δa + g X δaHab(x)δ(S0) + RS (δ).
a∈S 0	a,b∈S 0
Where R2* S 0(δ) denotes terms With elements in δ(S0) of higher than the second order.
In this Way, the Shapley interaction Iab is given as
Iab(δ) = X |S|!(n TSl-2)![v(S ∪ {a, b}) - v(S ∪ {b}) - v(S ∪ {a}) + V(S)]
(n - 1)!
S⊆Ω∖{a,b}	'	)
=X 倒((；』1-2)! {[L(X)+
S⊆Ω∖{a,b}	\	'
X	ga' (X)δaθ + 2 X	Sa，Ha，b，(X)Sb，+R2SU{a，b})(S)]
a0 ∈S ∪{a,b}	a0,b0 ∈S ∪{a,b}
-[L(x) +	X	ga，(X)Sa，+ 2	X	δa,Ha,b,(X)δb,+ R2S^^b'}) (δ)]
a0∈S∪{b}	a0,b0∈S∪{b}
-[L(x) +	X	ga，(X)δa, + 2	X	δa,Ha，b，(x^	+ R2S∪{a'}∖δ)]
a0∈S∪{a}
a0,b0 ∈S ∪{a}
+ L(X)+ X go, (/"a，+ ； X δa, Ha，b，(x)δv + R^ (δ)}
a0∈S	a0,b0∈S
=
S⊆Ω∖{a,b}
+X
S ⊆Ω∖{a,b}
|S|!((n TSl-2)! {δaHab(X)δb}
(n - 1)!
|S|!((n TS|—2)! [R2S∪{a,b}(δ)) - R2S∪{a})(δ) - R2S∪{b})(δ) + r2S)(δ)]
(n - 1)!
^^{^^≡
^
R2(δ)
}
X X	sz- J [δaHab(X)δb] ∖ + R2(δ)
S = O S⊆Ω∖{a,b},	'卜	I
|S|=s	
17
Published as a conference paper at ICLR 2021
(nX-2(n - 2)!	s! (n - s - 2)!
s!(n - s - 2)!	(n - 1)!瓦 Hab(X)词)+ R2⑷
___ , _ ʌ ,
=δaHab(x)δb + R¾3}
where R2(δ) denotes terms With elements m δ of higher than the second order.	□
Lemma 2. The update of the perturbation with the multi-step attack at step t defined in Equa-
tion (13) can be written as &需m认出=α [I + αH(x)]t-1 g(x) + R, where g(x) =ef Vχ'(h(x),y)
represents the gradient, and H(x) ⅛fVX'(h(x),y) represents the Hessian matrix. R denotes terms
with elements in δmt-ul1ti of higher than the first order.
Proof. If t = 1, ∆x1multi =α ∙ g(x).
Let ∀t0 < t, ∆xm0uiti = α [I + αH (x)]t -1 g(x) + Rf, then we have
△xmnuiti = α ∙ g(x + δm-ι1J // According to Equation (13)
=α ∙ g(x + ∆x1multi + ∆xmmulti +-----+ ∆xmnul1i) // According to Equation (14)
t-1
=α ∙ g(x + α ∣+ + [I + αH (x)] + [I + αH (x)]2 + …+ [I + αH (x)]t-2] g(x) + ^X R；
t0=1
where R denotes terms of elements in bm-t； of higher than the first order.
Using the Taylor series, we get
t-；
△xmjlti = α ∙ g(x) + α2H(x)T(x) + αH(x) ^X R + R；-1,
t0=；	(15)
'{z}
RRI
where RIT denotes terms with elements δmιu1i of higher than the first order. T(x) in Equation (15)
is given as follows.
T(x) = [i + [I + αH(x)] + [I + αH(x)]2 + …+ [I + αH(x)]t-2] g(x).	(16)
Multiply (I + αH (x)) on both sides of Equation (16), and we get
(I + aH(x))T(x) = α ∙ [[I + aH(x)] + [I + αH(x)]2 + …+ [I + αH(x)]t-1] g(x).	(17)
Then, according to Equation (17) and Equation (16), we get
H (x)T (x) = h[I + αH (x)]t-； - Ii g(x).	(18)
Substituting Equation (18) back to Equation (15), we have
△xmulti = α [I + αH(x)]t-1 g(x) + R；.
In this way, we have proved that ∀t ≥ 1, ∆xmulti = α [I + αH(x)]t-1 g(x) + R；.	□
18
Published as a conference paper at ICLR 2021
Proposition 1. The adversarial perturbation generated by the multi-step attack via gradient descent
is given as δ黑出 = α Ptm-I Vχ'(h(x + δImUhi), y), where δImUlti denotes the perturbation after the
t-th step of updating, andm is referred to as the total number of steps. The adversarial perturbation
generated by the single-step attack is given as δSingle = αmVχ'(h(x), y). The expectation ofin-
teractions between perturbation units in δmmulti, Ea,b [Iab(δmmulti)], is larger than Ea,b [Iab(δsingle)], i.e.
Ea,b [Iab(δmmulti)] ≥ Ea,b [Iab(δsingle)].
E.1 Fairness of comparisons of interactions inside different perturbations
Proposition 1 is valid for different loss functions of generating of adversarial perturbations. In this
section, we discuss the fairness of comparisons of interactions inside different perturbations.
When we compare interactions inside different perturbations, magnitudes of these perturbations
should be similar, because the comparison of interactions between adversarial perturbations of dif-
ferent magnitudes is not fair. For fair comparisons, in Section 3.1, this paper controls the magnitude
of the single-step attack by setting the step size of the single-step attack as αm, where α and m de-
notes the step size and the total number of steps of the multi-step attack, respectively. The equivalent
step size αm makes the magnitude of perturbations generated by the single-step attack to be similar
to that of perturbations generated by the multi-step attack, when we use the target score before the
Softmax layer to generate adversarial perturbations, such as '(h(x), y) = maxy,ry h(x) — hy (x). In
this case, the magnitude of the gradient Vχ'(h(x), y) is relatively stable. In particular, this type of
loss has been widely used. For example, one of the most widely used attacking (Carlini & Wagner,
2017), uses the score before the softmax layer for targeted attacking.
E.2 Proof of Proposition 1
Proof. According to Lemma 2, the update of the perturbation with the multi-step attack at the step
t is given as follows.
∆xmnulti = α [I + αH(x)]t-1 g(x) + 用,
(19)
where RVI denotes terms with elements in bm-； of higher than the first order, and a represents the
step size.
To simplify the notation without causing ambiguity, we write g(x) and H(x) as g and H, respec-
tively. In this way, according to Equation (14) and Equation (19), δmmulti can be written as follows.
m
bmmulti = α [l + [I + αH] + [I + αH]2 + ∙∙∙ + [I + αH]m Ii g +)：RI
t=1
=α[mI + αm(m - 1) H + ... 1 g + X R,
2	t=1
(20)
where m represents the total number of steps. According to Lemma 1, the Shapley interaction
between perturbation units a, b in δmmulti is given as follows.
,___ ʌ , .
Iab(δmulti) = δmllti,aHabδmllti,b + R2(δmumi),
(21)
where R2(δmuIti) denotes terms with elements in δmmulti of higher than the second order.
19
Published as a conference paper at ICLR 2021
According to Equation (20) and Equation (21), we have
m	α2m(m - 1)
Iab(δmulti) = Hab amga H-------------- / ^(Hab0gb0) + •…+ ʌ,
b0 =1	t=1
o(δmulti,a)	][
terms of δmtulti,a
of higher than the first order,
which corresponds to the term of
R1 in Equation (20)
αmgb + α2m(m - 1) XX (Ha0bga0) + ••• +
a0=1
]
o(δmt ulti,b)
'—{—}
+R2(δmulti)
terms of δmtulti,b
of higher than the first order,
which corresponds to the term of
Rt in Equation (20)
α m ga gb Hab
X----------{--------}
first-order terms w.r.t. elements in H
+
3(m - 1)m2 n	α3(m - 1)m2 n
2 gb〉J (Hab0gb0 ) +	2	ga〉」 (Ha0bga0 )
b0 =1	a0 =1
{^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
second-order terms w.r.t. elements in H
4	22n	n
α4(m - 1)2m2
+ --------4------ (Ha (Hab0 gb0) ʌ, (Ha0 bga0)Hab + …
b0=1	a0=1
'-------------------------7------------------------
R2multi(H)
mm
+ [∑ O]δmulti,a)]Habδmιlti,b + [£ O(δmulti,b)]Habδmιιti,a + RMδmUιti)
t=1
、
t=1
R2 (δmuιti)
3	2n
=α2m2gagbHab +	(	- )	gaHab X (Ha，bg0，)
a0=1
+ α3(m -1)m2 gbHab XX (Habgb，) + R2(δm∏ti) + Rmulti(H),
b0=1
Hab
/
(22)
where Rmulti (H) represents terms with elements in H of higher than the second order, and R2 (δmulti)
represents terms with elements in δmmulti of higher than the second order.
Let us consider the single-step attack. When we compare interactions inside different perturbations,
magnitudes of these perturbations should be similar, because the comparison of interactions between
adversarial perturbations of different magnitudes is not fair. For fair comparisons, in Section 3.1,
this paper controls the magnitude of the single-step attack, as follows. The single-step attack only
uses the gradient information on the original input x, which generates adversarial perturbations as:
δsingle = αmg.
Therefore, according to Lemma 1, the interaction between perturbation units a, b of δsingle is given
as follows.
Iab (δsingle) = δsingle,aHabδsingle,b + R2 (δsingle)
m α ga gb Hab + R2 (δsingle ),
(23)
where R (63里用) denotes terms with elements in 63用用 of higher than the second order. In this way,
according to Equation (22) and Equation (23), the expectation of the difference between Iab(δmmulti)
and Iab(δsingle) is given as follows.
20
Published as a conference paper at ICLR 2021
VGG16 ReSNet-32 DenSeNet-121
Histograms of the value of the
Pb∙⅞b
Hessian element
w.r.t.
∑α=1 3a^ab
different values of b
Histograms of the value of the
Hessian element Hab w.r.t. different
values of a, b
-‰10 -0‰5
0.00000
(a)
0.βtX)D5
O.OC01O
-0.005	0.005
(b)
-0.015
Figure 4: (a) Histograms of the value of the Hessian element Hab(x) w.r.t. different values of
a, b. (b) Histograms of the value of
gbHbb
pn=ι gaHab
w.r.t. different values of b. Because the Hessian
of the DNN with the ReLU activation is not well defined, we replace the ReLU activation with the
Softplus activation f(x) = 1 log(1 + e-βx). We train VGG-16, ReSNet-32, and DensetNet-121 on
the CIFAR-10 dataset (Krizhevsky et al., 2009), and use the cross-entropy loss as the classification
loss.
Ea,b [Iab (δmulti ) - Iab (δsingle)]
3	2n
α (m- 1)m
=Ea,b [-------2------gaHab T (Ha0bga0 ) +
a0=1
+ R (δm1lti)+RmUIti(H)- R2(δsingle )i
α3 (m - 1)m2
n
gbHab	(Hab0 gb0 )
b0=1
where
α3(m - 1)m2
(2 )	Ea,b
gaHab	(Ha0bga0) + gbHab	(Hab0 gb0)
+ Ea,b [Rab] ,
a0=1
'----------
Uab
b0=1
} X-------------
Uba
n
2
n
}
Rab = R2(δm∏ti)+RmUIti(H)- R2(δsingle).
Assumption 1: Magnitudes of elements in the Hessian matrix H(x) is small that |Hab(x)|	1,
where 1 ≤ a, b ≤ n. Therefore, Hk(x) ≈ 0, if k > 2.
We verify the assumption by directly measuring the value of Hab(x). As Figure 4 (a) shows, the
value ofHab(x) is very small that |Hab(x)|	1.
According to Assumption 1, we have R2multi(H) ≈ 0. Note that the magnitude of δmmulti and the
magnitude of 6.骐 are small, then 左2(6黑山)≈ 0, and R2(δsingle) ≈ 0. In this way, We have
Ea,b[Rab] = Ea,b[R2(δmUlti) + 尺2忌叫用)+ RmUIti(H)] ≈ 0.
Moreover, for the expectation of Uab, we have
n
n
Ea，b[Uab] = n(n- 1)
ΣΣgaHab	(ga0Ha0b)
b=1 a6=b
a0=1
21
Published as a conference paper at ICLR 2021
Multi-step / noise
Multi-step / single-step
Histograms of the value of w.r.t. different values of ɑ
ReSNet-32 DenSeNet-121
Figure 5: Histograms of the value of Eb[Iab] w.r.t. different values of a
1
n(n - 1)
E l EgaHab - gbHbb|	Σ>oHaθb
b=1 I	∖a=1
I I-------
A
a0=1
B '-----------
A
Let us focus on terms ofA and B. Note that A is the sum ofn terms (n is large). In comparisons, B
is just a single term in A. Therefore, the sign of A - B is usually dominated by the term A. In this
way, we get Prob [sign(A - B) = sign(A)] ≈ 1. Therefore, Prob [(A - B)A ≥ 0] ≈ 1. We verify
this assumption by measuring the value of
gbHbb
pn=ι gaHab
. If P rob h|
gbHbb
pn=ι gaHab
|	1 ≈ 1, then we
have Prob [sign(A - B) = sign(A)] ≈ 1. As Figure 4 (b) shows, the value of
gbHbb
small that |
gbHbb
Pn=I gaHab
is very
Pn=I gaHab
|	1. To this end, we have (A - B)B ≥ 0, and we get
n
n
n
Ea,b[Uab] ≥ 0
Due to the symmetry of a, b, we have Ea,b [Uba] = Ea,b [Uab]. Therefore,
Ea,b [Iab (δmulti) - Iab (δsingle)]
=α3(m -1m Ea,b [Uab + Uba]+ Ea,b [Rab]
≈ α3(m - 1)m2Ea,b[Uab] + 0
≥ 0.
(24)
□
E.3 Verification of Proposition 1
We verify that perturbations generated by the multi-step attack tend to exhibit larger interaction
than those generated by the single-step attack by measuring the value of Eb [Iab]. As shown in
Appendix F, we prove that Eb[Iab] = v(Ω) - v(Ω \ {a}) - v({a}) + v(0). Because the image data
is high-dimensional, the cost of computing Eb [Iab] is high. As Appendix J.1 demonstrates, given
the input image, we can measure the interaction at the grid level, instead of the pixel level, to reduce
the computational cost. Therefore, we divide the input image into 16×16 (L = 16) grids, and use
Equation (39) to compute the interaction as E(p0,q0) I(p,q),(p0q0) (δ) = v(Λ) - v(Λ \ {Λpq}) -
v({Λpq}) + v(0), where (p, q) denotes the coordinate of a grid. The experiments were conducted
with ImageNet validation images on ResNet-32 and DenseNet-121.
For fair comparisons, the magnitude of perturbations generated by the single-step attack is controlled
to be same as that generated by the multi-step attack. As Figure 5 (left) shows, perturbations gener-
ated by the multi-step attack tend to exhibit larger interaction than those generated by the single-step
attack.
22
Published as a conference paper at ICLR 2021
E.4 Perturbations generated by the multi-step attack tend to exhibit larger
interaction than Gaussian noise
Moreover, we compare the interaction inside perturbation units generated by the multi-step attack
with the Gaussian noise perturbation. Similarly, for fair comparisons, the magnitude of the Gaussian
noise is controlled to be similar to that generated by the multi-step attack. As Figure 5 (right) shows,
perturbations generated by the multi-step attack tend to exhibit larger interaction than Gaussian
noise.
F Expectation of the S hapley Interaction
In Equation (3), the Shapley interaction between two perturbation units i, j is given as follows.
Iij(δ) = Φ(Sij∣Ω \ {i,j} ∪ Sij) - (φ(i∣Ω \ {j}) + φ(j∣Ω \ {i})),
where φ(Sj∣Ω \ {i,j} ∪ Sij) is the Shapley value of the singleton unit Sij = {i,j}, when pertur-
bation units i,j form a coalition. φ(i∣Ω \ {j}) and φ(j∣Ω \ {i}) are Shapley values of perturbation
units i, j , when these two perturbation units work individually. In this way, we can write the Shapley
interaction in a closed form as follows.
Iij (δ) =	X	|S]!(n ] |S!-2)![v(S ∪ {i,j}) - v(S ∪ {j}) - v(S ∪ {i}) + V(S)], (25)
S⊆Ω∖{i,j}	l 入
where ∀S ⊆ Ω,v(S) = maxyo=y h：) (X + δ(S)) - hl(f)(x + δ(S)). The expectation of interaction is
given as follows.
Ei,j [Iij(δ)] = n-ɪEi [v(Ω) - v(Ω \ {i}) - v({i}) + V(0)],	(26)
which is proved as follows.
Proof. As proved in Appendix D, Iij (δ) = Ii0j (δ). Therefore, the interaction between players i and
j is given as follows.
Iij(δ) =	X	|S|!(n-T1-2)! [ [v(S ∪ {i,j}) - V(S ∪ {j})] - [v(S ∪ {i}) - v(S)]]
S⊆Ω∖{i,j}	l }
=φj,w/ i - φj,w∕oi .
The expectation of the interaction can be written as follows.
Ei,j [Iij (δ)]
(n - 1) Ei (	^X [φj,w∕i - φj,w∕oi∖ }.
〈J	[j∈Ω∖{i}
According to the efficiency property of Shapley values (please refer to Appendix A.1 for details):
E φj,w∕i = V(C)-V{i}
j∈Ω∖{i}
X φj,w∕oi = v(ω \ {i}) - v(0).
j∈Ω∖{i}
In this way,
Ei,j [Iij(δ)] = n-1 Ei [v(Ω) - v(Ω \ {i}) - v({i}) + v(0)].
□
23
Published as a conference paper at ICLR 2021
G	Details of ob serving the negative correlation between the
TRANSFERABILITY AND THE INTERACTION
In Section 3.2, we directly measure the transfer utility and interactions of different adversarial
perturbations. Here, we give more details of the experiments. We measure the transfer utility as
Transfer Utility = [maxy06=y h(yt0) (x + δ) - h(yt)(x + δ)] - [maxy0 6=y h(yt0)(x) - h(yt)(x)]. We measure
the interaction as Ei,j [Ij(δ)] = n-iEi [v(Ω) - v(Ω \ {i}) - v({i}) + v(0)]. As Appendix J.1
demonstrates, to reduce the computational cost, given the input image, we can measure the interac-
tion at the grid level, instead of the pixel level. Therefore, we divide the input image into 16×16
(L = 16) grids, and use Equation (39) to compute the interaction as E(p,q),(p0,q0) I(p,q),(p0 q0) (δ) =
L21-1 E(p,q) [v(Λ) - v(Λ \ {Λpq}) - v({Λpq}) + v(0)], Where (p, q) denotes the coordinate of a
grid.
Using the validation set of the ImageNet dataset (Russakovsky et al., 2015), We generate adversarial
perturbations on four types of DNNs, including ResNet-34/152(RN-34/152) (He et al., 2016) and
DenseNet-121/201(DN-121/201) (Huang et al., 2017). We transfer adversarial perturbations gener-
ated on each ResNet to DenseNets. Similarly, We also transfer adversarial perturbations generated
on each DenseNet to ResNets. Given an input image x, adversarial perturbations are generated using
Equation (8), i.e. minδ -'(h(x + δ),y) + C ∙ ∣∣δkp s.t. X + δ ∈ [0,1]n, where C ∈ R is a scalar
constant. In this Way, We gradually change the value of c as different hyper-parameters to generate
different adversarial perturbations, i.e. Ck = kβ + C0, where β ∈ R is a constant. Moreover, to
ensure adversarial perturbations generated with different values of Ck change smoothly, we use the
perturbation generated with Ck-1 to initialize the perturbation for Ck, i.e. δi(ncikt ) = γδ(ck-1), where
γ ∈ R is a constant. In our experiments, we set γ = 0.6. For fair comparisons, we need to ensure
adversarial perturbations generated with different hyper-parameters C to be comparable with each
other. Thus, we select a constant τ and let ∣δ∣2= τ as the stopping criteria of all adversarial attacks.
We set the number of steps as 1000. The threshold τ is set to ensure that attacks with different
hyper-parameters τ are almost converged when the L2 norm of the perturbation ∣δ ∣2 reaches τ .
Note that different attacking methods may successfully attack different sets of testing samples, so
we select testing samples that can be successfully attacked by all attacking methods with different
Ck values (i.e. those having reached the stopping criteria under all attacks). The interaction and the
transfer utility reported in Figure 1 are measured on the selected samples for fair comparisons.
H Proof of Proposition 2
To simplify the problem setting, we do not consider some tricks in adversarial attacking, such as
gradient normalization and the clip operation. In VR attack (Wu et al., 2018) , the final perturbation
generated after t steps is given as follows.
t-1
δtrd=fα X Vχ%(h(x + δVr),y),
t0=0
where
ʌ,, , . 一 一，一 ， ,. _______________________________________________
'(h(x), y) = Eξ~N(0,σ21)['(h(x + ξ), y)].	(27)
A	1∙	. T-I	. ∙	/Cr∖ . 1	1∙	.	1 . 1 T T ♦	.	∙	i' ∕J/ 7 /	∖	∖ ∙	♦	i' 11
According to Equation (27), the gradient and the HeSSian matrix of '(h(x), y) is given as follows.
,、 ___ ʌ, _ ,、、
g(x) = Vχ '(h(x),y)
=Eξ~N(0,σ2i) [Vx'(h(x + ξ), y)],
HH(X) = Vxah(X)
=Eξ~N(0,σ2I) [Vx'(h(x + ξ),y)].
(28)
where α represents the step size.
Lemma 3. Given the Gaussian smoothed loss '(χ) = Eξ~N(。炉/)['(h(x), y)],
where '(h(x),y) is the original classification loss, ∀a = b,∀c = a, we have
24
Published as a conference paper at ICLR 2021
ExIg2(x)H22b(x) - g2(x)H2b(x)]	≤ 0, Ex [^α(x)gb(x)Hɑb(x) - ga(x)gb(x)Hab(x)^
0,
and Ex [^α(x)^c(x)Hαb(x)HCb(X)- gα(x)gc(x)Hα,b(x)Hcb(x)] = 0.
∂ga(X + ξ)
∂Xb
Ex0 〜N (x,σ21)
∂ga (x0)
∂Xb
EXO 〜N (x,σ2I)[Hab(X0)]∙
Proof. According to Equation (28), we have
gα (X) = Eξ〜N(0,σ2I) [ga (X + ξ)] = Ex0〜N(χ,σ2I) [ga (X )],
Hab(X) = Eξ 〜N (0,σ2I)
This indicates that the gradient and the Hessian matrix in the VR attack are both smoothed by
the Gaussian noise. Because the Lipschitz constants of ga (X) and Hab(X) are usually limited to a
certain range, we can ignore the tiny probability of large gradients and large elements in the Hessian
matrix, and roughly assume that ga(X)〜N(gα(X), σga), and Hab(X)〜N(Hab(X), °H J, where
σga , σHab ∈ R are tow constants denoting the standard deviation. Thus, ga (X) and Hab(X) can be
written as follows.
ga(X)= Oa(X) + Ega ,	Ega 〜”(。,金卜
Hab(X)= Hab(X) + EHab,	EHab 〜N(0, σH .b )∙	(29)
To simplify the notation without causing ambiguity, We write g(∕) and H(x) as g and H, respec-
tively. Moreover, we write g(X) and H(X) as g and H, respectively. In this way, we have
EX h^aH2b - g^2∏ibi
=EXEega ,βHab hg^2lHH2b-(ga + Ega )2(Hab + EHab )2i
=-EXEega ,eHab hEgaH22b + EHab (ga + Ega )2 + 2EgagaHab + 2EHabHab(ga + Ega )2i
≤ -EXEega ,eHab [2Ega ^HOb + ZfHab(ga + Ega )]
=-Ex {Eega [Ega] 2^a Hab + EegahEeHab [EHab]2Hab(ga + Ega )2i}
=-Ex {0 ∙ 2gaHOb + Eegah0 ∙ 2Hab(ga + Ega )[}
= 0.
According to Equation (29), we have ga = ga + Ega, gb = gb + Egb. Thus, we have
Ex ∖gagbHbab - gagbHab∖
=EX [θa(X)ObHab - (ga + Ega )(gb + Egb )(Hab + EHab )]
= - x,ega ,ega ,eHab Ega Egb EH + Ega Egb	ab + Egb EHab ga
+ EHab Egagb + EgbgbHab + EgbgaHab + EHabgagb]
=-EXhEega [Ega ]Eegb [Egb ]EeHab [EH ]+ Eega [Ega]Eegb [Egb]Hab + Ee% [EgblEe/b [EHab]^a
+ EeHab [EHab]Eega [Ega ]gb + Eegakga ]gbHab + Eegbkgb]gaHab + EeHab [EHab]9a9b^
[_ _ _ _ _ ʌ _ _ . ʌ ʌ . ∖
0 ∙ 0 ∙ 0 + 0 ∙ 0 ∙ Hab + 0 ∙ 0 ∙ ga + 0 ∙ 0 ∙ gb + 0 ∙ gbHab + 0 ∙ gaHab + 0 ∙ ga ^bj
0.
Moreover, according to Equation (29), we have
EX IgagcHabHcb - gagcHabHcb]
25
Published as a conference paper at ICLR 2021
Ex,ega，€gc ,eHab ,eHcb [gagc
HabHCb - (ga + Ega )(gc + Cgc) (Hab + EH
ab )(HCb + EHcb )]
ʌ ʌ
ʌ
- x,ga ,gc ,Hab ,Hcb Ega Egc EHab EHcb
ʌ
ʌ
+ Egc EHab EHcb ga + Ega EHab EHcb gC + Ega Egc EHcb Hab + Ega Egc EHab HCb
+ Ega Egc HabHCb + Ega EHabgCHCb + Ega EHcbgCHab
+ Egc EHab ga HCb + Egc EHcb ga Hab + EHab EHcb gagC
+ Ega gCHabHCb + Egc ga HabHCb + EHab ga gCHCb + EHcb gagCHab
=-ExhEega [Ega]%c [Egc]%ab [EHab]%cb "]+ %c kgc ] 呸/b [EHab]%cb
_ 一	,_	一	,_	一 , . _ 一	,_	一 ,_ 一 , ʌ
+ Eega [Ega]EeHab [EHab ]E€Hcb [EHcb ]9 C + E€ga [Ega ]E€gc [Egc ]E€Hcb [EHcb	ab
+ Eega [Ega]Eegc[Egc]EeHab[EHab]HCb
_ 一	,一	, ʌ ʌ _ r ,_ . ʌ _ 一 ,_ 一 , . ʌ
+ EegalEga]Eegc IEgclHabHCb + EegalEga]EeHab EHabgCHCb + EegaIEga ]EeHcb IEHcb]gCHab
+ EegcIEgclEeHab [EHab]gaHCb + EegcIEgc]EeHcb [EHcb]gaHab + EeHabIEHab]EeHcbIEHcblOapC
+ Eega IEga]gcHabHCb + Eegc kgc]^aHabHCb + Ee/b【EHablgagCHCb + EeHcb IEHcb]∂a^cHab
[_ _ _ _ _ _ ʌ	_ ʌ
0 + 0 ∙ ga + 0 ∙ gC + 0 ∙ Hab + 0 ∙ HCb
_ ʌ ʌ . ʌ . ʌ ʌ ʌ _
+ 0 ∙ HabHCb + 0 ∙ gCHCb + 0 ∙ gCHab + 0 ∙ gaHCb + 0 ∙ gaHab + 0 ∙ ga^c
_ . ʌ ʌ ʌ ʌ . ʌ _
+ 0 ∙ gcHabHCb + 0 ∙ gaHabHCb + 0 ∙ gagcHCb + 0 ∙
gagcH^θ]
0.
□
Proposition 2. The adversarial perturbation generated by multi-step attack is denoted by δmmulti =
a Pm-I ^x'(h(x + δ* uiti), y). The adversarial perturbation generated by VR Attack is denoted by
δm = α Pm-I Vxl(h(x + δVr),y), where '(h(X + δVr),y) = Eξ〜N(0口2i) I'(h(x + δtr + ξ),y)].
Perturbation units of δvmr tend to exhibit smaller interaction than δmmulti, i.e. ExEa, bIIab(δvmr )l ≤
ExEa,bIIab(δmmulti)l.
Proof To simplify the notation without causing ambiguity, We write g(x) and H(x) as g and H,
respectively. Moreover, we write g(x) and H(x) as g and H, respectively.
Just like the conclusion in Equation (22), we can write the interaction between δvmr,a and δvmr,b as
follows.
Iab(δm ) = α2m2^a^bHab + α3(m -1)m2 ga H“b XX (H^a0bga0 )
a0 =1
+ a" I1" gbHab XT (HabOgbO) + 塔(δm ) + Rr(H),
(30)
b0 =1
where α denotes the step size, and m denotes the total number of steps. To enable fair comparisons,
we use the same step size α and number of steps m as multi-step attack to make the magnitude of
δvr match the magnitude of δmulti. Rv2r(H) represents terms with elements in H of higher than the
second order, and Rv2r(δvmr ) represents terms with elements in δvmr of higher than the second order.
In this way, according to Equation (30) and Equation (22), the expectation of the difference between
Iab(δvmr ) and Iab(δmmulti) is given as follows.
ExEa,bIIab(δvr) -Iab(δmulti)l
=a3(m -1)m2 Ea,b {Ex [[02% - ff2Hab∖ + h^ι∏2b—g2 Habiio+EaMIRau
26
Published as a conference paper at ICLR 2021
where
vr α3 (m - 1)m2
ab =	2
X	[(gaga0 HabHa'b - gaga，HabHM
a0∈{1,2,...,n}\{a}
X----------------------------------------------}
^{z
Vab
+ X	^gb^b0 HabHab，一 gbgb0 HabHab，)]
b0∈{1,2,…,n}∖{b}
χ---------------------------------------------------
--
Vba
+ α2m2 [gagbHab - gagbHab] + 期(δv) 一 R0羞币)+ R^H) 一 RmUlti(H)
}
The expectation of Rvarb is give as follows.
ExEa,b[Rvarb]
α3(m 一 1)m2
2
Ea,b Ex [Vab] + Ex [Vba] +-7-TTEx ∖gagbHab 一 gagbHabi \
α(m 一 1)
+ EχEa,b[度(δm) - R^2(δmuιti) + Rvr(H) - RmUlti(H)] ≈ 0.
According to AssUmption 1, we have R2vr(H) ≈ 0, and R2mUlti (H) ≈ 0. Note that the magnitUde
of δm and the magnitude of 6皿皿廿 are small, then R2(δvm) ≈ 0, and R2 (δmmUlti) ≈ 0. According to
Lemma 3, we have Ex Igagb Hab - gagbHab] = 0, Ex [(^aga0 HabHab - gaga，HabHa，b)] = 0.
Therefore, we get Ex [Vab] = 0. In this way, ExEa,b [Rvarb] ≈ 0.
Furthermore, according to Lemma 3, we
have Ex [g2 H2b]
- Ex ga2Ha2b ≤ 0.
Therefore,
ExEa,b[Iab(δvr) -Iab(δmulti)]
=α3(m -1)m2Ea,bnEx	[gaHab	-g2Hab] +	Ex	h^2H2b	-gbH2biθ + ExEa,b[Rab]
≈	α3(m -1)m2Ea,bnEx	[^2Hab	-g2Habi +	Ex	h^2H2b	-gbHabiθ + 0
≤0
□
I	Proof of Proposition 3
To simplify the problem setting, we do not consider some tricks in adversarial attacking, such as
gradient normalization and the clip operation. Note that the original MI Attack and the multi-step
attack cannot be directly compared, since that magnitudes of the generated perturbations cannot be
fairly controlled. The value of interactions is sensitive to the magnitude of perturbations. Comparing
perturbations with different magnitudes is not fair. Thus, we slightly revise the MI Attack as
gm id=f μgm 1+a - 〃)vx '(h(χ+δm- 1),y),	(3i)
where t denotes the step and μ = (t - 1)/t. '(h(χ), y) is referred as the classification loss. To
simplify the notation, We use g(x) to denote Vx'(h(x), y), i.e. g(x)d=fVx'(h(x), y).
In MI attack, the final perturbation generated after t steps is given as follows.
t-1
δmt i =e α X gmt i ,
t，=0
27
Published as a conference paper at ICLR 2021
where α represents the step size.
Furthermore, we define the update of perturbation with the MI attack at each step t as follows.
△xmi = α ∙ gmi∙
(32)
In this way, the perturbation can be written as follows.
δm i=δXmi+δXmi+…+δXm-1.
(33)
Lemma 4. The update of the perturbation with the MI attack at step t defined in Equation (32)
can be written as ∆Xtmi = α [I + αt21 H(x) + RI(H(x))] g(x) + RR1, where RI(H(x)) denotes
t1
terms of elements in H(X) higher than the first order, and Rt1 denotes terms with elements in δmt-i 1
of higher than the first order.
Proof. If t = 1, ∆xmni = α ∙ g(x).
Let ∀t0 < t, ∆xm0i = α [l + αt-1 H(x) + 或(H(x))] g(x) + RII
According to Equation (31) and Equation (32), we have
△x'i=α ∙ ~~t-gm- 1+tg(X+δm-I)
Applying the Taylor series to the term of g(X + δmt-i 1), we get
△xmi=α ∙ t--ɪgm- 1 +1 [g(χ)+H(X)δm-1)]+ri-1,	(34)
where r1t-1 denotes terms of elements in δmt-i 1 of higher than the first order.
According to Equation (33) and Equation (34), we get
△/tni = α ∙ { ~~t-gm- 1 + t [g(X) + H(X) [△X1ni + △X2ni +	+ △Xm- 1]] + r1-1
Because ∀t0
< t	∆Xmi	= α	[l	+ αt-1 H(x)	+ RI(H(x))]	g(X)+	RJwehave ∆Xm^ 1
α ∙ [I + αi-22H(x) + RI-I(H(x))] g(X)+ R2-1. According to Equation (32), We get gmi 1
[I + α与2H(x) + Rt-1(H(x))] g(X)+ J¾-1.
28
Published as a conference paper at ICLR 2021
In this way, we get
∆Xtmi
H(x) + Rt1-1 (H (x)) g(x)
1 ʌ , (t -2Xt-I)
1) + α —4—
t-1
t-1
H(X) + ERIO(H(X)) g(X) + ER10
t0=1
t0=1
+ r1t-1
0
0
α ∙
一RIT(H(χ)) + (t- 24tt- 1) H2(χ) + tH(χ)X RtI(H(χ))
t0=1
'------------------------V-------------------------}
Rt1(H(x))
+I + α —2- H (x)+
α I + α力 2 1H(x) + R1(H(x)) g(x) + R；.
1 t-1
g(X) +1X r1 + rt-1
t t0=1
X----------------/
^Z
RI
where RI(H(x)) denotes terms of elements in H(x) higher than the first order, and R denotes
terms with elements in δmt-i 1 of higher than the first order. In this way, we have proved that ∀t ≥
1, ∆xmi = α [I + αt-1 H(x) + RI(H(x))] g(x) + Rf.	□
Proposition 3. The adversarial perturbation generated by multi-step attack is denoted by δmmulti =
α Pt=-I Vχ'(h(x + δmuiti), y). The adversarial perturbation generated by multi-step attack incor-
porating the momentum is computed as δmmi = α Ptm=-01 gmt i Perturbation units of δmmi exhibit smaller
interactions than δmmulti, i.e. Eij [Iij (δmmi)] ≤ Eij [Iij(δmmulti)].
Proof. According to Lemma 4, the update of the perturbation with the MI attack at the step t is
given as follows.
△xmni = α I + JjH(x) + Rf (H(x)) g(x) + RR1.	(35)
where Rf (H (x)) denotes terms of elements in H (x) of higher than the first order. R denotes terms
with elements in δmt-i 1 of higher than the first order.
To simplify the notation without causing ambiguity, we write g(X) and H(X) as g and H, respec-
tively. In this way, according to Equation (33) and Equation (35), δmmi can be written as follows.
(	1) m	m
δmm = α ml + —(I~)H + X R1(H) g + X R1.	(36)
t=f	t=f
where m represents the total number of steps. According to Lemma 1, the Shapley interaction
between perturbation units a, b in δmmi is given as follows.
, . ___ ~ , .
Iab(δmm) = δmi,aHabδm,b + 冗2局)，
where R2(δmmi) denotes terms with elements in δmi of higher than the second order.
(37)
29
Published as a conference paper at ICLR 2021
According to Equation (36) and Equation (37), we get
Iab (δmi) = Hab [αmga +
α* 2 m(m - 1)
4
nm
X (Hab，gb0 ) + …+ X
b0=1	t=1
o(δmti,a)	][
、V}
terms of δmt i,a
of higher than the first order,
which corresponds to the term of
R1 in Equation (36)
αmgb +
α2m(m - 1)
4
nm
X (Ha，bga，) + ∙→ X
a，=1	t=1
o(δm i,b)	]+R⅛(δm)
∙~}}
terms of δmt i,b
of higher than the first order,
which corresponds to the term of
R1 in Equation (36)
α2m2gagbHab
X-------{-------}
first-order terms w.r.t. elements in H
α3(m - 1)m2	n	α3(m - 1)m2	n
+	------4-------gb T(HabOgbO) +-----------4------ga ʌ, (Ha0bga0 )
b，=1	a，=1
'{
second-order terms w.r.t. elements in H
α4(m - 1)2m2
16
+
nn
(HabOgbO)	(HaObgaO)Hab+...
bO=1	aO=1
R2mi(H)
m
+ [∑ o(δmi ,a)]Habδm,b+[o(δmi ,b)]Habδm,a+显3@
t=1
V-----------------------{------------------------}
〜一.
R2(δm)
2 2	α3(m - 1)m2	n
ɑ m gagbHab +------4-----gaHab 上(HaObgaO )
4	aO=1
+ α3(m -1)m2 gbHabX (HabO gbo) + R (δmm)+Rmi(H),
bO=1
Hab
/
(38)
where R2mi(H) denotes terms of elements in H higher than the second order, and R02(δmmi) denotes
terms of elements in δmmi higher than the second order
According to Equation (22) and Equation (38), the expectation of the difference between Iab(δmmi)
and Iab(δmmulti) is given as follows.
Ea,b [Iab(δmmi) - Iab(δmmulti)]
α3(m	1)m2	n	n
=(4 )	Ea,b[ ga Hab X (HaObga，)+ gbHab X (Hab gb )] + Ea,b Mb ],
aO=1	bO=1
、---------V---------}	'---------V--------}
Uab	Uba
where
Rmb = R2(δmm) - R2(δ 输J+Rmi(H) - RmUlti (H).
According to Assumption 1, we have R2mi(H) ≈ 0, and R2multi(H) ≈ 0. Note that the magnitude
of δm and the magnitude of 6得用 are small, then R2(δm) ≈ 0, and R2(δmU用)≈ 0. Therefore,
Ea,b [Rmb] = Ea,b[R2(δmm) - R2(δmultJ + Rmi(H) - RmUlti(H)] ≈ 0. Moreover, similar to Equa-
tion (24) in the proof of Proposition 1, we have Ea,b [Uab] = Ea,b [Uba] ≥ 0.
30
Published as a conference paper at ICLR 2021
Therefore,
Ea,b [Iab(δmi) - Iab(δmulti)]
=-a3(m —Dm2 Ea,b [Uab + Uba ] + Ea,b [R：S ]
≈-α3(m - 1)m2 Ea,b [Uab ]+0 ≤ 0.
2
□
Note that Proposition 3 just shows the revised MI Attack usually decreases the interaction between
perturbation units. The proof towards all types of MI Attacks is still a challenge.
J	Implementation of the Interaction-Reduced Attack (IR Attack)
J.1	Grid-level interactions for image data
Although the computation of Ei,j [Iij (δ)] can be simplified using Equation (26), the computational
cost of Ei,j [Iij (δ)] is still high. Therefore, as Figure 6 shows, using the local property of images
(Chen et al., 2018a), we can divide the entire image into L × L grids, and compute interactions at
the grid level, instead of the pixel level. Let Λ = {Λ11, Λ12, . . . , ΛLL} denote the set of grids. We
use (p, q ) to denote the coordinate of a grid. In this way, the expectation of interactions between
perturbation grids is given as follows.
E(p,q),(p0,q0) [(PM。，q,) (δ)] = L^E(p,q) MS) - V(A \ {Λpq}) - v({Λpq}) + V(0)] ,(39)
Figure 6: For the input image, We Can divide the image into grids, and compute interactions at the
grid level.
J.2 SCALABILITY OF THE INTERACTION LOSS
In this section, we discuss about two kinds of scalability of the interaction loss.
•	Is the computational cost of the interaction loss affordable when the number of players is
large? To this end, we have proved in Equation (4) that the computational complexity of the expec-
tation of the interaction is linear, which is scalable. In fact, we do not directly compute interaction
using Equation (3). Instead, we compute the expectation of interactions with Equation (4). The
computational cost of the IR Attack can be further reduced by calculating the grid-level interactions
of images.
We further conducted experiments to measure the time cost of generating perturbations using the
IR Attack. We conducted the IR attack for 100 steps on the ImageNet dataset. The time cost was
measured using PyTorch 1.6 (Paszke et al., 2019) on Ubuntu 18.04, with the Intel(R) Core(TM)
i7-9800X CPU @ 3.80GHZ and a Titan RTX GPU. Table 6 shows the average computational cost
of generating adversarial perturbations on an input image with size 224 × 224 by the IR Attack for
100 steps. It shows that the IR Attack is computationally applicable to high-dimensional data and
deep neural networks.
31
Published as a conference paper at ICLR 2021
Table 6: Average computational cost of generating adversarial perturbations over an input image by
the IR Attack for 100 steps on different source DNNs.
	RN-34	RN-152	DN-121	DN-201
Time (seconds)	12.882	48.774	27.519	44.812
•	Is the computation cost of the interaction loss affordable when we consider the continuous
space of adversarial perturbations? It has been widely discussed (Ancona et al., 2019; Sundarara-
jan & Najmi, 2019) that when applying the Shapley value, the feature space is regarded as binary. It
is because as (Sundararajan & Najmi, 2019) shows that although there exist the Shapley-value-like
attribution in a continuous space, only the Shapley value in the binary space is the unique attribution
that satisfies the linearity axiom, the dummy axiom, the symmetry axiom, and the efficiency axiom
that only in the binary space. Thus, when we compute the interaction, the perturbation can be re-
garded in the binary space, i.e., whether the perturbation unit is added to the input or not, which
enables scalability.
K Evaluation of the transferability via leave-one-out
VALIDATION
As Figure 7 shows, the highest transferability of the MI Attack is achieved in an intermediate step,
rather than in the last step. This phenomenon presents a challenge for fair comparisons of the
transferability between different attacking methods.
To this end, in order to enable fair comparisons of transferability between different methods, we
estimate the adversarial perturbations with the highest transferability for each input image via the
leave-one-out (LOO) validation as follows. Given a set of clean examples {(xi, yi)}iN=1, where
yi ∈ {1, 2, . . . , C}, we use xit to denote the adversarial example at step t w.r.t. the clean ex-
ample xi, where t ∈ {1, 2, . . . , T }, and T is the number of total step. Given a target DNN
h(∙) and an input example x, where h(∙) denotes the output before the Softmax layer, We use
C(X) = argmaxk hk (x), k ∈ {1,...,C} to denote the prediction of the example x.
-def t*
Xi = Xii, s.t. ti = arg max Ei0∈{i,2,...,N}∖{} [l[C(xto) = yi，]],
where IH is the indicator function. Then the average transferability is given as follows.
Transferability = Ei [1[C(Xi) = y[].
Figure 7: The curve of transferability in different steps.
L	Additional Related Work
Some studies paid attention to intermediate features to improve transferability. Activation At-
tack (inkawhich et al., 2019) forced the intermediate features of the input image to be similar with
the intermediate features of a target image, in order to generate highly transferable targeted example.
Distribution Attack (inkawhich et al., 2020) explicitly modeled the feature distribution of each class,
and improve the targeted transferability by driving the feature of perturbed input image into the dis-
tribution of a specific target class. intermediate Level Attack (Huang et al., 2019) improved the
32
Published as a conference paper at ICLR 2021
transferability of an adversarial example by maximizing the feature perturbation of a pre-specified
layer. In comparison, we explain and improve the transferability based on game theory. Moreover,
we discover the negative correlation between the transferability and interactions.
(40)
M Additional Experiments on Interaction-Reduced Loss
M.1 Interaction Reduction on Other Attacks
To further demonstrate the effectiveness of the interaction loss, we have applied the interaction
loss on other attacks besides the PGD Attack, including the MI Attack, the SGM Attack, and the
VR Attack. More specifically, we added the interaction loss on the MI Attack (namely the MI+IR
Attack), the SGM Attack (namely the SGM+IR Attack), and the VR Attack (namely the VR+IR
Attack), respectively.
For the MI Attack and the SGM Attack, we directly applied Equation (7) to these attacks, because
these attacks were compatible with the interaction loss. Besides, for the VR Attack, its objective
function is given as follows.
maximize Eξ〜N(。口2)['(h(X + δ + ξ), y)]	s.t. ∣∣δ∣∣p≤ g X + δ ∈ [0,1]n,
δ
Therefore, the VR+IR Attack was implemented via sampling as follows.
1K
maximize K £ ['(h(x + δ + ξk),y) - Eij [Ij(δ)]], ξk 〜N (0,σ2)
k=1
s.t. kδkp≤ , X + δ ∈ [0, 1]n,
where the interaction loss was computed by considering the input image as X + ξk, rather than X
in Equation (26). The VR Attack reported in Table 4 followed the original paper (Wu et al., 2018)
to set K = 20. However, a crucial issue for applying the interaction loss to the VR attack was its
extremely high computational cost. Therefore, for the implementation of the VR+IR Attack, we set
K = 5 and reduce the number of steps from 100 to 50. Just like experiments in Table 1, we also
used the LOO strategy for evaluation.
Table 7, Table 8, and Table 9 compare the success rates of attacks with and without the interaction
loss. The results demonstrated that the performance of the MI Attack, the SGM Attack, and the VR
Attack can be further enhanced by directly adding the interaction loss to reduce interactions inside
perturbations.
Table 7: The success rates of L∞ black-box attacks crafted by MI and MI+IR on four source
models (RN-34/152, DN-121/201) against seven target models. The interaction loss can boost the
transferability of MI.
(41)
Source	Method	VGG-16 RN152	DN-201	SE-154 IncV3 IncV4 IncResV2
RNr 2/	MI	80.1±0.5 73.0±2.3 77.7±0.5 48.9±0.8 46.2±1.2 39.9±0.5 34.8±2.5
RN-34	MI+IR	90.0±0.5 85.7±0.3 88.5±0.6 67.0±0.1 66.9±1.8 60.2±0.7 53.9±2.3
RNr 1 V。 RN-152	MI	70.3±0.6	-	74.8±1.4 51.7±0.8 47.1±0.9 40.5±1.6 36.8±2.7
	MI+IR	78.9±1.4	-	82.2±2.0 68.3±0.3 63.6±1.2 59.0±0.4 56.3±1.0
ΓΛNT 1 ɔl	MI	83.0±4.9 72.0±0.7 91.5±0.2 58.4±2.6 54.6±1.6 49.2±2.4 43.9±1.5
DN-121	MI+IR	89.0±0.8 83.2±1.5 93.4±0.6 74.2±0.7 69.6±0.9 64.7±0.5 58.2±2.3
ΓΛNT Ω∩1	MI	77.3±0.8 74.8±1.4	-	64.6±1.0 56.5±2.5 51.1±2.1 47.8±1.9
DN-201	MI+IR	87.3±0.3 81.6±2.0	―	75.4±0.6 66.6±3.3 60.0±1.0 62.1±0.7
M.2 Attacks on the CIFAR- 1 0 dataset
In Table 1, we have shown that reduction of interactions could improve the adversarial transfer-
ability on the ImageNet dataset (Russakovsky et al., 2015). To further demonstrate the broad ap-
plicability of such a negative correlation, we also conducted the targeted attack on the CIFAR-10
dataset (Krizhevsky & Hinton, 2009) to test the transferability of perturbations generated with the
interaction loss.
33
Published as a conference paper at ICLR 2021
Table 8: The success rates of L∞ black-box attacks crafted by SGM and SGM+IR on four source
models (RN-34/152, DN-121/201) against seven target models. The interaction loss can boost the
transferability of SGM.
Source	Method	VGG-16	RN152	DN-201	SE-154	IncV3	IncV4	IncResV2
RN-34	SGM	91.8±0.6	89.0±0.9	90.0±0.4	68.0±1.4	63.9±0.3	58.2±1.1	54.6±1.2
	SGM+IR	94.7±0.6	91.7±0.6	93.4±0.8	72.7±0.4	68.9±0.9	64.1±1.3	61.3±1.0
RN-152	SGM	88.2±0.5	—	90.2±0.3	72.7±1.4	63.2±0.7	59.1±1.5	58.1±1.2
	SGM+IR	92.0±1.0	—	92.5±0.4	79.3±0.1	69.6±0.8	66.2±1.0	63.6±0.9
DN-121	SGM	88.7±0.9	88.1±1.0	98.0±0.4	78.0±0.9	64.7±2.5	65.4±2.3	59.7±1.7
	SGM+IR	91.7±0.2	90.4±0.4	94.3±0.1	87.0±0.4	78.8±1.3	79.5±0.2	75.8±2.7
DN-201	SGM	87.3±0.3	92.4±1.0	一	82.9±0.2	72.3±0.3	71.3±0.6	68.8±0.5
	SGM+IR	89.5±0.9	91.8±0.7	一	87.3±1.2	82.5±0.8	80.3±0.3	81.5±0.5
Table 9: The success rates of L∞ black-box attacks crafted by VR and VR+IR on four source
models (RN-34/152, DN-121/201) against seven target models. The interaction loss can boost the
transferability of VR.
Source	Method	VGG-16	RN152	DN-201	SE-154	IncV3	IncV4	IncResV2
RN-34	VR	85.1 -	85.3	87.0	55.7	54.3	50.7	43.7
	VR+IR	90.8	92.2	93.3	75.4	75.4	67.5	66.1
DN-121	VR	88.8-	88.4	98.2	72.9	73.5	72.5	63.6
	VR+IR	93.0	93.5	96.2	83.7	82.8	84.0	79.8
Following Wu et al. (2018), we chose three DNNs as the source DNN or the target DNN, which
included: LeNet (LeCun et al., 1998), RN-20 (He et al., 2016), and DN-121 (Huang et al., 2017).
We conducted the targeted attack under the L∞ norm constraint, and chose he plane class as the
target category. The norm constraint was set to 16/255, and the step size was set to 2/255. The
transferability was computed based on the best adversarial perturbation during 50 steps via the leave-
one-out (LOO) validation., which has been introduced in Appendix K.
As Table 10 shows, the transferability could be enhanced by reducing interactions on the targeted
attack on the CIFAR-10 dataset. Particularly, when the source DNN is RN-20 and the target DNN
is DN-121, the transferability improvement was about 30%, which was a considerable gain.
Table 10: The success rates of L∞ targeted black-box attacks on three source models, including
LeNet, RN-20, DN-121, against three target models._________________
Source	Method	LeNet	RN-20	DN-121
LeNet	PGD	—	34.1±0.1	19.6±0.4
	PGD+IR	—	44.2±0.3	29.7±1.1
RN-20	PGD	10.8±0.8	—	41.9±1.3
	PGD+IR	19.7±0.3	—	71.8±1.0
DN-121	PGD	10.0±0.7	44.2±0.3	—
	PGD+IR	18.9±0.7	58.5±1.0	—
Table 11: The average interaction inside adversarial perturbations generated by PGD, DI and TL
	MethOd		RN-34	DN-121
Baseline (PGD Attack)	0.422	0.926
DI Attack	0.241	0.499
TI Attack	0.379	0.618
N Empirical Verification of Other Transferability-boosting
Attacks
We have theoretically analyzed the MI Attack, the VR Attack, and the SGM Attack. However,
for other methods of improving adversarial transferability, such as Diversity Input (DI) (Xie et al.,
34
Published as a conference paper at ICLR 2021
2019), which uses random data augmentation during attacking, it is difficult to mathematically prove
that they essentially reduce interactions. Nevertheless, as Table 11 shows, we empirically demon-
strated that two widely-used transferability-boosting attacks, DI and TI (Dong et al., 2019), also
reduced interactions.
O	Additional Experiments on Effects of the Interaction Loss
We conducted additional experiments to test the effects of the interaction loss. We conducted attacks
on two source DNNs (RN-34, DN-121), and transferred adversarial perturbations to seven target
DNNs (VGG16, RN-152, DN-201, SE-154, IncV3, IncV4, IncResV2).
We used the following two experimental settings to compare the transferability of adversarial per-
turbations generated with different λ values.
First, we re-drew the curves in Figure 3(a) by extending the λ from the range of [0, 1.2] to the range
of [0, 2.0], in order to show the performance of different λ values. We simply changed the λ value in
the objective function (i.e. Equation (5)) without any other revisions. This was the most direct way
to test the effects of λ. Experimental results are shown in Figure 8.
Besides above experimental settings, we also compared adversarial perturbations generated with
different λ values, when we controlled each perturbation to have the same attacking utility. The
attacking utility was defined as follows.
Attacking U tility = max hy0 (x + δ) - hy(x + δ)
y0 6=y
where y denote the label of the input image x. This setting also ensured the fairness of comparisons
from a new perspective. Please see Figure 9 for experimental results.
In sum, under both experimental settings, We found that the large λ value usually yielded a high
adversarial transferability in our experiments.
Source: DN-121
05
L
A-_qB」①JSU
——VGG16
RN152
—DN201
—SE154
—lncV3
——lncV4
lncResV2
0.4	0.8	1.2	1.6	2.0
λ
Figure 8:	The success rates ofblack-box attacks with the IR Attack using different values of λ under
the first experimental setting.
Source: RN-34
63°
玄一qe」-sue」l
Source: DN-121
Figure 9:	The success rates ofblack-box attacks with the IR Attack using different values of λ under
the second experimental setting.
35