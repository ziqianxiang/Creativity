Published as a conference paper at ICLR 2021
Improving Transformation Invariance in
Contrastive Representation Learning
Adam Foster*, Rattana Pukdee* & Tom Rainforth
Department of Statistics
University of Oxford
{adam.foster,rainforth}@stats.ox.ac.uk
Ab stract
We propose methods to strengthen the invariance properties of representations
obtained by contrastive learning. While existing approaches implicitly induce a
degree of invariance as representations are learned, we look to more directly en-
force invariance in the encoding process. To this end, we first introduce a train-
ing objective for contrastive learning that uses a novel regularizer to control how
the representation changes under transformation. We show that representations
trained with this objective perform better on downstream tasks and are more ro-
bust to the introduction of nuisance transformations at test time. Second, we pro-
pose a change to how test time representations are generated by introducing a
feature averaging approach that combines encodings from multiple transforma-
tions of the original input, finding that this leads to across the board performance
gains. Finally, we introduce the novel Spirograph dataset to explore our ideas in
the context of a differentiable generative process with multiple downstream tasks,
showing that our techniques for learning invariance are highly beneficial.
1	Introduction
Learning meaningful representations of data is a central endeavour in artificial intelligence. Such
representations should retain important information about the original input whilst using fewer bits
to store it (van der Maaten et al., 2009; Gregor et al., 2016). Semantically meaningful representations
may discard a great deal of information about the input, whilst capturing what is relevant. Knowing
what to discard, as well as what to keep, is key to obtaining powerful representations.
By defining transformations that are believed a priori to distort the original without altering semantic
features of interest, we can learn representations that are (approximately) invariant to these transfor-
mations (Hadsell et al., 2006). Such representations may be more efficient and more generalizable
than lossless encodings. Whilst less effective for reconstruction, these representations are useful in
many downstream tasks that relate only to the semantic features of the input. Representation invari-
ance is also a critically important task in of itself: it can lead to improved robustness and remove
noise (Du et al., 2020), afford fairness in downstream predictions (Jaiswal et al., 2020), and enhance
interpretability (Xu et al., 2018).
Contrastive learning is a recent and highly successful self-supervized approach to representation
learning that has achieved state-of-the-art performance in tasks that rely on semantic features, rather
than exact reconstruction (van den Oord et al., 2018; Hjelm et al., 2018; Bachman et al., 2019; He
et al., 2019). These methods learn to match two different transformations of the same object in
representation space, distinguishing them from contrasts that are representations of other objects.
The objective functions used for contrastive learning encourage representations to remain similar
under transformation, whilst simultaneously requiring different inputs to be well spread out in rep-
resentation space (Wang & Isola, 2020). As such, the choice of transformations is key to their
success (Chen et al., 2020a). Typical choices include random cropping and colour distortion.
However, representations are compared using a similarity function that can be maximized even for
representations that are far apart, meaning that the invariance learned is relatively weak. Unfor-
* Equal contribution
1
Published as a conference paper at ICLR 2021
tunately, directly changing the similarity measure hampers the algorithm (Wu et al., 2018; Chen
et al., 2020a). We therefore investigate methods to improve contrastive representations by explic-
itly encouraging stronger invariance to the set of transformations, without changing the core self-
supervized objective; we look to extract more information about how representations are changing
with respect to transformation, and use this to direct the encoder towards greater invariance.
To this end, we first develop a gradient regularization term that, when included in the training loss,
forces the encoder to learn a representation function that varies slowly with continuous transforma-
tions. This can be seen as constraining the encoder to be approximately transformation invariant.
We demonstrate empirically that while the parameters of the transformation can be recovered from
standard contrastive learning representations using just linear regression, this is no longer the case
when our regularization is used. Moreover, our representations perform better on downstream tasks
and are robust to the introduction of nuisance transformations at test time.
Test representations are conventionally produced using untransformed inputs (Hjelm et al., 2018;
Kolesnikov et al., 2019), but this fails to combine information from different transformations and
views of the object, orto emulate settings in which transformation noise cannot simply be removed at
test time. Our second key proposal is to instead create test time representations by feature averaging
over multiple, differently transformed, inputs to address these concerns and to more directly impose
invariance. We show theoretically that this leads to improved performance under linear evaluation
protocols, further confirming this result empirically.
We evaluate our approaches first on CIFAR-10 and CIFAR-100 (Krizhevsky et al., 2009), using
transformations appropriate to natural images and evaluating on a downstream classification task.
To validate that our ideas transfer to other settings, and to use our gradient regularizer within a fully
differentiable generative process, we further introduce a new synthetic dataset called Spirograph.
This provides a greater variety of downstream regression tasks, and allows us to explore the interplay
between nuisance transformations and generative factors of interest. We confirm that using our
regularizer during training and our feature averaging at test time both improve performance in terms
of transformation invariance, downstream tasks, and robustness to train-test distributional shift.
In summary, the contributions of this paper are as follows:
•	We derive a novel contrastive learning objective that leads to more invariant representations.
•	We propose test time feature averaging to enforce further invariance.
•	We introduce the Spirograph dataset.
•	We show empirically that our approaches lead to more invariant representations and achieve
state-of-the-art performance for existing downstream task benchmarks.
2	Probabilistic formulation of contrastive learning
The goal of unsupervized representation learning is to encode high-dimensional data, such as im-
ages, retaining information that may be pertinent to downstream tasks and discarding information
that is not. To formalize this, we consider a data distribution p(x) on X and an encoder fθ : X → Z
which is a parametrized function mapping from data space to representation space.
Contrastive learning is a self-supervized approach to representation learning that learns to make rep-
resentations of differently transformed versions of the same input more similar than representations
of other inputs. Of central importance is the set of transformations, also called augmentations (Chen
et al., 2020a) or views (Tian et al., 2019), used to distort the data input x. In the common appli-
cation of computer vision, it is typical to include resized cropping; brightness, contrast, saturation
and hue distortion; greyscale conversion; and horizontal flipping. We will later introduce the Spiro-
graph dataset which uses quite different transformations. In general, transformations are assumed
to change the input only cosmetically, so all semantic features such as the class label are preserved;
the set of transformations indicates changes which can be safely ignored by the encoder.
Formally, we consider a transformation set T ⊆ {t : X → X } and a probability distribution p(t)
on this set. A representation z of x is obtained by applying a random transformation t to x and
then encoding the result using fθ. Therefore, we do not have one representation ofx, but an implicit
distribution p(z∣x). A sample of p(z∣x) is obtained by sampling t 〜p(t) and setting Z = fθ(t(x)).
2
Published as a conference paper at ICLR 2021
If the encoder is to discard irrelevant information, we would expect different encodings of x formed
with different transformations t to be close in representation space. Altering the transformation
should not lead to big changes in the representations of the same input. In other words, the distri-
bution p(z|x) should place most probability mass in a small region. However, this does not provide
a sufficient training signal for the encoder fθ as it fails to penalize trivial solutions in which all x
are mapped to the same z. To preserve meaningful information about the input x whilst discarding
purely cosmetic features, we should require p(z|x) to be focused around a single z whilst simul-
taneously requiring the representations of different inputs not to be close. That is, the marginal
p(z) = Ep(x) [p(z|x)] should distribute probability mass over representation space.
This intuition is directly reflected in contrastive learning. Most state-of-the-art contrastive learning
methods utilize the InfoNCE objective (van den Oord et al., 2018), or close variants of it (Chen
et al., 2020a). InfoNCE uses a batch x1, ..., xK of inputs, from which we form pairs of representa-
tions (z1, z01), ..., (zK, z0K) by applying two random transformations to each input followed by the
encoder fθ . In probabilistic language
Xi 〜P(X) for	i =	1,…,K	(1)
Zi, Zi 〜p(z∣x =	Xi)	conditionally independently given	Xi,	for i = 1,…，K, (2)
such that Zi, Zi = fθ(t(x)), fθ(t0(x)) for i.i.d. transformations t,t0 〜 p(t). Given a learnable
similarity score sφ : Z × Z → R, contrastive learning methods minimize the following loss
1 K	ι K / K	∖
LGo) = -KE s。(Zi, Zi) + K Elog EeXp Is。(Zi, Zj)].	⑶
K i=1	K i=1 j=1
Written in this way, we see that the loss will be minimized when sφ(Zi, Zi) is large, but s。②，Zj)
is small for i 6= j. In other words, InfoNCE makes the two samples Zi, Z0i of p(Z|X = Xi) similar,
whilst making samples Zi, Z0j of p(Z) dissimilar. This can also be understood through the lens of
mutual information, for more details see Appendix A.
In practice, the similarity measure used generally takes the form (Chen et al., 2020a)
Sφ(Z,Z0)
gφ(Z)›gφ(Z0)
T kgφ(Z)gkg。3)^
(4)
where g。 is a small neural network and τ is a temperature hyperparameter. If the encoder fθ is
perfectly invariant to the transformations, then Zi = Zi and sφ(Zi, Zi) will be maximal. However,
there are many ways to maximize the InfoNCE objective without encouraging strong invariance in
the encoder.1 In this paper, we show how we can learn stronger invariances, above and beyond what
is learned through the above approach, and that this benefits downstream task performance.
3 Invariance by gradient regularization
Contrastive learning with InfoNCE can gently encourage invariance by maximizing s。 (Z, Z0), but
does not provide a strong signal to ensure this invariance. Our first core contribution is to show
how we can use gradient methods to directly regulate how the representation changes with the trans-
formation and thus ensure the desired invariance. The key underlying idea is to differentiate the
representation with respect to the transformation, and then encourage this gradient to be small
so that the representation changes slowly as the transformation is varied.
To formalize this, we begin by looking more closely at the transformations T which are used to de-
fine the distribution p(Z|X). Many transformations, such as brightness adjustment, are controlled
by a transformation parameter. We can include these parameters in our set-up by writing the
transformation t as a map from both input space X and transformation parameter space U , i.e.
t : X ×U → X. In this formulation, we sample a random transformation parameter from U 〜P(U)
which is a distribution on U. A sample from p(Z|X) is then obtained by taking Z = fθ(t(X, u)), with
t now regarded as a fixed function.
1This is because the function gφ is not an injection, so we may have gφ (z) = gφ (z0) but z 6= z0. Johnson
& Lindenstrauss (1984) gives conditions under which a projection of this form will preserve approximate
distances, in particular, the required projection dimension is much larger than the typical value 128.
3
Published as a conference paper at ICLR 2021
The advantage of this change of perspective is that it opens up additional ways to learn stronger
invariance of the encoder. In particular, it may make sense to consider the gradient Vuz, which
describes the rate of change ofz with respect to the transformation. This only makes sense for some
transformation parameters—we can differentiate with respect to the brightness scaling but not with
respect to a horizontal flip.
To separate out differentiable and non-differentiable parameters we write u = α, β where α are
the parameters for which it makes sense to consider the derivative Vαz. Intuitively, this gradient
should be small to ensure that representations change only slowly as the transformation parameter α
is varied. For clarity of exposition, and for implementation practicalities, it is important to consider
gradients of a scalar function, so we introduce an arbitrary direction vector e ∈ Z and define
F(α, β, x, e) = e ∙
fθ(t(x, α, β))
kfθ(t(x, α, β))k2
(5)
so that F : A × B × X × Z → R calculates the scalar projection of the normalized representation
z/kzk2 in the e direction. To encourage an encoder that is invariant to changes in α, we would like
to minimize the expected conditional variance of F with respect to α:
V = Ep(x)p(β)p(e) Varp(α) [F (α, β, x, e) | x, β, e] ,	(6)
where we have exploited independence to write p(x, β, e) = p(x)p(β)p(e). Defining V requires
a distribution for e to be specified. For this, we make components of e independent Rademacher
random variables, justification for which is included in Appendix B.
A naive estimator of V can be formed using a direct nested Monte Carlo estimator (Rainforth et al.,
2018) of sample variances, which, including Bessel’s correction, is given by
1K 1 L	2	1
V≈ K∑ I L-I∑f(αij,βi,xi,ei) - L(L- 1)
L
F (αik , βi, xi , ei )
k=1
2	(7)
where Xi, βi, ei 〜P(X)P(β)p(e) and αj 〜p(α). However, this estimator requires LK forward
passes through the encoder fθ to evaluate. As an alternative to this computationally prohibitive
approach, we consider a first-order approximation2 to F
F(α0, β, x, e) - F(α, β, x, e) = VaF(α, β, x, e) ∙ (α0 - α) + o(∣∣α0 - α∣∣)	(8)
and the following alternative form for the conditional variance (see Appendix B for a derivation)
Varp(a) [F(α, β, x, e)	| x, β, e]	=	2Ep(a)p3)	[(F(α,	β, x,	e)	- F(α0, β, x, e))2 | x, β,	e]	(9)
Combining these two ideas, we have
V = Ep(X)P(β)p(e) [1 Ep(a)p(a0) [(F(α, β, x, e) - F(α0, β, x, e))2 | x, β, e]]	(10)
≈ Ep(x)p(β)p(e)
[2Ep(a)p(a0) [(VaF(α, β, x, e) ∙ (α0 - α))2 | x, β, e]].
(11)
Here we have an approximation of the conditional variance V that uses gradient information. In-
cluding this as a regularizer within contrastive learning will encourage the encoder to reduce the
magnitude of the conditional variance V, forcing the representation to change slowly as the trans-
formation is varied and thus inducing approximate invariance to the transformations.
An unbiased estimator of equation 11 using a batch x1, ..., xK is
Vregularizer = K X I 2L X [VaF(αi, βi, xi, ei) ∙ (αij - αi)]
(12)
i=1	j=1
where xi, αi, βi, e^ 〜 p(x)p(α)p(β)p(e), αj 〜 p(α). We can cheaply use a large number of
samples for α0 without having to take any additional forward passes through the encoder: we only
require K evaluations of F. Our final loss function is
1K	1K	K
L(θ,φ) = - κ∑^sΦ(zi,Zi) + κ ∑Slog IΣ eχp [sφ (zi,zj)]
λKL
+ Lκ∑ 斗 VaF (αi, βi,xi, ei) ∙ (αij - ai)]2
(13)
2We use the notation a(x) = o(b(x)) to mean a(x)/b(x) → 0 as x → ∞.
4
Published as a conference paper at ICLR 2021
where λ is a hyperparameter controlling the regularization strength. This loss does not require
us to encode a larger number of differently transformed inputs. Instead, it uses the gradient at
(x, α, β , e) to control properties of the encoder in a neighbourhood of α. This can effectively reduce
the representation gradient along the directions corresponding to many different transformations.
This, in turn, creates an encoder that is approximately invariant to the transformations.
4	B etter test time representations with feature averaging
At test time, standard practice (Hjelm et al., 2018; Kolesnikov et al., 2019) dictates that test repre-
sentations be produced by applying the encoder to untransformed inputs (possibly using a central
crop). It may be beneficial, however, to aggregate information from differently transformed versions
of inputs to enforce invariance more directly, particularly when our previously introduced gradient
regularization can only be applied to a subset of the transformation parameters. Furthermore, in
real-world applications, it may not be possible to remove nuisance transformations at test time or, as
in our Spirograph dataset, there may not be only one unique ‘untransformed’ version of x.
To this end, we propose combining representations from different transformations using feature
averaging. This approach, akin to ensembling, does not directly use one encoding from the network
fθ as a representation for an input x. Instead, We sample transformation parameters αι,..., αM 〜
p(α), βι,..., Bm 〜p(β) independently, and average the encodings of these differently transformed
versions ofx to give a single feature averaged representation
1M
Z(M)(X) = M ffθ(t(x, αm, βm)).	(14)
m=1
Using Z(M) aggregates information about x by averaging over a range of possible transformations,
thereby directly encouraging invariance. Indeed, the resulting representation has loWer conditional
variance than the single-sample alternative, since
Varp(ai：M )p(βiM) he ∙ Z(M)(x)|x, ei = MrVarp(α1)p(β1) he ∙ Z(I)(X) R e] .	(15)
Further, unlike gradient regularization, this approach takes account of all transformations, including
those which we cannot differentiate with respect to (e.g. left-right flip). It therefore forms a natural
test time counterpart to our training methodology to promote invariance.
We do not recommend using feature averaged representations during training. During training, we
need a training signal to recognize similar and dissimilar representations, and feature averaging will
weaken this training signal. Furthermore, the computational cost of additional encoder passes is
modest when used once at test time, but more significant when used at every training iteration.
As a test time tool though, feature averaging is powerful. In Theorem 1 below, we show that for
certain downstream tasks the feature averaged representation will always perform better than the
single-sample transformed alternative. The proof is presented in Appendix C.
Theorem 1. Consider evaluation on a downstream task by fitting a linear classification model with
softmax loss or a linear regression model with square error loss on with representations as features.
For a fixed classifier or regressor and M0 ≥ M we have
Ep(x,y)p(aiM0)p(βiM0) [' (z(M ),y)] ≤ Ep(x,y)p(aiM)p(βiM)[' (2加),y) ] .	(16)
Empirically we find that, using the same encoder and the same linear classification model, feature
averaging can outperform evaluation using untransformed inputs. That is, even when it is possible
to remove the transformations at test time, it is beneficial to retain them and use feature averaging.
5	Related work
Contrastive learning (van den Oord et al., 2018; Henaff et al., 2019) has progressively refined the
role of transformations in learning representations, with Bachman et al. (2019) applying repeated
data augmentation and Tian et al. (2019) using Lab colour decomposition to define powerful self-
supervized tasks. The range of transformations has progressively increased (Chen et al., 2020a;b),
5
Published as a conference paper at ICLR 2021
Figure 1: Samples from the spirograph dataset. Two sets of four images (left and right): each set
shows different transformations applied to the same generative factors of interest.
whilst changing transformations can markedly improve performance (Chen et al., 2020c). Recent
work has attempted to further understand and refine the role of transformations (Tian et al., 2020).
The idea of differentiating with respect to transformation parameters dates back to the tangent prop-
agation algorithm (Simard et al., 1998; Rifai et al., 2011). Using the notation of this paper, tangent
propagation penalizes the norm of the gradient of a neural network evaluated at α = 0, encourag-
ing local transformation invariance near the original input. In our work, we target the conditional
variance (Equation 6), leading to gradient evaluations across the α parameter space with random
α 〜p(α) and a regularizer that is not a gradient norm (Equation 12).
Our gradient regularization approach also connects to work on gradient regularization for Lipschitz
constraints. A small Lipschitz constant has been shown to lead to better generalization (Sokolic
et al., 2017) and improved adversarial robustness (Cisse et al., 2017; Tsuzuku et al., 2018; Barrett
et al., 2021). Previous work focuses on constraining the mapping x 7→ z to have a small Lipschitz
constant which is beneficial for adversarial robustness. In our work we focus on the influence of α
on z, which gives rise to transformation robustness. Appendix D provides a more comprehensive
discussion of related work.
6	Experiments
6.1	Datasets and set-up
The methods proposed in this paper learn representations that discard some information, whilst re-
taining what is relevant. To more deeply explore this idea, we construct a dataset from a generative
process controlled by both generative factors of interest and nuisance transformations. Represen-
tations should be able to recover the factors of interest, whilst being approximately invariant to
transformation. To aid direct evaluation of this, we introduce a new dataset, which we refer to as the
Spirograph dataset. Its samples are created using four generative factors and six nuisance transfor-
mation parameters. Figure 1 shows two sets of four samples with the generative factors fixed in each
set. Every Spirograph sample is based on a hypotrochoid—one of a parametric family of curves that
describe the path traced out by a point on one circle rolling around inside another. This generative
process is fully differentiable in the parameters, meaning that our gradient regularization can be ap-
plied to every transformation. We define four downstream tasks for this dataset, each corresponding
to the recovery of one of the four generative factors of interest using linear regression. The final
dataset consists of 100k training and 20k test images of size 32 × 32. For full details of this dataset,
see Appendix E.
As well as the Spirograph dataset, we apply our ideas to CIFAR-10 and CIFAR-100 (Krizhevsky
et al., 2009). We base our contrastive learning set-up on SimCLR (Chen et al., 2020a). To use
our gradient regularization, we adapt colour distortion (brightness, contrast, saturation and hue ad-
justment) as a fully differentiable transformation giving a four dimensional α; we also included
random cropping and flipping but we did not apply a gradient regularization to these. We used
ResNet50 (He et al., 2016) encoders for CIFAR and ResNet18 for Spirograph, and regularization
parameters λ = 0.1 for CIFAR and λ = 0.01 for Spirograph. For comprehensive details of our
set-up and additional plots, see Appendix F. For an open source implementation of our methods, see
https://github.com/ae-foster/invclr.
6.2	Gradient regularization leads to strongly invariant representations
We first show that our gradient penalty successfuly learns representations that are more invariant
to transformation than standard contrastive learning. First, we estimate the conditional variance
6
Published as a conference paper at ICLR 2021
(a) CIFAR-10
No regularization
Gradient
regularization
Proportion of train data
(b) CIFAR-100
(c) Spirograph
Figure 3: Downstream task performance of gradient regularized representations. (a)(b) Top-1 test
accuracy for various levels of semi-supervision (higher better). (c) Test loss on four downstream
regression tasks on Spirograph that recover the generative factors of interest (lower better). The loss
is rescaled for legibility, see Table 9 for raw values. Error bars are ±1 standard error from 3 runs.
of the representation that was used as the starting
point for motivating our approach, i.e. Equation 6,
using the slower, but more exact, nested Monte Carlo
estimator of Equation 7 to evaluate this. In Figure 2
we see that the gradient penalty strikingly reduces
the conditional variance on CIFAR-10 compared to
standard contrastive learning.
As an additional measure of representation invari-
ance, we fit a linear regression model that predicts
α from z, for which higher loss indicates a greater
degree of invariance. We also compute a reference
loss: the loss that would be obtained when predict-
ing α using only a constant. In Table 1, we see
that unlike standard contrastive learning, after train-
ing with gradient regularization the linear regression
model cannot predict α from z any better than us-
ing a constant prediction. The loss is actually higher
than the reference value because the former is ob-
tained by training a regressor for a finite number of
steps, whilst the latter is a theoretical optimum value.
Similar results for other datasets are in Appendix F.
Figure 2: Conditional variance for CIFAR-
10 as per Equation 6. Error bars represent
±1 standard error from 3 runs.
Table 1: Test loss when linear regression
is used to predict α from z on CIFAR-10.
The reference value is Meani Var(αi). We
present the mean and ± 1 s.e. from 3 runs.
No regularization
Regularization
Reference value
Test loss
0.0353 ± 0.0002
0.0415 ± 0.00006
0.0408
6.3	Gradient regularization for downstream tasks and test time dataset shift
We now show that these more invariant representations perform better on downstream tasks. For
CIFAR, we produce representations for each element of the training and test set (by applying the
encoder fθ to untransformed inputs). We then fit a linear classifier on the training set, using different
fractions of the class labels. This allows us to assess our representations at different levels of super-
vision. We use the entire test set to evaluate each of these classifiers. In Figures 3(a) and (b), we see
that the test accuracy improves across the board with gradient regularization.
For Spirograph, we take a similar approach to evaluation: we create representations for the training
and test sets and fit linear regression models with representations as features for each of the four
downstream tasks. In Figure 3(c), we see the test loss on each task with the baseline scaled to 1.
Here we see huge improvements across all tasks, presumably due to the ability to apply gradient
regularization to all transformations (unlike for CIFAR).
We further study the effect of transformation at test time, showing that gradient penalized repre-
sentations can be more robust to shifts in the transformation distribution. For CIFAR-10, we apply
colour distortion transformations at test time with different levels of variance. By focusing on colour
distortion at test time, we isolate the transformations that the gradient regularization targetted. In
Figure 4(a) we see that when the test time distribution is shifted to have higher variance than the
training regime, our gradient penalized representations perform better than using contrastive learn-
7
Published as a conference paper at ICLR 2021
(a) CIFAR-10 var shift
(b) Spirograph mean shift
Figure 4: Assessing representation robustness to test time distribution shift. (a) Changing the vari-
ance of colour distortions; 0 is no transformation and 0.5 is the training regime. (b) Mean shifting
of the distribution of the transformation parameter h. (c) Variance shifting of the background colour
distribution. In (b)(c), 0 shift indicates the training regime. Error bars are ±1 s.e. from 3 runs.
(c) Spirograph var shift
Figure 5: The impact of feature averaging on CIFAR-10, CIFAR-100 and Spirograph. (a)(b) Test
accuracy for CIFAR-10 and CIFAR-100 respectively for various values ofM. Dashed lines represent
evaluation with untransformed inputs. (c) Test mean square error averaged over all four tasks for
Spirograph (untransformed inputs not valid here). Error bars are ±1 s.e. from 3 runs.
ing alone. For Spirograph, we investigate changing both the mean of the transformation distribution,
moving the entire test distribution away from the training regime, and increasing the variance of
transformations to add noise. Results are shown in Figure 4(b) and (c). In 4(c) in particular, we see
that gradient regularized representations are robust to a greater level of distortion at test time.
6.4	Feature averaging further improves performance
We now assess the impact of feature averaging on test time performance. For CIFAR, we apply
feature averaging using all transformations, including random crops etc., and compare with the
standard protocol of using untransformed inputs to form the test representations. Figures 5(a) and (b)
show that feature averaging leads to significant improvements. This adds to the result of Theorem 1,
which implies that test loss decreases as M is increased. In Figure 5(c), we see that feature averaging
has an equally beneficial impact on Spirograph. It is interesting to note that in both cases there is
still significant residual benefit from gradient regularization, even with a large value of M .
6.5	Our methods compare favourably with other published baselines
Our primary aim was to show that both gradient regularization and feature averaging lead to im-
provements compared to baselines that are in other respects identical. Our methods are applicable
to almost any base contrastive learning approach, and we would expect them to deliver improve-
ments across this range of different base methods. In Table 2, we present published baselines on
Table 2: Comparative best test accuracy of various self-supervized representation learning tech-
niques, evaluated using linear classification.
Method	CIFAR-10 acc.	CIFAR-100 acc.
AMDIM small (Bachman et al., 2019)	89.5%	68.1%
AMDIM large (Bachman et al., 2019)	91.2%	70.2%
SimCLR (Chen et al., 2020a)	94.0%	-
Ours (SimCLR base)	94.9%	75.1%
8
Published as a conference paper at ICLR 2021
(a) Conditional variance
(b) Downstream task performance
Figure 6: The impact of the regularization hyperparameter λ on representation learning with the
Spirograph dataset. (a) Conditional variance of Equation 6. (b) The total mean square error on all
four downstream tasks. Error bars are ±1 s.e. from 3 runs. Smaller is better in both cases giving an
optimum around λ = 10-3, but with stable performance as λ is increased above this.
Table 3: Results for representation learning on CIFAR-100 with MoCo v2 as the base contrastive
learning method, with gradient regularization in isolation and in combination with feature averaging.
We trained two different encoder architectures. We present test accuracy from linear classification
evaluation. Feature averaging uses M = 40. Errors are ±1 s.e. from multiple runs.
Method	ResNet18 acc.	ResNet50 acc.
MoCov2	52.3%	±	0.3	57.9%	± 0.1
MoCo v2 with	gradient penalty	54.1%	±	0.1	58.9%	± 0.2
MoCo v2 with	gradient	penalty and feature averaging 60.6% ±	0.1	64.4%	± 0.2
CIFAR datasets, along with the results that we obtain using our gradient regularization and feature
averaging with SimCLR as a base method. This is the default base method that we recommend,
and that was used in our previous experiments. Interestingly, the best ResNet50 encoder from our
experiments achieves an accuracy of 94.9% on CIFAR-10, which outperforms the next best pub-
lished result from the contrastive learning literature by almost 1%, and 75.1% on CIFAR-100, an
almost 5% improvement over a significantly larger encoder architecture. As such, we see our results
actually provide performance that is state-of-the-art for contrastive learning on these benchmarks.
In fact, our performance increases almost entirely close the gap to the state-of-the-art performance
for fully supervized training with the same architecture on CIFAR-10 (95.1%, Chen et al. (2020a)).
To demonstrate that our ideas generalize to other contrastive learning base methods, we apply our
ideas to MoCo v2 (Chen et al., 2020c). Table 3 shows that, whilst MoCo v2 itself does not perform
as well as SimCLR on CIFAR-100, the addition of gradient regularization and feature averaging still
leads to significant improvements in its performance. Table 3 further illustrates that both gradient
regularization and feature averaging contribute to the performance improvements offered by our
approach and that our techniques generalize across diffrent encoder architectures.
6.6	Hyperparameter sensitivity
As a further ablation study, we investigated the sensitivity of our method to changes in the gradient
regularization hyperparameter λ (as defined in Equation 13). In Figure 6(a) we see that, as expected,
the conditional variance of representations decreases as λ is increased. The downstream task per-
formance Figure 6(b) similarly improves as we increase λ, reaching an optimum around λ = 10-3,
before beginning to increase due to over-regularization. We see that a wide range of values of λ
deliver good performance and the method is not overly sensitive to careful tuning of λ.
7	Conclusion
Viewing contrastive representation learning through the lens of representation invariance to trans-
formation, we derived a gradient regularizer that controls how quickly representations can change
with transformation, and proposed feature averaging at test time to pull in information from multi-
ple transformations. These approaches led to representations that performed better on downstream
tasks. Therefore, our work provides evidence that invariance is highly relevant to the success of con-
trastive learning methods, and that there is scope to further improve upon these methods by using
invariance as a guiding principle.
9
Published as a conference paper at ICLR 2021
Acknowledgments
AF gratefully acknowledges funding from EPSRC grant no. EP/N509711/1. AF would also like to
thank Benjamin Bloem-Reddy for helpful discussions about theoretical aspects of this work.
References
Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing
mutual information across views. In Advances in Neural Information Processing Systems, pp.
15535-15545, 2019.
Ben Barrett, Alexander Camuto, Matthew Willetts, and Tom Rainforth. Certifiably robust variational
autoencoders. arXiv preprint arXiv:2102.07559, 2021.
Olivier Catoni. Pac-bayesian supervised classification: the thermodynamics of statistical learning.
arXiv preprint arXiv:0712.0248, 2007.
Ken Chatfield, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Return of the devil in the
details: Delving deep into convolutional nets. arXiv preprint arXiv:1405.3531, 2014.
Shuxiao Chen, Edgar Dobriban, and Jane H Lee. Invariance reduces variance: Understanding data
augmentation in deep learning and beyond. arXiv preprint arXiv:1907.10905, 2019.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020a.
Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big self-
supervised models are strong semi-supervised learners. arXiv preprint arXiv:2006.10029, 2020b.
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum
contrastive learning. arXiv preprint arXiv:2003.04297, 2020c.
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval
networks: Improving robustness to adversarial examples. arXiv preprint arXiv:1704.08847, 2017.
Taco Cohen and Max Welling. Group equivariant convolutional networks. In International confer-
ence on machine learning, pp. 2990-2999, 2016.
Taco S Cohen, Mario Geiger, Jonas Kohler, and Max Welling. Spherical CNNs. arXiv preprint
arXiv:1801.10130, 2018.
Harris Drucker and Yann Le Cun. Improving generalization performance using double backpropa-
gation. IEEE Transactions on Neural Networks, 3(6):991-997, 1992.
Wenchao Du, Hu Chen, and Hongyu Yang. Learning invariant representation for unsupervised
image restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 14483-14492, 2020.
Benjamin Graham. Fractional max-pooling. arXiv preprint arXiv:1412.6071, 2014.
Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, and Daan Wierstra. Towards
conceptual compression. In Advances In Neural Information Processing Systems, pp. 3549-3557,
2016.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of wasserstein GANs. In Advances in neural information processing systems, pp.
5767-5777, 2017.
Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant
mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recogni-
tion (CVPR’06), volume 2, pp. 1735-1742. IEEE, 2006.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
10
Published as a conference paper at ICLR 2021
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. arXiv preprint arXiv:1911.05722, 2019.
Olivier J Henaff, Ali Razavi, Carl Doersch, SM Eslami, and Aaron van den Oord. Data-efficient
image recognition with contrastive predictive coding. arXiv preprint arXiv:1905.09272, 2019.
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. arXiv preprint arXiv:1808.06670, 2018.
Ayush Jaiswal, Daniel Moyer, Greg Ver Steeg, Wael AbdAlmageed, and Premkumar Natarajan.
Invariant representations through adversarial forgetting. In AAAI, pp. 4272-4279, 2020.
William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert space.
Contemporary mathematics, 26(189-206):1, 1984.
Alexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Revisiting self-supervised visual represen-
tation learning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recog-
nition, pp. 1920-1929, 2019.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep con-
volutional neural networks. Advances in neural information processing systems, 25:1097-1105,
2012.
Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv
preprint arXiv:1608.03983, 2016.
Clare Lyle, Mark van der Wilk, Marta Kwiatkowska, Yarin Gal, and Benjamin Bloem-Reddy. On
the benefits of invariance in neural networks. arXiv preprint arXiv:2005.00178, 2020.
Sherjil Ozair, Corey Lynch, Yoshua Bengio, Aaron Van den Oord, Sergey Levine, and Pierre Ser-
manet. Wasserstein dependency measure for representation learning. In Advances in Neural
Information Processing Systems, pp. 15578-15588, 2019.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alche Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024-8035. Curran
Associates, Inc., 2019.
Ben Poole, Sherjil Ozair, Aaron van den Oord, Alexander A Alemi, and George Tucker. On varia-
tional bounds of mutual information. arXiv preprint arXiv:1905.06922, 2019.
Tom Rainforth, Rob Cornish, Hongseok Yang, Andrew Warrington, and Frank Wood. On nesting
Monte Carlo estimators. In International Conference on Machine Learning, pp. 4267-4276.
PMLR, 2018.
Salah Rifai, Yann N Dauphin, Pascal Vincent, Yoshua Bengio, and Xavier Muller. The manifold
tangent classifier. Advances in neural information processing systems, 24:2294-2302, 2011.
Patrice Y Simard, Yann A LeCun, John S Denker, and Bernard Victorri. Transformation invariance
in pattern recognitiontangent distance and tangent propagation. In Neural networks: tricks of the
trade, pp. 239-274. Springer, 1998.
Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel RD Rodrigues. Robust large margin deep
neural networks. IEEE Transactions on Signal Processing, 65(16):4265-4280, 2017.
Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick
Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point
clouds. arXiv preprint arXiv:1802.08219, 2018.
11
Published as a conference paper at ICLR 2021
Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. arXiv preprint
arXiv:1906.05849, 2019.
Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What
makes for good views for contrastive learning. arXiv preprint arXiv:2005.10243, 2020.
Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. Lipschitz-margin training: Scalable certifi-
cation of perturbation invariance for deep neural networks. In Advances in neural information
processing Systems,pp. 6541-6550, 2018.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018.
Laurens van der Maaten, Eric Postma, and Jaap van den Herik. Dimensionality reduction: a com-
parative. Journal of Machine Learning Research, 10(66-71):13, 2009.
Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through align-
ment and uniformity on the hypersphere. arXiv preprint arXiv:2005.10242, 2020.
Jim Winkens, Rudy Bunel, Abhijit Guha Roy, Robert Stanforth, Vivek Natarajan, Joseph R Ledsam,
Patricia MacWilliams, Pushmeet Kohli, Alan Karthikesalingam, Simon Kohl, et al. Contrastive
training for improved out-of-distribution detection. arXiv preprint arXiv:2007.05566, 2020.
Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-
parametric instance discrimination. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 3733-3742, 2018.
Tian Xu, Jiayu Zhan, Oliver GB Garrod, Philip HS Torr, Song-Chun Zhu, Robin AA Ince, and
Philippe G Schyns. Deeper interpretability of deep networks. arXiv preprint arXiv:1811.07807,
2018.
Donggeun Yoo, Sunggyun Park, Joon-Young Lee, and In So Kweon. Multi-scale pyramid pooling
for deep convolutional representation. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition Workshops, pp. 71-80, 2015.
Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks. arXiv
preprint arXiv:1708.03888, 2017.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and
Alexander J Smola. Deep sets. In Advances in neural information processing systems, pp. 3391-
3401, 2017.
12
Published as a conference paper at ICLR 2021
A	Mutual information
In Section 2, we saw that the InfoNCE objective equation 3 fulfills the need to make p(z|x) tightly
focused on a single point whilst simultaneously requiring p(z) to be well spread out over represen-
tation space. In this appendix, we show that the same general principle of making p(z|x) tightly
focused on a single point whilst simultaneously requiring p(z) to be well spread out over represen-
tation space connects to mutual information maximization.
To establish the connection to mutual information, we take the differential entropy as our measure
of ‘spread’. Recall the differential entropy of a random variable w is
H[p(w)] := Ep(w) [- log p(w)].	(17)
We then translate our intuition to make p(z|x) tightly focused on a single point whilst simultaneously
requiring p(z) to be well spread out over representation space into requiring Ep(x) [H[p(z|x)]] to be
minimized whilst H [p(z)] should be simultaneously maximized. This suggests the following loss
function
Lentropy = Ep(x)[H[p(z|x)]] - H [p(z)] = -I(x; z)	(18)
which is the (negative) mutual information between x and z. Note that in this formulation, it is the
distribution p(z|x) as much as the InfoMax principle which determines how this loss will behave.
Finally, there is a clear connection between the InfoNCE loss and mutual information, specifically
the InfoNCE loss is, in expectation and up to an additive constant, a lower bound on I(x; z) van den
Oord et al. (2018); Poole et al. (2019).
B	Method
B.1	An alternative variance formula
We present a derivation of our alternative formula for the variance (dropping the conditioning from
the notation for conciseness)
2Ep(α)p(α0) [(F(a, β, x, e) - F(α0, β, x, e))2]
=1 Ep(α)p(α0) [(F(a, β, x, e) - Eα[F(α, β, x, e)] + E0[F(α, β, x, e)] - F(α0, β, x, e))2]
=1Ep(α)p(α0) [(F(α, β, x, e) - Eα[F(α, β, x, e)])2 + (E°[F(α, β, x, e)] - F(α0, β, x, e))2]
+ Ep(α)p(α0) [(F (α, β, x, e) - Eα [F (α, β, x, e)])(Eα [F (α, β, x, e)] - F(α0, β, x, e))]
=1 Ep(α)p(α0) [(F(α, β, x, e) - Eα[F(α, β, x, e)])2 + (F(α0, β, x, e)] - E0[F(α, β, x, e)])2]
= Varp(α) [F (α, β, x, e)] .
B.2	Motivating the Rademacher distribution
We are interested in the conditional variance ofz with respect to α, but as z is a vector valued random
variable We properly need to consider the conditional covariance matrix Σ = Cova(z|x, β). We
henceforth consider x, β to be fixed. To reduce conditional variance in all directions, it makes sense
to reduce the trace Tr Σ. Due to computational limitations, We cannot directly estimate this trace
at each iteration, instead we must estimate Var(e ∙ z) = e> Σe. However, by carefully selecting
the distribution for e We can effectively target the trace of the covariance matrix by taking the
expectation over e. Specifically, suppose that the components of e are independent Rademacher
random variables (±1 with equal probability). Then
Ep(e)	e>Σe	= Ep(e)	ei Σij ej	=	Σij Ep(e)	[eiej]	=	Σij δij	= Tr Σ.	(19)
C	Theory
We present the proof of Theorem 1 which is restated for convenience.
13
Published as a conference paper at ICLR 2021
Theorem 1. Consider evaluation on a downstream task by fitting a linear classification model with
softmax loss or a linear regression model with square error loss on with representations as features.
For a fixed classifier or regressor and M0 ≥ M we have
Ep(x,y)p(aiM0)p(βiM0) [' (z(M ),y)] ≤ Ep(x,y)p(aiM)p(βiM)[' (工加),y) ] .	(16)
Proof. We have the softmax loss
'(z, y) = -w>Z + log (X exp (w>z)j	(20)
or the square error loss
`(z, y) = y - w>z2 .	(21)
We first show that both loss functions considered are convex in the argument z. To show this, we fix
0 ≤ p = 1 - q ≤ 1. For softmax loss, we have
` (pz1 + qz2 , y)
-pw>zι - qw>z2 + log EeXp (w>zι)p exp (w>z2)q
≤-pw>zι - qw>z2 + log (EeXp (w>zι)
q
EeXp (w>z2)l
by Holder's Inequality
=-pw>zι - qw>z2 + Plog (Xexp (w>zι)j + qlog (X exp (w>z2)
=p' (zι,y) + q' (z2,y)
(22)
(23)
(24)
(25)
(26)
(27)
and for square error loss we have
` (pz1 + qz2, y) = |y - w>(pz1 + qz2)|2	(28)
= |p(y - w>z1)	+ q(y - w>z2)|2	(29)
= p|y - w>z1|2	+ q|y - w>z2|2	+ (p2 - p)	w>z1 - w>z22	(30)
≤ p|y - w>z1|2	+ q|y - w>z2|2	(31)
=P' (zι,y) + q' (z2,y)	(32)
For the inequality in the Theorem, we consider drawing M0 ≥ M samples, and randomly choosing
an M -subset. Let S represent this subset and let z(SM) represent the feature averaged representation
that uses the subset S. We have
EP(x,y)p(ai：M )p(βι-.M) ['(z(W,y) ] = Ep(x,y)p(ti：M 0 )p(αιM )p(β^M 0 )p(S) [' (zS ∖y) ]	(33)
≥ Ep(X,y)P(a1：M0 )P(β1M0 ) h' (Ep(S) [zS)]，y)i	(34)
=Ep(X,y)p3iM0)p(βiM0) h' (z(M ),y)]	(35)
where the inequality at equation 34 is by Jensen's Inequality. This completes the proof.	□
We provide an informal discussion of other theoretical results that relate to our work. Lyle et al.
(2020) explored PAC-Bayesian approaches to analyzing the role of group invariance in generaliza-
tion of supervized neural network models. The central bound based on Catoni (2007) is given in
14
Published as a conference paper at ICLR 2021
Theorem 1 of Lyle et al. (2020) and depends on the empirical risk R'(Q, Dn) and the KL term
KL(Q||P) which represents the PAC-Bayesian KL divergence between distributions on hypothe-
Sis space. Theorem 7 of Lyle et al. (2020) shows KL(Q^∣∣P◦) ≤ KL(Q∣∣P), where Q° and P◦
are formed by symmetrization such as feature averaging over the group of transformations. In our
context, although the transformations do not form a group, we could still consider a symmetriza-
tion operation with feature averaging. If the symmetrization does not affect the empirical risk, then
Theorem 9 of Lyle et al. (2020) would apply to our setting and we would be able to obtain a tighter
generalization bound for our suggested approach of feature averaging.
D	Related work
D. 1 The role of transformations in contrastive learning
Recent work on contrastive learning, initiated by the development of Contrastive Predictive Coding
(van den Oord et al., 2018; Henaff et al., 2019), has progressively moved the transformations to a
more central position in understanding and improving these approaches. In Bachman et al. (2019),
multiple views of a context are extracted, on images this utilizes repeated data augmentation such
as random resized crop, random colour jitter, and random conversion to grayscale, and the model
is trained to maximize information between these views using an InfoNCE style objective. Other
approaches are possible, for instance Tian et al. (2019) obtained multiple views of images using
Lab colour decomposition. In SimCLR (Chen et al., 2020a;b), the approach of applying multiple
data augmentations (including flip and blur, as well as crops, colour jitter and random grayscale)
and using an InfoNCE objective was simplified and streamlined, and the central role of the aug-
mentations was emphasized. By changing the set of transformation operations used, Chen et al.
(2020c) were able to improve their contrastive learning approach and achieve excellent performance
on downstream detection and segmentation tasks. Tian et al. (2020) studied what the best range
of transformations for contrastive learning is. The authors found that there is a ‘sweet spot’ in the
strength of transformations applied in contrastive learning, with transformations that are too strong
or weak being less favourable. Winkens et al. (2020) showed that contrastive methods can be suc-
cessfully applied to out-of-distribution detection. We note that for tasks such as out-of-distribution
detection, transformation covariance may be a more relevant property than invariance.
D.2 Gradient regularization to enforce Lipschitz constraints
Constraining a neural network to be Lipschitz continuous bounds how quickly its output can change
as the input changes. In supervized learning, a small Lipschitz constant has been shown to lead to
better generalization (Sokolic et al., 2017) and improved adversarial robustness (Cisse et al., 2017;
Tsuzuku et al., 2018). One practical method for constraining the Lipschitz constant is gradient
regularization (Drucker & Le Cun, 1992; Gulrajani et al., 2017). Lipschitz constraints have also
been applied in a self-supervized context: in Ozair et al. (2019), the authors used a Wasserstein
dependency measure in a contrastive learning setting by using gradient penalization to ensure that the
function x, x0 7→ sφ(fθ(x), fθ(x0)) is 1-Lipschitz. Our work uses a gradient regularizer to control
how quickly representations can change, but unlike existing work we focus on how representations
change with α as x is fixed, instead of how they change with x.
D.3 Group invariant neural networks
A large body of recent work has focused on designing neural network architectures that are perfectly
invariant, or equivariant, to a set of transformations T in the case when T forms a group. Cohen &
Welling (2016) showed how convolutional neural networks can be generalized to have equivariance
to arbitrary group transformations applied to their inputs. This can apply, for instance, to rotation
groups on the sphere (Cohen et al., 2018), rotation and translation groups on point clouds (Thomas
et al., 2018), and permutation groups on sets (Zaheer et al., 2017). Transformations that form a group
cannot remove information from the input (because they must be invertible) and can be composed
in any order. This means that the more general transformations considered in our work cannot form
a group—they cannot be composed (repeated decreasing of brightness to zero is not allowed) nor
inverted (crops are not invertible). We have therefore considered methods that improve invariance
under much more general transformations.
15
Published as a conference paper at ICLR 2021
(b) Pixel intensity
(c) Colour sample
Figure 7: A sample from the Spirograph dataset with m = 4, b = 0.4, h = 2, σ
1, (fr	fg	fb)	= (0.9 0.8	0.7),	(br	bg	bb)	= (0.3	0.4	0.5).
D.4 Feature averaging and pooling
The concepts of sum-, max- and mean-pooling have a rich history in deep learning (Krizhevsky
et al., 2012; Graham, 2014). For example, pooling can be used to down-scale representations in
convolutional neural networks (CNNs) as part of a single forward pass through the network with a
single input. In our work, however, we apply feature averaging, or mean-pooling, using multiple,
differently transformed versions of the same input. This is more similar to Chatfield et al. (2014),
who considered pooling or stacking augmented inputs as part of a CNN, and Yoo et al. (2015) who
proposed a multi-scale pyramid pooling approach. Unlike these works, we apply pooling in an
unsupervized contrastive representation learning context. Our feature averaging occurs on the final
representations, rather than in a pyramid, and not on intermediate layers of the network. We also
use the transformation distribution that is used to define the self-supervized task itself. Other work
has explored theoretical aspects of feature averaging (Chen et al., 2019; Lyle et al., 2020) in the
supervized learning setting, showing conditions on the invariance properties of the underlying data
distribution that can be exploited to obtain improved generalization using feature averaging. For a
detailed discussion of Lyle et al. (2020) and its connections with our own work, see Section C.
E Spirograph dataset
We propose a new dataset that allows the separation of generative factors of interest from nuisance
transformation factors and that is formed from a fully differentiable generative process. A stan-
dalone implementation of this dataset can be found at https://github.com/rattaoup/
spirograph. Our dataset is inspired by the beautiful spirograph patterns some of us drew as
children, which are mathematically hypotrochoids given by the following equations
(m - h) sin(t) - h sin
(m - h) cos(t) + hcos
(m - h)t
b-
(36)
(37)
Figure 7(a) shows an example. To create an image dataset from such curves, we choose 40 equally
spaced points ti with t1 = 0 and t40 = 2π, giving a sequence of points (x1 , y1), ..., (x40, y40) on
the chosen hypotrochoid. For smoothing parameter σ, the pixel intensity at a point (u, v) is given by
(38)
x
y
For a grid of pixel, the intensity values are normalized so that the maximum intensity is equal to
1. Figure 7(b) shows the pixel intensity with σ = 0.5. Finally, for a foreground colour with RGB
values (fr, fg, fb) and background colour (br, bg, bb), the final RGB values at a point (u, v) is
fr	br
c(u, v) = i(u, v) fg	+ (1 - i(u, v)) bg	(39)
16
Published as a conference paper at ICLR 2021
The final coloured sample image is shown in Figure 7(c).
The Spirograph sample is fully specified by the parameters m, b, h, σ, fr , fg , fb , br , bg , bb. In our
experiments, we treat m, b, σ, fr as parameters of interest. We treat h and the remaining colour pa-
rameters as nuisance parameters. That is, we take x = (m, b, σ, fr) and α = (h, fg , fb, br , bg , bb)
and the transformation t(x, α) is the full generative process described above. There are no addi-
tional parameters β for this dataset. Figure 1 shows two sets of four samples from the Spirograph
dataset, in each set the generative factors of interest are fixed and the nuisance parameters are var-
ied. In general for the Spirograph dataset, the distinction between generative factors of interest and
nuisance parameters can be changed to attempt to learn different aspects of the data. The transfor-
mation t is fully differentiable, meaning that we can apply gradient penalization to all the nuisance
parameters of the generative process. In our experiments, we took the following distributions to
sample random values of the parameters: m 〜 U(2,5),b 〜 U(0.1,1.1), h 〜 U(0.5, 2.5),σ 〜
U(0.25,1), fr, fg, fb 〜U(0.4,1), br, bg,瓦〜U(0,0.6). We synthesized 100,000 training images
and 20,000 test images with dimension 32 × 32.
F Experiment details
Our experiments were implemented in PyTorch (Paszke et al., 2019) and ran on 8 Nvidia GeForce
GTX 1080Ti GPUs. See https://github.com/ae- foster/invclr for an implementa-
tion of our approaches.
F.1 Differentiable colour distortion
We want to improve the representations learned from contrastive methods by explicitly encouraging
stronger invariance to the set of transformations. Our method is to restrict gradients of the repre-
sentations with respect to certain transformations. Ensuring that the transformations are practically
differentiable within PyTorch (Paszke et al., 2019) required a thorough study of the transformations.
The subset of transformations we apply gradient regularization to includes colour distortions which
are conventionally treated as a part of data preprocessing. Rewriting this as a differentiable module
within the computational graph allows us to practically compute the gradient regularizer of equa-
tion 11. We will consider adjusting brightness, contrast, saturation, hue ofan image. In fact, most of
these transformations are simply linear transformations of the original image. First, the brightness
adjustment is simply defined as
xbrt = xαbrt	(40)
when αbrt is a scale factor. If we write x = r, g, b, for the three colour channels of x, then greyscale
conversion of x is given by
xgs = 0.299r + 0.587g + 0.114b.	(41)
Adjusting the saturation ofx is a linear combination ofx and xgs, the greyscale version ofx
xsat = xαsat + xgs (1 - αsat)	(42)
when αsat is a scale factor. Adjusting the contrast of x is a linear combination of x and mean(xgs),
which the mean over all spatial dimensions of xgs. With a scaling parameter αcon we have
xcon = xαcon + mean(xgs )(1 - αcon).	(43)
We utilize a linear approximation for hue adjustment. We perform hue adjustment by converting
to the YIQ colour space, and then applying rotation on the IQ components. The transformation
between RGB and YIQ colour space is given by the following linear transformation
Y	0.299
I = 0.5959
Q	0.2115
0.587	0.114	R	R
-0.2746	-0.3213	G	= TYIQ	G
-0.5227	0.3112	B	B
(44)
Note that the Y component is exactly the greyscale version xgs defined above. We transform YIQ
back to RGB by
R	1	0.956	0.619	Y	Y
G	=	1	-0.272	-0.647	I	=	TRGB	I	(45)
B	1	-1.106	1.703	Q	Q
17
Published as a conference paper at ICLR 2021
Parameter	CIFAR	Spirograph
Encoder model	ResNet50	ResNet18
Training batch size	512	512
Training epochs	1000	50
Optimizer	LARS	LARS
Scheduler	Cosine annealing	Cosine annealing
Learning rate	3	3
Momentum	0.9	0.9
Temperature τ	0.5	0.5
Table 4: Hyperparameters used for CIFAR-10, CIFAR-100 and Spirograph
In YIQ format, we can adjust hue of an image by θ = 2παhue by multiplying with a rotation matrix
10	0
Rθ =	0 cos θ - sin θ	(46)
0 sin θ cos θ
Therefore, our hue adjustment is given by
xhue = TRGB Rαhue TY IQ x	(47)
where the matrices operate on the three colour channels of x and in parallel over all spatial dimen-
sions. Each operation is followed by pointwise clipping of pixel values to the range [0, 1].
F.2 Set-up
Our set-up is quite similar to the setup in Chen et al. (2020a) with two main differences: we treat
colour distortions as a differentiable module while in Chen et al. (2020a) the transformation was
performed in the preprocessing step, and we add the gradient penalty term in addition to the original
loss in Chen et al. (2020a).
F.2.1 Transformations
First, for a batch x1, ..., xK of inputs, we form a pair of (x1 , x01), ..., (xK, x0K) by applying two
random transformations: random resized crop and random horizontal flip for each input. We then
apply our differentiable colour distortion function which is composed of random colour jitter with
probability p = 0.8 and random greyscale with probability p = 0.2. (Colour jitter is the composi-
tion of adjusting brightness, adjusting contrast, adjusting saturation, adjusting hue in this order.) We
sample α, the parameter that controls how strong the adjustment is for each image from the follow-
ing distributions: brightness, contrast and saturation adjustment parameters from U (0.6, 1.4) and
hue adjustment parameter from U (-0.1, 0.1). We call the resultant pairs (x1, x01), ..., (xK, x0K).
F.2.2 Contrastive learning
Similar to Chen et al. (2020a), we use the transformed (x1, x01), ..., (xK, x0K) as an input to an
encoder to learn a pair of representations (z1, z01), ..., (zK, z0K). The final loss function that we use
for training is equation 13. Table 4 shows all hyperparameters that were used for training. The small
neural network gφ is a MLP with the two layers consisting of a fully connected linear map, ReLU
activation and batch normalization. We use LARS optimizer (You et al., 2017) and apply cosine
annealing (Loshchilov & Hutter, 2016) to the learning rate.
F.2.3 Gradient regularization
In this part, we explain our setup for calculating the gradient penalty as in equation 12. We sample
a random vector e with independent Rademacher components and independently for each sample
in the batch. We generate L samples of α for each element of the batch to compute the regularizer.
Finally, we clip the penalty from above to prevent instability at the onset of training. In practice,
this meant the gradient regularization was not enforced for about the first epoch of training. Table 5
shows hyperparameters that we used within gradient penalty calculation.
18
Published as a conference paper at ICLR 2021
Parameter CIFAR-10 Spirograph
L
λ
Clip value
100
0.1
100
0.01
1000
1
Table 5:	Hyperparameters for gradient penalty calculation
Parameter
Evaluation model
Evaluation loss
Weight decay
Optimization
CIFAR-10
Linear classification
Cross entropy loss
10-5
L-BFGS 500 steps
Spirograph
Linear regression
Mean squared error
10-8
L-BFGS 500 steps
Table 6:	Hyperparameters for model evaluation
F.2.4 Evaluation
We use our representations as features in linear classification and regression tasks. We train these
linear models with L-BFGS with hyperparameters as shown in Table 6 on the training set and eval-
uate performance on the test set.
F.2.5 MOCO V2
To empirically demonstrate that our ideas transfer to alternative base contrastive learning methods,
we applied both gradient regularization and feature averaging to the MoCo v2 (Chen et al., 2020c)
base set-up. We also explored two different ResNet (He et al., 2016) architectures. We closely fol-
lowed the MoCo v2 implementation at https://github.com/facebookresearch/moco.
As for SimCLR, we adapted the transformations to be a differentiable module. We also made adap-
tations for CIFAR-100 in an identical way as in our previous experiments. As in MoCo v2, we
removed batch normalization in the projection head gφ ; we used SGD optimization with learning
rate 0.06 for a batch size of 512, and used the MoCo parameters K = 2048 and m = 0.99 for
ResNet18 and K = 4096, m = 0.99 for ResNet50. We did not conduct extensive hyperparameter
sweeps, but we did investigate larger values of K which did not lead to improved performance on
CIFAR-100. (In particular, the original settings K = 65536, m = 0.999 appeared to perform less
well on this dataset.) Other hyperparameters and settings were identical to Chen et al. (2020c). We
did 3 independent runs with a ResNet18 and 2 runs with a ResNet50. We conducted linear classi-
fication evaluation with fixed representations in exactly the same way as for our other experiments.
Feature averaging results used M = 40.
F.2.6 Computational cost
We found that gradient regularization increased the total time to train encoders by a factor of at
most 2. For feature averaging at test time with a fixed dataset, the computation of features z(M) is
an O(M) operation, whilst the training and testing of the linear classifier is O(1). Training time
remained by far the larger in all experiments by orders of magnitude.
F.3 Additional experimental results
F.3.1 Comparison with ensembling
Feature averaging is an approach that bears much similarity with ensembling. To experimentally
compare these two approaches, we applied both approaches to encoders trained on CIFAR-10. To
provide a suitable comparison with feature averaging using z(M) we first a trained a linear classi-
fierp(y|z) using an M -fold augmented dataset of representations with a standard cross-entropy loss
using L-BFGS optimization using the same weight decay as for feature averaging. For CIFAR-10,
which has a training set of length 50000, the feature averaging classifier was trained using 50000
averaged representations, whereas the ensembling classifier was trained with 50000M examples
using data augmentation. At test time, we averaged prediction probabilities using M different rep-
19
Published as a conference paper at ICLR 2021
(a) Accuracy
(b) Cross-entropy loss
—X- Feature averaging
-+- Augmentation ensembling
Figure 8: A comparison between feature averaging and augmentation ensembling using representa-
tions obtained with gradient regularization on CIFAR-10. Error bars are 1 s.e. from 3 runs.
(a) Accuracy
(b) Cross-entropy loss
Figure 9: A comparison between feature averaging and augmentation ensembling using representa-
tions obtained with SimCLR on CIFAR-10. Error bars are 1 s.e. from 3 runs.
resentations of each test image. Specifically, if p(y|z) is the classifier trained by the aforementioned
procedure and αι,…,αM 〜p(α), βι,…，Bm 〜p(β) are independent transformation parameters,
the probability of assigning label y to input x is given by
1M
PenSemble(y|X) = M^〉： p(y fθ(t(x, αm, Bm))).	(48)
m=1
The results outlined in Figure 8 show that ensembling gives very similar performance to feature
averaging in terms of accuracy, but is significantly worse in terms of loss. We can understand this
result intuitively because ensembling includes probabilities from every transformed version of the
input (including where the classifier is uncertain or incorrect) whereas feature averaging combines
transformations in representation space and uses only one forward pass of the classifier. More for-
mally, the difference in test loss makes sense in light of Theorem 1. Figure 9 shows additional
results obtained using representations trained with standard SimCLR on CIFAR-10. We see the
same pattern—a similar test accuracy but worse test loss when using augmentation ensembling.
F.3.2 Gradient regularization leads to strongly invariant representations
We first show that our gradient penalty successfully learns
representations that have greater invariance to transfor-
mation than their counterparts generated by contrastive
learning. We consider two metrics: the conditional vari-
ance targetted directly by the gradient regularizer, and the
loss when z is used to predict α with linear regression.
Table 7 and Figure 10 are the equivalents of Table 1 and
Figure 2 for CIFAR-100, showing the conditional vari-
ance and the regression loss for predicting α respectively.
In Table 8 we present the same results for Spirograph. We
see very similar results to CIFAR-10 in both cases—the
gradient penalty dramatically reduces conditional vari-
ances, and prediction of α by linear regression gives a
Figure 10: The conditional variance of
Equation 6 on CIFAR-100. Error bars
represent 1 s.e. from 3 runs.
loss that is better than a constant prediction only for standard contrastive representations.
F.3.3 Gradient regularized representations perform better on downstream
TASKS AND ARE ROBUST TO TEST TIME TRANSFORMATION
For downstream performance on Spirograph, we evaluate the performance of encoders trained with
gradient regularization and without gradient regularization on the task of predicting the generative
20
Published as a conference paper at ICLR 2021
Table 7:	The test loss when a linear regression model is used to predict α from z on CIFAR-100.
The reference value is Meani Var(αi). We present the mean and ± 1 s.e. from 3 runs.
No regularization
Test loss
0.0357 ± 0.0003
Regularization 0.0417 ± 0.00007
Reference value 0.0408
Table 8:	Invariance metrics for Spirograph. We present the conditional variance, and the test loss
when a linear regression model is used to predict α from z. The reference value is Meani Var(αi).
We present the mean and ± 1 s.e. from 3 runs.
Conditional variance Test loss
No regularization^^0.789 ± 0.0069	0.0751 ± 0.0003
Regularization	0.0016 ± 0.00004	0.0808 ± 0.00009
Reference value	-	0.0806
parameters of interest. In our set-up, we use 100,000 train images and 20,000 test images and train
the encoders on the training set for 50 epochs. For evaluation, we train a linear regressor on the
representations from encoders to predict the actual generative parameters. Setting for the linear
regressor is shown in the Table 6. To accompany the main results in Figure 3(c), we include the
exact values used in this figure in Table 9.
We now turn to our experiments used to investigate robustness—we investigate scenarios when we
change the distribution of transformation parameters α at test time, but use encoders that were
trained with the original distribution. We investigate on both CIFAR and Spirograph datasets.
For CIFAR, we chose to vary the distribution of pa-
rameters for colour distortions at test time. We could
write the distribution of parameter of brightness, sat-
uration, contrast as U (1 - 0.8S, 1 + 0.8S) and the
distribution of hue as U (-0.2S, 0.2S) where S is
a parameter controlling the strength of the distor-
tion. In the original setup, we have S = 0.5. By
varying the value of S used at test time, we can in-
crease the variance of the nuisance transformations,
including stronger transformations than those that
were present when encoders were trained. This is
visualized in Figure 11. Figure 14 is a companion
plot for Figure 4(a) applied on CIFAR-100. We see
broadly similar trends—our representations outper-
form those from standard contrastive learning across
a range of test time distributions.
Figure 14: Robustness of performance on
CIFAR-100 under variance scaling of trans-
formation parameters.
For robustness on SPirograPh recall α = (h,fg,fb,br,bg,bb) when h 〜 U(0.5,2.5), fg,f 〜
U(0.4,1), br,bg,bb 〜U(0,0.6). We chose to vary the distribution of the background colour
(br , bg , bb ) and the distribution of h which is a structure-related transformation Parameter. We con-
sider two aPProaches, mean shifting where we shift the uniform distribution by S, for examPle
U(a, b) → U(a + S, b + S) and we consider changing variance where we increase the range of the
Table 9: The raw values used to Produce Figure 3(c). Each of the downstream tasks is a generative
Parameters. Values are the test mean square error ± 1 s.e. from 3 runs.
Parameters	No regularization	Gradient regularization
m	0.0006773 ± 0.0000509	0.0005073 ± 0.0000078
b	0.0112480 ± 0.0002555	0.0073607 ± 0.0001079
σ	0.0000914 ± 0.0000108	0.0000527 ± 0.0000026
fr	0.0000232 ± 0.0000004	0.0000028 ± 0.0000001
21
Published as a conference paper at ICLR 2021
(a) Scale 0.1	(b) Scale 0.2	(c) Scale 0.3	(d) Scale 0.4
Figure 11:	Visualization of test time distortions applied to CIFAR-10 for various variance scalings.
(a) h = 0.5	(b) h = 1.0	(c) h = 1.5	(d) h = 2.0
Figure 12:	The effect of varying h on the structure of a Spirograph image.

(a) Shift 0	(b) Shift 0.15	(c) Shift 0.30	(d) Shift 0.45
Figure 13:	The effect of shifting the background colour distribution used for Spirograph images.
uniform distribution by 2S, for example, U (a, b) → U(a - S, b + S). We compare performance
of the trained encoders at epoch 50 on predicting the generative parameters (m, b, σ, fr) and we use
the same setting for linear regressors as in Table 6.
Figure 12 is a visualization of the effect of varying h from 0.5 to 2.0 while other parameters are kept
constant. The figure 13 shows the effect of varying the background colour of an image by adding
S = 0.15, 0.30, 0.45 to each of the background RGB channels.
For varying the distribution of h, We consider shifting the mean of h 〜 U(0.5,2.5) by S =
±0.1, ±0.3, ±0.5 and increasing the variance of h by S = 0.1, 0.3, 0.5. For the distribution of
the background colour (br, bg, bb), We consider shifting the distribution of (br, bg, bb) by S =
0.1, 0.2, 0.3, 0.4 and increasing variance by the same amount. We note that (br, bg, bb) controls
the background colour of an image, so We are varying the 3 distributions at the same time. Since,
the foreground colour has the distribution f ,fg,f 〜U(0.4,1), we are shifting the distribution
of (br , bg, bb) toWard (fr, fg, fb) and this Will make the background and foreground colours more
similar. For example, with S = 0.4, when we apply a mean shift we change the distribution of
(br, bg,bb) to br,bg, bb 〜U(0.4,1), and when we increase the variance the distribution becomes
br,bg,bb 〜U(0,1).
22