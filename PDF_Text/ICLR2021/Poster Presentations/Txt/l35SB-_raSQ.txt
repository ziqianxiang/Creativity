Published as a conference paper at ICLR 2021
A Hypergradient Approach to
Robust Regression without Correspondence
Yujia Xie； Yixiu Mao； Simiao Zuo, Hongteng Xu, Xiaojing Ye, Tuo Zhao, Hongyuan Zhat
Ab stract
We consider a regression problem, where the correspondence between input and
output data is not available. Such shuffled data is commonly observed in many
real world problems. Taking flow cytometry as an example, the measuring in-
struments are unable to preserve the correspondence between the samples and the
measurements. Due to the combinatorial nature, most of existing methods are
only applicable when the sample size is small, and limited to linear regression
models. To overcome such bottlenecks, we propose a new computational frame-
work - ROBOT- for the shuffled regression problem, which is applicable to large
data and complex models. Specifically, we propose to formulate the regression
without correspondence as a continuous optimization problem. Then by exploit-
ing the interaction between the regression model and the data correspondence,
we propose to develop a hypergradient approach based on differentiable program-
ming techniques. Such a hypergradient approach essentially views the data cor-
respondence as an operator of the regression, and therefore allows us to find a
better descent direction for the model parameter by differentiating through the
data correspondence. ROBOT is quite general, and can be further extended to the
inexact correspondence setting, where the input and output data are not necessar-
ily exactly aligned. Thorough numerical experiments show that ROBOT achieves
better performance than existing methods in both linear and nonlinear regression
tasks, including real-world applications such as flow cytometry and multi-object
tracking.
1 Introduction
Regression analysis has been widely used in various machine learning applications to infer the the
relationship between an explanatory random variable (i.e., the input) X ∈ Rd and a response random
variable (i.e., the output) Y ∈ Ro (Stanton, 2001). In the classical setting, regression is used on
labeled datasets that contain paired samples {xi, yi}in=1, where xi, yi are realizations of X, Y ,
respectively.
Unfortunately, such an input-output correspondence is not always available in some applications.
One example is flow cytometry, which is a physical experiment for measuring properties of cells,
e.g., affinity to a particular target (Abid & Zou, 2018). Through this process, cells are suspended
in a fluid and injected into the flow cytometer, where measurements are taken using the scattering
of a laser. However, the instruments are unable to differentiate the cells passing through the laser,
such that the correspondence between the cell proprieties (i.e., the measurements) and the cells is
unknown. This prevents us from analyzing the relationship between the instruments and the mea-
surements using classical regression analysis, due to the missing correspondence. Another example
is multi-object tracking, where we need to infer the motion of objects given consecutive frames in
* Equal Contributions.
^Yujia Xie, Simiao Zuo, Tuo Zhao are affiliated with Georgia Institute of Technology. Emails:
{xieyujia, simiaozuo, tourzhao}@gatech.edu. Yixiu Mao is affiliated with Shanghai Jiao
Tong University. Email: 956986044myx@gmail.com. Hongteng Xu is affiliated with Gaoling School
of Artificial Intelligence, Renmin University of China, and Beijing Key Laboratory of of Big Data Manage-
ment and Analysis Methods. Email: hongtengxu@ruc.edu.cn. Xiaojing Ye is affiliated with Georgia
State University. Email: xye@gsu.edu. Hongyuan Zha is affiliated with School of Data Science, Shenzhen
Institute of Artificial Intelligence and Robotics for Society, the Chinese University of Hong Kong, Shenzhen.
Email: zhahy@cuhk.edu.cn.
1
Published as a conference paper at ICLR 2021
a video. This requires us to find the correspondence between the objects in the current frame and
those in the next frame.
The two examples above can be formulated as a shuffled regression problem. Specifically, we con-
sider a multivariate regression model
Y = f(X,Z ； w)+ ε,
where X ∈ Rd , Z ∈ Re are two input vectors, Y ∈ Ro is an output vector, f : Rd+e → Ro is the
unknown regression model with parameters w and ε is the random noise independent on X and Z.
When we sample realizations from such a regression model, the correspondence between (X, Y) and
Z is not available. Accordingly, we collect two datasets D1 = {xi , yi }in=1 and D2 = {zj}jn=1, and
there exists a permutation ∏* such that (Xi ,z∏(i)) corresponds to yi in the regression model. Our goal
is to recover the unknown model parameter w. Existing literature also refer to the shuffled regression
problem as unlabeled sensing, homomorphic sensing, and regression with an unknown permutation
(Unnikrishnan et al., 2018). Throughout the rest of the paper, we refer to it as Regression WithOut
Correspondence (RWOC).
A natural choice of the objective for RWOC is to minimize the sum of squared residuals with respect
to the regression model parameter W UP to the permutation ∏(∙) over the training data, i.e.,
minw,π L(w, π) = Pin=1 kyi - f xi, zπ(i); w k22.	(1)
Existing works on RWOC mostly focus on theoretical properties of the global optima to equation 1
for estimating w and π (Pananjady et al., 2016; 2017b; Abid et al., 2017; Elhami et al., 2017;
Hsu et al., 2017; Unnikrishnan et al., 2018; Tsakiris & Peng, 2019). The development of practical
algorithms, however, falls far behind from the following three aspects:
•	Most of the works are only applicable to linear regression models.
•	Some of the existing algorithms are of very high computational complexity, and can only handle
small number of data points in low dimensions (Elhami et al., 2017; Pananjady et al., 2017a; Tsakiris
et al., 2018; Peng & Tsakiris, 2020). For example, Abid & Zou (2018) adopt an Expectation Maxi-
mization (EM) method where Metropolis-Hastings sampling is needed, which is not scalable. Other
algorithms choose to optimize with respect to w and π in an alternating manner, e.g., alternating
minimization in Abid et al. (2017). However, as there exists a strong interaction between w and
π, the optimization landscape of equation 1 is ill-conditioned. Therefore, these algorithms are not
effective and often get stuck in local optima.
•	Most of the works only consider the case where there exists an exact one-to-one correspondence
between D1 and D2 . For many more scenarios, however, these two datasets are not necessarily
well aligned. For example, consider D1 and D2 collected from two separate databases, where the
users overlap, but are not identical. As a result, there exists only partial one-to-one correspondence.
A similar situation also happens to multiple-object tracking: Some objects may leave the scene in
one frame, and new objects may enter the scene in subsequent frames. Therefore, not all objects
in different frames can be perfectly matched. The RWOC problem with partial correspondence
is known as robust-RWOC, or rRWOC (Varol & Nejatbakhsh, 2019), and is much less studied in
existing literature.
To address these concerns, We propose a new computational framework - ROBOT (Regression
withOut correspondence using Bilevel OptimizaTion). Specifically, we propose to formulate the
regression without correspondence as a continuous optimization problem. Then by exploiting the
interaction between the regression model and the data correspondence, we propose to develop a
hypergradient approach based on differentiable programming techniques (Duchi et al., 2008; Luise
et al., 2018). Our hypergradient approach views the data correspondence as an operator of the
regression, i.e., fora given w, the optimal correspondence is
πb(w) = arg minπ L(w, π).	(2)
Accordingly, when applying gradient descent to (1), we need to find the gradient with respect to
w by differentiating through both the objective function L and the data correspondence πb(w). For
simplicity, we refer as such a gradient to “hypergradient”. Note that due to its discrete nature, πb(w)
is actually not continuous in w. Therefore, such a hypergradient does not exist. To address this issue,
we further propose to construct a smooth approximation of πb(w) by adding an additional regularizer
to equation 2, and then we replace πb(w) with our proposed smooth replacement when computing
the hyper gradient of w. Moreover, we also propose an efficient and scalable implementation of
2
Published as a conference paper at ICLR 2021
hypergradient computation based on simple first order algorithms and implicit differentiation, which
outperforms conventional automatic differentiation in terms of time and memory cost.
ROBOT can also be extended to the robust RWOC problem, where D1 and D2 are not necessarily
exactly aligned, i.e., some data points inD1 may not correspond to any data point in D2. Specifically,
We relax the constraints on the permutation ∏(∙) (Liero et al., 2018) to automatically match related
data points and ignore the unrelated ones.
At last, We conduct thorough numerical experiments to demonstrate the effectiveness of ROBOT. For
RWOC (i.e., exact correspondence), We use several synthetic regression datasets and a real gated
floW cytometry dataset, and We shoW that ROBOT outperforms baseline methods by significant
margins. For robust RWOC (i.e., inexact correspondence), in addition to synthetic datasets, We
consider a vision-based multiple-object tracking task, and then We shoW that ROBOT also achieves
significant improvement over baseline methods.
Notations. Let ∣∣∙k 2 denote the '2 norm of Vectors,〈•，•)the inner product of matrices, i.e.,(A, Bi =
Pi,j Aij Bij for matrices A and B. ai:j are the entries from index i to index j of vector a. Let 1n
denote an n-dimensional vector of all ones. Denote 黜 the gradient of scalars, and ▽(,)(•) the
Jacobian of tensors. We denote [v1,v2] the concatenation of two vectors vι and v2. N(μ, σ2) is the
Gaussian distribution with mean μ and variance σ2.
2 ROBOT: A Hypergradient Approach for RWOC
We develop our hypergradient approach for RWOC. Specifically, we first introduce a continuous
formulation equivalent to (1), and then propose a smooth bi-level relaxation with an efficient hyper-
gradient descent algorithm.
2.1	Equivalent Continuous Formulation
We propose a continuous optimization problem equivalent to (1). Specifically, we rewrite an equiv-
alent form of (1) as follows,
minw minS∈Rn×n L(w, S) = hC (w), Si subject to S ∈ P,	(3)
where P denotes the set of all n × n permutation matrices, C(w) ∈ Rn×n is the loss matrix with
Cij(w) = kyi - f (xi,zj;w) k22.
Note that we can relax S ∈ P, which is the discrete feasible set of the inner minimization problem
of (3), to a convex set, without affecting the optimality, as suggested by the next theorem.
Proposition 1. Given any a ∈ Rn and b ∈ Rm, we define
Π(a, b) = {A ∈ Rn×m : A1m =a,A>1n = b, Aij ≥ 0}.
The optimal solution to the inner discrete minimization problem of (3) is also the optimal solution
to the following continuous optimization problem,
minS∈Rn×n hC (w), Si, s.t. S ∈ Π(1n, 1n).	(4)
This is a direct corollary of the Birkhoff-von Neumann theorem (Birkhoff, 1946; Von Neumann,
1953), and please refer to Appendix A for more details. Theorem 1 allows us to replace P in (3)
with Π(1n, 1n), which is also known as the Birkhoff polytope1(Ziegler, 2012). Accordingly, we
obtain the following continuous formulation,
minw minS∈Rn×n hC (w), Si subject to S ∈ Π(1n, 1n).	(5)
Remark 1. In general, equation 3 can be solved by linear programming algorithms (Dantzig, 1998).
2.2	Conventional Wisdom: Alternating Minimization
Conventional wisdom for solving (5) suggests to use alternating minimization (AM, Abid et al.
(2017)). Specifically, at the k-th iteration, we first update S by solving
S(k) = arg minS∈Π(1n,1n) L(w(k-1), S),
and then given S(k), we update w using gradient descent or exact minimization, i.e.,
W⑹=w(kT)- ηVwL(W(k-1), S⑹).
However, AM works poorly for solving (5) in practice. This is because w and S have a strong
interaction throughout the iterations: A slight change to W may lead to significant change to S.
Therefore, the optimization landscape is ill-conditioned, and AM can easily get stuck at local optima.
1This is a common practice in integer programming (Marcus & Ree, 1959).
3
Published as a conference paper at ICLR 2021
2.3	Smooth Bi-level Relaxation
To tackle the aforementioned computational challenge, we propose a hypergradient approach, which
can better handle the interaction between w and S. Specifically, we first relax (5) to a smooth bi-
level optimization problem, and then we solve the relaxed bi-level optimization problem using the
hypergradient descent algorithm.
We rewrite (5) as a smoothed bi-level optimization problem,
minw Fe(W) = hC (w), S*(w)i, subject to S*(w) = argmins∈∏qn,in)(C(w), S)+ EH (S), (6)
where H(S) = hlog S, Si is the entropy of S. The regularizer H(S) in equation 6 alleviates the
sensitivity of S * (W) to w. Note that if without such a regularize], We solve
S *(W) = argmins∈∏(1n,1n)hc (W),Si.	(J)
The resulting S* (W) can be discontinuous in W. This is because S* (W) is the optimal solution of
a linear optimization problem, and usually lies on a vertex of Π(1n, 1n). This means that if we
change W, S* (W) either stays the same or jumps to another vertex of Π(1n, 1n). The jump makes
S* (W) highly sensitive to W. To alleviate this issue, we propose to smooth S* (W) by adding an
entropy regularizer to the lower level problem. The entropy regularizer enforces Se* (W) to stay
in the interior of Π(1n, 1n), and Se* (W) changes smoothly with respect to W, as suggested by the
following theorem.
Theorem 1. For any E > 0, Se*(W) is differentiable, if the cost C(W) is differentiable with respect
to W. Consequently, the objective Fe(W) = hC(W), Se* (W)i is also differentiable.
The proof is deferred to Appendix C. Note that (6) provides us a new perspective to interpret the
relationship between W and S. As can be seen from (6), W and S have different priorities: W is the
parameter of the leader problem, which is of the higher priority; S is the parameter of the follower
problem, which is of the lower priority, and can also be viewed as an operator of W - denoted by
Se* (W). Accordingly, when we minimize (6) with respect to W using gradient descent, we should
also differentiate through Se*. We refer to such a gradient as “hypergradient” defined as follows,
Vw Fe(W)
∂Fe(w) ∂C(w)	∂Fe(W) ∂S*(w)
∂C(w) ∂w ∂ ∂S*(w) ∂w
VwL(W, S) +
∂Fe(w) ∂S*(w)
∂S*(w) ∂w
We further examine the alternating minimization algorithm from the bi-level optimization perspec-
tive: Since VwL(W(k-1), S(k)) is not differentiable through S(k), AM is essentially using an inexact
gradient. From a game-theoretic perspective2, (6) defines a competition between the leader W and
the follower S. When using AM, S only reacts to what W has responded. In contrast, when using
the hypergradient approach, the leader essentially recognizes the follower’s strategy and reacts to
what the follower is anticipated to response through ,F： (W) dS∂Ww). In this way, we can find a better
descent direction for W.
Remark 2. We use a simple example of quadratic minimization to illustrative why we expect the
bilevel optimization formulation in (6) to enjoy a benign optimization landscape. We consider a
quadratic function
L(a1, a2) = a>Pa + b>a,	(8)
where a1 ∈ Rd1, a2 ∈ Rd2, a = [a1, a2], P ∈ R(d1+d2)×(d1+d2), b ∈ Rd1 +d2. Let
P = ρ1d1+d2 1d> +d + (1 - ρ)Id1+d2, where Id1+d2 is the identity matrix, and ρ is a constant.
We solve the following bilevel optimization problem,
mina1 F(a1) = L(a1, a*2(a1)) subject to a*2(a1) = arg mina2 L(a1, a2) + λka2k22, (9)
where λ is a regularization coefficient. The next proposition shows that V2F (a1) enjoys a smaller
condition number than V2a1a1L(a1, a2), which corresponds to the problem that AM solves.
Proposition 2. Given F defined in (9), we have
λmax(V2F(aι)) =1 ι 1 - ρ + λ ∙必 and λmax(vαιaι L(ai,a2))	必
λmin(v2F(al))	d2p - P + λ + 1 1 - P	λmin(▽a】a】 L(a1, a2))	1 - P
The proof is deferred to Appendix B.1. As suggested by Proposition 2, F(a1) is much better-
conditioned than L(a1, a2) in terms of a1 for high dimensional settings.
2The bilevel formulation can be viewed as a Stackelberg game.
4
Published as a conference paper at ICLR 2021
2.4	S olving RWOC by Hypergradient Descent
We present how to solve (6) using our hypergradient approach. Specifically, we compute the “hy-
pergradient” of F (w) based on the following theorem.
Theorem 2. The gradient of F with respect to w is
1 n,n
Vw Fe(W)=-E
i,j=1
n,n
n,n
(I- CijMij+ E Ch'St,h'Phij + E Ch'Se,h'Q'ij	VwCij. (10)
h,'=1
h,'=1
The definition of P and Q and the proof is deferred to Appendix C. Theorem 2 suggests that we first
solve the lower level problem in (6),
s： =argmins∈∏(in,in)hC(w),Si + 冉(S),	(11)
and then substitute Se into (10) to obtain VwFe(w).
Note that the optimization problem in (11) can be efficiently solved by a variant of Sinkhorn al-
gorithm (Cuturi, 2013; Benamou et al., 2015). Specifically, (11) can be formulated as an entropic
optimal transport (EOT) problem (Monge, 1781; Kantorovich, 1960), which aims to find the optimal
way to transport the mass from a categorical distribution with weight μ = [μι,..., μn]> to another
categorical distribution with weight ν = [ν1, . . . , νm]>,
γ* = argminr∈∏(",ν)hM, ri + eH (γ),
with Π(μ, V) = {Γ ∈ Rn×m : Γ1m = μ, Γ>1n = ν, Γj ≥ 0},
(12)
where M ∈ Rn×m is the cost matrix with Mij the transport cost. When we set the two categorical
distributions as the empirical distribution of D1 and D2 , respectively,
M = C (W) and μ = V = 1n/n,
one can verify that (12) is a scaled lower problem of (6), and their optimal solutions satisfies Se* =
nΓ*. Therefore, we can apply Sinkhorn algorithm to solve the EOT problem in equation 12: At the
`-th iteration, we take
p('+1) = Gqh and q('+1) = G>P('+1), Where q⑼=-1n and Gij = exp (-Cij(W)),
G ∈ Rn×n , and the division here is entrywise. Let p* and q* denote the stationary points. Then we
obtain Se*,ij =npi*Gijqj*.
Remark 3. The Sinkhorn algorithm is iterative and cannot exactly solve (11) within finite steps. As
the Sinkhorn algorithm is very efficient and attains linear convergence, it suffices to well approxi-
mate the gradient VwFe (W) using the output inexact solution.
3	ROBOT for Robust Correspondence
We next propose a robust version of ROBOT to solve rRWOC (Varol & Nejatbakhsh, 2019). Note
that in (6), the constraint S ∈ Π(1n, 1n) enforces a one-to-one matching between D1 and D2. For
rRWOC, however, such an exact matching may not exist. For example, we have n < m, where
n = |D1|, m = |D2|. Therefore, we need to relax the constraint on S.
Motivated by the connection between (6) and (12), we propose to solve the lower problem3 4:
(S*(w),μ* ,P*) =argmins∈∏(μ,p) hC (w),Si + eH(S),	(13)
SUbjeCt to μ 1n = n, ν 1m = m, kμ - 1n k 2 ≤ ρ1, kν - 1m k 2 ≤ ρ2,
where Sr* (W) ∈ Rn×m denotes an inexact correspondence between D1 and D2 . As can be seen
in (13), We relax the marginal constraint Π(1,1) in (6) to Π(μ, P), where μ, V are required to not
deviate much from 1. Problem (13) relaxes the marginal constraints Π(1, 1) in the original problem
to Π(μ, V), where ”, V are picked such that they do not deviate too much from 14. Illustrative
examples of the exact and robust alignments are provided in Figure 1.
3The idea is inspired by the marginal relaxation of optimal transport, first independently proposed by Kon-
dratyev et al. (2016) and Chizat et al. (2018a), and later developed by Chizat et al. (2018c); Liero et al. (2018).
Chizat et al. (2018b) share the same formulation as ours.
4Here we measure the deviation using the Euclidean distance, and more detailed discussions can be found
in Appendix F
5
Published as a conference paper at ICLR 2021
Computationally, (13) can be solved by taking the Sinkhorn iteration and the projected gradient
iteration in an alternating manner (See more details in Appendix D). Given S* (w), We solve the
upper level optimization in (6) to obtain w*, i.e.,
w* = argm⅛w〈C (w),S* (w))∙
Similar to the previous section, we use a first-order algorithm to solve this problem, and we derive
explicit expressions for the update rules. See Appendix E for details.
Figure 1: Illustrative example of exact (L) and robust (R) alignments. The robust alignment can drop
potential outliers and only match data points close to each other.
4	Experiment
We evaluate ROBOT and ROBOT-
robust on both synthetic and real-
world datasets, including flow cy-
tometry and multi-object tracking.
We first present numerical results
and then we provide insights in the
discussion section. Experiment de-
tails and auxiliary results can be
found in Appendix G.
4.1	Unlabeled Sensing
Figure 2: Unlabeled sensing. Results are the mean over 10
runs. SNR= ∣∣wk2/pnoise is the signal-to-noise ratio.
Data Generation. We follow the unlabeled sensing setting (Tsakiris & Peng, 2019) and generate
n = 1000 data points {(yi, zi)}in=1, where zi ∈ Re. Note here we take d = 0. We first generate
Zi, w 〜N(0e, Ie), and εi 〜N(0, PnoiSe). Then we compute yi = z>W + εi. We randomly permute
the order of 50% of zi so that we lose the Z-to-Y correspondence. We generate the test set in the
same way, only without permutation.
Baselines and Training. We consider the following scalable methods:
1.	Oracle: Standard linear regression where no data are permuted.
2.	Least Squares (LS): Standard linear regression, i.e., treating the data as if they are not permuted.
3.	Alternating Minimization (AM, Abid et al. (2017)): We iteratively solve the correspondence
given w, and update w using gradient descent with the correspondence.
4.	Stochastic EM (Abid & Zou, 2018): A stochastic EM approach to recover the permutation.
5.	Robust Regression (RR, Slawski & Ben-David (2019); Slawski et al. (2019a)). A two-stage
block coordinate descent approach to discard outliers and fit regression models.
6.	Random Sample (RS, Varol & Nejatbakhsh (2019)): A random sample consensus (RANSAC)
approach to estimate w.
We initialize AM, EM and ROBOT using the output of RS with multi-start. We adopt a linear
model f (Z; w) = Z>w. Models are evaluated by the relative error on the test set, i.e., error =
Pi(yi — yi)2/ Pi (yi — y)2, where yi is the predicted label, and y is the mean of {yi}.
Results. We visualize the results in Figure 2. In all the experiments, ROBOT achieves better results
than the baselines. Note that the relative error is larger for all methods except Oracle as the dimen-
sion and the noise increase. For low dimensional data, e.g., e = 5, our model achieves even better
performance than Oracle. We have more discussions on using RS as initializations in Appendix G.5.
4.2	Nonlinear Regression
Data Generation. We mimic the scenario where the dataset is collected from different platforms.
Specifically, we generate n data points {(yi , [xi, zi])}in=1, where xi ∈ Rd and zi ∈ Re. We first
generate Xi 〜N(0d, Id), Zi 〜N(0e, Ie), W 〜N(0d+e, Id+e), and εi 〜N(θ,pnnoise). Then we
6
Published as a conference paper at ICLR 2021
0.0
_ 0.0
e -0.5
士 -1.0
⅛-1.5
⅛-2-0
Figure 3:
compute yi = f ([xi , zi]; w) + εi . Next, we randomly permute the order of {zi} so that we lose
the data correspondence. Here, D1 = {(xi, yi)} and D2 = {zj} mimic two parts of data collected
from two separate platforms. Since we are interested in the response on platform one, we treat all
data from platform two, i.e., D2, as well as 80% of data in D1 as the training data. The remaining
data from D1 are the test data. Notice that we have different number of data on D1 and D2, i.e., the
correspondence is not exactly one-to-one.
Baselines and Training. We consider a nonlinear function f(X, Z; w) = Pkd=1 sin ([X, Z]kwk).
In this case, we consider only two baselines — Oracle and LS, since the other baselines in the
previous section are designed for linear models. We evaluate the regression models by the transport
cost divided by P/y% - y)2 on the test set.
Results. As shown in Figure 3, ROBOT-robust consistently outperforms ROBOT and LS, demon-
strating the effectiveness of our robust formulation. Moreover, ROBOT-robust achieves better per-
formance than Oracle when the number of training data is large or when the noise level is high.
4.3	Flow Cytometry
In flow cytometry (FC), a sample containing particles
is suspended in a fluid and injected into the flow cy-
tometer, but the measuring instruments are unable to pre-
serve the correspondence between the particles and the
measurements. Different from FC, gated flow cytometry
(GFC) uses “gates” to sort the particles into one of many
bins, which provides partial ordering information since the
measurements are provided individually for each bin. In
practice, there are usually 3 or 4 bins.
Settings. We adopt the dataset from Knight et al. (2009).
Following Abid et al. (2017), the outputs yi ’s are normal-
⑶FC
0.080
0.075
0.070
0.065
0.060
0.055
0.050
0.045
0.040-
(b) GFC
Figure 4: Relative error of different
methods.
ized, and We select the top 20 significant features by a linear regression on the top 1400 items in the
dataset. We use 90% of the data as the training data, and the remaining as test data. For ordinary
FC, we randomly shuffle all the labels in the training set. For GFC, the training set is first sorted by
the labels, and then divided into equal-sized groups, mimicking the sorting by gates process. The
labels in each group are then randomly shuffled. To simulate gating error, 1% of the data are shuf-
fled across the groups. We compare ROBOT with Oracle, LS, Hard EM (a variant of Stochastic EM
proposed in Abid & Zou (2018)), Stochastic EM, and AM. We use relative error on the test set as
the evaluation metric.
Results. As shown in Figure 4, while AM achieves good performance on GFC when the number of
groups is 3, it behaves poorly on the FC task. ROBOT, on the other hand, is efficient on both tasks.
4.4	Multi-Object Tracking
In this section we extend our method to vision-based Multi-Object Tracking (MOT), a task with
broad applications in mobile robotics and autonomous driving, to show the potential of applying
RWOC to more real-world tasks. Given a video and the current frame, the goal of MOT is to
predict the locations of the objects in the next frame. Specifically, object detectors (Felzenszwalb
et al., 2009; Ren et al., 2015) first provide us the potential locations of the objects by their bounding
boxes. Then, MOT aims to assign the bounding boxes to trajectories that describe the path of
individual objects over time. Here, we formulate the current frame and the objects’ locations in the
current frame as D2 = {zj }, while we treat the next frame and the locations in the next frame as
D1 = {(xi, yi)}.
7
Published as a conference paper at ICLR 2021
Table 1: Experiment results on MOT.
Data	Method	MOTA↑ MOTP↑ IDF1↑ MT↑ MLL FPZ						FNL	IDSL
MOT17 (train)	ROBOT	48.3	82.6	55.3	407	553	22,443	149,988 1,811
	w/o ROBOT	44.0	81.3	49.9	404	550	36,187	149,131 3,204
	ROBOT	48.2	76.6	43.4	455	904	29,419	259,714 3,228
MOT17 (dev)	w/o ROBOT	42.1	75.0	36.8	414	890	61,210	259,318 6,138
	SORT	43.1	77.8	39.8	295	997	28,398	287,582 4,852
MOT20 (train)	ROBOT	56.2	84.9	47.6	805	288	113,752 377,247 5,888	
	w/o ROBOT	48.8	81.5	40.2	769	290	186,245	384,562 10,153
	ROBOT	45.0	76.9	34.0	394	257	70,416	210,425 3,683
MOT20 (dev)	w/o ROBOT	38.5	75.1	27.0	383	233	104,958 207,627 5,696	
	SORT	42.7	78.5	45.1	208	326	27,521	264,694 4,470
Existing deep learning based MOT algorithms require
large amounts of annotated data, i.e., the ground truth
of the correspondence, during training. Different from
them, our algorithm does not require the correspon-
dence between D1 and D2, and all we need is the
video. This task is referred to as unsupervised MOT
(He et al., 2019).
Related Works. To the best of our knowledge, the
only method that accomplishes unsupervised end-to-
end learning of MOT is He et al. (2019). How-
ever, it targets tracking with low densities, e.g.,
Sprites-MOT, which is different from our focus.
Figure 5: One frame in MOT20 with de-
tected bounding boxes in yellow.
Settings. We adopt the MOT17 (Milan et al., 2016) and the MOT20 (Dendorfer et al., 2020) datasets.
Scene densities of the two datasets are 31.8 and 170.9, respectively, which means the scenes are
pretty crowded as we illustrated in Figure 5. We adopt the DPM detector (Felzenszwalb et al., 2009)
on MOT17 and the Faster-RCNN detector (Ren et al., 2015) on MOT20 to provide us the bounding
boxes. Inspired by Xu et al. (2019b), the cost matrix is computed as the average of the Euclidean
center-point distance and the Jaccard distance between the bounding boxes,
Cij(W) = 1( kc(j+W(yi)k2+J (f(Zj; w),%))，
where c(∙) is the location of the box center, H and W are the height and the width of the video
frame, and J(•，∙)is the JaCCard distance defined as 1-IoU (InterseCtion-over-Union). We utilize
the single-object tracking model SiamRPN5 (Li et al., 2018) as our regression modelf. We apply
ROBOT-robust with ρ1 = ρ2 = 10-3. See Appendix G for more detailed settings.
Results. We demonstrate the experiment results in Table 1, where the evaluation metrics follow
Ristani et al. (2016). In the table, ↑ represents the higher the better, and ] represents the lower the
better. ROBOT signifies the model trained by ROBOT-robust, and w/o ROBOT means the pretrained
model in Li et al. (2018). The scores are improved significantly after training with ROBOT-robust.
We also include the scores of the SORT model (Bewley et al., 2016) obtained from the dataset
platform. Different from SiamRPN and SiamRPN+ROBOT, SORT is a supervised learning model.
As shown, our unsupervised training framework achieves comparable or even better performance.
5 Discussion
Sensitivity to initialization. As stated in Pananjady et al. (2017b), obtaining the global optima of
(1) is in general an NP-hard problem. Some “global” methods use global optimization techniques
and have exponential complexity, e.g., Elhami et al. (2017), which is not applicable to large data.
The other “local” methods only guarantee converge to local optima, and the convergence is very
sensitive to initialization. Compared with existing “local” methods, our method is computationally
efficient and greatly reduces the sensitivity to initialization.
5The initial weights of f are obtained from https://github.com/foolwood/DaSiamRPN.
8
Published as a conference paper at ICLR 2021
To demonstrate such an advantage, we run AM and
ROBOT with 10 different initial solutions, and then we
sort the results based on (a) the averaged residual on
the training set, and (b) the relative prediction error on
the test set. We plot the percentiles in Figure 6. Here
we use fully shuffled data under the unlabeled sensing
setting, and we set n = 1000, e = 5, ρ2noise = 0.1, and
= 10-2. We can see that ROBOT can find “good”
solutions in 30% of the cases (The relative prediction
error is smaller than 1), but AM is more sensitive to
the initialization and cannot find “good” solutions.
ROBOT v.s. Automatic Differ-
entiation (AD). Our algorithm
computes the Jacobian matrix di-
rectly based on the KKT condi-
tion of the lower problem (11).
An alternative approach to ap-
proximate the Jacobian is the
automatic differentiation through
the Sinkhorn iterations for updat-
ing S when solving (11). As sug-
gested by Figure 7 (a), running
Sinkhorn iterations until conver-
4 3 2 1
PBnPlSox .dw
25	50	75
PerCentile
(a) Training residual
2-
1
25	50	75
Percentile
(b) Test error
Figure 6: Results of different initialization
ofAM and ROBOT.
(a)	(b)	(c)
Figure 7: The comparisons to AD. (a) Convergence under differ-
ent number of Sinkhorn iterations of AD. (b) Time comparison.
(c) Memory comparison.
gence (200 Sinkhorn iterations) can lead to a better solution6. In order to apply AD, we need to
store all the intermediate updates of all the Sinkhorn iterations. This require the memory usage to be
proportional to the number of iterations, which is not necessarily affordable. In contrast, applying
our explicit expression for the backward pass is memory-efficient. Moreover, we also observe that
AD is much more time-consuming than our method. The timing performance and memory usage
are shown in Figure 7 (b)(c), where We set n = 1000.
Connection to EM. Abid & Zou (2018) adopt an Expectation Maxi-
mization (EM) method for RWOC, where S is modeled as a latent ran-
dom variable. Then in the M-step, one maximizes the expected likeli-
hood of the data over S. This method shares the same spirit as ours: We
avoid updating w using one single permutation matrix like AM. How-
ever, this method is very dependent on a good initialization. Specifically,
if we randomly initialize w, the posterior distribution of S in this itera-
tion would be close to its prior, which is a uniform distribution. In this
Figure 8: Expected Cor-
respondence in EM.
way, the follow-up update for w is not informative. Therefore, the solution of EM would quickly
converge to an undesired stationary point. Figure 8 illustrates an example of converged correspon-
dence, where we adopt n = 30, o = e = 1, d = 0. For this reason, we initialize EM with good
initial points, either by RS or AM throughout all experiments.
Related works with additional constraints. There is another line of research which improves
the computational efficiency by solving variants of RWOC with additional constraints. Specifically,
Haghighatshoar & Caire (2017); Rigollet & Weed (2018) assume an isotonic function (note that such
an assumption may not hold in practice), and Shi et al. (2018); Slawski & Ben-David (2019); Slawski
et al. (2019a;b); Varol & Nejatbakhsh (2019) assume only a small fraction of the correspondence is
missing. Our method is also applicable to these problems, as long as the additional constraints can
be adapted to the implicit differentiation.
More applications of RWOC. RWOC problems generally appear for two reasons. First, the mea-
suring instruments are unable to preserve the correspondence. In addition to GFC and MOT, we
list a few more examples: SLAM tracking (Thrun, 2007), archaeological measurements (Robinson,
1951), large sensor networks (Keller et al., 2009), pose and correspondence estimation (David et al.,
2004), and the genome assembly problem from shotgun reads (Huang & Madan, 1999). Second, the
data correspondence is masked for privacy reasons. For example, we want to build a recommender
system for a new platform, borrowing user data from a mature platform.
6We remark that running one iteration sometimes cannot converge.
9
Published as a conference paper at ICLR 2021
Acknowledgement
This works is partially supported by NSF IIS-2008334. Hongteng Xu is supported in part by Beijing
Outstanding Young Scientist Program (NO. BJJWZYJH012019100020098) and National Natural
Science Foundation of China (No. 61832017). Xiaojing Ye is partially supported by NSF DMS-
1925263. Hongyuan Zha is supported in part by a grant from Shenzhen Institute of Artificial Intel-
ligence and Robotics for Society. We also appreciate the fruitful discussions with Bo Dai and Yan
Li.
References
Abubakar Abid and James Zou. A stochastic expectation-maximization approach to shuffled linear
regression. In 2018 56th Annual Allerton Conference on Communication, Control, and Comput-
ing (Allerton),pp. 470-477. IEEE, 2018.
Abubakar Abid, Ada Poon, and James Zou. Linear regression with shuffled labels. arXiv preprint
arXiv:1705.01342, 2017.
Jean-David Benamou, GUillaUme Carlier, Marco Cuturi, LUca Nenna, and Gabriel Peyre. Iterative
bregman projections for regularized transportation problems. SIAM Journal on Scientific Com-
puting, 37(2):A1111-A1138, 2015.
Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, and Ben Upcroft. Simple online and realtime
tracking. In 2016 IEEE International Conference on Image Processing (ICIP), pp. 3464-3468.
IEEE, 2016.
Garrett Birkhoff. Three observations on linear algebra. Univ. Nac. Tacuman, Rev. Ser. A, 5:147-151,
1946.
Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. Siam Review, 60(2):223-311, 2018.
Lenaic Chizat, Gabriel Peyre, Bernhard Schmitzer, and FranCois-Xavier Vialard. An interpolating
distance between optimal transport and fisher-rao metrics. Foundations of Computational Math-
ematics, 18(1):1-44, 2018a.
Lenaic Chizat, Gabriel Peyre, Bernhard Schmitzer, and FranCois-Xavier Vialard. Scaling algorithms
for unbalanced optimal transport problems. Mathematics of Computation, 87(314):2563-2609,
2018b.
LenaIC Chizat, Gabriel Peyre, Bernhard Schmitzer, and FranCoiS-Xavier Vialard. Unbalanced opti-
mal transport: Dynamic and kantorovich formulations. Journal of Functional Analysis, 274(11):
3090-3123, 2018c.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in
neural information processing systems, pp. 2292-2300, 2013.
George Bernard Dantzig. Linear programming and extensions, volume 48. Princeton university
press, 1998.
Philip David, Daniel Dementhon, Ramani Duraiswami, and Hanan Samet. Softposit: Simultaneous
pose and correspondence determination. International Journal of Computer Vision, 59(3):259-
284, 2004.
P. Dendorfer, H. Rezatofighi, A. Milan, J. Shi, D. Cremers, I. Reid, S. Roth, K. Schindler,
and L. Leal-Taixe. Mot20: A benchmark for multi object tracking in crowded scenes.
arXiv:2003.09003[cs], March 2020. URL http://arxiv.org/abs/1906.04567. arXiv:
2003.09003.
John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. Efficient projections onto
the l 1-ball for learning in high dimensions. In Proceedings of the 25th international conference
on Machine learning, pp. 272-279, 2008.
10
Published as a conference paper at ICLR 2021
Golnooshsadat Elhami, Adam James Benjamin, Bejar Haro, and Martin Vetterli. Unlabeled sensing:
Reconstruction algorithm and theoretical guarantees. In 2017 42nd IEEE International Confer-
ence on Acoustics, Speech and Signal Processing, 2017.
Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva Ramanan. Object detection
with discriminatively trained part-based models. IEEE transactions on pattern analysis and ma-
chine intelligence, 32(9):1627-1645, 2009.
Tanner Fiez, Benjamin Chasnov, and Lillian J Ratliff. Convergence of learning dynamics in stack-
elberg games. arXiv preprint arXiv:1906.01217, 2019.
Saeid Haghighatshoar and Giuseppe Caire. Signal recovery from unlabeled samples. IEEE Trans-
actions on Signal Processing, 66(5):1242-1257, 2017.
Zhen He, Jian Li, Daxue Liu, Hangen He, and David Barber. Tracking by animation: Unsupervised
learning of multi-object attentive trackers. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 1318-1327, 2019.
Daniel J Hsu, Kevin Shi, and Xiaorui Sun. Linear regression without correspondence. In Advances
in Neural Information Processing Systems, pp. 1531-1540, 2017.
Xiaoqiu Huang and Anup Madan. Cap3: A dna sequence assembly program. Genome research, 9
(9):868-877, 1999.
Leonid Vitaliyevich Kantorovich. Mathematical methods of organizing and planning production.
Management science, 6(4):366-422, 1960.
Lorenzo Keller, M Jafari Siavoshani, Christina Fragouli, Katerina Argyraki, and Suhas Diggavi.
Identity aware sensor networks. In IEEE INFOCOM 2009, pp. 2177-2185. IEEE, 2009.
Christopher G Knight, Mark Platt, William Rowe, David C Wedge, Farid Khan, Philip JR Day, Andy
McShea, Joshua Knowles, and Douglas B Kell. Array-based evolution of dna aptamers allows
modelling of an explicit sequence-fitness landscape. Nucleic acids research, 37(1):e6-e6, 2009.
Stanislav Kondratyev, Leonard Monsaingeon, Dmitry Vorotnikov, et al. A new optimal transport
distance on the space of finite radon measures. Advances in Differential Equations, 21(11/12):
1117-1164, 2016.
Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, and Xiaolin Hu. High performance visual tracking with
siamese region proposal network. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 8971-8980, 2018.
Matthias Liero, Alexander Mielke, and GiUsePPe Savare. Optimal entropy-transport problems and
a new hellinger-kantorovich distance between positive measures. Inventiones mathematicae, 211
(3):969-1117, 2018.
GiUlia LUise, Alessandro RUdi, Massimiliano Pontil, and Carlo Ciliberto. Differential properties of
sinkhorn approximation for learning with wasserstein distance. In Advances in Neural Informa-
tion Processing Systems, pp. 5859-5870, 2018.
Marvin MarcUs and Rimhak Ree. Diagonals of doUbly stochastic matrices. The Quarterly Journal
of Mathematics, 10(1):296-302, 1959.
A. Milan, L. Leal-Taixe, I. Reid, S. Roth, and K. Schindler. MOT16: A benchmark for multi-object
tracking. arXiv:1603.00831 [cs], March 2016. URL http://arxiv.org/abs/1603.
00831. arXiv: 1603.00831.
Gaspard Monge. MemOire sur la theorie des deblais et des remblais. Histoire de YAcademie Royale
des Sciences de Paris, 1781.
Ashwin Pananjady, Martin J Wainwright, and Thomas A Courtade. Linear regression with an un-
known permutation: Statistical and computational limits. In 2016 54th Annual Allerton Confer-
ence on Communication, Control, and Computing, pp. 417—-424, 2016.
11
Published as a conference paper at ICLR 2021
Ashwin Pananjady, Martin J Wainwright, and Thomas A Courtade. Denoising linear models with
permuted data. In 2017 IEEE International Symposium on Information Theory (ISIT), pp. 446-
450. IEEE, 2017a.
Ashwin Pananjady, Martin J Wainwright, and Thomas A Courtade. Linear regression with shuf-
fled data: Statistical and computational limits of permutation recovery. IEEE Transactions on
Information Theory, 64(5):3286-3300, 2017b.
Liangzu Peng and Manolis C Tsakiris. Linear regression without correspondences via concave
minimization. arXiv preprint arXiv:2003.07706, 2020.
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object
detection with region proposal networks. In Advances in neural information processing systems,
pp. 91-99, 2015.
P. Rigollet and J. Weed. Uncoupled isotonic regression via minimum wasserstein deconvolution.
arXiv preprint arXiv:1806.10648, 2018.
Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara, and Carlo Tomasi. Performance mea-
sures and a data set for multi-target, multi-camera tracking. In European Conference on Computer
Vision, pp. 17-35. Springer, 2016.
William S Robinson. A method for chronologically ordering archaeological deposits. American
antiquity, 16(4):293-301, 1951.
X. Shi, X. Lu, and T. Cai. Spherical regresion under mismatch corruption with application to auto-
mated knowledge translation. arXiv preprint arXiv:1810.05679, 2018.
Richard Sinkhorn and Paul Knopp. Concerning nonnegative matrices and doubly stochastic matri-
ces. Pacific Journal of Mathematics, 21(2):343-348, 1967.
M. Slawski and E. Ben-David. Linear regression with sparsely permuted data. Electronic Journal
of Statistics, 2019.
M. Slawski, E. Ben-David, and P. Li. A two-stage approach to multivariate linear regression with
sparsely mismatched data. arXiv preprint arXiv:1907.07148, 2019a.
Martin Slawski, Mostafa Rahmani, and Ping Li. A sparse representation-based approach to linear
regression with partially shuffled labels. In 35th Conference on Uncertainty in Artificial Intelli-
gence, UAI 2019, 2019b.
Jeffrey M Stanton. Galton, pearson, and the peas: A brief history of linear regression for statistics
instructors. Journal of Statistics Education, 9(3), 2001.
Sebastian Thrun. Simultaneous localization and mapping. In Robotics and cognitive approaches to
spatial mapping, pp. 13-41. Springer, 2007.
Manolis Tsakiris and Liangzu Peng. Homomorphic sensing. In Kamalika Chaudhuri and Ruslan
Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning,
volume 97 of Proceedings of Machine Learning Research, pp. 6335-6344, Long Beach, Cali-
fornia, USA, 09-15 Jun 2019. PMLR. URL http://proceedings.mlr.press/v97/
tsakiris19a.html.
Manolis C Tsakiris, Liangzu Peng, Aldo Conca, Laurent Kneip, Yuanming Shi, Hayoung Choi, et al.
An algebraic-geometric approach to shuffled linear regression. arXiv preprint arXiv:1810.05440,
2018.
Jayakrishnan Unnikrishnan, Saeid Haghighatshoar, and Martin Vetterli. Unlabeled sensing with
random linear measurements. IEEE Transactions on Information Theory, 64(5):3237-3253, 2018.
Erdem Varol and Amin Nejatbakhsh. Robust approximate linear regression without correspondence.
arXiv preprint arXiv:1906.00273, 2019.
Titouan Vayer, Remi Flamary, Romain Tavenard, Laetitia Chapel, and Nicolas Courty. Sliced
gromov-wasserstein. arXiv preprint arXiv:1905.10124, 2019.
12
Published as a conference paper at ICLR 2021
John Von Neumann. A certain zero-sum two-person game equivalent to the optimal assignment
problem. Contributions to the Theory of Games, 2(0):5-12,1953.
Mengdi Wang, Yichen Chen, Jialin Liu, and Yuantao Gu. Random multi-constraint projection:
Stochastic gradient methods for convex optimization with many constraints. arXiv preprint
arXiv:1511.03760, 2015.
Yujia Xie, Hanjun Dai, Minshuo Chen, Bo Dai, Tuo Zhao, Hongyuan Zha, Wei Wei, and Tomas
Pfister. Differentiable top-k operator with optimal transport. arXiv preprint arXiv:2002.06504,
2020.
Hongteng Xu, Dixin Luo, Hongyuan Zha, and Lawrence Carin. Gromov-wasserstein learning for
graph matching and node embedding. arXiv preprint arXiv:1901.06003, 2019a.
Yihong Xu, Yutong Ban, Xavier Alameda-Pineda, and Radu Horaud. Deepmot: A differentiable
framework for training multiple object trackers. arXiv preprint arXiv:1906.06618, 2019b.
Gunter M Ziegler. Lectures on polytopes, volume 152. Springer Science & Business Media, 2012.
13
Published as a conference paper at ICLR 2021
A Connection between OT and RWOC
Theorem 1.	Denote Π(a, b) = {S ∈ Rn×m : S1m = a, S> 1n = b, Sij ≥ 0} for any a ∈ Rn and
b ∈ Rm . Then at least one of the optimal solutions of the following problem lies in P.
minS∈Rn×n hC (w), Si, s.t. S ∈ Π(1n, 1n).	(14)
Proof. Denote the optimal solution of (14) as Z*. As We mentioned earlier, this is a direct
corollary of Birkhoff-Von Neumann theorem (Birkhoff, 1946; Von Neumann, 1953). Specifically,
Birkhoff-Von Neumann theorem claims that the polytope Π(1n, 1n) is the convex hull of the set
of n × n permutation matrices, and furthermore that the vertices of Π(1n, 1n) are precisely the
permutation matrices.
On the other hand, (14) is a linear optimization problem. There Would be at least one optimal
solutions lies at the Vertices giVen the problem is feasible. As a result, there Would be at least one
Z* being a permutation matrix.	□
B Two Perspectives of the Motivations of B ilevel Optimization
B.1 Faster Convergence
The bileVel optimization formulation has a better gradient descent iteration complexity than alter-
nating minimization. To see this, consider a quadratic function F (a1, a2) = a>Pa + b>a, Where
a1 ∈ Rd1, a2 ∈ Rd2, a = [a1>, a2>]> ∈ R(d1+d2), P ∈ R(d1+d2)×(d1+d2), b ∈ R(d1+d2). To further
simplify the discussion, We assume P = ρ1(d1+d2)1(>d1+d2) + (1 - ρ)Id1+d2, Where Id1+d2 is the
identity matrix. Then We haVe the folloWing proposition.
Proposition 1. GiVen F defined in (9), We haVe
λmax(V2F(OI)) = 1 + 1 - P + λ d1 P and	λmax"αιaι L(a1, a2) = 1 + d1P
λmin(V2F(aI))	1 - P d2p - P + λ + 1	λmin(▽a】a】 L(a1 ,a2力	1 - P
Proof. For alternating minimization, the Hessian for a1 is a submatrix of P, i.e.,
HAM = P1d1 1d>1 + (1 - P)Id1 ,
Whose condition number is
CAM = 1 + 1d⅜
We noW compute the condition number for ROBOT. Denote
P
P11	P12
P21	P22 ,
b1
b2 ,
b
Where P11 ∈ Rd1×d1, P12 ∈ Rd1×d2, P21 ∈ Rd2×d1, P22 ∈ Rd2×d2, and b1 ∈ Rd1, b2 ∈ Rd2.
ROBOT first minimize oVer a2,
a2*(a1) = argminF(a1, a2) = -(P22 + λId2)-1 (P21 a1 + b2/2).
a2
Substituting a2*(a1) into F(a1, a2), We can obtain the Hessian for a1 is
HROBOT = P11 - P12(P22 + λId2)-1 P21.
Using Sherman-Morrison formula, We can explicitly express P2-21 as
P-1 = 1-P + λId2 -
Substituting it into HROBOT,
P
(1 - P + λ)(1 - P+ λ + Pd2)
1d21d>2.
HROBOT = P11 - P12P2-21P21 = (1 - P)Id1 + P -
d2P2
d2P - P + λ + 1
1d11d>1.
Therefore, the condition number is
CROBOT = 1 +
1 -P+λ
d1P
1 - P	d2 P - P + λ + 1 .
□
14
Published as a conference paper at ICLR 2021
Note that CAM increases linearly with respect to d1. Therefore, the optimization problem inevitably
becomes ill-conditioned as dimension increase. In contrast, CROBOT can stay in the same order of
magnitude when d1 and d2 increase simultaneously.
Since the iteration complexity of gradient descent is proportional to the condition number (Bottou
et al., 2018), ROBOT needs fewer iterations to converge than AM.
C Differentiability
Theorem 2.	For any e > 0, S*(w) is differentiable, as long as the cost C(W) is differentiable with
respect to w. As a result, the objective Le(W) = hC(w),S鼠Wyi is also differentiable.
Proof. The proof is analogous to Xie et al. (2020).
We first prove the differentiability of S*(w). This part of proof mirrors the proof in Luise et al.
(2018). By Sinkhorn’s scaling theorem (Sinkhorn & Knopp, 1967),
Se (W) = diag(eξ^w) )e-Cw)diag(eZ^w).
Therefore, since Cij(W) is differentiable, Γe,e is differentiable if (ξe(w), Ze(W)) is differentiable as
a function of W.
Let us set
n,m
L(ξ,ζ； μ, ν, C) = ξτμ + ζTV - e X e--j F "
i,j=1
and recall that (ξe, Ze) = argmaxξ< L(ξ, Z; μ, ν, C). The differentiability of (ξe, Ze) is proved
using the Implicit Function theorem and follows from the differentiability and strict convexity in
(ξe, Ze) of the function L.	□
Theorem 3.	Denoting Le = hC(W), See(W)i. The gradient of Le with respect to W is
1	n,n	n,n	dξe	n,n	dZe
▽w Le= - X I (1 - Cij )Se,ij + X Ch'se,h' dCΓ + X Ch'Se,h' -C- I
dC	dC
i,j = 1 ∖	h,'=1	ij h,'=1	ij )
VwCij, (15)
,∣"Vc ξ*"∣	Γ-H-1D
where ▽CCZe =	0
n 1 ∫δ∕ij,
D'ij = 乂…,
with
' =1,…，n;
' =n + 1,…
- H-1D ∈ R(2n-1)×n×n, 0 ∈ R1×n×n
, 2n - 1,
H-1 = -
(diag(μ))-1 + (diag(μ))-1S？∣sK-1S∕∣sT (diag(μ))-1
-KTSrT (diag(μ))-1
-(diag(μ))-1SΓ*K-1
K-1
and K = diag(P) - SST(diag(μ))-1^¾,	D = vi：n — i,	SS = S‰n,Ln-ι∙
Proof. This result is straightforward combining the Sinkhorn’s scaling theorem and Theorem 3 in
XieetaL(2020).	□
D Algorithm of the Forward Pass for ROBOT-robust
For better numerical stability, in practice we add two more regularization terms,
s；(w),""L = argmins∈∏(μM, μ,ν∈∆n hC(w),si + EH(S)+ €1 h(μ) + ⑦九⑸	(I6)
s.t. F (μ, μ) ≤ ρι, F (V, v) ≤ ρ2,
where h(μ) = Pi μ% log μi is the entropy function for vectors. This can avoid the entries of μ and V
shrink to zeros when updated by gradient descent. We remark that since we have entropy term H(S),
the entries of S would not be exactly zeros. Furthermore, We have μ = S1 and μ = S1. Therefore,
theoretically the entries of μ and V will not be zeros. We only add the two more entropy terms for
numerical consideration. The detailed algorithm is in Algorithm 1. Although the algorithm is not
guaranteed to converge to a feasible solution, in practice it usually converges to a good solution
(Wang et al., 2015).
15
Published as a conference paper at ICLR 2021
Algorithm 1 Solving Sr for robust matching
Require: C ∈ Rm×n, μ, ν, K, e, L, η
—	—Cij
Gij = e £
μ = μ,V = V
b = In
for l = 1, ∙ , L do
a = μ∕(Gb), b = V/(GTa)
μ = μ — η(e£ + €i * log"), V = V — η(e£ + e2 * log V)
μ = max{μ, 0}, V = max{ν, 0}
μ =_ μ∕(μ>D,v = ν∕(ν>D
if kμ 一 μk2 > ρι then
μ = μ+√ρ1 k⅛
end if
if kv — v∣∣2 > ρ2 then
V = V + √ρ2 k⅛
end if
end for
S = diag(α) Θ G Θ diag(b)
E Algorithm of THE Backward Pass for ROBOT-ROBUST
Since the derivation is tedious, We first summarize the outline of the derivation, then provide the
detailed derivation.
E.1 Summary
Given μr, Vr, Sr(W), we compute the Jacobian matrix dS'r(w)∕dw using implicit differentiation
and differentiable programming techinques. Specifically, the Lagrangian function of Problem (16)
is
L =hc, Si + CH(S) + eιh(μ) + e2h(v) — ξ>(Γlm — μ) — ζτ(Γτln — V)
+ λ1(μ>1n — 1) + λ2(v>1m — 1) + λ3(∣μ — μ∣∣2 — P1) + λ4(∣V — V||2 — p2)∙
where ξ and Z are dual variables. The KKT conditions (Stationarity condition) imply that the optimal
solution Γ*,e can be formulated using the optimal dual variables ξr and Zr as,
Sr = diag(e J )e-F diag(e ;).	(17)
By the chain rule, we have
dS； _ dS； dc _( ∂Sr 吃 dξ* bs； dζr ∖ dc
dw = ^dCdW =(与C + 页dc + BFdcd dw'
Therefore, we can compute dS； (w)∕dw if we obtain 凭 and 务.
Substituting (17) into the Lagrangian function, at the optimal solutions we obtain
L = L(ξr,Z *",v*H,λ2 ,λr,λ4; c).
Denote rr = [(ξr)>,(Zr)>, (μ)>, (v)>, λ1 ,λ2,λ3,X]>,and φ(r*; C) = ∂L(rr; C)M Atthe
optimal dual variable rr, the KKT condition immediately yields φ(rr; C) ≡ 0. By the chain rule,
we have
dφ(rr; C) _ ∂φ(rr; c) ι ∂φ(rr; C) drr
—dC — = —∂C — + 一∂rr — dC
Rerranging terms, we obtain
dr*	( ∂φ(rr; C) )-1 ∂φ(rr; C)
dc =  V-∂r*- ∂	∂C
Combining (17), (18), and (19), we can then obtain dSr(w)∕dw.
(18)
(19)
16
Published as a conference paper at ICLR 2021
E.2 Details
Now We provide the detailed derivation for computing dS；/dw.
Since Sr is the optimal solution of an optimization problem, we can follow the implicit function
theorem to solve for the closed-form expression of the gradient. Specifically, we adopt F(μ, V)=
Pi(pi — μi)2, and rewrite the optimization problem as
mVS i 十 WSij dog Sij
i
—I) + e1 ɪ2 μi(lθg μi — I) + e2 ɪ2 νj (IOg Vj - I),
ij
s.t., ESij = Pi,	ESij = Vj,
j
X μi = 1,
i
i
X Vj =1,
j
£(“i -Mi)2 ≤ ρ1,	E(Vj -Vj)2 ≤ ρ2∙
ij
The Language of the above problem is
L(C,S,μ,V,ξ, Z,λι,λ2,λ3,λ4)
=hC, S)+ e X Sij (log Sij — 1) + £1 X μi(log μi — 1) + (⅛ X Vj QOgVj- 1)
ij	i	j
—ξ丁(Slm — μ) — ζ"> (STIn — V)
+ λ1(E μi - 1) + 尢(£ Vj - 1) + λ3(y^(μi - μi)2 — P1) + λ4 (E(Vj 一 Vj)2 一 P2).
Easy to see that the Slater,s condition holds. Denote
r* 一 C(C C* 匚* 3* e* 广* ʌ r ʌ r ʌ r ∖*∖
L = L(C)Sr , μ ,v ,ξ , ζ ,λ1 ,λ2, λ3, λ4)∙
Following the KKT conditions,
	dL* k = Cij + £ log S*,ij — ξ* — ζ* =0. “r,ij
Therefore, S*,j 二	砥十弓一Cij =e e . Then we have dS； _ ∂S* ∂S* dξ* ∂S* dζ* dC dw = ( ^∂c + ∂ξr dC + ∂ζr dC) dW
dξ*	dζ* _	_____	ξi + ζj-Cij _
So all we need to do is to compute 舟 and 念.Denote Fij = e ≡	. Denote
φ =	dL_.	G :a=μ - F 1m,
ψ =	二看=v — F >1n,
p=	dV =ξ + λ11n + 2λ3(μ — μ) + £1 log μ)
q =	dL 赤=C + λ21m + 2λ4(V-V) + £2 log v,
X1 :	_ dL _ t1	1 =匹=μ 1n- 1,
X2	dL	_> =匹=v 1m -1,
X3 X4 :	=λ3(kμ - μ∣∣2— ρ1), =λ4(kv — v∣∣2 — ρ2)∙
Denote X = [χ1,χ2,χ3, χ4], and λ = [λ1, λ2, λ3, λ4]. Following the KKT conditions, we have
φ = 0,ψ = 0,p = 0, q = 0,χ = 0,
17
Published as a conference paper at ICLR 2021
at the optimal solutions. Therefore, for the optimal solutions We have
dC
dψ
dC
曳初劭而虫初
= = =
⅜z⅛z⅜z
∂φ
∂C
∂ψ
∂C
, +唠dC		+也C + ∂ζ* IC		∂φ dμ* + ∂“* dC		ι ∂φ dV* +后Ic		ι ∂φ dλ* +而而二		=0,
+	∂ψ dξ*	+	∂ψ dζ*	+	∂ψ dμ*	+	∂ψ dν*	+	∂ψ dλ*	=0,
	唠dC		∂ζ* IC		∂μ* dC		后Ic		而Ic二	
+		+	∂pdζ*	+	∂p dμ*	+	∂p dν*	+	∂p dλ*	=0,
	唠dC		∂Z* IC		∂μ* dC		后Ic		而Ic二	
+	如且	+	∂^dζ*	+	∂q dμ*	+	∂q dν*	+	∂q dλ*	=0
	芯dC		药dC		而范		∂V* dC		而记二	
+	∂χdC	+	∂χdζ*	+	∂χ dμ*	+	∂χ dν*	+	∂χ dλ*	=0.
	∂ξ7 dC		药dC		而范		∂V* dC		而记二	
-1
Therefore, We have
「dξ*
dC
After some derivation, We have
-dξ*
de
dζ*
有
dμ
TC
dν*
猊

猊
梃
皂
-dC -
and
—
-∂φ	∂φ	∂φ	∂φ	∂φ -
∂ξ*	∂ζ*	而	∂V*	∂λ*
∂ψ	∂ψ	∂ψ	∂ψ	∂ψ
∂ξ*		∂μ*	∂ν*	而
∂p	∂p	∂p	∂p	∂p
∂ξ*	∂ζ*	而	∂V*	∂λ*
∂q	∂q	∂q	∂q	∂q
∂ξ*		∂μ*	∂V*	而
∂χ	∂χ	∂χ	∂χ	∂χ
-∂ξ*	∂Z*	∂μ*	∂V*	而一
一 ∂φ 一
∂C
-	1	_ 	diag(μ) --(s*)t ε In 0 0 0 0 _	0	-1S; 1 e _ 	diag(ν) ε 0	2 Im 0 0 0 0	In 0 λ3In + diag( M ) 0 IT 0 2λ3(μ - μ)τ 0		0 Im 0 2λ4Im + diag(署) 0 Im 0 2λ4 (V — V)T	0 0 In 0 0 0 0 0	0 0 0 1 m 0 0 0 0		0 0 2(口 - μ) 0 0 0 IIm - μk2 - Pi 0	0 0 0 2(V — V) 0 0 0 l∣ν - νk2 - P2.	-1	■ ∂φ - ∂C 0 0 0 0 0
∂φh =	= ∂Cij	1										
	Jhi Sij Nh	=1,…	,n,	i = 1, ∙ ∙ ∙,n, j	=	1,	♦ ♦ ♦ ,	m			
∂ψ' =	= ∂Cij	"jSij ,∀'二	二1,…	m —	1, i = 1,∙∙∙,n,		j	= 1,	∙ ∙ ∙ ,m.			
To efficiently solve for the inverse in the above equations, we denote				
	-ɪ diag(μ)	「P	In	0
A =	-ɪ (S* )t	1 -_ diag(ν)	0	Im
	e!n	E 0	2λ3In + diag(齐)	0
	0	Im	0	2λ4Im + diag(詈工
1n	0	2(μ - μ)	0
0	1m 0	2(V — V)
.	1›
0
2尢(“ -M)T
0
0
ιm
0
2λ4(^ — V )t
18
Published as a conference paper at ICLR 2021
■0	0	0	0
0	0	0	0
0 0 llμ - μk2 - ρι	0
_0 0	0	— ν∣∣2- P2.
We first A-1 using the rules for inverting a block matrix,
A-1
K
—LK
—KL
L+LKL
: A1
: A3
A2
A4
where
L
2λ3In +diag( ɪ* 1 )	0
0	2λ4Im + diag( ^1)
，K=G
diag(μ) S*
(Sr)>	diag(P)
+L-1
Then using the rules of inverting a block matrix again, we have
~ξe~
dC
_dC.
~∂f
(Ai + A2B1(D — C1A4B1 )-1C1 A3) ∂C
_dC.
Therefore, the bottleneck of computation is the inverting step in computing K. Note L is a diagonal
matrix, we can further lower the computation cost by applying the rules for inverting a block matrix
again. The value of λ3 and λ4 can be estimated from the fact p = 0, q = 0 . We detail the algorithm
in Algorithm 2.
Algorithm 2 Computing the gradient for w
Require: C ∈ Rm×n, μ, ν, e, dC
Run forward pass to get S = S*, μ, V, ξ, Z
x1
Pd=12e (μi - μi),x2 = Pn=「n/2] (μi - μi),bi = - Pd=12e ξi,b2 = - Pn=「n/2] ξ
[λ1, λ3]> = [dn/2e, x1; n — dn/2e, x2]-1[b1, b2]>
x1
Pdm/2e (Vj-Vj ),X2 = Pm=rm∕2e (Vj-Vj ), bi = —工：7/21G, b?=—工晨皿倒
[λ2,λ4]> = [dm/2e,xi;m — dm/2e,x2]-i[b1,b2]>
μ = μ + e(2λ31n + ʒ=1) 1,ν = ν + e(2λ41m + 詈 ) 1
ν0 = p[: —1], S0 = S[:,: —1]
K — diag(p0) — (S0)T (diag(μ))-i S0
H1 — (diag(μ))-i + (diag(μ))-1S0Κ-1(S0 )>(diag(μ))-1
H2 — —(diag(μ))-1S0K-1
H J (H2)>
H jK-1
Pad H2 to be [n, m] with value 0
Pad H3 to be [m, n] with value 0
Pad H4 to be [m, m] with value 0
L = diag(k(2λ31n + ^1 )-1, e(2λ41m + 署)-1])
A1 = [H1,H2;H3,H4]
A2 = —A1 ∙ L
A3 = A2>
A4 = L + L ∙ A1 ∙ L
E = A1 + A2 ∙ B1(D — C ∙ A4 ∙ B)-1C • A3, where B1, C1, D defined above
[J1, J2; J3, J4] = E, where J1 ∈ Rn×n, J2 ∈ Rn×m, J3 ∈ Rm×n, J4 ∈ Rm×m
dξ*
[dC]nij J [J1]niSij + [J2]njSij
[dC]mij j- [J3]miSij + [J4]mjSij
Pad ddC to be [m, n, m] with value 0
[dL]ij J 7 (-CijSj + Pn,m CnmSnm [dC]nij + Pn,m CnmSnm [dC]mij ) + Sij
dL dC
return 万丁
dC dw
19
Published as a conference paper at ICLR 2021
(a) `2 distance
(b) KL-divergence
Figure 9: Illustration with different choice of F.
F Different forms of marginal relaxation
In this paper we adopt F to be the Euclidean distance. This is because this choice provides an
OT plan that fits our intuition - the data points with significantly larger transportation cost should
not be considered. Figure 9 shows an illustration. Here, the input distributions are the empirical
distributions of the scalars on the left and the bottom. Notice that there are three support points in
μ that are far away from others, i.e., 10.72,10.89,10.96. In Figure 9 (a), the optimal solution「；
automatically ignores them, matching only the rest of the scalars. One alternative choice ofF is the
Kullback-Leibler (KL) divergence (Chizat et al., 2018b), whose resulted formulation possesses an
efficient algorithm for the forward pass, and the differentiability for the backward pass. We do not
adopt it because the OT plan generated by this choice does not fit out intuition: As shown in Figure
9 (b), the OT plan tends to ignore the points that are away from the mean, even with a very small ρ1
and ρ2. For both figures, we adopt = 10-5.
G More on Experiments
G.1 Unlabeled Sensing
We now provide more training details for experiments in Section 4.1. Here, AM and ROBOT is
trained with batch size 500 and learning rate 10-4 for 2, 000 iterations. For the Sinkhorn algorithm
in ROBOT we set = 10-4. We run RS for 2 × 105 iterations with inlier threshold as 10-2. Other
settings for the hyper-parameters in the baselines follows the default settings of their corresponding
papers.
G.2 Nonlinear Regression
For the nonlinear regression experiment in Section 4.2, ROBOT and ROBOT-robust is trained
with learning rate 10-4 for 80 iterations. For n = 100, 200, 500, 1000, 2000, we set batch size
10, 30, 50, 100, 300, respectively.We set = 10-4 for the Sinkhorn algorithm in ROBOT. For Oracle
and LS, we perform ordinary regression model and ensure convergence, i.e., learning rate 5 × 10-2
for 100 iterations.
G.3 Flow Cytometry
We provide more details for the Flow Cytometry experiment in Section 4.3. In the FC seting,
ROBOT is trained with batch size 1260 and learning rate 10-4 for 80 iterations. In the GFC seting,
ROBOT is trained with batch size 1260 and learning rate 6 × 10-4 for 60 iterations. We set = 10-4
for the Sinkhorn algorithm in ROBOT. Other settings for the hyper-parameters in the baselines
follows the default settings of their corresponding papers. EM is initialized by AM.
20
Published as a conference paper at ICLR 2021
G.4 Multi-Object Tracking
For the MOT experiments in Section 4.4, the reported results of MOT17 (train) and MOT17 (dev)
is trained on MOT17 (train), and the reported results of MOT20 (train) and MOT20 (dev) is trained
on MOT20 (train). Each model is trained for 1 epoch. We adopt Adam optimizer with learning
rate= 10-5, = 10-4, and η = 10-3. To track the birth and death of the tracks, we adapt the
inference code of Xu et al. (2019b).
G.5 Combination with RS
As suggested in Figure 2, al-
though RS cannot perform well
itself, retraining the output of RS
using our algorithms increases
the performance by a large mar-
gin. To show that combining RS
and ROBOT can achieve better
results than RS alone, we com-
pare the following two cases: i).
Subsample 2 × 105 times using
RS; ii). Subsample 105 times us-
Table 2: Pairwise comparisons between RS alone and the com-
bination of RS and ROBOT. The relative error ratio is the ra-
tio of the relative errors of RS alone and RS+ROBOT combina-
tion. Ratios larger than 1 suggest that RS performs worse than
RS+ROBOT combination.
Proportion	25%	50%	75%
Rel. error ratio	1.04 ± 0.20	1.29 ± 0.32	1.27 ± 0.34
ing RS followed by ROBOT for 50 training steps. The result is shown in Table 2. For a larger
permutation proportion, RS alone cannot perform as well as RS+ROBOT combination. Here, we
have 10 runs for each proportion. We adopt SNR= 100, d = 5 for data, and = 10-4, learning rate
10-4 for ROBOT training.
G.6 The Effect of ρ1 and ρ2
We visualize Sr computed from the robust optimal transport problem in Figure 10. The two input
distributions are Unif(0, 2) and Unif(0, 1). We can see that with large enough ρ1 and ρ2, Unif(0, 1)
would be aligned with the first half of Unif(0, 2).
0.12
0.14
0.41
0.46
0.52
0.64
0.78
0.80
0.94
0.98
(a) ρ1 = 0, ρ2 = 0
(b) ρ1 = 0.1, ρ2 = 0.1
(c) ρ1 = 0.2, ρ2 =0.2
Figure 10: Computed Sr for robust optimal transport problem.
G.7 Comparison of Residuals in Linear Regression
Settings. We generate n data points {(yi, [xi, zi])}in=1, where xi ∈ Rd and zi ∈ Re. We first
generate Xi 〜N(0d,Id), Zi 〜N(0e, Ie), W 〜N(0d+e, Id+e), and εi 〜N(0,PnOise). Then we
compute yi = f([xi, zi]; w) + εi. Next, we randomly permute the order of {zi} so that we lose the
data correspondence. Here, D1 = {(xi, yi)} and D2 = {zj} mimic two parts of data collected from
two separate platforms.
We adopt a linear model f(x; w) = x>w. To evaluate model performance, we use error= Pi(ybi -
yi)2/ Pi(yi — y)2, where bi is the predicted label, and y is the mean of {yi}.
Baselines. We use Oracle, LS, Stochastic-EM as the baselines. Notice that without a proper ini-
tialization, Stochastic-EM performs well in partially permuted cases, but not in fully shuffled cases.
21
Published as a conference paper at ICLR 2021
For better visualization, we only include this baseline in one experiment. Furthermore, we adopt
two new baselines: Sliced-GW (Vayer et al., 2019) and Sinkhorn-GW (Xu et al., 2019a), which can
be used to align distributions and points sets.
Results. We visualize the fitting error of regression models in Figure 11. We can see that ROBOT
outperforms all the baselines except Oracle. Also, our model can beat the Oracle model when the
dimension is low or when the noise is large.
Figure 11: Linear regression. We use n = 1000, d = 2, e = 3, ρ2noise = 0.1 as defaults.

22