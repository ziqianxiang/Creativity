Published as a conference paper at ICLR 2021
More or Less: When and How to Build
Convolutional Neural Network Ensembles
Abdul Wasay
Harvard University
awasay@seas.harvard.edu
Stratos Idreos
Harvard University
stratos@seas.harvard.edu
Ab stract
Convolutional neural networks are utilized to solve increasingly more complex
problems and with more data. As a result, researchers and practitioners seek
to scale the representational power of such models by adding more parameters.
However, increasing parameters requires additional critical resources in terms of
memory and compute, leading to increased training and inference cost. Thus a
consistent challenge is to obtain as high as possible accuracy within a parameter
budget. As neural network designers navigate this complex landscape, they are
guided by conventional wisdom that is informed from past empirical studies. We
identify a critical part of this design space that is not well-understood: How to
decide between the alternatives of expanding a single convolutional network model
or increasing the number of networks in the form of an ensemble. We study this
question in detail across various network architectures and data sets. We build an
extensive experimental framework that captures numerous angles of the possible
design space in terms of how a new set of parameters can be used in a model.
We consider a holistic set of metrics such as training time, inference time, and
memory usage. The framework provides a robust assessment by making sure
it controls for the number of parameters. Contrary to conventional wisdom, we
show that when we perform a holistic and robust assessment, we uncover a wide
design space, where ensembles provide better accuracy, train faster, and deploy at
speed comparable to single convolutional networks with the same total number of
parameters.
1	Introduction
Scaling capacity of deep learning models. Convolutional neural network models are becoming as
accurate as humans on perceptual tasks. They are now used in numerous and diverse applications
such as drug discovery, data compression, and automating gameplay. These models increasingly
grow in size with more parameters and layers, driven by two major trends. First, there is a continuous
rise in data complexity and sizes in many applications (Shazeer et al., 2017). Second, there is an
increasing need for higher accuracy as models are utilized in more critical applications - such as
self-driving cars and medical diagnosis (Grzywaczewski, 2017). This effect is especially pronounced
in computer vision and natural language processing: Model sizes are three orders of magnitude larger
than they were just three years ago (Sanh et al., 2019).
With bigger model sizes, the time, computation, and memory needed to train and deploy such models
also increase. Thus, it is a consistent challenge to design models that maximize accuracy while
remaining practical with respect to the resources they need (Lee et al., 2015; Huang et al., 2017b). In
this paper, we study the following question: Given a number of parameters (neurons), how to design
a convolutional neural network to optimize holistically for accuracy, training cost, and inference cost?
The holistic design space is very complex. Designers of convolutional neural network models
navigate a complex design landscape to address this question: First, they need to decide on network
architecture. Then, they have to consider whether to use a single network or build an ensemble
model with multiple networks. Additionally, they have to decide how many neural networks to use
and their individual designs, i.e., the depth, width, and number of networks in their model. Modern
1
Published as a conference paper at ICLR 2021
applications with diverse requirements further complicate these decisions as what is desirable varies.
Facebook, for instance, requires convolutional neural network models that strike specific tradeoffs
between accuracy and inference time across 250 different types of smartphones (Wu et al., 2019).
As a result, not just accuracy but a diversity of metrics - such as inference time and memory usage -
inform whether a model gets used (Sze et al., 2017b).
Scattered conventional wisdom. There exist bits and pieces of scattered conventional wisdom to
guide a neural network designer. These take the form of various empirical studies that demonstrate
how depth and width in a single neural network model relate to certain metrics such as accuracy.
First, it is generally known that deeper and wider networks can improve accuracy. In fact, recent
convolutional architectures - such as ResNets and DenseNets - are designed precisely to enable
this outcome (He et al., 2016; Huang et al., 2017b;a). The caveat with beefing up a neural network
is that accuracy runs into diminishing returns as we continue to add more layers or widen existing
ones (Coates et al., 2011; Dauphin and Bengio, 2013). On the other hand, increasing the number
of networks in the model, i.e., building ensembles, is considered a relatively robust but expensive
approach to improve accuracy as ensemble models train and deploy k networks instead of one
(Russakovsky et al., 2015; Wasay et al., 2020). The consensus is to use ensembles when the goal is to
achieve high accuracy without much regard to training cost, inference time, and memory usage, e.g.,
competitions such as COCO and ImageNet (Lee et al., 2015; Russakovsky et al., 2015; Huang et al.,
2017a; Ju et al., 2017). All these studies, however, exist in silos. Any form of cross-comparison is
impossible as they use different data sets, network architectures, and hardware.
Lack of a robust and holistic assessment. Most past studies operate within the confines of a single
convolutional network and do not consider the dimension of ensemble models. Those that compare
with ensembles mostly do so unfairly comparing ensembles with k networks against a model that
contains only one such network (Lee et al., 2015; Russakovsky et al., 2015; Huang et al., 2017a;
Ju et al., 2017). There are recent studies that make this comparison under a fixed parameter budget
(Chirkova et al., 2020; Kondratyuk et al., 2020). However, these studies consider only the metric of
generalization accuracy and explore a very small part of the design space - two different classes of
convolutional architectures with a single depth.
A holistic analysis needs to include resource-related metrics such as training time, inference cost, and
memory usage. All these metrics are critical for practical applications (Sze et al., 2017a; Wu et al.,
2019). Furthermore, to provide reliable guidance to a model designer, a robust comparison needs to
consider a range of architectures and model sizes with various depth and width configurations. This
is critical, especially because varying just the width of convolutional networks in isolation, as done
by recent studies (Chirkova et al., 2020; Kondratyuk et al., 2020), is known to be far less effective to
improve accuracy (Eigen et al., 2013; Ba and Caruana, 2014).
Single networks vs. ensembles. In this paper, we bridge the gap in the understanding of the design
space by providing answers to the following questions. Given specific requirements in terms of
accuracy, training time, and inference time, should we train and deploy a convolutional model with a
single network or one that contains an ensemble of networks? How should we design the networks
within an ensemble? As these constraints and requirements evolve, should we switch between these
alternatives, why, and when?
Method. We introduce the following methodology to map the design space accurately. Since
there is no robust theoretical framework to consistently analyze the design space and the complex
interactions among its many parameters and metrics, we develop a detailed and extensive experimental
framework to isolate the impact of the critical design knobs: (i) depth, (ii) width, and (iii) number
of networks, on all relevant metrics: (i) accuracy, (ii) training time, (iii) inference time, and (iv)
memory usage. Crucially the number of parameters is a control knob in our framework, and we only
compare alternatives under the same parameter budget. To establish the robustness of our findings,
we experiment across various architectures, data complexities, and classification tasks. We present
and analyze data amounting to over one year of GPU run time. We also explain trends breaking down
metrics into their constituents when necessary.
Results: The Ensemble Switchover Threshold (EST). (i) Contrary to conventional wisdom, we
show that when we make a holistic and robust comparison between single convolutional networks
2
Published as a conference paper at ICLR 2021
Param.: |S| | DePth: d | Width: w
Depth
(a) Single network
Param.: |S| | Depth: d | Width: w,
E1 I	I I	I I I
E2 I	I I	I I I
E3
(b) Depth-equivalent ensemble
Param.: |S| | Depth: d, | Width: w
E1 E2	E3
(C) Width-equivalent ensemble
I	I	I	I	I	I
I	I	I	I	I	I
I	I	I	I	I	I
Figure 1: We explore a design space consisting of three design classes: (a) Single convolutional
network models, (b) Depth-equivalent ensembles, and (c) Width-equivalent ensembles. The two
ensemble design classes are created by distributing either the width factor or the depth corresponding
to the single network amongst the ensemble networks while keeping the other factor fixed.
and ensembles of networks, we discover a vast design space where ensembles provide not just
better overall accuracy but also train faster compared to a single network. (ii) Specifically, we
uncover the Ensemble Switchover Threshold (EST). This is the amount of resources (measured in
terms of the number of parameters and training epochs) beyond which ensembles provide superior
generalization accuracy to a single model. (iii) We show that EST occurs consistently across
numerous data sets and architectures. (iv) We demonstrate that the number of networks in an
ensemble and their individual designs determine the EST. (v) Ensembles can also provide comparable
inference times for a considerable part of the design space. (vi) We also show that ensembles
require significantly less memory to train for the same number of parameters. (vii) Finally, we
make available a superset of our results for visual exploration and help with model design at:
daslab.seas.harvard.edu/more-or-less. ,
2	Framework: Design Space
The design space we explore consists of single convolutional neural network models and two classes
of architecturally-homogenous ensembles. These ensemble classes help isolate the effect of the two
design knobs - depth and width - on the quality and cost of an ensemble design. We first describe
how we ensure a robust comparison of alternative model designs and then explain the degrees of
freedom we explore.
Establishing grounds for fair ranking. A key element of our framework is that the possible model
designs are compared to each other only under equivalent resources. We ensure this by only comparing
designs that have the same number of parameters. This comparison allows us to separate the quality
of a design from the amount of resources given to it. Another way to think about this is that given
a parameter budget, we can investigate how the three design classes rank for all relevant metrics
(training and inference time, accuracy, and memory usage).
We fix the number of parameters because of its two distinctive properties over other metrics (that we
could have fixed), such as training time, inference time, or accuracy: First, the number of parameters
of a network is directly proportional to all other resource-related metrics (Jain et al., 2020; Wasay
et al., 2020). Second, the number of parameters is independent of the hardware or the software
platform used and can be computed exactly from a network specification.
The single network versus ensemble design space. Our design space considers a convolutional
neural network architecture S(w,d) from a class of neural network architectures C. S(w,d) has
width factor w, depth d, and number of parameters |S |. Similarly an ensemble is described as
E = {E1 . . . Ek}. Ensembles are architecturally-homogenous i.e., all ensemble networks E1 . . . Ek
have the same architecture and each network has |E |/k parameters. When we compare a single
network S(w,d) from C with an ensemble E we ensure that E1 . . . Ek ∈ C and |E1 |+. . .+|Ek| = |S|.
The reason why we restrict the design space to homogenous ensembles is to reduce the otherwise
intractably large space1 of all possible ensembles given a single network to a size that we can
1Given a single network with |S| number of parameters, there are {|S|} (Stirling number of the second kind)
as many ways of forming ensembles of size k. This number grows at a similar rate to exponential polynomials,
(Boyadzhiev, 2009) e.g., 1040} ≈ 1059.
3
Published as a conference paper at ICLR 2021
feasibly and thoroughly experiment with and reason about. Furthermore, many neural network
ensembles introduced in research and used in practice are similarly homogenous, for instance,
SnapShot Ensembles and Fast Geometric Ensembles (Huang et al., 2017a; Garipov et al., 2018).
Additionally, our method provides a deterministic procedure of going between single network models
and ensembles given a certain amount of parameters. Major sources of diversity in neural network
ensembles are random weight initialization and stochastic training, both of which we incorporate in
our framework.
Depth-equivalent and width-equivalent ensembles. Convolutional neural network architectures
are determined by two design knobs - the depth and the width factor. Corresponding to these two
design knobs, we create two classes of ensembles: depth-equivalent ensembles and width-equivalent
ensembles. These are depicted in Figure 1: In depth-equivalent ensembles, the depth of the individual
ensemble networks is the same as S (i.e., d), and the width factor is set to the highest possible value
(i.e., w0) without exceeding the parameter budget of |S|. In width-equivalent ensembles, on the other
hand, the width factor is conserved across all ensemble networks (i.e., w), and the depth is modulated
to the highest possible value (i.e., d0) without exceeding |S|:
w0 : k ∙ ∣E(w0,d)∣ ≤ |S(W⑷ | ≤ k ∙ ∣E(w0+1⑷| d0 : k ∙ ∣E(w,dO)I ≤ IS(W⑷| ≤ k ∙ ∣E(w,d0+I)I
The above definition follows that neural networks in depth-equivalent ensembles have higher depth
than those in width-equivalent ensembles. Width-equivalent ensembles contain wider neural networks
than their depth-equivalent counterparts. In this way, we isolate and study the effect of depth and
width on ensemble accuracy and resource requirement.
Overall, our design space spans three classes of convolutional neural network designs: (i) single
network models, (ii) width-equivalent ensembles, and (iii) depth-equivalent ensembles. Every class
contains several model designs instantiated by the four-tuple {w, d, ISI, C}. We next describe how we
designed an exhaustive experimental framework to cover various configurations of these four-tuples.
3	Framework: Data, Architectures, and Metrics
Datasets and architectures. We include widely-used state-of-the-art network architectures and data
sets in our study. These include DenseNets, and (Wide) ResNets architectures as well as SVHN, C10
and C100, and ImageNet datasets. Table A in the appendix summarizes these networks and training
data sets as well as corresponding hyperparameters. We implement our experimental framework in
PyTorch and used an Nvidia V100 GPU to run all experiments.
Evaluation metrics. We study all three design classes - single network, width- and depth-equivalent
ensembles - across five metrics: (i) generalization accuracy, (ii) training time per epoch, (iii) time
to accuracy, (iv) inference time, and (v) memory usage. When considered together, these metrics
provide a holistic picture of the quality and practicality of models.
4	Ensembles Outperform S ingle Network Models After a Low to
Moderate Parameter Threshold
We observe that both classes of ensembles - depth- and width-equivalent - outperform single network
models after a resource threshold. We call this threshold the Ensemble Switchover Threshold (EST).
Beyond the EST, ensemble models achieve 1 to 3 percent lower test error rates (across various
architectures and data sets) compared with single networks having the same number of parameters.
The EST appears consistently across a wide range of data sets and architectures (Figure 2(a) through
Figure 2(f)) as well as ensemble sizes (Figure 4(a) through Figure 4(c) and appendix Figure B(a)
through Figure B(c)). In these figures, we use discrete heat maps to visualize which of the three design
classes - single network models (single), depth-equivalent ensembles (deq), and width-equivalent
ensembles (weq) - dominates in terms of generalization accuracy for a given resource budget. This
resource budget takes the form of the number of parameters (on the x-axis) and epochs (on the y-axis).
We also mark areas where both classes of ensembles outperform single network models. Figure 3
shows the test error rates achieved on various data sets for DenseNet models. We present this metric
for the rest of the architectures in Figure A in the appendix.
4
Published as a conference paper at ICLR 2021
(a) DenSeNetS CIFAR-10 (k=4)	, “	(b) DenSeNetS CIFAR-100 (k=4)	(C) DenSeNetS Tiny ImageNet (k=4)
both ens.
■ weq only
■ deq only
single
Sqɔodəjo Jωq!unu
GGMG女MM右次女女次石次右右
GGMG女MM右次女女次石次右右
64 | 5.49
88 — 5.31
88 | 3.26
64 | 3.13
40 | 2.59
64 | 1.92
88 | 1.7
40 | 1.48
64 | 1.01
40 | 0.91
88 | 0.65
40 | 0.48
64 | 0.39
40 | 0.19
^4| 5...1
88 | 5.23
88 | 3.1.9
64 | 3.07
40 | 2.54
64 | 1.88
88 | 1.66
40 | 1.45
64 | 0.98
40 | 0.89
88 | 0.62
40 | 0.46
64 | 0.37
40 | 0.18
(f) Wide ResNets CIFAR-10 (k=4)
50
both ens.
Weq only
deq only
single
(e) ReSNetS CIFAR-100 (k=4)
width factor | depth | number of parameters (M)
150
12 |58 - 122.22
12 - 52 - 108.28
8 - 58 -54.34
12 - 28 - 52.52
8 - 52 - 48.14
Sqɔodə JO Jωq!unu
width factor | depth | number of parameters (M)

Figure 2:	The Ensemble Switchover Threshold (EST) occurs consistently across various network
architectures and data sets. Beyond this resource threshold, ensemble designs outperform single
network models.
The occurrence of EST both expands and questions the general consensus on the relative effectiveness
of ensemble versus single network models. First, even when allocated the same amount of resources,
ensemble models still outperform single network models. This observation expands upon past
empirical studies that only show how a k-network ensemble is more accurate than any of the single
network models that it contains (Lee et al., 2015; Huang et al., 2017a). Second, the EST occurs in
low- to moderate-resource settings. For instance, in all of our experiments, we observe the EST at the
1M to 1.5M parameter range2 and after no later than half of the training epochs. This trend challenges
the widespread notion that neural network ensembles are useful only when we have tons of resources
at our disposal (Lee et al., 2015; Ju et al., 2017). Overall, our results indicate that ensembles of
convolutional models are preferable to single network models for a much wider range of use cases
than previously understood.
On the superior generalization of ensembles under a parameter budget. To interpret why en-
sembles outperform single network models under a parameter budget, we use the phenomenon
of diminishing returns on increasing model sizes. In the past, this effect has been independently
investigated by Eigen et al. (2013) and Dauphin and Bengio (2013) for a single network model. We
hypothesize that as we increase the number of parameters, single network models exhibit more dimin-
ishing returns and plateau faster than ensemble networks. When the single network’s generalization
accuracy starts showing diminishing returns, the corresponding width-equivalent and depth-equivalent
ensembles have smaller networks with 1/k as many parameters (assuming the parameters are spread
equally along k ensemble networks). These individual networks in the ensemble are affected less by
the plateau because they have 1/k as many parameters as the single network model. Thus, utilizing
these networks in an ensemble leads to better generalization accuracy overall because they do not hit
the threshold of the diminishing returns while still being able to benefit from the known properties
that ensembles provide: (i) They enrich the space of hypotheses that are considered by the base
model class and (ii) By averaging over various models, ensembles reduce the variance of base models,
smoothing out variations due to initialization and the learning process (Lee et al., 2015).
2Wide ResNets are an exception: This is because, compared to other convolutional architectures, Wide
ResNets have an order of magnitude more parameters even for modest depths and width factors. For instance,
networks presented in the Wide ResNet paper have anywhere between 8M to 37M parameters compared to the
1M to 10M range for DenseNets (Zagoruyko and Komodakis, 2016).
5
Published as a conference paper at ICLR 2021
(C) DenseNets Tiny ImageNet (k=4)
(b) DenSeNetS CIFAR-100 (k=4)
48
48
36
48
36
28
28
36
48
20
28
20
36
20
12
28
12
20
12
12
48
48
36
48
36
28
28
36
48
20
28
20
36
20
12
28
12
20
12
12
112 114.1
88 | 9.32
112 | 8.02
112 | 13.96
88 | 9.22
112 | 7.91
width factor | depth | number of parameters (M)
Figure 3:	Ensembles arrive at lower test error rates than single network models after the EST has
been reached.
(c) DenseNets CIFAR-10 (k=8)
both ens.
Weq only
deq only
single
120
100
80
60
40
20
(b) DenseNets CIFAR-10 (k=6)
120
100
80
(a) DenSeNetS CIFAR-10 (k=4)
Sqɔodəjo Jωq!unu
GGMG女MM右次女女次右次右右 GGMG以MM右次女女次右次右右 GGMG以MM右次女女次石次右右
64 | 5.41
88 | 5.23
88 | 3.19
64 | 3.07
40 | 2.54
64 | 1.88
88 | 1.66
40 | 1.45
64 | 0.98
40 | 0.89
88 | 0.62
40 | 0.46
64 | 0.37
40 | 0.18
64 | 5.41
88 | 5.23
88 | 3.19
64 | 3.07
40 | 2.54
64 | 1.88
88 | 1.66
40 | 1.45
64 | 0.98
40 | 0.89
88 | 0.62
40 | 0.46
64 | 0.37
40 | 0.18
64|5.41
88 | 5.23
88 | 3.19
64 | 3.07
40 | 2.54
64 | 1.88
88 | 1.66
40 | 1.45
64 | 0.98
40 | 0.89
88 | 0.62
40 | 0.46
64 | 0.37
40 | 0.18
width factor | depth factor | num param (M)
Figure 4:	The Ensemble Switchover Threshold moves to the right as we increase the number of
networks in the ensemble.
Next, we parse out how the data complexity and the composition of the ensemble networks affect the
EST and, in turn, the ranking of the three design classes.
Ensembles are even more effective for more complex data sets. We observe that the EST shifts
closer to the origin as the training data set’s complexity increases. This can be seen in Figure
2(a) through 2(c) where we train DenseNets on progressively more complex data sets (CIFAR-10,
CIFAR-100, and Tiny ImageNet). This observation indicates that ensemble models are preferable to
single models when training on more complex data sets for an even wider range of available resources.
This observation again expands the utility of ensembles. There is theoretical and empirical work
establishing that ensembles do better for complex data (Bonab and Can, 2017; Huang et al., 2017a).
We, however, establish this phenomenon in the resource-equivalent setting as opposed to past studies
that do so for ensembles and single networks with drastically different numbers of parameters.
Large ensembles are effective under a large parameter budget. As we increase the number of
networks k within an ensemble without increasing the parameter budget, the overall accuracy of
ensemble designs diminishes, pushing the EST to a higher resource limit. Figure 4 demonstrates this
phenomenon for DenseNets and Figure B in the appendix shows it for ResNets. For instance, for
DenseNet models, the EST moves from the 1.5M range for k = 4 to the 3M range for k = 6 and,
then, to the 5M range for k = 8. This shift can be explained by looking at individual accuracy of
ensemble networks. Figure C in the appendix shows test error rates of the ensemble as a whole as
well as the average test error rates of individual ensemble networks corresponding to Figure 4. We
observe that as we increase the number of networks k (from k = 4 in Figure C(a) to k = 8 in Figure
C(c)), the individual test error rates (shown as dotted lines) increases. This increase happens because
individual networks’ size goes down (as we keep a fixed parameter budget). This observation implies
that larger-size ensembles are desirable over smaller sizes only when we have a sufficient parameter
budget to assign to every single network in the model. As opposed to previous work, our experiments
decouple the parameter budget from the number of networks in the model. We discover that just
increasing the ensemble size without increasing the total number of parameters hurts accuracy.
6
Published as a conference paper at ICLR 2021
■UltU) AaBJnoɔB-6us,O1 3tu=
■ single weq deq
64 |
88 |
40 |
64 |
112
40 |
88 |
40 |
64 |
40 |
■UItU) AaBJn。。"-8tπs O- ωsa
≈°2∞li0
■ single ■ Weq ■ deq
■UItU) AaBJnoɔB-«UIS O1 3tull
■ single
I I I
112 | 2.5
112| 13.96
88 | 9.22
112 | 7.91
64 | 5.41
12 | 58 1122.22
12 | 52 | 108.28
width factor | depth | number of Parameters (M)
width factor | depth | number of Parameters (M)
width factor | depth | number of parameters (M)
(a)	DenseNets CIFAR-10 (k=4)
(b)	ResNets CIFAR-10 (k=4)
(c)	Wide ResNets CIFAR-10 (k=4)


Figure 5:	When ensemble designs can provide better accuracy, they can also do so faster than single
network models (missing bars indicate that designs cannot reach single network model accuracy).
Ooooooo
0 5 0 5 0 5
(S) IPOd3 J3d 3sUlUllB
i40i
i88i
i40i
i64i
i40i
0.91
0.65
0.48
0.39
0.19
|112|0.97
i64i
i88i
i40i
i64i
i1.92
i1.7
i1.48
i1.01
111212.56
i88i
i64i
i40i
i3.26
i3.13
i2.59
111214.91
i64i
i88i
i5.49
— 5.31
|88|9.32
|112|8.02
■ single ■ Weq ■ deq
width factor | depth | number of Parameters (M)
(b) ResNets CIFAR-10 (k=4)
(S) IPOd3 J3d 3tu=«UIUrBJ-
12|58|122.29
12|52|108.35
8|58|54.38
12|28|52.59
8|52|48.19
8|28|23.4
4|58|13.62
4|52|12.07
4i28i5.87
Ooooooo
0 5 0 5 0 5
■ single ■ Weq ■ deq
ill ill ill III
width factor | depth | number of parameters (M)
width factor | depth | number of parameters (M)
(a) DenseNets CIFAR-10 (k=4)
(c) Wide ResNets CIFAR-10 (k=4)
Figure 6:	Depth-equivalent ensembles take longer to train per epoch as compared to single network
models. Width-equivalent ensembles take comparable time.
Depth-equivalent ensembles outperform width-equivalent ensembles. We observe that depth-
equivalent ensembles are overall more accurate than width-equivalent ensembles (as shown in Figure
3 and appendix Figure A). They also consistently demonstrate EST at a lower resource range. This
can be explained by the fact that modern convolutional neural network architectures provide better
accuracy with increasing depth (Eigen et al., 2013; Urban et al., 2016). Here, depth-equivalent
ensembles have deeper ensemble networks with better individual accuracy. Thus, when used together
in an ensemble, they also provide better ensemble accuracy. This way, when designing ensemble
models for high accuracy, deeper networks are preferable to wider networks.
EST vs. Memory Split Advantage. For a limited part of the design space, recent work has observed
the existence of a parameter limit beyond which depth-equivalent ensembles outperform single
networks. This is termed as the Memory Split Advantage or the MSA (Kondratyuk et al., 2020). The
EST, however, is not defined just with respect to the number of parameters but also the number of
training epochs, inference cost, and memory usage as by only looking at these metrics in conjunction,
we can get a complete picture of the relative effectiveness of ensembles vs. single networks. In this
way, the EST subsumes the MSA, and we also verify the MSA across a significantly larger design
space (e.g., we consider 3× more data sets and twice as many architecture types) than has been done
before.
5 Ensembles Train Faster and Provide Comparable Inference Time
First, we analyze the training time. Despite taking longer per epoch, both ensemble classes achieve
the accuracy of single network models significantly faster for a considerable part of the design space
(e.g., 1.2× to 5× faster across our experiments). This happens after the EST has been reached, i.e.,
when ensemble designs can provide better accuracy, they can also do so faster than single network
models. This can be seen in Figure 5. Here, we plot the total training time needed for any of the three
design classes to achieve the maximum accuracy of single network models under the same parameter
budget. Figure 6 shows the corresponding training time per epoch.
The combined depth determines per epoch training time. We observe that both classes of ensem-
bles, on average, take longer to train per epoch as compared to single network models as they train
7
Published as a conference paper at ICLR 2021
(StU) 38BtUIJ3d 3s 30u3J3Jtπ
■ single ■ Weq ■ deq
I , I」，I " Tl
48|88|9.32
36|112|8.02
48|64|5.49
36|88|5.31
112i4.91
88i3.26
64i3.13
40i2.59
112i2.56
64i1.92
8811.7
40i1.48
64i1.01
11210.97
40i0.91
8810.65
40i0.48
64i0.39
40i0.19
(StU) 38BtuIJ3d 3tu= 30u3JjiUI
■ single ■ Weq ■ deq
48
48
32
48
32
32
24
24
32
24
16
24
16
16
26i3.3
50i3.03
38i2.25
62i2.14
62i3.8
38i5.05
50i6.79
48|62|8.54
-2|58|-22.29
12|52|108.35
8|58|54.38
12|28|52.59
8|52|48.19
8|28|23.4
4|58|13.62
4|52|12.07
4|28|5.87
4 3 2 1 0
(StU) 38BtuIJ3d 3tu30UAI3JUI
■ single ■ Weq ■ deq
width factor | depth | number of parameters (M)
width factor | depth | number of Parameters (M)	width factor | depth | number of parameters (M)
(a) DenseNets CIFAR-10 (k=4)	(b) ResNets CIFAR-10 (k=4)	(c) Wide ResNets CIFAR-10 (k=4)
Figure 7:	Width-equivalent ensembles take comparable time to single network models for inference.
Depth-equivalent ensembles take significantly longer.
k networks instead of one. How much more time ensembles take per epoch depends heavily on
the ensemble networks’ design: This additional time is negligible for width-equivalent ensembles
whereas, for depth-equivalent ensembles, it results in 2× more expensive per epoch training. This
trend can be explained by how the training time per epoch scales with respect to the width and depth
of convolutional neural network models. This ultimately connects to how GPUs process data, which
favors networks with few wider layers over those that have multiple thinner layers.
We break down the training time per epoch of all designs into two constituents: time spent per layer
and number of layers. Figure E in the appendix shows this breakdown for various architectures. We
observe that the total number of layers in a model (for ensembles, this is the sum of all networks’
depth) majorly determines the training time per epoch. For the same parameter budget, depth-
equivalent ensembles have proportionally more layers, whereas width-equivalent ensembles have
proportionally more width. The average time per layer depends on the width and does not increase
significantly as we move from depth-equivalent ensembles to width-equivalent ensembles. On the
other hand, the total number of layers scales linearly with depth. For the same parameter budget,
the total number of layers is significantly higher for depth-equivalent ensembles than the other two
designs, resulting in higher per epoch training.
From a GPU perspective, wider and shallower networks are more efficient to execute than narrower
and deeper networks for the same parameter budget. This can be attributed to the massive amount
of data parallelism in modern GPUs. Increasing the network’s width just increases the number of
kernels within layers. This increase more efficiently utilizes GPU’s massive capacity to perform the
same operation on multiple data items. On the other hand, deepening a network introduces new layers
(and operations) that require additional synchronization steps slowing down the overall execution.
Networks in ensemble models converge faster than single network models for the same param-
eter budget. The fact that ensemble designs can reach the same accuracy faster than single network
models can be attributed to the fact that, for the same parameter budget, all networks in the ensemble
model are smaller than the single network model. Smaller networks are known to converge faster
albeit to lower accuracy than larger networks.
However, we observe that the distinct advantage ensemble designs provide over the single model is
that when we use smaller networks in an ensemble, we get the best of both worlds. We converge
faster at an individual network level, and ensembling makes up for the generalization accuracy.
Overall, these observations again question the conventional wisdom of ensembles being significantly
slower to train than single network models. When we analyze the design space under a fixed parameter
budget, we uncover that for a vast range of the design space: (i) width-equivalent ensembles introduce
negligible overhead to per epoch training time as compared to single network models and (ii) both
ensemble designs achieve and surpass accuracy of single network models in considerably less training
time.
Width-equivalent ensembles provide competitive inference time. We provide the inference time
per image in Figure 7 and observe a similar trend to training time per epoch. While depth-equivalent
ensembles are significantly slower, width-equivalent ensembles provide comparable inference speed
to single network models. Again, this questions conventional wisdom that expects ensembles to be
substantially slower in inference.
8
Published as a conference paper at ICLR 2021
5 0 5
() 3^Bsn XJOtu.tu
■ single Weq deq
111111111 11
48|4|9.32
36|112|8.02
48|64|5.49
368i5.31
28i112i4.91
28i88i3.26
36i64i3.13
48i40i2.59
20i112i2.56
28i64i1.92
20i88i1.7
36i40i1.48
20i64i1.01
12i112i0.97
28i40i0.91
1218810.65
20i40i0.48
12i64i0.39
12i40i0.19
width factor | depth | number of parameters (M)
(a)	DenseNets CIFAR-10 (k=4)
(°α) BSn ^5sωs
■ single ■ Weq ■ deq
1111111 I I
16|38|0.57
16|26|0.37
16|50|0.76
48|62|8.54
ill
-2|58|-22.29
12|52|108.35
8|58|54.38
12|28|52.59
8|52|48.19
8|28|23.4
4|58|13.62
4|52|12.07
4|28|5.87
() 38BSn Xjotuatu .XBtU
■ single ■ Weq ■ deq
width factor | depth | number of Parameters (M)	width factor | depth | number of parameters (M)
(b)	ResNets CIFAR-10 (k=4)
(c)	Wide ResNets CIFAR-10 (k=4)
I I I I I I I
Figure 8:	Both classes of ensemble models are significantly more memory efficient.
6	Ensembles are Memory-Efficient
Regarding memory usage we observe that the trend favors both classes of ensemble designs over
single network models. Figure 8 provides the amount of memory used as we train depth-equivalent
ensembles, width-equivalent ensembles, and single network models. This is the minimum amount of
memory that a GPU needs to train any of these designs for the batch sizes provided in Table A. This
memory is majorly used to store model parameters and intermediate results (Jain et al., 2020).
The superior memory efficiency of ensemble models is because when we train a k-networks ensemble,
at any point during the training process, we only need as much memory to train one of the k networks
(having 1 as many parameters compared to the single network). This observation has two important
implications: First, we can use larger batch sizes for the same GPU while training an ensemble of
networks. This, for instance, is useful when training complex data sets such as ImageNet (Smith
et al., 2018). Additionally, we can feasibly train the same number of parameters in an ensemble using
lower-end GPUs with less memory.
Additional results. We show that the same resource-related trends hold for the rest of architectures
and data sets from Table A. Figure D, Figure F, Figure G, and Figure H demonstrate these trends
for metrics of time to accuracy, time per epoch, inference time, and memory usage respectively.
We also provide results on a downsampled version of ImageNet-1K data set, CIFAR-100 with data
augmentation, and SVHN data sets in Figure I, Figure J, and Figure L in the appendix respectively.
Table B through Table D provide the conversion between the width factor and depth of single network
models and the two ensemble design classes.
7	The Complete Picture
The results presented question conventional wisdom concerning the design decision on whether to
use an ensemble of convolutional networks or not. By creating a detailed framework that (a) allows
to fix resources and (b) spans a large design space, we show that for a considerable part of the design
space, given an amount of resources, ensembles (i) achieve better accuracy than single network
models, (ii) train faster, (iii) provide comparable inference, and (iv) need much less memory. Future
work includes the addition of fast ensemble training approaches (such as SnapShot, TreeNets, and
MotherNets (Lee et al., 2015; Huang et al., 2017a; Wasay et al., 2020)) and parallel training, both of
which can move EST further in favor of ensembles.
Limitations and future directions. The analysis framework as presented in this paper covers
extensively homogeneous ensembles. There are several opportunities for parts of the design space
not covered by this work where it would be extremely useful to verify the results further. In
particular, this includes (1) heterogeneous ensembles (e.g., various ratios of width-equivalent and
depth-equivalent networks), (2) arbitrary network architectures (including using network architecture
search approaches), and (3) application domains other than image classification such as object
detection, machine translation, and deep generative models.
Acknowledgements. Harvard DASlab member Pablo R. Ruiz, Haochen Yang, Longshen Ou, and
Ziyi Guo helped with parts of the implementation and the demo. This work is partly funded by the
USA Department of Energy Project DE-SC0020200.
9
Published as a conference paper at ICLR 2021
References
J.	Ba and R. Caruana. Do deep nets really need to be deep? In Advances in neural information
processing systems, pages 2654-2662, 2014.
H. R. Bonab and F. Can. Less is more: A comprehensive framework for the number of components
of ensemble classifiers. arXiv preprint arXiv:1709.02925, 2017.
K.	N. Boyadzhiev. Exponential polynomials, stirling numbers, and evaluation of some gamma
integrals. In Abstract and Applied Analysis. Hindawi, 2009.
N. Chirkova, E. Lobacheva, and D. Vetrov. Deep ensembles on a fixed memory budget: One wide
network or several thinner ones?, 2020.
P. Chrabaszcz, I. Loshchilov, and F. Hutter. A downsampled variant of imagenet as an alternative to
the cifar datasets. arXiv preprint arXiv:1707.08819, 2017.
A. Coates, A. Ng, and H. Lee. An analysis of single-layer networks in unsupervised feature learning.
In Proceedings of the fourteenth international conference on artificial intelligence and statistics,
pages 215-223, 2011.
E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le. Randaugment: Practical automated data augmentation
with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition Workshops, pages 702-703, 2020.
Y. N. Dauphin and Y. Bengio. Big neural networks waste capacity. arXiv preprint arXiv:1301.3583,
2013.
D. Eigen, J. Rolfe, R. Fergus, and Y. LeCun. Understanding deep architectures using a recursive
convolutional network. arXiv preprint arXiv:1312.1847, 2013.
T. Garipov, P. Izmailov, D. Podoprikhin, D. P. Vetrov, and A. G. Wilson. Loss surfaces, mode
connectivity, and fast ensembling of dnns. In Advances in Neural Information Processing Systems,
pages 8789-8798, 2018.
A. Grzywaczewski.	Training ai for self-driving vehicles:	the chal-
lenge of scale.	https://developer.nvidia.com/blog/
training-self-driving-vehicles-challenge-scale/, 2017.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016.
G. Huang, Y. Li, G. Pleiss, Z. Liu, J. E. Hopcroft, and K. Q. Weinberger. Snapshot ensembles: Train
1, get m for free. arXiv preprint arXiv:1704.00109, 2017a.
G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional
networks. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 4700-4708, 2017b.
P. Jain, A. Jain, A. Nrusimha, A. Gholami, P. Abbeel, J. Gonzalez, K. Keutzer, and I. Stoica.
Checkmate: Breaking the memory wall with optimal tensor rematerialization. Proceedings of
Machine Learning and Systems, 2:497-511, 2020.
C.	Ju, A. Bibaut, and M. J. van der Laan. The relative performance of ensemble methods with deep
convolutional neural networks for image classification. arXiv preprint arXiv:1704.01664, 2017.
D.	Kondratyuk, M. Tan, M. Brown, and B. Gong. When ensembling smaller models is more efficient
than single large models, 2020.
S. Lee, S. Purushwalkam, M. Cogswell, D. Crandall, and D. Batra. Why m heads are better than one:
Training a diverse ensemble of deep networks. arXiv preprint arXiv:1511.06314, 2015.
O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla,
M. Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of
Computer Vision, 115(3):211-252, 2015.
10
Published as a conference paper at ICLR 2021
V. Sanh, L. Debut, J. Chaumond, and T. Wolf. Distilbert, a distilled version of bert: smaller, faster,
cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.
N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large
neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538,
2017.
S. L. Smith, P.-J. Kindermans, C. Ying, and Q. V. Le. Don’t decay the learning rate, increase the
batch size. In International Conference on Learning Representations, 2018.
V. Sze, Y. Chen, J. S. Einer, A. Suleiman, and Z. Zhang. Hardware for machine learning: Challenges
and opportunities. In 2017 IEEE Custom Integrated Circuits Conference, CICC 2017, Austin,
TX, USA, April 30 - May 3, 2017, pages 1-8,2017a. doi: 10.1109/CICC.2017.7993626. URL
https://doi.org/10.1109/CICC.2017.7993626.
V. Sze, Y.-H. Chen, T.-J. Yang, and J. S. Emer. Efficient processing of deep neural networks: A
tutorial and survey. Proceedings of the IEEE, 105(12):2295-2329, 2017b.
G. Urban, K. J. Geras, S. E. Kahou, O. Aslan, S. Wang, R. Caruana, A. Mohamed, M. Philipose,
and M. Richardson. Do deep convolutional nets really need to be deep and convolutional? arXiv
preprint arXiv:1603.05691, 2016.
A. Wasay, B. Hentschel, Y. Liao, S. Chen, and S. Idreos. Mothernets: Rapid deep ensemble learning.
In Proceedings of the Conference on Machine Learning and Systems (MLSys), 2020.
C.-J. Wu, D. Brooks, K. Chen, D. Chen, S. Choudhury, M. Dukhan, K. Hazelwood, E. Isaac, Y. Jia,
B. Jia, et al. Machine learning at facebook: Understanding inference at the edge. In 2019 IEEE
International Symposium on High Performance Computer Architecture (HPCA), pages 331-344.
IEEE, 2019.
S. Zagoruyko and N. Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.
11
Published as a conference paper at ICLR 2021
APPENDIX
Table A: For all networks, we use training hyperparameters listed in their respective papers (lr:
learning rate, wd: weight decay).
Architecture	Data sets	Epochs	Lr schedule	batch size	Wd
DenseNet	SVHN	100	0.1,0.01(30), 0.001(60)^^	128	0.001
DenseNet	C10 and C100	120	0.1, 0.01(60), 0.001(90)	128	0.001
DenseNet	Tiny ImageNet	90	0.1, 0.01(30), 0.001(60)	64	0.001
DenseNet	ImageNet32-1K	90	0.1, 0.01(30), 0.001(60)	64	0.001
ResNet	C10	250	0.1, 0.01(100), 0.001(200)	128	0.0001
ResNet	C100	500	0.1, 0.01(250), 0.001(375)	128	0.0001
Wide ResNet	C10 and C100	200	0.1,0.01(100), 0.001(200)	128	0.0001
(C) VGGNet CIFAR-10 (k=4)
(a) ResNets CIFAR-10 (k=4)
(b) ResNets CIFAR-100 (k=4)
(％)sEJ jouəss
48
48
48
32
48
32
32
24
24
32
24
16
24
16
16
16
48
48
48
32
48
32
32
24
24
32
24
16
24
16
16
16
62
50
38
62
26
50
38
62
50
26
38
62
26
50
38
26
62
50
38
62
26
50
38
62
50
26
38
62
26
50
38
26
-| 60.5
二 41.91
)| 35.71
| 34.04
二 29.52
| 23.58
| 20.1
| 16.61
| 15.14
| 10.49
| 8.94
| 7.39
| 3.79
| 2.63
| 2.24
| 1.85
8.52
6.77
5.03
3.79
3.28
3.01
2.24
2.13
1.7
0.56
0.37
8.52
6.77
5.03
3.79
3.28
3.01
2.24
2.13
1.7
0.56
0.37
width factor | depth | number of parameters (M)
Figure A: Ensembles arrive at lower test error rates than single network models after the EST has
been reached.
A On the Instability of Width-Equivalent Ensembles
The performance of width-equivalent ensembles (weq ensembles) exhibits unstable behavior. In
particular, it has local spikes when it comes to the test error rates. This is particularly pronounced for
the ResNet ensembles (Figure A(a) and Figure A(b)).
Our interpretation of this phenomenon is that these local spikes have to do with the relative depth of
networks in the width-equivalent ensemble designs. When comparing designs that are close together
in the parameter range, we observe that the designs with more depth (of ensemble networks) generally
outperform those with less depth even if the latter have more parameters. The depth of networks
in the weq ensemble plays a more dominant role than the total number of parameters they have.
As an example of this, consider Figure A(a) and A(b) in the revised version of the paper (ResNet
CIFAR-10 (k=4) and ResNet CIFAR-100 (k=4). Weq exhibit three spikes at 2.24M, 3.28M, and
5.03M parameters. All three of these points are flanked on both sides by designs that have similar
number of parameters but more depth.
This observation is consistent with past observations that depth is more influential in determining the
accuracy of networks (Eigen et al., 2013).
B Additional Experiments in Underfitting Scenarios
We now provide experiments on two additional data sets from underfitting scenarios: (i) ImageNet-
1K-32 a downsampled version of ImageNet-1k with 1.2 million images and 1000 labels (every image
downsampled to be of size 32 × 32), and (ii) CIFAR-100 with aggressive rand-augment (Cubuk et al.,
2020).
The result of these experiment can be seen in Figure I and Figure J respectively. Overall, our
observations hold for these data sets as well. After a certain parameter budget (defined in terms of
number of parameters and training epochs), ensembles of neural networks consistently outperform
the single network and also reach that accuracy faster when compared to the single model.
12
EI
.(AOEJnOOE ləpouɪ 3μoMsu əimuis qopωj JoUUBɔ sumisəpjFqJ Ojeoipui s*q0βUISSμu) SIəpouɪ 3μoMsu
əuɪs UBqjJ əjsbj OS OP OSIB upɔ "ə-ɔ 巴 nɔɔp Esq əpjAOjd upɔ Su-səp ə-uɪəsuə U-M 号-工
(寸工)。。1&4工13 sjəNSOx (O)
(寸工)jəNə00EuIl "∙sI SjəNəsuəɑ (q)
(寸工)。。∙Qi百。SJonOSUOCI (E)
(≡) SJasUIBIBd JO ιaquιnu -Bdəp - J2。必5PTM
(≡) SI2ωuIP.IPdJo Iωqumu - IPdəp - I0s£ ιPPIM
.əɔEdS əojnosəj
lq∙-q əip2ISωə5BU=JIqS Aɔunɔɔp ə-uɪəsuə∙≡UO=ɔnpəj =EjəAo UF∙≡SjInSəj SIql .səspəjɔəp
əuɪəsuə əuɪ s5μoΛΛu WnPlAIPJO "ɔunɔɔpesəIquləsuə JO əs əip əspəjɔiŋ ə 发 Svəjn工

(JAI) UlEJEd UmU - JOsD qldop - JosDsPIΛ∖



∞U3 O'HVHI。ssNəsuəɑ (。)
(9上)O'HVHI。ssNəsuəɑ (q)
bop I
3τβUIS
(bE) OT女HIO SjəNəsuəɑ (E)
∞
Ol
Zl
E
91
(%) 9期J JOJJ9 jsəj
.sɪəpouɪ jəNsəX JOJ UOUəuɪouəiɪd sμμ əmjsuouɪəp əM gəH .ə-uɪəsuə ə-∙s-jowəu
JO jəeɪuɪnu ə-əspəjɔuɪ ə 发 SF Jq∙J ə-2səAOul PIOqSə-ɪ jəAOqO-ΛΛS ə-uɪəsu^əeɪ M əjn-工
{JΛI) SJ9sUIEJEd JO Joqumu --dəp - JOsD5PI出

62
50
38
62
26
50
38
62
50
26
38
62
26
50
38
26
62
50
38
62
26
50
38
62
50
26
38
62
26
50
38
26
62
50
38
62
26
50
38
62
50
26
38
62
26
50
38
26
(寸上)O'HVHIosNSOH (E)
484848248222424224624666
suɔodə io Joqumu
IZoZ XqUI JF jəd^d əɔuəjəjuoɔ F SF p-s=qnd
Published as a conference paper at ICLR 2021
^tπtu) J3<⅛τ3d 3tu=
■ single ■ Weq ■ deq
SJ3<⅛I IiB-s
■UItU) J3<⅛-J3d 3tu= SJ3A-PB12
Mli0
width factor | depth | number of parameters (M)
■ single ■ Weq ■ deq
ιlιllιιl
48-26-3.28
32-50-3.01
3216213.79
4813815.03
4815016.77
4816218.52
width factor | depth | number of Parameters (M)
■UItU) J3<⅛-J3d 3tu=
4
■ single
SJ3<⅛- PB-2
width factor | depth | number of parameters (M)
(a) DenseNets on CIFAR-10
(b) ResNets on CIFAR-10
(c) Wide ResNets on CIFAR-10
Figure E: We break down per epoch training time into: (i) time spent per layer and (ii) total number
of layers. We observe that the total number of layers in the model more significantly determines the
per epoch training time as compared to the width.
■ single ■ Weq ■ deq
12i40i0.19
1216410.39
28i40i0.91
12iio.65
20i40i0.48
48i40i2.59
20i112i2.56
2816.92
20i88i1.7
36i1.4
20i64il
12i112i0.97
48i88i9.32
36i112i8.02
4i64i5.49
36i88i5.31
24.91
2ii3.26
36163.13
width factor | depth | number of parameters (M)
■ single ■ Weq ■ deq
16I38I0.57
16I26I0.37
24124.83
6—5。|。.76
24i62i2.14
241.
3212.47
2413811.27
20.96
48i26i3.3
32ii3.3
32i3i2.25
48i62i8.54
48i50i6.79
48i38i5.05
32i3.8
width factor | depth | number of parameters (M)
width factor | depth | number of parameters (M)
(a) DenseNets on CIFAR-100 (b) DenseNets on Tiny ImageNet (c) ResNets on CIFAR-100
Figure F: Depth-equivalent (deq) ensembles take longer to train per epoch as compared to single
network models. Width-equivalent ensembles (weq) take comparable time.
(StU) BtuIJ3d 3tu=30UAI3JU-
■ single ■ Weq ■ deq
(StU) 3SBtu-J3d 3s 3OU3J3JUI
2 5 15 0
o°o°
(StU) BtUlJ3d 3s 3OU3J3JU-
■ single ■ Weq ■ deq
|40|1.12
|40|1.75
|64|2.27
16413.58
18813.74
i50i0.76
i38i0.57
i26i0.37
i38i1.27
i62i0.96
i26i0.83
i26647
i50i1.71
|62|3.8
i2663
i50i3.03
i38i2.25
i62i2.14
|62|8.54
|50|6.79
|38|5.05
width factor | depth | number of parameters (M)
width factor | depth | number of Parameters (M)
width factor | depth | number of parameters (M)
(a) DenseNets on CIFAR-100
(b) DenseNets on Tiny ImageNet
(c) ResNets on CIFAR-100
Figure G: Ensemble models take longer to perform inference.
■ single ■ Weq ■ deq
48
48
48
32
48
32
32
24
24
32
24
16
24
16
16
16
(°α) BSn Xiotustu .XBtU
(°α) BSn Xiotustu .XBtU
5 0 5 0
() 3^Bsn XJOtu.tu .XBtU
|62|8.54
i50079
|38|5.05
16213.8
i26i3.3
i50i3.03
i38i2.25
i62i2.14
18813.74
16413.58
i64i2.27
i40i1.75
i40i1.12
8819.32
11218.02
64i5.49
88i5.31
112i4.91
88i3.26
64i3.13
40i2.59
112i2.56
64i1.92
8811.7
40i1.48
64i1.01
11210.97
40i0.91
8810.65
4010.48
64i0.39
40i0.19

width factor | depth | number of parameters (M)
width factor | depth | number of Parameters (M)
width factor | depth | number of parameters (M)
(c) ResNets on CIFAR-100
(b) DenseNets on Tiny ImageNet
(a) DenseNets on CIFAR-10
Figure H: Ensemble models are significantly more memory efficient.
14
Published as a conference paper at ICLR 2021
80
60
40
20
SlPOdə JO ιaqιu∏N
both ens.
Weq only
deq only
single
— single -∙-deq -∙-weq
(％) aJ jouə
(.UIuJ) XOEmOOE a∙aUIS Ol əiuij
■ single ■ weq ■ deq
Ll
width factor | depth | number of parameter (M)
width factor | depth | number of parameter (M)
width factor | depth | number of parameter (M)
(a)	Accuracy heat map
(b)	Final test error rate
(c)	Time to single model accuracy



Figure I: We observe the same accuracy and resource related trends for DenseNets trained on
ImageNet32, a downsampled version of the ImageNet-1k data set (Chrabaszcz et al., 2017).
500
400
300
200
100
Snoodə Jo IωqumN
both ens.
Weq only
deq only
single
-•-single -∙-weq -∙-deq
(<⅛) 9I ɪoiɪə 1səɪ
(<⅛) 9aI ɪouə 1səɪ
0,4000020001
■ single ■ Weq ■ deq
iiiihH
Iilll
width factor | depth | number of parameters (M)
width factor | depth | number of parameters (M)
width factor | depth | number of parameters (M)
(a)	Accuracy heat map
(b)	Final test error rate
(c)	Time to single model accuracy
Figure J: We observe the same accuracy and resource related trends for ResNets trained on CIFAR-100
with agressive data augmentation to mimic underfitting scenario. In particular, we use RandAugment
with N = 2andM = 14 (Cubuk et al., 2020).
30”201510
SlPOdə JO ιaqιu∏N
both ens.
Weq only
deq only
single
SlPOdə JO ιaqιu∏N
Weq only
deq only
single
ens.
48
48
28
48
28
28
12
12
12
Oooooo
5 0 5 0 5
(.UIUJ) XoE,m8E aUIS Ol əuɪu
width factor | depth | number of parameter (M)
width factor | depth | number of parameter (M)
width factor | depth | number of parameter (M)

(a) Total budget: 30 epochs	(b) Total budget: 90 epochs (c) Time to single acc. (90 epochs)
Figure K: We repeat our experiments with different total training budgets and set up the learning
rate schedule proportionally for every budget. Here we show results for training DenseNets on the
CIFAR-10 data set. We observe the same trend as the total budget increase, ensembles provide better
accuracy and can train faster.
15
.jəs CJeP nh>s
əip uo pəug SjəNəsuəa JOJ SPUəɪlpəj-əj əojnosəj pup AOEJnOOE JBlPu-S əajə-o əʌv"əjn--工
ə00FSn Ajoulωns
ə00Ful二ədəuɪQ əouəjəjui (ə)
ɪpodə jəd əuɪQ00u-sWJH(P)
UlnU 一 IPdəp 一 I0s£ IPPIM
48∣88∣9.22
36∣112∣7.91
48∣64∣5.41
36∣88∣5.23
28∣112∣4.83
28∣88∣3.19
36∣64∣3.07
48∣40∣2.54
20∣112∣2.5
28∣64∣1.88
20∣88∣1.66
36∣40∣1.45
20∣64∣0.98
12∣112∣0.93
28∣40∣0.89
12∣88∣0.62
2O∣4O∣O.46
12∣64∣0.37
12∣40∣0.18

（H£））撷Sn Xiouiaui ∙χeuι
(≡) SIωsUIP.IPdJo Iωqumu 一 IPdəp 一 I2。EJIPPIM
(JX) SIωsUIP.IPdJo Iωqlunu 一 IPdəp 一 I0s£ IPPI
（S） uɔodə ɪəd əuɪɪj Suxuren
AWnOOF ɪəpouɪ 900∙ss OJ əuɪ^(ɔ)
əjRl jo,liə jsəj if∙sh (q)
dmu JFOqɑRm004 (F)
(≡) J2auIRIBd JO Jaqurnu - ɪpdəp - J0s£5PTM
(≡) JasUIB,IBd JO Jaqumu - ɪpdəp - Jo5£ m-m
(苍 J2au!BIBd JO Jaqumu - ɪpdəp - J0s£5PTM
suɔoɑə io 工叫UxnN
IZoZ XqDl JF jəd^d əɔuəjəjuoɔ F SF pəllsɪ-nd
91
I Deq Width factor ]	EeE	O寸8寸	O寸8寸	O寸8 V") O∖ i—H i—H	kɔ i—H i—H i—H	kɔ i—H i—H i—H	kɔ i—H i—H i—H
I Weq depth ∣	6666681111。。EEE(N(N(N(N IIIIIZEEEE 寸寸寸寸寸 SSSS
I SiIIgleWidthfaCto口	(NO∞<D∞(NO∞<0∞(NO∞<0∞(NO∞<0 I(N(NE 寸 I(N(NE 寸 I(N(NE 寸 I(N(NE
I SilIgle depth ∣	(N(N(N(N 。。。。。寸寸寸寸寸 88888iiii 寸寸寸寸寸 9999988888iiii
jəNəsuəɑ-ɑ ə-pɪ
Published as a conference paper at ICLR 2021
Table C: ResNet
single depth	single width factor	Weq depth	Deq width factor
^^6	^T6	1	1
26	24	8	12
26	32	8	16
26	48	8	24
38	16	8	8
38	24	8	12
38	32	8	16
38	48	8	24
50	16	14	8
50	24	14	12
50	32	14	16
50	48	14	24
62	16	14	8
62	24	14	12
62	32	14	16
62		48		14		24	
Table D: WideResNet
single depth	single width factor	Weq depth	Deq width factor
^^8	"4	TG	~L
28	8	10	3
28	12	10	5
52	4	16	1
52	8	16	3
52	12	16	5
58	4	16	1
58	8	16	3
58		12		16		5	
17