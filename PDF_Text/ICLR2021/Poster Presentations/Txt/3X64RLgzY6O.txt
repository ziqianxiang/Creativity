Published as a conference paper at ICLR 2021
Direction Matters: On the Implicit Bias of
Stochastic Gradient Descent with Moderate
Learning Rate
Jingfeng Wu
Computer Science Department
Johns Hopkins University
Baltimore, MD 21218, USA
uuujf@jhu.edu
Difan Zou
Computer Science Department
University of California, Los Angeles
Los Angeles, CA 90095, USA
knowzou@cs.ucla.edu
Vladimir Braverman
Computer Science Department
Johns Hopkins University
Baltimore, MD 21218, USA
vova@cs.jhu.edu
Quanquan Gu
Computer Science Department
University of California, Los Angeles
Los Angeles, CA 90095, USA
qgu@cs.ucla.edu
Ab stract
Understanding the algorithmic bias of stochastic gradient descent (SGD) is one of
the key challenges in modern machine learning and deep learning theory. Most of
the existing works, however, focus on very small or even infinitesimal learning rate
regime, and fail to cover practical scenarios where the learning rate is moderate
and annealing. In this paper, we make an initial attempt to characterize the partic-
ular regularization effect of SGD in the moderate learning rate regime by studying
its behavior for optimizing an overparameterized linear regression problem. In
this case, SGD and GD are known to converge to the unique minimum-norm so-
lution; however, with the moderate and annealing learning rate, we show that they
exhibit different directional bias: SGD converges along the large eigenvalue di-
rections of the data matrix, while GD goes after the small eigenvalue directions.
Furthermore, we show that such directional bias does matter when early stopping
is adopted, where the SGD output is nearly optimal but the GD output is sub-
optimal. Finally, our theory explains several folk arts in practice used for SGD
hyperparameter tuning, such as (1) linearly scaling the initial learning rate with
batch size; and (2) overrunning SGD with high learning rate even when the loss
stops decreasing.
1	Introduction
Stochastic gradient descent (SGD) and its variants play a key role in training deep learning models.
From the optimization perspective, SGD is favorable in many aspects, e.g., scalability for large-scale
models (He et al., 2016), parallelizability with big training data (Goyal et al., 2017), and rich theory
for its convergence (Ghadimi & Lan, 2013; Gower et al., 2019). From the learning perspective,
more surprisingly, overparameterized deep nets trained by SGD usually generalize well, even in the
absence of explicit regularizers (Zhang et al., 2016; Keskar et al., 2016). This suggests that SGD
favors certain “good” solutions among the numerous global optima of the overparameterized model.
Such phenomenon is attributed to the implicit bias of SGD. It remains one of the key theoretical
challenges to characterize the algorithmic bias of SGD, especially with moderate and annealing
learning rate as typically used in practice (He et al., 2016; Keskar et al., 2016).
In the small learning rate regime, the regularization effect of SGD is relatively well understood,
thanks to the recent advances on the implicit bias of gradient descent (GD) (Gunasekar et al., 2017;
2018a;b; Soudry et al., 2018; Ma et al., 2018; Li et al., 2018; Ji & Telgarsky, 2019b;a; Ji et al.,
2020; Nacson et al., 2019a; Ali et al., 2019; Arora et al., 2019; Moroshko et al., 2020; Chizat &
1
Published as a conference paper at ICLR 2021
(a) Small learning rate regime	(b) Moderate learning rate regime
Figure 1: Illustration for the 2-D example studied in Section 3. Here κ = 4 and w0 = (0.6, 0.6). (a): Small
learning rate regime. The small learning rate is 0.1∕κ. In this regime SGD and GD behave similarly and they
both converge along e2. (b): Moderate learning rate regime. The initial moderate learning rate is η = 1.1/κ
and the decayed learning rate is η0 = 0.1∕κ. In this regime GD converges along e2 but SGD converges along
e1 , the larger eigenvalue direction of the data matrix. Please refer to Section 3 for further discussions.
Bach, 2020). According to classical stochastic approximation theory (Kushner & Yin, 2003), with
a sufficiently small learning rate, the randomness in SGD is negligible (which scales with learning
rate), and as a consequence SGD will behave highly similar to its deterministic counterpart, i.e.,
GD. Based on this fact, the regularization effect of SGD with small learning rate can be understood
through that of GD. Take linear models for example, GD has been shown to be biased towards max-
margin/minimum-norm solutions depending on the problem setups (Soudry et al., 2018; Gunasekar
et al., 2018a; Ali et al., 2019); correspondingly, follow-ups show that SGD with small learning rate
has the same bias (up to certain small uncertainty governed by the learning rate) (Nacson et al.,
2019b; Gunasekar et al., 2018a; Ali et al., 2020). The analogy between SGD and GD in the small
learning rate regime is also demonstrated in Figures 1(a) and 3.
However, the regularization theory for SGD with small learning rate cannot explain the benefits of
SGD in the moderate learning rate regime, where the initial learning rate is moderate and followed
by annealing (Li et al., 2019; Nakkiran, 2020; Leclerc & Madry, 2020; Jastrzebski et al., 2019).
In particular, empirical studies show that, in the moderate learning rate regime, (small batch) SGD
generalizes much better than GD/large batch SGD (Keskar et al., 2016; JaStrzebSki et al., 2017; ZhU
et al., 2019; Wu et al., 2020) (see Figure 3). This observation implies that, instead of imitating the
bias of GD as in the small learning rate regime, SGD in the moderate learning rate regime admits
superior bias than GD — it requires a dedicated characterization for the implicit regularization effect
of SGD with moderate learning rate.
In this paper, we reveal a particular regularization effect of SGD with moderate learning rate that in-
volves convergence direction. In specific, we consider an overparameterized linear regression model
learned by SGD/GD. In this setting, SGD and GD are known to converge to the unique minimum-
norm solution (Zhang et al., 2016; Gunasekar et al., 2018a) (see also Section 2.1). However, with a
moderate and annealing learning rate, we show that SGD and GD favor different convergence direc-
tions: SGD converges along the large eigenvalue directions of the data matrix; in contrast, GD goes
after the small eigenvalue directions. The phenomenon is illustrated in Figure 1(b). To sum up, we
make the following contributions in this work:
1.	For an overparameterized linear regression model, we show that SGD with moderate learning rate
converges along the large eigenvalue directions of the data matrix, while GD goes after the small
eigenvalue directions. To our knowledge, this result initiates the regularization theory for SGD in
the moderate learning rate regime, and complements existing results for the small learning rate.
2.	Furthermore, we show the particular directional bias of SGD with moderate learning rate benefits
generalization when early stopping is used. This is because converging along the large eigenvalue
directions (SGD) leads to nearly optimal solutions, while converging along the small eigenvalue
directions (GD) can only give suboptimal solutions.
3.	Finally, our results explain several folk arts for tuning SGD hyperparameters, such as (1) linearly
scaling the initial learning rate with batch size (Goyal et al., 2017); and (2) overrunning SGD
with high learning rate even when the loss stops decreasing (He et al., 2016).
2
Published as a conference paper at ICLR 2021
2	Preliminary
Let (x, y) ∈ Rd × R be a pair of d-dimensional feature vector and 1-dimensional label. We consider
a linear regression problem with square loss defined as '(χ, y; w) := (w>X - y)2, where W ∈ Rd is
the model parameter. LetD be the population distribution over (x, y), then the test loss is LD(w) :=
E(χ,y)〜D ['(x,y; w)]. Let S ：= {(xi, yi)]n=ι be a training set of n data points drawn i.i.d. from the
population distribution D. Then the training/empirical loss is defined as the average of the individual
loss over all training data points,
1n
LS (w) ：= — £'i(w), where 'i(w) := '(Xi,yi； W) = (w>Xi - yi)2.
n i=1
We use {ηk} to denote a learning rate scheme (LR). Then gradient descent (GD) iteratively performs
the following update:
wk+1
Wk - ηk v LS (Wk)
2ηk n
Wk ——n~y^Xi(x>Wk - yi).
i=1
(GD)
Next we introduce mini-batch stochastic gradient descent (SGD).1 Let b be the batch size. For sim-
plicity suppose n = mb for an integer m (number of mini-batches). Then at each epoch, SGD first
randomly partitions the training set into m disjoint mini-batches with size b, and then sequentially
performs m updates using the stochastic gradients calculated over the m mini-batches. Specif-
ically, at the k-th epoch, let the mini-batch index sets be B1k, B2k, . . . , Bmk , where |Bjk| = b and
Sjm=1 Bjk = {1, 2, . . . ,n}, then SGD takes m updates as follows
Wk,j+1 = Wkj - η X V 'i(Wk,j) = Wkj - 2ηk X Xi(x> Wkj - yi), j = 1,...,m. (SGD)
i∈Bjk	i∈Bjk
We also write Wk+1 = Wk,m+1 and Wk = Wk,1 to be consistent with notations in (GD).
2.1	The minimum-norm bias
Before presenting our results on the directional bias, let us first recap the well-known minimum-
norm bias for SGD/GD optimizing linear regression problem (Zhang et al., 2016; Gunasekar et al.,
2018a; Belkin et al., 2019; Bartlett et al., 2020). We rewrite the training loss as LS (W) =
n ∣∣X >w - Y∣∣2, where X = (χι,...,Xn) ∈ Rd×n and Y = (yι,... ,yn)> ∈ Rn. Then its
global minima are given by W* := {w ∈ Rd : PW = w*, w* := X(X>X)-1Y} , where P is the
projection operator onto the data manifold, i.e., the column space of X. We focus on overparame-
terized cases where W* contains multiple elements.
Notice that every gradient V 'i(w) = 2xi (χ> W - yi) is spanned in the data manifold, thus (GD) and
(SGD) can never move along the direction that is orthogonal to the data manifold. In other words,
(GD) and (SGD) implicitly admit the following hypothesis class:
HS = {w ∈ Rd : P⊥w = P⊥w°},	(1)
where W0 is the initialization and P⊥ = I - P is the projection operator onto the orthogonal
complement to the column space of X .
Putting things together, for any global optimum W ∈ W* (hence PW = W* ), we have
kW - W0k22 = kPW - PW0k22 + kP⊥W - P⊥W0k22 = kW* - PW0k22 + kP⊥W - P⊥W0k22 ,
where the right hand side is minimized when P⊥W = P⊥W0, i.e., W ∈ HS, thus W is the solution
found by SGD/GD in the non-degenerated cases (when the learning rate is set properly so that the
algorithms can find a global optimum). In sum, SGD/GD is biased to find the global optimum that
is closest to the initialization, which is referred as the “minimum-norm” bias in literature since the
initialization is usually set to be zero.
1In this paper we focus on SGD without replacement, nonetheless our results and techniques are ready to
be extended to SGD with replacement as well.
3
Published as a conference paper at ICLR 2021
3	Warming up: A 2-Dimensional Case Study
In this section we conduct a 2-dimensional case study to motivate our understanding on the direc-
tional bias of SGD in the moderate learning rate regime. Let us consider a training set consisting of
two orthogonal points, S = {(x1, y1 = 0), (x2, y2 = 0)} where
xι = √∕K ∙	eι =	(√K,	0)>, x2 =	e2	=	(0,	1)>,	κ >	2.
Clearly w* = 0 is the unique minimum of LS (w). The Hessian of the empirical loss is V2 LS (W)=
x1x1> + x2x2> = diag (κ, 1) , which has two eigenvalues: the smaller one 1 is contributed by data
x2, and the larger oneκ contributed by data x1. Hence LS (w) is κ-smooth. Similarly the Hessian of
the individual losses are V2 'ι(w) = 2xιx> = diag (2κ, 0) and V2 '2(W) = 2x2x> = diag (0, 2).
Thus '2 (W) is 2-smooth, but 'ι(w), the individual loss for data xι, is only 2κ-smooth, which is more
ill-conditioned compared to LS(W) and '2 (w).
Next We consider a moderate initial learning rate η ∈ (K, τ¾) ∙ According to convex optimization
theory (Boyd et al., 2004), gradient step with such learning rate is convergent for LS (W) and '2(W),
but oscillating for '1(W). In other words, (GD) is convergent; and (SGD) is convergent along direc-
tion x2 (or e2), but oscillating along direction x1 (or e1). We also see this by analytically solving
(GD) and (SGD) for this example:
Wkgd = (1 - ηκ)k
(1 - η)k W0,
sgd (1 - 2ηκ)k
Wk =	(1 - 2η)k W0,
(2)
where |1 一 ηκ∣ < |1 一 η∣ < 1 and |1 一 2η∣ < 1 < |1 一 2ηκ∣.
By Eq. (2), with moderate learning rate GD is convergent for both directions e1 and e2 . Moreover,
GD fits eι faster since the contraction parameter is smaller, i.e., |1 一 ηκ∣ < |1 一 n| < 1. Thus
observing the entire optimization path, GD approaches the minimum W* = 0 along e2, which
corresponds to the smaller eigenvalue direction of V2 LS (W). This is verified by the blue dots in
Figure 1(b). We note this directional bias for GD also holds in the small learning rate regime, as
shown in Figure 1(a).
As for SGD in the initial phase where the learning rate is moderate, Eq. (2) shows it converges along
e2 but oscillates along eι since |1 一 2η∣ < 1 < |1 一 2η∣κ∖. In other words, SGD cannot fit eι before
the learning rate decays; however when this happens, e2 is already well fitted. Overall, SGD fits e2
first then fits e1, i.e., SGD converges to the minimum W* = 0 along e1, which corresponds to the
larger eigenvalue direction ofV2 LS (W). This is verified by the red dots in Figure 1(b). We note this
particular directional bias for SGD is dedicated to the moderate learning rate regime; in the small
learning rate regime, as discussed before, SGD behaves similar to GD thus goes after the smaller
eigenvalue direction, which is illustrated in Figure 1(a).
The above idea can be carried over to more general cases: the training loss usually has relatively
smooth curvature because of the empirical averaging; yet some individual losses can possess bad
smoothness condition, corresponding to the data points that contribute to the large eigenvalues of
the Hessian/data matrix. Then with a moderate learning rate, while GD is convergent, SGD is
convergent for the smooth individual losses but oscillating for the ill-conditioned individual losses.
Thus SGD can only fit the latter losses after the learning rate anneals. Therefore, in the moderate
learning rate regime, SGD tends to converge along the large eigenvalue directions while GD tends
to go after the small eigenvalue directions. We will rigorously justify the above intuitions in the
following section.
4	Main Results
In this section we present our main theoretical results. The proofs are deferred to Appendix B.
We specify the population distribution of (x, y) ∈ Rd × R in the following manner. (1) We consider
the feature vector as X = Z ∙ ξ, where Z and ξ are two independent random variables that represent
the magnitude and angle of x, respectively. That is, ζ ∈ R is bounded in (0, 1], and ξ ∈ Rd obeys a
sphere uniform distribution, U(Sd-1). (2) We consider a realizable setting where the label is given
by y = W*>x, i.e., there exists a true parameter W* ∈ Rd that generates the label from the feature
4
Published as a conference paper at ICLR 2021
vector* 2. Then the test loss is LD(W) = E(χ,y)〜D [(w - w*)>xx>(w - w" = μ ∣∣w - w*∣∣2,
where μ = E[Z2]∕d. For an i.i.d. generated training set S = {(xi,期%)}；=[the training loss and the
individual losses are
LS (W) = n (W — w*)> XX > (W — w*),	'i(w) = (W — w*)> XiX> (W — w*) , i = 1,... ,n,
where X = (x1, . . . , xn ). We denote by P the projection operator onto the column space of X (the
data manifold). For i ∈ [n], we denote λi := ∣xi ∣22 = ζi2 ∈ (0, 1]. Without loss of generality,
We assume {1力就网 are sorted in a descending order, i.e., λι ≥ λ2 ≥ ∙∙∙ ≥ λn. With the these
preparations, we are ready to state our main theorems.
4.1	The directional bias of SGD
We first present Theorems 1 and 2 that characterize the different directional biases of SGD and GD
in the moderate learning rate regime.
Theorem 1 (The directional bias of SGD with moderate LR, informal). Suppose d ≥ poly (n)3. De-
note V = n∕√d (which is small). Then with high probability it holds that λι > λ2 + Θ(ν), λn-ι >
λn + Θ (ν) , λn > Θ (ν) . Suppose the initialization is set such that xi> (W0 - W*) 6= 0 for every
i ∈ [n] 4. Consider (SGD) with the following moderate learning rate scheme
Jn ∈ ( λι-Θ(ν) , λ2+Θ(ν) ), k = 1, ... , k1;
[n0 ∈ (0, 2λ1),	k = k1 + 1,...,k2,
(3)
then for e such that Poly (e) > V, there exist kι = O (log ɪ + k2) and k2 > 0 such that with high
probability the output of SGD Wsgd := Wk2 satisfies
P (Wsgd
(1 - e) ∙ γι ≤ ---------
-W*))> ∙ XX> ∙ P (Wsgd
kp (Wsgd- W*)k2
- W*
≤ γ1 ,
(4)
where γ1 is the largest eigenvalue of the data matrix XX>.
Theorem 2	(The directional bias of GD with moderate or small LR, informal). Under the same
conditions as Theorem 1, consider (GD) with the following moderate or small learning rate scheme
nk ∈ N 2λι ;θ(ν) I	k = 1,...,k2
(5)
then for any e > 0, if k2 > O (log ɪ), then with high probability the output of GD Wgd := w^
satisfies
γn ≤ P(Wgd -
W*))> ∙ XX> ∙ P (Wgd - W*
∣p(Wgd- w*)∣2
≤ (I + e) ∙ γn,
(6)
where γn is the smallest eigenvalue of the data matrix XX > restricted in the column space of X.
Remark 1. As the Rayleigh quotient (4) (resp. (6)) converges to its maximum (resp. minimum), the
vector gets closer to the eigenvector of the largest (resp. smallest) eigenvalue (Trefethen & Bau III,
1997). Thus Theorem 1 and 2 suggest that, when projected onto the data manifold, SGD and GD
converge to the optimum along the largest and smallest eigenvalue direction respectively. Here we
are only interested in the projection onto the data manifold, since SGD/GD cannot move along the
direction that is orthogonal to the data manifold as discussed in Section 2.1.
Remark 2. In Theorem 1 we use the gap between λ1 and λ2 to show a learning rate scheme such
that SGD converges along the largest eigenvalue direction. This can be extended by considering
the gap between λr and λr+1 and the learning rate scheme defined similarly, then SGD converges
along the subspace spanned by the eigenvectors of the top r eigenvalues. Similar extension applies
to Theorem 2 as well.
2This is for the conciseness of presentation. Our results can be easily generalized to linear regression with
well-specified noise, i.e., noise that is independent of the feature vector.
3For two sequences {xn ≥ 0} and {yn ≥ 0}: xn = O (yn) if there exist constants C > 0 and N such that
xn ≤ Cyn for every n ≥ N; xn = Θ (yn) ifxn = O (yn) and yn = O (xn). xn = o (yn) if for every > 0
there exists a positive constant N() > 0 such that xn ≤ Cyn for every n ≥ N (); xn = poly (yn) if there
exists large absolute constant D > 0 such that xn = Θ ynD .
4This holds with probability 1 if w0 is initialized randomly and follows, e.g., Gaussian distribution.
5
Published as a conference paper at ICLR 2021
Remark 3. We legitimately assume b < n/2 - Θ (ν) since it is not very meaningful to discuss
SGD that uses more than (roughly) half of the training set as a mini-batch. Then the learning rate
schedule in (3) intersects with that in (5), i.e., (5) covers both moderate and small learning rate
schemes. Their intersection determines a moderate learning rate scheme, where SGD converges
along the large eigenvalue directions while GD goes after the small eigenvalue directions. This
justifies the regularization effect of SGD with moderate learning rate.
Remark 4. Technically, in Theorem 1 one can set b = n to include GD as a special case, so that GD
also follows the large eigenvalue directions. However, the initial learning rate in (3) needs to be at
least λn ≥ n ignoring the small order term, which is way too large to be even numerically stable in
practical big data circumstances. This observation is also directly supported by Theorem 2, where
(5) specifies the range of learning rate such that GD converges along small eigenvalue directions.
Note the upper bound in (5) linearly scales with n, and is large enough to include all learning rate
that can be adopted in practice. Thus one cannot have a legitimate learning rate for GD to converge
along the large eigenvalue directions as SGD does with moderate learning rate.
Note GD with small learning rate also converges along the small eigenvalue directions, since (5)
covers the small learning rate scheme. In complement, the following Theorem 3 shows that in the
small learning rate regime, SGD is imitating GD and converges along the small eigenvalue directions
as well. Theorems 1, 2 and 3 together show that, converging along the large eigenvalue directions is
a distinct regularization effect that is unique to SGD with moderate learning rate.
Theorem 3	(The directional bias of SGD with small LR, informal). Theorem 2 applies to (SGD)
with the following small learning rate scheme
ηk = η0 ∈ (0, 77；~~,bc∕、), k = 1,..., k2.	G)
2λ1 + Θ (ν)
Experiment Figure 2 shows two experiments for verifying Theorems 1, 2 and 3. The details of
the experimental setup are deferred to Appendix D. In Figure 2(a), we run SGD and GD in both
moderate and small learning rate regimes, and directly compare their Rayleigh quotients as defined
in (4) and (6). We can see that the Rayleigh quotient reaches its maximum for SGD with moderate
learning rate, and reaches its minimum for both GD and SGD with small learning rate, which ver-
ifies Theorems 1, 2 and 3. In Figure 2(b), we run the algorithms to optimize a neural network on
a subset of the FashionMNIST dataset. Since the neural network is non-convex and have multiple
local minima, we compare the relative Rayleigh quotients, i.e., the Rayleigh quotients of the conver-
gence directions divided by the maximum absolute eigenvalue of the Hessian (see Appendix D.3).
Figure 2(b) shows that SGD with moderate learning rate converges along relatively large eigenvalue
directions while GD/SGD with small learning rate converges along relatively small eigenvalue di-
rections. This distinguishes the directional bias of SGD and GD in the moderate learning rate regime
and provides evidence from neural network training to support our theory.
4.2 Effects of the directional bias
Next we justify the benefit of the particular directional bias of SGD with moderate learning rate.
Recall the hypothesis class HS (Eq. (1)) for SGD and GD. Then for an algorithm output walg , we
have the following generalization error decomposition (Shalev-Shwartz & Ben-David, 2014),
LD (walg) - inf LD (w) = LD (walg) - inf LD (w0) + inf LD (w0) - inf LD (w) .
w	w0 ∈HS	w0∈HS	w
'------------------{---------------
∆(walg), estimation error
}|
}
{^^^^^^^~
approximation error
The approximate error is an intrinsic error determined by the hypothesis class, and is not improvable
unless enlarging the hypothesis class. In contrast, the estimation error ∆(walg) is determined by the
algorithm as well as its hyperparameters. Thus, in the following theorem, we use the estimation
error to compare the generalization performance of the SGD and GD outputs in different learning
rate regimes.
Theorem 4 (Effects of the directional bias, informal). Let Wα := w ∈ HS : LS (w) = α be
an α-level set of the training loss LS (W). Let ʌa := infw∈wα ∆(w) be the minimum estimation
error within the α-level set Wα. Under the same conditions as Theorems 1- 3 and assuming that
b < n/2 - Θ (ν), then the following holds with high probability:
6
Published as a conference paper at ICLR 2021
1.0
⅛ 0.9
QJ
S 0.8
o
g-0.7
⅛θ-6
*0.5
S. 0.4
0.3
SGD, small LR
---GD, small LR
---SGD, moderate LR
---GD, moderate LR
Ki
Yn
0	2000 4000 6000 8000 10000
# Iteration
(b) Neural network on a subset of FashionMNIST
(a) Linear regression on synthetic data
Figure 2: Comparison of the (relative) Rayleigh quotients. (a): A linear regression example. We randomly
draw 100 samples from a 10, 000-dimensional space as described in Section 4, where Z 〜U([0.5,1]). The
small learning rate scheme is specified by(η0, k2) = (0.2, 104), and the moderate learning rate scheme is
specified by (η, η0, k1 , k2) = (1.05, 0.1, 2 × 103 , 3 × 103). Numerical results show the Rayleigh quotient
converges to its maximum for SGD with moderate learning rate, and converges to its minimum for GD and SGD
with small learning rate, which verifies Theorems 1, 2 and 3. (b): A neural network example. The plots are
averaged over 10 runs. We randomly draw 2, 000 samples from FashionMNIST as the training set. The model is
a 5-layer convolutional neural network. The small learning rate scheme is specified by(η0 , k2) = (10-3 , 104),
and the moderate learning rate scheme is specified by (η, η0, k1 , k2) = (10-2, 10-3, 2.5 × 103, 104). Since
neural network is non-convex, we compare the relative Rayleigh quotient of the concerned algorithms, i.e., the
Rayleigh quotient of the convergence directions divided by the maximum absolute eigenvalue of the Hessian
(see Appendix D.3).
•	The output of (SGD) with moderate LR (3) in Theorem 1 satisfies ∆(wsgd) < (1 + e) ∙ △：, where
α is the training loss of wsgd and is a small constant;
•	The output of (GD) with moderate or small LR (5) in Theorem 2 satisfies ∆(wgd) > M ∙ △[,
where a is the training loss of wgd and M =(1 一 e) ∙ γι∕γn is a constant larger than 1 + e;
•	The output of (SGD) with small LR (7) in Theorem 3 satisfies ∆(wsgd) > M ∙ △[, where a is the
training loss of wsgd and M = (1 — e) ∙ γι∕γn, is a constant larger than 1 + e.
Theorem 4 suggests that: (1) in the moderate learning rate regime, there is a separation between
the test error of SGD and that of GD. In detail, early stopped SGD finds a nearly optimal solution
thanks to its particular directional bias. In contrast, early stopped GD can only find a suboptimal one;
and (2) in the small learning rate regime, however, SGD no longer admits the dedicated directional
bias for moderate learning rate. Instead it behaves similarly as GD, and hence outputs suboptimal
solutions when early stopping is adopted.
Remark 5. In practice it is usually intractable and unnecessary to achieve the exact global minima
ofthe training loss; instead we often early stop the algorithm once obtaining a small enough training
loss, i.e., reaching an α-level set. In this spirit, Theorem 4 compares the generalization ability of
SGD with moderate learning rate vs. GD/SGD with small learning rate within a level set.
Remark 6. We note the second conclusion in Theorem 4 is also obtained by Nakkiran (2020) for
a different purpose. In specific, Nakkiran (2020) show the separation between the test error of GD
with “large” and annealing learning rate, and test error of GD with small learning rate. However,
the “large” learning rate for GD in their analysis is linear in the training sample size and is not
practical as we have discussed in Remark 4. In contrast, we show that under the practically used
moderate learning rate, there is a separation between the generalization abilities of SGD and GD.
To our knowledge, our work gives the first theoretical justification of the phenomenon that SGD
outperforms GD when the learning rate is moderate.
Experiment Figure 3 shows the generalization performance of a neural network trained by SGD
and GD in both moderate and small learning rate regimes. The setup details are included in Ap-
pendix D. We can observe that (1) SGD with moderate learning rate generalizes the best, and (2)
GD and SGD with small learning rate perform similarly, but both are worse than SGD with moderate
learning rate. The empirical results suggest SGD with moderate learning has certain benign regular-
ization effect. This is explained by the distinct directional bias for SGD with moderate learning rate
shown in previous theorems.
7
Published as a conference paper at ICLR 2021
5 Related Work
In this section, we review related works and discuss their
similarities and differences to our work.
The implicit bias of GD The implicit bias of GD has
been extensively studied in recent years. We summa-
rize several representative results as follows. For homo-
geneous model with exponentially-tailed loss, GD con-
verges along the max-margin direction (Gunasekar et al.,
2017; 2018a;b; Soudry et al., 2018; Ma et al., 2018; Ji
& Telgarsky, 2019a; Ji et al., 2020; Nacson et al., 2019a).
For least square problem and its variants, GD is biased to-
wards minimum-norm solution (Gunasekar et al., 2018a;
Ali et al., 2019; Suggala et al., 2018). Note that this is
also the foundation for the learning theory in the interpo-
lation regime such as double descent (Belkin et al., 2019)
and benign overfitting (Bartlett et al., 2020). For matrix
factorization and linear network, Gunasekar et al. (2017);
Li et al. (2018) show the GD solution minimizes nuclear
norm in special cases; more generally, Arora et al. (2019);
Ji & Telgarsky (2019b) show GD balances/aligns layers;
Moroshko et al. (2020) suggest the GD bias relies on ini-
tialization scale and training accuracy. For infinite-width
network, Chizat & Bach (2020) show GD finds a max-
85
80
3 75
y
ra
⅛J7°
①
65
60
SGD, small LR (81.84 ± 0.25)
GD, small LR (81.61 ± 0.29)
——SGD, moderate LR (82.47 ± 0.57)
—— GD, moderate LR (81.46 ±1.10)
0	2000 4000 6000 8000 10000
# Iteration
Figure 3: The test accuracy of a neural
network on a subset of FashionMNIST. The
plots are averaged over 10 runs. The exper-
imental setting is identical to that in Figure
2(b). The plots show that SGD with moder-
ate learning rate achieves the highest test ac-
curacy, and GD and SGD with small learn-
ing rate perform similarly, but are worse than
the former.
margin classifier in a functional space. The regularization theory for GD is fruitful; While some
of them can be applied to SGD with small learning rate as have discussed (Nacson et al., 2019b;
Gunasekar et al., 2018a; Ali et al., 2020), none of them can be carried over to SGD in the moderate
learning rate regime. As far as we know, our paper is the first to study the regularization effect of
SGD with moderate learning rate.
The stability of SGD Stability is another approach to justify the generalization ability of SGD,
where a stable algorithm is guaranteed to have small generalization error (Bousquet & Elisseeff,
2002). Along this line, several works show that SGD is stable under certain assumptions and there-
fore generalizes well (Hardt et al., 2016; Kuzborskij & Lampert, 2018; Charles & Papailiopoulos,
2018; Bassily et al., 2020). More interestingly, Charles & Papailiopoulos (2018) show a simple
example where SGD is stable but GD is not, which partly explains the empirically superior perfor-
mance of SGD. We also aim to justify the benefits of SGD, but take a different approach from the
algorithmic regularization.
The escaping behavior of SGD A popular theory (Jastrzebski et al., 2017; ZhU et al., 2019; Sim-
sekli et al., 2019) attributes the regularization effect of SGD to its behavior of escaping from sharp
minima. These works are built upon the continuous approximation of SGD via stochastic differential
equations (Li et al., 2017; Hu et al., 2017; Xu et al., 2018; Simsekli et al., 2019). However it requires
a small learning rate for the approximation to hold. In contrast, our main interest is in the moderate
learning rate regime. Another related work in this line is (Wu et al., 2018), where they study the
dynamical stability of minima and how SGD chooses them, but they do not show a directional bias
introduced in our work.
Non-small learning rate The regularization effect of non-small learning rate has received increas-
ing attentions recently. Theoretically, Li et al. (2019); HaoChen et al. (2020) study the generalization
of certain stochastic dynamics equipped with annealing learning rate scheme. However, their results
cannot cover vanilla SGD. Lewkowycz et al. (2020) study the role of initial large learning rate for
infinity-width network. Our work is motivated by Nakkiran (2020), which shows that a large initial
learning rate helps GD generalize. This result can be recovered from our theorems; more impor-
tantly, we characterize the directional bias of SGD. Empirically, Jastrzebski et al. (2019); Leclerc &
Madry (2020) investigate the impact of two-phase learning rate for training with SGD, but they do
not provide any theoretical analysis.
8
Published as a conference paper at ICLR 2021
6	Discussions
Linear scaling rule Linearly enlarging the initial learning rate according to batch size, or the linear
scaling rule (Goyal et al., 2017), is an important folk art for paralleling SGD with large batch size
while maintaining a good generalization performance. Interestingly, the linear scaling rule arises
naturally from Theorem 1, where LR (3) suggests the initial learning rate η to scale linearly with
batch size b to guarantee the desired directional bias. Our theory thus partly explains the mechanism
of the linear scaling rule.
Overrunning SGD with high learning rate Besides the moderate initial learning rate, another key
ingredient in Theorem 1 is a sufficiently large k1 , i.e., SGD needs to run with moderate learning rate
for sufficiently long time (to fit small eigenvalue directions). This requirement coincidentally agrees
with the folk art to overrun SGD with high learning rate. For example, see Figure 4 in (He et al.,
2016): from the 〜2 X 105-th to the 〜3 X 105-th iteration, even the training error seems to make no
progress, practitioners let SGD run with a relatively high learning rate for nearly 105 iterates. Our
theory sheds light on understanding the hidden benefits in this “overrunning” phase: indeed the loss
is not decreasing since SGD with high learning rate cannot fit the large eigenvalue directions, but on
the other hand the overrunning lets SGD fit the small eigenvalue directions better, which in the end
leads to the directional bias that SGD converges along the large eigenvalue directions according to
Theorem 1. Thus overrunning SGD with high learning rate is helpful.
Key technical challenges With non-small learning rate, analyzing SGD is usually hard since mea-
sure concentration turns vacuous and as a result one cannot relate the SGD iterates to that of GD.
Alternatively, we control the SGD iterates epoch by epoch, then bound their composition to charac-
terize the long run behavior of SGD. However, controlling the epoch-wise update of SGD is highly
non-trivial, since in different epochs the sequence of stochastic gradient steps varies, and they do not
commute due to the issue of matrix multiplication. To overcome this difficulty, we adopt techniques
from matrix perturbation theory (Horn & Johnson, 2012) and conduct an analysis in the overparam-
eterized regime. We believe these techniques are of independent interest.
7	Conclusion and Future Work
We characterize a distinct directional regularization effect of SGD with moderate learning rate,
where SGD converges along the large eigenvalue directions of the data matrix. In contrast, neither
GD nor SGD with small learning rate can achieve this effect. Moreover, we show this directional
bias benefits generalization when early stopping is adopted. Finally, our theory explains several folk
arts used in practice for SGD hyperparameter tuning.
As an initial attempt, our results are limited to overparameterized linear models, and we ignore
other factors that may contribute to the good generalization of SGD for training neural networks,
e.g., network structures and explicit regularization. It is left as a future work to extend our results to
nonlinear/nonconvex models (with explicit regularization).
Acknowledgement
We would like to thank the anonymous reviewers for their helpful comments. QG is partially sup-
ported by the National Science Foundation CAREER Award 1906169, IIS-2008981 and Salesforce
Deep Learning Research Award. DZ is supported by the Bloomberg Data Science Ph.D. Fellow-
ship. VB is supported in part by NSF CAREER grant 1652257, ONR Award N00014-18-1-2364
and the Lifelong Learning Machines program from DARPA/MTO. JW is supported by ONR Award
N00014-18-1-2364. The views and conclusions contained in this paper are those of the authors and
should not be interpreted as representing any funding agencies.
References
Alnur Ali, J Zico Kolter, and Ryan J Tibshirani. A continuous-time view of early stopping for least
squares regression. In The 22nd International Conference on Artificial Intelligence and Statistics,
pp.1370-1378,2019.
9
Published as a conference paper at ICLR 2021
Alnur Ali, Edgar Dobriban, and Ryan J Tibshirani. The implicit regularization of stochastic gradient
flow for least squares. arXiv preprint arXiv:2003.07802, 2020.
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. In Advances in Neural Information Processing Systems, pp. 7413-7424, 2019.
Peter L Bartlett, Philip M Long, Ggbor Lugosi, and Alexander Tsigler. Benign ovefitting in linear
regression. Proceedings of the National Academy of Sciences, 2020.
RaefBassily, Vitaly Feldman, Cristdbal Guzmdn, and KUnal Talwar. Stability of stochastic gradient
descent on nonsmooth convex losses. arXiv preprint arXiv:2006.06914, 2020.
Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. arXiv
preprint arXiv:1903.07571, 2019.
Olivier Bousquet and Andre Elisseeff. Stability and generalization. Journal of Machine Learning
Research, 2:499-526, 2002.
Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge uni-
versity press, 2004.
Zachary Charles and Dimitris Papailiopoulos. Stability and generalization of learning algorithms
that converge to global optima. In International Conference on Machine Learning, pp. 745-754.
PMLR, 2018.
Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks
trained with the logistic loss. arXiv preprint arXiv:2002.04486, 2020.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochas-
tic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Robert Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and Peter
Richtdrik. Sgd: General analysis and improved rates. arXiv preprint arXiv:1901.09401, 2019.
Priya Goyal, Piotr Dolldr, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro.
Implicit regularization in matrix factorization. In Advances in Neural Information Processing
Systems, pp. 6151-6159, 2017.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in
terms of optimization geometry. In ICML, 2018a.
Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent
on linear convolutional networks. In Advances in Neural Information Processing Systems, pp.
9461-9471, 2018b.
Jeff Z HaoChen, Colin Wei, Jason D Lee, and Tengyu Ma. Shape matters: Understanding the
implicit bias of the noise covariance. arXiv preprint arXiv:2006.08680, 2020.
Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. In International Conference on Machine Learning, pp. 1225-1234, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Roger A Horn and Charles R Johnson. Matrix analysis. Cambridge university press, 2012.
Wenqing Hu, Chris Junchi Li, Lei Li, and Jian-Guo Liu. On the diffusion approximation of noncon-
vex stochastic gradient descent. arXiv preprint arXiv:1705.07562, 2017.
10
Published as a conference paper at ICLR 2021
StanisIaW Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua
Bengio, and Amos Storkey. Three factors influencing minima in sgd. arXiv preprint
arXiv:1711.04623, 2017.
StanislaW Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek Tabor, Kyunghyun
Cho, and Krzysztof Geras. The break-even point on optimization trajectories of deep neural
netWorks. In International Conference on Learning Representations, 2019.
ZiWei Ji and Matus Telgarsky. The implicit bias of gradient descent on nonseparable data. In
Conference on Learning Theory, pp. 1772-1798, 2019a.
ZiWei Ji and Matus Jan Telgarsky. Gradient descent aligns the layers of deep linear netWorks. In 7th
International Conference on Learning Representations, ICLR 2019, 2019b.
Ziwei Ji, Miroslav Dudk Robert E Schapire, and MatUS Telgarsky. Gradient descent follows the
regularization path for general losses. In Conference on Learning Theory, pp. 2109-2136, 2020.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016.
Harold Kushner and G George Yin. Stochastic approximation and recursive algorithms and appli-
cations, volume 35. Springer Science & Business Media, 2003.
Ilja Kuzborskij and Christoph Lampert. Data-dependent stability of stochastic gradient descent. In
International Conference on Machine Learning, pp. 2815-2824. PMLR, 2018.
Guillaume Leclerc and Aleksander Madry. The two regimes of deep network training. arXiv preprint
arXiv:2002.10376, 2020.
Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large
learning rate phase of deep learning: the catapult mechanism. arXiv preprint arXiv:2003.02218,
2020.
Qianxiao Li, Cheng Tai, et al. Stochastic modified equations and adaptive stochastic gradient algo-
rithms. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pp. 2101-2110. JMLR. org, 2017.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized
matrix sensing and neural networks with quadratic activations. In Conference On Learning The-
ory, pp. 2-47. PMLR, 2018.
Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial large
learning rate in training neural networks. In Advances in Neural Information Processing Systems,
pp. 11674-11685, 2019.
Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen. Implicit regularization in nonconvex sta-
tistical estimation: Gradient descent converges linearly for phase retrieval and matrix completion.
In International Conference on Machine Learning, pp. 3345-3354, 2018.
Edward Moroshko, Suriya Gunasekar, Blake Woodworth, Jason D Lee, Nathan Srebro, and Daniel
Soudry. Implicit bias in deep linear classification: Initialization scale vs training accuracy. arXiv
preprint arXiv:2007.06738, 2020.
Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique Pamplona Savarese, Nathan
Srebro, and Daniel Soudry. Convergence of gradient descent on separable data. In The 22nd
International Conference on Artificial Intelligence and Statistics, pp. 3420-3428. PMLR, 2019a.
Mor Shpigel Nacson, Nathan Srebro, and Daniel Soudry. Stochastic gradient descent on separable
data: Exact convergence with a fixed learning rate. In The 22nd International Conference on
Artificial Intelligence and Statistics, pp. 3051-3059. PMLR, 2019b.
Preetum Nakkiran. Learning rate annealing can provably help generalization, even for convex prob-
lems. arXiv preprint arXiv:2005.07360, 2020.
11
Published as a conference paper at ICLR 2021
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-
rithms. Cambridge university press, 2014.
UmUt Simsekli, Mert Gurbuzbalaban, Thanh HUy Nguyen, Gael Richard, and Levent Sagun. On
the heavy-tailed theory of stochastic gradient descent for deep neural networks. arXiv preprint
arXiv:1912.00018, 2019.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The im-
plicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19
(1):2822-2878, 2018.
Arun Suggala, Adarsh Prasad, and Pradeep K Ravikumar. Connecting optimization and regulariza-
tion paths. In Advances in Neural Information Processing Systems, pp. 10608-10619, 2018.
Lloyd N Trefethen and David Bau III. Numerical linear algebra, volume 50. Siam, 1997.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010.
Jingfeng Wu, Wenqing Hu, Haoyi Xiong, Jun Huan, Vladimir Braverman, and Zhanxing Zhu. On
the noisy gradient descent that generalizes as sgd. The 37th International Conference on Machine
Learning, 2020.
Lei Wu, Chao Ma, andE Weinan. How sgd selects the global minima in over-parameterized learning:
A dynamical stability perspective. In Advances in Neural Information Processing Systems, pp.
8279-8288, 2018.
Pan Xu, Jinghui Chen, Difan Zou, and Quanquan Gu. Global convergence of langevin dynamics
based algorithms for nonconvex optimization. In Advances in Neural Information Processing
Systems, pp. 3122-3133, 2018.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma. The anisotropic noise in stochastic
gradient descent: Its behavior of escaping from sharp minima and regularization effects. The 36th
International Conference on Machine Learning, 2019.
12
Published as a conference paper at ICLR 2021
A Preliminaries
A. 1 Additional notations
We adopt the notations and settings in main text. In addition we make the following notations.
For a vector X ∈ Rd, denote its direction as X :=	. For simplicity assume the training data
kxk2
{X1, . . . , Xn} are linear independent. For training data Xi, i ∈ [n], we denote λi = kXik22, then by
construction we have λ% ∈ (0,1]. Without loss of generality let λι ≥ ∙∙∙ ≥ λn. We define
X := (X1,...,Xn) ∈ Rd×n,
X-1 := (X2,X3,...,Xn) ∈ Rd×(n-1).
Then based on the above definitions, we define the following two projection operators
P = X(X>X)-1X>,
P⊥ = I - P.
Clearly for any v ∈ Rd, Pv projects v onto subspace span {X1, . . . , Xn}, while P⊥v projects v onto
the orthogonal complement of span {X1, . . . , Xn}. Furthermore we introduce two more projection
operators
P-1 = X-1(X->1X-1)-1X->1,
P1 = P - P-1 = I - P⊥ - P-1.
For any v ∈ Rd, P-1v projects v onto subspace span {X2, . . . , Xn}, while P1v projects v into the
orthogonal complement of span {X2, . . . , Xn} with respect to span {X1, . . . , Xn}. In the following,
we often write the column space of P, which refers to Pv : v ∈ Rd , similarly for P-1, P1 and
P⊥ as well. Clearly the column space of P is also span {X1, . . . , Xn}; the column space of P-1 is
also the data manifold span {X2 , . . . , Xn }. We highlight that the total space Rd can be decomposed
as the direct sum of the column space of P-1, P1 and P⊥, i.e., I = P-1 + P1 + P⊥. By definition,
it is easy to verify that
P⊥X = 0,
PX = X,
P1 X = (P1 X1 , 0, . . . , 0) ,
P-1X = (P-1X1,X2, . . . ,Xn) .
Then we define the following matrices which will be repeatedly used in the subsequent proof.
H := XX>,
H-1 := (P-1X)(P-1X)>,
H1 := (P1X)(P1X)>,
Hc := (P-1X1)(P1X1)> + (P1X1)(P-1X1)>.
Based on the above definitions, it is easy to show that
H = (P1X+P-1X)(P1X+P-1X)>
= (P-1X)(P-1X)> + (P1X)(P1X)> + (P1X)(P-1X)> + (P-1X)(P1X)>
= H-1 +H1 +Hc.
A.2 Lemmas
We present the following theorems and lemmas as preparation for our analysis.
Theorem (Gershgorin circle theorem, restated for symmetric matrix). Let A ∈ Rn×n be a symmet-
ric matrix. Let Aij be the entry in the i-th row and the j-th column. Let
Ri(A) :=	|Aij | , i = 1, . . . ,n.
j6=i
13
Published as a conference paper at ICLR 2021
Consider n Gershgorin discs
Di(A) := {z ∈ R, |z - Aii| ≤ Ri(A)} , i = 1, . . . , n.
The eigenvalues of A are in the union of Gershgorin discs
n
G(A) := [Di(A).
i=1
Furthermore, if the union of k of the n discs that comprise G(A) forms a set Gk(A) that is disjoint
from the remaining n - k discs, then Gk (A) contains exactly k eigenvalues of A, counted according
to their algebraic multiplicities.
Proof. See, e.g., Horn & Johnson (2012), ChaP 6.1, Theorem 6.1.1.	□
Theorem (Hoffman-Wielandt theorem, restated for symmetric matrix). Let A, E ∈ Rn×n be sym-
metric. Let λ1, . . . , λn be the eigenvalues ofA, arranged in decreasing order. Let λ1, . . . , λn be the
eigenvalues of A + E, arranged in decreasing order. Then
n
X∣^i - λi∣2 ≤kE kF.
i=1
Proof. See, e.g., Horn & Johnson (2012), Chapter 6.3, Theorem 6.3.5 and Corollary 6.3.8.	□
Lemma 1. Let d ≥ 4log(2n2∕δ) for some δ ∈ (0,1). Then with probability at least 1 一 δ, we have
lhχi,χjil < ι := Oe (√1d) ,	i = j.
Proof. SeeSectionC.1.	□
By Lemma 1 we can assume d ≥ poly (n) such that nι is sufficiently small depends on requirements.
The following two lemmas characterize the projected components of each training data onto the
column space of P1, P-1, and P⊥.
Lemma 2. For xj 6= x1, we have
•	P-1 xj = xj ;
•	P1 xj = 0;
•	P⊥ xj = 0.
Proof. These are by the construction of the projection operators.	□
Lemma 3. Assume √nι ≤ 1 /4. With probability at least 1 一 δ, we have
•	0 ≤ ∣∣P-1X1∣∣2 ≤ 2√nι;
•	√1 - 4n∣2 ≤ ∣∣P1X1∣∣2 ≤ 1；
•	P⊥ x1 = 0.
Proof. See Section C.2.	□
The following four lemmas characterize the spectrum of the matrices H, H-1, H1 and Hc.
Lemma 4. Let γ1 , . . . , γn be the n non-zero eigenvalues ofH := XX> in decreasing order. then
λn 一 nι ≤ γ1 , . . . , γn ≤ λ1 + nι.
Furthermore, if there exist λr and λr+1 such that λr > λr+1 + 2nι, then
λn 一 nι ≤ γr+1 , . . . , γn ≤ λr+1 + nι < λr 一 nι ≤ γ1 , . . . , γr ≤ λ1 + nι.
14
Published as a conference paper at ICLR 2021
Proof. See Section C.3.	□
Lemma 5. Assume λn ≥ 3nι. Consider the symmetric matrix H-1 := P-1X(P-1X)> ∈ Rd×d.
•	0 is an eigenvalue of H-1 with algebraic multiplicity being d - n + 1, and its corresponding
eigenspace is the column space ofP1 + P⊥.
•	Restricted in the column space ofP-1, the n - 1 eigenvalues of H-1 belong to
(λn - nι, λ2 + nι).
Proof. See Section C.4.	□
Lemma 6. Consider matrix H1 := P1X(P1X)> ∈ Rd×d. We have H1 has only one non-zero
eigenvalue, which belongs to
[λ1 (1 - 4nι2) , λ1].
Moreover, the corresponding eigenspace is the column space of P1, which is 1-dim.
Proof. Clearly H1 is rank-1 since the column space of P1 is 1-dim. Thus it has only one non-zero
eigenvalue, which is given by
n
tr(HI) = X IIPIxik2 = kP1x1 k2 ∈ [λ1 (1 - 4nι2) , λ1],
i=1
where the last equality follows from Lemma 3.	□
Lemma 7. Consider matrix Hc := (P-1 x1)(P1 x1)> + (P1x1)(P-1x1)> ∈ Rd×d.
∣∣Hc∣∣2 ≤ 2λι kP-ixι∣2 ≤ 4√nι.
Proof.
IHck2 ≤ 2 kP-1x1k2 ∙ kP1x1k2 ≤ 2λι kP-1x1k2 ≤ 4√n∣,
where the last equality follows from Lemma 3 and λ1 ≤ 1.
□
B Missing Proofs for the Theorems in Main Text
B.1 The directional bias of SGD with moderate learning rate
Reloading notations Let πk := B1k, . . . , Bmk be a randomly chosen uniform m-partition of [n],
where n = mb. Then the SGD iterates at the k-th epoch can be formulated as:
Wk,j+1 = Wk,j 一 2ηk X xix> (wk,j 一 w*), j = 1,...,m,
i∈Bjk
where we assume that the learning rate is fixed within each epoch. Note here πk is independently
and randomly chosen at each epoch. For simplicity we often ignore the epoch-indicator k, and write
the uniform partition as π := {B1, . . . , Bm} . It is clear from context that π is random over epochs.
For a mini-batch Bj ∈ π, we denote H(Bj) := Pi∈B xixi>.
Considering translating the variable by
V = W - w*,
then we can reformulate the SGD update rule as
vk,j + 1 = Vkj--牛 H(Bj )vk,j = (I----号 H(Bj j vk,j,	j = 1,...,m.	⑻
15
Published as a conference paper at ICLR 2021
Let
Mn := YY (I- 2ηkH(Bj))
j=1	b
:=(I — 2ηkH(Bm)) ∙ (I - 2ηkH(Bm-I))…(I - 2ηkH(Bι)).
Here the matrix production over a sequence of matrices Mi ∈ Rd×d jm=1 is defined from the left
to right with descending index,
m
" Mj := Mm X Mm-1 ×∙∙∙× Ml.
j=1
Let vk+1 = vk,m+1 and vk = vk,1. Then we can further reformulate Eq. (8) and obtain the epoch-
wise update of SGD as
vk+1 =	(I--bkH(Bm))	∙	(I-bkH(Bm-I))	...	(I--IbkH(BI)) ∙ vk = Mnvk	⑼
In light of the notion of v, the following lemma restates the related notations of loss functions,
hypothesis class, level set, and estimation error defined in Section 2 and 4.
Lemma 8 (Reloading SGD notations). Regarding repramαterization V = W 一 w*, we can reload
the following related notations:
•	Empirical loss and population loss are
LS (v) = l(P1v)>H1(P1v) + 1(P-ιv)> H-I(P-Iv) + 1(Pv)> Hc(Pv),
nn	n
LD(V) = μ ∣∣vk2 .
•	The hypothesis class is
HS = {v ∈ Rd : P⊥v = P⊥vo}.
• The α-level set is
V = v ∈ HS : LS (v) = α}.
• For v ∈ HS, the estimation error is
δ(V) = μ kPvk2.
Moreover,
∆* = inf ∆(v)
v∈V
μnα
Yi
Proof. See Section C.5.
□
Based on the above definitions, the following lemma characterizes the one-step update of SGD.
Lemma 9 (One step SGD update). Consider the j-th SGD update at the k-th epoch as given by
Eq. (8). Set the learning rate be constant η during that epoch. Then for j = 1, . . . , m we have
(Pivk,j+1 ) = (I - 2ηPiH(Bj)Pi	-晋PiH(Bj)P-1 ) ( Pivkj )
P-Ivk,j+J =	-2ηP-iH(Bj)Pi	I - 2ηP-iH(Bj)P-iJ ∙ P-Ivkj
Moreover, if1 ∈/ Bj, i.e., xi is not used in the j-the step, then
(Pivk,j+i ) = (I	0	)	( Pivk,j )
P-ivk,j+i) =	0 I - 2ηP-iH(Bj)P-i)	P-ivk,j
Proof. See Section C.6.
□
16
Published as a conference paper at ICLR 2021
Eq. (9) indicates the key to analyze the convergence of vk+1 is to characterize the spectrum of the
matrix Mπ. In particular the following lemma bounds the spectrum of Mπ when projected onto the
column space of P-1.
Lemma 10. Suppose 3nι < λn. Suppose 0 < η < 入？ +b3n∣ ∙ Let ∏ ：= {Bι,∙∙∙, Bm} be a uniform
m partition of index set [n], where n = mb. Consider the following d × d matrix
M-ι := YY I- - 2ηP,-ιH(Bj)P-ι) ∈ Rd×d.
j=1
Then for the spectrum of M>-1M-1 we have:
•	1 is an eigenvalue of M->1M-1 with multiplicity being d - n + 1; moreover, the corresponding
eigenspace is the column space ofP1 + P⊥.
•	Restricted in the column space of P-1, the eigenvalues of M->1M-1 are upper bounded by
(q-1 (η))2 < 1, where
q-i(η) ：=max ]∣1-2η(λ2 +n∣) +3nηl,	1 - W(λn - nι) +学]< 1.
b b b b
Proof. See Section C.7.
□
Consider the projections of vk onto the column space of P-1 and P1. For simplicity let
Ak := kP-1vkk2 , Bk := kP1vkk2 .
The following lemma controls the update of Ak and Bk .
Lemma 11 (Update rules for Ak and Bk). Suppose 3nι < λn. Suppose 0 < η < 入？ +b3n∣ ∙ Consider
the k-th epoch of SGD iterates given by Eq. (9). Set the learning rate in this epoch to be constant η.
Denote
ξ(η)：=胃
q1 (η) :
ι -竿 kPιXι k2
q-1(η) := max
1 ——b (λ2 + nι)
+ 3nη∣
1----b (λn - nI)
Then we have the following:
•	Ak+ι ≤ q-i(n) ∙ Ak + ξ(η) ∙ Bk.
•	Bk+1 ≤ qι(η) ∙ Bk + ξ(η) ∙ Ak.
•	Bk+1 ≥ qι(η) ∙ Bk - ξ(η) ∙ Ak.
Proof. See Section C.8.
□
Note we can rephrase the update rules for Ak and Bk as
(Bk+1)≤ (q-ι1ηη) 需)∙ (Bk)
where "≤" means “entry-wisely smaller than”.
The following two lemmas characterize the long run behaviors of Ak and Bk with different learning
rate.
17
Published as a conference paper at ICLR 2021
Lemma 12 (The long run behavior of SGD with moderate LR). Suppose 3nι < λn, and λ2 +4nι <
λ1 . Suppose v0 is far away from 0. Consider the first k1 epochs of SGD iterates given by Eq. (9).
Set the learning rate during this stage to be constant, i.e., ηk = η for 0 ≤ k < k1. Suppose
b
b
λι — 3√nι < " < λ2 + 3nι
Then for 0 < e < 1 and 0 < β <	βo	<	Bo	satisfying	√nι ≤ Poly	(eβ), there exists	kι	≥
O (log 5) such that
•	Aki ≤ e ∙ β.
•	BkI ≤ Ilpv0k2 ∙ ρki + f ∙β = poly (A).
•	For all k = 0, 1, . . . , k1, Bk > β0 .
Proof. See Section C.9.
□
Lemma 13 (The long run behavior of SGD with small LR). Suppose 3nι < λn, and λ2 +4nι < λ1.
Suppose v0 is far away from 0. Consider another k2 — k1 epochs of SGD iterates given by Eq. (9).
Set the learning rate to be constant during the updates, i.e., ηk = η0 for k1 ≤ k < k2. Suppose
0 <η < M
2λ1
Consider the e and β given in Lemma 12. Then for k ≥ k1, we have
•	Ak ≤ e ∙ β.
ʃq ∙ Bk-1,
β
k-1 > β, where q ∈ (0, 1) is a constant.
Bk-1 < β.
Proof. See Section C.10.
□
Theorem 5 (Theorem 1, formal version). Suppose 3nι < λn and λ2 + 4nι < λ1. Suppose v0 is
away from 0. Consider the SGD iterates given by Eq. (9) with the following moderate learning rate
scheme
=In ∈ (λι-3√n∣, λ2+3n∣) , k = 1,... ,k1;
11k	[n0 ∈ (0, 2⅛) ,	k = k1 + 1, ...,k2.
Thenfor 0 < e < 1 such that √nι ≤ poly (e), there exist kι > O (log ɪ) and k2 such that
(1 — e) ∙ γι ≤
vk>2Hvk2
kPvk2 k2
≤ γ1 .
Proof. We choose k1 and k2 as in Lemma 12 and Lemma 13 with β set as a small constant, then we
are guaranteed to have
Ak2 ≤ e ∙ 8 ≤ e ∙ Bk2 ,
from where we have
∣Pιvk2∣2 =	Bk
IPVk2 k2	Ak2+B22
≥ —二 ≥ 1 — e2.
—1 + e2 一
18
Published as a conference paper at ICLR 2021
Then we have
v>2 Hvk = (PIvk2 )>H1(P1Vk2) + (P-Ivk2 )>H-1(P-1。%)+ (Pvk2 )丁 Hc(Pvk2)
kPvk2 k2	kPvk2 k2	kPvk2 k2	kPvk2 k2
≥ λι(1 — 4nι2
kP1vk2k2
kPvk2 k2
+ 0 — 4√nι
≥ λι(1 — 4nι2) • (1 — €2) — 4√nι
≥ (γι — nι)(1 — 4nι2) ∙ (1 — €2) — 4√nι	(since γι ≤ λι + nι by Lemma 4)
=γι(1 — 4nι2)(1 — €2) — nι(1 — 4nι2)(1 — €2) — 4√nι
≥ γι(1 — 0.5e) — 0.5γιe	(since √nι ≤ Poly (e))
= γ1 (1 — €).
□
Theorem 6 (Theorem 4 first part, formal version). Suppose 3nι < λn and λ2 + 4nι < λ1. Suppose
v0 is away from 0. Consider the SGD iterates given by Eq. (9) with the following moderate learning
rate schedule
k = Jn ∈ (λι-3√n∣，λ2+3n∣)，k = 1,..., k1;
∣n0 ∈ 仅,衾),	k = kι + 1,..., k2.
Thenfor 0 < e < 1 satisfying √nι ≤ poly(e), there exist kι and k2 such that SGD outputs an
€-optimal solution.
Proof. We set
β0
and apply Lemma 12 to choose a k1 such that
kP-1vkι ∣∣2 ≤ e ∙ 8 = e ∙
kP1 vk k2 ≥ β0
∀0 ≤ k ≤ k1.
(10)
(11)
(12)
(13)
Thus for all 0 ≤ k ≤ k1 ,
LS (vk) = 1(Pvk )>XX T(Pvk)
n
≥ Yn ∣∣Pvk k2	(Yn is the smallest eigenvalue of XXT in the column space of P)
≥ Yn IIPivkII；
> α,	(by Eq. (13))
which implies SGD cannot reach the α-level set during the iteration of first stage, i.e., SGD does not
terminate in this stage.
We thus consider the second stage. From Lemma 13 we know kP1vknk2 will keep decreasing before
being smaller than β, and kP-1vk k2 stays small during this period, i.e., SGD fits P1v while in the
same time does not mess up P-1v. Mathematically speaking, there exists k2 and α such that
Ak2 := kP-Ivk2 k2 ≤ e ∙ β = e ∙
LS (vk2) = α,
19
Published as a conference paper at ICLR 2021
which implies SGD terminates at the k2-th epoch. Then by Lemmas 3 and 8, we have
nα = nLS (vk2)
= (P1vk2)>H1(P1vk2) + (P-1vk2)>H2(P-1vk2) + (Pvk2)>Hc(Pvk2)
≥ (Pivk2n)>H1(Pivk2n) -∣∣p-ιχι∣∣2 ∙ ∣∣Pvk2n ∣∣2
≥ (λ1 - nι)Bk22 - 4nι2 (A2k2 + Bk22 )
≥ (γ1 - 3nι)Bk22 - 4nι2A2k2 ,
which yields
≤ nα + 4：2线 ≤ ° + I) ^ 吧
2 γ1 - 3nι 2 γ1
Then we can bound the estimation error as
NPk2) = μ kPvk2 k2
=〃毓 + AkJ
≤ (1 + i ) ∙竺α + i2 ∙竺α
≤ (1 +1) ∙ μna	1
γ1
= (1 + I) ∙ △*,
where We use the fact that △ = μnɑ∕γι from Lemma 8. Hence SGD is e-near optimal.
□
B.2 The directional bias of GD with moderate or small learning rate
Reloading notations Denote the eigenvalue decomposition of XX > as
XX> = GΓG>, Γ := diag (γ1, . . . ,γn, 0, . . . , 0) , G = (g1, . . . ,gn, . . . , gd) ,
where G ∈ Rd×d is orthonormal, and γ1 , . . . , γn are given by Lemma 4.
Clearly, span {g1, . . . , gn} = span {x1, . . . , xn} . Let
Gk = (g1 , . . . , gn ) , G⊥ = (gn+1 , . . . , gd ) ,
then
P = GkGk>, P⊥ = G⊥G>⊥.
Recall the GD iterates at the k-th epoch:
Wk + 1 = Wk----XX> (Wk — W*).
n
Considering translating then rotating the variable as,
u= G>(W —W*),
then we can reformulate the GD iterates as
2ηk	2ηk
uk+1 = Uk-----γ Uk = I-------γ Uk.	(14)
nn
We present the following lemma to reload the related notations regarding the parameterization U =
G>(w — w*).
Lemma 14 (Reloading GD notations). Regarding reparametrization U = G>(w — w*), we can
reload the following related notations:
• Empirical loss and population loss are
1n	2
LS(U) = - X : Yi (U ' ) , LD (U) = μ ∣∣uk 2 .
n i=1
20
Published as a conference paper at ICLR 2021
•	The hypothesis class is
HS = {u ∈ Rd : u(i) = u0i), for i = n + 1,...,d}.
•	The α-level set is
U = u ∈ Hu : LS (u) = α .
•	Foru ∈ HS, the estimation error is
n2
δ(U) = μ X (U(I)).
i=1
Moreover,
△* = μnα.
γ1
Proof. See Section C.11.
The following lemma sovles GD iterates in Eq. (14).
Lemma 15. For t = 0, . . . , T,
Uki) = (Qk=N1-吗Y)∙
1	≤ i ≤ n;
n + 1 ≤ i ≤ d.
Proof. This is by directly solving Eq. (14) where Γ is diagonal.	□
Theorem 7 (Theorem 2, formal version). Suppose λn + 2nι < λn-1. Suppose U0 is away from 0.
Consider the GD iterates given by Eq. (14) with learning rate scheme
n
2λι+ 2nι
Thenfor e ∈ (0,1), if k ≥ O (log ɪ) , then we have
Yn ≤ -Uk ：；、2 ≤ (I + e) ∙ Yn
Pin=1 U(ki)
ηk ∈
• η, where η ∈(0, 2」i + 2n∣ J ∙ Then We have
nn
Proof. For i = 1,..., n, denote qi(η) = 1 一 2γi
0 < qi(η) < 1 since
n
η < 2(λι + nι) < 2γι ' 2γi,
where the second inequality follows from Y1 < λ1 + nι by Lemma 4. Furthermore, since λn + nι <
λn1 - nι, Lemma 4 gives us
0 < Yn < Yn1 ≤ • • • ≤ Y1 < 1,
(15)
which implies
1 > qn(η) > qn1(η) ≥ • • • ≥ q1 (η) > 0.
(16)
Moreover,
f(η) :
qn-i(n)
qn(η)
1 一 2γn-1 η
n /
1 一 2γn η
nη
□
is increasing, let q = max77< —n— f (η), then q < 1 by our assumption on the learning rate.
2λ1+2nι
From Lemma 15 we have
k-1
U(ki) =	qi(ηt) • U(0i), i = 1, . . . ,n.
=0
(17)
21
Published as a conference paper at ICLR 2021
By the assumption that
log
k> 2 ∙一
Yin Pn=l(u0i) )2
log q
(18)
we have
Pn=I (Uki)) 2	1 + X 上
(Ukn))2	= (Ukn))2
1 + X Qk-o1 m(ηt) .(UOi))2
台 Qk-1 qn(ηt)2 ∙(u0n))2
(by Eq. (17))
≤
≤
1+
1+
2
n-1 k-1
•XY
i=1 t=0
2
k-1
-•n • Y'
t=0
2
qi(ηt)2
qn(ηt)2
qn(ηt)2
(by Eq. (16))
≤
1+
• n ∙ q2k
(by Eq. (18))
which further yields
≤
γn
1 +----e,
γ1
≥ 1 + ⅛e ≥
1 - Yne.
γ1
(19)
1
By the above inequalities we have
U>ΓUk	_
Pn=1 (Uki)) 2
X	(Uki))2	• γ
5 Pn=1 (Uki)) 一
^ Yn + X
Pin=1 (U(ki))	i=1
• Yi
Pin=1 U(ki)
n-1	(U(i))2
≤ Yn + £-----------k/ /、、2 • Yi	(by Eq.(15))
i=1 Pin=1 (U(ki))
γn +
≤ Yn + Yne ∙ Y1	(by Eq. (19))
γ1
= Yn • (1 + e).
Finally we note that
PZrlUk)T
≥ Yn since Yn is the smallest in {Yi}in=1.
□
Theorem 8 (Theorem 4 second part, formal version). Suppose λn + 2nι < λn-1 . Suppose U0 is
away from 0. Consider the GD iterates given by Eq. (14) with learning rate scheme
n
ηk ∈ V, 2λi + 2nι).
22
Published as a conference paper at ICLR 2021
Thenfor e ∈ (0,1) ,if k ≥ O (log ɪ) ,, then GD outputs an M-SuboPtimal solution, where M
γ1 (1 — e) > 1 is a constant.
γn
Proof. Consider an α-level set where
α = LS (Uk ) = 一Uk ruk.
(20)
n
From Lemma 15 we know LS(uk) is monotonic decreasing thus GD cannot terminate before the
k-epoch, i.e., the output of GD is uk .
Thus
∆(u)
∆*
Pn=I (Uki))2
Y1---------------
nα
Pin=1 (U(ki))2
Y1	u>Γuk
1
≥ Y1 ∙ (1 + e)Yn
≥ γ1 (I-e)
γn
=: M,
(by Lemma 14)
(by Eq. (20))
(by Theorem 7)
where We have M > 1 by letting e < 1 — Y.
□
B.3 The directional bias of SGD with small learning rate
We analyze SGD with small learning rate by repeating the arguments in previous two sections.
Let us denote X-n := (x1, x2, . . . , xn-1) and
P-n = X-n(X->nX-n)-1X->n
Pn = P — P-n .
That is, P-n is the projection onto the column space of X-n and Pn is the projection onto the
orthogonal complement of the column space of X-n with respect to the column space of X.
Let us reload
H := XX>,
H-n := (P-nX)(P-nX)>,
Hn := (PnX)(PnX)>,
Hc := (P-n xn )(Pn xn ) + (Pn xn )(P-n xn ) .
Then
H =H-n+Hn+Hc.
Following a routine check we can reload the following lemmas.
Lemma 16 (Variant of Lemma 10). Suppose 3nι < λn. Suppose 0 < η < 、 2 . Let π :=
λ1 +3nι
{	B1, . . . , Bm} be a uniform m partition of index set [n], where n = mb. Consider the following
d × d matrix
M-n := Y I— — 2ηP-nH(Bj )P-n) ∈ Rd"
j=1
Then for the spectrum of M->nM-n we have:
•	1 is an eigenvalue ofM->nM-n with multiplicity being d — n + 1; moreover, the corresponding
eigenspace is the column space ofPn + P⊥.
23
Published as a conference paper at ICLR 2021
•	Restricted in the column space of P-n, the eigenvalues of M->nM-n are upper bounded by
(q-n(η))2 < 1, where
q-n (η) := max
1----b (λ1 + nI)
+ 3nη∣
1 - 2η (λn-1- nι)
Proof. This is by a routine check of the proof of Lemma 10.
□
Consider the projections of vk onto the column space of P-n and Pn . For simplicity we reload the
following notations
Ak := kP-nvkk2 , Bk := kPnvkk2 .
The following lemma controls the update of Ak and Bk .
Lemma 17 (Variant of Lemma 11). Suppose 3nι < λn. Suppose 0 < η < 人］:39.Consider the
k-th epoch of SGD iterates given by Eq. (9). Set the learning rate in this epoch to be constant η.
Denote
qn (η) :
ξ(η) ：= 4⅛⅛
=1	^bn IlPnxnk2 ,
q-n(η) := max
1-
+ nι)
2η
3nηι 2η
+ 丁，1 - ~b (λn
-1 - nι) +
?)< 1.
b
Then we have the following:
•	Ak+ι ≤ q-n(η) ∙ Ak + ξ(η) ∙ Bk.
•	Bk+ι ≤ qn(η) ∙ Bk + ξ(η) ∙ Ak.
•	Bk+ι ≥ qn(η) ∙ Bk 一 ξ(η) ∙ Ak.
Proof. This is by a routine check of the proof of Lemma 11.
□
Lemma 18 (Variant of Lemma 13). Suppose 3nι < λn and λn + 4nι < λn-1. Consider the SGD
iterates given by Eq. (9) with the following small learning rate scheme
ηk = η0∈ (°，2λT+2nι)， k = 1,...,k2
Thenfor 0 < e < 1 satisfying √nι ≤ Poly (e), if k? ≥ O (log ɪ) , then A2 ≤ e.
24
Published as a conference paper at ICLR 2021
Proof. From the assumption We have η0 < 23+n∣) and η0 < 破-,thus
ξ ：= ξ(η,) = 4⅛⅛
qn := qn(η0) = |l - 2ηjn ∣∣PnXnk2
=1 - 2η-λn- kpnχnk2
≤ 1 - "n(l - 'n )η0,	(since ∣∣PnXnk2 ≥ 1 - 4nι2 by reloading Lemma 3 )
<1
q-n ：= q-n(η0) = max j ∣1 - 2η- (ʌi + n∣) +
3nη0ι
b
,1 — 2η0(λn-i- n∣)
+ 3nη0ι
2η0	3nη0ι	2η0	3nη0ι
max J 1----b~ (ʌi + nι) + 《-，1 b~ (λn-i - n∣) + b-
2η0	3nη0ι
1-；-(λn-1 - nι) +-； 
bb
1-
2(λn-1
Moreover, by the gap assumption λn + 4nι < λn-1 We have
qn - q-n ≥ η0
2(λn-1 - nι) - 3nι	2λn(1 - 4nι2)
∖2η0c ∖ 一
≥ -Γ~ (λn-1 - λn - 3nι)
b
> 0.
Therefore 0 < q-n < qn < 1. Thus we can set ξ = 4η 产 to be small such that
b
b
0<q:
q-n
Moreover, since √nι ≤ Poly (e) and ξ
qn - ξ ∙ A0/B0
4η√nι ,wehave
< 1.
(21)
qn - ξ ∙ A0/B0 一
(I - q)e
-2-
(22)
Now we recursively show Bk
Bk+1 in the following
Ak+1 ≤
Bk+1 -
≤ BA0. Clearly it holds for k = 0. Suppose B ≤ ^0, we consider
q-n ∙ Ak + ξ ∙ Bk
qn ∙ Bk - ξ ∙ Ak
q-n ∙聋 十 ξ
qn - ξ ∙ BAk
q-nBBk+ ξ
qn - ξ ∙ Ao/B0
q-n Ak
(by Lemma 17)
(by inductive assumption)
≤
ξ
+ qn→‰
≤
≤
≤
qn - ξ ∙ Ao/Bo Bk
Ak	(1 - q)e
q ∙瓦+ ^^
Ao	(1 - q)e
q ∙瓦 + ^^
Ao
(by Eq. (21) and (22))
Bo,
25
Published as a conference paper at ICLR 2021
where in the last inequality We assume f < B0.
Moreover, from the above we have
Ak+1	Ak	(1 - q)
Bk+1 ≤q ∙ B + —2—
which implies
∕ ≤ 小 2 ∙ Ao + ɪ
Bk2 ≤ q Bo + 1 - q
(I - q)e
-2-

2 + 2= e,
where we set k2 ≥ O (log ɪ).
□
Next we prove the directional bias of SGD with small learning rate.
Theorem 9 (Theorem 3, formal version). Suppose 3nι < λn and λn + 4nι < λn-1. Suppose v0
is away from 0. Consider the SGD iterates given by Eq. (9) with the following small learning rate
scheme
ηk = η0 ∈ (0, χτ——：— ), k = 1,...,k2.
2λ1 + 2nι
Thenfor 0 < e < 1 satisfying √nι ≤ Poly (e), if k2 ≥ O (log ɪ) , then
≤ V1 Hvk2
Yn - kPVk2 k2
≤ (1 + e) ∙ Yn.
Proof. First by Lemma 18 we have
Ak + V,
1
e2 + 1
≥ 1 - e2.
(23)
Next by H = Hn + H-n + Hc we obtain
v>2 Hvk2 = (Pnvk2 )>Hn(PnVk2 ) + (P-nVk2 )> H-n (P-nVk2 ) + (Pvk2 )>Hc(Pvk2 )
kPvk, k2 -	kPvk, k2	kPvk, k2	kPvk2 k2
≤ λn	∙	k4vk2k;	+	(λ1	+	n∣)	∙	kP-nvk2k2 +	4√n∣	by reloading Lemma 5, 6, 7
一	kPvk, k2	kPvk, k2
Ak	L
≤ λn + (λ1 + nι) ∙ A2^^+ 2.2 + 4√nι
≤ γn + nι + (λ1 + nι) ∙ e2 + 4√nι	by reloading Lemma 4 and Eq. (23)
≤ Yn + Yn ∙ e.	since √nι ≤ poly (e)
Finally, we note vk2~vk⅜ ≥ Yn since Yn is the smallest eigenvalue of H restricted in the column
IF k2
space of P.	□
Theorem 10 (Theorem 4 third part, formal version). Suppose 3nι < λn and λn + 4nι < λn-1.
Suppose v0 is away from 0. Consider the SGD iterates given by Eq. (9) with the following small
learning rate scheme
ηk=η0∈ (0,2Λ1τ2m),	k=1,...,k2.
Then for 0 < e < 1 such that √n∣ ≤ poly (e), if k2 ≥ O (log ɪ) , then SGD outputs an M-
SuboPtimaI solution where M = Yγ1 (1 一 e) > 1 is a ConStant
26
Published as a conference paper at ICLR 2021
Proof. From Eq. (9) and W < + We know that restricted in the column space of P, the eigenvalues
ofMπ is smaller than 1, thus vk indeed converges to 0.
Consider an α-level set where		α=	LS(vk) =	n v>2 Hvk2.
Then	∆(u) ∆*	= γ1	∣pvk∣2 nα	(by Lemma 8)
		= γ1	∣pvk∣2 • —	 vk>Hvk	(by Eq. (24))
		≥ γ1	1	(by Theorem 9)
			(I + e)γn	
		≥ γ1 (1 - e)		
		γn		
: M,
where we have M > 1 by letting e < 1 - Yn.
(24)
□
C Proof of Auxiliary Lemmas in Sections A and B
C.1 Proof of Lemma 1
ProofofLemma 1. Note that Xi follows uniform distribution on the sphere Sd-1. Therefore, let
ξ be a random variable following distribution Xd distribution and define Zi = ξ ∙ Xi, we have
zi follows standard normal distribution in the d-dimensional space. Then it suffices to prove that
|hzi,zji|/(kzik2kzjk2) ≤ ιforalli 6=j.
First we will bound the inner product hzi, zji. Note that we have each entry in zi is 1-subgaussian,
it can be direcly deduced that
d
hzi,zji = X zi(k)zj(k) = X
k=1
is d-subexponential, where zi(k)
k=1
(k)	(k)
Zi	+ Zj
2
(k)	(k)	2
Zi	- Zj	)
denotes the k-th of the vector Zi . Then if follows that
P (lhZi,Zjil ≥ t) ≤ 2exp (-td∙).
d
Next we will lower bound ∣Zi ∣2 . Note that
d
∣Zi ∣22 - d = X
2
k=1
Since Zi(k) is 1-subgaussian, we have ∣Zi∣2 - d is d-subexpoential, then
P (|kZik2- d∣ ≥t) ≤ 2eχp (一∙d∙).
Finally, applying the union bound for all possible i, j ∈ [n], we have with probability at least 1 - δ,
the following holds for all i 6= j ,
匚 2n2
ιhzi,zj i| ≤ Vd log-,
IlZill 2 ≥ d- d d log ɪ- ∙
Assume d ≥ 4log(2n2∕δ), we have ∣∣Zi∣2 ≥ d/2. Then it follows that
,,	_ . .	∣hzi, Zji∣ C ∕Γ^	2n2
lhxi, Xj i| = μ^^∣∣^^∣∣~~H- < 2∖ lo log "ɪ =: l∙
∣Zi ∣2 ∣Zj∣2	d δ
This completes the proof.
□
27
Published as a conference paper at ICLR 2021
C.2 Proof of Lemma 3
Proof of Lemma 3. Similar to the proof of Lemma 1, we consider translating x1 , . . . , xn to
z1, . . . , zn by introducing χd2 random variables. Let Z-1 = (z2, . . . , zn) ∈ Rd×(n-1), in which
each entry is i.i.d. generated from Gaussian distribution N (0, 1). Then we have
P-1X1 = X-1(X>1X-1)-1X>1X1 = Z-ι(Z>ιZ-ι)-1Z>ιXι.
Then conditioned on Xi, We have each entry in Z>1X1 i.i.d. follows N(0,1). Then it is clear that
∣∣Z>1X11∣2 follows from χ2τ-ι distribution, implying that with probability at least 1 - δ0, we have
kZ>ιXik2 ≤ (n - 1) + P(n - 1)log(2∕δ0).
Then by Corollary 5.35 in Vershynin (2010), we know that with probability at least 1 - δ0 it holds
that
√d - √n - 1 - P2 log(2/6O) ≤ σmin(Z-I) ≤ σmaχ(Z-I) ≤ √d + √n - 1 + ʌ/2 log(2/6O).
Therefore, assume，(n - 1) +，2 log(2∕δ0) ≤ √d∕8, we have with probability at least 1 - δ0
IIZ-I(Z>1Z-1)T∣∣2 ≤ √d + √n-1 +，2log(2/S0)
√d - √n - 1 - p2 log(2∕δ0))
≤√d(1+4(rniɪ+r u")).
Combining with the upper bound of ∣ Z>iX112, set δ0 = δ∕2, we have with probability at least 1 - δ
that
kP-1X1k2 ≤ ∣∣Z-1(Z>1Z-1)-1∣∣2 ∙kX>1X1k2
≤(1+4 (rniɪ+产严!! ∙ L+sρWg≡)
≤ (1 + 4√nι) ∙ √nι,
where the last inequality follows from the definition of ∣. Then assume √n∣ ≤ 1 ∕4, we are able to
completes the proof of the first argument. NOtethatkP1X11∣2 + kP-ιXι∣∣2 = ∣∣Xι∣∣2 = 1, we have
IlP-IX1k2 = ∖J∖ -∣∣PιXιk2 ≥ p1 — 4nι2 ≥ 1 - 4nι2.
This completes the proof of the second argument. The third argument holds trivially by the con-
struction of P⊥.
□
C.3 Proof of Lemma 4
Proof of Lemma 4. Clearly XX> ∈ Rd×d is of rank n and symmetric, thus XX> has n real, non-
zero (potentially repeated) eigenvalues, denoted as γ1, . . . , γn in non-decreasing order. Moreover,
γ1 , . . . , γn are also eigenvalues of X> X ∈ Rn×n , thus it is sufficient to locate the eigenvalues of
X>X, where (X>XIij = χ>Xj.
We first calculate the diagonal entry
(X >X )ii= X>Xi = λi.
Then we bound the off diagonal entries. For j 6= i,
(X>X) ij = X>Xj = √λiλjE,Xji ∈ (-∣, ∣),
where we use 0 < λ1, . . . , λn ≤ 1. Thus we have
Ri(X>X) = Xl(X>X)ij∣ ≤ nι, i =1,...,n,
j6=i
Finally our conclusions hold by applying Gershgorin circle theorem.	□
28
Published as a conference paper at ICLR 2021
C.4 Proof of Lemma 5
Proof of Lemma 5. The first conclusion is clear since by construction, we have P-1P1 = P-1P⊥ =
0.
Note that H-1 is a rank n - 1 symmetric matrix. Let τ2, . . . , τn be the n - 1 non-zero eigenvalues
of H-1. Clearly, τ2, . . . , τn with τ1 := 0 give the spectrum of
H-0 1 := (P-1X)>P-1X ∈ Rn×n.
We then bound τ2, . . . , τn by analyzing H-0 1.
From Lemma 3 We have ∣∣P-1X1 |卜 ≤ 2√nι. From Lemma 2 We have
P-1X = (P-1x1, P-1x2, . . . ,P-1xn) = (P-1x1, x2, . . . ,xn) .
Then We calculate the diagonal entries:
∣∣P-ιxιk2 ≤ λι ∙ 4nι2 ≤ 4nι2, i = 1;
kxik22 = λi,	i 6= 1.
Then We bound the off diagonal entries. Letj 6= i. Then at least one of them is not 1. Without loss of
generality let i 6= 1, Which yields xi = P-1xi by Lemma (2). Thus hxi, P1xji = hP-1xi, P1xji =
0. Thus We have
(H-l)ij = (p-1xi)> P-1xj
= xi>P-1xj
= xi> xj - xi> P1 xj
>
xi xj
∈ (-ι, ι) .
Thus We have
Ri(H-I)=XkH-Jijl≤nι,	i=1,...,n.
j6=i
Finally, We set 4nι2 + 2nι < λn, so that the first Geoshgorin disc does not intersect With the others,
then Gershgorin circle theorem gives our second conclusion.	□
C.5 Proof of Lemma 8
Proof of Lemma 8. For the empirical loss, it is clear that
LS(v) = — (w — w*)> XXT (w — w*) = 'v>XXTv = 'v>Hv
n	nn
=1(Pv)TH (Pv)
n
=1(Pv)THI(Pv) + 1(Pv)TH-I(Pv) + 1(Pv)THc(Pv)
nn	n
=1(Piv)tHi(Piv) + 1(P-ιv)TH-I(P-Iv) + 1(Pv)THc(Pv),
nn	n
Where We use Lemma 4, Lemma 5, and Lemma 6. For the population loss,
LD(V) = μ kw — w*k2 = μ Ilvk2.
For the hypothesis class HS = {w ∈ Rd : P⊥w = P⊥ wo }, applying w—w* = v and wo—w* = vo,
We obtain
HS = {v ∈ Rd : P⊥v = P⊥vo}.
For the α-level set, We note the optimal training loss is L*S = inf v∈HS LS (v) = 0.
29
Published as a conference paper at ICLR 2021
As for the estimation error, We note that inf'veHSs LD(V) = infp⊥v=p⊥vo μ Ilvll2 = μ ∣∣P⊥vo∣∣2.
thus for v ∈ HS , we have
δ(V) = LD(v) - inf LD(VO) = μ ∣∣vk2 - μ ∣∣P⊥v0k2 = μ ∣∣Pvk2 .
v0∈V
Finally, consider v ∈ V, i.e., nα = v>XX>v, thus
∆* = inf ∆(v) = inf μ IlPvk2 = μnα
*	ν∈V ' n	nα=v>XX >v	2	γi ,
Where γ1 is the largest eigenvalue of the matrix XX> and the inferior is attended by setting
parallel to the first eigenvector of XX> .
v
□
C.6 Proof of Lemma 9
Proof of Lemma 9. From Eq. (8) We have
vk,j+1 = I- - 2ηH(Bj C vk,j, j = 1,...,m.	(25)
Recall the folloWing property of projection operators:
P1 =P1P1, P-1 = P-1P-1
0 = P1P-1 = P-1P1.
Moreover since xi>P⊥v = 0, We have
H(Bj )P⊥v = X xixi>P⊥v = 0.
i∈Bj
Applying P1 to Eq. (25) We have
Pivk,j+i	= Pi	I-	W (Bj η	vk,j
	= Pi	(I-	2bηH (Bj))	(Pivk,j + P-i vk,j + P⊥ vk,j )
	= Pi	(I-	2bηH (Bj))	Pivkj+ PI (I 孑H(Bj D PTvkj
	=(I	-2η	PiH(Bj)P	i) ∙ Pivk,j - (^bɪPiH(Bj)P-i) ∙ P-ivk,j.
Similarly applying P-i to Eq. (25) We have				
P-ivk,j+i =	P-i	I-	2bηH (Bj))	vk,j
=	P-i	I-	2bηH (Bj))	(Pi vk,j + P-ivk,j + P⊥ vk,j )
=	P-i	I-	2bηH (Bj))	Pivkj + P-i (I - ~b~H(Bj)) P-ivk,j
=	-(2bηP-		iH(Bj)Pi	• Pivkj + (I	b~P-iH(Bj)P-i) ∙ p-ivk,j
To sum up We have
(P1vk,j+1 ∖ = (I-2PiH(Bj)Pι	-晋PiH(Bj)P-1 ∖ ( Pιvk,j ʌ
P-ιvk,j+ι) = - 2η P-iH (Bj )Pi	I -第 P-IH(Bj )P-J . P-Ivkj
Notice that if 1 ∈/ Bj , i.e., xi is not used in the j-th step, then We claim
PiH(Bj) =H(Bj)Pi =0,
30
Published as a conference paper at ICLR 2021
since H(Bj) = Pi∈B xixi> is composed by the data belonging to the column space of P-1. There-
fore if 1 ∈/ Bj we have
(P1vk,j + 1 A = (I	0	A ∙ ( P1vk,j A
IP-Ivk,j+J = 0 I - 2bη P-IH(Bj )P-J	p-ιvk,j
□
C.7 Proof of Lemma 10
Proof of Lemma 10. Clearly for each component in the production, the column space of P1 + P⊥,
which is (n - d + 1)-dimensional, belongs to its eigenspace of eigenvalue 1, which yields the first
claim.
In the following, we restrict ourselves in the column space of P-1. Let us expand M-1:
m 2η
M-ι = ∏(I- TPTH(Bj)P-ι)
=(I - "1H(Bm)P-1)…(I - "iH(BI)P-1)
2m
=I -乎 X P-ιH(Bj)P-1
j=1
'------{z----}
H-1
+ ( 2bη A	X P-ιH (Bj )P-ιH (Bi)P-I + ....
1≤i<j≤n
|
{z
C
}
We first analyze matrix H-1. Since H(Bj) = Pi∈B xixi> and π = {B1, . . . , Bm} is a partition for
index set [n], we have
m
H-1 = XP-1H(Bj)P-1
m
= X P-1 X xixi>P-1
j=1	i∈Bj
n
= P-1	xixi>P-1
i=1
= P-1XX>P-1,
which is exactly the matrix we studied in Lemma 5, and from where we have H-1 has eigenvalue
zero (with multiplicity being n-d+ 1) in the column space ofP1 +P⊥, and restricted in the column
space of P-1, the eigenvalues of H-1 belong to (λn - nι, λ2 + nι).
Then we analyze matrix C .
P-1H(Bj)P-1H(Bi)P)-1 = P-1 X xi0 xi>0 P-1	P-1 X xj0 xj>0 P-1
i0∈Bi	j0 ∈Bj
= X X (P-1xi0)hP-1xi0, P-1xj0 i(P-1xj0)>.	(26)
i0∈Bi j0 ∈Bj
Remember that Bi ∩ Bj = 0 for i = j, thus xi，= xj∙∕ for i0 ∈ Bi and j0 ∈ Bj. Then from Lemma 1
We have,	_____
IhP-IXi0, P-IXj0 i| ≤ lhχi0, χj0 i| ≤ pλi0 λj0 ∙ ι ≤ ι.
31
Published as a conference paper at ICLR 2021
Inserting this into Eq. (26) we obtain
kP-iH(Bj)P-iH(Bi)P)-ikF ≤ b2 ∙ max	,P-iXj0i|2 ≤ b2∣2.
We can bound the Frobenius norm of the higher degree terms in matrix C in a similar manner; in
sum for the Frobenius norm of C, we have
m
kCkF≤X
s ∙ bs ∙∣s ∙(m
s=2
m
X B •(:
m
X(2η∣)s ∙
1 - 2mηι
s=0
= (1+2ηι)m- 1 - 2mηι
≤ 1 + m ∙ 2ηι + m ɔZze ∙ (2ηι)2 — 1 — 2mηι	(for 2ηι < —ɪ-)
2	2m
≤ 4m2η2 ι2,
where for the second to the last inequality We notice that for f (t) = (1 + t)m and t ∈ [0,*],we
have f "(t) = m(m —1)(1+ t)m-2 ≤ m(m —1)(1 + 2m)m-2 ≤ m(m — 1) ∙√e, which implies f (t)
is (m2√e)-smooth for t ∈ [0, *]; moreover, by the assumption that 3nι < λn and η <
we can indeed verify that
b
λn+3nι,
2bι
2bι	1
幼,< λn + 3nι — 6nι — 2m'
(27)
Now we rephrase M->1M-1 as
M->1M-1 = I -
I-
2η
~b
2η
~b
H-1 +
I-
H-12+C> I-
2η
~b
2η
~b
H-1 + C
H-ι) + I- - 2ηH-ι) C + C>C
i ——	,
(28)
{z
D
Restricting ourselves in the column space of P-1, the eigenvalues of H-1 belong to
(λn — n∣, λ2 + n∣), thus the eigenvalues of(I —筌H-ι) are upper bounded by
max
1 — 2η (λ2 + nι))2
(29)
where the last inequality is guaranteed by our assumptions on η and ι. For simplicity we defer the
verification to the end of the proof.
Consider the following eigen decomposition I — 2ηH-ι = U diag (μ1,...,μn-1,1,..., 1) C >,
where μι,..., μn-ι ∈ ( —1,1) by Eq.(29). Then we have
k(I — ηH-i)C kF = 11 diag (μι,…，μn-i,1,...,1)C >CC∣∣F
≤ U>CUF= kCkF.
Therefore we can bound the Frobenius norm of D by
kD∣∣F ≤ 2 k(I - 2ηH-ι)。||尸 + ∣∣C∣∣F
≤ 2kCkF+ kCk2F
≤ 8m2η2ι2 + 16m4η4ι4
≤ 9m2η2ι2,	(30)
32
Published as a conference paper at ICLR 2021
where the last inequality follows from 2ηι ≤ 1 /(2m) proved in Eq. (27).
Finally, applying Hoffman-Wielandt theorem with Eq. (28), (29) and (30), we conclude that, re-
stricted in the column space of P_1, the eigenvalues of M11M-ι are upper bounded by
max ] (1 — ɪ(λ2 + n∣)) + 9m2η2ι2, (1 — ɪ(λn — n∣)) + 9m2η212 }
≤ max
{(
1 — 2η (λ2+叫
1 — 2η (λn — n∣)
b
+ 3mηι
(q-ι(η))2.
(31)
At this point we left to verify Eq. (29) and
q-ι(η) := max
1 ——b (λ2 + ni)
+ 3nηι
1 — 2η (λn — nι)
b
(32)
Clearly it suffices to verify Eq. (32).
1 - 2η (λ2 + nι) +?< 1
b	b
⇔
⇔
U
U
Similarly, we verify that
3nι	2(λ2 + nι)	3nι
~Tη - 1 < 1-b—η< 1 — 万η
2λ2-nι
b-
2λ2+5nι^ / 9
b 〃 < 2
η>0
2λ2 — nι> 0
η < λ2 + 2.5nι
3nι < λn	(since λ2 ≥ Xn)
0<η< x⅛
⇔
⇔
U
U
1 -苧(λn - nι) +?< 1
b b
"η - 1 < 1 - 2(λn - nl) η< 1 —空η
b	b	b
2λn-5nιη > 0
2λn+nιC / Ω
b- 〃 < 2
η>0
2λn — 5nι > 0
η < λn+0.5nι
{3n∣ < λn
0 <η< ⅛
(since λ2 ≥ λn)
These complete our proof.
C.8 PROOF OF LEMMA 11
ProofofLemma 11. Note that during one epoch of SGD updates, χ1 is used for only once. Without
loss of generality, assume SGD uses xi at the l-th step, i.e., 1 ∈ Bi and 1 ∈ Bj for j = l. Recursively
(
η > 0
□
33
Published as a conference paper at ICLR 2021
applying Lemma 9, We have
GPvZ+11) = (I ∏m=ι+1 (I- $P-IH(鸟)P-1)) ×
(I-筌P1H(Bl)PI	-2ηP1H(Bl)P-I A ×
-筌 P-1H (Bl)PI I -争 P-IH (Bj )P-J
(I	。	A γ ( P1vk,1 A
I。∏j=1 (I -筌P-1H(Bj)P-1)J × IP-Ivk, J
Let vk+1 = vk,m+1, vk = vk,1 and
Ml= I - 2ηP-ιH(Bl)P-I
M>l ：= Y (I- PP-IH(Bj)P-J
j=l+1 ∖	)
M<l := Y (i---∕p-1H(Bj)P-1)
j=1 ∖	)
M-ι := M>l ∙ Ml ∙ M<l = YY(I -号P-IH(Bj)P-J
j=1 ∖	/
then We have
(P1Vk+1 A =	(I	0	A(I	-筌P1H(Bl)PI	-筌P1H(Bl)P-IA	(I	0 A (	PlVk A
P-IVk+J = 0 M>l	-筌 P-1H (Bl)P1	Ml	0 M<l	P-IVkJ
(I -筌P1H(Bl)P1	-(筌P1H(Bl)P-1) M<lA ( P. A
-M>l (筌P-1H(Bl)P1)	M-1	P-IVkJ
In the following we bound the norm of each entries in the above coefficient matrix.
According to Lemma 5, we have the eigenvalues of P_1H(Bj )P-1 are upper bounded by λ + nι.
Thus the assumption η <』 yields
I - 2ηP-1H(Bj)P-1	≤ 1,
which further yields
∣∣M>l∣∣2 ≤ 1,	∣∣M<l∣∣2 ≤ 1.
On the other hand notice that P1Xi = 0 for i = 1, thus
(34)
P1H(Bl )P-1 = P1 E XiX>P-1 = P1x1x>P-1,
i∈Bι
P-1H(Bl )P1 = P-1 X XiXJPI = P-1X1X;P1,
i∈Bι
which yield
max{∣∣P1H(Bl)P-1∣∣2, ∣∣P-1H(Bl)P1∣∣2} ≤ ∣∣P1X1∣∣2 ∙ ∣∣P-1 X1k2 ≤ 2√⅛,	(35)
where the last inequality is from Lemma 3 and λ1 = ∣∣X1∣∣2 ≤ 1. Eq. (34) and (35) imply
max	(IPIH(Bl)P-1j M<l
M>l (2bηP-1H(Bl)PI) ≤ ≤ 4ηbni =: ξ(η) (36)
Next, by P1Xi = 0 for i = 1 we have
P1H(Bl)P1 = P1 X Xix>P1 = P1X1 x>P1 = (P1X1)(P1X1 )τ,
i∈Bi
34
Published as a conference paper at ICLR 2021
from where we know kP1x1 k22 is the only non-zero eigenvalue of the rank-1 matrix P1H(Bl)P1, and
the corresponding eigenspace is the column space of Pi. Therefore 1 - 2η ∣∣P1χ1 k2 is an eigenvalue
of the matrix I - 2η PiH (Bi)Pi, and the corresponding eigenspace is the column space of Pi, which
implies
I-
2 = l(ι- 2bη "Piχik2) Pivk
=∣1 - 2bη kPiXik2 ∙ kPivkk2
=: qi㈤∙ kPivkl∣2.
(37)
Finally, according to Lemma 10, we have, restricted in the column space ofP-i, the right eigenval-
ues of M-i is upper bounded by (q-i (η))2 , which implies
kM-iP-ivk∣∣2 ≤ q-i(η) ∙ kP-ivkk2.	(38)
Note we have q-i(η) < 1 by Lemma 10.
Combining Eq. (33) with Eq. (36), (37), (38), and letting Bk := kPivkk2 , Ak := kP-ivkk2, we
obtain
Bk+i ≤ qi(η) ∙ Bk + ξ(η) ∙ Ak
Bk+i ≥ qi(η) ∙ Bk - ξ(η) ∙ Ak
Ak+i ≤ q-i ∙ (In)Ak + ξ(η) ∙ Bk.
□
C.9 Proof of Lemma 12
Proof of Lemma 12. Let
ξ := ξ(η) =当丝
qi := qi(η) = 1 - 2n" kPixik2
(39)
q-i := q-i(η)
max < 11 -(入2 + nι)
+ 3nη∣
1 - 2η (λn - nι)
b
Then for 0 < k ≤ ki , Lemma 11 gives us
Bk ≥ qiBk-i - ξAk-i,
(40)
(41)
where "≤" means “entry-wisely smaller than”.
Let θ, ρ-i, ρi determine the eigen decomposition of the coefficient matrix, i.e.,
q-i ξ cos θ sinθ	ρ-i	0 cos θ - sin θ
ξ	qi	= - sin θ cos θ 0 ρi sin θ cos θ
(42)
35
Published as a conference paper at ICLR 2021
Then Eq. (41) and Eq. (42) yield
cos θ	sin θ	ρk-1	0	cos θ	- sin θ	A0
- sin θ	cos θ	0	ρ1k	sin θ	cos θ	B0
ρk-1 + (ρ1k - ρk-1) sin2 θ	(ρ1k - ρk-1) cos θ sinθ	A0
(ρ1k - ρk-1) cos θ sin θ	ρ1k - (ρ1k - ρk-1) sin2 θ	B0
=(A0 ∙ ρ-ι + (Pk — ρ-ι) (Ao Sin θ + Bo cos θ) Sin θ
I Bo ∙ ρk + (Pk — ρ-ι) (Ao cos θ — Bo sin θ) sin θ
≤ (AO ∙ P-1 + ∣ρk - P-11 pA2 + B2 sin θʌ
一IBO ∙ ρk + ∣ρk - ρ-ι∣ PA + BO sin θ J
= (AO ∙ ρ-1 + IPk-P-1∣ ∙ ∣∣PvOk2 ∙ sin θʌ
一IBO ∙ Pk + ∣Pk - P-1∣ ∙ kpvOk2 ∙ sin θ 广
(43)
We claim the following inequalities hold by our assumptions:
0<P-1 < 1 <P1 ≤q1+ξ	(44a)
P-1ιkPvo∣2 ≤ I ∙ β	(44b)
PkIkPvok2 sinθ ≤ I ∙ β,	(44c)
ξ ∙ (AO +	< (qi - I)βo∙	(44d)
The verification of Eq. (44) is left later. In the following we prove the conclusions using Eq. (44).
We first bound Ak1 using Eq. (43) and Eq. (44):
Aki ≤ Ao ∙ P-1 + ∣Pk1 — P-11∣ ∙ ∣Pvo∣2 ∙ sinθ
≤ kPvo∣2 ∙ P-I + Pk1 ∙kPvo∣2 ∙ sinθ
≤1∙β+l ∙ β
=e ∙ β,
which justifies the first conclusion. In addition we can obtain an uniform upper bound for Ak for
k = 0, 1, . . . , k1:
Ak ≤ Ao ∙ Pk-1 + ∣∣P1k — Pk-1 ∣∣ ∙ kPvo k2 ∙ sin θ
≤ Ao + P1k ∙ kPvok2 ∙ sin θ
≤ Ao + ∣ ∙ β.	(45)
Next we bound Bk1 using Eq. (43) and Eq. (44):
Bk1 ≤ Bo ∙ P1k1 + ∣∣P1k1 — Pk-11 ∣∣ ∙ kPvo k2 ∙ sin θ
≤ kP vok2 ∙ P1k1 + P1k1 ∙kPvok2∙sinθ
≤kPvoko ∙ Pk1 + ∣ ∙ β,
which justifies the second conclusion.
We proceed to derive the uniform lower bound for Bk for k = 0, 1, . . . , k1. We do it by induction.
For k = 0, by assumption we have Bo ≥ βo . Suppose Bk-1 ≥ βo , then by Eq. (40), (44) and (45)
36
Published as a conference paper at ICLR 2021
we have
Bk ≥ qi ∙ Bk-I - ξ ∙ Ak-I
≥ qi ∙ βo 一 ξ ∙ (A0 + 2e)
≥ qi ∙ Bo 一 ξ ∙ (A0 + 2万。)
≥ β0,
Whichjustifies the third conclusion.
Verification ofEq. (44)
From Eq. (42) and Gershgorin circle theorem we have
qi 一 ξ ≤ pi ≤ qi + ξ,
q-i - ξ ≤ p-i ≤ q-i + ξ∙
(46)
Moreover, reformatting Eq. (42) as
q-i
ξ
ξ cos θ sin θ ρ-i
qi	— — sin θ cos θ 0
ρ-i cos2 θ + pi sin2 θ
I (pi — ρ-i) cos θ sin θ
0 cos θ - sin θ
ρi sin θ cos θ
(pi — ρ-i) cos θ sin θ
ρ-i sin2 θ + ρi cos2 θ
ρ-i + (pi — ρ-i) sin2 θ	(pi — ρ-i) cos θsin θ
(pi — ρ-i) cos θ sin θ	pi — (pi — ρ-i) sin2 θ
we then have
ξ =	(pi - p-i) cos θ Sin θ	= 1 f
qi — q-i	(pi - p-i)(1 — 2sin2 θ)	2 ɛɪɪ
(47)
For Eq. (44a), using Eq. (46) it suffices to show
0 < qi — ξ,
q-i + ξ < 1,
1 < qi ― ξ∙
(48a)
(48b)
(48c)
Notice the definitions of qi, q-i and ξ are given in Eq. (39). Firstly, Eq. (48c) holds trivially when
√n > 4/3. Secondly, for Eq. (48b), noticing that ξ = 4η√nι ≤ 竿 when n ≥ 16, it suffices to
show
max
4nη∣
2η
1 b^ (λ2 + nι) +------b-, 1------b^ (Xn - ni) +
b
2(λn -nl)
b
η< 1 -等 η
-η< 1 —等 η
⇔
[华 η - 1 < 1 —
lɪ η — 1 < 1 —
2(λ2 +n∣)
b
b
4nηι]	T
ɪ( <1
'2入2 — 2nι
b
2λ2 + 6nι
⇔
η > 0
η < 2
-η > 0
勺< 2
b
2λn,-6n ι
b
2λn + 2n ι
b
V
V
η > 0
λ2 — nι > 0
< λn — 3nι > 0
η < λ2+3nι
η < λn+nι
{3nι< Xn
0 <η< "
37
Published as a conference paper at ICLR 2021
which are given in assumptions. Thirdly, for Eq. (48c) it suffices to show
2ηλ1	2	4η√nι、ι
~~b~ kPιxιk2 - 1-b— > 1
Q 2λ1(1-4n∙ η — 丝飞> 2	(byLemma3)
bb
b
U η › λι(1 — 4nι2) — 2√nι
b
Q η >  ----C L ,	(Smce nι < 1)
λ1 — 3 nι
which are given in aSSumptionS.
For Eq. (44b), it SufficeS to Show Set
kι = 1 + ¾≡ = O N ⅛),
aS given in aSSumptionS.
For Eq. (44c), uSing Eq. (44b) it SufficeS to Show
sin θ ≤
P-ι
Pi
1 1 _ log P1
0.5eβ )	log ρ-ι
kpv⅛J
Q
Q
Q
sin θ ≤
q-i ― ξ
qi + ξ
0.5eβ
ρv⅛J
log(qι+ξ)
log(q-1 -ξ)
ξ ≤ 0.9(qι — q-i) •
/	1 _ logSι+,
q-l — ξ ( 0.5eβ	1-log(q-ι-ξ)
qi + ξ IkPvOlU
(by Eq. (47))
√n∣ ≤ poly (eβ).	(by Eq. (39))
For Eq. (44c), it SufficeS to Show
e - (qi - 1)β0
ξ ≤ —----------—
Ao + 0.5eβo
√n∣ ≤ O (1).	(by Eq. (39))
C.10 Proof of Lemma 13
Q
□
Proof of Lemma 13. Let
E = ξ(η0) = 4η0√nι,
qi := qi(η0) = 1 — 2bλi kPixik2
q-0 i := q-i(η0) = max
Then for ki < k ≤ k2 , Lemma 11 giveS uS
1 -半(λ2 + nι) +*,
q-0 i
1----b~ (λn — nι)
where "≤" means “entry-wisely smaller than”. Denote
B := IIPvok2 ∙ PkI + 2 ∙β = poly ()
+ 3nη0ι
(49)
(50)
(51)
38
Published as a conference paper at ICLR 2021
We claim the following inequalities hold by our assumptions:
0 < q10 < q-0 1 < 1,	(52a)
ξ0∙ E ≤ q-1 - q1,	(52b)
ξ0∙ B ≤ (1-q-ι) ∙ e∙ β.	(52c)
The verification of Eq. (52) is left later. In the following we prove the main conclusions in the lemma
using Eq. (52). We proceed by induction. Clearly the conclusions are true for k = k1 . Suppose for
k1 , . . . , k - 1, the conclusions are also true. Then the induction assumptions give us
Ak-1 ≤ 〜β,	(53)
Bk-1 ≤ Bk1 ≤ B,	(54)
where the last inequality is due to B ≥ Bk1 ≥ β0 > β. Then by Eq. (50) we have
Bk ≤ ql ∙ Bk-I + ξ0 ∙ Ak-I
≤ q'ι ∙ Bk-1 + ξ0 ∙ e ∙ β	(by Eq. (53))
≤ q1 ∙ Bk-1 + (q-ι-q1) ∙ β (by Eq. (52b))
≤ [V BjBkT >β,	(byEq.(52a))
β,	Bk-1 ≤ β.
Also by Eq. (50) we have
Ak ≤ q-ι ∙ Ak-I + ξ0 ∙ Bk
≤ q-ι ∙ e ∙ β + ξ ∙ B (by Eq. (53) and (54))
≤ q-ι ∙e ∙ B + (1 - q-1) ∙e ∙ B	(by Eq.(52C))
=e ∙ β.
Verification of Eq. (52) Notice the definitions of q10 , q-0 1 and ξ0 are given in Eq. (39). Recall
q0 ι < 1 is already justified by the choice of learning rate η0 < 、 JQ (e.g., see Lemma 10), thus
-	λ2 +3nι
for Eq. (52a), it suffices to show
2η0λ1	2	2η0	3nη0ι
0 < 1-------b- IlPIxI k2 < 1----b- (λn - nι) +-----b-
U
U
Ji-9 > 0
2λ1 (1 - 4nι2) > 2(λn - nι) + 3nι
(η0 < 长
λ1 > λn + 2nι
(by Lemma 3)
which are given in assumptions.
For Eq. (52b), it suffices to show
U
ι≤O
which is implied by nι ≤ poly (eB).
For Eq. (52c), it suffices to show
ξ0 ≤ 五∙ e • (1 - q-l) β
B
U √nι ≤ Poly (eβ).	(by Eq. (51))
We complete our proof.
□
39
Published as a conference paper at ICLR 2021
C.11 Proof of Lemma 14
Proof of Lemma 14. For the empirical loss,
LS(u) = — (w — w*)> XX> (W — w*) = U>G>τXX>Gu = U>ΓUu
n	nn
n2
1X Yi (u(i)).
i=1
For the population loss,
LD(u) = μ ∣∣w — w*∣∣2 = μ IlGuk2 = μ ∣∣u∣∣2 .
For the hypothesis class HS = w ∈ Rd : P⊥w = P⊥w0 , Note P⊥G = diag (0, . . . , 0,1, . . . ,1).
Apply w — w* = Gu and notice wo — w* = Gu0, then We obtain
HS = {u ∈ Rd : P⊥Gu = P⊥Guo}
={u ∈ Rd : u(i) = UOi), for i = n + 1,...,d}.
For the level set, we only need to note that L*S = infu∈HS LS (u) = 0.
As for the estimation error, we note that
d2
UnULD(U) = μ X (UOi)),
i=n+1
thus for u ∈ U , We have
Nu) = L(U)- inf L(U) = μ Iluk2
u0 ∈U
d2
-μ X (UOi))
d
μ X (u(i))
i=n+1
i=n+1
2 - μ X	(UOi)) 2
i=n+1
Now consider U ∈ U, i.e., n Pn=1 Yi (u(i))2 = α, then
A =UnU N(U)= na=Pn=n Yi(S2 J
where the inferior is attended when, e.g., U(I) = ±^PYa and u(2)
u(n) =0.
□
D Details of the Experiments
In this section, we describe the details for our experiments.
D.1	2-D example
This part corresponds to Section 3 and Figure 1.
The two training data points are
xι = (√κ, 0)>,	X2 = (0, 1)>, K = 4,
and the corresponding individual losses are
'ι(w) = w>xιx>w,	'2(w) = w>x2x> w.
Then the training loss is
LS (w) = 0.5('ι(w) + '2(w)).
40
Published as a conference paper at ICLR 2021
We initialize the algorithms from w0 = (0.6, 0.6)>. Two kinds of learning rate regime are consid-
ered. In the small learning rate regime, the learning rate is
ηk =0.1∕κ,	k = 1,..., 800.
In the moderate learning rate regime, the learning rate is
1.1∕κ, k = 1, . . . , 100;
ηk = 0.1∕κ, k = 101,. ..,800.
For SGD, the mini-batch size is 1.
D.2 Linear regression on synthetic data
This part corresponds to Section 4 and Figure 2(a).
The model is an overparameterized linear model, with d = 104 and n = 100. The true parameter
w* is randomly drawn from an d-dimensional Gaussian distribution, N(0,0.1 ∙ Id×d).
We then randomly draw n = 100 samples from the d-dimensional space as described in Section 4,
where Z 〜U([0.5,1]).
We initialize the algorithms from zero. We consider two kinds of learning rate regimes. The small
learning rate scheme is specified by
ηk = 0.2, k = 1, . . . , 104 .
The moderate learning rate scheme is specified by
1.05, k = 1, . . . , 2 × 103;
ηk = 0.1,	k = 1 +2 × 103,. ..,3 × 103.
For SGD, the mini-batch size is 1.
D.3 Neural network on a subset of FashionMNIST
This part corresponds to Figure 2(b) and Figure 3.
Model We use a LeNet-alike convolutional network:
input ⇒ conv1 ⇒ ReLU ⇒ max_pool ⇒ conv2 ⇒ ReLU ⇒
max_pool ⇒ fc1 ⇒ ReLU ⇒ fc2 ⇒ ReLU ⇒ linear ⇒ output.
The first convolutional layer uses 5 × 5 kernels with 10 channels and no padding and the second
convolutional layer uses 5 × 5 kernels with 16 channels and no padding. The number of hidden units
between the two fully connected layers are 60.
Dataset https://github.com/zalandoresearch/fashion-mnist
We randomly choose 2, 000 original test data as our training set, and use the 60, 000 original training
data as our test set. Thus we have 2, 000 training data and 60, 000 test data. We scale the image data
to [0, 1].
Algorithms We randomly initialize the algorithms from a Gaussian distribution with zero mean
and standard deviation 0.02. We consider two kinds of learning rate regimes. The small learning
rate scheme is specified by
ηk = 10-3, k = 1, . . . , 104.
The moderate learning rate scheme is specified by
10-2, k= 1,...,2.5 × 103;
ηk = 10-3, k = 1 +2.5 × 103,. ..,104.
For SGD, the mini-batch size is 25. For both GD and SGD, the weight decay parameter is set as
0.002.
41
Published as a conference paper at ICLR 2021
85
80
65-
ro
= 75-
2 70
ω
	7rr	—		
			
	UΓ7			
l	∖	——GD, LR=0.001 (82.22) GD, LR=0.01 (82.61) ——GD, LR=0.02 (82.45) ——GD, LR=0.04 (81.63) -GD, LR=0.08 (78.85) ——GD, LR=0.16 (10.00)	
			
60
0	2000 4000 6000 8000 10000
# Iteration
(a) GD with increasing LRs
85-
80
A
u
ro
= 75-
u
u
2 70
ω
Φ
65-
60
—— SGD, LR=0.001 (81.85)
SGD, LR=0.01 (83.37)
——SGD, LR=0.02 (82.76)
——SGD, LR=0.04 (80.76)
SGD, LR=0.08 (10.00)
—— SGD, LR=0.16 (10.00)
O 2000 4000 6000 8000 IOOOO
# Iteration
(b) SGD with increasing LRs
Figure 4: Test accuracy vs. number of iteration for algorithms with increasing learning rates. The model
is a 5-layer convolutional neural network, and the dataset is a subset of FashionMNIST dataset. Details are
described in Appendix D.3.
Table 1: Test accuracy (%) of GD/SGD in different LR on a neural network example
Experiment	#1	#2	#3	#4	#5	#6	#7	#8	#9	#10
SGD small LR	81.57	81.85	81.71	81.74	82.29	81.38	82.10	82.05	81.95	81.77
GD small LR	81.45	81.25	82.22	81.73	81.89	81.32	81.71	81.75	81.43	81.35
SGD moderate LR	82.32	83.37	81.40	82.30	82.58	82.61	81.68	83.24	82.53	82.65
GD moderate LR	81.54	82.62	78.39	81.81	81.86	80.91	81.84	82.15	81.65	81.83
Relative Rayleigh quotient We discuss the relative Rayleigh quotients calculated in Figure 2(b).
Unlike the linear regression model where the Hessian is a constant, for neural networks the loss
function is non-convex. In other words, not only there are multiple local minima, but also the
Hessian varies at different points. Therefore, a direct comparison in terms of the Rayleigh quotients
for the iterates of different algorithms makes little sense. Instead, we consider the relative Rayleigh
quotient, i.e., the Rayleigh quotient normalized by the maximum absolute eigenvalue of the Hessian
at that point. Mathematically, the relative Rayleigh quotient is defined as
RRQ(w) :
▽L(w)>
k∕w)k2
・ V2L(w) ∙
▽ L(W)
k∕w)k2
kV2L(w)∣∣2
where VL(w)/ ∣∣VL(w)k2 is the convergence direction of gradient methods, and ∣∣V2L(w)k2 is the
operator norm of the Hessian, i.e., its maximum absolute eigenvalue. Note that it is computationally
hard in practice to project a parameter onto the data manifold, thus we use the vanilla convergence
direction instead of the projected one in the above definition of the relative Rayleigh quotient. Since
our goal is to compare the relative Rayleigh quotient between different algorithms, this simplification
would not affect our conclusions. We obtain the maximum absolute eigenvalue of the Hessian by
running power method for 5 iterates.
Additional experiments: GD and SGD with increasing learning rates We conduct numerical
experiments of training neural networks on a subset of FashionMNIST dataset for SGD and GD
with different learning rates η ∈ {0.001, 0.01, 0.02, 0.04, 0.08, 0.16}. The test accuracy results are
displayed in Figure 4.
Several conclusions can be drawn from the plots. (1) For a learning rate over η = 0.08, SGD cannot
converge (thus only gives about 10% test accuracy). SGD generalizes best with a moderate learning
rate η = 0.01. (2) GD fails to converge when η = 0.16. Also, as the learning rate increases, the test
accuracy of GD first increases then decreases; but even at its peak (η = 0.01), GD performs worse
than SGD with a moderate learning rate.
42
Published as a conference paper at ICLR 2021
Paired t-test for Figure 3 We also conduct statistical test to show SGD with moderate learn-
ing rate is significantly better than the other baselines. Recall that the experiments are repeated
for 10 runs, at each run, we first fix a random seed, then run the four algorithms (GD/SGD with
small/moderate learning rate) under the same seed. In Table 1, we report the complete results from
the 10 runs. By running a paired t-test at the 5% significance level, we find that SGD with moder-
ate learning rate is significantly better than GD with moderate learning rate (p-value = 0.0043).
Similarly, SGD with moderate learning rate is significantly better than SGD with small learn-
ing rate (p-value = 0.012), and is also significantly better than GD with small learning rate
(p-value = 0.0095).
43