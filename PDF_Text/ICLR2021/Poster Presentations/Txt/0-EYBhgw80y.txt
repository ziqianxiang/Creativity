Published as a conference paper at ICLR 2021
MoPro: Webly Supervised Learning with
Momentum Prototypes
Junnan Li, Caiming Xiong, Steven C.H. Hoi
Salesforce Research
{junnan.li,cxiong,shoi}@salesforce.com
Ab stract
We propose a webly-supervised representation learning method that does not suf-
fer from the annotation unscalability of supervised learning, nor the computa-
tion unscalability of self-supervised learning. Most existing works on webly-
supervised representation learning adopt a vanilla supervised learning method
without accounting for the prevalent noise in the training data, whereas most prior
methods in learning with label noise are less effective for real-world large-scale
noisy data. We propose momentum prototypes (MoPro), a simple contrastive
learning method that achieves online label noise correction, out-of-distribution
sample removal, and representation learning. MoPro achieves state-of-the-art per-
formance on WebVision, a weakly-labeled noisy dataset. MoPro also shows su-
perior performance when the pretrained model is transferred to down-stream im-
age classification and detection tasks. It outperforms the ImageNet supervised
pretrained model by +10.5 on 1-shot classification on VOC, and outperforms
the best self-supervised pretrained model by +17.3 when finetuned on 1% of
ImageNet labeled samples. Furthermore, MoPro is more robust to distribution
shifts. Code and pretrained models are available at https://github.com/
salesforce/MoPro.
1 Introduction
Large-scale datasets with human-annotated labels have revolutionized computer vision. Supervised
pretraining on ImageNet (Deng et al., 2009) has been the de facto formula of success for almost
all state-of-the-art visual perception models. However, it is extremely labor intensive to manually
annotate millions of images, which makes it a non-scalable solution. One alternative to reduce an-
notation cost is self-supervised representation learning, which leverages unlabeled data. However,
self-supervised learning methods (Goyal et al., 2019; He et al., 2019; Chen et al., 2020a; Li et al.,
2020b) have yet consistently shown superior performance compared to supervised learning, espe-
cially when transferred to downstream tasks with limited labels.
With the help of commercial search engines, photo-sharing websites, and social media platforms,
there is near-infinite amount of weakly-labeled images available on the web. Several works have
exploited the scalable source of web images and demonstrated promising results with webly-
supervised representation learning (Mahajan et al., 2018; Sun et al., 2017; Li et al., 2017; Kolesnikov
et al., 2020). However, there exists two competing claims on whether weakly-labeled noisy datasets
lead to worse generalization performance. One claim argues that the effect of noise can be over-
powered by the scale of data, and simply applies standard supervised learning method on web
datasets (Mahajan et al., 2018; Sun et al., 2017; Li et al., 2017; Kolesnikov et al., 2020). The
other claim argues that deep models can easily memorize noisy labels, resulting in worse general-
ization (Zhang et al., 2017; Ma et al., 2018). In this paper, we show that both claims are partially
true. While increasing the size of data does improve the model’s robustness to noise, our method
can substantially boost the representation learning performance by addressing noise.
There exists a large body of literature on learning with label noise (Jiang et al., 2018; Han et al.,
2018; Guo et al., 2018; Tanaka et al., 2018; Arazo et al., 2019; Li et al., 2020a). However, existing
methods have several limitations that make them less effective for webly-supervised representation
learning. First, most methods do not consider out-of-distribution (OOD) samples, which is a major
1
Published as a conference paper at ICLR 2021
Figure 1: Illustration of the normalized embedding space learned with MoPro. Samples from the same class
gather around their class prototype, whereas OOD samples are separated from in-distribution samples. Label
correction and OOD removal are achieved based on a sample’s distance with the prototypes.
source of noise in real-world web datasets. Second, many methods perform computation-heavy
procedures for noise cleaning (Jiang et al., 2018; Li et al., 2019; 2020a), or require access to a set
of samples with clean labels (Vahdat, 2017; Veit et al., 2017; Lee et al., 2018), which limit their
scalability in practice.
We propose a new method for efficient representation learning from weakly-labeled web images.
Our method is inspired by recent developments in contrastive learning for self-supervised learn-
ing (He et al., 2019; Chen et al., 2020a; Li et al., 2020b) We introduce Momentum Prototypes (Mo-
Pro), a simple component which is effective in label noise correction, OOD sample removal, and
representation learning. A visual explanation of our method is shown in Figure 1. We use a deep
network to project images into normalized low-dimensional embeddings, and calculate the prototype
for a class as the moving-average embedding for clean samples in that class. We train the network
such that embeddings are pulled closer to their corresponding prototypes, while pushed away from
other prototypes. Images with corrupted labels are corrected either as another class or as an OOD
sample based on their distance to the momentum prototypes.
We experimentally show that:
•	MoPro achieves state-of-the-art performance on the upstream weakly-supervised learning task.
•	MoPro substantially improves representation learning performance when the pretrained model is
transferred to downstream image classification and object detection tasks. For the first time, we
show that weakly-supervised representation learning achieves similar performance as supervised
representation learning, under the same data and computation budget. With a larger web dataset,
MoPro outperforms ImageNet supervised learning by a large margin.
•	MoPro learns a more robust and calibrated model that generalizes better to distribution variations.
2	Related work
2.1	Webly-supervised representation learning
A number of prior works exploit large web datasets for visual representation learning (Divvala et al.,
2014; Chen & Gupta, 2015; Joulin et al., 2016; Mahajan et al., 2018; Sun et al., 2017; Li et al., 2017;
Kolesnikov et al., 2020). These datasets contain a considerable amount of noise. Approximately
20% of the labels in the JMT-300M dataset (Sun et al., 2017) are noisy, whereas 34% of images in
the WebVision dataset (Li et al., 2017) are considered outliers. Surprisingly, most prior works have
chosen to ignore the noise and applied vanilla supervised method, with the claim that the scale of
data can overpower the noise (Mahajan et al., 2018; Sun et al., 2017; Li et al., 2017). However, we
show that supervised method cannot fully harvest the power of large-scale weakly-labeled datasets.
2
Published as a conference paper at ICLR 2021
Our method achieves substantial improvement by addressing noise, and advances the potential of
webly-supervised representation learning.
2.2	Learning with label noise
Learning with label noise has been widely studied. Some methods require access to a small set
of clean samples (Xiao et al., 2015; Vahdat, 2017; Veit et al., 2017; Lee et al., 2018; Zhang et al.,
2020), and other methods assume that no clean labels are available. There exist two major types of
approaches. The first type performs label correction using predictions from the network (Reed et al.,
2015; Ma et al., 2018; Tanaka et al., 2018; Yi & Wu, 2019; Yang et al., 2020). The second type
separates clean samples from corrupted samples, and trains the model on clean samples (Han et al.,
2018; Arazo et al., 2019; Jiang et al., 2018; Wang et al., 2018; Chen et al., 2019; Li et al., 2020a).
However, existing methods have yet shown promising results for large-scale weakly-supervised rep-
resentation learning. The main reasons include: (1) most methods do not consider OOD samples,
which commonly occur in real-world web datasets; (2) most methods are computational-heavy due
to co-training (Han et al., 2018; Li et al., 2020a; Jiang et al., 2018; 2020), iterative training (Tanaka
et al., 2018; Yi & Wu, 2019; Wang et al., 2018; Chen et al., 2019), or meta-learning (Li et al., 2019;
Zhang et al., 2019).
Different from existing methods, MoPro achieves both label correction and OOD sample removal
on-the-fly with a single step, based on the similarity between an image embedding and the momen-
tum prototypes. MoPro also leverages contrastive learning to learn a robust embedding space.
2.3	Self-supervised representation learning
Self-supervised methods have been proposed for representation learning using unlabeled data. The
recent developments in self-supervised representation learning can be attributed to contrastive learn-
ing. Most methods (He et al., 2019; Chen et al., 2020a; Oord et al., 2018; Wu et al., 2018) leverage
the task of instance discrimination, where augmented crops from the same source image are enforced
to have similar embeddings. Prototypical contrastive learning (PCL) (Li et al., 2020b) performs
clustering to find prototypical embeddings, and enforces an image embedding to be similar to its
assigned prototypes. Different from PCL, we update prototypes on-the-fly in a weakly-supervised
setting, where the momentum prototype of a class is the moving average of clean samples’ embed-
dings. Furthermore, we jointly optimize two contrastive losses and a cross-entropy loss.
Current self-supervised representation learning methods are limited in (1) inferior performance in
low-shot task adaptation, (2) huge computation cost, and (3) inadequate to harvest larger datasets.
We show that weakly-supervised learning with MoPro addresses these limitations.
3	Method
In this section, we delineate the details of our method. First, we introduce the components in our
representation learning framework. Then, we describe the loss functions. Finally, we explain the
noise correction procedure for label correction and OOD sample removal. A pseudo-code of MoPro
is provided in appendix B.
3.1	Representation Learning Framework
Our proposed framework consists of the following components. Figure 2 gives an illustration.
•	A noisy training dataset {(xi, yi)}in=1, where xi is an image and yi ∈ {1, ..., K} is its class label.
•	A pseudo-label y% for each image Xi, which is its corrected label. Details for generating the
pseudo-label is explained in Sec 3.3.
•	An encoder network, which maps an augmented image Xi to a representation vector Vi ∈ Rde.
We experiment with ResNet-50 (He et al., 2016) as the encoder, where the activations of the final
global pooling layer (de = 2048) are used as the representation vector.
•	A classifier (a fully-connected layer followed by softmax) which receives the representation vi as
input and outputs class predictions pi .
3
Published as a conference paper at ICLR 2021
. loss function
I	I positives
I	I negatives
CNN
Softmax	cross-entropy
■ ■	“Chihuahua”
Figure 2: Proposed weakly-supervised learning framework. We jointly optimize a prototypical contrastive loss
using momentum prototypes, an instance contrastive loss using momentum embeddings, and a cross-entropy
loss using pseudo-labels. The pseudo-label for a sample is generated based on its original training label, the
model’s prediction, and the sample’s distance to the prototypes.
•	A projection network, which maps the representation vi into a low-dimensional embedding zi ∈
Rdp (dp = 128). zi is always normalized to the unit sphere. Following SimCLR (Chen et al.,
2020a), we use a MLP with one hidden layer as the projection network.
•	Momentum embeddings zi0 generated by a momentum encoder. The momentum encoder has the
same architecture as the encoder followed by the projection network, and its parameters are the
moving-average of the encoder’s and the projection network’s parameters. Same as in MoCo (He
et al., 2019), we maintain a queue of momentum embeddings of past samples.
•	Momentum prototypes C ∈ Rdp ×K. The momentum prototype of the k-th class, ck, is the
normalized moving-average embedding for samples With pseudo-label yi = k.
3.2	Contrastive Loss
As illustrated in Figure 1, We aim to learn an embedding space Where samples from the same class
gather around its class prototype, While samples from different classes are seperated. We achieve it
With tWo contrastive losses: (1) a prototypical contrastive loss Lpro Which increases the similarity
between an embedding and its corresponding class prototype, (zi, Cyi), in contrast to other Proto-
types; (2) an instance contrastive loss Lins Which increases the similarity betWeen tWo embeddings
of the same source image, (zi, zi0), in contrast to embeddings of other images. Specifically, the
contrastive losses are defined as:
Li	= loe	exP(Zi∙ Cyi /T)	Li	= loe	exP(Zi	∙	z0/T)	(1)
LPro =	lθg SK	/	, , ,	Lins = lθg「R	0	0 / ∖ ,	(1)
∑k=1 exP(zi ∙ Ck/τ)	r=o=0 exP(zi ∙ zr/τ)
where T is a temperature parameter, and yi is the pseudo-label. We use R negative momentum
embeddings to construct the denominator of the instance contrastive loss.
We train the classifier with cross-entropy loss, using pseudo-labels as targets.
Lce = - lθg(pyi )	⑵
We jointly optimize the contrastive losses and the classification loss. The training objective is:
n
L = X(Lice +λProLiPro +λinsLiins)	(3)
i=1
For simplicity, we set λPro = λins = 1 for all experiments.
3.3	Noise Correction
We propose a simple yet effective method for online noise correction during training, which cleans
label noise and removes OOD samples. For each sample, we generate a soft pseudo-label qi by
4
Published as a conference paper at ICLR 2021
combining the classifier’s output probability pi with si, a class probability distribution calculated
using the sample’s similarity w.r.t the momentum prototypes:
qi = αpi + (1 - α)si,
Sk =	eχp(Zi ∙ ck/)	(4)
i	Pk=I eχp(zi∙ck∕τ)
where the combination weight is simply set as α = 0.5 in all experiments.
We convert qi into a hard pseudo-label y% based on the following rules: (1)if the highest score of qi
is above certain threshold T , use the class with the highest score as the pseudo-label; (2) otherwise,
if the score for the original label yi is higher than uniform probability, use yi as the pseudo-label;
(3) otherwise, label it as an OOD sample.
(argmaxk qi if maxk qk > T,
yi	elseif qiyi > 1∕K,
OOD	otherwise.
(5)
We remove OOD samples from both the cross-entropy loss and the prototypical contrastive loss
so that they do not affect class-specific learning, but include them in the instance contrastive loss
to further separate them from in-distribution samples. Examples of OOD images and corrected
pseudo-labels are shown in the appendices.
3.4	Momentum Prototypes
For each class k, we calculate its momentum prototype as a moving-average of the normalized
embeddings for samples with pseudo-label k. Specifically, we update ck by:
Ck J Normalize(mci + (1 — m)zi), ∀i ∈ {i | yi = k},	(6)
where Normalize(c) = c∕ kck2. The momentum coefficient m is set 0.999 in our experiments.
4	Experiments
4.1	Dataset for upstream training
We use the WebVision (Li et al., 2017) dataset as the noisy training data. It consists of images
automatically crawled from Google and Flickr, using visual concepts from ImageNet as queries.
We experiment with three versions of WebVision with different sizes: (1) WebVision-V1.0 contains
2.44m images with the same classes as the ImageNet-1k (ILSVRC 2012) dataset; (2) WebVision-
V0.5 is a randomly sampled subset of WebVision-V1.0, which contains the same number of images
(1.28m) as ImageNet-1k; (3) WebVision-V2.0 contains 16m images with 5k classes.
4.2	Implementation details
We follow standard settings for ImageNet training: batch size is 256; total number of epochs is
90; optimizer is SGD with a momentum of 0.9; initial learning rate is 0.1, decayed at 40 and 80
epochs; weight decay is 0.0001. We use ResNet-50 (He et al., 2016) as the encoder. For MoPro-
specific hyperparameters, we set τ = 0.1, α = 0.5, T = 0.8 (T = 0.6 for WebVision-V2.0). The
momentum for both the momentum encoder and momentum prototypes is set as 0.999. The queue
to store momentum embeddings has a size of 8192. We apply standard data augmentation (crop and
horizontal flip) to the encoder’s input, and stronger data augmentation (color changes in MoCo (He
et al., 2019)) to the momentum encoder’s input. We warm-up the model for 10 epochs by training
on all samples with original labels, before applying noise correction.
4.3	Upstream task performance
In Table 1, we compare MoPro with existing weakly-supervised learning methods trained on
WebVision-V1.0, where MoPro achieves state-of-the-art performance. Since the training dataset
5
Published as a conference paper at ICLR 2021
WebViSion ImageNet
Method	Architecture
			top-1	top-5 I top-1		top-5
Cross-Entropy (Tu et al., 2020)	ResNet-50	66.4	83.4	57.7	78.4
MentorNet (Jiang et al., 2018)	InceptionResNet-V2	70.8	88.0	62.5	83.0
CurriculumNet (Guo et al., 2018)	Inception-V2	72.1	89.1	64.8	84.9
CleanNet (Lee et al., 2018)	ResNet-50	70.3	87.8	63.4	84.6
CurriculumNet (Guo et al., 2018; Tu et al., 2020)	ResNet-50	70.7	88.6	62.7	83.4
SOM (Tu et al., 2020)	ResNet-50	72.2	89.5	65.0	85.1
Distill (Zhang et al., 2020)	ResNet-50	-	-	65.8	85.8
Cross-Entropy (decoupled)	ResNet-50	72.4	89.0	65.7	85.1
MoPro (ours)	ResNet-50	73.9	90.0	67.8	87.0
Table 1: CompariSon with State-of-the-art methodS on WebViSion-V1.0. NumberS denote accuracy (%) on
the clean WebViSion-V1.0 validation Set and the ILSVRC 2012 validation Set. CleanNet (Lee et al., 2018) and
DiStill (Zhang et al., 2020) require data with clean annotationS.
haS imbalanced number of SampleS per-claSS, inSpired by Kang et al. (2020), we perform the fol-
lowing decoupled training StepS to re-balance the claSSifier: (1) pretrain the model with MoPro; (2)
perform noiSe correction on the training data uSing the pretrained model, following the method in
Section 3.3; (3) keep the pretrained encoder fixed and finetune the claSSifier on the cleaned dataSet,
uSing Square-root data Sampling (Mahajan et al., 2018) which balanceS the claSSeS. We retrain the
claSSifier for 15 epochS, uSing a learning rate of 0.01 which iS decayed at 5 and 10 epochS. SurpriS-
ingly, we alSo find that a vanilla croSS-entropy method with decoupled claSSifier re-balancing can
alSo achieve competitive performance, outperforming moSt exiSting baSelineS.
5	Transfer learning
In thiS Section, we tranSfer weakly-SuperviSed learned modelS to a variety of downStream taSkS.
We Show that MoPro yieldS Superior performance in image claSSification, object detection, inStance
Segmentation, and obtainS better robuStneSS to domain ShiftS. Implementation detailS for the tranSfer
learning experimentS are deScribed in appendix C.
5.1	Low-shot image classification on fixed representation
FirSt, we tranSfer the learned repreSentation to downStream taSkS with few training SampleS. We
perform low-Shot claSSification on two dataSetS: PASCAL VOC2007 (Everingham et al., 2010)
for object claSSification and PlaceS205 (Zhou et al., 2014) for Scene recognition. Following the
Setup by Goyal et al. (2019); Li et al. (2020b), we train linear SVMS uSing fixed repreSentationS
from pretrained modelS. We vary the number k of SampleS per-claSS and report the average reSult
Method	Pretrain dataset	VOC07 k=1 k=2 k=4 k=8 k=16	Places205 k=1 k=2 k=4 k=8 k=16
MoCo v2*	ImageNet PCL v2*	ImageNet	46.3 58.4 64.9 72.5 76.1 47.9 59.6 66.2 74.5 78.3	11.9 17.0 22.6 28.1	32.4 12.5 17.5 23.2 28.1	32.3
CE (Sup.)	ImageNet	54.3 67.8 73.9 79.6	82.3	14.9 21.0 26.9 32.1	36.0
CE CE	WebVision-V0.5 MoPro (ours)	49.8 63.9 69.9 76.1	79.2	13.5 19.3 24.7 29.5	33.8
	54.3 67.8 73.5 79.2	81.8	15.0 21.2 26.6 31.8	36.0
CE CE	WebVision-V1.0 MoPro (ours)	54.5 67.1 72.8 78.4	81.4	15.1 21.5 27.2 32.1	36.4
	59.5 71.3 76.5 81.4	83.7	16.9 23.2 29.2 34.5	38.7
CE CE	WebVision-V2.0 MoPro (ours)	63.0 73.8 78.7 83.0	85.4	21.8 28.6 35.1 40.0 43.6
	64.8 74.8 79.9 83.9 86.1	22.2 29.2 35.6 40.9 44.4
Table 2: Low-shot image classification on VOC07 and PlaceS205 uSing linear SVMS trained on fixed repre-
SentationS. We vary the number of labeled exampleS per-claSS (k), and report the average mAP (for VOC) and
accuracy (for PlaceS) acroSS 5 independent runS. WebViSion-V0.5 haS the Same number of training SampleS aS
ImageNet. The self-supervised learning methods* are trained for 200 epochs, while other methods are trained
for 90 epochS. MoPro outperformS vanilla CE pretrained on Web dataSetS, aS well aS Self-SuperviSed learning
and supervised learning methods pretrained on ImageNet.
6
Published as a conference paper at ICLR 2021
across 5 independent runs. Table 2 shows the results. When pretrained on weakly-labeled datasets,
MoPro consistently outperforms the vanilla CE method. The improvement of MoPro becomes less
significant when the number of web images increases from 2.4m to 16m, suggesting that increasing
dataset size is a viable solution to combat noise.
When compared with ImageNet pretrained models, MoPro substantially outperforms self-supervised
learning (MoCo v2 (Chen et al., 2020b) and PCL v2 (Li et al., 2020b)), and achieves comparable
performance with supervised learning when the same amount of web images (i.e. WebVision-V0.5)
is used. Our results for the first time show that weakly-supervised representation learning can be as
powerful as supervised representation learning under the same data and computation budget.
5.2	Low-resource transfer with finetuning
Next, we perform experiment to evaluate whether the pretrained model provides a good basis for
finetuning when the downstream task has limited training data. Following the setup by Chen et al.
(2020a), we finetune the pretrained model on 1% or 10% of ImageNet training samples. Table 3
shows the results. MoPro consistently outperforms CE when pretrained on Web datasets. Compared
to self-supervised learning methods pretrained on ImageNet, weakly-supervised learning achieves
significantly better performance with fewer number of epochs.
Surprisingly, pretraining on the larger WebVision-V2 leads to worse performance compared to V0.5
and V1.0. This is because WebVision-V0.5 and V1.0 contain the same 1k class as ImageNet,
whereas V2 also contains 4k extra classes. Hence, the representations learned from V2 are less
task-specific and more difficult to adapt to ImageNet, especially with only 1% of samples for fine-
tuning. This suggests that if the classes for a downstream task are known a priori, it is more effective
to curate a task-specific weakly-labeled dataset with the same classes.
	Pretrain Method	Pretrain dataset	#Pretrain epochs	Top-1		Top-5	
				1%	10%	1%	10%
Random init.	None	None	None	25.4	56.4	48.4	80.4
	PCL		200	48.8	62.9	75.3	85.6
Self-supervised	SimCLR	ImageNet	1000	48.3	65.6	75.5	87.8
	BYOL		1000	53.2	68.8	78.4	89.0
	SwAV		800	53.9	70.2	78.5	89.9
	CE	WebViSion-V0.5	90	65.9	72.4	87.0	90.9
	MoPro (ours)			69.3	73.3	89.1	91.7
Weakly-supervised	^CE MoPro (ours)	WebViSion-V1.0	90	ɪ6- 71.2	73.5 74.8	88.3 90.5	91.7 92.4
	^CE	WebViSion-V2.0	90	62.1	72.9	86.9	91.4
	MoPro (ours)			65.3	73.7	88.2	92.1
Table 3: Low-resource finetuning on ImageNet. A pretrained model is finetuned with 1% or 10% of Ima-
geNet training data. Weakly-supervised learning with MoPro substantially outperforms self-supervised learning
methods: PCL (Li et al., 2020b), SimCLR (Chen et al., 2020a), BYOL (Grill et al., 2020), and SwAV (Caron
et al., 2020). Result for random init. is from Zhai et al. (2019).
5.3	Object detection and instance segmentation
We further transfer the pretrained model to object detection and instance segmentation tasks on
COCO (Lin et al., 2014). Following the setup by He et al. (2019), we use the pretrained ResNet-50
as the backbone for a Mask-RCNN (He et al., 2017) with FPN (Lin et al., 2017). We finetune all
layers end-to-end, including BN. The schedule is the default 1× or 2× in Girshick et al. (2018)
Table 4 shows the results. Weakly-supervised learning with MoPro outperforms both supervised
learning on ImageNet and self-supervised learning on one billion Instagram images.
5.4	Robustness
It has been shown that deep models trained on ImageNet lack robustness to out-of-distribution sam-
ples, often falsely producing over-confident predictions. Hendricks et al. have curated two bench-
mark datasets to evaluate models’ robustness to real-world distribution variation: (1) ImageNet-
R (Hendrycks et al., 2020) which contains various artistic renditions of object classes from the
7
Published as a conference paper at ICLR 2021
Method	Pretrain dataset	APbb	AP5b)	APb7b5	APmk	AP5m0k	AP7m5k
random	None	31.0	49.5	33.2	28.5	46.8	30.4
CE (Sup.)	ImageNet	38.9	59.6	42.7	35.4	56.5	38.1
MoCo	Instagram-IB	38.9	"394	42.3	-35.4	56.5	37.9
CE	WebVision-V1.0	392	^600	-42.9	-35.6	"368	ɪð
MoPro		39.7 (+0.8)	60.9 (+1.3)	43.1 (+0.4)	36.1 (+0.7)	57.5 (+1.0)	38.6 (+0.5)
MoPro	WebVision-V2.0	40.7 (+1.8)	61.7 (+2.1)	44.5 (+1.8)	36.8 (+1.4)	58.4 (+1.9)	39.6 (+1.5)
(a) 1× schedule
Method	Pretrain dataset	APbb	AP5b0	APb7b5	APmk	AP5m0k	AP7m5k
random	None	36.7	56.7	40.0	33.7	53.8	35.9
CE (Sup.)	ImageNet	40.6	61.3	44.4	36.8	58.1	39.5
MoCo	Instagram-IB	41.1	^6T8	45.1	374	59.1	40.2
CE	WebVision-V1.0	ɪ^	""61.6	^44^	372	"387	IOn
MoPro		41.2 (+0.6)	62.2 (+0.9)	45.0 (+0.6)	37.4 (+0.6)	58.9 (+0.8)	40.3 (+0.8)
MoPro	WebVision-V2.0	41.8 (+1.2)	62.6 (+1.3)	45.6 (+1.2)	37.8 (+1.0)	59.5 (+1.4)	40.6 (+1.1)
(a) 2× schedule
Table 4: Object detection and instance segmentation using Mask-RCNN with R50-FPN fine-tuned on
COCO train2017. We evaluate bounding-box AP (APbb) and mask AP (APmk) on val2017. Weakly-
supervised learning with MoPro outperforms both supervised learning on ImageNet and self-supervised learn-
ing (MoCo (He et al., 2019)) on one billion Instagram images.
Method	Pretrain dataset	ImageNet-R		ImageNet-A	
		Accuracy (↑)	Calib. Error (^)	Accuracy (↑)	Calib. Errora)
CE (Sup.)		 ImageNet	36.14	19.66	0.03	62.50
CE	WebVision-V1.0	4956	10:05	1024	37:84
MoPro		54.87	5.73	11.93	35.85
Table 5: Evaluation of model robustness on images with artistic and natural distribution shifts. Weakly
supervised learning with MoPro leads to a more robust and well-calibrated model.
original ImageNet dataset, and (2) ImageNet-A (Hendrycks et al., 2019) which contains natural im-
ages where ImageNet-pretrained models consistently fail due to variations in background elements,
color, or texture. Both datasets contain 200 classes, a subset of ImageNet’s 1,000 classes.
We evaluate weakly-supervised trained models on these two robustness benchmarks. We report
both accuracy and the `2 calibration error (Kumar et al., 2019). The calibration error measures the
misalignment between a model’s confidence and its accuracy. Concretely, a well-calibrated classifier
which give examples 80% confidence should be correct 80% of the time. Results are shown in
Table 5. Webly-supervised learning show significantly higher accuracy and lower calibration error.
The robustness to distribution shift could come from the higher diversity of samples in Web images.
Compared to vanilla CE, MoPro further improves the model’s robustness on both datasets. Note that
we made sure that the training data of WebVision does not overlap with the test data.
6	Ablation S tudy
We perform ablation study to verify the effectiveness of three important components in MoPro:
(1) prototypical contrastive loss Lpro , (2) instance contrastive loss Lins , (3) prototypical similarity
si used for noise correction (equation 4). We choose low-resource finetuning on 1% of ImageNet
training data as the benchmark, and report the top-1 accuracy for models pretrained on WebVision-
V0.5. As shown in Table 6, all of the three components contribute to the efficacy of MoPro.
7	Conclusion
This paper introduces a new contrastive learning framework for webly-supervised representation
learning. We propose momentum prototypes, a simple component that is effective in label noise
8
Published as a conference paper at ICLR 2021
I MoPro I Wlo Lpro ∣ WIo Linstl Wlo Si (i.e. α = 1) ∣ WIo LPro & Linst &si ∣ CE
ImageNetacc. ∣ 69.3 ∣	68.0	∣	68.2	∣	68.4	∣	66.9	∣ 65.9
Table 6: Ablation study Where different components are removed from MoPro. Models are pre-trained on
WebVision-V0.5 and finetuned on 1% of ImageNet data.
correction, OOD sample removal, and representation learning. MoPro achieves state-of-the-art per-
formance on the upstream task of learning from real-World noisy data, and superior representation
learning performance on multiple doWn-stream tasks. Webly-supervised learning With MoPro does
not require the expensive annotation cost in supervised learning, nor the huge computation budget
in self-supervised learning. For future Work, MoPro could be extended to utilize other sources of
free Web data, such as Weakly-labeled videos, for representation learning in other domains.
References
Eric Arazo, Diego Ortego, Paul Albert, Noel E. O’Connor, and Kevin McGuinness. Unsupervised
label noise modeling and loss correction. In ICML, pp. 312-321, 2019.
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr BojanoWski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint
arXiv:2006.09882, 2020.
Pengfei Chen, Benben Liao, Guangyong Chen, and Shengyu Zhang. Understanding and utilizing
deep neural netWorks trained With noisy labels. In ICML, pp. 1062-1070, 2019.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple frameWork for
contrastive learning of visual representations. In ICML, 2020a.
Xinlei Chen and Abhinav Gupta. Webly supervised learning of convolutional netWorks. In ICCV,
pp. 1431-1439, 2015.
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines With momentum
contrastive learning. arXiv preprint arXiv:2003.04297, 2020b.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: A large-scale
hierarchical image database. In CVPR, pp. 248-255, 2009.
Santosh Kumar Divvala, Ali Farhadi, and Carlos Guestrin. Learning everything about anything:
Webly-supervised visual concept learning. In CVPR, pp. 3270-3277, 2014.
Mark Everingham, Luc Van Gool, Christopher K. I. Williams, John M. Winn, and AndreW Zis-
serman. The pascal visual object classes (VOC) challenge. International Journal of Computer
Vision, 88(2):303-338, 2010.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. LIBLINEAR:
A library for large linear classification. JMLR, 9:1871-1874, 2008.
Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Dollar, and Kaiming He. Detectron.
https://github.com/facebookresearch/detectron, 2018.
Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. Scaling and benchmarking self-
supervised visual representation learning. In ICCV, pp. 6391-6400, 2019.
Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre H. Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Ghesh-
laghi Azar, Bilal Piot, Koray Kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap your own
latent: A neW approach to self-supervised learning. arXiv preprint arXiv:2006.07733, 2020.
Sheng Guo, Weilin Huang, Haozhi Zhang, Chenfan Zhuang, Dengke Dong, MattheW R. Scott, and
Dinglong Huang. Curriculumnet: Weakly supervised learning from large-scale Web images. In
ECCV, pp. 139-154, 2018.
9
Published as a conference paper at ICLR 2021
Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor W. Tsang, and Masashi
Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In
NeurIPS, pp. 8536-8546, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, pp. 770-778, 2016.
Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross B. Girshick. Mask R-CNN. In ICCV, pp.
2980-2988, 2017.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. arXiv preprint arXiv:1911.05722, 2019.
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial
examples. arXiv preprint arXiv:1907.07174, 2019.
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul
Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical
analysis of out-of-distribution generalization. arXiv preprint arXiv:2006.16241, 2020.
Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning data-
driven curriculum for very deep neural networks on corrupted labels. In ICML, pp. 2309-2318,
2018.
Lu Jiang, Di Huang, Mason Liu, and Weilong Yang. Beyond synthetic noise: Deep learning on
controlled noisy labels. In ICML, 2020.
Armand Joulin, Laurens van der Maaten, Allan Jabri, and Nicolas Vasilache. Learning visual fea-
tures from large weakly supervised data. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max
Welling (eds.), ECCV, 2016.
Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis
Kalantidis. Decoupling representation and classifier for long-tailed recognition. In ICLR, 2020.
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,
and Neil Houlsby. Large scale learning of general visual representations for transfer. In ECCV,
2020.
Ananya Kumar, Percy Liang, and Tengyu Ma. Verified uncertainty calibration. In Hanna M. Wal-
lach, Hugo Larochelle, Alina Beygelzimer, Florence d,Alche-Buc, Emily B. Fox, and Roman
Garnett (eds.), NeurIPS, pp. 3787-3798, 2019.
Kuang-Huei Lee, Xiaodong He, Lei Zhang, and Linjun Yang. Cleannet: Transfer learning for
scalable image classifier training with label noise. In CVPR, pp. 5447-5456, 2018.
Junnan Li, Yongkang Wong, Qi Zhao, and Mohan S. Kankanhalli. Learning to learn from noisy
labeled data. In CVPR, pp. 5051-5059, 2019.
Junnan Li, Richard Socher, and Steven C.H. Hoi. Dividemix: Learning with noisy labels as semi-
supervised learning. In ICLR, 2020a.
Junnan Li, Pan Zhou, Caiming Xiong, Richard Socher, and Steven C.H. Hoi. Prototypical contrastive
learning of unsupervised representations. arXiv preprint arXiv:2005.04966, 2020b.
Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, and Luc Van Gool. Webvision database: Visual
learning and understanding from web data. arXiv preprint arXiv:1708.02862, 2017.
Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dollar, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In ECCV,, pp.
740-755, 2014.
Tsung-Yi Lin, Piotr Dollar, Ross B. Girshick, Kaiming He, Bharath Hariharan, and Serge J. Be-
longie. Feature pyramid networks for object detection. In CVPR, pp. 936-944, 2017.
10
Published as a conference paper at ICLR 2021
Xingjun Ma, Yisen Wang, Michael E. Houle, Shuo Zhou, Sarah M. Erfani, Shu-Tao Xia, Sudanthi
N. R. Wijewickrema, and James Bailey. Dimensionality-driven learning with noisy labels. In
ICML,pp. 3361-3370, 2018.
Dhruv Mahajan, Ross B. Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan
Li, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised
pretraining. In ECCV, pp. 185-201, 2018.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018.
Scott E. Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew
Rabinovich. Training deep neural networks on noisy labels with bootstrapping. In ICLR, 2015.
Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable ef-
fectiveness of data in deep learning era. In ICCV, pp. 843-852, 2017.
Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa. Joint optimization frame-
work for learning with noisy labels. In CVPR, pp. 5552-5560, 2018.
Yi Tu, Li Niu, Dawei Cheng, and Liqing Zhang. Protonet: Learning from web data with memory.
In CVPR, 2020.
Arash Vahdat. Toward robustness against label noise in training deep discriminative neural networks.
In NIPS, pp. 5601-5610, 2017.
Andreas Veit, Neil Alldrin, Gal Chechik, Ivan Krasin, Abhinav Gupta, and Serge J. Belongie. Learn-
ing from noisy large-scale datasets with minimal supervision. In CVPR, pp. 6575-6583, 2017.
Yisen Wang, Weiyang Liu, Xingjun Ma, James Bailey, Hongyuan Zha, Le Song, and Shu-Tao Xia.
Iterative learning with open-set noisy labels. In CVPR, pp. 8688-8696, 2018.
Zhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua Lin. Unsupervised feature learning via non-
parametric instance discrimination. In CVPR, pp. 3733-3742, 2018.
Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive noisy
labeled data for image classification. In CVPR, pp. 2691-2699, 2015.
Jingkang Yang, Litong Feng, Weirong Chen, Xiaopeng Yan, Huabin Zheng, Ping Luo, and Wayne
Zhang. Webly supervised image classification with self-contained confidence. In ECCV, 2020.
Kun Yi and Jianxin Wu. Probabilistic end-to-end noise correction for learning with noisy labels. In
CVPR, 2019.
Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S4l: Self-supervised semi-
supervised learning. In ICCV, pp. 1476-1485, 2019.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In ICLR, 2017.
Weihe Zhang, Yali Wang, and Yu Qiao. Metacleaner: Learning to hallucinate clean representations
for noisy-labeled visual recognition. In CVPR, 2019.
Zizhao Zhang, Han Zhang, Sercan Omer Arik, Honglak Lee, and Tomas Pfister. Distilling effective
supervision from severe label noise. In CVPR, pp. 9291-9300, 2020.
Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep
features for scene recognition using places database. In NIPS, pp. 487-495, 2014.
11
Published as a conference paper at ICLR 2021
Appendix A	Noisy sample visualization
In Figure 3, we show example images randomly chosen from the out-of-distribution samples filtered
out by our method. In Figure 4, we show random examples where their pseudo-labels are different
from the original training labels. By visual examination, we observe that our method can remove
OOD samples and correct noisy labels at a high success rate.
airliner
wine bottle
orange
basketball
magpie
goldfish, carassius auratus
Figure 3: Examples of randomly selected out-of-distribution samples filtered out by our method. The original
training labels are shown below the images.
12
Published as a conference paper at ICLR 2021
dining table
Figure 4: Examples of randomly selected samples with noisy labels corrected by our method. The original
training labels are shown in red and corrected pseudo-labels are shown in green.
e×presso maker
miniskirt
airliner
gray whale
backpack
hartebeest
13
Published as a conference paper at ICLR 2021
Appendix B	Pseudo-code of MoPro
Algorithm 1 summarizes the proposed method.
Algorithm 1: MoPro’s main algorithm.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
Input: number of classes K, temperature T, threshold T, momentum m, encoder network f (∙),
projection network g(∙), classifier h(∙), momentum encoder g0(f 0(∙)).
for {(xi, yi)}ib=1 in loader do	// load a minibatch of noisy training data
for i ∈ {1, ..., b} do
Xi = weak_aUg(Xi)	// weak augmentation
Xi =Strong_aug(Xi)	// strong augmentation
Vi = f(Xi)	// representation
zi = g(vi)	// normalized low-dimensional embedding
Zi = g0(f0(Xi))
pi = h(vi)
c. _ ∕eklK	ek _	eχp(Zi-ck/T)
i	i ijk = 1, i	PK=I exp(zi∙ck/τ)
// noise correction
qi = (pi + si )/2
if maxk qik > T then
y ^ = arg max® qk
else if qiyi > 1/K then
y yi = yi
else
I 备= OOD
end
// calculate losses
Li	= _ “	eχp(Zi∙z0 IT)
Lins= log pR=o eχp(Zi∙zr/τ)
if yi is not OOD then
Li = _ loo. eχp(Zi∙/i∕T)
pro =	g PK=I exp(Zi∙Ck∕T)
Lce = -Iog(Pyi )
else
I LPro = Lce = 0
end
// update momentum prototypes
Cyi J Normalize(mcyi + (1 — m)Zi)
end
L = Pib=1(Lice + Lipro + Liins)
update networks f, g, h to minimize L.
// momentum embedding
// class prediction
// prototypical score
// soft pseudo-label
// instance contrastive loss
// prototypical contrastive loss
// cross entropy loss
// total loss
end
Appendix C	Transfer learning implementation details
For low-shot image classification on Places and VOC, we follow the procedure in Li et al. (2020b)
and train linear SVMs on the global average pooling features of ResNet-50. We preprocess all
images by resizing to 256 pixels along the shorter side and taking a 224 × 224 center crop. The
SVMs are implemented in the LIBLINEAR (Fan et al., 2008) package.
For low-resource finetuning on ImageNet, we adopt different finetuning strategy for different ver-
sions of WebVision pretrained models. For WebVision V0.5 and V1.0, since they contain the same
1000 classes as ImageNet, we finetune the entire model including the classification layer. We train
with SGD, using a batch size of 256, a momentum of 0.9, a weight decay of0, and a learning rate of
0.005. We train for 40 epochs, and drop the learning rate by 0.2 at 15 and 30 epochs. For WebVision
2.0, since it contains 5000 classes, we randomly initialize anew classification layer with 1000 output
14
Published as a conference paper at ICLR 2021
dimension, and finetune the model end-to-end. We train for 50 epochs, using a learning rate of 0.01,
which is dropped by 0.1 at 20 and 40 epochs.
For object detection and instance segmentation on COCO, we adopt the same setup in MoCo (He
et al., 2019), using Detectron2 (Girshick et al., 2018) codebase. The image scale is in [640, 800]
pixels during training and is 800 at inference. We fine-tune all layers end-to-end. We finetune on the
train2017 set (〜118k images) and evaluate on val2017.
Appendix D	S tandard deviation for low-shot classification
Table 7 reports the standard deviation for the low-shot image classification experiment in Section 5.1.
Method	Pretrain dataset	VOC07		Places205		
		k=1	k=2	k=4	k=8	k=1	k=2	k=4	k=8
CE (Sup.) ImageNet		54.3±4.8 67.8±4.4 73.9±0.9	79.6±0.8	14.9±1.3	21.0±0.3	26.9±0.6 32.1±0.4
MoPro	WebVision-V1.0	59.5±5.2 71.3±2.2 76.5±1.1	81.4±0.6	16.9±1.3	23.2±0.3	29.2±0.6 34.5±0.3
MoPro	WebVision-V2.0	64.8±6.7 74.8±2.6 79.9±1.4	83.9±1.0	22.2±1.3	29.2±0.5	35.6±0.7 40.9±0.3
Table 7: Low-shot image classification experiments. Mean and standard deviation are calculated across 5 runs.
15