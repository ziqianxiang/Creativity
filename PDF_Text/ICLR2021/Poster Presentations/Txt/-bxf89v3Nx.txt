Calibration tests beyond classification
David Widmann
Department of Information Technology
Uppsala University, Sweden
david.widmann@it.uu.se
Fredrik Lindsten
Division of Statistics and Machine Learning
Linkoping University, Sweden
fredrik.lindsten@liu.se
Dave Zachariah
Department of Information Technology
Uppsala University, Sweden
dave.zachariah@it.uu.se
Ab stract
Most supervised machine learning tasks are subject to irreducible prediction errors.
Probabilistic predictive models address this limitation by providing probability
distributions that represent a belief over plausible targets, rather than point esti-
mates. Such models can be a valuable tool in decision-making under uncertainty,
provided that the model output is meaningful and interpretable. Calibrated models
guarantee that the probabilistic predictions are neither over- nor under-confident.
In the machine learning literature, different measures and statistical tests have been
proposed and studied for evaluating the calibration of classification models. For
regression problems, however, research has been focused on a weaker condition of
calibration based on predicted quantiles for real-valued targets. In this paper, we
propose the first framework that unifies calibration evaluation and tests for general
probabilistic predictive models. It applies to any such model, including classifica-
tion and regression models of arbitrary dimension. Furthermore, the framework
generalizes existing measures and provides a more intuitive reformulation of a
recently proposed framework for calibration in multi-class classification. In par-
ticular, we reformulate and generalize the kernel calibration error, its estimators,
and hypothesis tests using scalar-valued kernels, and evaluate the calibration of
real-valued regression problems.1
1	Introduction
We consider the general problem of modelling the relationship between a feature X and a target Y in a
probabilistic setting, i.e., we focus on models that approximate the conditional probability distribution
P(Y |X) of target Y for given feature X . The use of probabilistic models that output a probability
distribution instead of a point estimate demands guarantees on the predictions beyond accuracy,
enabling meaningful and interpretable predicted uncertainties. One such statistical guarantee is
calibration, which has been studied extensively in metereological and statistical literature (DeGroot
& Fienberg, 1983; Murphy & Winkler, 1977).
A calibrated model ensures that almost every prediction matches the conditional distribution of targets
given this prediction. Loosely speaking, in a classification setting a predicted distribution of the
model is called calibrated (or reliable), if the empirically observed frequencies of the different classes
match the predictions in the long run, if the same class probabilities would be predicted repeatedly. A
classical example is a weather forecaster who predicts each day if it is going to rain on the next day.
If she predicts rain with probability 60% for a long series of days, her forecasting model is calibrated
for predictions of 60% if it actually rains on 60% of these days.
If this property holds for almost every probability distribution that the model outputs, then the model
is considered to be calibrated. Calibration is an appealing property of a probabilistic model since it
1The source code of the experiments is available at https://github.com/devmotion/
Calibration_ICLR2021.
1
provides safety guarantees on the predicted distributions even in the common case when the model
does not predict the true distributions P(Y |X). Calibration, however, does not guarantee accuracy
(or refinement)—a model that always predicts the marginal probabilities of each class is calibrated
but probably inaccurate and of limited use. On the other hand, accuracy does not imply calibration
either since the predictions of an accurate model can be too over-confident and hence miscalibrated,
as observed, e.g., for deep neural networks (Guo et al., 2017).
In the field of machine learning, calibration has been studied mainly for classification prob-
lems (Brocker, 2009; Guo et al., 2017; KUll et al., 2017; 2019; Kumar et al., 2018; Platt, 2000;
Vaicenavicius et al., 2019; Widmann et al., 2019; Zadrozny, 2002) and for quantiles and confidence
intervals of models for regression problems with real-valued targets (Fasiolo et al., 2020; Ho & Lee,
2005; Kuleshov et al., 2018; Rueda et al., 2006; Taillardat et al., 2016). In our work, however, we do
not restrict ourselves to these problem settings but instead consider calibration for arbitrary predictive
models. Thus, we generalize the common notion of calibration as:
Definition 1. Consider a model PX := P(Y |X) of a conditional probability distribution P(Y |X).
Then model P is said to be calibrated if and only if
P(Y |PX) = PX	almost surely.	(1)
If P is a classification model, Definition 1 coincides with the notion of (multi-class) calibration by
Brocker (2009); Kull et al. (2019); Vaicenavicius et al. (2019). Alternatively, in classification some
authors (Guo et al., 2017; Kumar et al., 2018; Naeini et al., 2015) study the strictly weaker property
of confidence calibration (Kull et al., 2019), which only requires
P (Y = arg max PX | maxPX) = max PX	almost surely.	(2)
This notion of calibration corresponds to calibration according to Definition 1 for a reduced problem
with binary targets Y := 1(Y = arg max PX) and Bernoulli distributions PX := Ber(max PX) as
probabilistic models.
For real-valued targets, Definition 1 coincides with the so-called distribution-level calibration by Song
et al. (2019). Distribution-level calibration implies that the predicted quantiles are calibrated, i.e., the
outcomes for all real-valued predictions of the, e.g., 75% quantile are actually below the predicted
quantile with 75% probability (Song et al., 2019, Theorem 1). Conversely, although quantile-based
calibration is a common approach for real-valued regression problems (Fasiolo et al., 2020; Ho & Lee,
2005; Kuleshov et al., 2018; Rueda et al., 2006; Taillardat et al., 2016), it provides weaker guarantees
on the predictions. For instance, the linear regression model in Fig. 1 empirically shows quantiles
that appear close to being calibrated albeit being uncalibrated according to Definition 1.
a
2
1
A 0
-1
1.00
X4"U0P
O 5
5 2
0∙70∙50∙2
x4=-qeqo∙!d 0>nEno
-2	0.00	0.00-
-1.0	-0.5	0.0	0.5	1.0 -1.0	-0.5	0.0	0.5	1.0
0.00	0.25	0.50	0.75	1.00
quantile level
Figure 1: Illustration of a conditional distribution P(Y |X) with scalar feature and target. We consider
a Gaussian predictive model P, obtained by ordinary least squares regression with 100 training data
points (orange dots). Empirically the predicted quantiles on 50 validation data points appear close to
being calibrated, although model P is uncalibrated according to Definition 1. Using the framework
in this paper, on the same validation data a statistical test allows us to reject the null hypothesis that
model P is calibrated at a significance level of α = 0.05 (p < 0.05). See Appendix A.1 for details.
Figure 1 also raises the question of how to assess calibration for general target spaces in the sense of
Definition 1, without having to rely on visual inspection. In classification, measures of calibration
such as the commonly used expected calibration error (ECE) (Guo et al., 2017; Kull et al., 2019;
2
Naeini et al., 2015; Vaicenavicius et al., 2019) and the maximum calibration error (MCE) (Naeini
et al., 2015) try to capture the average and maximal discrepancy between the distributions on the left
hand side and the right hand side of Eq. (1) or Eq. (2), respectively. These measures can be generalized
to other target spaces (see Definition B.1), but unfortunately estimating these calibration errors from
observations of features and corresponding targets is problematic. Typically, the predictions are
different for (almost) all observations, and hence estimation of the conditional probability P (Y |PX),
which is needed in the estimation of ECE and MCE, is challenging even for low-dimensional target
spaces and usually leads to biased and inconsistent estimators (Vaicenavicius et al., 2019).
Kernel-based calibration errors such as the maximum mean calibration error (MMCE) (Kumar et al.,
2018) and the kernel calibration error (KCE) (Widmann et al., 2019) for confidence and multi-class
calibration, respectively, can be estimated without first estimating the conditional probability and
hence avoid this issue. They are defined as the expected value of a weighted sum of the differences of
the left and right hand side of Eq. (1) for each class, where the weights are given as a function of the
predictions (of all classes) and chosen such that the calibration error is maximized. A reformulation
with matrix-valued kernels (Widmann et al., 2019) yields unbiased and differentiable estimators
without explicit dependence on P(Y |PX), which simplifies the estimation and allows to explicitly
account for calibration in the training objective (Kumar et al., 2018). Additionally, the kernel-based
framework allows the derivation of reliable statistical hypothesis tests for calibration in multi-class
classification (Widmann et al., 2019).
However, both the construction as a weighted difference of the class-wise distributions in Eq. (1) and
the reformulation with matrix-valued kernels require finite target spaces and hence cannot be applied
to regression problems. To be able to deal with general target spaces, we present a new and more
general framework of calibration errors without these limitations.
Our framework can be used to reason about and test for calibration of any probabilistic predictive
model. As explained above, this is in stark contrast with existing methods that are restricted to simple
output distributions, such as classification and scalar-valued regression problems. A key contribution
of this paper is a new framework that is applicable to multivariate regression, as well as situations
when the output is of a different (e.g., discrete ordinal) or more complex (e.g., graph-structured) type,
with clear practical implications.
Within this framework a KCE for general target spaces is obtained. We want to highlight that for
multi-class classification problems its formulation is more intuitive and simpler to use than the
measure proposed by Widmann et al. (2019) based on matrix-valued kernels. To ease the application
of the KCE we derive several estimators of the KCE with subquadratic sample complexity and their
asymptotic properties in tests for calibrated models, which improve on existing estimators and tests
in the two-sample test literature by exploiting the special structure of the calibration framework.
Using the proposed framework, we numerically evaluate the calibration of neural network models
and ensembles of such models.
2	Calibration error: A general framework
In classification, the distributions on the left and right hand side of Eq. (1) can be interpreted
as vectors in the probability simplex. Hence ultimately the distance measure for ECE and MCE
(see Definition B.1) can be chosen as a distance measure of real-valued vectors. The total variation,
Euclidean, and squared Euclidean distances are common choices (Guo et al., 2017; Kull et al.,
2019; Vaicenavicius et al., 2019). However, in a general setting measuring the discrepancy between
P(Y |PX ) and PX cannot necessarily be reduced to measuring distances between vectors. The
conditional distribution P(Y |PX) can be arbitrarily complex, even if the predicted distributions are
restricted to a simple class of distributions that can be represented as real-valued vectors. Hence in
general we have to resort to dedicated distance measures of probability distributions.
Additionally, the estimation of conditional distributions P(Y |PX) is challenging, even more so than
in the restricted case of classification, since in general these distributions can be arbitrarily complex.
To circumvent this problem, we propose to use the following construction: We define a random
variable ZX 〜 Pχ obtained from the predictive model and study the discrepancy between the joint
distributions of the two pairs of random variables (PX, Y ) and (PX, ZX), respectively, instead of
3
the discrepancy between the conditional distributions P(Y |PX) and PX. Since
(PX, Y ) =d (PX, ZX) if and only if P(Y |PX) = PX almost surely,
model P is calibrated if and only if the distributions of (PX, Y ) and (PX, ZX) are equal.
The random variable pairs (PX, Y ) and (PX, ZX) take values in the product space P × Y, where P is
the space of predicted distributions PX and Y is the space of targets Y . For instance, in classification,
P could be the probability simplex and Y the set of all class labels, whereas in the case of Gaussian
predictive models for scalar targets P could be the space of normal distributions and Y be R.
The study of the joint distributions of (PX, Y ) and (PX, ZX) motivates the definition of a generally
applicable calibration error as an integral probability metric (Muller, 1997; SriPerumbudur et al., 2009;
2012) between these distributions. In contrast to common f -divergences such as the Kullback-Leibler
divergence, integral probability metrics do not require that one distribution is absolutely continuous
with respect to the other, which cannot be guaranteed in general.
Definition 2. Let Y denote the space of targets Y, and P the space of predicted distributions Pχ. We
define the calibration error with respect to a space of functions F of the form f: P ×Y → R as
CEF ：= SUp IEPX,yf(Pχ,Y) — EPX,zχ f(Pχ,Zχ)∣.	(3)
f ∈F
By construction, if model P is calibrated, then CEF = 0 regardless of the choice of F. However, the
converse statement is not true for arbitrary function spaces F. From the theory of integral probability
metrics (see, e.g., Muller, 1997; Sriperumbudur et al., 2009; 2012), we know that for certain choices
ofFthe calibration error in Eq. (3) is a well-known metric on the product space P × Y, which implies
that CEF = 0 if and only if model P is calibrated. Prominent examples include the maximum mean
discrepancy2 (MMD) (Gretton et al., 2007), the total variation distance, the Kantorovich distance,
and the Dudley metric (Dudley, 1989, p. 310).
As pointed out above, Definition 2 is a generalization of the definition for multi-class classification
proposed by Widmann et al. (2019)—which is based on vector-valued functions and only applicable
to finite target spaces—to any probabilistic predictive model. In Appendix E we show this explicitly
and discuss the special case of classification problems in more detail. Previous results (Widmann
et al., 2019) imply that in classification MMCE and, for common distance measures d(∙, ∙) such as
the total variation and squared Euclidean distance, ECEd and MCEd are special cases of CEF. In
Appendix G we show that our framework also covers natural extensions of ECEd and MCEd to
countably infinite discrete target spaces, which to our knowledge have not been studied before and
occur, e.g., in Poisson regression.
The literature of integral probability metrics suggests that we can resort to estimating CEF from i.i.d.
samples from the distributions of (Pχ, Y) and (Pχ, Zχ). For the MMD, the Kantorovich distance,
and the Dudley metric tractable strongly consistent empirical estimators exist (Sriperumbudur et al.,
2012). Here the empirical estimator for the MMD is particularly appealing since compared with the
other estimators “it is computationally cheaper, the empirical estimate converges at a faster rate to the
population value, and the rate of convergence is independent of the dimension d of the space (for
S = Rd)” (Sriperumbudur et al. (2012)).
Our specific design of (Pχ, Zχ) can be exploited to improve on these estimators. If
Ezx~pxf (Pχ, Zx) can be evaluated analytically for a fixed prediction Pχ, then CEF can be es-
timated empirically with reduced variance by marginalizing out ZX. Otherwise Ezx~px f (Px, Zx)
has to be estimated, but in contrast to the common estimators of the integral probability metrics
discussed above the artificial construction of Zχ allows us to approximate it by numerical integration
methods such as (quasi) Monte Carlo integration or quadrature rules with arbitrarily small error
and variance. Monte Carlo integration preserves statistical properties of the estimators such as
unbiasedness and consistency.
2As we discuss in Section 3, the MMD is a metric if and only if the employed kernel is characteristic.
4
3	Kernel calib ration error
For the remaining parts of the paper we focus on the MMD formulation of CEF due to the appealing
properties of the common empirical estimator mentioned above. We derive calibration-specific
analogues of results for the MMD that exploit the special structure of the distribution of (PX , ZX) to
improve on existing estimators and tests in the MMD literature. To the best of our knowledge these
variance-reduced estimators and tests have not been discussed in the MMD literature.
Let k : (P × Y) × (P × Y) → R be a measurable kernel with corresponding reproducing kernel
Hilbert space (RKHS) H, and assume that
EPX,Y k1/2((PX ,Y), (Px ,Y)) < ∞ and EPX ,ZX k1/2((PX,Zχ), (PX, Zx)) < ∞.
We discuss how such kernels can be constructed in a generic way in Section 3.1 below.
Definition 3. Let Fk denote the unit ball in H, i.e., F := {f ∈ H|kf kH ≤ 1}. Then the kernel
calibration error (KCE) with respect to kernel k is defined as
KCEk := CEFk = sup EPX,Yf(PX, Y) - EPX,ZX f(PX, ZX).
f∈Fk
As known from the MMD literature, a more explicit formulation can be given for the squared kernel
calibration error SKCEk := KCE2k (see Lemma B.2). A similar explicit expression for SKCEk was
obtained by Widmann et al. (2019) for the special case of classification problems. However, their
expression relies on Y being finite and is based on matrix-valued kernels over the finite-dimensional
probability simplex P. A key difference to the expression in Lemma B.2 is that we instead propose
to use real-valued kernels defined on the product space of predictions and targets. This construction
is applicable to arbitrary target spaces and does not require Y to be finite.
3.1	Choice of kernel
The construction of the product space PXY suggests the use of tensor product kernels k = kp 0 kγ,
where kP : P × P → R and kY : Y × Y → R are kernels on the spaces of predicted distributions
and targets, respectively.3 4
By definition, so-called characteristic kernels guarantee that KCE = 0 if and only if the distributions
of (PX , Y) and (PX, ZX) are equal (Fukumizu et al., 2004; 2008). Many common kernels such
as the Gaussian and Laplacian kernel on Rd are characteristic (FUkUmizU et al., 2008).4 Szab6 &
Sriperumbudur (2018, Theorem 4) showed that a tensor product kernel kP 0 kY is characteristic
if kP and kY are characteristic, continuous, bounded, and translation-invariant kernels on Rd, but
the implication does not hold for general characteristic kernels (Szab6 & Sriperumbudur, 2018,
Example 1). For calibration evaluation, however, it is sufficient to be able to distinguish between
the conditional distributions P(Y|PX) and P(ZX |PX) = PX. Therefore, in contrast to the regular
MMD setting, it is sufficient that kernel kY is characteristic and kernel kP is non-zero almost surely,
to guarantee that KCE = 0 if and only if model P is calibrated. Thus it is suggestive to construct
kernels on general spaces of predicted distributions as
kp(p,p') = exp ( — λdp(p,p')),	(4)
where dp (∙, ∙) is a metric on P and ν,λ> 0 are kernel hyperparameters. The Wasserstein distance
is a widely used metric for distributions from optimal transport theory that allows to lift a ground
metric on the target space and possesses many important properties (see, e.g., Peyre & Cuturi, 2019,
Chapter 2.4). In general, however, it does not lead to valid kernels kp , apart from the notable
exception of elliptically contoured distributions such as normal and Laplace distributions (Peyre &
Cuturi, 2019, Chapter 8.3).
3As mentioned above, our framework rephrases and generalizes the construction used by Widmann et al.
(2019). The matrix-valued kernels that they employ can be recovered by setting kP to a Laplacian kernel on the
probability simplex and kY(y, y0) = δy,y0 .
4For a general discussion about characteristic kernels and their relation to universal kernels we refer to the
paper by Sriperumbudur et al. (2011).
5
In machine learning, common probabilistic predictive models output parameters of distributions such
as mean and variance of normal distributions. Naturally these parameterizations give rise to injective
mappings φ : P → Rd that can be used to define a Hilbertian metric
dP(p,p0) = kφ(p) - φ(p0)k2.
For such metrics, kP in Eq. (4) is a valid kernel for all λ > 0 and ν ∈ (0, 2] (Berg et al., 1984,
Corollary 3.3.3, Proposition 3.2.7). In Appendix D.3 we show that for many mixture models, and
hence model ensembles, Hilbertian metrics between model components can be lifted to Hilbertian
metrics between mixture models. This construction is a generalization of the Wasserstein-like distance
for Gaussian mixture models proposed by Chen et al. (2019; 2020); Delon & Desolneux (2020).
3.2	Estimation
Let (X1, Y1), . . . , (Xn, Yn) be a data set of features and targets which are i.i.d. according to the law
of (X, Y ). Moreover, for notational brevity, for (p, y), (p0, y0) ∈ P × Y we let
h((p,y), (P0,yO))= k((p,y), (P0,yO)) - EZ〜P k((P)Z), (P0,yO))
-Ezo〜p0 k((p, y), (p0, Z0)) + EZ〜P,Z0〜p0 k((p, Z),(P, Z0)).
Note that in contrast to the regular MMD we marginalize out Z and ZO . Similar to the MMD, there
exist consistent estimators of the SKCE, both biased and unbiased.
Lemma 1. The plug-in estimator of SKCEk is non-negatively biased. It is given by
1n
SKCEk = n E M(PXi,Yi), (Pχj,Yj)).
n i,j=1
Inspired by the block tests for the regular MMD (Zaremba et al., 2013), we define the following
class of unbiased estimators. Note that in contrast to SKCEk they do not include terms of the form
M(PXi ,Yi), (PXi ,Yi)).
Lemma 2. The block estimator of SKCEk with block size B∈ {2, . . . , n}, given by
-1 bn/B c B -1
SKCEk,B := B X 2	X	M(PXi,Yi),(Pχj,Yj)),
b=1	(b-1)B<i<j≤bB
is an unbiased estimator of SKCEk.
The extremal estimator with B = n is a so-called U-statistic of SKCEk (Hoeffding, 1948; van der
Vaart, 1998), and hence it is the minimum variance unbiased estimator. All presented estimators are
consistent, i.e., they converge to SKCEk almost surely as the number n of data points goes to infinity.
2
The sample complexity of SKCEk and SKCEk,B is O(n2) and O(Bn), respectively.
3.3	Calibration tests
A fundamental issue with calibration errors in general, including ECE, is that their empirical estimates
do not provide an answer to the question if a model is actually calibrated. Even if the measure is
guaranteed to be zero if and only if the model is calibrated, usually the estimates of calibrated models
are non-zero due to randomness in the data and (possibly) the estimation procedure. In classification,
statistical hypothesis tests of the null hypothesis
H0 : model P is calibrated,
so-called calibration tests, have been proposed as a tool for checking rigorously if P is cali-
brated (Brocker & Smith, 2007; Vaicenavicius et al., 2019; Widmann et al., 2019). For multi-class
classification, Widmann et al. (2019) suggested calibration tests based on the asymptotic distributions
of estimators of the previously formulated KCE. Although for finite data sets the asymptotic distri-
butions are only approximations of the actual distributions of these estimators, in their experiments
with 10 classes the resulting P-value approximations seemed reliable whereas P-values obtained by
6
so-called consistency resampling (BrGcker & Smith, 2007; Vaicenavicius et al., 2019) underestimated
the p-value and hence rejected the null hypothesis too often (Widmann et al., 2019).
For fixed block sizes P[n∕B] (SKCEk,b - SKCEk) → N(0, σB) as n → ∞, and, under Ho,
nSKCEk,n -→d i∞=1 λi (Zi - 1) as n → ∞, where Zi are independent χ21 distributed random
variables. See Appendix B for details and definitions of the involved constants. From these results
one can derive calibration tests that extend and generalize the existing tests for classification problems,
as explained in Remarks B.1 and B.2. Our formulation illustrates also the close connection of these
tests to different two-sample tests (Gretton et al., 2007; Zaremba et al., 2013).
4	Alternative approaches
For two-sample tests, Chwialkowski et al. (2015) suggested the use of the so-called unnormalized
mean embedding (UME) to overcome the quadratic sample complexity of the minimum variance
unbiased estimator and its intractable asymptotic distribution. As we show in Appendix C, there exists
an analogous measure of calibration, termed unnormalized calibration mean embedding (UCME),
with a corresponding calibration mean embedding (CME) test.
As an alternative to our construction based on the joint distributions of (PX, Y ) and (PX, ZX),
one could try to directly compare the conditional distributions P(Y |PX) and P(ZX |PX) = PX.
For instance, Ren et al. (2016) proposed the conditional MMD based on the so-called conditional
kernel mean embedding (Song et al., 2009; 2013). However, as noted by Park & Muandet (2020),
its common definition as operator between two RKHS is based on very restrictive assumptions,
which are violated in many situations (see, e.g., Fukumizu et al., 2013, Footnote 4) and typically
require regularized estimates. Hence, even theoretically, often the conditional MMD is “not an exact
measure of discrepancy between conditional distributions” (Park & Muandet (2020)). In contrast,
the maximum conditional mean discrepancy (MCMD) proposed in a concurrent work by Park &
Muandet (2020) is a random variable derived from much weaker measure-theoretical assumptions.
The MCMD provides a local discrepancy conditional on random predictions whereas KCE is a global
real-valued summary of these local discrepancies.5
5	Experiments
In our experiments we evaluate the computational efficiency and empirical properties of the proposed
calibration error estimators and calibration tests on both calibrated and uncalibrated models. By
means of a classic regression problem from statistics literature, we demonstrate that the estimators
and tests can be used for the evaluation of calibration of neural network models and ensembles of
such models. This section contains only an high-level overview of these experiments to conserve
space but all experimental details are provided in Appendix A.
5.1	Empirical properties and computational efficiency
We evaluate error, variance, and computation time of calibration error estimators for calibrated and
uncalibrated Gaussian predictive models in synthetic regression problems. The results empirically
confirm the consistency of the estimators and the computational efficiency of the estimator with block
size B = 2 which, however, comes at the cost of increased error and variance.
Additionally, we evaluate empirical test errors of calibration tests ata fixed significance level α = 0.05.
The evaluations, visualized in Fig. 2 for models with ten-dimensional targets, demonstrate empirically
that the percentage of incorrect rejections of H0 converges to the set significance level as the number
of samples increases. Moreover, the results highlight the computational burden of the calibration test
that estimates quantiles of the intractable asymptotic distribution of nSKCEk,n by bootstrapping.
5In our calibration setting, the MCMD is almost surely equal to supf ∈F EY |PX f(Y)|PX -
EZXIPX If(ZX )∣Pχ )∣, where FY := {f: Y→ R∣kf∣∣HY ≤ 1} for an RKHS HY with kernel kγ : Y×Y一
R. If kernel kY is characteristic, MCMD = 0 almost surely if and only if model P is calibrated (Park &
Muandet, 2020, Theorem 3.7). Although the definition of MCMD only requires a kernel kY on the target space,
a kernel kP on the space of predictions has to be specified for the evaluation of its regularized estimates.
7
As expected, due to the larger variance of SKCEk,2 the test with fixed block size B = 2 shows a
decreased test power although being computationally much more efficient.
calibrated model
」0」」① -Od--0」」① --Od-
-0」」① -edE①
0.75 -
0.50 -
0.25 -
0.00 -
uncalibrated model
o SKCE (B = 2)
□	SKCE (B = √n)
△	SKCE (B = n)
◊	CME
--significance level
Figure 2: Empirical test errors for 500 data sets of n ∈ {4, 16, 64, 256, 1024} samples from models
with targets of dimension d = 10. The dashed black line indicates the set signficance level α = 0.05.
5.2	Friedman 1 regression problem
The Friedman 1 regression problem (Friedman, 1979; 1991; Friedman et al., 1983) is a classic
non-linear regression problem with ten-dimensional features and real-valued targets with Gaussian
noise. We train a Gaussian predictive model whose mean is modelled by a shallow neural network and
a single scalar variance parameter (consistent with the data-generating model) ten times with different
initial parameters. Figure 3 shows estimates of the mean squared error (MSE), the average negative
log-likelihood (NLL), SKCEk, and a p-value approximation for these models and their ensemble
on the training and a separate test data set. All estimates indicate consistently that the models are
overfit after 1500 training iterations. The estimations of SKCEk and the p-values allow to focus
on calibration specifically, whereas MSE indicates accuracy only and NLL, as any proper scoring
rule (Brocker, 2009), provides a summary of calibration and accuracy. The estimation of SKCEk in
addition to NLL could serve as another source of information for early stopping and model selection.

10-2∙4 _
10-1∙s
10-,∙2 -
400
800
1200
training
-models
-ensemble
test
-models
-ensemble
iteration
Figure 3: Mean squared error (MSE), average negative log-likelihood (NLL), SKCEk (SKCE
(biased)), and p-value approximation (p-value) of ten Gaussian predictive models for the Friedman 1
regression problem versus the number of training iterations. Evaluations on the training data set
(100 samples) are displayed in green and orange, and on the test data set (50 samples) in blue and
purple. The green and blue line and their surrounding bands represent the mean and the range of the
evaluations of the ten models. The orange and purple lines visualize the evaluations of their ensemble.
8
6	Conclusion
We presented a framework of calibration estimators and tests for any probabilistic model that captures
both classification and regression problems of arbitrary dimension as well as other predictive models.
We successfully applied it for measuring calibration of (ensembles of) neural network models.
Our framework highlights connections of calibration to two-sample tests and optimal transport theory
which we expect to be fruitful for future research. For instance, the power of calibration tests could
be improved by heuristics and theoretical results about suitable kernel choices or hyperparameters (cf.
Jitkrittum et al., 2016). It would also be interesting to investigate alternatives to KCE captured by our
framework, e.g., by exploiting recent advances in optimal transport theory (cf. Genevay et al., 2016).
Since the presented estimators of SKCEk are differentiable, we imagine that our framework could be
helpful for improving calibration of predictive models, during training (cf. Kumar et al., 2018) or
post-hoc. Currently, many calibration methods (see, e.g., Guo et al., 2017; Kull et al., 2019; Song
et al., 2019) are based on optimizing the log-likelihood since it is a strictly proper scoring rule and
thus encourages both accurate and reliable predictions. However, as for any proper scoring rule,
“Per se, it is impossible to say how the score will rank unreliable forecast schemes [. . .]. The lack
of reliability of one forecast scheme might be outbalanced by the lack of resolution of the other”
(BrOcker (2009)). In other words, if one does not use a calibration method such as temperature
scaling (Guo et al., 2017) that keeps accuracy invariant6, it is unclear if the resulting model is trading
off calibration for accuracy when using log-likelihood for re-calibration. Thus hypothetically flexible
calibration methods might benefit from using the presented calibration error estimators.
Acknowledgments
We thank the reviewers for all the constructive feedback on our paper. This research is financially
supported by the Swedish Research Council via the projects Learning of Large-Scale Probabilistic
Dynamical Models (contract number: 2016-04278), Counterfactual Prediction Methods for Heteroge-
neous Populations (contract number: 2018-05040), and Handling Uncertainty in Machine Learning
Systems (contract number: 2020-04122), by the Swedish Foundation for Strategic Research via the
project Probabilistic Modeling and Inference for Machine Learning (contract number: ICA16-0015),
by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and
Alice Wallenberg Foundation, and by ELLIIT.
References
M. A. Arcones and E. Gin2 On the bootstrap of U and V statistics. The Annals of Statistics, 20(2):
655-674,1992.
C. Berg, J. P. R. Christensen, and P. Ressel. Harmonic Analysis on Semigroups. Springer New York,
1984.
J.	BrOcker and L. A. Smith. Increasing the reliability of reliability diagrams. Weather and Forecasting,
22(3):651-661, June 2007.
Jochen BrOcker. Reliability, sufficiency, and the decomposition of proper scores. Quarterly Journal
of the Royal Meteorological Society, 135(643):1512-1519, July 2009.
Y. Chen, T. T. Georgiou, and A. Tannenbaum. Optimal transport for Gaussian mixture models. IEEE
Access, 7:6269-6278, 2019.
Y. Chen, J. Ye, and J. Li. Aggregated Wasserstein distance and state registration for hidden Markov
models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(9):2133-2147,
September 2020.
K.	Chwialkowski, A. Ramdas, D. Sejdinovic, and A. Gretton. Fast two-sample testing with analytic
representations of probability measures. In Proceedings of the 28th International Conference on
Neural Information Processing Systems, pp. 1981-1989, Cambridge, MA, USA, 2015. MIT Press.
6Temperature scaling can be defined and applied for general probabilistic predictive models, see Appendix F.
9
M. H. DeGroot and S. E. Fienberg. The comparison and evaluation of forecasters. The Statistician,
32(1/2):12, March 1983.
C. Deledalle, S. Parameswaran, and T. Q. Nguyen. Image denoising with generalized Gaussian
mixture model patch priors. SIAM Journal on Imaging Sciences, 11(4):2568-2609, January 2018.
J. Delon and A. Desolneux. A Wasserstein-type distance in the space of Gaussian mixture models.
SIAM Journal on Imaging Sciences, 13(2):936-970, January 2020.
R. M. Dudley. Real analysis and probability. Wadsworth & Brooks/Cole Pub. Co, Pacific Grove,
Calif, 1989.
M. Fasiolo, S. N. Wood, M. Zaffran, R. Nedellec, and Y. Goude. Fast calibrated additive quantile
regression. Journal of the American Statistical Association, pp. 1-11, March 2020.
J. H. Friedman. A tree-structured approach to nonparametric multiple regression. In Lecture Notes in
Mathematics, pp. 5-22. Springer Berlin Heidelberg, 1979.
J. H. Friedman. Multivariate adaptive regression splines. The Annals of Statistics, 19(1):1-67, 1991.
J. H. Friedman, E. Grosse, and W. Stuetzle. Multidimensional additive spline approximation. SIAM
Journal on Scientific and Statistical Computing, 4(2):291-301, June 1983.
K. Fukumizu, F. R. Bach, and M. I. Jordan. Dimensionality reduction for supervised learning with
reproducing kernel Hilbert spaces. Journal of Machine Learning Research, 5(Jan):73-99, 2004.
K. Fukumizu, A. Gretton, X. Sun, and B. Scholkopf. Kernel measures of conditional dependence. In
Advances in Neural Information Processing Systems 20, pp. 489-496. 2008.
K. Fukumizu, L. Song, and A. Gretton. Kernel Bayes’ rule: Bayesian inference with positive definite
kernels. Journal of Machine Learning Research, 14(82):3753-3783, 2013.
M. Gelbrich. On a formula for the l2 Wasserstein metric between measures on Euclidean and Hilbert
spaces. Mathematische Nachrichten, 147(1):185-203, 1990.
A. Genevay, M. Cuturi, G. Peyra and F. R. Bach. Stochastic optimization for large-scale optimal
transport. In Advances in Neural Information Processing Systems 29, pp. 3440-3448. 2016.
X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks.
In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,
volume 9 of Proceedings of Machine Learning Research, pp. 249-256. PMLR, 5 2010.
E.	G6mez, M. A. Gdmez-Viilegas, and J. M. Marin. A multivariate generalization of the power
exponential family of distributions. Communications in Statistics - Theory and Methods, 27(3):
589-600, January 1998.
E. Gdmez-Sdnchez-Manzano, M. A. Gdmez-Villegas, and J. M. Marin. Multivariate exponential
power distributions as mixtures of normal distributions with Bayesian applications. Communica-
tions in Statistics - Theory and Methods, 37(6):972-985, February 2008.
A. Gretton, K. Borgwardt, M. Rasch, B. Scholkopf, and A. J. Smola. A kernel method for the
two-sample-problem. In Advances in Neural Information Processing Systems 19, pp. 513-520.
2007.
A. Gretton, K. Fukumizu, Z. Harchaoui, and B. K. Sriperumbudur. A fast, consistent kernel two-
sample test. In Advances in Neural Information Processing Systems 22, pp. 673-681. 2009.
C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. On calibration of modern neural networks. In
Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings
of Machine Learning Research, pp. 1321-1330. PMLR, 8 2017.
F.	K. Gustafsson, M. Danelljan, and T. B. Schon. Evaluating scalable Bayesian deep learning methods
for robust computer vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) Workshops, 2020.
10
Y. H. S. Ho and S. M. S. Lee. Calibrated interpolated confidence intervals for population quantiles.
Biometrika, 92(1):234-241, March 2005.
W. Hoeffding. A class of statistics with asymptotically normal distribution. The Annals of Mathemat-
ical Statistics, 19(3):293-325, September 1948.
H. Hotelling. The generalization of student’s ratio. The Annals of Mathematical Statistics, 2(3):
360-378, August 1931.
M.	Innes. Flux: Elegant machine learning with Julia. Journal of Open Source Software, 3(25):602,
May 2018.
M. Innes, E. Saba, K. Fischer, D. Gandhi, M. C. Rudilosso, N. M. Joy, T. Karmali, A. Pal, and
V. Shah. Fashionable modelling with Flux, 2018.
W. Jitkrittum, Z. Szab6, K. P. ChWialkoWski, and A. Gretton. Interpretable distribution features with
maximum testing power. In Advances in Neural Information Processing Systems 29, pp. 181-189.
2016.
N.	L. Johnson, S. Kotz, and N. Balakrishnan. Continuous univariate distributions: Vol. 1. Wiley,
NeW York, 2nd edition, 1994.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR (Poster), 2015.
V. Kuleshov, N. Fenner, and S. Ermon. Accurate uncertainties for deep learning using calibrated
regression. In Proceedings of the 35th International Conference on Machine Learning, volume 80
of Proceedings of Machine Learning Research, pp. 2796-2804. PMLR, 7 2018.
M. Kull, T. Silva Filho, and P. Flach. Beta calibration: a Well-founded and easily implemented
improvement on logistic calibration for binary classifiers. In Proceedings of the 20th International
Conference on Artificial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning
Research, pp. 623-631. PMLR, 4 2017.
M. Kull, M. Perello Nieto, M. Kangsepp, T. Silva Filho, H. Song, and P. Flach. Beyond temperature
scaling: Obtaining Well-calibrated multi-class probabilities With Dirichlet calibration. In Advances
in Neural Information Processing Systems 32, pp. 12316-12326. 2019.
A. Kumar, S. SaraWagi, and U. Jain. Trainable calibration measures for neural netWorks from kernel
mean embeddings. In Proceedings of the 35th International Conference on Machine Learning,
volume 80 of Proceedings of Machine Learning Research, pp. 2805-2814. PMLR, 7 2018.
A. M. Mathai and S. B. Provost. Quadratic forms in random variables: Theory and applications,
volume 126. M. Dekker, NeW York, 1992.
C. A. Micchelli and M. Pontil. On learning vector-valued functions. Neural Computation, 17(1):
177-204, January 2005.
A. Muller. Integral probability metrics and their generating classes of functions. Advances inApplied
Probability, 29(2):429-443, June 1997.
A. H. Murphy and R. L. Winkler. Reliability of subjective probability forecasts of precipitation and
temperature. Applied Statistics, 26(1):41, 1977.
M. P. Naeini, G. Cooper, and M. Hauskrecht. Obtaining Well calibrated probabilities using Bayesian
binning. In AAAI Conference on Artificial Intelligence, 2015.
J. Park and K. Muandet. A measure-theoretic approach to kernel conditional mean embeddings. In
Advances in Neural Information Processing Systems, volume 33, pp. 21247-21259, 2020.
G.	Peyre and M. Cuturi. Computational optimal transport. Foundations and Trends in Machine
Learning, 11(5-6):355-607, 2019.
J. Platt. Probabilities for SV Machines, pp. 61-73. MIT Press, 2000.
11
Y. Ren, J. Zhu, J. Li, and Y. Luo. Conditional generative moment-matching networks. In Advances in
Neural Information Processing Systems 29,pp. 2928-2936. 2016.
M. Rueda, S. Martinez-Puertas, H. Martinez-Puertas, and A. Arcos. Calibration methods for estimat-
ing quantiles. Metrika, 66(3):355-371, December 2006.
R. J. Serfling (ed.). Approximation Theorems of Mathematical Statistics. John Wiley & Sons, Inc.,
November 1980.
H.	Song, T. Diethe, M. Kull, and P. Flach. Distribution calibration for regression. In Proceedings of
the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine
Learning Research, pp. 5897-5906. PMLR, 6 2019.
L. Song, J. Huang, A. J. Smola, and K. Fukumizu. Hilbert space embeddings of conditional distribu-
tions with applications to dynamical systems. In Proceedings of the 26th Annual International
Conference on Machine Learning, ICML ’09, pp. 961-968. Association for Computing Machinery,
2009.
L. Song, K. Fukumizu, and A. Gretton. Kernel embeddings of conditional distributions: A unified
kernel framework for nonparametric inference in graphical models. IEEE Signal Processing
Magazine, 30(4):98-111, July 2013.
B. K. Sriperumbudur, K. Fukumizu, A. Gretton, B. Scholkopf, and G. R. G. Lanckriet. On integral
probability metrics, φ-divergences and binary classification, 2009.
B. K. Sriperumbudur, K. Fukumizu, and G. R.G. Lanckriet. Universality, characteristic kernels and
RKHS embedding of measures. Journal of Machine Learning Research, 12(70):2389-2410, 2011.
B. K. Sriperumbudur, K. Fukumizu, A. Gretton, B. Scholkopf, and G. R. G. Lanckriet. On the
empirical estimation of integral probability metrics. Electronic Journal of Statistics, 6(0):1550-
1599, 2012.
Z. Szab6 and B. K. Sriperumbudur. Characteristic and universal tensor product kernels. Journal of
Machine Learning Research, 18(233):1-29, 2018.
M. Taillardat, O. Mestre, M. Zamo, and P. Naveau. Calibrated ensemble forecasts using quantile
regression forests and ensemble model output statistics. Monthly Weather Review, 144(6):2375-
2393, June 2016.
J. Vaicenavicius, D. Widmann, C. Andersson, F. Lindsten, J. Roll, and T. B. Schon. Evaluating
model calibration in classification. In Proceedings of Machine Learning Research, volume 89 of
Proceedings of Machine Learning Research, pp. 3459-3467. PMLR, 4 2019.
A. W. van der Vaart. Asymptotic Statistics. Cambridge University Press, October 1998.
C.	Villani. Optimal Transport. Springer Berlin Heidelberg, 2009.
D.	Widmann, F. Lindsten, and D. Zachariah. Calibration tests in multi-class classification: A unifying
framework. In Proceedings of the 32th International Conference on Neural Information Processing
Systems, pp. 12236-12246. 2019.
S. J. Yakowitz and J. D. Spragins. On the identifiability of finite mixtures. The Annals of Mathematical
Statistics, 39(1):209-214, February 1968.
Bianca Zadrozny. Reducing multiclass to binary by coupling probability estimates. In Advances in
Neural Information Processing Systems 14, pp. 1041-1048. MIT Press, 2002.
W. Zaremba, A. Gretton, and M. Blaschko. B-test: A non-parametric, low variance kernel two-sample
test. In Advances in Neural Information Processing Systems 26, pp. 755-763. 2013.
12
A Experiments
The source code of the experiments and instructions for reproducing the results are available
at https://github.com/devmotion/Calibration_ICLR2021. Additional material
such as automatically generated HTML output and Jupyter notebooks is available at https:
//devmotion.github.io/Calibration_ICLR2021/.
A.1 Ordinary least squares
We consider a regression problem with scalar feature X and scalar target Y with input-dependent
Gaussian noise that is inspired by a problem by Gustafsson et al. (2020). Feature X is distributed
uniformly at random in [-1, 1], and target Y is distributed according to
Y 〜sin(πX) + |1+ X|e,
where e 〜N(0,0.152). We train a linear regression model P with homoscedastic variance using
ordinary least squares and a data set of 100 i.i.d. pairs of feature X and target Y (see Fig. 4).
P)(YIX)	P(Y∣X)
-1.0	-0.5	0.0	0.5	1.0 -1.0	-0.5	0.0	0.5	1.0
X
X

B-1.00
-0.75
-0.50
-0.25
-0.00
Figure 4:	Data generating distribution P(Y|X) and predicted distribution P(Y|X) of the linear
regression model. Training data is indicated by orange dots.
A validation data set of n = 50 i.i.d. pairs of X and Y is used to evaluate the empirical cumulative
probability
n
n-1 X i[0,τ](P(Y ≤ YdX = Xi))
i=1
of model P for quantile levels τ ∈ [0, 1]. Model P would be quantile calibrated (Song et al., 2019) if
T = Pχo,γo (P(Y ≤ Y0|X = X0) ≤ τ)
for all τ ∈ [0, 1], where (X, Y) and (X0, Y0) are independent identically distributed pairs of random
variables (see Fig. 5).
Additionally, we compute a p-value estimate of the null hypothesis H0 that model P is calibrated
using an estimation of the quantile of the asymptotic distribution of nSKCEk,n with 100000 bootstrap
samples on the validation data set (see Remark B.2). Kernel k is chosen as the tensor product kernel
k((p,y), (p0,y0)) = exp (- W2(p,p0)) exp (- (y - y0)2∕2)
=exp ( - q(mp - my)2 + (σp - bp，)2) exp ( - (y - y0)2∕2),
where W2 is the 2-Wasserstein distance and mp , mp0 and σp , σp0 denote the mean and the standard
deviation of the normal distributions p and p0 (see Appendix D.1). We obtain p < 0.05 in our
experiment, and hence the calibration test rejects H0 at the significance level α = 0.05.
13
1.00∏
A-=qeqo-ld ①>4-nEnu
0.75-
0.50 —
0.25—
-ideal
-data
0.00-P-------1-------1------1-------[
0.00	0.25	0.50	0.75	1.00
quantile level
Figure 5:	Cumulative probability versus quantile level for the linear regression model on the validation
data (orange curve). The green curve indicates the theoretical ideal for a quantile-calibrated model.
A.2 Empirical properties and computational efficiency
We study two setups with d-dimensional targets Y and normal distributions PX of the form
N(c1d, 0.12Id) as predictions, where C 〜 U(0,1). Since calibration analysis is only based on
the targets and predicted distributions, we neglect features X in these experiments and specify only
the distributions of Y and PX .
In the first setup we simulate a calibrated model. We achieve this by sampling targets from the
predicted distributions, i.e., by defining the conditional distribution of Y given PX as
Y | Px = N(μ, ∑)〜N(μ, ∑).
In the second setup we simulate an uncalibrated model of the form
Y | PX = N(μ, ∑)〜N([0.1, μ2,..., μd]T, ∑).
We perform an evaluation of the convergence and computation time of the biased estimator SKCEk
and the unbiased estimator SKCEk,B with blocks of size B ∈ {2, √n, n}. We use the tensor product
kernel
k((p,y), (p0,y0)) = exp (- W2(p,p0)) exp (- (y - y0)2∕2)
exp
(mp - mp0)2 + (σp - σp0)2 exp ( - (y - y0)2∕2),
where W2 is the 2-Wasserstein distance and mp , mp0 and σp , σp0 denote the mean and the standard
deviation of the normal distributions p and p0 .
Figures 6 to 9 visualize the mean absolute error and the variance of the resulting estimates for the
calibrated and the uncalibrated model with dimensions d = 1 and d = 10 for 500 independently
drawn data sets of n ∈ {4, 16, 64, 256, 1024} samples of (PX, Y). Computation time indicates the
minimum time in the 500 evaluations on a computer with a 3.6 GHz processor. The ground truth
values of the uncalibrated models were estimated by averaging the estimates of SKCEk,1000 for 1000
independently drawn data sets of 1000 samples of (PX, Y) (independent from the data sets used
for the evaluation of the estimates). Figures 6 and 7 illustrate that the computational efficiency of
SKCEk,2 in comparison with the other estimators comes at the cost of increased error and variance
for the calibrated models for fixed numbers of samples.
~	1.1	..	.	.	1	1	.1 Z. . 1 1 X	. ..	1. . .1	.. C 「\—=TRT=
We compare calibration tests based on the (tractable) asymptotic distribution of bn/BcSKCEk,B
with fixed block size B ∈ {2, √n} (See Remark B.1), the (intractable) asymptotic distribution of
nSKCEk,n which is approximated with 1000 bootstrap samples (see Remark B.2), and a Hotelling’s
14
10-2∙4H
①uro8lu
①uueye>
10-5
10-3-
10-4 —
22	24	10	10	210	io/	Ic)-0∙5	-o-3∙o
#	samples	time [s]
o SKCE ---SKCE (B = 2) △ SKCE (B = √n) ◊ SKCE (B = n)
Figure 6: Mean absolute error and variance of 500 calibration error estimates
n ∈ {4,16, 64, 256,1024} samples from the calibrated model of dimension d = 1.
10-2-
for data sets of
①uueue>
10-4"
10-6∙o-
10-7"
10-9-θ-
①Ue①E
22	24	26	28	210	10-6∙o	Ic)-4.5	Io-3.0 Io-I
#	samples	time [s]
0 SKCE 口 SKCE (B = 2) △ SKCE (B = √n) ◊ SKCE (B = n)
Figure 7: Mean absolute error and variance of 500 calibration error estimates for
n ∈ {4,16, 64, 256,1024} samples from the calibrated model of dimension d = 10.
data sets of
15
① UUeUe>
10-1-2-
①uro8lu
10-3∙2-
10-4-θ-
22	24
26	28	210
10-6∙O	Ier4,5	10-3.0
# samples
time [s]
o SKCE ---SKCE (B = 2) △ SKCE (B = √n) ◊ SKCE (B = n)
for data sets of
n ∈ {4,16, 64, 256,
model of dimension d = 1.
Figure 8: Mean absolute error and variance of 500 calibration error estimates
10-1∙2-
- -
MO O
ɪ II
①u8lu
10-2∙4-
10-3∙2-
10-4∙o-
10-4.8_
# samples
o SKCE ---SKCE (B = 2)
SKCE (B = √n) O SKCE (B = n)
time [s]
①uueue>
Figure 9:	Mean absolute error and variance of 500 calibration error estimates for data sets of
n ∈ {4,16, 64, 256,1024} samples from the uncalibrated model of dimension d = 10.
16
T 2-statistic for UCMEk, 10 with 10 test locations (see Appendix C). We compute the empirical test
errors (percentage of false rejections of the null hypothesis H0 that model P is calibrated if P is
calibrated, and percentage of false non-rejections of H0 if P is not calibrated) at a fixed significance
level α = 0.05 and the minimal computation time for the calibrated and the uncalibrated model with
dimensions d = 1 and d = 10 for 500 independently drawn data sets of n ∈ {4, 16, 64, 256, 1024}
samples of (PX, Y ). The 10 test predictions of the CME test are of the form N(m, 0.12Id) where
m is distributed uniformly at random in the d-dimensional unit hypercube [0, 1]d, the corresponding
10 test targets are i.i.d. according to N(0, 0.12Id).
Figures 10 and 11 show that all tests adhere to the set significance level asymptotically as the number
of samples increases. The convergence of the CME test with 10 test locations is found to be much
slower than the convergence of all other tests. The tests based on the tractable asymptotic distribution
of bn/BcSKCEk,B for fixed block size B are orders of magnitudes faster than the test based on
the intractable asymptotic distribution of nSKCEk,n, approximated with 1000 bootstrap samples. We
see that the efficiency gain comes at the cost of decreased test power for smaller number of samples,
explained by the increasing variance of SKCEk,B for decreasing block sizes B. However, in our
examples the test based on SKCEk,√n still achieves good test power for reasonably large number of
samples (> 30).
calibrated model
一。」」① 一 OdM-OS = OdAa
」0」」①-①a -e--dE①
0.75 -
0.50 -
0.25 -
0.00 -
uncalibrated model
0.6 -
0.4-
0.2 -
0.0 -.
22	24	2s
time [s]
o SKCE (B = 2)
-■-SKCE (B = √n)
△ SKCE (B = n)
◊ CME
--significance level
# samples
Figure 10:	Empirical test errors for 500 data sets of n ∈ {4, 16, 64, 256, 1024} samples from models
with targets of dimension d = 1. The dashed black line indicates the set signficance level α = 0.05.
calibrated model
」0」一①-①d--0」」①--①d-
」0」」① as①a -edE①
0.75 -
0.50 -
0.25 -
0.00 -t
uncalibrated model
o SKCE (B = 2)
□	SKCE (B = √n)
△	SKCE (B = n)
◊	CME
--significance level
Figure 11:	Empirical test errors for 500 data sets of n ∈ {4, 16, 64, 256, 1024} samples from models
with targets of dimension d = 10. The dashed black line indicates the set signficance level α = 0.05.
17
A.3 Friedman 1 regression problem
We study the so-called Friedman 1 regression problem, which was initially described for 200 inputs
in the six-dimensional unit hypercube (Friedman, 1979; Friedman et al., 1983) and later modified
to 100 inputs in the 10-dimensional unit hypercube (Friedman, 1991). In this regression problem
real-valued target Y depends on input X via
Y = 10 sin (πX1X2) + 20(X3 - 0.5)2 + 10X4 + 5X5 +,
where noise is typically chosen to be independently standard normally distributed. We generate a
training data set of 100 inputs distributed uniformly at random in the 10-dimensional unit hypercube
and corresponding targets with identically and independently distributed noise following a standard
normal distribution.
We consider models P (θ,σ2) of normal distributions with fixed variance σ2
Px(θ,σ2) =N(fθ(x),σ2),
where fθ(x), the model of the mean of the distribution P(Y |X = x), is given by a fully connected
neural network with two hidden layers with 200 and 50 hidden units and ReLU activation functions.
The parameters of the neural network are denoted by θ.
We use a maximum likelihood approach and train the parameters θ of the model for 5000 iterations
by minimizing the mean squared error on the training data set using ADAM (Kingma & Ba, 2015)
(default settings in the machine learning framework Flux.jl (Innes, 2018; Innes et al., 2018)). In each
iteration, the variance σ2 is set to the maximizer of the likelihood of the training data set.
We train 10 models with different initializations of parameters θ . The initial values of the weight
matrices of the neural networks are sampled from the uniform Glorot initialization (Glorot & Bengio,
2010) and the offset vectors are initialized with zeros. In Fig. 12, we visualize estimates of accuracy
and calibration measures on the training and test data set with 100 and 50 samples, respectively, for
5000 training iterations. The pinball loss is a common measure and training objective for calibration
of quantiles (Song et al., 2019). It is defined as
Ex,y LT (Y, quantile(Pχ ,τ)),
where Lτ(y, y) = (1 - T)(y - y)+ + T(y - y)+ and quantile(Pχ, τ) = infy{Pχ(Y ≤ y) ≥ T}
for quantile level τ ∈ [0, 1]. In Fig. 12 we plot the average pinball loss (pinball) for quantile levels
T ∈ {0.05, 0.1, . . ., 0.95}. We evaluate S\KCEk,n (SKCE (unbiased)) and S\KCEk (SKCE (biased))
for the tensor product kernel
k((p,y), (p0,y0)) = exp ( - W2(p,p0)) exp (- (y - y0)2∕2)
=exp ( - q(mp - mp0)2 + (σp - σp)) exp ( - (y - y0)2∕2),
where W2 is the 2-Wasserstein distance and mp , mp0 and σp , σp0 denote the mean and the standard
deviation of the normal distributions p and p0 (see Appendix D.1). The p-value estimate (p-value) is
computed by estimating the quantile of the asymptotic distribution of nSKCEk,n with 1000 bootstrap
samples (see Remark B.2). The estimates of the mean squared error and the average negative log-
likelihood are denoted by MSE and NLL. All estimators indicate consistently that the trained models
suffer from overfitting after around 1000 training iterations.
Additionally, we form ensembles of the ten individual models at every training iteration. The
evaluations for the ensembles are visualized in Fig. 12 as well. Apart from the unbiased estimates
of SKCEk, the estimates of the ensembles are consistently better than the average estimates of the
ensemble members. For the mean squared error and the negative log-likelihood this behaviour is
guaranteed theoretically by the generalized mean inequality.
B	Theory
B.1	General setting
Let (Ω, A, P) be a probability space. Define the random variables X: (Ω, A) → (X, Σχ) and
Y: (Ω, A) → (Y, Σγ) such that Σχ contains all singletons, and denote a version of the regular
conditional distribution of Y given X = x by P(Y |X = x) for all x ∈ X.
18
MSE
SKCE (unbiased)
training
-models
-ensemble
test
-models
-ensemble
iteration
Figure 12: Estimates of different accuracy and calibration measures of ten Gaussian predictive models
for the Friedman 1 regression problem versus the number of training iterations. Evaluations on the
training data set (100 samples) are displayed in green and orange, and on the test data set (50 samples)
in blue and purple. The green and blue line and their surrounding bands represent the mean and the
range of the evaluations of the ten models. The orange and purple lines visualize the evaluations of
their ensemble.
Let P : (X , ΣX) → P, B(P) be a measurable function that maps features in X to probability
measures in P on the target space Y. We call P a probabilistic model, and denote by Px := P (x)
its output for feature X ∈ X. This gives rise to the random variable Pχ : (Ω, A) → (P, B(P)) as
PX := P (X). We denote a version of the regular conditional distribution of Y given PX = Px by
P(Y |Px = Px) forall Px ∈P.
B.2	Expected and maximum calibration error
The common definition of the expected and maximum calibration error (Guo et al., 2017; Kull et al.,
2019; Naeini et al., 2015; Vaicenavicius et al., 2019) for classification models can be generalized to
arbitrary predictive models.
Definition B.1. Let d(∙, ∙) be a distance measure of probability distributions of target Y, and let μ be
the law of Pχ . Then we call
ECEd = E d( P(Y |PX ),Pχ)	and MCEd = μ- esssup d( P(Y ∣Pχ ),Pχ)
the expected calibration error (ECE) and the maximum calibration error (MCE) of model P with
respect to measure d, respectively.
B.3	Kernel calibration error
Recall the general notation: Let k : (P × Y) × (P × Y) → Rbe a kernel, amd denote its corresponding
RKHS by H.
If not stated otherwise, we assume that
(K1) k(∙, ∙) is Borel-measurable.
(K2) k is integrable with respect to the distributions of (Pχ , Y) and (Pχ , Zχ), i.e.,
EPX ,Y k1/2((PX ,Y), (Px ,Y)) < ∞
and
EPX ,ZX k1/2((PX, Zχ), (Pχ, Zχ)) < ∞.
19
Lemma B.1. There exist kernel mean embeddings μpχ Y ,μpχ ZX ∈ H such that for all f ∈ H
hf,μPχ Y iH = EPX ,Y f(PX ,y )	and	hf,μPχ ZXiH = EPX ,Zχ f (PX ,ZX )∙
This implies that
μPXY = EPX,y k(∙, (Pχ,Y))	and	NPxZX = EPX,Zx k(∙, (PX, ZX)).
Proof. The linear operators TPXYf := EPX,Y f(PX,Y) andTPXZXf := EPX,ZX f(PX,ZX) for
all f ∈ H are bounded since
|Tpx y f | = | EPX ,γ f(Pχ ,Y )∣≤ EPX ,y |f (Px ,Y )| = EPX ,y Kk((PX ,Y ),∙),fiH∣
≤ Epx ,Y kk((Pχ ,Y), ∙)kHkfkH] = kfkH Epx ,Y k1/2((PX ,Y), (Px ,Y))
and similarly
|TPX ZX f| ≤ kfkH EPX,ZX k1/2 ((PX, ZX ), (PX , ZX )).
Thus Riesz representation theorem implies that there exist μpχ Y ,μpχ ZX ∈ H such that TPX Y f =
hf, μpχ y〉h and TPX ZX f = hf, μpχ Zχ >H∙ The reproducing property of H implies
μpχ y (P,y) = hk((P,y), ∙),μpχ Y iH = EPX ,y k((P,y), (PX,γ))
for all (p, y) ∈P×Y, and similarly μpχZX (p,y) = EPX,Zχ k((p,y), (PX,Zχ)).	口
Lemma B.2. The squared kernel calibration error (SKCE) with respect to kernel k, defined as
SKCEk := KCE2k, is given by
SKCEk = EPχ ,Y,Pχ0,Y0 k((PX ,y ), (PXO ,y 0)) - 2 EPχ ,Y,Pχ0,Zχ0 k((PX,Y), (PXO ,ZX0))
+ EPχ,Zχ,Pχo,Zχo k((PX,ZX), (PXO,ZX0)),
where (PXO, Y0, ZXO) is independently distributed according to the law of(PX, Y, ZX)
Proof. From Lemma B.1 we know that there exist kernel mean embeddings μPχ Y,μPχZX ∈ H that
satisfy
hf,μPχy - μPχZXiH = hf,μPχYiH - hf,μPχZXiH
=EPχ,Yf(PX,Y)-EPχ,Zχf(PX,ZX)
for all f ∈ H. Hence by the definition of the dual norm
CEFk = sup EPχ,Yf(PX,Y) -EPχ,Zχf(PX,ZX)
f∈Fk
=SUP I hf, μPχ,Y - μPχ ,Zχ iH I = ∣∣μPχ,Y - μPχ ,Zχ Il H,
f∈Fk
which implies
SKCEk = hμPχ Y - μPχ Zχ , μPχ Y - μPχ Zχ i H.
From Lemma B.1 we obtain
SKCEk= EPχ ,Y,Pχo ,Y0 k((PX ,Y ), (PX0 ,Y 0)) 一 2 EPχ ,Y,Pχo ,Zχo k((PX ,γ), (PXO, ZX))
+ ePx,Zχ,Pχ0,Zχ M(PX, ZX), (PX0ZX)),
which yields the desired result.	口
Recall that (PX1 , Y1), . . . , (PXn, Yn) is a validation data set that is sampled i.i.d. according to the
law of (PX, Y) and that for all (P, y), (P0, y0) ∈ P × Y
h((P,y), (P0, y0)) := k((P,y), (P0, y0)) - EZ~p k((p, Z), (p0,y0))
—EZ0~p0 k((P, y), (p0, Z0)) + EZ~p,Z0~p0 k((P, Z), (P0, Z0)).
Lemma B.3. For all i, j = 1, . . . , n,
∣h((PXi,匕),(PXj,Y∙))1 < ∞
almost surely.
20
Proof. Let i, j ∈ {1, . . . , n}. By assumption (K2) we know that
Ik((PXi,Yi), (Pχj,Yj))∣ ≤ k1/2((PXi,Yi),(Pχi,匕))k14(Pχj,Yj),(Pχj,Yj)) < ∞
almost surely. Moreover,
I EZXi k((PXi ,ZxJ,(PXj ,Yj ))∣ ≤ EZXilk((PXi ,ZXi), (PXj,Yj ))∣
≤ EZXi (k1/2((PXi，ZXi)，(PXi,ZXi))k1/2((PXj,Yj), (PXj,Yj))) < ∞
almost surely, and similarly I EZX ,ZX k (PXi, ZXi), (PXj , ZXj ) I < ∞ almost surely. Thus
∣h((PXi ,Yi), (Px3 ,Yj- ))∣ ≤ ∣k((PXi ,Yi), (PXj,Yj ))∣ + ∣ EZXi k((Pχ NXM (PXj ,Yj- ))∣
+ ∣ EZXj k((PXi ,Yi), (PXj,ZXj))I + ∣ EZXi ,ZXj k((PXi ,ZXi ), (PXj ,ZXj ))∣ < ∞
almost surely.	□
Lemma 1. The plug-in estimator of SKCEk is non-negatively biased. It is given by
1n
SKCEk = n E h((PXi,Yi),(PXj,Yj)).
n i,j =1
Proof. From Lemma B.2 we know that KCEk < ∞, and Lemma B.3 implies that SKCEk < ∞
almost surely.
For i = 1, . . . , n, the linear operators Tif := EZX f (PXi , ZXi ) for f ∈ H are bounded almost
surely since
|Tif| = ∣ EZXif(PXi,ZXi)∣ ≤ EZXiIf(PXi,ZXi)∣ = EZXiIhk((PXi,ZQ ∙),/川
≤ EZXi (Ilk((PXi,ZXi), ∙) IlHkfkH) = kf kH EZXik1/2 ((PXi, ZXi), (PXi, ZXi)).
Hence Riesz representation theorem implies that there exist ρi ∈ H such that Tif = hf, ρiiH
almost surely. From the reproducing property of H we deduce that ρi(p,y) = hk ((p,y), ∙) ,ρi〉h =
EZXik((p, y), (PXi ,ZXi)) for all (p, y) ∈P×Y almost surely.
Thus by the definition of the dual norm the plug-in estimator KCEk satisfies
n
KCEk = sup
f∈Fk
1∣ n
n ∑(f(PXi,Yi) - EZXif(PXi,ZXi))
sup
f∈Fk
i=1
1 ∣∣ n
T £〈k((PXi ,Yi), ∙) — ρi,f>H
n
sup -
f∈Fk n
i=1
(X (k((PXi ,匕), ∙) — ρi),f)
n
i=1
n
-ρi IIH
n
Ek((PXi, Yi), ∙) — ρi,Ek((PXi, Yi), ∙) —
i=1	i=1
P)H)
1 n	1/2	1/2
- Eh((PXi,Yi),(PXj,Yj)) =SKCEk <∞
i,j=1
1/2
almost surely, and hence indeed SKCEk is the plug-in estimator ofKCEk.
21
Since (PX, Y ), (PX0, Y 0), (PX1 , Y1), . . . , (PXn, Yn) are identically distributed and pairwise inde-
pendent, we obtain
n
n2 ESKCEk = X EPXi,Yi,PXj,Yj h((PXi,Yi), (PXj,Y))
i,j=1,
i6=j
n
+ XEPXi,Yi M(PXi,Yi), (PXi,Yi))
i=1
=n(n — 1) EPX ,Y,Pχo,Y，M(PX ,Y),(改,Y 0)) + n EPX ,y M(PX ,Y), (PX ,Y))
=n(n — 1)SKCEk + n EPX ,γ M(PX, Y), (PX ,Y)).
(B.1)
With the same reasoning as above, there exist ρ, ρ0 ∈ H such that for all f ∈ H EZX f(PX , ZX ) =
hf, ρiH andEZX0f(PX0,ZX0) = hf, ρ0iH almost surely. Thus we obtain
h((PX,y), (PXO,y0)) = hk((PX,Y), ∙) — ρ,k((PX0,y0), ∙) — ρ0iH
almost surely, and therefore by Lemma B.2 and the Cauchy-Schwarz inequality
SKCEk = EPX ,Y,Pxo,Y0 h((PX,Y ), (PX 0,Y 0))
=EPX,Y,Pxo,Y0 hk((PX,Y), ∙) — P, k((G0,γ0), ∙) — P0iH
≤ EPX,Y,Pxo,Y0 ∣hk((PX,Y), ∙) — P,k((PX0,Y0), •) — P0iH∣
≤ EPx,Y,Pxo,Y0 j∣k((PX,Y), ∙) — PIlHIlk((PX0,Y0), •) — ρ0∣∣H
≤ EP2,Y Ilk((PX,γ ),∙) - P∣∣H EPX0,Y 0Ik((PX0,γ 0), ∙) - P0IIH.
Since (PX , Y) and (PX0 , Y0) are identically distributed, we obtain
SKCEk ≤ Epx ,y IIk((PX ,γ), ∙) — pIIH = Epx ,y h((PX ,Y), (PX ,Y)).
Thus together with Eq. (B.1) we get
n2 E S\KCEk ≥ n(n — 1)SKCEk + nSKCEk = n2SKCEk,
and hence SKCEk has a non-negative bias.	□
Lemma 2. The block estimator of SKCEk with block size B ∈ {2, . . . , n}, given by
-1 bn/B c B -1
SKCEk,B := B X 2	X	h((PXi,Yi),(PXj,Y∙)),
b=1	(b-1)B<i<j≤bB
is an unbiased estimator of SKCEk.
Proof. From Lemma B.2 we know that SKCEk < ∞, and Lemma B.3 implies that SKCEk,B < ∞
almost surely.
For b ∈ {1, . . . , bn/B c}, let
bb := (B) 1 X	h((PXi,Yi),(PXj,Y))
(b-1)B<i<j≤bB
(B.2)
be the estimator of the bth block. From Lemma B.3 it follows that ηbb < ∞ almost surely for all b.
Moreover, for all b, ηbb is a so-called U-statistic of SKCEk and hence satisfies Eηbb = SKCEk (see,
e.g., van der Vaart, 1998). Since (PX1 , Y1), . . . , (PXn, Yn) are pairwise independent, this implies
that SKCEk,B is an unbiased estimator of SKCEk.	□
22
B.4 Calibration tests
LemmaB.4. Let B ∈ {2,...,n}. If VPXYPXOY 0 h((Pχ, Y), (Pχo ,Y0)) < ∞, then for all
b∈{1,...,bn/Bc}
Vbb = σB ：= (B)	(2(B - 2)Zι + VPX,γ,Pχ,,γ, M(PX, Y), (Pχ,,Y0))),
where ηbb is defined according to Eq. (B.2) and
Zi := EPX ,γ EpXO ,γ 0 h((Pχ ,Y), (Pχo ,Y0)) - SKCEk ∙	(B∙3)
If model P is calibrated, it simplifies to
Proof. Let b ∈ {1,..., [n∕B]}. Since VPX,y,Px0,γ0 h((Pχ,Y), (Pχo, Y0)) < ∞, the Cauchy-
Schwarz inequality implies V ηbb < ∞ as well.
As mentioned in the proof of Lemma 2 above, ηbb is a U-statistic of SKCEk. From the general formula
of the variance of a U-StatistiC (see, e.g., Hoeffding,1948, p. 298-299) We obtain
V bb =(B Γ(G(B -12 b 1 + (2)(B - ；) VPX ZPxoY WPX ,Y ),(PX0, YO)))
=(B)	(2(B - 2)Zi + Vpx ,γ,Pχo ,Y 0 h((Pχ ,Y), (Pχo ,Y0))),
Where
Zi = Epx ,Y Epxo,γo M(PX ,Y), (Pχo ,Y0)) - SKCEk.
If model P is calibrated, then (PX, Y) =d (PX, Z), and hence for all (p, y) ∈ P × Y
Epx ,Y h((p,y), (Pχ ,Y)) = Epx ,Y k((p,y),(Pχ ,Y)) - Ez，〜P EPX ,γ k((p, Z0), (Pχ ,Y))
-ePx ,Z k((p,y), (PX ,Z)) + EZO 〜P ePx ,Z k((P, Z0), (PX ,y ))
= 0.
This implies Zi = EPX,γ EPX0 γ0 ”(Px, Y), (Pxo,Y0)) = 0 and SKCEk = 0 due to Lemma B.2.
Thus	X ,
as stated above.	□
Corollary B.1. Let B ∈ {2,...,n}. If VPX ,γ,Pχo,γ，h((Pχ, Y), (Pχo ,Y0)) < ∞ ,then
VS\KCEk,B = bn∕Bc-iσB2.
where σB2 is defined according to Lemma B.4.
Proof. Since the estimators bi,..., b〔n/Bj in each block are pairwise independent, this is an imme-
diate consequence of Lemma B.4.	□
Corollary B.2. Let B ∈ {2,...,n}. If VPX,γ,Pχo,γ，h((Pχ, Y), (Pχo, Y0)) < ∞, then
Pbn/BC (SKCEfcjB - SKCEk) → N (0,σB)	as n → ∞,
where block size B is fixed and σB2 is defined according to Lemma B.4.
Proof. The result follows from Lemma 2, Lemma B.4, and the central limit theorem (see, e.g.,
Serfling, 1980, Theorem A in Section 1.9).	□
23
Remark B.1. Corollary B.2 shows that SKCEk,B is a consistent estimator of SKCEk in the large
sample limit as n → ∞ with fixed number B of samples per block. In particular, for the linear
estimator with B = 2 we obtain
P[n∕2j (SKCEk,2 - SKCEk) → N(0, σ∣)	as n →∞.
Moreover, Lemma B.4 and Corollary B.2 show that the p-value of the null hypothesis that model P
is calibrated can be estimated by
Φ-
r.-, _ . -C7^:**~^ -
√bn/BcSKCEk,B
σbB
where Φ is the cumulative distribution function of the standard normal distribution and σbB is the
empirical standard deviation of the block estimates bι,... ,bbn/Bc，and
Φ-
/[n/BCB(B - 1)SKCEk,B
√2b	F
Y0 h2 (PX, Y), (PX0, Y0) . Similar p-value approximations
where σb2 is an estimate of EPX,Y,PX0 ,
for the two-sample test with blocks of fixed size were used by Chwialkowski et al. (2015).
Corollary B.3. AssumeNPX ,γ,Pχo,γ0 h((Pχ ,Y), (Pχ0 ,Y0)) < ∞ ∙ Let S ∈ {1,∙∙∙, [n/2]}. Then
for all b ∈ {1, . . . , s}
√B(bb - SKCEk) → N(0,4Zι)	as B → ∞,
(B.4)
where ηbb is defined according to Eq. (B.2) with n = Bs, the number s of equally-sized blocks is fixed,
and ζ1 is defined according to Eq. (B.3).
Ifmodel P is calibrated, then BB(rj^ — SKCEk) = JBm is asymptotically tight since Zi = 0, and
∞
Bηbb-→ X λi (Zi-	1) as B → ∞,
(B.5)
i=1
where Zi are independent χ12 distributed random variables and λi ∈ R are eigenvalues of the
Hilbert-Schmidt integral operator
Kf(p,y) := Epx,Y (h((p,y),(PX,YXf(PX,Y))
for Borel-measurable functions f : P × Y → R with EPX ,Yf2(PX,Y) < ∞.
Proof. Let s ∈ {1, . . . , bn/2c} and b ∈ {1, . . . , s}. As mentioned above in the proof of Lemma 2,
the estimator ηbb, defined according to Eq. (B.2), is a so-called U-statistic of SKCEk (see, e.g., van der
Vaart, 1998). Thus Eq. (B.4) follows from the asymptotic behaviour of U-statistics (see, e.g., van der
Vaart, 1998, Theorem 12.3).
IfP is calibrated, then we know from the proof of Lemma B.4 that ζ1 = 0, and hence ηbb is a so-called
degenerate- U-statistic (see, e.g., van der Vaart, 1998, Section 12.3). From the theory of degenerate
U-statistics it follows that the sequence Bηbb converges in distribution to the limit distribution in
Eq. (B.5), which is known as Gaussian chaos.	口
Corollary B.4. Assume VPX,γ,Pχo,γo h((Pχ, Y), (Pχo,Y0)) < ∞. Let S ∈ {1,..., [n∕2". Then
√B(SKCEk,B - SKCEk) → N(0,4s-1Zι)	as B → ∞,
where the number S of equally-sized blocks is fixed, n = BS, and ζ1 is defined according to Eq. (B.3).
Ifmodel P is Calibrated, then √B(SKCEk,B - SKCEk) = √BSKCEk,B is asymptotically tight
since ζ1 = 0, and
∞
BS\KCEk,B -→d s-1 Xλi(Zi - s) asB → ∞,
i=1
where Zi are independent χs2 distributed random variables and λi ∈ R are eigenvalues of the
Hilbert-Schmidt integral operator
Kf(p,y) := Epx,Y (h((p,y),(Pχ,YAf(PX,Y))
for Borel-measurable functions f : P × Y → R with EPX ,Yf2(PX,Y) < ∞.
24
Proof. Since the estimators ηb1, . . . , ηbs in each block are pairwise independent, this is an immediate
consequence of Corollary B.3.	□
Remark B.2. Corollary B.4 shows that SKCEk,B is a consistent estimator of SKCEk in the large
sample limit as B → ∞ with fixed number bn/Bc of blocks. Moreover, for the minimum variance
unbiased estimator with B = n, Corollary B.4 shows that under the null hypothesis that model P is
calibrated
∞
nS\KCEk,n -→d Xλi(Zi - 1) asn → ∞,
i=1
where Zi are independent χ12 distributed random variables. Unfortunately quantiles of the limit
distribution of Pi∞=1 λi (Zi - 1) (and hence the p-value of the null hypothesis that model P is
calibrated) can not be computed analytically but have to be estimated by, e.g., bootstrapping (Arcones
& Gin6, 1992), using a Gram matrix spectrum (Gretton et al., 2009), fitting Pearson curves (Gretton
et al., 2007), or using a Gamma approximation (Johnson et al., 1994, p. 343, p. 359).
Corollary B.5. AssumeVPX ,γ,Pχo,γ0 h((Pχ, Y), (Pχ0 ,Y0)) < ∞ .Then
Pbn/BCB(SKCEfc,B - SKCEk) → N(0, 4Zι)	as B → ∞ and bn/Bc → ∞,	(B.6)
where B is the block size and s is the number of equally-sized blocks, n = Bs, and ζ1 is defined
according to Eq. (B.3).
Ifmodel P is calibrated, then PbnTBIB(SKCEk,b — SKCEk) = PbnTBBSKCEk,B is asymp-
totically tight since ζ1 = 0, and
Pbn7B7BSKCEfc,B → N(0, XX λi2	as B → ∞ and bn/Bc → ∞,
i=1
where λi ∈ R are eigenvalues of the Hilbert-Schmidt integral operator
Kf (p, y) := EPX ,Y (h((p, y), (PX ,Y ))f(Pχ ,Y))
for Borel-measurable functions f : P × Y → R with EPX ,Y f2(PX,Y) < ∞.
Proof. The result follows from Corollary B.3 and the central limit theorem (see, e.g., Serfling, 1980,
Theorem A in Section 1.9).	□
Remark B.3. Corollary B.5 shows that SKCEk,B is a consistent estimator of SKCEk in the large
sample limit as B → ∞ and bn7Bc → ∞, i.e., as both the number of samples per block and the
number of blocks go to infinity. Moreover, Corollaries B.3 and B.5 show that the p-value of the null
hypothesis that P is calibrated can be estimated by
Φ
—
r.-, _ . -c7^:**~^ -
√WBTSKCEk,B
σbB
where bβ is the empirical standard deviation of the block estimates bi,..., b∖j∕B∖ ∙ Similar p-value
approximations for the two-sample problem with blocks of increasing size were proposed and applied
by Zaremba et al. (2013).
C Calibration mean embedding
C.1 Definition
Similar to the unnormalized mean embedding (UME) proposed by Chwialkowski et al. (2015) in
the standard MMD setting, instead of the calibration error CEFk = kμpχ Y — μpχZX IlH we can
consider the unnormalized calibration mean embedding (UCME).
25
Definition C.1. Let J ∈ N. The unnormalized calibration mean embedding (UCME) for kernel k
with J test locations is defined as the random variable
J
UCMEk,j = JT X (μpχY(Tj) - μpχZX (Tj))2
j=1
J
=JT X (EPX,Y k(Tj,(Px, Y))- EPX,Zχ k(T, (Pχ, ZX)))2,
j=1
where T1, . . . , TJ are i.i.d. random variables (so-called test locations) whose distribution is absolutely
continuous with respect to the Lebesgue measure on P × Y .
As mentioned above, in many machine learning applications we actually have P × Y ⊂ Rd (up to
some isomorphism). In such a case, if k is an analytic, integrable, characteristic kernel, then for each
J ∈ N UCMEk,J is a random metric between the distributions of (PX, Y) and (PX, ZX), as shown
by Chwialkowski et al. (2015, Theorem 2). In particular, this implies that UCMEk,J = 0 almost
surely if and only if the two distributions are equal.
C.2 Estimation
Again we assume (PX1, Y1), . . . , (PXn, Yn) is a validation data set of predictions and targets, which
are i.i.d. according to the law of (PX, Y). The consistent, but biased, plug-in estimator of UCME2k,J
is given by
2
U\CMEk,J
J
J-1X
j=1
i=1
2
C.3 Calibration mean embedding test
As Chwialkowski et al. (2015) note, if model P is calibrated, for every fixed sequence of unique
2	2
test locations √nUCMEfc,j converges m distribution to a sum of correlated χ2 random variables,
as n → ∞. The estimation of this asymptotic distribution, and its quantiles required for hypothesis
testing, requires a bootstrap or permutation procedure, which is computationally expensive. Hence
Chwialkowski et al. (2015) proposed the following test based on Hotelling’s T2 -statistic (Hotelling,
1931).
For i = 1, . . . , n, let
/k(Tι, (PXi,匕))-EZXi k(Tι, (PXi,ZχJ)∖
Zi :=	.	I ∈ RJ,
U(TJ, (PXi ,Yi)) - EZXi k(Tj, (PXi, ZXiA)
and denote the empirical mean and covariance matrix of Zι,...,Zn by Z and S, respectively. If
UCMEk,J is a random metric between the distributions of (PX, Y) and (PX, ZX), then the test
statistic
Qn := nZTSTZ
is almost surely asymptotically χ2 distributed with J degrees of freedom if model P is calibrated,
as n → ∞ with J fixed; moreover, if model P is uncalibrated, then for any fixed r ∈ R almost
surely P(Qn > r) → 1 as n → ∞ (Chwialkowski et al., 2015, Proposition 2). We call the resulting
calibration test calibration mean embedding (CME) test.
D Kernel choice
A natural choice for the kernel k : (P × Y) × (P × Y) → R on the product space of predicted
distributions P and targets Y is a tensor product kernel of the form k = kp 0 kγ, i.e., a kernel of the
form
k (p, y), (p0, y0) = kP (p, p0)kY (y, y0),
26
where kP : P × P → R and kY : Y × Y → R are kernels on the spaces of predicted distributions
and targets, respectively.
As discussed in Section 3.1, if kernel k is characteristic, then the kernel calibration error KCEk of
model P is zero if and only if P is calibrated. Unfortunately, as shown by Szab6 & SriPerUmbUdUr
(2018, Example 1), even if kp and kγ are characteristic, the tensor product kernel k = kp 0 kγ might
not be characteristic. However, when analyzing calibration, it is sUfficient to be able to distingUish
distribUtions for which the conditional distribUtions P(Y |PX) and P(ZX |PX) = PX are not eqUal
almost sUrely. ThUs it is sUfficient if kY is characteristic and kP is non-zero almost sUrely.
Many common kernels sUch as the GaUssian and Laplacian kernel on Rd are characteristic and can
therefore be chosen as kernel kY for real-valUed target spaces. The choice of kP might be less
obvioUs since P is a space of probability distribUtions. IntUitively one might want to Use kernels of
the form
kp(p,p') = exp ( — λdp(p,p0)),	(D.1)
where dP : P × P → R is a metric on P and ν, λ > 0 are kernel hyperparameters. Kernels of this
form woUld be a generalization of the GaUssian and Laplacian kernel, and woUld clearly be non-zero
almost sUrely.
UnfortUnately, this constrUction does not necessarily yield valid kernels. Most prominently, the
Wasserstein distance does not lead to valid kernels kp in general (Peyre & Cuturi, 2019, Chapter 8.3).
However, if dp (∙, ∙) is a Hilbertian metric, i.e., a metric of the form
dp(p,p0) = φ(p) - φ(p0)H
for some Hilbert space H and mapping φ: P → H, then kp in Eq. (D.1) is a valid kernel for all
λ > 0 and ν ∈ (0, 2] (Berg et al., 1984, Corollary 3.3.3, Proposition 3.2.7).
D.1 Normal distributions
Assume that Y = Rd and P = {N(μ, Σ): μ ∈ Rd, Σ ∈ Rd×d psd}, i.e., the model outputs normal
distributions PX = N(μχ, Σχ). The distribution of these outputs is defined by the distribution of
their mean μχ and covariance matrix Σχ.
Let Px = N(μχ, Σχ) ∈ P, y ∈ Y = Rd, and γ > 0. We obtain
EZx 〜Px eχp (— YkZx — yk2)
= IId + 2yςxI	e eχp ( - γ(μx - y)T(Id + 2yςx)	(μx - y))
from Mathai & Provost (1992, Theorem 3.2.a.3). In particular, if Σx = diag(Σx,1, . . . , Σx,d), then
EZx 〜Px eχp (— YkZx — yk2)
d
=Y (1 + 2Yςx,, e eχp (- Y(1 + 2Y^,i)	(μx,i - yi)).
i=1
Let Pxo = N(μxo, Σxo) be another normal distribution. Then We have
EZx〜Px,zxo〜Pxo eχp ( 一 YIlZx - Zx0k2)
= lId	+ 2yςx I	/	EZxO〜Pxo	eχp ( - γ(μx	-	ZxO)	(Id +	2yςx)	(μx	- ZxO))
=∣Id + 2γNx + ςXO)I / eχp (― γ(μx - μxo) (Id + 2γNx + 夕XO))	(μx - μxo)).
Thus if Σx = diag(Σx,ι,..., Σx,d) and ∑ = diag(∑xO,ι,..., ∑xO,d), then
EZx〜Px,ZxO〜PxO eχp ( - YkZx - ZxOk2)
d
=Y (1 + 2Y^x,i + ςxo,J)	/ eχp ( - Y(1 + 2Y(%,i + ςxo,J)	(μx,i - μx-i)).
i=1
27
Hence we see that a Gaussian kernel
kY (y,y0) =eχp (- Yky - y0k2)
with inverse length scale γ > 0 on the space of targets Y = Rd allows us to compute
Ezx~px kγ(Zχ, y) and EZxZPxZxO~pχ0 kγ(Zχ, Zχo) analytically. Moreover, the Gaussian kernel is
characteristic on Rd (Fukumizu et al., 2008). Hence, as discussed above, by choosing a kernel kP
that is non-zero almost surely we can guarantee that KCEk = 0 if and only if model P is calibrated.
On the space of normal distributions, the 2-Wasserstein distance with respect to the Euclidean distance
between Px = N(μχ, Σχ) and Pχ0 = N(μχ0, Σχθ) is given by
W2(Pχ,Pχ0) = kμx — μx0 k2 +Tr 卜工 + ∑xo — 2(£x，1/2£x£x，1/2)”),
which can be simplified to
w2(Px,Px0) = ∖∖μx -μx0∣∣2 + kx/2 — ∑x/2∣Lb,
if ΣxΣxo = Σxo Σx. This shows that the 2-Wasserstein distance is a Hilbertian metric on the space of
normal distributions. Hence as discussed above, the choice
kp(Px,Pxo) =exp ( — λWS(Px,Pxo))
yields a valid kernel for all λ > 0 and ν ∈ (0, 2].
Thus for all λ, γ > 0 and ν ∈ (0, 2]
k((p, y), (p', y0)) = exp ( — λWν(p,p0)) exp ( — Yky — y'k2)
is a valid kernel on the product space P × Y of normal distributions on Rd and Rd that allows to
evaluate h (p, y), (p0, y0) analytically and guarantees that KCEk = 0 if and only if model P is
calibrated.
D.2 Laplace distributions
Assume that Y = R and P = {L(μ, β) : μ ∈ R, β > 0}, i.e., the model outputs Laplace distributions
PX = L(μX , βX ) with probability density function
PX Iy) = 2β^eχp (— βX1ly — μχ |)
for y ∈ Y = R. The distribution of these outputs is defined by the distribution of their mean μX and
scale parameter βX .
Let Px =L(μx,βx) ∈P,y ∈Y=R,andY > 0.Ifβx 6= Y-1, we have
EZxZPxx eχp( — YIZx - y|)
=(βx Y2 — 1)-1(βxγ eχp (— β-1lμx — y|) — eχp (— Y |〃x — y|))
Additionally, ifβx = Y-1, the dominated convergence theorem implies
EzxZPx eχp (— YIZx - y|)
=lim, (βxY2 —1)-1 (βxyeχp(— β-1lμx — y|) — eχp(— Y|〃x — y|))
γ→βx-1
=2 (1 + Y|〃x — y|) eχp (— y |〃x — y|).
Let Px，= L(μxo, ex，)be another Laplace distribution. If βx = Y-1, βχ = Y-1, and βx = ex，，we
obtain
EZxZPx,Zx，ZPx， eχp — YIZx — Zx， I
________γβx_________
(βxγ2 - I)(e2 - ex，2)
eχp (— β-1lμx — μx' |)
+
________γβ3________
(ex，y 2 - i)(ex，2-e2)
eχp (— ex， 1μx — μx0|)
+
1
(exγ 2-i)(ex，2γ2 -i)
eχp( — γ∣μx — μx0 |).
28
As above, all other possible cases can be deduced by applying the dominated convergence theorem.
More concretely,
•	if βx = βx0 = γ-1, then
EZx〜Px,Zx0ZPxO eχp ( - Y|Zx - Zx0 |)
=8 (3 + 3Y|〃x - μχo| + γ2Wx - μx0『)eχp (- γWχ - μx0
•	if βx = βx0 and βx 6= γ-1, then
EZx〜Px,Zx0〜PxO exp (- Y|Zx - Zx0|) = " 21 八2 exp ( - Y|Mx - μχ0
(βx2γ2 - 1)
(γ(βχ + lμχ - μχo D
+ I	2(β2Y2 - 1)
βxγ	)
(βXγ2 -1)2)
exp ( - β- 1lμχ - μχ0 । ),
—
•	if βx 6= βx0 and βx = γ-1, then
βx03γ3	-1
EZx〜Px,Ζx0〜PxO exp ( - γlZχ - Zx0 D =(尸 2 2----2eχ exp ( - βx0 lμx - μx0 |
(1 + γWx - μxo|
V 2(βx02Y2 - 1)
W) eχp(-γlμx-"x01),
+
•	and if βx 6= βx0 and βx0 = Y-1 , then
EZxZPx,Zx0ZPx0 exp - Y|Zx - Zx0 |	=
βx3Y3
(βx2Y2 - 1)
2 exp ( - β- 1lμx - μx0 |
—
1 + Y|〃x	- μxo|	l β2γ2 ʌ / I
2(βxY2	- 1)	+ (βxγ2 -	1)2 ) exp(	- γlμx	- μx0
The calculations above show that by choosing a Laplacian kernel
kγ (y,y0) = exp (- γ |y - y0l)
with inverse length scale Y > 0 on the space of targets Y = R, we can compute EZxZPx kY (Zx, y)
and EZxZPx,Zx0ZPx0 kY(Zx, Zx0) analytically. Additionally, the Laplacian kernel is characteristic on
R (Fukumizu et al., 2008).
Since the Laplace distribution is an elliptically contoured distribution, we know from Gelbrich
(1990, Corollary 2) that the 2-Wasserstein distance with respect to the Euclidean distance between
PX = L(μx, βx) and Px，= L(μx0,万方，)can be computed in closed form and is given by
W2 (Px,Px0) = (μx - μxO)2 + 2(βx - βx0)2.
Thus we see that the 2-Wasserstein distance is also a Hilbertian metric on the space of Laplace
distributions, and hence
kp(Px,Px0) =exp ( - λWS(Px,Pxθ))
is a valid kernel for 0 < ν ≤ 2 and all λ > 0.
Therefore, as discussed above, for all λ, Y > 0 and ν ∈ (0, 2]
k((p, y), (p, y0)) = exp ( - λW2(p,p0)) exp ( - γ∣y - y0∣)
is a valid kernel on the product space P × Y of Laplace distributions and R that allows to evaluate
h (p, y), (p0, y0) analytically and guarantees that KCEk = 0 if and only if model P is calibrated.
29
D.3 Predicting mixtures of distributions
Assume that the model predicts mixture distributions, possibly with different numbers of components.
A special case of this setting are ensembles of models, in which each ensemble member predicts a
component of the mixture model.
Let p,p0 ∈ P with p = Pi πipi and p0 = Pj πj0 p0j, where π, π0 are histograms and pi,p0j are the
mixture components. For kernel kY and y ∈ Y we obtain
Ez~p kγ (Z, y) = Eni EW~pi kγ (Z, y)
i
and
Ez~p,zo~p0 kγ(Z, Z0) = Eninj Ez~pi,zo~pj kγ(Z,Z0).
i,j
Of course, for these derivations to be meaningful, we require that they do not depend on the choice of
histograms n, n0 and mixture components pi, p0j .
Definition D.1 (see Yakowitz & Spragins (1968)). A family P of finite mixture models is called
identifiable if two mixtures p = PiK=1 nipi ∈ P and p0 = PjK=1 nj0 p0j ∈ P, written such that all pi
and all p0j are pairwise distinct, are equal if and only if K = K0 and the indices can be reordered
such that for all k ∈ {1, . . . , K} there exists some k0 ∈ {1, . . . , K} with nk = nk0 0 and pk = p0k0.
Clearly, if P is identifiable, then the derivations above do not depend on the choice of histograms
and mixture components. Prominent examples of identifiable mixture models are Gaussian mixture
models and mixture models of families of products of exponential distributions (Yakowitz & Spragins,
1968).
Moreover, similar to optimal transport for Gaussian mixture models by Chen et al. (2019; 2020);
Delon & Desolneux (2020), we can consider metrics of the form
w∈Πin(πf,π0) Xwi,jcs(pi,p0j)1/s,
where
Π(n, n0) = w :	wi,j = nj0 ∧	wi,j = ni ∧ ∀i, j : wi,j ≥ 0
ij
are the couplings of n and n0, and c(∙, ∙) is a cost function between the components of the mixture
model.
Theorem D.1. Let P be a family of finite mixture models that is identifiable in the sense of Defini-
tion D.1, and let s ∈ [1, ∞).
If d(∙, ∙) is a (Hilbertian) metric on the space of mixture components, then the Mixture Wasserstein
distance of order s defined by
1/s
MWs (p, p0) :=	inf	wi,j ds (pi, p0j )	,	(D.2)
w∈Π(π,π )	i,j
is a (Hilbertian) metric on P .
Proof. First of all, note that for all p,p0 ∈ P an optimal coupling W exists (Villani, 2009, Theo-
rem 4.1). Moreover, Pij Wijds(pi,pj) ≥ 0, and hence MWs(p,p0) exists. Moreover, since P
is identifiable, we see that MWs (p, p0) does not depend on the choice of histograms and mixture
components. Thus MWs is well-defined.
Clearly, for all p, p0 ∈ P we have MWs(p, p0) ≥ 0 and MWs(p, p0) = MWs(p0,p). Moreover,
MWss(p, p) =	min	Wi,jds(pi,pj) ≤	niδi,jds(pi,pj)
w∈Π(π,π)
i,j	i,j
=	nids(pi,pi) =	ni02 = 0,
ii
30
and hence MWs (p, P) = 0. On the other hand, let p,p0 ∈ P with optimal coupling W with respect to
π and π0, and assume that MWs (p, p0) = 0. We have
p=	πipi = Σ Wi,jPi = Σ	Wi,jPi.
i	i,j	i,j : Wi,j >0
Since MWs(p,p0) = 0, we have Wi,j ds(pi,pj)	=	0 for all i,j, and hence	ds(pi,pj)	= 0 if Wij	> 0.
Since d is a metric, this implies Pi = Pj if Wij	>	0. Thus we get
p = E	WijPi = E	WijPj = EWijPj = EnjPj = p0.
i,j ： Wij >0	i,j ： Wij >0	i,j	j
Function MWs also satisfies the triangle inequality, following a similar argument as Chen et al.
(2019). Let P(1) , P(2) , P(3) ∈ P and denote the optimal coupling with respect to π(1) and π(2) by
W(12), and the optimal coupling with respect to ∏(2) and n(3) by W(23). Define w(13) by
Wi(,1k3) := X
j : πj(2) 6=0
(13)
Clearly Wi,k ≥ 0 for all i, k, and we see that
W(12)W(23)
X, „(13) X	X	Wij Wj,k
乙 Wi,k =Z^	1	(2)~
i	i j : πj(2) 6=0	πj
j:
W(12)w(23)
X X Wij Wj,k
乙乙	7r⑵
πj(2) 6=0 i	πj
X	πjj = X	W" ∏⑶-X	Wj(2k3
j : πj(2) 6=0	j	j : πj(2) 6=0	j : πj(2) =0
for all k. Since for all j, k, πj2) ≥ Wyk), we know that πj2) = 0 implies WTk) = 0 for all k. Thus
for all k
X(13)	(3)
Wi,k = π(3)
i
Similarly we obtain for all i
XWi(,1k3)=π(1).
k
31
Thus w(13) ∈ Π(π(1), π(3)), and therefore by exploiting the triangle inequality for metric d and the
Minkowski inequality we get
MWs(P⑴,p⑶)≤
W^dsipRp"ys=(X X
i,k j: πj(2) 6=0
(2)
πj
1/s
≤
1/s
s
≤
≤
1/s
Ξ Σ
i,k j : πj(2) 6=0
w(12)w(23)
X X	-⅜F(d(p(1),pj2)) + d(pj2),pk3)))
i,k j: πj(2) 6=0	πj
+
i,k j: π(2) 6=0
Wff)Wg
πj2)
12)
1/s
MWs(P⑴,p⑵)+MWs(p ⑵,p ⑶).
Thus MWs is a metric, and it is just left to show that it is Hilbertian if d is Hilbertian. Since d is a
Hilbertian metric, there exists a Hilbert space H and a mapping φ such that
d(x, y) = kφ(x) - φ(y)kH.
Let r1, . . . , rn ∈ R with Pi ri = 0 and P(1), . . . , P(n) ∈ P. Denote the optimal coupling with
respect to ∏(i) and ∏(j) by W(ij). Then We have
X TirjX w⅛)kφ(Pki))kH = X rikφ(pk'))kH X rj X Wfkj
i,j k,l	i,k	j l
= Xrikφ(P(ki))k2H X rj πk(i)	(D.3)
i,k	j
= Xriπk(i)kφ(P(ki))k2H Xrj = 0,
i,k	j
and similarly
X IrirjX Wkij)kφ(Pj) )kH =0.	(D.4)
i,j	k,l
Moreover, for all k, l We get
X rirjwkij) (φ (Pki)), φ (Pj))〉H =(X ri Jwki,lj)φ (Pki)), X Tj Wkj φ (Pj))〉H
i,j	i	j
=IlX riq<Φ(Pki))BH ≥0,
and hence
Xrirj X Wkij)<φ(Pki)),φ(Pj))% ≥0,	(D.5)
32
and similarly
X TiTjX w(⅛j<φ(Pj) ),φ(Pk)))H ≥ 0.	(Ds)
i,j	k,l
Hence from Eqs. (D.3) to (D.6) we get
X TiTj MWss(P⑴,Pj)) = X TiTj X WkjdS (Pki),P(j))
i,j	i,j	k,l
=X TiTjX w⅛[∣φ(Pki))- φ(p(j)) ∣∣H
i,j	k,l	H
=X TiTj X w⅛j)∣∣φ(Pki))∣∣H
i,j	k,l
—X TiTjX Klj Dφ (Pki)), φ (Pj))EH
i,j	k,l
—X TiTjX Klj Dφ (Pj)), φ (Pki))EH
i,j	k,l
+X TiTjX wki,j)∣∣φ(Pj))∣∣H
i,j	k,l
≤ 0,
which shows that MWss is a negative definite kernel (Berg et al., 1984, Definition 3.1.1). Since
0 < 1/s < ∞, MWs is a negative definite kernel as well (Berg et al., 1984, Corollary 3.2.10), which
implies that metric MWs is Hilbertian (Berg et al., 1984, Proposition 3.3.2).	□
Hence we can lift a Hilbertian metric for the mixture components to a Hilbertian metric for the mixture
models. For instance, if the mixture components are normal distributions, then the 2-Wasserstein
distance with respect to the Euclidean distance is a Hilbertian metric for the mixture components.
When we lift it to the space P of Gaussian mixture models we obtain the MW2 metric proposed by
Chen et al. (2019; 2020); Delon & Desolneux (2020). As shown by Delon & Desolneux (2020), the
discrete formulation of MW2 obtained by our construction is equivalent to the definition
MW22(P, P0) :
inf
γ∈Π(p,p0)∩GMM2n (∞)
/
Rn ×Rn
d2(y, y0) dγ(y, y0)
(D.7)
for two Gaussian mixtures P, P0 on Rn, where Π(P, P0) are the couplings of P and P0 (not of the
histograms!) and GMM2n(∞) = ∪k≥0GMM2n(k) is the set of all finite Gaussian mixture distribu-
tions on R2n . The construction of the discrete formulation as a solution to a constrained optimization
problem similar to Eq. (D.7) can be generalized to mixtures of t-distributions. However, it is not
possible for arbitrary mixture models such as mixtures of generalized Gaussian distributions, even
though they are elliptically contoured distributions (Deledalle et al., 2018; Delon & Desolneux, 2020).
The optimal coupling of the discrete histograms can be computed efficiently using techniques from
linear programming and optimal transport theory such as the network simplex algorithm and the
Sinkhorn algorithm. As discussed above, if metric dP is of the form in Eq. (D.2), functions of the
form
kp(P,P0) = exp ( — λdp (p,P)
are valid kernels on P for all λ > 0 and ν ∈ (0, 2].
Thus taken together, if kγ is a characteristic kernel on the target space Y and d(∙, ∙) is a Hilbertian
metric on the space of mixture components, then for all s ∈ [1, ∞), λ > 0, and ν ∈ (0, 2]
k((P,y), (P,y) = exp ( — λMW"P,P0))kγ(y,y0)
is a valid kernel on the product space P × Y of mixture distributions and targets that allows to evaluate
h (P, y), (P0, y0) analytically and guarantees that KCEk = 0 if and only if model P is calibrated.
33
E Classification as a special case
We show that the calibration error introduced in Definition 2 is a generalization of the calibration
error for classification proposed by Widmann et al. (2019). Their formulation of the calibration error
is based on a weighted sum of class-wise discrepancies between the left hand side and right hand side
of Definition 1, where the weights are output by a vector-valued function of the predictions. Hence
their framework can only be applied to finite target spaces, i.e., if |Y| < ∞.
Without loss of generality, we assume that Y = {1, . . . , d} for some d ∈ N\ {1}. In our notation, the
previously defined calibration error, denoted by CCE (classification calibration error), with respect
to a function space G ⊂ {f : P → Rd} is given by
CCEG = sup EPX fX( P(Y = y|PX) - Pχ ({y}))gy (PX ))∣∙
g∈G	y∈Y
For the function class
F := {f: P×Y→ R, (p,y)→ gy (P)Ig ∈ G}
we get
CCEG = supIIEPX,Yf(PX,Y)-EPX,ZXf(PX,ZX)II =CEF.
f∈F
Similarly, for every function class F ⊂ {f : P × Y → R}, we can define the space
G ：= {g： P→Rd,P→ (f(P,i),...,f(P,d))T∣f ∈ F0，
for which
CEF = SUp EPX ( X (P(Y = y∣Pχ) - PX({y}))gy(PX)) ∣ = CCEG.
g∈G	y∈Y
Thus both definitions are equivalent for classification models but the structure of the employed
function classes differs. The definition of CCE is based on vector-valued functions on the probability
simplex whereas the formulation presented in this paper uses real-valued function on the product
space of the probability simplex and the targets.
An interesting theoretical aspect of this difference is that in the case of KCE we consider real-valued
kernels on P × Y instead of matrix-valued kernels on P, as shown by the following comparison. By
ei ∈ Rd we denote the ith unit vector, and for a prediction P ∈ P its representation vp ∈ Rd in the
probability simplex is defined as
(Vp)y = p({y})
for all targets y ∈ Y .
Let k : (P × Y) × (P × Y) → R. We define the matrix-valued function K : P × P → Rd×d by
[K(p,p0)]y,yo = k((p,y), (p0,y0))
for all y, y0 ∈ Y and P, P0 ∈ P . From the positive definiteness of kernel k it follows that K is a
matrix-valued kernel (Micchelli & Pontil, 2005, Definition 2). We obtain
SKCEk = EPX,Y,PX0,Y 0 K(PX,PX0)Y,Y0 - 2 EPX,Y,PX0,ZX0 K(PX,PX0)Y,ZX0
+ EPX,ZX,PX0,ZX0 K(PX,PX0)ZX,ZX0
= EPX,Y,PX0,Y 0 eYT K(PX, PX0)eY0 - 2 EPX,Y,PX0,Y 0 eYT K(PX, PX0)vPX0
+ EPX,Y,PX0,Y 0 vPTXK(PX,PX0)vPX0
= EPX,Y,PX0,Y 0 (eY - vPX)TK(PX, PX0)(eY0 - vPX0 ),
which is exactly the result by Widmann et al. (2019) for matrix-valued kernels.
As a concrete example, Widmann et al. (2019) used a matrix-valued kernel of the form (P, P0) 7→
exp (-γkP - P0 k)Id in their experiments. In our formulation this corresponds to the real-valued
tensor product kernel ((p,y), (p0,y0)) → exp(-γIlP - p0k)δy,yo.
34
F Temperature scaling
Since many modern neural network models for classification have been demonstrated to be uncali-
brated (Guo et al., 2017), it is of high practical interest being able to improve calibration of predictive
models. Generally, one distinguishes between calibration techniques that are applied during training
and post-hoc calibration methods that try to calibrate an existing model after training.
Temperature scaling (Guo et al., 2017) is a simple calibration method for classification models with
only one scalar parameter. Due to its simplicity it can trade off calibration of different classes (Kull
et al., 2019), but conveniently it does not change the most-confident prediction and hence does not
affect the accuracy of classification models with respect to the 0-1 loss.
In regression, common post-hoc calibration methods are based on quantile binning and hence
insufficient for our framework. Song et al. (2019) proposed a calibration method for regression
models with real-valued targets, based on a special case of Definition 1. This calibration method was
shown to perform well empirically but is computationally expensive and requires users to choose
hyperparameters for a Gaussian process model and its variational inference. As a simpler alternative,
we generalize temperature scaling to arbitrary predictive models in the following way.
Definition F.1. Let Px be the output of a probabilistic predictive model P for feature x. If Px has
probability density function Px with respect to a reference measure μ, then temperature scaling with
respect to μ with temperature T > 0 yields a new output Qx whose probability density function qx
with respect to μ satisfies
qx Y px/T.
The notion for classification models given by Guo et al. (2017) can be recovered by choosing the
counting measure on the classes as reference measure.
For some exponential families on Rd we obtain particularly simple transformations with respect to
the Lebesgue measure λd that keep the type of predicted distribution and its mean invariant. Hence
in contrast to other calibration methods, for these models temperature scaling yields analytically
tractable distributions and does not negatively impact the accuracy of the models with respect to the
mean squared error and the mean absolute error.
For instance, temperature scaling of multivariate power exponential distributions (G6mez et al., 1998)
in Rd , of which multivariate normal distributions are a special case, with respect to λd corresponds to
multiplication of their scale parameter with T1/e, where β is the so-called kurtosis parameter (G6mez-
SgnCheZ-ManZanO et al., 2008). For normal distributions, this corresponds to multiplication of the
covariance matrix with T .
Similarly, temperature scaling of Beta and Dirichlet distributions with respect to reference measure
μ(dx) := x-1(1 — x)-11(0,1)(x)λ1(dx)
and
μ(dx) := (Y x-1) l(0,i)d(χ)λd(dχ),
respectively, corresponds to division of the canonical parameters of these distributions by T without
affecting the predicted mean value.
All in all, we see that temperature scaling for general predictive models preserves some of the nice
properties for classification models. For some exponential families such as normal distributions
reference measure μ can be chosen such that temperature scaling is a simple transformation of the
parameters of the predicted distributions (and hence leaves the considered model class invariant)
that does not affect accuracy of these models with respect to the mean squared error and the mean
absolute error.
G Expected calibration error for countably infinite discrete
TARGET SPACES
In literature, ECEd and MCEd are defined for binary and multi-class classification problems (Guo
et al., 2017; Naeini et al., 2015; Vaicenavicius et al., 2019). For common distance measures on the
35
probability simplex such as the total variation distance and the squared Euclidean distance, ECEd
and MCEd can be formulated as a calibration error in the framework of Widmann et al. (2019), which
is a special case of the framework proposed in this paper for binary and multi-class classification
problems.
In contrast to previous approaches, our framework handles countably infinite discrete target spaces
as well. For every problem with countably infinitely many targets, such as, e.g., Poisson regression,
there exists an equivalent regression problem on the set of natural numbers. Hence without loss
of generality we assume Y = N. Denote the space of probability distributions on N, the infinite
dimensional probability simplex, with ∆∞. Clearly, ∆∞ can be viewed as a subspace of the sequence
space `1 that consists of all sequences x = (xn)n∈N with xn ≥ 0 for all n ∈ N and kxk1 = 1.
Theorem G.1. Let 1 <p < ∞ with Holder conjugate q. If
F = {f: ∆∞ × N → R | EPX k(f(Pχ,n))n∈N∣P ≤ 1},
then
CEqF = EPX kP(Y|PX)-PXkqq.
Let μ be the law of PX. If F := {f: ∆∞ × N → R | EPX k(f(Pχ ,n))n∈N∣ι ≤ 1} ,then
CEF = μ-esssupsup |P(Y = y∣Pχ = ξ) - ξ({y})∣.
ξ∈∆∞ y∈N
Moreover, if F = {f: ∆∞ X N → R | μ - ess supξ∈∆∞ suPy∈N |f (ξ, y)| ≤ 1} ,then
CEF = EPX kP(Y|PX) -PXk1.
Proof. Let 1 ≤ P ≤ ∞, and let μ be the law of PX and V be the counting measure on N. Since both
μ and V are σ-finite measures, the product measure μ 0 V is uniquely determined and σ-finite as well.
Using these definitions, we can reformulate F as
F = {f ∈ Lp(∆∞ × N； μ 0 v) | kf |除“滉 ≤ 1}.
Define the function δ: ∆∞ × N → R (μ 0 V)-almost surely by
δ(ξ, y) := P(Y = y | PX = ξ) - ξ({y}).
Note that δ is well-defined since we assume that all singletons on ∆∞ are μ-measurable. Moreover,
δ ∈ Lq(∆∞ × N; μ 0 v), which follows from (ξ, y) → P(Y = y | Pχ = ξ) and (ξ, y) → ξ({y})
being functions in Lq(∆∞ × N; μ 0 V).
Since μ 0 V is a σ-finite measure, the extremal equality of Holder,s inequality implies that
CEF = sup EPX ,Y f(PX , Y) - EPX ,ZX f(PX, ZX )
f∈F
fs∈upFEPX,Yf(PX,Y)-EPX,ZXf(PX,ZX)
sup
f∈F
L	^f (ξ,y)δ(ξ,y) (μ 0 V)(d(ξ,y))
∣∣δkqWβ)ν .
Note that the second equality follows from the symmetry of the function spaces F: for every f ∈ F,
also -f ∈ F.
Hence for 1 < p ≤ ∞, we obtain
CEF=LS /'(',y""(μ0V)(d(ξ,y))
=EPX k(δ(PX,y))y∈Nkqq=EPX kP(Y|PX)-PXkqq.
For p = 1, we get
CEF = μ-esssupsup ∣δ(ξ,y)∣ = μ-esssupsup | P(Y = y∣Pχ = ξ) - ξ({y})∣,
ξ∈∆∞ y∈N	ξ∈∆∞ y∈N
which concludes the proof.
□
36
We see that our framework deals with countably infinite discrete target spaces seamlessly whereas
the previously proposed framework by Widmann et al. (2019) is not applicable to such spaces. It
is mathematically pleasing to see that for countably infinite discrete targets the calibration errors
obtained in Theorem G.1 within our framework coincide with the natural generalization of ECEd
and MCEd given in Appendix B.2.
37