Published as a conference paper at ICLR 2021
Provab le Rich Observation Reinforcement
Learning with Combinatorial Latent States
Dipendra Misra*	Qinghua Liu	Chi Jin	John Langford
Microsoft Research	Princeton University	Princeton University	Microsoft Research
Ab stract
We propose a novel setting for reinforcement learning that combines two com-
mon real-world difficulties: presence of observations (such as camera images)
and factored states (such as location of objects). In our setting, the agent receives
observations generated stochastically from a latent factored state. These obser-
vations are rich enough to enable decoding of the latent state and remove partial
observability concerns. Since the latent state is combinatorial, the size of state
space is exponential in the number of latent factors. We create a learning algo-
rithm FactoRL (Fact-o-Rel) for this setting which uses noise-contrastive learning
to identify latent structures in emission processes and discover a factorized state
space. We derive polynomial sample complexity guarantees for FactoRL which
polynomially depend upon the number factors, and very weakly depend on the
size of the observation space. We also provide a guarantee of polynomial time
complexity when given access to an efficient planning algorithm.
1 Introduction
Most reinforcement learning (RL) algorithms scale polynomially with the size of the state space,
which is inadequate for many real world applications. Consider for example a simple navigation task
in a room with furniture where the set of furniture pieces and their locations change from episode
to episode. If we crudely approximate the room as a 10 × 10 grid and consider each element in the
grid to contain a single bit of information about the presence of furniture, then we end up with a state
space of size 2100, as each element of the grid can be filled independent of others. This is intractable
for RL algorithms that depend polynomially on the size of state space.
The notion of factorization allows tractable solutions to be developed. For the above example, the
room can be considered a state with 100 factors, where the next value of each factor is dependent
on just a few other parent factors and the action taken by the agent. Learning in factored Markov
Decision Processes (MDP) has been studied extensively (Kearns & Koller, 1999; Guestrin et al.,
2003; Osband & Van Roy, 2014) with tractable solutions scaling linearly in the number of factors
and exponentially in the number of parent factors whenever planning can be done efficiently.
However, factorization alone is inadequate since the agent may not have access to the underlying
factored state space, instead only receiving a rich-observation of the world. In our room example, the
agent may have access to an image of the room taken from a megapixel camera instead of the grid
representation. Naively, treating each pixel of the image as a factor suggests there are over a million
factors and a prohibitively large number of parent factors for each pixel. Counterintuitively, thinking
of the observation as the state in this way leads to the conclusion that problems become harder as the
camera resolution increases or other sensors are added. It is entirely possible, that these pixels (or
more generally, observation atoms) are generated by a small number of latent factors with a small
number of parent factors. This motivates us to ask: can we achieve PAC RL guarantees that depend
polynomially on the number of latent factors and very weakly (e.g., logarithmically) on the size of
observation space? Recent work has addressed this for a rich-observation setting with a non-factored
latent state space when certain supervised learning problems are tractable (Du et al., 2019; Misra
et al., 2020; Agarwal et al., 2020). However, addressing the rich-observation setting with a latent
factored state space has remained elusive. Specifically, ignoring the factored structure in the latent
space or treating observation atoms as factors yields intractable solutions.
* Correspondence at: dimisra@microsoft.com
1
Published as a conference paper at ICLR 2021
Room Navigation Task Identify Emission Structure —► Learn State Decoder —► Learn Model and Policy Cover
x0[u]
x0[u0]
x0[v]
x0[w]
ψ = {∙∙∙ ,∏1,u, ∙∙∙}
∏1;U trained to reach φι(x0) = U
φl(x)	Φι(x0)
Φ2(x)	φ2(XO)
x/
Φ3(x)	φ3(XO)
Figure 1: Left: A room navigation tasks as a Factored Block MDP setting showing atoms and factors.
Center and Right: Shows the different stages executed by the FactoRL algorithm. We do not show
the observation x emitted by s for brevity. In practice a factor would emit many more atoms.
Contributions. We combine two threads of research on rich-observation RL and factored MDP by
proposing a new problem setup called Factored Block MDP (Section 2). In this setup, observations
are emitted by latent states that obey the dynamics of a factored MDP. We assume observations to be
composed of atoms (which can be pixels for an image) that are emitted by the latent factors. A single
factor can emit a large number of atoms but no two factors can control the same atom. Following
existing rich-observation RL literature, we assume observations are rich enough to decode the current
latent state. We introduce an algorithm FactoRL that achieves the desired guarantees for a large
class of Factored Block MDPs under certain computational and realizability assumptions (Section 4).
The main challenge that FactoRL handles is to map atoms to the parent factor that emits them. We
achieve this by reducing the identification problem to solving a set of independence test problems
with distributions satisfying certain properties. We perform independence tests in a domain-agnostic
setting using noise-contrastive learning (Section 3). Once we have mapped atoms to their parent
factors, FactoRL then decodes the factors, estimates the model, recovers the latent structure in the
transition dynamics, and learns a set of exploration policies. Figure 1 shows the different steps of
FactoRL. This provides us with enough tools to visualize the latent dynamics, and plan for any given
reward function. Due to the space limit, we defer the discussion of related work to Appendix B.
To the best of our knowledge, our work represents the first provable solution to rich-observation RL
with a combinatorially large latent state space.
2	The Factored Block MDP Setting
There are many possible ways to add rich observations to a factored MDP resulting in inapplicability
or intractability. Our goal here is to define a problem setting that is tractable to solve and covers
potential real-world problems. We start with the definition of Factored MDP (Kearns & Koller, 1999),
but first review some useful notation that we will be using:
Notations: For any n ∈ N, we use [n] to denote the set {1, 2,…,n}. For any ordered set (or a
vector) U of size n, and an ordered index setI ⊆ [n] and length k, we use the notation U[I] to denote
the ordered set (U[I[1]],U[I[2]],…，U[I[k]]).
Definition 1. A Factored MDP (S, A, T, R, H) consists of a d-dimensional discrete state space
S ⊆ {0, 1}d, a finite action space A, an unknown transition function T : S × A → ∆(S), an
unknown reward function R : S × A → [0, 1] and a time horizon H. Each state s ∈ S consists
of d factors with the ith factor denoted as s[i]. The transition function satisfies T (s0 | s, a) =
Qid=1 Ti(s0[i] | s[pt(i)], a) for every s, s0 ∈ S and a ∈ A, where Ti : {0, 1}|pt(i)| × A → ∆({0, 1})
defines a factored transition distribution and a parent function pt : [d] → 2[d] defines the set of parent
factors that can influence a factor at the next timestep.
We assume a deterministic start state. We also assume, without loss of generality, that each state and
observation is reachable at exactly one time step. This can be easily accomplished by concatenating
the time step information to state and observations. This allows us to write the state space as
S = (Si, S2, •…,SH) where Sh is the set of states reachable at time step h.
2
Published as a conference paper at ICLR 2021
A natural question to ask here is why we assume factored transition. In tabular MDPs, the lower
bound for sample complexity scales linearly w.r.t. the size of the state set (Kakade, 2003). If we
do not assume a factorized transition function then we can encode an arbitrary MDP with a state
space of size 2d, which would yield a lower bound of Ω(2d) rendering the setting intractable. Instead,
we will prove sample complexity guarantees for FactoRL that scales in number of factors as dO(κ)
where κ := maxi∈[d] |pt(i)| is the size of the largest parent factor set. The dependence of κ in the
exponent is unavoidable as we have to find the parent factors from all possible κd combinations, as
well as learn the model for all possible values of the parent factor. However, for real-world problems
we expect κ to be a small constant such as 2. This yields significant improvement, for example, if
κ = 2 and d = 100 then dκ = 100 while 2d ≈ 1030.
Based on the definition of Factored MDP, we define the main problem setup of this paper, called
Factored Block MDP, where the agent does not observe the state but instead receives an observation
containing enough information to decode the latent state.
Definition 2. A Factored Block MDP consists of an observation space X = Xm and a latent
state space S ⊆ {0, 1}d. A single observation x ∈ X is made ofm atoms with the kth denoted by
x[k] ∈ X. Observations are generated stochastically given a latent state s ∈ S according to a
factored emission function q(x | s) = Qid=1 qi(x[ch(i)] | s[i]) where qi : {0, 1} → ∆(X |ch(i)|) and
Ch : [d] → 2[m] is a childfunction satisfying ch(i)∩ch(j) = 0 whenever i = j. The emissionfunction
satisfies the disjointness property: for every i ∈ [d], we have supp (qi(∙ | 0)) ∩ supp (qi(∙ | 1)) = 0.1
The dynamics of the latent state space follows a Factored MDP (S, A, T, R, H), with parent function
pt and a deterministic start state.
The notion of atoms generalizes commonly used abstractions. For example, if the observation is an
image then atoms can be individual pixels or superpixels, and if the observation space is a natural
language text then atoms can be individual letters or words. We make no assumption about the
structure of the atom space X or its size, which can be infinite. An agent is responsible for mapping
each observation X ∈ X to individual atoms (χ[1],…，χ[m]) ∈ Xm. For the two examples above,
this mapping is routinely performed in practice. If observation is a text presented to the agent as
a string, then it can use off-the-shelf tokenizer to map it to sequence of tokens (atoms). Similar to
states, we assume the set of observations reachable at different time steps is disjoint. Additionally,
we also allow the parent (pt) and child function (ch) to change across time steps. We denote these
functions at time step h by pth and chh .
The disjointness property was introduced in Du et al. (2019) for Block MDPs—a class of rich-
observation non-factorized MDPs. This property removes partial observability concerns and enables
tractable learning. We expect this property to hold in real world problems whenever sufficient sensor
data is available to decode the state from observation. For example, disjointness holds true for the
navigation task with an overhead camera in Figure 1. In this case, the image provides us with enough
information to locate all objects in the room, which describes the agent’s state.. Disjointness allows
us to define a decoder φ? : X|ch(i)| 一 {0,1} for every factor i ∈ [d], such that φ↑(x[ch(i)~) = s[i]
if x[ch(i)] ∈ supp (qi(. | s[i])). We define a shorthand 0?(x) = φ^(x[ch(i)]) whenever Ch is clear
from the context. Lastly, we define the state decoder φ? : X → {0,1}d where φ？(χ)[i] = 0?(x).
The agent interacts with the environment by taking actions according to a policy π : X → ∆(A).
These interactions consist of episodes {sι,xι,aι,rι, s2,x2,a2,r2, •…，a，H, SH} with si = ~, Xh 〜
q(. | Sh), Irh = R(xh,ah), and sh+i 〜T(. | Sh,ah). The agent never observes {si,…，SH}.
Technical Assumptions. We make two assumptions that are specific to the FactoRL algorithm. The
first is a margin assumption on the transition dynamics that enables us to identify different values of a
factor. This assumption was introduced by Du et al. (2019), and we adapt it to our setting.
Assumption 1 (Margin Assumption). For every h ∈ {2,3, .…，H}, i ∈ [d], let Ui be the uniform
distribution jointly over actions and all possible reachable values of sh-i [pt(i)]. Then we assume:
∣∣Pui(∙, ∙ | Sh[i] = 1) - Pui(∙, ∙ | Sh[i] = 0)kTV ≥ σ where Pui(sh-i∖p-t(i)∖,a | sh[i]) is the back-
ward dynamics denoting the probability over parent values and last action given sh[i] and roll-in
distribution ui, and σ > 0 is the margin.
1The notation supp(p) denotes the support of the distribution p. Formally, supp(p) = {z | p(z) > 0}.
3
Published as a conference paper at ICLR 2021
Assumption 1 captures a large set of problems, including all deterministic problems for which the
value of σ is 1. Assumption 1 helps us identify the different values of a factor but it does not help with
mapping atoms to the factors from which they are emitted. In order to identify if two atoms come
from the same factor, we make the following additional assumption to measure their dependence.
Assumption 2 (Atom Dependency Bound). For any h ∈ [H], u, v ∈ [m] and u 6= v, if ch-1(u) =
ch-1 (v), i.e., atoms xh[u] and xh [v] have the same factor. Then under any distribution D ∈ ∆(Sh)
we have kPD(xh[u],xh[v]) - PD(xh[u])PD(xh[v])kTV ≥ βmin.
Dependence assumption states that atoms emitted from the same factor will be correlated. This is
true for many real-world problems. For example, consider a toy grid-based navigation task. Each
state factor s[i] represents a cell in the grid which can be empty (s[i] = 0) or occupied (s[i] = 1).
In the latter case, a randomly sampled box from the set {red box, yellow box, black box}, occupies
its place. We expect Assumption 2 to hold in this case as pixels emitted from the same factor come
from the same object and hence will be correlated. More specifically, if one pixel is red in color, then
another pixel from the same cell will also be red as the object occupying the cell is a red box. This
assumption does not remove the key challenge in identifying factors. As atoms from different factors
can still be dependent due to actions and state distributions from previous time steps.
Model Class. We use two regressor classes F and G. The first regressor class F : X × X → [0, 1]
takes a pair of atoms and outputs a scalar in [0, 1]. To define the second class, we first define a decoder
class Φ : X* → {0,1}. We allow this class to be defined on any set of atoms. This is motivated
by empirical research where commonly used neural network models operate on inputs of arbitrary
lengths. For example, the LSTM model can operate on a text of arbitrary length (Sundermeyer et al.,
2012). However, this is without loss of generality as we can define a different model class for different
numbers of atom. We also define a model class U : X × A × {0, 1} → [0, 1]. Finally, we define
the regressor class G : X ×A× X* → [0,1] as {(x, a, X) → u(x, a, φ(x)) | U ∈ U,φ ∈ Φ}. We
assume F and G are finite classes and derive sample complexity guarantees which scale as log |F|
and log |G|. However, since we only use uniform convergence arguments extending the guarantees
to other statistical complexity measures such as Rademacher complexity is straightforward. Let
Πall : S → A denote the set of all non-stationary policies of this form. We then define the class of
policies Π : X → A by {x → 夕(φ*(x)) | ∀夕 ∈ Πan}, which We use later to define our task. We use
Pπ[E] to denote probability of an event E under the distribution over episodes induced by policy π.
Computational Oracle. We assume access to two regression oracles REG for model classes F and G .
Let D1 be a dataset of triplets (x[u], x[v], y) where u, v denote two different atoms and y ∈ {0, 1}.
Similarly, let D2 be a dataset of quads (x, a, x0, y) where X ∈ X, a ∈ A, X ∈ X*, and y ∈ {θ, 1}.
Lastly, let ED [∙] denote the empirical mean over dataset D. The two computational oracles compute:
REG(DLF) = argminEd*/(x[u],x[v]) - y)2], REG(D2, G) = arg min ED2h(g(x, a,X) - y)2].
f∈F	g∈GN
We also assume access to a ∆pl-optimal planning oracle planner. Let S = (Si,…，Sh) be a
learned state space and T = (Ti, ∙∙∙ ,TH) with Th : Sh-I ×A→ ∆(Sh) be the learned dynamics,
and R : S × A → [0,1] be a given reward function. Let 夕:S → A be a policy and V (夕;T, R)
be the policy value. Then for any ∆pl > 0 the output of planner 夕=Planner(T, R, ∆pl) satisfies
V(夕；T, R) ≥ SuPW V(夕；T, R) - ∆pl, where supremum is taken over policies of type S → A.
Task Definition. We focus on a reward-free setting with the goal of learning a state decoder and
estimating the latent dynamics T. Since the state space is exponentially large, we cannot visit every
state. However, the factorization property allows us to estimate the model by reaching factor values.
In fact, we show that controlling the value of at most 2κ factors is sufficient for learning the model.
Let C≤k(U) denote the space of all sets containing at most k different elements selected from the set
U including 0. We define the reachability probability ηh(K, Z) for a given h ∈ [H], K ⊆ [d], and
Z ∈ {0, 1}|K|, and the reachability parameter ηmin as:
ηh(K, Z) := suP Pπ(sh [K] = Z),	ηmin := inf inf inf	ηh(K, s[K]).
π∈ΠNS	h∈[H] s∈Sh K∈C≤2κ([d])
Our sample complexity scales polynomially with ηm-iin. Note that we only require that if sh [K] = Z
is reachable, then it is reachable with at least ηmin probability, i.e., either ηh (K, Z) = 0 or it is
at least ηmin . These requirements are similar to those made by earlier work for non-factored state
4
Published as a conference paper at ICLR 2021
space (Du et al., 2019; Misra et al., 2020). The key difference being that instead of requiring every
state to be reachable with ηmin probability, we only require a small set of factor values to be reachable.
For reference, if every policy induces a uniform distribution over S = {0, 1}d, then probability of
visiting any state is 2-d but the probability of two factors taking certain values is only 0.25. This
gives us a more practical value for ηmin .
Besides estimating the dynamics and learning a decoder, we also learn an α-policy cover to enable
exploration of different reachable values of factors. We define this below:
Definition 3 (Policy Cover). A set of policies Ψ is an α-policy cover of Sh for any α > 0 and h if:
∀s ∈Sh,K ∈ C≤2κ([d]),	sup Pπ(sh[K] = s[K]) ≥αηh(K,s[K]).
π∈Ψ
3	Discovering Emission S tructure with Contrastive Learning
Directly applying the prior work (Du et al., 2019; Misra et al., 2020) to decode a factored state from
observation results in failure, as the learned factored state need not obey the transition factorization.
Instead, the key high-level idea of our approach is to first learn the latent emission structure ch, and
then use it to decode each factor individually. We next discuss our approach for learning ch.
Reducing Identification of Latent Emission Structure to Independence Tests. Assume we are
able to perfectly decode the latent state and estimate the transition model till time step h - 1. Our
goal is to infer the latent emission structure chh at time step h, which is equivalent to: given an
arbitrary pair of atoms u and v, determine if they are emitted from the same factor or not. This is
challenging since we cannot observe or control the latent state factors at time step h.
Let i = ch-1(u) andj = ch-1 (v) be the factors that emit x[u] and x[v]. Ifi = j, then Assumption 2
implies that these atoms are dependent on each other for any roll-in distribution D ∈ ∆(Sh-1 × A)
over previous state and action. However, if i 6= j then deterministically setting the previous action
and values of the parent factors pt(i) or pt(j), makes x[u] and x[v] independent. For the example
in Figure 1, fixing the value of s[1], s[2] and a would make x[u] and x[u0] independent of each other.
This observation motivates us to reduce this identification problem to performing independence
tests with different roll-in distributions D ∈ ∆(Sh-1 × A). Naively, we can iterate over all subsets
K ∈ C≤2κ([d]) where for each K we create a roll-in distribution such that the values of sh-1 [K] and
the action ah-1 are fixed, and then perform independence test under this distribution. If two atoms
are independent then there must exist a K that makes them independent. Otherwise, they should
always be dependent by Assumption 2.
However, there are two problems with this approach. Firstly, we do not have access to the latent states
but only a decoder at time step h - 1. Further, it may not even be possible to find a policy that can set
the values of factors deterministically. We later show that our algorithm FactoRL can learn a decoder
that induces a bijection between learned factors and values, and the real factors and values. Therefore,
maximizing the probability of EK;Z = {φh-1(xh-1)[K] = Z} for a set of learned factors K and
their values Z, implicitly maximizes the probability of EK0;Z0 = {sh-1[K0] = Z0} for corresponding
ʌ
real factors K0 and their values Z0. Since the event EK;Z is observable we can use rejection sampling
to increase its probability sufficiently close to 1 which makes the probability of EK0;Z0 close to 1.
The second problem is to perform independence tests in a domain agnostic setting. Directly estimating
mutual information I (x[u]; x[v]) can be challenging. Instead, we propose an oraclized independence
test that reduces the problem to binary classification using noise-contrastive learning.
Oraclized Independent Test. Here, we briefly sketch the main idea of our independence test scheme
and defer the details to Appendix C. We comment that the high-level idea of our independence testing
subroutine is similar to Sen et al. (2017). Suppose we want to test if two random variables Y and Z
are independent. Firstly, we construct a dataset in the following way: sample a Bernoulli random
variable W 〜 Bern(I/2), and two pairs of independent realizations (y(1), z(1)) and (y(2),z(2)); if
w = 1, add (y(1), z(1), w) to the dataset, and (y(1), z(2), w) otherwise. We repeat the sampling
procedure n times and obtain a dataset {(yi, zi, wi)}in=1. Then we can fit a classifier that predicts the
value of wi using (yi, zi). If Y and Z are independent, then (yi, zi) will provide no information about
wi and thus no classifier can do better than random guess. However, if Y and Z are dependent, then
5
Published as a conference paper at ICLR 2021
the Bayes optimal classifier would perform strictly better than random guess. As a result, by looking
at the training loss of the learned classifier, we can determine whether Y and Z are dependent or not.
4	FactoRL: Reinforcement Learning in Factored Block MDPs
In this section, we present the main algorithm FactoRL (Algorithm 1). It takes as input the model
classes F, G, failure probability δ > 0, and five hyperparameters σ, ηmin, βmin ∈ (0, 1) and d, κ ∈
N.2 We use these hyperparamters to define three sample sizes nind, nabs , nest and rejection sample
frequency k. For brevity, we defer the exact values of these constants to Appendix D.7. FactoRL
returns a learned decoder φbh : X → {0, 1}dh for some dh ∈ [m], an estimated transition model
Th, learned parent pth and child functions chh, and a 1/2-policy cover Ψh of Sh for every time
step h ∈ {2,3, •一，H}. We use Sh to denote the learned state at time step h. Formally, Sh
^ ^
一^一、
(φhi(xh),…，φhdh (Xh)). Inthe analysis of FactoRL, We show that dh-ι = d, and Chh is equivalent
to chh up to permutation with high probability. Further, we
show that φh and chh together learn
a
bijection between learned factors and their values and real factors and their values.
FactoRL operates inductively over the time steps (Algorithm 1, line 2-8). In the hth iteration, the
algorithm performs four stages of learning: identifying the latent emission structure, decoding the
factors, estimating the model, and learning a policy cover. We describe these below.
Algorithm 1 FactoRL(F, G, δ, σ, ηmin, βmin, d, κ). RL in Factored Block MDPs.
1:	Initialize Ψh = 0 for every h ∈ [H] and φι = X → {0}. Set global constants nind, nabs, nest, k.
2:	for h ∈{2, 3, ∙∙∙ ,H} do
3:	chh = FactorizeEmission(Ψh-1, φh-1, F) // stage 1: discover latent emission structure
4:	φh = LearnDecoder(G, Ψh-1, chh)	// stage 2: learn a decoder for factors
5:	Th,pth = EstModel(Ψh-1, φh-1, φh)	// stage 3: find latent pth and estimate model
6:	forI∈ C≤2κ([d]),Z∈ {0,1}|I| do
∕∖'
7:	⅛^hiz = Planner(T, RhIZ, ∆pl) where RhIZ := 1{bh [I] = Z} // stage 4: planning
8:	IfVQhIZ]T,RhIz) ≥ 3ηmin∕4 then Ψh  Ψh ∪ {夕hiz ◦ φh}
return {cchh, φbh, Tbh, pbth, Ψh}hH=2
Identifying Latent Emission Process. The FactorizeEmission collects a dataset of observations
for every policy in Ψh-1 and action a ∈ A (Algorithm 2, line 1-4). Policies in Ψh-1 are of the
type nI；Z where I ∈ C≤2κ([dh-1]) and Z ∈ {0,1}|I|. We can inductively assume nI；Z to be
maximizing the probability of EI；Z = {Sh-ι [I] = Z}. If our decoder is accurate enough, then we
hope that maximizing the probability of this event in turn maximizes the probability of fixing the
values of a set of real factors. However, it is possible that P∏%Z (Sh-ι[I] = Z) is only O(ηmin).
Therefore, as explained earlier, we use rejection sampling to drive the probability of this event close
to 1. Formally, we define a procedure RejectSamp(nI；Z, EI；Z, k) which rolls-in at time step h 一 1
with πI；Z to observe xh-1 (line 3). If the event EI；Z holds for xh-1 then we return xh-1, otherwise,
we repeat the procedure. If we fail to satisfy the event k times then we return the last sample. We use
this to define our main sampling procedure Xh 〜 DIZ,。:= RejectSamp(nI；Z, EI；Z, k) ◦ a which
first samples xh-1 using the rejection sampling procedure and then takes action a to observe xh. We
collect a dataset of observation pairs (X(1), X(2)) sampled independently from DI,Z,a.
For every pair of atoms u, v ∈ [m], we calculate if they are independent under the distribution
induced by DI,Z,a using IndTest with dataset DI,Z,a (line 5-7). We share the dataset across atoms
for sample efficiency. If there exists at least one (I, Z, a) triple such that we evaluate X[u], X[v] to
be independent, then we mark these atoms as coming from different factors. Intuitively, such an I
would contain parent factors of at least chh-1(u) or chh-1(v). If no such I exists then we mark these
atoms as being emitted from the same factor.
2Our analysis can use any non-zero lower bound on ηmin , βmin , σ and an upper bound on d and κ.
6
Published as a conference paper at ICLR 2021
A 1 一 . J∕l- --GL .	∙ L ∙	∙ /F	^7'	-T-∖
Algorithm 2 FactorizeEmission(Ψh-1, φh-1, F).
1:
2:
3:
4:
for (∏i;Z, a) ∈ Ψh-ι XA and i ∈ [nind] do
Define EI;Z := 1{φbh-1(xh-1)[I] = Z}
Sample XhI),x? 〜RejectSamp(ni；Z, Ei；Z, k) ◦ a
DIZ;a J DI;Z;a ∪{(XhI),xh2))}
// rejection sampling procedure
// initialize Di；z；a = 0
5:	for U ∈{1, 2,…，m — 1} and V ∈ {u + 1,…,m} do
-1	-1
6:	Mark u, v as coming from the same factor, i.e., chh (u) = chh (v) if∀(I, Z, a)
7:	the oraclized independence test finds Xh[u], Xh [v] as dependent using DI;Z;a and F
return chh	// label ordering of parents does not matter.
Algorithm 3 LearnDecoder(G, Ψh-1, cchh).	Child function has type cchh : [dh] → 2[m]
一一.
1:	for i in [dh], define ω = chh(i), D = 0 do
2:	for nabs times do // collect a dataset of real (y = 1) and imposter (y = 0) transitions
3:	Sample (x* (1), a(1),xo(1)), (x(2),a(2),xo(2))〜Unf (Ψh-ι) ◦ Unf(A) and y 〜Bern(2)
4:	Ify = 1 then D J D ∪ (X(1), a(1), X0(1) [ω], y) else D J D ∪ (X(1), a(1), X0(2) [ω], y)
5:	ubi, φi = REG(D, G)	// train the decoder using noise-contrastive learning
return φ : X → {0, 1}dh where for any X ∈ X and i ∈ [dh] we have φ(X)[i] = φi(X[chh(i)]).
Decoding Factors. LearnDecoder partitions the set of atoms into groups based on the learned child
th
function chh (Algorithm 3). For the ith group ω, we learn a decoder φhi : X ? → {0, 1} by adapting
the prediction problem of Misra et al. (2020) to Factored Block MDP setting. We define a sampling
procedure (x, a,x0)〜Unf(Ψh-ι) ◦ Unf(A) where X is observed after roll-in with a uniformly
selected policy in Ψh-ι till time step h — 1, action a is taken uniformly, and x0 〜T(∙ | x, a) (line 3).
We collect a dataset D of real and imposter transitions. A single datapoint in D is collected by
sampling two independent transitions (X(1), a(1), X0(1)), (X(2), a(2), X0(2))
〜Unf (Ψh-ι) ◦ Unf(A)
and a Bernoulli random variable y 〜Bern(I/2). If y = 1 then We add the real transition
(X(1), a(1), X0(1) [ω], y) to D, otherwise we add the imposter transition (X(1), a(1), X0(2) [ω], y) (line 4).
The key difference from Misra et al. (2020) is our use X0 [ω] instead of X0 which allows us to decode a
specific latent factor. We train a model to predict the probability that a given transition (X, a, X0[ω]) is
real by solving a regression task with model class G (line 5). The bottleneck structure of G allows us
to recover a decoder φi from the learned model. The algorithm also checks for the special case where
a factor takes a single value. If it does, then we return the decoder that always outputs 0, otherwise
∕∖'
we stick with φi. For brevity, we defer the details of this special case to Appendix D.2.2. The decoder
for the hth timestep is given by composition of decoders for each group.
Algorithm 4 EstModel(Ψh-1, φh-1, φh).
1:	Collect dataset D of nest triplets (x, a, x0)〜Unf (Ψh-ι) ◦ Unf(A)
2:	forI, J ∈ C≤κ([dh-1]) satisfying I ∩ J = 0 do
3:	Estimate b(sh[k] | ^h-ι[I], Sh-ι[J],a) from D using φ, ∀a ∈ A,k ∈ [dh].
4:	For every k define Pth(k) as solution to following: (where we bind ^0 = Sh and S = Sh-ι)
^
^
argmin max	∣P(s0[k]	|	s[I]	= u, s[J1]	= wι,a)	— P(^0[k]	|	s[I]	= u, s[J2]	= w2,a) I
I u,J1,J2,w1,w2,a	TV
5:	Define Th(^0 | S,a) = Qk P(S[k] | S血h(k)],a) and return Th,pth
Estimating the Model. EstModel routine first collects a dataset D of nest independent transitions
(x, a, x0)〜Unf (Ψh,-ι) ◦ Unf(A) (Algorithm 4, line 1). We iterate over two disjoint sets of factors
I, J of size at most κ. We can view I as the control set and J as the variable set. For every
7
Published as a conference paper at ICLR 2021
learned factor k ∈ [dh], factor set I, J and action a ∈ A, we estimate the model P(s%[k] |
^h-ι[I], ^h-ι[J],a) using count based statistics on dataset D (line 3).
—∙^~∙
Consider the case where the cht = cht for every t ∈ [h] and where we ignore the label permutation for
brevity. If I contains the parent factors pt(k), then we expect the value of P(s0[k] | s[I],s[J ],a) ≈
Tk(^0[k] | S[pt(k)],a) to not change significantly on varying either the set J or its values. This
motivates us to define the learned parent set as the I which achieves the minimum value of this gap
(line 4). When computing the gap, we take max only over those values of S[I] and s[J] which can
be reached jointly using a policy in Ψh-1. This is important since we can only reliably estimate the
.—.
model for reachable factor values. The learned parent function pth need not be identical to pth even
upto relabeling. However, finding the exact parent factor is not necessary for learning an accurate
model, and may even be impossible. For example, two factors may always take the same value
making it impossible to distinguish between them. We use the learned parent function pth to define
Tbh similar to the structure of T (line 5).
Learning a Policy Cover. We plan in the latent space using the estimated model {Tt}th=1, to find a
policy cover for time step h. Formally, for every I ∈ C≤2κ([dh]) and Z ∈ {0, 1}|I|, we find a policy
OhIZ to reach {Sh[I] = Z} using the planner (Algorithm 1, line 7). This policy acts on the learned
state space and is easily lifted to act on observations by composition with the learned decoder. We
add every policy that achieves a return of at least O(ηmin) to Ψh (line 8).
5 Theoretical Analysis and Discussion
In this section, we present theoretical guarantees for FactoRL. For technical reasons, we make the
following realizability assumption on the function classes F and G .
Assumption 3 (Realizability). For any h ∈ [H], i ∈ [d] and diStribution ρ ∈ ∆({0, 1}), there exiStS
gihρ ∈ G, such that for all ∀(x, a, x0) ∈ Xh-ι XAXXh and X = x0[chh (i)] we have:
T；(0?(X) | φ*(x),a)
gihρ(χ,a,χ) = Ti3?(x)। φ*(χ),a)+Pm∙
For any h ∈ [H], u, v ∈ [m] with u 6= v, and any D ∈ ∆(Sh), there exiStS fuvD ∈ F SatiSfying:
∀s ∈ Supp(D),x ∈ Supp(q(∙ | s)),	fuvD(x[u],x[v])
D(x[u], x[v])
D(x[u], x[v]) + D(x[u])D(x[v])
Assumption 3 requires the function classes to be expressive enough to represent optimal solutions for
our regression tasks. Realizability assumptions are common in literature and are in practice satisfied
by using deep neural networks (Sen et al., 2017; Misra et al., 2020).
.—.
Theorem 1 (Main Theorem). For any δ > 0, FactoRL returnS a tranSition function Th, a parent
function Chh, a decoder φh, and a Set of policies Ψh, for every h ∈ {2,3, •…，H}, that with
—∙^~∙
probability at leaSt 1 - δ SatiSfieS: (i) chh iS equal to chh upto permutation, (ii) Ψh iS a 1/2 policy
cover of Sh, (iii) For every h ∈ [H], there exiStS a permutation mapping θh : {0, 1}d → {0, 1}d Such
that for every s ∈ Sh-1, a ∈ A, s0 ∈ Sh and x0 ∈ Xh we have:
P(bh(x0)= θ-1(s0) I s0) ≥ 1 -O (η⅛) , |卜(s0 | s,a)-Th(θ-1(s0) | θ--ι(s),a)∣∣IV ≤ 罂,
and the sample complexity is poly(d16κ, ∣A∣, H, nɪ-, 1,套,1, ln m, ln ∣F∣, ln |G|).
Discussion. The proof and the detailed analysis of Theorem 1 has been deferred to Appendix C-D.
Our guarantees show that FactoRL is able to discover the latent emission structure, learn a decoder,
estimate the model, and learn a policy cover for every timestep. We set the hyperparmeters in order
to learn a 1/2-policy cover, however, they can also be set to achieve a desired accuracy for the decoder
or the transition model. This will give a polynomial sample complexity that depends on this desired
accuracy. It is straightforward to plan a near-optimal policy for a given reward function in our learned
latent space, using the estimated model and the learned decoder. This incurs zero sample cost apart
from the samples needed to learn the reward function.
8
Published as a conference paper at ICLR 2021
Our results show that we depend polynomially on the number of factors and only logarithmic in the
number of atoms. This appeals to real-world problem where d and m can be quite large. We also
depend logarithmically on the size of function classes. This allows us to use exponentially large
function classes, further, as stated before, our results can also be easily extended to Rademacher
complexity. Our algorithm only makes a polynomial number of calls to computational oracles. Hence,
if these oracles can be implemented efficiently then our algorithm has a polynomial computational
complexity. The squared loss oracles are routinely used in practice, but planning in a fully-observed
factored MDP is EXPTIME-complete (see Theorem 2.24 of Mausam (2012)). However, various ap-
proximation strategies based on linear programming and dynamic programming have been employed
succesfully (Guestrin et al., 2003). These assumptions provide a black-box mechanism to leverage
such efforts. Note that all computational oracles incur no additional sample cost and can be simply
implemented by enumeration over the search space.
Comparison with Block MDP Algorithms. Our work is closely related to algorithms for Block
MDPs, which can be viewed as a non-factorized version of our setting. Du et al. (2019) proposed
a model-based approach for Block MDPs. They learn a decoder for a given time step by training
a classifier to predict the decoded state and action at the last time step. In our case, this results in
a classification problem over exponentially many classes which can be practically undesirable. In
contrast, Misra et al. (2020) proposed a model-free approach that learns a decoder by training a
classifier to distinguish between real and imposter transitions. As optimal policies for factored MDPs
do not factorize, therefore, a model-free approach is unlikely to succeed (Sun et al., 2019). Feng
et al. (2020) proposed another approach for solving Block MDPs. They assume access to a purely
unsupervised learning oracle, that can learn an accurate decoder using a set of observations. This
oracle assumption is significantly stronger than those made in Du et al. (2019) and Misra et al. (2020),
and reduces the challenge of learning the decoder. Crucially, these three approaches have a sample
complexity guarantee which depends polynomially on the size of latent state space. This yields an
exponential dependence on d when applied to our setting. It is unclear if these approaches can be
extended to give polynomial dependence on d. For general discussion of related work see Appendix B.
Proof of Concept Experiments. We empirically evaluate FactoRL to support our theoretical
results, and to provide implementation details. We consider a problem with d factors each emitting
2 atoms. We generate atoms for factor s[i], by first defining a vector zi = [1, 0] if s[i] = 0, and
zi = [0, 1] otherwise. We then sample a scalar Gaussian noise gi with 0 mean and 0.1 standard
deviation, and add it to both component of zi . Atoms emitted from each factor are concatenated
to generate a vector z ∈ R2d . The observation x is generated by applying a fixed time-dependent
permutation to z to shuffle atoms from different factors. This ensures that an algorithm cannot figure
out the children function by relying on the order in which atoms are presented. We consider an
action space A = {a1,a2,…，ad} and non-stationary dynamics. For each time step t ∈ [H], We
define σt as a fixed permutation of {1, 2, .…，d}. Dynamics at time step t are given by: Tt(st+ι |
st , a) = Qi=1 Tti (st+1 [i] | st [i], a), Where Tti (st+1 [i] = st [i] | st [i], a) = 1 for all a 6= aσt (i) , and
Tti(st+1 [i] = 1 - st[i] | st[i], aσt(i)) = 1. We evaluate on the setting d = 10 and H = 10.
We implement model classes F and G using feed-forWard neural netWorks. Specifically, for G We
apply the Gumbel-softmax trick to model the bottleneck folloWing Misra et al. (2020). We train the
models using cross-entropy loss instead of squared loss that We use for theoretical analysis.3 For the
independence test task, We declare tWo atoms to be independent, if the best log-loss on the validation
set is greater than c. We train the model using Adam optimization and perform model selection using
a held-out set. We defer the full model and training details to Appendix F.
For each time step, We collect 20,000 samples and share them across all routines. This gives a sample
complexity of 20, 000 × H . We repeat the experiment 3 times and found that each time, the model
Was able to perfectly detect the latent child function, learn a 1/2-policy cover, and estimate the model
With error < 0.01. This is in sync With our theoretical findings and demonstrates the empirical use of
FactoRL. We Will make the code available at: https://github.com/cereb-rl.
Conclusion. We introduce Factored Block MDPs that model the real-World difficulties of rich-
observation environments With exponentially large latent state spaces. We also propose a provable
RL algorithm called FactoRL for solving a large class of Factored Block MDPs. We hope the setting
and ideas in FactoRL Will stimulate both theoretical and empirical Work in this important area.
3We can also easily modify our proof to use cross-entropy loss by using generalization bounds for log-loss
(see Appendix E in AgarWal et al. (2020))
9
Published as a conference paper at ICLR 2021
References
Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. Flambe: Structural complexity
and representation learning of low rank mdps. Advances in Neural Information Processing Systems,
2020.
Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforce-
ment learning. In International Conference on Machine Learning, 2017.
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.
Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information
Processing Systems, 2016.
Ronen I. Brafman and Moshe Tennenholtz. R-MAX - A general polynomial time algorithm for
near-optimal reinforcement learning. The Journal of Machine Learning Research, 2002.
Richard Y Chen, Szymon Sidor, Pieter Abbeel, and John Schulman. UCB exploration via Q-
Ensembles. arXiv:1706.01502, 2017.
Simon S Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford.
Provably efficient RL with rich observations via latent state decoding. In International Conference
on Machine Learning, 2019.
Fei Feng, Ruosong Wang, Wotao Yin, Simon S Du, and Lin Yang. Provably efficient exploration for
reinforcement learning using unsupervised learning. Advances in Neural Information Processing
Systems, 2020.
Carlos Guestrin, Relu Patrascu, and Dale Schuurmans. Algorithm-directed exploration for model-
based reinforcement learning in factored mdps. In International Conference on Machine Learning,
2002.
Carlos Guestrin, Daphne Koller, Ronald Parr, and Shobha Venkataraman. Efficient solution algorithms
for factored mdps. Journal of Artificial Intelligence Research, 2003.
Ji He, Mari Ostendorf, Xiaodong He, Jianshu Chen, Jianfeng Gao, Lihong Li, and Li Deng. Deep
reinforcement learning with a combinatorial action space for predicting popular reddit threads. In
Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, 2016.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. Journal of Machine Learning Research, 2010.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with Gumbel-Softmax. In
International Conference on Learning Representations, 2016.
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contex-
tual decision processes with low Bellman rank are PAC-learnable. In International Conference on
Machine Learning, 2017.
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning provably efficient?
In Advances in Neural Information Processing Systems, 2018.
Sham Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis, Gatsby Computa-
tional Neuroscience Unit, University College London, 2003.
Michael Kearns and Daphne Koller. Efficient reinforcement learning in factored mdps. In Interna-
tional Joint Conference on Artificial Intelligence, 1999.
Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time.
Machine learning, 2002.
Hyoungseok Kim, Jaekyeom Kim, Yeonwoo Jeong, Sergey Levine, and Hyun Oh Song. Emi:
Exploration with mutual information. In International Conference on Machine Learning, 2019.
Hyunjik Kim and Andriy Mnih. Disentangling by factorising. arXiv preprint arXiv:1802.05983,
2018.
10
Published as a conference paper at ICLR 2021
Akshay Krishnamurthy, Alekh Agarwal, and John Langford. PAC reinforcement learning with rich
observations. In Advances in Neural Information Processing Systems, 2016.
Adrien Laversanne-Finot, Alexandre P6r6, and Pierre-Yves Oudeyer. Curiosity driven exploration of
learned disentangled goal spaces. arXiv preprint arXiv:1807.01521, 2018.
Lihong Li, Michael L Littman, Thomas J Walsh, and Alexander L Strehl. Knows what it knows: a
framework for self-aware learning. Machine learning, 2011.
Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng. Rectifier nonlinearities improve neural
network acoustic models. In ICML Workshop on Deep Learning for Audio, Speech and Language
Processing, 2013.
Andrey Kolobov Mausam. Planning with markov decision processes: an ai perspective. Morgan &
Claypool Publishers, 2012.
Dorde Miladinovic, Muhammad Waleed Gondal, Bernhard Scholkopf, Joachim M Buhmann, and
Stefan Bauer. Disentangled state space representations. arXiv preprint arXiv:1906.03255, 2019.
Dipendra Misra, Mikael Henaff, Akshay Krishnamurthy, and John Langford. Kinematic state abstrac-
tion and provably efficient rich-observation reinforcement learning. In International Conference
on Machine Learning, 2020.
Ofir Nachum, Shixiang Gu, Honglak Lee, and Sergey Levine. Near-optimal representation learning
for hierarchical reinforcement learning. In International Conference on Learning Representations,
2019.
Ian Osband and Benjamin Van Roy. Near-optimal reinforcement learning in factored mdps. In
Advances in Neural Information Processing Systems, 2014.
Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by
self-supervised prediction. In International Conference on Machine Learning, 2017.
Rajat Sen, Ananda Theertha Suresh, Karthikeyan Shanmugam, Alexandros G Dimakis, and Sanjay
Shakkottai. Model-powered conditional independence test. In Advances in Neural Information
Processing Systems, 2017.
Sahil Sharma, Aravind Suresh, Rahul Ramesh, and Balaraman Ravindran. Learning to factor policies
and action-value functions: Factored action space representations for deep reinforcement learning.
arXiv preprint arXiv:1705.07269, 2017.
Aravind Srinivas, Michael Laskin, and Pieter Abbeel. Curl: Contrastive unsupervised representations
for reinforcement learning. arXiv preprint arXiv:2004.04136, 2020.
Alexander L. Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael L. Littman. PAC
model-free reinforcement learning. In International Conference on Machine Learning, 2006.
Alexander L Strehl, Carlos Diuk, and Michael L Littman. Efficient structure learning in factored-state
mdps. In Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence, 2010.
Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based
rl in contextual decision processes: Pac bounds and exponential improvements over model-free
approaches. In Conference on Learning Theory, 2019.
Martin Sundermeyer, Ralf Schluter, and Hermann Ney. Lstm neural networks for language modeling.
In Thirteenth annual conference of the international speech communication association, 2012.
Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John
Schulman, Filip DeTurck, and Pieter Abbeel. #Exploration: A study of count-based exploration
for deep reinforcement learning. In Advances in Neural Information Processing Systems, 2017.
Valentin Thomas, Emmanuel Bengio, William Fedus, Jules Pondard, Philippe Beaudoin, Hugo
Larochelle, Joelle Pineau, Doina Precup, and Yoshua Bengio. Disentangling the independently
controllable factors of variation by interacting with the world. arXiv preprint arXiv:1802.09484,
2018.
11
Published as a conference paper at ICLR 2021
Tsachy Weissman, Erik Ordentlich, Gadiel Seroussi, Sergio Verdu, and Marcelo J Weinberger.
Inequalities for the l1 deviation of the empirical distribution. Hewlett-Packard Labs, Tech. Rep,
2003.
12
Published as a conference paper at ICLR 2021
Appendix Organization
This appendix is organized as follows.
•	Appendix A provides a list of notations used in this paper.
•	Appendix B covers related work
•	Appendix C describes the independence test algorithm and its sample complexity guarantees
•	Appendix D provides sample complexity guarantees for FactoRL
•	Appendix E provides list of supporting results
•	Appendix F provides details of the experimental setup and optimization
A	Notations
We present notations and their definition in Table 1. In general, calligraphic notations represent sets.
All logarithms are with respect to base e.
B Related Work
There is a rich literature on sample-efficient reinforcement learning in tabular MDPs with a small
number of observed states (Brafman & Tennenholtz, 2002; Strehl et al., 2006; Kearns & Singh, 2002;
Jaksch et al., 2010; Azar et al., 2017; Jin et al., 2018). While recent state-of-the-art results along
this line achieve near-optimal sample complexity, these algorithms do not exploit the latent structure
in the environment, and therefore, cannot scale to many practical settings such as rich-observation
environments with possibly a large number of factored latent states.
In order to overcome this challenge, one line of research has been focusing on factored MDPs (Kearns
& Koller, 1999; Guestrin et al., 2002; 2003; Strehl et al., 2010), which allow a combinatorial number
of observed states with a factorized structure. Planning in factored MDPs is EXPTIME-complete
(Mausam, 2012) yet often tractable in practice, with factored MDPs statistically learnable with
polynomial samples in the number of parameters that encode the factored MDP (Osband & Van Roy,
2014; Li et al., 2011). There has also been several empirical works that either focus on the factored
state space setting (e.g., Kim & Mnih (2018); Thomas et al. (2018); Laversanne-Finot et al. (2018);
Miladinovic et al. (2019)), or the factored action space setting (e.g., He et al. (2016); Sharma et al.
(2017)). However, these works do not directly address our problem and do not provide sample
complexity guarantees.
Another line of work focuses on exploration in a rich observation environment. Empirical results (Tang
et al., 2017; Chen et al., 2017; Bellemare et al., 2016; Pathak et al., 2017) have achieved inspiring
performance on several RL benchmarks, while theoretical works (Krishnamurthy et al., 2016; Jiang
et al., 2017) show that it is information-theoretically possible to explore these environments. As
discussed before, recent works of Du et al. (2019), Misra et al. (2020) and Feng et al. (2020) provide
computationally and sample efficient algorithms for Block MDP which is a rich-observation setting
with a latent non-factored state space. Nevertheless, this line of results crucially relies on the number
of latent states being relatively small.
Finally, we comment that the contrastive learning technique used in this paper has been used by
other reinforcement learning algorithms for learning feature representation (e.g., Kim et al. (2019);
Nachum et al. (2019); Srinivas et al. (2020)) without theoretical guarantee.
C Independence testing using Noise Contrastive Estimation
In this section, we introduce the independence testing algorithm, Algorithm 5 and provide its theoretic
guarantees. Algorithm 5 will be used in Algorithm 2 as a subroutine for determining if two atoms are
emitted from the same latent factor. We comment that the high-level idea of Algorithm 5 is similar to
Sen et al. (2017), which reduces independence testing to regression by adding imposter samples.
13
Published as a conference paper at ICLR 2021
Notation	Description
"N Z Z≥o [N] C≤k (U) u[I] [I; J ] ∆(U) X X Xh m X S Sh d S φ? φ? A T : S×A→ ∆(S) H Pth :[d] → 2[d] Chh ：[d] → 2[m] q : S→ ∆(X) Ti qi F, G Φ: X→{0,1}	Space of natural numbers {1, 2,…，} Space of integers {…，-2, -1,0,1, 2,…，} Space of positive integers {0,1,2,…，}, equivalent to N ∪ {0}. Given n ∈ N, this notation denotes the set {1,2,…，N}. Given an ordered set U and k ∈ Z≥o, this denotes the set of all ordered subsets of U with at most k elements including the empty set. For a given n-dimensional vector U and I = (i1,i2, ∙∙∙ ,ik) ∈ 2[n], u[I] denotes the k-dimensional vector (u[i1],u[i2],…，u[ik]) Denotes concatenation of two ordered sets I and J Set of all possible distributions over the set U. Set of atoms Set of all observations Set of all observations reachable at time step h Number of atoms in the observation Observation consisting of m atoms denoted by (x[1], ∙∙∙ ,x[m]). Set of latent states Set of latent states reachable at time step h Number of latent factors Latent state represented by a vector in {0,1}d. Decoder function which maps an observation X ∈ X to latent state S ∈ S. Decoder function which maps X ∈ X to its ith latent state value. Formally, ∀x ∈ X and i ∈ [d], 0?(x) = φ*(x)[i]. Action space Transition function on the latent dynamics Horizon of the problem denoting the maximum number of actions an agent takes in a single episode. Parent function for time step h. We drop h when it is clear. Child function for time step h. By definition, for any i, j ∈ [d] and i = j We have chh(i) ∩ chh(j) = 0. We drop h when clear. Emission function that generates X 〜q(∙ | S) given S ∈ S. ith product term in the transition function. Formally, for s,s0 ∈ S and a ∈ A we have T(s0 | s,a) = Qd=ι Ti(s0[i] | s[pt(i)],a). ith product term in emission function. Formally for X ∈ X with φ*(x) = S we have q(X | S) = Qd qi(X[ch(i)] | S[i]) Regressor classes. We will denote individual member of the class as f ∈ F and g ∈ G Decoder class.
Table 1: List of notations and their definition.
14
Published as a conference paper at ICLR 2021
C.1 Algorithm description
Let D ∈ ∆(Sh-1 × A) be our roll-in distribution that induces a probability distribution
PD ∈ ∆(Xh) over observations at time step h. Let u, v ∈ [m] be two different atoms, and
PD(x[u], x[v]), PD(xh[u]) and PD(xh[v]) be the joint and marginal distributions over xh[u], xh [v]
with respect to roll-in distribution D. The goal of our algorithm to determine if xh [u] and xh [v] are
independent under PD .
Algorithm 5 IndTest(F, D, u, v, β) Oraclized Independency Test. We initialize Dtrain = 0.
1:	Initialize Dtrain = 0 and sample z1,z2, ∙∙∙ ,Zn 〜Bern( 11).
2:	for i ∈ [n] do
3:	if zi = 1 then
4:	Dtrain — Dtrain ∪ { (x(i, 1) [u],X(i, 1) [v], 1)}.
5:	else
6:	Dtrain — Dtrain ∪ { (x(i,1) [u],X(i⑵[v], 0)}.
ʌ
7:	Compute f := arg minf ∈F L(Dtrain, f), where
L(Dtrain,f):= 1	X	{f (x[u],x[v]) - z}1 .
n
(x[u],x[v],z)∈Dtrain
8:	return Independent if L(Dtrain, f) > 0.25 — β2∕103 else return Dependent.
We solve this task using the IndTest algorithm (Algorithm 5) which takes as input a dataset of
observation pairs D = {(χ(i,1),χ(i⑵)}n=1 where x(i,1),x(i,2) 〜PD(∙, ∙), and a scalar β ∈ (0,1).
We use D to create a dataset Dtrain of real and imposter atom pairs (x[u], x[v]). This is done by
taking every datapoint in D and sampling a Bernoulli random variable Zi 〜Bern(1∕1) (line 1). If
zi = 1 then we add the real pair (x(i,1) [u], x(i,1) [v], 1) to Dtrain (line 4), otherwise we add the
imposter pair (x(i,1) [u], x(i,2) [v], 0) (line 6). We train a classifier to predict if a given atom pair
(x[u], x[v]) is real or an imposter (line 7). The Bayes optimal classifier for this problem is given by:
∀x ∈ SUpp(PD ), fD (x[u],x[v])：= PD (Z = 1 | x[u],x[v])= PD (χ[u],X[D]()TPχ(X][U])P(χ[v]).
If x[u] and x[v] are independent then we have PD(x[u], x[v]) = PD(x[u])PD(x[v]) everywhere on
the support of PD. This implies fDD (x) = ɪ and its training loss will concentrate around 0.25. Intu-
itively, this can be interpreted as the classifier having no information to tell real samples from imposter
samples. However, if x[u], x[v] are dependent and kPD(x[u], x[v]) — PD(x[u])PD(x[v])kTV ≥ β
then we can show the training loss of f? is less than 0.25 — O(β2) with high probability. The
remainder of this section is devoted to a rigorous proof for this argument.
C.2 Analysis of Algorithm 5
Before analyzing Algorithm 5, we want to slightly simplify the problem in terms of notations. We
introduce two simple notations X and Y which represents the random variables x[u] and x[v],
respectively. We will simply use D to denote the joint distribution of X and Y . Define Dtrain to be
the distribution of the training data (Xtrain, Ytrain, Z) produced in Algorithm 5. It’s easy to verify that
Dtrain(Xtrain,Ytrain, Z) = 2 [zD(Xtrain, Ytrain) + (1 — Z)D(Xtrain)D(Ytrain)] ∙	(I)
Suppose the distribution D is specially designed such that at least one of the following hypothesis
holds (which can be guaranteed when we invoke Algorithm 5):
Ho ： kD(X,Y) — D(X)D(Y)k1 ≥ β
β2
v.s. H1 : kD(X,Y) — D(X)D(Y)k1 ≤ a.
In the remaining part, we will prove that Algorithm 5 can correctly distinguish between H0 and H1
with high probability.
15
Published as a conference paper at ICLR 2021
C.2.1 Two properties of the Bayes optimal classifier
Our first lemma shows that the Bayes optimal classifier for the optimization problem in line 7 is a
constant function equal to 1/2 if X and Y are independent.
Lemma 1 (Bayes Optimal Classifier for Independent Case). In Algorithm 5, if X and Y (atoms u
and v) are independent under distribution D, then for the optimization problem in line 7 , the Bayes
optimal classifier is given by:
∀(Xtrain , Ytrain )
f? (Xtrain , Ytrain)
1
2
Proof. From Bayes rule we have:
f? (Xtrain , Ytrain)
=Dtrain (z = 1 | Xt
rain , Ytrain)
Dtrain (Xtrain , Ytrain | z = 1)Dtrain(z = 1)
Dtrain(Xtrain, Ytrain | z = 1)Dtrain(z = 1) + Dtrain(Xtrain, Ytrain | z = 0)Dtrain(z = 0)
Dtrain (Xtrain , Ytrain | z = 1)
Dtrain (Xtrain , Ytrain | z = 1) + Dtrain(Xtrain, Ytrain | z = 0)
where the last identity uses Dtrain(z = 1) = Dtrain(z = 0) = 1/2.
When z = 0, we collect Xtrain and Ytrain from two independent samples. Therefore, we have
Dtrain (Xtrain, Ytrain | z = 0) = D(Xtrain)D(Ytrain).
When z = 1, using the fact that Xtrain and Ytrain are independent under distribution D, we also have
Dtrain(Xtrain, Ytrain | z = 1) = D(Xtrain, Ytrain) = D(Xtrain)D(Ytrain).
Consequently, f?(Xtrain, Ytrain) ≡ 1/2.
□
Our second lemma provides an upper bound for the expected training loss of the Bayes Optimal
Classifier. Later we will use this lemma to show the training loss is less than 0.25 - O(β2) with high
probability when H0 holds.
Lemma 2 (Square Loss of the Bayes Optimal Classifier). In Algorithm 5 line 7, the Bayes optimal
classifier has expected square loss
EDtrainLf?, Dtrain) ≤ 4 - 2E ED
1 _	D(X,Y)
2 D(X )D(Y) + D(X,Y)
Proof. Recall the formula of the Bayes optimal classifier in Lemma 1,
D(X,Y)
D(X)D(Y) + D(X,Y).
Plugging it into the square loss, we obtain
EDtrain (f?(X,Y)-y)2
=EDtrain hf?(X,Y)(f?(X,Y)-1)2+(1-f?(X,Y))(f?(X,Y))2i
=EDtrain[f?(X,Y)(1-f?(X,Y))]
1 _E “二	D(X,Y)
4	Dtrain [ VE	D(X)D(Y) + D(X, Y)
≤ 4 — (EDtrat
1 _	D(X,Y)
2 D(X )D(Y) + D(X,Y)
1 —	D(X,Y)
2 D(X )D(Y) + D(X,Y)
□
16
Published as a conference paper at ICLR 2021
C.2.2 Three useful lemmas
To proceed, we need to take a detour and prove three useful technical lemmas.
Lemma 3. Let μ and V be two probability measures defined on a countable set X. If ∣∣μ 一 VkTV ≥ c,
then
E 〜 _μ(x)__________1 ≥ c.
x μ μ(x) + v(x)	2 — 4
Proof.
Ex〜μ
μ(x)	1
μ(x) + v(x)	2
μ(x) 一 v(x)
μ(x) + v (x)
≥ 2 Ex〜μ
1{μ(x) > v(x)}
μ(x) 一 v(x)
μ(x) + V (x)
1
2
1 X μ(X) 1{μ(X) > V(X)} μ(x) 一 V(X)
2 X∈X	L	μ(X) +V(X)
≥ 1 X [1{μ(X) > V(X)} (μ(X)-V(x))]
4 x∈X
c
≥ 4.
□
Lemma 4. Fix δ ∈ (0, 1). Then with probability at least 1 一 δ, we have
∣L(∕, Dtrain)- EDtrain L(f ?, Dtrain)I ≤ 1。ʌ/CF^ ,
where Dtrain is the training set consisting of n i.i.d. samples sampled from Dtrain, f is the empirical
minimizer of L(f, Dtrain) over F, f ? is the population minimizer, and C(F, δ) := ln 存 is the
complexity measure of function class F.
Proof. By Hoeffding’s inequality and union bound, with probability at least 1 一 δ, for every f ∈ F,
We have
|L(f, Dtrain)- EDtrain L(f, Dtrain)1 ≤ 1。JCF∙
ʌ
Because f is the empirical optimizer,
L(f, Dtrain) ≤ L(f ?, Dtrain) ≤ EDtrain L(f? Dtrain) + 1。JCFL∙
Because f? is the population optimizer,
L(f, Dtrain) ≥ EDtrainL(f, Dtrain) 一 ∖/CF≥ EDtrain L(f ?, Dtrain)- 1Q^ CF^ .
tran	n	tran	n
Combining the two inequalities above, We finish the proof.	□
For notational convenience, We introduce the folloWing factor distribution Dfactor defined on the
same domain of (X, Y, z):
Dfactor (X,Y,z) = 1 D(X )D(Y).
Lemma 5. Suppose F contains the constant function f ≡ 1/2. Then with probability at least 1 - δ,
we have
L(∕, Dtrain)- 4
≤10 JCF^+2kDtrain - DfactOrkTV.
17
Published as a conference paper at ICLR 2021
Proof. By Lemma 4, with probability at least 1 - δ, we have
∣L(∕, Dtrain)- EDtrainL(f*, Dtrain)I ≤ 1。JCF^ .	⑵
Noticing that L is bounded by 1, we have for every f ∈ F
|EDfactorL(f,Dtrain)-EDtrainL(f,Dtrain)| ≤2kDtrain-DfactorkTV,	(3)
where EDfactorL(f, Dtrain) defines the expected loss of f over Dtrain where Dtrain consists of
samples i.i.d. sampled from Dfactor.
Since y is a symmetric Bernoulli r.v. independent of (X, Y ) under distribution Dfactor, we have
miF EDfactorL(f,Dtrain)=1 ∙	(4)
Using the inequality | minf L1(f ) - minf L2(f )| ≤ maxf |L1(f ) - L2(f )| for any functionals
L1 , L2 , along with (4) and (3) we bound the minimum loss under distribution Dtrain as:
miF EDtrainL(f, Dtrain)- | ≤ 2∣∣Dtrain - DfactorkTV.	(5)
Combining (5) With (2) completes the proof.	□
C.2.3 Main theorem for Algorithm 5
Finally, We are ready to state and prove the main theorem for Algorithm 5.
Theorem 2. Under the realizability assumption and n ≥ Ω(CFδ), Algorithm 5 can distinguish
β
Ho ： kD(X,Y) — D(X)D(Y)kι ≥ 2
β2
v.s. Hi : kD(X,Y) - D(X)D(Y)kι ≤ 京
correctly with probability at least 1 - δ.
Proof. If H1 holds, by Lemma 5, We have the folloWing loWer bound for the training loss of the
empirical minimizer,
L(S ≥ 4 — 10户子 — 襦.
(6)
In contrast, if H0 is true, applying Lemma 4, We obtain
L(f, D) ≤ EDtrain L(fR + ^ ^CFδL .
Invoke Lemma 2 and Lemma 3,
EDtrainL(f*, Dtrain) ≤ 4 — ($ED
1 β2
≤ 4 - 256.
1 _ D(X,Y)
2 D(X )D(Y) + D(X,Y)
Therefore, under H0, the training loss of the empirical minimizer is upper bounded as beloW
”S ≤ 1 - 总 + 10「
4 256 n
Plugging n ≥ O( C(FlM) into (6) and (7), We complete the proof.
(7)
□
18
Published as a conference paper at ICLR 2021
D Theoretical Analysis of FactoRL
In this section, we provide a detailed theoretical analysis of FactoRL. The structure of the algorithm
is iterative making an inductive case argument appealing. We will, therefore, make an induction
hypothesis for each time step that we will guarantee at the end of the time step.
Induction Hypothesis. We make the following induction assumption for FactoRL under Assump-
tion 1-3 and across all time steps. For all t ∈ {2,3, •…，H}, at the end of time step t (AlgO-
rithm 1, line 8), FactoRL finds a child function ccht : [d] → 2[m], a decoder φbt : X → {0, 1}d, a
transition function Tbt : {0, 1}d × A → {0, 1}d, and a set of policies Ψt satisfying the following:
IH.1
IH.2
IH.3
IH.4
—∙^~∙
cht : [d] → 2m and cht : [d] → 2m are same upto relabeling, i.e, for all u, v ∈ [m] we
-1	-1
have cht (u) = cht (v) if and only if cht-1 (u) = cht-1(v). Note that a child function is
—∙^~∙
invertible by definition. We can ignore this label permutation and assume cht = cht for
—∙^~∙
cleaner expressions. This can be done without any effect. We will assume cht = cht when
stating the next three induction hypothesis.
There exists a permutation mapping θt : {0,1}d → {0,1}d and % ∈ (0,品)such that for
every i ∈ [d] and s ∈ St we have:
P(φbt (xt)[i] = θt-1(s)[i] | s[i]) ≥ 1 - %,
1
P(bt(χt) = θ-1(s) |S) ≥ 1 - d% ≥ 2
The two distributions are independent of the roll-in distribution at time step t. The first one
—∙^~∙
^
holds as φt(xt)[i] only depends upon the value xt[cht(i)] = xt[cht(i)] which only depends
on s[i]. The second one holds as xt is independent of everything else given st. The form of
% will become clear the end of analysis.
For every s ∈ St-1, s0 ∈ St and a ∈ A we have:
Tbt(θt-1(s0) | θt--11(s), a) - T (s0 | s, a)	≤ 3d(∆est + ∆app),
where ∆est , ∆app > 0 denote estimation and approximation errors whose form will become
clear at the end of analysis.
For every s ∈ St and K ∈ C≤2κ([d]), let Z = s[K] and Zb = θt-1(s)[K], then there exists a
policy πKZb ∈ Ψt such that:
PπKZb (st [K] = Z) ≥ αηt (K, Z) ≥ αηmin.
Base Case. In the base case (t = 1), we have a deterministic start state. Therefore, we can without
—∙^~∙
loss of generality assume a single factor and define ch1 [1] = m. As we can also define ch1 [1] = [m]
without loss of generality, therefore, this trivially satisfies the induction hypothesis 1. We define
φb1 : X → [0]d (Algorithm 1, line 1). This satisfies induction hypothesis 2 with θ1 being the identity
map. The induction hypothesis 3 is vacuous since there is no transition function before time step
1. For the last condition, we have for any K, Z = [0]|K| and Zb = [0]|K|. For any policy π we have
P∏(sι[K] = Z) = Pn(Φ1(x1)[K] = Z) = 1 ≥ Tnmin. Note that we never take any action from this
policy, therefore, We can simply define Ψι = 0.
D. 1 Graph structure identification
In this section, we analyze the performance of Algorithm 2, given as input Ψh-1, φh-1, F, β and n.
We will analyze the performance a fixed pair of atoms u, v ∈ [m] and then apply the full result using
union bound. We first state the result for the rejection sampling.
Lemma 6. For policy ∏τ.b ∈ Ψh-ι, event 8工.玄={^h-ι[I] = Z} and k ∈ N, let DrIjb ：=
RejectSamp(∏ι.Zb，Ei;b，k) be the distribution induced by our rejection sampling procedure. Let
19
Published as a conference paper at ICLR 2021
Z = θ(Z) denote the real factor values corresponding Z. Then we have:
PDreject (sh-1[I] = Z) ≥ 1 - % - (1 - "min )
(8)
Proof. From IH.4 We have Pn …(sh-ι[I] = Z) ≥ ηmin. This implies:
I;Z	2
- - ^. --∙^∙. -- -- .
PnI;Zb(Sh-1 [I] = Z) ≥ PnIZ(sh-ι[I] = Z | Sh-ι[I]= Z)P∏I;Z(sh-ι[I] = Z)
≥ (1 - d%)ηmin ≥ 午, (using IH.2 and IH.4).
Let a = PnA茎(^h-1 [I] = Z) be the acceptance probability of event EI;b. then it is easy to See that
the probability of the event occurring under DIre;jZebct is:
PDreject(EI.Z) = a +(1 - a) a +(1 - a)2a + …(1 - a)k-1a = 1 - (1 - a)k ≥ 1 -(1 - ηmn )k
We express the desired failure probability as shoWn:
PDreject (sh-i[I ] = Z )= PD 瞑t (sh-i[I ] = Z, Sh-i∖I ] = Z) +PD 峻t (sh-i[I ] = Z, ^h-i[I] = N
I;Zb	I;Zb	I;Zb
(9)
We bound the tWo terms beloW:
PD霁(Sh-1 [I]	= Z,Sh-1 [I] =	Z)	≤ PD窦(^h-ι[I]	= Zb)	≤(1	-	ηmin广	(10)
PDr吧卜h-1[I] = Z, sh-1 [I] = Z) ≤ PDreject 卜h-1[I]=2 | sh-1[I] = Z,) ≤ % (II)
Combining Equation 9, Equation 10 and Equation 11 We get:
PDreject (sh-1[Ζ] = Z) = 1 - PDreject (sh-1[Ζ] = Z) ≥ 1 - % - (1 - -ʒj-) .	(12)
I;Zb	I;Zb	4
□
We noW analyze the situation for a given pair of atoms. Recall for any distribution D ∈ ∆(Sh-1)
and a ∈ A, we denote D ◦ a as the distribution over Sh where s0 〜D ◦ a is sampled by sampling
s 〜D and then s0 〜T(. | s, a). We want to derive roll-in distributions at time step h, such that
atoms coming from the same parent satisfy hypothesis H0 and atoms coming from different parents
satisfy hypothesis H1 under this roll-in distribution. This will allow us to use independence test to
identify the parent structure in the emission process. Specifically, we consider the roll-in distributions
induced by Drejebct ◦ a for some sets I, J and action a. Instantiating the definition of these hypothesis
I ;Z
from Appendix C, with these roll-in distributions and setting β = βmin gives us:
H0 ： kPDrej≈‰ (X [u],x[v]) - PDrej≈‰(X [u])PD 丽 °。(X [v])k1 ≥ βmn
I;Zb	I;Zb	I;Zb	2
β2
v.s. H1 ： kPDrej≈‰(X[u],x[v]) - PDrej≈‰(X [u])PDrej≈‰(X [v])k1 ≤ —in
I;Zb	I;Zb	I;Zb	100
Lemma 7 (Same Factors). If for two atoms u, v we have chh-1 (u) = chh-1 (v), i.e., they are from the
same factor then the hypothesis H0 is true for D ◦ a for any D ∈ ∆(Sh-1) and a ∈ A. In particular,
this is true for DIre;jZebct ◦ afor any choice of sets I, J and action a.
Proof. Follows trivially from Assumption 2.
□
Lemma 8 (Different Factors). If for two atoms u, v we have chh-1(u) = i and chh-1(v) = j and
i = j, then if I contains pth(i) ∪ pth(j), thenfor % ≤ 1∣00 and k ≥ n-8一 ln	^, the hypothesis
H holds for DIjZcto a for any Z such that ∏τ.bb ∈ Ψh-1 and a ∈ A.
20
Published as a conference paper at ICLR 2021
Proof. Let D0 ∈ ∆(Sh-1) be a distribution that deterministically sets sh-1 [I] = Z. Then it is easy
to verify that PDo°a(χh[u],Xh[v]) = P do ◦a (xh [u])P do ◦a (xh [v]) for any a ∈ A and Xh ∈ Xh.
Then for any Zb and action a ∈ A we have using triangle inequality:
PDreject ◦a(xh[u], xh [v]) - PDreject ◦a(xh [u])PDreject ◦a(xh [v])
I;Zb	I;Zb	I;Zb	1
≤ PDrejectoa(xh[u],Xh[v]) - P Do oa(xh[u],Xh [v]) +
I;Zb	1
PD0oa(Xh[u]) - PDrejectoa(Xh[u])	+ P。，oa ^h [v]) - PDrej戈％a (xh [v])
I;Zb	1	I;Zb	1
As xh[u] and xh [v] come from different factors, therefore, we have
P(xh[u], xh[v] | sh-1, a) = P(xh[u] | sh-1[I], a)P(xh [v] | sh-1[I], a).
We use this to bound the three terms in the summation above.
PDreject ◦a(xh [u],Xh[v]) - PD0oa(xh[u],Xh[v])l
I;Zb	1
= X	ll	X	P(xh[u]	|	sh-1[I], a)P(xh[v]	| sh-1[I],	a)	PDreject (sh-1 [I]) - PDo(sh-1[I])
xh [u],xh [v] lsh-1 [I]	;
≤	P(xh	[u]	|	sh-1	[I],	a)P(xh	[v]	|	sh-1	[I],	a)	PDreject (sh-1	[I]) -	PDo (sh-1 [I])
l	I;Zb
sh-1 [I] xh [u],xh [v]
≤
sh-1 [I]
PDreject (sh-1 [I]) - PDo (sh-1 [I])
I;Zb
= ll1 - PDreject (sh-1 [I] = Z)ll + X PDreject (sh-1 [I])
l	I;Zb	l	I;Zb
sh-1 [I]6=Z
=2(1- PDreject (Sh-1 [I]=Z)) ≤ 2%+2(1- nmn).
The other two terms are bounded similarly which gives us:
PDrejK‰ (Xh [u], Xh [v]) - PDrejectoa (Xh [u])P Do oa (Xh [v])
I;Zb	I;Zb
≤ 6% + 6(1 - ηmn)k
We want this quantity to be less than 需 to satisfy hypothesis Hi. We distribute the errors equally
and use ln(1 + a) ≤ a for all a > -1 to get:
β2
% ≤ ɪmin-
% 一 1200,
k ≥ 8 in
ηmin
(13)
□
Theorem 3 (Learning ch). Fix δn ∈ (0,1). If % ≤ ImO and k ≥ n8- in (肃)and nn ≥
O (含 in m AIF(2ed)—~) , then learned Chh is equivalent to Chh Upto label permutation with
probability at least 1 - δind.
Proof. For any pair of atom u, v, if they are from the same factor then H0 holds from Lemma 7 and
IndTest mark them dependent with probability at least 1 - δ. This holds for every triplet of I, Z , a
and there are at most ∣A∣(2ed)2κ+i, of them. Hence, from union bound we mark u, V correctly as
coming from different factors with probability at least 1 - ∣A∣(2ed)2κ+1δ.
If u and v have different factors then for any I containing the parents of both of them, and any value
of Z and a, H1 always holds from Lemma 8 and IndTest marks them as independent. Note that
21
Published as a conference paper at ICLR 2021
Figure 2: Scheme showing the important variables for the decoding step.
such an I will exists since the we iterate over all possible sets of size upto 2κ. Hence, with probability
at least 1 - ∣A∣2κδ, We find U and V to be independent for every Z and a. Hence, our algorithm
correctly will mark them as coming from different factors.
For a given U and v, we correctly predict their output with probability at least 1 - ∣A∣(2ed)2κ+1δ.
Therefore, using union bound We correctly output right result for each u and v With probability at least
1 — |A|m2 (2ed)2κ+1δ. From Theorem 2, we require nind ≥ O (大 ln IFɪ). Binding ∣A∣(2ed)2κ+1δ
to δind then gives us the required value of nind to achieve a success probability of at least 1 - δind. If we
correctly assess the dependence for every pair of atoms correctly, then trivially partitioning them using
一^一、
the dependence equivalence relation gives US Chh which is same as Chh upto label permutation. □
D.2 Learning a State Decoder
We focus on the task of learning an abstraction at time step h using Algorithm 3. We have access to
ʌ
Chh which is same as Chh upto label permutation. We showed how to do this in Appendix C. We
will ignore the label permutation to avoid having to complicate our notations. This would essentially
mean that we will recover a backward decoder φh = I φhι,…，φhd 1, where there is a bijection
between {φhi}i and {φ? },
∕∖'
As we learn each decoder {φhi} independently of each other, therefore, we will focus on learning
the decoder φhi for a fixed i. The same analysis will hold for other decoder and with application of
union bound, we will establish guarantees for all decoders. Further, since we are learning the decoder
at a fixed time step h, therefore, we will drop the h from the subscript for brevity. We use additional
shorthand described below and visualize some of them in Figure 2.
•	s and x denote a state sh-1 and an observation xh-1 at time step h - 1
•	s0 and x0 denotes state sh and observation xh at time step h
•	s0 [i] denotes ith factor of state at time step h
•	S denotes s∖p-th (i)] which is the set of parent factors of s0[i]. Recall that from the factoriza-
tion assumption, we have T(s0[i] | s, a) = Ti(s0[i] | S, a) for any s, a.
th
•	φi denotes φhi decoder for ith factor at time step h
•	ω denotes pt(i) which is the set of indices of atoms emitted by S0[i].
•	X denotes χ0[chh,(i)] which is the collection of atoms generated by s0[i].
•	N = ∣Ψh-ι | is the size of policy cover for previous time step.
Let D = {(x⑹，a⑹,X(k),y⑹)}n=sι be a dataset of nabs real transitions (y = 1) and imposter
transitions (y = 0) collected in Algorithm 3, line 2-4. We define the expected risk minimizer (ERM)
solution as:
nabs	2
gi = arg min--------^X {g(x(k), a(k),X(k)) — y(k)}
abs k=1
(14)
22
Published as a conference paper at ICLR 2021
1 ʌ	11,1 , 1	,1	,	,	CC	1	ʌ	/人 G' 1	人 一、A，	1 x? _ T	GZ'±	Cc 1 T
Recall that by the structure of G, We have gi = (Ui) φi) where Wi ∈ W2 and φi ∈ Φ : X* → {0,1}
is the learned decoder. Our algorithm only cares about the properties of the decoder and we throw
away the regressor Ui.
Let D(x, a, X) be the marginal distribution over transitions. We get the marginal distribution by
marginalizing out the real (y = 1) and imposter transition (y = 0). We also define D(x, a) as the
marginal distribution over x, a. We have D(x,a) = μh-ι(x)备 as both real and imposter transitions
involve sampling X 〜μh-ι and taking action uniformly. Recall that μh-ι is generated by roll-in with
a uniformly selected policy in Ψh-ι till time step h - 1. Let P(x,a,X | y = 1) be the probability of
a transition being real and P(x, a,X | y = 0) be the probability of the transition being imposter. We
can express these probabilities as:
P(x, a,X | y =1)= D(x, a)T(X | x, a),	P(x, a,X | y = 0) = D(x, a)ρ(X),	(15)
where ρ(X) = E(χ,a)〜D [T(X | x,a)] is the marginal distribution over X. We will overload the
notation P to also define p(x0) = E(χ,a)〜D[T(x0 | x, a)]. Lastly, we can express the marginal
distribution over transition as:
D(x, a, X) = P(x, a, X | y = 1)P(y = 1)+ P(x, a, X | y = 0)P(y = 0)
="h-A") {T (χ | χ,a)+P(X)}
We start by expressing the Bayes optimal classifier for problem in Equation 14.
Lemma 9 (Bayes Optimal Classifier). Bayes optimal classifier g? for problem in Equation 14 is
given by:
∀(χ, a, X) ∈ SuppD,
?( n =	TXφ?(X) | φ?(X)[pt(i)],a)
g ( , , ) = Ti(0?(X) | φ*(χ)[pt(i)],a) + ρ(φ}(X))
(16)
Proof. The Bayes optimal classifier is given by g?(x, a, X) = P(y = 1 | χ, a, X) which can be
expressed using Bayes rule as:
P(y = 1 | χ, a, X)
____________P(χ,a,X | y = 1)P(y = 1)___________
P(χ, a, X | y = 1)P(y = 1) + P(χ, a,x | y = 0)P(y = 0)
京-------IP(X,a)χ|怖 J I) 1---------T,	usingp(y) = Bern(I∕2)
P(χ, a, χ | y = 1)+ P(χ, a, χ | y = 0)
D(χ, a)T(X | χ, a)
D(χ, a)T(X | χ, a) + D(χ, a)ρ(X)
T(X | χ, a)
T(X | χ, a) + ρ(X)
___________qi(X | φ?(X))Ti(φ?(X) | χ,a)_________
qi(χ | φ?(X))Ti(φ?(X) | χ, a) + qi(χ | φ?(X))P(φ?(X))
Ti(φ?(X) | χ,a)	=	Ti(φ?(X) | φ*(χ)[Pt(i)],a)
Ti(0?(X) | X, a) + ρ(φ?(X))	Ti(φ?(X) | φ*(χ)[pt(i)],a) + ρ(φ}(X))
□
Theorem 4 (Decoder Regression Guarantees). For any given δabs ∈ (0, 1) and nabs ∈ N we have the
following with probability at least 1 - δabs:
Ex,a,历〜D kgi(χ,a,X) - g?(x,a,X))2] ≤ ∆(nabs,δabs, |G|),
where ∆(nabs, δabs, |G|) := nc^ ln δG- and C is a universal constant.
Proof. This is a standard regression guarantee derived using Bernstein’s inequality with realizability
(3). For example, see Proposition 11 in Misra et al. (2020) for proof.	□
23
Published as a conference paper at ICLR 2021
Corollary 5. For any given δabs ∈ (0, 1) and nabs ∈ N we have the following with probability at
least 1 - δabs:
Ex,a,而〜D [lgi (x, a, X) - g (X, a, X) |] ≤ *∖∕δ5abs, δabs, |GI)	(17)
Proof. Applying Jensen's inequality (E[√Z] ≤，E[Z]) to Theorem 4 gives us:
Eχ,a,x〜D [∣g(x, α,X) - g*(x,α,X)∣] = Eχ,a,x〜D J∣g(x, α,X) - g*(x,a,X)∣2
≤ 'Kχ,α,X〜D ^g(x, a, X) - g?(x, a, X))2]
≤ √∆(nabs,δabs, |G|).
□
Coupling Distribution We introduce a coupling distribution following Misra et al. (2020).
DcOUP(X,a,X1,X2) = D(x,a)ρ(X1)ρ(X2).	(18)
We also define the following quantity which will be useful for stating our results:
ξ(Xl, X2, X, a)
T(Xi ∣ X, a)
P(XI)
T(X2 I X, a)
P(X2)
(19)
Lemma 10. For any fixed δabs ∈ (0, 1) we have the following with probability at least 1 - δabs:
[l{bi(Xi) = bi(X2)} ∣ξ(X1,X2,X,a)∣] ≤ 8p∆(nabs,δabs, ∣G∣).
>Λ Z- ɪʃ T 1	，、	1	.1 -I .	. ∙	C	τ(^^7'∕-∖	G/-、、 C F	♦, ɪʃ T 1	IC
Proof. We define a shorthand notation E = 1{φi(Xι) = φi(X2)} for brevity. We also define a
different coupled distribution Dc0oup given below:
Dcoup(x, a, Xi, X2) = D(x, a)D(X1 ∣ X, a)D(X2 ∣ X, a)	(20)
where D(X ∣ x, a) = 2 {T(X ∣ x, a) + ρ(X)}. It is easy to see that marginal distribution of Dcoup
over χ, a, Xi is same as D(x, a, Xi).
We first use the definition of ξ (Equation 19) and g? (Equation 9) to express their relation:
|g?(x, a,Xi) - g?(x, a,X2)∣ =		P(Xi)P(X2) ∣ξ(Xi,X2,x,a)∣	 (T(Xi ∣ χ, a) + ρ(Xi))(T(Xi ∣ χ, a) + ρ(X2)) "P(Xi))Pffi)I	Jξ(Xi,X2,x,a)∣.	(21) 4D(Xi ∣ χ,a)D(χ2 ∣, χ, a)
The second line uses the definition of D(X ∣ χ, a). We can view DP(Xx)Ο)and DPxx)°)as importance
weight terms. Multiplying both sides by E and taking expectation with respect to Dc0oup then gives us:
EDCOUP [E |g?(X,a,Xi)- g?(x,a,X2)|] = 1 EDcoup [Elξ(χi,χ2,χ,a)1]	(22)
We bound the left hand side of Equation 22 as shown below:
EDCOUP [E|g?(X,a, Xi)- g?(X, a,χ2)|]
≤ EDCOUP	[E|g?(X,	a,χi) -	gi(χ,a,χi)1]	+ EDCOUP	[E|gi(x, a,χi) -	g*(χ,a,	X2)|]
=EDCOUP	[E |g?(X,	a,χi) -	gi(χ,a,χi)1]	+ EDCOUP	[Elgi(χ, a,χ2) -	g*(χ,a,	X2)|]
=2Edcoup [E |g?(x, a,Xi) - gi(χ,a,Xi)∣] = 2Ed [E ∣g?(χ,a,X) - g(χ, a,X)∣]
≤ 2√∆(nabs,δabs, G)
24
Published as a conference paper at ICLR 2021
Here the first inequality follows from triangle inequality. The second step is key where we use
gi(x, a, Xi) = ^i(x, a, X2) whenever E = 1. This itself follows from the bottleneck structure of G
where gi(x, a, Xi) = Wi(x, a, φi(x)). The third step uses the symmetry of xi and x2 In DcouP whereas
the fourth step uses the fact that marginal distribution of Dc0oup is same as D. Lastly, final inequality
uses E ≤ 1 and the result of Corollary 5. Combining the derived inequality with Equation 22 proves
our result.	□
We define the quantity P(s0[i] = Z | D0) := E(s,a)〜d，[1{s0[i] = z}] for any distribution D0 ∈
∆(Sh-i × A). From the definition of ρ, we have ρ(s0[i] = z) = P(s0[i] = z | D). Intuitively, as we
have policy cover at time step h - 1 and we take actions uniformly, therefore, we expect to have
good lower bound on P(s0[i] = z | D) for every i ∈ [d] and reachable z ∈ {0, 1}. Note that if z = 0
(z = 1) is not reachable then it means we always have sh [i] = 1 (sh [i] = 0) from our reachability
assumption (see Section 2). We formally prove this next which will be useful later.
Lemma 11. For any z ∈ {0, 1} such that s0[i] = z is reachable, we have:
P(s0[i] = Z)= P(s0[i] = z I D) ≥ ONmAL
Proof. Fix Z in {0, 1}. As s0[i] = Z is reachable, therefore, from the definition of ηmin we have:
ηmin ≤ sup P∏(s0[i] = Z) ≤ sup X Pn(S)T(s0[i] = Z | s,a)
π∈π	π s,a
≤ X Sup Pn⑸T(S0[i] | s,a) = X η(S)T(S0[i] | s, a)
s,a n∈π	s,a
We use the derived inequality to bound P(S0[i] = Z | D) as shown:
p(s0[i] = Z | D) = X "h-A(S)T(s'[i] = Z | s,a) ≥
N¾ Ση(S)T(S0[i]= Z1S,a) ≥ αW∙
The first inequality uses the fact that μh-ι is created by roll-in with a uniformly selected policy in
Ψh-i which is an α policy cover. Recall that N = ∣Ψh-ι∣. The second inequality uses the derived
result above.	□
Lemma 12. Forany X1,X2 such that 0?(Xi) and 0?(X2) is reachable, we have:
Ex,a〜D [lξ(xi,x2,x,a)1] ≥ 1{。?(XI) = φi(x2)} ^^°
Proof. For any x, a, X1,X2 We can express ξ (Equation 19) as:
∣ξ(X1,X2,x,a)∣
T(O?(Xi) I φ*(x)[pt(i)],a)	T(0?(X2) I φ*(x)[pt(i)],a)
_____________________
ρ(φ?(XI))____________ρ(φ*(χ2))
where we use the factorization assumption and decodability assumption. Note that we are implicitly
assuming 0?(Xi) and 0?(X2) are reachable, for the quantity ξ(X1,X2,χ, a) to be well defined.
We define Di to be the marginal distribution over S [pt(i)] × A. Taking expectation on both side
gives us:
Eχ,a〜D [∣ξ(Xi,X2,X,a)∣] = Eaa〜Di
T(。?(XI) | S,a)	T(。?(X2)| S,a)
______________ _ 	
ρ(φ?(XI))_____ρ(φrG¾))
EIPDi(S,a | φ?(XI))- PDi(R,a | ^?(X2))|
s,a
2 kPDi(.,.∣ φ?(XI))- PDi(.,.∣ 0?(X2))kTV
The second equality uses the definition of backward dynamics PDi over S[pt(i)] × A and the
identity ρ(S0[i]) = P(s0[i] ∣ D). If 0?(Xi) = 0?(X2) then the quantity on the right is 0. Otherwise,
this quantity is given by 2 kPDi (., . I S0[i] = 1) - PDi (., . I S0[i] = 0kTV. In the later case, both
25
Published as a conference paper at ICLR 2021
s0[i] = 1 and s0[i] = 0 configurations are reachable, and without loss of generality we can assume
P(s0[i] = 0 | D) ≥ 1/2. Our goal is to bound this term using the margin assumption (Assumption 1).
We do so using importance weight as shown below:
2kPDi(.,.|s0[i]=1)-PDi(.,.|s0[i]=0)kTV
Σ
S,a
PUi (s,a | s0 [i] = I)
PDi(R,a | s0[i] = I)
Pui (S, a | s0[i] = 1)
- Pui (sR, a | s0 [i] = 0)
PDi(S,a | s0[i] = 0)
Pui(S,a | s0[i] = 0)
Di(S, a)
七 ui(s,a)
Pui (SR, a | S0 [i]
1)
P(s0[i] = 1 | Ui)
P(s0[i] = 1 | Di)
- Pui (SR, a | S0 [i] = 0)
P(s0[i] =0 | Ui)
P(s0[i] = 0 | Di)
≥ min D^P(s0[i]=0 | Di)
s,a Ui(S, a) P(s0[i] = 0 | Ui)
kPui (., . | S0[i] = 1) - Pui (., . | S0[i] = 0)kTV
≥ i Di(S,a) P(s0[i]=0 | Di)
-Isl,O1 Ui(S, a) P(s0[i] = 0 | Ui) σ
The first step applies importance weight. As Ui has support over all reachable configurations SR
and actions a ∈ A, hence, we can apply importance weight. The second step uses the definition
of backward dynamics (PD, Pui). The third step uses Lemma H.1 of Du et al. Du et al. (2019)
(see Lemma 24 in Appendix E for statement). Finally, the last step uses Assumption 1. We bound the
two multiplicative terms below:
We have D(S, a) = μh-ι(S)备 ≥ ONmn. The first equality uses the fact that actions are taken
uniformly and second inequality uses the fact that μh-ι is an α-policy cover. As Ui is the uniform
distribution over S[pt(i)] X A, therefore, We have Ui(S, a) = 23(1)||4 . This gives us /(；，：) ≥
αηmmin 2∣pt(i)∣. We bound the other multiplicative term as shown below:
PU=0 | Di)	≥	P(s0[i]=0	|	D∙)	≥ 1
P(s0[i]=0 | Ui)	≥	P(S [i]	1	Di)	— 2.
Combining the lower bounds for the two multiplicative terms and using 2|pt(i)| ≥ 1 we get:
2 kPDi(.,. | s0[i] = 1) - PDi(.,. | s0[i] = 0)kTV ≥ αηgnσ∙	(23)
Lastly, recall that our desired result is given by 2 kPDi (., . | S0[i] = 1) - PDi (., . | S0[i] = 0)kTV
whenever 0?(Xi) = φ^?(X2) and 0 otherwise. Therefore, using the derived lower bound multiplied by
1{φ:(Xι) = φ:(Xz)} gives us the desired result.	□
Corollary 6. We have the following with probability at least 1 - δabs:
Ex1 ,x2~p [1{φi(XI) = bi(x2)}1{φ?(XI) = 0?(x2)}] ≤
16N
αηminσ
,∆(n abs ,δ ab ,|G|).
Proof. The proof trivially follows from applying the bound in Lemma 12 to Lemma 10 as shown
below:
Ex,a,xι,5⅛〜DOTlP [1{φi(XI) = bi(x2)} lξ(x1, x2, x, a) |]
=E历1 ,而2〜P [l{bi(Xι) = bi(X2)}Ex,a,〜D [∣ξ(Xl,X2,X, a)|]]
≥ QnNnOExι,x2〜ρ [[1{bi(XI)= bi(x2)}1{φ?(XI) = 0?(x2)}]
The inequality here uses Lemma 12. The left hand side is bounded by 8∙∖∕∆(nabs, δabs, |G|) us-
ing Lemma 10. Combining the two bounds and rearranging the terms proves the result.	□
At this point we analyze the two cases separately. In the first case, S0[i] can be set to both 0 and 1. In
the second case, S0 [i] can only be set to one of the values and we will call S0[i] as degenerate at time
step h. We will show how we can detect the second case, at which point we just output a decoder that
always outputs 0. We analyze the first case below.
26
Published as a conference paper at ICLR 2021
D.2.1 CASE A: WHEN s0[i] CAN TAKE ALL VALUES
.—.
Corollary 6 allows us to define a correspondence between learned state (i.e., output of φi) and the
actual state (i.e., output of φi?). We show this correspondence in the result.
Theorem 7 (Correspondence Theorem). For any state factor s0[i], there exists exists U° ∈ {0,1}
and ^ι = 1 一 U° with probability at least 1 一 δabs such that:
P(φi(x) = U0 | s [i]=0) ≥ 1 ― %,
P(φi(x) = Ui | s [i] = 1) ≥ 1 一 %,
where % := 12^叱 P∆(nabs,δabs, |G|) and X 〜ρ, provided % ∈(0, 2).
α ηmin σ
Proof. For any U, z ∈ {0, 1} we define the following quantities:
.—.
Pz := Ex〜ρ[1{φ?(X)= z}],	Puz := E历〜ρ[1{φi(X)= u}1{φ?(X) = z}].
It is easy to see that these quantities are related by: PZ = PUZ + P(I-U)Z. We define Uo =
arg maxu∈{o,i} PUo and Ui = 1 一 Uo. This can be viewed as the learned bit value which is in most
correspondence with s[i] = 0. We will derive lower bound on pu00∕P0 and Pu∖∖∣Ρ∖ which gives us the
desired result. We first derive the following lower bound on PU。。：
≥ P
PUoθ ≥
PUoθ + PUIO
2
(24)
where we use the fact that max is greater than average. Further, for any U, z ∈ {0, 1} we have:
E¾,送〜ρ [1{φi(Xi) = φi(χ2)}1{φ?(Xi) = φt(x2)}]
≥ ExI ,x2〜P
[l{φi(Xi) = U}l{φi(X2) = U}1{φ?(Xi) = z}1{φ}(X2) = 1 一 z"
PUzPU(i-z)
We define a shorthand notation ∆0 := ɑ；6^ /∆(nabs, δabs, |G|). Then from Corollary 6 we have
proven that PUzPU(i-z) ≤ ∆0 for any U, z ∈ {0, 1}. This allows us to write:
∆0
PUIi = Pi - PUOi ≥ Pi 一 万—	⇒
PUoO
PU i	∆0	2∆0
-^1i ≥ 1---------≥ 1--------
Pi -	PiPUoO -	POPi
where the last inequality uses Equation 24. We will derive the same result for Puoo/Po.
PUo0 = P0 - PU10 ≥ P0 — Pb ⇒ PPT ≥ 1 — P⅛7 ≥ 1 — POP： 2∆0,
where the last inequality uses derived bound for Pu∖∖∕Pi. If we assume ∆0 ≤ P0P1- then we get
PuOO、1	2∆0
≥ 1 一 PP.
As PO + Pi = 1, therefore, we get POPi = PO — P(2 = Pi - P2. If PO ≤ i then PO 一 Pl2 ≥ PO.
Otherwise, PO > 2 which implies Pi ≤ 2 and Pi 一 P2 ≥ PL. This gives us POPi ≥ min{P, PL}.
Using lower bounds for PO and Pi from Lemma 11 gives us POPi ≥ OnmA^, and allows us to write:
PUii 、 4	4N∣A∣∆0	PUoO 、 4	4N∣A∣∆0
≥ — 1	,	≥ — 1	.
Pi	αηmin	PO	αηmin
It is easy to verify that % = 4；『心.AS PP0o = Ρ(φi(X0) = Uo | s[i] = 0) and PPL = P(φi(x0)=
Ui | s[i] = 1), therefore, we prove our result. The only requirement we used is that ∆0 ≤ P0P1-
which is ensured if % ∈(0, i).	□
27
Published as a conference paper at ICLR 2021
D.2.2 CASE B: WHEN s0[i] TAKES A SINGLE VALUE
We want to be able to detect this case with high probability so that we can learn a degenerate decoder
that only takes value 0. This would trivially give us a correspondence result similar to Theorem 7.
We describe the general form of the LearnDecoder subroutine in Algorithm 6. The key difference
from the case We covered earlier is line 6-10. For a given factor i, We first learn the model gi
containing the decoder φi , as before using noise contrastive learning. We then sample ndeg iid triplets
Ddeg = {(xj,aj,Xj)}jn=ι where (xj, aj)〜D and Xj 〜P (line 6-8). Recall X = x[ch%(i)]. Next,
We compute the width of prediction values over Ddeg as defined beloW:
max Ig(Xj,aj,Xj) - g(xk,ak,Xk)|	(25)
j,k∈[ndeg]
If the Width is smaller than a certain value then We determine the factor to be degenerate and output a
degenerate decoder φi := 0, otherWise, We stick to the decoder learned by our regressor task. The
form of sample size ndeg Will become clear at the end of analysis, and We Will determine the reason
for the choice of threshold for Width in line 9. Intuitively, if the latent factor only takes one value
then the optimal classifier Will alWays output 1/2 and so our prediction values should be close to one
another. HoWever, if the latent factor takes tWo values then the model prediction should be distinct.
一--
Algorithm 6 LearnDecoder(G, Ψh-1 , chh).
一--
for i in [dh], define ω = chh(i), D = 0, Ddeg
Child function has type cchh : [dh] → 2[m]
0 do
1
2
3
4
5
6
7
8
9
10
for nabs times do	// collect a dataset of real (y = 1) and imposter (y = 0) transitions
Sample (x(1), a(1),xo(1)), (x(2),a(2),x0(2))〜Unf (Ψh-ι) ◦ Unf(A) and y 〜Bern( 1)
If y = 1 then D — D ∪ (X⑴,a(I),x0⑴[ω],y) else D — D ∪ (X⑴,a(I),x0⑵[ω],y)
/、''、
gi := bi, φi = REG(D, G)	// train the decoder using noise-contrastive learning
for ndeg times do	// detect degenerate factors
Sample (x(1), a(1),X0(1)), (X(2),a(2),X0(2))〜Unf (Ψh-ι) ◦ Unf(A)
Ddeg — Ddeg ∪ {(X(1),a(1),X0(2))}
22
if maxj,k∈[ndeg] Igi(Xj,aj,Xj[ω]) - gi(Xk, a®,xk[ω])∣ ≤ 40⑼：;：问人| then//max over Ddeg
φi := 0	// output a decoder that alWays returns 0
return φb : X → {0, 1}dh Where for any X ∈ X and i ∈ [dh] We have φb(X)[i] = φbi(X[cchh(i)]).
For convenience, we define D0(x, a,X) = D(x, a)ρ(X), and so (Xj, aj, Xj)〜 D0. For brevity
reasons, we do not add additional qualifiers to differentiate Xj, aj-, Xj from the dataset of real and
imposter transitions, we used in the previous section for the regression task. In this part alone, we
will use Xj, aj, Xj to refer to the transitions collected for the purpose of detecting degenerate factors.
Lemma 13 (Markov Bound). Let {(Xj, aj-, Xj )}；=1 be a dataset ofiid transitions SamPledfrom D0.
Fix a > 0. Then with probability at least 1 一 δabs — 22ndegVZ包；abs'δabs'1GI) we have：
∀j ∈ [ndeg],	∣g(Xj,aj,Xj) - g?(Xj,aj,Xj I ≤ a.
Proof. It is straightforward to verify that for any (Xj, aj∙, Xj) we have D(Xj,aj∙, Xj) ≥ DO(Xj,aj,xj)/2.
Using Corollary 5 we get:
Ex,a,X 〜D0 [∣^(X,a, X) - g?(x, a,X)∣] ≤ 2√∆(nabs,δabs, |G|)
Let Ej denote the event {∣^(xj∙, aj-, Xj) - g*(xj∙, aj-, Xj)∣ ≤ a} and Ej be its negation, then:
P(∩n=1Ej) ≥ 1 - XT P(Ej) ≥ 1 - 2ndeg VZAnabs,δa⅛JG},
j=1	a
where the first inequality uses union bound and the second inequality uses Markov’s inequal-
ity. As Corollary 5 holds with probability δabs, our overall failure probability is at most
§ b + 2ndeg ,∆(nabs,δabs ,∣G∣)	口
28
Published as a conference paper at ICLR 2021
Lemma 14. For any reachable parent factor values s, action a ∈ A and reachable s0[i] ∈ {0,1},
we have D0(s, a, s0[i]) ≥
N⅛ ∙
22
Proof∙ We have D0(s, a, s0[i]) = "h-A∖，ρ(s0[i]) ≥ N2mi2, where used the induction hypothe-
IaI	N IaI
sis IH.4 that Ψ is an α-policy cover of Sh-ι and Lemma 11.	□
Lemma 15 (Degenerate Factors). Fix a > 0∙ If s0[i] only takes a single value then with probability
at least 1 - δabs -	nZ*5'、,abs@ We havee
max ∣^(xj,aj,Xj) - ^(xk,ak,Xk)| ≤ 2a
j,k∈[ndeg]
Proof∙ When s0[i] takes a single value then g? is the constant function 2. For any j and k we get the
following using Lemma 13 and triangle inequality.
∣g(xj,aj,Xj) — g(xk,ak,Xk)| ≤ ∣^(xj,aj,Xj) - g?(xj,a，j,Xj)| +
|g?(xk,ak,Xk) - g(xk,ak,Xk)| ≤ 2a.
□
Lemma 16 (Non Degenerate Factors). Fix a > 0 and assume ndeg ≥ N IAI , then we have:
α ηmin
α2ηm2inσ
max IIg(Xj ,aj ,xj ) g(Xk ,ak,xk ) | ≥ 1 2 Nr 2 I - I 2a
j,k∈[ndeg]	16N |A|
with probability at least 1 一 δabs — 2ndegVZ*(；abs、,abs'lGl) — 4exp (一 \觐|靠%).
Proof. Equation 23 implies that there exists s, a such that
T(s0[i] = 1 ∣ S,a)	T(s0[i] = 0 ∣ s, a)
___
〉αηminσ
≥ 2Ν
ρ(s0[i] = 1)
ρ(s0[i] = 0)
Combining this with Equation 21 we get:
|g?(s, a, s0[i] = I)- g?(s, a, s0[i] = O)I ≥
________ρ(s0[i] = 1)ρ(s0[i] = O)___________αηminσ
4D(s0[i] = 1 ∣ s, a)D(s0[i] = 0 ∣ s, a)____2N
22
≥ α ηminσ
≥ 16N 2∣A∣
(26)
where Equation 26 uses ρ(s0[i] = 1)ρ(s0[i] = 0) ≥ ONmA|, as one of the terms is at least 1∕2 and
other can be bounded using Lemma 11.
Say we have two examples in our dataset, say {(xi, aι, Xi), (χ2, a2, X2)} without loss of generality,
such that φ*(xι)[ω] = φ?(x2)[ω] = s, action ai = a2 = a, 0?(Xi) = 1, and 0?(Xz) = 0. Then we
have:
max I^(xj,aj,Xj) - g(xk,ak,Xk)∣ ≥ ∣g(x1,a1,X1) - g(x2,a2,X2)∣
j,k∈[ndeg]
≥ |g?®a, I)- g*(S,a, 0)|
-∣^(X1,a1,X1) - g*(Xι,aι,Xι)∣
-∣^(X2,a2,X2) - g*(X2,a2,X2)∣
22
≥ α ηminσ _ 2
-16N2∣A∣ - 2a
(using Equation 26 and Lemma 13)
We use Lemma 13 which has a failure probability of δabs + 2ndegA/A(nabsabsJG1). Further, we also
assume that our dataset contains both (s, a, s0[i] = 1) and (s, a, s0[i] = 0). Probability of one of these
29
Published as a conference paper at ICLR 2021
events is given by Lemma 14. Therefore, if ndeg ≥
N2 |A| then from Lemma 25 and union bound,
α2ηm2 in
the probability that at least one of these transitions does not occur is given by 4 exp
(_ 仪2η宾inndeg ∖
1一^3N 2∣A∣2 〉
The total failure probability is given by union bound and computes to:
22
α2ηm inndeg
3N 2∣A∣2
δabs + 2ndeg VZa(nabskabd^ + 4 exp -_
a
□
22
If we fix a = 8oηm∣A∣ then in the two case We have:
(Degenerate Factor)
(Non-Degenerate Factor)
max ∣g(xj,aj,Xj) - g(xk,a，k,Xk)∣
j,k∈[ndeg]
max ∣g(xj,aj,Xj) - g(xk,a，k,Xk)∣
j,k∈[ndeg]
22
≤ α ηminσ
≤ 40N2|A|
≥ 3m碌inσ
≥ 80N2|A|
Theorem 8 (Detecting Degenerate Case). We correctly predict if s0 [i] is a degenerate factor or not
when using ndeg = 3NJA1 log (言)and % ≤ 03oNmin^? log-1 (言),With probability at least
1 - 3δabs.
Proof. The result follows by combining Lemma 16 and Lemma 15, and using the value of a described
above. These two results hold with probability at least:
1 - δabs -
2ndeg VZ∆(nbs,δabs,∣G∣)
a
22
- 4exP 卜WN帛
Setting the hyperparameters to satisfy the following:
ndeg
3N 2AI2
2^^2
α2 ηm2 in
∆(nabs,δabs, |G|)-1/2 ≥
480N 4∣A∣3
α4ηm in δabsσ
gives a failure probability of at most 3δabs. The later condition can be expressed in terms of % which
gives us the desired bounds (see Theorem 7 for definition of %). Lastly, note that setting ndeg this
way also satisfies the requirement in Lemma 16. Lastly, note that the resultant bound on % is much
stronger than required for Theorem 7. Therefore, we can significantly improve the complexity bounds
in the setting where there are no degenerate state factors.	□
D.2.3 Combining Case A and Case B
Theorem 8 shows that we can detect degenerate state factors with high probability. If we have a
degenerate state factor and we detect it, then correspondence theorem holds trivially. However, if we
don’t have degeneracy and we correctly predict it, then we stick our learned decoder and Theorem 7
holds true. These two results allows us to define a bijection between real states and learned states that
we explain below.
Bijective Mapping between real and learned states For a given time step h and state bit s[i] = z,
we will define Uhiz as the corresponding learned state bit. When h and i will be clear from the
context then we will express this as Uz. We will use the notation S to denote a learned state
at time step h - 1 and ^0 to denote learned state at time step h. Let pt(i) = (iι,…，iι) and
s[pt(i)] = W := (w1,w2,…wι), then we define W = (U(h-i)iιwι …U⑺—口观Wl) as the learned
state bits corresponding to w. More generally, for a given set K ∈ 2d, we denote the real state factors
as s[K] (or s0[K]) and the corresponding learned state factors as S[K] (or ^0[K]).
We define a mapping θh : {0, 1}d → {0, 1}d from learned state to real state. We will drop the
subscript h when the time step is clear from the context. We denote the domain of θh by Sh which
is a subset of {0, 1}d. Note that every real state may not be reachable at time step h. E.g., maybe
our decoder outputs ^0 = (0,0) but that the corresponding real state is not reachable at time step
h. Figure 3 visualizes the mapping.
30
Published as a conference paper at ICLR 2021
Figure 3: Bijection between learned state
space Sbh and the state
maps every state in Sh to a
unique state in Sh . However Sh
space at time step h. The bijection
also contains learned states that do not
correspond to a reachable state at this time step. This happens due to error in the decoder.
For a learned state ^ We have S = θ(S) if S = (zι,…，zd) and S = (uhizι,…，Uhdzd). We would
also overload our notation to write w = θ(W) for a given S[K] = W where w = θ(S)[K], whenever
factor set K is clear from the context.
We call s0 (or S) as reachable if s0 ∈ Sh (or S ∈ Sh-1). Similarly, we call ^0 (or ^) as reachable if
θh(s0) (or θh-ι(^)) are reachable. For a given set of factors K, we call W ∈ {0,1}|K| as reachablefor
K if there exists a reachable state with factors K taking on value w. Similarly, we define W ∈ {0,1}|K|
as reachable for a given K if θ(W) is reachable for K.
We use the mapping to state the correspondence theorem for the whole state.
Corollary 9 (General Case). If ndeg = 3N^ log (我)and % ≤ WONi霏 log-1 (我)holds,
then with probability at least 1 - 3dδabs, we have:
∀s' ∈ Sh,	P(^0[i] = θ-1(S0)[i] | S0[i]) ≥ 1 - %, P(^0 = Θ-1(s0) | s0) ≥ 1 - d%.
Proof. The first result directly follows from being able to detect if we are in degenerate setting or not
and if not, then applying Theorem 7, and if yes then result holds trivially. This holds with probability
at least 1 - 3δabs from Theorem 8. Applying union bound over all d learned factors gives us success
probability across all factors of at least 1 - 3dδabs. The second one follows from union bound.
d
P(S = θ-1 (s) | s) = P(∃i : S[i] = θ-1(S)[i] | S[i]) ≤ fP(^[i] = θ-1(S)[i] | S[i]) ≤ d%.
i=1
□
As we noticed before, our bounds can be significantly improved in the case of no degenerate factors.
This prevents application of expensive Markov inequality. Therefore, we also state bounds for the
special case below.
Corollary 10 (Degenerate Factors Absent). If % ∈(0, 2), then with probability at least 1 - dδabs,
we have:
∀S0 ∈Sh, P(^0[i] = θ-1(S0)[i] | S0[i]) ≥ 1 - %, P(^0 = θ-1(S0) | S0) ≥ 1 - d%.
Proof. Same as Corollary 9 except we can directly apply Theorem 7 as we don’t need to do any
expensive check for a degenerate factor.	□
D.3 Model Estimation
Our next goal is to estimate a model Th : Sh-1 × A → ∆(Sh) and latent parent structure pth,
given roll-in distribution D ∈ ∆(Sh-1), and the learned decoders {φt}t≤h. Recall that our approach
estimates the model by count-based estimation. Let D = {(x(k), a(k) , x0(k) )}kn=est1 be the sample
collected for model estimation.
Recall that we estimatepbth(i) be using a set of learned factors I that we believe is pbth(i) and varying
a disjoint set of learned factors J . If pbt(i) ⊆ I then we expect the learned model to behave the same
31
Published as a conference paper at ICLR 2021
irrespective of how we vary the factors J. However, if pbt(i) 6⊆ I then there exists a parent factor that
on varying will different values for the learned dynamics. Let K = I ∪ J be the set of all factors in
the control group (I) and variable group (J ).
We will first analyze the case for a fixed i ∈ Sbh and control group I and variable group J . For
a given V ∈ {0,1}, W ∈ {0,1}|K| and a ∈ A, we have PD(^0[i] = V | s[K] = W, a) denoting the
estimate probability derived from our count based estimation (Algorithm 4, line 3). Let PD (^0[i]=
V | s[K] = W, a) be the probabilities that are being estimated. It is important to note that we use
subscript D for these notations as the learned states S are not Markovian and K may not contain pt(i),
therefore, the estimated probabilities P and expected probabilities P will be dependent on the roll-in
distribution D.
.--.
In order to estimate PD(. | ^h-ι[K] = W, a) we want good lower bounds on PD(Sh-ι [K] = W, a)
for every a ∈ A and W reachable for K. Our roll-in distribution D only guarantees lower bound on
PD(sh-1 [K], a). However, we can use IH.2 to bound the desired quantity below.
Lemma 17 (Model Estimation Coverage). If % ≤ 2 then for all K ∈ C≤2κ([d]), a ∈ A and
W ∈ {0,1}|K| reachable for K, we have:
PD(sh-1[K] = W，a) ≥ /A।
Proof. We can express PD(^h-ι[K] = W, a)= 备PD(^h-ι[K] = W) as actions are taken uniformly.
Let Sh-ι = θh-1(Sh-1) and W = Sh-ι[K]. We bound PD(Sh-ι [K] = W) as shown:
Pd(Sh-ι[K] = W) ≥ Pd (Sh-ι[K] = W,sh-ι[K]= w)
=PD (Sh-l[K] = W | Sh-l[K] = w) PD (sh-l[K] = w)
=Y PD (Sh-i[k] = Wk | Sh-i[k] = Wk) PD (sh-i[K] = w)
k∈K
2κ αηmin	αηmin
≥(1一%) N ≥ 4κN ,
where the third step uses the fact that value of learned state Sh-ι[k] is independent of other decoders
given the real state bit sh-1[k]. The fourth step uses IH.2 and the fact that we have good coverage
over all sets of state factors of size at most 2κ. Last inequality uses |K| ≤ 2κ and % ≤ 11.	□
We now show that our count-based estimator PbD converges to PD and derive the rate of convergence.
Lemma 18 (Model Estimation Error). Fix δest ∈ (0, 1). Then with probability at least 1 - δest for
^very K ∈ C≤2κ([d]), W ∈ {0,1}|K| reachable for K, and a ∈ A we have the following:
X J Pd (Sh [i] = V | Sh-i∖K] = W,a) - PD (S%[i] = V | Sh-i[K] = W,a)∣ ≤ 2∆ est (n est ,δ est)
v∈{0,1}
where ∆est(nest, δest) := 1 (g5N^ Y ln (4e4⅛
2	αηmin nest	est
Proof. We sample nest samples by roll-in at time step h - 1 with distribution D and taking actions
uniformly. We first analyze the failure probability for a given K, W, a. Let E(K, W, a) denote the
event {^h-ι[K] = W, ah-ι = a}. If E(K, W, a) occurs in our dataset at least m times for some
m ≥ 16 ln(1∕δ) then from Corollary 15 we have
X JPD(sh[i] = V | Sh-i[K] = W, a) - Pd(Sh[i] = V | ^h-i[K] = W,a)J ≤ e,
v∈{0,i}
with probability at least 1 - δ. Lemma 17 shows that probability of E(K, W, a) is at least 4KNin .
Therefore, from Lemma 28 if nɛst ≥ 2 +ηmN|A| ln (e) then we get at least m samples of event
E(K, W, a) with probability at least 1 - δ. Therefore, the total failure probability is at most 2δ: δ due
to not getting at least m samples and δ due to Corollary 15 on getting m samples. This holds for
every triplet (K, W, a) and Lemma 23 shows that there are at most 2(ed)2κ |A| such triplets. Hence,
with application of union bound we get the desired result.	□
32
Published as a conference paper at ICLR 2021
Lemma 19 (Model Approximation Error). For any i ∈ [d], K ∈ C≤2κ([d]), s ∈ Sh-1, a ∈ A, s0 ∈
Sh ,let S = θ--ι(s)and ^0 = θ-1(s0). Then we have:
|Pd(^0[i] | S[K],a) - PD(s0[i] | s[K],a)∣ ≤ △ app ：
5κ%N
αηmin
Proof. We will first bound |Pd(s0[i] | s[K], a) - PD(s0[i] | s[K], a)| and then use correspondence
result (Corollary 9) to prove the desired result. We start by expressing our conditional probabilities as
ratio of joint probabilities.
PD(s0[i] | S[K],a)
PD (s0[i],S[K],a)
PD (S[K],a)
PD(s0[i] | s[K], a)
PD (s0[i],s[K],a)
PD (s[K],a)
From Lemma 29 we have:
IPD(s0[i] | s[K],a) - PD(s0[i] | s[K],a)| ≤	； + ；2、, Where	(27)
PD(s[K], a)
ει ：= |Pd(s0[i], S[K],a) — PD(s0[i], s[K],a)∣ and ε? := |Pd(S[K], a) — PD(s[K],a)∣. We bound
these two quantities below:
ει = E Pd (s0[i],S[K],Sh-i[K],a)- E PD (s0[i],^h-i [K],s[K],a)
Sh-ι[K]	^h-ι[K]
= X	Pd(s0[i], S[K], Sh-i[K],a) -	X	PD(s0[i], Sh-i[K], s[K], a)
Sh-ι[K]=s[K]	Sh-ι[K] = s[K]
≤ max E	Pd(s0[i], s[K], Sh-i[K],a),
Sh-ι[K] = s[K]
|-------------------{-------------------}
Term 1
E	Pd(s0[i],Sh-i[K],s[K],a) >
Sh-ι[K] = s[K]
|-----------------{------------------}
Term 2
Where the first inequality uses |a - b| ≤ max{a, b} for a, b > 0.
We bound Term 1 below:
Term 1:
ɪ X	Pd(s0[i] I S[K],Sh-i[K],a)P(S[K] I sh-i[K])Pd(sh-i[K])
sh-1 [K]6=s[K]
≤ ɪ X	Pd(s0[i]∣ S[K],Sh-i[K],a)PD(sh-i[K])
sh-1 [K]6=s[K]
'看 XgPD(ShT[K"'高
sh-1 [K]6=s[K]
The key inequality here is P(^[K] ∣ Sh-ι[K]) = "痣兀 P(S[k] ∣ Sh-ι[k]) ≤ %, as there exist at least
one j ∈ K such that Sh-I [j] = s[j] and for this j we have P(^[j] ∣ Sh-Ij]) ≤ %.
We bound Term 2 similarly:
Term2:	ɪ X	PD(s0[i] I ^h-ι[K],s[K],a)PD(^h-ι[K] I s[Κ])Pd(s[K])
I I 8h-ι[K]=s[K]
≤ 两 X	pD(@h-1[K] I s[K]) = ∣-J∣ {1 - PD(s[K] i s[K])}
i i Sh-ι[^^] = S[K]	i i
≤ I⅛(1-(1-%)1K1) ≤ 2κ%
where we use PD(S[K] I s[K]) = Qk∈κ P(S[k] I s[k]) ≥ (1 - %)1K| and IKI ≤ 2κ.
33
Published as a conference paper at ICLR 2021
This gives Us ει ≤ 播.The proof for ε is similar.
ε = E PD (S[K],Sh-i[K],a)- E PD (Sh-i[K],s[K],a)
Sh-ι[K]	Sh-ι[K]
= X	PD (^[K],Sh-i[K],a)- X	PD (^h-i[K],s[K],a)
Sh-ι[K] = s[K]	^h-ι[K]=^[K]
max E	Pd (s[K]
sh-1 [K]6=s[K]
|---------------{z—
Term 3
sh-1[K],a),
8h-ι[K] = s[K]
」 1
Pd(Sh-i[K],s[K],a) >
_ -	}
{z
Term 4
We boUnd Term 3 below similar to Term 1:
Term 3:	∣-A-	X	PD (S[K]∖sh-ι [K])PD(sh-ι[K])
sh-1 [K]6=s[K]
≤⅛ XjD(ShT[K]) ≤ 高
sh-1 [K]6=s[K]
and Term 4 is boUnded similar to Term 2 below:
Term4:	ɪ X	PD(sh-ι[K] ∖ s[K])Pd(s[K])
11 Sh-ι[K] = s[K]
≤∣⅛ {1 — Pd(^[K] ∖ '[K/
≤⅛ {1-1}≤ 2A%
This gives US ε ≤ 件.Plugging bounds for ει and ε in Equation 27 and using PD(s[K],a)=
PD (S[K]) ≥ ɑηmin σivpc nc∙
|A|	≥ N|A| gives us:
∖pD (S0[i] ∖ S[K],a) - PD (S0[i] ∖ s[K],a∖ ≤ I 4 ∣p ( %rι7 ≤ -%-∙	(28)
∖A∖PD(s[K],a)	αηmin
We can use correspondence result to derive a lower bound:
PD(^0[i] ∖ S[K],a) ≥ PD(^0[i] ∖ s0[i])PD(S0[i] ∖ S[K],a)
≥ (1 - %)Pd(s0[i] ∖ S[K],a) ≥ Pd(s0[i] ∖ S[K],a) — %
and an upper bound:
Pd(^0[i] ∖ S[K],a)= PD(^0[i] ∖ sR)Pd(s0[i] ∖ S[K],a)+
P(^0[i] ∖ 1 — s0[i])PD(1 — s0[i] ∖ S[K],a)
≤ Pd(s0[i] ∖ S[K],a) + %
Combing the lower and upper bounds with Equation 28 gives us:
∖PD(80[i] ∖ ^[K], a) — PD (s0[i] ∖ s[K], a)∖ ≤ ---+ % ≤ -----.
αηmin
αηmin
which is the desired result.
□
We can merge the estimation error and approximation error to generate the total error.
34
Published as a conference paper at ICLR 2021
Lemma 20 (K-Model Error). For any i ∈ [d], K ∈ C≤2κ([d]), s ∈ Sh-1, a ∈ A, s0 ∈ Sh, let
S = θh-ι(s) and ^0 = θ-1(s0). Then we have:
I Pd (^0[i] | S[K],a) - PD (s0[i] | s[K],a)∣ ≤ △ est (n est ,δest) + ∆ tw.
with probability at least 1 - δest.
Proof. Follows trivially by combining the estimation error (Lemma 18) and approximation error
(Lemma 19) with application of triangle inequality.	□
D.4 DETECTING LATENT PARENT STRUCTURE IN TRANSITION pth
We are now ready to analyze the performance of learned parent function pth. Let K1 , K2 ∈
C≤2κ([2d]) and Wi ∈ {0,1}lKll,ι^2 ∈ {0,1}lK2l. We will assume Wi is reachable for K1 and
W^2 is reachable for K. For convenience we will define the following quantity Ω to measure total
variation distance between distributions PD(s0[i] | ∙, ∙) conditioned on setting s[Ki] = wi and
^[K2] = W2, and for a fixed action a ∈ A:
Ω ia(K1,W1, K2,W2):= 2	X	I Pd (^0[i]	= V |	S[Ki]	= Wi,a) - PD (^0[i]	= V |	^[K2]	=	W2,a)∣ .
v∈{0,1}
We can compute Ω for every value of i, K1,W1, K2,W2,a in computational time of O ((2ed)3κ+3 |A|).
We also define a similar metric for the true distribution for any Ki , K2 and V ∈ {0, 1}, Wi ∈
{0, 1}|K1|,W2 ∈ {0, 1}|K2| and a ∈ A:
Ωia(K1,w1, K2,W2) := 2 X |Pd (s0[i] = V | s[Ki] = wi,a) - PD (s0[i] = V | s[K2] = w2,a)∣.
v∈{0,i}
Recall that [I; J] denotes concatenation of two ordered sets I and J . We use this notation to state
our next result.
Lemma 21 (Inclusive Case). Fix i ∈ [d] and I ∈ C≤κ([d]). If pt(i) ⊆ I then for all a ∈ A and
U ∈ {0,1}|I| we get:
^	,「一	.r、	.	,	_、	..
max	Ω ia([I ； Ji], [u; Wi], [I ； J2], [u; W2]) ≤ 2∆ est (n est ,δest) + 2∆ app,
J1,J2,W1 ,W2
where max is taken over Ji, J2 ∈ C≤κ([d]), ^^i ∈ {0,1}lJ1l, W2 ∈ {0,1} 1J2| such that [U; Wi] is
reachablefor [I; Ji], [U; W2] is reachablefor [I; J2], and I ∩ Ji = I∩J2 = 0.
Proof. We fix Ji, J2,U,Wi,W2,a and let Ki = [I; Ji], K2 = [I; J2], V = θ(V), u = θ(U),
wi = θ(Wi), and w2 = θ(W2). As pt(i) ⊆ I, therefore, we have:
PD(s0[i] = V | s[Ki] = [U; Wi], a) = Ti(s0[i] = V | s[I] = U, a) = PD(s0[i] = V | s[K2] = [U; W2], a)
Using this result along with Lemma 20 and application of triangle inequality we get:
目(^0[i] = V | s[Ki] = [U; Wi],a) - PD(^0[i] = V | s[K2] = [U; W2],a)1 ≤ 2∆est(nest,δest)+2∆app.
.—.
Summing over V, dividing by 2, and using the definition of Ω proves the result.	□
The following is a straightforward corollary of Lemma 21.
Corollary 11. Fix i ∈ [d] then there exists an I such thatforall a ∈ A and U ∈ {0,1}|I|:
^	，「一	_r	.	,	,	.	.	...	.	.	，	_	、	..
max	Ω ia([I ； Ji], [u; Wi], [I ； J2], [u; W2]) ≤ 2∆ est IneSt ,δest) + 2∆ app,
J1,J2,W1 ,W2
where max is taken over Ji, J2,Wi,W2 satisfy the restrictions stated in Lemma 21.
Proof. Take any I such that pt(i) ⊆ I and apply Lemma 21. Note that we are allowed to pick such
an I as ∣pt(i) | ≤ K by our assumption.	□
35
Published as a conference paper at ICLR 2021
Recall that we define pbt(i) as the solution of the following problem:
jpt(i) := argmin max	Ωia([I; J1], [u; Wι], [I； J2], [U; W2]),	(29)
I	a,U,J1,J2 ,W1,W2
where I ∈ C≤κ([d]), a ∈ A, and U, Ji, J2,W1,W2 satisfy the restrictions stated in Lemma 21.
.—.
We are now ready to state our main result for pt.
Theorem 12 (Property of pt). Forany S ∈ Sh-1, a ∈ A, s0 ∈ Sh, let s = θ--11(s) and ^0 = θ-1(s0).
Then the learned parent function pt satisfies:
∀ ∈ [d],	IbD(^0[i] | S[pt(i)],a)- Ti(s0[i] | s[pt(i)],a)∣ ≤ 3∆est(nest,δest) + 3∆app.
Proof. Fix i ∈ [d]. Let J = pt(i) - pbt(i) and K = pbt(i) ∪ J . As pt(i) ⊆ K, therefore, we have:
PD(s0[i] | s[K], a) = Ti(s0[i] | s[pt(i)], a)
Combining this result with Lemma 20 we get:
回(^0[i] | ^[K],a) - Ti(s0[i] | s[pt(i)],a)1 ≤ ∆est(nest,δest)+∆app.
From the definition of pbt(i) (Equation 29) and Corollary 11 we have:
同(^0[i] | ^血(i); 0],a) - PD(^0[i] | ^血(i); J],a)∣ ≤ 2∆est(nest,δest) + 2∆app.
Note that we are allowed to use Corollary 11 as S[pt(i); 0] and ^[pt(i); J] are both reachable since
they are derived from a reachable real state s, |pbt(i)| ≤ κ, |[pbt(i); J]| ≤ |[pbt(i); pt(i)]| ≤ 2κ, and
pbt(i) ∩ 0 = 0 = pbt(i) ∩ J. Combining the previous two inequalities using triangle inequality
completes the proof.	□
D.5 B ound Total Variation between Estimated Model and True Model
√-1 ∙	. 1 1	1 .	∙ . ∙	. Γ∙	. ∙	^l	1 i'	. )	∙ . ∙	1 1
Given the learned transition parent function pth we define the transition model as:
d
Thi⑹[i]	|	s[pth(i儿a)=bD⑹[i]	|	Slpth⑶],a),	Th(so | s,a)=γThie[i]	|	wpth⑶],a).
i=1
From Theorem 12 we have for any i ∈ [d], S ∈ Sh-ι,a ∈ A,s0 ∈ Sh and S = θ-1(s), ^0 = θ-1(s0):
IThi(^0[i] | S沛(i)],α) - T(s0[i] | s[pt(i)],a)∣ ≤ 3∆est(nest,δest) + 3∆app.
Transition Closure. A subtle point remains before we prove the model error between Th and
T. Theorem 12 only states guarantee for those S that are inverse of a reachable state s. However,
as stated before, due to decoder error we can reach a state S which does not have a corresponding
reachable state, i.e, θ(S) ∈ Sh (see Figure 3). We cannot get model guarantees for these unreachable
states S since we may reach them with arbitrarily small probability. However, we can still derive model
error if we can simply define the real transition probabilities in terms of the learned probabilities for
these states. This will not cause a problem since the real model will never reach these states. We start
by defining the closure of the transition model T◦ for time step h as:
∀s ∈ Sh-1, a ∈ A, S0 ∈ Sh)
TT(θ(^0) I θ(S),a)= {T(：?1"(S)，a)，
Th(S | s,a),
if θ(S) ∈Sh-ι
otherwise
We also define the state space domain of Th as Sh-ι = {θh-ι(S) I ∀^ ∈ Sh-1}. It is easy to see that
八	1 ....	含	ICC
θh-ι represents a bijection between Sh-1 and Sh-r
We will derive our guarantees with respect to T◦ which will allow us to define a bijection between
the domain of T and T◦, and use important lemmas from the literature. The next result shows that
our use of T◦ is harmless as it assigns the same probability as T to any event.
36
Published as a conference paper at ICLR 2021
Lemma 22 (Closure Result). Let T◦ be the closure of transition model with respect to some learned
transition model. Then for any policy π ∈ Π and any event E which is a function of an episode
sampled using π, we have Pn(E; T) = Pn (E; T◦), where Pn (E; T0) denotes the probability ofevent
E when sampling from π and using transition model T0.
Proof. The proof follows form observing that when using T◦ We will never reach a state S ∈ Sh-I
for any h 一 1 by definition of Sh-1. From definition of T◦ this means that both T◦ and T will
generate the same range of episodes sampled from π and will assign the same probabilities to them.
As E is a function of an episode, therefore, its probability remains unchanged.	□
With the definition of closure, we are now ready to state our last result in this section, which bounds
the total variation between the estimated model and the transition closure under the bijection map θ.
Theorem 13 (Model Error). For any S ∈ Sh-1 and a ∈ A we have:
X ∣Th(^0 | ^,a) - Th(θ(S'0) | θ(S),a)∣ ≤ 6d (∆est(nest,δest) + ∆app).
^0 ∈Sh
Proof. If θ(S) ∈ Sh-1 then by definition T◦ the bound holds trivially. Therefore, we focus on
θ(S) ∈ Sh-1 for which T◦ = T. We define the quantity for every j ∈ [d]:
∣∣ d	d	∣∣
Sj=	X	YThi(^0[i] | ^[pt(i)],a)- YTi(θ(^0)[i] | θ(S)[pt(i)],a)	(30)
^0[j]…^0[d]∈{0,1} i=j	i=j
We claim that Sj ≤ 6(d 一 j + 1)(∆est + ∆app) for every j ∈ [d]. For base case we have:
Sd = X IThd(^0[d] | ^[pt(d)],a)- T(θ(^0)[d] | θ(S)[pt(d)],a)∣ ≤ 6(∆est + ∆app),
^0[d]∈{0,1}
from Theorem 12. We will assume the induction hypothesis to be true for Sk for all k > j . We
handle the inductive below with triangle inequality:
d
Sj ≤	X Y rThi(ss0[i] | S[pt(i)],a)|Thj(s0[j] | s[ppt(j)],α)-
s0[j]…^0[d]∈{0,1} i=j + 1
T(θ(^0)[j] | θ(^)[pt(j)],a)∣ +
d
E	T(θ(^0)[j] | θ(S)[pt(j)],a)∣ ∏ Thi(^0[i] | S[p⅜)],a)-
^0[j]…^0[d]∈{0,1}	i=j + 1
d
∏ T(θ(^0)[i] | θ(^)[pt(i)],a)∣
i=j+1
____________________________________	. ^ ..... . . ^ , . . 、 一 ,  ......... .  	
The first term is equivalent to Ps0j]∈{0,i} ∣Thj(s0[j] | s[pt(j)],a) - T(θ(s0)[j] | θ(s)[pt(j)],a)∣
which is bounded by 6(∆est + ∆app) following base case analysis. The second term is equivalent to
Sj+1 which is bounded by 6(d 一 j)(∆est + ∆app) by induction hypothesis. Combining these two
bounds proves the induction hypothesis and the result then follows from bound for Si.	□
D.6 Learning a Policy Cover
In this section we show how we learn the policy cover. We start by defining some notation.
Two MDPs. After time step h, we can define two Markov Decision Processes (MDPs) at this time
Mh and Mh. Mh is the true MDP consists of state space (S◦,…，Sh), action space A, horizon
h, a deterministic start state si = {0}d, and transition function Tt : Sf-1 × A → ∆(St) for all
t ∈ [h]. Recall that the set Sh ⊆ {0, 1}d denote states which are reachable at time step h, and the set
37
Published as a conference paper at ICLR 2021
St ⊆ St ⊆ {0,1}d represents the closure of state space Sh based on the learned state space St. For
any t ∈ [h], s ∈ St andK ∈ C≤2κ([d]) we know supπ∈ΠNS Pπ(st[K] = s[K]) ≥ ηmin.
The second MDP Mh consists of the learned state space (Si, •…，Sh), action space A, horizon h, a
deterministic start state si = {0}d, and transition function Tt : St-i ×A→ ∆(St).
For every t ∈ [h], We have θt : St → St represent a bijection from the learned state space to the
closure of the set of reachable states at time step t. The learned decoder φt predict θt(s) given s ∈ St
With high probability for all t < h by IH.2 and fort = h due to Corollary 9.
Lastly, the transition model Ttt and Tt are close in Li distance for t < h due to IH.3 and for t = h
due to Theorem 13.
These results enable us to utilize the analysis ofDu et al. (2019) for learning to learn a policy cover.
Let。： S → A denote a non-stationary deterministic policy that operates on the learned state space.
Similarly,夕：So → A denote a non-stationary deterministic policy that operates on the real state.
We denote O 二夕◦ θ if for every s ∈ S, <7^(^)=夕(θ(S)). Similarly, we denote 夕=Cp ◦ θ-i if for
every S ∈ So,0(S) = O(θ-1(s)). Let π : X → A be a non-stationary deterministic policy operating
on the observation space. We say ∏ =0◦ φ if for every X ∈ X we have ∏(χ) = o^(φ(χ)), Similarly,
we define ∏ =夕◦ φ? if for every X ∈ X we have ∏(χ)=0(φ*(x)).
We will use Pπ [E] to denote probability of an event E when actions are taken according to policy
∏ : X → A. We will use Pφ [E] to denote the probability of event E when we operate directly on
the real state and take actions using 夕.Similarly, we define Pg [E] to denote the probability of event
E when we operate on the learned state space. Lastly, let Pg[E] denote probability of an event E
when actions are taken according to policy。operating directly over the latent state and following
our estimated transition dynamics T : S × A → ∆(S). Recall that our planner will be optimizing
with respect to Pg [E].
Theorem 14 (Planner Guarantee). Fix ∆Pl ≥ 0,h ∈ [H]. Let I ∈ C≤2κ([d]) and W ∈ {0,1}lIL
We define a reward function R : S → [0,1] where R(s) := 1{τ(s) = h ∧ s[I] = w}. Let
ʌ	r	/令…A 、7	1.	1	i T ,1	1	ʃ , ʌ	ʌ V ,1
夕R = Planner(T, R, h, ∆Pl) be the policy learned by the planner. Let π :二0R ◦ φ then:
Pn (sh[I] = θ(W)) ≥ η(sh∖I] = θ(w)) - 2d%H - 12dH∆est - 12dH∆app - ∆Pl,	(31)
further, we have:
ʌ
P0R({^h[I] = w}) ≥ η(sh[I] = θ(w)) - 6dH∆est - 6dH∆app - ∆Pl,	(32)
and if {sh,[I] = θ(W)} is unreachable, then
ʌ
PgR ({^h[I] = W}) ≤ 6dH∆est + 6dH∆app.	(33)
ʌ
Proof. We define two events E := {sh[I] = θ(W)} and E := {^h[I] = W}. We define a policy
Or = Or ◦ θ-i where for every S ∈ S we have 夕R(S) = 0r(Θ-1(s)). We also define ∏ : X → A
as ∏(χ) = or ◦ Φ (x). If for a given X ∈ X and φ*(x) = S we have φ(χ) = θ 1(s) then
∏(χ) = or(Φ(x)) = ∏(χ). Hence, every time our decoder outputs the correct mapped state θ(s),
policies ∏ and ∏ take the same action. We use the result of Du et al. (2019) stated in Lemma 30
(setting ε set to d% using Corollary 9) to write:
|Pn(E) - Pn(E)| = |Pn(E) - PgR(E)| ≤ 2d%H	(34)
Let O : So → A be any policy on real state space and let O : S → A be the induced policy on
learned state space given by <o(s) = o ◦ θ(s) = o(θ(s)) for any S ∈ S. We showed in Theorem 13
that T and T have small Li distance under the bijection θ. Therefore, from the perturbation result
of Du et al. (2019) stated in Lemma 31 we have:
X ∣Pg(θ-1(^h)) - Pg(sh)∣ ≤ hε ≤ Hε,
sh∈sh
38
Published as a conference paper at ICLR 2021
where ε := 6d (∆est(nest, δest) + ∆app) due to Theorem 13. As {sh[I] = θ(W)} ⇔ {^h[I] = W},
therefore, we can derive the following bound:
W(E) - P0(E)∣=	X	Pr(Sh)- X	Ps^(θ-1(sh))
Sh ∈ShiSh[I]=θ(W)	Sh∈Sh；Sh[I\=θ(^^)
≤ X ∣Pr(Sh)- Pr(θ-1(sh))∣ ≤ Hε	(35)
Sh∈sh
Let 夕？ = arg max Pφ [E] be the optimal policy to satisfy {sh[I] = θ(w)}. Note that 夕？ is also the
latent policy that optimizes the reward function R on the real dynamics. Let ⅛^?=夕？ ◦ θ-1 be the
induced policy on learned states. We now bound the desired quantity as shown:
Pn (E) ≥ PrR [E] - 2d%H	(using Equation 34)
≥ P0r (E) - 2d%H - Hε	(using Equation 35)
≥ Pr? (E) - 2d%H - Hε - ∆pl	(夕R is ∆pl-optimal on T)
≥ Pr? (E) - 2d%H - 2Hε - ∆pl	(using Equation 35)
= η(Sh [I] = θ(w)) - 2d%H - 2Hε - ∆pl.
This proves Equation 31 and Equation 32. Note that our calculations above show:
ʌ ʌ
P rR (E) ≤ PrR [E ]+ H
If {sh[I] = θ(W)} is unreachable then PrR [E] = 0. Plugging this in the above equation proves Equa-
tion 33 and completes the proof.	□
D.7 Wrapping up the proof for FactoRL
We are almost done. All we need to do is to make sure is to set the hyperparameters and verify each
induction hypothesis. We first set hyperparameters.
Setting Hyperparameters. Let {sh[I] = θ(W)} be reachable for some I ∈ C≤2κ([d]) and W ∈
{0, 1}|I|. Then applying Theorem 14 and using the definition of ηmin we have:
Pn (sh[I] = θ(W)) ≥ ηmin - 2d%H - 12dHδθs1 - 12dH∆app - δPl
As we want the right hand side to be at least αηmin we divide the error equally between the three
terms. This gives us:
(Planning Error) ∆pl ≤ (1-α)ηmin∕4	(36)
(Model Approximation Error)
△app ≤ (1-α)ηmin∕48dH ⇒ % ≤
α(1 - α)ηmin
240KdHN
(37)
(Model Estimation Error) ∆est ≤ (1-α)ηmin∕48dH
⇒ nest ≥ 184322Kd：H23NA| ln2 (4e|A1(ed)2K )	(38)
α(1 - α)2ηm3 in	δest
(Decoding Error) % ≤ (1-α)ηmin∕8dH	(39)
The model approximation error places a more stringent requirement on % than the decoding error for
planning. However, throughout the proof for FactoRL in this section, we made other requirements on
our hyperparameters. For % this is given by min{βmin∕1200,1∕2} = βmn∕1200 by combining constraints
in Lemma 8 and Theorem 7, and an additional constraint for detecting non-degenerate factors stated
in Corollary 9. Due to the inefficiency of the non-degenerate factors detection, we state results
separately for the two cases:
% ≤ min
βm in
1200,
α(1 - α)ηmin ∖
240κdHN ʃ
% ≤ min
βm in
1200,
α(1 - α)ηmin a2ηminδabs
240KdHN , 30N2|A|2
(no non-degenerate factor)
(general case)
39
Published as a conference paper at ICLR 2021
Using the definition of % from Theorem 7, we get a value of nabs for non-degenerate factor (Equa-
tion 40) and general case (Equation 41) given below:
nabs ≥
nabs ≥
38402N4∣A∣2ι ( |G| A J κ2d2H2N2	25 1
α4ηminσ2	n VabsJ max t α2(1 - α)2ηmin , % ʃ
38402N4∣A∣2 ln (⑧a max j κ2d2H2N2	25	N4∣A∣4	2 (ɪN
α4ηminσ2	Vabs √	lα2(1 - α)2ηmin , β44in ,。4错1 inδ2bs	Vabs √ J
(40)
(41)
Recall that for detecting degenerate factors we collect ndeg samples. Corollary 9 gives value of this
hyperparameter as
ndeg
3N 2|A|2
2^^2
α2ηm2 in
which also satisfies the condition in Lemma 16. Lastly, Theorem 3 gives number of samples for
independence testing nind and rejection sampling frequency k as:
nind ≥ O
m2∣A∣∣F∣(2ed)2κ+1
δind
k ≥ 8 in
ηmin
30
βmin
Failure probabilities for a single timestep are bounded by δind due to identification of emission
structure (Theorem 3), 3dδabs due to decoding (Corollary 9), and δest due to model estimation
(Lemma 17). The total failure probability using union bound for a single step is given by δind +
3dδabs + δest, and for the whole algorithm is given by δindH + 3dδabs H + δest H. Binding δindH 7→
δ∕3, 3dδabsH → δ∕3, δestH → δ∕3, gives Us total failure probability of δ and the right value of
hyperparameters.
Sample complexity of FactoRL is at most kHnind + Hnabs + Hndeg + H nest episodes which is order
of:
Poly d di6κ, |A|, H) —, -, ——, —, in m, in|F|, in |g|)I,
ηmin δ βmin σ
where use the fact that N = ∣Ψh-ι | can be at most 2(ed)2κ from Lemma 23. Note that if We did not
have to apply the expensive degeneracy detection step, then we would get logarithmic dependence
on 1∕δabs. Cheaper ways of detecting degeneracy can, therefore, significantly improve the sample
complexity.
We have not attempted to optimize the degree and exponent in the sample complexity above.
For our choice of two hyperparameters nest and nabs, we can bound the model error and decoding
failure by:
(Model Error)	6d(∆mod + ∆app) ≤ (1~Emn, (Decoding Failure) % ≤ α(1 - Ca)^in.
4H	240κdH N
Verifying Induction Hypothesis. Finally, we verify the different induction hypothesis below.
—∙^~∙
1.	We already verified IH.1 with Theorem 3. We learn a chh that is equivalent to chh upto
label permutation.
2.	We already verified IH.2 with Corollary 9. Given a real state s ∈ Sh, our decoder outputs
the corresponding learned state with high probability. We also derived the form of %.
3.	We already verified IH.3 with Theorem 13. We also derived the form of ∆est and ∆app.
4.	Lastly, Theorem 14 and our subsequent calculations for hyperparameter show that Ψh is
an α-policy cover of Sh and that the size of Ψh is at most 2(ed)2κ from Lemma 23. Lastly,
for all reachable factor values we get the value of learned policy as at least (1+α)ηmin∕2
using Equation 32 and our choice of hyperparameter values. Similarly, from Equation 33 we
get the value of learned policy for all unreachable factor values as at most (1-α)ηmin∕4. This
allows us to filter all unreachable factor values. In the main paper, we focus on the value of
α = 1/2, which explains why on Algorithm 1, line 8 we only keep those policies with value
at least (1+α)ηmin∕2 = 3ηmin∕4. This verifies IH.4.
This completes the analysis for FactoRL.
40
Published as a conference paper at ICLR 2021
E S upporting Result
Lemma 23 (Assignment Counting Lemma). For a given k, d ∈ N and k ≤ d, the cardinality of the
set {(K, u) | K ∈ C≤k ([d]), u ∈ {0, 1}|K|} is bounded by 2(ed)k.
Proof. Assume k ≥ 2. The cardinality of this set is given by Pk=0 (d)2i which can be bounded as
shown below:
XX (d)2i = 1 + 2d + XX (d)2i ≤ 1 + 2d + XX 5 2i
i=0	i=2	i=2
k
≤ 1 + 2d + X (ed)i
i=2
k
<	(ed)i .
i=0
The first inequality here uses the well-known bound for binomial coefficients (n) ≤匿斗 for any
n, i ∈ N and i ≤ n. Further bounding the above result using ed - 1 ≥ ed/2 gives us:
k
(ed)i ≤
i=0
(ed)k+1
ed - 1
≤ 2(ed)k .
The proof is completed by checking that inequality holds for k < 2.
□
Lemma 24 (Lemma H.1 inDu et al. (2019)). Let u, v ∈ Rd+ with kuk1 = kvk1 = 1 and ku-vk1 ≥ ε.
Thenfor any a > 0 we have ∣∣αu 一 u∣∣ ι ≥ ε.
Lemma 25 (Chernoff Bound). Let q be the probability of an event occurring. Then given n iid
samples with n ≥ q, the probability that the event occurred at least once is at least 1 — 2 exp( -qn).
Proof. Let Xi be a 0-1 indicator denoting if the event occurred or not, and let X = Pin=1 Xi . We
have E[Xi] = q and E[X] = qn. Let t = 1 一 1/qn. We will assume that qn > 1 and so t ∈ (0, 1).
Then the probability that the event never occurs is bounded by:
P(X < 1) = P(X < (1 一 t)qn) ≤ exp
一qnt2
3
<2exp {一 qn}.
□
Lemma 26 (Hoeffding,s Inequality). Let Xi, X2, ∙∙∙ , Xn be independent random variables bounded
by the interval [0,1]. Let empirical mean ofthese random variables be X = n Pn=I Xn, then for
any t > 0 we have:	_	_
P(IX — E[X]∣ ≥ t) ≤ 2exp(-2nt2).
Lemma 27 (Theorem 2.1 of Weissman et al. (2003)). Let P be a probability distribution over a
discrete Set of size a. Let Xn = Xi, X2, ∙∙∙ , Xn be independent identical distributed random
ʌ
variables distributed according to P. Let PXn be the empirical probability distribution estimated
from sample set Xn. Then for all > 0:
P(∣P 一 PXn ∣i ≥ ) ≤ (2a 一 2) exp
The next result is a direct corollary of Lemma 27.
Corollary 15. For any m ≥ 8a ln(1∕δ) samples, we have ∣∣P — Pχ n ||i < e with probability at least
1 一 δ.
Lemma 28. Let Xi, X2, •…,Xn be 0-1 independent identically distributed random variables with
mean μ. Let X = Pn=I Xi. Fix m ∈ N and δ ∈ (0,1). If n ≥ 2m ln (∣) then P(X < m) ≤ δ.
Proof. This is a standard Chernoff bound argument. We have E[X] = nμ. Assuming n ≥ m∕μ then
from multiplicative Chernoff bound we have:
P(X < m) = P X ≤
≤ exp
≤ exp
n nμ∖
mn - τJ
Setting right hand side equal to δ and solving gives us n ≥ μ (m + ln(∣)) which is satisfied
whenever n ≥ 2m ln (e).	□
41
Published as a conference paper at ICLR 2021
Lemma 29 (Lemma H.3 of Du et al. (2019)). For any a, b, c, d > 0 with a ≤ b and c ≤ d we have:
I a C I ≤ |a — c| + |b — d|
b d	max{b, d}
The next Lemma is borrowed from Du et al. (2019). They state their Lemma for a specific event
(E = α(S) in their notation) but this choice of event is not important and their proof holds for any
event.
_	一一	—一 一_	_	____ _ ，一	一、 -0	,O	个、-
Lemma 30 (Lemma G.5 of Du et al. (2019)). Let S = (Si,…，SH) and S = (Si,…，SH) be
?
the real and learned state space. We assume access to a decoder φ : X → S and let φ? : X → S
1 . 1	1 1	1 T . Zi	言	i~∙ 1	1 ∙ ∙	「	1 — Γ T7^l	7 八 含. C ，
be the oracle decoder. Let θh : Sh → Sh be a bijection for every h ∈ [H] and θ : S → S where
i
θ(s) = θh(s) for s ∈ Sh. Forany h ∈ [H] and Sh ∈ Sh, we assume P(Sh = θ- 1(sh) | Sh) ≥ 1 — ε,
i.e., given the real state sh, our decoder (φb) will map it to θh-i(sh) with probability at least 1 - ε.
Let 夕：S → A be a deterministic Policy on the real state space and。： S → A be the induced
Policy given by ⑦(S) = 夕(θ(S)) for every S ∈ Sb. Let π,∏ : X → A where π(x) = 夕(φ*(x)) and
ʌ/ 、	、一	一“ L	i
π(x)=夕(φ(x)) for every X ∈ X. For every random event E we have:
∣P∏(E) — Pn(E)| ≤ 2εH
—∙^~∙
Lemma 31 (Lemma H.2. of Du et al. (2019)). Let there be two tabular MDPs M and M. Let
C /C	C 、7	， ，	7 G / G
S = (Si, • • • , SH) be the state space ofM and S = (Si, • •
—∙^~∙
• , SH) be the state space of M. Both
MDPs have a A be the action space of both MDPs and horizon of H. For every h ∈ [H], there exists
a bijection θh : Sh → Sh. Let T : S × A → ∆(S) and T : S × A → ∆(S) be transition dynamics
for M and Mc satisfying:
^
∀h ∈ [H],a ∈ A,S ∈S,	X ∣Th(θ(^0) | θ(S),a) — Th(^0 | S,a)∣ ≤ ε
^~Sh
Let 夕:S → A be a policy for M. Let 0 : S → A be the induced policy
S ∈ S we have 0(S)=夕(θ(S)). Thenfor any h ∈ [H] we have:
on
Mc such that for
any
X ∣p0(θ-1(^h)) — Pr(Sh)∣ ≤ hε
sh ∈Sh
F Experiment Details
We provide details for our proof of concept experiment below.
Modeling Details. We model F, used for performing independence test, using a single-layer feed-
forward network θF with Leaky ReLu non-linearity (Maas et al., 2013) and a softmax output layer.
Give a pair of atoms x[u] and x[v], we concatenate these atoms and map it to a probability distribution
over {0, 1} by applying θF.
We implement the model class G for learning state decoder following suggestion of Misra et al. (2020).
Recall that a function in G maps a transition (χ,a,X) ∈ X ×A× X? to a value in [0,1]. We first
map x and X to vectors vi and v respectively, using two separate linear layers. We map the action
a to its one-hot vector representation 1a. We map the vector v2 to a probability distribution using
the GUmbel-softmax trick (Jang et al., 2016), by computing qi α exp(v2 [i] + Hi) for all i ∈ {1,2},
where 必 is sampled independently from the Gumbel distribution. We concatenate the vectors vi, 1a
and q and map it to a probability distribution over {0, 1}, through a single layer feed-forward network
θG with Leaky ReLu non-linearity. We recover a decoder φ from the model that maps a set of atoms
X to φ(x) = argmaxi∈{o,i} qi+i.
42
Published as a conference paper at ICLR 2021
Learning Details. We train the two models using cross-entropy loss. Formally, given a dataset
Dind = {(xi [u], xi [v], yi)}in=ind1 for performing independence testing, and a dataset Dabs =
{(xi, a” Xi, yi)}n=ab1 for learning a decoder, We optimize the model by minimizing the cross-entropy
loss as shown below:
1
f = arg max——〉/n f(y | XiHxi[v]),
f∈F nind
■)—
1 nabs
g = arg max ——V"ln g(y | Xi,ai,Xi).
g∈G nabs
■)—
Here We overload our notation to alloW the output of models to be distribution over {0, 1} rather than
a scalar value in [0, 1] as We assumed before. This is in sync With hoW We implement these model
class and alloWs us to conveniently perform cross-entropy loss minimization.
Planner Details. We use a simple planner based on approximate dynamic programming. Given
model estimate, reWard function and a set of visited learned states, We perform dynamic programming
to compute optimal Q-values for the set of visited states. We assume the Q-values for non-visited
states to be 0. This alloWs us to compute Q-values in a computationally-efficient manner. Later, if the
agent visits an unvisited state, then it simply takes random action.
Hyperparameters. We set the hidden dimension of θF to 10 and that of θG to 56. We set the
threshold c on held-out log-loss value, When performing independence test to be 0.65. For reference,
if one uses a random uniform classifier then its performance is - ln(0.5) ≈ 0.69. We train both
models for 10 epochs using Adam optimization With learning rate of 0.001, and a batch size of 32.
We remove 0.2% of the training data and use it as a validation set. We evaluate on the validation
set after every epoch, and use the model With the best performance on the validation set. We used
PyTorch 1.6 to develop the code and used default initialization scheme for all parameters.
43