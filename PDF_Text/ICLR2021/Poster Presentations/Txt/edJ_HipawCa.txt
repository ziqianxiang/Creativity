Published as a conference paper at ICLR 2021
Impact of Representation Learning in Linear
Bandits
Jiaqi Yang
Tsinghua University
yangjq17@gmail.com
Wei Hu
Princeton University
huwei@cs.princeton.edu
Jason D. Lee
Princeton University
jasonlee@princeton.edu
Simon S. Du
University of Washington
ssdu@cs.washington.edu
Ab stract
We study how representation learning can improve the efficiency of bandit prob-
lems. We study the setting where we play T linear bandits with dimension d
concurrently, and these T bandit tasks share a common k( d) dimensional lin-
ear representation. For the finite-action setting, we present a new algorithm which
achieves O(T√kN +，dkNT) regret, where N is the number of rounds We
play for each bandit. When T is sufficiently large, our algorithm significantly
outperforms the naive algorithm (playing T bandits independently) that achieves
O(T√dN) regret. We also provide an Ω(T√kN + √dkNT) regret lower bound,
showing that our algorithm is minimax-optimal up to poly-logarithmic factors.
Furthermore, we extend our algorithm to the infinite-action setting and obtain
a corresponding regret bound which demonstrates the benefit of representation
learning in certain regimes. We also present experiments on synthetic and real-
world data to illustrate our theoretical findings and demonstrate the effectiveness
of our proposed algorithms.
1 Introduction
This paper investigates the benefit of using representation learning for sequential decision-making
problems. Representation learning learns a joint low-dimensional embedding (feature extractor)
from different but related tasks and then uses a simple function (often a linear one) on top of the
embedding (Baxter, 2000; Caruana, 1997; Li et al., 2010) The mechanism behind is that since the
tasks are related, we can extract the common information more efficiently than treating each task
independently.
Empirically, representation learning has become a popular approach for improving sample efficiency
across various machine learning tasks (Bengio et al., 2013). In particular, recently, representation
learning has become increasingly more popular in sequential decision-making problems (Teh et al.,
2017; Taylor & Stone, 2009; Lazaric & Restelli, 2011; Rusu et al., 2015; Liu et al., 2016; Parisotto
et al., 2015; Higgins et al., 2017; Hessel et al., 2019; Arora et al., 2020; D’Eramo et al., 2020).
For example, many sequential decision-making tasks share the same environment but have different
reward functions. Thus a natural approach is to learn a succinct representation that describes the
environment and then make decisions for different tasks on top of the learned representation.
While representation learning is already widely applied in sequential decision-making problems
empirically, its theoretical foundation is still limited. One important problem remains open:
When does representation learning provably improve efficiency of sequential decision-making
problems?
We take a step to characterize the benefit of representation learning in sequential decision-making
problems. We tackle the above problem in the linear bandits setting, one of the most fundamental
1
Published as a conference paper at ICLR 2021
and popular settings in sequential decision-making problems. This model is widely used in applica-
tions as such clinical treatment, manufacturing process, job scheduling, recommendation systems,
etc (Dani et al., 2008; Chu et al., 2011). We study the multi-task version of linear bandits, which
naturally models the scenario where one needs to deal with multiple different but closely related
sequential decision-making problems concurrently.
We will mostly focus on the finite-action setting. Specifically, we have T tasks, each of which is
governed by an unknown linear coefficient θt ∈ Rd. At the n-th round, for each task t ∈ [T], the
player chooses an action an,t that belongs to a finite set, and receive a reward rn,t with expectation
Ern,t = hθt, Xn,t,an,ti Where Xn,t,an,t represents the context of action an,t. For this problem,
a straightforward approach is to treat each task independently, which leads to O(T√dN)1 total
regret. Can We do better?
Clearly, if the tasks are independent, then by the classical Ω(√dN) per task lower bound for linear
bandit, it is impossible to do better. We investigate how representation learning can help if the tasks
are related. Our main assumption is the existence ofan unknown linear feature extractor B ∈ Rd×k
with k dand a set of linear coefficients {wt}tT=1 such that θt = Bwt. Under this assumption,
the tasks are closely related as B is a shared linear feature extractor that maps the raw contexts
xn,t,a ∈ Rd to a low-dimensional embedding B>xn,t,a ∈ Rk . In this paper, we focus on the
regime where k d, N, T. This regime is common in real-world problems, e.g., computer vision,
where the input dimension is high, the number of data is large, many task are related, and there
exists a low-dimension representation among these tasks that we can utilize. Problems with similar
assumptions have been studied in the supervised learning setting (Ando & Zhang, 2005). However,
to our knowledge, this formulation has not been studied in the bandit setting.
Our Contributions We give the first rigorous characterization on the benefit of representation
learning for multi-task linear bandits. Our contributions are summarized below.
•	We design anew algorithm for the aforementioned problem. Theoretically, we show our algorithm
incurs O(VdkTN + T√kN) total regret in N rounds for all T tasks. Therefore, our algorithm
outperforms the naive approach with O(T√dN) regret. To our knowledge, this is the first theo-
retical result demonstrating the benefit of representation learning for bandits problems.
•	To complement our upper bound, we also provide an Ω( VdkTN+T√kN) lower bound, showing
our regret bound is tight up to polylogarithmic factors.
•	We further design a new algorithm for the infinite-action setting, which has a regret
<e(d1∙5k√TN + kT√N), which outperforms the naive approach with O(Td√N) regret in the
regime where T = Ω(dk2).
•	We provide simulations and an experiment on MNIST dataset to illustrate the effectiveness of our
algorithms and the benefits of representation learning.
Organization This paper is organized as follows. In Section 2, we discuss related work. In Sec-
tion 3, we introduce necessary notation, formally set up our problem, and describe our assumptions.
In Section 4, we present our main algorithm for the finite-action setting and its performance guar-
antee. In Section 5, we describe our algorithm and its theoretical guarantee for the infinite-action
setting. In Section 6, we provide simulation studies and real-world experiments to validate the ef-
fectiveness of our approach. We conclude in Section 7 and defer all proofs to the Appendix.
2	Related Work
Here we mainly focus on related theoretical results. We refer readers to Bengio et al. (2013) for
empirical results of using representation learning.
For supervised learning, there is a long line of works on multi-task learning and representation learn-
ing with various assumptions (Baxter, 2000; Ando & Zhang, 2005; Ben-David & Schuller, 2003;
Maurer, 2006; Cavallanti et al., 2010; Maurer et al., 2016; Du et al., 2020; Tripuraneni et al., 2020).
1O(∙) omits logarithmic factors.
2
Published as a conference paper at ICLR 2021
All these results assumed the existence of a common representation shared among all tasks. How-
ever, this assumption alone is not sufficient. For example, Maurer et al. (2016) further assumed every
task is i.i.d. drawn from an underlying distribution. Recently, Du et al. (2020) replaced the i.i.d. as-
sumption with a deterministic assumption on the input distribution. Finally, it is worth mentioning
that Tripuraneni et al. (2020) gave the method-of-moments estimator and built the confidence ball
for the feature extractor, which inspired our algorithm for the infinite-action setting.
The benefit of representation learning has been studied in sequential decision-making problems,
especially in reinforcement learning domains. D’Eramo et al. (2020) showed that representation
learning can improve the rate of approximate value iteration algorithm. Arora et al. (2020) proved
that representation learning can reduce the sample complexity of imitation learning. Both works
require a probabilistic assumption similar to that in (Maurer et al., 2016) and the statistical rates are
of similar forms as those in (Maurer et al., 2016).
We remark that representation learning is also closely connected to meta-learning (Schaul &
Schmidhuber, 2010). Raghu et al. (2019) empirically suggested that the effectiveness of meta-
learning is due to its ability to learn a useful representation. There is a line of works that analyzed
the theoretical properties of meta-learning (Denevi et al., 2019; Finn et al., 2019; Khodak et al.,
2019; Lee et al., 2019; Bertinetto et al., 2018). We also note that there are analyses for other rep-
resentation learning schemes (Arora et al., 2019; McNamara & Balcan, 2017; Galanti et al., 2016;
Alquier et al., 2016; Denevi et al., 2018).
Linear bandits (stochastic linear bandits / linearly parameterized bandits / contextual linear bandits)
have been studied in recent years (Auer, 2002; Dani et al., 2008; Rusmevichientong & Tsitsiklis,
2010; Abbasi-Yadkori et al., 2011; Chu et al., 2011; Li et al., 2019a;b). The studies are divided into
two branches according to whether the action set is finite or infinite. For the finite-action setting,
Θ (√dN) has been shown to be the near-optimal regret bound (ChU et al., 2011; Li et al., 2019a),
and for the infinite-action setting, Θ(d√N) regret bound has been shown to be near-optimal (Dani
et al., 2008; Rusmevichientong & Tsitsiklis, 2010; Li et al., 2019b).
Some previous work studied the impact of low-rank structure in linear bandit. Lale et al. (2019)
studied a setting where the context vectors share a low-rank structure. Specifically, in their setting,
the context vectors consist of two parts, i.e. X = X+ψ, so that X is from a hidden low-rank subspace
and ψ is i.i.d. drawn from an isotropic distribution. Jun et al. (2019) and Lu et al. (2020) studied
the bilinear bandits with low-rank structure. In their setting, the player chooses two actions X, y and
receives the stochastic reward with mean X> Θy, where Θ is an unknown low-rank bilinear form.
The algorithms proposed in the aforementioned papers share some similarities with our Algorithm 2
for our infinite-action setting, in that both used Davis-Kahan theorem to recover and exploit the
low-rank structure.
Some previous work proposed multi-task bandits with different settings. Deshmukh et al. (2017)
proposed a setting under the contextual bandit framework. They assumed similarities among arms.
Bastani et al. (2019) studied a setting where the coefficients of the tasks were drawn from a gaussian
distribution fixed across tasks and proposed an algorithm based on Thompson sampling. Soare et al.
(2018) proposed a setting where tasks were played one by one sequentially and the coefficients
of the tasks were near in `2 distance. In our setting, the tasks are played simultaneously and the
coefficients share a common linear feature extractor.
3	Preliminaries
Notation. We use bold lowercases for vectors and bold uppercases for matrices. For any positive
integer n, we use [n] to denote the set of integers {1, 2, . . . , n}. For any vector X, we use kXk
to denote its `2 norm. For a matrix A, we use kAk to denote the 2-norm of A, kAkF to denote
the Frobenius norm, and kAkmax = maxi,j |Aij | to denote the max-norm. For two expressions
α, β > 0, we denote α . β if there is a numerical constant c > 0 such that α ≤ cβ . We denote
α & β if β . α.
Problem Setup. Letd be the ambient dimension and k(≤ d) be the representation dimension. In
total, we have T tasks and we play each task concurrently for N rounds. Each task t ∈ [T] has an
3
Published as a conference paper at ICLR 2021
unknown vector θt ∈ Rd. At each round n ∈ [N], the player chooses action an,t ∈ An,t for each
task t ∈ [T] where An,t is the action set at round n for the task t.
After the player commits to a batch of actions {an,t}t∈[T], it receives a batch of rewards {rn,t}t∈[T],
where we assume rn,t = han,t, θti + εn,t. Here we assume the noise εn,t are independent 1-sub-
Gaussian random variables, which is a standard assumption in the literature.
We use the total expected regret to measure the performance of our algorithm. When we have N
rounds and T tasks, it is defined as RN,T = PnN=1 PtT=1 maxa∈An,t ha, θti - han,t, θti. When the
action set is finite, we assume that all An,t have the same size K, i.e. |An,t| ≡ K. Furthermore, we
write An,t = {xn,t,1, . . . , xn,t,K}. Besides, we interchangeably use the number an,t ∈ [K] and the
vector an,t = xn,t,an,t ∈ Rd to refer to the same action.
Assumptions. Our main assumption is the existence of a common linear feature extractor.
Assumption 1 (Common Feature Extractor). There exists a linear feature extractor B ∈ Rd×k and
a set of linear coefficients {wt}tT=1 such that the expected reward of the t-th task at the n-th round
satisfies E[rt,n] = hwt, B>xn,t,an,ti.
For simplicity, we let W = [w1, . . . , wT]. Assumption 1 implies that Θ , [θ1, . . . , θT] = BW .
Note this assumption is in a sense necessary to guarantee the effectiveness of representation learning
because without it one cannot hope that representation learning helps.
In this paper, we mostly focus on the finite-action setting. We put the following assumption on
action sets.
Assumption 2. Marginally, for every n ∈ [N],t ∈ [T],a ∈ [K], the contexts satisfy Xn,t,a 〜
N(0, ∑t) such that λmaχ(∑t) ≤ O(l∕d) and λmin(∑t) ≥ Ω(1∕d).
With this assumption, we have an unknown covariance matrix Σt for each task. At each round,
the actions of the t-th task are sampled from a Gaussian distribution with covariance Σt . This is a
prototypical setting for theoretical development on linear bandits with finite actions (See e.g., Han
et al. (2020)). At a population level, each one of the K actions is equally good but being able to
select different actions based on the realized contexts allows the player to gain more reward.
We will also study the infinite-action setting. We first state our assumption about the action sets.
Assumption 3 (Ellipsoid Action Set). We assume An,t = At = {x>Qt-1x ≤ 1 : x ∈ Rd} is an
ellipsoid with λmin(Qt) ≥ λo = Ω(1).
The first assumption states that each action set is an ellipsoid that covers all directions. This is a
standard assumption, e.g., see Rusmevichientong & Tsitsiklis (2010).
In this setting, we will also need to put some additional assumptions on the underlying parameters
B and W .
Assumption 4 (Diverse Source Tasks). We assume that λmin(TWW>) ≥ k, where V = Ω(1).
This assumption roughly states that the underlying linear coefficients {wt}tT=1 equally spans all
directions in Rk. This is a common assumption in representation learning literature that enables
us to learn the linear feature extractor (Du et al., 2020; Tripuraneni et al., 2020). For example, the
assumption holds with high probability when wi is uniformly chosen from the sphere Sk-1 .
Assumption 5. We assume ∣∣wtk ≥ ω = Ω(1).
This is a normalization assumption on the linear coefficients.
4	Main Results for Finite-Action Setting
In this section focus on the finite-action setting. The pseudo-code is listed in Algorithm 1. Our
algorithm uses a doubling schedule rule (Gao et al., 2019; Simchi-Levi & Xu, 2019; Han et al., 2020;
Ruan et al., 2020). We only update our estimation of θ after an epoch is finished, and we only use
samples collected within the epoch. In Line 5, We solve an empirical '2-risk minimization problem
on the data collected in the last epoch to estimate the feature extractor B and linear predictors
4
Published as a conference paper at ICLR 2021
Algorithm 1: MLinGreedy: Multi-task Linear Bandit with Finite Actions
1： Let M = dlog2 log2 N], Go = 0, Gm = N, Gm = N1-2 m for 1 ≤ m ≤ M 一 1, let θo,t — 0;
2： for m - 1,...,M do
3： for n - Gm-1 + 1,..., Gm do
4：	For each task t ∈ [T]: choose action an,t = arg maxa∈[K] xn>,t,aθbm-1,t;
5：	COmPUte BW J arg min	Pn=Gm-1+1 PT=1[x>,t,antBwt - rn,t]2;
B∈Rd×k,W ∈Rk×T	,
6： For each task t ∈ [T]: let θm,t = Bwbt;
W, similar to DU et al. (2020). Given estimated featUre
extractor B and linear Predictors W,
we
comPUte oUr estimated linear coefficients of task t as θbt , Bb wbt in Line 6. For choosing actions,
for each task, we Use a greedy rUle, i.e., we choose the action that maximizes the inner ProdUct with
oUr estimated θ (cf. Line 4).
The following theorem gives an UPPer boUnd on the regret of Algorithm 1.
Theorem 1 (Regret of Algorithm 1). Suppose K, T ≤ poly(d) and N ≥ d2. Under Assumption 1
and Assumption 2, the expected regret of Algorithm 1 is upper bounded by
E[RN,T ]=O(T √kN + √dkNT).
There are two terms in Theorem 1, and We interpret them separately. The first term O(Ty/kN)
rePresents the regret for Playing T indePendent linear bandits with dimension k for N roUnds. This
is the regret we need to pay even if we know the optimal featUre extractor, with which we can
redUce the original problem to playing T independent linear bandits with dimension k (recall wt are
different for different tasks). The second term O( dkNT) represents the price we need to pay to
learn the feature extractor B. Notably, this term shows we are using data across all tasks to learn B
as this term scales with NNT.
Now comparing with the naive strategy that plays T independent d-dimensional linear bandits with
regret O(T√dN), our upper bound is smaller as long as T = Ω(k). Furthermore, when T is large,
our bound is significantly stronger than O(T√dN), especially when k《d. To our knowledge,
this is the first formal theoretical result showing the advantage of representation learning for bandit
problems. We remark that requiring T = Ω(k) is necessary. One needs at least k tasks to recover
the span of W, so only in this regime representation learning can help.
Our result also puts a technical requirement on the scaling K, T ≤ poly(d) and N ≥ d2 . These are
conditions that are often required in linear bandits literature. The first condition ensures that K and
T are not too large, so we need not characterize log(KT) factors in regret bound. If they are too
large, e.g. K, T ≥ Ω(ed), then we would have log KT = O(d) and we could no longer omit the
logarithmic factors in regret bounds. The second condition ensures one can at least learn the linear
coefficients up to a constant error. See more discussions in (Han et al., 2020, Section 2.5).
While Algorithm 1 is a straightforward algorithm, the proof of Theorem 1 requires a combination
of representation learning and linear bandit techniques. First, we prove the in-sample guarantee
of representation learning, as done in Lemma 2. Second, we exploit Assumption 2 to show that
the learned parameters could extrapolate well on new contexts, as shown in Lemma 4. The regret
analysis then follows naturally. We defer the proof of Theorem 1 to Appendix A.
The following theorem shows that Algorithm 1 is minimax optimal up to logarithmic factors.
Theorem 2 (Lower Bound for Finite-Action Setting). Let A denote an algorithm and I denote a
finite-actioned multi-task linear bandit instance that satisfies Assumption 1 and Assumption 2. Then
for any N, T, d, k ∈ Z+ with k ≤ d, k ≤ T, we have
inf SUp E[RN,T] = Ω (T√kN + √dkNT) .	(1)
Theorem 2 has the exactly same two terms as in Theorem 1. This confirms our intuition that the
two prices to pay are real: 1) playing T independent k-dimensional linear bandits and 2) learning
5
Published as a conference paper at ICLR 2021
Algorithm 2: E2TC: Explore-Explore-Then-Commit
Input: N: total number of rounds , N1: number of rounds for stage 1 , N2: number of rounds
for stage 2
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
Stage 1: Estimating Linear Feature Extractor with Method-of-Moments;
for ∀t ∈ [T], n ∈ [N1] do
Play Xn,t 〜Unif(λo ∙ Sd-I) and receive reward rn,t；
Compute C — N1T Pn= 1 PT=I rn,txn,tx>,t;
>
Let BDB 1 J top-k singular value decomposition
"De , 7- ,1	.,1	1
of M. Denote bi the i-th column of B;
Stage 2: Estimating Optimal Actions on Low-dimensional Space ;
for ∀t ∈ [T], i ∈ [k] do
Play vt,i，√λ0bi for N?/k times and receive rewards {rn,t}N=NιN(/-1)N2∕k+1;
for ∀t ∈ [T] do
Estimate Wt J argminw∈Rk 2N2 Pn=ΛN+ 1[hxn,t, Bwi — rn,t]2;
Let θbt = Bbwbt;
Stage 3: Committing to Near-optimal Actions;
for ∀t ∈ [T],n = N1 + N2 + 1, . . . ,Ndo
TTAl	/ Λ^ ∖	1	1
Play an,t J arg maxa∈A ha, θti and receive reward rn,t;
the d × k-dimensional feature extractor. We defer the proof of Theorem 2 to Appendix B. At a high
level, we separately prove the two terms in the lower bound. The first term is established by the
straightforward observation that our multi-task linear bandit problem is at least as hard as solving
T independent k-dimensional linear bandits. The second term is established by the observation
that multi-task linear bandit can be seen as solving k independent d-dimensional linear bandits,
each has NT /k rounds. Note that the observation would directly imply the regret lower bound
ky∕d(NT∕k) =，dkNT, which is exactly the second term. To our knowledge, this lower bound
is also the first one of its kind for multi-task sequential decision-making problems. We believe our
proof framework can be used in proving lower bounds for other related problems.
5 Extension to Infinite-Action Setting
In this section we present and analyze an algorithm for the infinite-action setting. Pseudo-code is
listed in Algorithm 2. Our algorithm has three stages, and we explain each step below.
Stage 1: Estimating Linear Feature Extractor with Method-of-Moments. The goal of the first
stage is to estimate the linear feature extractor B . Our main idea to view this problem as a low-
rank estimation problem for which we use a method-of-moments estimator. In more detail, we first
sample each Xn,t 〜Unif[λ0 ∙ Sd] for N times. We can use this sampling scheme because the
action set is an ellipsoid. Note this sampling scheme has a sufficient coverage on all directions,
which help us estimate B . Next, we compute the empirical weighted covariance matrix Mc =
NT PN= ι PT=I rn2,txn,tx>,t. To proceed, we compute the singular value decomposition of Mc
and keep its top-k column space as our estimated linear feature extractor Bb, which is a sufficiently
accurate estimator (cf. Theorem 5).
Stage 2: Estimating Optimal Actions on Low-Dimensional Space. In the second stage, we use
our estimated linear feature extractor to refine our search space for the optimal actions. Specifically,
we denote B = [bi,..., bk] and for i ∈ [k] and t ∈ [T], we let vt,i = √λ0bi. Under Assumption 3,
we know vt,i ∈ At for all i ∈ [k]. Therefore, we can choose vt,i to explore. Technically, our choice
of actions vt,i also guarantees a sufficient cover in the sense that λmin Pik=1 Bb>vt,ivt>,iBb ≥ λ0.
In particular, this coverage is on a low-dimensional space instead of the original ambient space.
The second stage has N2 rounds and on the t-th task, we just play each vt,i for N2/k rounds. After
that, we use linear regression to estimate wt for each task. Given the estimation wbt, we can obtain
an estimation to the true linear coefficient θbt , Bbwbt.
6
Published as a conference paper at ICLR 2021
k = 5	k = 10	k = 15
400
3 350
⅛300
京250
K
200
Figure 1: Comparisons of Algorithm 1 with the naive algorithm for d = 30 on synthetic data.
k = 5
k = 10
k = 15
Ooo
Q 5 Q
3 2 2
10	20	30	40	50	10	20	30	40	50	10	20	30	40	50
T: Number of Tasks
yseτjωd ləj0I)ə 工
Figure 2: Comparisons of Algorithm 1 with the naive algorithm for d = 20 on synthetic data.
Stage 3: Committing to Near-Optimal Actions. After the second stage, we have an estimation
θt for each task. For the remaining (N - N1 - N2) rounds, we just commit to the optimal action
indicated by our estimations. Specifically, We play the action a%t J argmaxa∈∕t(at, θR for
round n = N1 + N2 + 1, . . . , N.
The folloWing theorem characterizes the regret of our algorithm.
Theorem 3	(Regret of Algorithm 2). Ifwe choose Ni = cιdljsk^N and N = c2k√Nfor some
constants c1 , c2 > 0. The regret of Algorithm 2 is upper bounded by
E[RN,T] = O(d1∙5k√TN + kT √N).
The first term represents the regret incurred by estimating the linear feature extractor. Notably, the
term scales with √TN, which means we utilize all TN data here. The first term scales with d1.5k,
Which We conjecture is unavoidable at least by our algorithm. The second term represents playing
T independent k-dimensional infinite-action linear bandits.
Notice that if one uses a standard algorithm, e.g. the PEGE algorithm (Rusmevichientong & Tsit-
siklis, 2010), to play T tasks independently, one can achieve an O(dT√N) regret. Comparing with
this bound, our bound S second term is always smaller and the first is smaller when T = Ω(dk2).
This demonstrates that more tasks indeed help us learn the representation and reduce the regret.
We complement our upper bound with a lower bound below. This theorem suggests our second
term is tight but there is still a gap in the first term. We leave it as an open problem to design new
algorithm to match the lower bound or to prove a stronger lower bound.
Theorem 4	(Lower Bound for Infinite-Action Setting). Let A denote an algorithm andI denote an
infinite-action multi-task linear bandit instance that satisfies Assumption 1, Assumption 3, Assump-
tion 4, Assumption 5. Then for any N, T, d, k ∈ Z+ with k ≤ d, k ≤ T, we have
inf sup E[RN,T] = Ω (d√kNT + kT
(2)
7
Published as a conference paper at ICLR 2021
Figure 3: Comparisons of Algorithm 1 with the naive algorithm for T = 10 on MNIST .
Figure 4: Comparisons of Algorithm 1 with the naive algorithm for T = 45 on MNIST.
6	Experiments
In this section, we use synthetic data and MNIST data to illustrate our theoretical findings and
demonstrate the effectiveness of our Algorithm for the finite-action setting. We also have simulation
studies for the infinite-action setting, which we defer to Appendix F. The baseline is the naive
algorithm which plays T tasks independently, and for each task, thie algorithm uses linear regression
to estimate θt and choose the action greedily according to the estimated θt .
6.1	Synthetic Data
Setup. The linear feature extractor B is uniformly drawn from the set of d × k matrices with
orthonormal columns.2 Each linear coefficient wt is uniformly chosen from the k-dimensional
sphere. The noises are i.i.d. Gaussian: εn,t,a = N(0, 1) for every n ∈ [N], t ∈ [T], a ∈ [K]. We fix
K = 5 and N = 10000 for all simulations on finite-action setting. We vary k, d and T to compare
Algorithm 1 and the naive algorithm.
Results and Discussions. We present the simulation results in Figure 1 and Figure 2. We emphasize
that the y-axis in our figures corresponds to the regret per task, which is defined as RN,T /T. We fix
K = 5, N = 10000. These simulations verify our theoretical findings. First, as the number of tasks
increases, the advantage of our algorithm increases compared to the naive algorithm. Secondly, we
notice that as k becomes larger (relative to d), the advantage of our algorithm becomes smaller. This
can be explained by our theorem that as k increases, our algorithm pays more regret, whereas the
naive algorithm’s regret does not depend on k.
6.2	Finite-Action Linear Bandits for MNIST
Setup. We create a linear bandits problem on MNIST data (LeCun et al., 2010) to illustrate the
effectiveness of our algorithm on real-world data. We fix K = 2 and create T = 120 tasks and each
task is parameterized by a pair (i, j), where 0 ≤ i < j ≤ 9. We use Di to denote the set of MNIST
images with digit i. At each round n ∈ [N], for each task (i, j), we randomly choose one picture
2We uniformly (under the Haar measure) choose a random element from the orthogonal group O(d) and
uniformly choose k of its columns to generate B .
8
Published as a conference paper at ICLR 2021
from Di and one from Dj , then we present those two pictures to the algorithm and assign the picture
with larger digit with reward 1 and the other with reward 0. The algorithm is now required to select
an image (action). We again compare our algorithm with the naive algorithm.
Results and Discussions. The experimental results are displayed in Figure 3 for T = 10 (done by
constructing tasks with first five digits) and Figure 4 for T = 45. We observe for both T = 10 and
T = 45, our algorithm significantly outperforms the naive algorithm for all k. Interestingly, unlike
our simulations, we find the advantage of our algorithm does not decrease as we increase k. We
believe the reason is the optimal predictor is not exactly linear, and we need to develop an agnostic
theory to explain this phenomenon, which we leave as a future work.
7 Conclusion
We initiate the study on the benefits of representation learning in bandits. We proposed new al-
gorithms and demonstrated that in the multi-task linear bandits, if all tasks share a common linear
feature extractor, then representation learning provably reduces the regret. We demonstrated empir-
ical results to corroborate our theory. An interesting future direction is to generalize our results to
general reward function classes (Li et al., 2017; Agrawal et al., 2019). In the following, we discuss
some future directions.
Adversarial Contexts For the finite-action setting, we assumed the context are i.i.d. sampled
from a Gaussian distribution. In the bandit literature, there is a large body on developing low-
regret algorithms for the adversarial contexts setting. We leave it as an open problem to develop
a algorithm with an O(TVkN + JdkNT) upper bound or show this bound is not possible in the
adversarial contexts setting. One central challenge for the upper bound is that existing analyses
for multi-task representation learning requires i.i.d. inputs even in the supervised learning setting.
Another challenge is how to develop a confidence interval for an unseen input in the multi-task linear
bandits setting. This confidence interval should utilize the common feature extractor and is tighter
than the standard confidence interval for linear bandits, e.g. LinUCB.
Robust Algorithm The current approach is tailored to the assumption that there exists a common
feature extractor. One interesting direction is to develop a robust algorithm. For example, consider
the scenario where whether there exists a common feature extractor is unknown. We want to de-
velop an algorithm with regret bound as in this paper when the common feature extractor exists and
gracefully degrades to the regret of T independent linear bandits when the common feature extractor
does not exist.
General Function Approximation In this paper, we focus on linear bandits. In the bandits litera-
ture, sublinear regret guarantees have been proved for more general reward function classes beyond
the linear one (Li et al., 2017; Agrawal et al., 2019). Similarly, in the supervised representation
learning literature, general representation function classes have also been studied (Maurer et al.,
2016; Du et al., 2020). An interesting future direction is to merge these two lines of research by de-
veloping provably efficient algorithms for multi-task bandits problems where general function class
is used for representation.
Acknowledgments
The authors would like to thank the anonymous reviewers for their comments and suggestions on our
paper. WH is supported by NSF, ONR, Simons Foundation, Schmidt Foundation, Amazon Research,
DARPA and SRC. JDL acknowledges support of the ARO under MURI Award W911NF-11-1-0303,
the Sloan Research Fellowship, and NSF CCF 2002272.
References
Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic
bandits. In Advances in Neural Information Processing Systems, pp. 2312-2320, 2011.
Shipra Agrawal, Vashist Avadhanula, Vineet Goyal, and Assaf Zeevi. Mnl-bandit: A dynamic
learning approach to assortment selection. Operations Research, 67(5):1453-1485, 2019.
9
Published as a conference paper at ICLR 2021
Pierre Alquier, The Tien Mai, and Massimiliano Pontil. Regret bounds for lifelong learning. arXiv
preprint arXiv:1610.08628, 2016.
Rie Kubota Ando and Tong Zhang. A framework for learning predictive structures from multiple
tasks and unlabeled data. Journal of Machine Learning Research, 6(Nov):1817-1853, 2005.
Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saunshi.
A theoretical analysis of contrastive unsupervised representation learning. In Proceedings of the
36th International Conference on Machine Learning, 2019.
Sanjeev Arora, Simon S Du, Sham Kakade, Yuping Luo, and Nikunj Saunshi. Provable represen-
tation learning for imitation learning via bi-level optimization. arXiv preprint arXiv:2002.10544,
2020.
Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine
Learning Research, 3(Nov):397-422, 2002.
Hamsa Bastani, David Simchi-Levi, and Ruihao Zhu. Meta dynamic pricing: Learning across ex-
periments. arXiv preprint arXiv:1902.10918, 2019.
Jonathan Baxter. A model of inductive bias learning. Journal of artificial intelligence research, 12:
149-198, 2000.
Shai Ben-David and Reba Schuller. Exploiting task relatedness for multiple task learning. In Learn-
ing Theory and Kernel Machines, pp. 567-580. Springer, 2003.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828,
2013.
Luca Bertinetto, Joao F Henriques, Philip HS Torr, and Andrea Vedaldi. Meta-learning with differ-
entiable closed-form solvers. arXiv preprint arXiv:1805.08136, 2018.
Rajendra Bhatia. Matrix analysis, volume 169. Springer Science & Business Media, 2013.
Rich Caruana. Multitask learning. Machine learning, 28(1):41-75, 1997.
Giovanni Cavallanti, Nicolo Cesa-Bianchi, and Claudio Gentile. Linear algorithms for online mul-
titask classification. Journal of Machine Learning Research, 11(Oct):2901-2934, 2010.
Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff func-
tions. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and
Statistics, pp. 208-214, 2011.
Varsha Dani, Thomas P. Hayes, and Sham M. Kakade. Stochastic linear optimization under bandit
feedback. In COLT, 2008.
Giulia Denevi, Carlo Ciliberto, Dimitris Stamos, and Massimiliano Pontil. Incremental learning-to-
learn with statistical guarantees. arXiv preprint arXiv:1803.08089, 2018.
Giulia Denevi, Carlo Ciliberto, Riccardo Grazzi, and Massimiliano Pontil. Learning-to-learn
stochastic gradient descent with biased regularization. In Proceedings of the 36th International
Conference on Machine Learning, 2019.
Carlo D’Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, and Jan Peters. Sharing knowl-
edge in multi-task deep reinforcement learning. In International Conference on Learning Repre-
sentations, 2020. URL https://openreview.net/forum?id=rkgpv2VFvr.
Aniket Anand Deshmukh, Urun Dogan, and Clay Scott. Multi-task learning for contextual bandits.
In Advances in neural information processing systems, pp. 4848-4856, 2017.
Simon S Du, Wei Hu, Sham M Kakade, Jason D Lee, and Qi Lei. Few-shot learning via learning
the representation, provably. arXiv preprint arXiv:2002.09434, 2020.
Kai Wang Fang. Symmetric multivariate and related distributions. CRC Press, 2018.
10
Published as a conference paper at ICLR 2021
Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online meta-learning. In
Proceedings of the 36th International Conference on Machine Learning, 2019.
Tomer Galanti, Lior Wolf, and Tamir Hazan. A theoretical framework for deep transfer learning.
Information and Inference: A Journal of the IMA, 5(2):159-209, 2016.
Zijun Gao, Yanjun Han, Zhimei Ren, and Zhengqing Zhou. Batched multi-armed bandits problem.
In Advances in Neural Information Processing Systems, pp. 503-513, 2019.
Yanjun Han, Zhengqing Zhou, Zhengyuan Zhou, Jose Blanchet, Peter W Glynn, and Yinyu
Ye. Sequential batch learning in finite-action linear contextual bandits. arXiv preprint
arXiv:2004.06321, 2020.
Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, and Hado van
Hasselt. Multi-task deep reinforcement learning with popart. In Proceedings of the AAAI Confer-
ence on Artificial Intelligence, volume 33, pp. 3796-3803, 2019.
Irina Higgins, Arka Pal, Andrei Rusu, Loic Matthey, Christopher Burgess, Alexander Pritzel,
Matthew Botvinick, Charles Blundell, and Alexander Lerchner. Darla: Improving zero-shot trans-
fer in reinforcement learning. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70, pp. 1480-1490. JMLR. org, 2017.
Kwang-Sung Jun, Rebecca Willett, Stephen Wright, and Robert Nowak. Bilinear bandits with low-
rank structure. In International Conference on Machine Learning, pp. 3163-3172, 2019.
Mikhail Khodak, Maria-Florina Balcan, and Ameet Talwalkar. Adaptive gradient-based meta-
learning methods. arXiv preprint arXiv:1906.02717, 2019.
Sahin Lale, Kamyar Azizzadenesheli, Anima Anandkumar, and Babak Hassibi. Stochastic linear
bandits with hidden low rank structure. arXiv preprint arXiv:1901.09490, 2019.
Alessandro Lazaric and Marcello Restelli. Transfer from multiple mdps. In Advances in Neural
Information Processing Systems, pp. 1746-1754, 2011.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online].
Available: http://yann.lecun.com/exdb/mnist, 2, 2010.
Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with
differentiable convex optimization. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 10657-10665, 2019.
Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to
personalized news article recommendation. In Proceedings of the 19th international conference
on World wide web, pp. 661-670, 2010.
Lihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear contex-
tual bandits. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pp. 2071-2080. JMLR.org, 2017.
Yingkai Li, Yining Wang, and Yuan Zhou. Nearly minimax-optimal regret for linearly parameterized
bandits. In Conference on Learning Theory, pp. 2173-2174, 2019a.
Yingkai Li, Yining Wang, and Yuan Zhou. Tight regret bounds for infinite-armed linear contextual
bandits. arXiv preprint arXiv:1905.01435, 2019b.
Lydia T Liu, Urun Dogan, and Katja Hofmann. Decoding multitask dqn in the world of minecraft.
In The 13th European Workshop on Reinforcement Learning (EWRL) 2016, 2016.
Yangyi Lu, Amirhossein Meisami, and Ambuj Tewari. Low-rank generalized linear bandit problems.
arXiv preprint arXiv:2006.02948, 2020.
Andreas Maurer. Bounds for linear multi-task learning. Journal of Machine Learning Research, 7
(Jan):117-139, 2006.
11
Published as a conference paper at ICLR 2021
Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The benefit of multitask
representation learning. The Journal ofMachine Learning Research, 17(1):2853-2884, 2016.
Daniel McNamara and Maria-Florina Balcan. Risk bounds for transferring representations with and
without fine-tuning. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70, pp. 2373-2381. JMLR. org, 2017.
Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and
transfer reinforcement learning. arXiv preprint arXiv:1511.06342, 2015.
Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature reuse?
towards understanding the effectiveness of maml. arXiv preprint arXiv:1909.09157, 2019.
Yufei Ruan, Jiaqi Yang, and Yuan Zhou. Linear bandits with limited adaptivity and learning distri-
butional optimal design. arXiv preprint arXiv:2007.01980, 2020.
Paat Rusmevichientong and John N Tsitsiklis. Linearly parameterized bandits. Mathematics of
Operations Research, 35(2):395-411, 2010.
Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirk-
patrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy distil-
lation. arXiv preprint arXiv:1511.06295, 2015.
Tom SchaUl and Jurgen Schmidhuber. Metalearning. Scholarpedia, 5(6):4650, 2010.
David Simchi-Levi and Yunzong Xu. Phase transitions and cyclic phenomena in bandits with switch-
ing constraints. In Advances in Neural Information Processing Systems, pp. 7523-7532, 2019.
Marta Soare, Ouais Alsharif, Alessandro Lazaric, and Joelle Pineau. Multi-task linear bandits. 2018.
Matthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey.
Journal of Machine Learning Research, 10(Jul):1633-1685, 2009.
Yee Teh, Victor Bapst, Wojciech M Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas
Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning. In Advances in
Neural Information Processing Systems, pp. 4496-4506, 2017.
Nilesh Tripuraneni, Chi Jin, and Michael I Jordan. Provable meta-learning of linear representations.
arXiv preprint arXiv:2002.11684, 2020.
Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018.
Zirui Wang, Zihang Dai, Barnabas Poczos, and Jaime Carbonell. Characterizing and avoiding nega-
tive transfer. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 11293-11302, 2019.
12
Published as a conference paper at ICLR 2021
A Proof of Theorem 1
Lemma 1 (General Hoeffding’s inequality, Vershynin (2018), Theorem 2.6.2). Let X1, . . . , Xn be
independent random variables such that E[Xi] = 0 and Xi is σi-sub-Gaussian. Then there exists a
constant c > 0, such that for any δ > 0, we have
Pr
n
XXi
i=1
≥c
≥ ct
n
X σ2 IOg(I/6
i=1
≤ δ.
(3)
Lemma 2. Let T be the number of tasks and N0 be the number of samples. For every (n, t) ∈
[N0] × [T], let xn,t ∈ Rd be fixed vectors and let yn,t = xn>,tθt + εn,t, where θt ∈ Rd is a vector
and εn,t is an independent 1-sub-Gaussian variable. Let
N0 T
Bb Wc =	arg min	X X[xn>,tBbwbt - yn,t]2 ,	(4)
Bb∈Rd×k,Wc∈Rk×T n=1 t=1
where W = (Wι	•…	WT). Then with probability 1 一 δ, we have
N0 T
X X[x>,t(BWt- Bwt)]2 . (dk + kT) Iog(NTdk) + log(1∕δ).
n=1 t=1
Proof. By (4), we have
N0 T	N0 T
X X[xn>,tBbWbt - yn,t]2 ≤ X X[xn>,tBWt - yn,t]2.
n=1 t=1	n=1 t=1
Since yn,t = xn>,tBWt + εn,t, we have
N0 T	N0 T
XX[xn>,t(BbWbt-BWt)+εn,t]2≤XXε2n,t,
n=1 t=1	n=1 t=1
which implies
N0 T	N0 T
XX[xn>,t(BbWbt - BWt)]2 ≤ X X 2εn,txn>,t(BbWbt - BWt).	(5)
Next we bound the right-hand side of (5) via a uniform concentration argument. We let
B={B0∈Rd×k: kB0kmax ≤ 1},	W={W0∈Rk×T: kW0kmax≤1}.
For any fixed matrices B0 ∈ B, W0 ∈ W, We write W0 = (wj .… WT) and define
ηn,t(B 0, W0) = 2εn,txn>,t(B 0Wt0 - BWt).
Note that ηn,t (B0, W0) is an independent sub-Gaussian variable with sub-Gaussian norm
2xn>,t(B0Wt0 - BWt). By the general Hoeffding’s inequality (Lemma 1), with probability 1 - δ, we
have
No T	___________________
XXηn,t(B0, W0) . Pf(B0, W0)log(1∕δ),	(6)
n=1 t=1
NT
where f(B0, W0) = XX[xn>,t(B0Wt0 - BWt)]2.
n=1 t=1
Next we apply the e-net. Let B0 = N (B, ∣∣∙∣∣max, e), W 0 = N (W, ∣∣∙∣∣max, e). Applying an union
bound over B 0 × W0 for (6), we have
-	No T	-
Pr ∀(B0, W0) ∈ B0 × W0 : XX
ηn,t . Pf (B0, W0)log(1∕δ) ≥ 1 - δ∣B0 X W0|.
n=1 t=1
13
Published as a conference paper at ICLR 2021
Since f(B0, W0) is (3NTdk)-Lipschitz with respect to ∣∣∙∣∣mαχ, We have
-	No T	-
Pr ∀(B0, W0) ∈ B × W : XX
ηn,t . Pf (B0, W0)log(1∕δ) + 24NTdke ≥ 1 - δ∣B0 × W0|.
n=1 t=1
Note that	|B0|	= O((1∕e)dk)	and |W0|	=	O((1∕e)kT).	Let δ =	δo∕∣B0	X	W0|	and e
(24N T dk)-1. We have
"	No T	，
Pr ∀(B0, W0) ∈ B X W : XXηn,t . Pf(B0, W0)[(dk + kT)Iog(NTdk) + log(1∕δo)]
n=1 t=1
Now we assume that the above event holds. Combining (5) and (7), we have
Pr Jf (B, W) . P(dk + kT) Iog(NTdk) + log(1∕δo)
f(
Bb,Wc ) . f(Bb,Wc )[(dk + kT) log(N T dk) + log(1∕δ0)]
≥ 1 - δ0,
which proves our lemma.
□
Lemma 3. With probability 1 - O((N T)-2), for every m ∈ [M], t ∈ [T], we have
λmin I X	Xn,"X>,t,j
n=Gm-1+1
Proof. The proof can be done by following the proof of Lemma 4 in Han et al. (2020).
□
Lemma 4. For each epoch m ∈ [M], with probability 1 - O((NT-2)), we have
BW _ bw∣∣2 . (dk + kT)log(NTdk) + log(1∕δ)
∣∣f .	(Gm -Gm-ι)∕d	,
where B, W are computed at Line 5 in Algorithm 1.
Proof. Placing N0 = Gm- Gm-1 in Lemma 2, with probability 1 - O((NT)-2), we have
Gm	T
X X[xn>,t,an,t(Bbwbt -Bwt)]2 . (dk+kT)log(NTdk).
n=Gm-1 t=1
By Lemma 3, with probability 1 - O((NT)-2), we have
Gm	T
X X[xn>,t,an,t (Bb wbt -Bwt)]2
n=Gm-1 t=1
T	( Gm	∖
=E(B Wt- Bwt)1 I E xn,t,an,t x>,t,an,t I (B wt - Bwt)
t=1	n=Gm-1
T	GG
& X(BWt- Bwt)> Gm (G	(BWt- Bwt)
t=1
=Gm -GmTIIBW - bw∣∣2.
≥ 1 - δ0.
(7)
(8)
(9)
We conclude by combining (9) with (8).
□
14
Published as a conference paper at ICLR 2021
Let
Gm	T
Rm =	X X max hxn,t,a, θti - hxn,t,an,t, θti
n=Gm-1+1 t=1
be the regret incurred in the m-th epoch. We have the following lemma.
Lemma 5. We have
E[Rm] . (√NTdk + TVkN)Plog(NTdk)log(NKT).
Proof. At round n that belongs to epoch m, for task t, we have
max θt (xn,t,a - xn,t,an,t) ≤ max {θt (xn,t,a - xn,t,an,t) + θm-1,t(xn,t,an,t - xn,t,a)}
a∈[K]	a∈[K]
= max (θt - θm-1,t) xn,t,a + (θm-1,t - θt) xn,t,ant
a∈[K]	,
≤ 2 max (θt - θm-1,t ) xn,t,a .
By Assumption 2 and a union bound over K actions, T tasks, and (Gm - Gm-1) rounds, we have
with probability 1 - (NKT)-2, for every n ∈ (Gm-1, Gm] and t ∈ [T],
max (θt - θbm-1,t)>xn,t,a
log(NKT)
d
(10)
Using a union bound over (10) and Lemma 4, with probability 1 - O((N T)-2), the regret incurred
in the m-th epoch is
Gm T
E[Rm] .	m∈[aKx]θt>(xn,t,a - xn,t,ant )
n=Gm-1+1 t=1 a∈[K]	,
.(Gm - Gm-I)X g bm-1,t∣∣r0g(NKTI ≤ Gm√T]θ 一 Θ |\r0gNKTl
t=	(11)
≤Gm√T S (dk+ kT )?NTdk)产产	(12)
(Gm-1 - Gm-2)/d	d
.PNT(dk + kT) Iog(NTdk) Iog(NKT)	(13)
.(√NTdk + T√kN) Plog(NTdk) Iog(NKT),	□
where We denote Θ = (θm,-ι,ι .… θm-ι,τ) m (11) and the second inequality m (11) uses
Cauchy, (12) uses Lemma 4, (13) uses Gm-ι . Gm-ι 一 Gm-2 and Gm/Gm-ι . √N. Since that
the above bound holds with probability 1 - O((N T)-2) and that the regret is bounded by NT, we
prove the lemma.
Proof of Theorem 1. The regret is bounded by
M
E[RN,T] = X E[Rm] . M(√NTdk + T√kN)Plog(NTdk)log(NKT)
m=1
=(√NTdk + T√kN) PIog(NTdk) Iog(NKT) log log N.
□
B Proof of Theorem 2
In this appendix, we assume that Σt = I in Assumption 2 and that all noises are gaussian, i.e.
εn,t 〜N(0,1) for all n ∈ [N],t ∈ [T].
15
Published as a conference paper at ICLR 2021
For each task t ∈ [T], we denote the regret incurred on task t as
RN,(t) =	max hxn,t,a, θti - hxn,t,an,t,θti.
n=1 a∈[K]
We divide Theorem 2 into the following two lemmas.
Lemma 6. Under the setting of Theorem 2, we have inf a SuPI E[RN,T ] ≥ Ω(T √kN).
Lemma 7. Under the setting of Theorem 2, we have inf a SuPI E[RN,T ] ≥ Ω( y/dkNT).
Proofof Theorem 2. We combine Lemma 6 and Lemma 7.	□
Our proofs to the lemmas will be based on the lower bounds for the (single-task) linear bandit
setting, which corresponds to the T = 1 case in our multi-task setting. For this single-task setting,
we assume k = d and B = Id . We write the regret as RN = RN,1 and call algorithms for the
single-task setting as single-task algorithms.
Lemma 8 (Han et al. (2020), Theorem 2). Assume N ≥ d2 and d ≥ 2. Let N(μ, Σ) be the
multivariate normal distribution with mean μ and covariance matrix Σ. There is a constant C > 0,
such that for any single-task algorithm S, we have
sup E[RNI] ≥ C√dN,
kwk≤1	,
where I is the instance with hidden linear coefficients w.
Next we use it to prove Lemma 6 and Lemma 7. The main idea to prove Lemma 6 is to note that we
can treat our setting as T independent k-dimensional linear bandits.
Proof of Lemma 6. Suppose there is an algorithm A that achieves SuPI E[RNA,,IT] ≤ CTVkN.Then
we have
-T	-
SuP E X RSt ≤ SuP E[RN,T] ≤ CT√kN.
kwtk≤1	t=1	,	I	,
Therefore, there exists t ∈ [T] such that
sup E[RN,(tt] ≤ -1 CT√kN = C√kN,
I	A,I	T
which contradicts Lemma 8.	□
Proof of Lemma 7. Suppose there is an algorithm A that achieves SuPI E[RNA,,IT] ≤ CVdkNT. We
complete the proof separately, based on whether k ≤ 由 or not. Note that when k > $,the lower
bound in Lemma 6 becomes Ω(T√kN) = Ω(T√dN). Since T ≥ k, we have √dkTN . T√dN.
Thus we conclude by Lemma 6.
In the remaining, we assume k ≤ d. Without loss of generality, we assume that d is even and that
2k divides T. For i = 1, . . . , k, we denote the regret of group i as
iT/k
RN,((i))	X	RN,(t)
RA,I	=	RA,I .
t=(i-1)T /k+1
We consider instances such that tasks from the same group share the same hidden linear coefficients
θt . Since there are k groups, we have
-k	-
SuP E X RNIi) ≤ Sup E[rN,T] ≤ C√dkNT.	(14)
kθtk≤1	i=1	,	I	,
16
Published as a conference paper at ICLR 2021
Therefore, there exists a group i ∈ [k] such that
SUp E[RN,,I(i))] ≤ 1 c√dkNT = CjdNT,
(15)
which means that the regret incurred in group i is less than C，dNT/k. Since the tasks in group i
share the same hidden linear coefficients, they could be regarded as one large linear bandit problem.
Since there are T/k tasks in group i, the large linear bandit is played for N ∙ T/k rounds. By
Lemma 8, the algorithm A must have incurred C,dNT/k regret on group i, which contradicts
with (15).	□
C Method-of-Moments Estimator under Bandit Setting
The following theorem shows the guarantee of the method-of-moments (MoM) estimator we used
to find the linear feature extractor B . In this appendix, for a matrix B with orthogonal columns, we
write B⊥ to denote its orthogonal complement matrix (a matrix whose columns are the orthogonal
complement of those of B).
Theorem 5 (MoM Estimator). Assume NT & Polylog(Nι, T) ∙ d1V∙ We have Withprobability at
least 1 - (N1T)-100,
Bl. O（二）.
(16)
The theorem guarantees that our
estimated Bb is close to the underlying B in the operator
norm so
long as the values N1 and T are sufficiently large. We add a remark that our theorem is similar to
Theorem 3 in Tripuraneni et al. (2020). The key differences are: (i) we use a uniform distribution
to find the feature extractor, while they assumed the input distribution is standard d-dimensional
Gaussian; (ii) the SNR (signal-to-noise ratio) in our linear bandit setting is worse than that in their
supervised learning setting, and thus we get an extra d factor in our theorem.
In the sequel, we prove the theorem.
Lemma 9 (Hoeffding). Let ε1 , . . . , εN be i.i.d. 1-sub-Gaussian random variables. We have
Pr
1N
N ΣS(εi - Eεi) ≥ t
A	t2
≤ 2e-2N.
Lemma 10 (Matrix Bernstein’s inequality, Vershynin (2018), Theorem 5.4.1). Let X1, . . . , Xm ∈
Rd×d be independent, mean zero, symmetric random matrices that kXi k ≤ M almost surely for all
i ∈ [m]. Let σ2 = llPim=1 EXi2 ll. We have
Pr
m
XXi
i=1
≥δ
≤ 2d exp
—
Equivalently, with probability at least 1 - δ, we have
m
X Xi . √σ2 log(d∕δ) + M log(d∕δ).
i=1 l
Lemma 11 (Moments of Uniform Distribution on Sphere). Let X 〜 Unif(Sd-1) be a uniformly
chosen unit vector. We have E x? = d(d+11d+4), E x4 = 或小),E x2 = d. Moreover, we have
E χ1χ2 = d(d+23(d+4).
Proof. We need to recall the fact that when X 〜Unif(Sd-1), its coordinate Xi follows the Beta
distribution: x1+1 〜 Beta(d-1, d-1). Then We prove the lemma by noting the moments of the
Beta distribution (Fang, 2018).	□
17
Published as a conference paper at ICLR 2021
Corollary 6 (Uniform Distribution on Sphere). Let X 〜Unif(SdT) be a uniformly chosen Unit
vector. We have the following statements.
(a)	E hx, θi2xx> = 2θθ++I∙
(b)	E hχ θi4χχ> = 12θθ>+3I
Ib) E hx，θi Xx = d(d+2)(d+4”
Proof. Let e1 = (1, 0, . . . , 0) ∈ Rd be the unit vector. For (a), note that
(E hx, e1i2xx>)ij = E x21xixj
0,
0,
1
d(d+2)，
3
d(d+2),
i, j 6= 1 and i 6= j,
i= 1 6=jorj= 1 6=i,
i = j 6= 1,
i=j=1.
Therefore, We have E[hx, eQ2xx>] = djd+2) (2eιe> +1). By the isotropy (rotation invariance) of
uniform distribution, we have
E hx, θi2xx>=d(d⅛(2θθ>+I).
For (b), note that
(E hx, e1i4xx>)ij = E x41 xixj
0,
0,
3
d(d+2)(d+4)，
15
d(d+2)(d+4)，
i, j 6= 1 and i 6= j,
i=1 6=jorj=1 6=i,
i = j 6= 1,
i=j=1.
Therefore, we have E hx, eι>4xx> = d(d+2)(d+4) (12eιe> + 3I). By the isotropy, we have
E hx，θi4xx> = d(d +21)(d + 4)(12θθ> + 3I).
□
Let An,t = rn,tXn,tχ>,t, M = NT PN= 1 PT=I An,t. We decompose M into three terms M
M1 + M2 + M3 , where
M1
M2
M3
N1	T
NITE Ehxn,t, θti2xn,tx>,t,
1	n=1 t=1
1	N1	T
NrEE2εn,thxn,t, θtixn,txn,t,
1	n=1 t=1
N1	T
NT XX εn,txn,tx>,t.
n=1 t=1
Let Aint = N1Thxn,t, θti2xn,tx>,t. Next we analyze each error ∣∣Mi - EMik.
Lemma 12. With probability at least 1 - N31T3, we have
log(dN1T) * log(dN1T)
kM1 - EMIk . V d3ΝιT	+ NT
Proof. We have
2	1	4	>	12θt θt> + 3I
E AInt = N2T2 E hxn,t, θti XMXnt = d(d +2)(d +4)N2T2.
Using Lemma 10 with m = NT, M = N1T, σ2 . m ∙ 4331? = ^NNT, we have with probability
at least 1 — N3T3,
log(dN1T)	log(dN1T)
kM1 - EM1 k . V d3NT	+	N1T
□
18
Published as a conference paper at ICLR 2021
Next we analyze the errors of M2 and M3 . Since these errors contain the unbounded sub-Gaussian
terms εn,t, we need to cut their tails before applying the matrix Bernstein inequality. Define ε0n,t =
εn,t I{∣εn,t∣ ≤ R}. We have
∣Eεn,t - Eεn,t∣ ≤ E 战"| I{%,t| > R}
=R ∙ Pr[∣εn,t∣ > R] + Z	Pr[∣εn,t∣ > x] dx
R
一	R2	∕,+∞	χ2、
≤ 2Re-ɪ +	2e-ɪ dx
R
≤ 2(R +ɪ)e-畛.
R
Let ε'n,t = εn,t I{|ln,t| ≤ R}. Wehave
∣∣E ε2n,t - E ε0n0,t∣∣ ≤ E ε2n,t I{ε2n,t >R2}
+∞
=R2 ∙ Pr [εn,t > R2]+ /	pMεn,t > x) ddχ
R2
R2	+∞ x
≤ 2R2e-ɪ +	2e-2 ddx
R2
R2
≤ (2R2 +4)e-F.
Define
1 N1 T
M2 = NT ΣΣ2ε'n,t hxn,t, θtixn,txn>,t,
1 n=1 t=1
N1 T
M3' = NT XXεn,txn,tx>,t.
1 n=1 t=1
Lemma 13. With probability at least 1 - (N1 T)-3, we have
'	'Ilog(dNιT) . R"NιT)
kM2 - E M2k .1-NTT+ + NT
Proof. Let A2nt
N1T2ε0nthxn,t, θtixn,tx>,t. We find that
E A2nt = 4EN⅛^ E hxn,t, θti2xn,tx>,t =	2^ .
1	1	(+	)
We conclude by using Lemma 10 with m = NT, M = NRT, σ2 . m ∙
Lemma 14. With probability at least 1 - (N1T)-3, we have
1
1
------- _ ' 〜 '
d2 N2T2__d2N1T .
□
kM300 - E M300k .
Iog(dN1T)+ R2 log(dN1T)
dN1T
N1T
Proof. Let A3nt = NTε^x%tx>,t∙ We find that
2	E (ε0n0,t)2	>
E A3nt = N2T2 E xn,txn,t
E ©J I
N2T2 d.
We conclude by using Lemma 10 with m = NT, M = NT, σ
Lemma 15. With probability at least 1 - (N1T)-2, we have
dN2T2 = dNιT .
2 . m ∙
1
1
□
kM-EMk.
log(dN1T)+ log2 (dN1T)
dN1T
N1T
19
Published as a conference paper at ICLR 2021
Proof. Let R =，8 Iog(NIT). With probability at least 1 -(N↑T)-3, We have ∣ε%t | ≤ R for every
n ∈ [N1] and t ∈ [T]. Note that in this case, we have M2 = M20 and M3 = M300. Using a union
bound over Lemma 12, Lemma 13, and Lemma 14, We have With probability at least 1 - (N1T)-2,
kM -EMk ≤ kM1-EM1k + kM2 -EM2k + kM3 -EM3k
= kM1-EM1| + kM20 -EM2k + kM300 -EM3k
≤ kM1-EM1k + kM20 -EM20k + kE M20 -EM2k
+ kM300 - E M300k + kE M300 -EM3k
.l log(dNιT) + log2(dN1 T)
.V	dNιT	+	N1T	.
Proof of Theorem 5. We note that σk+1(M) - σk+1(E M) ≤ kEk. Under Assumption 3, We have
E ML d(⅛ 附> + c1I,	E M2=0,	E M3 = c3I,
Where c1, c3 ∈ R are constants. Since
σk(Tθθ>) - σk+1(Tθθ>) = σk(TWW>) = k,
T	T	Tk
We have
σk(E M)- σk+1(E M) X 去
Assume NT & Polylog(Nι,T) ∙dk So that kEk ≤ d0ν. TogetherWithDavis-Kahan sin θ theorem,
We have With probability at least 1 - (N1T)-100,
kB >Bk.	kB>EBk
k ⊥ k . σk(EM) - σk+ι(EM) - ∣∣E∣∣
≤kEI.	kEk
≤ σk(EM) - σk+ι(EM) - ∣IEk 〜λoν∕d2k
(17)
log(dN1T)
dNιT
log2 (dNT)
+ —NT一
d1.5 k
).∙ Polylog(d,N,T),
N1T
where (17) uses the Davis-Kahan sin θ theorem (Bhatia, 2013, Section VII.3).	□
D Proof of Theorem 3
Our proof is similar to the proof of Theorem 3.1 of Rusmevichientong & Tsitsiklis (2010).
Lemma 16 (Rusmevichientong & Tsitsiklis (2010), Lemma 3.5). For two vectors u, v ∈ Rd, we
have
u v	2ku - v k
kuk	kvk ≤ max{kuk, kvk}.
ɪ . . ._Yfr,	/ A ∖ τττ 1
Lemma 17. Let xt = arg maxx∈At hx, θti. We have
maxhx, θti - hxt, θti
x∈At
≤ Jkθt- θtk2
≤	kθtk
20
Published as a conference paper at ICLR 2021
Proof. For θ ∈ Rd, We define ft(θ) = maxα∈At {a, θ). Let 唠 =argmaXxeAt {x,仇〉.Then We
have
max hx, θti - hxt, θti = hxt - xt, θti = hxt, θt - θti + hxt - xt, θti + hxt, θt - θti
χ∈At
,... ^, ,	^ ≤ hxt, θt	— θti+ hxt,	θt	—	, ^ . θti = hxt - xt, θt - θti
. ^. ^. =hft(θt) — ft(θt), θt — θti	_	^ θt	θt =hft(两)-ft(两), θt - θti
∩ ∩ ≤kft(南)-ft(两)k'	kθt-θtk≤jk∣θ⅛-∣⅛"θt-θtk (18)
<9J kθt - θtk2 ≤	kθtk ,	(19)
where the first inequality in (18) uses Cauchy and (19) uses Lemma 16.	□
Lemma 18. For each task t ∈ [T], we have E∣∣θt — θt∣∣2 . λk~N^ + ∣∣Bb>Bk2.
Proof. We define θ0 = BBτθt. Note that θt = θ0 + B⊥B>. We have
θt - θt = B Wt — (B B τθt + 瓦 BT)θt
^ . -	^-r	.	^	^ -T
=B(Wt- Bτθt) — B⊥B⊥Bwt.
Note that B is perpendicular to B⊥ and that ∣Bk = ∣∣B⊥∣ = 1. We have
∣∣θt — θtk2 = IlB(Wt- Bτθt)k2 + ∣∣B⊥B>Bwtk2 ≤ ∣Wt — Bτθt∣∣2 + IlBTb∣∣2.
Let Vi = √λ0bi. The OLS estimator is given by
/N1 + N2 + l	∖T N1 + N2
Wt =( X	BTvi VTB)	X BTxn"n,t
∖n=Nι + 1	J	n=Nι + 1
∕N1+N2+1 ʌ	八 T N1+N2 ʌ	ʌ
=I	^X	BTxn,t xn,t B)	^X Bτ Xn,t(xn,tBwt + εn,t)
n n=N1 + 1	n	n=N1+1
/N1 + N2 + l	T N1+N2 ʌ
=wt + I	bt	BTxn,txn,tB I	bt BTxn,tεn,t.
∖n=N1 + 1	J	n=N1 + 1
Write A = PN=NN++1 BTxn,txn,tB.
N1 +N2
EkWt- wtIi2 = X	xn,tBAT2BTxn,tEɛn,t
n=N1 + 1
N1 +N2
≤ X	xn,tBAT2BTxn,t
n=N1 + 1
k C
≤ ∙AT2k≤ N2(E)2
k2
λ0N2
Putting together, We have
ʌ _ k2 一 一
E∣∣θt — θtll ≤ Tn2 + kB⊥Bk .
□
21
Published as a conference paper at ICLR 2021
Proof of Theorem 3. Note that J = O(1) under our assumptions, as indicated by Rusmevichientong
& Tsitsiklis (2010). So we have
E[RN,T] ≤ TNi + TN + T(N - Ni - NjJ吗-刎2
kθtk
≤ TN1 + TN2 + TNJkB>Bk2 + k2∕(λ2N2)
ω
d3 k2	k2
.TNi + TN2 + TN ∙- log3 (NT) + TNKr
Ni T	N2
Nd3 k2	k2
≤ TNi + -^r- log3 (NT) + TN2 + TNk
Ni	N2
≤ di-5k√NTlog3(NT) + kT√N.	□
E	Proof of Theorem 4
In this appendix, we assume all action sets are spherical, i.e. An,t ≡ Sd-i for n ∈ [N], t ∈ [T].
Note that these action sets meet Assumption 3.
For each task t ∈ [T], we use RN,(t) = PnN=i [maxx∈An,t hx, θti -E hxn,t, θti] to denote the regret
incurred on task t.
Lemma 19. Under the setting of Theorem 4, we have inf a SuPI E[RN'T] ≥ CkT√N, where
C = 0.0001.	,
Lemma 20. Under the setting of Theorem 4, we have inf a SuPI E[RN'T ] ≥ CdVkTN, where
C = 0.0001.	,
Proof of Theorem 4. We combine Lemma 19 and Lemma 20.
□
The proofs in this appendix would largely follow the proofs in Appendix B, yet this appendix is
much longer, because we need to construct instances that satisfies Assumption 3, Assumption 4, and
Assumption 5.
Our proofs to the lemmas are based on the lower bounds for the (single-task) linearly parameterized
bandit setting, which corresponds to the T = 1 case in our setting. For this single-task setting, we
assume k = d and B = Id . (This setting need not meet Assumption 4 and Assumption 5). We write
the regret as RN = RN,i and call algorithms for the single-task setting as single-task algorithms.
We invoke the following lower bound for the single-task setting.
Lemma 21. Assume N ≥ d2 and d ≥ 2. Let N(μ, Σ) be the multivariate normal distribution with
mean μ and Covariance matrix Σ. For any Single-task algorithm, we have
E	[RN ∙ I{0.09 ≤ IlWk ≤ 3}] ≥ 0.006d√N.	(20)
W 〜N (0,id∕d)
The lemma can be proved by following the proof of Theorem 2.1 of Rusmevichientong & Tsitsiklis
(2010). Next we use it to prove Lemma 19 and Lemma 20. The main idea to prove Lemma 19 is to
note that we can treat our setting as T independent k-dimensional linear bandits.
Let μd be the conditional probability measure whose density function is given by
f(x) =
g(x)
PrX [0.09≤∣∣Xk≤3],
0,
0.09 ≤ kxk ≤ 3,
otherwise,
where X 〜N(0, Id/d) is the multivariate gaussian vector and g(χ) is the probability density
function of X . Then (20) implies
E [RN] ≥ 0.006d√N.
W〜μd
(21)
22
Published as a conference paper at ICLR 2021
Proof of Lemma 19. Without loss of generality, we assume 2k divides T . Suppose, for contradic-
tion, that there is an algorithm A0, such that for every instance I, it incurs regret E[RNA0,,TI] ≤
CTk√N. We replace the condition ∣∣wtk ≤ 1 by ∣∣wtk ≤ 3 in our setting. Note that A0 implies an
algorithm A that incurs regret E[RN,T] ≤ 3CTk√N.
We construct the following instances I = (B, W), where B = (Ik 0)> and W =
(wι … WT) as follows. Let
w1 = ,一wT∕2k = e1, wτ∕2k+1 = ,一=wT/k = e2,∙∙∙, w(k-1)T∕2k + 1 = ,一=wT∕2 = ek,
where e1, . . . , ek ∈ Rk is the standard basis. Let
wT∕2+1, ..., wT 〜μd
be i.i.d. drawn. Thanks to the first T tasks, the instance I always satisfies Assumption 4. Note that
∣wt∣ ≥ 0.09, so the instance also satisfies Assumption 5. Then we have
E	[RN,T] ≤ 3CTk√N.
wτ∕2+ι,…,wτ〜μd	,
Thus
T
X
t=T /2+1
E	[RAN,,I(t)] ≤
WT∕2 + 1,...,WT 〜Vd
E	[RN,T] ≤ 3CTk√N.
WT∕2 + 1,...,WT ~Nd	，
Therefore, we can find t ∈ [T/2 + 1, T] such that
E	[RN善]≤ 3C2k√N = 6Ck√N.
WT/2 + 1 ,∙∙∙,WT 〜Vd	T/2
We note that the expectation operator Ewτ∕2+1,...,wτ〜μd is over all randomness on tasks τ = t and
its parameter wτ . So there is a realization of the randomness on other tasks τ 6= t that satisfies
E [RN,∣t) | wτ, ετ,τ = t] ≤ 6Ck√N.
Wt〜μd	,
Based on the realization wτ , ετ , we design a single-task algorithm S, which plays task t and simu-
lates other tasks τ 6= t with wτ , ετ . The algorithm achieves
E [RNI] = E [RN,∣t) | wτ, ετ,τ = t] ≤ 6Ck√Ν,
W 〜μd	,	Wt 〜μd	,
which contradicts to (21) because C = 0.0001.	□
Proof of Lemma 20. Without loss of generality, we assume that d is even and that 2k divides T.
Suppose, for contradiction, that there is an algorithm A0, such that for every instance I, it incurs
regret E[RN0Tι] ≤ Cd√kTΝ. We replace the condition ∣∣wt∣ ≤ 1 by ∣wt ∣ ≤ 3 in our setting.
Note that A0 implies an algorithm A that incurs regret E[RN'T] ≤ 3Cd√kTN.
We prove the lemma separately, based on whether k ≥ d or not.
1.	Consider k ≤ d. We generate k vectors ψι,...,ψk so that ψi 〜μd-i+ι. For every dimension i,
we consider a map
ci : Rd×i → Rd×(d-i+1),
(x1, . . . ,xi) 7→ (y1, . . . , yd-i+1),
so that when {x1, . . . , xi} are orthogonal, the set {y1, . . . , yd-i+1} is the orthonormal basis of the
orthogonal complement span{x1, . . . , xi}⊥ ofRd. Note that ci can be computed efficiently, e.g. by
Gram-Schmidt process.
23
Published as a conference paper at ICLR 2021
We define φι = ψι and φi = ci-1(φ1,..., φi-ι) ∙ ψi for i ≥ 2. We observe that {φι,..., φk} are
orthogonal. Next we define our instance I = (B, W), where B
For W = (wι •… WT), we let
bk)and bi = kφik.
w1 =…=wT/k = kφ1k ∙ e1,
wT/k+1 = •一=w2T∕k = kφ2k∙ e2,
w(k-1)T∕k + 1 =…=wT = kφk Il ∙ ek,
where {e1, . . . , ek} ⊆ Rk is the standard basis. Note that the instance I always satisfies Assump-
tion 4 and Assumption 5 by our choice ofwt. Note that we have divided the tasks into k groups, so
that each group share the same vector wt. For i = 1, . . . , k, we denote the regret of group i as
iT/k
RN,((i))	X	RN,(t)
RA,I =	RA,I .
t=(i-1)T /k+1
For the algorithm A, it incurs regret
E 同，)] ≤ 3Cd√kNT.
φ1,...,φk	,
So there is a group i ∈ [k], such that
E	[RAN,∣(i))] ≤ %Cd√kNT = 3Cd
φ1 ,...,φk ,	k
NT
k
Similar to the proof of Lemma 19, we can fix the randomness for groups j 6= i to obtain a realization,
such that
E
φi∖φj(j=i)
[RAN,I((i))
,
| φj, εj, j = i] ≤ 3 3Cd√kNT
k
NT
3CdV ɪ
Here We note that φi could depend on φ∣ for ∣ ≥ i + 1. Now We let ψ0 〜μd-k+ι and let
φ0i = Aψi0,	A = ck-1(φ1, . . . , φi-1, φi+1, . . . , φk).	(22)
Note that φ0i and φi | φj(j 6=i) are identical, so we have
E [RN,I(i)) | φj,εj,j = i] ≤ 3Cd√kNT = 3CdJNT.	(23)
φi=φ0i	,	k	k
We complete the proof by showing that (23) implies a single-task algorithm S that plays a (d-k+1)-
dimensional linear bandit for NT times. Let w = ψ0. Then w is independently drawn from μd-k+ι.
Next we design the algorithm S, which runs A by playing the tasks t ∈ T = {(i - 1)T /k +
1, . . . , iT /k} and simulates other tasks τ ∈/ T. Note that playing the task t ∈ T is the same as
playing the single-task bandit defined by w, because we have θt = Bwt = φi = Aψi0 and the
matrix A is known (as in (22)) after we fix the randomness on other tasks. Since |T| = T and A is
played for N times, S can play the single-task bandit specified by W for NT times. As a result, we
have
E	[RSN,I(I)] = E [RAN,,I((i)) | φj,εj,j 6=i]
w^μd-k+ι	Φi=Φi
NT
≤ 3CdVɪ
≤ 6C(d-k+1)
NT
k,
which contradicts to (21) because C = 0.0001.
2.	Consider k > d. In this case, the lower bound in Lemma 19 becomes Ω(Tk√N) = Ω(Td√N).
Since T ≥ k, we have d√kTN . Td√N. Thus we conclude by Lemma 19.	□
24
Published as a conference paper at ICLR 2021
5 0 5 0 5 0
♦ ♦♦♦♦♦
3 3 2 2 1 1
*s舀 jədJx
100	200	300 100	200	300 100	200	300
T: Number of Tasks
Figure 5: Comparisons of E2TC with PEGE for k = 3.
0 5 0 5 0
♦ ♦ ♦ ♦ ♦
3 2 2 1 1
*sg jəd jəjəm
50
100
150 50	100	150 50
T: Number of Tasks
100	150
Figure 6: Comparisons of E2TC with PEGE for k = 2.
F	Experiments for Infinite-Arm Setting
Setup In all experiments, we set d = 10, N = 104 and the action At = Sd-1. The parameters
are generated as follows. We consider k = 2,3 in our experiments. The noise ε%t 〜 N(0,1) are
i.i.d. Gaussian random variables. To verify our theoretical results, We consider a hyper-parameter
C ∈ {0.5,1,1.5, 2}. For each c, we run E2TC with N = dckypN and N = k√N.
Results and Discussions We present the simulation results in Figure 5 and Figure 6. We empha-
size that the y-axis in our figures corresponds to the regret per task, which is defined as RegN,T.
Our main observation is that only when the number of tasks T is large and we choose the right
scaling N = d1.5k JT, our method can outperform the PEGE algorithm. We discuss several
implications of our results. First, representation learning does help, especially when there are many
tasks available for us to learn the representation, as we see in all figures that the regret per task of
E2TC decreases as T increases. Second, the help of representation learning is bounded. In the
figures, we see that the curves of E2TC bends to a horizontal line as T increases, which suggests
a lower bound on the regret per task. Meanwhile, we also proved an Ω(k√N) lower bound on the
regret per task in Theorem 4. Third, representation learning may have adverse effect without enough
task. In our figures, this was established by noting that our algorithm cannot outperform PEGE
when T is small. This corresponds to the “negative transfer” phenomenon observed in previous
work (Wang et al., 2019). Fourth, the correct hyper-parameter c = 1.5 is crucial. For hyper-
parameter other than c = 1.5, the figures show that our algorithm would require much more tasks
to outperform PEGE. Lastly, by comparing the two figures, we notice that our algorithm has bigger
advantage when k is smaller, which corroborates the scaling with respect to k in our regret upper
bound. In contrast, PEGE does not benefit from a smaller k.
25