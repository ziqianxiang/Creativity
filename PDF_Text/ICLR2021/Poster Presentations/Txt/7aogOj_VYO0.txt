Published as a conference paper at ICLR 2021
Do not Let Privacy Overbill Utility: Gradient
Embedding Perturbation for Private Learning
Da Yu1,2，*, Huishuai Zhang2，*, Wei Chen2, Tie-Yan Liu2
1	School of Computer Science and Engineering, Sun Yat-sen University
2	Microsoft Research Asia
1	yuda3@mail2.sysu.edu.cn
2	{huzhang,wche,tyliu}@microsoft.com
Ab stract
The privacy leakage of the model about the training data can be bounded in the
differential privacy mechanism. However, for meaningful privacy parameters,
a differentially private model degrades the utility drastically when the model
comprises a large number of trainable parameters. In this paper, we propose an
algorithm Gradient Embedding Perturbation (GEP) towards training differentially
private deep models with decent accuracy. Specifically, in each gradient descent
step, GEP first projects individual private gradient into a non-sensitive anchor
subspace, producing a low-dimensional gradient embedding and a small-norm
residual gradient. Then, GEP perturbs the low-dimensional embedding and the
residual gradient separately according to the privacy budget. Such a decomposition
permits a small perturbation variance, which greatly helps to break the dimensional
barrier of private learning. With GEP, we achieve decent accuracy with reasonable
computational cost and modest privacy guarantee for deep models. Especially, with
privacy bound = 8, we achieve 74.9% test accuracy on CIFAR10 and 95.1% test
accuracy on SVHN, significantly improving over existing results.
1	Introduction
Recent works have shown that the trained model may leak/memorize the information of its training
set (Fredrikson et al., 2015; Wu et al., 2016; Shokri et al., 2017; Hitaj et al., 2017), which raises
privacy issue when the models are trained with sensitive data. Differential privacy (DP) mechanism
provides a way to quantitatively measure and upper bound such information leakage. It theoretically
ensures that the influence of any individual sample is negligible with the DP parameter or (, δ).
Moreover, it has been observed that differentially private models can also resist model inversion
attack (Carlini et al., 2019), membership inference attack (Rahman et al., 2018; Bernau et al., 2019;
Sablayrolles et al., 2019; Yu et al., 2021), gradient matching attack (Zhu et al., 2019), and data
poisoning attack (Ma et al., 2019).
One popular way to achieve differentially private machine learning is to perturb the training process
with noise (Song et al., 2013; Bassily et al., 2014; Shokri & Shmatikov, 2015; Wu et al., 2017;
Fukuchi et al., 2017; Iyengar et al., 2019; Phan et al., 2020). Specifically, gradient perturbation
perturbs the gradient at each iteration of (stochastic) gradient descent algorithm and guarantees
the privacy of the final model via composition property of DP. It is worthy to note that gradient
perturbation does not assume (strongly) convex objective and hence is applicable to various settings
(Abadi et al., 2016; Wang et al., 2017; Lee & Kifer, 2018; Jayaraman et al., 2018; Wang & Gu, 2019;
Yu et al., 2020). Specifically, for given gradient sensitivity S, a general form of gradient perturbation
is to add an isotropic Gaussian noise z to the gradient g ∈ Rp independently for each step,
g = g + z, where Z 〜N(0,σ2S2Ip×p).	(1)
One can set proper variance σ2 to make each update differentially private with parameter (, δ). It is
easy to see that the intensity of the added noise E[kzk2] scales linearly with the model dimension p.
* Authors contribute equally to this work.
1
Published as a conference paper at ICLR 2021
ReSNet20 ReSNet32 ResNet44 ReSNet56 ReSNetilO
Model Name
Figure 1: Noise norm vs gradient norm
of ResNet20 at initialization. The noise
variance is chosen such that SGD sat-
isfies (5, 10-5)-DP after 90 epochs in
Abadi et al. (2016).
Figure 2: Stable rank ∣∣ ∙ kF/k∙∣∣2 (TroPP
et al., 2015) of batch gradient matrix of
given grouPs (with p Parameters). The
setting is ResNet20 on CIFAR-10. The
stable rank is small throughout training.
This indicates that as the model becomes larger, the useful signal, i.e., gradient, would be submerged
in the added noise (see Figure 1). This dimensional barrier restricts the utility of deeP learning models
trained with gradient Perturbation.
The dimensional barrier is attributed to the fact that the added noise is isotroPic while the gradients
live on a very low dimensional manifold, which has been observed in (Gur-Ari et al., 2018; Vogels
et al., 2019; Gooneratne et al., 2020; Li et al., 2020) and is also verified in Figure 2 for the gradients
of a 20-layer ResNet (He et al., 2016). Hence to limit the noise energy, it is natural to think
“Can we reduce the dimension of gradients first and then add the isotroPic noise onto a low-dimensional
gradient embedding?"
The answer is affirmative. We ProPose a new algorithm Gradient Embedding Perturbation (GEP),
illustrated in Figure 3. SPecifically, we first comPute anchor gradients on some non-sensitive auxiliary
data, and identify an anchor subspace that is sPanned by several toP PrinciPal comPonents of the
anchor gradient matrix. Then we Project the Private gradients into the anchor subsPace and obtain
low-dimensional gradient embeddings and small-norm residual gradients. Finally, we Perturb the
gradient embedding and residual gradient seParately according to the sensitivities and Privacy budget.
We intuitively argue why GEP could reduce the Perturbation variance and achieve good utility for
large models. First, because the gradient embedding has a very low dimension, the added isotroPic
noise on embedding has small energy that scales linearly only with the subsPace dimension. Second,
if the anchor subsPace can cover most of the gradient information, the residual gradient, though
high dimensional, should have small magnitude, which Permits smaller added noise to guarantee the
same level Privacy because of the reduced sensitivity. Overall, we can use a much lower Perturbation
comPared with the original gradient Perturbation to guarantee the same level of Privacy.
We emPhasize several ProPerties of GEP. First, the non-sensitive auxiliary data assumPtion is weak.
In fact, GEP only requires a small number of non-sensitive unlabeled data following a similar
feature distribution as the Private data, which often exist even for learning on sensitive data. In our
exPeriments, we use a few unlabeled samPles from ImageNet to serve as auxiliary data for MNIST,
SVHN, and CIFAR-10. This assumPtion is much weaker than the Public data assumPtion in Previous
works (PaPernot et al., 2017; 2018; Alon et al., 2019; Wang & Zhou, 2020), where the Public data
should follow exactly the same distribution as the Private data. Second, GEP Produces an unbiased
estimator of the target gradient because of releasing both the Perturbed gradient embedding and
the Perturbed residual gradient, which turns out to be critical for good utility. Third, we use power
method to estimate the PrinciPal comPonents of anchor gradients, achievable with a few matrix
multiPlications. The fact that GEP is not sensitive to the choices of subsPace dimension further allows
a very efficient imPlementation.
ComPared with existing works of differentially Private machine learning, our contribution can be
summarized as follows: (1) we ProPose a novel algorithm GEP that achieves good utility for large
models with modest differential Privacy guarantee; (2) we show that GEP returns an unbiased
estimator of target Private gradient with much lower Perturbation variance than original gradient
Perturbation; (3) we demonstrate that GEP achieves state-of-the-art utility in differentially Private
learning with three benchmark datasets. SPecifically, for = 8, GEP achieves 74.9% test accuracy
2
Published as a conference paper at ICLR 2021
Figure 3: Overview of the proposed GEP approach. 1) We estimate an anchor subspace on some
non-sensitive data; 2) We project the private gradients into the anchor subspace, producing low-
dimensional embeddings and residual gradients; 3) We perturb the gradient embedding and residual
gradient separately to guarantee differential privacy. The auxiliary data are only required to share
similar features as the private data. In our experiments, we use 2000 images from ImageNet as
auxiliary data for MNIST, SVHN, and CIFAR-10 datasets.
on CIFAR-10 with a ResNet20 model. To the best of our knowledge, GEP is the first algorithm that
can achieve such utility with training deep models from scratch for a “single-digit" privacy budget1.
1.1	Related work
Existing works studying differentially private machine learning in high-dimensional setting can be
roughly categorized into two sets. One is treating the optimization of the machine learning objective
as a whole mechanism and adding noise into this process. The other one is based on the knowledge
transfer of machine learning models, which trains a differentially private publishable student model
with private signals from teacher models. We review them one by one.
Differentially private convex optimization in high-dimensional setting has been studied extensively
over the years (Kifer et al., 2012; Thakurta & Smith, 2013; Talwar et al., 2015; Wang & Xu, 2019;
Wang & Gu, 2019). Although these methods demonstrate good utility on some convex settings, their
analyses can not be directly applied to non-convex setting. Right before the submission, we note two
independent and concurrent works (Zhou et al., 2020; Kairouz et al., 2020) that also leverage the
gradient redundancy to reduce the added noise. Specifically, Kairouz et al. (2020) track historical
gradients to do dimension reduction for private AdaGrad. Zhou et al. (2020) requires gradients on
some public data and then project the noisy gradients into a public subspace at each update. One core
difference between these two works and GEP is that we introduce residual gradient perturbation and
GEP produces an unbiased estimator of the private gradients, which is essential for achieving the
superior utility. Moreover, we weaken the auxiliary data assumption and introduce several designs
that significantly boost the efficiency and applicability of GEP.
One recent progress towards training arbitrary models with differential privacy is Private Aggregation
of Teacher Ensembles (PATE) (Papernot et al., 2017; 2018; Jordon et al., 2019). PATE first trains
independent teacher models on disjoint shards of private data. Then it trains a student model with
privacy guarantee by distilling noisy predictions of teacher models on some public samples. In
comparison, GEP only requires some non-sensitive data that have similar natural features as the
private data while PATE requires the public data follow exactly the same distribution as the private data
and in practice it uses a portion of the test data to serve as public data. Moreover, GEP demonstrates
better performance than PATE especially for complex datasets, e.g., CIFAR-10, because GEP can
train the model with the whole private data rather than a small shard of data.
2	Preliminaries
We introduce some notations and definitions. We use bold lowercase letters, e.g., v, and bold capital
letters, e.g., M, to denote vectors and matrices, respectively. The L2 norm of a vector v is denoted
by kvk. The spectral norm and the Frobenius norm of a matrix M are denoted by kM k and kM kF,
respectively. A sample d = (x, y) consists of feature x and label y. A dataset D is a collection
of individual samples. A dataset D0 is said to be a neighboring dataset of D if they differ in a
IAbadi etal. (2016) achieve 73% accuracy on CIFAR-10 but they need to pre-train the model on CIFAR-100.
3
Published as a conference paper at ICLR 2021
single sample, denoted as D 〜D0. Differential privacy ensures that the outputs of an algorithm on
neighboring datasets have approximately indistinguishable distributions.
Definition 1 ((, δ)-DP (Dwork et al., 2006a;b)). A randomized mechanism M guarantees (, δ)-
differential privacy iffor any two neighboring input datasets D 〜 D andfor any subset of outputs S
it holds that Pr[M(D) ∈ S] ≤ ePr[M(D0) ∈ S] + δ.
By its definition, (, δ)-DP controls the maximum influence that any individual sample can produce.
One can adjust the privacy parameters to trade off between privacy and utility. Differential privacy
is immune to post-processing (Dwork et al., 2014), i.e., any function applied on the output of a
differentially private algorithm would not increase the privacy loss as long as it does not have
new interaction with the private dataset. Differential privacy also allows composition, i.e., the
composition of a series of differentially private mechanisms is also differentially private but with
different parameters. Several variants of (, δ)-DP have been proposed (Bun & Steinke, 2016; Dong
et al., 2019) to address certain weakness of (, δ)-DP, e.g., they achieve better composition property.
In this work, We use Renyi differential privacy (Mironov, 2017) to track the privacy loss and then
convert it to (, δ)-DP.
Suppose that there is a private dataset D = {(xi, yi)}in=1 with n samples. We want to train a model f
to learn the mapping in D. Specifically, f takes x as input and outputs a label y , and f has parameter
θ ∈ Rp. The training objective is to minimize an empirical risk 1 Pn=I '(f (xi), yi), where '(∙, ∙)
is a loss function. We further assume that there is an auxiliary dataset D(a) = {(Xj, yj)}m=1 that X
shares similar features as X in D while y could be random.
3	Gradient embedding perturbation
An overview of GEP is given in Figure 3. GEP has three major ingredients: 1) first, estimate an anchor
subspace that contains the principal components of some non-sensitive anchor gradients via power
method; 2) then, project private gradients into the anchor subspace and produce low-dimensional
embeddings of private gradients and residual gradients; 3) finally, perturb gradient embedding and
residual gradient separately to establish differential privacy guarantee. In Section 3.1, we present the
GEP algorithm in detail. In Section 3.2, we given an analysis on the residual gradients. In Section 3.3,
we give a differentially private learning algorithm that updates the model with the output of GEP.
3.1	The GEP algorithm and its privacy analysis
The pseudocode of GEP is presented in Algorithm 1. For convenience, we write a set of gradients
and a set of basis vectors as matrices with each row being one gradient/basis vector.
The anchor subspace is constructed as follows. We first compute the gradients of the model on an
auxiliary dataset D(a) with m samples, which is referred to as the anchor gradients G(a) ∈ Rm×p.
We then use the power method to estimate the principal components of G(a) to construct a subspace
basis B ∈ Rk×p , which is referred to as the anchor subspace. All these matrices are publishable
because D(a) is non-sensitive. We expect that the anchor subspace B can cover most energy of
private gradients when the auxiliary data are not far from private data and m, k are reasonably large.
Suppose that the private gradients are G ∈ Rn×p . Then, we project the private gradients into the
anchor subspace B . The projection produces low-dimensional embeddings W = GBT and residual
gradients R = G - GBT B. The magnitude of residual gradients is usually much smaller than
original gradient even when k is small because of the gradient redundancy.
Then, we aggregate the gradient embeddings and the residual gradients, respectively. We perturb
the aggregated embedding and the aggregated residual gradient respectively to guarantee certain
differential privacy. Finally, we release the perturbed embedding and the perturbed residual gradient
and construct an unbiased estimator of the private gradient: V := (WTB + r)/n. This construction
process does not resulting in additional privacy loss because of DP’s post-processing property. The
privacy analysis of the whole process of GEP is given in Theorem 3.1.
Theorem 3.1. Let Si and S2 be the SenSitiVity of W and r, respectively, the output ofAlgorithm 1
satisfies (e, δ) -DPforany δ ∈ (0,1) and E ≤ 2log(1∕δ) if we choose σι ≥ 2Si √21og(1∕δ)∕e and
σ ≥ 2S2 P21og(1∕δ)∕e.
4
Published as a conference paper at ICLR 2021
Algorithm 1: Gradient embedding perturbation
1:	Input: anchor gradients G(a) ∈ Rm×p; number of basis vectors k; private gradients G ∈ Rn×p;
clipping thresholds S1, S2; standard deviations σ1, σ2; number of power iterations t.
2:	//First stage: Compute an orthonormal basis for the anchor subspace.
3:	Initialize B ∈ Rk×p randomly.
4:	for i = 1 to t do
5:	Compute A = G(a)BT andB = AT G(a).
6:	Orthogonalize B and normalize row vectors.
7:	end for
8:	Delete G(a) to free memory.
9:	//Second stage: project the private gradients G into anchor subspace B
10:	Compute gradient embeddings W = GBT and Clip 心 rows With Si to obtain W.
11:	Compute residual gradients R = G - WB and CliP its rows with S? to obtain R.
12:	//Third stage: perturb gradient embedding and residual gradient separately
13:	Perturb embedding with noise Z(I)〜N(0,σ2Ik×k): W := Pi Wi,：, W := W + z(1).
14:	Perturb residual gradient with noise z(2)〜N(0, σ2Ip×p): r := Pi Ri,：, r := r + z(2).
15:	Return V := (WTB + r)/n.
A Common praCtiCe to Control sensitivity is to Clip the output with a pre-defined threshold. In our
experiments, we use different thresholds S1 and S2 to Clip the gradient embeddings and residual
gradients, respeCtively. The privaCy loss of GEP Consists of two parts: the privaCy loss inCurred by
releasing the perturbed embedding and the privaCy loss inCurred by releasing the perturbed residual
gradient. We compose these two parts via the Renyi differential privacy and convert it to (e, δ)-DP.
We highlight several implementation teChniques that make GEP widely appliCable and implementable
with reasonable computational cost. Firstly, auxiliary non-sensitive data do not have to be the same
source as the private data and the auxiliary data can be randomly labeled. This non-sensitive data
assumption is very weak and easy to satisfy in practical scenarios. To understand why random label
works, a quick example is that for the least squares regression problem the individual gradient is
aligned with the feature vector while the label only scales the length but does not change the direction.
This auxiliary data assumption avoids conducting principal component analysis (PCA) on private
gradients, which requires releasing private high-dimensional basis vectors and hence introduces large
privacy loss. Secondly, we use power method (Panju, 2011; Vogels et al., 2019) to approximately
estimate the principal components. The new operation we introduce is standard matrix multiplication
that enjoys efficient implementation on GPU. The computational complexity of each power iteration
is 2mkp, where p is the number of model parameters, m is the number of anchor gradients and k is
the number of subspace basis vectors. Thirdly, we divide the parameters into different groups and
compute one orthonormal basis for each group. This further reduces the computational cost. For
example, suppose the parameters are divided into two groups with size p1, p2 and the numbers of
basis vectors are k1, k2, the computational complexity of each power iteration is 2m(k1p1 + k2p2),
which is smaller than 2m(k1 +k2)(p1 +p2). In Appendix B, we analyze the additional computational
and memory costs of GEP compared to standard gradient perturbation.
Curious readers may wonder if we can use random projection to reduce the dimensionality as
Johnson-Lindenstrauss Lemma (Dasgupta & Gupta, 2003) guarantees that one can preserve the
pairwise distance between any two points after projecting into a random subspace of much lower
dimension. However, preserving the pairwise distance is not sufficient for high quality gradient
reconstruction, which is verified by the empirical observation in Appendix C.
3.2	An analysis on the residual gradients of GEP
Let g := * Pi Gi,： be the target private gradient. For a given anchor subspace B, the residual
gradients are defined as R := G - GBT B . We then analyze how large the residual gradients could
be. The following argument holds for all time steps and we ignore the time step index for simplicity.
5
Published as a conference paper at ICLR 2021
For the ease of discussion, we introduce ξi := (Gi,:)T for i ∈ [n] to denote the the private
gradients and the ξ := (Gja))T for j ∈ [m] to denote the anchor gradients. We use λk(∙) to
denote the kth largest eigenvalue of a given matrix. We assume that the private gradients ξ1, ..., ξn
and the anchor gradients ξ1, ..., ξm are sampled independently from a distribution P. We assume
Σ := Eξ〜Pξξτ ∈ Rp×p to be the population gradient (uncentered) covariance matrix. We also
consider the (uncentered) empirical gradient covariance matrix S =* Pm=I ξiξT.
One case is that the population gradient covariance matrix Σ is low-rank k . In this case we can argue
that the residual gradients are 0 once the number of anchor gradients m > k.
Lemma 3.1. Assume that the population covariance matrix Σ is with rank k and the distribution P
satisfies P(ξ ∈ Fs) = 0forall S-flats Fs in Rp with 0 ≤ s < k. Let Σ = VkKVk and S = Vk，AVk
be the eigendecompositions of Σ and the empirical covariance matrix S, respectively, such that
λk0 (S) > 0 and λk0+1(S) = 0. Then if m ≥ k, we have with probability 1,
k0 = k and kVkVT - VkVT∣∣2 = 0.	(2)
Proof. The proof is based on the non-singularity of covariance matrix. See Appendix D. □
We note that s-flat is the translate Fs = x + Fs(0) of an s-dimensional linear subspace Fs(0) in Rp and
the normal distribution satisfies such condition (Eaton & Perlman, 1973; Muirhead, 2009). Therefore,
we have seen that for low-rank case of population covariance matrix, the residual gradients are 0 once
m > k. In the general case, we measure the expected norm of the residual gradients.
Lemma 3.2. Assume that ξ 〜P and ∣∣ξk2 < T almost surely. Let Σ = VKVT be the eigendecom-
position of the population covariance matrix Σ. Let S = Vk KVkT be the eigendecomposition of the
empirical covariance matrix S. Then we have with probability 1 - 2 exp(-δ),
E∣ξ - ∏vk(ξ)∣2 ≤ X λko(∑) + pkc/m+τp2δ∕m,	(3)
k0>k
WhereC = [Ekξk4 - Pi *(£)] + [m1 Pm=I llξj- k4 - Pi λ2(S)], πvk is aprojection OPeratOr
onto the subspace Vk and the E is taken over the randomness of ξ 〜P.
Proof. The proof is an adaptation of Theorem 3.1 in Blanchard et al. (2007).	□
From Lemma 3.2, we can see the larger the number of anchor gradients and the dimension of the
anchor subspace k, the smaller the residual gradients. We can choose m, k properly such that the
upper bound on the expected residual gradient norm is small. This indicates that we may use a smaller
clipping threshold and consequently apply smaller noises with achieving the same privacy guarantee.
We next empirically examine the projection error r = Pi Ri,: by training a 20-layer ResNet on
CIFAR10 dataset. We try two different types of auxiliary data to compute the anchor gradients: 1)
samples from the same source as private data with correct labels, i.e., 2000 random samples from
the test data; 2) samples from different source with random labels, i.e., 2000 random samples from
ImageNet. The relation between the dimension of anchor subspace k and the projection error rate
(∣l nr Il / kg∣) is presented in Figure 4. We can see that the project error is small and decreases with
k, and the benefit of increasing k diminishes when k is large, which is implied by Lemma 3.2. In
practice one can only use small or moderate k because of the memory constraint. GEP needs to store
at least k individual gradients and each individual gradient consumes the same amount of memory as
the model itself. Moreover, we can see that the projection into anchor subspace of random labeled
auxiliary data yields comparable projection error, corroborating our argument that unlabeled auxiliary
data are sufficient for finding the anchor subspace.
We also verify that the redundancy of residual gradients is small, by plotting the stable rank of residual
gradient matrix in Figure 5. The stable rank of residual gradient matrix is an order of magnitude
higher than the stable rank of original gradient matrix. This implies that it could be hard to further
approximate R with low-dimensional embeddings.
We next compare the GEP with a scheme that simply discards the residual gradients and only outputs
the perturbed gradient embedding, i.e., the output is U := WTB∕n.
6
Published as a conference paper at ICLR 2021
Figure 4: Relative projection error (∣∣ nr ∣∣ / ∣∣gk) of the second
stage in ResNet20. The number of anchor gradients is 2000. The
dimension of anchor subspace is k. The learning rate is decayed
by 10 at epoch 30. The left plot uses random samples from
ImageNet. The right plot uses random samples from test data.
The benefit of increasing k becomes smaller when k is larger.
Figure 5: Stable rank of the
residual gradient matrix versus
original gradient matrix. The
gradients are computed on full
batch data for the first stage in
ResNet20. The dimension of an-
chor subspace is k = 1000.
Remark 1. Let U := WTB/n be the reconstructed gradientfrom noisy gradient embedding and V
be the output of GEP. If ignoring the effect of gradient clipping, we have
E[U] = g — r/n,	E[v] = g.	(4)
where r = Ei Ri,： is the aggregated residual gradients, W, B are given in Algorithm 1 and the
expectation is over the added random noises.
This indicates that U contains a systematic error that makes U always deviate from g by the residual
gradient. This systematic error is the projection error, which is plotted in Figure 4. The systematic
error cannot be mitigated by reducing the noise magnitude (e.g., increasing the privacy budget or
collecting more private data). We refer to the algorithm releasing U directly as Biased-GEP or B-GEP
for short, which can be viewed as an efficient implementation of the algorithm in (Zhou et al., 2020).
In our experiments, B-GEP can outperform standard gradient perturbation when k is large but is
inferior to GEP. We note that the above remark is made with ignoring the clipping effect (or set a
large clipping threshold). In practice, we do apply clipping for the individual gradients at each time
step, which makes the expectations in Remark 1 obscure (Chen et al., 2020b). We note that the claim
that v is an unbiased estimator of g is not that precise when applying gradient clipping.
3.3 Private learning with gradient embedding perturbation
GEP (Algorithm 1) describes how to release one-step gradient with privacy guarantee. In this section,
we compose the privacy losses at each step to establish the privacy guarantee for the whole learning
process. The differentially private learning process with GEP is given in Algorithm 2 and the privacy
analysis is presented in Theorem 3.2.
Algorithm 2: Differentially private gradient descent with GEP.
1:	Input: private dataset D; auxiliary dataset D(a); number of updates T; learning rate η;
configuration of GEP C; loss function `;
2:	Output: Differentially private model θT .
3:	for t = 0 to T — 1 do
4:	Compute the private gradients Gt and anchor gradients Gt(a) of loss with respect to θt.
5:	Call GEP with Gt, G(a) and configuration C to get Vt.
6:	Update model θt+ι = θt — ηVt.
7:	end for
Theorem 3.2.	For any e < 2log(1∕δ) and δ ∈ (0,1), the output ofAlgorithm 2 satisfies (e, δ)-DP if
we set σ ≥ 2,2T log(1∕δ)∕e.
7
Published as a conference paper at ICLR 2021
If the private gradients are randomly sampled from the full batch gradients, the privacy guarantee
can be strengthened via the privacy amplification by subsampling theorem of DP (Balle et al., 2018;
Wang et al., 2019; Zhu & Wang, 2019; Mironov et al., 2019). Theorem 3.3 gives the expected excess
error of Algorithm 2. Expected excess error measures the distance between the algorithm’s output
and the optimal solution in expectation.
Theorem 3.3.	Suppose the loss L(θ) = 1 P(x y)∈D '(fθ(x), y) is I-Lipschitz, convex, and β-
smooth. If η = 1, T = n√β⅛, and θ = T PT=I θt ,then We have E[L ⑹]一 L(θ*) ≤
O (√klog(1© + r√plog(I/δ)), where 尸= T PT-1 rt andrt = maxi k(Rt)i,：k is the sensitivity
of residual gradient at step t.
The r term represents the average projection error over the training process. The previous best
expected excess error for gradient perturbation is O(ʌ/plog(1∕δ)/(ne)) (Wang et al., 2017). As
shown in Lemma 3.1, if the gradients locate in a k-dimensional subspace over the training process,
尸=0 and the excess error is O(，k log(1∕δ)/(n)), independent of the problem ambient dimension
p. When the gradients are in general position, i.e., gradient matrix is not exact low-rank, Lemma 3.2
and the empirical result give a hint on how small the residual gradients could be. However, it is hard to
get a good bound on maxi k(Rt)i,: k and the bound in Theorem 3.3 does not explicitly improve over
previous result. One possible solution is to use a clipping threshold based on the expected residual
gradient norm. Then the output gradient becomes biased because of clipping and the utility/privacy
guarantees in Theorem 3.3/3.2 require new elaborate derivation. We leave this for future work.
4	Experiments
We conduct experiments on MNIST, extended SVHN, and CIFAR-10 datasets. Our implementation
is publicly available2. The model for MNIST has two convolutional layers with max-pooling and
one fully connected layer. The model for SVHN and CIFAR-10 is ResNet20 in He et al. (2016). We
replace all batch normalization (Ioffe & Szegedy, 2015) layers with group normalization (Wu & He,
2018) layers because batch normalization mixes the representations of different samples and makes
the privacy loss cannot be analyzed accurately. The non-private accuracy for MNIST, SVHN, and
CIFAR-10 is 99.1%, 95.9%, and 90.4%, respectively.
We also provide experiments with pre-trained models in Appendix A. Tramer & Boneh (2020) show
that differentially private linear classifier can achieve high accuracy using the features produced by
pre-trained models. We examine whether GEP can improve the performance of such private linear
classifiers. Notably, using the features produced by a model pre-trained on unlabeled ImageNet, GEP
achieves 94.8% validation accuracy on CIFAR10 with = 2.
Evaluated algorithms We use the algorithm in Abadi et al. (2016) as benchmark gradient perturba-
tion approach, referred to as “GP”. We also compare GEP with PATE (Papernot et al., 2017). We
run the experiments for PATE using the official implementation. The privacy parameter of PATE
is data-dependent and hence cannot be released directly (see Section 3.3 in Papernot et al. (2017)).
Nonetheless, we report the results of PATE anyway.
Implementation details At each step, GEP needs to release two vectors: the noisy gradient em-
bedding and the noisy residual gradient. The gradient embeddings have a sensitivity of S1 and
the residual gradients have a sensitivity of S2 because of the clipping. The output of GEP can
be constructed as follows: (1) normalize the gradient embeddings and residual gradients by 1/S1
and 1/S2, respectively, (2) concatenate the rescaled vectors, (3) release the concatenated vector via
gaussian mechanism with sensitivity √2, (4) rescale the two components by Si and S2. B-GEP only
needs to release the normalized noisy gradient embedding. We use the numerical tool in Mironov
et al. (2019) to compute the privacy loss. For given privacy budget and sampling probability, σ is set
to be the smallest value such that the privacy budget is allowable to run desired epochs.
All experiments are run on a single Tesla V100 GPU with 16G memory. For ResNet20, the parameters
are divided into five groups: input layer, output layer, and three intermediate stages. For a given
quota of basis vectors, we allocate it to each group according to the square root of the number of
parameters in each group. We compute an orthonormal subspace basis on each group separately.
2https://github.com/dayu11/Gradient-Embedding-Perturbation
8
Published as a conference paper at ICLR 2021
Table 1: Test accuracy (in %) with varying choices of privacy bound . The numbers under symbol ∆
denote the improvement over GP baseline.
Dataset	Algorithm	=2	^∆	e = 5	∆	€ = 8	^∆
GP	94.7	+0.0	96.8	+0.0	97.2	^00
		PATE MNIST	上		98.5	+3.8	^985-	+1.7	98.6	^4
MNIST	B-GEP	93.1	-Γ6-	94.5	-2.3	95.9	-Γ3-
-GEP	96.3	+1.6	97.9	+1.1	98.4	+1.2
GP	87.1	+0.0	^9T3-	+0.0	91.6	^00
一	PATE SVHN			80.7	--6.4-	91.6	+0.3	91.6	^00
SVHN	B-GEP	88.5	+1.4	91.8	+0.5	^9Σ3-	^07
-GEP	92.3	+5.2	94.7	+3.4	95.1	+3.5
GP	43.6	+0.0	52.2	+0.0	56.4	^00
CIFAR-10 PATE	34.2	--9.4-	41.9	-10.3	43.6	-12.8
B-GEP	50.3	+6.7	59.5	+7.3	63.0	+6.6
	 GEP	59.7	+16.1	70.1	+17.9	74.9	+18.5
Figure 6:	Test accuracy when varying the dimension of anchor subspace. GEP significantly outper-
forms B-GEP for all k. Moreover, the performance of GEP is not that sensitive to k .
Then we concatenate the projections of all groups to construct gradient embeddings. The number
of power iterations t is set as 1 as empirical evaluations suggest more iterations do not improve the
performance for GEP and B-GEP.
For all datasets, the anchor gradients are computed on 2000 random samples from ImageNet. In
Appendix C, we examine the influence of choosing different numbers of anchor gradients and different
sources of auxiliary data. The selected images are downsampled into size of 32 × 32 (28 × 28 for
MNIST) and we label them randomly at each update. For SVHN and CIFAR-10, k is chosen from
[500, 1000, 1500, 2000]. For MNIST, we halve the size of k. We use SGD with momentum 0.9 as
the optimizer. Initial learning rate and batchsize are 0.1 and 1000, respectively. The learning rate is
divided by 10 at middle of training. Weight decay is set as 1 × 10-4. The clipping threshold for is
10 for original gradients and 2 for residual gradients. The number of training epochs for CIFAR-10
and MNIST is 50, 100, 200 for privacy parameter = 2, 5, 8, respectively. The number of training
epochs for SVHN is 5, 10, 20 for privacy parameter = 2, 5, 8, respectively. Privacy parameter δ is
1 × 10-6 for SVHN and 1 × 10-5 for CIFAR-10 and MNIST.
Results The best accuracy with given is in Table 4. For all datasets, GEP achieves considerable
improvement over GP in Abadi et al. (2016). Specifically, GEP achieves 74.9% test accuracy on
CIFAR-10 with (8, 10-5)-DP, outperforming GP by 18.5%. PATE achieves best accuracy on MNIST
but its performance drops as the dataset becomes more complex.
We also plot the relation between accuracy and k in Figure 6. GEP is less sensitive to the choice of k
and outperforms B-GEP for all choices of k. The improvement of increasing k becomes smaller as k
becomes larger. We note that the memory cost of choosing large k is high because we need to store at
least k individual gradients to compute anchor subspace.
5	Conclusion
In this paper, we propose Gradient Embedding Perturbation (GEP) for learning with differential
privacy. GEP leverages the gradient redundancy to reduce the added noise and outputs an unbiased
estimator of target gradient. The several key designs of GEP significantly boost the applicability of
GEP. Extensive experiments on real world datasets demonstrate the superior utility of GEP.
9
Published as a conference paper at ICLR 2021
References
Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with differential privacy. In ACM SIGSAC Conference on Computer and
Communications Security, 2016.
Noga Alon, Raef Bassily, and Shay Moran. Limits of private learning with access to public data. In
Advances in Neural Information Processing Systems, 2019.
Borja Balle, Gilles Barthe, and Marco Gaboardi. Privacy amplification by subsampling: Tight
analyses via couplings and divergences. In Advances in Neural Information Processing Systems,
2018.
Raef Bassily, Adam Smith, and Abhradeep Thakurta. Differentially private empirical risk minimiza-
tion: Efficient algorithms and tight error bounds. Annual Symposium on Foundations of Computer
Science, 2014.
Daniel Bernau, Philip-William Grassal, Jonas Robl, and Florian Kerschbaum. Assessing differentially
private deep learning with membership inference. arXiv preprint arXiv:1912.11328, 2019.
Gilles Blanchard, Olivier Bousquet, and Laurent Zwald. Statistical properties of kernel principal
component analysis. Machine Learning, 66(2-3):259-294, 2007.
Mark Bun and Thomas Steinke. Concentrated differential privacy: Simplifications, extensions, and
lower bounds. In Theory of Cryptography Conference, 2016.
Nicholas Carlini, Chang Liu, Ulfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer: Evalu-
ating and testing unintended memorization in neural networks. In USENIX Security Symposium,
2019.
Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big
self-supervised models are strong semi-supervised learners. arXiv preprint arXiv:2006.10029,
2020a.
Xiangyi Chen, Steven Z Wu, and Mingyi Hong. Understanding gradient clipping in private sgd: A
geometric perspective. Advances in Neural Information Processing Systems, 33, 2020b.
Sanjoy Dasgupta and Anupam Gupta. An elementary proof of a theorem of johnson and lindenstrauss.
Random Structures & Algorithms, 2003.
Jinshuo Dong, Aaron Roth, and Weijie J Su. Gaussian differential privacy. arXiv preprint
arXiv:1905.02383, 2019.
Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data,
ourselves: Privacy via distributed noise generation. In Annual International Conference on the
Theory and Applications of Cryptographic Techniques, 2006a.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in
private data analysis. In Theory of cryptography conference, 2006b.
Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Foundations
and Trends® in Theoretical Computer Science, 2014.
Morris L Eaton and Michael D Perlman. The non-singularity of generalized sample covariance
matrices. The Annals of Statistics, pp. 710-717, 1973.
Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confi-
dence information and basic countermeasures. In ACM SIGSAC Conference on Computer and
Communications Security, 2015.
Kazuto Fukuchi, Quang Khai Tran, and Jun Sakuma. Differentially private empirical risk minimiza-
tion with input perturbation. In International Conference on Discovery Science, 2017.
10
Published as a conference paper at ICLR 2021
Mary Gooneratne, Khe Chai Sim, Petr Zadrazil, Andreas KabeL Frangoise Beaufays, and Giovanni
Motta. Low-rank gradient approximation for memory-efficient on-device training of deep neural
network. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).
IEEE, 2020.
Guy Gur-Ari, Daniel A Roberts, and Ethan Dyer. Gradient descent happens in a tiny subspace. arXiv
preprint arXiv:1812.04754, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
2016.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Conference on Computer Vision and Pattern
Recognition, 2020.
Briland Hitaj, Giuseppe Ateniese, and Fernando Perez-Cruz. Deep models under the gan: information
leakage from collaborative deep learning. In Proceedings of the 2017 ACM SIGSAC Conference
on Computer and Communications Security, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning, 2015.
Roger Iyengar, Joseph P Near, Dawn Song, Om Thakkar, Abhradeep Thakurta, and Lun Wang.
Towards practical differentially private convex optimization. In IEEE Symposium on Security and
Privacy, 2019.
Bargav Jayaraman, Lingxiao Wang, David Evans, and Quanquan Gu. Distributed learning without
distress: Privacy-preserving empirical risk minimization. In Advances in Neural Information
Processing Systems, 2018.
James Jordon, Jinsung Yoon, and Mihaela van der Schaar. Pate-gan: Generating synthetic data with
differential privacy guarantees. In International Conference on Learning Representations, 2019.
Peter Kairouz, M6nica Ribero, Keith Rush, and Abhradeep Thakurta. Dimension independence in
unconstrained private erm via adaptive preconditioning. arXiv preprint arXiv:2008.06570, 2020.
Daniel Kifer, Adam Smith, and Abhradeep Thakurta. Private convex empirical risk minimization and
high-dimensional regression. In Conference on Learning Theory, 2012.
Jaewoo Lee and Daniel Kifer. Concentrated differentially private gradient descent with adaptive
per-iteration privacy budget. In Proceedings of the 24th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining, 2018.
Xinyan Li, Qilong Gu, Yingxue Zhou, Tiancong Chen, and Arindam Banerjee. Hessian based analysis
of sgd for deep nets: Dynamics and generalization. In SIAM International Conference on Data
Mining, 2020.
Yuzhe Ma, Xiaojin Zhu, and Justin Hsu. Data poisoning against differentially-private learners: attacks
and defenses. In Proceedings of the 28th International Joint Conference on Artificial Intelligence,
pp. 4732—4738. AAAI Press, 2019.
Ilya Mironov. Renyi differential privacy. In IEEE Computer Security Foundations Symposium, 2017.
Ilya Mironov, Kunal Talwar, and Li Zhang. Renyi differential privacy of the sampled gaussian
mechanism. arXiv, 2019.
Robb J Muirhead. Aspects of multivariate statistical theory, volume 197. John Wiley & Sons, 2009.
Maysum Panju. Iterative methods for computing eigenvalues and eigenvectors. arXiv preprint
arXiv:1105.1185, 2011.
Nicolas Papernot, Mart^n Abadi, Ulfar Erlingsson, Ian Goodfellow, and Kunal Talwar. Semi-
supervised knowledge transfer for deep learning from private training data. In International
Conference on Learning Representations, 2017.
11
Published as a conference paper at ICLR 2021
Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, and Ulfar Erlings-
son. Scalable private learning with pate. In International Conference on Learning Representations,
2018.
NhatHai Phan, My T Thai, Han Hu, Ruoming Jin, Tong Sun, and Dejing Dou. Scalable differential
privacy with certified robustness in adversarial learning. International Conference on Machine
Learning, 2020.
Md Atiqur Rahman, Tanzila Rahman, Robert Laganiere, Noman Mohammed, and Yang Wang.
Membership inference attack against differentially private deep learning model. Transactions on
Data Privacy, 2018.
Alexandre Sablayrolles, Matthijs Douze, Yann Ollivier, Cordelia Schmid, and HerVe J6gou. White-
box vs black-box: Bayes optimal strategies for membership inference. International Conference
on Machine Learning, 2019.
Reza Shokri and Vitaly Shmatikov. Privacy-preserving deep learning. In ACM SIGSAC conference
on computer and communications security, 2015.
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks
against machine learning models. In IEEE Symposium on Security and Privacy (SP), 2017.
Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with differen-
tially private updates. In Global Conference on Signal and Information Processing (GlobalSIP),
2013.
Kunal Talwar, Abhradeep Guha Thakurta, and Li Zhang. Nearly optimal private lasso. In Advances
in Neural Information Processing Systems, 2015.
Abhradeep Guha Thakurta and Adam Smith. Differentially private feature selection via stability
arguments, and the robustness of the lasso. In Conference on Learning Theory, 2013.
Florian Tramer and Dan Boneh. Differentially private learning needs better features (or much more
data). arXiv preprint arXiv:2011.11660, 2020.
Joel A Tropp et al. An introduction to matrix concentration inequalities. Foundations and Trends® in
Machine Learning, 8(1-2):1-230, 2015.
Thijs Vogels, Sai Praneeth Karimireddy, and Martin Jaggi. Powersgd: Practical low-rank gradient
compression for distributed optimization. In Advances in Neural Information Processing Systems,
2019.
Di Wang and Jinhui Xu. On sparse linear regression in the local differential privacy model. In
International Conference on Machine Learning, 2019.
Di Wang, Minwei Ye, and Jinhui Xu. Differentially private empirical risk minimization revisited:
Faster and more general. In Advances in Neural Information Processing Systems, 2017.
Jun Wang and Zhi-Hua Zhou. Differentially private learning with small public data. In AAAI, 2020.
Lingxiao Wang and Quanquan Gu. Differentially private iterative gradient hard thresholding for
sparse learning. In International Joint Conference on Artificial Intelligence, 2019.
Yu-Xiang Wang, Borja Balle, and Shiva Prasad Kasiviswanathan. Subsampled renyi differential
privacy and analytical moments accountant. In International Conference on Artificial Intelligence
and Statistics, 2019.
Xi Wu, Matthew Fredrikson, Somesh Jha, and Jeffrey F Naughton. A methodology for formalizing
model-inversion attacks. In IEEE Computer Security Foundations Symposium, 2016.
Xi Wu, Fengan Li, Arun Kumar, Kamalika Chaudhuri, Somesh Jha, and Jeffrey Naughton. Bolt-on
differential privacy for scalable stochastic gradient descent-based analytics. In ACM International
Conference on Management of Data, 2017.
12
Published as a conference paper at ICLR 2021
Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on
computer vision (ECCV), 2018.
Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu. Gradient perturbation is underrated for
differentially private convex optimization. In Proc. of 29th Int. Joint Conf. Artificial Intelligence,
2020.
Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu. How does data augmentation affect
privacy in machine learning? In Proc. of the AAAI Conference on Artificial Intelligence, 2021.
Yingxue Zhou, Zhiwei Steven Wu, and Arindam Banerjee. Bypassing the ambient dimension: Private
sgd with gradient subspace identification. arXiv preprint arXiv:2007.03813, 2020.
Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. In Advances in Neural
Information Processing Systems, 2019.
YUqing ZhU and YU-Xiang Wang. Poission subsampled r6nyi differential privacy. In International
Conference on Machine Learning, 2019.
13
Published as a conference paper at ICLR 2021
A Experiments with pre-trained models
Recent works have shown that pre-training the models on unlabeled data can be beneficial for
subsequent learning tasks (Chen et al., 2020a; He et al., 2020). Tramer & Boneh (2020) demonstrate
that differentially private linear classifier can achieve high accuracy using the features produced by
those per-trained models. We show that GEP can also benefit from such pre-trained models.
Inspired by Tramer & Boneh (2020), we use the output of the penultimate layer of a pre-trained
ResNet152 model as feature to train a private linear classifier. The ResNet152 model is pre-trained
on unlabeled ImageNet using SimCLR (Chen et al., 2020a). The feature dimension is 4096.
Implementation Details We choose the privacy parameter from [0.1, 0.5, 1, 2]. The privacy param-
eter δ is 1 × 10-5. We run all experiments for 5 times and report the average accuracy. The clipping
threshold of residual gradients is still one-fifth of the clipping threshold of the original gradients. The
dimension of anchor subspace is set as 200 ` √p where p = 40960 is the model dimension. We
randomly sample 500 samples from the test set as auxiliary data and evaluate performance on the rest
test samples. The optimizer is Adam with default momentum coefficients. Other hyper-parameters
are listed in Table 2.
Hyperparameter	Values
Learning rate	0.01,0.05, 0.1
Running steps	50,100,400
Clipping threshold	0.01,0.1,1—
Table 2: Hyperparameter values used in Appendix A.
Results The experiment results are shown in Table 3. GEP outperforms GP on all values of . With
privacy bound = 2, GEP achieves 94.8% validation accuracy on CIFAR10 dataset, improving over
the GP baseline by 1.4%. For very strong privacy guarantee ( = 0.1), B-GEP performs on par with
GEP because strong privacy guarantee requires large noise and the useful signal in residual gradient
is submerged in the added noise. B-GEP benefits less from larger compared to GP or GEP. For
= 1 and 2, the performance of B-GEP is worse than the performance of GP. This is because larger
can not reduce the systematic error of B-GEP (see Remark 1 in Section 3.2).
Table 3: Validation accuracy (in %) on CIFAR10 with varying choices of . We train a private linear
model on top of the features from a ReSNet152 model, which is pre-trained on unlabeled ImageNet.
	=0.1	E = 0.5	E=1	E=2
Non private	96.3	963	96.3	96.3
GP	88.2 (±0.16)	91.1 (±0.17)	93.2 (±0.19)	93.4 (±0.12)
B-GEP	91.0 (±0.07)	92.9 (±0.03)	93.1 (±0.10)	93.2 (±0.08)
GEP	90.9 (±0.19)	93.5 (±0.06)	94.3 (±0.09)	94.8 (±0.06)
B Complexity Analysis
We provide an analysis of the computational and memory costs of the construction of anchor
subspace. The computation of the anchor subspace is the dominant additional cost of GEP compared
to conventional gradient perturbation. Notations: k, m, n, andp are the dimension of anchor subspace,
number of anchor gradients, number of private gradients, and the model dimension, respectively. In
order to reduce the computational and memory costs, we divide the parameters into g groups and
compute one orthonormal basis for each group. We refer to this approach as ‘parameter grouping’. In
this section, we assume the parameters and the dimension of the anchor subspace are both divided
evenly. Table 4 summarizes the additional costs of GEP with/without parameter grouping. Using
parameter grouping can reduce the computational/memory cost significantly.
14
Published as a conference paper at ICLR 2021
Table 4: Computational and memory costs of a single power iteration in Algorithm 1. The computation
cost is measured by the number of floating point operations. The memory cost is measured by the
number of floating-point numbers we need to store. ‘GEP+PG’ denotes GEP with parameter grouping
and g denotes the number of groups. Notations: k, m, n, and p are the dimension of anchor subspace,
number of anchor gradients, number of private gradients, and the model dimension, respectively.
	Computational Cost	Memory Cost
GEP	2mkp + pk2	max (0, (m - n + k) p + mk)
GEP+PG	2mkp/g + pk2 /g2	max(0, (m — n + k∖ P + mk)
C Ablation Study
The influence of choosing different auxiliary datasets. We conduct experiments with different
choices of auxiliary datasets. For CIFAR10, we try 2000 random test samples from CIFAR10, 2000
random samples from CIFAR100, and 2000 random samples from ImageNet. When the auxiliary
dataset is CIFAR10, we try both correct labels and random labels. For all choices of auxiliary
datasets, the test accuracy is evaluated on 8000 test samples of CIFAR10 that are not used as auxiliary
data. Other implementation details are the same as in Section 4. The results are shown in Table 5.
Surprisingly, using samples from CIFAR10 with correct labels yields the worst accuracy. This may
because the model ‘overfits’ the auxiliary data when it has access to correct labels, which makes the
anchor subspace contains less information about the private gradients. The best accuracy is achieved
using samples from CIFAR10 with random labels, this makes sense because in this case the features
of auxiliary data and private data have the same distribution. Using samples from CIFAR100 or
ImageNet as auxiliary data has a small influence on the test accuracy.
Table 5: Test accuracy on CIFAR10 with different choices of auxiliary datasets. The privacy guarantee
is (8,10-5)-DP. We report the average accuracy of five runs with standard deviations in brackets.
Auxiliary Data	Random Label?	Test Accuracy
CIFAR10	No	72.9 (±0.31)
CIFAR10	Yes	75.1 (±0.42)
CIFAR100	Yes	74.7 (±0.46)
ImageNet	Yes	74.8 (±0.39)
The influence of the number of anchor gradients. In the main text, the size of auxiliary dataset is
m = 2000. We conduct more experiments with different sizes of auxiliary dataset to examine the
influence of m. The auxiliary data is randomly sampled from ImageNet. Table 6 reports the test
accuracy on CIFAR10 with different choices of m. For both B-GEP and GEP, increasing m leads to
slightly improved performance.
Table 6: Test accuracy on CIFAR10 with different sizes of auxiliary dataset. The privacy guarantee is
(8, 10-5)-DP. We report the average accuracy of five runs with standard deviations in brackets.
Algorithm	m = 1000	~^m = 2000	m = 4000
B-GEP	62.2 (±0.26)	^^62.6 (±0.24)^^	63.3 (±0.27)
GEP	74.6 (±0.41)	74.8 (±0.39)	75.2 (±0.34)
The projection error of random basis vectors. It is tempting to construct the anchor subspace using
random basis vectors because Johnson-Lindenstrauss Lemma (Dasgupta & Gupta, 2003) guarantees
that one can preserve the pairwise distance between any two points after projecting into a random
subspace of much lower dimension. We empirically verify the projection error of Gaussian random
basis vectors on CIFAR10 and SVHN. The experiment settings are the same as in Section 4. The
projection errors over the training process are plotted in Figure 7. The projection error of random
basis vectors is very high (> 95%) throughout training. This is because preserving the pairwise
15
Published as a conference paper at ICLR 2021
Figure 7:	Projection error rate of random basis vectors. The dimension of subspace is denoted by k .
distance is not sufficient for high quality gradient reconstruction, which requires one to preserve the
average ‘distance’ between any individual gradient and all other gradients.
D Missing Proofs
Lemma 3.1. Assume that the population covariance matrix Σ is with rank k and the distribution P
satisfies P(ξ ∈ Fs)= 0 forall S-flats Fs in Rp with 0 ≤ s < k. Let Σ = Vk AVkT and S = VkO Λ VkT
be the eigendecompositions of Σ and the empirical covariance matrix S, respectively, such that
λk0 (S) > 0 and λk0+1 (S) = 0. Then if m ≥ k, we have with probability 1,
k0 = k and ∣∣VkVk - VkVT∣∣2 = 0.	(2)
Proof. We extend the Theorem 3.2 in Eaton & Perlman (1973) to the low-rank case.
Theorem D.1 (Theorem 3.2 in Eaton & Perlman (1973)). Let X = (x1, ..., xn) where the xi are
i.i.d. random vectors in Rp, n ≥ p. If P{x1 ∈ M} = 0 for all proper manifolds M ⊂ Rp, then
P{X is non-singular}=1.
We note that the subspace spanned by Vk0 is in the space spanned by Vk by definition. Hence k0 ≤ k.
Let Xi := VFξi ∈ Rk for i ∈ [m]. Then X := (Xi ,…，Xm) is non-singular because of the assump-
tion and Theorem D.1. That is rank(X) = k. Therefore rank((ξ1, ..., ξm)) ≥ k, rank(S) ≥ k
and k0 ≥ k. Therefore k0 = k and the subspace spanned by Vk0 and the subspace spanned by Vk are
identical.	□
Theorem 3.1.	Let Si and S2 be the sensitivity of W and r, respectively, the output ofAlgorithm 1
satisfies (e, δ) -DPforany δ ∈ (0,1) and e ≤ 2log(1∕δ) if we choose σι ≥ 2Si，2 log(1∕δ)∕e and
σ2 ≥ 2S2 P2log(1∕δ)∕e.
ProofofTheorem 3.1. We first introduce some background knowledge of Renyi differential privacy
(RDP) (Mironov, 2017). RDP measures the Renyi divergence between two output distributions.
Definition 2 ((λ, γ)-RDP). A randomized mechanism f is said to guarantee (λ, γ)-RDP if for any
neighboring datasets D, D0 and λ > 1 it holds that
Dλ(f (D)||f(D0)) ≤ γ,
where Dλ(∙∣∣∙) denotes the Renyi divergence of order λ.
We next introduce some useful properties of RDP.
Lemma D.2 (Gaussian mechanism of RDP). Let S = max。〜d，∣∣f (D) - f (D0) k be the l2 sensitivity,
then Gaussian mechanism M = f (D) + Z satisfies (λ, 2S2)-RDP where z 〜N(0, σ2Ip×p).
Lemma D.3 (Composition of RDP). If Mi, M2 satisfy (λ, γi)-RDP and (λ, γ2)-RDP respectively,
then their composition satisfies (λ, γi + γ2 )-RDP.
16
Published as a conference paper at ICLR 2021
Lemma D.4 (Conversion from RDP to (, δ)-DP). If M obeys (λ, γ)-RDP, then M obeys (γ +
log(1∕δ)∕(λ — 1), δ)-DPforall 0 < δ < L
Now we proof Theorem 3.1. Let W, W0 be the gradient embeddings of two neighboring datasets
D 〜D0 and R, R be corresponding residual gradients. Without loss of generality, suppose W (R)
has one more row than W0 (R0). For given sensitivity S1, S2,
max kw — w0k = max kWn :k ≤ S1, max kr — r0k = max kRn :k ≤ S2.
D 〜D0	W 〜W0	,	D 〜D0	R 〜R0	,
If We set σι = Sισ and σ2 = S2σ for some σ, then Algorithm 1 satisfies (λ,福)-RDP because of
Lemma D.2 and D.3. In order to guarantee (, δ)-DP, we need
λ IlOg(I ∕δ) V
σ + t-r ≤ J
(5)
Choose λ = 1+ 2log(1∕δ) and rearrange Eq (5), we need
σ2 ≥
2(e + 2log(1∕δ))
2
(6)
Then using the constraint on concludes the proof.
□
Theorem 3.2.	For any < 2 lOg(1∕δ) and δ ∈ (0, 1), the output of Algorithm 2 satisfies (, δ)-DP if
we set σ ≥ 2p2T log(1∕δ)∕e.
Proofof Theorem 3.2. From the proof of Theorem 3.1, we have each call of GEP satisfies (λ,福)-
RDP. Then by the composition property of RDP (Lemma D.3), the output of Algorithm 2 satisfies
(λ, Tσλ )-RDP. Plugging Tσλ into Equation 5 and 6 concludes the proof.
□
Theorem 3.3.	Suppose the loss L(θ) = 1 P(x y)∈D '(fθ(x), y) is I-LipSchitz, convex, and β-
smooth. If η = β, T = n√P, and θ = T PT=I θt, then We have E[L⑹]—L(θ*) ≤
O (√klog(1∕δ) + r"lng(I/δ)), where 尸= T PT-1 rt andrt = maxi k(Rt)i,：k is the sensitivity
of residual gradient at step t.
Proof of Theorem 3.3. The β-smooth condition gives
L(θt+ι) ≤ L(θt) + "L(θt), θt+ι — θti + β kθt+ι — θtk2.	⑺
Based on the update rule of GEP we have
Θt+1 — θt = —ηv = —ηVL(θt) — η(zt1)B + z(2)),	(8)
n
where z(1) 〜 N(0, σ2Ik×k), z(2) 〜N(0,σ2r2Ip×p) are the perturbation noises and rt =
maxi k(Rt)i,: k is the sensitivity of residual gradients at step t.
Take expectation on Eq (7) with respect to the perturbation noises.
E[L(θt+ι)] ≤ E[L(θt)] - (η - en2/2)E[kVL(et)k2] + 空(k + Pr).	⑼
Subtract L(θ*) from both sides, we have
βη2σ2
E[L(θt+ι)] — L(θ*) ≤ E[L(θt)] — L(θ*) — (η — βη2∕2)E[kVL(θt)k2] + β2n2- (k + pr2)
≤ EKVL(θt), θt - θ*il - (n -的力闽敝⑸32]+ 冬
(k + pr2).
(10)
17
Published as a conference paper at ICLR 2021
The second inequality holds because L is convex. Then choose η = 1 and plug VL(θt)
(θt - θt+ι)∕η - (ztB + Zt)/n into Eq(10).
E[L(θt+ι)] - L(θ*) ≤ βE[hθt - θt+ι, θt - θ*i] - ^7E[kθt - θt+ιk2] + 万一2 (k + pr2)
2	βn2
=2 (E[kθt - θ*k2] - E[kθt+1 - θ*k2]) + βσ22 (k + Pr).
Sum over t = 0, . . . , T - 1 and use convexity, we have
e[l ⑹]- La) ≤ 而 ιιθo- θ*k + -x-2 (k+而 X rt)
2T	βn2	T
t=0
(11)
(12)
Then substituting T = √ and σ =O(PT log(1∕δ)∕c) yields the desired bound.
□
18