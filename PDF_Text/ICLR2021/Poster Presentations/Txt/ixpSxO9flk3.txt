Published as a conference paper at ICLR 2021
No MCMC for me: Amortized sampling for fast
and stable training of energy-based models
Will Grathwohl*
University of Toronto & Vector Institute
Google Research
wgrathwohl@cs.toronto.edu
Jacob Kelly*
University of Toronto & Vector Institute
jkelly@cs.toronto.edu
Milad Hashemi
Google Research
miladh@google.com
Mohammad Norouzi & Kevin Swersky
Google Research
{mnorouzi, kswersky}@google.com
David Duvenaud
University of Toronto & Vector Institute
duvenaud@cs.toronto.edu
Ab stract
Energy-Based Models (EBMs) present a flexible and appealing way to represent
uncertainty. Despite recent advances, training EBMs on high-dimensional data
remains a challenging problem as the state-of-the-art approaches are costly, unsta-
ble, and require considerable tuning and domain expertise to apply successfully.
In this work we present a simple method for training EBMs at scale which uses
an entropy-regularized generator to amortize the MCMC sampling typically used
in EBM training. We improve upon prior MCMC-based entropy regularization
methods with a fast variational approximation. We demonstrate the effectiveness
of our approach by using it to train tractable likelihood models. Next, we apply
our estimator to the recently proposed Joint Energy Model (JEM), where we match
the original performance with faster and stable training. This allows us to extend
JEM models to semi-supervised classification on tabular data from a variety of
continuous domains.
1 Introduction
Energy-Based Models (EBMs) have recently regained popularity within machine learning, partly
inspired by the impressive results of Du & Mordatch (2019) and Song & Ermon (2020) on large-
scale image generation. Beyond image generation, EBMs have also been successfully applied to
a wide variety of applications including: out-of-distribution detection (Grathwohl et al., 2019; Du
& Mordatch, 2019; Song & Ou, 2018), adversarial robustness (Grathwohl et al., 2019; Hill et al.,
2020; Du & Mordatch, 2019), reliable classification (Grathwohl et al., 2019; Liu & Abbeel, 2020)
and semi-supervised learning (Song & Ou, 2018; Zhao et al.). Strikingly, these EBM approaches
outperform alternative classes of generative models and rival hand-tailored solutions on each task.
Despite progress, training EBMs is still a challenging task. As shown in Table 1, existing train-
ing methods are all deficient in at least one important practical aspect. Markov chain Monte Carlo
(MCMC) methods are slow and unstable during training (Nijkamp et al., 2019a; Grathwohl et al.,
2020). Score matching mechanisms, which minimize alternative divergences are also unstable and
most methods cannot work with discontinuous nonlinearities (such as ReLU) (Song & Ermon,
2019b; Hyvarinen, 2005; Song et al., 2020; Pang et al., 2020b; GrathWohl et al., 2020; Vincent,
2011). Noise contrastive approaches, which learn energy functions through density ratio estimation,
typically don’t scale Well to high-dimensional data (Gao et al., 2020; Rhodes et al., 2020; Gutmann
& Hyvarinen, 2010; Ceylan & Gutmann, 2018).
* Equal Contribution. Code available at github.com/wgrathwohl/VERA
1
Published as a conference paper at ICLR 2021
Training Method	Fast Stable	High	No aux. Unrestricted Approximates training dimensions model architecture likelihood
Markov chain Monte Carlo Score Matching Approaches Noise Contrastive Approaches	XX	✓✓/	✓ ✓	X	✓	✓	X	X ✓✓	X	X	✓	X
VERA (ours)	✓	✓	JXJ	✓
Table 1: Features of EBM training approaches. Trade-offs must be made when training unnormal-
ized models and no approach to date satisfies all of these properties.
VERA	PCD	Ground Truth
Entropy Regularization	SGLD σ
Figure 1: Comparison of EBMs trained with VERA and PCD. We see that as entropy regularization
goes to 1, the density becomes more accurate. For PCD, all samplers produce high quality samples,
but low-quality density models as the distribution of MCMC samples may be arbitrarily far away
from the model density.
In this work, we present a simple method for training EBMs which performs as well as previous
methods while being faster and substantially easier to tune. Our method is based on reinterpreting
maximum likelihood as a bi-level variational optimization problem, which has been explored in
the past for EBM training (Dai et al., 2019). This perspective allows us to amortize away MCMC
sampling into a GAN-style generator which is encouraged to have high entropy. We accomplish this
with a novel approach to entropy regularization based on a fast variational approximation. This leads
to the method we call Variational Entropy Regularized Approximate maximum likelihood (VERA).
Concretely, we make the following contributions:
•	We improve the MCMC-based entropy regularizer of Dieng et al. (2019) with a paralleliz-
able variational approximation.
•	We show that an entropy-regularized generator can be used to produce a variational bound
on the EBM likelihood which can be optimized more easily than MCMC-based estimators.
•	We demonstrate that models trained in this way achieve much higher likelihoods than meth-
ods trained with alternative EBM training procedures.
•	We show that our approach stabilizes and accelerates the training of recently proposed Joint
Energy Models (Grathwohl et al., 2019).
•	We show that the stabilization of our approach allows us to use JEM for semi-supervised
learning, outperforming virtual adversarial training when little prior domain knowledge is
available (e.g., for tabular data).
2 Energy Based Models
An energy-based model (EBM) is any model which parameterizes a density as
efθ (x)
pθ (X) = Zθ)
(1)
where fθ : RD → R and Z(θ) = efθ(x)dx is the normalizing constant which is not explicitly
modeled. Any probability distribution can be represented in this way for some fθ. The energy-based
2
Published as a conference paper at ICLR 2021
parameterization has been used widely for its flexibility, ease of incorporating known structure, and
relationship to physical systems common in chemistry, biology, and physics (Ingraham et al., 2019;
Du et al., 2020; Noe et al., 2019).
The above properties make EBMs an appealing model class, but because they are unnormalized
many tasks which are simple for alternative model classes become challenging for EBMs. For
example, exact samples cannot be drawn and likelihoods cannot be exactly computed (or even lower-
bounded). This makes training EBMs challenging as we cannot simply train them to maximize
likelihood. The most popular approach to train EBMs is to approximate the gradient of the maximum
likelihood objective. This gradient can be written as:
Vθ logPθ(x) = Vθfθ(X)- Epθ(χ0)[Vθfθ(x0)].
(2)
MCMC techniques are used to approximately generate samples from pθ (x) (Tieleman, 2008). Prac-
tically, this approach suffers from poor stability and computational challenges from sequential sam-
pling. Many tricks have been developed to overcome these issues (Du & Mordatch, 2019), but they
largely still persist. Alternative estimators have been proposed to circumvent these challenges, in-
eluding score matching (Hyvarinen, 2005), noise contrastive estimation (Gutmann & Hyvarinen,
2010), and variants thereof. These suffer from their own challenges in scaling to high dimensional
data, and sacrifice the statistical efficiency of maximum likelihood.
In Figure 1 we visualize densities learned with our approach and Persistent Contrastive Diver-
gence (Tieleman, 2008) (PCD) training. As we see, the sample quality of the PCD models is quite
high but the learned density models do not match the true model. This is due to accrued bias in the
gradient estimator from approximate MCMC sampling (Grathwohl et al., 2020). Prior work (Ni-
jkamp et al., 2019b) has argued that this objective actually encourages the approximate MCMC
samples to match the data rather than the density model. Conversely, we see that our approach (with
proper entropy regularization) recovers a high quality model.
3 Variational Maximum Likelihood
We seek the energy function which maximizes likelihood given in Equation 1. We examine the
intractable component of the log-likelihood, the log partition-function log Z(θ) = log efθ (x) dx.
We can re-write this quantity as the optimum of
log Z(θ) = mqax Eq(x) [fθ(x)] + H(q)
(3)
where q is a distribution and H(q) = -Eq(x) [log q(x)] denotes its entropy 1 (see the Appendix A.1
for the derivation). Plugging this into our original maximum likelihood statement we obtain:
θ = argmax
θ
Epdata(x)[fθ(x)] - mqax Eq(x)[fθ(x)] + H(q)
(4)
which gives us an alternative method for training EBMs. We introduce an auxiliary sampler qφ which
we train online to optimize the inner-loop of Equation 4. This objective was used for EBM training
in Kumar et al. (2019); Abbasnejad et al. (2019); Dai et al. (2017), Dai et al. (2019) (motivated by
Fenchel Duality (Wainwright & Jordan, 2008)). Abbasnejad et al. (2019) use an implicit generative
model and Dai et al. (2019) propose to use a sampler which is inspired by MCMC sampling from
pθ(x) and whose entropy can be computed exactly.
Below we describe our approach which utilizes the same objective with a simpler sampler and a new
approach to encourage high entropy. We note that when training pθ (x) and q(x) online together, the
inner maximization will not be fully optimized. This leads our training objective for pθ (x) to be an
upper bound on logpθ(x). In Section 5.1 we explore the impact of this fact and find that the bound
is tight enough to train models that achieve high likelihood on high-dimensional data.
1For continuous spaces, this would be the differential entropy, but we simply use entropy here for brevity.
3
Published as a conference paper at ICLR 2021
4	Method
We now present a method for training an EBM pθ (x) = efθ(x)/Z(θ) to optimize Equation 4. We
introduce a generator distribution of the form qφ(χ) = Jz qφ(χ∣z)q(z)dz such that:
qφ(x∣z) = N(gψ (z),σ2I),	q(z) = N(0, I)	(5)
where gψ is a neural network with parameters ψ and thus, φ = {ψ, σ2 }. This is similar to the
decoder of a variational autoencoder (Kingma & Welling, 2013). With this architecture it is easy
to optimize the first and second terms of Equation 4 with reparameterization, but the entropy term
requires more care.
4.1	Entropy Regularization
Estimating entropy or its gradients is a challenging task. Multiple, distinct approaches have been
proposed in recent years based on Mutual Information estimation (Kumar et al., 2019), variational
upper bounds (Ranganath et al., 2016), Denoising Autoencoders (Lim et al., 2020), and nearest
neighbors (Singh & POczos, 2016).
The above methods require the training of additional auxiliary models or do not scale well to high
dimensions. Most relevant to this work are Dieng et al. (2019); Titsias & Ruiz (2019) which present
a method for encouraging generators such as ours to have high entropy by estimating VφH(qφ).The
estimator takes the following form:
VφH(qφ) = VφEqφ(x) [log qφ(x)]
= VφEp(z)p() [log qφ(x(z, ))] (Reparameterize sampling)
= Ep(z)p()[Vφ log qφ(x(z, ))]
= Ep(z)p() [Vx logqφ(x(z, ))TVφx(z, )] (Chain rule)	(6)
where we have written x(z, ) = gψ (z) + σ. All quantities in Equation 6 can be easily computed
except for the score-function Vx log qφ(x). The following estimator for this quantity can be easily
derived (see Appendix A.2):
Vx log qφ(x) = Eqφ(z∣χ) Vxlog qφ(x∣z)]	⑺
which requires samples from the posterior qφ(z∣χ) to estimate. Dieng et al. (2019); Titsias & Ruiz
(2019) generate these samples using Hamiltonian Monte Carlo (HMC) (Neal et al., 2011), a gradient-
based MCMC algorithm. As used in Dieng et al. (2019), 28 sequential gradient computations must
be made per training iteration. Since a key motivation of this work is to circumvent the costly
sequential computation of MCMC sampling, this is not a favourable solution. In our work we
propose a more efficient solution that we find works just as well empirically.
4.2	Variational Approximation with Importance Sampling
We propose to replace HMC sampling of qφ(z∣χ) with a variational approximation
ξ(z | z0) ≈ qφ(z | x) where z0 is a conditioning variable we will define shortly. We can use this
approximation with self-normalized importance sampling to estimate
Vx log qφ(x)
Eqφ(z∣x) Vxlog qφ(X | Z)]
Ep(zo)qφ(z∣x) Vxlog qφ(X | Z)]
q$(z | X)
.ξ(Z | ZO)
Vx log qφ(X | Z)
Ep(Z0)ξ3z0) qφ(Xφξ(z )zo) Vxlog qφ(X 1 Z)

k
yZ^V- Vx log qφ(x | Zi) ≡ V x log qφ(χi{Zi}k=ι,Zo)
i=1	j=1 wj
(8)
where 也}k=1 〜ξ(Z | zo) and Wi ≡ ξ*iZx). We use k = 20 importance samples for all experi-
ments in this work. This approximation holds for any conditioning information we would like to use.
4
Published as a conference paper at ICLR 2021
To choose this, let Us consider how samples X 〜qφ(χ) are drawn. We first sample zo 〜N(0, I) and
then x 〜qφ(x | zo). In our estimator we want a variational approximation to qφ(z | x) and by Con-
strUction, z0 is a sample from this distribUtion. For this reason we let oUr variational approximation
be
ξη(z | zo) = N(z | zo, η2I),	(9)
or simply a diagonal Gaussian centered at the zo which generated x. For this approximation to be
useful we must tune the variance η2 . We do this by optimizing the standard Evidence Lower-Bound
at every training iteration
Lelbo(η; zo,x) = Eξθc [log(qφ(x | z)) + log(q0(z))] + H(ξη(Z | zo)).	(10)
We then use ξη (Z | zo) to approximate Nx log qφ(χ) which we plug into Equation 6 to estimate
VφH(qφ for training our generator. A full derivation and discussion can be found in Appendix A.3.
Combining the tools presented above we arrive at our proposed method which we call Variational
Entropy Regularized Approximate maximum likelihood (VERA), outlined in Algorithm 1. We
found it helpful to further add an '2-regularizer with weight 0.1 to the gradient of our model's
likelihood as in Kumar et al. (2019). In some of our larger-scale experiments we reduced the weight
of the entropy regularizer as in Dieng et al. (2019). We refer to the entropy regularizer weight as λ.
Algorithm 1: VERA Training
Input : EBM pθ(x) H efθ(x), generator qφ(x, z), approximate posterior ξη(z|zo),
entropy weight λ, gradient penalty γ = .1
Output: Parameters θ such that pθ ≈ p
while True do
Sample mini-batch x, and generate mini-batch Xg, zo 〜qφ(χ, z)
Compute LELBO (η; zo, xg) and update η	// Update posterior
Compute logfθ(x) 一 logfθ(Xg) + γ∣∣Vx logfθ(x)||2 and update θ // Update EBM
Sample {zi}k=ι 〜g〃(z|zo)
Compute s = Vx log qφ(x; {zi}ik=1, zo)	// Estimate score fn (Eq.8)
Compute g = sT Vφxg	// Estimate entropy gradient (Eq.6)
Update φ with Vφ log fθ(xg) + λg	// Update generator
end
5	EBM Training Experiments
We present results training various models with VERA and related approaches. In Figure 1 we
visualize the impact of our generator’s entropy on the learned density model and compare this with
MCMC sampling used in PCD learning. In Section 5.1, we explore this quantitatively by training
tractable models and evaluating with test-likelihood. In Section 5.2 we explore the bias of our
entropy gradient estimator and the estimator’s effect on capturing modes.
5.1	Fitting Tractable Models
Optimizing the generator in VERA training minimizes a variational upper bound on the likelihood
of data under our model. If this bound is not sufficiently tight, then training the model to maximize
this bound will not actually improve likelihood. To demonstrate the VERA bound is tight enough to
train large-scale models we train NICE models (Dinh et al., 2014) on the MNIST dataset. NICE is a
normalizing flow (Rezende & Mohamed, 2015) model - a flexible density estimator which enables
both exact likelihood computation and exact sampling. We can train this model with VERA (which
does not require either of these abilities), evaluate the learned model using likelihood, and generate
exact samples from the trained models. Full experimental details2 can be found in Appendix B.3
and additional results can be found in Appendix C.1.
2This experiment follows the NICE experiment in Song et al. (2020) and was based on their implementation.
5
Published as a conference paper at ICLR 2021
We compare the performance of VERA with
maximum likelihood training as well as a
number of approaches for training unnor-
malized models; Maximum Entropy Genera-
tors (MEG) (Kumar et al., 2019), Persistent
Contrastive Divergence (PCD), Sliced Score
Matching (SSM) (Song et al., 2020), Denoising
Score Matching (DSM) (Vincent, 2011), Cur-
vature Propagation (CP-SM) (Martens et al.,
2012), and CoopNets (Xie et al., 2018). As an
ablation we also train VERA with the HMC-
based entropy regularizer of Dieng et al. (2019),
denoted VERA-(HMC). Table 2 shows that
VERA outperforms all approaches that do not
require a normalized model.
Figure 2 shows exact samples from our NICE
models. For PCD we can see (as observed in
Figure 1) that while the approximate MCMC
samples resemble the data distribution, the true
samples from the model do not. This is further
reflected in the reported likelihood value which
falls behind all methods besides DSM. Coop-
Nets perform better than PCD, but exhibit the
same behavior of generator samples resembling
the data, but not matching true samples. We at-
tribute this behavior to the method’s reliance on
MCMC sampling.
Conversely, models trained with VERA gen-
erate coherent and diverse samples which rea-
sonably capture the data distribution. We also
see that the samples from the learned generator
Figure 2: Left: Exact samples from NICE model
trained with various methods. Right: Approxi-
mate samples used for training. For VERA, MEG,
and CoopNet, these come from the generator, for
PCD these are approximate MCMC samples.
much more closely match true samples from the NICE model than PCD and MEG. When we re-
move the entropy regularizer from the generator (λ = 0.0) we observe a considerable decrease in
likelihood and we find that the generator samples are far less diverse and do not match exact samples
at all. Intriguingly, entropy-free VERA outperforms most other methods. We believe this is because
even without the entropy regularizer we are still optimizing a (weak) bound on likelihood. Con-
versely, the score-matching methods minimize an alternative divergence which will not necessarily
correlate well with likelihood. Further, Figure 2 shows that MEG performs on par with entropy-free
VERA indicating that the Mutual Information-based entropy estimator may not be accurate enough
in high dimensions to encourage high entropy generators.
Maximum Likelihood	VERA λ=1.0 λ=0.0	VERA (HMC) λ = 1.0	MEG	PCD	SSM	DSM	CP-SM	CoopNet
-791	I	-1138	-1214	-1165	-1219	-4207	-2039	-4363	-1517	-1465
Table 2: Fitting NICE models using various learning approaches for unnormalized models. Results
for SSM, DCM, CP-SM taken from Song et al. (2020).
5.2	Understanding Our Entropy Regularizer
In Figure 3, we explore the quality of our score function estimator on a PCA (Tipping & Bishop,
1999) model fit to MNIST, a setting where we can compute the score function exactly (see Ap-
pendix B.4 for details). The importance sampling estimator (with 20 importance samples) has some-
what larger variance than the HMC-based estimator but has a notably lower bias of .12. The HMC
estimator using 2 burn-in steps (recommended in Dieng et al. (2019)) has a bias of .48. Increasing
the burn-in steps to 500 reduces the bias to .20 while increasing variance. We find the additional
variance of our estimator is remedied by mini-batch averaging and the reduced bias helps explain
the improved performance in Table 2.
6
Published as a conference paper at ICLR 2021
Further, we compute the effective sample size (Kong, 1992) (ESS) of our importance sampling
proposal on our CIFAR10 and MNIST models and achieve an ESS of 1.32 and 1.29, respectively
using 20 importance samples. When an uninformed proposal (N (0, I)) is used, the ESS is 1.0 for
both models. This indicates our gradient estimates are informative for training. More details can be
found in Appendix B.6.
Next, we count the number of modes captured on a dataset with 1,000 modes consisting of 3 MNIST
digits stacked on top of one another (Dieng et al., 2019; Kumar et al., 2019). We find that both
VERA and VERA (HMC) recover 999 modes, but training with no entropy regularization recovers
998 modes. We conclude that entropy regularization is unnecessary for preventing mode collapse in
this setting.
6	Applications to Joint Energy Models
Joint Energy Models (JEM) (Grathwohl et al., 2019)
are an exciting application of EBMs. They reinterpret
standard classifiers as EBMs and train them as such to
create powerful hybrid generative/discriminative models
which improve upon purely-discriminative models at out-
of-distribution detection, calibration, and adversarial ro-
bustness.
Traditionally, classification tasks are solved with a func-
tion fθ : RD → RK which maps from the data to K un-
constrained real-valued outputs (where k is the number of
classes). This function parameterizes a distribution over
labels y given data x: pθ(y|x) = ef(x)[y]∕Py0 efθ(x)[y0].
The same function fθ can be used to define an EBM
for the joint distribution over x and y as: pθ(x, y) =
efθ(x)[y] /Z(θ). The label y can be marginalized out to
give an unconditional model pθ (x) = Py ιef> (x)[y]∕Z (θ).
JEM models are trained to maximize the factorized like-
Burn-in steps / Importance Samples
O IOO 200 300 400 500
Burn-in steps / Importance Samples
Figure 3: Bias (top) and standard devi-
ation (bottom), both per dimension, of
the score function estimator using HMC
and our proposed importance sampling
scheme.
lihood:
log Pθ (x,y) = α log Pθ (y∣x)+log p (x)	(11)
where α is a scalar which weights the two terms. The first
term is optimized with cross-entropy and the second term
is optimized using EBM training methods. In Grathwohl
et al. (2019) PCD was used to train the second term. We train JEM models on CIFAR10, CIFAR100,
and SVHN using VERA instead of PCD. We examine how this change impacts accuracy, generation,
training speed, and stability. Full experimental details can be found in Appendix B.7.
Speed and Stability While the results presented in Grathwohl et al. (2019) are promising, training
models as presented in this work is challenging. MCMC sampling can be slow and training can
easily diverge. Our CIFAR10 models train 2.8x faster than the official JEM implementation3 with
the default hyper-parameters. With these default parameters JEM models would regularly diverge.
To train for the reported 200 epochs training needed to be restarted multiple times and the number
of MCMC steps needed to be quadrupled, greatly increasing run-time.
Conversely, we find that VERA was much more stable and our models never diverged. This allowed
us to remove the additive Gaussian noise added to the data which is very important to stabilize
MCMC training (Grathwohl et al., 2019; Nijkamp et al., 2019a; Du & Mordatch, 2019).
Hybrid Modeling In Tables 3 and 4 we compare the discriminative and generative performance
of JEM models trained with VERA, PCD (JEM), and HDGE (Liu & Abbeel, 2020). With α = 1we
find that VERA leads to models with poor classification performance but strong generation per-
formance. With α = 100 VERA obtains stronger classification performance than the original
JEM model while still having improved image generation over JEM and HDGE (evaluated with
FID (Heusel et al., 2017)).
3https://github.com/wgrathwohl/JEM
7
Published as a conference paper at ICLR 2021
Model	CIFAR10	CIFAR100	SVHN	Model	I	FID J
Classifier	I 95.8	74.2	97.7	JEM HDGE	38.4 37.6
			96.6 N/A		
JEM HDGE	92.9 96.7	72.2 74.5		SNGAN (Miyato et al., 2018) NCSN (Song & Ermon, 2019b)	25.50 23.52
VERA α = 100	93.2	72.2	96.8	VERA α = 100	30.5
VERA α = 1	76.1	48.7	94.2	VERA α = 1	27.5
Table 3: Classification on image datasets.				Table 4: FID on CIFAR10.	
Unconditional samples can be seen from our CIFAR10 and CIFAR100 models in Figure 4. Samples
are refined through a simple iterative procedure using the latent space of our generator, explained in
Appendix B.7.1. Additional conditional samples can be found in Appendix C.5
上才口。巨富”!SlBl且口细 R 后
♦队τκe*slaM∣曲巴・盟史
Figure 4: Unconditional samples on CIFAR10 (left) and CIFAR100 (right).
Out-of-Distribution Detection JEM is a powerful approach for out-of-distribution detection
(OOD), greatly outperforming tractable likelihood models like VAEs and flows (Nalisnick et al.,
2018). In Table 5, reporting AUROC (Hendrycks & Gimpel, 2016), we see that for all but 1 dataset,
VERA outperforms JEM with PCD training but under-performs contrastive training (HDGE). In-
triguingly, VERA performs poorly on CelebA. This result, along with the unreliable performance
of DSM models at this task (Li et al., 2019) leads to questions regarding special benefits of MCMC
training that are lost in our method as well as DSM. We leave this to future work.
Model	I SVHN	CIFAR100	CIFAR10-Interp	CelebA
JEM	0.67	0.67	0.65	0.75
HDGE	0.96	0.91	0.82	0.80
GLOW	0.05	0.51	0.55	0.57
VERA	0.83	0.73	0.86	0.33
Table 5: Out-of-distribution Detection. Model trained on CIFAR10. Values are AUROC (↑).				
6.1	Tabular Data
Training with VERA is much more stable and easy to apply to domains beyond images where EBM
training has been extensively tuned. To demonstrate this we show that JEM models trained with
VERA can provide a benefit to semi-supervised classification on datasets from a variety of domains.
Considerable progress has been made in semi-supervised learning but the most impressive results
require considerable domain knowledge (Chen et al., 2020). In domains like images, text, and
audio such knowledge exists but for data from particle accelerators, gyroscopes, and satellites, such
intuition may not be available and these techniques cannot be applied. In these settings there are far
fewer options for semi-supervised learning.
We present VERA as one such option. We train semi-supervised JEM models on data from a variety
of continuous domains. We perform no data augmentation beyond removing redundant features and
standardizing the remaining features. To further demonstrate the versatility of VERA we use an
identical network for each dataset and method. Full experimental details can be found in Appendix
B.8.
In Table 6, we find on each dataset tested, we find that VERA outperforms the supervised baseline
and outperforms VAT which is the strongest domain agnostic semi-supervised learning method we
are aware of.
7	Related Work
Kumar et al. (2019) train EBMs using entropy-regularized generators, attempting to optimize the
same objective as our own. The key difference is how the generator’s entropy is regularized. Kumar
4We treat MNIST as a tabular dataset since we do not use convolutional architectures.
8
Published as a conference paper at ICLR 2021
Model	HEPMASS	CROP	HUMAN	MNIST4
Supervised Baseline	81.2	87.8	81.0	91.0
VAT	86.7	94.5	84.0	98.6
MEG	72.1	87.5	83.5	94.7
JEM	54.2	18.5	77.2	10.2
VERA	88.3	94.9	89.9	98.6
Full-label	90.9	99.7	98.0	99.5
Table 6: Accuracy of semi-supervised learning on tabular data with 10 labeled examples per class.				
et al. (2019) utilize a Mutual Information estimator to approximate the generator’s entropy whereas
we approximate the gradients of the entropy directly. The method of Kumar et al. (2019) requires
the training of an additional MI-estimation network, but our approach only requires the optimization
of the posterior variance which has considerably fewer parameters. As demonstrated in Section 5.1,
their approach does not perform as well as VERA for training NICE models and their generator
collapses to a single point. This is likely due to the notorious difficulty of estimating MI in high
dimensions and the unreliability of current approaches for this task (McAllester & Stratos, 2020;
Song & Ermon, 2019a).
Dai et al. (2019) train EBMs using the same objective as VERA. The key difference here is in
the architecture of the generator. Dai et al. (2019) use generators with a restricted form, inspired
by various MCMC sampling methods. In this setting, a convenient estimator for the generator’s
entropy can be derived. In contrast, our generators have an unconstrained architecture and we focus
on entropy regularization in the unconstrained setting.
Abbasnejad et al. (2019) train EBMs and generators as well to minimize the reverse KL-divergence.
Their method differs from ours in the architecture of the generator and the method for encouraging
high entropy. Their generator defines an implicit density (unlike ours which defines a latent-variable
model). The entropy is maximized using a series approximation to the generator function’s Jacobian
log-determinant which approximates the change-of-variables for injective functions.
Gao et al. (2020) train EBMs using Noise Contrastive Estimation where the noise distribution is
a normalizing flow. Their training objective differs from ours and their generator is restricted to
having a normalizing flow architecture. These architectures do not scale as well as the GAN-style
architectures we use to large image datasets.
As well there exist CoopNets (Xie et al., 2018) which cooperatively train an EBM and a generator
network. Architecturally, they are similar to VERA but are trained quit differently. In CoopNets, the
generator is trained via maximum likelihood on its own samples refined using MCMC on the EBM.
This maximum likleihood step requires MCMC as well to generate posterior samples as in Pang et al.
(2020a). In contrast, the generator in VERA is trained to minimize the reverse KL-divergence. Our
method requires no MCMC and was specifically developed to alleviate the difficulties of MCMC
sampling.
The estimator of Dieng et al. (2019) was very influential to our work. Their work focused on
applications to GANs. Our estimator could easily be applied in this setting and to implicit variational
inference (Titsias & Ruiz, 2019) as well but we leave this for future work.
8	Conclusion
In this work we have presented VERA, a simple and easy-to-tune approach for training unnormal-
ized density models. We have demonstrated our approach learns high quality energy functions and
models with high likelihood (when available for evaluation). We have further demonstrated the su-
perior stability and speed of VERA compared to PCD training, enabling much faster training of
JEM (Grathwohl et al., 2019) while retaining the performance of the original work. We have shown
that VERA can train models from multiple data domains with no additional tuning. This enables
the applications of JEM to semi-supervised classification on tabular data - outperforming a strong
baseline method for this task and greatly outperforming JEM with PCD training.
9
Published as a conference paper at ICLR 2021
References
M Ehsan Abbasnejad, Qinfeng Shi, Anton van den Hengel, and Lingqiao Liu. A generative adver-
sarial density estimator. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition ,pp.10782-10791, 2019.
JE Besag. Comments on “representations of knowledge in complex systems” by u. grenander and
mi miller. J. Roy. Statist. Soc. Ser. B, 56:591-592, 1994.
Ciwan Ceylan and Michael U Gutmann. Conditional noise-contrastive estimation of unnormalised
models. arXiv preprint arXiv:1806.03664, 2018.
Tong Che, Ruixiang Zhang, Jascha Sohl-Dickstein, Hugo Larochelle, Liam Paull, Yuan Cao, and
Yoshua Bengio. Your gan is secretly an energy-based model and you should use discriminator
driven latent sampling. arXiv preprint arXiv:2003.06060, 2020.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020.
Bo Dai, Zhen Liu, Hanjun Dai, Niao He, Arthur Gretton, Le Song, and Dale Schuurmans. Exponen-
tial family estimation via adversarial dynamics embedding. In Advances in Neural Information
Processing Systems, pp. 10979-10990, 2019.
Zihang Dai, Amjad Almahairi, Philip Bachman, Eduard Hovy, and Aaron Courville. Calibrating
energy-based generative adversarial networks. arXiv preprint arXiv:1702.01691, 2017.
Adji B Dieng, Francisco JR Ruiz, David M Blei, and Michalis K Titsias. Prescribed generative
adversarial networks. arXiv preprint arXiv:1910.04302, 2019.
Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components esti-
mation. arXiv preprint arXiv:1410.8516, 2014.
Yilun Du and Igor Mordatch. Implicit generation and generalization in energy-based models. arXiv
preprint arXiv:1903.08689, 2019.
Yilun Du, Joshua Meier, Jerry Ma, Rob Fergus, and Alexander Rives. Energy-based models for
atomic-resolution protein conformations. arXiv preprint arXiv:2004.13167, 2020.
Ruiqi Gao, Erik Nijkamp, Diederik P Kingma, Zhen Xu, Andrew M Dai, and Ying Nian Wu. Flow
contrastive estimation of energy-based models. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 7518-7528, 2020.
Will GrathWohL KUan-Chieh Wang, Jorn-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi,
and Kevin Swersky. Your classifier is secretly an energy based model and you should treat it like
one. arXiv preprint arXiv:1912.03263, 2019.
Will Grathwohl, Kuan-Chieh Wang, Jorn-Henrik Jacobsen, David Duvenaud, and Richard ZemeL
Learning the stein discrepancy for training and evaluating energy-based models Without sampling.
2020.
Michael Gutmann and Aapo Hyvarinen. Noise-contrastive estimation: A new estimation principle
for unnormalized statistical models. In Proceedings of the Thirteenth International Conference
on Artificial Intelligence and Statistics, pp. 297-304, 2010.
Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution
examples in neural networks. arXiv preprint arXiv:1610.02136, 2016.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in neural information processing systems, pp. 6626-6637, 2017.
Mitch Hill, Jonathan Mitchell, and Song-Chun Zhu. Stochastic security: Adversarial defense using
long-run dynamics of energy-based models. arXiv preprint arXiv:2005.13525, 2020.
10
Published as a conference paper at ICLR 2021
AaPo Hyvarinen. Estimation of non-normalized statistical models by score matching. Journal of
Machine Learning Research, 6(Apr):695-709, 2005.
John Ingraham, Adam J Riesselman, Chris Sander, and Debora S Marks. Learning Protein structure
with a differentiable simulator. In ICLR, 2019.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In
Advances in neural information processing systems, pp. 10215-10224, 2018.
Augustine Kong. A note on importance sampling using standardized weights. University of Chicago,
Dept. of Statistics, Tech. Rep, 348, 1992.
Rithesh Kumar, Sherjil Ozair, Anirudh Goyal, Aaron Courville, and Yoshua Bengio. Maximum
entropy generators for energy-based models. arXiv preprint arXiv:1901.08508, 2019.
Zengyi Li, Yubei Chen, and Friedrich T Sommer. Annealed denoising score matching: Learning
energy-based models in high-dimensional spaces. arXiv preprint arXiv:1910.07762, 2019.
Jae Hyun Lim, Aaron Courville, Christopher Pal, and Chin-Wei Huang. Ar-dae: Towards unbiased
neural entropy gradient estimation. arXiv preprint arXiv:2006.05164, 2020.
Hao Liu and Pieter Abbeel. Hybrid discriminative-generative training via contrastive learning. arXiv
preprint arXiv:2007.09070, 2020.
James Martens, Ilya Sutskever, and Kevin Swersky. Estimating the hessian by back-propagating
curvature. arXiv preprint arXiv:1206.6464, 2012.
David McAllester and Karl Stratos. Formal limitations on the measurement of mutual information.
In International Conference on Artificial Intelligence and Statistics, pp. 875-884, 2020.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Gorur, and Balaji Lakshminarayanan. Do
deep generative models know what they don’t know? arXiv preprint arXiv:1810.09136, 2018.
Radford M Neal et al. Mcmc using hamiltonian dynamics. Handbook of markov chain monte carlo,
2(11):2, 2011.
Erik Nijkamp, Mitch Hill, Tian Han, Song-Chun Zhu, and Ying Nian Wu. On the anatomy of mcmc-
based maximum likelihood learning of energy-based models. arXiv preprint arXiv:1903.12370,
2019a.
Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying Nian Wu. Learning non-convergent non-
persistent short-run mcmc toward energy-based model. In Advances in Neural Information Pro-
cessing Systems, pp. 5232-5242, 2019b.
Frank Noe, Simon Olsson, Jonas Kohler, and Hao Wu. Boltzmann generators: Sampling equilibrium
states of many-body systems with deep learning. Science, 365(6457):eaaw1147, 2019.
Bo Pang, Tian Han, Erik Nijkamp, Song-Chun Zhu, and Ying Nian Wu. Learning latent space
energy-based prior model. Advances in Neural Information Processing Systems, 33, 2020a.
Tianyu Pang, Kun Xu, Chongxuan Li, Yang Song, Stefano Ermon, and Jun Zhu. Efficient learning of
generative models via finite-difference score matching. arXiv preprint arXiv:2007.03317, 2020b.
11
Published as a conference paper at ICLR 2021
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Rajesh Ranganath, Dustin Tran, and David Blei. Hierarchical variational models. In International
Conference on Machine Learning, pp. 324-333, 2016.
Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. arXiv
preprint arXiv:1505.05770, 2015.
Benjamin Rhodes, Kai Xu, and Michael U Gutmann. Telescoping density-ratio estimation. arXiv
preprint arXiv:2006.12204, 2020.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in neural information processing systems,
pp. 2234-2242, 2016.
Shashank Singh and Barnabas Poczos. Analysis of k-nearest neighbor distances with application to
entropy estimation. arXiv preprint arXiv:1603.08578, 2016.
Jiaming Song and Stefano Ermon. Understanding the limitations of variational mutual information
estimators. arXiv preprint arXiv:1910.06222, 2019a.
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
In Advances in Neural Information Processing Systems, pp. 11918-11930, 2019b.
Yang Song and Stefano Ermon. Improved techniques for training score-based generative models.
arXiv preprint arXiv:2006.09011, 2020.
Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach
to density and score estimation. In Uncertainty in Artificial Intelligence, pp. 574-584. PMLR,
2020.
Yunfu Song and Zhijian Ou. Learning neural random fields with inclusive auxiliary generators.
arXiv preprint arXiv:1806.00271, 2018.
Tijmen Tieleman. Training restricted boltzmann machines using approximations to the likelihood
gradient. In International Conference on Machine Learning, pp. 1064-1071, 2008.
Michael E Tipping and Christopher M Bishop. Probabilistic principal component analysis. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):611-622, 1999.
Michalis K Titsias and Francisco Ruiz. Unbiased implicit variational inference. In The 22nd Inter-
national Conference on Artificial Intelligence and Statistics, pp. 167-176, 2019.
Pascal Vincent. A connection between score matching and denoising autoencoders. Neural compu-
tation, 23(7):1661-1674, 2011.
Martin J Wainwright and Michael Irwin Jordan. Graphical models, exponential families, and vari-
ational inference. Now Publishers Inc, 2008.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In
Proceedings of the 28th international conference on machine learning (ICML-11), pp. 681-688,
2011.
Jianwen Xie, Yang Lu, Ruiqi Gao, Song-Chun Zhu, and Ying Nian Wu. Cooperative training of de-
scriptor and generator networks. IEEE transactions on pattern analysis and machine intelligence,
42(1):27-45, 2018.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint
arXiv:1605.07146, 2016.
Stephen Zhao, Jorn-Henrik Jacobsen, and Will GrathWohL Joint energy-based models for Semi-
supervised classification.
12
Published as a conference paper at ICLR 2021
A Key Derivations
A. 1 Derivation of Variational Log-Partition Function
Here we show that the variational optimization given in equation 3 recovers log Z(θ).
mqax Eq(x) [fθ(x)] + H(q)
= max	q(x)fθ (x)dx -	q(x) log(q(x))dx
qx	x
max
q
max
q
max
q
max
q
q(x) log
x
q(x) log
x
q(x) log
x
exp(fθ (x))
q(χ)
exp(fθ (x))
q(X)
dx
dx - logZ(θ) + logZ(θ)
exp(fθ (x))∕Z (θ)
q(x)
dx + logZ(θ)
-KL(q(x)kpθ(x)) + log Z(θ)
logZ(θ).
A.2 Score Function Estimator
Here We derive the equivalent expression for Yx log qΦ(x) given in Equation 7.
Vχ log qφ(x) = ▽*(X)
Vx ʃz qφ(x,z)dz
qφ(X)
z Vxqφ(X, z)dz
qφ(X)
Vxqφ(x,z)
dz
z
Zz
Vxqφ(x | z)qφ(z)
dz
((Vx log qφ(χ | z))qφ(x | z)qφ(z) dz
Jz	qφ(x)
Eqφ(z∣x)[Vx log qφ(x∣z)].
A.3 Entropy Gradient Estimator
From Equation 6 We have
VφH(qφ) = Ep(z0)p() [Vx log qφ(X)TVφX(z0, )].
13
Published as a conference paper at ICLR 2021
Plugging in our score function estimator gives
VφH(qφ) = Ep(zo)p(e)[Vχ log qφ(x)TVφx(zo,e)]
=Ep(zo)p(e) [Eqφ(z∣x) [Vx log qφ(X | Z)] Vφx(z0, €)|
(12)
-qφ(z | x)
一ξη(Z | z0)
Vx log qφ(x | Z)	Vφx(Z0, )
-	q@ (X)Z)
.Qφ(x)ξη(Z । ZO)
Wi
k
j=1 Wj
qφ(zi,x)
ξ(Zi |z0) .
Vx log qφ(x | Z)	Vφx(Z0, )
≈ EP(Z0)P(e) 1X
Vx log qφ(X | Zi)	VφX(Z0, )
where {zi}1k=1 〜ξ(z | zo) and Wi ≡
A.3.1 Discussion
We discuss when the approximations in Equation 13 will hold. The importance sampling estimator
will be biased when qψ(Z | X) differs greatly from ξ(Z | Z0). Since the generator function gψ(Z) is a
smooth, Lipschitz function (as are most neural networks) and the output Gaussian noise is small, the
space of Z values which could have generated X should be concentrated near Z0 . In these settings,
Z0 should be useful for predicting q(Z|X).
The accuracy of this approximation is based on the dimension of Z compared to X and the Lipschitz
constant of gψ. In all settings we tested, dim(Z) dim(X) where this approximation should hold.
If dim(Z) dim(X) the curse of dimensionality would take effect and Z0 would be less and less
informative about q(Z|X). In settings such as these, we do not believe our approach would be as
effective. Thankfully, almost all generator architectures we are aware of have dim(Z) dim(X).
The approximation could also break down if the Lipschitz constant blew up. We find this does not
happen in practice, but this can be addressed with many forms of regularization and normalization.
14
Published as a conference paper at ICLR 2021
B Experimental Details
B.1 Hyperparameter Recommendations
Type		Hyperparameter	Value
Optimization	βι (ADAM) β2 (ADAM) learning rate (energy) learning rate (generator) learning rate (posterior)	0 0.9 10-4* 2∙10-4* 2∙10-4*
Regularization	λ (entropy regularization weight) γ (gradient norm penalty)	10-4t 0.1
JEM	α (classification weight) β (classification entropy)	{1,10, 30,100}* {1,0.1 * α, α}
Table 7: Hyperparameters for VERA.
*When α > 1, learning rates were divided by α.
*We found λ = 10-4 to work best on large image datasets, but in general We recommend starting
with λ = 1 and trying successively smaller values of λ until training is stable.
We give some general tips on how to set hyperparameters when training VERA in Table 7. In all
VERA experiments, we use the gradient norm penalty with weight 0.1. This was not tuned during
our experiments. When using VERA and MEG we train with Adam (Kingma & Ba, 2014) and set
β1 = 0, β2 = .9 as is standard in the GAN literature (Miyato et al., 2018). In general, we recommend
setting the learning rate for the generator to twice the learning rate of the energy function and equal
to the learning rate of the approximate posterior sampler.
B.1.1	IMPACT OF λ
Let us rewrite the generator’s training objective
L(q; λ) = Eq(x) [fθ(x)] + λH(q).
We can easily see that this objective is equivalent (up to a multiplicative constant) to
Eq(x)
fθ (x)
λ
+ H(q).
(13)
(14)
From this, it is clear that maximizing Equation 14 is the same as minimizing the KL-divergence
between q and a tempered version ofpθ(x) defined as
efθ (X)I 入
-Z-
(15)
Tempering like this is standard practice in EBM training and is done in many recent works. Tem-
pering has the effect of increasing the weight of the gradient signal in SGLD sampling relative to
the added Gaussian noise. In all of Du & Mordatch (2019); Grathwohl et al. (2019); Nijkamp et al.
(2019a;b) the SGLD samplers used a temperature of 1∕λ = 20,000. This value is near the value of
10, 000 that we use in this work.
Thus we can see that our most important hyper-parameter is actually the temperature of the sampler’s
target distribution. Ideally this temperature would be set to 1, but this can lead to unstable training
(as it does with SGLD). To train high quality models, we recommend setting λ as close to 1 as
possible, decreasing if training becomes unstable.
B.2	Toy Data Visualizations
We train simple energy functions on a 2D toy data. The EBM is a 2-layer MLP with 100 hidden
units per layer using the Leaky-ReLU nonlinearity with negative slope .2. The generator is a 2-layer
15
Published as a conference paper at ICLR 2021
MLP with 100 hidden units per layer and uses batch normalization (Ioffe & Szegedy, 2015) with
ReLU nonlinearities. All models were trained for 100,000 iterations with all learning rates set to
.001 and used the Adam optimizer (Kingma & Ba, 2014).
The PCD models were trained using an SGLD sampler and a replay buffer with 10,000 examples,
reinitialized every iteration with 5% probability. We used 20 steps of SGLD per training iteration to
make runtime consistent with VERA. We tested σ values outside of the presented range but smaller
values did not produce decent samples or energy functions and for larger values, training diverged.
B.3	Training NICE Models
The NICE models were exactly as in Song et al. (2020). They have 4 coupling layers and each
coupling layer had 5 hidden layers. Each hidden layer has 1000 units and uses the Softplus non-
linearity. We preprocessed the data as in Song et al. (2020) by scaling the data to the range [0, 1],
adding uniform noise in the range [-1/512, 1/512], clipping to the range [.001, .999] and applying
the logit transform log(x) - log(1 - x). All models were trained for 400 epochs with the Adam
optimizer (Kingma & Ba, 2014) with β1 = 0 and β2 = .9. We use a batch size of 128 for all models.
We re-ran the score matching model of Song et al. (2020) to train for 400 epochs as well and found
it did not improve performance as its best test performance happens very early in training.
For all generator-based training methods we use the same fixed generator architecture. The generator
has a latent-dimension of 100 and 2 hidden layers with 500 units each. We use the Softplus nonlin-
earity and batch-normalization (Ioffe & Szegedy, 2015) as is common with generator networks.
For VERA the hyper-parameters we searched over were the learning rates for the NICE model and
for the generator. Compared to Song et al. (2020) we needed to use much lower learning rates. We
searched over learning rates in {.0003, .00003, .000003} for both the generator and energy function.
We found .000003 to work best for the energy function and .0003 to work best for the generator. This
makes intuitive sense since the generator needs to be fully optimized for the bound on likelihood to
be tight. When equal learning rates were used (.0003, .0003) we observed high sample quality from
the generator but exact samples and likelihoods from the NICE model were very poor.
For PCD we search over learning rates in {.0003, .00003, .000003}, the number of MCMC steps
in {20, 40} and the SGLD noise standard-deviation in {1.0, 0.1}. All models with 20 steps and
SGLD standard-deviation 1.0 quickly diverged. Our best model used learning rate .000003, step-
size 0.1, and 40 steps. We tested the gradient-norm regularizer from Kumar et al. (2019) and found it
decreased performance for PCD trained models. Most models with 20 MCMC steps diverged early
in training.
For review the MCMC sampler we use is stochastic gradient Langevin dynamics (Welling & Teh,
2011). This sampler updates its samples by
2
Xt = Xt-1 + -2 Vχfθ(x) + eσ, e 〜N(0, I)	(16)
where σ is the noise standard-deviation and is a parameter of the sampler.
For Maximum Entropy Generators (MEG) (Kumar et al., 2019) we must choose a mutual-
information estimation network. We follow their work and use an MLP with LeakyReLU non-
linearities with negative slope .2. Our network mirrors the generator and has 3 hidden layers with
500 units each. We searched over the same hyper-parameters as VERA. We found MEG to perform
almost identically to training with no entropy regularization at all. We believe this has to do with the
challenges of estimating MI in high dimensions Song & Ermon (2019a).
For CoopNets (Xie et al., 2018) we use the same flow and generator architectures as VERA. Fol-
lowing the MNIST experiments in Xie et al. (2018) we train using 10 SGLD steps per iteration. We
tried the recommended learning rates of .007 and .0001 for the flow and generator, respectively but
found this to lead quick divergence. For this reason, we search over learning rates for the flow and
generator from {.0003, .00003, .000003} as we did for VERA and found the best combination to
be .000003 for the flow and .0003 for the generator. Other combinations resulted in higher quality
generator samples but much worse likelihood values. We tested the recommended SGLD step-size
of .002 and found this to lead to divergence as well in this setup. Thus, we searched over larger
values of {.002, .02, .1} found .1 to perform the best, as with PCD.
16
Published as a conference paper at ICLR 2021
B.4	Estimation of Bias of Entropy Regularizer
If we restrict the form of our generator to a linear function
X = Wz + μ + σe, z, e ~ N(0, I)
then we have
q(x∣z) = N(Wz + μ, σ21),	q(x) = N(μ, WTW + σ21)
meaning We can exactly compute log q(x), and Nx log q(x) which is the quantity that VERA (HMC)
approximates with the HMC estimator from Dieng et al. (2019) and we approximate with VERA. To
explore this, we fit a PCA model on MNIST and recover the parameters W, μ of the linear generator
and the noise parameter σ. Samples from this model can be seen in Figure 5.
Both VERA and VERA (HMC) have some parameters which are tuned automatically to improve the
estimator throughout training. For VERA this is the posterior variance which is optimized according
to Equation 10 with Adam with default hyperparameters. For VERA (HMC) this is the stepsize of
HMC which is tuned automatically as outlined in (Dieng et al., 2019). Both estimators were trained
with a learning rate of .01 for 500 iterations with a batch size of 5000. Samples from the estimators
during training were taken with the default parameters, in particular the number of burn-in steps or
the number of importance samples was not varied as they were during evaluation.
The bias of the estimators were evaluated on a batch of 10 samples from the generator. For each
example in the batch, 5000 estimates were taken from the estimator and averaged to be taken as
an estimate of the score function for this batch example. This estimate of the score function was
subtracted from the true score function and then averaged over all dimensions and examples in the
batch and taken as an estimate of the bias per dimension.
For VERA (HMC) we varied the number of burn-in steps used for samples to evaluate the bias of
the estimator. We also tried to increase the number of posterior samples taken off the chain, but we
found that this did not clearly reduce the bias of this estimator as the number of samples increased.
For VERA we computed the bias on the default of 20 importance samples.
Figure 5: Samples from linear model trained with PCA on MNIST.
B.5	Mode Counting
We train VERA without labels on the S tackedMNIS T dataset. This dataset consists of 60,000
examples of3 stacked MNIST images sampled with replacement from the original 60,000 MNIS T
train set. As in (Dieng et al., 2019) we resize the images to 64 × 64 and use the DCGAN (Radford
et al., 2015) architecture for both the energy-function and generator. We use a latent code of 100
dimensions. We train for 17 epochs with a learning rate of .001 and batch size 100.
We estimate the number of modes captured by taking S = 10, 000 samples as in (Dieng et al., 2019)
and classifying each digit of the 3 stacked images separately with a pre-trained classifier on MNIST.
B.6	Effective Sample Size
When performing importance sampling, the quality of the proposal distribution has a large impact.
If the proposal is chosen poorly, then typically 1 sample will dominate in the expectation. This can
17
Published as a conference paper at ICLR 2021
be quantified using the effective sample size (Kong, 1992) (ESS) which is defined as
wi =
P(X)
q(X)
Wi =
ESS =
(17)
where p(X) is the target distribution, q(X) is the proposal distribution and N is the number of sam-
ples. If the self-normalized importance weights are dominated by one weight close to 1 then the
ESS will be 1. If the proposal distribution is identical to the target, so that the self-normalized im-
portance weights are then uniform, then the ESS will be N . When ESS = N , importance sampling
is as efficient as using the target distribution.
To understand the effect of the proposal distribution on ESS we plot the ESS when doing importance
sampling with 20 importance samples from a 128-dimensional Gaussian target distribution with
μ = 0 and Σ = I. We USe a proposal which is a 128-dimensional GaUSSian with μ increasing from
0 to 5. Results can be seen in Figure 6. We see when the means differ by greater than 2, the ESS is
approximately 1.0 and importance sampling has effectively failed.
FigUre 6: Effective sample size of importance sampling as the proposal degrades.
B.7	JEM Models
OUr energy fUnction Used the same architectUre as in Grathwohl et al. (2019); LiU & Abbeel (2020),
a Wide ResNet ZagorUyko & Komodakis (2016) 28-10 with batch-norm removed and with dropoUt.
OUr generator architectUre is identical to Miyato et al. (2018). As in Grathwohl et al. (2019); LiU &
Abbeel (2020) we set the learning rate for the energy fUnction eqUal to 0.0001. We set the learning
rate for the generator eqUal to 0.0002. We train for 200 epochs Using the Adam optimizer with
β1 = 0 and β2 = .9. We set the batch size to 64. ResUlts presented are from the models after 200
epochs with no early stopping. We believe better resUlts coUld be obtained from fUrther training.
We trained models with α ∈ {1, 30, 100} and foUnd classification to be best with α = 100 and
generation to be best with α = 1.
Prior work on PCD EBM training (Grathwohl et al., 2019; DU & Mordatch, 2019; Nijkamp et al.,
2019a;b) recommends adding GaUssian noise to the data to stabilize training. WithoUt this, PCD
training of JEM models very qUickly diverges. Early in oUr experiments we foUnd training with
VERA was stable withoUt the addition of GaUssian noise so we do not Use it.
As mentioned in Dieng et al. (2019), when the strength of the entropy regUlarizer λ is too high, the
generator may fall into a degenerate optimUm where it jUst oUtpUts high-entropy GaUssian noise. To
combat this, as sUggested in Dieng et al. (2019), we decrease the strength of λ to .0001 for all JEM
experiments. This valUe was chosen by decreasing λ from 1.0 by factors of 10 Until learning took
place (qUantified by classification accUracy).
18
Published as a conference paper at ICLR 2021
B.7.1	MCMC Sample Refinement
Our generator qφ(x) is trained to approximate our EBM pθ (x). After training, the samples from
the generator are of high quality (see Figures 9 and 10, left) but they are not exactly samples from
pθ (x). We can use MCMC sampling to improve the quality of these samples. We use a simple
MCMC refinement procedure based on the Metropolis Adjusted Langevin Algorithm (Besag, 1994)
applied to an expanded state-space defined by our generator and perform the Accept/Reject step in
the data space.
We can reparameterize a generator sample X 〜qφ(χ) as a function χ(z, e) = gψ(Z) + eσ, and We
can define an unnormalized density over {z, e}, log h(z, e) ≡ fθ(gψ(z)+ eσ) - log Z(θ, φ) which
is the density (under pθ (x)) of the generator sample.
Starting with an initial sample zo, e0 〜 N(0, I) we define the proposal distribution
p(ztlzt-1, Ct-I) = N (zt-1 + 2Nzt-I log h(Zt-1, Et-I), δ2I
P(et|zt-i, et-i) = N (zt-i + 2NQt-I log h(zt-i, Et-I), δ2I
p(zt, Ct|zt-1, Ct-1) =p(zt|zt-1, Ct-1)p(Ct|zt-1, Ct-1)
and accept a new sample with probability
min
-h(zt,et)p(zt,et∣zt-i,et-i)	；
_h(zt-i,et)p(zt-i,et-i|zt,et), 一.
Here, δ is the step-size and is a parameter of the sampler. We tune δ with a burn-in period to target
an acceptance rate of 0.57.
A similar sampling procedure was proposed in Kumar et al. (2019) and Che et al. (2020) and in both
works was found to improve sample quality. In all experiments,
We clarify that this procedure is nota valid MCMC samplerforpθ(x) due to the augmented variables
and the change in density of gψ which are not corrected for. The density of the samples will be a
combination ofpθ (x) and qφ(x). As the focus of this work was training and not sampling/generation,
we leave the development of more correct generator MCMC-sampling to future work. Regardless,
we find this procedure to improve visual sample quality. In Figure 7 we visualize a sampling chain
using the above method applied to our JEM model trained on SVHN.
Figure 7: Visualization of our MALA-inspired sample refinement procedure. Samples come from
JEM model trained on SVHN. Chains progress to the right and down. Each image is a consecutive
step, no sub-sampling is done.
19
Published as a conference paper at ICLR 2021
B.7.2	Image Quality Metrics
We present results on Inception Score (Salimans et al., 2016) and Frechet Inception Distance (Heusel
et al., 2017). These metrics are notoriously fickle and different repositories are known to give very
different results (Grathwohl et al., 2019). For these evaluations we generate 12,800 samples from
the model and (unless otherwise stated) refine the samples with 100 steps of our latent-space MALA
procedure (Appendix B.7.1). The code to generate our reported FID comes from this publicly avail-
able repository. The code to generate our reported Inception Score can be found here.
B.8	Semi-supervised learning on Tabular Data
B.8.	1 Data
We provide details about each of the datasets used for the experiments in Section 6.1. HEPMASS5
is a dataset obtained from a particle accelerator where we must classify signal from background
noise. CROP6 is a dataseset for classifying crop types from optical and radar sensor data. HUMAN7
is a dataset for human activity classification from gyroscope data. MNIST is an image dataset of
handwritten images, treated here as tabular data.
Dataset	HEPMASS	CROP	HUMAN	MNIST
Features	15	174	523	784
Examples	6,300,000	263,926	6,617	60,000
Classes	2	7	6	10
Max class %	50.0	26.1	19.4	11.2
Table 8: Basic information about each tabular dataset.
For HEPMASS and HUMAN data we remove features which repeat the same exact same value
more than 5 times. For CROP data we remove features which have covariation greater than 1.01.
For MNIST we linearly standardize features to the interval [-1, 1].
We take a random 10% subset of the data to use as a validation set.
B.8.2 Training
We use the same architecture for all experiments and baselines. It has 6 layers of hidden units with
dimensions [1000, 500, 500, 250, 250, 250] and a Leaky-ReLU nonlinearity with negative-slope .2
between each layer of hidden units. The only layers which change between datasets are the input
layer and the output layer which change according to the number of features and number of classses
respectively.
The training process for semi-supervised learning is similar to JEM with an additional objective
commonly used in semi-supervised learning:
logpθ(x,y) = αlogpθ(y | x) + logpθ(x) + βH(pθ(y | x))	(18)
where H(p(y|x)) is the entropy of the predictive distribution over the labels.
For all models we report the accuracy the model converged to on the held-out validation set. We
report the average taken over three training runs with different seeds.
We use equal learning rates for the energy model, generator, and the entropy estimator. We tune the
learning rate and decay schedule for supervised models on the full-set of labels and 10 labels per
class.
5http://archive.ics.uci.edu/ml/datasets/HEPMASS
6https://archive.ics.uci.edu/ml/datasets/Crop+mapping+using+fused+
optical-radar+data+set
7https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+
Using+Smartphones
20
Published as a conference paper at ICLR 2021
On VERA we tune the learning rate and decay schedule, the weighting of the entropy regularization
λ and the weighting of the entropy of classification outputs β .
For VAT we tune the perturbation size ∈ {.01, .1, 1, 3, 10}. All other hyperparameters were fixed
according to tuning on VERA.
For MEG we used the hyperparameters tuned according to VERA.
For JEM we tune the number of MCMC steps in the range κ ∈ {20, 40, 80}. We generate samples
using SGLD with stepsize 1 and noise standard deviation 0.01 as in Grathwohl et al. (2019).
Dataset	HEPMASS	CROP	HUMAN	MNIST
Learning rate	10-5	10-4	10-5	10-5
Number of epochs	1	20	800	200
Batch size	64	64	64	64
Decay rate	.3	.3	.3	.3
Decay epochs	{.05, .1}	{15, 17}	{200, 300, 400, 600}	{150, 175}
z (latent) dimension	10	128	128	128
λ	10-4	10-4	10-4	10-4
α	1	1	10	1
β	1	1	1	1
(VAT)	.1	3	.1	3
κ (JEM)	20	20	40	80
Table 9: Hyperparameters for semi-supervised learning on tabular data
21
Published as a conference paper at ICLR 2021
C Additional Results
C.1 Training Mixtures of Gaussians
We present additional results training mixure of Gaussian models using VERA and PCD. Each
model consits of 100 diagonal Gaussians and mixing weights. Our experimental setup and hyper-
parameter search was identical to that presented in Appendiex B.2. We see in Table 10 that VERA
outperforms PCD.
	Max. Likelihood	λ = 0.0	VERA λ= .01 λ=0.1		λ=1.0	PCD
Moons	-2.32	-3.87	-3.63	-3.10	-2.58	-3.82
Circles	-3.17	-3.58	-3.64	-3.74	-3.40	-4.24
Rings	-2.83	-3.5	-3.44	-3.24	-3.17	-4.13
Table 10: Fitting a mixture of 100 diagonal Gaussians using ML, MCMC approximate ML and
generator approximate ML.
C.2 Samples from SSL Models
We present some samples from our semi-supervised MNIST models in Figure 8.
/5 G 彳/3/6
<Γ%λ^6∕4,
g / S
y夕q
I ? z
。49
C Q I
H ‹f 3
? 57
d q 5
2 76331og
/ / k 5Sq「
7 ʃ 9ys∕q3
3 / Λ 彳 I7g3
⅛6J 4<βsG2
「5r7o(oVgq
/ 白L1⅛-?6
r 73±1f∕(2
7¾oqcv5 7
7c⅜3 夕Z彳-i>0o
Ir'∕sγ<∂3 夕
Figure 8: Unconditional MNIST Samples. Left: samples from the generator and right: samples after
100 steps if MCMC refinement using MALA.
C.3 Hybrid Modeling
We present an extended Table 4 with Inception Score Salimans et al. (2016) and which includes
more comparisons.
Model	I FID IS
JEM	38.4	8.76
HDGE	37.6	9.19
SNGAN (Miyato et al., 2018)	25.50	8.59
NCSN (Song & Ermon, 2019b)	23.52	8.91
ADE (Dai et al., 2019)	N/A	7.55
IGEBM (Du & Mordatch, 2019)	37.9	8.30
Glow (Kingma & Dhariwal, 2018)	48.9	3.92
FCE (Gao et al., 2020)	37.3	N/A
VERA α = 100	30.5	8.11
VERA α = 1	27.5	8.00
VERA α = 1 (generator samples)	32.4	7.34
22
Published as a conference paper at ICLR 2021
C.4 Unconditional Image Generation: CIFAR 1 0
Figure 9: Unconditional CIFAR10 Samples. Left: samples from the generator and right: samples
after 100 steps if MCMC refinement using MALA.
C.5 Unconditional Image Generation: CIFAR 1 00
Figure 10: Unconditional CIFAR100 Samples. Left: samples from the generator and right: samples
after 100 steps if MCMC refinement using MALA.
23
Published as a conference paper at ICLR 2021
C.6 Conditional Image Generation: CIFAR10 and CIFAR 1 00
2H■ E≡ΠL Esak as 幻=ES<丝≡∙S国匚：⅛E DBQ^Q
E3EHr A* R邺以■ 一_ H≡>R≡yHV r≡κ≡aEΠS≡KS1B≡) Cl
E叁□⅛XJ!ElbΞl=aROHE"H・■纹∙px≡FES07vc≡w⅛ql x≡
Q"一 NBNBq^ss随' .>徐汇□然 R WNB 二二口玉 F 喀S■乂 ・ SB=H-
□∙j≡M S G≡4πl平QLlutf ❷-;丁、以»■ m 一H口〕警用HαEl,≡≡
BQSHHaMBHE MIliiaaRE =□≡ɑE XPSs%
SBttBQ≡ -→1 Bli≡ ⅛iS
物中“▼ jΞκQα≡ M 晨 IhlHx0£≡H小s力≡0nIΛsi嗡EIq□krwH
Hi 0xB,≡κEll 口蜷raMwl⅛HlsH7p>≡a憾aEl±R≡≈B≡ a
BH7 舒！aSΓH≡≡αsπ季d,∙⅞m≡ħ≡e Sa X T≡≡≡= 口S
7□qGκΞfiiE不■ z≡入口磬U"'*X=一qNHHHIXUMQe二Rfi・
点a≡RPsH阳 ^^⅛H⅞>HSE / ∙HHn。)1 Fn0Q
L⅞κx√∙,∖lκ≡
阳β0 -Itf≡≡≡Jlia5NH阳RrGue3EKE Js ^ ^s≡Daκ XX
二储呼 JBJElγ≡πτ ls>wn□,v∙π≡lτ-w□≡H=S 场口M<N
≡x⅛κ 妻 EIl.D『l』NB≡sR"rj≡:- -F≡7>aππ3口 1Λ≡C
巧妻Bja~ 附至 3s≡x≡Hq- ^≡i i‹-≡x0πsq≡bhXKQE
WEBQq 二as≡sm∖BE≡1B9一墨<«B4「一 ■⅛!hh:π1≡dH&KI，>
3■■= ZMYΠU Bsp ■ Ert≡C∙HFBL:濯≡E1蜀Fl>Sa□≡
ElQqsf3≡lla生≡κ□Qα源H≡* X 2q KHHKHaκaR=国人 L£。
一二一防西点Ξ∙≡3一名 Nn 塞αsn-⅛□4Q≡Bκħħh零≡sHHnH3
BE itt>x Fv ra ES2CQ×i⅛QΞP昌瓯口 r≡ 二)XSB
πsEaQ2Qi≡3r=7/ 魅二H□Bii0l 啕BS9^HXM 下二qu~5□J)上
总，・5!11a二肩上梃RDY 二 □□〕⅛≡7≡kN / ~ 修&二百
垢 ZIL!3s同HEZM,>>∙s□≡弓 TU @ e』,A霞口E≡□a≡.M 潮≡□SES≡
§ .Eli^≡≡1lq产≡n*≡x S 7B%aBM≡
ia-DHE≡ -QSeiii
⅛--Bκ40aQm∙ s»≡ TH -
Hs≡的 TEp<Q7ym_»・x ÷ S RJ 4απ^uD GiIQ- - QaSB
乜・ a^□^s^≡≡z≡≡Ξ≡H3r□□α⅛ ≡- - □□ a TQgaSa
I WBC≡∙ iaaΒ≡∙s≡≡S≡Q□X 3>□pκ- N≡FF≡≡∕≡a
-SSSBQlHi XEl-二 ql≡■二-ZXE5
R≡H"DB∙≡πlr UH 口三ΞxaHEΞXaBΞVH 4LRQRaHΞB
三-."*、・ 一≡G x≡w E≡I- .l≡κlEa氨ξ≡xl≡ħa≡= Hx ■ . ≡
αnB∙≡ssg≡i□gΞB≡ 送Q』」*Eax32HΞsuH SEDHEM
Figure 11: Class-conditional samples from CIFAR10
24
Published as a conference paper at ICLR 2021
ħ⅝瓢附甦里≡BEK
'N就，^ss
唾szsκ⅝⅛s∙茅瓜
里里矗sur卑
Ss
占二苜个
s≡⅛
言*咋圆
UBiE
生⅛zw⅛二*
二：二匚一二
国πN 4
良羯篇Is-
SHIH
他r
0S
KM S
-S3,≡⅛
liei^
易庵窟目丛里与绘凶一
同0M*-"Ξ『『⅛
S i^iæ
^SB£BR⅛
E谟区理给噂
Si
∙⅛<二J身二”潜；•.: Q
∙i⅞衣」F凤小器建后戛自具
Un砂康三段号/ *KΞ∖⅛晨
lEEK=K⅛n
旨El隹0最亳曷惠Rra⅛
^⅛^ls≡si
僵 SSP½≡0w争⅛⅛器
M ”F望0』而缶W≡LB更
⅛s∙,二二Bm,¾酸嘴口,r,κ 裔-eaR &爨为帝
HM导，Ξ二K出多聒即NH局W晏冕比所能
Hs^ ,^iisB⅛u, ⅛fe S K
≡震日善晶C部益心i^‰Γa- !!
H«，闻'.f⅞wlgt之 ▼艮*^l¾
' Fte;』BUΓn, Mr-
芯回配Ξ，■滥吸-嗫信”
谓重胃，••■>&>片陶: T--Sw 誓 Y⅛π-
EW 0B4 定居≡w⅛⅛等q Lyτ⅛1⅛出
^s r r上邑口惠三G巴四≡弟ΠMsq羲
雪置份谬亮Har髭Wia⅛κ⅛L净
.8sn S-ip
l¾np ≡ 需豪E⅜RP察3用『』
■田声二二□』"二Mo% SKE3备SSS.篷售融哥昙 QrU 昼≡p
S蓄i域号邑上Ew荔忠，g不r二- A琴Ξ黑苣G Ssn
⅛rL-⅛ 驾 0昂.FElW - r j堡^⅞帚;。口 rΓ⅛⅛为Ej 二？Rπ
宣Ξ典 r⅜ si w j SMgHK 扁Sg网春注❸囊CM a Xr三 WW二T」
7:1R3tm芸选Wj弟寓N存Sni雪降金≡:雪翳凝毒
--,⅜10p嚏忘AS伞 §s^ S^S0
己三F F山⑥L::: Eu就E i⅛ Sss si⅛
不⅛⅛m Lg二3A，r -0;访茶R9≡12i⅛wwmτAH
碎券F货中区MG IlZITS国
玲H⅜0且电E宅 0 覆3卷％⅛qug
治里WE⅛BS缸”粤s≡器 3 (ΞΞ粤
S沼岑嬴装n⅝g⅛⅛' En,^
嚣 BHM 望∙*⅛n
迪士 IW鸿
误≡*f M二 MU，喝君≡⅜
F⅛15A 爆等.ETrEngmW
题砌PU空≡∙L福注晨UE史嫉
意两鼻髭胃瞌⅛a≡Bj≡≡w
Figure 12: Class-conditional samples from CIFAR10
25
Published as a conference paper at ICLR 2021
√Γjβ ^ Jmk - ft
Figure 13: Class-conditional samples from CIFAR10
YJ 工	UrtFEd 流
Σ二

4 4 `匡①IMF
二一*
26
Published as a conference paper at ICLR 2021
F缚曰一□"*El型姗祝到国仪
，口例初习日
I 一 ―一	-V* _	・ 1
£iRklrE豳画型■■考以印
« LiJEaJL ：门上用LN四彤9圈i I W IMLi⅛rr
n、兰胆.，里里金今"爰於I日制4的溜阿______
Figure 14:	Class-conditional samples from CIFAR10
27
Published as a conference paper at ICLR 2021


0≡πp蠲M二一
Figure 15:	Class-conditional samples from CIFAR10
28
Published as a conference paper at ICLR 2021
Figure 16: Class-conditional samples from CIFAR10
29
Published as a conference paper at ICLR 2021
Figure 17: Class-conditional samples from CIFAR10
30
Published as a conference paper at ICLR 2021
7fir 野&篇Ea∙≡图，ɪ也就本⅛raHH幅邸居域 ΛElκ⅜⅛i
缢KZ”『.还E匿r岁社菊商⅛∙i
S匕血血蓼HRW曲m&N国帝.
Bil
军力*务旗S3IK黑工日一
力二鹿喝DE变阳卷J≡g⅝l
沙容B
一二鹿里国有目值
* g∙QκM⅛?施
.网&20
EIE *ml ZE≡ 彳 riEE 4 k=≡u■阙^≡EK■匮 AK 必武占于上
W 国 i....as. H5,1⅝^-Q1
LArSrjR□崔论麻航
R⅛Fau
L二月腐
i≡Eτ3A 曲
蜃MK旧园ra.
££ ≡is ⅛rElE∕5H⅛⅛
9*Μ4 二■m ,bq∙f 倒∙nldlκ3∖ ■,・• l⅛r .一di
⅛B ⅛Afis⅛^
口府工□rM3蒸/X口公疯UE
-3⅛lis^ S 苣
丁峻也.皿/普目E YlsN囱龈
Il瑞性圈%q 0 K，£,翻的簿M
Uk整改蕾端诔B1
k二

R和一仁 2IBHg3F0
/m::ɑr
此餐由Nk
K血霹蠹』
心雷口 "不 emR!!sc3h HGF Y函 M篇
BΛxfzaljf弱仁写可ŋ朝中E曼山钳飞
Kra ⅛ .i½raFS.^
赧X而后也
3思£霹7口七强
口南面G

sτ,⅛必毫位小国
!saia冷・lit⅛Ul以EL
『止避股B匹
R 商3R⅛r vτ^ir-
飞二廓Sl曲盟BB3k当⅛

SRi
工帛壮口嘘中函冰
二⅜力匿Ia匕空4
一言向k流
I二二 k M
El V
也不黑盾岁溢yΞ洞辱爵正
≡Ξ3羽蠡片L
JISL爵
■XFTJ HETI⅛□lz⅛督随届意理院飞曲ΞηK溜 F 藐KJIUE=
」铲M3"⅛¾编舞0色用ssu^-1-对逐球「
T号耍?--——「而□溢上13FL睢m国・广Jn^>≡
≡∙g^ra2qE!制斑raKM'二题阈掇曾!Γ声H噂必整A通宵，瘗「7Ki3等侬nEao
Wa施m≡≡H为口照近/⅛逖≡E2阖建富B胸用鲍四知Γβ而R阳SC踵日㈤E 金
⅛ ,∙∙Γ≡⅛1上rJ僦时三温绘SiLS通列孩年以⅜esr飞固面∖s□号W匾F海也Q公
Mrzʒ髀1kκd上布蒙陋匿理晶罐M/罂河芯>1⅛l⅛ K/二二Nul三内13
须批我皖S5MΞ^Π:?如2 二部捕璃£:JH醒瑜 K 壬二ξk⅛l空π*≡或上仁说
逸旺.MXmmN腐累你而以需白不琴EH心弗瑞肖区Nia国K⅛srτ▲归4aMkr¼话
Figure 18: Class-conditional samples from CIFAR10
31
Published as a conference paper at ICLR 2021
⅛玛απ!*∙yEl-NFIH■番■■F・sIHHS Hr!
■图网寻■≡β■ E一西 9・1* £ 电雪fa≡E=≡≡⅞l≡或 Sfl



■Un 皋∙≡l⅛∙∙ *■■■■■Kym看芟皑≡B0≡≡雪3吗

!B

I
IaHB-
∙*∙b≡b∙ħ⅛« 第 i-hHl■网更胸βB■襄电号≡事»■■£■
≡R≡≡S⅛⅛二M(■海 B∙Er∙Hw'FQ≡≡El≡∙ll时
VlIE∙≡≡ILM∙llllB∙e∙≡∙∙∙ll∙∙s理一≡≡慝IH ■
Ss S 7 ⅛!≡N≡I:EJHi⅝≡fn4例 4 =Y 口
≡ħ
■■
WI! ______Il____WiJz≡≡豳E≡=M 修"
IL一g⅛≡≡∙ E 同委礴■ ~ y 4 二
■■■■■■■■■■I蒙«■鼠弱ħ=≡≡≡∙≡ħs≡成同普》
HB同■■■■■■■£■■一备 3∙≡hse≡∙ħ0=s=≡富
W立三二
虱一-

一
1>_-■事总≡Ξ∙E*α¾'kET齐生画第』・
E∙5∙iaJiFB3≡ ~ Z≡三e≡∙ħ3门零四!≡-ħ≡
一 一一三


二三H
■£■■ L≡≡Hs-≡喝.≡≡∙≡ll≡≡ar≡gHe∙ BnIliW
≡EK≡B≡≡B≡ ≡ Ssw∙π≡KiIRSHS.=
X 二二一
■■■■■■■号 =手9・_W∙q=r∙⅛∙ħrh≡∙∙
S ∙rq=E⅛≡s!昼 M 尊BHQ«Ilq露 5 IrE≡≡5)・一
Figure 19: Class-conditional samples from CIFAR10
32
Published as a conference paper at ICLR 2021
Figure 20: Class-conditional samples from CIFAR10
Ey ・
CE0im∙
∙>rrrB∏n
Tl
HNFHHπMHMπ
UHKUU HB i i
2加'；<v^Hn1H «
M^⅛ιB^sm∕≡σ^
HMML^UraaRV
UM□UH旧也一
Figure 21: Class-conditional samples from CIFAR100
33
Published as a conference paper at ICLR 2021
Figure 22: Class-conditional samples from CIFAR100

工磔信00g・觉•• i _不一
+01潴■腰编篇个疑个中芦添
im^GlS^≡Ξfl!⅝
露SI"∙E!S空二MM副孤昨

国口国
IE3Rβ
IIU已■盘用D制名曲已飞漏匕
B圜蛤副■■■副向况同E2E∙
IiiILl.L3LiUifii '乜庐
01：!馍点.Dk口网IJMEl ・
Figure 23: Class-conditional samples from CIFAR100
Figure 24: Class-conditional samples from CIFAR100
34
Published as a conference paper at ICLR 2021
Figure 25: Class-conditional samples from CIFAR100
盘式国&岷口曰♦3力哂
因飞回晶际/笛划口扁期
M嫌客上也亡*上底7等
IF二口?■孤* 人■■
曲■好I iHM■£窗■
图■，Ji IImq
a 蔻。J 上雪H1 ■垢
人∙UXJI ,L ,十 A
ɪ lll‰HBH vκW
碇 / ∙7∖*∙l!,■甲
ɪ ■” 1 Λ 一
tι146 志a∙∙k'
■ ■■gH 1- J 『 」■・
ɪɪ ■ h - k 4≡xr5
≡≡■K委 =τhT
 
■KEUT2£ S
-▼% _・-∙qγτ,一T
工T4EHNZ工工天量
f 蠡 MMV
工κ≡x■工 7∙t
 
一▲,■■・看矣工工KN
 
 
，WE工rH££

Figure 26:
from CIFAR100
Figure 27: Class-conditional samples from CIFAR100
35