Published as a conference paper at ICLR 2021
Contrastive Learning with
Hard Negative Samples
Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, Stefanie Jegelka
Massachusetts Institute of Technology
Cambridge, MA, USA
{joshrob,cychuang,suvrit,stefje}@mit.edu
Ab stract
How can you sample good negative examples for contrastive learning? We argue
that, as with metric learning, contrastive learning of representations benefits from
hard negative samples (i.e., points that are difficult to distinguish from an anchor
point). The key challenge toward using hard negatives is that contrastive methods
must remain unsupervised, making it infeasible to adopt existing negative sampling
strategies that use true similarity information. In response, we develop a new
family of unsupervised sampling methods for selecting hard negative samples
where the user can control the hardness. A limiting case of this sampling results in
a representation that tightly clusters each class, and pushes different classes as far
apart as possible. The proposed method improves downstream performance across
multiple modalities, requires only few additional lines of code to implement, and
introduces no computational overhead.
1	Introduction
Owing to their empirical success, contrastive learning methods (Chopra et al., 2005; Hadsell et al.,
2006) have become one of the most popular self-supervised approaches for learning representations
(Oord et al., 2018; Tian et al., 2019; Chen et al., 2020a). In computer vision, unsupervised con-
trastive learning methods have even outperformed supervised pre-training for object detection and
segmentation tasks (Misra & Maaten, 2020; He et al., 2020).
Contrastive learning relies on two key ingredients: notions of similar (positive) (x, x+) and dissimilar
(negative) (x, x- ) pairs of data points. The training objective, typically noise-contrastive estimation
(Gutmann & Hyvarinen, 2010), guides the learned representation f to map positive pairs to nearby
locations, and negative pairs farther apart; other objectives have also been considered (Chen et al.,
2020a). The success of the associated methods depends on the design of informative of the positive
and negative pairs, which cannot exploit true similarity information since there is no supervision.
Much research effort has addressed sampling strategies for positive pairs, and has been a key driver
of recent progress in multi-view and contrastive learning (Blum & Mitchell, 1998; Xu et al., 2013;
Bachman et al., 2019; Chen et al., 2020a; Tian et al., 2020). For image data, positive sampling
strategies often apply transformations that preserve semantic content, e.g., jittering, random cropping,
separating color channels, etc. (Chen et al., 2020a;c; Tian et al., 2019). Such transformations have
also been effective in learning control policies from raw pixel data (Srinivas et al., 2020). Positive
sampling techniques have also been proposed for sentence, audio, and video data (Logeswaran &
Lee, 2018; Oord et al., 2018; Purushwalkam & Gupta, 2020; Sermanet et al., 2018).
Surprisingly, the choice of negative pairs has drawn much less attention in contrastive learning.
Often, given an “anchor” point x, a “negative” x- is simply sampled uniformly from the training
data, independent of how informative it may be for the learned representation. In supervised and
metric learning settings, “hard” (true negative) examples can help guide a learning method to correct
its mistakes more quickly (Schroff et al., 2015; Song et al., 2016). For representation learning,
informative negative examples are intuitively those pairs that are mapped nearby but should be far
apart. This idea is successfully applied in metric learning, where true pairs of dissimilar points are
available, as opposed to unsupervised contrastive learning.
Code available at: https://github.com/joshr17/HCL
1
Published as a conference paper at ICLR 2021
With this motivation, we address the challenge of selecting informative negatives for contrastive
representation learning. In response, we propose a solution that builds a tunable sampling distribution
that prefers negative pairs whose representations are currently very similar. This solution faces two
challenges: (1) we do not have access to any true similarity or dissimilarity information; (2) we need
an efficient sampling strategy for this tunable distribution. We overcome (1) by building on ideas from
positive-unlabeled learning (Elkan & Noto, 2008; Du Plessis et al., 2014), and (2) by designing an
efficient, easy to implement importance sampling technique that incurs no computational overhead.
Our theoretical analysis shows that, as a function of the tuning parameter, the optimal representations
for our new method place similar inputs in tight clusters, whilst spacing the clusters as far apart as
possible. Empirically, our hard negative sampling strategy improves the downstream task performance
for image, graph and text data, supporting that indeed, our negative examples are more informative.
Contributions. In summary, we make the following contributions:
1.	We propose a simple distribution over hard negative pairs for contrastive representation
learning, and derive a practical importance sampling strategy with zero computational
overhead that takes into account the lack of true dissimilarity information;
2.	We theoretically analyze the hard negatives objective and optimal representations, showing
that they capture desirable generalization properties;
3.	We empirically observe that the proposed sampling method improves the downstream task
performance on image, graph and text data.
Before moving onto the problem formulation and our results, we summarize related work below.
1.1 Related Work
Contrastive Representation Learning. Various frameworks for contrastive learning of visual
representations have been proposed, including SimCLR (Chen et al., 2020a;b), which uses augmented
views of other items in a minibatch as negative samples, and MoCo (He et al., 2020; Chen et al.,
2020c), which uses a momentum updated memory bank of old negative representations to enable the
use of very large batches of negative samples. Most contrastive methods are unsupervised, however
there exist some that use label information (Sylvain et al., 2020; Khosla et al., 2020). Many works
study the role of positive pairs, and, e.g., propose to apply large perturbations for images Chen et al.
(2020a;c), or argue to minimize the mutual information within positive pairs, apart from relevant
information for the ultimate prediction task (Tian et al., 2020). Beyond visual data, contrastive
methods have been developed for sentence embeddings (Logeswaran & Lee, 2018), sequential data
(Oord et al., 2018; Henaff et al., 2020), graph (Sun et al., 2020; Hassani & Khasahmadi, 2020; Li et al.,
2019) and node representation learning (Velickovic et al., 2019), and learning representations from
raw images for off-policy control (Srinivas et al., 2020). The role of negative pairs hase been much
less studied. Chuang et al. (2020) propose a method for “debiasing”, i.e., correcting for the fact that
not all negative pairs may be true negatives. It does so by taking the viewpoint of Positive-Unlabeled
learning, and exploits a decomposition of the true negative distribution. Kalantidis et al. (2020)
consider applying Mixup (Zhang et al., 2018) to generate hard negatives in latent space, and Jin et al.
(2018) exploit the specific temporal structure of video to generate negatives for object detection.
Negative Mining in Deep Metric Learning. As opposed to the contrastive representation learning
literature, selection strategies for negative samples have been thoroughly studied in (deep) metric
learning (Schroff et al., 2015; Song et al., 2016; Harwood et al., 2017; Wu et al., 2017; Ge, 2018; Suh
et al., 2019). Most of these works observe that it is helpful to use negative samples that are difficult
for the current embedding to discriminate. Schroff et al. (2015) qualify this, observing that some
examples are simply too hard, and propose selecting “semi-hard” negative samples. The well known
importance of negative samples in metric learning, where (partial) true dissimilarity information is
available, raises the question of negative samples in contrastive learning, the subject of this paper.
2 Contrastive Learning Setup
We begin with the setup and the idea of contrastive representation learning. We wish to learn an
embedding f : X → Sd-1/t that maps an observation x to a point on a hypersphere Sd-1/t in Rd of
radius 1/t, where t is the “temperature” scaling hyperparameter.
Following the setup of Arora et al. (2019), we assume an underlying set of discrete latent classes C
that represent semantic content, so that similar pairs (x, x+) have the same latent class. Denoting
2
Published as a conference paper at ICLR 2021
Sample Hard
Negatives
(our method)
Figure 1: Schematic illustration of negative sampling methods for the example of classifying species
of tree. Top row: uniformly samples negative examples (red rings); mostly focuses on very different
data points from the anchor (yellow triangle), and may even sample examples from the same class
(triangles, vs. circles). Bottom row: Hard negative sampling prefers examples that are (incorrectly)
close to the anchor.
the distribution over latent classes by ρ(c) for c ∈ C, we define the joint distribution px,c(x, c) =
p(χ∣c)ρ(c) whose marginalp(x) We refer to simply as p, and assume SUPP(P) = X. For simplicity,
we assume ρ(c) = τ+ is uniform, and let τ- = 1 - τ+ be the probability of another class. Since
the class-prior τ + is unknown in practice, it must either be treated as a hyperparameter, or estimated
(Christoffel et al., 2016; Jain et al., 2016).
Let h : X → C be the true underlying hypothesis that assigns class labels to inputs. We write X 〜χ0 to
denote the label equivalence relation h(x) = h(x0). We denote bypx+(x0) = p(x0|h(x0) = h(x)), the
distribution over points with same label as x, and by px-(x0) = p(x0|h(x0) 6= h(x)), the distribution
over points with labels different from x. We drop the subscript x when the context is clear. Following
the usual convention, we overload '〜’ and also write X 〜P to denote a point sampled from p.
For each data point X 〜p, the noise-contrastive estimation (NCE) objective (Gutmann & Hyvarinen,
2010) for learning the representation f uses a positive example X+ with the same label as X, and
negative examples {Xi-}iN=1 with (supposedly) different labels, h(Xi-) 6= h(X), sampled from q:
Ex~p, x+~p+
{χ-}N=ι~q
ef(x)Tf(x+)
—log---------------------τ7------------
ef(χ)Tf(χ+) + Q PN=I ef(χ)Tf(χ-)
(1)
The weighting parameter Q is introduced for the purpose of analysis. When N is finite we take
Q = N, yielding the usual form of the contrastive objective. The negative sample distribution q is
frequently chosen to be the marginal distribution p, or, in practice, an empirical approximation of it
(Tian et al., 2019; Chen et al., 2020a;c; He et al., 2020; Chen et al., 2020c; Oord et al., 2018; Henaf
et al., 2020). In this paper we ask: is there a better way to choose q?
3 Hard Negative Sampling
In this section we describe our approach for hard negative sampling. We begin by asking what makes
a good negative sample? To answer this question we adopt the following two guiding principles:
Principle 1. q should only sample “true negatives” Xi- whose labels differ from that of the anchor X.
Principle 2. The most useful negative samples are ones that the embedding currently believes to be
similar to the anchor.
In short, negative samples that have different label from the anchor, but that are embedded nearby
are likely to be most useful and provide significant gradient information during training. In metric
learning there is access to true negative pairs, automatically fulfilling the first principle.
In unsupervised contrastive learning there is no supervision, so upholding Principle 1 is impossible
to do exactly. In this paper we propose a method that upholds Principle 1 approximately, and
simultaneously combines this idea with the key additional conceptual ingredient of “hardness”
(encapsulated in Principle 2). The level of “hardness” in our method can be smoothly adjusted,
3
Published as a conference paper at ICLR 2021
allowing the user to select the hardness that best trades-off between an improved learning signal from
hard negatives, and the harm due to the correction of false negatives being only approximate. This
important since the hardest points are those closest to the anchor, and are expected to have a high
propensity to have the same label. Therefore the damage from the approximation not removing all
false negatives becomes larger for harder samples, creating the trade-off. As a special case our our
method, when the hardness level is tuned fully down, we obtain the method proposed in (Chuang
et al., 2020) that only upholds Principle 1 (approximately) but not Principle 2. Finally, beyond
Principles 1 and 2, we wish to design an efficient sampling method that does not add additional
computational overhead during training.
3.1 Proposed Hard Sampling Method
Our first goal is to design a distribution q on X that is allowed to depend on the embedding f and the
anchor x. From q we sample a batch of negatives {xi-}iN=1 according to the principles noted above.
We propose sampling negatives from the distribution qβ- defined as
q-(x-) ：= qβ(x-∣h(x) = h(x-)), where qβ(x-) H eβf(x)>f(x ) ∙p(x-),
for β ≥ 0. Note that qβ- and qβ both depend on x, but we suppress the dependance from the notation.
The exponential term in qβ is an unnormalized Von Mises-Fisher distribution with mean direction
f(x) and “concentration parameter” β (Mardia & Jupp, 2000). There are two key components to qβ- ,
corresponding to each principle: 1) conditioning on the eVent {h(x) 6= h(x- )} which guarantees
that (x, x- ) correspond to different latent classes (Principle 1); 2) the concentration parameter β
term controls the degree by which qβ up-weights points x- that haVe large inner product (similarity)
to the anchor x (Principle 2). Since f lies on the surface of a hypersphere of radius 1/t, we haVe
kf(x) - f(x0)k2 = 2/t2 - 2f (x)>f (x0) so preferring points with large inner product is equiValent
to preferring points with small squared Euclidean distance.
Although we haVe designed qβ- to haVe all of the desired components, it is not clear how to sample
efficiently from it. To work towards a practical method, note that we can rewrite this distribution by
adopting a PU-learning Viewpoint (Elkan & Noto, 2008; Du Plessis et al., 2014; Chuang et al., 2020).
That is, by conditioning on the eVent {h(x) = h(x- )} we can split qβ(x- ) as
qβ(x-) = τ-qβ- (x-) +τ+qβ+(x-),	(2)
where qβ+(x-') = qβ(x-∣h(x) = h(x-)) H eβf(x)>f(x ) ∙p+(x-). Rearranging equation 2 yields a
formula qβ-	(x- )	=	qβ (x- ) -	τ+qβ+(x-	)	/τ-	for the negatiVe sampling distribution	qβ-	in terms of
two distributions that are tractable since we haVe samples from p and can approximate samples from
p+ using a set of semantics-preserVing transformations, as is typical in contrastiVe learning methods
(see Appendix E for extra discussion of practical implications of this approximation).
It is possible to generate samples from qβ and approximately from qβ+ using rejection sampling
and data augmentations to generate positiVes. HoweVer, rejection sampling inVolVes an algorithmic
complication since the procedure for sampling batches must be modified. To aVoid this, we instead
take an importance sampling approach. To obtain this, first note that fixing the number Q and taking
the limit N → ∞ in the objectiVe (1) yields,
ef(x)Tf(x+)
L(f,q) = Exxlp+ Tog ef(x)Tf(x+)+ QEx-〜q[ef(x)Tf(x-)] .	⑶
The original objectiVe (1) can be Viewed as a finite negatiVe sample approximation to L(f, q) (note
implicitly L(f, q) depends on Q) . Inserting q = qβ- and using the rearrangement of equation (2) we
obtain the following hardness-biased objectiVe:
ef (x)T f (x+ )
E x〜P	— log----------------K------------------------------------------
x十〜p+ [	ef(x)Tf(x+)+ T-(Ex-〜qβ [ef(x)Tf(x-)] — τ+Ev〜度[ef(x)Tf(v)])
This objective suggests that we need only to approximate expectations Ex-f[ef(X)Tf(x )] and
Ev〜q+ [ef(X)Tf(v)] over qβ and qβ+ (rather than explicily sampling). This can be achieved using
4
Published as a conference paper at ICLR 2021
classical Monte-Carlo importance sampling techniques using samples from p and p+ as follows:
Ex-〜qβ [ef(x)Tf(x-)] = Ex-〜p[ef(X)Tf(Cqe/p] = Ex-〜p[e(β+1)f(X)Tf(X--IZe],
Ev 〜q+ [ef (X)T f(v)] = Ev 〜p+ [ef (X)T f (v)q+∕p+ ] = Ev 〜p+ [e(β+1)f (x)T f (v)/Z+],
where Ze , Ze+ are the partition functions of qe and qe+ respectively. The right hand terms readily
admit empirical approximations by replacing P and p+ with p(x)=4 PN=1 64-(x) and p+ (x)=
i
吉 PM= ι δ/+ (x) respectively (δw denotes the Dirac delta function centered at w). The only unknowns
left are the partition functions, Ze = Ex-〜p[eβf(X)Tf(X )] and Z, = E)+〜p+ [eβf(X)Tf(x+ )] which
themselves are expectations over p and p+ and therefore admit empirical estimates,
NM
Ze = NN X a),	Z+ = MM X eβf (x)>f (x+).
i=1	i=1
It is important to emphasize the simplicity of the implementation of our proposed approach. Since
we propose to reweight the objective instead of modifying the sampling procedure, only two extra
lines of code are needed to implement our approach, with no additional computational overhead.
PyTorch-style pseudocode for the objective is given in Fig. 13 in Appendix D.
4	Analysis of Hard Negative Sampling
4.1	Hard Sampling Interpolates Between Marginal and Worst-Case Negatives
Intuitively, the concentration parameter β in our proposed negative sample distribution qe- controls
the level of “hardness” of the negative samples. As discussed earlier, the debiasing method of Chuang
et al. (2020) can be recovered as a special case: taking β = 0 to obtain the distribution q0- . This
case amounts to correcting for the fact that some samples in a negative batch sampled from p will
have the same label as the anchor. But what interpretation does large β admit? Specifically, what
does the distribution qe- converge to in the limit β → ∞, if anything? We show that in the limit qe-
approximates an inner solution to the following zero-sum two player game.
ef(X)Tf(X+)
inf SUP 产f, q) = EXp+ [ - log ef(x)Tf(x+)+ QEx_q[ef(x)Tf(x-)]
(5)
where Π = {q = q(∙; x,f) : SUPP (q(∙; x,f)) ⊆ {χ0 ∈ X : X W x},∀x ∈ X} is the set of
distributions with support that is disjoint from points with the same class as x (without loss of
generality We assume {χ0 ∈ X : χ0 W x} is non-empty). Since q = q(∙; x, f) depends on X and f it
can be thought of as a family of distributions. The formal statement is as follows.
Proposition 3. Let L*(f) = supq∈∏ L(f, q). Thenfor any t > 0 and f : X → Sd-1∕t we observe
the convergence L(f, q-) —→ L*(f) as β → ∞.
Proof. See Appendix A.1.	□
To develop a better intuitive understanding of the worst case negative distribution objective L (f)=
supq∈Π L(f, q), we note that the supremum can be characterized analytically. Indeed,
supL(f, q) = -E X〜p f (x)Tf (x+) + supE x~p log {ef(X)Tf(x+) + QEx-〜q[ef(X)Tf(x)]0
q∈∏	x+〜p+	q∈∏ x+〜p+
=-E X〜p f(x)Tf (x+) + E X〜p log {ef(X)Tf(x+) + Q ∙ SUpEX-〜q[ef(X)Tf(x-)]}
x+〜p+	x+〜p+	q∈∏
The supremum over q can be pushed inside the expectation since q is a family of distribution indexed
by x, reducing the problem to maximizing EX-〜q [ef (x) f(x )], which is solved by any q* whose
support is a subset of arg supX-:X- X ef(X)T f(X-) if the supremum is attained. However, computing
such points involves maximizing a neural network. Instead of taking this challenging route, using
qe- defines a lower bound by placing higher probability on x- for which f(x)Tf(x-) is large. This
lower bound becomes tight as β → ∞ (Proposition 3).
5
Published as a conference paper at ICLR 2021
4.2	Optimal Embeddings on the Hypersphere for Worst-Case Negative Samples
What desirable properties does an optimal contrastive embedding (global minimizer of L) possess
that make the representation generalizable? To study this question, we first analyze the distribution
of an optimal embedding f * on the hypersphere when negatives are sampled from the adversarial
worst-case distribution. We consider a different limiting viewpoint of objective (1) as the number
of negative samples N → ∞. Following the formulation of Wang & Isola (2020) we take Q = N
in (1), and subtract logN. This changes neither the set of minimizers, nor the geometry of the loss
surface. Taking the number of negative samples N → ∞ yields the limiting objective,
L∞(f, q) = E x~p
x+〜p+
ef (x)T f (x+)
一 log Eχ.q [ef (X)Tf (XF _.
(6)
Theorem 4. Suppose the downstream task is classification (i.e. C is finite), and let L*∞ (f) =
supq∈Π L∞(f, q) . The infimum inff: measurable L*∞(f) is attained, and any f* achieving the global
minimum is such that f* (x) = f*(x+) almost surely. Furthermore, letting vc = f* (x) for any x
such that h(x) = c (so vc is well defined up to a set of x of measure zero), f* is characterized as
being any solution to the following ball-packing problem,
r max	P(C) ∙ min kvc- vc0k2.
{3 "}c∈C C∈C	c=c
(7)
Proof. See Appendix A.2.
□
Interpretation: The first component of the result is that f* (x) = f*(x+) almost surely for an
optimal f*. That is, an optimal embedding f* must be invariant across pairs of similar inputs
x, x+ . The second component is characterizing solutions via the classical geometrical Ball-Packing
Problem of Tammes (1930) (Eq. 7) that has only been solved exactly for uniform P, for specific of |C|
and typically for S2 (Schutte & Van derWaerden,1951; MUSin & TaraSoV,2015; Tammes, 1930).
When the distribution P over classes is uniform this problem is solved by a set of |C| points on the
hypersphere such that the average SqUared-'2 distance from a point to the nearest other point is as
large as possible. In other words, sUppose we wish to place |C| nUmber of balls1 on Sd-1 so that
they do not intersect. Then solutions to Tammes’ Problem (7) expresses (twice) the largest possible
average squared radius that the balls can have. So, we have a ball-packing problem where instead of
trying to pack as many balls as possible of a fixed size, we aim to pack a fixed number of balls (one
for each class) to have as big radii as possible. Non-uniform P adds importance weights to each fixed
ball. In summary, solutions of the problem minf L*∞(f) are a maximum margin clustering.
This understanding of global minimizers of L*∞ (f) = supq∈Π L∞ (f, q) can further developed
into a better understanding of generalization on downstream tasks. The next result shows that
representations that achieve small excess risk on the objective L*∞ still separate clusters well in the
sense that a simple 1-nearest neighbor classifier achieves low classification error.
Theorem 5. Suppose P is uniform on C and f is such that L∞(f) 一 inff measurable L∞(f) ≤ ε
with ε ≤ 1. Let {vC ∈ Sd-1∕t}c∈c be a solution to Problem 7, and define the constant ξ =
minc,c-：c=c- J] v* — Vc-Il > 0. Then there exists a set of vectors {vc ∈ Sd-1∕t}c∈c such that the
1-nearest neighbor classifier h(x) = argminδ∈c∣∣f (x) — vc∣∣ (ties broken arbitrarily) achieves
misclassification risk,
_	，G ，、，、
Pχ,c(h(x) = C) ≤
8ε
(ξ2 - 2|C| (1 + 1∕t)ε1/2)2
Proof. See Appendix A.3.
□
In particular, P(h(x) 6= C) = O(ε) as ε → 0, and in the limit ε → 0 we recover the invariance claim
of Theorem 4 as a special case. The result can be generalized to arbitrary P by replacing |C| in the
bound by 1/ minc P(C). The result also implies that it is possible to build simple classifiers for tasks
that involve only a subset of classes from C, or classes that are a union of classes from C . The constant
ξ = minc,c-:c6=c- vc* - vc*- > 0 is a purely geometrical property of spheres, and describes the
minimum separation distance between a set of points that solves the Tammes’ ball-packing problem.
1For a manifold M ⊆ Rd, we say C ⊂ M is a ball if it is connected, and there exists a Euclidean ball
B = {x ∈ Rd : kxk2 ≤ R} for which C = M ∩B.
6
Published as a conference paper at ICLR 2021
5	Empirical Results
Next, we evaluate our hard negative sampling method empirically, and apply it as a modification to
state-of-the-art contrastive methods on image, graph, and text data. For all experiments β is treated
as a hyper-parameter (see ablations in Fig. 2 for more understanding of how to pick β). Values
for M and τ + must also be determined. We fix M = 1 for all experiments, since taking M > 1
would increase the number of inputs for the forward-backward pass. Lemma 11 in the appendix
gives a theoretical justification for the choice of M = 1. Choosing the class-prior τ+ can be done
in two ways: estimating it from data (Christoffel et al., 2016; Jain et al., 2016), or treating it as a
hyper-parameter. The first option requires the possession of labeled data before contrastive training.
5.1	Image Representations
We begin by testing the hard sampling method on vision tasks using the STL10, CIFAR100 and
CIFAR10 data. We use SimCLR (Chen et al., 2020a) as the baseline method, and all models
are trained for 400 epochs. The results in Fig. 2 show consistent improvement over SimCLR
(q = p) and the particular case of our method with β = 0 proposed in (Chuang et al., 2020) (called
debiasing) on STL10 and CIFAR100. For N = 510 negative examples per data point we observe
absolute improvements of 3% and 7.3% over SimCLR on CIFAR100 and STL10 respectively, and
absolute improvements over the best debiased baseline of 1.9% and 3.2%. On tinyImageNet (Tab.
1) we observe an absolute improvement of 3.6% over SimCLR, while on CIFAR10 there is a slight
improvement for smaller N, which disappears at larger N . See Appendix C.1 results using MoCo-v2
for large negative batch size, and Appendix D.1 for full setup details.
Negative Sample Size N (STLlO)
Negative Sample SizeN(CIFARlOO)
Negative Sample Size N (CIFAR10)
Figure 2: Classification accuracy on downstream tasks. Embeddings trained using hard, debiased,
and standard (β = 0, τ+ = 0) versions of SimCLR, and evaluated using linear readout accuracy.
5.2	Graph Representations
Second, we consider hard negative sampling in the context
of learning graph representations. We use the state-of-the-
art InfoGraph method introduced by Sun et al. (2020) as
the baseline, which is suitable for downstream graph-level
classification. The objective is of a slightly different form
from the NCE loss. Because of this we use a generalization
SimCLR	Debiased	Hard (β = 1)
53.4%	53.7%	57.0%
Table 1: Top-1 linear readout on tinyIm-
ageNet. Class prior is set to τ+ = 0.01.
of the formulation presented in Section 3 (See Appendix B for details). In doing so, we illustrate that
it is easy to adapt our hard sampling method to other contrastive frameworks.
Fig. 3 shows the results of fine-tuning an SVM (Boser et al., 1992; Cortes & Vapnik, 1995) on
the fixed, learned embedding for a range of different values of β . Hard sampling does as well as
InfoGraph in all cases, and better in 6 out of 8 cases. For ENZYMES and REDDIT, hard negative
samples improve the accuracy by 3.2% and 2.4%, respectively, for DD and PTC by 1 - 2%, and
for IMDB-B and MUTAG by at least 0.5%. Usually, multiple different choices of β > 0 were
competitive with the InfoGraph baseline: 17 out of the 24 values of β > 0 tried (across all 8 datasets)
achieve accuracy as high or better than InfoGraph (β = 0).
5.3	Sentence Representations
Third, we test hard negative sampling on learning representations of sentences using the quick-
thoughts (QT) vectors framework introduced by Logeswaran & Lee (2018), which uses adjacent
sentences (before/after) as positive samples. Embeddings are trained using the unlabeled BookCorpus
dataset (Kiros et al., 2015), and evaluated following the protocol of Logeswaran & Lee (2018) on six
downstream tasks. The results are reported in Table 2. Hard sampling outperforms or equals the QT
7
Published as a conference paper at ICLR 2021
A0tβJn00v
DD
H⅛Φ
72.5% 72.8% 72.2% 73.8%
ENZYMES
InfoGraph (β = 0)	Hard (β = l)	Hard (β = 2)	Hard (β = 10)
»*s
3
»«
»5»
∙Λ>S
UW
«7S
MM
Average Accuracy
Figure 3: Classification accuracy on downstream tasks. We compare graph representations on
four classification tasks. Accuracies are obtained by fine-tuning an SVM readout function, and are
the average of 10 runs, each using 10-fold cross validation. Results in bold indicate best performer.
baseline in 5 out of 6 cases, the debiased baseline (Chuang et al., 2020) in 4 out of 6, and both in 3
out of 6 cases. Setting τ + > 0 led to numerical issues in optimization for hard sampling.
Objective	MR	CR	SUBJ	MPQA	TREC	MSRP	
						(Acc)	(F1)
QT (β = 0, τ+ = 0)	76.8	81.3	86.6	93.4	89.8	73.6	81.8
Debiased (τ+ = 0.01)	76.2	82.9	86.9	93.7	89.1	74.7	82.7
Hard (β = 1, τ+ = 0)	77.1	82.5	87.0	92.9	89.2	73.9	82.2
Hard (β = 2, τ+ = 0)	77.4	83.6	86.8	93.4	88.7	73.5	82.0
Table 2: Classification accuracy on downstream tasks. Sentence representations are learned using
quick-thoughts (QT) vectors on the BookCorpus dataset and evaluated on six classification tasks.
Evaluation of binary classification tasks (MR, CR, SUBJ, MPQA) uses 10-fold cross validation.
6	A Closer Look at Hard Sampling
6.1	Are Harder Samples Necessarily Better?
By setting β to large values, one can focus on only the hardest samples in a training batch. But
is this desirable? Fig. 4 (left, middle) shows that for vision problems, taking larger β does not
necessarily lead to better representations. In contrast, when one uses true positive pairs during
training (green curve, uses label information for positive but not negative pairs), the downstream
performance monotonically increases with β until convergence (Fig. 4 , middle). Interestingly, this
is achieved without using label information for the negative pairs. This observation suggests an
explanation for why bigger β hurts performance in practice. Debiasing (conditioning on the event
{h(x) 6= h(x-)}) using the true p+ corrects for sampling x- with the same label as x. However,
since in practice we approximate p+ using a set of data transformations, we can only partially correct.
This is harmful for large β since this regime strongly prefers x- for which f(x-) is close to f (x),
many of whom will have the same label as x if not corrected for. We note also that by annealing β
(gradually decreasing β to 0 throughout training; see Appendix D.1 for details) it is possible to be
more robust to the choice of initial β, with marginal impact on downstream accuracy compared to the
best fixed value of β .
6.2	Does Avoiding False Negatives Improve Hard Sampling?
Our proposed hard negative sampling method conditions on the event {h(x) 6= h(x-)} in order to
avoid false negatives (termed “debiasing” (Chuang et al., 2020)). But does this help? To test this, we
train four embeddings: hard sampling with and without debiasing, and uniform sampling (β = 0)
with and without debiasing. The results in Fig. 4 (right) show that hard sampling with debiasing
obtains the highest linear readout accuracy on STL10, only using hard sampling or only debiasing
yields (in this case) similar accuracy. All improve over the SimCLR baseline.
8
Published as a conference paper at ICLR 2021
B (STLlO)
B (Cifarioo)
6u=dujes
PJeH
6u=dujes
pHsN
Debiased Not Debiased
87.44 %	84.41 %
84.26 %
80.15 %
匚 near ev-uat-on accuracy
Auu8nb3JH
Figure 4: Left: the effect of varying concentration parameter β on linear readout accuracy. Middle:
linear readout accuracy as concentration parameter β varyies, in the case of contrastive learning (fully
unsupervised), using true positive samples (uses label information), and an annealing method that
improves robustness to the choice of β (see Appendix D.1 for details). Right: STL10 linear readout
accuracy for hard sampling with and without debiasing, and non-hard sampling (β = 0) with and
without debiasing. Best results come from using both simultaneously.
not H & not D (SimCLR)
PoSitiVe
PairS
XousnbaiH
Cosine Similarity
Cosine Similarity
Cosine Similarity
Cosine Similarity
Figure 5: Histograms of cosine similarity of pairs of points with the same label (top) and different
labels (bottom) for embeddings trained on STL10 with four different objectives. H=Hard Sampling,
D=Debiasing. Histograms overlaid pairwise to allow for convenient comparison.
Fig. 5 compares the histograms of cosine similarities of positive and negative pairs for the four
learned representations. The representation trained with hard negatives and debiasing assigns much
lower similarity score to a pair of negative samples than other methods. On the other hand, the
SimCLR baseline assigns higher cosine similarity scores to pairs of positive samples. However, to
discriminate positive and negative pairs, a key property is the amount of overlap of positive and
negative histograms. Our hard sampling method achieves less overlap than SimCLR, by better trading
off higher dissimilarity of negative pairs with less similarity of positive pairs. Similar tradeoffs are
observed for the debiased objective, and hard sampling without debiasing.
6.3	How do Hard Negatives Affect Optimization?
Fig. 11 (in Appendix C due to space constraints) shows the performance on STL10 and CIFAR100
of SimCLR versus using hard negatives throughout training. We use weighted k-nearest neighbors
with k = 200 as the classifier and evaluate each model once every five epochs. Hard sampling with
β = 1 leads to much faster training: on STL10 hard sampling takes only 60 epochs to reach the same
performance as SimCLR does in 400 epochs. On CIFAR100 hard sampling takes only 125 epochs to
reach the same performance as SimCLR does in 400 epochs. We speculate that the speedup is, in
part, due to hard negatives providing non-negligible gradient information during training.
7	Conclusion
We argue for the value of hard negatives in unsupervised contrastive representation learning, and
introduce a simple hard negative sampling method. Our work connects two major lines of work:
contrastive learning, and negative mining in metric learning. Doing so requires overcoming an
apparent roadblock: negative mining in metric learning uses pairwise similarity information as a
core component, while contrastive learning is unsupervised. Our method enjoys several nice aspects:
having desirable theoretical properties, a very simple implementation that requires modifying only a
couple of lines of code, not changing anything about the data sampling pipeline, introducing zero
extra computational overhead, and handling false negatives in a principled way.
9
Published as a conference paper at ICLR 2021
References
Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saunshi.
A theoretical analysis of contrastive unsupervised representation learning. In Int. Conference on
Machine Learning (ICML),pp. 5628-5637, 2019.
Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing
mutual information across views. In Advances in Neural Information Processing Systems (NeurIPS),
pp. 15535-15545, 2019.
Avrim Blum and Tom Mitchell. Combining labeled and unlabeled data with co-training. In Proceed-
ings of the eleventh annual conference on Computational learning theory, pp. 92-100, 1998.
Bernhard E Boser, Isabelle M Guyon, and Vladimir N Vapnik. A training algorithm for optimal
margin classifiers. In Proceedings of the fifth annual workshop on Computational learning theory,
pp. 144-152, 1992.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In Int. Conference on Machine Learning (ICML), pp.
10709-10719, 2020a.
Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big
self-supervised models are strong semi-supervised learners. In Advances in Neural Information
Processing Systems (NeurIPS), 2020b.
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum
contrastive learning. arXiv:2003.04297, 2020c.
Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning a similarity metric discriminatively, with
application to face verification. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), volume 1, pp. 539-546, 2005.
Marthinus Christoffel, Gang Niu, and Masashi Sugiyama. Class-prior estimation for learning from
positive and unlabeled data. In Asian Conference on Machine Learning, pp. 221-236, 2016.
Ching-Yao Chuang, Joshua Robinson, Lin Yen-Chen, Antonio Torralba, and Stefanie Jegelka. De-
biased Contrastive Learning. In Advances in Neural Information Processing Systems (NeurIPS),
2020.
Corinna Cortes and Vladimir Vapnik. Support-Vector Networks. Machine learning, 20(3):273-297,
1995.
Bill Dolan, Chris Quirk, and Chris Brockett. Unsupervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Proceedings of the 20th international conference
on Computational Linguistics, pp. 350. Association for Computational Linguistics, 2004.
Marthinus C Du Plessis, Gang Niu, and Masashi Sugiyama. Analysis of learning from positive and
unlabeled data. In Advances in Neural Information Processing Systems (NeurIPS), pp. 703-711,
2014.
Charles Elkan and Keith Noto. Learning classifiers from only positive and unlabeled data. In ACM
SIGKDD international conference on Knowledge discovery and data mining, pp. 213-220, 2008.
Weifeng Ge. Deep metric learning with hierarchical triplet loss. In Europ. Conference on Computer
Vision (ECCV), pp. 269-285, 2018.
Michael GUtmann and AaPo Hyvarinen. Noise-Contrastive Estimation: A new estimation principle
for unnormalized statistical models. In Proc. Int. Conference on Artificial Intelligence and Statistics
(AISTATS), pp. 297-304, 2010.
Raia Hadsell, SUmit Chopra, and Yann LeCUn. Dimensionality redUction by learning an invariant
mapping. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1735-
1742, 2006.
10
Published as a conference paper at ICLR 2021
Ben Harwood, Vijay Kumar BG, Gustavo Carneiro, Ian Reid, and Tom Drummond. Smart mining
for deep metric learning. In Int. Conference on Computer Vision (ICCV), pp. 2821-2829, 2017.
Kaveh Hassani and Amir Hosein Khasahmadi. Contrastive multi-view representation learning on
graphs. In Int. Conference on Machine Learning (ICML), pp. 3451-3461, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
770-778, 2016.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 9729-9738, 2020.
Olivier J Henaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, SM Eslami, and
Aaron van den Oord. Data-efficient image recognition with contrastive predictive coding. In Int.
Conference on Machine Learning (ICML), pp. 6661-6671, 2020.
Minqing Hu and Bing Liu. Mining and summarizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 168-177,
2004.
Shantanu Jain, Martha White, and Predrag Radivojac. Estimating the class prior and posterior
from noisy positives and unlabeled data. In Advances in Neural Information Processing Systems
(NeurIPS), pp. 2693-2701, 2016.
SouYoung Jin, Aruni RoyChowdhury, Huaizu Jiang, Ashish Singh, Aditya Prasad, Deep Chakraborty,
and Erik Learned-Miller. Unsupervised hard example mining from videos for improved object
detection. In Europ. Conference on Computer Vision (ECCV), pp. 307-324, 2018.
Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane Larlus. Hard
negative mixing for contrastive learning. In Advances in Neural Information Processing Systems
(NeurIPS), 2020.
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron
Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. arXiv:2004.11362, 2020.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Int. Conf. on
Learning Representations (ICLR), 2015.
Ryan Kiros, Yukun Zhu, Russ R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. Skip-Thought Vectors. In Advances in Neural Information Processing Systems
(NeurIPS), pp. 3294-3302, 2015.
Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, and Pushmeet Kohli. Graph matching networks
for learning the similarity of graph structured objects. In Int. Conference on Machine Learning
(ICML), pp. 3835-3845, 2019.
Lajanugen Logeswaran and Honglak Lee. An efficient framework for learning sentence representa-
tions. In Int. Conf. on Learning Representations (ICLR), 2018.
K. V. Mardia and P. Jupp. Directional Statistics. John Wiley and Sons Ltd., second edition, 2000.
Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations.
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6707-6717, 2020.
Christopher Morris, Nils M Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion
Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. In Graph
Representation Learning and Beyond, ICML Workshop, 2020.
Oleg R Musin and Alexey S Tarasov. The tammes problem for n= 14. Experimental Mathematics, 24
(4):460-468, 2015.
11
Published as a conference paper at ICLR 2021
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: Training generative neural samplers
using variational divergence minimization. In Advances in Neural Information Processing Systems
(NeurIPS),pp. 271-279, 2016.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding. arXiv:1807.03748, 2018.
Bo Pang and Lillian Lee. A sentimental education: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of the 42nd annual meeting on Association for
Computational Linguistics, pp. 271. Association for Computational Linguistics, 2004.
Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization
with respect to rating scales. In Proceedings of the 43rd annual meeting on association for
computational linguistics, pp. 115-124. Association for Computational Linguistics, 2005.
Senthil Purushwalkam and Abhinav Gupta. Demystifying contrastive self-supervised learning:
Invariances, augmentations and dataset biases. arXiv:2007.13916, 2020.
Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face
recognition and clustering. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 815-823, 2015.
K Schutte and BL Van der Waerden. AUf Welcher kugel haben 5, 6, 7, 8 oder 9 PUnkte mit
mindestabstand eins platz? Mathematische Annalen, 123(1):96-124, 1951.
Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine HsU, Eric Jang, Stefan Schaal, Sergey
Levine, and Google Brain. Time-contrastive netWorks: Self-sUPervised learning from video. In
2018 IEEE International Conference on Robotics and Automation (ICRA), PP. 1134-1141, 2018.
HyUn Oh Song, YU Xiang, Stefanie Jegelka, and Silvio Savarese. DeeP metric learning via lifted
strUctUred featUre embedding. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), PP. 4004-4012, 2016.
Aravind Srinivas, Michael Laskin, and Pieter Abbeel. CURL: Contrastive UnsUPervised rePre-
sentations for reinforcement learning. In Int. Conference on Machine Learning (ICML), PP.
10360-10371, 2020.
YUmin SUh, BohyUng Han, Wonsik Kim, and KyoUng MU Lee. Stochastic class-based hard examPle
mining for deeP metric learning. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), PP. 7251-7259, 2019.
Fan-YUn SUn, Jordan Hoffmann, Vikas Verma, and Jian Tang. InfoGraPh: UnsUPervised and semi-
sUPervised graPh-level rePresentation learning via mUtUal information maximization. In Int. Conf.
on Learning Representations (ICLR), 2020.
Tristan Sylvain, Linda Petrini, and Devon Hjelm. Locality and comPositionality in zero-shot learning.
In Int. Conf. on Learning Representations (ICLR), 2020.
Pieter MerkUs LambertUs Tammes. On the origin of nUmber and arrangement of the Places of exit on
the surface of pollen-grains. Recueil des travaux botaniques neerlandais, 27(1):1-84, 1930.
Yonglong Tian, DiliP Krishnan, and PhilliP Isola. Contrastive MUltivieW Coding. In Europ. Confer-
ence on Computer Vision (ECCV), pp. 770-786, 2019.
Yonglong Tian, Chen SUn, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What
makes for good vieWs for contrastive learning? arXiv:2005.10243, 2020.
Petar Velickovic, William Fedus, William L Hamilton, Pietro Lid, Yoshua Bengio, and R Devon
Hjelm. Deep Graph Infomax. In Int. Conf. on Learning Representations (ICLR), 2019.
Ellen M Voorhees and Donna Harman. OvervieW of trec 2002. 2002.
Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through align-
ment and uniformity on the hypersphere. In Int. Conference on Machine Learning (ICML), pp.
9574-9584, 2020.
12
Published as a conference paper at ICLR 2021
Janyce Wiebe, Theresa Wilson, and Claire Cardie. Annotating expressions of opinions and emotions
in language. Language resources and evaluation, 39(2-3):165-210, 2005.
Chao-Yuan Wu, R Manmatha, Alexander J Smola, and Philipp Krahenbuhl. Sampling matters in
deep embedding learning. In Int. Conference on Computer Vision (ICCV), pp. 2840-2848, 2017.
Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv:1304.5634, 2013.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In Int. Conf. on Learning Representations (ICLR), 2019.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. In Int. Conf. on Learning Representations (ICLR), 2018.
13
Published as a conference paper at ICLR 2021
A Analysis of Hard Sampling
A. 1 Hard Sampling Interpolates Between Marginal and Worst-Case Negatives
We begin by proving Proposition 3. Recall that the proposition stated the following.
Proposition 6. Let L*(f)= supq∈Π L(f, q). Then for any t > 0 and measurable f : X → Sd-1/t
we observe the convergence L(f, q-) —→ L*(f) as β → ∞.
Proof. Consider the following essential supremum,
M(x) = ess sup f (x)Tf (x-) = sup{m > 0 : m ≥ f (x)Tf (x-) a.s. for X-〜p-}.
X- ∈X:X- ~x
The second inequality holds since supp(p) = X . We may rewrite
ef(X)Tf(X+)
L(f) = EXx[p+	- log ef(χ)Tf(χ+) + QeM(X),
-	ef(X)Tf(X+ )
L(f,qβ )= EX 二 p+ - log ef(x)T f(x+)+ QEχ--q- F(X)T f(x-)].
The difference between these two terms can be bounded as follows,
ef (X)T f (X+ )	ef (X)T f (X+ )
L(f) -Lfqe )1 ≤ EXx二p+ - log ef(X)Tf(X+) + QeM(X) + log ef(X)Tf∙+) + QEX-〜q- [ef(")Tf(X-)]
=E X〜P log (ef(X)Tf(X+) + QE - - [ef(X)Tf(X-力—log (ef(X)Tf(X+)+ QeMC
x+〜p+	∖	X 〜qβ	J	、
≤ e—— ∙ E X〜P ef(X)Tf(x+) + QE - - [ef(X)Tf(x-)] — ef(X)Tf(x+) — QeM(X)
_ Q + 1	X+〜p+	X f
=Q+Q ∙ Ex~p ∣Eχ-〜q- [ef(X)Tf(X-)] - eM(X)
≤ e1" ∙ EX〜PEX-〜q- kM(X)- ef(X)Tf(x-)|
where for the second inequality we have used the fact that f lies on the hypersphere of radius 1/t to
restrict the domain of the logarithm to values greater than (Q+ 1)e-1/t. Because of this the logarithm
is Lipschitz with parameter e1/t/(Q + 1). Using again the fact that f lies on the hypersphere we
know that |f (x)T f (x-)| ≤ 1/t2 and hence have the following inequality,
EX〜PEq-IeM(X)- ef(X)Tf(x-)| ≤ e1"2EX〜PEq-IM(x) - f (x)Tf(x-)∣
Let us consider the inner expectation Eβ(x) = Eq- |M (x) - f(x)Tf(x-)|. Note that since f is
bounded, Eβ (x) is uniformly bounded in x. Therefore, in order to show the convergence L(f, qβ-) →
L*(f) as β → ∞, it suffices by the dominated convergence theorem to show that Ee(x) → 0
pointwise as β → ∞ for arbitrary fixed x ∈ X .
From now on we denote M = M(x) for brevity, and consider a fixed x ∈ X. From the definition of
q- it is clear that q-《p-. That is, since q- = C ∙ P- for some (non-constant) c, it is absolutely
continuous with respect to p-. So M(χ) ≥ f (X)Tf (χ-) almost surely for X-〜q-, and We
14
Published as a conference paper at ICLR 2021
may therefore drop the absolute value signs from our expectation. Define the following event
Gε = {x- : f(x)>f(x-) ≥ M - ε} where G is refers to a “good” event. Define its complement
Bε = Gεc where B is for “bad”. For a fixed x ∈ X and ε > 0 consider,
Ee (X) = Ex-~q-|M (X)- f (X)T f (XF
=Pχ-~q- (Gε) ∙ Eχ-~q- ]∣M(X)- f(x)Tf(x-)∣∣Gε] + Pχ-~q- (Bε) ∙ Eχ-~q- [∣M(x) - f(x)Tf(χ-)∣∣Bε
≤ Pχ-~q- (Gε) ∙ ε + 2Pχ-~q- (Bε)
≤ ε + 2Px-~q- (Bε).
We need to control Pχ-~q- (Bε). Expanding,
Px-~q-(Bε )
f(X)Tf(X-) < M(X) - ε
eβf(x)Tf(X-) ∙ p-(X-)

Zβ
where Zβ = RX eβf(x)Tf(x-)p-(X-)dX- is the partition function of qβ-. We may bound this
expression by,
f(X)Tf(X-) < M-ε
eβ(M-ε) ∙ p-(X-)
Zβ
dX- ≤
eβ(M-ε)
Zβ
eβ(M-ε)
< M - ε p- (X-)dX-
Pχ-~p-(Bε)
Zβ
eβ(M-ε)
≤ —-——
Zβ
Note that
Ze = Z eef(X)Tf(χ-)p-(X-)dX- ≥ ee(M-ε∕^Pχ-~p- (f (x)tf (x-) ≥ M - ε∕2).
X
By the definition of M = M(x) the probability ρε = Pχ-~p- (f (X)T f (x-) ≥ M - ε∕2) > 0, and
we may therefore bound,
eβ(M-ε)
Pχ-~q- (Bε = eβ(M-ε∕2)ρε
=e-βε/2 /Pε
-→ 0 as β → ∞.
We may therefore take β to be sufficiently big so as to make Pχ-~q- (Bε) ≤ ε and therefore
Ee(x) ≤ 3ε. In other words, Ee(x) —→ 0 as β → ∞.	口
A.2 Optimal Embeddings on the Hypersphere for Worst-Case Negative Samples
In order to study properties of global optima of the contrastive objective using the adversarial worst
case hard sampling distribution recall that we have the following limiting objective,
ef(x)Tf(x+)
L∞(f，q) = Exx2 Tog Eχ-〜qβ [ef(x)Tf(x-)]
(8)
We may separate the logarithm of a quotient into the sum of two terms plus a constant,
L∞(f, q) = Lalign(f) + Lunif(f, q) - 1/t2
15
Published as a conference paper at ICLR 2021
where Lalign(f) = Eχ,x+ kf(x) - f(x+)k2∕2 and LUnif(f,q) = Eχ~plogEχ-^qef(x)>f(x-). Here
we have used the fact that f lies on the boundary of the hypersphere of radius 1/t, which gives us the
following eqUivalence between inner prodUcts and sqUared EUclidean norm,
2∕t2 - 2f(x)>f(x+) = kf(x)k2 + kf(x+)k2 - 2f(x)>f(x+) = kf(x) - f(x+)k2.	(9)
Taking supremum to obtain L∞ (f) = supq∈∏ L∞ (f, q) We find that the second expression simplifies
to,
LUnif(f) = supLunif(f, q) = Eχ~p log Sup ef(x)>f(x ) = Eχ~p sup f(x)>f(x-).
q∈Π	x-x	x-x
Using Eqn. (9), this can be re-expressed as,
Ex~p sup f(x)>f(x-) = -Eχ~p inf kf (X)- f (x-)k2∕2 + 1∕t2.	(10)
x- x	x- x
The forthcoming theorem exactly characterizes the global optima of minf L∞(f)
Theorem 7.	Suppose the downstream task is classification (i.e. C is finite), and let L∞(f)=
supq∈∏ L∞(f, q). The infimum inff： measurable L∞(f) is attained, and any f * achieving the global
minimum is such that f *(x) = f *(x+) almost surely. Furthermore, letting Vc = f *(x) for any X
such that h(x) = c (so vc is well defined up to a set ofx of measure zero), f* is characterized as
being any solution to the following ball-packing problem,
r max	P(C) ∙ min kvc- vc0k2.
{vc∈S	"}c∈C C∈C	c=c
(11)
Proof. Any minimizer of Lalign (f) has the property that f(X) = f(X+) almost surely. So, in order
to prove the first claim, it suffices to show that there exist functions f ∈ arg inff Lu*nif(f) for which
f(X) = f(X+) almost surely. This is because, at that point, we have shown that arg minf Lalign (f)
and arg minf Lu*nif(f) intersect, and therefore any solution of L*∞(f) = Lalign (f) + Lu*nif(f) must
lie in this intersection.
To this end, suppose that f ∈ arg minf Lu*nif(f) but that f(X) 6= f(X+) with non-zero probability.
We shall show that we can construct a new embedding f such that f(X) = f(X+) almost surely, and
Lu*nif (f) ≤ Lu*nif(f). Due to Eqn. (10) this last condition is equivalent to showing,
Eχ~p inf k/(X)- f(x-)k2 ≥ Eχ~p inf kf (X)- f(x-)k2.	(12)
x-x	x-x
Fix a c ∈ C, and let Xc ∈ arg maxx:h(x)=c infx-x kf (X) - f(X-)k2. The maximum is guaranteed
to be attained, as we explain now. Indeed we know the maximum is attained at some point in
the closure ∂{X : h(X) = c} ∪ {X : h(X) = c}. Since X is compact and connected, any point
X ∈ ∂{x : h(x) = c}∖{x : h(x) = c} is such that infx-四比 kf (x) — f (x-)k2 = 0 since X must
belong to {x : h(∕) = c0} for some other c0. Such an X cannot be a solution unless all points in
{X : h(X) = c} also achieve 0, in which case we can simply take Xc to be a point in the interior of
{X : h(X) = c}.
Now, define f(X) = f(Xc) for any X such that h(X) = c and f(X) = f(X) otherwise. Let us first aim
to show that Eqn. (12) holds for this f. Let us begin to expand the left hand side of Eqn. (12),
16
Published as a conference paper at ICLR 2021
Ex〜P inf k/(X)- f(x-)k2
x-x
=Ec〜PEx〜p(∙∣c) inf k∕(X)- f(x-)k2
x-x
=P(C)Ex〜p(∙∣c) inf k∕(X)- f(x-)k,2
x-x
+ (1- P(C))Ec〜ρ(∙∣c=c)Ex〜p(∙∣c) inf k∕(X)- /(x-)k2
x-x
=P(C)Ex〜p(∙∣c) inf kf(Xc)- f(x-)k2
x-x
+ (1- P(C))Ec〜ρ(∙∣c=c)Ex〜p(∙∣c) inf k∕(X)- f‰-)k2
x-x
= P(C) inf kf(Xc) - f(X-)k2
x- xc
+ (1- P(C))Ec〜ρ(∙∣c=c)Ex〜p(∙∣c) ,infι k∕(X)- /(x-)∣∣2	(13)
h(x- )=c
By construction, the first term can be lower bounded by infx-xc kf (Xc) - f (X-)k2 ≥
Ex〜p(∙∣c) infh(x-)=c Ilf (x) - f (x-)∣∣2 for any X SUch that h(∕) = c. To lower bound the sec-
ond term, consider any fixed C = C and X 〜p(∙∣^) (so h(∕) = ^). Define the following two subsets
of the input space X
A = {f (x ) : f (x ) = c for X ∈ X} A = {f (x ) ∈ X : f(X ) = C for X ∈ X}.
know that Ab ⊆ A. Combining
Since by construction the range of f is a subset of the range of f, we
this with the fact that f (χ) = f (χ) whenever h(x) = c = C we see,
inf Jf(X)- f(x-)k2 = inf kf(X)- f(x-)k2
h(x-)=c	h(x-) = c
= inf kf (X) - uk2
u∈A
≥ inf kf (X) - uk2
u∈A
= inf kf(X) - f(X-)k2
h(x- ) = c
Using these two lower bounds we may conclude that Eqn. (13) can be lower bounded by,
P(C)Ex〜p(∙∣c)	inf kf(x) - f(χ-)k2 + (I-P(C))Ec〜ρ(∙∣c=c)Ex〜p(∙∣c) JnflJf(X)- f(χ-)k2
h(x- )=c	h(x-)=c
which equals Ex〜P infx-^x kf (χ) - f (χ-)∣2. We have therefore proved Eqn. (12). To summarize
the current progress; given an embedding f we have constructed a new embedding f that attains
lower Lunif loss and which is constant on X such that f is constant on {X : h(X) = C}. Enumerating
C = {C1, C2 . . . , C|C| }, we may repeatedly apply the same argument to construct a sequence of
embeddings f1, f2, . . . , f|C| such that fi is constant on each of the following sets {X : h(X) = Cj} for
j ≤ i. The final embedding in the sequence f * = fc∣ is such that Lunif(f *) ≤ Lunif(f) and therefore
f * is a minimizer. This embedding is constant on each of {χ : h(χ) = Cj} for j = 1,2,...,∣C∣. In
other words, f* (X) = f* (X+) almost surely. We have proved the first claim.
Obtaining the second claim is a matter of manipulating L*∞(f*). Indeed, we know that L*∞(f*) =
Lu*nif (f *) - 1/t2 and defining vc = f* (X) = f(Xc) for each C ∈ C, this expression is minimized if
and only if f * attains,
17
Published as a conference paper at ICLR 2021
mqxEx〜P inf ∣∣f(x) - f(x-)k2 = mqxEc〜PEx〜Pac)	inf	∣∣f(x) - f(x-)k2
f	x- x	f	h(x- )6=c
= maxXP(C)，“infJkf(X)- f(X-)k2
f c∈C	h(x )6=c
=r max	P(C) ∙ min Ilvc - vc0k2
{Vc∈Sd-1∕t}ceC z-<	c0 = c
c∈C
where the final equality inserts f * as an optimal f and reparameterizes the maximum to be over the
set of vectors {vc ∈ Sd-1∕t}c∈c.	□
A.3 Downstream Generalization
Theorem 8.	Suppose P is uniform on C and f is such that L∞(f) 一 inff measurable L∞(f) ≤
ε with ε ≤ 1. Let {v* ∈ Sd-1∕t}c∈c be a SoIUtion to Problem 7, and define ξ =
minc,c-：c=c-J] v* — vC-U > 0. Then there exists a set of vectors {vc ∈ Sd-1∕t}c∈c such that
the following 1-nearest neighbor classifier,
h(x) = C,	where C = arg min J f (x) — vj∣ (ties broken arbitrarily)
achieves misclassification risk,
8ε
((X) = C) ≤ (ξ2 - 2|C| (1 + 1∕t)ε1/2)2
Proof. To begin, using the definition of h we know that for any 0 < δ < ξ,
Px,c(h(x) = C)= Px,c (∣∣f (x) - vc∣∣ ≤ min ∣∣f (x) - Vc-II)
c- :c- 6=c
≥ Px,c ∣∣f (X) - vc∣∣ ≤ δ, and δ ≤ min ∣∣f (X) - vc- ∣∣
c- :c- 6=c
≥ 1 - Px,c (∣∣f(x) - vc∣∣ > δ) - Px,c( min ∣∣f (x) - vc-∣∣<δ)
c- :c- 6=c
So to prove the result, our goal is now to bound these two probabilities. To do so, we use the bound
on the excess risk. Indeed, combining the fact L∞ (f) - inf j measurable L∞ (f) ≤ ε with the notational
rearrangements before Theorem 7 we observe that Ex,x+ ∣∣f (X) - f(X+)∣∣2 ≤ 2ε.
We have,
2ε ≥ Ex,x+ ∣∣f (x) - f (x+)∣∣2 = Ec〜PEx+〜p(∙∣c)Ex〜p(∙∣c)∣∣f(x) - f (x+)∣∣2.
For fixed c,x+, let Xc ∈ argmin{x+：h(x+)=c} Ex〜p(.|c)∣∣f (x) - f (x+)∣∣2 where we extend the
minimum to be over the closure, a compact set, to guarantee it is attained. Then we have
2ε ≥ Ec~ρEx+〜p(∙∣c)Ex〜p(∙∣c) ∣∣f(x) - f(x+)∣∣ ≥ Ec〜PEx〜p(∙∣c) ∣∣f(x) - vc∣∣
where we have now defined vc = f(Xc ) for each C ∈ C. Note in particular that vc lies on the surface
of the hypersphere Sd-1/t. This enables us to obtain the follow bound using Markov’s inequality,
18
Published as a conference paper at ICLR 2021
Pχ,c (Ilf(X)-VCIl > s) = Pχ,c (∣∣∕(χ) - vc∣∣2 > δ2)
Ex,c∖∖f (x) - vc∣∣2
δ2
≤ fε.
so it remains still to bound Px,c(min/：C-=C 11 f (x) - Vc-11	< δ). Defining
mi□c,c-:C=C-IlVC - Vc- k, We have the following fact (proven later).
Fact (see lemma 9): W ≥ ,ξ2 - 2|C| (1 + 1∕t)√ε.
Using this fact we are able to get control over the tail probability as follows,
%c (LminJ∣f(χ) - Vc-1∣ < s) ≤ 叱,° (∣ ∣f ⑵-v0∣∣ > w - s)
≤ Pχ,c (∣∣f (x) - Vc∣∣ >ξ - √ξ2 - 2|C| (1 + 1∕t)ει/2 - δ)
=Pχ,c (∣∣ f(X) - Vc∣∣2 > (√ξ2 - 2|C| (1 + 1∕t)ει/2 - δ)2)
2ε
≤ (√ξ2 - 2|C| (1 + 1∕t)ε1/2 - δ)2 .
where this inequality holds for for any 0 ≤ δ ≤ ,ξ2 - 2|C| (1 + 1∕t)ε1/2.
Gathering together our tail probability bounds we find that Px,c(h(x) = C) ≥ 1 - ∙∣f -
(√ξ2-2∣C∣(1+l∕绢∕I)2 forany 0 ≤ δ ≤ √ξ2- 2|CI(I + 1∕∙ε1∕2. That is,
2ε	2ε
x,c 丰 -δ2 + (√ξ2 - 2|C| (1 + 1∕t)ει/2 - δ)2
Since this holds for any 0 ≤ δ ≤ √ξ2 - 2|C| (1 + 1∕t)ε1/2,
Pχc(h(x) = C) ≤ min < ^ɪ +---------------/	麦	------\.
,	0≤δ≤ 尸订 Iδ2	(√ξ2 - 2|C| (1 + 1∕t)ε1/2 - δ)2∫
Elementary calculus shows that the minimum is attained at δ = ʌʌ -ZCIa+i/.e / . PIUgging this in
yields the final bound,
8ε
((X) = C) ≤ (ξ2 - 2|C| (1 + 1∕t)ε1/2)2 .
□
Lemma 9. Consider the same setting as introduced in Theorem 5. In particular define
ξ0 = min I∣vc - VC-Il,	ξ = min ∣ H - v*- 11.
c,c- :c=c-	c,c- :c=c-
where (v* ∈ Sd-1∕t}c∈c is a solution to Problem 7, and (vc ∈ SdT∕t}c∈c is defined via VC
f (Xc) with XC ∈ arg min{x+：h(x+)=c} Ex〜4⑹ ∣ ∣ f (x) — f(x+) ∣ ∣ for each C ∈ C. Then we have,
ξ0 ≥ √ξ2 - 2|C| (1 + 1∕t)ει/2.
19
Published as a conference paper at ICLR 2021
Proof. Define,
X = Ain ∣∣vc - Vc-『,	X* = Ain ∣∣v* - v"2 .
c-: C-=C	c- :c- =c
X and X * are random due to the randomness of C 〜ρ. We can split up the following expectation by
conditioning on the event {X ≤ X*} and its complement,
E|X - X*| = P(X ≥ X*)E[X - X*] + P(X ≤ X*)E[X* - X].	(14)
Using L∞ (f)-inf 于 measurable L∞ (f) ≤ ε and the notational re-writing of the objective L∞ introduced
before Theorem 7, we observe the following fact, whose proof we give in a separate lemma after the
conclusion of this proof.
Fact (see lemma 10): EX* — 2(1 + 1∕t)√ε ≤ EX ≤ EX*.
This fact implies in particular E[X - X*] ≤ 0 and E[X* - X] ≤ 2(1 + 1∕t)√ε. Inserting both
inequalities into Eqn. 14 we find that E|X - X*| ≤ 2(1 + 1∕t)√ε. In other words, since P is
uniform,
由 X c-*CkVC-VC-『-Ccllv* - v*- Il2 ≤ 2(1 W
From which we can say that for any C ∈ C ,
min ∣∣vC - VC-『-min ∣∣v* - v*-∣∣2 ≤ 2|C|(1 + 1/t)V?.
C-: C-=C	C- :C-=C
So min。-：C-=CkVC - v°-∣	≥	Jmin°-:C- =C∣∣vC - v*-∣∣2 - 2|C| (1 + 1∕t)εi/2	≥
,ξ2 - 2|C| (1 + 1∕t)ε1/2.	Since this holds for any C ∈ C , we conclude that
E ≥ √ξ2 - 2|C| (1 + 1∕t)ε1/2.	□
Lemma 10. Consider the same setting as introduced in Theorem 5. Define also,
X = Jmin ∣∣vC- VC-『,	X* = JminJ∣v* - vC-1∣2,
C- :C- =C	C- :C- =C
where VC = f (xc) With XC ∈ argmin{x+：h(x+)=C} Ex〜4⑹ ∣∣f (x) - f (x+)∣∣ for each C ∈ C.We
have,
EX* - 2(1 + 1∕t)√7 ≤ EX ≤ EX*.
Proof. By Theorem 7 we know there is an f * attaining the minimum inf f measurable L∞ (/) and that
this f * attains L*lign(f *) = 0, and also minimizes the uniformity term LUnif(f), taking the value
LUnif(f *) = EC〜P maxc-：C-=C v^τv*-. Because of this we find,
LUnif(f) ≤ (L∞ (f) - L∞(f *)) + (L%n(f *) - L%n(f)) + Lf *)
≤ (L∞(f)-L∞(f*)) + LUmf(f*)
≤ ε + LUiɪif(f*)
=ε + EC〜P max VcTV*-.
C- :C- =C
20
Published as a conference paper at ICLR 2021
Since We would like to bound Ec~ρ maxc-：c-=c Vc>Vc- in terms of Ec~ρ maxc-：c-=c v%[v；,
this observation means that is suffices to bound Ec~ρ maxc-：。-=。vjvc- in terms of £；曲(f). To
this end, note that for a fixed c, and x such that h(x) = c we have,
SUp f(x)>f(x-)= SUp {vjf(x-) + (f(x) - Vc)>f(x-)}
x-x	x-x
= SUp vc>f(x-) - f (x) - vc /t
x-x
≥ χ-∈maχc∈cvc>f (X-Hf(X)-vj/t
= max vc>vc- -f(x) - vc /t
where the inequality follows since {Xc}c∈C is a subset of the closure of {X- : X-	X}. Taking
expectations over c, X,
Lunif(f) = Eχ,c SUp f(x)>f(x-)
x-x
≥ Ec~ρ max Vc> Vc-
c- 6=c
≥ Ec~ρ max vc> vc-
c- 6=c
≥ Ec~ρ max Vc> vc-
c- 6=c
-	Ex,cf(X) - vc /t
-	qEχ,c∣∣f (χ) - vc∣∣2 /t
-	√ε∕t.
So since ε ≤ √ε, We have found that
Ec~ρ max vc>vc- ≤ √ε∕t + ε + Ec~ρ max vC>v*- ≤ (1 + 1∕t)√ε + Ec~ρ max vC>v*-.
c- 6=c	c- :c- 6=c	c- :c- 6=c
Of course We also have,
Ec~ρ max vC>v*- = Lunif(f*) ≤ Ec~ρ max vjvc-
c- :c- 6=c	c- :c- 6=c
since the embedding f(X) = vc Whenever h(X) = c is also a feasible solution. Combining these tWo
inequalities With the simple identity x>y = 1/t2 -kx - yk2 /2 for all length 1/t vectors x, y, We
find,
1/t2 -	Ec~ρ	max ∣∣R - vC-∣∣2	/2	≤	1/t2 -	Ec~ρ	max	∣∣vc	- Vc-	∣∣2 /2
c- :c- 6=c	c- :c- 6=c
≤ 1/t2 - Ec~ρ max ∣∣vC - v] ∣∣2 /2+(1 + 1/t)√ε.
c- :c- 6=c
Subtracting 1/t2 and multiplying by -2 yields the result.
□
B Graph Representation Learning
We describe in detail the hard sampling method for graphs Whose results are reported in Section 5.2.
Before getting that point, in the interests of completeness We cover some required background details
on the InfoGraph method of Sun et al. (2020). For further information see the original paper (Sun
et al., 2020).
21
Published as a conference paper at ICLR 2021
B.1	Background on Graph Representations
We observe a set of graphs G = {Gj ∈ G}jn=1 sampled according to a distribution p over an ambient
graph space G. Each node u in a graph G is assumed to have features h(u0) living in some Euclidean
space. We consider a K-layer graph neural network, whose k-th layer iteratively computes updated
embeddings for each node v ∈ G in the following way,
hVk)= COMBINE(k) IhvkT), AGGREGATE(k) ({ (h,kT),%kT),e“v) :u ∈N (v)} )J
where COMBINE(k) and AGGREGATE(k) are parameterized learnable functions and N(v) denotes
the set of neighboring nodes of v. The K embeddings for a node u are collected together to
obtain a single final summary embedding for u. As recommended by Xu et al. (2019) we use
concatenation, hu = hu(G) = CONCAT {h(uk)}kK=1 to obtain an embedding in Rd. Finally, the
node representations are combined together into a length d graph level embedding using a readout
function,
H(G) = READOUT {hu}u∈G
which is typically taken to be a simple permutation invariant function such as the sum or mean. The
InfoGraph method aims to maximize the mutual information between the graph level embedding
H(G) and patch-level embeddings hu(G) using the following objective,
maxEg~piG X I (hu(G); H(G))
|G| u∈G
In practice the population distribution p is replaced by its empirical counterpart, and the mutual
information I is replaced by a variational approximation IT . In line with Sun et al. (2020) we use the
Jensen-Shannon mutual information estimator as formulated by Nowozin et al. (2016). It is defined
using a neural network discriminator T : R2d → R as,
IT (hu(G); H(G)) = EG~p /sp(-T (hu(G),H(G)))] -E(G,G')~p×p 1P(T (hu(G),H(G0)))]
where sp(z) = log(1+ez) denotes the softplus function. The finial objective is the joint maximization
over h and T ,
maxEG ɪ X IT (hu(G); H(G))
θ,ψ	iGi u∈G
B.2	Hard Negative Sampling for Learning Graph Representations
In order to derive a simple modification of the NCE hard sampling technique that is appropriate for
use with InfoGraph, we first provide a mildly generalized view of hard sampling. Recall that the
NCE contrastive objective can be decomposed into two constituent pieces,
L(f, q) = Lalign(f) + Lunif(f, q)
where q is in fact a family of distributions q(x-; x) over x- that is indexed by the possible values of
the anchor x. Lalign performs the role of “aligning” positive pairs (embedding near to one-another),
while Lunif repels negative pairs. The hard sampling framework aims to solve,
22
Published as a conference paper at ICLR 2021
inf sup L(f, q).
fq
In the case of NCE loss we take,
Lalign(f) = -E X〜P f(x)Tf(x+ ),
J	X+〜p+
Lunif(f,q)= E X 〜P log {ef(x)T f (x+) + QEX-F [ef (x)T f (x-)
x+〜p+
View this view, we can easily adapt to the InfoGraph framework, taking
Lalign(h,T) = -Eg?。X sp(-T (hu(G),H(G))),
|G| u∈G
LUnif(h,T,q) = -Eg?⅛ X Eg，〜qSP(T (hu(G),H(G')))
|G| u∈G
Denote by p^ the distribution over nodes U ∈ RS defined by first sampling G 〜p, then sampling
u ∈ G uniformly over all nodes of G. Then these two terms can be simplified to
Laiign(h,T) = -Eu〜psp(-T (hu(G),H(G))),
LUnif(h,T, q) = -E(u,G0)〜p×qsp(T (hu(G),H(G0)))
At this point it becomes clear that, just as with NCE, a distribution q* ∈ arg maxq L(f, q) in the
InfoGraph framework if it is supported on arg maxG0 ∈G sp(T hu(G), H(G0) ). Although this is
still hard to compute exactly, it can be approximated by,
qβ(G0) (X exp(βT(hu(G),H(G))) ∙ p(G0).
C Additional Experiments
C.1 Hard negatives with large batch sizes
The vision experiments in the main body of the
paper are all based off the SimCLR framework
(Chen et al., 2020a). They use a relatively small
batch size (up to 512). In order to test whether our
hard negatives sampling method can help when the
negative batch size is very large, we also run ex-
periments using MoCo-v2 with standard negative
memory bank size N = 65536 (He et al., 2020;
Chen et al., 2020c). We adopt the official MoCo-
v2 code2. Embeddings are trained for 200 epochs,
with batch size 128. Figure 6 summarizes the re-
sults. We find that hard negative sampling can still
improve the generalization of embeddings trained
on CIFAR10: MoCo-v2 attains linear readout accu-
racy of 88.08%, and MoCo-v2 with hard negatives
(β = 0.2, τ+ = 0) attains 88.47%.
C.2 Ablations
88.2
88.1
<
rH
O
tD 88.4
L-
n
U 88.3
β (CIFAR10)
Figure 6: Hard negative sampling using MoCo-
v2 framework. Results show that hard negative
samples can still be useful when the negative
memory bank is very large (in this case N =
65536).
2https://github.com/facebookresearch/moco
23
Published as a conference paper at ICLR 2021
To study the affect of varying the concentration
parameter β on the learned embeddings Figure 9
plots cosine similarity histograms of pairs of sim-
ilar and dissimilar points. The results show that
for β moving from 0 through 0.5 to 2 causes both
the positive and negative similarities to gradually
skew left. In terms of downstream classification, an
important property is the relative difference in sim-
ilarity between positive and negative pairs. In this
case β = 0.5 find the best balance (since it achieves
the highest downstream accuracy). When β is taken
very large (β = 6), we see a change in conditions.
Both positive and negative pairs are assigned higher
similarities in general. Visually it seems that the
positive and negative histograms for β = 6 overlap
a lot more than for smaller values, which helps ex-
plain why the linear readout accuracy is lower for
β=6.
β (CIFAR10)
Figure 7: The effect of varying concentration
parameter β on linear readout accuracy for CI-
FAR10. (Complements the left and middle plot
from Figure 4.)
6=6, acc=65.54%
B = O, acc=67.73% Ml β = 0.5, acc=69.61%	B = 2, acc=67.68%
Figure 12 gives real examples of hard vs. uniformly sampled negatives. Given an anchor x (a
monkey) and trained embedding f (trained on STL10 using standard SimCLR for 400 epochs), we
sample a batch of 128 images. The top row shows the ten negatives x- that have the largest inner
product f(x)>f(x-), while the bottom row is a random sample from from the same batch. Negatives
with the largest inner product with the anchor correspond to the items in the batch are the most
important terms in the objective since they are given the highest weighting by qβ-. Figure 12 shows
that “real” hard negatives are conceptually similar to the idea as proposed in Figure 1: hard negatives
are semantically similar to the anchor, possessing various similarities, including color (browns and
greens), texture (fur), and objects (animals vs machinery).
NegatlVe
PairS
0Λ 04	— 1Λ
Cosine Similarity
Figure 8: Histograms of cosine similarity of pairs of points with different label (bottom) and same
label (top) for embeddings trained on CIFAR100 with different values of β . Histograms overlaid
pairwise to allow for easy comparison.
XousnbaiH
H & not D	not H & D
not H & not D (SimCLR)
NegatlVe
6-42-
0.0	0.5	l：0 - 0.0	0.5	l：0 ^ 0：0	0：5	l：0 - 0.0	0.5	l：0 ^ θJθ θV l：0 - 0：0	。：5
Cosine Similarity Cosine Similarity Cosine Similarity Cosine Similarity Cosine Similarity Cosine Similarity
Figure 9: Histograms of cosine similarity of pairs of points with the same label (top) and different
labels (bottom) for embeddings trained on CIFAR100 with four different objectives. H=Hard
Sampling, D=Debiasing. Histograms overlaid pairwise to allow for convenient comparison.
24
Published as a conference paper at ICLR 2021
not H & not D (SimCLR)
H & D	H & not D	not H & D
Figure 10: Histograms of cosine similarity of pairs of points with the same label (top) and different
labels (bottom) for embeddings trained on CIFAR10 with four different objectives. H=Hard Sampling,
D=Debiasing. Histograms overlaid pairwise to allow for convenient comparison.
0.0	0：5	l：0 0：0	0.5	l：0 0：0	0：5	1.0 0.0	0.5	1.0 0.0
Cosine Similarity Cosine Similarity Cosine Similarity Cosine Similarity Cosine Similarity Cosine Similarity
POSItIVe NegatIVe
PairS PairS
>U2⊃UU<
SimCLR
Hard (β = 1.0,τ+ =0.05)
Hard (β = 0.5,τ+ =0.05)

O
Ooooo
8 7 6 5 4
AUe.InUUEnOPeBa
50 IOO 150 200 250 300 350 400
Epochs (STLlO)
Figure 11: Hard sampling takes much fewer epochs to reach the same accuracy as SimCLR does in
400 epochs; for STL10 with β = 1 it takes only 60 epochs, and on CIFAR100 it takes 125 epochs
(also with β = 1).
O 50 IOO 150 200 250 300 350 400
Epochs (ClFARlOO)
D	Experimental Details
Figure 13 shows PyTorch-style pseudocode for the standard objective, the debiased objective, and the
hard sampling objective. The proposed hard-sample loss is very simple to implement, requiring only
two extra lines of code compared to the standard objective.
D. 1 Visual Representations
We implement SimCLR in PyTorch. We use a ResNet-50 (He et al., 2016) as the backbone with
embedding dimension 2048 (the representation used for linear readout), and projection head into the
lower 128-dimensional space (the embedding used in the contrastive objective). We use the Adam
optimizer (Kingma & Ba, 2015) with learning rate 0.001 and weight decay 10-6. Code available at
https://github.com/joshr17/HCL. Since we adopt the SimCLR framework, the number
of negative samples N = 2(batch size - 1). Since we always take the batch size to be a power of 2
(16, 32, 64, 128, 256) the negative batch sizes are 30, 62, 126, 254, 510 respectively. Unless otherwise
stated, all models are trained for 400 epochs.
Annealing β Method: We detail the annealing method whose results are given in Figure 4. The
idea is to reduce the concentration parameter down to zero as training progresses. Specifically,
suppose we have e number of total training epochs. We also specify a number ` of “changes” to the
concentration parameter we shall make. We initialize the concentration parameter β1 = β (where
this β is the number reported in Figure 4), then once every e/' epochs We reduce βi by β/'. In
other words, if we are currently on βi, then βi+ι = βi - β/', and we switch from βi to βi+ι in
epoch number i ∙ e/'. The idea of this method is to select particularly difficult negative samples early
on order to obtain useful gradient information early on, but later (once the embedding is already
quite good) we reduce the “hardness” level so as to reduce the harmful effect of only approximately
correcting for false negatives (negatives with the same labels as the anchor).
25
Published as a conference paper at ICLR 2021
Figure 12: Qualitative comparison of hard negatives and uniformly sampled negatives for embedding
trained on STL10 for 400 epochs using SimCLR. Top row: selecting the 10 images with highest
inner product with anchor in latent space from a batch of 128 inputs. Bottom row: a set of random
samples from the same batch. Hard negatives are semantically much more similar to the anchor than
uniformly sampled negatives - hard negatives possess many similar characteristics to the anchor,
including texture, colors, animals vs machinery.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
# Pos
# neg
#N
#t
# tau_plus
# beta
exp Of inner products for positive examples
exp of inner products for negative examples
number of negative examples
temperature scaling
class probability
concentration parameter
#Original objective
standard_loss = -log(pos.sum() / (pos.sum() + neg.sum()))
#Debiased objective
Neg = max((-N*tau_plus*pos + neg).sum() / (1-tau_plus), e**(-1/t))
debiased_loss = -log(pos.sum() / (pos.sum() + Neg))
#Hard sampling objective (Ours)
reweight = (beta*neg) / neg.mean()
Neg = max((-N*tau_plus*pos + reweight*neg).sum() / (1-tau_plus), e**(-1/t))
hard_loss = -log( pos.sum() / (pos.sum() + Neg))
Figure 13:	Pseudocode for our proposed new hard sample objective, as well as the original NCE
contrastive objective, and debiased contrastive objective. In each case we take the number of positive
samples to be M = 1. The implementation of our hard sampling method only requires two additional
lines of code compared to the standard objective.
We also found the annealing in the opposite direction (“down”) achieved similar performance.
Bias-variance of empirical estimates in hard-negative objective: Recall the final hard negative
samples objective we derive is,
ef(x)Tf(x+)
E X〜P	— log-----------；-----K------------------------------------------------
X+〜p+ [	ef(X)Tf(X+) + M(Ex-〜qβ [ef(X)Tf(X-)] — τ+Ev_q+ [ef(X)Tf(V)D
(15)
This objective admits a practical counterpart by using empirical approximations to
Ex-〜qβ [ef (X) f(X )] and Ev〜q+ [ef (X) f(v)]. In practice we use a fairly large number of samples
(e.g. N = 510) to approximate the first expectation, and only M = 1 samples to approximate the
second. Clearly in both cases the resulting estimator is unbiased. Further, since the first expectation
is approximated using many samples, and the integrand is bounded, the resulting estimator is well
concentrated (e.g. apply Hoeffding’s inequality out-of-the-box). But what about the second expec-
tation? This might seem uncontrolled since we use only one sample, however it turns out that the
random variable X = ef (X)T f (V) where X 〜P and V 〜qi+ has variance that is bounded by Lalign(f).
Lemma 11. Consider the random variable X = ef (X)Tf (V) where X 〜 P and V 〜 q+. Then
Var(X) ≤θ(Laiign (f)).
26
Published as a conference paper at ICLR 2021
Recall that Lalign (f) = Ex,x+ kf(x) - f(x+)k2/2 is termed alignment, and Wang & Isola (2020)
show that the contrastive objective jointly optimize alignment and uniformity. Lemma 11 therefore
shows that as training evolves, the variance of the X = ef(X)Tf(V) where X 〜P and V 〜q+ is
bounded by a term that we expect to see becoming small, suggesting that using a single sample
(M = 1) to approximate this expectation is not unreasonable. We cannot, however, say more than
this since we have no guarantee that Lalign(f) goes to zero.
Proof. Fixan X and recall that we are considering q^β(∙) = *(•; x). First let X0 beani.i.d. copy of X,
and note that, conditioning on x, we have 2Var(X |x) = Var(X|x) + Var(X 0 |x) = Var(X -X0|x) ≤
E(X - X0)2|X. Bounding this difference,
E[(X - X0)2∣x] = Ev,v0~q+ (ef(x)>f(v) - ef(X)Tf(VO))2
≤ Ev,v,~q+ (e1/t2 [f(x)>f(v) - f(x)>f(v0)])
≤ e1"4Ev,v0~q+ ([f(X)Mf(V)-f(v0)∕)2
1/t4
=~tΓEv,v0~q+llf(V)- f (VO)Il
≤O(Ev,vo~p+||f(v)-f(v0)||2)
where the first inequality follows since f lies on the sphere of radius 1/t, the second inequality by
CaUChy-Schwarz, the third again since f lies on the sphere of radius 1/t, and the fourth since qj is
absolutely continuous with respect to p+ with bounded ratio.
Since p+ (X+) = p(X+|h(X)) only depends on c = h(X), rather than X itself, taking expectations over
x 〜p is equivalent to taking expectations over C 〜ρ. Further, P(C)P(V∣c)p(v0∣c) = P(V)P(Vlc)=
p(v)p+(v0). So Ec~ρEv,v,~p+lf(v) - f(V0)∣∣2 = Eχ,χ+f(x)- f(x+)『=2Lalign(f), where
x ~ p and χ+ ~ p+. Thus we obtain the lemma.	口
1
2
3
4
5
6
7
8
9
10
11
12
# pos
# neg
#N
#t
# tau_plus
# beta
exp of inner products for positive examples
exp of inner products for negative examples
number of negative examples
temperature scaling
class probability
concentration parameter
#Clipping negatives trick before computing reweighting
reweight = 2*neg / max( neg.max().abs(), neg.min().abs() )
reweight = (beta*reweight) / reweight.mean()
Neg = max((-N*tau_plus*pos + reweight*neg).sum() / (1-tau_plus), e**(-1/t))
hard_loss = -log( pos.sum() / (pos.sum() + Neg))
Figure 14:	In cases where the learned embedding is not normalized to lie on a hypersphere we found
that clipping the negatives to live in a fixed range (in this case [-2, 2]) stabilizes optimization.
D.2 Graph Representations
All datasets we benchmark on can be downloaded at www.graphlearning.io from the TU-
Dataset repository of graph classification problems (Morris et al., 2020). Information on basic
statistics of the datasets is included in Tables 3 and 4. For fair comparison to the original In-
foGraph method, we adopt the official code, which can be found at https://github.com/
27
Published as a conference paper at ICLR 2021
fanyun-sun/InfoGraph. We modify only the gan_losses.py script, adding in our pro-
posed hard sampling via reweighting. For simplicity we trained all models using the same set of
hyperparameters: we used the GIN architecture (Xu et al., 2019) with K = 3 layers and embedding
dimension d = 32. Each model is trained for 200 epochs with batch size 128 using the Adam opti-
mizer (Kingma & Ba, 2015). with learning rate 0.001, and weight decay of 10-6. Each embedding is
evaluated using the average accuracy 10-fold cross-validation using an SVM as the classifier (in line
with the approach taken by Morris et al. (2020)). Each experiment is repeated from scratch 10 times,
and the distribution of results from these 10 runs is plotted in Figure 3.
Since the graph embeddings are not constrained to lie on a hypersphere, for a batch we clip all the
inner products to live in the interval [-2, 2] while computing the reweighting, as illustrated in Figure
14. We found this to be important for stabilizing optimization.
Dataset	DD	PTC	REDDIT-B	PROTEINS
No. graphs	1178	344	2000	1113
No. classes	2	2	2	2
Avg. nodes	284.32	14.29	429.63	39.06
Avg. Edges	715.66	14.69	497.75	72.82
Table 3: Basic statistics for graph datasets.
Dataset	ENZYMES	MUTAG	IMDB-B	IMDB-M
No. graphs	600	188	1000	1500
No. classes	6	2	2	3
Avg. nodes	32.63	17.93	19.77	13.00
Avg. Edges	62.14	19.79	96.53	65.94
Table 4: Basic statistics for graph datasets.
D.3 Sentence Representations
We adopt the official quick-thoughts vectors experimental settings, which can be found at https:
//github.com/lajanugen/S2V. We keep all hyperparameters at the default values and change
only the s2v-model.py script. Since the official BookCorpus dataset Kiros et al. (2015) is not
available, we use an unofficial version obtained using the following repository: https://github.
com/soskek/bookcorpus. Since the sentence embeddings are also not constrained to lie on a
hypersphere, we use the same clipping trick as for the graph embeddings, illustrated in Figure 14.
After training on the BookCorpus dataset, we evaluate the embeddings on six different classification
tasks: paraphrase identification (MSRP) (Dolan et al., 2004), question type classification (TREC)
(Voorhees & Harman, 2002), opinion polarity (MPQA) (Wiebe et al., 2005), subjectivity classification
(SUBJ) (Pang & Lee, 2004), product reviews (CR) (Hu & Liu, 2004), and sentiment of movie reviews
(MR) (Pang & Lee, 2005).
E Further Discussion
Comparison with Kalantidis et al. (2020): Kalantidis et al. (2020) also consider ways to sample
negatives, and propose a mixing strategy for hard negatives, called MoCHi. The main points of
difference are: 1) MoCHi considers the benefit of hard negatives, but does not consider the possibility
of false negatives (Principle 1), which we found to be valuable. 2) MoCHi introduces three extra
hyperparameters, while our method introduces only two (β, τ+). If we discard Principle 1 (i.e. τ+)
then only β requires tuning. 3) our method introduces zero computational overhead by utilizing
within-batch reweighting, whereas MoCHi involves a small amount of extra computation.
Limitations of proposed method. There are two main sources of approximation in the hard
negative sampling algorithm we propose to use in practice. First, our hard negatives objective requires
computing an expectation overpx+(x+), the distributions on points with the same x+ class label as x.
28
Published as a conference paper at ICLR 2021
Due to lack of supervision, we are not able to sample exactly from px+ . To circumnavigate this, we
propose using samples generated using data augmentation, which we find works well in practice, but
deviates from the original formulation and framework. We find this approximation has important
implications for practitioners, since we observe that when using oracle access to px+ the downstream
performance improves monotonically as the concentration parameter β increases (see Fig. 4, middle),
while when using the practical approximation, large values of β start to hurt performance, thereby
requiring that β be tuned (values in the range (0.5, 2) tend to work well in general). Second, we
estimate the expectation Ev〜q+ [ef (X) f (v)] using just a single (importance weighted) sample. This is
done in the interests of efficiency - since using M > 1 samples significantly increases the effective
batch size (this is the more costly multi-view setting), increasing memory and runtime costs. We
anticipate that using M > 1 would improve performance further, in exchange for this extra price, but
leave experimentation to future work. That said, our analysis finds that the variance of the variable
ef(x)> f(v) with v
〜q~+ is controlled by the alignment objective, as shown in Lemma 11, which
suggests that in later stage training the variance of the single-sample estimator may be reasonably
small.
29