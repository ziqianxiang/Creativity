Published as a conference paper at ICLR 2021
Long Live the Lottery: The Existence of Win-
ning Tickets in Lifelong Learning
Tianlong Chen1*, Zhenyu Zhang2*, Sijia Liu3,4, Shiyu Chang4, Zhangyang Wang1
1 University of Texas at Austin,2University of Science and Technology of China
3Michigan State University, 4MIT-IBM Watson AI Lab, IBM Research
{tianlong.chen,atlaswang}@utexas.edu, zzy19969@mail.ustc.edu.cn
liusiji5@msu.edu, shiyu.chang@ibm.com
Ab stract
The lottery ticket hypothesis states that a highly sparsified sub-network can be
trained in isolation, given the appropriate weight initialization. This paper ex-
tends that hypothesis from one-shot task learning, and demonstrates for the first
time that such extremely compact and independently trainable sub-networks can
be also identified in the lifelong learning scenario, which we call lifelong tickets.
We show that the resulting lifelong ticket can further be leveraged to improve the
performance of learning over continual tasks. However, it is highly non-trivial to
conduct network pruning in the lifelong setting. Two critical roadblocks arise: i)
As many tasks now arrive sequentially, finding tickets in a greedy weight pruning
fashion will inevitably suffer from the intrinsic bias, that the earlier emerging tasks
impact more; ii) As lifelong learning is consistently challenged by catastrophic
forgetting, the compact network capacity of tickets might amplify the risk of for-
getting. In view of those, we introduce two pruning options, e.g., top-down and
bottom-up, for finding lifelong tickets. Compared to the top-down pruning that
extends vanilla (iterative) pruning over sequential tasks, we show that the bottom-
up one, which can dynamically shrink and (re-)expand model capacity, effectively
avoids the undesirable excessive pruning in the early stage. We additionally intro-
duce lottery teaching that further overcomes forgetting via knowledge distillation
aided by external unlabeled data. Unifying those ingredients, we demonstrate the
existence of very competitive lifelong tickets, e.g., achieving 3 - 8% of the dense
model size with even higher accuracy, compared to strong class-incremental learn-
ing baselines on CIFAR-10/CIFAR-100/Tiny-ImageNet datasets. Codes available
at https://github.com/VITA- Group/Lifelong- Learning- LTH.
1 Introduction
The lottery ticket hypothesis (LTH) (Frankle & Carbin, 2019) suggests the existence of an extremely
sparse sub-network, within an overparameterized dense neural network, that can reach similar per-
formance as the dense network when trained in isolation with proper initialization. Such a sub-
network together with the used initialization is called a winning ticket (Frankle & Carbin, 2019).
The original LTH studies the sparse pattern of neural networks with a single task (classification),
leaving the question of generalization across multiple tasks open. Following that, a few works
(Morcos et al., 2019; Mehta, 2019) have explored LTH in transfer learning. They study the trans-
ferability of a winning ticket found in a source task to another target task. This provides insights on
one-shot transferability of LTH. In parallel, lifelong learning not only suffers from notorious catas-
trophic forgetting over sequentially arriving tasks but also often comes at the price of increasing
model capacity. With those in mind, we ask a much more ambitious question:
Does LTH hold in the setting of lifelong learning when different tasks arrive sequentially?
Intuitively, a desirable “ticket” sub-network in lifelong learning (McCloskey & Cohen, 1989; Parisi
et al., 2019) needs to be: 1) independently trainable, same as the original LTH; 2) trained to perform
* Equal Contribution.
1
Published as a conference paper at ICLR 2021
competitively to the dense lifelong model, including both maintaining the performance of previous
tasks, and quickly achieving good generalization at newly added tasks; 3) found online, as the
tasks sequentially arrive without any pre-assumed order. We define such a sub-network with its
initialization as a lifelong lottery ticket.
This paper seeks to locate the lifelong ticket in class-incremental learning (CIL) (Wang et al., 2017;
Rosenfeld & Tsotsos, 2018; Kemker & Kanan, 2017; Li & Hoiem, 2017; Belouadah & Popescu,
2019; 2020), a popular, realistic and challenging setting of lifelong learning. A natural idea to
extend the original LTH is to introduce sequential pruning: we continually prune the dense network
until the desired sparsity level, as new tasks are incrementally added. However, we show that the
direct application of the iterative magnitude pruning (IMP) used in LTH fails in the scenario of
CIL since the pruning schedule becomes critical when tasks arrive sequentially. To circumvent this
challenge, we generalize IMP to incorporate a curriculum pruning schedule. We term this technique
top-down lifelong pruning. When the total number of tasks is pre-known and small, then with some
“lottery” initialization (achieved by rewinding (Frankle et al., 2019) or similar), we find that the
pruned sparse ticket can be re-trained to similar performance as the dense network. However, if the
number of tasks keeps increasing, the above ticket will soon witness performance collapse as its
limited capacity cannot afford the over-pruning.
The limitation of top-down lifelong pruning reminds us of two unique dilemmas that might chal-
lenge the validity of lifelong tickets. i) Greedy weight pruning v.s. all tasks’ performance: While
the sequential pruning has to be performed online, its greedy nature inevitably biases against later
arriving tasks, as earlier tasks apparently will contribute to shaping the ticket more (and might even
use up the sparsity budget). ii) Catastrophic forgetting v.s. small ticket size: To overcome the notori-
ous catastrophic forgetting (McCloskey & Cohen, 1989; Tishby & Zaslavsky, 2015), many lifelong
learning models have to frequently consolidate weights to carefully re-assign the model capacity
(Zhang et al., 2020) or even grow model size as tasks come in (Wang et al., 2017). Those seem to
contradict our goal of pruning by seeing more tasks.
To address the above two limitations, we propose a novel bottom-up lifelong pruning approach,
which allows for re-growing the model capacity to compensate for any excessive pruning. It there-
fore flexibly calibrates between increasing and decreasing tickets throughout the entire learning
process, alleviating the intrinsic greedy bias caused by the top-down pruning. We additionally in-
troduce lottery teaching to overcome forgetting, which regularizes previous task models’ soft logit
outputs by using free unlabeled data. That is inspired by lifelong knowledge preservation techniques
(Castro et al., 2018; He et al., 2018; Javed & Shafait, 2018; Rebuffi et al., 2017).
For validating our proposal, we conduct extensive experiments on CIFAR-10, CIFAR-100, and Tiny-
ImageNet datasets for class-incremental learning (Rebuffi et al., 2017). The results demonstrate
the existence and the high competitiveness of lifelong tickets. Our best lifelong tickets (found by
bottom-up pruning and lottery teaching) achieve comparable or better performance across all se-
quential tasks, with as few as 3.64% parameters, compared to state-of-the-art dense models. Our
contributions can be summarized as:
•	The problem of lottery tickets is formulated and studied in lifelong learning (class incremental
learning) for the first time.
•	Top-down pruning: a generalization of iterative weight magnitude pruning used in the original
LTH over continual learning tasks.
•	Bottom-up pruning: a novel pruning method, which is unique to allow for re-growing model
capacity, throughout the lifelong process.
•	Extensive experiments and analyses demonstrating the promise of lifelong tickets, in achieving
superior yet extremely light-weight lifelong learners.
2	Related Work
Lifelong Learning A lifelong learning system aims to continually learn sequential tasks and ac-
commodate new information while maintaining previously learned knowledge (Thrun & Mitchell,
1995). One of its major challenges is called catastrophic forgetting (McCloskey & Cohen, 1989;
Kirkpatrick et al., 2017; Hayes & Kanan, 2020), i.e., the network cannot maintain expertise on tasks
that they have not experienced for a long time.
2
Published as a conference paper at ICLR 2021
This paper’s study subject is class-incremental learning (CIL) (Rebuffi et al., 2017; Elhoseiny et al.,
2018): a popular, realistic, albeit challenging setting of lifelong learning. CIL requires the model
to recognize new classes emerging over time while maintaining recognizing ability over old classes
without access to the previous data. Typical solutions are based on regularization (Li & Hoiem,
2017; Kirkpatrick et al., 2017; Zenke et al., 2017; Aljundi et al., 2018a; Ebrahimi et al., 2019), for
example, knowledge distillation (Hinton et al., 2015) is a common regularizer to inherit previous
knowledge through preserving soft logits of those samples (Li & Hoiem, 2017) while learning new
tasks. Besides, several approaches are learning with memorized data (Castro et al., 2018; Javed &
Shafait, 2018; Rebuffi et al., 2017; Belouadah & Popescu, 2019; 2020; Lopez-Paz & Ranzato, 2017;
Chaudhry et al., 2018). And some generative lifelong learning methods (Liu et al., 2020; Shin et al.,
2017) mitigate catastrophic forgetting by generating simulated data of previous tasks. There also
exist a few architecture-manipulation-based lifelong learning methods (Rajasegaran et al., 2019;
Aljundi et al., 2018b; Hung et al., 2019; Abati et al., 2020; Rusu et al., 2016; Kemker & Kanan,
2017), while their target is dividing a dense model into task-specific parts for lifelong learning,
rather than localizing sparse networks and the lottery tickets.
Pruning and Lottery Ticket Hypothesis It is well-known that deep networks could be pruned
of excess capacity (LeCun et al., 1990b). Pruning algorithms can be categorized into unstructured
(Han et al., 2015b; LeCun et al., 1990a; Han et al., 2015a) and structured pruning (Liu et al., 2017;
He et al., 2017; Zhou et al., 2016). The former sparsifies weight elements based on magnitudes,
while the latter removes network sub-structures such as channels for more hardware friendliness.
LTH (Frankle & Carbin, 2019) advocates the existence of an independently trainable sparse sub-
network from a dense network. In addition to image classification (Frankle & Carbin, 2019; Liu
et al., 2019; Wang et al., 2020; Evci et al., 2019; Frankle et al., 2020; Savarese et al., 2020; You et al.,
2020; Ma et al., 2021; Chen et al., 2020a), LTH has been explored widely in numerous contexts, such
as natural language processing (Gale et al., 2019; Chen et al., 2020b), reinforcement learning (Yu
et al., 2019), generative adversarial networks (Chen et al., 2021b), graph neural networks (Chen
et al., 2021a), and adversarial robustness (Cosentino et al., 2019). Most of them adopt unstructured
weight magnitude pruning (Han et al., 2015a; Frankle & Carbin, 2019) to obtain the ticket, which
we also follow in this work. (Frankle et al., 2019) analyzes large models and datasets, and presents
a rewinding technique that re-initializes ticket training from the early training stage rather than
from scratch. (Renda et al., 2020) further compares different retraining techniques and endorses the
effectiveness of rewinding. (Mehta, 2019; Morcos et al., 2019; Desai et al., 2019) pioneer to study
the transferability of the ticket identified on one source task to another target task, which delivers
insights on one-shot transferability of LTH.
One latest work (Golkar et al., 2019) aimed at lifelong learning in fixed-capacity models based on
pruning neurons of low activity. The authors observed that a controlled way of “graceful forgetting”
after training each task can regain network capacity for new tasks, meanwhile not suffering from
forgetting. Sokar et al. (2020) further compresses the sparse connections of each task during training,
which reduces the interference between tasks and alleviates forgetting.
3	Lottery Ticket from Single-Task Learning to CIL
3.1	Problem setup
In CIL, a model continuously learns from a sequential data stream in
which new tasks (namely, classification tasks with new classes) are
added over time, as shown in Figure 1. At the inference stage, the
model can operate without having access to the information of task
IDs. Following (Castro et al., 2018; He et al., 2018; Rebuffi et al.,
2017), a handful of samples from previous classes are stored in a
fixed memory buffer.
Class	Class
{cι√-,c⅞1} ⅛+ι,---,c⅜a}
TaskTɪ 0	0 Task 为
Class-incremental Leamer)
TaSk为0	……
{cfc1+Class,c*2}
Figure 1: Basic CIL Setting.
More formally, let T,石，…represent a sequence of tasks, and the ith task T contains data that
fall into (k - ki-ι) classes Ci = {cki-ι +1,Cki-1+2, ∙∙∙ ,cki}, where ko = 0 by convention. Let
Θ(i) = {θ(i), θ(i)} denote the model of the learner used at task i, where θ(i) corresponds to the
base model cross all tasks from Ti to Ti, and θ(i) denotes the task-specific classification head at Ti.
3
Published as a conference paper at ICLR 2021
Thus, the size of θ(i) is fixed, but the dimension of θc(i) aligns with the number of classes, which
have been seen at Ti . In general, the learner has access to two types of information at task i: the
current training data D(i), and certain previous information P(i). The latter includes a small amount
of data from previous tasks {Tj}i1-1 stored in the memory buffer, and the previous model Θ(i-1) at
task Ti-1. This is commonly used to overcome the catastrophic forgetting issue of the current task i
against the previous tasks. Based on the aforementioned setting, we state the CIL problem as below.
Problem of CIL. At the current task i, we aim to learn a full model Θ(i) = {θ(i), θc(i)} based on
the information (D(i), P(i)) such that Θ(i) not only (I) yields the generalization ability to the newly
added data at task Ti but also (II) does not lose its power to the previous tasks {Tj}i1-1.
We note that the aforementioned problem statement applies to CIL with any fixed-length learning
period. That is, for n time stamps (one task per time), the validity of the entire trajectory {Θ(i) }1n is
justified by each Θ(i) from the CIL criteria (I) and (II) stated in ‘Problem of CIL’.
3.2	Lifelong lottery tickets
It was shown by LTH (Frankle & Carbin, 2019) that a standard (unstructured) pruning technique can
uncover the so-called winning ticket, namely, a sparse sub-network together with proper initialization
that can be trained in isolation and reach similar performance as the dense network. In this paper, we
aim to prune the base model θ(i) over time. And we ask: Do there exist winning tickets in lifelong
learning? If yes, how to obtain them? To answer these questions, a prerequisite is to define the
notion of lottery tickets in lifelong learning, which we call lifelong lottery tickets.
Following LTH (Frankle & Carbin, 2019), a lottery ticket consists of two parts: 1) a binary mask
m ∈ {0, 1}kθ(i) k0 obtained from a one-shot or iterative pruning algorithm, and 2) initial weights or
rewinding weights θ0. The ticket (m, θ0) is a winning ticket if training the subnetwork m	θ0 (
denotes element-wise product), identified by the sparse pattern m with initialization θ0 , wins the
initialization lottery to match the performance of the original (fully trained) network. In CIL, at the
presence of sequential tasks {T (i)}i=1,2,..., we define lifelong lottery tickets (m(i), θ0(i)) recursively
from the perspective of dynamical system:
m(i) = m(i-1) + A(D(i), P(i), m(i-1)),	and θ0(i) ∈ {θ(0),θr(wi)},	(1)
where A denotes a pruning algorithm used at the current task T(i) based on the information D(i),
P(i) and m(i-1), θ(0) denotes the initialization prior to training the model at T(1), and θr(wi) denotes
a rewinding point at T (i). In Eq. (1), we interpret the (non-trivial) pruning operation A by weight
perturbations, with values drawn from {-1, 0, 1}, to the previous binary mask. Here -1 denotes the
removal of a weight, 0 signifies to keep a weight intact, and 1 represents the addition of a weight.
Moreover, the introduction of weight rewinding is spurred by the so-called rewinding ticket (Renda
et al., 2020; Frankle et al., 2020). For example, if θr(wi) = θ(i-1) , then we pick the model weights
learnt at the previous task T (i-1) to initialize the training at T(i) . We also note that θ(0) can be
regarded as the point rewound to the earliest stage of the lifelong learning. Based on Eq. (1), we
then state the definition of winning tickets in CIL.
Lifelong winning tickets. Given a sequence of tasks {Ti}1n, the lifelong lottery tickets
{(m(i) , θ0(i) )}1n given by (1) are winning tickets if they can be trained in isolation to match the
CIL performance (i.e., criteria I and II) of the corresponding full model {θ(i)}1n, where n ∈ N+.
In the next section, we will design the lifelong pruning algorithm A, together with ticket initializa-
tion schemes formulated in Eq. (1)
4	Proposed Pruning Method to Find Lifelong Winning Tickets
4.1	Revisiting IMP over sequential tasks: Top-down (TD) pruning
In order to find the potential tickets at the current task T (i), it is natural to specify A in Eq. (1)
as the iterative magnitude pruning (IMP) algorithm (Han et al., 2015a) to prune the model from
4
Published as a conference paper at ICLR 2021
m(i-1) θ(i-1). Following (Frankle & Carbin, 2019; Renda et al., 2020), IMP iteratively prunes
Pn(i) (%) non-zero weights of m(i-1) Θ θ(i-1) over n(i) rounds at T(i). Thus, the number of non-
1(i)
zero weights in the obtained mask m(i) is given by ((1 一 Pn(i) )n ∙ ∣∣m(i-1) ∣∣o). However, in the
application of IMP to the sequential tasks {T (i)}, we find that the schedule of IMP over sequential
tasks, in terms of {n(i)}, is critical to make pruning successful in lifelong learning. We refer readers
to Appendix A2.1 for detailed justifications.
Curriculum schedule ofTD pruning is a key to success The conventional method is to set {n(i)}
as a uniform schedule, namely, IMP prunes a fixed portion of non-zeros at each task. However, this
direct application fails quickly as the number of incremental tasks increases, implying that “not all
tasks are created equal” in the learning/pruning schedule. Inspired by the recent observation that
training with more classes helps consolidate a more robust sparse model (Morcos et al., 2019), we
propose a curriculum pruning schedule, in which IMP is conducted more aggressively for new tasks
arriving later, with n(i) ≥ n(i-1),
until reaching the desired sparsity. For example, if there are 12
times of pruning on five sequentially arrived tasks, we arrange them in a linearly increasing way, i.e.,
(T1:1,T2:1,T3:2,T4:3,T5:5). Note that TD pruning relies on the heuristic curriculum schedule, and
thus inevitably greedy and suboptimal over continual learning tasks. In what follows, we propose a
more advanced pruning scheme, bottom-up (BU) pruning, that obeys a different principle of design.
4.2	B ottom-up (BU) lifelong pruning: An advanced scheme
[(O, — )：Random Initialization / Connection ( ),一 ):Weights / Connection from Ti ( ),- ):Weights / Connection from 7ι~2 ( ),— ):Weights / Connection from‰3
Figure 2: Framework of our proposed bottom-up (BU) lifelong pruning which is based on sparse model
consolidation. Tickets founded by BU pruning keep expanding for each newly added task.
Why we need more than top-down pruning? TD pruning is inevitably greedy and suboptimal.
Earlier added tasks contribute more to shaping the final mask, due to the nested dependency be-
tween intermediate masks. In the later training stage, we often observe the network is already too
heavily pruned to learn more tasks. Inspired by the recently proposed model consolidation (Zhang
et al., 2020), we propose the BU alternative of lifelong pruning, to dynamically compensate for the
excessive pruning by re-growing previously reduced networks.
Full reference model & rewinding point For BU lifelong pruning, we maintain a full (unpruned)
model θr(ei)f as a reference throughout lifelong learning. First, θr(ei)f provides a reference performance
R(rie)f obtained at T(i). Once the validation accuracy of the current sparse model is no worse than
the reference performance, the sparse model is considered to still have sufficient capacity and can
be further pruned. Otherwise, capacity expansion is needed. On the other hand, the reference model
offers a rewinding point for network parameters, which preserves knowledge of all previous tasks
prior to T (i). It naturally extends the rewinding concept (Frankle et al., 2019) to lifelong learning.
BU pruning method BU lifelong pruning expands the previous mask m(i-1) to m(i). Different
from TD pruning, the model size grows along the task sequence, namely, ∣m(i) ∣0 ≥ ∣m(i-1) ∣0.
Thus, BU pruning enforces A in Eq. (1) to draw non-negative perturbations. As illustrated in Fig-
ure 2, for each newly added Ti, we first re-train the previous sparse model m(i-1) Θ θ(i-1) under
5
Published as a conference paper at ICLR 2021
the current information (D(i), P(i)) and calculate the validation accuracy R(i). If R(i) is above than
the reference performance R(rie)f, we proceed to keep the sparse mask m(i) = m(i-1) intact and use
re-trained θ(i-1) as θ(i) at Ti. Otherwise, an expansion from m(i-1) is required to ensure sufficient
(i)
learning capacity. To do so, we restart from the full reference model θref and iteratively prune its
weights using IMP until the performance gets just below Rr(ie)f. Here the previous non-zero weights
localized by m(i-1) are excluded from the pruning scope of IMP but the values of those non-zero
weights could be re-trained. As a result, IMP will yield the updated mask m(i) with a larger size
than m(i-1). We repeat the aforementioned BU pruning method when a new task arrives.
Although never observed in our CIL experiments, a potential corner case of expansion is that the
ticket size may hit the size of the full model. We consider this as an artifact of limited model capacity
and suggest future work of combining lifelong tickets with (full) model growing (Wang et al., 2017).
Ticket initialization Given the pruning mask found by the BU (or TD) pruning method, we next
determine the initialization scheme of a lifelong ticket. We consider three specifications of θ0(i)
in Eq. (1) to initialize the sparse model m(i) for re-training the found tickets. They include: (I)
θ0(i) = θ(0), i.e., the original “from the same random” initialization (Frankle & Carbin, 2019), (II)
a random re-initialization θreinit which is independent of θ(0), and (III) previous-task rewinding,
i.e., θ0(i) = θ(i-1) . The initialization schemes I-III together with m(i) yield the following tickets
m(i) at T(i): (1) BU (or TD) tickets, namely, m(i) found by BU (or TD) pruning with initialization
I; (2) random BU (or TD) tickets, namely, m(i) with initialization II; (3) task-rewinding BU (or
TD) tickets, namely, m(i) with initialization III. In experiments, we will show that both BU (or TD)
tickets and their task-rewinding (TR) counterparts are winning tickets, which outperform unpruned
full CIL models. Compared BU with TD pruning, TR-BU tickets surpass the best TD tickets.
4.3 Lottery Teaching: A Plug-in Regularization
Catastrophic forgetting poses a severe challenge to class-incremental learning, especially for com-
pact models. (Castro et al., 2018; He et al., 2018; Javed & Shafait, 2018; Rebuffi et al., 2017) are
early attempts for undertaking the forgetting dilemma by introducing knowledge distillation regu-
larization (Hinton et al., 2015), which employs a handful of stored previous data in addition to new
task data. (Zhang et al., 2020) takes advantage of unlabeled data to handle the forgetting quandary.
We adapt their philosophy (Li & Hoiem, 2017; Hinton et al., 2015; Zhang et al., 2020) to presenting
lottery teaching, enforcing previous information into the new tickets via a knowledge distillation
term on external unlabeled data. Lottery teaching consists of two steps: i) we query more similar
unlabeled data “for free” from a public source, by utilizing a small number of prototype samples
from previous tasks’ training data. In this way, the storage required for previous tasks could be
minimal, while the queried surrogate data functions similarly for our purpose; ii) we then enforce the
output soft logits of the current subnetwork {m(i) θ(i), θc(i)} on each queried unlabeled sample x to
be close to the logits from previously trained subnetwork {m(i-1) θ(i-1), θc(i-1)}, via knowledge
distillation (KD) regularization based on the K-L divergence. For all experiments of our methods
hereinafter, we by default append the lottery teaching as it is widely beneficial. An ablation study
will also follow in Section 5.3.
5	Experiments
Experimental Setup We briefly discuss the key facts used in our experiments and refer readers
to Appendix A2.3 for more implementation details. We evaluate our proposed lifelong tickets on
three datasets: CIFAR-10, CIFAR-100, and Tiny-ImageNet. We adopt ResNet18 (He et al., 2016)
as our backbone. We evaluate the model performance by standard testing accuracy (SA) averaged
over three independent runs.
CIL baseline: We consider a strong baseline framework derived from (Zhang et al., 2019), a recent
state-of-the-art (SOTA) method introduced for imbalanced data training (see more illustrations in
Appendix A1.1). We implement (Zhang et al., 2019) for CIL, and compare with two latest CIL
6
Published as a conference paper at ICLR 2021
SOTAs: iCaRL (Rebuffi et al., 2017) and IL2M (Belouadah & Popescu, 2019)1. Our results demon-
strate the adapted CIL method from (Zhang et al., 2019) outperforms the others significantly, (1.65%
SA better than IL2M and 4.88% SA better than iCaRL on CIFAR-10)2, establishing a new SOTA
bar. The proposed lottery teaching further improves the performance of the baseline adapted from
Zhang et al. (2019), given by 4.4% SA improvements on CIFAR-10. Thus, we use (Zhang et al.,
2019), combined with/without lottery teaching, to train the original (unpruned) CIL model.
CIL pruning: To the best of our knowledge, We are not aware of any effective CIL pruning baseline
comparable to ours. Thus, we focus on the comparison among different variants of our methods. We
also compare the proposal with the ordinary IMP, showing its incapability in CIL pruning. Further-
more, we demonstrate that given our proposed pruning frameworks, standard pruning methods such
as IMP and `1 pruning (by imposing `1 sparsity regularization) then turn to be successful.
Results on TD tickets We begin by showing that TD pruning is non-trivial in CIL. We find that the
ordinary IMP (Han et al., 2015a) fails: It leads to 10.21% SA degradation (from 72.79% to 62.58%
for SA) with 4.40% parameters left. By contrast, our proposed lifelong tickets yield substantially
better performance which even surpasses the full dense model, with fewer parameters left than the
ordinary IMP (Han et al., 2015a).
In what follows, We evaluate TD lifelong pruning using different weight rewindings, namely, i) TD
tickets; ii) random TD tickets; iii) task-rewinding TD tickets; iv) late-rewinding TD tickets; and v)
Fine-tuning. The late-rewinding tickets is a strong baseline claimed in Mehta (2019).
Figure 3 and Table A4 demonstrate the high
competitiveness of our proposed TD ticket
(blue lines). It matches and most of the time
outperforms the full model3 (black dash lines).
Even with only 6.87% model parameters left,
the TD ticket still surpasses the dense model
by 0.49% SA. The task-rewinding tickets, in
second place, exceeds the dense model un-
til reaching the extreme sparsity of 4.40%.
Moreover, we see late-rewinding TD tickets
dominate over other rewinding/fine-tuning op-
tions, echoing the finding in single-task learn-
ing (Frankle et al., 2019).
Remaining Weights %
Figure 3: Evaluation performance (standard accuracy)
of top-down lifelong tickets on CIFAR-10.
However, TD pruning cannot afford a lot more incremental tasks due to its greedy weight (over-
)pruning. Our results show that TD tickets pruned from only tasks T1 and T2 clearly overfit the first
two tasks, even after incrementally learning the remaining three tasks. In this inappropriate pruning
schedule (in contrast to T ~ T scheme), the resultant ticket drops to 59.28% SA which is 13.51%
lower than the dense model, as shown in Table A3. More results can be found in the appendix.
Therefore, bottom-up lifelong pruning is proposed, as a remedy for relieving laborious tuning of
pruning schedules.
Results on BU lifelong tickets The bottom-up lifelong pruning allows the sparse network to regret
if they could not deal with the newly added tasks, which compensates for the excessive pruning and
reaches a substantially better trade-off between sparsity and generalization ability. Compared to TD
pruning, it does not require any heuristic pruning schedules.
In Table 1, we first present the performance of BU tickets, random BU tickets, and task-rewinding
BU (TR-BU) tickets, as mentioned in Section 4.2. As we can see, TR-BU tickets obtain the supreme
performance. A possible explanation is that task-rewinding (i.e., θ0(i) = θ(i-1)) maintains full in-
formation of learned tasks which mitigates the catastrophic forgetting, while other weight rewinding
points lack sufficient task information to prevent compact models from forgetting. Next, we observe
that TR-BU tickets significantly outperform the full dense model by 0.52% SA with only 3.64%
1Both are implemented using official codes. The comparison has been strictly controlled to be fair, including
dataset splits, same previously stored data, due diligence in hyperparameter tuning for each, etc.
2More comparisons with the latest CIL SOTAs are referred to the Appendix A1.1
3Full model, denoting the performance of the dense CIL model (Zhang et al., 2019) with lottery teaching.
7
Published as a conference paper at ICLR 2021
Table 1: Comparison results across full dense model, BU Tickets with different ticket initialization, and `1 BU
Tickets when training incrementally on CIFAR-10. T1~i denotes the learned sequential tasks Tl - Ti.
Dataset SettingS^^^^^^^	CIFAR-10 (Standard Accuracy / Remaining Weights 111mo0)				
	Ti (%)/100%	Tis (%) /100%	Tis (%)/100%	Tis4 (%)/100%	小5 (%)/100%
Full Dense Model	97.75	89.10	82.83	76.99	72.79
	Ti (%)/2.81%	T1s2 (%)/3.11%	K (%)/3.40%	T1~4 (%)/3.64%	TS (%)/3.64%
BU tickets	98.05	86.77	75.87	70.81	68.77
random BU tickets	96.55	82.08	77.97	72.84	71.17
TR-BU tickets	98.05	88.90	81.37	74.66	73.31
	Ti (%)/1.80%	T1s2 (%)/2.93%	Tis3 (%)/2.93%	T1~4 (%)/4.05%	T1~5 (%)/5.16%
`1 BU tickets	96.80	87.05	77.58	74.53	72.88
次?-&e-nuu‹ PJePUea
% 5ω- >UE3υu< PJePUBS
Ooo
8 6 4
23456789
The number of Learned Tasks
1	2	3	4	5
The number of Learned tasks
au-ωM 6u≡slu,uκ
Ti Tι-2 Tχ~3	ʃi ~4	A~5
j2ll-αjM 6-≡U-eluα,cc
■ TR-BU Tickets
TD Tickets
I~I ■■	lβ.78
ls.35 ls.63 K.63 K.92
Ti Ti-2 %~3 %~4 Ti-5 ʃi-e Tχ~ι 7i-β %—9 ʃi-io
Figure 4: Performance and sparsity comparison between TR-BU tickets and TD tickets when training models
incrementally. Left: CIFAR-10. Right: CIFAR-100. Upper: Comparison of SA. Bottom: Comparison of
remaining weights in tickets. Above all, tickets located by TD pruning continue to shrink with the growth of
incremental tasks. On the contrary, tickets founded by BU pruning keep expanding for each newly added task.
parameters left and 'ι BU tickets obtain matched performance to the full dense model with 5.16%
remaining parameters. It suggests that IMP, 'ι and even other adequate pruning algorithms can be
plugged into our proposed BU pruning framework to identify the lifelong tickets.
In Figure 4, we present the performance comparison between TR-BU tickets (the best subnetworks
in Table 1) and TD tickets. TR-BU tickets are identified through bottom-up lifelong pruning, whose
sparse masks continue to subtly grow along with the incremental tasks, from sparsity 2.81% at the
first task to sparsity 3.64% at the last task. As we can see, at any incremental learning stage, TR-BU
tickets attain a superior performance with significantly fewer parameters. Particularly, after learning
all tasks, TR-BU tickets surpass TD tickets by 1.01% SA with 0.76% fewer weights on CIFAR-10;
3.07% with 2.46% fewer weights on CIFAR-100. Results demonstrate TR-BU tickets have a better
generalization ability and parameter-efficiency compared with TD tickets. In addition, on Tiny-
ImageNet in Table A7, TR-BU tickets outperform full model with only 12.08% remaining weights.
It is worth to mention that both TR-BU tickets and TD tickets have a superior performance than full
dense model. We refer readers to Table A5 and A6 in the appendix for more detailed results.
From the above results, we further observe that TR-BU tickets achieve comparable accuracy to full
models which have more than 30× times in network capacity, implying that bottom-up lifelong
pruning successfully discovers extremely sparse sub-networks, and yet they are powerful enough to
inherit previous knowledge and generalize well on newly added tasks. Furthermore, our proposed
lifelong pruning schemes can be directly plugged into other CIL models to identify the lifelong
ticket, as shown in Appendix A1.
Ablation studies In what follows, we summarize our results on ablation studies and refer readers
to the appendix A1.2.1 for more details. In Figure 5, we show the essential role of the curriculum
schedule in TD pruning compared to the uniform pruning schedule. We notice that the curriculum
8
Published as a conference paper at ICLR 2021
Figure 5: Left: the results of TD Tickets with/without lottery teaching. Right: the comparison of TD tickets
(10.74%) obtained from uniform and curriculum pruning schedule. Experiments are conducted on CIFAR-10.
pruning scheme generates stronger TD tickets than the uniform pruning in terms of accuracy, which
confirms our motivation that pruning heavier in the late stage of lifelong learning with more classes
is beneficial. In Table A8, we demonstrate the effectiveness of our proposals against different num-
bers of incremental tasks. In the Figure 5, we show that lottery teaching injects previous knowledge
through applying knowledge distillation on external unlabeled data, and greatly alleviates the catas-
trophic forgetting issue in lifelong pruning (i.e., after learning all tasks, utilizing lottery teaching
obtains a 4.34% SA improvement on CIFAR-10). It is worth mentioning that we set a buffer of
fixed storage capacity to store 128 unlabeled images queried from public sources at each training
iteration. We find that leveraging newly queried unlabeled data offers a better generalization-ability
than storing historical data in past tasks. The latter only reaches 70.60% SA on CIFAR-10, which is
2.19% worse than the use of unlabeled data.
6	Conclusion
We extend the Lottery Ticket Hypothesis to lifelong learning, in which networks incrementally learn
from sequential tasks. We pose top-down and bottom-up lifelong pruning algorithms to identify life-
long tickets. Systematical experiments are conducted to validate that located tickets obtain strong(er)
generalization ability across all incremental learned tasks, compared with unpruned models. Our fu-
ture work aims to explore lifelong tickets with the (full) model growing approach.
References
Davide Abati, Jakub Tomczak, Tijmen Blankevoort, Simone Calderara, Rita Cucchiara, and
Babak Ehteshami Bejnordi. Conditional channel gated networks for task-aware continual learn-
ing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 3931-3940, 2020.
Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars.
Memory aware synapses: Learning what (not) to forget. In Proceedings of the European Confer-
ence on Computer Vision (ECCV), pp. 139-154, 2018a.
Rahaf Aljundi, Marcus Rohrbach, and Tinne Tuytelaars. Selfless sequential learning. arXiv preprint
arXiv:1806.05421, 2018b.
Anonymous. Why lottery ticket wins? a theoretical perspective of sample complexity on sparse
neural networks. In Submitted to International Conference on Learning Representations, 2021.
URL https://openreview.net/forum?id=8pz6GXZ3YT. under review.
Eden Belouadah and Adrian Popescu. Il2m: Class incremental learning with dual memory. In The
IEEE International Conference on Computer Vision (ICCV), October 2019.
Eden Belouadah and Adrian Popescu. Scail: Classifier weights scaling for class incremental learn-
ing. In The IEEE Winter Conference on Applications of Computer Vision, pp. 1266-1275, 2020.
Francisco M Castro, Manuel J Marln-Jimenez, Nicolas GUiL Cordelia Schmid, and Karteek Alahari.
End-to-end incremental learning. In Proceedings of the European Conference on Computer Vision
(ECCV), pp. 233-248, 2018.
9
Published as a conference paper at ICLR 2021
Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient
lifelong learning with a-gem. arXiv preprint arXiv:1812.00420, 2018.
Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Michael Carbin, and
Zhangyang Wang. The lottery tickets hypothesis for supervised and self-supervised pre-training
in computer vision models. arXiv preprint arXiv:2012.06908, 2020a.
Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, and
Michael Carbin. The lottery ticket hypothesis for pre-trained bert networks. arXiv preprint
arXiv:2007.12223, 2020b.
Tianlong Chen, Yongduo Sui, Xuxi Chen, Aston Zhang, and Zhangyang Wang. A unified lottery
ticket hypothesis for graph neural networks, 2021a.
Xuxi Chen, Zhenyu Zhang, Yongduo Sui, and Tianlong Chen. {GAN}s can play lottery tick-
ets too. In International Conference on Learning Representations, 2021b. URL https:
//openreview.net/forum?id=1AoMhc_9jER.
Justin Cosentino, Federico Zaiter, Dan Pei, and Jun Zhu. The search for sparse, robust neural
networks. arXiv preprint arXiv:1912.02386, 2019.
Shrey Desai, Hongyuan Zhan, and Ahmed Aly. Evaluating lottery tickets under distributional shifts.
arXiv preprint arXiv:1910.12708, 2019.
Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, and Marcus Rohrbach. Uncertainty-guided
continual learning with bayesian neural networks. arXiv preprint arXiv:1906.02425, 2019.
Mohamed Elhoseiny, Francesca Babiloni, Rahaf Aljundi, Marcus Rohrbach, Manohar Paluri, and
Tinne Tuytelaars. Exploring the challenges towards lifelong fact learning. In Asian Conference
on Computer Vision, pp. 66-84. Springer, 2018.
Utku Evci, Fabian Pedregosa, Aidan Gomez, and Erich Elsen. The difficulty of training sparse
neural networks. arXiv preprint arXiv:1906.10732, 2019.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. 2019. URL https://openreview.net/forum?id=rJl-b3RcF7.
Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. The lottery ticket
hypothesis at scale. arXiv preprint arXiv:1903.01611, 2019.
Jonathan Frankle, David J. Schwab, and Ari S. Morcos. The early phase of neural network
training. In International Conference on Learning Representations, 2020. URL https:
//openreview.net/forum?id=Hkl1iRNFwS.
Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. arXiv
preprint arXiv:1902.09574, 2019.
Siavash Golkar, Michael Kagan, and Kyunghyun Cho. Continual learning via neural pruning. arXiv
preprint arXiv:1903.04476, 2019.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015a.
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for
efficient neural network. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett
(eds.), Advances in Neural Information Processing Systems 28, pp. 1135-1143. Curran Asso-
ciates, Inc., 2015b.
Tyler L Hayes and Christopher Kanan. Lifelong machine learning with deep streaming linear dis-
criminant analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops, pp. 220-221, 2020.
Chen He, Ruiping Wang, Shiguang Shan, and Xilin Chen. Exemplar-supported generative repro-
duction for class incremental learning. In British Machine Vision Conference, 2018.
10
Published as a conference paper at ICLR 2021
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural net-
works. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1389-1397,
2017.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
Ching-Yi Hung, Cheng-Hao Tu, Cheng-En Wu, Chien-Hung Chen, Yi-Ming Chan, and Chu-Song
Chen. Compacting, picking and growing for unforgetting continual learning. In Advances in
Neural Information Processing Systems, pp. 13669-13679, 2019.
Khurram Javed and Faisal Shafait. Revisiting distillation and incremental classifier learning. In
Asian Conference on Computer Vision, pp. 3-17. Springer, 2018.
Ronald Kemker and Christopher Kanan. Fearnet: Brain-inspired model for incremental learning.
arXiv preprint arXiv:1711.10563, 2017.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcom-
ing catastrophic forgetting in neural networks. Proceedings of the national academy of sciences,
114(13):3521-3526, 2017.
A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Master’s
thesis, Department of Computer Science, University of Toronto, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Yann LeCun, John S. Denker, and Sara A. Solla. Optimal brain damage. In D. S. Touretzky (ed.),
Advances in Neural Information Processing Systems 2, pp. 598-605. Morgan-Kaufmann, 1990a.
URL http://papers.nips.cc/paper/250-optimal-brain-damage.pdf.
Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural
information processing systems, pp. 598-605, 1990b.
Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis
and machine intelligence, 40(12):2935-2947, 2017.
Xialei Liu, Chenshen Wu, Mikel Menta, Luis Herranz, Bogdan Raducanu, Andrew D Bagdanov,
Shangling Jui, and Joost van de Weijer. Generative feature replay for class-incremental learn-
ing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
Workshops, pp. 226-227, 2020.
Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learn-
ing efficient convolutional networks through network slimming. In Proceedings of the IEEE
International Conference on Computer Vision, pp. 2736-2744, 2017.
Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of
network pruning. In International Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=rJlnB3C5Ym.
David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In
Advances in neural information processing systems, pp. 6467-6476, 2017.
Haoyu Ma, Tianlong Chen, Ting-Kuei Hu, Chenyu You, Xiaohui Xie, and Zhangyang Wang. Good
students play big lottery better. arXiv preprint arXiv:2101.03255, 2021.
Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The
sequential learning problem. In Psychology of learning and motivation, volume 24, pp. 109-165.
Elsevier, 1989.
11
Published as a conference paper at ICLR 2021
Rahul Mehta. Sparse transfer learning via winning lottery tickets. arXiv preprint arXiv:1905.07785,
2019.
Ari Morcos, Haonan Yu, Michela Paganini, and Yuandong Tian. One ticket to win them all: gener-
alizing lottery ticket initializations across datasets and optimizers. In Advances in Neural Infor-
mation Processing Systems,pp. 4933-4943, 2019.
German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual
lifelong learning with neural networks: A review. Neural Networks, 2019.
Jathushan Rajasegaran, Munawar Hayat, Salman H Khan, Fahad Shahbaz Khan, and Ling Shao.
Random path selection for continual learning. In Advances in Neural Information Processing
Systems, pp. 12669-12679, 2019.
Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl:
Incremental classifier and representation learning. In Proceedings of the IEEE conference on
Computer Vision and Pattern Recognition, pp. 2001-2010, 2017.
Alex Renda, Jonathan Frankle, and Michael Carbin. Comparing rewinding and fine-tuning in neural
network pruning. In International Conference on Learning Representations, 2020. URL https:
//openreview.net/forum?id=S1gSj0NKvB.
Amir Rosenfeld and John K Tsotsos. Incremental learning through deep adaptation. IEEE transac-
tions on pattern analysis and machine intelligence, 2018.
Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray
Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint
arXiv:1606.04671, 2016.
Pedro Savarese, Hugo Silva, and Michael Maire. Winning the lottery with continuous sparsification,
2020. URL https://openreview.net/forum?id=BJe4oxHYPB.
Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative
replay. In Advances in Neural Information Processing Systems, pp. 2990-2999, 2017.
Ghada Sokar, Decebal Constantin Mocanu, and Mykola Pechenizkiy. Spacenet: Make free space
for continual learning. arXiv preprint arXiv:2007.07617, 2020.
Sebastian Thrun and Tom M Mitchell. Lifelong robot learning. Robotics and autonomous systems,
15(1-2):25-46, 1995.
Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In
2015 IEEE Information Theory Workshop (ITW), pp. 1-5. IEEE, 2015.
Antonio Torralba, Rob Fergus, and William T Freeman. 80 million tiny images: A large data set for
nonparametric object and scene recognition. IEEE transactions on pattern analysis and machine
intelligence, 30(11):1958-1970, 2008.
Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by
preserving gradient flow. arXiv preprint arXiv:2002.07376, 2020.
Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Growing a brain: Fine-tuning by increas-
ing model capacity. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 2471-2480, 2017.
Haoran You, Chaojian Li, Pengfei Xu, Yonggan Fu, Yue Wang, Xiaohan Chen, Richard G. Baraniuk,
Zhangyang Wang, and Yingyan Lin. Drawing early-bird tickets: Toward more efficient training of
deep networks. In International Conference on Learning Representations, 2020. URL https:
//openreview.net/forum?id=BJxsrgStvr.
Haonan Yu, Sergey Edunov, Yuandong Tian, and Ari S Morcos. Playing the lottery with rewards
and multiple languages: lottery tickets in rl and nlp. arXiv preprint arXiv:1906.02768, 2019.
Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence.
Proceedings of machine learning research, 70:3987, 2017.
12
Published as a conference paper at ICLR 2021
Junjie Zhang, Lingqiao Liu, Peng Wang, and Chunhua Shen. To balance or not to balance: A
SimPle-yet-effective approach for learning with long-tailed distributions. arXiv, pp. arXiv-1912,
2019.
Junting Zhang, Jie Zhang, Shalini Ghosh, Dawei Li, Serafettin Tasci, Larry Heck, Heming Zhang,
and C-C Jay Kuo. Class-incremental learning via deep model consolidation. In The IEEE Winter
Conference on Applications of Computer Vision, pp. 1131-1140, 2020.
Hao Zhou, Jose M Alvarez, and Fatih Porikli. Less is more: Towards compact cnns. In European
Conference on Computer Vision, pp. 662-677. Springer, 2016.
13
Published as a conference paper at ICLR 2021
A1 More Experiment Results
A1.1 More Baseline Results
Comparison with the Latest CIL SOTAs We find (Zhang et al., 2019) can be naturally intro-
duced to class-incremental learning, which tackles the intrinsic training bias between a handful of
previously stored data and a large amount of newly added data. It adopts random and class-balanced
sampling strategies, combined with an auxiliary classifier, to alleviate the negative impact from im-
balanced classes. Extensive results, shown in Table A2, demonstrates that adopting (Zhang et al.,
2019) as the simple baseline surpasses previous SOTAs iCaRL (Rebuffi et al., 2017) and IL2M
(Belouadah & Popescu, 2019) by a significant performance margin (1.65%/0.57% SA better than
IL2M and 4.88%/7.60% SA better than iCaRL on CIFAR-10/CIFAR-100, respectively)4, establish-
ing a new SOTA bar. With the assistance of lottery teaching, (Zhang et al., 2019) obtains an extra
performance boost, 4.4% SA on CIFAR-10 and 7.34% SA on CIFAR-100.
It is worth mentioning that a lifelong ticket also exists in other CIL models. Take IL2M on
CIFAR-10 as an example, bottom-up (BU) ticket achieves accuracy 68.92% with 11.97% parameters
vs. the dense unpruned model with an accuracy of 66.74%.
Table A2: Comparison between our dense model and two previous SOTA CIL methods on CIFAR-
10 and CIFAR-100. Reported performance it the final accuracy for each task T. Simple baseline
donates the dense CIL model (Zhang et al., 2019). Full model represents our proposed framework
which combines lottery teaching technique with the simple baseline.
^^^^CIFAR-10 Methods	T1 (%)	T2 (%)	T3 (%)	T4 (%)	T5 (%)	Average (%)
iCaRL	76.45	79.00	75.70	50.85	35.55	63.51
IL2M	78.20	64.05	60.40	38.95	92.10	66.74
Simple Baseline	75.05	71.50	54.25	52.05	89.10	68.39
Full Model	79.70	79.12	68.23	63.45	73.43	72.79
^^^CIFAR-100 Methods	T (%) T (%) T (%) T (%) T (%) T (%) T (%) T (%) T (%) Tιo (%) J Average (%)
iCaRL	5.90	7.50	4.50	2.80	9.00	8.00	28.20	38.50	59.60	80.20	24.42
IL2M	19.90	24.10	19.80	12.90	21.30	21.70	29.90	34.80	40.30	89.80	31.45
Simple Baseline	21.20	32.10	23.00	22.70	21.70	31.70	39.60	33.80	40.20	54.30	32.02
Full Model	29.04	33.94	32.54	27.94	32.74	29.64	47.94	45.34	47.24	67.24	39.36
Pruning Schedule is Important As shown in Table A3, an inappropriate pruning schedule across
T 〜T2,the resultant ticket drops to 59.28% accuracy which is 13.51% lower than the dense model.
On the contrary, the adequate scheme across T 〜 T in Table A3, generates a TD winning ticket
with a higher test accuracy (+0.49% SA) and extreme fewer parameters (6.87%), compared with
the dense CIL model.
Table A3: Evaluation performance of TD tickets (6.87%) pruned from different task ranges.
Pruning Schedule	TD Tickets (6.87%) on CIFAR-1C T (%) T (%) T3 (%) T (%) T (%)	Average (%)
Prune across T 〜T Prune across T 〜T	74.05	88.80	78.25	28.40	26.90 78.90	82.15	71.55	63.80	70.00	59.28 73.28
A1.2 More Lifelong Tickets Results
Top-down Lifelong Tickets We also report several performance reference baselines: (a) Full
model, denoting the achievable performance of the dense CIL model (Zhang et al., 2019) combined
with lottery teaching. (b) CILlower denoting a vanilla CIL model without using lottery teaching
4To ensure fair compassion, iCaRL and IL2M both are implemented with their official codes. The compar-
ison has been strictly controlled to be fair, including dataset splits, same previously stored data, due diligence
in hyperparameter tuning for each, etc.
A14
Published as a conference paper at ICLR 2021
nor storing/utilizing previous data in any form; (c) MTupper training a dense model using full data
from all tasks simultaneously in a multi-task learning scheme. While it is not CIL (and much easier
to learn), we consider MTupper as an accuracy “upper bound” for (dense) CIL ; (d) MTLT by di-
rectly pruning MTupper to obtain its lottery ticket (Frankle & Carbin, 2019). The detailed evaluation
performance of TD tickets at different sparsity levels on CIFAR-10 are collected in Table A4.
IOO-
W
_ra
40
80
60 =
20 -
0
----Full Model
----CIL∣ower
MTUPPer
----MTLT
----TD Tickets
random TD Tickets
---- task-rewind!ng TD Tickets
---- late-rewind!ngTD Tickets
—Fine-tuning
Remaining Weights %

Figure A6: Evaluation performance (standard accuracy) of top-down lifelong tickets. The right
figure zooms in the red dash-line box in the left figure.
Table A4: Evaluation performance of TD tickets at different sparsity levels on CIFAR-10. Re-
ported performance is the final accuracy for each task T. Differences (+/-) are calculated w.r.t. the
full/dense model performance.
Remaining Weights	T (%)	T2 (%)	TD Tickets on CIFAR-10			
			T (%)	T4 (%)	T5 (%)	Average (%)
MTupper (100.00%)	97.48	97.48	93.33	90.83	94.03	94.63
CILlower (100.00%)	0.00	0.00	0.00	0.00	93.70	18.74
100.00%	79.70	79.12	68.23	63.45	73.43	72.79
32.77%	84.30	80.05	70.70	67.75	69.55	74.47 + 1.68
10.74%	78.90	77.75	76.30	63.55	71.05	73.51+0.72
6.87%	78.90	82.15	71.55	63.80	70.00	73.28 + 0.49
4.40%	82.25	78.55	65.10	65.38	70.23	72.30 - 0.49
2.25%	78.20	78.30	69.50	58.20	65.00	69.84 - 2.45
Bottom-up Lifelong Tickets As shown in Table A5 and Table A6, even compared with the best
TD tickets in terms of the trade-off between sparsity and accuracy, TR-BU tickets consistently remain
prominent on both CIFAR-10 (a slightly higher accuracy and 3.23% fewer weights) and CIFAR-
100 (2.37% higher accuracy and 4.88% fewer weights). From the results, we further observe that
TR-BU tickets achieve comparable accuracy to full models which have more than 30× times in
network capacity, implying that bottom-up lifelong pruning successfully discovers extremely sparse
sub-networks, and yet they are powerful enough to inherit previous knowledge and generalize well
on newly added tasks.
A1.2.1 More Ablation Results
Uniform v.s. Curriculum Lifelong Pruning We discuss different pruning schedules of top-down
lifelong pruning, which play an essential role in the performance of TD tickets. From the right figure
in Figure A7, we notice that the curriculum pruning scheme generates stronger TD tickets than the
uniform pruning in terms of accuracy, which confirms our motivation that pruning heavier in the late
stage of lifelong learning with more classes is beneficial.
The Number of Incremental Tasks Here we study the influence of increment times in our lifelong
learning settings. Table A8 shows the results of TR-BU20 tickets incrementally learn from 20 tasks (5
A15
Published as a conference paper at ICLR 2021
Table A5: Evaluation performance of TR-BU/TD tickets on CIFAR-10. T1 〜i, i ∈ {1,2, 3,4,5} do-
nates that models have learned from T1,…，T incrementally. l∣lml∣l; represents the current network
sparsity.
Compact Weights	CIFAR_10									
	Tl (%)	1mh	Tl〜2 (%)	Umh	Tl〜3 (%)	|m|b	Tl〜4 (%)	|m|b	Tl〜5 (%)	1mh WT
MTupper	-	-	-	-	-	-	-	-	94.63	100.00%
CILlower	97.60	100.00%	49.80	100.00%	32.57	100.00%	23.23	100.00%	18.74	100.00%
Full Model	97.75	100.00%	89.10	100.00%	82.83	100.00%	76.99	100.00%	72.79	100.00%
TD tickets (6.87%)	98.05	80.00%	87.55	64.00%	80.20	40.96%	73.21	16.78%	73.28	6.87%
TD tickets (4.40%)	98.10	80.00%	87.83	64.00%	79.30	32.77%	72.34	13.42%	72.30	4.40%
TR-BU tickets (3.64%)	96.80	2.81%	88.90	3.11%	81.37	3.40%	74.66	3.64%	73.31 ~	3.64%
Table A6: Evaluation performance of TR-BU/TD tickets when training incrementally on CIFAR-100.
Compact Weights	Tl (%)	|m|b	Tl〜2 (%)	||m||0 IRK	CIFA Tl〜3 (%)	R-100 ||m||0	Tl〜4 (%)	||m||0 IRK	TI〜5 (%)	||m||0
MTupper CILlower	- 87.40	- 100.00%	- 44.65	- 100.00%	- 28.67	- 100.00%	- 20.08	- 100.00%	- 17.44	- 100.00%
Full Model	88.30	100.00%	74.90	100.00%	63.70	100.00%	53.58	100.00%	48.52	100.00%
TD Tickets (12.08%)	88.34	80.00%	71.60	64.00%	58.80	51.2%	48.88	40.96%	43.60	32.77%
TD Tickets (9.66%)	88.20	80.00%	71.50	64.00%	58.73	51.2%	48.45	40.96%	44.08	32.77%
BU Tickets (7.20%)	85.70	5.50%	74.75	5.78%	65.93	6.07%	53.15	6.07%	48.18	6.07%
Compact Weights	Tl〜6 (%)	-ιmι∣0-	Tl〜7 (%)	-ιmι∣0-	Tl〜8 (%)	-||m||0-	Tl〜9 (%)	-||m||0-	Tl〜io (%)	-||m||0-
MTupper	-	-	-	-	-	-	-	-	74.11	100.00%
CILlower	14.07	100.00%	12.64	100.00%	10.91	100.00%	9.66	100.00%	8.64	100.00%
Full Model	45.82	100.00%	44.07	100.00%	42.35	100.00%	40.93	100.00%	39.36	100.00%
TD Tickets (12.08%)	41.50	26.21%	38.19	20.97%	37.28	16.78%	37.63	13.42%	37.42	12.08%
TD Tickets (9.66%)	41.18	26.21%	39.37	20.97%	38.06	16.78%	37.30	13.42%	36.72	9.66%
BU Tickets (7.20%)	47.58	6.35%	43.59	6.63%	41.35	6.63%	39.80	6.92%	39.79	7.20%
Table A7: Evaluation performance of TR-BU/TD tickets when training incrementally on Tiny-
ImageNet.
Compact Weights	CIFAR-100									
	Tl (%)	||m||0	Tl〜2 (%)	||m||0	Tl〜3 (%)	||m||0	Tl〜4 (%)	||m||0 >!	TI〜5 (%)	||m||0
Full Model	73.70	100.00%	59.60	100.00%	52.07	100.00%	43.85	100.00%	41.32	100.00%
BU Tickets (12.08%)	75.00	10.74%	58.60	11.01%	54.93	11.28%	47.23	11.28%	43.36	11.28%
Compact Weights	Tl〜6 (%)	||m||0	Tl〜7 (%)	||m||0	Tl〜8 (%)	||m||0	Tl〜9 (%)	||m||0 >!	Tl〜io (%)	||m||0
Full Model	37.32	100.00%	34.69	100.00%	30.23	100.00%	29.94	100.00%	28.29	100.00%
BU Tickets (12.08%)	36.93	11.28%	36.43	11.54%	32.41	11.54%	29.16	11.81%	28.33	12.08%
3	4
The number of Learned Tasks
<⅛ I⅞ΦM 6ucmUJ8a:
Ooooo
1 8 6 4 2
3	4
The number of Learned Tasks
求<s) 3enuu<pepu5S
求 I⅛ΦM 6ucmE8a:
loioQ!o
8 6 4 2
100
求
S 90-
Figure A7: Left: the results of TD Tickets with/without lottery teaching. Right: the comparison
of TD tickets (10.74%) obtained from uniform and curriculum pruning schedule. Experiments are
conducted on CIFAR-10.
classes per task); Table A6 presents the results of TR-BU10 tickets incrementally learn from 10 tasks
(10 classes per task). Comparing between two tickets, TR-BU10 tickets reach 6.55% higher accuracy
at the expense of 1.77% more parameters. Possible reasons behind it are that: i) the increasing of
incremental learning times aggravates the forgetting issue, which causes TR-BU20 tickets fall in a
A16
Published as a conference paper at ICLR 2021
Table A8: Evaluation performance of TR-BU Tickets when models incrementally learn 20 tasks on
CIFAR-100.
Compact Weights	T (%)	|m||0 l∣θ∣∣0	T1~2 (%)	|m||0 l∣θ∣∣0	CIFAR-100 T1~3 (%)懒		T1z4 (%)	|m||0 l∣θ∣∣0	T1-5 (%)	|m||0 l∣θ∣∣0
Full Model	89.20	100.00%	73.60	100.00%	67.33	100.00%	59.40	100.00%	52.16	100.00%
TR-BU Tickets (5.43%)	86.80	2.81%	75.20	3.11%	66.47	3.40%	60.75	3.40%	53.68	3.40%
Compact Weights	T1-6 (%)	|m||0 TMV	T1~7 (%)	|m||0	T1z8 (%)	|m||0 TMV	T1~9 (%)	|m||0 >^iV	T1~10 (%)	|m||0 >I^
Full Model	50.10	100.00%	46.80	100.00%	43.35	100.00%	41.71	100.00%	38.62	100.00%
TR-BU Tickets (5.43%)	50.40	3.40%	47.11	3.69%	44.10	3.98%	43.29	4.27%	38.70	4.27%
Compact Weights	T1-ιι (%)	|m||0 TMV	T1~12 (%)	Um11。	T1~13 (%)	|m||0 TMV	T1~14 (%)	|m||0 >^iV	T1~15 (%)	|m||0 >I^
Full Model	36.38	100.00%	35.77	100.00%	35.14	100.00%	34.66	100.00%	35.41	100.00%
TR-BU Tickets (5.43%)	37.35	4.27%	35.88	4.27%	34.62	4.27%	33.87	4.27%	35.48	4.56%
Compact Weights	T1~16 (%)	|m||0 WK	T1~17 (%)	|m||0	T1z18 (%)	|m||0 WK	T1~19 (%)	|m||0	T1~20 (%)	|m||0 >!
Full Model	34.43	100.00%	33.98	100.00%	34.14	100.00%	32.65	100.00%	33.13	100.00%
TR-BU Tickets (5.43%)	34.64	4.56%	34.13	4.85%	33.69	5.14%	31.84	5.14%	33.24	5.43%
worse accuracy decay; ii) at each incremental stage, TR-BU10 tickets learn more knowledge (10 v.s.
5 classes per task), which requires a large network capacity.
With v.s. Without Lottery Teaching Comparison results between TD tickets with lottery teaching
and the ones without lottery teaching are collected in this section. As shown in Figure A7 (left
figure), the performance of TD tickets without lottery teaching (black dash curves), quickly falls
into a worse decay along with the times of incremental learning increase. After learning all tasks,
utilizing lottery teaching obtains a 4.34% accuracy improvement on CIFAR-10. It suggests that our
proposed lottery teaching injects previous knowledge through applying knowledge distillation on
external unlabeled data, and greatly alleviates the catastrophic forgetting issue.
A2 More Methodology and Implementation Details
A2.1 More Lifelong Pruning Details
(O ,	)：Random Initialization / Connection ((ɔ,	):Weights / Connection from Ti(O,	)： Weights / Connection from 7ι~2 (O,	): Weights / Connection from‰3
Figure A8: Framework of our proposed top-down lifelong pruning algorithms. The top-down (TD) lifelong
pruning performs like iterative magnitude pruning (IMP) by unrolling the sequential tasks. Tickets located by
TD pruning continue to shrink with the growth of incremental tasks.
1
More Technical Details of Top-down Pruning In our implementation, We set P n(i) = 20%
as (Frankle & Carbin, 2019; Renda et al., 2020) and adjust {n(i) } to control the pruning sched-
ule of IMP over sequential tasks. The aforementioned lifelong pruning method is illustrated in
Figure A8, and we call it top-down lifelong pruning since the model size is sequentially reduced,
namely, km(i) k0 ≤ km(i-1) k0.
Pruning Algorithms We summarize the workflow of the top-down pruning and bottom-up prun-
ing in Algorithm 1 and 2, respectively. For pruning hyperparameters, we follow the original LTH’s
setting (Frankle & Carbin, 2019), i.e. ∆p = 20%. If we change ∆p to 40%, it will drop 2.04%
accuracy at the same sparsity level on CIFAR-10.
A2.2 More Class-incremental Learning Details
Lottery teaching regularization In order to mitigate the catastrophic forgetting effect, we apply
knowledge distillation (Hinton et al., 2015) RKD to enforce the similarity between previous y) and
A17
Published as a conference paper at ICLR 2021
current y soft logits on unlabeled data. We state RKD as follows:
Rkd (y, y) = -H(t(y),t(y))
where t(y)i = Pyi)：/T/t
Li & Hoiem, 2017).
=—∑St(y)j log t(y)j
j
T = 2 in our case, following the standard setting in (Hinton et al., 2015;
Algorithm 1: Top-Down Pruning
Input: Full dense model f(θ0, θc(0); x),
a desired sparsity Pm, samples x
from a storage S and sequential
tasks 71 〜n, soft logits from
previous model on queried
unlabeled data, pruning ratio ∆p
Output: An updated sparse model
f (θ m, θc(n) ; x)
ι Set i = 1 and mask m = 1 ∈ Rllθll0
2 Train f(θ0 m, θc(0); x) with data from
S and T1 .
3	while 1 一 l∣ml∣l0 ≤ Pm and i ≤ n do
llθll0	— m	—
4	Iterative weight magnitude (IMP)
pruning ∆p and obtaining new
maskm, where ∖∖τħ||o < ∣∣m∣∣o
5	Rewind weight to θ0
6	m=m
7	Retrain f(θ0 m, θc(i); x) on the
current task Ti and S . Lottery
teaching is applied (A knowledge
distillation constrain with soft
logits)
8	Seti = i+ 1
9	end
Algorithm 2: Bottom-Up Pruning				
1	Input: f(θ0, θc(0); x), Pm, x, soft logits and ∆p defined in Algorithm 1, f(θi, θ(i); x) has learned Ti〜i and has performance R*, i ∈ {1,…，n} Output: An updated sparse model f(θ θ m, θCn ； x) Set i = 1 and mask m = 0 ∈ Rllθll0			
2	Train f (θo Θ m, θ(0); x) with data from S			
3	and Ti . Calculate accuracy Ri . while i ≤ n and ||m||o < ∣∣θ∣∣o do			
4 5 6 7 8 9 10 11 12 13 14 15	e	if e e S nd	Ri C lse S r u nd et i	≥ Ri or ∣∣m∣∣0 = I∣θ∣∣o then ontinue tart from f(θi, θc(i); x), m = 1 epeat pruning ∆p of θi Θ (m 一 m), obtain new mask mi , where ∣∣mi∣∣o ≥ |m||o and m ∈ mi Retrain f (θi-i Θ mi, θc(i); x) and calculate accuracy Ri m = mi ntil Ri 〜Ri and Set m = mi; =i+1
Our Dense Full CIL Model We consider a strong baseline framework derived from (Zhang et al.,
2019) with our proposed lottery teaching as our dense full CIL model. It adopts random and class-
balanced sampling strategies, an auxiliary classifier, and the knowledge distillation regularize] Rkd .
For incrementally learning task Ti , the training objective is depicted as:
LCIL(θ, θc(i), θa(i)) = γ2 ×L(θ,θc(i))+L(θ,θa(i))
L(θ, θc(i)) = E(x,y)∈Db hLXE(f (θ, θc(i), x), y)i
+ Yi X Eχ∈Du [Rkd(∕(Θ, θ(i), x), yc)i,
L(θ, θa(i)) = E(x,y)∈Dr hLXE(f (θ, θa(i), x), y)i
+ Yi X Eχ∈Du [Rkd(∕(Θ, θiai, x), ya)i ,
where Db is the class-balanced sampled dataset, Dr represents the randomly sampled dataset, and
Du stands for the queried unlabeled dataset. θ(i) and θαi are the main and auxiliary classifiers. y^c
and y。are soft logits on previous tasks of the main and auxiliary classifiers. We adopt γι = 1,γ2 =
0.5 in our experiment according to grid search.
A18
Published as a conference paper at ICLR 2021
A2.3 More Other Implementation Details
Datasets and Task Splittings We evaluate our proposed lifelong tickets on CIFAR-10, CIFAR-
100, and Tiny-ImageNet datasets, all being standard and state-of-the-art benchmarks for CIL
(Krizhevsky & Hinton, 2009). For all three datasets, we randomly split the original training dataset
into training and validation with a ratio of 9 : 1. On CIFAR-10, we divide the 10 classes into splits
of 2 classes with a random order (10/2 = 5 tasks); On CIFAR-100, we divide the 100 classes into
splits of 10 classes with a random order (100/10 = 10 tasks); On Tiny-Imagenet, we divide the
200 classes into splits of 20 classes with a random order (200/20 = 10 tasks). In this way, when
models learn a new incoming task, the dimension of classifiers will increase by 2 for CIFAR-10, 10
for CIFAR-100, and 20 for Tiny-ImageNet. Additionally, 100 images, 10 images, and 5 images per
class of learned tasks will be stored for CIFAR-10, CIFAR-100, and Tiny-ImageNet respectively.
Unlabeled Dataset All queried unlabeled data for CIFAR-10/CIFAR-100 are from 80 Million
Tiny Image dataset (Torralba et al., 2008), and for Tiny-ImageNet are from ImageNet dataset
(Krizhevsky et al., 2012). At each incremental learning stage, 4, 500, 450 and 450 images per
class of learned tasks will be queried, based on the feature similarity with stored prototypes
{m(i-1) θ(i-1) , θc(i-1)} in top-down Pruning and {θ(i-1) , θc(i-1)} in bottom-up Pruning at the
ith CIL stage for CIFAR-10, CIFAR-100, and Tiny-ImageNet respectively. The feature similarity is
defined by `2 norm distance.
Training and Evaluation Models are trained using Stochastic Gradient Descent (SGD) with 0.9
momentum and 5 × 10-4 weight decay. For 100 epochs training, a multi-step learning rate schedule
is conducted, starting from 0.01, then decayed by 10 times at epochs 60 and 80. During the iterative
pruning, we retrain the model for 30 epochs using a fixed learning rate of 10-4. The batch size for
both labeled and unlabeled data is 128. We pick the trained model of the highest validation accuracy
and report their performance on the hold-out testing set.
Other Training Details (i) CIFAR-10 and CIFAR-100 can be download at https://www.cs.
toronto.edu/~kriz/cifar.html. (ii) 80 Million Tiny Image dataset is referred to http:
//horatio.cs.nyu.edu/mit/tiny/data/index.html. (iii) All of our experiments are
conducted on NVIDIA GTX 1080-Ti GPUs.
A3 Discussion
Challenges of Theoretical Analysis and Future Work The theoretical justification of the lot-
tery ticket hypothesis is very limited, except for very shallow networks (Anonymous, 2021). In
the meantime, class-incremental learning makes the theoretical analysis more difficult. It is a chal-
lenging lifelong learning problem, and the current progress lies in the empirical side rather than the
theoretical side. The theoretical analysis is out of scope for this paper and we would like to explore
it in the future.
A19