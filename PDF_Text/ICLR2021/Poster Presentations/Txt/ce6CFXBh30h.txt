Published as a conference paper at ICLR 2021
Federated Semi-Supervised Learning with
Inter-Client Consistency & Disjoint Learning
Wonyong Jeong1, Jaehong Yoon2, Eunho Yang1,3, and Sung Ju Hwang1,3
Graduate School of AI1, KAIST, Seoul, South Korea
School of Computing2, KAIST, Daejeon, South Korea
AITRICS 3, Seoul, South Korea
{wyjeong, jaehong.yoon, eunhoy, sjhwang82}@kaist.ac.kr
Ab stract
While existing federated learning approaches mostly require that clients have fully-
labeled data to train on, in realistic settings, data obtained at the client-side often
comes without any accompanying labels. Such deficiency of labels may result
from either high labeling cost, or difficulty of annotation due to the requirement of
expert knowledge. Thus the private data at each client may be either partly labeled,
or completely unlabeled with labeled data being available only at the server, which
leads us to a new practical federated learning problem, namely Federated Semi-
Supervised Learning (FSSL). In this work, we study two essential scenarios of FSSL
based on the location of the labeled data. The first scenario considers a conventional
case where clients have both labeled and unlabeled data (labels-at-client), and the
second scenario considers a more challenging case, where the labeled data is only
available at the server (labels-at-server). We then propose a novel method to tackle
the problems, which we refer to as Federated Matching (FedMatch). FedMatch
improves upon naive combinations of federated learning and semi-supervised
learning approaches with a new inter-client consistency loss and decomposition
of the parameters for disjoint learning on labeled and unlabeled data. Through
extensive experimental validation of our method in the two different scenarios,
we show that our method outperforms both local semi-supervised learning and
baselines which naively combine federated learning with semi-supervised learning.
The code is available at https://github.com/wyjeong/FedMatch.
1	Introduction
Federated Learning (FL) (McMahan et al., 2017; Zhao et al., 2018; Li et al., 2018; Chen et al.,
2019a;b), in which multiple clients collaboratively learn a global model via coordinated communica-
tion, has been an active topic of research over the past few years. The most distinctive difference of
federated learning from distributed learning is that the data is only privately accessible at each local
client, without inter-client data sharing. Such decentralized learning brings us numerous advantages
in addressing real-world issues such as data privacy, security, and access rights. For example, for
on-device learning of mobile devices, the service provider may not directly access local data since
they may contain privacy-sensitive information. In healthcare domains, the hospitals may want to
improve their clinical diagnosis systems without sharing the patient records.
Existing federated learning approaches (McMahan et al., 2017; Wang et al., 2020; Li et al., 2018)
handle these problems by aggregating the locally learned model parameters. A common limitation is
that they only consider supervised learning settings, where the local private data is fully labeled. Yet,
the assumption that all of the data examples may include sophisticate annotations is not realistic for
real-world applications. Suppose that we perform on-device federated learning, the users may not
want to spend their time and efforts in annotating the data, and the participation rate across the users
may largely differ. Even in the case of enthusiastic users may not be able to fully label all the data in
the device, which will leave the majority of the data as unlabeled (See Figure 1 (a)). Moreover, in
some scenarios, the users may not have sufficient expertise to correctly label the data. For instance,
suppose that we have a workout app that automatically evaluates and corrects one’s body posture.
In this case, the end users may not be able to evaluate his/her own body posture at all (See Figure 1
1
Published as a conference paper at ICLR 2021
Local Models Labeled & Unlabeled @ Client
(a) Labels-at-Client Scenario
Local Models Unlabeled Examples @ Client
(b) Labels-at-Server Scenario
Figure 1: Illustrations of Two Practical Scenarios in Federated Semi-Supervised Learning (a) Labels-at-
Client scenario: both labeled and unlabeled data are available at local clients. (b) Labels-at-Server scenario:
labeled instances are available only at server, while unlabeled data are available at local clients.
(b)). Thus, in many realistic scenarios for federated learning, local data will be mostly unlabeled.
This leads us to practical problems of federated learning with deficiency of labels, namely Federated
Semi-Supervised Learning (FSSL).
A naive solution to these scenarios is to simply perform Semi-Supervised Learning (SSL) using
any off-the-shelf methods (e.g. FixMatch (Sohn et al., 2020), UDA (Xie et al., 2019)), while using
federated learning algorithms to aggregate the learned weights. Yet, this does not fully exploit
the knowledge of the multiple models trained on heterogeneous data distributions. To address this
problem, we present a novel framework which we refer to as Federated Matching (FedMatch), which
enforces the consistency between the predictions made across multiple models. Further, conventional
semi-supervised learning approaches are not applicable for scenarios where labeled data is only
available at the server (Figure 1 (b)), which is a unique SSL setting for federated learning. Also,
even when the labeled data is available at the client (Figure 1 (a)), learning from the unlabeled data
may lead to forgetting of what the model learned from the labeled data. To tackle these issues, we
decompose the model parameters into two, a dense parameter for supervised and a sparse parameter
for unsupervised learning. This sparse additive parameter decomposition ensures that training on
labeled and unlabeled data are effectively separable, thus minimizing interference between the two
tasks. We further reduce the communication costs with both the decomposed parameters by sending
only the difference of the parameters across the communication rounds. We validate FedMatch on
both scenarios (Figure 1 (a) and (b)) and show that our models significantly outperform baselines,
including a naive combination of federated learning with semi-supervised learning, on the training
data which are both non-i.i.d. and i.i.d. data. The main contributions of this work are as follows:
•	We introduce a practical problem of federated learning with deficiency of supervision, namely
Federated Semi-Supervised Learning (FSSL), and study two different scenarios, where the
local data is partly labeled (Labels-at-Client) or completely unlabeled (Labels-at-Server).
•	We propose a novel method, Federated Matching (FedMatch), which learns inter-client con-
sistency between multiple clients, and decomposes model parameters to reduce both interference
between supervised and unsupervised tasks, and communication cost.
•	We show that our method, FedMatch, significantly outperforms both local SSL and the naive
combination of FL with SSL algorithms under the conventional labels-at-client and the novel
labels-at-server scenario, across multiple clients with both non-i.i.d. and i.i.d. data.
2	Problem Definition
We begin with formal definition of Federated Learning (FL) and Semi-Supervised Learning (SSL).
Then, we define Federated Semi-Supervised Learning (FSSL) and introduce two essential scenarios.
2.1	Preliminaries
Federated Learning Federated Learning (FL) aims to collaboratively learn a global model via
coordinated communication with multiple clients. Let G be a global model and L = {lk }kK=1 be a set
of local models for K clients. Let D = {xi, yi}iN=1 be a given dataset, where xi is an arbitrary training
instance with a corresponding one-hot label yi ∈ {1, . . . , C} for the C -way multi-class classification
problem and N is the number of instances. D is composed of K sub-datasets Dlk = {xlik , ylik }iN=l1k
privately collected at each client or local model lk. At each communication round r, G first randomly
selects A local models that are available for training Lr ⊂ L and |Lr | = A. The global model G
then initializes Lr with global weights θG , and the active local models la ∈ Lr perform supervised
2
Published as a conference paper at ICLR 2021
learning to minimize loss `s (θla) on the corresponding sub-dataset Dla . G then aggregates the
learned weights θG J NNa PA θla and broadcasts newly aggregated weights to local models that
would be available at the next round r + 1, and repeat the learning procedure until the final round R.
Semi-Supervised Learning Semi-supervised learning (SSL) refers to the problem of learning with
partially labeled data, where the ratio of unlabeled data is usually much larger than that of the labeled
data (e.g. 1 : 10). For SSL, D is further split into labeled and unlabeled data. Let S = {xi, yi}iS=1
be a set of S labeled data instances and U = {ui}iU=1 be a set of U unlabeled samples without
corresponding label. Here, in general, |S | |U |. With these two datasets, S and U , we now
perform semi-supervised learning. Let pθ (y|x) be a neural network that is parameterized by weights
θ and predicts softmax outputs y with given input x. Our objective is to minimize loss function
`final(θ) = `s (θ) + `u(θ), where `s (θ) is loss term for supervised learning on S and `u(θ) is loss
term for unsupervised learning on U
2.2	Federated Semi-Supervised Learning
Now we further describe a practical problem of deficiency of labels in federated learning, which we
refer to as Federated Semi-Supervised Learning (FSSL), in which the data obtained at the clients may
or may not come with accompanying labels. Given a dataset D = {xi, yi}iN=1, D is split into a labeled
set S = {xi , yi }iS=1 and a unlabeled set U = {ui }iU=1 as in the standard semi-supervised learning.
Under the federated learning framework, we have a global model G and a set of local models L where
the unlabeled dataset U is privately spread over K clients hence Ulk = {ulik}iU=lk1 . For a labeled set S,
on the other hand, we consider two different scenarios depending on the availability of labeled data at
clients, namely the Labels-at-Client and the Labels-at-Server scenario, of which problem settings
and learning procedures will be discussed later.
3	Federated Matching
We now describe our Federated Matching (FedMatch) algorithm to tackle the federated semi-
supervised learning problem. We describe its core components in detail in the following subsections.
3.1	Inter-Client Consistency Loss
Consistency regularization (Xie et al., 2019; Sohn et al., 2020; Berthelot et al., 2019b;a) is one of most
popular approaches to learn from unlabeled examples in a semi-supervised learning setting. Conven-
tional consistency-regularization methods enforce the predictions from the augmented examples and
original (or weakly augmented) instances to output the same class label, ∣∣pθ(y|u) 一 pθ(y∣∏(u))∣∣2,
where ∏(∙) is a stochastic transformation function. Based on the assumption that class semantics are
unaffected by small input perturbations, these methods basically ensures consistency of the prediction
across the multiple perturbations of same input. For our federated semi-supervised learning method,
we additionally propose a novel consistency loss that regularizes the models learned at multiple
clients to output the same prediction. This novel consistency loss for FSSL, inter-client consistency
loss, is defined as follows:
1H
H AKL%hj (y|U)31 (y|U)]	⑴
where p.. (y|x) are helper agents that are selected from the server based on model similarity to
the client (which we describe later), that are not trained at the client (* denotes that we freeze the
parameters). The server selects and broadcasts H helper agents at each communication round. We
also use data-level consistency regularization at each local client similarly to FixMatch (Sohn et al.,
2020). Our final consistency regularization term Φ(∙) can be written as follows:
1H
Φ(∙) = CrOSSEntropy(9,Pθi(y∣∏(u))) + H £口嗫,(y∣u)∣∣Pθi (y|u)]	(2)
H j=1
where ∏(u) performs RandAugment (Cubuk et al., 2019) on unlabeld instance u. ^ is our novel
pseudo-labeling technique, which we refer to as the agreement-based pseudo label, defined as follows:
3
Published as a conference paper at ICLR 2021
Instance
Consistency Loss
Pseudo-Labeling
Figure 2: Illustration of Inter-Client Consistency Loss. We illustrate each step of our inter-client consistency
regularization process performed at local client. We provide the detailed explanations in Section 3.1.
CE(y, Pβ(
π(u)
Local Consistency
Regularization
H
y = Max(Mpθι (y|U)) + X ι(pθhj (y|U)))	(3)
j=1
where 1(∙) produces one-hot labels With given SoftmaX values , and Max(∙) outputs one-hot labels on
the class that has the maximum agreements. We discard instances with low-confident predictions
below confidence threshold τ when generating pseudo-labels. We then perform standard cross-entropy
minimization with the pseudo-label ^.
For helper agents, we select the H helper agents「；为旧(y|u) for each client as the most relevant
models from other clients. Specifically, we represent each model by its prediction m on the same
arbitrary input a located at server (we use random Gaussian noise), such that ml= pθl (m|a). The
server tries to keep and update all model embeddings m1:K from clients once each client updates its
weights to server, and creates K-Dimensional Tree (KD Tree) on m in the current round r for nearest
neighbor search to rapidly select the H helper agents for each client in the next rounds. We send
helper agents for every 10 rounds, and if a certain client has not yet updated its weights to server in
the previous step, then server simply skips sending helpers to the client at the round.
3.2	Parameter Decomposition for Disjoint Learning
In the standard semi-supervised learning approaches, learning on labeled and unlabeled data is
simultaneously done with a shared set of parameters. However, since this is inapplicable to the
disjoint learning scenario (Figure 1 (b)) and may result in forgetting of knowledge of labeled data (see
Figure 6 (c)), we separate the supervised and unsupervised learning via the decomposition of model
parameters into two sets of parameters, one for supervised learning and another for unsupervised
learning. To this end, we decompose our model parameters θ into two variables, σ for supervised
learning and ψ for unsupervised learning, such that θ = σ + ψ. We perform standard supervised
learning on σ, while keeping ψ fixed during training, by minimizing the loss term as follows:
minimize Ls(σ) = λsCrossEntropy(y,pσ+ψ* (y∣x))	(4)
where x and y are from labeled set S. For learning on unlabeled data, we perform unsupervised
learning conversely on ψ , while keeping σ fixed for the learning phase, by minimizing the consistency
loss terms as follows:
minimize Lu(ψ) = XccsΦ0*+ψ(∙) + 人乙？〃。* - ψ∣∣2 + Al/*|i	(5)
where all λs are hyper-parameters to control the learning ratio between the terms. We additionally
add L2- and L1-Regularization on ψ such that ψ is sparse, while not drifting far from knowledge that
σ has learned. To sum up, our decomposition technique allows us:
• Preservation Reliable Knowledge from Labeled Data: We empirically find that learning on
both labeled and unlabeled data with a single set of parameters may result in the model to forget
about what it learned from the labeled data (see Figure 6 (c)). Our method can effectively prevent
the inter-task interference via utilizing disjoint parameters only for supervised learning.
• RedUction of CommUnication Costs: sparsity on the unsupervised parameter ψ allows to
reduce communication cost. In addition, we further minimize the cost by subtracting the learned
l G	lG
knowledge for each parameter, such that ∆ψ = ψr - ψr and ∆σ = σr - σr , and transmit only
the differences ∆ψ and ∆σ as sparse matrices for both client-to-server and server-to-client costs.
• Disjoint Learning: In federated semi-supervised learning, labeled data can be located at either
client or server, which requires the model’s learning procedure to be flexible. Our decomposition
technique allows the model for the supervised training to be done separately elsewhere.
4
Published as a conference paper at ICLR 2021
Algorithm 1 Labels-at-Client Scenario
1:	RunServer()
2:	initialize σ0 and ψ0
3:	for each round r = 1, 2, ..., R do
4:	Lr — (select random A clients from L)
5:	for each client lar ∈ Lr in parallel do
6:	ψr:H — GetNearestNeighbors(ψr)
7:	σ∖,ψa — RUnClient(σr,ψr, ψr:H)
8:	EmbedLocalModel(σar , ψar )
9:	end for
10:	σr+1- ɪ PA=ι(σla)
11:	Ψr+1 - ⅛ PA=ι(Ψιra)
12:	end for
13:	RunClient(σ,ψ,ψi:H)
14:	θιa — σ + ψ, θhι:H — σ + ψi:H
15:	for each local epoch e from 1 to EL do
16:	for minibatch s ∈ Sιa and u ∈ Uιa do
17:	θσ + ψ*《-θσ+ψ* - ην's(θσ+ψ* ; θhi: H , S)
18:	θσ* +ψ《-θσ * + ψ - nV 'u (θσ*+ψ ; θh]∙H , U)
19:	end for
20:	end for
Figure 3: Illustrative Running Example of Labels-
at-Client Scenario We describe training and commu-
nication procedure between local and global model
under Labels-at-Client scenario corresponding to the
Algorithm 1. More details are described in Section 4.
4 Labels-at-Client Scenario
Problem Definition The Labels-at-Client scenario posits that the end-users intermittently annotate
a small portion of their local data (i.e., 5% of the entire data), leaving the rest of data instances
unlabeled as illustrated in Figure 1 (a). This is a common scenario for user-generated personal data,
where the end-users can easily annotate the data but may not have time or motivation to label all
the data (e.g. annotating faces in pictures for photo albums or social networking). We assume that
clients train on both labeled and unlabeled data, while the server only aggregates the updates from the
clients and redistributes the aggregated parameters back to the clients. In this scenario, labeled data S
is a set of individual sub-datasets Slk = {xlik , ylik}i4 S=lk1, yielding K sub-datasets for K local models
l1:K. The overall learning procedure of the global model is the same as that of conventional federated
learning (global model G aggregates updates from the selected subset of clients and broadcasts
them), except that active local models l1:A perform semi-supervised learning by minimizing the loss
`final (θla) = `s (θla) + `u(θla) respectively on Sla andUla.
FedMatch Algorithm for Labels-at-Client Scenario Now we introduce our FedMatch algorithm
for the labels-at-client scenario. As shown in Figure 3, which illustrates an example case of the
labels-at-Client scenario, active local models li：A at the current round r learn both σlr:A and ψlr:A on
both the labeled data Sl1:A and unlabeled data U l1:A at each local environment. After the completion
of local training, the clients update both their learned knowledge σl1A and ψlr:A to the server. The
server then aggregates bl1:A and ψl1:A, respectively, after embedding local models based on model
similarity as well as create KD-Tree to rapidly retrieve the top-H nearest neighbors ψh1H for each
client. At the next round, the server transmits the aggregated σr+1 and ψr+1. For helper agents,
server retrieves H helper agents, @h1:H, to each client for every 10 rounds. More details of the
training procedures for FedMatch, for the labels-at-client scenario, is described in Algorithm 1.
5	Labels-at-Server Scenario
Problem Definition We now describe another realistic setting, which is the labels-at-server scenario.
This scenario assumes that the supervised labels are only available at the server, while local clients
work with unlabeled data as described in Figure 1 (b). This is a common case of real-world
applications where labeling requires expert knowledge (e.g. annotating medical images, evaluating
body postures for exercises), but the data cannot be shared due to privacy concerns. In this scenario,
S G is identical to S and is located at server. The overall learning procedure is the same as that of
conventional federated learning, except the global model G performs supervised learning on SG by
minimizing the loss `s (θG) before broadcasting θG to local clients. Then, the active local clients
l1：A at communication round r perform unsupervised learning which solely minimizes `u (θla) on
the unlabeled data U la .
5
Published as a conference paper at ICLR 2021
Algorithm 2 Labels-at-Server Scenario
1:	RunServer()
2:	initialize σ0,ψ0
3:	for each round r = 1, 2, ..., R do
4:	for each server epoch e from 1 to EG do
5:	for minibatch s ∈ SG do
6：	θσ+ψ* <- θσ+ψ* — nV's (θσ+ψ* ； S)
7:	end for
8： end for
9:	L — (select random A clients from L
10：	for each client lra ∈ Lr in parallel do
11:	ψ1H — GetNeareStNeighbors(ψr)
12:	ψa — RunQient(σr+1, ψr, ψ∖∙H)
13:	EmbedLocalModel(σr+1 , ψar)
14:	end for
15:	ψr+1 - ⅛ PA=1(Ψfa )
16:	end for
17:	RunClient(σ,ψ,ψi:H)
18:仇—σ* + ψ, θhi:H — σ* + ψi:H
19:	for each local epoch e from 1 to EL do
20:	for minibatch u ∈ Ula do
21:	θσ* + ψ 4- θσ* +ψ - nV'u(θσ* +ψ ; θhi:H ,
22:	end for
23:	end for
Figure 4: Illustrative Running Example of Labels-
at-Server Scenario We depict learning and transmit-
ting procedure between a client and the global server
under Labels-at-Server scenario corresponding to the
Algorithm 2. Note that, in labels-at-server scenario, the
labeled data is only available at the server, and thus
u) global model at the server learns on labeled data, while
local models at clients learn on only unlabeled data.
Further details are explained in Section 5.
FedMatch Algorithms for Labels-at-Server Scenario We now describe our FedMatch algorithm
for the labels-at-server scenario. As depicted in Figure 4, which describes an illustrative running
example for labels-at-server scenario, the global model G learns σ on labeled data SG at the server
and the active local clients li： A at the current round r learn Sl1:A on unlabeled data ULA at each local
environment. After the completion of local training, clients update their learned knowledge Sl1:A to
the server. The server then embeds local models based on model similarity and create a KD-Tree for
rapid nearest neighbor search for the top-H most similar ψh1H models for each client. At the next
round, server transmits its learned σr+1 and the aggregated ψr+1 . Server transmits top-H similar
ψh1:H to each client for every 10 communication rounds. Further training details of FedMatch for the
labels-at-server scenario is described in Algorithm 2.
6	Experiments
We now experimentally validate our method, FedMatch, on three tasks, such as Batch-IID, Batch-
NonIID, and Streaming-NonIID, under both scenarios, Labels-at-Client and Labels-at-Server.
6.1 Experimental Setup
Tasks 1) Batch-IID: We use CIFAR-10 for this task and split 60, 000 instances into training
(54, 000), valid (3, 000), and test (3, 000) sets. We extract 5 labeled instances per class (C=10) for
each client (K=100) as labeled set S, and the rest of instances (49, 000) are used as unlabeled data
U, so that we can evenly split S and U into Sl1:100 and U l1:100, such that local models l1:100 learn on
corresponding labeled and unlabeled data during training. 2) Batch-NonIID (class-imbalanced):
The setting of this task is mostly the same with the Batch-IID task, except we arbitrarily control
the distribution of the number of instances per class for each client to simulate class-imbalanced
environments. 3) Streaming-NonIID (class-imbalanced): In this task, data streams into each client
from class-imbalanced distributions. We use Fashion-MNIST dataset for this task, and split 70, 000
instances into training (63, 000), valid (3, 500), and test (3, 500) sets. From train set, we extract 5
labeled instances per class (C =5) for each client (K=10) for a labeled set S. We discard labels for the
rest of instances to construct an unlabeled set U (62, 000). Then, we split S and U into S l1:100 and
U l1:100 based on a class-imbalanced distribution. For individual local unlabeled dataUlk, we again
split all instances into Utlk, t ∈ {1, 2, ..., T}, where T is the number of total streaming steps (we set
T =10). We train each streaming step for 10 rounds. We describe above tasks under Labels-at-Client
scenario. For Labels-at-Server scenario, S is simply located at server without any partition. Please
see Figure 7 in Appendix, which we visualize the concepts of dataset configuration.
6
Published as a conference paper at ICLR 2021
Table 1: Performance Comparison on Batch-IID & NonIID Tasks We use 100 clients (F =0.05) for 200
rounds. We measure global model accuracy and averaged communication costs. Note that the SL (Supervised
Learning) models learn on both S and U with full labels, and are utilized as the upper bounds for each experiment.
CIFAR-10, Batch-IID Task With 100 Clients (K=100, F=0.05, H=2
	LabeIS-at-CIient Scenario			LabelS-at-Server Scenario		
Methods	Acc.(%)	S2C Cost	C2S Cost	Acc.(%)	S2C Cost	C2S Cost
FedAvg-SL	58.60 ± 0.42	100%	-100^%-	52.45 ± 0.23	100 %	100 %
FedProx-SL	59.30 ± 0.31	_ _100_% _	_ _100_% _	JL ±0.38	_ 100 %_	_ T00% _
_ FedAvg-UDA	46.35 ± 021	一—100% —	一 ^ioo"% 一	^24.8Γ±0.73 ^	_ T00%^ 一	_ T00% _ 一
FedProx-UDA	47.45 ± 0.21	100%	100%	19.91 ± 0.31	100 %	100 %
FedAvg-FixMatch	47.01 ± 0.43	100%	100%	11.95 ± 0.60	100 %	T00 %
FedProx-FixMatch	47.20 ± 0.12	_ _100_% _	_ _100_% _	25.61 ± 0.32	_ T00%_	_ T00% _
FedMatch(OUrsy	32.13 ± M	一—79%— 一	——46 %——	一 44.95 ±0.49-	—45% ――	’—22% ——
CIFAR-10, Batch-NonIID Task With 100 CHents (K=100, F=0.05,~H=2
FedAvg-SL	55.T5 ± 0.2T	T00 %	T00%	5T.50 ± 0.5T	T00 %	T00 %
FedProx-SL	57.75 ± 0.T5	_」00_% _	_ J00_% _	_49.3L ±0.T8	_ T00%_	_ T00% _
- FedAvg-UDA	44.35 ± 031	一—两% 一	_ 100_% 一	^27.6Γ±0.7i ^	_ T00%^ 一	― T00% — —
FedProx-UDA	46.3T ± 0.63	T00 %	T00 %	26.0T ± 0.78	T00 %	T00 %
FedAvg-FixMatch	46.20 ± 0.52	T00 %	T00 %	09.45 ± 0.34	T00 %	T00 %
FedProx-FixMatCh	45.55 ± 0.63	_ J0^% _	_ J00_% _	_092L ±0.24	_ T00%_	_ T00% _
FedMatch (OUrsF	32.25 ± %1	一—85%— 一	——49 %——	44.17 ± 0.19	一 42% ――	‘- 20~% -—
50
50
Batch-IlDTask (100 Clients)
-- 1 , , , ,
Oooooo
6 5 4 3 2 1
() AUeJnU3V
50	100 150 200
Communication Round
OOOOO
5 4 3 2 1
(≠) >US3U<
BatCh-NOnIID (IOO ClientS)
FedProχ*SL
FedProx*UDA
FedProχ*FixMatch
FedMatch (Ouns)
50 IOO 150 200
Communication Round
*40
eɜo
120
10
60BatchzHp Task (100 Clients
.50 1∞ .15°. 2∞
Communication Round
FedProχ*SL
FedPrOX*UDA
——FedProχ*FixMatch
——FedMatch (Oura)
*40
10
eɜo
120
60 Batch-NonlID (100 Clients)
FedAvgwSL
FedPrOX*UDA
FedProxwFixMatch
FedMatch (Ours)
50 IOO 150 200
Communication Round
O.

⅛
(a) Labels-at-Client Scenario	(b) Labels-at-Server Scenario
Figure 5: Test Accuracy Curves on Batch-IID & NonIID Tasks We visualize test accuracy curves of model
performance corresponding to the Table 1. Note that the SL (Supervised Learning) models learn on both S and
U with full labels, and are utilized as the upper bounds for each experiment.
Baselines and Training Details Our baselines are: 1) Local-SL: local supervised learning (SL)
with full labels (S + U ) without sharing locally learned knowledge. 2) Local-UDA and 3) Local-
FixMatch: local semi-supervised learning, including UDA and FixMatch, without sharing local
knowledge. 4) FedAVG-SL and 5) FedProx-SL: supervised learning with full labels (S + U) while
sharing local knowledge via FedAvg and FedProx frameworks. 6) FedAvg-UDA and 7) FedProx-
UDA: naive combinations of FedAvg/Prox with UDA. 8) FedAvg-FixMatch and 9) FedProx-
FixMatch: naive combination of with FixMatch with FedAvg/Prox. For training, we use SGD
with adaptive-learning rate decay introduced in (Serra et al., 2018) with the initial learning rate 1e-3.
We use ResNet-9 networks as the backbone architecture for all baselines and our methods. We ensure
that all hyper-parameters are set equally for all base models and ours to perform fair evaluation and
comparison. Please see the Section A in the Appendix for further details. For all experiments, we
report the mean and the standard deviation over 3 runs.
6.2 Experimental Results
Results on Batch-IID & NonIID Tasks Table 1 shows performance comparison of our models
and naive Fed-SSL algorithms on Batch-IID and NonIID tasks under the two different scenarios.
We observe that our model outperforms all naive Fed-SSL baselines for all tasks and scenarios. In
particular, under labels-at-server scenario, which is more challenging than labels-at-client scenario,
we observe that the naive combination models significantly suffer from the forgetting issue and their
performances keeps deteriorating after a certain communication round. This phenomenon is mainly
caused by the base models failing to properly perform disjoint learning, in which case the learned
knowledge from the labeled and unlabeled data causes inter-task interference. Contrarily, our methods
show consistent and robust performance regardless where the labeled data exists, which shows that
our decomposition techniques effectively handles the challenging disjoint learning scenario. In
addition, when the class-wise distribution is imbalanced for each client (Non-IID task), we observe
that the base models’ performance slightly drops by 1-3%p, while our methods show consistent.
7
Published as a conference paper at ICLR 2021
Table 2: Averaged Local Performance on Streaming-NonIID Task We use 10 clients 100 rounds. We
measure averaged local model accuracy and communication costs. Note that the SL models learn on both S and
U with full labels, and are utilized as the upper bounds for each experiment.
Fashion-MNIST, Streaming-NonIID Task With 10 Clients (K=10, F=1.0, H=2)
	LabeIS-at-Cleint Scenario			LabelS-at-Server SCenario		
Methods	Acc.(%)	S2C Cost	C2S Cost	Acc.(%)	S2C Cost	C2S Cost
Local-SL	87.19 ± 0.36	N/A	-N/A	N/A	N/A	N/A
Local-UDA	70.70 ± 0.28	N/A	N/A	N/A	N/A	N/A
LoCal-FixMatCh	62.62 ± 0.32	_ _ N/A_ _	_ _N/A_ _	_ _ NZA _	_ _n/a _	_ _N/A _
^ FedProx-SL	82.06 ± d	一—100% —	一 ^ioo"% 一	77.43 ± o.42	_ Too%- 一	^ Too%^ -
FedProx-UDA	73.71 ± 0.17	100%	100%	83.34 ± 0.21	100 %	100 %
FedProx-FixMatCh	62.40 ± 0.4^	_ _100_% _	_ _100_% _	73.71 ± o.32	_ too%_	_ Too%_
FedMatch(OUrsy	五无± M	一—37%— 一	——48 %——	-84.i5 ±而一	一 i4% 一	—63% -—
Inter-Client Consistency
Ooo
5 4 3
(％) X3aln8<
50	100 150 200
Communication Round
FedPrOX*U DA
FedProx*Fi×Match
FedMatch w/o ICCL
FedMatch
Parameter Decomposition
Ooo
5 4 3
(％) X3aln8<
——FedPrcx*∪DA
---FedProx*FlxMatth
----FedMatch w/o SIgma
----FedMatch w⅛ Psl
----FedMatch
_ 50 IOO 150 200
Communication Round
Accuracy on Labeld Data
(％) X3aln8<
Batch-IlDTask (CIFAR-IO)
Ooo
6 5 4
{%) Auajnuu<
30
1 .. 5, ,W . 15	20
Numberof Labels per Class
(a) Inter-Client Consistency (b) Sigma & Psi (c) Inter-Task Interference (d) Number of Labels
Figure 6: Ablation Study and Additional Analysis on FedMatch Algorithm We study effectiveness of each
components of our method, (a) inter-client consistency loss and (b) parameter decomposition. (c) We effectively
tackle the inter-task interference. (d) Performance improvement of our method when labeled data is increased.
This shows that our inter-client consistency effectively enhances consistency with the helper agents
selected from server based on model similarity, which is good at, in particular, class-imbalanced tasks.
We also visualize the test accuracy curve for our models and naive Fed-SSL in Fig. 5. Our method
(Red line) trains faster and consistently outperforms the base models, and is most robustness against
inter-task interference in both scenarios. For analysis on the averaged communication costs, please
see Section B.1 in the Appendix.
Results on Streaming-NonIID Task Table 2 shows averaged local model performance on
Streaming-NonIID tasks with 10 synchronized clients. For the labels-at-client scenario, our proposed
method outperforms local-SSL and naive Fed-SSL models with large margins, 4-15%p, except for
the SL models. There is no huge difference of performance between local SSL and Fed-SSL models,
and this implies that our method effectively utilizes inter-client knowledge in this streaming setting.
In the labels-at-server scenario, interestingly, the performance of FedProx-SL decreases by around
5%p compared to the labels-at-client scenario, while Fed-SSL models obtain improved performance.
We conjecture that this is because, for streaming situation, the model may not sufficiently train on the
new data, while Fed-SSL models overcomes it by utilizing only the consistent pseudo-labels. Even
on this task, FedMatch outperforms all baselines with significantly smaller communication cost on
average (see Section B.1 for detailed analysis of the averaged communication costs).
Effectiveness of Inter-Client Consistency To show the effectiveness of our inter-client consistency
loss, we eliminate the loss, while learning on Batch-IID task with 100 clients (F =0.05). In Figure 6
(a), when we remove our inter-client consistency loss, we observe that the performance has slightly
dropped (Pink line) from one with the loss term (Red line). This gap clearly tells us that our inter-
client consistency loss improves model consistency across multiple models while keeping reliable
knowledge. Interestingly, our model without inter-client consistency loss still outperforms base
models. This additionally implies that our another proposed method, parameter decomposition for
disjoint learning, also effectively enhances model performance.
Effectiveness of Parameter Decomposition Our model with parameter decomposition alone,
without inter-client consistency loss, outperforms base models. For further analysis, we show the
effect of each decomposed variables, σ and ψ , in Figure 6 (b). Removing either of σ and ψ results in
substantial drop in the performance, with larger performance degeneration when dropping σ, which
captures much more essential knowledge from labeled data (Green line). Such decomposition is
effective since there exists knowledge interference between supervised learning and unsupervised
learning. We show this with an experiment where we perform semi-supervised learning with 5
labeled instances per class and 1, 000 unlabeled instances for 100 rounds. We measure accuracy on
the labeled set at each training steps. As shown in Figure 6 (c), our method effectively preserves
8
Published as a conference paper at ICLR 2021
learned knowledge from labeled set, while other base models suffer from knowledge interference. This
effective separation of supervised and unsupervised learning tasks enhances the overall performance
of our methods even without inter-client consistency loss as shown in Figure 6 (a) (shown in pink).
Number of Labels per Class We increase the number of labels per class for each client in a range
of 1, 5, 10, and 20 on Batch-IID task (CIFAR-10). Our method shows consistent performance
improvement as the number of labels increases. Interestingly, we observe that baseline models,
FedProx-UDA/FixMatch, show performance degradation even when the labeled data increases
(5 → 10). These results show that our method effectively utilize knowledge from labeled and
unlabeled data in federated semi-supervised learning settings, while other naive combinations of
FSSL could fail to learn properly from labeled and unlabeled data in federated learning framework.
7	Related Work
Federated Learning A variety of approaches for averaging local weights at server have been
introduced in the past few years. FedAvg (McMahan et al., 2017) performs weighted-averaging
on local weights according to the local train size. FedProx (Li et al., 2018) uniformly averages
the local updates while clients perform proximal regularization against the global weights, while
FedMA (Wang et al., 2020) matches the hidden elements with similar feature extraction signatures
in layer-wise manner when averaging local weights. PFNM (Yurochkin et al., 2019) introduces
aggregation policy which leverages Bayesian non-parametric methods. Beyond focusing on averaging
local knowledge, there are various efforts to extend FL to the other areas, such as continual learning
under federated learning frameworks (Yoon et al., 2020a) inspired by parameter decomposition
techniques proposed by (Yoon et al., 2020b). Recently, interests of tackling scarcity of labeled data
in FL are emerging and discussed in (Jin et al., 2020; Guha et al., 2019; Albaseer et al., 2020).
Semi-Supervised Learning While there exist numerous work on SSL, we mainly discuss consis-
tency regularization approaches. Consistency regularization (Sajjadi et al., 2016) assumes that the
class semantics will not be affected by transformations of the input instances, and enforces the model
output to be the same across different input perturbations. Some extensions to this technique perturb
inputs adversarially (Miyato et al., 2018), through dropout (Srivastava et al., 2014), or through data
augmentation (French et al., 2018). UDA (Xie et al., 2019) and ReMixMatch (Berthelot et al., 2019a)
use two sets of augmentations, weak and strong, and enforce consistency between the weakly and
strongly augmented examples. Recently, in addition to enforcing consistency between weak-strong
augmented pairs, FixMatch (Sohn et al., 2020) performs pseudo-label refinement on model predictions
via thresholding. Entropy minimization (Grandvalet & Bengio, 2004) which enforces the classifier
to predict low-entropy on unlabeled data, is another popular technique for SSL. Pseudo-Label (Lee,
2013) constructs one-hot labels from highly confident predictions on unlabeled data and uses these
as training targets inn a standard cross-entropy loss. MixMatch (Berthelot et al., 2019c) performs
sharpening on target distribution on unlabeled data, to further refine the generated pseudo-label.
8	Conclusion
In this work, we introduced two practical scenarios of Federated Semi-Supervised Learning (FSSL)
where each client learns with only partly labeled data (Labels-at-Client scenario), or supervised labels
are only available at the server, while clients work with completely unlabeled data (Labels-at-Server
scenario). To tackle the problem, we propose a novel method, Federated Matching (FedMatch),
which introduces the inter-client consistency loss that aims to maximize the agreement between the
models trained at different clients, and the parameter decomposition for disjoint learning which
decomposes the parameters into one for labeled data and the other for unlabeled data for preservation
of reliable knowledge, reduction of communication costs, and disjoint learning. Through extensive
experimental validation, we show that FedMatch significantly outperforms both local semi-supervised
learning methods and naive combinations of federated learning algorithms with semi-supervised
learning on diverse and realistic scenarios. As future work, we plan to further improve our model to
tackle the scenario where pretrained models deployed at each client adapts to a completely unlabeled
data stream (e.g. on-device learning of smart speakers).
9
Published as a conference paper at ICLR 2021
Acknowledgements This work was supported by Samsung Research Funding Center of Samsung
Electronics (No. SRFC-IT1502-51), Samsung Advanced Institute of Technology, Samsung Electron-
ics Co., Ltd., Next-Generation Information Computing Development Program through the National
Research Foundation of Korea(NRF) funded by the Ministry of Science, ICT & Future Plannig
(No. 2016M3C4A7952634), the National Research Foundation of Korea(NRF) grant funded by the
Korea government(MSIT) (2018R1A5A1059921), and Center for Applied Research in Artificial
Intelligence (CARAI) grant funded by DAPA and ADD (UDI190031RD). Also, this work was
supported by Institute of Information communications Technology Planning Evaluation (IITP) grant
funded by the Korea government(MSIT) (No.2019-0-00075, Artificial Intelligence Graduate School
Program(KAIST))
References
Abdullatif Albaseer, Bekir Ciftler, Mohamed Abdallah, and Ala Al-Fuqaha. Exploiting unlabeled
data in smart cities using federated learning. 01 2020.
David Berthelot, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and
Colin Raffel. Remixmatch: Semi-supervised learning with distribution alignment and augmentation
anchoring. arXiv preprint arXiv:1911.09785, 2019a.
David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin Raffel.
Mixmatch: A holistic approach to semi-supervised learning. arXiv preprint arXiv:1905.02249,
2019b.
David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A
Raffel. Mixmatch: A holistic approach to semi-supervised learning. In Advances in Neural
Information Processing Systems 32, pp. 5049-5059. Curran Associates, Inc., 2019c.
Yang Chen, Xiaoyan Sun, and Yaochu Jin. Communication-efficient federated deep learn-
ing with asynchronous model update and temporally weighted aggregation. arXiv preprint
arXiv:1903.07424, 2019a.
Yujing Chen, Yue Ning, and Huzefa Rangwala. Asynchronous online federated learning for edge
devices. arXiv preprint arXiv:1911.02134, 2019b.
Muhammad EH Chowdhury, Tawsifur Rahman, Amith Khandakar, Rashid Mazhar, Muhammad Ab-
dul Kadir, Zaid Bin Mahbub, Khandaker Reajul Islam, Muhammad Salman Khan, Atif Iqbal,
Nasser Al-Emadi, et al. Can ai help in screening viral and covid-19 pneumonia? arXiv preprint
arXiv:2003.13145, 2020.
Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V. Le. Randaugment: Practical data
augmentation with no separate search. CoRR, abs/1909.13719, 2019. URL http://arxiv.
org/abs/1909.13719.
Geoff French, Michal Mackiewicz, and Mark Fisher. Self-ensembling for visual domain adaptation.
In International Conference on Learning Representations, 2018. URL https://openreview.
net/forum?id=rkpoTaxA-.
Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In
Proceedings of the 17th International Conference on Neural Information Processing Systems,
NIPS’04, pp. 529-536, Cambridge, MA, USA, 2004. MIT Press.
Neel Guha, Ameet Talwlkar, and Virginia Smith. One-shot federated learning. 02 2019.
Yilun Jin, Xiguang Wei, Yang Liu, and Qiang Yang. A survey towards federated semi-supervised
learning. 02 2020.
Dong-Hyun Lee. Pseudo-label : The simple and efficient semi-supervised learning method for deep
neural networks. ICML 2013 Workshop : Challenges in Representation Learning (WREPL), 07
2013.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127, 2018.
10
Published as a conference paper at ICLR 2021
H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Areas.
Communication-efficient learning of deep networks from decentralized data. In AISTATS, 2017.
Takeru Miyato, Shin ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: A
regularization method for supervised and semi-supervised learning. PAMI, 2018.
Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transfor-
mations and perturbations for deep semi-supervised learning. In D. D. Lee, M. Sugiyama, U. V.
Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29,
pp. 1163-1171. Curran Associates, Inc., 2016.
Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic
forgetting with hard attention to the task. In Jennifer Dy and Andreas Krause (eds.), Proceedings
of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pp. 4548-4557, Stockholmsmassan, Stockholm Sweden, 10-15 Jul 2018.
PMLR. URL http://proceedings.mlr.press/v80/serra18a.html.
Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk, Alex
Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying semi-supervised learning with
consistency and confidence. arXiv preprint arXiv:2001.07685, 2020.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine
Learning Research, 15(56):1929-1958, 2014. URL http://jmlr.org/papers/v15/
srivastava14a.html.
Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni. Fed-
erated learning with matched averaging. In International Conference on Learning Representations,
2020. URL https://openreview.net/forum?id=BkluqlSFDS.
Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V. Le. Unsupervised data
augmentation for consistency training. 2019.
Jaehong Yoon, Wonyong Jeong, Giwoong Lee, Eunho Yang, and Sung Ju Hwang. Federated continual
learning with weighted inter-client transfer. In arXiv preprint arXiv:2003.03196, 2020a.
Jaehong Yoon, Saehoon Kim, Eunho Yang, and Sung Ju Hwang. Scalable and order-robust contin-
ual learning with additive parameter decomposition. In International Conference on Learning
Representations, 2020b. URL https://openreview.net/forum?id=r1gdj2EKPB.
Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Trong Nghia Hoang,
and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. 2019.
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated
learning with non-iid data. arXiv preprint arXiv:1806.00582, 2018.
11
Published as a conference paper at ICLR 2021
Organization We describe detailed experimental setups in Section A, such as our baselines (Sec-
tion A.1), model architecture (Section A.2), and the training configurations (Section A.3). We also
provide additional analysis and experimental results in Section B, including analysis on communi-
cation costs (Section B.1) and number of labels per class (Section B.2), experiments on real-world
dataset (Section B.3), different backbone architecture (Section B.4), fraction of clients per communi-
cation round (Section B.5).
A Experimental Details
We describe our experimental setups in detail, such as our baseline models, network architecture that
is used for all base models and our method, and the detailed training setups.
A.1 Baseline Models
We consider UDA (Xie et al., 2019) and FixMatch (Sohn et al., 2020) as our baselines, since
they are state-of-the-art SSL models and are based on the consistency-based mechanisms that are
conceptually similar to our inter-client consistency loss. We reimplement UDA with the Training
Signal Annealing (TSA) and exponential scheduling for its best performance as reported in their paper
(we use RandAugment (Cubuk et al., 2019) for consistency regularization with random magnitude).
We also reimplement FixMatch algorithms with strong augmentation as RandAugment (Cubuk et al.,
2019). For weak augmentation (filp-and-shift), however, as the performance has significantly dropped
when we apply the weak augmentation, we use original images rather than weakly augmenting
the images. We fix confidence threshold τ =0.85 for all FixMatch and our model experiments. For
federated learning frameworks, we use FedAvg (McMahan et al., 2017) and FedProx (Li et al., 2018)
algorithms since they are the standard baselines for federated learning and can be easily combined
with the SSL baselines. Detailed hyper-parameter settings are described in Table 4.
A.2 Network Architecture
Table 3: Network Architecture of ResNet-9
Layer	Filter Shape	Stride	Output
Input	N/A	N/A	32 × 32 × 3-
一 Conv 1 一	一^3 × ^3 × 3 × 64 ―	一^Γ 一	-32×32 × 64 -
Conv 2	3×3×64× 128	1	32 × 32 × 128
Pool 1	2×2	2	16 × 16 × 128
Conv 3	3 × 3 × 128 × 128	1	16 × 16 × 128
Conv 4	3 × 3 × 128 × 128	1	16 × 16 × 128
Conv 5	3 × 3 × 128 × 256	1	16 × 16 × 256
Pool 2	2×2	2	8 × 8 × 256
Conv 6	3 × 3 × 256 × 512	1	8 × 8 × 512
Pool 3	2×2	2	4× 4× 512
Conv 7	3× 3× 512× 512	1	4× 4× 512
Conv 8	3× 3× 512× 512	1	4× 4× 512
Pool4	4 × 4	4	1 X 1 X 512
Softmax	512 × 10	_ N/A—	--1 ×「× 1个一 一
We build ResNet-9 networks as our
base architecture for all base model
and our method. In the architecture,
the first two convolutional neural lay-
ers have 64 and 128 filters and the
same 3 × 3 kernel sizes followed by
2 × 2 max-pooling layer. Then we have
a skip connection between the subse-
quent two convolution layers with 128
filters. We then double the filter size
from 128 to 256 with the next conv
layer and down-sample via the follow-
ing 2 × 2 max-pooling layer. We repeat
the previous step, such that we have
512 filter size and 4 × 4 kernel size.
Then, we perform another skip connec-
tion through the two subsequent conv layers with 512 filters. As a final step, we down-sample the
kernel size from 4 × 4 to 1 × 1, then perform softmax classifier with the last fully connected layer.
All layers are equally initialized based on the varaiance scalining method. The model architecture is
described in Table 3.
A.3 Training Details
We use Stochastic Gradient Descent (SGD) to optimize our model with initial learning rate 1e-3. We
also adopt adaptive learning rate decay which is introduced by (Serra et al., 2018). The learning rate
strategy gradually reduces the learning rate by a factor of 3 for every 5 epochs that validation loss
does not consecutively decreases. We use L2 weight decay regularization on the base architecture
with L2 factor to be 1e-4. All hyper-parameters and other training setups are equally set for fair
comparison as shown in Table 4. In the table, we denote LPC as number of labels per class for each
12
Published as a conference paper at ICLR 2021
Figure 7: Illustration of Dataset Partition for Experimental Tasks We split the dataset D into a set of
labeled data S and a set of unlabeled data U . U is divided into K subsets which are distributed to K clients
(Batch Task). For streaming tasks, we further split all instances in each subset into T subsets for T streaming
steps. For class-imbalanced tasks, we additionally control the number of instances per class for each client.
Table 4: Hyper-Parameters & Training Setups We provide all hyper-parameters and training setups for all
baseline models and our method. Detailed hyper-parameters are also available in the code.
Labels-at-Client Scenario
Methods	lr	wd	λs	λu	λICCS	λLl	λL2	LPC	Bclient	U Bclient	Bserver	μ
SL	1e-3	1e-4	10	-	-	-	-	5	10	100	-	1e-2
UDA	1e-3	1e-4	10	1	-	-	-	5	10	100	-	1e-2
FixMatch	1e-3	1e-4	10	1	-	-	-	5	10	100	-	1e-2
FedMatch	1e-3	1e-4	10	-	1e-2	1e-4	10	5	10	100	-	-
Labels-at-Server Scenario												
SL	1e-3	1e-4	10	-	-	-	-	100	-	100	100	1e-2
UDA	1e-3	1e-4	10	1	-	-	-	100	-	100	100	1e-2
FixMatch	1e-3	1e-4	10	1	-	-	-	100	-	100	100	1e-2
FedMatch	1e-3	1e-4	10	-	1e-2	1e-5	10	100	-	100	100	-
client (or at server). BS and B U denote batch-size of labeled set S and unlabeled set U. μ is a
hyper-parameter for FedProx framework. We additionally provide visual illustration of our dataset
configuration. Please see Figure 7.
B Additional Analysis and Experimental Results
In this section, we additionally provide more analysis and experimental results, such as analysis on
communication costs and number of labels per class, experiments on real-world dataset, different
backbone architecture, fraction of clients per communication round.
B.1	The Efficient Communication of FedMatch
Since the actual bit-level compression techniques are rather implementation issues, which are beyond
our research scope, we only consider the reduction of the amount of information that needs to
be transmitted between the server and the client. To minimize the communication costs, we not
only learn ψ to be sparse, but also subtract the parameters between server and client, such that
∆ψ = ψrl - ψrG and ∆σ = σrl - σrG, then send only the difference, ∆ψ and ∆σ, as sparse matrices
from both directions of server-to-client (S2C) and client-to-server (C2S). Here, S2C and C2S costs
are the sums of ∆σ and ∆ψ . When transmitting the difference for each parameter to either way, we
discard almost unchanged values in an element-wise manner, so that only meaningful neural values
can be updated either server- or client-side. We observe that the range of the threshold values is from
1e-5 to 5e-5, such that the model performance is well-preserved and not significantly harmed, while
maximizing the reduction of communication costs.
As shown in Figure 8, we observe that both the S2C and C2S costs are gradually decreased during
the learning phases on both batch and streaming datasets under labels-at-client (Figure 8 (a) and
(b)) and labels-at-server scenarios (Figure 8 (c) and (d)). This is because each parameter separately
learns different tasks (i.e. supervised and unsupervised learning) effectively, which results in rapid
convergence to optimal points, respectively. Further, for the labels-at-server scenario, since labeled
data is not available at client, client even does not need to transfer ∆σ to the server (see Figure 8
(c) right and (d) right), which is extremely efficient than the labels-at-client scenario where both
∆σ and ∆ψ must be transferred to the server. For both scenarios, indeed, S2C contains the cost
13
Published as a conference paper at ICLR 2021
(％) 4SO。co-⅛u-c3EEOυ
Serverto Client Cost
Communication Round
(％) 4SO。co-⅛u-c3EEOυ
Communication Round
(a)	Batch-NonIID (Labels-at-Client)
(b)	Streaming-NonIID (Labels-at-Client)
(％)tt0u co-us-c3EEOυ
Communication Round
Client to Server Cost
(％)tt0u co-us-c3EEOυ
Communication Round
Serverto Client Cost
(％)tt0u co-us-c3EEOυ
Communication Round
Client to Server Cost
(％)tt0u co-us-c3EEOυ
Communication Round
(c) Batch-NonIID (Labels-at-Server)	(d) Streaming-NonIID (Labels-at-Server)
Figure 8:	Communication Cost Curves of FedMatch (ResNet-9) Corresponding to the Table 1 and 2. We
measure the communication costs for each parameters, ∆σ and ∆ψ, during training phase. The communication
costs under the labels-at-client scenario are visualized in (a) and (b) on the upper row. (c) and (d) on the lower
row represent the communication costs under labels-at-server scenario.
COVID-19 Radiography Dataset
	LabelS-at-Client	LabelS-at-Server
Methods	Acc.(%)	Acc.(%)
F.Prx-UDA	-74.24 ± 0.25-	80.11 土 0.18-
F.Prx-FixMtch	70.02 ± 0.28	72.15 ± 0.14
FedMatch	78.67 ± 0.23	—34.32 ± 0λT —
9qFOVID-19 DataseMLabegyiIenQ
0I 20	4 0	60	80	100
Communication Round
'60
30
90COVID-19 Dataset (Labe∣s-at-Server)
FedProχ-U□A
FedProx_FixMatch
FedMatch (Ours)
2?^^40. ..60. 89
Communication Round
100
(a) Labels-at-Client
(b) Labels-at-Server
Figure 9:	Experimental Results on COVID-19 Radiography Dataset. Left: Performance comparison of our
method (FedMatch) with the naive federated semi-supervised learning algorithms (FedProx-UDA/FixMatch).
Right: Test accuracy curves corresponding to the left performance table. Our method trains stably and
consistently outperforms all base models.
of helper agents, such that ∆ψ1:H = PH=ι ψj - ψlr. However, as shown in Figure 8, transmitting
multiple helper agents (H=2 in our experiments) does not significantly affect the total S2C costs
thanks to our novel decomposition techniques as well as efficient subtracting method, such that model
reconstruction can be possible without meaningful information loss at either server- or client-side.
B.2	Further Analysis of the Number of Labels per Class
We explain our analysis of the number of labels per class under Section 6.2, and here we further	Table 5: Analysis of the Number of Labels per Class				
		Number of Labeled Examples per Class			
	-	1	5	10	20
provide additional experimental re- sults. We conduct experiments					
	Methods	Acc.(%)	Acc.(%)	Acc.(%)	Acc.(%)
	FedPrx*UDA	31.95	47.45-	41.4	47.15
with our method without the de-	FedPrx*FxMtch	_30.01_	_ 47.2_ _	_ 34.25 _	_ _44.5 _
composition technique. As shown	FedMatch (w/o)	一刃二一	ɪ 47.51 1	二 5二15 二	二豆7 二-
in Table 5, our method without de-	FedMatch	一 一37.65-—	—5475——	—W65 —	一 一66.1 -—
composition technique (indicated
as FedMatch (w/o)) shows not much performance improvement when the number of labels per class
increases from 5 to 10 (around 3.x%p) than from 1 to 5 (around 9.x%p) and from 10 to 20 (10.x%p),
which are the similar tendency with the baseline models in Table 5. However, with the decomposition
technique, our method shows consistent performance improvement, which implies that our proposed
technique has the effectiveness to handle inter-task interference and preserve reliable knowledge in
the novel federated semi-supervised learning scenarios.
14
Published as a conference paper at ICLR 2021
Table 6: Performance Comaprison utilizing AlexNet-Like architecture We use 100 clients for 100 rounds
for streaming task and 200 rounds for batch tasks. We measure global model accuracy, while varying experimen-
tal settings (i.e. fraction of available clients and the accessibility of labeled data).
Experiments based on AleXNet-Like Architecture
	Streaming-NonIID (F=1.0)		Batch-IID (LabelS-at-Client)		
	Labels-at-Client	LabelS-at-Server	F =0.05	F =0.10	F =0.20
Methods	Acc.(%)	Acc.(%)	Acc.(%)	Acc.(%)	Acc.(%)
FedAvg-SL	68.20 ± 0.29	-70.51 ± 0.11	47.23 ± 0.31	47.87 ± 0.73	48.73 ± 0.15
FedProx-SL		_ ^8.47 ± 0.13_ _	70.55 ± 0.72	47.54 ± 0.28	48.01 ± LL	49.20 ± 0.64
FedAvg-UDA	52.25 ± 0.04	— 一46.28 ± 0.32 ^ 一	-35.27±0.29 -	其20 ± %，	^36.21 口12
FedProx-UDA _ _	_ 52.84 ± 0.15_ _	46.35 ± 0.31	_ 34.94 ±0.46	36.67 ± 0.73	35.80 ± 0.43
FedAvg-FixMatch	—57.09 ± 0?89- 一	52.67 ± 0.78	一 32.33 ±0.51 ^	36.27 ± 03，	^37.61 ^.05
FedProx-FixMatCh	_ 57.12 ± 0.41_ _	51.51 ± 0.32	36.83 ± 0.23	36.37 ± 0.39	37.40 ± 0.18
FedMatch (OursΓ	63.84 ± 0.18	59.12 ± 0.35	41.67 ± 0.32	41.97 ± 0i1	一42.18N “27
B.3	Experiments on Real-world Dataset
To show our method consistently work with real-world dataset, we further conduct experiment on
COVID-19 Radiography Dataset (Chowdhury et al., 2020) which is a real-world dataset that consists
of X-ray images from COVID and non-COVID patients. The COVID-19 dataset contains 219 X-ray
images from the patients diagnosed of COVID-19, 1341 images from normal (healthy) patients, and
1341 images from patients diagnosed of viral pneumonia. We use 10 clients with a fraction of 1.0
(communication rate). We use 5 labeled examples per class for each client, leaving the rest of the
image as unlabeled. We find this setting to be realistic as the datasets are, since we may not not have
skilled radiologists that can fully label the X-ray images taken at the local hospitals. We compared
our method against baselines which naively combine semi-supervised learning and federated learning
models during training 100 rounds. As shown in the left table of Figure 9, our method consistently
outperforms all base models with large margins (around 4%p-10%p) in both scenarios. The test
accuracy curves in Figure 9, we can see that our method trains faster than the base models and shows
more stability during training. We believe that these additional experimental results further strengthen
our paper.
B.4	Backbone Architecture
Most existing works on federated learning considers smaller networks since the focus is on-device
learning of low-resource devices, and thus we utilize a smaller backbone networks than ResNet-9.
To verify that our method also successfully works on the smaller & different architecture, we adopt
AlexNet-Like (Serra et al., 2018), of which the first three layers are convolutional neural layers with
64, 128, and 256 filters with the 4, 3, and 2 kernel sizes followed by the two fully-connected layers of
2048 units, while 2 × 2 max-pooling layers are followed after each convolutional layer. In Table 6, for
both Streaming-NonIID and Batch-IID tasks, our methods still outperforms all naive Fed-SSL models
with the similar tendency with that of the results based on ResNet-9. This shows that our methods
can be applied to the smaller and different base networks, and still effectively utilize inter-client and
reliable knowledge across multiple clients than naive algorithms.
B.5	Fraction of Available Clients per Communication Round
To see the effect of participation rate of clients, we increase the fraction of available clients per com-
munication round in a range of 0.05, 0.10, and 0.2. This means, for every round, server can connect
to the arbitrary 5, 10, and 20 available clients out of 100 clients, and perform distributed learning
on each individual local data through each client and updates global knowledge by aggregating the
locally-learned knowledge. The experimental results are shown in Table 6 Batch-IID. We observe
that the performances of all models are slightly improved when the faction increases. This is natural
that the more knowledge the client updates, the more the global performance is improved. We are not
able to find any extraordinary phenomenon on the fraction of the number of clients per round.
15