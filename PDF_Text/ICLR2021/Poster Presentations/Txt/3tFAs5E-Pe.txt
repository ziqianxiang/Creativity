Published as a conference paper at ICLR 2021
Continuous Wasserstein-2 Barycenter
Estimation without Minimax Optimization
Alexander Korotin
Skolkovo Institute of Science and Technology
Advanced Data Analytics in Science and
Engineering Group
Moscow, Russia
a.korotin@skoltech.ru
Justin Solomon
Massachusetts Institute of Technology
Geometric Data Processing Group
Cambridge, Massachusetts, USA
jsolomon@mit.edu
Lingxiao Li
Massachusetts Institute of Technology
Geometric Data Processing Group
Cambridge, Massachusetts, USA
lingxiao@mit.edu
Evgeny Burnaev
Skolkovo Institute of Science and Technology
Advanced Data Analytics in Science
and Engineering Group
Moscow, Russia
e.burnaev@skoltech.ru
Ab stract
Wasserstein barycenters provide a geometric notion of the weighted average of
probability measures based on optimal transport. In this paper, we present a scal-
able algorithm to compute Wasserstein-2 barycenters given sample access to the
input measures, which are not restricted to being discrete. While past approaches
rely on entropic or quadratic regularization, we employ input convex neural net-
works and cycle-consistency regularization to avoid introducing bias. As a result,
our approach does not resort to minimax optimization. We provide theoretical
analysis on error bounds as well as empirical evidence of the effectiveness of the
proposed approach in low-dimensional qualitative scenarios and high-dimensional
quantitative experiments.
1	Introduction
Wasserstein barycenters have become popular due to their ability to represent the average of probabil-
ity measures in a geometrically meaningful way. Techniques for computing Wasserstein barycenters
have been successfully applied to many computational problems. In image processing, Wasserstein
barycenters are used for color and style transfer (Rabin et al., 2014; Mroueh, 2019), and texture
synthesis (Rabin et al., 2011). In geometry processing, shape interpolation can be done by computing
barycenters (Solomon et al., 2015). In online machine learning, barycenters are used for aggregating
probabilistic predictions of experts (Korotin et al., 2019b). Within the context of Bayesian infer-
ence, the barycenter of subset posteriors converges to the full data posterior, thus enabling efficient
computational methods based on finding the barycenters (Srivastava et al., 2015; 2018).
Fast and accurate barycenter algorithms exist for discrete distributions (See Peyre et al. (2019) for a
survey), while for continuous distributions the situation is more difficult and remains unexplored until
recently (Li et al., 2020; Fan et al., 2020; Cohen et al., 2020). The discrete methods scale poorly with
the number of support points of the barycenter and thus cannot approximate continuous barycenters
well, especially in high dimensions.
In this paper, we present a method to compute Wasserstein-2 barycenters of continuous distributions
based on a novel regularized dual formulation where the convex potentials are parameterized by input
convex neural networks (Amos et al., 2017). Our algorithm is straightforward without introducing
bias (e.g. Li et al. (2020)) or requiring minimax optimization (e.g. Fan et al. (2020)). This is
made possible by combining a new congruence regularizing term combined with cycle-consistency
regularization (Korotin et al., 2019a). As we will show in the analysis, thanks to the properties of
1
Published as a conference paper at ICLR 2021
Wasserstein-2 distances, the gradients of the resulting convex potentials “push” the input distributions
close to the true barycenter, allowing good approximation of the barycenter.
2	Preliminaries
We denote the set of all Borel probability measures on RD with finite second moment by P2(RD).
We use P2,ac(RD) ⊂ P2(RD) to denote the subset of all absolutely continuous measures (w.r.t. the
Lebesgue measure).
Wasserstein-2 distance. For P, Q ∈ P2 (RD), the Wasserstein-2 distance is defined by
W2(P, Q) =f ∏∈miP^Q) yRd ×rd kx-yk2 dπ(x,y),
(1)
where Π(P, Q) is the set of probability measures on RD × RD whose marginals are P, Q, respectively.
This definition is known as Kantorovich’s primal form of transport distance (Kantorovitch, 1958).
The Wasserstein-2 distance W2 is well-studied in the theory of optimal transport (Brenier, 1991;
McCann et al., 1995). In particular, it has a dual formulation (Villani, 2003):
W22 (P, Q)
L kxk2dP(χ) + £ TdQ(y)-
min
ψ∈Conv
ψ	ψ(χ)dP(χ) + [ Ψ(y)dQ(y),
RD	RD
(2)
where the minimum is taken over all the convex functions (potentials) ψ	: RD → R ∪ {∞}, and
ψ(y) = maXχ∈RD ((x, y)- ψ(x)) : RD → R ∪{∞} is the convex conjugate of ψ (Fenchel, 1949),
which is also a convex function. The optimal potential ψ* is defined UP to an additive constant.
Brenier (1991) shows that if P does not give mass to sets of dimensions at most D - 1, then the
optimal plan ∏ is uniquely determined by ∏ = [idRD, T*]]P, where T* : RD → RD is the unique
solution to the Monge’s problem
T * = arg min Z kx - T (x)k2 dP(x).	(3)
T]P=Q RD	2
The connection between T* and the dual formulation (2) is that T* = Vψ*, where ψ* is the optimal
solution of (2). Additionally, ifQ does not give mass to sets of dimensions at most D - 1, then T* is
invertible and
T *(x) = Vψ*(x) = (VF)T(x),	(T *)-1(y) = Vψ*(y) = (Vψ*)-1 (y).
In particular, the above discussion applies to the case where P, Q ∈ P2,ac(RD).
Wasserstein-2 barycenter. Let P1 , . . . , PN ∈ P2,ac(RD). Then, their barycenter w.r.t. weights
α1, . . . , αN (αn > 0 and PnN=1 αn = 1) is
N
P =f arg min XαnW22(Pn,P).	(4)
P∈P2 (RD ) n=1
Throughout this paper, we assume that at least one of P1, . . . , PN ∈ P2,ac(RD) has bounded density.
Under this assumption, P is unique and absolutely continuous, i.e., P ∈ P2,ac(RD), and it has
bounded density (Agueh & Carlier, 2011, Definition 3.6 & Theorem 5.1).
For n ∈ {1, 2,. ..,N}, let (ψn,ψn) be the optimal pair of (mutually) conjugate potentials that
transport Pn to P, i.e., Vψ*]Pn = P and Vψn]P = Pn. Then {ψn} satisfy
M 「一,、	r	3 一、 kxk2
EanVψn (X)= X	and	fanψn (x) = -2- + c.	(5)
n=1	n=1
for all x ∈ RD (AgUeh & Carlier, 2011; Alvarez-Esteban et al., 2016). Since optimal potentials are
defined up to a constant, for convenience, we set c = 0. The condition (5) serves as the basis for our
algorithm for computing Wasserstein-2 barycenters. We saythatpotentialsψ1, . . . ,ψN are congruent
w.r.t. weights aι,...,an if their conjugate potentials satisfy (5), i.e., PD=I αnψn(x) = kx2k- for all
x ∈ RD.
2
Published as a conference paper at ICLR 2021
3	Related Work
Most algorithms in the field of computational optimal transport are designed for the discrete setting
where the input distributions have finite support; see the recent survey by Peyre et al. (2019) for
discussion. A particular popular line of algorithms are based on entropic regularization that gives rise
to the famous Sinkhorn iteration (Cuturi, 2013; Cuturi & Doucet, 2014). These methods are typically
limited to a support of 105 - 106 points before the problem becomes computationally infeasible.
Similarly, discrete barycenter methods (Cuturi & Doucet, 2014), particularly the ones that rely on
a fixed support for the barycenter (Dvurechenskii et al., 2018; Staib et al., 2017), cannot provide
precise approximation of continuous barycenters in high dimensions, since a large number of samples
is needed; see experiments in Fan et al. (2020, §4.3) for an example. Thus we focus on the existing
literature in the continuous setting.
Computation of Wasserstein-2 distances and maps. Genevay et al. (2016) demonstrate the pos-
sibility of computing Wasserstein distances given only sample access to the distributions by param-
eterizing the dual potentials as functions in the reproducing kernel Hilbert spaces. Based on this
realization, Seguy et al. (2017) propose a similar method but use neural networks to parameterize
the potentials, using entropic or L2 regularization w.r.t. P × Q to keep the potentials approximately
conjugate. The transport map is recovered from optimized potentials via barycentric projection.
As we note in §2, W2 enjoys many useful theoretical properties. For example, the optimal potential
ψ* is convex, and the corresponding optimal transport map is given by Vψ*. By exploiting these
properties, Makkuva et al. (2019) propose a minimax optimization algorithm for recovering transport
maps, using input convex neural networks (ICNNs) (Amos et al., 2017) to approximate the potentials.
An alternative to entropic regularization is the cycle-consistency regularization proposed by Korotin
et al. (2019a). It uses the property that the gradients of optimal dual potentials are inverses of each
other. The imposed regularizer requires integration only over the marginal measures P and Q, instead
of over P × Q as required by entropy-based alternatives. Their method converges faster than the
minimax method since it does not have an inner optimization cycle.
Xie et al. (2019) propose using two generative models with a shared latent space to implicitly compute
the optimal transport correspondence between P and Q. Based on the obtained correspondence, the
authors are able to compute the optimal transport distance between the distributions.
Computation of Wasserstein-2 barycenters. A few recent techniques tackle the barycenter prob-
lem (4) using continuous rather than discrete approximations of the barycenter:
•	Measure-based (generative) optimization: Problem (4) optimizes over probability mea-
sures. This can be done using the generic algorithm by Cohen et al. (2020) who employ generative
networks to compute barycenters w.r.t. arbitrary discrepancies. They test their method with the
maximum mean discrepancy (MMD) and Sinkhorn divergence. This approach suffers from the
usual limitations of generative models such as mode collapse. Applying it to W2 barycenters
requires estimation of W22(Pn, P). Fan et al. (2020) test this approach using the minimax method
by Makkuva et al. (2019), but they end up with a challenging min-max-min problem.
•	Potential-based optimization： Li et al. (2020) recover the optimal potentials {ψn} via a
non-minimax regularized dual formulation. No generative model is needed: the barycenter is
recovered by pushing forward measures using gradients of potentials or by barycentric projection.
4	Methods
inspired by Li et al. (2020) we use a potential-based approach and recover the barycenter by using
gradients of the potentials as pushforward maps. The main differences are: (1) we restrict the
potentials to be convex, (2) we enforce congruence via a regularizing term, and (3) our formulation
does not introduce bias, meaning the optimal solution of our formulation gives the true barycenter.
3
Published as a conference paper at ICLR 2021
4.1	Deriving the Dual Problem
Let P be the true barycenter. Our goal is to recover the optimal potentials {ψn ,ψn } mapping the
input measures Pn into P.
To start, we express the barycenter objective (4) after substituting the dual formulation (2):
N
X αnW2(Pn, P)
n=1
min
{ψn}∈Conv
NN
X an	ψn(x)dPn(x) + X an	ψn(y)dP(y)
n=1	RD	n=1	RD
(6)
The minimum is attained not just among convex potentials {ψn}, but among congruent potentials
(see discussion under (5)); thus, we can add the constraint that {ψn} are congruent to (6). Hence,
N
X anW2(Pn, P)
n=1
N
an
n=1
- min
{ψn } congruent
N
an	ψn(y)dPn(y)	.
n=1	RD
'-----------V-----------}
MUItiCorr({αn,Pn}∣{ψn})
(7)
To transition from (6) to (7), we used the fact that for congruent {ψn} we have
Pn=I anψn(X)=呼,so Pn=I Rrd anψn(y)dP(y) = RRD kyf-dP(y).
We call the value inside the minimum in (7) the multiple correlation of {Pn} with weights {an}
w.r.t. potentials {ψn}. Notice that the true barycenter P appears nowhere on the right side of (7).
Thus the optimal potentials {ψnl} can be recovered by solving the following
min	MultiCorr ({ an, Pn}∣{ψn}) = min
{ψn } congruent	{ψn } congruent
N
an
n=1	RD
ψn(y)dPn(y)
(8)
4.2	Imposing the Congruence Condition
It is challenging to impose the congruence condition on convex potentials. What if we relax the
congruence condition? The following theorem bounds how close a set of convex potentials {ψn} is
to {ψn } in terms of the difference of multiple correlation.
Theorem 4.1. Let P ∈ P2,ac(RD) be the barycenter of Pi,...,Pn ∈ P2,ac(RD) w.r.t. weights
ɑι,...,ɑN. Let {ψn} be the optimal congruent potentials of the barycenter problem. Sup-
pose we have B-smooth1 convex potentials {ψn } for some B ∈ [0, +∞], and denote ∆ =
MUItiCorr({an, Pn} ∣{ψn}) - MUItiCorr({an, Pn} | {ψn}). Then,
∆+
Z X [anψn(y) -	]dP(y) ≥
RD n=1	2
X-----------------------------/
1N
2B E ankVψn (X)- Vψn(X)IlPn .
n=1
(9)
Congruence mismatch
Here ∣∣ ∙ ∣∣μ denotes the norm induced by inner product in Hilbert space L2 (RD → RD, μ). We call
the second term on the left of (9) the congruence mismatch.
We prove this in Appendix B. Note that if the congruence mismatch is non-positive, then
1 N	ι N	_
∆ ≥ 2B J>nkVψn (x) -Vψn(x)kPn ≥B ^>nW2(Vψn]Pn, P),	(10)
n=1	n=1
where the last inequality of (10) follows from (Korotin et al., 2019a, Lemma A.2). From (10),
we conclude that for all n ∈ {1,..., N}, we have W2(Vψn]Pn, P) ≤ B∆. ThiS shows that if
the congruence mismatch is non-positive, then ∆, the difference in multiple correlation, provides
1We say that a diffirentiable function f : RD → R is B-smooth if its gradient Vf is B-Lipschitz.
4
Published as a conference paper at ICLR 2021
an upper bound for the Wasserstein-2 distance between the true barycenter and each pushforward
Vψn]Pn. ThisjUstifies the use of Vψn]Pn to recover the barycenter. Notice for optimal potentials,
the congruence mismatch is zero.
Thus to penalize positive congruence mismatch, we introduce a regularizing term
而{αn}E) =f HX αF)-T
dP(y).
+
(11)
Because we take the positive part of the integrand of (9) to get (11) and that the right side of (9) is
non-negative, we have
[MultiCorr({αn,Pn} | {ψn}) + 1 ∙ RP({an}, {ψ;})] - MUItiCorr({αn,Pn} | {ψn}) ≥ 0
for all convex potentials {ψn}. Onthe other hand, for optimal potentials {ψn} = {ψn},the inequality
turns into equality, implying that adding the regularizing term 1 ∙ Rp({ɑn}, {ψn}) to (8) will not
introduce bias - the optimal solution still yields {ψn}.
However, evaluating (11) exactly requires knowing the true barycenter P a priori. To remedy this
issue, one may replace P with another absolutely continuous measure T ∙ P (τ ≥ 1 and P is a
probability measure) whose density bounds that of P from above almost everywhere. In this case,
T ∙Rb({an}, {ψn}) = T ∙[
RD
[X ɑnψn(y) - kyk2]+dP ≥ RP({αn}, {ψ□).
n=1
(12)
Hence We obtain the following regularized version of (8) where {ψn } is the optimal solution:
mm	[MultiCorr({αn ,Pn}∣{ψn})+ T ∙RP({an}, {ψn})].	(13)
{ψn}∈Conv
C 1 ,♦	T1⅞ •	. 1 ∙	X 1	• 1 .1	1	r-ΓΓΛ 、	1	.
Selecting a measure T ∙ PIS not obvious. Consider the case when {Pn} are supported on compact sets
X1 , . . . , XN ⊂ RD and P1 has density upper bounded by h < ∞. In this scenario, the barycenter
density is upper bounded by h ∙ α-D (Alvarez-Esteban et al., 2016, Remark 3.2). Thus, the measure
T ∙ P supported on COnVeXHUll(Xι,..., XN) with this density is an upper bound for P. We will
address the question of how to choose T, P properly in practice in §4.4.
4.3	Enforcing Conjugacy of Potentials Pairs
Throughout this subsection, We assume the upper bound finite measure T ∙ P of the P is known. The
optimization problem (13) involves not only the potentials {ψn}, but also their conjugates {ψn}.
This brings practical difficulty since evaluating conjugate potentials is hard (Korotin et al., 2019a).
Instead we parameterize potentials ψn and ψn separately using input convex neural networks (ICNN)
as ψn and ψ% respectively. We add an additional cycle-consistency regularizer to enfore the conjugacy
of ψn and ψn as in Korotin et al. (2019a). This regularizer is defined as
Rpn (ψn ,ψn) =f
Note that RPn (ψn, ψη) = 0 this condition is necessary for ψn and ψ* to be conjugate with each other.
Also, it is a sufficient condition for convex functions to be conjugates up to an additive constant.
We use one-sided regularization. In our case, computing the regularizer of the other direction
∣∣Vψn ◦ Vψn 一 id>D HP is infeasible, since P is unknown. If fact, Korotin et al. (2019a) demonstrates
that such one-sided condition is sufficient.
∣	HVψn ◦ Vψn (x) - χ∣2 dPn(x) = ∣∣vψn ◦ Vψn - idRD HPn.
RD
5
Published as a conference paper at ICLR 2021
In this way We use 2N input convex neural networks for {ψn ,ψn}. By adding the new cycle
consistency regularizer into (13), we obtain our final objective:
Approximate multiple correlation
ʌ
≈ψn (x)
N
___ N	___
τ∙Rb({ψ1^})+λXɑnRPn(ψn,ψ1). (14)
'c - 7Z /	n=1
Congruence reg. 、	.z 	/
Cycle regularizer
Note that we express the aproximate multiple correlation by using both potentials {ψn} and {ψn}.
This is done to eliminate the freedom of an additive constant on {ψn} that is not addressed by cycle
r，+、
regularization. We denote the entire objective as MultiCorr({Pn} | {ψ+}, {ψ4；τ, P, λ). Analogous
to Theorem 4.1, we have following result showing that this new objective enjoys the same properties
as the unregularized version from (8).
Theorem 4.2. Let P ∈ P2,ac(RD) be the barycenter of Pi,...,Pn ∈ P2,ac(RD) w.r.t. weights
ɑι,...,aN. Let {ψn} be the optimal congruent potentials ofthe barycenter problem. Suppose we
have τ, P such that T ≥ 1 and T ∙ P ≥ P. Suppose we have convex potentials {ψn} and β*-strongly
convex and B^ -smooth convex potentials {ψn} with 0 < β* ≤ B^ < ∞ and λ > 2(；；y.Then
MultiCorr({an,Pn} | {ψn},{ψn};τ,P,λ) ≥ MultiCorr({an,Pn} | {ψn}).	(15)
Denote ∆ = MultiCorr({an,Pn} | {ψn}, {ψn};t, b,λ) — MultiCorr({an,Pn} | {ψn}). Thenfor
all n ∈ {1, . . . , N}, we have
V 2∆
αn
(16)
Informally, Theorem 4.2 states that the better we solve the regularized dual problem, (14) the closer
we expect each Vψn]Pn to be to the true barycenter P in W2. It follows from (15) that our final
objective (14) is unbiased: the optimal solution is obtained by {ψn, ψn}.
4.4	Practical Aspects and Optimization Procedure
In practice, even if the choice of τ, P does not satisfy T ∙ P ≥ P, we observe the pushforward
measures Vψn ]Pn often converge to P. To partially bridge the gap between theory and practice, we
dynamically update the measure P so that after each optimization step we set (for γ ∈ [0, 1])
N
P0 := Y ∙ P +(1 — Y) ∙ X an ∙ [Vψt]Pn],
n=1
i.e., the probability measure P0 is a mixture of the given initial measure P and the current barycenter
estimates {Vψ+]Pn}. For the initial P one may use the barycenter of {N(μpn, Σpn)}. It can be
efficiently computed via an iterative fixed point algorithm (Alvarez-Esteban et al., 2016; Chewi et al.,
2020). During the optimization, these estimates become closer to the true barycenter and can thus
improve the congruence regularizer (12).
We use mini-batch stochastic gradient descent to solve (14) where the integration is done by Monte-
Carlo sampling from input measures {Pn} and regularization measure P, similar to Li et al. (2020).
We provide the detailed optimization procedure (Algorithm 1) and discuss its computational complex-
ity in Appendix A. In Appendix C.3, we demonstrate that the impact of the considered regularization
on our model: we show that cycle consistency and the congruence condition of the potentials are well
satisfied.
5 Experiments
The code is written on PyTorch framework and is publicly available at
6
Published as a conference paper at ICLR 2021
https://github.com/iamalexkorotin/Wasserstein2Barycenters.
We compare our method [CW2B] with the potential-based method [CRWB] by Li et al. (2020)
(with Wasserstein-2 distance and L2-regularization) and with the measure-based generative method
[SCW2B] by Fan et al. (2020). An considered methods recover 2N potentials {ψn, ψn} ≈ {ψn, ψn}
and approximate the barycenter as pushforward measures {Vψn ]Pn}. Regularization in [CRWB]
allows access to the joint density of the transport plan, a feature of their method that we do not
consider here. The method [SCW2B] additionally outputs a generated barycenter g]S ≈ P where g
is the generative network and S is the input noise distribution.
To assess the quality of the computed barycenter, we consider the unexplained variance percentage
W (P,P)
,二、
defined as UVP(P) = 100
%. When UVP ≈ 0%, P is a good approximation of P. For values
1∕2 Var(P)
≥ 100%, the distribution P is undesirable: a trivial baseline P0 = δ晞[y] achieves UVP(P0) = 100%.
Evaluating UVP in high dimensions is infeasible: empirical estimates of W22 are unreliable due
to high sample complexity (Weed et al., 2019). To overcome this issue, for barycenters given by
Vψn]Pn We use L2-UVP defined by
LSψ, Pn)=f 100 kvGnkPn %
≥ uvP(vψn ]Pn),
(17)
where the inequality in brackets follows from (Korotin et al., 2019a, Lemma A.2). We report the
weighted average of L2-UVP of all pushforward measures w.r.t. the weights αn . For barycenters
given in an implicit form g]S, we compute the Bures-Wasserstein UVP defined by
bw2-UVS=f 100 BWVIF %
≤ UVP(g]S) ,
(18)
where BW2(P, Q) = W2(N(μp, Σp),N(μQ, ∑q)) is the Bures-Wasserstein metric and we use
μp, Σp to denote the mean and the covariance of a distribution P (CheWi et al., 2020). It is known that
BW22 lower-bounds W22 (Dowson & Landau, 1982), so the inequality in the brackets of (18) follows.
A detailed discussion of the adopted metrics is given in Appendix C.2.
5.1	High-Dimensional Location-Scatter Experiments
7
Published as a conference paper at ICLR 2021
In this section, we consider N = 4 with (α1, . . . , α4) = (0.1, 0.2, 0.3, 0.4) as weights. We consider
the location-scatter family of distributions (Alvarez-Esteban et al., 2016, §4) whose true barycenter
can be computed. Let P0 ∈ P2,ac and define the following location-scatter family of distributions
F (Po) = {fs,u ]Po | S ∈ MD×d ,u ∈ RD}, Wherefs,u : RD → RD isalinearmap fs^(χ)=
Sx + U with positive definite matrix S ∈ MD×D. When {Pn } ⊂ F (Po), their barycenter P is also
an element of F(P0) and can be computed via fixed-point iterations (Alvarez-Esteban et al., 2016).
Figure 1a shows a 2-dimensional location-scatter family generated by using the Swiss roll distribution
as Po. The true barycenter is shown in Figure 1b. The generated barycenter g]S of [SCW2B] is given
in Figure 1c. The pushforward measures Vψn ]Pn of each method are provided in Figures 1d, 1e,
1f, respectively. In this example, the pushforward measures Vψn]Pn all reasonably approximate P,
whereas the generated barycenter g]S of [SCW2B] (Figure 1c) visibly underfits.
For quantitative comparison, we consider two choices for Po： the D-dimensional standard Gaussian
distribution and the uniform distribution on [一√3, +√3]D. Each Pn is constructed as fsτΛsn,o]Po ∈
F (Po), where Sn is a random rotation matrix and A is diagonal with entries [ 1 b0,1 b1,..., 2] where
b = D-14. We consider only centered distributions (i.e. zero mean) because the barycenter of
non-centered {Pn} ∈ P2,ac(RD) is the barycenter of {P：} shifted by PN=I αnμpn, where {Pn} are
centered copies of {Pn} (Alvarez-Esteban et al., 2016). Results are shown in Table 1 and 2.
In these experiments, our method outperforms [CRWB] and [SCW2B]. For [CRWB], dimension
〜16 is the breakpoint: the method does not scale well to higher dimensions. [SCW2B] scales with
the increasing dimension better, but its errors L2-UVP and BW22-UVP are twice as high as ours.
This is likely due to the generative approximation and the difficult min-max-min optimization in
[SCW2B]. For completeness, we also compare our algorithm to the proposed in Cuturi & Doucet
(2014) which approximates the barycenter by a discrete distribution on a fixed number of free-support
points. In our experiment, similar to Li et al. (2020), we set 5000 as the support size. As expected,
the BW22-UVP error of the method increases drastically as the dimension grows and the method is
outperformed by our approach.
To show the scalability of our method with the number of input distributions N , we conduct an analo-
gous experiment with a high-dimensional location-scatter family for N = 20. We set a： = N(2N+i)
for n = 1, 2,…,20 and choose the uniform distribution on [—√3, +√3]D as Po and construct distri-
butions Pn ∈ F(Po) as before. The results for dimensions 32, 64 and 128 are provided in Table 3.
Similar to the results from Tables 1 and 2, we see that our method outperforms the alternatives.
Metric	Method	D=2	4	8	16	32	64	128	256
bw2-uvp, %	[FCWB], CUtUri & DOUCet (2014)	0.7	0.68	1.41	3.87	8.85	14.08	18.11	21.33
	[SCW2B], (Fan et al., 2020)	0.07	0.09	0.16	0.28	0.43	0.59	1.28	2.85
L2-UVP, % (potentials)		0.08	0.10	0.17	0.29	0.47	0.63	1.14	1.50
	[CRWB],(Li et al., 2020)	0.99	2.52	8.62		67.01		>100			
	[CW2B], ours					ɪɪ	0.24				
Table 1: Comparison of UVP for the case {Pn} ⊂ F(Po), Po = N (0, ID), N = 4.
Metric	Method	D=2	4	8	16	32	64	128	256
bw2-uvp, %	[FCWB], CUtUri & DOUCet (2014)	0.64	0.77	1.22	3.75	8.92	14.3	18.46	21.64
	[SCW2B], (Fan et al., 2020)	o.12	o.ιo	o.19	o.29	o.46	o.6	1.38	2.9
L2-UVP, % (potentials)		o.17	o.12	o.2	o.31	o.47	o.62	1.21	1.52
	[CRWB],(Li et al., 2020)	o.58	1.83	8.o9		55.17		 > ιoo			
	[CW2B], ours	o.17	ɪɪ			o.2						
Table 2: Comparison of UVP for the case {Pn} ⊂ F (Po), Po = Uniform([-√3, +√3]D), N = 4.
Metric	Method	D=32	64	128
BW2-UVP, %	[FCWB], CUtUri & DOUCet (2014)	14.09	26.21	38.43
	[SCW2 B], (Fan et al., 2020)	o.62	o.93	1.83
-L2-UVP, %- (potentials)		o.6o	o.86	1.52
	[CW2B], ours	ɪɪ	ɪɪ	
Table 3: Comparison of UVP for the case {Pn} ⊂ F (Po), Po = Uniform ([-√3, +√3]D), N = 20.
5.2	Subset Posterior Aggregation
We apply our method to aggregate subset posterior distributions. The barycenter of subset posteriors
converges to the true posterior (Srivastava et al., 2018). Thus, computing the barycenter of subset
8
Published as a conference paper at ICLR 2021
posteriors is an efficient alternative to obtaining a full posterior in the big data setting (Srivastava
et al., 2015; Staib et al., 2017; Li et al., 2020).
Analogous to (Li et al., 2020), we consider Poisson and negative binomial regressions for predicting
the hourly number of bike rentals using features such as the day of the week and weather conditions.2
We consider the posterior on the 8-dimensional regression coefficients for both Poisson and negative
binomial regressions. We randomly split the data into N = 5 equally-sized subsets and obtain 105
samples from each subset posterior using the Stan library (Carpenter et al., 2017). This gives the
discrete uniform distributions {Pn } supported on the samples. As the ground truth barycenter P, We
consider the full dataset posterior also consisting of 105 points.
We use BW2-UVP(P, P) to compare the estimated barycenter P (pushforward measure Vψ( ]Pn or
generated measure g]S) With the true barycenter. The results are in Table 4. All considered methods
perform well (UVP< 2%), but our method outperforms the alternatives.
	Regression	SCW2B, (Fan et al., 2020)1		I [CRWB], (Li et al., 2020)1	CW2 B, our〕
		P = g]S		P = 7ψn ]Pn			
BW2 -UVP, %	Poisson	0.67	0.41	1.53	0.1 一
	negative binomial	0.15	0.15 —	1.26	—	
Table 4: Comparison of UVP for recovered barycenters in our subset posterior aggregation task.
5.3	Color Palette Averaging
For qualitative study, we apply our method to aggregating color palettes of images. For an RGB image
I, its color palette is defined by the discrete uniform distribution P(I) of all its pixels ∈ [0, 1]3. For 3
images {In } we compute the barycenter P of each color palette Pn = P(In) w.r.t. uniform weights
αn = 3. We apply each computed potential Vψ( pixel-wise to In to obtain the “pushforward” image
Vψn ]In. These “pushforward” images should be close to the barycenter P of {Pn}.
(a) Original images {In }.
(b) Color palettes {Pn} of original images.
(c) Images with averaged color palette {Vψ[ ]In }.
(d) Barycenter palettes {▽ ψ[]Pn }.
Figure 2: Results of our method applied to averaging color palettes of images.
The results are provided in Figure 2. Note that the image Vψχ ]I1 inherits certain attributes of images
I2 and I3 : the sky becomes bluer and the trees becomes greener. On the other hand, the sunlight in
images VψJ ]I2, Vψ3 ]I3 has acquired an orange tint, thanks to the dominance of orange in I1.
Acknowledgments
The Skoltech Advanced Data Analytics in Science and Engineering Group acknowledges the support
of Russian Foundation for Basic Research grant 20-01-00203, Skoltech-MIT NGP initiative and
thanks the Skoltech CDISE HPC Zhores cluster staff for computing cluster provision.
The MIT Geometric Data Processing group acknowledges the generous support of Army Research
Office grant W911NF2010168, of Air Force Office of Scientific Research award FA9550-19-1-031,
of National Science Foundation grant IIS-1838071, from the CSAIL Systems that Learn program,
from the MIT-IBM Watson AI Laboratory, from the Toyota-CSAIL Joint Research Center, from a
gift from Adobe Systems, from an MIT.nano Immersion Lab/NCSOFT Gaming Program seed grant,
and from the Skoltech-MIT Next Generation Program.
2http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset
9
Published as a conference paper at ICLR 2021
References
Martial Agueh and Guillaume Carlier. Barycenters in the wasserstein space. SIAM Journal on
Mathematical Analysis, 43(2):904-924, 2011.
Pedro C Alvarez-Esteban, E Del Barrio, JA Cuesta-Albertos, and C Matrdn. A fixed-point approach
to barycenters in wasserstein space. Journal of Mathematical Analysis and Applications, 441(2):
744-762, 2016.
Brandon Amos, Lei Xu, and J Zico Kolter. Input convex neural networks. In Proceedings of the 34th
International Conference on Machine Learning-Volume 70, pp. 146-155. JMLR. org, 2017.
Yann Brenier. Polar factorization and monotone rearrangement of vector-valued functions. Commu-
nications on pure and applied mathematics, 44(4):375-417, 1991.
Bob Carpenter, Andrew Gelman, Matthew D Hoffman, Daniel Lee, Ben Goodrich, Michael Be-
tancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. Stan: A probabilistic
programming language. Journal of statistical software, 76(1), 2017.
Sinho Chewi, Tyler Maunu, Philippe Rigollet, and Austin J Stromme. Gradient descent algorithms
for bures-wasserstein barycenters. arXiv preprint arXiv:2001.01700, 2020.
Samuel Cohen, Michael Arbel, and Marc Peter Deisenroth. Estimating barycenters of measures in
high dimensions. arXiv preprint arXiv:2007.07105, 2020.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in
neural information processing systems, pp. 2292-2300, 2013.
Marco Cuturi and Arnaud Doucet. Fast computation of wasserstein barycenters. 2014.
DC Dowson and BV Landau. The frechet distance between multivariate normal distributions. Journal
of multivariate analysis, 12(3):450-455, 1982.
Pavel Dvurechenskii, Darina Dvinskikh, Alexander Gasnikov, Cesar Uribe, and Angelia Nedich.
Decentralize and randomize: Faster algorithm for wasserstein barycenters. In Advances in Neural
Information Processing Systems, pp. 10760-10770, 2018.
Jiaojiao Fan, Amirhossein Taghvaei, and Yongxin Chen. Scalable computations of wasserstein
barycenter via input convex neural networks. arXiv preprint arXiv:2007.04462, 2020.
Werner Fenchel. On conjugate convex functions. Canadian Journal of Mathematics, 1(1):73-77,
1949.
Aude Genevay, Marco Cuturi, Gabriel Peyre, and Francis Bach. Stochastic optimization for large-
scale optimal transport. In Advances in neural information processing systems, pp. 3440-3448,
2016.
Sham Kakade, Shai Shalev-Shwartz, and Ambuj Tewari. On the duality of strong convexity and
strong smoothness: Learning applications and matrix regularization. Unpublished Manuscript,
http://ttic. uchicago. edu/shai/papers/KakadeShalevTewari09. pdf, 2(1), 2009.
Leonid Kantorovitch. On the translocation of masses. Management Science, 5(1):1-4, 1958.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alexander Korotin, Vage Egiazarian, Arip Asadulaev, Alexander Safin, and Evgeny Burnaev.
Wasserstein-2 generative networks. arXiv preprint arXiv:1909.13082, 2019a.
Alexander Korotin, Vladimir V’yugin, and Evgeny Burnaev. Integral mixability: a tool for efficient
online aggregation of functional and probabilistic forecasts. arXiv preprint arXiv:1912.07048,
2019b.
Lingxiao Li, Aude Genevay, Mikhail Yurochkin, and Justin Solomon. Continuous regularized
wasserstein barycenters. arXiv preprint arXiv:2008.12534, 2020.
10
Published as a conference paper at ICLR 2021
Ashok Vardhan Makkuva, Amirhossein Taghvaei, Sewoong Oh, and Jason D Lee. Optimal transport
mapping via input convex neural networks. arXiv preprint arXiv:1908.10962, 2019.
Robert J McCann et al. Existence and uniqueness of monotone measure-preserving maps. Duke
MathematicaIJournaI, 80(2):309-324,1995.
Youssef Mroueh. Wasserstein style transfer. arXiv preprint arXiv:1905.12828, 2019.
Barak A Pearlmutter. Fast exact multiplication by the hessian. Neural computation, 6(1):147-160,
1994.
Gabriel Peyra Marco Cuturi, et al. Computational optimal transport. Foundations and Trends® in
Machine Learning, 11(5-6):355-607, 2019.
Julien Rabin, Gabriel Peyra Julie Delon, and Marc Bernot. Wasserstein barycenter and its application
to texture mixing. In International Conference on Scale Space and Variational Methods in
Computer Vision, pp. 435-446. Springer, 2011.
Julien Rabin, Sira Ferradans, and Nicolas Papadakis. Adaptive color transfer with relaxed optimal
transport. In 2014 IEEE International Conference on Image Processing (ICIP), pp. 4852-4856.
IEEE, 2014.
Vivien Seguy, Bharath Bhushan Damodaran, Remi Flamary, Nicolas Courty, Antoine Rolet, and
Mathieu Blondel. Large-scale optimal transport and mapping estimation. arXiv preprint
arXiv:1711.02283, 2017.
Justin Solomon, Fernando De Goes, Gabriel Peyre, Marco Cuturi, Adrian Butscher, Andy Nguyen,
Tao Du, and Leonidas Guibas. Convolutional wasserstein distances: Efficient optimal transportation
on geometric domains. ACM Transactions on Graphics (TOG), 34(4):1-11, 2015.
Sanvesh Srivastava, Volkan Cevher, Quoc Dinh, and David Dunson. Wasp: Scalable bayes via
barycenters of subset posteriors. In Artificial Intelligence and Statistics, pp. 912-920, 2015.
Sanvesh Srivastava, Cheng Li, and David B Dunson. Scalable bayes via barycenter in wasserstein
space. The Journal of Machine Learning Research, 19(1):312-346, 2018.
Matthew Staib, Sebastian Claici, Justin M Solomon, and Stefanie Jegelka. Parallel streaming
Wasserstein barycenters. In Advances in Neural Information Processing Systems, pp. 2647-2658,
2017.
Cedric Villani. Topics in optimal transportation. Number 58. American Mathematical Soc., 2003.
Jonathan Weed, Francis Bach, et al. Sharp asymptotic and finite-sample rates of convergence of
empirical measures in wasserstein distance. Bernoulli, 25(4A):2620-2648, 2019.
Yujia Xie, Minshuo Chen, Haoming Jiang, Tuo Zhao, and Hongyuan Zha. On scalable and efficient
computation of large scale optimal transport. volume 97 of Proceedings of Machine Learning
Research, pp. 6882-6892, Long Beach, California, USA, 09-15 Jun 2019. PMLR. URL http:
//proceedings.mlr.press/v97/xie19a.html.
11
Published as a conference paper at ICLR 2021
A The Algorithm
The numerical procedure for solving our final objective (14) is given below.
Algorithm 1: Numerical Procedure for Optimizing Multiple Correlations (14)
Input:Distributions Pi,..., PN with sample access;
Weights α1, . . . , αN ≥ 0 with PnN=1 αn = 1;
Regularization distribution Pb0 given by a sampler;
Congruence regularizer coefficient τ ≥ 1;
Balancing coefficient γ ∈ [0, 1];
Cycle-consistency regularizer coefficient λ > 0;
2N ICNNS {ψθn,ψωn}；
Batch size K > 0;
for t = 1, 2, . . . do
1.	Sample batches Xn 〜Pn for all n = 1,..., N;
2.	Compute the pushforwards Yn = Vψθn ]Xn, for all n = 1,...,N;
3.	Sample batch Y0 〜P;
4.	Compute the Monte-Carlo estimate of the congruence regularizer:
1	3 V-「£	-—,、	IlylI2]
LCongruence := K^ ∙ Σ γn	α
n0ψωn0 (y)----2Γ] +
n=1	y∈Yn n0=1
where γo = Y and Yn = an ∙ (1 一 Y) for n = 1, 2,..., N;
5.	Compute the Monte-Carlo estimate of the cycle-consistency regularizer:
1N
LCyCle ：= KE
n=1
αn
E kvψωn ◦ vψθn (X)-Xk2
6.	Compute the Monte-Carlo estimate of multiple correlations:
一	£「	1 L. 一	-- .............1
LMUItiCorr ：= E αn ∙ KE [hx, vψθn (X)i - ψω⅛ (Vψθn (X))]；
n=1	x∈Xn
7.	Compute the total loss:
LTotal ：= LMultiCorr + λ ∙ LCycle + T ∙ LCongruence;
8.	Perform a gradient step over {θn,ωn} by using d^Total}：
end
Parametrization of the potentials. To parametrize potentials {ψθn, ψωn}, we use DenseICNN
(dense input convex neural network) with quadratic skip connections; see (Korotin et al., 2019a,
Appendix B.2). As an initialization step, we pre-train the potentials to satisfy
ψθn(X) ≈ lχk-	and	ψωn(y) ≈ *.
Such pre-training provides a good start for the networks: each ψθn is approximately conjugate to
the corresponding ψωn. On the other hand, the initial networks {ψθn } are approximate congruent
according to (5).
Computational Complexity. For a single training iteration, the time complexity of both forward
(evaluation) and backward (computing the gradient with respect to the parameters) passes through
the objective function (14) is O(NT). Here N is the number of input distributions and T is the time
taken by evaluating each individual potential (parameterized as a neural network) on a batch of points
sampled from either Pn or P. This claim follows from the well-known fact that gradient evaluation
vθhθ (X) of hθ ： RD → R, when parameterized as a neural network, requires time proportional
12
Published as a conference paper at ICLR 2021
to the size of the computational graph. Hence, gradient computation requires computational time
proportional to the time for evaluating the function hθ (x) itself. The same holds when computing the
derivative with respect to x. Then, for instance, computing the term Vψn ◦ Vψn (x) in (14) takes
O(T) time. The gradient of this term with respect to θ also takes O(T) time: Hessian-vector products
that appear can be calculated in O(T) time using the famous Hessian trick, see Pearlmutter (1994).
In practice, we compute all the gradients using automatic differentiation. We empirically measured
that for our DenSeICNN potentials, the computation of their gradient w.r.t. input x, i.e., Vψ* (x),
requires roughly 3-4x more time than the computation of ψ*(x).
B Proofs
In this section, we prove our main Theorems 4.1 and 4.2.
We use L2 (RD → RD, μ) to denote the Hilbert space of functions f : RD → RD with integrable
square w.r.t. a probability measure μ. The corresponding inner product for f1,f2 ∈ L2 (RD → RD, μ)
is denoted by
hf1,f2iμ =f /QhfI(X),f2(x)idμ(x),
where hfι(x), f2(x)i is the Euclidean dot product. We use ∣∣ ∙ ∣∣μ =，(., •)* to denote the norm
induced by the inner product in L2 (RD → RD, μ).
We also recall a useful property of lower semi-continuous convex function ψ : RD → R:
Vψ(x) = arg max [{y,x> — ψ(y)],
y∈RD
which follows from the fact that
y = arg max [(y,x)一 ψ(y)] ^⇒ X — Vψ(y) = 0.
y∈RD
We begin with the proof of Theorem 4.1.
Proof. We consider the difference between the estimated correlations and true ones:
NN
∆ = X an / ψn(x)dPn(X)- X &n / ψnn (x)dPn(x)=
n=1	RD	n=1	RD
Nf	__
Ean	[hVψn(x),xi — ψn (Vψn(x))] dPn(x)
n=1	RD
—
Nf	__
Ean	["ψn(X),xi- ψn (vc,(X))] dpn(X)
n=1	RD
where We twice use (19) for f = ψn and f = ψnb. We note that
NN
X αn	hvψn(X),xidPn(X) = X an	hy, vψ* (y)idP(y)
n=1	RD	n=1	RD
LDhy, X Qmy = LD hy,yidp(y) = kidRD kP,
where We use of Change-Of-Variable formula for Vψn]Pn = P and (5). Analogously,
NN
X αn	ψn (vψn (X))dPn (x) = X an	ψ (y)dP(y)=
n=1	RD	n=1	RD
IRoX an砾(y)dP(y) = £ TdP(y) = 2 ∣idRD ∣P.
(19)
(20)
(21)
(22)
13
Published as a conference paper at ICLR 2021
Since each ψn is B-Smooth, We conclude that ψn is 击-strongly convex, see (Kakade et al., 2009).
Thus, we have
ψn (Vψn (X))) ≥
1
ψn(Vψn(X)))	+ hVψn ◦ Vψn (X),	Vψn (X)- Vψn(X)i	+ —kVψn (X)- Vψn(X)k2	=
X-------{z----}	2B
=x
@n(V@n(X))) + hX, V@n (X)- V@n(X)i + 费|尸也(X)- 0/叫匕(23)
or equivalently
―麻①时(X))) ≥ ―麻①次(X)))+ hX, Nψ (X)-V@n(X)i + ^BkV以(X)-V@n(X)k2 . (24)
We integrate (24) W.r.t. Pn and sum over n = 1, 2, . . . , N With Weights αn:
N
- αn
n=1	RD
N
-	αnhX, Vψn(X)iPn +
n=1
N
-nX=1αnZRD
N
ΨΣ(Vψn(X))dPn(X) + X αnhX, Vψn(X)iPn
n=1
N1
Ean 菰步若(X)- V%(X)kPn
n=1
NN
- X anψn(y)dP(y) + X anhX, Nψ (X)〉P,
RD n=1	n=1
n-
NN
αnhX, Vψn(X)iPn +	αn2BkVψn (x) -Vψn(X)kPn
n=1	n=1
We note that
-Z X anψn(y)dP(y) = Z [kyk-- X anψn(y)] dP(y) - Z kyk-dP(y)
RD n=1	RD	2	n=1	RD 2
Z [⅛7	X anψn(y)]dP(y) - 2 IIidRD kp.
RD 2	n=1	2
NoW We substitute (25), (26), (21) and (22) into (20) to obtain (9).
(25)
(26)
□
Next, We prove Theorem 4.2.
Proof. Since ψn is β^ strongly convex, its conjugate ψn is 吉-smooth, i.e. has 寺-Lipschitz gradient
Vψn (Kakade et al., 2009). Thus, for all x, x0 ∈ RD:
kVψn(χ) - Vψn(χ0)k2 ≤ (jɪ)2 ∙kχ-χ0k2.
We substitute χ0 = Vψη ◦ Vψn (y) = (Vψη) 1 ◦ Vψn (y) and obtain:
1	,	C	IC	-Γ	<	C
kVψn(χ) - Vψn(χ)k2 ≤ (*)2kχ - Vψn ◦ Vψn(χ)k2.	(27)
β
Since the function ψn is BkSmooth, we have for all X ∈ RD:
B*..
ψn(Vψn(χ)) ≤ ψn(Vψn(χ)) + hVψn ◦ Vψn(χ), Vψn(χ) - Vψn(χ)i + —∣Vψn(χ) - Vψn(χ)k2,
'------V-----}	2
=x
14
Published as a conference paper at ICLR 2021
that is equivalent to:
B*
hχ, vψn(x)i- ψn(vψn(x)) ≥hχ, vψn(x)i - ψn(vψn(χ)) -■-kvψn(X)- vψn(χ)k2. (28)
、-----------{----------}	2
ψn(X)
We combine (28) with (27) to obtain
」..	-T,	」..	」.	B*	..	-T	κ ..c
hx, vψn(X)i - ψn(vψn(X)) ≥ ψn(X)- 2(/*)2 • 口出。- vψn ◦ vψnk2.	(29)
For every n = 1, 2, . . . , N we integrate (29) w.r.t. Pn and sum up the corresponding cycle-consistency
regularization term:
[[hχ, vψn(X))- ψn(Vψn(X))]dPn(X)+λ ∙ kvψn ◦ vψn - idRD kρn ≥
RD
B	Bt
IDψ (X)dPn(X) + (λ - 2βψ
)∙ kvψn ◦ vψn - idRD IlPn .
|
^^^^^{^^^^^™
RPn (ψn ,ψn)
}
We sum (30) for n = 1, 2, . . . , N w.r.t. weights αn to obtain:
N	F	__ N	_
X an	[hX, vψn(X)i - ψn (vψn(X))加八⑺+λ x αnRPn(ψn ,ψn) ≥
n=1	RD	n=1
n	F	n	Bt	—
Ean	ψ*3dPn(X)+ £ αn (λ - . * ) ∙Rpn (ψn ,ψn )
n=1	RD	n=1	2(β*)2
X-----------------}
MWtiCorr({αn,Pn}∣{ψn })
We add T ∙ RP({ψn}) to both sides of (31) to get
MUItiCorr({an,Pn} | {ψn}, {ψn}; τ, P,λ) ≥ MUItiCorr({an, Pn} | {ψ±}) +
ʌ — N	Bt	—
τ ∙Rb({ψn}) + £ an(λ - 2B>) ∙RPn(ψn, ψn).
We sUbstract MUltiCorr({αn, Pn} | {ψn* }) from both sides and Use Theorem 4.1 to obtain
(30)
(31)
f	N	—
∆ ≥ -	αnψn* (y) -
些]dP(y) + β X αnkvψn(x) - vψn (叫优 十
(32)
n=1
N
4-Γ	ɪ一,	,
T ∙RP({ψn}) + ]Tɑn(λ -
N
X αn(λ -
n=1
n=1
挤)∙Rpn(ψn ,ψi)+β*
指)∙RPn(ψn,ψ*) ≥
N
Xan∣vψn(x) -vψn(x)kPn.
n=1
(33)
(34)
In transition from (33) to (34), we explot the fact that the sUm of the first term of (32) with the
cb" -Γ∖∖	Bt
regularizer T ∙RP({ψn}). Since λ > 2(B^)2, from (34) We immediately conclude ∆ ≥ 0; i.e., the
multiple correlations upper bound (15) holds trUe. On the other hand, for every n = 1, 2, . . . , N
We have
2∆	2∆
kvψn(x) -vψn(X)kPn ≤ —寻 and kvψn ◦ vψn- idRD kPn ≤ —-—Brr. (35)
αnβ	an ∙ (λ - 2(β*)2 )
We combine the second part of (35) With (27) integrated W.r.t. Pn :
2∆
k ψn - ψnkPn ≤ an ∙(λ(β*)2 -号).	(36)
15
Published as a conference paper at ICLR 2021
Finally, We use the triangle inequality for ∣∣ ∙ ∣∣Pn and conclude
kvψn - vψn IlPn ≤ ∣vψn - vψn IlPn + ∣vψn - vψnIlPn ≤
i.e.,
1
λ(β 士 )2 -
2∆
≤ —
αn
(37)
w2(vψn]Pn,P) ≤ ∣vψn -vψn IlPn
BI
2
Where the first inequality folloWs from (Korotin et al., 2019a, Lemma A.2).
□
C Experimental details and extra results
In this section, We provide experimental details and additional results. In Subsection C.1, We
demonstrate qualitative results of computed barycenters in the 2-dimensional space. In Subsection
C.2, We discuss used metrics in more detail. In Subsection C.4, We list the used hyperparameters of
our method (CW2B) and methods [SCW2B], [CRWB].
C.1 Additional Toy Experiments in 2D
We provide additional qualitative examples of computed barycenters of probability measures on R2 .
In Figure 3, We consider the location-scatter family F(P0) with P0 = Uniform[-心,V3]D. In
principle, all the methods capture the true barycenter. HoWever, the generated distribution g]S of
[SCW2B] (Figure 3c) provides samples that lies outside of the actual barycenter’s support (Figure
3b). Also, in [CRWB] method, one of the potentials’ pushforWard measure (top-right in Figure 3e)
has visual artifacts.
In Figure 4, We consider the Gaussian Mixture example by (Fan et al., 2020). The barycenter
computed by [SCW2B] method (Figure 4b) suffers from the behavior similar to mode collapse.
16
Published as a conference paper at ICLR 2021
(a) Inputs {Pn }
(b)SCW2B g]S
(C) SCW2 B vψn ]Pn
(d) CRWB vψn ]Pn
Figure 4: BaryCenter of a two 2D Gaussian mixtures.
(e) CW2B Vψn ]Pn
C.2 Metrics
The unexplained varianCe perCentage (UVP) (introduCed in SeCtion 5) is a natural and straightforward
metriC to assess the quality of the Computed baryCenter. However, it is diffiCult to Compute in high
dimensions: it requires Computation of the Wasserstein-2 distanCe. Thus, we use different but highly
related metriCs L2-UVP and BW22-UVP.
To access the quality of the recovered potentials {ψn } We use L2-UVP defined in (17). L2-UVP
compares not just pushforward distribution Vψn ]Pn with the barycenter P, but also the resulting
transport map with the optimal transport map Vψn. It bounds UVP(Vψ[]Pn) from above, thanks
to (Korotin et al., 2019a, Lemma A.2). Besides, L2-UVP naturally admits unbiased Monte Carlo
estimates using random samples from Pn .
For measure-based optimization method, we also evaluate the quality of the generated measure g]S
using Bures-Wasserstein UVP defined in (18). For measures P, Q whose covariance matrices are not
degenerate, BW22 is given by
BW2(P, Q) = 1 kμp — μQ∣∣2 + [ 1Tr∑P + 1 Tr∑Q - TrH ∑Q∑P2)1L
2	22
Bures-Wasserstein metric compares P, Q by considering only their first and second moments. It is
known that BW22(P, Q) is a lower bound for W22(P, Q), see (Dowson & Landau, 1982). Thus, we
have BW22-UVP(g]S) ≤ UVP(g]S). In practice, to compute BW22-UVP(g]S), we estimate means
and covariance matrices of distributions by using 105 random samples.
C.3 Cycle Consistency and Congruence in Practice
To assess the effect of the regularization of cycle consistency and the congruence condition in practice,
we run the following sanity checks.
For cycle consistency, for each input distribution Pn we estimate (by drawing samples from Pn) the
value ∣∣Vψn ◦ Vψn (x) 一 x∣p"Var(Pn). This metric can be viewed as an analog of the L2-UVP
that we used for assessing the resulting transport maps. In all the experiments, this value does not
exceed 2%, which means that cycle consistency and hence conjugacy are satisfied well.
For the congruence condition, we need to check that PN=I αnψ[(x) = ∣∣x∣2/2. However, we do not
know any straightforward metric to check this exact condition that is scaled properly by the variance
of the distributions. Thus, we propose to use an alternative metric to check a slightly weaker condition
on gradients, e.g., that PN=I αnVψn(X)= x. This is weaker due to the ambiguity of the additive
constants. For this we can compute ∣∣ PN=I αnVψ∖(x) ― XkP/Var(P), where the denominator is
17
Published as a conference paper at ICLR 2021
the variance of the true barycenter. We computed this metric and found that it is also less than 2% in
all the cases, which means that congruence condition is mostly satisfied.
C.4 Training Hyperparameters
The code is written using the PyTorch framework. The networks are trained on a single GTX 1080Ti.
C.4. 1 WASSERSTEIN-2 CONTINUOUS BARYCENTERS (CW2B, OUR METHOD)
^	^	zT->^
Regularization. We use T = 5 and P = N(0, Id) in our congruence regularizer T ∙ RP. We use
λ = 10 for the cycle regularization λ ∙ RPn for all n = 1,2,... ,N.
Neural Networks (Potentials). To approximate potentials {ψn, ψn} in dimension D, We use
DenseICNN[2; max(64, 2D), max(64, 2D), max(32, D)]
With CELU activation function. DenseICNN is an input-convex dense architecture With additional
convex quadratic skip connections. Here 2 is the rank of each input-quadratic skip-connection’s
Hessian matrix. Each following number max(∙, ∙) represents the size of a hidden dense layer in the
sequantial part of the netWork. For detailed discussion of the architecture see (Korotin et al., 2019a,
Section B.2).
Training process. We perform training according to Algorithm 1 of Appendix A. We set batch size
K = 1024 and balancing coefficient γ = 0.2. We use Adam optimizer by (Kingma & Ba, 2014) with
a fixed learning rate 10-3. The total number of iterations is set to 50000.
C.4.2 SCALABLE COMPUTATION OF WASSERSTEIN BARYCENTERS (SCW2 B)
Generator Neural Network. For the input noise distribution of the generative model we use
S = N(0, ID). For the generative network g : RD → RD we use a fully-connected sequential ReLU
network with hidden layer sizes
[max(100, 2D), max(100, 2D), max(100, 2D)].
Before the main optimization, we pre-train the network to satisfy g(z) ≈ z for all z ∈ RD. This has
been empirically verified as a better option than random initialization of network’s weights.
Neural Networks (Potentials). We used exactly the same networks as in Subsection C.4.1.
Training process. We perform training according to the min-max-min procedure described by (Fan
et al., 2020, Algorithm 1). The batch size is set to 1024. We use Adam optimizer by (Kingma & Ba,
2014) with fixed learning rate 10-3 for potentials and 10-4 for generative network g. The number
of iterations of the outer cycle (min-max-min) number of iterations is set to 15000. Following (Fan
et al., 2020), we use 10 iterations per the middle cycle (min-max-min) and 6 iterations per the inner
cycle (min-max-min).
C.4.3 CONTINUOUS REGULARIZED WASSERSTEIN BARYCENTERS (CRWB)
Regularization. [CRWB] method uses regularization to keep the potentials conjugate. The authors
impose entropy or L2 regularization w.r.t. some proposal measure P; see (Li et al., 2020, Section
3) for more details. Following the source code provided by the authors, we use L2 regularization
(empirically shown as a more stable option than entropic regularization). The regularization measure
P is set to be the uniform measure on a box containing the support of all the source distributions,
estimated by sampling. The regularization parameter is set to 10-4.
Neural Networks (Potentials). To approximate potentials {ψn ,ψ*} in dimension D, we use fully-
connected sequential ReLU neural networks with layer sizes given by
[max(128, 4D), max(128, 4D), max(128, 4D)].
We have also tried using DenseICNN architecture, but did not experience any performance gain.
Training process. We perform training according to (Li et al., 2020, Algorithm 1). We set batch size
to 1024. We use Adam optimizer by (Kingma & Ba, 2014) with fixed learning rate 10-3. The total
number of iterations is set to 50000.
18