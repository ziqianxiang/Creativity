Published as a conference paper at ICLR 2021
Grounding Physical Concepts of Objects and
Events Through Dynamic Visual Reasoning
Zhenfang Chen
The University of Hong Kong
Kwan-Yee K. Wong
The University of Hong Kong
Jiayuan Mao
MIT CSAIL
Joshua B. Tenenbaum
MIT BCS, CBMM, CSAIL
Jiajun Wu
Stanford University
Chuang Gan
MIT-IBM Watson AI Lab
Ab stract
We study the problem of dynamic visual reasoning on raw videos. This is a chal-
lenging problem; currently, state-of-the-art models often require dense supervision
on physical object properties and events from simulation, which are impractical to
obtain in real life. In this paper, we present the Dynamic Concept Learner (DCL),
a unified framework that grounds physical objects and events from dynamic scenes
and language. DCL first adopts a trajectory extractor to track each object over
time and to represent it as a latent, object-centric feature vector. Building upon
this object-centric representation, DCL learns to approximate the dynamic interac-
tion among objects using graph networks. DCL further incorporates a semantic
parser to parse question into semantic programs and, finally, a program executor
to run the program to answer the question, levering the learned dynamics model.
After training, DCL can detect and associate objects across the frames, ground
visual properties and physical events, understand the causal relationship between
events, make future and counterfactual predictions, and leverage these extracted
presentations for answering queries. DCL achieves state-of-the-art performance
on CLEVRER, a challenging causal video reasoning dataset, even without using
ground-truth attributes and collision labels from simulations for training. We fur-
ther test DCL on a newly proposed video-retrieval and event localization dataset
derived from CLEVRER, showing its strong generalization capacity.
1	Introduction
Visual reasoning in dynamic scenes involves both the understanding of compositional properties,
relationships, and events of objects, and the inference and prediction of their temporal and causal
structures. As depicted in Fig. 1, to answer the question “What will happen next?” based on the
observed video frames, one needs to detect the object trajectories, predict their dynamics, analyze the
temporal structures, and ground visual objects and events to get the answer “The blue sphere and the
yellow object collide”.
Recently, various end-to-end neural network-based approaches have been proposed for joint un-
derstanding of video and language (Lei et al., 2018; Fan et al., 2019). While these methods have
shown great success in learning to recognize visually complex concepts, such as human activities (Xu
et al., 2017; Ye et al., 2017), they typically fail on benchmarks that require the understanding of
compositional and causal structures in the videos and text (Yi et al., 2020). Another line of research
has been focusing on building modular neural networks that can represent the compositional structures
in scenes and questions, such as object-centric scene structures and multi-hop reasoning (Andreas
et al., 2016; Johnson et al., 2017b; Hudson & Manning, 2019). However, these methods are designed
for static images and do not handle the temporal and causal structure in dynamic scenes well, leading
to inferior performance on video causal reasoning benchmark CLEVRER (Yi et al., 2020).
To model the temporal and causal structures in dynamic scenes, Yi et al. (2020) proposed an oracle
model to combine symbolic representation with video dynamics modeling and achieved state-of-
the-art performance on CLEVRER. However, this model requires videos with dense annotations for
visual attributes and physical events, which are impractical or extremely labor-intensive in real scenes.
Project page: http://dcl.csail.mit.edu
1
Published as a conference paper at ICLR 2021
Observed Frames
Question: What will happen next?
Predicted Dynamics
O Object Grounding ∖ Event Grounding
Answer: The blue sphere and the yellow object collide.
Figure 1: The process to handle visual reasoning in dynamic scenes. The trajectories of the target blue and
yellow spheres are marked by the sequences of bounding boxes. Object attributes of the blue sphere and yellow
sphere and the collision event are marked by blue, yellow and purple colors. Stroboscopic imaging is applied for
motion visualization.
We argue that such dense explicit video annotations are unnecessary for video reasoning, since they
are naturally encoded in the question answer pairs associated with the videos. For example, the
question answer pair and the video in Fig. 1 can implicitly inform a model what the concepts “ sphere ”,
“ blue ”, “ yellow ” and “ collide ” really mean. However, a video may contain multiple fast-moving
occluded objects and complex object interactions, and the questions and answers have diverse forms.
It remains an open and challenging problem to simultaneously represent objects over time, train an
accurate dynamic model from raw videos, and align objects with visual properties and events for
accurate temporal and causal reasoning, using vision and language as the only supervision.
Our main ideas are to factorize video perception and reasoning into several modules: object tracking,
object and event concept grounding, and dynamics prediction. We first detect objects in the video,
associating them into object tracks across the frames. We can then ground various object and event
concepts from language, train a dynamic model on top of object tracks for future and counterfactual
predictions, analyze relationships between events, and answer queries based on these extracted
representations. All these modules can be trained jointly by watching videos and reading paired
questions and answers.
To achieve this goal, we introduce Dynamic Concept Learner (DCL), a unified neural-symbolic
framework for recognizing objects and events in videos and analyzing their temporal and causal
structures, without explicit annotations on visual attributes and physical events such as collisions
during training. To facilitate model training, a multi-step training paradigm has been proposed. We
first run an object detector on individual frames and associate objects across frames based on a
motion-based correspondence. Next, our model learns concepts about object properties, relationships,
and events by reading paired questions and answers that describe or explain the events in the video.
Then, we leverage the acquired visual concepts in the previous steps to refine the object association
across frames. Finally, we train a dynamics prediction network (Li et al., 2019b) based on the refined
object trajectories and optimize it jointly with other learning parts in this unified framework. Such
a training paradigm ensures that all neural modules share the same latent space for representing
concepts and they can bootstrap the learning of each other.
We evaluate DCL’s performance on CLEVRER, a video reasoning benchmark that includes descrip-
tive, explanatory, predictive, and counterfactual reasoning with a uniform language interface. DCL
achieves state-of-the-art performance on all question categories and requires no scene supervision
such as object properties and collision events. To further examine the grounding accuracy and
transferability of the acquired concepts, we introduce two new benchmarks for video-text retrieval
and spatial-temporal grounding and localization on the CLEVRER videos, namely CLEVRER-
Retrieval and CLEVRER-Grounding. Without any further training, our model generalizes well to
these benchmarks, surpassing the baseline by a noticeable margin.
2	Related Work
Our work is related to reasoning and answering questions about visual content. Early studies like (Wu
et al., 2016; Zhu et al., 2016; Gan et al., 2017) typically adopted monolithic network architectures
and mainly focused on visual understanding. To perform deeper visual reasoning, neural module
networks were extensively studied in recent works (Johnson et al., 2017a; Hu et al., 2018; Hudson &
Manning, 2018; Amizadeh et al., 2020), where they represent symbolic operations with small neural
networks and perform multi-hop reasoning. Some previous research has also attempted to learn
2
Published as a conference paper at ICLR 2021
Input Question and Choice
Parsed Programs
Concept EmbeddingS
Question: Which of the
following will not happen if
the yellow cube is removed?
Choice: The collision between
the sphere and the cylinder.
ɪ—>
Trajectory
Detector
Input video
Program
Parser
------->
O - I FiIter ∣ - - -∣ FiIter 卜-∣ Belong_to_^| Negate
CUbe & Yellow COUnterfact
Filter ∣----j	ɪ	ι
SPhere ■
o- Finen——1
Cylinder
_ -I Filter
ColliSion
Trajectory Refinement
{"$}%=$
{b%=]
{"&}%=$
{"'}%=$
Object Trajectories
Dynamic
Predictor
{"$'}%#]
{"&!}%=$
｛虎'｝墨
Symbolic
Executor
Feature
Extractor
Counterfactual
Object Trajectories
B-^1^B 1
Cube
Yellow
Static
Concepts
Obj1
Obj2
Average
Visual
Feature
Stationary
Moving
Q&A
AYes
Dynamic Event
Concepts Concepts
Symbolic Executor
Obj1
■ iM ■
Obj2
■…■
Obj1, Obj2
Obj1, Obj3
・ J・
Temporal
Sequence
Feature
Interactive
Feature
Representation for objects
and events


Figure 2: DCL's architecture for Counterfactual questions during inference. Given an input video and its
corresponding question and choice, we first use a program parser to parse the question and the choice into
executable programs. We adopt an object trajectory detector to detect trajectories of all objects. Then, the
extracted objects are sent to a dynamic predictor to predict their dynamics. Next, the extracted objects are sent to
the feature extractor to extract latent representations for objects and events. Finally, we feed the parsed programs
and latent representation to the symbolic executor to answer the question and optimize concept learning.
visual concepts through visual question answering (Mao et al., 2019). However, it mainly focused on
learning static concepts in images, while our DCL aims at learning dynamic concepts like moving
and collision in videos and at making use of these concepts for temporal and causal reasoning.
Later, visual reasoning was extended to more complex dynamic videos (Lei et al., 2018; Fan et al.,
2019; Li et al., 2020; 2019a; Huang et al., 2020). Recently, Yi et al. (2020) proposed CLEVRER, a
new video reasoning benchmark for evaluating computational models’ comprehension of the causal
structure behind physical object interaction. They also developed an oracle model, combining the
neuro-symbolic visual question-answering model (Yi et al., 2018) with the dynamics prediction
model (Li et al., 2019b), showing competitive performance. However, this model requires explicit
labels for object attributes, masks, and spatio-temporal localization of events during training. Instead,
our DCL has no reliance on any labels for objects and events and can learn these concepts through
natural supervision (i.e., videos and question-answer pairs).
Our work is also related to temporal and relational reasoning in videos via neural networks (Wang &
Gupta, 2018; Materzynska et al., 2020; Ji et al., 2020). These works typically rely on specific action
annotations, while our DCL learns to ground object and event concepts and analyze their temporal
relations through question answering. Recently, various benchmarks (Riochet et al., 2018; Bakhtin
et al., 2019; Girdhar & Ramanan, 2020; Baradel et al., 2020; Gan et al., 2020) have been proposed to
study dynamics and reasoning in physical scenes. However, these datasets mainly target at pure video
understanding and do not contain natural language question answering. Much research has been
studying dynamic modeling for physical scenes (Lerer et al., 2016; Battaglia et al., 2013; Mottaghi
et al., 2016; Finn et al., 2016; Shao et al., 2014; Fire & Zhu, 2015; Ye et al., 2018; Li et al., 2019b).
We adopt PropNet (Li et al., 2019b) for dynamics prediction and feed the predicted scenes to the
video feature extractor and the neuro-symbolic executor for event prediction and question answering.
While many works (Zhou et al., 2019; 2018; Gan et al., 2015) have been studying on the problems of
understanding human actions and activities (e.g., running, cooking, cleaning) in videos, our work’s
primary goal is to design a unified framework for learning physical object and event concepts (e.g.,
collision, falling, stability). These tasks are of great importance in practical applications such as
industrial robot manipulation which requires AI systems with human-like physical common sense.
3	Dynamic Concept Learner
In this section, we introduce a new video reasoning model, Dynamic Concept Learner (DCL), which
learns to recognize video attributes, events, and dynamics and to analyze their temporal and causal
structures, all through watching videos and answering corresponding questions. DCL contains five
modules, 1) an object trajectory detector, 2) video feature extractor, 3) a dynamic predictor, 4) a
language program parser, and 5) a neural symbolic executor. As shown in Fig. 2, given an input
3
Published as a conference paper at ICLR 2021
video, the trajectory detector detects objects in each frame and associates them into trajectories;
the feature extractor then represents them as latent feature vectors. After that, DCL quantizes the
objects’ static concepts (i.e., color, shape, and material) by matching the latent object features with the
corresponding concept embeddings in the executor. As these static concepts are motion-independent,
they can be adopted as an additional criteria to refine the object trajectories. Based on the refined
trajectories, the dynamics predictor predicts the objects’ movement and interactions in future and
counterfactual scenes. The language parser parses the question and choices into functional programs,
which are executed by the program executor on the latent representation space to get answers.
The object and event concept embeddings and the object-centric representation share the same latent
space; answering questions associated with videos can directly optimize them through backpropa-
gation. The object trajectories and dynamics can be refined by the object static attributes predicted
by DCL. Our framework enjoys the advantages of both transparency and efficiency, since it enables
step-by-step investigations of the whole reasoning process and has no requirements for explicit
annotations of visual attributes, events, and object masks.
3.1	Model Details
Object Detection and Tracking. Given a video, the object trajectory detector detects object
proposals in each frame and connects them into object trajectories O = {on}nN=1, where on =
{btn}tT=1 and N is the number of objects in the video. bt = [xtn , ytn , wtn , htn] is an object proposal
at frame t and T is the frame number, where (xtn , ytn) denotes the normalized proposal coordinate
center and wtn and htn denote the normalized width and height, respectively.
The object detector first uses a pre-trained region proposal network (Ren et al., 2015) to generate
object proposals in all frames, which are further linked across connective frames to get all objects’
trajectories. Let {bit}iN=1 and {btj+1}jN=1 to be two sets of proposals in two connective frames. Inspired
by Gkioxari & Malik (2015), we define a connection score sl between bit and btj+1 to be
Sl(bi, bj+ι) = sc(bt) + Sc(bj+ι) + λι ∙ IoU(bi, bj+ι),	(1)
where sc(bit) is the confidence score of the proposal bit, IoU is the intersection over union and λ1
is a scalar. Gkioxari & Malik (2015) adopts a greedy algorithm to connect the proposals without
global optimization. Instead, we assign boxes {btj+1}jN=1 at the t + 1 frame to {bit}iN=1 by a linear
sum assignment.
Video Feature Extraction. Given an input video and its detected object trajectories, we extract
three kinds of latent features for grounding object and event concepts. It includes 1) the average
visual feature fv ∈ RN×D1 for static attribute prediction, 2) temporal sequence feature fs ∈ RN×4T
for dynamic attribute and unary event prediction, and 3) interactive feature fc ∈ RK×N×N×D2 for
collision event prediction, where D1 and D2 denote dimensions of the features and K is the number
of sampled frames. We give more details on how to extract these features in Appendix B.
Grounding Object and Event Concepts. Video Reasoning requires a model to ground object and
event concepts in videos. DCL achieves this by matching object and event representation with object
and event embeddings in the symbolic executor. Specifically, DCL calculates the confidence score
that the n-th object is moving by cos(Smoving, mda (fns )) - δ /λ, where fns denotes the temporal
sequence feature for the n-th object, Smoving denotes a vector embedding for concept moving, and
mda denotes a linear transformation, mapping fns into the dynamic concept representation space. δ
and λ are the shifting and scaling scalars, and cos() calculates the cosine similarity between two
vectors. DCL grounds static attributes and the collision event similarly, matching average visual
features and interactive features with their corresponding concept embeddings in the latent space. We
give more details on the concept and event quantization in Appendix E.
Trajectory Refinement. The connection score in Eq. 1 ensures the continuity of the detected object
trajectories. However, it does not consider the objects’ visual appearance; therefore, it may fail
to track the objects and may connect inconsistent objects when different objects are close to each
other and moving rapidly. To detect better object trajectories and to ensure the consistency of visual
appearance along the track, we add a new term to Eq. 1 and re-define the connection score to be
sl({bm}m=0, bj+1)= Scw) + Sc(^j+l) + λl ∙ IoU(bt, bj+l) + λ? ∙ fappear({bm}m=0,吊+)(2)
4
Published as a conference paper at ICLR 2021
where fappear({bim}tm=0, btj+1) measures the attribute similarity between the newly added proposal
btj+1 and all proposals ({bim}tm=0 in previous frames. We define fappear as
1t
faρρear({bm}m=0, bt+ι) = 3 × 力ΣΣfattr (bim , btj+1),
(3)
attr m=0
where attr ∈ {color, material, shape}. fattr(bm, bt+1) equals to 1 when bim and btj+1 have the same
attribute, and 0 otherwise. In Eq. 2, fappear ensures that the detected trajectories have consistent visual
appearance and helps to distinguish the correct object when different objects are close to each other in
the same frame. These additional static attributes, including color, material, and shape, are extracted
without explicit annotation during training. Specifically, we quantize the attributes by choosing
the concept whose concept embedding has the best cosine similarity with the object feature. We
iteratively connect proposals at the t + 1 frame to proposals at the t frame and get a set of object
trajectories O = {on}nN=1, where on = {btn}tT=1.
Dynamic Prediction. Given an input video and the refined trajectories of objects, we predict the
locations and RGB patches of the objects in future or counterfactual scenes with a Propagation
Network (Li et al., 2019b). We then generate the predicted scenes by pasting RGB patches into the
predicted locations. The generated scenes are fed to the feature extractor to extract the corresponding
features. Such a design enables the question answer pairs associated with the predicted scenes to
optimize the concept embeddings and requires no explicit labels for collision prediction, leading to
better optimization. This is different from Yi et al. (2020), which requires dense collision event labels
to train a collision classifier.
To predict the locations and RGB patches, the dynamic predictor maintains a directed graph hV, Di =
{vn}nN=1 , {dn1,n2 }nN,=N1,n =1 . The n-th vertex vn is represented by a concatenation of tuple
hbtn , ptni over a small time window w, where btn = [xtn , ytn , wtn , htn] is the n-th object’s normalized
coordinates and ptn is a cropped RGB patch centering at (xtn, ytn). The edge dn1,n2 denotes the
relation between the n1-th and n2-th objects and is represented by the concatenation of the normalized
coordinate difference btn1 - btn2. The dynamic predictor performs multi-step message passing to
simulate instantaneous propagation effects.
During inference, the dynamics predictor predicts the locations and patches at frame k + 1 using the
features of the last w observed frames in the original video. We get the predictions at frame k + 2
by autoregressively feeding the predicted results at frame k + 1 as the input to the predictor. To
get the counterfactual scenes where the n-th object is removed, we remove the n-th vertex and its
associated edges from the input to predict counterfactual dynamics. Iteratively, we get the predicted
normalized coordinates {in }N=K fco=1 and RGB patches 通,}NK ko=1 at all predicted K0 frames.
We give more details on the dynamic predictor at Appendix C.
Language Program Parsing. The language program parser aims to translate the questions and
choices into executable symbolic programs. Each executable program consists of a series of opera-
tions like selecting objects with certain properties, filtering events happening at a specific moment,
finding the causes of an event, and eventually enabling transparent and step-by-step visual reasoning.
Moreover, these operations are compositional and can be combined to represent questions with
various compositionality and complexity. We adopt a seq2seq model (Bahdanau et al., 2015) with an
attention mechanism to translate word sequences into a set of symbolic programs and treat questions
and choices, separately. We give detailed implementation of the program parser in Appendix D.
Symbolic Execution. Given a parsed program, the symbolic executor explicitly runs it on the latent
features extracted from the observed and predicted scenes to answer the question. The executor
consists of a series of functional modules to realize the operators in symbolic programs. The last
operator’s output is the answer to the question. Similar to Mao et al. (2019), we represent all object
states, events, and results of all operators in a probabilistic manner during training. This makes the
whole execution process differential w.r.t. the latent representations from the observed and predicted
scenes. It enables the optimization of the feature extractor and concept embeddings in the symbolic
executor. We provide the implementation of all the operators in Appendix E.
3.2	Training and inference
Training. We follow a multi-step training paradigm to optimize the model: 1) We first extract
object trajectories with the scoring function in Eq. 1 and optimize the video feature extractor and
5
Published as a conference paper at ICLR 2021
Methods	Extra Labels Descriptive			Explanatory per opt. per ques.		Predictive		Counterfactual	
	Attr.	Prog.				per opt. per ques.		per opt.	per ques.
CNN+MLP			48.4	54.9	18.3	50.5	13.2	55.2	9.0
CNN+LSTM			51.8	62.0	17.5	57.9	31.6	61.2	14.7
Memory	No	No	54.7	53.7	13.9	50.0	33.1.	54.2	7.0
HCRN			55.7	63.3	21.0	54.1	21.0	57.1	11.5
MAC (V)			85.6	59.5	12.5	51.0	16.5	54.6	13.7
TVQA+	Yes	No	72.0	63.3	23.7	70.3	48.9	53.9	4.1
MAC (V+)			86.4	70.5	22.3	59.7	42.9	63.5	25.1
IEP (V)			52.8	52.6	14.5	50.0	9.7	53.4	3.8
TbD-net (V)	No	Yes	79.5	61.6	3.8	50.3	6.5	56.1	4.4
DCL (Ours)			90.7	89.6	82.8	90.5	82.0	80.4	46.5
NS-DR			88.1	87.6	79.6	82.9	68.7	74.1	42.4
NS-DR (NE)	Yes	Yes	85.8	85.9	74.3	75.4	54.1	76.1	42.0
DCL-Oracle (Ours)			91.4	89.8	82.0	90.6	82.1	80.7	46.9
Table 1: Question-answering accuracy on CLEVRER. The first and the second parts of the table show the models
without and with visual attribute and event labels during training, respectively. Best performance is highlighted
in boldface. DCL and DCL-Oracle denote our models trained without and with labels of visual attributes and
events, respectively.
concept embeddings in the symbolic executor with only descriptive and explanatory questions; 2) We
quantize the static attributes for all objects with the feature extractor and the concept embeddings
learned in Step 1) and refine object trajectories with the scoring function Eq. 2; 3) Based on the
refined trajectories, we train the dynamic predictor and predict dynamics for future and counterfactual
scenes; 4) We train the full DCL with all the question answer pairs and get the final model. The
program executor is fully differentiable w.r.t. the feature extractor and concept embeddings. We use
cross-entropy loss to supervise open-ended questions and use mean square error loss to supervise
counting questions. We provide specific loss functions for each module in Appendix H.
Inference. During inference, given an input video and a question, we first detect the object trajectories
and predict their motions and interactions in future and counterfactual scenes. We then extract object
and event features for both the observed and predicted scenes with the feature extractor. We parse the
questions and choices into executable symbolic programs. We finally execute the programs on the
latent feature space and get the answer to the question.
4	Experiments
To show the proposed DCL’s advantages, we conduct extensive experiments on the video reasoning
benchmark CLEVRER. Existing other video datasets either ask questions about the complex visual
context (Tapaswi et al., 2016; Lei et al., 2018) or study dynamics and reasoning without question
answering (Girdhar & Ramanan, 2020; Baradel et al., 2020). Thus, they are unsuitable for evaluating
video causal reasoning and learning object and event concepts through question answering. We
first show its strong performance on video causal reasoning. Then, we show DCL’s ability on
concept learning, predicting object visual attributes and events happening in videos. We show
DCL’s generalization capacity to new applications, including CLEVRER-Grounding and CLEVRER-
Retrieval. We finally extend DCL to a real block tower video dataset (Lerer et al., 2016).
4.1	Implementation Details
Following the experimental setting in Yi et al. (2020), we train the language program parser with
1000 programs for all question types. We train all our models without attribute and event labels. Our
models for video question answering are trained on the training set, tuned on the validation set, and
evaluated in the test set. To show DCL’s generalization capacity, we build CLEVRER-Grounding
and CLEVRER-Retrieval datasets from the original CLEVRER videos and their associated video
annotations. We provide more implementation details in Appendix A.
4.2	Comparisons on Temporal and Causal Reasoning
We compare our DCL with previous methods on CLEVRER, including Memory (Fan et al., 2019),
IEP (Johnson et al., 2017b), TbD-net (Mascharka et al., 2018), TVQA+ (Lei et al., 2018), NS-
6
Published as a conference paper at ICLR 2021
Methods	Static Attributes		Dynamic Attributes	Events		
	Color Shape	Material	Moving Stationary	In	Out Collision	
DCL	99.7	99.2	99.6	89.7	93.3	99.2	98.9	96.9
Table 2: Evaluation of video concept learning on the validation set.
DR (Yi et al., 2020), MAC (V) (Hudson & Manning, 2018) and its attribute-aware variant, MAC
(V+). We refer interested readers to CLEVRER (Yi et al., 2020) for more details. Additionally,
we also include a recent state-of-the-art VQA model HCRN (Le et al., 2020) for performance
comparison, which adopts a conditional relation network for representation and reasoning over videos.
To provide more extensive analysis, we introduce DCL-Oracle by adding object attribute and collision
supervisions into DCL’s training. We summarize their requirement for visual labels and language
programs in the second and third columns of Table 1.
According to the results in Table 1, we have the following observations. Although HCRN achieves
state-of-the-art performance on human-centric action datasets (Jang et al., 2017; Xu et al., 2017;
2016), it only performs slightly better than Memory and much worse than NS-DR on CLEVRER. We
believe the reason is that HCRN mainly focuses on motion modeling across frames while CLEVRER
requires models to perform dynamic visual reasoning on videos and analyze its temporal and causal
structures. NS-DR performs best among all the baseline models, showing the power of combining
symbolic representation with dynamics modeling. Our model achieves the state-of-the-art question
answering performance on all kinds of questions even without visual attributes and event labels from
simulations during training, showing its effectiveness and label-efficiency. Compared with NS-DR,
our model achieves more significant gains on predictive and counterfactual questions than that on
the descriptive questions. This shows DCL’s effectiveness in modeling for temporal and causal
reasoning. Unlike NS-DR, which directly predicts collision event labels with its dynamic model,
DCL quantizes concepts and executes symbolic programs in an end-to-end training manner, leading
to better predictions for dynamic concepts. DCL-Oracle shows the upper-bound performance of the
proposed model to ground physical object and event concepts through question answering.
4.3	Evaluation of Object and Event Concept Grounding in Videos
Previous methods like MAC (V) and TbD-net (V) did not learn explicit concepts during training,
and NS-DR required intrinsic attribute and event labels as input. Instead, DCL can directly quantize
video concepts, including static visual attributes (i.e. color, shape, and material), dynamic attributes
(i.e. moving and stationary) and events (i.e. in, out, and collision). Specifically, DCL quantizes the
concepts by mapping the latent object features into the concept space by linear transformation and
calculating their cosine similarities with the concept embeddings in the neural-symbolic executor.
We predict the static attributes of each object by averaging the visual object features at each sampled
frame. We regard an object to be moving if it moves at any frame, and otherwise stationary. We
consider there is a collision happening between a pair of objects if they collide at any frame of the
video. We get the ground-truth labels from the provided video annotation and report the accuracy in
table 2 on the validation set.
We observe that DCL can learn to recognize different kinds of concepts without explicit concept
labels during training. This shows DCL’s effectiveness to learn object and event concepts through
natural question answering. We also find that DCL recognizes static attributes and events better than
dynamic attributes. We further find that DCL may misclassify objects to be “stationary” if they are
missing for most frames and only move slowly at specific frames. We suspect the reason is that we
only learn the dynamic attributes through question answering and question answering pairs for such
slow-moving objects rarely appear in the training set.
4.4	Generalization
We further apply DCL to two new applications, including CLEVRER-Grounding, spatio-temporal
localization of objects or events in a video, and CLEVRER-Retrieval, finding semantic-related
videos for the query expressions and vice versa.
We first build datasets for video grounding and video-text retrieval by synthesizing language ex-
pressions for videos in CLEVRER. We generate the expressions by filling visual contents from the
video annotations into a set of pre-defined templates. For example, given the text template, “The
<static_attribute> that is <dynamic_attribute> <time_identifier>”, We can fill it and generate "The
7
Published as a conference paper at ICLR 2021
Query: The collision that happens after the blue
sphere exits the scene.
App1: CLEVRER-Grounding.
Query: A video that contains a gray metal cube that
enters the scene.
App2: CLEVRER-Retrieval.
Figure 3: Examples of CLEVRER-Grounding and CLEVRER-Retrieval Datasets. The target region are marked
by purple boxes and stroboscopic imaging is applied for visualization purposes. In CLEVRER-Retrieval, we
mark randomly-selected positive and negative gallery videos with green and red borders, respectively.
Mthd Spatial Acc.↑ Spatial mIoU.↑ Frame Diff.1				
Obj.	Col.	Obj.	Col.	In Out Col.
WSSTG 34.4 DCL 91.9	10.3 88.3	34.9 90.0	15.6 79.0	21.4 50.4 37.5 5.5 4.4 4.5
Table 3: Evaluation of video grounding. For spatial grounding,
we consider it to be accurate if the IoU between the detected
trajectory and the ground-truth trajectory is greater than 0.5.
Methods Text-to-Video↑ Video-to-Teχt↑
Obj.	In	Out	Col.
WSSTG	2.2	1.3	3.4	3.3	7.7
HGR	16.9	17.2	18.7	22.2	15.5
DCL	73.1	81.9	88.5	85.4	78.6
Table 4: Evaluation of CLEVRER-Retrieval.
Mean average precision (mAP) is adopted as
the metric.
metal cube that is moving when the video ends.”. Fig. 3 shows examples for the generated datasets,
and we provide more statistics and examples in Appendix G. We transform the grounding and retrieval
expressions into executable programs by training new language parsers on the expressions of the
synthetic training set. To provide more extensive comparisons, we adopt the representative video
grounding/ retrieval model WSSTG (Chen et al., 2019) as a baseline. We provide more details of the
baseline implementation in Appendix A.
CLEVRER-Grounding. CLEVRER-Grounding contains object grounding and event grounding.
For video object grounding, we localize each described object’s whole trajectory and compute the
mean intersection over union (IoU) with the ground-truth trajectory. For event grounding, including
collision, in and out, we temporally localize the frame that the event happens at and calculate the
frame difference with the ground-truth frames. For collision event, we also spatially localize the
collided objects’ the union box and compute it’s IoU with the ground-truth. We don’t perform spatial
localization for in and out events since the target object usually appears to be too small to localize at
the frame it enters or leaves the scene.
Table 3 lists the results. From the table, we can find that our proposed DCL transforms to the
new CLEVRER-Grounding task well and achieves high accuracy for spatial localization and low
frame differences for temporal localization. On the contrary, the traditional video grounding method
WSSTG performs much worse, since it mainly aligns simple visual concepts between text and images
and has difficulties in modeling temporal structures and understanding the complex logic.
CLEVRER-Retrieval. For CLEVRER-Retrieval, an expression-video pair is considered as a
positive pair if the video contains the objects and events described by the expression and otherwise
negative. Given a video, we define its matching similarity with the query expression to be the
maximal similarity between the query expression and all the object or event proposals. Additionally,
we also introduce a recent state-of-the-art video-text retrieval model HGR (Chen et al., 2020) for
performance comparison, which decomposes video-text matching into global-to-local levels and
performs cross-modal matching with attention-based graph reasoning. We densely compare every
possible expression-video pair and use mean average precision (mAP) as the retrieval metric.
We report the retrieval mAP in Table 4. Compared with CLEVRER-Grounding, CLEVRER-Retrieval
is more challenging since it contains many more distracting objects, events and expressions. WSSTG
performs worse on the retrieval setting because it does not model temporal structures and understand
its logic. HGR achieves better performance than the previous baseline WSSTG since it performs
hierarchical modeling for events, actions and entities. However, it performs worse than DCL since it
doesn’t explicitly model the temporal structures and the complex logic behind the video-text pairs in
CLEVRER-Retrieval. On the other hand, DCL is much more robust since it can explicitly ground
object and event concepts, analyze their relations and perform step-by-step visual reasoning.
8
Published as a conference paper at ICLR 2021
Methods	Question Type Query Exist Count			Average
MAC (V)	92.8	95.5	75.0	87.7
DCL (ours)	97.0	95.5	84.1	92.6
Table 5: QA results on the block tower dataset.
Method	Static Color	Dynamic “falling”
DCL (ours)	98.5	91.8
Table 6: Evaluation of concept learning on the block
tower dataset. Our DCL can learn to quantize the new
concept “falling” on real videos through QA.
Q1: How many objects are falling? A1: 2.
Q2: Are there any falling red objects? A2: No.
Q3: Are there any falling blue objects? A3: Yes.
Falling block tower
Q1: What is the color of the block that is at the
bottom? A1: Blue.
Q2: Are there any falling yellow objects? A2: No.
Stable block tower.
Figure 4: Typical videos and question-answer pairs of the block tower dataset. Stroboscopic imaging is applied
for motion visualization.
4.5 Extension to real videos and the new concept
We further conduct experiments on a real block tower video dataset (Lerer et al., 2016) to learn the
new physical concept “falling”. The block tower dataset has 493 videos and each video contains
a stable or falling block tower. Since the original dataset aims to study physical intuition and
doesn’t contain question-answer pairs, we manually synthesize question-answer pairs in a similar
way to CLEVRER (Yi et al., 2020). We show examples of the new dataset in Fig 4. We train
models on randomly-selected 393 videos and their associated question-answer pairs and evaluate
their performance on the rest 100 videos.
Similar to the setting in CLEVRER, we use the average visual feature from ResNet-34 for static
attribute prediction and temporal sequence feature for the prediction of the new dynamic concept
“falling”. Additionally, we train a visual reasoning baseline MAC (V) (Hudson & Manning, 2018) for
performance comparison. Table 5 lists the results. Our model achieves better question-answering
performance on the block tower dataset especially on the counting questions like “How many objects
are falling?”. We believe the reason is that counting questions require a model to estimate the states of
each object. MAC (V) just simply adopts an MLP classifier to predict each answer’s probability and
doesn’t model the object states. Differently, DCL answers the counting questions by accumulating
the probabilities of each object and is more transparent and accurate. We also show the accuracy of
color and “falling” concept prediction on the validation set in Table 6. Our DCL can naturally learn
to ground the new dynamic concept “falling” in the real videos through question answering. This
shows DCL’s effectiveness and strong generalization capacity.
5 Discussion and future work
We present a unified neural symbolic framework, named Dynamic Concept Learner (DCL), to study
temporal and causal reasoning in videos. DCL, learned by watching videos and reading question-
answers, is able to track objects across different frames, ground physical object and event concepts,
understand the causal relationship, make future and counterfactual predictions and combine all these
abilities to perform temporal and causal reasoning. DCL achieves state-of-the-art performance on
the video reasoning benchmark CLEVRER. Based on the learned object and event concepts, DCL
generalizes well to spatial-temporal object and event grounding and video-text retrieval. We also
extend DCL to real videos to learn new physical concepts.
Our DCL suggests several future research directions. First, it still requires further exploration for
dynamic models with stronger long-term dynamic prediction capability to handle some counterfactual
questions. Second, it will be interesting to extend our DCL to more general videos to build a stronger
model for learning both physical concepts and human-centric action concepts.
Acknowledgement This work is in part supported by ONR MURI N00014-16-1-2007, the Center
for Brain, Minds, and Machines (CBMM, funded by NSF STC award CCF-1231216), the Samsung
Global Research Outreach (GRO) Program, Autodesk, and IBM Research.
9
Published as a conference paper at ICLR 2021
References
Saeed Amizadeh, Hamid Palangi, Oleksandr Polozov, Yichen Huang, and Kazuhito Koishida. Neuro-symbolic
visual reasoning: Disentangling” visual” from” reasoning”. In ICML, 2020. 2
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In CVPR, 2016. 1
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to
align and translate. In ICLR, 2015. 5, 14
Anton Bakhtin, Laurens van der Maaten, Justin Johnson, Laura Gustafson, and Ross Girshick. Phyre: A new
benchmark for physical reasoning. In NeurIPS, 2019. 3
Fabien Baradel, Natalia Neverova, Julien Mille, Greg Mori, and Christian Wolf. Cophy: Counterfactual learning
of physical dynamics. In ICLR, 2020. 3, 6
Peter W Battaglia, Jessica B Hamrick, and Joshua B Tenenbaum. Simulation as an engine of physical scene
understanding. Proceedings of the National Academy of Sciences, 2013. 3
Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, and Ben Upcroft. Simple online and realtime tracking. In
ICIP, 2016. 15, 16
Shizhe Chen, Yida Zhao, Qin Jin, and Qi Wu. Fine-grained video-text retrieval with hierarchical graph reasoning.
In CVPR, 2020. 8, 13
Zhenfang Chen, Lin Ma, Wenhan Luo, and Kwan-Yee Kenneth Wong. Weakly-supervised spatio-temporally
grounding natural sentence in video. In ACL, 2019. 8, 13
Chenyou Fan, Xiaofan Zhang, Shu Zhang, Wensheng Wang, Chi Zhang, and Heng Huang. Heterogeneous
memory enhanced multimodal attention model for video question answering. In CVPR, 2019. 1, 3, 6
Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction through video
prediction. In NeurIPS, 2016. 3
Amy Fire and Song-Chun Zhu. Learning perceptual causality from video. ACM Transactions on Intelligent
Systems and Technology, 2015. 3
Chuang Gan, Naiyan Wang, Yi Yang, Dit-Yan Yeung, and Alex G Hauptmann. Devnet: A deep event network
for multimedia event detection and evidence recounting. In CVPR, pp. 2568-2577, 2015. 3
Chuang Gan, Yandong Li, Haoxiang Li, Chen Sun, and Boqing Gong. Vqs: Linking segmentations to questions
and answers for supervised attention in vqa and question-focused semantic segmentation. In ICCV, pp.
1811-1820, 2017. 2
Chuang Gan, Jeremy Schwartz, Seth Alter, Martin Schrimpf, James Traer, Julian De Freitas, Jonas Kubilius,
Abhishek Bhandwaldar, Nick Haber, Megumi Sano, et al. Threedworld: A platform for interactive multi-modal
physical simulation. arXiv preprint arXiv:2007.04954, 2020. 3
Rohit Girdhar and Deva Ramanan. Cater: A diagnostic dataset for compositional actions and temporal reasoning.
In ICLR, 2020. 3, 6
Georgia Gkioxari and Jitendra Malik. Finding action tubes. In CVPR, 2015. 4, 15
Alex Graves, Santiago Fernandez, and Jurgen Schmidhuber. Bidirectional lstm networks for improved phoneme
classification and recognition. In ICANN, 2005. 14
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
CVPR, 2016. 13
Ronghang Hu, Jacob Andreas, Trevor Darrell, and Kate Saenko. Explainable neural computation via stack
neural module networks. In ECCV, 2018. 2
Deng Huang, Peihao Chen, Runhao Zeng, Qing Du, Mingkui Tan, and Chuang Gan. Location-aware graph
convolutional networks for video question answering. In AAAI, volume 34, pp. 11021-11028, 2020. 3
Drew Hudson and Christopher D Manning. Learning by abstraction: The neural state machine. In NeurIPS,
2019. 1
Drew A Hudson and Christopher D Manning. Compositional attention networks for machine reasoning. In
ICLR, 2018. 2, 7, 9
10
Published as a conference paper at ICLR 2021
Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatio-temporal
reasoning in visual question answering. In CVPR, 2017. 7
Jingwei Ji, Ranjay Krishna, Li Fei-Fei, and Juan Carlos Niebles. Action genome: Actions as compositions of
spatio-temporal scene graphs. In CVPR, 2020. 3
Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick.
Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In CVPR, 2017a. 2
Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Judy Hoffman, Li Fei-Fei, C Lawrence Zitnick,
and Ross Girshick. Inferring and executing programs for visual reasoning. In ICCV, 2017b. 1, 6
Rudolph Emil Kalman. A new approach to linear filtering and prediction problems. J. Basic Eng., Trans. ASME,
D, 82:35-45,1960. 15
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv:1412.6980, 2014. 13
Thao Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran. Hierarchical conditional relation networks for
video question answering. In CVPR, 2020. 7, 13
Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg. Tvqa: Localized, compositional video question
answering. In EMNLP, 2018. 1, 3, 6
Adam Lerer, Sam Gross, and Rob Fergus. Learning physical intuition of block towers by example. In ICML,
2016. 3, 6, 9
Qing Li, Siyuan Huang, Yining Hong, Yixin Chen, Ying Nian Wu, and Song-Chun Zhu. Closed loop neural-
symbolic learning via integrating neural perception, grammar parsing, and symbolic reasoning. In ICML,
2020. 3
Xiangpeng Li, Jingkuan Song, Lianli Gao, Xianglong Liu, Wenbing Huang, Xiangnan He, and Chuang Gan.
Beyond rnns: Positional self-attention with co-attention for video question answering. In AAAI, volume 33,
pp. 8658-8665, 2019a. 3
Yunzhu Li, Jiajun Wu, Jun-Yan Zhu, Joshua B Tenenbaum, Antonio Torralba, and Russ Tedrake. Propagation
networks for model-based control under partial observation. In ICRA, 2019b. 2, 3, 5
Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B Tenenbaum, and Jiajun Wu. The neuro-symbolic concept
learner: Interpreting scenes, words, and sentences from natural supervision. In ICLR, 2019. 3, 5
David Mascharka, Philip Tran, Ryan Soklaski, and Arjun Majumdar. Transparency by design: Closing the gap
between performance and interpretability in visual reasoning. In CVPR, 2018. 6
Joanna Materzynska, Tete Xiao, Roei Herzig, Huijuan Xu, Xiaolong Wang, and Trevor Darrell. Something-else:
Compositional action recognition with spatial-temporal interaction networks. In CVPR, 2020. 3
Roozbeh Mottaghi, Mohammad Rastegari, Abhinav Gupta, and Ali Farhadi. “what happens if...” learning to
predict the effect of forces in images. In ECCV. Springer, 2016. 3
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with
region proposal networks. In NeurIPS, 2015. 4
Ronan Riochet, Mario Ynocente Castro, Mathieu Bernard, Adam Lerer, Rob Fergus, Veronique Izard,
and Emmanuel Dupoux. Intphys: A framework and benchmark for visual intuitive physics reasoning.
arXiv:1803.07616, 2018. 3
Tianjia Shao, Aron Monszpart, Youyi Zheng, Bongjin Koo, Weiwei Xu, Kun Zhou, and Niloy J Mitra. Imagining
the unseen: Stability-based cuboid arrangements for scene understanding. ACM TOG, 2014. 3
Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and Sanja Fidler.
MovieQA: Understanding Stories in Movies through Question-Answering. In CVPR, 2016. 6
Xiaolong Wang and Abhinav Gupta. Videos as space-time region graphs. In ECCV, 2018. 3
Nicolai Wojke, Alex Bewley, and Dietrich Paulus. Simple online and realtime tracking with a deep association
metric. In ICIP, 2017. 15, 16
Qi Wu, Chunhua Shen, Lingqiao Liu, Anthony Dick, and Anton van den Hengel. What value do explicit high
level concepts have in vision to language problems? In CVPR, 2016. 2
11
Published as a conference paper at ICLR 2021
Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question
answering via gradually refined attention over appearance and motion. In ACM MM, 2017. 1, 7
Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and
language. In CVPR, pp. 5288-5296, 2016. 7
Tian Ye, Xiaolong Wang, James Davidson, and Abhinav Gupta. Interpretable intuitive physics model. In ECCV,
2018. 3
Yunan Ye, Zhou Zhao, Yimeng Li, Long Chen, Jun Xiao, and Yueting Zhuang. Video question answering via
attribute-augmented attention network learning. In ICLR, 2017. 1
Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Josh Tenenbaum. Neural-symbolic
vqa: Disentangling reasoning from vision and language understanding. In NeurIPS, 2018. 3, 13
Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua B Tenenbaum.
Clevrer: Collision events for video representation and reasoning. In ICLR, 2020. 1, 3, 5, 6, 7, 9, 14
Luowei Zhou, Chenliang Xu, and Jason J Corso. Towards automatic learning of procedures from web instruc-
tional videos. In AAAI, 2018. 3
Luowei Zhou, Yannis Kalantidis, Xinlei Chen, Jason J Corso, and Marcus Rohrbach. Grounded video description.
In CVPR, 2019. 3
Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in images.
In CVPR, 2016. 2
12
Published as a conference paper at ICLR 2021
A	Implementation Details
DCL Implementation Details. Since it’s extremely computation-intensive to predict the object
states and events at every frame, we evenly sample 32 frames for each video. All models are trained
using Adam (Kingma & Ba, 2014) for 20 epochs and the learning rate is set to 10-4. We adopt a
two-stage training strategy for training the dynamic predictor. For the dynamic predictor, we set
the time window size w, the propogation step L and dimension of hidden states to be 3, 2 and 512,
respectively. Following the sample rate at the observed frames, we sample a frame for prediction
every 4 frames. We first train the dynamic model with only location prediction and then train it with
both location and RGB patch prediction. Experimentally, we find this training strategy provides a
more stable prediction. We train the language parser with the same training strategy as Yi et al. (2018)
for fair comparison.
Baseline Implementation. We implement the baselines HCRN (Le et al., 2020), HGR Chen et al.
(2020) and WSSTG (Chen et al., 2019) carefully based on the public source code. WSSTG first
generate a set of object or event candidates and match them with the query sentence. We choose the
proposal candidate with the best similarity as the grounding result. For object grounding, we use
the same tube trajectory candidates as we use for implementing DCL. For grounding event concepts
in and out, we treat each object at each sampled frame as a potential candidate for selection. For
grounding event concept collision, we treat the union regions of any object pairs as candidates. For
CLEVRER-Retrieval, we treat the proposal candidate with the best similarity as the similarity score
between the video and the query sentence. We train WSSTG with a synthetic training set generated
from the videos of CLEVRER-VQA training set. A fully-supervised triplet loss is adopted to optimize
the model.
B	Feature Extraction
We evenly sample K frames for each video and use a ResNet-34 (He et al., 2016) to extract visual
features. For the n-th object in the video, We define its average visual feature to be fn =表 PK=I 段,
where fkn is the concatenation of the regional feature and the global context feature at the k-th
frame. We define its temporal sequence feature fns to be the contenation of [xtn , ytn , wtn , htn ] at all
T frames, Where (xtn , ytn ) denotes the normalized object coordinate centre and wtn and htn denote
the normalized Width and height, respectively. For the collision feature betWeen the n1-th object
and n2-th objet at the k-th frame, We define it to be fnc1,n2,k = fnu1,n2,k||fnlo1c,n2,k, Wherefnu1,n2,k
is the ResNet feature of the union region of the n1-th and n2-th objects at the k-th frame and
fnloc,n ,k is a spatial embedding for correlations betWeen bounding box trajectories. We define
fnloc,n ,k = IoU(sn1 , sn2)||(sn1 - sn2)||(sn1 × sn2), Which is the concatenation of the intersection
over union (IoU), difference (-) and multiplication (×) of the normalized trajectory coordinates for
the n1-th and n2-th objects centering at the k-th frame. We padding fnu1,n2 k With a zero vector if
either the n1-th or the n2-th objects doesn’t appear at the k-th frame.
C Dynamic Predictor
To predict the locations and RGB patches, the dynamic predictor maintains a directed graph hV, Di =
{vn }nN=1, {dn1,n2 }nN,=N1,n =1 . The n-th vertex on is represented by the concatenation of its
normalized coordinates btn = [xtn , ytn , wtn , htn ] and RGB patches ptn . The edge dn1,n2 is represented
by the concatenation of the normalized coordinate difference btn1 - btn2 . To capture the object
dynamics, We concatenate the features over a small history WindoW. To predict the dynamics at the
k + 1 frame, We first encode the vertexes and edges
eon,k = fOenc(||tk=k-w(btn||ptn)),	ern1,n2,k =fRenc(||tk=k-w(btn1-btn2)),	(4)
Where || indicates concatenation, w is the history WindoW size, fOenc and fRenc are CNN-based
encoders for objects and relations. w is set to 3. We then update the object influences {hln,k }nN=1 and
13
Published as a conference paper at ICLR 2021
relation influences {eln ,n ,k}nN,=N1,n =1 through L propagation steps. Specifically, we have
en1,n2,k = fR(en1,n2,k, hn1,k, hn2,k),	hn,k = fO (en,k,	en1,n2,k, hn,k ),
(5)
n1,n2
where l ∈ [1, L], denoting the l-th step, fO and fR denote the object propagator and relation
propagator, respectively. We initialize hon,t = 0. We finally predict the states of objects and relations
at the k+1 frame to be
bn+ι = fo：d(e(n,k, hL,k), Pn+ι = foored(en,k, hL,k),	(6)
where fOpred and fOpred are predictors for the normalized object coordinates and RGB patches at the
next frame. We optimize this dynamic predictor by mimizing the L2 distance between the predicted
bn+ι, pn+i and the real future locations b，+、and extracted patches pk+ι.
During inference, the dynamics predictor predicts the locations and patches at k+1 frames by using the
features of the last w observed frames in the original video. We get the predictions at the k+2 frames
by feeding the predicted results at the k+1 frame to the encoder in Eq. 4. To get the counterfactual
scenes where the n-th object is removed, we use the first w frames of the original video as the start
point and remove the n-th vertex and its associated edges of the input to predict counterfactual
N K0
dynamics. Iteratively, we get the predicted normalized coordinates {bkn0}nN=,K1,k0=1 and RGB patches
{Pno }N=K,k0=ι at all predicted K0 frames.
D Program Parser
Following Yi et al. (2020), we use a seq2seq model (Bahdanau et al., 2015) with attention mechanism
to word sequences into a set of symbolic programs and treat questions and choices, separately. The
model consists of a Bi-LSTM (Graves et al., 2005) to encode the word sequences into hidden states
and a decoder to attentively aggregate the important words to decode the target program. Specifically,
to encode the word embeddings{wi}iI=1 into the hidden states, we have
→i, →i = -STM(fwnc(Wi), →",%i,工i=to-(fwnc(wi), Xi+1),	(7)
where I is the number of words and fwenc is an encoder for word embeddings. To decode the encoded
vectors {ei}iI=1 into symbolic programs {pj}jJ=1, we have
qj = LSTM(fcdec(pj-1)),
exp(qj ei)
Pi exp(qj ei)
Pj 〜softmax(W ∙ (qj|| £ αi,je。),
i
(8)
where ei = -→ei ||Xe-i and J is the number of programs. The dimension of the word embedding and all
the hidden states is set to 300 and 256, respectively.
E CLEVRER Operations and Program Execution
We list all the available data types and operations for CLEVRER VQA (Yi et al., 2020) in Table 8
and Table 7. In this section, we first introduce how we represent the objects, events and moments in
the video. Then, we describe how we quantize the static and dynamic concepts and perform temporal
and causal reasoning. Finally, we summarize the detailed implementation of all operations in Table 9.
Representation for Objects, Events and Time. We consider a video with N objects and T frames
and we sample K frames for collision prediction. The objects in Table 8 can be represented by a
vector objects of length N, where objectsn ∈ [0, 1] represents the probability of the n-h object
being referred to. Similarly, we use a vector eventsin of length N to representing the probability
of objects coming into the visible scene. We additionally store frame indexes tin for event in, where
tinn indicates the moment when the n-th object first appear in the visual scene. We represent event
out in a similar way as we represent event in. For event collision, we represent it with a matrix
eventscol ∈ RN×N×K , where eventscnol,n ,k represents the n1-th and n2-th objects collide at
the k-th frame. Since CLEVRER requires temporal relations of events, we also maintain a time mask
M ∈ RT to annotate valid time steps, where Mt = 1 indicates the t-th is valid at the current step
and Mt = 0 indicating invalid. In CLEVRER, Unique also involves transformation from objects
(object set) to object (a single object). We achieve by selecting the object with the largest probability.
We perform in a similar way to transform events to event.
14
Published as a conference paper at ICLR 2021
Object and Event Concept Quantization. We first introduce how DCL quantizes different con-
cepts by showing an example how DCL quantizes the static object concept cube. Let fnv denote the
latent visual feature for the n-th object in the video, SA denotes the set of all static attributes. The
concept cube is represented by a semantic vector sCube and an indication vector icube. icube is of
length |SA| and L-1 normalized, indicating concept Cube belongs to the static attribute Shape. We
compute the confidence scores that an object is a Cube by
PCUbe = X (iCube cos(scube,msa(fn))- δ)
Pn	= / v (isa	λ	)，
sa∈S A
(9)
where δ and λ denotes the shifting and scaling scalars and are set to 0.15 and 0.2, respectively.
cos() calculates the cosine similarity between two vectors and msa denotes a linear transformation,
mapping object features into the concept representation space. We get a vector of length N by
applying this concept filter to all objects, denoted as ObjF ilter(cube).
We perform similar quantization to temporal dynamic concepts. For event in and out, we simply
replace fnv with temporal sequence features fns ∈ R4T to get eventsinn . For event collision, we replace
fnv with fnc ,n ,k to predict the confidence that the n1-th and the n2-th objects collide at the k-frame
and get eventsonut,n ,k. For moment-specific dynamic concepts moving and stationary, we adopt
frame-specific feature fs,t* ∈ R4T for concept prediction. We denote the filter result on all objects
as ObjFilter(moving, t*). Specifically, We generate the sequence feature f[t* at the t*-th frame
by only concatenating [xjn, yf, wf, hf] from t* - T to t* + T frames and padding other dimensions
With 0.
Temporal and causal Reasoning. One unique feature for CLEVRER is that it requires a
model to reason over temporal and causal structures of the video to get the ansWer. We handle
Filter^efore and Filter_after by updating the valid time mask M. For example, to filter
events happening after a target event. We first get the frame t* that the target event happens at and
update valid time mask M by setting Mt = 1 if t > t* else Mt = 0. We then ignore the events
happening at the invalid frames and update the temporal sequence features to be fns0 = fns ◦ Mexp ,
Where ◦ denotes the component-Wise multiplication and Mexp = [M; M ; M; M] ∈ R4T .
For Filter_order of eventstype, we first filter all the valid events by find events who eventtype >
η. η is simply set to 0 and type ∈ {in, out, collision}. We then sort all the remain events based on
ttype to find the target event.
For Filter.ancestor of a collision event, we first predict valid events by finding events type > η.
We then return all valid events that are in the causal graphs of the given collision event.
We summarize the implementation of all operations in Table 9.
F Trajectory Performance Evaluation.
In this section, we compare different kinds of methods for generating object trajectory proposals.
Greedy+IoU denotes the method used in (Gkioxari & Malik, 2015), which adopts a greedy
Viterbi algorithm to generate trajectories based on IoUs of image proposals in connective frames.
Greedy+IoU+Attr. denotes the method adopts the greedy algorithm to generate trajectory
proposals based on the IoUs and predicted static attributes. LSM+IoU denotes the method that we use
linear sum assignment to connect the image proposals based on IoUs. LSM+IoU+Attr. denotes
the method we use linear sum assignment to connect image proposals based on IoUs and predicted
static attributes. LSM+IoU+Attr.+KF denotes the method that we apply additional Kalman
filtering (Kalman, 1960; Bewley et al., 2016; Wojke et al., 2017) to LSM+IoU+Attr.. We evaluate
the performance of different methods by compute the IoU between the generated trajectory proposals
and the ground-truth trajectories. We consider it a “correct” trajectory proposal if the IoU between the
proposal and the ground-truth is larger than a threshold. Specifically, two metrics are used evaluation,
precision = NcorreCt and recall = NcorreCt, where Ncorrect, NP and Ngt denotes the number of correct
Np	Ngt
proposals, the number of proposals and the number of ground-truth objects, respectively.
Table 10 list the performance of different thresholds. We can see that Greedy+IoU achieve bad
performance when the IoU threshold is high while our method based on linear sum assignment and
15
Published as a conference paper at ICLR 2021
static attributes are more robust. Empirically, we find that linear sum assignment and static attributes
can help distinguish close object proposals and make the correct image proposal assignments. Similar
to normal object tracking algorithms (Bewley et al., 2016; Wojke et al., 2017), we also find that
adding additional Kalman filter can further slightly improve the trajectory quality.
G Statistics for CLEVRER-Grounding and CLEVRER-Retrieval
We simply use the videos from original CLEVRER training set as the training videos for
CLEVRER-Grounding and CLEVRER-Retrieval and evaluate their performance on the validation set.
CLEVERER-Grounding contains 10.2 expressions for each video on average. CLEVERER-Retrieval
contains 7.4 expressions for each video in the training set. We for evaluating the video retrieval
task on the validation set. We evaluate the performance of CLEVRER-Grounding task on all 5,000
videos from the original CLEVRER validation set. For CLEVERER-Retrieval, We additionally
generate 1,129 unique expressions from the validation set as query and treat the first 1,000 videos
from CLEVRER validation set as the gallery. We provide more examples for CLEVRER-Grounding
and CLEVRER-Retrieval datasets in Fig. 5, Fig. 6 and Fig. 7. It can be seen from the examples that
the newly proposed CLEVRER-Grounding and CLEVRER-Retrieval datasets contain delicate and
compositional expressions for objects and physical events. It can evaluate models’ ability to perform
compositional temporal and causal reasoning.
H	Training Objectives
In this section, we provide the explicit training objectives for each module. We optimize the feature
extractor and the concept embeddings in the executors by question answering. We treat each option
of a multiple-choice question as an independent boolean question during training and we use different
loss functions for different question types. Specifically, we use cross-entropy loss to supervise
open-ended questions and use mean square error loss to supervise counting questions. Formally, for
open-ended questions, we have
C
LQA,open = -E l{ya = c} Iog(Pc),	(10)
where C is the size of the pre-defined answer set, pc is the probability for the c-th answer and ya is
the ground-truth answer label. For counting questions, we have
LQA,count = (ya - z) ,	(11)
where z is the predicted number and ya is the ground-truth number label.
We train the program parser with program labels using cross-entropy loss,
J
Lprogram = ―〉：{-ypp = j} Iog(Pj),	(12)
j=1
where J is the size of the pre-defined program set, Pj is the probability for the j -th program and pp is
the ground-truth program label.
We optimize the dynamic predictor with mean square error loss. Mathematically, we have
N	N Np Np
Ldynamic = X k bn- bn k 2 + XXX k Phi2 - Pl,i2 k2,	(⑶
n=1	n=1 i1 =1 i2 =1
where bn is the object coordinates for the n-th object, Pin ,i is the pixel value of the n-th object’s
cropped patch at (i1,i2), and Np is the cropped size. bn and Pn i2 are the dynamic predictor's
predictions for bn and Pin1 ,i2 .
16
Published as a conference paper at ICLR 2021
Type	Operation	Signature
Input Modules	Objects Returns all objects in the video Events Returns all events happening in the video UnseenEvents Returns all future events happening in the video Start Returns the special “start” event end Returns the special “end” event	() → objects () → events () → events () → event () → event
Object Filter Modules	Filter_StatiJcOncept Select objects from the input list with the input static concept Filter_dynamic_concept Selects objects in the input frame with the dynamic concept	(objects, concept) → objects (objects, concept, frame) → objects
Event Filter Modules	Filter_in Select incoming events of the input objects Filterqut Select existing events of the input objects Filter-collision Select all collisions that involve an of the input objects Get_col_partner Return the collision partner of the input object Filter 上efore Select all events before the target event Filter_after Select all events after the target event Filterqrder Select the event at the specific time order Filter.ancestor Select all ancestors of the input event in the causal graph Getframe Return the frame of the input event in the video	(events, objects) → events (events, objects) → events (events, objects) → events (event, object) → object (events, events) → events (events, events) → events (events, order) → event (event, events) → events (event) → frame
Output Modules	Query-Attribute Returns the query attribute of the input object Count Returns the number of the input objects/ events Exist Returns “yes” if the input objects is not empty Belong_to Returns “yes” if the input event belongs to the input event sets Negate Returns the negation of the input boolean	(object) → concept (objects) → int (events) → int (objects) → bool (event, events) → bool (bool) → bool
	Unique Return the only event /object in the input list	(events) → event (objects) → object
Table 7: Operations available on CLEVRER dataset.
Type	Semantics
object	A single object in the video.
objects	A set of objects in the video.
event	A single event in the video.
events	A set of events in the video.
order	The chronological order of an event, e.g. “First”, “Second” and “Last”.
static concept	Object-level static concepts like “Red”, “Sphere” and “Mental”.
dynamic concept	Object-level dynamic concepts like “Moving” and “Stationary”.
attribute	Static attributes including “Color”, “Shape” and “Material”.
frame	The frame number of an event.
int	A single integer like “0” and “1”.
bool	A single boolean value, “True” or “False”.
Table 8: The data type system of CLEVRER-VQA.
17
Published as a conference paper at ICLR 2021
Type	Operation/ Signature	Implementation
Input Modules	Objects () → objects Events () → events UnseenEvents () → events Start () → M end ()→ M	objects = 1 eventstype for type ∈ {in, out, col.} eventscol0 and eventsout0 Mt = 1 if t < 5 else Mt = 0 Mt = 1ift > (T - 5) else Mt = 0
Object Filter Modules	Filter_Static-Concept (objs: objects, sa: concept) → objects Filter_dynamic_concept (objs: objects, da: concept, t: frame) → objects	min(objs, ObjFilter(sa)) min(objects, ObjFilter(da,t))
Event Filter Modules	Filter_in (eventsin : events, objs: objects) → events Filter_out (eventsout : events, objs: objects) → events Filter-collision (eventscol : events, objs: objects) → events Get-col .partner (eventscol : events, objn : object) → objects Filter-before (eventsinn : events, event1: event) Filter_after (eventsinn : events, event1: event) → events Filter_order (eventsinn : events, or: order) → event Filter.ancestor (event1: event, events1: events) → events Get-frame (event1: event) → frame	min(objs, eventsin) min(objs, eventsout) min(objsexp, eventscol.) maxk∈[1,K] (eventscno,kl) eventsinn = -1 if tinn > tevent1 eventsinn = -1 if tinn < tevent1 eventsinn > 0 if ordernin = or {event1n > 0 and events1n in the causal graph of event1} tevent1
	Query-Attribute (obj: object, a: attribute) → concept Count (objs: objects) → int Exist (objs: objects) → bool Belong_to (event1: event, events1: events) → bool Negate (bl: bool) → bool	P op =	ObjFilter (op) ∙ia
Output Modules		Pθp0 ObjFilter(op0)∙iαpl Pn (objsn > 0) (Pn(objsn > 0)) > 0 True if event1 ∈ events1 else False False if bl else True
Table 9: Neural operations in DCL. eventscol0 denotes the collision events happening at the unseen future frames.
objsexp ∈ RN×N×K and objsenxp,n ,k = max(obj sn1 , obj sn2). eventscno,kl denotes all the collision events that
the n-th object get involved at the k-th frame.
	0.5		0.6		0.7		0.8		0.9	
	prec.	recall	prec.	recall	prec.	recall	prec.	recall	prec.	recall
Greedy+IoU	87.2	88.3	71.4	72.3	57.3	58.0	46.9	47.5	39.6	40.0
Greedy+IoU+Attr.	97.0	97.4	95.1	95.5	93.1	93.5	89.9	90.3	83.6	83.9
LSM+IoU	97.6	98.8	96.9	98.1	96.0	97.2	93.8	95.0	88.5	89.6
LSM+IoU+Attr.	99.1	98.4	98.6	97.9	97.9	97.2	96.2	95.5	91.5	90.8
LSM+IoU+Attr.+KF	99.1	98.4	98.6	97.9	98.0	97.3	96.2	95.6	91.6	90.9
Table 10: The evaluation of different methods for object trajectory generation.
18
Published as a conference paper at ICLR 2021
Query: The collision that happens before the gray object enters the scene.
Query: The green object enters the scene before the rubber sphere enters the scene
Query: The cube exits the scene after the sphere enters the scene
Query: The metal cylinder that is stationary when the sphere enters the scene
Figure 5: Typical examples of CLEVRER-Grounding datasets. The target regions are bounded with purple
boxes.
19
Published as a conference paper at ICLR 2021
Query: A video that contains a collision that happens after the yellow metal cylinder enters the
scene.
Positive video sample (a)
Positive video sample (b)
Positive video sample (c)
Positive video sample (d)
Figure 6: A exemplar query expression and 4 of its associated positive videos from CLEVRER-Retrieval dataset.
The target regions in videos are bounded with purple boxes.
1.	A video that contains an object that collides with the brown metal cube.
2.	A video that contains an object that collides with the gray metal sphere.
3.	A video	that contains an object to collide with the brown metal cube.
4.	A video	that contains an object to collide with the gray	metal sphere.
5.	A video	that contains a collision that happens after the	yellow metal cube	enters the	scene.
6.	A video	that contains a collision that happens after the	brown metal cube	enters the	scene.
7.	A video that contains a collision that happens before the yellow metal cube enters the scene.
Figure 7: An typical query video and its associated positive expressions from CLEVRER-Retrieval dataset.
20