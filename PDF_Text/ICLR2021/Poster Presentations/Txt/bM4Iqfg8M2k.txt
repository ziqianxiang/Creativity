Published as a conference paper at ICLR 2021
Graph Information Bottleneck for Subgraph
Recognition
Junchi Yu1,2,3；Tingyang Xu3, Yu Rong3, Yatao Bian3, Junzhou Huang3,Ran He1,2,4t
1	NLPR&CRIPAC, Institute of Automation, Chinese Academy of Sciences, China
2	University of Chinese Academy of Sciences, China
3	Tencent AI LAB, China
4	Center for Excellence in Brain Science and Intelligence Technology, CAS, China
yujunchi2019@ia.ac.cn, tingyangxu@tencent.com, yu.rong@hotmail.com
yatao.bian@gmail.com, jzhuang@uta.edu, rhe@nlpr.ia.ac.cn
Ab stract
Given the input graph and its label/property, several key problems of graph learn-
ing, such as finding interpretable subgraphs, graph denoising and graph compres-
sion, can be attributed to the fundamental problem of recognizing a subgraph of
the original one. This subgraph shall be as informative as possible, yet contains
less redundant and noisy structure. This problem setting is closely related to the
well-known information bottleneck (IB) principle, which, however, has less been
studied for the irregular graph data and graph neural networks (GNNs). In this
paper, we propose a framework of Graph Information Bottleneck (GIB) for the
subgraph recognition problem in deep graph learning. Under this framework, one
can recognize the maximally informative yet compressive subgraph, named IB-
subgraph. However, the GIB objective is notoriously hard to optimize, mostly due
to the intractability of the mutual information of irregular graph data and the unsta-
ble optimization process. In order to tackle these challenges, we propose: i) a GIB
objective based-on a mutual information estimator for the irregular graph data; ii)
a bi-level optimization scheme to maximize the GIB objective; iii) a connectiv-
ity loss to stabilize the optimization process. We evaluate the properties of the
IB-subgraph in three application scenarios: improvement of graph classification,
graph interpretation and graph denoising. Extensive experiments demonstrate that
the information-theoretic IB-subgraph enjoys superior graph properties.
1	Introduction
Classifying the underlying labels or properties of graphs is a fundamental problem in deep graph
learning with applications across many fields, such as biochemistry and social network analysis.
However, real world graphs are likely to contain redundant even noisy information (Franceschi et al.,
2019; Yu et al., 2019), which poses a huge negative impact for graph classification. This triggers an
interesting problem of recognizing an informative yet compressed subgraph from the original graph.
For example, in drug discovery, when viewing molecules as graphs with atoms as nodes and chem-
ical bonds as edges, biochemists are interested in identifying the subgraphs that mostly represent
certain properties of the molecules, namely the functional groups (Jin et al., 2020b; Gilmer et al.,
2017). In graph representation learning, the predictive subgraph highlights the vital substructure
for graph classification, and provides an alternative way for yielding graph representation besides
mean/sum aggregation (Kipf & Welling, 2017; Velickovic et al., 2017; Xu et al., 2019) and pooling
aggregation (Ying et al., 2018; Lee et al., 2019; Bianchi et al., 2020). In graph attack and defense, it
is vital to purify a perturbed graph and mine the robust structures for classification (Jin et al., 2020a).
Recently, the mechanism of self-attentive aggregation (Li et al., 2019) somehow discovers a vital
substructure at node level with a well-selected threshold. However, this method only identifies
isolated important nodes but ignores the topological information at subgraph level. Consequently, it
*This work was done when JunChi Yu was a research intern at TenCent AI LAB.
,Corresponding Author
1
Published as a conference paper at ICLR 2021
leads to a novel challenge as subgraph recognition: How can we recognize a compressed subgraph
with minimum information loss in terms of predicting the graph labels/properties?
Recalling the above challenge, there is a similar problem setting in information theory called infor-
mation bottleneck (IB) principle (Tishby et al., 1999), which aims to juice out a compressed data
from the original data that keeps most predictive information of labels or properties. Enhanced with
deep learning, IB can learn informative representation from regular data in the fields of computer
vision (Peng et al., 2019; Alemi et al., 2017; Luo et al., 2019), reinforcement learning (Goyal et al.,
2019; Igl et al., 2019) and natural language precessing (Wang et al., 2020). However, current IB
methods, like VIB (Alemi et al., 2017), is still incapable for irregular graph data. It is still challeng-
ing for IB to compress irregular graph data, like a subgraph from an original graph, with a minimum
information loss.
Hence, we advance the IB principle for irregular graph data to resolve the proposed subgraph recog-
nition problem, which leads to a novel principle, Graph Information Bottleneck (GIB). Different
from prior researches in IB that aims to learn an optimal representation of the input data in the hid-
den space, GIB directly reveals the vital substructure in the subgraph level. We first i) leverage the
mutual information estimator from Deep Variational Information Bottleneck (VIB) (Alemi et al.,
2017) for irregular graph data as the GIB objective. However, VIB is intractable to compute the
mutual information without knowing the distribution forms, especially on graph data. To tackle
this issue, ii) we adopt a bi-level optimization scheme to maximize the GIB objective. Meanwhile,
the continuous relaxation that we adopt to approach the discrete selection of subgraph will lead to
unstable optimization process. To further stabilize the training process and encourage a compact
subgraph, iii) we propose a novel connectivity loss to assist GIB to effectively discover the maxi-
mally informative but compressed subgraph, which is defined as IB-subgraph. By optimizing the
above GIB objective and connectivity loss, one can recognize the IB-subgraph without any explicit
subgraph annotation. On the other hand, iv) GIB is model-agnostic and can be easily plugged into
various Graph Neural Networks (GNNs).
We evaluate the properties of the IB-subgraph in three application scenarios: improvement of graph
classification, graph interpretation, and graph denoising. Extensive experiments on both synthetic
and real world datasets demonstrate that the information-theoretic IB-subgraph enjoys superior
graph properties compared to the subgraphs found by SOTA baselines.
2	Related Work
Graph Classification. In recent literature, there is a surge of interest in adopting graph neural net-
works (GNN) in graph classification. The core idea is to aggregate all the node information for graph
representation. A typical implementation is the mean/sum aggregation (Kipf & Welling, 2017; Xu
et al., 2019), which is to average or sum up the node embeddings. An alternative way is to lever-
age the hierarchical structure of graphs, which leads to the pooling aggregation (Ying et al., 2018;
Zhang et al., 2018; Lee et al., 2019; Bianchi et al., 2020). When tackling with the redundant and
noisy graphs, these approaches will likely to result in sub-optimal graph representation. Recently,
InfoGraph (Sun et al., 2019) maximize the mutual information between graph representations and
multi-level local representations to obtain more informative global representations.
Information Bottleneck. Information bottleneck (IB), originally proposed for signal processing,
attempts to find a short code of the input signal but preserve maximum information of the code
(Tishby et al., 1999). (Alemi et al., 2017) firstly bridges the gap between IB and the deep learning,
and proposed variational information bottleneck (VIB). Nowadays, IB and VIB have been wildly
employed in computer vision (Peng et al., 2019; Luo et al., 2019), reinforcement learning (Goyal
et al., 2019; Igl et al., 2019), natural language processing (Wang et al., 2020) and speech and acous-
tics (Qian et al., 2020) due to the capability of learning compact and meaningful representations.
However, IB is less researched on irregular graphs due to the intractability of mutual information.
Subgraph Discovery. Traditional subgraph discovery includes dense subgraph discovery and fre-
quent subgraph mining. Dense subgraph discovery aims to find the subgraph with the highest density
(e.g. the number of edges over the number of nodes (Fang et al., 2019; Gionis & Tsourakakis, 2015)).
Frequent subgraph mining is to look for the most common substructure among graphs (Yan & Yan,
2002; Ketkar et al., 2005; Zaki, 2005). At node-level, researchers discover the vital substructure
2
Published as a conference paper at ICLR 2021
via the attention mechanism (Velickovic et al., 2017; Lee et al., 2019; Knyazev et al., 2019). Ying
et al. (2019) further identifies the important computational graph for node classification. Alsentzer
et al. (2020) discovers subgraph representations with specific topology given subgraph-level anno-
tation. Recently, it is popular to select a neighborhood subgraph of a central node to do message
passing in node representation learning. DropEdge (Rong et al., 2020) relieves the over-smoothing
phenomenon in deep GCNs by randomly dropping a portion of edges in graph data. Similar to
DropEdge, DropNode (Chen et al., 2018; Hamilton et al., 2017; Huang et al., 2018) principle is also
widely adopted in node representation learning. FastGCN (Chen et al., 2018) and ASGCN (Huang
et al., 2018) accelerate GCN training via node sampling. GraphSAGE (Hamilton et al., 2017) lever-
ages neighborhood sampling for inductive node representation learning. NeuralSparse (Zheng et al.,
2020) select Top-K (K is a hyper-parameter) task-relevant 1-hop neighbors of a central node for
robust node classification. Similarly, researchers discover the vital substructure at node level via the
attention mechanism (Velickovic et al., 2017; Lee et al., 2019; Knyazev et al., 2019).
3	Notations and Preliminaries
Let {(G1, Y1), . . . , (GN, YN)} be a set of N graphs with their real value properties or categories,
where Gn refers to the n-th graph and Yn refers to the corresponding properties or labels. We denote
by Gn = (V, E, A, X) the n-th graph of size Mn with node set V = {Vi|i = 1, . . . , Mn}, edge
set E = {(Vi, Vj)|i > j; Vi, Vj is connected}, adjacent matrix A ∈ {0, 1}Mn ×Mn, and feature
matrix X ∈ RMn ×d of V with d dimensions, respectively. Denote the neighborhood of Vi as
N(Vi) = {Vj∣(Vi, Vj) ∈ E}. We use Gsub as a specific subgraph and Gsub as the complementary
structure of Gsub in G. Let f : G → R/[0,1, ∙∙∙ ,n] be the mapping from graphs to the real value
property or category, Y , G is the domain of the input graphs. I(X, Y ) refers to the Shannon mutual
information of two random variables.
3.1	Graph convolutional network
Graph convolutional network (GCN) is widely adopted to graph classification. Given a graph G =
(V, E) with node feature X and adjacent matrix A, GCN outputs the node embeddings X0 from
the following process:
0	11
X =GCN(A, X; W)=ReLU(D-2 AD-2XW),	(1)
where D refers to the diagonal matrix with nodes’ degrees and W refers to the model parameters.
One can simply sum up the node embeddings to get a fixed length graph embeddings (Xu et al.,
2019). Recently, researchers attempt to exploit hierarchical structure of graphs, which leads to
various graph pooling methods (Li et al., 2019; Gao & Ji, 2019; Lee et al., 2019; Diehl, 2019; Zhang
et al., 2018; Ranjan et al., 2020; Ying et al., 2018). Li et al. (2019) enhances the graph pooling with
self-attention mechanism to leverage the importance of different nodes contributing to the results.
Finally, the graph embedding is obtained by multiplying the node embeddings with the normalized
attention scores:
E = Att(X0) = softmax(Φ2tanh(Φ1X0T))X0,	(2)
where Φ1 and Φ2 refers to the model parameters of self-attention.
3.2	Optimizing Information bottleneck objective
Given the input signal X and the label Y , the objective of IB is maximized to find the the internal
code Z: maxZ I(Z, Y ) - βI(X, Z), where β refers to a hyper-parameter trading off informative-
ness and compression. Optimizing this objective will lead to a compact but informative Z. Alemi
et al. (2017) optimize a tractable lower bound of the IB objective:
N XJ
LV IB
p(z∣xi)logq@(yi∣z)dz - βKL(p(z∣xi)∣r(z)),
(3)
where qφ(yi∖z) is the variational approximation to Pφ(yi∣z) and r(z) is the prior distribution of
Z . However, it is hard to estimate the mutual information in high dimensional space when the
distribution forms are inaccessible, especially for irregular graph data.
3
Published as a conference paper at ICLR 2021
Figure 1: Illustration of the proposed graph information bottleneck (GIB) framework. We employ
a bi-level optimization scheme to optimize the GIB objective and thus yielding the IB-subgraph.
In the inner optimization phase, we estimate I(G, Gsub) by optimizing the statistics network of the
DONSKER-VARADHAN representation (Donsker & Varadhan, 1983). Given a good estimation
of I(G, Gsub), in the outer optimization phase, we maximize the GIB objective by optimizing the
mutual information, the classification loss Lcls and connectivity loss Lcon .
4	Optimizing the Graph Information Bottleneck Objective for
Subgraph Recognition
In this section, we will elaborate the proposed method in details. We first formally define the graph
information bottleneck and IB-subgraph. Then, we introduce a novel framework for GIB to effec-
tively find the IB-subgraph. We further propose a bi-level optimization scheme and a graph mutual
information estimator for GIB optimization. Moreover, we do a continuous relaxation to the gener-
ation of subgraph, and propose a novel loss to stabilize the training process.
4.1	graph information bottleneck
We generalize the information bottleneck principle to learn an informative representation of irregular
graphs, which leads to the graph information bottleneck (GIB) principle.
Definition 4.1 (Graph Information Bottleneck). Given a graph G and its label Y , the GIB seeks for
the most informative yet compressed representation Z by optimizing the following objective:
maxI(Y,Z) s.t. I(G, Z) ≤ Ic.	(4)
Z
where Ic is the information constraint between G and Z . By introducing a Lagrange multiplier β to
Eq. 4, we reach its unconstrained form:
maxI(Y,Z)-βI(G,Z).	(5)
Z
Eq. 5 gives a general formulation of GIB. Here, in subgraph recognition, we focus on a subgraph
which is compressed with minimum information loss in terms of graph properties.
Definition 4.2 (IB-subgraph). For a graph G, its maximally informative yet compressed subgraph,
namely IB-subgraph can be obtained by optimizing the following objective:
max I(Y,Gsub)-βI(G,Gsub).	(6)
Gsub∈Gsub
where Gsub indicates the set of all subgraphs of G.
IB-subgraph enjoys various pleasant properties and can be applied to multiple graph learning tasks
such as improvement of graph classification, graph interpretation, and graph denoising. However,
the GIB objective in Eq. 6 is notoriously hard to optimize due to the intractability of mutual infor-
mation and the discrete nature of irregular graph data. We then introduce approaches on how to
optimize such objective and derive the IB-subgraph.
4
Published as a conference paper at ICLR 2021
4.2	Bi-level optimization for the GIB objective
The GIB objective in Eq. 6 consists of two parts. We examine the first term I(Y, Gsub) in Eq. 6, first.
This term measures the relevance between Gsub and Y . We expand I(Y, Gsub) as:
I(Y, Gsub) =	p(y, Gsub) log p(y|Gsub)dy dGsub + H(Y).	(7)
H(Y ) is the entropy of Y and thus can be ignored. In practice, we approximate p(y, Gsub) with
an empirical distribution p(y, Gsub) ≈ N PL δy(yi)δGsub (Gsubi ), where δ() is the Dirac function
to sample training data. Gsubi and yi are the output subgraph and graph label corresponding to
i-th training data. By substituting the true posterior p(y|Gsub) with a variational approximation
qφ1 (y|Gsub), we obtain a tractable lower bound of the first term in Eq. 6:
I (Y, Gsub ) ≥	p(y , Gsub ) log qφ1 (y|Gsub )dy dGsub
1 N	(8)
≈ N)： log qφι (yi lGsubi ) =: -LcIs (qφι (y |Gsub), ygt),
i=1
where ygt is the ground truth label of the graph. Eq. 8 indicates that maximizing I(Y, Gsub) is
achieved by the minimization of the classification loss between Y and Gsub as Lcls . Intuitively,
minimizing Lcls encourages the subgraph to be predictive of the graph label. In practice, we choose
the cross entropy loss for categorical Y and the mean squared loss for continuous Y , respectively.
For more details of deriving Eq. 7 and Eq. 8, please refer to Appendix A.1.
Then, we consider the minimization of I (G, Gsub ) which is the second term of Eq. 6. Remind that
Alemi et al. (2017) introduces a tractable prior distribution r(Z) in Eq. 3, and thus results in a vari-
ational upper bound. However, this setting is troublesome as it is hard to find a reasonable prior
distribution forp(Gsub), which is the distribution of graph substructures instead of latent representa-
tion. Thus we go for another route. Directly applying the DONSKER-VARADHAN representation
(Donsker & Varadhan, 1983) of the KL-divergence, we have:
I(G, Gsub) =	sup	EG,Gsub∈p(G,Gsub)fφ2 (G, Gsub) - log EG∈p(G),Gsub∈p(Gsub)efφ2 (G,Gsub),
fφ2 :G×G→R
(9)
where fφ2 is the statistics network that maps from the graph set to the set of real numbers. In
order to approximate I(G, Gsub) using Eq. 9, we design a statistics network based on modern GNN
architectures as shown by Figure 1: first we use a GNN to extract embeddings from both G and
Gsub (parameter shared with the subgraph generator, which will be elaborated in Section 4.3), then
concatenate G and Gsub embeddings and feed them into a MLP, which finally produces the real
number. In conjunction with the sampling method to approximatep(G, Gsub), p(G) andp(Gsub), we
reach the following optimization problem to approximate1 I(G, Gsub):
1N	1 N
max	LMI(φ2, Gsub) =k f2 fφ2 (Gi, Gsubi) - log 方	efφ2 (Gi,Gsubj).	(10)
φ2	N	N
i=1	i=1,j6=i
With the approximation to the MI in graph data, we combine Eq. 6 , Eq. 8 and Eq. 10 and formulate
the optimization process of GIB as a tractable bi-level optimization problem:
Lmin	L(Gsub,Φ1,Φ2)= Lcιs(qφι (y∣Gsub),ygt)+ βLMi(Φ2, Gsub)	(11)
Gsub ,φ1
s.t.	Φ2 = argmaxLmi(02, Gsub).	(12)
φ2
We first derive a sub-oPtimal φ2 notated as φ2 by optimizing Eq. 12 for T steps as inner loops. After
the T-step optimization of the inner-loop ends, Eq. 10 is a proxy for MI minimization for the GIB
objective as an outer loop. Then, the parameter φ1 and the subgraph Gsub are optimized to yield
IB-subgraph. However, in the outer loop, the discrete nature of G and Gsub hinders applying the
gradient-based method to optimize the bi-level objective and find the IB-subgraph.
1Notice that the MINE estimator (Belghazi et al., 2018) straightforwardly uses the DONSKER-
VARADHAN representation to derive an MI estimator between the regular input data and its vectorized repre-
sentation/encoding. It cannot be applied to estimate the mutual information between G and Gsub since both of
G and Gsub are irregular graph data.
5
Published as a conference paper at ICLR 2021
Table 1: Classification accuracy. The pooling methods yield pooling aggregation while the back-
bones yield mean aggregation. The proposed GIB method with backbones yields subgraph embed-
ding by aggregating the nodes in subgraphs.
Method	MUTAG	PROTEINS	IMDB-BINARY	DD
SortPool	0.844 ± 0.141	0.747 ± 0.044	0.712 ± 0.047	0.732 ± 0.087
ASAPool	0.743 ± 0.077	0.721 ± 0.043	0.715 ± 0.044	0.717 ± 0.037
DiffPool	0.839 ± 0.097	0.727 ± 0.046	0.709 ± 0.053	0.778 ± 0.030
EdgePool	0.759 ± 0.077	0.723 ± 0.044	0.728 ± 0.044	0.736 ± 0.040
AttPool	0.721 ± 0.086	0.728 ± 0.041	0.722 ± 0.047	0.711 ± 0.055
GCN	0.743±0.110	0.719±0.041	0.707 ± 0.037	0.725 ± 0.046
GraphSAGE	0.743±0.077	0.721 ± 0.042	0.709 ± 0.041	0.729 ± 0.041
GIN	0.825±0.068	0.707 ± 0.056	0.732 ± 0.048	0.730 ± 0.033
GAT	0.738 ± 0.074	0.714 ± 0.040	0.713 ± 0.042	0.695 ± 0.045
GAT + DropEdge	0.743±0.081	0.711±0.043	0.710±0.041	0.717±0.035
GCN+GIB	0.776 ± 0.075	0.748 ± 0.046	0.722 ± 0.039	0.765 ± 0.050
GraphSAGE+GIB	0.760 ± 0.074	0.734 ± 0.043	0.719 ± 0.052	0.781 ± 0.042
GIN+GIB	0.839 ± 0.064	0.749 ± 0.051	0.737 ± 0.070	0.747 ± 0.039
GAT+GIB	0.749 ± 0.097	0.737 ± 0.044	0.729 ± 0.046	0.769 ± 0.040
GAT+GIB+DropEdge	0.754±0.085	0.737±0.037	0.731±0.003	0.776±0.034
Table 2: The mean and standard deviation of absolute property bias between the graphs and the
corresponding subgraphs.
Method	QED	DRD2	HLM-CLint	MLM-CLint
GCN+Att05	0.48± 0.07	0.20± 0.13	0.90± 0.89	0.92± 0.61
GCN+Att07	0.41± 0.07	0.16± 0.11	1.18± 0.60	1.69± 0.88
GCN+GIB	0.38± 0.12	0.06± 0.09	0.37± 0.30	0.72± 0.55
4.3 The Subgraph Generator and connectivity loss
To alleviate the discreteness in Eq. 11, we propose the continuous relaxation to the subgraph recog-
nition and propose a loss to stabilize the training process.
Subgraph generator: For the input graph G, We generate its IB-SUbgraPh with the node assignment
S which indicates the node is in Gsub or Gsub. Then, we introduce a continuous relaxation to the
node assignment with the probability of nodes belonging to the Gsub or Gsub For example, the i-th
row of S is a 2-dimensional vector [p(Vi ∈ Gsub∣Vi),p(Vi ∈ Gsub∣Vi)]. We first use an l-layer GNN
to obtain the node embedding and employ a multi-layer perceptron (MLP) to output S :
Xl = GNN(A, Xl-1; θι),	S = Softmax(MLP(Xl; θ2)).	(13)
S is a n × 2 matrix, where n is the number of nodes. We add row-wise Softmax to the output of
MLP to ensure the nodes are either in or out of the subgraph. For simplicity, we compile the above
modules as the subgraph generator, denoted as g(; θ) with θ := (θ1, θ2). When S is well-learned, the
assignment of nodes is supposed to saturate to 0/1. The representation of Gsub, which is employed
for predicting the graph label, can be obtained by taking the first row of ST Xl .
Connectivity loss: However, poor initialization will cause P(Vi ∈ Gsub∣Vi) and P(Vi ∈ Gsub∣Vi)
to be close. This will either lead the model to assign all nodes to Gsub / Gsub or result that the
representations of Gsub contain much information from the redundant nodes. These two scenarios
will cause the training process to be unstable. On the other hand, we suppose our model to have
an inductive bias to better leverage the topological information while S outputs the subgraph at a
node-level. Therefore, we propose the following connectivity loss:
Lcon = ||Norm(STAS)-I2||F,	(14)
where Norm(∙) is the row-wise normalization, ∣∣∙∣∣f is the Frobenius norm, and I is a 2 X 2 identity
matrix. Lcon not only leads to distinguishable node assignment, but also encourage the subgraph to
be compact. Take (STAS)1: for example, denote a11, a12 the element 1,1 and the element 1,2 of
6
Published as a conference paper at ICLR 2021
Table 3: Ablation study on Lcon and LMI . Note that we try several initiations for GIB w/o Lcon
and LMI to get the current results due to the instability of optimization process.
Method	QED	DRD2	HLM-CLint	MLM-CLint
GIB w/o Lcon	0.46± 0.07	0.15± 0.12	0.45± 0.37	1.58± 0.86
GIB w/o LMI	0.43± 0.15	0.21± 0.13	0.48± 0.34	1.20± 0.97
GIB	0.38± 0.12	0.06± 0.09	0.37± 0.30	0.72± 0.55
ST AS,
a11 = X Aijp(Vi ∈ Gsub |Vi)p(Vj ∈ Gsub |Vj ), a12 = X Aijp(Vi ∈ Gsubk)P(V∙ ∈ G sub | Vj )
i,j	i,j
(15)
Minimizing Lcon results in ɑ；：；］? → 1. ThiS occurs if Vi is in Gsub the elements of N(Vi)
have a high probability in Gsub Minimizing Lcon also encourages aia+a2 → 0∙ ThiS encourages
p(V ∈ GsubIVi) → 0/1 and less cuts between Gsub and Gsub This also holds for Gsub when
analyzing a21 and a22 .
In a word, Lcon encourages distinctive S to stabilize the training process and a compact topology in
the subgraph. Therefore, the overall loss is:
min	L(θ, φι, φ22) = Lcls距(g(G; θ)), ygt) + αLcon(g(G; θ)) + βLMi(φ:, Gsub)
θ,φ1
s.t.	Φ2 = arg max Lmi(02, Gsub).
φ2
(16)
We provide the pseudo code in the Appendix to better illustrate how to optimize the above objective.
5 Experiments
In this section, we evaluate the proposed GIB method on three scenarios, including improvement of
graph classification, graph interpretation and graph denoising.
5.1	Baselines and settings
Improvement of graph classification: For improvement of graph classification, GIB generates
graph representation by aggregating the subgraph information. We plug GIB into various backbones
including GCN (Kipf & Welling, 2017), GAT (Velickovic et al., 2017), GIN (Xu et al., 2019) and
GraphSAGE (Hamilton et al., 2017). We compare the proposed method with the mean/sum ag-
gregation (Kipf & Welling, 2017; Velickovic et al., 2017; Hamilton et al., 2017; Xu et al., 2019)
and pooling aggregation (Zhang et al., 2018; Ranjan et al., 2020; Ying et al., 2018; Diehl, 2019) in
terms of classification accuracy. Moreover, we apply DropEdge (Rong et al., 2020) to GAT, namely
GAT+DropEdge, which randomly drop 30% edges in message-passing at node-level. Similarly, we
apply GIB to GAT+DropEdge, resulting in GAT+GIB+DropEdge. For fare comparisions, all the
backbones for different methods consist of the same 2-layer GNN with 16 hidden-size.
Graph interpretation: The goal of graph interpretation is to find the substructure which shares the
most similar property to the molecule. If the substructure is disconnected, we evaluate its largest
connected part. We compare GIB with the attention mechanism (Li et al., 2019). That is, we atten-
tively aggregate the node information for graph prediction. The interpretable subgraph is generated
by choosing the nodes with top 50% and 70% attention scores, namely Att05 and Att07. GIB outputs
the interpretation with the IB-subgraph. Then, we evaluate the absolute property bias (the absolute
value of the difference between the property of graph and subgraph) between the graph and its in-
terpretation. Similarly, for fare comparisons, all the backbones for different methods consist of the
same 2-layer GNN with 16 hidden-size.
Graph denoising: We translate the permuted graph into the line-graph and use GIB and attention
to 1) infer the real structure of graph, 2) classify the permuted graph via the inferred structure. We
further compare the performance of GCN and DiffPool on the permuted graphs.
7
Published as a conference paper at ICLR 2021
Table 4: Quantitative results on graph denoising. We report the classification accuracy (Acc), num-
ber of real edges over total real edges (Recall) and number of real edges over total edges in subgraphs
(Precision) on the test set
Method	GCN	DiffPool	GCN+Att05	GCN+Att07	GCN+GIB
Recall	-	-	0.226±0.047	0.324± 0.049	0.493± 0.035
Precision	-	-	0.638± 0.141	0.675± 0.104	0.692 ±0.061
Acc	0.617	0.658	0.649	0.667	0.684
Figure 2: The molecules with their interpretable subgraphs discovered by different methods. These
subgraphs exhibit similar chemical properties compared to the molecules on the left.
5.2	datasets
Improvement of graph classification: We evaluate different methods on the datasets of MUTAG
(Rupp et al., 2012), PROTEINS (Borgwardt et al., 2005), IMDB-BINARY and DD (Rossi &
Ahmed, 2015) datasets. 2. The statistics of the datasets are available in Table 7 of Appendix.
Graph interpretation: We construct the datasets for graph interpretation on four molecule prop-
erties based on ZINC dataset, which contains 250K molecules. QED measures the drug likeness
of a molecule, which is bounded within the range (0, 1.0). DRD2 measures the probability that a
molecule is active against dopamine type 2 receptor, which is bounded with (0, 1.0). HLM-CLint
and MLM-CLint are estimated values of in vitro human and mouse liver microsome metabolic sta-
bility (base 10 logrithm of mL/min/g). We sample the molecules with QED ≥ 0.85, DRD2 ≥ 0.50,
HLM-CLint ≥ 2, MLM-CLint ≥ 2 for each task. We use 85% of these molecules for training, 5%
for validating and 10% for testing. The statistics of the datasets are available in Table 8 of Appendix.
Graph denoising: We generate a synthetic dataset by adding 30% redundant edges for each graph
in MUTAG dataset. We use 70% of these graphs for training, 5% for validating and 25% for testing.
5.3	Results
Improvement of Graph Classification: In Table 1, we comprehensively evaluate the proposed
method and baselines on improvement of graph classification. We train GIB on various back-
bones and aggregate the graph representations only from the subgraphs. We compare the perfor-
mance of our framework with the mean/sum aggregation and pooling aggregation. This shows
that GIB improves the graph classification by reducing the redundancies in the graph structure.
Graph interpretation: Table 2 shows
the quantitative performance of different
methods on the graph interpretation task.
GIB is able to generate precise graph inter-
pretation (IB-subgraph), as the substruc-
tures found by GIB has the most similar
property to the input molecules. In Fig. 2,
GIB generates more compact and reason-
Table 5: Average number of disconnected substruc-
tures per graph selected by different methods
Method	QED	DRD2	HLM	MLM
GCN+Att05	3.38	1.94	3.11	5.16
GCN+Att07	2.04	1.76	2.75	3.00
GCN+GIB	1.57	1.08	2.29	2.06
able interpretation to the property of molecules confirmed by chemical experts. More results are
2We follow the protocol in https://github.com/rusty1s/pytorch_geometric/tree/master/benchmark/kernel
8
Published as a conference paper at ICLR 2021
Table 6: The influence of the hyper-parameter α of Lcon to the size of subgraphs.
ɑ	1	3	5	10
All	0.483±0.143	0.496±0.150	0.494±0.147	0.466±0.150
Max	0.387±0.173	0.413±0.169	0.411±0.169	0.391±0.172
provided in the Appendix. In Table 5, we compare the average number of disconnected substruc-
tures per graph since a compact subgraph should preserve more topological information. GIB gen-
erates more compact subgraphs to better interpret the graph property. Moreover, compared to the
baselines, GIB does not require a hyper-parameter to control the sizes of subgraphs, thus being more
adaptive to different tasks. Please refer to Table 9 and Table 10 of Appendix for details. The training
dynamic is shown in Fig. 7. We provide results with other MI estimators in Table 11 in Appendix.
Graph denoising: Table 4 shows the performance of different methods on noisy graph classifica-
tion. GIB outperforms the baselines on classification accuracy by a large margin due to the superior
property of IB-subgraph. Moreover, GIB is able to better reveal the real structure of permuted graphs
in terms of precision and recall rate of true edges.
5.4	Ablation study
To further understand the rolls of Lcon and LMI, we derive two variants of our method by deleting
Lcon and LMI, namely GIB w/o Lcon and GIB w/o LMI. Note that GIB w/o LMI is similar
to InfoGraph (Sun et al., 2019) and GNNExplainer (Ying et al., 2019), as they only consider to
maximize MI between latent embedding and global summarization and ignore compression. When
adapted to sub graph recognition, it is likely to be G = Gsub . We evaluate the variants with 2-layer
GCN and 16 hidden size on graph interpretation. In practice, we find that the training process of
GIB w/o Lcon is unstable as discussed in Section 4.3. Moreover, we find that GIB w/o LMI is very
likely to output Gsub = G , as it does not consider compression. Therefore, we try several initiations
for GIB w/o Lcon and LMI to get the current results. As shown in Table 3, GIB also outperforms
the variants, and thus indicates that every part of our model does contribute to the improvement of
performance.
5.5	More discussion on connectivity loss
Lcon is proposed for stabilizing the training process and resulting in compact subgraphs. As it poses
regularization for the subgraph generation, we are interested in its potential influence on the sizes of
the chosen IB-subgraph. Therefore, we show the influence of different hyper-parameters of Lcon to
the sizes of the chosen IB-subgraph. We implement the experiments with α varies in {1, 3, 5, 10}
on QED dataset and compute the mean and standard deviation of the sizes of IB-subgraph (All)
and their largest connected parts (Max). As shown in Table 6, we observe that different α result in
similar sizes of IB-subgraph. Therefore, its influence on the size of chosen subgraphs is weak.
6 Conclusion
In this paper, we have studied a subgraph recognition problem to infer a maximally informative
yet compressed subgraph. We define such a subgraph as IB-subgraph and propose the graph in-
formation bottleneck (GIB) framework for effectively discovering an IB-subgraph. We derive the
GIB objective from a mutual information estimator for irregular graph data, which is optimized by
a bi-level learning scheme. A connectivity loss is further used to stabilize the learning process. We
evaluate our GIB framework in the improvement of graph classification, graph interpretation and
graph denoising. Experimental results verify the superior properties of IB-subgraphs.
Acknowledgements
This work is partially funded by Beijing Natural Science Foundation (Grant No. JQ18017) and
Youth Innovation Promotion Association CAS (Grant No. Y201929).
9
Published as a conference paper at ICLR 2021
References
Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin Murphy. Deep variational information
bottleneck. In The International Conference on Representation Learning, 2017.
Emily Alsentzer, Samuel G Finlayson, Michelle M Li, and Marinka Zitnik. Subgraph neural net-
works. arXiv preprint arXiv:2006.10538, 2020.
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, R. De-
von Hjelm, and Aaron C. Courville. Mutual information neural estimation. In International
Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp.
530-539, 2018.
Filippo Maria Bianchi, Daniele Grattarola, and Cesare Alippi. Spectral clustering with graph neural
networks for graph pooling. In Proceedings of the 37th International Conference on Machine
Learning, 2020.
Karsten M. Borgwardt, Cheng Soon Ong, Stefan Schanauer, S. V. N. Vishwanathan, Alexander J.
Smola, and Hans-Peter Kriegel. Protein function prediction via graph kernels. In ISMB (Supple-
ment of Bioinformatics), pp. 47-56, 2005.
Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: fast learning with graph convolutional networks via
importance sampling. arXiv preprint arXiv:1801.10247, 2018.
Frederik Diehl. Edge contraction pooling for graph neural networks. CoRR, abs/1905.10990, 2019.
M. D. Donsker and S. R. S. Varadhan. Asymptotic evaluation of certain markov process expectations
for large time. Communications on Pure and Applied Mathematics, 36(2):183-212, 1983.
Yixiang Fang, Kaiqiang Yu, Reynold Cheng, Laks V. S. Lakshmanan, and Xuemin Lin. Efficient
algorithms for densest subgraph discovery. Proceedings of VLDB Endowment, 12(11):1719-
1732, 2019.
Luca Franceschi, Mathias Niepert, Massimiliano Pontil, and Xiao He. Learning discrete structures
for graph neural networks. In ICML, volume 97 of Proceedings of Machine Learning Research,
pp. 1972-1982. PMLR, 2019.
Hongyang Gao and Shuiwang Ji. Graph u-nets. In ICML, volume 97 of Proceedings of Machine
Learning Research, pp. 2083-2092. PMLR, 2019.
Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural
message passing for quantum chemistry. Proceedings of the 34th International Conference on
Machine Learning, 70:1263-1272, 2017.
Aristides Gionis and Charalampos E. Tsourakakis. Dense subgraph discovery: Kdd 2015 tutorial.
In Knowledge Discovery and Data Mining, pp. 2313-2314. ACM, 2015.
Anirudh Goyal, Riashat Islam, Daniel Strouse, Zafarali Ahmed, Matthew Botvinick, Hugo
Larochelle, Yoshua Bengio, and Sergey Levine. Infobot: Transfer and exploration via the in-
formation bottleneck. In The International Conference on Representation Learning, 2019.
William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In Advances in neural information processing systems, pp. 1024-1034, 2017.
Wenbing Huang, Tong Zhang, Yu Rong, and Junzhou Huang. Adaptive sampling towards fast graph
representation learning. arXiv preprint arXiv:1809.05343, 2018.
Maximilian Igl, Kamil Ciosek, Yingzhen Li, Sebastian Tschiatschek, Cheng Zhang, Sam Devlin,
and Katja Hofmann. Generalization in reinforcement learning with selective noise injection and
information bottleneck. In Advances in neural information processing systems, 2019.
Wei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, and Jiliang Tang. Graph structure
learning for robust graph neural networks. CoRR, abs/2005.10203, 2020a.
10
Published as a conference paper at ICLR 2021
Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Multi-objective molecule generation using
interpretable substructures. In International Conference on Machine Learning, 2020b.
Nikhil S Ketkar, Lawrence Bruce Holder, and Diane Cook. Subdue: compression-based frequent
pattern discovery in graph data. In Knowledge Discovery and Data Mining, 2005.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In The International Conference on Representation Learning, 2017.
Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and general-
ization in graph neural networks. In NeurIPS, pp. 4204-4214, 2019.
Junhyun Lee, Inyeop Lee, and Jaewoo Kang. Self-attention graph pooling. In Proceedings of the
36th International Conference on Machine Learning, 2019.
Jia Li, Yu Rong, Hong Cheng, Helen Meng, Wenbing Huang, and Junzhou Huang. Semi-supervised
graph classification: A hierarchical graph perspective. In The World Wide Wed Conference, 2019.
Yawei Luo, Ping Liu, Tao Guan, Junqing Yu, and Yi Yang. Significance-aware information bottle-
neck for domain adaptive semantic segmentation. In ICCV, pp. 6777-6786. IEEE, 2019.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers
using variational divergence minimization. In NeurIPS, 2016.
XueBin Peng, Angjoo Kanazawa, Sam Toyer, Pieter Abbeel, and Sergey Levine. Variational dis-
criminator bottleneck: Improving imitation learning, inverse rl, and gans by constraining infor-
mation flow. In The International Conference on Representation Learning, 2019.
Kaizhi Qian, Yang Zhang, Shiyu Chang, David Cox, and Mark Hasegawa-Johnson. Unsupervised
speech decomposition via triple information bottleneck. In Proceedings of the 37th International
Conference on Machine Learning, 2020.
Ekagra Ranjan, Soumya Sanyal, and Partha Pratim Talukdar. Asap: Adaptive structure aware pool-
ing for learning hierarchical graph representations. In AAAI, 2020.
Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph
convolutional networks on node classification. In International Conference on Learning Repre-
sentations, 2020. URL https://openreview.net/forum?id=Hkx1qkrKPr.
Ryan A. Rossi and Nesreen K. Ahmed. The network data repository with interactive graph analytics
and visualization. In AAAI, 2015.
Matthias Rupp, Alexandre Tkatchenko, Klaus-Robert Muller, and O. Anatole von Lilienfeld. Fast
and accurate modeling of molecular atomization energies with machine learning. Phys. Rev. Lett.,
108(5):058301, January 2012.
Fan-Yun Sun, Jordan Hoffman, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semi-
supervised graph-level representation learning via mutual information maximization. In Interna-
tional Conference on Learning Representations, 2019.
Naftali Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck method. In
Proceedings of the 37-th Annual Allerton Conference on Communication, Control and Comput-
ing, pp. 368-377, 1999.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lia, and Yoshua
Bengio. Graph attention networks. In International Conference on Learning Representation,
2017.
Rundong Wang, Xu He, Runsheng Yu, Wei Qiu, Bo An, and Zinovi Rabinovich. Learning efficient
multi-agent communication: An information bottleneck approach. In Proceedings of the 37th
International Conference on Machine Learning, 2020.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How Powerful are Graph Neural
Networks? In Proceedings of the 7th International Conference on Learning Representations,
ICLR ’19, pp. 1-17, 2019.
11
Published as a conference paper at ICLR 2021
Xifeng Yan and Jiawei Yan. gspan: graph-based substructure pattern mining. In IEEE International
Conference on Data Mining, pp. 721-724, 2002.
Rex Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L. Hamilton, and Jure Leskovec.
Hierarchical graph representation learning with differentiable pooling. In Advances in neural
information processing systems, 2018.
Rex Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnnexplainer: Gen-
erating explanations for graph neural networks. In Advances in neural information processing
systems, 2019.
Donghan Yu, Ruohong Zhang, Zhengbao Jiang, Yuexin Wu, and Yiming Yang. Graph-revised
convolutional network. CoRR, abs/1911.07123, 2019.
Mohammed Javeed Zaki. Efficiently mining frequent embedded unordered trees. Fundamenta
Information, 66(1-2):33-52, 2005.
Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. An end-to-end deep learning
architecture for graph classification. In Thirty-Second AAAI Conference on Artificial Intelligence,
2018.
Cheng Zheng, Bo Zong, Wei Cheng, Dongjin Song, Jingchao Ni, Wenchao Yu, Haifeng Chen,
and Wei Wang. Robust graph representation learning via neural sparsification. In International
Conference on Machine Learning, pp. 11458-11468. PMLR, 2020.
12
Published as a conference paper at ICLR 2021
A Appendix
A. 1 More details about Eq. 7 and Eq. 8
Here we provide more details about how to yield Eq. 7 and Eq. 8.
I(Y, Gsub)
p(y, Gsub) log p(y|Gsub)dy dG
sub
p(y, Gsub) log p(y)dy dGsub
p(y, Gsub) log p(y|Gsub)dy dGsub + H(Y)
≥
/p(y, Gsub)log q©、(y∖Gsu)dy dGsub + KL(p(y∣Gsub)∣qφι (y∣Gsub))
≥
/p(y, Gsub)log qφι (y∖Gsub)dy dGsub
(17)

1N
Nfqφι 3 |GsUbi)
N i=1
-Lcls (qφ1 (y|Gsub), ygt)
A.2 case study
To understand the bi-level objective to MI minimization in Eq. 11, we provide a case study in which
we optimize the parameters of distribution to reduce the mutual information between two variables.
Consider p(x) = sign(N(0, 1)), p(y|x) = N(y; x, σ2)3. The distribution of Y is:
p(y ) =	p(y|x)p(x)dx
=	p(y|xi)p(xi)
i
= p(y|x = 1)p(x = 1) + p(y|x = -1)p(x = -1)
=0.5(N(y; 1,σ2) + N(y; -1, σ2))
(18)
We optimize the parameter σ2 to reduce the mutual information between X and Y . For each epoch,
We sample 20000 data points from each distribution, denoted as X = {χι, χ2, ∙∙∙ , χ20000}, Y =
{y1,y2,…，ya。。。。}. The inner-step is set to be 150. After the inner optimization ends, the model
yields a good mutual information approximator and optimize σ 2 to reduce the mutual information
by minimizing LMI . We compute the mutual information With the traditional method and compare
it With LMI:
I (X,Y) = P p(χ, y) log p(ylχ) dχdy
p(y)
2。。。。
≈ ɪ X log PW
20000 白 g p(yi)
(19)
As is shoWn in Fig .9, the mutual information decreases as LMI descends. The advantage of such
bi-level objective to MI minimization in Eq. 11 is that it only requires samples instead of forms of
distribution. Moreover, it needs no tractable prior distribution for variational approximation. The
draWback is that it needs additional computation in the inner loop.
A.3 Algorithm
The algorithm is shoWn as folloWing:
3We use the toy dataset from https://github.com/mzgubic/MINE
13
Published as a conference paper at ICLR 2021
Figure 3: We use the bi-level objective to minimize the mutual information of two distributions. The
MI is consistent with the loss as LMI declines.
Algorithm 1 Optimizing the graph information bottleneck.
Input: Graph G = {A, X}, graph label Y , inner-step T , outer-step N.
Output: Subgraph Gsub
1:	function GIB(G = {A, X}, Y, T, N)
2:	θ 一 θ0,	φι 一 φi
3:	for i = 0 → N do
4:	Φ2 — φ2
5:	for t = 0 → T do
6:	φ2+1 — φ2 + η1 NΦ2 LMI
7:	end for
8:	θi+1 — θi- η2VθiL(θi,Φ1,ΦT)
9：	Φi+1 — φi- η2 Vφ1 La ,φi,φt)
10:	end for
11:	Gsub — g (G; θN)
12:	return Gsub
13:	end function
A.4 More results on graph interpretation
In Fig. 4, we show the distribution of absolute bias between the property of graphs and subgraphs.
GIB is able to generate such subgraphs with more similar properties to the original graphs.
In Fig. 5, we provide more results of four properties on graph interpretation.
A.5 More results on noisy graph classification
We provide qualitative results on noisy graph classification in Fig. 6.
A.6 Details of Datasets
We provide the statistics of datasets in experiments. For graph classification, we evaluate the pro-
posed method on four datasets, including MUTAG, PROTEINS, IMDB-BINARY and DD. The
14
Published as a conference paper at ICLR 2021
Figure 4: The histgram of absolute bias between the property of graphs and subgraphs.
Figure 5: The molecules with its interpretation found by GIB. These subgraphs exhibit similar
chemical properties compared to the molecules on the left.
statistics of these datasets are shown in Table 7 4. For graph interpretation, we preprocess the ZINC
dataset and obtain four datasets, namely QED, DRD2, HLM-CLint and MLM-CLint. The details of
these datasets are shown in Table 8. The synthetic dataset for graph denoising is basically generated
from MUTAG dataset, please refer to Section 5.2 for details.
4The statistics of datasets in graph classification are collected from http://networkrepository.com
15
Published as a conference paper at ICLR 2021
miss :1 WrOngj
miss:ɜ wrong: 2
miss:2 wrong:2
miss:3 wrong: 3
miss:2 wrong: 1
miss:ɜ wrong: 1
miss:2 wrong: 2
GCN+AttO7
miss:2 wrong:2
miss:2 wrong: 2
GCN+GIB
Permuted
Line Graph
miss :4 wrong:2
miss:5 wrong: 3
GCN+AttO5
miss: 3 wrong: 2
Figure 6: We show the blindly denoising results on permuted graphs. Each method operates on the
line-graphs and tries to recover the true topology by removing the redundant edges. Columns 4,5,6
shows results obtained by different methods, where “miss: m, wrong: n” means missing m edges
and there are n wrong edges in the output graph. GIB always recognizes more similar structure to
the ground truth (not provided in the training process) than other methods.
Table 7: Statistics of datasets in improvement of graph classification.
	MUTAG	PROTEINS	IMDB-BINARY	DD
Nodes	97.9K	43.5K	19.8K	334.9K
Edges	202.5K	162.1K	386.1K	1.7M
Density	4.2 ×10-5	1.7 ×10-4	2.0 ×10-3	3.0 ×10-5
Maximum degree	20	50	540	38
Minimum degree	2	2	4	2
Average degree	4	7	39	10
Number of triangles	2.8K	366K	18.8M	7.1M
Average number of triangles	0	8	951	21
Maximum number of triangles	12	136	17.8K	160
Average clustering coefficient	0.001965	0.316645	0.831934	0.413379
Fraction of closed triangles	0.003160	0.315106	0.803561	0.410832
Maximum k-core	5	9	117	15
Lower bound of Maximum Clique	6	5	18	4
A.7 Sizes of the chosen sub graphs in graph interpretation
In the graph interpretation task, the hyper-parameter of Lcon , α, is set to be 5 on four datasets. We
show the mean and standard deviation of the sizes of subgraphs in percent in Table 9 and Table 10.
Note that the sizes of chosen subgraphs mainly depend on task relevant information. For example,
as DRD2 measures the probability of being active against dopamine type 2 receptor, it depends on
almost the whole structure of a molecule. In contrast, HLM-CLint measures vitro human micro-
some metabolic stability, which is greatly influenced by small motifs. As shown in Table 9 and
Table 10, GCN+GIB can recognize the subgraphs with adaptive sizes on different tasks, leading
to better performance. However, in GCN+Att05 and GCN+Att07, the size of subgraphs is explic-
itly controlled by the hyper-parameter (preserve top 50% or 70 % nodes with the highest attention
scores). Therefore, the performances of these methods are limited.
16
Published as a conference paper at ICLR 2021
Table 8: Statistics of datasets in graph interpretation.
QED DRD2 HLM-CLint MLM-CLint
Number of graphs	35000	3000	25850	16666
Maximum number of nodes	29	66	37	37
Minimum number of nodes	12	13	9	7
Average number of nodes	21.82	27.43	25.14	22.44
Maximum number of edges	34	74	42	42
Minimum number of edges	12	14	9	7
Average number of edges	23.43	30.24	22.23	24.19
Dimension of node features	9	8	9	9
Figure 7: From (a) to (d), we show the loss in the training process on QED, DRD2, HLM-CLint and
MLM-CLint respectively.
A.8 The training dynamic
We show the loss in the training process in Figure. 7.
A.9 Implementation with other mutual information estimators
As shown in (Sun et al., 2019; Nowozin et al., 2016), the f-divergence family can also approximate
the mutual information. Here we provide the results of GCN+GIB with Jensen-Shannon Divergence
(JSD) and χ2 Divergence (χ2) on graph classification in Table 11. Experiment results show that our
model can also employ other mutual information estimators for bilevel optimization.
17
Published as a conference paper at ICLR 2021
Table 9: Size of the chosen subgraphs on four datasets in percent.
Method	QED	DRD2	HLM-CLint	MLM-CLint
GCN+Att05	43.5±5.4	46.8±3.1	48.1±4.1	45.1±4.1
GCN+Att07	65.8±3.4	66.7±3.0	65.8±5.7	67.6±5.5
GCN+GIB	49.6±15.0	94.8±5.3	47.7±13.7	54.7±17.2
Table 10: Size of largest connected parts used for graph interpretation in percent.
Method	QED	DRD2	HLM-CLint	MLM-CLint
GCN+Att05	22.5±9.5	34.7±9.4	23.5±7.2	29.7±7.9
GCN+Att07	43.3±11.8	54.2±13.0	45.0±15.2	41.2±8.4
GCN+GIB	41.3±16.9	92.8±10.4	29.1±10.6	36.9±16.2
A.10 Influence of Initialization
As the initialization of our model may potentially influence the final chosen subgraphs, we rerun
our model five times on the QED dataset for graph interpretation task. Then, we employ the inter-
section over union (IoU) to measure the overlap between the subgraphs in 5 different runs and the
results reported in Table 2. Similarly, we compute the IoU between the chosen subgraphs and their
largest connected parts separately, which refer to I oUall and IoUmax . We finally report the mean
and standard deviation of IoUall, I oUmax on the testing set in Table 12. We notice that different
initialization has limited influence on the chosen subgraphs, as all the results of five additional runs
have high portions of common nodes with the initial run.
18
Published as a conference paper at ICLR 2021
Table 11: Classification accuracy on graph classification. We implement the mutual information
estimator with Jensen-Shannon Divergence (JSD) and χ2 Divergence (χ2)
Method	QED	DRD2	HLM-CLint	MLM-CLint
GCN	0.743± 0.110	0.719± 0.041	0.707± 0.037	0.725± 0.046
GCN+GIB(DV)	0.776± 0.075	0.748± 0.046	0.722± 0.039	0.765± 0.050
GCN+GIB(JSD)	0.758± 0.087	0.741± 0.040	0.718± 0.044	0.759± 0.057
GCN+GIB(χ2)	0.756± 0.097	0.746± 0.057	0.721± 0.033	0.755± 0.049
Table 12: The overlap between the chosen subgraphs with different initialization.
Run	1	2	3	4	5
IoUall	0.848±0.163	0.765±0.106	0.784±0.112	0.829±0.166	0.813±0.186
I oUmax	0.779±0.330	0.696±0.310	0.742±0.333	0.757±0.335	0.762±0.304
19