Published as a conference paper at ICLR 2021
Learning Robust	State Abstractions for
Hidden-Parameter Block MDPs
AmyZhang*123	ShagunSodhani2	KhimyaKhetarpal13	JoellePineau123
1	McGill University
2	Facebook AI Research
3Mila
Abstract
Many control tasks exhibit similar dynamics that can be modeled as having com-
mon latent structure. Hidden-Parameter Markov Decision Processes (HiP-MDPs)
explicitly model this structure to improve sample efficiency in multi-task settings.
However, this setting makes strong assumptions on the observability of the state
that limit its application in real-world scenarios with rich observation spaces. In
this work, we leverage ideas of common structure from the HiP-MDP setting, and
extend it to enable robust state abstractions inspired by Block MDPs. We derive
instantiations of this new framework for both multi-task reinforcement learning
(MTRL) and meta-reinforcement learning (Meta-RL) settings. Further, we provide
transfer and generalization bounds based on task and state similarity, along with
sample complexity bounds that depend on the aggregate number of samples across
tasks, rather than the number of tasks, a significant improvement over prior work
that use the same environment assumptions. To further demonstrate the efficacy
of the proposed method, we empirically compare and show improvement over
multi-task and meta-reinforcement learning baselines.
1 Introduction
A key open challenge in AI research that remains is how
to train agents that can learn behaviors that generalize
across tasks and environments. When there is common
structure underlying the tasks, we have seen that multi-task
reinforcement learning (MTRL), where the agent learns
a set of tasks simultaneously, has definite advantages (in
terms of robustness and sample efficiency) over the single-
task setting, where the agent independently learns each
task. There are two ways in which learning multiple tasks
can accelerate learning: the agent can learn a common
representation of observations, and the agent can learn a
common way to behave. Prior work in MTRL has also
Figure 1: Visualizations of the typical
MTRL setting and the HiP-MDP setting.
leveraged the idea by sharing representations across tasks (D’Eramo et al., 2020) or providing per-
task sample complexity results that show improved sample efficiency from transfer (Brunskill & Li,
2013). However, explicit exploitation of the shared structure across tasks via a unified dynamics has
been lacking. Prior works that make use of shared representations use a naive unification approach
that posits all tasks lie in a shared domain (Figure 1, left). On the other hand, in the single-task
setting, research on state abstractions has a much richer history, with several works on improved
generalization through the aggregation of behaviorally similar states (Ferns et al., 2004; Li et al.,
2006; Luo et al., 2019; Zhang et al., 2020b).
In this work, we propose to leverage rich state abstraction models from the single-task setting, and
explore their potential for the more general multi-task setting. We frame the problem as a structured
super-MDP with a shared state space and universal dynamics model conditioned on a task-specific
hidden parameter (Figure 1, right). This additional structure gives us better sample efficiency, both
* Corresponding author: amy.x.zhang@mail.mcgill.ca
1
Published as a conference paper at ICLR 2021
theoretically, compared to related bounds (Brunskill & Li, 2013; Tirinzoni et al., 2020) and empirically
against relevant baselines (Yu et al., 2020; Rakelly et al., 2019; Chen et al., 2018; Teh et al., 2017).
We learn a latent representation with smoothness properties for better few-shot generalization to
other unseen tasks within this family. This allows us to derive new value loss bounds and sample
complexity bounds that depend on how far away a new task is from the ones already seen.
We focus on multi-task settings where dynamics can vary across tasks, but the reward function is
shared. We show that this setting can be formalized as a hidden-parameter MDP (HiP-MDP) (Doshi-
Velez & Konidaris, 2013), where the changes in dynamics can be defined by a latent variable, unifying
dynamics across tasks as a single global function. This setting assumes a global latent structure
over all tasks (or MDPs). Many real-world scenarios fall under this framework, such as autonomous
driving under different weather and road conditions, or even different vehicles, which change the
dynamics of driving. Another example is warehouse robots, where the same tasks are performed in
different conditions and warehouse layouts. The setting is also applicable to some cases of RL for
medical treatment optimization, where different patient groups have different responses to treatment,
yet the desired outcome is the same. With this assumed structure, we can provide concrete zero-shot
generalization bounds to unseen tasks within this family. Further, we explore the setting where the
state space is latent and we have access to only high-dimensional observations, and we show how to
recover robust state abstractions in this setting. This is, again, a highly realistic setting in robotics
when we do not always have an amenable, Lipschitz low-dimensional state space. Cameras are a
convenient and inexpensive way to acquire state information, and handling pixel observations is
key to approaching these problems. A block MDP (Du et al., 2019) provides a concrete way to
formalize this observation-based setting. Leveraging this property of the block MDP framework, in
combination with the assumption of a unified dynamical structure of HiP-MDPs, we introduce the
hidden-parameter block MDP (HiP-BMDP) to handle settings with high-dimensional observations
and structured, changing dynamics.
Key contributions of this work are a new viewpoint of the multi-task setting with same reward
function as a universal MDP under the HiP-BMDP setting, which naturally leads to a gradient-
based representation learning algorithm. Further, this framework allows us to compute theoretical
generalization results with the incorporation of a learned state representation. Finally, empirical
results show that our method outperforms other multi-task and meta-learning baselines in both fast
adaptation and zero-shot transfer settings.
2	Background
In this section, we introduce the base environment as well as notation and additional assumptions
about the latent structure of the environments and multi-task setup considered in this work.
A finite1, discrete-time Markov Decision Process (MDP) (Bellman, 1957; Puterman, 1995) is a
tuple hS, A, R, T, γi, where S is the set of states, A is the set of actions, R : S × A → R is the
reward function, T : S × A → Dist(S) is the environment transition probability function, and
γ ∈ [0, 1) is the discount factor. At each time step, the learning agent perceives a state st ∈ S, takes
an action at ∈ A drawn from a policy π : S × A → [0, 1], and with probability T (st+1 |st, at) enters
next state st+1 , receiving a numerical reward Rt+1 from the environment. The value function of
policy π is defined as: Vn (S) = En [P∞=0 Y tRt+1∣S0 = s]. The optimal value function V * is the
maximum value function over the class of stationary policies.
Hidden-Parameter MDPs (HiP-MDPs) (Doshi-Velez & Konidaris, 2013) can be defined by a tuple
M: hS, A, Θ, Tθ , R, γ, PΘi where S is a finite state space, A a finite action space, Tθ describes
the transition distribution for a specific task described by task parameter θ 〜Pθ, R is the reward
function, γ is the discount factor, and PΘ the distribution over task parameters. This defines a family
of MDPs, where each MDP is described by the parameter θ 〜 Pθ. We assume that this parameter θ
is fixed for an episode and indicated by an environment id given at the start of the episode.
Block MDPs (Du et al., 2019) are described by a tuple hS, A, X,p, q, Ri with an unobservable state
space S, action space A, and observable space X. p denotes the latent transition distribution p(s0 |s, a)
for s, s0 ∈ S, a ∈ A, q is the (possibly stochastic) emission mapping that emits the observations
q(x|s) for x ∈ X, s ∈ S, and R the reward function. We are interested in the setting where this
1We use this assumption only for theoretical results, but our method can be applied to continuous domains.
2
Published as a conference paper at ICLR 2021
mapping q is one-to-many. This is common in many real world problems with many tasks where the
underlying states and dynamics are the same, but the observation space that the agent perceives can be
quite different, e.g. navigating a house of the same layout but different decorations and furnishings.
Assumption 1 (Block structure (Du et al., 2019)). Each observation x uniquely determines its
generating state s. That is, the observation space X can be partitioned into disjoint blocks Xs, each
containing the support ofthe conditional distribution q(∙∣s).
Assumption 1 gives the Markov property in X, akey difference from partially observable MDPs (Kael-
bling et al., 1998; Zhang et al., 2019), which has no guarantee of determining the generating state
from the history of observations. This assumption allows us to compute reasonable bounds for our
algorithm in k-order MDPs2 (which describes many real world problems) and avoiding the intractabil-
ity of true POMDPs, which have no guarantees on providing enough information to sufficiently
predict future rewards. A relaxation of this assumption entails providing less information in the
observation for predicting future reward, which will degrade performance. We show empirically that
our method is still more robust to a relaxation of this assumption compared to other MTRL methods.
Bisimulation is a strict form of state abstraction, where two states are bisimilar if they are behaviorally
equivalent. Bisimulation metrics (Ferns et al., 2011) define a distance between states as follows:
Definition 1 (Bisimulation Metric (Theorem 2.6 in Ferns et al. (2011))). Let (S, A, P, r) be a finite
MDP and met the space of bounded pseudometrics on S equipped with the metric induced by the
uniform norm. Define F : met 7→ met by
F(d)(s, s0) = m∈aχ(∣rα -Ts I + YW(G(P；,P；，工
where W(d) is the Wasserstein distance between transition probability distributions. Then F has a
unique fixed point d which is the bisimulation metric.
A nice property of this metric d is that difference in optimal value between two states is bounded by
their distance as defined by this metric.
Theorem 1 (V * is LiPschitz With respect to d (Ferns et al., 2004)). Let V * be the optimal value
function for a given discount factor γ. Then V* is Lipschitz continuous with respect to d with
Lipschitz Constant ι-1γ,
1
IV * (s) - V * (s0)I≤ 「 d(s,s0).
Therefore, We see that bisimulation metrics give us a Lipschitz value function With respect to d.
For doWnstream evaluation of the representations We learn, We use Soft Actor Critic (SAC) (Haarnoja
et al., 2018), an off-policy actor-critic method that uses the maximum entropy frameWork for soft
policy iteration. At each iteration, SAC performs soft policy evaluation and improvement steps. The
policy evaluation step fits a parametric soft Q-function Q(st, at) using transitions sampled from the
replay buffer D by minimizing the soft Bellman residual,
J(Q) = E(s
t,st,rt,st+ι)~D](Q(St,aD-rt - γV(Xt+1))].
The target value function V is approximated via a Monte-Carlo estimate of the following expectation,
V(xt+ι) = Est+1 〜∏ [Q(xt+1, at+ι) - αlog∏(at+ι∣st+ι)],
where Q is the target soft Q-function parameterized by a weight vector obtained from an exponentially
moving average of the Q-function weights to stabilize training. The policy improvement step then
attempts to project a parametric policy π(atISt) by minimizing KL divergence between the policy
and a Boltzmann distribution induced by the Q-function, producing the following objective,
J(∏) = ESt〜DiEat〜∏[αlog(∏(at∣st)) - Q(st,at)].
2Any k-order MDP can be made Markov by stacking the previous k observations and actions together.
3
Published as a conference paper at ICLR 2021
Figure 2: Graphical model of HiP-BMDP setting (left). Flow diagram of learning a HiP-BMDP (right). Two
environment ids are selected by permuting a randomly sampled batch of data from the replay buffer, and the loss
objective requires computing the Wasserstein distance of the predicted next-step distribution for those states.
3	THE HIP-BMDP SETTING
The HiP-MDP setting (as defined in Section 2) assumes full observability of the state space. However,
in most real-world scenarios, we only have access to high-dimensional, noisy observations, which
often contain irrelevant information to the reward. We combine the Block MDP and HiP-MDP
settings to introduce the Hidden-Parameter Block MDP setting (HiP-BMDP), where states are
latent, and transition distributions change depending on the task parameters θ. This adds an additional
dimension of complexity to our problem — we first want to learn an amenable state space S, and a
universal dynamics model in that representation3. In this section, we formally define the HiP-BMDP
family in Section 3.1, propose an algorithm for learning HiP-BMDPs in Section 3.2, and finally
provide theoretical analysis for the setting in Section 3.3.
3.1	The Model
A HiP-BMDP family can be described by tuple hS, A, Θ, Tθ, R, γ, PΘ, X, qi, with a graphical model
of the framework found in Figure 2. We are given a label k ∈ {1, ..., N} for each of N environments.
We plan to learn a candidate Θ that unifies the transition dynamics across all environments, effectively
finding T(∙, ∙, θ). For two environment settings θi, θj∙ ∈ Θ, we define a distance metric:
W(Tθi (s,a),Tθj (s,a)).	⑴
The Wasserstein-1 metric can be written as Wd(P, Q) = supf∈F ∣∣Eχ~Pf(x) - Ey~Qf(y)l,
where Fd is the set of 1-Lipschitz functions under metric d (Miiller, 1997). We omit d but use
d(x, y) = kx - yk1 in our setting. This ties distance between θ to the maximum difference in the
next state distribution of all state-action pairs in the MDP.
Given a HiP-BMDP family MΘ, we assume a multi-task setting where environments with specific
θ ∈ Θ are sampled from this family. We do not have access to θ, and instead get environment labels
I1, I2, ..., IN. The goal is to learn a latent space for the hyperparameters θ4. We want θ to be smooth
with respect to changes in dynamics from environment to environment, which we can set explicitly
through the following objective:
W2 (p>(,s^t+ι |st ,at ,Ψ(I1 )),p(st+ι |st ,at,Ψ(I2))),	⑵
given environment labels I1 , I2 and ψ : Z+ 7→ Rd, the encoder that maps from environment label,
the set of positive integers, to θ .
3.2	LEARNING HIP-BMDPS
The premise of our work is that the HiP-BMDP formulation will improve sample efficiency and
generalization performance on downstream tasks. We examine two settings, multi-task reinforcement
3We overload notation here since the true state space is latent.
4We again overload notation here to refer to the learned hyperparameters as θ, as the true ones are latent.
d(θi,θj) := s,am∈{aSx,A}
llψ(II) - ψ(I2)||1 = max
4
Published as a conference paper at ICLR 2021
learning (MTRL) and meta-reinforcement learning (meta-RL). In both settings, we have access to N
training environments and a held-out set of M evaluation environments, both drawn from a defined
family. In the MTRL setting, we evaluate model performance across all N training environments
and ability to adapt to new environments. Adaptation performance is evaluated in both the few-shot
regime, where we collect a small number of samples from the evaluation environments to learn each
hidden parameter θ, and the zero-shot regime, where we average θ over all training tasks. We evaluate
against ablations and other MTRL methods. In the meta-RL setting, the goal for the agent is to
leverage knowledge acquired from the previous tasks to adapt quickly to a new task. We evaluate
performance in terms of how quickly the agent can achieve a minimum threshold score in the unseen
evaluation environments (by learning the correct θ for each new environment).
Learning a HiP-BMDP approximation of a family of MDPs requires the following components: i) an
encoder that maps observations from state space to a learned, latent representation, φ : X 7→ Z, ii) an
environment encoder ψ that maps an environment identifier to a hidden parameter θ, iii) a universal
dynamics model T conditioned on task parameter θ. Figure 2 shows how the components interact
during training. In practice, computing the maximum Wasserstein distance over the entire state-action
space is computationally infeasible. Therefore, we relax this requirement by taking the expectation
over Wasserstein distance with respect to the marginal state distribution of the behavior policy. We
train a probabilistic universal dynamics model T to output the desired next state distributions as
Gaussians5 *, for which the 2-Wasserstein distance has a closed form:
W2(N(m1, Σι), N (m2, ∑2))2 = ||mi - m2∣∣2 + ∣∣∑. - ∑?"F,
where ∣∣∙∣∣f is the FrobeniUs norm.
Given that we do not have access to the true universal dynamics function across all environments, it
mUst be learned. The objective in EqUation (2) is accompanied by an additional objective to learn T ,
giving a final loss fUnction:
L(ψ,T) = MSE[^∖ψ(I1) - ψ(I2)∣∣2,W2(T(SI,∏(s∣),Ψ(I1)),T(SI,∏(sI),Ψ(I2)))
X------------------------------------------------------------------------
Θ learning error
+MSE	, atI1, ψ(I1)), StI+1 1	+MSE	,atI2,ψ(I2)),StI+21
'-------------------------------------{-------------------------------------}
Model learning error
}
(3)
where red indicates gradients are stopped. Transitions {StI1 , atI1 , StI+1 1, I1} and {StI2, atI2, StI+2 1, I2}
from two different environments (I1 6= I2) are sampled randomly from a replay bUffer. In practice,
we scale the Θ learning error, oUr task bisimUlation metric loss, Using a scalar valUe denoted as αψ .
3.3	Theoretical Analysis
In this section, we provide valUe boUnds and sample complexity analysis of the HiP-BMDP approach.
We have additional new theoretical analysis of the simpler HiP-MDP setting in Appendix B. We first
define three additional error terms associated with learning a R, T , θ-bisimUlation abstraction,
R :=	sup	∣R(x1, a) - R(x2, a)∣,
a∈A,
x1,x2 ∈X,φ(x1)=φ(x2)
T :=	sup	ΦT(x1, a) - ΦT(x2, a)1,
a∈A,	1
x1,x2 ∈X,φ(x1)=φ(x2)
..O …
eθ := kθ - θk 1.
ΦT denotes the lifted version of T, where we take the next-step transition distribUtion from observa-
tion space X and lift it to latent space S. We can think of eR , eT as describing a new MDP which
is close - but not necessarily the same, if eR, eτ > 0 -to the original Block MDP These two error
terms can be compUted empirically over all training environments and are therefore not task-specific.
5This is not a restrictive assumption to make, as any distribution can be mapped to a Gaussian with an
encoder of sufficient capacity.
5
Published as a conference paper at ICLR 2021
θ, on the other hand, is measured as a per-task error. Similar methods are used in Jiang et al. (2015)
to bound the loss of a single abstraction, which we extend to the HiP-BMDP setting with a family of
tasks.
Value Bounds. We first evaluate how the error in θ prediction and the learned bisimulation repre-
Sentation affect the optimal QM of the learned MDP, by first bounding its distance from the optimal
θ
Q* of the true MDP for a single-task.
Theorem 2 (Q error). Given an MDP MM© built on a (cr, ET, cq)-approximate bisimulation abstrac-
tion of an instance ofa HiP-BMDP Mq , we denote the evaluation of the optimal Q function of MM ^
on M as [QM ]mθ. The value difference with respect to the optimal QM is upper bounded by
Ms
IIQMθ - [QM^]Mθ∣l∞ ≤ eR + Y(CT + eθ)2(1maxγ).
Proof in Appendix C. As in the HiP-MDP setting, we can measure the transferability of a specific
policy π learned on one task to another, now taking into account error from the learned representation.
Theorem 3 (Transfer bound). Given two MDPs Mqi and Mqj, we can bound the difference in Qπ
between the two MDPs for a given policy π learned under an cR, cT, cqi -approximate abstraction of
Mqi and applied to
llQMθj - [QMθJMθj ll∞ ≤ er + Y(eT + eθi + kθi - θjk1) 2(1B-IY) ∙
This result clearly follows directly from Theorem 2. Given a policy learned for task i, Theorem 3
gives a bound on how far from optimal that policy is when applied to task j . Intuitively, the more
similar in behavior tasks i and j are, as denoted by kθi - θj k1 , the better π performs on task j .
Finite Sample Analysis. In MDPs (or families of MDPs) with large state spaces, it can be unreal-
istic to assume that all states are visited at least once, in the finite sample regime. Abstractions are
useful in this regime for their generalization capabilities. We can instead perform a counting analysis
based on the number of samples of any abstract state-action pair.
We compute a loss bound with abstraction φ which depends on the size of the replay buffer D,
collected over all tasks. Specifically, we define the minimal number of visits to an abstract state-action
pair, nφ(D) = minχ∈φ(s),a∈/ ∣Dχ,a∣∙ This sample complexity bound relies on a Hoeffding-style
inequality, and therefore requires that the samples in D be independent, which is usually not the case
when trajectories are sampled.
Theorem 4 (Sample Complexity). For any φ which defines an (cR, cT, cq)-approximate bisimulation
abstraction on a HiP-BMDPfamily Mθ, we define the empirical measurement of QM
MlS
over D to be
QM D. Then, with probability ≥ 1 一 δ,
llQMθ —[QMD]MθL ≤er + Y(ET + cθ)2(R-γy+(⅛⅛ j2n⅛ylog2φXM∙⑷
This performance bound applies to all tasks in the family and has two terms that are affected by using
a state abstraction: the number of samples nφ(D), and the size of the state space ∣φ(X)|. We know
that ∣φ(X)| ≤ |X| as behaviorally equivalent states are grouped together under bisimulation, and
nφ(D) is the minimal number of visits to any abstract state-action pair, in aggregate over all training
environments. This is an improvement over the sample complexity of applying single-task learning
without transfer over all tasks, and the method proposed in Brunskill & Li (2013), which both would
rely on the number of tasks or number of MDPs seen.
4 Experiments & Results
We use environments from Deepmind Control Suite (DMC) (Tassa et al., 2018) to evaluate our
method for learning HiP-BMDPs for both multi-task RL and meta-reinforcement learning settings.
6
Published as a conference paper at ICLR 2021
We consider two setups for evaluation: i) an interpolation setup and ii) an extrapolation setup
where the changes in the dynamics function are interpolations and extrapolations between the
changes in the dynamics function of the training environment respectively. This dual-evaluation
setup provides a more nuanced understanding of how well the learned model transfers across the
environments. Implementation details can be found in Appendix D and sample videos of policies at
https://sites.google.com/view/hip-bmdp.
Environments. We create a family of MDPs using the exist-
ing environment-task pairs from DMC and change one environ-
ment parameter to sample different MDPs. We denote this pa-
rameter as the perturbation-parameter. We consider the follow-
ing HiP-BMDPs: 1. Cartpole-Swingup-V0: the mass of the
pole varies, 2. Cheetah-Run-V0: the size of the torso varies, 3.
Figure 3:	Variation in
Cheetah-Run-V0 tasks.
Walker-Run-V0: the friction coefficient between the ground and the walker’s legs varies, 4.
Walker-Run-V1: the size of left-foot of the walker varies, and 5. Finger-Spin-V0: the size
of the finger varies. We show an example of the different pixel observations for Cheetah-Run-V0
in Figure 3. Additional environment details are in Appendix D.
We sample 8 MDPs from each MDP family by sampling different values for the perturbation-
parameter. The MDPs are arranged in order of increasing values of the perturbation-parameter
such that we can induce an order over the family of MDPs. We denote the ordered MDPs as
A - H. MDPs {B, C, F, G} are training environments and {D, E} are used for evaluating the
model in the interpolation setup (i.e. the value of the perturbation-parameter can be obtained by
interpolation). MDPs {A, H} are for evaluating the model in the extrapolation setup (i.e. the value
of the perturbation-parameter can be obtained by extrapolation). We evaluate the learning agents by
computing average reward (over 10 episodes) achieved by the policy after training for a fixed number
of steps. All experiments are run for 10 seeds, with mean and standard error reported in the plots.
Multi-Task Setting. We first consider a multi-task setup where the agent is trained on four related,
but different environments with pixel observations. We compare our method, HiP-BMDP, with the
following baselines and ablations: i) DeepMDP (Gelada et al., 2019) where we aggregate data across
all training environments, ii) HiP-BMDP-nobisim, HiP-BMDP without the task bisimulation metric
loss on task embeddings, iii) Distral, an ensemble of policies trained using the Distral algorithm (Teh
et al., 2017) with SAC-AE (Yarats et al., 2019) as the underlying policy, iv) PCGrad (Yu et al., 2020),
and v) GradNorm (Chen et al., 2018). For all models, the agent sequentially performs one update
per environment. For fair comparison, we ensure that baselines have at least as many parameters
as HiP-BMDP. Distral has more parameters as it trains one policy per environment. Additional
implementation details about baselines are in Appendix D.1.
In Figures 4, and 10 (in Appendix), we observe that for all the models, performance deteriorates when
evaluated on interpolation/extrapolation environments. We only report extrapolation results in the
main paper because of space constraints, as they were very similar to the interpolation performance.
The gap between the HiP-BMDP model and other baselines also widens, showing that the proposed
approach is relatively more robust to changes in environment dynamics.
At training time (Figure 9 in Appendix), we observe that HiP-BMDP consistently outperforms other
baselines on all the environments. The success of our proposed method can not be attributed to task
embeddings alone as HiP-BMDP-nobisim also uses task embeddings. Moreover, only incorporating
the task-embeddings is not guaranteed to improve performance in all the environments (as can be
seen in the case of Cheetah-Run-V0). We also note that the multi-task learning baselines like
Distral, PCGrad, and GradNorm sometimes lag behind even the DeepMDP baseline, perhaps because
they do not leverage a shared global dynamics model.
Meta-RL Setting. We consider the Meta-RL setup for evaluating the few-shot generalization
capabilities of our proposed approach on proprioceptive state, as meta-RL techniques are too time-
intensive to train on pixel observations directly. Specifically, we use PEARL (Rakelly et al., 2019),
an off-policy meta-learning algorithm that uses probabilistic context variables, and is shown to
outperform common meta-RL baselines like MAML-TRPO (Finn et al., 2017) and ProMP (Rothfuss
et al., 2019) on proprioceptive state. We incorporate our proposed approach in PEARL by training
the inference network qφ(z|c) with our additional HiP-BMDP loss. The algorithm pseudocode can
be found in Appendix D. In Figure 5 we see that the proposed approach (blue) converges faster to a
7
Published as a conference paper at ICLR 2021
Environment Steps (*10^6)
■o 600
400
? 200
S
0
0.0	0.2	0.4	0.6	0.8
WaIker-Run-VO
Environment Steps (*10^6)
600
Ξ
to
Φ 400
O 200
Walker-Run-Vl
0
0.0	0.2	0.4	0.6	0.8
Environment Steps (*10^6)
Finger-SPin-VO
尸⅛q⅛⅛m
1000
P
m 800
M
⅛ 600
⅛ 400
i -
LU
0
OQ 0：2	04	0：6	03
Environment Steps (*10^6)
HiP-BMDP	HiP-BMDP-nobisim
DeepMDP	Distral	GradNorm	PCGrad
Figure 4: Multi-Task Setting. Zero-shot generalization performance on the extrapolation tasks. We see that
our method, HiP-BMDP, performs best against all baselines across all environments. Note that the environment
steps (on the x-axis) denote the environment steps for each task. Since we are training over four environments,
the actual number of steps is approx. 3.2 million.
threshold reward (green) than the baseline for Cartpole-Swingup-V0 and Walker-Walk-V1.
We provide additional results in Appendix E.
Evaluating the Universal Transition Model. We investigate
how well the transition model performs in an unseen environment
by only adapting the task parameter θ . We instantiate a new MDP,
sampled from the family of MDPs, and use a behavior policy to
collect transitions. These transitions are used to update only the
θ parameter, and the transition model is evaluated by unrolling
the transition model for k-steps. We report the average, per-
step model error in latent space, averaged over 10 environments.
While We expect both the proposed setup and baseline setups to
adapt to the new environment, we expect the proposed setup to Figure 6: Average per-step model
adapt faster because of the exploitation of underlying structure. error (in latent space) after unrolling
In Figure 6, We indeed observe that the proposed HiP-BMDP the transition model for 100 steps.
model adapts much faster than the ablation HiP-BMDP-nobisim.
HiP-BMDP
HiP-BMDP-nobisim
Relaxing the Block MDP Assumption. We incorporate sticky observations into the environment
to determine hoW HiP-BMDP behaves When the Block MDP assumption is relaxed. For some
probability p (set to 0.1 in practice), the current observation is dropped, and the agent sees the
previous observation again. In Figure 7, We see that even in this setting the proposed HiP-BMDP
model outperforms the other baseline models.
5 Related Work
Multi-task learning has been extensively studied in RL With assumptions around common properties
of different tasks, e.g., reWard and transition dynamics. A lot of Work has focused on considering tasks
as MDPs and learning optimal policies for each task While maximizing shared knoWledge. HoWever,
in most real-World scenarios, the parameters governing the dynamics are not observed. Moreover, it
is not explicitly clear hoW changes in dynamics across tasks are controlled. The HiP-BMDP setting
provides a principled Way to change dynamics across tasks via a latent variable.
Much existing Work in the multi-task reinforcement learning (MTRL) setting focuses on learning
shared representations (Ammar et al., 2014; Parisotto et al., 2016; Calandriello et al., 2014; Maurer
et al., 2016; Landolfi et al., 2019). D’Eramo et al. (2020) extend approximate value iteration bounds
in the single-task setting to the multi-task by computing the average loss across tasks and Brunskill
& Li (2013) offer sample complexity results, Which still depend on the number of tasks, unlike
ours. Sun et al. (2020); Tirinzoni et al. (2020) also obtain PAC bounds on sample complexity for
the MTRL setting, but Sun et al. (2020) relies on a constructed state-action abstraction that assumes
Train steps (* 10^3)
Walker-Walk-Vl
-ŋ 350
g 300
250
u 200
1150
■5.100
山50
6	100	200	300	400
Train Steps (* 10 人3)
WaIker-Walk-VI
-ŋ 350
§ 300
250
u 200
1 150
-5.100
lij 50
0
Train Steps (* 10 人3)
Figure 5: FeW-shot generalization performance on the interpolation (2 left) and extrapolation (2 right) tasks.
Green line shoWs a threshold reWard. 100 steps are used for adaptation to the evaluation environments.
8
Published as a conference paper at ICLR 2021
Environment Steps (*10^6)
Finger-Spin-VO
800
Ξ
600
(υ
t£.
.y 400-
P
O
IΛ
Q.200
LU
0
0.0	0.2	0.4	0.6	0.8
Environment Steps (*10^6)
HiP-BMDP	HiP-BMDP-nobisim	DeepMDP
Distral	GradNorm	PCGrad
Figure 7: Zero-shot generalization performance on the evaluation tasks in the MTRL setting with partial
observability. HiP-BMDP (ours) consistently outperforms other baselines (left). We also show decreasing
performance by HiP-BMDP as p increases (right). 10 seeds, 1 stderr shaded.
a discrete and tractably small state space. Tirinzoni et al. (2020) assumes access to a generative
model for any state-action pair and scales with the minimum of number of tasks or state space. In
the rich observation setting, this minimum will almost always be the number of tasks. Similar to
our work, Perez et al. (2020) also treats the multi-task setting as a HiP-MDP by explicitly designing
latent variable models to model the latent parameters, but require knowledge of the structure upfront,
whereas our approach does not make any such assumptions.
Meta-learning, or learning to learn, is also a related framework with a different approach. We focus
here on context-based approaches, which are more similar to the shared representation approaches
of MTRL and our own method. Rakelly et al. (2019) model and learn latent contexts upon which a
universal policy is conditioned. However, no explicit assumption of a universal structure is leveraged.
Amit & Meir (2018); Yin et al. (2020) give a PAC-Bayes bound for meta-learning generalization that
relies on the number of tasks n. Our setting is quite different from the typical assumptions of the
meta-learning framework, which stresses that the tasks must be mutually exclusive to ensure a single
model cannot solve all tasks. Instead, we assume a shared latent structure underlying all tasks, and
seek to exploit that structure for generalization. We find that under this setting, our method indeed
outperforms policies initialized through meta-learning.
The ability to extract meaningful information through state abstractions provides a means to gen-
eralize across tasks with a common structure. Abel et al. (2018) learn transitive and PAC state
abstractions for a distribution over tasks, but they concentrate on finite, tabular MDPs. One approach
to form such abstractions is via bisimulation metrics (Givan et al., 2003; Ferns et al., 2004) which
formalize a concrete way to group behaviorally equivalent states. Prior work also leverages bisim-
ulation for transfer (Castro & Precup, 2010), but on the policy level. Our work instead focuses on
learning a latent state representation and established theoretical results for the MTRL setting. Recent
work (Gelada et al., 2019) also learns a latent dynamics model and demonstrates connections to
bisimulation metrics, but does not address multi-task learning.
6 Discussion
In this work, we advocate for a new framework, HiP-BMDP, to address the multi-task reinforcement
learning setting. Like previous methods, HiP-BMDP assumes a shared state and action space
across tasks, but additionally assumes latent structure in the dynamics. We exploit this structure
through learning a universal dynamics model with latent parameter θ, which captures the behavioral
similarity across tasks. We provide error and value bounds for the HiP-MDP (in appendix) and
HiP-BMDP settings, showing improvements in sample complexity over prior work by producing a
bound that depends on the number of samples in aggregate over tasks, rather than number of tasks
seen at training time. Our work relies on an assumption that we have access to an environment
id, or knowledge of when we have switched environments. This assumption could be relaxed by
incorporating an environment identification procedure at training time to cluster incoming data into
separate environments. Further, our bounds rely L∞ norms for measuring error and the value and
transfer bounds. In future work we will investigate tightening these bounds with Lp norms.
9
Published as a conference paper at ICLR 2021
References
David Abel, Dilip Arumugam, Lucas Lehnert, and Michael Littman. State abstractions for lifelong
reinforcement learning. In International Conference on Machine Learning, pp. 10-19, 2018.
Ron Amit and Ron Meir. Meta-learning by adjusting priors based on extended PAC-Bayes theory. In
International Conference on Machine Learning (ICML), pp. 205-214, 2018.
Haitham Bou Ammar, Eric Eaton, Paul Ruvolo, and Matthew Taylor. Online multi-task learning for
policy gradient methods. In International conference on machine learning, pp. 1206-1214, 2014.
Richard Bellman. Dynamic Programming. Princeton University Press, Princeton, NJ, USA, 1 edition,
1957.
Dimitri P. Bertsekas and John N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific, 1st
edition, 1996. ISBN 1886529108.
Emma Brunskill and Lihong Li. Sample complexity of multi-task reinforcement learning. Uncertainty
in Artificial Intelligence - Proceedings of the 29th Conference, UAI 2013, 09 2013.
Daniele Calandriello, Alessandro Lazaric, and Marcello Restelli. Sparse multi-task reinforcement
learning. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.),
Advances in neural information processing systems 27, pp. 819-827. Curran Associates, Inc., 2014.
Pablo Samuel Castro and Doina Precup. Using bisimulation for policy transfer in mdps. In Twenty-
Fourth AAAI Conference on Artificial Intelligence, 2010.
Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient
normalization for adaptive loss balancing in deep multitask networks. In Jennifer G. Dy and
Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning,
ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings
of Machine Learning Research, pp. 793-802. PMLR, 2018. URL http://proceedings.
mlr.press/v80/chen18a.html.
Carlo D’Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, and Jan Peters. Sharing knowledge
in multi-task deep reinforcement learning. In International Conference on Learning Representa-
tions, 2020.
Finale Doshi-Velez and George Konidaris. Hidden Parameter Markov Decision Processes: A Semi-
parametric Regression Approach for Discovering Latent Task Parametrizations. arXiv:1308.3513
[cs], August 2013. arXiv: 1308.3513.
Simon S. Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford.
Provably efficient RL with rich observations via latent state decoding. CoRR, abs/1901.09018,
2019.
Norm Ferns, Prakash Panangaden, and Doina Precup. Metrics for finite markov decision processes. In
Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence, UAI ’04, pp. 162-169,
Arlington, Virginia, United States, 2004. AUAI Press. ISBN 0-9749039-0-6.
Norm Ferns, Prakash Panangaden, and Doina Precup. Bisimulation metrics for continuous markov
decision processes. SIAM J. Comput., 40(6):1662-1714, December 2011. ISSN 0097-5397. doi:
10.1137/10080484X.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pp. 1126-1135. JMLR. org, 2017.
Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc G Bellemare. Deep-
mdp: Learning continuous latent space models for representation learning. arXiv preprint
arXiv:1906.02736, 2019.
Robert Givan, Thomas Dean, and Matthew Greig. Equivalence notions and model minimization in
markov decision processes. Artificial Intelligence, 147(1-2):163-223, 2003.
10
Published as a conference paper at ICLR 2021
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. volume 80 of Proceedings
OfMachine Learning Research, pp. 1861-1870, Stockholmsmassan, Stockholm Sweden, 10-15 Jul
2018. PMLR. URL http://proceedings.mlr.press/v80/haarnoja18b.html.
Nan Jiang. Notes on State Abstractions. pp. 12, 2018. URL https://nanjiang.cs.
illinois.edu/files/cs598/note4.pdf.
Nan Jiang, Alex Kulesza, and Satinder Singh. Abstraction selection in model-based reinforcement
learning. In InternatiOnal COnference On Machine Learning, pp. 179-188, 2015.
Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. Planning and acting in
partially observable stochastic domains. Artif. Intell., 101(1-2):99-134, May 1998. ISSN 0004-
3702.
Nicholas C. Landolfi, Garrett Thomas, and Tengyu Ma. A model-based approach for sample-efficient
multi-task reinforcement learning, 2019.
Lihong Li, Thomas J. Walsh, and Michael L. Littman. Towards a unified theory of state abstraction
for mdps. In PrOceedings Of the Ninth InternatiOnal SympOsium On Artificial Intelligence and
Mathematics, pp. 531-539, 2006.
Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorith-
mic framework for model-based deep reinforcement learning with theoretical guarantees. In
InternatiOnal COnference On Learning RepresentatiOns, 2019.
Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The benefit of multitask
representation learning. J. Mach. Learn. Res., 17(1):2853-2884, January 2016. ISSN 1532-4435.
Remi Munos. Error bounds for approximate value iteration. In Proceedings of the 20th National
COnference On Artificial Intelligence - VOlume 2, AAAI’05, pp. 1006-1011. AAAI Press, 2005.
ISBN 157735236x.
Alfred MUller. Integral probability metrics and their generating classes of functions. Advances in
Applied Probability, 29(2):429-443, 1997. doi: 10.2307/1428011.
Emilio Parisotto, Jimmy Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and transfer
reinforcement learning. In ICLR, 2016.
Christian F. Perez, Felipe Petroski Such, and Theofanis Karaletsos. Generalized Hidden Parameter
MDPs Transferable Model-based RL in a Handful of Trials. arXiv:2002.03072 [cs, stat], February
2020. arXiv: 2002.03072.
Martin L Puterman. Markov decision processes: Discrete stochastic dynamic programming. Journal
of the Operational Research Society, 1995.
Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Efficient off-policy
meta-reinforcement learning via probabilistic context variables. In Kamalika Chaudhuri and Ruslan
Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning,
volume 97, pp. 5331-5340, Long Beach, California, USA, 09-15 Jun 2019. PMLR.
Jonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, and Pieter Abbeel. ProMP: Proximal
meta-policy search. In International Conference on Learning Representations, 2019.
Yanchao Sun, Xiangyu Yin, and Furong Huang. Temple: Learning template of transitions for sample
efficient multi-task rl, 2020.
Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden,
Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy Lillicrap, and Martin Riedmiller.
DeepMind control suite. Technical report, DeepMind, January 2018.
Yee Teh, Victor Bapst, Wojciech M. Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas
Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning. In I. Guyon, U. V.
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in
neural information processing systems 30, pp. 4496-4506. Curran Associates, Inc., 2017.
11
Published as a conference paper at ICLR 2021
Andrea Tirinzoni, Riccardo Poiani, and Marcello Restelli. Sequential transfer in reinforcement
learning with a generative model. In Proceedings of the 35th International Conference on Machine
Learning, ICML 2020, Proceedings of Machine Learning Research. PMLR, 2020.
Denis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, and Rob Fergus. Improving
sample efficiency in model-free reinforcement learning from images. 2019.
Mingzhang Yin, George Tucker, Mingyuan Zhou, Sergey Levine, and Chelsea Finn. Meta-learning
without memorization. In International Conference on Learning Representations, 2020.
Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn.
Gradient surgery for multi-task learning, 2020.
Amy Zhang, Zachary C. Lipton, Luis Pineda, Kamyar Azizzadenesheli, Anima Anandkumar, Laurent
Itti, Joelle Pineau, and Tommaso Furlanello. Learning causal state representations of partially
observable environments. The Multi-disciplinary Conference on Reinforcement Learning and
Decision Making, 2019.
Amy Zhang, Clare Lyle, Shagun Sodhani, Angelos Filos, Marta Kwiatkowska, Joelle Pineau, Yarin
Gal, and Doina Precup. Invariant causal prediction for block mdps. In International Conference
on Machine Learning (ICML), 2020a.
Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning
invariant representations for reinforcement learning without reconstruction. arXiv preprint
arXiv:2006.10742, 2020b.
12
Published as a conference paper at ICLR 2021
A B isimulation Bounds
We first look at the Block MDP case only (Zhang et al., 2020a), which can be thought of as the
single-task setting in a HiP-BMDP. We can compute approximate error bounds in this setting by
denoting φ an (R, T)-approximate bisimulation abstraction, where
R :=	sup	R(x1, a) - R(x2, a),
a∈A,
x1,x2 ∈X,φ(x1)=φ(x2)
T :=	sup	ΦT (x1, a) - ΦT (x2, a)1.
a∈A,	1
x1,x2 ∈X,φ(x1)=φ(x2)
ΦT denotes the lifted version of T , where we take the next-step transition distribution from observa-
tion space X and lift it to latent space S.
Theorem 5.	Given an MDP MM built on a (cr, ET)-approximate bisimulation abstraction ofBlock
MDP M, we denote the evaluation of the optimal Q function of Mt on M as [QM]m. The value
difference with respect to the optimal QM is upper bounded by
IIQM - [QM]MII∞ ≤ eR + YeT 2(1maaxγ)).
Proof. From Theorem 2 in Jiang (2018).
□
B Theoretical Results for the HiP-MDP Setting
We explore the HiP-MDP setting, where a low-dimensional state space is given, to highlight the
results that can be obtained just from assuming this hierarchical structure of the dynamics.
B.1 Value B ounds
Given a family of environments MΘ, we bound the difference in expected value between two sampled
MDPs, Mθi , Mθj ∈ MΘ using d(θi, θj). Additionally, we make the assumption that we have a
behavior policy ∏ that is near both optimal policies ∏θ%,∏θ.. We use KL divergence to define this
neighborhood for ∏^.,
dKLE,πθJ = Es〜ρ∏ [KL(n(1S),πθi(IS))1/2].	⑸
We start with a bound for a specific policy π . One way to measure the difference between two tasks
Mθi , Mθj is to measure the difference in value when that policy is applied in both settings. We
show the relationship between the learned θ and this difference in value. The following results are
similar to error bounds in approximate value iteration (Munos, 2005; Bertsekas & Tsitsiklis, 1996),
but instead of tracking model error, we apply these methods to compare tasks with differences in
dynamics.
Theorem 6.	Given policy π, the difference in expected value between two MDPs drawn from the
family of MDPs Mθi, Mθj ∈ MΘ is bounded by
I吗∏ -% I ≤ 1-γkθi-θjk1.	⑹
Proof. We use a telescoping sum to prove this bound, which is similar to Luo et al. (2019). First, we
let Zk denote the discounted sum of rewards if the first k steps are in Mθi , and all steps t > k are in
Mθj,
∞
Zk := E	∀t≥0,at〜π(st)	£ YtR(St, at) .
'∀j>t≥0,st+ι 〜Tθi(st ,at) t= t=0	-
∀t≥j,st+ι~Tθj (st,at)
By definition, we have Z∞ = Vθπ and Z0 = Vθπ . Now, the value function difference can be written
as a telescoping sum,
∞
Vθπi - Vθπj = X(Zk+1 - Zk).	(7)
k=0
13
Published as a conference paper at ICLR 2021
Each term can be simplified to
Zk+1 - Zk= γk+1Esw∏,% Esk+1 〜%(.∣sk,"k),[%(Sk+1) - Vn(sk+1].
Sk +1~T%(Tsk ,ak )
Plugging this back into Equation (7),
Vn - Va = 1 - γ E SZPI)Ii, Es0^Tθi (-∖s,a)Vθj (s0)-Es0 〜% (-∖s,a)Vθj (s') ∙
/ a〜π(s)
This expected value difference is bounded by the Wasserstein distance between Tq., Tq.,
I啜-V∏l ≤ 六W(Tθi,Tθj)
Y “八	AIl
= -----∣∣θi - %k ι using Equation (1).
1 - Y
□
Another comparison to make is how different the optimal policies in different tasks are with respect
to the distance ∣% - θj∣∣.
Theorem 7. The difference in expected optimal value between two MDPs Mq. , Mq. ∈ Mθ is
bounded by,
V：-唳 l≤	kθi-θj ∣1.	(8)
Proof.
IVθ*(S)- Vs (S)I = I max Qθi(s,α) - max (QQj(S,az)∣
a	a
≤ max ∣Qq.(s,a) - QQ (s,a)∣
We can bound the RHS with
sup IQei(s, a)-Qθj (s, ɑ)l ≤ sup ∣γq.(s, o)-γqj (s, α)∣+γsup 区，〜%(∙∖s,α)啜(SO)-ES〃〜”.(«,。)%； (s'')∣
s,a	s,a	s,a	Z	3
All MDPs in Mθ have the same reward function, so the first term is 0.
sup IQθi (S, a) - Qθj (S, a) I ≤ γ sup IEszzTθi (-∖s,a)vθi (SZ)- ES〃〜Tθ. (-∖s,a)vθj (SzZ)I
s,a	s,a	%	j
=γ sup ESO〜Tθi(∙∖s,a) [Vθ* (SZ)- vθj (SZ)] + ES〃〜Tθj (∙∖s,a), [VQ (SZ)- Vθ* (SZZ)]
s,a	S0zTθi (-∖S,a)
≤ γ sup I ESzzTθi (-∖S,a) [Vθi (SZ)- vθj (SZ)] 1 + γ sup I ES 〃〜Tθ.(∙∖S,a), [%； (SZ)- Vθ* (SZZ)] 1
S，a	S，a	S0zTθi (-∖S,a)
≤ γ sup IESO〜Tθi(∙∖S,a) [Vθ* (SZ)- Vθ* (SZ)] 1 + 1 Y Il θi - θj l∣1
S,a	1 - Y
≤ Y maχ i Ve(s) - VQ (S)I+ ι - γ Il θi- θj l∣ι
=Y max i max QQ. (s, a) — max
Sai	a0
≤ Y sup I QQi(s,a) - QQj (s,α)∣ + 7ɪ Il θi - θj∣ι
S,a	1 - Y
QQj(S,α')∣ + 1 - Ilθi - θjl∣ι
Solving for supS,a 1 QQi (S,a) - QQj (S, a)1,
sup I QQi (S,a) - QQj (S,a) 1 ≤ 门]、2 llθi - θj 111.
S,a	(1 - Y)
Plugging this back in,
V：(s) - VQ3 (s)l ≤ (T-^∣θi-θj∣ι.
□
14
Published as a conference paper at ICLR 2021
Both these results lend more intuition for casting the multi-task setting under the HiP-MDP formalism.
The difference in the optimal performance between any two environments is controlled by the
distance between the hidden parameters for corresponding environments. One can interpret the
hidden parameter as a knob to allow precise changes across the tasks.
B.2 Expected Error Bounds
In MTRL, we are concerned with the performance over a family of tasks. The empirical risk is
typically defined as follows for T tasks (Maurer et al., 2016):
1T
eavg (θ) = T fE['(ft(h(wt(X ))),Y))].	(9)
t=1
Consequently, we bound the expected loss over the family of environments E with respect to θ. In
particular, we are interested in the average approximation error and define it as the absolute model
error averaged across all environments:
Theorem 8. Given a family of environments MΘ, each parameterized with an underlying true
hidden parameter θ1,θ2, •…,Θe , and let θ1,θ2, ∙∙∙ ,Θe be their respective approximations such that
the average approximation error across all environments is bounded as follows:
eavg (θ) ≤ (I-YP,	(11)
where each environment’s parameter θi is -close to its approximation θi i.e. d(θi, θi) ≤ , where d
is the distance metric defined in Eq. 1.
Proof. We here consider the approximation error averaged across all environments as follows:
1E
eavg(θ) = * X∣Vθi(S)- Vθ* (s)∣
i=1
1E
Eavg (θ) = E ɪ^l mfx Qθi(s,a) - ma X Qθi (s, a0)l
E i=1	a	a
1E
≤ E EmaX Qi(s,a) - Qθi(s,a)|
i=1
(12)
Let us consider an environment θi ∈ ME for which we can bound the RHS with
SUp IQθi(s, a)-Qθi(s, a)| ≤ SUp %i(s, a)-rθi(s, a)l+YSUp 屿〜τθ^(∙∣s,a)Vθ^)(SO)-Es，，〜Toi(.|s,a)%：(s00)∣
s,a i	s,a	s,a	i	i
Considering the family of environments ME have the same reward function and is known, resulting
in first term to be 0.
15
Published as a conference paper at ICLR 2021
sup IQ"Ga)- Qtei (s,a)l ≤ Y sup |Es0 〜Tθ^(∙∣s,a)V7(SO)- Es00 〜Tθi(∙∣s,a)%: (SOO)I
s,a	s,a
=Y sup Es0〜Tθ^ (∙∣s,α) [Vθei (SO)- Vθ* (SO)] + Es00〜Tθi(∙∣s,a), [V^∙ (SO)- Vθ* (SOO)]
s,a	*	*	SEi (∙∣s,α)	2
≤ Y sup lEs0〜Tθ^(∙∣s,a)[啜(SO)- Vθ* (S')] l + Y sup IEs00〜h&稔(∙∣s,α),[嗫(SO)- Vθ* (S")] |
s,a	2	2	s,a	s0 〜Tθ^ (∙∣s,α)	2
≤ Y sup IEs0〜T9y∙∣s,α) [V^e (SO)- Vθ* (S')] l + γ~^-- Mi -幻
s,a	θi	i	1 - Y
≤ Y max ιVe(S) - Ve (S)I+ι--Y1a-切
=Y max | max Q； (s, a) — max Qe (s, aO
≤ Y sup IQθi (S, a) - Qθi (S, a)| + 1 - Y lθi - θi |
)I +「lθi-θil
Solving for suPs,a〔QtzGa) - Qei (s,a)I,
supIQ^(s,a) - Qei(s,a)I ≤ @ 二尸 |庆-%|	(13)
Plugging Eq. 13 back in Eq. 12,
1E Y
eavg ⑹ ≤ E £ (1 - y)2 lθi - θi 1
Y
=7√i-----司 I%=1 - θi=1∣ + I%=2 - θi=2∣ +----+ ∣θi=Ε - θi=E |
E(1 - Y)2
5 T	• 1 .1 . . 1 1 • .	1	.1	•	. 1 ∕⅛'	1 . 1	1 1 •	1 • 1 1	.
We now consider that the distance between the approximated θi and the underlying hidden parameter
θi ∈ ME is defined as in Eq. 1, such that: d(θi, θi) ≤ θ
Plugging this back concludes the proof,
eavg(θ) ≤ (1 — Y)2 .
□
It is interesting to note that the average approximation error across all environments is independent
of the number of environments and primarily governed by the error in approximating the hidden
parameter θ for each environment.
C Additional Results and Proofs for HiP-BMDP Results
We first compute L∞ norm bounds for Q error under approximate abstractions and transfer bounds.
Theorem 9 (Q error). Given an MDP M© built on a (er, ET, 6t)-approximate bisimulation abstrac-
tion of an instance ofa HiP-BMDP Mt, we denote the evaluation of the optimal Q function of M ©
on M as [QM ]m@. The value difference with respect to the optimal QM is upper bounded by
mθ
IIQMθ - [QM^]Mθ∣l∞ ≤ eR + YeT + eθ)2(f-Y) ∙
Proof. In the HiP-BMDP setting, we have a global encoder φ over all tasks, but the difference in
transition distribution also includes θ. The reward functions are the same across tasks, so there is
16
Published as a conference paper at ICLR 2021
no change to R. However, we now must incorporate difference in dynamics in T . Assuming we
have two environments with hidden parameters θi, θj ∈ Θ, we can compute θTi ,θj across those two
environments by joining them into a super-MDP:
θTi,θj =	sup	ΦTθi(x1, a) - ΦTθj(x2,a)1
a∈A,	1
x1,x2 ∈X,φ(x1)=φ(x2)
≤	sup	ΦTθi(x1,a) - ΦTθi(x2,a)1 + ΦTθi (x2, a) - ΦTθj (x2, a)1
a∈A,	1	1
x1,x2 ∈X,φ(x1)=φ(x2)
≤	sup	ΦTθi(x1, a) - ΦTθi(x2, a)1 +	sup	ΦTθi(x2, a) - ΦTθj(x2,a)1
a∈A,	a∈A,
x1,x2 ∈X,φ(x1)=φ(x2)	x1,x2∈X,φ(x1)=φ(x2)
= θTi + kθi - θj k1
This result is intuitive in that with a shared encoder learning a per-task bisimulation relation, the
distance between bisimilar states from another task depends on the change in transition distribution
between those two tasks. We can now extend the single-task bisimulation bound (Theorem 5) to the
HiP-BMDP setting by denoting approximation error of θ as ∣∣θ - θ∣∣ ι < ⑦.	□
Theorem 4. For any φ which defines an (R, T , θ)-approximate bisimulation abstraction on a
HiP-BMDP family Mθ, we define the empirical measurement of QM^ over D to be QMD. Then,
with probability ≥ 1 - δ,
UQMθ-[QMD]MθL ≤ eR + Y(CT + eθ)2(R-Y) + (⅛Y)2S2n1D)lθg2φX产.(14)
Proof.
IIqMθ - [QMD]Mθ∣l∞ ≤ IlQM0- [QMθ]Mθ∣l∞ + ll[QMJM°- [QMD]Mθ∣l∞
=UQM0- [QM JM° ll∞ + IqM ^-QMD l∣∞
The first term is solved by Theorem 2, so we only need to solve the second term using McDiarmid’s
inequality and the knowledge that the value function of a bisimulation representation is ɪ--Y-Lipschitz
from Theorem 1.
First, we write this difference to be a deviation from an expectation in order to apply the concentration
inequality.
∣∣QMθ - QMd L = IIqm0 - TDQM0 + TDQMθ - TDQMD L
≤	IIqM θ - tdqM θ k∞ + YkQM ^- qM D k∞
≤	占kTDφQMθ-TφQM0k∞
Now we can apply McDiarmid’s inequality,
P	DIQM °- QM d I ≥〔 ≤2eχp(- RmLY∣2).
Solve for the t that makes this inequality hold for all (φ(x), a) ∈ X × A with a union bound over all
∣φ(X)∖∖A∖ abstract states,
八 Rmax「一“2|。(X )∖∖A∖
t> L V 2nφ(D)log —δ 一.
Combine to get
Il Q*	∣∙Q*	] Ilr _|_ ，	、 RmaX	.	Rmax	I 1 I	2∖φ(X) ∖∖A∖
IlQM°- [QM d ]m。ll∞ ≤ eR+MeT+ eθ)2(i-γ)+ (Γ-ʒy X 2nφ(D)log -δ-.
□
17
Published as a conference paper at ICLR 2021
Algorithm 1 HiP-BMDP training for the Multi-task RL setting.
Require: Along with DeePMDP components (Actor, Critic, Dynamics Model (M)), an additional
environment encoder ψ to generated task-specific θ parameters.
1:	for each timestep t = 1..T do
2:	for each Ti do
3:	a：〜πi"St).
4:	Sti 〜pidst,at)	.
5:	D -D∪ (st, at,r(sii, at), Sti), i
6:	UPDATECRITIC(D, i) (uses data only from ith task)
7:	UPDATEACTOR(D, i) (uses data only from ith task)
8:	UPDATEUSINGHIP-BMDPLOSS(D, i)
9:	end for
10:	end for
Algorithm 2 UpdateModelUsingHip-BMDPLoss
Require: Batches of data for the different tasks {Ti}i=1...T sampled from the Replay Buffer D,
learning rates α1 and α2, index of the current task i, Transition Model M, environment encoder
ψ.
1:	for each batch of dataset t = 1..T, t 6= i do
2:	Compute L(ψ, M) = Li (ψ, M, i, t) using Equation (3)
3:	ψ  ψ - αιVθ Pi L
4:	M <— M - α2 Vθ Pi L
5:	end for
D Additional Implementation Details
In Figure 8, we show the variation in the left foot of the walker.
Figure 8: Variation in Walker (V1) across different tasks.
MTRL Algorithm The Multitask RL algorithm for the HiP-BMDP setting can be found in Al-
gorithm 1. We take the DeepMDP baseline Gelada et al. (2019) and incorporate our HiP-BMDP
objective (text shown in red color)
Meta-RL Algorithm The meta-RL algorithm for the HiP-MDP setting can be found in Algorithm 3.
We take the PEARL algorithm (Rakelly et al., 2019) and incorporate our HiP-MDP objective (text
shown in red color)
D.1 Baselines
For PCGrad, the authors recommend projecting the gradient with respect to all previous tasks. In
practice, that leads to very poor training. Instead, we observe that it is better to project the gradients
with respect to any one task (randomly selected per update). We use this scheme in all the experiments.
For GradNorm, we observe that the learned weights wi (for weighing per-task loss) can become
negative for some tasks (which means the model tries to unlearn those tasks). In practice, we clamp
the wi values to not become smaller than a threshold.
18
Published as a conference paper at ICLR 2021
D.2 Hyper Parameters
D.2.1 MTRL Algorithm
All the hyper parameters (for MTRL algorithm) are listed in Table 1.
Parameter name	Value
Actor learning rate	-10-3-
Actor update frequency	2
Actor log Stddev bounds	[-10, 2]
Alpha learning rate	10-4
Batch size	128
Critic learning rate	10-3
Critic target update frequency	2
Critic Q-function soft-update rate TQ	0.01
Critic encoder soft-update rate TenC	0.05
Discount Y	0.99
Decoder learning rate	10-3
Critic update frequency	1
Encoder feature dimension	100
Encoder learning rate	10-3
Number of encoder layers	4
Number of filters in encoder	32
Hidden Dimension	1024
Replay buffer capacity	1000000
Optimizer	Adam
Temperature Adam's βι	0.5
Init temperature	0.1
Number of embeddings in ψ	10
Embedding dimension for ψ	100
Learning rate for ψ	10-3
αψ (ie α for ψ) for Finger-SPin-V0	0.1
αψ (ie a for ψ) for Cheetah-Run-V0	0.1
αψ (ie a for ψ) for WaIker-Run-V0	1.0
αψ (ie a for ψ) for Walker-Run-V1	0.01
adistai (ie a for Distral) for Finger-Spin-V0	0.05
αdistai (ie a for Distral) for Cheetah-Run-V0	0.01
αdistai (ie a for Distral) for Walker-Run-V0	0.05
adistai (ie a for Distral) for Walker-Run-V1	0.01
βdistal (ie β for DiStral)		1.0
Table 1: A complete overview of used hyper parameters.
D.2.2 MetaRL Algorithm
For MetaRL, we use the same hyperparameters as used by PEARL Rakelly et al. (2019). We set
αφ = 0.01 for all environments, other than Walker-Stand environments where αφ = 0.001.
E Additional Results
Along with the environments described in 4, we considered the following additional environments:
1.	Walker-Stand-V0: Walker-Stand task where the friction coefficient, between the
ground and the walker’s leg, varies across different environments.
2.	Walker-Walk-V0: Walker-Walk task where the friction coefficient, between the
ground and the walker’s leg, varies across different environments.
3.	Walker-Stand-V1: Walker-Stand task where the size of left-foot of the walker
varies across different environments.
19
Published as a conference paper at ICLR 2021
Algorithm 3 HiP-MDP training for the meta-RL setting.
Require: Batch of training tasks {Ti}i=1...T from p(T), learning rates α1 , α2, α3, αφ
1: 2:	Initialize replay buffers Bi for each training task while not done do
3:	for each Ti do
4:	Initialize context Ci = {}
5:	for k = 1, . . . , K do
6:	Sample Z 〜 q。(Zei)
7:	Gather data from πθ (a|s, Z) and add to Bi
8:	Update Ci = {(sj, aj, sj, rj)}j：i...N 〜Bi
9:	end for
10:	end for
11:	for step in training steps do
12:	for each Ti do
13:	Sample context Ci 〜Sc(Bi) and RL batch bi 〜Bi
14:	Sample Z 〜 q。(ZIC i)
15:	Lactor = Lactor(bi, Z)
16:	Licritic = Lcritic(bi,Z)
17:	LKL = βDKL(q(Zei )||r (Z))
18:	Sample a RL batch bj from any other task j
19:	Compute LiBiSim = Li (q, T, i, j) using the equation 3
20:	end for
21:	φ J φ - α1 Vφ Pi (l'Icritic + LKL + αφ × LBiSim)
22:	θ∏ J θ∏ - α2Ne Pi Lactor
23:	θQ J θQ - α3Ve Pi Lcritic
24:	end for
25:	end while
4.	Walker-Walk-V1: Walker-Walk task where the size of left-foot of the walker varies
across different environments.
E.1 Multi-Task Setting
In Figure 10, we observe that the HiP-BMDP method consistently outperforms other baselines
when evaluated on the interpolation environments (zero-shot transfer). As noted previously, the
effectiveness of our proposed model can not be attributed to task-embeddings alone as HiP-BMDP-
nobisim model uses the same architecture as the HiP-BMDP model but does not include the task
bisimulation metric loss. We hypothesise that the Distral-Ensemble baseline behaves poorly because
it cannot leverage a shared global dynamics model.
Cheetah-Run-VO	WaIker-Run-VO	Walker-Run-Vl	Finqer-Spin-VO
O
PJeMαjαypo-d 山
0.0	0.2	0.4	0.6	0.8
Environment Steps (*10^6)
PJeMαjαypo-d 山
0.0	0.2	0.4	0.6	0.8
Environment Steps (*10^6)
PJeMαjαypo-d 山
0.0	0.2	0.4	0.6	0.8
Environment Steps (*10^6)
PJeMαjαypo-d 山
0.0	0.2	0.4	0.6	0.8
Environment Steps (*10^6)
HiP-BMDP	HiP-BMDP-nobisim
DeepMDP	Distral	GradNorm	PCGrad
Figure 9: Multi-Task Setting. Performance on the training tasks.Note that the environment steps (on
the x-axis) denote the environment steps for each task. Since we are training over four environments,
the actual number of steps is approximately 3.2 million.
E.2 Meta-RL Setting
We provide the Meta-RL results for the additional environments. Recall that we extend the PEARL
algorithm (Rakeny et al., 2019) by training the inference network qφ(z∣c) with our additional HiP-
BMDP loss. The algorithm pseudocode can be found in Appendix D. In Figure 13, we show the
20
Published as a conference paper at ICLR 2021
Cheetah-Run-VO	WaIker-Run-VO
Walker-Run-Vl	Finger-Spin-VO
4 ♦,
PJeMea~po~d 山
HiP-BMDP
0.0	0.2	0.4	0.6	0.8
Environment Steps (*10^6)
PJeM ① B ypo~d 山
0.0	0.2	0.4	0.6	0.8
Environment Steps (*10^6)
PJeM ① B ypo~d 山
Distral
0.0	0.2	0.4	0.6	0.8
Environment Steps (*10^6)
PJeM ① B ypo~d 山
PCGrad
0.0	0.2	0.4	0.6	0.8
Environment Steps (*10^6)
HiP-BMDP-nobisim
DeepMDP
GradNorm
Cheetah-Run-VO
WaIker-Run-VO
Figure 10: Zero-shot generalization performance on the interpolation tasks in the MTRL setting. HiP-BMDP
(ours) consistently outperforms other baselines. 10 seeds, 1 standard error shaded.
Figure 11: Average per-step model error (in latent space) after unrolling the transition model for 5 steps.
results on the interpolation setup and in Figure 14, we show the results on the extrapolation setup.
In some environments (eg Walker-Walk-V1), the proposed approach (blue) converges faster to a
threshold reward (green) than the baseline. In the other environments, the gains are quite small.
E.3 Evaluating the Universal Transition Model.
We investigate how well the transition model performs in an unseen environment by only adapting
the task parameter θ . We instantiate a new MDP, sampled from the family of MDPs, and use a
behavior policy to collect transitions. These transitions are used to update only the θ parameter, and
the transition model is evaluated by unrolling the transition model for k-steps. In Figures 11 and 12,
we report the average, per-step model error in latent space, averaged over 10 environments over 5
and 100 steps respectively. While we expect both the proposed setup and baseline setups to adapt
to the new environment, we expect the proposed setup to adapt faster because of the exploitation
of underlying structure. We indeed observe that for both 5 step and 100 step unrolls, the proposed
HiP-BMDP model adapts much faster than the baseline HiP-BMDP-nobisim (Figures 11 and 12)
21
Published as a conference paper at ICLR 2021
Walker-Run-Vl
2015105
(ʤS OOI)」0±l山φpoz
10	20	30	40	50	60
Number of Updates (*10^3)
Figure 12: Average per-step model error (in latent space)
after unrolling the transition model for 100 steps.
i 1 I ■
p」① B ypo~d 山
Waker-Wak-VO
HiP-BMDP
PEARL
PJeMea~po~d 山
WakemStandNO
HiP-BMDP
PEARL
-O 200
⅛
I 150
,y ιoo
P
O
义50
d
LU
0
O 250 500 750 IOOO 1250 1500
Train Steps (* 10 ^3)
O 500 IOOO 1500 2000 2500
Train Steps (* 10 ^3)
O 200 400 600 800 IOOO 1200
Train Steps (* 10 ^3)
O 100	200	300	400
Train Steps (* 10 ^3)
Walker-Stand-Vl
400
馆350
ω 300
t≤
U 250
200
∣S-150
6	200 400 600 800 IOOO 1200
Train Steps (* 10 ^3)

Figure 13: Few-shot generalization performance on the interpolation tasks on Walker-Run-V0, Walker-Walk-V0
Walker-Stand-V0, Cheetah-Run-V0 (top row), Walker-Run-V1, Walker-Walk-V1 and Walker-Stand-V1 (bottom
row) respectively. We note that for the Walker-Walk-V1, the proposed approach (blue) converges faster to a
threshold reward (green) than the baseline. In other environments, the gains are quite small.
WaIker-WaIk-VO
WaIker-Run-VO
1000 2000 3000 4000 5000
Train Steps (* 10 ^3)
* * » 1 I ■
PJeMea-po-d 山
HiP-BMDP
PEARL
PJeM ① B--POMdW
WaIker-Stand-VO
HiP-BMDP
PEARL
Cheetah-Run-VO
-o 200
⅛
1150
u 100
P
O
与50
d
LU
0
HiP-BMDP
PEARL
250 500 750 1000 1250 1500
Train Steps (* 10 ^3)
0	1000 2000 3000 4000 5000
Train Steps (* 10 ^3)
WakemRunNl
⅛ 200
M
9 150
⅛1oo
I 50
LU
0
HiP-BMDP
PEARL
0	500	1000 1500 2000 2500
0	100	200	300	400
Train Steps (* 10 ^3)
200 400 600 800 1000 1200
Train Steps (* 10 ^3)
0	200 400 600 800 1000 1200
Train Steps (* 10 ^3)
Train Steps (* 10 ^3)

Figure 14: Few-shot generalization performance on the extrapolation tasks on Walker-Run-V0, Walker-Walk-
V0, Walker-Stand-V0, Cheetah-Run-V0 (top row), Walker-Run-V1, Walker-Walk-V1 and Walker-Stand-V1
(bottom row) respectively. We note that for the Walker-Walk-V1, the proposed approach (blue) converges faster
to a threshold reward (green) than the baseline. In other environments, the gains are quite small.
22