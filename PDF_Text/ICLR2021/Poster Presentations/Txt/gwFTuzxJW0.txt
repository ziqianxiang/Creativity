Published as a conference paper at ICLR 2021
Stochastic Security: Adversarial Defense
Using Long-Run Dynamics of Energy-Based
Models
Mitch Hill *
Department of Statistics and Data Science
University of Central Florida
mitchell.hill@ucf.edu
Jonathan Mitchell * * & Song-Chun Zhu ^ ^
Department of Computer Science*
Department of Statistics^
University of California, Los Angeles
jcmitchell@ucla.edu
sczhu@stat.ucla.edu
Ab stract
The vulnerability of deep networks to adversarial attacks is a central problem for
deep learning from the perspective of both cognition and security. The current most
successful defense method is to train a classifier using adversarial images created
during learning. Another defense approach involves transformation or purification
of the original input to remove adversarial signals before the image is classified.
We focus on defending naturally-trained classifiers using Markov Chain Monte
Carlo (MCMC) sampling with an Energy-Based Model (EBM) for adversarial
purification. In contrast to adversarial training, our approach is intended to secure
highly vulnerable pre-existing classifiers. To our knowledge, no prior defensive
transformation is capable of securing naturally-trained classifiers, and our method
is the first to validate a post-training defense approach that is distinct from current
successful defenses which modify classifier training.
The memoryless behavior of long-run MCMC sampling will eventually remove
adversarial signals, while metastable behavior preserves consistent appearance
of MCMC samples after many steps to allow accurate long-run prediction. Bal-
ancing these factors can lead to effective purification and robust classification.
We evaluate adversarial defense with an EBM using the strongest known at-
tacks against purification. Our contributions are 1) an improved method for
training EBM’s with realistic long-run MCMC samples for effective purifica-
tion, 2) an Expectation-Over-Transformation (EOT) defense that resolves ambi-
guities for evaluating stochastic defenses and from which the EOT attack nat-
urally follows, and 3) state-of-the-art adversarial defense for naturally-trained
classifiers and competitive defense compared to adversarial training on CIFAR-
10, SVHN, and CIFAR-100. Our code and pre-trained models are available at
https://github.com/point0bar1/ebm-defense.
1	Motivation and Contributions
Deep neural networks are highly sensitive to small input perturbations. This sensitivity can be
exploited to create adversarial examples that undermine robustness by causing trained networks to
produce defective results from input changes that are imperceptible to the human eye (Goodfellow
et al., 2015). The adversarial scenarios studied in this paper are primarily untargeted white-box
attacks on image classification networks. White-box attacks have full access to the classifier (in
particular, to classifier gradients) and are the strongest attacks against the majority of defenses.
Many whitebox methods have been introduced to create adversarial examples. Strong iterative
attacks such as Projected Gradient Descent (PGD) (Madry et al., 2018) can reduce the accuracy of a
naturally-trained classifier to virtually 0. Currently the most robust form of adversarial defense is to
train a classifier on adversarial samples in a procedure known as adversarial training (AT) (Madry
* Equal Contribution
1
Published as a conference paper at ICLR 2021
et al., 2018). Another defense strategy, which we will refer to as adversarial preprocessing (AP),
uses defensive transformations to purify an image and remove or nullify adversarial signals before
classification (Song et al. (2018); Guo et al. (2018); Yang et al. (2019), and others). AP is an attractive
strategy compared to AT because it has the potential to secure vulnerable pre-existing classifiers.
Defending naturally-trained classifiers is the central focus of this work.
Athalye et al. (2018) revealed that many preprocessing defenses can be overcome with minor
adjustments to the standard PGD attack. Both stochastic behavior from preprocessing and the
computational difficulty of end-to-end backpropagation can be circumvented to attack the classifier
through the defensive transformation. In this paper we carefully address Athalye et al. (2018) to
evaluate AP with an EBM using attacks with the greatest known effectiveness and efficiency.
Figure 1: Left: Visualization of calculating our stochastic logits FH (x) from (5). The input image x
is replicated H times and parallel Langevin updates with a ConvNet EBM are performed on each
replicate to generate {Xh}H=「Purified samples are sent in parallel to our naturally-trained classifier
network f (x) and the resulting logits {f (Xh)}H=ι are averaged to produce FH(x). The logits FH(x)
give an approximation of our true classifier logits F (x) in (4) that can be made arbitrarily precise by
increasing H. Right: Graphical diagram of the Langevin dynamics (3) that we use for T (x). Images
are iteratively updated with a gradient from a naturally-trained EBM (1) and Gaussian noise Zk .
Langevin sampling using an EBM with a ConvNet potential (Xie et al., 2016) has recently emerged as
a method for AP (Du & Mordatch, 2019; Grathwohl et al., 2020). However, the proposed defenses are
not competitive with AT (see Table 1 and Croce & Hein (2020)). In the present work we demonstrate
that EBM defense of a naturally-trained classifier can be stronger than standard AT (Madry et al.,
2018) and competitive with state-of-the-art AT (Zhang et al., 2019; Carmon et al., 2019).
Our defense tools are a classifier trained with labeled natural images and an EBM trained with
unlabeled natural images. For prediction, we perform Langevin sampling with the EBM and send the
sampled images to the naturally-trained classifier. An intuitive visualization of our defense method
is shown in Figure 1. Langevin chains constitute a memoryless trajectory that removes adversarial
signals, while metastable sampling behaviors preserve image classes over long-run trajectories.
Balancing these two factors leads to effective adversarial defense. Our main contributions are:
•	A simple but effective adjustment to improve the convergent learning procedure from
Nijkamp et al. (2020). Our adjustment enables realistic long-run sampling with EBMs
learned from complex datasets such as CIFAR-10.
•	An Expectation-Over-Transformation (EOT) defense that prevents the possibility of a
stochastic defense breaking due to random variation in prediction instead of an adversarial
signal. The EOT attack (Athalye et al., 2018) naturally follows from the EOT defense.
•	Experiments showing state-of-the-art defense for naturally-trained classifiers and competitive
defense compared to state-of-the-art AT.
2	Improved Convergent Learning of Energy-Based Models
The Energy-Based Model introduced in (Xie et al., 2016) is a Gibbs-Boltzmann density
P(X; θ) = z1θ) exp{-U (x; θ)}
(1)
2
Published as a conference paper at ICLR 2021
where x ∈ RD is an image signal, U (x; θ) is a ConvNet with weights θ and scalar output, and
Z(θ) = X exp{-U (x; θ)}dx is the intractable normalizing constant. Given i.i.d. samples from a
data distribution q(χ), one can learn a parameter θ* such that p(x; θ*) ≈ q(x) by minimizing the
expected negative log-likelihood L(θ) = Eq[- logp(X; θ)] of the data samples. Network weights θ
are updated using the loss gradient
nm
VL(θ) ≈ — X VθU(X+; θ) — — X VθU(X- θ)
n i=1	m i=1
(2)
where {Xi+ }in=1 are a batch of training images and {Xi-}im=1 are i.i.d. samples from p(x; θ) obtained
via MCMC. Iterative application of the Langevin update
Xk + 1 = Xk----2 VXk U(Xk; θ) + TZk ,	⑶
where Zk 〜 N(0,Id) and τ > 0 is the step size parameter, is used to obtain the samples {X-}m=ι.
SGD
75k batches
100k batches
150k batches
nur-gno
Adam
10k batches
klfe
25k batches
50k batches
控晒黑
遛出目E]9
■是有艮逅
Figure 2: Comparison of long-run and short-run samples over model updates for our improved
method of convergent learning. The model is updated in a non-convergent learning phase with the
Adam optimizer for the first 50,000 batches. The majority of short-run synthesis realism is learned
during this phase, but the long-run samples are very unrealistic. The second learning phase uses
SGD with a low learning rate. Short-run synthesis changes very little, but the long-run distribution
gradually aligns with the short-run distribution.
Nijkamp et al. (2020) reveal that EBM learning heavily gravitates towards an unexpected outcome
where short-run MCMC samples have a realistic appearance and long-run MCMC samples have an
unrealistic appearance. The work uses the term convergent learning to refer to the expected outcome
where short-run and long-run MCMC samples have similar appearance, and the term non-convergent
learning to refer to the unexpected but prevalent outcome where models have realistic short-run
samples and oversaturated long-run samples. Convergent learning is essential for our defense strategy
because long-run samples must be realistic so that classifiers can maintain high accuracy after
Langevin transformation (see Section 4.1 and Appendix D).
As observed by Nijkamp et al. (2020), we were unable to learn a convergent model when updating
θ with the Adam optimizer (Kingma & Ba, 2015). Despite the drawbacks of Adam for convergent
learning, it is a very effective tool for obtaining realistic short-run synthesis. Drawing inspiration
from classifier training from Keskar & Socher (2017), we learn a convergent EBM in two phases.
The first phase uses Adam to update θ to achieve realistic short-run synthesis. The second phase
uses SGD to update θ to align short-run and long-run MCMC samples to correct the degenerate
steady-state from the Adam phase. This modification allows us to learn the convergent EBMs for
complex datasets such as CIFAR-10 using 20% of the computational budget of Nijkamp et al. (2020).
See Figure 2. We use the lightweight EBM from Nijkamp et al. (2019) as our network architecture.
We use long run chains for our EBM defense to remove adversarial signals while maintaining
image features needed for accurate classification. The steady-state convergence property ensures
adversarial signals will eventually vanish, while metastable behaviors preserve features of the initial
state. Theoretical perspectives on our defense can be found in Appendix C and a comparison of
convergent and non-convergent EBM defenses can be found in Section 4.1 and Appendix D.
3
Published as a conference paper at ICLR 2021
3	Attack and Defense Formulation
3.1	Classification with Stochastic Transformations
Let T(x) be a stochastic pre-processing transformation for a state x ∈ RD. Given a fixed input x,
the transformed state T (x) is a random variable over RD. In this work, T (x) is obtained from K
steps of the Langevin update (3) starting from X0 = x. One can compose T (x) with a deterministic
classifier f (x) ∈ RJ (for us, a naturally-trained classifier) to define a new classifier F (x) ∈ RJ as
F(x) = ET (x) [f (T (x))].	(4)
F (x) is a deterministic classifier even though T (x) is stochastic. The predicted label for x is
then c(x) = arg maxj F(x)j. In this work, f(x) denotes logits and F(x) denotes expected logits,
although other choices are possible (e.g. softmax outputs). We refer to (4) as an Expectation-Over-
Transformation (EOT) defense. The classifier F (x) in (4) is simply the target of the EOT attack
(Athalye et al., 2018). The importance of the EOT formulation is well-established for adversarial
attacks, but its importance for adversarial defense has not yet been established. Although direct
evaluation of F(x) is generally impossible, the law of large numbers ensures that the finite-sample
approximation of F (x) given by
1H
FH(x) = HE f (Xh) where， Xh 〜T(x) i.i.d.,
(5)
h=1
can approximate F(x) to any degree of accuracy for a sufficiently large sample size H. In other
∕∖
words, F (x) is intractable but trivial to accurately approximate via FH (x) given enough computation.
logit value Fh(x)j for class j
Stochastic Logits for Stable Classification
logit value PH(X)J for class j
Figure 3: The histograms display different realizations of the logits FH (x) for the correct class and
the second most probable class for images x1 (left) and x2 (right) over different choices of H. In both
cases, F (x) (dashed vertical lines) gives correct classification. However, the overlap between the
logit histograms of FH(x1) indicate a high probability of misclassification even for largeH, while
FH(x2) gives correct prediction even for small H because the histograms are well-separated. The
EOT defense formulation (4) is essential for securing borderline images such as x1.
In the literature both attackers (Athalye et al., 2018; Tramer et al., 2020) and defenders (Dhillon et al.,
2018; Yang et al., 2019; Grathwohl et al., 2020) evaluate stochastic classifiers of the form f (T (x))
∕∖ ∕∖
using either F1(x) or FHadv (x) where Hadv is the number of EOT attack samples, typically around 10
∕∖
to 30. This evaluation is not sound when FH (x) has a small but plausible chance of misclassification
because randomness alone could cause x to be identified as an adversarial image even though FH (x)
gives the correct prediction on average (see Figure 3). In experiments with EBM defense, we identify
many images x that exhibit variation in the predicted label
^h (x) = arg max FH (x)j	(6)
j
4
Published as a conference paper at ICLR 2021
for smaller H ≈ 10 but which have consistently correct prediction for larger H ≈ 150. Fair
evaluation of stochastic defenses must be based on the deterministic EOT defense F in (4) and
attackers must use sufficiently large H to ensure that FH (x) accurately approximates F (x) before
declaring that an attack using adversarial image x is successful. In practice, we observe that F150 is
sufficiently stable to evaluate F over several hundred attacks with Algorithm 1 for our EBM defense.
3.2	Attacking Stochastic Classifiers
Let L(F (x), y) ∈ R be the loss (e.g. cross-entropy) between a label y ∈ {1, . . . , J} the outputs
F(x) ∈ RJ of a classifier (e.g. the logits) for an image x ∈ RD. For a given pair of observed data
(x+, y), an untargeted white-box adversarial attack searches for the state
xadv(x+,y) = argmaxL(F(x),y)	(7)
x∈S
that maximizes loss for predicting y in a set S ⊂ RD centered around x+ . In this work, a natural
image x+ will have pixels intensities from 0 to 1 (i.e. x+ ∈ [0, 1]D). One choice of S is the
intersection of the image hypercube [0, 1]D and the l∞-norm ε-ball around x+ for suitably small
ε > 0. Another option is the intersection of the hypercube [0, 1]D with the l2-norm ε-ball.
The Projected Gradient Descent (PGD) attack (Madry et al., 2018) is the standard benchmark when
S is the ε-ball in the lp norm. PGD begins at a random x0 ∈ S and maximizes (7) by iteratively
updating xi with
xi+1 =	(xi + αg(xi, y)),
S
g(x, y) = arg max v>∆(x, y),
kvkp≤1
(8)
where ΠS denotes projection onto S, ∆(x, y) is the attack gradient, and α > 0 is the attack step size.
Standard PGD uses the gradient Δpgd(x, y) = PxL(F(χ),y).
The EOT attack (Athalye et al., 2018) circumvents the intractability of F by attacking finite sample
logits FHadv, where HadV is the number ofEOT attack samples, with the gradient
δEOT(X,y) = VxL(FHadV (x),y).	⑼
The EOT attack is the natural adaptiVe attack for our EOT defense formulation. Another challenge
when attacking a preprocessing defense is the computational infeasibility or theoretical impossibil-
ity of differentiating T (x). The Backward Pass Differentiable Approximation (BPDA) technique
(Athalye et al., 2018) uses an easily differentiable function g(x) such that g(x) ≈ T(x) to attack
F(x) = f (T (x)). One calculates the attack loss using L(f (T (x)), y) on the forward pass but calcu-
lates the attack gradient using VxL(f (g(x)), y) on the backward pass. A simple but effectiVe form
of BPDA is the identity approximation g(x) = x. This approximation is reasonable for preprocessing
defenses that seek to remoVe adVersarial signals while preserVing the main features of the original
image. When g(x) = x, the BPDA attack gradient is ∆BPDA(x, y) = Vz L(f (z), y) where z = T (x).
IntuitiVely, this attack obtains an attack gradient with respect to the purified image and applies it to
the original image.
Combining the EOT attack and BPDA attack with identity g(x) = x giVes the attack gradient
1 HadV	1	HadV
δbPDA+EOT(X, y)=以 X VxhL(HdvX f (xh),y}	xh 〜T(X) i∙id	(⑼
The BPDA+EOT attack represents the strongest known attack against preprocessing defenses, as
shown by its effectiVe use in recent works (Tramer et al., 2020). We use ∆BPDA+EOT(X, y) in (8) as
our primary attack to eValuate the EOT defense (4). Pseudo-code for our adaptiVe attack can be found
in Algorithm 1 in Appendix A.
4	Experiments
We use two different network architectures in our experiments. The first network is the lightweight
EBM from Nijkamp et al. (2019). The second network is a 28 × 10 Wide ResNet classifier (Zagoruyko
5
Published as a conference paper at ICLR 2021
& Komodakis, 2016). The EBM and classifier are trained independently on the same dataset. No
adversarial training or other training modifications are used for either model. The EBM defense
in our work is given by Equation (4) and approximated by Equation (5), where T (x) is K steps of
the Langevin iterations in Equation (3).We use the parameters from Algorithm 1 for all evaluations
unless otherwise noted. In Section 4.1, we examine the effect of the number of Langevin steps K
and the stability of Langevin sampling paths on defense. Section 4.2 examines our defense against
a PGD attack from the base classifier, and Section 4.3 examines our defense against the adaptive
BPDA+EOT attack.
4.1	Comparison of Convergent and Non-Convergent Defenses
Convergent EBM Defense Accuracy vs.
Langevin Steps (Cifar-10,100 norm, ε = 8/255)
Non-Convergent EBM Defense Accuracy vs.
Langevin Steps (Cifar-10, ∕β0 norm, ε = 8/255)
1.0H-----------------:-------：-------------------
ʌ	----- natural accuracy
0 8	⅛	I00 robustness
>uro^⊃uuro
0.0
0	1000	2000	3000	4000	5000
Langevin steps K
>uro^⊃uuro
K=0
K=250
K = 1500
K = 5000
K= 100k
QOl d____________________________________________
0	200	400	600	800	1000
Langevin steps K
K=0
K=20
K=20
K= 50
K = 250
利与K∙WK,∙R, 0目曲国户M寸过
Figure 4: Accuracy on natural images and adversarial images from a BPDA+EOT attack (10) for
EBM defense with different number of Langevin steps, and images sampled from the EBM. Left:
Defense with a convergent EBM. Using approximately 1500 Langevin steps yields a good balance
of natural and robust accuracy. Right: Defense with non-convergent EBM. Oversaturated long-run
images prevent non-convergent EBM defense from achieving high natural or robust accuracy.
We examine the effect the number of Langevin steps has on defense accuracy for the CIFAR-10
dataset (see Figure 4). Each point displays either the baseline accuracy of our stochastic classifier
(blue) or the results of a BPDA+EOT attack (orange) on 1000 test images. The attacks used to
make this diagram use a reduced load of Hadv = 7 replicates for EOT attacks so these defense
accuracies are slightly higher than the full attack results presented in Figure 5. Short-run Langevin
with K ≤ 100 steps yields almost no adversarial robustness. Increasing the number of steps gradually
increases robustness until the defense saturates at around K = 2000. We chose K = 1500 steps in
our experiments as a good tradeoff between robustness, natural accuracy, and computational cost.
For comparison, we run the same experiment using a non-convergent EBM. The network structure and
training are identical to our convergent model, except that we use Adam instead of SGD throughout
training. The non-convergent EBM defense cannot achieve high natural accuracy with long-run
sampling because of the oversaturated features that emerge. Without a high natural accuracy, it is
impossible to obtain good defense results. Thus convergent EBMs that can produce realistic long-run
samples are a key ingredient for the success of our method. In the context of EBM defense, short-run
sampling refers to trajectories of up to about 100 steps where no defensive properties emerge, and
long-run sampling refers to trajectories of about 1000 or more steps where defense becomes possible.
4.2	PGD Attack from Base Classifier for CIFAR- 1 0 Dataset
We first evaluate our defense using adversarial images created from a PGD attack on the classifier
f (x). Since this attack does not incorporate the Langevin sampling from T (x), the adversarial images
in this section are relatively easy to secure with Langevin transformations. This attack serves as a
benchmark for comparing our defense to the IGEBM (Du & Mordatch, 2019) and JEM (Grathwohl
6
Published as a conference paper at ICLR 2021
et al., 2020) models that also evaluate adversarial defense with a ConvNet EBM (1). For all methods,
we evaluate the base classifier and the EBM defense for K = 10 Langevin steps (as in prior defenses)
and K = 1500 steps (as in our defense). The results are displayed in Table 1.
Table 1: CIFAR-10 accuracy for our EBM defense and prior EBM defenses against a PGD attack
from the base classifier f(x) with l∞ perturbation ε = 8/255. (*evaluated on 1000 images)
	Base Classifier f (x)		EBM Defense,		EBM Defense,	
			K =	10	K =	1500
	Nat.	Adv.	Nat.	Adv.	Nat.	Adv.
Ours	0.9530	0.0000	0.9586	0.0001	0.8412	0.7891
(Du & Mordatch, 2019)	0.4714	0.3219	0.4885	0.3674	0.487*	0.375*
(Grathwohl et al., 2020)	0.9282	0.0929	0.9093	0.1255	0.755*	0.238*
Our natural classifier f (x) has a high base accuracy but no robustness. The JEM base classifier
has high natural accuracy and minor robustness, while the IGEBM base classifier has significant
robustness but very low natural accuracy. Short-run sampling with K = 10 Langevin steps does not
significantly increase robustness for any model. Long-run sampling with K = 1500 steps provides a
dramatic increase in defense for our method but only minor increases for the prior methods. Further
discussion of the IGEBM and JEM defenses can be found in Appendix G.
4.3	BPDA+EOT Attack
In this section, we evaluate our EBM defense using the adaptive BPDA+EOT attack (10) designed
specifically for our defense approach. This attack is recently used by Tramer et al. (2020) to evaluate
the preprocessing defense from Yang et al. (2019) that is similar to our method.
Table 2: Defense vs. whitebox attacks with l∞ perturbation ε = 8/255 for CIFAR-10.
Defense	f (x) Train Ims.	T(x) Method	Attack	Nat.	Adv.
Ours	Natural	Langevin	BPDA+EOT	0.8412	0.5490
(Madry et al., 2018)	Adversarial	—	PGD	0.873	0.458
(Zhang et al., 2019)	Adversarial	—	PGD	0.849	0.5643
(Carmon et al., 2019)	Adversarial	—	PGD	0.897	0.625
(Song et al., 2018)	Natural	Gibbs Update	BPDA	0.95	0.09
(Srinivasan et al., 2019)	Natural	Langevin	PGD	—	0.0048
(Yang et al., 2019)	Transformed	Mask + Recon.	BPDA+EOT	0.94	0.15
CIFAR-10. We ran 5 random restarts of the BPDA+EOT attack in Algorithm 1 with the the listed
parameters on the entire CIFAR-10 test set. In particular, the attacks use adversarial perturbation
ε = 8/255 and attack step size α = 2/255 in the l∞ norm. One evaluation of the entire test set took
approximately 2.5 days using 4x RTX 2070 Super GPUs. We compare our results to a representative
selection of AT and AP defenses in Table 2. We include the training method for the classifier,
preprocessing transformation (if any), and the strongest attack for each defense.
Our EBM defense is stronger than standard AT (Madry et al., 2018) and comparable to modified AT
from Zhang et al. (2019). Although our results are not on par with state-of-the-art AT (Carmon et al.,
2019), our defense is the first method that can effectively secure naturally-trained classifiers.
We now examine the effect of the perturbation ε, number of attacks N, and number of EOT attack
replicates Hadv on the strength of the BPDA+EOT attack. To reduce the computational cost of the
diagnostics, we use a fixed set of 1000 randomly selected test images for diagnostic attacks.
Figure 5 displays the robustness of our model compared against two AT models, standard AT (Madry
et al., 2018) and Semi-supervised AT (Carmon et al., 2019) for l∞ and l2 attacks across perturbation
size ε. Our model is attacked with BPDA+EOT while AT models are attacked with PGD. Our defense
is more robust than standard AT for a range of medium-size perturbations in the l∞ norm and much
7
Published as a conference paper at ICLR 2021
Cifar-IO l0θ Robustness across ε
▲ Ours (baseline)
• Ours (50 attacks)
Madry et a∣.
• Carmon et al.
6-4-2-0.
Oooo
Aejne
0.00	0.02	0.04	0.06	0.08	0.10	0.12
perturbation size ε
Cifar-IO ⅛ Robustness across ε
0.8
Auejnuue
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
perturbation size ε
Figure 5: Accuracy across perturbation ε for l∞ and l2 attacks against our defense, standard AT
(Madry et al., 2018) and Semi-supervised AT (Carmon et al., 2019).
Defense over 1000 Whitebox Attacks
Effect of EOT replicates
(Cifar-IO,晨 norm, £ = 8/255)
0.65-	∖	一 H 前V
----Hdef
IO0	ιo1	ιo2
EOT replicates H
Figure 6: Effect of number of attack steps N and number of EOT replicates Hadv and Hdef.
more robust than standard AT for medium and large ε in the l2 norm. Our model is less robust than
Semi-supervised AT for the l∞ norm but more robust for the l2 norm. AT-based models are trained to
defend against a specific attack while our method does not use any attacks during training. This is
likely a reason why our defense outperforms l∞-trained AT against l2 attacks.
Figure 6 visualizes the effect of increasing the computational power of the attacker and defender in
Algorithm 1. The left figure compares our defense with AT over 1000 attacks using an increased
number Hadv = 20 of attack replicates. The majority of breaks happen within the first 50 attacks as
used in the CIFAR-10 experiment in Table 2, while a small number of breaks occur within a few
hundred attack steps. It is likely that some breaks from long-run attacks are the result of lingering
stochastic behavior from FHdef (x) rather than the attack itself. The right figure shows the effect of the
number of EOT replicates. The strength of the EOT attack saturates when using 20 to 30 replicates.
A small gap in attack strength remains between the 15 replicates used in our attacks and the strongest
possible attack. Some of this effect is likely mitigated by our use of 5 random restarts. Defense with
Hdef = 150 is close to the maximum achievable defense for our method, although more replicates
would slightly strengthen our results.
The evaluation in Table 2 pushes the limits of what is computationally feasible with widely available
resources. The diagnostics indicate that our defense report is an accurate approximation of the defense
of our ideal classifier F(x) in (4) against the BPDA+EOT attack (10), although more computation
would yield a slightly more accurate estimate.
SVHN and CIFAR-100. The attack and defense parameters for our method are identical to those
used in the CIFAR-10 experiments. We compare our results with standard AT. Overall, our defense
performs well for datasets that are both simpler and more complex than CIFAR-10. In future work,
further stabilization of image appearance after Langevin sampling could yield significant benefits
for settings where precise details need to be preserved for accurate classification. The AT results for
CIFAR-100 are from Balaji et al. (2019) and the results for SVHN are from our implementation.
8
Published as a conference paper at ICLR 2021
Table 3: Defense vs. whitebox attacks with l∞ perturbation ε = 8/255 for SVHN and CIFAR-100.
	SVHN		CIFAR-100	
	Nat.	Adv.	Nat.	Adv.
Ours	0.9223	0.6755	0.5166	0.2610
(Madry et al., 2018)	0.8957	0.5039	0.5958	0.2547
5	Related Work
Adversarial training learns a robust classifier using adversarial images created during each weight
update. The method is introduced by Madry et al. (2018). Many variations of AT have been explored,
some of which are related to our defense. He et al. (2019) apply noise injection to each network
layer to increase robustness via stochastic effects. Similarly, Langevin updates with our EBM can
be interpreted as a ResNet (He et al., 2016) with noise injected layers as discussed by Nijkamp
et al. (2019). Semi-supervised AT methods (Alayrac et al., 2019; Carmon et al., 2019) use unlabeled
images to improve robustness. Our EBM also leverages unlabeled data for defense.
Adversarial preprocessing is a strategy where auxiliary transformations are applied to adversarial
inputs before they are given to the classifier. Prior forms of pre-processing defenses include rescaling
(Xie et al., 2018a), thermometer encoding (Buckman et al., 2018), feature squeezing (Xu et al.,
2018), activation pruning (Dhillon et al., 2018), reconstruction (Samangouei et al., 2018), ensemble
transformations (Raff et al., 2019), addition of Gaussian noise (Cohen et al., 2019), and reconstruction
of masked images (Yang et al., 2019). Prior preprocessing methods also include energy-based models
such as Pixel-Defend (Song et al., 2018) and MALADE (Srinivasan et al., 2019) that draw samples
from a density that differs from (1). It was shown by Athalye et al. (2018); Tramer et al. (2020) that
many preprocessing defenses can be totally broken or dramatically weakened by simple adjustments
to the standard PGD attack, namely the EOT and BPDA techniques. Furthermore, many preprocessing
defenses such as Cohen et al. (2019); Raff et al. (2019); Yang et al. (2019) also modify classifier
training so that the resulting defenses are analogous to AT, as discussed in Appendix F. A concurrent
work called Denoised Smoothing (Salman et al., 2020) prepends a Denoising Autoencoder to a natural
image classifier. The joint classifier fixes the natural classifier weights and the denoiser is trained
by minimizing the cross-entropy loss of the joint model using noisy training inputs. Our defensive
transformation provides security without information from any classifier. No prior preprocessing
defense is competitive with AT when applied to independent naturally-trained classifiers.
Energy-based models are a probabilistic method for unsupervised modeling. Early energy-based
image models include the FRAME (Zhu et al., 1998) and RBM (Hinton, 2002). The EBM is a
descriptive model (Guo et al., 2003; Zhu, 2003). Our EBM (1) is introduced as the DeepFRAME
model by Xie et al. (2016) and important observations about the learning process are presented by
Nijkamp et al. (2020; 2019). Other studies of EBM learning include Gao et al. (2018); Xie et al.
(2018b); Gao et al. (2020). Preliminary investigations for using the EBM (1) for adversarial defense
are presented by Du & Mordatch (2019); Grathwohl et al. (2020) but the results are not competitive
with AT (see Section 4.2). Our experiments show that the instability of sampling paths for these
non-convergent models prevents Langevin trajectories from manifesting defensive properties. We
build on the convergent learning methodology from Nijkamp et al. (2020) to apply long-run Langevin
sampling as a defense technique.
6	Conclusion
This work demonstrates that Langevin sampling with a convergent EBM is an effective defense for
naturally-trained classifiers. Our defense is founded on an improvement to EBM training that enables
efficient learning of stable long-run sampling for complex datasets. We evaluate our defense using
non-adaptive and adaptive whitebox attacks for the CIFAR-10, CIFAR-100, and SVHN datasets.
Our defense is competitive with adversarial training. Securing naturally-trained classifiers with
post-training defense is a long-standing open problem that this work resolves.
9
Published as a conference paper at ICLR 2021
References
Jean-Baptiste Alayrac, Jonathan Uesato, Po-Sen Huang, Alhussein Fawzi, Robert Stanforth, and
Pushmeet Kohli. Are labels required for improving adversarial robustness? In Advances in Neural
Information Processing Systems, volume 32, 2019.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. In Proceedings of the 35th International
Conference on Machine Learning, pp. 274-283, 2018.
Yogesh Balaji, Tom Goldstein, and Judy Hoffman. Instance adaptive adversarial training: Improved
accuracy tradeoffs in neural nets. arXiv preprint arXiv:1910:08051, 2019.
Giancarlo Benettin, Luigi Galgani, and Jean-Marie Streclyn. Kolmogorov entropy and numerical
experiments. Physical Review A, 14(6):2338-2345, 1976.
Anton Bovier and Frank den Hollander. Metastability: A potential theoretic approach. International
Congress of Mathematicians, 3:499-518, 2006.
Jacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow. Thermometer encoding: One hot way
to resist adversarial examples. In International Conference on Learning Representations, 2018.
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Unlabeled
data improves adversarial robustness. In Advances in Neural Information Processing Systems,
volume 32, 2019.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In Proceedings of the 36th International Conference on Machine Learning, pp.
1310-1320, 2019.
Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble
of diverse parameter-free attacks. In Proceedings of the 37th International Conference on Machine
Learning, 2020.
Guneet S. Dhillon, Kamyar Azizzadenesheli, Jeremy D. Bernstein, Jean Kossaifi, Aran Khanna,
Zachary C. Lipton, and Animashree Anandkumar. Stochastic activation pruning for robust adver-
sarial defense. In International Conference on Learning Representations, 2018.
Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. In
Advances in Neural Information Processing Systems, volume 32, 2019.
Ruiqi Gao, Yang Lu, Junpei Zhou, Song-Chun Zhu, and Ying Nian Wu. Learning generative convnets
via multi-grid modeling and sampling. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 9155-9164, 2018.
Ruiqi Gao, Erik Nijkamp, Diederik P Kingma, Zhen Xu, Andrew M Dai, and Ying Nian Wu.
Flow contrastive estimation of energy-based models. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 7518-7528, 2020.
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2015.
Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi,
and Kevin Swersky. Your classifier is secretly an energy based model and you should treat it like
one. In International Conference on Learning Representations, 2020.
Cheng-En Guo, Song-Chun Zhu, and Ying Nian Wu. Modeling visual patterns by integrating
descriptive and generative methods. International Journal of Computer Vision, 53:5-29, 2003.
Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens van der Maaten. Countering adversarial
images using input transformations. In International Conference on Learning Representations,
2018.
10
Published as a conference paper at ICLR 2021
Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
In Proceedings ofthe IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778,
2016.
Zhezhi He, Adnan Siraj Rakin, and Deliang Fan. Parametric noise injection: Trainable randomness
to improve deep neural network robustness against adversarial attack. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 588-597, 2019.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural
Information Processing Systems, volume 33, pp. 6629-6640, 2017.
Geoffrey E. Hinton. Training products of experts by minimizing contrastive divergence. Neural
Computation, 14(8):1771-1800, 2002.
Nitish Shirish Keskar and Richard Socher. Improving generalization by switching from adam to sgd.
arXiv preprint arXiv:1712.07628, 2017.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2015.
Ying-Cheng Lai, Zonghua Liu, Lora Billiings, and Ira B. Schartz. Noise-induced unstable dimension
variability and transition to chaos in random dynamical systems. Physical Review E, 67, 2003.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018.
Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying Nian Wu. Learning non-convergent non-
persistent short-run MCMC toward energy-based model. In Advances in Neural Information
Processing Systems, volume 32, 2019.
Erik Nijkamp, Mitch Hill, Tian Han, Song-Chun Zhu, and Ying Nian Wu. On the anatomy of
MCMC-based maximum likelihood learning of energy-based models. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 34, 2020.
Edward Raff, Jared Sylvester, Steven Forsyth, and Mark McLean. Barrage of random transforms
for adversarially robust defense. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 6528-6537, 2019.
Hadi Salman, Mingjie Sun, Greg Yang, Ashish Kapoor, and J. Zico Kolter. Denoised smoothing: A
provable defense for pretrained classifiers. In Advances in Neural Information Processing Systems,
volume 33, pp. 21945-21957, 2020.
Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-GAN: Protecting classifiers
against adversarial attacks using generative models. In International Conference on Learning
Representations, 2018.
Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman. Pixeldefend: Lever-
aging generative models to understand and defend against adversarial examples. In International
Conference on Learning Representations, 2018.
Vignesh Srinivasan, Arturo Marban, Klaus-Robert Muller, Wojciech Samek, and Shinichi Nakajima.
Defense against adversarial attacks by langevin dynamics. arxiv preprint arXiv:1805.12017, 2019.
Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to
adversarial example defenses. In Advances in Neural Information Processing Systems, volume 33,
pp. 1633-1645, 2020.
Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. Mitigating adversarial effects
through randomization. In International Conference on Learning Representations, 2018a.
Jianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu. A theory of generative convnet. In
Proceedings of the 33rd International Conference on Machine Learning, pp. 2635-2644, 2016.
11
Published as a conference paper at ICLR 2021
Jianwen Xie, Yang Lu, Ruiqi Gao, and Ying Nian Wu. Cooperative learning of energy-based model
and latent variable model via mcmc teaching. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 32, 2018b.
Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep
neural networks. In 25th Annual Network and Distributed System Security Symposium, NDSS,
2018.
Yuzhe Yang, Guo Zhang, Dina Katabi, and Zhi Xu. Me-net: Towards effective adversarial robustness
with matrix estimation. In Proceedings of the 36th International Conference on Machine Learning,
pp. 7025-7034, 2019.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,
2016.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.
Theoretically principled trade-off between robustness and accuracy. In Proceedings of the 36th
International Conference on Machine Learning, pp. 7472-7482, 2019.
Song-Chun Zhu. Statistical modeling and conceptualization of visual patterns. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 25:691-712, 2003.
Song-Chun Zhu, Ying Nian Wu, and David Mumford. Filters, random fields and maximum entropy
(FRAME): towards a unified theory for texture modeling. International Journal of Computer
Vision, 27(2):107-126, 1998.
12
Published as a conference paper at ICLR 2021
A	Attack Algorithm
Algorithm 1 BPDA+EOT adaptive attack to evaluate EOT defense (4)
Require: Natural images {x+m}mM=1, EBM U (x), classifier f (x), Langevin noise τ = 0.01,
Langevin updates K = 1500, number of attacks N = 50, attack step size α =嬴,maximum
perturbation size ε =募,EOT attack samples HadV = 15, EOT defense samples Hdef = 150
Ensure: Defense record {dm}mM=1 for each image.
for m=1:M do
Calculate large-sample predicted label of the natural image CHdef (Xm) With (6).
if 2Hdef (Xm) = ym then
Natural image misclassified. dm J False. End loop iteration m.
else
dm J True.
end if
Randomly initialize X0 in the lp ε-ball centered at X+m and project to [0, 1]D.
for n=1:(N+1) do
Calculate small-sample predicted label CHadV (Xn-I) with (6).
Calculated attack gradient ∆BPDA+EOT(Xn-1, ym) With (10).
if CHadV (Xn-I) = ym then
Calculate large-sample predicted label CHdef (Xn-ι) with (6).
if CHdef (Xn-I) = ym then
The attack has succeeded. dm J False. End loop iteration m.
end if
end if
Use ∆BPDA+EOT(Xn-1, ym) with the lp ε-bounded PGD update (8) to obtain Xn.
end for
end for
Algorithm 1 is pseudo-code for the attack and defense framework described in Section 3. One notable
aspect of the algorithm is the inclusion of an EOT defense phase to verify potentially successful
attacks. Images which are identified as broken for the smaller sample used to generate EOT attack
gradients are checked again using a much larger EOT defense sample to ensure that the break is
due to the adversarial state and not random finite-sample effects. This division is done for purely
computational reasons. It is extremely expensive to use 150 EOT attack replicates but much less
expensive to use 15 EOT attack replicates as a screening method and to carefully check candidates
for breaks when they are identified from time to time using 150 EOT defense replicates. In our
experiments we find that the EOT attack is close to its maximum strength after about 15 to 20
replicates are used, while about 150 EOT defense replicates are needed for consistent evaluation of F
from (4) over several hundred attacks. Ideally, the same number of chains should be used for both
EOT attack and defense, in which case the separate verification phase would not be necessary.
B Improved Learning of Convergent EBMs
Algorithm 2 provides pseudo-code for our improvement of the convergent learning method from
Nijkamp et al. (2020). This implementation allows efficient learning of convergent EBMs for complex
datasets.
C Erasing Adversarial Signals with MCMC Sampling
This section discusses two theoretical perspectives that justify the use of an EBM for purifying
adversarial signals: memoryless and chaotic behaviors from sampling dynamics. We emphasize that
the discussion applies primarily to long-run behavior of a Langevin image trajectory. Memoryless and
chaotic properties do not appear to emerge from short-run sampling. Throughout our experiments,
we never observe significant defense benefits from short-run Langevin sampling.
13
Published as a conference paper at ICLR 2021
Algorithm 2 ML with Adam to SGD Switch for Convergent Learning of EBM (1)
Require: ConvNet potential U (x; θ), number of training steps J = 150000, step to switch from
SGD to Adam JSGD = 50000, initial weight θ1, training images {xi+}iN=da1ta, data perturbation
τdata = 0.02, step size τ = 0.01, Langevin steps K = 100, Adam learning rate γAdam = 0.0001,
SGD learning rate γSGD = 0.00005.
Ensure: Weights θJ+1 for energy U(x; θ).
Set optimizer g — Adam(YAdam). Initialize persistent image bank as Ndata uniform noise images.
for j=1:(J+1) do
if j = JSGD then
Set optimizer g J SGD(YSgd).
end if
1.	Draw batch images {x(+i)}im=1 from training set, where (i) indicates a randomly selected
index for sample i, and get samples Xi = x(i)+ TdataZi, where Zi 〜N(0,Id) i.i.d.
2.	Draw initial negative samples {Yi(0)}im=1 from persistent image bank. Update
{Yi(0)}im=1
with the Langevin equation
Yikk= YfT)- τ22 ∂yU (Yi(k-1); θj)+ τZi,k,
where Zi,k 〜N(0,Id) i.i.d., for K steps to obtain samples {X-}m=ι = {Yi(K)}m=ι.
Update persistent image bank with images {Yi(K)}im=1.
3.	Update the weights by θj+1 = θj - g(∆θj), where g is the optimizer and
∂	1n	1m
△j ∂⅛ X U (Xi+; θj)- m X U(X-； θj)
i=1	i=1
is the ML gradient approximation.
end for
Energy of Natural, Adversarial, and Noise Images
Figure 7: Energy U(x; θ) of natural, adversarial, and noise images.
C.1 Memoryless Dynamics
The first justification of EBM defense is that iterative probabilistic updates will move an image
from a low-probability adversarial region to a high-probability natural region, as discussed in prior
works (Song et al., 2018; Srinivasan et al., 2019; Grathwohl et al., 2020). Comparing the energy of
adversarial and natural images shows that adversarial images tend to have marginally higher energy,
14
Published as a conference paper at ICLR 2021
which is evidence that adversarial images are improbable deviations from the steady-state manifold
of the EBM that models the distribution of natural images (see Figure 7).
The theoretical foundation for removing adversarial signals with MCMC sampling comes from
the well-known steady-state convergence property of Markov chains. The Langevin update (3) is
designed to converge to the distribution p(x; θ) learned from unlabeled data after an infinite number of
Langevin steps. The steady-state property guarantees that adversarial signals will be erased from the
sampled state as long as enough steps are used because the sampled state will have no dependence on
the initial state. This property is actually too extreme because full MCMC mixing would completely
undermine classification by causing samples to jump between class modes.
The quasi-equilibrium and metastable behaviors of MCMC sampling (Bovier & den Hollander, 2006)
can be as useful as its equilibrium properties. Metastable behavior refers to intramodal mixing that
can occur for MCMC trajectories over intermediate time-scales, in contrast to the limiting behavior
of full intermodal mixing that occurs for trajectories over arbitrarily large time-scales. Although
slow-mixing and high autocorrelation of MCMC chains are often viewed as major shortcomings,
these properties enable EBM defense by preserving class-specific features while sampling erases
adversarial signals.
Our EBM samples always exhibit some dependence on the initial state for computationally feasible
Langevin trajectories. Mixing within a metastable region can greatly reduce the influence of an
initial adversarial signal even when full mixing is not occurring. Successful classification of long-run
MCMC samples occurs when the metastable regions of the EBM p(x; θ) are aligned with the class
boundaries learned by the classifier network f(x). Our experiments show that this alignment naturally
occurs for convergent EBMs and naturally-trained classifiers. No training modification for f (x) is
needed to correctly classify long-run EBM samples. Our defense relies on a balance between the
memoryless properties of MCMC sampling that erase noise and the metastable properties of MCMC
sampling that preserve the initial state.
C.2 Chaotic Dynamics
Chaos theory provides another perspective for justifying the erasure of adversarial signals with long-
run iterative transformations. Intuitively, a deterministic system is chaotic if an initial infinitesimal
perturbation grows exponentially in time so that paths of nearby points become distant as the system
evolves (i.e. the butterfly effect). The same concept can be extended to stochastic systems. The SDE
dX
dt
V(X)+ηξ(t),
(11)
where ξ(t) is Brownian motion and η ≥ 0, that encompasses the Langevin equation is known to
exhibit chaotic behavior in many contexts for sufficiently large η (Lai et al., 2003).
One can determine whether a dynamical system is chaotic or ordered by measuring the maximal
Lyapunov exponent λ given by
λ
lim - log
t→∞ t
∣δXη (t)|
∣δXη (0)|
(12)
where δXη(t) is an infinitesimal perturbation between system state at time t after evolution according
to (11) from an initial perturbation δXη(0). For ergodic dynamics, λ does not depend on the initial
perturbation δXη(0). Ordered systems have a maximal Lyapunov exponent that is either negative
or 0, while chaotic systems have positive Lyapunov exponents. The SDE (11) will have a maximal
exponent of at least 0 since dynamics in the direction of gradient flow are neither expanding nor
contracting. One can therefore detect whether a Langevin equation yields ordered or chaotic dynamics
by examining whether its corresponding maximal Lyapunov exponent is 0 or positive.
We use the classical method of Benettin et al. (1976) to calculate the maximal Lyapunov exponent of
the altered form Langevin transformation (3) given by
Tn (X) = X — τ22 VXU(X; θ) + ητZk	(13)
for a variety of noise strengths η. Our results exhibit the predicted transition from noise to chaos. The
value η = 1 which corresponds to our training and defense algorithms is just beyond the transition
15
Published as a conference paper at ICLR 2021
Figure 8: Left: Maximal Lyapunov exponent for different values of η. The value η = 1 which
corresponds to our training and defense sampling dynamics is just above the transition from the
ordered region where the maximal exponent is 0to the chaotic region that where the maximal exponent
is positive. Right: Appearance of steady-state samples for different values of η . Oversaturated images
appear for low values of η, while noisy images appear for high η . Realistic synthesis is achieved in a
small window around η = 1 where gradient and noise forces are evenly balanced.
from the ordered region to the chaotic region. Our defense dynamics occur in a critical interval where
ordered gradient forces that promote pattern formation and chaotic noise forces that disrupt pattern
formation are balanced. Oversaturation occurs when the gradient dominates and noisy images occur
when the noise dominates. The results are shown in Figure 8.
The unpredictability of paths under Tη is an effective defense against BPDA because informative
attack gradients cannot be generated through chaotic transformation. Changes in adversarial perturba-
tion from one BPDA attack to the next attack are magnified by the Langevin transformation and it
becomes difficult to climb the loss landscape L(F (x), y) to create adversarial samples. Other chaotic
transformations, either stochastic or deterministic, might be an interesting line of research as a class
of defense methods.
D	Effect of Number of Langevin Steps on FID Score
We examine the difference between convergent and non-convergent EBM sampling paths by mea-
suring the FID score (Heusel et al., 2017) of samples initialized from the data (see Figure 9). The
convergent EBM maintains a reasonably low FID score across many steps so that long-run samples
can be accurately classified. The non-convergent EBM experiences a quick increase in FID as
oversaturated samples appear. Labels for the oversaturated samples cannot be accurately predicted by
a naturally-trained classifier, preventing successful defense.
E Discussion of Defense Runtime
Robustness does not depend on the computational resources of the attacker or defender (Athalye
et al., 2018). Nonetheless, we took efforts to reduce the computational cost of our defense to make
it as practical as possible. Our defense requires less computation for classifier training but more
computation for evaluation compared to AT due to the use of 1500 Langevin steps as a preprocessing
procedure. Running 1500 Langevin steps on a batch of 100 images with our lightweight EBM takes
about 13 seconds on a RTX 2070 Super GPU. While this is somewhat costly, it is still possible
to evaluate our model on large test sets in a reasonable amount of time. The cost of our defense
also poses a computational obstacle to attackers, since the BPDA+EOT attack involves iterative
application of the defense dynamics. Thoroughly evaluating our defense on CIFAR-10 took about
2.5 days for the entire testing set with 4 RTX 2070 Super GPUs.
Our EBM is significantly smaller and faster than previous EBMs used for adversarial defense. Our
EBM has less than 700K parameters and sampling is about 20× to 30× faster than IGEBM and JEM,
as shown in Appendix G. Generating adversarial images using PGD against a large base classifier
is also expensive (about 30× slower for a PGD step compared to a Langevin step because our base
16
Published as a conference paper at ICLR 2021
FID Score vs. Langevm Steps for
Data-Initialized Samples (CIFAR-10 dataset)
us4j① q-JBMO-)①」OUS 0-π-
Figure 9: FID scores of samples from a convergent and non-convergent EBM that are initialized from
training data. The FID of the convergent model remains reasonably low because the steady-state of the
convergent model is aligned with the data distribution. The FID of the non-convergent model quickly
increases because the oversaturated steady-state of the non-convergent model differs significantly
from the data distribution. Maintaining a sampling distribution close to the data distribution is
essential for achieving high prediction accuracy of a natural classifier on transformed states.
classifier uses the same Wide ResNet backbone as the JEM model). Therefore 50 PGD steps against
the base classifier takes about the same time as 1500 Langevin steps with our EBM, so our model
can defend images at approximately the same rate as an attacker who generates adversarial samples
against the base classifier. Generating adversarial examples using BPDA+EOT is much slower
because defense dynamics must be incorporated. Further reducing the cost of our defense is an
important direction for future work.
Our approach is amenable toward massive parallelization. One strategy is to distribute GPU batches
temporally and run our algorithm in high FPS with a time delay. Another strategy is to distribute the
workload of computing FH across parallel GPU batches.
F A Note on Modified Classifier Training for Preprocessing
Defenses
Many works that report significant robustness via defensive transformations (Cohen et al. (2019);
Raff et al. (2019); Yang et al. (2019) and others) also modify classifier learning by training with
transformed images rather than natural images. Prior defensive transformations that are strong enough
to remove adversarial signals have the side effect of greatly reducing the accuracy of a naturally-
trained classifier. Therefore, signals introduced by these defensive transformations (high Gaussian
noise, ensemble transformations, ME reconstruction, etc.) can be also considered "adversarial"
signals (albeit perceptible ones) because they heavily degrade natural classifier accuracy much like
the attack signals. From this perspective, modifying training to classify transformed images is a direct
analog of AT where the classifier is trained to be robust to "adversarial" signals from the defensive
transformation itself rather than a PGD attack. Across many adversarial defense works, identifying
a defensive transformation that removes attack signals while preserving high accuracy of a natural
classifier has universally proven elusive.
Our experiments train the classifier with natural images alone and not with images generated from our
defensive transformation (Langevin sampling). Our approach represents the first successful defense
17
Published as a conference paper at ICLR 2021
based purely on transformation and validates an entirely different approach compared to defenses
which modify training of the base classifier (Madry et al. (2018); Cohen et al. (2019); Raff et al.
(2019); Yang et al. (2019) etc.). To our knowledge, showing that natural classifiers can be secured
with post-training defensive transformation is a contribution that is unique in the literature. Our
task independent approach has the potential of securing images for many applications using a single
defense model, while AT and relatives must learn a robust model for each application.
G Discussion of IGEBM and JEM Defenses
We hypothesize that the non-convergent behavior of the IGEBM (Du & Mordatch, 2019) and JEM
(Grathwohl et al., 2020) models limits their use as an EBM defense method. Long-run samples from
both models have oversaturated and unrealistic appearance (see Figure 10). Non-convergent learning
behavior is a consequence of training implementation rather than model formulation. Convergent
learning may be a path to robustness using the IGEBM and JEM defense methods.
IGEBM
JEM
Running Time vs.
Number of Langevin Steps
nl2
(SPUOUeS)E= UrU
3 2
O O
1 1
0	2500 5000 7500 10000
number of Lanqevin steps K
Relative Runtime vs. Our EBM
Qoo
3 2 1
E=UΓU>=J
Q-
0	2500 5000 7500 10000
number of Lanqevin steps K
Figure 10: Left: Approximate steady-state samples of the IGEM and JEM models. Both exhibit
oversaturation from non-convergent learning that can interfere with defense capabilities. Right:
Comparison of running time for Langevin sampling with a batch of 100 images. The small scale and
fast sampling of our EBM are important for the computational feasibility of our defense.
Both prior works use very large networks to maximize scores on generative modeling metrics. As a
result, sampling from these models can be up to 30 times slower than sampling from our lightweight
EBM structure from Nijkamp et al. (2019) (see Figure 10). The computational feasibility of our
method currently relies on the the small scale of our EBM. Given the effectiveness of the weaker and
less expensive PGD attack in Section 4.2 and the extreme computational cost of sampling with large
EBM models, wedonotto apply BPDA+EOT to the IGEBM or JEM defense.
The original evaluations of the IGEBM and JEM model use end-to-end backpropagation through
the Langevin dynamics when generating adversarial examples. On the other hand, the relatively
weak attack in Section 4.2 is as strong or much stronger than the theoretically ideal end-to-end
attack. Gradient obfuscation from complex second-order differentiation might hinder the strength of
end-to-end PGD when attacking Langevin defenses.
The IGEBM defense overcomes oversaturation by restricting sampling to a ball around the input
image, but this likely prevents sampling from being able to manifest its defensive properties. An
adversarial signal will be partially preserved by the boundaries of the ball regardless of how many
sampling steps are used. Unrestricted sampling, as performed in our work and the JEM defense, is
essential for removing adversarial signals.
18