Published as a conference paper at ICLR 2021
On the Universality of Rotation Equivariant
Point Cloud Networks
Nadav Dym	Haggai Maron
Duke University	NVIDIA Research
nadavdym@gmail.com	hmaron@nvidia.com
Abstract
Learning functions on point clouds has applications in many fields, including com-
puter vision, computer graphics, physics, and chemistry. Recently, there has been
a growing interest in neural architectures that are invariant or equivariant to all
three shape-preserving transformations of point clouds: translation, rotation, and
permutation. In this paper, we present a first study of the approximation power
of these architectures. We first derive two sufficient conditions for an equivariant
architecture to have the universal approximation property, based on a novel char-
acterization of the space of equivariant polynomials. We then use these conditions
to show that two recently suggested models (Thomas et al., 2018; Fuchs et al.,
2020) are universal, and for devising two other novel universal architectures.
1	Introduction
Designing neural networks that respect data symmetry is a powerful approach for obtain-
ing efficient deep models. Prominent examples being convolutional networks which re-
spect the translational invariance of images, graph neural networks which respect the per-
mutation invariance of graphs (Gilmer et al., 2017; Maron et al., 2019b), networks such
as (Zaheer et al., 2017; Qi et al., 2017a) which respect the permutation invariance of
sets, and networks which respect 3D rotational symmetries (Cohen et al., 2018; Weiler
et al., 2018; Esteves et al., 2018; Worrall & Brostow, 2018; Kondor et al., 2018a).
While the expressive power of equivariant models is re-
duced by design to include only equivariant functions, a
desirable property of equivariant networks is universal-
ity: the ability to approximate any continuous equivariant
function. This is not always the case: while convolutional
networks and networks for sets are universal (Yarotsky,
2018; Segol & Lipman, 2019), popular graph neural net-
works are not (Xu et al., 2019; Morris et al., 2018).
In this paper, we consider the universality of networks
that respect the symmetries of 3D point clouds: translations, rotations, and permutations. Design-
ing such networks is a popular paradigm in recent years (Thomas et al., 2018; Fuchs et al., 2020;
Poulenard et al., 2019; Zhao et al., 2019). While there have been many works on the universality of
permutation invariant networks (Zaheer et al., 2017; Maron et al., 2019c; KeriVen & Peyre, 2019),
and a recent work discussing the universality of rotation equivariant networks (Bogatskiy et al.,
2020), this is a first paper which discusses the universality of networks which combine rotations,
permutations and translations.
We start the paper with a general, architecture-agnostic, discussion, and derive two sufficient con-
ditions for universality. These conditions are a result of a novel characterization of equivariant
polynomials for the symmetry group of interest. We use these conditions in order to prove univer-
sality of the prominent Tensor Field Networks (TFN) architecture (Thomas et al., 2018; Fuchs et al.,
2020). The following is a weakened and simplified statement of Theorem 2 stated later on in the
paper:
Theorem (Simplification of Theorem 2). Any continuous equivariant function on point clouds can
be approximated uniformly on compact sets by a composition of TFN layers.
1
Published as a conference paper at ICLR 2021
We use our general discussion to prove the universality of two additional equivariant models: the
first is a simple modification of the TFN architecture which allows for universality using only low
dimensional filters. The second is a minimal architecture which is based on tensor product represen-
tations, rather than the more commonly used irreducible representations of SO(3). We discuss the
advantages and disadvantages of both approaches.
To summarize, the contributions of this paper are: (1) A general approach for proving the universal-
ity of rotation equivariant models for point clouds; (2) A proof that two recent equivariant models
(Thomas et al., 2018; Fuchs et al., 2020) are universal; (3) Two additional simple and novel universal
architectures.
2	Previous work
Deep learning on point clouds. (Qi et al., 2017a; Zaheer et al., 2017) were the first to apply neural
networks directly to the raw point cloud data, by using pointwise functions and pooling operations.
Many subsequent works used local neighborhood information (Qi et al., 2017b; Wang et al., 2019b;
Atzmon et al., 2018). We refer the reader to a recent survey for more details (Guo et al., 2020).
In contrast with the aforementioned works which focused solely on permutation invariance, more
related to this paper are works that additionally incorporated invariance to rigid motions. (Thomas
et al., 2018) proposed Tensor Field Networks (TFN) and showed their efficacy on physics and chem-
istry tasks.(Kondor et al., 2018b) also suggested an equivariant model for continuous rotations. (Li
et al., 2019) suggested models that are equivariant to discrete subgroups of SO(3). (Poulenard et al.,
2019) suggested an invariant model based on spherical harmonics. (Fuchs et al., 2020) followed TFN
and added an attention mechanism. Recently, (Zhao et al., 2019) proposed a quaternion equivariant
point capsule network that also achieves rotation and translation invariance.
Universal approximation for invariant networks. Understanding the approximation power of in-
variant models is a popular research goal. Most of the current results assume that the symmetry
group is a permutation group. (Zaheer et al., 2017; Qi et al., 2017a; Segol & Lipman, 2019; Maron
et al., 2020; Serviansky et al., 2020) proved universality for several Sn-invariant and equivariant
models. (Maron et al., 2019b;a; Keriven & Peyre, 2019; Maehara & NT, 2019) studied the approx-
imation power of high-order graph neural networks. (Maron et al., 2019c; Ravanbakhsh, 2020) tar-
geted universality of networks that use high-order representations for permutation groups(Yarotsky,
2018) provided several theoretical constructions of universal equivariant neural network models
based on polynomial invariants, including an SE(2) equivariant model. In a recent work (Bogatskiy
et al., 2020) presented a universal approximation theorem for networks that are equivariant to sev-
eral Lie groups including SO(3). The main difference from our paper is that we prove a universality
theorem for a more complex group that besides rotations also includes translations and permutations.
3	A framework for proving universality
In this section, we describe a framework for proving the universality of equivariant networks. We
begin with some mathematical preliminaries:
3.1	Mathematical setup
An action of a group G on a real vector space W is a collection of maps ρ(g) : W → W defined
for any g ∈ G, such that ρ(g1) ◦ ρ(g2) = ρ(g1g2) for all g1, g2 ∈ G, and the identity element of G
is mapped to the identity mapping on W . We say ρ is a representation of G if ρ(g) is a linear map
for every g ∈ G. As is customary, when it does not cause confusion we often say that W itself is a
representation of G .
In this paper, we are interested in functions on point clouds. Point clouds are sets of vectors in R3
arranged as matrices:
X = (x1,...,xn) ∈ R3×n.
Many machine learning tasks on point clouds, such as classification, aim to learn a function which
is invariant to rigid motions and relabeling of the points. Put differently, such functions are required
2
Published as a conference paper at ICLR 2021
to be invariant to the action of G = R3 o SO(3) × Sn on R3×n via
ρG(t,R,P)(X) =R(X-t1Tn)PT,	(1)
where t ∈ R3 defines a translation, R is a rotation and P is a permutation matrix.
Equivariant functions are generalizations of invariant functions: If G acts on W1 via some action
ρ1 (g), and on W2 via some other group action ρ2 (g), we say that a function f : W1 → W2 is
equivariant if
f (ρ1 (g)w) = ρ2(g)f(w),∀w ∈ W1 andg ∈ G.
Invariant functions correspond to the special case where ρ2(g) is the identity mapping for all g ∈ G.
In some machine learning tasks on point clouds, the functions learned are not invariant but rather
equivariant. For example, segmentation tasks assign a discrete label to each point. They are invariant
to translations and rotations but equivariant to permutations - in the sense that permuting the input
causes a corresponding permutation of the output. Another example is predicting a normal for each
point of a point cloud. This task is invariant to translations but equivariant to both rotations and
permutations.
In this paper, we are interested in learning equivariant functions from point clouds into WTn , where
WT is some representation of SO(3). The equivariance of these functions is with respect to the
action ρG on point clouds defined in equation 1, and the action of G on WTn defined by applying the
rotation action from the left and permutation action from the right as in 1, but ‘ignoring’ the trans-
lation component. Thus, G-equivariant functions will be translation invariant. This formulation of
equivariance includes the normals prediction example by taking WT = R3 , as well as the segmen-
tation case by setting WT = R with the trivial identity representation. We focus on the harder case
of functions into WTn which are equivariant to permutations, since it easily implies the easier case
of permutation invariant functions to WT .
Notation. We use the notation N+ = N ∪ {0} and Nj = Ur∈N N]. We set [D] = {1,..., D} and
[D]0 ={0,...,D}.
Proofs. Proofs appear in the appendices, arranged according to sections.
3.2	Conditions for universality
The semi-lifted approach In general, highly expressive equivariant neural networks can be
achieved by using a ‘lifted approach’, where intermediate features in the network belong to high
dimensional representations of the group. In the context of point clouds where typically n 3,
many papers, e.g., (Thomas et al., 2018; Kondor, 2018; Bogatskiy et al., 2020) use a ‘semi-lifted’
approach, where hidden layers hold only higher dimensional representations of SO(3), but not high
order permutation representations. In this subsection, we propose a strategy for achieving universal-
ity with the semi-lifted approach.
We begin by an axiomatic formulation of the semi-lifted approach (see illustration
in inset):	we assume that our neural networks are composed of two main compo-
nents: the first component is a family Ffeat of parametric continuous G-equivariant
functions ffeat which map the original point cloud R3×n to a semi-lifted point cloud
Wfneat = Lin=1 Wfeat, where Wfeat is a lifted (i.e., high-order) representation of SO(3).
The second component is a family of paramet-
ric linear SO(3)-equivariant functions Fpool,
which map from the high order representa-
tion Wfeat down to the target representation
WT. Each such SO(3)-equivariant function A :
Wfeat → WT can be extended to a SO(3) × Sn
equivariant function A : Wnat → WT by ap-
plying A elementwise. For every positive inte-
ger C , these two families of functions induce a
family of functions FC obtained by summing
C different compositions of these functions:
C
FC (Ffeat, Fpool) = {f∣f(X) = X A c(gc(X)), (Ac ,gc) ∈ Fpool X Feat}.	(2)
c=1
3
Published as a conference paper at ICLR 2021
Conditions for universality We now describe two conditions that guarantee universality using the
semi-lifted approach. The first step is showing, as in (Yarotsky, 2018), that continuous G-equivariant
functions CG(R3×n, WTn) can be approximated by G-equivariant polynomials PG(R3×n, WTn).
Lemma 1. Any continuous G-equivariant function in CG(R3×n, WTn) can be approximated uni-
formly on compact sets by G-equivariant polynomials in PG (R3×n , WTn).
Universality is now reduced to the approximation of G-equivariant polynomials. Next, we provide
two conditions which guarantee that G-equivariant polynomials of degree D can be expressed by
function spaces FC (Ffeat , Fpool) as defined in equation 2. The idea behind these conditions is that
explicit characterizations of polynomials equivariant to the joint action of translations, rotations and
permutations is challenging. However, it is possible to explicitly characterize polynomials equiv-
ariant to translations and permutations (but not rotations). The key observation is then that this
characterization can be rewritten as a sum of functions to Wfneat , a high dimensional representa-
tions of SO(3) which is equivariant to translations, permutations and rotations, composed with a
linear map which is permutation equivariant (but does not respect rotations). Accordingly, our first
condition is that Ffeat contains a spanning set of such functions to Wfneat . We call this condition
D-spanning:
Definition 1 (D-spanning). For D ∈ N+, let Ffeat be a subset of CG(R3×n, Wfneat). We say that
Ffeat is D-spanning, if there exist f1, . . . , fK ∈ Ffeat, such that every polynomial p : R3×n → Rn
of degree D which is invariant to translations and equivariant to permutations, can be written as
K
P(X ) = X Λ k (fk (X)),	⑶
k=1
where Λk : Wfeat → R are all linear functionals, and Λ k : Wfnat → Rn are the functions defined
by elementwise applications ofΛk.
In Lemma 4 we explicitly construct a D-spanning family of functions. This provides us with a
concrete condition which implies D-spanning for other function families as well.
The second condition is that Fpool contains all SO(3) linear equivariant layers. We call this con-
dition Linear universality. Intuitively, taking linear rotation equivariant Λk in equation 3 ensures
that the resulting function p will be rotation equivariant and thus fully G-equivariant, and linear
universality guarantees the ability to express all such G invariant functions.
Definition 2 (Linear universality). We say that a collection Fpool of equivariant linear functionals
between two representations Wfeat and WT of SO(3) is linearly universal, if it contains all linear
SO(3)-equivariant mappings between the two representations.
When these two conditions apply, a rather simple symmetrization arguments leads to the following
theorem:
Theorem 1. If Ffeat is D-spanning and Fpool is linearly universal, then there exists some C(D) ∈
N such that for all C ≥ C(D) the function space FC (Ffeat, Fpool) contains all G-equivariant
polynomials of degree ≤ D.
Proof idea. By the D-spanning assumption, there exist f1, . . . , fK ∈ Ffeat such that any vector
valued polynomial invariant to translations and equivariant to permutations is of the form
K
p(X ) = X Λ k (fk (X)),	(4)
k=1
While by definition this holds for functions p whose image is Rn, this is easily extended to functions
to WTn as well.
It remains to show that when p is also SO(3)-equivariant, we can choose Λk to be SO(3) equivariant.
This is accomplished by averaging over SO(3).	□
4
Published as a conference paper at ICLR 2021
As a result of Theorem 1 and Lemma 1 we obtain our
universality result (see inset for illustration)
Corollary 1. For all C, D ∈ N+, let FC,D denote
function spaces generated by a pair of functions spaces
which are D-spanning and linearly universal as in equa-
tion 2. Then any continuous G-equivariant function in
CG(R3×n, WTn) can be approximated uniformly on compact sets by equivariant functions in
F =	FC(D),D .
D∈N
3.3	universality conditions in action
In the remainder of the paper, we prove the universality of several G-equivariant architectures, based
on the framework we discussed in the previous subsection. We discuss two different strategies for
achieving universality, which differ mainly in the type of lifted representations of SO(3) they use:
(i) The first strategy uses (direct sums of) tensor-product representations; (ii) The second uses (direct
sums of) irreducible representations. The main advantage of the first strategy from the perspective of
our methodology is that achieving the D-spanning property is more straightforward. The advantage
of irreducible representations is that they almost automatically guarantees the linear universality
property.
In Section 4 we discuss universality through tensor product representations, and give an example
of a minimal tensor representation network architecture that would satisfy universality. In section 5
we discuss universality through irreducible representations, which is currently the more common
strategy. We show that the TFN architecture (Thomas et al., 2018; Fuchs et al., 2020) which follows
this strategy is universal, and describe a simple tweak that achieves universality using only low order
filters, though the representations throughout the network are high dimensional.
4	Universality with tensor representations
In this section, we prove universality for models that are based on tensor product repre-
sentations, as defined below. The main advantage of this approach is that D-spanning
is achieved rather easily. The main drawbacks are that its data representation is some-
what redundant and that characterizing the linear equivariant layers is more laborious.
Tensor representations We begin by defining tensor rep-
resentations. For k ∈ N+ denote Tk = R3k . SO(3) acts
on Tk by the tensor product representation, i.e., by ap-
plying the matrix Kronecker product k times: ρk (R) :=
Rxk. The inset illustrates the vector spaces and action for
k = 1, 2, 3. With this action, for any i1, . . . , ik ∈ [n], the map from R3×n to Tk defined by
(Xl, . . . , Xn) → Xi1 0 Xi2 ... 0 Xik	(5)
is SO(3) equivariant.
A D-spanning family We now show that tensor representations can be used to define a finite set of
D-spanning functions. The lifted representation Wfeat will be given by
D
WfTeat=MTT.
T=0
The D-spanning functions are indexed by vectors ~r = (r1, . . . , rK), where each rk is a non-negative
integer. Denoting T = krk1, the functions Q(~r) : R3×n → TTn, Q(~r) = (Q(jr) )jn=1 are defined for
fixed j ∈ [n] by
n
Q(j~r)(X) =	X	Xjxr1 0 Xix2r2 0Xix3r30...0XixKrK.	(6)
i2,..	.,iK =1
5
Published as a conference paper at ICLR 2021
The functions Q(j~r) are SO(3) equivariant as they are a sum of equivariant functions from equation 5.
Thus Q(r~) is SO(3) × Sn equivariant. The motivation behind the definition of these functions is that
known characterizations of permutation equivariant polynomials Segol & Lipman (2019) tell us that
the entries of these tensor valued functions span all permutation equivariant polynomials (see the
proof of Lemma 2 for more details).
To account for translation invariance, we compose the functions Q(r~) with the centralization opera-
tion and define the set of functions
Qd = {∣ ◦ Q(~)(X - 1X 1n1T)lk~kι ≤ D},	(7)
nn
where ι is the natural embedding that takes each TT into WfTeat = LTD=0 TT. In the following
lemma, we prove that this set is D-spanning.
Lemma 2. For every D ∈ N+, the set QD is D-spanning.
Proof idea. It is known (Segol & Lipman, 2019) (Theorem 2) that polynomials p : R3×n → Rn
which are Sn-equivariant, are spanned by polynomials of the form pα~ = (pjα~)jn=1, defined as
n
pjα~(X) =	X	xjα1xiα22 ...xiαkk	(8)
i2,..	.,iK =1
where α~ = (α1 , . . . , αK ) and each αk ∈ N3+ is a multi-index. We first show that these polynomials
can be extracted from Q(~) and then use them to represent p.	口
A minimal universal architecture Once we have shown that QD is D-spanning, we can design
D-spanning architectures, by devising architectures that are able to span all elements of QD. As
we will now show, the compositional nature of neural networks allows us to do this in a very clean
manner.
We define a parametric function f(X,V ∣Θ1,Θ2) which maps R3×n ㊉Tjn to R3×n ㊉Tkn+1 as follows：
For all j ∈ [n], We have f (X, V) = (xj, Vj (X, V)), where
V(X,V∣Θ1,Θ2) = θι (Xj 乳 Vj) + θ2 X (Xi 乳 VO	(9)
i
We denote the set of functions (X, V) → f (X, V∣Θ1,Θ2) obtained by choosing the parameters
θ1 , θ2 ∈ R, by Fmin . While in the hidden layers of our network the data is represented using both
coordinates (X, V), the input to the network only contains an X coordinate and the output only
contains a V coordinate. To this end, we define the functions
ext(X) = (X, 1n) andπV(X,V) = V.	(10)
We can achieve D-spanning by composition of functions in Fmin with these functions and central-
izing:
Lemma 3. The function set QD is contained in
Ffeat = {∣ ◦ ∏V ◦ f1 ◦ f2 ◦…◦ fT ◦ ext(X - 1X 1n1T)| fj ∈ Fmin,T ≤ D}.	(11)
nn
Thus Ffeat is D-spanning.
Proof idea. The proof is technical and follows by induction on D.	口
To complete the construction ofa universal network, we now need to characterize all linear equivari-
ant functions from WfTeat to the target representation WT . In Appendix G we show how this can be
done for the trivial representation WT = R. This characterization gives us a set of linear functions
Fpool, which combined with Ffeat defined in equation 11 (corresponds to SO(3) invariant functions)
6
Published as a conference paper at ICLR 2021
gives us a universal architecture as in Theorem 1. However, the disadvantage of this approach is that
implementation of the linear functions in Fpool is somewhat cumbersome.
In the next section we discuss irreducible representations, which give us a systematic way to address
linear equivariant mappings into any WT . Proving D-spanning for these networks is accomplished
via the D-spanning property of tensor representations, through the following lemma
Lemma 4. If all functions in QD can be written as
1K
I◦ Q(~)(X - - XIniT) = EAkfk(X),
nn
k=1
where fk ∈ Ffeat, Ak : Wfeat → WfTat and Ak : Wfnat → (WfTat)n is defined by elementwise
application of Ak, then Ffeat is D-spanning.
We note that as before, Ak are not necessarily SO(3)- equivariant.
Proof idea. The lemma follows directly from the assumptions.
□
5	Universality with irreducible representations
In this section, we discuss how to achieve universality when using irreducible representations of
SO(3). We will begin by defining irreducible representations, and explaining how linear univer-
sality is easily achieved by them, while the D-spanning properties of tensor representations can be
preserved. This discussion can be seen as an interpretation of the choices made in the construction
of TFN and similar networks in the literature. We then show that these architectures are indeed
universal.
5.1	IRREDUCIBLE REPRESENTATIONS OF SO(3)
In general, any finite-dimensional representation W of a compact group H can be decomposed into
irreducible representations: a subspace W0 ⊂ W is H-invariant if hw ∈ W0 for all h ∈ H, w ∈ W0.
A representation W is irreducible if it has no non-trivial invariant subspaces. In the case of SO(3), all
irreducible real representations are defined by matrices D(') (R), called the real Wigner D-matrices,
acting on w` := R2'+1 by matrix multiplication. In particular, the representation for ' = 0,1 are
D(0)(R) = 1 andD(1)(R) = R.
Linear maps between irreducible representations As mentioned above, one of the main advan-
tages of using irreducible representations is that there is a very simple characterization of all linear
equivariant maps between two direct sums of irreducible representations. We use the notation Wl
for direct sums of irreducible representations, where l = ('ι,...,'κ) ∈ NK and Wl = LK=I W4.
Lemma 5. Letl(I) = ('11),..., 'K)) and l ⑵=('12),..., 'K)). A function Λ = (Λι,..., ΛK) is
a linear equivariant mapping between Wl(1) and Wl(2), if and only if there exists a K1 × K2 matrix
M with Mij = 0 whenever 'i(1) 6= '(j2), such that
K1
Λj(V) =XMijVi	(12)
i=1
where V = (Vi)K11 and Vi ∈ W'⑴ for all i = 1,..., Ki.
Proof idea. This lemma is a simple generalization of Schur’s lemma, a classical tool in representa-
tion theory, which asserts that a non-zero linear map between irreducible representations is a scaled
multiply of the identity mapping. Lemma 5 was stated in the complex setting in Kondor (2018).
While Schur’s lemma, and thus Lemma 5, does not always hold for representations over the reals,
we observe here that it holds for real irreducible representations of SO(3) since their dimension is
always odd.	□
7
Published as a conference paper at ICLR 2021
Clebsch-Gordan decomposition of tensor products As any finite-dimensional representation of
SO(3) can be decomposed into a direct sum of irreducible representations, this is true for tensor
representations as well. In particular, the Clebsch-Gordan coefficients provide an explicit formula
for decomposing the tensor product of two irreducible representations W^ and W^ into a direct sum
of irreducible representations. This decomposition can be easily extended to decompose the tensor
product Wii 0 Wi2 into a direct sum of irreducible representations, where lι, l2 are now vectors.
In matrix notation, this means there is a unitary linear equivariant U(lι, l2) mapping of Wi1 0 Wi2
onto Wl, where the explicit values of l = l(l1, l2) and the matrix U(l1, l2) can be inferred directly
from the case where `1 and `2 are scalars.
By repeatedly taking tensor products and applying Clebsch-Gordan decompositions to the result,
TFN and similar architectures can achieve the D-spanning property in a manner analogous to tensor
representations, and also enjoy linear universality since they maintain irreducible representations
throughout the network.
5.2	Tensor field networks
We now describe the basic layers of the TFN architecture (Thomas et al., 2018), which are based on
irreducible representations, and suggest an architecture based on these layers which can approximate
G-equivariant maps into any representation Win,lτ ∈ Nj. There are some superficial differences
between our description of TFN and the description in the original paper, for more details see Ap-
pendix F.
We note that the universality of TFN also implies the universality of Fuchs et al. (2020), which
is a generalization of TFN that enables adding an attention mechanism. Assuming the attention
mechanism is not restricted to local neighborhoods, this method is at least as expressive as TFN.
TFNs are composed of three types of layers: (i) Convolution (ii) Self-interaction and (iii) Non-
linearities. In our architecture, we only use the first two layers types, which we will now describe:1.
Convolution. Convolutional layers involve taking tensor products of a filter and a feature vector
to create a new feature vector, and then decomposing into irreducible representations. Unlike in
standard CNN, a filter here depends on the input, and is a function F : R3 → WiD , where lD =
[0, 1, . . . , D]T. The `-th component of the filter F(x) = F (0) (x), . . . , F(D) (x) will be given by
Fm)(X)=R(') (Ilxk)Ym(X), m=—',...,'	(13)
where X = x/kxk if x = 0 and X = 0 otherwise, Ym are spherical harmonics, and R(`)any
polynomial of degree ≤ D. In Appendix F we show that these polynomial functions can be replaced
by fully connected networks, since the latter can approximate all polynomials uniformly.
The convolution of an input feature V ∈ Win and a filter F as defined above, will give an output
feature V = (Va)n=ι ∈ Win, where l° = I(If ,l), which is given by
工(X,V) = U(If ,li) (θ0Va + XX F(Xa - Xb) 0 %) .	(14)
More formally we will think of convolutional layer as functions of the form f(X, V ) =
(X, V (X, V )). These functions are defined by a choice of D, a choice of a scalar polynomial
R('),' = 0,...,D, and a choice of the parameter θo ∈ R in equation 14. We denote the set of all
such functions f by FD .
Self Interaction layers. Self interaction layers are linear functions from Λ : Win → Win, which are
obtained from elementwise application of equivariant linear functions Λ : Wi → WiT . These linear
functions can be specified by a choice of matrix M with the sparsity pattern described in Lemma 5.
Activation functions. TFN, as well as other papers, proposed several activation functions. We find
that these layers are not necessary for universality and thus we do not define them here.
Network architecture. For our universality proof, we suggest a simple architecture which depends
on two positive integer parameters (C, D): For given D, we will define Ffeat (D) as the set of
1Since convolution layers in TFN are not linear, the non-linearities are formally redundant
8
Published as a conference paper at ICLR 2021
function obtained by 2D recursive convolutions
Ffeat (D) = {πV ◦ f2D ◦ . . . f2 ◦ f1 ◦ ext(X)| fj ∈ FD},
where ext and πV are defined as in equation 10. The output of a function in Ffeat(D) is in Wln(D),
for some l(D) which depends on D. We then define Fpool (D) to be the self-interaction layers
which map Wln(D) to Wln . This choice of Ffeat (D) and Fpool (D), together with a choice of the
number of channels C, defines the final network architecture FCT,FDN = FC (Ffeat (D), Fpool(D)) as
in equation 2. In the appendix we prove the universality of TFN:
Theorem 2. For all n ∈ N, lτ ∈ Nj,
1.	For D ∈ N+, every G-equivariant polynomial p : R3×n → WTn of degree D is in FCT(FDN),D.
2.	Every continuous G-equivariant function can be approximated uniformly on compact sets
by functions in ∪D∈N+ FCT(FDN),D
As discussed previously, the linear universality of Fpool is guaranteed. Thus proving Theorem 2
amounts to showing that Ffeat (D) is D-spanning. This is done using the sufficient condition for
D-spanning defined in Lemma 4.
Proof idea. The proof is rather technical and involved. A useful observation (see Dai & Xu (2013))
used in the proof is that the filters of orders ` = 0, 1, . . . , D, defined in equation 13, span all
polynomial functions of degree D on R3 . This observation is used to show that all functions in QD
can be expressed by Ffeat(D) and so Ffeat is D-spanning, as stated in Lemma 2.	□
Alternative architecture The complexity of the TFN network used to construct G-equivariant
polynomials of degree D, can be reduced using a simple modifications of the convolutional layer in
equation 14: We add two parameters θ1 , θ2 ∈ R to the convolutional layer, which is now defined as:
nn
θl X F(Xa - Xb) X Vb + θ2 X F(Xa - Xb) X 匕.	(15)
b=1	b=1
With this simple change, we can show that Ffeat (D) is D-spanning even if we only take filters of
order 0 and 1 throughout the network. This is shown in Appendix E.
6 Conclusion
In this paper, we have presented a new framework for proving the universality of G-equivariant point
cloud networks. We used this framework for proving the universality of the TFN model Thomas et al.
(2018); Fuchs et al. (2020), and for devising two additional novel simple universal architectures. In
the future we hope to extend these simple constructions to operational G-equivariant networks with
universality guarantees and competitive practical performance.
Our universal architectures do not require activation functions, and use a single self-interaction layer.
In Appendix H we present an experiment indicating that the performance of TFN is not significantly
altered by these simplifications. Our architectures also require high order representations, and our
experiments show that using increasingly high order representations does indeed improve perfor-
mance. To date, practical TFN implementation included a relatively small amount of layers, and
did not use very high order representations. We believe our theoretical results will inspire interest
in stable implementation of larger architectures. On the other hand, an interesting open problem is
understanding whether universality can be achieved using only low-dimensional representations.
Finally, we believe that the framework we developed here will be useful for proving the universality
of other G-equivariant models for point cloud networks, and other related equivariant models. We
note that large parts of our discussion can be easily generalized to symmetry groups of the form
G = Rd o H × Sn acting on Rd×n, where H can be any compact topological group.
Acknowledgments The authors would like to thank Fabian B. Fuchs for making code available
and Taco Cohen for helpful discussion. N.D. is supported by THEORINET Simons award 814643.
9
Published as a conference paper at ICLR 2021
References
Matan Atzmon, Haggai Maron, and Yaron Lipman. Point convolutional neural networks by exten-
sion operators. arXiv preprint arXiv:1803.10091, 2018.
Alexander Bogatskiy, Brandon Anderson, Jan T Offermann, Marwah Roussi, David W Miller, and
Risi Kondor. Lorentz group equivariant neural network for particle physics. arXiv preprint
arXiv:2006.04780, 2020.
Taco S Cohen, Mario Geiger, Jonas Kohler, and Max Welling. Spherical cnns. arXiv preprint
arXiv:1801.10130, 2018.
Feng Dai and Yuan Xu. Approximation theory and harmonic analysis on spheres and balls, vol-
ume 23. Springer, 2013.
Carlos Esteves, Christine Allen-Blanchette, Ameesh Makadia, and Kostas Daniilidis. Learning so
(3) equivariant representations with spherical cnns. In Proceedings of the European Conference
on Computer Vision (ECCV), pp. 52-68, 2018.
Fabian B Fuchs, Daniel E Worrall, Volker Fischer, and Max Welling. Se (3)-transformers: 3d roto-
translation equivariant attention networks. arXiv preprint arXiv:2006.10503, 2020.
William Fulton and Joe Harris. Representation theory: a first course, volume 129. Springer Science
& Business Media, 2013.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.
Yulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu, and Mohammed Bennamoun. Deep
learning for 3d point clouds: A survey. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 2020.
Nicolas Keriven and Gabriel Peyre. Universal invariant and equivariant graph neural networks.
CoRR, abs/1905.04943, 2019. URL http://arxiv.org/abs/1905.04943.
Risi Kondor. N-body networks: a covariant hierarchical neural network architecture for learning
atomic potentials. arXiv preprint arXiv:1803.01588, 2018.
Risi Kondor, Zhen Lin, and Shubhendu Trivedi. Clebsch-gordan nets: a fully fourier space spherical
convolutional neural network. In Advances in Neural Information Processing Systems, pp. 10117-
10126, 2018a.
Risi Kondor, Hy Truong Son, Horace Pan, Brandon Anderson, and Shubhendu Trivedi. Covariant
compositional networks for learning graphs. arXiv preprint arXiv:1801.02144, 2018b.
Hanspeter Kraft and Claudio Procesi. Classical invariant theory, a primer. Lecture Notes, Version,
2000.
Jiaxin Li, Yingcai Bi, and Gim Hee Lee. Discrete rotation equivariance for point cloud recognition.
In 2019 International Conference on Robotics and Automation (ICRA), pp. 7269-7275. IEEE,
2019.
Takanori Maehara and Hoang NT. A simple proof of the universality of invariant/equivariant graph
neural networks, 2019.
Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph
networks. arXiv preprint arXiv:1905.11136, 2019a.
Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph
networks. In International Conference on Learning Representations, 2019b. URL https:
//openreview.net/forum?id=Syx72jC9tm.
Haggai Maron, Ethan Fetaya, Nimrod Segol, and Yaron Lipman. On the universality of invariant
networks. In International conference on machine learning, 2019c.
10
Published as a conference paper at ICLR 2021
Haggai Maron, Or Litany, Gal Chechik, and Ethan Fetaya. On learning sets of symmetric elements.
arXiv preprint arXiv:2002.08599, 2020.
Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav
Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks.
arXiv preprint arXiv:1810.02244, 2018.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017.
Adrien Poulenard, Marie-Julie Rakotosaona, Yann Ponty, and Maks Ovsjanikov. Effective rotation-
invariant point cnn with spherical harmonics kernels. In 2019 International Conference on 3D
Vision (3DV),pp. 47-56. IEEE, 2019.
Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets
for 3d classification and segmentation. Proc. Computer Vision and Pattern Recognition (CVPR),
IEEE, 1(2):4, 2017a.
Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical
feature learning on point sets in a metric space. In Advances in neural information processing
systems, pp. 5099-5108, 2017b.
Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. Quantum
chemistry structures and properties of 134 kilo molecules. Scientific data, 1(1):1-7, 2014.
Siamak Ravanbakhsh. Universal equivariant multilayer perceptrons. arXiv preprint
arXiv:2002.02912, 2020.
Nimrod Segol and Yaron Lipman. On universal equivariant set networks. arXiv preprint
arXiv:1910.02421, 2019.
Hadar Serviansky, Nimrod Segol, Jonathan Shlomi, Kyle Cranmer, Eilam Gross, Haggai Maron, and
Yaron Lipman. Set2graph: Learning graphs from sets. arXiv preprint arXiv:2002.08772, 2020.
Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick
Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point
clouds. arXiv preprint arXiv:1802.08219, 2018.
Minjie Wang, Lingfan Yu, Da Zheng, Quan Gan, Yu Gai, Zihao Ye, Mufei Li, Jinjing Zhou,
Qi Huang, Chao Ma, et al. Deep graph library: Towards efficient and scalable deep learning
on graphs. arXiv preprint arXiv:1909.01315, 2019a.
Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon.
Dynamic graph cnn for learning on point clouds. Acm Transactions On Graphics (tog), 38(5):
1-12, 2019b.
Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco Cohen. 3D Steerable
CNNs: Learning Rotationally Equivariant Features in Volumetric Data. 2018. URL http:
//arxiv.org/abs/1807.02547.
Daniel Worrall and Gabriel Brostow. Cubenet: Equivariance to 3d rotation and translation. In
Proceedings of the European Conference on Computer Vision (ECCV), pp. 567-584, 2018.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In International Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=ryGs6iA5Km.
Dmitry Yarotsky. Universal approximations of invariant maps by neural networks. arXiv preprint
arXiv:1804.10306, 2018.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov,
and Alexander J Smola. Deep sets. In Advances in neural information processing systems, pp.
3391-3401, 2017.
11
Published as a conference paper at ICLR 2021
Yongheng Zhao, Tolga Birdal, Jan Eric Lenssen, Emanuele Menegatti, Leonidas Guibas, and Fed-
erico Tombari. Quaternion equivariant capsule networks for 3d point clouds. arXiv preprint
arXiv:1912.12098, 2019.
A Notation
We introduce some notation for the proofs in the appendices. We use the shortened notation X =
X - 1X 1n1T and denote the columns of X by (Xi,..., xn). We denote
∑T = {~ ∈ N+∣M∣1 = T}
B Proofs for Section 3
B.1	G-EQUIVARIANT POLYNOMIALS ARE DENSE
A first step in proving denseness of G-equivariance polynomials, and in the proof used in the next
subsection is the following simple lemma, which shows that translation invariance can be dealt with
simply by centralizing the point cloud.
In the following, ρWT is some representation of SO(3) on a finite dimensional real vector space WT.
this induces an action ρWT ×Sn of SO(3) × Sn on WTn by
ρWT×Sn(R,P)(Y) =ρWT(R)YPT
This is also the action of G which we consider, ρG = ρWT ×Sn, where we have invariance with
respect to the translation coordinate. The action of G on R3×n is defined in equation 1.
Lemma B.1. A function f : R3×n → WTn is G-equivariant, if and only if there exists a function h
which is equivariant with respect to the action of SO(3) × Sn on R3×n, and
f (X) = h(X - 1XInlT)	(16)
nn
Proof. Recall that G-equivariance means SO(3) × Sn equivariance and translation invariance. Thus
if f is G-equivariant then equation 16 holds with h = f .
On the other hand, if f satisfies equation 16 then we claim it is G-equivariant. Indeed, for all
(t,R,P) ∈Rd o SO(3) × Sn,sincePT1n1Tn = 1n1Tn = 1n1TnPT,
f (PG(t, R, P)(X)) =f (R(X + t1n)PT) = h(R(X + t1n)PT - 1 R(X + t1)PTInlT)
nn
=h(R(X - 1XInlT)PT) = h (ρR3×Sn(R,P)(X - nXIniT))
=PWτ×Sn (R, P)h (X - nXInIT)
=ρG(t,R,P)f(X).
□
We now prove denseness of G-equivariant polynomials in the space of G-invariant continuous func-
tions (Lemma 1).
Lemma 1. Any continuous G-equivariant function in CG(R3×n, WTn) can be approximated uni-
formly on compact sets by G-equivariant polynomials in PG(R3×n, WTn).
Proof of Lemma 1. Let K ⊆ R3×n be a compact set. We need to show that continuous G-
equivariant functions can be approximated uniformly in K by G-equivariant polynomials. Let K0
denote the compact set which is the image of K under the centralizing map X → X - nXInIT.
By Lemma B.1, itis sufficient to show that every SO(3) × Sn equivariant continuous function f can
be approximated uniformly on K0 by a sequence of SO(3) × Sn equivariant polynomials pk. The
argument is now concluded by the following general lemma:
12
Published as a conference paper at ICLR 2021
Lemma B.2. Let G be a compact group, Let ρ1 and ρ2 be continuous2 representations of G on the
Euclidean spaces W1 and W2. Let K ⊆ W1 be a compact set. Then every equivariant function
f : W1 → W2 can be approximated uniformly on K by a sequence of equivariant polynomials
pk : W1 7→ W2.
Let μ be the Haar probability measure associated with the compact group G. Let Ki denote the
compact set obtained as an image of the compact set G × K under the continuous mapping
(g, X ) 7→ ρ1 (g)X.
Using the Stone-Weierstrass theorem, let pk be a sequence of (not necessarily equivariant) polyno-
mials which approximate f uniformly on K1 . Every degree D polynomial p : W1 → W2 induces a
G-equivariant function
hpi(X) =
G
P2(g-1)p(ρι(g)X )dμ(g).
This function hpi is a degree D polynomial as well: This is because hpi can be approximated uni-
formly on K1 by “Riemann Sums” of the form PjN=1 wjρ2(gj-1)p(ρ1(gj)X) which are degree D
polynomials, and because degree D polynomials are closed in C(K1).
Now for all X ∈ K1, continuity of the function g 7→ ρ2(g-1) implies that the operator norm of
ρ2(g-1) is bounded uniformly by some constant N > 0, and so
|hpki(X)-f(X)|
G
P2(g-1)Pk(PI(O)X) - P2(g-1)f (PI(O)X)dμ(g)
G
P2(O-1) [pk (P1 (O)X)
-f (ρι(g)X)] dμ(g)
≤ Nkf-pkkC(K1) →0
□
B.2	Proof of Theorem 1
Theorem 1. If Ffeat is D-spanning and Fpool is linearly universal, then there exists some C(D) ∈
N such that for all C ≥ C(D) the function space FC (Ffeat, Fpool) contains all G-equivariant
polynomials of degree ≤ D.
Proof. By the D-spanning assumption, there exist f1, . . . , fK ∈ Ffeat such that any vector valued
polynomial p : R3×n → Rn invariant to translations and equivariant to permutations is of the form
K
P(X ) = X Λ k (fk (X)),	(17)
k=1
where Λk are linear functions to R. If p is a matrix valued polynomial mapping R3×n to
WTn = Rt×n , which is invariant to translations and equivariant to permutations, then it is of the
form p = (pij)i∈[t],j∈[n], and each pi = (pij)j∈[n] is itself invariant to translations and permutation
equivariant. It follows that matrix valued p can also be written in the form equation 17, the only
difference being that the image of the linear functions Λk is now Rt .
Now let p : R3×n → WTn be a G-equivariant polynomial of degree ≤ D. It remains to show that
we can choose Λk to be SO(3) equivariant. We do this by a symmetrization argument: denote the
Haar probability measure on SO(3) by ν, and the action of SO(3) on Wfeat and WT by P1 and P2
respectively Denote p = (pj)jn=1 and fk = (fkj)jn=1. For every j = 1, . . . , n, we use the SO(3)
equivariance of pj and fkj to obtain
pj(X) =Z	P2(R-1)	◦ pj(RX)dν(R) =XK	Z	P2(R-1) ◦	Λk	◦	fjk(RX)dν(R)
SO(3)	k=1 SO(3)
KK
=X	P2(R-i) ◦ Λk (PI(R) ◦ fk(X)) dν(R) = XΛk ◦ fk(X),
k=1 SO(3)	k=1
2By this we mean that the maps (g, X) 7→ ρj (g)X, j = 1, 2 are jointly continuous
13
Published as a conference paper at ICLR 2021
where Λk stands for the equivariant linear functional from Wfeat to WT, defined for W ∈ Wfeat by
Λk(W)= [	P2(R-1) ◦ Λk (pι(R)w) dν(R).
SO(3)
Thus we have shown that P is in FC (Ffeat, FPool) for C = K, as required.	□
C Proofs for Section 4
We prove Lemma 2
Lemma 2. For every D ∈ N+, the set QD is D-spanning.
Proof. It is known (Segol & Lipman, 2019) (Theorem 2) that polynomials p : R3×n → Rn which
are Sn-equivariant, are spanned by polynomials of the form pα~ = (pjα~)jn=1, defined as
n
pjα~(X)=	X	xjα1 xiα22... xiαkk	(18)
i2,...,iK =1
where α~ = (α1 , . . . , αK ) and each αk ∈ N3+ is a multi-index. It follows that Sn-equivariant
polynomials of degree ≤ D are spanned by polynomials of the form Pa where PK=I ∣αk | ≤ D.
Denoting r% = ∣αk |, k = 1,...K, the sum of all r% by T, and ~ = (rk)3ι,we see that there exists
a linear functional Λα~,~r : TT → R such that
Pa (X )=Λa,~ ◦ Q~ (X)
where we recall that Q~r = Q(j~r)(X)	is defined in equation 6 as
n
Qj~)(x) =	X	X产㊈X察2㊈X产㊈...㊈XfKK.
i2 ,...,iK =1
Thus polynomials P = (Pj )jn=1 which are of degree ≤ D, and are Sn equivariant, can be written as
Pj (X )= XX	X	Λa,~ (Qj~)(X ))= XX Λ~ (1 ◦ QjF)(X)) ,j = 1,...,n,
T≤D ~r∈ΣT a~||ak|=rk	T≤D ~r∈ΣT
where Λ~ = P~∣∣afc∣=rfc Λ~,~ ◦ ιT1, and ιT1 is the left inverse of the embedding ∣. If P is also
translation invariant, then
p(X ) = P(X - 1X InlT )= X X Λ ~ (ι ◦ Q(F)(X - 1X 1n 1T)).
nn	nn
T≤D Fr∈ΣT
Thus Qd is D-spanning.	□
We prove Lemma 3
Lemma 3. The function set QD is contained in
Ffeat = {∣ ◦	∏V ◦	f1 ◦ f 2	◦ ... ◦	f T ◦ ext(X	—	1X InlT )| f j	∈	Fmin,T ≤	D}.	(11)
nn
Thus Ffeat is D-spanning.
Proof. In this proof we make the dependence of Ffeat on D explicit and denote Ffeat (D).
We prove the claim by induction on D. Assume D = 0. Then Q0 contains only the constant function
X 7→ 1n ∈ T0n, and this is precisely the function πV ◦ ext ∈ Ffeat (0).
Now assume the claim holds for all D0 with D - 1 ≥ D0 ≥ 0, and prove the claim for D. Choose
~r = (r1, . . . , rk) ∈ ΣT for some T ≤ D, we need to show that the function Q(rF) is in Ffeat(D).
Since Ffeat(D - 1) ⊆ Ffeat(D) we know from the induction hypothesis that this is true if T < D.
Now assume T = D. We consider two cases:
14
Published as a conference paper at ICLR 2021
1. If ri > 0, We set r = (ri - I,r2,..., rκ). We know that ι ◦ Q(r)(X) ∈ Ffeat(D - 1) by
the induction hypothesis. So there exist f2 , . . . , fD such that
ι ◦ ∏v ◦ f2 ◦ ... ◦ /d ◦ ext(X) = ι ◦ Q(r)(X).	(19)
Now choose fi ∈ Fmin to be the function whose V coordinate V = (Vj)jn=i, is given by
Vj-(X, V) = Xj 0 Vj, obtained by setting θi = I,θ2 = 0 m equation 9. Then , We have
n
Vj (X ,Q(r)(X))=	X	Xj 0 xjr1τ) 0 X 著 0 ... 0 XlfKK
i2,..	.,iK =i
=Qj~)(X).
and so
ι ◦ ∏v ◦ fi ◦ f2 ◦ ... ◦ /d ◦ ext(X — LX 1n1T) = ∣ ◦ Q(r)(X).	(20)
nn
and ∣ ◦ Q(~)(X - 1XInlT) ∈ Ffeat(D).
2.	If ri = 0. We assume without loss of generality that r > 0. Set r = (r2 - I,r3,..., rκ).
As before by the induction hypothesis there exist f2, . . . , fD which satisfy equation 19.
This time we choose fi ∈ Fmln to be the function whose V coordinate V = (Vj )jn=i, is
given by Vj (X, V) =	j Xj 0 Vj, obtained by setting θi = 0, θ2 = 1 in equation 9. Then
we have
nn
Vj(X,Q(r)(X)) = X X	Xj 0xf(r2-1) 0Xfr2 0 ... 0XfKrK
j=i l3,...,lK =i
n
= X	Xfr2 0 Xfr2 0 ... 0 XtfKK
l2,l3 ,...,lK =i
=Qj~)(X).
Thus equation 20 holds, and so again we have that ∣ ◦ Q(~)(X - nX 1n1n) ∈ Ffeat(D).
□
Finally we prove Lemma 4
Lemma 4. If all functions in QD can be written as
1K
∣◦ Q(~)(X - - XIniT) = EAkfk(X),
nn
k=i
where fk ∈ Ffeat, Ak : Wfeat → WfTat and Ak : Wfnat → (WfTat)n is defined by elementwise
application of Ak, then Ffeat is D-spanning.
Proof. If the conditions in Lemma 4 hold, then since QD is D-spanning, every translation invariant
and permutation equivariant polynomials p of degree D can be written as
p(X)=	X	Λ~(∣◦ Q(~)(X- 1XiniT))= X Λ~(X∣◦ Ak,~fk,~(X)j
~∣k~kι≤D	'	n	~	~∣k~kι ≤D	∖k = 1	)
Kr~
=X	XΛk,~(fk,~(X))
~∣k~kι≤Dk=1
where we denote Λk,~ = Λ~ ◦ ∣ ◦ Ak,~. Thus we proved Ffeat is D-SPanning.	□
15
Published as a conference paper at ICLR 2021
D Proofs for Section 5
We prove Lemma 5
Lemma 5. Letl(I) = ('11),..., 'K)) and l(2) = ('R..., 'K)). A function Λ = (Λι,..., Λk ) is
a linear equivariant mapping between Wl(1) and Wl(2), if and only if there exists a K1 × K2 matrix
M with Mij = 0 whenever 'i(1) 6= '(j2), such that
K1
Λj(V) =	MijVi	(12)
i=1
where V = (Vi)K11 and Vi ∈ W'⑴ for all i = 1,..., Ki.
Proof. As mentioned in the main text, this lemma is based on Schur’s lemma. This lemma is typi-
cally stated for complex representations, but holds for odd dimensional real representation as well.
We recount the lemma and its proof here for completeness (see also (Fulton & Harris, 2013)).
Lemma D.1 (Schur's Lemma for SO(3)). Let Λ : W'1 → W^ be a linear equivariant map. If
'1 6= '2 then Λ = 0. Otherwise Λ is a scalar multiply of the identity.
Proof. Let Λ : W^ → W^ be a linear equivariant map. The image and kernel of Λ are invariant
subspaces of W^ and W^, respectively. It follows that if Λ = 0 then Λ is a linear isomorphism
so necessarily 'i = '2. Now assume '1 = '2. Since the dimension of W^ is odd, Λ has a real
eigenvalue λ. The linear function Λ - λI is equivariant and has a non-trivial kernel, so Λ - λI =
0.	□
We now return to the proof of Lemma 5. Note that each Λj :吗⑴ → W'⑶ is linear and SO(3)
`j
equivariant. Next denote the restrictions of each Λj to w`(i) , i = l,...,K2 by Λij, and note that
i
K1
Λj(V1,...,VK1) =XΛij(Vi).
i=1
(21)
By considering vectors in 吗⑴ of the form (0,..., 0, Vi, 0 ..., 0) We see that each Λij :叫⑴ →
W'(2)is linear and SO(3)-equivariant. Thus by Schur,s lemma, if 'i1) = 'j2) then Λij (Vi) = MijVi
j
for some real Mij, and otherwise Mij = 0. Plugging this into equation 21 we obtain equation 12.
□
We prove Theorem 2 which shows that the TFN network described in the main text is universal:
Theorem 2. For all n ∈ N, lτ ∈ Nj,
1.	For D ∈ N+, every G-equivariant polynomial p : R3×n → WTn of degree D is in FCT(FDN),D.
2.	Every continuous G-equivariant function can be approximated uniformly on compact sets
by functions in ∪D∈N+ FCT(FDN),D
Proof. As mentioned in the main text, we only need to show that the function space Ffeat (D) is
D-spanning. Recall that Ffeat (D) is obtained by 2D consecutive convolutions with D-filters. In
general, we denote the space of functions defined by applying J consecutive convolutions by GJ,D .
If Y is a space of functions from R3×n → Y n, we denote by hY, TTi the space of all functions
p : R3×n → TTn of the form
K
P(X ) = X Akfk (X),	(22)
k=1
16
Published as a conference paper at ICLR 2021
where Ak : Y → TT are linear functions, Ak : Yn → TTn are induced by elementwise application,
and fk ∈ Y. This notation is useful because: (i) by Lemma 4 it is sufficient to show that Q(~)(X)
is in hG2D,D , TTi for all ~r ∈ ΣT and all T ≤ D, and because (ii) it enables comparison of the
expressive power of function spaces Y1, Y2 whose elements map to different spaces Y1n, Y2n, since
the elements in hYi, TTi, i = 1, 2 both map to the same space. In particular, note that if for every
f ∈ Y2 there is a g ∈ Yi and a linear map A : Y1 → Y2 SUCh that f (X) = A ◦ g(X), then
hY2,TTi ⊆ hY1,TTi.
We now use this abstract discussion to prove some useful results: the first is that for the purpose
of this lemma, we can ‘forget about’ the multiplication by a unitary matrix in equation 14, used
for decomposition into irreducible representations: To see this, denote by Gj,d the function space
obtained by taking J consecutive convolutions with D-filters without multiplying by a unitary matrix
in equation 14. Since Kronecker products of unitary matrices are unitary matrices, we obtain that the
elements in GJ,D and GJ,D differ only by multiplication by a unitary matrix, and thus hGJ,D, TTi ⊆
hGJ,D,TTi and hGJ,D,TTi ⊆ hGJ,D, TT i, so both sets are equal.
Next, we prove that adding convolutional layers (enlarging J) or taking higher order filters (enlarg-
ing D) can only increase the expressive power ofa network.
Lemma D.2. For all J, D, T ∈ N+,
1.	hGJ,D, TTi ⊆ hGJ+1,D, TTi.
2.	hGJ,D, TTi ⊆ hGJ,D+1, TTi.
Proof. The first claim follows from the fact that every function f in hGJ,D , TTi can be identified
with a function in hGJ+1,D, TTi by taking the J + 1 convolutional layer in equation 14 with θ0 =
1, F = 0.
The second claim follows from the fact that D-filters can be identified with D + 1-filters whose
D + 1-th entry is 0.	□
The last preliminary lemma we will need is
Lemma D.3. For every J, D ∈ N+, and every t, s ∈ N+, if p ∈ hGJ,D , Tti, then the function q
defined by
n
qa(X) = X(Xa - Xb产乳 Pb(X)
b=1
isinhGJ+1,D,Tt+si.
Proof. This lemma is based on the fact that the space of s homogeneous polynomial on R3
is spanned by polynomials of the form ∣∣xks-`Ymm(x) for ' = s,s - 2,s - 4... (Dai & Xu,
2013). For each such `, and s ≤ D, these polynomials can be realized by filters F (`) by setting
R(')(∣χ∣) = IlXks so that
Fm (X) = kχksYm (X) = kχks-'γm (x).
For every D ∈ N and s ≤ D, we can construct a D-filter Fs,D = (F(0), . . . , F(D)) where
F(s), F(s-2), . . . are as defined above and the other filters are zero. Since both the entries of
Fs,D(x), and the entries of X汽 span the space of s-homogeneous polynomials on R3, it follows
that there exists a linear mapping Bs : WlD → Ts so that
Xe)S = Bs(Fs，D(x)),∀x ∈ R3.	(23)
Thus, since p can be written as a sum of compositions of linear mappings with functions in GJ,D as
in equation 22, and similarly x0s is obtained as a linear image of functions in Gi,d as in equation 23,
we deduce that
nn
E(Xa - Xb) 0 Pb(X) = E(Xa — Xb) 0 Pb(X)
is in hGJ+1,D, Tt+si
□
17
Published as a conference paper at ICLR 2021
As a final preliminary, we note that D-filters can perform an averaging operation by setting R(0) = 1
and θ0, R(1), . . . , R(D) = 0 in equation 13 and equation 14 . We call this D-filter an averaging filter.
We are now ready to prove our claim: we need to show that for every D, T ∈ N+ where T ≤ D,
for every ~r ∈ ΣT, the function Q(~r) is in hG2D,D, TTi. Note that due to the inclusion relations in
Lemma D.2 it is sufficient to prove this for the case T = D. We prove this by induction on D. For
D = 0, vectors ~r ∈ Σ0 contains only zeros and so
Q⑺(X) = 1n = ∏v ◦ ext(X) ∈ hG0,0, T0i.
We now assume the claim is true for all D0 with D > D0 ≥ 0 and prove the claim is true for D. We
need to show that for every ~r ∈ ΣD the function Q(~r) is in hG2D,D, TDi. We prove this yet again by
induction, this time on the value of ri: assume that ~ ∈ ∑d and ri = 0.. Denote by r the vector in
ΣD-1 defined by
r=(r2 - I,r3,∙∙∙,rκ).
By the induction assumption on D, We know that Q(r)(X) ∈ G2(D-1),D-1,D-1 and so
nn	n
qa(X) = X(Xa -Xb)区 Qbr)(X)= X(Xa - Xb)0 土产 - 1 0 X	X^r3 0 ... 0 X京K
b=i	b=i	i3,...,iK =i
=H 0 X Qbr)(X)) -Q(~)(X)
is in hG2D-i,D-i, TDi by Lemma D.3, which is contained in hG2D-i,D, TDi by Lemma D.2. Since
Xa has zero mean, while Qar)(X) does not depend on a since ri = 0, applying an averaging
filter to qa gives US a constant value -Qar)(X) in each coordinate a ∈ [n], and so Q(~)(X) is in
hG2D,D , TD i.
Now assume the claim is true for all ~r ∈ ΣD which sum to D, and whose first coordinate is smaller
than some ri0 ≥ 1, we now prove the claim is true when the first coordinate of ~r is equal to ri0 . The
vector r = (r2,...,rκ) obtained from r by removing the first coordinate, sums to D0 = D - ri <
D, and so by the induction hypothesis on D we know that Q(r) ∈ hG2D0,D0, TD，). By Lemma D.3
we obtain a function qa ∈ hG2D0+i,D0, TDi ⊆ hG2D,D, TDi defined by
n
qa (X )= X(Xa - Xb)0r1 0 Qbr) (X)
b=i
nn
=X(Xa -Xb产1 0Xb)r2 0 X	X微r3 0...0XfKK
b=i	i3,...,iK =i
= Qa~)(X) + additional terms
where the additional terms are linear combinations of functions of the form PDQar )(X) where
r0 ∈ ΣD and their first coordinate ri is smaller than ri0 , and PD : TD → TD is a permutation. By
the induction hypothesis on ri, each such Q(r0) is in hG2D,D,Td). It follows that PDQar )(X),a =
1,...,n, and thus Q(~)(X), are in hG2D,D, TD) as well. This concludes the proof of Theorem 2.
□
E	Alternative TFN architecture
In this appendix we show that replacing the standard TFN convolutional layer with the layer defined
in equation 15:
nn
θiXF(Xa-Xb)0Vb+θ2XF(Xa-Xb)0Va ,
b=i	b=i
we can obtain D-spanning networks using 2D consecutive convolutions with 1-filters (that is, filters
in Wl1, where li = [0, 1]T). Our discussion here is somewhat informal, meant to provide the general
18
Published as a conference paper at ICLR 2021
ideas without delving into the details as we have done for the standard TFN architecture in the proof
of Theorem 2. In the end of our discussion we will explain what is necessary to make this argument
completely rigorous.
We will only need two fixed filters for our argument here: The first is the 1-filter FId = (F(0), F(1))
defined by setting R(0)(kxk) = 0 and R(1) (kxk) = kxk to obtain
FId(X) = l∣χ∣∣Y 1(x) = IlxIlx = χ.
The second is the filter F1 defined by setting R(0)(kxk) = 1 and R(1) (kxk) = 0, so that
F1 (x) = 1.
We prove our claim by showing that a pair of convolutions with 1-filters can construct any convolu-
tional layer defined in equation 9 for the D-spanning architecture using tensor representations. The
claim then follows from the fact that D convolutions of the latter architecture suffice for achieving
D-spanning, as shown in Lemma 3.
Convolutions for tensor representations, defined in equation 9, are composed of two terms:
n
VtensorJ(X,V) = xa ㊈匕 and 璟ensor,2 3(χ,V)= X观㊈监
b=1
To obtain the first term Vatensor,1, we set θ1 = 0, θ2 = 1/n, F = FId in equation 15 we obtain
(the decomposition into irredUcibles of)吟ensor,1(X, V) = xa X 匕.Thus this term can in fact be
expressed by a single convolution. We can leave this outcome unchanged by a second convolution,
defined by setting θ1 = 0, θ2 = 1/n, F = F1.
To obtain the second term Vtensorv We apply a first convolution with θι = -1, F = Fid, θ2 = 0,
to obtain
nn	n
X(xb — xa) 0 Vb = X(xb — xa) 0 Vb = Vtensor,2 (V,X) - xa 0 X Vb
b=1	b=1	b=1
By applying an additional averaging filter, defined by setting θι = n, F = F1,θ2 = 0, we obtain
Vtensor,2(V, X). This concludes our ‘informal proof’.
Our discussion here has been somewhat inaccurate, since in practice Fid(x) = (0, x) ∈ Wo ㊉ Wi
and Fi (x) = (1,0) ∈ Wo ㊉ Wi. Moreover, in our proof we have glossed over the multiplication
by the unitary matrix used to obtain decomposition into irreducible representations. However the
ideas discussed here can be used to show that 2D convolutions with 1-filters can satisfy the sufficient
condition for D-spanning defined in Lemma 4. See our treatment of Theorem 2 for more details.
F Comparison with original TFN paper
In this Appendix we discuss three superficial differences between the presentation of the TFN archi-
tecture in Thomas et al. (2018) and our presentation here:
1. We define convolutional layers between features residing in direct sums of irreducible rep-
resentations, while (Thomas et al., 2018) focuses on features which inhabit a single irre-
ducible representation. This difference is non-essential, as direct sums of irreducible rep-
resentations can be represented as multiple channels where each feature inhabits a single
irreducible representation.
2. The term θoVa in equation 14 appears in (Fuchs et al., 2020), but does not appear explicitly
in (Thomas et al., 2018). However it can be obtained by concatenation of the input of a
self-interaction layer to the output, and then applying a self-interaction layer.
3. We take the scalar functions R(') to be polynomials, while (Thomas et al., 2018) take them
to be fully connected networks composed with radial basis functions. Using polynomial
scalar bases is convenient for our presentation here since it enables exact expression of
19
Published as a conference paper at ICLR 2021
equivariant polynomials. Replacing polynomial bases with fully connected networks, we
obtain approximation of equivariant polynomials instead of exact expression. It can be
shown that if p is a G-equivariant polynomial which can be expressed by some network
FC,D defined with filters coming from a polynomial scalar basis, then p can be approx-
imated on a compact set K, up to an arbitrary error, by a similar network with scalar
functions coming from a sufficiently large fully connected network.
G	Tensor Universality
In this section we show how to construct the complete set Fpool of linear SO(3) invariant functionals
from WfTeat = LTD=0 TT to R. Since each such functional Λ is of the form
Λ(w0, . . . , wD) =	ΛT (wT),
T=0
where each ΛT is SO(3)-invariant, it is sufficient to characterize all linear SO(3)-invariant function-
als Λ : TD → R.
It will be convenient to denote
W = R3 and W0D = R3D = Td.
We achieve our characterization using the bijective correspondence between linear functional Λ :
W0D → R and multi-linear functions Λ : WD → R: each such Λ corresponds to a unique A, such
that
Λ(eiι,. .. ,eiD) = A" 0 ... 0 e%D), ∀(iι,...,iD) ∈ [3]D,	(24)
where e1, e2, e3 denote the standard basis elements of R3. We define a spanning set of equivariant
linear functionals on W0D via a corresponding characterization for multi-linear functionals on WD.
Specifically, set
KD = {k ∈ N+ | D - 3k is even and non-negative. }
For k ∈ KD we define a multi-linear functional:
Λk(wι,..., WD) = det(wι, W2, W3) X ... X det(w3k-2, W3k-1, w3k) X hw3k+1, w3k+2i X ...
× hwD-1, wDi,	(25)
and for (k, σ) ∈ KD X SD we define
Λk,σ(w1, . . . , wD) = Λk (wσ(1), . . . , wσ(D))	(26)
Proposition 1. The space of linear invariant functions from TD to R is spanned by the set of linear
invariant functionals λD = {Λk,σ | (k, σ) ∈ KD X SD} induced by the multi-linearfUnctional Λk,σ
described in equation 25 and equation 26
We note that (i) equation 24 provides a (cumbersome) way to compute all linear invariant functionals
Λk,σ explicitly by evaluating the corresponding Λk,σ on the 3D elements of the standard basis and
(ii) the set λD is spanning, but is not linearly independent. For example, since hw1, w2i = hw2, w1i,
the space of SO(3) invariant functionals on 石 = W02 is one dimensional while ∣λ21 = 2.
Proof of Proposition 1. We first show that the bijective correspondence between linear functional
Λ : W0D → R and multi-linear functions Λ : WD → R, extends to a bijective correspondence
between SO(3)-invariant linear/multi-linear functionals. The action of SO(3) on WD is defined by
P(R)(W1,..., wD) = (Rwι,..., RwD).
The action ρ(R) = R0D of SO(3) on W0D is such that the map
(w1 , . . . , wD ) 7→ w1 0 w2 . . . wD
is SO(3)- equivariant. It follows that if Λ and Λ satisfy equation 24, then for all R ∈ SO(3), the
same equation holds for the pair Λ ◦ P(R) and Λ ◦ ρ(R). Thus SO(3)-invariance of Λ is equivalent
to SO(3)-invariance ofΛ.
20
Published as a conference paper at ICLR 2021
Multi-linear functionals on W D invariant to P area subset of the set of polynomials on W D invariant
to p. It is known (see (Kraft & Procesi, 2000), page 114), that all such polynomials are algebraically
generated by functions of the form
det(wi1, wi2, wi3) and hwj1, wj2 i, where i1,i2,i3,j1,j2 ∈ [D].
Equivalently, SO(3)-invariant polynomials are spanned by linear combinations of polynomials of
the form
det(wi1, wi2, wi3) det(wi4, wi5, wi6) . . . hwj1, wj2 ihwj3, wj4 i ....... (27)
When considering the subset of multi-linear invariant polynomials, we see that they must be spanned
by polynomials as in equation 27, where each w1 , . . . , wD appears exactly once in each polynomial
in the spanning set. These precisely correspond to the functions in λD.
□
H Experiments
Table 1: Results obtained on the QM9 dataset Ramakrishnan et al. (2014) for different design choices
in the TFN architecture. Results are reported for the homo target variable, and are multiplied by
103.	_________________________________________________________________________________
Model variant	Mean `1 error
Order 0 irreps	111.2 ± 0.3
Order 0-1 irreps	82.1 ± 0.4
Order 0-2 irreps	62.1 ± 1.5
Order 0-3 irreps	53.2 ± 1.9
Order 0-4 irreps	51.2 ± 0.4
Order 0-3 irreps without non-linearity	53.1 ± 0.8
Order 0-3 irreps + self-interaction only in the final layer	55.2 ± 0.3
This section provides an experimental evaluation of different design choices of the TFN architecture,
inspired by our theoretical analysis. We study the following questions:
1.	The importance of non-linear activation. Our proof shows that using non-linear acti-
vation functions is not necessary for proving universality. Here, we empirically test the
possibility of removing these layers.
2.	The importance of high-dimensional irreducible representations. Our theoretical anal-
ysis shows that in order to represent/approximate high degree polynomials, high-order rep-
resentations should be used. Here, we check whether using high-order representations has
practical benefits.
3.	The effect of self-interaction layers. Our proof suggests that it is enough to use self
interaction linear layers at the end of the model. We empirically compare this approach
with the more common approach of using self-interaction layers after each convolutional
layer.
Dataset. We use the QM9 (Ramakrishnan et al., 2014) dataset for our experiments. The dataset
contains 134K molecules, with node 3D positions, 5 categorical node features and4 categorical edge
features. The task is a molecule property prediction regression task.
Framework. We used pytorch (Paszke et al., 2017)as the deep learning framework and the Deep
Graph Library (DGL) (Wang et al., 2019a) as the graph learning framework. All experiments ran on
NVIDIA GV100 GPUs.
Experimental setup. We use the the TFN implementation from Fuchs et al. (2020). We trained
each model variant for 50 epochs on the homo target variable using an `1 loss function and the
ADAM optimizer with learning rate 10-3 and report results on the test set on the final epoch aver-
aged over two runs. We used the default parameters and data splits from Fuchs et al. (2020).
21
Published as a conference paper at ICLR 2021
Architecture. The architecture consists of 4 TFN convolutional layers, each followed by a linear
self-interaction layer. We used 16 copies of each irreducible representation used. We used norm-
based non-linearity as in the original TFN paper (Thomas et al., 2018). These convolutional layers
are followed by a max-pooling layer and two fully connected layers with 16d features in the hidden
layer, where d is the maximal degree of irreducible representations used.
Results. Table 1 and Figure 1 present the re-
sults. The main conclusions are: (1) The ex-
periments show that, at least for this task, using
non-linear activations does not improve perfor-
mance. This result fits our theoretical analysis
which shows that these layers are not needed
for universality. (2) Figure 1 presents a plot
of error vs representation degrees used. The
plot clearly shows that using high-dimensional
representations (up to order 3) improves per-
formance, which also fits our analysis. Using
representation orders higher than 3 is signifi-
cantly more time consuming, and was found to
have little effect on the results (as in in Fuchs
et al. (2020)), though we believe this to be
application-dependent. (3) Using self interac-
tion layers only at the end of the model is shown
to have marginal negative effect on the results.
Figure 1: `1 error versus maximal irreducible
representation used. It is clear that the error is
reduced as higher order representation are used.
Semi-transparent color represents standard devia-
tion.
22