Published as a conference paper at ICLR 2021
Understanding Overparameterization
in Generative Adversarial Networks
YOgesh BalajiI；Mohammadmahdi Sajedi2*, Neha Mukund Kalibhat1, Mucong Ding1,
Dominik StOger2, Mahdi Soltanolkotabi2, Soheil Feizi1
1	University of Maryland, College Park, MD
2	University of Southern California, Los Angeles, CA
Ab stract
A broad class of unsupervised deep learning methods such as Generative Adver-
sarial Networks (GANs) involve training of overparameterized models where the
number of parameters of the model exceeds a certain threshold. Indeed, most suc-
cessful GANs used in practice are trained using overparameterized generator and
discriminator networks, both in terms of depth and width. A large body of work
in supervised learning have shown the importance of model overparameterization
in the convergence of the gradient descent (GD) to globally optimal solutions.
In contrast, the unsupervised setting and GANs in particular involve non-convex
concave mini-max optimization problems that are often trained using Gradient
Descent/Ascent (GDA). The role and benefits of model overparameterization in
the convergence of GDA to a global saddle point in non-convex concave problems
is far less understood. In this work, we present a comprehensive analysis of the
importance of model overparameterization in GANs both theoretically and em-
pirically. We theoretically show that in an overparameterized GAN model with a
1-layer neural network generator and a linear discriminator, GDA converges to a
global saddle point of the underlying non-convex concave min-max problem. To
the best of our knowledge, this is the first result for global convergence of GDA in
such settings. Our theory is based on a more general result that holds for a broader
class of nonlinear generators and discriminators that obey certain assumptions (in-
cluding deeper generators and random feature discriminators). Our theory utilizes
and builds upon a novel connection with the convergence analysis of linear time-
varying dynamical systems which may have broader implications for understand-
ing the convergence behavior of GDA for non-convex concave problems involving
overparameterized models. We also empirically study the role of model overpa-
rameterization in GANs using several large-scale experiments on CIFAR-10 and
Celeb-A datasets. Our experiments show that overparameterization improves the
quality of generated samples across various model architectures and datasets. Re-
markably, we observe that overparameterization leads to faster and more stable
convergence behavior of GDA across the board.
1	Introduction
In recent years, we have witnessed tremendous progress in deep generative modeling with some
state-of-the-art models capable of generating photo-realistic images of objects and scenes (Brock
et al., 2019; Karras et al., 2019; Clark et al., 2019). Three prominent classes of deep generative
models include GANs (Goodfellow et al., 2014), VAEs (Kingma & Welling, 2014) and normalizing
flows (Dinh et al., 2017). Of these, GANs remain a popular choice for data synthesis especially in the
image domain. GANs are based on a two player min-max game between a generator network that
generates samples from a distribution, and a critic (discriminator) network that discriminates real
distribution from the generated one. The networks are optimized using Gradient Descent/Ascent
(GDA) to reach a saddle-point of the min-max optimization problem.
* First two authors contributed equally. Correspondence to yogesh@cs.umd.edu, sajedi@usc.edu
1
Published as a conference paper at ICLR 2021
Figure 1: Overparameterization in GANs. We train DCGAN models by varying the size of the
hidden dimension k (larger the k, more overparameterized the models are, see Fig. 8 for details).
Overparameterized GANs enjoy improved training and test FID scores (the left panel), generate
high-quality samples (the middle panel) and have fast and stable convergence (the right panel).
k=8
k=128
One of the key factors that has contributed to the successful training of GANs is model overpa-
rameterization, defined based on the model parameters count. By increasing the complexity of
discriminator and generator networks, both in depth and width, recent papers show that GANs can
achieve photo-realistic image and video synthesis (Brock et al., 2019; Clark et al., 2019; Karras
et al., 2019). While these works empirically demonstrate some benefits of overparameterization,
there is lack of a rigorous study explaining this phenomena. In this work, we attempt to provide a
comprehensive understanding of the role of overparameterization in GANs, both theoretically and
empirically. We note that while overparameterization is a key factor in training successful GANs,
other factors such as generator and discriminator architectures, regularization functions and model
hyperparameters have to be taken into account as well to improve the performance of GANs.
Recently, there has been a large body of work in supervised learning (e.g. regression or classifica-
tion problems) studying the importance of model overparameterization in gradient descent (GD)’s
convergence to globally optimal solutions (Soltanolkotabi et al., 2018; Allen-Zhu et al., 2019; Du
et al., 2019; Oymak & Soltanolkotabi, 2019; Zou & Gu, 2019; Oymak et al., 2019). A key ob-
servation in these works is that, under some conditions, overparameterized models experience lazy
training (Chizat et al., 2019) where optimal model parameters computed by GD remain close to a
randomly initialized model. Thus, using a linear approximation of the model in the parameter space,
one can show the global convergence of GD in such minimization problems.
In contrast, training GANs often involves solving a non-convex concave min-max optimization prob-
lem that fundamentally differs from a single minimization problem of classification/regression. The
key question is whether overparameterized GANs also experience lazy training in the sense that
overparameterized generator and discriminator networks remain sufficiently close to their initializa-
tions. This may then lead to a general theory of global convergence of GDA for such overparame-
terized non-convex concave min-max problems.
In this paper we first theoretically study the role of overparameterization for a GAN model with a
1-hidden layer generator and a linear discriminator. We study two optimization procedures to solve
this problem: (i) using a conventional training procedure in GANs based on GDA in which gen-
erator and discriminator networks perform simultaneous steps of gradient descent to optimize their
respective models, (ii) using GD to optimize generator’s parameters for the optimal discriminator.
The latter case corresponds to taking a sufficiently large number of gradient ascent steps to update
discriminator’s parameters for each GD step of the generator. In both cases, our results show that
in an overparameterized regime, the GAN optimization converges to a global solution. To the best
of our knowledge, this is the first result showing the global convergence of GDA in such settings.
While in our results we focus on one-hidden layer generators and linear discriminators, our theory is
based on analyzing a general class of min-max optimization problems which can be used to study a
much broader class of generators and discriminators potentially including deep generators and deep
random feature-based discriminators. A key component of our analysis is a novel connection to ex-
ponential stability of non-symmetric time varying dynamical systems in control theory which may
have broader implications for theoretical analysis of GAN’s training. Ideas from control theory have
2
Published as a conference paper at ICLR 2021
also been used for understanding and improving training dynamics of GANs in (Xu et al., 2019; An
et al., 2018).
Having analyzed overparameterized GANs for relatively simple models, we next provide a compre-
hensive empirical study of this problem for practical GANs such as DCGAN (Radford et al., 2016)
and ResNet GAN (Gulrajani et al., 2017) trained on CIFAR-10 and Celeb-A datasets. For example,
the benefit of overparamterization in training DCGANs on CIFAR-10 is illustrated in Figure 1. We
have three key observations: (i) as the model becomes more overparameterized (e.g. using wider
networks), the training FID scores that measure the training error, decrease. This phenomenon
has been observed in other studies as well (Brock et al., 2019). (ii) overparameterization does not
hurt the test FID scores (i.e. the generalization gap remains small). This improved test-time per-
formance can also be seen qualitatively in the center panel of Figure 1, where overparameterized
models produce samples of improved quality. (iii) Remarkably, overparameterized GANs, with a lot
of parameters to optimize over, have significantly improved convergence behavior of GDA, both in
terms of rate and stability, compared to small GAN models (see the right panel of Figure 1).
In summary, in this paper
•	We provide the first theoretical guarantee of simultaneous GDA’s global convergence for an
overparameterized GAN with one-hidden neural network generator and a linear discrimi-
nator (Theorem 2.1).
•	By establishing connections with linear time-varying dynamical systems, we provide a
theoretical framework to analyze simultaneous GDA’s global convergence for a general
overparameterized GAN (including deeper generators and random feature discriminators),
under some general conditions (Theorems 2.3 and A.4).
•	We provide a comprehensive empirical study of the role of model overparameterization
in GANs using several large-scale experiments on CIFAR-10 and Celeb-A datasets. We
observe overparameterization improves GANs’ training error, generalization error, sample
qualities as well as the convergence rate and stability of GDA.
2	Theoretical results
2.1	Problem formulation
Given n data points of the form x1 , x2, . . . , xn ∈ Rm, the goal of GAN’s training is to find a
generator that can mimic sampling from the same distribution as the training data. More specifically,
the goal is to find a generator mapping Gθ(z) : Rd → Rm, parameterized by θ ∈ Rp, so that
Gθ(z1), Gθ(z2), . . . , Gθ(zn) with z1, z2, . . . , zn generated i.i.d. according toN(0, Id) has a similar
empirical distribution to x1 , x2, . . . , xn1. To measure the discrepancy between the data points and
the GAN outputs, one typically uses a discriminator mapping Dθe : Rm → R parameterized with
θe ∈ Rpe. The overall training approach takes the form of the following min-max optimization
problem which minimizes the worst-case discrepancy detected by the discriminator
1n	1n
minmaχ	— X De(Xi)- - X De(Gθ (Zi)) + R(e).	⑴
θ	θe	n θ	n θ
θ	i=1	i=1
Here, R(θ) is a regularizer that typically ensures the discriminator is Lipschitz. This formulation
mimics the popular Wasserstein GAN (Arjovsky et al., 2017) (or, IPM GAN) formulations. This
optimization problem is typically solved by running Gradient Descent Ascent (GDA) on the mini-
mization/maximization variables.
The generator and discriminator mappings G and D used in practice are often deep neural networks.
Thus, the min-max optimization problem above is highly nonlinear and non-convex concave. Sad-
dle point optimization is a classical and fundamental problem in game theory (Von Neumann &
Morgenstern, 1953) and control (Gutman, 1979). However, most of the classical results apply to the
1In general, the number of observed and generated samples can be different. However, in practical GAN
implementations, batch sizes of observed and generated samples are usually the same. Thus, for simplicity, we
make this assumption in our setup.
3
Published as a conference paper at ICLR 2021
convex-concave case (Arrow et al., 1958) while the saddle point optimization of GANs is often non
convex-concave. If GDA converges to the global (local) saddle points, we say it is globally (locally)
stable. For a general min-max optimization, however, GDA can be trapped in a loop or even diverge.
Except in some special cases (e.g. (Feizi et al., 2018) for a quadratic GAN formulation or (Lei et al.,
2019) for the under-parametrized setup when the generator is a one-layer network), GDA is not
globally stable for GANs in general (Nagarajan & Kolter, 2017; Mescheder et al., 2018; Adolphs
et al., 2019; Mescheder et al., 2017; Daskalakis et al., 2020).
None of these works, however, study the role of model overparameterization in the global/local con-
vergence (stability) of GDA. In particular, it has been empirically observed (as we also demonstrate
in this paper) that when the generator/discriminator contain a large number of parameters (i.e. are
sufficiently overparameterized) GDA does indeed find (near) globally optimal solutions. In this
section we wish to demystify this phenomenon from a theoretical perspective.
2.2	Definition of Model Overparameterization
In this paper, we use overparameterization in the context of model parameters count. Informally
speaking, overparameterized models have large number of parameters, that is we assume that the
number of model parameters is sufficiently large. In specific problem setups of Section 2, we pre-
cisely compute thresholds where the number of model parameters should exceed in order to observe
nice convergence properties of GDA. Note that the definition of overparameterization based on
model parameters count is related, but distinct from the complexity of the hypothesis class. For
instance, in our empirical studies, when we say we overparameterize a neural network, we fix the
number of layers in the neural network and increase the hidden dimensions. Our definition does not
include the case where the number of layers also increases, which forms a different hypothesis class.
2.3	Results for one-hidden layer generators and random discriminators
In this section, we discuss our main results on the convergence of gradient based algorithms when
training GANs in the overparameterized regime. We focus on the case where the generator takes the
form of a single hidden-layer ReLU network with d inputs, k hidden units, and m outputs. Specifi-
cally, G (Z) = V ∙ ReLU (Wz) with W ∈ Rk×d and V ∈ Rm×k denoting the input-to-hidden and
hidden-to-output weights. We also consider a linear discriminator of the form D(x) = dTx with an
`2 regularizer on the weights i.e. R(d) = - kdk`2 /2. The overall min-max optimization problem
(equation 1) takes the form
1 n	kdk`2
min max	L(W, d)	:=〈d, —	(Xi	— VReLU (Wzi)))—
w∈Rk×d d∈am	,	, :	',n 乙 Ii	' i	2
i=1
(2)
Note that we initialize V at random and keep it fixed throughout the training. The common approach
to solve the above optimization problem is to run a Gradient Descent Ascent (GDA) algorithm. At
iteration t, GDA takes the form
dt+1 =	dt + μ%dL (Wt, dt)
Wt+1 = Wt — ηVwL (Wt, dt)
(3)
Next, we establish the global convergence of GDA for an overparameterized model. Note that a
global saddle point (W*, d*) is defined as
L(W*, d) ≤ L(W*, d*) ≤ L(W, d*)
for all feasible W and d. If these inequalities hold in a local neighborhood, (W*, d*) is called a
local saddle point.
Theorem 2.1 Let xι, x2,..., Xn ∈ Rm be n training data with their mean defined as X :=
n En=I Xi. Consider the GAN model with a linear discriminator oftheform D(X) = dτX parame-
terized by d ∈ Rm and a one hidden layer neural network generator of the form G(z) = Vφ(Wz)
parameterized by W ∈ Rk×d with V ∈ Rm×k a fixed matrix generated at random with
i.i.d. N(0, σv2) entries. Also assume the input data to the generator {zi}in=1 are generated i.i.d. ac-
cording to 〜N(0, σ^Id). Furthermore, assume the generator weights at initialization Wo ∈ Rk×d
4
Published as a conference paper at ICLR 2021
are generated i.i.d. according to N(0, σw2 ). Furthermore, assume the standard deviations above
obey σvσwσz ≥ ∣∣xk'2 /(md5 logd2). Then, as long as
k ≥ C ∙ md4 log (d)3
with C a fixed constant, running GDA updates per equation 3 starting from the random W0 above
and do = 02 with step-sizes obeying 0 < μ ≤ 1 and η = η- μ-1------, with η ≤ 1, satisfies
324∙k∙ d+∙σ2∙σ2
n	vz
1n
—VR VReLU(WτZi) - X	≤ 5 (1 - 10-5 ∙ ημ)τ
n i=1	`2
1n
一	VReLU(WoZi) — X
n
i=1	`2
(4)
This holds with probability at least 1 — (n + 5) e-ɪmoθ — 5k ∙ e-c1∙n — (2k + 2) e-2⅛ —
ne-c2∙md3 log(d)2 where ci, c2 are fixed numerical constants.
To better understand the implications of the above theorem, note that the objective of equation 2 can
be simplified by solving the inner maximization in a closed form so that the min-max problem in
equation 2 is equivalent to the following single minimization problem:
2
min L(W):= ɪ
1n
—£V ReLU (Wzi) — X
n i=1
(5)
`2
which has a global optimum of zero. As a result equation 4 in Theorem 2.1 guarantees that run-
ning simultaneous GDA updates achieves the global optimum. This holds as long as the generator
network is sufficiently overparameterized in the sense that the number of hidden nodes is polynomi-
ally large in its output dimension m and input dimension d. Interestingly, the rate of convergence
guaranteed by this result is geometric, guaranteeing fast GDA convergence to the global optima.
To the extent of our knowledge, this is the first result that establishes the global convergence of
simultaneous GDA for an overparameterized GAN model.
While the result proved above shows the global convergence of GDA for a GAN with 1-hidden
layer generator and a linear discriminator, for a general GAN model, local saddle points may not
even exist and GDA may converge to approximate local saddle points (Berard et al., 2020; Farnia
& Ozdaglar, 2020). For a general min-max problem, (Daskalakis et al., 2020) has recently shown
that approximate local saddle points exist under some general conditions on the lipschitzness of the
objective function. Understanding GDA dynamics for a general GAN remains an important open
problem. Our result in Theorem 2.1 is a first and important step towards that.
We acknowledge that the considered GAN formulation of equation 2 is very simpler than GANs
used in practice. Specially, since the discriminator is linear, this GAN can be viewed as a moment-
matching GAN (Li et al., 2017) pushing first moments of input and generative distributions towards
each other. Alternatively, this GAN formulation can be viewed as one instance of the Sliced Wasser-
stein GAN (Deshpande et al., 2018). Although the maximization on discriminator’s parameters is
concave, the minimization over the generator’s parameters is still non-convex due to the use of a
neural-net generator. Thus, the overall optimization problem is a non-trivial non-convex concave
min-max problem. From that perspective, our result in Theorem 2.1 partially explains the role of
model overparameterization in GDA’s convergence for GANs.
Given the closed form equation 5, one may wonder what would happen if we run gradient de-
scent on this minimization objective directly. That is running gradient descent updates of the form
Wτ +1 = Wτ — ηVL(Wr) with L(W) given by equation 5. This is equivalent to GDA but instead
of running one gradient ascent iteration for the maximization iteration we run infinitely many. Inter-
estingly, in some successful GAN implementations (Gulrajani et al., 2017), often more updates on
the discriminator’s parameters are run per generator’s updates. This is the subject of the next result.
Theorem 2.2 Consider the setup of Theorem 2.1. Then as long as
k ≥ C ∙ md4 log (d)3
2The zero initialization of d is merely done for simplicity. A similar result can be derived for an arbitrary
initialization of the discriminator’s parameters with minor modifications. See Theorem 2.3 for such a result.
5
Published as a conference paper at ICLR 2021
Figure 2: Convergence plot a GAN model with linear discriminator and 1-hidden layer generator
as the hidden dimension (k) increases. Final mse is the mse loss between true data mean and the
mean of generated distribution. Over-parameterized models show improved convergence
with C a fixed numerical constant, running GD updates of the form Wτ+1 = WT - η VL(Wτ)
the loss given in equation 5 with step-size η = -n--, with η ≤ 1, satisfies
243k∙ d+产∙σV ∙σ2
on
1n
—£V ReLU (WT Zi) - X ≤(1 - 4 X 10-6 ∙ η)τ
n
i=1	`2
1n
-VVReLU (WoZi) - X
n
i=1	`2
(6)
This holds with probability at least 1 — (n + 5) e- ImO — 5k ∙ e-c1 'n — (2k + 2) e- 2d6 —
ne-c2∙md3 log ⑷2 with c1, c2 fixed numerical constants.
This theorem states that if we solve the max part of equation 2 in closed form and run GD on the
loss function per equation 5 with enough overparameterization, the loss will decrease at a geometric
rate to zero. This result holds again when the model is sufficiently overparameterized. The proof of
Theorem 2.2 relies on a result from (Oymak & Soltanolkotabi, 2020), which was developed in the
framework of supervised learning. Also note that the amount of overparameterization required in
both Theorems 2.1 and 2.2 is the same.
2.4 Can the analysis be extended to more general GANs?
In the previous section, we focused on the implications of our results for one-hidden layer generator
and linear discriminator. However, as it will become clear in the proofs, our theoretical results are
based on analyzing the convergence behavior of GDA on a more general min-max problem of the
form
kdk`2
miRp mxm h(θ,d):= hd,f (θ)- yi- T,	⑺
where f : Rp → Rm denotes a general nonlinear mapping.
Theorem 2.3 (Informal version of Theorem A.4) Consider a general nonlinear mapping f :
Rp → Rm with the singular values of its Jacobian mapping around initialization obeying certain
assumptions (most notably σmin(J (θ0)) ≥ α). Then, running GDA iterations of the form
fdt+1 = dt + μVd h(θt ,dt)
[θt+ι = θt- ηj h(θt, dt)
with sufficiently small step sizes η and μ obeys
kf(θt)- yk'2 ≤Y (1- η2-) qkf(θo)- yk22 + kdok22.
(8)
Note that similar to the previous sections one can solve the maximization problem in equation 7 in
closed form so that equation 7 is equivalent to the following minimization problem
m⅛L(θ) :=1 kf ⑹-yk22，
θ ∈Rp	2
(9)
6
Published as a conference paper at ICLR 2021
with global optima equal to zero. Theorem 2.3 ensures that GDA converges with a fast geometric rate
to this global optima. This holds as soon as the model f(θ) is sufficiently overparameterized which is
quantitatively captured via the minimum singular value assumption on the Jacobian at initialization
(σmin(J (θ0)) ≥ α which can only hold when m ≤ p). This general result can thus be used to
provide theoretical guarantees for a much more general class of generators and discriminators. To
be more specific, consider a deep GAN model where the generator Gθ is a deep neural network with
parameters θ and the discriminator is a deep random feature model of the form Dd(x) = dTψ(x)
parameterized with d and ψ : Rd → Rm a deep neural network with random weights. Then the min-
max training optimization problem equation 1 with a regularizer R(d) = - kdk`2 /2 is a special
instance of equation 7 with
nn
M= n X ψ(Gθ(Zi)) and y := n X ψ3)
i=1
i=1
Therefore, the above result can in principle be used to rigorously analyze global convergence of
GDA for an overparameterized GAN problem with a deep generator and a deep random feature
discriminator model. However, characterizing the precise amount of overparameterization required
for such a result to hold requires a precise analysis of the minimum singular value of the Jacobian
of f(θ) at initialization as well as other singular value related conditions stated in Theorem A.4. We
defer such a precise analysis to future works.
Numerical Validations: Next, we numerically
study the convergence of GAN model considered
in Theorems 2.1 and 2.2 where the discriminator is
a linear network while the generator is a one hid-
den layer neural net. In our experiments, we gen-
erate xi ’s from an m-dimension Gaussian distribu-
tion With mean μ and an identity covariance matrix.
The mean vector μ is randomly generated. We train
tWo variants of GAN models using (1) GDA (as con-
sidered in Thm 2.1) and (2) GD on generator While
solving the discriminator to optimality (as consid-
ered in Thm 2.2).
In Fig. 2, We plot the converged loss values of GAN
models trained using both techniques (1) and (2) as
the hidden dimension k of the generator is varied.
Hidden dimension k
Figure 3: MLP Overparameterization on
The MSE loss betWeen the true data mean and the
data mean of generated samples is used as our eval- MNIST.
uation metric. As this MSE loss approaches 0, the
model converges to the global saddle point. We ob-
serve that overparameterized GAN models shoW improved convergence behavior than the narroWer
models. Additionally, the MSE loss converges to 0 for larger values of k Which shoWs that With
sufficient overparamterization, GDA converges to a global saddle point.
3	Experiments
In this section, We demonstrate benefits of overparamterization in large GAN models. In particular,
We train GANs on tWo benchmark datasets: CIFAR-10 (32 × 32 resolution) and Celeb-A (64 × 64
resolution). We use tWo commonly used GAN architectures: DCGAN and Resnet-based GAN. For
both of these architectures, We train several models, each With a different number of filters in each
layer, denoted by k . For simplicity, We refer to k as the hidden dimension. Appendix Fig. 8 illustrates
the architectures used in our experiments. NetWorks With large k are more overparameterized.
We use the same value of k for both generator and discriminator netWorks. This is in line With
the design choice made in most recent GAN models (Radford et al., 2016; Brock et al., 2019),
Where the size of generator and discriminator models are roughly maintained the same. We train
each model till convergence and evaluate the performance of converged models using FID scores.
FID scores measure the Frechet distance betWeen feature distributions of real and generated data
7
Published as a conference paper at ICLR 2021
(a) DCGAN, CIFAR
(b) DCGAN, CelebA
(d) Resnet, CIFAR
(e) Resnet, CelebA
(c) DCGAN
(f) Resnet
目Ii
F≡s英
Figure 4: Overparamterization Results: We plot the FID scores (lower, better) of DCGAN and
Resnet DCGAN as the hidden dimension k is varied. Results on CIFAR-10 and Celeb-A are shown
on the plots on the left and right panels, respectively. Overparameterization gives better FID scores.
(a) CIFAR
(b) CelebA
Figure 5: DCGAN Training Results: We plot the FID scores across training iterations of DCGAN
on CIFAR-10 and Celeb-A for different values of hidden dimension k. Remarkably, we observe that
over-parameterization improves the rate of convergence of GDA and its stability in training.
distributions (Heusel et al., 2017). A small FID score indicates high-quality synthesized samples.
Each experiment is conducted for 5 runs, and mean and the variance of FID scores are reported.
Overparameterization yields better generative models: In Fig. 4, we show the plot of FID scores
as the hidden dimension (k) is varied for DCGAN and Resnet GAN models. We observe a clear
trend where the FID scores are high (i.e. poor) for small values of k, while they improve as models
become more overparameterized. Also, the FID scores saturate beyond k = 64 for DCGAN models,
and k = 128 for Resnet GAN models. Interestingly, these are the standard values used in the existing
model architecures (Radford et al., 2016; Gulrajani et al., 2017).This trend is also consistent on MLP
GANs trained on MNIST dataset (Fig. 3). We however notice that FID score in MLP GANs increase
marginally as k increases from 1024 to 2048. This is potentially due to an increased generalization
gap in this regime where it offsets potential benefits of over-parameterization
Overparameterization leads to improved convergence of GDA: In Fig. 5, we show the plot of
FID scores over training iterations for different values for k. We observe that models with larger
8
Published as a conference paper at ICLR 2021
(a) DCGAN	(b) Resnet
Figure 6: Generalization in GANs: We plot the NND scores as the hidden dimension k is varied
for DCGAN (shown in (a)) and Resnet (shown in (b)) models.
values of k converge faster and demonstrate a more stable behavior. This agrees with our theoretical
results that overparameterized models have a fast rate of convergence.
Generalization gap in GANs: To study the generalization gap, we compute the FID scores by using
(1) the training-set of real data, which we call FID train, and (2) a held-out validation set of real
data, which we call FID test. In Fig. 4, a plot of FID train (in blue) and FID test (in green) are shown
as the hidden dimension k is varied. We observe that FID test values are consistently higher than the
the FID train values. Their gap does not increase with increasing overparameterization.
However, as explained in (Gulrajani et al., 2019), the FID score has the issue of assigning low values
to memorized samples. To alleviate the issue, (Gulrajani et al., 2019; Arora et al., 2017) proposed
Neural Net Divergence (NND) to measure generalization in GANs. In Fig. 6, we plot NND scores
by varying the hidden dimensions in DCGAN and Resnet GAN trained on CIFAR-10 dataset. We
observe that increasing the value of k decreases the NND score. Interestingly, the NND score of
memorized samples are higher than most of the GAN models. This indicates that overparameterized
models have not been memorizing training samples and produce better generative models.
4	Conclusion
In this paper, we perform a systematic study of the importance of overparameterization in training
GANs. We first analyze a GAN model with one-hidden layer generator and a linear discriminator
optimized using Gradient Descent Ascent (GDA). Under this setup, we prove that with sufficient
overparameterization, GDA converges to a global saddle point. Additionally, our result demonstrate
that overparameterized models have a fast rate of convergence. We then validate our theoretical
findings through extensive experiments on DCGAN and Resnet models trained on CIFAR-10 and
Celeb-A datasets. We observe overparameterized models to perform well both in terms of the rate
of convergence and the quality of generated samples.
5	Acknowledgement
M. Sajedi would like to thank Sarah Dean for introducing (Rugh, 1996). This project was sup-
ported in part by NSF CAREER AWARD 1942230, HR00112090132, HR001119S0026, NIST
60NANB20D134 and Simons Fellowship on “Foundations of Deep Learning.” M. Soltanolkotabi is
supported by the Packard Fellowship in Science and Engineering, a Sloan Research Fellowship in
Mathematics, an NSF-CAREER under award 1846369, the Air Force Office of Scientific Research
Young Investigator Program (AFOSR-YIP) under award FA9550-18-1-0078, DARPA Learning with
Less Labels (LwLL) and FastNICS programs, and NSF-CIF awards 1813877 and 2008443.
9
Published as a conference paper at ICLR 2021
References
Leonard Adolphs, Hadi Daneshmand, Aurelien Lucchi, and Thomas Hofmann. Local saddle point
optimization: A curvature exploitation approach. In The 22nd International Conference on Arti-
ficial Intelligence and Statistics,pp. 486-495, 2019.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the
36th International Conference on Machine Learning, ICML 2019, Long Beach, California, USA,
June 9-15, 2019, volume 97 of Proceedings of Machine Learning Research, pp. 242-252, 2019.
Wangpeng An, Haoqian Wang, Qingyun Sun, Jun Xu, Qionghai Dai, and Lei Zhang. Apid controller
approach for stochastic optimization of deep networks. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 8522-8531, 2018.
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney,
Australia, Aug 6-11, 2017, Proceedings of Machine Learning Research, pp. 214-223, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium
in generative adversarial nets (gans). arXiv preprint arXiv:1703.00573, 2017.
Kenneth J Arrow, Leonid Hurwicz, and Hirofumi Uzawa. Studies in linear and non-linear program-
ming. Cambridge Univ. Press, 1958.
Hugo Berard, Gauthier Gidel, Amjad Almahairi, Pascal Vincent, and Simon Lacoste-
Julien. A closer look at the optimization landscapes of generative adversarial net-
works. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=HJeVnCEKwH.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity
natural image synthesis. In International Conference on Learning Representations, 2019.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
In Advances in Neural Information Processing Systems, pp. 2937-2947, 2019.
Aidan Clark, Jeff Donahue, and Karen Simonyan. Adversarial video generation on complex datasets.
arXiv preprint arXiv:1907.06571, 2019. URL https://arxiv.org/abs/1907.06571.
Constantinos Daskalakis, Stratis Skoulakis, and Manolis Zampetakis. The complexity
of constrained min-max optimization. arXiv preprint arXiv:2009.09623, 2020. URL
https://arxiv.org/abs/2009.09623.
Ishan Deshpande, Ziyu Zhang, and Alexander G Schwing. Generative modeling using the sliced
wasserstein distance. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 3483-3491, 2018.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. In 5th
International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
2017, Conference Track Proceedings, 2017.
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In 7th International Conference on Learning Representa-
tions, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, Conference Track Proceedings, 2019.
Farzan Farnia and Asuman Ozdaglar. Gans may have no nash equilibria. arXiv preprint
arXiv:2002.09124, 2020.
Soheil Feizi, Farzan Farnia, Tony Ginart, and David Tse. Understanding GANs: the LQG setting.
arXiv preprint arXiv:1710.10793, 2018. URL https://arxiv.org/abs/1710.10793.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling,
C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Pro-
cessing Systems 27, pp. 2672-2680, 2014.
10
Published as a conference paper at ICLR 2021
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of wasserstein gans. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30,
pp. 5767-5777, 2017.
Ishaan Gulrajani, Colin Raffel, and Luke Metz. Towards GAN benchmarks which require
generalization. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=HkxKH2AcFm.
Shaul Gutman. Uncertain dynamical systems-a lyapunov min-max approach. IEEE Transactions
on Automatic Control, 24(3):437-443, 1979.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in neural information processing systems 30, pp. 6626-6637, 2017.
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR
2019, Long Beach, CA, USA, June 16-20, 2019, pp. 4401-4410. Computer Vision Foundation /
IEEE, 2019.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and Yann
LeCun (eds.), 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB,
Canada, April 14-16, 2014, Conference Track Proceedings, 2014.
Michel Ledoux. The concentration of measure phenomenon. Number 89 in Mathematical Surveys
and Monographs. American Mathematical Soc., 2001.
Qi Lei, Jason D Lee, Alexandros G Dimakis, and Constantinos Daskalakis.	Sgd
learns one-layer networks in wgans.	arXiv preprint arXiv:1910.07030, 2019.	URL
https://arxiv.org/abs/1910.07030.
Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabas Poczos. Mmd gan: To-
wards deeper understanding of moment matching network. In I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information
Processing Systems 30, pp. 2203-2213, 2017.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of GANs. In Advances in
Neural Information Processing Systems 30, pp. 1823-1833, 2017.
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods
for gans do actually converge? arXiv preprint arXiv:1801.04406, 2018. URL
https://arxiv.org/abs/1801.04406.
Vaishnavh Nagarajan and J Zico Kolter. Gradient descent GAN optimization is locally stable. In
Advances in Neural Information Processing Systems 30, pp. 5591-5600, 2017.
Samet Oymak and Mahdi Soltanolkotabi. Overparameterized nonlinear learning: Gradient descent
takes the shortest path? In Proceedings of the 36th International Conference on Machine Learn-
ing, ICML 2019, Long Beach, California, USA, June 9-15, 2019, Proceedings of Machine Learn-
ing Research, pp. 4951-4960, 2019.
Samet Oymak and Mahdi Soltanolkotabi. Towards moderate overparameterization: global con-
vergence guarantees for training shallow neural networks. IEEE Journal on Selected Areas in
Information Theory, 2020.
Samet Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi. Generalization guaran-
tees for neural networks via harnessing the low-rank structure of the jacobian. arXiv preprint
arXiv:1906.05392, 2019. URL https://arxiv.org/abs/1906.05392.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. In Yoshua Bengio and Yann LeCun (eds.), 4th
International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May
2-4, 2016, Conference Track Proceedings, 2016.
11
Published as a conference paper at ICLR 2021
Wilson J Rugh. Linear system theory. Prentice-Hall, Inc., 1996.
Mahdi Soltanolkotabi. Structured signal recovery from quadratic measurements: Breaking sample
complexity barriers via nonconvex optimization. IEEE Transactions on Information Theory, 65
(4):2374-2400, 2019.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization
landscape of over-parameterized shallow neural networks. IEEE Transactions on Information
Theory, 65(2):742-769, 2018.
Dave Van Veen, Ajil Jalal, Mahdi Soltanolkotabi, Eric Price, Sriram Vishwanath, and Alexandros G
Dimakis. Compressed sensing with deep image prior and learned regularization. arXiv preprint
arXiv:1806.06438, 2018. URL https://arxiv.org/abs/1806.06438.
John Von Neumann and Oskar Morgenstern. Theory of games and economic behavior. Princeton
university press, 1953.
Kun Xu, Chongxuan Li, Huanshu Wei, Jun Zhu, and Bo Zhang. Understanding and stabilizing gans’
training dynamics with control theory. arXiv preprint arXiv:1909.13188, 2019.
Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural
networks. In Advances in Neural Information Processing Systems 32, pp. 2055-2064, 2019.
12
Published as a conference paper at ICLR 2021
Appendix
A Proofs
In this section, we prove Theorems 2.1 and 2.2. First, we provide some notations we use throughout
the remainder of the paper in Section A.1. Before proving these specialized results for one hidden
layer generators and linear discriminators (Theorems 2.1 and 2.2), we state and prove a more general
result (formal version of Theorem 2.3) on the convergence of GDA on a general class of min-max
problems in Section A.3. Then we state afew preliminary calculations in Section A.4. Next, we state
some key lemmas in Section A.5 and defer their proofs to Appendix B. Finally, we prove Theorems
2.1 and 2.2 in Sections A.6 and A.7, respectively.
A. 1 Notation
We will use C, c, c1, etc. to denote positive absolute constants, whose value may change throughout
the paper and from line to line. We use φ (z) = ReLU (z) = max (0, z) and its (generalized)
derivative φ0 (z) = 1{z≥0} with 1 being the indicator function. σmin (X) and σmax (X) = kXk
denote the minimum and maximum singular values of matrix X . For two arbitrary matrices A and
B, A 0 B denotes their kroneCker product. The spectral radius of a matrix A ∈ Cn×n is defined as
P (A) = max {∣λι∣,..., ∣λn∣}, where λ∕s are the eigenvalues of A. Throughout the proof We shall
assume φ := ReLU to avoid unnecessarily long expressions.
A.2 Proof sketch of the main results
In this section, we provide a brief overview of our proofs. We focus on the main result in this
manuscript, which is about the convergence of GDA (Theorem 2.1). To do this we study the converge
of GDA on the more general min-max problem of the form (see Theorem A.4 for a formal statement)
kdk`2
θ∈innmRmhGd) := hd,f ⑹—yi- 丁2.
In this case the GDA iterates take the form
ʃdt+1 = (I- μ) dt + μ (f (θt) - y)
θt+1 = θt - ηJT (θt) dt	.
Our proof for global convergence of GDA on this min-max loss consists of the following steps.
(10)
(11)
Step 1: Recasting the GDA updates as a linear time-varying system
In the first step we carry out a series of algebraic manipulations to recast the GDA updates (equa-
tion 11) into the following form
rt+1	A rt
dt+1 = At dt ,
where rt = f (θt ) - y denotes the residuum and At denotes a properly defined transition matrix.
Step 2: Approximation by a linear time-invariant system
Next, to analyze the behavior of the time-varying dynamical system above we approximate it by the
following time-invariant linear dynamical system
_1	Γ _	_E ，一、	_ , 一、-1	「~ ■
ret+1 = I -ηJT (θ0)J (θ0) ret
dt+1 μI (I ― μ) I	dt
where θ0 denotes the initialization. The validity of this approximation is ensured by our assumptions
on the Jacobian of the function f, which, among others, guarantee that it does not change too much
in a sufficiently large neighborhood around the initialization and that the smallest singular value of
J (θ0 ) is bounded from below.
Step 3: Analysis of time-invariant linear dynamical system
To analyze the time-invariant dynamical system above, we utilize and refine intricate arguments
13
Published as a conference paper at ICLR 2021
from the control theory literature involving the spectral radius of the fixed transition matrix above to
obtain
Step 4: Completing the proof via a perturbation argument
Γ -1	Γ~ 1
rt	rt
In the last step of our proof we show that the two sequences dt and de will remain close to
each other. This is based on a novel perturbation argument. The latter combined with Step 3 allows
us to conclude
which finishes the global convergence of GDA on equation 10 and hence the proof of Theorem A.4.
In order to deduce Theorem 2.1 from Theorem A.4, we need to check that the Jacobian at the
initialization is bounded from below at the origin and that it does not change too quickly in a large
enough neighborhood. In order to prove that we will leverage recent ideas from the deep learning
theory literature revolving around the neural tangent kernel. This allows us to guarantee that this
conditions are indeed met, if the neural network is sufficiently wide and the initialization is chosen
large enough.
The second main result of this manuscript, Theorem 2.2, can be deduced more directly from re-
cent results on overparameterized learning (see Oymak & Soltanolkotabi (2020)). Hence, we have
deferred its proof to Section A.7.
A.3 Analysis of GDA: A control theory perspective
In this section we will focus on solving a general min-max optimization problem of the form
.......................................................∣∣d∣∣2,
min max h(θ, d) := (d,f (θ) - y〉-	2,
θ∈Rnd∈Rm	2
(12)
where f : Rn → Rm is a general nonlinear mapping. In particular, we focus on analyzing the
convergence behavior of Gradient Descent/Ascent (GDA) on the above loss, starting from initial
estimates θ0 and d0 . In this case the GDA updates take the following form
d dt+1 = (I- μ) dt + μ (f (θt) - y)
θt+1 = θt - ηJT (θt)dt
We note that solving the inner maximization problem in equation 12 would yield
min kf Ilf (θ) 一 y∣∣2 .
θ∈Rn 2	y' ''2
(13)
(14)
In this section, our goal is to show that when running the GDA updates of equation 13, the norm of
the residual vector defined as rt := f (θt) - y goes to zero and hence we reach a global optimum
of equation 14 (and in turn equation 12).
Our proof will build on ideas from control theory and dynamical systems literature. For that, we are
first going to rewrite the equations 13 in a more convenient way. We define the average Jacobian
along the path connecting two points x,y ∈ Rn as
J (y, x) = Z
0
J (x + α (y - x)) dα,
where J (θ) ∈ Rm×n is the Jacobian associated with the nonlinear mapping f . Next, from the
fundamental theorem of calculus it follows that
rt+1 = f (θt+ι) - y = f (θt - ηJTdt) - y
= f (θt ) - ηJt+1,tJtT dt - y
= rt - ηJt+1,tJtT dt,	(15)
14
Published as a conference paper at ICLR 2021
where we used the shorthands Jt := J (θt) and Jt+1,t := J (θt+1, θt) for exposition purposes.
Next, we combine the updates rt and dt into a state vector of the form zt := drt ∈ R2m. Using
this notation the relationship between the state vectors from one iteration to the next takes the form
zt+ι=U -η⅛jtT]zt, t ≥0,	(ι6)
、--------{z---------}
=:At
which resembles a time-varying linear dynamical system with transition matrix At . Now note that
to show convergence of rt to zero it suffices to show convergence of zt to zero. To do this we
utilize the following notion of uniform exponential stability, which will be crucial in analyzing the
solutions of equation 16. (See Rugh (1996) for a comprehensive overview on stability notions in
discrete state equations.)
Definition 1 A linear state equation of the form zt+1 = Atzt is called uniformly exponentially
stable if for every t ≥ 0 we have ∣∣ztk'2 ≤ γλt ∣∣z0k'2, where Y ≥ 1 is a finite Constant and
0 ≤ λ < 1.	2	2
Using the above definition to show the convergence of the state vector zt to zero at a geometric
rate it suffices to show the state equation 16 is exponentially stable.3 For that, we are first going
to analyze a state equation which results from linearizing the nonlinear function f (θ) around the
initialization θ0. In the next step, we are going to show that the behavior of these two problems are
similar, provided we stay close to initialization (which we are also going to prove). Specifically, we
consider the linearized problem
min max hlin θe,de := de, f (θ0) + J0
θe∈Rn de∈Rm
-yE — 色
(17)
We first analyze GDA on this linearized problem starting from the same initialization as the original
problem, i.e. θ0 = θ0 and d0 =d0. The gradient descent update for θt takes the form
θet+1 = θet - ηJ0T det,
and the gradient ascent update for dt takes the form
dt+ι = dt + μ ff (θo) + JO (θt - θo) - y - dt)
/1	∖ V ~
=(I ― μ) dt + μrt,
(18)
(19)
where we used the linear residual defined as ret = f (θ0)+J0
from one iterate to the next can be written as follows
- y . Moreover, the residual
ret+1 =f(θ0)+J0 θet+1 -θ0 -y
=f(θ0)+J0 θet - ηJ0T det - θ0 -y	(20)
= ret - ηJ0J0T det .
rt
Again, we define a new vector zet =	∈ R2m and by putting together equations 19 and 20 we
dt
arrive at
et+1 = [μI!	-IJJT ] et = Aet, t ≥ 0，	(21)
3We note that technically the dynamical system equation 16 is not linear. However, we still use exponential
stability with some abuse of notation to refer to the property that kzt k` ≤ γλt kz0 k` holds. As we will see
in the forth-coming paragraphs, our formal analysis is via a novel perturbation analysis of a linear dynamical
system and therefore keeping this terminology is justified.
15
Published as a conference paper at ICLR 2021
which is of the form of a linear time-invariant state equation. As a first step in our proof, we are
going to show that the linearized state equations are uniformly exponentially stable. First, recall
the following well-known lemma, which characterizes uniform exponential stability in terms of the
eigenvalues of A.
Lemma A.1 (Rugh, 1996, Theorem 22.11) A linear state equation of the form zet+1 = Azet with A
a fixed matrix is uniformly exponentially stable if and only if all eigenvalues of A have magnitudes
strictly less than one, i.e. ρ (A) < 1. In this case, it holds for all t ≥ 0 and all z that
kAtzk≤γρ(A)tkzk,
where γ ≥ 1 is an absolute constant, which only depends on A.
In the next lemma, We prove that under suitable assumptions on J and the step sizes μ and η the
state equations 21 indeed fulfill this condition.
Lemma A.2 Assume that α ≤ σmin (J0) ≤ σmax (J0) ≤ β and consider the matrix A
I
μI
(JJμJl]. Suppose that μ ≥ 4β2. Then it holds that P (A) ≤ 1 一 ηɑ2.
Proof Suppose that λ is an eigenvalue of A. Hence, there is an eigenvector [x, y]T 6= 0 such that
% -J [y]=λ [x]
holds. By a direct calculation We observe that this yields the equation
ηJoJ0TX = (一(1 一 X +(1 — λ)) x.
In particular, x must be an eigenvector of J0J0T. Denoting the corresponding eigenvalue With s, We
obtain the identity
μ (1 λ) + ηs 0.
Hence, We must have
{1 μ	I μ2	1 μ	I μ2
1 一 2 + ʌ/ ɪ 一 μηs; 1 一 2 -V ɪ 一 μηs .
Note that the square root is indeed well-defined, since
μ2
ɪ 一 μηs ≥ μηβ2 - μns ≥ 0,
where in the first inequality we used the assumption μ ≥ 4β2 and in the second line we used that
s ≤ β2, Which is a consequence of our assumption on the singular values of J0. Hence, it folloWs
by the reverse triangle inequality that
囚一
μηs< 2 - ηs ≤ 2 - ηα2,
where the second inequality is valid as μ 一 ηs ≥ 0 is implied by μ ≥ 2nβ2 > ηs. In the last
inequality we used the fact that α2 ≤ s, which is a consequence of our assumption on the singular
values of Jo. By rearranging terms, we obtain that ∣λ∣ < 1 一 ηα2. Since λ was an arbitrary
eigenvalue of A, the result follows.
Since the last lemma shows that under suitable conditions it holds that P (A) < 1, Lemma A.3
yields uniform exponential stability of our state equations. However, this will not be sufficient for
our purposes. The reason is that Lemma A.3 does not specify the constant γ and that in order to deal
with the time-varying dynamical system we will need a precise estimate. The next lemma shows
that for the state equations 21 we have, under suitable assumptions, γ ≤ 5.
16
Published as a conference paper at ICLR 2021
Lemma A.3 Consider the linear, time invariant system of equations
et+ι=]μi (TJJIet -
t ≥ 0.
Furthermore, assume that α ≤ σmin (J0) ≤ σmax (J0) ≤ β and suppose that the condition μ ≥ 8β2
is satisfied. Then there is a constant γ ≤ 5 such that for all t ≥ 0 it holds that
ketll'2 ≤ Y (1 - ηα2)t ke0k'2.
Proof Denote the SVD decomposition ofJ0 by WΣV T and note that
I -ηJ0J0T
μI (1 — μ) I
W0 I
0 W	μI
—η∑∑T] ΓwT
(1 — μ) I 0
0
WT.
This means we can write
I
μI
-ηJoJ0T] _ Γw
(1 — μ) I =	0
C1
0
0
PT ΓWT	0
P [0	WT
Cm
where P is a permutation matrix and the matrices Ci are of the form Ci
-ησ2
(1 - μ)
, for
1 ≤ i ≤ m, where the σ∕s denote the singular values of J Using this decomposition We can
deduce
0
W
P
1
μ
ketk'2 = 1— ≤ Mtllke0k'2 = (1m≤m IICtII) kzok'2.
Now suppose that ViDiVi-1 is the eigenvalue decomposition of Ci, where the columns of Vi
contain the eigenvectors and Di is a diagonal matrix consisting of the eigenvalues. (Note that it
follows from our assumptions on μ and η that the matrix Ci is diagonalizable.) We have
IlCtII = IlViDtViTIl ≤ kVik IlDtII ∣∣vτ1∣∣ = Ki ∙ P(Ciy,
where we defined κi := kVik IVi-1 I. From Lemma A.2 we know that the assumptionη ≥4β2
results in ρ (A) ≤ 1 — ηα2. Therefore, defining γ := max κi and noting ρ (A) = max ρ (Ci),
1≤i≤m	1≤i≤m
we obtain that
kZtk'2 ≤ (1m≤m IlCtIl) kZ0k'2 ≤ Y (1 -ηα2)t kZ0k'2.
In order to finish the proof we need to show that γ ≤ 5. For that, note that calculating the eigenvec-
tors of Ci directly reveals that we can represent this matrix as
M-4 T
2
1
1- f4η^
2
1
Since kMk = Jλmax (ViViT) and I ViTII = Jλmin (Vi匕T), We calculate ViViT, which yields
ViViT
1 — 2ησ2
μ
1
1
2
17
Published as a conference paper at ICLR 2021
This representation allows us to directly calculate the two eigenvalues of ViViT , which shows that
6
≤ —,
2,1-4 等
< 5,
22
where the last inequality holds because of ημi- ≤ ηβ- ≤ 1. Since Y = ^max κi, this finishes the
proof.
Now that we have shown that the linearized iterates converge to the global optima we turn our
attention to showing that the nonlinear iterates 16 are close to its linear counterpart 21. For that, we
make the following assumptions.
Assumption 1: The singular values of the Jacobian at initialization are bounded from below
σmin (J (θ0)) ≥ α
for some positive constants α and β .
Assumption 2: In a neighborhood with radius R around the initialization, the Jacobian map-
ping associated with f obeys
kJ(θ)k≤β
for all θ ∈ Br (θo), where BR (θo):= {θ ∈ Rp : ∣∣θ — θo∣h ≤ R}.
Assumption 3: In a neighborhood with radius R around the initialization, the spectral norm
of the Jacobian varies no more than in the sense that
kJ(θ)-J(θ0)k≤
for all θ ∈ BR (θ0).
With these assumptions in place, we are ready to state the main theorem.
Theorem A.4 Consider the GDA updates for the min-max optimization problem 12
dt+1 _ dt + μVdh(θt, dt)
θt+J = [Θt - ηVθh(θt, dt)
and consider the GDA updates of the linearized problem 21
dt+1 - dt + μVdhiin(θt, dt)
=
θt+1	θt - ηVθhlin(θt, dt)
(22)
(23)
, where rt := f (θt) - y and ret = f (θ0) + J0 θet - θ0 - y
rt
Set zt := dt and zt :
denote the residuals.
Assume that the SteP sizes of the gradient descent ascent updates satisfy η ≥ 8β2 as well as 0 <
18
Published as a conference paper at ICLR 2021
μ ≤ L Moreover, assume that the assumptions 1-3 hold for the Jacobian J (θ) of f (θ) around the
initialization θ0 ∈ Rn with parameters α, β, , and
0
z0
+
`2
18eβ2γ2
α4
kz0k'2 ,
(24)
which satisfy 4γβe ≤ α2. Here, 1 ≤ γ ≤ 5 is a constant, which only depends on μ, η, and J0.
By J we denote the pseudo-inverse of the Jacobian at initialization Jo. Then, assuming the same
initialization θ0 = θ0, d0 = d0 (and, hence, z0 = ze0), the following holds for all iterations t ≥ 0.
•	kzt k` converges to 0 with a geometric rate, i.e.
kztk'2 ≤ Y(1- ηα21Eh.
(25)
The trajectories of zt and zet stay close to each other and converge to the same limit, i.e.
l∣Zt - etk'2 ≤ 2ηγ2βe ∙ t
kz0k'2
(26)
≤
e(15lnDO2 kz0k'2 .
•	The parameters of the original and linearized problems stay close to each other, i.e.
g %2≤ 9eβ2≠ kzok'2 ,	(27)
•	The parameters of the original problem stay close to the initialization, i.e.
kθt - θ0k'2 ≤ TP	(28)
22
Theorem A.4 will be the main ingredient in the proof of Theorem 2.1. However, as discussed
in Section 2.4 we believe that this meta theorem can be used to deal with a much richer class of
generators and discriminators.
A.3.1 Proof of Theorem A.4
We will prove the statements in the theorem by induction. The base case for τ = 0 is trivial. Now
assume that the equations equation 25 to equation 28 hold for τ = 0, . . . , t - 1. Our goal is to show
that they hold for iteration t as well.
Part I: First, we are going to show that θt ∈ BR (θ0). Note that by the triangle inequality and the
induction assumption we have that
kθt	- θ0k'2 ≤	kθt	-	θt-lll'2	+	llθt-1 - θ0k'2
≤	kθt	-	θt-ιk'2	+	2.
Hence, in order to prove the claim it remains to show that ∣θt 一 θt-1 k七 ≤ RR. For that, We compute
η kθt -θtτk'2=∣∣J T (θt-ι) dt-ι∣∣'2
≤ JT (θt-1) det-1	+ JT (θt-1)	dt-1 - det-1
≤ ∣∣J0T det-1 ∣∣	+ lJ	(θt-1)	-	J0l	∣∣det-1∣∣	+ ∣∣JT (θt-1)∣∣	∣∣dt-1 -	det-1∣∣
(≤) ”I]]	JT Hie + e ∙ Y kz0k'2 + e (lXe⅛ α2
(≤, γβ2IIJ J0⅛3⅞f kz0k'2,
lz0 l`2
19
Published as a conference paper at ICLR 2021
where γ ≤ 5 is a constant. Let us verify the last two inequalities. Inequality (ii) holds because
1 ≤ γ, 1 ≤ α2,and
J0T
0
z0
X σ12 (hwi, roi2+hwi, doi2)=β2 UJO
≤ β2
≤βt
(29)
Also (i) follows from assumptions 1-3,
dt-1
tion assumption equation 26,
det-1UU
- det-1 UU
`2
≤ kzt-1 - zet-1 k` together with induc-
`2
≤ ket-1k'2 ≤ kz0k'2 ,and
UUUJ0Tdet-1UUU	≤ UUJ0TTreet-1UU
∣∣'2	J(T dt-ιj '2
= UU I	-ηJ0TJ0
U [μI	(1 - μ) I
= UUU I	-ηJ(TJ(
U [μI	(1 - μ) I
J(Tret-2UU
JoTdt-J Ik
-1 J(T re( UUU
J
≤ Y (1- ηα2)t-1
J(T
0
(30)
where in the last inequality We applied Lemma A.3. Finally, by using η ≤ 薪 We arrive at
kθt - θt-1k'2
≤ HMt	J0⅛3η⅞f kzok'2
≤ 8 UJt	Jtzo	+3⅛ kz0k'2
`2
R
≤ ^2,
Where the last line is directly due to inequality (24), γ ≤ 5, and α ≤ β . Hence, We have established
θt ∈ BR (θo).
Part II: In Lemma A.3 We shoWed that the time invariant system of state equations zet+1 = Azet is
uniformly exponentially stable, i.e. kzet k` goes doWn to zero exponentially fast. NoW by using the
assumption that the Jacobian remains close to the Jacobian at the initialization Jo, We aim to shoW
the exponential stability of the time variant system of the state equations 16. For that, We compute
zt = At-1zt-1
I
μI
I
μI
-ηJt,t-1JtT-1
(1 - μ) I
zt-1
-ηJOJ0T卜,]+ η (JOJT - Jt,t-1J-1)dt-1
: Azt-1 + ∆t-1.
20
Published as a conference paper at ICLR 2021
Now set λ := 1 - ηα2. By induction, we obtain the relation zt = Atz0 + Pit=-01 At-1-i∆i. Hence,
t-1
kztk`2 = Atz0+XAt-1-i∆i
i=0	`2
t-1
≤Mtz0L + XMjAL
i=0
t-1
≤ γλt kz0k'2 + X γλt-1-i ∣∣η (Jo j0t - Ji+ι,iJT) Ilkdik'2
i=0
t-1
≤ γλt kz0k'2 + Xηγλt-1-i (2βe) Ilzik'2 .	(31)
i=0
The second inequality holds because of Lemma A.3. The last inequality holds because by combining
our assumptions 1 to 3 with θt ∈ BR (θ0) and the induction assumption 28 for 0 ≤ i ≤ t - 1, we
have that
∣J0J0T-Ji+1,iJiT∣ = ∣J0J0T
- J0JiT + J0JiT - Ji+1,iJiT∣
≤ kJ0k kJ0 - Jik + kJ0 - Ji+1,ik kJik
≤ β kJ0 - Ji k + β kJ0 - Ji+1,i k
≤ 2β.	(32)
In order to deal with inequality 31, we will rely on the following lemma.
Lemma A.5 (Rugh, 1996, Lemma 24.5) Consider two real sequences p (t) and φ (t), where p (t) ≥
0 for all t ≥ 0 and
I ψ,
φ(t) ≤ I ψ + ηXp(i)φ(i),
i=0
ift=0
ift≥1
where η and ψ are constants with η ≥ 0. Then for all t ≥ 1 we have
t-1
Φ(t) ≤ Ψ Y(1 + η ∙p(i)).
i=0
Now We define φt = kz；t'2 and rewrite inequality 31as
A X XX q X 2ηγJj
φt ≤ γφ0 + ʌ 一ʌ一φi.
λ
i=0
Hence, Lemma A.5 yields that
φt ≤ γφ0 Y (1 +号e
i=0
γφ0 1 +
(i)
≤ γφ0	1 +
(ii)
= γφ0
2ηγβe)t
ηα2 t
^2λ)
2t
-ηɑ- ʌ
1 - ηα2
1
21
Published as a conference paper at ICLR 2021
where (i) follows from 4γβ ≤ α2 and (ii) holds by inserting λ = 1 - ηα2. Inserting the definition
of φ0 and φt we obtain that
kztk'2 ≤Y (1 - η~^~) ιιzok'2.
This completes the proof of Part II.
Part III: In this part, our aim is to show that the error vector et := zt - zet obeys inequal-
ity 26. First, note that
〜
et = zt - zet
= (Azt-1 + ∆t-1) - Azet-1
= Aet-1 + ∆t-1,
where in (*) We used the same notation as in Part II for ∆t-ι. Using a recursive argument as well
as e0 = 0 we obtain that
t-1
ketk`2 = XAt-1-i∆i
i=0	`2
t-1
≤ Xηγ (1 - ηɑ2)t-1- k^ik'2
i=0
(i)	t-1
≤ Xηγ (1 - ηα2)ji Jo J(T-Ji+1, JTllkdik'2
i=0
(ii)	t-1
≤ X2ηβeγ (1 - ηα2)	llzik'2.
i=(
The first inequality follows from the triangle inequality and Lemma A.3. Inequality (i) follows from
the definition of ∆i. Inequality (ii) follows from inequality 32. Setting c := 2ηβe we continue
t-1
ketk'2 ≤ XCY (1 -ηα2)t-iτ ∣∣zik'2
i=(
(iii)
≤
(iv)
≤
kz( k`2
α2 t-1
2ηγ2βe ∙ t (I- 等)Ilzoll'2 .
Here (iii) holds because of our induction hypothesis 25 and (iv) follows simply from
2
1 一 η02 ≤ 1 一 n~2~. This shows the first part of equation 26 for iteration t. Finally, to de-
rive the second part of equation 26 we observe that for all t ≥ 0 and 0 < x ≤ 表 we have
t-1	α2 α2
t (1 一 x) 一 ≤ e(15 ]116)). Since η2- ≤	≤ 在 we can use this estimate, which yields
ketk`2 ≤
2 t-1
2ηγ2βe ∙ t (1 - 4-)	llzoh
≤ e (15ln 15) α2 kz0k'2 .
Hence, we have shown equation 26.
22
Published as a conference paper at ICLR 2021
Part IV:	In this part, we aim to show that the parameters of the original and linearized problems are
close. For that, we compute that
IMt-NL
t-1
X Vθh(θi, di) - Vθhiin (θi, di)
i=0
t-1
XJT (θi)di - J0T dei
`2
i=0
t-1
`2
t-1
≤ x II(JT (θi) -JT)dik+x JT (θi) (di - di) k
(i)	t-1	t-1
≤) X e kZik'2 + β X keik'2
i=0	i=0
(ii)	t-1	i	t-1	ηα2 i-1
≤ Ye X(1-ηα2) I∣zok'2 +2ηγ 2β2e X i (1-亏)Ilz0k'2 .
Here (i) follows from assumptions 2 and 3, and (ii) holds because of Lemma A.3 and our induction
hypothesis 26. Hence, using the formula Pi=° ixi = xO+t：.—［；；+I)X ) We obtain that
1llθt- θ⅛≤ γe kz0k'2
1 - (1 - ηɑ2)t
ηa2
∖
+ 2ηβ2Y
≤ Ye kz0k'2
/
12
-2 + 2ηβ2γ
ηα2
∖
(iii)
≤ Ye kz0k'2
8β2γ
ηα4
9eβ2γ2
ηα4
IZh ,
where (iii) holds due to 1 ≤ Y and 1 ≤ 胃.Hence, we have established inequality 27 for iteration
t.
Part V:	In this part, we are going to prove equation 28 for iteration t. First, it follows from the
triangle inequality that
〜
〜
kθt- θ0k'2 ≤ Hθt- θ0∣∣'2 + Hθt- θt∣∣'2
≤ Het- θ0∣`2 + 9eβ2≠ kz0k`2,
23
Published as a conference paper at ICLR 2021
where in the second inequality we have used Part IV. Now we bound θet - θ0	from above as
follows
θet - θ0	= η
`2
t-1
X J0T dei
i=0
t-1
≤ ηX J0T dei
`2
(i)
i=0
t-1
`2
≤ ηγ
X(1 - ηɑ2)i
i=0
1 — (1 — ηα2)t
=ηγ 诉
(≤)γβ2∣∣Jt	o]
α2∣∣[ 0	J"
J0T
0
z0
`2
z0
z0
`2
`2
where (i) holds by equation 30 and (ii) holds by equation 29. Hence, it follows from the definition
of R (equation 24) that
kθt- θ0k'2 ≤ YmIJO	j0[
R
=—
.
2
z0
+ 勺2 kzok'2
`2 α
This completes the proof.
A.4 preliminaries for proofs of results with one-hidden layer generator and
LINEAR DISCRIMINATOR
In this section, we gather some preliminary results that will be useful in proving the main results
i.e. Theorems 2.1 and 2.2. We begin by noting that Theorem 2.1 is an instance of Theorem A.4 with
f (W) = n pn=ι V ∙ φ (Wzi). We thus begin this section by noting that f (W) can be rewritten
as follows
-n1 Pi=Iφ (WT zi)-
f (W ) = V ∙
Lnl P 乙 Φ (WT z，」
Furthermore, the Jacobian of this mapping f (W) takes the form
1n
J(W) =	(V ∙ diag (φ0 (Wzi)))③ zT.
n
(33)
i=1
To characterize the spectral properties of this Jacobian it will be convenient to write down the ex-
pression forJ (W)J (W)T which has a compact form
n
J(W)J(W)T =) / X ((V ∙diag(φ0 (Wzi)))③ ZT) (diag (φ0 (Wzj)) VT ③ Zj)
n i,j=1
n
" n2 X (Vdiag (φ0 (Wzi)) diag (φ0 (Wzj)) VT)乳(W j
n i,j=1
3 V diag I '
n '=1,...,k ∖ ；
J2 V ∙ D2 ∙ VT,
n2
n
X Ziφ0 (WT Zi)
i=1
2 VT
`2
24
Published as a conference paper at ICLR 2021
where D is a diagonal matrix with entries
n
d`' = XZiφ0(wTZi)	=IlZTΦ0(Zw')∣∣'2 ,
i=1	`2
(34)
and Z ∈ Rn×d contains the zi ’s in its rows. Note that we used simple properties of kronecker
product in ⑴ and (ii), namely (A 0 B)T = AT 0 BT and (A 0 B)(C 0 D) = (AC) 0 (BD).
The next lemma establishes concentration of the diagonal entries of matrix D2 around their mean,
which will be used in the future lemmas regarding the spectrum of the Jacobian mapping. The proof
is deferred to Appendix B.1.
Lemma A.6 Suppose W ∈ Rd is a fixed vector, zι, Z2, •一，Zn ∈ Rd are distributed as N (0, σz2Id)
and constitute the rows of Z ∈ Rn×d. Then for any 0 ≤ δ ≤ 3 the random variable D =
∣∣ZTφ (Zw)IIp satisfies
(1 - δ) E (D2) ≤ D2 ≤ (1 + δ) E (D2)
with probability at least 1 — 2 (e-n18~ + e-^54 + e-c1nδ) where ci is a positive constant. Moreover
we have
E (D2)=σ2 (n2d + —).
Furthermore, using the above equation we have
EJ(W)J(W)T
σz (d + n-1) VVT
2n	.
A.5 Lemmas regarding the initial misfit and the spectrum of the Jacobian
In this section, we state some lemmas regarding the spectrum of the Jacobian mapping and the initial
misfit, and defer their proofs to Appendix B. First, we state a result on the minimum singular value
of the Jacobian mapping at initialization.
Lemma A.7 (Minimum singular value of the Jacobian at initialization) Consider our GAN model
with a linear discriminator and the generator being a one hidden layer neural network of the form
z — Vφ(Wz), where we have n independent data points zi, Z2,…，Zn ∈ Rd distributed as
N(0, σz2Id) and aggregated as the rows ofa matrix Z ∈ Rn×d, and V ∈ Rm×k has i.i.dN(0, σv2)
entries. We also assume that W0 ∈ Rk×d has i.i.d N(0, σw2 ) entries and all entries ofW0, V, and
Z are independent. Then the Jacobian matrix at the initialization point obeys
σmin (J (W0)) ≥
，(1 — δ)2 k — (1 + δ)2 — √m (1 + η)(i + δ)
σv σz l∕dΞ≡
2n
η2 m
with probability at least 1 — 3e- 8 — 2k ∙
constant.
nδ2
dδ2
e--18^ + e-B + e
-c1 nδ , where ci is a positive
3
0 ≤ δ ≤ —
2
Next lemma helps us bound the spectral norm of the Jacobian at initialization, which will be used
later to derive upper bounds on Jacobian at every point near initialization.
Lemma A.8 (spectral norm of the Jacobian at initialization) Following the setup of previous
lemma, the operator norm of the Jacobian matrix at initialization point W0 ∈ Rk×d satisfies
kJ (W0)k ≤ (1+δ)σvσz
(√k+2√m) en≡,
3
0 ≤ δ ≤ —
2
with probability at least 1 — e-m — k ∙
e-nδ2 + e-d54 + e-c1nδ
, with ci a positive constant.
The next lemma is adapted from Van Veen et al. (2018) and allows us to bound the variations in the
Jacobian matrix around initialization.
25
Published as a conference paper at ICLR 2021
Lemma A.9 (single-sample Jacobian perturbation) Let V ∈ Rm×k be a matrix with
i.i.d. N 0, σv2 entries, W ∈ Rk×d, and define the Jacobian mapping J (W; z) =
(Vdiag (φ0 (WZ))) 0 ZT. Then, by taking Wo to be a random matrix with i.i.d. N(0, σW) en-
tries, we have
kJ(W； Z)-J(Wo; z)k ≤ σ kzk'2
(
2√m +
∖
(2kR ) 3
∖ σw )
6
for all W ∈ Rk×d obeying ∣∣ W 一 Wok ≤ R With probability at least 1 一 e-号一
e
Our final key lemma bounds the initial misfit f (Wo) 一 y := 1 PZi VΦ (WOzi) - x.
Lemma A.10 (Initial misfit) Consider our GAN model with a linear discriminator and the gen-
erator being a one hidden layer neural network of the form z Vφ(WZ), where we have n
independent data points zi, z2, …,zn ∈ Rd distributed as N(0, σ∣Id) and aggregated as the
rows ofa matrix Z ∈ Rn×d, and V ∈ Rm×k has i.i.d N(0, σv2) entries. We also assume that the
initial Wo ∈ Rk×d has i.i.d N(0, σw2 ) entries. Then the following event
1n
VΦ (WOzi)- X
n i=1
holds with probability at least 1
constant.
≤ (1 + δ)-1= σvσwσζ√kdm + ∣∣xk'2 , 0 ≤ δ ≤ 3
2π	2
`2
一 (k ∙ e-c2n(δ/27)2 + e- (δ/9) m + e-("32 kd ), with c? a fixed
A.6 Proof of Theorem 2.1
In this section, we prove Theorem 2.1 by using our general meta Theorem A.4. To do this we need to
check that Assumptions 1-3 are satisfied with high probability. Specifically, in our case the parameter
θ is the matrix W and the non-linear mapping f is given by f (W) = 1 PZi VΦ (Wzi). We
note that in our result do = 0 and thus kzok` = krok` , which simplifies our analysis.
To prove Assumption 1 note that by setting δ = 1 and η = 3 in Lemma A.7, We have
σmin (J (Wo)) ≥ σvσz
: α.
This holds with probability at least 1 — 3e-m2 - 4k ∙ e-cn - 2k ∙ e-2d6, concluding the proof of
Assumption 1. Next, by setting δ = 2 in Lemma A.8 we have
3	/	、/ d + n-1
kJ (WO)k ≤ Z = 2 σv σ (k++2√m) X —2nπ-
With probability at least 1 一 e-m ― 2k • e-c∙n — k ∙ e-2d6. Now to bound spectral norm of Jacobian
at W Where kW 一 Wo k ≤ R (the value of R is defined in the proof of assumption 3 beloW), We
use triangle inequality to get
kJ (W )k ≤ kJ (Wo )k + kJ (W) -J (Wo )k.
This last inequality together With assumption 3, Which We Will prove beloW, yields
kJ(W)k ≤ kJ(Wo)k + e ≤ kJ(Wo)k + 4γ2β ≤ kJ(Wo)k + JWr
26
Published as a conference paper at ICLR 2021
Therefore by choosing β = 2ζ we arrive at
kJ(W)k≤kJ(W0)k+
kJ(W0)k+
kJ(W0)k2
4β
J(W0)『
8Z
≤ kJ (W0)k +
J(W0)『
8 kJ (Wo)k
≤ 2 kJ (W0)k
≤ 2ζ = β,
establishing that assumption 2 holds with
β = 3σvσz (√k + 2√m^ j
d+n-1
2n
With probability at least 1 - e-mm - 2k ∙ e-c n - k ∙ e-2⅛.
Finally to show that Assumption 3 holds, we use the single-sample Jacobian perturbation result of
Lemma A.9 combined With the triangle inequality to conclude that
kJ (W) -J (W0)k
=1 (XX J(W； Zi)-J(W0； Zi)
1n
≤ - EkJ(W; Zi) -J (Wo； Zi)k
n i=1
/
7~；~p
' 3 (警Y'
σv
≤ --
n
2√m+
t
2
6( 2kR y log
σw
≤ σv ≡
n
2√m+
Cii) 5	二
≤ /vσzy d	2√m +
,	、2
6( 2kR y log
σw
2
V(2kR )3
2
2kR∖ 3,
6	log
σw
(
k
2
V (暇T
(35)
∖
/
k N
/
∖
∖
(
k
/
「\

where (i) holds by CaUchy-SchWarz inequality, and (ii) holds because for a Gaussian matrix Z ∈
Rn×d With N (0, σz2) entries the folloWing holds
P(kZkF ≤ 5σz√nd) ≥ P(kZkF
32
≤ 2σznd
≥1-
_ nd
e- 24.
27
Published as a conference paper at ICLR 2021
Now We set e = j and show that Assumption 3 holds with this choice of e and with radius R,
whose value will be defined later in the proof. First, note that
α2
e =------
4γβ
σVσZ ( 2 √k - 9 - 2√m) ( d∖nπ )
12γσv σz (√k+2√m) ∖J d∖n，
(i)
≥
σvσz
60 (3√k)
›σv σz √k
≥ 42000
where (i) holds by assuming k ≥ C ∙ m with C being a large positive constant. Combining the last
inequality with equation 35, we observe that a sufficient condition for assumption 3 to hold is
5
4 σv σz
(
2√m+
∖
log
< σv σz √k
≤ 42000
∖
which is equivalent to
105000√md + 52500 ∙ √d ∙
t
2
2kR∖ 3,
6	log
σw
(
k
2
V( IRr
Now the first term in the L.H.S. is upper bounded by ɪ √ if k ≥ (210000)2 md, and for the second
term we need
105000 ∙ √d ∙
t
6
log
≤ √k,
2
which by defining X = 3d ( :√) 3 is equivalent to
d1
g x - 2 ∙ 1050002.
This last inequality holds for X ≤ Iogd with c < 1 a sufficiently small positive constant, which
translates into
R≤c
σw √k
3
(d log d)2
(36)
~
~
2
So far we have shown that Assumption 3 holds with e =枭 and with radius R defined as R :=
σ…、√k
C W k 3, and we conclude that it holds for any radius R less than R as well. Now we work with
(d log d) 2
28
Published as a conference paper at ICLR 2021
the definition of R in eqUation 24 to show that R ≤ R:
Jt 0 一
.0 Jl
2
Rβ
E = Ya
z0
2
9eβ2Y2
+ :2- kz0 k'2
`2 α
(i)	β2
≤ ^oɜ kr0k'2 +
9 4⅛ β 2γ2
α4
kr0k'2
20
(ii)
≤ 20
Y kr0k'2( a+4 a
σv σz (22 √k 一 9 一 2√m) J
λ3 kr0k'2
d+n-1∖
2n
(iii)	1	(2	________
≤ C • -------T= ∙ wσvσwσzVZk ∙ d ∙ m + ∣∣X∣∣'2
σvσz k 3	2
where (i) holds because
≤ 1 and 4%βe = α2, (ii) holds as 1 ≤
from Lemma A.3, and (iii) follows from k ≥ C ∙ m and using δ
sUfficient condition for eqUation 36 to hold is that
β andaswe SUbstitUte Y = 5 &
3 in Lemma A. 10. Now a
1
-7= ∙ 2σσvσwσz√k ∙ d ∙ m + ∣∣Xk',) ≤ C σw" 3
√k ∖3	2J	(d log d)2
σvσz
which is eqUivalent to
2
3 σv σw σz
3	------- 3	_
• (dlog d)2 v k ∙ d ∙ m + (dlog d)2 ∣∣xk'2 ≤ c ∙ ko。σwσz.
Finally, this inequality is satisfied by assuming k ≥ C ∙ md4 log (d)3 and setting σvσwσ%
-Fk'2 3 . This shows that assumption 3 holds with probability at least 1 一 ne-mm
md 2 log d 2
ne-c-md3 log®)2 - k ∙ e-c-n - e-Tmb - e-⅛⅛, concluding the proof of Theorem 2.1.
≥
—
A.7 Proof of Theorem 2.2
Consider a nonlinear least-sqUares optimization problem of the form
miRpL⑻ :=2kf ⑻-yk22
with f : Rp 7→ Rm and y ∈ Rm . SUppose the Jacobian mapping associated with f satisfies the
following three assUmptions.
Assumption 1 We assUme σmin (J (θ0)) ≥ 2α for a fixed point θ0 ∈ Rp.
Assumption 2 Let k ∙ k be a norm dominated by the FrobeniUs norm i.e. ∣∣θk ≤ |向恨
holds for all θ ∈ Rp. Fix a point θ0 and a number R > 0. For any θ satisfying kθ - θ0 k ≤ R, we
have kJ (θ)-J (θo) k ≤ 3.
Assumption 3 We assume for all θ ∈ Rp obeying kθ - θ0 k ≤ R, we have kJ (θ)k ≤ β.
With these assumptions in place we are now ready to state the following result from Oymak
& Soltanolkotabi (2020):
Theorem A.11 Given θ0 ∈ Rp, suppose assumptions 1, 2, and 3 hold with
R 3 kf(θ0)-yk'2
R =---------------.
(37)
α
29
Published as a conference paper at ICLR 2021
Then, using a learning rate η ≤ 备,all gradient descent updates obey
kf(θτ)-yk'2 ≤ (1- ηα2)τkf(θ°)-yk'2 .
(38)
We are going to apply this theorem in our case where the parameter is W, the nonlinear mapping is
f (W) = n1 Pn=1 Vφ (Wzi) with φ = ReLU, and the norm ∣∣ ∙ ∣∣ set to the operator norm.
Similar to previous part, by using Lemma A.7 we conclude that with probability at least
1 — 3e-m2 — 4k ∙ e-c∙n — 2k • e- 2d6, assumption 1 is satisfied with
Next we show that assumption 2 is valid for α as defined in the previous line and for radius R defined
later. First we note that
3 ≥ C • σv σz • √k,
where the inequality holds by assuming K ≥ C • m for a sufficiently large positive constant C . Now
by using equation 35 assumption 2 holds if
5 σv σz √d
/
2√m+
2
log
≤ C • σv σz • √k,

which is equivalent to
c√md+CVd •
t
,	、2
6( 2kR y log
σw
/
k
O ( 2kR∖ 3
∖3( R
≤ √k.
The first term in the L.H.S. of the inequality above is upper bounded by ɪ√k if k ≥ C • md. For
upper bounding the second term it is sufficient to show that
2
which by defining X = 3d ( 2√-) is equivalent to X • log (d) ≤ C. Now this last inequality
σw k	x
holds if We have X ≤ logc(d)for a sufficiently small constant c, which by rearranging terms amounts
to showing that R ≤ C ∙ ®；w / 3 . Hence up to this point, We have shown that assumption 2
√	√	√	√	√.. √k,
holds with radius R := C ∙ - W k 3, and this implies that it holds for all values of R less than R.
(d∙log(d)) 2
Therefore, we work with the definition of R in equation 37 to show that R ≤ R as follows:
R =3 kf(θo)-yk'2
α
(i) 3 (2	/______ 、
≤ 一	3°vGwGz√k • d • m + ∣∣x∣'
α3	2
2 22σvσwσz√k • m • d + 3 kxh)
σvσz (ɪ√k 一 9 —
—1
1	∙	/ ∙∖	FT	A ⅛ Z∖ ∙ .1 C-	1 TT	C	1	∙	T-i	^Γ^ ∙ .	<∙C	.	∖	. ∖
where in (i) we used Lemma A.10 with δ = 1. Hence for showing R ≤ R it suffices to show that
2 12σvGwQz√k • m • d + 3 kxlh)
σvσz (1 √k 一 9 一 2√m) ʌ/ ^2r∏ -
Qw √k
3
(d ∙ log (d))2
30
Published as a conference paper at ICLR 2021
which by assuming k ≥ C ∙ m simplifies to
3	3
σvσwQz (d ∙ log (d))2 √k ∙ m ∙ d + (d ∙ log (d))2 kx- ≤ C ∙ k ∙ σvσ.Qz.
Now this last inequality holds if k ≥ C ∙ md4log(d)3 and by setting QvQwσz ≥
kxk'2
md 2 log d 2
Therefore Assumption 2 holds for radius R defined in equation 37 with probability at least 1 一
ne-mm 一 ne-c-md3 IOgw)2 一 k ∙ e-c^n 一 e- 1500 一 e-摇.
Finally to show assumption 3 holds, we note that for all W satisfying kW 一 W0 k ≤ R, where the
value of R is defined in equation 37, it holds that
kJ (W )k ≤ kJ (Wo)k + kJ (W) -J (Wo)k
α
≤kJ (Wo)k + 3
≤ kJ(Wo)k + Qmin (J(WO))
6
≤ 2 kJ (Wo)k
≤ 3QvQz (√k + 2√m) jd+2nπ ,
where the last inequality holds by using lemma A.8, hence establishing that assumption 3 holds with
β = 3σvσz (√k + 2√m);
d+n-1
2n
With probability at least 1 一 e-mm 一 2k ∙ e-cn 一 k ∙ e-2dθ, finishing the proof of Theorem 2.2.
B Proofs of the auxiliary lemmas
In this section, we first provide a proof of Lemma A.6 and next go over the proofs of the key lemmas
stated in Section A.5.
B.1 Proof of Lemma A.6
Recall that
1n
J(W) J(W)t = UE (Vdiag(φ0(Wzi))diag(φ0(Wzj)VT) (zTzj)
n i,j=1
1 ʊ N
F V diag
n '=1,...,k
n
Ezi φ0(wT Zi)
i=1
VT = ɪ V ∙ D2 ∙ VT,
n2
`2
Where D is a diagonal matrix With entries
In
D'' = ^Ziφ0(wT Zi)
i=1
IlZ T Φ0(ZW')∣∣'2.
(39)
`2
The matrix Z ∈ Rn×d contains the zi ’s in its rows. In order to proceed we are going to analyze the
entries of the diagonal matrix D2. We observe that
IIZT Φ0(Zw)∣I22
T
wwT
(I-E
I2
)ZT φ0(Zw)II	+
{Z
A
First, we compute the expectation of A. We observe that
`2
.}
T
wwT	T
2ZTφ (Zw)
||w||2
{z
B
`2
}
nT
χ(I-w⅛ )ziφ0(wτ Zi)
`2
|
I
2
2
A
31
Published as a conference paper at ICLR 2021
Conditioned on w,(I - WwT2) Zi is distributed as N(0, σZ(I - WwT) and WTZi has distribu-
tion N 0, σz2kwk2 . Moreover, these two random variables are independent, because w is in the
wwT
null space of I - wWw2. ThiS observation yields
E(A) = E
nT
X(I-∣∣W∣∣2 )ziφ0(wT Zi)
2
`2
nn
XXE (I-
i=1 j=1
T
w^)z∙ (I
∣∣w∣∣2) , (I
Ww2)zj) φ0(WTZi)φ0(WTZj)
Xi=n1E	(I-
T
WWT
iW )Zi
n1
X 2(d - 1)σz = 2(d - 1)σz.
i=1 2	2
Next We show that A is concentrated around its mean. Because II - Ww2) Zi is independent from
WT Zi, we use Zi0 as an independent copy of Zi. Hence we can write
T 2
A=I(I-W⅛)ZT φ0(Zw)l'2
T 2
I(I-W⅛ W φ(Z 0w)I'2
nT
=X(I - ∣∣W∣∣2 ')“φ (WTZG
n
giui
i=1
2
`2
where gi =(I - Ww2) Zi 〜N 仪后(I - Ww2)) and Ui = φ0 (WTZi)〜bern( 1 ),4 and
these are all independent from each other. Note that IlPn=I giuik22 has the same distribution as
kgk22 ∙ kuk22, where g 〜 N(0, σ2 II - Wwk2)) and U is a vector with entries Ui. Note that for
the norm of u, the event
n(i - δ) ≤kuι22 ≤ 2(i + δ)
holds with probability at least 1 - 2e-*. Recall that for g 〜N(0, σ2Id) and 0 < δ ≤ 1 we have
p (kgk22 ≥
p(kgk22 ≤
(1 + δ)E(kgk2j) ≤ e-噜,
(1 - δ)E(kg脸)) ≤ e-d42.
(40)
By applying the union bound and noting that E (Ilgk2)= (d - 1) σ2, for 0 < δ ≤ 2, we obtain
that the event
Ikgk22 ku* - 2(d - 1)σZ∣ ≤δ2(d - 1)σZ
4Here, bern(2) means that the random variable takes values 0 and 1 each with probability 1/2.
32
Published as a conference paper at ICLR 2021
nδ2
holds With probability at least 1 - 2e-B
In order to analyze B , we first note that
B
-2e-dδ2
T
wwT	T
2Z φ (Zw)
||w||2
2
`2
T 2
嬴 Z Tφ0(Zw)
=k∣⅛φ0(Zw)X
=I hg,φ0(ιιwι∣g)i I
n
Egi ∙ i(gi≥o)
i=1
ReLU(gi)!
where gi = NT k^ 〜N (0,σ2). It follows that
E(B) =E XReLU(gi)
n
XE ReLU2(gi) + XE ReLU(gi)ReLU(gj)
i=1	i6=j
σz2
n n(n - 1)
2 + —2∏一
which results in
E (D2e) = E (A) + E (B)= σ2
nd	n(n - 1)
^2+~Π^)
1 ≤ ` ≤ k.
Next, in order to show that B concentrates around its mean, we note that ReLU (gi) is a sub-
Gaussian random variable with ψ2 -norm Cσz, where C is a fixed constant. Therefore X =
pn=ι ReLU (gi) is SUb-GaUSSian with ψ2-norm C√nσz. By the sub-exponential tail bound for
X2 - E(X2) we obtain
P(|B - E(B)∣ ≥ t) ≤ 2e-cnσ2 .
Finally by putting these results together and using union bounds we have
P {∣D2e - E (D2e) ∣ ≥ δE (D2e)} ≤ 2e-二 +2e-dδ2 + 2e-c1nδ,	0 ≤ δ ≤ 3,
finishing the proof of Lemma A.6.
B.2 Proof of lemma A.7
Our main tool for bounding the minimum singular value of the Jacobian mapping will be the fol-
lowing lemma from Soltanolkotabi (2019):
Lemma B.1 Let d ∈ Rk be a fixed vector with nonzero entries and D = diag (d) . Also, let
A ∈ Rk×m have i.i.d. N (0, 1) entries and T ⊆ Rm. Define
bk (d) = E [kDgk∕，
where g 〜N (0, Ik). Also define
σ(T) := mv∈aTx kvk`2 .
Then for all u ∈ T we have
i llDAuk'2 - bk(d) kuk'21 ≤ kdk'∞ ω (T) + η
____- η2__
with probability at least 1 — 6e8kdk2∞ σ2(T).
33
Published as a conference paper at ICLR 2021
In order to apply this lemma, We set the elements of d to be d`' as in equation 39 and choose
T = Sm-1 and A = V T ∈ Rk×m with N(0, σv2) entries. It follows that
bk (d) = EkDgk'2 = JE (kDg%) - Var (口。9必),
Where
k
E (kDg%) = kdk22 = XD2'.
'=1
We are going to use the fact that for a B-LiPSchitz function φ and normal random variable g 〜
N(0, 1), based on the Poincare inequality (Ledoux, 2001) We have Var(φ (g)) ≤ B2. By noting that
for a diagonal matrix D
∣kDxk'2 - kDyk'2∣ ≤ kDx - Dyk'2 ≤ l∣dk'∞ kx - yk'2,
We get
EkDgk'2 =，E(kDgk22)- Var(kDgk'2)
≥ q⅛τk⅛.
This combined with ω (Sm-1) ≤ √m and Lemma B.1 yields that the event
σmin (VD) ≥ σv ( VZkdk22 - kdk2∞ - kdk'∞ √m - η
(41)
holds with probability at least 1 -
一η2
3e 8kdk2∞
2
'2
Next, using the concentration bound for D'2', which we obtained in Section B.1, we bound kdk
and kdk'co, where we have set di = Dii for 1 ≤ i ≤ k. For 0 ≤ δ ≤ 3 we compute that
p (maXkdi ≥(1+δ) qE[di)=p (q d2 ≥(1+δ)2 E mJ
≤ k ∙ P(d2 ≥ (1 + δ)2 E [d2])
≤ k ∙ P (d2 ≥ (1 + δ)E [d2]) ≤ k ∙卜-嘴 + e-警 + e-c1nδ),
(42)
as well as
P (kdk'2 ≤ (1 — δ) √kqE[di) ≤ P ([J d2 ≤ (1 - δ)2 E [d2])
≤ k ∙ P(d2 ≤ (1 - δ)2 E [d2])
≤ k ∙ P (d2 ≤ (1 - δ)E [d2]) ≤ k ∙ (e-nδ2 + e-ɪ + e-c1nδ).
(43)
Finally by replacing η with η kdk` √m in equation 41, combined with equation 42 and equa-
tion 43, for a random W0 with i.i.d. N 0, σw2 entries we have:
σmin (J(Wo)) = n1 σmin (VD)
≥ σv NQ-δ)2k-(1 + δ)2 -√m(1 + η)(1 + δ)) qE^[d2I
q(1 - δ)2 k - (1 + δ)2 - √m(1 + η)(1 + δ)) σvσz∖ d+ π ,	0 ≤ δ ≤ 3,
2n	2
with probability at least 1 一 3e-η^sm 一 2k ∙
of Lemma A.7.
nδ2
dδ2
-c1nδ . This completes the proof
e-^ɪ8^ + e-B + e
34
Published as a conference paper at ICLR 2021
B.3 Proof of lemma A.8
Recall that
J (W )J(W )T = n12 V '=diagk
n
EzMwT zi)
=1
Vt = ɪ V ∙ D2 ∙ VT,
n2
which implies that
kJ(W0)k = 1kV ∙ Dk ≤ 1 kVkkDk∙
nn
For matrix V ∈ Rm×k with i.i.d N(0, σv2) the event
IlVk ≤ σv (√k + 2√m)
holds with probability at least 1 一 e--m. Regarding matrix D by repeating equation 42 the following
event
kDk = ImaxDii ≤ (1 + δ) qE[D2] =(1 + δ) σz Jnd + nnm,	0 ≤ δ ≤ 3
1≤i≤k	2	2π	2
holds with probability at least 1 一 k ∙
that the event
nδ2
dδ2
e-^ɪ8^ + e-B + e
-c1nδ . Putting these together it yields
/	、/ d + n-1	3
kJ(W0)k ≤ (1 + δ) σvσz (√k + 2√m) X	?；，	0 ≤ δ ≤ ]
holds with probability at least 1 一 e-mm 一 k ∙
Lemma A.8.
e-nδ2 + e-dδ2 + e-c1nδ
, finishing the proof of
B.4 Proof of lemma A.10
First, note that if W has i.i.d. N (0,σ^) entries and V, W, Z are all independent, then
kf (W)k'2 = n IlVφ (WZT) 1n×ι∣∣'2 has the same distribution as ∣∣v∣∣'2 Mkg2, where V 〜
N (0,σVlm) and a = ɪφ (WZT) 1 has independent sub-Gaussian entries, so its '2-norm is
concentrated. Note that conditioned on W, ai = n pn=ι ReLU (ZTwi) is sub-Gaussian with
∣∣aikψ2 = C kwi√n2σz, and it is concentrated around Eai = √1= ∣∣wik'2 σz. This gives
-C δ2(Eai)2
P{ai ≤ (1 + δ)Eai} ≥ 1 一 e	kaikψ2 = 1 一 e-cnδ2
which implies that
P {a2 ≤ (1 + 3δ)(Eai)2} ≥ P {a2 ≤ (1 + δ)2(Eai)2} ≥ 1 一 e-cnδ2,	0 ≤ δ ≤ 1.
Due to the union bound we get that
(k
kak`22 ≥ (1 + δ) X(Eai)2
i=1
≤ P	[k ai2 ≥ (1 +δ) (Eai)2
≤
k
X P {a2 ≥ (1 + δ) (Eai)2} ≤ k ∙ e-cn(δ/3)2, 0 ≤ δ ≤ 3.
i=1
By substituting Pk=I(Eai)2 = 2∏σ2 ∣∣WkF this shows
P {kak'2 ≤ (1 + δ) √2∏σz kW∣∣f} ≥ P {1㈤区 ≤ (1 + δ) 2∏σ∣ kWkF} ≥ 1一 k ∙ e-cn(δ⑶2
0 ≤δ≤3.
35
Published as a conference paper at ICLR 2021
We also have the following result for V 〜N(0,σv2Im)
/2"
P {kvk'2 ≤ (1 + δ) σv√m} ≥ 1 - e-ɪ.
By combining the above results we obtain
P {kak'2 kvk'2 ≤ (1 + δ) √2∏σvσz√m kWHf} ≥ P {kak'2 kvk'2 ≤ (1 + δ/3)2 √2∏σvσz√m kWkF
≥ 1 - k ∙ e-cn(δ/9)2 - e- ("3)2m , 0 ≤ δ ≤ 3.
Furthermore, we can bound kW kF by the tail inequality
P {kW∣∣f ≤ (1 + δ)σw√kd} ≥ 1-e-δ22kd.
Hence, by combining the last two results we have that
P {kak'2 kvk'2 ≤ (I + δ) √2∏σv σz σw√k ∙ d ∙ m ≥ ≥ P {kak'2 kvk'2 ≤
≥ 1 - k ∙ e-cn(δ/27)2
Therefore, due to the triangle inequality the event
kf(W0) - xk'2 ≤ (1 + δ)-1= σvσwCiz√k ∙ d ∙ m + ∣∣xk'2 ,
2	2π	2
(1 + δ∕3)2	__σvσzσw√k ∙ d ∙ m
2π
(δ∕9)2m	(δ∕3)2kd
—e 2	— e 2	, 0 ≤ δ ≤ 3.
0≤δ≤3
2	(δ∕9)2m	(δ / 3)2 kd
holds with probability at least 1 - k ∙e-c2n(δ/27) - e	2 - —e	2 for some positive constant
c2, completing the proof of Lemma A.10.
C Additional Experiments
Effect of single component overparameterization: In Section 3 of the main paper, we performed
experiments in the setting where the size of generator and discriminator are held roughly the same
(both discriminator and generator uses the same value of k). In this part, we analyze single-
component overparameterization where we study the effect of overparameterization when one of
the components (generator / discriminator) has varying k, while the other component uses the stan-
dard value of k (64 for DCGAN and 128 for Resnet GAN). The FID variation of single-component
overparameterization are shown in Fig. 7. We observe similar trends as the previous case where
increasing overparameterization leads to improved FID scores. Interestingly, increasing the value
of k beyond the default value used in the other component leads to a slight drop in performance.
Hence, choosing comparable sizes of discriminator and generator models is recommended.
D Experimental Details
The model architectures we use this in the experiments are shown in Figure 8. In both DCGAN and
Resnet-based GANs, the papemeter k controls the number of convolutional filters in each layer. The
larger the value of k is, the more overparameterized the models are.
Optimization: Both DCGAN and Resnet-based GAN models are optimized using the commonly
used hyper-parameters: Adam with learning rate 0.0001 and betas (0.5, 0.999) for DCGAN, gradi-
ent penalty of 10 and 5 critic iterations per generator’s iteration for both DCGAN and Resnet-based
GAN models. Models are trained for 300, 000 iterations with a batch size of 64.
E	Nearest Neighbor visualization
In this section, we visualize the nearest neighbors of samples generated using GAN models trained
with different levels of overparameterization. More specifically, we trained a DCGAN model with
k = 8 and k = 128, synthesize random samples from the trained model and query the nearest
neighbors in the training set. The plot of obtained samples is shown in Figure. 10. We observe that
overparameterized models generate samples with high diversity.
36
Published as a conference paper at ICLR 2021
(a) Discriminator overparameterization
(b) Generator overparameterization
FigUre 7: Single Component Overparamterization Results: We plot FID scores of Resnet GAN
as the hidden dimension of one of the components are varied, while the hidden dimension of other
component is held fixed. Even in this case, overparameterization improves model convergence.
FigUre 8: Architectures used in over-parameterization experiments. The nUmber of oUt-channels
in convolutional layers is indicated in red. Parameter k controls the width of the architectures — larger
the k , more over-parameterized the models are.
Z	寸
I	CN
in	o
Hidden Dimension (k)
Hidden Dimension (k)
(a)	Discriminator trained to optimal-
ity
(b)	Gradient Descent Ascent
Hidden Dimension (k)
§
(C) diter steps of discriminator UP-
date per generator iteration
FigUre 9: Convergence plot GAN model trained on the Two-Moons dataset, with linear discrim-
inator and 1-hidden layer generator as the hidden dimension (k) increases. Over-parameterizated
models show improved convergence
37
Published as a conference paper at ICLR 2021
IBScuou⊂ω匕仁一 SJOq⅛φu IS①」B① N
SB-dlues pwe」① uωD
Figure 10: Nearest neighbor visualization. We visualize the nearest neighbor samples in training
set for generations from DCGAN model trained on CIFAR-10 dataset. Left panel shows DCGAN
trained with k = 8, while the right one shows the one with k = 128. We observe that overparame-
terized models generate samples with high diversity.
A0.P 曲■»■■
SqdlUBS PsBJ①Ua
38