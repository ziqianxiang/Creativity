Published as a conference paper at ICLR 2021
Efficient Reinforcement Learning in Factored
MDPs with Application to Constrained RL
Xiaoyu Chen Jiachen Hu
Key Laboratory of Machine Perception, MOE,
School of EECS, Peking University
{cxy30, NickH}@pku.edu.cn
Lihong Li
Amazon
llh@amazon.com
Liwei Wang
Key Laboratory of Machine Perception, MOE,
School of EECS, Peking University
Center for Data Science, Peking University
wanglw@cis.pku.edu.cn
Ab stract
Reinforcement learning (RL) in episodic, factored Markov decision processes
(FMDPs) is studied. We propose an algorithm called FMDP-BF, whose regret
is exponentially smaller than that of optimal algorithms designed for non-factored
MDPs, and improves on the previous FMDP result of Osband & Van Roy (2014b)
by a factor of y∕nH∖Si∖, where ∣S∕ is the cardinality of the factored state SUb-
space, H is the planning horizon and nis the number of factored transitions. We
also provide a lower bound, which shows near-optimality of our algorithm w.r.t.
timestep T , horizon H and factored state-action subspace cardinality. Finally, as
an application, we study a new formulation of constrained RL, RL with knap-
sack constraints (RLwK), and provide the first sample-efficient algorithm based
on FMDP-BF.
1	Introduction
Reinforcement learning (RL) is concerned with sequential decision making problems where an agent
interacts with a stochastic environment and aims to maximize its cumulative rewards. The environ-
ment is usually modeled as a Markov Decision Process (MDP) whose transition kernel and reward
function are unknown to the agent. A main challenge of the agent is efficient exploration in the
MDP, so as to minimize its regret, or the related sample complexity of exploration.
Extensive study has been done on the tabular case, in which almost no prior knowledge is assumed
on the MDP dynamics. The regret or sample complexity bounds typically depend polynomially on
the cardinality of state and action spaces (e.g., Strehl et al., 2009; Jaksch et al., 2010; Azar et al.,
2017; Dann et al., 2017; Jin et al., 2018; Dong et al., 2019; Zanette & Brunskill, 2019). Moreover,
matching lower bounds (e.g., Jaksch et al., 2010) imply that these results cannot be improved without
additional assumptions. On the other hand, many RL tasks involve large state and action spaces, for
which these regret bounds are still excessively large.
In many practical scenarios, one can often take advantage of specific structures of the MDP to
develop more efficient algorithms. For example, in robotics, the state may be high-dimensional,
but the subspaces of the state may evolve independently of others, and only depend on a low-
dimensional subspace of the previous state. Formally, these problems can be described as factored
MDPs (Boutilier et al., 2000; Kearns & Koller, 1999; Guestrin et al., 2003). Most relevant to the
present work is Osband & Van Roy (2014b), who proposed a posterior sampling algorithm and a
1
Published as a conference paper at ICLR 2021
UCRL-like algorithm that both enjoy √T regret, where T is the maximum timestep. Their regret
bounds have a linear dependence on the time horizon and each factored state subspace. It is unclear
whether this bound is tight or not.
In this work, we tackle this problem by proposing algorithms with improved regret bounds, and de-
veloping corresponding lower bounds for episodic FMDPs. We propose a sample- and computation-
efficient algorithm called FMDP-BF based on the principle of optimism in the face of uncertainty,
and prove its regret bounds. We also provide a lower bound, which implies that our algorithm is
near-optimal with respect to the timestep T, the planning horizon H and factored state-action sub-
space cardinality |X [Zi]|.
As an application, we study a novel formulation of constrained RL, known as RL with knapsack
constraints (RLwK), which we believe is natural to capture many scenarios in real-life applications.
We apply FMDP-BF to this setting, to obtain a statistically efficient algorithm with a regret bound
that is near-optimal in terms of T, S, A, and H .
Our contributions are summarized as follows:
1.	We propose an algorithm for FMDP, and prove its regret bound that improves on the previ-
ous result of Osband & Van Roy (2014b) by a factor of 'nH∣S∕.
2.	We prove a regret lower bound for FMDP, which implies that our regret bound is near-
optimal in terms of timestep T, horizon H and factored state-action subspace cardinality.
3.	We apply FMDP-BF in RLwK, a novel constrained RL setting with knapsack constraints,
and prove a regret bound that is near-optimal in terms of T, S, A and H.
2	Preliminaries
We consider the setting of a tabular episodic Markov decision process (MDP), (S, A, H, P, R),
where S is the set of states, A is the action set, H is the number of steps in each episode. P is the
transition probability matrix so that P(∙∣s, a) gives the distribution over states if action a is taken on
state s, and R(s, a) is the reward distribution of taking action a on state s with support [0, 1]. We
use R(s, a) to denote the expectation E[R(s, a)].
In each episode, the agent starts from an initial state s1 that may be arbitrarily selected. At each step
h ∈ [H], the agent observes the current state sh ∈ S, takes action ah ∈ A, receives a reward rh
sampled from R(sh, ah), and transits to state sh+1 with probability P(sh+1 |sh, ah). The episode
ends when sH+1 is reached.
A policy π is a collection of H policy functions {πh : S → A}h∈[H] . We use Vhπ : S → R to de-
note the value function at step h under policy π, which gives the expected sum of remaining rewards
received under policy π starting from sh = s,i.e. Vhπ(s) =E PhH0=h R(sh0,πh0(sh0)) |sh=s .
Accordingly, we define Qhπ (s, a) as the expected Q-value function at step h: Qπh (s, a) =
E IR(sh, ah) + PH0=h+ι R(ShO,∏ho(sho)) | Sh = s,ah = a]. We use Vh= and Qh to denote the
optimal value and Q-functions under optimal policy ∏= at step h.
The agent interacts with the environment for K episodes with policy πk = {πk,h : S → A}h∈[H ]
determined before the k-th episode begins. The agent’s goal is to maximize its cumulative rewards
PkK=1 PhH=1 rk,h over T = KH steps, or equivalently, to minimize the following expected regret:
K
Reg(K) d=ef X [V1=(Sk,1) - V1πk (Sk,1)],
k=1
where Sk,1 is the initial state of episode k.
2.1	Factored MDPs
A factored MDP is an MDP whose rewards and transitions exhibit certain conditional independence
structures. We start with the formal definition of factored MDP (Boutilier et al., 2000; Osband &
Van Roy, 2014b; Xu & Tewari, 2020; Lu & Van Roy, 2019). LetP(X, Y) denote the set of functions
that map x ∈ X to the probability distribution on Y .
2
Published as a conference paper at ICLR 2021
Definition 1. (Factored set) Let X = Xi × ∙∙∙ × Xd be a factored set. For any subset of indices
Z ⊆ {1, 2,..., d}, we define the scope set X [Z] := 0i∈zXi. Further, for any X ∈ X, define the
scope variable x[Z] ∈ X [Z] to be the value of the variables xi ∈ Xi with indices i ∈ Z. If Z is a
singleton, we will write x[i] for x[{i}].
Definition 2. (Factored reward) The reward function class R ⊂ P(X, R) is factored over S × A =
X = Xi X …XXd with SCoPeS Zi,…，Zm if for all R ∈ R,x ∈ X, there exist functions
{Ri ∈ P (X [Zi], [0,1])}m=I such that r 〜R(X) is equal to mm Pm=I r With each r 〜Ri(x[Zi])
individually observed. We use Ri to denote the expectation E[Ri].
Definition 3. (Factored transition) The transition function class P ⊂ P(X, S) is factored over
SXA = X = Xi X…XXd and S = Si X…XSn with scopes Zi,…，Zn if and only if for
all P ∈ P, X ∈ X, s ∈ S, there exist functions {Pj ∈ P (X [Zj] , Sj)}jn=i such that P(s | X) =
Qjn=i Pj (s[j] |X[Zj]).
A factored MDP is an MDP with factored rewards and transitions. A factored MDP is fully
CharaCteriZed by M = ({Xi}d=i; {ZR}m=i; {Ri}m=i; {Sj}n=i ； {ZP}"=i ； {Pj}n=i ； H), Where
X = S X A, {ZiR }im=i and {ZjP }jn=i are the scopes for the reward and transition functions, which
We assume to be knoWn to the agent.
An exCellent example of faCtored MDP is given by Osband & Van Roy (2014), about a large produC-
tion line With d maChines in sequenCe With Si possible states for maChine i. Over a single time-step
eaCh maChine Can only be influenCed by its direCt neighbors. For this problem, the sCopes ZiR and
ZiP of maChine i ∈ {2, ..., d - 1} Can be defined as {i - 1, i, i + 1}, and the sCopes of maChine 1
and maChine d are {1, 2} and {d - 1, d} respeCtively. Another possible example to explain faCtored
MDP is about robotiCs. For a robot, the transition dynamiCs of its different parts (e.g. its legs and
arms) may be relatively independent. In that Case, the faCtored transition Can be defined for eaCh
part separately.
For notation simplicity, we use X[i : j] and S[i : j] to denote X[∪k=i,…,jZk] and Xk=iSk re-
speCtively. Similarly, We use P[i:j] (s0[i : j] | s, a) to denote Qjk=i P(s0 [k]|(s, a)[ZkP]). For every
V : S → R and the right-linear operators P, we define PV (s, a) d=ef Ps0∈S P(s0 | s, a)V (s0). A
state-aCtion pair Can be represented as (s, a) or X. We also use (s, a)[Z] to denote the Corresponding
X[Z] for notation ConvenienCe. We mainly foCus on the Case where the total time step T = KH is
the dominant faCtor, and assume that T ≥ |Xi | ≥ H during the analysis.
3	Related Work
Exploration in Reinforcement Learning ReCent years have witnessed a tremendous of work for
provably effiCient exploration in reinforCement learning, inCluding tabular MDP (e.g., Dann et al.,
2017; AZar et al., 2017; Jin et al., 2018; Zanette & Brunskill, 2019), linear RL (e.g., Jiang et al.,
2017; Yang & Wang, 2019; Jin et al., 2020; Zanette et al., 2020), and RL with general funCtion ap-
proximation (e.g., Osband & Van Roy, 2014a; Ayoub et al., 2020; Wang et al., 2020). For algorithms
in tabular setting, the regret bounds inevitably depend on the Cardinality of state-aCtion spaCe, whiCh
may be exCessively large. Based on the ConCept of eluder dimension (Russo & Van Roy, 2013), many
reCent works proposed effiCient algorithms for RL with general funCtion approximation (Osband &
Van Roy, 2014a; Ayoub et al., 2020; Wang et al., 2020). SinCe eluder dimension of the funCtion
class for factored MDPs is at most O (Pm=I |X[ZR] | + Pn=I ∣X[Zp ]∣),it is possible to apply their
algorithms and regret bounds to our setting, though the direCt appliCation of their algorithms leads
to a loose regret bound.
Factored MDP Episodic FMDP was studied by Osband & Van Roy (2014b), in which they pro-
posed both PSRL and UCRL style algorithm with near-optimal Bayesian and frequentist regret
bound. In non-episodic scenarios, Xu & Tewari (2020) recently generaliZes the algorithm of Os-
band & Van Roy (2014b) to the infinite horiZon average reward setting. However, both their results
suffer from linear dependence on the horiZon (or the diameter) and factored state space’s cardinality.
Concurrent with our work is the recent paper by Tian et al. (2020), which also applies UCBVI
and EULER to factored MDPs. Compared with their results, we propose a more refined variance
3
Published as a conference paper at ICLR 2021
decomposition theorem for factored Markov chains (Theorem 1), which results in a better regret
by a factor of √n; the theorem is also of independent interest with potential use in other problems
in factored MDPs. Furthermore, we formulate the RLwK problem, and provide a sample-efficient
algorithm based on our FMDP algorithm.
Constrained MDP and knapsack bandits The knapsack setting with hard constraints has already
been studied in bandits with both sample-efficient and computational-efficient algorithms (Badani-
diyuru et al., 2013; Agrawal et al., 2016). This setting may be viewed as a special case of RLwK
with H = 1. In constrained RL, there is a line of works that focus on soft constraints where the con-
straints are satisfied in expectation or with high probability (Brantley et al., 2020; Zheng & Ratliff,
2020), or a violation bound is established (Efroni et al., 2020; Ding et al., 2020). RLwK requires
stronger constraints that is almost surely satisfied during the execution of the agents. A more related
setting is proposed by Brantley et al. (2020), which studies a sample-efficient algorithm for knap-
sack episodic setting with hard constraints on all K episodes. However, we require the constraints
to be satisfied within each episode, which we believe can better describe the real-world scenarios.
The setting of Singh et al. (2020) is closer to ours since they are focusing on “every-time” hard
constraints, although they consider the non-episodic case.
4	Main Results
In this section, we introduce our FMDP-BF algorithm, which uses empirical variance to construct
a Bernstein-type confidence bound for value estimation. Besides FMDP-BF, we also propose a
simpler algorithm called FMDP-CH with a slightly worse regret, which follows the similar idea of
UCBVI-CH (Azar et al., 2017). The algorithm and the corresponding analysis are more concise and
easy to understand; details are deferred to Section B.
4.1	Estimation Error Decomposition
Our algorithm will follow the principle of “optimism in the face of uncertainty”. Like ORLC (Dann
et al., 2019) and EULER (Zanette & Brunskill, 2019), our algorithm also maintains both the opti-
mistic and pessimistic estimates of state values to yield an improved regret bound. We use Vk,h
and Vkk,h to denote the optimistic estimation and pessimistic estimation of 以，respectively. To
guarantee optimism, We need to add confidence bonus to the estimated value function Vk,h at each
step, so that Vk,h(s) ≥ Vh=(S) holds for any k ∈ [K], h ∈ [H] and S ∈ S. Suppose Rk,i and
Pk,j denote the estimated value of each expected factored reward Ri and factored transition prob-
ability Pj before episode k respectively. By the definition of the reward R and the transition P,
we use R def ml Pm=I Rk,i and Pk def Q；=i Pk,j as the estimation of R and P. Following the
previous framework, this confidence bonus needs to tightly characterize the estimation error of the
one-step backup R(s, a) + PV= (s, a); in other words, it should compensate for the estimation errors,
(Rk - R) (s, a) and (Pk - P) V=(s, a), respectively.
For the estimation error of rewards (Rk - R) (sk,h, ak,h), since the reward is defined as the average
of m factored rewards, it is not hard to decompose the estimation error of R(s, a) to the average of
the estimation error of each factored rewards. In that case, we separately construct the confidence
bonus of each factored reward Ri. Suppose CBR ” (s, a) is the confidence bonus that compensates
k,Zi
for the estimation error Rk,i — Ri, then we have CBR(s, a) def ml Pm=I CBRZR (s, a).
m	,i
For the estimation error of transition (Pk - P) Vh+ι(sk,h,ak,h), the main difficulty is that Pk
is the multiplication of n estimated transition dynamics Pk,m. In that case, the estimation error
(Pk - P) Vh+ι(sk,h, ak,h) may be calculated as the multiplication of n estimation error for each
factored transition Pk,i, which makes the analysis much more difficult. Fortunately, we have the
following lemma to address this challenge.
4
Published as a conference paper at ICLR 2021
Lemma 4.1. (Informal) Let the transition function class P ∈ P(X , S) be factored over X =
Xi ×∙∙∙× Xd, and S = Si × …×Sn with scopes ZP,…,Zn. For a givenfUnctiOn V : S → R,
the estimation error ofone-step value |(Pk 一 P)V(s, a)| can be decomposed by:
n
|(Pk 一 P)V(s,a)∣ ≤ X (Pk,i- Pi)
i=i
YY Pj)V (s,a)
j6=i,j=i
+ βk,h(s, a)
Here, βk,h(s, a), formally defined in Lemma E.1, are higher order terms that do not harm the order
of the regret. This lemma allows Us to decompose the estimation error (Pk — P) Vh^+ι(sk,h, ak,h)
into an additive form, so we can construct the confidence bonus for each factored transition
Pj separately. Let CBkPZP (s, a) be the confidence bonUs for the estimation error (Pk,j 一
k,Zj
Pj) Qtn6=j,t=i Pt V (s, a).	Then, CBkP(s, a) d=ef Pjn=i CBkP,ZP(s,a) + ηk,h(s, a), where
ηk,h(s, a) collects higher order factors that will be explicitly given later.
Finally, we define the confidence bonUs as the sUmmation of all confidence bonUses for rewards and
transition: CBk (s, a) = CBkR(s, a) + C BkP (s, a).
4.2	Variance of Factored Markov Chains
After the analysis in Section 4.1, the remaining problem is how to define the confidence bonUs
CBkR,ZR (s, a) and CBkP,ZP (s, a). In this sUbsection, we tackle this problem by deriving the vari-
ance decomposition formUla for factored MDP. To begin with, we consider Markov chains with
stochastic factored transition and stochastic factored rewards, and dedUce the Bellman eqUation of
variance for factored Markov chains. The analysis shows how to define the empirical variance in the
confidence bonUs for factored MDP and gives an Upper boUnd on the sUmmation of per-step variance
(Corollary 1.1).
In the Markov chain setting, the reward is defined to be a mapping from S to R. SUppose Jt1:t2 (s)
denotes the total rewards the agent obtains from step ti to step t2 (inclUsively), given that the agent
starts from state s in step ti. Jt1:t2 is a random variable depending on the randomness of the
trajectory from step ti to t2, and stochastic rewards therein. Following this definition of Jt1:t2 , we
define Ji:H to be the total reward obtained dUring one episode. We Use st to denote the random state
that the agent encoUnters at step t. We define ωh2 (s) d=ef E (Jh:H(sh) 一 Vh(sh))2 |sh = s to be
the variance of the total gain after step h, given that sh = s.
We define σR<s) = V [Ri(ξ)∣ξ = s] to be the variance of the i-th factored reward,
given that the cUrrent state is s. Given the cUrrent state s, we define the variance
of the next-state valUe fUnction w.r.t. the i-th factored transition as: σP2 ,i,h(s) d=ef
Esh+1[i:i-i] Vsh+1[i] Esh+1[i+i:n] [Vh+i(sh+i)] | sh = s . That is, for each given s0[1 : i], we
firstly take expectation over all possible valUes of s0 [i + 1 : n] w.r.t. P[i+i:n] . Then, we calcUlate
the variance of transition s0[i]〜 Pi(∙∣(s, a)[Zf ]) given fixed s0[1 : i 一 1]. Finally, We take the
expectation of this variance w.r.t. s0[1 : i 一 1]〜P[i：i—i].
Theorem 1. Forany horizon h ∈ [H], we have ω2(S) = £§, P(s0∣s)ω2+ι(s0) + PZi σp% h(s) +
mi2 Pm=ι σR,i(s).	’'
Theorem 1 generalizes the analysis of MUnos & Moore (1999), which deals with non-factored MDPs
and deterministic rewards. From the Bellman eqUation of variance, we can give an Upper boUnd to
the expected sUmmation of per-step variance.
Corollary 1.1. Suppose the agent takes policy π during an episode. Let wh(s, a) denote the proba-
bility of entering state s and taking action a in step h. Then we have the following inequality:
Hn	m
X X Wh(s,a) XσP,i(V∏+ι,s,a) + 点 XσR,,(s,a) ≤ H2,
h=i (s,a)∈X	i=i	i=i
5
Published as a conference paper at ICLR 2021
where	σR,i(s,a)	=	V 卜i(ξ, Z)∣ξ = s,Z = a]	is the
tored reward given the current state-action pair (s, a),
Esh+1[1:i-1] Vsh+1[i] Esh+1[i+1:n] Vhπ+1(sh+1)	| sh = s is the
transition given current state s.
variance of i-th fac-
and σP2,i(Vhπ+1, s,a) =
variance of i-th factored
This corollary makes it possible to construct confidence bonus with variance for each factored re-
wards and transition separately. Please refer to Section F.2 for the detailed proof of Theorem 1 and
Corollary 1.1.
4.3	Algorithm
Our algorithm is formally described in Alg. 1, with a more detailed explanation in Section C. De-
note by Nk ((s, a)[Z]) the number of steps that the agent encounters (s, a)[Z] during the first k
episodes. In episode k, we estimate the mean value of each factored reward Ri and each fac-
tored transition Pi with empirical mean value Rk,i and Pk,i of the previous history data L respec-
tively. After that, We construct the optimistic MDP MM based on the estimated rewards and transi-
tion functions. For a certain (s, a) pair, the transition function and reward function are defined as
Rk (s,a) = + P= Rk,i((s,a)[ZR]) and Pk(s0 | s,a) = Q；=i P k,j (s0[j] | (s,a) [Zf]).
Algorithm 1 FMDP-BF
5:
Input: δ
L = 0, initialize N((s, a)[Zi]) = 0 for any factored set Zi and any (s, a)[Zi] ∈ X[Zi]
for episode k = 1, 2,•…do
Set Vk,H+ι(s) = Vk,H+ι(S) = 0 for all s, a.
Estimate the empirical mean Rk and Pk with history data L.
for horizon h = H, H - 1, ..., 1 do
for S ∈ S do
10:
for a ∈ A do
Qk,h(S, a) = min{H, Rk(S, a) + CBk(S, a) + PkVk,h+1(S, a)}
end for
∏k,h(s) = argmaxa Qk,h(s, a)
Vk,h(S) = maXa∈A Qk,h(s, a)
max
end for
{0,RRk (S,πk,h(S))- CBk (s,
πk,h
(S)) + PkVk,h+ι(s,πk,h(s))}
15:	end for
Take action according to πk,h for H steps in this episode.
Update L = L S{Sk,h, ak,h, rk,h, Sk,h+1}h=1,2,...,H, and update counter Nk-1((S, a)[Zi]).
end for
Following the analysis in Section 4.1, we separately construct the confidence bonus of each factored
reward Ri with the empirical variance: CBkRZR (S, a) =
k,Zi
[m], where LR =f log (18mT IX[ZR]∣ /δ), and
/ 26R,k,i(Sla)LR +_______8LR
N Nk-ι((s,a)[ZR]) + 3Nk-ι((s,a)[ZR]),
i∈
toredreward Ri, * i∙e∙ σR,k,i(S, a) = Nk-ι((s1,a)[zp])
(Rk,i((S,a)[ZR ]))2.
σR k i is the empirical variance of the i-th fac-
Pt=-I)HI [(St,at)[ZR] = (S,a)[ZR]] ∙吃-
We define Lp = log(18nTSA∕δ) for short. Following the idea of Lemma 4.1, we sepa-
rately construct the confidence bonus of scope ZiP for transition estimation: CBkP,ZP (S, a) =
√4'P ,k,i(V k,h + 1 ,s,α)LP	2Uk,h,i(s,a)LP
~"Nk-I((S,a)[ZP])	+ N Nk-ι((s,O)[ZP]) + nk，h，i(S, a), i ∈ [n], Where Op,k,i(S,a) and
uk,h,i (S, a) are defined later. ηk,h,i (S, a) collects the additional bonus terms that do not affect the
order of the final regret. The precise expression of ηk,h,i(S, a) is deferred to Section C.
6
Published as a conference paper at ICLR 2021
The definition of σp,k,i(Vk,h+ι, s, a) corresponds to σp,i(V∏, s, a) in Corollary 1.1, which
can be regarded as the empirical variance of transition Pk,i: σpk i(Vk,h+ι, s, a) =
Es0[1:i-1]~Pk,[i：i-i](.1s,a) [Vs0[i]~M,i(∙∣(s,a)[Zp ]) (Es0 [i+1:n] ~Pk,[升*] (∙∣ s,a)V k,h+1(s )) ] .
To guarantee optimism, We need to use the empirical variance σp 卜 <V*, s, a) to upper bound the
estimation error in the proof. Since we do not know V * beforehand, we use σp 卜 <V k,h+ι, s, a) as
a surrogate in the confidence bonus. However, we cannot guarantee that σp,k,i(V*, s, a) is upper
bounded by σp 卜 i(Vk,h+ι, s, a). To compensate for the error due to the difference between V*+1
and Vk,h+ι, we add J；：；(：(：；打)to the confidence bonus, where uk,h,i(s, a) is defined as:
uk,h,i (s, a) = Es01：i]〜Pk,[Li](∙∣S,a) (Esi]〜旌,[i+Ln](∙∣S,a) (V k,h+1 —V k,h+1) (S^ .
4.4 Regret
Theorem 2.	Suppose X [ZiR] ≤ JR, X [ZjP] ≤ JP for i ∈ [m], j ∈ [n], then with prob. 1 - δ,
the regret of Alg. 1 is O (PJRT log(mTJR∕δ) log T + PnHJPT log(nTSA∕δ) log T).
Note that the regret bound does not depend on the cardinalities of state and action spaces, but only
has a square-root dependence on the cardinality of each factored subspace X [Zi]. By leverag-
ing the structure of factored MDP, we achieve regret that scales exponentially smaller compared
with that of UCBVI (Azar et al., 2017). The best previous regret bound for episodic factored
MDP is achieved by Osband & Van Roy (2014b). When transformed to our setting, it becomes
O (P JRT log(mTJR∕δ) + nHPrJPT log(nTJP/δ)), where Γ is an upper bound of |Sj |. This
is worse than our results by a factor of VnHr. Concurrent to our results, Tian et al. (2020) also pro-
pose efficient algorithms for episodic factored MDP. When transformed to our setting, their regret
bound is O (√JRT + n√HJPT). Compared with their bounds, we further improve the regret by
a factor of √n with a more refined variance decomposition theorem (Theorem 1).
4.5 Lower Bound
In this subsection, we propose the regret lower bound for factored MDP. The proof of Theorem 3 is
deferred to Section G.
Theorem 3.	Suppose log2 (|Si|) ≤ H for any i ∈ [n], the regret of any algorithm on the factored
MDP problem is lower bounded by Ω (m1 Pm=I J∣X [ZR]∣ T + 1 Pin=1 J∣X ZP ]∣ HT).
The lower bound of Tian et al. (2020) is Ω (max {maxi JIX[ZR]∣ T, maxj JIX[ZP]∣ HT}),
which is derived from different hard instance construction. Their lower bound is of the same order
with ours, while our bound measures the dependence on all the parameters including the number of
the factored transition n and the factored rewards m. If IX[ZiR]I = JR and IX [ZiP]I = JP, the
lower bound turns out to be Ω (√ JRT + √HJPT), which matches the upper bound in Theorem 2
except for a factor of √n and logarithmic factors.
5	RL with Knapsack Constraints
In this section, we study RL with Knapsack constraints, or RLwK, as an application of FMDP-BF.
5.1	Preliminaries
We generalize bandit with knapsack constraints or BwK (Badanidiyuru et al., 2013; Agrawal et al.,
2016) to episodic MDPs. We consider the setting of tabular episodic Markov decision process,
7
Published as a conference paper at ICLR 2021
(S, A, H, P, R, C), which adds to an episodic MDP with a d-dimensional stochastic cost vector,
C(s, a). We use Ci(s, a) to denote the i-th cost in the cost vector C(s, a). If the agent takes action
a in state s, it receives reward r sampled from R(s, a), together with cost c, before transitioning to
the next state s0 with probability P(s0|s, a). In each episode, the agent’s total budget is B. We also
use Bi to denote the total budget of i-th cost. Without loss of generality, we assume Bi ≤ B for
all i. An episode terminates after H steps, or when the cumulative cost Ph ch,i of any dimension
i exceeds the budget Bi , whichever occurs first. The agent’s goal is to maximize its cumulative
reward PkK=1 PhH=1 rk,h in K episodes.
5.2	Comparison with Other Settings
While RLwK might appear similar to episodic constrained RL (Efroni et al., 2020; Brantley et al.,
2020), it is fundamentally different, so those algorithms cannot be applied here.
As discussed in Section 3, the episodic constrained RL setting can be roughly divided into two cat-
egories. A line of works focus on soft constraints where the constraints are satisfied in expectation,
i.e. PhH=1 E[ck,h] ≤ B. The expectation is taken over the randomness of the trajectories and the
random sample of the costs. Another line of work focuses on hard constraints in K episodes. To be
more specific, they assume that the total costs in K episodes cannot exceed a constant vector B, i.e.
PkK=1 PhH=1 ck,h ≤ B. Once this is violated before episode K1 < K, the agent will not obtain any
rewards in the remaining K - K1 episodes. Though both settings are interesting and useful, they
do not cover many common situations in constrained RL. For example, when playing games, the
game is over once the total energy or health reduce to 0. After that, the player may restart the game
(starting a new episode) with full initial energy again. In robotics, a robot may episodically interact
with the environment and learn a policy to carry out a certain task. The interaction in each episode
is over once its energy is used up. In these two examples, we cannot just consider the expected
cost or the cumulative cost across all episodes, but calculate the cumulative cost in every individual
episode. Moreover, in many constrained RL applications, the agent’s optimal action should depend
on its remaining budget. For example, in robotics, the robot should do planning and take actions
based on its remaining energy. However, previous results do not consider this issue, and use poli-
cies that map states to actions. Instead, in RLwK, we need to define the policy as a mapping from
states and remaining budget to actions. Section H gives further details, including two examples for
illustrating the difference between these settings.
5.3	Algorithm
We make the following assumptions about the cost function for simplicity. Both of them hold if all
the stochastic costs are integers with an upper bound.
Assumption 1. The budget Bi as well as the possible value of costs Ci (s, a) of any state s and
action a is an integral multiple ofthe unit cost .
Assumption 2. The stochastic cost Ci (s, a) has finite support. That is, the random variable
Ci(s, a) can only take at most n possible values.
The reason for Assumption 2 is that we need to estimate the distribution of the cost, instead of just
estimating its mean value. We discuss the necessity of the assumptions and the possible methods for
continuous distribution in Section H.3.
From the above discussion, we know that we need to find a policy that is a mapping from state and
budget to action. Therefore, it is natural to augment the state with the remaining budget. It follows
that the size of augmented state space is S ∙ (Bm)d. Directly applying UCBVI algorithm (Azar
et al., 2017) will lead to a regret of order O (PHSAT(Bmd). Our key observation is that the
constructed state representation can be represented as a product of subspaces. Each subspace is rela-
tively independent. For example, the transition matrix over the original state space S is independent
of the remaining budget. Therefore, the constructed MDP can be formulated as a factored MDP, and
the compact structure of the model can reduce the regret significantly.
By applying Alg. 1 and Theorem 2 to RLwK, we can reduce the regret to the order of
O (HSSA+ + dBm)rT) roughly, which is exponentially smaller. However, the regret still de-
8
Published as a conference paper at ICLR 2021
pends on the total budget B and the discretization precision m, which may be very large for contin-
uous budget and cost. Another observation to tackle the problem is that the cost of taking action a on
state s only depends on the current state-action pair (s, a), but has no dependence on the remaining
budget B. To be more formal, we have bh+1 = bh - ch, where bh is the remaining budget at step
h, and ch is the cost suffered in step h. As a result, we can further reduce the regret to roughly
O ZHdSAT^ by estimating the distribution of cost function. A similar model has been discussed
in Brunskill et al. (2009), which is named as noisy offset model.
Our algorithm, which is called FMDP-BF for RLwK, follows the same basic idea of Alg. 1. We
defer the detailed description to Section H to avoid redundance. The regret can be upper bounded
by the following theorem:
Theorem 4. With prob. at least 1 - δ, the regret of Alg. 4 is upper bounded by
O NdHSAT (log(SAT) + dlog(Bm)))
Compared with the lower bound for non-factored tabular MDP (Jaksch et al., 2010), this regret
bound matches the lower bound w.r.t. S, A, H and T. There may still be a gap in the dependence of
the number of constraints d, which is often much smaller than other quantities.
It should be noted that, though we achieve a near-optimal regret for RLwK, the computational com-
plexity is high, scaling polynomially with the maximum budget B, and exponentially with the num-
ber of constraints d. This is a consequence of the NP-hardness of knapsack problem with multiple
constraints (Martello, 1990; Kellerer et al., 2004). However, since the policy is defined on the state
and budget space with cardinality SBd, this computational complexity seems unavoidable. How to
tackle this problem, such as with approximation algorithms, is an interesting future work.
6	Conclusion
We propose a novel RL algorithm for solving FMDPS with near optimal regret guarantee. It im-
proves the best previous regret bound by a factor of，nH|Si|. We also derive a regret lower bound
for FMDPs based on the minimax lower bound of multi-armed bandits and episodic tubular MDPs
(Jaksch et al., 2010). Further, we formulate the RL with Knapsack constraints (RLwK) setting, and
establish the connections between our results for FMDP and RLwK by providing a sample efficient
algorithm based on FMDP-BF in this new setting.
A few problems remain open. The regret upper and lower bounds have a gap of approximately √n,
where n is the number of transition factors. For RLwK, it is important to develop a computationally
efficient algorithm, or find a variant of the hard-constraint formulation. We hope to address these
issues in the future work.
Acknowledgements
This work was supported by Key-Area Research and Development Program of Guangdong Province
(No. 2019B121204008)], National Key R&D Program of China (2018YFB1402600), BJNSF
(L172037) and Beijing Academy of Artificial Intelligence.
References
Shipra Agrawal, Nikhil R Devanur, and Lihong Li. An efficient algorithm for contextual bandits
with knapsacks, and an extension to concave objectives. In Conference on Learning Theory, pp.
4-18,2016.
Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement
learning with value-targeted regression. In International Conference on Machine Learning, pp.
463-474. PMLR, 2020.
Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforce-
ment learning. arXiv preprint arXiv:1703.05449, 2017.
9
Published as a conference paper at ICLR 2021
Ashwinkumar Badanidiyuru, Robert Kleinberg, and Aleksandrs Slivkins. Bandits with knapsacks.
In 2013 IEEE 54th Annual Symposium on Foundations ofComputer Science, pp. 207-216. IEEE,
2013.
Craig Boutilier, Richard Dearden, and Moises Goldszmidt. Stochastic dynamic programming with
factored representations. Artificial intelligence, 121(1-2):49-107, 2000.
Kiante Brantley, Miroslav Dudik, Thodoris LykoUris, Sobhan Miryoosefi, Max Simchowitz, Alek-
sandrs Slivkins, and Wen Sun. Constrained episodic reinforcement learning in concave-convex
and knapsack settings. arXiv preprint arXiv:2006.05051, 2020.
Emma Brunskill, Bethany R. Leffler, Lihong Li, Michael L. Littman, and Nicholas Roy. Provably
efficient learning with typed parametric models. Journal of Machine Learning Research, 10(68):
1955-1988, 2009. URL http://jmlr.org/papers/v10/brunskill09a.html.
Christoph Dann, Tor Lattimore, and Emma Brunskill. Unifying pac and regret: Uniform PAC
bounds for episodic reinforcement learning. In Advances in Neural Information Processing Sys-
tems, pp. 5713-5723, 2017.
Christoph Dann, Lihong Li, Wei Wei, and Emma Brunskill. Policy certificates: Towards accountable
reinforcement learning. In International Conference on Machine Learning, pp. 1507-1516, 2019.
Dongsheng Ding, Xiaohan Wei, Zhuoran Yang, Zhaoran Wang, and Mihailo R Jovanovic. Provably
efficient safe exploration via primal-dual policy optimization. arXiv preprint arXiv:2003.00534,
2020.
Kefan Dong, Yuanhao Wang, Xiaoyu Chen, and Liwei Wang. Q-learning with UCB exploration is
sample efficient for infinite-horizon MDP. arXiv preprint arXiv:1901.09311, 2019.
Yonathan Efroni, Shie Mannor, and Matteo Pirotta. Exploration-exploitation in constrained MDPs.
arXiv preprint arXiv:2003.02189, 2020.
Carlos Guestrin, Daphne Koller, Ronald Parr, and Shobha Venkataraman. Efficient solution algo-
rithms for factored MDPs. Journal of Artificial Intelligence Research, 19:399-468, 2003.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. Journal of Machine Learning Research, 11:1563-1600, 2010.
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Con-
textual decision processes with low Bellman rank are PAC-learnable. In International Conference
on Machine Learning, pp. 1704-1713, 2017.
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is Q-learning provably effi-
cient? In Advances in Neural Information Processing Systems, pp. 4863-4873, 2018.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement
learning with linear function approximation. In Conference on Learning Theory, pp. 2137-2143.
PMLR, 2020.
Michael Kearns and Daphne Koller. Efficient reinforcement learning in factored MDPs. In IJCAI,
volume 16, pp. 740-747, 1999.
Hans Kellerer, Ulrich Pferschy, and David Pisinger. Multidimensional knapsack problems. In Knap-
sack problems, pp. 235-283. Springer, 2004.
Tor Lattimore and Csaba Szepesvari. Bandit algorithms. Cambridge University Press, 2020.
Lihong Li. A unifying framework for computational reinforcement learning theory. PhD thesis,
Rutgers University-Graduate School-New Brunswick, 2009.
Xiuyuan Lu and Benjamin Van Roy. Information-theoretic confidence bounds for reinforcement
learning. In Advances in Neural Information Processing Systems, pp. 2461-2470, 2019.
Silvano Martello. Knapsack problems: algorithms and computer implementations. Wiley-
Interscience series in discrete mathematics and optimiza tion, 1990.
10
Published as a conference paper at ICLR 2021
Remi Munos and Andrew Moore. Influence and variance of a Markov chain: Application to adaptive
discretization in optimal control. In Proceedings of the 38th IEEE Conference on Decision and
Control (Cat. No. 99CH36304), volume 2, pp. 1464-1469. IEEE, 1999.
Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the eluder dimension.
arXiv preprint arXiv:1406.1853, 2014a.
Ian Osband and Benjamin Van Roy. Near-optimal reinforcement learning in factored MDPs. In
Advances in Neural Information Processing Systems, pp. 604-612, 2014b.
Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic
exploration. In NIPS, pp. 2256-2264. Citeseer, 2013.
Rahul Singh, Abhishek Gupta, and Ness B Shroff. Learning in Markov decision processes under
constraints. arXiv preprint arXiv:2002.12435, 2020.
Alexander L. Strehl, Lihong Li, and Michael L. Littman. Reinforcement learning in finite MDPs:
PAC analysis. Journal of Machine Learning Research, 10:2413-2444, 2009.
Yi Tian, Jian Qian, and Suvrit Sra. Towards minimax optimal reinforcement learning in factored
Markov decision processes. arXiv preprint arXiv:2006.13405, 2020.
Ruosong Wang, Russ R Salakhutdinov, and Lin Yang. Reinforcement learning with general value
function approximation: Provably efficient approach via bounded eluder dimension. Advances in
Neural Information Processing Systems, 33, 2020.
Tsachy Weissman, Erik Ordentlich, Gadiel Seroussi, Sergio Verdu, and Marcelo J Weinberger. In-
equalities for the L1 deviation of the empirical distribution. Hewlett-Packard Labs, Tech. Rep,
2003.
Ziping Xu and Ambuj Tewari. Near-optimal reinforcement learning in factored MDPs: Oracle-
efficient algorithms for the non-episodic setting. arXiv preprint arXiv:2002.02302, 2020.
Lin F Yang and Mengdi Wang. Sample-optimal parametric q-learning using linearly additive fea-
tures. arXiv preprint arXiv:1902.04779, 2019.
Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement
learning without domain knowledge using value function bounds. In International Conference on
Machine Learning, pp. 7304-7312. PMLR, 2019.
Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near
optimal policies with low inherent bellman error. arXiv preprint arXiv:2003.00153, 2020.
Liyuan Zheng and Lillian J Ratliff. Constrained upper confidence reinforcement learning. arXiv
preprint arXiv:2001.09377, 2020.
11
Published as a conference paper at ICLR 2021
A Notations
Before presenting the proof, we restate the definition of the following notations.
Symbol
Sk,h, ak,h
LP
LiR
XiP
XiR
Nk((S,a)[Z])
PV (S, a)
φk,i (S, a)
Explanation
The state and action that the agent encounters in episode k and step h
log(18nTSA∕δ)
log (l8mT∣X[ZR]∣∕δ)
||X [ZiP]||
∣∣X[ZiR]∣∣
the number of steps that the agent encounters (S, a)[Z] during the first k episodes
A shorthand of s0 ∈S
/	4∣Sj∣LP — +
N Nk-ι((s,a)[ZP]) +
P(s0|s,a)V (s0)
4∣Sj∣LP ..
3Nk-ι((s,a)[ZP ])
P,i
(V, s, a)
σp,k,i(V, s, a)
◎k,h
Ω
wk,h,Z (s, a)
wk,Z (s, a)
wk,h(s, a)
wk(s, a)
The empirical variance of reward Ri
the next state variance of PV for the transition Pi ,
i.e. ES0[1：i—1]~P[i：i]G[s,a) [Vs0[i]~Pi(∙∣(s,a)[ZP]) (Es0[i+1：n]~p[i+i：n](TS,a)V(S D]
the empirical next state variance of PkV for the transition Pk,i,
i∙e∙ Es011:i-1]~Pk,[Li](∙∣s,a) [Vs0[i]~Pk,i(∙∣(s,a)[zp ]) (Es0 匕1：句』,(∙∣s,α) V(S'))]
The optimism and pessimism event for k, h: {Vk,h ≥ Vh= ≥ Vk,h}
The optimism and pessimism events for all 1 ≤ k ≤ K, 1 ≤ h ≤ H, i.e. ∪k,hΩk,h
The probability of entering (S, a)[Z] at step h in episode k
PhH=
1wk,h,Z (S, a)
The probability of entering (S, a) at step h in episode k,
i.e. wk,h,Z (S, a) with Z = {1, 2, ..., d}
PH=I wk,hGa)
B Omitted details for FMDP-CH
In this section, we introduce our algorithm with Hoeffding-type confidence bonus and present the
corresponding regret bound. Our algorithm, which is described in Algorithm 2, is related to UCBVI-
CH algorithm (Azar et al., 2017), in the sense that Algorithm 2 reduces to UCBVI-CH ifwe consider
a flat MDP with m = n = d = 1.
Let Nk ((S, a)[Z]) denote the number of steps that the agent encounters (S, a)[Z] during the
first k episodes, and Nk ((S, a)[Zj], Sj) denotes the number of steps that the agent transits to a
state with S[j] = Sj after encountering (S, a)[Zj] during the first k episodes. In episode k,
we estimate the mean value of each factored reward Ri and each factored transition Pi with
empirical mean value Rk,i and Pk,i respectively. To be more specific, Rk,i((s,a)[ZR]) =
Pt≤(k-1)H W(St，at)[Z丹=(S，a)[Z丹] ∙rt,i
Nk-1((s,a)[zR])
Pkj(Sj]∣(s,a)[ZP]) = :
, where rt,i denotes the reward Ri sampled in step t, and
NNI((SfZP]j]). After that, We construct the optimistic MDP MM based
on the estimated rewards and transition functions. For a certain (S, a) pair, the transition func-
tion and reward function are defined as Rk(s, a) = ml Pm=I Rk,i((s, a)[ZR]) and Pk(s0 | s,a)=
口；=1 Pkj (s0[j]∣ (s,a)[ZP]).	"ɪ —'
We define LR =
/	4∣Si∣LP — +
V Nk-1 ((s,a)[zp]) +
log (18mT|X[ZR]∣ /δ), LP
log (18nTSA∕δ) and φk,i(s, a)
3N 4[S⅞zP]). We separately construct the confidence bonus of each fac-
σ
σ
2
12
Published as a conference paper at ICLR 2021
Algorithm 2 FMDP-CH
Input: δ,
history data L = 0, initialize N((s, a)[Zi]) = 0 for any factored set Zi and (s, a)[Zi] ∈ X[Zi]
for episode k = 1, 2, ... do
Set 1Vk,H+ι(s) = 0 for all s.
5:	Estimate Rk,i(s,a) With empirical mean value if Nk-ι((s, a)[ZR]) > 0, otherwise
Rk,i(s, a) = 1, then calculate RR(s, a) = + Pm=I Ri((S, a)[ZR])
Let KP = {(S, a) ∈ S × A, ∪i∈[n]Nk ((S, a)[ZP D > 0}
Estimate Pk (∙∣s, a) with empirical mean value for all (s, a) ∈ KP
for horizon h = H, H - 1, ..., 1 do
for all (S, a) ∈ S × A do
10:	if (S.a) ∈ KP then
Qk,h(S, a) = min{H, Rk (S, a) + CBk(S,a) +PkVk,h+1(S,a)}
else
Qk,h(S, a) = H
end if
15:	Vk,h(S) = maxa∈A Qk,h(S, a)
end for
end for
for step h = 1,…，H do
Take action ak,h = arg maxa Qk,h(Sk,h, a)
20:	end for
Update history trajectory L = L S{Sk,h, ak,h, rk,h, Sk,h+1}h=1,2,...,H, and update history
counter Nk-1((S, a)[Zi]).
end for
tored reward Ri and factored transition Pi in the following way:
CBRZRGa)=SN二S‰Rj,	i ∈ [m]
一 D ， 、 I	2H 2LP	一，、，，，、
CBkZP(s，a) =y Nk ]((s a)[zP]) + Hφk,i(s, a) E φk,j(S，a)，
i ∈ [n]
(1)
(2)
We define the confidence bonus as the summation of all confidence bonus for rewards and transition,
Le. CBk (s,a) =* PZi CBRZR G。) + P；=1 CBPZP ⑶办
We propose the following regret upper bound for Alg. 2.
Theorem 5. With prob. 1 - δ, the regret of Alg. 2 is upper bounded by
(1 m ________________________________ n _____________________________∖
-X JIX[ZR]∣Tlog(mT|X[ZR]∣∕δ) + XH√∣X[zp]∣Tlog(nTSA∕δ)
m i=1	j=1
Here O hides the lower order terms with respect to T.
C Omitted Details in Section 4
In this section, we clarify the omitted details in Section 4. The detailed algorithm is described
in Alg. 3. we denote Nk ((S, a)[Z]) as the number of steps that the agent encounters (S, a)[Z]
during the first k episodes, and Nk ((S, a)[Zj], Sj) as the number of steps that the agent tran-
sits to a state with S[j] = Sj after encountering (S, a)[Zj] during the first k episodes. In
episode k, we estimate the mean value of each factored reward Ri and each factored transition Pi
with empirical mean value Rk,m and Pk,m respectively. To be more specific, RRk,m((s, a)[ZR])=
13
Published as a conference paper at ICLR 2021
Pt≤(k-1)H W(St，at)[Z川=(S，a)[Z川] ∙rt,i
Nk-1((s,a)[zR])
Pkj(Sj]∣(s,a)[ZPD =-
, where rt,i denotes the reward Ri sampled in step t, and
The formal definition of the confidence bonus for Alg. 1 is:
2	,	、	∕^^2σR7I(sΓα)L^
CBRZR(s,a)=V NkRIk(is,a)[ZiR]) +
8LR
3Nk—ι((s,a)[ZR])
(3)
CBP t 4 _ ∕4σP,k,i(Vk,h+ι,s,a)Lp
CBkZP(S，a)=V	Nl((S,a)[ZP ])
2 2uk,h,i(s, a)LP
N Nkτ((s,a)[ZP])
(4)
4	4|Sj ILP	!1+ S 4|Sj ILP ʌ
〈Nk—ι((s,a)[Zp])] +y 3Nk-1(s,a)[ZP] j
(5)
n
+	Hφk,i (S, a)φk,j (S, a),	(6)
j=1
where φk,i(s, a) = ʌ/ Nk-4(Sj,L)[zP^+
3Nk-1S⅛p[zP]). Thedefinition ofηk,h,i(s, a) is
16H 2LP
Nk—ι((s,a)[ZP ])
n
X
j=1
(4Nk-4(S⅛[ZP])!	+ SI3Nk4%,LP[ZP]j +XHφk,i(s, a)φkj(S, a).

+
Theorem 6. (Refined Statement of Theorem 2) With prob. at least 1 - δ, the regret of Alg. 1 is upper
bounded by
O
∖
n
X HlX [ZP ]∣Τ lοg(nTSA∕δ)lοg T
i=1
For clarity, we also present a cleaner single-term regret bound under a symmetric problem setting.
Suppose M is a set of factored MDP with m = n, ISiI = Si, IXiI = SiAi and IZiRI = IZjP I = ζ
for i = 1, ..., m andj = 1, ..., n, we write Xi = (SiAi)ζ and assume that Xi ≤ J and Si ≤ Γ.
Corollary 6.1. Suppose M * ∈ M, with prob. 1 一 δ ,the regret of FMDP-BF is upper bounded by
O (PnHJT log(nTSA∕δ)).
The minimax regret bound for non-faCtored MDP is O ({HSAT log(SAT∕δ)). Compared with
this result, our algorithm’s regret is exponentially smaller when n and ζ are relatively small. Under
this problem setting, the regret of Osband & Van Roy (2014b) is O (nHPrJT log(n JT)). Our
results is better by a factor of √nHT.
D High Probability Events
In this section, we discuss the high-prob. events, and assume that these events happen during the
proof.
14
Published as a conference paper at ICLR 2021
Algorithm 3 FMDP-BF (Detailed Description of Alg. 1)
Input: δ
L = 0, initialize N((s, a)[Zi]) = 0 for any factored set Zi and any (s, a)[Zi] ∈ X[Zi]
for episode k = 1, 2,•…do
Set Vk,H+ι(s) = VkH+i(s)=0 foralls,a.
5:	Let K = {(s,a) ∈ S × A ： ∩i=ι,...,nNk((s,a)[Zp]) > 0}
Estimate Rki(S, a) as the empirical mean if Nk-1((s, a)[ZR]) > 0, and 1 otherwise
R(s,a) = m1 Pi=I Ri((S,a)[ZR])
Estimate Pk (∙∣s,α) with empirical mean value for all (s,a) ∈ K
for horizon h = H, H - 1, ..., 1 do
10:	for S ∈ S do
for a ∈ A do
if (S, a) ∈ K then
Qk,h(S, a) = min{H, Rk(S, a) + CBk(S, a) + PkV k,h+1(S, a)}
else_
15:	Qk,h(s,a) = H
end if
20:
end for
πk,h(S)
arg maxa
V k,h (S) = maXa∈A
.Qk,h(s,a)
Qk,h(s,a)
τ τ ∕∖	∖ ι-∖ A /	∕∖∖ cc∕	/W . πτ⅛ τ λ	/	/
Vk,h(S) = max ∣0, Rk(s,∏k,h(s)) — CBk(s,∏k,h(s)) + PkVk,h+ι(s,∏k,h(s
end for
end for
for step h = 1,…，H do _
Take action ak,h = argmax。Qk,h(sk,h, a)
25:	end for
Update history trajectory L = L S{Sk,h, ak,h, rk,h, Sk,h+1}h=1,2,...,H, and update history
counter Nk-1((S, a)[Zi]).
end for
Lemma D.1. (High prob. event) With prob. at least 1 一 2δ∕3, the following events hold for any
k, h, S, a:
∣Rk,i((s, a)[ZR]) - Ri((s, a)[ZR])∣ ≤ tnLR)[Z阴，i ∈ H	(7)
n	2H2LP
∣Pk,i∏Pk,jVT(s,a) - ∏PjVr (s,a)l ≤ N Nk_x((s a)[zP]),	i ∈ [n]	(8)
j 6=i	j=1	-	i
I(Pk,i-Pk，i)(・|(s,a)[zp])|1 ≤ 2SNk-E)[zpj + 3Nk-7S⅝⅛[Zpn	i ∈[n]	⑼
I(PkL Pk,i)(Sl(S,a)[zp])|S S2PiNMTaP + 3Nk-i(LPa)[ZP])	i ∈ [n] (IO)
K H
XX P
Vk,h+1 一 Vπ+ι) (Sk,h, ak,h) 一 (^y,h,^1l 一 V∏+ι) (Sk,h+ι)) ≤ P2HTlog(18SAT)
k=1 h=1
(11)
K H
XX P
Vk,h+1 一 Vr+1)(Sk,h, ak,h) 一 (^y,h,^1l 一 Vh+ι) (Sk,h+ι)) ≤ P2HTlog(18SAT)
k=1 h=1
(12)
We define the above events as Λ1 , and assume it happens during the proof.
15
Published as a conference paper at ICLR 2021
Proof. By Hoeffding’s inequality and union bounds over all i ∈ [m], step k ∈ [K] and (s, a) ∈
X[ZR], We know that Inq. 7 holds with prob. 1 - δ for any i ∈ [m],k ∈ [K], (s, a) ∈ X[ZP].
Similarly, by Hoeffding’s inequality and union bounds over all i ∈ [n], step t and (s, a) ∈ X,
Inq. 8 also holds with prob. 1 - 9 for any i, s, a, k. Inq. 9 is the high probability bound on the Li
norm of the Maximum Likelihood Estimate, which is proved by Weissman et al. (2003). Inq. 10
can be proved with the use of Bernstein inequality and union bound (See Azar et al. (2017) for a
similar derivation). Inq. 11 and Inq. 12 can be regarded as the summation of martingale difference
sequences, which can be derived with the application of Azuma’s inequality. Finally, we take union
bounds over all these inequalities, which indicates that Λι holds with prob. at least 1 - 2δ∕3. □
For the proof of Thm. 2, we also need to consider the following high-prob. events. We define the
following events as Λ2. During the proof of Thm. 2, we assume both Λ1 and Λ2 happen.
Lemma D.2. With prob. at least 1 — δ/3, the following events hold for any k, h, s, a:
Mi((S,a)[ZR]) - Rk,l((s,a)[ZR])∖ ≤ ∖ERiiaLR^ +^	8^,i ∈ [m]
Nk-1((s, a)[ZiR])	3Nk-1((s, a)[ZiR])
(13)
ʌ	L	∕2σP i(Vh* 1, s,a)LP	2HLp
(PkL Pi) YPM+i ZI⅛≡ + 3Nk-i(…),i ∈ [n] (* 14)
Nk((s,a)[ZP]) ≥ 2 X Wj,zp (s,a) - Hlog(18nXpH∕δ),i ∈ [n]	(15)
j<k
Proof. Inq. 13 can be proved directly by empirical Bernstein inequality. Now we mainly focus on
Inq. 14. By Bernstein’s inequality and union bounds over all s, a, k, h, we know that the following
inequality holds with prob. at least 1 - 99.
(Pk,i- Pi) ∏Pj-Vh+i(s,a)
j6=i
∖n
X P(s0[1: i - 1]∣s,a) (Pk,i- Pi) ∏ PjVh+ι(s,a)
s0[Li-1]∈X [i:i-i]	j=i+1
≤
Σ
s0[1:i—1]∈X [i:i-1]
P(S0[1 : i - 1]|S, a)t
2Vars0[i]〜Pi(∙∣(s,α)[zp]) (Es0[i+Ln]〜P[i+Ln](∙∣s,α)Vh+1(SO) | s0[1 : i - 1]) LP
Σ
s0 [1:i— 1] ∈X [1:i—1]
+
2HLp
P(S [j- 1]|S,a)3Nk-i((s,a)[ZP])
/ ∕2σ2 *(Vh*+1,s,a)Lp	2HLp
≤V Nk-i((s,a)[ZP])) + 3Nk-i((s,a)[ZP])
The last inequality is due to Jensen’s inequality. That is,
Σ
s0[1:i—1]
I	Ci	~ < X	CiP(s0[1: i- 1])
V Nk-i((s,a)[Zp]) ≤t s,念i] Nk-i((s,a)[Zp])
= \/2a2(Vh+I,S,a)LP ∙ Nk-i((s1a)[Zip])
where P(S0[1 : i - 1]) is a shorthand ofP(S0[1 : i - 1]|S, a), and Ci here denotes
2 Vars0[i]〜Pi(∙∣(s,α)[Zp]) (Es0[i+Ln]〜P[i+Ln](∙∣s,α)V(SO) | s0[1 : i - 1]) LP∙
Inq. 15 follows the same proof of the failure event F N in section B.1 ofDann et al. (2019).	□
16
Published as a conference paper at ICLR 2021
E Proof of Theorem 5
E.1 ESTIMATION ERROR DECOMPOSITION
Lemma E.1. The estimation error can be decomposed in the following way:
n
|(Pk- P)(∙∣s,α)∣ι ≤ X |(Pk,i - Pi)(∙∣(s,α)[Zip])∣ι	(16)
i=1
n	( n	∖
∣(Pk-P)V(S,α)∣≤ X (Pk,i-Pi) I Y Pj) V (s,α)
i=1	V≠i,j=1	)
nn
+ XX ∣V ∣∞∣(Pfc,i - P) (∙∣(s, a)[ZP ]) I 1∙∣(P k,j - Pj) (∙∣(s, a)[ZP ]) ∣ ɪ,
i=1 j=i,j=1
(17)
here V denotes any value function mappingfrom S to R, e.g. Vj++ι or Vk,九+1 - 以十「
Proof. Inq. 16 has the same form of Lemma 32 in Li (2009) and Lemma 1 in Osband & Van Roy
(2014b). We mainly focus on Inq. 17. We can decompose the difference in the following way:
∣ (Pk - P)V(s,α) ∣
n-1
≤ (Pk,n - Pn) Y PiV(s,a)
i=1
n-1	n-1
+ Pn( Y Pk,i - Y Pi)V(S,a) +
i=1	i=1
n-1	n-1
(Pk,n - Pn)( Y Pk,i - Y Pi)V*(s,a)
i=1	i=1
(18)
For the last term of Inq. 18, we have
n-1	n-1
(Pk,n - Pn)( ^ɪ Pk,i - ^ɪ Pi)V(S,a)
i=1	i=1
n-1	n-1
≤ ∣ (Pk,n - Pn) (∙∣(s, a)[ZP ]) ∣ ι∙ Y P k,i (∙∣S, a[ZP ]) - Y Pi(∙∣s, a[Zf ]) ∙ ∣V∣∞
i=1	i=1	1
n-1
≤ ∣ (Pk,n - Pn) (∙∣(s, a)[ZP]) IlX ∣ (Pk,i - P, (∙∣(s,a)[Zip]) ∣ ɪ ∙∣V∣∞,
Where the last inequality is due to Inq. 16.
For the second part of Inq. 18, we can further decompose the term as:
(n-1	n-1	∖
Y Pk,i - Y Pi V(s,a)
i=1	i=1
n-2
≤ Pn (Pk,n-1 - Pn-1) Y PiV(s,a) + PnPn-I
i=1
n-2	n-2
Y Pk,i - Y Pi V(s,a)
i=1	i=1
n-2	n-2
Y Pk,i - Y Pi V(s,a)
i=1	i=1
(19)
Following the same decomposition technique, we can prove Inq. 17 by recursively decomposing the
second term over all possible n:
n	/ n	∖
∣(Pk - P)V *(s,a)∣≤ X (Pk,i - Pi) I Y Pj) V (s,a)
i=1	V=i,j=1	)
nn
+ X X ∣ (Pk,i - Pi) (∙∣(s,a)[Zip ]) ∣ ι∙∣(Pk,j - Pj) (∙∣(s,a)[Zjp]) ∣ ɪ ∙∣V ∣∞
i=1 j=i,j=1
17
Published as a conference paper at ICLR 2021
□
Lemma E.2. Under event Λ∖,then thefollowing Inequality holds:
ʌ z 、〜ι 1 P I	2Lr
IRk(S, a) - RG α)l≤ m 二 V NkT((S,a)[ZR])
4∖Si∖LP
3Nk-i((s,a)[ZΡ ])
(20)
(21)
n n
+ XX H
i=1 j=,j = 1
4∖Si∖LP
Nk-1((s,α)[Zf ])
4∖Si∖LP	∖ 4 /	4∖SjILP	. 1	4∖Sj∖LP
3Nk-i((s,a)[ZΡ]))	Nl((SM[Zjp]) + 3Nk-1((s,a)[Zjp])
(22)
y
+
Proof. Inq. 20 can be proved by Lemma D.1:
ʌ	.	.	1ι .	1 1 I	2Lr
IRk Ga)- R(S,a) ∖ ≤ m X IRk (S,a - RGa) ∖ ≤ m X N NkT((S,a)[ZR ])
Inq. 21 follows directly by applying Lemma D.1 to Lemma E.1.
n
∖(Pk- P)(∙∖s,a)∖ι ≤ X ∖(Pk,i - Pi)(∙∖(s,a)[Zf ])∖1
i=1
4∖Si∖Lp
Nk-i((s,a)[ZP ])
4∖Si∖LP
3Nk-1 ((s,a)[ZP ])
+
Similarly, Inq 22 can be proved by:
n	/ n	∖
≤ X (Pk,i - Pi) I ∏ Pj) V*(s,a)
i=1	∖j≠i,j = 1	)
nn
+ X X HI(Pk,i- Pi) (∙∖(s,a)[Zip ]) I ι∙∣(Pk,j - Pj)(∙∖(s,a)[Zjp ]) L
i=1 j=i,j = 1
≤ X/
i=1
2H 2Lp
nn
+X X H
i=1 j=i,j = 1	∖ V
4∖Si∖Lp
4∖Si∖Lp
Nk-ι((s,a)[Zp ]) + 3Nk-ι((s,a)[Zp ])
)tf
4∖SjILP
4∖Sj ILP
□
E.2 Optimism
LemmaE.3. (Optimism) Under event Λι, V；,%(s) ≥ Vh(S) forany k, h, s.
Proof. We prove the Lemma by induction. Firstly, for h = H + 1, the inequality holds trivially
since Vk,h+ι(s) = VH +ι(s) = 0.
%,h(s)- VnS)
≥Rk(s,π^(s))+ CBk(s,域(s))+ Pk%,h+ι(s,π<(s)) - R(s,π‹(s)) - P*ι(s,^(s))
ʌ .... — , .... .... ʌ , ʌ . , .... , ʌ ....
=Rk(s,π^(s)) - R(s,π‹(s)) + CBk(S,需(s)) + PkCl4,%+ι - V⅛i)(s,域(S)) + (Pk- P)%ι(s, π*(s))
ʌ .... 一 . .... . .... . ʌ . . ....
≥Rk(s,唁(s)) - R(s,唁(s)) + CBk(s,π^,(s)) + (Pk- P)‰(s,π*(s))
≥0
18
Published as a conference paper at ICLR 2021
EI r∙ . ∙	ι ∙ . ∙ ι . τ1τ / ∖ ⅛ z-ι /	⅛ / ∖ ∖ EI	ι ∙	ι ∙ . /` n	ι ∙ ι . ∙
The first inequality is due to Vk,h(s) ≥ Qk,h(s, ∏h(s)). The second inequality follows by induction
condition that Vk,h+ι(s) ≥ Vh+ι(s) for all s. The last inequality is due to Inq. 20 and Inq. 22 in
LemmaE.2.	□
E.3 Proof of Theorem 5
Now we are ready to prove Thm. 5.
Proof. (Proof of Thm. 5)
Vhh(Sk,h) - Vhnk (Sk,h)
sk,h(sk,h)- Vnk(Sk,h)
7⅛ ，一 一 ，一 、、 I 1⅛ τV ，一 一 ，一 、、 I /ɔ rɔ ，一 _ ，一 、、
=Rk (Sk,h, πk,h(Sk,h)) + PkVk,h+1 (Sk,h, πk,h(Sk,h)) + CBk(Sk,h, πk,h(Sk,h))
-R(Sk,h,∏k,h(Sk,h)) - PVn+ι(Sk,h,∏k,h(Sk,h))
nk	h
=Vk,h+1 (Sk,h+1) - Vh+k1(Sk,h+1) + Rk(S, πk,h(Sk,h)) - R(S, πk,h(Sk,h)) + CBk (Sk,h, πh(S))
+P
(Vk,h+1 - Vh+J(Sk,h, πk,h(Sk,h^ -
R k - P) vh^+ι(sk7h,πk,h(sk,h))
ʌ,
Vk,h+1 - vh-kl) (sk,h+1)
+
+ (Pk - P)(Vk,h+1 - Vh+l) (Sk,h, nk,h(Sk,h))
The first inequality is due to optimism Vk,h(Sk,h) ≥ Vh(Sk,h). The first equality is due to Bellman
equation for Vnk and Vk,h.
For notation simPlicity, we define
δk,h = Rk (S, πk,h (Sk,h)) - R(S, πk,h(Sk,h)
；,h = P(Vk,h+1 - Vh+l) (Sk,h,nk,h (Sk,h)) - (Vk,h+1 - Vnkj(Sk,h+1)
,h
-P) Vh*+1(sk,h, πk,h(sk,h))
Firstly we focus OntheUPPerboundof 仅2 - P) (Vk,h+ι - Vh+ι) (sk,h,∏k,h(sk,h)). WeboUnd
this term following the idea of Azar et al. (2017).
(Pk- P)(Vk,h+1 - Vh+l) (Sk,h, ak,h)
n
n
≤ X(Pi - Pi)	∏	Pj (%,h+ι- Vh+J (Sk,h,ak,h)
i=1	j=1,j6=i
nn
+XXH
KPi-Pi)(.|(Sk,h ,ak,h)[ZP ])H(Pj- Pj) (∙|(Sk,h,ak,h)[ZP ])∣ι
i=1 j =1
≤XX( X
i=1	s0 [i]∈S[i]
nn
LP
Pi(s0[i]∣X [ZP])LP +_________________
Nk-I((S,a)[ZP ])	3Nk-i((s,a)[ZP ])
n
Y	Pj(Vk,h+1 - Vh+i) (Sk,h,ak,h)
j=1,j6=i
+H
i=1 j6=i,j=1
≤Xn	X	s2
i=1 s0 [i]∈S[i]
nn
+XX H
i=1 j6=i,j=1
4|Si|LP
4|Si|LP
Nk-i((s,a)[ZP ]) + 3Nk-i((s,a)[ZP ])
)(/
4∣Sj∣Lp
4|Sj ILP
Pi(S0[i]∣X[ZPDLP	Y P (V - Vh ʌ (S a X	SIHLP
Nk-1 ((S,a)[ZPD	ɪl j( k,h+1	V	k，h，k,h	£1 3Nk-i((S,a)[ZP])
j =1,j 6=i	i=1
(/
4|Si|LP
4|Si|LP
Nk-i((s,a)[ZP ]) + 3Nk-i((s,a)[ZP])
)y
4|Sj|LP
4|Sj|LP
2
y
19
Published as a conference paper at ICLR 2021
The first inequality is due to Lemma E.1. The second inequality is because of Lemma D.1, and the
last inequality is due to the fact that ∣V^k,h+ι - Vh+ι ∣	≤ H.
For each i ∈ [n], we consider those s0[i] satisfying Nk-1((s, a)[ZiP])Pi(s0[i]|(sk,h, ak,h)[ZiP]) ≥
2n2H2LP and Nk-1((s, a)[ZiP])Pi(s0[i]|(sk,h, ak,h)[ZiP]) ≤ 2n2H2LP separately.
For those s0[i] satisfying Nk-1((s, a)[ZiP])Pi(s0[i]|sk,h, ak,h) ≥ 2n2H2LP, the first term can be
bounded by
Xn	X 2
i=1 s0[i]∈S[i]
n
Pi(s0[i]|X[ZP])LP	Y P (^	V *、
Nk-ι((s,a)[ZP])	11 Pj (Vk,h+1 - Vh+1J (sk，h, ak，h)
-	i	j=1,j 6=i
Pi(s0[i]|X[ZiP])
i=1 s0[i]∈S[i]
LP
2 Pi(s0[i]∣X [ZP ])Nk-i((s,a)[ZP ])
n
Y Pj (%,h+1 - Vh+1)(sk,h, ak,h)
j=1,j 6=i

≤ HP(Vk,h+1 - vh+l) (Sk,h, ak,h)
=H (Vk,h+1 - Vh+l) (sk,h+1, ak,h+1)
+ H (P (Vk,h+1 - Vh+l) (Sk,h, ak,h) - (Vk,h+1 - Vh+l) (sk,h+1, ak,h+1 D
where the second term can be regarded as a martingale difference sequence, and we denote it as
δk4,h.
For those S0[i] satisfying Nk-1((S, a)[ZiP])Pi(S0[i]|Sk,h, ak,h) ≤ 2n2H2LP, the summation can be
bounded by
n
X
i=1
nH 2∣Si∣Lp
Nk-i((s,a)[ZP ])
For notation simplicity, we define δk5,h as:
y
δk5,h
nn
XXH
i=1 j6=i,j=1
4∣Si∣Lp
Nk-i((s,a)[ZP ])
4∣Si∣Lp	! 4 /	4|SjILP	- 1	4|SjILP
3Nk-i((s,a)[ZP])) Nk Nk-I((S,a)∖ZP]) + 3Nk-i((s,a)[Zf ])
+
n
+X
i=1
2nH 2∣Si∣Lp
Nk-i((s,a)[ZP ])
To sum up, by the above analysis, we prove that
(Pk,h - P)(Vk,h+1 - Vh+l) (Sk,h,πk,h(Sk,h) ≤ H (Vk,h+1 - Vh+l) (sk,h+1,ak,h+1) + δk,h + δk,h
Now we are ready to summarize all the terms in the regret. Firstly, we recursively calculate the
regret for all h ∈ [H].
V* (Sk,1,ak,I)- VInk (Sk,1, ak,I) ≤ Vk,1(Sk,1, ak,I)- V"k (Sk,1,ak,I)
≤CB(Sk,1,ak,l) + δk,1 + δk,1 + δk,1 + δ4,1 + δk,1 + (I +	(Vk,2 (Sk,2, ak,2) - Vnk (Sk,2,ak,2))
H
H	1	h-1
≤ E (1 + H)	(CBk(Sh, ah) + δk,h + δk,h + δk,h + δk,h + δk,h)
h=1	H
H
≤ Xe (CBk(Sh, ah) + δk,h + δk,h + δk,h + δk,h + δ5,h)
h=1
20
Published as a conference paper at ICLR 2021
Then we sum up the regret over k episodes,
K
Reg(K) ≤ X(Vr(S1,a1) - V1πk(s1,a1))
k=1
KH
≤ X Xe (CBk(Sh, ah) + δk,h + δk,h + δk,h + δk,h + δk,h)
k=1 h=1
δk2,h and δk4,h can be regarded as martingale difference sequence, the summation of which can
be bounded by O(Hy∕Tlog(T)) by Lemma E.2, while δ1 % and δ3 h can also be bounded by
Lemma E.2. The summation of different terms in δk1,h, δk3,h, δk4,h and δk5,h can be separated into
the following categories. In the following proof, we use C to denote the dependence of other pa-
rameters except the counters Nk ((Sk,h, ak,h)[Zi])
For those terms of the form / C , we have
Nk ((sk,h ,ak,h )[Zi])
kh
C
NNk((sk,h, ak,h)[Zi])
≤HC+
x[Zi]∈X [Zi]
NK (x[Zi])
X
c=1
=HC +	E	C√Nκ(X[ZiD
x[Zi]∈X [Zi]
≤HC + C VZX [Zi]∣T
The last inequality is due to Cauchy-Schwarz inequality. This term influence the main factors in the
final regret.
For those terms of the form τr77----C——, We have
Nk ((sk,h,ak,h)[Zi])
kh
_________C__________
Nk ((Sk,h, ak,h)[Zi])
NK(x[Zi])
≤HC+ X X
x[Zi]∈X[Zi]	c=1
C
c
≤HC+	Cln(NK(x[Zi]))
x[Zi]∈X[Zi]
≤HC + C X [%] |ln T,
which has only logarithmic dependence on T .
For those terms of the form /	C	. we define
Nk ((sk,h ,ak,h )[Zi])Nk ((sk,h ,ak,h )[Zj ])
Nk ((S, a)[Zi], (S, a)[Zj]) as the number of times that agent has encountered (S, a)[Zi]
and (S, a)[Zj] simultaneously for the first k episodes. It is not hard to find that
Nk((S, a)[Zi]) ≥ Nk((S, a)[Zi], (S, a)[Zj]) and Nk((S, a)[Zj]) ≥ Nk((S, a)[Zi], (S, a)[Zj]).
kh
≤kh
___________________C____________________
N'N(((sk,h, ak,h)[Zi])Nk((sk,h, ak,h)[Zj])
_________________C__________________
Nk ((Sk,h, ak,h)[Zi], (sk,h, ak,h)[Zj])
≤HC +	Cln (Nk((Sk,h, ak,h)[Zi], (Sk,h, ak,h)[Zj]))
x[Zi ∪Zj]∈X [Zi ∪Zj]
≤HC + CX [Zi ∪ Zj]|ln T,
which also has only logarithmic dependence on T.
For other terms with the form of
____________C_____________
(Nk((Sk,h ,ak,h)[Zi]))2
and
___________________C
Nk ((sk,h ,ak,h)[z i]) ^∖J Nk ((sk,h ,ak,h)[Z j])
the
summation of these terms has no dependence on T, which is negligible since T is the dominant
factor.
21
Published as a conference paper at ICLR 2021
By bounding these different kinds of terms with the above methods, we can finally show that
(1 m _______________________________ n ____________________________
mm X，|X[ZR]∣Tlog(10mT|X[ZR]∣∕δ) + X H，|X[Zp]∣Tlog(10nTSA∕δ)
Here O hides the lower-order factors w.r.t T.	□
F Proof of Theorem 2
F.1 Estimation Error Decomposition
Lemma F.1. Under event Λ1 and Λ2, we have
I R /	]	R/	XV 1 X 2 2σR,i(s, a)LR
IRk G a- RG a)∣≤ m ∑y Nk-i((s,a)[zR])
ɪ X	8LR
+ m = 3Nk-ι(Ga)[ZR])
.,ʌ ..
|(Pk- P)Vh+ι(s,a)∣
2HLp
3Nk-1((s,a)[ZP ])
4|Si|LP
nn
+XX H
i=1 j6=i,j=1
4|Si|Lp
Nk-i((s,a)[Zp ]) + 3Nk-1((s,a)[ZP ])
1W
4∣Sj∣Lp
4|Sj ILP
Proof. The first inequality follows directly by the definition that R(s, a)= * Pm=I Ri(S,a) and
Lemma D.2. We now prove the second inequality. By Lemma E.1, we have
nI
∣(Pk- P)Vh+ι(s,a)∣ ≤ X (Pk,i - Pi)
i=1 I
YY Pj)Vh+ι(s,a)
j 6=i,j=1
nn
+ X X HI(Pk,i- P)(∙∣(s,a)[ZP ])∣ι∙∣(Pk,j - P,(∙∣(s,a)[Zp ])],
i=1 j 6=i,j=1
By Inq. 9 in Lemma D.1 and Inq. 14 in Lemma D.2, we have
I (P - P)Vh+ι(s,a)∣
2HLp
3Nk-1((s,a)[ZP ])
nn
+XX H
i=1 j6=i,j=1
41SiILP
Nk-1((s,a)[ZP ])
41SiILP
3Nk-i((s,a)[ZP])
)y
4ISjILp
+
□
y
+
F.2 Omitted proof in Section 4.2
Proof. (Proof of Theorem. 1)
ωh2(s)
=E (Jh:H(sh) - Vh(sh))2 I sh = s
=XP(s0 Is)E (Jh+1:H(sh+1) + rh - Vh(sh))2 I sh = s, sh+1 = s0
s0
=XP(s0 Is)E Jh2+1:H(sh+1) + rh2+ Vh2(sh) I sh= s, sh+1 = s0
s0
+	P(s0Is)E	[2rh (Jh+1:H (sh+1) -	Vh(sh))	-	2Jh+1:H (sh+1)Vh (sh)	I	sh = s,	sh+1	=	s0]
s0
22
-S
(EZ) V H I+VS 一 (I+VS)I+* I (I+vs) H=+闪)aSLS⅛κ +
(ɛ)ɪm(S5S⅛M≈) I ('s) I+* (SLs⅛M+ f: Vs-(VS)¾l 空目
'S = I+v?SU Vs 一 ((''S)I+N( Sss⅛κ) I (I+3 H -JraSLs⅛κ+ fs U Vs 一 (Vs)¾l≥alu
-S
LS = I+v?SUVs 一 Z((Vs)因 I (Vs)苣 — (I+vs)HisaSLs⅛κ + fs U Cs - (VS)¾Isal=
LS = I+V?: Vs-(Vs)N(I+VS)I+W I ((Vs)旷 I (I+VS)I+N)(Vs)因 aaSLs⅛κ +
-S
LS = I+v?SU Vs-(Vs)N+T+ (I+VS)H;aSLs⅛κu
(S)芍
OAEq əM eəjojəjəm
LSU I+qs 6 U q(qs)(I+qs)I+^ H LSU I+q?SUq(qs)(I+qs)E=+qr国
LSU I+qs 6 U q((qs)I (I+qs)I+)(qs)国 H LSU I+qs 6 U q((qs)I (I+qs)E二+qr)q∖国
OAEq
əM SnqIqUəpuədəpu=Mupu8 əjb (I+qs)E=十qr PUE 飞亡 ESu I+q?SU £s s>δ
IZOZ xu0IaJOdEd əɔuəjəjus E SE poqs=qnd

□
I=I=-S
W"MM + (SF 喈M+ (aJ⅜⅛MH S3
UZL S
OAEq əM EZ ∙ubaɔEq 寸 Z ∙ubud
I=( % J -S
(X) O = (S)WTM— (ɛ)ɪ+WSSS⅛M) — (`s)ɪw(SLS⅛M
ql MoqS unɔ əM CAEM OAOqE ωqlzi n : ZU ;0J
(S)*u0Eqns AILS qM pəɔdəj 三s qMs jo-ɔəA ətp sə-ouəp -W -//S/)JH
CN
SEMoβ∙sM-I0J əf∙soOUBμBA ətp əsoduɪsəp əMdo≡SUBbpəjo-ɔEJ ə-jo工
I=
(SF喈KF = CS = Vs 一(vs)¾lsal
UL L
OAEq əM 6 əaluəpuədəpIEUOPU8 əjbq∖ Spibmoj əip əɔuCSPJBMəj pəjo-ɔEJ ətp」0 4
∙(∕s)I+(%)& 飞+ (S)I H (S)父q 二 ɔEJ əf o-ənp WAlnBnbə pusəs OqI
Published as a conference paper at ICLR 2021
Proof. (Proof of Corollary 1.1) We can regard the MDP with given policy π as a Markov chain. By
Theorem 1, we have
n	1m
ω2(S) = X P(S0|s,n(S))ω2+ι(SO) + X σP ,i(Vn ,s,a) + m2 X σR ,i(s,a)
s0	i=1	i=1
By recursively decomposing the variance until step H , we have:
H	n	1m
ωh(SI) = X X Wh(S, a) (XσP,i(Vh ,s,a) + m XσR,i(s, a) j
h=1 (s,a)∈X	i=1	i=1
Since ωh2(S1) = E (Jh:H (Sh) - Vh(S))2 |Sh = S ≤ H2, we can immediately reach the conclu-
sion.	□
F.3 The ”good” Set Construction
The construction of the ”good” set is similar with that in Dann et al. (2017) and Zanette & Brunskill
(2019), though we modify it to handle this more complicated factored setting. The idea is to partition
each factored state-action subspace at each episode into two sets, the set of state-action pairs that
have been visited sufficiently often (so that we can lower bound these visits by their expectations
using standard concentration inequalities) and the set of (S, a) that were not visited often enough to
cause high regret. That is:
Definition 4. (The Good Set) The set Lk,i for factored transition Pi is defined as:
Lk,i = 卜x[Zp]) ∈ X[Zp] : 4 X Wj,zp (x) ≥ H log(18nXPH∕δ) + H 卜
l	j<k
The following two Lemmas follow the same idea of Lemma 6 and Lemma 7 in Zanette & Brunskill
(2019).
Lemma F.2. Under event Λ1 and Λ2, if (S, a)[ZiP] ∈ Li,k, we have
Nk((S,a)[zf ]) ≥ 4 X wj,zP (S,a)
j<k
Proof. By Lemma D.2, we have
Nk((S,a)[Zp]) ≥ 2 XWj((S,a)[Zp]) - Hlog(18nXPH∕δ).
j<k
Since (s, a)[ZP] ∈ Li,k, we have 1 Pj<k Wj,zp (x) ≥ H log(18nXPH∕δ) + H. That is,
Nk((S,a)[ZP]) ≥ 2 XWj((S,a)[ZP]) - Hlog(18nXPH∕δ)
j<k
≥ 1X Wj ((S,a)[zP ])- 4 X Wj ((S,a)[zP ])
j<k	j<k
=4 X Wj ((S,a)[zf ])
j<k
□
Lemma F.3. It holds that
KH
XX X
Wk,h,zP (S, a) ≤ 8HXiP log(10nXiP H∕δ).
k=1 h=1(s,a)[ZP ]∈Lk,i	'
24
Published as a conference paper at ICLR 2021
Proof. For those (s,a)[ZP] ∈ Lk,i, we have 4 P/ Wj,zp(x) ≤ Hlog(10nXPH∕δ) + H ≤
2Hlog(10nXPH∕δ).Thatis,	'"
KH
XX X	wk,h,ZiP (s, a) ≤ X 8H log(10nXiP H∕δ) ≤8HXiPlog(10nXiPH∕δ)
k = 1 h=1(s,a)[ZP ]∕Lk,i	(s,a)[ZP ]
□
Lemma F.2 shows that we can lower bound the visiting count of a certain (s, a)[ZiP] if the visiting
probability of (s, a)[ZiP] is sufficient large. Lemma F.3 shows that those (s, a)[ZiP] with little
visiting probability cause little contribution to the final regret.
Lemma F.4.
KH
k=1 h=1 (s,a)[ZiP]∈Lk,i
wk,h,ZP Ga)
Nk((s,a)[ZiP ])
≤ 4XiP logT.
Proof. For those (s, a)[ZiP] ∈ Lk,i, we have Nk((s,a)[Zp]) ≥ 4 Pj<k wj,ZP (s, a). Therefore,
we have
KH
XX X
k=1 h=1 (s,a)[ZiP]∈Lk,i
wk,h,ZP (s,a)
Nk ((s,a)[ZP ])
KH
≤XX X
k=1 h=1 (s,a)[ZiP]∈Lk,i
4wk,h,zp Ga)
Pj<k wj,zp (S,a)
K
≤XX
(s,a)[ZiP]∈XiPk=1
wk,zp Ga)
Pj<k wj,zp (S,a)
≤4XiP log T
Lemma F.5. For factored set ZiP of transition, we have:
KH
XX X
k=1 h=1 (s,a)∈X
wk,h(S, a)
≤8XiPlogT
KH
XXX I	wk，h(s,a)	二 ≤8XiPlogT
k=1 h=1 (s,a)∈X	Nk-1((S, a)[ZiP])Nk-1((S, a)[ZjP])	i
K H	…/一 一、	,—
XX X -------------------Wkj----------------1 ≤8∖∕XpjT1/4 log T
Mh=1(s⅛∈x PNk-i((s,a)ZP]) (Nk-I((S,a)∖ZP]))4	V ,
where XiP,j = |X[ZiP ∪ ZjP]|.
For factored set ZiR of rewards, similarly we have:
KH
XX X
k=1 h=1 (s,a)∈X
Wk,h(s, a)
Nk-i((s,a)[ZR ])
KH
XX X
k=1 h=1 (s,a)∈X
___________Wk,h(s, a)__________
qNk-i((s,a)[ZR])Nk-i((s,a)[ZR ])
KH
XX X
k=1 h=1 (s,a)∈X
wk,h(S, a)
PN-(SWΠ (Nk-I((S,a)[ZR]))1
≤8XiR log T
≤8XiR log T
≤8√XRjT1/4 logT
□
(25)
(26)
(27)
(28)
(29)
(30)
where XRj = |X[ZR ∪ ZR]|.
25
Published as a conference paper at ICLR 2021
Proof. We only prove the inequalities for the factored set of transition. The inequalities for the
factored set of rewards can be proved in the same manner.
For Inq. 25, we define Xi((s, a)[ZiP]) = {x ∈ X | x[ZiP] = (s, a)[ZiP]}, then we have
KH
XX X
k=1 h=1 (s,a)∈X
Wk,h(s, a)
Nk-i((s,a)[ZP ])
E Σ
k,h (s,a)[ZiP]∈X [ZiP]
wk,h (s1 ,a1 )
wk,h,ZP (s, a) 2∙√sι,aI)∈Xi((S,a)[ZP]) wk,h,zP (s,a)
E Σ
k,h (s,a)[ZiP]∈X [ZiP]
wk,h,ZP (S,a)
Nk-i((S,a)[ZP ])
Σ Σ
k,h (s,a)[ZiP]∈Lk,i
wk,h,ZiP (S, a)
+
k,h (s,a)[ZP]∕Lki
wk,h,ZiP (S, a)
≤
k,h (s,a)[ZiP]∈Lk,i
wk,h,ZiP (S, a)
S (s,a焉/Lki wk,h,zP(S，a) 鼻(s,a)m/Lki
+
t
≤4XiP log T + /8HXpiog(10nXP H∕δ)
≤8XiP log T
In the first equality, we firstly categorize (S, a) based on their value (S, a)[ZiP] and sum up over all
possible choice of (S, a)[ZiP], then we sum up the value in each category in the inner summation.
The second equality is due to P(sι,aι)∈Xi((s,a)[Zp]) WwkhhZsXIa)
1. The first inequality is due to
Cauchy-Schwarz inequality. The second inequality is due to Lemma F.4 and Lemma F.3. The last
inequality is due to the assumption that XiP ≥ H log(10nXiP H∕δ).
For Inq. 26 and Inq. 27, we define ZiP,j = ZiP ∪ ZjP. For the factored set ZiP,j, similarly we have
KH
XX X
k=1 h=1 (s,a)[ZiP,j]∈Lk,i
wk,h,ZP Ga)
Nk ((s,a)[ZPj ])
≤4XiP,j logT
KH
X X X	wk,h,ZiP,j (S, a) ≤8HXiP,j log(10nXiP,jH∕δ),
k = 1 h=1(s,α)[ZPj ]∕Lk,i	"
By the definition of ZiP,j, we know that Nk-1((S, a)[ZiP])	≥	Nk-1((S, a)[ZiP,j]) and
Nk-1((S, a)[ZjP]) ≥ Nk-1((S, a)[ZiP,j]). Therefore, we have
KH
XX X
k=1 h=1 (s,a)∈X
Wk,h(s,a)______
qNk-1(M[zpi)N-(M[zpD
KH
≤XX X
k=1 h=1 (s,a)∈X
Wk,h(s,a)
Nk-i((s,a)[ZP,j ])
KH
XX X
k=1 h=1 (s,a)∈X
wk,h(S, a)
KH
≤XX X
k=1 h=1 (s,a)∈X
wk,h(S, a)
The following proof of Inq. 26 and Inq. 27 shares the same idea of the proof of Inq. 25.	□
F.4 Technical Lemmas about Variance
In this subsection, we prove several technical lemmas about variance. For notation simplicity, we
use Ei and E[i:j]
as a shorthand of Es0[kPi(∙∣(s,α)[Zp]) and Es0[ij]〜P[ij](∙∣(s,α)[ZPj]]). Similarly, We
26
Published as a conference paper at ICLR 2021
USe Vi and V[ij] asashorthandof %0[i]〜吗(.∣(s,α)[zP ]) and V√[ij]〜pij](∙∣(s,α)[Z 焉]).FOrthOSeW.r.t
the empirical transition Pk
τττι ∙>τCr,∙>	. . ι	λ∙	.	ι ♦
we use Ek and Vk to denote the corresponding expectation and variance.
FOreXample, ES0[i]〜Pk,i(∙∣(s,a)[zp]) is denotedas Ek,i.
Lemma F.6. Under ^vent Λι, Λ, we have:
∣σP ,k,i(Ks,α) - σP ,i(K s,a)∣ ≤ 4H2 E (2
4|SjILP
|Sj ∖Lp
Nk-i((s,a)[ZP ]) + 3Nk-I((SM[ZJd])广
where V denotes some given funCtiOn mappingfrom S to R.
Proof.
∣σp,k,i(V, S, a) - σP,i(V, S, a)∣ =∣E工一]V,E[i+Ln] V(s0) - 旧比一跖旧忆十上旬 V(s0)∣
,ʌ ʌ ʌ _ ʌ. ʌ
≤∣e [1:i-1] ViE[i+1:n]V (s ) - E[1:i-1] ViE[i+1:n] V (s )∣
+ IE[1:i-1]ViE[i+1:n]V(s ) - E[1:i-1]ViE[i+1:n]V(s )∣
We bound Equ. 32 and 33 separately.
For equ. 32, we have
IE[1:i-1]ViE [i+1:n]V(s ) - E[1:i-1]ViE[i+1:n] V (s )∣
= X	(P[1:i-1] - P[1:i-1]) (s0[1 : i - 1]∣s, a)ViE[i+1:n]V(s0)
sz[1:i-1]eS[1:i-1]
≤ 肉Li-1](∙∣s,a) - P[Li-1](∙∣s,a) ∣ ɪ ∙ ∣ ViEi+1:„V(s0) L
i-1
≤H2 X∣Pj(∙∣s,a) - Pj(∙∣s,a) ∣
j=1
≤ H2 X (2 S	∣Sj ILP	+	4∣Sj ILP	!
≤	j⅛∖ VNk-I((S,a)[ZP]) + 3Nk-1((s,a)[ZP])J
(31)
(32)
(33)
(34)
The last inequality is due to Lemma D.1.
For equ. 33, given fixed s0[1 : i 一 1], we have
∣ V iE [i+1:n] V (SZ) - ViE[i+1:n]V (SZ) ∣ ≤ E i 偿[i+1:n]V (Sz)) - Ei (E[i+1:n]V (SZ))
+(E[i:„]V(S0))2 - (E[i:„]V(S0))2 ∣
≤ Ei (E[i+1:„]V(SZ))2 - Ei (E[i+Ln]V(s0))2∣
+ Ei (E[i+1:„]V(S0))2 - Ei(E[i+1:„]V(S0))2
+(E[i:„]V (S0))2 - (E [i:n] V(SO))[
≤ Ei (E[i+1:n]V(SO)) - Ei (E[i+1:n]V (s0))2 ∣	(35)
+ Ei (E[i+1：n]V(s0))2 - Ei(E[i+1:n]V(S0))2 ∣	(36)
+ 2H∣ E[i:n]V(s0) - E[i:n]V(s0) ∣	(37)
27
Published as a conference paper at ICLR 2021
The first inequality is due to the definition of variance. The last inequality is due to E[i:n] V (s0) +
ʌ _ _ , .. __
E[i：n]V(s0) ≤ 2H.
All Equ. 35, 36 and 37 can be bounded with the same manner of Equ. 32. That is, we first bound each
term with the L1-distance of transition probability multiplying the L∞-norm of the value function,
then we upper bound the L1-distance by Lemma D.1. This leads to the following results:
∣ViE[i+i：n]V(s0) - ViE[i+i：n]V(s0)∣ ≤4H2 XX(2\S	ISjLPU +	'S"；,^ !
j=i	Nk-1((s, a)[ZjP])	3Nk-1((s, a)[ZjP])
This bound doesn't depend on the given fixed s0[1 : i -1]. By taking expectation over s0[1 : i -1]〜
P[Li-i](∙∣s,a),we have
E[1:i-1] ∣ViE[i+1:n]V(s0) - ViE[i+1:n]V(s0)∣
≤ Ei (E…]V(s0))2 - Ei (E…]V(S]『I ≤ 4H2 X (2SNk-1jPZP}) + 3N「S€PZP])!
Combining with Equ. 34, we have
IσP，k，i(V，S，a)- σP，i(V，S，a)I ≤ 4H2 XX (2SNk-ι∣SS∖j^+ 3NkJS⅞[Zpi)!
□
Lemma F.7. Under ^vent Λι, Λ2 and Ω, we have
σP，i(K+1,s, a) - 2σP，k，i(V k，h+1, s, a) ≤ uk'h,i(s, a) + 4H 2 E (2S NWM + 3Nk-4j)[zpi)
where uk,h,i(s, a) is defined in Section 4.3:
uk,h,i(s,a) = Es0i：irpk,[i：i] (∙∣s,a) ](%+i：n]~pk,[i+i：n](.|s,a) (Vk,h+1-Vk,h+1) (SO))].
Proof. We can decompose the difference in the following way:
σP ,i (Vh+1, s, a) - 2σP ,k,i(V k,h+1, s, a)
≤σP,k,i(Vh+1, s, a) - 2σP,k,i(Vk,h+1, s, a) + σP,i(Vh+1, s, a) - σP,k,i(Vh+1, s, a)
By Lemma F.6, we know that
IσP，k，i(K+1，S，a)- σP，i(Vh+1,S,a)I ≤ 4H2 X (2SNk-ι∣S⅛/ + 3Nk-4∣SS,[[ZP]) ,
Now we only need to bound σp,k,i(Vh+ι, s, a) - 2σP仆(Vk,h+ι, s, a). By Lemma 2 of Azar et al.
(2017), we know that for two random variables X ∈ R and Y ∈ R, we have
V(X) ≤ 2[V(Y) +V(X - Y)]	(38)
That is,
σP,k,i(Vh+1, s, a) - 2σP,k,i(Vk,h+1, s, a)
ʌ	ʌ ʌ	__ ,八	.ʌ	ʌ ʌ	,八
=E[1:i-1]ViE[i+1:n] Vh+1(s ) - 2E[1:i-1]ViE[i+1:n]Vk,h+1(s )
≤2E[1：i-1]Vi (E[i+1:n]Vh+l(s0) - E[i+1：n]Vk,h+l(s0))
=2E[1：i-1]ViE[i+1：n] (Vh+ι(s0) - Vk,h+l(s0))
≤2E[i:i](E[i+i：n] (Vh+ι(s0) - Vk,h+ι(s0)))]
=uk,h,i (s, a)
28
Published as a conference paper at ICLR 2021
The first inequality is due to Inq. 38, and the second inequality is due to VX ≤ EX2 for any random
variable X ∈ R.
To sum up, we have
σp i(Vhi ι, s, a)——	2σPki(Vk h+ι,	s,	a)	≤	Uk hi(s, a)	+	8H2	X、( 2 J--ɪ-———+—	+--------ɪ-———p
P,i( h+1, , )	P,k,i( k,h+1, , ) ≤ k,h,i( , )+	VNk-ι((s,a)[ZP]) + 3Nk-i((s,a)[ZP])
□
LemmaE8. Underevent Λι, Λ? and Ω, suppose Reg(K) = PK=I Vk,ι(sι) — Vnk (si), we have:
KH
XX X wk,hGa)(σP,i(Vh+ι,s,a)——σP,i(Vh+ι,s,a))≤2H2Reg(K)
k=i h=i (s,a)∈X
KH
XX X wk,h(s,a)(σP,i(Vk,h+ι,s,a)——σP,i(Vh+ι,s,a))≤2H2Reg(K)
k=i h=i (s,a)∈X
Proof. We only prove the first inequality in detail. By replacing Vh+1with Vk,h+ι, We can prove
the second inequality in the same manner.
σp,i(Vh+i,s,a) - σp,i(Vh+ι,s,a) = E[i：i] M 咻+1：网％13))——Vi 曲+1：网陪13))]
Given fixed s0[1 : i —— 1], we bound the difference of the variances: Vi(E[i+i：n]Vh+i(s0))——
Vi(E[i+i：n]Vh+i(s0)).
Vi(E[i+i：n]Vh+i(s0))——Vi(E[i+i：n]Vh+i(s0))
=Eih(E[i+rnM+1(s0))2 ——(E[i+rn]Vh+ι(s0))2i ——(E即[VC+1(s0)])2 + (E[i:n][需 1(s')])2
≤Ei h(E[i+1：n]Vh+1(s0))2 ——(E[i+rn]Vh+ι(s0))2i
≤2HEi [E[i+i：n]Vh+i(s0)——E[i+1：n]Vh+1(s0)]
=2HE[i:n] [Vh+1(s0)——Vh+1(s0)]
The first inequality is due to Vh+ι(s0) ≥ Vh+i (s0), and the second inequality is due to
E[i+Ln]VT+1(s0)+ E[i+Ln]VΠ+ι(s0) ≤ 2H.
We then take expectation over all s0 [1 : i —— 1]. that is
σp,i(Vh+1, s, a)——σP,i(Vh+ι, s, a) ≤ 2HE[i：n] [Vh+1(s0)——Vh+ι(s0)]
Plugging the inequality into the former equation, we have
KH
XXX
wk,h(s, a) (σP ,i(Vh+ι, s, a) ——σP ,i(Vπ+ι, s, a))
k=i h=i (s,a)∈X
KH
≤ XX X wk,h(s,a)2H Es0~P(∙∣s,a) [Vh+I(SO)——Vh+i(s0)]
k=i h=i (s,a)∈X
KH
=XXX 2wk,h (s)H [琮(s)——Vhk (s)]
k=i h=2 s∈S
K
≤ X 2H2 [Vι*(sι)——Vink(si)]
k=i
K
≤ X 2H2 [Vk,1 (SI)——VInk (S1)]
k=i
=2H2 Reg(K)
29
Published as a conference paper at ICLR 2021
For the second inequality, this is because that by lemma E.15 of Dann et al. (2017), we have
X Wk,h(s)[Vns)- Vr (s)]
s
H
=X Xwk,hι (S)(R(S,π*(S))- R(S,πk(S)) + Pvh1 + 1(S,π*(S))- PVhI+1(S,πk(S)))
h1 =h s
H
V1*(S1)-V1πk (si) = X X wk,h (S)(R(S,π*(s)) - R(S,∏k(S))+ PVhi + i(s,π*(s)) - PV^+ι(S,∏k(S)))
h1 =1 s
This means that Ps wk,h(s)[Vh= (s) - Vhrk (s)] ≤ V*(si) - Vπk (si) for any k, h.	□
F.5 Optimism and Pessimism
Lemma F.9. Suppose that Λι, Λ2 and Ωk,h+ι happen, then we have thefollowing inequalities hold
for any a ∈ A and S ∈ S :
1m
∣R(s,a) - R(s,a)∣ ≤ — X CBR(s,a)	(39)
m
i=1
n
IPkVh+ι(s,a)- PVh+ι(s.α)∣ ≤ XCBP(s,a)	(40)
i=1
Proof. The first inequality follows directly by Lemma F.1 and the definition of CBiR(S, a). For the
second inequality, by Lemma F.1, we have
∣ (Pk - P) Vh+ι(s,a)∣
2HLP
3Nk-i((s,a)ZP ])
nn
+	Hφk,i(s, a)φk,j (s, a),
i=1 j 6=i,j=1
Where φk,i(s, a) = NNk-1(SslLpzp]y + 3Nk-1SisLP[zp]). τhe first inequality is due to Vk,h(S) ≥
Qk,h (s,∏*(s)).
By the definition of C BiP (s, a), we have
X CBP(S,a) -∣(Pk - P) Vh+ι(s,a)∣
i
(41)
XX ( S4σP,k,i(Vk,h+1, S，a)LP	S2σP,i(Vh+1, s, a*)LP \
≥ = yV	Nk-I((S,a*HZP])	- V Nk-I((S,a)[ZP]))
G I 2υkhi(s,a)LP	G [
+X Wa )ZP])+Xi
16H2LP
Nk-I((S,a)[ZP ])
X(( NdS‰!4+S
4∣Sj∣LP
3Nk-ι(s,a)[ZP ]
(43)
30
Published as a conference paper at ICLR 2021
We mainly focus on the bound of Eqn 42.
∕4σF ,k,i(V k,h+ι,s,a)LP	∕2σF ,i(Vh+i，s，a)LF
V	Nk-i((s,a)ZP ])	V Nk-i((s,a)[Zip ])
≥/ Y 2%(Vh+1，SNkLP(-：：3p(：k，hiS，a)LP	2σP ,k,i(V k,h+ι,s,a) ≤ σP ,i(¾ι,s,a)
(0	otherwise
(44)
(45)
For those 2σP,k,i(Vk,h+ι, s, a) ≤ σp,i(Vh+ι, s, a), by Lemma F.7, we have
σP,i(Vh+1, s, a) - 2σP,k,i(Vk,h+1, s, a)
≤uk,h,i (s, a) + 8H2 X 2
|Sj|LP
4|Si|LP
That is,
S4σF,k,i(Vk,h+1, s, a)LP	22σP,i(Vh*+1, s，a)LP
V	Nk-i((s,a)[ZP])	V Nk-i((s,a*)[ZP])
≥-
≥-s
ZukhiGaLF + 16H 2LP Pj=I (2J Nk-ISjsL)[zp不 + 3Nk-4(Ss,L* HZP ])
Nk-i((s,a*)[ZP ])
2uk,h,i(S, a)LP
16H2LP
Nk-i((s,a)[ZP ])
Nk-i((s,a)[ZP ])
n
X
j=1
4∣Sj∣LP	!1 + S	4∣Sj∣LP
Nk-i((s,a)[ZP])J +y 3Nk-i(s,a)[Zp]
-
Combining with Eq. 41, we prove that
n
IPkVh+ι(s,a)- PVh+ι(s.a)∣ ≤ XCBP(s,a)
i=1
□
The optimism is proved by induction.
Lemma F.10. (Optimism) Suppose that Λι, Λ2 and Ωk,h+ι happen, then we have Vk,h ≥ V^.
Proof.
Vk,h(s) - Vns)
=CBk(s,∏*(s))+ PkVk,h+ι(s,∏*(s)) - PVh+ι(s,π*(s))+ Rk(s,π*(s)) - Rk(s,∏*(s))
≥CBk (s,∏*(s))+ (P k — P) Vh+ι(s,∏*(s)) + Rk(s,π*(s)) - Rk(s,π*(s))
≥0
The first inequality is due to induction condition that Ωk,h+ι happens. The last inequality is due to
Lemma F.9.
□
Lemma F.11. (Pessimism) Suppose that Λι, Λ2 and Ωk,h+ι happen, then we have V卜力 ≤ V^.
31
Published as a conference paper at ICLR 2021
Proof.
V k,h(S)
=Rk(s,πk,h(s)) - CBk (S,πk,h(Sy) + Pk V k,h+Ms,πk,h(S))
≤R(s,∏k,h(s)) - CBk(s,∏k,h(s)) + PkVh+ι(s,∏k,h(s))
=R(s, πk,h (S)) + Pvh+ι(s, πk,h (S))
+ (R(s, ∏k,h(S)) - R(s, ∏k,h(S)) - m XX CBR(S, ∏k,h(S)))
+ (Pk Vh+ι(S, ∏k,h(S)) - PVh+ι(S, ∏k,h(S)) - X CBP(S, ∏k,h(S)))
The inequality is due to Vk,h+ι(S0) ≤ Vh+i(s0) since event Ω'k,h+∖ happens.
By lemma F.9, we have
1m
IRk (S,πk,h (S))- R(S,nk (S))I ≤ — ^X CBi (S,πk,h(Sy)
m i=1
n
∣P3k VC+ι(s,πk,h(S))- PVC+ι(s∙πk,h(S))I ≤ X CBP(S,πk,h(Sy)
i=1
Therefore, we have
Vk,h(s,πk,h(Sy) ≤R(s,πk,h(S)') + PVC+ι(s,πk,h(S))
≤R(S,∏h (s)) + PVC+ι(s,∏h(s))
≤VhC (S, a)
□
Lemma F.12. (Optimism and pessimism) Under ^vent Λι and Λ2, we have Ωk,h holdsfor all k and
h.
Proof. By Lemma F.10 and Lemma F.11, through induction over all possible k, h, we can prove the
Lemma.	□
F.6 Proof of Theorem 2
Proof. We decompose Reg(K) = PK=1 (Vk,1(sk,1, a®,i) - Vnk 由「a®,。) in the classical way
(Azar et al., 2017; Zanette & Brunskill, 2019; Dann et al., 2019), that is
K
X(Vk,ι(si) - Vπk (sι))	(46)
k=1
≤ΣΣwk,h(Sh, ah)CBk(Sh, ah)	(47)
k,h sh,ah
+ XX Wk,h(sh, ah) (Pk - P) VC+i(sh, ah)	(48)
k,h sh,ah
+ X X wk,h(Sh, ah) (Pk - P)(Vk,h+1 - VC+1) (Sh, ah)	(49)
k,h sh,ah
+XX
wk,h (Sh, ah) Rk (Sh , ah ) - R(Sh , ah )	(50)
k,h sh,ah
32
Published as a conference paper at ICLR 2021
We bound Equ. 47, 48, 49 and 50 separately by Lemma F.13, Lemma F.14, Lemma F.15 and
Lemma F.16. Combining the results of these Lemmas, we have
m
n
n
1
Reg(K) ≤ CI mm
i=1
i=1
HTXPLp logT + C3t nH2R%g(K) EXPLP logT
i=1
(51)
Here C1, C2, C3 denote some constants. Solving the Reg(K) in Inq 51, We can shoW that
~ , .
Reg(K) ≤ O
XiRTLiR
logT+ ut
i=1
HTXiPLPlogT	,
where O hides the lower order terms w.r.t T .
By the optimism principle (Lemma F.12), We have %* (Sk
the final result:
K
：,i,ak,i) ≤ Vk,1(sk,1,ak,1). This leads to
(Vr(Sk,1, ak,I)- Vnk (SkI, ak,I)) ≤ O
k=1
XiRTLiR
logT+ ut
i=1
HTXiPLP logT .
n
n
□
F.7 Bounding the Main Terms
Lemma F.13. Under event Λι, Λ? and Ωk,h, suppose Reg(K) = PK=I Vk,1(s1) 一 Vnk (si), we
have
KH
ΣΣΣwk,h(S, a)CBk (S, a)
k=1 h=1 s,a
≤O
TXiRLiRlogT +
n
HTXXiPLP logT +
i=1
∖
∖
n
H2nReg(K) X XPLp log T
i=1
Proof. By the definition of CBk(S, a), We have
wk,h (S, a)CBk (S, a)	(52)
k,h,s,a
mn
m X CBRi(S, a) + X CBPi(S,a)	(53)
i=1	i=1
wk,h (S, a)
k,h,s,a
ɪ X / 2σR,k,i(S,a)LR	X ∕4σp,k,i(Vk,h+i,S,q)Lp∖
m ⅛V Nk-I((S,a)[ZR]) ⅛V Nk-I((S,a)[ZP ]))
(54)
1 m	8LR
+ 二CIWkh(S a)» S 3Nk-i((S,a)[ZR])
k,h,s,a	i=1
n
+ Wk,h(S, a)
k,h,s,a	i=1
(S
16H2LP
Nk-I((S,a)[ZP ])
X(( "≡⅛!4+S
4|Sj|LP
3Nk-i(S,a)[Zp ]
(56)
+	Wk,h(S,a)
k,h,s,a	i=1 j 6=i,j=1
36H∣Si∣∣Sj∣(Lp )2
qNk-i((S,a)[ZP])Nk-i((S,a)[Zp])
(57)
l	/ ʌ / 2uk,h,i(S,a)LP
+ kS(IWk…币 Nk-I((S"I)
k,h,s,a	i=1
(58)
n
n
33
Published as a conference paper at ICLR 2021
By Lemma F.5, the upper bound of Eqn. 55, 56 and 57 is O(T 1), which doesn't contribute to the
main factor in the regret. We prove the upper bound of Eqn. 54 and Eqn. 58 in detail.
By Lemma F.6, we have
lσP,k,i(Vk,h+1, s, a) - σP,i(Vk,h+1, s,a)l ≤ 4H2 E I 2
4∣Sj∣LP
Nk-ι((s,a)[ZP ]) + 3Nk-ι((s,a)[ZP ])
|Sj|LP
Then Eqn. 54 can be bounded as
1 XX / 2σR,k,i(s,a)LR
m ⅛1V Nk-i((s,a)[ZR])
+Xn
i=1
4σP ,k,i(V k,h+ι,s,a)LP
Nk-i((s,a)[ZP ])
J XX / 2σR,k,i(s,a)LR
≤ m ⅛1V Nk-i((s,a)[ZR])
XX ∕4σP ,i(V k,h+ι,s,a)LP
+ i=1V	Nk-ι((s,a)[ZP ])
XX ∕4lσP,i(Vk,h+1, s, a) - σP,k,i(Vk,h+1, s, a)lLP
+ ⅛v	Nk-ι((s,a)[ZP ])
J XX / 2σR,k,i(s,a)LR
≤m ⅛1V Nk-i((s,a)[ZR])
(59)
+Xn
i=1
4σp ,i(V k,h+ι,s,a)LP
Nk-ι((s,a)[ZP ])
(60)
+8HXn
i=1
LP
Nk-ι((s,a)[ZP ])
n
X
j=1
((s,a)[ZP ])) +/
∣Sj∣Lp
Nk-1
4|Sj ILP
3Nk-ι((s,a)[ZP ]))
(61)
1
Similar with Eqn. 61, the summation of Eqn. 61 is upper bounded by O(T 1/4) by Lemma F.5. For
Eqn. 59, we have
wk,h(s,
XX / 2σR,k,i(s,aRR
⅛V Nk-ι((s,a)[ZR])
≤ X XX WIkhM m S √SSRj
k,h s,a	i=1
1 m I---
≤m∙ ΣJΣΣW…t
k,h s,a
2wk,h(s, a)LR
Nk-ι((s,a)[ZR])
≤ 1 X √T X X 2wk,h (S) a)LR
"m ⅛ t 右勺 Nk-i((s,a)[zR ])
The first inequality is due to σR ∣ i(s, a) ≤ 1. The second inequality is due to CaUchy-SchWarz
inequality. By Lemma F.5, the summation can be bounded by * Pm=I √ΧRLRTiOgT.
34
Published as a conference paper at ICLR 2021
For Eqn. 60, we have
wk,h(s, a)
XX 八σp,i(Vk,h+ι,sQLP
Nk-i((s,a)[ZP ])
≤t
n
E wk,h(s,a) £苏,i(Vk,h+ι,s,a) ∙
k,h,s,a	i=1
tk XX owk,h(s,a) X Nk-i^,a)[ZP ])
,,s,a	=
n
≤t
n
X 4XiPLPlogT
i=1
E wk,h(s,a) £苏,i(Vk,h+ι,s,a)
k,h,s,a	i=1
∖
n
X 4XiPLPlogT
i=1
n
X4XiPLPlogT
i=1
∖
n
X wk,h(s, a) X (σP,i(Vk,h+1, s, a) - σP,i(Vπ+ι,
k,h,s,a	i=1
≤t
n
X4XiPLP+
i=1
n
2H2nReg(K) X XPLP log T
i=1
≤t wk,h(s, a)
k,h,s,a
mn
m X σR ,i(S,a) + X σP ,i(Vnk ι,s,a)
i=1	i=1
n
X4XiPLPlogT+
i=1
n
2H2nReg(K) X XPLP log T
i=1
+
∖
∖
≤ Hht ∙
n
X4XiPLPlogT+
i=1
n
2H2nReg(K) X XPLP log T
i=1
The first inequality is due to Cauchy-Schwarz inequality. The second inequality is due to Lemma F.5.
The third inequality is due to Lemma F.8. The forth inequality is due to σR2,k,i (S, a) ≥ 0, and the
last inequality is because of Corollary 1.1.
For Eqn. 58, we have
wk,h(S, a)
k,h,s,a
Xn
i=1
2uk,h,i(s, a)LP
Nk-I((S,a)[ZΡ ])
≤ X ∖2LP IX,a Nk4w(k(,h,(S 薪]) )1X「(S，a)Uk，h，i(S，a)
n
≤	u64XiPLPlogT
i=1
k,h(S, a)uk,h,i(S, a)
(62)
k,h,s,a
By Lemma F.20, We know that the summation Pk h§a wk,h(s, a)uk,h,i(s, a) is of order O(T 1).
This means that Equ. 62 is of order O(T4), which doesn't contribute to the main term (O(√T)).
□
35
Published as a conference paper at ICLR 2021
LemmaE14. Under event Λι, Λ2 and Ω, suppose Reg(K) = PK=I Vk,1(s1) - V∏k (si), we have
KH
XX X wk,
h(s,a) (Pk- P) V*(s,a)
k=1 h=1 (s,a)∈X
≤O It
n
(HT + nH2Reg(K)) XXPLp
i=1
Proof. By Lemma F.1, we have
k1 H
XX X wk,
h(s,a) (Pk - P) V*(s,a)
k=1 h=1 (s,a)∈X
n
≤XXX X
k h i=1 (s,a)∈X
wk,h(s, a)
∕2σP ,i (Vh+1, s, a)LP
N Nk-i((s,a)[ZP ])
nn
+XXX X wk,h(s,a) X
k h i=1 (s,a)∈X	j 6=i,j=1
36H∣Si∣∣Sj I(LP )2
qNk-i((s,a)[ZP])Nk-i((s,a)[Zp ])
By Lemma F.5, the second term has only logarithmic dependence on T, which is negligible com-
pared with the main factor. We mainly focus on the first term.
n
XXXX
k h i=1 (s,a)∈X
2	2 ∕2σP,i(Vh+1, s, a)LP
wk,h(s,a)V Nk-i((s,a)∖ZP ])
n
E E 2wk,h(s,a) £理,i(Vh+ι,s,a) ∙
k,h (s,a)∈X	i=1
un
tuX X X
k,h (s,a)∈X i=1
Wk,h(s, a)Lp
Nk-i((s,π(s))[ZP ])
≤t
mn
m X σR ,i(s, a) + X σP ,i(Vh+ι, s, a)
i=1	i=1
wk,h(s,a)LP
⅛ (s⅜∈x i=1 Nl((S,∏(s))Zp ])
+ nH 2Reg∣
n
8XXiPLPlogT
i=1
The first inequality is due to Cauchy-Schwarz inequality. The second inequality is due to
σR2 ,k,i(s, a) ≥ 0. For the last inequality, the first part is the summation of the variance, which can be
bounded by Lemma 1.1 and Lemma F.8, while the second part can be bounded as Pi XiP LP logT
by Lemma F.5.	□
LemmaE15. Under event Λι, Λ2 and Ω, suppose Reg(K) = PK=I Vk,1(s1) - Vnk(SI), we have
KH
XX X wk,
h(s,a) (Pk - P)(Vk,h+1 -Vh+i) (s,α) ≤ O
k=1 h=1 (s,a)∈X
n _________ n ___________
X √TΧf |Sj∣Lp X 2/8ΧPLP logT
i=1	i=1
≤
t
∖
36
Published as a conference paper at ICLR 2021
Proof. By Lemma E.1, we can prove that
-P) (Vk,h+1 - Vh+l) (S, a)
n
≤ X(Pk,i - Pi)P[1：i-1]P[i+1：n] (Vk,h(sk,h, ak,h ) - Vh (sk,h , ak,h )
i=1
nn
+ XXH I (Pk,i - P)(∙∣(s,a)[ZPDLKPk,j- Pj)(∙∣(s,a)[zp])jɪ
i=1 j=1
≤ X(2	X
i=1	s0[i]∈Si
nn
f"i(χZp¾p 卜11P …(V k，h(sk，h，a®，“)— Vh(Sk，h，aΑ，"))
+	36H
i=1 j 6=i,j=1
∣Si∣∣Sj∣(Lp )2________+ X
qNk-i((s,a)[ZP])Nk-i((s,a)[Zp])	i=ι
|Si|LP
3Nk-i((s,a)[ZP ])
The second inequality is due to Lemma D.1.
We only focus on the summation of the first term, since the summation of other terms has only
logarithmic dependence on T by Lemma F.5.
n
wk，h(S,a)
2 X『NO?SSaa)ZPPLP) P[i：i—i]Pi+i：n (Vk,h(Sk,h"- Vh(Sk,h,ak,h)
n
wk，h(S, a)P1:i-1
i=1 k，h s，a
n
≤	P1:i-1	wk，h(S,a)
n
≤	wk，h(S,a)
i=1 k，h s，a
n
≤Xi=12utu
u
2 X t
s0 [i]∈Si
(
2 X
s0 [i]∈Si
Pi(S0[i]|X [ZP DLP (P[i+1:n] (V k,h(Sk,h, ak,h ) - Vh*(Sk,h, ak,h)))
Nk-1 ((S,a)[ZP ])
Pi(S0[i]|(S,a)[ZPDLP (P[i+1：n] (V k,h (Sk,h,ak,h) - V k,h(Sk,h,ak,h)y)2
∖	Nk-I((S,a)[Zip ])
.-	/	/一 ..	..、、)
∣Si∣LPEs (E[i+rn] (Vk,h(S0) -Vk,h(S0)))
Nk-1 ((S,a)[ZP ])
X N W：(S(a；ZP]) I ( X Wk,h(S,a)E[i：i] (E[i+I：n] (Vk,h+ι(S0) — Vk,h+ι(S0)))2
k，h，s，a k-1	, i	k，h，s，a
u
卜U
n _____________
≤ X 2,8XPLP log T
i=1
n ___________
H2 X TTXS|Sj |LP
i=1
The first inequality is due to the fact that Vk,h ≥ Vh= ≥ Vk,h The second and the third inequality is
due to Cauchy-Schwarz inequality. The last inequality is because of Lemma F.5 and Lemma F.20.
□
Lemma F.16. Under event Λι, Λ) and Ω, we have
X X Wk,h(Sh,ah) (Rk(Sh,ah) — R(Sh,ah)) ≤ O g X JtXrLr logT
k，h sh，ah	i=1
37
Published as a conference paper at ICLR 2021
Proof. By Lemma F.1, we have
XX
wk,h (Sh , ah ) Rk (Sh , ah ) - R(Sh, ah)
k,h sh,ah
1n
≤ m∑∑ Σ wk,h(Sh,ah
i=1 k,h sh,ah
1m
≤ m∑∑ Σ wk,h(Sh,ah
i=1 k,h sh,ah
8LiR
2σR,k,i (sh,ah )LR
Nk-i((sh,ah)[ZR]) + 3Nk-i((sh,ah)[ZR])
2LiR
8LiR
Nk-i((sh,ah)[ZR]) + 3Nk-i((sh,ah)[ZR])
1 m-^ /Pk,h Psh,αh 2wk,h(sh, ah)LR 1 m^	(	、
≤ m XV	N0(…ZR])	+ m X XsXhwk，h (Sh ah)
8LiR
3Nk-i((sh,ah)[ZR])
The second inequality is due to σR,k,i(s, a) ≤ 1. The last inequality is due to CaUchy-SchWarz
inequality. By Lemma F.5, we know that the summation is of order O (m PmtI pTXiRLR log T).
□
Lemma F.17. Under event Λ1 and Λ2, we have
(Vk,h- Vk,h) (S) ≤ Etrajk
G (…( f < ∣2H2 bg(18nTX0]“)∖ ,	_
S [	k(S,πk,i(S))+ A Nk-I((S,∏k,j(S))[ZP]) J Sh = S,πk
The expectation is over all possible trajectories in episode k given Sh = S following policy πk.
Proof.
(Vk,h - Vk,h)(S)
._ , , . . ʌ , . _________________ , ..
=2CBk (S,πk,h(S) + Pk(Vk,h+1(S)- Vk,h+1(S)')
=2CBk (S,πk,h(Sy) + (Pk- P)(Vk,h+1 - Vk,h+1)(S,nk,h(S)) + P(Vk,h+1 - Vk,h+1)(S,nk,h(S))
≤Xj=n1
H
=Etrajk X (2CBk (s, πk,i (Siy) + (Pk - P)(Vk,i+1 - Vk,i+1 )(Si, πk,i (Si))) | Sh = s, πk
i=h
The second term can be bounded as:
(Pk- P)(Vk,i+l - Vk,i+l)(Si,πk,i(Siy
.ʌ -. . — - - _ , , ,..
≤lPk - P|1|Vk,i+1 - Vk,i+∕∞(si, πk,i (Sy)
n
≤H X ∣Pk,i -Pi∣l(∙∣Si,∏k,i(Si))
i=1
2H 2 LP
Nk-I((S尸k,j (S)) [ZP])
□
Lemma F.18. Under event Λ1 and Λ2, we have
KH
ΣΣ Σ wk,h (S, a)CBk2 (S, a)
k=1 h=1 (s,a)∈X
≤ X 2(m + 2)HLRXRlog T + X 128n(m + n)H2Xf LP X |Sj |LP log T,
i=1	m	i=1	j=1
Which has only logarithmic dependence on T.
38
Published as a conference paper at ICLR 2021
Note that this bound is loose w.r.t parameters such as H, |Sj|, XiP and XiR. However, itis acceptable
since we regard T as the dominant parameter. This bound doesn’t influence the dominant factor in
the final regret.
Proof. By the definition of CBk(s, a), CBkR,i(s, a) and CBkP,i(s, a), we have
CBik(S,a) ≤(m+n)(X m (CBRi(S, a) +
m1
≤2(m + n)£ — ■
m2
i=1
n
+ 4n(m + n)
i=1
n (
+ 4n(m + n)
i=1
2H2LiR
64(LR)2
Nk-i((S,a)[ZR])+ 9(Nk-i((S,a)[ZR ]))2
4H2LP
2HLP
Nk-i((s,a)[ZP ]) + Nk-i((s,a)[ZP ])
32H2LP
Nk-i((s,a)[ZP]) j=1
4∣Sj∣LP
Nk-ι((S,a)[ZP ]) + 3Nk-ι((S,a)[Zp ])
4|Sj|LP
)
The second inequality is due to σ2,i(s, a) ≤ 1, σp,i(Vk,h+ι, S, a) ≤ H2 and uk,h,i(s, a) ≤ H.
NoW We are ready to bound PkK=1 PhH=1 P(s,a)∈X wk,h (S, a)CBk2 (S, a):
KH
XXX
k=1 h=1 (s,a)∈X
KH
wk,h(s, a)CBk2 (s, a)
≤	wk,h(s,a)
k=1 h=1 (s,a)∈X
Xi=m1
m
≤X
i=1
2(m+2)H2LiRXiRlogT
2(m + 2)H 2LR	X
m2Nk-i((S,a)[ZR ]) + =
n
128n(m + n)H2LP Pjn=1 |Sj |LP
Nk-i((s,a)[ZP ])
m2
+	128n(m + n)H2XiPLP	|Sj|LPlogT
i=1
j=1
n
The last inequality is due to Lemma F.5.
□
Lemma F.19. Under event Λ1 and Λ2, we have
KH	2
XXX wk,h(S,a)Es0[1:i]~P[i：i](.|s,a) (Es0[i+Ln]~P[i+Ln](∙∣s,a) (V k,h+1 - V k,h+J (SO)) ≤ O(log T),
k=1 h=1 (s,a)∈X
Here O hides the dependence on other parameters such as H, XiP, XiR except T.
Proof. For notation simplicity, We use Ei and 旧代引 as a shorthand of E§o团〜p{∣(s,α)[zP])and
Es0 [i j]~P[ij](∙∣s,a).
wk,h(S, a)E[1:i] [(E[i+Ln] (Vk,h+1 - Vk,h+l) (SO))]
Σ
k,h,s,a
≤X
k,h,s,a
wk,h(s, a)E[1:i]E[i+1:n] (Vk,h+1 - Vk,h+l) (S,a)
E wk,h(s,a)E[1：n] (Vk,h+1 - Vk,h+l)2 (s,a)
k,h,s,a
KH
ΣΣΣwk,h+1(s, a) (V k,h+1 - V k,h+J (S, a)
k=1 h=1 s,a
39
Published as a conference paper at ICLR 2021
Define Uk (s, a) = 2CBk (S, a) + Pn=ι Q Nk-HLZPi).
By Lemma F.17, we have
KH
ΣΣΣwk,h+1(s, a) (Vk,h+1 - Vk,h+l) (S,a)
k=1 h=1 s,a
2
≤	wk,h+1(S, a)	Σ Σ Pr(Sh1, ah1 |Sh+1 = S, ah+1 = a)Uk (Sh1, ah1)
k,h,s,a	h1=h+1 sh1,ah1
2
≤	wk,h+1(S, a)H Σ Σ Pr(Sh1, ah1 |Sh+1 = S, ah+1 = a)Uk (Sh1, ah1)
k,h,s,a	h1=h+1 sh1,ah1
H
≤	wk,h+1(S, a)H Σ Σ Pr(Sh1, ah1 |Sh+1 = S, ah+1 = a) (Uk (Sh1, ah1))2
k,h,s,a	h1=h+1 sh1,ah1
H
= H	wk,h1 (Sh1, ah1) (Uk (Sh1, ah1))2
k,h	h1=h+1 sh1,ah1
≤ X H 2X wk,h(Sh, ah) (Uk (Sh, ah))2	(63)
k,h	sh ,ah
Plugging the definition of Uk (S, a) into Equ. 63, we have:
KH
wk,h+l(s, a) (V k,h+1 - V k,h+J (S, a)	(64)
k=1 h=1 s,a
(	二 I	2 H 2LP	∖
2CBkGa)+ χ《Nk-1((S,^ZP]))	(65)
(„	二	2H 2LP	∖
CBk(S，a) + X Nk-1 ((s,a)[zp]))	(66)
≤X 2nH2 X	wk,h(Sh,ah)CBk2(S,a)+16nH4XiPLPlogT	(67)
k,h	sh ,ah
The last inequality is due to Lemma F.5. We can bound Pk,h 2nH2 Ps ,a wk,h(Sh, ah)CBk2(S, a)
by Lemma F.18. Summing up over all terms, we can show that
PK=ι Ph=I Ps,a wk,h+ι(s, a) (Vk,h+ι - Vk,h+ι)k (S, a) is of OrderO⑴.	□
Lemma F.20. Under event Λ1 and Λ2, for any i ∈ [n],we have
K H	n ________________
XXX wk,h(S,a)uk,h,i(S,a) ≤ O(H2 ∑ √TXP|SjILP),
k=1 h=1 (s,a)∈X	j =1
Here O hides the lower order terms w.r.t. T.
Proof. For notation simplicity, We use Ei and E^r as a shorthand of E§o[i]〜p{∣(s,a)[zP])and
Es0[ij]〜PZj-](∙∣(s,α)[ZP]). For those expectation w.r.t the empirical transition Pk, we use Ek to de-
note the corresponding expectation.
uk,h,i(S, a) is defined as:
uk,h,i (S, a) = E[1:i]
E[i+1:n] (Vk,h+1 - Vk,h+1
40
Published as a conference paper at ICLR 2021
Σ
k,h,s,a
X
k,h,s,a
ʌ
d
[i+1:n] (Vk,h+1 - Vk,h+l) (SO))
wk,h(s,a)E[1：i] [(E[i+Ln] (Vk,h+1 - Vk,h+J (SO))2]
+ E wk,hGa)E[i：i]
k,h,s,a
-	wk,h(s, a)E[1:i]
k,h,s,a
+	wk,h(s, a)E[1:i]
E[i+1:n] (Vk,h+1 - Vk,h+l) (SO))
E[i+1:n] (Vk,h+1 - Vk,h+l) (SO))
E[i+1:n] (Vk,h+1 - Vk,h+l) (SO))
k,h,s,a
-X wk,h(s,a)E[1:i] [(E[i+Ln] (Vk,h+1 - Vk,h+l) (SO))]
k,h,s,a
(68)
(69)
(70)
(71)
That is
We can bound Eqn. 70 and Eqn. 71 by Lemma D.1. For Eqn. 70, we have
wk,h
k,h,s,a
ʌ

[i+1:n] (Vk,h+1 - Vk,h+l) (SO))
-	wk,h(s, a)E[1:i]
k,h,s,a
E[i+1:n] (Vk,h+1 - Vk,h+l) (SO))
≤ X Wk,h(s,a) ∣P[Li](∙∣s,a) - P[i：i](・|s,a)L H2
k,h,s,a
≤ k,X,a wk，h(S，a) X fiSjH 2
i _________________
≤8H2 X JTXP∣Sj∣Lp logT
j=1
The first inequality is due to (E 区+上用 (V k,h+ι - V k,h+J (so)) ≤ H2 for any given so[1: i].
second inequality is due to Lemma D.1. The third inequality is due to Lemma F.5.
The
41
Published as a conference paper at ICLR 2021
For Eqn. 71, similarly we have
X wk,h(S,a)E[1:i]	(E[i+1:n] (Vk,h+1 - Vk,h+l) (SO))
2
k,h,s,a
-X wk,h(S,a)E[1:i] [(E[i+Ln] (Vk,h+1 - V-k,h+l) (SO))]
k,h,s,a
≤2H X wk,h(s, a)E[1:i] KE[i+1:n] (Vk,h+1 - Vk,h+l) (SO)) - (E[i+1:n] (Vk,h+1 - Vk,h+l) (SO))]
k,h,s,a
≤2H X Wk,h(S,a)E[i：i] [h 即+上旬(∙∣S,α) - P[i+i：n] (∙∣S,α)∣J
k,h,s,a
≤2H2kX,awk,h(S，a)E[1：i] jXι ∖/Nk-1ja)[ZP])
=2H HlXWkN ㈤ XI S Nk-jP[ZPD
n _______________________
≤16H2 X ^XXS |Sj ∣lp log t
j=i+1
Eqn. 69 can be bounded by Lemma F.19, which has only logarithmic dependence on X.	□
G Proof of Theorem 3
Proof. We consider the following two hard instances.
The first instance is an extension of the hard instance in Jaksch et al. (2010). They proposed a hard
instance for non-faCtored weakly-communicating MDP, which indicates that the lower bound in that
setting is Ω(VDSAX). When transformed to the hard instance for non-factored episodic MDP, it
shows a lower bound of order Ω( JHSAT) in episodic setting Azar et al. (2017); Jin et al. (2018).
Consider a factored MDP instance with d = m = n and X [ZiR] = X [ZiP] = Xi = Si × Ai, i =
1, ..., n. This factored MDP can be decomposed into n independent non-factored MDPs. By simply
setting these n non-factored MDPs to be the construction used in Jaksch et al. (2010), the regret
for each MDP is Ω(PH|X[Zp]|X). The total regret is Ω(P2ι PHTXiZPnT). Note that in
our setting, the reward R = ml Pmm=I R is [0,1]-bounded. Therefore, we need to normalize the
reward function in the hard instance by a factor of ^m. This leads to a final lower bound of order
Ω(mm Pn=I pH∣X[Zp]∣X) = Ω(n Pn=I PHXZPnT). SimiIar construction has been used to
prove the lower bound for factored weakly-communicating MDP (Xu & Tewari, 2020).
The second hard instance is an extension of the hard instance for stochastic multi-armed bandits.
The lower bound of stochastic multi-armed bandits shows that the regret of a MAB problem with
ko arms in Xo steps is lower bounded by Ω(√‰X0). Consider a factored MDP instance with d =
m = n and X[ZiR] = X[ZiP] = Xi = Si × Ai, i = 1, ..., n. There are m independent reward
functions, each associated with an independent deterministic transition. For reward function i, There
are log2(|Si|) levels of states, which form a binary tree of depth log2(|Si|). There are 2h-1 states
in level h, and thus |Si| - 1 states in total. Only those states in level log2(|Si|) have non-zero
rewards, the number of which is lSil. After taking actions at state so in level log2(∣S∕), the agent
will transits back to state SO in level log2(|Si|). That is to say, the agent can enter ”reward states”
at least H - log2(∣S∕) ≥ H times in one episodes. For each reward function i, the instance can
be regarded as an MAB problem ISiAiI arms running for KH steps1, thus the regret for reward i is
1The instance is not exactly an MAB with ISiAiI arms running for KH steps, since in each episode the
agent will choose a state s0, and then stay in s0 and choose different actions for H steps. However, this is a
mild difference and we can still follow the same proof idea of the lower bound for MAB (See e.g. Theorem
14.1 in Lattimore & SZePeSvari (2020))
42
Published as a conference paper at ICLR 2021
Ω( JISiAi| KH) = Ω(p∣X[ZR]|T). In this construction, the total reward function Can be regarded
as the average of m independent reward functions of m stochastic MDP. This indicates that the lower
bound is ω (m Pi=ι q∣X[ZR]∣ T) ≥ ω (m Pi=ι q∣XZRTτ).
To sum up, the regret is lower bounded by
(1 m ______________________________ n n	) ∖
max I m X JIX [ZR ]∣T,-X JHIX [ZP ]∣TJ,
which is of the same order as
(n ----------- n n ____________∖
mX JIX[ZR]∣T + -X JHIX[ZP]∣TJ.
i=1	j =1
□
H Omitted Details in Section 5
H.1 Specific instances
Figure 1: MDP Instances, Budget B0 = 0.5
We further explain the difference with two specific examples in Fig. H.1. No matter which setting
the previous work considers, the main idea of the algorithms in Efroni et al. (2020); Brantley et al.
(2020) is to explore the MDP environment, and then find a near-optimal policy satisfying that the
expected cumulative cost less than a constant vector B0, i.e. E[ h∈[H] ch] ≤ B0. However, in
our setting, the agent has to terminate the interaction once the total costs in this episode exceed
budget B . Because of this difference, their algorithm will converge to an sub-optimal policy with
unbounded regret in our setting. In the first MDP instance (Fig. H.1), the agent starts from state s0.
After taking action a1, it will transit to s1 with a deterministic cost c1 = 0.5. After taking action
a2, it will transit to s2. The cost of taking a2is 0 with prob. 0.5, and - with prob 0.5. There are no
rewards in state s0. In state s1 and s2, the agent will not suffer any costs. The deterministic rewards
are 0.5 and 0.8 respectively. s3 and s4 are termination states. The budget B0 is 0.5. For this MDP
instance, the optimal policy is to take action a1 in s0, since the agent can receive total rewards 0.5
by taking a1. If taking action a2, the agent will terminate at state s2 with no rewards with prob. 0.5,
which leads to an expected total rewards of 0.4. However, if we run the algorithm in Efroni et al.
(2020); Brantley et al. (2020), the algorithm will converge to the policy that always selects action a2
in s0, since the expected cumulative cost of taking a2is 0.5 ≤ B0.
We further show that the policies defined in the previous literature are not expressive enough in our
setting. In the second instance, the agent starts in state s0 with one action a0 . By taking a0 , the agent
transits to s1 with no rewards. The cost of taking a0 is 0 with prob. 0.5, and - with prob 0.5. In s1,
the agent needs to decide to take a1 or a2, with deterministic costs of 0 and 0.5 respectively. After
taking a1, the agent will transits to s2, in which it can obtain a reward r3 = 0.5. While by taking
a2, the agent can transits to s3, and obtain a reward r4 = -. The budget B = 0.5. In this instance,
the action taken in s1 depends on the remaining budget of the agent. That is to say, the policy is
not expressive enough if it is defined as a mapping from state to action. Instead, we need to define
it as a mapping from both state and remaining budget to action. However, previous literature only
considers policies on the state space, which cannot deal with this problem.
43
Published as a conference paper at ICLR 2021
H.2 Algorithm and Regret
We denote Vhπ (s, b) as the value function in state s at horizon h following policy π, and
the agent’s remaining budget is b. For notation simplicity, we define PSPCV (s, a) =
Ps0 Pc P(s0|s, a)P(C(s, a) = c0|s, a)V (s0, b - c0). We use PC,i(c0|s, a) to denote the ”tran-
sition probability” of budget i, i.e. P(Ci(s, a) = c0|s, a). The Bellman Equation of our setting is
written as:
Vhπ (s, b) =
R(s, ∏h(s,b)) + PSPCVh+ι(s, ∏h(s, b), b) b> 0
0
b≤0
(72)
Suppose Nk(s, a) denotes the number of times (s, a) has been encountered in the first k episodes.
We estimate the mean value of r(s, a), the transition matrix PS and PC in the following way:
A /	∖
Rk (s, a) =
Pk,h ɪ[sk,h = s, ak,h = a] ∙ rk,h
Nk-1(s, a)
Nk-i(s,a, s0)
Nk-i(s,a)
6	,,ʌ , ʌ ∑k,h ɪ[ɛk,h,i = c0,sk,h = s, ak,h = a]
PC,k,i (Ci(S, a) = c0|s, Q) = ---------------------,--ʌ--------------
Nk-1(s, a)
Following the definition in the factored MDP setting, we define the confidence bonus for rewards
and transition respectively (for 0 ≤ i ≤ d):
CBkR(s, a)
CBkP,i(s,a,b)
∕2σR(s,a) log(2SAT)	8log(2SAT)
N	Nk-1 (s, a)	+ 3Nk-i(s,a)
∕4σp ,i(V k,h+ι,s,a,b)L + ∣2uk,h,i (s,a,b)L
V	Nk-i(s,a)	N	Nk-i(s,a)
(73)
(74)
+H
j=1
32H2L X 〃	4nL
Nk-i(s, a) j=1 ((Nk-ι(s,a)
4nL
3Nk-1(s, a)
(75)
4 4 4∣Si∣LP	.	4|Si|L
Nk Nk-i(s, a)	3Nk-i(s, a)
4|Sj|LP
4|Sj|L
Nk-i(s,a) + 3Nk-i(s,a)广
(76)
where L = log(2dSAT) + d log(mB) is the logarithmic factors because of union bounds. The
additional d log(mB) is because that we need to take union bounds over all possible budget b. This
difference compared with factored MDP is mainly due to the noised offset model.
CBR(s, a) is the confidence bonus for rewards, and 3r(s, a) denotes the empirical variance of
reward R(s, a), which is defined as:
1 k-1 H	2
or(s, a) = N j(-) XX 1 [(s,a)k,h = (s,a)] ∙ (rk,h(sk,h,ak,h)) - (Rk (s,a))
Nk-1(s, a) k=1 h=1
+
n
CBpo(s, a) is the confidence bonus for state transition estimation PS, and {CBpi(s, a)}i=ι,…,d
is the confidence bonus for budget transition estimation {Pc,i}i=ι,…,d. σpi(Vk,h+ι, s, a) is the
empirical variance of corresponding transition:
σ0(Vk,h+1, s, a, b) = VarsO〜Ps,k(-∖(s,a)[ZP]) (Ec〜Pc,k(∙∣s,a)Vk,h+1(s，b -C))
σP,i(Vk,h+1, s,a, b)
=Es0~PS,k G\s,a)Ec[1：i-1]~PC,k,[1：i-1]G\s,ajVarci~PC,k,iG\s,a) (Ec[i+1：n]~PC,k,[i+1：d]G1s,a)"k,h+l(S , b
44
Published as a conference paper at ICLR 2021
where Pc,k,[di：d2] = Qi=2 d1 PC,k,i .
∕2uk,h,i(s,a,b)
V	Nk-ι(s,a)
is added to compensate the error due to the difference between
* and Vk,h+ι,
where uk,h,i (s, a) is defined as:
uk,h,0(s, a, b) = Es0〜^s,k(∙∣s,a)	(ESPC,k(∙∣s,a) (Vk,h+1 - Vk,h+l) (S'，b - C))
uk,h,i(s,a, b) = Es0~Ps,k(.|s,a)EC[i：i]~Pc,k,[i：i](.|s,a) J (EC[i+i：n]~Pc,k,[i+i：d](.|s,a) (Vk,⅛+1 - Vk,⅛+1) ISI b -
2
We calculate the optimistic value function and find the optimal policy π via the following value
iteration in our algorithm:
V h(S, b) = {
maxa{ [rR(s, a) + CB(s, a) + PSPCVh+ι(s, a, b)] }
0
b>0
b≤0
(77)
Algorithm 4 FMDP-BF for RLWK
Input: δ
Initialize N(S, a) = 0 for any (S, a) ∈ X
for episode k = 1, 2, •…do
Set Vk,H+ι(s, b) = Vk h+ι(s, b)=0 for all s, a, b.
5:	LetK = {(S, a) ∈ S × A : Nk(S,a) > 0}
for horizon h = H, H - 1, ..., 1 do
for S ∈ S and all possible budget b from 0 to B do
for a ∈ A do
if (S, a) ∈ K then
10:	Qk,h(S, a, b) = min{H,Rk(S,a) +CBk(S,a) + PS,kPC,kV k,h+1(S, a, b)}
elsj
Qk,h(S, a, b) = H
end if
15:
20:
end for	_
∏k,h(s, b) = argmaxa Qk,h(s,a,b)
Vk,h (s, b) = maxa∈A Qk,h (s, a, b)
Vk,h(s, b) = max{θ, Rk(s,∏k,h) — CBk(s,∏k,h,, b) + PkVk,h+ι(s, ∏k,h, b)}
end for
end for
for step h = 1,…，H do
E-I- - - , ♦一一	人 /	∖
Take action ak,h = arg maxa Qk,h(Sk,h, a)
end for
Update history trajectory L = LS{Si, ai,ri, Si+1}i=1,2,...,tk
end for
Proof. (Theorem 4) The proof follows almost the same proof framework of Thm. 2. The term
log(SAT) + dlog(Bm) is due to a union bound over all possible (T, S, a) and budget b. This
difference is because of the additional union bounds over all budget b.	□
H.3 Discussions about Assumption 1 and Assumption 2
Assumptions 1 and 2 limit our Algorithm 4 to problems with discrete costs. One may wonder
whether it is possible to construct -net for budget B and possible value of the costs when these
assumptions don’t hold. In that case, we only need to estimate the discrete cost distributions on
the -net, and then apply Algorithm 4 to tackle the problem. Unfortunately, we find that the -net
construction doesn’t work for continuous cost distributions, and it is unlikely to achieve efficient
45
Published as a conference paper at ICLR 2021
regret guarantee without further assumptions and the modification of the basic setting. If the cost
distributions are continuous and the remaining budget can take any value in R, a small perturbation
on the remaining budget may totally change the policy in the following steps and the optimal value.
To be more specific, suppose the agent enters a certain state with remaining budget b. There are
three actions to choose, with the cost of b - , b and b + , respectively. After suffering the cost, the
agent can achieve reward of 0, 0.5 and 1 respectively. Note that we can construct such hard instances
with extremely small. For these hard instances, we need to carefully estimate the value function
of any remaining budget b ∈ R and the density functions of the costs, after which we can calculate
the value through Bellman backup and find the optimal policy. However, estimating the density
functions requires infinite number of samples and makes the problem intractable. In other words, the
“non-smoothness” of the value and the policy w.r.t the remaining budget makes the problem difficult
for continuous value distribution without further assumptions. This “non-smoothness” phenomenon
also happens in the classical knapsack problem.
There are two possible ways to remove these assumptions and apply our algorithm to continuous
cost distributions with -net technique. The first idea is to allow the slight violation of the total
budget constraints, with the maximum violation threshold δ, or we assume that the value of any
initial state is lipschiz w.r.t the total budget B in a small neighborhood of B (With maximum L∞
distance δ). In that case, we can tolerate the estimation error of each cost function to be at most
H. e-net technique with E = H still works in this case and We can estimate the cost distribution
with precision H. This modification is somewhat reasonable since the agent,s policies are always
“smooth” w.r.t the total budget in a small region near B in many real applications such as games and
robotics. The second idea is to consider soft constraints. That is, when the budget constraints are
violated, the agent will suffer a loss that is linear w.r.t the violation of the constraints. We assume the
linear coefficient is relatively large compared with other parameters. This is also a possible method
to remove the non-smoothness w.r.t the total budget, which has wide applications in constrained
optimization.
46