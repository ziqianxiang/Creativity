Figure 1: Examples of how specialized attention heads in a Transformer recover protein structure andfunction, based solely on language model pre-training. Orange lines depict attention between aminoacids (line width proportional to attention weight; values below 0.1 hidden). Heads were selectedbased on correlation with ground-truth annotations of contact maps and binding sites. Visualizationsbased on the NGL Viewer (Rose et al., 2018; Rose & Hildebrand, 2015; Nguyen et al., 2017).
Figure 2: Agreement between attention and contact maPs across five Pretrained Transformer modelsfrom TAPE (a) and ProtTrans (b-e). The heatmaps ShoW the proportion of high-confidence attentionweights (αi,j > θ) from each head that connects Pairs of amino acids that are in contact with oneanother. In TapeBert (a), for example, We can see that 45% of attention in head 12-4 (the 12th layer’s4th head) maps to contacts. The bar plots shoW the maximum value from each layer. Note that thevertical striping in ProtAlbert (b) is likely due to cross-layer parameter sharing (see Appendix A.3).
Figure 3: ProPortion of attention focused on binding sites across five Pretrained models. The heatmaPsshow the ProPortion of high-confidence attention (αi,j > θ) from each head that is directed to bindingsites. In TaPeBert (a), for examPle, we can see that 49% of attention in head 11-6 (the 11th layer’s6th head) is directed to binding sites. The bar Plots show the maximum value from each layer.
Figure 5: Performance of probing classifiersby layer, sorted by task order in Figure 4.
Figure 4: Each plot shows the percentage ofattention focused on the given property, av-eraged over all heads within each layer. Theplots, sorted by center of gravity (red dashedline), show that heads in deeper layers focusrelatively more attention on binding sites andcontacts, whereas attention toward specificsecondary structures is more even across lay-ers.
Figure 7: Pairwise attention similarity (left) vs. substitution matrix (right) (codes in App. C.5)ACDEFGH1KLMNPha∕.lMv I・ACDE FGH I K LMN PQRSTVWYis encoded in attention weights primarily in the last 1-2 layers. These results are consistent withprior work in NLP that suggests deeper layers in text-based Transformers attend to more complexproperties (Vig & Belinkov, 2019) and encode higher-level representations (Raganato & Tiedemann,
Figure 8:	Percentage of each head’s attention that is focused on Helix secondary structure.
Figure 9:	Percentage of each head’s attention that is focused on Strand secondary structure.
Figure 10:	Percentage of each head’s attention that is focused on Turn/Bend secondary structure.
Figure 11: Top 10 heads (denoted by <layer>-<head>) for each model based on the proportion ofattention aligned with contact maPs [95% conf. intervals]. The differences between the attentionProPortions and the background frequency of contacts (orange dashed line) are statistically significant(p < 0.00001). Bonferroni correction aPPlied for both confidence intervals and tests (see APP. B.2).
Figure 12: ToP-10 contact-aligned heads for null models. See APPendix B.2 for details.
Figure 13: ToP 10 heads (denoted by <layer>-<head>) for eaCh model based on the ProPortion ofattention foCused on binding sites [95% Conf. intervals]. DifferenCes between attention ProPortionsand the baCkground frequenCy of binding sites (orange dashed line) are all statistiCally signifiCant(p < 0.00001). Bonferroni CorreCtion aPPlied for both ConfidenCe intervals and tests (see APP. B.2).
Figure 15: Percentage of each head’s attention that is focused on Post-translational modifications.
Figure 16: ToP 10 heads (denoted by <layer>-<head>) for eaCh model based on the ProPortion ofattention foCused on PTM Positions [95% Conf. intervals]. The differenCes between the attentionProPortions and the baCkground frequenCy of PTMs (orange dashed line) are statistiCally signifiCant(p < 0.00001). Bonferroni CorreCtion aPPlied for both ConfidenCe intervals and tests (see APP. B.2).
Figure 17:	ToP-10 heads most focused on PTMs for null models. See APPendix B.2 for details.
Figure 18:	Percentage of each head’s attention that is focused on the given amino acid, averaged overa dataset (TapeBert).
Figure 19:	Percentage of each head’s attention that is focused on the given amino acid, averaged overa dataset (cont.)Table 3: Amino acids and the corresponding maximally attentive heads in the standard and randomizedversions of TapeBert. The differences between the attention percentages for TapeBert and thebackground frequencies of each amino acid are all statistically significant (p < 0.00001) taking intoaccount the Bonferroni correction. See Appendix B.2 for details. The bolded numbers represent thehigher of the two values between the standard and random models. In all cases except for Glutamine,which was the amino acid with the lowest top attention proportion in the standard model (7.1), thestandard TapeBert model has higher values than the randomized version.
