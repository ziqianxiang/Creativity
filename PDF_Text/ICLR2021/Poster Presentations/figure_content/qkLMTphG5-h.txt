Figure 1: USA converts uncertainty into layer-wise adapted stepsize. Here α = 0.01. (a) is standarddeviation of trained ensemble models’ weights. Each layer has a difference uncertainty. (b) isadapted stepsize by USA. Each layer has a different stepsize based on the uncertainty.
Figure 2: Applying UFGSM to 4-ConvNet on miniImageNet with = 0.05. Starting from the cleanimage x, We add the signed gradient sig∏(Vχlθ (x, y)) after rescaling it by the uncertainty over theinput gradient u, to generate the adversarial example x0(UFGSM). Note how UFGSM generated amore natural image than FGSM (rightmost, u = 1).
Figure 3: Ablation study for 5-way 1-shot classification on Flowers dataset.
Figure 4:	Cross-domain performance while meta-training on miniImageNet. X-axis is training iter-ation on meta-training on miniImageNet and y-axis is classification accuracy.
Figure 5:	5-way 1-shot classification results with AT on SGD.
Figure 6:	5-way 1-shot classification results with AT on Adam.
Figure 9:	5-way 1-shot classification results to verify proposal 2. We compared to UFGSM,I/UFGSM and FGSM.
Figure 10:	(a) shows the re-scaled input gradient uncertainty to generate the UFGSM example. (b)is a flatten plot of the re-scaled input gradient uncertainty. (c) shows a histogram of the uncertainty.
Figure 11:	(a) shows the re-scaled inverse input gradient uncertainty to generate the UFGSM exam-ple. (b) is a flatten plot of the re-scaled inverse input gradient uncertainty. (c) shows a histogram ofthe uncertainty.
Figure 12: 5-way 1-shot (Top) and 5-way 5-shot (Bottom) classification results of freezing BNlayers. We tested freezing BN on SGD+All. SGD+All (w/ BN freezing) outperformed SGD+All(w/o BN freezing) in the higher stepsize range (>0.1).
