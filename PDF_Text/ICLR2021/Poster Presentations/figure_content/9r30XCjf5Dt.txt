Figure 1: An example of difficult poisoning.
Figure 2: Online PoiSoning-Iearning process.
Figure 3:	Comparison of mean per-episode reward gained by VPG, PPO, A2C, ACKTR on various environ-ments, under no poisoning, random poisoning, AC-P and VA2C-P.
Figure 4:	(a) VA2C-P also works under black-box setting; (b) hybrid-aim poisoning could be better thansingle-aim poisoning; (c)(d) VA2C-P successfully forces the agent to choose the target policy.
Figure 5: Different poison aims of poisoning in supervised learning and reinforcement learning.
Figure 6: The online poisoning vs learning. Blue solid lines denote the learning processes, while reddashed lines denote the poisoning processes. in iteration k, the learner uses its previous policy πk-1 to rollout observations Ok from current MDP Mk, then updates its model and policy by πk = f (πk-1, Ok) =argmaxπ J (π, πk-1 , O). The attacker may (a) poison observations after they are generated, (b) poison MDPbefore the learner generates observations, or (c) poison the policy when it is used to generate observations.
Figure 7: An intuitive example of MDP poisoning.
Figure 8:	Additional Experimental ResultsG.4 Extension to Off-policy LearnersAlthough we focus on on-policy policy gradient learners in this paper, our poisoning method isalso applicable to off-policy learners which update their policies using sampled mini-batches fromall historical observation (trajectories). If the adversary can manipulate the mini-batch the learnersamples at every step, our proposed poisoning process works as usual. We implement this idea andtest it for one of the state-of-the-art off-policy learning method SAC (Haarnoja et al., 2018), and theresults are shown in Figure 9, where VA2C-P significantly reduces the reward gained by the learner.
Figure 9:	Poisoning off-policy algorithm SAC with VA2C-P. D = Or, C/K = 1, = 0.6.
