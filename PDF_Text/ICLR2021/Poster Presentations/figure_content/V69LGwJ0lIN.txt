Figure 1: Visualization of (a subset of) diverse datasets for (a) antmaze medium and (c) antmazelarge, along with trajectories sampled from CQL+OPAL trained on diverse datasets of (b) antmazemedium and (d) antmaze large.
Figure 2: Overview of offline RL with OPAL. OPAL is trained on unlabelled data D using autoen-coding objective. For offline RL, the encoder first labels the reward-labelled data Dr with latents,and divides it into Dhri and Dlro . The task policy πψ is trained on Dhri using offline RL while theprimitive policy πθ is finetuned on Dlo using behavioral cloning (BC).
Figure 3: State visitation heatmaps for antmaze medium policies learned using (1) CQL and (2)CQL+OPAL, and antmaze large policies learned using (3) CQL and (4) CQL+OPAL.
Figure 4: Visualization of(a subset of) dataset trajectories colored according to their assigned clusterusing (a) Offline DADS and (b) Offline CARML. We use k = 10.
Figure 5: State visitation heatmaps for antmaze medium policies learned using (1) CQL, (2)CQL+OPAL, (2) CQL+Offline DADS and (4) CQL+Offline CARML. Note that while offlineCARML and offline DADS get stuck at various corners of the maze, OPAL is able to find its paththrough the maze to the goal location on the top right.
