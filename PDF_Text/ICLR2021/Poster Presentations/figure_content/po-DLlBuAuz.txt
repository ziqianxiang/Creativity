Figure 1: (a) A grid world with sparse rewards.
Figure 2: Visualization of the error in soft Q value estimation and quality of the learned policy. In the firstfour columns, triangles represent the error for actions that move in different directions. Darker color indicateshigher error. To investigate the performance of the learned policy πθ1000, the length of arrows represents theprobability of taking each actions in each states. We run πθ1000 in the grid world and visualize the visitationcount in the last column (heatmap). Darker color means more visitation.
Figure 3: Learning curves of average reward over 5 runs on Mujoco tasks with α = 0.6. The shaded area withlight color indicates the standard deviation of the reward. The gray vertical lines on the yellow curves indicatewhere we take the checkpointed policy, according to the measure of Q value variance, as our final solution.
Figure 4: Learning curves of average reward over 3 runs on Atari games.
