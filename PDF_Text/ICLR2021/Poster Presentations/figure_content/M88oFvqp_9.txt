Figure 1: Each block above follows [Appearance, Shape → Output]. We propose a gener-ative model that disentangles and combines shape and appearance factors across multiple domains,to create hybrid images which do not exist in any single domain.
Figure 2: Each domain can berepresented with e.g., a set ofobject shapes (XA/B) and ap-pearances (YA/B). The abilityto generate images of the formIAA/BB requires the system tolearn intra-domain disentangle-ment (Singh et al., 2019) of la-tent factors, whereas the abil-ity to generate images of theform IAB (appearance/shapefrom domain A/B, respectively)requires inter-domain disentan-glement of factors, which is thegoal of this work.
Figure 3: Left: A simplified model architecture of the base model FineGAN (Singh et al., 2019),where images are generated in a stagewise manner, by generating the background, shape, and ap-pearance of the object, in that respective order. Each of the latent codes controls certain propertiesabout the image (e.g. x code controlling the shape). Right: The intra-domain disentanglement ca-pability of the base model. Each block demonstrates the effect of changing one latent code (shownabove each image), which controls a single factor in the generated image.
Figure 4: (a) The process of computing the frequency-based representation. All steps are differ-entiable, making the representation learnable as part of the optimization process. (b) We constructpositive samples differently for Lfilter and Lhybrid, whereas negatives remain the same for both.
Figure 5: Results using CycleGAN, MUNIT, and AdaIn. When the two domains don’t have struc-tural correspondence, the image-to-image translation methods fail to disentangle shape from ap-pearance. AdaIn suffers, as images don’t always have a homogeneous style to be extracted from,resulting in misalignment in the definition of object appearance and style.
Figure 6: FineGAN (top) vs. Ours (bottom). The only difference between the ‘Output’ and ‘Shape’image is the latent appearance vector, which is borrowed from the ‘Appearance’ image. Our outputspreserve the characteristic appearance details much better than FineGAN. More results in Sec. A.6.
Figure 7: Results of our method with different settings of filters used to approximate frequency-based representation. As the effective receptive field size increases from top-left to bottom right, wenotice that the method can better transfer characteristic appearance details.
Figure 8: Analysis of the effect of training only the appearance stage vs both shape and appearancestage.
Figure 9: Effect of the temperature parameter (τ), used in Lhybrid and Lfilterwe choose τ = 0.5 as the default value. In this section, we study whether our method’s performanceis sensitive to the value used for T. Results are presented in Fig. 9, where We consider the birds —cars setting, and run our algorithm for three values of τ, 0.1, 0.5 and 0.9. We observe that the abilityto accurately transfer the appearance from one object to another remains largely consistent acrossthe three cases. This indicates that our method is flexible, where one doesn’t need to tune the valueof τ for accurate inter-domain disentanglement.
Figure 10: Analysis of the resistance of a model to alter the appearance. Generations using FineGANdon’t fully relinquish the appearance, resulting in a low standard deviation for pixel changes acrossthe spatial locations compared to our approach.
Figure 11: Sample binary masks generated on the standard deviation image for different shapes, byapplying appropriate thresholdA.4.3 Evaluating part correspondences in appearance transferIn Sec. 2, we discussed that when the two domains under consideration have part correspondences(e.g. eyes/mouth of dogs and fox), transferring appearance from one to another happens in a waywhere properties around common object parts remain similar. We illustrate examples of this in Fig. 6animals 什 animals, where (i) the brown patches near the eye of the dog get transferred to the similarregion in husky (second last row), (ii) the brown patch near the cheeks of the dog get transferred tothe cheeks of the leopard.
Figure 12: Each cell block follows [Appearance, Shape → Output]. Assorted results fromdifferent multi-domain settings using our method.
Figure 13: StarGANv2 results on using birds, cars and dogs as multiple domains. The images inthe topmost rows and leftmost columns are real input images, while the rest are generated by thealgorithm.
Figure 14: Comparing the disentanglement learnt by StarGANv2 (Choi et al., 2020) and our method.
