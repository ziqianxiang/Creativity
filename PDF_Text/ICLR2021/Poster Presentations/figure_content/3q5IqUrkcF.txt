Figure 1: Implicit gradient regularization and explicit gradient regularization for a simple 2-d model.
Figure 2: Implicit regularization and test accuracy: (a) Here, each dot represents a different MLP,with different learning rates and network sizes. Implicit gradient regularization RIG is reported foreach model, at the time of maximum MNIST test accuracy and 100% train accuracy. We see thatmodels with larger implicit regularization rate λ have smaller values of RIG . (b) Networks withhigher values of λ also have higher maximum test accuracy values.
Figure 3: (a) We measure the loss surface slope at the time of maximum test accuracy for modelstrained on MNIST, and we observe that the loss surface slope is smaller for larger learning rates, andthe loss surface slopes remain small for larger parameter perturbations compared to models trainedwith small learning rates. We perturb our networks by adding multiplicative Gaussian noise to eachparameter (up to 300% of the original parameter size). (b) We measure the test accuracy robustnessof models trained to classify MNIST digits and see that the robustness increases as the learningrate increases. (Here, solid lines show averages across perturbations and dashed lines demarcateone standard deviation across 100 realizations.) (c) Explicit gradient regularization (EGR) for aResNet-18 trained on CIFAR-10.
