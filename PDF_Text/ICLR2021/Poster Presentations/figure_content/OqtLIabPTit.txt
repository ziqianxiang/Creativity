Figure 1: Feature spaces learned with different losses given an imbalanced dataset. The supervised CroSS-entropy (CE) learns a space biased to the dominant class. The space learned by unsupervised contrastive loss isbalanCed but less semantiCally disCriminative. Our proposed k-positive Contrastive loss learns a balanCed anddiscriminative feature space. The shadow area (	) indicates the decision boundary of each class.
Figure 2: Classification accuracy (left) and bal-ancedness (right) of the representations learned fromcross-entropy (CE) loss and contrastive loss (CL) ondatasets (LT0 to LT) with increasing imbalance.
Figure 3: Out-of-distribution generalization on Ima-geNet. Top 1 and Top 5 testing accuracy of the modelare learned from datasets LT0 to LT that are increas-ingly more imbalanced.
Figure 4: Comparison ofdifferent methods on theirfeature space balancedness(left) and class-wise accuracy(right). Here the linear clas-sifier is fine-tuned on the fullImageNet for representationevaluation (see Sec. 3.2).
Figure 5: Validation accuracies of KCL onImageNet-LT as the value of k varies.
Figure 6: Training instance num-ber distributions of the datasets weuse in our empirical studies.
