Figure 1: CSC (Algorithm 1). env.step(a) steps the simulator to the next state S and provides R(s, a) andC(s0) values to the agent. If C(s0) = 1 (failure), episode terminates. QC is the learned safety critic.
Figure 2: Illustrations of the five environments in our experiments: (a) 2D Point agent navigation avoidingtraps. (b) Car navigation avoiding traps. (c) Panda push without toppling. (d) Panda push within boundary. (e)Laikago walk without falling.
Figure 3: Top row: Average task rewards (higher is better). Bottom row: Average catastrophic failures (loweris better). x-axis: Number of episodes (each episode has 500 steps). Results on four of the five environmentswe consider for our experiments. For each environment, we plot the average task reward, the average episodicfailures, and the cumulative episodic failures. The safety threshold is χ = 0.03 for all the baselines in all theenvironments. Results are over four random seeds. Detailed results including plots of cumulative failures arein Fig. 6 of the Appendix.
Figure 4: Top row: Average task rewards (higher is better). Bottom row: Average catastrophic failures (loweris better). x-axis: Number of episodes (each episode has 500 steps). Results on four of the five environmentswe consider for our experiments. For each environment we plot the average task reward, the average episodicfailures, and the cumulative episodic failures. All the plots are for our method (CSC) with different safetythresholds χ, specified in the legend. From the plots it is evident that our method can naturally trade-off safetyfor task performance depending on how strict the safety threshold is set to. Results are over four random seeds.
Figure 5: Results on the five environments we consider for our experiments. For each environment we plotthe average task reward, the average episodic failures, and the cumulative episodic failures. All the plots arefor our method with different safety thresholds χ. From the plots it is evident that our method can naturallytrade-off safety for task performance depending on how strict the safety threshold χ is set to. In particular, for astricter χ (i.e. lesser value), the avg. failures decreases, and the task reward plot also has a slower convergencecompared to a less strict threshold.
Figure 6: Results on the five environments we consider for our experiments. For each environment we plot theaverage task reward, the average episodic failures, and the cumulative episodic failures. Since Laikago is anextremely challenging task, for all the baselines, we initialize the agent’s policy with a controller that has beentrained to keep the agent standing, while not in motion. The task then is to bootstrap learning so that the agentis able to remain standing while walking as well. The safety threshold χ = 0.05 for all the baselines in all theenvironments.
Figure 7: Comparison between two RL algorithms TRPO (Schulman et al., 2015a), and SAC (Haarnoja et al.,2018) in the Point agent 2D Navigation environment. We see that TRPO has slightly faster convergence interms of task rewards and also slightly lower average and cumulative failures, and so consider TRPO as theBase RL baseline in Figures 3 and 4.
Figure 8: Results on the Car navigation environment after seeding the replay buffer with 1000 tuples.
Figure 9: Results on the Car navigation environment using continuous safety signal.
