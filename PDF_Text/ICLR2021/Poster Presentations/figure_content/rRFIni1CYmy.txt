Figure 1: Overview of the ESM module. The module consists of projection and quantization steps,used to bring the belief from the previous agent frame to the current agent frame.
Figure 2: High level schematics of the ESM-integrated network architectures ESMN-RGB andESMN, as well as other baseline architectures used in the experiments: Mono, LSTM and NTM.
Figure 3: Visualization of (a) Drone Reacher and(b) Manipulator Reacher tasks.
Figure 4: Sample trajectories through the memory for (a) ESMN on DR-Ego-Shape, and ESMN-RGBon (b) DR-Ego-Color, (c) DR-Freeform-Color, (d) MR-Ego-Color, (e) MR-Freeform-Color. Theimages each correspond to features in the full 90 Ã— 180 memory at that particular timestep t.
Figure 6: Example image sequences from both egocentric (E) and freeform (F) cameras, on bothreacher tasks. The images are all time-aligned, and correspond to the same agent motion in the scene.
Figure 7: Top: Segmentation predictions in ESM memory for Mono and ESMN, and ground truth.
Figure 8: Image-to-action imitation learning network architectures for Mono, LSTM/NTM, ESMN-RGB and ESMN.
Figure 10: Top: pixel-wise attention classification targets for the LSTM-Aux network on DR (left)and MR (right) tasks. Bottom: the corresponding monocular images from the DR (left) and MR(right) tasks.
Figure 11: Auxiliary attention (left) and heading (right) losses on the validation set during trainingfor each of the different imitation learning reacher tasks.
Figure 9: Network architecture for the LSTM-Aux baseline.
Figure 12: Network losses on the training set (top 8) and validation set (bottom 8) during the courseof training for imitation learning from the offline datasets, for the different reacher task.
Figure 13: Principal Components (PCs) of the features from the pre-ESM encoder of the ESMNarchitecture on some example images for each of the four shape-conditioned reacher tasks, with eachof the six the principal components mapped to different colors to maximise clarity. PCs go from mostdominant on the left (green) to least dominant on the right (purple). Lighter values correspond tohigher PC activation, with black indicating low activation.
Figure 14: Average return during RL training on sequential reacher task over 5 seeds. Shaded regionrepresent the min and max across trials.
Figure 15: Object segmentation network architectures for Mono,ESMN-RGB and ESMN.
Figure 16: Left: point cloud representation of the ego-centric memory around the camera after fullrotation in ScanNet scene 0002-00, with RGB features. Mid: (top) Equivalent omni-directionalRGB image, (bottom) equivalent omni-directional depth image, both without smoothing to betterdemonstrate the quantization holes. Right: (top) A single RGB frame, (bottom) a single depth frame.
