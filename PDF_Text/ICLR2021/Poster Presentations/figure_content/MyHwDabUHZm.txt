Figure 1: Label Representations beyond Categorical Probabilities: We study the role of label repre-sentation in training neural networks for image classification. We find that high-dimensional labelswith high entropy lead to more robust and data-efficient feature learning.
Figure 2: Training and validationlosses on CIFAR-10 dataset withResNet-110 image encoder for mod-els with the speech / shuffled speech/ composition of Gaussians / con-stant matrix labels (left) and categor-ical labels (right). All of the mod-els are trained to converge. Themodel trained with constant matrixlabels converges slower than modelstrained with other high dimensionallabels.
Figure 3: Test accuracy under adversarial attacks on CIFAR-10 (left four columns) and CIFAR-100(right four columns). The accuracy evaluated by the threshold and the nearest neighbor is plottedin solid and dotted lines respectively. We show the results of targeted and untargeted FGSM anditerative method on three image encoders with three random seeds. The horizontal axis indicates thestrength of different attacks.
Figure 4: Test accuracy when limited training data is available. Accuracy is computed using thenearest-neighbor method5.4	What is Special about Audio Labels ?Our experiments for robustness and data efficiency suggest that high-dimensional labels hold someinteresting inherent property beyond just high-dimensionality that encourage learning of more robust7Published as a conference paper at ICLR 2021and effective features. We hypothesize that high-dimensional label representations with high entropyprovide stronger learning signals which give rise to better feature representations.
Figure 5: T-SNE progression for speech (toprow) and categorical (bottom row) modelswith ResNet-110 image encoder. From left toright, the plot shows 10%, 30%, 50%, 70%,and 100% progress in training. The speechmodel develops distinctive clusters at an ear-lier stage and has better separated clustersoverall.
Figure 6: Test accuracy under adversarial attacks on CIFAR-10 (left four columns) and CIFAR-100(right four columns) for the initially correct subset of the test images. The accuracy evaluated bythe threshold and the nearest neighbor is plotted in solid and dotted lines respectively. The generaltrend from the subset is similar to that from the full test set.
Figure 7: Full results of the robustness evaluation on CIFAR-10A.4 HyperparametersA.4. 1 Implementation DetailsWe train the categorical models for 200 epochs with a starting learning rate of 0.1, and decay thelearning rate by 0.1 at epoch 100 and 150. The high-dimensional models are trained for 600 epochswith the same initial learning rate, and we drop the learning rate by 0.1 at epoch 300 and 450. Allmodels are trained with a batch size of 128 using the SGD optimizer with 0.9 momentum and 0.0001weight decay. One exception is when train categorical models with the VGG19 image encoder, weuse a larger weight decay, 0.0005. We implement our models using PyTorch Paszke et al. (2019). Allexperiments are performed on a single GeForce RTX 2080 Ti GPU. The limited-data experimentsare conducted using the same settings as the full data experiments.
Figure 8: T-SNE of the best uniform (left), speech (middle), and categorical (right) models trainedwith the same random seed. Speech models show best separated clusters across all three models.
Figure 9: Grad-CAM visualization of learned features. Each triplet contains (from left to right) anunaltered image, a categorical model visualization, and a speech model visualization. From top tobottom, the activations are taken from some beginning, intermediate, and final layer in the respectiveimage encoders. Speech models learn more discriminative features than categorical models.
