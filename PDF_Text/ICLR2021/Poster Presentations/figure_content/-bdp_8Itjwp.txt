Figure 1: High-level takeaways for our main results using information diagrams. (a) We present to learnminimal and sufficient self-supervision: minimize H(ZX |S) for discarding task-irrelevant information andmaximize I(ZX; S) for extracting task-relevant information. (b) The resulting learned representation ZX *contains all task relevant information from the input with a potential loss info and discards task-irrelevantinformation with a fixed gap I(X; S|T). (c) Our core assumption: the self-supervised signal is approximatelyredundant to the input for the task-relevant information.
Figure 2: Remarks on contrastive and predictive learning objectives for self-supervised learning. Betweenthe representation ZX and the self-supervised signal S, contrastive objective performs mutual informationmaximization and predictive objectives perform log conditional likelihood maximization. We show that the SSLobjectives aim at extracting task-relevant and discarding task-irrelevant information. Last, we summarize thecomputational blocks for practical deployments for these objectives.
Figure 3: Comparisons for different compositions of SSL objectives on Omniglot and CIFAR10.
Figure 4: Comparisons for different settings on self-supervised visual-textual representation training. We reportmetrics on MS COCO validation set with mean and standard deviation from 5 random trials.
Figure 5: Comparisons for different objectives/compositions of SSL objectives on self-supervised visualrepresentation training. We report mean and its standard error from 5 random trials.
Figure 6: Comparisons for different self-supervised signal construction strategies. The differences between theinput and the self-supervised signals are {drawing styles, image augmentations} for our construction strategyand only {image augmentations} for SimCLR Chen et al. (2020)â€™s strategy. We choose LCL as our objective,reporting mean and its standard error from 5 random trials.
