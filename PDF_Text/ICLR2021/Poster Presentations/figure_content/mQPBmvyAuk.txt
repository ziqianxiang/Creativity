Figure 1: Illustration of our pipeline to create subpopulation shift benchmarks. Given a dataset,We define superclasses based on the semantic hierarchy of dataset classes. This allows us to treatthe dataset labels as subpopulation annotations. Then, we construct a Breeds task of specifiedgranularity (i.e., depth in the hierarchy) by posing the classification task in terms of superclasses atthat depth and then partitioning their respective subpopulations into the source and target domains.
Figure 3: Sample images from random object categories for the Entity-13 and Living-17 tasks.
Figure 4: Human performance on (binary) Breeds tasks. Annotators are provided with labeledimages from the source distribution for a pair of (undisclosed) superclasses, and asked to classifysamples from the target domain (‘T’) into one of the two groups. As a baseline we also measureannotator performance without subpopulation shift (i.e., on test images from the source domain, ‘S’)and tasks created via the WordNet hierarchy (cf. Appendix A.6). We observe that annotators are fairlyrobust to subpopulation shift. Further, they consistently perform better on Breeds task comparedto those based on WordNet directly—indicating that our modified class hierarchy is indeed bettercalibrated for object recognition. (We discuss model performance in Section 5.)Human performance. We find that, across all tasks, annotators perform well on unseen data fromthe source domain, as expected. More importantly, annotators also appear to be quite robust tosubpopulation shift, experiencing only a small accuracy drop between the source and target domains(cf. Figure 5). This indicates that the source and target domains are indeed perceptually similarfor humans, making these benchmarks suitable for studying model robustness. Finally, across allbenchmarks, annotators perform better on Breeds tasks, compared to their WordNet equivalents—even on source domain samples. This indicates that our modified class hierarchy is indeed betteraligned with the underlying visual recognition task.
Figure 5: Robustness of standard models to subpopulation shifts. For each task, we plot the accuracyof various model architectures (denoted by different symbols) on the target domain as a function oftheir source accuracy. We find that model accuracy drops significantly between domains (orange vs.
Figure 6: Effect of train-time interventions on model robustness to subpopulation shift. We measuremodel performance in terms of relative accuracy-i.e., the ratio between its target and source accu-racies. This allows us to visualize the accuracy-robustness trade-off along with the correspondingPareto frontier (dashed). (Also shown are 95% confidence intervals computed via bootstrapping.) Weobserve that some interventions do improve model robustness to subpopulation shift—specifically,erase noise and adversarial training—albeit by a small amount and often at the cost of source accuracy.
Figure 12: Sample MTurk annotation task to obtain human baselines for Breeds benchmarks.
Figure 15: Distribution of annotator accuracy over pairwise superclass classification tasks. Weobserve that human annotators consistently perform better on tasks constructed using our modifiedImageNet class hierarchy (i.e., Breeds) as opposed to those obtained directly from WordNet.
Figure 16: Per-class source and target accuracies for AlexNet on Breeds tasks.
Figure 18: Per-class source and target accuracies for DenseNet-121 on Breeds tasks.
Figure 19: Different ways to partition the subpopulations of a given superclass into the source andtarget domains. Depending on how closely related the subpopulations in the two domain are, we canconstruct splits that are more/less adversarial.
Figure 20: Model robustness as a function of the nature of subpopulation shift within specific Breedstasks. We vary how the underlying subpopulations of each superclass are split between the sourceand target domain—we compare random splits (used in the majority of our analysis), to ones thatare more (bad) or less adversarial (good). When models are tested on samples from the sourcedomain, they perform equally well across different splits, as one might expect. However, undersubpopulation shift (i.e., on samples from the target domain), model robustness varies drastically,and is considerably worse when the split is more adversarial. Yet, for all the splits, models havecomparable target accuracy after retraining their final layer.
Figure 21: Target accuracy of models after they have been retrained (only the final linear layer)on data from the target domain (with 95% bootstrap confidence intervals). Models trained withrobustness interventions often have higher target accuracy than standard models post retraining.
