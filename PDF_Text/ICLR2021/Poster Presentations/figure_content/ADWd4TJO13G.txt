Figure 1: Average gain w.r.t. no-components NFT across tasks and data sets immediately aftertraining on eaCh task (forward) and after all tasks had been trained (final), using soft ordering (top)and soft gating (bottom). Algorithms within our framework (C and D+C) outperformed baselines.
Figure 2: Learning curves averaged across MNIST and Fashion using ER and soft ordering. Eachcurve shows a single task trained for 100 epochs and continually evaluated during and after training.
Figure 3: Generated MNIST “4” digits on the last two tasks seen by compositional ER with softordering, varying the intensity with which a single specific component is selected. The learnedcomponent performs a functional primitive: the more the component is used (left to right on eachrow), the thinner the lines of the digit become. The magnitude of this effect decreases with depth(moving from top to bottom), with the digit completely disappearing at the earliest layers, but onlybecoming slightly sharper at the deepest layers. This effect is consistent across both tasks.
