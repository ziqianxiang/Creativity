Figure 1: Illustration of Lookahead-minmax(Alg.1) With GDA on: minx maxy X ∙ y, Withα=0.5. The solution, trajectory {xt, yt}tT=1,and the lines betWeen (xP, yP) and (xt, yt)are shoWn With red star, blue line, and dashedgreen line, resp. The backtracking step ofAlg. 1 (lines 10 & 11) alloWs the otherWisenon-converging GDA to converge, see § 4.2.1.
Figure 2: FID (1) of Lookahead-minmax on 32x 32 ImageNet(§ 5), see Fig. 12 for IS. We observe significant improvementsafter each LA-step, What empirically confirms the existenceof rotations and thus the intuition of the behaviour of LA-minmax on games, as Well as the relevance of the bilineargame for real World applications-Fig.1.
Figure 3: Distance to the optimum of equa-tion SB-G using different full-batch methods,averaged over 5 runs. Unless otherwise speci-fied, the learning rate is η = 0.3. See § 4.2.1.
Figure 4: Convergence of Adam, ExtraAdam, Extragradient, SVRE and LA-GDA, on equation SB-G, for severalminibatch sizes B, averaged over 5 runs-with random initialization of both the parameters and the data points(A, b and c). Fast (0, θ) and slow (φ, θ) weights OfLA-GDA are shown with solid and dashed lines, resp.
Figure 5: Eigenvalues of v0(θ, φ) at100K iterations on MNIST, see § 5.2.
Figure 6: Improved stability of LA-AltGAN relative to its baselines on SVHN, CIFAR-10 and ImageNet, over5 runs. The median and the individual runs are illustrated With ticker solid lines and With transparent lines,respectively. See § 5.2 for discussion.
Figure 7: Distance to the optimum as a function of the number of passes, for Adam, Extra-Adam, Extragradient,and LA-GAN, all combined with EMA (β = 0.999). For small batch sizes, the high variance of the problem ispreventing convergence for all methods but Lookahead-Minmax.
Figure 8: Sensitivity of LA-GDA to the value of the hyperparameter k in Alg. 1 for two combinations of batchsizes and η. The y-axis is the distance to the optimum at 20000 passes. The jolting of the curves is due to thefinal value being affected by how close it is to the last equation LA step, i.e. lines 10 and 11 of Alg. 1.
Figure 9: Illustration of Lookahead-minmax on the bilinear game minx maxy X ∙ y, for different values of thelearning rate η ∈ {0.1, 0.4, 1.5}, with fixed k = 5 and α = 0.5. The trajectory of the iterates is depicted withgreen line, whereas the the interpolated line between (ωt, ω t,k), t = 1,...,T, k ∈ R with ωt = (θt, Pt) isshown with dashed red line. The transparent lines depict the level curves of the loss function, and ω? = (0.0).
Figure 10: Convergence of GDA, LA-GDA, ExtraGrad, and LA-ExtraGrad on equation QP-1.
Figure 11: Convergence of GDA, LA-GDA, ExtraGrad, and LA-ExtraGrad on equation QP-2.
Figure 12: IS (higher is better) of LA-GAN on ImageNet With relatively large k = 10000. The backtrackingstep is significantly improving the model’s performance every 10000 iterations. This shows how a large k cantake advantage of the rotating gradient vector field.
Figure 13: LA-ExtraGrad, NLA-ExtraGrad and Extragrad models trained on ImageNet. For LA-Extragrad(k = 5000), the lighter and darker colors represent the fast and sloW Weights respectively. For NLA-ExtraGrad(ks = 5, kss = 5000), the lighter and darker colors represent the fast and super-sloW Weights respectively. Inour experiments on ImageNet, While LA-ExtraGrad is more stable than ExtraGrad, it still has a tendency todiverge early. Using Alg. 6 We notice a clear improvement in stability.
Figure 14: Comparison between different extensions of Lookahead to games on CIFAR-10. We use prefix jointand alt to denote Alg. 1 and Alg. 7, respectively of which the former is the one presented in the main paper. Wecan see some significant improvements in FID when using the joint implementation, for both LA-AltGAN (left)and LA-ExtraGrad (right).
Figure 15: (a): Comparison of the joint Lookahead-minmax implementation (Joint prefix, see Algorithm 1)and the alternating Lookahead-minmax implementation (Alt prefix, see Algorithm 7) on the SVHN dataset.
Figure 16: Samples from our generator model with the highest IS score. We can clearly see some unrealisticartefacts. We observed that the IS metric does not penalize these artefacts, whereas FID does penalize them.
Figure 17: Improved stability of LA-ExtraGrad relative to its ExtraGrad baseline on SVHN and CIFAR-10,over 5 runs. The median and the individual runs are illustrated With ticker solid lines and With transparent lines,respectively. See § I and H for discussion and details on the implementation, resp.
Figure 18: Analysis on MNIST at 100K iterations. Fig. 18a & 18b: Largest 20 eigenvalues of the Hessian ofthe generator and the discriminator. Fig. 18c: Eigenvalues of the Jacobian of JVF, indicating no rotations at thepoint of convergence of LA-AltGAN (see § 2).
Figure 19: Samples generated by our best performing trained generator on CIFAR-10, using LA-AltGAN andexponential moving average (EMA) on the slow weights. The obtained FID score is 12.193.
Figure 20: Images generated by our best model trained on 32×32 ImageNet, obtained with LA-AltGAN andEMA of the slow weights, yielding FID of 12.44.
Figure 21: Images generated by one of our best LA-ExtraGrad & EMA model (FID of 2.94) trained on SVHN.
