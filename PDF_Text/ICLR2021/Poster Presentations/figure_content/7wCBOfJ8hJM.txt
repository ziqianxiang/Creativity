Figure 1: An illustration of how the kNN distribution is computed. The datastore, which is con-structed offline, consists of representations of training set translation contexts and correspondingtarget tokens for every example in the parallel data. During generation, the query representation,conditioned on the test input as well as previously generated tokens, is used to retrieve the k nearestneighbors from the datastore, along with the corresponding target tokens. The distance from thequery is used to compute a distribution over the retrieved targets after applying a softmax tempera-ture. This distribution is the final kNN distribution.
Figure 2: Effect of the number of neighbors re-trieved and the softmax temperature on the val-idation BLEU score for en-zh. Temperaturesgreater than 1 are important to prevent the modelfrom overfitting to the most similar neighbor. Forhigher temperatures, more neighbors do not al-ways result in improvements.
Figure 3: Effect of datastore size on the valida-tion BLEU score for ru-en and en-zh. Perfor-mance improves monotonically with size but re-trieval can be slow for datastores containing bil-lions of tokens. Smaller datastores, which ac-count for a large fraction of the improvement, canbe used for faster retrieval.
Figure 4: Example retrievals using kNN-MT. Not only do the retrievals all correctly predict thetarget word military, but the local contexts tend to be semantically related. Both the source and thethree nearest retrievals express the concept of control over the military.
Figure 5:	An example where kNN-MT retrieves the same target token, changed, across all theneighbors. It matches local contexts on both the source and target sides, but not the global contextregarding Papua.
Figure 6:	An example where the model has access to a very short amount of target-side context thatis ambiguous. kNN-MT is able to rely on source context to resolve this and generate the correcttarget token, will.
