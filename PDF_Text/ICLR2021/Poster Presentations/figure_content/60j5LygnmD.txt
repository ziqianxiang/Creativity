Figure 1: Graphical model of data generationin mixed linear regressionis a vector of generating parameters (of dimension p), therefore p represents both the number ofparameters and the input dimension. All distributions are assumed Gaussian:ν2W 〜N w0,——IppX 〜N (0,Ip)	y|x, w 〜N (XTw,σ2)(4)3Published as a conference paper at ICLR 2021where Ip is the p×p identity matrix, σ is the label noise, w0 is the task mean and ν represents the taskvariability. Different meta-training tasks i correspond to different draws of generating parametersw(i), while the parameters for the meta-testing task are denoted by w0. We denote by superscripts t,v, r, s the training, validation, target and test data, respectively. A graphical model of data generationis shown in Figure 1.
Figure 2: Average test loss of MAML as a function of the learning rate, on overparameterizedmixed linear regression, as predicted by our theory and confirmed in experiments. a) Effect oflearning rate αr during adaptation. b) Effect of learning rate αt during training. The optimal learningrate during adaptation is positive, while that during training is negative. Values of parameters:nt = 30, nv = 2, nr = 20, m = 3, p = 60, σ = 1., ν = 0.5, ω0 = 0, w0 = 0. In panel a) we setαt = 0.2, in panel b) we set αr = 0.2. In the experiments, each run is evaluated on 100 test tasks of50 data points each, and each point is an average over 100 runs (a) or 1000 runs (b).
Figure 3: Average test loss as a function of the learning rate, on underparameterized mixed linearregression, as predicted by our theory and confirmed in experiments. a) Effect of learning rate αrduring testing. b) Effect of learning rate αt during training. The optimal learning rate during testingis always positive, while that during training is negative. Values of parameters: nt = 5, nv =25, nr = 10, m = 40,p = 30, σ = 0.2, ν = 0.2. In panel a) we set αt = 0.2, in panel b) we setαr = 0.2. In the experiments, the model is evaluated on 100 tasks of 50 data points each, and eachpoint is an average over 100 (a) or 1000 (b) runs.
Figure 4: Average test loss of MAML as a function of the learning rate, on overparameterized mixedlinear regression With Wishart covariances, as predicted by our theory and confirmed in experiments.
Figure 5: Average test loss of MAML as afunction of the learning rate, on nonlinear(quadratic) regression using a 2-layer feed-forward neural network. Optimal learningrate is negative, consistent with results on thelinear case. Each run is evaluated on 1000test tasks, and each point is an average over10 runs. Error bars show standard errors.
Figure 6: Average test loss of MAML as a function of the learning rate αt (training) on mixed linearregression, showing the transition from strongly overparameterized (a), to weakly overparameterized(b), weakly underparameterized (c) and strongly underparameterized (d). As expected, predictionsof theory are accurate only in panels (a) and (d). The amount of validation data increases from panels(a) to (d), with the following values: m = 1, nv = 2 (a), m = 5, nv = 5 (b), m = 10, nv = 10 (c),m = 10, nv = 40. Other parameters are equal to: nt = 40, nr = 40,p = 50, σ = 0.5., ν = 0.5,αr = 0.2, ω0 = 0, w0 = (0.1, 0.1, . . . , 0.1) (note that overfitting occurs since ω0 6= w0). In theexperiments, each run is evaluated on 100 test tasks of 50 data points each, and each point is anaverage over 100 runs.
