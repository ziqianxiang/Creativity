Figure 1: Simple illustration of channel tensorization (K = 2). We tensorize the channel dimensionof input feature as a multiplication of K sub-dimensions. Via performing spatial/temporal tensorseparable convolution along each sub-dimension, we can achieve a preferable balance between con-volutional efficiency and feature-interaction sufficiency. Introduction shows more explanations.
Figure 2: The pipelines of CT-Blocks and the overall architecture of CT-Net. We replace one ofevery two ResBlocks in ResNet with our CT-Block and the extra point-wise convolution in the lastsub-dimension (k = K) is ignored. More details can be found in Section3.3.
Figure 3: Comparison of visualization. Videos are sampled from Something-Something V1. Com-pared with R(2+1)D and CSN, our CT-Net can localize the action and object better both in spaceand time thanks to the larger spatial-temporal receptive field.
Figure 4: The implementation of our Tensor Excitation (TE) mechanism.
Figure 5: Accuracy vs per-clip GFLOPs on Kinetics-400.
