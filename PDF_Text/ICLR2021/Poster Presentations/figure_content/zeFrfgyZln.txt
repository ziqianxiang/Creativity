Figure 1: T-SNE (Maaten & Hinton,2008) representations of query, relevantdocuments, negative training instancesfrom BM25 (BM25 Neg) or randomlysampled (Rand Neg), and testing nega-tives (DR Neg) in dense retrieval.
Figure 2: ANCE Asynchronous Training. The Trainer learns the representation using negatives fromthe ANN index. The Inferencer uses a recent checkpoint to update the representation of documents inthe corpus and, once finished, refreshes the ANN index with most up-to-date encodings.
Figure 5: Training loss and testing NDCG of ANCE (FirstP) on documents.The sub captions list theANN index refreshing rate (e.g., per 10k Batch), Trainer:Inferencer GPU allocation (e.g., 4:4), andlearning rate (e.g., 1e-5). The x-axes are the training steps.
Figure 4: The loss and gradient norms during DR training (after BM25 warm up). The gradient normsare the per-layer average of the bottom (1-4), middle (5-8), and top (9-12) transformer layers. Blackdotted lines are the grad norm of the last layer in ANCE (FirstP). The x-axes are training steps.
Figure 6: t-SNE Plots for Winning Cases in Table 8. Qids and queries are listed in the sub-captions.
Figure 7: t-SNE Plots for Losing Cases in Table 9. Qids and queries are listed in the sub-captions.
