Figure 1: Our method acquires a policy for the tar-get domain by practicing in the source domain using a(learned) modified reward function.
Figure 2: Block diagram of DARC (Alg. 1)p(target l St, at, St+1) = p(St+1 l St,at,target)p(St, at l target)p(target)/p(St, at, St+1).
Figure 3: Tabular example of off-dynamics RLa policy that navigates directly to the goal (blue arroWs), and Will fail When used in the target do-main. We then apply our method: We collect trajectories from the source domain and real World tofit the tWo tabular classifiers. These classifiers give us a modified reWard, Which We use to learn apolicy in the source domain. The modified reWard causes our learned policy to navigate around theobstacle, Which succeeds in the target environment.
Figure 4: Visualizing the modified reward6---- DARC (ours)	RL on Target (le6 steps) ------ Finetuning	---- Importance Weighting --------- PETSRL on Target —— RL on Source	---- RL on Target (10x steps) ------- MBPO	—— MATLFigure 6: DARC compensates for crippled robots and obstacles: We apply DARC to four continuouscontrol tasks: three tasks (broken reacher, half cheetah, and ant) which are crippled in the target domain butnot the source domain, and one task (half cheetah obstacle) where the source domain omits the obstacle fromthe target domain. Note that naively ignoring the shift in dynamics (green dashed line) performs quite poorly,while directly learning on the crippled robot requires an order of magnitude more experience than our method.
Figure 6: DARC compensates for crippled robots and obstacles: We apply DARC to four continuouscontrol tasks: three tasks (broken reacher, half cheetah, and ant) which are crippled in the target domain butnot the source domain, and one task (half cheetah obstacle) where the source domain omits the obstacle fromthe target domain. Note that naively ignoring the shift in dynamics (green dashed line) performs quite poorly,while directly learning on the crippled robot requires an order of magnitude more experience than our method.
Figure 5: Environments: broken reacher, broken halfcheetah, broken ant, and half cheetah obstacle.
Figure 7: Ablation experiments (Left) DARC performs worse when only one classifier is used. (Right)Using input noise to regularize the classifiers boosts performance. Both plots show results for broken reacher;see Appendix D for results on all environments.
Figure 8: Without the reward correction, the agenttakes transitions where the source domain and target do-mains are dissimilar; after adding the reward correction,the agent’s transitions in the source domain are increas-ingly likely under the target domain.
Figure 9: Our method accounts for domain shift in thetermination condition, causing the agent to avoid tran-sitions that cause termination in the target domain.
Figure 10: Comparison with MATLdo not apply to MATL (Wulfmeier et al., 2017b) because their classifier is not conditioned on theaction. Indeed, our results on the four tasks in Fig. 6 indicate that DARC ourperforms MATL on alltasks. To highlight this difference, we compared DARC and MATL on a gridworld (right), wherethe source and target domains differed by assigning opposite effects to the “up” and “down” in thepurple state in the source and target domains. We collected data from a uniform random policy, sothe marginal distribution p(st+1 | st) was the same in the source and target domains, even thoughthe dynamics p(st+1 | st , at ) where different. In this domain, MATL fails to recognize that thesource and target domains are different. DARC succeeds in this task for 80% of trials while MATLsucceeds for 0% of trials. We conclude that the conditioning on the action, as suggested by ouranalysis, is especially important when using experience collected from stochastic policies.
Figure 11: Importance of using two classifiers:(left) on all environments.
Figure 12: Importance of regularizing the classifiers: Results of the ablation experiment fromFig. 7 (right) on all environments.
Figure 13: Copy of Fig. 8 overlaid with the targetdomain reward.
