Figure 1: A schematic illustration of the hierarchical policy execution. One high-level step corre-sponds to C low-level steps. The negative Euclidean distance in the latent space provides rewards forthe low-level policy.
Figure 2: (a) The NChain environment. (b) Re-sults on the 64-link chain environment. Eachline is the mean of 20 runs with shaded regionscorresponding confidential intervals of 95%.
Figure 3: Performance of each method on a suite of MuJoCo environments.
Figure 4: The color gradient of the trajectory is based on episode timestep (red for the beginning ofan episode, blue for the end). Black arrows denoting high-level actions point to the subgoals fromthe decision-making states. If the Ant robot moves directly towards the goal, it will fail to reach it,as it will push the movable block into the path to the goal.
Figure 5: Subgoal representations at different learning stages in the Ant Push (Images) task. The redtransparent arrows denote the trajectories from the start to the midpoint (easy goal), while the blueones denote the trajectories to the hard goal. The explored areas are visualized by 1000 experiencessampled from the replay buffer.
Figure 6: Ant FourRooms is a navigation task in a four-room maze. The source models are ran-domly picked from the runs shown in Figure 3.
Figure 7: A collection of environments that we use.
Figure 9: Early stopping the subgoal representation learning at early stages (0.2M, 0.35M, 0.5M and1M timesteps) in the Ant Push task with visual observations. The early stopping hurts the learningperformance, verifying that the subgoal representation function is gradually learned.
