Figure 1: ÎºI versus |I |Figure 2: Testing error versus number of passesAs compared to SGD (Section 4.1), Theorem 9 shows SARAH/SpiderBoost requires significantlyfewer iterations to achieve the same testing errors. This shows a clear advantage of stochasticvariance-reduced optimization over SGD in generalization other than training. Other than SARAHand SpiderBoost, we also develop generalization bounds for SVRG in Reddi et al. (2016), SCSG inLei et al. (2017) (Theorem D.3) and SNVRG-PL in Zhou et al. (2018a) (Theorem D.4).
Figure 2: Testing error versus number of passesAs compared to SGD (Section 4.1), Theorem 9 shows SARAH/SpiderBoost requires significantlyfewer iterations to achieve the same testing errors. This shows a clear advantage of stochasticvariance-reduced optimization over SGD in generalization other than training. Other than SARAHand SpiderBoost, we also develop generalization bounds for SVRG in Reddi et al. (2016), SCSG inLei et al. (2017) (Theorem D.3) and SNVRG-PL in Zhou et al. (2018a) (Theorem D.4).
