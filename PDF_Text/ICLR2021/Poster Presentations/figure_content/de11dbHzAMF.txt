Figure 1: CA-MTL base architecture with ouruncertainty-based sampling algorithm. Each taskhas its own decoder. The input embedding layer andthe lower Transformer layers are frozen. The up-per Transformer layer and Conditional Alignmentmodule are modulated with the task embedding.
Figure 2: Conditional Attention ModuleConditionalAttentionIScaIed DotProduct-l— s r —I— ʌ twhere iS the direct Sum operator (See Section A.6), Nis the number of block matrices An ∈ R(L/N) X(L/N) along the diagonal of the attention matrix,L iS the input Sequence, M(zi) = diag(A01, . . . , A0N) iS a block diagonal conditional matriX. Notethat An is constructed using L/N trainable and randomly initialized L/N dimensional vectors.
Figure 4: MT-Uncertainty vs. other task sam-pling strategies: median dev set scores on 8 GLUEtasks and using BERTBASE. Data for the Counter-factual and Task Size policy ∏∣task∣ (eq. 6) is fromGlover & Hokamp (2019).
Figure 5: CoLA/MNLI Dev set scores and Entropy forπrand (left) and MT-Uncertainty (right).
Figure 6: Task performance vs. avg. covariancesimilarity scores (eq. 7) for MTL and CA-MTL.
Figure 7: Effects of adding more datasets on avgGLUE scores. Experiments conducted on 3 epochs.
Figure 8: Task composition of MT-Uncertainty sampling and estimated task difficulty using EDM: numberof training samples per task at each iteration for batch size of 32. The occurrence of first peaks and estimateddifficulty follow the same order: From highest to lowest: MNLI > CoLA > RTE > QQP = MRPC > SST-2.
