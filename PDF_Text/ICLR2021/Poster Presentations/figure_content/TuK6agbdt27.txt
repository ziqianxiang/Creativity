Figure 1: A simplified illustration of ourproposed method where Λ refers to thewrite mechanism described in section3.1.1. Ft are the recurrent weights ofthe FWM which have been generated bythe LSTM. The LSTM is a regular slowRNN. The residual connection betweenthe FWM and the LSTM is not depicted.
Figure 2: QM model validation accuracy of the best-over-all seeds of each model over training steps.
Figure 3: A visualisation of the FWMs ability to chain independent associations to perform transitivereasoning on the catbAbI validation data. The colour of each grid cells represent the dot producthk1 0 k2, n 0 6 where k1, k2 are the write keys of each previous position while n, e refers tothe respective queries generated at “?” (second position from the right) for each of the Nr = 3memory reads. The first query matches most with the keys at the recent positions where the inputwas “gertrude” and “afraid” (first row of grid cells). The second query, which partially consists ofthe value retrieved from the first query, matches with the “getrude is a mouse” section. The thirdquery, which partially consists of the value retrieved from the second query, matches with the “miceare afraid of wolves” section. Finally, the FWM correctly outputs the next word and answer to thequestion: wolf (not seen). This likely completes the deduction: gertrude is a mouse, mice are afraidof wolves, therefore, gertrude is afraid of wolves.
Figure 4: Two randomly generated environments with the agent’s location coloured in green andthe reward location coloured in yellow. Edge labels indicate the set of valid actions (0, 1, or 2)to transition along that arrow. Invalid actions are not visualised. The graph and the locations ofthe agent and reward are set randomly at the beginning of the experiment. If the agent reaches thereward location or did not reach it after six steps, both are randomly reset.
Figure 5: Average total reward of the agent when trained on 600 random graphs (left plot) andtested on 600 different graphs (right plot). The FWM agent (blue) has a slow LSTM with 32 hiddenunits and a fast weight memory of size 16 × 162. We compare to LSTM agents with different sizedhidden states. The largest LSTM has 4096 hidden units (red) which roughly matches the number oftemporal variables of the FWM. The FWM has 14k trainable weights which is by far the lowest. Thelargest LSTM has 67.4M weights which is roughly 4814 times more than the FWM. The relativefactor of each LSTM is added to the legend. All LSTMs take longer to train and eventually overfiton the training data. Due to the overfitting, the LSTM does not have to explore, which results in ahigher total reward on training environments but a lower total reward on test environments.
Figure 6: Loss comparison between the LSTM and our FWM on a section of the PTB test set. Thecolour of the grid cells in the first row stands for the cross-entropy error of the LSTM and FWMmodel. The second row, for their respective difference. Our FWM sometimes shows a lower erroron rare subject words such as names of companies and people once they have been introduced.
Figure 7: Comparison of the FWM with the same seed but with different Nr .
Figure 8: FWM model with a concatenated keys compared with the tensor product of the keys.
Figure 9: Top: Hyperparameter search runs for different batch sizes and learning rates of the FWMmodel in the QM setting with the average accuracy on all tasks. Bottom: FWM performance over60,000 steps with three seeds.
Figure 10: Top: Hyperparameter search runs for different batch sizes and learning rates of the MNMmodel in the QM setting with the average accuracy on all tasks. Bottom: MNM model with threedifferent seeds, batch size 64, and learning rate 0.001 in the QM setting. Reported accuracy is theaverage on all tasks.
Figure 11: Hyperparameter search runs for different batch sizes and learning rates of theTransformer-XL in the QM setting with the average accuracy on all tasks. Left graph varies numberof layers and memory length. Right graph varies batch size and learning rate for 7 layers.
Figure 12: Long hyperparameter search runs for TXL with various layers and memory sizes. Theexperiments are grouped based on the number of layers. Many runs begin to diverge late into thetraining process.
Figure 13: Various seeds for the best Transformer-XL hyperparameters: 3-layers, memory windowsof 1200 tokens, a learning rate of 0.00025, and a batch size of 64.
Figure 14: Hyperparameter search runs for different batch sizes and learning rates of the LSTM inthe QM setting with the average accuracy on all tasks.
Figure 15: Average accuracy of three seeds of the best LSTM settings over all tasks on the catbAbIQM-mode dataset.
Figure 16: Hyperparameter search for the Fast Weights attending to the recent past by Ba et al.
Figure 17: Per-task test set performance comparison of the best catbAbI runs (first part).
Figure 18: Per-task test set performance comparison of the best catbAbI runs (second part).
