Figure 1: Two self-supervised pre-training objectives that teach text-to-text transformers withgenerative common sense: (1) Concept-to-Sentence Generation (C2S) pre-trains the model torecover the original sentence with a shuffled concept set, e.g., {forward, Simpson, ignore, information,prosecutor} → “The information was forwarded to Simpson’s prosecutors, but it was ignored.”(2) Concept Order Recovering (COR), similarly, teaches the model to correct the mispositionedconcepts in the original sentence. For example, the concepts (stops, fights, bar, drives, performance),are randomly reordered in the input, while the model should recover the original sentence.
Figure 2: Overview of Contrastive self-supervisedpre-training objectives. Generative QA style con-trastive objective requires the model to distinguishtruth sentences from less plausible ones.
Figure 3: Proposed Joint Training Framework. Given an input sentence x (“She was the firstwoman to hold the position.”), we extract concept-set C (woman, hold, position). Given x and C,we produce corrupted source sequence x0 either for C2S and COR. The generator trained with thecorresponding objective recovers sentences as distractors x00 to the discriminator. The discriminatoris trained to distinguish truth sentences from randomly selected distractor among two objectives.
Figure 4: Performance of compared models fine-tuned withdifferent fraction of the datasets.
