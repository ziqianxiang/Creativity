Figure 1: The toy example described in section 5.1. (a) The agent starts in the middle, and can moveto the right and to the left. The goal is to grab the flag when at the goal position. The agent collectsdata during two episodes indicated by the arrows. (b) The reward functions without shaping, withPB-RS, and with FV-RS. (c) The resulting Q-functions for policy π(s) = right, based on the datafrom the two episodes.
Figure 2: Robotic pushing task. (a) A box of size 0.4 × 0.4 × 0.2 (dark gray) lying on a table ofsize 3 × 3 (light gray) is supposed to be pushed to the green position by the spherical end effector(dark gray). (b) Top view of the table with 2-D projection of the planned 10-D trajectory (x and ycoordinates of the end effector). The planned trajectory is not executable in the noisy environment.
Figure 3: Comparison of the performance of PB-RS and FV-RS when using different reward shap-ing functions as defined in equation 14 and equation 15. (a) The first row shows a very simple ex-ample. (b) In this example, using ΦDiist shows the best result with PB-RS, followed by ΦGBuss(∙, 0.2)or ΦGBuss(∙, 0.4). (c) Similarly,中守 shows the best result with FV-RS, followed by ΦGVuss, whichseems to be robust against different choices of σ. (d) The second row shows a more difficult exam-ple, in which the end effector has to move around the box before initiating the push. (e) Here, usingthe purely distance-based potential function ΦPDBist is not successful, and the best result for PB-RS isachieved with ΦGVuss(∙, 0.2) or ΦGVuss(∙, 0.1). (f) With FV-RS, a similar result is obtained, althoughΦGVuss(∙, 0.2) performs significantly better here.
Figure 4: Three different slices of the function ΦGVuss(∙, 0.2) defined in equation 14, using theexample discussed in section 5.2 The state space is 10-dimensional; the figures show the functionvalues for fixed 7-dimensional box position and orientation (indicated by the dark grey boxes) andfixed z position of the end effector which is z = 0.14 for all three figures.
Figure 5: Additional pushing examples. Each row corresponds to a different example. With FV-RS,some agents are consistently successful after ca. 300 rollout episodes in all cases. With PB-RS, thereis one agents in the first example that learns to be consistently successful within the 1000 rolloutepisodes the experiment was run for. Across all examples, the training progress of the top quartileof agents is significantly faster with FV-RS than with PB-RS or without RS. Different evaluationsfrom the first example were also shown in figure 3Again, the plan (see figure 7b) is given as a sequence of states (p0,p1, ..., pL-1), and we constructthe potential-based shaped reward RPB and the final-volume-preserving reward RFV analogously toequation 12 and equation 13. This is motivated by the analysis done in appendix A.1, combined withthe observation that the scale and complexity pick-and-place example is comparable to the scale andcomplexity of the pushing example. In contrast to equation 13 however, the euclidean distanced(∙, ∙) also takes into account the binary information on whether a grasp has been established ornot. For PB-RS, we scale the potential function by K = 30. This is done in order to ensure that thereward functions of PB-RS and FV-RS are of similar scale. The results for 2 different pick-and-placeexamples are discussed in appendix A.2, specifically in figure 6. Here, we measure the asymptoticdistance over different agents and rollouts as detailed in section 5.2.
Figure 6: Pick-and-place examples. Each row corresponds to a different example. With FV-RS,some agents are consistently successful after ca. 300 rollout episodes in both cases. With PB-RSor without reward shaping, none of the agents were able to consistently reach the goal in theseexperiments. The training progress of the top quartile of agents is significantly faster with FV-RSthan with PB-RS or without reward shaping.
Figure 7: Robotic pick-and-place task. (a) A disk of radius 0.2 (dark gray) lying on a table of size3 × 3 (light gray) is supposed to be picked up in the red area and placed at the green position bythe spherical end effector (dark gray). (b) Top view of the table with 2-D projection of a plannedtrajectory (x and y coordinates of the end effector). The planned trajectory is not executable in thenoisy environment.
Figure 8: Pushing experiments using PPO instead of DDPG as the RL algorithm. (a, b, c) Onthe simpler example, FV-RS only slightly outperforms PB-RS. (d, e, f) This difference increaseshowever on the more difficult example. This relative advantage of using FV-RS over PB-RS usingPPO is very similar to the one reported in figure 3 for DDPG.
