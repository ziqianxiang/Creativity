Figure 1: Gradient Origin Networks (GONs; b) use gradients (dashed lines) as encodings thus only asingle network F is required, which can be an implicit representation network (c). Unlike VAEs (a)which use two networks, E and D, variational GONs (d) permit sampling with only one network.
Figure 2:	Gradient Origin Networks trained on CIFAR-10 are found to outperform autoencodersusing exactly the same architecture without the encoder, requiring half the number of parameters.
Figure 3:	The impact of activation function and number of latent variables on model performance fora GON measured by comparing reconstruction losses through training.
Figure 4:	Training implicit GONs with few parameters demonstrates their representation ability.
Figure 5:	By training an implicit GON on 32x32 images, then sampling at 256x256, super-resolutionis possible despite never observing high resolution data.
Figure 6: Spherical linear interpolations between points in the latent space for trained implicit GONsusing different datasets (approximately 2-10 minutes training per dataset on a single GPU).
Figure 7:	Random samples from a convolutional variational GON with normally distributed latents.
Figure 8:	Convergence of convolutional GONs with 74k parameters.
Figure 9: GONs are able to represent high resolution complex datasets to a high degree of fidelity.
Figure 10: Experiments comparing convolutional GONs with autoencoders on CIFAR-10, where theGON uses exactly same architecture as the AE, without the encoder. (a) At the limit autoencoderstend towards the identity function whereas GONs are unable to operate with no parameters. As thenumber of network parameters increases (b) and the latent size decreases (c), the performance lead ofGONs over AEs decreases due to diminishing returns/bottlenecking.
Figure 11: Training GONs with z0 sampled from a variety of normal distributions with differentstandard deviations σ, z0 〜N(0, σ21). Approach (a) directly uses the negative gradients asencodings while approach (b) performs one gradient descent style step initialised at z0 .
Figure 12: 2D latent space sam- Figure 13: Histogram of la-ples of an implicit GON clas- tent gradients after 800 implicitsifier trained on MNIST (class GON steps with a SIREN.
Figure 14:	Histogram of tra-ditional VAE latents after 800steps.
Figure 15:	GONs trained with early stopping can be sampled by approximating their latent spacewith a multivariate normal distribution. These images show samples from an implicit GON trainedwith early stopping.
Figure 16:	The discrepancy between training and test reconstruction losses when using a GON issmaller than equivalent autoencoders over a variety of datasets.
Figure 17: Super-sampling 28x28 MNIST test data at 256x256 coordinates using an implicit GON.
