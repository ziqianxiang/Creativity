Figure 1: Continous transition of a loss path between linear feedforward ("linear"), nonlinearfeedforward ("ReLU") and nonlinear residual ("ResNet") regimes. Graph: loss path near initializationof a ResNet56 v2 with LReLUs with negative slope α ∈ [0, 1] and residual branch weight ν ∈ [0, 1].
Figure 2: Absolute values of polynomialcoefficients: |tj | over j; see also Fig. 19.
Figure 3: Normalized power spectra zk per layer (red is deeper) in a 50-layer CNN at initialization,averaged over multiple neurons, for various φ. (a) On the left, all weights are varied and spectrum ofp(t) is shown. (b) On the right, only the first layer is varied and the spectrum ofp0(t) is shown.
Figure 4: "Average loss path" and its respective magnitude spectrum out of 500 random loss paths fordifferent variants of ResNet56 at initialization.
Figure 5:	Smoothness of the loss surface at ini-tialization for a Toy-CNN of varying depth withand without shortcuts.
Figure 6:	Average absolute path correlation perfrequency bin of the main branch and the resid-ual branch of a ResNet56 v2 with different ac-tivation functions at initialization. Lower binnumber indicates lower frequencies.
Figure 7: Normalized powerspectrum of the last layer ofa ResNet194 v1/v2 (post/pre-activation) at initialization.
Figure 8: Left: Comparing the training performance on Cifar10of a feature-averaging ResNet56 v1 "NoShort" to its vanilla,wide and residual equivalent. Right: Spectrum shift of averagingToy-CNN 50 layers at initialization, varying averaging factor a.
Figure 9: Comparing w-o smoothness and trainability for ResNets with varying depth and amountof nonlinearity. Upper row: training accuracy after 30 epochs of training on Cifar10. Bottom row:blueshift at initialization indicated by the fractal coefficient of the network’s last layer.
Figure 10: Repeating the experiments of Figure 8 for a single run on the Cifar100 dataset.
Figure 12: Repeating the experi-ments of Figure 3 (all layers, un-scaled) for TanH activations withdelta-orthogonal initialization for dif-ferent path lengths α.
Figure 11: Repeating the experiments of Figure 3 (alllayers, unscaled) during training.
Figure 13: Repeating the experiments of Figure3 (all layers, unscaled) at initialization in gradi-ent direction.
Figure 14: Repeating the experiments of Fig-ure 3 (all layers, unscaled) for variable networkwidth.
Figure 15:	Resulting loss paths and respectivepath-normalized power spectra when varyingparameters in only one layer of a Toy-CNN 50at initialization. The color of a path indicatesin which layer parameters were varied (red isdeeper).
Figure 16:	Smoothness of the loss surface atinitialization for a Toy-CNN of varying depthwith and without shortcuts on MNIST for abatch size of 256.
Figure 17: Repeating the experiments of Figure 3 (all layers, unscaled) for more nonlinearities.
Figure 18: Various Nonlinearities with their Chebyshev-Polynomial Approximation within theinterval [-5, 5].
Figure 19: Absolute value of the polynomial coeffients of the Chebyshev polynomial approximation.
