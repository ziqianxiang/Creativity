Figure 1: RL without plan-ning fails without resets.
Figure 2: Schematic for Lifelong Skill Planning (LiSP). LiSP learns a set of skills using syntheticmodel rollouts and performs long-horizon planning in the skill-space for stable, safe lifelong acting.
Figure 3: Learning without resets. Vertical lines denote task changes. Each blue line represents oneseed of LiSP out of 5; the other algorithms have lower variance (since they fail), so we only showthe mean of 3 seeds. Performance is normalized against 1 (for more details, see Appendix E.1.1).
Figure 4: 2D Volcano environment.
Figure 5: `2 error in the next state prediction of the dynamics model fφ for actions sampled fromthe skill policy vs. uniformly at random. The error variance is greatly reduced by only using skills.
Figure 6:	Long-horizon planning in Lifelong Hopper with skills instead of actions. Action-spaceplanning is not stable with long horizons. The dashed red line denotes the target velocity.
Figure 7:	Pretrained action-space MPC agent with a terminal value function placed in a reset-freesetting with continued gradient updates to the model and policy/value. Unlike the naive SAC agentthat acts according to greedy “1-step” planning, the MPC agent plans vs the same critic over a short5-step horizon and mostly avoids failure. Note this graph shows the same setting as Figure 1.
Figure 8: 2D Minecraft learning curves with varying skill-practice distributions. Practicing skillsthat maximize intrinsic reward outperforms naive methods for choosing which skills to practice.
Figure 9: Skill learning collapses for DADS-Off in Ant, even without the primitive dynamics modelfφ . Shown are the x-y positions, where each color denotes one skill starting from the origin. Morediverse skills are better. From left to right, skills are resampled during training every 1 (fully online,like LiSP), 5, 20, and 200 (fully episodic) timesteps.
Figure 10: Entropy of skills proposed by skill-practice distribution. The red line shows the maximumpossible entropy, which exists as the model class is a TanhGaussian (Haarnoja et al., 2018).
Figure 11: Intrinsic reward during the offline phase of training. Note that these are substantiallydifferent than those from episodic training, highlighting the role of exploration in skill discovery.
Figure 12:	Average height of the hopper for an evaluation episode of length 200 with randomlysampled latents, a proxy for measuring the average empowerment of the learned skills. LiSP showsa considerable improvement in sample efficiency vs DADS in this regard.
Figure 13:	Volcano environment (left) and 2D Minecraft environment (right). Colors denote differ-ent skills/latents learned by LiSP. Most skills seek to perform behaviors relevant to the task (reachingthe goal, interacting With the tiles), due to being trained on the data distribution of the agent.
