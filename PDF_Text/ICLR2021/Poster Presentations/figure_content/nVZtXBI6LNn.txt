Figure 1: Relaxations of a ReLU: (a) “triangle” relaxation in LP; (b)(c) No relaxation when u(i) ≤ 0 (alwaysinactive) or lji) ≥ 0 (always active); (d) linear relaxation in LiRPA when l(i) < 0, uji) > 0 (unstable).
Figure 2: Illustration of our optimized LiRPA bounds and the BaB process. Given a two-layer neural network,We aim to verify output f (x) ≥ 0. Optimized LiRPA chooses optimized slopes for ReLU lower bounds,allowing tightening the intermediate layer bounds l(i) and uji) and also the output layer lower bound f. BaBsplits two unstable neurons 虑2 and h，) to improve f and verify all sub-domains (f ≥ 0 for all cases).
Figure 3: Optimized LiRPA bound (0to 200 iterations) vs LP bounds.
Figure 4: Cactus plots for our method and other baselines in Base (Easy, Medium and Hard ), Wide and Deepmodels. We plot the percentage of solved properties with growing running time.
Figure 5: Running time of our method on the Base, Wide, and Deep networks when using 1, 2, 4and 8 CPU cores without a GPU (blue), and our method using 1 CPU core + 1 GPU (red) and astrong baseline method, GNN-Online (green). We report the baseline BaBSR verification time incaptions because they are out of range on the figures.
