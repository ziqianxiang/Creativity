Figure 1: Normalized local entropy ΦLE as a function of the squared distance d (left),training error difference δEtrain as a function of perturbation intensity σ (center) and testerror distribution (right) for a committee machine as defined in Eq. 7, trained with variousalgorithms on the reduced version of the Fashion-MNIST dataset. Results are obtained using50 random restarts for each algorithm.
Figure 2: Left: Test error of ResNet-18 on CIFAR-10. Right: Test error of ResNet-50 onTiny ImageNet. The curves are averaged over 5 runs. Training data consumed is the samefor SGD, rSGD and eSGD. Epochs are rescaled by y for rSGD and by L for eSGD (they arenot rescaled for rSGD×y).
Figure 3: Evolution of the flatness along the training dynamics, for ResNet-18 trained onCIFAR-10 with different algorithms. Figures show the train error difference with respect tothe unperturbed configurations. The value of the epoch, unperturbed train and test errors(%) are reported in the legends.The last panel shows that minima found at the end of anentropic training are flatter and generalize better. The value of the cross-entropy train lossof the final configurations is: 0.005 (SGD), 0.01 (eSGD), 0.005 (rSGD).
Figure 4: Train error difference δEtrain from Eq. (4), for minina obtained on variousarchitectures, datasets and with different algorithms, as a function of the perturbationintensity σ. Unperturbed train and test errors (%) are reported in the legends. The valuesof the train cross-entropy loss for the final configurations are: EfficientNet-B0: 0.08 (SGD),0.06 (rSGD); PyramidNet 0.07 (SGD), 0.07 (rSGD): ResNet-50: 0.04 (SGD), 0.006 (eSGD),0.1 (rSGD); DenseNet: 0.1 (SGD), 0.2 (eSGD), 0.12 (rSGD).
Figure 5: Train error difference δEtrain from Eq. (4) for ResNet-110 on Cifar-10. Valuesare computed along the training dynamics of different algorithms and as a function of theperturbation intensity σ. Unperturbed train and test errors (%) are reported in the legends.
Figure 6: Normalized local entropy ΦLE as a function of the squared distance d (left),training error difference δEtrain as a function of perturbation intensity σ (center) and testerror distribution (right) for a committee machine trained with SGD fast and differentdropout probabilities on the reduced version of the Fashion-MNIST dataset. Results areobtained using 50 random restarts for each algorithm.
Figure 7: Train error difference δEtrain from Eq. (4) in function of test error for a fixedvalue of the perturbation intensity σ = 0.5, for minima obtained on the same VGG-likearchitecture and dataset (CIFAR-10) and with different values of dropout, batch size andweight decay. Each point shows the mean and standard deviation over 64 realizations of theperturbation. The models are taken from the public data of the PGDL competition.
