Figure 1: (a): Conventional methods require expensive designing & training per deployment plat-form, which is infeasible to scale. (b): OFA co-trains a family of subnetworks of a teacher supernet.
Figure 2: Accuracy and latency heatmaps for varying uniform depth and expansion ratios and fixedkernel size=5, as measured in MobileNetV3 architecture. Latency is measured in ms on NVIDIARTX 2080 Ti GPU with BS=64.
Figure 3: CompOFA networks consistently achieve comparable and higher ImageNet accuracies forsimilar latency and FLOP constraints on CPU, GPU and mobile platforms within 1-5ms granulari-ties.
Figure 4: CDF comparisons of 50 randomly sampled models sampled in latency buckets of 5mseach. CompOFA has a higher fraction of its population at or better than a given classification error.
Figure 5: Random sampling of 20 models in each latency bucket of 5ms for CompOFA & OFAactual latency (left) and bucketed latency (right). CompOFA yields a higher average accuracy, i.e.
Figure 6: Left and Center: Comparing accuracies with and without progressive shrinking (PS).
Figure 7: Cumulative distribution function of accuracies of model configurations common to OFA& CompOFA with the base architecture changed from MobileNetV3 to ProxylessNAS. Despite thechange in architecture, the same heuristic allows CompOFA to train to the same or marginally higheraccuracies with half the training budget.
Figure 8: Distribution of accuracies of the randomly sampled models from OFA12Published as a conference paper at ICLR 2021Intel Xeon CPU Latency (ms)Figure 9: ImageNet accuracies and latencies on Samsung Note10 with reduced NAS iterations forCompOFA.
Figure 9: ImageNet accuracies and latencies on Samsung Note10 with reduced NAS iterations forCompOFA.
