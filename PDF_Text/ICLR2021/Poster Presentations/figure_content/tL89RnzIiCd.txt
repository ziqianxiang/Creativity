Figure 1: We generalize the energy of binary modern Hopfield networks to continuous states whilekeeping fast convergence and storage capacity properties. We also propose a new update rule thatminimizes the energy. The new update rule is the attention mechanism of the transformer. Formulaeare modified to express softmax as row vector. “=”-sign means “keeps the properties”.
Figure 2: Left: A standard deepnetwork with layers () propagateseither a vector or a set of vectorsfrom the input to the output. Right:A deep network, where layers ()are equipped with associative mem-ories via Hopfield layers ().
Figure 3: The layer Hopfield allows the association of two sets R () and Y (). It can beintegrated into deep networks that propagate sets of vectors. The Hopfield memory is filled with a setfrom either the input or previous layers. The output is a set of vectors Z ().
Figure 4: The layer HopfieldPooling enables pooling or summarization of sets, which areobtained from the input or from previous layers. The input Y () can be either a set or a sequence.
Figure 5: The layer HopfieldLayer enables multiple queries of the training set, a referenceset, prototype set, or a learned set (a learned matrix). The queries for each layer are computedfrom the results of previous layers. The input is a set of vectors R (). The output is also a set ofvectors Z (), where the number of output vectors equals the number of input vectors. The layerHopfieldLayer can realize SVM models, k-nearest neighbor, and LVQ.
