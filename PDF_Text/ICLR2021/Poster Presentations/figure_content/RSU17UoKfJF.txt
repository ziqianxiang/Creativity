Figure 1: In consideration of logistic loss, exponential loss and hinge loss, ∂d' μ is not monotonicw.r.t. μ. It is equal to 0 at μ = 0, after that it either approximates 0-, or equals to 0 after decreasingto μ = 1.
Figure 2: Estimating the privacy leakage of network through rank analysis. The critical layer forreconstruction has been red colored. First three columns show that even though bigger network hasmuch more parameters denoted by |W|, which means we can collect more gradients to reconstructthe data, but if the layer close to data is rank-deficient, we are not able to fully recover the data.
Figure 3: Performance of our approach and DLG over a CNN6 architecture. The diagram on theleft demonstrates the network architecture on which we perform attack. The activation functions areLeakyReLU, except the last one which is Sigmoid.
Figure 4: Left: Architectures of the ResNet18 with base width 16 and two variants. Variant 1cuts the skip connection of the third residual block. Variant 2 cuts the skip connection of the eighthresidual block. Upper right: Reconstruction examples of three networks. Lower right: Accuracy andreconstruction error of three networks. Training 200 epochs on CIFAR10 and saving the model withthe best performance on the validation set, three networks achieve a close accuracy. Two variantsperform even slightly better. In terms of gradient attacks, MSE of reconstructions from ResNet18and Variant 2 are similar, since Variant 2 cut the skip connection of a non-critical layer and theRA-i does not change. Whereas, by cuting the skip connection of a critical layer, according to therank analysis, increases RA-i substantially. MSE of the reconstructions from Variant 1 increases bynearly a factor of three with higher variance.
Figure 5: Twin data. The left figure demonstrates a twin data XX, which will trigger exactly the samegradients as the real data X does. Therefore, from the perspective of DLG, these two data are globalminimum for the objective function. The right figure shows that by adding noise to shift the twindata a little and using it as an initialization, DLG will converge to the twin data rather than real data.
Figure 6: Comparison of optimization-based gradient attacks over architectures with or withoutthe skip connection. The width of blue bars represents the number of features at each layer. Thefirst row shows that there is no impact on the reconstruction if the skip connection skips one layer.
Figure 7: Reconstruction over a FCN3 network with batch-size equal to 2. For FCN network, R-GAP is able to reconstruct sort of a linear combination of the input images. DLG will also worksperfectly on such architecture.
Figure 8: Reconstruction over a FCN3 network with batch-size equal to 5. Sometimes DLG willconverge to a image similar to the one reconstructed by the R-GAP.
Figure 9: In terms of least square as what we have used for R-GAP, overall increasing the width ofthe network will involve more constraints and hence enhance the denoising ability of the gradientattack. For O-GAP this also means a more stable optimization process and less noise in the recon-structed image, which has been empirically proven by Geiping et al. (2020). Increasing the widthof every layer will definitely decrease the RA-i, so the quality of reconstruction has been improved.
