Figure 1: Toy experiment comparing the efficacy of MS attacks between (i) Undefended Baselineand (ii) EDM targets. (a) Predictions and training data of the target model (b) OOD surrogatedata used by the adversary to query the target (c) Clone model trained using the predictions of thetarget on the surrogate data. The discontinuous predictions produced by EDM make MS attacks lesseffective compared to an undefended baseline. (Best viewed in color)hashing function. We develop a DNN-based perceptual hashing algorithm for this purpose, whichis invariant to simple transformations of the input and prevents adaptive attacks. Since differentmodels in the ensemble are used to service different queries and the models are trained to producedissimilar predictions, the adversary obtains predictions that are highly discontinuous in the inputspace. The clone model, when trained on these predictions, tries to approximate the complex dis-continuous decision boundary of the EDM. Our empirical evaluations show that the resulting clonemodel generalizes poorly on in-distribution data, degrading clone accuracy and reducing the efficacyofMS attacks. In contrast to existing defenses that rely on perturbing output predictions, our defensedoes not require modifying the output and instead uses a diverse ensemble to produce discontinuouspredictions that are inherently hard to steal.
Figure 2: Models in EDM are jointly trained on two objectives: (a) Accuracy Objective ensures thatthe models make correct preditions for in-distribution data. (b) Diversity Objective encourages themodels to make diverse decisions for auxiliary out-of-distribution data.
Figure 3: Coherence mea-sures the cosine of the small-est angle made by any pair ofvectors in a set.
Figure 4: EDM uses an input-based hash H(x) to select the model that is used to service an inputquery x. A perceptual hashing algorithm is used to prevent adaptive attacks.
Figure 5: Comparing the CDF of coherence values between a regular ensemble (trained withoutdiversity loss) and a Diverse Ensemble (trained with diversity loss). Diverse ensemble producesmuch lower coherence values on the adversaryâ€™s OOD data, indicating dissimilar predictions.
Figure 6:	Sorted histogram of index values outputs by the DNN-based pHash for perturbed inputsin CIFAR-10Table 6: Evaluation of EDM against KnockoffNets with a larger clone model. Using a larger clonemodel yields similar clone accuracies as the original clone model.
Figure 7:	Sorted histogram of index values outputs by the DNN-based pHash for perturbed inputsin CIFAR-10KnockoffNets attack, in addition to the query budget, the attacker is bound by the availability ofsurrogate data used for the attack. Thus, for our evaluations, we stop querying the target modelwhen we either run out of query budget or the surrogate data samples available to query the targetmodel. For the JBDA and JBDA-TR attacks, to utilize the larger query budget, we increase thenumber of augmentation rounds from 6 to 10. Our results are reported in Table 7. We observe onlya slight improvement in clone accuracy for KnockoffNets as the attack is limited by the availabilityof surrogate data (70K was the size of the largest surrogate dataset used). Even in the case ofJBDA and JBDA-TR attacks, the clone accuracy only improves marginally as the improvement inclone accuracy plateaus after 6 augmentation rounds.2 For all the attacks, EDM continues to have asignificantly lower clone accuracy compared to the baseline, even with a larger query budget.
