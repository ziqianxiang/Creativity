Figure 1: The AuxiLearn framework. (a) Learning to combine losses into a single coherent loss term. Here,the auxiliary network operates over a vector of losses. (b) Generating a novel auxiliary task. Here the auxiliarynetwork operates over the input space. In both cases, g(∙ ; φ) is optimized using IFT based on LA.
Figure 3: Loss images on test examples from NYUv2: (a) original image; (b) semantic segmentation groundtruth; (C) auxiliaries loss; (d) segmentation (main task) loss; (e) adaptive pixel-wise weight Pj ∂LT/∂'j.
Figure 4: t-SNE applied to auxiliary labels learned for Frog and Deer classes, in CIFAR10. Best viewed in color.
Figure 5: Optimizing task weights on the training set reduce to single-task learning.
Figure 6: Learning with noisy labels: task ID is proportional to the label noise.
Figure 7:auxiliary(a) Effect of auxiliary set size(b) Effect of DepthMean test accuracy (± SEM) averaged over 3 runs as a function of the number of samplesset (left) and the number of hidden layers (right). Results are on 5-shot CUB 200-2011 datasetin thePolynomial kernel - linear weightsFigure 8: Learned linear weights for a polynomial kernel on the loss terms of the tasks segmentation, depthestimation and normal prediction from the NYUv2 dataset.
Figure 8: Learned linear weights for a polynomial kernel on the loss terms of the tasks segmentation, depthestimation and normal prediction from the NYUv2 dataset.
