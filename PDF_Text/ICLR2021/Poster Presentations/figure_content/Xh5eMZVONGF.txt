Figure 1: Context and Structure both encapsulate valuable information about source code. In thisrealistic example, token 1 and 4 are distant in the sequence of tokens (Context), but only 5 hopsaway when traversing the Abstract Syntax Tree (Structure). As such, a method that relies onlyon the sequence of tokens could neglect the relationship between a method name and its returnvariable. Conversely, token 1 and 2 showcase the opposite setting. Hence, unifying Structure andContext leads to a more powerful representation of source code.
Figure 2: Structure distances used by our model.
Figure 3: Left: Sequence (Context) and AST (Structure) representation of an input code snippet.
Figure 4:	t-SNE visualization of the CODE Transformer's learned multilingual representations.
Figure 5:	Example snippet starting with parse (left) and its best embedding match from otherlanguages (right). Both methods parse an input string to convert it into a boolean value. Notethat even though they are semantically very similar, their method names are not; nonetheless, theirrepresentations in the Code Transformer encoder reflect their semantic similarity.
Figure 6: Example snippet and its corresponding AST obtained from GitHub Semantic.
Figure 7: Share of tokens in the labels also occurring in the bodies of methods.
