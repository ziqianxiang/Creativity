Figure 1: Comparison of reweighting and resampling with a1/a2 = 0.4/0.6 and f1/f2 = 0.9/0.1at the learning rate η = 0.5. The resampling strategy here is to randomly select the sub-population iwith the probability ai with replacement in each iteration. (1) For reweighting, the trajectory startingfrom θ0 = 1.1 can end up at θ = -1 after a few iterations, but θ = -1 is not the global minimizer.
Figure 2: Comparison of reweighting and resampling with learning rate η = 0.12. We set a1/a2 =0.4/0.6, f1/f2 = 0.9/0.1 and = 0.1. Both experiments start at θ0 = 0.9. The resamplingstrategy here is to randomly select the sub-population i with the probability ai with replacement ineach iteration. In (2) where reweighting is used, the trajectory skips to the local minimizer θ = —1later. In (3) where resampling is used, it stabilizes at the global minimizer θ = 1 all the time. Weinclude more comparisons with various learning rates in Appendix D to show that resampling ismore reliable for a wider range of η .
Figure 3: The ROC curve comparisons showthat the resampling has the largest area under thecurve.
Figure 4: The left plot shows the approximate value function obtained by the two methods. The rightplot is the evolution of the relative error log(et), where the absolute error et = ∣∣Vt(s) - Vπ(s)k∣.
Figure 5: A comparison of reweighting (upper row) and resampling (lower row) with a1/a2 =0.4/0.6 and f1/f2 = 0.9/0.1 at various learning rates η. All experiments start at θ0 = 1.6. We cansee that unless the learning rate η < 0.4, resampling is more stable near the minimizer θ = 1.
Figure 6: A comparison of reweighting (upper row) and resampling (lower row) with a1 /a2 =0.4/0.6, f1/f2 = 0.9/0.1 and = 0.1 at various learning rates η. All experiments start at θ0 = 0.9.
