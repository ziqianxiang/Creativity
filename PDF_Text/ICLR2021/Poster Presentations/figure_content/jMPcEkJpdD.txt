Figure 1: IMR network consists of three sub-networks encoding different information streamsprovided in compressed videos. We incorporate bidirectional dynamic connections to facilitateinformation sharing across streams. We train the model using two novel pretext tasks designed byexploiting the underlying structure of compressed videos.
Figure 2: Bidirectional dynamic connections are established between I & M pathways and betweenI & R pathways to facilitate information sharing between I-frames and P-frames. We incorporatemultimodal-gated attention to dynamically modulate the connections based on input. Feature tensors(orange and blue cubes) are placed in the T-HxW-C plane.
Figure 3: Pyramidal motion statistics prediction asks our network to find a region with the highestenergy of motion. Here we visualize two levels in a spatio-temporal pyramid for illustration.
Figure 4: Correspondence type prediction asks our network to categorize different types of datatransformations applied on P-frames. We illustrate four transformation types used in our experiments.
Figure 5: PMSP label visualization. The most vibrant regions, as highlighted by boxes of varyingsizes indicating different spatial scales ([2 × 2], [3 × 3], [4 × 4]), all successfully capture the mostsalient moving object (the hand with a eye brush) and its motion (applying eye makeup).
Figure 6: PMSP label visualization. At t = 1,the most vibrant region is the punching bag, correctlycapturing the semantic category of the video (BoxingPunchingBag). As we get temporally finer, theregions start to capture the boxer’s movement, e.g., the elongated green boxes in the third and thefourth I-frames at t = 3.
Figure 9: PMSP label visualization. Another example highlighting the benefit of our pyramidalformulation. While some regions at t = 1 miss the toddler in swing (see the first and the fifth I-framesin the first row), at t = 3 and t = 5 the boxes successfully track the toddler’s trajectory.
Figure 11: PMSP label visualization. Another challenging example containing a small, dynamicallymoving object (Surfing). At t = 1, all the boxes focus on the crushing waves on the bottom rightcorner, which is on average the most vibrant region in this video. Things are not much better at t = 3;the surfer is still too fast to capture, and thus the boxes fail to capture the surfer and instead highlightcrushing waves. At the finest temporal scale t = 5, the boxes begin to capture the surfer (see the first,second and the fifth frames on the bottom).
Figure 12: PMSP label visualization. This is a partial -failure example that shows boxes highlightinggame statistics during horse race TV broadcast (see the boxes at t = 1). The game statistics constantlychange frame-by-frame (e.g., time marks, rank, etc.), which caused those regions to exhibit on averagethe highest energy of motion for the entire duration of the video. Learning representations that strictlyfocus on those regions could lead to non-discriminative information (many sports videos show similargame statistics on screen). Fortunately, the boxes begin to highlight the horse riders at a finer temporalscale; see the green boxes ([3 × 3]) at t = 3 and t = 5. This, again, suggests that the pyramidalformulation makes our PMSP task robust to a variety of challenging real-world scenarios.
Figure 13: PMSP prediction results. Overall, the predicted regions tend to highlight salientmoving objects (although sometimes different from the ground-truth). (a): BoxingPunchingBag, (b):BaseballPitch, (c): Archery, (d): ThrowDiscus.
Figure 14: PMSP prediction results. Overall, the predicted regions contain the salient movingobjects (although sometimes different from the ground-truth). (a): Biking, (b): BoxingPunchingBag,(c): CricketShot, (d): FrontCrawl.
Figure 15: Video-to-video retrieval result. Ours finds the most similar video to the query in termsof both the appearance (a gymnast) and the motion (handspring). The 3D Rotation baseline capturesperhaps more similar appearance (a gymnast with the audience in the back) but less similar motion(horizontal bar jump vs. handspring). The ImageNet baseline fails to capture both appearance andmotion (ImageNet does not contain a category relevant to floor gymnastics).
Figure 16: Video-to-video retrieval result. Ours finds the most similar video to the query in termsof both the appearance (swim stadium) and motion (swimming). The ImageNet baseline does capturesimilar appearance (water), but fails to capture motion (swimming vs. surfing). The 3D Rotationbaseline shows little to no semantic similarity to the query video.
Figure 17: Video-to-video retrieval result. Ours finds the most similar video to the query in termsof both the appearance (scene layout) and the motion (pitching). The ImageNet baseline does capturesimilar high-level semantics appearance-wise (baseball pitcher) but motion is relatively less similar(different camera angle, no catcher and no hitter). The 3D Rotation baseline shows little to nosemantic similarity to the query video.
Figure 18: Video-to-video retrieval result. All three retrieval results fail to find videos that belong tothe same semantic category as the query video (pole vault). However, ours finds a video that containssimilar appearance (running track) and similar motion (running and jumping). The ImageNet baselinealso captures similar appearance (javelin throw) but less similar motion (running at a substantiallyslower pace). The 3D Rotation baseline shows little to no semantic similarity to the query video.
