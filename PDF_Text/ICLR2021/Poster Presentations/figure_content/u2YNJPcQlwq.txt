Figure 1: Estimation of empowerment via variational lower bounds.
Figure 2: Empowerment maximization via Latent-GCEAlgorithm 1 Latent Gaussian Channel Empowerment Maximization1:	Input: πθ0 (a | s) - control policy, H - prediction horizon.
Figure 3: Evaluating whether methods learn to stabilize simple dynamical systems. All methodssucceed for ball-in-box, whereas only our method succeeds for pendulum.
Figure 4: Empowerment landscapes (normalized) for ball-in-box (top) and pendulum (bottom). Inball-in-box, central positions correspond to high empowerment. For pendulum, upright positionscorrespond to high empowerment; x-axis is the angle to top, y-axis is the angular velocity. Fromleft to right, the plots show: ours, Mohamed & Rezende (2015), Karl et al. (2019), Eysenbach et al.
Figure 5: Relative standard deviation (RSD) in average empowerment value across 10 seeds. Ourmethod converges faster to a lower RSD during training.
Figure 6: Reconstruction in the latent dynamics vs the ground truth. Starting from the same uprightposition, a different action sequence is taken in each row (action +1: counter-clockwise torque, 0: notorque, -1: clockwise torque). Reconstruction matches the actual observation in all cases.
Figure 7: Empowerment estimates for pixel-based pendulum. Latent-GCE learns a reasonableempowerment function from pixels. Left: Analytical (x axis shifted by π from Figure 4). Middle:Latent-GCE along with the swinging-up trajectory of the final control policy. Right: VIM.
Figure 8: Average torso height over an evaluation episode of length 200 for the Hopper environment.
Figure 9: Analytical Empowerment Landscapewhere σi(s), pi, k, and P, are the i-th singular value of G(s), the corresponding actuator signalpower, the number of nonzero singular values, and the total power of the actuator signal, respectively.
Figure 10: Snapshots of the empowerment landscape (top), the control policy (middle) and the stateconcentration (bottom). The x-axis is θ (angle) and the y-axis is θ (angular velocity). The angle ismeasured from the upright position, (θ = 0 rad corresponds to the upright position). Each epochcorresponds to 2 ∙ 104 steps. The policy corresponds to the optimal policy for the stabilization taskwith external reward (Doya (2000)).
Figure 11: Cart-Pole - Comparison of unsupervised and supervised reward functions. The "densereward" is -θ2 and the “sparse reward” is l{∣θ∣ < 而}.
Figure 12: Left sub-figure: Double tunnel environment. The goal is marked in red. The agent in blue.
Figure 13: Trajectories of trained policy. As β increase, the agent develops a stronger preference overa safer route, sacrificing hitting time.
