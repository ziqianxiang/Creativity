Figure 1: Schematic illustration of negative sampling methods for the example of classifying speciesof tree. Top row: uniformly samples negative examples (red rings); mostly focuses on very differentdata points from the anchor (yellow triangle), and may even sample examples from the same class(triangles, vs. circles). Bottom row: Hard negative sampling prefers examples that are (incorrectly)close to the anchor.
Figure 2: Classification accuracy on downstream tasks. Embeddings trained using hard, debiased,and standard (β = 0, τ+ = 0) versions of SimCLR, and evaluated using linear readout accuracy.
Figure 3: Classification accuracy on downstream tasks. We compare graph representations onfour classification tasks. Accuracies are obtained by fine-tuning an SVM readout function, and arethe average of 10 runs, each using 10-fold cross validation. Results in bold indicate best performer.
Figure 4: Left: the effect of varying concentration parameter β on linear readout accuracy. Middle:linear readout accuracy as concentration parameter β varyies, in the case of contrastive learning (fullyunsupervised), using true positive samples (uses label information), and an annealing method thatimproves robustness to the choice of β (see Appendix D.1 for details). Right: STL10 linear readoutaccuracy for hard sampling with and without debiasing, and non-hard sampling (β = 0) with andwithout debiasing. Best results come from using both simultaneously.
Figure 5: Histograms of cosine similarity of pairs of points with the same label (top) and differentlabels (bottom) for embeddings trained on STL10 with four different objectives. H=Hard Sampling,D=Debiasing. Histograms overlaid pairwise to allow for convenient comparison.
Figure 6: Hard negative sampling using MoCo-v2 framework. Results show that hard negativesamples can still be useful when the negativememory bank is very large (in this case N =65536).
Figure 7: The effect of varying concentrationparameter β on linear readout accuracy for CI-FAR10. (Complements the left and middle plotfrom Figure 4.)6=6, acc=65.54%B = O, acc=67.73% Ml β = 0.5, acc=69.61%	B = 2, acc=67.68%Figure 12 gives real examples of hard vs. uniformly sampled negatives. Given an anchor x (amonkey) and trained embedding f (trained on STL10 using standard SimCLR for 400 epochs), wesample a batch of 128 images. The top row shows the ten negatives x- that have the largest innerproduct f(x)>f(x-), while the bottom row is a random sample from from the same batch. Negativeswith the largest inner product with the anchor correspond to the items in the batch are the mostimportant terms in the objective since they are given the highest weighting by qβ-. Figure 12 showsthat “real” hard negatives are conceptually similar to the idea as proposed in Figure 1: hard negativesare semantically similar to the anchor, possessing various similarities, including color (browns andgreens), texture (fur), and objects (animals vs machinery).
Figure 8: Histograms of cosine similarity of pairs of points with different label (bottom) and samelabel (top) for embeddings trained on CIFAR100 with different values of β . Histograms overlaidpairwise to allow for easy comparison.
Figure 9: Histograms of cosine similarity of pairs of points with the same label (top) and differentlabels (bottom) for embeddings trained on CIFAR100 with four different objectives. H=HardSampling, D=Debiasing. Histograms overlaid pairwise to allow for convenient comparison.
Figure 10: Histograms of cosine similarity of pairs of points with the same label (top) and differentlabels (bottom) for embeddings trained on CIFAR10 with four different objectives. H=Hard Sampling,D=Debiasing. Histograms overlaid pairwise to allow for convenient comparison.
Figure 11: Hard sampling takes much fewer epochs to reach the same accuracy as SimCLR does in400 epochs; for STL10 with β = 1 it takes only 60 epochs, and on CIFAR100 it takes 125 epochs(also with β = 1).
Figure 12: Qualitative comparison of hard negatives and uniformly sampled negatives for embeddingtrained on STL10 for 400 epochs using SimCLR. Top row: selecting the 10 images with highestinner product with anchor in latent space from a batch of 128 inputs. Bottom row: a set of randomsamples from the same batch. Hard negatives are semantically much more similar to the anchor thanuniformly sampled negatives - hard negatives possess many similar characteristics to the anchor,including texture, colors, animals vs machinery.
Figure 13:	Pseudocode for our proposed new hard sample objective, as well as the original NCEcontrastive objective, and debiased contrastive objective. In each case we take the number of positivesamples to be M = 1. The implementation of our hard sampling method only requires two additionallines of code compared to the standard objective.
Figure 14:	In cases where the learned embedding is not normalized to lie on a hypersphere we foundthat clipping the negatives to live in a fixed range (in this case [-2, 2]) stabilizes optimization.
