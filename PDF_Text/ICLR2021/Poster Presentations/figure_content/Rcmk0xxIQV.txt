Figure 1: (a) The dueling mixing network structure. (b) The overall QPLEX architecture. (c) Agentnetwork structure (bottom) and Transformation network structure (top).
Figure 2: (a) Payoff matrix for a harder one-step game. Boldface means the optimal joint actionselection from the payoff matrix. The strikethroughs indicate the original matrix game proposed byQTRAN. (b) The learning curves of QPLEX and other baselines. (c) The learning curve of QPLEX,whose suffix aLbH denotes the neural network size with a layers and b heads (multi-head attention)for learning importance weights λi (see Eq. (9) and (10)), respectively.
Figure 3:	(a) A special two-state MMDP used to demonstrate the training stability of the multi-agentQ-learning algorithms. r is a shorthand for r(s, a). (b) The learning curves of kQtotk∞ in a specifictwo-state MMDP.
Figure 4:	(a) The median test win %, averaged across all 17 scenarios. (b) The number of scenariosin which the algorithms’ median test win % is the highest by at least 1/32 (smoothed).
Figure 5:	Learning curves of StarCraft II with online data collection.
Figure 6: The learning curves of QPLEX and other baselines on the origin matrix game.
Figure 7:	The learning curves of StarCraft II with online data collection on remaining scenarios.
Figure 8:	(a) The median test win %, averaged across all 14 scenarios proposed by SMAC (Samvelyanet al., 2019). (b) The number of scenarios in which the algorithms’ median test win % is the highestby at least 1/32 (smoothed).
Figure 9:	Ablation studies on QPLEX with the median test win %, averaged benchmark scenarios.
Figure 10:	The learning curves of median test win rate % for QPLEX, QPLEX’s ablation QPLEX-wo-duel-atten, QMIX, and Qatten with online data collection.
Figure 11:	Learning curves of median test win rate % for QPLEX, QMIX, and Large QMIX withonline data collection.
Figure 12: Visualized strategies of QPLEX and QMIX on 5s10z map of StarCraft II benchmark. Redmarks represent learning agents, and blue marks represent build-in AI agents.
Figure 13: Deferred learning curves of StarCraft II with offline data collection on tested scenarios.
Figure 14: Ablation study about QPLEXwith different network capacities in Star-Craft II.
Figure 15: Deferred figures of median test win rate % for QPLEX-1L4H, QPLEX-1L10H, andQPLEX-2L4H with online data collection.
Figure 16: Ablation study about QTRANwith online data collection.
Figure 17:	Figures of median test win rate % for QPLEX, QPLEX’s ablation QPLEX-wo-trans-atten,QTRAN, and QTRAN-w-trans with online data collection.
Figure 18:	Learning curves of median test return for QPLEX, QTRAN, QMIX, and WQMIX(OW-QMIX and CW-QMIX) in the toy predator-prey task.
