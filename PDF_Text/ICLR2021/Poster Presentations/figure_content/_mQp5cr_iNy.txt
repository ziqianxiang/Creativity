Figure 1: (a,b) Frames from the 3-D navigation task VizdoomMyWayHome. (c) MiniGrid-KeyCorridorS6R3. (d) MiniGrid-ObstructedMazeFull.
Figure 2: Performance evaluation of AGAC.
Figure 3: Sensitivity analysis of AGAC in KeyCorridorS4R3.
Figure 5: State visitation heatmaps for RND, Count, a random uniform policy, RIDE, and AGACtrained in a singleton environment (top row) and procedurally-generated environments (bottom row)without extrinsic reward for 10M timesteps in the MUltiRoomN10S6 task.
Figure 4: Average return on N10S6 with andwithout extrinsic reward.
Figure 6: State visitation heatmaps of the last ten episodes of an agent trained in procedurally-generated environments without extrinsic reward for 10M timesteps in the MultiRoomN10S6 task.
Figure 7: State visitation heatmaps of the last ten episodes of an agent trained in a singletonenvironment With no extrinsic reWard 10M timesteps in the MultiRoomN10S6 task. The agent iscontinuously engaging into neW strategies.
Figure 8: Performance evaluation of AGAC compared to RIDE, AMIGo, Count, RND and ICM onextremely hard-exploration problems.
Figure 9: Average intrinsic reward for different methods trained in MultiRoomN12S10.
Figure 10: A simple schematic illustration of AGAC. Left: the adversary minimizes the KL-divergencewith respect to the action probability distribution of the actor. Right: the actor receives a bonus whencounteracting the predictions of the adversary.
Figure 11: Artificial neural architecture of the critic, the actor and the adversary.
