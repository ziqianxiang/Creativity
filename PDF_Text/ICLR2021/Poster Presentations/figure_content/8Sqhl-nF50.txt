Figure 1: Comparison of training dynamics on different types of functionals. (a) and (b): usingthe linear RNN model with the GD optimizer; (c): using the nonlinear RNN model (with tanhactivation) with the Adam optimizer. The shaded region depicts the mean ± the standard deviationin 10 independent runs using randomized initialization. Observe that learning complex functionals(Airy, Lorenz) suffers from slow-downs in the form of long plateaus.
Figure 2: The training dynamics of the target functional defined by (119) using the model (120).
Figure 3: The timescale of plateauing and parameter separation. Here the model and target are bothselected the same as Figure 2, but with a larger width m = 10. We see the logarithm of time ofPlateaUing and parameter separation is almost linear to the memory 1∕ω.
Figure 4: Numerical verifications of the plateauing behavior under general settings, with non-diagonal recurrent kernels, the non-linear activation (tanh), and the Adam (momentum-based) op-timizer. Here We use the target functional the same as Figure 2 with the memory 1∕ω = 20. Thetime horizon is chosen as T = 32, and 128 input samples are generated from a standard white noise.
