Figure 1: Affinity and Diversity parameterize the performance of a model trained with augmentation.
Figure 2: Affinity is a model-sensitive measure ofdistribution shift. Contours indicate lines of equal(left) Affinity, or (right) KL Divergence between thejoint distribution of the original data and targets andthe shifted data. The two axes indicate the actual shiftsthat define the augmentation. Affinity captures model-dependent features, such as the decision boundary.
Figure 3: Augmentation performance is determined by both Affinity and Diversity. (a) Test accuracyplotted against each of Affinity and Diversity for the two datasets, showing that neither metric alone predictsperformance. In the CIFAR-10 plots (top), blue highlights (also in inset) are the augmentations that increasetest accuracy above the clean baseline. Dashed lines indicate the clean baseline. (b) and (c) show test accuracyrelative to clean baseline on the color scale in the plane of Affinity and Diversity. The three star markers in(b) are (left to right) RandAugment, AutoAugment, and mixup. For fixed values of Affinity, test accuracygenerally increases with higher values of Diversity. For fixed values of Diversity, test accuracy generallyincreases with higher values of Affinity. To quantify this tendency we measured what fraction of points satisfyInequality 3. For CIFAR-10 99.1% of point pairs satisfy the inequality, while for ImageNet 97.5% satisfy it.
Figure 4: Test accuracy varies differently than either Affinity or Diversity. Here, the probability ofRotate(fixed, 60deg) on CIFAR-10 is varied from 10% (cyan) to 90% (pink). Left: as probabilityincreases, Affinity decreases linearly while the accuracy changes non-monotonically. Center: accuracy andDiversity vary differently from each other as probability is changed. Right: test accuracy is maximized atintermediate values.
Figure 5:	(a) Switching off regularizers yields aperformance boost: Three examples of how turn-ing off a regularizer increases the validation accu-racy. This slingshot effect can speed up training andimprove the best validation accuracy. Top: train-ing with no augmentation (clean baseline), comparedto constant augmentation, and augmentation that isturned off at 55k steps. Here, the augmentation isRotate(fixed, 20deg,100%). Middle: Base-line with constant `2 . This is compared to turning off`2 regularization part way through training. Bottom:Constant learning rate of 0.1 compared to trainingwhere the learning rate is decayed in one step by afactor of 10. (b) Bad augmentations can becomehelpful if switched off: Colored lines connect thetest accuracy with augmentation applied throughouttraining (top) to the test accuracy with switching mid-training. Color indicates the amount of Switch-offLift; blue is positive and orange is negative. Thedashed purple line indicates accuracy of clean model.
Figure 6:	Static augmentations decreasediversity and performance. CIFAR-10,static augmentation performance is less thanthe clean baseline, (0, 0), and less than thedynamic augmentation case (left). Diversityin the static case is less than in the dynamiccase (right). Augmentations with no stochas-ticity are excluded because they are triviallyequal on the two axes. Diagonal line indi-cates where static and dynamic cases wouldbe equal.
Figure 7: CIFAR-10: Labeled map of tested augmentations on the plane of Affinity and Diversity.
Figure 8: ImageNet: Labeled map of tested augmentations on the plane of Affinity and Diversity.
Figure 9: CIFAR-10: Three different diversity metrics are strongly correlated for high entropyaugmentations. Here, the entropy is calculated only for discrete augmentations.
Figure 10: CIFAR-10: (Left) Diversity as the average loss over the first 10 epochs (out of 222) epochsof training (10 measurements, one per epoch). This represents an 96.5% reduction in computationalcost. 98.9% of pairs satisfy Inequality 3. (Right) Early Diversity compared to Diversity computed forfull training, this represents a Spearman correlation of 0.8723Published as a conference paper at ICLR 20211.3二1 2jn j∙∕Φ≥Q ι,ιk-<0山1.00.5Affinity1.0+0.60
Figure 11:	ImageNet: (Left) Diversity as the average loss over the first 13000 (out of 112000) stepsof training (5 measurements, one per epoch), this represents an 88% reduction in computational cost.
Figure 12:	Affinity correlates with two other measures of how augmented images are related to atrained model’s distribution: logsumexp of the logits (left, for CIFAR-10, and right, for ImageNet)is the mean log likelihood for the image. WAIC (middle, for CIFAR-10) corrects for a possible biasin that estimate. In all three plots, numbers are referenced to the clean baseline, which is assigned avalue of 0.
Figure 13:	(Left) Augmented model performance on CIFAR-10.1 relative to a clean baseline (acc80.1). We see that robust accuracy is higher for models trained with more diverse higher affinityaugmentation policies. (Middle) Gap in performance between CIFAR-10.1 and CIFAR-10 accuracy.
Figure 14: (Left) Augmented model performance on ImageNet-V2 relative to a clean baseline (acc63.7). We see that robust accuracy is higher for models trained with more diverse higher affinityaugmentation policies. (Middle) Gap in performance between ImageNet-V2 and ImageNet accuracy.
Figure 15: Mean corruption error on CIFAR-10-C (lower is better): Augmentations with highAffinity and high Diversity generally lead to lower mean corruption scores, and thus better robustnessperformance. Patch Gaussian augmentations (red circles), however, outperform this trend, potentiallydue to the similarity between some of the corruption transformations and Gaussian noise.
Figure 16: Robustness performance on CIFAR-10-C: Figures show corruption error for 18 differentcorruption types averaged over five corruption strengths and compared to the baseline model, AlexNet,used by Hendrycks & Dietterich (2019). Robust performance for some corruptions, such as brightness,is maximal for models trained with high Affinity high Diversity augmentations. For other corruptions,such Gaussian noise or speckle, we do not see this. Red circles indicate models trained with patchGaussian augmentation which are particularly robust to paired families of augmentations.
