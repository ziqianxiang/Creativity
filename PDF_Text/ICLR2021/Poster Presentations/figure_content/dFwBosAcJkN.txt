Figure 1: Relationships between various adver-sarial threat models. Lp and spatial adversarialattacks are nearly contained within the percep-tual threat model, while patch attacks may beperceptible and thus are not contained. In this pa-per, we propose a neural perceptual threat model(NPTM) that is based on an approximation of thetrue perceptual distance using neural networks.
Figure 2: Area-proportional Venn diagram val-idating our threat model from Figure 1. Eachellipse indicates a set of vulnerable ImageNet-100 examples to one of three attacks: L2 , StAdvspatial (Xiao et al., 2018), and our neural per-ceptual attack (LPA, Section 4). Percentagesindicate the proportion of test examples success-fully attacked. Remarkably, the NPTM encom-passes both other types of attacks and includesadditional examples not vulnerable to either.
Figure 3: Adversarial examples generated using self-bounded and externally-bounded LPA perceptualadversarial attack (Section 4) with a large bound. Original images are shown in the left column andmagnified differences from the original are shown to the right of the examples. See also Figure 7.
Figure 4: Results of the perceptual study described in Section 6 across five narrow threat models andour two perceptual attacks, each with three bounds. (a) The perceptibility of adversarial examplescorrelates well with the LPIPS distance (based on AlexNet) from the natural example. (b) TheLagrangian Perceptual Attack (LPA) and Perceptual PGD (PPGD) are strongest at a given percepti-bility. Strength is the attack success rate against an adversarially trained classifier. (c) Correlationbetween the perceptibility of attacks and various distance measures: L2, SSIM (Wang et al., 2004),and LPIPS (Zhang et al., 2018) calculated using various architectures, trained and at initialization.
Figure 5: We generate samples via different adversarial attacks using narrow threat models inImageNet-100 and measure their distances from natural inputs using Lp and LPIPS metrics. Thedistribution of distances for each metric and threat model is shown as a violin plot. (a-b) Lp metricsassign vastly different distances across perturbation types, making it impossible to train against all ofthem using Lp adversarial training. (c-d) LPIPS assigns similar distances to similarly perceptibleattacks, so a single training method, PAT, can give good robustness across different threat models.
Figure 6: Creating an adversarial example in the LPIPS threat model.
Figure 7: Adversarial examples generated using self-bounded and externally-bounded PPGD andLPA perceptual adversarial attacks (Section 4) with a large bound. Original images are shown in theleft column and magnified differences from the original are shown to the right of the examples.
Figure 8: Several classifiers trained with PAT, adversarial training, and normal training on CIFAR-10and ImageNet-100 are plotted with their clean accuracy and accuracy against the union of narrowthreat models (see Section 7 for robustness evaluation methodology). PAT models on both datasetsoutperform adversarial trained models in both clean and robust accuracy.
