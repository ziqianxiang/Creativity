Figure 1: Gamer normalized medianscore on the Atari benchmark of 55games with sticky actions at 200M steps.
Figure 2: World Model Learning. The training sequence of images Xt is encoded using the CNN.
Figure 3: Actor Critic Learning. The world model learned in Figure 2 is used for learning a policyfrom trajectories imagined in the compact latent space. The trajectories start from posterior statescomputed during model training and predict forward by sampling actions from the actor network.
Figure 4: Atari performance over 200M steps. See Table 1 for numeric scores. The standards inthe literature to aggregate over tasks are shown in the left two plots. These normalize scores bya professional gamer and compute the median or mean over tasks (Mnih et al., 2015; 2016). InSection 3, we point out limitations of this methodology. As a robust measure of performance, werecommend the metric in the right-most plot. We normalize scores by the human world record(Toromanoff et al., 2019) and then clip them, such that exceeding the record does not further increasethe score, before averaging over tasks.
Figure 5: Clipped record normalized scores of various ablations of the DreamerV2 agent. Thisexperiment uses a slightly earlier version of DreamerV2. The score curves for individual tasks areshown in Figure H1. The ablations highlight the benefit of using categorical over Gaussian latentvariables and of using KL balancing. Moreover, they show that the world model relies on imagegradients for learning its representations. Stopping reward gradients even improves performanceon some tasks, suggesting that representations that are not specifically trained to predict previouslyexperienced rewards may generalize better to new situations.
Figure A1: Behavior learned by DreamerV2 on the Humanoid Walk task from pixel inputs only.
Figure A2: Performance on the humanoidwalking task from only pixel inputs.
Figure B1: Behavior learned by DreamerV2 on the Atari game Montezuma’s Revenge, that poses ahard exploration challenge. Without any explicit exploration mechanism, DreamerV2 reaches aboutthe same performance as the exploration method ICM.
Figure B2: Performance on the Atari gameMontezuma’s Revenge.
Figure F1: Comparison of DreamrV2Xo the top model-free RL methods IQN and Rainbow. Thecurves show mean and standard deviation over 5 seeds. IQN and Rainbow additionally average eachpoint over 10 evaluation episodes, explaining the smoother curves. DreamerV2 outperforms IQN andRainbow in all four aggregated scores. While IQN and Rainbow tend to succeed on the same tasks,DreamerV2 shows a different performance profile.
Figure G1: ComParison那DeamerV2, Gaussian5⅛te0d舟 categoricaP latent Variables, and no KLbalancing. The ablation experiments use a slightly earlier version of the agent. The curves showmean and standard deviation across two seeds. Categorical latent variables and KL balancing bothsubstantially improve performance across many of the tasks. The importance of the two techniques isreflected in all four aggregated scores.
Figure H1: Comparison of leveraging image prediction, reward prediction, or both for learning themodel representations. While image gradients are crucial, reward gradients are not necessary forour world model to succeed and their gradients can be stopped. Representations learned purelyfrom images are not biased toward previously encountered rewards and outperform reward-specificrepresentations on a number of tasks, suggesting that they may generalize better to unseen situations.
Figure I1: Comparison Ofeleveraging Ren⅛rce廨fiθdieni汕附aight-throUgh函fiθdients, or both fortraining the actor. While Reinforce gradients are crucial, straight-through gradients are not importantfor most of the tasks. Nonetheless, combining both gradients yields substantial improvements on asmall number of games, most notably on Seaquest. We conjecture that straight-through gradientshave low variance and thus help the agent start learning, whereas Reinforce gradients are unbiasedand help converging to a better solution.
