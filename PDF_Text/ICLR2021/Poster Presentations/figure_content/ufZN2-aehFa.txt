Figure 1: Multi-task CLV model withtask-specific global latent variables z` anda task-independent variable θ describingstatistical structure shared between tasks.
Figure 2: Comparison of aggregation mechanisms in CLV models. Dashed lines correspond to learnedcomponents of the posterior approximation qφ (z| Dc). BA avoids the detour via a mean-aggregatedlatent observation r and aggregates Dc directly in the statistical description of z. This allows toincorporate a quantification of the information content of each context tuple (xcn, ync ) as well as of zinto the inference in a principled manner, while MA assigns the same weight to each context tuple.
Figure 3: Predictions on two instances (dashed lines) of the 1D quadratic function class, given N = 3context data points (circles). We show mean and standard deviation predictions (solid line, shadedarea), and 10 function samples (AR samples for deterministic methods). Cf. also App. 7.6.
Figure 4: Model architectures used for our experiments in Sec. 5. For the ANP architecture we referthe reader to Kim et al. (2019). Orange rectangles denote MLPs. Blue rectangles denote aggregationoperations. Variables in green rectangles are sampled from normal distributions with parametersgiven by the incoming nodes. To arrive at a fair comparison, we optimize all MLP architectures,the latent space dimensionality dz , as well as the Adam learning rate, individually for all modelarchitectures and all experiments, cf. App. 7.5.3.
Figure 5: Posterior predictive log-likelihood in dependence of the context set size N for the 1D and3D Quadratic experiments, the Furuta dynamics experiment as well as the 2D image completionexperiment.
Figure 6:	Predictions on two instances (dashed lines) of the 1D quadratic function class, given N = 3context data points (circles). We plot mean and standard deviation (solid line, shaded area) predictionstogether with 10 function samples (for deterministic methods we employ AR sampling).
Figure 7:	Predictions on two instances (dashed lines) of the 1D quadratic function class, givenN = 19 context data points (circles). We plot mean and standard deviation (solid line, shaded area)predictions together with 10 function samples (for deterministic methods we employ AR sampling).
Figure 8:	Predictions on two instances (dashed lines) of the RBF GP function class, given N = 20context data points (circles). We plot mean and standard deviation (solid line, shaded area) predictionstogether with 10 function samples (for deterministic methods we employ AR sampling).
Figure 9:	Predictions on two instances (dashed lines) of the RBF GP function class, given N = 60context data points (circles). We plot mean and standard deviation (solid line, shaded area) predictionstogether with 10 function samples (for deterministic methods we employ AR sampling).
Figure 10:	Predictions on two instances (dashed lines) of the Weakly Periodic GP function class, givenN = 20 context data points (circles). We plot mean and standard deviation (solid line, shaded area)predictions together with 10 function samples (for deterministic methods we employ AR sampling).
Figure 11: Predictions on two instances (dashed lines) of the Weakly Periodic GP function class, givenN = 60 context data points (circles). We plot mean and standard deviation (solid line, shaded area)predictions together with 10 function samples (for deterministic methods we employ AR sampling).
Figure 12:	Predictions on two instances (dashed lines) of the Matern-5/2 GP function class, givenN = 20 context data points (circles). We plot mean and standard deviation (solid line, shaded area)predictions together with 10 function samples (for deterministic methods we employ AR sampling).
Figure 13:	Predictions on two instances (dashed lines) of the Matern-5/2 GP function class, givenN = 60 context data points (circles). We plot mean and standard deviation (solid line, shaded area)predictions together with 10 function samples (for deterministic methods we employ AR sampling).
