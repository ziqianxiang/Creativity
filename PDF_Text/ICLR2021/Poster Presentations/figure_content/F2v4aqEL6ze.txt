Figure 1: In typical regularization-based CL (top), when the low-error ellipsoid around local minima is sharpand narrow, the space for candidate model parameters that perform well on all tasks (i.e., the intersection of theellipsoid for each task) quickly becomes very small as learning continues, thus, an inevitable trade-off betweenstability and plasiticty occurs. In contrast, when the wide local minima exists for each task (bottom), it is morelikely the ellipsoids will significantly overlap even when the learning continues, hence, finding a well performingmodel for all tasks becomes more feasible.
Figure 2: CPR can be understoodas a projection onto a finite radiusball around PU .
Figure 3: Verifying the regularization for wide local minima3.2	Quantifying the role of wide local minima regularizationWe first demonstrate the effect of applying CPR with varying trade-off parameter β in (3) by takingEWC (Kirkpatrick et al., 2017) trained on CIFAR-100 as a running example. Fig. 3(a) shows how theaforementioned metrics varies as β changes over [0.1, . . . , 1]. First, we observe that A10 certainlyincreases as β increases. Moreover, we can break down the gain in terms of I1,10 and F10 ; weobserve I1,10 monotonically decreases as β increases, but F10 does not show the similar monotonicityalthough it also certainly decreases with β . This suggests that enlarged wide local minima is indeedhelpful for improving both plasticity and stability. In the subsequent experiments, we selected βusing validation sets by considering all three metrics; among the β’s that achieve sufficiently highA10, we chose one that can reduce F10 more than reducing I1,10, since it turns out improving thestability seems more challenging. (In fact, in some experiments, when we simply consider A10, thechosen β will result in the lowest I1,10 but with even higher F10 than the case without CPR.) Forcomparison purposes, we also provide experiments using Deep Mutual Learning (Zhang et al., 2018)and Label Smoothing (Szegedy et al., 2016) regularizer for achieving the wide local minima in theSM; their performance was slightly worse than CPR.
Figure 4: Experimental results on CL benchmark datasetOmniglot This dataset is well-suited to evaluate CL with long task sequences (50 tasks). InTable 1, it is clear that the CPR considerably increases both plasticity and stability in long tasksequences. In particular, CPR significantly decreases F10 for EWC and leads to a huge improvementin A10 . Interestingly, unlike the previous datasets, I1,10 is negative, implying that past tasks help inlearning new tasks for the Omniglot dataset; when applying CPR, the gains in I1,10 are even better.
Figure 5: Ablation studies on CL With wide local minimaWe study the ablation of the CPR on the regularization-based methods using CIFAR-100 with thebest (λ, β) found previously, and report the averaged results over 5 random initializations and tasksequences in Fig. 5. The ablation is performed in two cases: (i) using CPR only at task t, denoted asEWC + CPR (only t-th task), and (ii) using CPR except task t, denoted as EWC + CPR (w/o t-th task).
Figure 6: Accumulated normalized rewardFigure 6 shows the experimental results and fine-tuning means the result of continual learning withoutregularization. x-axis is the training step and y-axis means the normalized accumulated reward,which is the sum of each task reward normalized by the reward of fine-tuning. We observe that CPRincreases the accumulated reward of each method. From the analysis, we found out that, trainingwith the best hyperparameter for each method already do not suffer from catastrophic forgetting onprevious tasks but each method shows the difference on the ability to learn a new task well. However,we check that CPR can averagely increase the average reward of each task by 27%(EWC), 6%(MAS)and 5%(AGS-CL) and we believe that this is the reason why CPR leads to the improvement of theaccumulated reward of each method.
