Figure 1: Visualization of a learned perturbation set trained on CIFAR10 common corruptions. (toprow) Interpolations from fog, through defocus blur, to pixelate corruptions. (middle row) Randomcorruption samples for three examples. (bottom row) Adversarial corruptions that misclassify anadversarially trained classifier at = 10.2.
Figure 2: Visualizations from a learned perturbation set trained on the multi-illumination dataset. (toprow) Interpolations between different lighting angles. (bottom row) Random lighting perturbationsfor two scenes.
Figure 3: Pairs of MI scenes (left) and their adversarial lighting perturbations (right).
Figure 4: A simple example demonstrating how the expected value of a function can tend to zerowhile the maximum tends to infinity as a → ∞.
Figure 5: Samples of '∞ (top) and RTS (bottom) perturbations for MNIST from the convolutional'∞ and RTS-1 models.
Figure 6: Distribution of `2 norms for latent encodings of CIFAR10 common corruptions on the testset for each pairing strategy.
Figure 7: A larger version of the density of the `2 norms of latent encodings of CIFAR10 commoncorruptions, broken down by type of corruption.
Figure 8: Random perturbations from the CVAE prior, showing four random samples for eachexample.
Figure 9: Interpolations between fog (left), defocus blur (middle), and pixelate (right) corruptions asrepresentative distinct types of corruptions from the weather, blur, and digital corruption categories.
Figure 10: Adversarial examples that cause misclassification for an adversarially trained classifier.
Figure 11: Certified accuracy against CIFAR10 common corruptions using randomized smoothingat various noise levels. The horizontal axis plots the certified radius, and the vertical axis plots thefraction of examples certified at that radius.
Figure 12: Additional interpolations between three (left, middle, right) randomly chosen lightingperturbations in each row.
Figure 13: Additional random samples from the CVAE prior showing variety in lighting perturbationsin various different scenes.
Figure 14: Adversarial examples that can cause on average 9.9% more pixels in the shown segmenta-tion maps to be incorrect for a model trained with data augmentation. The first two rows are a benignlighting perturbation and its corresponding predicted material segmentation, and the next two rowsare an adversarial lighting perturbation and its corresponding predicted material segmentation. Forreference, the final row contains the true material segmentation.
Figure 15: Adversarial examples that can cause on average 3% more pixels in the shown segmentationmaps to be incorrect for an adversarially trained model.
Figure 16: Adversarial examples at the full radius of = 17 that for an adversarially trained model,which are starting to cast dark shadows to obscure the objects in the image.
Figure 17: Certified accuracy for material segmentation model using randomized smoothing. Thehorizontal axis denotes the certified radius, and the vertical axis denotes the fraction of pixels thatare certifiably correct at that radius. Note that a radius of = 17 is the maximum radius of theperturbation set.
