Figure 1: Left: Most conventional NAS approaches need to repeatedly train NAS model on each giventarget dataset, which results in enormous total search time on multiple datasets. Middle: We propose a novelNAS framework that generalizes to any new target dataset to generate specialized neural architecture withoutadditional NAS model training after only meta-training on the source database. Thus, our approach cut down thesearch cost for training NAS model on multiple datasets from O(N) to O(1). Right: For unseen target dataset,we utilize amortized meta-knowledge represented as set-dependent architecture generative representations.
Figure 2: Overview of MetaD2A The proposed generator with θ and φ meta-learns the set-dependent graphrepresentations on the meta-training tasks, where each task contains a subset of ImageNet-1K and high-qualityarchitecture for the subset. The proposed predictor with ω meta-learns to predict performance, consideringthe dataset as well as the graph. In the meta-test (searching) phase, the meta-learned MetaD2A generalizes tooutput set-specialized neural architecture for new target datasets without additional NAS model training.
Figure 3: Performance on Unseen Datasets (Meta-Test) We show accuracy over flop of both MetaD2Aand a transferable NAS referred as to NSGANetV2 (Lu et al., 2020) after meta-training MetaD2A on sourcedatabase consisting of subsets of ImageNet-1K and architectures in MobileNetV3 search space. Note that eachplot point is searched within 125 GPU seconds by MetaD2A.
Figure 4: T-SNE vis. of Latent Space Figure 5: The Quality of Generated ArchitecturesWe first visualize cross-modal latent embeddings {z} of unseen datasets encoded by the meta-8Published as a conference paper at ICLR 2021learned generator with T-SNE in Figure 4. Each marker indicates {z} of the sampled subsets ofeach dataset with different seeds. We observe that the generator classifies well embeddings {z}by datasets in the latent space while clusters z of the subset of the same dataset. Furthermore, weinvestigate the quality of generated architectures from those embeddings {z}. In the Figure 5, thegenerator sample 2000 architecture candidates from the embeddings encoded each target datasetand computes the validate accuracy of those architectures. The proposed generator generates morehigh-performing architectures than the simple random architecture sampler for each target dataset.
