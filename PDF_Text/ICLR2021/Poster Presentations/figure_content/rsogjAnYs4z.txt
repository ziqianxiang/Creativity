Figure 1: Effects of data parallelism and sparsity on neural network training for various workloadswith a fixed (a-c) or decaying learning rate (d). Across all workloads and sparsity levels, the samescaling pattern is observed for the relationship between batch size and steps-to-result: it starts withthe initial phase of linear scaling, followed by the region of diminishing returns, and eventuallyreaches to maximal data parallelism. Also, the effect of data parallelism in training sparse networksis no worse than that of the dense counterpart, despite the general difficulty of training the former.
Figure 2: Comparing different optimization algorithms for the effects of data parallelism and spar-sity on the workload {MNIST, Simple-CNN, SGD/Momentum/Nesterov} and study {batch size(2-16384), sparsity levels (0, 50, 70, 90%)} settings; there is no normalization or averaging. Acrossall sparsity levels, momentum optimizers (i.e., Momentum, Nesterov) record lower steps-to-resultin a large batch regime and have much bigger critical batch sizes than SGD without momentum.
Figure 3: Metaparameter search results (100 samples in total) for Simple-CNN on MNIST trainedusing Momentum optimizer. Sparsity level (S) and batch size (B) are denoted at the top of eachplot. The best trial that records the lowest steps to reach the goal (i.e., steps-to-result) is marked bygold star (?). Complete/incomplete refer to the trials of goal reached/not reached given a maximumtraining step budget, while infeasible refers to the trial of divergence during training.
Figure 4: LiPschitz constant of Vf measured locally over the course of training for networks withdifferent sparsity levels. The more a network is pruned, the higher the Lipschitz constant becomes;e.g., for 0, 50, 70, 90% sParsity levels, the average LiPschitz constants are 0.57, 0.72, 0.81, 1.76for SimPle-CNN and 3.87, 8.74, 9.54, 11.18 for ResNet-8, resPectively. This indicates that Pruningresults in a network whose gradients are less smooth during training. We further Provide additionaltraining logs and exPlain how smoothness is measured in APPendix C.
Figure 5: Metaparameter search results for the workload of {MNIST, Simple-CNN, SGD}, wherethe metaparameter being tuned is the learning rate η. The blue circles (∙) denote successful runs (i.e.,it reached the goal error), and the best trial that records the steps-to-result is marked by gold star (?);also, the grey triangles (N) and red crosses (×) refer to incomplete and infeasible runs, respectively.
Figure 6: Training logs of Simple-CNN and ResNet-8 used for the smoothness analysis. The sparsenetworks that recorded high Lipschitz constants show worse training performance, indicating thatlow smoothness may be the potential cause of hampering the training of sparse neural networks.
Figure 7: Results for the effects of data parallelism for the workloads of {MNIST, Simple-CNN,SGD/Momentum/Nesterov} with a constant learning rate.
Figure 8: Results for the effects of data parallelism for the workloads of {Fashion-MNIST, Simple-CNN, SGD/Momentum/Nesterov} with a constant learning rate and the goal error of 0.12.
Figure 9: Results for the effects of data parallelism for the workloads of {Fashion-MNIST, Simple-CNN, SGD/Momentum/Nesterov} with a constant learning rate and the goal error of 0.14.
Figure 10: Results for the effects of data parallelism for the workloads of {CIFAR-10, ResNet-8,SGD/Momentum/Nesterov} with a linear learning rate decay.
Figure 11: Results for the effects of data parallelism for the workloads of {CIFAR-10, ResNet-8,SGD/Momentum} with a constant learning rate.
Figure 12: Differences in ratio between (90%) sparse network’s steps-to-result to dense network’s,across different batch sizes for all workloads presented in this work. The difference ranges between(1.5, 4.5) overall. Note that the ratio difference > 1 indicates that it requires more number oftraining iterations (i.e., steps-to-result) for sparse network compared to dense network. Also, thedifference seems to decrease as batch size increases, especially for Momentum based optimizers.
Figure 13: Meataparameter search results for the workloads of {MNIST, Simple-CNN, SGD} witha constant learning rate.
Figure 14: Meataparameter search results for the workloads of {MNIST, Simple-CNN, Momentum}with a constant learning rate.
Figure 15: Meataparameter search results for the workloads of {MNIST, Simple-CNN, Nesterov}with a constant learning rate.
Figure 16: Meataparameter search results for the workloads of {CIFAR-10, ResNet-8, SGD} with aconstant learning rate.
Figure 17: Meataparameter search results for the workloads of {CIFAR-10, ResNet-8, Momentum}with a constant learning rate.
Figure 18: Meataparameter search results for the workloads of {CIFAR-10, ResNet-8, Nesterov}with a linear learning rate decay.
