Figure 1: Difference between accuracy (or error rate) between square loss and CE for each initial-ization. (Square loss acc. - CE acc.) is shown for accuracy, (CE - Square loss) for error rate.
Figure 2: Training curvesWe compare the convergence speed in terms of accuracy, and find that for 2-class NLP classificationtasks, the training curves of training with the square loss and the cross-entropy are quite similar. Fig-ure 2 (a) gives the accuracy of three model architectures trained with the square loss and the cross-entropy along different epochs for QNLI dataset. For all three models, BERT, LSTM+Attention,and LSTM+CNN, using the square loss converges as fast as cross-entropy loss, and achieves bet-ter/comparable accuracy to training with the cross-entropy.
