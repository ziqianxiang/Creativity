Figure 1: The Stochastic Gradient Noise Analysis. The histogram of the norm of the gradient noisescomputed with the three-layer fully-connected network on MNIST (LeCun, 1998). (a) and (c): thehistograms of the norms of two kinds of gradient noise: (a) “SGN” is computed over parameters,which is actually stochastic gradient rather than SGN; (c) SGN is computed over minibatches. (b)and (d): the histograms of the norms of (scaled) Gaussian noise and Levy noise. Based on (a) and (b),Simsekli et al. (2019) argued that gradient noise across parameters is heavy-tailed LeVy noise. Basedon (c) and (d), we show that SGN without the isotropic restriction is approximately Gaussian.
Figure 2: We empirically verified C(θ) = HBθ) by using three-layer fully-connected network onMNIST (LeCun, 1998). The pretrained Models are usually near critical points, while randomlyInitialized Models are far from critical points. We display all elements H(i,j) ∈ [1e - 4, 0.5] of theHessian matrix and the corresponding elements C(i,j) of gradient noise covariance matrix in thespace spanned by the eigenvectors of Hessian. Another supplementary experiment on Avila Dataset(De Stefano et al., 2018) in Appendix C reports Cavila ≈ 1.004H. The small difference factorbetween the empirical result and the ideal Equation is mainly because the pretrained network is notperfectly located at a critical point.
Figure 3: Kramers Escape Problem. a1 and aa are minima of two neighboring valleys. b is the saddlepoint separating the two valleys. c locates outside of Valley a1.
Figure 4:	The mean escape time analysis of SGD by using Styblinski-Tang Function. The PearsonCorrelation is higher than 0.99. Left Column: Sharpness. Middle Column: Batch Size. Right Column:Learning Rate.
Figure 5:	The mean escape time analysis of SGD by training neural networks on Avila Dataset. LeftColumn: Sharpness. Middle Column:BatCh Size. Right Column: Learning Rate. We leave the resultson Banknote AuthentiCation, CardiotoCography, and Sensorless Drive Diagnosis in Appendix D.
Figure 6:	The mean esCape time analysis of SGLD. Subfigure (a) and (b): Styblinski-Tang FunCtion.
Figure 7:	The Gradient Noise Analysis. The histogram of the norm of the gradient noises computedwith ResNet18 (He et al., 2016) on CIFAR-10 (Krizhevsky et al., 2009).
Figure 8:	The plot of the SGN covariance and the Hessian by training fully-connected network onMNIST. We display all elements Haj) ∈ [-0.03,0.03] of the Hessian matrix and the correspondingelements in gradient noise covariance matrix in the original coordinates.
Figure 9: The plot of the SGN covariance and the Hessian by training fully-connected network onAvila. We display all elements H(i,j) ∈ [1e - 4, 0.5] of the Hessian matrix and the correspondingelements in gradient noise covariance matrix in the space spanned by the eigenvectors of Hessians.
Figure 12: (a) B = 1, (b) B = 1, (c) B = 1, (d) B = 1. In Figure 13: (a) η = 0.0002, B = 100,(b) η = 0.001, B = 100, (c) η = 0.0002, B = 100, (d) η = 0.0001, B = 100. In Figure 14: (a)η = 0.0002, B = 100, D = 0.0002, (b) η = 0.001, B = 100, D = 0.0001, (c) η = 0.0002, B =100, D = 0.0005, (d) η = 0.0001, B = 100, D = 0.0003. We note that the hyperparameters needbe tuned for each initialized pretrained models, due to the stochastic property of deep learning.
Figure 10: The escape rate exponentially depends on the “path Hessians” in the dynamics of SGD.
Figure 11: The escape rate exponentially depends on the batch size in the dynamics of SGD. - log(γ)is linear with B.
Figure 12: The escape rate exponentially depends on the learning rate in the dynamics of SGD.
Figure 13: The relation of the escape rate and the isotropic diffusion coefficient D. The escapeformula that - log(γ) is linear with D is validated.
Figure 14: The relation of the escape rate and the Hessian determinant in the dynamics of whitenoise.The escape formula that Y is linear with k is validated.
Figure 19: (a) B = 1, (b) B = 1, (c) B = 1. We note that the hyperparameters are recommendedand needn’t be fine tuned again. The artificially initialized parameters avoids the stochastic propertyof the initial states.
Figure 15:	The relation of the escape rate and the diffusion coefficient D in the dynamics of SGLD.
Figure 16:	The relation of the escape rate and the Hessian determinants in the dynamics of SGLD.
Figure 17:	The escape rate exponentially depends on the sharpness in the dynamics of SGD. Theescape formula that - log(γ) is linear with 1 is validated in the setting of Styblinski-Tang Function,Logistic Regression and MLP.
Figure 18:	The escape rate exponentially depends on the batch size in the dynamics of SGD. Theescape formula that - log(γ) is linear with B is validated in the setting of Styblinski-Tang Function,Logistic Regression and MLP.
Figure 19:	The escape rate exponentially depends on the learning rate in the dynamics of SGD. Theescape formula that - log(γ) is linear with 1 is validated in the setting of Styblinski-Tang Function,Logistic Regression and MLP.
