Figure 1: Hard and Soft Decision Trees. A. Hard: is the classic “hard” oblique decision tree. Each nodepicks the child node with the largest inner product, and visits that node next. Continue until a leaf is reached.
Figure 2: Building Induced Hierarchies. Step A. Load the weights of a pre-trained model’s final fully-connected layer, with weight matrix W ∈ R ×K . Step B. Take rows wk ∈ W and normalize for each leafnode’s weight. For example, the red w1 in A is assigned to the red leaf in B. Step C. Average each pair of leafnodes for the parents’ weight. For example, w1 and w2 (red and purple) in B are averaged to make w5 (blue) inC. Step D. For each ancestor, average all leaf node weights in its subtree. That average is the ancestor’s weight.
Figure 3: ImageNet Results. NBDT outperforms all competing decision-tree-based methods by at least 14%,staying within 0.6% of EfficientNet accuracy. “EfficientNet” is EfficientNet-EdgeTPU-Small.
Figure 4: CIFAR10 Blurry Images. To make the classification task difficult for humans, the CIFAR10 imagesare downsampled by 4×. This forces at least partial reliance on model predictions, allowing us to evaluatewhich explanations are convincing enough to earn the user’s agreement.
Figure 5: Types of Ambiguous Labels. All these examples have ambiguous labels. With NBDT (top), thedecision rule deciding between equally-plausible classes has low certainty (red, 30-50%). All other decisionrules have high certainty (blue, 96%+). The juxtaposition of high and low certainty decision rules makesambiguous labels easy to distinguish. By contrast, ResNet18 (bottom) still picks one class with high probability.
Figure 6: ImageNet Ambiguous Labels. These images suggest that NBDT path entropy uniquely identifiesambiguous labels in Imagenet, without object detection labels. We plot ImageNet validation samples that in-duce the most 2-class confusion, using TinyImagenet200-trained models. Note that ImageNet classes do notinclude people. (Left) Run ResNet18 and find samples that (a) maximize entropy between the top 2 classes and(b) minimize entropy across all classes, where the top 2 classes are averaged. Despite high model uncertainty,half the classes are from the training set - bee, orange, bridge, banana, remote control - and do not show visualambiguity. (Right) For NBDT, compute entropy for each node’s predicted distribution; take the difference be-tween the largest and smallest values. Now, half of the images contain truly ambiguous content for a classifier;we draw green boxes around pairs of objects that could each plausibly be used for the image class.
