Figure 1: An illustration of the domain gen-eralization protocol on the VisDA-17 dataset,where real target domain (test) images are as-sumed unavailable during model training.
Figure 2: Feature diversity on VisDA-17 test images in R2 with Gaussian kernel density estimation (KDE).
Figure 3: (a) Previous work (Chen et al., 2018; 2020c) consider “learning without forgetting” which minimizesa distillation loss between a synthetic model and an ImageNet pre-trained one (either on features or modelparameters) to avoid catastrophic forgetting. (b) The proposed CSG framework with a “push and pull” strategy.
Figure 4: (a) For each input image, A-pool computes an attention matrix a based on the inner product betweenthe global average pooled feature vector V and vector at each position v：,i,j (V, v：,i,j ∈ RC). (b) Example offour generated reweighting matrices on different images. Note that the values are defined as the ratio of theattention over uniform weight. The attention is visualized with upsampling to match the input size (224×224).
Figure 5: An illustration of model attention by GradCAM(Selvaraju et al., 2017) on the VisDA-17 validation set.
Figure 6: Feature diversity on Cityscapes test images in R2 with Gaussian kernel density estimation (KDE).
Figure 7: Generalization results on GTA5 → Cityscapes. Rows correspond to sample images in Cityscapesvalidation set. From left to right, columns correspond to original images, ground truth, predication results ofbaseline (DeepLabv2-ResNet50 Chen et al. (2017)), and prediction by model trained with our CSG framework.
