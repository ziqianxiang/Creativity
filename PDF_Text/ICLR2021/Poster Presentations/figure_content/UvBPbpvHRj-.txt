Figure 1: Graphical representation of the artificial neurons for closely related methods. The subscriptd and the superscript l refer to the d-th unit in the l-th layer, respectively. (a) In standard NeuralNetworks (NN), both the weights and the activation function are deterministic. (b) In BayesianNNs, weights are stochastic and the activation is deterministic. (c) In auNN (this work), weights aredeterministic and the activation is stochastic. (d) Deep GPs do not have a linear projection throughweights, and the output is modelled directly with a GP defined on the Dl-1 -dimensional input space.
Figure 3: Predictive distribution (mean and one standard devia-tion) after training on a 1D dataset with two clusters of points.
Figure 4: Test NLL (a) and RMSE (b) for the gap splits in Energy and Naval datasets (mean and onestandard error, the lower the better). Activation-level uncertainty, specially through the triangularkernel, avoids the dramatic failure of BNN and fBNN in terms of NLL (see the scale). The similarvalues in RMSE reveal that this failure actually comes from an extremely overconfident estimationby BNN and fBNN, see also Figure 5.
Figure 5: Predictive distribution (mean and one standard deviation) over a segment that crosses thegap, joining two training points from different connected components. auNN avoids overconfidentpredictions by allocating more uncertainty in the gap, where there is no observed data.
Figure 6: (a) One example of activation function (mean and standard deviation) learned by auNNwith each kernel. RBF’s one is smoother, whereas TRI’s is piecewise linear, inspired by ReLu. Blackdots represent (the mean of) the inducing point values. Green dots are the locations of input datawhen propagated to the corresponding unit. (b) Test NLL of auNN and DGP for different values ofM (number of inducing points) and D (number of hidden units). The lower the better. The resultsare the average over five independent runs with different splits. Whereas DGP improves “by columns”(i.e. with M), auNN does it “by rows” (i.e. with D). This is as hypothesized, and is convenientfrom a scalability viewpoint. (c) Test NLL with increasing depth (L = 2, 3, 4). This supports thatauNN might benefit more than DGP from deeper networks. Moreover, the aforementioned differentinfluence of M and D on DGP and auNN is confirmed here.
Figure 7: Performance of the compared methods in the gap splits for six UCI datasets. Mean and onestandard error of NLL (upper row) and RMSE (lower row) are shown, the lower the better.
Figure 8: A complete example of the activation functions learned by auNN with RBF and TRI kernels.
Figure 9:	Test RMSE of auNN and DGP for different values of M (number of inducing points) andD (number of hidden units). Results are the average over 5 independent runs on the UCI Kin8 dataset.
Figure 10:	Test RMSE with increasing depth (L = 2, 3, 4). This supports that auNN might benefitmore than DGP from deeper networks. Moreover, the aforementioned different influence of M andD on DGP and auNN is confirmed here.
Figure 11: Representation of two hidden layers (with two units per layer) for auNN (a), DGP (b), andDGP-add (c).
Figure 12: Test NLL on Power dataset for different values of D and M (the lower the better).
Figure 13:	Test RMSE on Power dataset for different values of D and M (the lower the better).
Figure 14:	DGP and GP trained on the dataset of Section 3.1. The experimental details are analogousto those in Section 3.1, see Appendix B. Whereas DGP underestimates the uncertainty for in-betweendata, a simpler GP does provide increased uncertainty in the gap.
