Figure 1: Intuition of Reward RandomizationTo further illustrate the effectiveness of RPG, We introduce three Markov games - two gridworldgames and a real-world online game Agar.io. All these games have multiple NEs including both“risky” cooperation strategies and “safe” non-cooperative strategies. We empirically show that evenwith state-of-the-art exploration techniques, PG fails to discover the “risky” cooperation strategies.
Figure 2: PPO in staghunt, with a=4, b=3, d=1and various c (10 seeds).
Figure 3: Monster-HuntAgent1Lit gridNextlit gridAgent2Figure 4: EscalationAgar.io is a popular multiplayer online game. Players control cells in a Petri dish to gain as muchmass as possible by eating smaller cells while avoiding being eaten by larger ones. Larger cells moveslower. Each player starts with one cell but can split a sufficiently large cell into two, allowing themto control multiple cells (Wikipedia, 2020). We consider a simplified scenario (Fig. 5) with 2 players(agents) and tiny script cells, which automatically runs away when an agent comes by. There is alow-risk non-cooperative strategy, i.e., two agents stay away from each other and hunt script cellsindependently. Since the script cells move faster, it is challenging for a single agent to hunt them.
Figure 4: EscalationAgar.io is a popular multiplayer online game. Players control cells in a Petri dish to gain as muchmass as possible by eating smaller cells while avoiding being eaten by larger ones. Larger cells moveslower. Each player starts with one cell but can split a sufficiently large cell into two, allowing themto control multiple cells (Wikipedia, 2020). We consider a simplified scenario (Fig. 5) with 2 players(agents) and tiny script cells, which automatically runs away when an agent comes by. There is alow-risk non-cooperative strategy, i.e., two agents stay away from each other and hunt script cellsindependently. Since the script cells move faster, it is challenging for a single agent to hunt them.
Figure 5: Agar.io: (a) a simplified 2-player setting; (b) basic motions: split, hunt script cells, merge.
Figure 6: Full process ofRPG in Monster-Hunt6Published as a conference paper at ICLR 2021Figure 7: Emergent cooperative (approximate) NE strategies found by RPG in Monster-Hunt(b) The final strategy after fine-tuningFigure 9: Emergent strategies in standard Agar.io:(a) agents cooperate to hunt efficiently; (b) a largeragent breaks the cooperation by attacking the other.
Figure 7: Emergent cooperative (approximate) NE strategies found by RPG in Monster-Hunt(b) The final strategy after fine-tuningFigure 9: Emergent strategies in standard Agar.io:(a) agents cooperate to hunt efficiently; (b) a largeragent breaks the cooperation by attacking the other.
Figure 9: Emergent strategies in standard Agar.io:(a) agents cooperate to hunt efficiently; (b) a largeragent breaks the cooperation by attacking the other.
Figure 8: RR in Escalationwell as popular exploration methods, i.e., count-based exploration (PG+CNT) (Tang et al., 2017) andMAVEN (Mahajan et al., 2019). We also consider an additional baseline, DIAYN (Eysenbach et al.,2019), which discovers diverse skills using a trajectory-based diversity reward. For a fair comparison,we use DIAYN to first pretrain diverse policies (conceptually similar to the RR phase), then evaluatethe rewards for every pair of obtained policies to select the best policy pair (i.e., evaluation phase,shown with the dashed line in Fig. 6), and finally fine-tune the selected policies until convergence(i.e., fine-tuning phase). The results of RPG and the 6 baselines are summarized in Fig. 6, where RPGconsistently discovers a strategy with a significantly higher payoff. Note that the strategy with theoptimal payoff may not always directly emerge in the RR phase, and there is neither a particular valueof w constantly being the best candidate: e.g., in the RR phase, w = [5, 0, 2] frequently produces asub-optimal cooperative strategy (Fig. 7(a)) with a reward lower than other w values, but it can alsooccasionally lead to the optimal strategy (Fig. 7(b)). Whereas, with the fine-tuning phase, the overallprocedure of RPG always produces the optimal solution. We visualize both two emergent cooperativestrategies in Fig. 7: in the sub-optimal one (Fig. 7(a)), two agents simply move to grid (1,1) together,stay still and wait for the monster, while in the optimal one (Fig. 7(b)), two agents meet each otherfirst and then actively move towards the monster jointly, which further improves hunting efficiency.
Figure 10: Sacrifice strategy,w=[1, 1], aggressive setting.
Figure 11: Perpetual strategy, w=[0.5, 1] (by chance), aggressive setting, i.e., two agents mutuallysacrifice themselves. One agent first splits to sacrifice a part of its mass to the larger agent while theother agent also does the same thing later to repeat the sacrifice cycle.
Figure 12: Results on Monster-Hunt with 3 agents (3 seeds).
Figure 13: Statistics of different policy profiles in Monster-Hunt.#Coop.-Hunt: frequency of bothagents catching the monster; #Single-Hunt: frequency of agents meeting the monster alone; #Apple:apple frequency.
Figure 14:	Results in original Monster-Hunt. Original: PG in the original game; Share reward: PGwith shared reward in the original game; Finetune: fine-tuning the best policy obtained in the RRphase and yielding the highest reward in the original game.
Figure 15:	statistics of standard setting of Agar.io. (a) to (d) illustrate frequencies of Split, Hunt,Attack and Cooperate during training under different reward parameters and algorithms. Split meanscatching a script agent ball by splitting, Hunt means catching a script agent ball without splitting,Attack means catching a learn-based agent ball, Cooperate means catching a script agent ball whilethe other learn-based agent is close by.(the same below) (e) illustrates rewards of different policies.
Figure 16: Statistics of aggressive setting of Agar.io. (a) to (d) illustrate frequencies of Split, Hunt,Attack and Cooperate during training under different reward parameters and algorithms.(e) illustratesrewards of different policies.
Figure 17: Statistics of Universal policy of Agar.io. (a) to (d) illustrate the frequency of Split, Hunt,Attack and Cooperate when fixing different w while evaluating.
Figure 18: Find different policy profiles via Reward Randomization in Iterative Stag-Hunt. #Stag-Stag: the frequency of two agents both hunt the stag. #Stag-Hare: the frequency of agent1 hunts thestag while agent2 eats the hare. #Hare-Stag: the frequency of agent1 eat the hare while agent2 huntsthe stag. #Hare-Hare: the frequency of two agents both eat the hare. Frequency: times of certainbehavior performed in one episode.
Figure 19: Adaption training curve in Iterative Stag-Hunt. #Stag-Stag: frequency of both agentshunting the stag. #Stag-Hare: frequency of agent1 hunting the stag while agent2 eating the hare.
Figure 20: Adaption training statistics of Monster-Hunt. #Coop.-Hunt: frequency of both agentscatching the monster; #Single-Hunt: the adaptive agent meets the monster alone; #Apple: applefrequency.
Figure 21: Statistics of adaptation experiments of Agar.io. (a),(b) illustrate frequencies of Cooperateand Attack when the adaptive policy was facing different partners. In (a), we can see that the agentlearned to cooperate when the partner was cooperative; In (b), the descend of the "v.s. competitivepartner" line at the beginning indicates that the adaptive policy was learning to avoid being exploit;The rising of both lines in the end indicates that the adaptive policy was also learning to exploit itspartner.
