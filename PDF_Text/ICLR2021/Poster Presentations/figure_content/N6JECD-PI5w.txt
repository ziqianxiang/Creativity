Figure 1: (a) Contrastive learning framework of FairFil: Sentence x and its augmentation x0 areencoded into embeddings d and d0 , respectively. wp is the embedding of a sensitive attribute wordselected from x. INCE maximizes the mutual information between d and d0 ; ICLUB eliminates thebias information of wp from d. (b) Illustration of information in d and d0 : The blue and red circlesrepresent the information in d and d0 , respectively. The intersection is the mutual informationbetween d and d0 . The shadow area represents the bias information of both embeddings.
Figure 2: Influence of the training data proportion to debias degree of BERT.
Figure 3: T-SNE plots of sentence embedding mean of each words contextualized in templates. Theleft-hand side is from the original pretrained BERT; the right-hand side is from our FairFil.
