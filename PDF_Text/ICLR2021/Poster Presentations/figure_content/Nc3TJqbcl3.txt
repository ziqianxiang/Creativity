Figure 1: Environments and exemplary behaviors of the learned policy using APEX. From left toright: Fetch Pick&Place (sparse reward), Door (sparse reward), and Humanoid S tandup.
Figure 2: Variance of DAgger actions when relabeling 10 times the same trajectory in case ofunguided iCEM (a) vs. guided iCEMπ(b), for the FETCH PICK&PLACE task. The action variance ofiCEM is considerably higher than the one of iCEMπ-GPS guided by the shown policy (std-dev=0.30vs 0.13). The policy π is trained from a single expert rollout.
Figure 3: Effect of adaptive λ throughout iCEM∏ iterations and success rate on the FetchPICK&PLACE task. (a) The action sampling distribution is shoWn over the iCEM-iterations (ata predefined time-step) and one of the 4 action-dimensions When guiding With a Weak policy. Dashedlines indicate the action of the policy and of a high compute-budget icEM expert. Fixed λ shifts thedistribution too early, resulting in a collapse to the policy behavior and failure to find a good solution.
Figure 4: Policy performance on the test environments for APEX and baselines. SAC performance isprovided for reference.
Figure 5:	Interplay between policy and expert. Policy performance (solid line) and expert perfor-mances (dashed lines) on selected test environments for APEX. Due to warm-starting and addingpolicy samples, experts improve with the policy. For low budgets this effect is stronger, see Fig. 8.
Figure 6:	Ablation experiments. We remove different components of the APEX algorithm, see legend.
Figure 7:	The expert performance for APEX and APEX without adding policy sample. As seen, theexpert performance improves with learnt policy as the added policy sample directs expert distributiontowards a better solution space.
Figure 8:	Same as Fig. 5 but for different compute-budgets: 45, 100 (normal), 300. Notice, that inthe case of low and normal budgets in Halfcheetah Running the policy outperforms the iCEMexpert. In Fetch Pick&Place the policies are able to match the iCEM performance.
Figure 9: Evolution of the adaptive λ parameter during planning. Left for a weak, right for a mediumpolicy. The light and dark orange curves show the original min/max cost (J) among the elites. Theblue curves show how lambda changed due to the differences in the original costs.
