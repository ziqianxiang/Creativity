Figure 1: (a) Temperature scaling (TS), equally sized-histogram binning (HB), and our proposal,i.e., sCW I-Max binning are compared for post-hoc calibrating a CIFAR100 (WRN) classifier. (b)Binning offers a reliable ECE measure as the number of evaluation samples increases.
Figure 2: Histogram and KDE of CIFAR100 (WRN) logits in S constructed from 1k calibrationsamples. The bin edges of Eq. mass binning are located at the high mass region, mainly coveringclass-0 due to the imbalanced two class ratio 1 : 99. Both Eq. size and I-Max binning cover the highuncertainty region, but here only I-Max yields reasonable bin widths ensuring enough mass per bin.
Figure 3: MI evaluation: The KDEs of p(λ∣y) for y ∈ {0,1} shown in Fig. 2 are used as the groundtruth distribution to synthesize a dataset Skde and evaluate the MI of Eq. mass, Eq. size, and I-Maxbinning trained over Skde. (a) The developed iterative solution for I-Max bin optimization over Skdesuccessfully increases the MI over iterations, approaching the theoretical upper bound I(y; λ). Forcomparison, I-Max is initialized with both Eq. size and Eq. mass bin edges, both of which aresuboptimal at label information preservation. (b) We compare the three binning schemes with 2 to16 quantization levels against the IB limit (Tishby et al., 1999) on the label-information I(y; Q(λ))vs. the compression rate I(λ; Q(λ)). The information-rate pairs achieved by I-Max binning are veryclose to the limit. The information loss of Eq. mass binning is considerably larger, whereas Eq. sizebinning gets stuck in the low rate regime, failing to reach the upper bound even with more bins.
Figure A1: Histogram of ImageNet (InceptionResNetv2) logits for (a) CW and (b) sCW training. Bymeans of the set merging strategy to handle the two-class imbalance 1 : 999, S has K=1000 timesmore class-1 samples than Sk with the same 10k calibration samples from C.
Figure A2: Empirical approximation error of S vs. Sk, where Jensen-Shannon divergence (JSD) isused to measure the difference between the empirical distributions underlying the training sets forclass-wise bin optimization. Overall, the merged set S is a more sample efficient choice over Sk .
Figure A3: Histogram of CIFAR100 (WRN) logits in S constructed from 1k calibration samples,using the same setting as Fig. 2 in the main paper. Instead of categorizing the logits according totheir two-class label yk ∈ {0, 1} as in Fig. 2, here we sort them according to their ranks given by theCIFAR100 WRN classifier. As a baseline, we also plot the KDE of logits associated to the groundtruth classes, i.e., GT.
Figure A4: Distribution of the top-1 predictions and its log-space counterparts, i.e., λ = log q -log(1 - q).
