Figure 1: (a) Prediction uncertainty of ST-T, our proposed method, for an ECG time-series basedon 10 runs. To the left of the red line ST-T classifies a heart beat as normal. To the right of thered line, ST-τ makes wrong predictions. Due to its drop in certainty, however, it can alert medicalpersonnel. (b) Given a sentence with negative sentiment, ST-τ reads the sentence word by word.
Figure 2: Illustration of several ways to model uncertainty in recurrent models. Left, top: Instandard recurrent models the sequence of hidden states is identical for any two runs on the sameinput. Uncertainty is typically modeled with a softmax distribution over the classes (here: acceptand reject). Left, bottom: Bayesian NNs make the assumption that the weights are drawn from adistribution. Uncertainty is estimated through model averaging. Right: The proposed class of RNNsassumes that there is a finite number of states between which the RNN transitions. Uncertainty foran input sequence is modeled through a probability distribution over possible state-transition paths.
Figure 3: One step of ST-τ (for batch size b = 1). First, previous hidden state ht-1, previous cellstate ct-1 (if given) and input xt, are passed into an RNN cell (e.g. LSTM). The RNN cell returnsan updated hidden state ut and cell state ct . Second, ut is further processed by using the learnablestates matrix St in the state transition step (Right) and returns a new hidden state ht.
Figure 4:	Two different probabilistic finite-state RNNs.
Figure 5:	(a)+(c): ground truth probabilistic automata (PAs) used for generating training samples.
Figure 6: Calibration plots (for one run) on the BIH dataset (corresponding to the left half of Table1) using N = 10 bins. ST-τ (k = 2) is closest to the diagonal line, that is, perfect calibration.
Figure 7: (a) As training progresses the learned temperature τ deCreases. This indiCates that themodel has reduCed its epistemiC unCertainty. (b, C) The extraCted DFAs at different iterations onthe Tomita grammar 3 with input symbol and, in square braCkets, the transition probability andunCertainty, quantified by the varianCe. At the earlier stage in training (b), the probabilities are stillfar away from being deterministiC, whiCh is also indiCated by the higher value of τ and the non-zero varianCe. Later (C), the transition probabilities are Close to being deterministiC, the temperaturehas lowered and there is no more epistemiC unCertainty. States 1-3, 8, 10 are missing as the modelChoose not to use these states.
Figure 8: True PA (a) used for generating training samples and the respective extracted PAs (b, c)from the trained ST-τ models with k = 6 states. We merged reject and accept nodes in (b) to retainone reject (start state 1 is not merged) and accept node in (c). For a given state and input symbol, theprobability of accept and reject for (a) and (c) are nearly equivalent. For example, at state sA in (a),input “0”, p(accept|sA, “0”) = 0.3 ≈ 0.29 = p(accept|s1, “0”) and p(accept|sA, “0”) = 0.7 ≈0.71 = 0.5 + 0.21 = p(accept|s1, “0”).
Figure 9: Calibration plots for the IMDB dataset (corresponding to the right half of Table 1) usingN = 10 bins. BBB, VD and ST-τ are all quite well calibrated on this data set. The first run isdisplayed. Best viewed in colour.
Figure 10: For VD We plot various dropout probabilities at prediction time and their correspondingerror rates (PE, ECE, MCE). Additionally we report the results of ST-τ. For VD, while lowering theprobability rate decreases the predictive error (PE), it at the same time increases MCE and/or ECE.
Figure 11: The test mean squared error (MSE) of different methods on predicting power consump-tion over time steps (x axis). The results are based on averaging 3 runs. The shaded areas presentstandard deviation, the first 100 samples are displayed. ST-τ and VD provide the best predictiveperformance, while ST-τ exhibits tighter uncertainty bounds.
Figure 12:	Average cumulative reward and standard deviation (in log scale, the shade areas) overtime steps (episode) on the Cartpole task. Results are averaged over 5 randomly initialized runs. Inall cases ST-T achieves a higher averaged cumulative reward given lower sample complexity.
Figure 13:	In the cartpole task the goal is to balance the pole upright by moving the cart left or rightat each time step (β is the angle).
Figure 14:	Apblation study for ST-τ with (1) different number of states (k=2, 10, 20, 50) and(2) learning temperature τ (green) and fixed temperature τ = 1 (blue) on the Out-of-DistributionDetection (OOD) task. The max-probability based O-AUPR and O-AUROC are reported. Top:IMDB(In)/Customer(Out). Bottom: IMDB(In)/Movie(Out). The results are averaged over 10 runs.
