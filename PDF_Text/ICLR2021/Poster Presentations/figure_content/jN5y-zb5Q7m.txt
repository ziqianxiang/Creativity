Figure 1: Measures of Ensemble Diversity - Direction of KL DivergenceFor autoregressive models, these measures represent uncertainty in the prediction given a specificcombination of input x and context y<l . HoWever, at the beginning of a sequence token-levelmeasures of uncertainty are more sensitive to the input x and at the end of a sequence become moresensitive to the context y<l .
Figure 2: BLEU and NLL ablation study. Shading indicates ±2σEnsemble Size (models)(b) NMT EN2DE NLL18Published as a conference paper at ICLR 2021(a) ASR LTC WER(c) ASR LTO WERENS-PrEx AMIENS-ExPrAMI(b) ASR LTC NLL——ENS-PrEx LTO——ENS-ExPr LTO(d) ASR LTO NLL(e) ASR AMI WER	(f) ASR AMI NLLFigure 3:	WER and NLL ablation study. Shading indicates ±2σD	Sequence-level Error DetectionThe current appendix provides a description of the Prediction Rejection Ratio metric, the rejectioncurves which correspond to results in section 4, and histograms of sentence-WER and sentence-BLEUwhich provide insights into the behaviour of the corresponding rejection curves.
Figure 3:	WER and NLL ablation study. Shading indicates ±2σD	Sequence-level Error DetectionThe current appendix provides a description of the Prediction Rejection Ratio metric, the rejectioncurves which correspond to results in section 4, and histograms of sentence-WER and sentence-BLEUwhich provide insights into the behaviour of the corresponding rejection curves.
Figure 4:	Example Prediction Rejection Curves (Malinin, 2019)oracle in some particular order based on estimates of uncertainty. If the estimates of uncertainty areuninformative, then, in expectation, the rejection curve would be a straight line from base error rateto the lower right corner, given the error metric is a linear function of individual errors. However, ifthe estimates of uncertainty are ‘perfect’ and always bigger for a misclassification than for a correctclassification, then they would produce the ‘oracle’ rejection curve. The ‘oracle’ curve will go downlinearly to 0% classification error at the percentage of rejected examples equal to the number ofmisclassifications. A rejection curve produced by estimates of uncertainty which are not perfect, butstill informative, will sit between the ‘random’ and ‘oracle’ curves. The quality of the rejection curvecan be assessed by considering the ratio of the area between the ‘uncertainty’ and ‘random’ curvesARuns (orange in figure 4) and the area between the ‘oracle’ and ‘random’ curves ARorc (blue infigure 4). This yields the prediction rejection area ratio PRR:PRR = ARunsARorc(43)A rejection area ratio of 1.0 indicates optimal rejection, a ratio of 0.0 indicates ‘random’ rejection. Anegative rejection ratio indicates that the estimates of uncertainty are ‘perverse’ - they are higher foraccurate predictions than for misclassifications. An important property of this performance metric isthat it is independent of classification performance, unlike AUPR, and thus it is possible to comparemodels with different base error rates. Note, that similar approaches to assessing misclassification
Figure 5: Sequence-level rejection curves for NMT and ASR.
Figure 6: Sentence BLEU and WER Histograms.
Figure 7: Histograms of predicted-token confidence for ASR and NMT.
Figure 8: Sensitivity of uncertainty measures to number samples on NMT and ASR sequence errordetection.
Figure 9: Sensitivity of uncertainty measures to number samples on NMT and ASR OOD detection.
Figure 10:	Sensitivity of uncertainty measures to importance weighting calibration.
Figure 11:	Sensitivity of uncertainty measures to importance weighting calibration.
