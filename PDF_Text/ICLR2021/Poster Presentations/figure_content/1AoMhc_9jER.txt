Figure 1: The FreChet Inception Distance (FID) curve of subnetworks of SNGAN (left) and CyCleGAN (right)generated by iterative magnitude pruning (IMP) on CIFAR-10 and horse2zebra. The dashed line indicates theFID score of the full model on CIFAR-10 and horse2zebra. The 95% confidence interval of 5 runs is reported.
Figure 2: Visualization by sampling and interpolation of SNGAN Winning Tickets found by IMP. Sparsity ofbest winning tickets : 48.80%. Extreme sparsity of matching subnetworks: 73.79%.
Figure 3: Visualization of CycleGAN Winning Tickets found by IMP. Sparsity of best winning tickets :59.04%. Extreme sparsity of matching subnetworks: 67.24%. Left: visualization results on horse2zebra.
Figure 4: The FID score of Left: The FID score of best subnetworks generated by two different pruningsettings: IMPG and IMPGD . Right: The FID score of best subnetworks generated by two different pruningsettings: IMPG and IMPFG . IMPG : iteratively prune and reset the generator. IMPGD : iteratively prune andFreset the generator and the discriminator. IMPG : iteratively prune and reset the generator, and iteratively prunebut not reset the discriminator.
Figure 5: The FID curve of best subnetworks gen-erated by three different pruning methods: IMPG,IMPGD and IMPKGDD . IMPKGDD : iteratively prune andreset both the generator and the discriminator, and trainthem with the KD regularization.
Figure 7: The FID curve of best subnetworks gener-ated by three different pruning settings: IMPG, RP,and RT. RP: iteratively randomly prune the genera-tor. RT: iteratively prune the generator but reset theweights randomly.
Figure 8: Relationship between the best IS score ofSNGAN subnetworks generated by channel pruningand the percent of remaining weights. GS-32: GANSlimming without quantization (Wang et al., 2020b).
Figure A9: The curve of precision and recall ofSNGANs under different sparsities. baseline: Fullmodel. best: Best winning tickets (Sparsity: 48.80%).
