Figure 1: A standard embedding (or fully-connected) layer of 20 input neurons and 10 output neu-rons. Its corresponding matrix A ∈ R20×10 has 200 parameters, where the ith row in A is the vectorof weights of the i neuron in the input layer.
Figure 2: Factorization of the embedding layer (matrix) A ∈ R20×10 from Figure 1 via standardmatrix factorization (SVD) to obtain two smaller layers (matrices) U ∈ R20×4 and V ∈ R4×10.
Figure 3: Example of our compression scheme (MESSI) from A to Z. Here j = 3 and k = 2,and we compress the embedding layer from figure 1: (i) find the set of k = 2 subspaces, each ofdimension j = 3, that minimizes the sum of squared distances from each point (row in A) to itsclosest subspace. (ii) Partition the rows of A into k = 2 different subsets A1 and A2 , where tworows are in the same subset if there closest subspace is the same, (iii) for each subset, factor itscorresponding matrix into two smaller matrices based on its closest subspace to obtain the 2k = 4matrices U1, V1, U2 and V2 (where for every i ∈ {1, ∙∙∙ ,k}, the matrix UiVi is a low (j = 3)rank approximation for Ai), (iii) replace the original fully-connected (embedding) layer by 2 layers,where in the first (red color) we have k = 2 parallel fully-connected layers for (initialized by) U 1and U2 as in the figure, and the second (blue color) is a fully-connected layer with all the previewsk = 2, and its weights corresponds to V1 and V2 as follow. For every i ∈ {1, ∙∙∙ ,k}, the weightsform the j = 3 neurons (nodes) that are connected in the previous layer with Ui are initialized byVi. The result is a compressed layer that consists of nj + kjd = 20 × 3 + 2 × 3 × 10 = 120parameters. See more details in the figure.
Figure 4: Why k subspaces? Here, we have n = 120 data points in R3 that are spread around k = 3lines (j = 1). Factoring this data based on the optimal plane P results with large errors, since somepoints are far from this plane as can be seen in the left hand side of the figure. On the right handside, factoring the data based the 3 optimal lines `1, `2, and `3 gives a much smaller errors. Also,storing the factorization based on the plane P requires 2(120 + 3) = 246 parameters, compared to120 X 1 + 3 X 3 = 129 parameters based on '1,'2, and '3. I.e., less memory and a better result.
Figure 5: Results on RoBERTa: Accuracy drop as a function of compression rate, with fine tuningfor 2 epochs after compression. To illustrate the dependence of MESSI on the choice of k, we haveplotted several contours for constant-k. As the reader will notice, the same dataset may be ideallyhandled by different values of k depending on the desired compression.
Figure 6: Results on DistilBERT: Accuracy drop as a function of compression rate, with fine tuningfor 2 epochs after compression.
Figure 7: Implementation. Example of the factorization A = UV in our implementation. Heren = 7 and d = 7. The matrix U is built such that row z contains a row from Ui where point z waspartitioned to the ith subspace. In this example, the 4th and 6th rows were both clustered to the firstsubspace. Hence, the first 3 coordinates of the corresponding rows in the representation matrix Uare nonzero, and the other entries are zero. In this way, we usedjk dimensions so that none of the ksubspaces of dimension j interact.
Figure 8: Compressing RoBERTa results: Accuracy drop as a function of compression rate, withoutfine tuning.
Figure 9: Compressing LeNet-300-100: Accuracy drop as a function of compression rateVGG-19. We used the implementation at 2. The network consists of 16 convolutional layers, fol-lowed by 2 dense hidden (fully-connected) layers with 512 neurons each. Finally, the classificationlayer has 10 neurons. The fully-connected layers consists of 530442 parameters.
Figure 10: Compressing VGG-19: Accuracy drop as a function of compression rate of the fully-connected layers in the network2https://github.com/chengyangfu/pytorch-vgg-cifar10/blob/master/vgg.py15Published as a conference paper at ICLR 2021D MESSI-ensembleIn this section we show only the best computed results of DistilBERT: that is obtained by trainingmodels at several k values and then evaluating the model that achieves the best accuracy on thetraining set. Specifically, given a fully-connected layer of n input neurons and d output neurons, fora given compression rate x (e.g., x = 0.4 means that we want to remove 40% of the parameters),we try multiple values of k via binary search on k. For every such k value we compute the impliedvalue j = (1 - x)dn/(n + kd), and we compress the network based on those k andj via the MESSIpipeline. Finally, we save the model that achieves the best accuracy on the training set, and evaluateits results on the test set. Figure 11 reports the results for this approach.
Figure 11: Results on DistilBERT: Accuracy drop as a function of compression rate, with fine tuningfor 2 epochs after compression. The red line (MESSI, ensemble) is obtained by training models atseveral k values and then evaluating the model that achieves the best accuracy on the training set.
Figure 12:	Compressing RoBERTa (with two epochs of fine-tuning): Accuracy drop as a functionof compression rate, comparing the projective clustering approach to the known k-means clustering.
Figure 13:	Compressing RoBERTa (without fine-tuning): Accuracy drop as a function of compres-sion rate, comparing the projective clustering approach to the known k-means clustering.
Figure 14: Compressing DistilBERT (with two epochs of fine-tuning): Accuracy drop as a functionof compression rate, comparing the projective clustering approach to the known k-means clustering.
Figure 15: Compressing DistilBERT (without fine-tuning): Accuracy drop as a function of com-pression rate, comparing the projective clustering approach to the known k-means clustering.
Figure 16: Compressing LeNet-300-100: Accuracy drop as a function of compression rate. Here wecompare the projective clustering approach to the known k-means clustering.
