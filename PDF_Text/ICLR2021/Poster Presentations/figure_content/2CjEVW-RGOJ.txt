Figure 1: Skip-window computes update gates at the beginning of each window of inputs.
Figure 2: TopK-based Skip-Window cell implementation example.
Figure 3: Impact of the parameter K on accuracy/updates tradeoff for HAR. Comparison betweenThrRNN (thr=0.5 to 1.0), SkipW(L=4,8,16; thr=0.5, K=1 to L) and SkipRNN (λ=1e-3 to 3e-1).
Figure 4: Impact of the parameter K on mean square error and inputs processed for Adding Task.
Figure 5:	Impact of the parameter K on accuracy and inputs processed for MNIST. Comparisonbetween ThrRNN (λ=1e-4, thr=0.5 to 1.0) and SkipW (λ=1e-4 and λ=1e-2, L=28, thr=0.5, K=1to L). The dotted line corresponds to the performance of individual SkipRNN models trained withdifferent values of λ (from λ=1e-4 to λ=1e-2). The dashed line represents the trade-off achieved bydifferent SkipW models (L=28) (from λ=0 to λ=1e-2).
Figure 6:	Impact of the parameter K on accuracy and inputs processed for IMDB. Comparisonbetween ThrRNN (λ=1e-4, thr=0.5 to 0.9) and SkipW (λ=0, L=10 or 25, thr=0.5, K=1 to L).
Figure 7: Left: Impact of the K parameter on the computational cost upper limit (SkipW: λ=6e-2,L=8, thr=0.5). Right: ThrRNN with a similar average inputs processed is provided for comparison.
Figure 8: Proportion of inputs processed in Adding Task for different parts pi of the sequence definedby equations 12 and 13 while changing K with thr=0.5, for a model trained with λ=2e-4 and L=10.
Figure 9: Modifying (thr, K) during the sequence on HAR for SkipW (λ=1e-2, L=8): fromtradeoff1(thr=0.4, K=8, acc/updates tradeoff = 99.3%/15.5%) to tradeoff2(thr=0.65, K=1,acc/updates tradeoff=73.8%/4%).
Figure 10: Boxplots of inference latency time for a HAR Service based on SkipW (λ=1e-2, L=8)coupled with a Posenet model running on Jetson Nano.
Figure 11: Validation loss during training for SkipW and SkipRNN on HAR. These two models useapproximately the same number of inputs, respectively 11% and 13%.
Figure 12: Impact of the thr parameter on accuracy/updates tradeoff for HAR. Comparison betweenThrRNN ( thr=0.5 to 1.0), SkipW (thr=0.5 to 1.0, K = L) and SkipRNN (λ=1e-3 to 3e-1).
Figure 13:	Modifying K during the analysis of a sequence on Adding Task for SkipW (λ=2e-4,L=10): from tradeoff1 (thr=0.5, K=10, MSE/updates = 3.56e-5/44.4%) to tradeoff2(thr=0.5, K=5,MSE/updates=1.66e-2/34.2%).
Figure 14: Impact of the K parameter on the computational cost upper limit (SkipW: λ=2e-4, L=10,thr=0.5).
Figure 15:	Impact of the K parameter on the proportion of markers skipped. (SkipW: λ=2e-4, L=10,thr=0.5).
Figure 16:	Distribution of the inputs processed for Adding Task using different models with thr= 0.5. From top to bottom: SkipW K = L = 10 and λ=2e-4, SkipW with no index in sequenceK = L = 10 and λ=3e-4, ThrRNN λ=3e-4.
Figure 17: Distribution of the inputs processed for Adding Task using a SkipW model trained withthr=0.5, L=10 and λ=2e-4; inference performed with thr=0 and different values of K.
Figure 18: Distribution of the inputs processed for Adding Task using a SkipW model trained withL=10 and λ=2e-4 with thr=0.5 and different values of K.
Figure 19: Comparison of SkipRNN and SkipW and impact of the K parameter on accuracy/updatestradeoff for MNIST. For both SkipW (L=28) and SkipRNN several values of λ were tested: fromλ=1e-4 to λ=1e-2. This figure is the same as Figure 5 and illustrate some results listed in Table 2.
Figure 20: Examples of skip patterns by SkipRNN on MNIST with λ=1e-4. Blue pixels are skipped.
Figure 21: Examples of skip patterns by SkipRNN on MNIST with λ=1e-4. Blue pixels are skipped.
Figure 22: Examples of skip patterns by SkipRNN on MNIST with λ=1e-3. Blue pixels are skipped.
Figure 23: Examples of skip patterns by SkipW (λ=1e-4) on MNIST with L=28 and K=28. Thisparticular model achieves an accuracy of 96.9% and uses on average 26.9% of inputs. Blue pixelsare skipped. All images are correctly classified.
Figure 24: Examples of skip patterns by SkipW (λ=1e-4) on MNIST with L=28 and K=7. Thisparticular model achieves an accuracy of 57.4% and uses on average 21% of inputs. Blue pixels areskipped. The predicted label is on top of each image.
Figure 25: Examples of skip patterns by SkipW (λ=1e-2) on MNIST with L=28 and K=28. Thisparticular model achieves an accuracy of 86.7% and uses on average 4.5% of inputs. Blue pixels areskipped. The predicted label is on top of each image.
Figure 26: Examples of skip patterns by SkipW (λ=1e-2) on MNIST with L=28 and K=2. Thisparticular model achieves an accuracy of 83% and uses on average 4.3% of inputs. Blue pixels areskipped. The predicted label is on top of each image.
Figure 27: Modifying (thr, K) during the sequence on HAR for SkipW (λ=2e-2, L=16): fromtradeoff1: (thr=0.5, K=16, acc/updates = 98.5%/9.5%) to tradeoff2: (thr=0.5, K=1, acc/updates =94.4%/5.1%).
Figure 28: Modifying (thr, K) during the sequence on HAR for SkipW (λ=2e-2, L=16): fromtradeoff1: (thr=0.4, K=3, acc/updates = 99.1%/13.3%) to tradeoff2: (thr=0.504, K=1, acc/updates= 80.8%/3.1%).
Figure 29: Sequence latency for different devices when processing HAR Service using SkipW(λ=1e-2, L=8) coupled with PoseNet (MobileNetV1 0.75). Individual contribution of models (CNN:PoseNet, RNN: SkipW) are also reported.
Figure 30: Average energy consumption for the analysis of a HAR sequence using SkipW (λ=1e-2,L=8) and PoseNet (MobileNet 0.75) on a Jetson Nano and a Jetson TX2. The dotted line correspondsto the energy level for no activity and the dashed one to the maximum instantaneous level measuredwhen models are running.
Figure 31: Impact of the K parameter on accuracy/updates tradeoff for HAR. Comparison betweenThrRNN (thr=0.5 to 1.0), SkipW(L=4,8,16; thr=0.5, K=1 to L) and SkipRNN (λ=1e-3 to 3e-1).
Figure 32:	Replacing the sample mechanisms by other variants (without the binarization function)decreases accuracy on HAR with K=8.
Figure 33:	Replacing the sample mechanisms by other variants (without the binarization function)negatively impact the number of inputs processed on HAR with K=8.
Figure 34: Replacing the sample mechanisms by other variants (without the binarization function)decreases accuracy on HAR with K=16.
Figure 35: Replacing the sample mechanisms by other variants (without the binarization function)negatively impact the number of inputs processed on HAR with K=16.
Figure 36: Other variants of selectK do not affect accuracy on HAR with L=8.
Figure 37: Other variants of selectK do not affect accuracy on HAR with L=16.
