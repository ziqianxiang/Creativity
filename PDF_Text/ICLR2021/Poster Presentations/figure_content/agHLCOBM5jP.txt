Figure 1: Training and prediction/inference with DIVNOISING. (top) A DivNoising VAE can be trainedfully unsupervised, using only noisy data and a (measured, bootstrapped, or co-learned) pixel noise modelpNMpxi|siq (see main text for details). (bottom) After training, the encoder can be used to sample multipleZk „ qφ (z|x), giving rise to diverse denoised samples Sk. These samples Can further be used to infer consensuspoint estimates such as a MMSE or a MAP solution.
Figure 2: Qualitative denoising results. We compare two DIVNOISING samples, the MMSE estimate(derived by averaging 1000 sampled images), and results by the supervised CARE baseline. The diversitybetween individual samples is visualized in the column of difference images. (See Appendix A.9 for additionalimages of DivNoising results.)5 DivNoisingIn DivNoising, we build on the VAE setup but interpret it from a denoising-specific perspective.
Figure 3: Sensibility of Noise Models. For each predicted signalintensity (x-axis), we show the variance of noisy observations (y-axis). The plot is generated from experiments on the Convallariadataset. The dashed red line shows the true noise distribution (mea-sured from pairs of noisy and clean calibration data). This truedistribution, as well as the noise model created via bootstrapping,and the noise model we co-learned with DivNoising, show sim-ple (approximately) linear relationships between signal intensitiesand noise variance. Such a relationship is known to coincide withthe physical reality of Poisson noise (shot noise) (Zhang et al.,2019). The implicitly learned noise model of the vanilla VAEhas to independently predict the noise variance for each pixel. Itspredictions clearly deviate from the true linear relationship. SeeAppendix A.13 for results on BioID Face dataset and more details.
Figure 4: Exploring the learned posterior. The MMSE estimate (average of 10k samples) shows faintlyoverlaid letters as a consequence of ambiguities in noisy input. Among these samples from the posterior, weuse mean shift clustering (on smaller crops) to identify diverse and likely points in the posterior. We show 9such cluster centers in no particular order. We also obtain an approximate MAP estimate (see SupplementaryMaterial), which has most artifacts of the MMSE solution removed.
Figure 5: DIVNOISING enables downstream segmentation. We show input images (upper row) and resultsof a fixed (untrained) segmentation pipeline (lower row). Cells that were segmented incorrectly (merged orsplit) are indicated in magenta. While segmentations of the noisy raw data are of very poor quality, sampledDivNoising results give rise to much better and diverse solutions (cols. 2-5). We then use two label fusionmethods to find consensus segmentations (col. 6), which are even outperforming segmentation results on highSNR (GT) images. Quantitative results are presented in Appendix Fig. 11.
Figure 6: The fully convolutional architecture used for depth 2 networks. We show the depth 2DIVNOISING network architecture used for FU-PN2V Convallaria, FU-PN2V Mouse nuclei, FU-PN2V Mouse actin, all W2S channels and DenoiSeg Mouse datasets. These networks count about200k parameters and have a GPU memory footprint of approximately 1.8GB on a NVIDIA TITAN Xp.
Figure 7: The fully convolutional architecture used for depth 3 networks. We show the depth 3DIVNOISING network architecture used for DenoiSeg Flywing and eBook datasets. These networkscount about 700k parameters and have a GPU memory footprint of approximately 5GB on a NVIDIATITAN Xp.
Figure 8: Generating synthetic images with the DivNoising VAE for the FU-PN2V Conval-laria dataset Krull et al. (2020); Prakash et al. (2020). DIVNOISING can also be used to generateimages by sampling from the unit normal distribution ppzq and then using the decoder to produce animage. Here, we compare generated images and randomly cropped real images. We show images ofdifferent resolutions to see how well the VAE captures structures at different scales. The VAE weuse for denoising is only able to realistically capture small local structures. Note that the network weuse is quite shallow (see Appendix Fig. 6).
Figure 9: Generating synthetic images with the DivNoising VAE for the DenoiSeg Fly-wing Buchholz et al. (2020) dataset. DIVNOISING can also be used to generate images by samplingfrom the unit normal distribution ppzq and then using the decoder to produce an image. Here, wecompare generated images and randomly cropped real images. We show images of different reso-lutions to see how well the VAE captures structures at different scales. Note that the network (seeAppendix Fig. 7) we use is a bit deeper compared to Supplementary Fig. 8. This VAE captures largerstructures a little better but struggles to produce crisp high frequency structures. This is likely aconsequence of the increased depth of the used network.
Figure 10: Generating synthetic images with the DivNoising VAE for the MNIST LeCunet al. (1998) dataset. DIVNOISING can also be used to generate images by sampling from theunit normal distribution ppzq and then using the decoder to produce an image. Here, we comparegenerated images and random ground truth images. Our fully convolutional architecture allows us togenerate images of different sizes (despite all input images being only of size 28 X 28).
Figure 11: DIVNOISING enables downstream segmentation. Evaluation of segmentation results (using theF1 score (Van Rijsbergen, 1979), Jaccard score (Jaccard, 1901) and Average Precision (Lin et al., 2014). On thex-axis we plot the number of DIVNOISING samples used. The performance of BIC is only evaluated up to 100samples because we limited run-time to 30 minutes). Remarkably, Consensus (Avg) using only 30 DIVNOISINGsegmentation labels, outperforms segmentations obtained from high SNR images.
Figure 12: Analyzing the denoising quality and diversity of DivNoising samples with differ-ent factors for the DenoiSeg Flywing dataset. (a) The heatmap shows how the quality (PSNR indb) of DivNoising MMSE estimate changes with averaging increasingly larger number of samples(numbers shown for 1 run). Unsurprisingly, the more samples are averaged, the better the results get.
Figure 13: Qualitative analysis of the effect of weighting KL loss term with factor β for De-noiSeg Flywing dataset. (a) We show the DIVNOISING MMSE estimate obtained by averaging1000 samples for all considered β values (Supplementary Eq. 5). We observe that the reconstruc-tion quality suffers on either increasing β > 1 or decreasing β < 1. Best results (with respect toPSNR) are obtained with β “ 1, as demonstrated in Fig. 12a. (b) For each β value, we show threerandomly chosen DIVNOISING samples as well as difference images. Increasing β > 1, allowsthe DivNoising network to generate structurally very diverse denoised solutions, while typicallyleading to textural smoothing. Decreasing β < 1 generates DIVNOISING samples with overall muchreduced structural diversity, introducing reconstruction artefacts/structures at smaller scales.
Figure 14: Additional qualitative results for the DenoiSeg Mouse Buchholz et al. (2020) dataset.
Figure 15: Additional qualitative results for the DenoiSeg Flywing Buchholz et al. (2020)dataset. Here, we show qualitative results for two cropped regions (green and cyan). The MMSEestimate was produced by averaging 1000 sampled images. We choose 3 samples to display toillustrate the diversity of DivNoising results.
Figure 16: Additional qualitative results for the FU-PN2V Convallaria Krull et al. (2020);Prakash et al. (2020) dataset. Here, we show qualitative results for two cropped regions (greenand cyan). The MMSE estimate was produced by averaging 1000 sampled images. We choose 3samples to display to illustrate the diversity of DivNoising results.
Figure 17: Additional qualitative results for the FU-PN2V Mouse nuclei Prakash et al. (2020)dataset. Here, we show qualitative results for two cropped regions (green and cyan). The MMSEestimate was produced by averaging 1000 sampled images. We choose 3 samples to display toillustrate the diversity of DivNoising results.
Figure 18: Additional qualitative results for the FU-PN2V Mouse actin Prakash et al. (2020)dataset. Here, we show qualitative results for two cropped regions (green and cyan). The MMSEestimate was produced by averaging 1000 sampled images. We choose 3 samples to display toillustrate the diversity of DivNoising results.
Figure 19: Additional qualitative results for the W2S Zhou et al. (2020) dataset (ch. 0, avg1).
Figure 20: Additional qualitative results for the W2S Zhou et al. (2020) dataset (ch. 1, avg1).
Figure 21: Additional qualitative results for the W2S Zhou et al. (2020) dataset (ch. 2, avg1).
Figure 22: Additional qualitative results for the MNIST LeCun et al. (1998) dataset. Here, weshow qualitative results for two cropped regions (green and cyan). The MMSE estimate was producedby averaging 1000 sampled images. We choose 3 samples to display to illustrate the diversity ofDivNoising results.
Figure 23: Additional qualitative results for the KMNIST Clanuwat et al. (2018) dataset. Here,we show qualitative results for two cropped regions (green and cyan). The MMSE estimate wasproduced by averaging 1000 sampled images. We choose 3 samples to display to illustrate thediversity of DivNoising results.
Figure 24: Additional qualitative results for the eBook Marsh (2004) dataset. Here, we showqualitative results for two cropped regions (green and cyan). The MMSE estimate was producedby averaging 1000 sampled images. We choose 3 samples to display to illustrate the diversity ofDivNoising results.
Figure 25: Additional qualitative results for the BioID Face noa dataset. Here, we showqualitative results for two cropped regions (green and cyan). The MMSE estimate was producedby averaging 1000 sampled images. We choose 3 samples to display to illustrate the diversity ofDivNoising results.
Figure 26: Qualitative results for the BSD68 dataset Roth & Black (2005). Our relatively smallDivNoising networks fail to capture the ample structural diversity present in natural photographicimages thereby exhibiting sub-par performance. However, diversity at adequately small imagescales (with respect to the used network’s capabilities) can still be observed, as demonstratedwith the different samples and the difference images corresponding to the green and cyan insets.
Figure 27: Graphical model of the data generation process.
Figure 28: Comparison of noise models and variance maps predicted by the vanilla VAE andDivNoising. (a) For each predicted signal intensity (x-axis), We shoW the variance of noisyobservations (y-axis). The plot is generated from experiments on the BioID Face dataset. Thedashed red line shoWs the true noise distribution (Gaussian noise With σ2 “ 225). The noise modelcreated via bootstrapping, and the noise model We co-learned With DivNoising, correctly shoW(approximately) constant values across all signal intensities. The implicitly learned noise model ofthe vanilla VAE has to independently predict the noise variance for each pixel. Its predictions clearlydeviate from the true constant noise variance. (b) We visually compare the denoising results and shoWhoW the predicted variance varies across the image. While the variance predicted by the implicitlyco-learned vanilla VAE model varies depending on the image content, the variance predicted by theco-learned DivNoising model correctly remains flat.
Figure 29: Comparison of noise models and variance maps predicted by the vanilla VAEand DivNoising. (a) For each predicted signal intensity (x-axis), we show the variance of noisyobservations (y-axis). The plot is generated from experiments on the Convallaria dataset. The dashedred line shows the true noise distribution (measured from pairs of noisy and clean calibration data).
