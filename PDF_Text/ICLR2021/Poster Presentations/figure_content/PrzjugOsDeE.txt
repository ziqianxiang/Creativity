Figure 1: HVE (Eq. (6)) and SVE (Eq. (7)) estimate p(x|y) (a univariate Gaussian conditional on y)using two samples in hard and soft vicinities, respectively, of y. To estimate p(x|y) (the red Gaussiancurve) only from samples drawn from p(x|y1) and p(x|y2) (the blue Gaussian curves), estimation isbased on the samples (red dots) in a hard vicinity (defined by y ± κ) or a soft vicinity (defined by theweight decay curve) around y. The histograms in blue are samples in the hard or soft vicinity. Thelabels y1 , y, and y2 on the x-axis denote the means of x conditional on y1 , y, and y2 , respectively.
Figure 2: Visual results for the Circular 2-D Gaussians simulation. (a) shows 1,200 training samplesfrom 120 Gaussians, with 10 samples per Gaussian. In (b) to (d), each GAN generates 100 fakesamples at each of 12 means not appearing in the training set, where green and blue dots stand forfake and real samples respectively.
Figure 4: Three UTKFace example images foreach of 10 ages: real images and example fakeimages from cGAN and two proposed CcGANs,respectively. CcGANs produce face images withhigher visual quality and more diversity.
Figure 3: Three RC-49 example images for eachof 10 angles: real images and example fake im-ages from cGAN and two proposed CcGANs,respectively. CcGANs produce chair imageswith higher visual quality and more diversity.
Figure 5: Line graphs of FID/NIQE versus regression labels on RC-49 and UTKFace. Figs. 5(a) to5(d) show that two CcGANs consistently outperform cGAN across all regression labels. The graphsof CcGANs also appear smoother than those of cGAN because of HVDL and SVDL.
