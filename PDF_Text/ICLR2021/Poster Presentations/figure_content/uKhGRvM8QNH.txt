Figure 1: Results overview.
Figure 2: Details of our methods: (a) Attention-guided distillation generates the spatial and channelattention with average pooling in the channel and spatial dimension, respectively. Then, studentsare encouraged to mimic the attention of teachers. Besides, students are also trained to mimicthe feature of teachers, which is masked by the attention of both students and teachers. (b) Non-local distillation captures the relation of pixels in an image with non-local modules. The relationinformation of teachers is learned by students with L2 norm loss. (c) The architecture of non-localmodules. ‘1x1’ is convolution layer with 1x1 kernel. (d) Distillation loss is applied to backbonefeatures with different resolutions. The detection head and neck are not involved in our methods.
Figure 3: Comparison between the proposedattention-guided distillation and other methods.
Figure 4: Visualization and distribution of the spatial attention with different T. With a smaller T,the pixels of high attention values are emphasized more in knowledge distillation.
Figure 5: Hyper-parameter sensitivity study of α, β, γ, T with Faster RCNN on MS COCO2017.
Figure 6: Qualitative analysis on MS COCO2017 with distilled and baseline Faster RCNN. We markthe undetected and wrongly detected objects of the baseline detector with orange boxes.
Figure 8: Relation between stu-dents and teachers on FasterRCNN and Cascade RCNN inMS COCO2017.
Figure 7: Distribution of error types on distilled and baselineFaster RCNN. Loc - Localization error; Sim & Oth - Classifica-tion error on similar & not similar classes; BG - False positiveprediction fired on background. FN - False Negative prediction.
