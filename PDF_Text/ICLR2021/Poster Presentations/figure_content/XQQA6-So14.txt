Figure 1: Color is used to denotep(x|t), which can be evaluated for Neu-ral STPPs. After observing an event inone mode, the model is instantaneouslyupdated as it strongly expects an event inthe next mode. After a period of no ob-servations, the model smoothly revertsback to the marginal distribution.
Figure 2: Visualization of the sampling paths of Neural STPP models for a 1-D spatio-temporal data set where{ti }i4=1 are event times. The Jump CNF uses instantaneous jumps to update its distribution based on newlyobserved events while the Attentive CNF depends continuously on the sampling paths of prior events. Weadditionally visualize a second sequence for the Attentive CNF where the random base samples {x(0i)}i4=2 are thesame as in sequence 1. Even so, the sampling paths are different because the first event is different, effectivelyleading to different conditional spatial distributions. See Figure 9 for visualizations of the learned density.
Figure 3: Lower variance estimates of the log-likelihood allows training better Attentive CNFs.
Figure 5: Evolution of spatial densities on PINWHEEL data. top: Attentive CNF. bottom: Jump CNF. (a) Beforeobserving any events at t=0, the distribution is even across all clusters. (b-f) Each event increases the probabilityof observing a future event from the subsequent cluster in clock-wise ordering. (g-h) After a period of no newevents, the distribution smoothly returns back to the initial distribution (a).
Figure 7: Snapshots of conditional spatial distributions modeled by the Jump CNF (top) and a conditional kerneldensity estimator (KDE; bottom). (a) Distribution before any events at t=0. (b-d) The Jump CNFâ€™s distributionsconcentrate around tectonic plate boundaries where earthquakes and aftershocks gather, whereas the KDE mustuse a large variance in order to capture propagation of aftershocks in multiple directions.
Figure 8: Runtime comparison of Jump and Attentive CNF.
Figure 9: Both the Jump CNF and Attentive CNF are capable of modeling different the spatial distributions basedon event history, so the appearance of a new event effectively shifts the distribution instantaneously. Shown on asynthetic 1-D data set similar to Pinwheel, except we use a mixture of three Gaussians. Each event increasesthe likelihood of events for the cluster to the right.
Figure 10:	Learned attention weights for random event sequences.
Figure 11:	Histograms of the number of events per sequence in each processed data set.
