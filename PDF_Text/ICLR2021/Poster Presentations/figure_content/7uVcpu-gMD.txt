Figure 1: Proportion of shared weights per layer on addition/multiplication. Left: FNN, right: LSTM.
Figure 2: Double addition task: proportion of weights shared per operation in case of (a) feedforwardnetwork, (b) LSTM, both inputs presented together. The first and last layers have no shared weights.
Figure 3: Proportion of the weights of a task sharedwith any of the previous tasks. Every second task onpermuted MNIST. Each task corresponds to a permu-tation. The last layer has the lowest capacity, filling upfirst, forcing subsequent runs to share weights.
Figure 4: Results of experiments on SCAN. (a) Test accuracy on split shown on x-axis with maskstrained on the full problem (blue, orange) and with masks trained on split shown on x-axis (green,red). LSTM: blue, green, Transformer: orange, red (b) Percentage of weights removed per tokenfrom the output layer of the LSTM decoder trained on the “Add jump” split.
Figure 5:	Accuracy on the “hard” test set of differ-ent tasks of the Mathematics Dataset: model withoutmasks, masks trained on IID data and masks trainedon “easy” set. A performance drop can be observed,because of the sample-specific weights. 5 seeds/task.
Figure 6:	(a) Relative drop in performance for simple CNN, simple CNN without dropout andResNet-110. (b) Largest performance drop in a non-target class relative to the drop in the target class.
Figure 7: Accuracy drop for masks applied to half of the weights. See Sec. B.2 for details.
Figure 8: Double addition task with manually edited input/output weight matrices to reuse the hiddenlayers. Proportion of weights shared per operation in case of FFN.
Figure 9:	Histogram (normalized as a 500-bin PDF) of expected values of the mask elements(μi = σ(li)) on different tasks. (a) Shown on a linear scale. Values < 0.0002 (bottom 10% of thefirst bin) are removed from the calculation because their number vastly exceeds the number of keptweights for most of the networks, making the details invisible. (b) All mask means, μ%, (withoutsmall values removed) shown on log-scale.
Figure 10:	Sensitivity analysis for hyperparameter β = bα (b is the batch size) on addition/multi-plication experiments. Note the logarithmic x-axis. The color indicates the total amount of sharing[%]. The red line and the star indicate the value chosen for our experiments. Each point is a mean of10 independent seeds. The network is not very sensitive to the exact choice of β. (a) Big network,with 5 layers of size 2000. (b) Medium network, with 5 layers of size 800. It can be seen that thehyperparameter transfers well between network sizes.
Figure 11: Addition/multiplication task: the total proportion of shared weights for the “add” operationfor different network sizes. “small” means a 4 layer network with hidden sizes of 400, 400, 200,“medium” 5 layers / hidden sizes of 800, “big” 5 layers / 2000, “huge” 5 layers / 4000.
Figure 12: Addition/multiplication taks: number of weights per operation for each layer in (a)feedforward network, (b) LSTM.
Figure 13: Analysis of FNN (a, b, c) and LSTM (d, e, f) performance degradation on the addition/multiplication task. The y-axis shows the target operation. The x-axis shows the actual operationperformed. “none” means the predicted number is neither the result of addition nor multiplication.
Figure 14: Proportion of weights shared per layer after every second task on permuted MNIST, fora network with masks initialized to prefer reusing the old weights. Old weights are sampled withP ≈ 0.88 probability. Each task corresponds to a permutation. Decreasing the probability of newweights forces increased sharing.
Figure 15: The networks’ performance when it is directly trained and tested on the splits indicated onthe x-axis, without masking. This is the standard setup from Lake & Baroni (2018). Performancewhen the network is trained on IID split, then masks are trained on train split indicated on the x-axis.
Figure 16: Examples from Mathematics Dataset. One sample for every task we use.
Figure 17: Confusion matrix on CIFAR10 with masks trained on all classes. It can be seen thatperformance without dropout is a few percent lower, as expected. ResNet-110 has a significantlybetter performance in all classes.
Figure 18: Simple CNN: The change in confusion matrix for all CIFAR10 classes, whenindicated by the caption, is removed.
Figure 19: Simple CNN without dropout: The change in confusion matrix for all CIFAR10 classes,when class indicated by the caption, is removed. The network has the same architecture as Table 5,but without the dropout layers. The performance drop is reduced by roughly 30%-40% compared tothe same architecture with dropout (Fig. 18).
Figure 20: ResNet-110: The change in confusion matrix for all CIFAR10 classes, when class indicatedby the caption, is removed.
