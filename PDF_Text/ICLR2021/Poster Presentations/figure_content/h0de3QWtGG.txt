Figure 1: Explaining decision-making behaviour in terms of preferences over What if outcomes.
Figure 2: Counterfactual inverse reinforcementlearning (CIRL). Counterfactuals are used todefine φ(h, a), to estimate feature expectationsμπ of candidate policy π in batch setting andto learn optimal policy for reward weights W.
Figure 3: Reward weights recovered bybenchmarks over 10 runs. The weights ofthe expert are w1 = -0.3 and w2 = -0.7.
Figure 4: Radar plot of rewardweights magnitude for assign-ing antibiotics.
Figure 5: The two different components part of our method for obtaining interpretable parametriza-tions of decision making: (a) Counterfactual model which estimates the potential outcomes for takingaction at given patient history ht . Note that we do not propose a new counterfactual estimationalgorithm, but instead use already existing methods which could have limitations in terms of howthey handle large action spaces and large patient covariates. (b) CIRL algorithm which represents thenovelty of this work. We propose incorporating the estimated counterfactuals as part of a method forbatch IRL that can recover the preferences of the expert over the "what if" patient outcomes. As partof the limitations of CIRL are the use of linear rewards and time-invariant reward weights w.
