Figure 1: Illustration of the proposed LiftDoWnPool and LiftUpPool vs. MaxPool and MaxUp-Pool on an image from CIFAR-100. Where MaxPool takes the maximum activations from the input,LiftDownPool decomposes the input into four sub-bands: LL, LH, HL and HH. LL containslow-pass coefficients. It better reduces aliasing compared to MaxPool. LH, HL and HH repre-sent details along horizontal, vertical and diagonal directions. For simplicity, we just upsample thedown-pooled results for illustrating the up-pooling. MaxUpPool generates a sparse map with lost de-tails. LiftUpPool produces a refined output from the recorded details by performing LiftDownPoolbackwards.
Figure 2: LiftDoWnPool and Lift-UpPool implementations. (a)LiftDownPool-ID. x is split intoxe and xo. The Predictor and UPdatergenerate details d and an approximationS. (b) LiftUPPool-1D. By runningLiftDownPool backwards, xe and xoare generated from S and d, and thenmerged into x.
Figure 3: LiftDoWnPool visualization. Selected feature maps of an image in CIFAR-100, from thefirst LiftDoWnPool layer in VGG13. LL represents smoothed feature maps with less details. LH,HL and HH represent detailed features along horizontal, vertical and diagonal directions. Eachsub-bands contains different correlation structures.
Figure 4: Shift Robustness com-parisons between various pool-ing methods including MaxPool,Skip, AveragePool and the pro-posed LiftDownPool. LiftDown-Pool improves classification con-SistenCy and meanwhile boosts theaccuracy, independent of the back-bone used.
Figure 5: LiftUPPool for Semantic Segmentation. Visualization of semantic segmentation mapson PASCAL-VOC12 based on SegNet with varying up-pooling methods. LiftUpPool presents morecompleted, precise segmentation maps with smooth edges.
Figure 6: Comparisons between MaxPool and LiftDoWnPool, MaxUpPool and LiftUpPooLMaxPool looses details. With the recorded maximum indices, MaxUpPool generates a very sparseoutput. LiftDownPool decomposes the input into an approximation and several details sub-bands. Itrealizes a pooling by summing up all sub-bands. LiftUpPool produces a refined output by perform-ing LiftDownPool backwards.
Figure 7: High-resolution feature maps visualization. LiftDoWnPool better maintains local struc-ture.
Figure 8: Illustration how LiftDoWnPool reduces aliasing compared to downsizing (Sweldens,1998). Dashed line means original signal. (a) solid line is after doWnsizing. (b) solid line is afterLiftDownPool. The solid and dashed lines cover the same area in (b).
Figure 9: Comparisons between the robustness of various pooling methods to per kind of cor-ruption on ImageNet-C and perturbation on ImageNet-P. LiftDownPool presents stronger ro-bustness to almost all the corruptions and perturbations.
Figure 10: Visualization of feature maps Per-Predicted-Category from the last layer of SegNet.
