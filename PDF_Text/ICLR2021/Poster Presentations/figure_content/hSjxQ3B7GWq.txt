Figure 1: Overview of Sample-efficient AutoRL(SEARL).
Figure 2: Performance comparison of SEARL, modified PBT, and random search regarding therequired total steps in the environment. Random search and modified PBT require an order ofmagnitude more steps in the environment to reach the same reward as SEARL. The blue line depictsSEARL, the green line of modified PBT, and the red line of random search. All approaches areevaluated over 10 random seeds. The x-axis shows the required environment interactions in log-scale.
Figure 3: Comparison of the mean performance over 10 random seeds of SEARL (blue line) against 10randomly sampled configurations of TD3, evaluated over 10 random seeds (red) over 2M environmentframes. The shaded area represents the standard deviation.
Figure 5: Comparison of the performance impact when removing different SEARL features individu-ally. All results show mean performance across five different random seeds. The standard deviationis shown as shaded areas.
Figure 6:	This plot shows the impact of target re-creation and optimizer re-initialization after eachepisode in the TD3 algorithm. The default configuration uses a soft target update and only initializesthe optimizer once at the beginning of the training. The plots show the mean performance across 20random seeds on the MuJoCo suite, and the shaded area represents the standard deviation.
Figure 7:	The mean performance over 10 random seeds of SEARL using the default configurationfor hyperparameter initialization (blue line) compared with random initialization (orange) over 2Menvironment steps. The shaded area represents the standard deviation.
Figure 8:	Change of the total network size (as the sum of hidden nodes in all layers, orange) and thelearning rates (actor learning rate in red, critic learning rate in blue) over time. A plot contains allpopulation individuals configurations over 2M steps for one SEARL run.
Figure 9:	The comparison of SEARL performance against a held-out tuned TD3 configuration, tunedwith the same budget of a SEARL run (2M environment interactions), and evaluated over 2M steps.
Figure 10: Performance comparison of SEARL and random search regarding the required total stepsin the environment. Random search requires an order of magnitude more steps in the environment toreach the same reward level as SEARL. The blue line depicts SEARL and the red line random search.
