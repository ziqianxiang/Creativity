Figure 1: Histograms of token-level NLL loss using standard models on NQG and CNN/DM dev sets.
Figure 2: Left: Avg human ratings vs. generationlength, on 736 NQG samples. (Colored regions:95% confidence interval.) Each data point has â‰¥30annotations. The quality of long generations fromMLE-trained model drops heavily, but stays stableacross lengths for GOLD-s generations. Right:Avg NLL loss of tth token given the gold prefixtokens vs. time-step t, on NQG dev set. Withoutexposure bias, NLL loss stays stable across lengths.
Figure 3: Exposure bias related figures on NQG dev set. Vertical axis: avg unsmoothed sentence-levelBLEU. Horizontal axis: sentence length. The colored regions represent 95% confidence intervalobtained using standard bootstrapping. Subfigures (a) and (c) show BLEU on randomly shuffledtargets (from dev set); BLEU does not appear to punish long sentences. Note the scale of the vertialaxes. Subfigures (b) and (d) show BLEU vs. generation length; BLEU on generations from MLE-trained model decreases by length, but BLEU on generations from GOLD-trained model appears tostay relatively stable.
Figure 4: Accuracy of correct predictions of tth token given all prefix reference tokens on NQGdev set. Colored regions represents 95% confidence interval obtained using standard bootstrapping.
Figure 5:	Interface for NQG pairwise comparisons, using Amazon Mechanical Turk.
Figure 6:	Interface for summarization pairwise comparisons, using Amazon Mechanical Turk.
Figure 7:	Interface for NQG human ratings, using Amazon Mechanical Turk.
