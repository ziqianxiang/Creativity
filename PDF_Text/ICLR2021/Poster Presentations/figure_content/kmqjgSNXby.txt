Figure 1: Standard probabilistic dynamics models (e.g., Chua et al. (2018)) use a neural network to predict themean and standard deviation of different dimensions of the next state and reward simultaneously. By contrast,we use the same neural network architectures with several additional inputs and predict the mean and standarddeviation of each dimension of the next state conditional on previous dimensions of the next state. As empiricalresults indicate, this small change makes a big difference in the expressive power of dynamics models. Note thatreward prediction is not shown on the right to reduce clutter, but it can be thought of as (n+1)th state dimension.
Figure 2:	Network parameter count vs. validation negative log-likelihood for autoregressive and feedforwardmodels. Autoregressive models often have a lower validation NLL irrespective of parameter count.
Figure 3:	Validation negative log-likelihood vs.OPE correlation coefficients on different tasks. On 4 RLUnplugged tasks, We conduct an extensive experiment in which 48 Autoregressive and 48 Feedforward Dynamicsmodels are used for OPE. For each dynamics model, We calculate the correlation coefficient between model-based value estimates and ground truth values at a discount factor of 0.995. We find that low validation NLLnumbers generally correspond to accurate policy evaluation, while higher NLL numbers are less meaningful.
Figure 4: Comparison of model-based OPE using autoregressive and feedforward dynamics models withstate-of-the-art FQE methods based on L2 and distributional Bellman error. We plot OPE estimates on the y-axisagainst ground truth returns with a discount of .995 on the x-axis. We report the Pearson correlation coefficient(r) in the title. While feedforward models fall behind FQE on most tasks, autoregressive dynamics models areoften superior. See Figure B.4 for additional scatter plots on the other environments.
Figure 5: Model-based offline policy optimization results. With planning and data augmentation, we improvethe performance over CRR exp (our baseline algorithm). When using autoregressive dynamics models (CRR-planning AR), we outperform state-of-the-art on Cheetah run and Fish swim. Previous SOTA results (Gulcehreet al., 2020; Wang et al., 2020) are obtained using different offline RL algorithms: Cheetah run - CRR exp, Fishswim - CRR binary max, Finger turn hard - CRR binary max, Cartpole swingup - BRAC (Wu et al., 2019).
