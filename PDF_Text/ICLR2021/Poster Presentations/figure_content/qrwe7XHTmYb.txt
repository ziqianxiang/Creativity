Figure 1: Multilingual translation quality (average ∆BLEU comparing to bilingual baselines) improved as MoEmodel size grows up to 600B, while the end-to-end training cost (in terms of TPU v3 core-year) only increasedsublinearly. Increasing the model size from 37.5B to 600B (16x), results in computation cost increase from 6 to22 years (3.6x). The 600B parameters model that achieved the best translation quality was trained with 2048TPU v3 cores for 4 days, a total cost of 22 TPU v3 core-years. In contrast, training all 100 bilingual baselinemodels would have required 29 TPU v3 core-years. Our best quality dense single Transformer model (2.3Bparameters) achieving ∆BLEU of 6.1, was trained with GPipe for a total of 235.5 TPU v3 core-years.
Figure 2: Illustration of scaling of MoE Transformer Encoder Layers. Decoder modification is similar. (a)Standard Transformer. (b) Replacing every other feed forward layer with a MoE layer (c) The MoE layer issharded across multiple devices, while all other layers are replicated.
Figure 3: Translation quality comparison of multilingual MoE Transformer models trained with GShard andmonolingual baselines. MoE(128E, 12L) represents the model with 12 layers and 128 experts per layer. Positionsalong the x-axis represent languages, raging from high- to low-resource. ∆BLEU represents the quality gainof a single multilingual model compared to a monolingual Transformer model trained and tuned for a specificlanguage. MoE Transformer models trained with GShard are reported with solid trend-lines. Dashed trend-linerepresents a single 96 layer multilingual Transformer model T(96L) trained with GPipe on same dataset. Eachtrend-line is smoothed by a sliding window of 10 for clarity. (Best seen in color)held-out test sets, we also expect them to exhibit high transfer capability across languages as anothermanifestation of generalization performance Lampinen & Ganguli (2018).
Figure 4: Comparison between MPMD and our proposed SPMD partitioning of a Dot operator ([M, K] ×[K, N] = [M, N]) across 4 devices. In this example, both operands are partitioned along the contractingdimension K, where each device computes the local result and globally combines with an AllReduce. MPMDpartitioning generates separate operators for each device, limiting its scalability, whereas SPMD partitioninggenerates one program to run on all devices. Note that the compilation time with our SPMD partitioning isnot-dependent of the number of devices being used.
Figure 5:	Examples of Einsum partitioning with cross-device communication.
Figure 6:	An example of two different device assignments based on the device topology. A 2D tensor is split by2x4 partitions and the communication pattern is between partitions along the rows of the tensor. The numbersrepresent device ids.
Figure 7: Per-device memory consumption in gigabytes.
Figure 8: Measured vs roofline execution time breakdown. Only the forward pass is shown, and the backwardpass has similar breakdown. “MoE dispatch and combine” represents cross-partition communication withAllToAll.
Figure 9: Performance scaling of communication, AllReduce and AllToAll. Log scale on both axes. AllReducecost is roughly O(1), and AllToAll cost is roughly O(√zD), where D is the number of partitions. We measuretheir performance with 8MB and 32MB data. For AllToAll, that means each partition initially has 8MB (or32MB) data, then divides it to D pieces, and sends each piece to a different receiving partition.
Figure 10: Training loss curves from different model scales under low (upper graph) and high (lower graph)compute budget. Lower loss can be obtained by growing the model capacity until some budget-dependentmaximum size, which increased with higher training budget. Given the same five TPU core years training budget,we observed that the 150B model hits the lowest training loss. But with 30 TPU core years, 600B achieved thelowest loss.
