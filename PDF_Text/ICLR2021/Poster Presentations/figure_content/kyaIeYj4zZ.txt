Figure 1: An overview of Grappa pre-training approach. We first induce a SCFG given someexamples in Spider. We then sample from this grammar given a large amount of tables to generatenew synthetic examples. Finally, Grappa is pre-trained on the synthetic data using SQL semanticloss and a small amount of table related utterances using MLM loss.
Figure 2: The development exact set match score in Spider vs. the number of training steps.
Figure 3:	Attention visualization on the last self-attention layer.
Figure 4:	Attention visualization on the last self-attention layer.
