Figure 1: Accuracy when training deep (left) and wide (center) ResNets for CIFAR-10 and deepResNets for ImageNet (right) as described in Table 1 when all parameters are trainable (blue) and allparameters except γ and β are trainable (purple). Training with γ and β enabled results in accuracy0.5% to 2% (CIFAR-10) and 2% (ImageNet) higher than with γ and β disabled.
Figure 2: Accuracy of ResNets for CIFAR-10 (top left, deep; top right, wide) and ImageNet (bottomleft, top-1 accuracy; bottom right, top-5 accuracy) with different sets of parameters trainable.
Figure 3: The relationship between BatchNorm parameter count and accuracy when scaling depthand width of CIFAR-10 ResNets.
Figure 4:	The distribution of γ for ResNet-110 and ResNet-101 aggregated from five (CIFAR-10) orthree replicates (ImageNet). Distributions of γ and β for all networks are in Appendix H.
Figure 5:	Fraction of Y parameters for which ∣γ∣ is smaller than various thresholds.
Figure 6:	Accuracy change when clamping Y values with ∣γ∣ below various thresholds to 0.
Figure 7:	The fraction of ReLU activations for which Pr[activation = 0] > 0.99.
Figure 8: Accuracy when training VGG networks. Accuracy does not differ when training with Y andβ disabled.
Figure 9: Accuracy of VGG networks for CIFAR-10 when making certain parameters trainable.
Figure 10: Comparing training only BatchNorm with training all parameters in small ResNets foundvia grid search.
Figure 11:	Accuracy of ResNets for CIFAR-10 (top left, deep; top right, wide) and ImageNet(bottom left, top-1 accuracy; bottom right, top-5 accuracy) with the original BatchNorm initialization(Y 〜U[0,1], β = 0) and an alternate initialization (Y = 1, β = 1).
Figure 12:	Accuracy of ResNets for CIFAR-10 (top left, deep; top right, wide) and ImageNet (bottomleft, top-1 accuracy; bottom right, top-5 accuracy) when making output and shortcut layers trainablein addition to BatchNorm.
Figure 13:	Accuracy of ResNets for CIFAR-10 (left, deep; right, wide) when training only a certainnumber of randomly-selected parameters per convolutional channel. When training two randomparameters per-channel, we are training the same number of parameters as when training onlyBatchNorm.
Figure 14:	Accuracy of ResNets for CIFAR-10 (top left, deep; top right, wide) and ImageNet (bottomleft, top-1 accuracy; bottom right, top-5 accuracy) when making two random parameters per channeland the output layer trainable.
Figure 15:	The distributions of γ for the deep CIFAR-10 ResNets.
Figure 16:	The distributions of β for the deep CIFAR-10 ResNets.
Figure 17: The distributions of γ for the wide CIFAR-10 ResNets.
Figure 18: The distributions of β for the wide CIFAR-10 ResNets.
Figure 19: The distributions of γ for the ImageNet ResNets.
Figure 20: The distributions of β for the ImageNet ResNets.
Figure 21: The distributions of γ for the VGG networks.
Figure 22: The distributions of β for the VGG networks.
Figure 23: Per-ReLU activation freqencies for deep CIFAR-10 ResNets.
Figure 24: Per-ReLU activation freqencies for CIFAR-10 WRNs.
Figure 25: Per-ReLU activation freqencies for ImageNet ResNets.
