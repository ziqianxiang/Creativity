Figure 1: (a) Illustration of semi-supervised object detection, where the model observes a set oflabeled data and a set of unlabeled data in the training stage. (b) Our proposed model can efficientlyleverage the unlabeled data and perform favorably against the existing semi-supervised object de-tection works, including CSD (Jeong et al., 2019) and STAC (Sohn et al., 2020b).
Figure 2: Validation Losses of our model and the model trained with labeled data only. When thelabeled data is insufficient (1% and 5%), RPN and ROIhead classifiers suffer from overfitting, whileRPN and ROIhead regression do not suffer from overfitting. Our model can significantly alleviatesthe overfitting issue in classifiers and also improves the validation box regression loss.
Figure 3: Overview of Unbiased Teacher. Unbiased Teacher consists of two stages. Burn-In: wefirst train the object detector using available labeled data. Teacher-Student Mutual Learning con-sists of two steps. Student Learning: the fixed teacher generates pseudo-labels to train the Student,while Teacher and Student are given weakly and strongly augmented inputs, respectively. TeacherRefinement: the knowledge that the Student learned is then transferred to the slowly progressingTeacher via exponential moving average (EMA) on network weights. When the detector is traineduntil converge in the Burn-In stage, we switch to the Teacher-Student Mutual Learning stage.
Figure 4: Pseudo-label improvement on (a) accuracy, (b) mIoU, and (c) number of bounding boxesin the case of COCO-standard 1% labeled data. We measure the (a) accuracy and (b) mIoU bycomparing the ground-truth boxes and pseudo boxes. The Burn-In limit curves indicate the pseudo-boxes obtained from the model right after the Burn-In stage without further refinement (i.e., themodel trained on labeled data only). GT curve on the number of boxes figure indicates the averagednumber of bounding boxes in the GT labels, and we showed that there are around 7 bounding boxesper image on average in MS-COCO. This result indicates our model can generate more accuratepseudo-labels after the Burn-In stage (i.e., 2k iterations).
Figure 5: Ablation study on the EMA and the Focal loss in the case of COCO-standard 1% labeleddata. (a) mAP of the models using the Focal loss or cross-entropy and applying the EMA or standardtraining. (b) Class empirical distribution (i.e., histogram) of pseudo-labels generated by each modeland compute KL-divergence between the ground-truth labels distribution and the pseudo-label dis-tribution. Among these models, the model using the Focal loss and EMA training (i.e., green curve)achieves the best mAP with the most balanced pseudo-labels .
Figure 6: Ablation study on EMA at different training iterations. Both the models with EMA andwithout EMA have pseudo-label distributions, which are similar to the ground-truth distributionsin the early stage of training iterations. However, the model without EMA tends to generate morebiased pseudo-label distribution later during training.
Figure 7: In the case of COCO-standard 1% labeled data, (a) Unbiased Teacher with Burn-In stageachieve higher mAP againSt UnbiaSed Teacher without Burn-In Stage. USing Burn-In Stage reSultSin the early improvement of (b) boX accuracy and (c) mIoU. (d) UnbiaSed Teacher with Burn-InStage can derive more pSeudo-boXeS than UnbiaSed Teacher without Burn-In Stage.
Figure 8: PSeudo-label accuracy improvement with the uSe of confidence threSholding. We meaSurethe accuracy by comparing the ground-truth labelS and predicted labelS before and after confidencethreSholding. ThiS reSult indicateS that confidence threSholding can Significantly improve the qualityof pSeudo-labelS.
Figure 9: (a) Validation AP and (b) number of pseudo-label bounding boxes per image with variouspseudo-labeling thresholds δ. With an excessively low threshold (e.g., δ = 0.6), the model has alower AP, as it predicts more pseudo-labeled bounding boxes compared to the number of boundingboxes in ground-truth labels. On the other hand, the performance of the model using an excessivelyhigh threshold (e.g., δ = 0.9) drops as it cannot predict sufficient number of bounding boxes in itsgenerated pseudo-labels.
Figure 10: Validation AP on the Teacher model with various MMA rates α. (a) With a small MMArate (e.g., α = 0.5), the Teacher model has lower AP and larger variance. In contrast, as the MMArate grows to 0.99, the Teacher model can gradually improve along the training iterations. However,when the MMA grows to 0.9999, the Teacher model grows overly slow but has lowest variance. (b)We breakdown the AP metric into APs from AP50 to AP95 .
Figure 11: Evaluation metric breakdown of all methods on 0.5% labeled data.
