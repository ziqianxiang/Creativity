Figure 1: How useful are synthetic compared to natural images for interpreting neural networkactivations? A: Human experiment. Given extremely activating reference images (either syntheticor natural), a human participant chooses which out of two query images is also a strongly activatingimage. Synthetic images were generated via feature visualization (Olah et al., 2017). B: Coreresult. Participants are well above chance for synthetic images — but even better when seeingnatural reference images.
Figure 2: Example trial in psychophysical experiments. A participant is shown minimally andmaximally activating reference images for a certain feature map on the sides and is asked to selectthe image from the center that also strongly activates that feature map. The answer is given byclicking on the number according to the participant’s confidence level (1: not confident, 2: somewhatconfident, 3: very confident). After each trial, the participant receives feedback which image wasindeed the maximally activating one. For screenshots of each step in the task, see Appendix Fig. 7.
Figure 3: Participants are better, more confident and faster at judging which of two query imagescauses higher feature map activation with natural than with synthetic reference images. A: Perfor-mance. Given synthetic reference images, participants are well above chance (proportion correct:82 ± 4%), but even better for natural reference images (92 ± 2%). Without reference images (base-line comparison “None”), participants are close to chance. B: Confidence. Participants are muchmore confident (higher rating = more confident) for natural than for synthetic images on correctlyanswered trials (χ2, p < .001). C: Reaction time. For correctly answered trials, participants areon average faster when presented with natural than with synthetic reference images. We show addi-tional plots on confidence and reaction time for incorrectly answered trials and all trials in the Ap-pendix (Fig. 16); for Experiment II, see Fig. 17.). The p-values in A and C correspond to Wilcoxonsigned-rank tests.
Figure 4: Performance is high across (A) a broad range of layers and (B) all branches of the Inceptionmodules. The latter differ in their kernel sizes (1 × 1, 3 × 3, 5 × 5, pool). Again, natural imagesare (mostly) more helpful than synthetic images. Additional plots for the none condition as well asExperiment II can be found in the Appendix in respectively Fig. 18 and Fig. 19.
Figure 5: We found no evidence for large effects of expert level or feature map selection. However,performance does improve with additional information. A: Expert level. Both experts and layparticipants perform equally well (RM ANOVA, p = .44), and consistently better on natural than onsynthetic images. B: Selection mode. There is no significant performance difference between hand-picked feature maps selected for interpretability and randomly selected ones (Wilcoxon test, p = .18for synthetic and p = .59 for natural reference images). C: Presentation scheme. Presenting bothmaximally and minimally activating images simultaneously (Min+Max) and presenting nine insteadof one single reference image tend to improve performance, especially for natural reference images.
Figure 6: The subjective intuitiveness of feature visualizations varies greatly (see A for the ratingsfrom the beginning of Experiment I and B for the ratings at the beginning and end of Experiment II).
Figure 7: Forward Simulation Task. The progress bar at the bottom of the screen indicates theprogress within one block of trials.
Figure 8:	Detailed structure of the two experiments with different foci. A: Experiment I. Here,the focus is on comparing performance of synthetic and natural reference images to the most simplebaseline: no reference images (“None”). To counter-balance conditions, the order of natural andsynthetic blocks is alternated across participants. For each of the three reference image types (syn-thetic, natural and none), 45 relevant trials are used plus additional catch, practice and repeated trials.
Figure 9:	Sampling of natural images. A: Distribution of activations. For an example channel(mixed3a, kernel size 1 × 1, feature map 25), the smoothed distribution of activations for all 50, 000ImageNet validation images is plotted. The natural stimuli for the experiment are taken from thetails of the distribution (shaded background). B: Zoomed-in tail of activations distribution. In thepresentation schemes with 9 images, 10 bins with 20 images each are created (10 because of 9reference plus 1 query image). C: In order to obtain 20 batches with 10 images each, the 20 imagesfrom one bin are randomly distributed to the 20 batches. This guarantees that each batch contains afair selection of extremely activating images. The query images are always sampled from the mostextreme bins in order to give the best signal possible. In the case of the presentation schemes with 1reference image, the number of bins in B is reduced to 2 and the number of images per batch in C isalso reduced to 2.
Figure 10: Mean activations and standard deviations (not two standard errors of the mean!) ofthe minimally (below 0) and maximally (above 0) activating synthetic and natural images used inExperiment I. Note that there are 10 (i.e. accidentally not 9) synthetic images and 20 ∙ 10 = 200natural images (because of 20 batches) in Experiment I for both minimally and maximally activatingimages. Please also note that the standard deviations for the selected natural images are invisiblebecause they are so small. Limited synthetic images refer to feature visualizations which are theresult of stopping the optimization process early with the goal of matching the activation level ofnatural stimuli.
Figure 12: Experiment I: Example trials of the three reference images conditions: synthetic refer-ence images (first row), natural reference images (second row) or no reference images (third row).
Figure 13: Experiment II: Example trials of the four presentation schemes: Max 1, Min+max 1,Max 9, Min+Max 9. The left column contains synthetic reference images, the right column containsnatural reference images.
Figure 14: Trials for intuitiveness judgment. The tested feature maps are from layer mixed3a (chan-nel 43), mixed4b (channel 504) and mixed 5b (channel 17). They are the same in Experiment I andin Experiment II.
Figure 15: Catch trials. An image from the reference images is copied as a query image, whichmakes the answer obvious. The purpose of these trials is to integrate a mechanism into the experi-ment which allows us to check post-hoc whether a participant was still paying attention.
Figure 16: Task performance (a), distribution of confidence ratings (b-d) and reaction times (e-g)of Experiment I. The p-values are calculated with Wilcoxon sign-rank tests. Note that unlike in themain paper, these figures consistently include the “None” condition. For explanations, see Sec. 4.1.
Figure 17: Task performance (a), distribution of confidence ratings (b-d) and reaction times (e-g)of Experiment II, averaged over expert level and presentation schemes. The p-values are calculatedwith Wilcoxon sign-rank tests. The results replicate our findings of Experiment I. For explanationson the latter, see Sec. 4.1.
Figure 18: High performance across (a) layers and (b) branches of the Inception modules in Exper-iment I. Note that unlike in the main paper these figures consistently include the “None” condition.
Figure 19:	High performance across (a) layers and (b) branches of the Inception modules in Ex-periment II. Note that only every second layer is tested here (unlike in Experiment I). The resultsreplicate our findings of Experiment I. For explanations, see Sec. 4.230Published as a conference paper at ICLR 2021A.2.3 Details on performance of experts split by different levels of expertiseEven though Experiment II does not show a significant performance difference for lay and expertparticipants, it is an open question whether the level of expertise or the background of experts mat-ters. For the data from experts, we hence further divide participants into subgroups according to theirexpertise (see Fig. 20a-f) and background level (see Fig. 20g-h). Expertise level 1 means that partici-pants are familiar with CNNs, but not feature visualizations; expertise level 2 means that participantshave heard of or read about feature visualizations; and expertise level 3 means that participants haveused feature visualizations themselves. We note that we also accepted feature visualizations meth-ods other than the one by Olah et al. (2017), e.g. DeepDream (Mordvintsev et al., 2015) for level2 and 3. Regarding background, we distinguished computational neuroscientists from researchersworking on computer vision and / or machine learning. We note that some subgroups only hold oneparticipant and hence may not be representative.
Figure 20:	Performance of experts split by different levels of expertise: The first (second) rowshows the data of Experiment I (II) split up by different levels of familiarity with CNNs and featurevisualizations. The third row shows the data of Experiment I split up by different backgrounds.
Figure 21:	Repeated trials in Experiment I.
Figure 23: An easy feature map (here: 5a, pool*) from Experiment I where all participants an-swered correctly for both synthetic and natural reference images. The shown stimuli were shown toparticipant 1, for stimuli shown to participant 2 and 3, see Supplementary Material Fig 1.
Figure 25: A feature map (here: 4a, Pool) from Experiment I where the feature is small (eyes)and a participant might perceive conflicting information (eyes and extremity-like structure in minreference images vs. eyes and earth-colors in max reference images). In this specific example, eight(nine) out of ten participants gave the correct answer for this feature map given synthetic (natural)reference images. The displayed stimuli were shown to participant 1, for stimuli shown to participant2 and 3, see Supplementary Material Fig. 10.
