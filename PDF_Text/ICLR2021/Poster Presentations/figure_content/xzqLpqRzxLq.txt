Figure 1: Schematic of our approach to FSL. Given a training episode, We apply 2D rotationsby 0, 90, 180, and 270 degrees to each instance to generate four extended episodes. After goingthrough a feature extraction CNN, four losses over three branches are designed: (1) In the top branch,We employ a self-supervised rotation classifier With the instance-level SSL loss Linst . (2) In themiddle branch, an FSL classifier is exploited to predict the FSL classification probabilities for eachepisode. We maximize the classification consistency among the extended episodes by forcing the fourprobability distributions to be consistent using Lepis . The average supervised FSL loss Laux is alsocomputed. (3) In the bottom branch, We utilize an integration transformer module to fuse the featuresextracted from each instance With different rotation transformations; they are then used to computean integrated FSL loss Linteg . Among the four losses, Linst and Lepis are the self-supervised losses,and Laux and Linteg are the supervised losses.
Figure 2: (a) Comparison among different combination methods over episodes for FSL with self-supervision. (b) Illustration of the effect of different choices of R on the performance of our model(R denotes the number of extended episodes used for SSL). (c) Comparative results obtained by ourIEPT using different basic FSL classifiers (i.e. ProtoNet, FEAT, and IMP). It can be seen clearly thatintegrated episode-based fusion leads to more separation between classes. All figures present 5-way1-shot/5-shot results on miniImageNet, using Conv4-64 as the feature extractor.
Figure 3: Illustration of the self-supervised strategy by shuffling image patches.
Figure 4: Feature visualizations of a number of test episodes using the UMAP algorithm (McInneset al., 2018). Each row indicates a group of test extended episodes (the first four columns, rotationby 0°, 90°, 180°, 270。)and their integrated episode (the last column). The 5-way 5-shot FSL (withConv4-64) is adopted on miniImageNet.
Figure 5: Feature and attention visualization of two test episodes (left and right). Both figures present5-way 5-shot results on miniImageNet, using Conv4-64 as the feature extractor.
Figure 6: Visualization of our hyper-parameter analysis under 5-way 1-shot (left) and 5-shot (right)on miniImageNet. Conv4-64 is used as the feature extractor.
