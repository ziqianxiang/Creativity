Figure 1: We propose to use humanâ€™s interactions with their visual surrounding as a training signalfor representation learning. We record first person observations as well as the movements and gazeof people living their daily routines and use these cues to learn a visual embedding. We use thelearned representation on a variety of diverse tasks and show consistent improvements compared tostate-of-the-art self-supervised vision-only techniques.
Figure 2: Dataset examples. Two sequences from our dataset are shown on the left. The first rowshows the sequence of the images and the second row shows the movements of the body parts ac-cording to the IMU readings. We visualize the gaze using the red circle. This is just for visualizationpurposes and does not exist in the image. On the right, we show the data collection setup.
Figure 3: Model Overview. We learn a muscly-supervisedrepresentation by jointly optimizingvisual, movement and center of focus (gaze) objectives. The portion outlined with a rectangle is thebackbone that is used to evaluate the representation for target tasks. All parts of the network areinitialized randomly and trained from scratch.
Figure 4: Dataset Statistics. Left: The approximate distribution of top 20 scene classes accordingto a scene classifier trained on the Places (Zhou et al., 2017) dataset. We show how often each scenecategory is predicted as top-1. Middle: The distribution of gaze across the dataset. Right: Theaverage magnitude of the change in the orientation of body parts between consecutive frames.
