Figure 1: Summary of P-DRO: At every step of training, (x, y) pairs are sampled from the datadistribution p and fed to both the model θ and the adversary ψ. For every sample, the model producesloss values '(χ, y; θ) and the adversary produces densities qψ (x, y). Both are combined into Lmodeiand Ladv, which are used to update the θ and ψ respectively, via simultaneous gradient updates.
Figure 2: A toy classifi-cation task.
Figure 3: Evolution of the robust validation accuracy of the model selected by Greedy-Minmax asa function of the KL threshold κvalidIn all experiments, we try 4 different values for κ: 0.01, 0.1, 1 and 10. Unless indicated otherwise,we perform early stopping and hyper-parameter selection using our Minmax criterion using thenon-parametric weights as adversaries on the validation data.
