Figure 1: Vector fields for continuous-time neu-ral networks. Integral curves with arrows showthe trajectories of data points under the influenceof the network. Top left: Standard NODEs learn asingle time-independent flow (in black) that mustaccount for the whole data space. Top right: N-CODE learns a family of vector fields (red vs yel-low vs blue), enabling the system to flexibly ad-just the trajectory for every data point. Bottomleft: Trained NODE trajectories of initial valuesin a data set of concentric annuli, colored greenand yellow. The NODE transformation is a home-omorphism on the data space and cannot separatethe classes as a result. Colored points are the ini-tial state of the dynamics, and black points are thefinal state. Bottom right: Corresponding flowsfor N-CODE which easily separate the classes.
Figure 2: Diagram of a general N-CODE module:Given a initial state x(0), the module consists of anaugmented dynamical system that couples activity statex and weights θ over time (red arrows). A mapping γinfers initial control weights θ0 defining an initial flow(open-loop control). This flow can potentially evolve intime as θ might be driven by a feedback signal from x(closed-loop, dotted line). This meta-parameterizationof f can be trained with gradient descent by solving anaugmented adjoint sensitivity system (blue arrows).
Figure 3: Example of dynamical control augmenta-tion: The Van der Pol oscillator: As a second orderdifferential equation, the dynamics cannot be approx-imated by a single 1-dimensional NODE with a con-stant control θ (degenerate solution in red). However, ifthe dynamics are decomposed into a planar system witha dynamic control variable θ(t), then the parameter μcan be adapted to fit a particular oscillatory regime (inblack). This is a particular form of augmentation dis-cussed in (Dupont et al. (2019); Norcliffe et al. (2020))that showcases the benefit of using additional variablesas evolving parameters of the dynamical system.
Figure 4: Trajectories over time for three types of continuous-time neural networks learning the 1-dimensionalreflection map φ(x) = -x. Left: (NODE) Irrespective of the form of f, a NODE module cannot learn SUCha function as trajectories cannot intersect (Dupont et al., 2019). Middle: (Open-loop N-CODE). The modelis able to learn a family of vector fields by controlling a single parameter of f conditioned on x(0). Right:(Closed-loop N-CODE) With a fixed initialization, x(0), the controller model still learns a deformation of thevector field to learn φ. Note that the vector field is time-varying in this case, contrary to the two others versions.
Figure 5: The concentric annuli problem. A classifier must separate two classes in R2, a central disk and anannulus that encircles it. Left: Soft decision boundaries (blue to white) for NODE (First), N-CODE open-loop(Second) and closed-loop (First) models. Right: Training curve for the three models.
Figure 6: Left: Diagram of an episode of the few-shot memorization task Miconi et al. (2018). Three, 5-length-1, 1-valued bit patterns (n = 5 here for visualization) are presented to the system at regular intervals. At querytime, tq, a degraded version of one of the patterns is presented and the task of the model is to complete thispattern. Right: Performance of a closed-loop N-CODE, an LSTM, a continuous-time ODE-RNN Rubanovaet al. (2019) and the data-conditioned NODE of Massaroli et al. (2020b) averaged over 3 runs. All modelsbesides N-CODE plateau at high values (chance is .25). N-CODE, on the other hand, learns immediately andwith extremely high accuracy across all n. (see Appendix)have suppressed t for concision. We ran this experiment on the closed-loop version of N-CODEas well as an LSTM, continuous-time ODE-RNN (Rubanova et al., 2019) and the data-controlledmodel of Massaroli et al. (2020b). All systems had the same number of learnable parameters, exceptfor the LSTM which was much larger. We found that all considered models, except closed-loop N-CODE, struggled to learn this task (Fig. 6). Note that chance is .25 since half the bits are degraded.
Figure 7: Left to right: Temporal evolutions of an image reconstruction along controlled orbits of AutoN-CODE for representative CIFAR-10 and CelabA test images. Last column: ground truth image.
Figure 8: Left Reconstructions of random test images from CelebA for different autoencodingmodels. Right: Random samples from the latent space. Deterministic models are fit with 100-components gaussian mixture model.
