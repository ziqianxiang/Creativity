Figure 1: Emergence of the block structure with increasing width or depth. As we increase the depth orwidth of neural networks, we see the emergence of a large, contiguous set of layers with very similar represen-tations — the block structure. Each of the panes of the figure computes the CKA similarity between all pairs oflayers in a single neural network and plots this as a heatmap, with x and y axes indexing layers. See AppendixFigure C.1 for block structure in wide networks without residual connections.
Figure 2: Block structure emerges in narrower networks when trained on less data. We plot CKA Simi-larity heatmaps as we increase network width (going right along each row) and also decrease the dataset size(down each column). AS a reSult of the increaSed model capacity (with reSpect to the taSk) from Smaller dataSetSize, Smaller (narrower) modelS now alSo exhibit the block Structure.
Figure 3: Block structure arises from preserving and propagating the (dominant) first principal compo-nent of the layer representations. Above are two sets of four plots, for layers of a deep network (left) anda wide network (right). CKA of the representations (top right), shows block structure in both networks. Bycomparing this to the variance explained by the top principal component of each layer representation (bottomleft), we see that layers in the block structure have a highly dominant first principal component. This principalcomponent is also preserved throughout the block structure, seen by comparing the squared cosine similarity ofthe first principal component across pairs of layers (top left), to the CKA representation similarity (top right).
Figure 4: Linear probe accu-racy. Top: CKA between lay-ers of individual ResNet mod-els, for different architecturesand initializations. Bottom:Accuracy of linear probes foreach of the layers before (or-ange) and after (blue) the resid-ual connections.
Figure 5: Effect of deletingblocks on accuracy for mod-els with and without blockstructure. Blue lines showthe effect of deleting blocksbackwards one-by-one withineach ResNet stage. (Note theplateau at the block structure.)Vertical green lines reflectboundaries between ResNetstages. Horizontal gray linereflects accuracy of the fullmodel.
Figure 6: Representations within “block structure” differ across initializations. Each group of plots showsCKA between layers of models with the same architecture but different initializations (off the diagonal) orwithin a single model (on the diagonal). For narrow, shallow models such as ResNet-38 (1×), there is no blockstructure, and CKA across initializations closely resembles CKA within a single model. For wider (middle)and deeper (right) models, representations within the block structure are unique to each model.
Figure 7: Systematic per-example and per-class performance differences between wide and deep models.
