Figure 1: Functional sample information of samples in the iCassava classification task with 1000samples, where the network is a pretrained ResNet-18. A: histogram of sample informations, B: 10least informative samples, C: 10 most informative samples.
Figure 2: Applications of functional sample information. (a) Different sources of data for the sametask (digit classification) can have a vastly different amount of information. (b) As expected, sampleswith the wrong labels carry more unique information. (c) Test accuracy as a function of the ratio ofremoved training examples using different strategies.
Figure 3: A toy dataset and key distributions involved in upper bounding the unique sample infor-mation with leave-one-out KL divergence.
Figure 4: Functional sample information of samples in MNIST 4 vs 9 classification task (top) andDogs vs. Cats (bottom), with A: histogram of sample informations, B: 10 least informative samples,C: 10 most informative samples.
Figure 5: Correlations between functional sample information scores computed for different archi-tectures and training lengths. On the left: correlations between F-SI scores of the 4 pretrainednetworks, all computed with setting t = 1000 and η = 0.001. On the right: correlations betweenF-SI scores computed for pretrained ResNet-18s, with learning rate η = 0.001, but varying traininglengths t. All reported correlations are averages over 10 different runs. The training dataset consistsof 1000 examples from the Kaggle cats vs dogs classification task.
Figure 6: Top 10 most informative examples from Kaggle cats vs dogs classification task for threepretrained networks: ResNet-18, ResNet-50, and DenseNet-121.
Figure 7: Testing how much F-SI scores computed for different networks are qualitatively differ-ent. On the left: the MNIST vs SVHN experiment with a pretrained DesneNet-121 instead of apretrained ResNet-18. On the right: Data summarization for the MNIST 4 vs 9 classification task,where the F-SI scores are computed for a one-hidden-layer network, but a two-hidden-layer networkis trained to produce the test accuracies.
Figure 8: Additional results comparing the information content of samples with correct and incorrectlabels on MNIST 4 vs 9 and Dogs vs Cats.
Figure 9: Histogram of the functional sample information of samples from the three subclasses ofthe Pets vs Deer. Since the sub-class “cat” is under-represented in the dataset, cat images tend tohave on average more unique information than dog images, even if they belong to the same class.
Figure 10: Histogram of the functional sample information of samples of the Kaggle cats vs dogsclassification task, where 10% of examples are adversarial examples.
