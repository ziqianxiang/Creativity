Figure 1: Images from the simulated dataset (left)and from the real-world setup (right).
Figure 2: Latent traversals of a trained model that perfectly disentangles the dataset’s FoVs. In eachcolumn, all latent variables but one are fixed.
Figure 3: Left: Disentanglement metrics aggregating all hyperparameters except for supervisiontype. Right: Rank correlations (Spearman) of ELBO, reconstruction loss, and the test error of aGBT classifier trained on 10,000 labelled data points with disentanglement metrics. The upper rankcorrelations correspond to the unsupervised models and the lower to the weakly supervised models.
Figure 4:	Higher disentanglement corresponds to better generalization across all OOD1 scenarios, asseen from the transfer scores (left). The transfer score is computed as the mean absolute predictionerror of ground truth factor values (lower is better). This correlation is particularly evident in theGBT case, whereas MLPs appear to exhibit better OOD1 transfer with very high disentanglementonly. These results are mirrored in the Spearman rank correlations between transfer scores anddisentanglement metrics (right).
Figure 5:	Disentanglement affects generalization across the OOD2 scenarios only minimally as seenfrom transfer scores (left) and corresponding rank correlations with disentanglement metrics (right).
Figure 6: Noise improves generalization across the OOD2 scenarios and less so for the OOD1scenarios as seen from the transfer scores. Top row: Spearman rank correlation coefficients betweentransfer metrics and presence of noise in the input.
Figure 7: Zero-shot trans-fer to real-world observa-tions of our models trainedin simulation. Left: input;right: reconstruction.
Figure 8: Schemes of the encoder (top) and decoder (bottom) architectures. In both schemes, in-formation flows left to right. Blue blocks represent convolutional layers: those labeled “conv” have5x5 kernels and stride 2, while those labeled “1x1” have 1x1 kernels. Each orange block representsa pair of residual blocks (implementation details of a residual block are provided in Table 3). Greenblocks in the encoder represent average pooling with stride 2, and those in the decoder denote bi-linear upsampling by a factor of 2. Red blocks represent fully-connected layers. The block labeled“norm” indicates layer normalization. Dashed lines denote tensor reshaping.
Figure 9: Feasible states of the 2nd and 3rd DoF when the angle of the 1st DoF is 0. Angles are inradians.
Figure 10: Density of feasible states of 2nd and 3rd DoF over the whole training dataset. Darkershades of blue indicate regions of higher density. Angles are in radians.
Figure 11: Samples generated by a trained model. This model was selected based on the ELBO.
Figure 12: Input reconstructions by a trained model. This model was selected based on the ELBO.
Figure 13: Latent traversals for a model with low DCI score (0.15) in (a), medium DCI score (0.5)in (b), and high DCI score (1.0) in (c).
Figure 14: Scatter plots of unsupervised metrics (left: ELBO; right: reconstruction loss) vs disentan-glement (top: MIG; bottom: DCI) for 1,080 trained models, color-coded according to supervision.
Figure 15: Transfer metric in OOD2-A (top) and OOD2-B (bottom) settings, decomposed accordingto the factor of variation and presence of input noise. When noise is added to the input duringtraining, the inferred cube position error is relatively low (the scores are the mean absolute error,and they are normalized to [0, 1]). This is particularly useful in the OOD2-B setting (real world)where the joint state is anyway considered known, while object position has to be inferred withtracking methods.
Figure 16: Reconstructions of real-world images (OOD2-B) for a model with low DCI score (0.15)in (a), medium DCI score (0.5) in (b), and high DCI score (1.0) in (c).
Figure 17: Reconstructions of simulated images with encoder out-of-distribution colors (OOD2-A)for a model with low DCI score (0.15) in (a), medium DCI score (0.5) in (b), and high DCI score(1.0) in (c).
