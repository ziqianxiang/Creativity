Figure 1: We partition datasets into four sections, defined by the features (spurious and/or target)that hold. We sample training datasets D, which provide varying amounts of evidence against thespurious feature, in the form of s-only examples. In the illustration above, the s-only rate is 10 =0.2, i.e., 20% of examples in D provide evidence that S alone should not be used to predict y.
Figure 2: Results on Synthetic Data. Error on each partition of the test set, as a function of s-onlyrate. A model that has learned to use the target feature alone to predict the label will achieve zeroerror across all partitions. s-only and t-only error reach 0 quickly when t is as easy to extract as s(i.e., the relative extractability is 1). However, when t is harder to extract than s (rel. extractability< 1), performance lags until evidence from s-only examples is quite strong.
Figure 3: Relative Extractability Correlates with Target Feature Use. In (a) we show the Spear-man’s ρ between the test F-Score vs measures of extractability of the (s, t) pairs; * indicates signifi-cance. Relative extractability, whether ratio (MDL(s)/MDL(t)) or difference (MDL(s) - MDL(t)),explains learning behavior better than absolute extractability of either feature.
Figure 4: Learning Curves for BERT & T5. Curves show use of spurious feature (s-only accuracy)as a function of training evidence (s-only rate). Each line represents one (s, t) pair (described in§4.1). Pairs vary in the relative extractability of t vs. s (measured by the ratio MDL(s)/MDL(t)and summarized in the bar chart). When t is much harder to extract relative to s (lower ratios),the classifier requires much more statistical evidence during training (higher s-only rate) in order toachieve low error. We find similar patterns GPT2 and RoBERTa; see Appendix A for all the results.
Figure 6: T5.
Figure 7: BERT.
Figure 8: GloVe.
Figure 9: GPT2.
Figure 10: RoBERTa.
Figure 11: Overfitting on the Synthetic Tasks.
