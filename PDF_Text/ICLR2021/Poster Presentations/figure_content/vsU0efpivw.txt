Figure 1: Shapley representations span beyond one-dimensional manifolds and depend on both theinner function and the reference values. In both groups, the gray-scale background indicates therespective function value while the rainbow-scale color indicates correspondence between input (left)and Shapley representation (right) along with function values—red means highest and purple meanslowest function values. The red cross represents the reference values. More details in subsection 2.1.
Figure 2: An example of how we construct ShapNet from simple Shapley modules (left) whichexplicitly compute the SHAP explanation values for the arbitrary function fj . Shallow SHAPNETs(middle) are based on computing many Shapley modules in parallel—in particular, we show comput-ing all pairs of features—and then applying a summation to aggregate the modules (where implicitzeros are not computed). Deep ShapNet (right) are composed of Shapley transform blocks wherethe output explanation of the previous layer is used as input for the next layer. We show an exampleusing disjoint subsets for each Shapley module within a Shapley transform and then use a butterflypermutation (as in the Fast Fourier transform (FFT) algorithm) to enable complex dependencies asdiscussed in subsection 2.3. The edge weights shown here are direct results of Eqn. 1 with d = 2.
Figure 3: A visualization of Corollary 4 and pruning in action: Notice 1) that once a pixel get toclose to reference values at an earlier stage, it will stay that way for the rest of the computationalgraph and 2) `1 regularization promotes the sparsity and hence decreases the amount of computationrequired. The first row is retrieved from regularly trained model, the second row from model with `1regularization on the last layer discussed in subsection 2.4, and the last row from the model with `1regularization applied on all inter-layer representations.
Figure 4: Our intrinsic Deep SHAPNET explanations perform better than post-hoc explanations inidentifying the features that can flip the model prediction or that contribute most to the prediction asin figures on the left showing the results of flipping digits as introduced in Shrikumar et al. (2017)and right showing the remaining activation after removing the top k features identified by eachexplanation method. While Deep ShapNet explanations did not perform the best in the middlewhere we show the results after removing least k-salient features as introduced in Srinivas & Fleuret(2019), our model still scores the second. All results are measured on MNIST test set. More resultsfor digit flipping, in Fig. 11, show the same conclusion with statistical significance in Table 7.
Figure 5:1) Pruning values from earlier layers have at least equal or stronger effects on the perfor-mance than from later layers due to Corollary 4; 2) `1 regularized model performs better especiallywhen the regularization is applied on all all the layers instead of just the output; 3) Most of the valuescan be removed and retain the accuracy. We prune the values in the order from the least salient (inmagnitude of '1 norm) to the most pixel by pixel. This is measured on the MNIST test set.
Figure 6: MNIST ShapNet explanations for different regularizations qualitatively demonstrate theeffects of regularization. We notice that `1 only puts importance on a few key features of the digitswhile '∞ spreads out the contribution over more of the image. Red and blue correspond to positiveand negative contribution respectively. Details in subsection I.4.
Figure 7: Both '1 and '∞ regularizations often introduce some robustness to the model againstfeature removal (the dotted and dashed lines are often above the solid lines). Three different featureorders are used: 1) most positive to most negative (top-down), 2) most negative to most positive(reversed), and 3) randomly chosen as a baseline. `1 regularization puts most importance on the first100 features with relatively low importance to other features (seen in both top-down and reversed).
Figure 8: The explanations produced by our model trained without explanation regularization for all10 classes. From left to right are class index (also corresponding digits) from 0 to 9.
Figure 9: `1 regularization promotes sparsity in feature importance, which, by comparison with Fig. 8and Fig. 10, one can easily argue has been successfully achieved. This shows that `1 regularization,combined with cross entropy loss, does (almost) remove negative contribution and only focuses oneach digit’s distinguishing features.
Figure 10: With '∞ regularization, the contribution among pixels are more laid out and less ConCen-trated than they would be other wise as shown in Fig. 8. The explanations produced by our modeltrained with '∞ explanation regularization for all 10 classes. However, it is still easy to notice whichpart of the images Contribute positively or negatively for a partiCular Class.
Figure 11: Digit flipping results: Deep ShapNet explanations perform well against post-hocmethods.
