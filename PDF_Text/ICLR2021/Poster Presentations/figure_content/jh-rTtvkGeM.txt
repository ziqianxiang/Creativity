Figure 1: Gradient descent typically occurs at the Edge of Stability. On three separate architec-tures, we run gradient descent at a range of step sizes η, and plot both the train loss (top row) and thesharpness (bottom row). For each step size n, observe that the sharpness rises to 2/n (marked by thehorizontal dashed line of the appropriate color) and then hovers right at, or just above, this value.
Figure 2: Gradient descent on a quadraticwith eigenvalues ai = 20 and a2 = 1.
Figure 3: So long as the sharpness is less than 2∕η, it tends to continually increase duringgradient descent. We train a network to completion (99% accuracy) using gradient descent with avery small step size. We consider both MSE loss (left) and cross-entropy loss (right).
Figure 4: Once the sharpness crosses 2/n, gradient descent becomes destabilized. We run gradi-ent descent at n = 0.01. (a) The sharpness eventually reaches 2/n. (b) Once the sharpness crosses2/n, the iterates start to oscillate along q1 with ever-increasing magnitude. (c) Somehow, GD doesnot diverge entirely; instead, the train loss continues to decrease, albeit non-monotonically.
Figure 5: After the sharpness reaches 2∕η, gradient descent enters the Edge of Stability. Anetwork is trained with gradient descent at a range of step sizes (see legend), using both MSE loss(top row) and cross-entropy (bottom row). Left: the train loss curves, with a vertical dotted line atthe iteration where the sharpness first crosses 2∕η. Center: the sharpness, with a horizontal dashedline at the value 2∕η. Right: sharpness plotted by time (= iteration X η) rather than iteration.
Figure 6: Momentum. We run GDwith step size η = 0.01 and Polyak orNesterov momentum at various β . Foreach algorithm, the horizontal dashedline marks the MSS from Equation 1.
Figure 7: After a learning rate drop, progressive sharp-ening resumes. We start training at η = 2/200 (orange)and then after 6000 iterations (dotted vertical black line),we cut the step size to η = 2/300 (green). Observe that assoon as the step size is cut, the sharpness starts to rise.
Figure 8: With batch norm and MSE loss, the sharpness sometimes drops precipitously: We usegradient flow to train a CNN with ReLU activations and batch norm using the MSE loss. Observethat the sharpness drops substantially at the beginning of training, and never recovers to its initialvalue. This seems to occur due to the combination of both batch normalization and MSE loss.
Figure 9: Left: logistic loss function ` as a function of yz. Right: its second derivative `00 as afunction of yz .
Figure 10: We train a network using the logistic loss on the binary classification problem of CIFAR-10 airplane vs. automobile. (a) The train loss. (b) The train accuracy. (c) The leading eigenvalueof the Hessian and of the Gauss-Newton matrix; observe that the latter is a great approximation tothe former. (d) The leading eigenvalue of the matrix ɪ P1=1 Vθh(x%; θ)Vθh(x%; θ)T, which is theGauss-Newton matrix without the `00 terms; observe that this value constantly rises — it does notdip at the end of training. (e) The margin yizi of 10 examples; observe that all the margins rise atthe end of training. (f) the value '0(zi; y%) for 10 examples; observe that all of these curves declineat the end of training.
Figure 11: We train a network using the cross-entropy loss on CIFAR-10. (a) The train loss. (b) Thetrain accuracy. (c) The leading eigenvalue of the Hessian and of the Gauss-Newton matrix; observethat the latter is a great approximation to the former. (d) The leading eigenvalue of the matrix* pn=1 JTJi, which is the GaUss-NeWton matrix except the V2' terms; observe that this valueconstantly rises — it does not dip at the end of training. (e) The margin zi[yi] - maxj6=yi zi[j] of 10examples; observe that all these margins rise at the end of training. (f) the value pi [yi](1 - pi [yi])for 10 examples; observe that all of these curves decline at the end of training.
Figure 12: NTK parameterization: evolution of the sharpness. We use gradient flow to trainNTK-parameterized networks, and we track the evolution of the sharpness during training. Foreach width, we train from five different random initializations (different colors). Observe that thesharpness rises more when training narrow networks than when training wide networks.
Figure 13: NTK parameterization: summary statistics. Left: For each network width, we plotthe mean and standard deviation (over the five different random initializations) of the maximumsharpness λmax along the gradient flow trajectory. Observe that λmax decreases in expectation asthe width increases. Right: For each network width, we plot the mean and standard deviation (overthe five different random initializations) of the maximum sharpness gain λmaχ∕λo along the gradientflow trajectory. Observe that λmaχ∕λo decreases in expectation as the width increases. Indeed, NTKtheory predicts that λmaχ∕λ0 should deterministically tend to 1 as width → ∞ and this plot isconsistent with that prediction.
Figure 14: Standard parameterization: evolution of the sharpness. We use gradient flow to trainstandard-parameterized networks, and we track the evolution of the sharpness during training. Foreach width, we train from five different random initializations (different colors). Observe that thesharpness rises more when training narrow networks than when training wide networks.
Figure 15:	Standard parameterization: summary statistics. Left: For each network width, weplot the mean and standard deviation (over the five different random initializations) of the maximumsharpness λmax along the gradient flow trajectory. Observe that λmax tends to decrease in expec-tation as the width increases, though it is not clear whether this pattern still holds when movingfrom width 512 to width 1024 — more samples are needed. Right: For each network width, weplot the mean and standard deviation (over the five different random initializations) of the maximumsharpness gain λmaχ∕λo along the gradient flow trajectory. Observe that λmaχ∕λo decreases in ex-pectation as the width increases. It is not clear whether or not λmaχ∕λ0 is deterministically tendingto 1, but that does seem possible.
Figure 16:	The effect of depth: cross-entropy. We use gradient flow to train networks of var-ious depths, ranging from 1 hidden layer to 4 hidden layers, using cross-entropy loss. We traineach network from five different random initializations (different colors). Observe that progressivesharpening occurs to a greater degree for deeper networks.
Figure 17:	The effect of depth: mean squared error. We use gradient flow to train networks of var-ious depths, ranging from 1 hidden layer to 4 hidden layers, using MSE loss. We train each networkfrom five different random initializations (different colors). Observe that progressive sharpeningoccurs to a greater degree for deeper networks.
Figure 18: The effect of dataset size. We use gradient flow to train a network on varying-sizedsubsets of CIFAR-10. Observe that progressive sharpening occurs to a greater degree as the datasetsize increases.
Figure 19: We train a neural network using cross-entropy loss (left) and MSE loss (right). In Figure20 and 21, we show what happens when, at the iterations marked above by vertical dotted lines,we switch from gradient descent on the real neural training objective to gradient descent on thequadratic Taylor approximation.
Figure 20: Cross-entropy loss (η = 2/60). At six different iterations during the training of thenetwork from Figure 19 (marked by the vertical dotted black lines), we switch from running gradientdescent on the real neural training objective (for which the train loss is plotted in blue) to runninggradient descent on the quadratic Taylor approximation around the current iterate (for which thetrain loss is plotted in orange). Top row are timesteps {200, 400, 600} before gradient descent hasentered the Edge of Stability; observe that the orange line (Taylor approximation) closely tracks theblue line (real objective). Bottom row are timesteps {800, 1000, 1200} during the Edge of Stability;observe that the orange line quickly diverges, whereas the blue line does not.
Figure 21: MSE loss (η = 2/200). At six different iterations during the training of the networkfrom Figure 19 (marked by the vertical dotted black lines), we switch from running gradient descenton the real neural training objective (for which the train loss is plotted in blue) to running gradientdescent on the quadratic Taylor approximation around the current iterate (for which the train loss isplotted in orange). Top row are timesteps (2000, 3000, 4000) before gradient descent has enteredthe Edge of Stability; observe that the orange line (Taylor approximation) closely tracks the blueline (real objective). Bottom row are timesteps (5000, 6000, 7000) during the Edge of Stability;observe that the orange line quickly diverges, whereas the blue line does not.
Figure 22:	A dynamic step size ηt = 1∕λt underperforms a fixed step size of ηt = 1∕λ0.
Figure 23:	A dynamic step size n = 1.9∕λt underperforms a fixed step size of n = 1.9∕λo.
Figure 24: Evolution of sharpness during SGD. We train a network using SGD at various batchsizes, and plot the evolution of the sharpness. In the top row, we train with cross-entropy loss andstep size η = 0.02; in the bottom row, we train with mean squared error and step size η = 0.01.
Figure 25: SGD does not consistently decrease the training loss in expectation. We train the tanhnetwork from section 3 with MSE loss, using SGD with step size 0.01 and batch size 32. Periodicallyduring training, we compute (left) the full-batch training loss, and (right) the expected change in thefull-batch training loss that would result from taking an SGD step (where the expectation is over therandomness in sampling the minibatch). Strikingly, note that after the very beginning of training,the expected loss change is sometimes negative (as desired) but oftentimes positive. See Figure 26.
Figure 26: An SGD step with a smaller learning rate or a larger batch size than the ones usedduring training would consistently decrease the loss in expectation. At regular intervals duringthe training run depicted in Figure 25 (with η = 0.01 and batch size 32), we measure the expectedchange in the full-batch training loss that would result from an SGD step with a different step sizeor batch size. Observe that taking an SGD step with a smaller step size or a larger batch size wouldconsistently have decreased the loss in expectation, while taking an SGD step with a larger step sizeor a smaller batch size would have consistently increased the loss in expectation.
Figure 27: Control experiment. Above, in Figure 26(a), as we trained a network with step size 0.01and batch size 32, we evaluated the expected change in training loss that would result from takingan SGD step with step size 0.01 and batch size 64. Here, in Figure 27(a), as a “control experiment,”we train the network with step size 0.01 and batch size 64, and evaluate the expected change intraining loss that would result from taking an SGD step with the same step size and batch size. Weobserve that an SGD step using the same step size and batch size that are used during training wouldsometimes increase and sometimes decrease the training loss in expectation. The other three panesare analogous.
Figure 28: Gradient flow. We train the network to 99% accuracy with gradient flow, by using theRunge-Kutta method to discretize the gradient flow ODE (details in §I.5). We plot the train loss(left), sharpness (center), and train accuracy (right) over time. Observe that the sharpness tends tocontinually increase (except for a slight decrease at initialization).
Figure 29: Gradient descent. We train the network to 99% accuracy using gradient descent at arange of step sizes η. Top left: we plot the train loss curves, with a vertical line marking the iterationwhere the sharpness first crosses 2∕η. Observe that the train loss monotonically decreases before thispoint, but behaves non-monotonically afterwards. Top middle: we plot the sharpness (measured atregular intervals during training). For each step size, the horizontal dashed line of the appropriatecolor marks the maximum stable sharpness 2∕η. Observe that the sharpness tends to increase duringtraining until reaching the value 2∕η, and then hovers just a bit above that value. Bottom left: Wetrack the `2 distance between (random projections of) the gradient flow iterate at time t and thegradient descent iterate at iteration t/n (details in §I.6). For each step size η, the vertical dottedline marks the time when the sharpness first crosses 2/n. Observe that the distance is essentiallyzero until this time, and starts to grow afterwards. From this, we can conclude that gradient descentclosely tracks the gradient flow trajectory (moving at a speed η) until the point on that trajectorywhere the sharpness reaches 2/n. Bottom middle: to further visualize the previous point, we plotthe evolution of sharpness during gradient descent, but with time (= iteration × step size) on thex-axis rather than iteration. We plot the gradient flow sharpness in black. Observe that the gradientdescent sharpness matches the gradient flow sharpness until reaching the value 2/n.
Figure 30: Gradient flow. Refer to the Figure 28 caption for more information. Additionally, in thisfigure the sharpness drops at the end of training due to the cross-entropy loss.
Figure 31: Gradient descent. Refer to the Figure 29 caption for more information.
Figure 32:	Gradient flow. Refer to the Figure 28 caption for more information.
Figure 33:	Gradient descent. Refer to the Figure 29 caption for more information.
Figure 34: Gradient flow. Refer to the Figure 28 caption for more information. Additionally, in thisfigure the sharpness drops at the end of training due to the cross-entropy loss.
Figure 35: Gradient descent. Refer to the Figure 29 caption for more information.
Figure 36: Gradient flow. Refer to the Figure 28 caption for more information.
Figure 37: Gradient descent. Refer to the Figure 29 caption for more information.
Figure 38: Gradient flow. Refer to the Figure 28 caption for more information. Additionally, in thisfigure the sharpness drops at the end of training due to the cross-entropy loss.
Figure 39: Gradient descent. Refer to the Figure 29 caption for more information.
Figure 40: Runge-Kutta. We train the network to 99% accuracy using the Runge-Kutta algorithm.
Figure 41: Gradient descent. All panes except bottom left: refer to the Figure 29 caption formore information. Bottom left: We track the `2 distance between (random projections of) theRUnge-KUtta iterate at time t and the gradient descent iterate at iteration t∕η. For each step size η,the vertical dotted loss marks the time when the sharpness first crosses 2∕η. In contrast to Figure29, here the distance between gradient descent and gradient flow starts to noticeably grow from thebeginning of training. From this we conclUde that for this architectUre, gradient descent does nottrack the RUnge-KUtta trajectory at first. FUrthermore, for this architectUre, observe that the trainingloss starts behaving non-monotonically (and the sharpness begins to plateaU) before the sharpnesshits 2∕η. As discussed in Appendix A ("Caveats"), We believe that this is also due to the fact thatReLU is non-differentiable.
Figure 42: Runge-Kutta. Refer to the Figure 40 caption for more information. Additionally, in thisfigure the sharpness drops at the end of training due to the cross-entropy loss.
Figure 43: Gradient descent. Refer to the Figure 41 caption for more information.
Figure 44: Runge-Kutta. Refer to the Figure 40 caption for more information.
Figure 45: Gradient descent. Refer to the Figure 41 caption for more information.
Figure 46: Runge-Kutta. Refer to the Figure 40 caption for more information. Additionally, in thisfigure the sharpness drops at the end of training due to the cross-entropy loss.
Figure 47: Gradient descent. Refer to the Figure 41 caption for more information.
Figure 48: Runge-Kutta. We train the network to 99% accuracy using the Runge-Kutta algorithm.
Figure 49: All panes except bottom left: refer to the Figure 29 caption for more information.
Figure 50: Runge-Kutta. Refer to the Figure 48 caption for more information. Additionally, in thisfigure the sharpness drops at the end of training due to the cross-entropy loss.
Figure 51: Gradient descent. Refer to the Figure 49 caption for more information.
Figure 52:	Runge-Kutta. Refer to the Figure 48 caption for more information.
Figure 53:	Gradient descent. Refer to the Figure 49 caption for more information.
Figure 54: Runge-Kutta. Refer to the Figure 48 caption for more information. Additionally, in thisfigure the sharpness drops at the end of training due to the cross-entropy loss.
Figure 55: Gradient descent. Refer to the Figure 49 caption for more information.
Figure 56: Runge-Kutta. Refer to the Figure 48 caption for more information.
Figure 57: Gradient descent. Refer to the Figure 49 caption for more information.
Figure 58: Runge-Kutta. Refer to the Figure 48 caption for more information. Additionally, in thisfigure the sharpness drops at the end of training due to the cross-entropy loss.
Figure 59: Gradient descent. Refer to the Figure 49 caption for more information.
Figure 60: Gradient flow. Refer to the Figure 28 caption for more information. Additionally, in thisfigure the sharpness drops at the end of training due to the cross-entropy loss.
Figure 61: Gradient descent. Refer to the Figure 29 caption for more information.
Figure 62: Gradient flow. Refer to the Figure 28 caption for more information.
Figure 63: Gradient descent. Refer to the Figure 29 caption for more information.
Figure 64: Gradient flow. Refer to the Figure 28 caption for more information. Additionally, in thisfigure the sharpness drops at the end of training due to the cross-entropy loss.
Figure 65: Gradient descent. Refer to the Figure 29 caption for more information.
Figure 66: Runge-Kutta. Refer to the Figure 48 caption for more information.
Figure 67: Gradient descent. Refer to the Figure 49 caption for more information.
Figure 68: Runge-Kutta. Refer to the Figure 48 caption for more information. Additionally, in thisfigure the sharpness drops at the end of training due to the cross-entropy loss.
Figure 69: Gradient descent. Refer to the Figure 49 caption for more information.
Figure 70: We train a ELU CNN (+ BN) using gradient flow.
Figure 71: We train an ELU CNN (+ BN) using gradient decent at a range of step sizes. (a) weplot the train loss, with a vertical dotted line marking the iteration where the sharpness on the pathfirst crosses 2∕η. The inset shows that the red curve is indeed behaving non-monotonically. (b) Wetrack the sharpness “between iterates.” This means that instead of computing the sharpness rightat the iterates themselves (as we do elsewhere in the paper, and in pane (e) here), we compute themaximum sharpness on the line between between successive iterates. Observe that this quantityrises to 2/n (marked by the horizontal dashed line) and then hovers right at, OrjUSt above that value.
Figure 72: We train a tanh CNN (+ BN) using gradient flow.
Figure 73: We train a tanh CNN (+ BN) using gradient decent.
Figure 74: We train a ReLU CNN (+ BN) using gradient flow.
Figure 75:	We train a ReLU CNN (+ BN) using gradient descent.
Figure 76:	On a 5,000-size subset of CIFAR-10, we train a ReLU CNN both with BN (top row) andwithout BN (bottom row) at the same grid of step sizes. We plot the sharpness/smoothness (centercolumn) as well as the effective smoothness (Santurkar et al., 2018) (right column). Observe thatfor both networks, the effective smoothness hovers around zero initially, and jumps up to 2/n oncegradient descent enters the Edge of Stability.
Figure 77: When training a deep linear network without BN, we measure effective smoothness at adistance α = 30η that is 30x larger than the step size.
Figure 78: When training a deep linear network with BN, we measure effective smoothness at adistance α = 30η that is 30x larger than the step size.
Figure 79:	When training a deep linear network without BN, we measure effective smoothness at adistance α = η that is equal to the step size.
Figure 80:	When training a deep linear network with BN, we measure effective smoothness at adistance α = η that is equal to the step size.
Figure 81: Training a Transformer using gradient flow.
Figure 82: Training a Transformer using gradient descent. We train a Transformer for 15,000iterations on the WikiText-2 language modeling dataset. (For this problem, training to completionWould not be computationally practical.) Top left: We plot the train loss curves, With a verticaldotted line marking the iteration where the sharpness first crosses 2/n. The train loss decreasesmonotonically before this line, but behaves non-monotonically afterWards. Top right: We plot theevolution of the sharpness, with a horizontal dashed line marking the value 2/n. Observe that foreach step size, the sharpness rises to 2/n and then hovers right at, or just above, that value. Bottomleft: for the initial phase of training, we plot the distance between the gradient descent trajectory andthe gradient flow trajectory, with a vertical dotted line marking the gradient descent iteration wherethe sharpness first crosses 2/n. Observe that the distance begins to rise from the start of training,indicating that for this architecture, gradient descent does not track the gradient flow trajectoryinitially. Bottom right: for the initial phase of training, we plot the evolution of the sharpness by“time” = iteration × n rather than iteration. The black dots are the sharpness during gradient flow.
Figure 83: The datasets for the toy one-dimensional regression task.
Figure 84: Fitting Chebyshev polynomials using gradient flow. We use gradient flow to fit aone-hidden-layer tanh network to the Chebyshev polynomials of degrees 3, 4, and 5. We observethat the sharpness rises more when fitting a higher-degree polynomial, likely because the dataset ismore complex.
Figure 85: Chebyshev degree 4 (gradient descent). We fit the Chebyshev polynomial of degree 4using gradient descent at a range of step sizes (see legend).
Figure 86: Chebyshev degree 5 (gradient descent). We fit the Chebyshev polynomial of degree 5using gradient descent at a range of step sizes (see legend).
Figure 87: Training a deep linear network using gradient flow. We use gradient flow to train adeep linear network. Since it is unclear whether this network can be trained to zero loss (or howlong that would take), we arbitrarily chose to stop training at time 100.
Figure 88: Training a deep linear network using gradient descent. We train a deep linear networkusing gradient descent at a range of step sizes (see legend). (Note that in the top left pane, the blue,orange and green dotted lines are directly on top of one another.)64Published as a conference paper at ICLR 2021M	Experiments: standard architectures on CIFAR- 1 0In this appendix, we demonstrate that our findings hold for three standard architectures on the stan-dard dataset CIFAR-10. The three architectures are: a VGG with batch normalization (Figures89-90), a VGG without batch normalization (Figures 91-92), and a ResNet with batch normalization(Figures 93 -94). See §I.2 for full experimental details.
Figure 89: We train a VGG with BN to completion using gradient flow (Runge-Kutta). Observethat the sharpness rises dramatically from 6.38 at initialization to a peak of 2227.59.
Figure 90: We train a VGG with BN to completion using gradient descent at different step sizes(see legend in the top right pane). Top left: we plot the train loss, with a vertical dotted linemarking the iteration where the sharpness first crosses 2∕η. Top center: We plot the evolution ofthe sharpness, with a horizontal dashed line (of the appropriate color) marking the value 2∕η. Topright: we plot the train accuracy. Bottom left: for the initial phase of training, we monitor thedistance between the gradient descent iterate at iteration t∕η, and the gradient flow solution at timet, with a vertical dotted line (of the appropriate color) marking the time when the sharpness crosses2∕η. Observe that the distance between gradient descent and gradient flow is almost zero beforethis instant, but starts to rise shortly afterwards. Bottom center: we plot the sharpness by time (=iteration ×η) rather than iteration. The black dots are the sharpness of the gradient flow trajectory,which shoots up immediately. Bottom right: we plot the test accuracy.
Figure 91: We train a VGG without BN to 37.1% accuracy using gradient flow (Runge-Kutta).
Figure 92: We train a VGG without BN to completion using gradient descent at a range of stepsizes (see legend in the top right pane). Refer to the Figure 90 caption for more information.
Figure 93: We train a ResNet to 43.2% accuracy using gradient flow (Runge-Kutta). Observe thatthe sharpness rises dramatically from 1.07 at initialization to 760.63 at 43.2% accuracy. We trainthis network only partway because training this network to completion would be too computationallyexpensive: Runge-Kutta runs very slowly when the sharpness is high (because it is forced to takesmall steps) and for this network the sharpness is extremely high when the train accuracy is only42% (which means that there is a long way to go).
Figure 94:	We train a ResNet to completion using gradient descent at a range of step sizes (seelegend in the top right pane). Refer to the Figure 90 caption for more information.
Figure 95:	Gradient descent with Polyak momentum, β = 0.9.
Figure 96:	Gradient descent with Nesterov momentum, β = 0.9.
Figure 97: Gradient descent with Polyak momentum, βAusnbeiteration0	100	200	300	400iteration0.9.
Figure 98: Gradient descent with Nesterov momentum, β0.9.
Figure 99: Gradient descent with Polyak momentum, β = 0.9.
