Figure 1: The schematic illustration of the proposed MELR model. It consists of two main com-ponents for modeling episode-level relationships: Cross-Episode Attention Module (CEAM) andCross-Episode Consistency Regularization (CECR). For clarity, only the 5-way 1-shot setting ispresented here. Each red/blue cuboid denotes a single instance.
Figure 2: (a) Ablative results for our MELR. (b) Comparison among different ProtoNett+CEAMalternatives. (c) Comparison among different ProtoNett+CECR alternatives. All results are obtainedunder the 5-way 1-shot and 5-shot settings on miniImageNet with Conv4-64 as the feature extractor.
Figure 3: (a) - (c) Visualizations of data distributions obtained by ProtoNet*, ProtoNett+CEAM,and ProtoNett+CEAM+CECR (i.e., our MELR) for the same meta-test episode, respectively. (d)-(e) Visualizations of attention maps for query and support sets, respectively. All results are obtainedunder the 5-way 5-shot setting on miniImageNet with Conv4-64 as the feature extractor.
Figure 4: The first three subfigures in each row are the visualizations of data distributions obtained byProtoNett, ProtoNett+CEAM, and ProtoNett+CEAM+CECR (i.e., our MELR) for the same meta-test episode, respectively. The last two subfigures in each row are the visualizations of attention mapsfor query and support sets, respectively. All results are obtained under the 5-way 5-shot setting onminiImageNet with Conv4-64 as the feature extractor.
Figure 5: Illustration of hyper-parameter analysis on the miniImageNet dataset. Conv4-64 is usedas the feature extractor.
Figure 6: Schematic illustration of our CEAM with a pair of episodes as its input. For easy un-derstanding (but without loss of generality), a toy visual example is considered: only one outlyinginstance X exists in the support set S⑴ of the first episode, but the support set S ⑵ of the secondepisode is properly sampled. Since S(D=S(1) \ {χ} and S(2) have similar data distributions (fromthe same classes), the outlying instance X is pulled back to S(1) by attending on it with S(2)(whichcan not be done by attending on it with S(1)), i.e., its negative effect is mitigated by our CEAM.
Figure 7: Visualization results of 10 meta-test episodes on miniImageNet under the 5-way 5-shotsetting (Conv4-64 is used as the backbone). For each meta-test episode, We visualize three datadistributions (from left to right) obtained by ProtoNet*, ProtoNett+CeAM, and our MELR,respec-tively. That is, each meta-test episode is denoted by three subfigures and each row has two episodes.
Figure 8: Visualization of the generalization ability of our MELR on the test split of miniImageNetunder the 5-way 1-shot setting with Conv4-64 as the backbone. Note that we check the test perfor-mance of the model at each training epoch (i.e., every 100 training iterations).
