Figure 1: DICE better leverages en-semble size. Without weights sharing,5 networks trained with DICE match 7networks trained independently. Withlow-level weights sharing, 4 branchestrained with DICE match 7 traditionalbranches. Dataset: CIFAR-100. Back-bone: ResNet-32. Details in Table 8.
Figure 2: Outline. DICE prevents features from be-ing predictable from each other conditionally upon thetarget class. Features extracted by members (1, 2) fromone input (∙,∙) should not share more information thanfeatures from two inputs in the same class (•,▲): i.e.,(∙,-) should not be able to differentiate (-,∙) and (-,▲).
Figure 3: Venn Information Di-agram (Yeung, 1991). DICEminimizes conditional redundancy(green vertical stripes ) with nooverlap with relevancy (red stripes).
Figure 4: Learning strategy overview. Blue arrows represent training criteria: (1) classificationwith conditional entropy bottleneck applied separately on members 1 and 2, and (2) adversarialtraining to delete spurious correlations between members and increase diversity. X and X0 belongto the same Y for conditional redundancy minimization. See Figure 13 for a larger version.
Figure 5: Ensemble diversity/individual accu-racy trade-off for different strategies. DICE(r. CEBR) is learned with different δcr (r. δr).
Figure 6: Impact of the diversity coefficient δcrin DICE on the training dynamics on validation:CR is negatively correlated with diversity.
Figure 7: Confidence estimates separate images from CIFAR-100and OOD images from TinyImageNet (crop) for different strategies(AUROC ↑). DICE×w uses the discriminator to scale its confidence:1 - w’s predictions behave like an ”input-dependant temperature”.
Figure 8:	Training dynamics on the validation dataset while training on 95% of the trainingdataset. A higher diversity coefficient decreases individual performance (lower left), but increasesensemble performance in terms of accuracy (upper left), uncertainty estimation (upper right) up to avalue, found at δcr = 0.2 for 4-branches ResNet-32. Calibration before temperature scaling (lowerright) highly benefits from higher diversity. Learning rate updates create ”steps” in the curves.
Figure 9:	Diversity dynamics on train and validation dataset. DICE increases diversity for pair-wise (ratio errors, agreement, Q-statistics) and non-pairwise (entropy, Kohavi-Wolpert variance)measures.
Figure 10: Discriminator dynamics and learning curve. The task becomes harder for higher valuesof δcr : the joint and product features distributions tend to be indistinguishable.
Figure 11: The discrimina-tor remains calibrated evenat the end of the adversarialtraining.
Figure 12: Training dynamics and ablation study of components from equation 11. Addingthe RHS overall decreases ensemble performances, in terms of accuracy (upper left) or uncertaintyestimation (upper right), when combined with CEB or DICE(=LHS). It decreases diversity (lowerright) with no clear impact on individual accuracy (lower left).
Figure 13: Learning strategy overview. Blue arrows represent training criteria: (1) classification with conditional entropy bottleneck applied separately onmembers 1 and 2, and (2) adversarial training to delete spurious correlations between members and increase diversity. X and X, belong to the same Y forconditional redundancy minimization.
