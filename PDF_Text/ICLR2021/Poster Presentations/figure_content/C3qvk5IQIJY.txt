Figure 1: Overparameterization in GANs. We train DCGAN models by varying the size of thehidden dimension k (larger the k, more overparameterized the models are, see Fig. 8 for details).
Figure 2: Convergence plot a GAN model with linear discriminator and 1-hidden layer generatoras the hidden dimension (k) increases. Final mse is the mse loss between true data mean and themean of generated distribution. Over-parameterized models show improved convergencewith C a fixed numerical constant, running GD updates of the form Wτ+1 = WT - η VL(Wτ)the loss given in equation 5 with step-size η = -n--, with η ≤ 1, satisfies243k∙ d+产∙σV ∙σ2on1n—£V ReLU (WT Zi) - X ≤(1 - 4 X 10-6 ∙ η)τni=1	`21n-VVReLU (WoZi) - Xni=1	`2(6)This holds with probability at least 1 — (n + 5) e- ImO — 5k ∙ e-c1 'n — (2k + 2) e- 2d6 —ne-c2∙md3 log ⑷2 with c1, c2 fixed numerical constants.
Figure 3: MLP Overparameterization onThe MSE loss betWeen the true data mean and thedata mean of generated samples is used as our eval- MNIST.
Figure 4: Overparamterization Results: We plot the FID scores (lower, better) of DCGAN andResnet DCGAN as the hidden dimension k is varied. Results on CIFAR-10 and Celeb-A are shownon the plots on the left and right panels, respectively. Overparameterization gives better FID scores.
Figure 5: DCGAN Training Results: We plot the FID scores across training iterations of DCGANon CIFAR-10 and Celeb-A for different values of hidden dimension k. Remarkably, we observe thatover-parameterization improves the rate of convergence of GDA and its stability in training.
Figure 6: Generalization in GANs: We plot the NND scores as the hidden dimension k is variedfor DCGAN (shown in (a)) and Resnet (shown in (b)) models.
Figure 10: Nearest neighbor visualization. We visualize the nearest neighbor samples in trainingset for generations from DCGAN model trained on CIFAR-10 dataset. Left panel shows DCGANtrained with k = 8, while the right one shows the one with k = 128. We observe that overparame-terized models generate samples with high diversity.
