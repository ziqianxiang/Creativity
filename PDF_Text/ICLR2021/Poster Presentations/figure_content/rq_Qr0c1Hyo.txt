Figure 1: (a) Explicitly including the implicit regularizer in the loss improves the test accuracy whentraining with small learning rates. (b) The optimal regularization coefficient λopt = 2-6 is equal tothe optimal learning rate opt = 2-6. (c) Increasing either the learning rate or the regularizationcoefficient λ reduces the value of the implicit regularization term Creg(ω) at the end of training.
Figure 3: (a) Different batch sizes achieve thesame test accuracy if the ratio of the learning rateto the batch size (/B) is constant and B is nottoo large. (b) The test accuracy is independentof the batch size if the ratio of the regularizationcoefficient to the batch size (λ∕B) is constant.
Figure 2:	(a) There is a clear generalizationbenefit to large learning rates when training onthe original loss C(ω) with λ = 0. (b) Whenwe include the implicit regularizer explicitly inCmod (ω) and set λ = 2-6, the generalizationbenefit of large learning rates is diminished.
Figure 4: (a) When training for 400 epochs, smaller values of n are stable at larger bare learningrates α, and this enables them to achieve higher test accuracies. (b) Similar conclusions hold whentraining for a fixed number of updates. (c) We show the test accuracy at the optimal learning rate fora range of epoch budgets. We find that smaller values of n consistently achieve higher test accuracy.
Figure 5: (a) When training on the original loss C(ω), the training accuracy achieved within 6400epochs is maximized when the learning rate = 2-9. Smaller learning rates have not yet converged,while larger learning rates reach a plateau. When training on the modified loss Cmod (ω) with fixedlearning rate = 2-9 and regularization coefficient λ, we achieve high training accuracies when λis small, but are unable to achieve high training accuracies when λ is large. (b) We plot the value ofthe original loss C(ω) at the end of training. Remarkably, the training losses obtained when trainingon the original loss with a large learning rate are similar to the training losses achieved when trainingon the modified loss with small learning rate ( = 2-9) and a large regularization coefficient λ.
Figure 6: (a) Tuning the regularization scale λ has a similar influence on test accuracy to tuning thelearning rate , although unlike our CIFAR-10/Wide-ResNet experiments the optimal values of λand do not coincide. (b) We obtain similar accuracies at different batch sizes if the ratio /B isconstant. (C) We obtain similar accuracies at different batch sizes if the ratio (λ∕B) is constant.
Figure 7: (a) We train with n-step SGD for 400 epochs, and achieve similar test accuracies fordifferent values of n, so long as the bare learning rate α is the same. This is consistent with backwarderror analysis of n-step SGD, but contradicts the predictions of the SDE analogy. (b) We observe asimilar phenomenon when training for a fixed number of parameter updates. (c) Different values ofn achieve similar test accuracies at their optimal learning rate, across a range of epoch budgets.
