Figure 1: Full declarative proof the irrationality of vz2 in ISabelle/HOL.
Figure 2: Architecture of the encoder of the hierarchical transformer (HAT). There are two types oflayers, the local layers model the correlation between tokens within a proposition, and the globallayers model the correlation between propositions. The input to the network is the sum of the tokenembedding, the positional information, and the embedding of the corresponding category.
Figure 3: Accuracy of differ-Importance of Category Information We subsequently investi- ent source sequence lengths.
Figure 5: Attention visualisation of the last layer of the transformer encoder for the source propositionsF.2: F.3: x70 ∈ x39 F.4: x57 ⊆ x39 . The generated target proposition is x70 ∈ x57.
Figure 4: Nearest neighbours of the tokens ‘Borel measurable’ (left) and ‘arrow’ (right) in cosinespace. The 512-dimensional embeddings are projected into 3-dimensional embeddings. Neighboursare found by picking the top 50 tokens whose embeddings are closest to the selected token.
