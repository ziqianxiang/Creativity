Figure 1: Degradation due to delayin a two-state MDP.
Figure 2: Optimality of policy types in ED-MDPs: Markovness is sufficient but non-stationarity is necessary.
Figure 3: Delayed-Q algorithm diagram.
Figure 4: Maze: Time complexity asa function of mTabular Maze Domain. We begin with testing Delayed-Q on a Maze domain (Brockman et al.,2016)[tinyurl.com/y34tmfm9]. It is based on tabular Q-learning and enables us to study themerits of our method decoupled from the coming DDQN added complexities. Moreover, it conveysthe exponential complexity of Augmented-Q. The forward-model we construct is naturally tabular aswell: it predicts a state s1 according to the highest visitation frequency given ps, a). The objective inMaze is to find the shortest path from a start position to a goal state in a randomly-generated N X Nmaze. Reaching the goal yields a reward of 1, and —1{p10N2) per step otherwise. The maximalepisode length is 10N2 steps, so the cumulative reward is in r—1, 1s. We also create a Noisy Mazeenvironment that perturbs each action w.p. p Pr0, 0.5s.
Figure 5: Delayed-Q (median over 5 seeds): (a) Total reward after 5000 training episodes on 10 X 10Maze. Performance is sensitive to both delay value and stochasticity. (b) Noisy Cartpole. (c) Rewardon varying Maze sizes. Abscissa is in log-scale, so the return decreases exponentially with m.
Figure 6: Convergence plots for Maze, Noisy Cartpole and Atari MsPacman. Note that the scale ofthe y-axes (performance) may change from figure to figure.
Figure 7: Performance as a function of the delay (from left to right): Maze, Noisy Cartpole, NoisyAcrobot. For AUgmented-Q in Maze, m > 10 is missing due to explosion of the state-space.
Figure 9: Noisy Cartpole: Performance gap between true and trained forward model.
Figure 8: Performance gap forDelayed-Q trained with a delay ofm “ 10.
Figure 10: Experiment summary: mean of episodic return for all domains. Delayed-Q outperformsthe alternatives in 39 of 42 experiments. Due to negative reward, a positive translation of 1 is appliedfor Maze and 500 for Acrobot. Atari x-axis is the gain relative to lowest result in each experiment.
