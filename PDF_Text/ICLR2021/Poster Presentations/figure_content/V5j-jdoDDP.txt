Figure 1: Examples where SMUG outperforms other compared methods (top 4 rows), and where SMUGperforms less favorably (last 2 rows) based on LSC. The green box on the original image highlights thegroundtruth box; for the saliency methods it represents the bounding box with the best LSC score. Numberson top denote the LSC score, the fractional area of the bounding box (a), and the confidence of the classifier(c) on the cropped region. Red and blue colors denote regions of high and low importance respectively. Morequalitative examples can be found in Appendix DIG vs SMUG and SMUGbase. From Table 1, we observe that SMUG and SMUGbase achieve asignificantly better score (-1.26 and -1.23 resp.) compared to IG (-0.34). As observed in somequalitative examples (Fig. 1, Appendix D), SMUG tends to assign high scores to a much morelocalized set of pixels whereas IG distributes high scores more widely (spatially). As LSC metricfavors compactness, which is desirable for human interpretability, it results in better scores forSMUG and SMUGbase.
Figure 2: OPTB OX vs SMUG. 6 columns on the left correspond to the images for which OPTB OX gets abetter score than SMUG. 2 columns on the right correspond to the images for which SMUG got a better LSCscore than OPTB OX. Numbers at the top denote the LSC score, fractional area of the bounding box a and theconfidence of the classifier c on the cropped region.
Figure 3: (a) shows an image of a cat (catbig) placed on a white background that is classified with a confidenceof 0.83. (b) shows an image of the same cat (catsmall), scaled to a quarter of its original size, that is classifiedwith a confidence of 0.15. (c) By placing catbig next to catsmall we observe a significant jump in the classifier’sconfidence from 0.15 with catsmall alone to 0.84 on 2-cats. While (d) SMUG and (e) IG correctly attribute themodel confidence to catbig, (f) OPTB OX exploits the object rescaling in LSC, favoring the more compact object.
Figure 4: Example comparing our method (SMUG, SMUGbase) with SIS and IG on a test sample from theBeer Reviews dataset. Green color signifies a positive relevance, red color signifies negative relevance. Theunderlined words are human annotations. More examples can be found in Appendix C.
Figure 5: MNIST images and the corresponding masks corresponding to Sec. 3.1.
Figure 6: Hyperparameter Comparisiontop-k We analyze images with top-k ∈ {500, 1000, 3000, 5000}. Increasing the k value increases thereceptive field and the discovered input masks also grow in size with increasing values of k . Figure 6shows how the solver runtime and the mask size vary with k . As expected, larger k values results inlarger number of constraints and therefore larger solving times as well as larger mask sizes.
Figure 7: Qualitative examples of the masks generated by SMUG on examples from Imagenet fordifferent choices of top-k (columns) and γ (rows). γ = 0 is the most minimal mask. Even at lowvalues of top-k and γ SMUG highlights pixels relevant for the object class.
Figure 8: Examples comparing our method (SMUG, SMUGbase) with SIS and IG on test samplesfrom the Beer Reviews dataset. Green color signifies a positive relevance, red color signifies negativerelevance. The underlined words are human annotations.
Figure 9: Examples showing the boolean masks and the saliency maps produced by SMUG onseveral ImageNet examples.
Figure 10: Examples comparing saliency maps where SMUG outperforms S MUGbase, and IG. The green boxon the original image highlights the groundtruth box; for the saliency methods it represents the bounding boxwith the best LSC score. Numbers on top denote the LSC score, the fractional area of the bounding box (a), andthe confidence of the classifier (c) on the cropped region.
Figure 11: Examples comparing saliency maps where SMUGbase, or IG outperforms SMUG. The green boxon the original image highlights the groundtruth box; for the saliency methods it represents the bounding boxwith the best LSC score. Numbers on top denote the LSC score, the fractional area of the bounding box (a), andthe confidence of the classifier (c) on the cropped region.
Figure 12: [Model bias] Examples where the model correctly predicts the class as “parallel bars” and“horizontal bars” for the corresponding inputs but the model’s focus is on the person leaping over thebar as opposed to the bar (i.e., the predicted object class) itself.
