Figure 1: The overview of our method. The transformer based encoder is stacked of N identical layerswith the local self-attention mechanism, which aims to efficiently process long features sequencesextracted from music in a GPU-memory-economic way. The decoder exploits a recurrent structureto predict dance movement yi frame by frame conditioned on hidden state hi of decoder and latentvector zi . Note that y0 is a predefined Begin Of Pose (BOP). During the training phase, the proposedcurriculum learning strategy gently increases the number of steps of autoregressive scheme accordingto the number of training epochs. This strategy bridges the gap between training and inference ofautoregressive models, and thus alleviates error accumulation at inference.
Figure 2: Human evaluation results on motion realism, style consistency and smoothness. Weconduct a human evaluation to ask annotators to select the dances that are more realistic and moresmooth regardless of music, more style-consistent with music through pairwise comparison.
Figure 3: Beat tracking curves for music and dance by onset strength and motion standard deviationrespectively. The left is a short clip of ballet while the right is a short clip of hip-hop. The red circlesare kinematic beats and dash lines denote the musical beats.
Figure 4: FID curves of different methods over time.
Figure 5: Beat tracking curves for the first 20 seconds of a music clip randomly sampled from audiodata under the sampling rates of 15,400Hz and 22,050Hz. Most of the beats are aligned within thetime offset threshold âˆ†t = 1/15s.
