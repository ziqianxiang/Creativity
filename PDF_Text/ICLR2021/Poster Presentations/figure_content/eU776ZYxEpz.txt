Figure 1: Illustration of DANN architecture. Lines with arrow ends indicate excitatory projections.
Figure 2: Empirical verification of update correction terms: Cosine of the angle between gradientsmultiplied by an approximation of the diagonal of the Fisher inverse for each layer, and eitheruncorrected gradients (black) or corrected gradients (orange) over 50 epochs. Plot displays a movingaverage over 500 updates5	Experimental ResultsHaving derived appropriate parameter initialisation and updates for DANNs, we now explore howthey compare to traditional ANNs and ColumnEi models on simple benchmark datasets. In brief, wefind that column constrained models perform poorly, failing even to achieve zero training-set error,whereas DANNs perform equivalently to traditional ANNs.
Figure 3: Model comparison on MNIST dataset. nc - no update corrections, 1i - one inhibitory unitEpoch #nc ■ DANN 1i ■ CoIumnEi7Published as a conference paper at ICLR 2021We first compared model performance on the MNIST dataset (Fig 3). We observed that ColumnEimodels generalised poorly, and failed to achieve 0 % training error within the 50 epochs. This confirmsthe fact that such models cannot learn as well as traditional ANNs. In contrast, we observed thatDANNs performed equivalently to multi-layer perceptrons (MLPs), and even generalised marginallybetter. This was also the case for ColumnEi and DANN models constructed with more inhibitoryunits (Supp. Fig. 6, 100 inhibitory units per layer). In addition, performance was only slightlyworse for DANNs with one inhibitory unit per layer. These results show that DANN performancegeneralizes to different ratios of excitatory-to-inhibitory units. We also found that not correctingparameter updates using the corrections derived from the Fisher significantly impaired optimization,further verifying the correction factors (Fig 3).
Figure 4: Model comparison on Fashion MNIST and Kuzushiji MNIST datasets.
Figure 5: DANNs trained on Fashion MNIST with gradient normalisation and learning rate scaling.
Figure 6: Model comparison on MNIST dataset as in Fig. 3 but including models with 100 inhibitoryunits. nc - no update corrections, #i - no. inhibitory units, ColumnEi #e #i.
Figure 7: Convolutional network results on CIFAR-10 with control and DANN VGG16 models. Plotsshow mean training and test set error over 6 random seeds.
