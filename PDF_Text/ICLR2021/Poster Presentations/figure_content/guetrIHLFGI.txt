Figure 1: Three architectures trained from scratch on CIFAR-5m, a CIFAR-10-like task. The RealWorld is trained on 50K samples for 100 epochs, while the Ideal World is trained on 5M samples in1 pass. The Real World Test remains close to Ideal World Test, despite a large generalization gap.
Figure 2: Real vs Ideal World: CIFAR-5m. SGD with 50K samples. (a): Varying learning-rates0.1 (â€¢), 0.01 (), 0.001 (N). (b): Random architectures from DARTS space (Liu et al., 2019).
Figure 3:	ImageNet-DogBird. Real World models trained on 10K samples.
Figure 4:	Effect of Sample Size.
Figure 5:	Deep Phenomena in Real vs. Ideal Worlds.
Figure 6: SoftError vs. Error vs. Loss: ResNet-18.
Figure 7: Toy Example. Examples of settings with large and small bootstrap error.
Figure 8:	The corresponding train soft-errors for Figure 1.
Figure 9:	Random DARTS Architectures. Panel (b) shows zoomed view of panel (a).
Figure 10: Effect of Data Augmentation in the Ideal World.
Figure 11: Effect of Data Augmentation.
Figure 12: Adam Experiments. For various architectures on 50K samples from CIFAR-5m.
Figure 13: Real vs. Ideal Worlds for Vision Transformer on CIFAR-5m, with and w/o pretraining.
Figure 14: ImageNet-Pretraining: MLP[3x2048].
Figure 15: Effect of Learning Rate Drop.
Figure 16: SoftError vs. Error vs. Loss: MLP[5x2048].
Figure 17: Measuring Test Error instead of SoftError. Compare to Figure 2a(a) Test Error.
Figure 18: Real vs. Ideal: Training with Squared Loss.
Figure 19: CIFAR-5m Samples. Random samples from each class (by row).
Figure 21: ImageNet-DogBird Samples. Random samples from each class. Annotated by theiroriginal ImageNet class for reference.
