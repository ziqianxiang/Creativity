Figure 1: An illustration of federated learning in a toy 2D setting with two clients and quadratic objectives. Left:Contour plots of the client objectives, their local optima, as well as the corresponding global optimum. Middle:Learning curves for MB-SGD and FedAvg with 10 and 100 steps per round. FedAvg makes fast progressinitially, but converges to a point far away from the global optimum. Right: Learning curves for FEDPA with 10and 100 posterior samples per round and shrinkage ρ = 1. More posterior samples (i.e., more local computation)results in faster convergence and allows FedPA to come closer to the global optimum. Shaded regions denotebootstrapped 95% CI based on 5 runs with different initializations and random seeds. Best viewed in color.
Figure 2: Evaluation metrics for FedAvg and FedPA computed at each training round on (a) CIFAR-100 and(b) StackOverflow LR. During the initial rounds (the “burn-in phase")，FedPA computes deltas the same way asFedAvg; after that, FedPA computes deltas using Algorithm 3 and approximate posterior samples.
Figure 3: The bias and variance tradeoffs for FEDAVG and FEDPA as functions of the estimation parameters.
Figure 4: The ESS statistics for samples produced by IASG on random synthetic least squares linear regressionproblems of dimensionality 10, 100, 1000. Total number of data points per problem: 500, batch size: 10. In (a)and (b) the learning rate was set to 0.1 for 10 and 100 dimensions, and 0.01 for 1000 dimensions.
Figure 5: Evaluation metrics for FedAvg and FedPA computed at each training round on EMNIST-62.
Figure 6: Evaluation metrics for FedAvg and FedPA computed at each training round on (a) CIFAR-100 and(b) StackOverflow NWP tasks.
Figure 7: Evaluation metrics for FEDAVG and FEDPA computed at each training round on StackOverflow LR.
