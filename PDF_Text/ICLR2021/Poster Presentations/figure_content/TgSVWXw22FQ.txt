Figure 1: Training and transfer processes. (a) Training style encoder Es with objective Ii： Allvoice samples are encoded into style embedding space. For style embedding sui of xui , we min-imize its distance with speaker u's style centroid μu, and maximize its distance to other speakerstyle centroids μv. (b) Training for content encoder Ec and decoder D as objectives I2,13: Weencode content cui from voice xui from speaker u. The style of speaker u is encoded from anotherspeaker u's voice Xuj. The dependency of style and content embedding is minimized with I3. WithCui and su, the decoder reconstructs the voice Xui as Xui = D(su, Cui). Then I2 is calculated basedon the original voice Cui and the reconstruction Cui. (c) Transfer process: for zero-shot voice styletransfer, with Xui from speaker u and Xvj from speaker v, we encode content Cui and style sv , andcombine them together to generate a transferred voice Xu→v,i = D(Sv, cui).
Figure 2: Left: t-SNE visualization for speaker embeddings. Right: t-SNE visualization for contentembedding. The embeddings are extracted from the voice samples of 10 different speakers.
