Figure 1: Demonstration of five steps of Bayesian Optimization with FSBO for maximizing a sinewave (blue). One maximum is discovered within only three steps. Expected Improvement has beenscaled to improve readability. In black the predictions of the surrogate model. Bottom right areexamples of the source tasks. The deep kernel consists of a spectral kernel (Wilson & Adams, 2013)combined with a two-layer neural network (1 → 64 → 64).
Figure 2: Examples for the mutation and crossover operation with I = 3.
Figure 3: Comparison of the contribution of the various FSBO components to the final solution.
Figure 4: The warm start initialization yields the best results on GLMNet and SVM for all initial-ization lengths. For AdaBoost it is comparable to LHS.
Figure 5:	FSBO is the best of all considered methods.
Figure 6:	Left two plots: Training MetaBO for different number of trials T . Right plot: comparingthe final policy against a random search. Results for T = 15 match the results reported by Volppet al. (2020).
