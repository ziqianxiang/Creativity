Figure 1: (a) Example of Bayesian recovery from corrupted data with a TomograPhiC AUto-EnCoder(TAE) on corrupted MNIST. The TAE recovers posterior probability densities q(x∣yi) for eachcorrupted sample yi. We can draw from these to explore different possible clean solutions. (b) Twodimensional Bayesian recovery experiment. (i) Observed set of corrupted data Y, with the pointWe are inferring from yi highlighted. (ii) Ground truth hidden clean data with the target point Xihighlighted, along with the posterior q(x∣yi) reconstructed by a VAE. (iii) Posterior q(x∣yi) recoveredwith our TAE. While the VAE posterior collapses to a single point, the TAE reconstructs a richposterior that adjusts to the data manifold.
Figure 2: Training LVMs for data recovery. (a) Structure of the reconstruction LVM used to inferapproximate posteriors q(x|y) of clean data X from corrupted observations y as conditional inputs. (b)Training of q(x|y) using a VAE. A prior in the latent space Z is introduced as a regulariser, howeverno explicit regularisation is imposed in x. (c) Training of q(x|y) using our TAE model. An empiricalprior p(x) = J p(zp)p(x∣zp)dzp is instead introduced in clean data space x.
Figure 3: MNIST data recovery from missing entries and noise. (a) Recoveries using an MVAE andour TAE, showing average reconstruction and samples from the trained posteriors. (b) PSNR betweenground truths and mean reconstruction. (c) ELBO assigned by the recovered posteriors to the groundtruth data. The mean inference performance is very similar for the two models (PSNR values), whilethe probabilistic performance (ElBO values) is significantly higher for our TAE model. We can seeevidence of this difference in the reconstruction examples. The MVAE and TAE return similarlyadequate mean solutions, but the MVAE posterior,s draws are all very similar, suggesting that theposterior has collapsed on a particular reconstruction. Contrarily, the posteriors returned by the TAEexplore different possible solutions that are consistent with the associated corrupted observation.
Figure 4: Propagating uncertainty to a classification task. Draws from the MVAE posterior areall very similar to each other. As a result, the imputed images are almost always classified in thesame way and the uncertainty of the task is underestimated. The TAE posterior explores variedpossible solutions to the recovery task. These can be recognised as different classes, resulting in lessconcentrated distributed probabilities that better reflect the associated uncertainty.
Figure 5: Classification accuracy after imputation. Classifying using TAE imputations gives anadvantage in this downstream task over using raw corrupted data and MVAE imputations, especiallywhen the number of missing entries is high. This is because the MVAE collapses on single imputations,while the TAE generates diverse samples for each corrupted observation. The TAE classifier trainswith data augmentations consistent with observed corrupted images, instead of single estimates.
Figure 6: Unsupervised missing value imputation with our TAE on raw depth maps from the NYUrooms data set, compared with a median filter approach and the standard MVAE. Missing pixels inthe observed images are in white. The median filter results in overly smoothed images and is unableto fill pixels that are surrounded by large missing areas. The MVAE returns adequate reconstructions,however, it over-fits to inaccurate solutions in certain locations, returning low uncertainty. The TAEreturns good reconstructions and assigns high uncertainty to locations where reconstructions are mostinaccurate, as shown by the marginal standard deviations.
Figure 7: ELBO for MNIST with 90% missing values and additive noise as a function of chosenhyper-parameters C and λ (in log scale). The performance of TAE exceeds that of a standard VAEapproach over a broad range of values. If the values are too large, the model collapses duringoptimisation, making such situation easy to diagnose.
Figure 8: Examples of Bayesian reconstructions with MVAE and TAE on structured missing values.
Figure 9: Missing value imputation performance on MNIST in the absence of noise. As in thenoisy case, the PSNR values between the MVAE and the TAE are very similar. The TAE presentssignificantly superior ELBO values at low ratios of observed entries, but in this case, the gap isreduced as more entries are observed. This is because in the noiseless case, the solution space whenmost entries are observed is much more localised than in the noisy case, and therefore the MVAEcollapsed posteriors do not fail as much to capture it.
Figure 10: De-noising performance on MNIST. As in the missing value imputtion case, the MVAEand TAE perform very similarly in their mean reconstructions, but the TAE presents significantlybetter performance in capturing the distributions of clean solutions, as the test ELBO values arehigher.
Figure 11: Unsupervised missing value imputation with our TAE on raw depth maps from the NYUrooms data set, compared with a median filter approach and the standard MVAE. Missing pixels inthe observed images are in white. The median filter results in overly smoothed images and is unableto fill pixels that are surrounded by large missing areas. The MVAE returns adequate reconstructions,however, it over-fits in certain locations and its uncertainty is largely over-estimated. The TAEreturns good reconstructions and assigns high uncertainty to locations where reconstruction is mostinaccurate, as shown by the marginal standard deviations.
