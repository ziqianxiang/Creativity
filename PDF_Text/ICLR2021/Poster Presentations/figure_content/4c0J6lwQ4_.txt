Figure 1: Architecture of the mTAND module. It takes irregularly sampled time points and corre-sponding values as keys and values and produces a fixed dimensional representation at the query timepoints. The attention blocks (ATT) perform a scaled dot product attention over the observed valuesusing the time embedding of the query and key time points. Equation 3 and 4 defines this operation.
Figure 2: Architecture of the proposed encoder-decoder framework mTAND-Full. The classifier isrequired only for performing classification tasks. The mTAND module is shown in Figure 1.
Figure 3: Interpolations on the synthetic interpolation dataset. The columns represent 3 differentexamples. First row: Ground truth trajectories with observed points, second row: reconstructionson the complete range t ∈ [0, 1] using the proposed model mTAN, third row: reconstructions on thecomplete range t ∈ [0, 1] using the Latent ODE model with ODE encoder.
Figure 4: Visualization of attention weights. mTAN learns an interpolation over the query time pointsby attending to the observed values at key time points. The brighter edges correspond to higherattention weights.
