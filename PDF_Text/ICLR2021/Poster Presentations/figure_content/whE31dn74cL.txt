Figure 1: (a). The relations between neural network kernel Σ(h), temporal kernel KT and theneural-temporal kernel Σ(Th). (b). Composing a single-layer RNN with temporal kernel where thehidden out from GRU cells become f (h) (x, t) ≡ f (h) (x) ◦ φ(x, t). We use the RF-INN blocks todenote the random feature representation parameterized by INN. The optional outputs y(t) can beobtained in a way similar to f(h) (x, t).
Figure 2: The computation architecture of the proposed method using RNN as an example (followingFigure 1b). Here, We use f (h) (θ, ∙) to denote the standard GRU cell, and use g(ψ, ∙) to denote theinvertible neural network. At the ith step, the GRU cell takes the feature vector xi , and the hiddenstate of the previous steps ~hi , as input. The RF-INN module is called once for each batch.
Figure 3: The mean absolute error on testing data for the standard neural networks: RNN, CausalCNN(denoted by CNN) and self-attention (denoted by Att), for the temporal kernel approach and thebaselines methods in Case2 and Case3. The numerical results are averaged over five repetitions.
