Figure 1: Invariances present in protein structures.
Figure 2: Distances betweenatoms in our protein graph.
Figure 3: Intrinsic-extrinsic convolution on our multi-graph for atom A. First, we detect the neighbor-ing atoms involved in the convolution using a ball query (a function returning all nodes closer thanr to a point x). For each atom, the extrinsic (pink) and two intrinsic (blue and green) distances areinput into the kernel and the result is multiplied by the atom’s features. Lastly, all contributions fromneighboring atoms are summed up.
Figure 4: Hierarchical protein pooling: We segment the protein into amino acids (blue, orange,green). First, (a), we apply spectral clustering on each independent amino acid graph. Then, (b), eachresulting amino acid is pooled to its α-carbon. After that, we apply two backbone pooling operations,(c) and (d). Lastly, we apply the symmetric operation average, (e), to obtain the final feature vector.
Figure 5: The architecture of our model. The input is composed of atom features and an atomembedding learned together with the network. Each layer is composed of two ResNet (He et al.,2016) bottleneck blocks, for which the radius in angstrom and the number of features are indicated inparentheses, followed by a pooling operation. An illustration of a single ResNet bottleneck blockis presented in the right for D input features (before each convolution we use batch normalizationand a Leaky ReLU). The global protein features are processed by an MLP which computes the finalprobabilities. Protein graphs used in each level are indicated at the bottom.
Figure 7: Visualization of the amino acid clustering matrices obtained with the spectral clusteringalgorithm (von Luxburg, 2007).
