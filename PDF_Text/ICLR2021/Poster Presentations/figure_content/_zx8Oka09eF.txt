Figure 1: Test accuracy of ResNet-18 as a functionof width. Performance improves as width is increased,even though the number of weights is fixed. Please seeSection 2.3 for more details.
Figure 2: Schematic illustration of the methods we use to increase network width while keeping the numberof weights constant. Blue polygons represent weight tensors, red stripes represent non-linear activations, anddiagonal white stripes denote a sparsified weight tensor. We use f to denote the widening factor.
Figure 3: Best test accuracy obtained by ResNet-18 models widened using the bottleneck methods. Theperformance of the baseline network (before introducing bottleneck layers) is denoted by dashed lines. Thelegend indicates the number of weights in the baseline network. In (a), it is equal to the number of weightsbefore introducing bottleneck layers. In (b), it is equal to the total number of weights.
Figure 4: Illustration of our algorithm for distributing sparsity over model layers. The code imple-menting this algorithm is included in Appendix B.
Figure 5: Test accuracy (color-coded) on MNIST obtained by MLP models with 1 hidden layer and ReLU orlinear activations, as a function of overall connectivity (horizontal axis) and of connectivity in the last layer(vertical axis). White stars indicate points with the highest test accuracy.
Figure 6:	Best test accuracy obtained by ResNet-18 models of different width and size (i.e., total number ofweights; approximate value shown in the legend). The leftmost data point of each color corresponds to the densebaseline model, and all subsequent data points correspond to its wider and sparser variants. The decline ofperformance at larger widening is because sparsity at these levels harms the optimization procedure, making thetraining accuracy deteriorate. See Appendix E for the same data plotted as a function of network connectivity,the corresponding training accuracy, and for the version with error bars.
Figure 7:	The fraction of test accuracy improvement (black triangles) obtained by widening the model withoutincreasing the number of weights, compared to increasing the number of weights along with the width. Thedashed vertical line indicates widening factor for maximal training accuracy (red circles). Note that for ImageNet,at width 90 (widening factor 1.4) the sparse model attained a higher test accuracy than the dense model, asreported in Table 1.
Figure 8: MLP with 1 hidden layer and no biases trained on a subset of MNIST. (a) Test accuracy achievedby dense (filled circles) and sparse (crosses) models of different width. (b) Mean squared distance D of thesparse model’s kernel from the infinite-width model’s kernel computed at initialization (both experimental (blue)and theoretical (grey) result), and the test error attained by trained models (pink). The empirical distance D isobtained by averaging the squared distance (ΘGP (x, y) - ΘG∞P (x, y))2 over 104 pairs of test samples and over10 random initializations. See Appendix A for additional details.
Figure 5: All networks are MLPs with one hidden layer, a total of 3970 weights (base width is 5),and either (a) ReLU or (b) Linear activation function. The networks are parameterized accordingto the standard Pytorch implementation (weights and biases are randomly initialized from theuniform distribution). We train these models on MNIST for a fixed number of 300 epochs (ensuringconvergence), with SGD optimizer, no momentum, Cross-Entropy loss, with a constant learning rate0.1 and mini-batch size 100.
Figure 8: The MLP has one hidden layer, no biases, ReLU activation function, and NTK-styleparameterization. It is trained on a subset of 2048 samples from the MNIST training set and tested onthe full MNIST test set. The input is normalized with pixel mean and standard deviation as (image- mean)/stdev. We train for 300 epochs with vanilla SGD using Cross-Entropy loss and batchsize 256. The learning rate was tuned separately for each width. The reported numbers are averagesover 10 random seeds.
Figure 9: Test accuracy achieved by ResNet-18 models on CIFAR-100, comparing performance ofsparse wide models of different size (number of weights indicated by color and printed in the legend)given sparsity distribution in the convolutional layers along all layer dimensions (filled circle) versusalong input/output dimensions only (cross).
Figure 10: Layer-wise sparsity distribution in ResNet-18 of various widths with 1.8e5 weights, forthe CIFAR-100 dataset.
Figure 12: Same data as in Figure 6, but plotted as a function of network connectivity instead of width.
Figure 13:	The improvement in test accuracy due to widening, with constant or increasing number ofweights. Data from the same experiments as in Table 1.
Figure 14:	Training accuracy from the same experiments as in Figures 6 and 12. Larger models attain100% training accuracy. For smaller models, the training accuracy shows a similar behavior as thetest accuracy: It increases up to a certain connectivity and then deteriorates when the connectivitydecreases further.
