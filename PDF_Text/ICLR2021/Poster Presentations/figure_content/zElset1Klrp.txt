Figure 1: a) The regular TA mapping R → Rk , with each output element hi corresponds to a different bin. b)The FTA with η > 0, permitting both overlap in activation, and nonzero gradient between the vertical red andgray lines. c) Larger values for η extends the sloped lines further from either side of each plateau, increasing theregion that has non-negligible gradients.
Figure 2: A visualization of an FTA layerWe now show that the FTA maintains one of the key properties of the tiling activation: sparsity. Thedistinction with many existing approaches is that our sparsity is guaranteed by design and hence is2The word fuzzy reflects that an input can partially activate a tile, with a lower activation than 1, as ananalogy to the concept of partial inclusion and degrees of membership from fuzzy sets.
Figure 3: (a) and (b) contain sample trajectories of Xt which are (a) mildly correlated with d = 0.41and (b) severely correlated with d = 0.98. Both share the same equilibrium distribution (in gray).
Figure 4: Evaluation learning curves of DQN-FTA(black), DQN(red), and DQN-Large(blue), showingepisodic return versus environment time steps. The dotted line indicates algorithms trained with target networks.
Figure 5: Evaluation learning curves of DDPG-FTA(black), DDPG(red), and DDPG-Large(blue), averagedover 10 runs with shading indicating standard error. The dotted line indicates algorithms trained with targetnetworks. The learning curve is smoothed over a window of size 30 before averaging across runs.
Figure 6: Evaluation learning curves of DQN-FTA(black), DQN-RBF(forest green), DQN-L2(red), and DQN-L1(blue), averaging over 20 runs with the shade indicating standard error. Allalgorithms are trained without using target networks. The learning curve is smoothed over a windowof size 10 before averaging across runs.
Figure 7: (a) Evaluation learning curves showing episodic return versus environment time steps of DQN-FTAusing different number of tiles k as labeled. This is equivalent to varying tile width δ as δ = 2u/k. The resultsare averaged over 5 random seeds. (b) Evaluation learning curves of DQN without using a target NN as wechange the number of the second hidden layer units as labeled in the figure. (c) Evaluation learning curves ofDQN-FTA uses [-u, u] as tiling bound where u ∈ {0.01, 0.1, 1.0, 10.0, 20.0}. The results are averaged over10 runs. The standard error is not shown but is sufficient small to differentiate the two groups (i.e., {0.1, 1.0}and {0.01, 10.0, 20.0}) corresponding to appropriate bound and too large/small bound. To generate this figure,we fix on using FTA with 20 tiles (i.e., tile width δ = 2u/20). (d) The overlap-instance sparsity ratio v.s.
Figure 9: Instance sparsity v.s. number of time steps on MountainCar, CartPole, Acrobot, LunarLan-der. The results are averaged over 20 random seeds and the shade indicates standard error.
Figure 10: Overlap sparsity (number of simultaneously activated entries in the sparse feature vectors)v.s. number of time steps by averaging over 20 random seeds and the shade indicates standard error.
Figure 11: DQN-FTA compares with TCNN. Evaluation learning curves are averaged over 20random seeds and the shade indicates standard error. All variants are trained without target networks.
Figure 12:	DQN-FTA trained with linear activation function with different regularization weight(-reg-1.0 means regularization is set as 1.0). One can see that our FTA is not sensitive to this choice asthose learning curves are almost overlapping with each other. Evaluation learning curves are averagedover 10 random seeds and the shade indicates standard error. All variants are trained without targetnetworks.
Figure 13:	Sensitivity of η, δ on LunarLander-v2.
Figure 14:	Evaluation learning curves of of DQN-FTA(black), DQN(red), and DQN-Large(blue), showingepisodic return versus environment time steps. The dotted line indicates algorithms trained with target networks.
Figure 15:	Evaluation learning Curves of DDPG-FTA(black), DDPG(red), and DDPG-Large(blue) onMujoCo environments, averaged over 20 runs with shading indiCating standard error. All algorithms are trainedwithout target networks. The learning Curve is smoothed over a window of size 30 before averaging aCross runs.
Figure 16: Gradient inference measurements on mountain car domains, averaging over 10 runs withthe shade indicating standard error. (d)(e)(f) are measured for those parameters in the second hiddenlayer only (which are supposed to directly affect representation). All algorithms are trained withoutusing target networks except DQN. The curve is smoothed over a window of size 20 before averagingacross runs.
Figure 17:	Evaluation learning curves on mountain car. The results are averaged over 20 runs withthe shade indicating standard error. All algorithms are trained without using target networks.
Figure 18:	(a) The Highway environment. (b) The evaluation learning curve. (c) The cumulative numberof car crashes as a function of driving time steps. Results are averaged over 30 runs with shading indicatingstandard error. The learning curve is smoothed over a window of size 10 before averaging across runs.
Figure 19:	Sample trajectories of {Xt}t∈N (blue) and {St}t∈N (black) across a range of difficulties.
Figure 20:	Left, Middle: Learning curve of loss over stationary distribution during training onlow difficulty (dotted) and high difficulty (solid) settings for two layer neural nets. The curves aresmoothed over a window of 50. Right: The final loss over stationary distribution after 20K trainingiterations across a range of difficulty settings, shown as the mean of 30 runs with the shaded regioncorresponding to p = 0.001. The final loss per run is computed as the mean over the final 2.5Kiterations, with the iid setting d = 0 (dotted) shown for baseline comparison.
Figure 21:	Learning rate sensitivity of FTA and ReLU for iid, mildly correlated, and severelycorrelated Xt (left, middle, right, respectively.) Final loss performance is shown as the mean of 30runs with the shaded region corresponding p = 0.001. These curves corroborate our findings that, ingeneral, FTA prefers lower learning rates (but converges more quickly nonetheless.)(a) FTA(b) ReLU-large	(c) ReLU-large and FTA, d ∈ [0, 1)Figure 22:	Left, Middle: Learning curve for single run using loss over stationary distribution duringtraining on low difficulty (dotted) and high difficulty (solid) settings for two layer neural nets, bothhaving similar numbers of learnable parameters. The curves are smoothed over a window of 50. Right:The final loss over stationary distribution after 20K training iterations across a range of difficultysettings, shown as the mean of 10 runs with the shaded region corresponding p = 0.05. The finalloss per run is computed as the mean over the final 2.5K iterations, with the iid setting d = 0 (dotted)shown for baseline comparison.
Figure 22:	Left, Middle: Learning curve for single run using loss over stationary distribution duringtraining on low difficulty (dotted) and high difficulty (solid) settings for two layer neural nets, bothhaving similar numbers of learnable parameters. The curves are smoothed over a window of 50. Right:The final loss over stationary distribution after 20K training iterations across a range of difficultysettings, shown as the mean of 10 runs with the shaded region corresponding p = 0.05. The finalloss per run is computed as the mean over the final 2.5K iterations, with the iid setting d = 0 (dotted)shown for baseline comparison.
Figure 23: The final loss over stationary distribution after 20K training iterations across a range ofdifficulty settings, shown as the mean of 10 runs with the shaded region corresponding p = 0.001.
