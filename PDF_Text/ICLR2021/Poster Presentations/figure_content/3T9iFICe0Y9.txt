Figure 1: Visualization ofa simple RNN that high-lights a cell (purple), alayer (red) and the initialhidden state of each layer(green). (Best viewed incolor.)Neural Tangent Kernel (NTK). Let fθ (x) ∈ Rd be the output of a DNN with parameters θ. Fortwo input data sequences x and x0, the NTK is defined as (Jacot et al., 2018)θ S(X, XO) = hVθs fθs (X), Vθs fθs (χ0)i,where fθs and θs are the network output and parameters during training at time s.1 Let X andY be the set of training inputs and targets, '(y, y) : Rd X Rd → R+ be the loss function, andL = 责 P(X y)∈χ×γ '(fθs (x), y) be the the empirical loss. The evolution of the parameters θs andoutput of the network fθs on a test input using gradient descent with infinitesimal step size (a.k.agradient flow) with learning rate η is given byd∂θs = -ηvθs fθs(X)TTVfes (X)Ldfθs(X)∂s-ηVθsfθs(X)Vθsfθs(X)TVfθs(X)L= -ηΘbs(X,X)Vfθs(X)L.
Figure 2: Empirical demonstration of a wide, single-layer RNN converging to its limiting RNTK. Left:convergence for a pair of different-length inputs x = {1, -1, 1} and x0 = {cos(α), sin(α)}, with varyingα = [0, 2π]. The vertical axis corresponds to the RNTK values for different values of α. Right: convergence ofweight-tied and weight-untied single layer RNN to the same limiting RNTK with increasing width (horizontalaxis). The vertical axis corresponds to the average of the log-normalized error between the empirical RNTKcomputed using finite RNNs and the RNTK for 50 Gaussian normal signals of length T = 5.
Figure 3: Per time step t (horizontal axis) sensitivity analysis (vertical axis) of the RNTK for the ReLU (toprow) and erf (bottom row) activation functions for various weight noise hyperparameters. We also experimentwith different RNTK hyperparameters in each of the subplots, given by the subplot internal legend. Clearly, theReLU (top-row) provides a more stable kernel across time steps (highlighted by the near constant sensitivitythrough time). On the other hand, erf (bottom row) sees a more erratic behavior either focusing entirely on earlytime-steps or on the latter ones.
Figure 4: Performance of the RNTK on the synthetic sinusoid and real-world Google stock price data setscompared to three other kernels. We vary the input lengths (a,c), the input noise level (b), and training set size(d). We compute the average SNR by repeating each experiment 1000 times. The RNTK clearly outperforms allof the other kernels under consideration. Figure 4b suggests that the RNTK performs better when input noiselevel is low demonstrating one case where time recurrence from RNTK might be sub-optimal as it collects andaccumulate the high noise from each time step as opposed to other kernels treating each independently.
