Figure 1: An example use case of task reduction, the key technique of our approach. The task is topush the blue box to the red position.
Figure 2: Heatmap of learned value function. (a) current state; as a test-time planner. We(b) value for moving the elongated box elsewhere; (c) value for evaluate the success rates onmoving the cubic box to goal w.r.t. different elong. box pos.; (d) test tasks when additional TRcomposite value for different reduction targets: moving elong. steps are allowed in the execu-box away from door yields high values for task reduction.	tion of the SAC-trained policy.
Figure 3: Avg. success rate w.r.t. samples overcurrent task distribution (left) and in hard cases(right) in the Push scenario.
Figure 4: SIR-learned strategy for stacking 3 boxes in the Stack scenario. The yellow spot in the airis the goal, which requires a yellow box to be stacked to reach it.
Figure 5: Training results in the Stack sce-nario with at most 3 boxes. We evaluate suc-cess rate across training curriculum (left) andin hard cases, i.e., stacking 3 boxes (right).
Figure 6: SIR-learned strategy for a hard case in the 4-Room maze scenario. The agent(green) istasked to move the blue box to the target(red). All three doors are blocked by elongated boxes, so theagent needs to clear them before pushing the blue box towards the target.
Figure 7: Results in 4-Room maze. Left: suc-cess rate in the training task distribution; Right:success rate evaluated only in the hard cases.
Figure 8: Navigation in U-Wall maze with im-age input. Left: task visualization; Right: avg.
Figure 9: Comparison of HRL baselines and SIR in the fixed-goal Maze scenarios.
Figure 10: Comparison with curriculumlearning methods in the U-Shape maze (fixedgoal). We consider SIR with a uniform tasksampler and standard PPO with GoalGANand a manually designed curriculum.
Figure 11: Heatmap of value function with different composition operators. (a) value for movingelongated box elsewhere, (b) value for moving cubic box to goal w.r.t. different elongated boxposition, (c) composite value with product operator, (d) composite value with min operator, (e)composite value with average operator.
Figure 12: Values of all the reduction targets with different baseline algorithms. Green: successfultargets; Orange: failed targets; Red (in the right plot): running average of values.
Figure 13:	Training results of SAC-based algorithms in the Push scenario with 2 different tasksamplers. In each case, we show avg. success rate w.r.t. samples over current task distribution (left)and in hard cases (right).
Figure 14:	Training results of PPO-based algorithms in the Push scenario with 2 different tasksamplers. In each case, we show avg. success rate w.r.t. samples over current task distribution (left)and in hard cases (right).
Figure 15: SIR-learned strategy for a hard case in the Stack scenario with 2 boxes. The agent aims tostack the blue box towards the blue spot which is two-stories high.
Figure 16: Training results in the Stack scenario with a maximum of 2 boxes. We evaluate successrate across training curriculum (left) and in hard cases, i.e., stacking 2 boxes (right).
Figure 17: Comparison between SAC andPPO in terms of wall-clock time in 3-Roommaze scenario. The x-axis is measured inseconds.
Figure 18: Results of SAC-based algorithms in3-Room maze.
Figure 19: Complete training results of PPO-based algorithms in 4-Room maze.
