Figure 1: Visualization of GraphCNF for an example graph of five nodes. We add the node and edgeattributes, as well as the virtual edges stepwise to the latent space while leveraging the graph structurein the coupling layers. The last step considers a fully connected graph with features per edge.
Figure 2: Visualization of molecules from (a) the Zinc250k dataset and (b,c) generated by GraphCNF.
Figure 3: Example graphwith |V | = 26 and generatedvalid coloring by GraphCNF.
Figure 4: Results on language modeling. The reconstruction error is shown in a lighter colorcorresponding to the model. The exact results including standard deviations can be found in Table 11.
Figure 5: Visualization of the mixture model encoding and decoding for 3 categories. Best viewed incolor. (a) Each category is represented by a logistic distribution with independent mean and scalewhich are learned during training. (b) The posterior partitions the latent space which we visualizeby the background color. The borders show from when on we have an almost unique decoding ofthe corresponding mixture (> 0.95 decoding probability). Note that these borders do not directlycorrespond to the euclidean distance as we use logistic distributions instead of Gaussians.
Figure 6: Visualization of the mixture model encoding of the edge attributes for a trained model onmolecule generation. The majority of the space is assigned to category 1, the single bond, as it is byfar the most common edge type. Across multiple models however, we did not see a consistent trendof position/standard deviation of each categoryâ€™s encoding.
Figure 7:	Visualization of the linear flow encoding and decoding for 3 categories. Best viewed incolor. (a) The distribution per category is not restricted to a simple logistic and can be multi-modal,rotated or transformed even more. (b) The posterior partitions the latent space which we visualize bythe background color. The borders show from when on we have an almost unique decoding of thecorresponding category distribution (> 0.95 decoding probability).
Figure 8:	Visualization of the encoding distribution q(z|x) of latent NF on set summation.
Figure 9: Plotting the loss over batch iterations on the language modeling dataset text8 for LatentNFand CNF. The batch size is 128, and we average the loss over the last 10 iterations. Sub-figure (b)shows a zoomed version of sub-figure (a) to show the larger fluctuation even on smaller scale, whileCNF provides a smooth optimization.
Figure 10: Plotting the loss over batch iterations on the language modeling dataset Wikitext103for LatentNF and CNF. The batch size is 128, and we average the loss over the last 10 iterations.
Figure 11: Visualization of molecules generated by GraphCNF which has been trained on theZinc250k (Irwin et al., 2012) dataset. Nodes with black connections and no description representcarbon atoms. All of the presented molecules are valid. Best viewed in color and electronically forlarge molecules.
Figure 12: Failure cases in molecule generation besides sub-graph generation. The invalid edges havebeen indicates with a green arrow. Changing the edges to single bonds in the examples above wouldconstitute a valid molecule.
Figure 13: Examples of valid graph color assignments from the dataset (best viewed in color). Due tothe graph sizes and dense adjacency matrices, edges can be occluded or cluttered in (c) and (d).
