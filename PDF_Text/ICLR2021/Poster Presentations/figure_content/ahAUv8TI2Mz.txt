Figure 1: Overview of AGZSL. The same color reflects the same class association between visualfeatures and attributes. (a) Image-adaptive mechanism (marked as purple) based on x yields properattention to adapt the semantic features of a and a0 so that their S2V embedding (marked as orange)displays the desired effects in X (attracted to x) and in X0 (repelled from x). (b) We generatevirtual classes from a pair of classes of semantic features a and a0 with selected weights λ via betadistribution B(α, β). Xv/X0v and av/a0v represent visual and semantic features of virtual classes.
Figure 2: The proposed seen and unseen experts. In the learning phase, the seen expert fs is trainedwith seen visual features xs, seen semantic features As, and corresponding class labels ys, while theunseen expert fu is trained via meta-learning with virtual data. In the inference phase, given a testingsample x, fs is additionally fed with all semantic features A, and fu only with unseen semanticfeatures Au. The decision rule leading to the class label y is based on (2). Note that in both trainingand inference stages, the image-adaptive mechanism to semantic features is carried out.
Figure 3: The architectures of IAS and S2V (FC: linear layer, L2 : L2-normalization).
Figure 4: The cosine similarity margin between the correct class and other nearest class. The bluearea depicts the cosine similarity margin of baseline without IAS and generative learning. On theother hand, the orange area represents the cosine similarity margin of ours. Most of the samplesimprove their margin by our method, suggesting the proposed techniques are beneficial for GZSL.
Figure 5: The semantic features of virtual/seen/unseen classes over the four datasets.
Figure 6: The difference of cosine similarity between our model and baseline. In our experiment,most of the samples improve their embedding with IAS and virtual classes.
Figure 7: Beta distribution.
