Figure 1: Percentage of weights kept after SBP applied to a randomly initialized FFNN with depth 100 andwidth 100 for 70% sparsity on MNIST. Each pixel (i, j) corresponds to a neuron and shows the proportion ofconnections to neuron (i, j) that have not been pruned. The EOC (a) allows us to preserve a uniform spread ofthe weights, whereas the Chaotic phase (b), due to exploding gradients, prunes entire layers.
Figure 2: Percentage of non-pruned weights per layer in a ResNet32 for our Stable ResNet32 and standardResnet32 with Kaiming initialization on CIFAR10. With Stable Resnet, we prune less aggressively weights inthe deeper layers than for standard Resnet.
Figure 3: Accuracy on MNIST with different initialization schemes including EOC with rescaling, EOC withoutrescaling, Ordered phase, with varying depth and sparsity. This shows that rescaling to be on the EOC allows usto train not only much deeper but also sparser models.
Figure 4: Percentage of pruned weights per layer in a ResNet32 for our scaled ResNet32 and standardResnet32 with Kaiming initialization36Published as a conference paper at ICLR 2021(b) ELU with EOC Init(c) ELU with Ordered phaseInit(a) ELU with EOC Init &Rescaling(e) Tanh with EOC Init(d) Tanh with EOC Init &RescalingFigure 5: Accuracy on MNIST with different initialization schemes including EOC with rescaling,EOC without rescaling, Ordered phase, with varying depth and sparsity. This figure clearly illustratesthe benefits of rescaling very deep and sparse FFNN.
Figure 5: Accuracy on MNIST with different initialization schemes including EOC with rescaling,EOC without rescaling, Ordered phase, with varying depth and sparsity. This figure clearly illustratesthe benefits of rescaling very deep and sparse FFNN.
