Figure 1: Self-supervised contrastive learning relies on data augmentations as depicted in (a) tolearn visual representations. However, current methods introduce inductive bias by encouragingneural networks to be less sensitive to information w.r.t. augmentation, which may help or may hurt.
Figure 2: Framework of the Leave-one-out Contrastive Learning approach, illustrated with twotypes of augmentations, i.e., random rotation and color jittering. We generate multiple views withleave-one-out strategy, then project their representations into separate embedding spaces with con-trastive objective, where each embedding space is either invariant to all augmentations, or invariantto all but one augmentation. The learnt representation can be the general embedding space V (blueregion), or the concatenation of embedding sub-spaces Z (grey region). Our results show that eitherof our proposed representations are able to outperform baseline contrastive embeddings and do notsuffer from decreased performance when adding augmentations to which the task is not invariant(i.e., the red X’s in Figure 1).
Figure 3: Top nearest-neighbor retrieval results ofLooC vs. corresponding invariant MoCo base-line with color (left) and rotation (right) augmentations on IN-100 and iNat-1k. The results showthat our model can better preserve information dependent on color and rotation despite being trainedwith those augmentations.
Figure 4: Histograms of correct predictions (activations × weights of classifier) by eachaugmentation-dependent head from IN-100 and iNat-1k. The classifier on IN-100 heavily relieson texture-dependent information, whereas it is much more balanced on iNat-1k. This is consistentwith the improvement gains observed when learning with multiple augmentations.
