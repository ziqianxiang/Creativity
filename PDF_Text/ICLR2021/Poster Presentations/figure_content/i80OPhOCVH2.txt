Figure 1: The bottleneck that existed in RNN seq2seq models (before attention) is strictly moreharmful in GNNs: information from a node’s exponentially-growing receptive field is compressedinto a fixed-size vector. Black arrows are graph edges; red curved arrows illustrate information flow.
Figure 2: The NEIGHBORSMATCH: green nodes (a, <b, (c) have blue neighbors (■) and analphabetical label. The goal is to predict the label (A, B, or C) of the green node that has the samenumber of blue neighbors as the target node ( ? )inthe same graph. In this example, the correct labelis C because the target node has two blue neighbors, like the node marked with C in the same graph.
Figure 3: Accuracy across problem radius (tree depth)in the NeighborsMatch problem. Over-squashingstarts to affect GCN and GIN even at r = 4.
Figure 5: An example of a TREE-NEIGHBORSMATch, that is an instance of the general Neigh-BORSMATCH problem that We examine in Section 4. The target node ( ? ) is the root of a tree ofdepth=3 (from the target node to the green nodes). The green nodes ( a, ∣b, (c , ...) have blueneighbors (C) and an alphabetical label. The node B has a single blue neighbor; the node (C hastwo blue neighbors; and the node <d has no blue neighbors; each other green node has another uniquenumber of blue neighbors. The goal it to predict a label for the target node (?) according to itsnumber of blue neighbors. The correct answer is C in this example, because the target node hastWo blue neighbors, like the green node that is marked With C in the same graph. To make a correctprediction, the network must propagate information from all leaves toward the target node, and makethe decision given a single fixed-sized vector that compresses all this information.
