Figure 1: The points represent items shaded by their relevances, and the contours represent thepredicted scores. The minority items lie on the horizontal z1-axis because their z2 value is corruptedto 0. The blue star and black star correspond to minority and majority items that are close in the fairmetric with nearly the same relevance. However, they have wildly different predicted scores underthe baseline. Using SenSTIR, as ρ increases, they eventually have the same predicted scores.
Figure 2: The (i, j )-th entries of these heatmaps represent the proportion of times that the i-th rankeditem is moved to position j under the corresponding hypothetical ranking. With large enough ρ,SenSTIR ranks the original queries and hypothetical queries similarly as desired.
Figure 3: Individual (left) and group fairness (right) versus accuracy for the German credit data setGroup Exposure for Agebetween test queries and their gender-flipped hypotheticals versus the average stochastic NDCG. Themaximum Kendall’s tau correlation is 1, which SenSTIR achieves with relatively high NDCG. Weemphasize that the sensitive subspace that SenSTIR utilizes to define the fair query metric directlyrelates to age, not gender. In other words, our goal is to mitigate unfairness that arises from agein the training data, not gender. However, age is correlated with gender, so this metric shows theindividually fair properties of SenSTIR generalize beyond age on the test set. We imagine that MLsystems can be unfair to people with respect to features that can be difficult to know before deployingthese systems, so although flipping gender is a simplistic choice, it illustrates that SenSTIR canbe meaningfully individually fair with respect to these potentially unknown features that were notgiven special consideration when choosing the fair metric or in training with SenSTIR. Furthermore,SenSTIR gracefully trades off NDCG for individual fairness unlike Fair-PG-Rank. “Project" is worsein terms of individual fairness than vanilla training without enforcing fairness. Without direct ageinformation, perhaps “Project" must more heavily rely on gender to learn accurate rankings, whichillustrates that SenSTIR’s generalization properties from age to gender are non-trivial. Disparity ofgroup exposure (where smaller numbers are better) versus NDCG is depicted on the right plot ofFigure 3. This group fairness metric is exactly what Fair-PG-Rank regularizes with. On average, forthe same value of NDCG, SenSTIR typically outperforms Fair-PG-Rank showing that individualfairness can be adequate for group fairness but not vice versa. While “Project" improves mildly upon
Figure 4: Individual (left) and group fairness (right) versus NDCG for the MSLR data setGroup Exposure for Quality6	ConclusionWe proposed SenSTIR, an algorithm to learn provably individually fair LTR models with an optimaltransport-based regularizer. This regularizer encourages the LTR model to produce similar rankingpolicies, i.e., distributions over rankings, for similar queries where similarity is defined by a fairmetric. Our notion of a fair ranking is complementary to prior definitions that require allocatingexposure to items fairly with respect to merit. In fact, we empirically showed that enforcing individualfairness can lead to allocating exposure fairly for groups but allocating exposure fairly for groupsdoes not necessarily lead to individually fair LTR models. An interesting future work direction isstudying the fairness of LTR systems in the context of long-term effects (Mladenov et al., 2020).
