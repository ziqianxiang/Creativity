Figure 1: Real image manipulation on scene (top two rows, photo from Flickr) and face (bottomtwo rows, unseen image from CelebA-HQ) using pretrained StyleGAN2 (Karras et al., 2019b): Wereconstruct the real images (col.1) by finding a latent vector with the best inversion result (col.2) onStyleGAN2 (Abdal et al., 2019). After that, we transform the latent vectors for single- and multiple-attribute manipulations (col.3-6). Note that unlike ours, the baseline method (Shen et al., 2019)either changes image identity or confounds semantic properties, or both.
Figure 2: Our overall framework (top row) and 3 training strategies different from prior work(bottom row). In (a), G takes z and an edited latent vector separately to synthesize images.
Figure 3: Comparison of image-space editing approaches with respect to “removing clouds”(top), “enhancing night” (middle) and “adding snow” (bottom). The original images (col.1) arefollowed by a given editing task. From (a-e): (a) CycleGAN (Zhu et al., 2017a), (b) StarGANv2 (Choi et al., 2020), (c) RelGAN (Wu et al., 2019), (d) DRIT++ (Lee et al., 2020), and (e) Ours.
Figure 4: Comparison of latent-space editing approaches on StyleGAN2. (a) Shen et al. (2019);(b) Voynov & Babenko (2020); (c) Ours. Original synthetic image (col.1); edited results of a seman-tic manipulation noted on top (col.2-4; col.5-7). Shen & Zhou (2021) mention that “age,” “gender,”and “glasses” directions are hard to disentangle potentially due to data bias, while our results suggesta better direction disentanglement.
Figure 5: User study for image editing and image identity preservation. Blue bars show thepredicted presence of a given attribute for images (I +, higher is better). The orange bars measure-Smile - Smile + Smile	+ Smile - Smile+ AgeFigure 6: Real image synthesis with multiple attributes. Shen et al. (2019) (top row); ours(bottom row): original real image (col.1); inverted result (col.2) from Abdal et al. (2019); editedresults (col.3-7) with target attribute changes shown at the bottom.
Figure 6: Real image synthesis with multiple attributes. Shen et al. (2019) (top row); ours(bottom row): original real image (col.1); inverted result (col.2) from Abdal et al. (2019); editedresults (col.3-7) with target attribute changes shown at the bottom.
Figure 7: Continuous editing results on PGGAN (Karras et al., 2017) with the “black hair” (left)and the “smile” (right) attribute.
Figure 8: Comparison of global and local transformations on PGGAN. Original synthetic images(col.1); the edited results on enhancing “smile” (col.2-5): (a) a global T , (b) a local T , with linear-layer direction functions, (c) a local T with MLP direction functions trained via Lreg loss only, and(d) a local T with MLP direction functions training via the entire loss L.
Figure 9: Example MLP architecture on PGGAN.
Figure 10: Visual comparisons of “Smile” editing with different reconstructed images: (col.1)Original image; (col.2) reconstructed images with 500 (top) and 4k (bottom) inversion optimizationsteps; (col.3-6) results of editing the “Smile” attribute.
Figure 11: Visual comparisons of single- and joint-distribution training. Single-distributiontraining refers to train one attribute direction at a time, while joint-distribution training means totrain multiple attribute directions simultaneously.
Figure 12: Additional results. Continuous image edits using StyleGAN2 (Karras et al., 2019b) onthe “clouds” attribute.
Figure 13: Additional results. Continuous image edits using StyleGAN2 (Karras et al., 2019b) onthe “brightness” attribute.
Figure 14: Additional results. Continuous image edits using StyleGAN2 (Karras et al., 2019b) onthe “snow” attribute.
Figure 15: Additional results. Continuous image edits using StyleGAN2 (Karras et al., 2019b) onthe “summer” attribute.
