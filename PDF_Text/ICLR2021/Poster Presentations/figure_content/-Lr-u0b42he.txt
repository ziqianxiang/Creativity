Figure 1: Given a single image-language example regarding new concepts (e.g., blue and carrot), ourmodel can parse the object into its shape and style codes and ground them with Blue and Carrot labels,respectively. On the right, we show tasks the proposed model can achieve using this grounding.(a) Itcan detect the object under novel style, novel pose, and in novel scene arrangements and viewpoints.
Figure 2: Architecture for disentangling 3D prototypical networks (D3DP-Nets). (a) Givenmulti-view posed RGB-D images of scenes as input during training, our model learns to map a singleRGB-D image to a completed scene 3D feature map at test time, by training for view prediction. Fromthe completed 3D scene feature map, our model learns to detect objects from the scene. (b) In each 3Dobject box, we apply a shape-style disentanglement autoencoder that disentangles the object-centricfeature map to a 3D (feature) shape code and a 1D style code. (c) Our model can compose thedisentangled representations to generate a novel scene 3D feature map. We urge the readers to referthe video in the supplimentary material for an intuitive understanding of the architecture.
Figure 3: D3DP-VQA Modular Networks. Given a question-image pair and a list of learnedprototype dictionaries (left), D3DP-Nets parse the visual scene to object shapes, styles, locations andsizes codes (top-right), while the semantic language parser converts the question to an executableprogram. The generated program is executed sequentially to answer the question (bottom-right).
Figure 4: Replica dataset. On the left, We show two objects in different scenes belonging to thesame shape cateogry 'Plant’. On the right, we show two objects belonging to the same style category‘Cream’.
Figure 6: (a) View predictions of D3DP-Nets and GQN (ESIami et al., 2018) in novel scenes. (b)Generating novel scenes using only a single example for each style and shape category.
Figure 5: t-SNE visualization on styles codes.
Figure 7: Example scene/QA pairs from the training dataset used for VQAQ: How many cheeses areeither small gray shinyobjects or small objects?A: 2Q: What material is theother gray thing that is thesame shape as the tinymatte object?A: metalFigure 8: Example scene/QA pairs from the novel only one shot test dataset used for VQA.
Figure 8: Example scene/QA pairs from the novel only one shot test dataset used for VQA.
Figure 9: (a) The left scene/question pair is from the in domain test set, and the right scene/questionpair is from the one shot test set. The colors, materials, sizes, and spatial relationships tested in bothsplits are the same. The only difference is that the one shot test set contains shapes the model did notsee while training and was only exposed to one example before the testing phase. (b) The prototypeimages shown to the model before starting the one shot testing phase.
Figure 11: Contrasting examples for each concept category. First column specifies the concept andthe contrasting attributes shown for that concept. Next two columns show the images differing onlyin that specified concept.
Figure 10: RGB neural renders of the novel3D scene feature maps generated by ourimagination module. Although we don’tget pixel accurate generation, our synthe-sized 3D feature map encodes the semanticstructure of the scene.
Figure 12: Generating novel scenes using only a single example for each style and content class.
