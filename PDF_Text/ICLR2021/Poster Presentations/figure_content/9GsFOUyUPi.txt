Figure 1: Test accuracies on CIFAR-10 (ResNet50) for different pruning methods. Each point is theaverage over 3 runs of prune-train-test. The shaded areas denote the standard deviation of the runs(too small to be visible in some cases). Random corresponds to removing connections uniformly.
Figure 2: Test accuracies on CIFAR-10 for different pruning methods. With increased number ofbatches (-MB) one-shot methods are more robust at higher sparsity levels, but our gradual pruningapproaches can go even further. Moreover, FORCE consistently reaches higher accuracy than othermethods across most sparsity levels. Each point is an average of â‰¥ 3 runs of prune-train-test. Theshaded areas denote the standard deviation of the runs (too small to be visible in some cases). Notethat in (b), GRASP and GRASP-MB overlap.
Figure 3: Test accuracies on CIFAR-100 and Tiny Imagenet for different pruning methods. Eachpoint is the average over 3 runs of prune-train-test. The shaded areas denote the standard deviation ofthe runs (too small to be visible in some cases).
Figure 4: Left: FORCE saliency (6) obtained with (7) when varying T normalized by the saliencywith one-shot SNIP (T = 1). Pruning iteratively brings more gains for higher sparsity levels. Errorbars not shown for better visualization. Middle: Test acc pruning with FORCE and Iter SNIP at99.5% sparsity for different T . Both methods are extremely robust to the choice of T . Right: Walltime to compute pruning masks for CIFAR10/Resnet50/TeslaK40m vs acc at 99.5% sparsity; (x b)means we used x batches to compute the gradients while (x it) denotes we used x pruning iterations,with one batch per iteration. Numbers in red indicate performance below random pruning.
Figure 5: Test accuracies on CIFAR-10/100 and Tiny Imagenet for different pruning methods. Eachpoint is the average over 3 runs of prune-train-test. The shaded areas denote the standard deviation ofthe runs (too small to be visible in some cases).
Figure 6: FORCE saliency (6) obtained with iterative pruning normalized by the saliency obtainedwith one-shot SNIP, T = 1. (a) Applying the FORCE algorithm (b) Using Iterative SNIP. Note howboth methods have similar behaviour.
Figure 7: FORCE saliency computed for masks as we vary sparsity. FORCE (sparsification) refers tomeasuring FORCE when we allow the gradients of zeroed connections to be non-zero, while FORCE(pruning) cuts all backward signal of any removed connection. As can be seen on the plot, they arestrongly correlated.
Figure 8: Visualization of remaining weights after pruning 99.9% of the weights of Resnet-50 andVGG-19 for CIFAR-10. (a) and (d) show the fraction of remaining weights for each prunable layer.
Figure 9: Normalized amount of globally pruned and recovered weights at each pruning iteration forResnet50, CIFAR-10 when pruned with (a) Iterative SNIP and (b) FORCE. As expected, the amountof recovered weights for Iterative SNIP is constantly zero, since this is by design. Moreover, theamount of pruned weights decays exponentially as expected from our pruning schedule. On the otherhand, we see the amount of recovered weights is non-zero with FORCE, interestingly the amount ofpruned/recovered weights does not decay monotonically but has a clear peak, indicating there is an"exploration" and a "convergence" phase during pruning.
Figure 10: Test accuracies on CIFAR-10 for different pruning methods. Each point is the averageover 3 runs of prune-train-test. The shaded areas denote the standard deviation of the runs (too smallto be visible in some cases). Early pruning is at most on par with gradual pruning at initializationmethods and has a strong drop in performance as we go to higher sparsities.
Figure 11: Test accuracies on CIFAR-10 for different pruning methods on Mobilenet architecture.
Figure 12: Test accuracies for different datasets and networks when pruned with different methods.
Figure 13: (a) - (c) Portion of remaining weights for each layer. Each point is an average of 3 runsand the error bars (hardly visible) denote standard deviation. (b) - (d) Portion of remaining weightsfor each intermediate mask that is computed during iterative pruning with global portion of remainingweights of 0.01.
