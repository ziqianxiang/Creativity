Figure 1: Illustration of our theoretical results. The goal of the SEDSC model is to train a network(typically an auto-encoder) to map data from a union of non-linear manifolds (Left) to a union oflinear subspaces (Center). However, we show that for many of the formulations that have beenproposed in the literature the global optimum of the model will have a degenerate geometry in theembedded space. For example, in the Dataset and Channel/Batch normalization schemes Theorem2 shows that the globally optimal geometry will have all points clustered near the origin with theexception of two points, which will be copies of each other (to within a sign-flip) (Right).
Figure 2: Clustering accuracy results for YaleB (38 faces), COIL100, and ORL datasets with(Dashed Lines) and without (Solid Lines) the post-processing step on C matrix proposed in Jiet al. (2017). (Raw Data) Clustering on the raw data. (Autoenc only) Clustering features from anautoencoder trained without the F(Z, C) term. (Full SEDSC) The full model in (3).
Figure 3: Results for synthetic data using the dataset normalization scheme. (Left) Original datapoints (Blue) and the data points at the output of the autoencoder when the full model (3) is used(Red). (Center Left) Data representation in the embedded domain when just the autoencoder istrained without the F (Z, C) term (Blue) and the full SEDSC model is used (Red). (Center Right)The absolute value of the recovered C encoding matrix when trained with the full model. (Right)Same plot as the previous column but with a logarithmic color scale to visualize small entries.
Figure 4: Experiments on Extended Yale B dataset. (Left) The norm of the embedded representa-tion Z as training proceeds. (Right) The singular values of the embedded representation of pointsfrom one class, normalized by the largest singular value. (Raw) The singular values of the raw data.
Figure 5: Showing results for the synthetic dataset for three normalization schemes (along therows). Instance Normalization (top); Dataset Normalization (center); Batch/Channel Normalization(bottom). The columns are the same as described in the main paper. (Left) Original data points(Blue) and the data points at the output of the autoencoder when the full model (3) is used (Red).
