Figure 1: Schematic diagram of Neural ODE Processes. Left: Observations from a time series, thecontext set ∙, are encoded and aggregated to form r which parametrises the latent variables D andL0 . Middle: A sample is drawn from L0 and D, initialising and conditioning the ODE, respectively.
Figure 2: Graphical model ofNDPs. The dark nodes de-note observed random vari-ables, while the light nodesdenote hidden random vari-ables. IC and IT representthe indexing sets for the con-text and target points, respec-tively. Full arrows show thegenerative process. Dotted ar-rows indicate inference.
Figure 3: We present example posteriors of trained models and the loss during training of the NP andNDP models for the sine data. We find that NDPs are able to produce a greater range of functionswhen a single context point is provided, and a sharper, better targeted range as more points in thetime series are observed. Quantitatively, NDPs train to a lower loss in fewer epochs, as may beexpected for functions that are generated by ODEs. Both models were trained for 30 epochs.
Figure 5: NPs and NDPs on the Lotka-Volterra task. Black is used for targets or ground truth, solidlines for mean predictions over 50 samples, and dashed lines for sample trajectories. In the left andmiddle plots, the shaded regions show the min-max range over 50 samples, in the right plot theshaded region was produced using kernel density estimation. Left: NPs are less able to model thedynamics, diverging from the ground truth even in regions with dense context sampling, whereasthe NDP is both more accurate and varies more appropriately. Middle: Plotting the theoreticallyconserved quantity V better exposes how the models deviate from the ground truth Right: In phasespace (u, v) the NDP is more clearly seen to better track the ground truth.
Figure 6: Predictions on the test set of Variable Rotating MNIST. NDP is able to extrapolate beyondthe training time range whereas NP cannot even learn to reconstruct the digit.
Figure 7: Training plots of NDP with ODEs of different sizes, training on the sine dataset. We seethat for dim(l) = 1, the model trains slowly, as would be expected for a sine curve where at least 2dimensions are needed to learn second order and test performance is close to the standard NP. Theother models train at approximately the same rate.
Figure 8:	Training model variants on 1D synthetic datasets. NPs train slower in all cases. AllNeural ODE Process variants train approximately at the same rate. With the latent-only variantsperforming slightly worse than the more expressive model variants. Additionally, ND2P performsslightly better than NDP on the damped oscillator and linear sets, because they are naturally easierto learn as second-order ODEs.
Figure 9:	Training NP and NDP on the Lotka-Volterra equations. Due to the additional encodingstructure of NDP, it can be seen that NDPs train in fewer iterations, to a lower loss than NPs.
Figure 10: Predictions on the simpler Rotating MNIST dataset. NPs are also able to perform wellon this task, but NDPs are not able to extrapolate beyond the maximum training time.
Figure 12: NPs and NDPs training on handwriting. NDPs perform slightly better, achieving alower loss in fewer iterations. However this is a marginal improvement, and we believe it is downto significant diversity in the dataset, due to there being no fundamental differential equation forhandwriting.
Figure 13: We test the models on drawing the letter “a” with varying numbers of context points.
