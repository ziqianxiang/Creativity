Figure 1: Our method performs knowledge distillation by minimizing the discrepancy between thepenultimate feature representations hT and hS of the teacher and the student, respectively. To thisend, we propose to use two losses: (a) the Feature Matching loss LFM, and (b) the so-called Soft-max Regression loss LSR. In contrary to LFM, our main contribution, LSR, is designed to takeinto account the classification task at hand. To this end, LSR imposes that for the same input im-age, the teacher’s and student’s feature produce the same output when passed through the teacher’spre-trained and frozen classifier. Note that, for simplicity, the function for making the feature di-mensionality of hT and hS the same is not shown.
Figure 2: Visualization of hS and hT on the test set of CIFAR-100. Better viewed in color.
