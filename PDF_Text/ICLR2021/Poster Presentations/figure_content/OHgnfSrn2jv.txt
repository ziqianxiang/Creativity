Figure 1: Different optimization methods using an objective L(θ) = Eqθ [ψ(x)] where qθ is agaussian of 100 dimensions with parameters θ = (μ,v). Here μ in bold is the mean vector, Vparameterizes the covariance matrix Σ, which is chosen to be diagonal. Two parameterizations forthe covariance matrix are considered: Σii = evi (log-diagonal) and Σii = vi (diagonal). ψ(x) is thesum of sinc functions over all dimensions. Training is up to 4000 iterations, with λ = .9 and β = .1unless they are varied. In Figure 1 (c), σ and μ refer to the Std of the first component of the gaussianσ = √Σ11 and μ = μ1. More details about the experimental setting are provided in Appendix D.3.
Figure 2: WNG-based algorithms provide large gains on tasks where initial progress is difficult.
Figure 3: Condition numbers for different tasks.
Figure 4: WNES methods more reliably overcome local maxima. Results obtained on the point(a) and quadruped (b) tasks. The mean ± standard deviation is plotted across 5 random seeds.
Figure 5: WNG methods improve computational efficiency. Average reward per minute is plottedfor both gradient tasks (left) and ES tasks (right) for the runs depicted above.
Figure 7: A visualization of the quadruped task. The agent receives receives more reward the closerit is to the goal (green). A naive agent will get stuck in the local maximum at the wall if it attemptsto move directly to the goal.
