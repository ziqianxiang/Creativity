Figure 1: A typical example of the different behaviors of BO, LS and our proposed BlendSearchin tuning a set of 11-dim hyperparameters for XGBoost. BO is prone to selecting expensive butnot necessarily good configs. LS avoids expensive configs in the beginning but is prone to gettingstuck in local regions. BlendSearch switches between one BO and multiple LS search threadsand prioritizes the more promising ones, and turns out to try more low-cost, high-quality configs.
Figure 2: Framework. Both paths from ‘Config Evaluator’ are executed independently.
Figure 3:	Optimization performance curve for XGBoost. Lines correspond to the mean loss over 10folds, and shades correspond to 95% confidence intervals. 1-auc is used for binary classification andlog-loss is used for multi-class classification.
Figure 4:	Aggregated rank and scaled loss on LightGBM and XGBoost. The scaled loss is obtainedby min-max scaling using the max and min loss over all time across all the methods. Sub-figure (a)& (b) share the same legend, and BS is short for BlendSearch in sub-figure (c). Both rank and scaledregret are aggregated first across datasets then across 10 folds of data in each dataset.
Figure 5:	Search thread selection inBlendSearch in tuning XGBoost on datasetKDDCup09. In this figure (and Figure 1(b),which shows the result for dataset Volkert), eachmarker corresponds to one time of search threadselection in BlendSearch. The diamondscorrespond to the global search thread, andthe solid circles correspond to the local searchthreads (different colors for different threads).
Figure 6: The distributions of the # of threadswitches and the # of local search threads in-duced by the GS (short for global search) inBlendSearch for tuning XGBoost. The resultis aggregated over all the datasets evaluated intunning XGBoot. The outliers with very large# of thread switches and threads induced by GSare from very small datasets, where the numberof evaluations is large.
Figure 7: Optimization performance curve for LightGBM (1h).
Figure 8: Optimization performance curve for LightGBM (4h).
Figure 9: Optimization performance curve for XGBoost (4h).
Figure 10:	Aggregated results on LightGBM and XGBoost. ‘PurneWithSample’ and ‘PruneWith-Iter’ represent ASHA using sample size and iteration number as resource dimension respectively.
Figure 11:	Aggregated rank and scaled loss on LightGBM with random initialization.
Figure 12:	Performance curves on cane, aggregated rank and aggregated loss on DeepTables.
