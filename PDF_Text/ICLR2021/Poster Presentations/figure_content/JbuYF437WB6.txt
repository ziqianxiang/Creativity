Figure 1: Processing of node v = 3 (orange). For each layer `, we collect representations h`vfor all nodes V in a matrix H', where each row represents one node. The initial feature matrix isX = H0. In the first layer, the representations of the direct predecessors P(v) = {0, 1, 2} (blue)have been computed; they are aggregated together with the past representation of v (orange) toproduce a message. The GRU treats the message as the hidden state and the past representationof v as input and outputs an updated representation for v (green). This new representation will beused by vâ€™s direct successors {4} in the same layer and also as input to the next layer. Note thatthe figure illustrates the processing of only one node. In practice, a batch of nodes is processed; seeSection 2.2.
Figure 2: Topological batching. Left: for the original graph G ; right: for the reverse graph G .
Figure 3: Average training time per epoch, on logarithmic scale. Standard deviation is negligible.
Figure 4: Extending Table 4 withfurther layers on TOK-15.
Figure 5: The Bayesian networkidentified by using Bayesian opti-mization over the latent space en-coded by DAGNN.
Figure 6: Distribution of the longest path lengths, for OGBG-CODE (left) and OGB G-CODE- 1 5(right). To improve readability, we ignored a tiny amount of graphs whose longest path length > 30.
