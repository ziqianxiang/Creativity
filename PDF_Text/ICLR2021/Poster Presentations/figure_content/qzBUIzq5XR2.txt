Figure 2: A generative neuro-Symbolic (GNS) model of character concepts. The type model GenerateType(P(ψ)) produces character types one stroke at a time, using an image canvas C as memory. At each step,the current canvas C is fed to procedure GeneratePart and a stroke sample is produced. The canvas isfirst processed by the location model, a CNN-MLP architecture that samples starting location y, and next bythe stroke model, a CNN-LSTM architecture that samples trajectory x while attending to the encoded canvas.
Figure 3: Classification fits and parsing. (a) Posterior parses from two training images were refit to the same testimage. The first row of each grid shows the training image and its top-3 predicted parses (best emboldened).
Figure 4: Generation tasks. (a) GNS produced 9 new exemplars for each of 5 target images, plotted here nextto human and BPL productions. (b) A grid of 36 new character concepts sampled unconditionally from GNS,shown next to bpl samples.
Figure A6: Spline representation. Raw strokes (left) are converted into minimal splines (right) using least-squaresoptimization. Crosses (left) indicate pen locations and red dots (right) indicate spline control points.
Figure A7: Token model sampling procedure.
Figure A8: The initial “base” parses proposed for an image With skeleton extraction and random walks.
Figure A9: A GNS model for the concept of a chair.
