Figure 1: Steady-state loss for varying K,of multiplicative late-phase weights (Ours)compared to an ensemble of models.
Figure 2: WRN 28-10, CIFAR-100,constant learning rate SWA (activatedat epoch 150). With BatchNorm late-phase weights (K=10, initialized atepoch 120) and without (K=1). Meantest acc. (%) ± std. over 5 seeds.
Figure 3: Flatness score. Mean score ±std. over 5 seeds, WRN 28-10, CIFAR-100, SGD, with and without Batch-Norm late-phase weights. Slower in-crease with σz is better.
Figure 4:	Pseudocode for a single parameter update for SWA and SGD with Nesterov momentum,the two main optimizers used in our experiments. These are either used standalone, or as Uθ andUφ in Algorithm 1 (main text). U in Algorithm 2 (SWA) serves as a placeholder for a parameterupdate rule such as SGD (with Nesterov momentum) or Adam. Training iteration t is counted fromthe activation of SWA in Algorithm 1.
Figure 5:	Complete pseudocode for an entire training session using late-phase weights. To avoidnotational clutter T , T0 and t are measured in numbers of minibatches consumed. In the paper, wemeasure T0 and T in epochs. For simplicity, we present the case where Uφ and Uθ are set to plainSGD (without momentum) and φk of dimension 1. Other optimization algorithms (e.g., Algorithm2 or Algorithm 3) can be used to replace Uφ and Uθ , as described in Algorithm 1. Note that weincrease t inside the inner loop. This highlights (i) that every specialist parameter is trained only on1/K data samples after t > T0 compared to θ, and (ii) that we count every minibatch drawn fromthe data to compare fairly to algorithms without an inner loop.
Figure 6: Sensitivity analysis of T0 . Mean AUROC score (OOD) and test set accuracy for differentvalues of T0 for WRN 28-10, CIFAR-100, SGD, with BatchNorm late-phase weights.
