Figure 1: Overall architecture of the IS scheme from the perspective of Agent iWe model the observation predictor fθ (ot, at, a-i) which is conditioned on the observation Ot, ownaction a", and the output of the action predictor ^-i. Here, we adopt the dynamics function thatpredicts the difference between the next observation and the current observation, i.e., oit+1 - oitinstead of the next observation oit+1 proposed in (Nagabandi et al. (2018)) in order to reduce modelbias in the early stage of learning. Hence, the next observation can be written as0t+ι= ot + fθ (ot ,at ,^-i).	(3)By injecting the predicted next observation and the received messages into the roll-out policy inITGM, we obtain the predicted next action ^"+1 = πi(o^t+1, mt-1). Here, we use the current policyas the roll-out policy. Combining 0"+1 and ^t+1, we obtain next imagined step at time step t + 1,τt+1 = (0>t+1 ,at+1). In order to produce an H-length imagined trajectory, we inject the output ofITGM and the received messages mt-1 into the input of ITGM recursively. Note that we use thereceived messages at time step t, mt-1, in every recursion of ITGM.14.2	Attention Module (AM)Instead of the naive approach that uses the imagined trajectory [τt, ∙ ∙ ∙ ,τt+H-1 ] directly as themessage, we apply an attention mechanism in order to learn the relative importance of imaginedsteps and encode the imagined trajectory according to the relative importance. We adopt the scale-dot product attention proposed in (Vaswani et al. (2017)) as our AM. Our AM consists of threecomponents: query, key, and values. The output of AM is the weighted sum of values, where theweight of values is determined by the dot product of the query and the corresponding key. In our
Figure 2: Considered environments: (a) predator-prey (PP), (b) cooperative-navigation (CN), and(c) traffic-junction (TJ)5.1	EnvironmentsPredator-prey (PP) The predator-prey environment is a standard task in multi-agent systems. Weused a PP environment that consists of N predators and fixed M preys in a continuous state-actiondomain. We control the actions of predators and the goal is to capture as many preys as possible ina given time. Each agent observes the positions of predators and preys. When C predators catcha prey simultaneously, the prey is captured and all predators get shared reward R1. At every timewhen all the preys are captured, the preys are respawn and the shared reward value R1 increasesby one with initial value one to accelerate the capture speed for the given time. We simulatedthree cases: (N = 2, C = 1), (N = 3, C = 1), and (N = 4, C = 2) with all M = 9 preys,where the fixed positions of the preys are shown in Fig.2(a). In the cases of (N = 2, C = 1) and(N = 3, C = 1), the initial positions of all predators are the same and randomly determined. Thus,the predators should learn not only how to capture preys but also how to spread out. In the case of(N = 4, C = 2), the initial positions of all predators are randomly determined independently. Thus,the predators should learn to capture preys in group of two.
Figure 3: Performance for MADDPG (blue), DIAL (green), TarMAC (red), Comm-OA (purple),ATOC (cyan) and the proposed IS method (black).
Figure 4: Performance for our proposed method with different length of imagined trajectory H andwithout attention module.
Figure 5: Imagined trajectories and attention weights of each agent on PP (N=3): 1st row - agent1(red), 2nd row - agent2 (green), and 3rd row - agent3 (blue). Black squares, circle inside the times-icon, and other circles denote the prey, current position, and estimated future positions, respectively.
Figure 6: Performance for MADDPG (blue), MADDPG-p (orange), and the proposed IS method(black).
