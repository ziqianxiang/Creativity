Figure 1: In standard image classification datasets, classes are typically composed of multiple clus-ters of similarly looking images. We call intraclass clustering a model’s ability to differentiate suchclusters despite their association to identical labels.
Figure 2: Our simplest measure (denoted c3) quantifies intraclass clustering through the ratio ofstandard deviations σn,Ci and σn,D associated to the class Ci and the entire dataset D respectively.
Figure 3: Visualization of the relationship between the four proposed intraclass clustering measuresand generalization performance, across datasets and network architectures. The four columns cor-respond to c1 , c2, c3 and c4 measures respectively. All measures display a tight connection withgeneralization performance, suggesting a crucial role for intraclass clustering in the implicit regu-larization of deep neural networks.
Figure 4: Plots showing how the Kendall coefficients of c1 and c3 change with parameter k (cfr.
Figure 5: Evolution of each measure (after averaging over 64 models) across layers. The overall in-crease of the measures with layer depth suggests that intraclass clustering occurs even in the deepestrepresentations of neural networks.
Figure 6: Evolution of the intraclass clustering measures over the course of training for three modelswith different generalization performances. We observe that the differences between models withhigh and low generalization performance arise essentially in the early phase of training.
Figure 7: Evolution along training of the preactivation distributions associated with the neurons thatare the most selective (cfr. Eq. 1) for ’Rocket’ and ’Ray’ subclasses. The neurons behave likethey were trained to identify these specific subclasses although no supervision or explicit trainingmechanisms were implemented to target this behaviour.
Figure 8: Histogram of performances of the set of models used in our experiments.
Figure 9: Evolution of the measures across layers for Resnets trained on CIFAR100Figure 10: Evolution of the measures across layers for VGGs trained on CIFAR10S①SSepqnS Jo #Subclass selectivityFigure 11: Distribution of neural subclass selectivity values (cfr. measure c1) over the 100 subclassesof CIFAR100. For each subclass, neural subclass selectivity is computed based on the most selectiveneuron in the neural network (i.e. k = 1). We observe that 1) only a few subclasses reach highselectivity values and 2) the selectivity values vary much across subclasses. We suspect that theoutliers with exceptionally high selectivity values cause the median operation to outperform themean in the measures c1 and c2 .
Figure 10: Evolution of the measures across layers for VGGs trained on CIFAR10S①SSepqnS Jo #Subclass selectivityFigure 11: Distribution of neural subclass selectivity values (cfr. measure c1) over the 100 subclassesof CIFAR100. For each subclass, neural subclass selectivity is computed based on the most selectiveneuron in the neural network (i.e. k = 1). We observe that 1) only a few subclasses reach highselectivity values and 2) the selectivity values vary much across subclasses. We suspect that theoutliers with exceptionally high selectivity values cause the median operation to outperform themean in the measures c1 and c2 .
Figure 11: Distribution of neural subclass selectivity values (cfr. measure c1) over the 100 subclassesof CIFAR100. For each subclass, neural subclass selectivity is computed based on the most selectiveneuron in the neural network (i.e. k = 1). We observe that 1) only a few subclasses reach highselectivity values and 2) the selectivity values vary much across subclasses. We suspect that theoutliers with exceptionally high selectivity values cause the median operation to outperform themean in the measures c1 and c2 .
