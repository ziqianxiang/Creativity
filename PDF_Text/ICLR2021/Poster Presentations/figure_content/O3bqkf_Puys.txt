Figure 1: Illustration of grid based and point based convolutions on spatio-temporal sequences. (a)For a grid based video, each grid represents a feature of a pixel, where C, L, H and W denote thefeature dimension, the number of frames, height and width, respectively. A 3D convolution encodesan input to an output of size C0 X L0 X H0 X W0. (b) A point cloud sequence consists of a coordinatepart (3 x L X N) and a feature part (C X L X N), where N indicates the number of points in a frame.
Figure 2: Illustration of the proposed point spatio-temporal (PST) convolution. The input containsL = 5 frames, with N = 8 points per frame. (a) Based on the temporal kernel size l = 3, temporalstride St = 2, and temporal padding P = 1, the 1st, 3rd, 5th frames are selected as temporal anchorframes. According to a spatial subsampling rate Ss = 4, 2 spatial anchor points are sampled byFPS in each anchor frame. The sampled anchor points are then transferred to the [ 2C = 1 nearestneighboring frames. A point tube is constructed with a spatial search radius r for the anchor points.
Figure 3: Influence of temporalkernel size and (initial) spatialsearch radius on MSR-Action3DFigure 4: Visualization of PST convolution,s output. Top: input point cloud sequences, where colorencodes depth. Bottom: output of PST convolution, where brighter color indicates higher activation.
Figure 4: Visualization of PST convolution,s output. Top: input point cloud sequences, where colorencodes depth. Bottom: output of PST convolution, where brighter color indicates higher activation.
Figure 5: Comparison ofrecognition running time pervideo on NTU RGB+D 60.
Figure 6: Illustration of the proposed PST transposed convolution. The input contains L0 = 3 frames,with N0 = 2 points per frame. The PST transposed convolution is to generate new original pointfeatures for the original point cloud sequence, which contains L = 5 frames, with N = 8 points perframe. The temporal kernel size l is 3 and the temporal stride is St is 2. (a) Temporal transposedconvolution. (b) Temporal interpolation. (c) Spatial interpolation.
Figure 7: Hierarchical PSTNet architecture for 3D action recognition.
Figure 8: Hierarchical PSTNet architecture for 4D semantic segmentation.
Figure 9: Influence of temporal kernel size on different actions of MSR-Action3D.
Figure 10: Feature visualizations on MSR-Action3D using t-SNE. Each sequence is visualizedas a point and sequences belonging to the same action have the same color. PSTNet features aresemantically separable compared to MeteorNet, suggesting that it learns better representations forpoint cloud sequences.
Figure 11: Comparison of PST convolution With (W/) and Without (W/o) disentangling structure onMSR-Action3D.
Figure 13: Running time proportion of mainoperations in PSTNet.
Figure 12: Impact of the number of PSTNet layerson running time and parameter.
Figure 15: Visualization of the output of each layer in PSTNet. For the input point cloud sequence, color encodes depth. For the outputs, brighter color indicateshigher activation. Input sequences consist of 24 frames. Due to the spatial subsampling SS and the temporal stride st, points and frames progressively decrease alongthe network depth.
Figure 16: Visualization of semantic segmentation examples from Synthia 4D (I). Top: inputs.
Figure 17: Visualization of semantic segmentation examples from Synthia 4D (II). Top: inputs.
