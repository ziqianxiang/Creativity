title,year,conference
 Character-levellanguage modeling with deeper self-attention,2019, In Proceedings of the AAAI Conference on ArtificialIntelligence
 The lottery ticket hypothesis for pre-trained bert networks,2020, arXiv preprintarXiv:2007
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Reformer: The efficient transformer,2020, arXivpreprint arXiv:2001
 Albert: A lite bert for self-supervised learning of language representations,2019, arXiv preprintarXiv:1909
 Fixed point quantization of deep convolu-tional networks,2016, In International Conference on Machine Learning
 Fastbert: a self-distillingbert with adaptive inference time,2020, arXiv preprint arXiv:2004
 Low-rank matrix factorization for deep neural network training with high-dimensional output targets,2013, InAcoustics
 Movement pruning: Adaptive sparsity byfine-tuning,2020, arXiv preprint arXiv:2005
 Fpga-based accelerators of deep learningnetworks for learning and classification: A review,2018, IEEE Access
 Mesh-tensorflow: Deeplearning for supercomputers,2018, In Advances in Neural Information Processing Systems
 Svd-softmax: Fastsoftmax approximation on large vocabulary neural networks,2017, In Advances in Neural InformationProcessing Systems
 Structured transforms for small-footprint deeplearning,2015, In Advances in Neural Information Processing Systems
 Patient knowledge distillation for bert modelcompression,2019, arXiv preprint arXiv:1908
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Linformer: Self-attention withlinear complexity,2020, arXiv preprint arXiv:2006
 Bert-of-theseus: Compressingbert by progressive module replacing,2020, arXiv preprint arXiv:2002
 Fast lstm inferenceby dynamic decomposition on cloud systems,2019, In ICDM
 On compressing deep models by lowrank and sparse decomposition,2017, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Q8bert: Quantized 8bit bert,2019, arXivpreprint arXiv:1910
 Accelerating neural transformer via an average attentionnetwork,2018, arXiv preprint arXiv:1805
