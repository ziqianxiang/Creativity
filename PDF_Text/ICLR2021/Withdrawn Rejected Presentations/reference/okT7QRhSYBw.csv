title,year,conference
 Critical learning periods in deep neuralnetworks,2017, arXiv preprint arXiv:1711
 Large scale distributed neural network training through online distillation,2018, arXiv preprintarXiv:1804
 Curriculum learning,2009, InProceedings ofthe 26th annual international conference on machine learning
 Beyondpoint estimate: Inferring ensemble prediction variation from neuron activation strength in recom-mender systems,2020, arXiv preprint arXiv:2008
 Reducing overfit-ting in deep networks by decorrelating representations,2015, arXiv preprint arXiv:1511
 Knowledge distillation: Asurvey,2020, arXiv preprint arXiv:2006
 Asymptotically optimal classification for multiple tests with empirically observedstatistics,1989, IEEE Transactions on Information Theory
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Paraphrasing complex network: Network compres-sion via factor transfer,2018, In Advances in neural information processing systems
 The bellkor solution to the netflix grand prize,2009, 2009
 Simple and scalable predic-tive uncertainty estimation using deep ensembles,2017, In Advances in neural information processingsystems
 Ad click prediction: a view fromthe trenches,2013, In Proceedings of the 19th ACM SIGKDD international conference on Knowledgediscovery and data mining
 Distillation of deep learning ensembles as a regularisationmethod,2018, In Advances in Hybridization OfIntelligentMethodS
 Subclass distillation,2020, arXiv preprintarXiv:2002
 Deterministic implementations for repro-ducibility in deep reinforcement learning,2018, arXiv preprint arXiv:1809
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Measuring the effects of data parallelism on neural network training,2018, arXivpreprint arXiv:1811
 Using the functional behavior of neurons forgenetic recombination in neural nets training,1993, Complex SyStemS
 Under-standing and improving knowledge distillation,2020, arXiv preprint arXiv:2002
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
 Deep mutual learning,2018, InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition
