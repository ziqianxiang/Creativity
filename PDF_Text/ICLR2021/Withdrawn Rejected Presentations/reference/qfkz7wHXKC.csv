title,year,conference
 Weighted transformer network for ma-chine translation,2017, arXiv preprint arXiv:1711
 Bert: Pre-training of deePbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 SPatially adaPtive comPutation time for residual networks,2017, In Proceedingsofthe IEEE Conference on Computer Vision and Pattern Recognition
 Adaptive computation time for recurrent neural networks,2016, arXiv preprintarXiv:1603
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Long short-term memory,1997, Neural computation
 Deep networks withstochastic depth,2016, In European conference on computer vision
 Depthwise separable convolutions for neuralmachine translation,2017, arXiv preprint arXiv:1706
 Sequence-level knowledge distillation,2016, arXiv preprintarXiv:1606
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Efficient neural architecturesearch via parameter sharing,2018, arXiv preprint arXiv:1802
 Neural machine translation of rare words withsubword units,2015, arXiv preprint arXiv:1508
 Self-attention with relative position representa-tions,2018, arXiv preprint arXiv:1803
 The evolved transformer,2019, arXiv preprintarXiv:1901
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Self-training with noisy studentimproves imagenet classification,2019, arXiv preprint arXiv:1911
 Training deep neural networks in gen-erations: A more tolerant teacher educates better students,2019, In Proceedings of the AAAI Conferenceon Artificial Intelligence
 Snapshot distillation: Teacher-studentoptimization in one generation,2019, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
