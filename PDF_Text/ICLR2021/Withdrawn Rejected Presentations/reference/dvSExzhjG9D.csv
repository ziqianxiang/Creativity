title,year,conference
 Learning to learn by gradient descent by gradientdescent,2016, In NeurIPS
 Practical recommendations for gradient-based training of deep architectures,2012, InNeural networks: Tricks of the trade
 Random search for hyper-parameter optimization,2012, JMLR
 Deep frank-wolfe for neural networkoptimization,2019, In ICLR
 Learning to learn without gradient descent by gradientdescent,2017, In ICML
 Stochastic algorithms withgeometric step decay converge linearly on sharp functions,2019, arXiv:1907
 Imagenet: A large-scalehierarchical image database,2009, In CVPR
 Sharp minima can generalize fordeep nets,2017, In ICML
 Model-agnostic meta-learning for fast adaptation ofdeep networks,2017, In ICML
 Forward and reversegradient-based hyperparameter optimization,2017, In ICML
 Sgd: General analysis and improved rates,2019, In ICML
 Deep residual learning for imagerecognition,2016, In CVPR
 Benchmarking neural network robustness to commoncorruptions and perturbations,2019, In ICLR
 Flat minima,1997, Neural Computation
 Long short-term memory,1997, Neural computation
 Evolved policy gradients,2018, In NeurIPS
 Densely connectedconvolutional networks,2017, In CVPR
 Automated Machine Learning,2019, Springer
 Three factors influencing minima in sgd,2017, arXiv:1711
 On large-batch training for deep learning: Generalization gap and sharp minima,2017, In ICLR
 Adam: A method for stochastic optimization,2015, In ICLR
 Learning multiple layers of features from tiny images,2009, Technical report
 Buildingmachines that learn and think like people,2017, Behavioral and brain sciences
 First-order methods almost alWays avoid saddle points,2019, Mathematical Programming
 Learning to optimize neural nets,2017, In ICLR
 Exponential step sizes for non-convexoptimization,2020, arXiv preprint arXiv:2002
 Meta-sgd: Learning to learn quickly for feW-shotlearning,2017, arXiv preprint arXiv:1707
 Sgdr: Stochastic gradient descent With Warm restarts,2017, In ICLR
 Learning gradient descent: Better generalization and longerhorizons,2017, In ICML
 Shufflenet v2: Practical guidelines forefficient cnn architecture design,2018, In ECCV
 Step size matters in deep learning,2018, In NeurIPS
 Readingdigits in natural images With unsupervised feature learning,2011, In NeurIPS Workshop on DeepLearning and Unsupervised Feature Learning
 A survey on transfer learning,2009, IEEE Transactions on knowledgeand data engineering
 First-order methods almost alWays avoidsaddle points: The case of vanishing step-sizes,2019, In NeurIPS
 Some methods of speeding up the convergence of iteration methods,1964, ComputationalMathematics and Mathematical Physics
 Optimization as a model for few-shot learning,2017, In ICLR
 A stochastic approximation method,1951, The annals of mathematicalstatistics
 L4: Practical loss-based stepsize adaptation for deep learning,2018, InNeurIPS
 Learning to optimize combinato-rial functions,2018, In ICML
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In CVPR
 No more pesky learning rates,2013, In ICML
 Small sample learning in big data era,2018, arXiv:1808
 Learning adaptive loss for robustlearning with noisy labels,2020, arXiv:2002
 Meta transition adaptation for robust deeplearning with noisy labels,2020, arXiv preprint arXiv:2006
 Practical bayesian optimization of machinelearning algorithms,2012, In NeurIPS
 Reminiscence and rote learning,1937, Psychological Monographs
 The marginalvalue of adaptive gradient methods in machine learning,2017, In NeurIPS
 How sgd selects the global minima in over-parameterized learning:A dynamical stability perspective,2018, In NeurIPS
 Understanding short-horizon bias instochastic meta-optimization,2018, In ICLR
 Transfer learning,2020, Cambridge UniversityPress
 Wide residual networks,2016, In BMVC
 Adaptive methodsfor nonconvex optimization,2018, In NeurIPS
 Adadelta: an adaptive learning rate method,2012, arXiv:1212
 Understandingdeep learning requires rethinking generalization,2017, In ICLR
 Learning transferable architecturesfor scalable image recognition,2018, In CVPR
