title,year,conference
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Pragmatically informative imagecaptioning with character-level inference,2018, arXiv preprint arXiv:1804
 Plug and play language models: a simple approach to controlled textgeneration,2020, ICLR
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model,2019, arXiv preprintarXiv:1906
 Realtoxici-typrompts: Evaluating neural toxic degeneration in language models,2020, EMNLP
 Hafez: an interactive poetrygeneration system,2017, In Proceedings of ACL 2017
 Knowledge and implicature: Modeling language Un-derstanding as social cognition,2013, Topics in cognitive science
 Learningto write with cooperative discriminators,2018, arXiv preprint arXiv:1805
 The curious case of neural textdegeneration,2020, In International Conference on Learning Representations
 Toward con-trolled generation of text,2017, In ICML
 Learning to generate reviews and discoveringsentiment,2017, arXiv preprint arXiv:1704
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Transformers: State-of-the-artnatural language processing,2019, arXiv preprint arXiv:1910
 Generative and discriminative textclassification with recurrent neural networks,2017, arXiv preprint arXiv:1703
 Character-level convolutional networks for text clas-sification,2015, In Advances in neural information processing systems
 Aligning books and movies: Towards story-like visual explanations by watchingmovies and reading books,2015, In The IEEE International Conference on Computer Vision (ICCV)
 Fine-tuning language models from human preferences,2019, arXivpreprint arXiv:1909
