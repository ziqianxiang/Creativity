title,year,conference
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Trainingand testing low-degree polynomial data mappings via linear svm,2010, JMLR
 Support-vector networks,1995, Machine learning
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In NAACL
 Similarity search in high dimensions viahashing,1999, In VLDB
 Realm: Retrieval-augmented language model pre-training,2019, In ICML
 Distilling the knowledge in a neural network,2014, NIPSDeep Learning Workshop
 Billion-scale similarity search with gpus,2019, IEEE Transactionson Big Data
 Document ranking and the vector-space model,1997, IEEEsoftware
 Retrieval-augmented gener-ation for knowledge-intensive nlp tasks,2020, In EMNLP
 Improving multi-task deep neu-ral networks via knowledge distillation for natural language understanding,2019, arXiv preprintarXiv:1904
 Passage re-ranking with bert,2019, arXiv preprintarXiv:1901
 Huggingfaceâ€™s transformers: State-of-the-art natural language processing,2019, ArXiv
