title,year,conference
 Asynchronous batch Bayesianoptimisation with improved local penalisation,2019, In Proceedings of the 36th International Conferenceon Machine Learning (ICML’19)
 Algorithms for hyperparameter optimization,2011, InJ
 A downsampled variant of ImageNet as an alternative tothe CIFAR datasets,2017, arXiv:1707
 Deep Gaussian processes formulti-fidelity modeling,2018, Technical report
 Speeding up automatic hyperparameter optimization ofdeep neural networks by extrapolation of learning curves,2015, In Proceedings of the 24th InternationalJoint Conference on Artificial Intelligence (IJCAI’15)
 Nas-bench-201: Extending the scope of reproducible neural architecturesearch,2020, arXiv:2001
 Dealing with asynchronicity in parallel Gaus-sian process based global optimization,2011, In 4th International Conference of the ERCIM WG onComputing and Statistics
 Google vizier: A servicefor black-box optimization,2017, In Proceedings of the 23rd ACM SIGKDD International Conferenceon Knowledge Discovery and Data Mining
 Deep residual learning for image recognition,2016, In 2016 IEEEConference on Computer Vision and Pattern Recognition (CVPR’16)
 Parallel and distributedThompson sampling for large-scale accelerated exploration of chemical space,2017, In D
 Long short-term memory,1997, Neural computation
 Non-stochastic best arm identification and hyperparameter optimiza-tion,2016, In Proceedings of the 17th International Conference on Artificial Intelligence and Statistics(AISTATS’16)
 Multi-fidelity Bayesian optimisation withcontinuous approximations,2017, In Proceedings of the 34th International Conference on MachineLearning (ICML’17)
 Almost optimal exploration in multi-armed bandits,2013, In InProceedings of the 30th International Conference on Machine Learning (ICML’13)
 Predicting the output from a complex computer code when fastapproximations are available,2000, Biometrika
 Adam: A method for stochastic optimization,2015, In International Conferenceon Learning Representations (ICLR’15)
 Learning curve prediction with Bayesianneural networks,2017, In International Conference on Learning Representations (ICLR’17)
 Learning multiple layers of features from tiny images,2009, Technical report
 Hyperband: Bandit-basedconfiguration evaluation for hyperparameter optimization,2017, In International Conference on LearningRepresentations (ICLR’17)
 Massivelyparallel hyperparameter tuning,2018, arXiv:1810
 Tune: A research platformfor distributed model selection and training,2018, arXiv preprint arXiv:1807
 Taking the human out of the loop:A review of Bayesian optimization,2016, Proceedings of the IEEE
 Practical Bayesian optimization of machine learningalgorithms,2012, In P
 Multi-task Bayesian optimization,2013, In C
 Freeze-thaw Bayesian optimization,2014, Technical ReportarXiv:1406
 Multi-fidelity bayesian optimization with max-value entropy search and its parallelization,2020, In Proceedingsof the 37th International Conference on Machine Learning (ICML’20)
 NAS valuation is frustratingly hard,2020, In InternationalConference on Learning Representations (ICLR’20)
 NAS-Bench-101: Towardsreproducible neural architecture search,2019, arXiv:1902
