title,year,conference
 Towards Federated Learningat Scale: System Design,2019, SysML
 EMNIST: an extension ofMNIST to handwritten letters,2017, arXiv preprint arXiv:1702
 Large scaledistributed deep networks,2012, In Proceedings of the International Conference on Neural InformationProcessing Systems
 Activefederated learning,2019, ArXiv
 On the convergence of local descent methods in federatedlearning,2019, arXiv preprint arXiv:1910
 A better alternative to error feedback for CommUniCation-efficient distributed learning,2020, arXiv preprint arXiv:2006
 Faster on-device trainingusing new federated momentum algorithm,2020, arXiv preprint arXiv:2002
 Advances and open problems infederated learning,2019, arXiv preprint arXiv:1912
 SCAFFOLD: Stochastic controlled averaging for on-device federatedlearning,2019, arXiv preprint arXiv:1910
 Not all samples are created equal: Deep learning with importancesampling,2018, In Proceedings of the International Conference on Machine Learning (ICML)
 Tighter theory for local SGD on identical and heteroge-neous data,2020, In The 23rd International Conference on Artificial Intelligence and Statistics (AISTATS2020)
 Aunified theory of decentralized SGD with changing topology and local updates,2020, arXiv preprintarXiv:2003
 Device heterogeneity infederated learning: A superquantile approach,2020, ArXiv
 Towards Fair and Privacy-Preserving Federated Deep Models,2020, IEEE Transactionson Parallel and Distributed Systems
 Fromlocal SGD to local fixed point methods for federated learning,2020, arXiv preprint arXiv:2004
 Agnostic federated learning,2019, In KamalikaChaudhuri and Ruslan Salakhutdinov (eds
 FedSplit: An algorithmic framework for fast federatedoptimization,2020, arXiv preprint arXiv:2005
 Adaptive federated optimization,2020, arXiv preprintarXiv:2003
 Communication-efficient federated learning via optimal clientsampling,2020, ArXiv
 Towards flexible device partici-pation in federated learning for non-iid data,2020, ArXiv
 Local SGD converges fast and communicates little,2018, arXiv preprintarXiv:1805
 The error-feedback framework: Better rates forSGD with delayed gradients and compressed communication,2019, arXiv preprint arXiv:1909
 Cooperative SGD: A unified framework for the design and analysisof communication-efficient SGD algorithms,2018, arXiv preprint arXiv:1808
 Tackling the ObjectiveInconsistency Problem in Heterogeneous Federated Optimization,2020, preprint
 Parallel restarted SGD for non-convex optimization withfaster convergence and less communication,2018, arXiv preprint arXiv:1807
 FedPD: A federated learningframework with optimal rates and adaptivity to non-IID data,2020, arXiv preprint arXiv:2005
