title,year,conference
 Qsgd:Communication-efficient sgd via gradient quantization and encoding,2017, In Advances in NeuralInf Proc
 signsgd:Compressed optimisation for non-convex problems,2018, July 2018
 A downsampled variant of imagenet as analternative to the cifar datasets,2017, arXiv preprint arXiv:1707
 Sketchml: Accelerating distributed machinelearning with data sketches,1269, In Proc
 Error feedbackfixes signsgd and other gradient compression schemes,2019, Int
 Deep gradient compression:Reducing the communication bandwidth for distributed training,2018, In Int
 Horovod: fast and easy distributed deep learning in Ten-sorFlow,2018, arXiv preprint arXiv:1802
 The scientist and engineerâ€™s guide to digital signal processing,1997, 1997
 Sparsified sgd with memory,2018, InAdvances in Neural Inf
 Atomo: Communication-efficient learning via atomic sparsification,2018, In Advances inNeural Inf
 Terngrad:Ternary gradients to reduce communication in distributed deep learning,2017, In Advances in neuralinformation processing systems
 Wide residual networks,2016, arXiv preprintarXiv:1605
 Communication-efficient distributed blockwise mo-mentum SgdWitherror-feedback,2019, In Advances in Neural Inf
