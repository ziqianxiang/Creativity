title,year,conference
 QSGD:Communication-efficient SGD via gradient quantization and encoding,2017, In Advances in NeuralInformation Processing Systems
 Scaling up machine learning: Parallel anddistributed approaches,2011, Cambridge University Press
 SIGNSGD:Compressed optimisation for non-convex problems,2018, arXiv preprint arXiv:1802
 Asynchronous stochastic convex op-timization: the noise is in the noise and SGD donâ€™t care,2015, In Advances in Neural InformationProcessing Systems
 Project adam:Building an efficient and scalable deep learning training system,2014, In 11th USENIX Symposiumon Operating Systems Design and Implementation (OSDI)
 MQGrad: Rein-forcement learning of gradient quantization in parameter server,2018, In Proceedings of the 2018 ACMSIGIR International Conference on Theory of Information Retrieval
 Large scale distributed deep networks,2012, InAdvances in Neural Information Processing Systems
 Ac-celerating distributed deep learning by adaptive gradient quantization,2020, In IEEE InternationalConference on Acoustics
 1-bit stochastic gradient descent and itsapplication to data-parallel distributed training of speech DNNs,2014, Conference of the InternationalSpeech Communication Association
 Sparsified SGD with memory,2018, InAdvances in Neural Information Processing Systems
 TernGrad:Ternary gradients to reduce communication in distributed deep learning,2017, Advances in NeuralInformation Processing Systems
 Error compensated quantized SGDand its applications to large-scale distributed optimization,2018, arXiv preprint arXiv:1806
 Character-level convolutional networks for text clas-Sification,2015, In Advances in Neural Information Processing Systems
 Towardan intelligent edge: wireless communication meets machine learning,2020, IEEE CommunicationsMagazine
