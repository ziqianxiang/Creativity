title,year,conference
 Computationally efficient convolved multiple outputGaussian processes,2011, JMLR
 Automatic tuning and adaptationfor pid controllers-a survey,1993, Control Engineering Practice
 Deep rewiring:Training very sparse deep networks,2018, In Proc
 Challenges and advances in parallel sparse matrix-matrix multipli-cation,2008, In Proc
 Nest: A neural network synthesis tool based on agrow-and-prune paradigm,2019, IEEE Trans
 ImageNet: A large-scalehierarchical image database,2009, In Proc
 Exploiting linearstructure within convolutional networks for efficient evaluation,2014, In Proc
 Sparse networks from scratch: Faster training without losingperformance,2019, arXiv:1907
 Learning to prune deep neural networks vialayer-wise optimal brain surgeon,2017, In Proc
 Second order derivatives for network pruning: Optimal brainsurgeon,1992, In Proc
 Deep residual learning for imagerecognition,2016, In Proc
 Identity mappings in deep residualnetworks,2016, In Proc
 Scalable variational Gaussianprocess classification,2015, In Proc
 Speeding up convolutional neural networkswith low rank expansions,2014, In Proc
 A simple procedure for pruning back-propagation trained neural networks,1990, IEEETrans
 Adam: A method for stochastic optimization,2015, In Proc
 Snip: Single-shot network pruningbased on connection sensitivity,2019, In Proc
 Pruning filters forefficient convnets,2017, In Proc
 Knowledge distillation for small-footprint highwaynetworks,2017, In Proc
 GPflow: A Gaussian process libraryusing tensorflow,2017, JMLR
 Mixedprecision training,2018, In Proc
 Scalable training of artificial neural networks with adaptive sparse connectivityinspired by network science,2018, Nature
 Parameter efficient training of deep convolutional neural networksby dynamic sparse reparameterization,2019, In Proc
 Skeletonization: A technique for trimming the fat from anetwork via relevance assessment,1988, In Proc
 Exploring sparsity in recurrentneural networks,2017, In Proc
 Channel-level acceleration of deep face representations,2015, IEEE Access
 Picking winning tickets before training bypreserving gradient flow,2020, In Proc
 Learning structured sparsity indeep neural networks,2016, In Proc
 Design principles for sparse matrix multiplication onthe GPU,2018, In Proc
