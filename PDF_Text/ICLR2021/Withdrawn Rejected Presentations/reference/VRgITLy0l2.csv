title,year,conference
 On the convergence rate of training recurrent neuralnetworks,2019, In Advances in Neural Information Processing Systems
 A convergence analysis of gradientdescent for deep linear neural networks,2018, arXiv preprint arXiv:1810
 Neural Networks for Pattern Recognition,1995, OXFORD University Press
 Gradient descent finds globalminima of deep neural networks,2018, arXiv preprint arXiv:1811
 Zhang neural network versus gradient-based neuralnetwork for time-varying linear matrix equation solving,2011, Neurocomputing
 Stable architectures for deep neural networks,2017, Inverse Problems
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In Advances in neural information processing systems
 Finite timeanalysis of linear two-timescale stochastic approximation with markovian noise,2020, arXiv preprintarXiv:2002
 A theoretical framework for back-propagation,1988, In Proceedings of the 1988 connectionist models summer school
 Deep learning theory review: An optimal controland dynamical systems perspective,2019, arXiv preprint arXiv:1908
 Dex: Deep expectation of apparent age froma single image,2015, In IEEE International Conference on Computer Vision Workshops (ICCVW)
 Exact solutions to the nonlinear dynam-ics of learning in deep linear neural networks,2013, arXiv preprint arXiv:1312
 A multistep lyapunov approach for finite-timeanalysis of biased stochastic approximation,2019, arXiv preprint arXiv:1909
 Reinforcement learning output feedback nn controlusing deterministic learning technique,2013, IEEE Transactions on Neural Networks and LearningSystems
 A recurrent neural network for solving sylvesterequation with time-varying coefficients,2002, IEEE Transactions on Neural Networks
