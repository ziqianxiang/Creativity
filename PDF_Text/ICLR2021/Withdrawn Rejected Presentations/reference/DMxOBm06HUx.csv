title,year,conference
 The fifth pascal recognizingtextual entailment challenge,2009, In TAC
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 Electra: Pre-trainingtext encoders as discriminators rather than generators,2020, arXiv preprint arXiv:2003
 A span-extraction dataset for chinese machine reading comprehension,2018, arXiv preprintarXiv:1810
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Zen: pre-training chinesetext encoder enhanced by n-gram representations,2019, arXiv preprint arXiv:1911
 KenLM: Faster and smaller language model queries,2011, In Proceedings of the SixthWorkshop on Statistical Machine Translation
 Span-bert: Improving pre-training by representing and predicting spans,2020, Transactions of the Associationfor Computational Linguistics
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Reformer: The efficient transformer,2020, arXivpreprint arXiv:2001
 Albert: A lite bert for self-supervised learning of language representations,2019, arXiv preprintarXiv:1909
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Deep contextualized word representations,2018, arXiv preprint arXiv:1802
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 Probing prior knowledge needed in challengingchinese machine reading comprehension,2019, arXiv preprint arXiv:1904
 Ernie: Enhanced representation through knowledge integration,2019, arXivpreprint arXiv:1904
 Synthesizer:Rethinking self-attention in transformer models,2020, arXiv preprint arXiv:2005
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 A multiscale visualization of attention in the transformer model,2019, arXiv preprintarXiv:1906
 Structbert: Incor-porating language structures into pre-training for deep language understanding,2019, arXiv preprintarXiv:1908
 A broad-coverage challenge corpus forsentence understanding through inference,2017, arXiv preprint arXiv:1704
 Huggingfaceâ€™s trans-formers: State-of-the-art natural language processing,2019, ArXiv
 Clue: A chinese language understanding evaluation benchmark,2020, arXivpreprint arXiv:2004
 Big bird: Transformers for longersequences,2020, arXiv preprint arXiv:2007
 Ernie: Enhancedlanguage representation with informative entities,2019, arXiv preprint arXiv:1905
 Chid: A large-scale chinese idiom dataset for clozetest,2019, arXiv preprint arXiv:1906
