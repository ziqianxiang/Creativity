title,year,conference
 Variancereduction in sgd by distributed importance sampling,2015, arXiv preprint arXiv:1511
 Finbert: Financial sentiment analysis with pre-trained language models,2019, arXiv preprintarXiv:1908
 Unilmv2: Pseudo-masked language models for unified languagemodel pre-training,2020, arXiv preprint arXiv:2002
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Variance-reducedlanguage pretraining via a mask proposal network,2020, arXiv preprint arXiv:2008
 Electra: Pre-trainingtext encoders as discriminators rather than generators,2019, In International Conference on LearningRepresentations
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Unified language model pre-training for natural language understandingand generation,2019, In Advances in Neural Information Processing Systems
 Train no evil: Selec-tive masking for task-guided pre-training,2020, arXiv preprint arXiv:2004
 Donâ€™t stop pretraining: Adapt language models to domains and tasks,2020, arXivpreprint arXiv:2004
 Deberta: Decoding-enhanced bertwith disentangled attention,2020, arXiv preprint arXiv:2006
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In Advances in neural information processing systems
 Measuringthe evolution of a scientific field through citation frames,2018, Transactions of the Association forComputational Linguistics
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Albert: A lite bert for self-supervised learning of language representations,2019, arXiv preprintarXiv:1909
 Biobert: a pre-trained biomedical language representation model for biomedical textmining,2020, Bioinformatics
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 S2orc: The semanticscholar open research corpus,2020, In Proceedings of the 58th Annual Meeting of the Association forComputational Linguistics
 Deep contextualized word representations,2018, arXiv preprint arXiv:1802
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Mass: Masked sequence to sequence pre-trainingfor language generation,2019, In ICML
 Ernie: Enhanced representation through knowledge integration,2019, arXivpreprint arXiv:1904
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Variance reduction for stochasticgradient optimization,2013, In Advances in Neural Information Processing Systems
 Structbert: Incor-porating language structures into pre-training for deep language understanding,2019, arXiv preprintarXiv:1908
 Finbert: A pretrained language model forfinancial communications,2020, arXiv preprint arXiv:2006
 Defending against neural fake news,2019, In Advances in Neural Information ProcessingSystems
 Character-level convolutional networks for text clas-sification,2015, In Advances in neural information processing systems
 Ernie: Enhancedlanguage representation with informative entities,2019, ACL
 Aligning books and movies: Towards story-like visual explanations by watchingmovies and reading books,2015, In Proceedings of the IEEE international conference on computervision
