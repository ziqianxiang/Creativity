title,year,conference
 Learning from noisy examples,1988, Machine Learning
 A closer lookat memorization in deep networks,2017, In ICML
 Curriculum learning,2009, InICML
 Understanding and utilizingdeep neural networks trained with noisy labels,2019, In ICML
 Learning withbounded instance-and label-dependent label noise,2020, In ICML
 Shake-shake regularization,2017, arXiv preprint arXiv:1705
 Training deep neural-networks using a noise adaptationlayer,2017, In ICLR
 Co-teaching: Robust training of deep neural networks with extremely noisy labels,2018, InNeurIPS
 Decision boundary analysis of adversarial examples,2018, In ICLR
 Active learning by querying informative andrepresentative examples,2010, In NeurIPS
 MentorNet: Learning data-driven curriculum for very deep neural networks on corrupted labels,2018, In ICML
 Robust active label correction,2018, In AISTATS
 Learning multiple layers of features from tiny images,2009, Technical report
 Self-paced learning for latent variablemodels,2010, In NeurIPS
 Gradient descent with early stopping isprovably robust to label noise for overparameterized neural networks,2020, In AISTATS
 Learning fromnoisy labels with distillation,2017, In ICCV
 Peer loss functions: Learning from noisy labels without knowing noiserates,2020, In ICML
 Dimensionality-driven learning with noisy labels,2018, In ICML
" Decoupling"" when to update"" from"" how to update""",2017, InNeurIPS
 Learning withnoisy labels,2013, In NeurIPS
 SELF: learning to filter noisylabels with self-ensembling,2020, In ICLR
 Learning with confident examples: Rankpruning for robust classification with noisy labels,2017, In UAI
 Makingdeep neural networks robust to label noise: A loss correction approach,2017, In CVPR
 Training deep neural networks on noisy labels with bootstrapping,2015, In ICLR
 On the importance of initializa-tion and momentum in deep learning,2013, In ICML
 Joint optimization frame-work for learning with noisy labels,2018, In CVPR
 Mean teachers are better role models: Weight-averaged consis-tency targets improve semi-supervised deep learning results,2017, In NeurIPS
 Robustness of conditionalgans to noisy labels,2018, In NeurIPS
 Combating label noise in deep learning using abstention,2019, In ICML
 Learningfrom noisy large-scale datasets with minimal supervision,2017, In CVPR
 Parts-dependent label noise: Towards instance-dependentlabel noise,2020, arXiv preprint arXiv:2006
 Learning from massive noisylabeled data for image classification,2015, In CVPR
 L_dmi: A novel information-theoretic lossfunction for training deep nets robust to label noise,2019, In NeurIPS
 Learning with biased complementarylabels,2018, In ECCV
 Understandingdeep learning requires rethinking generalization,2017, In ICLR
 Generalized cross entropy loss for training deep neural networkswith noisy labels,2018, In NeurIPS
