title,year,conference
 Stronger generalization bounds fordeep nets via a compression approach,2018, In International Conference on Machine Learning
 Thermophoretic depletion follows boltzmann distribution,2006, PhysicalReview Letters
 On the Computational Inefficiency of Large BatchSizes for Stochastic Gradient Descent,2018, arXiv preprint arXiv:1811
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In Advances in Neural Information Processing Systems
 Three Factors Influencing Minima in SGD,2017, arXiv preprintarXiv:1711
 Stochastic modified equations and adaptive stochasticgradient algorithms,2017, In International Conference on Machine Learning
 Generalization in Deep Networks: The Role of Distancefrom Initialization,2017, In Advances in Neural Information Processing Systems
 Exploring Gener-alization in Deep Learning,2017, In Advances in Neural Information Processing Systems
 The roleof over-parametrization in generalization of neural networks,2019, In International Conference onLearning Representations
 Non-Gaussianity ofStochastic Gradient Noise,2019, arXiv preprint arXiv:1910
 Deep learning generalizes becausethe parameter-function map is biased towards simple functions,2019, In International Conference onLearning Representations
 Brownian colloids in underdamped and overdamped regimes with nonhomogeneoustemperature,2015, Physical Review E
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 A Bayesian perspective on generalization and stochastic gradientdescent,2018, In International Conference on Learning Representations
 On the Generalization Benefit of Noise in StochasticGradient Descent,2020, arXiv preprint arXiv:2006
 Onthe Noisy Gradient Descent that Generalizes as SGD,2019, arXiv preprint arXiv:1906
 How SGD selects the global minima in over-parameterized learn-ing: A dynamical stability perspective,2018, In Advances in Neural Information Processing Systems
 UnderstandingDeep Learning Requires Rethinking of Generalization,2017, In International Conference on LearningRepresentations
 The anisotropic noise in stochas-tic gradient descent: Its behavior of escaping from sharp minima and regularization effects,2019, InInternational Conference on Machine Learning
