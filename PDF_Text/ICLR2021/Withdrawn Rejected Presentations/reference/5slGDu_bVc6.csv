title,year,conference
 Mirror descent and nonlinear projected subgradient methods forconvex optimization,2003, Operations Research Letters
 Curriculum learning,2009, InProceedings of the 26th annual international conference on machine learning
 Generativebridging network in neural sequence prediction,2017, arXiv preprint arXiv:1706
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Imitation learning by coaching,2012, In Advances in NeuralInformation Processing Systems
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Amc: Automl for modelcompression and acceleration on mobile devices,2018, In Proceedings of the European Conference onComputer Vision (ECCV)
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Knowledge distillation via route constrained optimization,2019, In Proceedings of the IEEEInternational Conference on Computer Vision
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Learning multiple layers of features from tiny images,2009, 2009
 Smooth imitation learning for onlinesequence prediction,2016, arXiv preprint arXiv:1606
 Guided policy search,2013, In International Conference on MachineLearning
 Cot: Cooperativetraining for generative modeling of discrete data,2018, arXiv preprint arXiv:1804
 Towards understanding knowledge distillation,2019, In InternationalConference on Machine Learning
 Model compression via distillation and quantiza-tion,2018, arXiv preprint arXiv:1802
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Patient knowledge distillation for bert modelcompression,2019, arXiv preprint arXiv:1908
 The recurrent temporal restricted boltzmannmachine,2009, In Advances in neural information processing systems
 Contrastive representation distillation,2019, arXivpreprint arXiv:1910
 Well-read students learn better:On the importance of pre-training compact models,2019, arXiv preprint arXiv:1908
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2018, arXiv preprintarXiv:1804
 A broad-coverage challenge corpus forsentence understanding through inference,2017, arXiv preprint arXiv:1704
 Quantized convolutionalneural networks for mobile devices,2016, In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition
 Wide residual networks,2016, arXiv preprint arXiv:1605
 Shufflenet: An extremely efficientconvolutional neural network for mobile devices,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Deep mutual learning,2018, InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Adashift:Decorrelation and convergence of adaptive learning rate methods,2018, arXiv preprint arXiv:1810
