title,year,conference
 Critical learning periods in deep neuralnetworks,2017, arXiv preprint arXiv:1711
 Large scale distributed neural network training through online distillation,2018, arXiv preprintarXiv:1804
 Layer normalization,2016, arXiv preprintarXiv:1607
 Continuously differentiable exponential linear units,2017, arXiv
 Curriculum learning,2009, InProceedings ofthe 26th annual international conference on machine learning
 Beyondpoint estimate: Inferring ensemble prediction variation from neuron activation strength in recom-mender systems,2020, arXiv preprint arXiv:2008
 Deep global-connected net with the generalized multi-piecewise reluactivation in deep learning,2018, arXiv preprint arXiv:1807
 Gradient Descent for Non-convex Problems in Modern Machine Learning,2019, PhD thesis
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Robust estimation of a location parameter,1992, In Breakthroughs in statistics
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Deeplearning with s-shaped rectified linear activation units,2015, arXiv preprint arXiv:1512
 Self-normalizingneural networks,2017, In Advances in neural information processing Systems
 SimPle and scalable Predic-tive uncertainty estimation using deep ensembles,2017, In Advances in neural information processingsystems
 Private communication to Quoc Le,2019, Unpublished
 Evolving normalization-activationlayers,2020, arXiv preprint arXiv:2004
 Ad click prediction: a view fromthe trenches,2013, In Proceedings of the 19th ACM SIGKDD international conference on Knowledgediscovery and data mining
 Mish: A self regularized non-monotonic neural activation function,2019, arXiv preprintarXiv:1908
 Deterministic implementations for repro-ducibility in deep reinforcement learning,2018, arXiv preprint arXiv:1809
 Activationfunctions: Comparison of trends in practice and research for deep learning,2018, arXiv preprintarXiv:1811
 Comparison of non-linear activation functions for deep neural networks on mnistclassification task,2018, arXiv preprint arXiv:1804
 Searching for activation functions,2017, arXivpreprint arXiv:1710
 On the invariance of the selu activation function on algorithmand hyperparameter selection in neural network recommenders,2019, In IFIP International Conferenceon Artificial Intelligence Applications and Innovations
 Weight normalization: A simple reparameterization to acceleratetraining of deep neural networks,2016, In Advances in neural information processing systems
 An elu network with total variation for imagedenoising,2017, In International Conference on Neural Information Processing
 Deep mutual learning,2018, InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Improving deep neural net-works using softplus units,2015, In 2015 International Joint Conference on Neural Networks (IJCNN)
