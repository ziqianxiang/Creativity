title,year,conference
 Qsgd:Communication-efficient sgd via gradient quantization and encoding,2017, In Advances in NeuralInformation Processing Systems
 The convergence of sparsified gradient methods,2018, In Advances in Neural InformationProcessing Systems
 More effective distributed ml via a stale synchronousparallel parameter server,2013, In Advances in neural information processing systems
 Large batch training does not need warmup,2020, arXivpreprint arXiv:2002
 Error feedbackfixes signsgd and other gradient compression schemes,2019, In International Conference on MachineLearning
 Federated learning: Strategies for improving communication efficiency,2016, arXivpreprint arXiv:1610
 Asynchronous parallel stochastic gradient fornonconvex optimization,2015, In Advances in Neural Information Processing Systems
 Deep gradient compression: Reducingthe communication bandwidth for distributed training,2018, In International Conference on LearningRepresentations
 Sgdr: Stochastic gradient descent with warm restarts,2016, arXivpreprint arXiv:1608
 1-bit stochastic gradient descent and itsapplication to data-parallel distributed training of speech dnns,2014, In Fifteenth Annual Conference ofthe International Speech Communication Association
 Faster distributeddeep net training: Computation and communication decoupled stochastic gradient descent,2019, arXivpreprint arXiv:1906
 Sparsified sgd with memory,2018, InAdvances in Neural Information Processing Systems
 Slowmo: Improvingcommunication-efficient distributed sgd with slow momentum,2019, arXiv preprint arXiv:1910
 Terngrad:Ternary gradients to reduce communication in distributed deep learning,2017, In Advances in neuralinformation processing systems
 Error compensated quantizedsgd and its applications to large-scale distributed optimization,2018, arXiv preprint arXiv:1806
 Training faster with compressed gradient,2020, arXiv preprintarXiv:2008
