title,year,conference
 Training a 3-node neural network is NP-complete,1993, In MachineLearning: From Theory to Applications
 Entropy-sgd: Biasing gradient descentinto wide valleys,2016, CoRR
 Diffusion maps,1063, Applied and Computational HarmonicAnalysis
 Sharp minima can generalize fordeep nets,2017, arXiv preprint arXiv:1703
 Essentially no barriersin neural network energy landscape,2018, In Jennifer Dy and Andreas Krause (eds
 Dynamic compres-sion and expansion in a classifying recurrent network,2019, bioRxiv
 Visualizing the phate ofneural networks,2019, In Advances in Neural Information Processing Systems
 Qualitatively characterizing neural networkoptimization problems,2014, arXiv preprint arXiv:1412
 Deep residual learning for image recognition,2016, In 2016 IEEEConference on Computer Vision and Pattern Recognition (CVPR)
 Flat minima,1997, Neural Computation
 Internal representation dynamics and geometry inrecurrent neural networks,2020, arXiv preprint arXiv:2001
 An empirical analysis of the optimization ofdeep network loss surfaces,2016, arXiv preprint arXiv:1612
 Predictinggeneralization in deep learning,2020, Competition at Neural Information Processing Systems 2020
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Uni-versality and individuality in neural dynamics across large populations of recurrent networks,2019, InAdvances in neural information processing systems
 Umap: Uniform manifold approximation and projection for dimensionreduction,2018, ArXiv
 Visualizing structure and transitions in high-dimensionalbiological data,2019, Nature Biotechnology
 Exploring general-ization in deep learning,2017, In Advances in neural information processing systems
 Signatures and mechanisms of low-dimensional neural predictive manifolds,2018, bioRxiv
 Local minima in training ofneural networks,2016, arXiv preprint arXiv:1611
 Wide residual networks,2016, In Edwin R
 Understanding deep learning requiresrethinking generalization,2017, ArXiv
