title,year,conference
 How to make the gradients small stochastically: Even faster convex and noncon-vex sgd,2018, In S
 Deep frank-wolfe for neural networkoptimization,2018, arXiv preprint arXiv:1811
 The tradeoffs of large scale learning,2008, In Advances in neuralinformation processing systems
 The relaxation method of finding the common point of convex sets and its applica-tion to the solution of problems in convex programming,1967, USSR computational mathematics andmathematical physics
 On empirical comparisons of optimizers for deep learning,2019, arXiv preprintarXiv:1910
 Gossip dual averaging fordecentralized optimization of pairwise functions,2016, arXiv preprint arXiv:1606
 Understanding the role of momentum in non-convex optimization: Practical insightsfrom a lyapunov analysis,2020, arXiv preprint
 On the ineffectiveness of variance reduced optimization for deeplearning,2019, In Advances in Neural Information Processing Systems
 On the convergence of adamand adagrad,2020, arXiv preprint arXiv:2003
 Asymptotic optimality in stochastic optimization,2016, arXiv preprintarXiv:1612
 Dual averaging for distributed optimiza-tion: Convergence analysis and network scaling,2011, IEEE Transactions on Automatic control
 Online distributed optimization via dualaveraging,2013, In 52nd IEEE Conference on Decision and Control
 Unifying mirror descent and dual averaging,2019, arXivpreprint arXiv:1910
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 On the convergence of stochastic gradient descent with adaptivestepsizes,2019, In The 22nd International Conference on Artificial Intelligence and Statistics
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Pointer sentinel mixturemodels,2016, arXiv preprint arXiv:1609
 Primal-dual subgradient methods for convex problems,2009, Mathematical programming
 Bleu: a method for automaticevaluation of machine translation,2002, In Proceedings of the 40th annual meeting of the Associationfor Computational Linguistics
 On the convergence of the stochasticheavy ball method,2020, arXiv preprint arXiv:2006
 End-to-end variational networks for acceleratedmri reconstruction,2020, arXiv preprint arXiv:2004
 Primal averaging: A new gradient evaluation stepto attain the optimal individual convergence,2018, IEEE transactions on cybernetics
 Push-sum distributed dual averagingfor convex optimization,2012, In 2012 ieee 51st ieee conference on decision and control (cdc)
 Adagrad stepsizes: sharp convergence over nonconvexlandscapes,2019, In International Conference on Machine Learning
 Regularization matters: Generalization andoptimization of neural nets vs their induced kernel,2019, In Advances in Neural Information ProcessingSystems
 Adahessian: Anadaptive second order optimizer for machine learning,2020, arXiv preprint arXiv:2006
 fastmri: An opendataset and benchmarks for accelerated mri,2018, arXiv preprint arXiv:1811
 On the convergence rate of stochastic mirror descent for nonsmoothnonconvex optimization,2018, arXiv preprint arXiv:1806
 Aligning books and movies: Towards story-like visual explanations by watchingmovies and reading books,2015, In Proceedings of the IEEE international conference on computervision
