title,year,conference
 Stability and generalization,2002, J
 The break-even point on optimization trajectories of deep neuralnetworks,2020, In International Conference on Learning Representations
 On large-batch training for deep learning: Generalization gap and sharp minima,2017, In 5thInternational Conference on Learning Representations
 Data-dependent stability of stochastic gradient de-scent,2018, In Proceedings of the 35th International Conference on Machine Learning
 Deep learning,2015, Nature
 A surprisinglinear relationship predicts test performance in deep networks,2018, CoRR
 Algorithmic stability and hypothesiscomplexity,2017, In Proceedings of the 34th International Conference on Machine Learning
 A pac-bayesian analysis of randomized learning with application to stochastic gradi-ent descent,2017, In Advances in Neural Information Processing Systems 30: Annual Conference onNeural Information Processing Systems 2017
 Stochastic gradient descent on separabledata: Exact convergence with a fixed learning rate,2019, In The 22nd International Conference onArtificial Intelligence and Statistics
 Norm-based capacity control in neuralnetworks,2015, In Proceedings ofThe 28th Conference on Learning Theory
 A bayesian perspective on generalization and stochastic gradientdescent,2018, In 6th International Conference on Learning Representations
 The implicit bias of gradientdescent on separable data,2018, In 6th International Conference on Learning Representations
 Stagewise training accelerates conver-gence of testing error over SGD,2019, In Advances in Neural Information Processing Systems 32:Annual Conference on Neural Information Processing Systems 2019
 Understandingdeep learning requires rethinking generalization,2017, In 5th International Conference on LearningRepresentations
