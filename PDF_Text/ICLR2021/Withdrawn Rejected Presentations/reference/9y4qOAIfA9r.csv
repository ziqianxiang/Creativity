title,year,conference
 Blackbox meets blackbox:Representational similarity and stability analysis of neural language models and brains,2019, arXivpreprint arXiv:1906
 Towards universal semantic tagging,2017, arXiv preprintarXiv:1709
 Predicting neuralactivity patterns associated with sentences using a neurobiologically motivated model of semanticrepresentation,2017, Cerebral Cortex
 The groningenmeaning bank,2017, In Handbook of linguistic annotation
 Hierarchical structure guides rapid linguistic predictionsduring naturalistic listening,2019, PloS one
 Enhancing machine translation with dependency-awareself-attention,2019, arXiv preprint arXiv:1909
 Language processing in brains and deep neural networks:computational convergence and its limits,2020, BioRxiv
 Minimal recursion semantics: Anintroduction,2005, Research on language and computation
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Improving zero-shot learning by mitigatingthe hubness problem,2014, arXiv preprint arXiv:1412
 Basic Linguistic Theory,2010, Oxford University Press
 Fast approximatehubness reduction for large high-dimensional data,2018, In 2018 IEEE International Conference on BigKnowledge (ICBK)
 The erp response to the amountof information conveyed by words in sentences,2015, Brain and language
 Does the brain represent words? an evaluation of brain decodingstudies of language understanding,2018, arXiv preprint arXiv:1806
 Assessing bertâ€™s syntactic abilities,2019, arXiv preprint arXiv:1901
 Announcing prague CzeCh-english dependencytreebank 2,2012,0
 A transition-based directed acyclic graphparser for UCCA,2017, In Proceedings of the 55th Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers)
 K0psala:Transition-based graph parsing via efficient training and effective encoding,2020, In Proceedings ofthe 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task onParsing into Enhanced Universal Dependencies
 Who did what to whom?a contrastive study of syntacto-semantic dependencies,2012, In Proceedings of the Sixth LinguisticAnnotation Workshop
 Similarity of neuralnetwork representations revisited,2019, arXiv preprint arXiv:1905
 Representational similarity analysis-connecting the branches of systems neuroscience,2008, Frontiers in systems neuroscience
 Linguisticknowledge and transferability of contextual representations,2019, arXiv preprint arXiv:1903
 Emergentlinguistic structure in artificial neural networks trained by self-supervision,2020, Proceedings of theNational Academy of Sciences
 Targeted syntactic evaluation of language models,2019, Proceedings ofthe Societyfor Computation in Linguistics
 Predicting human brain activity associated with themeanings of nouns,2008, science
 Toward a universal decoder of linguistic meaningfrom brain activation,2018, Nature communications
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 Artificial neural networks accurately predict languageprocessing in the brain,2020, bioRxiv
 Inducing brain-relevant bias in natural languageprocessing models,2019, In Advances in Neural Information Processing Systems
 The meaning of the sentence and its semantic andpragmatic aspects,1986, academia
 A gold standard dependency corpus for english,2014, InLREC
 Linguistically-informed self-attention for semantic role labeling,2018, In Proceedings of the 2018 Conference onEmpirical Methods in Natural Language Processing
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2018, In Proceedingsof the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks forNLP
 Fine-grained neuraldecoding with distributed word representations,2020, Information Sciences
 Aligning context-based statisticalmodels of language with brain activity during reading,2014, In Proceedings of the 2014 Conferenceon Empirical Methods in Natural Language Processing (EMNLP)
