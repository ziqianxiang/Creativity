title,year,conference
 Imagenet classification with deep convolu-tional neural networks,2012, In Advances in neural information processing Systems
 Dota 2 with large scaledeep reinforcement learning,2019, arXiv preprint arXiv:1912
 Alphastar:Mastering the real-time strategy game starcraft ii,2019, DeepMind blog
 Deep knowledge tracing,2015, In Advances in neural information processingsystems
 On empirical comparisons of optimizers for deep learning,2019, arXiv preprint arXiv:1910
 Learning step size controllers for robustneural network training,2016, In Thirtieth AAAI Conference on Artificial Intelligence
 Learning to learn by gradient descent by gradient descent,2016, In Advances inNeural Information Processing Systems
 Learning gradient descent: Better generalization and longerhorizons,2017, arXiv preprint arXiv:1703
 Learning unsupervisedlearning rules,2018, arXiv preprint arXiv:1804
 Meta-learning biologically plausible semi-supervised update rules,2019, bioRxiv
 Using a thousand optimization tasks to learn hyperparameter search strategies,2020, arXivpreprint arXiv:2002
 Ai memo 39-the new compiler,1962, Technical report
 Bilevel programming forhyperparameter optimization and meta-learning,2018, arXiv preprint arXiv:1806
 Evolutionsstrategie-optimierung technisher Systeme nach prinzipien der biologis-chen evolution,1973, 1973
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Learning transferable architecturesfor scalable image recognition,2018, Proceedings of the IEEE conference on computer vision andpattern recognition
 Long short-term memory,1997, Neural computation
 Empirical evaluation ofgated recurrent neural networks on sequence modeling,2014, arXiv preprint arXiv:1412
 Masked autoregressive flow for densityestimation,2017, In Advances in Neural Information Processing Systems
 Auto-encoding variational bayes,2013, arXiv preprintarXiv:1312
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Incorporating nesterov momentum into adam,2016, 2016
 Sgdr: Stochastic gradient descent with warm restarts,2016, arXivpreprint arXiv:1608
 On thetunability of optimizers in deep learning,2019, arXiv preprint arXiv:1910
 Identity mappings in deep residualnetworks,2016, In European conference on computer vision
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Neural optimizer search with reinforcementlearning,2017, 2017
 On the optimization of a synapticlearning rule,1992, In Preprints Conf
 Evolution and design of distributed learningrules,2000, In Combinations of Evolutionary Computation and Neural Networks
 Advances in optimizingrecurrent networks,2013, In 2013 IEEE International Conference on Acoustics
 Neural turing machines,2014, arXiv preprintarXiv:1410
 Updating quasi-newton matrices with limited storage,1980, Mathematics of computation
 Quantifying generalizationin reinforcement learning,2018, arXiv preprint arXiv:1812
 Taking thehuman out of the loop: A review of bayesian optimization,2015, Proceedings of the IEEE
 Learning to optimize neural nets,2017, arXiv preprint arXiv:1703
 Proximal policyoptimization algorithms,2017, arXiv preprint arXiv:1707
 Gradient-based optimization of hyperparameters,2000, Neural computation
 Automatic differentiation of algorithms for machinelearning,2014, arXiv preprint arXiv:1404
 Gradient-based hyperparameter optimizationthrough reversible learning,2015, In International Conference on Machine Learning
 Backpropagation through time: what it does and how to do it,1990, Proceedings of theIEEE
 Unbiasing truncated backpropagation through time,2017, arXiv preprintarXiv:1705
 Model-agnostic meta-learning for fast adaptation ofdeep networks,2017, arXiv preprint arXiv:1703
 An investigation of the gradient descent process in neural networks,1996, PhD thesis
 Pipps: Flexible model-basedpolicy search robust to the curse of chaos,2018, In International Conference on Machine Learning
 Variational optimization,2012, arXiv preprint arXiv:1212
 About convergence of random search method in extremal control of multi-parametersystems,1963, Avtomat
 Evolution strategies as ascalable alternative to reinforcement learning,2017, arXiv preprint arXiv:1703
 Guidedevolutionary strategies: Augmenting random search with surrogate gradients,2019, In InternationalConference on Machine Learning
 Matching networks for oneshot learning,2016, In Advances in neural information processing systems
 Evolved policy gradients,2018, arXiv preprint arXiv:1802
 Discovering reinforcement learning algorithms,2020, arXiv preprint arXiv:2007
 The google file system,2003, In Proceedings ofthe nineteenth ACM symposium on Operating systems principles
 Tensorflow: A system for large-scalemachine learning,2016, In OSDI
 Andrychowicz et al,1989, (2016) makes use of an LSTM(Hochreiterand Schmidhuber
1 Hyper parameter optimizationOne of the most common family of outer-learning methods used taken from the family of hyper-parameter optimization methods,2015, In the context of learning optimizers
