title,year,conference
 A convergence analysis of gradientdescent for deep linear neural networks,2018, CoRR
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, arXiv preprint arXiv:1802
 Implicit regularization in deep matrixfactorization,2019, arXiv preprint arXiv:1905
 Global optimality of local search forlow rank matrix recovery,2016, In Advances in Neural Information Processing Systems
 Convex optimization,2004, Cambridge university press
 A closerlook at few-shot classification,2018, In International Conference on Learning Representations
 Width provably matters in optimization for deep linear neural networks,1655, InInternational Conference on Machine Learning
 Convex geometry of two-layer relu networks: Implicit autoencodingand interpretable models,2020, In Silvia Chiappa and Roberto Calandra (eds
 Linearsemi-infinite optimization,1998, 01 1998
 Decorrelated batch normalization,2018, In Proceedingsof the IEEE Conference on Computer Vision and Pattern Recognition
 Gradient descent aligns the layers of deep linear networks,2019, InInternational Conference on Learning Representations
 Deep linear networks with arbitrary loss: All local minima areglobal,2018, In International Conference on Machine Learning
 In search of the real inductive bias: On therole of implicit regularization in deep learning,2014, arXiv preprint arXiv:1412
 Minimum “norm” neural networks are splines,2019, arXiv preprintarXiv:1910
 Exact solutions to the nonlinear dynam-ics of learning in deep linear neural networks,2013, arXiv preprint arXiv:1312
 Exponential convergence time of gradient descent for one-dimensional deep linearneural networks,2018, arXiv preprint arXiv:1809
 Fsnet: Feature selection network on high-dimensional biologicaldata,2020, arXiv preprint arXiv:2001
 On the margin theory of feedforward neuralnetworks,2018, arXiv preprint arXiv:1810
 Deep neural networks with multi-brancharchitectures are intrinsically less non-convex,2019, In The 22nd International Conference on ArtificialIntelligence and Statistics
 □Lemma A,2021,1
 Thisanalysis proves that the kink of each ReLU activation occurs exactly at one of the data points,2021, □25Under review as a conference paper at ICLR 2021Proposition 4
