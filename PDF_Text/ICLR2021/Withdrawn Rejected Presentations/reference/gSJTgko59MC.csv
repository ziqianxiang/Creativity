title,year,conference
 Mean-field analysis of two-layer neuralnetworks: Non-asymptotic rates and generalization bounds,2020, arXiv preprint arXiv:2002
 Kernel methods for deep learning,2009, In Proceedings of the22Nd International Conference on Neural Information Processing Systems
 Asymptotics of wide networks from feynman diagrams,2020, In InternationalConference on Learning Representations
 Finite depth and width corrections to the neural tangent kernel,2019, arXivpreprint arXiv:1909
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Finite versus infinite neural networks: an empirical study,2020, arXivpreprint arXiv:2007
 Tensor Methods in Statistics,2017, Dover Books on Mathematics
 A mean field view of the landscape of two-layer neural networks,0027, Proceedings ofthe National Academy of Sciences
 Rank Correlation and Product-Moment Correlation,1948, Biometrika
 Mcmc using hamiltonian dynamics,2011, Handbook of markov chain monte carlo
 On the use of the edgeworth expansion incosmology i: how to foresee and evade its pitfalls,2017, arXiv preprint arXiv:1709
 Understanding gaussian process regression using theequivalent kernel,2004, In International Workshop on Deterministic and Statistical Methods in MachineLearning
 Consistency and fluctuations forstochastic gradient langevin dynamics,2016, J
 A mean-field theory of lazy training in two-layer neural nets:entropic regularization and controlled mckean-vlasov dynamics,2020, arXiv preprint arXiv:2002
 Non-gaussian processes and neural networks at finite widths,2020, In Mathematical andScientific Machine Learning
 Quantum Field Theory in a Nutshell,2003, Nutshell handbook
 Using the tools developed byCohen et al,2021, (2019)
