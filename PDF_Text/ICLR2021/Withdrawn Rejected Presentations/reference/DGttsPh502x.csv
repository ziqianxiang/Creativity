title,year,conference
 Generating sentences from a continuous space,2016, In Proceedings of The 20th SIGNLLConference on Computational Natural Language Learning
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Language gans falling short,2020, In International Conference on Learning Representations
 Electra: Pre-trainingtext encoders as discriminators rather than generators,2020, In International Conference on LearningRepresentations
 Plug and play language models: A simple approach to con-trolled text generation,2020, In International Conference on Learning Representations
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Cyclicalannealing schedule: A simple approach to mitigating KL vanishing,2019, In Proceedings of the 2019Conference of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies
 Lagging inferencenetworks and posterior collapse in variational autoencoders,2019, In Proceedings of ICLR
 The curious case of neural textdegeneration,2020, In International Conference on Learning Representations
 Toward con-trolled generation of text,2017, volume 70 of Proceedings of Machine Learning Research
 Disentangled representationlearning for non-parallel text style transfer,2019, In Proceedings of the 57th Annual Meeting of theAssociation for Computational Linguistics
 CTRL- A Conditional Transformer Language Model for Controllable Generation,2019, arXiv preprintarXiv:1909
 Auto-encoding variational bayes,2013, arXiv preprintarXiv:1312
 Multiple-attribute text rewriting,2019, In International Conference on LearningRepresentations
 Opti-mus: Organizing sentences via pre-trained modeling of a latent space,2020, In EMNLP
 Exploring controllable text generation tech-niques,2020, ArXiv
 Style transfer from non-parallel textby cross-alignment,2017, In Advances in neural information processing systems
 Improved varia-tional autoencoders for text modeling using dilated convolutions,2017, In International Conference onMachine Learning
 A batch normalizedinference network keeps the KL vanishing away,2020, In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics
