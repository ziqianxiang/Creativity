title,year,conference
 Large scale distributed neural network training through online distillation,2018, InternationalConference on Learning Representations
 Wavelab and reproducible research,1995, In Wavelets andStatistics
 Launch and iterate: Reducingprediction churn,2016, In Advances in Neural Information Processing Systems 29
 Semi-supervised learning by entropy minimization,2005, InLaWrence K
 On calibration of modern neuralnetWorks,2017, In International Conference on Machine Learning
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Identity mappings in deep residualnetWorks,2016, In European conference on computer vision
 Distilling the knoWledge in a neural netWork,2015, arXivpreprint arXiv:1503
 Av-eraging Weights leads to Wider optima and better generalization,2018, arXiv preprint arXiv:1803
 Some new bounds on the general-ization error of combined classifiers,2001, In Advances in Neural Information Processing Systems 13
 Simple and scalable predictiveuncertainty estimation using deep ensembles,2017, In I
 Knowledge distillation by on-the-fly native ensemble,2018, InAdvances in Neural Information Processing Systems 31
 Co-validation: Using model disagreement onunlabeled data to validate classification algorithms,2004, Advances in neural information processingsystems
 Accessible reproducible research,2010, Science
 Non-determinism in tensorflow resnets,2020, arXiv preprintarXiv:2001
 In search of the real inductive bias: Onthe role of implicit regularization in deep learning,2015, In International Conference on LearningRepresentations workshop track
 Reproducible research in computational science,0036, Science
 Regularizingneural networks by penalizing confident output distributions,2017, arXiv preprint arXiv:1701
 Training deep neural networks on noisy labels with bootstrapping,2015, In Yoshua Bengioand Yann LeCun (eds
 Tensimple rules for reproducible research in jupyter notebooks,2018, arXiv preprint arXiv:1810
 Collaborative learning for deep neural networks,2018, In Advances inNeural Information Processing Systems 31
 Understand-ing deep learning requires rethinking generalization,2017, In International Conference on LearningRepresentations
 Deep mutual learning,2018, InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition
