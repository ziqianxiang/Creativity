title,year,conference
 On the optimization of deep networks:Implicit acceleration by overparameterization,2018, In International Conference on MachineLearning (ICML)
 A convergence analysis ofgradient descent for deep linear neural networks,2019, In 7th International Conference onLearning Representations
 On the exponential convergence of the time-invariant matrix Riccati differential equation,1992, In Proceedings of the 31st IEEE Conferenceon Decision and Control
 Dropout as a low-rank regularizer for matrix factorization,2018, In ArtificialIntelligence and Statistics (AISTATS)
 Implicit bias of gradient descent for wide two-layer neuralnetworks trained with the logistic loss,2020, In arXiv preprint arXiv:2002
 Width provably matters in optimization for deep linear neuralnetworks,2019, In International Conference on Machine Learning
 Algorithmic regularization in learning deep homo-geneous models: Layers are automatically balanced,2018, In Advances in Neural InformationProcessing Systems (NeurIPS)
 Gradient descent provablyoptimizes over-parameterized neural networks,2018, In International Conference on LearningRepresentations
 On dissipative symplectic integration with applicationsto gradient-based optimization,2020, arXiv:2004
 Implicit regularization of discretegradient dynamics in linear neural networks,2019, In Advances in Neural Information ProcessingSystems (NeurIPS)
 Characterizing implicitbias in terms of optimization geometry,2018, In Proceedings of the 35th International Conferenceon Machine Learning (ICML)
 Implicit bias of gradientdescent on linear convolutional networks,2018, In Advances in Neural Information ProcessingSystems 31 (NeurIPS)
 Gradient descent aligns the layers of deep linear networks,2019, InInternational Conference on Learning Representations (ICLR)
 On the regularizationproperties of structured dropout,2020, In IEEE Conference on Computer Vision and PatternRecognition
 On the finite escape phenomena for matrix Riccati equations,1982, IEEETransactions on Automatic Control
 Exact solutions to the nonlineardynamics of learning in deep linear neural networks,2014, In International Conference onLearning Representations (ICLR)
 A mathematical theory ofsemantic development in deep neural networks,0027, Proceedings of the National Academy ofSciences
 The implicit bias of gradient descent onseparable data,2018, In International Conference on Learning Representations (ICLR)
