title,year,conference
 High-dimensional dynamics of generalization error in neuralnetworks,2017, CoRR
 Optimizing neural architecture search usinglimited GPU time in a dynamic search space: A gene expression programming approach,2020, CoRR
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, In KamalikaChaudhuri and Ruslan Salakhutdinov (eds
 Learning long-term dependencies withgradient descent is difficult,1994, IEEE Trans
 A downsampled variant of imagenet as analternative to the CIFAR datasets,2017, CoRR
 Noisy differentiable architecture search,2020, CoRR
 Nas-bench-201: Extending the scope of reproducible neural architecturesearch,2020, In 8th International Conference on Learning Representations
 Gradient descent finds globalminima of deep neural networks,2019, In Proceedings of the 36th International Conference on MachineLearning
 Gradient descent provably optimizesover-parameterized neural networks,2019, In 7th International Conference on Learning Representations
 Adaptive subgradient methods for online learningand stochastic optimization,2011, J
 A study of gradient variance in deeplearning,2020, CoRR
 BOHB: robust and efficient hyperparameter optimiza-tion at scale,2018, In Proceedings of the 35th International Conference on Machine Learning
 Fastneural network adaptation via parameter remapping and architecture search,2020, In 8th InternationalConference on Learning Representations
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the Thirteenth International Conference on Artificial Intelligence andStatistics
 How to start training: The effect of initialization and architecture,2018, InAdvances in Neural Information Processing Systems 31: Annual Conference on Neural InformationProcessing Systems 2018
 Surprises in high-dimensional ridgeless least squares interpolation,2019, CoRR
 Fednas: Federated deep learning vianeural architecture search,2020, CoRR
 Milenas: Efficient neural architecture searchvia mixed-level reformulation,2020, CoRR
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In 2015 IEEE International Conference onComputer Vision
 Deep residual learning for imagerecognition,2016, In 2016 IEEE Conference on Computer Vision and Pattern Recognition
 Long short-term memory,1997, Neural Computation
 Densely connectedconvolutional networks,2017, In 2017 IEEE Conference on Computer Vision and Pattern Recognition
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In Proceedings of the 32nd International Conference on MachineLearning
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, In Samy Bengio
 Improved differentiablearchitecture search for language modeling and named entity recognition,2019, In Proceedings of the2019 Conference on Empirical Methods in Natural Language Processing and the 9th InternationalJoint Conference on Natural Language Processing
 Self-normalizingneural networks,2017, In Advances in Neural Information Processing Systems 30: Annual Conferenceon Neural Information Processing Systems 2017
 Wide neural networks of any dePth evolve as linear modelsunder gradient descent,2019, In Hanna M
 Layer normalization,2016, arXiv preprintarXiv:1607
 DARTS: differentiable architecture search,2019, In 7thInternational Conference on Learning Representations
 All you need is a good init,2016, In 4th International Conference onLearning Representations
 Transformers without tears: ImProving the normalization ofself-attention,2019, CoRR
 On the difficulty of training recurrent neuralnetworks,2013, In Proceedings of the 30th International Conference on Machine Learning
 The evolved transformer,2019, In Proceedings of the36th International Conference on Machine Learning
 On the imPortance ofinitialization and momentum in deeP learning,2013, In Proceedings of the 30th International Conferenceon Machine Learning
 Instance normalization: The missingingredient for fast stylization,2016, CoRR
 Simple statistical gradient-following algorithms for connectionist reinforcementlearning,1992, Mach
 Greedynas:Towards fast one-shot NAS with greedy supernet,2020, CoRR
 Fixup initialization: Residual learning withoutnormalization,2019, In 7th International Conference on Learning Representations
 Stochastic optimization with importance sampling for regularized lossminimization,2015, In Francis R
 Econas: Finding proxies for economical neural architecture search,2020, CoRR
