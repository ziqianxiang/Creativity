title,year,conference
 Multi-residual networks: Improving the speed and accuracy ofresidual networks,2016, arXiv preprint arXiv:1609
 Connectivity learning in multi-branch networks,2017, arXiv preprintarXiv:1709
 A convergence theory for deep learning via over-parameterization,2018, arXiv preprint arXiv:1811
 Hybrid attentive transformer,2020, 2020
 Onexact computation with an infinitely wide neural net,2019, In Advances in Neural Information ProcessingSystems
 On the global convergence of gradient descent for over-parameterizedmodels using optimal transport,2018, In Advances in Neural Information Processing Systems 31
 Xception: Deep learning with depthwise separable convolutions,2017, In Proceedings ofthe IEEE conference on computer vision and pattern recognition
 Batch normalization biases residual blocks towards the identity function in deepnetworks,2020, arXiv preprint arXiv:2002
 Gradient descent finds globalminima of deep neural networks,2018, arXiv preprint arXiv:1811
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the Thirteenth International Conference on Artificial Intelligence andStatistics
 Densely connectedconvolutional networks,2017, In The IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 Polylogarithmic width suffices for gradient descent to achievearbitrarily small test error with shallow ReLU networks,2019, arXiv preprint arXiv:1909
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Learning multiple layers of features from tiny images,2009, 2009
 Wide neural networks of any depth evolve as linear modelsunder gradient descent,2019, In Advances in neural information processing Systems
 Selective kernel networks,2019, In Proceedings ofthe IEEE conference on computer vision and pattern recognition
 Scaling neural machine translation,2018, InProceedings of the Third Conference on Machine Translation (WMT)
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Aggregated residualtransformations for deep neural networks,2017, In IEEE Conference on Computer Vision and PatternRecognition (CVPR)
 Mean field residual networks: On the edge of chaos,2017, In Advancesin Neural Information Processing Systems
 Amean field theory of batch normalization,2018, In International Conference on Learning Representations
 Resnest: Split-attention networks,2020, arXiv preprintarXiv:2004
 Deep neural networks with multi-brancharchitectures are less non-convex,2018, arXiv preprint arXiv:1806
 Fixup initialization: Residual learning withoutnormalization,2019, In International Conference on Learning Representations (ICLR)
 Stability and convergence theoryfor learning resnet: A full characterization,2019, arXiv preprint arXiv:1903
 Towards robustresnet: A small step but a giant leap,2019, In International Joint Conferences on Artificial Intelligence(IJCAI)
 Deep recurrent models with fast-forwardconnections for neural machine translation,2016, Transactions of the Association for ComputationalLinguistics
 An improved analysis of training over-parameterized deep neuralnetworks,2019, In Advances in Neural Information Processing Systems
 Stochastic gradient descent optimizesover-parameterized deep relu networks,2018, arXiv preprint arXiv:1811
