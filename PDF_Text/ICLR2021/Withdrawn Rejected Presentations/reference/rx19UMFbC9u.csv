title,year,conference
 Stronger generalization bounds for deepnets via a compression approach,2018, In International Conference on Machine Learning
 Language models are few-shot learners,2020, CoRR
" ""learning-compression"" algorithms for neural netpruning",2018, In IEEE Conference on Computer Vision and Pattern Recognition
 A back-propagation algorithm with optimal use of hidden units,1988, In Advances inNeural Information Processing Systems
 Rigging the lottery:Making all tickets winners,2019, CoRR
 The state of sparsity in deep neural networks,2019, CoRR
 Dynamic network surgery for efficient dnns,2016, In Advancesin Neural Information Processing Systems
 Optimal brain surgeon and general networkpruning,1993, In International Conference on Neural Networks
 Deep residual learning for imagerecognition,2016, In IEEE Conference on Computer Vision and Pattern Recognition
 Structural learning with forgetting,1996, Neural Networks
 A simple procedure for pruning back-propagation trained neural networks,1990, IEEETrans
 Optimal brain damage,1989, In Advances in NeuralInformation Processing Systems
 Gradient-basedlearning applied to document recognition,1998, Proceedings of the IEEE
 SNIP: single-shot network pruningbased on connection sensitivity,2019, In International Conference on Learning Representations
 Pruning filters forefficient convnets,2017, In International Conference on Learning Representations
 Autocompress:An automatic DNN structured pruning framework for ultra-high compression rates,2020, In AAAIConference on Artificial Intelligence
 Rethinking the value ofnetwork pruning,2019, In International Conference on Learning Representations
 Learning sparse neural networks throughl_0 regularization,2018, In International Conference on Learning Representations
 Scalable training of artificial neural networks with adaptive sparse connectivityinspired by network science,2018, Nature Communications
 Variational dropout sparsifies deepneural networks,2017, In International Conference on Machine Learning
 Parameter efficient training of deep convolutional neural networks bydynamic sparse reparameterization,2019, In International Conference on Machine Learning
 Exploring sparsity in recurrentneural networks,2017, In International Conference on Learning Representations
 Lookahead: A far-sighted alternative ofmagnitude-based pruning,2020, In International Conference on Learning Representations
 Pruning algorithms-a survey,1993, IEEE Trans
 Comparing rewinding and fine-tuning in neuralnetwork pruning,2020, In International Conference on Learning Representations
 Efficientnet: Rethinking model scaling for convolutional neuralnetworks,2019, In International Conference on Machine Learning
 Fixing the train-test resolutiondiscrepancy: Fixefficientnet,2020, CoRR
 Picking winning tickets before training bypreserving gradient flow,2020, In International Conference on Learning Representations
