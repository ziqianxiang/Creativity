title,year,conference
 Stochastic Mirror Descent on OverparameterizedNonlinear Models,2019, In International Conference on Machine Learning (ICML) GeneralizationWorkshop
 On exponential convergence of sgd in non-convexover-parametrized learning,2018, arXiv preprint arXiv:1811
 Reconciling modern machine-learning practice and the classical bias-variance trade-off,2019, Proceedings of the National Academyof Sciences
 Characterizing implicit bias interms of optimization geometry,2018, In International Conference on Machine Learning (ICML)
 Mirrorless Mirror Descent: A MoreNatural Discretization of Riemannian Gradient Flow,2020, arXiv preprint arXiv:2004
 Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-Lojasiewicz Condition,2016, In Joint European Conference onMachine Learning and Knowledge Discovery in Databases
 Adam: A method for stochastic optimization,2015, In InternationalConference on Learning Representations (ICLR)
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In Advances in Neural Information Processing Systems (NeurIPS)
 Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning,2020, arXiv preprintarXiv:2003
 Problem Complexity and Method Efficiency in Opti-mization,1983, SIAM
 Mirror descent and nonlinear pro-jected subgradient methodsfor convex optimization,2015, Machine Learning
 Theoretical insights into the optimiza-tion landscape of over-parameterized shallow neural networks,2019, IEEE transaction on InformationTheory (2018)
 Fast and Faster Convergence of SGD for Over-Parameterized Models and an Accelerated Perceptron,2019, In International Conference on ArtificialIntelligence and Statistics (AISTATS)
 AdaGrad Stepsizes: Sharp Convergence Over Non-convex Landscapes,2019, In International Conference on Machine Learning (ICML)
 Global convergence of adaptive gradient methods foran over-parameterized neural network,2019, arXiv preprint arXiv:1902
 Linear Convergence of Adaptive Stochastic GradientDescent,2020, In International Conference on Artificial Intelligence and Statistics (AISTATS)
