title,year,conference
 Layer normalization,2016, arXiv preprintarXiv:1607
 Quasi-recurrent neural net-works,2016, arXiv preprint arXiv:1611
 Generating long sequences with sparsetransformers,2019, CoRR
 Empirical evaluation ofgated recurrent neural networks on sequence modeling,2014, arXiv preprint arXiv:1412
 Pixel recursive super resolution,2017, In ICCV
 Transformer-xl:Attentive language models beyond a fixed-length context,2019, ArXiv
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Long short-term memory,1997, Neural computation
 Interlaced sparseself-attention for semantic segmentation,2019, ArXiv
 Reformer: The efficient transformer,2020, ArXiv
 Polygen: An autoregressivegenerative model of 3d meshes,2020, ICML
 Image transformer,2018, arXiv preprint arXiv:1802
 Compressive transformersfor long-range sequence modelling,2020, ArXiv
 Stand-alone self-attention in vision models,2019, ArXiv
 Efficient content-based sparse attentionwith routing transformers,2020, ArXiv
 Pixelcnn++: Improving thepixelcnn with discretized logistic mixture likelihood and other modifications,2017, arXiv preprintarXiv:1701
 Adaptive attention span intransformers,2019, In ACL
 Sparse sinkhorn attention,2020, ArXiv
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Axial-deeplab: Stand-alone axial-attention for panoptic segmentation,2020, ArXiv
 Linformer: Self-attentionwith linear complexity,2020, ArXiv
