title,year,conference
 Onexact computation with an infinitely wide neural net,2019, In H
 There are manyconsistent explanations of unlabeled data: Why you should average,2019, In International Conferenceon Learning Representations
 The true sample complexity of activelearning,2010, Machine Learning
 Adversarial active learning for deep networks: a marginbased approach,2018, 02 2018
 Batchbald: Efficient and diverse batchacquisition for deep bayesian active learning,2019, In H
 Pseudo-label : The simple and efficient semi-supervised learning method for deepneural networks,2013, ICML 2013 Workshop : Challenges in Representation Learning (WREPL)
 Risk bounds for statistical learning,2006, The Annals of Statistics
 Neural tangents: Fast and easy infinite neural networks in python,2020, InInternational Conference on Learning Representations
 Realisticevaluation of deep semi-supervised learning algorithms,2018, In S
 Active learning for convolutional neural networks: A core-setapproach,2018, In International Conference on Learning Representations
 Fixmatch: Simplifying semi-supervised learning withconsistency and confidence,2020, arXiv preprint arXiv:2001
 Mean teachers are better role models: Weight-averaged consistencytargets improve semi-supervised deep learning results,2017, In I
 A new active labeling method for deep learning,2014, In 2014 International JointConference on Neural Networks (IJCNN)
 Cost-effective active learning fordeep image classification,2017, IEEE Transactions on Circuits and Systems for Video Technology
 SVHN used leaky ReLU with negative slope0,1024,1 for Mean Teacher
