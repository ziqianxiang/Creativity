title,year,conference
 Theoretical analysis of auto rate-tuning by batchnormalization,2019, In International Conference on Learning Representations (ICLR)
 Layer normalization,2016, arXiv preprintarXiv:1607
 Understanding batch nor-malization,2018, In Advances in Neural Information Processing Systems (NeurIPS)
 Training a 3-node neural network is np-complete,1989, In Advances inNeural Information Processing Systems (NeurIPS)
 A quantitative analysis of the effect of batchnormalization on gradient descent,2019, In 36th International Conference on Machine Learning (ICML)
 Riemannian approach to batch normalization,2017, In Advances inNeural Information Processing Systems (NeurIPS)
 An investigation into neural net optimizationvia hessian eigenvalue density,2019, In 36th International Conference on Machine Learning (ICML)
 Norm matters: efficient and accuratenormalization schemes in deep networks,2018, In Advances in Neural Information Processing Systems(NeurIPS)
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In 32nd International Conference on Machine Learning (ICML)
 Adam: A method for stochastic optimization,2015, In InternationalConference on Learning Representations (ICLR)
 Revisit batch normalization: New understanding and refinement viacomposition optimization,2019, In The 22nd International Conference on Artificial Intelligence andStatistics (AISTATS)
 Readingdigits in natural images with unsupervised feature learning,2011, In NIPS Workshop on Deep Learningand Unsupervised Feature Learning
 Very deep convolutional networks for large-scale imagerecognition,2015, In International Conference on Learning Representations (ICLR)
 On the importance of initializationand momentum in deep learning,2013, In 30th International Conference on Machine Learning (ICML)
 Instance normalization: The missingingredient for fast stylization,2016, arXiv preprint arXiv:1607
 Group normalization,2018, In Proceedings of the European Conference onComputer Vision (ECCV)
