title,year,conference
 Unitary evolution recurrent neural networks,2016, InInternational Conference on Machine Learning
 Understanding deep neuralnetworks with rectified linear units,2016, arXiv preprint arXiv:1611
 Deep cauchy hashing for hamming spaceretrieval,2018, In IEEE Conference on Computer Vision and Pattern Recognition
 Hashnet: Deep learning to hashby continuation,2017, In IEEE International Conference on Computer Vision
 Rethinking the faster r-cnn architecture for temporal action localization,2018, In IEEEConference on Computer Vision and Pattern Recognition
 Long-term recurrent convolutional networks for visualrecognition and description,2015, In IEEE Conference on Computer Vision and Pattern Recognition
 Deep sparse rectifier neural networks,2011, InInternational Conference on Artificial Intelligence and Statistics
 Complexity of linear regions in deep networks,2019, In InternationalConference on Machine Learning
 Deep relu networks have surprisingly few activation patterns,2019, InAdvances in Neural Information Processing Systems
 Piecewise linear activations substantially shape theloss surfaces of neural networks,2020, In International Conference on Learning Representations
 Delving deep into rectifiers: Surpass-ing human-level performance on imagenet classification,2015, In IEEE International Conference onComputer Vision
 Deep residual learning for image recog-nition,2016, In Conference on Computer Vision and Pattern Recognition
 Identity mappings in deep residualnetworks,2016, In European Conference on Computer Vision
 The “echo state” approach to analysing and training recurrent neural networks-withan erratum note,2001, Bonn
 Equivalent and approximate transfor-mations of deep neural networks,2019, arXiv preprint arXiv:1905
 Simultaneous feature learning and hash codingwith deep neural networks,2015, In IEEE Conference on Computer Vision and Pattern Recognition
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Least squares quantization in pcm,1982, IEEE Transactions on Information Theory
 Rectifier nonlinearities improve neural net-work acoustic models,2013, In International Conference on Machine Learning
 On the number of response regions of deepfeed forward networks with piece-wise linear activations,2013, arXiv preprint arXiv:1312
 Expo-nential expressivity in deep neural networks through transient chaos,2016, In Advances in NeuralInformation Processing Systems
 Bounding and counting linearregions of deep neural networks,2018, In International Conference on Machine Learning
 Two-stream convolutional networks for action recognitionin videos,2014, In Advances in Neural Information Processing Systems
 Very deep convolutional networks for large-scale imagerecognition,2015, In International Conference on Learning Representations
 Exponentially vanishing sub-optimal local minima in multilayerneural networks,2018, In International Conference on Learning Representations Workshop
 Long-term temporal convolutions for action recogni-tion,2017, IEEE Transactions on Pattern Analysis and Machine Intelligence
 A survey on learning to hash,2017, IEEETransactions on Pattern Analysis and Machine Intelligence
 Supervised hashing for im-age retrieval via image representation learning,2014, In AAAI Conference on Artificial Intelligence
 On the number of linearregions of convolutional neural netWorks,2020, In International Conference on Machine Learning
 Unsupervised video summarization Withcycle-consistent adversarial lstm netWorks,2019, IEEE Transactions on Multimedia
 Empirical studies on the properties of linear regions in deep neuralnetWorks,2020, In International Conference on Learning Representations
 Deep hashing netWork for efficient simi-larity retrieval,2016, In AAAI Conference on Artificial Intelligence
 Bounding the number of linear regions in local area for neuralnetWorks With relu activations,2020, arXiv preprint arXiv:2007
 Specified number of training examples of MNIST are as-signed random labels according to the label noise ratios of 0,2021,1
