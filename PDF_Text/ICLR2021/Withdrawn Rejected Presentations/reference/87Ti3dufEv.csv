title,year,conference
 Structured sparsitythrough convex optimization,2012, Statistical Science
 Libsvm: Data repository,2011, 2011
 A Fast Reduced-Space Algorithmic Framework for Sparse Optimization,2018, PhD thesis
 Farsa for `1 -regularized convex optimization:local convergence and numerical experience,2018, Optimization Methods and Software
 On the ineffectiveness of variance reduced optimization for deeplearning,2019, In Advances in Neural Information Processing Systems
 Saga: A fast incremental gradient methodwith support for non-strongly convex composite objectives,2014, In Advances in neural informationprocessing systems
 See all by looking at a few: Sparse modelingfor finding representative objects,2012, In 2012 IEEE Conference on Computer Vision and PatternRecognition
 Spider: Near-optimal non-convex opti-mization via stochastic path-integrated differential estimator,2018, In Advances in Neural InformationProcessing Systems
 Escaping from saddle points—online stochasticgradient for tensor decomposition,2015, In Conference on Learning Theory
 Convergence theorems for gradient descent,2018, University of Illinois
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Learning with dynamic group sparsity,2009, In2009 IEEE 12th International Conference on Computer Vision
 The benefit of group sparsity,2010, The Annals of Statistics
 Structured sparse principal componentanalysis,2010, In Proceedings of the Thirteenth International Conference on Artificial Intelligence andStatistics
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In Advances in neural information processing systems
 Linear convergence of gradient and proximal-gradient methods under the Polyak-IojasieWicz condition,2016, In Joint European Conference onMachine Learning and Knowledge Discovery in Databases
 Learning multiPle layers of features from tiny images,2009, Master’s thesis
 Pruning filters forefficient convnets,2016, arXiv preprint arXiv:1608
 A simPle Proximal stochastic gradient method for nonsmooth nonconvexoPtimization,2018, In Advances in neural information processing systems
 Mrireconstruction via enhanced grouP sParsity and nonconvex regularization,2018, Neurocomputing
 Thinet: A filter level Pruning method for deeP neuralnetWork comPression,2017, In Proceedings of the IEEE international conference on computer vision
 Primal-dual subgradient methods for convex Problems,2009, Mathematical programming
 Proximal stochastic methodsfor nonsmooth nonconvex finite-sum oPtimization,2016, In Advances in Neural Information ProcessingSystems
 The group-lasso for generalized linear models: uniqueness ofsolutions and efficient algorithms,2008, In Proceedings of the 25th international conference on Machinelearning
 A stochastic gradient method with an exponentialconvergence _rate for finite training sets,2012, In Advances in neural information processing Systems
 Group sparse regular-ization for deep neural networks,2017, Neurocomputing
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 A stochastic extra-step quasi-newtonmethod for nonsmooth nonconvex optimization,2019, arXiv preprint arXiv:1910
 Adaptive methodsfor nonconvex optimization,2018, In Advances in neural information processing systems
 Multi-level composite stochastic optimization via nested variancereduction,2019, arXiv preprint arXiv:1908
 Statistical adaptive stochastic gradientmethods,2020, arXiv preprint arXiv:2002
 Recovery guaranteesfor one-hidden-layer neural networks,2017, In International Conference on Machine Learning
