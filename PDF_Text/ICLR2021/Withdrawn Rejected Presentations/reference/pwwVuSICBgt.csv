title,year,conference
 Towards federated learning atscale: System design,2019, In Conference on Machine Learning and Systems
 Tiny Transfer Learning: Towards memory-efficient on-device learning,2020, In IEEE Conference on Computer Vision and Pattern Recognition
 Backprop with approximate activations for memory-efficient network training,2019, In Advances in Neural Information Processing Systems
 Training deep nets with sublinearmemory cost,2016, arXiv preprint arXiv:1604
 Training of deep networks with half-precision float,2017, In Nvidia GPU Technology Conference
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In International Conference on Artificial Intelligence and Statistics
 Low-precision batch-normalized activations,2017, arXiv preprint arXiv:1702
 Memory-efficientbackpropagation through time,2016, In Advances in Neural Information Processing Systems
 Proxybnn: Learning binarized neural networks via proxy matrices,2020, In EuropeanConference on Computer Vision
 Norm matters: Efficient and accuratenormalization schemes in deep networks,2018, In Advances in Neural Information Processing Systems
 Adam: A method for stochastic optimization,2015, In InternationalConference on Learning Representations
 Towards accurate binary convolutional neural network,2017, InConference on Neural Information Processing Systems
 Bi-RealNet: Enhancing the performance of 1-bit CNNs with improved representational capability andadvanced training algorithm,2018, In European Conference on Computer Vision
 ReActNet: Towards pre-cise binary neural network with generalized activation functions,2020, In European Conference onComputer Vision
 Mixedprecision training,2018, In International Conference on Learning Representations
 Binary neuralnetworks: A survey,2020, Pattern Recognition
 HoW does batch normalization help binary train-ing,2019, arXiv preprint arXiv:1909
 LUTNet: Learn-ing FPGA configurations for highly efficient neural network inference,2020, IEEE Transactions onComputers
 Train-ing deep neural networks with 8-bit floating point numbers,2018, In Advances in Neural InformationProcessing Systems
 TernGrad:Ternary gradients to reduce communication in distributed deep learning,2017, In Advances in NeuralInformation Processing Systems
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
 Training and inference with integers in deepneural networks,2018, In International Conference on Learning Representations
 l1-norm batch nor-malization for efficient training of deep neural networks,2018, IEEE Transactions on Neural Networksand Learning Systems
 DoReFa-Net:Training low bitwidth convolutional neural networks with low bitwidth gradients,2016, arXiv preprintarXiv:1606
