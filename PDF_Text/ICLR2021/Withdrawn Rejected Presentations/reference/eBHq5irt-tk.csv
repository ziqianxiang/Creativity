title,year,conference
 Benign overfitting in linear regression,2019, arXivpreprint arXiv:1906
 Reconciling modern machine-learning practice and theclassical bias-variance trade-off,2019, Proceedings of the National Academy of Sciences
 Language models are few-shot learners,2020, arXiv preprintarXiv:2005
 Optimal rates for the regularized least-squares algorithm,2007, Foundationsof Computational Mathematics
 High-dimensional asymptotics of prediction: Ridge regression andclassification,2018, Ann
 The elements of statistical learning,2001, Springer series instatistics New York
 Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration,2018, In Advances in Neural InformationProcessing Systems
 An investigation into neural net optimization via hessianeigenvalue density,2019, In International Conference on Machine Learning
 Developments in maximum entropy data analysis,1989, In Maximum entropy and Bayesianmethods
 Generalized additive models,1990, Chapman and Hall
 Surprises in High-Dimensional RidgelessLeast Squares Interpolation,2019, arXiv:1903
 Identity mappings in deep residual networks,2016, In Europeanconference on computer vision
 Flat Minima,1997, Neural Computation
 Understandinggeneralization through visualizations,2019, arXiv preprint arXiv:1906
 Subspaceinference for bayesian deep learning,2019, Uncertainty in Artificial Intelligence (UAI)
 Fantastic generalization measuresand where to find them,2020, In International Conference on Learning Representations (ICLR)
 On large-batch trainingfor deep learning: Generalization gap and sharp minima,2017, International Conference on LearningRepresentations (ICLR)
 Adam: A method for stochastic optimization,2015, International Conference onLearning Representations (ICLR) arXiv:1412
 Eigenvalues of covariance matrices: Application to neural-network learning,1991, Physical Review Letters
 Bayesian model comparison and backprop nets,1992, In Advances in neural informationprocessing Systems
 Bayesian interpolation,1992, Neural computation
 A simple baseline for bayesianuncertainty in deep learning,2019, In Advances in neural information processing systems
 Detecting extrapolation with local ensembles,2019, arXivpreprint arXiv:1910
 The generalization error of random features regression: Precise asymptoticsand double descent curve,2019, arXiv preprint arXiv:1908
 The effective number of parameters: An analysis of generalization and regularization innonlinear learning systems,1992, In Advances in neural information processing systems
 Deep double descent: Wherebigger models and more data hurt,2020, In International Conference on Learning Representations(ICLR)
 High Dimensional Classification with Bayesian Neural Networks andDirichlet Diffusion Trees,2006, In I
 Exploring generalization in deeplearning,2017, In Advances in Neural Information Processing Systems
 The role of over-parametrizationin generalization of neural networks,2018, In International Conference on Learning Representations
 The full spectrum of deepnet hessians at scale: Dynamics with sgd training and samplesize,2018, arXiv preprint arXiv:1811
 Composable effects for flexible and accelerated probabilisticprogramming in numpyro,2019, arXiv preprint arXiv:1912
 Eigenvalues of the hessian in deep learning: Singularity andbeyond,2017, In ICLR Workshop track
 Outrageously largeneural networks: The sparsely-gated mixture-of-experts layer,2017, In International Conference onLearning Representations (ICLR)
 Fast generalization error bound of deep learning from a kernel perspective,2018, In A
 Going deeper with convolutions,2015, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Pyhessian: Neural networks through the lens ofthe hessian,2019, arXiv preprint arXiv:1912
 Understanding deep learning requiresrethinking generalization,2017, International Conference on Learning Representations (ICLR)
 Learning bounds for kernel regression using effective data dimensionality,2005, NeuralComputation
 Non-vacuous generalization boundsat the imagenet scale: a pac-bayesian compression approach,2018, In International Conference onLearning Representations (ICLR)
 44 of Jiang et al,2017, (2020))
