title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, arXiv preprintarXiv:1901
 Document image defect models,1992, In Structured Document Image Analysis
 Proximal point methods for optimization with non-convex functional constraints,2019, arXiv preprint arXiv:1908
 Convex Optimization,2004, Cambridge University Press
 Stability and generalization of learning algorithmsthat converge to global optima,2018, In International Conference on Machine Learning
 Invariance reduces variance: Understanding dataaugmentation in deep learning and beyond,2019, arXiv preprint arXiv:1907
 Autoaugment:Learning augmentation strategies from data,2019, In Proceedings of the IEEE conference on computervision and pattern recognition
 Incorporating nesterov momentum into adam,2016, 2016
 Gradient descent finds globalminima of deep neural networks,2019, In International Conference on Machine Learning
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
 Mini-batch stochastic approximation meth-ods for nonconvex stochastic composite optimization,2016, Mathematical Programming
 On the complexity of an augmented lagrangian methodfor nonconvex optimization,2019, arXiv preprint arXiv:1906
 Surprises in high-dimensional ridgeless least squares interpolation,2019, arXiv preprint arXiv:1903
 Faster autoaugment:Learning augmentation strategies using backpropagation,2019, arXiv preprint arXiv:1911
 Bag of tricks forimage classification with convolutional neural networks,2019, In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition
 Neural networks for machine learninglecture 6a overview of mini-batch gradient descent,2012, 2012
 Population based augmentation:Efficient learning of augmentation policy schedules,2019, In International Conference on MachineLearning
 Adam: A method for stochastic optimization,2015, In InternationalConference on Learning Representations
 Imagenet classification With deep convo-lutional neural netWorks,2012, In Advances in Neural Information Processing Systems
 Exponential step sizes for non-convex opti-mization,2020, arXiv preprint arXiv:2002
 A simple proximal stochastic gradient method for nonsmooth nonconvexoptimization,2018, In Advances in Neural Information Processing Systems
 Fast autoaugment,2019, InAdvances in Neural Information Processing Systems
 Online hyper-parameter learning for auto-augmentation strategy,2019, In Proceedings of theIEEE International Conference on Computer Vision
 Inexact proximal-point penalty methods for non-convex optimization With non-convex constraints,2019, arXiv preprint arXiv:1908
 Proximally constrained methods for Weakly convexoptimization With Weakly convex constraints,2019, arXiv preprint arXiv:1908
 Gradient methods for minimizing functionals,1963, Zhurnal Vychislitelâ€™noiMatematiki i Matematicheskoi Fiziki
 On the convergence of adam and beyond,2018, InInternational Conference on Learning Representations
 Deep learning in neural networks: An overview,2015, Neural networks
 On the generalization effects oflinear transformations in data augmentation,2020, arXiv preprint arXiv:2005
 Diverse neural network learns true target functions,2017, In ArtificialIntelligence and Statistics
 Stagewise training accelerates convergenceof testing error over sgd,2019, In Advances in Neural Information Processing Systems
 Wide residual networks,2016, arXiv preprintarXiv:1605
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
 mixup: Beyond empiricalrisk minimization,2018, In International Conference on Learning Representations
 Character-level convolutional networks for text clas-sification,2015, In Advances in neural information processing systems
 Learn-ing data augmentation strategies for object detection,2019, arXiv preprint arXiv:1906
 Gradient descent optimizes over-parameterized deep relu networks,2020, Machine Learning
