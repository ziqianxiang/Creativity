title,year,conference
 BERT: pre-training of deepbidirectional transformers for language understanding,2019, In Jill Burstein
 Depth-adaptive transformer,2020, In8th International Conference on Learning Representations
 Reducing transformer depth on demand with struc-tured dropout,2020, In 8th International Conference on Learning Representations
 schubert: Optimizing elements of BERT,2020, In Dan Jurafsky
 Reformer: The efficient transformer,2020, In8th International Conference on Learning Representations
 ALBERT: A lite BERT for self-supervised learning of language representations,2020, In8th International Conference on Learning Representations
 Roberta: A robustly optimized BERT pretrainingapproach,2019, CoRR
 The penn treebank: Annotating predicate argumentstructure,1994, In Human Language Technology
 Are sixteen heads really better than one? InHanna M,2019, Wallach
 Languagemodels are unsupervised multitask learners,2019, 2019
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, CoRR
 Megatron-lm: Training multi-billion parameter language models using model par-allelism,2019, CoRR
 HUggingface'stransformers: State-of-the-art natural language processing,2019, CoRR
 Lite transformer with long-shortrange attention,2020, In 8th International Conference on Learning Representations
 Deebert: Dynamic early exiting foraccelerating BERT inference,2020, In Dan Jurafsky
 Bert-of-theseus: CompressingBERT by progressive module replacing,2020, CoRR
 Bp-transformer: Modelling long-range context via binary partitioning,2019, CoRR
