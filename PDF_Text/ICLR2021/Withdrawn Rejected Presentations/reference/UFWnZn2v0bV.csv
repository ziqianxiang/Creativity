title,year,conference
 Adaptive regression and model selection in data mining problems,1999, PhD thesis
 Approximation and estimation for high-dimensional deep learningnetworks,2018, arXiv:1809
 Prediction error bounds for linear regression withthe TREX,2019, Test
 A practical scheme and fast algorithm to tune thelasso with optimality guarantees,2016, J
 On Lasso refitting strategies,2019, Bernoulli
 Deep learning forclassical Japanese literature,2018, arXiv:1812
 Sparse-input neural networks for high-dimensional nonparametric regressionand classification,2017, arXiv:1711
 On the piecewise analysis of networks of linear threshold neurons,1998, Neural Networks
 Digital selection and analogueamplification coexist in a cortex-inspired silicon circuit,2000, Nature
 Statistical learning with sparsity: The lasso andgeneralizations,2015, CRC press
 On the rate of convergence of fully connected very deep neural networkregression estimates,2019, arXiv:1908
 Gradient-based learning applied to documentrecognition,1998, Proc
 Deep learning,2015, Nature
 Estimating the lassoâ€™s effective noise,2020, arXiv:2004
 Sparse convolutional neural networks,2015, InIEEEInt Conf
 Group sparse regularization for deepneural networks,2017, Neurocomputing
 Deep learning in neural networks: An overview,2015, Neural Networks
 Balancing statistical and computational precision and applicationsto penalized linear regression with group sparsity,2016, arXiv:1609
 Statistical guarantees for regularized networks,2020, arXiv:2006
 Benefits of depth in neural networks,2016, In Annual Conference on Learning Theory
 Regression shrinkage and selection via the lasso,1996, J
 Neural Inf,2016, Process Syst
 Fashion-MNIST: a novel image dataset for benchmarkingmachine learning algorithms,2017, arXiv:1708
 Error bounds for approximations with deep ReLU networks,2017, Neural Networks
 Such tuning parameters are integralto regularization in deep learning and in machine learning more generally,2016, In sparse linear regression
