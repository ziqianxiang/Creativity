title,year,conference
 Learning representations by maximizingmutual information across views,2019, In NeurIPS
 Remixmatch: Semi-supervised learning with distribution alignment and augmenta-tion anchoring,2019, In ICLR
 Mixmatch: A holistic approach to semi-supervised learning,2019, In NeurIPS
 Unlabeleddata improves adversarial robustness,2019, In NeurIPS
 Adversarialrobustness: From self-supervised pre-training to fine-tuning,2020, In CVPR
 A simple framework forcontrastive learning of visual representations,2020, In ICML
 Big self-supervised models are strong semi-supervised learners,2020, arXiv preprint arXiv:2006
 Improved baselines with momentumcontrastive learning,2020, arXiv preprint arXiv:2003
 Improving the adversarial robustness of transferlearning via noisy feature distillation,2020, arXiv preprint arXiv:2002
 Randaugment: Practical automateddata augmentation with a reduced search space,2020, In CVPR Workshops
 ImageNet: A Large-Scale HierarchicalImage Database,2009, In CVPR
 On data augmentation and adversarialrisk: An empirical analysis,2020, arXiv preprint arXiv:2007
 Self-ensembling for visual domain adapta-tion,2018, In ICLR
 Dropblock: A regularization method for convolu-tional networks,2018, In NeurIPS
 Noise-contrastive estimation: A new estimation principlefor unnormalized statistical models,2010, In Proceedings of the Thirteenth International ConferenceOnArtificial Intelligence and Statistics
 Rethinking imagenet pre-training,2019, In Proceedings ofthe IEEE international conference on computer vision
 Momentum contrast forunsupervised visual representation learning,2020, In CVPR
 Benchmarking neural network robustness to common cor-ruptions and perturbations,2019, ICLR
 Using pre-training can improve model robustnessand uncertainty,2019, ICML
 Using self-supervised learningcan improve model robustness and uncertainty,2019, In NeurIPS
 The many faces of robustness: A criticalanalysis of out-of-distribution generalization,2020, arXiv preprint arXiv:2006
 Adversarial contrastive learning: Harvesting more robust-ness from unsupervised pre-training,2020, In NeurIPS
 Verified uncertainty calibration,2019, In NeurIPS
 Temporal ensembling for semi-supervised learning,2017, In ICLR
 Explicit inductive bias for transfer learning withconvolutional networks,2018, In ICML
 Learning without forgetting,2017, TPAMI
 Multitask learning strengthens adversarial robustness,2020, ECCV
 Self-supervised learning of pretext-invariant representa-tions,2020, In CVPR
 Representation learning with contrastive predic-tive coding,2018, arXiv preprint arXiv:1807
 A target-agnostic attack on deep models: Exploiting security vulner-abilities of transfer learning,2020, ICLR
 Adversarially robust transfer learning,2020, ICLR
 Fixmatch: Simplifying semi-supervised learningwith consistency and confidence,2020, arXiv preprint arXiv:2001
 Test-timetraining with self-supervision for generalization under distribution shifts,2020, ICML
 Whatmakes for good views for contrastive learning,2020, arXiv preprint arXiv:2005
 Unsupervised feature learning via non-parametric instance discrimination,2018, In CVPR
 Self-training with noisy studentimproves imagenet classification,2020, In CVPR
 Adversarial robustness through local lipschitzness,2020, arXiv preprint arXiv:2003
 Towards adversarially robust object detection,2019, In ICCV
 Random erasing data augmen-tation,2020, In AAAI
