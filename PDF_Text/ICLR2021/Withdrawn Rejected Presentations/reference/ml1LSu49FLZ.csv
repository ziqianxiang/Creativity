title,year,conference
 A neural attention model for abstractivesentence summarization,2015, In EMNLP
 Bottom-up abstractive summarization,2018, InEMNLP
 Learning phrase representations using rnn encoder-decoder forstatistical machine translation,2014, arXiv preprint arXiv:1406
 Image captioning: Transformingobjects into words,2019, In NeurIPS
 Learning phrase representations using RNN encoder-decoder forstatistical machine translation,2014, In EMNLP
 Long short-term memory,1997, Neural Computation
 Attention is all you need,2017, In NeurIPS
 Languagemodels are unsupervised multitask learners,2019, 2019
 Adaptive attentionspan in transformers,2019, In Proceedings of the 57th Conference of the Association for ComputationalLinguistics
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 Reformer: The efficient transformer,2020, In 8thInternational Conference on Learning Representations
 Large memory layers with product keys,2019, In Advances in Neural Information ProcessingSystems 32: Annual Conference on Neural Information Processing Systems 2019
 BERT: pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Improving language under-standing by generative pre-training,2018, 2018
 Language models are few-shot learners,2020, 2020
 Deep contextualized word representations,2018, In Proceedings of the 2018Conference of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, In NeurIPS
 Negative binomial process count and mixture modeling,2015, IEEETransactions on Pattern Analysis and Machine Intelligence
 Scalable deeppoisson factor analysis for topic modeling,2015, In ICML
 Dirichlet belief networks for topic structurelearning,2018, In NeurIPS
 TopicRNN: A recurrent neural networkwith long-range semantic dependency,2017, In ICLR
 A neural knowledge languagemodel,2016, CoRR
 Topically driven neural language model,2017, InProceedings of the 55th Annual Meeting of the Association for Computational Linguistics
 Topic compositional neural language model,2018, In AISTATS
 Recurrent hierarchical topic-guided neurallanguage models,2019, arXiv preprint arXiv:1912
 WHAI: Weibull hybrid autoencodinginference for deep topic modeling,2018, In ICLR
 Beta-negative binomialprocess and Poisson factor analysis,2012, In Artificial Intelligence and Statistics
 Deep latent Dirichlet allocation withtopic-layer-adaptive stochastic gradient Riemannian MCMC,2017, In ICML
 Adam: A method for stochastic optimization,2015, In InternationalConference on Learning Representations
 Pointer sentinel mixturemodels,2017, In ICLR
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2018, arXiv preprintarXiv:1804
