title,year,conference
 Tree-structured decoding with doubly-recurrent neuralnetworks,2016, 2016
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 An actor-critic algorithm for sequence prediction,2016, arXiv preprintarXiv:1607
 An empirical evaluation of generic convolutional andrecurrent networks for sequence modeling,2018, arXiv preprint arXiv:1803
 Trellis networks for sequence modeling,2018, arXivpreprint arXiv:1810
 Recursive neural networks canlearn logical semantics,2014, arXiv preprint arXiv:1406
 A fast unified model for parsing and sentence understanding,2016, arXiv preprintarXiv:1603
 Quasi-recurrent neuralnetworks,2016, arXiv preprint arXiv:1611
 Learning phrase representations using rnn encoder-decoder forstatistical machine translation,2014, arXiv preprint arXiv:1406
 Empirical evaluation ofgated recurrent neural networks on sequence modeling,2014, arXiv preprint arXiv:1412
 Language to logical form with neural attention,2016, arXiv preprintarXiv:1601
 Recurrent neural networkgrammars,2016, arXiv preprint arXiv:1602
 Adaptive computation time for recurrent neural networks,2016, arXiv preprintarXiv:1603
 Neural turing machines,2014, arXiv preprintarXiv:1410
 Learning hierarchical informa-tion flow with recurrent neural modules,2017, In Advances in Neural Information Processing Systems
 Cooperative learning of disjoint syntaxand semantics,2019, arXiv preprint arXiv:1902
 Long short-term memory,1997, Neural computation
 Towards neural phrase-based machine translation,2017, arXiv preprint arXiv:1706
 Learning hierarchicalstructures on-the-fly with a recurrent-recursive model for sequences,2018, In Proceedings of The ThirdWorkshop on Representation Learning for NLP
 Compound probabilistic context-free grammars forgrammar induction,2019, arXiv preprint arXiv:1906
 A clockwork rnn,2014, arXivpreprint arXiv:1402
 Simple recurrent units for highlyparallelizable recurrence,2018, In Proceedings of the 2018 Conference on Empirical Methods in NaturalLanguage Processing
 Towards binary-valued gates for robust lstm training,2018, arXiv preprint arXiv:1806
 Latent predictor networks for code generation,2016, arXiv preprintarXiv:1603
 On the variance of the adaptive learning rate and beyond,2019, arXiv preprint arXiv:1908
 A recursive recurrent neural network for statistical ma-chine translation,2014, In Proceedings of the 52nd Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers)
 Stanford neural machine translation systems forspoken language domains,2015, 2015
 Abstract syntax networks for code generationand semantic parsing,2017, arXiv preprint arXiv:1704
 Sequence level trainingwith recurrent neural networks,2015, arXiv preprint arXiv:1511
 Relational recurrent neuralnetworks,2018, In Advances in Neural Information Processing Systems
 Neural language modeling byjointly learning syntax and lexicon,2017, arXiv preprint arXiv:1711
 Ordered neurons: Integratingtree structures into recurrent neural networks,2018, arXiv preprint arXiv:1810
 Bivariate beta lstm,2019, arXiv preprintarXiv:1905
 Improved semantic representationsfrom tree-structured long short-term memory networks,2015, arXiv preprint arXiv:1503
 Recurrently controlled recurrent networks,2018, In Advancesin Neural Information Processing Systems
 The importance of being recurrent for modelinghierarchical structure,2018, arXiv preprint arXiv:1803
 Learning longer-term dependen-cies in rnns with auxiliary losses,2018, arXiv preprint arXiv:1803
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Tensor2tensor for neural machine translation,2018, CoRR
 Tree transformer: Integrating tree structuresinto self-attention,2019, arXiv preprint arXiv:1909
 Tranx: A transition-based neural abstract syntax parser forsemantic parsing and code generation,2018, arXiv preprint arXiv:1810
 Learning tocompose words into sentences with reinforcement learning,2016, arXiv preprint arXiv:1611
