title,year,conference
 Deep rewiring: Trainingvery sparse deep networks,2017, arXiv preprint arXiv:1711
 Estimating or propagating gradientsthrough stochastic neurons for conditional computation,2013, arXiv preprint arXiv:1308
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Sparse networks from scratch: Faster training without losingperformance,2019, arXiv preprint arXiv:1907
 The state of sparsity in deep neural networks,2019, arXivpreprint arXiv:1902
 Dynamic network surgery for efficient dnns,2016, InAdvances in neural information processing systems
 Second order derivatives for network pruning: Optimal brainsurgeon,1993, In Advances in neural information processing systems
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Channel pruning for accelerating very deep neural net-works,2017, In Proceedings of the IEEE International Conference on Computer Vision
 Optimal brain damage,1990, In Advances in neuralinformation processing systems
 Snip: Single-shot network pruningbased on connection sensitivity,2018, arXiv preprint arXiv:1810
 Pruning filters forefficient convnets,2016, arXiv preprint arXiv:1608
 Rethinking the value ofnetwork pruning,2018, arXiv preprint arXiv:1810
 Learning sparse neural networks throughl_0 regularization,2017, arXiv preprint arXiv:1712
 Thinet: A filter level pruning method for deep neuralnetwork compression,2017, In Proceedings of the IEEE international conference on computer vision
 The concrete distribution: A continuousrelaxation of discrete random variables,2016, arXiv preprint arXiv:1611
 Scalable training of artificial neural networks with adaptive sparse connec-tivity inspired by network science,2018, Nature communications
 Variational dropout sparsifies deep neuralnetworks,2017, arXiv preprint arXiv:1701
 Parameter efficient training of deep convolutional neural networksby dynamic sparse reparameterization,2019, arXiv preprint arXiv:1902
 Comparing rewinding and fine-tuning in neuralnetwork pruning,2020, arXiv preprint arXiv:2003
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Picking winning tickets before training bypreserving gradient flow,2020, arXiv preprint arXiv:2002
 Structured pruning of large language models,2019, arXivpreprint arXiv:1910
 Good subnet-works provably exist: Pruning via greedy forward selection,2020, arXiv preprint arXiv:2003
 Mlprune: Multi-layer pruning for automated neural networkcompression,2018, 2018
