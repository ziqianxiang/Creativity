title,year,conference
 Learning to learn by gradient descent bygradient descent,2016, Advances in Neural Information Processing Systems
 Learning to learn via self-critique,2019, ArXiv
 Meta-Iearning withdifferentiable closed-form solvers,2019, International Conference on Learning Representations
 Online fast adaptation andknowledge accumulation: a new approach to continual learning,2020, ArXiv
 Torch-meta: A Meta-Learning library for PyTorch,2019, 2019
 Uncertainty-guidedcontinual learning with bayesian neural networks,2020, Proceedings of the International Conferenceon Learning Representations
 Towards a neural statistician,2017, ArXiv
 Model-agnostic meta-learning for fast adaptationof deep networks,2017, International Conference on Machine Learning
 Probabilistic model-agnostic meta-learning,2018, Advancesin Neural Information Processing Systems
 Meta-learning with warped gradient descent,2020, International Conference on LearningRepresentations
 Forward and reversegradient-based hyperparameter optimization,2017, Proceedings of the 34th International Conferenceon Machine Learning
 Recasting gradient-based meta-learning as hierarchical bayes,2018, ArXiv
 La-maml: Look-ahead meta learning for continuallearning,2020, Advances in Neural Information Processing Systems
	Meta-learning representations for continual learn-ing,1820, In Advances in Neural Information Processing Systems 32
 Reconciling meta-learningand continual learning with online mixtures of tasks,2019, Advances in Neural Information ProcessingSystems
 Adam: A method for stochastic optimization,2014, InternationalConference on Learning Representations
 Overcoming catastrophic forgettingin neural networks,2017, Proceedings of the national academy of sciences
 One shot learning ofsimple visual concepts,2011, Conference of the Cognitive Science Society
 Gradient-based meta-learning with learned layerwise metric andsubspace,2018, International Conference on Machine Learning
 Marginal replay Vs conditional replayfor continual learning,2019, In ICANN
 Meta-sgd: Learning to learn quickly for few shotlearning,2017, ArXiv
 Personalizing dialogue agentsvia meta-learning,2019, Annual Meeting of the Association for Computational Linguistics
 Catastrophic interference in connectionist networks: Thesequential learning problem,1989, Psychology of learning and motivation
 Continual learning fornatural language generation in task-oriented dialog systems,2020, The 2020 Conference on EmpiricalMethods in Natural Language Processing
 Meta-trained agents implement bayes-optimal agents,2020, 34th Conference onNeural Information Processing Systems
 A simple neural attentive meta-learner,2018, International Conference on Learning Representations
 Variational continual learn-ing,2018, Proceedings of the International Conference on Learning Representations
 Amortized bayesian meta-learning,2019, International Conference onLearning Representations
 Incremental feW-shot learning Withattention attractor netWorks,2019, Advances in Neural Information Processing Systems
 Learning to learn Without forgetting by maximizing transfer and minimizing interfer-ence,2019, International Conference on Learning Representations
 A neural netWork that embeds its oWn meta-levels,1993, IEEE International Conferenceon Neural Networks
 Overcoming catastrophicforgetting With hard attention to the task,2018, Proceedings of the 35th International Conference onMachine Learning
 Generalizing from a few examples:A survey on few-shot learning,2020, 2020a
 Bayesian meta sampling forfast uncertainty adaptation,2020, International Conference on Learning Representations
 Caltech-UCSDBirds 200,2010, 2010
 Lifelong learning with dynamicallyexpandable networks,2018, International Conference on Learning Representations
 Xtarnet: Learning to extract task-adaptive representation for incremental few-shot learning,2020, International Conference on MachineLearning
