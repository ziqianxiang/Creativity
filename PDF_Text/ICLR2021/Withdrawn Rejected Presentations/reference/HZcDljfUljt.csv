title,year,conference
 ACIQ: analytical clip-ping for integer quantization of neural networks,2021, CoRR
 Estimating or propagating gradientsthrough stochastic neurons for conditional computation,2013, arXiv preprint arXiv:1308
 Data-free learning of student networks,2019, In Proceedings of the IEEEInternational Conference on Computer Vision
 PACT: parameterized clipping activation for quantized neuralnetworks,2018, CoRR
 Training with quantization noise for extreme model compression,2020, ArXiv
 The lottery ticket hypothesis: Training pruned neural net-works,2018, CoRR
 Bayesian optimized 1-bit cnns,2019, In Proceedings of the IEEE International Conferenceon Computer Vision
 Deep compression: Compressing deep neural networkswith pruning,2015, trained quantization and huffman coding
 Deep residual learning for image recog-nition,2015, CoRR
 Filter pruning via geometric medianfor deep convolutional neural networks acceleration,2019, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Searching for mobilenetv3,2019,	CoRR
 Mobilenets: Efficient convolutionalneural networks for mobile vision applications,2017,	CoRR
 Quantization and training of neural networks forefficient integer-arithmetic-only inference,2018, In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition
 Learning to quantize deep networks by optimizing quantizationintervals with task loss,2019, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Lca: Loss change allocation for neuralnetwork training,2019, In Advances in Neural Information Processing Systems
 Lognet:Energy-efficient neural networks using logarithmic computation,2017, In 2017 IEEE InternationalConference on Acoustics
 SGDR: stochastic gradient descent with restarts,2016, CoRR
 Importance estimationfor neural network pruning,2019, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Data distillation:Towards omni-supervised learning,2018, In Proceedings of the IEEE conference on computer visionand pattern recognition
 Aquantization-friendly separable convolution for mobilenets,2018, CoRR
 Very deep convolutional networks for large-scale imagerecognition,2015, In International Conference on Learning Representations
 Mixed precision dnns: All you need is a goodparametrization,2019, arXiv preprint arXiv:1905
 HAQ: hardware-aware automated quan-tization,2018, CoRR
 Gate decorator: Global filterpruning method for accelerating deep convolutional neural networks,2019, In Advances in NeuralInformation Processing Systems
 Paying more attention to attention: Improving the perfor-mance of convolutional neural networks via attention transfer,2016, arXiv preprint arXiv:1612
