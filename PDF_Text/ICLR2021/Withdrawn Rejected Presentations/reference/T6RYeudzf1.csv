title,year,conference
 CoCon: A self-supervised ap-proach for controlled text generation,2020, arXiv preprint arXiv:2006
 Electra: Pre-trainingtext encoders as discriminators rather than generators,2020, In International Conference on LearningRepresentations
 Style transformer: Unpaired text styletransfer without disentangled latent representation,2019, In Proceedings of the 57th Annual Meeting ofthe Association for Computational Linguistics
 Plug and play language models: A simple approach to con-trolled text generation,2020, In International Conference on Learning Representations
 A probabilistic formulationof unsupervised text style transfer,2020, In International Conference on Learning Representations
 Shakespearizing modern languageusing copy-enriched sequence to sequence models,2017, In Proceedings of the Workshop on StylisticVariation
 Multiple-attribute text rewriting,2019, In International Conference on LearningRepresentations
 A call for clarity in reporting BLEU scores,2018, In Proceedings of the Third Conference onMachine Translation: Research Papers
 Improving neural machine translation mod-els with monolingual data,2016, In Proceedings of the 54th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers)
 Style transfer from non-parallel text by cross-alignment,2017, In I
 Controllable unsupervised text attribute transfer via editingentangled latent representation,2019, In H
 On variational learning of controllable repre-sentations for text without supervision,2020, In Proceedings of the 37th International Conference onMachine Learning
 Un-supervised text style transfer using language models as discriminators,2018, In S
 Consistent dialogue generation with self-supervised feature learning,2020, arXiv preprintarXiv:1903
 Language styletransfer from sentences with arbitrary unknown styles,2018, CoRR
