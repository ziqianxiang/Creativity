title,year,conference
 Class-based n-gram models of natural language,1992, Computational linguistics
 Train neural networks with noise to reduce overfitting,2019, Machine Learning Mastery
 Clause restructuring for statistical machinetranslation,2005, In Proceedings of the 43rd Annual Meeting of the Association for ComputationalLinguistics
 BERT: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Findings of thesecond shared task on multimodal machine translation and multilingual image description,2017, InProceedings of the Second Conference on Machine Translation
 Learning image embeddings using convolutional neural networks forimproved multi-modal semantics,2014, In Proceedings of the 2014 Conference on Empirical Methodsin Natural Language Processing (EMNLP)
 Unicoder-vl: A universal encoderfor vision and language by cross-modal pre-training,2019, arXiv preprint arXiv:1908
 Microsoft coco: Common objects in context,2014, In Europeanconference on computer vision
 Multi-task deep neural netWorksfor natural language understanding,2019, In Proceedings of the 57th Annual Meeting of the Associationfor Computational Linguistics
 Vilbert: Pretraining task-agnostic visiolinguis-tic representations for vision-and-language tasks,2019, In Advances in Neural Information ProcessingSystems
 Hindi visual genome: A dataset formulti-modal english to hindi machine translation,2019, Computacion y Sistemas
 Glove: Global vectors for Word rep-resentation,2014, In Proceedings of the 2014 Conference on Empirical Methods in Natural LanguageProcessing (EMNLP)
 Deep contextualized Word representations,2018, In Proceedings of the 2018Conference of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies
 Improving language under-standing by generative pre-training,2018, Technical report
 Neural machine translation of rare Words WithsubWord units,2016, In Proceedings of the 54th Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers)
 Recursive deep models for semantic compositionality over a sentimenttreebank,2013, In EMNLP
 VL-BERT: Pre-training of generic visual-linguistic representations,2019, arXiv preprint arXiv:1908
 Videobert: A jointmodel for video and language representation learning,2019, arXiv preprint arXiv:1904
 Attention is all you need,2017, In I
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2018, In 2018EMNLP Workshop BlackboxNLP
 Glyce: Glyph-vectors for chinese character representations,2019, arXiv preprintarXiv:1901
 Image-enhanced multi-level sentence representation net for natural language inference,2018, In 2018 IEEEInternational Conference on Data Mining (ICDM)
 Unifiedvision-language pre-training for image captioning and vqa,2019, arXiv preprint arXiv:1909
