title,year,conference
 Learning with pseudo-ensembles,2014, In ZoubinGhahramani
 Curriculum learning,2009, InAndrea Pohoreckyj Danyluk
 Active bias: Training moreaccurate neural networks by emphasizing high variance samples,2017, In I
 Active bias: Training moreaccurate neural networks by emphasizing high variance samples,2017, In Isabelle Guyon
 Us-ing error decay prediction to overcome practical issues of deep active learning for named entityrecognition,2020, Machine Learning
 Semi-supervised learning,2010, 2010
 Seqvat: Virtual adversarial training forsemi-supervised sequence labeling,2020, In Proceedings of the 58th Annual Meeting of the Associationfor Computational Linguistics
 Variational sequential labelersfor semi-supervised learning,2019, arXiv preprint arXiv:1906
 Semi-supervised se-quence modeling with cross-view training,2018, arXiv preprint arXiv:1809
 Snips voice platform: an embedded spoken language understanding system forprivate-by-design voice interfaces,2018, In Privacy in Machine Learning and Artificial Intelligenceworkshop
 BERT: pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Semi-supervised learning with deep generative models,2014, In Zoubin Ghahramani
 Understanding black-box predictions via influence functions,2017, arXivpreprint arXiv:1703
 Learning active learning from data,2017, InAdvances in Neural Information Processing Systems
 Understanding self-training for gradual domainadaptation,2020, arXiv preprint arXiv:2002
 Asgard: A portable architecture formultilingual dialogue systems,2013, 2013 IEEE International Conference on Acoustics
 Roberta: A robustly optimized BERT pretrainingapproach,2019, CoRR
 Self-paced co-training,2017, In Doina Precupand Yee Whye Teh (eds
 Language as a latent variable: Discrete generative models for sen-tence compression,2016, In Jian Su
 Virtual adversarial training: aregularization method for supervised and semi-supervised learning,2018, IEEE transactions on patternanalysis and machine intelligence
 Cross-lingual name tagging and linking for 282 languages,2017, In Proceedings of the 55th Annual Meetingofthe Associationfor CompUtational Linguistics (Volume 1: Long Papers)
 Enhancing deep active learning using selective self-trainingfor image classification,2019, Masterâ€™s thesis
 Semi-supervisedsequence tagging with bidirectional language models,2017, arXiv preprint arXiv:1705
 Languagemodels are unsupervised multitask learners,2019, 2019
 Semi-supervised learning with ladder networks,2015, In Corinna Cortes
 Strong baselines for neural semi-supervised learning underdomain shift,2018, In Proceedings of the 56th Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers)
 Training region-based object detectorswith online hard example mining,2016, In Proceedings of the IEEE conference on computer vision andpattern recognition
 Mean teachers are better role models: Weight-averaged con-sistency targets improve semi-supervised deep learning results,2017, In 5th International Conferenceon Learning Representations
 Representing text chunks,1999, In Ninth Conference of theEuropean Chapter of the Association for Computational Linguistics
 Structvae: Tree-structured la-tent variable models for semi-supervised semantic parsing,2018, In Iryna Gurevych and Yusuke Miyao(eds
 Understandingdeep learning requires rethinking generalization,2017, In 5th International Conference on LearningRepresentations
 Multi-space variational encoder-decoders for semi-supervisedlabeled sequence transduction,2017, arXiv preprint arXiv:1704
