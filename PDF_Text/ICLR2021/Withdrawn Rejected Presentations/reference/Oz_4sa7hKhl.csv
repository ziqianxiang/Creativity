title,year,conference
 Contributions to the studyof sms spam filtering: new collection and results,2011, In Matthew R
 Brown university ripped : Recursive intent propagation using pretrained embedding dis-tances,2019, 2019
 Fromarguments to key points: Towards automatic argument summarization,2020, In Proceedings of the 58thAnnual Meeting of the Association for Computational Linguistics (ACL)
 Adversarial transfer learning forchinese named entity recognition with self-attention mechanism,2018, In EMNLP
 Deep clustering for unsupervised learningof visual features,2018, In ECCV
 Recognizing textual en-tailment: Models and applications,2013, Synthesis Lectures on Human Language Technologies
 Corpus wide argument mining-aworking solution,2020, In AAAI
 Injecting numerical reasoning skills into languagemodels,2020, In ACL
 Unsupervised representation learning bypredicting image rotations,2018, ArXiv
 Donâ€™t stop pretraining: Adapt language models to domains and tasks,2020, ArXiv
 Adam: A method for stochastic optimization,2015,	CoRR
 Siamese neural networks for one-shot image recognition,2015, 2015
 Albert: A lite bert for self-supervised learning of language representations,2020, In InternationalConference on Learning Representations
 Multi-task deep neural networks fornatural language understanding,2019, ArXiv
 Least squares quantization in pcm,1982, IEEE transactions on information theory
 Pre-training text representations asmeta learning,2020, ArXiv
 Passage re-ranking with bert,2019, arXiv preprintarXiv:1901
 Analyzing bert with pre-train on squad 2 ,2019, 0
 Glove: Global vectors for word repre-sentation,2014, In EMNLP
 Sentence encoders on stilts: Supplementarytraining on intermediate labeled-data tasks,2018, arXiv preprint arXiv:1811
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, ArXiv
 Weakly supervised cyberbullying detection with participant-vocabulary consistency,2018, Social Network Analysis and Mining
 Optimization as a model for few-shot learning,2017, In ICLR
 Unsupervised document classification us-ing sequential information maximization,2002, In Proceedings of the 25th Annual InternationalACM SIGIR Conference on Research and Development in Information Retrieval
 Prototypical networks for few-shot learning,2017, InAdvances in neural information processing systems
 Unsupervisedtransfer learning via bert neuron selection,2019, ArXiv
 Matching networks for oneshot learning,2016, In Advances in neural information processing systems
 Can you tell me how toget past sesame street? sentence-level pretraining beyond language modeling,2019, In Proceedings ofthe 57th Annual Meeting of the Association for Computational Linguistics
 Adaptive self-training for few-shot neural sequence labeling,2020, ArXiv
 Semi-supervised clustering for short text viadeep representation learning,2016, In Proceedings of The 20th SIGNLL Conference on ComputationalNatural Language Learning
 Domainadaptive training bert for response selection,2019, ArXiv
 BERT Post-Training for Review Reading Compre-hension and Aspect-based Sentiment Analysis,2019, In Proceedings of the 2019 Conference of theNorth American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Improving bert fine-tuning via self-ensemble andself-distillation,2020, ArXiv
 Xlnet: Gener-alized autoregressive pretraining for language understanding,2019, In NeurIPS
 Character-level convolutional networks for text clas-sification,2015, In Advances in neural information processing systems
