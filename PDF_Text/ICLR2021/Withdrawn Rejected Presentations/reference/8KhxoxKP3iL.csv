title,year,conference
 A Convergence Theory for Deep Learning viaOver-Parameterization,2019, In Proceedings of the 36th International Conference on Machine Learning
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, In Proceedings of the 35th International Conference onMachine Learning
 Neural machine translation by jointlylearning to align and translate,2014, CoRR
 Approximation and estimation for high-dimensional deeplearning networks,2018, arXiv preprint arXiv:1809
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 Gradient descent finds globalminima of deep neural networks,2019, In Proceedings of the 36th International Conference on MachineLearning
 Learning one-hidden-layer neural networks with landscapedesign,2017, CoRR
 Flat minima,1997, Neural Computation
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Effective approaches to attention-basedneural machine translation,2015, In Proceedings of the 2015 Conference on Empirical Methods inNatural Language Processing
 Learning word vectors for sentiment analysis,2011, In Proceedings of the 49th Annual Meetingof the Associationfor Computational Linguistics: Human Language Technologies
 From softmax to sparsemax: A sparse model of attentionand multi-label classification,2016, In International Conference on Machine Learning
 A mean field view of the landscape of two-layer neural networks,0027, Proceedings of the National Academy of Sciences
 Recurrent models of visualattention,2014, In Z
 The loss surface of deep and wide neural networks,2017, In Proceedingsof the 34th International Conference on Machine Learning (ICML 2017)
 The loss surface of deep and wide neural networks,2017, arXiv preprintarXiv:1704
 Piecewise convexity of artificial neural networks,2017, Neural Networks
 Theoretical insights into the optimizationlandscape of over-parameterized shallow neural networks,2017, CoRR
 No bad local minima: Data independent training error guaranteesfor multilayer neural networks,2016, arXiv preprint arXiv:1605
 Attention is all you need,2017, In Advances in Neural InformationProcessing Systems
 Introduction to the non-asymptotic analysis of random matrices,2010, arXiv preprintarXiv:1011
 Self-attention generativeadversarial networks,2018, CoRR
 The landscape of deep learning algorithms,2017, CoRR
 Then following the same empirical risk convergenceargument as Lemma 1,2021,2
