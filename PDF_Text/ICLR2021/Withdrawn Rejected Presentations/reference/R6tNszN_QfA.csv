title,year,conference
 Practical recommendations for gradient-based training of deep architectures,2012, InNeural networks: Tricks of the trade
 Demystifying mmdgans,2018, arXiv preprint arXiv:1801
 The relaxation method of finding the common point of convex sets and its applica-tion to the solution of problems in convex programming,1967, USSR computational mathematics andmathematical physics
 On loss functions for deep neural netWorks inclassification,2017, arXiv preprint arXiv:1702
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Mmd gan:ToWards deeper understanding of moment matching netWork,2017, In Advances in Neural InformationProcessing Systems
 Approximation and convergence proper-ties of generative adversarial learning,2017, In Advances in Neural Information Processing Systems
 Wasserstein generative adversarial networks,2017, In Proceedingsof the 34 th International Conference on Machine Learning
 The numerics of gans,2017, In Advances inNeural Information Processing Systems
 f-gan: Training generative neural samplersusing variational divergence minimization,2016, In Advances in neural information processing systems
 Density ratio estimation in machinelearning,2012, Cambridge University Press
 Deep learning using linear support vector machines,2013, arXiv preprint arXiv:1306
 Lipschitz generative adversarial nets,2019, arXiv preprint arXiv:1902
