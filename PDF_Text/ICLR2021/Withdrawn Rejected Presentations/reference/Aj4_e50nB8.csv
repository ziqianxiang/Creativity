title,year,conference
 Transformer to cnn: Label-scarce distillationfor efficient text classification,2019, arXiv preprint arXiv:1909
 What does bert lookat? an analysis of bert’s attention,2019, arXiv preprint arXiv:1906
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 A structural probe for finding syntax in word representa-tions,2019, In Proceedings of the 2019 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Dynabert: Dynamic bertwith adaptive width and depth,2020, Advances in Neural Information Processing Systems
 Sequence-level knowledge distillation,2016, arXiv preprintarXiv:1606
 Albert: A lite bert for self-supervised learning of language representations,2019, arXiv preprintarXiv:1909
 Fastbert: a self-distillingbert with adaptive inference time,2020, arXiv preprint arXiv:2004
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Glove: Global vectors for wordrepresentation,2014, In Proceedings of the 2014 conference on empirical methods in natural languageprocessing (EMNLP)
 Deep contextualized word representations,2018, arXiv preprint arXiv:1802
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Visualizing and measuring the geometry of bert,2019, In Advances in Neural InformationProcessing Systems
 Patient knowledge distillation for bert modelcompression,2019, arXiv preprint arXiv:1908
 Distilling task-specific knowledge from bert into simple neural networks,2019, arXiv preprint arXiv:1903
 Bert rediscovers the classical nlp pipeline,2019, arXivpreprint arXiv:1905
 What do you learn fromcontext? probing for sentence structure in contextualized word representations,2019, arXiv preprintarXiv:1905
 Similarity-preserving knowledge distillation,2019, In Proceedings of theIEEE International Conference on Computer Vision
 Well-read students learn better:On the importance of pre-training compact models,2019, arXiv preprint arXiv:1908
 Visualizing attention in transformer-based language representation models,2019, arXiv preprintarXiv:1904
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2018, arXiv preprintarXiv:1804
 Structure-levelknowledge distillation for multilingual sequence labeling,2020, arXiv preprint arXiv:2004
 Huggingface’s transformers: State-of-the-art natural language processing,2019, ArXiv
 Exploring the use of word relation features for sentiment classifica-tion,2010, In Coling 2010: Posters
 Paying more attention to attention: Improving the perfor-mance of convolutional neural networks via attention transfer,2016, arXiv preprint arXiv:1612
