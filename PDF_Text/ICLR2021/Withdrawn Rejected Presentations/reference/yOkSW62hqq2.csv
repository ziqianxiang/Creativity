title,year,conference
 Variationalinformation distillation for knowledge transfer,2019, In CVPR
 Large scale distributed neural network training through online distillation,2018, In ICLR
 Label refinery:Improving imagenet classification through label progression,2015, arXiv preprint arXiv:1805
 Grad-cam++:Improved visual explanations for deep convolutional networks,2018, 2018
 Data-free learning of student networks,2019, In ICCV
 Feature-map-level online adversarialknowledge distillation,2020, In ICML
 Modality distillation with multiple streamnetworks for action recognition,2018, In ECCV
 Rich feature hierarchies for accurateobject detection and semantic segmentation,2014, In CVPR
 Adversarially robust distillation,2020, InAAAI
 Deep residual learning for imagerecognition,2016, In CVPR
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Lifelong learning viaprogressive distillation and retrospection,2018, In ECCV
 Squeeze-and-excitation networks,2018, In CVPR
 Densely connectedconvolutional networks,2017, In CVPR
 Paraphrasing complex network: Network compres-sion via factor transfer,2018, In NeurIPS
 Sequence-level knowledge distillation,2016, In EMNLP
 Learning multiple layers of features from tiny images,2009, TechReport
 Imagenet classification with deep convolu-tional neural networks,2012, In NIPS
 Knowledge distillation by on-the-fly native ensemble,2018, InNeurIPS
 Self-supervised knowledge distillation usingsingular value decomposition,2018, In NeurIPS
 Pruning filters forefficient convnets,2017, In ICLR
 Learning without forgetting,2016, In ECCV
 Search to distill: Pearls are everywhere but not the eyes,2020, In CVPR
 Fully convolutional networks for semanticsegmentation,2015, In CVPR
 Data-free knowledge distillation for deepneural networks,2015, arXiv preprint arXiv:1710
 Ensemble distribution distillation,2020, In ICLR
 Relational knowledge distillation,2019, In CVPR
 Learning deep representations with probabilistic knowledgetransfer,2018, In ECCV
 Correlation congruence for knowledge distillation,2019, In ICCV
 Distillation-based training for multi-exit architectures,2019, InICCV
 Xnor-net: Imagenetclassification using binary convolutional neural networks,2016, In ECCV
 Fitnets: Hints for thin deep nets,2015, In ICLR
 Deeply-supervised knowledge synergy,2019, In CVPR
 Contrastive representation distillation,2020, In ICLR
 Similarity-preserving knowledge distillation,2019, In ICCV
 Cbam: Convolutional blockattention module,2018, In ECCV
 Peer collaborative learning for online knowledge distillation,2020, arXivpreprint arXiv:2006
 Condconv: Conditionally parameter-ized convolutions for efficient inference,2019, In NeurIPS
 Knowledge transfer via dense cross-layer mutual-distillation,2020, In ECCV
 Matching guided distillation,2020, In ECCV
 Wide residual networks,2016, In BMVC
 Paying more attention to attention: Improving theperformance of convolutional neural networks via attention transfer,2017, In ICLR
 Deep mutual learning,2018, In CVPR
 Exfuse: Enhancing featurefusion for semantic segmentation,2018, In ECCV
 Dorefa-net: Train-ing low bitwidth convolutional neural networks with low bitwidth gradients,2016, arXiv preprintarXiv:1606
