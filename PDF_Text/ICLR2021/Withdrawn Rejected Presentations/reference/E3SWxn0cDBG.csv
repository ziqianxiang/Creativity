title,year,conference
 A new regret anal-ysis for adam-type algorithms,2020, ICML
 A convergence theory for deep learning via over-parameterization,2019, ICML
 Heavy ball method on convex quadratic problem,2018, Lecture note
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, ICML
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, NeurIPS
 Beyond linearization: On quadratic and higher-order approximation ofwide neural networks,2020, ICLR
 On the inductive bias of neural tangent kernels,2019, NeurIPS
 Globally optimal gradient descent for a convnet with gaussianinputs,2017, ICML
 Accelerated linear convergence of stochasticmomentum methods in wasserstein distances,2019, ICML
 Understanding accelerated stochastic gradient descent via thegrowth condition,2020, arXiv:2006
 Sgd learns the conjugate kernel class of the network,2017, NeurIPS
 Memorizing gaussians with no over-parameterizaion via gradient decent on neuralnetworks,2020, arXiv:1909
 Generalized momentum-based methods: A hamiltonianperspective,2019, arXiv:1906
 Gradient descent findsglobal minima of deep neural networks,2019, ICML
 Gradient descent provably optimizesover-parameterized neural networks,2019, ICLR
 Optimization theory for relu neural networkstrained with normalization layers,2020, ICML
 Over parameterized two-level neural networks can learnnear optimal feature representations,2019, arXiv:1910
 Stochastic heavy ball,2016, arXiv:1609
 Learning two-layer neural networks withsymmetric inputs,2019, ICLR
 Global convergence of theheavy-ball method for convex optimization,2015, ECC
 Linearized two-layers neural networks in high dimension,2019, arXiv:1904
 Understanding the role of momentumin stochastic gradient methods,2019, NeurIPS
 Why momentum really works,2017, Distill
 Finite depth and width corrections to the neural tangent kernel,2020, ICLR
 Deep residual learning for image recog-nition,2016, Conference on Computer Vision and Pattern Recognition (CVPR)
 The surprising simplicity of the early-time learning dynamics of neural networks,2020, NeurIPS
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, NeurIPS
 Polylogarithmic width suffices for gradient descent to achieve arbi-trarily small test error with shallow relu networks,2020, ICLR
 On the insufficiency ofexisting momentum schemes for stochastic optimization,2018, ICLR
 Adam: A method for stochastic optimization,2015, ICLR
 Global convergence of second-orderdynamics in two-layer neural networks,2020, arXiv:2006
 Imagenet classification with deep convo-lutional neural networks,2012, NIPS
 Wide neural networks of any depth evolve as linear models under gradientdescent,2019, NeurIPS
 Generalized leverage scoresampling for neural networks,2020, arXiv:2009
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, NeurIPS
 Learning over-parametrized two-layer relu neuralnetworks beyond ntk,2020, COLT
 Accelerating sgd with momentum for over-parameterized learn-ing,2020, ICLR
 On the linearity of large non-linear models: when andwhy the tangent kernel is constant,2020, arXiv:2010
 Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning,2020, arXiv:2003
 An improved analysis of stochastic gradient descent withmomentum,2020, arXiv:2007
 Accelerated gossip via stochastic heavy ball method,2018, Allerton
 Adaptive gradient methods with dynamicbound of learning rate,2019, ICLR
 Towards moderate overparameterization: global conver-gence guarantees for training shallow neural networks,2019, arXiv:1902
 Effect of activation functions on the train-ing of overparametrized neural nets,2020, ICLR
 Gradient methods for minimizing functionals,1963, Zhurnal Vychislitelâ€™noi Matematiki iMatematicheskoi Fiziki
 Introduction to optimization,1987, Optimization Software
 Lyapunov analysis and the heavy ball method,2018, Lecture note
 On the convergence of adam and beyond,2018, ICLR
 The effects of mild over-parameterization on theoptimization landscape of shallow relu neural networks,2020, arXiv:2006
 On the convergence of the stochasticheavy ball method,2020, arXiv:2006
 Learning relus via gradient descent,2017, NeurIPS
 On learning over-parameterized neural networks: A functional approxi-mation perspective,2019, NeurIPS
 Non-ergodicconvergence analysis of heavy-ball algorithms,2019, AAAI
 On the importance of initializa-tion and momentum in deep learning,2013, ICML
 An analytical formula of population gradient for two-layered relu network and itsapplications in convergence and critical point analysis,2017, ICML
 Attention is all you need,2017, NIPS
 Escaping saddle points faster with stochasticmomentum,2020, ICLR
 Regularization matters: Generalization andoptimization of neural nets v,2019,s
 Themarginal value of adaptive gradient methods in machine learning,2017, NIPS
 Learning distributions generated byone-layer relu networks,2019, NeurIPS
 Global convergence of adaptive gradient methods foran over-parameterized neural network,2019, arXiv:1902
 Unified convergence analysis of stochastic momentummethods for convex and non-convex optimization,2018, IJCAI
 Learning a single neuron with gradient methods,2020, COLT
 Fast convergence of natural gradient descentfor over-parameterized neural networks,2019, NeurIPS
 Recovery guaranteesfor one-hidden-layer neural networks,2017, ICML
 An improved analysis of training over-parameterized deep neuralnetworks,2019, NeurIPS
 Stochastic gradient descent optimizesoverparameterized deep relu networks,2019, Machine Learning
