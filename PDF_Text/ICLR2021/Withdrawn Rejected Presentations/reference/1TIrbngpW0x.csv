title,year,conference
 Modular meta-learning,2018, arXiv preprintarXiv:1806
 Repulsive attention: Rethinking multi-head attention as bayesian inference,2020, arXivpreprint arXiv:2009
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 Learning deep architectures for AI,2009, Now Publishers Inc
 Suppression of acoustic noise in speech using spectral subtraction,1979, IEEE Transactionson acoustics
 Phase-aware single-stage speechdenoising and dereverberation with u-net,2020, arXiv preprint arXiv:2006
 What does bert look at?an analysis of bertâ€™s attention,2019, arXiv preprint arXiv:1906
 Mixed multi-headself-attention for neural machine translation,2019, In Proceedings of the 3rd Workshop on NeuralGeneration and Translation
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Speech Enhancement Using a Minimum Mean-Square ErrorShort-Time Spectral Amplitude Estimator,1984, IEEE Transactions on Audio
 Boosting ob-jective scores of speech enhancement model through metricgan post-processing,2020, arXiv preprintarXiv:2006
 Domain adaptation for large-scale sentimentclassification: A deep learning approach,2011, In ICML
 Recurrent independent mechanisms,2019, arXiv preprint arXiv:1909
 Speech Enhancement-ASignal Subspace Perspective,2015, Academic Press
 Adam: A method for stochastic optimization,2014, CoRR
 Exploring the best loss function fordnn-based low-latency speech enhancement with temporal convolutional networks,2020, arXiv preprintarXiv:2005
 Learning multiple layers of features from tiny images,2009, 2009
 The mnist database of handwritten digits,1998, arXiv preprintarXiv:2001
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Disentangling factors of variation in deep representation using adversarial training,2016, InAdvances in neural information processing systems
 Learning to combine top-down and bottom-up signals inrecurrent neural networks with attention over modules,2020, arXiv preprint arXiv:2006
 Learningindependent causal mechanisms,2018, In Proceedings of the 35th International Conference on MachineLearning (ICML)
 Deep contextualized word representations,2018, arXiv preprint arXiv:1802
 Languagemodels are unsupervised multitask learners,2019, 2019
 Contaminated speech training methods for robust DNN-HMM distant speech recognition,2015, In Proc
 Disentanglingfactors of variation for facial expression recognition,2012, In European Conference on Computer Vision
 Neural machine translation of rare words withsubword units,2016, In ACL
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Bertology meets biology: Interpreting attention in protein langUage models,2020, arXiv preprintarXiv:2006
 Aligning books and movies: Towards story-like visUal explanations by watchingmovies and reading books,2015, In arXiv preprint arXiv:1506
