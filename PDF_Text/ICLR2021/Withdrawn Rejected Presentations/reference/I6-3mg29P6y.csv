title,year,conference
 Energy landscapes for machine learning,2017, Physical ChemistryChemical Physics
 Deep Frank-Wolfe for neuralnetwork optimization,2018, arXiv preprint arXiv:1811
 Pattern recognition and machine learning,2006, springer
 Entropy-SGD: Biasinggradient descent into wide valleys,2016, arXiv preprint arXiv:1611
 Closing the generalization gap of adaptive gradient methodsin training deep neural networks,2018, arXiv preprint arXiv:1806
 An investigation into neural netoptimization via Hessian eigenvalue density,2019, arXiv preprint arXiv:1901
 MLRGdeep curvature,2019, arXiv preprint arXiv:1912
 Iterate averaging helps: An alternativeperspective in deep learning,2020, arXiv preprint arXiv:2003
 Asymmetric valleys: Beyond sharp and flat localminima,2019, arXiv preprint arXiv:1902
 Flat minima,1997, Neural Computation
 Batch normalization: Accelerating deep network trainingby reducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Three factors influencing minima in sgd,2017, arXiv preprintarXiv:1711
 Three factors influencing minima in SGD,2017, arXiv preprintarXiv:1711
 On the relation betWeen the sharpest directions of DNN loss and the SGDstep length,2018, 2018
 The break-even point on the optimization tra jec-tories of deep neural netWorks,2020, In International Conference on Learning Representations
 On the rationale of maximum-entropy methods,1982, Proceedings of the IEEE
 Improving generalization performance by sWitchingfrom Adam to SGD,2017, arXiv preprint arXiv:1712
 On large-batch training for deep learning: Generalization gap and sharpminima,2016, arXiv preprint arXiv:1609
 A simple Weight decay can improve generalization,1992, InAdvances in neural information processing systems
 Decoupled Weight decay regularization,2018, 2018
 Depth creates no bad local minima,2017, arXiv preprintarXiv:1702
 A simple baseline for Bayesian uncertainty in deep learning,2019, In Advances inNeural Information Processing Systems
 Loss surface of xorartificial neural networks,2018, Physical Review E
 Piecewise strong convexity of neural networks,2019, In Advances in NeuralInformation Processing Systems
 Exploringgeneralization in deep learning,2017, In Advances in Neural Information Processing Systems
 The full spectrum of deepnet hessians at scale: Dynamics with sgd trainingand sample size,2018, arXiv preprint arXiv:1811
 Fast exact multiplication by the Hessian,1994, Neural computation
 A scale invariant flatness measure for deep network minima,2019, arXiv preprintarXiv:1902
 Improved bounds on sample size for implicitmatrix trace estimators,2015, Foundations of Computational Mathematics
 Themarginal value of adaptive gradient methods in machine learning,2017, In Advances in NeuralInformation Processing Systems
 Towards understanding generalization of deep learning:Perspective of loss landscapes,2017, arXiv preprint arXiv:1706
 How sgd selects the global minima in over-parameterizedlearning: A dynamical stability perspective,2018, In Advances in Neural Information ProcessingSystems
 Hessian-basedanalysis of large batch training and robustness to adversaries,2018, In Advances in NeuralInformation Processing Systems
 Under-standing deep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
 Theory of deep learning iib: Optimization properties of sgd,2018, arXivpreprint arXiv:1801
 This is done so thatthe transforms can function even if the prediction set is only 1 sample5 ,1000, Previous worksinvestigating neural network Hessians (Papyan
