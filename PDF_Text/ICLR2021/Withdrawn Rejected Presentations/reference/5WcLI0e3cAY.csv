title,year,conference
 Neural machine translation by jointlylearning to align and translate,2015, In 3rd International Conference on Learning Representations
 BERT: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies (NAACL)
 Unified language model pre-training for natural language understandingand generation,2019, In Advances in Neural Information Processing Systems 32: Annual Conferenceon Neural Information Processing Systems
 Measuring nominal scale agreement among many raters,1971, Psychological bulletin
 Reality in advertising,1961, America
 SentiLR: Linguistic knowledgeenhanced language representation for sentiment analysis,2019, arXiv preprint arXiv:1911
 Informingunsupervised pretraining with external linguistic knowledge,2019, arXiv preprint arXiv:1909
 SenseBERT: Driving some sense into BERT,2020, In Proceedings of the58th Annual Meeting of the Association for Computational Linguistics (ACL)
 Automatic evaluation of summaries using n-gram co-occurrencestatistics,2003, In Proceedings of the 2003 Human Language Technology Conference of the NorthAmerican Chapter of the Association for Computational Linguistics (NAACL)
 RoBERTa: A robustly optimized BERT pre-training approach,2019, arXiv preprint arXiv:1907
 Multimodal attribute extraction,2017, arXivpreprint arXiv:1711
 AliCoCo: Alibaba e-commerce cognitive concept net,2020, In Proceedingsof the 2020 ACM SIGMOD International Conference on Management of Data (SIGMOD)
 Improving language under-standing by generative pre-training,2018, 2018
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Get to the point: Summarization withpointer-generator networks,2017, In Proceedings of the 55th Annual Meeting of the Association forComputational Linguistics (ACL) 
 MASS: masked sequence to sequencepre-training for language generation,2019, In Proceedings of the 36th International Conference onMachine Learning (ICML)
 ERNIE: Enhanced representation through knowledge integration,2019, arXivpreprint arXiv:1904
 SKEP:Sentiment knowledge enhanced pre-training for sentiment analysis,2020, In Proceedings of the 58thAnnual Meeting of the Association for Computational Linguistics (ACL)
 K-Adapter: Infusing knowledge into pre-trained models with adapters,2020, arXivpreprint arXiv:2002
 KEPLER: Aunified model for knowledge embedding and pre-trained language representation,2019, arXiv preprintarXiv:1911
 Sequential matching network: A newarchitecture for multi-turn response selection in retrieval-based chatbots,2017, In Proceedings of the55th Annual Meeting of the Association for Computational Linguistics (ACL)
 Pretrained encyclopedia:Weakly supervised knowledge-pretrained language model,2020, In 8th International Conference onLearning Representations (ICLR)
 CLUE: A chinese language understanding evaluation benchmark,2020, arXivpreprint arXiv:2004
 ProphetNet: Predicting future n-gram for sequence-to-sequence pre-training,2020, arXivpreprint arXiv:2001
 PEGASUS: Pre-training with ex-tracted gap-sentences for abstractive summarization,2020, In Proceedings of the 37th InternationalConference on Machine Learning (ICML)
 ERNIE: Enhancedlanguage representation with informative entities,2019, In Proceedings of the 57th Annual Meeting ofthe Association for Computational Linguistics (ACL)
 Multi-turn response selection for chatbots with deep attention matching network,2018, InProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume1: Long Papers)
 Multimodaljoint attribute prediction and value extraction for e-commerce product,2020, In Proceedings of the 2020Conference on Empirical Methods in Natural Language Processing (EMNLP)
