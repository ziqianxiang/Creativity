title,year,conference
 Imputer: Se-quence modelling via imputation and dynamic programming,2020, ICML
 Monotonic chunkwise attention,2018, ICLR
 A comparison of end-to-end models for long-form speech recognition,2019, ASRU
 Learning phrase representations using rnn encoder-decoder for statisti-cal machine translation,2014, EMNLP
 Attention-basedmodels for speech recognition,2015, Neurips
 Cif: Continuous integrate-and-fire for end-to-end speech recognition,2020, ICASSP
 Speech-transformer: A no-recurrence sequence-to-sequence modelfor speech recognition,2018, ICASSP
 Self-attention aligner: A latency-control end-to-end model for asrusing self-attention network and chunk-hopping,2019, ICASSP
 Parallel sched-uled samplings,2019, ArXiv
 Efficient wait-k models for simultaneous machinetranslation,2020, ArXiv
 Sequence transduction with recurrent neural networks,2012, ArXiv
 Generating sequences with recurrent neural networks,2013, ArXiv
 onnectionist tem-poral Clas-sification: labelling unsegmented sequence data with recurrent neural networks,2006, ICML
 Sequence-to-sequence speech recognition withtime-depth separable convolutions,2019, ArXiv
 Enhancing monotonic multihead attention forstreaming asr,2020, ArXiv
 A neuraltransducer,2015, ArXiv
 Attentionbased on-device streaming speech recognition with large speech corpus,2019, ASRU
 Joint ctc-attention based end-to-end speech recognitionusing multi-task learning,2016, ArXiv
 Subword regularization: Improving neural network translation models with multiple subwordcandidates,2018, ACL
 Gated recurrentcontext: Softmax-free attention for online encoder-decoder speech recognition,2020, ArXiv
 Jasper: An end-to-end convolutional neural acoustic model,2019, Interspeech
 Robutrans: A robust transformer-based text-to-speech model,2020, AAAI
 Letter-based speech recognition with gatedconvnets,2017, ArXiv
 Monotonic multihead attention,2019, ArXiv
 Streaming automatic speech recognition with the trans-former model,2020, ArXiv
 Librispeech: an asr corpus basedon public domain audio books,2015, ICASSP
 Image transformer,2018, ICML
 Online and linear-timeattention by enforcing monotonic alignments,2017, ICLR
 Streaming transformer asr with blockwise syn-chronous inference,2020, ArXiv
 Attention is all you need,2017, Neurips
 Speech commands: A dataset for limited-vocabulary speech recognition,2018,	ArXiv
 Transformer-transducer: End-to-end speech recogni-tion with self-attention,2019, ArXiv
 A comprehensive analysis on attentionmodels,2018, NIPS: Workshop IRASL
