title,year,conference
 Man isto compUter programmer as woman is to homemaker? debiasing word embeddings,2016, In D
 Identifying and redUcing gender bias in word-level langUagemodels,2019, In Proceedings of the 2019 Conference of the North American Chapter of the Associationfor Computational Linguistics: Student Research Workshop
 How do decisions emergeacross layers in neural models? interpretation with differentiable masking,2020, arXiv preprintarXiv:2004
 The secret sharer:Evaluating and testing unintended memorization in neural networks,2019, In 28th USENIX SecuritySymposium
 BERT: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Differentiable reasoning over a virtual knowledge base,2019, In InternationalConference on Learning Representations
 Does learning require memorization? a short tale about a long tail,2020, In Proceedingsof the 52nd Annual ACM SIGACT Symposium on Theory of Computing
 What neural networks memorize and why: Discovering thelong tail via influence estimation,2020, arXiv preprint arXiv:2008
 En-tities as experts: Sparse memory access with entity supervision,2020, arXiv preprint arXiv:2004
 Realm: Retrieval-augmented language model pre-training,2020, arXiv preprint arXiv:2002
 Bert-knn: Adding a knn search component to pretrained lan-guage models for better qa,2020, arXiv preprint arXiv:2005
 Generaliza-tion through memorization: Nearest neighbor language models,2020, In International Conference onLearning Representations
 Overcom-ing catastrophic forgetting in neural networks,2017, Proceedings of the national academy of sciences
 Albert: A lite bert for self-supervised learning of language representations,2019, arXiv preprintarXiv:1909
 Zero-shot relation extraction viareading comprehension,2017, arXiv preprint arXiv:1706
 Retrieval-augmented gener-ation for knowledge-intensive nlp tasks,2020, arXiv preprint arXiv:2005
 Continual learning for sentence representations usingconceptors,2019, In NAACL
 Gradient episodic memory for continual learning,2017, InAdvances in neural information processing systems
 Continual learning fornatural language generation in task-oriented dialog systems,2020, arXiv preprint arXiv:2010
 KILT: a benchmark for knowl-edge intensive language tasks,2020, arXiv preprint arXiv:2009
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Evaluating protein transfer learning with tape,2019, In Advances in NeuralInformation Processing Systems
 Lamol: Language modeling for lifelong languagelearning,2020, In International Conference on Learning Representations
 Facts as experts: Adaptableand interpretable neural memory over symbolic knowledge,2020, arXiv preprint arXiv:2007
 Memory networks,2014, arXiv preprintarXiv:1410
 ERNIE: Enhancedlanguage representation with informative entities,2019, In Proceedings of the 57th Annual Meeting ofthe Association for Computational Linguistics (ACL)
 Aligning books and movies: Towards story-like visual explanations by watchingmovies and reading books,2015, In Proceedings of the IEEE International Conference on ComputerVision (ICCV)
