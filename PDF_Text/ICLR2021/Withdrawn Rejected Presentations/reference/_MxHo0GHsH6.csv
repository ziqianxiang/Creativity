title,year,conference
 Estimating or propagating gradientsthrough stochastic neurons for conditional computation,2013, arXiv preprint arXiv:1308
 Lsq+: Im-proving low-bit quantization through learnable offsets and better initialization,2020, arXiv preprintarXiv:2004
 Xnor-net++: Improved binary neural networks,2019, arXivpreprint arXiv:1909
 Bats: Binary architecture search,2020, arXivpreprint arXiv:2003
 Once for all: Train one network and specialize it for efficientdeployment,2019, arXiv preprint arXiv:1908
 Pact: Parameterized clipping activation for quantized neuralnetworks,2018, arXiv preprint arXiv:1805
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Learned step size quantization,2019, arXiv preprint arXiv:1902
 Single path one-shot neural architecture search with uniform sampling,2019, arXiv preprintarXiv:1904
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Squeeze-and-excitation networks,2018, In Proceedings of the IEEEconference on computer vision and pattern recognition
 Adabits: Neural network quantization with adaptive bit-widths,2019, arXiv preprint arXiv:1912
 Qkd: Quantization-awareknowledge distillation,2019, arXiv preprint arXiv:1911
 Improvingone-shot nas by suppressing the posterior fading,2019, arXiv preprint arXiv:1910
 Darts: Differentiable architecture search,2018, arXivpreprint arXiv:1806
 Shufflenet v2: Practical guidelinesfor efficient cnn architecture design,2018, In Proceedings of the European Conference on ComputerVision (ECCV)
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Searching for accurate binary neural ar-chitectures,2019, In Proceedings of the IEEE International Conference on Computer Vision Workshops
 Efficientnet: Rethinking model scaling for convolutional neuralnetworks,2019, arXiv preprint arXiv:1905
 Haq: Hardware-aware automated quan-tization with mixed precision,2019, In Proceedings of the IEEE conference on computer vision andpattern recognition
 Mixedprecision quantization of convnets via differentiable neural architecture search,2018, arXiv preprintarXiv:1812
 Any-precision deepneural networks,2019, arXiv preprint arXiv:1911
 Bignas: Scaling up neural archi-tecture search with big single-stage models,2020, arXiv preprint arXiv:2003
 Dorefa-net: Train-ing low bitwidth convolutional neural networks with low bitwidth gradients,2016, arXiv preprintarXiv:1606
 The training dataset is made upof 1,1000,28 million images with resolution 224 Ã— 224 belonging to 1000 classes and the validation set has50k images
