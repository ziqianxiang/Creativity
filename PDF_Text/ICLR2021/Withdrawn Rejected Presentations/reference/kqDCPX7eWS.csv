title,year,conference
 Qsgd: Communication-efficient sgd via gradient quantization and encoding,2017, In NIPS
 Stochastic gradient descent tricks,2012, In Neural networks: Tricks of the trade
 Global convergence rate analysis of unconstrained optimiza-tion methods based on probabilistic models,2018, Mathematical Programming
 Towards theoretical understanding of large batch training in stochasticgradient descent,2018, arXiv preprint arXiv:1812
 On the computational inefficiency of large batch sizes for stochas-tic gradient descent,2018, ArXiv
 Learning multiple layers of features from tiny images,2009, 2009
 Asynchronous stochastic subgra-dient methods for general nonsmooth nonconvex optimization,2019, arXiv: Optimization and Control
 Sgdr: Stochastic gradient descent with warm restarts,2017, In ICLR
 Elas-tic consistency: A general consistency model for distributed stochastic gradient descent,2020, arXivpreprint arXiv:2001
 Automatic differentiation inpytorch,2017, 2017
 Design ofexperiments and focused grid search for neural network parameter optimization,2016, Neurocomputing
 Hogwild: A lock-free approach toparallelizing stochastic gradient descent,2011, In Advances in neural information processing systems
 Taming the wild: A unified analysis of hogwild-stylealgorithms,2015, Advances in neural information processing systems
 A bayesian perspective on generalization and stochastic gradientdescent,2018, In International Conference on Learning Representations
 Local sgd converges fast and communicates little,2018, arXiv preprintarXiv:1805
 On the importance of initialization andmomentum in deep learning,2013, In ICML
 Image classification at super-computer scale,2018, ArXiv
 Large batch training of convolutional networks,2017, arXiv:Computer Vision and Pattern Recognition
 Wide residual networks,2016, ArXiv
 Hogwild++: A new mechanism for decentralizedasynchronous stochastic gradient descent,2016, 2016 IEEE 16th International Conference on DataMining (ICDM)
 Hogwild++: A new mechanism for decentral-ized asynchronous stochastic gradient descent,2016, In 2016 IEEE 16th International Conference onData Mining (ICDM)
 On the convergence properties of a k-step averaging stochastic gradi-ent descent algorithm for nonconvex optimization,2017, arXiv preprint arXiv:1708
 Parallelized stochastic gradient descent,2010, InNIPS
