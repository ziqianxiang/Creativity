title,year,conference
 Constrained k-means clustering,2000, MicrosoftResearch
 Efficientlifelong learning with A-GEM,2019, In International Conference on Learning Representations
 On tiny episodic memories in continuallearning,2019, arXiv preprint arXiv:1902
 Adversarial attack ongraph structured data,2018, In Proceedings of the 35th International Conference on Machine Learning
 Inductive representation learning on largegraphs,2017, In Isabelle Guyon
 Dynamic graph models,1997, Mathematical and Computer Modelling
 A survey and taxonomy of graph sampling,2013, arXiv preprintarXiv:1308
 Variational graph auto-encoders,2016, arXiv preprintarXiv:1611
 Semi-supervised classification with graph convolutional net-works,2017, In 5th International Conference on Learning Representations
 Overcom-ing catastrophic forgetting in neural networks,2017, Proceedings of the national academy of sciences
 Sampling from large graphs,2006, In Proceedings of the 12th ACMSIGKDD international conference on Knowledge discovery and data mining
 Learning without forgetting,2017, IEEE transactions on pattern analysisand machine intelligence
 Gradient episodic memory for continual learning,2017, InAdvances in neural information processing systems
 EvolveGCN: Evolving graph convolutionalnetworks for dynamic graphs,2020, In AAAI
 iCaRL:Incremental classifier and representation learning,2017, In Proceedings of the IEEE conference onComputer Vision and Pattern Recognition
 Graph attention networks,2018, In 6th International Conference on Learning Representations
 Inductive represen-tation learning on temporal graphs,2020, arXiv preprint arXiv:2002
 Revisiting semi-supervised learning withgraph embeddings,2016, In International conference on machine learning
 Graph-saint: Graph sampling based inductive learning method,2019, In International Conference on LearningRepresentations
 Deep learning on graphs: A survey,2020, IEEE Transactionson Knowledge and Data Engineering
 Graph neural networks: A review of methods and applications,2018, arXiv preprintarXiv:1812
85 in the random jump method,1080, For the backbone modelGraphSAGE
 Settings of backbone models are thesame as node classification,1080, During the training process
