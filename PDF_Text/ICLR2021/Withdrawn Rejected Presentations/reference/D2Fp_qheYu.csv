title,year,conference
 On the distribution of the two-sample Cramer-von Mises criterion,1962, TheAnnals of Mathematical Statistics
 The Bures metric for taming mode collapse in generative adversarial networks,2021, InSubmitted to International Conference on Learning Representations
 Theory of reproducing kernels,1950, Transactions of the American MathematicalSociety
 An extension of Kakutaniâ€™s theorem on infinite product measures to the tensor prod-uct of semifinite W*-algebras,1969, Transactions of the American Mathematical Society
 The bures metric for taming mode collapse in generative adversarial networks,2020, arXivpreprint arXiv:2006
 Generative modeling using the slicedWasserstein distance,2018, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Sur la distance de deux lois de PrObabilite,1957, C
 Cryptographic distinguishability measures forquantum-mechanical states,1999, Information Theory
 Kernel choice and classifiability for RKHS embeddings of probability distributions,2009, InAdvances in Neural Information Processing Systems
 AutoGAN: Neural architecturesearch for generative adversarial networks,2019, In The IEEE International Conference on ComputerVision (ICCV)
 In B,2007, Scholkopf
 Im-proved training of Wasserstein GANs,2017, In Advances in Neural Information Processing Systems
 Cor-recting sample selection bias by unlabeled data,2007, In Advances in Neural Information ProcessingSystems
 Revisiting Frank-Wolfe: Projection-free sparse convex optimization,2013, In Proceedingsof the 30th International Conference on Machine Learning
 Adam: A method for stochastic optimization,2015, In InternationalConference on Learning Representations (ICLR)
 Sliced Wasserstein distance for learn-ing Gaussian mixture models,2018, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Generalizedsliced Wasserstein distances,2019, In Advances in Neural Information Processing Systems
 Improvedprecision and recall metric for assessing generative models,2019, In Advances in Neural InformationProcessing Systems
 Sliced Wassersteindiscrepancy for unsupervised domain adaptation,2019, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 MMD GAN:Towards deeper understanding of moment matching network,2017, In Advances in Neural InformationProcessing Systems
 Detecting and correcting for label shiftwith black box predictors,2018, In International Conference on Machine Learning
 McGan: Mean and covariance feature matchingGAN,2017, In International Conference on Machine Learning
 Integral probability metrics and their generating classes of functions,1997, Advances inApplied Probability
 Generalizing point embeddings using the Wasserstein space of el-liptical distributions,2018, In S
 Estimating divergence functionalsand the likelihood ratio by convex risk minimization,2010, IEEE Transactions on Information Theory
 DatasetShift in Machine Learning,2009, The MIT Press
 Random features for large-scale kernel machines,2008, In Advances inNeural Information Processing Systems
 Optimal transport for applied mathematicians,2015, Birkauser NY
 Rethink-ing the inception architecture for computer vision,2016, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Sliced Wasserstein generative models,2019, In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition
 Rel-ative density-ratio estimation for robust distribution comparison,2013, Neural Computation
