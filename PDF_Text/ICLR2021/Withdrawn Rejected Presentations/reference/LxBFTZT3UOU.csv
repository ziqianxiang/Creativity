title,year,conference
 Training neural networks for and byinterpolation,2019, arXiv preprint arXiv:1906
 Practical gauss-newton optimisation for deeplearning,2017, arXiv preprint arXiv:1706
 Empirical study towards understanding line search approxi-mations for training neural networks,2019, arXiv preprint arXiv:1909
 Econometric theory,1964, Econometric theory
 Gradient-only line searches: An alternative to probabilistic linesearches,2019, arXiv preprint arXiv:1903
 Adam: A method for stochastic optimization,2014, CoRR
 Adaptive gradient methods with dynamic bound oflearning rate,2019, In International Conference on Learning Representations
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine learning
 Parabolic approximation line search for dnns,2020, arXiv preprintarXiv:1903
 L-sr1: a second order optimization method,2017, 2017
 On the convergence of adam and beyond,2018, InInternational Conference on Learning Representations
 A stochastic approximation method,1951, Annals of Mathematical Statistics
 Descending through a crowded valley-benchmarking deep learning optimizers,2020, arXiv preprint arXiv:2007
 A stochastic quasi-newton method for onlineconvex optimization,2007, In Marina Meila and Xiaotong Shen (eds
 A walk with sgd,2018, arXiv preprintarXiv:1802
 Adadelta: An adaptive learning rate method,2012, CoRR
