title,year,conference
 Obfuscated gradients give a false sense of se-curity: Circumventing defenses to adversarial examples,2018, In Proceedings of the 35th InternationalConference on Machine Learning
 Decision-based adversarial attacks: Reliable attacks againstblack-box machine learning models,2018, In International Conference on Learning Representations
 Towards evaluating the robustness of neural networks,2017, In 2017ieee symposium on security and privacy (sp)
 Keeping the bad guys out: Protecting and vaccinating deep learning withjpeg compression,2017, arXiv preprint arXiv:1705
 Responses of cellsin monkey visual cortex during perceptual filling-in of an artificial scotoma,1995, Nature
 Efficientdecision-based black-box adversarial attacks on face recognition,2019, In Proceedings of the IEEEConference on Computer Vision and Pattern Recognition
 Humans treat unreliablefilled-in percepts as more real than veridical ones,2017, Elife
 A rotation and atranslation suffice: Fooling cnns with simple transformations,2017, ArXiv
 Explaining and harnessing adversarialexamples,2015, In International Conference on Learning Representations
 Learning with a strong adver-sary,2015, arXiv preprint arXiv:1511
 Black-box adversarial attacks withlimited queries and information,2018, In Proceedings of the 35th International Conference on MachineLearning
 Adversarial logit pairing,2018, ArXiv
 The neural mechanisms of perceptual filling-in,2006, Nature reviews neuroscience
 Photo-realistic sin-gle image super-resolution using a generative adversarial network,2017, In Proceedings of the IEEEconference on computer vision and pattern recognition
 Adversarial examples detection in deep networks with convolutional filterstatistics,2017, In Proceedings of the IEEE International Conference on Computer Vision
 Characterizing adversarial subspaces using localintrinsic dimensionality,2018, In 6th International Conference on Learning Representations
 An interactive activation model of context effects inletter perception: I,1981, an account of basic findings
 On detecting adversar-ial perturbations,2017, In Proceedings of 5th International Conference on Learning Representations(ICLR)
 Distillation as adefense to adversarial perturbations against deep neural networks,2016, In 2016 IEEE Symposium onSecurity and Privacy (SP)
 Practical black-box attacks against machine learning,2017, In Proceedings of the 2017 ACMon Asia conference on computer and communications security
 Adversarial robustness through locallinearization,2019, In Advances in Neural Information Processing Systems
 Predictive coding in the visual cortex: a functional interpreta-tion of some extra-classical receptive-field effects,1999, Nature neuroscience
 Defense-GAN: Protecting classifiersagainst adversarial attacks using generative models,2018, In International Conference on LearningRepresentations
 The small world of the cerebral cortex,2004, Neuroinformatics
 One pixel attack for fooling deepneural networks,2019, IEEE Transactions on Evolutionary Computation
 Intriguing properties of neural networks,2014, In International Conference onLearning Representations
 Ensemble adversarial training: Attacks and defenses,2018, In 6th International Conferenceon Learning Representations
 Autozoom: Autoencoder-based zeroth order optimization method for attack-ing black-box neural networks,2019, In Proceedings of the AAAI Conference on Artificial Intelligence
 Deep image prior,2018, In Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition
 Neuralencoding and decoding with deep learning for dynamic natural vision,2018, Cerebral Cortex
 Feature squeezing: Detecting adversarial examples in deepneural networks,2017, arXiv preprint arXiv:1704
 Defense against adversarial attacks using feature scattering-basedadversarial training,2019, In Advances in Neural Information Processing Systems
