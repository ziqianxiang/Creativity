title,year,conference
 A mean-field limit for certain deep neural networks,2019, arXiv:1906
 Fine-grained analysis of optimization and generalization foroverparameterized two-layer neural networks,2019, In International Conference on Machine Learning
 Mean-field analysis of two-layer neural networks: Non-asymptotic ratesand generalization bounds,2020, arXiv:2002
 On the global convergence of gradient descent for over-parameterized models using optimaltransport,2018, In Advances in neural information processing systems
 On lazy training in differentiable programming,2019, In Advances in NeuralInformation Processing Systems
 Wandering random measures in the fleming-viot model,1982, The Annals of Probability
 Training neural networks as learning data-adaptive kernels: Provable representation and ap-proximation benefits,2019, arXiv:1901
 Gradient descent finds global minima of deep neuralnetworks,2019, In International Conference on Machine Learning
 Gradient descent provably optimizes over-parameterized neuralnetworks,2019, In International Conference on Learning Representation
 Over parameterized two-level neural networks can learn nearoptimal feature represen-tations,2019, arXiv:1910
 Understanding the difficulty of training deep feedforward neural networks,2010, In Proceedings ofthe thirteenth international conference on artificial intelligence and statistics
 Identity matters in deep learning,2016, In International Conference on Learning Representation
 Ordinary differential equations,1964, Wiley
 Delving deep into rectifiers: Surpassing human-level performance onimagenet classification,2015, In Proceedings of the IEEE international conference on computer vision
 Deep residual learning for image recognition,2016, In Proceedings of theIEEE conference on computer vision and pattern recognition
 Neural tangent kernel: Convergence and generalization in neural networks,2018, InAdvances in neural information processing systems
 Learning overparameterized neural networks via stochastic gradient descent on structured data,2018, InAdvances in Neural Information Processing Systems
 Mean-field theory of two-layers neural networks: dimension-free boundsand kernel limit,2019, In Annual Conference on Learning Theory
 Neural networks as interacting particle systems: Asymptotic convexity of the losslandscape and universal scaling of the approximation error,2018, arXiv:1805
 Mean field analysis of neural networks: A central limit theorem,2019, StochasticProcesses and their Applications
 Mean field analysis of deep neural networks,2019, arXiv:1903
 An introduction to matrix concentration inequalities,2015, Foundations and TrendsR in Machine Learning
 Introduction to the non-asymptotic analysis of random matrices,2010, arXiv:1011
 Fixup initialization: Residual learning without normalization,2019, arXiv:1901
 Stochastic gradient descent optimizes over-parameterized deep relunetworks,2018, In Advances in neural information Processing systems
