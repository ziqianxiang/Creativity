title,year,conference
 Deep Gaussian processes,2013, In Artificial Intelligence andStatistics
 Finite rank deep kernel learning,2018, InThird Workshop on Bayesian Deep Learning
 Dropout as a Bayesian approximation: Representing modeluncertainty in deep learning,2016, In International Conference on Machine Learning
 On calibration of modern neuralnetworks,2017, In International Conference on Machine Learning
 Probabilistic backpropagation for scalable learn-ing of Bayesian neural networks,2015, In International Conference on Machine Learning
 Multilayer feedforward networks areuniversal approximators,1989, Neural Networks
 Scalable Gaussian processes with bil-lions of inducing inputs via tensor train decomposition,2018, In International Conference on ArtificialIntelligence and Statistics
 Neural tangent kernel: convergence and gen-eralization in neural networks,2018, In Advances in neural information processing systems
 Semi-supervised deep kernel learning,2016, In NIPSBayesian Deep Learning Workshop
 Variational dropout and the local reparameteri-zation trick,2015, In Advances in Neural Information Processing Systems
 Simple and scalable predic-tive uncertainty estimation using deep ensembles,2017, In Advances in neural information processingsystems
 Deep neural networks as Gaussian processes,2018, In International Conference onLearning Representations
 Multilayer feedforward net-works with a nonpolynomial activation function can approximate any function,1993, Neural Networks
 Implicitkernel learning,2019, In The 22nd International Conference on Artificial Intelligence and Statistics
 Approximation theory of the MLP model in neural networks,1999, Acta Numerica
 Random features for large-scale kernel machines,2007, Advances inneural information processing systems
 Weighted sums of random kitchen sinks: Replacing minimizationwith randomization in learning,2008, Advances in neural information processing systems
 Doubly stochastic variational inference for deep Gaussianprocesses,2017, In Advances in Neural Information Processing Systems
 Deep informationpropagation,2016, In International Conference on Learning Representations
 Learning kernels with random features,2016, In Advances in NeuralInformation Processing Systems
 A Bayesian perspective on generalization and stochastic gradientdescent,2018, In International Conference on Learning Representations
 Variational learning of inducing variables in sparse Gaussian processes,2009, In ArtificialIntelligence and Statistics
 Stochastic variationaldeep kernel learning,2016, In Advances in Neural Information Processing Systems
 The case for Bayesian deep learning,2020, arXiv preprint arXiv:2001
 Kernelnet: A data-dependent kernel parameterizationfor deep generative modeling,2019, arXiv preprint arXiv:1912
