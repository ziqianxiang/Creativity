title,year,conference
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 A large anno-tated corpus for learning natural language inference,2015, arXiv preprint arXiv:1508
 A fast unified model for parsing and sentence understanding,2016, arXiv preprintarXiv:1603
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Generating sequences with recurrent neural networks,2013, arXiv preprintarXiv:1308
 Hippo: Recurrent memorywith optimal polynomial projections,2020, arXiv preprint arXiv:2008
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 A simple way to initialize recurrent networksof rectified linear units,2015, arXiv preprint arXiv:1504
 Learned in translation:Contextualized word vectors,2017, In Advances in Neural Information Processing Systems
 Glove: Global vectors for wordrepresentation,2014, In Proceedings of the 2014 conference on empirical methods in natural languageprocessing (EMNLP)
 Deep contextualized word representations,2018, In Proc
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Natural language understandingwith the quora question pairs dataset,2019, arXiv preprint arXiv:1907
 Baseline needs more love: On simple word-embedding-based models and associated pooling mechanisms,2018, arXiv preprint arXiv:1805
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Legendre memory units: Continuous-time repre-sentation in recurrent neural networks,2019, In Advances in Neural Information Processing Systems
