title,year,conference
 Philosophy of language,1964, Prentice Hall
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 This lookslike that: Deep learning for interpretable image recognition,2019, In Advances in Neural InformationProcessing Systems
 Retain: An interpretable predictive model for healthcare using reverse time attentionmechanism,2016, In Advances in Neural Information Processing Systems
 Whatis one grain of sand in the desert? analyzing individual neurons in deep nlp models,2019, In Proceedingsof the AAAI Conference on Artificial Intelligence
 The role of frame-based representation in reasoning,1985, Communicationsof the ACM
 An interpretable LSTM neural network for autoregressive exogenousmodel,2018, arXiv preprint arXiv:1804
 Attention is not explanation,2019, arXiv preprint arXiv:1902
 Towards hierarchical importanceattribution: Explaining compositional semantics for neural sequence models,2019, arXiv preprintarXiv:1911
 The Bayesian case model: A generative approach forcase-based reasoning and prototype classification,2014, In Advances in Neural Information ProcessingSystems
 ADAM: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 A unified approach to interpreting model predictions,2017, In Advancesin neural information processing systems
 Beyond word importance: Contextual decomposition toextract interactions from lstms,2018, arXiv preprint arXiv:1801
 A prototype approach to sentences and sentencetypes,2008, Annual Review of Cognitive Linguistics
 GloVe: Global vectors for Wordrepresentation,2014, In Proceedings of the 2014 Conference on Empirical Methods in Natural LanguageProcessing (EMNLP)
 Sentence-bert: Sentence embeddings using Siamese BERT-networks,2019, arXiv preprint arXiv:1908
 ” why should i trust you?” explaining thepredictions of any classifier,2016, In Proceedings of the 22nd ACM SIGKDD international conferenceon knowledge discovery and data mining
 Stop explaining black box machine learning models for high stakes decisions and useinterpretable models instead,2019, Nature Machine Intelligence
 A study in rashomon curves and volumes: Anew perspective on generalization and model simplicity in machine learning,2019, arXiv preprintarXiv:1908
 Learning important features throughpropagating activation differences,2017, arXiv preprint arXiv:1704
 LSTMVis: A toolfor visual analysis of hidden state dynamics in recurrent neural networks,2017, IEEE Transactions onVisualization and Computer Graphics
 Axiomatic attribution for deep networks,2017, arXivpreprint arXiv:1703
 Can i trust you more? model-agnostichierarchical explanations,2018, arXiv preprint arXiv:1812
 MDNet: A semanticallyand visually interpretable medical image diagnosis network,2017, In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition
