title,year,conference
 Towards principled methods for training generative adversarialnetworks,2017, arXiv Preprint arXiv:1701
 Gradnorm: Gradientnormalization for adaptive loss balancing in deep multitask networks,2018, In International COnferenCeon MaChine Learning
 Multi-task learning for mul-tiple language translation,2015, In Proceedings of the 53rd AnnUaI Meeting of the ASSOCiatiOnfor Computational LingUiStiCS and the 7th International Joint COnferenCe on NatUraI LangUagePrOCeSSing (VQIUme 1: Long Papers)
 Convergence of learning dynamics in stack-elberg games,2019, arXiv PrePrint arXiv:1906
 Meta-learning with warped gradient descent,2019, arXiv PrePrint arXiv:1909
 Dynamic task priori-tization for multitask learning,2018, In PrOCeedingS of the EUrOPean Conference on COmPUter ViSiOn(ECCV)
 Mask r-cnn,2017, In Proceedings of theIEEE international COnferenCe on COmPUter ViSion
 Adam: A method for stochastic optimization,2014, arXiv PrePrintarXiv:1412
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Multi-task learning by a top-down control network,2020, arXiv PrePrintarXiv:2002
 Attentive single-tasking of multi-ple tasks,2019, In PrOCeedingS of the IEEE COnferenCe on COmPUter ViSiOn and Pattern Recognition
 Chexnet: Radiologist-level pneumo-nia detection on chest x-rays with deep learning,2017, arXiv PrePrint arXiv:1711
 An overview of multi-task learning in deep neural networks,2017, arXiv PrePrintarXiv:1706
 Multi-task learning as multi-objective optimization,2018, In AdVanceSin NeUraI InfOrmatiOn PrOCeSSing Systems
 Discovering structure in multiple learning tasks: The tcalgorithm,1996, In ICML
 Branchedmulti-task networks: deciding What layers to share,2019, arXiv PrePrint arXiv:1904
 Taskonomy: Disentangling task transfer learning,2018, In Proceedings of the IEEEconference on ComPUter ViSion and Pattem recognition
