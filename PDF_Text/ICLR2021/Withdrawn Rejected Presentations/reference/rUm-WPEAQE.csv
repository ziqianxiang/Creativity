title,year,conference
 Even faster accelerated coordinatedescent using non-uniform sampling,2016, In International Conference on Machine Learning
 signSGD:compressed optimisation for non-convex problems,2018, arXiv preprint arXiv:1802
 Optimization methods for large-scale machine learning,2018, SIAMReview
 Lag: Lazily aggregated gradient forcommunication-efficient distributed learning,2018, In Advances in Neural Information ProcessingSystems
 High-accuracy low-precision training,2018, arXiv preprintarXiv:1803
 SAGA: A fast incremental gradient methodwith support for non-strongly convex composite objectives,2014, In Advances in neural informationprocessing systems
 Optimal mini-batch and step sizes forSAGA,2019, arXiv preprint arXiv:1902
 Learning anddata selection in big datasets,2019, In Proc
 Stochastic quasi-gradient methods: Variancereduction via Jacobian sketching,2018, arXiv preprint arXiv:1805
 A survey of recent results in networkedcontrol systems,2007, Proceedings of the IEEE
 Nonconvex variance reduced optimization with arbitrarysampling,2019, In Proceedings of the 36th International Conference on Machine Learning
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In Advances in neural information processing systems
 Linear convergence of gradient and proximal-gradient methods under the Polyak-Lojasiewicz condition,2016, In Joint European Conference onMachine Learning and Knowledge Discovery in Databases
 Error feedbackfixes signSGD and other gradient compression schemes,2019, arXiv preprint arXiv:1901
 Semi-stochastic coordinate descent,2017, optimizationMethods and Software
 On maintaining linear convergence of dis-tributed learning and optimization under limited communication,2019, arXiv preprint arXiv:1902
 Faster coordinate descent via adaptiveimportance sampling,2017, arXiv preprint arXiv:1703
 SAGA with arbitrary sampling,2019, arXiv preprintarXiv:1901
 Safe adaptive importance sampling,2017, In Advances inNeural Information Processing Systems
 Sparsified sgd with memory,2018, InS
 Communication-efficient distributedlearning via lazily aggregated quantized gradients,2019, In Advances in Neural Information ProcessingSystems
 Communication-efficientdistributed deep learning: A comprehensive survey,2020, arXiv preprint arXiv:2003
 Cooperative SGD: A unified framework for the design and analysis ofcommunication-efficient SGD algorithms,2018, arXiv preprint arXiv:1808
 Asynchronous federated optimization,2019, arXiv preprintarXiv:1903
 LAGC: Lazily aggregated gradient coding for straggler-tolerantand communication-efficient distributed learning,2019, arXiv preprint arXiv:1905
 Deep learning with elastic averaging SGD,2015, InAdvances in Neural Information Processing Systems
 Communication-efficient algorithms forstatistical optimization,2012, In Advances in Neural Information Processing Systems
 Distributed nonparametric regression under communicationconstraints,2018, In Proc
