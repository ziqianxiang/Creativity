title,year,conference
 The benefits of over-parameterization at initialization in deeprelu networks,2019, arXiv PrePrint arXiv:1901
 k-means++: The advantages of careful seeding,2006, TechnicalrePort
 Greedy layerwise learning can scaleto imagenet,2019, In International conference on machine Iearning
 Refining initial points for k-means clustering,1998, In ICML
 A comparative study of efficient initial-ization methods for the k-means clustering algorithm,2013, ExPert SyStemS With applications
 Towards efficient model com-pression via learned global ranking,2020, In PrOceedingS of the IEEE COnference on COmPUter ViSiOnand Pattern Recognition
 A guide to deeplearning in healthcare,2019, NatUre medicine
 Rigging the lottery:Making all tickets winners,2019, arXiv PrePrint arXiv:1911
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In PrOceedingS of the thirteenth international conference on artificial intelligence andStatiStics
 Alternatives to the k-means algorithm that find better cluster-ings,2002, In PrOceedingS of the eleventh international conference on InfOrmatiOn and knowledgemanagement
 Complexity of linear regions in deep networks,2019, arXiv PrePrintarXiv:1901
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on CompUter vision
 Soft filter prUning for acceleratingdeep convolutional neural networks,2018, arXiv PrePrint arXiv:1808
 An efficient k-means clustering algorithm: Analysis and implementation,2002, IEEEtransactions on Pattern analysis and machine intelligence
 Effect of depth and width on localminima in deep learning,2019, NeUraI computation
 Optimal brain damage,1990, In AdVanCeS in neuralinformation PrOCeSSing systems
 SNIP: SINGLE-SHOT NETWORKPRUNING BASED ON CONNECTION SENSITIVITY,2019, In International COnference on LearningRePreSentations
 Halo: Hardware-aware learning to optimize,2020, In Proceedings of the EUrOPean COnference on COmPUter ViSiOn(ECCV)
 Learn-ing efficient convolutional networks through network slimming,2017, In PrOCeedingS of the IEEEInternational COnferenCe on COmPUter ViSion
 Rethinking the value ofnetwork pruning,2019, In International COnferenCe on Learning RePreSentations
 Thinet: A filter level pruning method for deep neuralnetwork compression,2017, In Proceedings of the IEEE international COnferenCe on COmPUter ViSion
 Rectifier nonlinearities improve neural net-work acoustic models,2013, In Proc
 Some methods for classification and analysis of multivariate observations,1967, InPrOCeedingS of the fifth BerkeIey SymPOSiUm on mathematical StatiStiCS andprobability
 All you need is a good init,2015, arXiv PrePrint arXiv:1511
 Clustering earthquake signals and background noises in continuous seismic data withunsupervised deep learning,2020, NatUre COmmUniCations
 Mastering the game of gowithout human knowledge,2017, nature
 A representer theorem for deep neural networks,2018, arXiv preprint arXiv:1802
 Drawing early-bird tickets: Toward more efficienttraining of deep networks,2020, In International COnference on Learning Representations
 Residual dense network forimage super-resolution,2018, In Proceedings of the IEEE conference on computer vision and patternrecognition
 An improved analysis of training over-parameterized deep neuralnetworks,2019, In Advances in Neural InfOrmatiOn PrOcessing Systems
