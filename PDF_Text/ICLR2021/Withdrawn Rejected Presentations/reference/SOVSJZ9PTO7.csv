title,year,conference
 Layer normalization,2016, arXiv preprintarXiv:1607
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Cognitive graph for multi-hopreading comprehension at scale,2019, arXiv preprint arXiv:1905
 En-tities as experts: Sparse memory access with entity supervision,2020, arXiv preprint arXiv:2004
0:Towards more challenging few-shot relation classification,2019, arXiv preprint arXiv:1910
 Realm: Retrieval-aUgmented langUage model pre-training,2020, arXiv preprint arXiv:2002
 Sensebert: Driving some sense into bert,2019, arXiv preprint arXiv:1908
 LingUisticknowledge and transferability of contextUal representations,2019, arXiv preprint arXiv:1903
 K-bert:Enabling langUage representation with knowledge graph,2020, In AAAI
 Roberta: A robUstly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Deep contextUalized word representations,2018, arXiv preprint arXiv:1802
 Knowledge enhanced contextual word representations,2019, In EMNLP
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 Sling: A framework for frame semanticparsing,2017, arXiv preprint arXiv:1710
 Ex-ploiting structured knowledge in text via graph-guided representation learning,2020, arXiv preprintarXiv:2004
 Matching theblanks: Distributional similarity for relation learning,2019, arXiv preprint arXiv:1906
 Ernie: Enhanced representation through knowledge integration,2019, arXivpreprint arXiv:1904
 olmpics-on what language modelpre-training captures,2019, arXiv preprint arXiv:1912
 Composition-based multi-relational graph convolutional networks,2019, arXiv preprint arXiv:1911
 Graph attention networks,2017, arXiv preprint arXiv:1710
 Facts as experts: Adaptableand interpretable neural memory over symbolic knowledge,2020, arXiv preprint arXiv:2007
 Deep graph library: Towards efficient and scalable deep learningon graphs,2019, arXiv preprint arXiv:1909
 Kepler: Aunified model for knowledge embedding and pre-trained language representation,2019, arXiv preprintarXiv:1911
 Huggingfaceâ€™s transformers: State-of-the-art natural language processing,2019, ArXiv
 Pretrained encyclope-dia: Weakly supervised knowledge-pretrained language model,2019, arXiv preprint arXiv:1912
 Ernie: Enhancedlanguage representation with informative entities,2019, arXiv preprint arXiv:1905
