title,year,conference
 Deep equilibrium models,2019, In H
 Model interpretabilitythrough the lens of computational complexity,2020, In Advances in Neural Information ProcessingSystems 33
 What does BERT lookat? An analysis of BERT’s attention,2019, CoRR
 Explaining explanations: Anoverview of interpretability of machine learning,2018, In Int
 EU regulations on algorithmic decision-making and a ”right to expla-nation”,2016, ArXiv
 Adaptive computation time for recurrent neural networks,2016, ArXiv
 Shallow-deep networks: Understanding andmitigating network overthinking,2019, In International Conference on Machine Learning
 FastBERT: a self-distilling BERT with adaptive inference time,2020, In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics
 Are sixteen heads really better thanone? In H,2019, Wallach
 Automatic differentiation inpytorch,2017, 2017
 Green ai,2019, ArXiv
 Energy and policy considerations for deep learning inNLP,2019, ArXiv
 Axiomatic attribution for deep networks,2017, In ICML
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Huggingface’s transformers: State-of-the-art natural language processing,2019, ArXiv
 DeeBERT: Dynamic early exitingfor accelerating BERT inference,2020, In Proceedings of the 58th Annual Meeting of the Associationfor ComPUtational Linguistics
 DeeBERT: Dynamic early exitingfor accelerating BERT inference,2020, In Proceedings of the 58th Annual Meeting of the Associationfor ComPutational Linguistics
