title,year,conference
 Critical learning periods in deep neural networks,2019, InInternational Conference on Learning Representations
 Quasi-recurrent neural networks,2016, arXivpreprint arXiv:1611
 cudnn: Efficient primitives for deep learning,2014, arXiv preprint arXiv:1410
 Deep ensembles: A loss landscape perspective,2019, arXivpreprint arXiv:1912
 The early phase of neural network training,2020, arXiv preprintarXiv:2002
 Stable architectures for deep neural networks,2017, Inverse Problems
 Delving deep into rectifiers: Surpassing human-levelperformance on imagenet classification,2015, In Proceedings of the IEEE international conference on computervision
 Deep residual learning for image recognition,2016, InProceedings of the IEEE conference on computer vision and pattern recognition
 Reproducibility of benchmarked deepreinforcement learning tasks for continuous control,2017, arXiv preprint arXiv:1708
 Adam: A method for stochastic optimization,2014, arXiv preprint arXiv:1412
 Similarity of neural networkrepresentations revisited,2019, arXiv preprint arXiv:1905
 Learning multiple layers of features from tiny images,2009, 2009
 Gradient-based learning applied to documentrecognition,1998, Proceedings of the IEEE
 Sgdr: Stochastic gradient descent with warm restarts,2016, arXiv preprintarXiv:1608
 Shufflenet v2: Practical guidelines for efficient cnnarchitecture design,2018, In Proceedings of the European conference on computer vision (ECCV)
 On model stability as a function of random seed,2019, In Conference onComputational Natural Language Learning
 Building a large annotated corpus of english:The penn treebank,1993, 1993
 The impact of nondeterminism on reproducibility in deepreinforcement learning,2018, 2018
 Hogwild: A lock-free approach to parallelizingstochastic gradient descent,2011, In Advances in neural information processing systems
 Scipy 1,2020,0: fundamental algorithms forscientific computing in python
 Regularization of neural networks usingdropconnect,2013, In International conference on machine learning
 Batchensemble: an alternative approach to efficient ensemble andlifelong learning,2020, arXiv preprint arXiv:2002
 Improving the robustness of deep neuralnetworks via stability training,2016, In Proceedings of the ieee conference on computer vision and patternrecognition
