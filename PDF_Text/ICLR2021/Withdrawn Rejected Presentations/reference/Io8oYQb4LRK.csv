title,year,conference
 Botorch: Programmable bayesian optimization in py-torch,2019, CoRR
 On-line learning rate adaptation with hypergradient descent,2018, In International Conference on LearningRepresentations
 The problem of learning long-term dependencies in recurrentnetworks,1993, In IEEE International Conference on Neural Networks
 Learning long-term dependencies with gradient descent isdifficult,1994, IEEE Transactions on Neural Networks
 Gradient-based optimization of hyperparameters,2000, Neural Comput
 Large scale GAN training for high fidelitynatural image synthesis,2019, In International Conference on Learning Representations
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Generic methods for optimization-based modeling,2012, In Neil D
 Hyperparameter Optimization,2019, Springer International Publishing
 Meta-learning with warped gradient descent,2020, In International Conference on LearningRepresentations
 Forward and reversegradient-based hyperparameter optimization,2017, In Doina Precup and Yee Whye Teh (eds
 Bilevelprogramming for hyperparameter optimization and meta-learning,2018, In International conference onMachine Learning
 Drmad: Distilling reverse-mode automatic differentiation for optimizing hyperparameters of deep neural networks,2016, In IJCAI
 DeeP residual learning for image recog-nition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Long short-term memory,1997, Neural computation
 Meta-learning in neuralnetworks: A survey,2020, arXiv preprint arXiv:2004
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In Francis Bach and David Blei (eds
 Population based train-ing of neural networks,2017, arXiv preprint arXiv:1711
 Adam: A method for stochastic optimization,2015, In Yoshua Bengioand Yann LeCun (eds
 Random search and reproducibility for neural architecture search,2019, InUAI
 DARTS: Differentiable architecture search,2019, InInternational Conference on Learning Representations
 Scalable gradient-based tuningof continuous regularization hyperparameters,2016, In International conference on Machine Learning
 Wavenet: A generative model forraw audio,2016, arXiv preprint arXiv:1609
 On the difficulty of training recurrent neuralnetworks,2013, In International conference on Machine Learning
 Meta-learning with im-plicit gradients,2019, In Advances in Neural Information Processing Systems
 Truncated back-propagationfor bilevel optimization,2019, In AISTATS
 Scalable bayesian optimization using deepneural networks,2015, In International conference on Machine Learning
 The Theory Of Market Economy,1952, Oxford University Press
 Dataset distillation,2018, arXivpreprint arXiv:1811
 Backpropagation through time: what it does and how to do it,1990, Proceedings of theIEEE
 A learning algorithm for continually running fully recurrent neuralnetworks,1989, Neural Computation
 Understanding short-horizon bias instochastic meta-optimization,2018, In International Conference on Learning Representations
 Wide residual networks,2016, In BMVC
