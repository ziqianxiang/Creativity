title,year,conference
 Variance reduction for faster non-convex optimization,2016, InInternational conference on machine learning
 Iterative soft-thresholding converges linearly,2007, arXiv preprintarXiv:0709
 Saga: A fast incremental gradient methodwith support for non-strongly convex composite objectives,2014, In Advances in neural informationprocessing systems
 Bohb: Robust and efficient hyperparameter opti-mization at scale,2018, In International Conference on Machine Learning
 Geometric measure theory,2017, Springer
 Bilevelprogramming for hyperparameter optimization and meta-learning,2018, In International Conference onMachine Learning
 Deep learning,2016, MIT press
 Asynchronous doubly stochastic group regularized learning,2018, InInternational Conference on Artificial Intelligence and Statistics (AISTATS 2018)
 Anew generalized error path algorithm for model selection,2015, In InternationalConference on Machine Learning
 Faster derivative-free stochastic algorithmfor shared memory machines,2018, In International Conference on Machine Learning
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Imagenet classification with deep convolu-Honal neural networks,2012, In Advances in neural information processing Systems
 Gradient-based hyperparameter optimizationthrough reversible learning,2015, In International Conference on Machine Learning
 Hyperparameter optimization with approximate gradient,2016, In InternationalConference on Machine Learning
 Principles of mathematical analysis,1964, 1976
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Practical bayesian optimization of machinelearning algorithms,2012, In Advances in neural information processing systems
 Lipschitz regularity of deep neural networks: analysis andefficient estimation,2018, In Advances in Neural Information Processing Systems
