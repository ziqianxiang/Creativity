title,year,conference
 A convergence theory for deep learning via over-parameterization,2018, arXiv preprint arXiv:1811
 A convergence analysis of gradientdescent for deep linear neural networks,2018, arXiv preprint arXiv:1810
 Complex-valued autoencoders,2012, Neural Networks
 Globally optimal gradient descent for a convnet with gaussianinputs,2017, arXiv preprint arXiv:1702
 “learning-compression” algorithms for neural netpruning,2018, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Gradient descent finds globalminima of deep neural networks,2018, arXiv preprint arXiv:1811
 The difficulty of training sparseneural networks,2019, arXiv preprint arXiv:1906
 The state of sparsity in deep neural networks,2019, arXivpreprint arXiv:1902
 Learning one-hidden-layer neural networks with landscapedesign,2017, arXiv preprint arXiv:1711
 Deep learning without poor local minima,2016, In Advances in neural informationprocessing systems
 Snip: Single-shot network pruningbased on connection sensitivity,2018, arXiv preprint arXiv:1810
 On the benefit of width for neural networks: Disappearanceof bad basins,2018, arXiv
 Rethinking the value ofnetwork pruning,2018, arXiv preprint arXiv:1810
 Bayesian compression for deep learning,2017, InAdvances in neural information processing Systems
 Learning sparse neural networks throughl_0 regularization,2017, arXiv preprint arXiv:1712
 Depth creates no bad local minima,2017, arXiv preprintarXiv:1702
 Proving the lottery tickethypothesis: Pruning is all you need,2020, arXiv preprint arXiv:2002
 A mean field view of the landscape of two-layer neural networks,2018, Proceedings of the National Academy of Sciences
 Variational dropout sparsifies deep neuralnetworks,2017, arXiv preprint arXiv:1701
 Exact solutions to the nonlinear dynam-ics of learning in deep linear neural networks,2013, arXiv preprint arXiv:1312
 Weight sharing is crucial to succesfuloptimization,2017, arXiv preprint arXiv:1706
 The global landscape ofneural networks: An overview,2020, IEEE Signal Processing Magazine
 Small nonlinearities in activation functions create badlocal minima in neural networks,2018, arXiv preprint arXiv:1802
