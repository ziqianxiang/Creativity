title,year,conference
 Language models arefew-shot learners,2020, arXiv:2005
 MANAS: Multi-agent neural architecture search,2019, arxiv:1909
 A survey of model compression and accelerationfor deep neural netWorks,2017, arXiv:1710
 A comprehen-sive survey on model compression and acceleration,2020, Artificial Intelligence Review
 ImageNet: A large-scalehierarchical image database,2009, In Computer Vision and Pattern Recognition (CVPR)
 BOHB: Robust and efficient hyperparameter op-timization at scale,2018, In International Conference on Machine Learning (ICML)
 Weight discretization paradigm for optical neu-ral netWorks,1990, In Optical interconnections and networks
 Search for better students to learn distilled knoWledge,2020, arXiv preprintarXiv:2001
 Learning both Weights and connections forefficient neural netWork,2015, In Advances in neural information processing systems
 Distilling the knoWledge in a neural netWork,2015, arXivpreprint arXiv:1503
 Gpipe: Efficient training of giant neu-ral netWorks using pipeline parallelism,2019, In Advances in Neural Information Processing Systems(NeurIPS
 Like What you like: KnoWledge distill via neuron selectivitytransfer,2017, arXiv preprint arXiv:1707
 Learning multiple layers of features from tiny images,2009, Technical report
 Optimal brain damage,1990, In Advances in neuralinformation processing systems
 Pruning filters forefficient convnets,2016, 2016a
 Hyperband:A novel bandit-based approach to hyperparameter optimization,2016, arXiv:1603
 DARTS: Differentiable architecture search,2019, InInternational Conference on Learning Representations (ICLR)
 Search to distill: Pearls are everywhere but not the eyes,2019, arXiv preprint arXiv:1911
 Model compression via distillation and quanti-zation,2018, arXiv preprint arXiv:1802
 Recognizing indoor scenes,2009, In Computer Vision and PatternRecognition (CVPR)
 Xnor-net: Imagenetclassification using binary convolutional neural networks,2016, In European conference on computervision
 Large-scale evolution of image classifiers,2017, In InternationalConference on Machine Learning (ICML)
 Multi-fidelity neural architecture search with knowledge distillation,2020, arXiv preprintarXiv:2006
 Similarity-preserving knowledge distillation,2019, In Proceedings of theIEEE International Conference on Computer Vision
 Exploring randomly wired neuralnetworks for image recognition,2019, arXiv:1904
 NAS evaluation is frustratingly hard,2020, InInternational Conference on Learning Representations (ICLR)
 Paying more attention to attention: Improving the perfor-mance of convolutional neural networks via attention transfer,2016, arXiv preprint arXiv:1612
 Towards automated deep learning:Efficient joint neural architecture and hyperparameter search,2018, arXiv:1807
 Trained ternary quantization,2016, arXivpreprint arXiv:1612
