title,year,conference
 An optimistic perspective on offlinereinforcement learning,2019, arXiv preprint arXiv:1907
 On the model-based stochastic value gradient for continuous reinforcement learning,2020, arXiv preprintarXiv:2008
 Learning to learn by gradient descent by gradient descent,2016, In Advances inNeural Information Processing Systems (NIPS)
 Feedback Systems: An Introduction for Scientists andEngineers,2008, Princeton University Press
 Planning by probabilistic inference,2003, In AISTATS
 Layer normalization,2016, arXiv preprintarXiv:1607
 Model-predictive planning via cross-entropyand gradient-based optimization,2020, In Learning for Dynamics and Control
 Openai gym,2016, arXiv preprint arXiv:1606
 Imagined value gra-dients: Model-based policy optimization with tranferable latent dynamics models,2020, In Conferenceon Robot Learning
 Deep reinforcement learn-ing in a handful of trials using probabilistic dynamics models,2018, In Advances in Neural InformationProcessing Systems
 Better exploration with optimisticactor critic,2019, In Advances in Neural Information Processing Systems
 Model-augmented actor-critic: Backpropagating throughpaths,2020, In International Conference on Learning Representations
 Inference suboPtimality in variational autoen-coders,2018, In International Conference on Machine Learning
 Addressing function aPProximation error inactor-critic methods,2018, In International Conference on Machine Learning
 Adaptive computation time for recurrent neural networks,2016, arXiv preprintarXiv:1603
 Multi-object representationlearning with iterative variational inference,2019, In International Conference on Machine Learning
 An investigation of model-freeplanning,2019, In International Conference on Machine Learning
 Soft actor-critic algorithms and appli-cations,2018, arXiv preprint arXiv:1812
 Learning latent dynamics for planning from pixels,2019, In International Conference onMachine Learning
 Learn-ing continuous control policies by stochastic value gradients,2015, In Advances in Neural InformationProcessing Systems
 Model-based planning with discrete andcontinuous actions,2017, arXiv preprint arXiv:1705
 Iterative refinement of the approximate posterior for directed belief networks,2016, InAdvances in Neural Information Processing Systems (NIPS)
 Long short-term memory,1997, Neural computation
 Qt-opt: Scalable deepreinforcement learning for vision-based robotic manipulation,2018, arXiv preprint arXiv:1806
 Semi-amortizedvariational autoencoders,2018, In Proceedings of the International Conference on Machine Learning(ICML)
 Stochastic gradient vb and the variational auto-encoder,2014, InProceedings of the International Conference on Learning Representations
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Conservative q-learning for offlinereinforcement learning,2020, arXiv preprint arXiv:2006
 Safe end-to-end imitation learning formodel predictive control,2019, In International Conference on Robotics and Automation
 A general method for amortizing variationalfiltering,2018, In Advances in Neural Information Processing Systems
 Iterative amortized inference,2018, In InternationalConference on Machine Learning
 Neural variational inference and learning in belief networks,2014, InInternational Conference on Machine Learning
 Asynchronous methods for deep reinforcementlearning,2016, In International conference on machine learning
 Safe and efficient off-policyreinforcement learning,2016, In Advances in Neural Information Processing Systems
 A game theoretic framework for modelbased reinforcement learning,2020, arXiv preprint arXiv:2004
 Stochastic backpropagation andapproximate inference in deep generative models,2014, In Proceedings of the International Conferenceon Machine Learning
 Trust regionpolicy optimization,2015, In International conference on machine learning
 Proximal policyoptimization algorithms,2017, arXiv preprint arXiv:1707
 Masteringthe game of go with deep neural networks and tree search,2016, nature
 Universal plan-ning networks: Learning generalizable representations for visuomotor control,2018, In InternationalConference on Machine Learning
 Training very deep networks,2015, InAdvances in neural information processing systems (NIPS)
 Reinforcement learning: An introduction,2018, MIT press
 Boosting trust region policy optimization by normalizing flowspolicy,2018, arXiv preprint arXiv:1809
 Issues in using function approximation for reinforcementlearning,1993, In Proceedings of the 1993 Connectionist Models Summer School Hillsdale
 Exploiting hierarchy for learningand transfer in kl-regularized rl,2019, arXiv preprint arXiv:1903
 Probabilistic inference for solving discrete and continuous statemarkov decision processes,2006, In Proceedings of the 23rd international conference on Machinelearning
 Deep reinforcement learning with double q-learning,2016, In Thirtieth AAAI conference on artificial intelligence
 Simple statistical gradient-following algorithms for connectionist reinforcementlearning,1992, In Reinforcement Learning
