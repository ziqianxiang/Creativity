title,year,conference
 Deep rewiring: Trainingvery sparse deep networks,2017, arXiv preprint arXiv:1711
 Learning long-term dependencies with gradientdescent is difficult,1994, IEEE transactions on neural networks
 Gradnorm: Gradientnormalization for adaptive loss balancing in deep multitask networks,2018, In International Conferenceon Machine Learning
 OPtimal brain damage,1990, In Advances in NeuralInformation Processing Systems
 Sparse networks from scratch: Faster training without losingperformance,2019, arXiv preprint arXiv:1907
 Activation function impact on sparse neural networks,2020, B
 The difficulty of training sparseneural networks,2019, arXiv preprint arXiv:1906
 Gradient flow in sparse neural net-works and how lottery tickets win,2020, arXiv preprint arXiv:2010
 Linear modeconnectivity and the lottery ticket hypothesis,2019, arXiv preprint arXiv:1912
 Stabilizing thelottery ticket hypothesis,2019, arXiv preprint arXiv:1903
 The state of sparsity in deep neural networks,2019, arXivpreprint arXiv:1902
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Comparing biases for minimal network construction withback-propagation,1989, In Advances in neural information processing systems
 293-299 vol,1993,1
 Second order derivatives for network pruning: Optimal brainsurgeon,1992, In NIPS
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Neural networks for machine learninglecture 6a overview of mini-batch gradient descent,2012, Cited on
 Untersuchungen zu dynamischen neuronalen netzen,1991, Diploma
 Multilayer feedforward networks areuniversal approximators,1989, Neural networks
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, CoRR
 Fantasticgeneralization measures and where to find them,2019, arXiv preprint arXiv:1912
 Deeplearning with s-shaped rectified linear activation units,2015, arXiv preprint arXiv:1512
 A simple procedure for pruning back-propagation trained neural networks,1990, IEEEtransactions on neural networks
 A new measure of rank correlation,1938, Biometrika
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in neural information processing systems
 SNIP: single-shot network pruningbased on connection sensitivity,2018, CoRR
 Snip: Single-shot network pruningbased on connection sensitivity,2018, arXiv preprint arXiv:1810
 A signal propagationperspective for pruning neural networks at initialization,2019, arXiv preprint arXiv:1906
 Finding trainable sparse networks through neural tangent trans-fer,2020, arXiv preprint arXiv:2006
 Rethinking the value ofnetwork pruning,2018, arXiv preprint arXiv:1810
 Rethinking the Value ofNetwork Pruning,2018, CoRR
 Merge-based parallel sparse matrix-vector multiplication,2016, InSCâ€™16: Proceedings of the International Conference for High Performance Computing
 Scalable training of artificial neural networks with adaptive sparse connec-tivity inspired by network science,2018, Nature communications
 Parameter efficient training of deep convolutional neural networksby dynamic sparse reparameterization,2019, arXiv preprint arXiv:1902
 Skeletonization: A technique for trim-ming the fat from a network via relevance assessment,1989, In D
 Connectionist learning of belief networks,1992, Artificial intelligence
 On the behavior of the gradient norm in thesteepest descent method,2002, Computational Optimization and Applications
 On the difficulty of training recurrent neuralnetworks,2013, In International conference on machine learning
 Swish: a self-gated activation function,2017, arXivpreprint arXiv:1710
 An overview of gradient descent optimization algorithms,2016, arXiv preprintarXiv:1609
 Highway networks,2015, arXivpreprintarXiv:1505
 Pruning neural networkswithout any data by iteratively conserving synaptic flow,2020, arXiv preprint arXiv:2006
 Picking winning tickets before training bypreserving gradient flow,2020, arXiv preprint arXiv:2002
 Individual comparisons by ranking methods,1945, Biometrics Bulletin
 Fashion-mnist: a novel image dataset for benchmark-ing machine learning algorithms,2017, arXiv preprint arXiv:1708
 Wide residual networks,2016, CoRR
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
