title,year,conference
 Learning to continually learn,2020, arXiv preprint arXiv:2002
 Model-free episodic control,2016, arXiv preprintarXiv:1606
 Empirical evaluation ofgated recurrent neural networks on sequence modeling,2014, arXiv preprint arXiv:1412
 Rl2 : Fastreinforcement learning via slow reinforcement learning,2016, arXiv preprint arXiv:1611
 Generalization of reinforcementlearners with working and episodic memory,2019, In Advances in Neural Information ProcessingSystems 
 Neural turing machines,2014, arXiv preprintarXiv:1410
 Optimizing agent behavior over long time scales by transport-ing value,1307, Nature Communications
 Adam: A method for stochastic optimization,2015, arXiv preprintarXiv:1412
 Hippocampal contributions to control: the third way,2008, In Advances inneural information processing systems
 Continuous control with deep reinforcement learning,2015, arXivpreprint arXiv:1509
 A simple neural attentive meta-learner,2017, arXiv preprint arXiv:1707
 Human-level controlthrough deep reinforcement learning,2015, Nature
 Reptile: a scalable metalearning algorithm,2018, arXiv preprintarXiv:1803
 Neural map: Structured memory for deep reinforcementlearning,2017, arXiv preprint arXiv:1702
 Stabilizingtransformers for reinforcement learning,2019, arXiv preprint arXiv:1910
 Efficient off-policymeta-reinforcement learning via probabilistic context variables,2019, In International conference onmachine learning
 Meta-learning with memory-augmented neural networks,2016, In International conference on machinelearning
 Proximal policyoptimization algorithms,2017, arXiv preprint arXiv:1707
 Mastering the game of go withouthuman knowledge,2017, Nature
 Learning to learn: Introduction and overview,1998, In Learning to learn
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 A perspective view and survey of meta-learning,2002, Artificialintelligence review
 Matching networks for oneshot learning,2016, In Advances in neural information processing systems
 Learning to reinforcement learn,2016, arXivpreprint arXiv:1611
 Unsupervised predictivememory in a goal-directed agent,2018, arXiv preprint arXiv:1803
 The trial horizon is 80,1000, The memory bank stores 8trajectories at most for each level
