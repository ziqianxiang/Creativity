title,year,conference
 A convergence theory for deep learning via over-parameterization,2018, arXiv preprint arXiv:1811
 Federated meta-learning for recommenda-tion,2018, arXiv preprint arXiv:1802
 On the convergence of local descent methods in federatedlearning,2019, arXiv preprint arXiv:1910
 Faster on-device training usingnew federated momentum algorithm,2020, arXiv preprint arXiv:2002
 Acceleratingstochastic gradient descent,2017, In Proc
 A linear speedup analysis of distributed deep learning with sparse andquantized communication,2018, In Advances in Neural Information Processing Systems
 Advancesand open problems in federated learning,2019, arXiv preprint arXiv:1912
 Scaffold: Stochastic controlled averaging for on-device federatedlearning,2019, arXiv preprint arXiv:1910
 Tighter theory for local sgd on identical and heterogeneousdata,2020, In The 23rd International Conference on Artificial Intelligence and Statistics (AISTATS 2020)
 First analysis of local gd on hetero-geneous data,2019, NeurIPS Workshop on Federated Learning for Data Privacy and Confidentiality
 On the insufficiency of existingmomentum schemes for stochastic optimization,2018, In 2018 Information Theory and ApplicationsWorkshop (ITA)
 Aunified theory of decentralized sgd with changing topology and local updates,2020, arXiv preprintarXiv:2003
 On the convergence offedavg on non-iid data,2020, ICLR
 Toward deeper understanding of noncon-vex stochastic optimization with momentum using diffusion approximations,2018, arXiv preprintarXiv:1802
 Accelerating federated learning via momentumgradient descent,2020, IEEE Transactions on Parallel and Distributed Systems
 Communication-efficientlearning of deep networks from decentralized data,2017, Proceedings of the 20 th InternationalConference on Artificial Intelligence and Statistics (AISTATS)
 Non-asymptotic analysis of stochastic approximation algorithmsfor machine learning,2011, In Advances in Neural Information Processing Systems
 Convex analysis,1970, Number 28
 Fast convergence of stochastic gradient descent under a stronggrowth condition,2013, arXiv preprint arXiv:1308
 Federated multi-tasklearning,2017, In Advances in Neural Information Processing Systems
 Local sgd converges fast and communicates little,2019, ICLR
 Cooperative sgd: A unified framework for the design and analysis ofcommunication-efficient sgd algorithms,2018, arXiv preprint arXiv:1808
 Minibatch vs local sgd for heterogeneousdistributed learning,2020, arXiv preprint arXiv:2006
 On the linear speedup analysis of communication efficientmomentum sgd for distributed non-convex optimization,2019, ICML
 Parallel restarted sgd with faster convergence and lesscommunication: Demystifying why model averaging works for deep learning,2019, In Proceedings ofthe AAAI Conference on Artificial Intelligence
 Federated accelerated stochastic gradient descent,2020, Advances in NeuralInformation Processing Systems
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
 On the convergence properties of a k-step averaging stochastic gradientdescent algorithm for nonconvex optimization,2018, IJCAI
