title,year,conference
 Once for all: Train one network and specialize it for efficientdeployment,2019, arXiv preprint arXiv:1908
 Linear modeconnectivity and the lottery ticket hypothesis,2020, In International Conference on Machine Learning
 The state of sparsity in deep neural networks,2019, arXivpreprint arXiv:1902
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Optimal brain damage,1990, In Advances in neuralinformation processing systems
 Pruning algorithms-a survey,1993, IEEE transactions on Neural Networks
 Comparing rewinding and fine-tuning inneural network pruning,2020, In International Conference on Learning Representations
 A constructive predictionof the generalization error across scales,2020, In International Conference on Learning Representations
 Efficientnet: Rethinking model scaling for convolutional neuralnetworks,2019, arXiv preprint arXiv:1905
 The initial learning rate is 0,2021,4
