title,year,conference
 Two-temperature logistic regressionbased on the tsallis divergence,2019, In The 22nd International Conference on Artificial Intelligenceand Statistics
 Spectrally-normalized margin bounds forneural networks,2017, In NeurIPS
 Frequency bias in neural networks for input of non-uniform density,2020, arXiv preprintarXiv:2003
 ImageNet: A Large-Scale HierarchicalImage Database,2009, In CVPR
 Gradient descent finds globalminima of deep neural networks,2019, In ICML
 Generalizable adversarial training via spectral normal-ization,2019, In ICLR
 On the robustness of convnets to training on noisy labels,2017, Technical report
 Training deep neural-networks using a noise adaptationlayer,2017, In ICLR
 Deep Learning,2016, MIT Press
 Semi-supervised learning by entropy minimization,2005, InNeurIPS
 Co-teaching: Robust training of deep neural networks with extremely noisy labels,2018, InNeurIPS
 Deep residual learning for image recog-nition,2016, In CVPR
 Denoising and regularization via exploiting the struc-tural bias of convolutional generators,2020, In ICLR
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In ICML
 Improving DNN robustness to adversarial attacks using Jacobianregularization,2018, In ECCV
 MentorNet: Learning data-driven curriculum for very deep neural networks on corrupted labels,2018, In ICML
 Learning deep networks from noisy labels with dropout regu-larization,2016, In ICDM
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 The unreasonable effectiveness of noisy data for fine-grainedrecognition,2016, In ECCV
 Gradient-based learning applied todocument recognition,1998, Proc
 Gradient-based learning applied todocument recognition,1998, In Proc
 Pseudo-label: The simple and efficient semi-supervised learning method for deepneural networks,2013, In ICML workshop on challenges in representation learning
 CleanNet: Transfer learning forscalable image classifier training with label noise,2018, In CVPR
 Gradient descent with early stopping isprovably robust to label noise for overparameterized neural networks,2020, In AISTATS
 Learning fromnoisy labels with distillation,2017, In ICCV
 Dimensionality-driven learning with noisy labels,2018, In ICML
 Decoupling “when to update” from “how to update”,2017, In NeurIPS
 Spectral normalizationfor generative adversarial networks,2018, In ICLR
 A pac-bayesian approach tospectrally-normalized margin bounds for neural networks,2018, In ICLR
 A function space view of boundednorm infinite width ReLU nets: The multivariate case,2020, In ICLR
 Makingdeep neural networks robust to label noise: A loss correction approach,2017, In CVPR
 On the spectral bias of neural networks,2019, In ICML
 Training deep neural networks on noisy labels with bootstrapping,2014, arXiv preprintarXiv:1412
 Deep learning is robust to massivelabel noise,2017, arXiv preprint arXiv:1705
 The convergence rate of neuralnetworks for learned functions of different frequencies,2019, In NeurIPS
 Learning with bad training data via iterative trimmed loss mini-mization,2019, In ICML
 Robust large margin deepneural networks,2017, IEEE Transactions on Signal Processing
 Striving forsimplicity: The all convolutional net,2014, arXiv preprint arXiv:1412
 Revisiting unreasonable ef-fectiveness of data in deep learning era,2017, In ICCV
 Joint optimization framework for learning withnoisy labels,2018, In CVPR
 Combating label noise in deep learning using abstention,2019, In ICML
 Deep image prior,2017, In CVPR
 Gradient regularization improves accuracy ofdiscriminative models,2017, arXiv preprint arXiv:1712
 Symmetric crossentropy for robust learning with noisy labels,2019, In ICCV
 Learning from massive noisylabeled data for image classification,2015, In CVPR
 LDMI : A novel information-theoretic lossfunction for training deep nets robust to label noise,2019, In NeurIPS
 Training behavior of deep neural network infrequency domain,2019, In ICONIP
 Deep learning from noisyimage labels with quality embedding,2018, IEEE Trans
 Spectral norm regularization for improving the generalizabilityof deep learning,2017, arXiv preprint arXiv:1705
 mixup: Beyond empiricalrisk minimization,2018, In ICLR
 Generalized cross entropy loss for training deep neural networkswith noisy labels,2018, In NeurIPS
