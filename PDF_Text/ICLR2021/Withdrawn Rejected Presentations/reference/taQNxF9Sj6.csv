title,year,conference
 Training deep nets with sublinearmemory cost,2016, arXiv
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Reformer: The efficient transformer,2019, InInternational Conference on Learning Representations
 FlauBERT: UnsUPervisedlanguage model pre-training for French,2020, In LREC
 BioBERT: a Pre-trained biomedical language rePresentation model for biomedicaltext mining,2020, Bioinformatics
 Roberta: A robustly oPtimized bert PretrainingaPProach,2019, arXiv preprint arXiv:1907
 Pointer sentinel mixturemodels,2016, arXiv preprint arXiv:1609
 Blockwiseself-attention for long document understanding,2019, arXiv preprint arXiv:1911
 Med-BERT: pre-trained contextual-ized embeddings on large-scale structured electronic health records for disease prediction,2020, arXiv
 Efficient content-based sparseattention with routing transformers,2020, arXiv preprint arXiv:2003
 Synthesizer:Rethinking self-attention in transformer models,2020, arXiv
 Efficient transformers: A survey,2020, arXivpreprint arXiv:2009
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Huggingface's transformers:State-of-the-art natural language processing,2019, ArXiv
