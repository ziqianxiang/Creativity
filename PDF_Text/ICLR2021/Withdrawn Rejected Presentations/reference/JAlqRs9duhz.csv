title,year,conference
 Language models are few-shot learners,2020, arXiv
 Long short-term memory,0899, Neural ComPut
 The curious case of neural textdegeneration,2020, In International Conference on Learning RePresentations
 Stochastic beams and where to find them:The Gumbel-toP-k trick for samPling sequences without rePlacement,2019, In Kamalika Chaudhuriand Ruslan Salakhutdinov (eds
 A hierarchical aPProach forgenerating descriPtive image ParagraPhs,2017, In ComPuter Vision and Patterm Recognition (CVPR)
 Rouge: A Package for automatic evaluation of summaries,2004, In Proc
 Learning word vectors for sentiment analysis,2011, In Proceedings of the 49th Annual Meetingof the Association for ComPutational Linguistics: Human Language Technologies
 Building a large annotatedcorPus of English: The Penn Treebank,1993, ComPutational Linguistics
 Training for diversity in image ParagraPhcaPtioning,2018, In Proceedings of the 2018 Conference on EmPirical Methods in Natural LanguageProcessing
 Pointer sentinel mixturemodels,2017, In ICLR
 Abstrac-tive text summarization using sequence-to-sequence RNNs and beyond,2016, In Proceedings of The20th SIGNLL Conference on ComPutational Natural Language Learning
 A deep reinforced model for abstractivesummarization,2018, In ICLR
 Get to the point: Summarization withpointer-generator networks,1073, In Proceedings of the 55th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers)
 Neural machine translation of rare words withsubword units,2016, In Proceedings of the 54th Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers)
 Association for Computational Linguistics,1331, doi:10
 Modeling coverage forneural machine translation,2016, In Proceedings of the 54th Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Papers)
 Attention is all you need,2017, In I
 Cider: Consensus-based imagedescription evaluation,2015, In Proceedings of the IEEE conference on computer vision and patternrecognition
 A learning algorithm for continually running fully recurrent neuralnetworks,1989, Neural Computation
 Prophetnet: Predicting future n-gram for sequence-to-sequence pre-training,2020, arXivpreprint arXiv:2001
 Association for Computational Linguistics,1053, doi:10
