title,year,conference
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, In International Conference on Machine Learning (ICML)
 A converge analysis of gradient descentfor deep linear neural networks,2019, In International Conference on Learning Representations (ICLR)
 Implicit regularization in deep matrixfactorization,2019, In Advances in Neural Information Processing Systems (NeurIPS)
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, In InternationalConference in Machine Learning (ICML)
 Algorithmic regularization in learning deep homogeneousmodels: Layers are automatically balanced,2018, In Advances in Neural Information Processing Systems(NeurIPS)
 Gradient descent provably optimizesover-parameterized neural networks ,2019, In International Conference on Learning Representations(ICLR)
 Implicit regularization of discrete gradientdynamics in linear neural networks,2019, In Advances in Neural Information Processing Systems(NeurIPS)
 Characterizing implicit bias interms of optimization geometry,2018, In Internation Conference on Machine Learning (ICML)
 Implicit bias of gradient descent onlinear convolutional networks,2018, In Advances in Neural Information Processing Systems (NeurIPS)
 Gradient descent aligns the layers of deep linear networks,2018, InInternational Conference on Learning Representations (ICLR)
 Gradient descent convergesto minimizers,2016, In Conference on Learning Theory (COLT)
 Algorithmic regularization in over-parameterizedmatrix sensing and neural networks with quadratic activations,2018, In Conference On Learning Theory(COLT)
 Memorization in overpa-rameterized autoencoders,2019, In ICML Workshop on Identifying and Understanding Deep LearningPhenomena
 Exact solutions to the nonlineardynamics of learning in deep linear neural networks,2014, In International Conference on LearningRepresentations (ICLR)
 A mathematical theory of semanticdevelopment in deep neural networks,0027, Proceedings of the National Academy of Sciences
 Exponential convergence time of gradient descent for one-dimensional deep linearneural networks,2018, arXiv preprint arXiv:1809
 Understand-ing deep learning requires rethinking generalization,2017, In International Conference on LearningRepresentations (ICLR)
 Identity crisis: Memorization andgeneralization under extreme overparameterization,2020, In International Conference on LearningRepresentations (ICLR)
 Stochastic gradient descent optimizesover-parameterized deep relu networks,2018, arXiv preprint arXiv:1811
