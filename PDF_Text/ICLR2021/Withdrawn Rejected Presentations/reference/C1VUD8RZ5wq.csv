title,year,conference
 Qsgd:Communication-efficient sgd via gradient quantization and encoding,2017, In Advances in NeuralInformation Processing Systems
 Stochastic gradient push fordistributed deep learning,2019, volume 97 of Proceedings of Machine Learning Research
 signsgd:Compressed optimisation for non-convex problems,2018, arXiv preprint arXiv:1802
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Bag of tricks forimage classification with convolutional neural networks,2019, In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition
 AdaScale SGD: A user-friendlyalgorithm for distributed training,2020, In International Conference on Machine Learning
 Scaling laws for neural languagemodels,2020, arXiv preprint arXiv:2001
 Adam: A method for stochastic optimization,2015, In InternationalConference on Learning Representations
 Gshard: Scaling giant models with conditionalcomputation and automatic sharding,2020, arXiv preprint arXiv:2006
 Can decentralizedalgorithms outperform centralized algorithms? a case study for decentralized parallel stochasticgradient descent,2017, In Advances in Neural Information Processing Systems
 Automatic differentiation inpytorch,2017, 2017
 Megatron-LM: Training multi-billion parameter language models using GPU modelparallelism,2019, arXiv preprint arXiv:1909
 Local SGD converges fast and communicates little,2018, arXiv preprintarXiv:1805
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Cooperative SGD: A unified framework for the design and analysisof communication-efficient SGD algorithms,2018, arXiv preprint arXiv:1808
 SlowMo: Improvingcommunication-efficient distributed SGD with slow momentum,2020, In International Conferenceon Learning Representations
 Parallel restarted sgd with faster convergence and lesscommunication: Demystifying why model averaging works for deep learning,2019, In Proceedings ofthe AAAI Conference on Artificial Intelligence
 Deep mutual learning,2018, In IEEEConf
