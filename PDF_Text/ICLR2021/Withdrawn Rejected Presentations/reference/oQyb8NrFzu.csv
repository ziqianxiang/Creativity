title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 Stronger generalization bounds fordeep nets via a compression approach,2018, In Proceedings of the 35th International Conference onMachine Learning
 Spectrally-normalized margin bounds forneural networks,2017, In Advances in Neural Information Processing Systems
 Entropy-sgd: Biasing gradient descentinto wide valleys,2017, In ICLR
 Stability and convergence trade-off of iterative optimizationalgorithms,2018, arXiv preprint arXiv:1804
 Distribution-free inequalities for the deleted and holdout errorestimates,1979, IEEE Transactions on Information Theory
 Gradient descent finds globalminima of deep neural networks,2019, In International Conference on Machine Learning
 Calibrating noise to sensitivity inprivate data analysis,2006, In Theory of cryptography conference
 High probability generalization bounds for uniformly stablealgorithms with nearly optimal rate,2019, arXiv preprint arXiv:1902
 Privacy amplification byiteration,2018, In 2018 IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS)
 Private stochastic convex optimization: optimalrates in linear time,2020, In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory ofComputing
 Escaping from saddle pointsâ€”online stochasticgradient for tensor decomposition,2015, In Conference on Learning Theory
 Simplifying neural nets by discovering flat minima,1995, InAdvances in neural information processing systems
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In Advances in neural information processing systems
 Generalization in deep learning,2017, arXivpreprint arXiv:1710
 Algorithmic stability and sanity-check bounds for leave-one-outcross-validation,1999, Neural computation
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Adam: A method for stochastic optimization,2014, In ICLR
 Rademacher processes and bounding the risk offunction learning,2000, In High dimensional probability II
 Learning multiple layers of features from tiny images,2009, 2009
 Data-dependent stability of stochastic gradient descent,2018, InInternational Conference on Machine Learning
 Visualizing the loss landscapeof neural nets,2018, In Advances in Neural Information Processing Systems
 Generalization bounds of sgld for non-convex learning: Two theoretical viewpoints,2018, volume 75 of Proceedings of Machine LearningResearch
 Uniform convergence may be unable to explain generalizationin deep learning,2019, In Advances in Neural Information Processing Systems
 Exploring general-ization in deep learning,2017, In Advances in Neural Information Processing Systems
 Making gradient descent optimal forstrongly convex stochastic optimization,2012, In Proceedings of the 29th International Coference onInternational Conference on Machine Learning
 On the convergence of adam and beyond,2018, In ICLR
 A finite sample distribution-free performance bound for localdiscrimination rules,1978, The Annals of Statistics
 Towards understanding generalization of deep learning: Perspective ofloss landscapes,2017, arXiv preprint arXiv:1706
 Bolt-ondifferential privacy for scalable stochastic gradient descent-based analytics,2017, In Proceedings of the2017 ACM International Conference on Management of Data
 Pyhessian: Neural networksthrough the lens of the hessian,2019, arXiv preprint arXiv:1912
