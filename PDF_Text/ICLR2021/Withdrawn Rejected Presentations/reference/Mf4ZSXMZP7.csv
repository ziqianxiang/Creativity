title,year,conference
 Knapsack pruning withinner distillation,2020, arXiv preprint arXiv:2002
 Aciq: Analytical clipping for integerquantization of neural networks,2018, 2018
 Zeroq:A novel zero shot quantization framework,2020, arXiv preprint arXiv:2001
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Fighting quantization bias with bias,2019, arXivpreprint arXiv:1906
 The knowledge within: Methods fordata-free model compression,2019, arXiv preprint arXiv:1912
 Quantization and training of neural networks for efficientinteger-arithmetic-only inference,2018, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Qkd: Quantization-awareknowledge distillation,2019, arXiv preprint arXiv:1911
 Quantizing deep convolutional networks for efficient inference: Awhitepaper,2018, arXiv preprint arXiv:1806
 Fixed point quantization of deep convolu-tional networks,2016, In International conference on machine learning
 Up ordown? adaptive rounding for post-training quantization,2020, arXiv preprint arXiv:2004
 Xnor-net: Imagenetclassification using binary convolutional neural networks,2016, In European conference on computervision
 Enhancements of the simple method for predictingincompressible fluid flows,1984, Numerical heat transfer
 Improving neural networkquantization without retraining using outlier channel splitting,2019, In International Conference onMachine Learning
 Dorefa-net: Train-ing low bitwidth convolutional neural networks with low bitwidth gradients,2016, arXiv preprintarXiv:1606
