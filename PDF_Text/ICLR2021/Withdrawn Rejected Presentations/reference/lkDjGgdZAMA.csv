title,year,conference
 In H,2019, Wallach
 A simple framework forcontrastive learning of visual representations,2020, arXiv preprint arXiv:2002
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Learning perceptually-aligned representations via adversarial robustness,2019, arXiv preprintarXiv:1906
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Learning deep representations by mutual information estimationand maXimization,2018, arXiv preprint arXiv:1808
 Revisiting self-supervised visual representa-tion learning,2019, In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition
 Similarity of neuralnetwork representations revisited,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Learning multiple layers of features from tiny images,2009, Handbook ofSystemic Autoimmune Diseases
 Imagenet classification with deep convolu-tional neural networks,2012, In Advances in neural information processing systems
 Towards explaining the regularization effect of initiallarge learning rate in training neural networks,2019, In H
 Insights on representational similarity in neuralnetworks with canonical correlation,2018, In S
 Representation learning with contrastive predictivecoding,2018, arXiv preprint arXiv:1807
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Ro-bustness may be at odds with accuracy,2019, In International Conference on Learning Representations
 Towardsunderstanding learning representations: To what extent do different neural networks learn thesame representation,2018, In S
 The two levels of original labels produce two tasks: C 100 and C 20,2012, Wefurther design two tasks
