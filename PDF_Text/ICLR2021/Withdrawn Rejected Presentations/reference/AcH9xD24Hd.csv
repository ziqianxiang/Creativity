title,year,conference
 Differentiable convex opti-mization layers,2019, In Advances in Neural Information Processing Systems
 Learning to learn by gradient descent by gradient descent,2016, arXiv preprintarXiv:1606
 Gradient-based optimization of hyperparameters,2000, Neural computation
 A progressive batchingL-BFGS method for machine learning,2018, arXiv preprint arXiv:1802
 Learning step size controllers for robust neural networktraining,2016, In AAAI
 Learning to learn using gradient descent,2001, InInternational Conference on Artificial Neural Networks
 Learning multiple layers of features from tiny images,2009, 2009
 Gradient-based learning applied to documentrecognition,1998, Proceedings of the IEEE
 Learning to optimize,2016, arXiv preprint arXiv:1606
 Understanding and cor-recting pathologies in the training of learned optimizers,2019, In International Conference on MachineLearning
 A linearly-convergent stochastic L-BFGS algorithm,2016, InArtificial Intelligence and Statistics
 Stochastic quasi-newton with line-search regularization,2019, arXiv preprintarXiv:1909
 Learning an adaptive learning rate schedule,2019, arXiv preprintarXiv:1909
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
 Stochastic adaptive quasi-newton methods for minimizingexpected values,2017, In International Conference on Machine Learning
