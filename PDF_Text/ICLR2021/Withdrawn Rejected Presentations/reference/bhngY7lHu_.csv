title,year,conference
 A finite time analysis of temporal differencelearning with linear fUnction approximation,2018, arXiv preprint arXiv:1806
 Openai gym,2016, arXiv preprint arXiv:1606
 Revisiting fUndamentals of experience replay,2020, arXiv preprintarXiv:2007
 Addressing fUnction approximation error inactor-critic methods,2018, arXiv preprint arXiv:1802
 The reactor: Asample-efficient actor-critic architectUre,2017, ArXiv
 Soft actor-critic: Off-policy maximUm entropydeep reinforcement learning with a stochastic actor,2018, In ICML
 Soft actor-critic algorithms and appli-cations,2018, arXiv preprint arXiv:1812
 DoUble q-learning,2010, In J
 Understanding mUlti-step deep reinforcementlearning: A systematic stUdy of the dqn target,2019, arXiv preprint arXiv:1901
 Rainbow: Combining improvements in deepreinforcement learning,2018, ArXiv
 DistribUted prioritized experience replay,2018, arXiv preprint arXiv:1803
 RecUrrent expe-rience replay in distribUted reinforcement learning,2019, 05 2019
 ContinUoUscontrol with deep reinforcement learning,2016, CoRR
 HUman-level control throUgh deep reinforcement learning,2015, Nature
 Safe and efficient off-policyreinforcement learning,2016, In NIPS
 Eligibility traces for off-policy policy evalUation,2000, InICML
 High-dimensional continUoUs control Using generalized advantage estimation,2015, arXiv preprintarXiv:1506
 Reinforcement learning: An introduction,2018, MIT press
 Issues in using function approximation for reinforcement learning,1999, 1999
 Deep reinforcement learning with double q-learning,2015, arXiv preprint arXiv:1509
