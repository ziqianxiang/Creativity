title,year,conference
 Skip RNN:learning to skip state updates in recurrent neural networks,2017, CoRR
 What does bert look at?an analysis of bert’s attention,2019, In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzingand Interpreting Neural Networksfor NLP
 Funnel-transformer: Filtering out sequentialredundancy for efficient language processing,2020, arXiv preprint arXiv:2006
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Accelerating sparse dnn models without hardware-supportvia tile-wise sparsity,2020, arXiv preprint arXiv:2008
 Permitted and forbidden sets in symmetric threshold-linear networks,2001, In Advances in neural information processing systems
 Long short-term memory,1997, Neural computation
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In International Conference on Machine Learning
 Triviaqa: A large scale distantlysupervised challenge dataset for reading comprehension,2017, In Proceedings of the 55th AnnualMeeting of the Association for Computational Linguistics (Volume 1: Long Papers)
 Reformer: The efficient transformer,2019, InInternational Conference on Learning Representations
 Roberta: A robustly optimized bert pretrainingapproach,2019, 2019
 Learning representations byback-propagating errors,1986, nature
 Newsqa: A machine comprehension dataset,2016, 2016
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Q-bert: A bert-based framework for computing sparql similarityin natural language,2020, In Companion Proceedings of the Web Conference 2020
 Huggingface’s transformers: State-of-the-artnatural language processing,2019, ArXiv
 Lite transformer with long-short rangeattention,2019, In International Conference on Learning Representations
 Big bird: Transformers for longersequences,2020, arXiv preprint arXiv:2007
 Bert loses patience:Fast and robust inference with early exit,2020, arXiv
