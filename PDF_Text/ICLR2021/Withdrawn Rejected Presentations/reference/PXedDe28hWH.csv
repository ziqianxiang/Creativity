title,year,conference
 Learning to learn by gradient descent by gradientdescent,2016, In Advances in neural information processing Systems
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Empirical bayes transductive meta-learning with synthetic gradients,2020, arXivpreprint arXiv:2004
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Learning to optimize,2016, arXiv preprint arXiv:1606
 Adaptive gradient methods with dynamicbound of learning rate,2019, arXiv preprint arXiv:1902
 Optimization as a model for few-shot learning,2017, In ICLR
 Meta-learning for semi-supervised few-shot classifica-tion,2018, arXiv preprint arXiv:1803
 Learning tolearn by zeroth-order oracle,2019, arXiv preprint arXiv:1910
 Deep reinforcement learningwith smooth policy,2020, arXiv preprint arXiv:2003
 Matching networks for oneshot learning,2016, In Advances in neural information processing Systems
 Generalizing from a few examples:A survey on few-shot learning,2019, arXiv preprint arXiv: 1904
 Understanding short-horizon bias instochastic meta-optimization,2018, arXiv preprint arXiv:1803
 Improved adversarial training via learned optimizer,2020, arXivpreprint arXiv:2004
