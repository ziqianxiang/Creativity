title,year,conference
 ProxylessNAS: Direct neural architecture search on targettask and hardware,2019, In International Conference on Learning Representations
 Training deep nets with sublinearmemory cost,2016, arXiv preprint arXiv:1604
 Improved regularization of convolutional neural networkswith cutout,2017, arXiv preprint arXiv:1708
 Pipedream: Fast and efficient pipeline parallel dnn training,2018, arXivpreprint arXiv:1806
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 torchgpipe: On-the-fly pipeline parallelism for training giantmodels,2020, arXiv preprint arXiv:2004
 On modelparallelization and scheduling strategies for distributed machine learning,2014, In Advances in neuralinformation processing systems
 DARTS: Differentiable architecture search,2019, InInternational Conference on Learning Representations
 Automatic differentiation inpytorch,2017, 2017
 Efficient neural architecturesearch via parameters sharing,2018, In Proceedings of the 35th International Conference on MachineLearning
 Large-scale evolution of image classifiers,2017, arXiv preprintarXiv:1703
 Regularized evolution for imageclassifier architecture search,2019, In Proceedings of the AAAI conference on artificial intelligence
 Horovod: fast and easy distributed deep learning in tensor-flow,2018, arXiv preprint arXiv:1802
 Alphax: explor-ing neural architectures with deep neural networks and monte carlo tree search,2019, arXiv preprintarXiv:1903
 PC-DARTS: Partial channel connections for memory-efficient architecture search,2020, In InternationalConference on Learning Representations
 Efficient neural architecture search viaproximal iterations,2020, In AAAI
 Neural architecture search with reinforcement learning,2016, arXiv preprintarXiv:1611
 Learning transferable architecturesfor scalable image recognition,2018, In Proceedings of the IEEE conference on computer vision andpattern recognition
