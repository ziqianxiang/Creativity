title,year,conference
 Compressing gans using knowledge distillation,2019, CoRR
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 Scalable methods for 8-bit trainingof neural networks,2018, CoRR
 On the efficacy of knowledge distillation,2019, In Proceedingsof the IEEE International Conference on Computer Vision
 Empirical evaluation ofgated recurrent neural networks on sequence modeling,2014, arXiv preprint arXiv:1412
 The lottery ticket hypothesis: Training pruned neural net-works,2018, arXiv preprint arXiv:1803
 Non-autoregressiveneural machine translation,2017, arXiv preprint arXiv:1711
 On calibration of modern neuralnetWorks,2017, CoRR
 Identity mappings in deep residualnetWorks,2016, In European conference on computer vision
 A com-prehensive overhaul of feature distillation,2019, In Proceedings of the IEEE International Conferenceon Computer Vision
 KnoWledge transfer via distillationof activation boundaries formed by hidden neurons,2019, In Proceedings of the AAAI Conference onArtificial Intelligence
 Distilling the knoWledge in a neural netWork,2015, arXivpreprint arXiv:1503
 Long short-term memory,1997, Neural computation
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, CoRR
 Paraphrasing complex network: Network compres-sion via factor transfer,2018, In Advances in Neural Information Processing Systems
 Efficient model for image classification withregularization tricks,2020, volume 123 of Proceedings of Machine Learning Research
 Sequence-level knowledge distillation,2016, arXiv preprintarXiv:1606
 Few-shot learning of neural networks from scratch by pseudo example optimization,2018, arXiv preprintarXiv:1802
 Few sample knowledge distillation forefficient network compression,2018, arXiv preprint arXiv:1812
 Towards explaining the regularization effect of initial largelearning rate in training neural networks,2019, In Advances in Neural Information Processing Systems
 Data-free knowledge distillation for deepneural networks,2017, arXiv preprint arXiv:1710
 Self-distillation amplifies regularizationin hilbert space,2020, arXiv preprint arXiv:2002
 Parameter efficient training of deep convolutional neural networksby dynamic sparse reparameterization,2019, CoRR
 Zero-shot knowledge distillation in deep networks,2019, arXiv preprintarXiv:1905
 Fitnets: Hints for thin deep nets,2014, arXiv preprint arXiv:1412
 Knowledge transfer with jacobian matching,2018, arXiv preprintarXiv:1803
 Rethink-ing the inception architecture for computer vision,2016, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Efficientnet: Rethinking model scaling for convolutional neuralnetworks,2019, arXiv preprint arXiv:1905
 Under-standing and improving knowledge distillation,2020, arXiv preprint arXiv:2002
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Knowledge extraction with no observabledata,2019, In Advances in Neural Information Processing Systems
 Paying more attention to attention: Improving the perfor-mance of convolutional neural networks via attention transfer,2016, arXiv preprint arXiv:1612
 Wide residual networks,2016, arXiv preprintarXiv:1605
 Improving neural networkquantization without retraining using outlier channel splitting,2019, In International Conference onMachine Learning
 Understanding knowledge distillation in non-autoregressive machine translation,2019, arXiv preprint arXiv:1911
