title,year,conference
 Second-order stochastic optimization for machinelearning in linear time,2016, arXiv preprint arXiv:1602
 Deep variational informationbottleneck,2016, arXiv:1612
 Memory-efficient adaptive optimizationfor large-scale learning,2019, arXiv preprint arXiv:1901
 Some informational aspects of visual perception,1954, Psychol
 On bayesian bounds,2006, In Proceedings of the 23rd international conference onMachine learning
 Possible principles underlying the transformations of sensory messages,1961, SensoryCommunication
 Learners that uselittle information,2017, arXiv PrePrint arXiv:1710
 The “independent components” of natural scenes areedge filters,1997, Vision Research
 Aprogressive batching l-BFGS method for machine learning,2018, In Jennifer Dy and Andreas Krause(eds
 Optimization methods for large-scale machinelearning,2018, SIAM Review
 Convex OPtimization,0521, Cambridge University Press
 Principal component analysis,2014, Analytical Methods
 The convergence of a class of double-rank minimization algorithms 2,1970, The newalgorithm
 A Stochastic Quasi-Newton Method for Large-ScaleOptimization,2014, arXiv PrePrint arXiv:1401
 Natural neuralnetworks,2015, In C
 Asymptotics of wide networks from feynman diagrams,2020, ArXiv
 Why neurons mix: high dimensionality for highercognition,2016, Current Opinion in Neurobiology
 Relative information loss in the pca,2012, In 2012 IEEE InformationTheory Workshop
 Color enhancement of highly correlatedimages,1986, i
 A family of variable-metric methods derived by variational means,1970, Mathematics ofcomputation
 A kronecker-factored approximate fisher matrix for convolutionlayers,2016, arXiv preprint arXiv:1602
 Shampoo: Preconditioned stochastic tensor optimiza-tion,2018, CoRR
 Fast probabilistic optimization from noisy gradients,2013, International Conference on MachineLearning
 Natural Image Statistics: A ProbabilisticApproach to Early Computational Vision,2009, Springer Publishing Company
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Caveats for information bottleneck indeterministic scenarios,2018, arXiv preprint arXiv:1808
 Classical invariant theory: a primer,1996, 1996
 Efficient backprop,1998, InNeural Networks
 Wide neural networks of any depth evolve as linear modelsunder gradient descent,2019, In Advances in neural information processing Systems
 Finite versus infinite neural networks:an empirical study,2020, in preparation
 The winograd schema challenge,2012, In 13thInternational Conference on the Principles of Knowledge Representation and Reasoning
 Block mean approximation forefficient second order optimization,2018, ArXiv
 Deep learning via Hessian-free optimization,2010, In Proceedings of the 27th InternationalConference on Machine Learning (ICML)
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine learning
 Kronecker-factored curvature approximations forrecurrent neural networks,2018, In International Conference on Learning Representations
 fMRIpattern classification using neuroanatomically constrained boosting,2006, NeuroImage
 Scalable and practical natural gradientfor large-scale deep learning,2020, IEEE Transactions on Pattern Analysis and Machine Intelligence
 Resurrecting the sigmoid in deeplearning through dynamical isometry: theory and practice,2017, In Advances in neural informationprocessing systems
 On the spectral bias of neural networks,2018, arXiv preprintarXiv:1806
 The convergence rate of neural net-works for learned functions of different frequencies,2019, In H
 Exact solutions to the nonlineardynamics of learning in deep linear neural networks,2014, In Yoshua Bengio and Yann LeCun (eds
 A stochastic quasi-Newton method for online convexoptimization,2007, AIstats
 Information in infinite ensembles of infinitely-wideneural networks,2019, arXiv preprint arXiv:1911
 Measuring the effects of data parallelism on neural network training,2018, arXivpreprint arXiv:1811
 Conditioning of quasi-Newton methods for function minimization,1970, Mathematics ofcomputation
 Rigorous quantitative sciences integration - the foundation of high-dimensional genomicresearch,2012, Clinical and Experimental Metastasis
 Natural image statistics and neural representation,2001, AnnualReview of Neuroscience
 Fast large-scale optimization by unifyingstochastic gradient and quasi-newton methods,2014, In International Conference on Machine Learning
 Variable metricstochastic approximation theory,2009, arXiv preprint arXiv:0908
 Lecture 6,2012,5—RmsProp: Divide the gradient by a running average of itsrecent magnitude
 Deep learning and the information bottleneck principle,2015, In 2015IEEE Information Theory Workshop (ITW)
 Krylov subspace descent for deep learning,2011, arXiv preprintarXiv:1111
 High-Dimensional Statistics: A Non-Asymptotic Viewpoint,2019, Cambridge Seriesin Statistical and Probabilistic Mathematics
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
 Wide residual networks,2016, CoRR
 ADADELTA: an adaptive learning rate method,2012, CoRR
 Block-diagonal hessian-freeoptimization for training neural networks,2017, CoRR
 33 and 34 lead to different network evolution due to the non-constancy of the NTKand the higher order terms in the learning rate,2020, For wide neural networks
