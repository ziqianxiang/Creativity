title,year,conference
 Unsupervisedlabel noise modeling and loss correction,2019, arXiv preprint arXiv:1904
 There are many con-sistent explanations of unlabeled data: Why you should average,2019, In International Conference onLearning Representations
 Mixmatch: A holistic approach to semi-supervised learning,2019, In NeurIPS
 Remixmatch: Semi-supervised learning with distribution matching and augmenta-tion anchoring,2020, In International Conference on Learning Representations
 Vicinal risk minimization,2001, InT
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Maximum l q -likelihood estimation,2010, Ann
 Training deep neural-networks using a noise adaptationlayer,2016, 2016
 Deep residual learning for image recog-nition,2016, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 Using trusted data to traindeep networks on labels corrupted by severe noise,2018, In S
 Augmix: A simple method to improve robustness and uncertainty under data shift,2020, InInternational Conference on Learning Representations
 Deep bilevel learning,2018, In Proceedings of the European Conferenceon Computer Vision (ECCV)
 Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels,2018, In International Conferenceon Machine Learning
 Learning multiple layers of features from tiny images,2009, 2009
 Fixmatch: Simplifying semi-supervised learningwith consistency and confidence,2020, In NeurIPS
 Temporal ensembling for semi-supervised learning,2017, In InternationalConference on Learning Representations
 Pseudo-label : The simple and efficient semi-supervised learning method for deepneural networks,2013, ICML 2013 Workshop : Challenges in Representation Learning (WREPL)
 Robust inference viagenerative classifiers for handling noisy labels,2019, volume 97 of Proceedings of Machine LearningResearch
 Learning to learn from noisy la-beled data,2019, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Dividemix: Learning with noisy labels as semi-supervised learning,2020, In International Conference on Learning Representations
 Learning fromnoisy labels with distillation,2017, pp
 Learning withnoisy labels,2013, In C
 Self: Learning to filter noisy labels with self-ensembling,2020, In International Conference on Learning Representations
 SELFIE: Refurbishing unclean samples for robustdeep learning,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 On the importance of initialization and momentumin deep learning,2013, 30th International Conference on Machine Learning
 Mean teachers are better role models: Weight-averaged con-sistency targets improve semi-supervised deep learning results,2017, In I
 Combating label noise in deep learning using abstention,2019, arXiv preprint arXiv:1905
 mixup: Beyond empiri-cal risk minimization,2018, In International Conference on Learning Representations
 Generalized cross entropy loss for training deep neural networkswith noisy labels,2018, In S
 Improving the robustness of deep neural networksvia stability training,2016, In 2016 IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
