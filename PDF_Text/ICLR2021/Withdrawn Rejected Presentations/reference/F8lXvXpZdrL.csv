title,year,conference
 Mirrordescent view for neural network quantization,2019, arXiv preprint arXiv:1910
 An empirical study of bi-nary neural networksâ€™ optimisation,2019, In International Conference on Learning Representations
 Estimating or propagating gradients through StoChas-tic neurons for conditional computation,2013, arXiv preprint arXiv:1308
 Pseudo-Boolean optimization,2002, Discrete Applied Mathematics
 Variational deep semantic hashing for text documents,2017, In SIGIR Conferenceon Research and Development in Information Retrieval
 Straight-throughestimator as projected wasserstein gradient flow,2019, arXiv preprint arXiv:1910
 GO gradient for expectation-based objectives,2019, InInternational Conference on Learning Representations
 Pairwisesupervised hashing with Bernoulli variational auto-encoder and self-control gradient estimator,2020, ArXiv
 Backpropagation through thevoid: Optimizing control variates for black-box gradient estimation,2018, In ICLR
 Practical variational inference for neural networks,2011, In NeurIPS
 Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification,2015, In ICCV
 Binarized neural net-works,2016, In Advances in neural information processing systems
 Batch normalization: Accelerating deep network training by reducinginternal covariate shift,2015, In ICML
 Categorical reparameterization with gumbel-softmax,2016, arXiv preprintarXiv:1611
 Learning algorithms from Bayesian principles,2020, August 2020
 Adam: A method for stochastic optimization,2014, CoRR
 Using very deep autoencoders for content-based image retrieval,2011, InESANN
 Fast and simple natural-gradient variational inferencewith mixture of exponential-family approximations,2019, In ICML
 Towards accurate binary convolutional neural network,2017, In Advances inNeural Information Processing Systems 30
 The concrete distribution: A continuous relaxation ofdiscrete random variables,2016, 2016
 Problem complexity and method efficiency inoptimization,1983, 1983
 Probabilistic binary neural networks,2018, arXiv preprint arXiv:1809
 Techniques for learning binary stochasticfeedforward neural networks,2015, In ICLR
 Dropout: Asimple way to prevent neural networks from overfitting,2014, JMLR
 Local expectation gradients for black box variational infer-ence,2015, In International Conference on Neural Information Processing Systems
 Binary deep neural networks for speech recognition,2017, In INTERSPEECH
 ARM: Augment-REINFORCE-merge gradient for stochastic binarynetworks,2019, In ICLR
 Understandingstraight-through estimator in training activation quantized neural nets,2019, arXiv preprint arXiv:1903
 On the convergence rate of stochastic mirror descent for nonsmooth nonconvexoptimization,2018, arXiv: Optimization and Control
 Dorefa-net: Training lowbitwidth convolutional neural networks with low bitwidth gradients,2016, arXiv preprint arXiv:1606
