title,year,conference
 Benign ovefitting in linearregression,2020, Proceedings of the National Academy of Sciences
 Training with noise is equivalent to tikhonov regularization,1995, Neural computation
 Optimization methods for large-scale machinelearning,2018, Siam Review
 Invariance reduces variance: Understanding dataaugmentation in deep learning and beyond,2019, stat
 Autoaugment:Learning augmentation strategies from data,2019, In Proceedings of the IEEE conference on computervision and pattern recognition
 Averaged least-mean-squares: Bias-variance trade-offs andoptimal sampling distributions,2015, In Artificial Intelligence and Statistics
 Improved regularization of convolutional neural networkswith cutout,2017, arXiv preprint arXiv:1708
 Population based augmentation:Efficient learning of augmentation policy schedules,2019, In International Conference on MachineLearning
 Implicit rugosityregularization via data augmentation,2019, arXiv preprint arXiv:1905
 On the training dynamics of deep networks with l_2 regular-ization,2020, arXiv preprint arXiv:2006
 The penalty imposed by ablated data aug-mentation,2020, arXiv preprint arXiv:2006
 A stochastic approximation method,1951, Ann
 Best practices for convolutional neural networks appliedto visual document analysis,2003, In Seventh International Conference on Document Analysis andRecognition
 A bayesian perspective on generalization and stochastic gradientdescent,2018, In International Conference on Learning Representations
 Dropout training as adaptive regularization,2013, InAdvances in neural information processing systems
 mixup: Beyond empiricalrisk minimization,2017, arXiv preprint arXiv:1710
1 and 6,2021,2
