title,year,conference
 Proxquant: Quantized neural networks via proximaloperators,2019, In International Conference on Learning Representations
 On the convergence of a class of adam-typealgorithms for non-convex optimization,2019, In International Conference on Learning Representations
 On the convergence of a class of adam-type algorithms for non-convex optimization,2019, In 7th International Conference on LearningRepresentations
 Stochastic subgradient methodconverges on tame functions,2020, Foundations of computational mathematics
 Mini-batch stochastic approximation methodsfor nonconvex stochastic composite optimization,2016, Mathematical Programming
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Adam: A method for stochastic optimization,2015, In InternationalConference on Learning Representation (ICLR)
 Proximal newton-type methods for convexoptimization,2012, In Advances in Neural Information Processing Systems
 Stochastic gradient descent for nonconvex learningwithout bounded gradient assumptions,2019, IEEE Transactions on Neural Networks and LearningSystems
 Learning sparse neural networks through l0regularization,2018, In International Conference on Learning Representations
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine learning
 Proxsarah: An efficient algorith-mic framework for stochastic composite nonconvex optimization,2019, arXiv preprint arXiv:1902
 Proximal stochastic methodsfor nonsmooth nonconvex finite-sum optimization,2016, In Advances in Neural Information ProcessingSystems
 On the convergence of adam and beyond,2018, InInternational Conference on Learning Representations
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 On the stability of inverse problems,1943, Doklady Akademii Nauk SSSR
 Spiderboost: A class of fastervariance-reduced algorithms for nonconvex optimization,2018, arXiv preprint arXiv:1810
 Learning structured sparsity in deepneural networks,2016, In Advances in neural information processing systems
 Stochastic optimization for DC functionsand non-smooth non-convex regularizers with non-asymptotic convergence,2019, In Internationalconference on machine learning
 Quantization networks,2019, In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition
 Proxsgd: Training structured neural networks under regularization and constraints,2020, InInternational Conference on Learning Representations
 Stochastic gradient methods with block diagonalmatrix adaptation,2019, arXiv preprint arXiv:1905
 Adaptive methodsfor nonconvex optimization,2018, In Advances in neural information processing systems
 Nearly unbiased variable selection under minimax concave penalty,2010, The Annalsof statistics
 Fast convergence of natural gradient descentfor over-parameterized neural networks,2019, In Advances in Neural Information Processing Systems
