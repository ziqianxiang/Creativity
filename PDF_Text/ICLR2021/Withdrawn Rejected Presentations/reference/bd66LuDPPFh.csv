title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 Label refinery:Improving imagenet classification through label progression,2018, arXiv preprint arXiv:1805
 Stability and generalization of learning algorithmsthat converge to global optima,2018, In International Conference on Machine Learning
 Towards better decoding and language model integration insequence to sequence models,2017, Proc
 Adaptive regularization oflabels,2019, arXiv preprint arXiv:1908
 Incorporating nesterov momentum into adam,2016, 2016
 Accelerated gradient methods for nonconvex nonlinear andstochastic programming,2016, Math
 Mini-batch stochastic approximation meth-ods for nonconvex stochastic composite optimization,2016, Mathematical Programming
 Bag of tricks forimage classification with convolutional neural networks,2019, In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition
 Neural networks for machine learninglecture 6a overview of mini-batch gradient descent,2012, 2012a
 Distilling the knowledge in a neural network,2014, InNeurIPS Deep Learning Workshop
 Improving neural networks by preventing co-adaptation of feature detectors,2012, arXiv preprintarXiv:1207
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Novel dataset for fine-grained image categorization: Stanford dogs,2011, In Proc
 Adam: A method for stochastic optimization,2015, In InternationalConference on Learning Representations
 Exponential step sizes for non-convex opti-mization,2020, arXiv preprint arXiv:2002
 Colam: Co-learning of deep neural networks and soft labels via alternating minimization,2020, arXiv preprintarXiv:2004
 A simple proximal stochastic gradient method for nonsmooth nonconvexoptimization,2018, In Advances in Neural Information Processing Systems
 Transformers without tears: Improving the normalization ofself-attention,2019, arXiv preprint arXiv:1910
 Towards robust detection of adversarial exam-ples,2018, In Advances in Neural Information Processing Systems
 Regularizingneural networks by penalizing confident output distributions,2017, arXiv preprint arXiv:1701
 Gradient methods for minimizing functionals,1963, Zhurnal Vychislitel’noiMatematiki i Matematicheskoi Fiziki
 On the convergence of adam and beyond,2018, InInternational Conference on Learning Representations
 Training deep neural networks on noisy labels with bootstrapping,2014, arXiv preprintarXiv:1412
 Self-augmentation: Generalizing deep net-works to unseen classes for few-shot learning,2020, arXiv preprint arXiv:2004
 Defending against adversarialattacks by suppressing the largest eigenvalue of fisher information matrix,2019, arXiv preprintarXiv:1909
 Transformation invariancein pattern recognition—tangent distance and tangent propagation,1998, In Neural networks: tricks ofthe trade
 Rethink-ing the incePtion architecture for comPuter vision,2016, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 On the inference calibration of neuralmachine translation,2020, arXiv preprint arXiv:2005
 Disturblabel: Regularizingcnn on the loss layer,2016, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Revisit knowledge distillation: ateacher-free framework,2019, arXiv preprint arXiv:1909
 Stagewise training accelerates convergenceof testing error over sgd,2019, In Advances in Neural Information Processing Systems
 Wide residual networks,2016, arXiv preprintarXiv:1605
 Adadelta: an adaPtive learning rate method,2012, arXiv preprint arXiv:1212
 Improved training of end-to-end atten-tion models for sPeech recognition,2018, Proc
 mixup: Beyond empiricalrisk minimization,2018, In International Conference on Learning Representations
 Learning transferable architecturesfor scalable image recognition,2018, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
