title,year,conference
 Crf autoencoder for unsupervised dependency parsing,2017, arXivpreprint arXiv:1708
 Shared logistic normal distributions for soft parameter tyingin unsupervised grammar induction,2009, In Proceedings of Human Language Technologies: The2009 Annual Conference of the North American Chapter of the Association for ComputationalLinguistics
 Unsupervised structure prediction with non-parallel multilingual guidance,2011, In Proceedings of the 2011 Conference on Empirical Methods inNatural Language Processing
 Dependency-based self-attention fortransformer nmt,2019, In Proceedings of the International Conference on Recent Advances in NaturalLanguage Processing (RANLP 2019)
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Unsupervisedlatent tree induction with deep inside-outside recursive auto-encoders,2019, In Proceedings of the 2019Conference of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies
 Transforming a constituency treebank into adependency treebank,2005, Procesamiento del lenguaje natural
 Sparsity independency grammar induction,2010, ACL 2010
 UnsUpervised learning of syntacticstrUctUre with invertible neUral projections,2018, In Proceedings of the 2018 Conference on EmpiricalMethods in Natural Language Processing
 Improving UnsUpervised dependencyparsing with richer contexts and smoothing,2009, In Proceedings of human language technologies:the 2009 annual conference of the North American chapter of the association for computationallinguistics
 A systematic assessmentof syntactic generalization in neUral langUage models,2020, arXiv preprint arXiv:2005
 UnsUpervised neUral dependency parsing,2016, Associationfor CompUtational LingUistics (ACL)
 Are pre-trained langUage mod-els aware of phrases? simple bUt strong baselines for grammar indUction,2020, arXiv preprintarXiv:2002
 Un-sUpervised recUrrent neUral network grammars,2019, In Proceedings of the 2019 Conference of theNorth American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Syntactic structure distillation pretraining for bidirectional encoders,2020, arXivpreprint arXiv:2005
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Encoding sentences with graph convolutional networks forsemantic role labeling,2017, arXiv preprint arXiv:1703
 Statistical language models based on neural networks,2012, Presentation at Google
 Dependency-based relative positional en-coding for transformer nmt,2019, In Proceedings of the International Conference on Recent Advancesin Natural Language Processing (RANLP 2019)
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Neural language modeling byjointly learning syntax and lexicon,2018, In International Conference on Learning Representations
 Straight to the tree: Constituency parsing with neural syntactic distance,2018, In Proceedingsof the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: LongPapers)
 Unsupervised dependencyparsing without gold part-of-speech tags,2011, In Proceedings of the 2011 Conference on EmpiricalMethods in Natural Language Processing
 Breaking out of local optima with counttransforms and model recombination: A study in grammar induction,2013, 2013
 Linguistically-informed self-attention for semantic role labeling,2018, arXiv preprint arXiv:1804
 Unambiguity regularization for unsupervised learning of probabilis-tic grammars,2012, In Proceedings of the 2012 Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational Natural Language Learning
 Attention is all you need,2017, In Advances in neural informationprocessing systems
