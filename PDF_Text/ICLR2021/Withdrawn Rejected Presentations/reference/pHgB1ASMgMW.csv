title,year,conference
 Obfuscated gradients give a false sense of se-curity: Circumventing defenses to adversarial examples,2018, In International Conference on MachineLearning
 Wild patterns: Ten years after the rise of adversarial machinelearning,2018, Pattern Recognition
 Towards evaluating the robustness of neural networks,2017, In 2017ieee symposium on security and privacy (sp)
 Maximum-entropy fine grainedclassification,2018, In Advances in Neural Information Processing Systems
 Adversarially robust distillation,2020, InProceedings of the AAAI Conference on Artificial Intelligence
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Towards end-to-end speech recognition with recurrent neuralnetworks,2014, In International Conference on Machine Learning
 Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor,2018, arXiv preprintarXiv:1801
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Adversarial logit pairing,2018, arXiv preprintarXiv:1803
 Improving adversarial robustness viapromoting ensemble diversity,2019, In International Conference on Machine Learning
 Distillation as adefense to adversarial perturbations against deep neural networks,2016, In 2016 IEEE Symposium onSecurity and Privacy (SP)
 Practical black-box attacks against machine learning,2017, In Proceedings of the 2017 ACMon Asia conference on computer and communications security
 Regularizingneural networks by penalizing confident output distributions,2017, arXiv preprint arXiv:1701
 Overfitting in adversarially robust deep learning,2020, arXivpreprint arXiv:2002
 Improving the generalization ofadversarial training with domain adaptation,2018, arXiv preprint arXiv:1810
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Rethink-ing the inception architecture for computer vision,2016, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Lipschitz-margin training: Scalable certifi-cation of perturbation invariance for deep neural networks,2018, In Advances in neural informationprocessing systems
 Adversarial examples: Attacks and defenses fordeep learning,2019, IEEE transactions on neural networks and learning systems
 Wide residual networks,2016, arXiv preprintarXiv:1605
 Attacks which do not kill training make adversarial learning stronger,2020, arXiv preprintarXiv:2002
 Maximum entropy inversereinforcement learning,2008, In Aaai
