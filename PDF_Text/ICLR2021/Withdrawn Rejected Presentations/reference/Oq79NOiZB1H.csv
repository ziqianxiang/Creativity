title,year,conference
 Optimization methods for large-scale machinelearning,2018, SiamReview
 Graphnorm: A prin-cipled approach to accelerating graph neural network training,2020, arXiv preprint arXiv:2009
 Stochastic training of graph convolutional networks withvariance reduction,2017, arXiv preprint arXiv:1710
 Fastgcn: fast learning with graph convolutional networks viaimportance sampling,2018, arXiv preprint arXiv:1801
 On generalization bounds ofa family of recurrent neuralnetworks,2019, arXiv preprint arXiv:1910
 Solving stochastic compositional optimization is nearlyas easy as solving stochastic optimization,2020, arXiv preprint arXiv:2008
 Cluster-gcn:An efficient algorithm for training deep and large graph convolutional networks,2019, arXiv preprintarXiv:1905
 Spider: Near-optimal non-convex op-timization via stochastic path-integrated differential estimator,2018, In Advances in Neural InformationProcessing Systems
 Protein interface prediction using graphconvolutional networks,2017, In NIPS
 Generalization and representational limits ofgraph neural networks,2020, arXiv preprint arXiv:2002
 Recovering low-rank matrices from few coefficients in any basis,2011, IEEE Transactionson Information Theory
 Biased stochastic gradient descent for conditionalstochastic optimization,2020, arXiv preprint arXiv:2002
 Adaptive sampling towards fast graphrepresentation learning,2018, In Advances in Neural Information Processing Systems
 Semi-supervised classification with graph convolutional net-works,2016, arXiv preprint arXiv:1609
 Stochastic variancereduction for nonconvex optimization,2016, In International conference on machine learning
 Modeling relational data with graph convolUtional networks,2018, In European Semantic WebConference
 Hybrid variance-redUced sgd algorithms fornonconvex-concave minimax problems,2020, arXiv preprint arXiv:2006
 Knowledge graph embedding: A sUrvey ofapproaches and applications,2017, IEEE Transactions on Knowledge and Data Engineering
 Graph-saint: Graph sampling based indUctive learning method,2019, arXiv preprint arXiv:1907
 A composite randomized incremental gradient method,2019, In InternationalConference on Machine Learning
 MUlti-level composite stochastic optimization via nested variance re-dUction,2019, arXiv preprint arXiv:1908
 A stochastic composite gradient method with incremental varianceredUction,2019, In Advances in Neural Information Processing Systems
 Efficientprobabilistic logic reasoning with graph neUral networks,2020, arXiv preprint arXiv:2001
 The full-batch gradient calculation ateach snapshot step is computationally expensive,2021, Heuristically
