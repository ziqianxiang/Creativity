title,year,conference
 Threat of adversarial attacks on deep learning in computer vision:A survey,2018, IEEEAccess
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, CoRR
 Deep convolutionalnetworks do not classify based on global object shape,2018, PLoS computational biology
 Decision-based adversarial attacks: Reliableattacks against black-box machine learning models,2017, arXiv preprint arXiv:1712
 A computational approach to edge detection,1986, IEEE Transactions on pattern analysisand machine intelligence
 Normalization as a canonical neural computation,2012, NatureReviews Neuroscience
 Towards evaluating the robustness of neural networks,2017, InIEEE Symposium on Security and Privacy
 Targeted backdoor attacks on deeplearning systems using data poisoning,2017, arXiv preprint arXiv:1712
 Simulating a primary visual cortex at the front of cnns improves robustness to imageperturbations,2020, BioRxiv
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 A study and comparison of human and deep learning recogni-tion performance under visual distortions,2017, In 2017 26th international conference on computercommunication and networks (ICCCN)
 A study of the effect of JPGcompression on adversarial images,2016, CoRR
 Shortcut learning in deep neural networks,2020, arXivpreprint arXiv:2004
 Explaining and harnessing adversarialexamples,2015, In Proc
 Countering adversarialimages using input transformations,2017, CoRR
 Benchmarking neUral network robUstness to common cor-rUPtions and PertUrbations,2019, arXiv preprint arXiv:1903
 The origins and Prevalence of textUre bias inconvolUtional neUral networks,2020, Advances in Neural Information Processing Systems
 ExPloring the origins and Prevalence of textUre bias inconvolUtional neUral networks,2019, arXiv preprint arXiv:1911
 Image-to-image translation withconditional adversarial networks,2017, In Proceedings of the IEEE conference on computer vision andpattern recognition
 Adam: A method for stochastic oPtimization,2014,	CoRR
 DeeP neUral networks: a new framework for modeling biological vision andbrain information Processing,2015, Annual review of vision science
 Imagenet classification with deeP convo-lUtional neUral networks,2012, In Advances in neural information processing systems
 Gradient-based learning applied todocUment recognition,1998, Proc
 Deep learning,2015, Nature
 Learning from brains how to regularizemachines,2019, In Advances in Neural Information Processing Systems
 Trojaning attack on neural networks,2017, 2017
 Magnet: A two-pronged defense against adversarial examples,2017, InProceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security
 Deepfool: A simple andaccurate method to fool deep neural networks,2016, In Proc
 Biologically inspiredmechanisms for adversarial robustness,2020, Advances in Neural Information Processing Systems
 Defense-gan: Protecting classifiers againstadversarial attacks using generative models,2018, arXiv preprint arXiv:1805
 Pixeldefend:Leveraging generative models to understand and defend against adversarial examples,2017, arXivpreprint arXiv:1710
 One pixel attack for fooling deepneural networks,2019, IEEE Transactions on Evolutionary Computation
 Intriguing properties of neural networks,2014, In In Proc
 Spatially trans-formed adversarial examples,2018, arXiv preprint arXiv:1801
 Shape features improvegeneral model robustness,2019, 2019
 Fashion-mnist: a novel image dataset for benchmark-ing machine learning algorithms,2017, arXiv preprint arXiv:1708
 Feature denoisingfor improving adversarial robustness,2019, In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition
 Holistically-nested edge detection,2015, In IEEE International Conferenceon Computer Vision
 Feature squeezing: Detecting adversarial examples in deepneural networks,2017, CoRR
 Interpreting adversarially trained convolutional neural net-works,2019, arXiv preprint arXiv:1905
