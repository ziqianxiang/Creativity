title,year,conference
 Meta-learning with differ-entiable closed-form solvers,2018, In International Conference on Learning Representations (ICLR)
 Mathematical programs with optimization problems in theconstraints,1973, Operations Research
 Generic methods for optimization-based modeling,2012, In Artificial Intelligence andStatistics (AISTATS)
 Hyperparameter optimization,2019, In Automated Machine Learning
 Model-agnostic meta-learning for fast adaptation ofdeep networks,2017, In Proc
 Learning constrained task similarities ingraphregularized multi-task learning,2014, Regularization
 Forward and reversegradient-based hyperparameter optimization,2017, In International Conference on Machine Learning(ICML)
 Bilevelprogramming for hyperparameter optimization and meta-learning,2018, In International Conferenceon Machine Learning (ICML)
 Approximation methods for bilevel programming,2018, arXiv preprintarXiv:1802
 On differentiating parameterized argmin and argmax problems with application to bi-leveloptimization,2016, arXiv preprint arXiv:1607
 On the iteration com-plexity of hypergradient computation,2020, In Proc
 A two-timescale frameworkfor bilevel optimization: Complexity analysis and application to actor-critic,2020, arXiv preprintarXiv:2007
 Convergence of meta-learning withtask-specific adaptation over partial parameters,2020, arXiv preprint arXiv:2006
 Multi-step model-agnostic meta-learning: Convergenceand improved algorithms,2020, arXiv preprint arXiv:2002
 Adam: A method for stochastic optimization,2014, InternationalConference on Learning Representations (ICLR)
 Actor-critic algorithms,2000, In Advances in neural informationprocessing systems (NeurIPS)
 Learning multiple layers of features from tiny images,2009, 2009
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Improved bilevel model: Fast and optimal algorithm withtheoretical guarantee,2020, arXiv preprint arXiv:2009
 Reviving and improving recurrent back-propagation,2018, In Proc
 Hyperparameter learning via bilevel nons-mooth optimization,2018, arXiv preprint arXiv:1806
 Tadam: Task dependent adaptivemetric for improved few-shot learning,2018, In Advances in Neural Information Processing Systems(NeurIPS)
 Meta-learning with im-plicit gradients,2019, In Advances in Neural Information Processing Systems (NeurIPS)
 Watch and learn: Optimizing from revealedpreferences feedback,2016, In Annual ACM Symposium on Theory of Computing (STOC)
 An extended kuhn-tucker approach for linear bilevelprogramming,2005, Applied Mathematics and Computation
 Prototypical networks for few-shot learning,2017, InAdvances in Neural Information Processing Systems (NIPS)
 Matching networks for oneshot learning,2016, In Advances in Neural Information Processing Systems (NIPS)
 Adversarial attacks on graph neural networks via metalearning,2019, In International Conference on Learning Representations (ICLR)
 Using an approach similar to eq,2021, (28)
