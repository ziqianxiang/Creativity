title,year,conference
 Finding approxi-mate local minima faster than gradient descent,2017, STOC
 Beyond linearization: On quadratic and higher-order approximation ofwide neural networks,2020, ICLR
 Saving phase: Injec-tivity and stability for phase retrieval,2014, Applied and Computational Harmonic Analysis
 Convex optimization: Algorithms and complexity,2014, Foundations and Trends inMachine Learning
 Optimal rates of convergence for noisy sparse phaseretrieval via thresholded wirtinger flow,2016, The Annals of Statistics
 Accelerated linear convergence of stochasticmomentum methods in wasserstein distances,2019, ICML
 Solving quadratic equations via phaselift when there areabout as many equations as unknowns,2014, Foundations of Computational Mathematics
 Phase retrieval via wirtinger flow:Theory and algorithms,2015, IEEE Transactions on Information Theory
 Understanding accelerated stochastic gradient descent via thegrowth condition,2020, arXiv:2006
 Solving random quadratic systems of equations is nearly aseasy as solving linear systems,2017, Communications on Pure and Applied Mathematics
 Gradient descent with random ini-tialization: Fast global convergence for nonconvex phase retrieval,2018, Mathematical Programming
 Nonconvex optimization meets low-rank matrix factoriza-tion: An overview,2019, arXiv:1809
 Escaping saddles withstochastic gradients,2018, ICML
 Generalized momentum-based methods: A hamiltonianperspective,2019, arXiv:1906
 Gradientdescent can take exponential time to escape saddle points,2017, NIPS
 Sharp analysis for nonconvex sgd escaping from saddlepoints,2019, COLT
 The numerics of phase retrieval,2020, Acta Numerica
 Stochastic heavy ball,2016, arXiv:1609
 Escaping from saddle points â€” online stochasticgradient for tensor decomposition,2015, COLT
 No spurious local minima in nonconvex low rank problems: Aunified geometric analysis,2017, ICML
 Learning one-hidden-layer neural networks with landscapedesign,2019, ICLR
 Global convergence of theheavy-ball method for convex optimization,2015, ECC
 Implicit regularization of discrete gradientdynamics in linear neural networks,2019, NeurIPS
 Understanding the role of momentumin stochastic gradient methods,2019, NeurIPS
 Why momentum really works,2017, Distill
 Matrix computations,1996, Johns Hopkins University Press
 Implicit regularization in matrix factorization,2017, NIPS
 Near-optimal methods for minimizing star-convexfunctions and beyond,2020, COLT
 How to escapesaddle points efficiently,2017, ICML
 Accelerated gradient descent escapes saddlepoints faster than gradient descent,2018, COLT
 Stochastic gradientdescent escapes saddle points efficiently,2019, arXiv:1902
 On the insufficiency ofexisting momentum schemes for stochastic optimization,2018, ICLR
 Global convergence of second-orderdynamics in two-layer neural networks,2020, arXiv:2006
 Provable accelerated gradient method for nonconvex low rank opti-mization,2020, Machine Learning
 Nonconvex matrix factorization from rank-onemeasurements,2019, AISTATS
 Algorithmic regularization in over-parameterizedmatrix sensing and neural networks with quadratic activations,2018, COLT
 An improved analysis of stochastic gradient descent withmomentum,2020, arXiv:2007
 Accelerated gossip via stochastic heavy ball method,2018, Allerton
 Optimization-based amp for phase retrieval: The impact ofinitialization and l2-regularization,2018, IEEE Transactions on Information Theory
 Complex dynamics in simple neural networks: Understandinggradient flow in phase retrieval,2020, arXiv:2006
 Phase retrieval using alternating minimiza-tion,2013, NIPS
 Fast exact multiplication by the hessian,1994, Neural computation
 Convolutional phase retrieval viagradient descent,2017, NIPS
 A generic approach for escaping saddle points,2018, AISTATS
 The effects of mild over-parameterization on theoptimization landscape of shallow relu neural networks,2020, arXiv:2006
 On the convergence of the stochasticheavy ball method,2020, arXiv:2006
 Escaping saddlepoints with adaptive gradient methods,2019, ICML
 Non-ergodicconvergence analysis of heavy-ball algorithms,2019, AAAI
 On the importance of initializa-tion and momentum in deep learning,2013, ICML
 Low-ranksolutions of linear matrix equations via procrustes flow,2016, ICML
 Solving systems of random quadraticequations via truncated amplitude flow,2017, IEEE Transactions on Information Theory
 Solving most systems ofrandom quadratic equations,2017, NIPS
 Escaping saddle points faster with stochasticmomentum,2020, ICLR
 The local convexity of solving systems ofquadratic equations,2016, Results in Mathematics
 Themarginal value of adaptive gradient methods in machine learning,2017, NIPS
 Analytical convergence regions of acceleratedgradient descent in nonconvex optimization under regularity condition,2020, Automatica
 First-order stochastic algorithms for escaping from saddlepoints in almost linear time,2018, NeurIPS
 Unified convergence analysis of stochastic momentummethods for convex and non-convex optimization,2018, IJCAI
 Misspecifiednonconvex statitical optimization for sparse phase retrival,2018, Mathematical Programming
 Robust recovery via implicit bias of discrepantlearning rates for double over-parameterization,2020, arXiv:2006
 Provable non-convex phase retrieval with outliers:Median truncated wirtinger flow,2017, ICML
 A nonconvex approach for phase retrieval:Reshaped wirtinger flow and incremental algorithms,2017, JMLR
 A convergent gradient descent algorithm for rank minimizationand semidefinite programming from random linear measurements,2015, NIPS
 Geometrical properties and accelerated gradientsolvers of non-convex phase retrieval,2016, IEEE Allerton Conference on Communication
