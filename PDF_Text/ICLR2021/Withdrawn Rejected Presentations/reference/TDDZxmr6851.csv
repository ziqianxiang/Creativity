title,year,conference
  A convergence theory for deep learning via over-parameterization,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Adaptive method of realizing natural gradientlearning for multilayer perceptrons,2000, Neural computation
 Onexact computation with an infinitely wide neural net,2019, In Advances in Neural Information ProcessingSystems
 Sgd learns the conjugate kernel class of the network,2017, In Advances in Neural InformationProcessing Systems
 Gradient descent finds globalminima of deep neural networks,2019, In Proceedings of the 36th International Conference on MachineLearning
 Asymptotics of wide networks from feynman diagrams,2020, In InternationalConference on Learning Representations
 Flat minima,1997, Neural Computation
   The break-even point on optimization trajectories of deep neuralnetworks,2020, arXiv preprint arXiv:2002
  Fantas-tic generalization measures and where to find them,2020,  In International Conference on LearningRepresentations
  On large-batch training for deep learning: Generalization gap and sharp minima,2016,  CoRR
 Deep neural networks as gaussian processes,2018, In International Conference on LearningRepresentations
      Wide   neural   networks   of   any   depth   evolveas  linear  models  under  gradient  descent,2019,      In  H
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In Advances in Neural Information Processing Systems
 Simple mathematical models with very complicated dynamics,1976, Nature
   A mean field view of the landscape oftwo-layer neural networks,2018, 115(33):E7665â€“E7671
 Bayesian deep convolutional networks with many channelsare gaussian processes,2019,  In International Conference on Learning Representations
 Neural tangents: Fast and easy infinite neural networks in python,2020, InInternational Conference on Learning Representations
 The effect of network widthon stochastic gradient descent and generalization:  an empirical study,2019,  CoRR
 Parameters as interacting particles: long time convergenceand asymptotic error scaling of neural networks,2018, In Advances in neural information processingsystems
  Mean field analysis of neural networks,2018,  arXivpreprint arXiv:1805
 A bayesian perspective on generalization and stochastic gradientdescent,2018,   In International Conference on Learning Representations
  Kernel anddeep regimes in overparametrized models,2019, arXiv preprint arXiv:1906
   A diffusion theory for deep learning dynamics:Stochastic  gradient  descent  escapes  from  sharp  minima  exponentially  fast,2020,    arXiv  preprintarXiv:2002
 Wide residual networks,2016, CoRR
  Stochastic gradient descent optimizesover-parameterized deep relu networks,2018, arXiv preprint arXiv:1811
