title,year,conference
 Feature purification: How adversarial training performs robust deeplearning,2020, ArXiv
 Obfuscated gradients give a false sense of security:Circumventing defenses to adversarial examples,2018, ArXiv
 A simple proof of therestricted isometry property for random matrices,2008, Constructive Approximation
 Stable signal recovery from incomplete and inaccurate measure-ments,2005, Communications on Pure and Applied Mathematics
 On evaluating adversarial robustness,2019, ArXiv
 Adversarially robust repre-sentations with smooth encoders,2020, In ICLR
 Adversarial vulnerability for any classifier,2018, InAdvances in neural information processing systems
 Adversarial spheres,2018, arXiv preprint arXiv:1801
 Adversarial attacks on variationalautoencoders,2018, arXiv preprint arXiv:1806
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Excessive invariance causes adversarialvulnerability,2019, 2019
 Auto-encoding variational bayes,2014, CoRR
 Adversarial examples for generative models,2018, In 2018 ieeesecurity and privacy workshops (spw)
 Towards deep learningmodels resistant to adversarial attacks,2018, ArXiv
 Universal adver-sarial perturbations,2017, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Zero-shotknowledge distillation in deep networks,2019, In ICML
 Stochastic backpropagation and approx-imate inference in deep generative models,2014, In ICML
 Adversarial manipulation of deeprepresentations,2016, CoRR
 Adversarialvulnerability of neural networks increases with input dimension,2018, ArXiv
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Robustness may be at oddswith accuracy,2019, arXiv: Machine Learning
 Dataset distillation,2018, arXivpreprint arXiv:1811
 Improving vaesâ€™ ro-bustness to adversarial attack,2019, arXiv: Machine Learning
 Interpreting adversarially trained convolutional neural net-works,2019, ArXiv
