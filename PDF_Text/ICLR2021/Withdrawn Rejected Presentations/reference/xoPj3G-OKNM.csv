title,year,conference
 Stochastic gradient methods with layer-wise adaptive moments for training of deep networks,2019, CoRR
 Beyond convexity: Stochastic quasi-convexoPtimization,2015, In Advances in Neural Information Processing Systems
 DeeP residual learning for image recog-nition,2016, In Proceedings of Conference on Computer Vision and Pattern Recognition
 On large-batch training for deeP learning: Generalization gaP and sharP minima,2017, InProceedings of the International Conference on Learning Representations
 Communication efficient distributedmachine learning with the parameter server,2014, 2014a
 Efficient mini-batch training forstochastic optimization,2014, In Proceedings of the ACM Conference on Knowledge Discovery andData Mining
 Can decentralizedalgorithms outperform centralized algorithms? A case study for decentralized parallel stochasticgradient descent,2017, In Advances in Neural Information Processing Systems
 Accelerating rescaled gradient descent: Fastoptimization of smooth functions,2019, In Advances in Neural Information Processing Systems
 Large batch optimization for deeplearning: Training BERT in 76 minutes,2020, In Proceedings of the International Conference onLearning Representations
 On the linear speedup analysis of communication efficient mo-mentum SGD for distributed non-convex optimization,2019, In Proceedings of the 36th InternationalConference on Machine Learning
 Parallel restarted SGD with faster convergence and lesscommunication: Demystifying why model averaging works for deep learning,2019, In Proceedings ofthe AAAI Conference on Artificial Intelligence
 Why gradient clipping acceleratestraining: A theoretical justification for adaptivity,2020, In Proceedings of the International Conferenceon Learning Representations
