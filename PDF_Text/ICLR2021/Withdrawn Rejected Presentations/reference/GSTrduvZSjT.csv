title,year,conference
 Sgd with shuffling: optimal rates without componentconvexity and large epoch requirements,2020, Advances in Neural Information Processing Systems
 A new regret analysisfor adam-type algorithms,2020, arXiv preprint arXiv:2003
 Training neural networks for and byinterpolation,2019, arXiv preprint:1906
 On the generalization ability of on-linelearning algorithms,2004, IEEE Transactions on Information Theory
 LIBSVM: A library for sUpport vector machines,2011, ACMTransactions on Intelligent Systems and Technology
 On the convergence of a class of Adam-type algorithms for non-convex optimization,2019, In 7th International Conference on LearningRepresentations
 On the convergence of Adamand AdaGrad,2020, arXiv preprint:2003
 Global convergence ofthe heavy-ball method for convex optimization,2015, In 2015 European control conference (ECC)
 Deep learning,2016, Adaptive computation andmachine learning
 Acceleratingstochastic gradient descent for least squares regression,2018, In Conference On Learning Theory
 Adam: A method for stochastic oPtimization,2015, In 3rd InternationalConference on Learning Representations
 On the convergence of stochastic gradient descent with adaPtivestePsizes,2019, In The 22nd International Conference on Artificial Intelligence and Statistics
 On the variance of the adaPtive learning rate and beyond,2020, In 8th International Conference onLearning Representations
 Sampling: design and analysis,2009, Nelson Education
 Linearly convergent stochastic heavy ball method for minimizinggeneralization error,2017, arXiv preprint:1710
 Stochastic PolyaksteP-size for SGD: An adaPtive learning rate for fast convergence,2020, arXiv preprint:2002
 SGDR: stochastic gradient descent with warm restarts,2017, In5th International Conference on Learning Representations
 AdaPtive gradient methods with dynamicbound of learning rate,2019, In 7th International Conference on Learning Representations
 Fast and furi-ous convergence: Stochastic second order methods under interPolation,2020, In The 23nd InternationalConference on Artificial Intelligence and Statistics
 Non-asymptotic analysis of stochastic approximation algorithmsfor machine learning,2011, In Advances in Neural Information Processing Systems
 Variants of RMSProp and AdaGrad with logarithmicregret bounds,2017, In Proceedings of the 34th International Conference on Machine Learning
 A modern introduction to online learning,2019, arXiv preprint arXiv:1912
 Gradient methods for minimizing functionals,1963, Zhurnal Vychislitel’noi Matematiki iMatematicheskoi Fiziki
 On the convergence of Adam and beyond,2018, In 6thInternational Conference on Learning Representations
 L4: practical loss-based stepsize adaptation for deep learning,2018, InAdvances in Neural Information Processing Systems
 Fast convergence of stochastic gradient descent under a stronggrowth condition,2013, arXiv preprint:1308
 On the convergence of the stochastic heavyball method,2020, arXiv preprint arXiv:2006
 Escaping saddle pointswith adaptive gradient methods,2019, In Proceedings of the 36th International Conference on MachineLearning
 Fast and faster convergence of SGD for over-parameterized models and an accelerated perceptron,2019, In The 22nd International Conference onArtificial Intelligence and Statistics
 SAdam: A variant of Adamfor strongly convex functions,2020, In 8th International Conference on Learning Representations
 Global convergence of adaptive gradient methods foran over-parameterized neural network,2019, arXiv preprint:1902
 ADADELTA: an adaptive learning rate method,2012, arXiv preprint:1212
 Understandingdeep learning requires rethinking generalization,2017, In 5th International Conference on LearningRepresentations
 On the convergence of adaptivegradient methods for nonconvex optimization,2018, arXiv preprint:1808
 The starting point is the quadratic inequality x2 - ax - ab ≤ 0,2011, Letting r1 ≤ r2be the roots of the quadratic
