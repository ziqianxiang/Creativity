title,year,conference
 Neural network learning: theoretical foundations,1999, Cambridge UniversityPress
 Dynamic node creation in backpropagation networks,1989, Connect
 Adaptive regression and model selection in data mining problems,1999, PhD thesis
 Rademacher and Gaussian complexities: risk bounds and structuralresults,2002, J
 Concentration inequalities: a nonasymptotic theory ofindependence,2013, Oxford University Press
 Inequalities of Bernstein-Jackson-type and the degree of compactness of operators in Banachspaces,1985, In Ann
 On the prediction performance of the lasso,2017, Bernoulli
 Sparse-input neural networks for high-dimensional nonparametric regressionand classification,2017, arXiv:1711
 Statistical learning with sparsity: the lasso andgeneralizations,2015, CRC press
 Layer sparsity in neural networks,2020, arXiv2006
 Rademacher penalties and structural risk minimization,2001, IEEE Trans
 Empirical margin distributions and bounding the generalizationerror of combined classifiers,2002, Ann
 Gradient-based learning applied to documentrecognition,1998, Proceedings of the IEEE
 Deep learning,2015, Nature
 Norm-based generalisation bounds for multi-class convolutionalneural networks,2019, arXiv:1905
 Bounds for Rademacher processes via chaining,2010, arXiv:1010
 Risk bounds for robust deep learning,2020, arXiv:2009
 No spurious local minima: on the optimization landscapes of wide and deep neuralnetworks,2020, arXiv:2010
 Estimating the lassoâ€™s effective noise,2020, arXiv:2004
 Sparse deep belief net model for visual area V2,2008, In Adv
 Sparse convolutional neural networks,2015, InIEEE Int
 On the method of bounded differences,1989, Surv
 Disease inference from health-relatedquestions via sparse deep learning,2015, IEEE Trans
 Group sparse regularization for deepneural networks,2017, Neurocomputing
 Deep learning in neural networks: An overview,2015, Neural Networks
 Nonparametric regression using deep neural networks with ReLU activationfunction,2020, Ann
 Regression shrinkage and selection via the lasso,1996, J
 Empirical processes in M-estimation,2000, Cambridge Univ
 Weak convergence and empirical processes,1996, Springer
 Neural Inf,2016, Process Syst
 Fashion-MNIST: a novel image dataset for benchmarkingmachine learning algorithms,2017, arXiv:1708
 Error bounds for approximations with deep ReLU networks,2017, Neural Networks
 Maximum regularized likelihood estimators: A general prediction theoryand applications,2018, Stat
