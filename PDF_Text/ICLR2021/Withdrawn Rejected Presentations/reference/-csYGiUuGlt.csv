title,year,conference
 Sparse communication for distributed gradient descent,2017, InEmpirical Methods in Natural Language Processing
 Qsgd: Communication-efficient sgd via gradient quantization and encoding,2017, In Advances in Neural Information ProcessingSystems
 Stochastic gradient push fordistributed deep learning,2019, In International Conference on Machine Learning
 Distributed optimizationand statistical learning via the alternating direction method of multipliers,2011, Foundations andTrendsR in Machine learning
 On the convergence of a class of adam-typealgorithms for non-convex optimization,2019, In International Conference for Learning Representations
 Approximate nearest neighbor search by residual vectorquantization,2010, Sensors
 Project adam:Building an efficient and scalable deep learning training system,2014, In Symposium on OperatingSystems Design and Implementation
 Dual averaging for distributed optimization:Convergence analysis and network scaling,2011, IEEE Transactions on Automatic control
 Prox-pda: The proximal primal-dualalgorithm for fast distributed nonconvex optimization and learning over networks,2017, In InternationalConference on Machine Learning
 Adam: A method for stochastic optimization,2015, In InternationalConference on Learning Representations
 Decentralized stochastic optimizationand gossip algorithms with compressed communication,2019, In International Conference on MachineLearning
 On the convergence of stochastic gradient descent with adaptivestepsizes,2019, In International Conference on Artificial Intelligence and Statistics
 Can decentralizedalgorithms outperform centralized algorithms? a case study for decentralized parallel stochasticgradient descent,2017, In Advances in Neural Information Processing Systems
 Deep gradient compression:Reducing the communication bandwidth for distributed training,2018, International Conference onLearning Representations
 Gnsd: A gradient-tracking basednonconvex stochastic algorithm for decentralized optimization,2019, In 2019 IEEE Data ScienceWorkshop (DSW)
 Adaptive gradient methods with dynamicbound of learning rate,2019, In International Conference for Learning Representations
 Dadam: A consensus-baseddistributed adaptive gradient method for online optimization,2019, arXiv preprint arXiv:1901
 Adaptive federated optimization,2020, arXiv preprintarXiv:2003
 On the convergence of adam and beyond,2018, InInternational Conference on Learning Representations
 A stochastic approximation method,1951, The annals of mathematicalstatistics
 Sparsified sgd with memory,2018, InAdvances in Neural Information Processing Systems
 D2 : Decentralized training overdecentralized data,2018, In International Conference on Machine Learning
 Doublesqueeze: Parallel stochasticgradient descent with double-pass error-compensated compression,2019, In International Conferenceon Machine Learning
 Atomo: Communication-efficient learning via atomic sparsification,2018, In Advances inNeural Information Processing Systems
 Adagrad stepsizes: Sharp convergence over nonconvexlandscapes,2019, In International Conference on Machine Learning
 A unified analysis of stochastic momentummethods for deep learning,2018, In International Joint Conference on Artificial Intelligence
 Adaptive methodsfor nonconvex optimization,2018, In Advances in Neural Information Processing Systems
 Under assumptions stated in Corollary 2,2021,1
