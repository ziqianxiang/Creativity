title,year,conference
 Knapsack pruning withinner distillation,2020, arXiv preprint arXiv:2002
 Botorch: Programmable bayesian optimization in pytorch,2019, arXivpreprint arXiv:1910
 Aows: Adaptive and optimalnetwork width search with latency constraints,2020, Proceedings IEEE CVPR
 Once-for-all: Train onenetwork and specialize it for efficient deployment,2020, In International Conference on LearningRepresentations
 Towards efficient model compres-sion via learned global ranking,2020, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Dpp-net: Device-aware progressive search for pareto-optimal neural architectures,2018, In Proceedings of the EuropeanConference on Computer Vision (ECCV)
 Depth-adaptive transformer,2019, arXivpreprint arXiv:1910
 Efficient multi-objective neural architecturesearch via lamarckian evolution,2018, arXiv preprint arXiv:1804
 Singlepath one-shot neural architecture search with uniform sampling,2019, arXiv preprint arXiv:1904
 Filter pruning via geometric medianfor deep convolutional neural networks acceleration,2019, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Searching for mobilenetv3,2019, InProceedings of the IEEE International Conference on Computer Vision
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Flexibo: Cost-aware multi-objective optimization of deep neural networks,2020, arXiv preprint arXiv:2001
 Shallow-Deep Networks: Understanding andmitigating network overthinking,2019, In Proceedings of the 2019 International Conference on MachineLearning (ICML)
 Pruning filters forefficient convnets,2016, arXiv preprint arXiv:1608
 Improved techniques for trainingadaptive deep networks,2019, In Proceedings of the IEEE International Conference on Computer Vision
 Compressingconvolutional neural networks via factorized convolutional filters,2019, In Proceedings of the IEEEConference on Computer Vision and Pattern Recognition
 Darts: Differentiable architecture search,2018, arXivpreprint arXiv:1806
 Metapruning: Meta learning for automatic neural network channel pruning,2019, In Proceedingsof the IEEE International Conference on Computer Vision
 Learn-ing efficient convolutional networks through network slimming,2017, In Proceedings of the IEEEInternational Conference on Computer Vision
 Rethinking the valueof network pruning,2019, In International Conference on Learning Representations
 Learning sparse neural networks throughl_0 regularization,2017, arXiv preprint arXiv:1712
 A bayesian optimization framework for neural network compression,2019, In Proceedings ofthe IEEE International Conference on Computer Vision
 Importance estimation forneural network pruning,2019, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 One ticket to win them all:generalizing lottery ticket initializations across datasets and optimizers,2019, In Advances in NeuralInformation Processing Systems
 A flexible framework for multi-objective bayesian optimization using random scalarizations,2019, In Amir Globerson and Ricardo Silva(eds
 Gaussian processes in machine learning,2003, In Summer School on MachineLearning
 Comparing rewinding and fine-tuning inneural network pruning,2020, In International Conference on Learning Representations
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Gaussian process opti-mization in the bandit setting: No regret and experimental design,2009, arXiv preprint arXiv:0912
 Learning structured sparsity in deepneural networks,2016, In Advances in neural information processing systems
 Learning sparsity and quantization jointly andautomatically for neural network compression via constrained optimization,2020, In Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition
 Rethinking the smaller-norm-less-informativeassumption in channel pruning of convolution layers,2018, In International Conference on LearningRepresentations
 Slimmable neural networks,2019, InInternational Conference on Learning Representations
 Bignas: Scaling up neural architec-ture search with big single-stage models,2020, arXiv preprint arXiv:2003
3 is the similarity for Î¸ across train-ing iterations,2019, In an extreme case
