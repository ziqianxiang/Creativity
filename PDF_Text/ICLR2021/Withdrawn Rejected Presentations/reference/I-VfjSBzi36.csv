title,year,conference
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Rigging the lottery:Making all tickets winners,2020, In MLSys
 Reducing transformer depth on demand withstructured dropout,2019, arXiv preprint arXiv:1909
 Deep residual learning for image recog-nition,2016, In CVPR
 Learn-ing efficient convolutional networks through network slimming,2017, In ICCV
 One ticket to win them all: gen-eralizing lottery ticket initializations across datasets and optimizers,2019, In NeurIPS
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Hopfieldnetworks is all you need,2020, arXiv preprint arXiv:2008
 Comparing rewinding and fine-tuning in neuralnetwork pruning,2020, arXiv preprint arXiv:2003
 Megatron-lm:Training multi-billion parameter language models using model parallelism,2019, arXiv
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Patient knowledge distillation for bert model com-pression,2019, In EMNLP
 Attention is all you need,2017, In NeurIPS
 Structured pruning of large language models,2019, arXivpreprint arXiv:1910
 Learning structured sparsity indeep neural networks,2016, In NeurIPS
 Huggingface's transformers:State-of-the-art natural language processing,2019, ArXiv
 DeeBERT: Dynamic early exitingfor accelerating BERT inference,2020, In ACL
 Drawing early-bird tickets: Toward more efficient trainingof deep networks,2020, In ICLR
 Large batch optimization for deeplearning: Training bert in 76 minutes,2020, In ICLR
 Playing the lottery with rewardsand multiple languages: lottery tickets in rl and nlp,2019, In ICLR
 Q8bert: Quantized 8bit bert,2019, arXivpreprint arXiv:1910
