title,year,conference
 A convergence theory for deep learning viaoverparameterization,2019, ICML
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, ICML
 Implicit regularization in deep matrixfactorization,2019, NerurIPS
 Why do larger models generalize better? a theoretical perspec-tive via the xor problem,2019, ICML
 Solving quadratic equations via phaselift when there areabout as many equations as unknowns,2014, Foundations of Computational Mathematics
 Phase retrieval via wirtinger flow:Theory and algorithms,2015, IEEE Transactions on Information Theory
 Solving random quadratic systems of equations is nearly aseasy as solving linear systems,2017, Communications on Pure and Applied Mathematics
 Gradient descent with random ini-tialization: Fast global convergence for nonconvex phase retrieval,2019, Mathematical Programming
 Gradient descent provably optimizesover-parameterized neural networks,2019, ICLR
 Neural networks and poly-nomial regression,2020, demystifying the overparametrization phenomena
 The numerics of phase retrieval,2020, Acta Numerica
 Agnostic learning of a single neuron with gradientdescent,2020, arXiv:2005
 Stationary points of shallow neural networkswith quadratic activation function,2019, arXiv:1912
 Mildly overparametrized neural nets can memorizetraining data efficiently,2019, arXiv:1909
 Implicit regularization of discrete gradientdynamics in linear neural networks,2019, NeurIPS
 Reliably learning the relu in polyno-mial time,2017, COLT
 Implicit regularization in matrix factorization,2017, NIPS
 Fittingrelus via sgd and quantized sgd,2019, ISIT
 No spurious local minima in deepquadratic networks,2019, arXiv:2001
 Adam: A method for stochastic optimization,2015, ICLR
 Nonconvex matrix factorization from rank-onemeasurements,2019, AISTATS
 Algorithmic regularization in over-parameterizedmatrix sensing and neural networks with quadratic activations,2018, COLT
 On the computational efficiency of trainingneural networks,2014, NeurIPS
 Aggregated momentum: Stabilitythrough passive damping,2019, ICLR
 Optimization and generaliza-tion of shallow neural networks with quadratic activation functions,2020, arXiv:2006
 Complex dynamics in simple neural networks: Understandinggradient flow in phase retrieval,2020, arXiv:2006
 The loss surface of deep and wide neural networks,2017, ICML
 The effects of mild over-parameterization on theoptimization landscape of shallow relu neural networks,2020, arXiv:2006
 Learning relus via gradient descent,2017, NeurIPS
 Theoretical insights into the optimizationlandscape of over-parameterized shallow neural networks,2018, IEEE Transactions on InformationTheory
 A geometric analysis of phase retrieval,2016, IEEE ISIT
 On the importance of initializa-tion and momentum in deep learning,2013, ICML
 Phase retrieval via randomized kaczmarz: Theoretical guar-antees,2018, Information and Inference
 Student specialization in deep rectified networks with finite width and input dimen-sion,2020, ICML
 Spurious valleys in one-hidden-layer neuralnetwork optimization landscapes,2019, JMLR
 The local convexity of solving systems ofquadratic equations,2016, Results in Mathematics
 Robust recovery via implicit bias of discrepantlearning rates for double over-parameterization,2020, arXiv:2006
