title,year,conference
 Neural machine translation by jointlylearning to align and translate,2015, In Yoshua Bengio and Yann LeCun (eds
 Generating long sequences with sparsetransformers,2019, CoRR
 Theoretical limitations of self-attention in neural sequence models,2019, CoRR
 Multilayer feedforward networks areuniversal approximators,1989, Neural Networks
 Inferring algorithmic patterns with stack-augmented re-current nets,2015, In Corinna Cortes
 Effective approaches to attention-based neural machine translation,2015, CoRR
 A formalhierarchy of RNN architectures,2020, In Proceedings of the 58th Annual Meeting of the Associationfor Computational Linguistics
 Extraction of rules from discrete-time recurrent neuralnetworks,1996, Neural Networks
 Improving transformer models by reordering theirsublayers,2020, In Dan Jurafsky
 Attention is all you need,2017, CoRR
 Extracting automata from recurrent neural networksusing queries and counterexamples,2018, In Proceedings of the 35th International Conference onMachine Learning
 Big bird: Transformers forlonger sequences,2020, CoRR
