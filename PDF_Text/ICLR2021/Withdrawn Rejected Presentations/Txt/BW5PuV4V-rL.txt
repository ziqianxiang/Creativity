Under review as a conference paper at ICLR 2021
Gradient-based training of Gaussian Mixture
Models for High-Dimensional Streaming Data
Anonymous authors
Paper under double-blind review
Ab stract
We present an approach for efficiently training Gaussian Mixture Models (GMMs)
by Stochastic Gradient Descent (SGD) with non-stationary, high-dimensional
streaming data. Our training scheme does not require data-driven parameter ini-
tialization (e.g., k-means) and has the ability to process high-dimensional samples
without numerical problems. Furthermore, the approach allows mini-batch sizes as
low as 1, typical for streaming-data settings, and it is possible to react and adapt to
changes in data statistics (concept drift/shift) without catastrophic forgetting. Major
problems in such streaming-data settings are undesirable local optima during early
training phases and numerical instabilities due to high data dimensionalities.We
introduce an adaptive annealing procedure to address the first problem,whereas
numerical instabilities are eliminated by using an exponential-free approximation
to the standard GMM log-likelihood. Experiments on a variety of visual and non-
visual benchmarks show that our SGD approach can be trained completely without,
for instance, k-means based centroid initialization, and compares to a favorably
online variant of EXPectation-MaXimization (EM) - stochastic EM (sEM), which it
outperforms by a large margin for very high-dimensional data.
1	Introduction
This contribution focuses Gaussian MiXture Models (GMMs), which rePresent a Probabilistic unsuPer-
vised model for clustering and density estimation and allowing samPling and outlier detection. GMMs
have been used in a wide range of scenarios, e.g., Melnykov & Maitra (2010). Commonly, free Param-
eters of a GMM are estimated by using the EXPectation-MaXimizations (EMs) algorithm (DemPster
et al., 1977), as it does not require learning rates and automatically enforces all GMM constraints. A
popular online variant is stochastic Expectation Maximization (sEM) (Cappe & Moulines, 2009),
which can be trained mini-batch wise and is, thus, more suited for large datasets or streaming data.
1.1	Motivation
Intrinsically, EM is a batch-type algorithm. Memory requirements can therefore become excessive
for large datasets. In addition, streaming-data scenarios require data samples to be processed one
by one, which is impossible for a batch-type algorithm. Moreover, data statistics may be subject to
changes over time (concept drift/shift), to which the GMM should adapt. In such scenarios, an online,
mini-batch type of optimization such as SGD is attractive, as it can process samples one by one, has
modest, fixed memory requirements and can adapt to changing data statistics.
1.2	Related Work
Online EM is a technique for performing EM mini-batch wise, allowing to process large datasets.
One branch of previous research Newton et al. (1986); Lange (1995); Chen et al. (2018) has been
devoted to the development of stochastic Expectation Maximization (sEM) algorithms that reduce
the original EM method in the limit of large batch sizes. The variant of CaPPe & Moulines (2009)
is widely used due to its simplicity and efficiency for large datasets. These approaches come at the
price of additional hyper-parameters (e.g., learning rate or mini-batch size), thus, removing a key
advantage of EM over SGD. Another common approach is to modify the EM algorithm itself by,
e.g., including heuristics for adding, splitting and merging centroids Vlassis & Likas (2002); Engel
1
Under review as a conference paper at ICLR 2021
& Heinen (2010); Pinto & Engel (2015); Cederborg et al. (2010); Song & Wang (2005); Kristan
et al. (2008); Vijayakumar et al. (2005). This allows GMM-like models to be trained by presenting
one sample after another. The models work well in several application scenarios, but their learning
dynamics are impossible to analyze mathematically. They also introduce a high number of parameters.
Apart from these works, some authors avoid the issue of extensive datasets by determining smaller
“core sets” of representative samples and performing vanilla EM Feldman et al. (2011).
SGD for training GMMs has, as far as we know, been recently treated only by Hosseini & Sra
(2015; 2019). In this body of work, GMM constraint enforcement is ensured by using manifold
optimization techniques and re-parameterization/regularization. Thereby, additional hyper-parameters
are introduced. The issue of local optima is sidestepped by a k-means type centroid initialization,
and the used image datasets are low-dimensional (36 dimensions). Additionally, enforcing positive
definiteness constraints by Cholesky decomposition is discussed.
Annealing and Approximation approaches for GMMs were proposed by Verbeek et al. (2005);
Pinheiro & Bates (1995); Ormoneit & Tresp (1998); Dognin et al. (2009). However, the regularizers
proposed by Verbeek et al. (2005); Ormoneit & Tresp (1998) significantly differ from our scheme.
GMM log-likelihood approximations, similar to the one used here, are discussed in, e.g., Pinheiro &
Bates (1995) and Dognin et al. (2009), but only in combination with EM training.
GMM Training in High-Dimensional Spaces is discussed in several publications: A conceptually
very interesting procedure is proposed by Ge et al. (2015). It exploits the properties of high-
dimensional spaces in order to achieve learning with a number of samples that is polynomial in the
number of Gaussian components. This is difficult to apply in streaming settings, since higher-order
moments need to be estimated beforehand, and also because the number of samples usually cannot
be controlled in practice. Training GMM-like lower-dimensional factor analysis models by SGD on
high-dimensional image data is successfully demonstrated in Richardson & Weiss (2018), avoiding
numerical issues, but, again, sidestepping the local optima issue by using k-means initialization. The
numerical issues associated with log-likelihood computation in high-dimensional spaces are generally
mitigated by using the “logsumexp” trick Nielsen & Sun (2016), which is, however, insufficient for
ensuring numerical stability for particularly high-dimensional data, such as images.
1.3	Goals and contributions
The goals of this article are to establish GMM training by SGD as a simple and scalable alternative to
sEM in streaming scenarios with potentially high-dimensional data. The main novel contributions
are:
•	a proposal for numerically stable GMM training by SGD that outperforms sEM for high data
dimensionalities
•	an automatic annealing procedure that ensures SGD convergence from a wide range of initial
conditions without prior knowledge of the data (e.g., no k-means initialization) which is especially
beneficial for streaming data
•	a computationally efficient method for enforcing all GMM constraints in SGD
Apart from these contents, we provide a publicly available TensorFlow implementation.1
2	Datasets
We use a variety of different image-based datasets as well as a non-image dataset for evaluation
purposes. All datasets are normalized to the [0, 1] range.
MNIST (LeCun et al., 1998) contains gray scale images, which depict handwritten digits from 0 to 9
in a resolution of 28 X 28 pixels - the common benchmark for computer vision systems.
SVHN (Wang et al., 2012) contains color images of house numbers (0-9, resolution 32 × 32).
FashionMNIST (Xiao et al., 2017) contains gray scale images of 10 cloth categories and is considered
as more challenging classification task compared to MNIST.
Fruits 360 (Murean & Oltean, 2018) consists of colored pictures showing different types of fruits
(100 × 100 × 3 pixels). The ten best-represented classes are selected from this dataset.
Devanagari (Acharya et al., 2016) includes gray scale images of handwritten Devanagari letters with
a resolution of 32 ×32 pixels - the first 10 classes are selected.
NotMNIST (Yaroslav Bulatov, 2011) is a gray scale image dataset (resolution 28 × 28 pixels) of
1https://github.com/gmm-iclr21/sgd-gmm
2
Under review as a conference paper at ICLR 2021
letters from A to J extracted from different public available fonts.
ISOLET (Cole & Fanty, 1990) is a non-image dataset containing 7 797 samples of spoken letters
recorded from 150 subjects. Each sample was encoded and is represented by 617 float values.
3	Gaussian Mixture Models
GMMs are probabilistic models that intend to explain the observed data X = {xn} by expressing
their density as a weighted mixture of K Gaussian component densities N(x; μk, Pk) ≡Nk(x):
p(x) = PkK πkNk(x). We work with precision matrices P = Σ-1 instead of covariances Σ. This
is realized by optimizing the (incomplete) log-likelihood
L = En log	πkNk(xn)
k
(1)
3.1	GMMS AND SGD
GMMs require the mixture weights to be normalized: k πk = 1 and the precision matrices to be
positive definite: xTPkx ≥ 0 ∀x. These constraints must be explicitly enforced after each SGD step:
Weights πk are adapted according to Hosseini & Sra (2015), which replaces them by other free
parameters ξk from which the πk are computed so that normalization is ensured:
exp(ξk)
Pj eχp(ξj).
(2)
Precision Matrices need to be positive-definite, so we re-parameterize these as Pk = DkT Dk ,
where the upper-diagonal matrices Dk result from a Cholesky decomposition. Consequently,
det Σk = det Pk-1 = det(DkTDk) -1 = tr(Dk) -2 can be computed efficiently. To avoid re-
computing the costly Cholesky decomposition of the Pk at every iteration, we perform it on the initial
precision matrices and just erase the elements below the diagonal in the Dk after each gradient step.
3.2	Max-Component Approximation for GMMs
The log-likelihood Eq. (1) is difficult to optimize by SGD (see Sec. 3.3). This is why we intend to
find a lower bound that we can optimize instead. A simple scheme is given by
L=En log	πkNk(xn)
k
≥ En [ log max® (∏k Nk (Xn))] = L = En [log (∏k* Nfc* (Xn))]
(3)
where k* = argmaXk nkNk(xn). This is What we call the max-component approximation of Eq. (3).
In contrast to the lower bound that is constructed for EM-type algorithms, this bound is usually
not tight. The advantages of L are the avoidance of local optima in SGD, and the elimination of
exponentials causing numerical instabilities for high data dimensions. The “logsumexp” trick is
normally employed with GMMs to rectify this by factoring out the largest component probability
Nk* . This mitigates, but does not avoid numerical problems when distances are high. To give an
example: we normalize the component probability Nk = e-101 (using 32-bit floats) by the highest
probability Nk* = e3, and we obtain NNk* = e-104, which is numerically problematic.
3.3	Undesirable Local Optima in SGD Training
An issue when performing SGD without k-means initialization concerns undesirable local optima.
Degenerate Solutions occur when naively optimizing L by SGD (see Fig. 1a). All components have
the same weight, centroid and covariance matrix: ∏k ≈ *,μk = E[X], ∑k = Cov(X) ∀k, in which
case all gradients vanish (see App. A.3 for a proof). These solutions are avoided by L, since only a
subset of components is updated by SGD, thereby breaking the symmetry between components.
Single/Sparse-Component Solutions occur when optimizing L by SGD (see Fig. 1b). They are
characterized by one or several components {ki } that have large weights with centroid and precision
3
Under review as a conference paper at ICLR 2021
matrices given by the mean and covariance of a significant subset Xki ⊂ X of the data X : πki 0,
μki = E[Xki], ∑ki = Cov(Xk J, whereas the remaining components k are characterized by ∏k ≈ 0,
μk = μ(t = 0), Pk = P(t = 0). Thus, these unconverged components are almost never best-matching
∂L
components k . The max-operation in L causes gradients like ∂L to contain δkk* (see App. A.3).
This implies that they are non-zero only for the best-matching component k*. Thus the gradients of
unconverged components vanish, implying that they remain in their unconverged state.
a) degenerate solution (optimizing L)
b) sparse-component solution (optimizing L)
Figure 1: Undesirable solutions during SGD, visualized for MNIST with component weights πk .
3.4	Annealing Procedure for Avoiding Local Optima
Our approach for avoiding undesirable solutions is to punish their characteristic response patterns by
a modification of L,the smoothed max-component log-likelihood Lσ:
Lσ = EnmaXk Egkj(σ)log ㈤Nj(Xn)) = En Egk*j(σ)log ㈤Nj(xn)).	(4)
jj
The entries of the gk are computed by a Gaussian function centered on component k with common
spatial standard deviation σ, where we assume that the K components are arranged on a √K X √K
grid with 2D Euclidean metric (see App. A.4). Eq. (4) essentially represents a smoothing of the
log (πkNk(x)) with a 2D convolution filter (we use periodic boundary conditions). Thus, Eq. (4)
is maximized if the log probabilities follow a uni-modal Gaussian profile of spatial variance 〜σ2,
which heavily punishes single-component solutions that have a locally delta-like response.
Annealing starts with large values of σ(t) = σ0 and reduces it over time to an asymptotic small value
of σ = σ∞, thus, smoothly transitioning from Lσ in Eq. (4) into L in Eq. (3).
Annealing Control is ensured by adjusting σ, which defines an effective upper bound on Lσ (see
App. A.2 for a proof). This implies that the loss will be stationary once this bound is reached,
which we consider a suitable indicator for reducing σ . We implement an annealing control that
sets σ J 0.9σ whenever the loss is considered sufficiently stationary. Stationarity is detected by
maintaining an exponentially smoothed average '(t) = (1 一 α)'(t — 1) + αLσ (t) on time scale α.
Every 1 iterations, we compute the fractional increase of Lσ as
∆
`(t) 一 `(t 一 α-1)
. .. ^ . .
'(t 一 a-1) 一 Lσ (t = 0)
(5)
and consider the loss stationary iff ∆ < δ (the latter being a free parameter). The choice of the time
, ,i'	. 1 ∙ Λ∕T ∙	, 1 ∙	1 ∙ ,1	11	∙	, ∙
constant for smoothing Lσ is outlined in the following section.
3.5	Training Procedure for SGD
Training GMMs with SGD is performed by maximizing the smoothed max-component log-likelihood
Λ∕T i'	-I—' Z Λ∖ A . ,1	, ∙	i'	,1	,	,1	,	∙ 1 ,	1
Lσ from Eq. (4). At the same time, we enforce the constraints on the component weights and
1

covariances as described in Sec. 3.1 and transition from Lσ into L by annealing (see Sec. 3.4). SGD
requires a learning rate to be set, which in turn determines the parameter α (see Sec. 3.4) as α =
since stationarity detection should operate on a time scale similar to that of SGD. Cholesky matrices
4
Under review as a conference paper at ICLR 2021
Dk are initialized to DmaxI and are clipped after each iteration so that diagonal entries are in the
range [0, Dm2 ax]. This is necessary to avoid excessive growth of precisions for data entries with
vanishing variance, e.g., pixels that are always black. Weights are uniformly initialized to ∏i = K,
centroids in the range [-μi, +μi] (see Alg. 1 for a summary). Please note that our SGD approach
requires no centroid initialization by k-means, as it is usually recommended when training GMMs
with EM. We discuss and summarize good practices for choosing hyper-parameters Sec. 5.
Algorithm 1: Steps of SGD-GMM training.
Data: initializer values: μi, K, He∞, σo/σ∞, δ and data X
Result: trained GMM model
ι μ JU(-μi, +μi), π J 1/K, P J IDmax, σ J σ0, E J ∈o	//initialization
2	forall t < T do	// training loop
3	g(t) J Create_annealing_mask(o,t)	// see Sec. 3.4 and App. A.4
σσ	σ
4	μ(t), P(t), ∏(t) = Edμ^ ^+ μ(t-1), E^∂P + P(t-1), E^∂∏—+ ∏(t-1)	// SGD update
5	P(t) J PreCiSiOnSdiPPing(P, DmaX)	// see Sec. 3.5
6	π(t) J normalization(π(t))	// see Eq. (2)
7	' (t) J (1 - α)'(t-1) + αLσ(x(t))	// sliding log-likelihood
8	if annealing update iteration then	// see Sec. 3.4
9	Lif ∆ < δ then σ(t) J 0.9σ(t — 1), E(t) J 0.9c(t — 1)	// ∆ see Eq. (5)
3.6	Training Procedure for stochastic Expectation Maximization
We use sEM as proposed by CaPPe & Moulines (2009). We choose the step size of the form
ρt = ρ0(t + 1)-0.5+α, with α ∈ [0, 0.5], ρ0 < 1 and enforce ρ(t) ≥ ρ∞. Values for these parame-
ters are determined via a grid search in the ranges ρ0 ∈ {0.01, 0.05, 0.1}, α ∈ {0.01, 0.25, 0.5} and
ρ∞ ∈ {0.01, 0.001, 0.0001}. Each sEM iteration uses a batch size B. Initial accumulation of suf-
ficient statics is conducted for 10% of an epoch, but not when re-training with new data statistics.
Parameter initialization and clipping of precisions is performed just as for SGD, see Sec. 3.5.
3.7	Comparing SGD and sEM
Since sEM optimizes the log-likelihood L, whereas SGD optimizes the annealed approximation Lσ,
the comparison of these measures should be considered carefully. We claim that the comparison is
fair assuming that i) SGD annealing has converged and ii) GMM responsibilities are sharply peaked
so that a single component has responsibility of ≈ 1. It follows from i) that Lσ ≈ L and ii) implies
that L ≈ L. Condition ii) is usually satisfied to high precision especially for high-dimensional inputs:
if it is not, the comparison is biased in favor of sEM, since L > L by definition.
4	Experiments
Unless stated otherwise, the experiments in this section will be conducted with the following parameter
values for SEM and SGD (where applicable): mini-batch size B = 1, K = 8 X 8, μi =0.1, σ0 = 2,
σ∞ = 0.01, E = 0.001, Dmax = 20. Each experiment is repeated 10 times with identical parameters
but different random seeds for parameter initialization. See Sec. 5 for a justification of these choices.
Due to input dimensionality, all precision matrices must be taken to be diagonal. Training/test data
are taken from the datasets mentioned in Sec. 2.
4.1	Robustness of SGD to Initial Conditions
Here, we train GMMs for three epochs on classes 1 to 9 for each dataset. We use different random
and non-random initializations of the centroids and compare the final log-likelihood values. Random
centroid initializations are parameterized by μi ∈ {0.1,0.3,0.5}, whereas non-random initializations
are defined by centroids from a previous training run on class 0 (one epoch). The latter is done to
have a non-random centroid initialization that is as dissimilar as possible from the training data. The
initialization of the precisions cannot be varied because empirical data shows that training converges
5
Under review as a conference paper at ICLR 2021
to undesirable solutions if the precisions are not initialized to large values. While this will have
to be investigated further, we find that convergence to near-identical levels, regardless of centroid
initialization for all datasets (see Tab. 1 for more details).
4.2	Added value of annealing
To demonstrate the beneficial effects of annealing, we perform experiments on all datasets with
annealing turned off. This is achieved by setting σ0 = σ∞ . This invariably produces sparse-
component solutions with strongly inferior log-likelihoods after training, please refer to Tab. 1.
Table 1: Effect of different random and non-random centroid initializations of SGD training. Given
are the means and standard deviations of final log-likelihoods (10 repetitions per experiment). To
show the added value of annealing, the right-most column indicates the final log-likelihoods when
annealing is turned off. This value should be co,pared to the leftmost entry in each row where
annealing is turned on. Standard deviations in this case where very small so they are omitted.
^initialization Dataset	μi = 0.1		random μi = 0.3		μi = 0.5		non-random init class 0		no ann. μi = 0.1 mean
	mean	Std	mean	Std	mean	Std	mean	Std	
MNIST	-205.47	-108	-205.46	-077	-205.68	078	-205.37	068	124.1
FashionMNIST	-231.22	-153	-231.58	-284	-231.00	-m	-229.59	0.59	183.0
NotMNIST 一	-48.41	-T77	-48.59	156	-48.32	-1Γ3	-49.37	2.32	-203.8
Devanagari	-15.95	-159	-15.76	134	-17.01	-m	-22.07	459	-263.4
Fruits 360	12 095.80	98.02	12 000.70	127.00	12 036.25	122.06	10912.79	1727.61	331.2
SVHN	1328.06	-094	1327.99	-159	1 328.40	-117	1327.80	094	863.2
ISOLET 一	354.34	0.04	354.36	0.04	354.36	0.04	354.20	0.05	201.5
4.3	Clustering Performance Evaluation
To compare the clustering performance of sEM and GMM the Davies-Bouldin score (Davies &
Bouldin, 1979) and the Dunn index (Dunn, 1973) are determined. We evaluate the grid-search results
to find the best parameter setup for each metric for comparison. sEM is initialized by k-means to
show that our approach does not depend on parameter initialization. Tab. 2 indicaties that SGD can
egalize sEM performance (see also App. A.5).
Table 2: Clustering performance comparison of SGD and sEM training using Davies-Bouldin score
(less is better) and Dunn index (more is better). Results are in bold face whenever they are better by
more than half a standard deviation.
∖ Dataset	Metric ^Algo.	Davies-Bouldin score				Dunn index			
		SGD		sEM		SGD		sEM	
		mean	Std	mean	Std	mean	Std	mean	Std
MNIST		2.50	"0.04	2.47	"004	0.18	a.02	0.16	"002^
FaShionMNIST		2.06	"005"	2.20	"004	0.20	0.03	0.19	"02
NotMNIST 一		2.30	"003"	2.12	"003"	0.15	a.03	0.14	"004
Devanagari		2.60	■0.04	2.64	"002^	0.33	a.01	0.27	"004
SVHN		2.34	^Q04	2.41	^Q03"	0.15	^Q02^	0.15	^Q02^
4.4	Streaming-Data Experiments with Constant Data Statistics
We train GMMs for three epochs (enough for convergence in all cases) using SGD and sEM on all
datasets as described in Secs. 3.5 and 3.6.The resulting centroids of our SGD-based approach are
shown in Fig. 2, whereas the final loss values for SGD and sEM are compared in Tab. 3. The centroids
for both approaches are visually similar, except for the topological organization due to annealing for
SGD, and the fact that in most experiments, some components do not converge for sEM while the
others do. Tab. 3 indicates that SGD achieves performances superior to sEM in the majority of cases,
in particular for the highest-dimensional datasets (SVHN: 3 072 dimensions and Fruits 360: 30 000
dimensions).
6
Under review as a conference paper at ICLR 2021
Table 3: Comparison of SGD and sEM training on all datasets in a streaming-data scenario. Each
time mean log-likelihoods (10 repetitions) at the end of training, and their standard deviations are
presented. Results are in bold face whenever they are higher by more than half a standard deviation.
Additionally, the averaged maximum responsibilities (pk*) for test data are given forjuStifying the
max-component approximation.
^^^^AlgOrithm DataSet^i^^^	SGD			SEM	
	0 max pk*	mean	std	mean	std
MNIST	0.992 674	-216.60	-031	-216.82	138
FaShiOnMNIST-	0.997 609	-234.59	-228	-222.95	603
NOtMNIST	0.998 713	--34.76	-116	-40.09	890
DeVanagari	0.999 253	--14.65	-1.09	-13.46	616
FrUitS 360	0.999 746	11754.32	75.63	5 483.00	1201.60
SVHN	0.998148	1329.83	-0780	1176.07	-16.91
ISOLET	0.994 069	354.21	0.01	354.55	0.37
BBE]B□Π□□HK	∏	C	Of	JB □□
□B□□DΠ□□B]∏C	匚		□E	TE □□
	IC		□匚	≡□∏
□□□□□□□□□DC	匚			]HCE1
qqqqħhqb oll			□匚	Z□CL
ΞΞQ□HΠE]□ c				=口 EZC
□ΞQB□E1□□ □匚二				^□□C
ΞSB□□□□□ [3113			3Γ	]□O□
∏□∏□□ 0□□□□□E0 ffi[≡[SS□EE
∏∏∏∏□ S□□□□0HS S][E 用[H□[Iff
m∏≡□ SQ□□□ΞE3□
nnHHsnnn
HHHHHHnn
HHHnHSBn

πππππ□□□
-IlIlIl I!I!IlIlI
___________
BJ□□ca≡ss
ffl[SHHSSSn
MNIST	SVHN FashionMNIST Devanagari NotMNIST Fruits 360
Figure 2:	Exemplary results for centroids learned by SGD, trained on full images.
Visualization of High-dimensional sEM Outcomes Fig. 3 was obtained after training GMMs
by sEM on both the Fruits 360 and the SVHN dataset. It should be compared to Fig. 2, where an
identical procedure was used to visualize centroids of SGD-trained GMMs. It is notable that the
effect of unconverged components does not occur at all for our SGD approach, which is due to the
annealing mechanism that “drags” unconverged components along.
Figure 3:	Visualization of centroids after exemplary training runs (3 epochs) on high-dimensional
datasets for sEM: Fruits 360 (left, 30 000 dimensions) and SVHN (right, 3 000 dimensions). Com-
ponent entries are displayed “as is”, meaning that low brightness means low RGB values. Visibly,
many GMM components remain unconverged, which is analogous to a sparse-component solution
and explains the low log-likelihood values especially for these high-dimensional datasets.
5	Discussion and Conclusion
The Relevance of this Article is outlined by the fact that training GMMs by SGD was recently
investigated in the community by Hosseini & Sra (2015; 2019). We go beyond, since our approach
does not rely on off-line data-driven model initialization, and works for high-dimensional streaming
data.The presented SGD scheme is simple and very robust to initial conditions due to the proposed
annealing procedure, see Sec. 4.1 and Sec. 4.2. In addition, our SGD approach compares favorably to
the reference model for online EM (CaPPe & Moulines, 2009) in terms of achieved log-likelihoods,
which was verified on multiple real-world datasets. Superior SGD performance is observed for the
high-dimensional datasets.
7
Under review as a conference paper at ICLR 2021
Analysis of Results suggests that SGD performs better than sEM on average, see Sec. 4.4, although
the differences are very modest. It should be stated clearly that it cannot be expected, and is not
the goal of this article, to outperfom sEM by SGD in the general case, only to achieve a similar
performance. However, if sEM is used without, e.g., k-means initialization, components may not
converge (see Fig. 3 for a visual impression) for very high-dimensional data like Fruits 360 and
SVHN datasets, which is why SGD outperforms sEM in this case. Another important advantage of
SGD over sEM is the fact that no grid search for finding certain hyper-parameter values is necessary,
whereas sEM has a complex and unintuitive dependency on ρ0, ρ∞ and α0 .
Small Batch Sizes and Streaming Data are possible with the SGD-based approach. Throughout
the experiments, we used a batch size of 1, which allows streaming-data processing without the need
to store any samples at all. Larger batch sizes are, of course, possible and increase execution speed.
In the experiments conducted here, SGD (and sEM) usually converged within the first two epochs,
which is a substantial advantage whenever huge sets of data have to be processed.
No Assumptions About Data Generation are made by SGD in contrast to the EM and sEM
algorithms. The latter guarantee that the loss will not decrease due to an M-step. This, however,
assumes a non-trivial dependency of the data on an unobservable latent variable (see App. A.1 for a
proof). In contrast, SGD makes no such hard-to-verify assumptions, which is a rather philosophical
point, but may be an advantage in certain situations where data are strongly non-Gaussian.
Numerical Stability is assured by our SGD training approach. It does not optimize the log-likelihood
but its max-component approximation. This approximation contains no exponentials at all and is
very well justified by the results of Tab. 3 which show that component probabilities are very strongly
peaked. In fact, it is the gradient computations where numerical problems (e.g., NaN values) occurred.
The “logsumexp” trick mitigates the problem, but does not eliminate it (see Sec. 3.2). It cannot be
used when gradients are computed automatically (what most machine learning frameworks do).
Hyper-Parameter Selection Guidelines are as follows: the learning rate must be set by cross-
validation (a good value is 0.001). We empirically found that initializing precisions to the cut-off
value Dmax and an uniform initialization of the πi are beneficial, and that centroids are best initialized
to small random values. A value of Dmax = 20 always worked in the experiments. Generally, the
cut-off must be much larger than the inverse of the data variance. In many cases, it should be possible
to estimate this roughly, even in streaming settings, especially when samples are normalized. For
density estimation, choosing higher values for K leads to higher final log-likelihoods (validated in
App. A.6). For clustering, K should be selected using standard techniques for GMMs. The parameter
δ controls loss stationarity detection for the annealing procedure and was shown to perform well for
δ = 0.05. Larger values will lead to faster decrease of σ(t), which may impair convergence. Smaller
values are always admissible but lead to longer convergence times. The annealing time constant α
should be set to the GMM learning rate or lower. Smaller values of α lead to longer convergence
times since σ(t) will be updated less often. The initial value σ0 needs to be large in order to enforce
convergence for all components. A typical value is 0.25√K. The lower bound on σ, σ∞ should be
as small as possible to achieve high log-likelihoods (e.g., 0.01, see proof in App. A.2).
6	Outlook
The presented work can be extended in several ways: First of all, annealing control could be simplified
further by inferring good δ values from α. Likewise, increases ofσ might be performed automatically
when the loss rises sharply, indicating a task boundary. As we found that GMM convergence times
grow linear with the number of components, we will investigate hierarchical GMM models that
operate like a Convolutional Neural Network (CNN), and in which individual GMMs only see a
local patch of the input and can therefore have low K. Lastly, we will investigate a replay by SGD-
trained GMMs for continual learning architectures. GMMs could compare favorably to Generative
Adversarial Nets (Goodfellow et al., 2014) due to faster training and the fact that sample generation
capacity can be monitored via the log-likelihood.
8
Under review as a conference paper at ICLR 2021
References
Shailesh Acharya, Ashok Kumar Pant, and Prashnna Kumar Gyawali. Deep learning based large
scale handwritten Devanagari character recognition. SKIMA 2015 - 9th International Conference
on Software, Knowledge, Information Management and Applications, 2016. doi: 10.1109/SKIMA.
2015.7400041.
Olivier CaPPe and Eric Moulines. On-line expectation-maximization algorithm for latent data models.
Journal ofthe Royal Statistical Society. Series B: Statistical Methodology,71(3):593-613, 2009.
ISSN 13697412. doi: 10.1111/j.1467-9868.2009.00698.x.
Thomas Cederborg, Ming Li, Adrien Baranes, and Pierre Yves Oudeyer. Incremental local online
Gaussian Mixture Regression for imitation learning of multiPle tasks. IEEE/RSJ 2010 International
Conference on Intelligent Robots and Systems, IROS 2010 - Conference Proceedings, PP. 267-274,
2010. doi: 10.1109/IROS.2010.5652040.
Jianfei Chen, Jun Zhu, Yee Whye Teh, and Tong Zhang. Stochastic exPectation maximization
with variance reduction. Advances in Neural Information Processing Systems, 2018-December
(NeurIPS):7967-7977, 2018. ISSN 10495258.
Ronald Cole and Mark Fanty. SPoken letter recognition. PP. 385-390, 1990. doi: 10.3115/116580.
116725.
David L. Davies and Donald W. Bouldin. A Cluster SeParation Measure. IEEE Transactions on
Pattern Analysis and Machine Intelligence, PAMI-1(2):224-227, 1979. ISSN 01628828. doi:
10.1109/TPAMI.1979.4766909.
A. P. DemPster, N. M. Laird, and D. B. Rubin. Maximum Likelihood from IncomPlete Data Via the
EM Algorithm . Journal of the Royal Statistical Society: Series B (Methodological), 39(1):1-22,
1977. doi: 10.1111/j.2517-6161.1977.tb01600.x.
Pierre L. Dognin, Vaibhava Goel, John R. Hershey, and Peder A. Olsen. A fast, accurate aPProximation
to log likelihood of Gaussian mixture models. ICASSP, IEEE International Conference on Acoustics,
Speech and Signal Processing - Proceedings, (3):3817-3820, 2009. ISSN 15206149. doi: 10.1109/
ICASSP.2009.4960459.
J. C. Dunn. A fuzzy relative of the ISODATA Process and its use in detecting comPact well-
seParated clusters. Journal of Cybernetics, 3(3):32-57, 1973. ISSN 00220280. doi: 10.1080/
01969727308546046.
Paulo Martins Engel and Milton Roberto Heinen. Incremental learning of multivariate Gaussian
mixture models. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial
Intelligence and Lecture Notes in Bioinformatics), 6404 LNAI:82-91, 2010. ISSN 03029743. doi:
10.1007/978-3-642-16138-4_9.
Dan Feldman, Matthew Faulkner, and Andreas Krause. Scalable training of mixture models via
coresets. Advances in Neural Information Processing Systems 24: 25th Annual Conference on
Neural Information Processing Systems 2011, NIPS 2011, PP. 1-9, 2011.
Rong Ge, Qingqing Huang, and Sham M. Kakade. Learning mixtures of gaussians in high dimensions.
Proceedings of the Annual ACM Symposium on Theory of Computing, 14-17-June-2015:761-770,
2015. ISSN 07378017. doi: 10.1145/2746539.2746616.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling,
C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Process-
ing Systems 27, PP. 2672-2680. Curran Associates, Inc., 2014. URL http://papers.nips.
cc/paper/5423-generative-adversarial-nets.pdf.
Reshad Hosseini and Suvrit Sra. Matrix manifold oPtimization for Gaussian mixtures. Advances in
Neural Information Processing Systems, 2015-January:910-918, 2015. ISSN 10495258.
Reshad Hosseini and Suvrit Sra. An alternative to em for gaussian mixture models: Batch and
stochastic riemannian oPtimization. Mathematical Programming, PP. 1-37, 2019.
9
Under review as a conference paper at ICLR 2021
Matej Kristan, Danijel Skocaj, and Ales Leonardis. Incremental learning with gaussian mixture
models. In Computer Vision Winter Workshop,pp. 25-32, 2008.
Kenneth Lange. A Gradient Algorithm Locally Equivalent to the EM Algorithm. Journal of the
Royal Statistical Society: Series B (Methodological), 57(2):425-437, 1995. ISSN 0035-9246. doi:
10.1111/j.2517-6161.1995.tb02037.x.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2323, 1998. ISSN 00189219. doi:
10.1109/5.726791. URL http://ieeexplore.ieee.org/document/726791.
Volodymyr Melnykov and Ranjan Maitra. Finite mixture models and model-based clustering.
Statistics Surveys, 4(October):80-116, 2010. ISSN 19357516. doi: 10.1214/09-SS053.
Horea Murean and Mihai Oltean. Fruit recognition from images using deep learning. Acta Universi-
tatis Sapientiae, Informatica, 10(1):26-42, 2018. ISSN 2066-7760. doi: 10.2478/ausi-2018-0002.
URL http://arxiv.org/abs/1712.00580.
J. Newton, D. M. Titterington, A. F. M. Smith, and U. E. Makov. Statistical Analysis of Finite
Mixture Distributions. Biometrics, 42(3):679, 1986. ISSN 0006341X. doi: 10.2307/2531224.
Frank Nielsen and Ke Sun. Guaranteed bounds on information-theoretic measures of univariate
mixtures using piecewise log-sum-exp inequalities. Entropy, 18(12), 2016. ISSN 10994300. doi:
10.3390/e18120442.
Dirk Ormoneit and Volker Tresp. Averaging, maximum penalized likelihood and Bayesian estimation
for improving Gaussian mixture probability density estimates. IEEE Transactions on Neural
Networks, 9(4):639-650, 1998. ISSN 10459227. doi: 10.1109/72.701177.
Jose C. Pinheiro and Douglas M. Bates. Approximations to the log-likelihood function in the
nonlinear mixed-effects model. Journal of Computational and Graphical Statistics, 4(1):12-35,
1995. ISSN 15372715. doi: 10.1080/10618600.1995.10474663.
Rafael Coimbra Pinto and Paulo Martins Engel. A fast incremental Gaussian mixture model. PLoS
ONE, 10(10), 2015. ISSN 19326203. doi: 10.1371/journal.pone.0139931.
Eitan Richardson and Yair Weiss. On GANs and GMMs. Advances in Neural Information Processing
Systems, 2018-December(NeurIPS):5847-5858, 2018. ISSN 10495258.
Mingzhou Song and Hongbin Wang. Highly efficient incremental estimation of Gaussian mixture
models for online data stream clustering. Intelligent Computing: Theory and Applications III,
5803:174, 2005. doi: 10.1117/12.601724.
J. J. Verbeek, N. Vlassis, and B. J.A. Krose. Self-organizing mixture models. Neurocomputing, 63
(SPEC. ISS.):99-123, 2005. ISSN 09252312. doi: 10.1016/j.neucom.2004.04.008.
Sethu Vijayakumar, Aaron D’Souza, and Stefan Schaal. Incremental online learning in high di-
mensions. Neural Computation, 17(12):2602-2634, 2005. ISSN 08997667. doi: 10.1162/
089976605774320557.
Nikos Vlassis and Aristidis Likas. A greedy EM algorithm for Gaussian mixture learning. Neural
Processing Letters, 15(1):77-87, 2002. ISSN 13704621. doi: 10.1023/A:1013844811137.
Tao Wang, David J Wu, Adam Coates, and Andrew Y Ng. End-to-end text recognition with
convolutional neural networks. In Proceedings of the 21st International Conference on Pattern
Recognition (ICPR2012), pp. 3304-3308. IEEE, 2012.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a Novel Image Dataset for Bench-
marking Machine Learning Algorithms. pp. 1-6, 2017. URL http://arxiv.org/abs/
1708.07747.
Yaroslav Bulatov. Machine Learning, etc notMNIST dataset, 2011. URL http://yaroslavvb.
blogspot.com.br/2011/09/notmnist- dataset.html.
10
Under review as a conference paper at ICLR 2021
A Supplementary Material
A.1 Assumptions made by EM and SGD
The EM algorithm assumes that the observed data samples {xn} depend on unobserved latent
variables zn in a non-trivial fashion. This assumption is formalized for a GMM with K components
by formulating the complete-data likelihood in which zn ≡ zn ∈ {0, . . . , K - 1} is a scalar:
p(xn , zn ) = πzn Nzn (xn )	(6)
where We have defined Nk(Xn) = N(xn； θk, μQ for brevity. It is assumed that the Zn are Unob-
servable random variables whose distribution is unknown. Marginalizing them out gives us the
incomplete-data likelihood p(xn) = Pk p(xn, zn). The derivation of the EM algorithm starts out
with the total incomplete-data log-likelihood
L = logp(X) = log	p(xn) =	logp(xn)
nn
log	p(xn, zn = k)
(7)
X log XXP(Zn= k)⅜z⅛k2
Due to the assumption that L is obtained by marginalizing out the latent variables, an explicit
dependency on Zn can be re-introduced. For the last expression, Jensen’ inequality can be used to
construct a lower bound:
L=~ X log X " k) ¾n⅛=k)
≥ XX P9n = k)log ⅞⅛k2 .
(8)
Since the realizations of the latent variables are unknown, we can assume any form for their distribu-
tion. In particular, for the choice P(Zn)〜p(xn,, Zn), the lower bound becomes tight. Simple algebra
and the fact that the distribution p(Zn) must be normalized gives us:
P(Zn = k, xn )
P(Zn = k=	P(Xn)
= P(Zn = k |xn )
_ P(Zn = k, Xn)	(9)
Pl P(Zn = l, Xn)
_ ∏kNk (Xn)
=Pl ∏lNl(Xn)
where we have used Eq. (6) in the last step. P(Zn = k|Xn) is a quantity that can be computed from
data with no reference to the latent variables. For GMMs it is usually termed responsibility and we
write it as P(Zn = k|Xn) ≡ γnk.
However, the construction of a tight lower bound, which is actually different from L, only works
when P(Xn , Zn ) depends non-trivially on the latent variable Zn . If this is not the case, we have
P(Xn, Zn) = K-1P(Xn) and the derivation of Eq. (8) goes down very differently:
L〜
logP(Xn) ≥	P(Zn = k) log
P(Xn,Zn = k)
P(Zn = k)
X X P(Zn = k)log ⅛7
Xlog K-1P(Xn) - X P(Zn = k)logP(Zn = k)
(10)
n
k

X (log P(Xn)- (log K -H[Zn]))
n
11
Under review as a conference paper at ICLR 2021
where H represents the Shannon entropy of p(z). The highest value this can have is log K for an
uniform distribution of the zn, finally leading to a lower bound for L of
L ≥ X log p(xn) = L	(11)
which is indeed tight, but trivial, and thus does not simplify the problem at all. In particular, no
closed-form solutions to the associated extreme value problem can be computed for this case.
This shows that optimizing GMMs by Expectation-Maximization assumes that each sample has been
drawn from a single element in a set of K uni-modal Gaussian distributions. Which distribution
is selected for sampling depends on a latent random variable. On the other hand, optimization
by SGD uses the incomplete-data log-likelihood L as basis for optimization, without probabilistic
interpretation. This may be advantageous for problems where the assumption of Gaussianity is badly
violated, although empirical studies indicate that optimization by EM works very well in a wide range
of scenarios.
A.2 PROOF THAT σ DEFINES AN UPPER BOUND ON Lσ
Let us assume that SGD optimization has reached a stationary point where the derivative w.r.t. all
GMM parameters is 0. In this situation, we claim that the only way to increase the loss is by
^ _ ^ _
σσ
manipulating σ. We ShoW here that d∂σ < 0∀σ > 0, and that d∂σ =° for σ =0. This means that
the loss can be increased by decreasing σ, up to the point where σ = 0.
For each sample, the 2D profile of log(πkNk) ≡ fk is assumed to be radially symmetric, cen-
tered on the best-matching component k and decreases with the distance as a function of
||k - k*||. We thus have fk = fk(r) with r ≡ ||k - k*||. Passing to the continuous domain,
the indices in the Gaussian “smoothing filter” gk*k become continuous variables, and we have
gk*k → g(||k - k*∣∣,σ) ≡ g(r,σ). Similarly, fk(r) → f(r). Using 2D polar coordinates, the
smoothed max-component likelihood Lσ becomes a polar integral around the position of the best-
matching component: Lσ 〜Rr2 g(r, σ)f (r)drdφ. We are interested in the change of Lσ when σ
undergoes an infinitesimal change. It is trivial to show that for the special case of a constant log-
probability profile, i.e., f(r) = L, Lσ does not depend on σ because Gaussians are normalized, and
that the derivative w.r.t. σ vanishes:
dLσ	∞
-----dr
dσ	0
=LZσd
0
≡ LN - LP
2
1] exP(-彳)L
2σ2
r2	∞ r2	r2
eχP(-2σ2) - L jσ (西 - 1)eχP(-2σ2)
(12)
where we have split the integral into parts where the derivative w.r.t. σ is negative(N) and positive
(P). We know that N = P since the derivative must be zero for a constant function f(r) = L due to
the fact that Gaussians are normalized to the same value regardless of σ .
For a function f(r) that satisfies f(r) > L∀r ∈ [0, σ[ and f(r) < L∀r ∈]σ, ∞[, the inner and outer
parts of the integral behave as follows:
N=I	g(r)(σ2	-	1]f(r)	</	g(r)(σ2	- DL =	LN
P = /∞ g(r)(鼻-1] f (r) < fσ∞ g(r)(W - DL = LP
(13)
since f(r) is minorized/majorized by L by assumption, and the contributions in both integrals have
the same sign for the whole domain of integration. Thus, it is shown that, for σ > 0
dLσ	~一~
——=N -P < LN — LP = 0
dσ
(14)
Λ Γ∙ .1	.1	. .1 •	1	∕'	CF	Λ∕T	1	1	1	/`	•
and, furthermore, that this derivative is zero for σ = 0 because Lσ no longer depends on σ for this
case.
12
Under review as a conference paper at ICLR 2021
Taking everything into consideration, in a situation where the log-likelihood Lσ has reached a
stationary point for a given value of σ, we have shown that:
•	The value of Lσ depends on σ.
•	Without changing the log-probabilities, We can increase Lσ by reducing σ, assuming that the
log-probabilities are mildly decreasing around the BMU.
•	Increasing Lσ works as long as σ > 0. At σ = 0 the derivative becomes 0.
Thus, σ indeed defines an upper bound to Lσ which can be increased by decreasing σ. The assumption
of log-probabilities that decrease around the best matching unit (BMU) is reasonable since such a
profile maximizes Lσ. All functions f (r) that, e.g., decrease monotonically around the BMU, fulfill
this criterion, where the precise form of the decrease is irrelevant. This proof works identically when
not resorting to integrals but using discrete sums.
A.3 Log-Likelihood Gradients
ʌ
The gradients of L read
ʌ
∂L
∂μk
∂ L
西
ʌ
∂L
∂∏k
En [Pk (Xn - μk) δkk*]
En [((Pk ) 1 - (xn - μk )(xn - μk)T) δkk* ]	(15)
πk-1En [δkk*] .
The gradients of L are obtained by replacing δkk* by the standard GMM responsibilities γnk. For the
case of a degenerate solution when optimizing L, only a single component k has a weight close to 1,
with its centroid and covariance matrix are given by the mean and covariance of the data: πk* ≈ 1,
μk* = E[X], P-I = Cov(X). In this case, the gradients w.r.t μ and P vanish. The gradient w.r.t.
πk does not vanish, but is δkk* which vanishes after enforcing the normalization constraint.
13
Under review as a conference paper at ICLR 2021
A.4 Visualization of 2D Annealing Grid
In Figure 4, three different states of the gk are visualized, depending on σ(t). Darker pixels indicate
larger values. Each gk is assigned to a single GMM component k, which is Why the gk are arranged
on the same √K X √K grid we place the components themselves. An intuitive interpretation of a
particular gk is that it encodes the contribution of neighbouring component log-probabilities on the
log-probability of component k entering into max-computation. Over time, σ(t) is reduced (middle
and right pictures) and thus only component k contributes. Please note that the grid we place the
components on is periodic for simplicity, so the gk are themselves periodic.
Figure 4: Visualization of different centered Gaussian function gk states controlled by σ.
14
Under review as a conference paper at ICLR 2021
A.5 Diagrams of Clustering Comparison
MNIST
5 0 5
3工 2
3joos mfplnOq səɪʌsp
0.20-
I 0.15 -
P
o.ιo-
50000	100 000	150000	0
iteration
50000	100000	150000
iteration
FashionMNIST
O 5
3Z
0X0。Spoq SOlABP
1500000
500 000	1000000
iteration
×9puι UUnP
NotMNIST
O 5
3Z
0X0。S UlPlnoq SOIAp
1500000
500 000	1000000
iteration
ɔeəpuɪ UUnP
Devanagari
10 000 20000 30 000 40000 50000
iteration
,DOS mfplnOq səɪʌBP
0	10000 20000 30000 40000 50000
iteration
SVHN
Figure 5: Trend of clustering capabilities for SEM and SGD trained GMM. Comparison by Davies-
Bouldin score (smaller is better) and Dunn index (higher is better) for the datasets MNIST, Fash-
ionMNIST, NotMNIST, Devanagari and SVHN. The lines visualize the average metric score/index
values of 10 repetitions with its standard deviation.
15
Under review as a conference paper at ICLR 2021
A.6 EFFECT OF THE GMM COMPONENT NUMBER K
The number of Gaussian components K is a key parameter for any GMM and has a huge impact
on performance. It should be stated clearly that this discussion is not specific any particular fashion
of performing GMM training, be it by SGD, sEM or EM since all of these methods optimize the
log-likelihood. This is why we do not propose a particular way of choosing K for SGD-trained
GMMs. For clustering, K can be chosen using standard techniques like BIC or AIC, in addition
to priors depending on the data and the concrete application in mind. For density estimation, it is
generally assumed that K should be set as high as possible since the real distribution of the data can be
approximated in better detail. This set of experiments aims at showing empirically, for completeness,
that this ”bigger is better” relation holds when training GMMs by SGD. We use the hyper-parameter
settings stated in Sec. 4, vary K and record the final log-likelihoods. Results are shown in Tab. 4
and suggest a very clear relationship between K and the (test) log-likelihood obtained at the end of
training. We run the experiments for a larger number of epochs to exclude that (non-)convergence
effects could play a role here.
Table 4: Log-likelihood values after 50 training epochs using different number of Gaussian compo-
nents K ∈ {25, 36, 49, 64, 81}. Best values of the final log-likelihood are in bold face.
^^^^omPonentS Dataset	K = 52 mean	K = 62 mean	SGD K = 72 mean	K = 82 mean	K = 92 mean
MNIST	-205.4	-215.2	-222.7	-229.6"	-235.1
FaShionMNIST	-215.9	-250.5"	-266.4	-273.9	-281.8
Devanagari	--15.7	-0.7	20.9	24.3"	6677
FruitS 360	12 095.7	13003.0	12 953.2	12 866.6	13193.7
16