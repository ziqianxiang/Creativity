Under review as a conference paper at ICLR 2021
Max-sliced Bures Distance for Interpreting
Discrepancies
Anonymous authors
Paper under double-blind review
Ab stract
We propose the max-sliced Bures distance, a lower bound on the max-sliced
Wasserstein-2 distance, to identify the instances associated with the maximum
discrepancy between two samples. The max-slicing can be decomposed into two
asymmetric divergences each expressed in terms of an optimal slice or equiva-
lently a ‘witness’ function that has large magnitude evaluations on a localized
subset of instances in one distribution versus the other. We show how witness
functions can be used to detect and correct for covariate shift through reweighting
and to evaluate generative adversarial networks. Unlike heuristic algorithms for
the max-sliced Wasserstein-2 distance that may fail to find the optimal slice, we
detail a tractable algorithm that finds the global optimal slice and scales to large
sample sizes. As the Bures distance quantifies differences in covariance, we gen-
eralize the max-sliced Bures distance by using non-linear mappings, enabling it
to capture changes in higher-order statistics. We explore two types of non-linear
mappings: positive semidefinite kernels where the witness functions belong to a
reproducing kernel Hilbert space, and task-relevant mappings corresponding to a
neural network. In the context of samples of natural images, our approach pro-
Vides an interpretation of the Frechet Inception distance by identifying the Syn-
thetic and natural instances that are either over-represented or under-represented
with respect to the other sample. We apply the proposed measure to detect imbal-
ances in class distributions in various data sets and to critique generative models.
1	Introduction
Divergence measures quantify the dissimilarity between probability distributions. They are funda-
mental to hypothesis testing and the estimation and criticism of statistical models, and serve as cost
functions for optimizing generative adversarial neural networks (GANs). Although a multitude of
divergences exists, not all of them are interpretable. A divergence is interpretable if can be expressed
in terms of a real-valued witness function ω(∙) whose level-sets identify the specific subsets that are
not well matched between the distributions, specifically, subsets which have much higher or much
lower probability under one distribution versus the other. Localizing these discrepancies is useful
for understanding and compensating for differences between two samples or distributions, to detect
covariate shift (Shimodaira, 2000; Quionero-Candela et al., 2009; Lipton et al., 2018) or to evaluate
generative models (Heusel et al., 2017).
While many divergences can be posed in terms of witness functions, not all witness functions are
readily obtained or interpreted. From an information-theoretic perspective, the most natural witness
function is the logarithm of the ratio of the densities (Kullback & Leibler, 1951) as in the Kullback-
Leibler divergence. Applying other convex functions to the density ratio constitutes the family of
f-divergences (Ali & Silvey, 1966; Renyi, 1961), which include the Hellinger, Jensen-Shannon,
and others. However, without a parametric model estimating the densities from samples is challeng-
ing (Vapnik, 2013). Following Vapnik’s advice to “try to avoid solving a more general problem as an
intermediate step,” previous work has sought to directly model the density ratio via kernel learning
(Nguyen et al., 2008; Kanamori et al., 2009; Yamada et al., 2011; 2013; Saito et al., 2018; Lee et al.,
2019) or to estimate an f -divergence by optimizing a function from a suitable family (Nguyen et al.,
2010) such as a neural network Nowozin et al. (2016).
1
Under review as a conference paper at ICLR 2021
Witness functions need not rely on the density ratio. A wide class of divergences called integral
probability metrics (IPMs) (Muller, 1997), which include total variation, the Wasserstein-I distance,
maximum mean discrepancy (MMD) (Gretton et al., 2007), and others (Mroueh et al., 2017), seek
a witness function that maximizes the distance between the first moments of the witness function
evaluations. In these cases the optimal witness function ω*(∙) has a greater expectation in one
distribution compared to the other distribution. An IPM between two measures μ and V is expressed
as sυPω∈F ∣Eχ 〜μ[ω(X)] - EY 〜V [ω(Y )]| for a given family of functions F.
A class of related divergences are the max-sliced Wasserstein-p distances, which seek a lin-
ear (Deshpande et al., 2019) or non-linear slicing function (Kolouri et al., 2019) that maximizes
the Wasserstein-p distance between the witness function evaluations for the two distributions. How-
ever, there are two difficulties with computing the max-sliced Wasserstein distance for two samples.
The first is that it is a saddlepoint optimization problem, whose objective evaluation requires sorting
the samples. Previous work has sought to approximate it using a first moment approximation (Desh-
pande et al., 2019) or to use a finite number of steps of a local optimizer (Kolouri et al., 2019),
without any guarantee of obtaining an optimal witness function. Another difficulty is in the inter-
pretation of the obtained witness function. Unlike the density ratio, there is no notion of whether
the witness function will take higher values for points associated to one distribution versus the other.
To address both of these issues we propose a max-sliced distance that replaces the Wasserstein-2
distance with a second-moment approximation based on the Bures distance (Dowson & Landau,
1982; Gelbrich, 1990). The Bures distance (Bures, 1969; Uhlmann, 1976) is a distance metric be-
tween positive semidefinite operators. It is well-known in quantum information theory (Nielsen &
Chuang, 2000; Koltchinskii & Xia, 2015) and machine learning (Brockmeier et al., 2017; Muzellec
& Cuturi, 2018; Zhang et al., 2020; Oh et al., 2020; De Meulemeester et al., 2020).
1.1	Contribution
We propose a novel IPM-like divergence measure, the “max-sliced Bures distance”, to identify
localized regions and instances associated with the maximum discrepancy between two samples.
The distance is expressed as the maximal difference between the root mean square (RMS) of
the witness function evaluations sυPω∈s IPEX〜μ[ω* * 2(X)] - PEY〜ν[ω2(Y)] ∣, where S is an ap-
propriate family of functions. As ∣∆∣ = max{∆, -∆}, the max-sliced Bures can be expressed
as the maximum of one-sided max-sliced divergences with optimal witness functions, ω*>ν =
argmaxω∈s ,Eμ[ω2(X)]—pEv[ω2(Y)], and3“<“ = argmaXω∈s PEν[ω2(Y)]-,Eμ[ω2(X)].
If the distributions are not well-matched, then ω*>v has large magnitude function evaluations under
a ‘localized, subset of μ and smaller magnitude values for V, and the opposite for ω*<v. The two
samples {xi}im=1, {yi}in=1 can be sorted by the magnitude of the witness function evaluations.1
Crucially, we detail a tractable optimization procedure that is guaranteed to yield a global optimum
witness function for the one-sided max-sliced Bures divergence. When X = Rd and the first or
second moments distinguish the distributions, linear witness functions can be used S = {ω(∙)=
h∙, Wi : W ∈ Sd-1}, where SdT denotes the unit sphere in Rd. The optimal witness function for the
one-sided max-sliced Bures divergence ω*>v(∙) =(•，Wμ>vi coincides with the subspace with the
greatest difference in RMS, Wμ>v = argmaXw∈sd-ι pw>E[XX>]w - pw>E[Yγ>]w. This
optimization problem depends on the dimension d; after computation of the covariance matrices,
it is independent of the sample sizes m ≥ n. In comparison, the optimal slice for the max-sliced
Wasserstein may not be obtained, and even gradient ascent to a local optimum requires O(m log m)
at each function/gradient evaluation. Furthermore, the slice that maximizes the max-sliced Wasser-
1 Four groups of ‘witness points’ (top-K instances) can be inspected to identify any discrepancies:
ω2>ν(X`(I)) ≥ …≥ ω2>ν(X`(K))	f	ω2>ν(yσ(i)) ≥ …≥ ω2>ν(yσ(κ))	,
v------------------{z-----------------}	V------------------{z-----------------}
` sorts {xi}m=ι to reveal examples from μ with large 3、>丫	σ sorts {yi}n=ι to find the examples from V with large ωj>ν
ω2<ν(χ∏(i)) ≥ …≥ ω2<ν(χ∏(κ))	f	ω2<ν(yσ(i)) ≥ …≥ ω2<ν(yσ(κ))	,
V------------------{z-----------------}	V-------------------------------------}
´ sorts {xi}m=ι to find examples from μ with large ωj<ν	σ sorts {yi}n=ι to find the examples from V with large 32<丫
(1)
(2)
1	X ZX Zl	.	. . ■	1 ɑ	1	1	，	, 1 ■	1 ■ , ■	■ , 1	1	1 " I'I'
where π,π,σ,σ denote permutations and》and《denote expected inequalities with a large difference.
2
Under review as a conference paper at ICLR 2021
引⑴,∙ ∙ ∙
3⅛(1),・・・，¾(10)
^⅛(1),∙∙∙ Real>Fake
QqQQg
仍⑴,…Real<Fake
雄⑴？ IqJr£。0)
*cπ(l) r r r ∙ Real>Fake
⅛⅛h • •. __
⅛3
妫⑴,… RealvFake
2∕σ(l)5 ・∙・

4


Figure 1: The magnitude of the witness functions obtained from max-sliced Bures indicate dis-
crepancies. (Left) A Gaussian kernel is used to construct non-linear witness functions and identify
witness points. (Right) Witness points for real and fake images from stacked MNIST and CIFAR10.
In each of the 6 frames, instances corresponding to the left hand sides of equation 1 and equation 2
are on the top; the bottom instances correspond to the right hand sides of the expected inequalities.
stein lacks an intrinsic ordering, and it is left to the user to determine whether instances from μ or V
have high or low values or magnitudes.
As second-order moments may be insufficient for distinguishing the distributions, we explore
non-linear mappings of the random variables. Firstly, we consider a reproducing kernel Hilbert
space (RKHS) H with the family of witness functions S = {ω(∙) = hφ(∙), ω)H : ω, φ(∙) ∈
H, hω, ωiH = 1}. An example with Gaussian kernels is shown in Figure 1. Secondly, we use a
pre-trained neural network to create a task-relevant mapping, computing the second-order statistics
of the hidden-layer activations, and apply this in the context of samples of natural images. This
enables interpretation of the FreChet Inception distance (FID) (Heusel et al., 2017) by identifying
the subspace and images associated with discrepancies between synthetic and natural images. We
prove that the max-sliced Bures distance provides a lower bound on the max-sliced FreChet distance.
Because of their similarity, we develop the max-sliced Bures distance in the context of max-sliced
versions of the total variation and Wasserstein-2 distances. The kernel-based versions of these are
novel contributions themselves. The max-sliced total variation distance is a special case of the
covariance feature matching proposed by Mroueh et al. (2017).
In experimental results, we show applications of the linear and kernel-based versions to detect im-
balances in class distributions of natural images and to critique GANs. We compare to other diver-
gences expressed in terms of witness functions including MMD. Finally, we propose algorithms to
reweight an empirical distribution in order to minimize max-sliced divergences (with applications to
generating conditional distributions and covariate shift correction).
2	Methodology
Consider a topological space X, a Borel σ-algebra BX , and the set Pr(X) of Borel probability
measures on X. Let μ,ν ∈ Pr(X) denote two probability measures, and X 〜μ and Y 〜 V
be two random variables X, Y ∈ X . Let κ denote a positive-definite, bounded kernel function
κ : X × X → B ⊂ R. For any κ, there is an implicit mapping φ : X → H that maps any element
x ∈ X to an element in the reproducing kernel Hilbert space (RKHS) φ(x) ∈ H such that κ(x, y) =
hΦ(x),φ(y))H = hκ(∙,x),κ(∙,y),H for x, y ∈ X, and ∀ω ∈ H,ω(x) = hω, k(∙,x))h (Aron-
szajn, 1950). ∀ω ∈ H, ∣∣ω∣∣2 = ʌ/hω, ω)H. When clear, We drop the H subscript on the inner
product. A rank-1 RKHS operator is denoted as ω ③ ψ ∈ H × H with h(ω 脸 ψ)φ(x),φ(y)i =
hψ, φ(x)ihω, φ(y)i = ψ(x)ω(y) for x, y ∈ X. Denote by mχ = EX〜〃[φ(X)] ∈ H and
mγ = E Y〜V [φ(Y)] ∈ H the first moments of the random variables in the RKHS. The uncen-
tered second moments are ρχ = E [φ(X) 0 φ(X)] ∈ H × H and PY = E [φ(Y) 0 φ(Y)] ∈H×H.
The covariance operators are Σχ = ρχ — mχ 0 mχ and Σy = ργ — mγ 0 mγ.
3
Under review as a conference paper at ICLR 2021
2.1	Divergences as Distance Metrics
Let D(μ, V) denote a divergence D : Pr(X) X Pr(X) → [0, ∞). It is a distance metric between
measures (a probability metric) if all of the following statements hold: (i) μ = V =⇒ D(μ, V) = 0,
(ii) D(μ,ν) = 0 =⇒ μ = v, (iii) D(μ,ν) = D(ν,μ), (iv) D(μ,ν) ≤ D(μ,ξ) + D(ν,ξ). It
is a semi-metric if all properties aside from (ii) hold. Muller (1997) defines the class of integral
probability metrics as the supremum of the absolute difference between expectations
DF(μ, v) = sup jʃ ω(x)dμ(x) — / ω(x)dv(x)
sup E[ω(X)] - E[ω(Y)].
ω∈F
With appropriate choice of the family of functions F, this form yields well-known diver-
gences (Sriperumbudur et al., 2010), e.g., when F is the set of functions with Lipschitz constant
less than 1, the resulting divergence is the Wasserstein-1 distance metric. Another example of an
IPM is when F = {ω ∈ H : kωk2 ≤ 1}, which yields MMD (Gretton et al., 2007), defined as
DHMD(μ, V)	= SUp	{E[ω(X)]	—	E[ω(Y)]	= E[hΦ(X)	—	φ(Y),ω>]}	= ∣∣mχ	- mγ∣∣2
ω∈H"∣ω∣∣2≤1
=	EX~μ,X0~μ[κ(X, X 0)] + EY ~ν,Y 0~ν [κ(Y,Y 0)] - 2Eχ~μ,γ ~” [κ(X,Y)]. (3)
For characteristic kernels such as the Laplacian and Gaussian kernels, the mean embedding
Ex~μ[Φ(X)] : Pr(X) → H is an injective function (Sriperumbudur et al., 2008; FUkUmizU et al.,
2009; Sriperumbudur et al., 2010), capturing the full statistics of μ. In these cases, MMD is a dis-
tance metric on Pr(X); likewise, distance metrics between the operators PX = E [φ(X) 0 φ(X)]
and PY = E [φ(Y) 0 φ(Y)] induce probability metrics for characteristic kernels (Zhang et al., 2020).
2.2	Operator Distances for Defining Divergences
Total variation (TV) is a well-known probability metric and an integral probability metric (Muller,
1997), taking the form (1/2) Pi |pi — qi| for discrete measures, for which Pi = μ(xi) and q% = V(Xi)
where {xi}i = X. The TV distance between operators in the RKHS is a divergence
DTV (μ, v) , dTV (PX ,PY ) , 2 IlPX - PY∣∣1,	(4)
where ∣∣∙∣ι denotes the trace norm (Schatten 1-norm), which is the sum of the singular values.
The Bures distance generalizes the Hellinger distance a∕(1∕2) Pi(√pi — √¾)2 to positive
semidefinite operators (Fuchs & Van De Graaf, 1999; Bromley et al., 2014; Bhatia et al., 2019).
The kernel Bures divergence DH (μ, V) and the Bures distance dβ (ρx ,Py) are defined as
DH(μ, V) = dB(PX,Py) , q∕∣Pxkι + kPYkι—2 k√PX√pY∣1.	(5)
The Bures distance is used to define the Wasserstein-2 (W2) distance between Gaussian measures,
i.e., the FreChet distance (Frechet, 1957; Dowson & Landau, 1982). The multivariate FreChet dis-
tance provides a lower bound for the W2 distance (Gelbrich, 1990).2 The kernel Gauss-Wasserstein
distance (Zhang et al., 2020; Oh et al., 2020) is defined as
Dgw(μ, V), ∕kmX — mγ∣2 + dB(ςx, ςy) = ∕∖dMMD(μ, V)]2 + dB(ςx, ςy).	(6)
Zhang et al. (2020) also proposed the kernel Wasserstein-p distance between μ and v,
1
wH(μ,v) ,( inf、,VE、[dK(x,Y )])p, P ≥ 1,	⑺
∖y∈γW,V)(X,γ hγ	)
where Γ(μ, V) defines the set of all joint distributions coupling μ and V, and dκ(χ, Y) = ∣∣φ(X)—
φ(Y)∣∣p.Forp = 2, dK(X,Y) = κ(X,X) + κ(Y,Y) — 2κ(X,Y). WhenP = 2 and φ(x) → X ∈ Rd
such that H = Rd, the standard W2 distance WR (μ, V) is obtained.
2In the finite-dimensional case, the multivariate FreChet distance (squared) is often expressed as ∣∣mχ 一
mγ∣∣2 +tr(Σχ + Σγ — 2√ΣχΣγ); the trace term is the squared Bures distance dB (Σχ, ΣY) = tr(ΣX +
Σγ) 一 2 ∣∣√Σχ√Σγ∣∣ι, where ∣∣√Σχ√Σγ心=tr(√∑XΣγ) (Dowson & Landau, 1982).
4
Under review as a conference paper at ICLR 2021
2.3 Divergences based on slicing Hilbert spaces
The sliced Wasserstein distance (Wu et al., 2019; Deshpande et al., 2018; Kolouri et al., 2018), and
max-sliced Wasserstein distance (Deshpande et al., 2019; Kolouri et al., 2019) evaluate discrepancies
in linear or non-linear one-dimensional subspaces. A motivation for this is the analytic solution
of the Wasserstein-p distance in one dimension. The max-sliced Wasserstein-p distance takes the
form max-WpR (μ,ν) H supw∈sd-ι infγ∈r(μ,ν) E (x,y)〜Y[〈X - K w〉p], P ≥ 1. Similarly, We
propose the max-sliced Bures, the kernel TV, and the kernel Wasserstein-p distances using the rank-
1 operator Ω = ω 0 ω ∈H×H, which projects (slices) the RKHS along a one-dimensional
subspace defined by the ray ω ∈ H, With hω, φ(X)i = ω(X), due to the reproducing property. In
this formulation, ω : X → R is the witness function from the set S= {ω ∈ H : kωk2 = 1}. Notably,
a linear slice in the RKHS is a possibly non-linear function in the input space.
For conciseness, we denote the mean square witness function evaluations E [ω2(X)] = hω, ρXωi =
k√Pxωk2 as ∣∣ωkμ, and E[ω2(Y)] = hω,ργω) = ∣∣√ρYω∣∣2 as ∣∣ω∣∣V.TheRMS ∣∣ω∣∣μ is an L2
semi-norm on ω induced by the positive semidefinite operator √ρX. The max-sliced kernel TV,
Bures, and W2 distances, derived in appendix A.1, are expressed as
max-DTV(μ,ν), 1maχ卜Upkωkμ - kωkν, supkωkν - kωkμ∖,
2	ω∈S	ω∈S
max-DH(μ, V)，max S sup∣∣ω∣∣μ 一 ∣∣ω∣ν, sup∣∣ω∣∣ν 一 ∣∣ω∣∣μ 卜,and
ω∈S	ω∈S
maxw兴μ V), ωus ∕ωkμ+kωkν - γ∈sup,ν)(χ‰ [2ω(X )ω(γ)],
(8)
(9)
(10)
respectively. The inner supremums in equation 8 and equation 9 are the one-sided divergences.
The max-sliced TV distance is an IPM with F = {ω2(x) = hφ(x), ωi2 : ∀ω ∈ S} and is a special
case of the IPMΣ divergence proposed by Mroueh et al. (2017). While not an IPM, the max-sliced
kernel Bures distance can be directly related to max-sliced versions of the TV, Frechet, and W2
distance, as detailed by the following results.
Theorem 1.	The square of the max-sliced Bures distance in the RKHS H is less than or equal to
twice the max-sliced TV distance, max- DH(μ, V) ≤ y 2 (max- DTV (μ, V)) ∙
Theorem 2.	The max-sliced Bures distance in the RKHS H is a lower bound on the kernel max-
sliced Gauss-Wasserstein distance, max- DH(μ, V) ≤ max L - DGW (μ, V) ≤ max U - DGW (μ, V).
Theorem 3.	The max-sliced Bures distance in the RKHS is a lower bound on the kernel max-sliced
W2 distance, max-DG(μ, V) ≤ max-Wτ(μ, V).
These results trivially translate to the linear kernel case H = Rd, S = Sd-1, ω(x) = hw, xi, for
w, x ∈ Rd and X ⊆ Rd . The latter two show that the max-sliced Bures distance is a lower-bound
on the max-sliced FreChet distance, which is a lower-bound on the max-sliced W2 distance. The
proofs and other relationships among the divergences are in Appendix A.2.
2.4	Computing the Max-sliced Divergences
The max-sliced kernel Bures, TV, and W2 distances require solving optimization problems to find
the optimal witness function. As noted by others investigating IPMs (Mroueh et al., 2017; Li
et al., 2017; Kolouri et al., 2019), the witness function can be defined using a family of functions
implemented as a neural network. In this context, Goodfellow et al. (2014) use the divergence
Da(m,v) = maxω E [logω(X)] + E [log (1 一 ω(Y))], where ω : X → (0,1). In comparison,
the Wasserstein-1 distance DWι (μ, V) = suPω∈Lipi E[ω(X)] 一 E[ω(Y)] requires a Lipschitz con-
5
Under review as a conference paper at ICLR 2021
straint (Arjovsky et al., 2017; Gulrajani et al., 2017).3 Table 1 compares the form and constraints.
Table 1: Divergences written in terms of witness functions. Closed-form solutions denoted ω*.
Da(μ,ν)	=maxωs(∙)∈(0,i) E[logω(X)]+ E[log(1 — ω(Y))],	ω*(∙)=			二	dμ(∙)
				dμG)+dVG) .
Dw 1 (μ, V)	=suPω∈Lip1 E[ω(χ)] — E[ω(Y)].			
DMMD (μ,V)	=suPω∈T"∣ωk2≤1 E[ω(X)] - E[ω(Y)],	ω* =		mχ—mγ -	
			kmx-mγk2 .	
max-DTV (μ, V)	=suPω∈HRωk2≤12 ∣E[ω2(χ)] - E[ω2(Y )]|.			
max-DH(μ, ν)	=suPω∈T"∣ωk2≤1	PE[ω2(X)] — PE[ω2(Y)]	.	
2.5	Sample-Based Estimators for Max-Sliced Divergences
We consider the case of finite samples expressed as empirical measures μ = Pm=I μ%δχi and V =
Pin=1 νiδyi for the samples {xi}im=1 and {yi}in=1 with discrete probability masses denoted as column
vectors [μι,...,μm]> = μ ∈ [0,1]m, hμ, 1〉= 1, and [νι,..., νn]> = V ∈ [0,1]n, <ν, 1〉= 1.
The kernel-based max-sliced divergences optimize the witness function ω(∙) = Pli=1 α%κ(∙, Zi) in
terms of the dual variables α ∈ Rl corresponding toa subset of the pooled sample {xi}im=1∪{yi}in=1.
The optimization problems and algorithms are detailed in appendix A.4.
For clarity, we proceed to the linear kernel case fora finite-dimensional embedding φ(x) = x ∈ Rd.
After embedding, kernel evaluations correspond to vector inner-products κ(x, y) = hφ(χ), φ(y)i =
x>y. Let X = [x1, . . . , xm ] ∈ Rd×m and Y = [y1, . . . , yn] ∈ Rd×n denote the sample points
with corresponding masses μ and V, respectively. The witness function is the inner product ω(x)=
w>x, where the variable W defines the slice with ∣∣ωkμ = w>ρχW with PX = XDμX> and
∣∣ωk^ = w>ργw with PY = YDVY>, where Dv is diagonal with entries v. For i.i.d. samples,
Dμ = ml I and DV = n I. The max-sliced TV, Bures, and W2 divergences are
max-DTV(μ, ^) = max	∣w>(Px — Py)w∣ = λι(ρχ — pγ),
w"∣w∣∣2≤1
max-DB (μ, V) = max^ ∣，w>pχW — ,w>pγW∣, and
(11)
(12)
(13)
max-WRd (μ, ^)=
max w>(PX + PY)w - 2 max w>XP>Y>w,
w"∣w∣∣2≤1 V	p∈pμ,ν
where λι(∙) denotes the largest magnitude eigenvalue of the argument and P^,^ = {P ∈
[0,1]m×n∣pin = μ, p>ιm = v} is a transportation polytope.
These three optimizations differ in difficulty. The first two require only the sample means mX , mY
and covariance matrices Σχ, Σγ, since pχ = mχ mX + m-1 Σχ and PY = mγ m> + n-1 Σγ
(assuming unbiased covariance estimates). The one-sided max-sliced TV divergences can be solved
by finding the eigenvectors associated to the largest eigenvalues ofPX -PY and PY -PX. Likewise
the optimal slice for each one-sided max-sliced Bures divergence requires solving a series of eigen-
vector problems. Specifically, if PX and PY are strictly positive definite, then the optimal witness
function is ω*>ν(∙) = (wγ?, •)，where γ? ∈ (0,1] solves the optimization problem
γ
arg max
0<γ ≤1
wγ> PX wγ
wγ> PYwγ,
wγ = arg max w> (γPX - PY)w.
w:kwk2 ≤1
(14)
—
The general case involves checking the nullspace of PY and is given in the Appendix A.5. In com-
parison, the max-sliced W2 distance is a saddlepoint optimization problem (Deshpande et al., 2019).
3In the context of GANs, the sum of the one-sided max-sliced Bures divergences may prove more appro-
priate for training, since it allows for separate witness functions for over- and under-representation. However,
as its witness functions tend to localize discrepancies, even two witness functions may not make efficient
use of the generator’s samples. Instead, a distributional-version of the sliced Bures distance akin to the re-
cent distributional-sliced Wasserstein proposal (Anonymous, 2021a) or the Bures distance itself (Anonymous,
2021b) could be used. Nonetheless, this localization property is what makes the max-sliced Bures interpretable.
6
Under review as a conference paper at ICLR 2021
Following Kolouri et al. (2019), gradient ascent on w can be performed with first-order solves. Each
gradient evaluation requires solving the transport map by sorting X>w and Y>w. For this, we use
ADAM (Kingma & Ba, 2015) and quasi-Newton approaches, such as MINFUNC (Schmidt, 2012).
The same approaches can be used to approximate max-DB after smoothing √∕∙ as √∙ + 0.01.
3	Experiments
We present various examples of using the proposed max-sliced divergences to identify the discrep-
ancies between two samples. We apply the proposed approach to detect mismatched distributions of
natural and fake images using the internal representation of the Inception Network (Szegedy et al.,
2016) as in the FreChet Inception distance (Heusel et al., 2017) and the Inception score (Salimans
et al., 2016). We investigate whether the witness functions detect covariate shift caused by class
imbalances. Then, we propose optimizing the weights ν to compensate for covariate shift. Finally,
we use the one-sided max-sliced Bures divergence to monitor mode dropping during GAN training.
3.1	INTERPRETING THE FRECHET INCEPTION DISTANCE
We use a linear witness function to identify instances that are not well matched between two sam-
ples of real or fake images represented by internal activations of the Inception object classify-
ing network (Szegedy et al., 2016). Specifically, we search for witness functions with the form
ω (x) = hw, φ(x)i, where the vector φ(x) ∈ R2048 is an Inception code—the internal activations of
penultimate layer of the network after pooling (Heusel et al., 2017).
Figure 2 shows the performance of the proposed measure to identify instances associated with im-
balanced representation of particular classes. In particular, μ is a uniform sample from the training
set and V is a sample from the test set with less instances from one class. Using the one-sided
max-sliced Bures divergence we obtain the optimal slice ωμ> and apply it to the imbalanced
sample ^, identifying the top-10 witness points with the largest magnitude witness function eval-
uations ωμ>ν(yσ(i)) ≥ ... ≥ /彳>”(yσ(io)), where σ is a permutation corresponding to sorting
{yi }in=1 by descending magnitude. (While this may seem counterintuitive as it is expected that
maxι≤i≤m ωμ>ν(Xi)》maxι≤i≤n ωj>ν(y，), since ωμ> corresponds to a one-dimensional sub-
space, {y'(i)}k=i are the K instances from V with the largest norm after projection to this subspace.)
The performance is quantified by the precision of the labels of these instances (ideally, these wit-
ness points should be from the underrepresented class). Notably, the mean precision of the top-10
instances is 0.79 or better across the classes with a mean average precision (MAP) of 0.94 when the
class probabilities differ by 2% (10.2% for majority and 8.2% for minority). This is compared to a
MAP of 0.82 for the first-moment based surrogate of the max-sliced W2 distance (Deshpande et al.,
2019). Computing the max-sliced W2 distance takes much longer to run on this sample size.
Figure 2: Max-sliced divergences using the Inception Network representation are applied to samples
with mismatched class distributions in CIFAR10. The first sample consists of the training set (bal-
anced classes with m=50,000), and the second sample is an imbalanced subset of the test set with
n=10,000. Each curve is the mean precision@10 (averaged across 10 random draws) for test sets
where the given class is subsampled at different levels of imbalance and other classes are balanced.
Next we generate a set of 50,000 synthetic images for an AutoGAN instance pre-trained on CI-
FAR10 (Gong et al., 2019), which has an Inception score of 8.525 and a FID score of 12.41.
We applied both one-sided max-sliced Bures divergences to identify the two subspaces that max-
imize the difference in RMS between fake and real images. Figure 3 details the top-10 images in
7
Under review as a conference paper at ICLR 2021
each subspace and their realism scores R (Kynkaanniemi et al., 2019).4 Applying the max-sliced
Wasserstein-2 distance yielded almost the same solution as wReal<Fake (a linear correlation of 0.992).
C
A
Figure 3: Max-slicing Inception codes to illustrate the AutoGAN discrepancies. One-sided max-
sliced Bures is used to identify two witness function (as linear subspaces of the Inception codes) that
differentiate the real from fake samples. (Left: A,C) Images in subspace under-represented by fake
wReal>Fake. (Right: B,D) Images in subspace over-represented by fake wReal<Fake. (Top: A,B) Real
CIFAR10 test images. (Bottom: C,D) Fake images. (C) Realism scores (median and range): 0.92
(0.84-1.03). (D) Realism scores (median and range): 0.68 (0.62-0.73)
3.2	Baseline Comparison on Covariate Shift Detection
We compare the proposed max-sliced Bures distance and the resulting max-sliced FreChet distance to
the max-sliced W2 distance for linear witness functions. Figure 4 compares the divergence estimates
instances associated with a simple case of covariate shift with the MNIST data set. Notably, the
precision of detecting class imbalances is higher for smaller samples using a kernel (Appendix A.8).
Sample size (n = m)
≡eol= Uo-s-ndol8
0LsUoIS-oəjd e6eleAa
102
103
104
Sample size (n = m)

Figure 4: Max-sliced distances applied to two samples from MNIST. The first sample μ is m images
drawn uniformly from the training set, and the second sample V is n images from the test set where
one digit is a minority class l ∈ {0, . . . , 9} with prevalence of 5%. (Left) Divergence estimates
across sample size with l = 7. For m < 2000, gradient-based approaches for the max-sliced W2
distance fail to obtain the optimal slice as it should upper bound the max-sliced Frechet distance.
(Center) Computation time. (Right) Each curve is the average precision@10 (averaged across the
10 classes). The one-sided max-sliced Bures yields the witness function ω*>ν(∙) = hw, •)，which is
applied to reliably identify the instances from μ that are from the minority class for m ≥ 1000.
3.3	Covariate Shift Correction by Reweighting
We consider the task of reweighting the instances in one sample to minimize the max-sliced Bures
distance. This optimization problem can be expressed as mi□ν∈Rno：P. νi=ι J(V), where J(V) 8
max-DB (μ, V). As shown in appendix A.7, this is a convex minimization problem with a simplex
constraint on V. We apply the Frank-Wolfe algorithm (Jaggi, 2013) to iteratively adjust the weight
of one instance at each iteration. The performance is quantified in terms of the FreChet Inception
distance between the real-test images of the class and the reweighted sample of fake images. For
comparison, we also optimize reweightings that minimize the W2 distance and the max-sliced W2
distance (using 10 mini-batches of size n = m =100 at each iteration). The average FID distance
across the classes is 49.15 for the max-sliced Bures reweighting compared to 68.1 and 72.5 for the
mini-batch W2 and max-sliced W2. (See Table 4 in the Appendix for full results).
4We also compute the realism scores of the entire set of fake images using the set of 10,000 test images and
3-nearest neighbor distances. The Spearman rank correlation between the realism scores for the full set of fake
images and the witness function evaluations is -0.70 for ωR2eal<Fake and it is 0.17 for ωR2eal>Fake. This means the
realism score has a strong inverse correlation with ω2ew<Fake (X —R, and a weak correlation for R (X ω2eal>Fake.
8
Under review as a conference paper at ICLR 2021
Figure 5 shows results of a reweighting uniform distributions to match target distributions using
either linear slices or random Fourier bases (Rahimi & Recht, 2008) for approximating a Gaussian
kernel.5 For the latter, using the max-sliced Bures as a loss achieves the lowest reweighted W2
distance, which is computed by solving a discrete transportation problem (Flamary & Courty, 2017).
Figure 5: Reweighting a uniform distribution to match various target distributions by minimizing
the max-sliced W2 distance or the max-sliced Bures distance with either a linear kernel or ran-
dom Fourier bases (d=2,000, σ ∈ {0.1, 0.2}). Examples follow Kolouri et al. (2019) and uses
ADAM (Kingma & Ba, 2015) defaults and a learning rate of 10-2 . A point’s size is proportional to
their weights after 100 iterations. Learning curves are the weighted W2 distance (log-scale).
3.4	Detecting Mode Dropping
We create a 3-channel “Stacked MNIST” data set with 500,000 images from the MNIST training set
to test mode dropping detection throughout GAN training (DCGAN architecture). The training set
μ has 1000 possible modes corresponding to all 3-digit combinations. At the end of each training
epoch (1000 iterations), a fake sample n = 104 is generated. To verify mode coverage we use a
4-layer conv. net trained on single-channel MNIST; any missing combination of 3-digit labels is
considered a dropped mode. Figure 6 details using the proposed approach to detect missing modes.
Figure 6: Detecting dropped modes using one-sided max-sliced Bures distance. The slice wReal>Fake
is used to identify the top-10 real training images with the largest magnitude witness function values
at each epoch. Precision@10 measures the fraction that correspond to dropped modes, as compared
to random selection. The curves are the mean (A) and median (B) across 100 GAN training trials.
4	Conclusion
We propose the max-sliced Bures distance, a lower-bound on the max-sliced W2 distance, which
can be computed optimally with a tractable algorithm. We show increased performance with kernel-
based witness functions for covariate shift detection and correction, and also highlight its utility
in the linear case when the feature space is the internal representation of a pre-trained network.
Importantly, the one-sided max-sliced Bures divergences enable direct interpretation of under- and
over-representation between two samples, which can be used to identify systematic discrepancies.
5 https://github.com/anon- author- dev/gsw
9
Under review as a conference paper at ICLR 2021
References
Syed Mumtaz Ali and Samuel D Silvey. A general class of coefficients of divergence of one distri-
bution from another. Journal of the Royal Statistical Society: Series B (Methodological), 28(1):
131-142,1966.
Theodore W Anderson. On the distribution of the two-sample Cramer-von Mises criterion. The
Annals of Mathematical Statistics, pp. 1148-1159, 1962.
Anonymous. Distributional sliced-Wasserstein and applications to generative modeling. In Sub-
mitted to International Conference on Learning Representations, 2021a. URL https://
openreview.net/forum?id=QYjO70ACDK. under review.
Anonymous. The Bures metric for taming mode collapse in generative adversarial networks. In
Submitted to International Conference on Learning Representations, 2021b. URL https://
openreview.net/forum?id=3xUBgZQ04X. under review.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 214-
223, 2017.
Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American Mathematical
Society, pp. 337-404, 1950.
Rajendra Bhatia, Tanvi Jain, and Yongdo Lim. On the Bures-Wasserstein distance between positive
definite matrices. Expositiones Mathematicae, 37(2):165-191, 2019.
Austin J. Brockmeier, Tingting Mu, Sophia Ananiadou, and John Y. Goulermas. Quantifying the
informativeness of similarity measurements. The Journal of Machine Learning Research, 18(1):
2592-2652, 2017.
Thomas R. Bromley, Marco Cianciaruso, Rosario Lo Franco, and Gerardo Adesso. Unifying ap-
proach to the quantification of bipartite correlations by Bures distance. Journal of Physics A:
Mathematical and Theoretical, 47(40):405302, 2014.
Donald Bures. An extension of Kakutani’s theorem on infinite product measures to the tensor prod-
uct of semifinite W*-algebras. Transactions of the American Mathematical Society, pp. 199-212,
1969.
Hannes De Meulemeester, Joachim Schreurs, Michael Fanuel, Bart De Moor, and Johan AK
Suykens. The bures metric for taming mode collapse in generative adversarial networks. arXiv
preprint arXiv:2006.09096, 2020.
Ishan Deshpande, Ziyu Zhang, and Alexander G Schwing. Generative modeling using the sliced
Wasserstein distance. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 3483-3491, 2018.
Ishan Deshpande, Yuan-Ting Hu, Ruoyu Sun, Ayis Pyrros, Nasir Siddiqui, Sanmi Koyejo, Zhizhen
Zhao, David Forsyth, and Alexander G Schwing. Max-sliced Wasserstein distance and its use for
GANs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
10648-10656, 2019.
D. C. Dowson and B. V. Landau. The Frechet distance between multivariate normal distributions.
Journal of Multivariate Analysis, 12(3):450-455, 1982.
Remi Flamary and Nicolas Courty. POT python optimal transport library, 2017. URL https:
//pythonot.github.io/.
Maurice Frechet. Sur la distance de deux lois de PrObabilite. C. R. Math. Acad. Sci Paris, 244(6):
689-692, 1957.
Christopher Fuchs and Jeroen Van De Graaf. Cryptographic distinguishability measures for
quantum-mechanical states. Information Theory, IEEE Transactions on, 45(4):1216-1227, 1999.
10
Under review as a conference paper at ICLR 2021
Kenji Fukumizu, Arthur Gretton, Gert R Lanckriet, Bernhard SchOlkopf, and Bharath K SriPerUm-
budur. Kernel choice and classifiability for RKHS embeddings of probability distributions. In
Advances in Neural Information Processing Systems, pp. 1750-1758, 2009.
Matthias Gelbrich. On a formula for the L2 Wasserstein metric between measures on Euclidean and
Hilbert spaces. Mathematische Nachrichten, 147(1):185-203, 1990.
Xinyu Gong, Shiyu Chang, Yifan Jiang, and Zhangyang Wang. AutoGAN: Neural architecture
search for generative adversarial networks. In The IEEE International Conference on Computer
Vision (ICCV), Oct 2019.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems, pp. 2672-2680, 2014.
Arthur Gretton, Karsten M. Borgwardt, Malte Rasch, Bernhard SchOlkopf, and Alexander J. Smola.
A kernel method for the two-sample-problem. In B. Scholkopf, J. Platt, and T. Hoffman (eds.),
Advances in Neural Information Processing Systems 19, pp. 513-520. MIT Press, Cambridge,
MA, 2007.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of Wasserstein GANs. In Advances in Neural Information Processing Systems,
pp. 5767-5777, 2017.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In Advances
in Neural Information Processing Systems, pp. 6626-6637, 2017.
Jiayuan Huang, Arthur Gretton, Karsten Borgwardt, Bernhard Scholkopf, and Alex J Smola. Cor-
recting sample selection bias by unlabeled data. In Advances in Neural Information Processing
Systems, pp. 601-608, 2007.
Martin Jaggi. Revisiting Frank-Wolfe: Projection-free sparse convex optimization. In Proceedings
of the 30th International Conference on Machine Learning, pp. 427-435, 2013.
Takafumi Kanamori, Shohei Hido, and Masashi Sugiyama. A least-squares approach to direct im-
portance estimation. The Journal of Machine Learning Research, 10:1391-1445, 2009.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations (ICLR), 2015.
Soheil Kolouri, Gustavo K Rohde, and Heiko Hoffmann. Sliced Wasserstein distance for learn-
ing Gaussian mixture models. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 3427-3436, 2018.
Soheil Kolouri, Kimia Nadjahi, Umut Simsekli, Roland Badeau, and Gustavo Rohde. Generalized
sliced Wasserstein distances. In Advances in Neural Information Processing Systems, pp. 261-
272, 2019.
Vladimir Koltchinskii and Dong Xia. Optimal estimation of low rank density matrices. The Journal
of Machine Learning Research, 16:1757-1792, 2015.
Solomon Kullback and Richard A Leibler. On information and sufficiency. The Annals of Mathe-
matical Statistics, 22(1):79-86, 1951.
Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved
precision and recall metric for assessing generative models. In Advances in Neural Information
Processing Systems, pp. 3927-3936, 2019.
Zinoviy Landsman. Minimization of the root of a quadratic functional under an affine equality
constraint. Journal of Computational and Applied Mathematics, 216(2):319-327, 2008.
Chen-Yu Lee, Tanmay Batra, Mohammad Haris Baig, and Daniel Ulbricht. Sliced Wasserstein
discrepancy for unsupervised domain adaptation. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 10285-10295, 2019.
11
Under review as a conference paper at ICLR 2021
Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabas Poczos. MMD GAN:
Towards deeper understanding of moment matching network. In Advances in Neural Information
Processing Systems, pp. 2203-2213, 2017.
Zachary Lipton, Yu-Xiang Wang, and Alexander Smola. Detecting and correcting for label shift
with black box predictors. In International Conference on Machine Learning, pp. 3122-3130,
2018.
Sebastian Mika, Gunnar Ratsch, Jason Weston, Bernhard Scholkopf, and Klaus-Robert Mullers.
Fisher discriminant analysis with kernels. In Neural Networks for Signal Processing IX: Proceed-
ings of the 1999 IEEE Signal Processing Society Workshop, pp. 41-48. IEEE, 1999.
Youssef Mroueh, Tom Sercu, and Vaibhava Goel. McGan: Mean and covariance feature matching
GAN. In International Conference on Machine Learning, pp. 2527-2535, 2017.
Alfred Muller. Integral probability metrics and their generating classes of functions. Advances in
Applied Probability, pp. 429-443, 1997.
Boris Muzellec and Marco Cuturi. Generalizing point embeddings using the Wasserstein space of el-
liptical distributions. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31, pp. 10237-
10248. Curran Associates, Inc., 2018.
Yurii Nesterov. Lectures on Convex Optimization. Springer Publishing Company, Incorporated, 2nd
edition, 2018. ISBN 3319915770.
XuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence functionals
and the likelihood ratio by penalized convex risk minimization. In Advances in Neural Informa-
tion Processing Systems, pp. 1089-1096, 2008.
XuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence functionals
and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory,
56(11):5847-5861, 2010.
Michael A. Nielsen and Isaac L. Chuang. Quantum Computation and Quantum Information. Cam-
bridge University Press, 2000.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: Training generative neural sam-
plers using variational divergence minimization. In D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp.
271-279. Curran Associates, Inc., 2016.
Jung Hun Oh, Maryam Pouryahya, Aditi Iyer, Aditya P Apte, Joseph O Deasy, and Allen Tannen-
baum. A novel kernel Wasserstein distance on Gaussian measures: An application of identifying
dental artifacts in head and neck computed tomography. Computers in Biology and Medicine, pp.
103731, 2020.
Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset
Shift in Machine Learning. The MIT Press, 2009.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in
Neural Information Processing Systems, pp. 1177-1184, 2008.
Alfred Renyi. On measures of entropy and information. In Proceedings of the Fourth Berkeley
Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of
Statistics. The Regents of the University of California, 1961.
Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classifier dis-
crepancy for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pp. 3723-3732, 2018.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training GANs. In Advances in Neural Information Processing Systems,
pp. 2234-2242, 2016.
12
Under review as a conference paper at ICLR 2021
FiliPPo Santambrogio. Optimal transport for applied mathematicians. Birkauser NY, 55(58-63):94,
2015.
Friedrich Schmid and Mark Trede. A distribution free test for the two sample problem for general
alternatives. Computational Statistics & Data Analysis, 20(4):409-419, 1995.
Mark Schmidt. minFunc, 2012. Software available at http://www.di.ens.fr/~mschmidt/
Software/minFunc.html.
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-
likelihood function. Journal of Statistical Planning and Inference, 90(2):227-244, 2000.
Bharath K Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Gert Lanckriet, and Bernhard
Scholkopf. Injective Hilbert space embeddings of probability measures. In 21st Annual Con-
ference on Learning Theory (COLT 2008), pp. 111-122. Omnipress, 2008.
Bharath K Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Bernhard Scholkopf, and Gert RG
Lanckriet. Hilbert space embeddings and metrics on probability measures. The Journal of Ma-
chine Learning Research, 11:1517-1561, 2010.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-
ing the inception architecture for computer vision. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 2818-2826, 2016.
Armin Uhlmann. The transition probability in the state space of a*-algebra. Reports on Mathemat-
ical Physics, 9(2):273-279, 1976.
Vladimir Vapnik. The Nature of Statistical Learning Theory. Springer Science & Business Media,
2013.
Jiqing Wu, Zhiwu Huang, Dinesh Acharya, Wen Li, Janine Thoma, Danda Pani Paudel, and Luc Van
Gool. Sliced Wasserstein generative models. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 3713-3722, 2019.
Makoto Yamada, Taiji Suzuki, Takafumi Kanamori, Hirotaka Hachiya, and Masashi Sugiyama. Rel-
ative density-ratio estimation for robust distribution comparison. In Advances in Neural Informa-
tion Processing Systems, pp. 594-602, 2011.
Makoto Yamada, Taiji Suzuki, Takafumi Kanamori, Hirotaka Hachiya, and Masashi Sugiyama. Rel-
ative density-ratio estimation for robust distribution comparison. Neural Computation, 25(5):
1324-1370, 2013.
Zhen Zhang, Mianzhi Wang, and Arye Nehorai. Optimal transport in reproducing kernel Hilbert
spaces: theory and applications. IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 42(7):1741-1754, 2020.
A Appendix
The appendix details the derivation of the proposed divergences, formal results relating them to
existing divergences, algorithms, and additional experimental results.
A.1 Derivation of the max-sliced kernel divergences
Let Ui = {ω 0 ω : ∣ω∣2 ≤ 1,ω ∈H} denote the set of rank-1 symmetric operators with bounded
trace norm. Let d denote either the TV distance (d = dTV ) or the squared Bures distance (d = d2B),
defined in equation 4 and equation 5, respectively. In these cases, the expression of the max-sliced
distance can be simplified as
max-d(ρχ,ργ) = SuP d(ΩρχΩ,ΩργΩ) = sup d(<Ω,ρχ〉hsΩ, 〈Ω,ργ〉hsΩ)
Ω∈Uι	Ω∈Uι
=SuP δ(hΩ,ρχ〉HS, hΩ,ργ〉HS) = Sup	δ(hω,ρχω)H, hω,ρχg〉h),
Ω∈Uι	ω∈HRωk2≤l
13
Under review as a conference paper at ICLR 2021
where(•，∙)hs : (HXH) X (HXH) → R denotes the inner-product defining the Hilbert-Schmidt
(Schatten-2 norm), (Ω, PiHS = (ω 0 ω, PiHS = (ω, ρω)H, and δ(p, q) = d(pΩ, qΩ) denotes the
distance between scaled versions of Ω, for P ∈ R≥o and q ∈ R≥o. The equalities follow from the fact
that ΩρΩ = (ω 0 ω) P (ω 0 ω) = (ω, ρω)Ω. The max-sliced TV distance and squared max-sliced
Bures distance yield expressions for δ(p, q) that match the form of the underlying TV and Hellinger
divergences: dτv(pΩ,qΩ) yields δ(p, q) = 1 |p一 q|, and dB(pΩ,qΩ) yields δ(p, q) = (√p-√q)2:
max-dτv(PX,Py)，sup 1 ∣hω,ρχω> 一 <ω,ργω)∣, and	(15)
ω∈S 2
max-dB (PX,Py)，sup∣√hω,Pχωi — ∙√(ω,Pγω)∣,	(16)
ω∈S
where S = {ω ∈ H : kωk2 = 1}. The kernel-based divergences can be expressed in terms of
the witness functions since hω, PXωi = E [hφ(X), ωihφ(X), ωi] = E [ω2 (X)] and likewise for PY.
The underlying divergences and distances are listed in Table 2.
Table 2: Relationship between scalar discrepancy δ, divergence D between continuous μ, V and
discrete measures μ, V, operator dissimilarity d, max-sliced dissimilarity, and kernel max-sliced
divergence for the TV and squared Bures divergences (squared Bures is twice the squared Hellinger).
	^^ΓV	(Bures)2
δ(P, q) D(μ, V) D(μ, V) d(PX , PY)	2 |p - q|	l√p-√ql2	； 2 Pilμi 一 ViI	Pi (√μi 一 √ν^)	2 2 Rx∣dμ(x) 一 dν(x)∣	JX (Pdμ(x) 一 PdV(x)) 1 kPX -PYkI	kPXk1 + kPYki — 2 IPXpY(
max-d(PX , PY) max-D(μ, ν)	2 supω∈Slhω, ρXωi 一 hω, ρYωil	supω∈S (Phω,ρXωi 一 Phω, PYωi) 2supω∈s∣E[ω2(X)] - E[ω2(Y)]| supω,∈s (,E[ω2(X)] - ,E[ω2(Y)])2
Slicing is natural for the operator-based distances, as it is inherent in their definition such that
they coincide with the corresponding divergences between discrete probability laws (Fuchs & Van
De Graaf, 1999). The scalar discrepancy measure δ(∙, ∙) can be accumulated across a complete set of
slices to obtain the original distances. Consider a set (or countably infinite sequence) of orthogonal
trace-norm operators O = {Ωι = ωι 0 ωι, Ω2 = ω2 0 ω2,..., Ωk = ωk 0 ωk}. Since these
operators are orthogonal and have unit trace-norm IlPk=I Ωik∞ = 1,
kk
d(Px, Py) ≥ y^d(hΩi,Pχ∖hsΩi, hΩi,Pγ∖hsΩi) = ^δ(hΩi,Px〉hs, hΩi,Pγ∖hs)
i=1	i=1
k
= X δ(pi(O), qi(O)),	(17)
i=1
wherePi(O) = 0i, PX〉hs and qi(O) = 0i, PY〉hs. To obtain the equality, one must optimize O
over all possible sets of orthogonal trace-norm operators.
A.1.1 Max-sliced total variation distance
The sliced kernel total variation distance is
dτv(CPXω, ωPYω) = 2I∣ω(PX - PY)ω∣∣i = 2∣Kω,px - PY>ω∣∣i = 2Kω,px - PYMkcki.
Maximizing over slices yields
2 sup Kω,Px - Pyi|kaki = 1sup Kω,Px - Pyi| = 1 SUp lkωkμ 一 kωkνl,	(18)
2 Ω∈Uι	2 Ω∈Uι	2 ω: kωk2 ≤ 1
14
Under review as a conference paper at ICLR 2021
where the first equality follows from the fact that distance is maximized when ∣∣Ωkι = 1. This
yields the expression
max-Dtv(μ,ν), 1	SUp lkωkμ- kω∣V|.	(19)
2 ω“∣ω∣∣2 = 1
Notably, the penultimate expression in equation 18 can be related to the operator norm,
sup ∣hΩ,ρχ - PYi∣ ≤	sup hΩ,ρχ 一 PYi = IlPX - ργ∣∣∞,	(20)
Ω∈Uι	ω∈{O∈H×H"∣O∣∣i≤1}
due to the dual norm definition. Since PX - PY is symmetric, the equality is achieved. Thus,
max-DTV(μ, V) = 11lPX - PY ∣∣∞. For a linear kernel, this can be computed by finding the largest
magnitude eigenvalue of PX 一 PY = EX〜μ[XX>] 一 EY〜V[YY>] ∈ Rd×d.
A.1.2 Max-sliced Bures distance
The sliced version of the Bures distance is
dB(ΩpxΩ, ΩpyΩ) = ,∣ΩpxΩ∣i + ∣ΩpyΩ∣ι - 2∣(ΩpxΩ)2(ΩpyΩ)2∣ι,
which can be simplified since ∣∣ΩpxΩ∣i = tr((ω 0 ω)PX(ω 0 ω)) = hω, pxωikω∣∣
kωkμ∣ωk2, and ∣(ΩρχΩ) 1(ΩρyΩ)1 ∣ι = Phω,ρχωihω,ρYωi∣Ω∣ι = jɪ^而V∣Ω∣ι
l∣ωkμkω∣ν ∣ω∣∣. Using these expressions, the max-sliced Bures distance is
kωk∣ 你ωι
SUp d，B (ΩpχΩ, ΩpYΩ)
Ω∈Uι
sup
ω∈H"∣ω∣∣2≤1
+ kω∣V 一 2∣ω∣μ∣ωkν
The expression is monotonic with the norm ofω yielding
max-DH(μ,ν)，	sup	((lωkμ - lωlν)∣
ω∈HRω∣∣2≤1
SUp	lkωkμ - lωlν I
ω∈HRω∣∣2≤1
(21)
sup
ω∈H"∣ω∣∣2 = 1
JEX〜μ[ω∣(X)] 一 PEY〜ν[ω∣(Y)]
A.1.3 Max-sliced Gauss-Wasserstein distances
Two max-sliced versions of the Gauss-Wasserstein or FreChet distance in the RKHS are
maxL-DGW(μ,ν) = sup ∖八∣Ω(mχ — my)k∣ + dB(ΩΣχΩ,Ω∑yΩ)
Ω∈Uι V
sup	∣∣ω∣∣A Ihmχ 一 mY,ω>∣ + (P<ω, Σχω) 一 P(ω, ∑yω))
ω∈T"∣ω∣∣2≤1	V	'	)
sup
ω∈H"∣ω∣∣2≤1
yhmx-mY,ω∣+^ρhω∑xωf-ρω∑
∣
sup	J(EX 〜μ[ω(X)]— EY 〜ν[ω(Y )])∣+ (σω(X) — σω(Y ))： (22)
ω∈GRωk2≤1
where σω(χ) = ,E[(ω(X) — hmχ,ω>)∣] is the standard deviation of ω(X), and σω(Y) =
PE[(ω(Y) 一(my,ω>)∣], and
maxu-DGW(μ, v) = I sup ∣∣Ω(mχ — mY)∣∣ + sup dB(ΩΣχΩ,Ω∑yΩ))
∖Ω∈Uι	Ω∈Uι	J
=JMMD∣(μ, v) + sup	(σω(x) - σω(Y))∣
ω	ω∈G"3∣2≤1
={MMD∣(μ, V) + max-dB (Σχ, ∑y).	(23)
Notably, maxL-DGW (μ, V) is the supremum of the FreChet distance over all witness functions.
15
Under review as a conference paper at ICLR 2021
A.1.4 Max-sliced Kernel Wasserstein
A sliced version of the kernel Wasserstein distance between μ and V relies on the the sliced distance
dκ,ω (X, Y) = ∣∣(ω ㊈ ω) (φ(X ) - φ(Y ))k2 = khω,φ(X ) - φ(Y )iωk2
=∣hω,φ(X) - φ(Y)i∣kω∣2 = ∣hω,φ(X)i-<ω,φ(Y)i∣∣ω∣2 = ∣ω(X) - ω(Y)∣∣ω∣2.
This distance is monotonic with the norm of ω and convex with respect to ω . The max-sliced kernel
Wasserstein-p distance, p ≥ 1, is
max- WH(μ,ν) =	sup inf
ω∈H"3∣2≤1 Y∈r(μ,ν)
sup inf
ω∈H"3∣2≤1 Y∈r(μ,ν)
p
E	(dκ,ω(X,Y))p
(X,Y M	,	J
.	1
P
E	∣ω(X) — ω(Y )∣p
.(x,y )〜Y	_
(24)
which is a one-dimensional optimal transport problem. Let g#m and s#v denote the pushforward
measures, then the divergence can be written as
1
max-WpH(μ,ν) =	sup	inf	E	|S 一 T∣p	,	(25)
ω∈H"∣ω∣∣2≤1 nEn(s#*,s#V) L(S,T)~n	-
where n(s#M, s#v) is the set of all joint distributions coupling the pushforward measures.
Assuming the measures are absolutely continuous and adopting the notation from Santambro-
gio (2015), let Fω,μ(w) = R-∞ ds#M = g#m((—8,w]) and Fω,ν(W) = R-∞ ds#v =
s#v((-g,w]) denote the cumulative distribution functions of the pushforward measures with
pseudo-inverses F-}(q) = inf{w ∈ R : Fω,μ(w) ≥ q} andF-V(q) = inf{w ∈ R : Fω,ν(w) ≥ q}.
As shown in Lemma 2.8 (Santambrogio, 2015), then the optimal transport plan ∏* has cumulative
distribution Gω(WX, WY) = min{Fω,μ(wχ), Fω,ν(WY)} and the divergence is
1
P
ω∈H^∣ωk2≤1 J01
sup
F-μ(q) - F-I(q)Γ dq ,
(26)
and for the case p = 1 (Santambrogio, 2015, Proposition 2.17),
max-WH(μ, V)
sup I ∣Fω,μ(w) - Fω,ν (w)| dW.
ω∈H: ∣ω ∣2≤1 √R
(27)
The objective in the last quantity is an L1-norm version of the Cramer-von Mises criterion (Schmid
& Trede, 1995; Anderson, 1962). That is, the max-sliced kernel Wasserstein-1 distance is equivalent
to a max-sliced L1-norm version of the Cramer-von Mises criterion, where the slicing corresponds
to a function in the RKHS that witnesses the largest discrepancies between the measures. The choice
of p = 2 simplifies, yielding the following optimization problem
max- WH(μ,ν) =	sup inf ( E	Γω2(X) + ω2(Y) — 2ω(X )ω(Y)]
ω∈H"3∣2≤ιγ∈r(μ,ν) Vx,yhτ
1
2
sup
ω∈H"∣ω∣2≤1
kωkμ + kωkν- γ∈sZν)(χ,E)∕2ω (X )ω(Y)])2
sup
ω∈H"∣ω∣2≤1
kωkμ + kω∣V- 2/ F-μ(q)FC-V(q)dq).
(28)
(29)
A.2 Relationship between the kernel-based max-sliced divergences
The Bures distance can be used to lower bound the TV distance, d2B (ρX, ρY) ≤ ∣ρX - ρY∣1 =
2dTV (ρX, ρY) (Fuchs & Van De Graaf, 1999). This inequality stems from the inequality between
the squared Hellinger and total variation distances for discrete probability laws. Pi ∣∣√pi- √qi∣2 ≤
Pi∣Pi - qi|, where the inequality holds for each summand, ∣√pi - √qi∣2 ≤ ∣√pi - √¾^∣(√pi +
√qi) = |pi - qi |. This inequality holds for the max-sliced divergences as stated in Theorem 1.
16
Under review as a conference paper at ICLR 2021
ProofofTheorem 1. (∣∣ω∣∣μ - ∣∣ωkν)2 ≤ ∣∣∣ω∣∣μ - ∣∣ωkV ∣, where the inequality is a consequence of
the inequality between the arithmetic and geometric means, let a = ∣∣ω∣μ and b = ∣∣ω∣V, then
(√α - ʌ/b)2 ≤ ∣√α - λ∕b∣(√α + √b) = |a - b|. Taking the supremum over ω ∈ H with the
constraint ∣ω∣2 ≤ 1 yields the desired the result (max-DB(μ, ν))2 ≤ 2 (max-DTV(μ, ν)).
□
We now consider the relationship between the Gauss-Wasserstein or FreChet distance■—which com-
bines the distances between the first and second-order moments—with the max-sliced Bures when
it is applied directly to the uncentered covariance matrices. For this we need the following lemma.
Lemma 4 (Reverse triangle inequality). For two vectors in R2, the difference between their Eu-
Clidean norms is less than or equal to the Euclidean norm of their differences. For a, b,c,d ∈ R,
∣√a2 + b2 - √c2 + d2 ∣ ≤ P(a - c)2 + (b - d)2.
2
Proof. Let e =	(ʌ/a2 + b2 — ʌ/e2	+	d2)	and f =	(a	— c)2	+	(b —	d)2.
e = a2 + b2 + c2 + d2 — 2 P (a2 + b2)(c2 + d2)
=a2 + b2 + c2 + d2 - 2Pa2c2 + b2d2 + b2c2 + a2d2
≤a
+ b2 +
c2 + d2 - 2	a2c2 + b2d2
+ 2vo2b2c2d2, by the arithmetic and geometric mean inequality
2
=a2	+ b2	+ c2	+ d2 - 2,(√02C2 + √b2d2)2
=a2	+ b2	+ c2	+ d2 - 2(√α2C2 + √b2d2)
≤ a2	+ b2	+ c2	+ d2 - 2(ac + bd) = (a - c)2 + (b	-	d)2	= f.
Taking the square root of each side yields the inequality √e ≤ √f.
□
Proof of Theorem 2. To relate the sliced Bures and Gauss-Wasserstein distances, we note that
∣∣ω∣μ = hρχ,ω 0 ω∖HS = h∑χ + mχ 0 mχ,ω 0 ω>Hs
kωkV. Then,
∣ q∣ω∣μ- P∣ω∣ν ∣ = ∣ qhmχ ,ωi2+σ2(χ)
σω2 (X) + hmX, ωi2 and likewise for
- hmγ, ωi2 +σω2(γ)
≤	hmX - mY,ωi2 + (σω(X) - σω(Y))2,	(30)
where the inequality relies on Lemma 4, with a = hmX, ωi, b = σω(X), c = hmY , ωi, and d =
σω(γ). Taking supremum over slices yields the desired inequality.	□
Theorem 2 shows that the max-sliced kernel Bures distance is a lower-bound on the max-sliced
kernel Wasserstein-2 distance, since the latter is lower bounded by the max-sliced kernel Gauss-
Wasserstein distance.
Proof of Theorem 3. Let ω(X) = (mχ, ω) + ω(X) and ω(Y) = (mγ, ω) + ω(Y), where ω(X)=
hΦ(X) - mχ,ω and ω(Y) = hΦ(Y) - mγ,ω) are zero mean, and E [ω2(X)] = σωJ(χ) and
E [ω2(Y)] = σω2(γ). The squared Frechet distance between random variables ω(X) and ω(Y) is
(E [ω(X)] - E [ω(Y)])2 + (σω(X) - σω(γ))2 = hmX - mγ, ωi2 + (σω(X) - σω(γ))2
= hmX,ωi2 +σω2(X) + hmγ, ωi2 + σω2(γ) - 2 hmX,ωihmγ,ωi + σω(X)σω(γ)
=E [ω2(X)] + E [ω2(Y)] - 2(E [ω(X)] E [ω(Y)] + ,E [ω2(X)] E [ω2 (Y)]).
By Holder,s inequality, E [ω(X)ω(Y)] ≤ σω(χ)σω(γ)= pE [ω2(X)] E [cω2(Y)]. Consequently,
E [ω2(X)] + E [ω2(Y)] - 2(E [ω(X)] E [ω(Y)] +，E [ω2(X)] E [ω2(Y)])	(31)
≤ E [ω2(X)] + E [ω2(Y)] - 2 (E [ω(X)] E [ω(Y)] + E [ω(X)ω(Y)])
=E[ω2(X)] +E[ω2(Y)] - 2 E [ω(X)ω(Y)] =E[(ω(X) -ω(Y))2].	(32)
17
Under review as a conference paper at ICLR 2021
Taking the infimum over all possible joint distributions (X, Y)〜 Y that are within the coupling
distribution γ ∈ Γ, yields the sliced Wasserstein-2 distance on the right hand side. Maximizing
over slices, yields maxU-DGW(μ, V) ≤ max-WH(μ, V) and combining with Theorem 2 yields the
desired result.	□
A.3 Relationship to Other Divergences
Finally, we note that the terms in the max-sliced Gauss-Wasserstein divergences are related to kernel
Fischer discriminant analysis (KFDA) (Mika et al., 1999). KFDA objective is based on the ratio of
the difference in means to the pooled variances:
DHA(μ, V)
hω, mX - mYi2
Sup σω(χ)+σω(γ)
hω,mχ - mγ)2
Sup hω, (Σχ + Σγ)ωi.
(33)
KFDA seeks a witness function which has widely separated means for the two measures, and mini-
mal variance.
A.4 Computing the Max-Sliced Kernel Divergences
We assume the witness function6 ω ∈ H is of the form ω = Pn=+∣m ɑiφ(zi) with ω(∙)
Pn=+ιm αiκ(∙,zi) where Zi = (Xi,	1 ≤ i ≤ m and α ∈ Rm+n. In this case,
i=1	yi-m, m < i ≤ n + m
mm
∣∣ω∣∣^ = hω 0 ω, (φ 0 O)#2〉= fμjhω 0 ω,φ(χj) 0 φ(Xj )i = fμjhω,φ(χj )i2
j=1	j=1
m n+m	m
£〃jh£ 久心(Zi),φ(Xj )i2 = £〃j
j =1 i=1	j =1
2
αiκ(Zi, Xj)
=hμ, (KXZα)°2i
ɑ>KXz diag(μ)Kχz α = ||D^ KXZ α∣∣2,
whereκ(Zi,Zj) = Kij,K = KKXY XX,, KKYXYY
KXZ , (∙)02 denotes the elementwise squaring of
a matrix/vector, and Dv = diag(v) denotes a diagonal matrix whose diagonal entries are the vector
v. SimiIarly, ∣∣ω∣∣^ = (ν, (Kγza)。2〉= IlDVKYZα∣∣2∙
In order for the constraintIω I22 ≤ 1 =⇒ α>Kα ≤ 1 to ensure a bounded solution, we assume
K is strictly positive definite. For this purpose, we add a small value to its diagonal K + 10-9I
when necessary in the optimization procedures. For computational purposes when m or n are large,
a subset (possibly random) of landmark points of size l < n + m can be used to form the witness
function ω = Pli=1αiφ(Zτi ), where {τi}li=1 ⊂ {1, . . . , m + n}. In this case, KXZ ∈ Rm×l and
KYZ ∈ Rn×l with [KX Z]i,j = κ(Xi, Zτj ) and [KYZ]i,j = κ(yi, Zτj ). In this case, the constraint
also needs to be adjusted.
A.4. 1 Max-Sliced Kernel TV Distance
Using these expressions, the max-sliced kernel TV distance is
max-DTV(μ, ^) = max 1 IkDjSKXZa∣∣2 -IlDVKYZa∣2∣	(34)
a:a>Kα≤1 2 I	I
=max	1 ∣a>(KXZD“Kxz - K>zDVKYZ)a|.	(35)
a:a> Kα≤ 1 2
The solution is the generalized eigenvector corresponding to the largest magnitude eigenvalue of
the generalized eigenvalue problem Av = λKv, where A = KXZDmKxz 一 K>zDVKYZ.
α? = argmaXa α>Aα. The witness function is ω*(∙) = PmIn α:κ(∙,Zi).
6Similar to kernel PCA, the constraint kω k2 ≤ 1 allows the use of the representer theorem for the RKHS.
18
Under review as a conference paper at ICLR 2021
A.4.2 Max-Sliced Kernel Bures Distance
The sample-based max-sliced kernel Bures distance is
max-DH(μ, ^)
I 1	1
max H∣DμμKXZα∣∣2 - kDVKYZα∣∣2
α : α>Kα≤1 I
1	1
max max s∣∣Dμ KXZ α∣2 — SkDV KYZ α∣2
s∈{-1, + 1} ag>Kα≤1
(36)
(37)
The last expression shows that the max-sliced Bures distance can be expressed as a bilevel optimiza-
tion problem, where the inner optimization problem—which we refer to as one-sided max-sliced
Bures divergence—is a difference of convex functions:
max-DH(μ^, ^) = max < min	g(α) — h(α)
[a:a> Kα≤ 1
1
g(α) = ∣DV KYZ α∣2,
1
h(α) = ∣Dμ KXZ α∣∣2.
min	h(α) — g(α)	,	(38)
(39)
(40)
Without loss of generality, we will consider the first case (s = 1),
min
αg>Kα≤1
g(α) — h(α).
(P)
Inspired by the approach Landsman (2008), we relate this problem to a quadratic program, specifi-
cally, the quadratically constrained quadratic program
min	cιgt2(α) — c2h2(α) = min	α> (ciK>z DV KYZ - c?KXz DmKxz )α,	(Q),
a:a> Kα≤ 1	α 二 α>Kα≤1
for c1, c2 ∈ R≥0. The solution of which can be obtained as in the max-sliced kernel TV distance by
solving a generalized eigenvalue problem. For (P), the Lagrangian function is
L(α, λ) = g(α) — h(α) — λ(α>Kα — 1).	(41)
Let G = {α : g(α) > 0, h(α) > 0} denote the set of points where g and h are differentiable. Then
for α ∈ G and K positive definite, L(α, λ) is differentiable, and
VaL(α,λ) = Vα g(α) — Vαh(α) — 2λKα =	1 Vɑg2(α) —	1	Vɑh2(α) — 2λKα,
2g(α)	2h(α)
where the equality follows from Vαg2(α) = 2g(α)Vαg(α) and likewise Vαh2 (α) =
2h(α)Vαh(α). For (Q), the Lagrangian function and its gradient are
L(α, λ) = c1g2(α) — c2h2(α) — λ(α>Kα — 1),	(42)
VαL(α, λ) = c1Vɑg2(α) — c2Vαh2(α) — 2λKα.	(43)
If c1 = 2g1a) and c2 = 2h1α), then VaL(α, λ) = VaL(α, λ).
Let α* denote a global optimum of (Q). If ci = 2g(θ) and c2 = 2h(0), then VaL(α, λ) |a=a* =
VaL(α, λ) |a=a*. Consequently, α* is a local optimum of (P). By the KarUsh-KUhn-TUcker Con-
ditions, it is a necessary condition for all optima of (P) in G to have this form. Thus, any global
optimum of (P) that lies in G corresponds to a global optimum of (Q) for particular values of c1, c2.
The family of solutions to (Q) that includes all local optima of (P) in G, is
αγ = arg max γh2(α) — g2(α),	Y = c2 ∈ (0,1],	(44)
ag>Ka≤1	c1
where the bounds are due to the non-negative functions and h2(αY) ≥ g2(αY) =⇒	⅛ ≥
ɪ =⇒ ci ≥ c2 =⇒ 丝 ≤ 1. Notably, Y = 1 =⇒ ci = c2 corresponds to the one-sided
2c1	c1
max-sliced TV. The global optimum of (P) within G is necessarily within {αγ}γ∈(0,i] and can be
found as the solution to the bound scalar optimization problem miηγ∈(o,i] g(αγ) — h(a；).
A remaining case for a global optimum is a non-differentiable point α? ∈/ G, specif-
ically, g(α?) = 0 (the case of h(α?) = 0 is trivial), which corresponds to α? ∈
19
Under review as a conference paper at ICLR 2021
Null(Dν KY Z). In this case, the generalized eigenvalue problem must be restricted to the
nullspace ɑ⅛ = argmaXα∈Nuii(DνKYZ)g>Kα≤ι h2(α). Let V ∈ R(m+n)×p denote a ma-
trix of P orthonormal columns that spans the nullspace, then α% = Vβ*, where β* =
argmaxβ∈Rp0>v>κvβ≤ιkDμKXZVβk2. Overall, the one-sided max-sliced kernel Bures dis-
tance can be computed using a combination ofa line search forγ ∈ (0, 1] and checking the solution
in the nullspace as described in Algorithm 1.
1
2
3
4
5
6
7
8
1
2
3
4
5
6
7
8
9
10
Algorithm 1: One-sided max-sliced kernel Bures divergence
Input: {(xi,μi)}m=1,{(yi,νi)}n=1,κ(∙, ∙),τ ⊆ [1,m + n}
xi	i ≤ m,
i	yi-m i + 1 ≤ i ≤ m + n
Dμμ KXZ = [√μiκ(xi, zτj )]i=，1,j = i
DV KYZ = [√νiκ(yi, zτj )]i== i,j=i
A = KXZ DμKχz	'
B = KY>ZDνKYZ
K = [κ(zTi,zTj )]m,ι,jl 1
α? = ONESIDEDMAXSLICEDKERNELBURES(A, B, K)
ωμ>ν (Z) : Z → Pi=I α?K(Z, ZTi)
Output： 3强>
OneSidedMaxSlicedKernelBures (A,B,K)
α7 : γ → argmaXαg>Kα≤ι α>(γA — B)α
γ? = arg max0<γ≤1 αγ>Aαγ —	αγ>Bαγ
ɑ0
arg max α>Aα
α∈Null(B)g> Kα≤1
if ^∣α>Aα0 > Ja>? Aαγ? — Ja>?Baγ? then
I α? = α0
else
I α? = α7?
end
return α
A.4.3 Max-Sliced Kernel Wasserstein
The empirical version of max-sliced kernel Wasserstein divergence can be expressed in terms of
either equation 24 or equation 26,
1
p
max
min
ω∈H"∣ω∣∣2≤1 P∈Pμ,ν
m,n
X PijIω(xi) — ω(yj)|p
i=1,j=1
max [ IF-^(q) — F-^(q)Ipdq
ω∈H"3∣2≤1 ∣√0	ω,μ	ω,八丫川：
(45)
(46)
where P^,^ = {P ∈ [0,1]m×n∣P1n = μ, P>1m = ν} is a transportation polytope. The empirical
distribution functions are Fω,μ(W) = Pm=I μi1ω(xa)≤w and 几,ν(w) = pi=ι %Iω(ya)≤w With in-
verses F-μ(q) = min{ω(xi),i ∈ {1,...,m} : Fω,μ(ω(xi)) ≥ q} and F-^(q) = min{ω(yi), i ∈
{1,...,n} : Fω,ν(ω(yi)) ≥ q}. For fixed ω, the optimal transport plan P is based on the sorted
values ω(x(i)) ≤ ω(x(2)) ≤ …≤ ω(xg)) and ω(y.)) ≤ ω(y(2)) ≤ …≤ ω(yg)). Denote the
sorted values as vectors ωX and ωY, with [ωX]i = ω(x(i)) and [ωY]i = ω(y(i)), and denote by μ
and V the corresponding permuted versions of μ and V.
20
Under review as a conference paper at ICLR 2021
In the case of equal samples sizes of uniform measure, m = n and μ = + 1m, and V =
* 1n, elements in P^,^ are scaled elements in the Birkhoff polytope (the set of doubly stochas-
tic matrices), and the solution to the linear program is a permutation and max-WH(^, ν) =
supω∈H"3∣2≤1 { [Pm=ι ∣ω(x(i)) — ω(y(i))∣p] p = ∣∣ωX - ωγkp}, where ∣∣∙∣∣p denotes the 'p
norm in m dimensions. As in the continuous case, the discrete transport plan between the sorted
measures has the cumulative distribution G ∈ [0,1]m×n with GGi,j = min{Pk=1 μ4, Pk=I Vk}.
The optimal transport plan between the	sorted measures is	given by taking the first difference over
1	.1	1	1	i'	*A TΛ	人	TΛ	人	人 TΛ 4 4	1
both rows and columns	of	G, P1,1 =	G1,1, P1,j = G1,j	- G1,j-1, Pi,1 = Gi,1 - Gi-1,1, and
Pi,j = Pi,j - Pi-1,j -	Pi,j-1. Using	the sorted witness function evaluations the distance can be
written as
max
ω∈H"∣ω∣∣2≤1
m,n
E	Pi,j ∣ω(Xei))-My(j))∣p
i=1,j=1
(47)
We now turn our attention to the optimization of the function ω parametrized in terms of α, ω(∙)
pm=+n αiκ(∙, zi). For arbitrary sample sizes with P = 2, the max-sliced kernel W2 distance is
1
max-W2H(μ, v)=ωe7maχ2‹i (kωkμ+ι∣ωkν—2PmaX.hp,ωχω>i) ,	(48)
with unsorted values ωX = [ω(x1), . . . , ω(xm)]> = KXZα and ωγ = [ω(y1), . . . , ω(yn )]> =
KγZα.
The max-sliced kernel W2 distance can be expressed in terms of equation 45 or equation 48:
[max-wH(μ, ν)]2
m,n
maX min Pij
α : α>Kα≤1 P∈Pμ,ν
一 μ, i=1,j=1
m+n
(κ(xi, zk) — κ(yj, zk))αk
k=1
maX
α : α>Kα≤1
{α>KXz DμKχz α + α>K>z DV KYZ ɑ
— 2 maX α> K>X Z PKY Z α
P∈P^,ν
)
maX min α> QPα
α = α>Kα≤1 P∈Pμ,ν
maχmmp∈⅞/QPa
α	α>Kα
(49)
where QP = KXZDμKχz + K>zDVKYZ-KXZPKYZ-K>zP>Kxz is a symmetric matrix
positive semidefinite matrix. (This can be seen since the objective is greater than or equal to zero
for all choices of α.) For fixed P, equation 49 is a convex maximization, that can be solved as
a generalized eigenvalue problem. For fixed α, the optimization in terms of P is a linear program
with the solution detailed above. However, when maximizing with respect to α, P is a matrix-valued
function of α. To find a local maximum for α we use the unconstrained optimization equation 49
and perform gradient ascent with respect to α, wherein each iteration we compute the optimal
transport plan. This approach is also used in computing the generalized max-sliced Wasserstein
distance Kolouri et al. (2019).
A.5 Computing the Max-Sliced Divergences for the Linear Case
The linear case follows from the kernel case with some further simplification. The objec-
tives of the one-sided max-sliced Bures divergences are each a difference of convex functions,
whose stationary points are maximum eigenvalue problems, and correspond to reweighted ver-
sions of the one-sided max-sliced TV divergence. Assuming the covariance matrices are strictly
positive definite max-DBd (μ, ν)
max0<γ<1 ∣ Jw>ρχWY - Jw>ργWY ∣, where WY
argmaXw^wk2≤ι w>(γρχ - pγ )w for the one-sided case. If the matrices are singular, then cases
where w is in the nullspace must be checked. Without loss of generality, the algorithm to obtain the
optimal slice for the one-sided max-sliced Bures divergence is described in Algorithm 2.
21
Under review as a conference paper at ICLR 2021
Algorithm 2: One-sided max-sliced Bures divergence
1
2
3
4
5
6
7
8
9
10
11
12
Input: ρX , ρY ∈ Rd×d
WY ： γ → argmaxw:kwk2≤1 w>(γPX - PY)W
γ? = arg max0<γ≤1 Wγ> ρX Wγ -	Wγ> ρY Wγ
if rank(PY) = d then
I W”>ν = Wγ?
else
v = arg max	W>PX W
w∈Null(ρY ):kwk2 ≤1
if pv>ρχV > Jw”PxWγ? - Jw>?PyWγ? then
I Wμ>ν = V
else
I Wμ>ν = Wγ?
end
end
Output： Wμ>ν
As an alternative to Algorithm 2, first-order algorithms can be applied, nevertheless, obtaining a
global optimal cannot be guaranteed easily in this case. To make the objective differentiable, the
square root, which is non-differentiable at 0, should be smoothed √∙ ≈ √∙ + e2 (e.g.,a=0.01).
A.6 Sample-based Max-Sliced Bures Distance is a Relaxation of the
Max-Sliced Wasserstein-2 Distance
We show that the max-sliced Bures distance is a relaxation of the max-sliced W2 distance. Let P =
ZZ› with Z ∈ Rd×p denote a strictly positive definite matrix, such that 'w>pw > 0, 'w>pw =
√w>ZZ>w = maxθ∈sp-ι <θ, Z>w)，where θ* = argmaXθ∈sp-ι <θ, Z>w)= √ TLT = Z>w.
∈	w>ZZ>w
Using this form, when PX and PY are non-singular, the one-sided sliced Bures can be expressed as
max hθι, √Pχwi — max hθ2, √PYWi = max hθ3, DμX>wi- max hθ4, DV2 Y>wi,
θ1∈Sd-1	θ2∈Sd-1	θ3∈Sm-1	θ4∈Sn-1
1	1	1	1
since PX = XDμ (XDμ)* 1 and PY = YDV2 (YDV2)1 . Squaring this quantity and taking the square
root yields the objective of the max-sliced Bures distance in a similar form to the max-sliced W2
distance,
max-DBd(μ, ^) = max	IWT(PX + PY)w - 2 max w>XQY>w,
w"∣w∣∣2≤1 V	Q∈Q^,ν
1	1
where Q^,^ = {DμΘDV2 : Θ ∈ Rm×n, 口㊀g ≤ 1}. This holds since
-I-_1 _ - 1__-I-	. -	- 1__I-	.	. _	_ 1_-I-	.
max	w>XDμμΘDV2 Y>w = max	hθ3, DμX>wihθ4, DV2 Y>w)
Θ“∣Θ∣∣1≤1	θ3∈Sm-1,θ4∈Sn-1
=	w>PXw	w>PYw.
(50)
(51)
(52)
A.7 Covariate Shift Correction Algorithms
For covariate shift correction the goal is to minimize the divergence by adjusting the weights ν of
the instances in one sample ^. This results in the following convex optimization problem:
min	J(ν),
ν∈R≥0S,D=1
(53)
22
Under review as a conference paper at ICLR 2021
where
J (V) = (max-DH(μ,V))2 = max	J(V, α)
a:a> Kα≤ 1
= max	hμ, (KXZa)°2)+{ν, (KYZɑ)°2) - 2,hμ,(Kχzɑ)吟 W, (KYZa)吟
α: α > K α ≤ 11	}	∣	^	}
Cα	kα
= max	Ca + hν, kai - 2VZCaVzhV^kOy.
a:a> Ka≤ 1
For fixed α the function J(∙, α) is a sum of a linear function and a convex function f (∙) = -√∙ is
convex since J is concave. J(V) = maXα:α>Kα≤ι J(V, α) is convex since maximizing over α
preserves convexity (Nesterov, 2018, Theorem 3.1.8).
In the linear kernel case, the cost is
J (V) = (max-DBd (μ,^))
max/ [ J(V, W)	hμ, (χ>W)。2〉一 JhV, (Y>W)°2i)
w[∣w∣∣2≤1 I	V	I
max (hμ, (X>w)02i + W, (Y>w)02i - 2,h”,(X>w)-2ihV, (Y>w)。2〉.
w:kwk2 ≤1
Again, J(v) is convex since J(∙, w) is convex for fixed w. In this case, the gradient has the intuitive
form of
VV J(V) =(1 - *) (ω> )。2 =(1 - phμ' (X>wv)。2i! (YTWV)。2.	(54)
∖	llωkν√	∖	，〈V, (YTWV)。2〉)
To solve this convex minimization over a probability simplex we apply the Frank-Wolfe (conditional
gradient) algorithm Jaggi (2013) to iteratively adjust the weight of one instance V J (1 - Y)V+γei,
where ei is an indicator vector, i = arg min1≤j≤n[VVJ(V)]j, and γ ∈ [0, 1] is the stepsize. A
benefit of the Frank-Wolfe scheme is that it requires only rank-1 updates of YDVYT, which are
needed for updating w.
A.8 Additional Experimental Results
We start by comparing the proposed max-sliced Bures distance to the max-sliced W2 distance
for two-dimensional data. We compare the fixed-point algorithm for solving each one-sided
max-sliced Bures divergence with gradient-based approaches for the max-sliced W2 distance us-
ing ADAM with parameters lr = 1e-3, beta1 = 0.9, beta2 = 0.999, epsilon
= 1e-08, capping the number of iterations at 1000 or until the change in the slice is minimal
kw - woldk∞ < 10-6.
In two-dimensions, a near optimal slice can be obtained by a fine grid search of the sliced Bures and
sliced W2 distance as shown in Figure 7 for two zero-mean Gaussian distributions. Figure 8 shows
the cases for success rate of the gradient-based optimizations across 10 trials at varying sample sizes
for 2- and 1000-dimensional zero-mean Gaussians. The effect of the number of gradient iterations
is reported in Figure 9.
For kernel-based divergences, maximum mean discrepancy (MMD) detects differences in the first
moments of the distributions in the RKHS. Using uncentered second-moments, the kernel-based
max-sliced Bures distance (MSB) may detect some of the same differences. Figure 10 details wit-
ness function evaluations of each for six data sets generated from two-dimensional distributions,
where a Gaussian kernel function is used. Notably, the one-sided MSB divergences correspond to
localized regions, which are not distributed outliers. This is beneficial for the ‘precision’ of the
witness function, but the ‘recall’ of MMD is better. This benefit of this localization depends on the
task.
A.9 Covariate Shift Detection with Max-sliced Kernel Divergences
We proceed to generalize the comparisons in Figure 4 on imbalanced samples on MNIST to the
kernel case. We use a Gaussian kernel κσ with the parameter σ set as the median Euclidean distance
23
Under review as a conference paper at ICLR 2021
0.4
0.3
0.2
0.1
0
Divergence between 2D Gaussians with known covariance
0.5--------1--------1-------1-------1-------1-------「0.5
)Tb'
sliced Wasserstein-2
......sliced BUres
------max-sliced Wasserstein-2
— ― one-sided max-sliced BUres(X,Y)
------one-sided max-sliced Bures(Y,X)
0.4
0.3
0.2
0.1
0
Estimated divergence between 2D GaUssians (m=n=100)
0	0.5	1	1.5	2	2.5	3	0	0.5	1	1.5	2	2.5	3
Figure 7: Sliced and max-sliced Bures and Wasserstein-2 distances are compared on population
statistics and samples of varying sizes. μ = N(0, C) and V = N(0, I), where C = ZZ>, and
Z ∈ R2×2 with entries that are originally standard normals and then row normalized such that C
is a correlation matrix. In the population case and for zero-mean Gaussians, the Bures distance is
equivalent to the W2 distance (Gelbrich, 1990). In the sample case, it is a lower bound. At both
m = 100 and m = 104 the gradient optimization of the max-sliced W2 distance fails to obtain the
global optimal slice (instead obtaining a local optimum).
log2(sample size)
Figure 8: Success rate of finding optimal slices for the max-sliced Bures and W2 distances across
samples of varying sizes (10 random runs per size). The distributions are zero-mean Gaussians, with
μ = N(0, C) and V = N(0, I), where C = ZZ>, and Z ∈ Rdxd With entries that are originally
standard normals and then row normalized such that C is a correlation matrix. (Left) In the case
of d = 2, success is obtained for a distance within 1% of the value obtained by fine-grid search
of angles. In this case, the gradient approach for the max-sliced Bures fails more often than the
max-sliced W2. (Right) For d = 1000 the eigenvalue-based approach (Algorithm A.5) defines the
global optimum. In the larger dimension, the gradient approach for the max-sliced Bures {ADAM}
succeeds in almost all of the cases, within 1% of the optimal value obtained by Algorithm A.5
(MSB), whereas the max-sliced W2 distance fails to upper bound the max-sliced Bures on roughly
half the trials.
0.8
φ
国0.6
1
―■- max-sliced WaSSerStein-2 {ADAM} > max-sliced BureS {Eig.}
-O- max-sliced WaSSerStein-2 {ADAM} > 95% max-sliced Bures {Eig.}
◊ | max-sliced Bures {ADAM} - MSB | < 1% max-sliced Bures {Eig.}
7	8	9	10	11	12
log2(sample size)
24
Under review as a conference paper at ICLR 2021

.max-siced Wasserslein-2 {ADAM} > max-sliced Bures
-6 max-siced Wassers*in-2 {ADAM} > 95% max-siced Bures
O | max-siced Bures {ADAM! - MSB | < 5% max-siced Bures
I max-sliced Wassersten-2 {ADAM}j
max-sliced BUres
max-sliced BUres


■ max-siced Wasserslein-2 {ADAM! > max-siced Bures
-O- max-siced Wassers*in-2 {ADAM! > 95% max-siced Bures
| max-sliced Bures {ADAM! - MSB | < 5% max-siced Bures
.max-siced Wasserstein-2 {ADAM} > max-sliced Bures
-e- max-siced Wasserstein-2 {ADAM} > 95% max-siced Bures
| max-siced Bures {ADAM} - MSB | < 5% max-siced Bures
∖...,	. J
I Ligd Wassersiein-Z {ADAM>]
max-siced Bures {Eig.)
I ma,,sited Bures {ADAM! ∣
max-Siced Wasserstein-2 {ADAM} > max-sliced Bures
max-Siced Wasserstein-2 {ADAM} > 95% max-siced Bures
| max-siced Bures {ADAM} - MSB | < 5% max-siced Bures
I	max-siced WaSSerSten-2 {ADAM>]
max-siced Bures {Eg.!
I	ma»sited Bures {ADAM! ∣

max-siced WassersIein-2 {ADAM! > max-siced Bures
max-siced Wasserslein-2 {ADAM! > 95% max-siced Bures
| max-sliced Bures {ADAM! - MSB | < 5% max-sliced Bures
1 ,∖ ,.
'le=∙°
'le=∙°
6	8	10	12	6
log2(samιple size)
, -♦
'le=∙°
, -♦
'le=∙°
max-sliced WasSe<stein-2 {ADAM!
------max-sliced Bures {Eg.!
max-sliced Bures {ADAM! __________∣
"J
'le=∙°
8	10	12
log2(samιpe SZe)
6	8	10	12
log2(samιple size)
log2(sampe size)
log2(sample size)
Figure 9: Performance of gradient algorithms for max-sliced Bures and max-sliced W2 distances
across d = 1000 dimensional samples of varying sizes (10 random runs per size) and number of
iterations in ADAM (Left to right: 50, 100, 200, 500, 1000). The samples are from zero-mean
Gaussians distributions, with μ = N(0, C) and V = N(0, I), where C = ZZ>, and Z ∈ Rd×d
with entries that are originally standard normals and then row normalized such that C is a correlation
matrix. (Top) For the max-sliced W2 a successful run is obtained when it is greater than or equal
to the optimal solution to the max-sliced Bures (blue solid line) or when it is greater than 95% of
max-sliced Bures (red dotted with circles). For the gradient approach to max-sliced Bures, success
is when the difference to the optimal is 5% of the optimal (yellow solid with diamonds). (Bottom)
Divergence values obtained across the 10 trials with increasing sample size.
2
0
MMD
MSB
2
0
2
0
-2
2
0
-2
MMD MSB
MMD MSB
2
2
2
-2
2
-2
1
0
1
1
0
-1
1.5
■
05
-2	0	2	-2	0	2
-2	0	2	-2	0	2
-1	0	1	-1	0	1
Figure 10: Maximum mean discrepancy (MMD) and max-sliced Bures distance (MSB) applied
to two-dimensional samples using a Gaussian kernel. For each data set (shown as a two-by-two
subplot), the contour plots indicate the squared magnitude of the witness function evaluations. For
MMD, positive witness function values are plotted in the top row and negative evaluations are in the
second row. For MSB, the rows correspond to the two one-sided divergences. The witness functions
for the one-sided MSB divergences correspond to localized regions.
MMD
MSB
-2	0	2
MMD
2
0

25
Under review as a conference paper at ICLR 2021
in the pooled sample. To ease computation for large-sample sizes, we let τ be a random subset of the
pooled samples {xi}m=ι ∪{yi}n=ι of size l = min {500, m+n}. Inthis case, the max-slicedFrechet
refers to maxL-DGHW, which is equal to the square root of the sum of square of MMD and the square
of the max-sliced Bures using centered kernels, as in equation 23. The kernel-based max-sliced W2
distance should be an upper bound of the max-sliced Frechet. However, in practice the optimal slice
(witness function) may not be obtained.
Figure 11: Kernel-based max-sliced distances are applied to balanced and imbalanced samples from
MNIST. The first sample μ consists of the training set (balanced classes with size m), and the second
sample V is a n-sized sample from the test set with a minority class l ∈ {0,..., 7} with prevalance
of 5%. (Left) Divergence estimates for increasing sample size for l = 7. Notably, for m < 2000
the max-sliced Wasserstein-2 distance fails to obtain the optimal slice as it should upper bound
the max-sliced kernel Gauss-Wasserstein (FreChet) distance. (Center) Corresponding computation
time. (Right) Each curve is the average precision@10 (averaged across the 10 classes). The witness
function for the one-sided max-sliced Bures ω^>^ can be used to reliably identify instances from μ
associated to the missing class.
We now compare the proposed kernel-based max-sliced divergences to existing baselines. A primary
baseline for this task is to train a logistic regression model with kernel basis functions to distinguish
the two samples, and then use the probability estimates of the instances as the witness function
evaluations ω(x) = Pr(Ho∣X = X) = 1 -Pr(Hι∣X = x), where H : X 〜 V and Hi : X 〜 μ.
As additional baselines we also tested three methods for importance reweighting and density ratio
estimation: kernel mean matching (KMM) (Huang et al., 2007), least-squares importance estima-
tion (uLSIF) (Kanamori et al., 2009), and relative density-ratio estimation (RuLSIF) (Yamada et al.,
2011), but all methods were outperformed by logistic regression with kernel bases. We also com-
pare with kernel Fischer discriminant analysis (KFDA) (Mika et al., 1999), and the linear cases of
max-sliced Wasserstein-2 distance, its first moment approximation, max-sliced Bures, and logistic
regression. For all kernel methods, a Gaussian kernel κσ is used with the parameter σ set as the
median Euclidean distance in the pooled sample.
Using the MNIST data set again, we test three scenarios of covariate shift. For each, one sample has
a mismatched probability for one class l ∈ {0, . . . , 9} and the other sample has a balanced sample:
(Scenario 1) μ is balanced and V is missing l; (Scenario 2) μ is imbalanced with l only appearing
in 2% of the cases, compared to 10.8% for the other classes and V is balanced; (Scenario 3) μ is
balanced and V consists of only images from l. In each case, μ is a sample of 500 images from
the training set and V is a sample of 500 images from the test set. A threshold-free way to assess
covariate shift detection is to use the area-under-the-curve (AUC) of the receiver operator curve
(ROC), where positive instances correspond to class l. For some methods, the witness function (or
its magnitude) may be ambiguous in sign, i.e., the values may be high (or large) for either the under-
or over-sampled instances (namely, max-sliced W2). To be generous, on each run we choose the
ordering with the highest AUC. The results are reported in Table 3.
The other baselines KMM, uLSIF, and RuLSIF are not shown (their AUC scores across the scenarios
are worse than the logistic regression with kernel baseline). In a separate set of runs we also compute
the realism scores (Kynkaanniemi et al., 2019) with k = 3 where V is considered the real set, and μ
are synthetic, to prioritize instances; results for the three scenarios are 0.75±0.11, 0.67±0.12, and
0.94±0.04, which is better than linear logistic regression and KFDA, but far worse than the kernel
logistic regression baseline.
26
Under review as a conference paper at ICLR 2021
Table 3: Unsupervised covariate shift outlier detection on MNIST. The goal is to identify instances
associated with an over- or underrepresented class l ∈ {0, . . . , 9}. Values are AUC where positives
are instances from class l. We report the mean and standard deviation and the number of times each
method has the highest AUC (including ties) across 100 trials (10 for each case l ∈ {0, . . . 9}).
	(Scenario 1)	(Scenario 2)	(Scenario 3)	(1)	(2)	(3)
logistic regression-linear	0.60 (0.05)	0.60 (0.08)	0.91 (0.06)	0	1	0
Max-Sliced Bures	0.89 (0.12)	0.86 (0.13)	0.95 (0.03)	2	3	3
Max-Sliced W2 (approx.)	0.86 (0.10)	0.84 (0.13)	0.96 (0.02)	1	2	0
Max-Sliced W2	0.87 (0.14)	0.85(0.14)	0.96 (0.02)	2	12	0
logistic regression-kernel	0.90 (0.07)	0.86 (0.10)	0.99 (0.01)	"T0^^	15	93
KFDA	0.58 (0.03)	0.61 (0.12)	0.91 (0.03)	0	2	1
MMD	0.87 (0.10)	0.85 (0.12)	0.97 (0.02)	1	3	0
Max-Sliced Kernel TV	0.85 (0.10)	0.83 (0.14)	0.96 (0.03)	2	0	1
Max-Sliced Kernel Bures	0.92 (0.11)	0.88 (0.14)	0.97 (0.02)	21	32	2
Max-Sliced Kernel W2	0.92 (0.11)	0.88(0.13)	0.97 (0.02)	61	35	0
A.10 Covariate S hift Correction for Class-Conditional Subsampling
Figure 12 shows 20 synthetic images—generated by AutoGAN trained on CIFAR10 (n=50,000)—
with the largest weights after reweighting in order to minimize the max-sliced Bures distance to the
subset of training images for each class separately (m=5,000). Computing the max-sliced Bures
distance with the entire training set of 50,000 points is tractable since it does not depend on the
sample size. The realism scores of the selected images have a median and range of 0.96 (0.63-
1.34). Figure 13 shows the same but based on the weights optimized by using the W2 distance with
the mini-batch optimization as the cost function. The realism scores of the selected images have a
median and range of 1.1 (0.93-1.29). Figure 14 shows the same but based on the weights optimized
by using the max-sliced W2 distance with the mini-batch optimization. The realism scores of the
selected images have a median and range of 0.97 (0.65-1.25). Finally, Figure 15 shows the synthetic
images selected for having the highest realism scores; notably this set lacks class correspondence.
The optimizations in the first three cases use the Frank-Wolfe algorithm (Jaggi, 2013) with sim-
plex constraints. The default step-size schedule Y = k++^ and the same stopping criterion is used
maxι≤i≤n∣ν(k) - ν(k-1)∣ < 10-3, where k is the iteration index. This yields roughly the same
number of iterations for each method. The optimization starts from a uniform weighting, which
means the weights for only ~2000 instances are actually individually adjusted (the rest are adjusted
by common scaling). The FreChet Inception distances after reweighting are detailed in Table 4.
Based on the quantitative and qualitative results it appears that the W2 distance with mini-batch
approximation assigns high weight to high-quality synthetic images, but the diversity of the highly
weighted instances may not capture the full distribution for a class. In this regard, the max-sliced
Bures better captures the diversity of the class, albeit choosing less realistic images.
27
Under review as a conference paper at ICLR 2021
Table 4: Frechet Inception distances (FID) between CIFAR10 test set images in each class and
reweighted sample of synthetic images from AutoGAN. The second column shows the FID to the
corresponding training set. The third column is a uniform weighting over all 50,000 synthetic im-
ages. The reweighting that minimizes the max-sliced Bures distance (MSB) to the subset of training
images performs the best on average. Using the W2 distance—estimated through 10 mini-batches
of 100 images on each iteration—performs best only on one-class. The max-sliced W2 (MSW2)
distance also uses mini-batches. The realism scores R of the 20 images with the highest weight for
each class (200 images for each method) are summarized by the median and range.
	Training set	Uniform	MSB	W2	MSW2
airplane	28.00	-108.67	57.82	74.67	81.85
automobile	20.20	133.10	39.88	66.76	74.81
bird	30.25	86.23	58.71	57.17	76.98
cat	35.53	84.63	61.27	83.89	75.57
deer	26.46	85.55	46.81	60.21	56.44
dog	28.99	106.76	56.88	67.94	78.77
frog	29.88	107.88	51.14	75.23	66.93
horse	24.33	111.30	42.68	66.77	63.24
ship	21.49	131.92	37.35	65.05	74.47
truck	17.77	141.56	38.97	63.73	76.05
Average	26.29	109.76	49.15	68.14	72.51
Median R Range R			0.96 0.63-1.34	1.10 0.93-1.29	0.97 0.65-1.25
Figure 12: Distribution matching based on minimizing max-sliced Bures distance max-DB(μ, V).
Synthetic images shown are those with the highest values of V, where V is the V-weighted distribu-
tion over 50,000 synthetic images from AUtoGAN and μ consists of CIFAR10 training images for
a single class in each row. Rows (top to bottom) correspond to airplane, automobile, bird, cat, deer,
dog, frog, horse, ship, and truck classes.
28
Under review as a conference paper at ICLR 2021
Figure 13: Distribution matching based on minimizing the Wasserstein-2 distance through mini-
batch. Synthetic images shown are those with the higest values of V, where V is the V-weighted
distribution over 50,000 synthetic images from AUtoGAN and μ consists of CIFAR10 training im-
ages for a single class in each row. Rows (top to bottom) correspond to airplane, automobile, bird,
cat, deer, dog, frog, horse, ship, and truck classes.
Figure 14: Distribution matching based on minimizing the max-sliced Wasserstein-2 distance
max-W2R (μ, ν) through mini-batch approximation. Synthetic images shown are those with the
highest values of V, where V is the V-weighted distribution over 50,000 synthetic images from Au-
toGAN and μ consists of CIFAR10 training images for a single class in each row. Rows (top to
bottom) correspond to airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck classes.
29
Under review as a conference paper at ICLR 2021
Figure 15: Selecting images directly with the highest realism scores. Synthetic images shown are
those with the highest realism values over 50,000 synthetic images generated by AutoGAN when
the realism scores used in each row are computed using the CIFAR10 training images for a single
class. Rows (top to bottom) correspond to airplane, automobile, bird, cat, deer, dog, frog, horse,
ship, and truck classes. Realism score correctly identifies “realistic” imagery, but is unable to find
samples that cover the real distribution. For example, the top row is missing airplanes, the third row
is missing birds, the fourth row is missing cats, etc.
30