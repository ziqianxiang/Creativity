Under review as a conference paper at ICLR 2021
Enabling Counterfactual Survival Analysis
with Balanced Representations
Anonymous authors
Paper under double-blind review
Ab stract
Balanced representation learning methods have been applied successfully to coun-
terfactual inference from observational data. However, approaches that account for
survival outcomes are relatively limited. Survival data are frequently encountered
across diverse medical applications, i.e., drug development, risk profiling, and clini-
cal trials, and such data are also relevant in fields like manufacturing (for equipment
monitoring). When the outcome of interest is time-to-event, special precautions
for handling censored events need to be taken, as ignoring censored outcomes may
lead to biased estimates. We propose a theoretically grounded unified framework
for counterfactual inference applicable to survival outcomes. Further, we formulate
a nonparametric hazard ratio metric for evaluating average and individualized
treatment effects. Experimental results on real-world and semi-synthetic datasets,
the latter which we introduce, demonstrate that the proposed approach significantly
outperforms competitive alternatives in both survival-outcome predictions and
treatment-effect estimation.
1 Introduction
Survival analysis or time-to-event studies focus on modeling the time of a future event, such as
death or failure, and investigate its relationship with covariates or predictors of interest. Specifically,
we may be interested in the causal effect of a given intervention or treatment on survival time. A
typical question may be: will a given therapy increase the chances of survival of an individual or
population? Such causal inquiries on survival outcomes are common in the fields of epidemiology
and medicine (Robins, 1986; Hammer et al., 1996; Yusuf et al., 2016). As an important current
example, the COVID-19 pandemic is creating a demand for methodological development to address
such questions, specifically, when evaluating the effectiveness of a potential vaccine or therapeutic
outside randomized controlled trial settings.
Traditional causal survival analysis is typically carried out in the context of a randomized controlled
trial (RCT), where the treatment assignment is controlled by researchers. Though they are the gold
standard for causal inference, RCTs are usually long-term engagements, expensive and limited in
sample size. Alternatively, the availability of observational data with comprehensive information
about patients, such as electronic health records (EHRs), constitutes a more accessible but also
more challenging source for estimating causal effects (Hayrinen et al., 2008; Jha et al., 2009). Such
observational data may be used to augment and verify an RCT, after a particular treatment is approved
and in use (Gombar et al., 2019; Frankovich et al., 2011; Longhurst et al., 2014). Moreover, the
wealth of information from observational data also allows for the estimation of the individualized
treatment effect (ITE), namely, the causal effect of an intervention at the individual level. In this
work, we develop a novel framework for counterfactual time-to-event prediction to estimate the ITE
for survival or time-to-event outcomes from observational data.
Estimating the causal effect for survival outcomes in observational data manifests two principal
challenges. First, the treatment assignment mechanism is not known a priori. Therefore, there may
be variables, known as confounders, affecting both the treatment and survival time, which lead to
selection bias (Bareinboim & Pearl, 2012), i.e., that the distributions across treatment groups are not
the same. In this work, we focus on selection biases due to confounding, but other sources may also
be considered. For instance, patients who are severely ill are likely to receive more aggressive therapy,
however, their health status may also inevitably influence survival. Traditional survival analysis
1
Under review as a conference paper at ICLR 2021
neglects such bias, leading to incorrect causal estimation. Second, the exact time-to-event is not
always observed, i.e., sometimes we only know that an event has not occurred up to a certain point in
time. This is known as the censoring problem. Moreover, censoring might be informative depending
on the characteristics of the individuals and their treatment assignments, thus proper adjustment is
required for accurate causal estimation (Cole & Herndn, 2004; Diaz, 2019).
Traditional causal survival-analysis approaches typically model the effect of the treatment or co-
variates (not time or survival) in a parametric manner. Two commonly used models are the Cox
proportional hazards (CoxPH) model (Cox, 1972) and the accelerated failure time (AFT) model (Wei,
1992), which presume a linear relationship between the covariates and survival probability. Further,
proper weighting for each individual has been employed to account for confounding bias from these
models (Austin, 2007; 2014; Herndn et al., 2005). For instance, probability weighting schemes
that account for both selection bias and covariate dependent censoring have been considered for
adjusted survival curves (Cole & Herndn, 2004; Diaz, 2019). Moreover, such probability weighting
schemes have been applied to causal survival-analysis under time-varying treatment and confounding
(Robins, 1986; Herndn et al., 2000). See van der Laan & Robins (2003); Tsiatis (2007); Van der
Laan & Rose (2011); Herndn & Robins (2020) for an overview. Such linear specification makes
these models interpretable but compromises their flexibility, and makes it difficult to adapt them
for high-dimensional data or to capture complex interactions among covariates. Importantly, these
methods lack a counterfactual prediction mechanism, which is key for ITE estimation (see Section 2).
Fortunately, recent advances in machine learning, such as representation learning or generative
modeling, have enabled causal inference methods to handle high-dimensional data and to characterize
complex interactions effectively. For instance, there has been recent interest in tree-based (Chipman
et al., 2010; Wager & Athey, 2018) and neural-network-based (Shalit et al., 2017; Zhang et al.,
2020) approaches. For pre-specified time-horizons, the nonparametric Random Survival Forest (RSF)
(Ishwaran et al., 2008) and Bayesian Additive regression trees (BART) (Chipman et al., 2010) have
been extended to causal survival analysis. RSF has been applied to causal survival forests with
weighted bootstrap inference (Shen et al., 2018; Cui et al., 2020) while a BART is extended to account
for survival outcomes in Surv-BART (Sparapani et al., 2016), and AFT-BART (Henderson et al.,
2020). See Hu et al. (2020) for an extensive investigation of the causal survival tree-based methods.
Alternatively, when estimating the ITE, neural-network-based methods propose to regularize the
transformed covariates or representations for an individual to have balanced distributions across
treatment groups, thus accounting for the confounding bias and improving ITE prediction. However,
most approaches employing representation learning techniques for counterfactual inference deal
with continuous or binary outcomes, instead of time-to-event outcomes with censoring (informative
or non-informative). Hence, a principled generalization to the context of counterfactual survival
analysis is needed.
In this work we leverage balanced (latent) representation learning to estimate ITE via counterfactual
prediction of survival outcomes in observational studies. We develop a framework to predict event
times from a low-dimensional transformation of the original covariate space. To address the specific
challenges associated with counterfactual survival analysis, we make the following contributions:
•	We develop an optimization objective incorporating adjustments for informative censoring, as well
as a balanced regularization term bounding the generalization error for ITE prediction. For the
latter, we repurpose a recently proposed bound (Shalit et al., 2017) for our time-to-event scenario.
•	We propose a generative model for event times to relax restrictive survival linear and parametric as-
sumptions, thus allowing for more flexible modeling. Our approach can also provide nonparametric
uncertainty quantification for ITE predictions.
•	We provide survival-specific evaluation metrics, including a new nonparametric hazard ratio
estimator, and discuss how to perform model selection for survival outcomes. The proposed model
demonstrates superior performance relative to the commonly used baselines in real-world and
semi-synthetic datasets.
•	We introduce a survival-specific semi-synthetic dataset and demonstrate an approach for leveraging
prior randomized experiments in longitudinal studies for model validation.
2
Under review as a conference paper at ICLR 2021
(a) Counterfactual inference
Figure 1: (a) Illustration of the proposed counterfactual survival analysis (CSA). Covariates X =
x are mapped into latent representation r via deterministic mapping r = Φ(x). The potential
outcomes are sampled from ta 〜P(TA|X = x) for A = a Via stochastic mapping hA(r, e), where
randomness is induced with a flow-based transformation, 6, of a simple distribution p(6), i.e., uniform
or Gaussian. (b) and (c) show the proposed causal graphs for non-informatiVe and informatiVe
censoring, respectiVely.
(b) Non-informative
(c) Informative
2 Problem Formulation
We first introduce the basic setup for performing causal surViVal analysis in obserVational studies.
Suppose we haVe N units, with N1 units being treated and N0 in the control group (N = N1 + N0).
For each unit (indiVidual), we haVe coVariates X, which can be heterogeneous, e.g., a mixture of
categorical and continuous coVariates which, in the context of medicine, may include labs, Vitals,
procedure codes, etc. We also haVe a treatment indicator A, where A = 0 for the controls and
A = 1 for the treated, as well as the outcome (eVent) of interest T. Under the potential-outcomes
framework (Rubin, 2005), let T0 and T1 be the potential eVent times for a giVen subject under control
and treatment, respectiVely. In practice we only obserVe one realization of the potential outcomes,
i.e., the factual outcome T = TA, while the counterfactual outcome T1-A is unobserVed.
In surViVal analysis, the problem becomes more difficult because we do not always obserVe the exact
eVent time for each indiVidual, but rather the time up to which we are certain that the eVent has not
occurred; specifically, we haVe a (right) censoring problem, most likely due to the loss of follow-up.
We denote the censoring time as C and censoring indicator as δ ∈ {0, 1}. The actual observed time
is Y = min(TA, C), i.e., the outcome is obserVed (non-censored) if TA < C and δ = 1.
In this work, we are interested in the expected difference between the T1 and T0 conditioned on
X for a giVen unit (indiVidual), which is commonly known as the individualized treatment effect
(ITE). Specifically, we wish to perform inference on the conditional distributions of T1 and T0, i.e.,
p(T1 |X) and p(T0|X), respectiVely, as shown in Figure 1a. In practice, we obserVe N realizations of
(Y, δ, X, A) for obserVed time, censoring indicator, coVariates and treatment indicator, respectiVely;
hence, from an obserVational study the dataset takes the form D = {(yi, δi, xi, ai)}iN=1. Below, we
discuss seVeral common choices of estimands in surViVal analysis.
Estimands of Interest We begin by considering surViVal analysis in the absence of an interVening
treatment choice, A. Let F (t|x) , P(T ≤ t|X = x) be the cumulatiVe distribution function of the
eVent (failure) time, t, giVen a realization of the coVariates, x. SurViVal analysis is primarily concerned
with characterization of the survival function conditioned on coVariates S(t|x) , 1 - F (t|x), and the
hazard function or risk score, λ(t∣x), defined below. S (t|x) is a monotonically decreasing function
indicating the probability of surViVal up to time t. The hazard function measures the instantaneous
probability of the eVent occurring between {t, t + ∆t} giVen T > t and ∆t → 0. From standard
definitions (Kleinbaum & Klein, 2010), the relationship between cumulatiVe and hazard function is
formulated as
λ(t∣x) = lim
dt→0
P(t <T<t+dt|X =x)	dlogS(t|x)	f (t|x)
P(T >t|X = x)dt
dt
S (t|x).
(1)
—
From (1) we see that f (t|x)，P(T = t|X = x) = λ(t∣x)S(t|x), is the conditional event time
density function (Kleinbaum & Klein, 2010).
GiVen the binary treatment A, we are interested in its impact on the surViVal time. For ITE estimation,
we are also interested in the difference between the two potential outcomes T1, T0. Let SA(t|x) and
λA(t|x) denote the survival and hazard functions for the potential outcomes TA, i.e., T1 and T0.
SeVeral common estimands of interest include (Zhao et al., 2012; Trinquart et al., 2016): difference
3
Under review as a conference paper at ICLR 2021
in expected lifetime: ITE(t, x) = R0tmax {S1 (t|x) - S0(t|x)}dt = E{T1 - T0|X = x}, difference in
survival function: ITE(t,x) = Sι(t∣x) 一 S0(t∣x), and hazard ratio: ITE(t,x) = λι(t∣x)∕λo(t∣x).
The inference difficulties associated with the above estimands from observational data are two-fold.
First, there are confounders affecting both the treatment assignment and outcomes, which stem from
selection bias, i.e., the treatment and control covariate distributions are not necessarily the same.
Also, we do not have direct knowledge of the conditional treatment assignment mechanism, i.e.,
P(A = a|X = x), also known as the propensity score. Let ⊥⊥ denote statistical independence. For
estimands to be identifiable from observational data, we make two assumptions: (i) {T1, T0} ⊥⊥ A|X,
i.e., no unobserved confounders or ignorability, and (ii) overlap in the covariate support 0 < P(A =
1|X = x) < 1 almost surely if p(X = x) > 0. Second, the censoring mechanism is also unknown
and may lead to bias without proper adjustment. We consider two censoring mechanisms in our
work, (i) conditionally independent or informative censoring: T ⊥⊥ C |X, A, and (ii) random or
non-informative censoring: T ⊥⊥ C. Note that for informative censoring, we also have to consider
potential censoring times C1 and C0 and their conditionals p(C1 |X) and p(C0|X), respectively.
Figure 1 shows causal graphs illustrating these modeling assumptions.
3 Modeling
To overcome the above challenges and adjust for observational biases, we propose a unified frame-
work for counterfactual survival analysis (CSA). Specifically, we repurpose the counterfactual bound
in Shalit et al. (2017) for our time-to-event scenario and introduce a nonparametric approach for
stochastic survival outcome predictions. Below we formulate a theoretically grounded and unified
approach for estimating (i) the encoder function r = Φ(x), which deterministically maps covari-
ates x to their corresponding latent representation r ∈ Rd, and (ii) two stochastic time-to-event
generative functions, ha(∙), to implicitly draw samples from both potential outcome conditionals
ta 〜Ph,φ(TA∣X = x), for A = {1,0}, and where ta indicates the sample frompk,φ(Ta∖X = x)
is for A = a. Further, we formulate a general extension that accounts for informative censoring
by introducing two stochastic censoring generative functions, VA(∙),to draw samples for potential
censoring times Ca 〜Pv,φ(Ca|X = x). The model-specifying functions, {ha(∙), va(∙), Φ(∙)}, are
parameterized via neural networks. See the Supplementary Material (SM) for details. Figure 1a
summarizes our modeling approach.
Accounting for selection bias We wish to estimate the potential outcomes, i.e., event times, which
are sampled by distributions parameterized by functions (hA(∙), Φ(∙)}, i.e.,
t 〜Ph,φ(TX = x,A = a)	(2)
ta 〜Ph,φ(Ta∖X = x)	(3)
We obtain (3) from (2) via the strong ignorability assumption, i.e., {T0, T1} ⊥⊥ A∖X (consis-
tent with the causal graphs in Figure 1b and 1c) and 0 < P(A = a∖X = x) < 1, and the
consistency assumption, i.e., T = TA∖A = a. A similar argument can be made for informative
censoring based on Figure 1c, so we can also write Ca 〜Pv,φ(Ca|X = x). Given (3), model
functions {hA(∙), Φ(∙)} and va(∙) for informative censoring can be learned by leveraging stan-
dard statistical optimization approaches, that minimize a loss hypothesis L given samples from
the empirical distribution (y, δ, x, a)〜p(Y, δ, X, A), i.e., from dataset D. Specifically, we write
L = E(y,δ,x,a)〜p(γ,δ,x,A) ['h,φ(ta, y, δ)], where 'h,φ(ta, y, δ) is a loss function that measures the
agreement of ta 〜Ph,φ(TA∖X = x) (and Ca 〜Pv,φ(Ca|X = x) for informative censoring) with
ground truth {y, δ}, the observed time and censoring indicator, respectively.
For some parametric formulations of event time distribution ph,Φ(TA∖X = x), e.g., exponential,
Weibull, log-Normal, etc., and provided the censoring mechanism is non-informative, -'h,φ(ta, y, δ)
is the closed form log likelihood. Specifically, -'h,φ(ta,y,δ) ，logPh,φ(Ta∖X = x) = δ ∙
log fh,φ(ta∖x) + (1 — δ) ∙ log Sh,φ(ta∖x), which implies that the conditional event time density and
survival functions can be calculated in closed form from transformations {hA(∙), Φ(∙)} of x. See the
SM for parametric examples of L accounting for informative censoring.
We further define the expected loss for a given realization of covariates x and treatment assignment a
over observed times y (censored and non-censored), and the censoring indicator δ as ζh,Φ (x, a) ,
E(y,δ,x)〜p(γ,δ∣x)'h,φ(ta, y, δ) as in Shalit et al. (2017). For a given subject with covariates X and
treatment assignment a, we wish to minimize both the factual and counterfactual losses, LF and LCF ,
4
Under review as a conference paper at ICLR 2021
respectively, by decomposing L = LF + LCF as follows
LF = E(x,a)〜p(A,X)ζh,Φ(X, a) ,	LCF = E(x,a)〜p(1-A,X)ζh,Φ(X, a) ∙	(4)
Let u , P (A = 1) denote the marginal probability of treatment assignment. We can readily
decompose the losses in (4) according to treatment assignments. The decomposed factual LF =
U ∙ LA=I + (1 - u) ∙ LA=0, and similarly, the decomposed Counterfactual LCF = (1 - u) ∙ LA=I + U ∙
LCAF=0. In practice, only factual outcomes are observed, hence, for a non-randomized non-controlled
experiment, we cannot obtain an unbiased estimate of LCF from data due to selection bias (or
confounding). Therefore, we bound LCF and L below following Shalit et al. (2017).
Corollary 1 Assume Φ(∙) is an invertible map, and a-1Zh,φ (x, a) ∈ G, where G is a family of
functions, pA=a，pφ(R|A = a) is the latent distributionfor group A = a, and α > 0 is a constant.
Then, we have:
LCF ≤ (1 - U) ∙ LA=1 + U ∙ LA= + α ∙ IPMg(pA=1,pA=0)
L ≤ LA=1 + LA=0 + α ∙ IPMg(pA=1,pA=0).	(5)
The integral probability metric (IPM) (Muller, 1997; Sriperumbudur et al., 2012) measures the
distance between two probability distributions p and q defined over M, i.e., the latent space of R.
Formally, IPMG(p, q) , supg∈G | M g(m) (p(m) - q(m)) dm|, where g : m → R, represents a
class of real-valued bounded measurable functions on M (Shalit et al., 2017). Therefore, model
functions {ha(∙), Φ(∙)} can be learned by minimizing the upper bound in (5) consisting of (i)
only factual losses under both treatment assignments and (ii) an IPM regularizer enforcing latent
distributional equivalence between the treatment groups. Note that if the data originates from a RCT
it follows (by construction) that IPMG(pΦA=1, pΦA=0) = 0.
Accounting for censoring bias Below We formulate an approach for estimating functions hA(∙)
and va(∙) for synthesizing (sampling) non-censored tα 〜ph,φ(TA|X = x) and censored Ca 〜
Pv,φ(Ca∣X = x) times, respectively. While some parametric assumptions for ph,φ(TA|X = x)
yield easy-to-evaluate closed forms for Sh,φ(ta∣χ) that can be used as likelihood for censored
observations, they are restrictive, and have been shown to generate unrealistic high variance samples
(Chapfuwa et al., 2018). So motivated, we seek a nonparametric likelihood-based approach that
can model a flexible family of distributions, with an easy-to-sample approach for event times t。〜
Ph,Φ(Tα∣X = x). We model the event time generation process with a source of randomness, p(e),
e.g. Gaussian or uniform, which is obtained from a neural-network-based nonlinear transformation.
In the experiments we use a planar flow formulation parameterized by {Uh, Wh, bh} (Rezende &
Mohamed, 2015), however, other specifications can also be used. Note that Miscouridou et al. (2018)
has previously leveraged normalizing flows for survival analysis, however, our approach is very
different in that it focuses on formulating i) a counterfactual survival analysis framework that accounts
for informative or non-informative censoring mechanisms and confounding, and ii) model event
times as a continuous variable instead of discretizing them. Specifically, we transform the source of
randomness, e, using a single layer specification as follows
Wh = e + Uh tanh(Whe + bh), e 〜Uniform(0, 1), t。= hA(r, Nh), r = Φ(x)	(6)
where {Uh, Wh} ∈ Rd×d, {bh, e} ∈ Rd, d is the dimensionality of the normalizing flow; each
component of e is drawn independently from Uniform(0, 1), and eWh may be viewed as a skip
connection with stochasticity in e. Further, hA(r, eWh) and Φ(x) are time-to-event generative and
encoding functions, respectively, parameterized as neural networks. For simplicity, the dimensions of
r and e are set to d, however, they can be set independently if desired. In practice, we are interested
in generating realistic event-time samples; therefore, we account for both censored and non-censored
observations by adopting the objective from Chapfuwa et al. (2018), formulated as
LCSA , E(y,δ,χ,a)〜P(Y,δ,x,A),e〜p(e) [δ ∙ (|y - ta|) + (1 - δ) ∙ (max(0, y - ta))] ,	(7)
where the first term encourages sampled event times ta to be close to y, the ground truth for observed
events, i.e., δ = 1, while penalizing ta for being smaller than the censoring time when δ = 0. Further,
the expectation is taken over samples (a minibatch) from empirical distribution p(Y, δ, X, A).
5
Under review as a conference paper at ICLR 2021
Informative censoring We model informative censoring similar to (7) but mirroring the censoring
indicators to encourage accurate censoring time samples ca for δ = 0, while penalizing ca for
being smaller than y for δ = 1 (observed events). Specifically, we set an independent source
of randomness like in (6) but parameterized by {Uν , Wν , bν } and censoring generative functions
VA(r, eν), parameterized as neural networks, where Ca 〜Pv,φ(Ca|X = x) formulated as
'c(ν, Φ) = E(y,δ,x,a)〜p(y,δ,X,A),e〜p(e) [(1 - δ) ∙ (|y - Ca |) + δ ∙ (max(0, y - Ca))] .	(8)
Further, we introduce an additional time-order-consistency loss that enforces the correct order of the
observed time relative to the censoring indicator, i.e., Ca < ta if δ = 0 and ta < Ca if δ = 1, thus
'τc(h, V, Φ) = E(δ,x,a)〜p(δ,X,A),e〜p(e) [δ ∙ (max(0, ta -Ca)) + (1 - δ) ∙ (max(0, Ca - ta))] (9)
Note that 'τc(h, ν, Φ) does not depend on the observed event times but only on the censoring
indicators. Finally, we write the consolidated CSA loss for informative censoring (CSA-INFO) by
aggregating (7), (8) and (9) as LCSATNFO，LCSA + '0 + 伺°.
Learning Model functions {hA(∙), Φ(∙), va(∙)} are learned by minimizing the bound (5), via
stochastic gradient descent on minibatches from D, with LFCSA for non-informative censoring and
LFCSA-INFO for informative censoring. Further, for the IPM regularization loss in (5), we optimize
the dual formulation of the Wasserstein distance, via the regularized optimal transport (Villani, 2008;
Cuturi, 2013). Consequently, we only require α-1ζh,Φ(x, a) to be 1-Lipschitz (Shalit et al., 2017)
and α is selected by grid search on the validation set using only factual data (details below).
4 Metrics
We propose a comprehensive evaluation approach that accounts for both factual and causal metrics.
Factual survival outcome predictions are evaluated according to standard survival metrics that measure
diverse performance characteristics, such as concordance index (C-Index) (Harrell Jr et al., 1984),
mean coefficient of variation (COV) and calibration slope (C-slope) (Chapfuwa et al., 2020). See the
SM for more details on these metrics. For causal metrics, defined below, we introduce a nonparametric
hazard ratio (HR) between treatment outcomes, and adopt the conventional precision in estimation of
heterogeneous effect (PEHE) and average treatment effect (ATE) performance metrics (Hill, 2011).
Note that PEHE and ATE require ground truth counterfactual event times, which is only possible in
(semi-)synthetic data. For HR, we compare our findings with those independently reported in the
literature from gold-standard RCT data.
Nonparametric Hazard Ratio In a medical setting, the population hazard ratio HR(t) between
treatment groups is considered informative thus has been widely used in drug development and RCT
(Yusuf et al., 2016; Mihaylova et al., 2012). For example, HR(t) < 1, > 1, or ≈ 1 indicate population
positive, negative and neutral treatment effects at time t, respectively. Moreover, HR(t) naturally
accounts for both censored and non-censored outcomes. Standard approaches for computing HR(t)
rely on the restrictive proportional hazard assumption from CoxPH (Cox, 1972), which is constituted
as a semi-parametric linear model λ(t∣a) = λb(t) exp(aβ). However, the constant covariate (time
independent) effect is often violated in practice (see Figure 2b). For CoxPH, the marginal HR between
treatment and control can be obtained from regression coefficient β learned via maximum likelihood
without the need for specifying the baseline hazard λb(t): HRCoXPH(t) = ：(；|：=1) = exp(β). So
motivated, we propose a nonparametric, model-free approach for computing HR(t), in which we do
not assume a parametric form for the event time distribution or the proportional hazard assumption
from CoxPH. This approach only relies on samples from the conditional event time density functions,
f(tι|x) and f (to|x), viat。= ha(∙) from (6).
Definition 1 We define the nonparametric marginal Hazard Ratio and its approximation, HR(t), as
HR(t)— λι(t) _ So(t)	S1 (t)
()= λo(t) = Sι(t)	S0(t),
HR(t)= SPKM (t) mι(t)
( )= SPKM(t) mo(t),
(10)
where for HR(t) we leveraged (1) to obtain (10) and S0(t) , dS(t)/dt. The nonparametric assump-
tion for S(t) makes the computation of S0(t) challenging. Provided that S(t) is a monotonically
decreasing function, for simplicity, we fit a linear function S(t) = m ∙ t + c, and set S0(t) ≈ m. Note
that the linear model is only used for estimating S0(t) from the nonparametric estimation of S(t).
6
Under review as a conference paper at ICLR 2021
Table 1: Performance comparisons on ACTG-SYNTHETIC data, with 95% HR(t) confidence interval.
The ground truth, test set, hazard ratio is HR(t) = 0.52(0.39,0.71).
Method	CPEHE	Caus CATE	al HR(t)	C-Index (A=0, A=1)	Factual Mean COV	C-Slope (A=0, A=1)
CoxPH-Uniform	NA	NA	0.97(0.86,1.09)	NA	NA	NA
CoxPH-IPW	NA	NA	0.48(0.03,7.21)	NA	NA	NA
CoxPH-OW	NA	NA	0.60(0.53,0.68)	NA	NA	NA
Surv-BART	352.07	77.89	0.0(0.0, 0.0)	(0.706, 0.686)	0.001	(0.398, ∞)
AFT-Weibull	367.92	133.93	0.47(0.47,0.47)	(0.21, 0.267)	6.209	(0.707, 0.729)
AFT-log-Normal	377.76	157.64	0.47(0.47,0.47)	(0.675, 0.556)	6.971	(0.707, 0.729)
SR		369.47	88.55	_ 0.38(0.33,0.65) _	(0.791,0.744)	0	(0.985, 1.027)
CSA (proposed)	一 358.72	0.8	0.45(0.39,0.65)	(0.787,0.767)	0.131 —	(0.985,—1426)「
CSA-INFO (proposed)	344.3	31.19	0.53(0.41,0.67)	(0.78, 0.764)	0.13	(0.999, 1.029)
Bias from S0 (t) can be reduced by considering more complex function approximations for S(t), e.g.,
polynomial or spline. For the nonparametric estimation of S(t) we leverage the model-free popu-
lation point-estimate-based nonparametric Kaplan-Meier (Kaplan & Meier, 1958) estimator of the
survival function SPKM (t) in ChaPfuWa et al. (2020) to marginalize both factual and Counterfactual
Predictions given covariates x. The aPProximated hazard ratio, HR(t), is thus obtained by combining
the approximations SpKM(t) and m°. A similar formulation for the conditional, HR(t∣x), can also
be derived. See the SM for full details on the evaluation or HR(t) and HR(t|x). Note that for some
AFT- or CoxPH-based parametric formulations, HR(t|x), can be readily evaluated because f(ta|x)
and S (ta|x) are available in closed form.
In the experiments, We Will use HR(t) to compare different approaches against results reported in
RCTs (see Tables 1 and 3). Further, We Will use HR(t|x) to illustrate stratified treatment effects (see
Figure 2). Note that though a neural-based survival recommender system (Katzman et al., 2018)
has been previously used to estimate HR(t|x), their approach does not account for confounding or
informative censoring thus it is susceptible to bias.
Precision in Estimation of Heterogeneous Effect (PEHE) A general individualized estimation
error is formulated as PEHE
JEX [(ITE(x) - ITE(X))2], where ITE(X) is the ground truth,
τrriι-1 /	∖	ττn Γ /m ∖	/m ∖ I PT-	1	ι / ∖ ∙	ι	∙	r∙	. ∙	τ	♦
ITE(X) = ET [γ (TI) - Y (To) |X = x] and γ(∙) is a deterministic transformation. In our experi-
ments, Y(∙) is the average over samples from t,a 〜Ph,φ(TA∣X = x). Alternative estimands, e.g.,
thresholding survival times γ(TA) = I{TA > τ}, can also be considered as described above.
Average Treatment Effect (ATE) The population treatment effect estimation error is defined as
CATE = |ATE - ATe∣, where ATE = EX[ITE(x)] (ground truth) and ATE = EX[ITE(x)].
5	Experiments
We describe the baselines and datasets that will be used to evaluate the proposed counterfactual
survival analysis methods (CSA and CSA-INFO). Pytorch code including the new semi-synthetic
dataset (see below) will be made publicly available. Throughout the experiments, we use the standard
HR(t) for CoxPH based methods and (10) for all others. The bound in (5) is sensitive to α, thus we
propose approximating proxy counterfactual outcomes {YCF, δCF} for the validation set, according to
the covariate Euclidean nearest-neighbour (NN) from the training set. We select the α that minimizes
the validation loss L = LF + LCF from the set (0, 0.1, 1, 10, 100).
Baselines We consider the following competitive baseline approaches: (i) propensity weighted
CoxPH (Schemper et al., 2009; Buchanan et al., 2014; Rosenbaum & Rubin, 1983); (ii) IPM (5)
regularized AFT (log-Normal and Weibull) models; (iii) an IPM (5) regularized deterministic semi-
supervised regression (SR) model with accuracy objective from (Chapfuwa et al., 2018), as a contrast
for the proposed stochastic predictors (CSA and CSA-INFO); and (iv) survival Bayesian additive
regression trees (Surv-BART) (Sparapani et al., 2016). For CoxPH, we consider three normalized
weighting schemes: (i) inverse probability weighting (IPW) (Horvitz & Thompson, 1952; Cao et al.,
2009), where IPWi = ai + ⅛; ii) overlapping weights (OW) (Crump et al., 2006; Li et al., 2018),
ei	1-ei
where OWi = a，i ∙ (1 - ^i) + (1 - aQ ∙ ^i; and iii) the standard RCT uniform assumption. A simple
linear logistic model ^i = σ(xi; w), is used as an approximation, ^i, to the unknown propensity score
P(A = 1|X = X). See the SM for a details of the baselines.
7
Under review as a conference paper at ICLR 2021
Table 3: Performance comparisons on FRAMINGHAM data, with 95% HR(t) confidence interval.
Test set NN assignment of yCF and δCF yields biased HR(t) = 1.23(1.17,1.25), while previous large
scale longitudinal RCT studies estimated HR(t) = 0.75(0.64,0.88) (Yusuf et al., 2016).
Method	Causal HR(t)	C-Index (A=0, A=1)	Factual Mean COV	C-Slope (A=0, A=1)
CoxPH-Uniform	1.69(1.38,2.07)	NA	NA	NA
CoxPH-IPW	1.09(0.76,1.57)	NA	NA	NA
CoxPH-OW	0.88(0.73,1.08)	NA	NA	NA
Surv-BART	14.99(14.9,14.9e8)	(0.629, 0.630)	0.003	(0.232, 0.084)
AFT-Weibull	1.09(1.09,1.09)	(0.734, 0.395)	8.609	(0.857, 0.89)
AFT-log-Normal	1.55(1.46,1.55)	(0.68, 0.56)	10.415	(0.979, 0.732)
SR	0.58(0.53,0.71)	(0.601,0.57)	0	(0.491,0.63)
CSA (proposed)	1.04(1.00,1.09)	(0.763；0：728)-	0.161 ^	(07891, 0.81)-
CSA-INFO (proposed)	0.81(0.77,0.83)	(0.752, 0.651)	0.156	(0.907, 0.881)
Datasets We consider the following datasets: (i)
Framingham, is an EHR-based longitudinal car-
diovascular cohort study that we use to evaluate the
effect of statins on future coronary heart disease out-
comes (Benjamin et al., 1994); (ii) ACTG, is a longi-
tudinal RCT study comparing monotherapy with Zi-
dovudine or Didanosine with combination therapy in
HIV patients (Hammer et al., 1996); and (iii) ACTG-
Table 2: Summary statistics of the datasets.
	FRAMINGHAM	ACTG	actg-synthetic
Events (%)	26.0	26.9	48.9
Treatment (%)	10.4	49.5	55.9
N	3,435	1,054	2,139
p	32	23	23
Missing (%)	0.23	1.41	1.38
tmax (days)	7,279	1,231	1,313
Synthetic, is a semi-synthetic dataset based on actg covariates. We simulate potential outcomes
according to a Gompertz-Cox distribution (Bender et al., 2005) with selection bias from a simple
logistic model for P(A = 1|X = x) and AFT-based censoring mechanism. The generative process
is detailed in the SM. Table 2 summarizes the datasets according to (i) covariates of size p; (ii)
proportion of non-censored events, treated units, and missing entries in the N × p covariate matrix;
and (iii) time range tmax for both censored and non-censored events. Missing entries are imputed
with median or mode if continuous or categorical, respectively.
Quantitative Results Experimental results for two data-sets in Tables 1 and 3, illustrate that AFT-
based methods are high variance, inferior in calibration and C-Index than accuracy-based methods
(SR, CSA, CSA-INFO). Surv-BART is the least calibrated but low variance method. CSA-INFO and
CSA outperform all methods across all factual metrics, whereas CSA-INFO is better calibrated, low
variance but slightly lower C-Index than CSA. Note that we fit CoxPH using the entire dataset; since
it does not support counterfactual inference, we do not present factual metrics. By properly adjusting
for both informative censoring and selection bias, CSA-INFO significantly outperforms all methods
in treatment effect estimation according to HR(t) and PEHE, across non-RCT datasets, while
remaining comparable to AFT-Weibull on the RCT dataset (see the SM). Further, RCT-based results
on ACTG data in the SM illustrate comparable HR(t) across all models except for AFT-log-Normal
and Surv-BART, which overestimate, and SR, which underestimates risk. For non-RCT datasets
(actg-Synthentic and framingham), CoxPH-OW has a clear advantage over all CoxPH based
methods, mostly credited to the well-behaved bounded propensity weights ∈ [0, 1]. Interestingly, the
Framingham observational data exhibits a common paradox, where without proper adjustment of
selection and censoring bias, naive approaches would result in a counter-intuitive treatment effect
from statins. However, there is severe confounding from covariates such as age, BMI, diabetes,
CAD, PAD, MI, stroke, etc., that influence both treatment likelihood and survival time. Table
3, demonstrates that CSA-INFO is clearly the best performing approach. Specifically, its HR(t),
reverses the biased observational treatment effect, to demonstrate positive treatment from statins,
which is consistent with prior large RCT longitudinal findings (Yusuf et al., 2016).
Qualitative Results Figure 2a demonstrates that CSA-INFO matches the ground truth population
hazard, HR(t), better than alternative methods on ACTG-SYNTHETIC data. See the SM for ACTG
and framingham. Figure 2b shows sub-population log hazard ratios for four patient clusters
obtained via hierarchical clustering on the individual log hazard ratios, log HR(t|x), of the test set of
Framingham data. Interestingly, these clusters stratify treatment effects into: positive (2), negative
(1 and 3), and neutral (4) sub-populations. Moreover, the estimated density of median log HR(t|x)
values in Figure 2c illustrates that nearly 70% of the testing set individuals have log HR(t|x) < 0,
thus may benefit from taking statins. Further, we isolated the extreme top and bottom quantiles,
HR(t|x) < 0.024 and HR(t|x) > 1.916, respectively, of the median log HR(t|x) values for the
8
Under review as a conference paper at ICLR 2021
(a) ACTG-SYNTHENTIC HR(t)
(b) FRAMINGHAM log HR(t∣x)
median log HR(t∣x)
(c) Framingham log HR(t∣x) Pdf
Figure 2: (a) Inferred population HR(t) compared against ground truth (EMP) on ACTG-SYNTHETIC
data. CSA-INFO-based (b) cluster-sPecific average log HR(t|x) curves and (c) estimated density of
median log HR(t|x) values on the test set of the FRAMINGHAM dataset. Clusters assignment were
obtained via hierarchical clustering of individualized log HR(t|x) traces.
test set of Framingham, as shown in Figure 2c. After comParing their covariates, we found that
individuals with the following characteristics may benefit from taking statins: young, male, diabetic,
without Prior history (CAD, PAD, stroke or MI), high BMI, cholesterol, triglycerides, fasting glucose,
and low high-density liPoProtein. There seem to be consensus that diabetics and high-cholesterol
Patients benefit from statins (Cheung et al., 2004; Wilt et al., 2004). See SM for additional results.
6	Conclusions
We have ProPosed a unified counterfactual inference framework for survival analysis. Our aPProach
adjusts for bias from two unknown sources, namely, confounding due to covariate dePendent selection
bias and censoring (informative or non-informative). Relative to comPetitive alternatives, we demon-
strate suPerior Performance for both survival-outcome Prediction and treatment-effect estimation,
across three diverse datasets, including a semi-synthetic dataset which we introduce. Moreover,
we formulate a model-free nonParametric hazard ratio metric for comParing treatment effects or
leveraging Prior randomized real-world exPeriments in longitudinal studies.
9
Under review as a conference paper at ICLR 2021
References
Peter C Austin. Propensity-score matching in the cardiovascular surgery literature from 2004 to 2006:
a systematic review and suggestions for improvement. The Journal of Thoracic and Cardiovascular
Surgery, 2007.
Peter C Austin. The use of propensity score methods with survival or time-to-event outcomes:
reporting measures of effect similar to those used in randomized experiments. Statistics in
Medicine, 2014.
Elias Bareinboim and Judea Pearl. Controlling selection bias in causal inference. In AISTATS, 2012.
Ralf Bender, Thomas Augustin, and Maria Blettner. Generating survival times to simulate cox
proportional hazards models. Statistics in medicine, 2005.
Emelia J Benjamin, Daniel Levy, Sonya M Vaziri, Ralph B D’Agostino, Albert J Belanger, and
Philip A Wolf. Independent risk factors for atrial fibrillation in a population-based cohort: the
framingham heart study. Jama, 1994.
Ashley L Buchanan, Michael G Hudgens, Stephen R Cole, Bryan Lau, Adaora A Adimora, and
Women’s Interagency HIV Study. Worth the weight: using inverse probability weighted cox
models in aids research. AIDS research and human retroviruses, 2014.
Weihua Cao, Anastasios A Tsiatis, and Marie Davidian. Improving efficiency and robustness of the
doubly robust estimator for a population mean with incomplete data. Biometrika, 2009.
P. Chapfuwa, C. Tao, C. Li, I. Khan, K. J. Chandross, M. J. Pencina, L. Carin, and R. Henao.
Calibration and uncertainty in neural time-to-event modeling. IEEE Transactions on Neural
Networks and Learning Systems, 2020.
Paidamoyo Chapfuwa, Chenyang Tao, Chunyuan Li, Courtney Page, Benjamin Goldstein, Lawrence
Carin, and Ricardo Henao. Adversarial time-to-event modeling. In ICML, 2018.
Bernard MY Cheung, Ian J Lauder, Chu-Pak Lau, and Cyrus R Kumana. Meta-analysis of large
randomized controlled trials to evaluate the impact of statins on cardiovascular outcomes. British
journal of clinical pharmacology, 2004.
Hugh A Chipman, Edward I George, Robert E McCulloch, et al. Bart: Bayesian additive regression
trees. The Annals of Applied Statistics, 2010.
StePhen R Cole and MigUel A Herndn. Adjusted survival curves with inverse probability weights.
Computer methods and programs in biomedicine, 2004.
David R Cox. Regression models and life-tables. Journal of the Royal Statistical Society: Series B
(Methodological), 1972.
Richard K Crump, V Joseph Hotz, Guido W Imbens, and Oscar A Mitnik. Moving the goalposts:
Addressing limited overlap in the estimation of average treatment effects by changing the estimand.
Technical report, National Bureau of Economic Research, 2006.
Yifan Cui, Michael R Kosorok, Stefan Wager, and Ruoqing Zhu. Estimating heterogeneous treatment
effects with right-censored data via causal survival forests. arXiv, 2020.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In NeurIPS, 2013.
Ivdn Diaz. Statistical inference for data-adaptive doubly robust estimators with survival outcomes.
Statistics in Medicine, 2019.
Jennifer Frankovich, Christopher A Longhurst, and Scott M Sutherland. Evidence-based medicine in
the emr era. N Engl J Med, 2011.
Saurabh Gombar, Alison Callahan, Robert Califf, Robert Harrington, and Nigam H Shah. It is time
to learn from patients like mine. NPJ digital medicine, 2019.
10
Under review as a conference paper at ICLR 2021
Scott M Hammer, David A Katzenstein, Michael D Hughes, Holly Gundacker, Robert T Schooley,
Richard H Haubrich, W Keith Henry, Michael M Lederman, John P Phair, Manette Niu, et al. A
trial comparing nucleoside monotherapy with combination therapy in hiv-infected adults with cd4
cell counts from 200 to 500 per cubic millimeter. New England Journal of Medicine, 1996.
Frank E Harrell Jr, Kerry L Lee, Robert M Califf, David B Pryor, and Robert A Rosati. Regression
modelling strategies for improved prognostic prediction. Statistics in medicine, 1984.
Kristiina Hayrinen, Kaija Saranto, and Pirkko Nykanen. Definition, structure, content, use and
impacts of electronic health records: a review of the research literature. International Journal of
Medical Informatics, 2008.
Nicholas C Henderson, Thomas A Louis, Gary L Rosner, and Ravi Varadhan. Individualized
treatment effects with censored data via fully nonparametric bayesian accelerated failure time
models. Biostatistics, 2020.
MigUel A Hemgn and James M Robins. Causal inference: What if. Boca Raton: Chapman &
Hill/CRC, 2020.
Miguel A Herngn, Stephen R Cole, Joseph Margolick, Mardge Cohen, and James M Robins. Structural
accelerated failure time models for survival analysis in studies With time-varying treatments.
Pharmacoepidemiology and Drug Safety, 2005.
Miguel Angel Herngn, Babette Brumback, and James M Robins. Marginal structural models to
estimate the causal effect of zidovudine on the survival of hiv-positive men. Epidemiology, 2000.
Jennifer L Hill. Bayesian nonparametric modeling for causal inference. Journal of Computational
and Graphical Statistics, 2011.
Daniel G Horvitz and Donovan J Thompson. A generalization of sampling Without replacement from
a finite universe. Journal of the American statistical Association, 1952.
Liangyuan Hu, Jiayi Ji, and Fan Li. Estimating heterogeneous survival treatment effect in observa-
tional data using machine learning. arXiv, 2020.
Hemant IshWaran, Udaya B Kogalur, Eugene H Blackstone, Michael S Lauer, et al. Random survival
forests. The annals of applied statistics, 2008.
Ashish K Jha, Catherine M DesRoches, Eric G Campbell, Karen Donelan, SoWmya R Rao, Timothy G
Ferris, Alexandra Shields, Sara Rosenbaum, and David Blumenthal. Use of electronic health
records in us hospitals. New England Journal of Medicine, 2009.
EdWard L Kaplan and Paul Meier. Nonparametric estimation from incomplete observations. Journal
of the American statistical association, 1958.
Jared L Katzman, Uri Shaham, Alexander Cloninger, Jonathan Bates, Tingting Jiang, and Yuval
Kluger. Deepsurv: personalized treatment recommender system using a cox proportional hazards
deep neural netWork. BMC medical research methodology, 2018.
David G Kleinbaum and Mitchel Klein. Survival analysis. Springer, 2010.
Fan Li, Kari Lock Morgan, and Alan M Zaslavsky. Balancing covariates via propensity score
Weighting. Journal of the American Statistical Association, 2018.
Christopher A Longhurst, Robert A Harrington, and Nigam H Shah. A ‘green button’for using
aggregate patient data at the point of care. Health affairs, 2014.
B Mihaylova, J Emberson, L BlackWell, A Keech, J Simes, EH Barnes, M Voysey, 3A Gray, R Collins,
and C Baigent. The effects of loWering ldl cholesterol With statin therapy in people at loW risk of
vascular disease: meta-analysis of individual data from 27 randomised trials., 2012.
Xenia Miscouridou, Adler Perotte, Noemie Elhadad, and Rajesh Ranganath. Deep survival analysis:
Nonparametrics and missingness. In Machine Learning for Healthcare Conference, 2018.
11
Under review as a conference paper at ICLR 2021
Alfred Muller. Integral probability metrics and their generating classes of functions. Advances in
Applied Probability, 1997.
Judea Pearl and Elias Bareinboim. External validity: From do-calculus to transportability across
populations. Statistical Science, 2014.
Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. In
ICML, 2015.
James Robins. A new approach to causal inference in mortality studies with a sustained exposure
period—application to control of the healthy worker survivor effect. Mathematical modelling,
1986.
Paul R Rosenbaum and Donald B Rubin. The central role of the propensity score in observational
studies for causal effects. Biometrika, 1983.
Donald B Rubin. Causal inference using potential outcomes. Journal of the American Statistical
Association, 2005.
Michael Schemper, Samo Wakounig, and Georg Heinze. The estimation of average hazard ratios by
weighted cox regression. Statistics in medicine, 2009.
Uri Shalit, Fredrik D Johansson, and David Sontag. Estimating individual treatment effect: general-
ization bounds and algorithms. In ICML, 2017.
Jincheng Shen, Lu Wang, Stephanie Daignault, Daniel E Spratt, Todd M Morgan, and Jeremy MG
Taylor. Estimating the optimal personalized treatment strategy based on selected variables to
prolong survival via random survival forest with weighted bootstrap. Journal of biopharmaceutical
statistics, 2018.
Rodney A Sparapani, Brent R Logan, Robert E McCulloch, and Purushottam W Laud. Nonparametric
survival analysis using bayesian additive regression trees (bart). Statistics in medicine, 2016.
Bharath K SriPerUmbUdur, Kenji Fukumizu, Arthur Gretton, Bernhard Scholkopf, Gert RG Lanckriet,
et al. On the empirical estimation of integral probability metrics. Electronic Journal of Statistics,
2012.
Ludovic Trinquart, Justine Jacot, Sarah C Conner, and Raphael Porcher. Comparison of treatment
effects measured by the hazard ratio and by the ratio of restricted mean survival times in oncology
randomized controlled trials. Journal of Clinical Oncology, 2016.
Anastasios Tsiatis. Semiparametric theory and missing data. Springer Science & Business Media,
2007.
Mark J van der Laan and James M Robins. Unified approach for causal inference and censored data.
In Unified Methods for Censored Longitudinal Data and Causality. Springer, 2003.
Mark J Van der Laan and Sherri Rose. Targeted learning: causal inference for observational and
experimental data. Springer Science & Business Media, 2011.
CedriC Villani. Optimal transport: old and new. Springer Science & Business Media, 2008.
Stefan Wager and Susan Athey. Estimation and inference of heterogeneous treatment effects using
random forests. Journal of the American Statistical Association, 2018.
Lee-Jen Wei. The accelerated failure time model: a useful alternative to the cox regression model in
survival analysis. Statistics in medicine, 1992.
Timothy J Wilt, Hanna E Bloomfield, Roderick MacDonald, David Nelson, Indulis Rutks, Michael
Ho, Gregory Larsen, Anthony McCall, Sandra Pineros, and Anne Sales. Effectiveness of statin
therapy in adults with coronary heart disease. Archives of internal medicine, 2004.
Salim Yusuf, Jackie Bosch, Gilles Dagenais, Jun Zhu, Denis Xavier, Lisheng Liu, Prem Pais, Patricio
Lbpez-Jaramillo, Lawrence A Leiter, Antonio Dans, et al. Cholesterol lowering in intermediate-risk
persons without cardiovascular disease. New England Journal of Medicine, 2016.
12
Under review as a conference paper at ICLR 2021
Yao Zhang, Alexis Bellot, and Mihaela van der Schaar. Learning overlapping representations for the
estimation of individualized treatment effects. In AISTATS, 2020.
Lihui Zhao, Lu Tian, Hajime Uno, Scott D Solomon, Marc A Pfeffer, Jerald S Schindler, and Lee Jen
Wei. Utilizing the integrated difference of two survival functions to quantify the treatment contrast
for designing, monitoring, and analyzing a comparative clinical study. Clinical trials, 2012.
13