Under review as a conference paper at ICLR 2021
Dropout’s Dream Land: Generalization from
Learned S imulators to Reality
Anonymous authors
Paper under double-blind review
Ab stract
A World Model is a generative model used to simulate an environment. World
Models have proven capable of learning spatial and temporal representations of
Reinforcement Learning environments. In some cases, a World Model offers an
agent the opportunity to learn entirely inside of its own dream environment. In
this work we explore improving the generalization capabilities from dream envi-
ronments to reality (Dream2Real). We present a general approach to improve a
controller’s ability to transfer from a neural network dream environment to reality
at little additional cost. These improvements are gained by drawing on inspiration
from domain randomization, where the basic idea is to randomize as much of a
simulator as possible without fundamentally changing the task at hand. Generally,
domain randomization assumes access to a pre-built simulator with configurable
parameters but oftentimes this is not available. By training the World Model using
dropout, the dream environment is capable of creating a nearly infinite number
of different dream environments. Our experimental results show that Dropout’s
Dream Land is an effective technique to bridge the reality gap between dream en-
vironments and reality. Furthermore, we additionally perform an extensive set of
ablation studies.
1	Introduction
Reinforcement learning (Sutton & Barto, 2018) (RL) has experienced a flurry of success in recent
years, from learning to play Atari (Mnih et al., 2015) to achieving grandmaster level performance
in StarCraft II (Vinyals et al., 2019). However, in all these examples, the target environment is a
simulator that can be directly trained in. Reinforcement learning is often not a practical solution
without a simulator of the environment.
Sometimes the target environment is expensive, dangerous, or even impossible to interact with. In
these cases, the agent is trained in a simulated source environment. Approaches that train an agent
in a simulated environment with the hopes of generalization to the target environment experience
a common problem referred to as the reality gap (Jakobi et al., 1995). One approach to bridge the
reality gap is domain randomization (Tobin et al., 2017). The basic idea is that an agent which can
perform well in an ensemble of simulations will also generalize to the real environment (Antonova
et al., 2017; Tobin et al., 2017; Mordatch et al., 2015; Sadeghi & Levine, 2016). The ensemble
of simulations is generally created by randomizing as much of the simulator as possible without
fundamentally changing the task at hand. Unfortunately, this approach is only applicable when a
simulator is provided and the simulator is configurable.
A recently growing field, World Models (Ha & Schmidhuber, 2018), focuses on the side of this
problem when the simulation does not exist. World Models offer a general framework for optimizing
controllers directly in learned simulated environments. The learned dynamics model can be viewed
as the agent’s dream environment. This is an interesting area because it removes the need for an
agent to operate in the target environment. Some related approaches (Eukasz Kaiser et al., 2020;
Hafner et al., 2019; 2020; Sekar et al., 2020; Sutton, 1990; Kurutach et al., 2018) focus on an
adjacent problem which allows the controller to continually interact with the target environment.
Despite the recent improvements (Eukasz Kaiser et al., 2020; Hafner et al., 2019; Sekar et al., 2020;
Kim et al., 2020; Hafner et al., 2020) of World Models, none of them address the issue that World
Models are susceptible to the reality gap. The learned dream environment can be viewed as the
source domain and the true environment as the target domain. Whenever there are discrepancies be-
tween the source and target domains the reality gap can cause problems. Even though World Models
1
Under review as a conference paper at ICLR 2021
suffer from the reality gap, none of the domain randomization approaches are directly applicable be-
cause the dream environment does not have easily configurable parameters.
In this work we present Dropout’s Dream Land (DDL), a simple approach to bridge the reality
gap from learned dream environments to reality. Dropout’s Dream Land was inspired by the first
principles of domain randomization, namely, train a controller on a large set of different simulators
which all adhere to the fundamental task of the target environment. We are able to generate a nearly
infinite number of different simulators via the insight that dropout (Srivastava et al., 2014) can be
understood as learning an ensemble of neural networks (Baldi & Sadowski, 2013).
Our empirical results demonstrate the advantage of Dropout’s Dream Land over baseline (Ha &
Schmidhuber, 2018; Kim et al., 2020) approaches. Furthermore, we perform an extensive set of
ablation studies which indicate the source of generalization improvements, requirements for the
method to work, and when the method is most useful.
2	Related Works
2.1	Dropout
Dropout (Srivastava et al., 2014) was introduced as a regularization technique for feedforward and
convolutional neural networks. In its most general form, each unit is dropped with a probability p
during the training process. Recurrent neural networks (RNNs) initially had issues benefiting from
Dropout. Zaremba et al. (2014) suggests not to apply dropout to the hidden state units of the RNN
cell. Gal & Ghahramani (2016b) shortly after show that the mask can also be applied to the hidden
state units, but the mask must be fixed across the sequence during training.
In this work, we follow the dropout approach from Gal & Ghahramani (2016b) when training the
RNN. More formally, for each sequence, the boolean masks mxi, mxf, mxw, mxo, mhi, mhf,
mhw, and mho are sampled, then used in the following LSTM update:
it = Wxi(xt	mxi) + Whi(ht-1	mhi) + bi,	(1)
ft =Wxf(xtmxf)+Whf(ht-1mhf)+bf,	(2)
wt = Wxw(xt mxw) + Whw(ht-1	mhw) + bw,	(3)
ot = Wxo (xt mxo) + Who(ht-1	mho) + bo,	(4)
where xt, ht, and ct are the input, hidden state, and cell state, respectively, Wxi, Wxf , Wxw ,
Wxo ∈ Rd×r Whi, Whf, Whw, Who ∈ Rd×d are the LSTM weight matrices, and bi, bf, bw,
bo ∈ Rd are the LSTM biases. The masks are fixed for the entire sequence, but may differ between
sequences in the mini-batch.
2.2	Domain Randomization
The goal of domain randomization (Tobin et al., 2017; Sadeghi & Levine, 2016) is to create many
different versions of the dynamics model with the hope that a policy generalizing to all versions
of the dynamics model will do well on the true environment. Figure 1 illustrates many simulated
environments (ej) overlapping with the actual environment (e*). Simulated environments are often
far cheaper to operate in than the actual environment. Hence, it is desirable to be able to perform the
majority of interactions in the simulated environments.
Randomization has been applied on observations (e.g., lighting, textures) to perform robotic grasp-
ing (Tobin et al., 2017) and collision avoidance of drones (Sadeghi & Levine, 2016). Randomization
has also proven useful when applied to the underlying dynamics of simulators (Peng et al., 2018).
Often, both the observations and simulation dynamics are randomized (Andrychowicz et al., 2020).
Domain randomization generally uses some pre-existing simulator which then injects randomness
into specific aspects of the simulator (e.g., color textures, friction coefficients). Each of the simulated
environments in Figure 1 can be thought of as a noisy sample of the pre-existing simulator. To the
best of our knowledge, domain randomization has yet to be applied to entirely learned simulators.
2.3	World Models
The world model (Ha & Schmidhuber, 2018) has three modules trained separately: (i) vision module
(V); (ii) dynamics module (M); and (iii) controller (C). A high-level view is shown in Algorithm 1.
2
Under review as a conference paper at ICLR 2021
Algorithm 1 World Models: Training in dreams.
1:	Initialize parameters of V, M, and C
2:	Collect N trajectories o, d, and a from e*
3:	Optimize V on observations o
4:	Generate embeddings z for o with V
5:	Optimize M on z and d
6:	Generate dream environment e from M
7:	for iteration=1, 2, . . . do
8:	Optimize C via interactions with e
Figure 1: e* is the actual environment, and
ej's are randomized variants of the simulated
environment.
The vision module (V) is a variational autoencoder (VAE) (Kingma & Welling, 2013), which maps
image observation (o) to a lower-dimensional representation z ∈ Rn .
The dynamics model (M) is a mixture density network recurrent neural network (MDN-RNN) (Ha
& Schmidhuber, 2018; Graves, 2013). It is implemented as an LSTM followed by a fully-connected
layer outputting parameters for a Gaussian mixture model with k components. Each feature has
k different π parameters for the logits of multinomial distribution, and (μ, σ) parameters for the
k components in the Gaussian mixture. At each timestep, the MDN-RNN takes in the state z and
action a as inputs and predicts π, μ, σ. To draw a sample from the MDN-RNN, We first sample the
multinomial distribution parameterized by π, which indexes which of the k normal distributions in
the Gaussian mixture to sample from. This is then repeated for each of the n features. Depending
on the experiments, Ha & Schmidhuber (2018) also include an auxiliary head to the LSTM which
predicts whether the episode terminates (d).
The controller (C) is responsible for deciding what actions to take. It takes features produced by the
encoder V and dynamics model M as input (not the raw observations). The simple controller is a
single-layer model which uses an evolutionary algorithm (CMA-ES (Hansen & Ostermeier, 2001))
to find its parameters. Depending on the problem setting, the controller (C) can either be optimized
directly on the target environment (e*) or on the dream environment (^). This paper is focused on
the case of optimizing exclusively in the dream environment.
3	Dropout’ s Dream Land
In this work we introduce Dropout’s Dream Land (DDL). Dropout’s Dream Land is the first work
to offer a strategy to bridge the reality gap between learned neural network dynamics models and
reality. Traditional domain randomization generates many different dynamics models by randomiz-
ing configurable parameters of a given simulation. This approach does not apply to neural network
dynamics models because they generally do not have configurable parameters (such as textures and
friction coefficients). In Dropout’s Dream Land, the controller can interact with billions1 of dream
environments, whereas previous works (Ha & Schmidhuber, 2018; Kim et al., 2020) only use one
dream environment. A naive way to go about this would be to train a large collection of neural
network world models. However, this would be computationally expensive.
To keep the computational cost low, we go about this by applying dropout to the dynamics model in
order to form different dynamics models. Crucially, dropout is applied at both training time and in-
ference time of M . Each unique dropout mask applied to the dynamics model M can be viewed as a
different environment. Similar to the spirit of domain randomization, an agent is expected to perform
well in the real environment if it can perform well in all the different simulated environments.
3.1	Learning the Dream Environment
The Dropout’s Dream Land environments are built around the dynamics model M. During training
ʌ
time the controller interactions are described by Figure 2. In Figure 2 r, d, and Z are generated
entirely by M. In this work M is an LSTM where x = [z>, a>]> from equations (1)-(4). The
LSTM is followed by multiple heads for predictions of the latent state (Z), reward (r) and termination
ʌ
(d). The reward and termination heads are simple fully-connected layers. Latent state prediction is
1In practice we are bounded by the total number of steps instead of every possible environment.
3
Under review as a conference paper at ICLR 2021
Figure 3: Interactions with the real environment.
The controller being optimized only interacts
with the real environment during the final testing
phase.
Figure 2: Interactions with the dream environ-
ment. A dropout mask is sampled at every step
yielding anew MMj.
done with a MDN-RNN (Ha & Schmidhuber, 2018; Graves, 2013), but this could be replaced by
any other neural network that supports dropout (e.g., GameGAN (Kim et al., 2020)).
3.1.1	Loss Function
The dynamics model M jointly optimizes all three heads where the loss of a single transition is
defined as:
LM = Lz + αrLr + αdLd.	(5)
Here, Lz = - Pn=ι log(P：=i ∏i,jN(zi∣^i,j,σ^^2j)) is a mixture density loss for the latent state
predictions, where n is the size of the latent feature vector z, ∏i,j is the jth component probability
for the ith feature, Rij, σ^ij are the corresponding mean and standard deviation. Lr = (r - r)2 is
the square loss on rewards, where r and r are the true and estimated rewards, respectively. Ld =
ʌ ʌ ʌ
-d log(d) - (1 -d) log(1 -d) is the cross-entropy loss for termination prediction, where d and d are
the true and estimated probabilities of the episode ending, respectively. Constants αd and αr in (5)
are for trading off importance of the termination and reward objectives. The loss (LM) is aggregated
over each sequence and averaged across the mini-batch.
3.1.2	TRAINING DYNAMICS MODEL M WITH DROPOUT
At training time of M (Algorithm 1 Line 5), we apply dropout (Gal & Ghahramani, 2016b) to the
LSTM to simulate different random environments. For each input and hidden unit, we first sample
a boolean indicator with probability ptrain. If the indicator is 1, the corresponding input/hidden unit
is masked. Masks mxi, mxf, mxw, mxo, mhi, mhf, mhw, and mho are sampled independently
(Equations (1)-(4)). When training the RNN, each mini-batch contains multiple sequences. Each
sequence uses an independently sampled dropout mask. We fix the dropout mask for the entire
sequence as this was previously found to be critically important (Gal & Ghahramani, 2016b).
Training the RNN with many different dropout masks is critical in order to generate multiple differ-
ent dynamics models. At the core of domain randomization is the requirement that the randomiza-
tions do not fundamentally change the task. This constraint is violated if we do not train the RNN
with dropout but apply dropout at inference time (explored further in Section 4.4). After optimiz-
ing the dynamics model M , we can use it to construct dream environments for controller training
(Section 3.2).
In this work, we never sample masks to apply to the action (a). We do not zero out the action
because in some environments this could imply the agent taking an action (ex: moving to the left).
This design choice could be changed depending on the environment, for example, when a zero’d
action corresponds to a no-op or a sticky action.
3.2 Training the Controller
3.2.1	Interacting with Dropout’ s Dream Land
Interactions with the dream environment (Algorithm 1 Line 8) can be characterized as training time
for the controller (C) and inference time of the dynamics model (M ). An episode begins by gen-
erating the initial latent state vector Z by either sampling from a standard normal distribution or
sampling from the starting points of the observed trajectories used to train M (Ha & Schmidhuber,
2018). The hidden cell (c) and state (h) vectors are initialized with zeros.
4
Under review as a conference paper at ICLR 2021
The controller (C) decides the action to take based on Z and h. In Figure 2 the controller also
ʌ
observes r and d but these are exclusively used for the optimization process of the controller. The
controller then performs an action a on a dream environment.
A new dropout mask is sampled (with probability pinfer) and applied to M. We refer to the masked
dynamics model as Mj and the corresponding Dropout,s Dream Land environment as ej. The
current latent state Z and action a are concatenated, and passed to Mj to perform a forward pass.
ʌ
The episode terminates based on a sample from a Bernoulli distribution parameterized by d. The
dream environment then outputs the latent state, LSTM’s hidden state, reward, and whether the
episode terminates.
Itis crucial to apply dropout at inference time (of the dynamics model M) in order to create different
versions of the dream environment for the controller C. In addition to randomizing masks at every
step, we will also consider several other variants of how to apply dropout in Sections 4.3 and 4.4.
DDL’s use of dropout is different from traditional applications of dropout. In related works (Kahn
et al., 2017) Monte-Carlo (MC) Dropout (Gal & Ghahramani, 2016a) has been used to approximate
the mean and variance of output predictions from an ensemble. We emphasize that DDL does not
use MC Dropout. The purpose of DDL’s approach to dropout is to generate many different versions
of the dynamics model. More explicitly, the controller is trained to maximize expected returns across
many different dynamics models in the ensemble as opposed maximizing expected returns on the
ensemble average.
Dropout has also traditionally been used as a model regularizer. Dropout as a model regularizer
is only applied at training time but not at inference time. The usual trade-off is lower test loss
at the cost of higher training loss (Srivastava et al., 2014; Gal & Ghahramani, 2016b). However,
DDL’s ultimate goal is not lower test loss. In fact, we expect applying dropout at inference time
will consistently make the test loss worse (this will be experimentally verified in Section 4.2). The
ultimate goal is providing dream environments to a controller so that the optimal policy in Dropout’s
Dream Land also maximizes expected returns in the target environment (e*).
3.2.2	Training with CMA-ES
We follow the same controller optimization procedure as was done in World Models (Ha & Schmid-
huber, 2018) and GameGAN (Kim et al., 2020) on their DoomTakeCover experiments. We train the
controller with CMA-ES (Hansen & Ostermeier, 2001). Each controller in the population (of size
Npop) reports their mean returns on a set of Ntrials episodes generated in Section 3.2.1. As controllers
in the population do not share a dream environment, the probability of controllers interacting with
the same sequence of dropout masks is vanishingly small. Let NmaX_ep」en be the maximum number
of steps in an episode. In a single CMA-ES iteration, the population as a whole can interact with
Npop X Ntrials X Nmax_ep_len different environments. In our experiments, Npop = 64, NtriaIS = 16,
and Nmax_ep_len is 1000 for CarRacing and 2100 for DoomTakeCover. This potentially results in
> 1000000 different environments at each generation.
3.2.3	Dream Leader Board
After every fixed number of generations (25 in our experiments), the best controller in the population
(which received the highest average returns across its respective Ntrials episodes) is selected for
evaluation (Ha & Schmidhuber, 2018; Kim et al., 2020). This controller is evaluated for another
Npop X Ntrials episodes in the Dropout’s Dream Land environments. The controller’s mean across
Npop X Ntrials trials is logged to the Dream Leader Board. After 2000 generations, the controller at
the top of the Dream Leader Board is evaluated in the real environment.
3.2.4	Interacting with the Real Environment
In Figure 3 We illustrate the controller,s interaction with the real environment (e*). The controller
only interacts with the target environment during testing. These interactions are never used to modify
parameters of the controller. At test time r, d, and o are generated by the target environment (e*)
and Z is the embedding of o from the VAE (V ). The only use of M when interacting with the target
environment is producing h as a feature for the controller.
Interactions with e* do not apply dropout to the input/hidden units of M. The purpose of applying
dropout when training (Sections 3.2.1 and 3.2.2) and selecting the controller (Section 3.2.3) was to
5
Under review as a conference paper at ICLR 2021
	dream	real
random policy	N/A	210 ± 108
-GameGAN-	N/A	765 ± 482
ACtiOn-LSTM	N/A	280 ± 104
WM	1465 ± 633	849 ± 499
DDL	1221 ± 664~	933 ± 552~
Table 1: Returns from baseline methods and
DDL (ptrain = 0.05 and pinfer = 0.1) on the
DoomTakeCover-v0 environment.
	CarRacingFixedN		CarRaCing-VO
	dream	real	
random policy	N/A	-50 ± 38	--53 ± 41-
WM	641± 351	399 ± 135	388 ± 157
DDL	881± 214一	625 ± 289-	610 ± 267-
Table 2: Returns from a random policy, World Mod-
els and DDL (ptrain = 0.05 and pinfer = 0.1) on the
CarRacingFixedN and the original CarRacing-v0 en-
vironments.
force the controller to do well on a variety of environments at the cost of dynamics model accuracy
(detailed experimental results are in Section 4.2). At the time of interaction with e* the controller's
parameters are frozen and thus there is no benefit to trading dynamics model accuracy.
4 Experiments
In this section, we perform experiments on the DoomTakeCover-v0 (Paquette, 2017) and CarRacing-
v0 (Klimov, 2016) environments from OpenAI Gym (Brockman et al., 2016). These have also been
used in related works (Kim et al., 2020; Ha & Schmidhuber, 2018). Architecture details of V , M,
and C are in Appendix A.1.
DoomTakeCover is a control task in which the goal is to dodge fireballs for as long as possible.
The controller receives a reward of +1 for every step it is alive. The maximum number of frames is
limited to 2100.
CarRacing is a continuous control task to learn from pixels. The race track is split up into “tiles”.
The goal is to make it all the way around the track (i.e., crossing every tile). We terminate an episode
when all tiles are crossed or when the number of steps exceeds 1000. Let Ntiles be the total number
of tiles. The simulator (Klimov, 2016) defines the reward r at each timestep as -N00 - 0.1 if a new
tile is crossed, and -0.1 otherwise. The number of tiles is not explicitly set by the simulator. We
generated 10000 tracks and observed that the number of tiles in the track appears to follow a normal
distribution with mean 289. To simplify the reward function, we fix Ntiles to 289 in the randomly
generated tracks, and call the modified environment CarRacingFixedN.
For all experiments the controller is trained exclusively in the dream environment (Section 3.2.1)
for 2000 generations. The controller only interacts with the target environments for testing (Sec-
tion 3.2.4). The target environment is never used to update parameters of the controller. Broadly
speaking, our experiments are focused on either evaluating the dynamics model (M) or the controller
(C). Accuracy of the dynamics model is evaluated against a training and testing set of trajectories
(Appendix A.2). The controller is evaluated by returns in the dream and real environments (Ap-
pendix A.2).
4.1 Comparison with Baselines
We compare the controller trained in Dropout’s Dream Land (DDL) with the baseline World Mod-
els (WM) controller and a random policy. On the Doom environment we also compare with
GameGAN (Kim et al., 2020) and Action-LSTM (Chiappa et al., 2017)2. All controllers are trained
entirely in dream environments. Results on the dream and real environments are in Tables 1 and 2.
The CarRacing-v0 results appear different from those found in World Models (Ha & Schmidhuber,
2018) because we are not performing the same experiment. In this paper we train the controller
entirely in the dream environment and only interact with the real environment during testing. In
World Models (Ha & Schmidhuber, 2018) the controller was trained directly in the CarRacing-v0
environment.
In Table 1 we observe that DDL offers performance improvements over all the baseline approaches.
The DoomTakeCover-v0 returns from DDL are lower than the returns reported by the temperature-
regulated variant in Ha & Schmidhuber (2018). Even though the temperature-regulated variant
increases uncertainty of the dream environment it is still only capable of creating one dream en-
vironment. Furthermore, we emphasize that adjusting temperature is only useful for a limited set
of dynamics models. For example, it would not be straightforward to apply temperature to any
dynamics model which does not produce a probability density function (ex: GameGAN); whereas
2Results on GameGAN and Action-LSTM returns are from Kim et al. (2020)
6
Under review as a conference paper at ICLR 2021
Inference Dropout Rate	Inference Dropout Rate
(a) DoomTakeCover.	(b) CarRacingFixedN.
Figure 4: Loss of DDL dynamics model (Ptrain = 0.05) at different inference dropout rates.
(a) DoomTakeCover.	(b) CarRacingFixedN.
Figure 5: DDL (ptrain = 0.05) returns at different inference dropout rates in the real environments.
the DDL approach of generating many different dynamics models is useful to any learned neural
network dynamics model.
In Table 2 we observe that DDL achieves a significant performance boost over WM in both the
real CarRacing-v0 and CarRacingFixedN environment variants. This is because the WM dream
environment was easier for the controller to exploit and overfit to errors between the simulator and
reality. Forcing the controller to succeed in many different dropout environments makes it difficult
to overfit to discrepancies between the dream environment and reality.
4.2	Does Dropout Affect Accuracy of the RNN?
In this experiment, we compare the losses in (5) achieved by a standard RNN and an RNN with
dropout (ptrain = 0.05). The same training and test sets described in Appendix A.2 are used.
Standard use cases of dropout generally observe a larger training loss but lower test loss relative
to the same model trained without dropout (Srivastava et al., 2014; Gal & Ghahramani, 2016a). In
Table 3, we do not observe any immediate performance improvements of the RNN trained with
dropout. In fact, we observe worse results on the test set. It is possible that the combination of the
RNN’s small size and dropout are causing the dynamics model to underfit.
The poor performance of both DDL RNNs indicates a clear conclusion about the results from Ta-
bles 1 and 2. The improved performance of DDL relative to World Models comes from forcing the
controller to operate in many different environments and not from a single more accurate dynamics
model M.
	DoomTakeCover		CarRacingFixedN	
	training loss	test loss	training loss	test loss
without dropout	0.89	-0:91-	2.36	-3710-
with dropout	0.93 —	0.91	3.19 —	3.57
Table 3: RNN’s loss with and without dropout (ptrain = 0.05 and pinfer = 0) during training.
4.3	Does the Inference Dropout Rate Affect Dream2Real Generalization?
It is uncommon to evaluate test loss while still applying dropout. In Dropout’s Dream Land this
is important because we still apply dropout at inference time of the RNN. In this experiment we
evaluate the relationship between the inference dropout rate (pinfer), model loss, and returns in the
target environment. Model loss is measured by the loss in (5) on the test sets for varying levels of
inference dropout (pinfer). Returns in the target environment are reported based on the best controller
(Section 3.2.3) trained with varying levels of inference dropout.
Model loss results are shown in Figure 4. As expected, as the inference dropout rate is increased our
model loss increases. However, we also observe in Figure 5 that increasing the inference dropout
7
Under review as a conference paper at ICLR 2021
rate improves generalization to the target environment. We believe that the increase in returns comes
from an increase in capacity to distort the dynamics model. Figures 4 and 5 suggest that we can trade
accuracy of the dream environment for better generalization on the target environment. However,
this should only be useful up to the point where the task at hand is fundamentally changed. Figure 5
suggests this point is somewhere between 0.1 and 0.2 for pinfer, though we suspect in practice this
will be highly dependent on network architecture and the environment.
In Figure 5 we observe relatively weak returns on the real CarRacingFixedN environment when the
inference dropout rate is zero. Recall from Table 3 that the dropout variant has a much higher test
loss than the non-dropout variant on CarRacingFixedN. This means that when pinfer = 0 the single
environment DDL is able to create is relatively inaccurate. It is easier for the controller to exploit
any discrepancies between the dream environment and target environment because only a single
dream environment exists. However, as we increase the inference dropout rate it becomes harder
for the controller to exploit the dynamics model, suggesting that DDL is especially useful when it is
difficult to learn an accurate World Model.
4.4	When Should Dropout Masks be Randomized During Controller Training?
In this ablation study we evaluate when the dropout mask should be randomized during training
of C. We consider two possible approaches of when to randomize the masks. The first case only
randomizes the mask at the beginning of an episode (episode randomization). The second case
samples a new dropout mask at every step (step randomization). We also consider if it is effective
to only apply dropout at inference time but not during RNN training (i.e., pinfer > 0, ptrain = 0).
Table 4 shows the results. As can be seen, randomizing the mask at each step offers better returns
on both real environments. Better returns in the real environment when applying step randomization
comes from the fact that the controller is exposed to a much larger number (> 1000×) of dream
environments. We also observe that applying step randomization without training the dynamics
model with dropout yields a weak policy on the real environment. This is due to the randomization
fundamentally changing the task. Training the RNN with dropout ensures that at inference time the
masked RNN is meaningful.
	DoomTakeCover		CarRacingFixedN	
	dream	real	dream	real
episode randomization (Ptrain = 0.05, Pinfer =。1)	505 ± 460	786 ± 469	346 ± 287	601± 197
step randomization (Pkain = 0.05, Pinfer =。1)	1221± 664	933 ± 552	881± 214	625 ± 289
step randomization (Ptrain = 0, Pinfer =。1)	320 ± 185	339 ± 90	331± 256	-43 ± 52
4.5 DOES THE NUMBER OF DREAM ENVIRONMENTS AFFECT RETURNS?
Table 4: Returns of the controller with different frequencies to randomize the dropout mask.
Motivated by a similar experiment from Cobbe
et al. (2019), we measure the relationship be-
tween the number of inference dropout masks
available to the dream environments and returns
of the controller in the dream and real CarRac-
ingFixedN environments. Limiting the number
of masks is the same as controlling for the num-
ber of possible environments.
At the beginning of controller training we sam-
ple m sets3 of masks according to pinfer = 0.1,
yielding m dream environments. While opti-
mizing the controller (Sections 3.2.2 and 3.2.3)
we randomize the dream environment at every
step by uniformly sampling from the m possi-
varying the number of dropout masks in the real
and dream CarRacingFixedN environment.
ble dream environments (ej where 0 ≤ j < m). Increasing m forces the controller to be capable of
maximizing expected returns across a larger number of dream environments.
As can be seen from Figure 6, interestingly, a relatively small number of masks offers substantial
improvements to the controller’s ability to generalize to the target environment. We also notice that
3Each set has mxi, mxf, mxw, mxo, mhi, mhf, mhw, and mho.
8
Under review as a conference paper at ICLR 2021
using more masks does not necessarily imply better generalization. This suggests that there exists
a smarter way to sample masks, aligned with similar observations in domain randomization (Mehta
et al., 2020).
4.6	Is Dropout’ s Dream Land More than a Regularizer?
In this experiment we compare Dropout’s Dream Land with three regularization methods. First,
we consider applying the standard use case of dropout (0 < ptrain < 1 and pinfer = 0). Second,
We consider a noisy variant of M, where every Z adds a small amount of noise (E 〜 N(0, 10-8)).
Third, we apply L2 regularization to the weights (with L2 penalty λ = 0.001) during the training
process of M .
In Table 5, we do not observe any of the standard regularization methods (L2, Dropout, Noise) to
improve World Models generalization from dream environments to target environments. L2 Regu-
larized World Models and Dropout World Models can both be viewed as regularizers on M . Noisy
World Models can be viewed as a regularizer on the controller C . The strong returns on the real
environment by DDL suggest that it is doing more than standard regularization techniques.
	CarRacingFixedN		CarRaCing-v0
	dream	real	
World Models	641± 351	399 ± 135	388 ± 157-
L2 Regularized World Models	760 ± 419	--76 ± 8-	--90 ± 1-
Dropout World Models	1585 ± 268	-36 ± 19	--36 ± 20-
Noisy World Models	639 ± 350	277 ± 135	270 ± 167-
Dropout’s Dream Land	881± 211	625 ± 289~	610 ± 267-
Table 5: Returns from World Models, L2-Regularized World Models (λ = 0.001), Dropout World
Models (ptrain = 0.05 and pinfer = 0.0), Noisy (N (0, 10-8)) World Models, and DDL (ptrain = 0.05
and pinfer = 0.1) on the CarRacingFixedN and the original CarRacing-v0 environments.
4.7	Comparison to Explicit Ensemble Methods
In this experiment we compare Dropout’s Dream Land with two other approaches for randomizing
the dynamics of the dream environment. We consider using an explicit ensemble of a population of
dynamics models. Each environment in the population was trained with a different initialization and
different mini-batches. With the population of World Models we train a controller with Step Ran-
domization and a controller with Episode Randomization. Note that the training cost of dynamics
models and RAM requirements at inference time scale linearly with the population size. Due to the
large computational cost we consider a population size of 2.
In Table 6, we observe that neither Population World Models (PWM) Step Randomization or
Episode Randomization help close the Dream2Sim gap. Episode Randomization does not help
because the controller is forced to understand the hidden state (h) representation of every M in the
population. Step Randomization performs even worse than Episode Randomization because ontop
of the previously state limitations, each of the dynamics models in the population is also forced to be
compatible with the hidden state (h) representation of all other dynamics models in the population.
Even though we use a modest population size, PWM is plagued with issues that will not be fixed by
a larger population size. DDL does not suffer from any of the previously stated issues and is also
computationally cheaper because only one M must be trained as opposed to an entire population.
	CarRacingFixedN		CarRaCing-v0
	dream	real	
World Models	641± 351	399 ± 135	388 ± 157-
PWM Episode Randomization	724 ± 491	398 ± 126	402 ± 142-
PWM Step Randomization	-10 ± 20-	-78± 14	--77 ± 13-
Dropout's Dream Land	881± 214一	625 ± 289	610 ± 267-
Table 6: Returns from Population World Models (PWM) Episode Randomization, PWM Step Ran-
domization, Noisy World Models, and DDL (ptrain = 0.05 and pinfer = 0.1) on the CarRacingFixedN
and the original CarRacing-v0 environments.
5	Conclusion
In this work we introduce an approach to improve controller generalization from dream environ-
ments to reality at little cost. To the best of our knowledge this is the first work to bridge the reality
gap between learned simulators and reality. Future direction for this work could be modifying the
dynamics model parameters in a targeted manner (Wang et al., 2019; 2020; Such et al., 2019). This
simple approach to generating different versions ofa model could also be useful in committee-based
methods (Settles, 2009; Sekar et al., 2020).
9
Under review as a conference paper at ICLR 2021
References
OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew,
Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning
dexterous in-hand manipulation. International Journal ofRobotics Research, 39(1):3-20, 2020.
Rika Antonova, Silvia Cruciani, Christian Smith, and Danica Kragic. Reinforcement learning for
pivoting task. Preprint arXiv:1703.00472, 2017.
Pierre Baldi and Peter J Sadowski. Understanding dropout. In Advances in Neural Information
Processing Systems, pp. 2814-2822, 2013.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. Preprint arXiv:1606.01540, 2016.
Silvia ChiaPPa, Sebastien Racaniere, Daan Wierstra, and Shakir Mohamed. Recurrent environment
simulators. Preprint arXiv:1704.02254, 2017.
Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John Schulman. Quantifying generaliza-
tion in reinforcement learning. In International Conference on Machine Learning, PP. 1282-1289.
PMLR, 2019.
Yarin Gal and Zoubin Ghahramani. DroPout as a bayesian aPProximation: RePresenting model
uncertainty in deeP learning. In International Conference on Machine Learning, PP. 1050-1059,
2016a.
Yarin Gal and Zoubin Ghahramani. A theoretically grounded aPPlication of droPout in recurrent
neural networks. In Advances in Neural Information Processing Systems, PP. 1019-1027, 2016b.
Alex Graves. Generating sequences with recurrent neural networks. PrePrint arXiv:1308.0850,
2013.
David Ha and Jurgen Schmidhuber. Recurrent world models facilitate policy evolution. In Advances
in Neural Information Processing Systems, PP. 2450-2462, 2018.
Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. In International Conference on
Machine Learning, pp. 2555-2565, 2019.
Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning
behaviors by latent imagination. In International Conference on Learning Representations, 2020.
Nikolaus Hansen and Andreas Ostermeier. Completely derandomized self-adaptation in evolution
strategies. Evolutionary computation, 9(2):159-195, 2001.
Nick Jakobi, Phil Husbands, and Inman Harvey. Noise and the reality gap: The use of simulation in
evolutionary robotics. In European Conference on Artificial Life, pp. 704-720. Springer, 1995.
Gregory Kahn, Adam Villaflor, Vitchyr Pong, Pieter Abbeel, and Sergey Levine. Uncertainty-aware
reinforcement learning for collision avoidance. arXiv preprint arXiv:1702.01182, 2017.
Seung Wook Kim, Yuhao Zhou, Jonah Philion, Antonio Torralba, and Sanja Fidler. Learning to
simulate dynamic environments with gamegan. In IEEE Conference on Computer Vision and
Pattern Recognition, pp. 1231-1240, 2020.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. Preprint arXiv:1312.6114,
2013.
Oleg Klimov. Carracing-v0, 2016. URL https://gym.openai.com/envs/CarRacing-v0/.
Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-ensemble
trust-region policy optimization. arXiv preprint arXiv:1802.10592, 2018.
Bhairav Mehta, Manfred Diaz, Florian Golemo, Christopher J Pal, and Liam Paull. Active domain
randomization. In Conference on Robot Learning, pp. 1162-1176, 2020.
10
Under review as a conference paper at ICLR 2021
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
Igor Mordatch, Kendall Lowrey, and Emanuel Todorov. Ensemble-cio: Full-body dynamic motion
planning that transfers to physical humanoids. In IEEE/RSJ International Conference on Intelli-
gent Robots and Systems, pp. 5307-5314, 2015.
Philip Paquette. Doomtakecover-v0, 2017. URL https://gym.openai.com/envs/DoomTakeCover-v0/.
Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer
of robotic control with dynamics randomization. In IEEE International Conference on Robotics
and Automation, pp. 1-8, 2018.
Fereshteh Sadeghi and Sergey Levine. Cad2rl: Real single-image flight without a single real image.
Preprint arXiv:1611.04201, 2016.
Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak.
Planning to explore via self-supervised world models. Preprint arXiv:2005.05960, 2020.
Burr Settles. Active learning literature survey. Technical report, University of Wisconsin-Madison
Department of Computer Sciences, 2009.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning
Research, 15(1):1929-1958, 2014.
Felipe Petroski Such, Aditya Rawal, Joel Lehman, Kenneth O Stanley, and Jeff Clune. Genera-
tive teaching networks: Accelerating neural architecture search by learning to generate synthetic
training data. Preprint arXiv:1912.07768, 2019.
Richard S Sutton. Integrated architectures for learning, planning, and reacting based on approxi-
mating dynamic programming. In International Conference on Machine learning, pp. 216-224,
1990.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT press,
2018.
Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Do-
main randomization for transferring deep neural networks from simulation to the real world. In
IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 23-30, 2017.
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, JUny-
oung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster
level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.
Rui Wang, Joel Lehman, Jeff Clune, and Kenneth O Stanley. Paired open-ended trailblazer (POET):
Endlessly generating increasingly complex and diverse learning environments and their solutions.
Preprint arXiv:1901.01753, 2019.
Rui Wang, Joel Lehman, Aditya Rawal, Jiale Zhi, Yulun Li, Jeff Clune, and Kenneth O Stanley.
Enhanced POET: Open-ended reinforcement learning through unbounded invention of learning
challenges and their solutions. Preprint arXiv:2003.08536, 2020.
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization.
Preprint arXiv:1409.2329, 2014.
Eukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad
Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Afroz Mohiuddin,
Ryan Sepassi, George Tucker, and Henryk Michalewski. Model based reinforcement learning for
atari. In International Conference on Learning Representations, 2020.
11
Under review as a conference paper at ICLR 2021
A Appendix
A. 1 Architecture Details
We follow the same architecture setup as World Models. We adopt the following notation (Kim
et al., 2020) to describe the VAE architecture.
Conv2D(a, b, c): 2D-Convolution layer with output channel size a, kernel size b, and stride c. All
use valid padding and relu activations.
T.Conv2D(a, b, c): Transposed 2D-Convolution layer with output channel size a, kernel size b,
stride c. The final layer uses a sigmoid activation but every other layer uses relu activations.
LSTM(a): LSTM layer with a units.
Dense(a): Fully Connected layer with output size a followed by a relu activation.
Linear(a): Linear layer with output size a.
Reshape(a): Reshape input to output size a.
DoomTakeCover	CarRacing
-Conv2D(32, 4, 2)-	-Conv2D(32, 4, 2)-
-Conv2D(64, 4, 2)-	-Conv2D(64, 4, 2)-
-Conv2D(128, 4, 2)-	-Conv2D(128,4, 2)-
-Conv2D(256, 4, 2)-	-Conv2D(256, 4, 2)-
ReshaPe(1024)	ReshaPe(1024)
Linear(32), Linear(32)	Linear(64), Linear(64)
Dense(1024)	Dense(1024)
ReshaPe(1,1,1024)	ReshaPe(1,1,1024)
TConv2D(128, 5, 2)	TConv2D(128, 5, 2)
TConv2D(64, 5, 2)	TConv2D(64, 5, 2)
TConv2D(32, 6, 2)	TConv2D(32, 6, 2)
TConv2D(3, 6, 2)	TConv2D(3, 6, 2)
Table 7: Architecture for VAE (V ) in DoomTakeCover and CarRacing.
DoomTakeCover	CarRacing
LSTM(512)~~	LSTM(256)
Dense(96i)	Dense(482) 一
Table 8: Architecture for the dynamics model (M) in DoomTakeCover and CarRacing.
DoomTakeCover	CarRacing
Linear(1)	Linear(3)~~
Table 9: Architecture for the controller (C) in DoomTakeCover and CarRacing.
A.2 Environment Details
Means and standard deviations of returns achieved by the best controller (Section 3.2.3) in the target
environment (Section 3.2.4) are reported based on 100 trials for CarRacing and 1000 trials for Doom-
TakeCover.4 Returns in the dream environment are reported based on 1024 trials (Section 3.2.3) for
both CarRacing and DoomTakeCover.
DoomTakeCover Environment For all tasks on this environment, we collect a training set of 10000
trajectories and a test set of 100 trajectories. A trajectory is a sequence of state (z), action (a), reward
(r), and termination (d) tuples. Both datasets are generated according to a random policy. Following
the same convention as World Models (Ha & Schmidhuber, 2018), on the DoomTakeCover envi-
ronment we concatenate z, h, and c as input to the controller. In (5), we set αd = 1 and αr = 0
because the Doom reward function is determined entirely based off whether the controller lives or
dies.
4100 trials are used for the baselines GameGAN and Action-LSTM.
12
Under review as a conference paper at ICLR 2021
CarRacing Environment For all tasks on this environment, the training set contains 5000 trajec-
tories and the test set contains 100 trajectories. Both datasets are collected by following an expert
policy with probability 0.9, and a random policy with probability 0.1. The expert policy was trained
directly on the CarRacing-v0 environment and received an average return of 885 ± 63 across 100
trials. In comparison, the performance of the random policy is -53 ±41. This is similar to the setup
in GameGAN (Kim et al., 2020) on the Pacman environment which also used an expert policy. For
this environment, we set αd = αr = 1 in (5).
13