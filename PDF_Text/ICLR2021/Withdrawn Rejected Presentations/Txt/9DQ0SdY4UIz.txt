Under review as a conference paper at ICLR 2021
Effective Subspace Indexing via Interpolation
on Stiefel and Grassmann manifolds
Anonymous authors
Paper under double-blind review
Ab stract
We propose a novel local Subspace Indexing Model with Interpolation (SIM-I)
for low-dimensional embedding of image data sets. Our SIM-I is constructed
via two steps: in the first step we build a piece-wise linear affinity-aware subspace
model under a given partition of the data set; in the second step we interpolate
between several adjacent linear subspace models constructed previously using the
“center of mass” calculation on Stiefel and Grassmann manifolds. The resulting
subspace indexing model built by SIM-I is a globally non-linear low-dimensional
embedding of the original data set. Furthermore, the interpolation step produces
a “smoothed” version of the piece-wise linear embedding mapping constructed
in the first step, and can be viewed as a regularization procedure. We provide
experimental results validating the effectiveness of SIM-I, that improves PCA
recovery for SIFT data set and nearest-neighbor classification success rates for
MNIST and CIFAR-10 data sets.
1 Introduction
Subspace selection algorithms have been successful in many application problems related to dimen-
sion reduction (Zhou et al. (2010), Bian & Tao (2011), Si et al. (2010), Zhang et al. (2009)), with
applications including, e.g., human face recognition (Fu & Huang (2008)), speech and gait recogni-
tion (Tao et al. (2007)), etc.. The classical approaches of subspace selection in dimension reduction
include algorithms like Principle Component Analysis (PCA, see Jolliffe (2002)) and Linear Dis-
criminant Analysis (LDA, see Belhumeur et al. (1997), Tao et al. (2009)). They are looking for
globally linear subspace models. Therefore, they fail to estimate the nonlinearity of the intrinsic
data manifold, and ignore the local variation of the data (Saul & Roweis (2003), Strassen (1969)).
Consequently, these globally linear models are often ineffective for search problems on large scale
image data sets. To resolve this difficulty, nonlinear algorithms such as kernel algorithms (Ham et al.
(2004)) and manifold learning algorithms (Belkin et al. (2006), Guan et al. (2011)) are proposed.
However, even though these nonlinear methods significantly improve the recognition performance,
they face a serious computational challenge dealing with large-scale data sets due to the complexity
of matrix decomposition at the size of the number of training samples.
Figure 1: The idea of “smoothing” a piece-wise linear low-dimensional embedding model: (a)
The piece-wise linear low-dimensional embedding model built from LPP; (b) The regularized low-
dimensional embedding by taking Stiefel/Grassmann manifold center-of-mass among adjacent lin-
ear pieces.
1
Under review as a conference paper at ICLR 2021
Here we propose a simple method, Subspace Indexing Model with Interpolation (SIM-I), that
produces from a given data set a piece-wise linear, locality-aware and globally nonlinear model of
low-dimensional embedding. SIM-I is constructed via two steps: in the first step we build a piece-
wise linear affinity-aware subspace model under a given partition of the data set; in the second step
we interpolate between several adjacent linear subspace models constructed previously using the
“center of mass” calculation on Stiefel and Grassmann manifolds (Edleman et al. (1999), Kaneko
et al. (2013), Marrinan et al. (2014)). The interpolation step outputs a “smoothed” version (Figure 1)
of the original piece-wise linear model, and can be regarded as a regularization process. Compared to
previously mentioned subspace methods, SIM-I enjoys the following advantages: (1) it captures the
global nonlinearity and thus the local fluctuations of the data set; (2) it is computationally feasible
to large-scale data sets since it avoids the complexity in matrix decomposition at the size of the
number of training samples; (3) it includes a regularization step via interpolating between several
adjacent pieces of subspace models. Numerical experiments on PCA recovery task for SIFT data
set and classification tasks via nearest-neighbor method for MNIST and CIFAR-10 data sets further
validate the effectiveness of SIM-I.
2 Piece-wise linear Locality Preserving Projection (LPP) model
If an image data point x ∈ RD is represented as a vector in a very high-dimensional space, then
we want to find a low-dimensional embedding y = f (x) ∈ Rd, d << D such that the embedding
function f retains some meaningful properties of the original image data set, ideally close to its
intrinsic dimension. If we restrict ourselves to linear maps of the form y = WTx ∈ Rd, where the
D × d projection matrix W = (wij)1≤i≤D,1≤j≤d (assuming full rank), then such a procedure is
called a locally linear low-dimensional embedding (see Roweis & Saulm (2000); Van Der Maaten
et al. (2009)). The target is to search for a “good” projection matrix W , such that the projection
x 7→ y = W T x must preserve certain locality in the data set (this is called a Locality Preserving
Projection, or LPP projection, see He & Niyogi (2003)). The locality is interpreted as a kind of
intrinsic relative geometric relations between the data points in the original high-dimensional space,
usually represented by the affinity matrix S = (sij)1≤i,j≤n (which is a symmetric matrix with
non-negative terms). As an example, given unlabelled data points x1, ..., xn ∈ RD, we can take
Sij = exp (- kxi-xjk ) when ∣∣xi - Xjk < ε and Sij = 0 otherwise. Here σ > 0 and ε > 0 is a
small threshold parameter, and kxi - xj k is the Euclidean norm in RD. Based on the affinity matrix
S = (Sij), the search for the projection matrix W can be formulated as the following optimization
problem
1n
mWinφ(W) = 2 E Sijkyi - yjk2,	(I)
i,j=1
in which yi = WTxi and yj = WTxj and the norm kyi - yj k is taken in the projected space Rd.
Usually when kxi - xj k is large, the affinity Sij will be small, and vice versa. Thus (1) is seeking
for the embedding matrix W such that close pairs of image points xi and xj will be mapped to close
pairs of embeddings yi = WTxi and yj = WTxj, and vice versa. This helps to preserve the local
geometry of the data set, a.k.a the locality. To solve (1), we introduce a weighted fully-connected
graph G where the vertex set consists of all data points x1, ..., xn and the weight on the edge con-
necting xi and xj is given by Sij ≥ 0. Consider the diagonal matrix D = diag(D11, ..., Dnn) where
n
Dii = P Sij , and we then introduce the graph Laplacian L = D - S. Then the minimization
j=1
n
problem (1), together with the normalization constraint P Diiyi2 = 1, reduces to the following
i=1
generalized eigenvalue problem (see He & Niyogi (2003))
XLXTw= λXDXT w,	(2)
where X = [x1, ..., xn] ∈ RD×n.
Assume we have obtained an increasing family of eigenvalues 0 = λ0 < λ1 ≤ ... ≤ λn-1. Let the
corresponding eigenvectors be w0, w1, ..., wn1. Then the low-dimensional embedding matrix can
2
Under review as a conference paper at ICLR 2021
be taken as W = [w1, ..., wd] (see He et al. (2005)). By choosing different affinity matrices S =
(sij ), the above LPP framework includes many commonly seen practical examples. For example, if
the data xi,…，xn are not labelled, then we can take Sij = — and (2) produces the classical Principle
n
Component Analysis (PCA). For labelled data forming subsets X1, ..., Xm with same labels in each
subset, We can take Sij =——when Xi, Xj ∈ Xk, and Sij = 0 other-wise. Here n is the cardinality
of Xk . This will produce Linear Discriminant Analysis (LDA). The detailed justifications of these
connections can be found in He et al. (2005).
Given an input data set X = {X1, ..., Xn} where each Xi ∈ RD, either labelled or unlabelled, we
can apply ak-d tree (Bentley (1975), Wang et al. (2011)) based partition scheme to divide the whole
data set X into non-overlapping subsets C1, ..., C2h where h is the depth of the tree. Conventional
subspace selection algorithms could be applied on the whole sample space before the whole space
is partitioned and indexed. For example, we can first apply a PCA to X, which selects the first d
bases [a1, ..., ad] with largest variance. Based on these bases, the covariance information obtained
from global PCA is utilized in the indexing as follows: (1) we project all sample points X1, ..., Xn
onto the maximum variance basis a1, then we find the median value m1 of the projected samples,
and split the whole collection of data along a1 at m1 , i.e., split the current node into left and right
children; (2) starting from level i = 2, for each left and right child, project the whole collection of
data along the i-th maximum variance basis ai , find the median value mi, and split all the children
at mi ; (3) increase the level from i to i + 1 and repeat (2) until i = h reaches the bottom of the tree.
We collect all the 2h children at level i = h and obtain the disjoint subsets C1, ..., C2h. Each subset
Ck, k = 1, 2, ..., 2h consists of a family of input data in RD. Based on them, using the above LPP
framework, for each Ck, a low-dimensional embedding matrix Wk ∈ RD×d can be constructed.
In this way, over the whole data set X, we have constructed a piece-wise linear low-dimensional
embedding model f(X) : RD → Rd (see Figure 1(a)) where X ∈ X. This model is given by the
linear embedding matrices W1 , ..., W2h ∈ RD×d .
The above model construction can be regarded as a training process from the data set X. For a given
test data point X ∈ RD, not included in X, we can find the closest subset Ck(x) to it, by selecting the
index k = k(X) ∈ {1, ..., 2h} with the smallest distance kX - mk(x) k. Here mk is the mean among
all data points in the subset Ck. With the subset Ck(x) chosen, we map the test point X ∈ RD to
its low-dimensional embedding f(X) = WkT(x)X ∈ Rd. Such a procedure extended the piece-wise
linear embedding model f(X) : RD → Rd to all testing data points in RD.
3	Calculating the “center of mass” on Stiefel and Grassmann
MANIFOLDS
Subspace Indexing (Wang et al. (2011)) provides a d-dimensional representation of the data set
{X1, ..., Xn} by the subspace span(w1, ..., wd) = {WTX, X ∈ RD} generated from the linear em-
bedding matrix W ∈ RD×d. In this case, we are only interested in the column space of W, so we
can assume that w0, w1, ..., wn-1 is an orthonormal basis 1. Such a matrix W belongs to the Stiefel
manifold, defined by
Definition 1 (Stiefel manifold) The compact Stiefel manifold St(d, D) is a submanifold of the Eu-
cilidean space RD×d such that
St(d, D) = {X ∈ RD×d : XTX = Id} .	(3)
As an example, if we are interested in signal recovery using low-dimensional PCA embedding, the
projections we calculated from PCA analysis will be on Stiefel manifolds. However, for classifica-
tion tasks, the exact distance information is less important than label information. In this case, two
such Stiefel matrices W1 and W2 produce the same embedding ifW1 = W2Od for some Od ∈ O(d),
where O(d) is the group of orthogonal matrices in dimension d. In this case, the relevant embedding
we obtained is a point on the Grassmann manifold, defined by
1 If this is not the case, we can replace the matrix [w0 w1 ... wn-1] by the Q matrix of the QR-decomposition
of itself, without changing the corresponding subspace.
3
Under review as a conference paper at ICLR 2021
Definition 2 (Grassmann manifold) The Grassmann manifold Gr(d, D) is defined to be the quo-
tient manifold Gr(d, D) = St(d, D)/O(d). A point on Gr(d, D) is defined by an equivalence class
[W] = {WOd,Od ∈ O(d)} where W ∈ St(d, D).
Given a family of elements on the Stiefel or Grassmann manifold, the center-of-mass is defined as
an element on the same manifold that minimizes the functional given by the weighted sum of square
distances. To be precise, we have
Definition 3 (Stiefel and Grassmann center-of-masses) Given a sequence of matrices W1, ..., Wl
∈ St(d, D) and a sequence of weights w1, ..., wl > 0, the Stiefel center-of-mass with respect to
the distance d(W1, W2) on St(d, D) is defined as a matrix Wc = WcSt(W1, ..., Wl; w1, ..., wl) ∈
St(d, D) such that
l
wjd2(W,Wj) .
j=1
Wc = WcSt(W1 , ..., Wl; w1 , ..., wl) ≡ arg min
c	W ∈St(d,D)
(4)
Similarly, if the corresponding equivalent classes are [W1], ..., [Wl] ∈ Gr(d, D), then the Grass-
mann center-of-mass with respect to the distance d([W1], [W2]) on Gr(d, D) is defined as the equiv-
alence class [Wc], where Wc = WcGr (W1, ..., Wl; w1, ..., wl) ∈ St(d, D) is such that
l
Wc = WcGr(W1, ..., Wl;w1, ..., wl) ≡ arg min	Xwjd2([W], [Wj]) .
W ∈St(d,D) j=1
(5)
The distances d(W1, W2) or d([W1], [W2]) can be taken in different ways. For example, for
W1, W2 ∈ St(d, D), one way is to consider d(W1, W2) = dF(W1, W2) = kW1 - W2 kF, the ma-
trix Frobenius norm of W1 - W2 . One can also take a more intrinsic distance, such as the geodesic
distance between W1 and W2 on the manifold St(d, D) with the metric given by embedded geom-
etry (see Edleman et al. (1999)). For [W1], [W2] ∈ Gr(d, D), one way is to consider the projected
Frobenius norm d([W1], [W2]) = dpF([W1], [W2]) = 2-1/2kW1W1T - W2W2TkF. There are also
many other choices, such as using the principle angles between the subspaces, chordal norms, or
other types of Frobenius norms (see Edleman et al. (1999, Section 4.3)).
With respect to matrix Frobenius norm and projected Frobenius norm, the Stiefel and Grassmann
center-of-masses can be calculated explicitly in the following theorems.
Theorem 1 (Stiefel center-of-mass with respect to Frobenius norm) We consider the singular
value decomposition of the matrix Plj=1 wjWj = O1∆O2, where O1 ∈ O(D) and O2 ∈ O(d),
∆ = diag(0λ1, ..., λd)d×d and λ1 ≥ ... ≥ λd ≥ 0 are the singular values. Then the Stiefel
center-of-mass with respect to the distance given by Frobenius norm d(W1, W2) = kW1 - W2 kF is
given by Wc = O1ΛO2 where Λ = diag(1,..., 1)d×d .
0(D-d)×d
Theorem 2 (Grassmann center-of-mass with respect to projected Frobenius norm) Set Ωj =
Plj=1 wj	wj .	We consider the singular value decomposition of the symmetric matrix
l
P ΩjWjWjT = Q∆Qτ where Q ∈ O(D) and ∆ = diag(σ2,…，σD), σ2 ≥ ... ≥ σD ≥ 0.
j=1
Then the Grassmann center-of-mass with respect to the distance given by projected Frobenius
norm dpF ([W1], [W2]) = 2-1/2 kW1 W1T - W2W2T kF is the equivalence class [Wc] determined
by Wc = QΛ, where Λ = diag(1, ..., 1)d×d.
0(D-d)×d
4	SIM-I: Interpolating the LPP model family
Recall that we have developed a piece-wise linear embedding model f(x) : RD → Rd over the data
set X = {x1 , ..., xn}. The embedding f(x) corresponds to a family of subspace indexing models
4
Under review as a conference paper at ICLR 2021
Figure 2: There are I = 3 nearby subsets C1, C2, C3 with means m1, m2, m3 in the training set.
(a) Test point x is apparently close to m1, and thus the low-dimensional embedding f(x) is taken
as f(x) = W1T x, where W1 is the LPP subspace based on C1; (b) Test point x has approximately
the same distances to m1 , m2, m3, and thus the embedding f(x) is taken as f(x) = WcT x, where
Wc is the Stiefel/Grassmann center-of-mass for the LPP subspace models W1, W2, W3 based on
C1,C2,C3.
W1, ..., W2h ∈ St(d, D) (e.g. for PCA signal recovery tasks) or [W1], ..., [W2h] ∈ Gr(d, D) (e.g.
for classification tasks). Each subspace model Wk is built from LPP embedding using the subset
Ck ⊂ X developed from k-d tree and h is the depth tree. Given a test point x ∈ RD that does not
lie in X, we can map it to the low-dimensional embedding f(x) = WkT(x)x ∈ Rd. The index k(x)
corresponds to the subset Ck(x) that lie closest to x. In practice, we can first compute the means mk
over all the data points in the subset Ck for each k = 1, 2, ..., 2h and sort the distances kx - mkk in
ascending order kx - mk1(x)k ≤ ... ≤ kx - mk2h (x) k, {k1(x), ..., k2h (x)} = {1, ..., 2h}. We then
take k(x) = k1 (x) to be the index k corresponding to the shortest distance. This is effective when
the test point x lies significantly close to one of the subsets Ck(x), see Figure 2(a).
Algorithm 1 SIM-I: Subspace Indexing Model with Interpolation
1:	Input: Data set X = {x1 , ..., xn ∈ RD} and its corresponding affinity matrix S =
(sij)1≤i,j≤n; test point x ∈ RD; threshold ratio rthr > 1; tree depth h; parameter K > 0
2:	Using an initial PCA and a k-d tree based partition scheme, decompose the data set X into
subsets C1, ..., C2h, where h is the depth of the tree
3:	For each subset Ck, calculate its mean (center) mk ∈ RD and its LPP embedding matrix Wk ∈
St(d, D) based on the affinity matrix S
4:	Sort the distances kx - mkk in ascending order kx - mk1(x) k ≤ ... ≤ kx - mk h(x) k,
{k1(x),...,k2h(x)}={1,...,2h}
5:	Determine I, which is the first sub-index i of ki(x) such that kx-mkI+1(x) k > rthrkx-mk1(x) k
6:	Set ji(x) = ki(x) for i = 1, 2, ..., I and obtain the embedding matrices Wj1(x) , ..., WjI(x) ∈
St(d, D) or their corresponding subspaces [Wj1(x)], ..., [WjI(x)] ∈ Gr(d, D), together with the
weights wi = exp(-K kx - mji(x) k2) > 0 for i = 1, ..., I
7:	Find a center-of-mass Wc = WcSt(Wj1(x) , ..., WjI(x) ; w1, ..., wI) (Stiefel case) or [Wc] =
[WcGr(Wj1(x) , ..., WjI (x) ; w1, ..., wI)] (Grassmann case) according to Definition 3 and Theo-
rems 1 and 2.
8:	Output: The low-dimensional embedding f (x) = WcTx ∈ Rd
However, for a general test point x ∈ RD , it might happen that this point lies at approximately the
same distances to the centers of each of the several different subsets adjacent to x (see Figure 2(b)).
5
Under review as a conference paper at ICLR 2021
In this case, we aim to interpolate between several subspace indexing models Wj1(x) , ..., WjI(x). To
do this, we first find the subspace indexes j1(x), ..., jI (x) from the first I subsets Cj1(x), ..., CjI (x)
closest to x, i.e., j1 (x) = k1(x), ..., jI (x) = kI (x) given the sorted distances kx - mk k mentioned
above. In practice, the number I = I(x) is depending on x and can be chosen in the following way:
I is the first sub-index i of ki(x) such that kx - mkI+1 (x) k > rthrkx - mk1 (x) k, where rthr > 1 is a
threshold ratio that can be tuned. We then pick the weights as wi = exp(-Kkx-mji(x) k2) for some
K > 0 and i = 1, 2, ..., I. This is indicating that the closer x is to Cji(x), the heavier weights we as-
sign to Wji(x) in the interpolation process. Given the embedding matrices Wj1(x) , ..., WjI(x) ∈
St(d, D) or their corresponding subspaces [Wj1(x)], ..., [WjI(x)] ∈ Gr(d, D), together with the
weights w1 , ..., wI > 0, we find a center-of-mass Wc = WcSt(Wj1(x) , ..., WjI (x) ; w1, ..., wI)
(Stiefel case) or [Wc] = [WcGr(Wj1(x), ..., WjI(x); w1, ..., wI)] (Grassmann case) according to Def-
inition 3 and Theorems 1 and 2. Finally, we map the test point x to the low-dimensional embedding
f (x) = WcTx ∈ Rd. Notice that when I = 1, the interpolation procedure reduces to projecting
x using Wk1(x) calculated from LPP analysis on the closest subset only. In general, the whole in-
terpolation procedure can be regarded as providing a regularized version of the piece-wise linear
embedding we discussed in Section 2 (also see Figure 1). We summarize our interpolation method
as the SIM-I Algorithm 1.
5 Experiments
5.1	PCA Recovery for SIFT data set
SIFT (Scale Invariant Feature Transform, see Lowe (2004)) data set is a data set that computes for
each keypoint a real-valued descriptor, based on the content of the surrounding patch in terms of
local intensity gradients. Given its remarkable performance, SIFT has been often used as starting
point for the creation of other descriptors. The final SIFT descriptor consists of 128 elements. This
means that each data point in SIFT data set has dimension D = 128, and we pick the embedding
dimension d = 16. The original SIFT data set has 10068850 data samples, that form the data
set sift_sample. We randomly collect n廿出口 = 200 X 213 elements from these sample points
as our target data set X = sift_train = {x1,..., xntrain ∈ RD}. We consider the recovery
efficiency of PCA embedding of the SIFT data set. Let X be a point in sift_sample and let
W be a Stiefel matrix in St(16, 128). The projection of x onto the 16-dimensional space is then
denoted by y = WTx. By recovery we meant to consider the point xb = (WT)-y where (WT)-
is the Moore-Penrose pseudo-inverse of WT. The recovery efficiency can then be quantified by the
recovery error kx - xbk, where the Euclidean norm is computed in R128 .
We pick h = 13 so that 2h = 8192. Then We decompose sift_train into 8192 SUb-
sets C1, ..., C8192 using k-d tree based partition. For each subset Ck, we calculate the mean
mk ∈ R128 and We obtain a PCA embedding matrix Wk ∈ St(16, 128). We sort the distances
kx - mkk, k = 1, 2, ..., 8192 in ascending order so that kx - mk1 k ≤ ... ≤ kx - mk8192 k, Where
{k1, ..., k8192} = {1, ..., 8192}. Then We find among the subset means mk, k = 1, 2, ..., 8192 the
first I nearest to x, With their indexes denoted by ki = ki(x), i = 1, .., I. The number I = I(x)
is depending on x and is chosen in the folloWing Way: I is the first sub-index i of ki such that
kx - mkI+1 k > rthrkx - mk1 k, Where rthr > 1 is a threshold ratio. We pick rthr = 2.
For test data set, we randomly pick from sift_sample\sift_train a subset of size ntest = 500,
and we denote the data set as sift_test. For each test point X ∈ sift_test, we consider
sending it to the nearest subset Ck1 and the corresponding Stiefel matrix is Wk1 ∈ St(16, 128).
We can then consider the recovery point Xb = (WkT1)-WkT1X and the benchmark recovery error
ErrOjbm = ∣∣x 一 b∣∣.
Consider the alternative recovery scheme using our method SIM-I. We calculate the weights wj =
exp(-K∣∣x 一 mkj k2) for j = 1,..., I and we choose the constant K = 10-8. We then find the
Stiefel center-of-mass Wc = WcSt(Wk1(x), ..., WkI(x); w1, ..., wI) using Theorem 1 and taking the
distance function to be the matrix Frobenius norm d(W1, W2) = ∣W1 一 W2∣F. We consider the
recovery point bc = ((Wc)T)-(Wc)TX and the recovery error Error_c = ∣∣x — bbc∣.
Over the test set sift_test, we find that for about 94.2% test points, Error_c < Error_bm,
which implies that SIM-I improved the efficiency of recovery. The empirical average Error_c is
6
Under review as a conference paper at ICLR 2021
Figure 3: Comparison of the PCA Recovery Errors: Blue = benchmark case using closest subset
PCA recovery, with the error sorted from low to high; Red = using SIM-I based on Stiefel center-
of-mass and d(W1, W2) = kW1 - W2 kF.
Figure 4: Differences Errojc — Error_bm in descending order.
402.089506, and the empirical average Error_bm is 454.452314. Figure 3 plots the Error_c and
Errojbm (vertical axis) as functions of the test sample indexes. The red curve is for Error_c and
blue curve is for Error_bm, where We have sorted Error_bm in ascending order and reordered the test
indexes correspondingly. Figure 4 gives the differences Error_c — Errojbm in descending order. It
can be apparently seen that Error_c — Error_bm < 0 for most of test samples.
5.2	Nearest-Neighbor Classification for MNIST and CIFAR-10
Here we consider a labeled data set data = {(xi, yi), i = 1, 2, ..., N} where xi ∈ RD are the
inputs and yi ∈ N are the labels. We randomly select the training set data_train = {(xi, Yi), i =
1,2, ...,ntrain} and the test set data_test = {(ai,bi),i = 1, 2, ...,ntest}, and we make them
disjoint. We first project the training set onto a kd_PCA-dimensional subspace via a standard PCA.
7
Under review as a conference paper at ICLR 2021
Based on this initial embedding, using a kd-tree with height h, We divide data_train into 2h
clusters Ck, k = 1, 2,..., 2h. For each Ck, we find an LPP embedding matrix Wk ∈ St(kd_LPP, D)
by setting the affinity matrix to be sij = exp(-kxi - xj k2) if xi, xj ∈ Ck are in the same class and
sij = 0 otherwise. Since we are having a classification problem, we can identify each Wk by the
subspace it spans, i.e., we consider [Wk] which is the equivalence class of Wk in Gr(kd_LPP, D).
As before, we compute in RD the means mk of each cluster Ck. For a test point X ∈ data_test,
we sort the distances kx - mk k, k = 1, 2, ..., 2h in ascending order so that kx - mk1 k ≤ ... ≤
kx - mk h k, where {k1, ..., k2h} = {1, ..., 2h}. Then we find among the cluster means mk, k =
1, 2, ..., 2h the first I nearest to x, with their indexes denoted by ki = ki(x), i = 1, ..., I. The
number I = I(x) is depending on x and is chosen in the following way: I is the first sub-index i of
ki such that kx - mkI+1 k > rthrkx - mk1 k, where rthr > 1 is a threshold ratio. The parameter rthr
can be treated as a hyper-parameter that we can tune here.
For baseline method, we do a nearest-neighbor classification for x on a low-dimensional embedding
of the cluster Ck1, and we pick the number of nearest-neighbors to be knn ≥ 1. Indeed we project
x and all training data points in Ck1 using Wk1 , and perform nearest-neighbor classification on the
resulting projection.
For our method SIM-I, we take the union Ck1 ∪ ... ∪ CkI. Recall that each Cki corresponds to an
LPP embedding projection matrix Wki. We set the weights wi = exp(-Kkx - mki k2), and we
pick K = 10-8 . We compute a center-of-mass Wc of the projection matrices Wki with weights
wi, i = 1, 2, ..., I using the Grassmann center-of-mass method, where the distance is taken as the
projected Frobenius norm of Grassmann matrices, i.e., d(W1, W2) = 2-1/2kW1W1T - W2 W2T kF
and k • kF is the matrix Frobenius norm. We obtain Wc = WcGr(Wk1(x) , ..., WkI (x) ; w1, ..., wI)
and we project x and all training data points in the union Ck1 ∪ ... ∪ CkI using Wc. We then
perform a nearest-neighbor classification for x on this low-dimensional embedding with the number
of nearest-neighbors being equal to knn.
Table 1: Nearest-neighbor classification success rates.
data set	tree height	rthr	knn	baseline	SIM-I with Grassmann center
MNIST	8	~U1~	1	93.58%	96.55%
MNIST	8	~U1~	75	87.52%	94.21%
CIFAR-10	8		1	29.49%	33.04%
CIFAR-10	8	~ΓT~	75	30.25%	32.74%
We have been experimenting on 2 different data sets, for both of them we pick kd_PCA = 128,
kd_LPP = 100: (1) The MNIST data set, with N = 70000,以面口 = 60000, ntest = 10000 data
points and D = 784; (2) The CIFAR-10 data set, with N = 60000, ntrain = 50000, ntest = 10000
data points and D = 3072. Table 1 shows the results, where the last 2 columns are the nearest-
neighbor classification success rates for baseline method, and for SIM-I using Grassmann center-
of-mass and the rows are for different experiments. The first 4 columns are for data sets, kd-tree
height, threshold value rthr, the number of nearest-neighbors knn, respectively. We slightly tuned
rthr to reach best performances. Clearly, SIM-I has its advantage.
References
P. N. Belhumeur, J. P. Hespanha, and D. J. Kriegman. Eigenfaces vs. fisherfaces: Recognition using
class specific linear projection. IEEE Trans. PatternAnal. Mach. Intell.,19(7):711-720,1997.
M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: A geometric framework for
learning from labeled and unlabeled examples. J. Mach. Learn. Res., 1:1-48, 2006.
J. L. Bentley. Multidimensional binary search trees used for associative searching. Communications
of the ACM, 18(9):509-517, 1975.
W. Bian and D. Tao. Max-min distance analysis by using sequential SDP relaxation for dimension
reduction. IEEE Trans. Pattern Anal. Mach. Intell., 33(5):1037-1050, 2011.
8
Under review as a conference paper at ICLR 2021
A. Edleman, T. Arias, and S.T. Smith. The Geometry of Algorithms with Othogonality Constraints.
SIAM Journal on Matrix Analysis and Applications, 20(2), 1999.
Y. Fu and T.-S. Huang. Image classification using correlation tensor analysis. IEEE Trans. Image
Process.,17(2):226-234, 2008.
N. Guan, D. Tao, Z. Luo, and B. Yuan. Manifold regularized discriminative non-negative matrix
factorization with fast gradient descent. IEEE Trans. Image Process, 2011.
J. Ham, D. D. Lee, S. Mika, and B. SchOlkopf. A kernel view of the dimensionality reduction of
manifolds. presented at the Int. Conf. Mach. Learning, 2004.
X. He and P. Niyogi. Locality Preserving Projections. Advances in Neural Information Processing
Systems (NIPS), 2003.
X. He, S. Yan, Y. Hu, P. Niyogi, and H-J. Hong-Jiang Zhang. Face Recognition Using Laplacian-
faces. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(3), 2005.
I. T. Jolliffe. Principal Component Analysis, 2nd ed. new york. 2002.
T. Kaneko, S. Fiori, and T. Tanaka. Empirical Arithmetic Averaging Over the Compact Stiefel
Manifold. IEEE Transactions on Signal Processing, 61(4), 2013.
D. G. Lowe. Distinctive image features from scale-invariant key points. Int. J. Comput. Vis., 60(2):
91-110, 2004.
T. Marrinan, J. R. Beveridge, B. Draper, and M. Kirby. Finding the Subspace Mean or Median to
Fit Your Need. CVPR, 2014.
S.T. Roweis and L.K Saulm. Nonlinear dimensionality reduction by Locally Linear Embedding.
Science, 290(5500):2323-2326, 2000.
L. K. Saul and S. T. Roweis. Think globally, fit locally: Unsupervised learning of low dimensional
manifolds. J. Mach. Learn. Res., 4:119-155, 2003.
S. Si, D. Tao, and B. Geng. Bregman divergence-based regularization for transfer subspace learning.
IEEE Trans. Knowl. Data Eng., 22(7):929-942, 2010.
V. Strassen. Gaussian elimination is not optimal. Numer Math., 13:54-356, 1969.
D. Tao, X. Li, X. Wu, and S. J. Maybank. General tensor discriminant analysis and gabor features
for gait recognition. IEEE Trans. Pattern Anal. Mach. Intell., 29(10):1700-1715, 2007.
D. Tao, X. Li, X. Wu, and S. J. Maybank. Geometric mean for subspace selection. IEEE Trans.
Pattern Anal. Mach. Intell., 31(2):260-274, 2009.
L. Van Der Maaten, E. Postma, and J. Van den Herik. Dimensionality reduction: a comparative
review. Tilburg University Technical Report, TiCC-TR 2009-005, 2009.
X. Wang, Z. Li, and D. Tao. Subspace Indexing on Grassmann Manifold for Large Scale Multimedia
Retrieval. IEEE Trans on Image Processing, 20(9):2627-2635, 2011.
T. Zhang, D. Tao, X. Li, and J. Yang. Patch alignment for dimensionality reduction. IEEE Trans.
Knowl. Data Eng., 21(9):1299-1313, 2009.
T. Zhou, D. Tao, and X. Wu. Manifold elastic net: A unified frame-work for sparse dimension
reduction. Data Min. Knowl. Disc., 22(3):340-371, 2010.
9
Under review as a conference paper at ICLR 2021
A Proof of Theorem 1
Let D ≥ d ≥ 1. Recall St(d, D) stands for the Stiefel manifold, that is, each matrix in St(d, D) is
a D by d matrix with columns being orthogonally normal. For any matrix M, recall that kM k2F =
[tr(M T M )]1/2 is the Frobenius norm of M .
Theorem 1 (Stiefel center-of-mass with respect to Frobenius norm) We consider the singular
l
value decomposition of the matrix	wjWj	=	O1∆O2,	where	O1	∈	O(D) and	O2	∈	O(d),
j=1
∆ =	diag(λ1, ..., λd)d×d and λ1 ≥ ... ≥ λd ≥ 0 are the singular values. Then the Stiefel
0(D-d)×d
center-of-mass with respect to the distance given by Frobenius norm d(W1, W2) = kW1 - W2 kF is
given by Wc = O1ΛO2 where Λ = diag(1,..., 1)d×d .
0(D-d)×d
l
Proof. For W ∈ St(d, D) we define f(W) = P wj kW - Wj k2F, and we are looking for a
j=1
minimizer Wc of f on St(d, D). Write
kW -Wjk2F = tr[(W -Wj)T(W-Wj)]
= tr(WTW) + tr(WjT Wj) - 2tr(WTWj)
= 2p- 2tr(WTWj).
It follows that
ll
f(W) = 2p P wj - 2 P wj tr(WTWj)
j=1	j=1
l
= 2p P wj - 2tr(WTB)
j=1
l
where B := P wj Wj . By singular value decomposition, there are D × D orthogonal matrix O1
j=1
and d × d orthogonal matrix O2 such that B = O1∆O2, where ∆ := diag(λ1, ..., λd)d×d	, and
0(D-d)×d
λι ≥ ∙∙∙ ≥ λd ≥ 0 are the singular values of B. It follows that tr(WTB) = tr(∆O2WTOι).
D
Observe that (O2 WTO1)T ∈ St(d, D). Write O2 WTO1 = (cij)d×D. Then P ci2j = 1 for each i.
j=1
Hence
dd
tr(WTB)=Xλicii ≤Xλi,
i=1	i=1
with equality holds when O2WTO1 = (Id, O) where O is the d × (D - d) matrix with all entries
being zero. This says that Wc = O1ΛO2 where Λ = diag(1, ..., 1)d×d is the maximizer of
0(D-d)×d
tr(WTB). The so obtained Wc serves as the minimizer of f(W) on St(d, D) and is thus the center-
of-mass.
B Proof of Theorem 2
Recall that Gr(d, D) = St(d, D)/O(d). Every point W on St(d, D) will correspond to an equiv-
alence class [W] = {WOd : Od ∈ O(d)} which is a point on Gr(d, D). To represent points on
Gr(d, D) using matrices, we notice that every point on Gr(d, D) corresponds to a unique choice of
matrix WWT where W ∈ St(d, D). In this way, we can define the projected Frobenius distance
between two classes [W1]and[W2] in Gr(d, D) as d2pF ([W1], [W2]) = 2-1/2kW1W1T-W2W2TkF.
10
Under review as a conference paper at ICLR 2021
Theorem 2 (Grassmann center-of-mass with respect to projected Frobenius norm) Set Ωj =
Plj=1 wj	wj .	We consider the singular value decomposition of the symmetric matrix
l
P ΩjWjWT = Q∆Qτ where Q ∈ O(D) and ∆ = diag(σ2,…,σD), σ2 ≥ ... ≥ σD ≥ 0.
j=1
Then the Grassmann center-of-mass with respect to the distance given by projected Frobenius
norm dpF ([W1], [W2]) = 2-1/2 kW1 W1τ - W2W2τkF is the equivalence class [Wc] determined
by Wc = QΛ, where Λ = diag(1, ..., 1)d×d.
0(D-d)×d
Proof. Set M = WWτ and Mj = WjWjτ, then we are looking for a minimizer Wc ∈ St(d, D) of
l
f(W) ≡ Pj=1 wj d2pF ([W], [Wj]) = 2-1 P wjkM - Mjk2F. For two D × D matrices M1 , M2
j=1
we define hM1 , M2i = tr(M1τM2). It is easy to verify that h, i is an inner product and kM k2F =
hM , M i. Thus we write
l
2f(W)	= PwjkM-Mjk2F
j=1
ll
P wj hM, Mi - 2 P wj hM, Mj i + P wj hMj , Mj i
j=1
l
j=1
l
j=1
l
EwjhM,M.hM, 2∑ΩjMji + EwjhMj,Mj)
j=1
l
j=1
l
j=1
l
W WjhM - Σ ΩjMj,M - E ΩjMji- E Wj
j=1
l
j=1
l
Ewj M - £ Ωj Mj
j=1
j=1
j=1
m
- P wj
j=1
j=1
l
P Ωj Mj
j=1
l
P Ωj Mj
j=1
l
2
l
+ P wjkMjk2F
F j=1
+ PwjkMjk2F .
F j=1
l
l
2
2
F
So the minimum is taken when
l
M - P Ωj Mj
j=1
2
is minimized. Since each Wj Wjτ is a symmetric
F
l
matrix, We consider the SVD decomposition P Ωj Wj WT = Q∆Qτ where Q ∈ O(D) and ∆ =
j=1
diag(σ12, ..., σD2 ), σ12 ≥ σ22 ≥ ... ≥ σD2 ≥ 0. Thus it suffices to set Wc = arg min kWWτ -
1	D 1	2	D	W ∈St(d,D)
Q∆Qτ k2F. Since W ∈ St(d, D), we have WWτ = PVPτ where P is an orthogonal matrix of
size D × D and V = diag(1, 1, .., 1, 0, 0, ..., 0) is an D × D matrix with rank(V ) = d. Moreover,
P can be chosen as P = (W Z) where Z is an D × (D - d) matrix. So
min	kWWτ -QVQτk2F = min kPVPτ - Q∆Qτ k2F .
W ∈St(d,D)	F	P ∈O(D)	F
Let the orthgonal matrix O = Q-1 P. Then we further have
min kPVPτ - Q∆Qτ k2F = min kQ(OV Oτ - ∆)Qτ k2F
P ∈O(n)	O∈O(n)
= min tr(Q(OV Oτ - ∆)2Qτ)
O∈O(n)
= min tr(OV Oτ - ∆)2 .
O∈O(n)
We then show that min tr(OV Oτ - ∆)2 is achieved at the orthogonal matrix
O*
×p	0
O(n-p)×(n-p)
11
Under review as a conference paper at ICLR 2021
for any Op×p ∈ O(p) and any O(n-p)×(n-p) ∈ O(n - p). Indeed, we have tr(OV OT - ∆)2 =
D
tr(OV OT) + tr(∆2) - 2tr(∆OV OT), so that we only have to maximize tr(∆OV OT) = P σi2cii
i=1
d
where OV OT = (cij)D×D. Let O = (vij)D×D, then it is easy to calculate that cii = P vi2j.
j=1
DD
Moreover since P vi2j = 1 for all i and P vi2j = 1 for all j , we know that 0 ≤ cii ≤ 1 and
j=1	i=1
D	Dd
P cii	=	d.	Thus from	σ12	≥ ... ≥	σD2	≥ 0 we see	tr(∆OV OT)	= P	σi2cii	≤ P	σi2	with
i=1	i=1	i=1
equality if and only if cii = 1 for i = 1, ..., d and cii = 0 for i = d + 1, ..., D. This gives that, for
dD
O = (vij)D×D, we have P vi2j = 1 when i = 1, ..., d and P vi2j = 1 when i = d + 1, ..., D.
j=1	j=d+1
Thus the minimum of min tr(OVOT - ∆)2 is achieved at O* = (Od×d C 0
O∈O(n)	0	O(D-d)×(D-d)
any Od×d ∈ O(d) and any O(D-d)×(D-d) ∈ O(D - d). We can pick Od×d = diag(1, ..., 1)
for
d×d,
and since P * = QO* = (Wc Z) where Z is an D X (D - d) matrix, We get the conclusion of the
Theorem about Wc .

12