Under review as a conference paper at ICLR 2021
Generalizing Graph Convolutional Networks
via Heat Kernel
Anonymous authors
Paper under double-blind review
Ab stract
Graph convolutional networks (GCNs) have emerged as a powerful framework
for mining and learning with graphs. A recent study shows that GCNs can be
simplified as a linear model by removing nonlinearities and weight matrices across
all consecutive layers, resulting the simple graph convolution (SGC) model. In
this paper, we aim to understand GCNs and generalize SGC as a linear model via
heat kernel (HKGCN), which acts as a low-pass filter on graphs and enables the
aggregation of information from extremely large receptive fields. We theoretically
show that HKGCN is in nature a continuous propagation model and GCNs without
nonlinearities (i.e., SGC) are the discrete versions of it. Its low-pass filter and con-
tinuity properties facilitate the fast and smooth convergence of feature propagation.
Experiments on million-scale networks show that the linear HKGCN model not
only achieves consistently better results than SGC but also can match or even beat
advanced GCN models, while maintaining SGC’s superiority in efficiency.
1	Introduction
Graph neural networks (GNNs) have emerged as a powerful framework for modeling structured and
relational data (Gori et al., 2005; Scarselli et al., 2008; Gilmer et al., 2017; Kipf & Welling, 2017). A
wide range of graph mining tasks and applications have benefited from its recent emergence, such as
node classification (KiPf & Welling, 2017; Velickovic et al., 2018), link inference (Zhang & Chen,
2018; Ying et al., 2018), and graph classification (Xu et al., 2019b).
The core Procedure of GNNs is the (discrete) feature ProPagation oPeration, which ProPagates
information between nodes layer by layer based on rules derived from the graPh structures. Take
the graPh convolutional network (GCN) (KiPf & Welling, 2017) for examPle, its ProPagation is
Performed through the normalized LaPlacian of the inPut graPh. Such a Procedure usually involves 1)
the non-linear feature transformation, commonly oPerated by the activation function such as ReLU,
and 2) the discrete ProPagation layer by layer. Over the course of its develoPment, various efforts have
been devoted to advancing the ProPagation based architecture, such as incorPorating self-attention in
GAT (Velickovic et al., 2018), mixing high-order neighborhoods in MixHoP (AbU-EI-Haija et al.,
2019), and leveraging graPhical models in GMNN (Qu et al., 2019).
Recently, WU et al. (WU et al., 2019) observe that the non-linear Part of GCNs’ featUre ProPagation
is actUally associated with excess comPlexity and redUndant oPerations. To that end, they simPlify
GCNs into a linear model SGC by removing all non-linearities between consecUtive GCN layers.
SUrPrisingly, SGC offers comParable or even better Performance to advanced GCN models, based on
which they argUe that instead of the non-linear featUre transformation, the rePeated graPh ProPagation
may contribUte the most to the exPressive Power of GCNs.
ThoUgh interesting resUlts generated, SGC still inherits the discrete natUre of GCNs’ ProPagation,
which can lead to strong oscillations dUring the ProcedUre. Take, for examPle, a simPle graPh of
two nodes v1 and v2 with one-dimension inPUt featUres x1 = 1 & x2 = 2 and one weighted edge
between them, the featUre UPdates of x1 and x2 dUring the GCN ProPagation is shown in FigUre 1
(a), from which we can clearly observe the oscillations of x1 and x2 steP by steP. This indicates that
thoUgh the featUres from mUlti-hoPs away may seem to be taken into consideration dUring the GCN
ProPagation, it is still far away to learn Patterns from them.
1
Under review as a conference paper at ICLR 2021
In this work, we aim to generalize GCNs into a continuous and linear propagation model, which
is referred to as HKGCN. We derive inspiration from Newton’s law of cooling by assuming graph
feature propagation follow a similar process. Straightforwardly, this leads us to leverage heat kernel
for feature propagation in HKGCN. Theoretically, we show that the propagation matrix of GCNs
is equivalent to the finite difference version of the heat kernel. In other words, using heat kernel as
the propagation matrix will lead to smooth feature convergence. In the same example above, we
show the heat kernel based propagation in HKGCN can prevent oscillations, as illustrated in Figure 1
(b). Finally, from the graph spectral perspective, heat kernel acts as a low-pass filter and the cutoff
frequency of heat kernel can be adjusted by changing the propagation time.
Empirically, we demonstrate the per-
formance of HKGCN for both trans-
ductive and inductive semi-supervised
node classification tasks. The exper-
iments are conducted on both tradi-
tional GNN datasets, such as Cora,
CiteSeer, Pubmed, and Reddit, and
latest graph benchmark data indexed
by Open Graph Benchmark (Hu et al.,
2020). The results suggest that the
simple and linear HKGCN model can
consistently outperform SGC on all
Figure 1: Feature propagation under GCN and HKGCN.
six datasets and match or even beat the performance of advanced graph neural networks on both tasks,
while at the same time maintaining the order-of-magnitude efficiency superiority inherited from SGC.
2	Related Work
Graph Neural Networks. Graph neural networks (GNNs) have emerged as a new paradigm for
graph mining and learning, as significant progresses have been made in recent years. Notably,
the spectral graph convolutional network (Bruna et al., 2013) is among the first to directly use
back propagation to learn the kernel filter, but this has the shortcoming of high time complexity.
Another work shows how to use Chebyshev polynomial approximation to fast compute the filter
kernel (Hammond et al., 2011). Attempts to further this direction leverage Chebyshev Expansion to
achieve the same linear computational complexity as classical CNNs (Defferrard et al., 2016). Later,
the graph convolutional network (GCN) (Kipf & Welling, 2017) simplifies the filter kernel to the
second-order of Chebyshev Expansion, inspiring various advancements in GNNs. GAT brings the
attention mechanisms into graph neural networks (VelickoVic et al., 2018). GMNN combines the
benefits of statistical relational learning and GNNs into a unified framework (Qu et al., 2019). To
enable fast and scalable GNN training, FastGCN interprets graph conVolutions as integral transforms
of features and thus uses Monte Carlo method to simulate the feature propagation step (Chen et al.,
2018). GraphSage treats the feature propagation as the aggregation from (sampled) neighborhoods
(Hamilton et al., 2017). LADIES (Zou et al., 2019) further introduces the layer-dependent importance
sampling technique for efficient training.
Recently, there are also research efforts deVoting on the theoretical or deep understanding of GCNs
(Xu et al., 2019b; Battaglia et al., 2018). For example, the feature propagation in GNNs can be also
explained as neural message passing (Gilmer et al., 2017). In addition, studies also find that the
performance of GNNs decreases with more and more layers, known as the oVer-smoothing issue (Li
et al., 2018; Zhao & Akoglu, 2020). To reduce GCNs’ complexity, SGC turns the GCN model into a
linear model by remoVing the non-linear actiVation operations between consecutiVe GCN layers (Wu
et al., 2019), producing promising results in terms of both efficacy and efficiency.
Heat Kernel. The properties of heat kernel for graphs are reViewed in detail by Chuang in (Chung &
Graham, 1997). Recently, heat kernel has been frequently used as the feature propagation modulator.
In (Kondor & Lafferty, 2002), the authors show that heat kernel can be regarded as the discretization
of the familiar Gaussian kernel of Euclidean space. Additionally, heat kernel is often used as the
window function for windowed graph Fourier transform (Shuman et al., 2016). In (Zhang et al.,
2019), the second-order heat kernel is used as the band-pass filter kernel to amplify local and global
structural information for network representation learning.
2
Under review as a conference paper at ICLR 2021
Concurrent work. Several recent works have developed similar idea. (Poli et al., 2020; Zhuang
et al., 2020) use the Neural ODE framework and parametrize the derivative function using a 2 or 3
layer GNN directly. (Xhonneux et al., 2020) improved ODE by developing a continuous message-
passing layer. All ODE models make feature converge to stable point by adding residual connection.
In contrast, our model outputs an intermediate state of feature, which is a balance between local and
global features. Some recent works (Xu et al., 2019a; Klicpera et al., 2019) propose to leverage heat
kernel to enhance low-frequency filters and enforce a smooth feature propagation. However, they do
not realize the relationship between the feature propagation of GCNs and heat kernel.
3	Generalizing (Simple) Graph Convolution via Heat Kernel
3.1	Problem and Background
We focus on the problem of semi-supervised node classification on graphs, which is the same as
GCN (Kipf & Welling, 2017). Without loss of generality, the input to this problem is an undirected
network G = (V, E), where V denotes the node set of n nodes {v1, ..., vn} and E represents the
edge set. The symmetric adjacency matrix of G is defined as A and its diagonal degree matrix as D
with Dii = Pj Aij . For each node vi ∈ V , it is associated with a feature vector xi ∈ X ∈ Rn×d
and a one-hot label vector yi ∈ Y ∈ {0, 1}n×C, where C is the number of classes. The problem
setting of semi-supervised graph learning is given the labels YL of a subset of nodes VL, to infer the
labels YU of the remaining nodes VU, where VU = V \VL.
Graph Convolutional Networks. Given the input graph G = (V, E) with A, D, X, and YL,
GCN can be understood as feature propagation over the graph structure. Specifically, it follows the
following propagation rule:
H(l+1) = σ(DD -1AD -1H⑴ W(I)),	(1)
where A = A + IN is the adjacency matrix with additional self-connections with IN as the identity
matrix, W(I) is a trainable weight matrix in the lth layer, σ(∙) is a nonlinear function such as ReLU,
and H(l) denotes the hidden node representation in the lth layer with the first layer H(0) = X.
The essence of GCN is that each GCN layer is equivalent to the first-order Chebyshev expansion
of spectral convolution (Kipf & Welling, 2017). It also assumes that the first-order coefficient a1 is
equal to the 0-th order coefficient a0 multiplied by -1, i.e., a1 = -a0. We will later prove that this
is just a discrete solution of heat equation.
Simple Graph Convolution. Since its inception, GCNs have drawn tremendous attention from
researchers (Chen et al., 2018; Velickovic et al., 2018; Qu et al., 2019). A recent study shows that
GCNs can be simplified as the Simple Graph Convolution (SGC) model by simply removing the
nonlinearities between GCN layers (Wu et al., 2019). Specifically, the SGC model is a linear model
and can be formalized by the following propagation rule:
Y = Softmax((D -1ADD- 2) XW)	(2)
Surprisingly, the linear SGC model yields comparable prediction accuracy to the sophisticated GCN
models in various downstream tasks, with significant advantages in efficiency and scalability due to
its simplicity.
Heat Equation and Heat Kernel. The heat equation, as a special case of the diffusion equation, is
used to describe how heat distributes and flows over time (Widder & Vernon, 1976).
Image a scenario of graph, in which each node has a temperature and heat energy could only transfer
along the edge between connected nodes, and the heat propagation on this graph follows Newton’s
law of cooling. So the heat propagation between node vi and node vj should be proportional to 1) the
edge weight and 2) the temperature difference between vi and vj . Let xi(t) denote the temperature of
vi at time t, the heat diffusion on graph G can be described by the following heat equation:
(t)
^d^ = -k X Aij (XF - Xjt)) = -k[Diiχit) - X Aijxjt)].
(3)
j
j
3
Under review as a conference paper at ICLR 2021
The equation under the matrix form is dχX(t) = -kLX(t), where L = D - A is the graph LaPlacian
matrix. By reparameterizing t and k into a single term t0 = kt, the equation can be rewritten as:
dX(t-) = -LX(V)	(4)
dt0	J
A heat kernel is the fundamental solution of the heat equation (Chung & Graham, 1997). The heat
kernel Ht is defined to be the n × n matrix:
Ht = e-Lt	(5)
Given the initial status X(0) = X, the solution to the heat equation in Eq. 4 can be written as
X(t) = HtX	(6)
Naturally, the heat kernel can be used as the feature propagation matrix in GCNs.
3.2	Connecting GCN and SGC to Heat Kernel
GCN's feature propagation follows D- 1 AD- 2, through which node features diffuse over graphs.
Note that the feature propagation in GCN is just one step each time/layer, hindering individual nodes
from learning global information. By analogy with the heat diffusion process on graphs, the heat
kernel solution can also be perfectly generalized to the feature propagation in the graph convolution.
1	1 KT 1	1	1	1
Instead of using L, we follow GCN to use the symmetric normalized Laplacian L = I - D 2 AD 2
to replace it. According to SGC (Wu et al., 2019), this convert also serves as a low-pass-type filter in
graph spectral. Then we have Eq. 4 as dχχ(t) = -LX(t) and the heat kernel in Eq. 5 as Ht = e-Lt.
Consider the finite difference of this heat equation, it could be written as:
X(t+∆t) - X(t)
∆t
(7)
Ifwe set ∆t = 1, we have X(t+1) = X㈤-LX⑴=D-2 AD-1X㈤.This is the same feature
propagation rule in Eq. 1. In other words, each layer of GCN’s feature propagation on graphs is equal
to the finite difference of the heat kernel. If we consider the multilayer GCN without the activation
1	1K
function, it could be written as SGC (Wu et al., 2019): Y = Softmax((D-2 AD-2) XW), where
Y is the classification result and W is the merged weight matrix. Using the finite difference of heat
equation above, it can be rewritten as:
Y = softmax(X(K)W),
(8)
which is still a multistep finite difference approximation of the heat kernel.
Reduce ∆t. Assuming t can be divided by ∆t, the number of iterations is n = ∆, making Eq. 7
become X(t) = (I — ∆tL)ni X. By fixing t = 1, we have
X(I) = (I- n1iL' X
(I-
ni L + m (ni - 1) L2
ni	n22	2!
~
(-1)nini! Lni
+— +-------n-------F)X
ni i	ni !
(9)
Therefore, ni iterations of t = 1 approximate to Taylor expansion of the heat kernel at order ni . So
GCN could also be seen as the first-order Taylor expansion of the heat kernel.
3.3	Heat Kernel as Feature Propagation
We have shown that the feature propagation in GCN is a multistep finite difference approximation of
the heat kernel. Next, we briefly illustrate the advantage of differentiation.
A case study. We illustrate how the features are updated during the GCN and heat kernel propaga-
tions. Let us consider a graph of two nodes v1 and v2 with one-dimension input features x1 = 1 and
x2 = 2 and one weighted edge A12 = A21 = 5 between them. Recall that with ∆t = 1 GCN is
equivalent to the finite difference of the heat kernel. Thus, we set ∆t = 1 for the heat kernel as well.
4
Under review as a conference paper at ICLR 2021
The updates of x1 and x2 as the propagation step t increases are shown in Figure 1. We can observe
that heat kernel shows much smoother and faster convergence than GCN.
1 KT	1	1
In GCN, the discrete propagation D 2 AD 2 layer by layer causes node features to keep oscillating
around the convergence point. The reason lies in GCN’s requirement for ∆t = 1, which is too large
to have a smooth convergence. Straightforwardly, the oscillating nature of GCN’s feature propagation
makes it sensitive to hyper-parameters and generate weak performance on large graphs. Theoretical
analysis is given in Section 3.5.
3.4	Generalizing Graph Convolution
We have shown that the feature propagation in GCN is merely the solution of the finite difference
version of the heat equation when ∆t = 1. As a result, using heat kernel as feature propagation can
lead to smooth convergence. Since the range of t is in real number field, the propagation time t in
heat kernel can be seen as a generalized parameter of the number of layers in GCN (Kipf & Welling,
2017) and SGC (Wu et al., 2019). The advantage of heat kernel also includes that t can change
smoothly compared to the discrete parameters.
In light of this, we propose to generalize graph convolution networks by using heat kernel and present
the HKGCN model. Specifically, we simply use the one layer linear model:
Y = Softmax(X ⑴ W) = Softmax(e(-Lt)XW),	(10)
where W is the n × C feature transformation weight and t can be a learnable scalar or a preset
hyper-parameter.
∙f.+
Using a preset t converts HKGCN to 1) a pre-processing jtep X = e( Lt)X without parameters and
2) a linear logistic regression classifier Y = Softmax(XW). This makes the training speed much
faster than GCN. The algorithm of presetting t is in Appendix (Algorithm 1).
F+
To avoid eigendecomposition, we use Chebyshev expansion to calculate e(-Lt) (Hammond et al.,
2011; Zhang et al., 2019). The first kind Chebyshev polynomials are defined as Ti+1(x) = 2xTi(x) -
Ti-1(x) with T0(x) = 1 and T1(x) = x. The requirement of the Chebyshev polynomial is that x
should be in the range of [-1, 1], however, the eigenvalues of L satisfy 0 = λ0 ≤ ... ≤ λn-1 ≤ 2.
To make the eigenvalues of L fall in the range of [-1, 1], we convert L to L = L/2. In addition, we
reparameterize t to t = 2t so that the heat kernel keeps in the original form. In doing so, we have
k-1
e(-Lt) = e(T，)≈ X CiG)Ti(L)	(11)
i=0
And the coefficient ci of the Chebyshev expansion can be obtained by:
Ci伍)
β「
∏ J-
T√X=e= dx = β(-1)iBi (t)
1 - x2
(12)
1	Cri	,	l-∖	. 1	∙	∏	rʌ	1 I > / 7"∖ ♦ .1	1 ∙ ∕' 1 1 ʌ	1 ∕'	. ∙	∕' . 1 ∕'
where β = 1 when i = 0, otherwise β = 2, and Bi (t) is the modified Bessel function of the first
kind (Andrews & of Photo-optical Instrumentation Engineers, 1998). By combining Eqs. 11 and 12
together, we can have e(-LtF) approximated as:
e(-L t)
k-1
i=1
(13)
3.5	Spectral Analysis
From the graph spectral perspective, heat kernel acts as a low-pass filter (Xu et al., 2019a). In
addition, as the propagation time t increases, the cutoff frequency decreases, smoothing the feature
propagation.
Graph Spectral Review. We define Λ = diag(λ1, ..., λn) as the diagonal matrix of eigenvalues of
L and U = (u1, ..., un) as the corresponding eigenvectors, that is, L = UΛUT.
5
Under review as a conference paper at ICLR 2021
Figure 2: Effects of different kernels.
The graph Fourier transform defines that X = UTX is the frequency spectrum of x, with the
inverse operation X = UX. And the graph convolution between the kernel filter g(∙) and X is
g * X = Ug(A)UTX with g(Λ) = diag(g(λι),…，g(λn)).
For a polynomial g(L), we have g(L) = Ug(A)UT. This can be verified by setting g(λi) =
Pj=0 ajλij, that is, Ug(A)UT = U Pj=0 ajAjUT = Pj=0 aj UAj UT = Pj=0 ajLj = g(L).
Heat Kernel. As g(L) = Ug(A)UT, the heat kernel Ht = e-Lt = Pi=0 1 tiLi could also be
seen as a polynomial of L. Thus its kernel filter is g(λi) = e-λit. Note that the eigenvalues of L
is in the range of [0, 2]. For ∀i, j, if λi < λj, we have g(λi) = e(λ2-λ1)t > 1. Thus g(λi) > g(λj).
Heat kernel acts as a low-pass filter. As t increases, the ratio e(λ2-λ1)t also increases, discounting
more and more high frequencies. That said, t acts as a modulator between the low frequency and
high frequency.
Different Kernels. We summary the kernel filters of
GCN, SGC, and HKGCN in Table 1. In Figure 2, we
use the eigenvalues of L to illustrate the effects of dif-
ferent kernel filters. We can observe that on Cora the
absolute values of the filtered eigenvalues by GCN
and SGC kernel filters do not decrease monotonically.
However, the filtered eigenvalues of heat kernels in
HKGCN do monotonically decrease. This is because
for g(λ) = (1 -λ)k, it monotonically increases when
λ ∈ [1, 2]. In other words, the kernel filter of GCN
acts as a band-stop filter, attenuating eigenvalues near
Table 1: Kernel Filters.
Model	I Kernel Filter			Propagation
GCN		g(4	=1-λ	~ _ I ≈ ~ _ I D 2 AD 2
SGC		g(4 =	(1 - λ)2	(D - 2 A D -1 )2
HKGCN t =	1	g(λ)	= e-λ	e-L
HKGCN t =	2	g(λ)	= e-2λ	^- e-2L
1.
mi TrrI	C IrTr ♦ ι Tn	C /	r-ɪ-ɪ-i ∙	ι	c∙ τ ι	∙ . ι	.
The Influence of High Frequency Spectrum. The eigenvalues of L and associated eigenvectors
satisfy the following relation (Shuman et al., 2016) (Cf Appendix for Proof):
1	12
λk =	A ij[ 广—Uk(Z)-------「—Ukj)]	(14)
(Vi,Vj )∈E	"Dii	，D jj
This means that similar to classical Fourier transform, those eigenvectors associated with high λ
n
oscillate more rapidly than those associated with low λ. And we know that L = i=1 λiui uiT ,
where ui uiT is the projection matrix project to ui . Since the eigenvector ui oscillates more rapidly as
the increase of i, the projection matrix ui uiT will also globally oscillate more rapidly as i increases.
n
Because the filter kernel follows g(L) = i=1 g(λi)uiuiT, the larger g(λi) is, the greater influence
on g(L) will UiUiT have. We show above that as i increase, the oscillation of UiUT also increase.
Since Y = g(L)X, an oscillating g(L) will cause oscillating output Y. Therefore, we want the
influence of higher i’s UiUiT as small as possible, i.e. large i’s g(λi) as small as possible. This
explains why we need a low-pass filter.
3.6	Complexity Analysis
For the preprocessing step, the time complexity for matrix multiplication between the sparse Laplacian
matrix and the feature vector in Chebyshev expansion is O(d|E|), with |E| is the number of edges
6
Under review as a conference paper at ICLR 2021
and d is the dimension size of input feature. Thus, the time complexity of the preprocessing step is
O(kd|E|), with k is the Chebyshev expansion step. For prediction, the time complexity of logistic
regression is O(|VU |dC) with |VU | is the number of unlabeled nodes, and C is the number of label
categories.
The space complexity is O(|E| + nd), where n is the number of nodes.
4	Experiments
4.1	Experimental setup
We follow the standard GNN experimental settings to evaluate HKGCN on benchmark datasets for
both transductive and inductive tasks. The reproducibility information is detailed in Appendix.
Datasets. For transductive learning, we use Cora, Citeseer and Pubmed (Kipf & Welling, 2017;
VeliCkovic et al., 2018). For inductive tasks, We use Reddit (Hamilton et al., 2017), ogbn-arxiv (HU
et al., 2020), and a new arXiv dataset collected by ourselves, which contains over one million nodes
and 10 million edges. In all inductive tasks, We train models on subgraphs Which only contain training
nodes and test models on original graphs. We adopt exactly the same data splitting as existing Work
for Cora, Citeseer, Pubmed, and Reddit.
The ogbn-arxiv dataset is accessed from
OGB	(https://ogb.stanford.	Table 2: Dataset Statistics
edu/docs/nodeprop/). It is the
citation network between ComPUter science	I Cora Citeseer PubmedlReddit arXiv ogbn-arxiv
arXiv papers and the task is to infer 	!	!	
arXiv papers, categories, such as cs.LG,	#NodeS	2,708	3,327 19,717 233K 1∙4M	169K
cs.SI, and cs.DB. Each paper is given a	盯 #Edges d	嘴	4,732	44,338	11.6M 16∙4M	1.2M YCC「	.	# 卜Iylr	# #Tralnlng-NOdeS 140	120	60 152K 1,1M	90K 128-dimension word embedding based	#Validation-Nodes 500	500	500	24K 121K	30K
feature vector. ThegraPhiSSPIitbaSedOn	#TeSt-NOdeS 1000	1000	1000	55κ 181K	49κ
time, that is, papers published until 2017	#CIaSSeS	7	6	3	41	175	40
as training, papers published in 2018 as
validation, and papers published in 2019 as
test. Inspired by ogbn-arxiv, We also constructed a full arXiv paper citation graph from the public
MAG (https://docs.microsoft.com/en-us/academic-services/graph/). We
folloW the same feature extraction and data splitting procedures as ogbn-arxiv and generate the arXiv
dataset, Which Will be made publicly available upon publication.
The statistics and splitting information of the six datasets are listed in Table 2.
Baselines. For the transductive tasks on Cora, Citeseer, and Pubmed, We use the same baselines
used in SGC (WU et al., 2019), including GCN (KiPf & Welling, 2017), GAT (Velickovic et al., 2018),
FastGCN (Chen et al., 2018), LanczosNet, AdaLanczosNet (Liao et al., 2019), DGI (Velickovic et al.,
2019), GIN (Xu et al., 2019b), and SGC (Wu et al., 2019).
For the inductive tasks, We use suPervised GraPhSage (mean) Without samPling (Hamilton et al.,
2017), GCN (KiPf & Welling, 2017), ClusterGCN (Chiang et al., 2019), GraPhSaint (Zeng et al.,
2019), MLP, and SGC (Wu et al., 2019) as baselines.
For the ProPosed HKGCN, the ProPagation time t is Preset based on the Performance on the validation
set. On Reddit, We folloW SGC (Wu et al., 2019) to train HKGCN and SGC With L-BFGS Without
regularization as oPtimizer (Liu & Nocedal, 1989), due to its raPid convergence and good Performance.
HoWever, this advantage brought by L-BFGS can not be observed in the other datasets, for Which We
use adam oPtimizer (Kingma & Ba, 2014), same as the other baselines.
4.2	Results
We rePort the Performance of HKGCN and baselines in terms of both effectiveness and efficiency.
Transductive. Table 3 rePorts the results for the transductive node classification tasks on Cora,
Citeseer, and Pubmed, as Well as the relative running time on Pubmed. As the reference Point, it
takes 0.81s for training HKGCN on Pubmed. Per community convention (KiPf & Welling, 2017;
7
Under review as a conference paper at ICLR 2021
VeliCkovic et al., 2018), We take the results of baselines from existing publications (Wu et al., 2019),
and report the results of our HKGCN model by averaging over 100 runs. Finally, the efficiency is
measured by the training time on a NVIDIA TITAN Xp GPU.
We can observe that 1) HKGCN outper-
forms SGC in all three datasets With similar
training time consumed, and 2) HKGCN
can achieve the best prediction results on
Citeseer and Pubmed among all methods
and comparable results on Cora, While us-
ing 2-3 orders of magnitude less time than
all baselines except FastGCN and SGC.
In addition, it is easy to notice that compar-
ing to complex GNNs, the performance of
HKGCN (and SGC) is quite stable across
100 runs, as it benefits from the the simple
and deterministic propagation.
Inductive. Table 4 summarizes the perfor-
mance of inductive tasks on large datasets,
including both test accuracy and relative
running time With HKGCN’s as the ref-
erence points. Most reported results are
averaged over 10 runs, except the super-
vised GraphSage (mean) and SGC meth-
ods’ accuracies on Reddit, Which are di-
rectly taken from the SGC Work (Wu et al.,
2019).
The results suggest that 1) the performance
HKGCN is consistently better than SGC
and comparable to advanced GNN models,
Table 3: Transductive results in terms of test accuracy.
		Cora	Citeseer	Pubmed	Pubmed (T)
GCN	81.4 ± 0.4	70.9 ± 0.5	79.0 ± 0.4	25x
GAT	83.3 ± 0.7	72.6 ± 0.6	78.5 ± 0.3	377x
FastGCN	79.8 ± 0.3	68.8 ± 0.6	77.4 ± 0.3	5x
GIN	77.6 ± 1.1	66.1 ± 0.9	77.0 ± 1.2	81x
LanczosNet	80.2 ± 3.0	67.3 ± 0.5	78.3 ± 0.6	826x
AdaLanczosNet	81.9 ± 1.9	70.6 ± 0.8	77.8 ± 0.7	689x
DGI	82.5 ± 0.7	71.6 ± 0.7	78.4 ± 0.7	236x
SGC	81.0 ± 0.0	71.9 ± 0.1	78.9 ± 0.0	0.9x
HKGCN	81.5 ± 0.0	72.8 ± 0.0	79.9 ± 0.0	(0.81s) 1x
Table 4: Inductive results in terms of test accuracy (left)
and running time (right), averaged over 10 runs.
	Reddit	arXiv	ogbn-arxiv ∣ Reddit		arXiv	ogbn-arxiv
	 GraphSage	95.0	61.0	69.4	68x	3.6x	3.8x
GCN	94.3	59.8	70.7	64x	2.9x	2.9x
ClusterGCN	92.6	60.7	55.9	108x	19.1x	43x
GraphSaint	92.3	59.2	56.1	138x	2.4x	4.0x
MLP	70.0	44.1	55.5	9.3x	1.75x	5.2x
SGC	94.9	60.1	69.6	0.5x	0.93x	0.95x
HKGCN	95.5	60.4	70.0	1x	1x	1x
such as supervised GraphSage, ClusterGCN, and GraphSaint. Additionally, We notice that HKGCN,
GraphSage, and GCN yield the best results on Reddit, arXiv, and ogbn-arxiv, respectively, indicating
the lack of universally-best GNN models.
Efficiency Wise, HKGCN costs 2.2s, 48.5s, and 5.2s on three datasets, respectively, Which are similar
to SGC, both of Which are clearly more efficient than other GNN models as Well as MLP.
A 1	∙ C 7 T ɪ ~r~rr z-l∙‰ τ ^ι t	∙	t
Analysis of t. In HKGCN, t determines hoW
long the feature propagation lasts for. A larger
t can amplify global features and a loWer one
can emphasize local information. From Figure
3, We can observe that t has a similar impact
on different datasets, and as the propagation
time increases, the model at first benefits from
information from its local neighbors. HoW-
ever, as the feature propagation continues (e.g.,
t > 9 or 12), the performance of the model
starts to decrease, likely suffering from the
over-smoothing issue (Li et al., 2018).
Cora
CiteSeer
pubmed
reddit
arXiv
ogbn-arxiv
ι-ι∙	CTrAe	^ι	∙ c∙	r-∖ . Ce
Figure 3: Performances With t varying from 0 to 30.
5	Conclusion and discussion
In this Work, We propose to generalize graph convolutional netWorks (GCNs) and simple graph
convolution (SGC) via heat kernel, based on Which, We present the HKGCN model. The core
idea of HKGCN is to use heat kernel as the feature propagation matrix, rather than the discrete
and non-linear feature propagation procedure in GCNs. We theoretically shoW that the feature
propagation of GCNs is equivalent to the finite difference version of heat equation, Which leads it
to overshoot the convergence point and causes oscillated propagation. Furthermore, We shoW that
8
Under review as a conference paper at ICLR 2021
heat kernel in HKGCN acts as a low-pass filter. On the contrary, the filter kernel of GCNs fails to
attenuate high-frequency signals, which is also a factor leading to the slow convergence and feature
oscillation. While in heat kernel, the cutoff frequency decreases as the increase of the propagation
time t. Consequently, the HKGCN model could avoid these oscillation issue and propagate features
smoothly. Empirical experiments on six datasets suggest that the proposed HKGCN model generates
promising results. Effectiveness wise, the linear HKGCN model consistently beats SGC and achieves
better or comparable performance than advanced GCN baselines. Efficiency wise, inherited from
SGC, HKGCN offers order-of-magnitude faster training than GCNs.
Notwithstanding the interesting results of the present work, there is still much room left for fu-
ture work. One interesting direction is to learn the propagation time t from data in HKGCN for
automatically balancing information between local and global features.
References
Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Hrayr Harutyunyan, Nazanin Alipourfard, Kristina
Lerman, Greg Ver Steeg, and Aram Galstyan. Mixhop: Higher-order graph convolution archi-
tectures via sparsified neighborhood mixing. In International Conference on Machine Learning
(ICML), 2019.
L.C. Andrews and Society of Photo-optical Instrumentation Engineers. Special Functions of Mathe-
matics for Engineers. Oxford science publications. SPIE Optical Engineering Press, 1998. ISBN
9780819426161. URL https://books.google.com/books?id=2CAqsF-RebgC.
Peter Battaglia, Jessica Blake Chandler Hamrick, Victor Bapst, Alvaro Sanchez, Vinicius Zambaldi,
Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar
Gulcehre, Francis Song, Andy Ballard, Justin Gilmer, George E. Dahl, Ashish Vaswani, Kelsey
Allen, Charles Nash, Victoria Jayne Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet
Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu. Relational inductive biases,
deep learning, and graph networks. arXiv, 2018. URL https://arxiv.org/pdf/1806.
01261.pdf.
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally
connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.
Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: fast learning with graph convolutional networks via
importance sampling. arXiv preprint arXiv:1801.10247, 2018.
Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-gcn: An
efficient algorithm for training deep and large graph convolutional networks. In Proceedings of
the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.
257-266, 2019.
Fan RK Chung and Fan Chung Graham. Spectral graph theory. Number 92. American Mathematical
Soc., 1997.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral filtering. In Advances in neural information processing systems,
pp. 3844-3852, 2016.
Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference on
Machine Learning - Volume 70, ICML’17, pp. 1263-1272. JMLR.org, 2017.
Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains.
In IJCNN’05, 2005.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In
Advances in neural information processing systems, pp. 1024-1034, 2017.
David K Hammond, Pierre Vandergheynst, and Remi GribonvaL Wavelets on graphs via spectral
graph theory. Applied and Computational Harmonic Analysis, 30(2):129-150, 2011.
9
Under review as a conference paper at ICLR 2021
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,
and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv
preprint arXiv:2005.00687, 2020.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
In International Conference on Learning Representations (ICLR), 2017.
Johannes Klicpera, Stefan WeiBenberger, and StePhan Gunnemann. Diffusion improves graph
learning. In Advances in Neural Information Processing Systems, pp. 13354-13366, 2019.
Risi Imre Kondor and John Lafferty. Diffusion kernels on graphs and other discrete structures. In
Proceedings of the 19th international conference on machine learning, volume 2002, pp. 315-22,
2002.
Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for
semi-supervised learning. In AAAI’18, 2018.
Renjie Liao, Zhizhen Zhao, Raquel Urtasun, and Richard S Zemel. Lanczosnet: Multi-scale deep
graph convolutional networks. arXiv preprint arXiv:1901.01484, 2019.
Dong C Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization.
Mathematical programming, 45(1-3):503-528, 1989.
Michael Poli, Stefano Massaroli, Junyoung Park, Atsushi Yamashita, Hajime Asama, and Jinkyoo
Park. Graph neural ordinary differential equations, 2020.
Meng Qu, Yoshua Bengio, and Jian Tang. Gmnn: Graph markov neural networks. ICML’19, 2019.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The
graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2008.
David I Shuman, Benjamin Ricaud, and Pierre Vandergheynst. Vertex-frequency analysis on graphs.
Applied and Computational Harmonic Analysis, 40(2):260-291, 2016.
Petar VeliCkovic, GUillem CUcurUlL Arantxa Casanova, Adriana Romero, Pietro Lid, and YoshUa
Bengio. Graph Attention Networks. International Conference on Learning Representations, 2018.
URL https://openreview.net/forum?id=rJXMpikCZ. accepted as poster.
Petar VeliCkoviC, William Fedus, William L. Hamilton, Pietro Lid, Yoshua Bengio, and R Devon
Hjelm. Deep Graph Infomax. In International Conference on Learning Representations, 2019.
URL https://openreview.net/forum?id=rklz9iAcKQ.
Widder and David Vernon. The heat equation, volUme 67. ACademiC Press, 1976.
Felix WU, AmaUri SoUza, Tianyi Zhang, Christopher Fifty, Tao YU, and Kilian Weinberger. Sim-
plifying graph ConvolUtional networks. In Proceedings of the 36th International Conference on
Machine Learning, pp. 6861-6871. PMLR, 2019.
LoUis-PasCal A. C. XhonneUx, Meng QU, and Jian Tang. ContinUoUs graph neUral networks, 2020.
Bingbing XU, HUawei Shen, Qi Cao, Keting Cen, and XUeqi Cheng. Graph ConvolUtional networks
Using heat kernel for semi-sUpervised learning. In Proceedings of the 28th International Joint
Conference on Artificial Intelligence, pp. 1928-1934. AAAI Press, 2019a.
KeyUlU XU, WeihUa HU, JUre LeskoveC, and Stefanie Jegelka. How powerfUl are graph neUral
networks? In International Conference on Learning Representations, 2019b. URL https:
//openreview.net/forum?id=ryGs6iA5Km.
Rex Ying, RUining He, Kaifeng Chen, Pong EksombatChai, William L Hamilton, and JUre LeskoveC.
Graph ConvolUtional neUral networks for web-sCale reCommender systems. In KDD’18, 2018.
10
Under review as a conference paper at ICLR 2021
Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. Graph-
saint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931, 2019.
Jie Zhang, Yuxiao Dong, Yan Wang, Jie Tang, and Ming Ding. Prone: fast and scalable network
representation learning. In Proc. 28th Int Joint Conf. Artif Intell., IJCAI, pp. 4278-4284, 2019.
Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. In NeurIPS’18,
2018.
Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. ICLR’20, 2020.
Juntang Zhuang, Nicha Dvornek, Xiaoxiao Li, and James S. Duncan. Ordinary differential equations
on graph networks, 2020. URL https://openreview.net/forum?id=SJg9z6VFDr.
Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. Layer-dependent
importance sampling for training deep and large graph convolutional networks. In NeurIPS’19,
2019.
11
Under review as a conference paper at ICLR 2021
A Reproducibility Information
A. 1 Hyperparameters
We introduce the hyperparameters used in the experiments, including weight decay, #epochs, learning
rate, t, and optimizer, which are also summarized in Table 5. The Chebyshev expansion step k is set
to 20.
Citation Networks. We train HKGCN for 100 epochs using the Adam optimizer (Kingma & Ba,
2014) with 0.2 as the learning rate, which are the same settings used in SGC (Wu et al., 2019). First,
We fix the weight decay to 5 X 10-6 and search t from {0, 3, 6, 9,12,15,18,21,24, 27, 30} based
on the performance on the validation set. Then we fix the best performance t and search the weight
decay from {10-6, 10-5, 10-4} based on the performance on the validation set.
Reddit dataset. We follow SGC (Wu et al., 2019) to train HKGCN and SGC with L-BFGS without
regularization as the optimizer (Liu & Nocedal, 1989), due to its rapid convergence (2 epochs) and
good performance. However, this advantage brought by L-BFGS can not be observed in the other
datasets, for which we use the adam optimizer (Kingma & Ba, 2014), same as the other baselines.
We search t from all non-negative integers equal or lower than 15, based on the performance on the
validation set.
arXiv and ogbn-arxiv datasets. We use the Adam optimizer with no weight decay to train HKGCN
and SGC, same as other baselines. Using the same treatments in citation networks, we fix the learning
rate to 0.2. We first fix 500 epochs and search t from all non-negative integers equal or lower than 15
by using the validation set. On the ogbn-arxiv dataset, the model does not converge after 500 epochs
and thus we extend it to 1000.
Situation for Large t. If t is greater than 10, the floating point precision error would be-
come an issue in calculating the Bessel function. Therefore, in practice, we convert e(-LF) into
—U	— U	— T
e(-L 3 )e(-L 3 )e(-L 3).
Situation for Very Large Graphs. Though the Chebyshev expansion is very close to the minimax
polynomial on the range of [-1, 1] and thus makes it converge very fast, the recurrence relation in
Eq. 13 is second-order. In other words, the calculation of Ti+1 (x) needs to store Ti(x), Ti-1 (x)
and L, making it require more GPU memory than the first-order recurrence relation expansion, such
as Taylor expansion. Therefore, instead of using the Chebyshev expansion, we leverage Eq. 7 as
Xlt+at) = (I 一 ∆tL)X(t) for the calculation. In this way, we only need to store Xt and L, which
requires much less memory space.
Training Time . We tested all experiments on NVIDIA TITAN Xp GPU with 12 GB memory. The
training time of HKGCN is the sum of 1) the feature pre-processing time and 2) logistic regression
training time. So we can see the largest training time difference between SGC and HKGCN occurs
on the Reddit dataset. This is because it only takes two epochs to converge on the Reddit dataset by
using L-BFGS, making the logistic regression step very fast. Thus the pre-processing step dominates
the training time for HKGCN and SGC on Reddit.
Table 5: Hyperparameters
	Cora	Citeseer	Pubmed	Reddit	arXiv	ogbn-arxiv
	 〜 t	12	9	15	6	6	8
weight decay	10-5	10-4	10-5	n/a	0	0
#epochs	100	100	100	2	500	1000
learning rate	0.2	0.2	0.2	1	0.2	0.2
optimizer	adam	adam	adam	l-bfgs	adam	adam
A.2 Baseline Methods
Per community convention (KiPf & Welling, 2017; VeIiCkOViC et al., 2018), the results of baselines in
the three citation networks (Cora, Citeseer, and Pubmed) are taken from existing publications (Wu
12
Under review as a conference paper at ICLR 2021
et al., 2019). The hyperparameters settings of inductive tasks on three large datasets (Reddit, arXiv,
and ogbn-arxiv) are introduced as follow.
SGC. To make a fair comparison, we select K in SGC based on the performance on the validation
set, which has similar meaning as t in HKGCN.
GCN. The learning rate is set to 0.01 on all datasets. The weight decay is 0. The number of layers is
3 on Reddit and ogbn-arxiv, and 2 on arXiv because of the GPU memory limit. The hidden layer size
is 256 on Reddit and ogbn-arxiv, and 128 on arXiv also due to memory limit. The number of epochs
is 500.
GraphSage. Results on the Reddit dataset are directly taken from the SGC paper (Wu et al., 2019).
The other parameter settings are the same as GCN above. We use the mean-based aggregator as it
provides the best performance.
MLP. The number of layers is 3 and the hidden layer size is 256, and Batch Norm is not used.
ClusterGCN. On the Reddit dataset, the number of partitions is 1500, the batch size is 20, the
hidden layer size is 128, which are the same parameter settings as the original SGC paper (Chiang
et al., 2019). On the arXiv dataset, the number of partitions is 15000, the batch size is 32, the hidden
layer size is 256. The number of epochs of these two datasets is 50. On the ogbn-arxiv dataset, the
number of partitions is 15 (as a large number would cause an unknown bug in the PyTorch-Geometric
library), the batch size is 32, the hidden layer size is 256. We set the epoch number to 200 on the
ogbn-arxiv dataset because it does not converge after 50 epochs.
GraphSaint. On the Reddit dataset, the number of layers is 4, the hidden layer size is 128, the
dropout rate is 0.2, the same parameter settings as used in SGC (Zeng et al., 2019). On the arXiv
dataset, the number of layers is 3, the hidden layer size is 256, the dropout rate is 0.5. On the
ogbn-arxiv dataset, we use the same parameter setting as the Reddit dataset, because both graphs are
on the same scale.
B ALGORITHM OF PRESETTING t
Algorithm 1 Preset t in HKGCN
Input: input features X; feature propagation duration time t; Chebyshev expansion step k; scaling
augmented normalized Laplacian L = L/2; Bi (t) is the modified Bessel function.
Output: pre-processed features X(t) ;
To(x) J X；
X㈤ J Bo(t)To(x);
Tι(x) J LX;
X㈤ j X㈤ — 2B1(f)T1(χ);
for i = 2...k do
Ti(X) J 2LTi-I(X) - Ti-2(x);
if i is odd then
X㈤ J X㈤-2Bi(t)Ti(x);
else
X㈤ J X㈤ +2Bie)Ti(x);
end if
end for
13
Under review as a conference paper at ICLR 2021
C PROOF OF EQ. 14
T
λk = ukT Luk
UT(I- ID-1AD-1 )uk
uk(i)2
i∈{1,...,n}
-X	∕2Aj	Uk(i)ukCj)
(Vi,Vj )∈E ʌ/DiiDjj
X	A j [ PU Uk (i)-
(vi,vj)∈E	Dii
12
/----uk(j)]
14