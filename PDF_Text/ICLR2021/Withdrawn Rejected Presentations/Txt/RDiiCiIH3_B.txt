Under review as a conference paper at ICLR 2021
A framework for learned CountSketch
Anonymous authors
Paper under double-blind review
Ab stract
Sketching is a compression technique that can be applied to many problems to
solve them quickly and approximately. The matrices used to project data to
smaller dimensions are called “sketches”. In this work, we consider the problem
of optimizing sketches to obtain low approximation error over a data distribution.
We introduce a general framework for “learning” and applying CountSketch, a
type of sparse sketch. The sketch optimization procedure has two stages: one
for optimizing the placements of the sketch’s non-zero entries and another for
optimizing their values. Next, we provide a way to apply learned sketches that has
worst-case guarantees for approximation error.
We instantiate this framework with three sketching applications: least-squares re-
gression, low-rank approximation (LRA), and k-means clustering. Our experi-
ments demonstrate that our approach substantially decreases approximation error
compared to classical and naively learned sketches.
Finally, we investigate the theoretical aspects of our approach. For regression
and LRA, we show that our method obtains state-of-the art accuracy for fixed
time complexity. For LRA, we prove that it is strictly better to include the first
optimization stage for two standard input distributions. For k-means, we derive a
more straightforward means of retaining approximation guarantees.
1	Introduction
In recent years, we have seen the influence of machine learning extend far beyond the field of
artificial intelligence. The underlying paradigm, which assumes that a given algorithm has an input
distribution for which algorithm parameters can be optimized, has even been applied to classical
algorithms. Examples of classical problems that have benefitted from ML include cache eviction
strategies, online algorithms for job scheduling, frequency estimation of data stream elements, and
indexing strategies for data structures (Lykouris & Vassilvitskii, 2018; Purohit et al., 2018; Hsu
et al., 2019; Kraska et al., 2018). This input distribution assumption is often realistic. For example,
many real-world applications use data streaming to track things like product purchasing statistics
in real time. Consecutively streamed datapoints are usually tightly correlated and closely fit certain
distributions.
We are interested in how this distributional paradigm can be applied to sketching, a data compres-
sion technique. With the dramatic increase in the dimensions of data collected in the past decade,
compression methods are more important than ever. Thus, it is of practical interest to improve the
accuracy and efficiency of sketching algorithms.
We study a sketching scheme in which the input matrix is compressed by multiplying it with a
“sketch” matrix with a small dimension. This smaller, sketched input is then used to compute an
approximate solution. Typically, the sketch matrix and the approximation algorithm are designed
to satisfy worst-case bounds on approximation error for arbitrary inputs. With the ML perspective
in mind, we examine if it is possible to construct sketches which also have low error in expectation
over an input distribution. Essentially, we aim for the best of both worlds: good performance in
practice with theoretical worst-case guarantees.
Further, we are interested in methods that work for multiple sketching applications. Typically,
sketching is very application-specific. The sketch construction and approximation algorithm are
tailored to individual applications, like robust regression or clustering (Sarlos, 2006; Clarkson &
Woodruff, 2009; 2014; 2017; Cohen et al., 2015; Makarychev et al., 2019). Instead, we consider
1
Under review as a conference paper at ICLR 2021
three applications at once (regression, LRA, k-means) and propose generalizable methods, as well
as extending previous application-specific work.
Our results. At a high level, our work’s aim is to make sketch learning more effective, general, and
ultimately, practical. We propose a framework for constructing and using learned CountSketch. We
chose CountSketch because it is a sparse, input-independent sketch (Charikar et al., 2002). Specif-
ically, it has one non-zero entry (±1) per column and does not need to be constructed anew for
each input matrix it is applied to. These qualities enable CountSketch to be applied quickly, since
sparse matrix multiplication is fast and we can reuse the same CountSketch for different inputs. Our
“learned” CountSketch will retain this characteristic sparsity pattern and input-independence1, but
its non-zero entries will range in R.
We list our main contributions and follow this with a discussion.
•	Two-stage sketch optimization: to first place the non-zero entries and then learn their
values.
•	Theoretical worst-case guarantees, two ways: we derived a time-optimal method which
applies to MRR, LRA, k-means, and more. We also proved a simpler method works for
k-means.
•	SOTA experimental results: we showed the versatility of our method on 5 data sets with
3 types. Our method dominated on the majority of experiments.
•	Theoretical analysis on the necessity of two stages: we proved that including the first
stage is strictly better for LRA and two common input distributions.
•	Empirical demonstration of the necessity of two stages: showed that including the first
stage gives a 12, 20% boost for MRR, LRA.
Our sketch learning algorithm first places the sparse non-zero entries using a greedy strategy, and
then learns their values using gradient descent. The resulting learned CountSketch is very differ-
ent from the classical CountSketch: the non-zero entries no longer have random positions and ±1
values. As a result, the usual worst-case guarantees do not hold.
We sought a way to obtain worst-case guarantees that was fast and reasonably general. Our solution
is a fast comparison step which performs an approximate evaluation of learned and classical sketches
and takes the better of the two. Importantly, we can run this step before the approximation algorithm
without increasing its overall time complexity. As such, this solution is time-optimal and applies to
MRR, LRA, k-means, and more.
An alternate method was proposed by a previous work, but it was only proved for LRA (Indyk
et al., 2019). This “sketch concatenation” method just involves sketching with the concatenation of
a learned and a classical sketch. Since it is somewhat simpler, we wanted to extend its applicability.
In a novel theoretical result, we proved this works for k-means as well.
We also ran a diverse set of experiments to demonstrate the versatility and practicality of our ap-
proach. We chose five data sets spanning three categories (image, text, and graph) to test our method
on three applications (MRR, LRA, k-means). Importantly, these experiments have real-world coun-
terparts. For example, LRA and k-means can be used to compress images, applying SVD (LRA) to
text data is the basis of a natural language processing technique, and LRA can be used to compute
approximate max cuts on graph adjacency matrices. Ultimately, our method dominated on the vast
majority of tests, giving a 31, 70% improvement over classical CountSketch for MRR, LRA.
Finally, we conducted ablation study of the components of our algorithm. In another novel theoreti-
cal result, we proved that including the time-consuming first optimization stage is strictly better than
not to for LRA and two input distributions (spiked covariance and Zipfian). Empirically, this is case
for all 3 applications.
Related work. In the last few years, there has been much work on leveraging ML to improve clas-
sical algorithms; we only mention a few examples here. One related body of work is data-dependent
1While learned CountSketch is data-dependent (it is optimized using sample input matrices), it is still con-
sidered input-independent because it is applied to unseen input matrices (test samples).
2
Under review as a conference paper at ICLR 2021
dimensionality reduction, such as an approach for pair-wise/multi-wise similarity preservation for
indexing big data (Wang et al., 2017) and a method for learning linear projections for general ap-
plications (Hegde et al., 2015). We note that multiplying an input matrix on the left with a sparse
sketch is equivalent to hashing its rows to a small number of bins. Thus, we also find connections
with the body of work on learned hashes, most of which addresses the nearest neighbor search prob-
lem (see Wang et al. for a survey). However, in order to obtain approximation guarantees, our “hash
function” (sparse sketch) must satisfy properties which these learned hashes usually do not, such as
affine -embedding (Def. A.1).
In particular, we build off of the work of Indyk et al. (2019), which introduced gradient descent op-
timization for the LRA application. It also gave an LRA-specific method for worst-case guarantees.
We have surpassed the sketching performance and breadth of this work. Namely, we introduce a
sparsity pattern optimization step which is clearly crucial for sparse sketches. We also provide a
more general method for worst-case guarantees and extend their method to k-means.
2	Preliminaries
Our learned sketches have the sparsity pattern of the classical CountSketch. The construction of
this sketch is described below. We also define affine -embeddings, which is a class of sketches that
includes CountSketch. This -embedding property is desirable because it allows us to prove that
certain sketching algorithms give (1 + )-approximations.
Definition 2.1 (Classical CountSketch). The CountSketch (abbreviated as CS) matrix S has one
non-zero entry in each column with a random location and random value in {±1}.
Definition 2.2 (Affine Embedding). Given a pair of matrices A and B, a matrix S is an affine
-embedding if for all X of the appropriate shape, kS(AX - B)k2F = (1 ± ) kAX - Bk2F.
Notation. We denote the singular value decomposition (SVD) of A by A = UΣV> with orthog-
onal U, V and diagonal Σ. Relatedly, the Moore-Penrose pseudo-inverse of A is At = VΣ-1U>,
where Σ-1 is constructed by inverting the non-zero diagonal entries.
3	Framework
We describe a framework for learned CountSketch that can be adopted by many different applica-
tions, including least-squares regression (MRR), low-rank approximation (LRA), and k-means clus-
tering. We will return to these applications in the next section. In this section, we first describe how
to optimize a CountSketch over a set of training samples. Then, we explain how to use the learned
CountSketch to achieve good expected performance with worst-case guarantees. By running a “fast
comparison” step before the approximation algorithm, we can do this in optimal time.
Sketch optimization
Most applications are optimization problems. That is, they are defined by an objective function,
L(∙). For example, least-squares regression solves
min kAX -Bk2F
X
given A ∈ Rn×d, B ∈ Rn×d0. Of course, the optimal solution is a function of the inputs:
X* = arg min ∣∣AX 一 BkF = AtB
X
However, in the sketching paradigm, we compute an approximately optimal solution as a function
of the sketched (compressed) inputs. Taking CountSketch S ∈ Rm×n for m	n, we have:
X * = (SA)t(SB)
Our goal is to minimize the expected approximation error with respect to S, which is constrained to
the set of CountSketch-sparse matrices (CS). However, this is simply equivalent to minimizing the
3
Under review as a conference paper at ICLR 2021
objective value of the approximate solution.
S*= argmin E	[La,b ((SAwSB))-La,b (X *)]
S∈CS (A,B)〜D
=argmin E	[LA,B((SA)t(SB))]
S∈CS (A,B)〜D
For ease of notation, We will define G(∙) as a function which maps a sketch and inputs to an ap-
proximate solution. G(∙) is defined by the application-specific approximation algorithm. For MRR,
G(S, (A, B)) = (SA)t(SB). More generally, the sketch optimization objective is:
S* = arg min E [LA(G(S,A))]	(3.1)
S∈CS A 〜D
If the application is regression, we let A be (A, B).
We will solve this constrained optimization in two stages. For both stages, we approximate the
expectation in empirical risk minimization (ERM) fashion. That is, we approximate the expectation
over the true distribution by averaging over a sampled batch of the training set. Now, in the first
optimization stage, we compute positions for the CountSketch-sparse nonzero entries. Then, in the
second stage, we fix the positions and optimize the nonzero values.
Stage 1: Placing the nonzero entries. We want to maintain the sparsity pattern of CS (one
nonzero entry per column), but we are free to place that nonzero entry wherever we like for each
column. A naive method would be to evaluate the objective for the exponential number of full place-
ments. This is clearly intractable, so we consider a greedy alternative. In essence, we construct the
sketch one nonzero entry at a time, and we choose the location of the next entry by minimizing (3.1)
over the discrete set of possibilities.
More precisely, we build the sketch S ∈ Rm×n iteratively, placing one nonzero entry at a time. For
each nonzero entry, we consider m locations and 2 values for each location (±1). We evaluate the
sketch optimization objective (3.1) for all 2m incremental updates to S and choose the minimizing
update. In the pseudo-code below, we iterate through the n columns of S, each of which contains a
non-zero entry. Note that Sw,j = S + w(e~j e~i>) adds a single entry w to the i-th column, j-th row
of the current, partially-constructed S.
Algorithm 1 GREEDY STAGE
Require: Atrain = {A1, ..., AN} with Ai ∈ Rn×d; sketch dimension m
1:	initialize S = Θm×n
2:	for i = 1 to n do
3:	w*, j* = arg min P	LAi (G(Sw,j, Ai)) where Sw,j = S + w(e~je~i>)
w∈{±1},j∈[m]Ai∈Atrain
4:	S[j*,i] =w*
5:	end for
For some applications it can be inefficient to evaluate (3.1), since it requires computing the approx-
imate solution. For MRR and LRA, the approximate solution has a closed form, but for k-means, it
must be computed iteratively. This is prohibitively expensive, since we perform many evaluations.
In this case, we recommend finding a surrogate L(∙) with a closed-form solution, as we illustrate in
later sections.
Stage 2: Optimizing the nonzero values. We now fix the positions of the nonzero entries and
optimize their values using gradient descent. To fix the positions, we represent S as just a vector of
its nonzero entries, ~v ∈ Rn. We will denote H (~v) : Rn → Rm×n as the function which maps this
concise representation of S to the full matrix. H(∙) depends on the positions computed in the last
stage, which are fixed.
Now, we simply differentiate E [LA(G(H (~v), A))] (3.1) with respect to ~v.
A〜D
4
Under review as a conference paper at ICLR 2021
Algorithm 2 GRADIENT STAGE
Require: Atrain = {Aι,…，AN} With Ai ∈ Rn×d; H(∙) from Alg. 1; learning rate a
1:	for i = 1 to niter do
2:	S = Θm×n
3:	sample Abatch from Atrain
4:	~ 一 ~ - α( P	dLA(GdH ⑹"]
A∈Abatch
5:	end for
Learned sketch with worst-case guarantees
We first run a fast comparison betWeen our learned sketch and a classical one and then run the ap-
proximation algorithm With the “Winner”. This alloWs us to compute an approximate solution (to
the applications We consider here; i.e., MRR and LRA) that does not perform Worse than classical
CountSketch. In other Words, our solution has the same Worst-case guarantees as classical CountS-
ketch. The benefit of this is that these guarantees hold for arbitrary inputs, so our method is protected
from out-of-distribution inputs as Well as in-distribution inputs Which Were not Well-represented in
the training data.
More precisely, for a given input A, We quickly compute a rough estimate of the approximation
errors for learned and classical CountSketches. This rough estimate can be obtained by sketching.
We take the sketch With the better approximation error and use it to run the usual approximation
algorithm.
The choice to compute a rough estimate of the approximation error rather than the exact value is
crucial here. It alloWs us to append this fast comparison step Without increasing the time complexity
of the approximation algorithm. Thus, the Whole procedure is still time-optimal.
Though this method is simple, an even simpler method exists for some applications. Indyk et al.
proved that “sketch concatenation” (sketching With a classical sketch appended to the learned one)
retains Worst-case guarantees for LRA. We prove that this also Works for k-means clustering (The-
orem 4.6).
Algorithm 3 LEARNED-SKETCH-ALGORITHM
Require: learned sketch Sl ; classical sketch SC; trade-off parameter β; input data A
1: Define ML, MC suchthatLA(G(SL,A)) = kMLk2F,LA(G(SC,A)) = kMCk2F
2: S 一 CountSketch ∈ Rβ2×n, R 一 CountSketch ∈ Rβ2 ×d
3:	Δl 一 ∣∣SMlR>∣∣F, ∆c 一 IlSMCR>∣∣F
4:	if ∆L ≤ ∆C then
5:	return G(SL, A)
6:	end if
7:	return G(SC, A)
This algorithm can be used for applications that minimize a Frobenius norm. In the MRR example,
La(G(Sl,A)) = ∣∣A [(SlA)*(SB)] - b∣∣F
so ML = A [(SlA)t(SB)] - B. Note that β is a parameter which trades off the precision of the
approximation error estimate and the runtime of Algorithm 3.
4 Instantiations
For each of 3 problems (least-squares regression, low-rank approximation, k-means clustering), we
define the problem,s objective, La(∙), and the approximation algorithm, G(∙), which maps a sketch
and inputs (S, A) to an approximate minimizer of La(∙).
5
Under review as a conference paper at ICLR 2021
4.1	Least-squares regression
We consider a generalized version of `2 regression called “multiple-response regression” (MRR).
Given a matrix of observations (A ∈ Rn×d, with n d) and a matrix of corresponding values
(B ∈ Rn×d0, with n d0), the goal of MRR is to solve
mXin L(A,B)(X) = mXin kAX - Bk* 1 2 3F
Approximation algorithm. The algorithm is simply to sketch A, B with a sparse sketch S (e.g.,
CS) and compute the closed-form solution on SA, SB, which are small (Algorithm 4).
Algorithm 4 Sketch-Regression (Sarlos, 2006; Clarkson & Woodruff, 2017)
Require: A ∈ Rn×d, B ∈ Rn×d0, S ∈ Rm×n
1:	return: (SA)*(SB)
Sketch optimization. For both the greedy and gradient stages, we use the objective
L(A,B)(G(S, (A, B))) = kA(SA)+(SB) - Bk2F.
Learned sketch algorithm. For fixed accuracy, our learned sketch algorithm achieves state-
of-the-art time complexity, besting the classical algorithm. We prove an equivalent statement:
the learned sketch algorithm yields better approximation error for fixed time complexity. (See
Lemma A.4 for the classical algorithm’s worst-case guarantee). In the following theorem, we as-
sume Alg. 3 uses a learned sketch which is an affine β-embedding and also a classical sketch of the
same size which is an affine -embedding. If β < , the above statement is true.
Theorem 4.1 (Learned sketching with guarantees for MRR). Given a learned sparse sketching
matrix SL ∈ Rd2 ×n which attains a (1 + γ)-approximation for MRR on A ∈ Rn×d, B ∈ Rn×d',
Alg. 3 gives a (1 + O(β + min(γ, )))-approximation for MRR on A in O(nnz(A) + nnz(B) +
d5 6d0-4 + dβ-4) time where β is a trade-off parameter.
Remark 4.2. By setting the trade-off parameter β-4 = O(-4d4d0), Alg. 3 has the same asymp-
totic runtime as the best (1 + )-approximation algorithm of MRR with classical sparse em-
bedding matrices. Moreover, Alg. 3 for MRR achieves a strictly better approximation bound
(1 + O(β + γ)) = (1 + o()) when γ = o(). On the other hand, in the worst case scenario
when the learned sketch performs poorly (i.e., Y = Ω(e)) the approximation guarantee of Alg. 3
remains (1 + O()).
4.2	Low-rank approximation
Given an input matrix A ∈ Rn×d and a desired rank k, the goal of LRA is to solve
min LA(X) = min kX - Ak2F
X,rank k	X,rank k
Approximation algorithm. We consider the time-optimal (up to low order terms) approximation
algorithm by Sarlos; Clarkson & Woodruff; Avron et al. (Algorithm 5) with worst-case guarantees
described in Lemma A.8.
Algorithm 5 SKETCH-LOWRANK (Sarlos, 2006; Clarkson & Woodruff, 2017; Avron et al., 2016).
Require: A ∈ Rn×d,S ∈ RmS×n,R ∈ RmR×d,S2 ∈ RmS2 ×n,R2 ∈ RmR2 ×d
1:	UC	[Tc	Tc]	J S2AR>,	]TD>]	UD	J SAR>	with Uc,	UD	orthogonal
2: G J S2AR>
3: ZL0 ZR0 J [Uc>GUD]k
4: ZL = [ZL0 TD-> 0] ,ZR= Tc-01ZR
5: Z = ZLZR
6: return: AR>ZSA in form Pn×k, Qk×d
6
Under review as a conference paper at ICLR 2021
Sketch optimization. For the greedy stage, we optimize sketches S and R individually. However,
we do not use LA(G((S, R, S2, R2), A)) = kX - Ak2F with X from Alg. 5 as the objective. This
is because the optimization for one sketch would then depend on the other sketches. Instead, we use
a proxy objective. We observe that the proof that Alg. 5 is an -approximation uses the fact that the
row space of SA and the column space of AR both contain a good rank-k approximation to A. Thus,
the proxy objectives for S and R are for k-rank approximation in the row/column space of SA/AR,
respectively. For example, the proxy objective for S is LA(G(S, A)) = [AV]kV > - A2F where
V is from the SVD SA = UΣV> and Hk takes the optimal k-rank approximation using truncated
SVD. The proxy objective for R is defined analogously.
For the gradient stage, we optimize all four sketches (S, R, S2 , R2) simultaneously using
LA(G((S, R, S2, R2), A)) = kX - Ak2F with X from Algorithm 5, since it can easily be imple-
mented with differentiable functions.
Learned sketch algorithm. Just like for regression (Section 4.1), we can prove that our learned
sketch algorithm achieves a better accuracy than the classical one given the same runtime.
Theorem 4.3 (Low-Rank Approximation). Given learned sparse sketching matrix SL	∈
RPoly(k)×n, RL ∈ RPoly(k)×d which attain a (1 + γ)-approximationfor LRA on A ∈ Rn×d, Alg. 3
k k4
gives a (1 + β + min(γ, O')-approximation for LRA on A in O(nnz(A) + (n + d) poly(工)+ 货∙
poly( k)) time where β is a trade-off parameter.
Remark 4.4. For	k(n + d)-4, by setting the trade-off parameter β-4 = O(k(n + d)-4), Alg. 3
has the same asymptotic runtime as the best (1 + )-approximation algorithm of LRA with classical
sparse embedding matrices. Moreover, Alg. 3 for LRA achieves a strictly better approximation
bound (1 + O(β + γ)) = (1 + o()) when γ = o(). On the other hand, in the worst case scenario
when the learned sketches perform poorly (i.e., Y = Ω(e)) the approximation guarantee of Alg. 3
remains (1 + O()).
Greedy stage offers strict improvement. Finally, we prove for LRA that including the greedy
stage is strictly better than not including it. We can show this for two different natural distributions
(spiked covariance and Zipfian). The intuition is that the greedy algorithm separates heavy-norm
rows (which are important “directions” in the row space) into different bins.
Theorem 4.5. Consider a matrix A from either the spiked covariance or Zipfian distribution. Let
SL denote a sparse sketch that Algorithm 1 has computed by iterating through indices in order of
non-increasing row norms of A. Let SC denote a CountSketch matrix. Then, there is a fixed η > 0
such that minrank-k X∈rowsP(SLA) kX - AkF ≤ (1 - η) minrank-k X ∈rowsP(SC A) kX - AkF.
4.3	k-MEANS CLUSTERING
Let A ∈ Rn×d represent a set of n points, A1, . . . , An ∈ Rd. In k-means clustering, the goal is to
find a partition ofA1, ..., An into k clusters C = {C1, . . . , Ck} to
k
minLA(C) = minX mind X IlAj-μikF
i=1 *	j∈Ci
where μ% denotes the center of cluster Ci.
Approximation algorithm. First, compress A into AV, where V is from SVD SA = UΣV> and
S ∈ RO(k2/e2)xn is a CountSketch. Then, We use an approximation algorithm for k-means, such
as Lloyd’s algorithm with a k-means++ initialization (Lloyd, 1982; Arthur & Vassilvitskii, 2007).
Solving with AV gives a (1 + )-approximation (Cohen et al., 2015) and Lloyd’s gives an O(log(k))
approximation, so we have a (1 + )O(log(k)) approximation overall.
Sketch optimization. k-means is an interesting case study because it presents a challenge for both
stages of optimization. For the greedy stage, we observe that k-means does not have a closed form
solution, which means that evaluating the objective requires a (relatively) time-consuming itera-
tive process. In the gradient stage, it is possible to differentiate through this iterative computation,
7
Under review as a conference paper at ICLR 2021
but propagating the gradient through such a nested expression is time-consuming. Thus, a proxy
objective which is simple and quick to evaluate would be useful for both stages.
Cohen et al. showed that we get a good k-means solution if A is projected to an approximate top
singular vector space. This suggests that a suitable proxy objective would be low-rank approxima-
tion: LA (G(S, A)) = [AV]kV > - A2F, like in Section 4.2. We use this objective for both the
greedy and gradient stages.
Learned sketch algorithm. For k-means, we can use a different method to obtain worst-case
guarantees. We prove that by concatenating a classical sketch to our learned sketch, our sketched
solution will be a (1 + O())-approximation.
Theorem 4.6 (Sketch monotonicity property for k-means). For a given classical CountSketch
SC ∈ RO(Poly韭/'"×n, sketching With any extension of SC (i.e., by a learned sparse sketch SL)
yields a better approximate solution for k-means than sketching with SC itself.
5 Evaluation
We implemented and compared the following sketches:
•	Ours: a sparse sketch for which both the values and the positions of the non-zero entries
have been optimized
•	GD only: a sparse sketch with optimized values and random positions for non-zero entries
•	Random: classical random CountSketch
We also consider two “naively” learned sketches for LRA, which are computed on just one sample
from the training set.
•	Exact SVD: sketch as AVm, where Vm contains the top m right singular vectors of random
sample Ai ∈ Atrain
•	Column sampling: sketch as AR, where R is computed from a randomly sampled
Ai ∈ Atrain . Each column of R contains one entry. The location of this entry is sam-
pled according to the squared column norms of Ai; the value of this entry is the inverse of
the selected norm.
Data sets. We used high-dimensional data sets of three different types (image, text, and graph)
to test the performance and versatility of our method. Note that the regression datasets are formed
from the LRA/k-means datasets by splitting each matrix into two parts.
Table 5.1: Data set descriptions
Name	Description	A shape (for LRA, k-means)	(A, B) shapes (for MRR)	Ntrain	Ntest
Friends	Frames from a scene in the TV show Friends2	5760 × 1080	(5760 × 1079),(5760 × 1)^^	^400	^lGG-
Hyper	Hyperspectral images depicting outdoor scenes (Imamoglu et al., 2018)	1024 × 768	(1024 × 767), (1024 × 1)	400	100
Logo	Frames from video of logo being painted3	5760 × 1080	(5760 × 1079),(5760 × 1)^^	^400	^Iqq-
Yelp	tf-idf (Ramos, 2003) of restaurant reviews, grouped by restaurant4	7000 × 800	(7000 × 640),(7000 × 160)	^^260	^"65
Graph	Graph adjacency matrices of social circles on Google+ (Leskovec & McAuley, 2012)	1052 × 1052	(1052 × 842), (1052 × 210)	147	37
2http://youtu.be/xmLZsEfXEgE
3http://youtu.be/L5HQoFIaT4I
4https://www.yelp.com/dataset
8
Under review as a conference paper at ICLR 2021
Evaluation metric. To evaluate the quality of a sketch S, we compute the difference between the
objective value of the sketched solution and the optimal solution (with no sketching). That is, the
values in this section’s tables denote
∆s = La(G(S, Atest))-LA
averaged over 10 trials.
Analysis of results. For least-squares regression and LRA, our method is the best. It is signif-
icantly better than random, obtaining improvements of around 31% and 70% for MRR and LRA
respectively compared to a classical sparse sketch. For k-means, “Column Sampling” and “Exact
SVD” dominated on several data sets. However, we note that our method was always a close second
and also, more importantly, the values for k-means were only trivially different (< 1%) between
methods.
Ablation of greedy stage We find empirically that including the greedy optimization stage is al-
ways better than not including it (compare the “Ours” vs. “SGD only” methods). For regression,
LRA, and k-means, including this step offers around 12%, 20%, and < 1% improvement respec-
tively. We should note that for k-means, the values generally do not differ much between methods.
Table 5.2: Average errors for least-squares regression
Parameters	Algorithm	Datasets				
m		Friends	Hyper	Logo	Yelp	Graph
^^0	Ours	0.4360 :5.6e-02	0.6242 :9.7e-03	0.1942 :1.2e-02	0.2734 :1.8e-03	58.6568 ±6.6e-02
	SGD only	0.5442 :9.0e-03	0.6790 :1.2e-02	0.2547 :3.7e-03	0.2781 :6.3e-04	58.7231 ±8.7e-02
	Random	0.7792 :2.5e-02	0.8226 :4.4e-02	0.3773 :1.5e-02	0.3173 :1.3e-03	59.5535 ±1.2e-01
^40	Ours	0.2485 :1.1e-02	0.5380 :8.3e-03	0.1607 :5.7e-03	0.2639 :4.1e-04	55.5267 ±8.0e-02
	SGDonly	0.3688 :9.0e-03	0.5718 :1.2e-02	0.1858 :2.8e-03	0.2701 :7.2e-04	55.6120 ±6.5e-02
	Random	0.5213 :1.6e-02	0.6414 :1.9e-02	0.2389 :6.9e-03	0.3114 :2.6e-03	55.7896 ±6.7e-02
Table 5.3: Average error for LRA
Parameters	Algorithm	Datasets				
rank k, m		Friends	Hyper	Logo	Yelp	Graph
(20, 40)	Ours	0.8998 ±2.7e-02	2.4977 ±1.8e-01	0.5009 ±2.2e-02	0.1302 ±3.5e-04	30.0969 = 1.3e-01
	SGD only	1.0484 ±1.3e-02	3.7648 ±4.2e-02	0.6879 ±8.8e-03	0.1316 ±8.8e-04	30.5237 = 1.5e-01
	Random	4.0730 ±1.7e-01	6.3445 ±1.8e-01	2.3721 ±8.3e-02	0.1661 ±1.4e-03	33.0651 = 3.4e-01
	Exact SVD	1.5774 ±1.1e-01	3.4406 ±8.7e-01	0.7470 ±1.0e-01	0.1594 ±3.7e-03	31.0617 =3.4e-01
	Column sampling	5.9837 ±6.6e-01	9.7126 ±8.2e-01	4.2008 ±6.0e-01	0.1881 ±3.2e-03	43.9920 =5.6e-01
(30, 60)	Ours	0.7945 ±1.5e-02	2.4920 ±2.6e-01	0.4929 ±2.3e-02	0.1128 ±3.2e-04	30.6163 = 1.8e-01
	SGD only	1.0772 1.2e-02	3.7488 ±1.8e-02	0.7348 ±7.1e-03	0.1137 ±5.9e-04	30.9279 =2.0e-01
	Random	2.6836 ±7.4e-02	5.3904 ±7.7e-02	1.6428 ±4.3e-02	0.1463 ±2.3e-03	32.7905 =2.1e-01
	Exact SVD	1.1678 ±5.2e-02	3.0648 ±8.5e-01	0.5958 ±7.2e-02	0.1326 ±2.3e-03	32.0861 = 5.9e-01
	Column sampling	4.1899 ±2.9e-01	8.2314 ±4.7e-01	3.2296 ±3.1e-01	0.1581 ±3.9e-03	44.2672 =5.2e-01
Table 5.4: Average errors for k-means clustering
Algorithm	Parameters	Data sets				
# clusters, m, rank k		Friends	Hyper	Logo		Yelp	Graph
(20, 40, 20)	Ours	0.2934 ±5.8e-05	0.6110 ±1.7e-04	0.1251 ±1.2e-04	2.4755 ±7.2e-04	2.9797 ±1.9e-03
	SGD only	0.2935 ±1.4e-04	0.6117 ±2.0e-04	0.1253 ±5.8e-05	2.4756 ±7.6e-04	2.9810 ±9.9e-04
	Random	0.2933 ±1.5e-04	0.6122 ±2.6e-04	0.1254 ±1.3e-04	2.4759 ±7.7e-04	2.9834 ±1.8e-03
	Exact SVD	0.2934 ±1.2e-04	0.6113 ±2.5e-04	0.1253 ±2.5e-04	2.4797 ±9.0e-04	2.9779
	Column sampling	0.2932 ±1.7e-04	0.6129 ±3.7e-04	0.1250 ±2.7e-04	2.4922 ±3.8e-03	3.0137 ±2.9e-03
(30, 60, 30)	Ours	0.2604 ±7.6e-05	0.5448 ±1.4e-04	0.1062 ±7.7e-05	2.4475 ±6.8e-04	2.8765 ±1.1e-03
	SGD only	0.2605 ±4.9e-05	0.5452 ±1.6e-04	0.1063 ±1.2e-04	2.4476 ±7.1e-04	2.8777 ±1.2e-03
	Random	0.2604 ±9.3e-05	0.5453 ±1.6e-04	0.1062 ±4.6e-05	2.4477 ±7.4e-04	2.8791 ±1.4e-03
	Exact SVD	0.2605 ±1.8e-04	0.5446 ±2.6e-04	0.1063 ±4.4e-05	2.4532 ±4.6e-04	2.8739 ±1.7e-03
	Column sampling	0.2602	0.5459 ±3.6e-04	0.1059	2.4665 ±1.1e-03	2.9002 ±2.2e-03
9
Under review as a conference paper at ICLR 2021
References
David Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. In Proceed-
ings of the 18th Annual ACM SIAM Symposium on Discrete Algorithms ,pp.1027-1035, 2007.
Haim Avron, Kenneth L Clarkson, and David P Woodruff. Sharper bounds for regularized data
fitting. arXiv preprint arXiv:1611.03225, 2016.
Matthew Brand. Fast low-rank modifications of the thin singular value decomposition. Linear
Algebra and its Applications, 415.1, 2006.
Moses Charikar, Kevin Chen, and Martin Farach-Colton. Finding frequent items in data streams.
In International Colloquium on Automata, Languages, and Programming, pp. 693-703. Springer,
2002.
Kenneth L Clarkson and David P Woodruff. Numerical linear algebra in the streaming model. In
Proceedings of the forty-first annual symposium on Theory of computing (STOC), pp. 205-214,
2009.
Kenneth L. Clarkson and David P. Woodruff. Sketching for m-estimators: A unified approach
to robust regression. In Proceedings of the 26th Annual ACM SIAM Symposium on Discrete
Algorithms, pp. 921-939, 2014.
Kenneth L Clarkson and David P Woodruff. Low-rank approximation and regression in input spar-
sity time. Journal of the ACM (JACM), 63(6):54, 2017.
Michael B Cohen, Sam Elder, Cameron Musco, Christopher Musco, and Madalina Persu. Dimen-
sionality reduction for k-means clustering and low rank approximation. In Proceedings of the
forty-seventh annual ACM symposium on Theory of computing, pp. 163-172, 2015.
Dan Feldman, Melanie Schmidt, and Christian Sohler. Turning big data into tiny data: Constant-size
coresets for k-means, pca and projective clustering. In Proceedings of the 24th annual ACM-SIAM
symposium on Discrete Algorithms, pp. 1434-1453, 2013.
Chinmay Hegde, Aswin C. Sankaranarayanan, Wotao Yin, and Richard G. Baraniuk. Numax: A
convex approach for learning near-isometric linear embeddings. In IEEE Transactions on Signal
Processing, pp. 6109-6121, 2015.
Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency estimation
algorithms. International Conference on Learning Representations, 2019.
Nevrez Imamoglu, Yu Oishi, Xiaoqiang Zhang, Guanqun Ding, Yuming Fang, Toru Kouyama, and
Ryosuke Nakamura. Hyperspectral image dataset for benchmarking on salient object detection.
In International Conference on Quality of Multimedia Experience, pp. 1-3. IEEE, 2018.
Piotr Indyk, Ali Vakilian, and Yang Yuan. Learning-based low-rank approximations. In Advances
in Neural Information Processing Systems, pp. 7400-7410, 2019.
Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned
index structures. In Proceedings of the 2018 International Conference on Management of Data,
pp. 489-504, 2018.
Jure Leskovec and Julian McAuley. Learning to discover social circles in ego networks. In Advances
in Neural Information Processing Systems, pp. 539-547, 2012.
Stuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):
129-137, 1982.
Thodoris Lykouris and Sergei Vassilvitskii. Competitive caching with machine learned advice. In
International Conference on Machine Learning, pp. 3302-3311, 2018.
Konstantin Makarychev, Yury Makarychev, and Ilya P. Razenshteyn. Performance of johnson-
lindenstrauss transform for k-means and k-medians clustering. In Proceedings of the 51st Annual
ACM SIGACT Symposium on Theory of Computing, STOC, pp. 1027-1038, 2019.
10
Under review as a conference paper at ICLR 2021
Xiangrui Meng and Michael W Mahoney. Low-distortion subspace embeddings in input-sparsity
time and applications to robust linear regression. In Proceedings of the forty-fifth annual ACM
symposium on Theory ofcomputing, pp. 91-100, 2013.
Jelani Nelson and HUy L Nguyen. Osnap: Faster numerical linear algebra algorithms via sparser
subspace embeddings. In Foundations of Computer Science (FOCS), 2013 IEEE 54th Annual
Symposium on, pp. 117-126, 2013.
Manish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ml predictions.
In Advances in Neural Information Processing Systems, pp. 9661-9670, 2018.
Juan Ramos. Using tf-idf to determine word relevance in document queries. In First Instructional
Conference on Machine Learning, pp. 133-142, 2003.
Tamas Sarlos. Improved approximation algorithms for large matrices via random projections. In
47th Annual IEEE Symposium on Foundations of Computer Science (FOCS), pp. 143-152, 2006.
Jingdong Wang, Ting Zhang, Nicu Sebe, and Heng Tao ShenWang. A survey on learning to hash.
In IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 769-790, 2017.
David P Woodruff. Sketching as a tool for numerical linear algebra. Foundations and TrendsR in
Theoretical Computer Science, 10(1-2):1-157, 2014.
A Appendix
Corresponding to Section 4.1: Least-Squares Regression
Definition A.1 (Subspace Embedding). Given an n × d matrix A, a matrix S is an affine -
embedding for (the column space of) A iffor x ∈ Rd, kS Axk22 = (1 ± ) kAxk22.
The following result is shown in Clarkson & Woodruff (2017) and sharpened with Nelson & Nguyen
(2013); Meng & Mahoney (2013).
Lemma A.2. Given matrices A, B with n rows, a sparse embedding matrix (i.e., random CS) with
O(rank(A)2/2) rows is an affine -embedding matrix with constant probability. Moreover, the
matrix product SA can be computed in O(nnz(A)) time, where nnz(A) denotes the number of
non-zero entries of matrix A.
Lemma A.3 (Clarkson & Woodruff (2017); Lemma 40). Let A be an n × d matrix and let S ∈
RO(3)×n be a randomly ChoSen sparse embedding matrix (i.e., random CS). Then with ConStant
probability, kS Ak2F = (1 ± ) kAk2F.
Lemma A.4 (Sarlos (2006); Clarkson & Woodruff (2017)). Given ClassiCal CountSketCh S ∈
Rm×n, SKETCH-REGRESSION(A, B, S) returns a (1 + )-approximation in time O(nnz(A) +
nnz(B) + dd0m2 + min(d2m, dm2)).
Proof: Since S is an affine -embedding matrix of A, B, then
kSAX-SBk2F = (1±)kAX-Bk2F
Next, by the normal equations, (SA)+(SB) is a minimizer of minX kSAX - SBk2F and is a (1 +
3)-approximate solution of minX kAX - Bk2F.
To bound the runtime, note that since S is a CountSketch we can compute SA and SB in time
O(nnz(A) + nnz(B)) and reduce the problem to an instance of multiple-response regression with
m rows. Then, we can solve the reduced size problem in time O(dd0m2 + min(d2m, dm2)):
O(min(dm2 , d2m)) to compute (S A)+ and O(dd0m2) to compute (S A)+ (SB).
Proofof Theorem 4.1. By Lemma A.2, a random CS SO with O( d2) rows is an affine e-embedding
matrix of A, B with constant probability.
Next, let XL and XO be respectively the solutions returned by SKETCH-REGRESSION(A, B, SL)
and SKETCH-REGRESSION(A, B, SO). By Lemma A.4, with constant probability,
kAXO - BkF ≤ (1 + 3e) min kAX - BkF .
X
11
Under review as a conference paper at ICLR 2021
Together with the assumption that the solution constructed from SL, which is denoted as XL , over
Atrain is a γ-approximate solution,
min(kAXL - Bk2F , kAXO - Bk2F) ≤ (1 + min(3,γ)) min kAX - Bk2F .	(A.1)
X
22
Hence, it only remains to compute the minimum of kAXL - B kF and kAXO - B kF efficiently.
Note that it takes Ω(n ∙ d ∙ d0) to compute these values exactly. However, for our purpose it suffices
to compare (1 + β)-estimates of these values and return the minimum of the estimates. To achieve
this, We use two applications OfLemma A.3 with R> ∈ RO(β2)×d , S ∈ RO(β2)×n. For any X0 (in
particular, both XO and XL), with constant probability,
kS(AX0 - B)Rk2F = R>(AX0-B)>S>2F = (1±β)k(AX0-B)k2F	(A.2)
Let ΓL and ΓO respectively denote AXL - B and AXO - B and let ΓM =
arg min(kSΓLRkF , kSΓORkF). By Eq. (A.2) and union bound over of XO and XL, with con-
stant probability,
∣∣Γmk	≤	(1	+ O(β)) ∙ ∣∣SΓmRkF	B	by	Lemma A.3
≤	(1	+ O(β)) ∙ min(∣Γo∣∣F , ∣Γl∣F)	B	by	Lemma A.3
≤	(1	+ O(β + min(, β))) ∣AX - B∣2F	B	by	Eq. (A.1)
Runtime Analysis. By Lemma A.4, XO an XL can be computed in O(nnz(A) + nnz(B) +
d5d0-4) time. Next, the time to compute ∆L and ∆O is
O(nnz(A) + nnz(XR) + nnz(B) + ɪ) = O(nnz(A) + nnz(B) + dd0 + ɪ),
where X is either X。or XL and we use that fact that nnz(X) ≤ d ∙ d0 (i.e., the total number of cells
in X).
Thus, the total runtime of Algorithm 3 is O(nnz(A) + nnz(B) + d5d0-4 + dβ-4).
Theorem A.5 (Least Squares Regression). Suppose there exists a learned, sparse, subspace em-
bedding matrix SL computed over Atrain With poly( d) rows that attains a β-approximation over
Atest. Then, there exists an algorithm that runs in time O(nnz(A) + poly(d/)) that outputs a
(1 + min(β, ))-approximation to the least squares regression problem.
Proof: Note that to solve minx∈Rd ∣Ax - b∣2, given a sparse embedding matrix S ∈ Rm×n for the
columns ofA together with the vector b, the problem can be solved within (1 + )-approximation in
time nnz(A) + poly(d/) (e.g., see Theorem 2.14 Woodruff (2014)).
The proof outline is similar to the proof of Theorem 4.1. First, we compute solutions xL , xO ∈ Rd
to the given instance using respectively a learned sketching matrix SL and a learning-free sketching
algorithm. Then, we compare ∣AxL - b∣2 and ∣AxO - b∣2 and report the better. Note that since
xL, xO are vectors, unlike the case of MRR, the naive comparison (i.e., without applying any sketch-
ing matrices) takes nnz(A). Hence, the algorithm runs in time nnz(A) + poly(d/) and returns a
(1 + min(, β))-approximate solution of the least squares regression problem.
Moreover, we can employ the best known sketching techniques for the least squares regression
problem and achieve the dependence d/2 in the running time (e.g., see Section 2.5 of Woodruff
(2014)).
A. 1 Corresponding to Section 4.2: Low-rank approximation
Lemma A.6. Suppose that S ∈ RmS ×n and R ∈ RmR×d are sparse affine -embedding matrices
for (A> , A) and ((SA)> , A> ). Then,
min
rank-k X
AR>XSA-A2F
≤ (1 + ) ∣Ak - A∣2F
Proof: Consider the following multiple-response regression problem:
min ∣AkX - A∣2F.
rank-k X
(A.3)
12
Under review as a conference paper at ICLR 2021
Note that since X = Ik is a feasible solution to Eq. (A.3), minrank-k X kAkX - Ak2F =
kAk-Ak2F. Let S ∈ RmS×n be a sketching matrix that satisfies the condition of Lemma A.9
for A := Ak and B := A. By the normal equations, the rank-k minimizer of kSAkX - S Ak2F is
(SAk)+SA. Hence,
Ak(SAk)+SA-A2F ≤ (1+)kAk-Ak2F,	(A.4)
which in particular shows that a (1 + ) rank-k approximation of A exists in the row space of SA.
In other words,
min kXSA-Ak2F ≤ (1+)kAk-Ak2F.	(A.5)
rank-k X
Next, let R ∈ RmR ×d be a sketching matrix that satisfies the condition of Lemma A.9 for A :
(SA)> and B := A>. Let Y denote the rank-k minimizer of R(S A)> X > - RA> 2F. Hence,
(SA)>Y>-A>2F ≤(1+) min kXSA - Ak2F
rank-k X
≤ (1 + O()) kAk - Ak2F
B Lemma A.9
B Eq. (A.5)	(A.6)
Note that by the normal equations, again rowsp(Y >) ⊆ rowsp(RA>) and we can write Y
AR>Z where rank(Z) = k. Thus,
min	AR>XSA-A2F ≤ AR>ZSA- A2F
rank-k X
= (SA)>Y> -A>2F	BY = AR>Z
≤ (1 + O()) kAk - AkF	B Eq. (A.6)
Lemma A.7 (Avron et al. (2016); Lemma 27). For C ∈ Rp×m0, D ∈ Rm×p0, G ∈ Rp×p0, the
following problem
min
rank-k Z
kCZD-Gk2F
(A.7)
can be solved in O(pm0rC + p0mrD + pp0(rD + rC)) time, where rC = rank(C) ≤ min{m0, p}
and rD = rank(D) ≤ min{m, p0}.
Proof: Let UC and UD> be orthogonal bases for colsp(C) and rowsp(D), respectively, so that for
each Z, CZD = UCZ0UD> for some Z0. Let PC and PD be the projection matrices onto the
subspaces spanned by the rows of C> and D>, respectively: PC = UC UC> and PD = UDUD> .
Then by the Pythagorean theorem,
kCZD-Gk2F = PCUCZ0UD>PD-G2F
=	PCUCZ0UD>PD	- PC GPD	F +	kPC G(I	-	PD)k2F	+	k(I	-	PC)Gk2F	,
where the first equality holds since PC UC = UC and UD>PD = UD> and the second equality follows
from the Pythagorean theorem. Hence,
argminkCZD-Gk2F =argminPCUCZUD>PD -PCGPD2F.
rank-k Z	rank-k Z
Moreover,
PCUCZUD>PD -PCGPD2F = UCZUD> -UCUC>GUDUD>2F = Z-UC>GUD2F,
where the first equality holds since UC>UC = I and UD>UD = I, and the second equality holds since
UC and UD> are orthonormal. Hence,
arg min kCZD - Gk2F = arg min Z - UC>GUD 2F .
rank-k Z	rank-k Z
Next, we can find Z = [UC>GUD]k by computing the SVD of UC>GUD in form of ZLZR where
ZL ∈ Rm0×k and ZR ∈ Rk×m.
13
Under review as a conference paper at ICLR 2021
Runtime analysis. We can compute UC and UD by the Gram-Schmidt process in time O(pm0rC +
p0mrD), and UC>GUD in time O(min{rCp0(p + rD), rDp(p0 + rC)}). Finally, the time to compute
Z (i.e., an SVD computation of UCGUD) is O(rcrD ∙ min{rc, r。}). Since rc ≤ min{p, m0} and
rD ≤ min{p0, m}, the total runtime to minimize Z in Eq. (A.7) is O(pm0rC + p0mrD + pp0(rC +
rD)).
Lemma A.8. Let S ∈ RPoly是/0×d, R ∈ RPolyM0×d be cs matrices Such that
min
rank-k X
AR>XSA-A2F ≤ (1+γ)kAk-Ak2F.
(A.8)
Moreover, let S2 ∈ Rβ2×n, and R2 ∈ Rk2×d be CS matrices. Then, Algorithm 5 runs in
O(nnz(A) + (n + d) poly(k/)) time and with constant probability gives a (1 + O(β + γ))-
approximate rank-k approximation of A.
Proof: The algorithm first computes C = S2AR> , D = S AR2> , G = S2AR2> which can be
done in time O(nnz(A)). As an example, we bound the time to compute C = S2AR. Note that
since S2 is a CS, S2A can be computed in O(nnz(A)) time and the number of non-zero entries
in the resulting matrix is at most nnz(A). Hence, since R is a CS as well, C can be computed
in time O(nnz(A) + nnz(S2A)) = O(nnz(A)). Then, it takes an extra poly(k∕e) ∙ 呈 time to
store C, D and G in matrix form. Next, as we showed in Lemma A.7, the time to compute Z in
Algorithm 5 is O(品∙poly(k∕e)). Finally, ittakes (n + d) poly(k∕e) time to compute Q = AR>Zl
and P = ZRSA and return the solution in the form of Pn×kQk×d. Hence, the total runtime is
O(nnz(A) + (n + d) poly(k∕e) + 生∙ poly(k∕e)).
The approximation guarantee follows from Eq. (A.8) and the fact that S2 and R2 are respectively
affine β-embedding matrices of AR> and SA (see Lemma A.2).
Lemma A.9 (Avron et al. (2016); Lemma 25). Suppose that A ∈ Rn×d and B ∈ Rn×d0. More-
over, let S be an oblivious sparse affine -embedding matrix (i.e., a random CS) with (rank(A)2/2)
rows. Then with constant probability, X = arg minrank-kχ ∣∣SAX 一 SB kF, satisfies
IlAX - B IlF ≤(I+e) raminXkAX - BkF.
In other words, in O(nnz(A) + nnz(B)) + (d+ d0)(rank(A)2/2) time, we can reduce the problem
to a smaller (multi-response regression) problem with (rank(A)2/2 ) rows whose optimal solution
is a (1 + )-approximate solution to the original problem.
Proof of Theorem 4.3. Let SO and RO be CountSketch matrices of size poly(k/) × n and
poly(k/) × d. Note that since rank(Ak) = k and rank((SO A)> ) ≤ poly(k/), SO and RO are
respectively affine -embedding matrices of (Ak, A) and ((SOA)> , A> ). Then, by an application
of Lemma A.6
min ∣∣ar>xsoa 一 a∣∣F ≤(1 + oq |山 一 AkF
rank-k X
(A.9)
Similarly, by the assumption that SL and RL finds a (1 + γ)-approximate solution of LRA over
matrices A ∈ Atest , then
min IAR>XSlA 一 A∣∣F ≤ (1 + O(γ)) kAk — AkF
rank-k X
(A.10)
Next, let (PL, QL) and (PO, QO) be respectively the rank-k approximations of A in factored form
using (SL, RL) and (SO, RO) (see Algorithm 5). Then, Eq. (A.10) together with Eq. (A.9) implies
that
min(kPLQL 一 Ak2F , kPOQO 一 Ak2F) = (1 + O(min(, γ))) kAk 一 Ak2F	(A.11)
Hence, it only remains to compute the minimum of kPLQL 一 Ak2F and kPOQO 一 Ak2F efficiently
and we proceed similarly to the proof of Theorem 4.1. We use two applications of Lemma A.3
with Rτ ∈ RO(β2)×d,S ∈ RO(β2)×n. Let Γl = PLQL 一 A, Γo = POQO 一 A and Γm =
argmin(kSΓLRkF , kSΓORkF). Hence,
kΓM k2F ≤	(1 +	O(β)) kS ΓM Rk2F	B	by Lemma A.3
≤	(1 +	O(β)) ∙ min(kΓLkF，k「。kF)	B	byLemmaA.3
≤	(1 +	O(β + min(,γ))) kAk 一 Ak2F	B	by Eq. (A.11)
14
Under review as a conference paper at ICLR 2021
Runtime analysis. By Lemma A.8, Algorithm 5 computes PL , QL and PO , QO in O(nnz(A) +
(n + d) poiy(k) + β ∙ poiy(k)).
Next, it takes O(nnz(A) + (n + d) ∙ k + 备)to compute Δl and ∆o. As an example, We bound
the amount of time required to compute SPLQLR - SAR corresponding to ∆L . Since S and R
are sparse sketching matrices, SPL, QLR and SAR can be computed in nnz(SPL) + nnz(QLR) +
nnz(A). Since SPL and QLR are respectively of size 表 Xk and k X 表,in total it takes O(nnz(A) +
含)to compute these three matrices. Then, we can compute SPLQLR and IISPLQLRSARkF in
time O(备).
Hence, the total runtime of Algorithm 3 for LRA is O(nnz(A) + (n + d) ∙ poly( ∣) + 号∙ poly( ∙∣ )).□
A.2 CORRESPONDING TO SECTION 4.3: k-MEANS
We restate notation and the main result below for ease of reference.
Notation. We define Am as the optimal rank-m approximation of A formed by truncated SVD:
Am = UmΣmVm> .
Given a matrix U with orthogonal columns, let πU (A) = AUU>, which is the projection of the
rows of A onto col (U). Let CU be the optimal k-means partition of ∏u (A). Further, we let μcu,i
denote the i-th cluster’s center in the optimal k-means clustering on πU (A).
We denote dist2(A, μ) as the k-means loss given cluster centers (and their corresponding partition):
diSt2(A,μ)= XX IAj-μik2
i∈[k] j∈Ci
Likewise, cost(C) is the k-means loss given a partition:
cost(C) = X min X kAj - μik2
μi
i∈[k]	j∈Ci
Definition A.10 (Projection-cost preserving sketch). A is a projection-cost preserving sketch of
A if for any low rank projection matrix P and c not dependent on P:
(1 - e) kA - PAkF ≤ IlA - PA∣∣2 + C ≤ (1 + e) kA - PAkF
Theorem (4.6: Sketch monotonicity property for k-means). Assume we have A ∈ Rn×d. We
also have random CountSketch S ∈ RO(Poly(k/C))×n and define U ∈ Rd×O(Poly(k/C)) With orthog-
onal columns such that colsp(U) = rowsp(SA). Then, any extension of S to S0 (for example,
concatenation with a learned CountSketch SL) yields a better approximate k-means approximation.
Specifically, define W with orthogonal columns such that Col(W) = row(S0A). Let C* denote
the optimal partition of A, CU denote the optimal partition of πU (A), and CW denote the optimal
partition of πW (A). Then
cost(CW ) ≤ (1 + O())cost(CU) ≤ (1 + O())cost(C*)
Proof:
cost(CW) = X min X kAj - μik2
μi
i∈[k]	j∈CW,i
≤ X X IlAj-μcw,i Il2
i∈[k] j∈CW,i
=X X kAj-∏w(Aj)k2 + ∣∣∏W(Aj)-μcw,i∣∣2	(A.12)
i∈[k] j∈CW,i
15
Under review as a conference paper at ICLR 2021
≤ XX IIAj-πw(Aj)k2 + llπw(Aj)- μcu,i∣∣2	(A.13)
i∈[k] j∈CU,i
=XX l|Aj-μcu,i∣∣2	(A.14)
i∈[k] j∈CU,i
≤ (1 + e) Xmin X l∣Aj — 〃i『	(A.15)
・ i μi .一「，
i∈k	j∈CU,i
= (1 + )cost(CU )
≤ (1 + O(E))Cost(C*)	□
(A.12): μcw ∈ ColsP(W) so We can apply the Pythagorean Theorem.
(A.13): (μcw, CW) is an optimal k-means clustering of the projected points ∏w (A).
(A.14): μcu ∈ ColsP(U) ⊂ ColsP(W), so we can apply the Pythagorean Theorem.
(A.15): We apply Corollary A.14.
Remark A.11. Our result shows that the “sketch monotonicity” property holds for sketching ma-
trices that provide strong coresets for k-means clustering. Besides strong coresets, an alternate
approach to showing that the clustering objective is approximately preserved on sketched inputs is
to show a weaker property: the clustering cost is preserved for all possible partitions of the points
into k groups Makarychev et al. (2019). While the dimension reduction mappings satisfying strong
coresets require Poly(k/E) dimensions, Makarychev et al. (2019) shows that O(log k/E2) dimen-
sions suffice to satisfy this “partition” guarantee. An interesting question for further research is if
the sketch monotonicity guarantee also applies to the construction of Makarychev et al. (2019).
Corollary A.12. Assume we have A ∈ Rn×d, j ∈ Z+, E > 0. Define m = min(O(Poly(j /E)), d).
Let Am be a (1 + e) -approximation to Am of the form √4m = AVV> where SA = UΣV> for
CountSketch S ∈ Rm×n. Let X ∈ Rd×j be a matrix whose columns are orthonormal, and let
Y ∈ Rd×(d-j) be a matrix with orthonormal columns that spans the orthogonal complement of
colsp(X). Then
∣∣AXX> - .4mXX>∣∣2 ≤ E∙kAYkF.
Proof:
∣∣AXX > — .4mXX > ∣j = ∣∣A(I - VV >)XX >∣∣F
≤j∣∣A(I-VV>)XX>∣∣22	(A.16)
≤j∣∣A(I-VV>)∣∣22	(A.17)
≤ j ∙ O( j) kA - AjkF	(A.18)
min(n,d)
= O(j) X σi2	(A.19)
i=j
≤ O(j) kAY k2F	(A.20)
□
(A.16) Note that rank(X) = j, so rank(A(I - VV>)XX>) = j. We use this fact to bound the
Frobenius norm by the operator norm.
(A.17) Using the fact that XX> is a projection.
(A.18) Using Lemma 18 from Cohen et al. (2015), where Aj is the optimal rank-j approximation to
A. We can apply this lemma because CountSketch is one of the eligible types of random projection
matrices.
(A.19) Letting σi be the singular values ofA.
(A.20) Pim=inj (n,d) σi2 = minY kAY k2F for Y ∈ Rd×(d-j) with orthonormal columns.
16
Under review as a conference paper at ICLR 2021
Theorem A.13. Assume we have A ∈ Rn×d, j ∈ Z+,	∈ (0, 1]. Define m =
min(O(poly(j/)), d). Let Am be a (1 + )-approximation to Am of the form Am = AV V >
wAβrβ SA = UΣV> for CountSketch S ∈ Rm×n. Then, for any non-empty set μ contained in a
j -dimensional subspace, we have:
dist2(Am, μ) + IAm — AlF — dist2(A, μ)
≤ C dist2(A, μ)
Proof: We follow the proof of Theorem 22 in Feldman et al. (2013), but substitute different analyses
in place of Corollaries 16 and 20. The result of Feldman et al. (2013) involves the best rank-m
approximation of A, Am,; We will show it for the approximate rank-m approximation, Am,.
Define X ∈ Rd×j with orthonormal columns such that ColsP(X) = span(μ). Likewise, define
Y ∈ Rd×(d-j) with orthonormal columns such that ColsP(Y) = span(μ)⊥. By the Pythagorean
theorem:
dist2(Am,μ) = IlAmY IlF + dist2(AmXXT ,μ)
and
dist2(A, μ) = ∣∣AY kF +dist2(AXXT ,μ).	(A.21)
Hence,
J (dist2 (Am, μ) + ||A - Am(F) - dist2(A, μ)
= IIlAmY IlF + dist2 (AmXX T, μ) + ||A - AmIIF - (IlAY IlF + dist2 (AXX T, μ))
≤∣∣AmYkF + IA - AmkF -IAYIF∣ + ∣dist2(AmXXT,μ) - dist2(AXXT,μ)∣	(A.22)
2
≤飞∙ ∣∣AY∣∣F + ∣dist2(AmXXT,μ) — dist2(AXXT,μ)∣	(A.23)
≤ε- ∙ dist2(A, μ) + ∣dist2(AmXXT, μ) 一 dist2(AXXT, μ)∣	B Used (A.21)	(A.24)
(A.22) Triangle inequality.
(A.23) Take C in Theorem 16 from Cohen et al. (2015) as c2∕8. This theorem implies that Am is
a projection-cost preserving sketch with the C term as ∣∣A 一 Am,∣∣ . Specifically, it says AV is a
project-cost preserving sketch, which means Am = AVV> is too: V has orthonormal columns so
I(I-P)AVI2F= ∣∣(I-P)AVV>∣∣2F.
By Corollary A.12,
22
∣∣AmXXT - AXXTIIF ≤ W ∙IAYIF.
Since μ ∈ rowsp(X), we have ∣∣AY∣∣F ≤ dist2(A, μ). Using Corollary 21 from Feldman et al.
(2013) while taking C as c∕4, A as AmXXT, and B as AXXT yields
| dist2(AmXXT ,μ)-dist2(AXX T ,μ)∣ ≤ C ∙dist2(AXX T, μ)+(1+4)∙∣∣ AmXXT - AXX t∣∣J
By A.21, dist2(AXXT,μ) ≤ dist2(A, μ). Finally, we combining the last two inequalities
with (A.24):
∣ (dist2 (Am, μ) + ∣∣A - AmIIF) - dist2(A, μ)
≤ — ∙ dist2 (A, μ) + — ∙ dist2 (A, μ)+-~— ∙ (1 + —) ∙ dist2 (A, μ)
8	4	8C
≤ c ∙ dist2(A, μ),
where in the last inequality we used the assumption C ≤ 1.

17
Under review as a conference paper at ICLR 2021
Corollary A.14. Assume we have A ∈ Rn×d and CountSketch S ∈ RO(Poly韭/'"×n, Then, define
U ∈ Rd×O(poly(k/)) with orthogonal columns spanning row(SA). Also define AU = πU (A) and
μu as the set of optimal cluster centers found on AU. Now, assume E ∈ (0,1/3]. Then, μu is a
(1 + e)-approximation to the optimal k-means clustering of A. That is, defining μU as the cluster
centers which minimize the cost of partition CU on A, we have:
dist2(A, μU) ≤ (1 + E) dist2(A, μU)
Proof: By using f in Theorem A.13 with j as k,
∣dist2(AU, μU) + Il A — AUllF — dist2(A, μU) ∣ ≤ 3 dist2(A, μU)
which implies that
(I - 3) dist2 (A, μU) ≤ dist2 (AU, μU) + 11A - AUIl F	(A.25)
Likewise, by Theorem A.13 on AU and μU (and taking j as k),
∣dist2(AU, μU) + ||a - AUIIF - dist2(A, μU) ∣ ≤ 3 dist2(A, μU)
which implies that
dist2(AU,μU)+∣∣A - AU∣∣F ≤ (1 + 3E)dist2(A,μU)	(A.26)
By (A.25) and (A.26) together, we have:
(I- 3) dist2(A, μU) ≤ dist2(AU,μU) + ∣∣a - AU∣∣f
≤ dist2 (AU ,μU) + ∣∣a - au∣∣f
≤(1 + 3)dist2(A, μU)
Now, j+；/3 ≤ 1 + e, so we have dist2(A, μU) ≤ (1 + E) dist2(A, μU).	□
B	S pectral Norm Guarantee for Zipfian Matrices
In this section, we show that if the singular values of the input matrix A follow a Zipfian distribution
(i.e., σi α i-α for a constant α), we can find a (1 + E) rank-k approximation of A with respect to
the spectral norm. A key theorem in this section is the following.
Theorem B.1 (Cohen et al. (2015), Theorem 27). Given an input matrix A ∈ Rn×d, there exists
an algorithm that runs in O(nnz(A)+(n+d) poly(k/E)) and returns a projection matrix P = QQ>
such that with constant probability the following holds:
kAP - Ak2 ≤ (1 + e) IlA - Akk2 + O(E) kA - AkIlF	(B.1)
k
Next, we prove the main claim in this section. There are various ways to prove this, using, for
example, a bound on the stable rank of A; we give the following short proof for completeness.
Theorem B.2. Given a matrix A ∈ Rn×d whose singular values follow a Zipfian distribution (i.e.,
σi H i-α) with a constant α ≥ 1/2, there exists an algorithm that computes a rank-k matrix B (in
factored form) such that IA - BI22 ≤ (1 + E) IA - AkI22.
Proof: Note that since the singular values of A follow a Zipfian distribution with parameter α, for
any value of k,
rank(A)	rank(A)
kA - AkkF = x σ = c ∙ x i-2ɑ
i=k+1	i=k+1
≤ C ∙ Z"A) x-2α dx
k
B σi
18
Under review as a conference paper at ICLR 2021
≤ k ∙	∙ (1 + 1)2α ∙C∕(k + 1)2α
2α - 1 k
=O(k ∙ σ2 +1)
=O(k ∙kA — Akk2)	(B.2)
By an application of Theorem B.1, we can compute a matrix B in a factored form in time
O(nnz(A) + (n + d) poly(k∕)) such that with constant probability,
kB - Ak2 ≤ (1 + C) IlA - Akk2 + O(y) IlA - AkIlF	B ByEq.(B.1)
k
≤ (1 + O(C)) IA - Ak I2	B Eq. (B.2)
C Greedy Initialization
In this section, we analyze the performance of the greedy algorithm on the two distributions men-
tioned in Theorem 4.5.
Preliminaries and Notation. Left-multiplying A by CountSketch S ∈ Rm×n is equivalent to
hashing the rows of A to m bins with coefficients in {-1, 1}. The greedy algorithm proceeds
through the rows of A (in some order) and decides which bin to hash to, denoting this by adding an
entry to S. We will denote the bins as bi and their summed contents as wi .
C.1 Spiked covariance model with sparse left singular vectors .
To recap, every matrix A ∈ Rn×d from the distribution Asp(s, `) has s < k “heavy” rows
(An, •…，Ars) of norm ' > 1. The indices of the heavy rows can be arbitrary, but must be the
same for all members of the distribution and are unknown to the algorithm. The remaining rows
(called “light” rows) have unit norm.
In other words: let R = {r1, . . . , rs}. For all rows Ai, i ∈ [n]:
A _ ʃ ' ∙ Vi if i ∈ R
i = vi o.w.
where vi is a uniformly random unit vector.
We also assume that Sr, Sg ∈ Rk×n and non-increasing row norm ordering for the greedy algorithm.
Proof sketch. First, we show that the greedy algorithm using a non-increasing row norm ordering
will isolate heavy rows (i.e., each is alone in a bin). Then, we conclude by showing that this yields
a better k-rank approximation error when d is sufficiently large compared to n. We begin with some
preliminary observations that will be of use later.
It is well known that a set of uniformly random vectors are C-almost orthogonal (i.e., the magnitudes
of their pairwise inner products are at most C).
Observation C.1. Let vι,…,Vn ∈ Rd be a set ofrandom unit vectors. Then with high probability
∣hvi,Vj i∣ ≤ 2∕0F, ∀ i<j≤ n.
Wedefine C = 2 ʌ/ɪodn.
Observation C.2. Let uι, ∙∙∙ , ut be a set of vectors such that for each pair of i < j ≤ t,
KkUuik, kujki| ≤ G andgi,…,gj ∈ {-1,1}. Then,
t
XIuiI22-2GX IuiI2IujI2 ≤
i=1	i<j≤t
t	2 t
giui	≤ IuiI22	+	2G	IuiI2IujI2
i=1	2	i=1	i<j≤t
(C.1)
Next, a straightforward consequence of G-almost orthogonality is that we can find a QR-factorization
of the matrix of such vectors where R (an upper diagonal matrix) has diagonal entries close to 1 and
entries above the diagonal are close to 0.
19
Under review as a conference paper at ICLR 2021
Lemma C.3. Let uι, ∙∙∙ , ut ∈ Rd be a set of unit vectors such that for any pair of i < j ≤ t,
|〈ui, Uji∣ ≤ E where E = O(t-2). There exists an orthonormal basis eι, ∙∙∙ , et for the subspace
spanned by uι, •…，Ut such that for each i ≤ t, Ui = Pj=ι aijej where α2,i ≥ 1 一 Pj=I j2 ∙ e2
and for each j < i, ai2,j ≤ j2 E2.
Proof: We follow the Gram-Schmidt process to construct the orthonormal basis eι,…,et of the
space spanned by uι, ∙∙∙ , ut. by first setting eι = uι and then processing u2, ∙∙∙ ,ut, one by one.
The proof is by induction. We show that once the first j vectors uι,…，uj are processed the
statement of the lemma holds for these vectors. Note that the base case of the induction trivially
holds as u 1 = e 1. Next, suppose that the induction hypothesis holds for the first' vectors u 1,… ,u`.
Claim C.4. For each j ≤ ', α2+ι j ≤ j2E2.
Proof: The proof of the claim is itself by induction. Note that, for j = 1 and using the fact that
∣huι, u'+ιi∣ ≤ e, the statement holds and O2+ι 1 ≤ e2. Next, suppose that the statement holds for
all j ≤ i < ', then by |〈@+1, u'+Q∣ ≤ e,
i
la'+1,i+1∣ ≤ (IhU'+1,ui+1∣ + E la'+1,j | ∙ lai+1,j |)/|ai+1,i+1|
j=1
i
≤ (E + Ej2E2)∕∣ai+1,i+1∣	B by induction hypothesis on a'+1,j for j ≤ i
j=1
ii
≤ (E+	j2E2)/(1 一 Ej2 ∙ E2)1/2	B by induction hypothesis on ai+1,i+1
j=1	j=1
ii	i
≤ (E + X j2E2) ∙ (1 - X j2 ∙ E2)1/2 ∙ (1+2 ∙ X j2E2)
ii
≤ (E + X j2E2) ∙ (1 + 2 ∙ X j2E2)
j=1	j=1
≤ E((Xj2E) ∙ (1 + 4e ∙ Xj2E) + 1)
j=1	j=1
≤ E(i+1)	BbyE = O(t-2)
Finally, since ∣∣u'+1∣∣2 = 1, a2+1,'+1 ≥ 1 ― p'=1 j2E2.	□
Corollary C.5. Suppose that E = O(t-2). There exists an orthonormal basis e1, ∙∙∙ , et for the
space spanned by the randomly picked vectors v1, ∙∙∙ , vt, of unit norm, so that for each i, Vi =
Pj=I ai,j ej- where a2,i ≥ 1 一 Pj=I j2 ∙ E2 and for each j < i, a2,j- ≤ j2 ∙ E2.
Proof: The proof follows from Lemma C.3 and the fact that the set of vectors v1, ∙∙∙ , vt are E-almost
orthogonal (by Observation C.1).	□
The first main step is to show that the greedy algorithm (with non-increasing row norm ordering)
will isolate rows into their own bins until all bins are filled. In particular, this means that the heavy
rows (the first to be processed) will all be isolated.
We note that because we set rank(SA) = k, the k-rank approximation cost is the simplified ex-
pression AV V > 一 A2F, where UΣV > = SA, rather than [AV]kV > 一 A2F. This is just the
projection cost onto row(SA). Also, we observe that minimizing this projection cost is the same as
maximizing the sum of squared projection coefficients:
min ∣∣A — AVV>∣∣F 〜min X kAi ― (〈Ai, v1iv1 + ... +〈Ai, VkXiv)k2
i∈[n]
〜mSn X (Mik2 - X hAi,vj i2)
i∈[n]	j∈[k]
20
Under review as a conference paper at ICLR 2021
〜max
S
hAi,vji* 2 *
i∈[n] j∈[k]
In the following sections, we will prove that our greedy algorithm makes certain choices by showing
that these choices maximize the sum of squared projection coefficients.
Lemma C.6. For any matrix A or batch of matrices A, at the end of iteration k, the learned CountS-
ketch matrix S maps each row to an isolated bin. In particular, heavy rows are mapped to isolated
bins.
Proof: For any iteration i ≤ k, we consider the choice of assigning Ai to an empty bin versus an
occupied bin. Without loss of generality, let this occupied bin be bi-1, which already contains Ai-1.
We consider the difference in cost for empty versus occupied. We will do this cost comparison for
Aj with j ≤ i - 2, j ≥ i + 1, and finally, j ∈ {i - 1, i}.
First, we let {e1, . . . , ei} be an orthonormal basis for {A1, . . . , Ai} such that for each r ≤ i, Ar =
pr=ι ar,jej where ar,r > 0. This exists by Lemma C.3. Let {eι,..., e「2, e} be an orthonormal
basis for {Aι,..., Ai+2, Ai-ι ± Ai}. Now, e = c0ei-ι + cιei for some co, ci because (Ai-ι ±
Ai) - proj{e1,...,e.-2}(Ai-1 土 Ai) ∈ span(ei-ι,ei). We note that Cq + c2 = 1 because we let e be
a unit vector. We can find c0 , c1 to be:
c0
ai-1,i-1 + ai,i-1______
(ai-1,i-1 + ai,i-1)q + aiq,i
c1
ai,i
(ai-1,i-1 + ai,i-1)q + aiq,i
1.	j ≤ i - 2: The cost is zero for both cases because Aj ∈ span({e1, . . . , ei-q}).
2.	j ≥ i + 1: We compare the rewards (sum of squared projection coefficients) and find that
{ei,..., ei-2, e} is no better than {ei,..., ei}.
hAj, ei2 = (co hAj, ei-ii + CIhAj, eii)2
≤ (ciq + c0q)(hAj, ei-iiq + hAj, eiiq)	B Cauchy-Schwarz inequality
= hAj, ei-iiq + hAj, eiiq
3.	j ∈ {i - 1, i}: We compute the sum of squared projection coefficients of Ai-i and Ai onto
e:
((ai-i,i-i + L-I)Q + a2,i) ∙(aQ-1，i-1(aiT，iT + ai,i-1)2
+ (ai,i-i (ai-i,i-i + ai,i-i) + ai,iai,i) )
= ((ai-i,i-i + [i-i)2 + aQ,i) ∙ ((a"1，"1 + ai，i-1)2(aQ-1，i-1	(CZ
+ aiQ，i-i) + ai4，i + 2ai，i-iaiQ，i(ai-i，i-i + ai，i-i))	(C.3)
On the other hand, the sum of squared projection coefficients of Ai-i and Ai onto ei-i ∪ei
is:
(
(ai-1，i-1 + ai，i-1)2 + aQ，i
(ai-i，i-i + ai，i-i)Q + ai，i
)，(aQ-1，i-1 + aQ，i-1
+ aiQ，i)
(C.4)
Hence, the difference between the sum of squared projections of Ai-ι and Ai onto e and
ei-i ∪ ei is ((C.4) - (C.3))
aj，i((ai-1，i-1 + ai，i-1)2 + aQ-1，i-1 + aQ，i-1 - 2ai，i-1(ai-1，i-1 + ai，i-1))
((ai-1，i-1 + ai，i-1)j + aj，i)
2aj，iaj-1，i-1	> 0
((ai-1，i-1 + ai，i-1) 2 + aj，i )
Thus, we find that {e1,...,ei} is a strictly better basis than {e1,..., ei-2, e}. This means the greedy
algorithm will choose to place Ai in an empty bin.
21
Under review as a conference paper at ICLR 2021
Next, we show that none of the rows left to be processed (all light rows) will be assigned to the same
bin as a heavy row. The main proof idea is to compare the cost of “colliding” with a heavy row to
the cost of “avoiding” the heavy rows. Specifically, we compare the decrease (before and after bin
assignment of a light row) in the sum of squared projection coefficients, lower-bounding it in the
former case and upper-bounding it in the latter.
We introduce some results that will be used in Lemma C.10.
Claim C.7. Let Ak+r, r ∈ [1, . . . , n - k] be a light row not yet processed by the greedy algorithm.
Let {e1 , . . . , ek} be the Gram-Schmidt basis for the current {w1 , . . . , wk}. Let β = O(n-1 k-3)
upper bound the inner products of normalized Ak+r, w1, . . . , wk. Then, for any bin i, hei, Ak+r i2 ≤
β2 ∙ k2.
Proof: This is a straightforward application of Lemma C.3. From that, we have hAk+r, eii2 ≤ i2β2,
for i ∈ [1, . . . , k], which means hAk+r, eii2 ≤ k2β2.
Claim C.8. Let Ak+r be a light row that has been processed by the greedy algorithm. Let
{e1, . . . , ek} be the Gram-Schmidt basis for the current {w1, . . . , wk}. If Ak+r is assigned to bin
bk-ι (w.l.o.g.), the squared projection coefficient of Ak+r onto ei,i = k 一 1 is at most 4β2 ∙ k2,
where β = O(n-1k-3) upper bounds the inner products of normalized Ak+r, wι, ∙∙∙ , Wk.
Proof: Without loss of generality, it suffices to bound the squared projection of Ak+r onto the
direction of Wk that is orthogonal to the subspace spanned by wι,…，Wk-ι. Let eι,…，ek be
an orthonormal basis of wι,…，Wk guaranteed by Lemma C.3. Next, we expand the orthonormal
basis to include ek+1 so that we can write the normalized vector of Ak+r as vk+r = Pjk=+11 bj ej .
By a similar approach to the proof of Lemma C.3, for each j ≤ k 一 2, bj ≤ β2j2. Next, since
|hWk, vk+ri| ≤ β,
|bk| ≤
1
|〈Wk, ekil
k-1
• (KWk,vk+ri| + E |bj • hwk,ej i|)
j=1
1
≤
,1- Pk-I β2 . j
k-2
(β + X β2 • j2 + (k - 1) ∙ β)
j=1
B |bk-1| ≤ 1
J-Pk-I β2
+(k-1)β
≤2(k-1)β-
• j2
β2(k - 1)2
,1一 Pk-I β2 . j
B similar to the proof of Lemma C.3
< 2β • k
Hence, the squared projection of Ak+r onto ek is at most 4β2 •k2 • kAk+r k22. We assumed kAk+r k.
Claim C.9. We assume that the absolute values of the inner products of vectors in v1 , • • • , vn are
at most e < 1∕(n2 EAa ∈b l∣Aik2) and the absolute values of the inner products of the normalized
vectors of wι, ∙•∙ ,Wk are at most β = O(n-3k~3). Suppose that bin b contains the row Ak+r.
Then, the squared projection of Ak+r onto the direction ofW orthogonal to span({W1, • • • , Wk} \
ik k+ k++rk+	"Ak+r"2	_i_ --(--2)	CHa / s∕7k I++1k+	"Ak+r "2	_ --2--2)
{W}) Cs at Imost	11 112	I (n (n )	anu Cs at e^cs>y ι 11 112	(n ( n ).
kwk2	kwk2
Proof: Without loss of generality, we assume that Ak+r is mapped to bk; W = Wk. First, we provide
an upper and a lower bound for |hvk+r, Wk)| where for each i ≤ k, we let Wi
⅛ denote the
normalized vector of Wi . Recall that by definition vk+r
Ak + r
k Ak + r k 2
∣hWk, Vk+ri∣ ≤
≤
∣Αk+rk2 + EAi∈bk W IIAiIl2
llWkk2
IlAk+rk2 + n-2
kWk ∣∣2
kAk+r∣∣2 + -2
记噎十n
2
一 _ .	n 2
B y	PAi∈bk ∣Aik2
BIWkI2 ≥ 1	(C.5)
≤
22
Under review as a conference paper at ICLR 2021
∣hWk,Vk+ri∣ ≥
kAk+rk2- EAi∈bkkAik2 ∙忑
kwkk2
≥ kAk+rk2 _ n-2
一kwk k2 一
(C.6)
Now, let {eι,…，ek} be an orthonormal basis for the subspace spanned by {wι,…，wk} guaran-
teed by Lemma C.3. Next, we expand the orthonormal basis to include ek+1 so that we can write
vk+r = Pjk=+11 bj ej . By a similar approach to the proof of Lemma C.3, we can show that for each
j ≤ k - 1, bj2 ≤ β2j2. Moreover,
1	k-1
|bk | ≤ ∣(Wk ek i∣ ∙ (lhwk,vk+r i| +ɪ^ |bj ∙ hwk ,ejil)
J- Pk-I β2
1
J1 - PM β2
k-1
=∙ (IhWk ,Vk+r i∣ + X β2 ∙ j2)
•	j	j=1
=2 + kAk+⅛ + Xβ2 • j2)
•	j2	kwkk2	j=1
<	β • k +	1	• (n-2 + kAk+rk2 )
Pl - β2k3	+ kWk 112
≤ O(n-2) + (1 + O(n-2)) kA：+；k2
kWk k2
and,
|bk| ≥
1	k-1
IhW eki∣ YKwk,vk+ri| -^lbj • hwk,eji|)
k-1
IhWk ,Vk+r iI - X β2 • j
j=1
kAk+rk2
kwk k2
kAk+rk2
kwk k2
- n-2
k-1
X β ∙ j
j=1
B by Lemma C.3
B by (C.5)
B similar to the proof of Lemma C.3
B by β = O(n-3k-2)
B kAk+rk2 ≤ 1
kwk k2 -	I
B since ∣hWk,eki∣ ≤ 1
B by (C.6)
B by β = O(n-3k-2)
≤
≤
1
：…)
≥
≥
≥
—
—
Hence, the squared projection of Ak+r onto ek is at most kAk+rk2 +O(n-2) and is at least kAk+rk2 -
k+r	k	kwkk22	kwkk22
O(n-2).
Now, we show that at the end of the algorithm no light row will be assigned to the bins that contain
heavy rows.
Lemma C.10. We assume that the absolute values of the inner products of vectors in v1, • • • , vn
are at most e < min{n-2k-5, (n PAa∈w ∣∣Aik2)-1}. At each iteration k + r, the greedy algorithm
will assign the light row Ak+r to a bin that does not contain a heavy row.
Proof: The proof is by induction. Lemma C.6 implies that no light row has been mapped to a bin
that contains a heavy row for the first k iterations. Next, we assume that this holds for the first
k + r - 1 iterations and show that is also must hold for the (k + r)-th iteration.
To this end, we compare the sum of squared projection coefficients when Ak+r avoids and collides
with a heavy row.
23
Under review as a conference paper at ICLR 2021
First, we upper bound β = maxi6=j≤k |hwi, wji|/(kwik2 kwj k2). Let ci and cj respectively denote
the number of rows assigned to bi and bj .
|hwi, wji|
β = max ∏一U J U
i6=j≤k kwi k2 kwj k2
≤ 16^√Ci亏
≤ n-1k-3
B Observation C.2
Be ≤ n-2k-5/3
Be ≤ n-2k-5/3
1. If Ak+r is assigned to a bin that contains c light rows and no heavy rows. In this case, the
projection loss of the heavy rows Ai, ∙∙∙ , As onto row(SA) remains zero. Thus, We only need to
bound the change in the sum of squared projection coefficients of the light rows before and after
iteration k + r.
Without loss of generality, let wk denote the bin that contains Ak+r . Since Sk-1 =
span({wι,…，wk-ι}) has not changed, we only need to bound the difference in cost between
projecting onto the component ofwk - Ak+r orthogonal to Sk-1 and the component ofwk orthog-
onal to Sk-ι, respectively denoted as ek and ek.
I.	By Claim C.7, for the light rows that are not yet processed (i.e., Aj forj > k + r), the squared
projection of each onto ek is at most β2k2. Hence, the total decrease in the squared projection
is at most (n 一 k 一 r) ∙ β2k2.
II.	By Claim C.8, for the processed light rows that are not mapped to the last bin, the squared
projection of each onto ek is at most 4β2k2. Hence, the total decrease in the squared projection
cost is at most (r 一 1) ∙ 4β2k2.
III.	For each row Ai 6= Ak+r that is mapped to the last bin, by Claim C.9 and the fact kAi k24 =
kAi k22 = 1, the squared projection of Ai onto ek is at most kw"kk2 一 k2 + O(n-2) and the
2	kwk -Ak+r k2
squared projection of Ai onto e.k is at least 1&* 一 O(n-2).
Moreover, the squared projection of Ak+r onto ek compared to e increases by at least
(kAk+rk2 一 O(n-2)) 一 O(n-2) = kAk+rk2 一 O(n-2).
kwkk22	n	n	kwkk22	n .
Hence, the total squared projection of the rows in the bin bk decreases by at least:
(X	H kAAl2 h2 + O(n-2)) -( X 阵 - O(n-2))
Ai∈Wk/{Ar+k} kwk - Ar+k k2	Ai∈Wk kwkk2
≤kwk 一 Ar+kk2 + O(n-1) 一 kwkk2 - OsT) + O(n-i)
kwk 一 Ar + k k2	kwkk2
≤O(n-1)
B by Observation C.2
Hence, summing up the bounds in items I to III above, the total decrease in the sum of squared
projection coefficients is at most O(n-1).
2.	If Ak+r is assigned to a bin that contains a heavy row. Without loss of generality, we can
assume that Ak+r is mapped to bk that contains the heavy row As. In this case, the distance of heavy
rows Ai,…，As-ι onto the space spanned by the rows of SA is zero. Next, we bound the amount
of change in the squared distance of As and light rows onto the space spanned by the rows of SA.
Note that the (k 一 1)-dimensional space corresponding to wi,…，wk-i has not changed. Hence,
We only need to bound the decrease in the projection distance of Ak+r onto ek compared to ek
(where ek, ek are defined similarly as in the last part).
1.	For the light rows other than Ak+r , the squared projection of each onto ek is at most
β2k2. Hence, the total increase in the squared projection of light rows onto ek is at most
(n 一 k) ∙ β2k2 = O(n-i).
24
Under review as a conference paper at ICLR 2021
2.	By Claim C.9, the sum of squared projections of As and Ak+r onto ek decreases by at least
kAsk22 -
( kAsk2 + kAk+rk4
kAs + Ar+kk2
+ O(n-1))
≥ kAsk22 -
( kAsk2 + kAk+rk2
kAsk2 + kAr+kk2 -n-O⑴
+ O(n-1))
B by Observation C.2
≥ kAr + k k； (kAsk2-kAk+rk2)-kAsk2 ∙ O(n-1 )
-	kAsk2 + kAr+kk2- O(n-1)
≥ kAr + k k2 (kAsk2 -kAk+rk2)-kAsk2 ∙ OS-1 )
-	kAsk2 + kAr+k k2
-	O(n-1)
-	O(n-1)
≥ kAr + k k2 (kAsk2-kAk+rk2) - O
-	kAsk2 + kAr + k k2
≥ kAr + k k2 (1-(kAk+rk2 / kAsk2))
-	1 + (kAr + k k2 / kAsk2 )
≥kAr+kk2 (1 -ɪnɪ) - O(n-1)
kAsk2
1 -a
1 + e2
≥1-
B
Hence, in this case, the total decrease in the squared projection is at least
kAr+kk； (1 - kAA+r^) - O(n-1) = 1 - kAA+k2) - O(n-1)	B kAr+kk2 = 1
kAs k2	kAs k2
=1 - (1∕√') - O(n-1)	B kAsk2 = √
Thus, for a sufficiently large value of `, the greedy algorithm will assign Ak+r to a bin that only
contains light rows. This completes the inductive proof and in particular implies that at the end of
the algorithm, heavy rows are assigned to isolated bins.
Corollary C.11. The approximation loss of the best rank-k approximate solution in the rowspace
Sg A for A 〜Αsp(s, ') where A ∈ Rn×d for d = Ω(n4k4 log n) and Sgis the CountSketch
constructed by the greedy algorithm with non-increasing order is at most n - s.
Proof: First, We need to show that absolute values of the inner products of vectors in v1, ∙∙∙ ,vn is
at most e < min{n-2k-2, (n PAa∈w kAik；)-1} SothatWecanaPPlyLemma C.10. To show this,
note that by Observation C.1, W ≤ 2 Jlo∣n ≤ n-2k-2 since d = Ω(n4k4 log n). The proof follows
from Lemma C.6 and Lemma C.10. Since all heavy rows are maPPed to isolated bins, the Projection
loss of the light rows is at most n - s.
Next, we bound the Frobenius norm error of the best rank-k-approximation solution constructed by
the standard CountSketch with a randomly chosen sparsity pattern.
Lemma C.12. Let s = αk where 0.7 < α < 1. The expected squared loss of the best rank-k
approximate solution in the rowspace Sr A for A ∈ Rn×d 〜 Αsp(s, ') where d = Ω(n6'2) and Sr
is the sparsity pattern ofCountSketch is chosen uniformly at random is at least n + 笠 一 (1 + ɑ)k 一
n-O(1)
n.
Proof: We can interpret the randomized construction of the CountSketch as a “balls and bins” ex-
periment. In particular, considering the heavy rows, we compute the expected number of bins (i.e.,
rows in SrA) that contain a heavy row. Note that the expected number of rows in SrA that do not
contain any heavy row is k ∙ (1 一 1 )s ≥ k ∙ e-k-1. Hence, the number of rows in Sr A that contain
ss
a heavy row of A is at most k(1 一 e-k-1). Thus, at least S 一 k(1 一 e-k-1) heavy rows are not
mapped to an isolated bin (i.e., they collide with some other heavy rows). Then, it is straightforward
to show that the squared loss of each such row is at least ` - n-O(1).
Claim C.13. Suppose that heavy rows A∏,…,Arc are mapped to the same bin via a CountSketch
S. Then, the total squared distances of these rows from the subspace spanned by SA is at least
(C - 1)' 一 O(n-1).
25
Under review as a conference paper at ICLR 2021
Proof: Let b denote the bin that contains the rows An, ∙∙∙ , Arc and suppose that it has c0 light rows
as well. Note that by Claim C.8 and Claim C.9, the squared projection of each row Ari onto the
subspace spanned by the k bins is at most
kAhkr …)
一 c' + C — 2e(c2' + Cd√' + c02)
≤_____'_____+ n-O(1)
~ — — n-O(1) 十
`2
≤F ∙(c' + O(n-1)+ O(n-1)
c2 `2
≤ ' + O(n-1)
c
+ O(n-1)
B by e ≤ n-3'-1
Hence, the total squared loss of these C heavy rows is at least c' — C ∙ (C + O(n-1)) ≥ (c — 1)' —
O(n-1).
Hence, the expected total squared loss of heavy rows is at least:
' ∙ (s — k(1 — e-k-1)) — S ∙ n-O⑴
≥' ∙ k(α — 1 + e-α) — 'α — n-O⑴
≥ 竺—' —n-O(1)
一 2e
≥ 'k - O(n-1)
4e
B s = a ∙ (k — 1) where 0.7 < α < 1
B α ≥ 0.7
B assuming k > 4e
Next, we compute a lower bound on the expected squared loss of the light rows. Note that Claim C.8
and Claim C.9 imply that when a light row collides with other rows, its contribution to the total
squared loss (which the loss accounts for the amount it decreases from the squared projection of the
other rows in the bin as well) is at least 1 — O(n-1). Hence, the expected total squared loss of the
light rows is at least:
(n — s — k)(1 — O(n-1)) ≥ (n — (1 + α) ∙ k) — O(n-1)
Hence, the expected squared loss of a CountSketch whose sparsity is picked at random is at least
---O(n-1) + n — (1 + α)k — O(n-1) ≥ n + / — (1 + α)k — O(n-1)

Corollary C.14. Let S = α(k — 1) where 0.7 < α < 1 and let ' ≥ (4e+k1)n. Let Sg be the
CountSketch whose sparsity pattern is learned over a training set drawn from Asp via the greedy
approach. Let Sr be a CountSketch whose sparsity pattern is picked uniformly at random. Then, for
an n X d matrix A 〜Asp where d = Ω(n6 '2), the expected loss of the best rank-k approximation
of A returned by Sr is worse than the approximation loss of the best rank-k approximation of A
returned by Sg by at least a constant factor.
Proof:
Esr [	min IlX - AllF] ≥ n + 'k — (1 + α)k — n-O⑴
rank-k X ∈rowsp(Sr A)	4e
≥ (1 + 1∕α)(n — s)
= (1 + 1∕α)	min	∣∣X - AkF
rank-k X ∈rowsp(Sg A)
□ I
B Lemma C.12
B ' ≥ (4e +1)n
αk
B Corollary C.11
26
Under review as a conference paper at ICLR 2021
C.2 Zipfian on s quared row norms.
Each matrix A ∈ Rn×d 〜Azipf has rows which are uniformly random and orthogonal. Each A has
2i+1 rows of squared norm n2 */22i for i ∈ [1, . . . , O(log(n))]. We also assume that each row has
the same squared norm for all members of Azipf.
In this section, the s rows with largest norm are called the heavy rows and the remaining are the light
rows. For convenience, we number the heavy rows 1 - s; however, the heavy rows can appear at
any indices, as long as any row of a given index has the same norm for all members of Azipf. Also,
we assume that s ≤ k/2 and, for simplicity, s = Pih=s 1 2i+1 for some hs ∈ Z+. That means the
minimum squared norm of a heavy row is n2 /22hs and the maximum squared norm of a light row
is n2/22hs+2.
The analysis of the greedy algorithm ordered by non-increasing row norms on this family of matrices
is similar to our analysis for the spiked covariance model. Here we analyze the case in which rows
are orthogonal. By continuity, if the rows are close enough to being orthogonal, all decisions made
by the greedy algorithm will be the same.
As a first step, by Lemma C.6, at the end of iteration k the first k rows are assigned to different bins.
Then, via a similar inductive proof, we show that none of the light rows are mapped to a bin that
contains one of the top s heavy rows.
Lemma C.15. At each iteration k + r, the greedy algorithm picks the position of the non-zero value
in the (k + r)-th column of the CountSketch matrix S so that the light row Ak+r is mapped to a bin
that does not contain any of top s heavy rows.
Proof: We prove the statement by induction. The base case r = 0 trivially holds as the first k rows
are assigned to distinct bins. Next we assume that in none of the first k + r - 1 iterations a light row
is assigned to a bin that contains a heavy row. Now, we consider the following cases:
1. If Ak+r is assigned to a bin that only contains light rows. Without loss of generality we can
assume that Ak+r is assigned to bk . Since the vectors are orthogonal, we only need to bound the
difference in the projection of Ak+r and the light rows that are assigned to bk onto the direction of
wk before and after adding Ak+r to bk . In this case, the total squared loss corresponding to rows in
bk and Ak+r before and after adding Ak+1 are respectively
before adding Ak+r to bk: kAk+rk22 +	kAj k22 -
Aj∈bk
after adding Ak+r to bk: kAk+rk22 +	kAj k22 -
Aj∈bk
(PAj ∈bkkAj k4)
(PAj ∈bkkAj k2)
( kAk+rk4 + PAj ∈bkkAj k4 )
( kAk+rk2 + PAj ∈bkkAj k2 )
Thus, the amount of increase in the squared loss is ( PAj∈bkkAj k4 ) - ( kAk+rk2 + PAj∈bkkAjk2 A~∈jb3 ∈bkEl)-( kAk+rk2 + PAj ∈bkkAjk2	kAk+rk2 ∙ PAj∈bk kAjk2 -kAk+rk2 ∙ PAj ∈b% kAj k2 )= -(PAj ∈bkkAjk2)(kAk+rk2 + PAj ∈bkkAjk2)- PAj∈bkkAjk4 _ kA	k2 k2	PAj ∈bk kAjk2-kAk+r k2 k+r 2 PAj ∈bk kAj k2 + kAk+r k2 ≤kA	k2 PAj∈bkMjk2 -kAk+rk2 -k+r 2 PAj ∈bk kAj k2 + kAk+r k2 (C.7)	I
2. If Ak+r is assigned to a bin that contains a heavy row. Without loss of generality and by the
induction hypothesis, we assume that Ak+r is assigned to a bin b that only contains a heavy row Aj .
Since the rows are orthogonal, we only need to bound the difference in the projection of Ak+r and
Aj In this case, the total squared loss corresponding to Aj and Ak+r before and after adding Ak+1
27
Under review as a conference paper at ICLR 2021
to b are respectively
before adding Ak+r to bk: kAk+r k22
after adding Ak+r to bk: kAk+r k2 + kAj k2 - ( kAk+r k2 + [A' [W )
kAk+r k2 + kAj k2
Thus, the amount of increase in the squared loss is
k A k2 _( kAk+rk2 + kAj k4 ) = k A k2	kAj k2 TAk+rk2
j 2	kAk+rk2 +	kAj k2	k+r	2	kAj	k2 + kAk+r	k2
(C.8)
Then (C.8) is larger than (C.7) if kAj k22 ≥ PA ∈b kAik22. Next, we show that at every inductive
iteration, there exists a bin b which only contains light rows and whose squared norm is smaller
than the squared norm of any heavy row. For each value m, define hm so that m = Pih=m1 2i+1 =
2hm+2 - 2.
2
Recall that all heavy rows have squared norm at least 2⅛s. There must be a bin b that only contains
light rows and has squared norm at most
kwk22 =	kAik22 ≤
Ai ∈b
2	Phn	2i+1n2
n + i=^=hι + 1	22i
22(hs + ι)	k - S
≤
≤
≤
<
2
n2
22(hs + 1)
2
n2
22(hs + 1)
2
n2
22hs + 1
kAsk22
2n2
+ 2hk(k - S)
2
n2
+ ——
22hk
B S ≤ k/2 and k > 2hk +1
B hk ≥ hs + 1
Hence, the greedy algorithm will map Ak+r to a bin that only contains light rows.
Corollary C.16. The squared loss of the best rank-k approximate solution in the rowspace ofSgA
for A ∈ Rn×d 〜Azipf where A ∈ Rn×d and Sg is the CountSketch constructed by the greedy
algorithm with non-increasing order is < Dhn_2.
2k
h	n2
Proof: At the end of iteration k, the total squared loss is Ei=h/^+1 2i+1 ∙ 2⅛. After that, in each
iteration k + r, by (C.7), the squared loss increases by at most kAk+r k22 . Hence, the total squared
loss in the solution returned by Sg is at most
hn	i+1 2	hn
2( X ⅛L )=4n2 ∙ X 1
i=hk +1 i=hk +1
4n2	n2
<	=
2hk	2hk -2

Next, we bound the squared loss of the best rank-k-approximate solution constructed by the standard
CountSketch with a randomly chosen sparsity pattern.
Observation C.17. Let us assume that the orthogonal rows An, •…，Arc are mapped to same bin
andfor each i ≤ c, kArι k2 ≥ kArik2. Then, the total squared loss of Ari ,… ，Arc after projecting
onto An 士…士 Arc is at least ∣∣Ar2 ∣∣2 + …+ kArc k2.
Proof: Note that since An, •…，Arc are orthogonal, for each i ≤ c, the squared projection of Ari
onto Ari ±∙∙∙± Arc is kA% k2 / Pc=IllArj 112. Hence, the sum of squared projection coefficients
of Ari,…，Arc onto A∏ 士…士 Arc is
Pc=1llArjll2
Pc=1llArjll2
≤ kAri k22
28
Under review as a conference paper at ICLR 2021
Hence, the total projection loss of An,…,Arc onto An ±∙∙∙± Arc is at least
c
X∣∣Ar∕∣2 -kArιk2 = kAr2 k2 + …+ kArck2 .
j=1

In particular, Observation C.17 implies that whenever two rows are mapped into a same bin, the
squared norm of the row with smaller norm fully contributes to the total squared loss of the solution.
Lemma C.18. For k > 210 - 2, the expected squared loss of the best rank-k approximate solution
in the rowspace of Sr A for An×d 〜Azipf, where Sr is the sparsity pattern ofa CountSketch Chosen
uniformly at random, is at least 端-2 .
Proof: In light of Observation C.17, we need to compute the expected number of collision between
rows with “large” norm. We can interpret the randomized construction of the CountSketch as a
“balls and bins” experiment.
2
For each 0 ≤ j ≤ hk, let Aj denote the set of rows with squared norm 22(n .)and let A>j =
Sj<i≤hk Ai. Note that for each j, |Aj| =2hk-j+1 and|A>j| = Pih=kj+1 2hk-i+1 = Pih=k1-j 2i =
2(2hk-j - 1). Moreover, note that k = 2(2hk+1 - 1). Next, for arow Ar in Aj (0 ≤ j < hk), we
compute the probability that at least one row in A>j collide with Ar .
Pr [at least one row in A>j collide with A" = (1 - (1 - 1)lA>j |)
|A>j|
≥ (1 - e--L)
2hk-j-1
=(1 - e-ET-I)
≥(I-e-2-j-)	B since 2hk-j-1 > 2-j-2
Hence, by Observation C.17, the contribution of rows in Aj to the total squared loss is at least
22
(1-e-2-j-) TAj∣∙ 22(hk-j) =(1 - e-2-j-) ∙ 2hkjΓ
2
=(1-e-2-j ) ∙ -r- ∙ 2j-1
2hk -2
Thus, the contribution of rows with “large” squared norm, i.e., A>0, to the total squared loss is at
least5
2	hk	2
∙ X 2j-1 ∙ (1 - e-2-j-2) ≥ 1.095 ∙
2hk -2	.	2hk-2
j=0
Bfor hk > 8

Corollary C.19. Let Sg be a CountSketch whose sparsity pattern is learned over a training set
drawn from Asp via the greedy approach. Let Sr be a CountSketch whose sparsity pattern is picked
uniformly at random. Then, for an n X d matrix A 〜 Azipf, for a sufficiently large value of k, the
expected loss of the best rank-k approximation ofA returned by Sr is worse than the approximation
loss of the best rank-k approximation of A returned by Sg by at least a constant factor.
Proof: The proof follows from Lemma C.18 and Corollary C.16.
Remark C.20. We have provided evidence that the greedy algorithm that examines the rows of A
according to a non-increasing order of their norms (i.e., greedy with non-increasing order) results in
a better rank-k solution compared to the CountSketch whose sparsity pattern is chosen at random.
However, still other implementations of the greedy algorithm may result in a better solution com-
pared to the greedy with non-increasing order. To give an example, in the following simple instance
the greedy algorithm that check the rows of A in a random order (i.e., greedy with random order)
achieves a rank-k solution whose cost is a constant factor better than the solution returned by the
greedy with non-increasing order.
5The numerical calculation is computed by WolframAlpha.
29
Under review as a conference paper at ICLR 2021
Let A be a matrix with four orthogonal rows u, u, v, w where kuk2 = 1 and kvk2 = kwk2 = 1 +
and suppose that the goal is to compute a rank-2 approximation of A. Note that in the greedy with
non-decreasing order, v and w will be assigned to different bins and by a simple calculation we can
show that the copies of u also will be assigned to different bins. Hence, the squared loss in the
computed rank-2 solution is 1 + z+；++". However, the optimal solution will assign V and W to
one bin and the two copies of u to the other bin which results in the squared loss of (1 + )2 which
is a constant factor smaller than the solution returned by the greedy with non-increasing order for
sufficiently small values of .
On the other hand, in the greedy algorithm with random order, with a constant probability (； + 8),
the computed solution is the same as the optimal solution. Otherwise, the greedy algorithm a with
random order returns the same solution as the greedy algorithm with a non-increasing order. Hence,
in expectation, the solution returned by the greedy with random order is better than the solution
returned by the greedy algorithm with non-increasing order by a constant factor.
D	Experiments - Appendix
D.1 Baselines
We comment on two of our baselines:
•	Exact SVD: In the canonical, learning-free sketching setting (i.e., any matrix is equally
probable), sketching using the top m singular vectors yields a (1 + )-approximation for
both LRA and k-means Cohen et al. (2015).
•	Column sampling: In the canonical, learning-free sketching setting, sketching via column
sampling yields a (1 + )-approximation for k-means Cohen et al. (2015).
D.2 Experimental parameters
For the tables in Section 5, we describe experimental parameters. First, we provide some general
implementation details.
We implemented both the greedy (Algorithm 1) and stochastic gradient descent (Algorithm 2) algo-
rithms in PyTorch. In the first case, PyTorch allowed us to harness GPUs to speed up computation
on large matrices. We used several Nvidia GeForce GTX 1080 Ti machines. In the second case,
PyTorch allowed us to effortlessly compute numerical gradients for each task’s objective function.
Specifically, PyTorch provides automatic differentiation, which is implemented by backpropagation
through chained differentiable operators.
There are also two points of note in the greedy algorithm implementation. First, we noticed that for
MRR and LRA, each iteration required computing the SVD for many rank-1 updates of the current
S. Instead of computing the SVD from scratch for each of these variants, we first computed the
SVD of S and then used fast rank-1 SVD updates Brand (2006). This greatly improved the runtime
of Algorithm 1. Second, we decided to set Dw (the set of candidate row weights) to 10 samples in
[-2, 2] because we noticed most weights were in this range after running Algorithm 2.
D.3 Running time
We examine the runtimes of our various sketching algorithms. In Table D.1, the times are obtained
for the LRA task with k = 30, m = 60 on the logo data set. However, similar trends should be
expected for other combinations of task, task parameters, and data sets.
We define the inference runtime as the time to apply the sketching algorithm. The training runtime is
the time to train a sketch on Atrain and only applies to learned sketches. Generally, the long training
times are not problematic because training is only done once and can be completed offline. On the
other hand, the inference runtime should be as fast as possible.
Note that inference was timed using 1 matrix from Atest on an Nvidia Geforce GTX 1080 Ti GPU.
The values were averaged over 10 trials.
30
Under review as a conference paper at ICLR 2021
We observe that sparse sketches (such as the ones used in GD only and Ours) have much lower
inference runtimes than the dense sketches of exact SVD.
Table D.1: Timing comparison of sketching algorithms for LRA (using Algorithm 1 from Indyk
et al. (2019))
	Time (sec)
random: inference	0.0114
exact SVD: inference	0.185
learned (random pattern): training	193 (3 min)
learned (random pattern): inference	0.0114
learned (greedy pattern): training	6300 (1 h 45 min)
learned (greedy pattern): inference	0.0114
31