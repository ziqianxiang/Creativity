Under review as a conference paper at ICLR 2021
On The Adversarial Robustness of 3D Point
Cloud Classification
Anonymous authors
Paper under double-blind review
Ab stract
3D point clouds play pivotal roles in various safety-critical fields, such as au-
tonomous driving, which desires the corresponding deep neural networks to be
robust to adversarial perturbations. Though a few defenses against adversarial
point cloud classification have been proposed, it remains unknown whether they
can provide real robustness. To this end, we perform the first security analysis of
state-of-the-art defenses and design adaptive attacks on them. Our 100% adaptive
attack success rates demonstrate that current defense designs are still vulnerable.
Since adversarial training (AT) is believed to be the most effective defense, we
present the first in-depth study showing how AT behaves in point cloud classi-
fication and identify that the required symmetric function (pooling operation) is
paramount to the model’s robustness under AT. Through our systematic analysis,
we find that the default used fixed pooling operations (e.g., MAX pooling) gener-
ally weaken AT’s performance in point cloud classification. Still, sorting-based
parametric pooling operations can significantly improve the models’ robustness.
Based on the above insights, we further propose DeepSym, a deep symmetric
pooling operation, to architecturally advance the adversarial robustness under AT
to 47.0% without sacrificing nominal accuracy, outperforming the original design
and a strong baseline by 28.5% (〜2.6×) and 6.5%, respectively, in PointNet.
1	Introduction
Despite the prominent achievements that deep neural networks (DNN) have reached in the past
decade, adversarial attacks (Szegedy et al., 2013) are becoming the Achilles’ heel in modern deep
learning deployments, where adversaries generate imperceptible perturbations to mislead the DNN
models. Numerous attacks have been deployed in various 2D vision tasks, such as classifica-
tion (Carlini & Wagner, 2017), object detection (Song et al., 2018), and segmentation (Xie et al.,
2017). Since adversarial robustness is a critical feature, tremendous efforts have been devoted to de-
fending against 2D adversarial images (Guo et al., 2017; Papernot et al., 2016; Madry et al., 2018).
However, Athalye et al. (2018) suggest that most of the current countermeasures essentially try to
obfuscate gradients, which give a false sense of security. Besides, certified methods (Zhang et al.,
2019) often provide a lower bound of robustness, which are not helpful in practice. Therefore,
adversarial training is widely believed as the most and only effective defense solution.
The emergence of 3D point cloud applications in safety-critical areas like autonomous driving raises
public concerns about their security of DNN pipelines. A few studies (Xiang et al., 2019; Cao et al.,
2019; Sun et al., 2020) have demonstrated that various deep learning tasks on point clouds are indeed
vulnerable to adversarial examples. Among them, point cloud classification models have laid solid
foundations upon which other complex models are built (Lang et al., 2019; Yu et al., 2018a). While
it seems intuitive to extend convolutional neural networks (CNN) from 2D to 3D for point cloud
classification, it is actually not a trivial task. The difficulty mainly inherits from that point cloud is
an unordered set structure that CNN cannot handle. Modern point cloud classification models (Qi
et al., 2017a; Zaheer et al., 2017) address this problem by leveraging a symmetric function, which
is permutation-invariant to the order of points, to aggregate local features, as shown in Figure 2.
Recently, a number of countermeasures have been proposed to defend against 3D adversarial point
clouds. However, the failure of gradient obfuscation-based defenses in the 2D space motivates us to
re-think whether current defense designs provide real robustness for 3D point cloud classification.
Especially, DUP-Net (Zhou et al., 2019) and GvG-PointNet++ (Dong et al., 2020a) claim to improve
the adversarial robustness significantly. However, we find that both defenses belong to gradient
1
Under review as a conference paper at ICLR 2021
obfuscation through our analysis, hence further design white-box adaptive attacks to break their
robustness. Unfortunately, our 100% attack success rates demonstrate that current defense designs
are still vulnerable.
As mentioned above, adversarial training (AT) is considered the most effective defense strategy; we
thus perform the first rigorous study of how AT behaves in point cloud classification by exploiting
projected gradient descent (PGD) attacks (Madry et al., 2018). We identify that the default used
symmetric function weakens the effectiveness of AT. Specifically, popular models (e.g., PointNet)
utilize fixed pooling operations like MAX and SUM pooling as their symmetric functions to aggregate
features. Different from CNN-based models that usually apply pooling operations with a small slid-
ing window (e.g., 2 × 2), point cloud classification models leverage such fixed pooling operations to
aggregate features from a large number of candidates (e.g., 1024). We find that those fixed pooling
operations inherently lack flexibility and learnability, which are not appreciated by AT. Moreover,
recent research has also presented parametric pooling operations in set learning (Wang et al., 2020;
Zhang et al., 2020), which also preserve permutation-invariance.We take a step further to system-
atically analyze point cloud classification models’ robustness with parametric pooling operations
under AT. Experimental results show that the sorting-based pooling design benefits AT well, which
vastly outperforms MAX pooling, for instance, in adversarial accuracy by 7.3% without hurting the
nominal accuracy1.
Lastly, based on our experimental insights, we propose DeepSym, a sorting-based pooling opera-
tion that employs deep learnable layers, to architecturally advance the adversarial robustness of point
cloud classification models under AT. Experimental results show that DeepSym reaches the best ad-
versarial accuracy in all chosen backbones, which on average, is a 10.8% improvement compared to
the default architectures. We also explore the limits of DeepSym based on PointNet due to its broad
adoption (Guo et al., 2020). We obtain the best robustness on ModelNet40, which achieves the ad-
versarial accuracy of 47.0%, significantly outperforming the default MAX pooling design by 28.5%
(〜2.6×). In addition, We demonstrate that PointNet with DeepSym also reaches the best adversar-
ial accuracy of 45.2% under the most efficient AT on ModelNet10 (Wu et al., 2015), exceeding MAX
PooIing by 17.9% (〜1.7×).
2	Background and Related Work
3D point cloud classification. Early works attemPt to classify Point clouds by adaPting deeP learn-
ing models in the 2D sPace (Su et al., 2015; Yu et al., 2018b). DeePSets (Zaheer et al., 2017) and
PointNet (Qi et al., 2017a) are the first to achieve end-to-end learning on Point cloud classification
and formulate a general sPecification (Figure 2) for Point cloud learning. PointNet++ (Qi et al.,
2017b) and DGCNN (Wang et al., 2019) build uPon PointNet set abstraction to better learn local
features. Lately, DSS (Maron et al., 2020) generalizes DeePSets to enable comPlex functions in
set learning. Besides, ModelNet40 (Wu et al., 2015) is the most PoPular dataset for benchmarking
Point cloud classification, which consists of 12,311 CAD models belonging to 40 categories. The
numerical range of the Point cloud data is normalized to [-1, 1] in ModelNet40.
Adversarial attacks and defenses on point clouds. Xiang et al. (2019) Perform the first study
to extend C&W attack (Carlini & Wagner, 2017) to Point cloud classification. Wen et al. (2019)
imProve the loss function in C&W attack to realize attacks with smaller Perturbations and Hamdi
et al. (2019) Present black-box attacks on Point cloud classification. Recently, Zhou et al. (2019)
and Dong et al. (2020a) ProPose to defend against adversarial Point clouds by inPut transformation
and adversarial detection. Besides, Liu et al. (2019) conduct a Preliminary investigation on extending
countermeasures in the 2D sPace to defend against simPle attacks like FGSM (Goodfellow et al.,
2014) on Point cloud data. In this work, we first design adaPtive attacks to break existing defenses
and analyze the adversarial robustness of Point cloud classification under adversarial training.
3	Breaking the Robustness of Existing Defenses
3.1	Adaptive Attacks on DUP-Net
DUP-Net (ICCV’19) Presents a denoiser layer and uPsamPler network structure to defend against
adversarial Point cloud classification. The denoiser layer g : X → X0 leverages kNN (k-nearest
1In this PaPer, we use nominal and adversarial accuracy to denote the model’s accuracy on clean and adver-
sarially Perturbed data, resPectively.
2
Under review as a conference paper at ICLR 2021
neighbour) for outlier removal. Specifically, the kNN of each point xi in point cloud X is defined as
knn(xi, k) so that the average distance di of each point xi to its kNN is denoted as:
di = T X ||xi - Xj ||2 , i = {1, 2,..∙, n}	(I)
k
xj ∈knn(xi,k)
where n is the number of points. The mean μ = * PZi di and Standard deviation σ =
J1 Pn=ι(di - μ)2 of all these distances are computed to determine a distance threshold as μ+α∙σ
to trim the point clouds, where α is a hyper-parameter. As a result, the denoised point cloud is rep-
resented as X0 = {xi | di < μ + α ∙ σ}. The denoised point cloud X0 will be further fed into
PU-Net (Yu et al., 2018a), defined as p : X0 → X00, to upsample X0 to a fixed number of points.
Combined with the classifier f, the integrated DUP-Net can be noted as (f ◦p ◦ g)(X). The hypoth-
esis is that the denoiser layer will eliminate the adversarial perturbations and the upsampler network
will re-project the denoised off-manifold point cloud to the natural manifold.
Analysis. The upsampler network p (i.e., PU-Net) is differentiable and can be integrated with
the classification network f . Therefore, f ◦ p is clearly vulnerable to gradient-based adaptive at-
tacks. Although the denoiser layer g is not differentiable, it can be treated as deterministic masking:
M(Xi) = 1di<μ+α∙σ so that the gradients can still flow through the masked points. By involving
M(Xi) into the iterative optimization process: Rxi (f ◦ P ◦ g)(X)∣χi=χ ≈ Pχ∕f ◦ p)(X)∣xi=x∙M(x),
similar to BPDA (Athalye et al., 2018), attackers may still find adversarial examples.
Experimentation. We leverage the open-sourced codebase2 of DUP-Net for experimentation.
Specifically, a PointNet (Qi et al., 2017a) trained on ModelNet40 is used as the target classifier
f. For the PU-Net, the upsampled number of points is 2048, and the upsampling ratio is 2. For the
adaptive attacks, we exploit targeted L2 norm-based C&W attack and untargeted L∞ norm-based
PGD attack with 200 iterations (PGD-200). Detailed setups are elaborated in Appendix A.1.
Table 1: Adversarial accuracy under adaptive attacks on PU-Net and DUP-Net. For the denoiser layer g, k = 2
and α = 1.1 are set the same as Zhou et al. (2019). f denotes the attack in the original paper.
Attack Method	Adversarial Accuracy			Mean L2 Norm Distance
	PointNet (f)	PU-Net (f ◦ P	DUP-Net (f ◦ P ◦ g)	
Clean point cloud	883%	87.5%	-863%-	00
C&W attack on f f	0.0%	23.9%	84.5%	0.77
C&W attack on f ◦ p	2.3%	0.0%	74.7%	0.71
Adaptive attack on f ◦ p ◦ g	1.1%	0.8%	0.0%	1.62
PGD attack ( = 0.01)	7.1%	5.9%	5.4%	-
PGD attack ( = 0.025)	3.5%	2.8%	2.1%	-
PGD attack ( = 0.05)	1.3%	1.0%	0.8%	-
PGD attack ( = 0.075)	0.0%	0.0%	0.0%	-
Discussion. As shown in Table 1, adaptive C&W attacks achieve 100% success rates on DUP-Net.
Though the mean distance of adversarial examples targeting DUP-Net is larger than those targeting
PU-Net, they are almost indistinguishable by human perception, as visualized in Appendix A.2.
We find that naive PGD attacks are also effective since the upsampler network is sensitive to L∞
norm-based perturbations. The design of DUP-Net is similar to ME-Net (Yang et al., 2019) in the
2D space, which recently has been shown vulnerable to adaptive attacks (Tramer et al., 2020). We
hereby demonstrate that such input transformation-based defenses cannot offer real robustness to
point cloud classification, either.
3.2	Adaptive Attacks on GvG-PointNet++
GvG-PointNet++ (CVPR’20) introduces gather vectors in the 3D point clouds as an adversarial
indicator. The original PointNet++ aggregates local features fi hierarchically to make final classifi-
cation. Gather vectors vi are learned from local features fi to indicate the global center ci of a point
cloud sample. If the indicated global center ci is far away from the ground-truth global center cg,
the corresponding local feature fi will be masked out:
ci = xci + Vi ； Mi = 1∣∣Cg-Ci∣∣<r ； Fg = {fi∙Mi}	⑵
2https://github.com/RyanHangZhou/DUP-Net
3
Under review as a conference paper at ICLR 2021
(a)	L2 norm-based adaptive C&W attack on DUP-Net
(C) L2 norm-based adaptive PGD attack on GVG-PointNet++
(b)	L∞ norm-based adaptive PGD attack on DUP-Net
(d) L∞ norm-based adaptive PGD attack on GvG-PointNet++
Figure 1: Sampled visualizations of adversarial examples generated by adaptive attacks on both defenses ( =
0.05 and δ = 0.16). More visualizations can be found in Appendix A.2.
where xci is the geometry center of the local point set, r is the distance threshold to mask the local
feature, and Fg is the cleaned feature set for final classification. To train GvG-PointNet++, it is
necessary to optimize a surrogate loss to correctly learn the gather vectors besides the cross-entropy
(xent) loss:
n0
Ltotal = Lxent + λ ∙ Lgather , Lgather =	||ci - cg||1
i=1
(3)
where n0 is the number of local features and λ is a hyper-parameter. Thus, GvG-PointNet++ essen-
tially applies self-attention to the local features and relies on it for robustness enhancement.
Analysis. Dong et al. (2020a) evaluate white-box adversaries on GVG-PointNet++ with naive L2
norm-based PGD attacks. Specifically, only Lxent is utilized in the adversarial optimization process
so that the masking Mi will hinder the gradient propagation. However, since Mi is learned from
the network itself, it is highly possible to further break this obfuscation with Lgather considered.
The adaptive attack can be then formulated as an optimization problem with the loss function:
Ladv = Lxent - β ∙ Lgather
(4)
where β is a hyper-parameter. By maximizing Ladv with L2 norm-based PGD attacks, adversaries
strive to enlarge the adversarial effect but also minimize the perturbations on gather vectors. We also
find that GvG-PointNet++ is by design vulnerable to PGD attacks on Lgather as such perturbations
will potentially affect most gather vector predictions to make gi masked out so that insufficient for
final classification.
Experimentation. We train GvG-PointNet++ based on single-scale grouped PointNet++ (Qi et al.,
2017b) on ModelNet40 and set r = 0.08 and λ = 1 as suggested by Dong et al. (2020a). The model
is trained by Adam (Kingma & Ba, 2014) optimizer with 250 epochs using batch size 16, and the
initial learning rate is 0.01. For the adaptive attack, we use 10-step binary search to find a appropriate
β . The setup of L2 norm-based PGD attacks is identical to Dong et al. (2020a), and we also leverage
L∞ norm-based PGD-200 in the evaluation. Detailed setups are elaborated in Appendix A.1.
Table 2: Adversarial accuracy under Lp norm-based adaptive attacks on GvG-PointNet++. and δ are the
perturbation boundaries. ↑ denotes the attack in the original paper.
Target Loss	Adversarial Accuracy (L∞)				Adversarial Accuracy (L2)		
	= 0.01	e = 0.025	e = 0.05	e = 0.075	δ=0.08	δ = 0.16	δ=0.32
Lxent f	30.6%	214%	54%	18%	25.2%	169%	15.4%
Ladv	20.1%	12.6%	2.2%	0.0%	7.5%	4.4%	2.1%
Lgather	17.9%	8.1%	0.0%	0.0%	8.5%	4.1%	2.7%
Discussion. As shown in Table 2, both adaptive PGD attacks achieve high success rates on GvG-
PointNet++. we also observe that the L∞ norm-based PGD attack is more effective on Lgather since
L∞ norm perturbations assign the same adversarial budget to each point, which can easily impact
a large number of gather vector predictions. However, it is hard for the L2 norm-based PGD attack
to influence so many gather vector predictions because it prefers to perturb key points rather than
the whole point set. GvG-PointNet++ leverages DNN to detect adversarial perturbations, which is
similar to MagNet (Meng & Chen, 2017) in the 2D space. We validate that adversarial detection also
fails to provide real robustness under adaptive white-box adversaries in point cloud classification.
4	Adversarial Training With Different Symmetric Functions
We have so far demonstrated that state-of-the-art defenses against 3D adversarial point clouds are
still vulnerable to adaptive attacks. While gradient obfuscation cannot offer real adversarial robust-
4
Under review as a conference paper at ICLR 2021
din=3
global features
SUM Pooling
MEDIAN Pooling
MAX Pooling
classification scores
Figure 2: The general specification of point cloud classification (σ ◦
ρ ◦ Φ)(X), where n is the number of points, di is the number
of hidden dimensions in the i-th feature map, Φ(∙) represents the
permutation-equivariant layers, ρ(∙) denotes the column-wise symmetric
(permutation-invariant) function, and σ(∙) is the fully connected layer.
Figure 3: Xent loss of PGD attack
on PointNet with three fixed pool-
ing operations (each value is aver-
aged over 100 runs from random
starting points).
ness, adversarial training (AT) is widely believed to be the most effective method. In this section,
we conduct the first thorough study showing how AT performs in point cloud classification.
4.1	Adversarial Training Preliminaries
Madry et al. (2018) formulate AT as a paddle point problem in Equation 5, where D is the under-
lying data distribution, L(∙, ∙, ∙) is the loss function, X is the training data with its label y, e is the
adversarial perturbation, and S denotes the boundary of such perturbations.
arg min	E(x,y)〜D max L(X + a y, θ)	(5)
θ	∈S
Adversarial training setups. We choose PointNet (Qi et al., 2017a), DeepSets (Zaheer et al., 2017),
and DSS (Maron et al., 2020) as the backbone networks. As shown in Section 3 and demonstrated
by Madry et al. (2018), L∞ norm-based PGD attack tends to be a universal first-order adversary.
Thus, we select PGD-7 into the training recipe, and empirically set the maximum per-point per-
turbation = 0.05 out of the point cloud range [-1, 1]. We follow the default PointNet training
setting to train models on the ModelNet40 training set. In the evaluation, we utilize PGD-200 to
assess their robustness on the ModelNet40 validation set with the same adversarial budget = 0.05.
Meanwhile, we also report the nominal accuracy on the clean validation set. Each PGD attack starts
from a random point in the allowed perturbation space. More details can be found in Appendix B.
4.2	Adversarial Training With Fixed Pooling Operations
As shown in Figure 2, modern models fundamentally follow a general specification (σ ◦ ρ ◦ Φ)(X)
for point cloud classification. Φ(∙) represents a set of permutation-equivariant layers to learn lo-
cal features from each point. ρ(∙) is a column-wise symmetric (permutation-invariant) function to
aggregate the learned local features into a global feature, and σ(∙) are fully connected layers for
final classification. PointNet, DeepSets, and DSS leverage different Φ(∙) for local feature learning,
but all depend on fixed pooling operations as their ρ(∙). Specifically, MAX pooling is by default
used in DeepSets (for point cloud classification) and PointNet (Zaheer et al., 2017; Qi et al., 2017a),
while DSS utilizes SUM pooling (Maron et al., 2020). We also additionally select MEDIAN pooling
due to its robust statistic feature (Huber, 2004). Though models with fixed pooling operations have
achieved satisfactory accuracy under standard training, they face various difficulties under AT. As
shown in Table 3, models with MEDIAN pooling achieve better nominal accuracy among fixed pool-
ing operations, but much worse adversarial accuracy, while SUM pooling performs contrarily. Most
importantly, none of them reach a decent balance of nominal and adversarial accuracy.
Table 3: Adversarial robustness of models with fixed pooling operations under PGD-200 at = 0.05.
Pooling Operation	Nominal Accuracy			Adversarial Accuracy		
	PointNet	DeepSets	DSS	PointNet	DeepSets	DSS
MAX	80.5%	71.1%	78.8%	16.1%	21.8%	21.5%
SUM	76.3%	54.1%	73.3%	25.1%	24.8%	25.3%
MEDIAN	84.6%	72.7%	82.4%	7.5%	11.0%	9.3%
Analysis. AT consists of two stages: 1) inner maximization to find the worst adversarial examples
and 2) outer minimization to update model parameters. Fixed pooling operations essentially leverage
5
Under review as a conference paper at ICLR 2021
(b) Sorting-based pooling operations sort each di-
mension to re-organize the feature set into an or-
dered matrix F to which complex operations (e.g.,
CNN) can be applied to aggregate features.
(a) Attention-based pooling operations apply self-attention
to each point-level feature vector fi. The learned weight αi
is multiplied with each element in fi , and the aggregated
feature is computed by a column-wise summation.
Figure 4: Design philosophy of attention-based and sorting-based pooling operations.
a single statistic to represent the distribution of a feature dimension (Murray & Perronnin, 2014).
Although MEDIAN pooling, as a robust statistic, intuitively should enhance the robustness, we find
it actually hinders the inner maximization stage from making progress. We utilize L∞ norm-based
PGD attack to maximize the xent loss of standard trained model with three fixed pooling operations.
Figure 3 validates that MEDIAN pooling takes many more steps to maximize the loss. Therefore,
MEDIAN pooling fails to find the worst adversarial examples in the first stage with limited steps.
Though MAX and SUM pooling are able to achieve higher loss value, they encounter challenges in
the second stage. MAX pooling backward propagates gradients to a single point at each dimension
so that the rest n-1 features do not contribute to model learning. Since n is oftentimes a large
number (e.g., 1024), the huge information loss and non-smoothness will fail AT (Xie et al., 2020).
While SUM pooling realizes a smoother backward propagation, it lacks discriminability because by
applying the same weight to each element, the resulting representations are strongly biased by the
adversarial perturbations. Thus, with SUM pooling, the models cannot generalize well on clean data.
4.3 Adversarial Training With Parametric Pooling Operations
Recent studies have also presented trainable parametric pooling operations for different tasks in
set learning, e.g., multiple instance learning, which are also qualified to be the symmetric function
ρ(∙) in point cloud classification models. Thus, We first group them into two classes: 1) attention-
based and 2) sorting-based pooling, and further benchmark their robustness under AT in point cloud
classification. It is worth noting that none of those parametric pooling operations are proposed to
improve the adversarial robustness, and we are the first to conduct such an in-depth analysis of how
they behave as the symmetric function under AT in point cloud classification.
4.3.1	Attention-based Pooling Operations
An attention module can be abstracted as mapping a query and a set of key-value pairs to an output,
making the models learn and focus on the critical information (Bahdanau et al., 2014). Figure 4(a)
shows the design principle of attention-based pooling, which leverages a compatibility function to
learn point-level importance. The aggregated global feature is computed as a column-wise weighted
sum of the local features. Two attention-based pooling operations, ATT and ATT-GATE, are first
proposed for multiple instance learning (Ilse et al., 2018). Let F = {f1, f2, . . . , fn} be a set of
features, ATT aggregates the global feature g by:
n
g = Eai ∙ fi ,
i=1
exp(w> ∙ tanh(V ∙ f>))
Pn=I exp(w> ∙ tanh(V ∙ f7))
(6)
where w ∈ RL×1 and V ∈ RL×dm are learnable parameters. ATT-GATE improves the expressive-
ness of ATT by introducing another non-linear activation Sigmoid(∙) and more trainable parameters
into weight learning. Furthermore, PMA (Lee et al., 2019) is proposed for general set learning, which
leverages multi-head attention (Vaswani et al., 2017) for pooling. We detail the design and our im-
plementation of ATT, ATT-GATE, and PMA in Appendix B.3, and adversarially train the backbone
models with these attention-based pooling operations.
4.3.2	Sorting-based Pooling Operations
Sorting has been recently considered in the set learning literature due to its permutation-invariant
characteristic, as shown in Figure 4(b). Let F ∈ Rn×dm be the matrix version of the feature set F,
6
Under review as a conference paper at ICLR 2021
FSPool (Zhang et al., 2020) aggregates F by feature-wise sorting in a descending order:
n
Fei,j = SortJ(Fj)i ； gj = X Wi,j ∙ Fij	⑺
i=1
where W ∈ Rn×dm are learnable parameters. Therefore, the pooled representation is column-wise
weighted sum of F . SoftPool (Wang et al., 2020) re-organizes F so that its j-th dimension is
sorted in a descending order, and picks the top k point-level embeddings Fj0 ∈ Rk×dm to further
form F = [F10, F20, . . . , Fd0 ]. Then, SoftPool applies CNN to each Fj → gj so that the pooled
representation is g = [g1, g2, . . . , gdm]. Implementation details of SoftPool are elaborated in
Appendix B.3. We also adversarially train the backbone models with FSPool and SoftPool.
Table 4: Adversarial robustness of models with parametric pooling operations under PGD-200 at = 0.05.
Pooling Operation	Nominal Accuracy			Adversarial Accuracy		
	PointNet	DeepSets	DSS	PointNet	DeepSets	DSS
ATT	73.5%	-52.3%-	72.8%	22.1%	23.2%	23.9%
ATT-GATE	75.1%	63.9%	73.3%	23.2%	24.8%	26.1%
PMA	73.9%	51.9%	72.5%	25.4%	20.9%	23.9%
FSPool	82.8%	73.8%	81.5%	29.8%	25.3%	26.1%
SoftPool	79.8%	72.1%	80.2%	30.1%	24.9%	26.5%
DeepSym (ours)	82.7%	74.2%	81.6%	33.6%	26.9%	31.4%
4.3.3	Experimental Analysis
Table 4 shows the results of AT with different parametric pooling operations. To meet the require-
ment of permutation-invariance, attention-based pooling is restricted to learn point-level importance.
For example, ATT applies the same weight to all dimensions of a point embedding. As a result, at-
tention barely improves the pooling operation’s expressiveness as it essentially re-projects the point
cloud to a single dimension (e.g., fi → ai in ATT) and differentiates them based on it, which
significantly limits their discriminability. Therefore, little useful information can be learned from
the attention module, explaining why they perform similarly to SUM pooling that applies the same
weight to each point under AT, as shown in Table 4. Sorting-based pooling operations naturally
maintain permutation-invariance as SortJ(∙) re-organizes the unordered feature set F to an ordered
feature map F . Thus, FSPool and SoftPool are able to further apply feature-wise linear trans-
formation and CNN. The insight is that feature dimensions are mostly independent of each other,
and each point expresses to a different extent in every dimension. By employing feature-wise learn-
able parameters, the gradients also flow smoother through sorting-based pooling operations. Table 4
validates that sorting-based pooling operations achieve much better adversarial accuracy, e.g., on
average, 7.3% better than MAX pooling while maintaining comparable nominal accuracy.
5	Improving The Adversarial Robustness With DEEPSYM
In the above analysis, we have shed light on that sorting-based pooling operations can benefit AT in
point cloud classification. We hereby explore to further improve the sorting-based pooling design in-
spired by existing arts. First, we notice that both FSPool and SoftPool apply SortJ (∙) right after
a ReLU function (Nair & Hinton, 2010). However, ReLU leads to some neurons being zero (Good-
fellow et al., 2016), which makes SortJg unstable. Second, recent studies have shown that AT
appreciates deeper neural networks (Xie & Yuille, 2019). Nevertheless, FSPool only employs one
linear layer to aggregate features, and SoftPool requires dm to be a small number. The reason is
that scaling up the depth in these existing sorting-based pooling designs requires exponential growth
of parameters, which will make the end-to-end learning intractable.
To address the above limitations, we propose a simple yet effective pooling operation, DeepSym,
that embraces the benefits of sorting-based pooling and also applies deep learnable layers to the
pooling process. Given a feature set after ReLU activation F ∈ R+n×dm, DeepSym first applies
another linear transformation to re-map F into Rn×dm so that fi = W ∙ fτ where W ∈ Rdm×dm
and F0 = {f10 , f20 , . . . , fn0 }. Let F 0 be the matrix version of F0, DeepSym also sorts F 0 in a
descending order (Equation 7) to get F 0 . Afterwards, we apply column-wise shared MLP on F 0 :
gj =MLP(Ff0:,j) ,	j= {1,2,...,dm}	(8)
7
Under review as a conference paper at ICLR 2021
Figure 5: Adversarial robustness of PointNet with various pooling operations under PGD-200 at = 0.05.
to learn the global feature representation g . Each layer of the MLP composes of a linear transforma-
tion, a batch normalization module (Ioffe & Szegedy, 2015), and a ReLU activation function. Com-
pared to FSPool that applies different linear transformations to different dimensions, DeepSym
employs a shared MLP to different dimensions. By doing so, DeepSym deepens the pooling pro-
cess to be more capable of digesting the adversarial perturbations. DeepSym can also address the
problem of SoftPool that is only achievable with limited dm because the MLP is shared by all
the feature channels so that it can scale up to a large number of dm with little complexity increases.
Moreover, DeepSym generalizes MAX and SUM pooling by specific weight vectors. Therefore, it
can also theoretically achieve universality with dm ≥ n (Wagstaff et al., 2019) while being more
expressive in its representation and smoother in gradients propagation. To deal with the variable-size
point clouds, DeepSym adopts column-wise linear interpolation in F0 to form a continuous feature
map and then re-samples it to be compatible with the trained MLP (Jaderberg et al., 2015). Last but
not least, DeepSym is by design flexible with the number of pooled features from each dimension.
In the paper, we only allow DeepSym to output a single feature for a fair comparison with others.
However, it is hard for other pooling operations to have this ability. For example, it requires a linear
complexity increase for FSPool to enable this capability.
5.1	Evaluations
We implement a 5-layer DeepSym with [512, 128, 32, 8, 1] hidden neurons on three backbone net-
works and adversarially train them on ModelNet40 the same way introduced in Section 4.1. Table 4
shows that almost all models with DeepSym reach the best results in both nominal and adversarial
accuracy, outperforming the default architecture by 10.8%, on average. Taking PointNet as an ex-
ample, DeepSym (33.6%) improves the adversarial accuracy by 17.5% (〜2.1 ×) compared to the
original MAX pooling architecture. Besides, DeepSym also achieves a 3.5% improvement in adver-
sarial accuracy compared to FSPool and SoftPool. Overall, we demonstrate that DeepSym can
benefit AT significantly in point cloud classification.
We further leverage various white- and black-box adversarial attacks to cross validate the robustness
improvements of DeepSym on PointNet. Specifically, we exploit well-known FGSM (Szegedy
et al., 2013), BIM (Kurakin et al., 2016), and MIM (Dong et al., 2018) as the white-box attack
methods. We set the adversarial budget = 0.05, and leverage 200 steps for the iterative attacks,
as well. For the black-box attacks, we choose two score-based methods: SPSA (Uesato et al.,
2018) and NES (Ilyas et al., 2018), and a decision-based evolution attack (Dong et al., 2020b).
We still select = 0.05 and allow 2000 queries to find each adversarial example. The detailed
setups are elaborated in Appendix C.1. As shown in Table 5, PointNet with DeepSym consistently
achieves the best adversarial accuracy under white-box attacks, except for FGSM. The reason is that
FGSM is a single-step method that has limited ability to find adversarial examples. Besides, we
find the black-box attacks are not as effective as the white-box attacks, which also demonstrate that
adversarial training with DeepSym is able to improve the robustness of point cloud classification
without gradient obfuscation (Carlini et al., 2019).
Since DeepSym brings deep trainable layers into the original backbones, it is necessary to report its
overhead. We leverage TensorFlow (Abadi et al., 2016) and NVIDIA profilers to measure the infer-
ence time, the number of trainable parameters, and GPU memory usage on PointNet. Specifically,
the inference time is averaged from 2468 objects in the validation set, and the GPU memory is mea-
sured on an RTX 2080 with batch size = 8. As shown in Table 6, DeepSym indeed introduces more
computation overhead by leveraging the shared MLP. However, we believe the overhead is relatively
small and acceptable, compared to its massive improvements on the adversarial robustness. To fur-
8
Under review as a conference paper at ICLR 2021
Table 5: Adversarial robustness of PointNet with different pooling operations under attacks at = 0.05.
Pooling Operation	White-box Attack			Black-box Attack		
	FGSM	BIM	MIM	SPSA	NES	Evolution
MAX	72.8%	24.3%	23.5%	69.2%	67.1%	-53.4%-
MEDIAN	77.6%	23.3%	14.5%	71.1%	65.2%	57.8%
SUM	44.4%	33.5%	37.5%	65.3%	62.3%	52.7%
ATT	43.1%	33.1%	35.0%	68.1%	64.8%	55.9%
ATT-GATE	43.9%	34.2%	33.9%	70.2%	65.9%	55.8%
PMA	47.2%	31.9%	30.1%	67.2%	64.1%	53.4%
FSPool	61.3%	45.4%	48.0%	72.8%	71.9%	69.9%
SoftPool	62.1%	47.6%	45.1%	69.2%	68.5%	70.0%
DeepSym (ours)	61.4%	52.5%	55.4%	72.4%	72.1%	73.1%-
ther have a lateral comparison, point cloud classification backbones are much more light-weight than
image classification models. For example, ResNet-50 (He et al., 2016) and VGG-16 (Simonyan &
Zisserman, 2014) have 23 and 138 million trainable parameters, respectively, and take much longer
time to do the inference. The reason that models with SoftPool and PMA have fewer trainable
parameters is that they limit the number of dimensions in the global feature by design.
Table 6: Overhead measurement of PointNet with different pooling operations.
	Inference Time (ms)	# Trainable Parameters	GPU Memory (MB)
MAX	2.21	815,336	989
MEDIAN	2.44	815,336	989
SUM	2.23	815,336	989
ATT	2.71	1,340,649	1980
ATT-GATE	3.07	1,865,962	2013
PMA	2.10	652,136	981
FSPool	2.89	1,863,912	1005
SoftPool	2.85	355,328	725
DeepSym (ours)	3.10	1,411,563	2013
5.2	Exploring The Limits of DEEPSYM
There is a trade-off between the training cost and adversarial robustness in AT. Increasing the number
of PGD attack steps can create harder adversarial examples (Madry et al., 2018), which could further
improve the model’s robustness. However, the training time also increases linearly with the number
of attack iterations increasing. Due to PointNet’s broad adoption (Guo et al., 2020), we here analyze
how it performs under various AT settings. Specifically, we exploit the most efficient AT with PGD-
1 on ModelNet10 (Wu et al., 2015), a dataset consisting of 10 categories with 4899 objects, and a
relatively expensive AT with PGD-20 on ModelNet40 to demonstrate the effectiveness of DeepSym.
Other training setups are identical to Section 4.1.
Figure 5 shows the results of the robustness of adversarially trained PointNet with various pooling
operations under PGD-200. We demonstrate that PointNet with DeepSym still reaches the best ad-
versarial accuracy of 45.2% under AT with PGD-1 on ModelNet10, which outperforms the original
MAX pooling by 17.9% (〜1.7×) and SoftPool by 4.0%. Surprisingly, PointNet with DeepSym
also achieves the best nominal accuracy of 88.5%. Moreover, DeepSym further advances itself un-
der AT with PGD-20 on ModelNet40. Figure 5(b) shows that PointNet with DeepSym reaches the
best 47.0% adversarial accuracy, which are 28.5% (〜2.6×) and 6.5% improvements compared to
MAX pooling and SoftPool, respectively while maintaining competent nominal accuracy. We also
report detailed evaluations using different PGD attack steps and budgets in Appendix C.1.
6	Conclusion
In this work, we perform the first rigorous study on the adversarial robustness of point cloud classifi-
cation. We design adaptive attacks and demonstrate that state-of-the-art defenses against adversarial
point clouds cannot provide real robustness. Furthermore, we conduct a thorough analysis of how
the required symmetric function affects the AT performance of point cloud classification models.
We are the first to identify that the fixed pooling generally weakens the models’ robustness under
AT, and on the other hand, sorting-based parametric pooling benefits AT well. Lastly, we propose
DeepSym that further architecturally advances the adversarial accuracy of PointNet to 47.0% under
AT, outperforming the original design and a strong baseline by 28.5% (〜2.6×) and 6.5%.
9
Under review as a conference paper at ICLR 2021
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, MatthieU
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-
scale machine learning. In 12th {USENIX} symposium on operating systems design and imple-
mentation ({OSDI} 16),pp. 265-283, 2016.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of secu-
rity: Circumventing defenses to adversarial examples. In Jennifer Dy and Andreas Krause (eds.),
Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceed-
ings ofMachine Learning Research, pp. 274-283, Stockholmsmassan, Stockholm Sweden, 10-15
Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/athalye18a.html.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Yulong Cao, Chaowei Xiao, Benjamin Cyr, Yimeng Zhou, Won Park, Sara Rampazzi, Qi Alfred
Chen, Kevin Fu, and Z Morley Mao. Adversarial sensor attack on lidar-based perception in
autonomous driving. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and
Communications Security, pp. 2267-2281, 2019.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. 2017
IEEE Symposium on Security and Privacy (SP), May 2017. doi: 10.1109/sp.2017.49. URL
http://dx.doi.org/10.1109/SP.2017.49.
Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris
Tsipras, Ian Goodfellow, Aleksander Madry, and Alexey Kurakin. On evaluating adversarial
robustness. arXiv preprint arXiv:1902.06705, 2019.
Xiaoyi Dong, Dongdong Chen, Hang Zhou, Gang Hua, Weiming Zhang, and Nenghai Yu. Self-
robust 3d point recognition via gather-vector guidance. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR), June 2020a.
Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boost-
ing adversarial attacks with momentum. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 9185-9193, 2018.
Yinpeng Dong, Qi-An Fu, Xiao Yang, Tianyu Pang, Hang Su, Zihao Xiao, and Jun Zhu. Benchmark-
ing adversarial robustness on image classification. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pp. 321-331, 2020b.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT press Cambridge, 2016.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens Van Der Maaten. Countering adversarial
images using input transformations. arXiv preprint arXiv:1711.00117, 2017.
Yulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu, and Mohammed Bennamoun. Deep
learning for 3d point clouds: A survey. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 2020.
Abdullah Hamdi, Sara Rojas, Ali Thabet, and Bernard Ghanem. Advpc: Transferable adversarial
perturbations on 3d point clouds. arXiv preprint arXiv:1912.00461, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Peter J Huber. Robust statistics, volume 523. John Wiley & Sons, 2004.
10
Under review as a conference paper at ICLR 2021
Maximilian Ilse, Jakub Tomczak, and Max Welling. Attention-based deep multiple instance learn-
ing. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Con-
ference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp.
2127-2136, Stockholmsmassan, Stockholm Sweden, 10-15 JUl 2018. PMLR. URL http:
//proceedings.mlr.press/v80/ilse18a.html.
Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with
limited qUeries and information. arXiv preprint arXiv:1804.08598, 2018.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
redUcing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In Ad-
vances in neural information processing systems, pp. 2017-2025, 2015.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014.
Alexey KUrakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world.
arXiv preprint arXiv:1607.02533, 2016.
Alex H Lang, SoUrabh Vora, Holger Caesar, LUbing ZhoU, Jiong Yang, and Oscar Beijbom. Pointpil-
lars: Fast encoders for object detection from point cloUds. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 12697-12705, 2019.
JUho Lee, Yoonho Lee, JUngtaek Kim, Adam Kosiorek, SeUngjin Choi, and Yee Whye Teh. Set
transformer: A framework for attention-based permUtation-invariant neUral networks. In Interna-
tional Conference on Machine Learning, pp. 3744-3753, 2019.
Daniel LiU, Ronald YU, and Hao SU. Extending adversarial attacks and defenses to deep 3d point
cloUd classifiers. In 2019 IEEE International Conference on Image Processing (ICIP), pp. 2279-
2283. IEEE, 2019.
Aleksander Madry, Aleksandar Makelov, LUdwig Schmidt, Dimitris Tsipras, and Adrian VladU. To-
wards deep learning models resistant to adversarial attacks. In International Conference on Learn-
ing Representations, 2018. URL https://openreview.net/forum?id=rJzIBfZAb.
Haggai Maron, Or Litany, Gal Chechik, and Ethan Fetaya. On learning sets of symmetric elements.
arXiv preprint arXiv:2002.08599, 2020.
DongyU Meng and Hao Chen. Magnet: a two-pronged defense against adversarial examples. In
Proceedings of the 2017 ACM SIGSAC conference on computer and communications security,
pp. 135-147, 2017.
Naila MUrray and Florent Perronnin. Generalized max pooling. In Proceedings of the IEEE confer-
ence on computer vision and pattern recognition, pp. 2473-2480, 2014.
Vinod Nair and Geoffrey E Hinton. Rectified linear Units improve restricted boltzmann machines.
In ICML, 2010.
Nicolas Papernot, Patrick McDaniel, Xi WU, Somesh Jha, and Ananthram Swami. Distillation as a
defense to adversarial pertUrbations against deep neUral networks. In 2016 IEEE Symposium on
Security and Privacy (SP), pp. 582-597. IEEE, 2016.
Charles R Qi, Hao SU, KaichUn Mo, and Leonidas J GUibas. Pointnet: Deep learning on point sets
for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 652-660, 2017a.
Charles RUizhongtai Qi, Li Yi, Hao SU, and Leonidas J GUibas. Pointnet++: Deep hierarchical
featUre learning on point sets in a metric space. In Advances in neural information processing
systems, pp. 5099-5108, 2017b.
Karen Simonyan and Andrew Zisserman. Very deep convolUtional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
11
Under review as a conference paper at ICLR 2021
Dawn Song, Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Florian
Tramer, Atul Prakash, and Tadayoshi Kohno. Physical adversarial examples for object detectors.
In 12th {USENIX} Workshop on Offensive Technologies ({WOOT} 18), 2018.
Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convo-
lutional neural networks for 3d shape recognition. In Proceedings of the IEEE international
conference on computer vision, pp. 945-953, 2015.
Jiachen Sun, Yulong Cao, Qi Alfred Chen, and Z. Morley Mao. Towards robust lidar-based per-
ception in autonomous driving: General black-box adversarial sensor attack and countermea-
sures. In 29th USENIX Security Symposium (USENIX Security 20), pp. 877-894. USENIX
Association, August 2020. ISBN 978-1-939133-17-5. URL https://www.usenix.org/
conference/usenixsecurity20/presentation/sun.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to
adversarial example defenses. arXiv preprint arXiv:2002.08347, 2020.
Jonathan Uesato, Brendan O’Donoghue, Aaron van den Oord, and Pushmeet Kohli. Adversarial risk
and the dangers of evaluating against weak attacks. arXiv preprint arXiv:1802.05666, 2018.
Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Duc Thanh Nguyen, and Sai-Kit Yeung.
Revisiting point cloud classification: A new benchmark dataset and classification model on real-
world data. In International Conference on Computer Vision (ICCV), 2019.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Edward Wagstaff, Fabian B Fuchs, Martin Engelcke, Ingmar Posner, and Michael Osborne. On the
limitations of representing functions on sets. arXiv preprint arXiv:1901.09006, 2019.
Yida Wang, David Joseph Tan, Nassir Navab, and Federico Tombari. Softpoolnet: Shape descriptor
for point cloud completion and classification. arXiv preprint arXiv:2008.07358, 2020.
Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M.
Solomon. Dynamic graph cnn for learning on point clouds. ACM Transactions on Graphics
(TOG), 2019.
Yuxin Wen, Jiehong Lin, Ke Chen, and Kui Jia. Geometry-aware generation of adversarial and
cooperative point clouds. arXiv preprint arXiv:1912.11171, 2019.
Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong
Xiao. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.
Chong Xiang, Charles R Qi, and Bo Li. Generating 3d adversarial point clouds. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9136-9144, 2019.
Cihang Xie and Alan Yuille. Intriguing properties of adversarial training at scale. arXiv preprint
arXiv:1906.03787, 2019.
Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, and Alan Yuille. Adversarial
examples for semantic segmentation and object detection. In Proceedings of the IEEE Interna-
tional Conference on Computer Vision, pp. 1369-1378, 2017.
Cihang Xie, Mingxing Tan, Boqing Gong, Alan Yuille, and Quoc V Le. Smooth adversarial training.
arXiv preprint arXiv:2006.14536, 2020.
Yuzhe Yang, Guo Zhang, Dina Katabi, and Zhi Xu. Me-net: Towards effective adversarial robustness
with matrix estimation. arXiv preprint arXiv:1905.11971, 2019.
12
Under review as a conference paper at ICLR 2021
Lequan Yu, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, and Pheng-Ann Heng. Pu-net: Point cloud
upsampling network. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition,pp. 2790-2799, 2018a.
Tan Yu, Jingjing Meng, and Junsong Yuan. Multi-view harmonized bilinear network for 3d object
recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 186-194, 2018b.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and
Alexander J Smola. Deep sets. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30,
pp. 3391-3401. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/
6931-deep-sets.pdf.
Huan Zhang, Hongge Chen, Chaowei Xiao, Sven Gowal, Robert Stanforth, Bo Li, Duane Boning,
and Cho-Jui Hsieh. Towards stable and efficient training of verifiably robust neural networks.
arXiv preprint arXiv:1906.06316, 2019.
Yan Zhang, Jonathon Hare, and Adam Prugel-Bennett. Fspool: Learning set representations with
featurewise sort pooling. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=HJgBA2VYwH.
Hang Zhou, Kejiang Chen, Weiming Zhang, Han Fang, Wenbo Zhou, and Nenghai Yu. Dup-net:
Denoiser and upsampler network for 3d adversarial point clouds defense. In Proceedings of the
IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.
13
Under review as a conference paper at ICLR 2021
A	Adaptive Attack Experimental Setup and Visualizations
A.1 Experimental Setups
Since DUP-Net is open-sourced, we target the publicly released PointNet and PU-Net models. For
the L2 norm-based C&W attack, we set the loss function as:
L = (max(Z(X0)i) - Z(X0)t0)+ + λ ∙∣∣X - X0∣∣2	(9)
where X ∈ Rn×3 is the matrix version of point cloud X, X0 is the optimized adversarial example,
Z(X)i is the i-th element of the output logits, and t0 is the target class. We leverage 10-step binary
search to find the appropriate hyper-parameter λ from [10, 80]. As suggested by Xiang et al. (2019),
we choose 10 distinct classes and pick 25 objects in each class from the ModelNet40 validation
set for evaluation. The step size of the adversarial optimization is 0.01 and we allow at most 500
iterations of optimization in each binary search to find the adversarial examples.
For the L∞ norm-based PGD attack, we adopt the formulation in Madry et al. (2018):
Xt+1 = Πχ+S(Xt + α ∙ Sign(VXtL(Xt, θ, y)))	(10)
where Xt is the adversarial example in the t-th attack iteration, Π is the projection function to
project the adversarial example to the pre-defined perturbation space S, which is the L∞ norm
ball in our setup, and α is the step size. We select the boundary of allowed perturbations
= {0.01, 0.025, 0.05, 0.075} out of the point cloud data range [-1, 1]. Since point cloud data
is continuous, We set the step size α =行.
For GvG-PointNet++, we train it based on the single scale grouping (SSG)-PointNet++ backbone.
The backbone netWork has three PointNet set abstraction module to hierarchically aggregate local
features, and We enable gather vectors in the last module, Which contains 128 local features (i.e.,
n0 = 128 in Section 3.2) With 256 dimensions. To learn the gather vectors, We apply three fully
connected layers With 640, 640, and 3 hidden neurons respectively, as suggested by Dong et al.
(2020a). Since the data from ModelNet40 is normalized to [-1,1], the global object center is cg =
[0, 0, 0].
For the L∞ norm-based PGD attack, We leverage the same setup as the attack on DUP-Net. For
the L2 norm-based PGD attack, we follow the settings in Dong et al. (2020a) to set the L2 norm
threshold E = δ√n X d^, where δ is selected in {0.08,0.16,0.32}, n is the number of points, and
din is the dimension of input point cloud (i.e., 3). The attack iteration is set to 50, and the step size
α = 50.
A.2 Visualizations
We visualize some adversarial examples generated by adaptive attacks on PU-Net and DUP-Net in
Figure 6 and Figure 7. It is expected that adversarial examples targeting DUP-Net are noisier than
the ones targeting PU-Net as the former needs to break the denoiser layer. However, as mentioned
in Section 3.1, they are barely distinguishable from human perception. We also visualize some
adversarial examples generated by untargeted adaptive PGD attacks on GvG-PointNet++ in Figure 8
with different perturbation budgets E.
B Adversarial Training S etup
B.1	PGD Attack in Adversarial Training
We also follow the formulation in Equation 18 to find the worst adversarial examples. Specifically,
we empirically select E = 0.05 into the training recipe as there is no quantitative study on how
much humans can bear the point cloud perturbations. Figure 8 shows that adversarial examples
with E = 0.05 are still recognizable by human perception. Moreover, because point cloud data is
continuous, we set the step size of PGD attacks as:
α
E
step ,
E
10,
step < 10
step ≥ 10
(11)
14
Under review as a conference paper at ICLR 2021
Poq JIOqSdOOq-e0q
ouqdjs
Jsqo JOls0日 EjOS
Figure 6: Visualizations of adversarial examples (2048 points) generated by L2 norm-based C&W adaptive
attacks on PU-Net.
in both training and evaluation phases to make sure PGD attacks reach the allowed maximum per-
turbations.
OIqEl -əuol OSEA
B.2	Point Cloud Classification Model Architecture Details
PointNet, DeepSets, and DSS are the fundamental architectures in point cloud classification. Other
models, such as PointNet++ and DGCNN, are built upon PointNet and DeepSets. Moreover, com-
PleX models oftentimes apply non-differentiable layers like knn(∙) into end-to-end learning, which
will make the adversarial training ineffective. In this work, we aim at exploring how the symmetric
(permutation-invariant) function can benefit adversarial training. To this end, we choose PointNet,
DeepSets, and DSS as the backbone networks. For the ModelNet40 dataset, we follow the default
setting to split into 9,843 objects for training and 2,468 objects for validation (Wu et al., 2015). We
randomly sample 1024 points from each object to form its point cloud, if not otherwise stated.
PointNet. We leverage the default architecture in PointNet codebase3 and eXclude the transfor-
mation nets (i.e., T-Net) and dropout layers for simplicity and reproducibility. PointNet leverages
shared fully connected (FC) layers as the permutation-equivariant layer φl : FCl (Fl:,i) → Fl+1:,i
and MAX pooling as the symmetric function ρ(∙).
3 https://github.com/charlesq34/pointnet
15
Under review as a conference paper at ICLR 2021
ouqdjsPoq JlOqsdooq-e0q
bed bookshelf bottle
chair monitor
airplane
÷
Jsqo JOls0日 EjOS
OIqEl -əuol OSEA
Figure 7: Visualizations of adversarial examples (2048 points) generated by L2 norm-based C&W adaptive
attacks on DUP-Net.
sofa
÷
DeepSets. We leverage the default architecture in DeepSets codebase4. Different from PointNet,
DeepSets first applies a symmetric function to each feature map and aggregate it with the origi-
nal feature map. Afterwards, DeepSets also leverages FC layers to further process the features:
φl : FCl(F:,i - Z(Fl)) → Fl+1: i, where Z(∙) is column-wise MAX pooling in the original imple-
mentation. Similarly, MAX pooling is still used as ρ(∙) in DeepSets.
DSS. DSS generalizes DeepSets architecture and applies another FC layer to Z(Fl) in DeepSets so
that φl : FCl1(Fl:,i) + FCl2(Z(Fl)) → Fl+1:,i. Different from other two achitectures, DSS utilizes
SUM pooling as ρ(∙). Since there is no available codebase at the time of writing, We implement DSS
by ourselves.
We visualize the differences of φ(∙) in Figure 9, and summarize the layer information in Table 7.
B.3	Parametric Pooling Design And Implementation
We have introduced ATT in Section 4.3.1. In our implementation, we choose L = 512 so that
V ∈ R512×1024 to train the backbone models.
4 https://github.com/manzilzaheer/DeepSets
16
Under review as a conference paper at ICLR 2021
Figure 8: Visualizations of adversarial examples (1024 points) generated by L∞ norm-based PGD adaptive
attacks on GvG-PointNet++.
ATT-GATE is a variant of ATT with more learnable parameters:
n
g = £a「fi ,
i=1
exp(w> ∙ (tanh(V ∙ f>)。Sigm(U ∙ f>)))
Pj=I exp(w> ∙ (tanh(V ∙ fj) Θ Sigm(U ∙ fj)))
(12)
where U, V ∈ RL ×M, sigm(∙) is the sigmoid activation function, and Θ is an element-wise multi-
plication. We also choose L = 512 in ATT-GATE to train the backbone models.
17
Under review as a conference paper at ICLR 2021
Table 7: Layer information of PointNet, DeepSets, and DSS. BN represents a batch normalization layer.
PointNet
φ1 : n × 3 → n × 64
BN + ReLU
φ2 : n × 64 → n × 64
BN + ReLU
φ3 : n × 64 → n × 128
BN + ReLU
φ4 : n × 128 → n × 1024
BN + ReLU
ρ: n × 1024 → 1024
σ1 : 1024 → 512
BN + ReLU
σ2 : 512 → 256
BN + ReLU
σ3 : 256 → 40
DeepSets
φι : n X 3 → n X 256
BN + ELU
φ2 : n X 256 → n X 256
BN + ELU
P : n X 256 → 256
σ1 : 256 → 256
BN + Tanh
σ2 : 256 → 40
DSS
φ1 : n X 3 → n X 64
BN + ReLU
φ2 : n X 64 → n X 256
BN + ReLU
φ3 : n X 256 → n X 256
BN + ReLU
ρ : n X 256 → 256
σ1 : 256 → 256
BN + ReLU
σ2 : 256 → 40
FC(
FC(
FC(
dl
dl+1
(b) φ(∙) in DeepSets.
dl
dl+1
FC(
(a) φ(∙) in PointNet.
dl	dl+1
(c) φ(∙) in DSS.
Figure 9: Different architectures of φ(∙) in PointNeL DeepSets, and DSS.
(d) The aggregated feature in (b) and (c).
PMA (Lee et al., 2019) adopts multi-head attention into pooling on a learnable set of k seed vectors
S ∈ Rk×dm Let F ∈ Rn×dm be the matrix version of the set of features.
PMAk(F) = MAB(S, FC(F))	(13)
MAB(X,Y) = H + FC(H)	(14)
where H = X + Multihead(X, Y , Y ; w)	(15)
where FC(∙) is the fully connected layer and Multihead(∙) is the multi-head attention mod-
ule (Vaswani et al., 2017). We follow the implementation in the released codebase5 to choose k = 1,
the number of head = 4, and the hidden neurons in FC(∙) = 128 to train the backbone models.
Since SoftPool (Wang et al., 2020) sorts the feature set in each dimension, it requires the number
of dimensions dm to be relatively small. We follow the description in their paper to choose dm = 8
and k = 32 so that each Fj0 ∈ R32×8. We apply one convolutional layer to aggregate each F0j into
gj ∈ R1×32 so that the final g ∈ R1×256. Therefore, for all backbone networks with SoftPool,
we apply the last equivariant layer as φ : n × dm-1 → n × 8 and ρ : n × 8 → 256.
5https://github.com/juho-lee/set_transformer
18
Under review as a conference paper at ICLR 2021
C DEEPSYM Ablations
It is worth noting that DeepSym does not require the final layer to have only one neuron. However,
to have a fair comparison with other pooling operations that aggregate into one feature from each
dimension, our implementation of DeepSym also aggregates into one feature from each dimension.
C.1 Evaluation Details
We also perform extensive evaluations using different PGD attack steps and budgets on PGD-
20 trained PointNet. Figure 10 shows that PointNet with DeepSym consistently achieves the best
adversarial accuracy. We also validate MEDIAN pooling indeed hinders the gradient backward prop-
agation. The adversarial accuracy of PointNet with MEDIAN pooling consistently drops even after
PGD-1000. However, the adversarial accuracy of PointNet with other pooling operations usually
converges after PGD-200. Figure 11 shows that DeepSym also outperforms other pooling opera-
tions under different adversarial budgets .
-* - MAX
-→-- SUM
-→-- MEDIAN
-*- ATT
-*- ATT-GATE
-→-- PMA
-→-- FSPooI
-→-- SoftPooI
T- DeepSym
Figure 10:	Adversarial accuracy of PGD-20 trained PointNet with different pooling operations. We leverage
the PGD attack with different steps to evaluate the model’s robustness.
0 5 0 5 0 5 0
9 7 6 4 3 1
(％) AUeJrmq -eyes」①>p4
-* - MAX
-→-- SUM
-→-- MEDIAN
-*- ATT
-*- ATT-GATE
-→-- PMA
-→-- FSPooI
-→-- SoftPooI
T- DeepSym
Figure 11:	Adversarial accuracy of PGD-20 trained PointNet with different pooling operations. We leverage
the PGD attack with different budgets to evaluate the model’s robustness.
We leverage the default setup in FGSM, BIM, and MIM in our evaluation. FGSM is a single-step
attack method, which can be represented as:
Xadv = X + L sign(VχL(X, θ, y))	(16)
The BIM attack is similar to PGD attacks described in Appendix A.1. The differences are 1) the
attack starts from the original point cloud X and 2) the step size α = /T , where T is the number
of attack steps. The MIM attack introduces momentum terms into the adversarial optimization:
gt+ι = μ ∙ gt +
NXt L(Xt, Θ, y)
l∣Vχt L(Xt, θ, y ))∣∣1
(17)
Xt+1 = Xt + α ∙ sign(gt+1)
(18)
19
Under review as a conference paper at ICLR 2021
Similar to BIM, the attack starts from the original point cloud X and the step size α = /T . We set
μ = 1 following the original setup (Dong et al., 2018).
Due to the computational resource constraints, we set the sample size = 32 and allow 2000 quires to
find each adversarial example in the score-based black-box attack (Uesato et al., 2018; Ilyas et al.,
2018). For the evolution attack, we use the default loss L as the fitness score, and initialize 32 sets
of perturbations from a Gaussian distribution N(0, 1). 4 sets of perturbations with top fitness scores
will remain for the next iteration, while others will be discarded. We also allow 2000 generations of
evolution to find the adversarial example.
C.2 Evaluation on ScanObjectNN
We also evaluate the adversarial robustness of different pooling operations on a new point cloud
dataset, ScanObjectNN (Uy et al., 2019), which contains 2902 objects belonging to 15 categories.
We leverage the same adversarial training setup as ModelNet10 (i.e., PGD-1). Table 8 shows the re-
sults. We find that PointNet with DeepSym still achieves the best adversarial robustness. Since
the point clouds from ScanObjectNN are collected from real-world scenes, which suffers from
occlusion and imperfection, both nominal and adversarial accuracy drops compared to the results
ModelNet40. We find that even some clean point clouds cannot be correctly recognized by human
perception. Therefore, the performance degradation is also expected and we believe the results are
not as representative as ones on ModelNet40.
Table 8: Adversarial robustness of PointNet with different pooling operations under PGD-200 at = 0.05.
Pooling Operation	Nominal Accuracy	Adversarial Accuracy
MAX	752%	16.8%
MEDIAN	68.4%	8.2%
SUM	63.5%	18.3%
ATT	62.7%	17.9%
ATT-GATE	59.8%	17.1%
PMA	61.2%	16.2%
FSPool	76.8%	20.1%
SoftPool	73.2%	17.2%
DeepSym (ours)	767%	228%
C.3 T-SNE Visualizations
We visualize the global feature embeddings of adversarially trained PointNet under PGD-20 with
different pooling operations in Figure 12 and their logits in Figure 13. Since it is hard to pick 40
distinct colors, though we put all data from 40 classes into the T-SNE process, we only choose 10
categories from ModelNet40 to realize the visualizations.
20
Under review as a conference paper at ICLR 2021
(b) FSPool on training data, validation data, and PGD-200 adversarial validation data.
Figure 12: T-SNE visualizations of PointNet feature embeddings with MAX, FSPool, SoftPool, and
DeepSym pooling operations. Three columns correspond to training data, validation data, and PGD-200 ad-
versarial validation data, from left to right.
21
Under review as a conference paper at ICLR 2021
(a)	MAX pooling on training data, validation data, and PGD-200 adversarial validation data.
(b)	FSPool on training data, validation data, and PGD-200 adversarial validation data.
(c)	SoftPool on training data, validation data, and PGD-200 adversarial validation data.
(d)	DeepSym on training data, validation data, and PGD-200 adversarial validation data.
Figure 13: T-SNE visualizations of PointNet logits with MAX, FSPool, SoftPool, and DeepSym pooling
operations. Three columns correspond to training data, validation data, and PGD-200 adversarial validation
data, from left to right.
22