Under review as a conference paper at ICLR 2021
A generalized probability kernel on dis-
crete distributions and its application in two-
SAMPLE TEST
Anonymous authors
Paper under double-blind review
Ab stract
We propose a generalized probability kernel(GPK) on discrete distributions with
finite support. This probability kernel, defined as kernel between distributions in-
stead of samples, generalizes the existing discrepancy statistics such as maximum
mean discrepancy(MMD) as well as probability product kernels, and extends to
more general cases. For both existing and newly proposed statistics, we estimate
them through empirical frequency and illustrate the strategy to analyze the re-
sulting bias and convergence bounds. We further propose power-MMD, a natural
extension of MMD in the framework of GPK, illustrating its usage for the task
of two-sample test. Our work connects the fields of discrete distribution-property
estimation and kernel-based hypothesis test, which might shed light on more new
possibilities.
1 Introduction
We focus on the two-sample problem, which is given two i.i.d samples
{x1, x2, ...xn} , {y1, y2, ..., yn}, could we infer the discrepancy between underlying distribu-
tions they are drawn from. For such a problem, the option of hypothesis test(two-sample test) is
most popular, and a variety of statistics in estimating the discrepancy is proposed. In recent years,
RKHS based method such as maximum mean discrepancy(MMD) has gained a lot of attention.
(Gretton et al., 2012) has shown that in a universal-RKHS F, MMD(F, p, q) = 0 if and only if
p = q, thus could be used for the two-sample hypothesis test. (Gretton et al., 2012) further provides
unbiased estimator of MMD with fast asymptotic convergence rate, illustrating its advantages.
On the other hand, estimating distribution properties with plugin(empirical) estimators on discrete
setting is an active research area in recent years, where people focus on problem settings with large
support size but not so large sample size. The Bernstein polynomial technique is introduced to ana-
lyze the bias of the plugin estimators in (Yi & Alon, 2020), which provides remarkable progress on
bias-reduction methods of the plugin estimators. It is thus interesting to ask if the plugin estimators
could motivate new results for the RKHS-based two-sample test.
Another interesting topic is about the probability kernel, defined as kernel function over probabil-
ities, instead of over samples. As is easily seen, any discrepancy measure of distribution p and q
could potentially be valid probability kernels, not so much work focuses on this. While (Jebara et al.,
2004) introduced the so called probability product kernels which generalize a variety of discrepancy
measures, its properties remain further study.
Motivated by above observations, our work focuses on a specialized probability kernel function
which is a direct generalization of sample-based RKHS methods such as MMD. We focus on using
plugin-estimator as the default estimator of the kernel function we defined, and illustrate that with the
help of Bernstein polynomial techniques, we could analyze the bias and convergence bounds of these
plugin-estimators. Our work thus connects the fields of discrete distribution-property estimation and
kernel-based hypothesis test, which brings interesting possibilities.
1
Under review as a conference paper at ICLR 2021
2	Notation
We use bold symbol p, q ∈ Rk to represent a probability function over a discrete support with
support size k, and pi, qi represents the ith entry of p and q. We use {v1, v2, ..., vk}, vi ∈ Rd
to represent the support of p, q. [k] := {1, 2, 3..., k} represents the set of indices of elements in
{v1, v2, ..., vk}. We use φ ◦ (p, q) to denote an element-wise function from Rk × Rk to Rk, where
(φ ◦ (p, q))i = φ ◦ (pi , qi) and φ ◦ p to denote an element-wise function from Rk to Rk, where
(φ ◦ p)i = φ ◦ pi . With a slight abuse of notation, we denote pρ , p - q as element-wise function
defined above. We use kernel(p, q) to denote kernel function which maps from Rk × Rk to real
value R. And kernel(x, y), x, y ∈ Rd represents a kernel function from Rd × Rd to real value R.
We use K to denote the gram matrix generated from kernel(x, y) on finite support {v1, v2, ..., vk},
where Kij = kernel(vi, Vj). WeUse {x1,x2,…,xn}〜 P and {y1,y2,…,yn} 〜 q to denote the
samples from distribution p and q, where n is the sample size.
3	Generalized probability kernel
Probability kernel fUnction, defined as kernel fUnction between distribUtions instead of samples, is a
natUral extension of the idea of kernel fUnction in sample space.
Definition 1. Given distribution p and q belongs to a family of discrete distribution with the same
finite support {v1, v2, ..., vk}, vi ∈ Rd, where k is the support size, we define the probability kernel
function as PK(p, q), which is a kernel function maps from Rk × Rk to real value R.
Many discrepancy measUres, sUch as MMD, can serve as probability kernel fUnctions, bUt peo-
ple UsUally don’t Use the term of probability kernel fUnction when describing them. The reason is
that for most of the time, we only consider a limited nUmber of distribUtions, and do not need or
have the resoUrces to navigate throUgh all the distribUtions within the family. For example, when
looking into the two-sample problem, we UsUally assUme two samples {x1, x2, ..., xn} ∈ Rd and
{y1, y2, ..., yn} ∈ Rd are i.i.d drawn from two distribUtions p and q, and Use the discrepancy mea-
sUre MMD[F, p, q] to determine if p and q are indistingUishable in the RKHS F. We do not
consider all other distribUtions in F that is irrelevant to oUr samples! So far the idea of kernel fUnc-
tion between distribUtions is in practice not so mUch UsefUl, however, here in this paper, we propose,
when considering the plUgin-estimator of many of the existing discrepancy measUres, it is beneficial
to view them as probability kernel fUnctions.
3.1	defination of generalized probability kernel
Definition 2 (Generalized probability kernel). Given the family S of discrete distribution on sup-
port {v1, v2, .., vk} where vi ∈ Rd. Let F be a unit ball in a universal-RKHS H with associated
continuous kernel RK(x, y), where for any x ∈ Rd and y ∈ Rd, RK(x, y) maps from Rd × Rd to R.
We denote gram matrix Kij = RK(vi, vj).
The generalized probability kernel function on distribution p, q ∈ S is GPKF,φ (p, q) = φ ◦
(p, q) Kφ ◦ (q, p)T = Pi∈[k] Pj∈[k] φ◦ (pi,qi)Kijφ ◦ (qj,pj)
where φ ◦ (p, q) is an element-wise mapping function on discrete distribution p, q ∈ S, which maps
from Rk × Rk to Rk,
ObvioUsly, Under this definition, the GPK is a symmetric probability kernel fUnction where
GPKF,φ(p, q) =GPKF,φ(q,p)
Mapping fUnction φ represent a great amoUnt of possibilities. For most cases, we need to narrow
down the region and eqUipped it with some convenient properties so that the GPK measUre coUld be
UsefUl.
One example is for the measUrement of discrepancy, where we want GPKF,φ(p, q) = 0 if and only
if p = q.
Definition 3 (discrepancy probability kernel). Let S be a family of discrete distribution p ∈ S
on support {v1, v2, ..., vk}. A discrepancy probability kernel is a kernel function PK(p, q) that
PK(p, q) = 0 if and only ifp = q
2
Under review as a conference paper at ICLR 2021
Theorem 1. GPKF,φ (p, q) with the mapping function φ that satisfies:
1.	symmetric or antisymmetric with respect to p and q: φ ◦ (p, q) = φ ◦ (q, p) or φ ◦ (p, q) =
-φ ◦ (q, p)
2.	kφ ◦ (p, q)k2 = ∣∣φ ◦ (q, p)k2 = 0 if and only if P = q, where ∣∣ ∙ ∣∣2 represents L2 norm.
is a discrepancy probability kernel.
Proof.
GPKF,φ(p,q)= £ ∑Φ◦(Pi,qi IKijφ ◦ (qj ,Pj)
i∈[k] j∈[k]
=φ ◦ (p, q) Kφ ◦ (q, P)T = ±φ ◦ (p, q) Kφ ◦ (p, q)T
= ±vKvT
K is a semipositive definite matrix, thus by definition of positive definite matrix, vKvT ≥ 0,
where equality holds if and only if v = 0, and since v = φ ◦ (P, q), this condition further means
φ ◦ (P, q) = 0, which holds if and only ifP = q.
□
Another example is the polynomial GPK, which is our main focus of this paper. Such a subclass of
GPK is interesting since we can build unbiased estimators of it using techniques of Bernstein poly-
nomial in (Qian et al., 2011). As we will show in section 5., we also have analyzable convergence
bounds for the resulting unbiased estimators, illustrating its potential usage for applications such as
two-sample test.
Definition 4 (polynomial GPK). The polynomial GPK is the subset of GPK that equipped with the
mapping function φ that is polynomial in P and q: φ ◦ (P, q) = lo=0 so=0 αl,sPlqs where o ∈ Z
is the degree of the polynomial, and al,s ∈ R is the coefficient
Below we give some examples of polynomial GPK, which include MMD proposed in (Gretton et al.,
2012), and the newly proposed power-MMD in this paper, which is a natural extension of MMD,
from the view point of probability kernels.
3.1.1	example 1: MMD as member of polynomial GPK
Given discrete distribution P, q with support {v1, v2, ..., vk}, we can rewrite MMD with distribution
probability function Pi , qi :
MMDF(P, q) = ∣Eχ~pf (x) - Eχ,~qf (x0)kH
2
X f (vi)Pi - X f(vi)qi
i∈[k]	i∈[k]	H
2
f(vi)Pi - f(vi)qi
i∈[k]	H
ΣΣ(Pi - qi)f(vi)f(vj)(Pj - qj) =ΣΣ(Pi - qi)Kij (Pj - qj)
i∈[k] j∈[k]	i∈[k] j∈[k]
-GPKF,φl(P,q)
Where φl ◦ (P, q) = P - q, H is the RKHS defined in MMD literature, and f is the function that
maps vi to H.
GPKF,φl (P, q) is a special case of polynomial GPK where α1,0 = 1, α0,1 = -1, and all other
coefficients are 0.
3.1.2	example 2: product GPK as members of polynomial GPK
Definition 5 (product GPK). Let P and q be probability distributions on support {v1, v2, ..., vk},
and l ∈ Z be nonnegative integer. The product GPK is a subset of polynomial GPK where αl,0 = 1,
and all other coefficients are 0. the corresponding mapping function is: φ(P, q) = Pl
3
Under review as a conference paper at ICLR 2021
The probability product kernel as in (Jebara et al., 2004) is a special case of product GPK where K
is a identity matrix.
3.1.3	example 3: power-MMD as members of polynomial GPK
Another interesting subset of polynomial GPK is the one extends MMD case into a power form and
we denote it as power-MMD:
Definition 6 (power-MMD). Let p and q be probability distributions on support {v1, v2, ..., vk}
and ρ ∈ Z be a positive integer. then the power-MMD is a subset of polynomial GPK where
αρ,0 = 1, α0,ρ = -1, and all other coefficients are 0. the corresponding mapping function is:
φ(p, q) =pρ - qρ
Apparently, MMD is a special case of power-MMD where ρ = 1, and power-MMD satisfies the
requirement in Theorem 1, thus has the potential usage of discrepancy measure. In section 5., we
will show that power-MMD has unbiased estimator with analyzable convergence bounds thus could
be used for two-sample test.
3.2 discussion of GPK in discrete setting
As one may easily notice, the definition of GPK includes a gram matrix generated by the kernel
function RK(vi, vj) which measures the discrepancy between vi, vj ∈ {v1 , v2, .., vk}. While con-
sidering the cases of categorical distribution, values of discrete variables does not relate to any notion
of distance, this raises the question: how the introduced gram matrix will be beneficial in any cases?
The answer is twofold: 1. Many natural processes produce discrete distributions where there
possibly exists a similarity measure in values which imply the similarity in frequencies of occur-
rence(probability values). For example, in the field of natural language process(NLP), one may
treat words as atomic units with no notion of similarity between words, as these are represented
as indices in a vocabulary. However, given large number of training samples, similarity measure
between words could be made possible using techniques such as words2vec(Mikolov et al., 2013).
Such techniques generally result in better performance and have become the important preprocess-
ing techniques for NLP tasks(Goodfellow et al., 2016). 2. As there are cases where the values of
discrete variables are totally irrelevant, or people may use kernel function RK(vi, vj) which doesn’t
correctly imply the similarity in probability values, the GPK framework may still capture the simi-
larity between distributions. One example is the case of MMD, which is, as we discussed above, an
element of GPK family. As proved in (Gretton et al., 2012), MMD is a distribution free measure-
ment between two samples, which means no matter what kind ofp, q and kernel(x, y), we have, the
MMD2F (p, q) measure will be 0 if and only if p = q. However, the bad choice of kernel function
does have a negative effect on convergence bounds of the empirical estimator proposed in (Gretton
et al., 2012), and will influence the results of two-sample test. For this reason, we mainly focus on
dataset with known relativity measures in our experiment section.
4 plugin-estimator for GPK
So far we have defined the GPK and discussed some subsets of GPK with potential usage of
two-sample test. Next we discuss how to build an estimator, given a member of GPK. In this
section, we propose the plugin-estimator, which based on the count of occurrence of each value
vi ∈ {v1 , v2, ..., vk} in samples {x1 , x2, ..., xn} ∈ p or q. We illustrate that by doing so, the
techniques of Bernstein polynomial in (Qian et al., 2011) could be used to help building unbiased
estimators for any members of polynomial GPK. Furthermore, we provide analyzable convergence
bounds of these estimators.
We begin with the definition of plugin-estimators:
Definition 7. Suppose we have i.i.d samples of distribution P as Xm := {xι, x2,..., xm } 〜 P
and Xn2 := {xm+1,Xn1+2,…,Xn1+n2} 〜 p. And also the i.i.d samples of distribution q as
Ymi := {y1, y2, ..., ymι } ~ q and Ym2 := {ymι + 1, ymι+2,…,ym1+m2 }〜q.
4
Under review as a conference paper at ICLR 2021
Let Ni(n1) denotes the number of occurrence of value vi ∈ {v1, v2, ..., vk} in sample Xn1, and
Si,n1 := (Ni(n1), n1) denotes the collection of Ni(n1) and n1. The same follows for Xn2, Ym1 and
Y
m2
We define the plugin-estimator of GPKF,φ (p, q) as
GPKE[F,φ,X,Y] =XX
fφ (Si,n1, Si,m1) Kij fφ (Sj
,m2, Sj,n2)
i∈[k] j∈[k]
where fφ is a function related to function φ, and K is the gram matrix brought by F.
Here our setting is different from the unbiased estimator MMD2u of (Gretton et al., 2012), where
in their setting Xn1 , Xn2 represent the same sample from p and so do for Ym1 , Ym2 from q. In-
stead, we are using the same setting as the linear time statistic MMDl2 proposed in (Gretton et al.,
2012). Another way of viewing this is that for our setting, given two samples {x1 , x2, ..., xn},
{y1, y2, ..., yn} from p and q, we depart each sample of x and y into two parts, yielding 4 different
samples with size n1, n2, m1, m2, and then calculate the empirical frequencies for plugin-estimator
defined above.
4.1	polynomial GPK with unbiased plugin-estimators
One of our main contributions of this paper is the proposal that we can always find an unbiased
plugin-estimator for any members in polynomial GPK family. The basic idea is that we can analyze
the expectation of plugin-estimators through Bernstein polynomial, and use the existing results of
(Qian et al., 2011) to build the unbiased plugin estimators.
Theorem 2.	Denote
gj(k,n) = [ gj(k,n)= ( k )( n )	，forj ≤ k
[	0,	forj >k
Then any member of polynomial GPK[F, φ, p, q] equipped with polynomial mapping function
φ(p, q) = Plo=0 Pso=0 αl,splqs of degree o ∈ Z, has an unbiased plugin-estimator with mapping
function fφ to be:
oo
fφ(Si,n1, Si,m1) = XXαl,sgl(Ni(n1),n1)gs(Ni(m1),m1)
l=0 s=0
Proof. The basic idea is directly using the result of Bernstein polynomial in (Qian et al., 2011) to
build unbiased estimators. We PUt our formal proof in appendix	□
For notation simplicity, we define the plugin-estimator discussed above to be the default-plugin-
estimator for polynomial GPK:
Definition 8 (default-plugin-estimator for polynomial GPK). The plugin-estimator defined in The-
orem 2 is the default-plugin-estimator for polynomial GPK.
This plugin-estimator, according to Theorem 2, is an unbiased estimator
4.2 deviation bound of plugin-estimators
Another topic about plugin-estimator is its deviation bound. We directly use the McDiamid’s in-
equality to derive the default-plugin-estimator for polynomial GPK:
Theorem 3.	The default-plugin-estimator of GPK[F, φ, p, q] equipped with polynomial mapping
function φ(p, q) = Plo=0 Pso=0 αl,splqs of degree o ∈ Z
has the convergence bound:
2a2
∀a > 0, Pr(∣GPKE [F ,φ,X,Y ] - E[GPKe [F ,φ,X,Y ]]| ≥ a) ≤ 2e--2z
5
Under review as a conference paper at ICLR 2021
where
Z
m2
τm(12),n2 2 + n2
oo
Φn,m = XXXαl,sgl(Ni(n),n)gs(Ni(m),m)
i∈[k] l=0 s=0
TnIm=SUp(XX —(n) ∙ |ai,s| ∙ gι (Nis) ,n)gs(Nim∖m'
i∈[k] l=0 s=0 Ni
42m = SUp(X X -sm)∙ ∣αι,s∣ ∙ gι(Ninn,n)gs(Nim,m))
,	i∈[k] l=0 s=0 Ni(m)
Kmax is the largest value of entries in K
Proof. The basic idea is to use the McDiamid’s inequality, and we put our formal proof into the
appendix.	□
5	example: power-MMD as a natural extension to MMD from
GPK viewpoint
In this section, we mainly discuss power-MMD as defined in 3.1.3. We analyze the bias and con-
vergence bound of its plugin-estimators using the techniques we introduced so far, illustrating that
such a natural extension to MMD from GPK viewpoint could be beneficial for two-sample test.
5.1	plugin-estimators of power-MMD
As we already discussed in section 4.1.3, power-MMD is a subset of polynomial GPK. According
to Theorem 2, any member GPKF,φρ (p, q) in power-MMD has a default-plugin-estimator with the
mapping function fφ (Si,n1, Si,m1) = gρ(Ni,n1) - gρ(Mi, m1)
Remark 3.1. When ρ = 1, the power-MMD return to the original MMD case. Remarkably, the
default-plugin-estimator of this case is equivalent to the linear time statistic MMDl2 proposed in
(Gretton et al., 2012): GPKE [F, φl, X, Y] = MMDl2 [F, X, Y] For details of the derivation, see
appendix
5.2	deviation bound of plugin-estimators of power-MMD
Corollary 3.1. Denote Tn = supi∈[k] (NPn)gρ(N(n),n)j
The default-plugin-estimator of power-MMD GPKF,φρ (p, q) has uniform convergence bound de-
fined in Theorem 3. with Tn(1,m) = Tn and Tn(2,m) = Tm
Corollary 3.2. Consider the case where n1 = n2 = m1 = m2 = n The default-plugin-estimator
of power-MMD GPKF,φρ (p, q) has uniform convergence bound:
-na2
—na2
______________________ na
Pr(IGPKE [F ,φρ,X,Y ] - E[GPKe [F ,φρ,X,Y ]]| ≥ a) ≤ 2e (ρ2φnι,mι+ρ2φm2n2)κ'm。。≤ 2e 5PKm ax
Proof. The first inequality above comes from:
P (N ρ _ ρ(Ni - 1)(Ni - 2)... (Ni- P + I)Y PNiP 1 / P
Nigρ(Ni,n) =	n(n - 1)...(n - P +1)	≤ 7嬴-1 ≤ 互
6
Under review as a conference paper at ICLR 2021
where supi∈[k (看gρ(N, n)) = P only stands for extreme case such that there exist Ni = n, i.e.
all the samples belongs to the same value vi ∈ {v1, v2, ..., vk}. And the second inequality above
comes from:
Φn,m
≤
i∈[k]
X gρ(Ni(n)
i∈[k]
Ninn
n
m
gρ(Ni(m),m)
Ni(m) Y
2
□
Remark 3.2. recall in (Gretton et al., 2012), the deviation bound for linear time estimator
MMDl2[F,X,Y] is
—na 2
∀a > 0, Pr(∣MMD2[F,X,Y] — E[MMD；[F,X,Y]]∣ ≥ a) ≤ 2e8KmαX
Interestingly, this bound is the same as the case ρ = 1 in Corollary 3.2. Note that according to
section 6.1, the default-plugin-estimator of power-MMD with ρ = 1 is actually in equivalent to
MMDl2 case in (Gretton et al., 2012). Our bound generalize the bound in (Gretton et al., 2012) and
provide a tighter version. Note that the bounds for special case of ρ = 1 has simpler derivation, and
the reader may refer to appendix for more details.
5.3	two-sample test using power-MMD
Corollary 3.3. A hypothesis test of level a for the null hypothesis P = q has the acceptance region
∣ GPKE√Zφρ,X,Y] ∣ < J1 log((2)~1) Where Z is defined in Corollary 3.1
The two-sample test for power-MMD then follows this procedure: 1. calculate v
IGPKE[√φρ,χ,Y] ∣. 2. check if V ‹
reject the null hypothesis.
if so, accept the null hypothesis, otherwise
Next we analyze the performance of our proposed two-sample test under two cases: ρ = 1 and
ρ>1
5.3.1	ρ = 1 CASE
For ρ = 1 case, since GPKE [F, φ1, X, Y] is equivalent to MMDl2[F, φ1, X, Y], the only difference
between our proposal and that of Gretton et al. (2012) is the convergence bound. According to
Remark 3.2, we provide a tighter bound for the test statistic, thus we will certainly have a better
performance using power-MMD.
5.3.2	ρ > 1 CASE
We need to answer two questions for the ρ > 1 case: 1. when applying power-MMD in practice,
is the proposed statistics numerical stable? 2. will the performance of two-sample test gets better
when ρ gets larger?
For the question of numerical stability, since gρ(Ni, n) ≤ (Nni)ρ, the term will exponentially de-
crease with the increase of ρ. This effect will cause numerical problem when Ni n and ρ is large.
One solution is to find an upper-bound of ∣GPKE√φρ,X,Y] ∣ which is numerical stable.
Corollary 3.4. Consider the simplest case where n1 = n2 = m1 = m2 = n. Define
7
Under review as a conference paper at ICLR 2021
to be the set of all counts of occurrence in the four samples Xn1 , Xn2, Xm1 , Xm2. Denote SN =
supN ∈C (Ni) to be the maximium value in the set CN We have:
GPKE[F,Φρ,X,Y] ≤	SN ∙ GPKE
√Z	— KmaχP√2n ∙ Φ0
where
GPK0E := X	gρ(Ni(n1),n)-gρ(Ni(m1),SN)Kij gρ(Nj(n2),SN)-gρ(Nj(m2),SN)
i,j∈[k]
and
Φ0 = X gρ(Ni(n),SN) -gρ(Ni(m),SN)
i∈[k]
For cases when Ni are not far less from SN , GPK0E will be much more numerical stable than
GPKE[F,φρ,X,Y].
To answer the question related to the performance of two-sample test when ρ get larger, we need to
analyze the case when P = q, if ∣ GPKE√,φρ'X,Y] ∣ increase With the increase of ρ. Unfortunately,
there is no clear answer to this.
6	summary
To summarize, we introduce the framework of generalized probability kernel(GPK). While GPK
represents a large family of probability kernels, we focus on polynomial GPK since all members
of such subset of GPK have unbiased plugin-estimators. Remarkably, a natural extension of MMD
from the viewpoint of polynomial GPK, which we call power-MMD, could be used for two-sample
test. Theoretical study shows that for ρ = 1 case, power-MMD outperforms linear time MMD
proposed in Gretton et al. (2012), and the performance of ρ > 1 case is left for future work. For
members of GPK which do not belong to polynomial GPK, it is not easy to design an unbiased
estimators. However, bais reduction techniqes proposed in (Yi et al., 2018) and (Yi & Alon, 2020)
could be used, and we still have the chance to apply two-sample test with the resulting estimators.
Such a possibility is also left for future work.
References
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT Press, 2016.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola.
A kernel two-sample test. Journal ofMachine Learning Research, 12:723-773, 2012.
Tony Jebara, Risi Kondor, and Andrew Howard. Probability product kernels. J. Mach. Learn. Res.,
5:819-844, December 2004. ISSN 1532-4435.
Tomas Mikolov, Kai Chen, Greg S. Corrado, and Jeffrey Dean. Efficient estimation of word repre-
sentations in vector space. 2013. URL http://arxiv.org/abs/1301.3781.
Weikang Qian, Marc D. Riedel, and Ivo Rosenberg. Uniform approximation and bernstein poly-
nomials with coefficients in the unit interval. European Journal of Combinatorics, 32(3):448
- 463, 2011. ISSN 0195-6698. doi: https://doi.org/10.1016/j.ejc.2010.11.004. URL http:
//www.sciencedirect.com/science/article/pii/S0195669810001666.
Hao Yi and Orlitsky Alon. Data amplification: Instance-optimal property estimation. International
Conference on Machine Learning, 2020.
Hao Yi, Orlitsky Alon, Theertha S. Ananda, and Wu Yihong. Data amplification: A unified and
competitiveapproach to property estimation. Neural Information Processing Systems, 2018.
8
Under review as a conference paper at ICLR 2021
A Appendix
A. 1 Bernstein polynomial
Drawing i.i.d. samples Ym from any distribution p the expected value of the empirical estimator for
a distribution property is
E [HE (Ym)] = XMijnkpi)
i∈[k]
Note that for any function f, m ∈ N, and x ∈ [0, 1], the degree-m Bernstein polynomial of f is
m
Bm(f,x):=	f
mj	xj (1 - x)m-j
j=0
Therefore, we can express the expectation of the empirical property estimator as
Y¥〜phHE (Ym)i = X Bm (h,Pi)
i∈[k]
A.2 proof of Theorem 2
Proof. Recall the definition of polynomial GPK:
o
GPK(F,φ,p,q) =XX
φ(pi, qi)Ki,jφ(qj, pj) = X X Ki,j X αl,sαr,tpliqisptjqjr
i∈[k] j∈[k]	i∈[k] j∈[k]	l,s,r,t=0
recall in (Qian et al., 2011)
n
Xj =〉： gj (k, n)bk,n (Pi)= Ek〜bin(pi,n) gj (k, n)
k=j
where gj (k, n) is defined in the beginning of the theorem
o
GPK(F,φ,p,q)= X X Ki,j X αl,sαr,tpliqisptjqjr
i∈[k] j∈[k]	l,s,r,t=0
o
=XX
Kij X al,sar,tEN(n1)〜bin(pi,ni)gl(Ni, nI)
i∈[k] j∈[k]	l,s,r,t=0
ENimI)〜bin(qi,mι)gs(Ni " ,n2 )ENjn2) 〜bin(pj ,n2 )gt (Nj	,n2 )EN(m2)〜bin© ,m2 )gr (Nj ?), m2
o
=E XXKi,j X αι,sα∙r,tgι(N" ,nι)gs(Nm1,mι)gt(N尸，n2)gr(Njm*m2 八
i∈[k] j∈[k]	l,s,r,t=0
=E[GPKE(F,φ,X,Y)]
□
A.3 proof of theorem 3.
Lemma 4. Let SNi,n1 := (Ni(n1), n1) denotes the collection of Ni(n1) andn1. The same follows for
Xn2, Ym1 and Ym2 . Also for notation simplicity, let Si,n1 := SNi,n1
For the plugin-estimator
GPKE [F ,Φ,X,Y ] = XX fφ (SNi,nι ,SNi,mι) Kij fφ (SN∙,m2 ,SNg)
i∈[k] j∈[k]
with mapping function fφ(SNi,n1 , SNi,m1 ) having the following properties:
9
Under review as a conference paper at ICLR 2021
• fφ(SNi,n1 , SNi,m1) is a monotonic function related to Ni(n1) and Ni(m1):
∀Ni >Ni0or∀Ni <Ni0 : fφ(SNi,n1,SNi,m1) >fφ(SNi0,n1,SNi,m1)
The same follows for Ni(m1)
lfφ(SNi±1,nι, SNi,mι ) - fφ (SNi,nι, SNi,mι )| ≤ τnι
and
lfφ(SNi,nι, SNi±1,mι ) - fφ(SNi,nι , SNi,m1 )| ≤ τm1
where τn1 is a constant related to sample size n1, τm1 is a constant related to sample size
m1 , the same follows for τn2 and τm2
We have:
-2a2
∀a > 0, Pr(∣GPKE [F ,φ,X,Y ] - E[GPKe [F ,φ,X,Y ]]| ≥ a) ≤ 2e ɪ
where
Z=((ni TnI + n2τ22)φ2+(miTm ι+m2「m 2)好)Km ax
Φ1 =	fφ(SNj,n1
j∈[k]
SNj ,m1 ) , Φ2 =	fφ(SNj,n2, SNj,m2)
j∈[k]
Kmax is the largest value of entries in K
Proof. recall McDiamid’s inequality
Theorem 5. Let Y1 , . . . , Ym be independent random variables taking values in ranges R1 , . . . , Rm
and let F : R1 × . . . × Rm → C with the property that if one freezes all but the wth co-
ordinate of F (y1, . . . , ym) for some 1 ≤ w ≤ m, then F only fluctuates by most cw > 0,
thus | F (y1, . . . , yw-1, yw, yw + 1, . . . , ym ) - F (y1, . . . , yw-1, IyW, yw + 1, . . . , ym ) l≤ CW for all
yj ∈ Rj and yw0 ∈ Rw for 1 ≤ j ≤ m
-2a2
Thenforany a > 0, one has Pr(∣F(Y) 一 E[F(Y)]| ≥ a) ≤ 2e Pn=Ic2
considering the plugin-estimator of GPK family:
GPKE [F, φ, X, Y] =XX
fφ (Si,n1, Si,m1) Kij fφ (Sj,m2, Sj,n2)
i∈[k] j∈[k]
Without loss of generality, we rewrite the function fφ as:
fφ (SNi ,n1 , SNi ,m1 ) = F(x1 , x2 , ..., xs , ..., xn1 , Ni	, m1 ) = FNi
Assume we freeze all but one element in Xn1 := {x1, x2, ..., xs, ..., xn1 }, and only xs is allowed to
change its value.
obviously, no matter how this element change, it always lies in the finite set of support
{v1, v2, ..., vk}, without loss of generality, we assume xs changes its value from vi to vii, thus
the corresponding count of occurrence Ni changes to Ni - 1 and Nii changes to Nii + 1
10
Under review as a conference paper at ICLR 2021
we have for xs ∈ Xn1
Cs = SUpxs IGPKE(xi,X2,…，Xn) - GPKE(Xl,X2,…，区，…，Xn) |
=SUpxs X (F(xι,X2,…，xs,…，Xni ,N(m1),mι) - F(x1,x2,…，区，…，/叼,Ni(m1),m1) Kijfφ (Sj,
m2 ， Sj,n2 )
i,j∈[k]
= SUpi,ii∈[k] I	((FNi-1 - FNi ) Ki,j + (FNii+1 - FNii,1) Kii,j ) fφ(SNj ,n2 ， SNj ,m2 )I
Ij∈[k]	I
≤〉： τnι (-Ki,j + Kiij )fφ(SNj ,n2 ,S Nj ,m2 ) ≤ |Tni KmaX 1): ∖fφ(SNj ,n2 ,SNj ,m2 ) |
Ij∈[k]	I	j∈[k]
= τn1 Kmax Φ2
where Φ2 = Pj∈[k] ∖∖fφ (SNj,n2 ， SNj,m2)∖∖
Note that Ki,j := Kij
Similarly, for Xs ∈ Xn2
cs ≤ τn2 Kmax Φ1
where φι = ∑j∈[k]lφ(PIj,qij)|
for Xs ∈ Ym1
for Xs ∈ Ym2
cs ≤ τm1 Kmax Φ2
cs ≤ τm2 Kmax Φ1
thus according to McDiamid’s inequality, we have
-2a
Pr(∣GPKE [F, φ, X,Y ] - E [GPKe [F, φ, X, Y ]]| ≥ a) ≤ 2e Pn=+n2+mι+m2 C
We set
for Xi ∈ {Xn1},
ci = τn1 Kmax Φ2
ci = τm1 Kmax Φ2
for Xi ∈ {Ym1},
ci = τn2 Kmax Φ1
for Xi ∈ {Xn2 },
ci = τm2 Kmax Φ1
for Xi ∈ {Ym2 }
and get
n1+n2+m1+m2
E	Ci2 = ni ∙ (τnι KmaχΦ2)2 + n ∙ (τn2 KmaχΦl)2
i=1
+ mi ∙ (Tmi KmaχΦ2)2 + m2 ∙ (Tm (KmaχΦl)2
=((niTnι + m1τmJ φ2 + (n2τn2 + m2τm2) φ2) Kmaχ
□
11
Under review as a conference paper at ICLR 2021
We are ready to proof theorem 3:
Proof. Define
Divn(1,l),s = |gl(Ni(n) + 1,n)gs(Ni(m),m) - gl (Ni(n), n)gs (Ni(m), m)|
(Ni + 1)Ni (Ni - 1)...(Ni - ρ + 2)	Ni(Ni - 1)...(Ni - ρ + 1)
= ---------:-----:---:---------:-----------:------:--:------------
n1(n1 - 1)...(n1 - ρ + 1)	n1(n1 - 1)...(n1 - ρ + 1)
=N(n) - l +1 gl(Ninn,n) ∙∣gs(Ni(m)，m)|
=N(n) l l + 1 gl Nrn , n)gs Nm), m)
∣∣gs(Ni(m),m)∣∣
Divn(2,l),s = |gl(Ni(n) - 1,n)gs(Ni(m),m) - gl(Ni(n),n)gs(Ni(m),m)|
=∣(Ni- 1)(Ni- 2)…(Ni- P) - Ni(Ni- 1)…(Ni- P + 1)
∣ nι(nι - 1)...(nι - P +1)	nι(nι - 1)...(nι - P +1)
= ∣⅛)gι(NiS),n) ∙ |gs(Ni(m),m)|
=(n) gl(Ninn ,n)gs(Nim ,m)
• ∣gs (Ni(m),m)|
Divn,l,s = maX(Divn-,Divnis) = VnJgl (Nirn,n)gs (Nm,m
Ni
Recall the mapping function of default-plugin-estimator of polynomial GPK fφ (Si,n, Si,m) :=
Plo=0Pso=0αl,sgl(Ni(n),n)gs(Ni(m),m)
Apparently fφ is a monotonic function with respect to Ni and Mi , thus condition 1. for Theorem 3.
is satisfied
Since we also have:
oo
∖fφ(SN(n) ±i,n，SN(m),m) - fΦ(SNi ,nι，SMi,mι )| ≤ ££ |al,sDivn,l,s |
l=0 s=0
oo
=XX lαl,s | —(nJ gl (Nnn, n)gs(Nim , m)
l=0 s=0	Ni
thus condition 2. for Theorem 3. is satisfied
□
12
Under review as a conference paper at ICLR 2021
A.4 proof of corollery 3.4
Proof. Define T = supNi∈cN Ngp(Ni,n) = SNgρ(SN,n), Φn,m = max{Φnι,m1 , Φm2 ,n2 }.
Since We have SgP(Nn) = gρ(N, M), We could get
GPKE F ,φρ,X,Y ] < Pij∈[k] (gρ(N"n) -gρ(Nri),n)) Kij (gρ(Mn*n) — gp(Njf)
λ∕Z	KmaxT φn,m √2n
Pij∈[k] gNΓ1,n)-gρ(Nm1,n) Kj (gρ(N产 n) - gρ(Njm2,n))
KmaxSN gρ(SN,n) Pi∈[k] ∣gρ(Nin),n) - gρ(Ni(m),n)| √2n
SnPij∈[k] ]gρ(N”,SN) — gρ(Ni(m1,SN)) Kj (gρ(Nj(n2),SN) - gρ(N^,Sn))
κmaxρ√2n ∙ Pi∈[k] ∣gρ (Ni(2 SN ) - gρ (Ni(叫 SN )|
□
A.5 details of Remark 3.1
GPKE[F,φl,X,Y]= XX g1(Ni(n1),n1)-g1(Ni(m1),m1) Kij g1(Nj(n2),n2)-g1(Nj(m2),m2)
i∈[k] j∈[k]
ΣΣ
i∈[k] j∈[k]
m1 m2	n1 n2	m1 n2	m2 n1
	XX k (χi,xj) +	XX k (yi,y)	XX k (Xi,yj)	XX k (xi, yj)
m1m2----------------------------------------------------------------n1n2-m1n2-m2n1
i=1 j=1	i=1j =1	i=1 j=1	i=1 j=1
MMDl2 [F, X, Y]
A.6 THE CONVERGENCE BOUNDS OF POWER-MMD WITH ρ = 1
When P = 1, τm = Ngρ(N,nQ = N- ∙ N = * For simplicity consider the case where nι
n2 = m1 = m2 = n, from Corollary 3.1. We have
—na2
Pr(∣GPKE F ,Φι,X,Y ] - E[GPKe [F ,φι ,X,Y ]]| ≥ a) ≤ 2e Kmax2φ2
N(n)	N(m)	N(n)	N(m)
Since φ = Pj∈[k] fφ(Sj,n,Sj,m)l = Pj∈[k] j - N^I ≤ Pj∈[k] IjI + Pj∈[k] INrI
2
We have
22
—na	— na
Pr(IGPKE[F,φι,X,Y] - E[GPKe[F,φι,X,Y]]∣ ≥ a) ≤ 2eKmaχ2φ2 ≤ 2e8κmαX
recall in (Gretton et al., 2012), the deviation bound for linear time estimator MMDl2[F, p, q] is
—na2
Pr(IMMD；[F, X, Y] - EMMD2[F, X,Y]]∣ ≥ a) ≤ 2e18Kmax
Thus our bound generalize the bound in (Gretton et al., 2012) and provide a tighter version.
A.7 IS BOUNDS GET TIGHTER WHEN ρ GETTING LARGER?
As We’ve already knoWn, ρ = 1 case is equivalent to MMDl in (Gretton et al., 2012), one question
rises: Would the performance of cases ρ > 1 better than Widely used ρ = 1 case?
13
Under review as a conference paper at ICLR 2021
22
—na	—na
According to Corollary 3.2., since e 8Kmaχ ≤ e 8ρ2Kmaχ, the convergence bounds for ρ > 1 cases
seem looser than ρ = 1 case, and this may give a negative answer to the question above.
However, the bound above is based on the worst cases where Supi(Ni) = n, such that Tn ≤ P and
Φ ≤ 2. In practice, we are less likely to come across such a phenomena, instead, we may assume
the supi(Ni) to be far smaller.
Without loss of generality, assume We have max( SUPp(N-), SuPp(Ni-)) ≤ 1, where ɑ ≥ 1, it is
easily seen:
T ≤ PN-1 ≤ ɪ
n n nρ-1	αρ-1n
and
Φn,m ≤
i∈[k]
Nn 丫+X( Nm !ρ ≤ 2
nm
i∈[k]
define T := max(Tn1,Tn2,Tm1,Tm2) andΦ := max(Φn1,m1, Φm2,n2)
We have
Z ≤ 4nτ2φ2Kmax ≤ 16ɑ2P-2n ((a) + (1- α)) Kmax= Zb
Plotting the Zb value with respect to variety value of ρ and α in Fig. 1, we can see that for α = 1,
the bound will be looser given larger ρ. However, for α larger than around 1.25, which means the
supi (Ni) is slightly smaller than the sample size, the bound will become tighter when ρ is large.
This illustrate the benefit of using power-MMD with larger ρ in practice.
We could also get a tighter bound according to Corollary 3.1. Practically, it will be
much more beneficial to calculate the Tn = supi∈[k] (^Pn)gp(Ni(rn),n)^ and Φn,m =
Pi∈[k] gρ(Ni(n), n) -gρ(Ni(m), m) on the fly. That is to say, we do not estimate the convergence
bounds before we receive the samples, instead, the calculation of the bounds is carried out together
with the calculation of default-plugin-estimators. Remarkably this is still a distribution-free bounds,
since we make no assumptions on the probability functions we apply hypothesis test upon.
However, the issue is although Z decreases when ρ increases, GPKE [F, φP, X, Y] also decreases
when P increases. It is not clear how P will influence the value of I GPKE√^ρ,X,Y] I.
Figure 1: log(Zb) with respect to variety of ρ and α
14