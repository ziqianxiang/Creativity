Under review as a conference paper at ICLR 2021
Wasserstein diffusion on graphs with missing
attributes
Anonymous authors
Paper under double-blind review
Abstract
Many real-world graphs are attributed graphs where nodes are associated
with non-topological features. While attributes can be missing anywhere
in an attributed graph, most of existing node representation learning ap-
proaches do not consider such incomplete information. In this paper, we
propose a general non-parametric framework to mitigate this problem. Start-
ing from a decomposition of the attribute matrix, we transform node features
into discrete distributions in a lower-dimensional space equipped with the
Wasserstein metric. On this Wasserstein space, we propose Wasserstein
graph diffusion to smooth the distribution representations of nodes with
information from their local neighborhoods. This allows us to reduce the
distortion caused by missing attributes and obtain integrated representations
expressing information of both topology structure and attributes. We then
pull the nodes back to the original space and produce corresponding point
representations to facilitate various downstream tasks. To show the power of
our representation method, we designed two algorithms based on it for node
classification (with missing attributes) and matrix completion respectively,
and demonstrate their effectiveness in experiments.
1	Introduction
Many real-world networks are attributed networks, where nodes are not only connected with
other nodes, but also associated with features, e.g., social network users with profiles or
keywords showing interests, Internet Web pages with content information, etc. Learning node
representations underlie various downstream graph-based learning tasks and have attracted
much attention (Perozzi et al., 2014; Grover & Leskovec, 2016; Pimentel et al., 2017; Duarte
et al., 2019). A high-quality node representation is able to express node-attributed and
graph-structured information and can better capture meaningful latent information.
Random walk based graph embedding approaches (Perozzi et al., 2014; Grover & Leskovec,
2016) exploit graph structure information to preserve pre-specified node similarities in the
embedding space and have proven successful in various applications based on plain graphs. In
addition, graph neural networks, many of which base on the message passing schema (Gilmer
et al., 2017), aggregate information from neighborhoods and allow us to incorporate attribute
and structure information effectively. However, most of the methods, which embed nodes
into a lower-dimensional Euclidean space, suffer from common limitations: they fail to model
complex patterns or capture complicated latent information stemming from the limited
representation capacity of the embedding space. There has recently been a tendency to
embed nodes into a more complex target space with an attempt to increase the ability
to express composite information. A prominent example is Wasserstein embedding that
represents nodes as probability distributions (Bojchevski & Giinnemann, 2018; Muzellec &
Cuturi, 2018; Frogner et al., 2019) equipped with Wasserstein metric. A common practice is
to learn a mapping from original space to Wasserstein space by minimizing distortion while
the ob jective functions are usually difficult to optimize and require expensive computations.
On the other hand, most representation learning methods highly depend on the completeness
of observed node attributes which are usually partially absent and even entirely inaccessible in
real-life graph data. For instance, in the case of social networks like Facebook and Twitter, in
1
Under review as a conference paper at ICLR 2021
which personal information is incomplete as users are not willing to provide their information
for privacy concerns. Consequently, presentation learning models that require fully observed
attributes may not be able to cope with these types of real-world networks.
In this paper, we propose a novel non-parametric framework to mitigate this problem.
Starting from a decomposition of the attribute matrix, we transform node features into
discrete distributions in a lower-dimensional space equipped with the Wasserstein metric and
implicitly implement dimension reduction which greatly reduces computational complexity.
Preserving node similarity is a common precondition for incorporating structural information
into representation learning. Based on this, we develop a Wasserstein graph diffusion process
to effectively propagate a node distribution to its neighborho od and contain node similarity
in the Wasserstein space. To some extent, this diffusion operation implicitly compensates for
the loss of information by aggregating information from neighbors. Therefore, we reduce
the distortion caused by missing attributes and obtain integrated node representations
representing node attributes and graph structure.
In addition to produce distribution representations, our framework can leverage the inverse
mapping to transform the node distributions back to node features (point representations).
Experimentally, we show that these node features are efficient node representations and well-
suited to various downstream learning tasks. More precisely, to comprehensively investigate
the representation ability, we examine our framework on node classification concerning two
missing cases: partially missing and entire node attributes missing. Moreover, we adapt our
framework for matrix completion to show our ability to recover absent values.
Contributions. We develop a novel non-parametric framework for node representation
learning to utilize incomplete node-attributed information. The contributions of our frame-
work are: 1. embedding nodes into a low-dimension discrete Wasserstein space through
matrix decomposition; 2. reducing distortion caused by incomplete information and pro-
ducing effective distribution representations for expressing both attributes and structure
information through the Wasserstein graph diffusion process; 3. reconstructing node features
which can be used for various downstream tasks as well as for matrix completion.
2	Background and Related Work
Graph representation learning
In this paper, we focus on learning node representations on attributed graphs. There are
many effective graph embedding approaches, such as DeepWalk (Bojchevski & Giinnemann,
2018), node2vec (Grover & Leskovec, 2016), GenVetor (Duarte et al., 2019), which embed
nodes into a lower-dimension Euclidean space and preserve graph structure while most
of them disregard node informative attributes. So far, there are little attention paid to
attribute information (Yang et al., 2015; Gao & Huang, 2018; Hong et al., 2019). The
advent of graph neural networks (Bruna et al., 2014; Kipf & Welling, 2017; Hamilton et al.,
2017; Velickovic et al., 2017; Gilmer et al., 2017; Klicpera et al., 2019a;b) fill the gap to
some extent, by defining graph convolutional operations in spectral domain or aggregating
neighborhood information in spatial domain. Their learned node representation integrate
both node attributes and graph structure information.
Due to the expressive limitation of Euclidean space, embedding into a more complex target
space has been explored. There are several works to leverage distributions equipped with
Wasserstein distance to model complex data since a probability is well-suited for modeling
uncertainty and flexibility of complex networks. For instance, Graph2Gauss (Bojchevski
& Guinnemann, 2018) represents node as Gaussian distributions such that uncertainty and
complex interactions across nodes are reflected in the embeddings. Similarly, Muzellec &
Cuturi (2018) presented a framework in which point embedding can be thought of as a
particular case of elliptical distributions embedding. Frogner et al. (2019) learn to compute
minimum-distortion embeddings in discrete distribution space such that the underlying
distances of input space are preserved approximately.
Missing attributes
2
Under review as a conference paper at ICLR 2021
The completeness and adequacy of attribute information is a precondition for learning
high-quality node embeddings of an attributed graph. To represent incomplete attributes in
the embeddings, a fundamental method is providing plausible imputations for missing values.
There is a variety of missing value imputation (MVI) techniques such as mean imputation,
KNN imputation (Troyanskaya et al., 2001), softimpute (Hastie et al., 2015), and so on.
As a consequence, the representation capacity of the generated embeddings is inherently
bounded by the reconstruction ability of the imputation methods. As the number of missing
attributes increases, this can lead to distortion problems as well as unstable learning. The
herein proposed method is the first work to compute embeddings of a graph with incomplete
attributes directly.
3	Wasserstein graph diffusion (WGD) framework
In this paper, we propose a non-parametric Wasserstein graph diffusion (WGD) framework
on graph with missing attributes to generate node representations. The WGD framework
(depicted in Fig 1) consists of three main components: 1.space transformation - to embed
nodes into a Wasserstein space; 2. Wasserstein graph diffusion - to update node distribution
representations; 3. inverse transformation - to pull nodes back to the original feature space.
SPaCe Of UmXk
InUeYSe mapping
BaSjS VkXn
Figure 1: The WGD Framework involves transformations among three space: feature space,
principal component space and discrete Wasserstein space. SVD is leveraged to generate
initial node representations Uk0in principal component space which will be transformed to
discrete distributions endowed with Wasserstein distance through a particular reversible
positive mapping. In this discrete Wasserstein space, we utilize Wasserstein barycenter to
aggregate distributional information of h - hop neighbors. In each WGD layer, the updated
node distributions will be transformed back to the principal component space to update
corresponding node representations which is also the input of next layer. The support points
are shared over layers. After L times update, we pull UkL back to the original feature space
through inverse mapping to generate new node representations.
3.1	Preliminary: Wasserstein distance
Wasserstein distance is an optimal transport metric, which measures the distance traveled in
transporting the mass in one distribution to match another. The p-Wasserstein distance
between two distributions μ and V over a metric space X is defined as
Wp (μ, V) =
inf
∖(x,y)~π(μ,ν)
/
X×X
1/p
d(x, y)pdπ(x, y)
where Π(μ, V) is the the set of probabilistic couplings π on (μ, V), d(x,y) is a ground metric
on X . In this paper, we take p = 2. The Wasserstein space is a metric space that endow
probability distributions with the Wasserstein distance.
3
Under review as a conference paper at ICLR 2021
3.2	The space transformation
Space transformation is the first step of our WGD framework, which attempts to transform
node features to discrete distributions endowed with the Wasserstein metric.
A common assumption for matrix completion is that the matrix is low-ranked, i.e. the
features lie in a smaller subspace, and the missing features can be recovered from this
space. Inspired by Alternating Least Square(ALS) Algorithm, a well known missing value
imputation method which follows this assumption and uses SVD to factorize matrix into
low-ranked submatrices, we first decompose the feature matrix X ∈ Rn×m into a principal
component matrix U, an singular value matrix A, and an orthogonal basis matrix V , i.e.
X = UAV >. For dimensionality reduction, we only account for the first k singular vectors
of V :
Uk,Λk,Vk = SVD(X, k),
(1)
here Uk ∈ Rn×k, Vk ∈ Rm×k and Ak ∈ Rk×k. To impute missing entries, ALS alternatively
optimizes these submatries. While our method is not for matrix completion and not need to
optimize Uk and Vk . We aim to generate expressive node representations in the principal
components space where Uk is the initial node embedding matrix. It is worth noting that
such node representations have strong semantic information: each feature dimension is
a basis vector from Vk . Moreover, in a broad sense (allowing the existence of negative
frequencies), we can express nodes as general histograms with principal components (the
rows of Uk, noted as row(Uk)), acting as frequencies and basis vectors (the columns of Vk,
noted as col(Vk)) acting as bins. Therefore, to capture the ground semantic information, we
transform Uk from a Euclidean space to a discrete distribution space. More precisely, this
space transformation involves a reversible positive function (here we use the exponential
function for implementation) and a normalization operation:
Uk := φ(Uk) = Normalize(exp(row(Uk))).
(2)
Here, Uk can be seen as the discrete distributions of nodes which have common support
points col(Vk) with the notation supp(Uk), i.e. supp(Uk) = col(Vk). Each row of Uk is a
discrete distribution with the form ui = j aij δvj where aij are weigths summing to 1, and
Vj is a column of Vk as well as a support point. The distance of various support points,
noting that A is the square root of the eigenvalues of X>X, i.e. we define the ground metric
d as follows:
d(Vi,vj) =k X(Vi-Vj) k2=k Aiivi - AjjVjk2= |A2i - A2j|.
(3)
Here, Vi and Vj refer to the i-th and j-th support points which are mutual orthogonal unit
vectors. In the meantime, we equip node discrete distributions Uk with Wasserstein metric:
W2(Ui,Uj|D) = mintr(DT>) subject to T1 = U%, T>1 = Uj.
(4)
Here D ∈ Rk×k refers to the underlying distance matrix with Dij = d(Vi, Vj).
3.3	The Wasserstein graph diffusion process
Although we produce node distribution representations from space transformation, the
information extracted from such representations is limited and distorted caused by missing
attributes. To reduce the distortion, we carry out the Wasserstein graph diffusion process to
smooth node distributions over its local neighborhood such that valid information extracted
from different node attributes can be shared, i.e. informational complementarity. The
Wasserstein graph diffusion can boil down to an aggregation operation realized by computing
Wasserstein barycenter that is exactly the updated node representations. In this way, both
topology structure information and the aforementioned integrated attribute information are
incorporated into node representations. Note that it is similar to the message aggregation in
graph neural networks, except we do it in a Wasserstein space and introduce no parameters.
We denote the node distribution update process as Barycenter Update:
U = BarycenterRpdate(u, D) := arg inf 1	^X w22(p,u0D),
P | (U)I u0∈N(U)
(5)
4
Under review as a conference paper at ICLR 2021
here N(u) = N(u) ∪ {u}, N(u) refers to the neighbors of u, |N (u)| equals to the degree of
U (with self-loop) and U0 refers to the discrete distribution of node u0. Similar to common
message aggregation on graph, the Wasserstein diffusion process included l times Barycenter
Update can affect l-hop neighbors.
Recall that the set of support points of barycenter, noted as Su , contains all possible
combination of the common support points sUpp(Uk) shared by all initial node distributions:
1	∣n (u)l
SV = {∣,r, ,∣ E xi∖xi ∈ SuPP(Uk )}.	(6)
|N (U)| i=1
Noting that, as the Barycenter UPdate Process goes on, nodes will not share the same suPPort
Points any more and their set of suPPort Points will be larger and larger that dramatically
increase the comPutation. On the other hand, such a free-suPPort Problem is notoriously
difficult to solve. Therefore, we leverage the fixed-suPPort Wasserstein barycenter for the
uPdate that Preserves the initial suPPort sUpp(Uk). Therefore, no matter how many times
we uPdate node distributions, they always share the common suPPort Points. In Practice,
we use Iterative Bregman Projection (IBP) algorithm (Benamou et al., 2015) to obtain the
fixed-suPPort barycenter (see Algorithm 1 ). The Wasserstein diffusion Process is formulated
as follows:
UkO) = φ(Uk); Ukl+1) = BarycentejUpdate(Ukl), D) With fixed SuPP(Uk).	⑺
Through the Wasserstein diffusion Process, we obtain the uPdated node distributions Uk
which have an effective ability to rePresent node attributes and graPh structure.
Algorithm 1 Iterative Bregman Pro jection for Barycenter UPdate
input: discrete distribution matrix Pd×n, distance matrix Dd×d0 , weights vector w, .
1:	initialize: K = exp(-D/), V0 = 1d0 ×n
2:	for i = 1 . . . M do
3:	U	=	-P-
3:	Ui	KVi-ι
4:	pi = exp(log(K>Ui )w)
5:	Vi	=	K毕Ui
6:	end for
7:	output: pM
3.4 The inverse transformation
We first derive an aPProximate inverse transformation from the PresPecified maPPing (2) to
convert the updated distribution representations Uk to the principal component space：
Uk = Gram_Schmidt_Ortho(col(log(Uk))).	(8)
Here Gram_Schmidt_Ortho refers to the Gram-Schmidt Orthogonalization processing used
to hold the semantic structure of the principal component space. In addition, as a side
effect, empirical results show that orthogonalizing node embeddings can efficiently alleviate
over-smoothing problem.
On the other hand, SVD separates the observed attribute information into two parts: one is
extracted by the updated node distributions while the other maintains in the fixed support
points. To generate more expressive node representation incorporating both the two parts of
information, we pull nodes back to the feature space:
X = Uk Λk V>.	(9)
Note that this X is not a matrix completion for the original X , since the elements in X
do not remain the same. Instead, it is a transformation of the representation Uk after our
Wasserstein diffusion process, by combing with neural networks it can be then used for
various downstreaming tasks, as we will show in the next section.
5
Under review as a conference paper at ICLR 2021
4 EMPIRICAL STUDY
As explained in the previous section, our WGD framework can incorporate node attributes
and graph structure to reconstruct node representations in the original space, i.e. the
feature matrix. Therefore, it is well-suited for various downstream learning tasks as well
as matrix completion. In this section, we adapt WGD for node classification tasks and
matrix completion then evaluate the quality of reconstructed node features (representations)
respectively.
4.1	Node classification on graphs with missing attributes
In this section, we apply our framework to node classification tasks on attributed graphs
with missing features. Algorithm 2 summarizes the architecture. Each WGD layer (the outer
loop) contains three main stages: space transformation, Wasserstein diffusion, and inverse
transformation. The explicit formulas of space transformation and inverse transformation
are given by (2) and (8). In the diffusion process, We conduct BarycenteJUPdate ⑺ h times
to aggregate h-hop neighbors probability information and take the mean outputs of each
layer as the updated principal components matrix to avoid over smoothing. We reconstruct
feature matrix (9) (acting as updated node representations) on the original space and feed
them to a tWo-layer MLP for node classification, termed WGD-MLP.
To confirm the importance of the Wasserstein diffusion process, We propose an ablation
frameWork, called SVD-GCN , in Which We skip the diffusion process and remain the other
stages unchanged. The forWard formula of l + 1-th SVD-GCN layer is:
X(l+1) = Uk(l)ΛkVk(l)>, withUk(l), Λk, Vk(l) =SVD(X(l),k).	(10)
Precisely, We leverage loW-rank SVD to factorize the matrix then feed the reconstructed
feature matrix to a tWo-layer GCN (Kipf & Welling, 2017). Furthermore, to shoW that matrix
decomposition is necessary, We provid an additional ablation frameWork, termed WE-MLP.
In this baseline, We directly transform node features to discrete distributions skipping the
stage of matrix decomposition. The forWard formula of l + 1-th WE-MLP layer is:
X (l+1) = log (Barycenter _Update(Normalize(exp( X (l))))).	(11)
After that, We feed the output that is identical to the updated node representations to a
tWo-layer MLP for node classification.
Algorithm 2 WGD adapted for node classification on graphs With missing attributes
input: attribute matrix Xn×m containing initial values, k.
1:	Apply k-rank SVD to X: Uk, Λk, Vk = SVD(X, k)
2:	Ukcl) — Uk
3:	for l = 1 → L do
4:	space transformation: Ukl-I) = NOrmalize(exp(row(Ukl-1))))
5：	Uk0) J UklT)
6:	for i = 1 → h do
7:	Wasserstein diffusion: Uki) =BaryCenter_Update(Uki-I),D)
8:	end for
9:	inverse transformation: Ukl) = Gram_Schmidt_Ortho(col(log(Ukh))))
10:	end for
11： Uk = mean(UkI),…，UkL))
12： X = UkΛkV>
13:	Apply MLP for node classification: Y = MLP(X)
Experimental Settings. In the experiments of node classification, we implicitly evaluate
the ability of WGD to produce high-quality node representations utilizing incomplete
attribute information. Since node attributes of common benchmarks of node classification,
such as citation networks, are usually fully collected, we artificially remove some attributes
at random. To thoroughly and quantitatively evaluate our representation capacity, we
6
Under review as a conference paper at ICLR 2021
conducte experiments on three common node classification benchmarks: Cora, Citeseer
and PubMed in two settings:
a.	Partially Missing Partially missing means some entities of the feature matrix are missing.
Precisely, we randomly remove the values of an attribute matrix in a given proportion (from
10% to 90%).
b.	Entirely Missing Entirely missing means some nodes have complete attributes while
the others have no feature at all. In this case, we remove some rows of the feature matrix in
a given proportion (from 10% to 90%) at random.
We apply various traditional imputation approaches including zero-impute, mean-impute,
soft-impute (Mazumder et al., 2010), and KNN-impute (Batista et al., 2002) to complete
the missing values and use a two-layer GCN to classify nodes based on imputed feature
matrices. We call them ZERO-GCN, MEAN-GCN, SOFT-GCN and KNN-GCN
respectively. Here, zero-impute and mean-impute means that we replace missing values with
zero value or the mean of observed features respectively. Moreover, we set the performance of
GCN only leveraging graph structure, i.e. with identity matrix as feature matrix, as a lower
bound, named GCN_NOFEAT. Moreover, in this situation, We use Label Propagation (LP)
Algorithm as a better low-bound. If the performance of a model is below the low-bounds, it
reflects that the model is unable to utilize incomplete attributes effectively.
In all the benchmark and experimental settings, We fix most of the hyperparameters of WGD-
MLP: 7 WGD layers including twice BarycenteJUPdate on each layer, two-layer MLP with
128 hidden units With 0.5 dropout. We apply rank 64 SVD on the incomplete attribute matrix
in which we replace the missing values with the mean value of observed features. The settings
of GCN used in all baselines and SVD-GCN are the same, i.e. 2 layers with 32 hidden units.
For all models, we use Adam optimizer with 0.01 learning rate and weight decay depending
on different missing cases. We early stop the model training process with patience 100, select
the best performing models based on validation set accuracy, and report the mean accuracy
for 10 runs as the final results. For more experimental details please refer to our codes:
https://anonymous.4open.science/r/3507bfe0-b3b1-4d18-a7b2-eb3643ceedb1
Experimental Results As shown in Fig 2, WGD-MLP outperforms all baselines and
gcn-nofeat ------ LP —zero-gcn —mean-gcn —knn-gcn —soft-gcn —svd_gcn —wgd_mlp
Figure 2: Average accuracy of models in entirely missing (top) and partially missing (bottom)
settings.
achieves significant improvement, especially when the missing rate is over 50%. There is a
clear trend of decreasing of baselines in two types of missing attributes. In contrast, the flat
curve of WGD-MLP reflects our remarkable robustness. On Cora and Citeseer, comparison
of the lower bound provided by GCN_NOFEAT with baselines confirms that imputation
strategies are ineffective and even counterproductive when observed attributes are grossly
7
Under review as a conference paper at ICLR 2021
inadequate. By contrast, WGD always shows a continuously satisfactory performance, even
when 90% of attributes are unavailable. For instance, in both two types of missing data,
the performance of WGD consistently falls by only 10% and 5%, respectively, on Cora and
Pubmed datasets. It illustrates that WGD can significantly reduce informative distortion
caused by missing data and learn effective latent node representations. On the other hand,
the inferior performance of SVD-GCN convincingly demonstrates the efficiency of Wasserstein
diffusion. For WE-MLP, we conduct experiments on Cora and Pubmed with a fixed missing
rate (0.1). Here is the results: Cora: 0.453 (entirely missing), 0.439 (partially missing);
Pubmed: 0.575 (entirely missing), 0.537 (partially missing). It implies a clear benefit of
matrix decomposition in the first step of WGD.
Sensitive Analysis To show how the number of WGD layers L and iterations h of Wasser-
stein diffusion in each layer influence our performance of node classification, we take Cora
as an example and present the results in Fig 3. As we know, many GNN models encounter
the over-smoothing issue when they go deep; however, Figure 3 shows contrary results and
demonstrates that our method can efficiently handle over-smoothing. This is due to two
strategies: self-connection (similar to the strategy in APPNPKlicpera et al. (2019a) ) and
orthogonalization which make nodes different.
Partial missing
L
Whole missing
L
Figure 3: Sensitive analysis for the number of WGD layers L and times h of Wasserstein
diffusion in each layer. The results (node classification accuracy on Cora dataset) show that
our method prevents over-smoothing.
4.2 Multi-Graph Matrix completion
An interesting aspect of the WGD framework is that they allow us to reconstruct feature
matrix with the introduction of reconstruction constrains and neural networks. In this
section, we test the ability of WGD to reconstruct missing values of a multi-graph matrix,
or more precisely, of recommendation systems with additional information including the
similarity of users and items represented as the users graph and the items graph, respectively.
Algorithm 3 summarizes the WGD framework adapted for matrix completion tasks.
Assume that X is the incomplete feature matrix of the users graph, then X> is that of
the items graph. Leveraging k-rank SVD, we have Uk , Λk , Vk = SVD(X, k). Through
the space transformation (2), we obtain φ(Uk), the discrete distributions of users with
supp(Uk) = col(Vk). In the same way, we have Vk, Λk, Uk = SVD(X >, k) and φ(Vk) with
supp(Vk) = col(Uk). Typically, the update of node distributions on one graph will lead to
unexpected changes in the support points of the other graph. However, this does not appear
to be the case in WGD, as the predefined distance matrix D (3) only depends on the fixed
Λk . Therefore, supp(Uk ) and supp(Vk) share a common distance matrix. It implies that,
two Wasserstein diffusion processes formulatd by (5), can simultaneously go on. Informally,
this generalized WGD framework, called the multi-graph WGD, can be thought of as an
overlay of two original WGDs. The difference appears in the last step: in the multi-graph
WGD, we concatenate the outputs of each layer, feed them to a simple MLP and normalize
columns of the learned Uk and Vk to be unit vectors.
Benchmarks. We conduct experiments on two popular matrix completion datasets with
multi-graph: Flixster and MovieLens-100K processed by Monti et al. (2017).
Baselines. We compare our multi-graph WGD framework with some advanced matrix
completion methods, including GRALS (Rao et al., 2015), sRMGCNN (Monti et al.,
2017), GC-MC (Berg et al., 2017), F-EAE (Hartford et al., 2018), and IGMC (Zhang
8
Under review as a conference paper at ICLR 2021
Algorithm 3 multi-graph WGD adapted for matrix completion
input: attribute matrix Xn×m containing initial values, k.
1:	Apply k-rank SVD to X: Uk, Λk, Vk = SVD(X, k)
2:	UkO) - Uk, Vf) 一 Vk
3:	for l = 1 → L do
4:	space transormation: Ukl-I) = Normalize(exp(row(ukl-1))))
5:	space transormation: V(l-1) = Normalize(exp(row(Vk'l-1))))
6:	Uk0) - UklT), V(O) 一 V(IT)
7:	for i = 1 → h do
8:	Wasserstein diffusion: Uki) =BarycenteJUPdate(Uki-I),D)
9:	Wasserstein diffusion: VV(i) =BarycenteJUPdate(V(i-1),D)
10:	end for
11:	inverse transformation: Ukl) = Gram_Schmidt_Ortho(col(log(Ukh))))
12:	inverse transformation: VkD = Gram-Schmidt-Ortho(col(log(V(h))))
13:	end for
14:	Uk = Concat(UkO),…UkL)), Vk = Concat(V(O),…V(L))
15:	Reconstruct Uk Uk = L2_normaIize(MLPU(Uk))
r C T->	I	I T r TΛ	T	7 ■	∕7lJ^T7~i∕T^Λ∖∖
16:	Reconstruct Vk: Vk = L2_normaIize(MLPv(V:))
17:	return X = UkΛkV>
Table 1: RMSE test results on Flixster and MovieLens-100K.
	GRALS	sRMGCNN	GC-MC	F-EAE	IGMC	WGD-MLP
FLIXSTER	1.313	1.179	0.941	0.908	0.872	0.883
ML-100K	0.945	0.929	0.910	0.920	0.905	0.910
& Chen, 2020). GRALS is a graPh regularization method and sRMGCNN is a factorized
matrix model. GC-MC directly aPPlies GCN for link Prediction on the user-item biPartite
graPh. F-EAE and IGMC are inductive matrix comPletion methods without using side
information. The former leverages exchangable matrix layers while the latter focus on local
subgraPhs around each rating and trains a GNN to maP the subgraPhs to ratings.
Experimental Settings and Results. We follow the exPerimental setuP of Monti et al.
(2017) and take the common metric Root Mean Square Error (RMSE) to evaluate the
accuracy of matrix comPletion. We set rank= 10 as the same as sRMGCNN for Flixster and
MovieLens-100K. In addition, we use 4-layer MLP with 160 hidden units in all exPeriments.
We set L = 5, h = 1 in Flixster and L = 7, h = 1 in MovieLens-100K. We train the model
using Adam oPtimizer with 0.001 learning rate. Table 1 Presents the exPerimental results.
As we can see, our WGD-MLP model outPerforms most methods and achieves comParable
Performance as the state of art model IGMC. However, the Parameter numbers in our model
is much less than the GNN based model IGMC.
5 Conclusion
GraPhs with missing node attributes are ubiquitous in the real-world, while most node
rePresentation learning aPProaches have limited ability to caPture such incomPlete informa-
tion. To mitigate this Problem, we introduced matrix decomPosition and Wasserstein graPh
diffusion for rePresentation learning such that observed node features can be transformed
into discrete distributions and diffused along the graPh. We develoPed a general framework
that can Produce high-quality node rePresentations with Powerful ability to rePresent at-
tribute and structure information and adaPted the framework for two aPPlications: node
classification and matrix comPletion. Extensive exPeriments on node classification under two
missing settings verified the Powerful rePresentation caPacity and suPerior robustness of our
framework. Our model also Proves effective to recover missing features in matrix comPletion.
9
Under review as a conference paper at ICLR 2021
References
Gustavo EAPA Batista, Maria Carolina Monard, et al. A study of k-nearest neighbour as an
imputation method. His, 87(251-260):48, 2002.
Jean-David Benamou, Guillaume Carlier, Marco Cuturi, Luca Nenna, and Gabriel Peyre.
Iterative bregman projections for regularized transportation problems. pp. Ann-A1138,
2015.
Rianne van den Berg, Thomas N Kipf, and Max Welling. Graph convolutional matrix
completion. Computing Research Repository, 2017.
Aleksandar Bojchevski and Stephan Gunnemann. Deep gaussian embedding of graphs:
Unsupervised inductive learning via ranking. International Conference on Learning
Representations, 2018.
Joan Bruna, Wo jciech Zaremba, Arthur Szlam, and Yann Lecun. Spectral networks and locally
connected networks on graphs. In International Conference on Learning Representations
(ICLR2014), CBLS, April 2014, pp. http-openreview, 2014.
Leandro Duarte, Jacqueline Souza Lima, Renan Maestri, Vanderlei Debastiani, and
Rosane Garcia Collevatti. Genvectors: An integrative analytical tool for spatial genetics.
pp. 330761, 2019.
Charlie Frogner, Farzaneh Mirzazadeh, and Justin Solomon. Learning embeddings into
entropic wasserstein spaces. International Conference on Learning Representations, 2019.
Hongchang Gao and Heng Huang. Deep attributed network embedding. In International
Joint Conferences on Artificial Intelligence, pp. 3364-3370, 2018.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl.
Neural message passing for quantum chemistry. In Proceedings of the 34th International
Conference on Machine Learning-Volume 70, pp. 1263-1272, 2017.
Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. 2016.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In Advances in Neural Information Processing Systems, pp. 1024-1034, 2017.
Jason Hartford, Devon R Graham, Kevin Leyton-Brown, and Siamak Ravanbakhsh. Deep
models of interactions across sets. International Conference on Machine Learning, 2018.
Trevor Hastie, Rahul Mazumder, Jason D Lee, and Reza Zadeh. Matrix completion and
low-rank svd via fast alternating least squares. pp. 3367-3402, 2015.
Richang Hong, Yuan He, Le Wu, Yong Ge, and Xindong Wu. Deep attributed network
embedding by preserving structure and attribute information. IEEE Transactions on
Systems, Man, and Cybernetics: Systems, 2019.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional
networks. International Conference on Learning Representations, 2017.
Johannes Klicpera, Aleksandar Bojchevski, and Stephan Giinnemann. Predict then propagate:
Graph neural networks meet personalized pagerank. In International Conference on
Learning Representations, 2019a.
Johannes Klicpera, Stefan WeiBenberger, and Stephan Gunnemann. Diffusion improves
graph learning. In Advances in Neural Information Processing Systems, pp. 13354-13366,
2019b.
Rahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral regularization algorithms
for learning large incomplete matrices. The Journal of Machine Learning Research, 11:
2287-2322, 2010.
10
Under review as a conference paper at ICLR 2021
Federico Monti, Michael Bronstein, and Xavier Bresson. Geometric matrix completion with
recurrent multi-graph neural networks. In Advances in Neural Information Processing
Systems, pp. 3697-3707, 2017.
Boris Muzellec and Marco Cuturi. Generalizing point embeddings using the wasserstein
space of elliptical distributions. In Advances in Neural Information Processing Systems,
pp. 10237-10248, 2018.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social
representations. 2014.
Tiago Pimentel, Adriano Veloso, and Nivio Ziviani. Unsupervised and scalable algorithm
for learning node representations. International Conference on Learning Representations,
2017.
Nikhil Rao, Hsiang-Fu Yu, Pradeep K Ravikumar, and Inderjit S Dhillon. Collaborative
filtering with graph information: Consistency and scalable methods. In Advances in Neural
Information Processing Systems, pp. 2107-2115, 2015.
Olga Troyanskaya, Michael Cantor, Gavin Sherlock, Pat Brown, Trevor Hastie, Robert
Tibshirani, David Botstein, and Russ B Altman. Missing value estimation methods for
dna microarrays. pp. 520-525, 2001.
Petar VeliCkovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and
Yoshua Bengio. Graph attention networks. Computing Research Repository, 2017.
Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, and Edward Y Chang. Network
representation learning with rich text information. In International Joint Conferences on
Artificial Intelligence, pp. 2111-2117, 2015.
Muhan Zhang and Yixin Chen. Inductive matrix completion based on graph neural networks.
International Conference on Learning Representations, 2020.
11