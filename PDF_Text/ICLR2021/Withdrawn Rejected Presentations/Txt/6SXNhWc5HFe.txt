Under review as a conference paper at ICLR 2021
Provab le Fictitious Play for General Mean-
Field Games
Anonymous authors
Paper under double-blind review
Ab stract
We propose a reinforcement learning algorithm for stationary mean-field games,
where the goal is to learn a pair of mean-field state and stationary policy that con-
stitutes the Nash equilibrium. When viewing the mean-field state and the policy
as two players, we propose a fictitious play algorithm which alternatively updates
the mean-field state and the policy via gradient-descent and proximal policy opti-
mization, respectively. Our algorithm is in stark contrast with previous literature
which solves each single-agent reinforcement learning problem induced by the it-
erates mean-field states to the optimum. Furthermore, we prove that our fictitious
play algorithm converges to the Nash equilibrium at a sublinear rate. To the best
of our knowledge, this seems the first provably convergent reinforcement learning
algorithm for mean-field games based on iterative updates of both mean-field state
and policy.
1	Introduction
Multi-agent reinforcement learning (MARL) (Shoham et al., 2007; Busoniu et al., 2008; Hernandez-
Leal et al., 2017; Hernandez-Leal et al.; Zhang et al., 2019) aims to tackle sequential decision-
making problems in multi-agent systems (Wooldridge, 2009) by integrating the classical reinforce-
ment learning framework (Sutton & Barto, 2018) with game-theoretical thinking (BaSar & Olsder,
1998). Powered by deep-learning (Goodfellow et al., 2016), MARL recently has achieved striking
empirical successes in games (Silver et al., 2016; 2017; Vinyals et al., 2019; Berner et al., 2019;
Schrittwieser et al., 2019), robotics (Yang & Gu, 2004; Busoniu et al., 2006; Leottau et al., 2018),
transportation (Kuyer et al., 2008; Mannion et al., 2016), and social science (Leibo et al., 2017;
Jaques et al., 2019; Cao et al., 2018; McKee et al., 2020).
Despite the empirical successes, MARL is known to suffer from the scalability issue. Specifically,
in a multi-agent system, each agent interacts with the other agents as well as the environment, with
the goal of maximizing its own expected total return. As a result, for each agent, the reward function
and the transition kernel of its local state also involve the local states and actions of all the other
agents. As a result, as the number of agents increases, the capacity of the joint state-action space
grows exponentially, which brings tremendous difficulty to reinforcement learning algorithms due
to the need to handle high-dimensional input spaces. Such a curse of dimensionality due to having
a large number of agents in the system is named as the “curse of many agents” (Sonu et al., 2017).
To circumvent such a notorious curse, a popular approach is through mean-field approximation,
which imposes symmetry among the agents and specifies that, for each agent, the joint effect of all
the other agents is summarized by a population quantity, which is oftentimes given by the empirical
distribution of the local states and actions of all the other agents or a functional of such an empirical
distribution. Specifically, to obtain symmetry, the reward and local state transition functions are
the same for each agent, which are functions of the local state-action and the population quantity.
Thanks to mean-field approximation, such a multi-agent system, known as the mean-field game
(MFG)(HUang et al., 2003; Lasry & Lions, 2006a;b; 2007; Huang et al., 2007; GUeant et al., 2011;
Carmona & Delarue, 2018), is readily scalable to an arbitrary number of agents.
In this work, we aim to find the Nash equilibrium (Nash, 1950) of MFG with infinite number of
agents via reinforcement learning. By mean-field approximation, such a game consists of a pop-
ulation of symmetric agents among which each individual agent has infinitesimal effect over the
1
Under review as a conference paper at ICLR 2021
whole population. By symmetry, it suffices to find a symmetric Nash equilibrium where each agent
adopts the same policy. Under such consideration, we can focus on a single agent, also known as
the representative agent, and view MFG as a game between the representative agent’s local policy
π and the mean-field state L which aggregates the collective effect of the population. Specifically,
the representative agent π aims to find the optimal policy when the mean-field state is fixed to L,
which reduces to solving a Markov decision process (MDP) induced by L. Simultaneously, we aim
to let L be the mean-field state when all the agents adopt policy π. The Nash equilibrium of such a
two-player game, (π*, L*), yields a symmetric Nash equilibrium π* of the original MFG.
Under proper conditions, the Nash equilibrium (∏*, L*) can be obtained via fixed-point updates,
which generate a sequence {πt, Lt } as follows. For any t ≥ 0, in the t-th iteration, we solve the
MDP induced by Lt and let πt be the optimal policy. Then we update the mean-field state by letting
Lt+1 be the mean-field state obtained by letting every agent follow πt . Under appropriate assump-
tions, the mapping from Lt to Lt+1 is a contraction and thus such an iterative algorithm converges
to the unique fixed-point of such a contractive mapping, which corresponds to L (Guo et al., 2019).
Based on the contractive property, various reinforcement learning methods are proposed to approxi-
mately implement the fixed-point updates and find the Nash equilibrium (∏*, L) (Guo et al., 2019;
2020; Anahtarci et al., 2019b;a; 2020). However, such an approach requires solving a standard
reinforcement learning problem approximately within each iteration, which itself is solved by an
iterative algorithm such as Q-learning (Watkins & Dayan, 1992; Mnih et al., 2015; Bellemare et al.,
2017) or actor-critic methods (Konda & Tsitsiklis, 2000; Haarnoja et al., 2018; Schulman et al.,
2015; 2017). As a result, this approach leads to a double-loop iterative algorithm for solving MFG.
When the state space S is enormous, function approximation tools such as deep neural networks
are equipped to represent the value and policy functions in the reinforcement learning algorithm,
making solving each inner subproblem computationally demanding.
To obtain a computationally efficient algorithm for MFG, we consider the following question:
Can we design a single-loop reinforcement learning algorithm for solving MFG which updates the
policy and mean-field state simultaneously in each iteration?
For such a question, we provide an affirmative answer by proposing a fictitious play (Brown, 1951)
policy optimization algorithm, where we view the policy π and mean-field state Las the two players
and update them simultaneously in each iteration. Fictitious play is a general algorithm framework
for solving games where each player first infers the opponent and then improves its own policy based
on the inferred opponent information. When it comes to MFG, in each iteration, the policy player π
first infers the mean-field state implicitly by solving a policy evaluation problem associated with π
on the MDP induced by L. Then the policy π is updated via a proximal policy optimization (PPO)
(Schulman et al., 2017) step with entropy regularization, which is adopted to ensure the uniqueness
of the Nash equilibrium. Meanwhile, the mean-field state L obtains its update direction by solving
how the mean-field state evolves when all the agents execute policy π with their state distribution
being L. Then Lis updated towards this direction with some stepsize. Such an algorithm is single-
loop as the mean-field state Lis updated immediately when π is updated.
Furthermore, since Lis a distribution over the state space S, when S is continuous, L lies in an
infinite-dimensional space, which makes it computationally challenging to be updated. To overcome
this challenge, we employ a succinct representation of Lvia kernel mean embedding, which maps
Lto an element in a reproducing kernel Hilbert space (RKHS) (Smola et al., 2007; Gretton et al.,
2008; Sriperumbudur et al., 2010). Such a mechanism enables us to update the mean-field state
within RKHS, which can be computed efficiently.
When the stepsizes for policy and mean-field state updates are properly chosen, we prove that our
single-loop fictitious play algorithm converges to the entropy-regularized Nash equilibrium at a sub-
linear Oe(T-1/5)-rate, where T is the total number of iterations and Oe(∙) hides logarithmic terms.
To our best knowledge, we establish the first single-loop reinforcement learning algorithm for mean-
field game with finite-time convergence guarantee to Nash equilibrium.
Our Contributions. Our contributions are two-fold. First, we propose a single-loop fictitious play
algorithm that updates both the policy and the mean-field state simultaneously in each iteration,
where the policy is updated via entropy-regularized proximal policy optimization. Moreover, we
utilize kernel mean embedding to represent the mean-field states and the policy update subroutine
2
Under review as a conference paper at ICLR 2021
can readily incorporate any function approximation tools to represent both the value and policy
functions, which makes our fictitious play method a general algorithmic framework that is able to
handle MFG with continuous state space. Second, we prove that the policy and mean-field state
sequence generated by the proposed algorithm converges to the Nash equilibrium of the MFG at a
sublinear Oe(T -1/5) rate.
Related Works. Our work belongs to the literature on discrete-time MFG. A variety of works have
focused on the existence of a Nash equilibrium and the behavior of Nash equilibrium as the number
of agents goes to infinity under various settings of MFG. See, e.g., Gomes et al. (2010); Tembine &
Huang (2011); Moon & BaSar (2014); BisWas (2015); Saldi et al.(2018b;a; 2019); Wiecek (2020)
and the references therein. In addition, our work is more related to the line of research that aims
to solve MFG via reinforcement learning methods. Most of the existing Works propose to find the
Nash equilibrium via fixed-point iterations in space of the mean-field states, Which requires solving
an MDP induced by a mean-field state Within each iteration (Guo et al., 2019; 2020; Anahtarci et al.,
2019a;b; Fu et al., 2019; uz Zaman et al., 2020; Anahtarci et al., 2020). Among these Works, Guo
et al. (2019; 2020); Anahtarci et al. (2019a;b; 2020) propose to solve each MDP via Q-learning
Watkins & Dayan (1992) or approximated value iteration (Munos & SzePesvðri, 2008), whereas FU
et al. (2019); uz Zaman et al. (2020) solve each MDP using actor-critic (Konda & Tsitsiklis, 2000)
under the linear-quadratic setting. Furthermore, more closely related works are Elie et al. (2019);
Perrin et al. (2020), which study the convergence of a version of fictitious play for MFG. Similar to
our algorithm, their fictitious play also regards the policy and the mean-field state as the two players.
However, for policy update, they compute the best response policy to the current mean-field state by
solving the MDP induced by the mean-field state to approximate optimality, and the obtained policy
is added to the set of previous policy iterates to form a mixture policy. As a result, their algorithm
is double-loop in essence due to solving an MDP in each iteration. In contrast, our fictitious play is
single-loop — the policy is updated via a single PPO step in each iteration, and the mean-field state
is updated before the policy solves any MDP associated with a mean-field state.
Notations. We use ∣∣∙kι to denote the vector 'ι-norm, and ∆(D) the probability simplex over
D. The Kullback-Leibler (KL) divergence between p1,p2 ∈ ∆(A) is defined as DKL(p1 kp2) :=
Pa∈A pι(a) log P1(a). Let 1n ∈ Rn denote the all-one vector. For two quantities X and y that may
depend on problem parameters (|A|, γ, etc.), if x ≥ Cy holds for a universals constant C > 0, we
write x & y, X = Ω(y) and y = O(x). We use O(∙) to denote O(∙) ignoring logarithmic factors.
2	Background and Preliminaries
In this section, we first review the standard setting of mean-field games (MFG) from Guo et al.
(2019), and then introduce a more general MFG with mean embedding and entropy regularization.
2.1	Mean-Field Games
Consider a discrete-time Markov game involving an infinite number of identical and interchangeable
agents. Let S ⊆ Rd and A ⊆ Rp be the state space and action space, respectively, that are common
to the agents. We assume that S is compact and A is finite. The reward and the state dynamic
for each agent depend on the collective behavior of all agents through the mean-field state, i.e., the
distribution of the states of all agents. As the agents are homogeneous and interchangeable, one can
focus on a single agent representative of the population. Let r : S × A × ∆(S) → [0, Rmax] be the
(bounded) reward function and P: S × A × ∆(S) → ∆(S) be the state transition kernel. At each
time t, the representative agent is in state st ∈ S, and the probability distribution of st, denoted by
Lt ∈ ∆(S), corresponds to the mean-field state. Upon taking an action at ∈ A, the agent receives
a reward r(st, at, Lt) and transitions to a new state st+ι 〜 P(∙∣st, at, Lt). A Markovian policy for
the agent is a function π : S → ∆(A) that maps her own state to a distribution over actions,1 i.e.,
π(a∣s) is the probability of taking action a in state s. Let Π be the set of all Markovian policies.
1In general, the policy may be a function of the mean-field state Lt as well. We have suppressed this
dependency since our ultimate goal is to find a stationary equilibrium, under which the mean-field state remains
fixed over time. See Guo et al. (2019); Saldi et al. (2018b) for a similar treatment.
3
Under review as a conference paper at ICLR 2021
When an agent is operating under a policy π ∈ Π and the mean-field population flow is L :=
(Lt)t≥0, we define the expected cumulative discounted reward (or value function) of this agent as
Vπ(s,L) := EPt∞=0γtr(st, at, Lt) | s0 = s,
where at 〜 ∏(∙∣st), st+ι 〜 P(∙∣st, at, Lt), and Y ∈ (0,1) is the discount factor. The goal of this
agent is to find a policy π that maximizes Vπ(s, L) while interacting with the mean-field L.
We are interested in finding a stationary (time-independent) Nash Equilibrium (NE) of the game,
which is a policy-population pair (π*, L*) ∈ Π X ∆(S) satisfying the following two properties:
•	(Agent rationality) Vπ* (s, L*) ≥ Vπ(s, L*),∀π ∈ Π,s ∈S.
•	(Population consistency) Lt = L*, ∀t under policy ∏* with initial mean-field state Lo = L.
That is, ∏* is the optimal policy under the mean-field L*, and L* remains fixed under ∏*. We
formalize the notion ofNE in Section 2.3 after introducing a more general setting of MFG.
2.2	Mean Embedding of Mean-Field States
Note that the mean-field state L* is a distribution over the states. When the state space is con-
tinuous, the NE (∏*, L*) is an infinite dimensional object, posing challenges for learning the NE.
To overcome this challenge, we make use of a succinct representation of the mean-field via mean
embedding, which embeds the mean-field states into a reproducing kernel Hilbert space (RKHS)
(Smola et al., 2007; Gretton et al., 2008; Sriperumbudur et al., 2010). Specifically, given a positive
definite kernel k : S×S → R, let H be the associated RKHS endowed with the inner product h∙, )H
and norm ∣∣∙∣∣h∙ For each L ∈ ∆(S), its mean embedding μc ∈ H is defined as
μc(s) := Ex〜L [k(x, s)],	∀s ∈ S.
Let M := {μL : L ∈ ∆(S)} ⊆ H be the set of all possible mean embeddings. Note that when k is
the identity kernel, we have μc = L and M = ∆(S). On the other hand, when k is more structured
(e.g., with a fast decaying eigen spectrum), M has significantly lower complexity than the set ∆(S)
of raw mean-field states.
We assume that the MFG respects the mean embedding structure, in the sense that the reward r : S ×
A × M → [0, Rmax] and transition kernel P : S × A × M → ∆(S) (with a slight abuse of notation)
depend on the mean-field state L through its mean embedding representation μc. In particular, at
each time t with state St and mean-field state Lt, the representative agent takes action at 〜 ∏(∙∣st),
receives reward r(st,at, μcj and then transitions to a new state st+ι 〜 P(∙∣st, at, μcj. The NE
of the game is defined analogously. As mentioned, when k is the identity kernel, the above setting
reduces to the standard setting in Section 2.1 with raw-mean field states.
We impose a standard regularity condition on the kernel k .
Assumption 1. The kernel k is bounded and universal, in the sense that k(s, s) ≤ 1, ∀s ∈ S and
the corresponding RKHS H is dense w.r.t. the L∞ norm in the space of continuous functions on S.
Assumption 1 is standard in the kernel learning literature (Caponnetto & De Vito, 2007; Muandet
et al., 2012; Szab6 et al., 2015; Lin et al., 2017). When the kernel is bounded, the embedding of
each L ∈ ∆(S) satisfies ∣∣μL∣∣H ≤ Jxy kk(x,∙)kH dx ≤ 1. When one uses a universal kernel (e.g.,
Gaussian or Laplace kernel), the mean embedding mapping is injective and hence each embedding
μ ∈ M uniquely characterizes a distribution L in ∆(S) (Gretton et al., 2008; 2012).
2.3	Entropy Regularization
To ensure the uniqueness of the NE and achieve fast algorithmic convergence, we use an entropy
regularization approach (Cen et al., 2020; Shani et al., 2019; Nachum et al., 2017), which augments
the standard expected reward objective with an entropy term of the policy. In particular, we define
the entropy-regularized value function as
∞
Vλ,π (S):= Eat〜π(∙∣st),st+ι〜P(∙∣st,at,μ) J EYt[r(st,at,μ) 一 λ log n(at|st)] | s0 = s ,
t=0
where the parameter λ > 0 controls the regularization level and μ is the mean-embedding of some
given mean-field state (fixed over time). Equivalently, one may view Vμλ,π as the usual value function
4
Under review as a conference paper at ICLR 2021
of π with an entropy-regularized reward
rλ,π(s, a) := r(s, a, μ) — λlogπ(a∣s),	∀s ∈ S,a ∈ A.	(1)
Also define the Q-function of a policy π as
Qλ,π(S, a) = r(S, a, μ) + γE [Vμλ,π(SI) | so = s,ao = a],	⑵
which is related to the value function as
Vjn(s) = Ea〜∏(∙∣s) [Qμ,π(s, a)—λlog∏(a∣s)] = gμ,π(Sq, 双忖〉+H 阿忖),⑶
where H (π(∙∣s)) := — Pa π(a∣s) log π(a∣s) is the Shannon entropy of the distribution π(∙∣s). Since
the reward function r is assumed to be Rmax -bounded, it is easy to show that the Q-function is also
bounded as ∣∣Qλ,π∣∣∞ ≤ Qmax ：= (Rmax + γλlog ∣A∣)∕(1 — Y); SeeLemma 5.
Single-Agent MDP. When the mean-field state and its mean-embedding remain fixed over time,
i.e., Lt = L and 祖匚七=μ, ∀t, a representative agent aims to solve the optimization problem
max Vλ,π (S)	(4)
∏iS→∆(A) μ
for each S ∈ S. This problem corresponds to finding the (entropy-regularized) optimal policy for a
single-agent discounted MDP, denoted by MDPμ := (S, A, P(∙∣∙, ∙,μ), r(∙, ∙, μ), γ), that is induced
by μ ∈ M. Let ∏λ,* be the optimal solution to the problem (4), that is, the optimal regularized
policy of MDPμ. The optimal policy is unique whenever λ > 0. One can thus define a mapping
Γλ : M → Π via Γλ(μ) = ∏j,*, which maps each embedded mean-field state μ to the optimal
regularized policy ∏j,* of MDP*. Let Qj,* be the optimal regularized Q-function corresponding to
the optimal policy ∏λ,*.
Throughout the paper, we fix a state distribution ν0 ∈ ∆(S), which will serve as the initial state of
our policy optimization algorithm. For each μ ∈ M and a policy π : S → ∆(A), define
Jλ(π):= Es“0 [Vμλ,π (s)]	(5)
as the expectation of the value function Vμλ,π (s) of policy ∏ on the regularized MDP*. We define
the discounted state visitation distribution ρμ induced by a policy ∏ on MDPμ as:
∞
ρμ (S)=(I-Y) X Y tP(St = s),	⑹
where P(St = s) is the state distribution when s0 +V° and the actions are chosen according to ∏.
Mean-field Dynamics. When all agents follow the same policy π, we can define another mapping
Γ2 : Π × M → M that describes the dynamic of the embedded mean-field state. In particular, given
the current embedding μ corresponding to some mean-field state L, the next embedded mean-field
state μ+ = Γ2 (π, μ) is given by
L+(S0) =	XL(S)π(a∣S)P(S0∣S,a,μ)dS,	μ+ = μc+ .	(7)
S a∈A
Note that the evolution of the mean-field depends on the agents’ policy in a deterministic manner.
Entropy-regularized Mean-field Nash Equilibrium (NE). With the above notations, we can
formally define our notion of equilibrium.
Definition 1. A stationary (time-independent) entropy-regularized Nash equilibrium for the MFG
is a policy-population pair (π*,μ*) ∈ Π XM that satisfies
(agent rationality)	π* = Γ) (μ*),
(population consistency)	μ* = Γ2 (π*, μ*).
When λ = 0, the above definition reduces to that of the (unregularized) NE discussed in Section 2.1,
which requires ∏ to the unregularized optimal policy of MDP**. For general values of λ, the
regularized NE (∏*, μ*) approximates the unregularized NE (Geist et al., 2019), in the sense that ∏*
is an approximate optimal policy of MDPμ* satisfying
maχ{jμ* (∏)}- jμ* (∏*) ≤ 入 log A∣∕(i—γ).
5
Under review as a conference paper at ICLR 2021
One may further define the composite mapping Λλ : M → M as Λλ(μ) = Γ2 (Γλ(μ),μ) . When
Λλ is a contraction, the regularized NE exists and is unique (Guo et al., 2019). Moreover, the iterates
{(∏t, μt)}t≥0 given by the two-step update
πt = γ 1 (μt),	μt+1 = γ2 (πt, μt)
converge to the regularized NE at a linear rate. Note that the first step above requires an oracle for
computing the exact optimal policy ∏μ,*. In most cases, such an exact oracle is not available; various
single-agent reinforcement learning algorithms have been considered for computing an approximate
optimal policy, including Q-learning (Guo et al., 2019) and policy gradient methods (Guo et al.,
2020; Subramanian & Mahajan, 2019). The recent work by Elie et al. (2019) considers fictitious
play iterative learning scheme. We remark that their convergence guarantee requires being able to
compute the approximate optimal policy to an arbitrary precision with high probability.
3	Fictitious Play Algorithm for MFG
In this section, We present a fictitious play algorithm, which simultaneously estimates the policy ∏*
and the embedded mean-field state μ* of the NE. As given in Algorithm 1, each iteration of the
algorithm involves three steps: policy evaluation (line 3), policy improvement (line 4), and updating
the embedded mean-field state (line 5). Below we explain each step in more details.
Algorithm 1 Mean-Embedded Fictitious Play
1:	Input: initial estimate (∏o, μo), step size sequence {αt, βt}t≥0, mixing parameter η.
2:	for Iteration t = 0, 1, 2, . . . , T - 1 do
3:	(Policy evaluation step) Compute an approximate version Qbtλ : S × A → [0, Qmax] of the
Q-function Qλtπt of policy ∏t with respect to the entropy-regularized MDPμt
4:	(Policy improvement step) Update the policy by
bt+ι(∙∣s) X (∏t(∙∣s))1-αtλexp (αtQλ(s, ∙))	(8)
πt+ι("S) = (I- n)bt+i(•⑸ + η∙ 1∣A∣(∙”lAl	⑼
5:	Update the embedded mean-field state by
μt+ι = (I- βt)μt + βt ∙ r2(πt+ι,μt).	(IO)
6:	end for
7:	Output: {(∏t,μt)}t=ι …T
Policy Evaluation. In each iteration, we first evaluate the current policy πt with respect to the
regularized single-agent MDPμt induced by the current mean-field estimate μt. In particular, we
compute an approximation Qtλ of the true Q-function Qjλ := Qλtπt, which can be done using,
e.g., TD(0) or LSTD methods. Our theorem characterizes how convergence depends on the policy
evaluation error in this step.
Policy Improvement. To update our policy estimate πt, we first compute an intermediate policy
πbt+1 by a single policy improvement step: for each S ∈ S,
bt+ι(∙∣s) = argmax ]αt(<Qλ(s, ∙) - λlog∏t(∙∣s),π(∙∣s) - ∏t(∙∣s)) - DKL (∏(∙∣s)k∏t(∙∣s))}，
∏(∙∣s)∈∆(A) l	J
(11)
where αt > 0 is the stepsize. This step corresponds to one iteration of Proximal Policy Optimization
(PPO) (Schulman et al., 2017). It can also be viewed as one mirror descent iteration, where the
shifted Q-function Qλ(s, ∙) 一 λ log∏t(∙∣s) plays the role of the gradient. The maximizer ∏t+ι in
equation (11) can be computed in closed form as done in equation (8) in Algorithm 1. We then
compute the new policy πt+1 by mixing πbt+1 with a small amount of uniform distribution, as done
in equation (9). “Mixing in” a uniform distribution is a standard technique to prevent the policy from
approaching the boundary of the probability simplex and becoming degenerate. Doing so allows us
to upper bound a quantity of the form DKL (p k ∏t+ι(∙∣s)) (cf. Lemma 2), which otherwise may be
infinite. It also ensures that the KL divergence satisfies a Lipschitz condition (cf. Lemma 3).
6
Under review as a conference paper at ICLR 2021
Mean-field Update. We next compute an updated (embedded) mean-field state μt+ι as a weighted
average of the current μt and the mean-field state Γ2(∏t+1, μt) induced by the new policy ∏t+ι,
namely, μt+ι = (1 - βt)μt + βt ∙ Γ2(∏t+1,μt), where βt ∈ (0,1) is the stepsize. This update can
be viewed as a single step of the (soft) fixed point iteration for the equation μ = Γ2 (∏t+ι ,μ).
We remark that our algorithm is similar to the classical fictitious play approach for finding NEs,
where each agent plays a response to the empirical average of its opponent’s past behaviors. In our
algorithm, the representative agent views the population of all agents collectively as an opponent.
Expanding the recursion (8) and ignoring the difference between πbt+1 and πt+1, we can write the
policy πt+1 as
∏t+ι(∙∣s) (X exp (PT=oWτ QT(s, ∙))
for some positive weights {wτ}. Therefore, the representative agent is playing a policy that responds
to the (weighted) average of all previous Q functions, which reflects the representative agent’s belief
on the aggregate population policy.
Also note that our algorithm only performs a single policy improvement step to compute the updated
policy ∏t+ι. It is unnecessary to compute the exact optimal policy π^+ι = Γλ(μt) under μt (which
would require an inner loop for solving MDP*J, as μt is only an approximate anyway of the true
NE mean-field μ*. Our algorithm updates ∏t and μt simultaneously within a single loop.
4	Main Results
In this section, we establish the theoretical guarantees on learning the regularized NE (∏*,μ*) of
the MFG for our fictitious play algorithm. To state our theorem, we first discuss several regularity
assumptions on the MFG model. Recall the definition (6) of the discounted state visitation distribu-
tion and let ρ* :二 ρ∏ ∈ ∆(S) be the visitation distribution induced by the NE (π*,μ*). We make
use of the following distance metric between two policies π, π0 ∈ Π:
D(π,π') ：= Es〜P* [kπ(1s) - π'(1s)kι] .	(12)
As in the classical MFG literature (Guo et al., 2020; Saldi et al., 2018b), we assume certain Lipschitz
properties for the two mappings Γ1λ : M → Π and Γ2 : Π × M → M defined in Section 2.3. The
first assumption states that Γ)(μ) is Lipschitz in the mean-embedded mean-field state μ with respect
to the RKHS norm.
Assumption 2. There exists a constant di > 0, such thatfor any μ,μ0 ∈ M, it holds that
D (Γλ(μ), Γλ(μ')) ≤ di kμ -洲丸.
The second assumption states that Γ2(∏, μ) is Lipschitz in each of its arguments when the other
argument is fixed.
Assumption 3. There exist constants d2 > 0, d3 > 0 such that for any policies π, π0 ∈ Π and
embedded mean-field states μ, μ0 ∈ M, it holds that
llr2(π,μ) - r2(π',μ)kH ≤ d2D (π,π'),	kr2(π,μ) - r2(π,μ0)kH ≤ d kμ — μkH.
Assumptions 2 and 3 immediately imply Lipschitzness of the composite mapping Λλ : M → M,
which we recall is defined as Λλ(μ) = Γ2 (Γλ(μ),μ) . The proof is provided in Appendix D.1.
Lemma 1. Suppose Assumptions 2 and 3 hold. Thenfor each μ,μ0 ∈ M, it holds that
∣∣Aλ(μ) - Aλ(μ')∣∣h ≤ (dιd2 + d3) kμ - μ'l∣H.
We next impose an assumption on the boundedness of certain concentrability coefficients. This type
of assumption, standard in analysis of policy optimization algorithms (Kakade & Langford, 2002;
Shani et al., 2019; Bhandari & Russo, 2019; Agarwal et al., 2020), allows one to define the policy
optimization error in an average-case sense with respect to appropriate distributions over the states.
Assumption 4 (Finite Concentrability Coefficients). There exist two constants Cρ,CP > 0 such
thatfor each μ ∈ M, it holds that
∣∣ρ7 /ρ*∣∣oo：=SUphpl"，(s"ρ"(s)] ≤Cp and {E 户*h∣ρ*(s"ρμμ,(s)| ]} / ≤CP.
7
Under review as a conference paper at ICLR 2021
Finally, our last assumption stipulates that the state visitation distributions are smooth with respect
to the (embedded) mean-field states of the MFG. This assumption is analogous to those in the liter-
ature on MDP and two-player games (Fei et al., 2020; Radanovic et al., 2019), which requires the
visitation distributions to be smooth with respect to the policy.
Assumption 5. There exists a constant do > 0, such thatfor any μ,μ0 ∈ M, it holds that
∣∣ρμμ - ρμμ ∣∣1 ≤ do kμ - μkH .
We now state our theoretical guarantees on the convergences of the policy-population sequence
{∏t,μt} in Algorithm 1 to the NE {∏*,μ*}. For the estimates of the embedded mean-field states, it is
natural to consider the distance ∣∣μt - μ* k h in RKHS norm. For convergence to NE policy μ*, recall
that μ* is the optimal policy to MDPμ*, and each iteration of our algorithm involves a single policy
improvement step to compute ∏t+ι rather than solving MDP*= to its optimal policy n：+i := Γ) (μt).
As such, We analyze the difference between these two policies in terms of D (∏t+ι,∏¢+ι), where
πt*+1
the metric D is defined in equation (12). Also let PJ= := ρμt+1 denote the discounted visitation
distribution induced by the optimal policy ∏J+ of MDPμ= .2 * With the above considerations in mind,
we have the following theorem, which is proved in Appendix B. .
Theorem 1. Suppose that ASSumPtionS 1-5 hold and d1d2 + d3 < 1 and that the error in the policy
evaluation step in Algorithm 1 satisfies
Es* [∣∣Qλ(s, ∙) - Qλ(s, ∙)∣∣∞] ≤ ε2,	∀t ∈ [T].
With the choice of
η = CηT-1,	αt ≡ α = CaT-2/5, βt ≡ β = cgT-4/5,
for some universal constants cη > 0, cα > 0 and cβ > 0 in Algorithm 1, the resulting policy and
embedded mean-field state Sequence {(∏t, μt)}T=ι satisfy
TT	T
d( T Xπt, T XnD ≤ T χD(πt,πt). √λ ∙ (PlogT ∙T-1/5+√ε),	(⑶
t=1	t=1	t=1	λ
∣1T	1T	1
∣∣ TXμt - μ IIh ≤ Tχ∣μt- μ IIh . √λ
(PlOgT ∙ T τ∕5 +
(14)
Theorem 1 bounds the distance between ∏ and the optimal policy ∏J of MDPμ*. By directly mea-
suring the distance between πt and the NE policy π = , we can define the notion of an δ-approximate
NE of the game.
Definition 2. For each δ > 0, a policy-population pair (∏, μ) is called an δ-approximate (entropy-
regularized) NE of the MFG if
D(∏,∏=) ≤ δ and ∣∣μ — μ*∣∣H ≤ δ.
The following corollary of Theorem 1 shows that after T iterations of our algorithm, the average
policy-population pair (+ PT=I ∏t, 1 PT=I μt) is an O (T-1 /5)-approximate NE.
Corollary 1. Under the assumptions of Theorem 1, we have
TT
d( T Xπt,π*) +1∣ T Xμt -μ*∣∣H. √λ ∙ (Plog T ∙ t-1/5+√ε).
We prove this corollary in Appendix C.
The above results require an '2-error of ε for policy evaluation. A variety of algorithms have been
shown to achieve such a guarantees, including TD(0) and LSTD (Bhandari et al., 2018).
We also remark that the '∞ condition on concentrability coefficient in Assumption 4 can be relaxed
πλ,*	2 1/2
to an '2 condition of the form {E [∣ρμμ (s)∕ρ=(s)∣ ]}	≤ C ρ, under which we can establish an
Oe(T-1/9) convergence rate; see Theorem 2 and Corollary 2 in Appendix E for the details.
2The subscript in P限 emphasizes that P限 only depends on the mean-field state μt at time t through ∏t+ι =
Γλ(μt).
8
Under review as a conference paper at ICLR 2021
References
Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. Optimality and approximation
with policy gradient methods in markov decision processes. In Conference on Learning Theory,
pp. 64-66, 2020.
Berkay Anahtarci, Can Deha Kariksiz, and Naci Saldi. Fitted Q-learning in mean-field games. arXiv
preprint arXiv:1912.13309, 2019a.
Berkay Anahtarci, Can Deha Kariksiz, and Naci Saldi. Value iteration algorithm for mean-field
games. arXiv preprint arXiv:1909.01758, 2019b.
Berkay Anahtarci, Can Deha Kariksiz, and Naci Saldi. Q-learning in regularized mean-field games.
arXiv preprint arXiv:2003.12151, 2020.
Tamer BaSar and Geert Jan Olsder. Dynamic noncooperative game theory. SIAM, 1998.
Marc G Bellemare, Will Dabney, and Remi Munos. A distributional perspective on reinforcement
learning. arXiv preprint arXiv:1707.06887, 2017.
Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, PrzemySIaW Debiak, Christy
Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large
scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.
Jalaj Bhandari and Daniel Russo. Global optimality guarantees for policy gradient methods. arXiv
preprint arXiv:1906.01786, 2019.
Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference
learning With linear function approximation. In Conference On Learning Theory, pp. 1691-1692,
2018.
Anup BisWas. Mean field games With ergodic cost for discrete time markov processes. arXiv preprint
arXiv:1510.08968, 2015.
George W BroWn. Iterative solution of games by fictitious play. Activity analysis of production and
allocation, 13(1):374-376, 1951.
Lucian Busoniu, Bart De Schutter, and Robert Babuska. Decentralized reinforcement learning con-
trol of a robotic manipulator. In 2006 9th International Conference on Control, Automation,
Robotics and Vision, pp. 1-6. IEEE, 2006.
Lucian Busoniu, Robert Babuska, and Bart De Schutter. A comprehensive survey of multiagent rein-
forcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications
and Reviews), 38(2):156-172, 2008.
Kris Cao, Angeliki Lazaridou, Marc Lanctot, Joel Z Leibo, Karl Tuyls, and Stephen Clark. Emergent
communication through negotiation. arXiv preprint arXiv:1804.03980, 2018.
Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm.
Foundations of Computational Mathematics, 7(3):331-368, 2007.
Rene Carmona and FrangoiS Delarue. Probabilistic Theory OfMean Field Games with Applications
I-II. Springer, 2018.
Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi. Fast global convergence of
natural policy gradient methods With entropy regularization. arXiv preprint arXiv:2007.06558,
2020.
Romuald Elie, Julien P6rolat, Mathieu Lauriere, Matthieu Geist, and Olivier Pietquin. On the con-
vergence of model free learning in mean field games. arXiv preprint arXiv:1907.02633, 2019.
Yingjie Fei, Zhuoran Yang, Zhaoran Wang, and Qiaomin Xie. Dynamic regret of policy optimization
in non-stationary environments. arXiv preprint arXiv:2007.00148, 2020.
9
Under review as a conference paper at ICLR 2021
Zuyue Fu, Zhuoran Yang, Yongxin Chen, and Zhaoran Wang. Actor-critic provably finds nash
equilibria of linear-quadratic mean-field games. arXiv preprint arXiv:1910.07498, 2019.
Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision
processes. In ICML 2019-Thirty-sixth International Conference on Machine Learning, 2019.
Diogo A Gomes, Joana Mohr, and Rafael Rigao Souza. Discrete time, finite state space mean field
games. Journal de mathematiquesPures et appliquees, 93(3):308-328, 2010.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bemhard Scholkopf, and Alexander Smola.
A kernel method for the two-sample problem. Journal of Machine Learning Research, 1:1-10,
2008.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola.
A kernel two-sample test. The Journal of Machine Learning Research, 13(1):723-773, 2012.
Olivier Gueant, Jean-Michel Lasry, and Pierre-Louis Lions. Mean field games and applications. In
Paris-Princeton lectures on mathematical finance 2010, pp. 205-266. Springer, 2011.
Xin Guo, Anran Hu, Renyuan Xu, and Junzi Zhang. Learning mean-field games. In Advances in
Neural Information Processing Systems, pp. 4967-4977, 2019.
Xin Guo, Anran Hu, Renyuan Xu, and Junzi Zhang. A general framework for learning mean-field
games. arXiv preprint arXiv:2003.06069, 2020.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-
policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint
arXiv:1801.01290, 2018.
Pablo Hernandez-Leal, Bilal Kartal, and Matthew E Taylor. Is multiagent deep reinforcement learn-
ing the answer or the question? a brief survey. learning, 21:22.
Pablo Hernandez-Leal, Michael Kaisers, Tim Baarslag, and Enrique Munoz de Cote. A sur-
vey of learning in multiagent environments: Dealing with non-stationarity. arXiv preprint
arXiv:1707.09183, 2017.
Minyi Huang, Peter E Caines, and Roland P Malhame. Individual and mass behaviour in large pop-
ulation stochastic wireless power control problems: centralized and nash equilibrium solutions.
In 42nd IEEE International Conference on Decision and Control (IEEE Cat. No. 03CH37475),
volume 1, pp. 98-103. IEEE, 2003.
Minyi Huang, Peter E Caines, and Roland P Malhame. Large-population cost-coupled LQG prob-
lems with nonuniform agents: individual-mass behavior and decentralized ε-Nash equilibria.
IEEE Transactions on Automatic Control, 52(9):1560-1571, 2007.
Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro Ortega, DJ Strouse,
Joel Z Leibo, and Nando De Freitas. Social influence as intrinsic motivation for multi-agent
deep reinforcement learning. In International Conference on Machine Learning, pp. 3040-3049.
PMLR, 2019.
Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
ICML, volume 2, pp. 267-274, 2002.
Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information
processing systems, pp. 1008-1014, 2000.
Lior Kuyer, Shimon Whiteson, Bram Bakker, and Nikos Vlassis. Multiagent reinforcement learning
for urban traffic control using coordination graphs. In Joint European Conference on Machine
Learning and Knowledge Discovery in Databases, pp. 656-671. Springer, 2008.
Jean-Michel Lasry and Pierre-Louis Lions. Jeux a champ moyen. i-le cas stationnaire. Comptes
Rendus Mathematique, 343(9):619-625, 2006a.
10
Under review as a conference paper at ICLR 2021
Jean-Michel Lasry and Pierre-Louis Lions. JeUx a champ moyen. ii-horizon fini et contr6le optimal.
Comptes Rendus Mathematique, 343(10):679-684, 2006b.
Jean-Michel Lasry and Pierre-Louis Lions. Mean field games. Japanese journal of mathematics, 2
(1):229-260, 2007.
Joel Z Leibo, Vinicius Zambaldi, Marc Lanctot, Janusz Marecki, and Thore Graepel. Multi-agent
reinforcement learning in sequential social dilemmas. arXiv preprint arXiv:1702.03037, 2017.
David L Leottau, Javier Ruiz-del Solar, and Robert Babuska. Decentralized reinforcement learning
of robot behaviors. Artificial Intelligence, 256:130-159, 2018.
Shao-Bo Lin, Xin Guo, and Ding-Xuan Zhou. Distributed learning with regularized least squares.
The Journal of Machine Learning Research, 18(1):3202-3232, 2017.
Patrick Mannion, Jim Duggan, and Enda Howley. An experimental review of reinforcement learning
algorithms for adaptive traffic signal control. In Autonomic road transport support systems, pp.
47-66. Springer, 2016.
Kevin R McKee, Ian Gemp, Brian McWilliams, Edgar A DuCEez-Guzmdn, Edward Hughes, and
Joel Z Leibo. Social diversity and social preferences in mixed-motive reinforcement learning.
arXiv preprint arXiv:2002.02325, 2020.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529-533, 2015.
Jun Moon and Tamer Bayar. Discrete-time lqg mean field games with unreliable communication. In
53rd IEEE Conference on Decision and Control, pp. 2697-2702. IEEE, 2014.
Krikamol Muandet, Kenji Fukumizu, Francesco Dinuzzo, and Bernhard Scholkopf. Learning from
distributions via support measure machines. In Advances in neural information processing sys-
tems, pp. 10-18, 2012.
RCmi Munos and Csaba Szepesvdri. Finite-time bounds for fitted value iteration. Journal of Machine
Learning Research, 9(May):815-857, 2008.
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between
value and policy based reinforcement learning. In Advances in Neural Information Processing
Systems, pp. 2775-2785, 2017.
John F Nash. Equilibrium points in n-person games. Proceedings of the National Academy of
Sciences of the United States of America, 36(1):48-49, 1950.
Sarah Perrin, Julien PCrolat, Mathieu Lauriere, Matthieu Geist, Romuald Elie, and Olivier Pietquin.
Fictitious play for mean field games: Continuous time analysis and applications. arXiv preprint
arXiv:2007.03458, 2020.
Goran Radanovic, Rati Devidze, David C Parkes, and Adish Singla. Learning to collaborate in
markov decision processes. arXiv preprint arXiv:1901.08029, 2019.
Naci Saldi, Tamer Basar, and Maxim Raginsky. Discrete-time risk-sensitive mean-field games.
arXiv preprint arXiv:1808.03929, 2018a.
Naci Saldi, Tamer Basar, and Maxim Raginsky. Markov-nash equilibria in mean-field games with
discounted cost. SIAM Journal on Control and Optimization, 56(6):4256-4287, 2018b.
Naci Saldi, Tamer Basyar, and Maxim Raginsky. Approximate nash equilibria in partially observed
stochastic games with mean-field interactions. Mathematics of Operations Research, 44(3):1006-
1033, 2019.
Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon
Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari,
go, chess and shogi by planning with a learned model. arXiv preprint arXiv:1911.08265, 2019.
11
Under review as a conference paper at ICLR 2021
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889-1897, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Lior Shani, Yonathan Efroni, and Shie Mannor. Adaptive trust region policy optimization: Global
convergence and faster rates for regularized mdps. arXiv preprint arXiv:1909.02769, 2019.
Yoav Shoham, Rob Powers, and Trond Grenager. If multi-agent learning is the answer, what is the
question? Artificial intelligence, 171(7):365-377, 2007.
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, and Marc Lanctot. Master-
ing the game of Go with deep neural networks and tree search. Nature, 529(7587):484, 2016.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, and Adrian Bolton. Mastering the game of Go
without human knowledge. Nature, 550(7676):354-359, 2017.
Alex Smola, Arthur Gretton, Le Song, and Bernhard Scholkopf. A hilbert space embedding for
distributions. In International Conference on Algorithmic Learning Theory, pp. 13-31. Springer,
2007.
Ekhlas Sonu, Yingke Chen, and Prashant Doshi. Decision-theoretic planning under anonymity in
agent populations. Journal of Artificial Intelligence Research, 59:725-770, 2017.
Bharath K Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Bernhard Scholkopf, and Gert RG
Lanckriet. Hilbert space embeddings and metrics on probability measures. The Journal of Ma-
chine Learning Research, 11:1517-1561, 2010.
Jayakumar Subramanian and Aditya Mahajan. Reinforcement learning in stationary mean-field
games. In Proceedings of the 18th International Conference on Autonomous Agents and MultiA-
gent Systems, pp. 251-259, 2019.
Richard S Sutton and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.
Zoltdn Szab6, Arthur Gretton, Barnabgs P6czos, and Bharath Sriperumbudur. Two-stage sampled
learning theory on distributions. In Artificial Intelligence and Statistics, pp. 948-957, 2015.
Hamidou Tembine and Minyi Huang. Mean field difference games: Mckean-vlasov dynamics. In
2011 50th IEEE Conference on Decision and Control and European Control Conference, pp.
1006-1011. IEEE, 2011.
Muhammad Aneeq Uz Zaman, Kaiqing Zhang, Erik Miehling, and Tamer Bayar. Approximate
equilibrium computation for discrete-time linear-quadratic mean-field games. In 2020 American
Control Conference (ACC), pp. 333-339. IEEE, 2020.
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, Juny-
oung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster
level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279-292, 1992.
Piotr Wiecek. Discrete-time ergodic mean-field games with average reward on compact spaces.
Dynamic Games and Applications, 10(1):222-256, 2020.
Michael Wooldridge. An introduction to multiagent systems. John Wiley & Sons, 2009.
Erfu Yang and Dongbing Gu. Multiagent reinforcement learning for multi-robot systems: A survey.
Technical Report CSM 04, 2004.
Kaiqing Zhang, Zhuoran Yang, and Tamer Basyar. Multi-agent reinforcement learning: A selective
overview of theories and algorithms. arXiv preprint arXiv:1911.10635, 2019.
12
Under review as a conference paper at ICLR 2021
Appendices
A Technical Lemmas
Lemma 2. Let p* and P ∈ ∆(A) and pb =
—η)p + η -∣Aaai . Then
DKL (p*kp) ≤ log A,
DKL (p*kb)- DKL (p*kp) ≤ 2η.
Proof. By definition we have
Dkl (p*kP) = X P*(a)log 修
=I i;ll
≤ X p*(6log0+⅛
a∈A	0+ 冈
= log |A|,
η
thereby proving the first inequality.
Note that
DKL (p*kp) - DKL (p*kP)= £p*(a)log (p(0)).
(15)
If ba) ≤1 forall a ∈ Athenwehave
Dkl (P*kb)- Dkl (p*kp) ≤ 0;
otherwise, there exists a0 such thatP(a0) ≥ Pb(a0) and we have
log Oog ((「+ η∕∣A∣
≤ log
P p(a) ∖
I(I- η)p(α') )
η
1-η
≤ 2η,
≤
where the third step follows from the fact that log(z) ≤ z - 1 for all z > 0 and the last step holds as
η ∈ [0,1 ]. Therefore, we have log (b(aο)) ≤ 24 Applying Holder's inequality to (15) completes
the proof.	□
Lemma 3. Let x, y and z ∈ ∆(A). If x(a) ≥ α1, y(a) ≥ α1 and z(a) ≥ α2 for all a ∈ A, then
DKL(Xkz)- DKL(ykZ) ≤ (1+logmm{[a2})
kx-yk1.
Proof. Under the lower bound assumption of the lemma, we have
dDKL(xkz)	x(a)	1
dx(a)	=1 + log 祠 ≤ 1 + log a2
and
dDκL(x∣∣z)
dx(a)
≤ -1 - log α1.
13
Under review as a conference paper at ICLR 2021
It follows that
dDκL(x∣∣z)	1	1	1	1 I / ι I ]	1
-dX(a)—	≤ max 11+log 后-1 - log αι ʃ ≤ 1 + 1。?皿Μ⑸ ɑ}.
Hence the function X → DKL(Xkz) is LiPschitz w.r.t. |卜||「the dual norm of k∙∣∣∞ .
□
B Proof of Theorem 1
In order to obtain an uPPer bound on the oPtimality gaP
σμ := kμt - μ*kH,	(16)
where μ* is the embedded mean-field state of the entropy regularized NE, we also need to estimate
the gap between ∏t+ι and the optimal solution to the entropy regularized MDPμt. We define
σ∏+1 := Es* [DKL (π+l3s)kπt+l3s))]	(17)
to quantify the convergence of policy sequence.
Before proceeding, we establish the following properties of entropy regularized MDPs, which are
central to the convergence analysis.
Properties of Regularized MDP. The following lemma quantifies the performance difference
between two policies for a regularized MDP — measured in terms of the expected total reward —
through the Q-function and their KL-divergence. The proof is provided in Appendix D.2.
Lemma 4 (Performance Difference). For each μ ∈ M and policies π : S → ∆(A), it holds that
Jλ(∏0) - Jλ(∏) + T-YEs〜ρμo [Dkl (∏0(∙∣s)k∏(∙∣s))]
=1~ Es 〜ρμ0 κQμ,π (s, ∙)- λ log n(“s),n(|s)- π3s)>],	(18)
where ρμ is the discounted state visitation distribution induced by the policy π0 on MDPμ.
We can characterize the optimal policy ∏j,* in terms of the optimal Q-function Q.,* as a Boltzmann
distribution of the form Cen et al. (2020); Nachum et al. (2017)
nj,*(a|s) H exp
(19)
For the setting where the reward function is bounded, we then can obtain a lower bound on∏μ,* ,as
stated in the following lemma. The proof is provided in Appendix D.3
Lemma 5. Suppose that there exists a constant Rmax > 0 such that 0 ≤
suP(s,a,μ)∈s×A×M r(s,a,μ) ≤ RmaX. For each μ ∈ M, and each policy ∏ : S → ∆(A),
we have
RmaX + γλ log |A|
1 - Y
IIQλ,πll∞ ≤ QmaX:
Also, the optimal policy ∏λ* for the regularized MDPμ satisfies
1
πμ,*(aJs) ≥
∀s ∈ S, a ∈ A.
Convergence Analysis. We now move to the convergence analysis. For clarity of exposition,
we use EP [k∏ - ∏0∣∣J as shorthand for Es〜P [k∏(∙∣s) - ∏0(∙∣s)kι], where P ∈ ∆(S); we also use
EP [Dkl (∏k∏0)] as shorthand for Es〜P Dkl (∏(∙∣s)k∏0(∙∣s))]. We recall that the step sizes are
chosen as
αt ≡ α = cαT -2/5 ,	βt ≡ β = cβT -4/5 ,
where the parameters cα and cβ satisfy that:
CaT-2∕5λ < 1,	cβT-4∕5d < 1.	(20)
Here d := 1 - d∖d2 -ʤ > 0, where di appears in Assumption 2, and d?, ʤ appear in Assumption 3.
14
Under review as a conference paper at ICLR 2021
Step 1: Convergence of Policy. To analyze the convergence of the optimality gap
σ+1 = kμt+1 - μ"kH, we first characterize the convergence behavior of the policy se-
quence {πt }t≥0 . In particular, we establish a recursive relationship between σπt+1 =
Es〜Pa [Dkl (π^+ι(∙∣s)k∏t+ι(∙∣s))] and σ∏, as stated in the following lemma. The proof is Pro-
vided in Section B.1.
Lemma 6. Under the setting of Theorem 1, for each t ≥ 1, we have
σ∏+1 ≤ (I - λαt)σ∏ +(I - λαt) (do log ~~ + KCPd) kμt-ι - μtkH + 2εαt + Qmaxα2 + 2η,
(21)
where K = T-Y log |A| + λ2R-γ).
Recall that μt = (1 - βt-ι)μt-ι + βt-ι ∙ Γ2(∏t, μt-1). Under Assumption 1, We have
kμt-ι - μtkH = Bt-i kμt-ι - r2(πt,μt-I)IlH ≤ 2βt-1.	(22)
Lemma 6 implies that
σ∏+1 ≤ (I - λαt)σ∏ +(I - λαt)Ci8t-i + 2εαt + Qmaxα2 + 2η,	(23)
where we define
C1 := 2 (d0 log |A| + KCPd1).
With αt ≡ α, βt ≡ β, from Equation (23) we have that
σ ≤ λα Wn- σπ+1) + (λα-1) C 1β+2ε + Qmaxα+λαα
Summing over ` = 0, 2, . . .T - 1 on both sides of (24) and dividing by t gives
(24)
1 T-1
TX σ∏
t=0
≤ ɪ (σ∏-σT) + (ɪ - l) Cιβ +2ε + QmXα + ∣η
Tλα	λα	λ 2λ λα
Y 1	0 q Cιβ q 2ε	Qmax	2η
≤ Tλɑσ∏ + rα + τ + κα+λa	_
When choosing α = O(T-2/5), β = O(T-4/5) and η = O(T-1), we have C1
Therefore, we obtain
(25)
O(log T).
ɪ P σt . JogT + 2ε
T π . XT2/5 + λ .
t=0
(26)
If we let T be a random number sampled uniformly from {1, . . . , T}, then the above equation can
be written equivalently as
T log T	2ε
ET [σ∏] . XT2/5 + λ
(27)
Step 2: Convergence of Mean-field Embedding. We now proceed to characterize the optimality
gap for the embedded mean-field state. We obtain the following upper bound on the optimality gap
σμ+1 = kμt+ι -μ*kH∙ The proof is provided in Section B.2.
Lemma 7. Under the setting of Theorem 1, for each t ≥ 0, we have
σμ+1≤ (1 - βtd) Otμ + d2 CPβ Pσ∏+1
where d = 1 — did2 — d3 > 0.
Lemma 7 implies that
σt ≤ ɪ
μ dβt
t+1) , d2C P
-σ ) + ɪ
σ∏+1
(28)
15
Under review as a conference paper at ICLR 2021
With βt ≡ β = O(T -4/5), averaging equation (28) over iteration t = 0, . . . , T - 1, we obtain
1 T-1
T X σμ
t=0
d1τ eμ - σT)+ ddCρ
T-1	____
Xpσπ+τ
t=0
≤
≤
σμ ι d2Cρ
+-- + —=--
dβT dT
T-1	____
Xpσπ+τ
t=0
≤
σμ + d2Cρ
dβT + d \
1T-1
T X σ∏+1
t=0
where the last inequality follows from Cauchy-Schwarz inequality.
From Eq. (26), we have
T TX σ. σ0 TT/5+d2Cρ r MT+2ε
< ∕l°g T I 2ε
〜V λT2/5 + λ
.ɪ (√l°IT + √ε)
〜√λ V T1/5 十 ε ).
This equation, together with Jensen’s inequality, proves equation (14) in Theorem 1.
Turning to equation (13) in Theorem 1, we have
1T
TED (πt,πt) = ET [D (πτ, πT)]
t=1
=ETEs〜ρ* [k∏T(∙∣s)- ∏τ(∙∣s)kι]
=ETEs〜ρτ 1 [-^ɪ(ɪ k∏T(∙∣s)-∏τ(∙∣s)kι
-	ρT-1(s)
≤ ∖ ETEs 〜ρj	P (S)J ] ∙ ETEs 〜PMIhknT "s)- πτ(∙ls)k2i
ρT-1(s)
(ii)	72
≤ √Cρ ∙ ETEsz_1 [2Dkl (∏T(∙∣s)k∏T(∙∣s))]
=EP ∙ 2ET [σT]
智ɪ (√≡ +由
.√λ k T1/5 十 ε ),
where step (i) follows from Cauchy-Schwarz inequality, step (ii) follows from Assumption 4 and
Pinsker’s inequality, and step (iii) follows from the bound in equation (27). The above equation,
together with Jensen’s inequality, proves equation (13). We have completed the proof of Theorem 1.
B.1	Proof of Lemma 6
The following lemma characterizes this policy improvement step. The proof is provided in Section
D.4.
Lemma 8. For any distributions p*,p ∈ ∆(A), state S ∈ S and function G : S ×A→ R ,it holds
for p0 ∈ ∆(A) with p0(∙) H p(∙) ∙ exp [aG(s, •)] that
Dkl (p*kp0) ≤ DKL (p*kp) - α hG(s, ∙),p* -pi + α2 kG(s, ∙)k∞ /2.
Recall that
bt+1(∙∣s) H ∏t(∙∣s) ∙ exp [at (Qλ(s, ∙) - λl°g∏t(∙∣s))]
16
Under review as a conference paper at ICLR 2021
Lemma 8 implies that for each S ∈ S, we have
DKL (∏M(∙∣s)kbt+ι(∙∣s))
≤dkl H+ι(∙∣s)kπt(∙∣s)) - Qt DQλ(S, ∙) - λlogπt(∙∣s)H+ι(∙∣s) -πt(∙∣s)) + llQλL α2∕2
=DKL H+1(∙∣s)kπt(∙∣s)) - Qt〈Q)(s, ∙) - λlogπt(∙∣s),π+ι(∙∣s) -πt(∙∣s))
+ Qt
≤dkl (<+ι(∙∣s)l∣πt(∙∣s)) — Qt 3(sQ - λIOgπt(∙∣s),πt+ι(∙∣s) - πt(∙∣s))
+2Qt I I Qt(s, ∙)- q) (SL)IlOO+∣∣q"∣oo Q2∕2∙
Recall that ∏t+ι(∙∣s) = (1 - η)∏t+ι(∙∣s) + 芍 1∣a∣ ∙ Lemma 2 implies that
DKL (X+ι(∙∣s)kπt+1(∙∣s))
≤Dkl H+ι(∙∣s)kbt+ι(∙∣s)) +2η.	(29)
≤dkl H+1(∙∣s)kπt(∙∣s)) - Qt〈Q)(s, ∙) - λlogπt(∙∣s),π+ι(∙∣s) -πt(∙∣s))
+ 2αt ∣∣Qλ(s, ∙) - Qλ(s, ∙)∣∣oo + ∣∣Qλ∣i q2∕2 + 2η.	(30)
'----------------------二-----------∞------------}
Yt(s)
Taking expectation over PJ= on both sides of (30) yields
E淳[DKL (π1+1 l∣πt+ι)]
≤E 遂[DKL (π=+1kπt)] - QtES 〜遂[<q)(SL)- λ log πt(∙∣S),<+1(∙∣S)- πt(∙∣S)〉] + ES 〜遂[匕(S)]
J'E 遂[DKL (πt+1kπt)] - (1 - Y )Qt [Jμt (π:+1) - Jμt (πt)] - Qt》E 遂[DKL (πt⅛1kπt)] + ES 〜遂[匕(S)]
(b)
≤ (1 - Qtλ)E遂[DKL (π=+1kπt)] + ES〜遂[Yt(s)]
≤ (1 - Qtλ) Ea [DKL (∏t k∏t )]+(1 - Qtλ) IEa [DKL (∏t+ιk∏t) - Dkl (∏t k∏t)] ∣ +Es 〜优
、--------------------V-------}	、-----------------V------------------}
Bi	B2
[匕(S)],
(31)
where step (a) follows from Lemma 4; step (b) follows from the fact that J)t (∏t) ≤ J) (∏=+ι), as
πj+ι = Γλ(μt) is the optimal policy for the regularized MDPμt; and step (c) holds due to triangle
inequality.
Next we bound the first and second terms on the RHS of (31) separately.
• For the second term B?: Note that π=+1 and 琮 are the optimal policy for the regularized
MDPμt and MDP*-i, respectively. Define
1
T := Mexp
RmaX + Yλ log ∣A∣
λ(1 - γ)
—
By Lemma 5, for all (s, a) ∈ S ×A,we have
∏=+ι(α∣s) ≥ τ, and π=(α∣s) ≥ τ.
Applying Lemma 3 yields
B2 ≤ KES*[∣ ∣ π=(∙∣s) -π=+ι(∙∣s) I I 1]
=KES 〜ρ* ] ρ∣(sj ∙ II 靖(∙∣S) - π=+ι(∙∣s)∣∣ι
≤ KCPES〜ρ* [∣∣ π=(∙∣s) - π=+1(∙∣s) ∣ ∣ 1]
=KCPD (Γ)(μt-1), Γ)(4t))
≤ KCPdI ∣∣μt-1 - μt∣∣H ,
Assumption 4
Assumption (2)	(32)
17
Under review as a conference paper at ICLR 2021
where
κ := 1 + log
|A|	2	Rmax
≤ 2 max I log —, γ―γ log Al + λ(1 - Y)
4	|A|	2Rmax
≤ Llog τ + λr-γy
4 KL + 2Rmax
.ιvumaχ + ʌ ∕1 ʌ .
-γ	λ(1 -γ)
ρ.(；「'，Dkl H(∙∣s)k∏t(∙∣s))
P*(s)
• KLmax,
• For the first term B1 : We have
Bi = EPL [Dkl Hk∏t)]+ (EPL E*) DKL (靖k∏t)]
=Eρ*-1 [Dkl (∏"∣∏t)]+ Es〜ρ*
(a)
≤ Eρ*-1 [Dkl Kk∏t)]+ Es〜ρ*
(b)
≤ Eρ*-ι [Dkl (∏"∣∏t)] + KLmaX ∙ do kμt - μt-ι∣∣H	(33)
where SteP (a) uses the fact that DKL (∏⅛(∙∣s)k∏t(∙∣s)) ≤ KLmaX ：= log l^Ajl (cf. Lemma
2) and step (b) follows from Assumption 5.
Combining (31), (32) and (33), we have
ep* [dkl (π+1 kπt+1
≤(1 - λαt)Eρ*-ι [Dkl (∏"g]
+	(1 - λαt)do • KLmaX ∣∣μt - μt-ikH + (1 - λαt)κCρdι ∣∣μt-i - μtkH + Es〜ρ* [Yt(S)]
=(1 - λαt)Eρ*-ι [Dkl (∏"∣∏t)]
+	(1 - λαt) (dθ • KLmaX + KCPdI) kμt-1 - MtkH + Es〜ρ* IYt(S)] ∙	(34)
Note that
Qbtλ2
Es〜P* [Yt(s)] = 2αtEs〜ρ* [∣∣Qλ(s, ∙) - Qλ(s, ∙) 口 + —∞播 + 2η
I
≤ 2αt.Es〜P* "q)(s, ∙) - Qλ(s, ∙)∣U + Y∞α2 + 2η
≤ 2εαt + QmaX α2 + 2η,
where the last steP holds by the assumPtion on the Policy evaluation error and the fact that Qbtλ-1 :
S × A → [0, QmaX] satisfies Qbtλ-1	≤ QmaX by definition. Combining the last two disPlay
equations Proves the lemma.	∞
18
Under review as a conference paper at ICLR 2021
B.2 Proof OF Lemma 7
Proof. According to the update rule (10) for the embedded mean-field state, we have
llμt+1 - μ*kH
=k(l - βt)μt + βtr2(πt+1,μt) — μ*∣∣H
= Il(I- βt)(μt - μ*) ÷ βt (γ2 (r)(μt), μt) — μ*) — βt [γ2 (Γ)(μt), μt) — γ2 (πt+1,μt)] IIH
≤(I- βt) ∣∣(μt - μ*)∣∣H ÷ βt ∣∣γ2 (r)(μt), μt) - μ*lH
÷ βt ∣∣γ2 (r)(μt),μt) - r2(πt+1, μt) ∣∣h
= (I- βt) ∣∣μt - μ*l∣H ÷ βt Ilγ2 (γi (μt), μt) - γ2 (γi (μ*), μ*) l l H
(a)
÷ βt ||r2 (r)(μt),μt) - r2(πt+1, μt) llH,	(35)
1----------------V-----------------}
(b)
where the equality (i) follows from the fact that μ* = Γ2 (Γ)(μ*), μ*).
Lemma 1 implies that Λ(μ) = Γ2 (Γ)(μ),μ) is d∖d2 ÷ d3 Lipschitz. It follows that
(a)	≤ (d1d2 ÷ d3) ∣∣μt - μ"∣H .	(36)
By Assumption 3, we have
(b)	≤ d2D (Γ) (μj ∏t+1) .	(37)
Combining Eqs. (35)-(37) yields
kμt+1 - μ*∣∣H ≤ (I- βtd) kμt - μ*∣∣H ÷ d2βtD (γi (μt), πt+1) ,	(38)
where d = 1 — d1d2 — d3 > 0.
Let us bound the second RHS term above. By the definition of policy distance D in equation (12),
we have
D (γ1 (μt),πt+1) = EP* [ l l γi (μt) - πt+1∣∣1]
=ES〜ρ* [ I I πLι(∙∣s) - πt+ι(∙∣s) l I 1]
p^ i 同+ι(∙∣s)- πt+ι(∙∣s)Uι
1/2
≤ CP JES〜优[DKL (πt+1(>∣s)lπt+ι(∙∣s))],
(39)
where the first inequality holds due to Cauchy-Schwartz inequality, the last inequality follows from
Assumption 4 and Pinsker,s inequality.
Combining (38)-(39) gives
kμt+1 - μ*∣H ≤ (1 - βtd) llμt - 4*∣H ÷ d2βtCPJES〜ρ* [DKL (πJ+1(∙∣s)∣πt+1(∙∣s))].
This completes the proof.	□
C Proof of Corollary 1
Proof. Note that for each t ∈ [T], we have
D(πt,π*) ≤ D(πtH)÷DH,π*)
=D (πt,靖)÷ D (Γ1 (μt), Γ1 (〃*))
≤ D (πtH) ÷ dι l∣μt - μ*∣H,
19
Under review as a conference paper at ICLR 2021
where the last step follows from Assumption 2 on the Lipschitzness of Γ1λ . It follows that
D(TXI
1C .
T T μt - μ
t=1	H
1T	1T
≤ T ED (∏t,∏*) + T Ekμt-μ*kH
1T	1T
≤ T E (D (πt, πt) + d1 kμt - μ*kH) + T E kμt - μ*kH
.√1λ (√T1g5T + √ε),
where in the last step we apply the bounds (13) and (14) in Theorem 1.
□
D Additional Proofs
D.1 Proof of Lemma 1
Proof. By the definition of Λ, we have
∣∣Λλ(μ)- Λλ(μ0)h
= ∣∣Γ2 (Γλ(μ),μ) -Γ2 (Γλ(μ0),μ0)∣∣H
≤ ∣∣Γ2 (Γ)(μ),μ) - Γ2 (Γλ(μ0),μ)∣∣H + ∣∣Γ2 M(μ0),μ) - Γ2 (「)(〃0),〃0) ∣∣H triangle inequality
≤d2D (Γ)(μ), Γ)(μ0)) + ʤ ∣∣μ - μ0∣∣H	Assumption 3
≤d1d2 ∣∣μ - μ0∣∣H + d3 ∣∣μ - μ0∣∣H ,	Assumption 2
which proves the lemma.	□
D.2 Proof of Lemma 4
Proof. By the definition of Vμλ,π in (4), We have
L(S)
∞
=Eat~∏0(st),st+ι ~P(∙∣st,at ,μ) X YtH,π0 (s,a) + Vλ,π(st) - Vλ,π(st)] | S0 = S .
t=0
∞
=Eat~∏0(st),st+ι ~P(∙∣st,at ,μ) X γt [rμλ,π0(s, a) + YVλ,π(st+ι) - Q,π(st)] | S0 = S + 吟"(s).
t=0
(40)
Recall that the Q-function Q),π of a policy ∏ for the regularized MDPμ is related to Vμλ,π as
Vμλ,π(s) = Ea〜∏(s) [Qλ,π(s,a) - λlogπ(a∣s)] = <Q)π(s, ∙),π(∙∣s)) + λH (π(∙∣s)),	∀s ∈ S,
Qy(s, a) =	r(s, a, μ)	+	γEsι〜p(∙∣s,a,μ)[吟,"(si)]	,	∀(s,	a)	∈ S ×	A.
We have
gμ,π(s, ∙),π0(.|s)〉= Ea"(s) [Qμ,π(s,a)],
=Ea~π0(s) [r(s,a,μ)+ YEsι~P(∙∣s,a,μ) [Vμ， (s1)]]
=Ea 〜∏0(s),s1 〜P(∙∣s,a,μ)卜 μ,π0(s, a) + 7吟,"(SI) + λ log n0(a|s)]
=Ea 〜∏0(s),s1 〜P(∙∣s,a,μ) hrμ,π0(s,a) + 7吟,"(s1)] - λH (π' (1S)) .
20
Under review as a conference paper at ICLR 2021
Therefore,
<Qλ,π(s,∙),∏0(∙∣s)-∏(∙∣s))
=Ea
〜π0(s),sι 〜P(∙∣ s,a,μ) 卜λ,π0(s, a, μ) + YV产(si)i - λH (π0(∙∣s)) - Vμλ,π(S) + λH (π(∙∣s))
=Ea
〜π0(s),sι 〜P(∙∣ s,a,μ) frλ,π0(s, a, μ) + YVy (si) — 唠"(s)] - X [H (∏0(∙∣s)) — H (∏(∙∣s))] .
(41)
Plugging (41) into (40), we have
Vy(S)- Vy(S)
∞
=Eat ~π0(st),st+i~p(∙lst,at,μ) IEYY <Qλ,π (St, ∙),π0(∙ist) -柏”)〉| so =s
t=0
∞
+ Eat〜π0(st),st+1 〜P(∙∣st,at,μ) X Ytλ (H (n0(1St))- H (π("St))) | s0 = S .	(42)
t=0
Recall the definition of J (∏) in (5). Taking expectation with respect to S 〜νo on both sides of
(42) yields	□
Jλ(∏0) — Jλ(∏)
∞
=ES0~νo,at~π0(st),st+ι~P(∙∣st ,at,μ) EYt QN(st, ∙),π0(∙∣st)-π(∙∣st))
t=0
∞
+ ES0~vo,at~no(st),st+i~PG|st,at,*) X Y λ (H (π (ISt))- H (n(ISt)))
t=0
=占Es〜ρμo [@尸(s,∙),∏0(∙∣s)- Π(∙∣s)> + λ (H (π0(∙∣s)) — H (π(∙∣s)))].
For the entropy term in (43), we have
Es〜ρμ0 [H (π0(1S))- H (π(1S))]
=Es〜ρ∏0 Klog ∏⅛)，n0(.|s),- (log 小，n(.|S)R
=Es~pn0 [(lθg^715 - log π4il4，n(|S)) - (log -15，n(1s))]
ρμ L∖	π(1S)	π(1S)	/	∖	π3S)	/」
=Es〜ρμο ](log 小，∏0(∙∣s)-∏(∙∣s), - DKL (∏0(∙∣s)k∏(∙∣s)).
Taking (44) into (43) yields the desired equation in Lemma 4.
(43)
(44)
D.3 Proof of Lemma 5
Proof. Note that the value function Vμλ,π can be written as
∞
Vμλ,π(s) = E XYtrμ,π(st，at)|so =S .
t=0
By the definition of r∕,π in (1), We have 0 ≤ En [rμ,π(st, at)] ≤ RmaX + λlog |A|. Therefore,
0 ≤ VjF(s) ≤ Rmax + λ：g |A|,	∀s ∈ S,
and
λ,π	Rmax + λlog |A| R
max + Y λ log |A|
0 ≤ Qμ, (s, a) ≤ Rmax + Y---------------- = -----------------,	∀S ∈S,a ∈A.
μ	1 - Y	1 - Y
21
Under review as a conference paper at ICLR 2021
For the second inequality, we have
哈*⑷S)
≥
exp (Qλ,*(s,a)∕λ)
Pb∈AexP (Qμ,*(s,b)∕λ)
1	_	1
Pb∈A exp (Qmaχ∕λ) = eQmaχ"∣A∣
as claimed.
□
D.4 Proof of Lemma 8
Proof. For any function g : A → R and distribution p ∈ ∆(A), let z : A → R be a constant
function defined by
z(a) = log I E p(a0) ∙ exp (αg(a0)) I .
a0∈A
Note that for any distributions p*,p0 ∈ ∆(A),(z,p* - p0)= 0. Since
p0(∙) ∞ P(I ∙ exp(αg(∙)),
We have αg(∙) = z(∙) + log(p0(∙)∕p(∙)). Hence
α hg,p* - p0i = hz + log(p0∕p),P* — P0i
=hz,P* — P0i + hiog(p*∕p),p*i + hiog(p0∕p*),P*i + hiog(p0∕p), -p0i
=DKL (P*kP)- DKL (P*kPO)- DKL (POkp).
Therefore, for each state S ∈ S , We have
α hG(s, ∙),P* — Pi = α hG(s, ∙),P* — P0i + α hG(S, ∙),P0 — Pi
=DKL	(P*kP)—	DKL (P*kP0) —	DKL (POkp)	+ α	hG(S, ∙),P0 — Pi
≤ DKL	(P*kP)—	DKL (P*kP0) —	DKL (POkP)	+ α	kG(s, ∙)k∞ ∙ kP — P0kι	.
Rearranging terms yields
DKL (P*kP0) ≤ DKL (PUP) — α hG(s, ∙),P* — Pi — DKL (POkP) + α kG(s, ∙)k∞ ∙ kP — P0kι . (45)
MeanWhile, by Pinsker’s inequality, it holds that
DKL (POkP) ≥ kP —POk21 ∕2.	(46)
By combining (45) and (46), We obtain
DKL (P*kPO) ≤ DKL	(P*kP)	— α	hG(s, ∙),P* — Pi	— kP — Po∣∣2 /2 + α kG(s,	∙)k∞ ∙ kP — POki
≤ DKL	(p*∣∣p)	— α	hG(s, ∙),p* — Pi	+ α2 kG(s, ∙)k∞ 2
which concludes the proof.	□
E A Weaker Assumption on Concentrability
In this section, we consider a weaker assumption on concentrability, under which Algorithm 1 learns
1/9
a policy-population pair that is O(T -1/9)-approximate NE after T iterations.
We consider the following distance metric between two policies π, π O ∈ Π:
W(π, πO)
辰”[kn(1s) — nO(・|s)k2i.
(47)
Similarly as before, we assume certain Lipschitz properties for the two mappings Γ1λ : M → Π and
Γ2 : Π × M → M defined in Section 2.3. In particular, we impose the following two assumtpions,
both stated in terms of the new distance metric W(∙, ∙) defined in (47) above.
Assumption 6. There exists a constant di > 0, such thatfor any μ,μ' ∈ M, it holds that
W (Γλ(μ), Γλ(*)) ≤ di kμ — 〃OkH .
22
Under review as a conference paper at ICLR 2021
Assumption 7. There exist constants d2 > 0, d3 > 0 such that for any policies π, π0 ∈ Π and
embedded mean-field states μ, μ0 ∈ M, it holds that
llr2(π,μ) - r2(π0,μ)kH ≤ d2W(π,π0),
l∣r2(π,μ) - r2(π,μ0)kH ≤ d3 kμ — μkH.
Assumptions 6 and 7 immediately imply Lipschitzness of the composite mapping Λλ : M → M,
which We recall is defined as Λλ(μ) = Γ2 (Γλ(μ),μ).
Lemma 9. Suppose Assumptions 6 and 7 hold. Thenfor each μ,μ0 ∈ M, it holds that
11 Aλ (M)- A,μO)IIH ≤ (did + d3) kμ - μ0kH.
We also consider the following relaxed, '2-type assumption on the concentrability coefficients.
Assumption 8 (Finite Concentrability Coefficients). There exist two constants Cρ,CP > 0 such
thatfor each μ ∈ M, it holds that
___入，*
Pμμ (S)
ρ* (s)
and IE λ,*	∣ P (S) ∣	∖	≤ C1
[s/	[小(S)IJ-'
We establish the following convergence result for Algorithm 1.
Theorem 2. Suppose that Assumptions 1, 5, 6, 7, and 8 hold and d1d2 + d3 < 1 and that the error
in the policy evaluation step in Algorithm 1 satisfies
Es〜P*	∣∣Qλ(s, ∙)	—	Qλ(s,	∙)∣∣2o	≤	ε2,	∀t	∈	[T].
With the choice of
η = CηT-i, αt ≡ α = CaT-4/9, βt ≡ β = cβT-8/9,
for some universal constants cη > 0, cα > 0 and cβ > 0 in Algorithm 1, the resulting policy and
embedded mean-field state SequenCe {(∏t, μt)}t=ι satisfy
W (T XX ∏t,T XX ∏j ≤ T XX W (∏tH) . λ⅛ (⅛TΓ + ε1/4) ,	(48)
t=1	t=1	t=1
T XXX …	≤ T XXX kμt - μ*kH . λ⅛ ((0TTΓ + ε1/4) .	(49)
∣	t=1	∣H	t=1
The following corollary of Theorem 2 shows that after T iterations of our algorithm, the average
policy-population pair G PT=I ∏t, T PT=I μ) is an O (T-1/9)-approximate NE.
Corollary 2. Under the assumptions of Theorem 2, we have
WG XX」
1T
T ∑>-μ*
t=1
.ɪ ((logT)1/4 + εi∕4
.λ1∕4 ∖	T 1/9	+
H
E.1 Proofs of Theorem 2 and Corollary 2
The proof follows similar lines as those of Theorem 1 and Corollary 1, with all appearances of the
distance D replaced by the new distance W . Below we only point out the modifications needed.
Lemma 6 remains valid as stated. For the proof of this lemma, the only different step is bounding
the term B2 in equation (31). In particular, the bounds in equation (32) should be replaced by the
23
Under review as a conference paper at ICLR 2021
following:
B2 ≤ κEs* [忱(忖-靖+1(忖||」
=κEs~ρ* ]pτ(s) ∙ Ilni(IS) -π+ι(IS)IlI
≤ KtEsw	(霖)2 ∙Esw h∣∣∏i(∙∣s)-∏i+ι(∙∣s)∣∣2i
≤ KCP ∙ JEs~ρ* [||璋(IS) - π+ι(IS)∣∣1i
=KCpW (Γλ(μt-1), Γλ(μt))
≤ KCPdI ∣∣μt-ι - μtkH .
Assumption 8
Assumption 6	(50)
Lemma 7 should be replaced by the following lemma.
Lemma 10. Under the setting of Theorem 2, for each t ≥ 0, we have
σμ+1≤ (1 - βtd) Otμ + d2qCρβ (σΠ+1)”,
where d = 1 — dιd2 — ʤ > 0.
The proof of Lemma 10 is similar to that of Lemma 7. The only different step is the term
D (Γλ(μt), ∏t+ι) in equation (38) should be replaced by W (Γλ(μt),∏t+ι), which can be bounded
as follows:
W (rλ(μt), πt+l) = 'Es 〜ρ* h∣∣πi+I(IS) — πt+1(IS)∣∣J
=jEs~ρ* Pl(S) 忱+1("S) - πt+ι3s)∣∣2
< TCP ∙ {Es 〜ρ* h∣∣∏∣+ι(∙∣S)-∏t+ι(∙∣S)∣∣1i}1/4
(ii)	[ZΓ	i /4
< √CP {Es〜ρ* [Dkl (∏i+ι(∙∣S)k∏t+ι(∙∣S))]}1∕4 .	(51)
where step (i) holds by Assumption 8 and the fact that ∣ν — ν0∣1 ≤ 2, ∀ν, ν0 ∈ ∆(A), and step (ii)
follows Pinsker’s inequality.
We now turn to the proof of Theorem 2.
We first establish the convergence for σπt by following the exactly same steps from equation (21) up
to equation (25). We restate the bound on T PT=OI σ∏ in (25) as follows:
When choosing α =	1 程 m <	1 Ml C ιβ∣ 2ε∣Qmaχ° , 2η Ttσ∏ ≤ Tλασ∏+ rα + X + κα+λɑ∙	(52) t=O O(T-4/9), β = O(T-8/9) and η = O(T-1), we have C i = O (log T).
Therefore, we obtain	T1 1 H m < log T , 2ε T 2^σ∏ . λT4∕9 + 又.	(53) t=O
If we let T be a random number sampled uniformly from {1, . . . , T}, then the above equation can
be written equivalently as
T log T	2ε
ET [σ∏] . λT4∕9 + λ
(54)
24
Under review as a conference paper at ICLR 2021
We now proceed to bound the average embedded mean-field state 1 PT=-o1 σtμ. Lemma 10 implies
σt ≤ ɪ (σt - σt+1) +
“-dβt' ” μ ,
(σ∏+1 )1/4
(55)
With βt ≡ β = O(T -8/9), averaging equation (55) over iteration t = 0, . . . , T - 1, we obtain
1T-1
T X σμ ≤
=0
1	d2 ∖ C p T-1	1 ∕λ
dβT ⑹-σT) + -2dT-ρ X(σ∏+1)1∕4
βT	T 0
σ0	d2∖∕C T-1	1,.
奇 + -ɪ X S')1/4
(i)	σ0	d2 国
≤ y + —上—A
-dβT d \
1 T-1	__
T xpσ+1
=0
(ii)	σ0
≤ =^+ +
d dβT
T-1	1/4
T X σ∏+1
=0
where steps (i) and (ii) follow from Cauchy-Schwarz inequality.
From equation (53), we have
1 T-1
ɪ X σ
=0
2ε 1/4
^λ;
≤
d2 JCρ
d
d
This equation, together with Jensen’s inequality, proves equation (49) in Theorem 2.
Turning to equation (48) in Theorem 2, we have
1T
TEW (πt, πt) = ET [W (πτ, nT)]
=1
ET JEs〜P* hkπT(IS) - πT(IS)k1i
(i)
≤	ETEs
(ii)
≤	ETEs
(iii)	(_2
‘〜PT-I
CPT-1
k∏T(∙∣s)-∏τ(∙∣s)k1
ρT-1 (S)
ρρɪ(s))∣2# • ETEs〜PT_1 [k∏T(∙∣s)- ∏T(∙∣s)k1 i)
• ETEs-ρ*-1
hkπτ(IS)-nT(IS)k2i} /
(iV) h (	1 1/4
.√Cρ • {ETES%-1 [Dkl (∏T(∙∣s)H∏t(∙∣s))]}
=q-P • {ET [σT]}1/4
(.ɪ ((log T)1/4 + ε1∕4}
.λ1∕4 1	T 1/9	+ J ,
where step (i) holds due to Jensen’s inequality, step (ii) follows from Cauchy-Schwarz inequality,
step (iii) follows from Assumption 8 and the fact that kν - ν0k1 ≤ 2, ∀ν, ν0 ∈ ∆(A), step (iv)
25
Under review as a conference paper at ICLR 2021
comes from Pinsker’s inequality, and step (v) follows from the bound in equation (54). The above
equation, together with Jensen’s inequality, proves equation (48). We have completed the proof of
Theorem 2.
The proof of Corollary 2 is the same as that of Corollary 1 and is omitted here.
26