Under review as a conference paper at ICLR 2021
Distributional Reinforcement Learning for
Risk-Sensitive Policies
Anonymous authors
Paper under double-blind review
Ab stract
We address the problem of learning a risk-sensitive policy based on the CVaR
risk measure using distributional reinforcement learning. In particular, we show
that applying the distributional Bellman optimality operator with respect to a risk-
based action-selection strategy overestimates the dynamic, Markovian CVaR. The
resulting policies can however still be overly conservative and one often prefers to
learn an optimal policy based on the static, non-Markovian CVaR. To this end, we
propose a modification to the existing algorithm and show that it can indeed learn
a proper CVaR-optimized policy. Our proposed approach is a simple extension of
standard distributional RL algorithms and can therefore take advantage of many
of the recent advances in deep RL. On both synthetic and real data, we empiri-
cally show that our proposed algorithm is able to produce a family of risk-averse
policies that achieves a better tradeoff between risk and the expected return.
1	Introduction
In standard reinforcement learning (RL) (Sutton & Barto, 2018), one seeks to learn a policy that
maximizes an objective, usually the expected total discounted rewards or the long-term average re-
wards. In stochastic domains, especially when the level of uncertainty involved is high, maximizing
the expectation may not be the most desirable since the solution may have high variance and occa-
sionally performs badly. In such scenarios one may choose to learn a policy that is more risk-averse
and avoids bad outcomes, even though the long-term average performance is slightly lower than the
optimal.
In this work we consider optimizing the conditional value-at-risk (CVaR) (Rockafellar & Uryasev,
2000), a popular risk measure, widely used in financial applications, and is increasingly being used
in RL. The CVaR objective focuses on the lower tail of the return and is therefore more sensitive to
rare but catastrophic outcomes. Various settings and RL approaches have been proposed to solve this
problem (Petrik & Subramanian, 2012; Chow & Ghavamzadeh, 2014; Chow & Pavone, 2014; Tamar
et al., 2015; Tamar et al., 2017; Huang & Haskell, 2020). Most of the proposed approaches, however,
involve more complicated algorithms than standard RL algorithms such as Q-learning (Watkins &
Dayan, 1992) and its deep variants, e.g. DQN (Mnih et al., 2015).
Recently, the distributional approach to RL (Bellemare et al., 2017; Morimura et al., 2010) has
received increased attention due to its ability to learn better policies than the standard approaches
in many challenging tasks (Dabney et al., 2018a;b; Yang et al., 2019). Instead of learning a value
function that provides the expected return of each state-action pair, the distributional approach learns
the entire return distribution of each state-action pair. While this is computationally more costly, the
approach itself is a simple extension to standard RL and is therefore easy to implement and able to
leverage many of the advances in deep RL.
Since the entire distribution is available, one naturally considers exploiting this information to opti-
mize for an objective other than the expectation. Dabney et al. (2018a) presented a simple way to do
so for a family of risk measures including the CVaR. The theoretical properties of such approach,
however, are not clear. In particular, it is not clear whether the algorithm converges to any particular
variant of CVaR-optimal policy. We address this issue in this work.
Our main contribution is to first show that the proposed algorithm in (Dabney et al., 2018a) overes-
timates the dynamic, Markovian CVaR but empirically can be as conservative. It has been demon-
1
Under review as a conference paper at ICLR 2021
strated that this variant of CVaR can be overly conservative in many scenarios (Tamar et al., 2017;
Yu et al., 2017), and one may prefer the static CVaR instead as the objective. Our second contri-
bution is to propose a modified algorithm that can help achieve this. Empirically, we show that the
proposed approach learns policies that perform better in terms of the overall CVaR objective on both
synthetic and real-world problems.
We close the introduction section with some references to related works. We formally present our
problem setup as well as our main analytical results in Section 2. Section 3 describes our proposed
algorithm and finally, Section 4 presents our empirical results.
1.1	Related Works
The literature on distributional RL has been greatly expanded recently (Morimura et al., 2010; Belle-
mare et al., 2017; Barth-Maron et al., 2018; Dabney et al., 2018a;b; Yang et al., 2019). Most of these
works focus on the modeling aspects, such as the choice of representations for the value distribu-
tions. The approach has been used to enhance exploration in RL (Mavrin et al., 2019; Zhang & Yao,
2019) and in risk-sensitive applications (Wang et al., 2019; Bernhard et al., 2019).
Solving Markov decision processes (MDP) with risk-sensitive objectives have been addressed in
many works (Howard & Matheson, 1972; Ruszczynski, 2010; BaUerle & Ott, 2011), including RL
approaches (Borkar, 2001; Tamar et al., 2012; L.A. & Ghavamzadeh, 2013). In particular, Chow
& Ghavamzadeh (2014); Tamar et al. (2015) deal with the static CVaR objectives while Petrik &
Subramanian (2012); Chow & Pavone (2014) deal with the dynamic CVaR objectives. Tamar et al.
(2017) proposed a policy-gradient approach that deals with both the static and the dynamic CVaR
objectives. Closest to ours is the work by Stanko & Macek (2019). Their proposed approach also
makes use of distributional RL but it is not clear whether their action selection strategy properly
optimizes either the static or the dynamic CVaR.
2	Problem Setup and Main Results
We consider a discrete-time Markov decision process with state space X and action space A. For
simplicity we assume that X and A are finite, although our results and algorithm can be read-
ily extended to more general state-action spaces. We assume that the rewards are bounded and
drawn from a countable set R ⊂ R. Given states xt, xt+1 ∈ X for any t ∈ {0, 1, . . .}, the
probability of receiving reward rt ∈ R and transitioning to xt+1 after executing at ∈ A in xt
is given by p(rt, xt+1 |xt, at). Without loss of generality we assume a fixed initial state x0, un-
less stated otherwise. Given a policy π : H → P(A), where H is the set of all histories so far
ht := (x0, a0, r0, x1, a1, r1, . . . , xt) ∈ H, and P(A) the space of distributions over A, its expected
total discounted reward over time is given by
∞
Vπ := Epπ X γtrt
t=0
where γ ∈ (0, 1) is a discount factor. The superscript π in the expectation indicates that the actions
at are drawn from π(ht). The subscript p indicates that the rewards and state transitions are induced
by p.
In standard RL, we aim to find a policy that maximizes V π . It is well-known that there exists
a deterministic stationary policy π : X → A whose decisions depend only on the current state,
that gives optimal V π , and therefore one typically works in the space of stationary deterministic
policies. Key to a dynamic-programming solution to the above problem is the use ofa value function
Qn(x, a) := En [p∞=o γtrt∣ x0 = x,a0 = a], which satisfies the Bellman equation
∀x, a,	Qπ (x, a) =	p(r, x0|x, a) [r + γQπ(x0, π(x0))] .
(1)
r,x0
The optimal value Q*(x, a) := Qπ* (x, a) for any optimal policy π* satisfies the Bellman optimality
equation
∀x, a,	Q*(x, a)= X p(r,x0∣x,a) [r + Y max Q*(x0,a0)].
r,x0
(2)
2
Under review as a conference paper at ICLR 2021
Furthermore, for any Q-function Q ∈ Q := {q : X × A → R | q(x, a) < ∞, ∀x, a}, one can
show that the operator Tπ defined by T πQ(x, a) := Pr,x0 p(r, x0|x, a)[r + γQ(x0, π(x0))] is a
γ-contraction in the sup-norm kQk∞ := maxx,a |Q(x, a)| with fixed-point satisfying (1). One can
therefore start with an arbitrary Q-function and repeatedly apply Tπ, or its stochastic approximation,
to learn Qπ . An analogous operator T can also be shown to be a γ-contraction with fixed-point
satisfying (2).
2.1	Static and Dynamic CVaR
The expected return V π is risk-neutral in the sense that it does not take into account the inherent
variability of the return. In many application scenarios, one may prefer a policy that is more risk-
averse, with better sensitivity to bad outcomes. In this work, we focus on the conditional value-at-
risk (CVaR), which is a popular risk measure that satisfies the properties of being coherent (Artzner
et al., 1999). The α-level CVaR for a random real-valued variable Z, for α ∈ (0, 1], is given by
(Rockafellar & Uryasev, 2000)
Cα(Z) := max S - 1 E[(s - Z)+]
s∈R α
where (x)+ = max{x, 0}. Note that we are concerned with Z that represents returns (the higher, the
better), so this particular version of CVaR focuses on the lower tail of the distribution. In particular,
the function s → S - 1 E[(s - Z)+] is concave in S and the maximum is always attained at the
α-level quantile, defined as
qα(Z) := inf {S : Pr(Z ≤ S) ≥ α}.
For α = 1, Cα reduces to the standard expectation. In the case Z is absolutely continuous, we have
the intuitive Cα(Z) = E[Z|Z < qα].
Our target random variable is the total discounted return Zπ := Pt∞=0 γtrt of a policy π, and our
objective is to find a policy that maximizes Cα(Zπ), where the optimal CVaR is given by
max max S — 工 En [(s — Zπ)+].	(3)
π s αp
In the context where Z is accumulated over multiple time steps, the objective (3) corresponds to
maximizing the so-called static CVaR. This objective is time-inconsistent in the sense that the op-
timal policy may be history-dependent and therefore non-Markov. This is, however, perfectly ex-
pected since the optimal behavior in the later time steps may depend on how much rewards have
been accumulated thus far - more risky actions can be taken if one has already collected sufficiently
large total rewards, and vice versa. From the point of view of dynamic programming, an alterna-
tive, time-consistent or Markovian version of CVaR may be more convenient. A class of such risk
measures was proposed by Ruszczynski (2010), and we shall refer to this version of CVaR as the
dynamic CVaR, defined recursively as1
∀∏,x, a, Dπ,o(x, a) ：= Cα[rt∣xt = x, at = a],
∀π,x,a,T > 0,	Dα,τ(x,a) := Cα[rt + yD；t-1(xt+1,∏(xt+1))∣xt = x,at = a], and
∀π, x, a,	Dαπ (x, a) := lim Dαπ T(x, a).
α	T→∞ α,T
It can be shown (Ruszczynski, 2010) that there exists a stationary deterministic optimal policy π*,
maximizing Da(x, a) for all x, a, whose dynamic CVaR is given by Da ：= D∏*. In particular, the
operator TD,α defined by
TD,αD(x, a) := Cα[rt + γ max D(xt+1, a0)|xt = x, at = a]	(4)
a0
for D ∈ Q is a γ-contraction in sup-norm with fixed-point satisfying
∀x, a,	Da(x,	a)	=	Ca[rt	+ Y max Da(xt+ι,	a0)∣xt	= x,	at	=	a].	(5)
a0
1We use a slightly different definition from that in (Ruszczynski, 2010), but conceptually they are essentially
the same.
3
Under review as a conference paper at ICLR 2021
The dynamic CVaR, however, can be overly conservative in many cases. We illustrate this with some
empirical results in Section 4. In such cases it may be favorable to use the static CVaR. BaUerIe &
Ott (2011) suggest an iterative process that can be used to solve for the optimal static CVaR policy.
The approach is based on (3):
1.	For a fixed s, one can solve for the optimal policy with respect to maxπ E[-(s - Zπ)+].
2.	For a fixed π, the optimal s is given by the α-level quantile of Zπ .
3.	Repeat until convergence.
Step one above can be done by solving an augmented MDP with states X = (χ,s) ∈ X X R, where
s is a moving threshold keeping track of the accumulated rewards so far. In particular, this MDP
has no rewards and state transition is given by p(0, (x0, s-r)|(x, s), a) := p(r, x0∣x, a). Solving
this augmented MDP directly using RL, however, can result in poor sample efficiency since each
example (x, a, r, x0 ) may need to be experienced many times under different threshold s. In this
work, we propose an alternative solution using the approach of distributional RL.
2.2 Distributional RL
In standard RL, one typically learns the Qπ(x, a) value for each (x, a) through some form of
temporal-difference learning (Sutton & Barto, 2018). In distributional RL (Bellemare et al., 2017),
one instead tries to learn the entire distribution of possible future return Zπ(x, a) for each (x, a).
The Q-value can then be extracted by simply taking the expectation Qπ (x, a) = E[Zπ (x, a)].
The objects of learning are distribution functions U ∈ Z := {Z : X×A → P(R) | E[|Z(x, a)|q] <
∞, ∀x, a, q ≥ 1}. For any state-action pair (x, a), we use U(x, a) to denote a random variable with
the respective distribution. Let Teπ be the distributional Bellman operator on Z such that
TeπU(x,a) :=D R + γU(X0, π(X0))
where D denotes equality in distribution, generated by the random variables R, X0 induced by
p(r, x0|x, a). We use the notation T instead of T when referring to a distributional operator, where
TπU(x, a) is a random variable. (Bellemare et al., 2017) show that Tπ is a γ-contraction in Z in
the following distance metric
d(U, V ) := sup W(U(x, a), V (x, a))
x,a
where W is the 1-Wasserstein distance between the distributions of U(x, a) and V (x, a). Further-
more, the operator Te defined by
TeU(x,a) :=D R + γU(X0, A0),	A0 = argmaxE[U(X0,a0)]	(6)
a0
can be shown to be a γ-contraction in Q in sup-norm under element-wise expectation, i.e.,
kETeU -ETeVk∞ ≤γkEU-EVk∞,
where ETU ∈ Q such that ETU(x, a) := E[TU(x, a)], and EU, EV , ETV all similarly defined.
In general, T is not expected to be a contraction in the space of distributions for the obvious reason
that multiple optimal policies can have very different distributions of the total return even though
they all have the same expected total return.
Since one keeps the full distribution instead of just the expectation, a natural way to exploit this is
to extract more than just the expectation from each distribution. In particular, in (6), one can select
the action a0 based on measures other than the expectation E[U (x0, a0)]. This is done by Dabney
et al. (2018a) where a distortion measure on the expectation is used to select actions using various
risk measures on U(x0, a0), including the CVaR. If we replace E[U(x0, a0)] with Cα[U(x0, a0)], one
may guess that it converges to the optimal dynamic CVaR policy satisfying (5). This is however, not
true in general. We now show that choosing actions using Cα [U(x0, a0)] results in overestimating
the dynamic CVaR value Da and Da.
4
Under review as a conference paper at ICLR 2021
Proposition 1. Let U ∈ Z. Let Cα [U] ∈ Q such that Cα [U](x, a) := Cα [U (x, a)]. Let TD,α as
defined in (4). The distributional Bellman operator TeD,α given by
TeD αU(x,a) :=D R + γU(X0, A0),	A0 = A(X0) := argmaxCα[U(X0,a0)]
,	a0
satisfies
v√	zɔ ∖^τ-^ TTf M `. (E zɔ Γr rl ʌ /	∖
∀x, a,	Cα [TD,α U (x, a)] ≥ (TD,α Cα [U])(x, a).
Similarly, for a fixed π, we have that
∀x, a,	Cα [TeDπ,αU(x, a)] ≥ (TDπ,αCα [U])(x, a).
Proof. We will use the properties of CVaR as a coherent risk measure (Artzner et al., 1999).
In particular, Cα (Z) is concave in Z (recall that we use the lower-tail version) where ∀λ ∈
[0, 1], Cα(λZ1 + (1 - λ)Z2) ≥ λCα(Z1) + (1 - λ)Cα(Z2), and satisfies both translation invariance
and positive homogeneity, where ∀c, λ ∈ R, λ ≥ 0, Cα (λZ + c) = λCα (Z) + c.
Cα[TeD,αU (x, a)] = Cα	p(r,x0|x, a) [r + γU(x0, A(x0))]
r,x0
(a)
≥	p(r, x0 |x, a)Cα[r + γU(x0, A(x0))]
r,x0
(=b) X p(r, x0|x, a)r + γCα[U(x0,A(x0))]
r,x0
= E [R + γCα [U(X0, A(X0))]]
(c)
≥ Cα[R+γCα[U(X0,A(X0))]]
= (TD,αCα[U])(x, a)
where we use the coherent properties of Cα in (a) and (b), and in (c) we use the fact that the
expectation E is Cα for α = 1 and upperbounds all other Cα . The proof for the fixed π case is
analogous.	□
It is easy to construct an example where the inequalities in Proposition 1 are strict and that TD,α
converges to a policy that is different from the optimal Da policy. Unfortunately, through empirical
observations, TeD,α still results in policies that are closer to those optimizing the dynamic CVaR
rather than the static CVaR.
It is now natural to ask whether we can optimize for the static CVaR instead while still staying
within the framework of distributional RL. Recall that it is possible to optimize for the static CVaR
by solving an augmented MDP as part of an iterative process. Instead of explicitly augmenting
the state space, we rely on the distributions U ∈ Z to implicitly “store” the information needed.
This approach will make the most of each transition example from (x, a), since it updates an entire
distribution, and indirectly the entire set of states (x, s) for all s in the augmented MDP. For this, the
action selection strategy in (6) plays a critical role.
Given U ∈ Z and s ∈ R, define ζ(U, s) ∈ Q such that ζ(U, s)(x, a) := E[-(s - U(x, a))+]. We
define a distributional Bellman operator for the threshold s as follows,
TSU(x,a) := R + YU(X0,A0),	A0 = AU(R,X0) := arg max Z (u, S-R) (X0,a0).
The following result shows that at least for a fixed target threshold, improvement is guaranteed after
each application of TS .
Proposition 2. For any U, V ∈ Z, and anys ∈ R,
ζ(TeSU,s)-ζ(TeSV,s)	≤γsupkζ(U,s0)-ζ(V,s0)k∞.
5
Under review as a conference paper at ICLR 2021
Proof. For each (x, a),
ζ(TesU, s)(x, a) - ζ(TesV, s)(x, a)
X p(r, x0|x, a)	-(s - (r + γu))+dU(x0, AsU (r, x0)) +	(s - (r + γv))+dV (x0, AsV (r, x0))
r,x0
=γ	p(r, x0 |x, a)
r,x0
dV (x0
=γ
p(r, x0∣x, a) ( max Z ( U, s——r ) (x0, a0) — max Z(V, s——r ) (x0, a00)
r,x0	a0	γ	a00	γ
(< Y X p(r, x0∣x, a) max Z (u, s~r) (x0, a0) - Z VV, s~r ) (x0,a0)
r,x0	a0	γ	γ
≤γ sup |Z(U, s0)(x0, a0) - Z(V, s0)(x0, a0)|
x0 ,a0 ,s0
where in (a) we use triangle inequality and the fact that | maxaf(a) -maxa0 g(a0)| ≤ maxa|f (a) -
g⑷1.	□
Since we only keep one distribution for each (x, a), we can only apply Tes for a single chosen s
during each update. Applying Ts can potentially change Z(TsU, s0)(x, a) for any other s0 and there
is no guarantee that similar improvement happens for these s0 . Recall that we seek to optimize
the long-term CVaR, where the “optimal” s is actually the α-quantile of the long-term return. We
therefore propose the following operator,
D
∀x, a,	Tα U(x, a) := Tqα (U (x,a)) U (x, a).
This can be easily implemented through distributional RL, which we describe in the next section.
3 Algorithm
Our proposed algorithm is based on distributional Q-learning using quantile regression (Dabney
et al., 2018b). It can be easily adapted to any other variants of distributional RL. Algorithm 1 shows
the main algorithm for computing the loss over a mini-batch containing m transition samples. Here,
each distribution θ(x, a) is represented by N quantiles θ = (θ1 . . . θN), each corresponds to a
quantile level Ti = i-N5. The quantile function qα(θ) Can therefore be easily extracted from θ. The
loss function is based on quantile regression, where ρτ (u) = u(τ - δu<0) where δu<0 = 1 ifu < 0
and 0 otherwise. The key difference from the ordinary quantile-regression distributional Q-learning
is our target action selection strategy for choosing a0k (Step 1(a) and (b)). For other implementation
details, we refer the reader to (Dabney et al., 2018b).
Algorithm 1 Quantile Regression Distributional Q-Learning for static CVaR
Input: γ, α, θ, θ0, mini-batch (xk, ak, rk, x0k) for k = 1 . . . m
1.	For each k = 1 . . . m,
(a)	Sk J qα(θ(xk, a，k))
(b)	a'k J argmax°oZ(θ0, sk-rk)(x%,a0)
0	00
(c)	Tθj(xk, ak) J rk + γθ0j (x0k, a0k)
2.	L J m1 Pmm=1 N PijPTi (T θj (Xk, ak ) - θi(xk, ak ))
3.	Output VθL.
The execution of a policy defined by θ requires an additional state information s, which summarizes
the rewards collected so far. This is not part of the MDP state x and can easily be updated after
6
Under review as a conference paper at ICLR 2021
observing each new reward. At the start of a new episode, s is reset. The complete algorithm for
executing the policy in one episode is given in Algorithm 2.
Algorithm 2 Policy execution for static CVaR for one episode
Input: γ, ɑ, θ
1.	X J Initial state
2.	a J argmaxao Z(θ, qa(θ(x, a0)))(x, a0)
3.	s J qα (θ(x, a))
4.	While x not terminal state,
(a)	Execute a in x, observe reward r and next state x0
(b)	s J s-r
(c)	x J x0
(d)	a J arg maxa0 ζ(θ, s)(x, a0)
4 Empirical Results
We implement Algorithm 1 and 2 and represent our policies using a neural network with two hidden
layers, with ReLU activation. All our experiments use Adam as the stochastic gradient optimizer
with learning rate 0.0001. For each action, the output consists of N = 100 quantile values. The
complete code to reproduce our results is included in the supplementary material.
4.1	Synthetic Data
We first evaluate our proposed algorithm in a simple task where we know the optimal stationary
policy for any CVaR level. The MDP has 4 states x0, x1 , x2 , x3 where state x0 is the initial state
and x3 is a terminal state. Each state has two actions a0 and a1. Action a0 generates an immediate
reward following a Gaussian N(1, 1) and action a1 has immediate reward N (0.8, 0.42). Clearly, a0
gives a better expected reward but with higher variance. Each action always moves the state from xi
to xi+1. We use γ = 0.9 for this task.
For α > 0.63, the optimal stationary policy is to choose action a0 in all states, while for α < 0.62,
the optimal stationary policy is to choose action a1 in all states. We compare our proposed algorithm
for static CVaR with the optimal stationary policy at various levels of CVaR. Figure 1 (left plot)
shows the results.
0
2
0.0	0.2	0.4	0.6	0.8	1.0
alpha level
[0 0 0]
—[Oil]
-[10 0]
一[111]
-[0 01]
-[010]
一[101]
[110]
Figure 1: Left: Comparison with optimal stationary policies. Middle: Ground truth CVaR at x0 .
Right: Ground truth CVaR at x2 .
We label the results for our proposed algorithm “Static” while the action-selection strategy based
on (Dabney et al., 2018a) “Dynamic”. We clearly see that the “Static” version outperforms “Dy-
namic” at all tested CVaR levels. In fact, our algorithm performs even better than the optimal sta-
tionary policy. Recall that the optimal CVaR policy may be non-stationary, where the actions in later
states depend on the rewards collected so far. This example shows that learning using Algorithm 1
and execution using Algorithm 2 can indeed result in non-stationary policies by storing the “extra”
information within the value distribution.
7
Under review as a conference paper at ICLR 2021
Further insights are revealed in the middle and right plots of Figure 1. These are the ground truth
CVaR values for all the stationary policies, where [1 0 0] means always choosing action a1 in x0
and a0 in the next two states. Notice the switching point around α = 0.625 in the middle plot and
around α = 0.83 in the right plot. The “Dynamic” CVaR action-selection strategy will choose action
a1 in x2 for α < 0.83 since this is the better action if one ignores the rewards collected since the
beginning. However, this results in a rather conservative strategy since the optimal strategy should
still favor a0 in x2 for α > 0.625.
4.2	Option Trading
We evaluate our proposed algorithm on the non-trivial real-world task of option trading, commonly
used as a test domain for risk-sensitive RL (Li et al., 2009; Chow & Ghavamzadeh, 2014; Tamar
et al., 2017). In particular, we tackle the task of learning an exercise policy for American options.
This can be formulated as a discounted finite-horizon MDP with continuous states and two actions.
The state xt includes the price of a stock at time t, as well as the number of steps to the maturity
date, which we set to T = 100. The first action, “hold”, will always move the state one time step
forward with zero reward, while the second action, “execute”, will generate an immediate reward
max{0, K - xt} and enter a terminal state. K is the strike price. In our experiments, we use K = 1
and always normalize the prices such that x0 = 1. At t = T - 1, all actions will be interpreted as
“execute”. We set γ = 0.999, which corresponds to a non-zero daily risk-free interest rate.
We use actual daily closing prices for the top 10 Dow components from 2005 to 2019. Prices
from 2005-2015 are used for training and prices from 2016-2019 for testing. To allow training on
unlimited data, we follow (Li et al., 2009) and create a stock price simulator using the geometric
Brownian motion (GBM) model. The GBM model assumes that the log-ratio of prices follows a
Gaussian distribution log xt+1 〜N(μ - σ2∕2,σ2) with parameters μ and σ, which We estimate
xt
from the real training data.
For each algorithm, each stock and each CVaR level, we trained 3 policies using different random
seeds. The policies are then tested on the synthetic data (generated using the same training model)
for 1000 episodes. The policies are further tested on the real data, using 100 episodes, each with
100 consecutive days of closing prices. The episode’s start and end dates are evenly spread out over
the 4 years of test period. All results are averaged over the 3 policies and over the 10 stocks.
Figures 2 and 3 show the test results on synthetic and real data, respectively. Again, we label the
algorithms “Static” and “Dynamic” as in the previous section. Clearly, when tested on both synthetic
and test data, the “Static” algorithm performs better across various CVaR level. The gap is especially
significant at lower α levels. Also included are the results from training using α = 1, and tested on
all α values. This corresponds to the standard action-selection strategy based on the expected return.
The learned strategies perform badly at low α levels, suggesting that they are taking too much risk.
Figure 2: Test results on synthetic data
Figure 3: Test results on real data
8
Under review as a conference paper at ICLR 2021
References
Philippe Artzner, Freddy Delbaen, Jean-Marc Eber, and David Heath. Coherent measures of risk.
MathematicaIFinance, 9(3):203-228,1999.
Gabriel Barth-Maron, Matthew Hoffman, David Budden, Will Dabney, Dan Horgan, Dhruva TB,
Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. Distributed distributional deterministic
policy gradients. 04 2018.
N. BaUerle and J. Ott. Markov decision processes with average-value-at-risk criteria. Math-
ematical Methods of Operations Research, 74(3):361-379, 2011. ISSN 1432-2994. doi:
10.1007/s00186-011-0367-0.
Marc G. Bellemare, Will Dabney, and Remi Munos. A distributional perspective on reinforcement
learning. In Proceedings of the 34th International Conference on Machine Learning - Volume 70,
ICML’17, pp. 449-458. JMLR.org, 2017.
J. Bernhard, S. Pollok, and A. Knoll. Addressing inherent uncertainty: Risk-sensitive behavior
generation for automated driving using distributional reinforcement learning. In 2019 IEEE In-
telligent Vehicles Symposium (IV), pp. 2148-2155, 2019.
V.S. Borkar. A sensitivity formula for risk-sensitive cost and the actor-critic algorithm. Sys-
tems & Control Letters, 44(5):339 - 346, 2001. ISSN 0167-6911. doi: https://doi.org/
10.1016/S0167-6911(01)00152-9. URL http://www.sciencedirect.com/science/
article/pii/S0167691101001529.
Y. Chow and M. Pavone. A framework for time-consistent, risk-averse model predictive control:
Theory and algorithms. In 2014 American Control Conference, pp. 4204-4211, 2014.
Yinlam Chow and Mohammad Ghavamzadeh. Algorithms for cvar optimization in
mdps. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q.
Weinberger (eds.), Advances in Neural Information Processing Systems 27, pp. 3509-
3517. Curran Associates, Inc., 2014. URL http://papers.nips.cc/paper/
5246-algorithms-for-cvar-optimization-in-mdps.pdf.
Will Dabney, Georg Ostrovski, David Silver, and Remi Munos. Implicit quantile networks for
distributional reinforcement learning. In Jennifer Dy and Andreas Krause (eds.), Proceedings of
the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pp. 1096-1105, Stockholmsmassan, Stockholm Sweden, 10-15 Jul 2018a.
PMLR. URL http://proceedings.mlr.press/v80/dabney18a.html.
Will Dabney, Mark Rowland, Marc G. Bellemare, and Remi Munos. Distributional reinforce-
ment learning with quantile regression. In Proceedings of the Thirty-Second AAAI Confer-
ence on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelli-
gence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence
(EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pp. 2892-2901, 2018b. URL
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17184.
Ronald A. Howard and James E. Matheson. Risk-sensitive markov decision processes. Management
Science, 18(7):356-369, 1972.
W. Huang and W. B. Haskell. Stochastic approximation for risk-aware markov decision processes.
IEEE Transactions on Automatic Control, pp. 1-1, 2020.
Prashanth L.A. and Mohammad Ghavamzadeh. Actor-critic algorithms for risk-sensitive
mdps. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q.
Weinberger (eds.), Advances in Neural Information Processing Systems 26, pp. 252-
260. Curran Associates, Inc., 2013. URL http://papers.nips.cc/paper/
4917- actor- critic- algorithms- for- risk- sensitive- mdps.pdf.
Yuxi Li, Csaba Szepesvari, and Dale Schuurmans. Learning exercise policies for american options.
In David van Dyk and Max Welling (eds.), Proceedings of the Twelth International Conference
on Artificial Intelligence and Statistics, volume 5 of Proceedings of Machine Learning Research,
pp. 352-359, Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA, 16-18 Apr 2009.
PMLR. URL http://proceedings.mlr.press/v5/li09d.html.
9
Under review as a conference paper at ICLR 2021
Borislav Mavrin, Hengshuai Yao, Linglong Kong, Kaiwen Wu, and Yaoliang Yu. Distributional re-
inforcement learning for efficient exploration. In Kamalika Chaudhuri and Ruslan Salakhutdinov
(eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Pro-
Ceedings of Machine Learning Research, pp. 4424-4434, Long Beach, California, USA, 09-15
Jun 2019. PMLR. URL http://proceedings.mlr.press/v97/mavrin19a.html.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G.
Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Pe-
tersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran,
Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep rein-
forcement learning. Nature, 518(7540):529-533, February 2015. ISSN 00280836. URL
http://dx.doi.org/10.1038/nature14236.
Tetsuro Morimura, Masashi Sugiyama, Hisashi Kashima, Hirotaka Hachiya, and Toshiyuki Tanaka.
Parametric return density estimation for reinforcement learning. In Proceedings of the Twenty-
Sixth Conference on Uncertainty in Artificial Intelligence, UAI’10, pp. 368-375, Arlington, Vir-
ginia, USA, 2010. AUAI Press. ISBN 9780974903965.
Marek Petrik and Dharmashankar Subramanian. An approximate solution method for large risk-
averse markov decision processes. In Proceedings of the Twenty-Eighth Conference on Uncer-
tainty in Artificial Intelligence, UAI’12, pp. 805-814, Arlington, Virginia, USA, 2012. AUAI
Press. ISBN 9780974903989.
R. Tyrrell Rockafellar and Stanislav Uryasev. Optimization of conditional value-at-risk. Journal of
Risk, 2:21-41, 2000.
Andrzej Ruszczynski. Risk-averse dynamic programming for markov decision processes. Math.
Program., 125(2):235-261, October 2010. ISSN 0025-5610.
Silvestr Stanko and Karel Macek. Risk-averse distributional reinforcement learning: A cvar opti-
mization approach. pp. 412-423, 01 2019. doi: 10.5220/0008175604120423.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press,
second edition, 2018. URL http://incompleteideas.net/book/the-book-2nd.
html.
A. Tamar, Y. Chow, M. Ghavamzadeh, and S. Mannor. Sequential decision making with coherent
risk. IEEE Transactions on Automatic Control, 62(7):3323-3338, 2017.
Aviv Tamar, Dotan Di Castro, and Shie Mannor. Policy gradients with variance related risk
criteria. In Proceedings of the 29th International Coference on International Conference on
Machine Learning, ICML’12, pp. 1651-1658, Madison, WI, USA, 2012. Omnipress. ISBN
9781450312851.
Aviv Tamar, Yonatan Glassner, and Shie Mannor. Optimizing the cvar via sampling. In Proceedings
of the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI’15, pp. 2993-2999. AAAI
Press, 2015. ISBN 0262511290.
P. Wang, Y. Li, S. Shekhar, and W. F. Northrop. Uncertainty estimation with distributional rein-
forcement learning for applications in intelligent transportation systems: A case study. In 2019
IEEE Intelligent Transportation Systems Conference (ITSC), pp. 3822-3827, 2019.
Christopher J. C. H. Watkins and Peter Dayan. Q-learning. Machine Learning, 8(3):279-292, May
1992. ISSN 1573-0565. doi: 10.1007/BF00992698. URL https://doi.org/10.1007/
BF00992698.
Derek Yang, Li Zhao, Zichuan Lin, Tao Qin, Jiang Bian, and Tie-Yan Liu. Fully pa-
rameterized quantile function for distributional reinforcement learning. In H. Wal-
lach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett
(eds.), Advances in Neural Information Processing Systems 32, pp. 6193-6202.
Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/
8850- fully- parameterized- quantile- function- for- distributional- reinforcement- learni
pdf.
10
Under review as a conference paper at ICLR 2021
P. Yu, W. B. Haskell, and H. Xu. Dynamic programming for risk-aware sequential optimization. In
2017 IEEE 56th Annual Conference on Decision and Control (CDC), pp. 4934-4$39, 2017.
Shangtong Zhang and Hengshuai Yao. QUOTA: the quantile option architecture for reinforcement
learning. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-
First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI
Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii,
USA, January 27 - February L 2019, pp. 5797-5804, 2019. doi: 10.1609∕aaai.v33iO1.33015797.
URL https://doi.org/10.1609/aaai.v33i01.33015797.
11