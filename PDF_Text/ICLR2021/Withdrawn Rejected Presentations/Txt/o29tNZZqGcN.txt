Lifelong Graph Learning
Chen Wangt
chenwang@dr.com
Yuheng Qiut	Dasong Gaot
yuhengq@andrew.cmu.edu dasongg@andrew.cmu.edu
Sebastian Scherert
basti@cmu.edu
tThe Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213
Abstract
Graph neural networks (GNNs) are powerful models for
many graph-structured tasks. Existing models often assume
that a complete structure of a graph is available during
training. In practice, however, graph-structured data is
usually formed in a streaming fashion so that learning a
graph continuously is often necessary. In this paper, we aim
to bridge GNN to lifelong learning by converting a graph
problem to a regular learning problem, so that GNN can
inherit the lifelong learning techniques developed for convo-
lutional neural networks (CNNs). To this end, we propose
a new graph topology based on feature cross-correlation,
namely, the feature graph. It takes features as new nodes
and turns nodes into independent graphs. This success-
fully converts the original problem of node classification
to graph classification, in which the increasing nodes are
turned into independent training samples. In the experi-
ments, we demonstrate the efficiency and effectiveness of
feature graph networks (FGN) by continuously learning a
sequence of classical graph datasets. We also show that
FGN achieves superior performance in two applications, i.e.,
lifelong human action recognition with wearable devices
and feature matching. To the best of our knowledge, FGN
is the first work to bridge graph learning to lifelong learn-
ing via a novel graph topology. Source code is available at
https://github.com/wang-chen/LGL.
1.	Introduction
Graph neural networks (GNN) have received increas-
ing attention and proved useful for many tasks with graph-
structured data, such as citation, social, and protein networks
[52]. However, graph data is sometimes formed in a stream-
ing fashion and real-world datasets are continuously evolving
over time, thus learning a streaming graph is expected in
many cases [46]. For example, in a social network, the num-
ber of users often grows over time and we expect that the
model can learn continuously with new users. In this paper,
Figure 1. We introduce feature graph network (FGN) for lifelong
graph learning. A feature graph takes the features as nodes and
turns nodes into graphs, resulting in a graph predictor instead of
the node predictor. This makes the lifelong learning techniques
for CNN applicable to GNN, as the new nodes in a regular graph
become individual training samples. Take the node a with label za
in the regular graph G as an example, its features xa = [1, 0, 0, 1]
are nodes {a1 , a2 , a3 , a4} in feature graph GaF. The feature adja-
cency is established via feature cross-correlation between a and its
neighbors N (a) = {a, b, c, d, e} to model feature “interaction.”
we extend graph neural networks to lifelong learning, which
is also known as continual or incremental learning [26].
Lifelong learning often suffers from “catastrophic forget-
ting” if the models are simply updated with new samples [35].
Although some strategies have been developed to alleviate
the forgetting problem for convolutional neural networks
(CNN), they are still difficult for graph networks. This is
because in the lifelong learning setting, the graph size can in-
crease over time and we have to drop off old data or samples
to learn new knowledge. However, the existing graph model
cannot directly overcome this difficulty. For example, graph
convolutional networks (GCN) require the entire graph for
training [20]. SAINT [58] requires pre-processing for the
entire dataset. Sampling strategies [7, 13, 58] easily forget
old knowledge when learning new knowledge.
Recall that regular CNNs are trained in a mini-batch man-
ner where the model can take samples as independent inputs
[23]. Our question is: can we convert a graph task into a
traditional CNN-like classification problem, so that (I) nodes
can be predicted independently and (II) the lifelong learning
1
techniques developed for CNN can be easily adopted for
GNN? This is not straightforward as node connections can-
not be modeled by a regular CNN-like classification model.
To solve this problem, we propose to construct a new graph
topology, the feature graph in Figure 1, to bridge GNN to
lifelong learning. It takes features as nodes and turns nodes
into graphs. This converts node classification to graph clas-
sification where the node increments become independent
training samples, enabling natural mini-batch training.
The contribution of this paper includes: (1) We introduce
a novel graph topology, i.e. feature graph, to convert a prob-
lem of growing graph to an increasing number of training
samples, which makes existing lifelong learning techniques
developed for CNNs applicable to GNNs. (2) We take the
cross-correlation of neighbor features as the feature adja-
cency matrix, which explicitly models feature “interaction”,
that is crucial for many graph-structured tasks. (3) Feature
graph is of constant computational complexity with the in-
creased learning tasks. We demonstrate its efficiency and
effectiveness by applying it to classical graph datasets. (4)
We also demonstrate its superiority in two applications, i.e.
distributed human action recognition based on subgraph clas-
sification and feature matching based on edge classification.
2.	Related Work
2.1.	Lifelong Learning
Non-rehearsal Methods Lifelong learning methods in this
category do not preserve any old data. To alleviate the for-
getting problem, progressive neural networks [36] leveraged
prior knowledge via lateral connections to previously learned
features. Learning without forgetting (LwF) [24] introduced
a knowledge distillation loss [15] to neural networks, which
encouraged the network output for new classes to be close
to the original outputs. Distillation loss was also applied
to learning object detectors incrementally [41]. Learning
without memorizing (LwM) [10] extended LwF by adding
an attention distillation term based on attention maps for
retaining information of the old classes.
EWC [21] remembered old tasks by slowing down learn-
ing on important weights. RWalk [6] generalized EWC and
improved weight consolidation by adding a KL-divergence-
based regularization. Memory aware synapses (MAS) [1]
computed an importance value for each parameter in an unsu-
pervised manner based on the sensitivity of output function
to parameter changes. [48] presented an embedding frame-
work for dynamic attributed network based on parameter
regularization. A sparse writing protocol is introduced to
a memory module [43], ensuring that only a few memory
spaces is affected during training.
Rehearsal Methods Rehearsal lifelong learning methods
can be roughly divided into rehearsal with synthetic data or
rehearsal with exemplars from old data [33]. To ensure that
the loss of exemplars does not increase, gradient episodic
memory (GEM) [26] introduced orientation constraints dur-
ing gradient updates. Inspired by GEM, [2] selected exem-
plars with a maximal cosine similarity of the gradient orien-
tation. iCaRL [32] preserved a subset of images with a herd-
ing algorithm [49] and included the subset when updating
the network for new classes. EEIL [5] extended iCaRL by
learning the classifier in an end-to-end manner. [51] further
extended iCaRL by updating the model with class-balanced
exemplars. Similarly, [3, 16] further added constraints to the
loss function to mitigate the effect of imbalance. To reduce
the memory consumption of exemplars, [18] applied the dis-
tillation loss to feature space without having to access to the
corresponding images. Rehearsal approaches with synthetic
data based on generative adversary networks (GAN) were
used to reduce the dependence on old data [14, 40, 50, 53].
2.2.	Graph Neural Networks
Graph neural networks have been widely used to solve
problems with graph-structured data [60]. The spectral net-
work extended convolution to graph problems [4]. Graph
convolutional network (GCN) [20] alleviated over-fitting
on local neighborhoods via the Chebyshev expansion. To
identify the importance of neighborhood features, graph at-
tention network (GAT) [42] added an attention mechanism
into GCN, further improving the performance on citation
networks and the protein-protein interaction dataset.
GCN and its variants require the entire graph during train-
ing, thus they cannot scale to large graphs. To solve this prob-
lem and train GNN with mini-batches, a sampling method,
SAGE [13] is introduced to learn a function to generate
node embedding by sampling and aggregating neighborhood
features. JK-Net [54] followed the same sampling strategy
and demonstrated a significant accuracy improvement on
GCN with jumping connections. DiffPool [57] learned a
differentiable soft cluster assignment to map nodes to a set
of clusters, which then formed a coarsened input for the
next layer. Ying et al. [56] designed a training strategy that
relied on harder-and-harder training examples to improve
the robustness and convergence speed of the model.
FastGCN [7] applied importance sampling to reduce vari-
ance and perform node sampling for each layer indepen-
dently, resulting in a constant sample size in all layers. [17]
sampled lower layer conditioned on the top one ensuring
higher accuracy and fixed-size sampling. Subgraph sam-
pling techniques were also developed to reduce memory
consumption. [8] sampled a block of nodes in a dense sub-
graph identified by a clustering algorithm and restricted the
neighborhood search within the subgraph. SAINT [58] con-
structed mini-batches by sampling the training graph.
Nevertheless, most of the sampling techniques still re-
quire a pre-processing of the entire graph to determine the
sampling process or require a complete graph structure,
2
which makes those algorithms not directly applicable to
lifelong learning. In this paper, we hypothesize that a dif-
ferent graph structure is required for lifelong learning, and
the new structure is not necessary to maintain its original
meaning. We noticed that there is some recent work focusing
on continuously learning a graph problem, but they have dif-
ferent formulation. For example, several exemplar selection
methods are tested in ER-GNN [59]. A weight preserving
method is introduced to growing graph [25]. A combined
strategy of regularization and data rehearsal is introduced
to streaming graphs in [46]. To overcome the incomplete
structure, [12] learns a temporal graph in sliding window.
3.	Problem Formulation
We start by defining regular graph learning before life-
long graph learning for completeness. An attribute graph
is defined as G = (V, E), where V is the set of nodes and
E ⊆ {va, vb} |(va, vb) ∈ V2 is the set of edges. Each
node v ∈ V is associated with a target zv ∈ Z and a multi-
channel feature vector xv ∈ X ⊂ RF ×C and each edge
e ∈ E is associated with a vector we ∈ W ⊂ RW. In regu-
lar graph learning, we learn a predictor f to associate a node
xv , v ∈ V0 with a target zv, given graph G, node features X,
edge vectors W, and part of the targets zv , v ∈ V \ V0.
In lifelong graph learning, we have the same objective, but
can only obtain the graph-structured data from a data contin-
uum GL = {(xi, ti, zi, Nk=1:K (xi), Wk=1:K (xi))i=1:N},
where each item is formed by a node feature xi ∈ X , a task
descriptor ti ∈ T, a target vector zi ∈ Zti , a k-hop neighbor
setNk=1:K(xi), and an edge vector set Wk=1:K (xi) associ-
ated with the k-hop neighbors. For simplicity, we will use
the symbol N(xi) to denote the available neighbor set and
their edges. We assume that every item (xi , N (xi), ti , zi)
satisfies (xi,N(Xi),zi)〜Pti(X, N(X), Z), where Pti is
a probability distribution of a single learning task. In lifelong
graph learning, we will observe, item by item, the continuum
of the graph-structured data as
(x1,N(x1),t1,z1), . . . , (xN,N(xN),tN,zN)	(1)
While observing (1), our goal is to learn a predictor fL to
associate a test sample (x, N (x), t) with a target z such that
(x, N(x), Z)〜Pt. SUch test sample can belong to a task
observed in the past, the current task, or a task observed
(or not) in the fUtUre. The task descriptors ti is defined for
compatibility with lifelong learning that reqUires them [26]
bUt is not Used in the experiments.
Note that samples are not drawn locally identically and
independently distribUted (i.i.d.) from a fixed probability
distribUtion, since we don’t know the task boUndary. In
the continUUm, we only know the label of xi , bUt have no
information aboUt the labels of its neighbors N(xi). The
items in (1) are Unavailable once they are observed and
dropped. This is in contrast to the settings in [46], where
all historical data are available dUring training. As shown
in the experiments, lifelong graph learning in practice often
reqUires that the nUmber of GNN layers L is larger than the
availability of K-hop neighbors, i.e. L > K, which also
leads many existing graph models inapplicable.
4.	Feature Graph Network
To better show the relationship with a regUlar graph, we
first review GCN [20]. Given an graph G = (V, E) described
in Section 3, the stacked node featUres can be written as
X = [xι, X2,…，XN]T ∈ RN×fc, where Xi ∈ RFC is
a vectorized node featUre. The GCN takes featUre channel
as C = 1, so that Xi ∈ RF,X ∈ RN×F. The l-th graph
convolUtional layer is defined as
X(i+1)= σ (A ∙X(i)∙ W),	⑵
where σ( ∙) is an activation function, W ∈ RF(l)×F(l+1) is a
learnable parameter, and A ∈ RN ×n is a normalized adja-
cency for A (refer to [20] for details). A graph convolutional
layer doesn’t change the number of nodes (rows of X) but
change it feature dimension from F(l) to F(l+1).
GCN has been applied to many graph-structured tasks
due to its simplicity and good generalization ability. How-
ever, the problems of GCN are also obvious. Besides the
forgetting problem in lifelong learning, its node features in
the next layer is a linear combination of the current layer,
thus GCN and its variants cannot directly model the feature
“interaction”. To this end, we introduce the feature graphs by
defining feature nodes and feature adjacency matrix.
4.1.	Feature Nodes
Recall that each node v in a regular graph G = (V, E)
is associated with a multi-channel feature vector X =
[xT :], ∙ ∙∙ , XTF J ∈ RF×C, where X[i,：] is the i-th feature
(row) of X. An attribute feature graph takes the features of a
regular graph as nodes. It can be defined as GF = (VF, EF),
where each node vF ∈ VF is associated with a feature X[Ti,:]
and will be denoted as XiF . Intuitively, the number of nodes
in the feature graph is the feature dimension in the regular
graph, i.e. VF = F, and the feature dimension in feature
graph is the feature channel in a regular graph, i.e. XiF ∈ RC.
Therefore, we define the feature nodes for feature graph as
V F = {XF, XF,…，XF,…，XF } .	(3)
In this way, for each node v ∈ V, we have a feature graph
GF . We next establish their relationship by defining the
feature adjacency matrices via feature cross-correlation.
4.2.	Feature Adjacency Matrix
For each item in continuum (1), the edges between X
and its neighbors N(X) imply the existence of correlations
3
Table 1. The relationship of a graph and feature graph.
Graph		Feature Graph	Relationship		
Node	7→	Graph	Node classification	7→	Graph classification
Feature	7→	Node	Fixed Feature	7→	Fixed Nodes
Edge	7→	Edges	Growing Adjacency	7→	Multiple Adjacency
Graph	7→	Samples	Dynamic graph	7→	Multiple samples
between their features. We model the feature adjacency as
the correlation over the k-hop neighborhood Nk (x) and for
each of the c channels independently, where c = 1, . . . , C:
AFc(X) , Sgnroot (Ey 〜Nk(X)Iwx,yχ["]yT,c] D ,⑷
where wx,y ∈ R is the associated edge weight and
Sgnroot(X) = Sign(X) ^∕∖x∖ retains the magnitude of node
features and the sign of their inner products. Note that AkF,c
preserves the connectivity information by only encoding in-
formation from connected nodes. For each sample X, this
produces C matrices of size F × F , where F N due
to the lifelong learning settings. For undirected graphs, we
change X[:,c]y[T:,c] to (X[:,c]y[T:,c] + y[:,c]X[T:,c]) for symmetry.
In practice, the expectation in (4) is approximated by
averaging over the observed neighborhood:
EL X-VT i ≈ Py∈Nk(χ)wx,yχ["]yT,c]	⑸
e wx,,χχ[-,c]y[,c] ≈	Nk(X)∖	.⑸
In this way, the feature adjacency matrix is constructed dy-
namically and independently from neighborhood samples
via (5), so that the continuum (1) is converted to graphs:
(G1F,t1,z1),...,(GiF,ti,zi),...,(GNF,tN,zN) (6)
where GiF = ViF , AF (χi) . This means that our objec-
tive of learning a node predictor becomes learning a graph
predictor fF : GF × T 7→ Z that predicts target z for
test sample (GF, t) so that (GF, t, z)〜PtF. Note that the
feature graphs in the new continuum (6) are still non-i.i.d.
In this way, a growing adjacency matrix is converted to
multiple small adjacency matrices. Hence, a lifelong graph
learning problem becomes a regular lifelong learning prob-
lem similar to [26] and the problem of increasing nodes can
be solved by applying lifelong learning to the graph contin-
uum (6). To be more clear, we list their detailed relationship
in Table 1, where the arrow 7→ refers to the conversion from
a regular graph to multiple feature graphs.
4.3.	Feature Graph Layers
Since feature graph is a new topology given by feature
adjacency matrix, we are able to define many different types
of layers. In this section we present three types of layers.
4.3.1	Feature Broadcast Layer
Inspired by GCN, the l-th broadcast layer is defined as
xF+1)= σ(AF ∙ xF) ∙ WF) ,	(7)
where σ( ∙) is a non-linear activation function, AF is the
associated normalized feature adjacency matrix, and W ∈
RC(l) ×C(l+1) is a learnable parameter. For simplicity, the
channel c is left out in (7) and the layer channel is broad-
casted independently. It is worth noting that, although the
definition of (7) appears similar to (2), they have different
dimension and represent different meanings.
4.3.2	Feature Transform Layer
Similar to graph convolutional layer (2), the feature broad-
cast layer doesn’t change the number of feature nodes. How-
ever, this is not always necessary, as the objective has been
turned into graph classification. Therefore, we define a fea-
ture transform layer which can change the number of feature
nodes and will be helpful for further reducing the number of
learnable parameters. Different from the feature broadcast
layer, we need to re-calculate the feature adjacency matrices
from transformed neighbors. Therefore, given the feature
graph GF, the l-th feature transform layer can be defined as
AF)(x)，AF (x(i), y(i)), ∀y ∈ Nk(x),	(8a)
xF+i) = σ	(WF ∙AF)(X)	∙	XFJ	,	(8b)
yF+i)= σ	(WF ∙A F)(X)	∙	yF)),	(8c)
where WF ∈ RF(l+1)×F(l) is a learnable parameter, σ( ∙) is
a non-linear activation function, and AF)(x) ∈ RF(l)×F(l) is
the normalized feature adjacency A(Fl)(x). The node features
sometimes can be smoothed due to graph propagation, hence
we can replace AF)(x) ∙ yFin (8c)to [AF)(x) ∙ yF, yF)]
by concatenating input features to prevent over-smoothing.
4.3.3	Feature Attention Layer
In the cases that a graph is fully connected or the edges
have no weights, wx,y in (4) will be not well defined. Prior
methods often rely on an attention mechanism, e.g. GAT
[42], to focus on important neighbors. Inspired by this, we
define the edge weights as an attention in (9).
_	exp(ex,y)
wx,y = Pz∈N(Z) exp(ex,z),
ex,y = LeakyReLU(axT x + ayTy + b),
(9a)
(9b)
where ax , ay ∈ RF , b ∈ R are learnable attention parame-
ters. We can also construct other types of layers based on the
topology GF, e.g. extending convolution to kervolution [44]
to improve the model expressivity or combining the pagerank
algorithm [22] to further reduce feature over-smoothness. In
this paper, we will mainly demonstrate the effectiveness of
the three types of layers introduced above.
4
4.4.	Analysis and Computational Complexity
We next provide an intuitive explanation for feature
graphs. A feature graph doesn’t retain the physical meaning
of its regular graph. Take a social network as an example, in
a regular graph, each user is a node and user connections are
edges. In feature graphs, each user is a graph, while the user
features, including the users’ age, gender, etc., are nodes.
Therefore, user behavior prediction becomes graph classi-
fication based on node information and new users simply
become multiple training samples. Since the number of user
features is more stable than the number of users, the size of
a feature graph is more stable than its regular graph. This
simplifies the learning on growing graphs dramatically by
reducing the problem to the sample-incremental learning.
On the other hand, a regular graph usually assumes that
some useful information of a node is often encoded into its
neighbors, thus graph propagation is able to improve the
model performance. A feature graph has the same assump-
tion. However, feature graph doesn’t directly propagate the
neighbor features, but encode the neighbors into the feature
adjacency matrices (4). Regardless of the nonlinear func-
tion, existing methods propagate neighbor features as AX,
where X = [xι,…，Xn], meaning they can only propagate
information element-wisely, as features in the next layer are
weighted average of the current features [20, 22, 42, 58].
In contrast, each feature graph propagate features via
AFxF, which explicitly model “interaction” between fea-
tures. In this sense, feature graph doesn’t lose information
of edges but encode them into the feature adjacency. This
explains its superiority over regular graph models in some
cases of conventional graph learning as shown in Section 5.
Take a citation graph as example, a keyword in an article may
influence another keyword in the article citing the former.
However, element-wise graph propagation like (2) cannot
explicitly model this relationship [20, 22, 42, 58]. Although
feature graph is also suitable for regular graph learning, we
mainly focus our discussion on lifelong graph learning.
Feature graphs have a low computational complexity.
Concretely, the complexity for calculating the feature adja-
cency is O(nF(2l)), where n is the expected number of node
neighbors in the continuum. Therefore, regardless of the
number of layers which is a constant for specific models, the
complexity of graph propagation for a feature broadcast layer
and a feature transform layer is of O(F(2l)C(l)C(l+1)) and
O(F(l)F(l+1)C(2l)), respectively. If we take E(l) = F(l)C(l)
as the number of elements of the features, the complexity for
calculating each sample is roughly of O(E2 + nF 2), where
we leave out the layer index for simplicity. Note that they
do not depend on the task number, thus feature graphs have
constant complexity with the increased learning tasks.
FGN is applicable even when the number of features F is
very large. In this case, we often use a feature transformation
layer (8) as a feature extractor to project the raw features
onto a lower dimension. For example, we reduced the feature
dimension on Cora from F(1) = 1433 to F(2) = 10, which
is also adopted by GCN, GAT, APPNP, SAGE, etc.
5.	Experiments
Implementation Details We perform comprehensive tests
on popular graph datasets including citation graph Cora, Cite-
seer, Pubmed [38], and ogbn-arXiv [29]. For each dataset,
we construct two different continuum: data-incremental and
class-incremental. In data-incremental tasks, all samples
are streamed randomly, while in class-incremental tasks, all
samples from one class are streamed before switching to the
next class. In both tasks, each node can only be present to
the model once, i.e., a sample cannot be used to update the
model parameters again once dropped. For this experiment,
we implement a two-layer feature graph network in PyTorch
[31] and adopt the SGD [19] optimizer. See further details
in Appendix A. We choose the most popular graph models
including GCN [20], GAT [42], SAGE [13], and APPNP
[22] as our baseline. Comparison with other methods such
as SAINT [12] are omitted as they require a pre-processing
of the entire dataset, which is incompatible with the lifelong
learning setting. The overall test accuracy after learning all
tasks is adopted as the evaluation metric [2].
Sequence Invariant Sampling We adopt the lifelong learn-
ing method described in [2] with a minor improvement for
all the baseline models. As reported in the Section 4.2 of [2],
it tends to select less earlier items in the continuum, which
discourages the memorization of earlier knowledge. We find
that this is because a uniform sampling is adopted for all
items. To compensate for such effect, we set a customized
selection probability for different items (See Appendix B).
Performance Lifelong learning has a performance upper
bound given by the regular learning settings, thus the per-
formance in regular learning is also an important indicator
for the effectiveness of feature graph. We present the overall
performance of regular learning in Table 2 and denote this
task as “R”, where the same dataset settings are adopted
for different models. To demonstrate the necessity of graph
models, we also report the performance of MLP, which ne-
glects the graph edges. It can be seen that feature graph
achieves a comparable overall accuracy in all the datasets.
This means that feature graph may also be useful for regular
graph learning. We next show that feature graph in lifelong
learning is approaching this upper bound and dramatically
alleviate the issue of “catastrophic forgetting”.
The overall averaged performance on the data-
incremental tasks is reported as task of “D” in Table 2, where
all performance is an average of three runs. We use a mem-
ory size of 500 for the datasets Cora, Citeseer and Pubmed,
and a memory size of 512 for ogbn-arXiv. It is also worth
noting that we have a low standard deviation, which means
5
Table 2. Overall performance comparison on all the datasets in Section 5.1
Method	TaSH	Cora	Citeseer	Pubmed	ogbn-arXiv
MLP		0.673	0.702	0.832	0.462
GCN		0.850	0.768	0.868	0.557
SAGE	R	0.850	0.768	0.858	0.615
GAT		0.877	0.776	0.872	0.619
APPNP		0.883	0.777	0.870	0.615
FGN		0.887	0.785	0.884	0.631
MLP		0.652 ± 0.006	0.707 ± 0.008	0.812 ± 0.006	0.349 ± 0.006
GCN		0.827 ± 0.003	0.760 ± 0.004	0.845 ± 0.002	0.397 ± 0.012
SAGE	D	0.778 ± 0.016	0.712 ± 0.015	0.816 ± 0.015	0.531 ± 0.006
GAT		0.857 ± 0.007	0.735 ± 0.009	0.847 ± 0.002	0.512 ± 0.003
APPNP		0.861 ± 0.001	0.725 ± 0.016	0.851 ± 0.003	0.511 ± 0.002
FGN		0.870 ± 0.011	0.762 ± 0.003	0.872 ± 0.009	0.555 ± 0.010
MLP		0.558 ± 0.010	0.597 ± 0.012	0.816 ± 0.013	0.237 ± 0.011
GCN		0.796 ± 0.022	0.695 ± 0.003	0.851 ± 0.012	0.328 ± 0.011
SAGE	C	0.826 ± 0.014	0.716 ± 0.001	0.841 ± 0.011	0.455 ± 0.003
GAT		0.833 ± 0.032	0.698 ± 0.009	0.833 ± 0.006	0.409 ± 0.006
APPNP		0.818 ± 0.017	0.689 ± 0.003	0.844 ± 0.013	0.423 ± 0.007
FGN		0.853 ± 0.014	0.733 ± 0.009	0.857 ± 0.003	0.494 ± 0.003
t We denote the best performance in bold and second best with Underline for each task. Due to the settings of lifelong learning,
dataset split is different from the original one, as we only test the final optimized model and don’t need a validation set (Appendix A).
^ Task of"R”，“D”，“C" denote regular, data-incremental, and class-incremental learning, respectively.
Table 3. Average class forgetting rate in task “C” (%).
Model	Cora	Citeseer	Pubmed	ogbn-arXiv
APPNP	6.38	8.03	3.61	16.3
GAT	5.50	7.05	3.50	17.7
FGN	1.35	0.41	3.49	14.8
that its performance is stable and not sensitive to the model
initialization. Samples in the class-incremental tasks are
non-i.i.d. and we have no prior knowledge on the task bound-
ary, thus it is generally more difficult than data-incremental
tasks. The overall test accuracy on class-incremental tasks
are listed as “C” in Table 2. Similar to the data-incremental
tasks, we use a memory size of 500 for Cora, Pubmed, and
Citeseer, and a memory size of 4096 for ogbn-arXix, which
is grouped into 8 tasks, each of which contains 5 classes.
Despite the difficulty, FGN still obtains higher performance
on ogbn-arXix in task “C”, compared to other state-of-the-
art methods, which verifies its superiority in lifelong graph
learning. We further report the forgetting rate [6] in the
task “C” for the best models in Table 3, which is the aver-
age precision drop compared to the regular learning. It can
be seen that FGN achieves the least forgetting rate, which
demonstrates its effectiveness in lifelong graph learning.
Memory Efficiency We report the number of learnable
parameters in Table 4. Due to its simplicity, FGN only
requires 42% less parameters compared to GCN and SAGE,
which demonstrates its memory efficiency.
Table 4. The number of model parameters used in Section 5.
Model MLP GCN SAGE GAT APPNP FGN
# Parameters 37,888 37,888 37,932 38,700 38,184 21,872
6.	Distributed Human Action Recognition
Implementation Details To demonstrate the flexibility of
FGN, we apply it to an application, i.e. distributed human
action recognition using wearable motion sensor networks
in Figure 2a (Data from [55]). Five sensors, each of which
consists of a triaxial accelerometer and a biaxial gyroscope,
are located at the left and right forearms, waist, left and right
ankles, respectively. Each sensor produces 5 data streams
and totally 5 × 5 data streams is available. The stream is
recorded at 30Hz and is comprised of human subjects with
ages from 19 to 75 and 13 daily action categories, includ-
ing rest at standing (ReSt), rest at sitting (ReSi), rest at
lying (ReLi), walk forward (WaFo), walk forward left-circle
(WaLe), walk forward right-circle (WaRi), turn left (TuLe),
turn right (TuRi), go upstairs (Up), go downstairs (Down),
jog (Jog), jump (Jump), and push wheelchair (Push).
We take every 25 sequential data points from a single
sensor as a node and perform recognition for every 50 se-
quential points (1.67s) shown in Figure 2b, which is also
adopted by [45]. This results in a temporally growing graph:
Each node is associated with a multi-channel (C = 5) 1-D
signal xit ∈ R25×5, where i = 1, 2 . . . 5 is the sensor index
6
(a) Sensors.

O-WCVE-Q gxg
Ξ	'—Y—'	\	\
25 Points Node Sub-Graph
(b) The temporal growing graph.
0.95
0.9
0.85
0.8
0.75
王王王

GCN
APPNP
GAT
FGN
Sequence
(c) Lifelong learning.
(d) Per-class precision.
Figure 2. The wearable sensor networks, the streaming graph, precesion during learning, and the final performance comparison.
Table 5. Backward maximum forgetting rate of all models on all actions (%).
Model	ReSt	ReSi	ReLi	WaFo	WaLe	WaRi	TuLe	TuRi	Up	DoWn	Jog	Jump	Push	Overall
GCN	5.34	12.49	0.99	1.32	2.88	0.59	0.89	1.58	1.30	2.81	1.24	0.26	5.31	2.85
APPNP	0.81	2.57	7.17	1.37	2.70	0.80	0.94	0.44	0.11	3.97	0.99	0.00	3.66	1.96
GAT	1.34	6.12	4.23	0.00	2.79	0.76	0.82	0.83	0.81	2.27	0.62	0.87	4.79	2.02
FGN	0.19	0.56	3.32	0.95	2.67	0.53	1.15	1.04	0.00	3.52	0.46	1.02	2.28	1.36
Table 6. Precision comparison on the action recognition.
MLP	GCN APPNP GAT	FGN
Regular 0.645±0.046 0.881±0.016 0.942±0.006 0.911±0.004 0.954±0.004
Lifelong J	0.864±0.013 0.898±0.004 0.826±0.014 0.914±0.009
* The notation “-" means that We cannot obtain meaningful performance on this task.
and t = 1, 2 . . . T is the time index. We assume all nodes
at adjacent time index are connected, i.e. for all t, nodes
(xt1, . . . , xt5, xt1+1, . . . , xt5+1) form a fully connected sub-
graph. Therefore, the problem of human action recognition
becomes a problem of sub-graph (10 nodes) classification.
Performance We list the overall performance of regular
and lifelong learning in Table 6, Which is obtained from an
average of three runs. It can be seen that FGN achieves
the best overall performance in regular learning and a much
higher performance in lifelong learning compared to the
state-of-the-art methods. This means that FGN has a loWer
forgetting rate, demonstrating its effectiveness.
We present the overall test accuracy during the lifelong
learning process in Figure 2c. It can be seen that FGN has
a much higher and stable performance than all the other
methods. We also shoWn their final per-class precision in
Figure 2d, Which indicates that FGN achieves a much higher
performance in nearly all the categories. Note that all mod-
els have a relatively loW performance on ReSt, ReSi, and
ReLi. This is because their signals are relatively flat and
similar due to their static posture. This phenomenon is also
observed by a template matching method KCC [45]. Note
that We don’t directly compare against to KCC, since KCC
mainly performs matching for single subject, While We aim
for prediction across different subjects.
We argue that the superior performance of FGN is be-
cause of that FGN is able to explicitly model the relationship
of different features. Take the Walking action as an example,
it is Well knoWn that the movement of a left arm is alWays
related to the movement of the right leg. Such information
can be explicitly modeled by the cross-correlation in the fea-
ture adjacency matrix. Moreover, it is also able to model the
feature relationship at different time step, e.g. the movement
of a left arm at time t is also related to the movement of the
right arm at time t + 1. As aforementioned, FGN can explic-
itly learn such relationship With k = 2 in (4), While the other
methods can’t directly do this. Besides, FGN also achieves
the least forgetting rate in Table 5 over all classes, Which is
the difference betWeen the final and best performance during
the entire learning process.
7.	Image Feature Matching
Implementation Details We next extend FGN to a more
challenging application, i.e. image feature matching. It is
crucial for many 3-D computer vision tasks including si-
multaneous localization and mapping (SLAM). As shoWn in
Figure 4, the interest point and their descriptors form an infi-
nite temporal groWing graph, in Which the feature points are
nodes and their descriptors are the node features as defined
in Section 3. In this Way, the problem of feature matching
becomes edge prediction for a temporal groWing graph. We
next shoW that the performance of SuperGlue [37], Which
used a regular graph attention model for feature matching,
can be simply improved by changing the graph-attention
matcher to our proposed FGN. For simplicity, We adopt a
frameWork similar to that of SuperGlue but removed the
cross-attention layers in the matching netWork. In image fea-
ture matching, We normally have a layered temporal graph,
Where edge Weights are undefined. Therefore, We construct
7
一％】 uoωo①」d 【％】 uoωo①」d
Indoors
Threshold [px]
Mixed
Threshold [px]
% ①。ualət:o
一％】 uoωo①」d 【％】 uoωo①」d
Outdoors
Threshold [px]
Threshold [px]
% ①。ualət:o
φ
O
U
φ
φ
it
Q
Natural
一％】 uoωo①」d
Threshold [px]
Hard
〔％〕uoωo①」CL
Threshold [px]
% ①。ualət:o
一％】 uoωo①」d 【％】 uoωo①」d
Artificial
Threshold [px]
Overall
Threshold [px]
% ①。ualət:o
Figure 3. The matching precision of our method (blue) and performance gap between our method and GAT (orange) at different level of
tolerances. Our method outperforms GAT at sub-pixel precision in mildly difficult categories.
Figure 4. Matching examples on TartanAir dataset. Feature match-
ing is a problem of edge prediction for temporal growing graph.
FGN by concatenating two feature broadcast layers (7) with
the attention edge weights wx,y defined in (9).
Performance We categorize the 80 test sequences from Tar-
tanAir dataset [47] into groups based on their characteristics.
Figure 4 shows several examples of consecutive matching
in Indoors, Outdoors&Artificial, and Outdoors&Mixed en-
vironments. We report the mean matching error in Table 7,
compared with the GAT-based matcher. It can be observed
that our method outperforms GAT by a noticeable margin
on all categories, especially on the difficult ones (Outdoors,
Natural, Hard). Specifically, FGN achieves an overall error
reduction of 24.2% compared with GAT.
We further report the difference in error landscape of
FGN and GAT in Figure 3. Although there is no significant
difference in matching precision at the single-pixel level,
our method experiences a greater boost in precision in the
sub-pixel region when error tolerance increases. This effect
is especially noticeable for the Artificial and Outdoors cate-
gories. This suggests that our method is more advantageous
at identifying high quality matches in mildly difficult envi-
Table 7. Mean Error (pixels) in image feature matching.1
Category	I	O	N	A	M	D	H	Overall
# Sequence	17	63	19	53	11	6	27	80
GAT	1.70	2.73	4.17	2.31	2.19	6.00	4.68	2.64
FGN (Ours)	1.52	2.05	2.88	1.85	1.56	4.75	3.25	2.00
Reduction (%) -10.6 -24.9 -30.9 -19.9 -28.8 -20.8 -30.6 -24.2
1Scene categories include: (I)ndoor and (O)utdoor, (N)aturalistic
(woods and sea floor), (A)rtificial (streets and buildings), (M)ixed
(containing both natural and artificial objects), (D)ark (poor light-
ing condition), (H)ard (violent motion and/or complex textures).
ronments, which is beneficial for downstream tasks such as
simultaneous localization and mapping (SLAM).
8.	Conclusion
In this paper, we focus on the problem of lifelong graph
learning and propose the feature graph as a new graph topol-
ogy, which solves the challenge of increasing nodes in a
streaming graph. It takes features of a regular graph as
nodes and takes the nodes as independent graphs. To con-
struct the feature adjacency matrices, we accumulate the
cross-correlation matrices of the connected feature vectors
to model feature interaction. This successfully converts the
original node classification to graph classification and turns
the problem from lifelong graph learning into regular life-
long learning. The comprehensive experiments show that
feature graph achieves superior performance in both data-
incremental and class-incremental tasks. The applications
on action recognition and feature matching demonstrates its
superiority in lifelong graph learning. To the best of our
knowledge, feature graph is the first work to bridge graph
learning to lifelong learning via a novel graph topology.
8
Acknowledgment
This work was sponsored by ONR grant #N0014-19-1-2
266 and ARL DCIST CRA award W911NF-17-2-0181.
References
[1]	Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny,
Marcus Rohrbach, and Tinne Tuytelaars. Memory aware
synapses: Learning what (not) to forget. In Proceedings of
the European Conference on Computer Vision (ECCV), pages
139-154, 2018. 2
[2]	Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Ben-
gio. Gradient based sample selection for online continual
learning. In Advances in Neural Information Processing Sys-
tems, pages 11816-11825, 2019. 2, 5, 11
[3]	Eden Belouadah and Adrian Popescu. Il2m: Class incre-
mental learning with dual memory. In Proceedings of the
IEEE International Conference on Computer Vision, pages
583-592, 2019. 2
[4]	Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann Le-
Cun. Spectral networks and locally connected networks on
graphs. In International Conference on Learning Representa-
tions (ICLR), 2014. 2
[5]	Francisco M Castro, ManuelJ Marin-JimCnez, NicolAs GuiL
Cordelia Schmid, and Karteek Alahari. End-to-end incremen-
tal learning. In Proceedings of the European Conference on
Computer Vision (ECCV), pages 233-248, 2018. 2
[6]	Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajan-
than, and Philip HS Torr. Riemannian walk for incremental
learning: Understanding forgetting and intransigence. In Pro-
ceedings of the European Conference on Computer Vision
(ECCV), pages 532-547, 2018. 2, 6
[7]	Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: fast learning
with graph convolutional networks via importance sampling.
arXiv preprint arXiv:1801.10247, 2018. 1, 2
[8]	Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio,
and Cho-Jui Hsieh. Cluster-gcn: An efficient algorithm for
training deep and large graph convolutional networks. In Pro-
ceedings of the 25th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining, pages 257-266,
2019. 2
[9]	Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-
novich. Superpoint: Self-supervised interest point detection
and description. In Proceedings of the IEEE conference on
computer vision and pattern recognition workshops, pages
224-236, 2018. 12
[10]	Prithviraj Dhar, Rajat Vikram Singh, Kuan-Chuan Peng,
Ziyan Wu, and Rama Chellappa. Learning without mem-
orizing. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 5138-5146, 2019. 2
[11]	David L Elliott. A better activation function for artificial
neural networks. Technical report, University of Maryland,
1993. 11
[12]	Lukas Galke, Iacopo Vagliano, and Ansgar Scherp. Incre-
mental training of graph neural networks on temporal graphs
under distribution shift. arXiv preprint arXiv:2006.14422,
2020. 3, 5
[13]	Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive
representation learning on large graphs. In Advances in neural
information processing systems, pages 1024-1034, 2017. 1,
2, 5
[14]	Chen He, Ruiping Wang, Shiguang Shan, and Xilin Chen.
Exemplar-supported generative reproduction for class incre-
mental learning. In BMVC, page 98, 2018. 2
[15]	Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-
ing the knowledge in a neural network. arXiv preprint
arXiv:1503.02531, 2015. 2
[16]	Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and
Dahua Lin. Learning a unified classifier incrementally via
rebalancing. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages 831-839,
2019. 2
[17]	Wenbing Huang, Tong Zhang, Yu Rong, and Junzhou Huang.
Adaptive sampling towards fast graph representation learning.
In Advances in neural information processing systems, pages
4558-4567, 2018. 2
[18]	Ahmet Iscen, Jeffrey Zhang, Svetlana Lazebnik, and Cordelia
Schmid. Memory-efficient incremental learning through fea-
ture adaptation. In European Conference on Computer Vision
(ECCV), 2020. 2
[19]	Jack Kiefer, Jacob Wolfowitz, et al. Stochastic estimation
of the maximum of a regression function. The Annals of
Mathematical Statistics, 23(3):462-466, 1952. 5
[20]	Thomas N. Kipf and Max Welling. Semi-supervised classifi-
cation with graph convolutional networks. In International
Conference on Learning Representations (ICLR), 2017. 1, 2,
3, 5
[21]	James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel
Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan,
John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska,
et al. Overcoming catastrophic forgetting in neural net-
works. Proceedings of the national academy of sciences,
114(13):3521-3526, 2017. 2
[22]	Johannes Klicpera, Aleksandar Bojchevski, and StePhan Gun-
nemann. Predict then propagate: Graph neural networks
meet personalized pagerank. In International Conference on
Learning Representations (ICLR), 2019. 4, 5
[23]	Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Im-
agenet classification with deep convolutional neural networks.
In Advances in neural information processing systems, pages
1097-1105, 2012. 1
[24]	Zhizhong Li and Derek Hoiem. Learning without forget-
ting. IEEE transactions on pattern analysis and machine
intelligence, 40(12):2935-2947, 2017. 2
[25]	Huihui Liu, Yiding Yang, and Xinchao Wang. Overcoming
catastrophic forgetting in graph neural networks. In AAAI
Conference on Artificial Intelligence (AAAI), 2021. 3
[26]	David Lopez-Paz and Marc’Aurelio Ranzato. Gradient
episodic memory for continual learning. In Advances in
Neural Information Processing Systems, pages 6467-6476,
2017. 1, 2, 3, 4
[27]	David G Lowe. Object recognition from local scale-invariant
features. In Proceedings of the seventh IEEE international
conference on computer vision, volume 2, pages 1150-1157.
Ieee, 1999. 11
[28]	Julian McAuley and Jure Leskovec. Image labeling on a net-
work: using social-network metadata for image classification.
In European conference on computer vision, pages 828-841.
9
Springer, 2012. 11
[29]	Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado,
and Jeff Dean. Distributed representations of words and
phrases and their compositionality. In Advances in neural
information processing systems, pages 3111-3119, 2013. 5,
11
[30]	David Novotny, Samuel Albanie, Diane Larlus, and Andrea
Vedaldi. Self-supervised learning of geometrically stable
features through probabilistic introspection. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 3637-3645, 2018. 12
[31]	Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban
Desmaison, Luca Antiga, and Adam Lerer. Automatic differ-
entiation in pytorch. In Conference on Neural Information
Processing Systems (NIPS), 2017. 5
[32]	Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl,
and Christoph H Lampert. icarl: Incremental classifier and
representation learning. In Proceedings of the IEEE con-
ference on Computer Vision and Pattern Recognition, pages
2001-2010, 2017. 2
[33]	Anthony Robins. Catastrophic forgetting, rehearsal and pseu-
dorehearsal. Connection Science, 7(2):123-146, 1995. 2
[34]	Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary
Bradski. Orb: An efficient alternative to sift or surf. In 2011
International conference on computer vision, pages 2564-
2571. Ieee, 2011. 11
[35]	David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams. Learning representations by back-propagating er-
rors. nature, 323(6088):533-536, 1986. 1
[36]	Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins,
Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Raz-
van Pascanu, and Raia Hadsell. Progressive neural networks.
arXiv preprint arXiv:1606.04671, 2016. 2
[37]	Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,
and Andrew Rabinovich. SuperGlue: Learning feature match-
ing with graph neural networks. In CVPR, 2020. 7, 12
[38]	Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor,
Brian Galligher, and Tina Eliassi-Rad. Collective classifica-
tion in network data. AI magazine, 29(3):93-93, 2008. 5,
11
[39]	Shital Shah, Debadeepta Dey, Chris Lovett, and Ashish
Kapoor. Airsim: High-fidelity visual and physical simulation
for autonomous vehicles. In Field and Service Robotics, 2017.
12
[40]	Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim.
Continual learning with deep generative replay. In Advances
in Neural Information Processing Systems, pages 2990-2999,
2017. 2
[41]	Konstantin Shmelkov, Cordelia Schmid, and Karteek Alahari.
Incremental learning of object detectors without catastrophic
forgetting. In Proceedings of the IEEE International Confer-
ence on Computer Vision, pages 3400-3409, 2017. 2
[42]	Petar VelickoviC, GUillem CUCUrUlL Arantxa Casanova, Adri-
ana Romero, Pietro Lid, and Yoshua Bengio. Graph Attention
Networks. International Conference on Learning Representa-
tions (ICLR), 2018. 2, 4, 5
[43]	Chen Wang, Wenshan Wang, YUheng QiU, Yafei HU, and
Sebastian SCherer. VisUal memorability for robotiC inter-
estingness via UnsUpervised online learning. In European
Conference on Computer Vision (ECCV), 2020. 2
[44]	Chen Wang, Jianfei Yang, LihUa Xie, and JUnsong YUan.
KervolUtional neUral networks. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
31-40, 2019. 4
[45]	Chen Wang, Le Zhang, LihUa Xie, and JUnsong YUan. Kernel
Cross-Correlator. In Proceedings of the AAAI Conference on
Artificial Intelligence, volUme 32, 2018. 6, 7
[46]	JUnshan Wang, GUojie Song, Yi WU, and Liang Wang. Stream-
ing graph neUral networks via ContinUal learning. In Proceed-
ings of the 29th ACM International Conference on Informa-
tion & Knowledge Management, pages 1515-1524, 2020. 1,
3
[47]	Wenshan Wang, Delong ZhU, Xiangwei Wang, YaoyU HU,
YUheng QiU, Chen Wang, Yafei HU, Ashish Kapoor, and
Sebastian SCherer. Tartanair: A dataset to pUsh the limits
of visUal slam. In IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS), 2020. 8, 12
[48]	Hao Wei, GUyU HU, Wei Bai, Shiming Xia, and Zhisong
Pan. Lifelong representation learning in dynamiC attribUted
networks. Neurocomputing, 358:1-9, 2019. 2
[49]	Max Welling. Herding dynamiCal weights to learn. In Pro-
ceedings of the 26th Annual International Conference on
Machine Learning, pages 1121-1128, 2009. 2
[50]	Chenshen WU, LUis Herranz, Xialei LiU, Joost van de Weijer,
Bogdan RadUCanU, et al. Memory replay gans: Learning to
generate new Categories withoUt forgetting. In Advances In
Neural Information Processing Systems, pages 5962-5972,
2018. 2
[51]	YUe WU, Yinpeng Chen, LijUan Wang, YUanCheng Ye,
ZiCheng LiU, Yandong GUo, and YUn FU. Large sCale in-
Cremental learning. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 374-382,
2019. 2
[52]	Zonghan WU, ShirUi Pan, Fengwen Chen, GUodong Long,
Chengqi Zhang, and S YU Philip. A Comprehensive sUrvey
on graph neUral networks. IEEE Transactions on Neural
Networks and Learning Systems, 2020. 1
[53]	Ye Xiang, Ying FU, Pan Ji, and HUa HUang. InCremental learn-
ing Using Conditional adversarial networks. In Proceedings
of the IEEE International Conference on Computer Vision,
pages 6619-6628, 2019. 2
[54]	KeyUlU XU, Chengtao Li, Yonglong Tian, Tomohiro Sonobe,
Ken-iChi Kawarabayashi, and Stefanie Jegelka. Representa-
tion learning on graphs with jUmping knowledge networks.
In International Conference on Machine Learning, 2018. 2
[55]	Allen Y Yang, Roozbeh Jafari, S Shankar Sastry, and RUzena
BajCsy. DistribUted reCognition of hUman aCtions Using wear-
able motion sensor networks. Journal of Ambient Intelligence
and Smart Environments, 1(2):103-115, 2009. 6
[56]	Rex Ying, RUining He, Kaifeng Chen, Pong EksombatChai,
William L Hamilton, and JUre LeskoveC. Graph ConvolUtional
neUral networks for web-sCale reCommender systems. In Pro-
ceedings of the 24th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining, pages 974-983,
2018. 2
[57]	Zhitao Ying, JiaxUan YoU, Christopher Morris, Xiang Ren,
Will Hamilton, and JUre LeskoveC. HierarChiCal graph repre-
10
sentation learning with differentiable pooling. In Advances
in neural information processing systems, pages 4800-4810,
2018. 2
[58]	Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal
Kannan, and Viktor Prasanna. GraphSAINT: Graph sampling
based inductive learning method. In International Conference
on Learning Representations (ICLR), 2020. 1, 2, 5
[59]	Fan Zhou and Chengtai Cao. Overcoming catastrophic for-
getting in graph neural networks with experience replay. In
AAAI Conference on Artificial Intelligence (AAAI), 2021. 3
[60]	Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan
Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. Graph
neural networks: A review of methods and applications. arXiv
preprint arXiv:1812.08434, 2018. 2
A.	Dataset Details
We perform comprehensive experiments on popular graph
datasets including citation graph Cora, Citeseer, Pubmed,
and ogbn-arXiv [28, 29, 38]. Their statistics are listed in
Table 8. Note that the dataset split is a little different from
their original settings, as we can only test the final optimized
model due to the requirement of lifelong learning, thus we
don’t need a validation set. For Cora, Citeseer, and Pubmed,
the model consists of two feature broadcast layers (7) with
C(1) = 1 and C(2) = 2 channels each. For obgn-arXiv,
we found that node features can be easily smoothed due
to multiple feature propagation, hence we use the feature
transform layers to concatenate its input features. We take
the one-hot vector as the target vector zi , adopt the cross-
entropy loss, and use the softsign [11] function σ(x) =
x/(1+|x|) as the non-linear activation.
Table 8. The statistics of the datasets used for lifelong learning.
Dataset	Nodes	Edges	Classes Features Labels		
Cora	2,708	5,429	7	1,433	0.421
Citeseer	3,327	4,732	6	3,703	0.337
Pubmed	19,717	44,338	3	500	0.054
obgn-arXiv	169,343	1,166,243	40	128	0.5
B.	Proof of Sequence Invariant Sampling
Let P be the probability that the observed items are se-
lected at time t, thus the probability that one item is still kept
in the memory after k selection is Pk. This explains that
earlier items have lower probability to be kept in the memory
and this phenomenon was reported in the Section 4.2 of [2].
To compensate for such effect and
Proposition B.1. To ensure that all items in the continuum
have the same probability to be kept in the memory at any
time t, we can set the probability that the n-th item is selected
at time t as
Pn(t) = ∖	M/n
[(n-I)/n
t6M
t > M, t = n ,
t > M, t > n
(10)
where M denotes the memory size.
Proof. It is obvious for t 6 M as we only need to keep all
items in the continuum. For t > M, the probability that the
n-th item is still kept in the memory at time t is
Pn,t = Pn(n) ∙ Pn(n + I)…Pn (t - I) , Pn(t),
M n t-2 t-1
=9∙ n+ι •…E ∙ 丁	Qi)
M
=—
.
t
This means the probability Pn,t is irrelevant to n and all
items in the continuum share the same probability. In prac-
tice, we always keep M items and sample balanced items
accross classes.	□
C.	Distributed Human Action Recognition
Implementation In practice, the temporal growing graph
can only be learned sequentially, thus we take the first 80%
of each sequence for training and the remaining 20% for
testing. Specifically, we define the radius of neighborhood
as the temporal distance. Therefore, all the nodes at the same
instant are 1-hop neighbors of each other. For each feature
graph we have K = 2 in the continuum (1). We construct
FGN using two feature transform layers (8) with attention
weights (9) and one fully connected layer to predict for the
sub-graph classification. For fairness, we use C(1) = 5,
F(1) = 25, C(2) = 32, and F(2) = 12 for all models. In
the experiments, we find that GCN, APPNP obtain the best
overall performance using the SGD optimizer, while MLP,
GAT, and FGN performed the best using the Adam optimizer.
Running time We also report the average running time for
the models in Table 9. Note that the efficiency of FGN is on
par with other methods. Considering that FGN has a much
better performance, we believe that FGN is more promising.
Table 9. Running time comparison on the action recognition.
MLP GCN APPNP GAT FGN
Runtime (ms) 3.51 5.38	5.36	5.48 5.76
D.	Image Feature Matching
Although many hand-crafted feature descriptors such
as SIFT [27] and ORB [34] have been proposed decades
ago, their performance is still unsatisfied for large view
11
point changes. Due to the well generalization ability, deep
learning-based feature detectors have received increasing
attentions. For example, SuperPoint [9] introduced a self-
supervised framework for extracting interest point detectors
and descriptors. SuperGlue [37] introduced graph attention
model into SuperPoint for feature matching.
Implementation In the experiments, we adopt C = 1,
K = 1, F = 256 in both FGN and FGN for fairness. The
training loss function is adapted from [30] which maximizes
the likelihood of predicting similar node embeddings cor-
responding to their spatial location. We recommend the
readers refer to SuperPoint [9], SuperGlue [37], and [30] for
more details of the loss functions.
Dataset We perform training and evaluation on the Tar-
tanAir dataset [47]. TartanAir is a large (about 3TB) and
very challenging visual SLAM dataset consisting of binoc-
ular RGB-D video sequences together with additional per-
frame information such as camera poses, optical flow, and
semantic annotations. The sequences are rendered in AirSim
[39], a photo-realistic simulator, which features modeled
environments with various themes including urban, rural,
nature, domestic, public, sci-fi, etc. Figure 4 contains sev-
eral example video frames from TartanAir. The dataset is
collected such that it covers challenging viewpoints and di-
verse motion patterns. In addition, the dataset also includes
other traditionally challenging factors in SLAM tasks such as
moving objects, changing lighting conditions, and extreme
weather. We randomly select 80% of the sequences for train-
ing and take the remaining for testing. We recommend the
readers refer to [47] for more details of the dataset.
E.	Limitation
Although we have shown that FGN outperformed the
SOTA methods in node classification, sub-graph classifi-
cation, and edge prediction, it also has several limitations.
First, our current implementation is not vectorized for tak-
ing a varying number of neighbors, which is less computa-
tionally efficient. Second, in the experiments, we assumed
scalar edge weights, while in the general case, the graph edge
weights are represented by a vector, as defined in the fea-
ture graphs. Third, since our main contribution is the novel
graph topology, i.e., feature graph, we mainly compared it
with the SOTA graph models such as GAT by applying an
off-the-shelf lifelong learning algorithm. However, it might
also be applicable to other lifelong learning algorithms. In
the future, we plan to fully optimize the codes, extend it
to applications with vector edge weights, and apply more
lifelong learning algorithms.
12