Under review as a conference paper at ICLR 2021
ON THE NEURAL TANGENT KERNEL OF EQUILIBRIUM
MODELS
Anonymous authors
Paper under double-blind review
ABSTRACT
Existing analyses of the neural tangent kernel (NTK) for infinite-depth networks
show that the kernel typically becomes degenerate as the number of layers grows.
This raises the question of how to apply such methods to practical “infinite depth”
architectures such as the recently-proposed deep equilibrium (DEQ) model, which
directly computes the infinite-depth limit of a weight-tied network via root-
finding. In this work, we show that because of the input injection component
of these networks, DEQ models have non-degenerate NTKs even in the infinite
depth limit. Furthermore, we show that these kernels themselves can be computed
by an analogous root-finding problem as in traditional DEQs, and highlight meth-
ods for computing the NTK for both fully-connected and convolutional variants.
We evaluate these models empirically, showing they match or improve upon the
performance of existing regularized NTK methods.
1	INTRODUCTION
Recent works empirically observe that as the depth of a weight-tied input-injected network increases,
its output tends to converge to a fixed point. Motivated by this phenomenon, DEQ models were
proposed to effectively represent an “infinite depth” network by root-finding. A natural question to
ask is, what will DEQs become if their widths also go to infinity? It is well-known that at certain
random initialization, neural networks of various structures converge to Gaussian processes as their
widths go to infinity (Neal, 1996; Lee et al., 2017; Yang, 2019; Matthews et al., 2018; Novak et al.,
2018; Garriga-Alonso et al., 2018). Recent deep learning theory advances have also shown that in
the infinite width limit, with proper initialization (the NTK initialization), training the network fθ
with gradient descent is equivalent to solving kernel regression with respect to the neural tangent
kernel (Arora et al., 2019; Jacot et al., 2018; Yang, 2019; Huang et al., 2020).
However, as the depth goes to infinity, Jacot et al. (2019) showed that the NTKs of fully-connected
neural networks (FCNN) converge either to a constant (freeze), or to the Kronecker Delta (chaos).
In this work, we show that with input injection, the DEQ-NTKs converge to meaningful fixed points
that depend on the input in a non-trivial way, thus avoiding both freeze and chaos. Furthermore, anal-
ogous to DEQ models, we can compute these kernels by solving an analogous fixed point equation,
rather than simply iteratively applying the updates associated with the traditional NTK. Moreover,
such derivations carry over to other structures like convolution DEQs (CDEQ) as well. We evaluate
the approach and demonstrate that it typically matches or improves upon the performance of existing
regularized NTK methods.
2	Background and PRELIMINARIES
Bai et al. (2019) proposed the DEQ model, which is equivalent to running an infinite depth network
with tied weight and input injection. These methods trace back to some of the original work in
recurrent backpropagation (Almeida, 1990; Pineda, 1988), but with specific emphasis on: 1) com-
puting the fixed point directly via root-finding rather than forward iteration; and 2) incorporating the
elements from modern deep networks in the single “layer”, such as self-attention transformers (Bai
et al., 2019), multi-scale convolutions (Bai et al., 2020), etc. The DEQ algorithm finds the infinite
depth fixed point using quasi-Newton root finding methods, and then backpropagates using implicit
differentiation without storing the derivatives in the intermediate layers, thus achieving a constant
1
Under review as a conference paper at ICLR 2021
memory complexity. Furthermore, although a traditional DEQ model does not always guarantee to
find a stable fixed point, with careful parameterization and update method, monotone operator DEQs
can ensure the existence of a unique stable fixed point (Winston & Kolter, 2020).
On the side of connecting neural networks to kernel methods, Neal (1996) first discovered that
a single-layered network with randomly initialized parameters becomes a Gaussian process (GP)
in the large width limit. Such connection between neural networks and GP was later extended to
multiple layers (Lee et al., 2017; Matthews et al., 2018) and various other architectures (Yang, 2019;
Novak et al., 2018; Garriga-Alonso et al., 2018). The networks studied in this line of works are
randomly initialized, and one can imagine these networks as having fixed parameters throughout the
training process, except for the last classification layer. Following the naming convention of Arora
et al. (2019), we call these networks WeakIy-trained, and networks where every layer is updated
are called fully-trained. Weakly-trained nets induce the kernel Θ(x,y) = Eθ 〜N [f (θ,x) ∙ f (θ,y)],
where x,y ∈ Rd are two samples, θ represents the parameters of the network, N is the initialization
distribution (often Gaussian) over θ, and f (θ, ∙) ∈ R is the output of the network.
One related topic in studying the relation between Gaussian process kernel and depth is the mean-
field theory. Poole et al. (2016); Schoenholz et al. (2016) showed that the correlations between
all inputs on an infinitely wide weakly-trained net become either perfectly correlated (order) or
decorrelated (chaos) as depth increases. This aligns with the observation in Jacot et al. (2019).
They suggested we should initialize the neural network on the “edge-of-chaos” to make sure that
signals can propagate deep enough in the forward direction, and the gradient does not vanish or
explode during backpropagation (Raghu et al., 2017; Schoenholz et al., 2016). These mean-field
behaviors were later proven for various other structures like RNNs, CNNs, and NTKS as well (Chen
et al., 2018a; Xiao et al., 2018; Gilboa et al., 2019; Hayou et al., 2019). We emphasize that despite
the similar appearance, our setting avoids the order vs. chaos scheme completely by adding input
injection. Such structure guarantees the converged nets depend nontrivially on the inputs, as we will
see later in the experiments.
It can be unsatisfying that the previous results only involve weakly-trained nets. Interestingly, similar
limiting behavior was proven by Jacot et al. (2018) to hold for fully-trained networks as well. They
showed the kernel induced by a fully-trained infinite width network is the following:
Θ“ e，~n]〈 5，dfS^ »
(1)
They also gave a recursive formulation for the NTK OfFCNN. Arora et al. (2019); Yang (2020) later
provided formulation for convolution NTK and other structures.
One may ask what happens if both the width and the depth go to infinity. It turns out that the vanilla
FCNN does not have a meaningful convergence: either it gives constant kernels or Kronecker Delta
kernels (Jacot et al., 2019). On the bright side, this assertion is not always the case for other network
structures. For example, the NTK induced by ResNet provides a meaningful fixed point in the large
depth limit (Huang et al., 2020). This may seem to give one explanation why ResNet outperforms
FCNN, but unfortunately they also show that the ResNet NTK with infinite depth is no different
from the ResNet NTK with just depth one. This conclusion makes the significance of infinite depth
questionable.
2.1	Notations
Throughout the paper, we write θ as the parameters for some network fθ or equivalently, f (θ, ∙). We
write capital letter W to represent matrices or tensors, which should be clear from the context, and
use [W]i to represent the element of W indexed by i. We write lower case letter X to represent vec-
tors or scalars. For a ∈ Z+, let [a] = {1,..., a}. Denote σ(x) = √z2max(0,x) as the normalized
ReLU and σ its derivative (which only need to be well-defined almost everywhere). The symbol σ2
with subscript is always used to denote the variance of some distribution. We write N(μ, Σ) as the
Gaussian distribution with mean μ ∈ Rd and covariance matrix Σ ∈ Rd×d. We let Sd-1 be the unit
sphere embedded in Rd.
2
Under review as a conference paper at ICLR 2021
3	DEQ-NTK of Fully-connected NEURAL NETWORKS
In this section, We show how to derive the NTK of the fully-connected DEQ (DEQ-NTK). From
now on, we simplify fully-connected DEQs as DEQs. Recall that DEQs are equivalent to infinitely
deep fully-connected neural nets with input injection (FCNN-IJ), and one can either exactly solve
the fixed point using root-finding (UP to machine epsilon and root-finding algorithm accuracy) or
approximate the DEQs by just doing finite depth forward iterations. In section 3.1, we show the NTK
of the approximated DEQ using finite depth iteration, and in section 3.2 we demonstrate how to get
the exact convergence point of DEQ-NTK. The details of this section can be found in appendix A.
3.1	Finite Depth Iteration of DEQ-NTK
Let d be the input dimension, x,y ∈ Rd be a pair of inputs, Nh be the width of the hth layers where
h ∈ [L + 1]. Let N = d and Nl+i = 1. Define the FCNN-IJ with L hidden layers as follows:
fθh)(χ) = N2WWW(h)g(h-1)(χ) + NNuu(h)x + y Nb(h), h ∈ [L +1]
g(L)(X) = σ(fθL)(X))
where W(h) ∈ RNh ×Nh-1, U(h) ∈ RNh ×d are the internal weights, and b(h) ∈ RNh are the bias
terms. These parameters are chosen using the NTK initialization. Let us pick σw, σu, σ⅛ ∈ R
arbitrarily in this section.
NTK initialization. We randomly initialize every entry of every W, U, b from N(0,1).
Without loss of generality (WLOG) we assume the width of the hidden layer Nh = N is the same
across different layers. We remark the readers to distinguish FCNN-IJ from a recurrent neural
network (RNN): our model injects the original input to each layer, whereas a RNN has a sequence
of input (xι,..., XT), and inject Xt to the tth-layer.
Here is a crucial distinction between finite width DEQs and infinite width DEQs:
Remark 1. In the finite width regime, one typically has to assume the DEQs have tied weights,
that is, W(I) = ... = W(L+1). Otherwise it is unlikely the network will converge at all. In fact,
one needs to be very careful with the parametrization of the weights to guarantee the fixed point is
unique and stable. This is not the case in the infinite width regime. As we shall see soon, even with
distinct weights in each layer, the convergence of DEQ-NTKs only depend on σW, σ^, σ* and the
nonlinearity σ. Assuming untied weights makes the analysis easier, but the same argument can be
made rigorously for tied weights as well, see Yang (2019; 2020).
Our main theorem is the following:
Theorem 1. Recursively define the following quantities for h ∈ [L]:
∑(%X,y) =	X Xτy		⑵
AQh (x, y)=	∑∕t)(x,x) ∑(hτ)(y,X)	N(hT) (X,y)∖ uR2×2 Σ(hT) (y,y) ∈ R	⑶
∑(h)(X,y) =	=σW E(u,v)〜N (0,Λ(h))[σ(U)σ(V)] + σU XTy + σb		(4)
∑ ()(X,y) =	=σWE(u,v)~N(0,Λ(h)) [σ(U)σ(V)]		(5)
Then the L-depth iteration to the DEQ-NTK can be expressed as:
L+1 /	l+i	∖
Θ(L)(X,y) = E	(∑(hT) (x,y)) ∙ 口 Σ(Y) (x,y)	,	(6)
h=1 ∖	h'=h
where by convention we set ΣL+1 (x, y) = Ifor the L-depth iteration.
3
Under review as a conference paper at ICLR 2021
Proof Sketch. The first step is to show that at each layer h ∈ [L], the representation f(h)(x) is
associated with a Gaussian process with kernel eq. (3) as N → ∞. Then use the characterization in
eq.(1), calculate the NTK by:
Θ3χ,y) = e，]( df∂θf2，5
=Eθ
^.
「/ ∂f (θ,x) ∂f (θ,y) ∖	/ ∂f(θ,x) ∂f (θ,y) ∖	/ ∂f (θ,x) ∂f(θ,y)∖]
[\^^，-∂^∕]+ Eθ K~^ιΓ-~m~∕]+ Eθ K-lbΓ/].
/	/
1	2	D
Calculating each term using the chain rule, We get eq. (6).
□
3.2 Fixed Point of DEQ-NTK
Based on eq. (6), We are now ready to answer what the fixed point of Θ(L) is. By convention, We
assume the two samples x,y ∈ Sd-1, and we require the parameters σW, σU, σb obey the DEQ-
NTK initialization.
DEQ-NTK initialization. Let every entry of every W,U,b follows the NTK initialization de-
scribed in section 3.1, as well as the additional requirement σW + σU + 琮=1.
Let the nonlinear activation function σ be the normalized ReLU: σ(x) = Λ∕2max(0, x).
Definition 3.1 (Normalized activation). We call an activation function σ : R → R normalized if
Eχ~N (0,ι)[σ(x)2] = 1.
Using normalized activations along with DEQ-NTK initialization, we can derive the main conver-
gence theorem:
Theorem 2. Use same notations and settings in theorem 1. With input data x, y ∈ Sd-1, parameters
σW, σU, σ2 follow the DEQ-NTK initialization, the fixed point OfDEQ-NTK is
Θ*(x,y) = lim Θ(L)(X,y)
L→∞
∑*(χ,y)
1 - ∑ *(χ,y)
⑺
where Σ*(x, y) = ρ* is the root of:
Rσ (P) - P, where R。(P) = σW (「" +(：-CosT 必)
+ σU XTy + σ2,
⑻
and
Σ*(x,y) = lim Σ(h)(x,y)
h→∞
2 π - CosT(P*)∖
σW(-------∏-----)-
Proof. Due to the fact that x ∈ Sd-1, σ being a normalized activation, and DEQ-NTK initialization,
one can easily calculate by induction that for all h ∈ [L]:
Σ(h) (x, x) = σW	E	[σ(u)2] + σV xτx + σ2 = 1
UZN (0,1)
(P 1),
This indicates that in eq. (3), the covariance matrix has a special structure Λ(h)(x,y)
where P = Σ(h-1) (x, y) depends on h, x, y. For simplicity we omit the h, x, y in A(h) (x, y). As
shown in Daniely et al. (2016):
E	[σ(u)σ(v)]
(u,v)~N (0,Λ)
1 - P2 + (π - CosT(P)) P
∏ - CosT(P)
E	[σ(u)σ(v)]=-------------
(u,v)~N (0,Λ)
(10)
(11)
⑼
π
π
4
Under review as a conference paper at ICLR 2021
Adding input injection and bias, We derive eq. (8) from eq. (10), and similarly, eq. (9) from eq. (11).
Notice that iterating eqs. (2) to (4) to solve for Σ(h) (x, y) is equivalent to iterating (Rσ ◦.…◦ Rσ )(ρ)
with initial input P = xτy. Take the derivative
筌1=σW(l-cos-π1(ρ))[ 1, if σW< 1 and-1 ≤ ρ< L
For X = y we have —1 ≤ ρ <c< 1 for some C (this is because we only have finite number of
inputs x, y) and by DEQ-NTK initialization we have σW < 1, so the above inequality hold. Hence
Rσ(ρ) is a contraction on [0,c], and we conclude that the fixed point ρ* is attractive.
By lemma 1, if σW < 1, then the limit of eq. (6) exists, so we can rewrite the summation form in
eq. (6) in a recursive form:
。⑼(x,y) = ∑ (O)(X,y)
Θ(L+1)(x,y) = Σ(L+1)(x,y) ∙ Θ(L)(x,y) + Σ(L+1)(x,y),
and directly solve the fixed point iteration:
lim Θ(L+I)(X,y)
L→∞
Pm (∑(L+1) (x, y) ∙ Θ(L) (x, y) + Σ(L+1) (x, y))
=⇒ lim Θ(L+1)(x,y) = Σ*(x,y) ∙ lim Θ(L)(x,y) + Σ*(x,y)
L→∞	L→∞
=⇒ lim Θ(L)(x,y) = Σ*(x,y) ∙ lim Θ(L)(x,y) + Σ*(x,y).
L→∞	L→∞
Solving for liιmL-∞ Θ(L) (x, y) we get Θ* (x, y)=&：；)).
1 -ς (x,y)
(12)
□
Remark 2. Note our Σ*(x,y) always depends on the inputs X and y, so the information between
two inputs is always preserved, even if the depth goes to infinity. On the contrary as pointed out
by Jacot et al. (2019), without input injection, Σ(h) (x, y) always converges to 1 as h → ∞, even if
χ = y.
4 DEQ WITH CONVOLUTION LAYERS
In this section we show how to derive the NTKs for convolution DEQs (CDEQ). Although in this
paper only the CDEQ with vanilla convolution structure is considered in experiments, we remark
that our derivation is general enough for other CDEQ structure as well, for instance, CDEQ with
global pooling layer. The details of this section can be found in appendix B.
Unlike the FCNN-IJ, whose intermediate NTK representation is a real number. For convolution
neural networks (CNN), the intermediate NTK representation is a four-way tensor. In the following,
we will present the notations, CNN with input injection (CNN-IJ) formulation, the CDEQ-NTK
initialization, and our main theorem.
Notation. We adopt the notations from Arora et al. (2019). Let x, y ∈ RP×Q be a pair of inputs,
let q ∈ Z+ be the filter size (WLOG assume it is odd as well). By convention, we always pad the
representation (both the input layer and hidden layer) with 0's. Denote the convolution operation as
following:
q — 1	q— 1
-Q~	~Q-
[w * x]ij = Σ Σ [w]a+ q+1 ,b+ q+1 [x]a+i,b+j for i ∈ [P],j ∈ [Q].
a=-q-1 b=- q-1
Denote
Dij,i'j' = { (i + a,j + b,i' + a',j' + b') ∈ [P] × [Q] × [P] × [Q]:
—(q - 1)/2 ≤ a,b,a',b' ≤ (q-1)/2上
Intuitively, Dj,ij' is a q × q × q × q set of indices centered at (ij,i'j'). For any tensor T ∈
RP×q×p×q, let [T]d，, '' be the natural sub-tensor and let Tr(T) = Ei j Tj,j.
5
Under review as a conference paper at ICLR 2021
Formulation of CNN-IJ. Define the CNN-IJ as follows:
•	Let the input x(0) = X ∈ RP×Q×C0, where Co is the number of input channels, and Ch is
the number of channels in layer h. Assume WLOG that Ch = C for all h ∈ [L]
•	For h = 1 ,...,L, let the inner representation
Ch-I	2Co	2
X((h = V σWwW(h)	* XST) + L JσuU(h)	* x(0)	(13)
x(β) = C Ch W(α),(β) * x(a)	+ C Ch u(α),(β) * x(α)	(13)
α=1	h	α=1 h
[x(h)Lj = ^[⅛ [σ(x(h)) ., fori∈ [P],j∈ Q	(14)
where W(O)),(β) ∈ Rq ×q represent the convolution operator from the αth channel in layer
h — 1 to the Pth channel in layer h. Similarly, U(h)(0)∈ Rq×q injects the input in each
convolution window. S ∈ RP×Q is a normalization matrix. Let W, U, S, σ2, σ2z be chosen
by the CDEQ-NTK initialization described later. Notice here We assume WLOG that the
number of channels in the hidden layers is the same.
• The final output is defined to be fθ(x) = EC=I(卬(0)+1),片(：)),where W(L+1) ∈
RP×q is sampled from standard Gaussian distribution.
CDEQ-NTK initialization. Let 1q ∈ Rq×q,X ∈ RP×Q be two all-one matrices. Let X ∈
R(P+2) X(Q+2) be the output of zero-padding X. We index the rows of X by {0,1,...,P +1} and
columns by {0,1,...,Q + 1}. For position i ∈ [P],j ∈ [Q], let ([S]j)2 = [1q * X]j in eq. (14).
Let every entry of every W, U be sampled from N(0,1) and σ2z + σ2 = 1.
Using the above defined notations, we now state the CDEQ-NTK.
Theorem 3. Let x,y ∈ RP×Q×C0 be s.t IIxij∣∣2 = Ilyij∣∣2 = 1 for i ∈ [P],j ∈ [Q]. Define
the following expressions recursively (some x, y are omitted in the notations), for (i,j,i',j') ∈
[P]	×	[Q]	×	[P]	×	[Q] ,h ∈	[L]	`	，
κi2,j, (x,y) = E x(α)③ y(α)
α∈[Co ]
P) "''C £ Tr ①)) *dm)
/
Λ(h,i'j' (x,y) =
∖
∣∑(hτ)(x,x)]
ij,ij
∣Σ(hτ)(y,x)]一
i'j',ij
[∑(hτ) (x,y)[“,
ij,i,j,
口τ(y,y)[,…
i'j',i'j'
(15)
(16)
(17)
[K (h)(x,y)[j"
σW
[S]ij ∙[S]i'j'
E	[σ(u)σ(v)] +
(u,v)~N(UN* ')
j,ij
σU
[S]ij ∙[S]i'j'
[K(O)]ij,i,j, (18)
[K (h)(x,y)j∙,
j,3 j
σW
[S]ij ∙[S]i'j'
E	[σ(u)σ(v)]
(u,v)~N(0,A(h)i' .’)
j, J
Fh)(X,y)」
(19)
(20)
Ir ("x、)
Define the linear operator L : RP ×Q×P ×Q → RP ×Q×P ×Q via [L(M )]ij,i'j' = Tr ([M]D,",J .
Then the CDEQ-NTK can be found solving the following linear system:
Θ* (x, y) = K* (x, y) Θ L(Θ* (x, y)) + K * (x, y),	QI)
where K * (x, y) = limL→∞ K(L) (x,y),K *(x, y) = limL→∞ K (L) (x, y). The limit exists if σ2z <
1.	’ ‘
6
Under review as a conference paper at ICLR 2021
5	Experiments
In this section, we evaluate the performance of DEQ-NTK and CDEQ-NTK on both MNIST and
CIFAR-10 datasets. We also compare the performance of finite depth NTK and finite depth iteration
OfDEQ-NTK.
Implementation. For DEQ-NTK, aligned with the theory, We normalize the dataset such that each
data point has unit length. The fixed point Σ* (x, y) is solved by using the modified Powell hybrid
method (Powell, 1970). Notice these root finding problems are one-dimensional, hence can be
quickily solved. For CDEQ-NTK, the input data X has dimension P × Q × C0, and We normalize
X s.t 11 Xij ∣∣2 = 1 for any i ∈ [P ],j ∈ [Q]. We set q = 3 and stride 1. The fixed point Σ*(x, y) ∈
RP×Q×P×q is approximated by running 20 iterations of eq. (17), eq. (18), and eq. (20). The actual
CDEQ-NTK Θ(x, y) is then calculated by solving the sparse linear system eq. (21).
After obtaining the NTK matrix, we apply kernel regressions (without regularization unless stated
otherwise). For any label y ∈ {1,...,n}, denote its one-hot encoding by e§. Let 1 ∈ Rn be an all-1
vector, we train on the new encoding -0.1 ∙ 1 + ey. That is, we change the “1” to 0.9, and the “0”
to -0.1, as suggested by Novak et al. (2018).
Figure 1: Finite depth NTK vs. finite depth iteration of DEQ-NTK. In all experiments, the NTK
is initialized with σW and σb in the title. For DEQ-NTK we set σU =琮—0.1 in the title, and
the actual σb = 0.1. All models are trained on 1000 CIFAR-10 data and tested on 100 test data
for 20 random draws. The error bar represents the 95% confidence interval (CI). As expected, as
the depth increases, the performance of NTKs drop, eventually their 95% CI becomes a singleton,
yet the performance of DEQs stabilize. Also note with larger σW, the freezing of NTK takes more
depths to happen.
Result. On MNIST data, we test the performance of DEQ-NTK with σW = 0.25, σU =
0.25, σ2 = 0.5 and achieve 98.6% test accuracy. The results are listed in table 1.
On CIFAR-10, we trained DEQ-NTK with three different sets of random initializations. These
initializations are not fine-tuned, yet we can still see they are comparable, or even superior, to the
finite-depth NTK with carefully chosen regularization. For CDEQ-NTK, we compute the kernel
matrix on 2000 training data and tested on 10000 samples. See the results in table 2.
7
Under review as a conference paper at ICLR 2021
MNIST
Method	Model size	Acc.
DEQ-NTK		98.6%
Neural ODE	84K	98.2%
MON DEQ	84K	98.2%
Table 1: Performance of DEQ-NTK on MNIST dataset, compared to neural ODE (Chen et al.,
2018b) and monotone operator DEQ, see these results from Winston & Kolter (2020).
We should emphasize that the calculation of NTK requires a huge amount of computing resource,
even for shallow networks. On the other hand, our method provides an efficient way to compute
a special NTK with infinite depth. Typically, training a DEQ-NTK on all CIFAR-10 data takes
around 400 CPU hour, and the training hour of CDEQ-NTK halves that of its finite-depth CNTK
counterpart, as We only need to calculate Σ(h), whereas the actual CNTK needs to calculate both
Σ(h) and Θ(h).
CIFAR-10
Method	Parameters	Acc.
DEQ-NTK	σW = 0.25, σU = 0.25, σ2 = 0.5	59.08%
DEQ-NTK	σW = 0.6, σU = 0.4, σ2 = 0	59.77%
DEQ-NTK	σW = 0.8, σU = 0.2, σ2 =0	59.43%
NTK with ZCA regularization	σW = 2,σ2 = 0.01	59.7%
DEQ-CNTK With 2000 training data	σW = 0.65, σU = 0.35	37.49%
Table 2: Performance of DEQ-NTK and CDEQ-NTK on CIFAR-10 dataset, see Lee et al. (2020).
for NTK with ZCA regularization.
With a smaller dataset with 1000 training data and 100 test data from CIFAR-10, we evaluate the
performance of NTK and the finite depth iteration of DEQ-NTK, as depth increases. See fig. 1 As
proven in Jacot et al. (2019), the NTK will always “freeze” in our setting. Therefore the NTK starts
to become linearly independent as the depth increases, and its kernel regression does not have a
unique solution. To circumvent this issue, we add a regularization term r α 'θ(χ,X), where n is the
size of the training data. Such regularization is known to guarantee uniform stability (Bousquet &
Elisseeff, 2002), and it still interpolates data in the classification sense (training accuracy is 100%).
6	Conclusion
We derive NTKs for both fully-connected DEQs and convolution DEQs, and show that they can be
computed more efficiently than finite depth NTK and CNTK, especially when the depth is deep.
Moreover, the performance of DEQ-NTK and CDEQ-NTK is comparable to their finite depth NTK
counterparts. Our analysis shows that one can avoid the freeze and chaos phenomenon in infinitely
deep NTKs by using input injection. One interesting question remained open is to further understand
the role of σW, σU, σ2 in the fixed point computation, and how they affect generalizations of DEQ-
NTKs.
REFERENCES
Luis B Almeida. A learning rule for asynchronous perceptrons with feedback in a combinatorial
environment. In Artificial neural networks: concept learning, pp. 102-111. 1990.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang.
On exact computation with an infinitely wide neural net. In Advances in Neural Information
Processing Systems, pp. 8141-8150, 2019.
Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In Advances in Neural
Information Processing Systems, pp. 690-701, 2019.
8
Under review as a conference paper at ICLR 2021
Shaojie Bai, Vladlen Koltun, and J Zico Kolter. Multiscale deep equilibrium models. arXivpreprint
arXiv:2006.08656, 2020.
Olivier Bousquet and Andre Elisseeff. Stability and generalization. Journal of machine learning
research ,2(Mar):499-526,2002.
Minmin Chen, Jeffrey Pennington, and Samuel S Schoenholz. Dynamical isometry and a mean field
theory of rnns: Gating enables signal propagation in recurrent neural networks. arXiv preprint
arXiv:1806.05394, 2018a.
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differential equations. In Advances in neural information processing systems, pp. 6571-6583,
2018b.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks:
The power of initialization and a dual view on expressivity. In Advances In Neural Information
Processing Systems, pp. 2253-2261, 2016.
Adria Garriga-Alonso, Carl Edward Rasmussen, and Laurence Aitchison. Deep convolutional net-
works as shallow gaussian processes. arXivpreprint arXiv:1808.05587, 2018.
Dar Gilboa, Bo Chang, Minmin Chen, Greg Yang, Samuel S Schoenholz, Ed H Chi, and Jeffrey
Pennington. Dynamical isometry and a mean field theory of lstms and grus. arXiv preprint
arXiv:1901.08987, 2019.
Soufiane Hayou, Arnaud Doucet, and Judith Rousseau. Mean-field behaviour of neural tangent
kernel for deep neural networks. arXiv preprint arXiv:1905.13654, 2019.
Kaixuan Huang, Yuqing Wang, Molei Tao, and Tuo Zhao. Why do deep residual networks gener-
alize better than deep feedforward networks?-a neural tangent kernel perspective. arXiv preprint
arXiv:2002.06262, 2020.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing systems, pp. 8571-
8580,2018.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Freeze and chaos for dnns: an ntk view of batch
normalization, checkerboard and boundary effects. arXivpreprint arXiv:1907.05715, 2019.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha
Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165,
2017.
Jaehoon Lee, Samuel S Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak,
and Jascha Sohl-Dickstein. Finite versus infinite neural networks: an empirical study. arXiv
preprint arXiv:2007.15801, 2020.
Alexander G deG Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani.
Gaussian process behaviour in wide deep neural networks. arXiv preprint arXiv:1804.11271,
2018.
Radford M Neal. Priors for infinite networks. In Bayesian Learningfor Neural Networks, pp. 29-53.
Springer, 1996.
Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A Abolafia,
Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with many
channels are gaussian processes. arXiv preprint arXiv:1810.05148, 2018.
Fernando J Pineda. Generalization of back propagation to recurrent and higher order neural net-
works. In Neural information processing systems, pp. 602-611, 1988.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponen-
tial expressivity in deep neural networks through transient chaos. In Advances in neural informa-
tion processing systems, pp. 3360-3368, 2016.
9
Under review as a conference paper at ICLR 2021
Michael JD Powell. A hybrid method for nonlinear equations. Numerical methods for nonlinear
algebraic equations, 1970.
Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the ex-
pressive power of deep neural networks. In international conference on machine learning, pp.
2847-2854. PMLR, 2017.
Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information
propagation. arXiv PreprintarXiv:1611.01232, 2016.
Ezra Winston and J Zico Kolter. Monotone operator equilibrium networks. arXiv preprint
arXiv:2006.08591, 2020.
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel S Schoenholz, and Jeffrey Penning-
ton. Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla
convolutional neural networks. arXiv preprint arXiv:1806.05393, 2018.
Greg Yang. Tensor programs i: Wide feedforward or recurrent neural networks of any architecture
are gaussian processes. arXiv preprint arXiv:1910.12478, 2019.
Greg Yang. Tensor programs ii: Neural tangent kernel for any architecture. arXiv preprint
arXiv:2006.14548, 2020.
A DETAILS OF SECTION 3
In this section, we give the detailed derivation of DEQ-NTK. There are two terms that are different
from NTK: Σ(h) (x, y) and the extra Eθ ∣( df∂Ux), df∂Uy))in the kernel.
Let us restate the FCNN-IJ here:
Let d be the input dimension, x,y ∈ Rd be a pair of inputs, Nh be the width of the hth hidden
layers. Let N° = d and Nl+i = 1. Define the FCNN-IJ with L layers as follows:
fθh)(χ)=NWWw w (h)g(h-ι)(χ)+NUu( u (h)χ+"N ”, h ∈ [l]
g(L)(X)=σfθL)(X))
where W(h) ∈ RNh×Nh-1, U(h) ∈ RN"×d are the internal weights, and b(h) ∈ RNh are the bias
terms. These parameters are chosen using the NTK initialization. Let us pick σw, σu, σ⅛ ∈ R
arbitrarily in this section.
Proof of theorem 1. First we note that
E
[f (h+1)(χ)[∙ f(h+1) (y)[ I f((I)
~2 N
=σNW Σ σ [f (h)(x)]j
j = 1	∖	j
→Σ(h+1)(x,y) a.s
XTy + σ2
where the first line is by expansion the original expression and using the fact that W,U,b are all
independent. The last line is from the strong law of large numbers. This shows how the covariance
changes as depth increases with input injection.
10
Under review as a conference paper at ICLR 2021
Recall the splitting:
i=M 5,甘)1
=Eθ
^.
∖∕∂f(θ,x) ∂f(θ,y)	∂f(θ,x) ∂f(θ,y)	∂f(θ,x) ∂f(θ,y)∖l
^∂^∕]+ Eθ K-^Γ--∂u~∕]+ Eθ K-^Γ-/[
-----V-------' --------V-------' -------V-------'
© © ©
The following equation has been proven in many places:
L+1	L+1
(D = E σw	E	[σ(u)σ(v)] ∙ 口 t,(h,)(X,y)
h=1	(u,v)~N O"*	h'=h
L+1 /	L+1
,③=∑(σ2 ∙ ∏ ∑⑹(x,y)
h=1	h′ = h
For instance, see Arora et al. (2019). So we only need to deal with the second term
Eθ [〈"喘x ,"端y)yi. Write f = f0(x) and f = fβ (y), by chain rule, we have
/f _f_\
∂ ∂U(h), ∂U(h)/
/ ∂f ∂f(h)) ∂f ∂Fh) )∖
∖∂f(h) ∂U(h) , ∂f(h) ∂U(h) /
L+1
→σUXTy ∙ ɪɪ ∑(h')(χ,y)
h′二h
where the last line uses the existing conclusion that ^ddf,
→ ∏L+=1h ∑(2(χ,y),this
convergence almost surely holds when N → ∞ by law of large numbers.
Finally, summing( ^优)，∂U‰^) over h ∈ [L] we conclude the assertion.
□
We now proceed to explain more about the fixed point convergence in theorem 1. Let us first show
the limit converges.
Lemma 1. Use the same notations and settings in theorem 1 and theorem 2. Θ(L) (x, y) in eq. (6)
converges absolutely if σW < 1.
Proof. Since we pick x, y ∈ Sd-1, and by DEQ-NTK initialization, we always have Σ(h) (x, y) < 1
for x = y. Let P = Σ(h) (x, y), by eq. (5) and eq. (11), if σW < 1, then there exists C such that
Σ(h)(x, y) < c < 1 for all finite number of X = y on Sd-1, and large enough h. This is because
Iimh→∞ ∑(h)(x,y) = ∑*(χ,y) < ∑*(χ,χ) < 1.
Use comparison test,
L+1I	L+1	L+1
jim ∑ (∑(hT) (x,y)) ∙ ∏ ∑W)(x, y) < 1 + Iim ∑ CL+1-h.
h=1 I	h′=h	h=1
Since c < 1, the geometric sum converges absolutely, hence Θ*(x, y) converges absolutely if σW <
1, and the limit exists.	□
11
Under review as a conference paper at ICLR 2021
B Details of section 4
We first explain the choice of S in the CDEQ-NTK initialization. In the original CNTK paper (Arora
et al., 2019), the normalization is simply 1/q2. However, due to the zero-padding, 1/q2 does not
normalize all Σ(h) (x, x)	as expected: only the variances that are away from the corners are
ij,i'j'
normalized to 1, but the ones near the corner are not. [S]j is simply the number of non-zero entries
in ∣^X ].
Dijij
Now We give the proof to our main theorem.
Proof of theorem 3. Similar to the proof of theorem 1, We can split the CDEQ-NTK in two terms:
O”/ eθ K 令，号
=Eθ
∂f(θ,χ) ∂f(θ,y)
∂W ,	∂W
+ Eθ
∂f(θ,x) ∂f(θ,y)
∂U ,	∂ U
、
>^~
Θ
Omit the input symbols x, y, let
/ 、
/
_2
σW
∣^K(h)]
ij,i'j'
E
[σ(U)σ(V)].
[S]ij ∙ [S]i'j' (u,v)~N(0,Λ(h),’ .')
ij,i j
As shown in Arora et al. (2019), We have
/ d fθ (X)	d fθ Qy)	∖ →	Tr K K(L) 0 L (K (LT) 0 L (…K(h) 0 L	(K h-1∖	…ʌʌʌ
∖∂W (h) ,	∂ W (h)	/ →	∖k 0 L〈K 0 LlK 0 LIK	I
Write f = fθ(x) and f = fθ(y). Following the same step, by chain rule, we have
I ∂f, ∂f ) → Tr 卜(L) 0 L (K (LT) 0 L (…K (h) 0 L (K (O))…)):
Rewrite the above two equations in recursive form, we can calculate the L-depth iteration of CDEQ-
NTK by:
•	For the first layer Θ(O)(x, y) = Σ(O)(x, y).
[θ(h) (x,y)]
• For h = L, let
• For h = 1,
f
(X,y) + K (h)(x,"J
(x,y) + K(h) (x,y)
(22)
(23)
•	The final kernel value is Tr(Θ(L) (x,y)).
Using eq. (22) and eq. (23), we can find the following recursive relation:
Θ(L+1) (x, y) = K(L+1) (x, y) 0 L (θ(L) (x, y)) + K(h+1) (x, y)	(24)
12
Under review as a conference paper at ICLR 2021
At this point, we need to show that K * (x,y) = limL→∞ K (L) (x,y) and K *(x,y) =
limL→∞ K(L) (x, y) exist. Let us first agree that for all h ∈ [L], (ij,i'j') ∈ [P] × [Q] × [P] × [Q],
the diagonal entries of Λ(hi'j' are all ones. Indeed, these diagonal entries are 1's at h = 0 by initial-
ization. Note that iterating eqs. (17) to (20) to solve for [Σ(h)(x, y)]ij,i'j' is equivalent to iterating
f : RP×Q×P×Q → RP×Q×P×Q:
where
Rσ (Pijzjj 与
…== L ([SK⅛ Rσ N )/P(O)= K(0)
+ σU KijOi j, (26)
(25)
is applied to P(h) entrywise.
Due to CDEQ-NTK initialization, if Pij j = 1 for i ∈ [P], j ∈ [Q], then PijhIj = 1 for all iterations
h. This is true by the definition of S.
Now if we can show f is a contraction, then Σ*(x,y) = limh→∞ Σ(h) (x, y) exists, hence K * and
K* also exist. We should keep the readers aware that f : RPXQXPXQ → RP×q×p×q, so we
should be careful with the metric spaces. We want every entry of Σ(h) (x, y) to converge, since this
tensor has finitely many entries, this is equivalent to say its l∞ norm (imagine flattenning this tensor
into a vector) converges. So we can equip the domain an co-domain of f with l∞ norm (though
these are finite-dimensional spaces so we can really equip them with any norm, but picking l∞ norm
makes the proof easy).
Now we have f = L ◦ [S]ij[S]i'j' Rσ : l∞ → l∞. If we flatten the four-way tensor P(h) into a
vector, then L can be represented by a (P × Q × P × Q) × (P × Q × P × Q) dimensional matrix,
whose (kl, k'l')-th entry in the (ij, i'j')-th row is 1 if (kl, k'l') ∈ Dj,i j', and 0 otherwise. In other
words, the I1 norm of the (ij, i'j')-th row represents the number of non-zero entries in Dj,i j', but
by the CDEQ-NTK initialization, the row I1 norm divided by [S]j ∙ [S]i'j' is at most 1! Using the
fact that ∣∣L∣∣ι∞→ι∞ is the maximum I1 norm of the row, and the fact Rσ is a contraction (proven in
theorem 2), we conclude that f is indeed a contraction.
With the same spirit, we can also show that eq. (23) is a contraction if σW < 1, hence eq. (21) is
indeed the unique fixed point. This finishes the proof.	□
B.1 Computation of CDEQ-NTK
One may wish to directly compute a fixed point (or more precisely, a fixed tensor) of Θ(L) ∈
RPXQXPXQ like eq. (8). However, due to the linear operator L (which is just the ensemble of the
trace operator in eq. (20)), the entries depend on each other. Hence the system involves a (P × Q ×
P × Q) × (P × Q × P × Q)-dimensional matrix that represents L. Even if we exploit the fact that
only entries on the same “diagonal” depend on each other, L is at least P × Q × P × Q, which is
324 for CIFAR-10 data.
Moreover, this system is nonlinear. Therefore we cannot compute the fixed point Σ* by root-finding
efficiently. Instead, we approximate it using finite depth iterations, and we observe that in experi-
ments they typically converge to 10-6 accuracy in l∞ within 15 iterations.
13