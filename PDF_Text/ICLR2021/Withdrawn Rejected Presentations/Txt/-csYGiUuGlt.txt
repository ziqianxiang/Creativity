Under review as a conference paper at ICLR 2021
Convergent Adaptive Gradient Methods in De-
centralized Optimization
Anonymous authors
Paper under double-blind review
Ab stract
Adaptive gradient methods including Adam, AdaGrad, and their variants have been
very successful for training deep learning models, such as neural networks, in the
past few years. Meanwhile, given the need for distributed training procedures,
distributed optimization algorithms are at the center of attention. With the growth
of computing power and the need for using machine learning models on mobile
devices, the communication cost of distributed training algorithms needs careful
consideration. In that regard, more and more attention is shifted from the traditional
parameter server training paradigm to the decentralized one, which usually requires
lower communication costs. In this paper, we rigorously incorporate adaptive
gradient methods into decentralized training procedures and introduce novel con-
vergent decentralized adaptive gradient methods. Specifically, we propose a general
algorithmic framework that can convert existing adaptive gradient methods to their
decentralized counterparts. In addition, we thoroughly analyze the convergence
behavior of the proposed algorithmic framework and show that if a given adaptive
gradient method converges, under some specific conditions, then its decentralized
counterpart is also convergent.
1	Introduction
Distributed training of machine learning models is drawing growing attention in the past few years
due to its practical benefits and necessities. Given the evolution of computing capabilities of CPUs
and GPUs, computation time in distributed settings is gradually dominated by the communication
time in many circumstances (Chilimbi et al., 2014; McMahan et al., 2017). As a result, a large amount
of recent works has been focussing on reducing communication cost for distributed learning (Alistarh
et al., 2017; Lin et al., 2018; Wangni et al., 2018; Stich et al., 2018; Wang et al., 2018; Tang et al.,
2019). In the traditional parameter (central) server setting, where a parameter server is employed to
manage communication in the whole network, many effective communication reductions have been
proposed based on gradient compression (Aji & Heafield, 2017) and quantization (Chen et al., 2010;
Ge et al., 2013; Jegou et al., 2010) techniques. Despite these communication reduction techniques,
its cost still, usually, scales linearly with the number of workers. Due to this limitation and with
the sheer size of decentralized devices, the decentralized training paradigm (Duchi et al., 2011b),
where the parameter server is removed and each node only communicates with its neighbors, is
drawing attention. It has been shown in Lian et al. (2017) that decentralized training algorithms can
outperform parameter server-based algorithms when the training bottleneck is the communication
cost. The decentralized paradigm is also preferred when a central parameter server is not available.
In light of recent advances in nonconvex optimization, an effective way to accelerate training is
by using adaptive gradient methods like AdaGrad (Duchi et al., 2011a), Adam (Kingma & Ba,
2015) or AMSGrad (Reddi et al., 2018). Their popularity are due to their practical benefits in
training neural networks, featured by faster convergence and ease of parameter tuning compared with
Stochastic Gradient Descent (SGD) (Robbins & Monro, 1951). Despite a large amount of studies
within the distributed optimization literature, few works have considered bringing adaptive gradient
methods into distributed training, largely due to the lack of understanding of their convergence
behaviors. Notably, Reddi et al. (2020) develop the first decentralized ADAM method for distributed
optimization problems with a direct application to federated learning. An inner loop is employed to
compute mini-batch gradients on each node and a global adaptive step is applied to update the global
parameter at each outer iteration. Yet, in the settings of our paper, nodes can only communicate to
their neighbors on a fixed communication graph while a server/worker communication is required
in Reddi et al. (2020). Designing adaptive methods in such settings is highly non-trivial due to the
already complex update rules and to the interaction between the effect of using adaptive learning
1
Under review as a conference paper at ICLR 2021
rates and the decentralized communication protocols. This paper is an attempt at bridging the gap
between both realms in nonconvex optimization. Our contributions are summarized as follows:
•	In this paper, we investigate the possibility of using adaptive gradient methods in the decen-
tralized training paradigm, where nodes have only a local view of the whole communication
graph. We develop a general technique that converts an adaptive gradient method from a
centralized method to its decentralized variant.
•	By using our proposed technique, we present a new decentralized optimization algorithm,
called decentralized AMSGrad, as the decentralized counterpart of AMSGrad.
•	We provide a theoretical verification interface, in Theroem 2, for analyzing the behavior of
decentralized adaptive gradient methods obtained as a result of our technique. Thus, we
characterize the convergence rate of decentralized AMSGrad, which is the first convergent
decentralized adaptive gradient method, to the best of our knowledge.
A novel technique in our framework is a mechanism to enforce a consensus on adaptive learning
rates at different nodes. We show the importance of consensus on adaptive learning rates by proving
a divergent problem instance for a recently proposed decentralized adaptive gradient method, namely
DADAM (Nazari et al., 2019), a decentralized version of AMSGrad. Though consensus is performed
on the model parameter, DADAM lacks consensus principles on adaptive learning rates.
After having presented existing related work and important concepts of decentralized adaptive
methods in Section 2, we develop our general framework for converting any adaptive gradient
algorithm in its decentralized counterpart along with their rigorous finite-time convergence analysis
in Section 3 concluded by some illustrative examples of our framework’s behavior in practice.
Notations: xt,i denotes variable X at node i and iteration t. ∣∣∙∣∣ abs denotes the entry-wise Li norm of
a matrix, i.e. kAkabs = Pi,j |Ai,j |. We introduce important notations used throughout the paper: for
any t > 0, Gt := [gt,N] where [gt,N] denotes the matrix [gt,1,gt,2,…,gt,N] (where gt,i is a column
vector), Mt := [mt,N], Xt := [xt,N], Vf (Xt) := N PIN=I Nfi(Xt,i), Ut_：= [ut,N], Ut ：= [Ut,N],
Vt := [vt,N], Vt := [^t,N], Xt := N PN=I Xt,i, Ut := N PN=I ut,i and Ut := N PN=I Ut,i.
2	Decentralized Adaptive Training and Divergence of DADAM
2.1	Related Work
Decentralized optimization: Traditional decentralized optimization methods include well-know
algorithms such as ADMM (Boyd et al., 2011), Dual Averaging (Duchi et al., 2011b), Distributed
Subgradient Descent (Nedic & Ozdaglar, 2009). More recent algorithms include Extra (Shi et al.,
2015), Next (Di Lorenzo & Scutari, 2016), Prox-PDA (Hong et al., 2017), GNSD (Lu et al., 2019),
and Choco-SGD (Koloskova et al., 2019). While these algorithms are commonly used in applications
other than deep learning, recent algorithmic advances in the machine learning community have shown
that decentralized optimization can also be useful for training deep models such as neural networks.
Lian et al. (2017) demonstrate that a stochastic version of Decentralized Subgradient Descent can
outperform parameter server-based algorithms when the communication cost is high. Tang et al.
(2018) propose the D2 algorithm improving the convergence rate over Stochastic Subgradient Descent.
Assran et al. (2019) propose the Stochastic Gradient Push that is more robust to network failures
for training neural networks. The study of decentralized training algorithms in the machine learning
community is only at its initial stage. No existing work, to our knowledge, has seriously considered
integrating adaptive gradient methods in the setting of decentralized learning. One noteworthy
work (Nazari et al., 2019) propose a decentralized version of AMSGrad (Reddi et al., 2018) and it is
proven to satisfy some non-standard regret.
Adaptive gradient methods: Adaptive gradient methods have been popular in recent years due to
their superior performance in training neural networks. Most commonly used adaptive methods
include AdaGrad (Duchi et al., 2011a) or Adam (Kingma & Ba, 2015) and their variants. Key
features of such methods lie in the use of momentum and adaptive learning rates (which means
that the learning rate is changing during the optimization and is anisotropic, i.e. depends on the
dimension). The method of reference, called Adam, has been analyzed in Reddi et al. (2018) where
the authors point out an error in previous convergence analyses. Since then, a variety of papers have
2
Under review as a conference paper at ICLR 2021
been focussing on analyzing the convergence behavior of the numerous existing adaptive gradient
methods. Ward et al. (2019), Li & Orabona (2019) derive convergence guarantees for a variant of
AdaGrad without coordinate-wise learning rates. Chen et al. (2019) analyze the convergence behavior
of a broad class of algorithms including AMSGrad and AdaGrad. Zou & Shen (2018) provide a
unified convergence analysis for AdaGrad with momentum. Noticeable recent works on adaptive
gradient methods can be found in Agarwal et al. (2019); Luo et al. (2019); Zaheer et al. (2018).
2.2	Decentralized Optimization
In distributed optimization (with N nodes), we aim at solving the following problem
1N
min 而 Efi(X),
x∈Rd N
i=1
(1)
where x is the vector of parameters and fi is only accessible by the ith node. Through the prism of em-
pirical risk minimization procedures, fi can be viewed as the average loss of the data samples located
at node i, for all i ∈ [N]. Throughout the paper, we make the following mild assumptions required
for analyzing the convergence behavior of the different decentralized optimization algorithms:
A1. For all i ∈ [N], fi is differentiable and the gradients is L-Lipschitz, i.e., for all (x, y) ∈ Rd,
l∣Vfi(x) - Vfi(y)k ≤ Lkx - yk.
A2. We assume that, at iteration t, node i accesses a stochastic gradient gt,i. The stochastic gradients
and the gradients of fi have bounded L∞ norms, i.e. kgt,ik ≤ G∞, kVfi(x)k∞ ≤ G∞.
A3. The gradient estimators are unbiased and each coordinate have bounded variance, i.e. E[gt,i] =
Vfi(xt,i) and E[([gt,i - fi(xt,i)]j)1 2 3 4 5 6 7 8 9 10] ≤ σ2,∀t, i,j .
Assumptions A1 and A3 are standard in distributed optimization literature. A2 is slightly stronger
than the traditional assumption that the estimator has bounded variance, but is commonly used for the
analysis of adaptive gradient methods (Chen et al., 2019; Ward et al., 2019). Note that the bounded
gradient estimator assumption in A2 implies the bounded variance assumption in A3. In decentralized
optimization, the nodes are connected as a graph and each node only communicates to its neighbors.
In such case, one usually constructs a N × N matrix W for information sharing when designing
new algorithms. We denote λ% to be its ith largest eigenvalue and define λ，max(∣λ21, |》n|). The
matrix W cannot be arbitrary, its required key properties are listed in the following assumption:
A4. The matrix W satisfies: (I) PN=I Wij = 1, PN=I Wij = 1, Wij ≥ 0, (II) λι = 1, ∣λ2∣ < 1,
∖λN | < 1 and (In) Wij = 0 if node i and node j are not neighbors.
We now present the failure to converge of current decentralized adaptive method before introducing
our proposed framework for general decentralized adaptive gradient methods.
2.3	Divergence of DADAM
Recently, Nazari et al. (2019) initiated an at-
tempt to bring adaptive gradient methods into
decentralized optimization with Decentralized
ADAM (DADAM), shown in Algorithm 1.
DADAM is essentially a decentralized version
of ADAM and the key modification is the use
of a consensus step on the optimization variable
x to transmit information across the network,
encouraging its convergence. The matrix W is a
doubly stochastic matrix (which satisfies A4) for
achieving average consensus of x. Introducing
such mixing matrix is standard for decentraliz-
ing an algorithm, such as distributed gradient
descent (Nedic & Ozdaglar, 2009; Yuan et al.,
2016). It is proven in Nazari et al. (2019) that DADAM admits a non-standard regret bound in the
online setting. Nevertheless, whether the algorithm can converge to stationary points in standard
offline settings such training neural networks is still unknown. The next theorem shows that DADAM
may fail to converge in the offline settings.
Algorithm 1 DADAM (with N nodes)
1: Input: α, current point Xt, u 1 ,i = v0,i = el,
m0 = 0 and mixing matrix W
2: for t = 1,2, ∙∙∙ ,T do
3: for all i ∈ [N] do in parallel
4:	gt,i JVfi(Xt,i) + ξt,i
5:	mt,i = β1mt-1,i + (1 - β1)gt,i
6:	vt,i = β2vt-1,i + (1 - β2 )gt2,i
7:	Vt,i = β3vt,i + (1 - β3) max(vt-1,i, vt,i)
8:	xt+1 ,i = Pj=I WijXtj
mti
9:	xt+1,i = xt+1 ,i - α √^^=
10: end for
3
Under review as a conference paper at ICLR 2021
Theorem 1. There exists a problem satisfying A1-A4 where DADAM fails to converge to a stationary
points with Nf(Xt) = 0.
Proof. Consider a two-node setting with objective function f(x) = 1/2 Pi2=1 fi(x) and f1(x) =
l[|x| ≤ 1]2x2 + 1[∣X∣ > 1](4∣X∣-2), f2(x) = l[|x —1| ≤ 1](x —1)2 + l[∣x —1| > 1](2∣x —1|-1).
We set the mixing matrix W = [0.5,0.5; 0.5,0.5]. The optimal solution is x* = 1/3. Both fι and
f2 are smooth and convex with bounded gradient norm 4 and 2, respectively. We also have L = 4
(defined in A1). If we initialize with x1,1 = x1,2 = -1 and run DADAM with β1 = β2 = β3 = 0
and e ≤ 1, we will get V1,1 = 16 and Vι,2 = 4. Since |gt」| ≤ 4, ∣gt,21 ≤ 2 due to bounded gradient,
and (Vt,ι, Vt,2) are non-decreasing, we have Vt,ι = 16, Vt,2 =4, ∀t ≥ 1. Thus, after t = 1, DADAM
is equivalent to running decentralized gradient descent (DGD) (Yuan et al., 2016) with a re-scaled f1
and f2, i.e. running DGD on f0 (x) = Pi2=1 fi0(x) with f10 (x) = 0.25f1(x) and f20 (x) = 0.5f2(x),
which unique optimal x0 = 0.5. Define Xt = (xt,ι + xt,2)/2, then by Th. 2 in Yuan et al. (2016), We
have when α < 1/4, f (Xt) - f (x0) = O(1∕(αt)). Since f has a unique optima χ0, the above bound
implies Xt is converging to x0 = 0.5 which has non-zero gradient on function Vf (0.5) = 0.5.	□
Theorem 1 shows that, even though DADAM is proven to satisfy some regret bounds (Nazari et al.,
2019), it can fail to converge to stationary points in the nonconvex offline setting (common for training
neural networks). We conjecture that this inconsistency in the convergence behavior of DADAM is
due to the definition of the regret in Nazari et al. (2019). The next section presents decentralized
adaptive gradient methods that are guaranteed to converge to stationary points under assumptions and
provide a characterization of that convergence in finite-time and independently of the initialization.
3	Convergence of Decentralized Adaptive Gradient Methods
In this section, we discuss the difficulties of designing adaptive gradient methods in decentralized
optimization and introduce an algorithmic framework that can turn some existing convergent adap-
tive gradient methods to their decentralized counterparts. We also develop the first convergent
decentralized adaptive gradient method, converted from AMSGrad, as an instance of this framework.
3.1	Importance and Difficulties of Consensus on Adaptive Learning Rates
The divergent example in the previous section implies that we should synchronize the adaptive
learning rates on different nodes. This can be easily achieved in the parameter server setting where
all the nodes are sending their gradients to a central server at each iteration. The parameter server can
then exploit the received gradients to maintain a sequence of synchronized adaptive learning rates
when updating the parameters, see Reddi et al. (2020). However, in our decentralized setting, every
node can only communicate with its neighbors and such central server does not exist. Under that
setting, the information for updating the adaptive learning rates can only be shared locally instead
of broadcasted over the whole network. This makes it impossible to obtain, in a single iteration, a
synchronized adaptive learning rate update using all the information in the network.
Systemic Approach: On a systemic level, one way to alleviate this bottleneck is to design communi-
cation protocols in order to give each node access to the same aggregated gradients over the whole
network, at least periodically if not at every iteration. Therefore, the nodes can update their individual
adaptive learning rates based on the same shared information. However, such solution may introduce
an extra communication cost since it involves broadcasting the information over the whole network.
Algorithmic Approach: Our contributions being on an algorithmic level, another way to solve the
aforementioned problem is by letting the sequences of adaptive learning rates, present on different
nodes, to gradually consent, through the iterations. Intuitively, if the adaptive learning rates can
consent fast enough, the difference among the adaptive learning rates on different nodes will not
affect the convergence behavior of the algorithm. Consequently, no extra communication costs need
to be introduced. We now develop this exact idea within the existing adaptive methods stressing on
the need for a relatively low-cost and easy-to-implement consensus of adaptive learning rates.
3.2	Decentralized Adaptive Gradient Unifying Framework
We now develop a method that implements consensus of adaptive learning rates. While each node
can have different vt,i in DADAM (Algorithm 1), one can keep track of the min/max/average of these
adaptive learning rates and use that quantity as the new adaptive learning rate.
4
Under review as a conference paper at ICLR 2021
The predefinition of some convergent lower and
upper bounds may also lead to a gradual syn-
chronization of the adaptive learning rates on dif-
ferent nodes as developed for AdaBound in Luo
et al. (2019). In this paper, we present an al-
gorithm framework for decentralized adaptive
gradient methods as Algorithm 2, which uses
average consensus of vt,i (see consensus update
in line 8 and 11) to help convergence. Algo-
rithm 2 can become different adaptive gradient
methods by specifying rt as different functions.
E.g., when we choose Vt,i = 1 Pik=1 gl,i , Al-
gorithm 2 becomes a decentralized version of
AdaGrad. When one chooses Vt,i to be the adap-
tive learning rate for AMSGrad, we get decen-
tralized AMSGrad (Algorithm 3). The intuition
of using average consensus is that for adaptive
gradient methods such as AdaGrad or Adam,
Algorithm 2 Decentralized Adaptive Gradient
Method (with N nodes)
1:	Input: α, initial point xι,i = Xinit, u 1 ,i =
^0,i, mo,i = 0, mixing matrix W
2:	for t = 1,2,…，T do
3:	for all i ∈ [N] do in parallel
4:	gt,i — ▽%(Xt,i) + ξt,i
5:	mt,i = β1mt-1,i + (1 - β1)gt,i
6:	vt,i = rt(g1,i, ∙ ∙ ∙ ,gt,i)
7:	xt+1 ,i = PN=I Wijxt,j
8:	ut,i = PN=I Wijut- 1 ,j
9:	ut,i = max(ut,i, e)
10:	Xt+ι i = X.f._|_ ι - - α -/1"
t+1,i	t+ 2,i	√UtJ
11:	ut+1 ,i = ut,i - vt-1,i + vt,i
12:	end for
Vt,i approximates the second moment of the gradient estimator, the average of the estimations of those
second moments from different nodes is an estimation of second moment on the whole network. Also,
this design will not introduce any extra hyperparameters that can potentially complicate the tuning
process (e in line 9 is important for numerical stability as in vanilla Adam). The main convergence
result for the framework Algorithm 2 reads as follows:
0.5
Theorem 2. Assume A1-A4. When a ≤ ^^, Algorithm 2 yields the following regret bound

1T
T X E
t=1
≤C1 (Ta(Ef(Z1)] - mχin f(χ))+αdN^
+ C2 a2 d + C3a3 d +	(C4 + C5 a) E
T√N
T
X k(—K-2 + Vt-I)Ilabs
t=1
(2)
where ∣∣ ∙ ∣∣abs denotes the entry-wise L norm ofa matrix (i.e IlAkabs = Ei j ∣Ai7-1). The constants
Ci = max(4,4L/e), C2 = 6((βι∕(1 - β1))2 + 1/(1 - λ)2)LG∞∕e1∙5, C3 = 16L2(1 - λ)G∞∕e2,
C4 = 2∕(e1∙5(I-λ))(λ + 尸1/(1— βI))G∞, C5 = 2∕(e2(I-λ))L(λ + 尸1/(1一/I))G∞ +4/(e (1 -
λ))LG∞ are independent of d, T and N. In addition, N PN=Illxt,i — Xt∣∣2 ≤ α2 (y-λ) dG∞ 1
which quantifies the consensus error.
Theorem 2 shows how the convergence guarantee is affected by different factors. In addition, one
can specify α to show convergence in terms of T, d, and N. An immediate result is by setting
α = √N/√Td, which is shown in Corollary 2.1.
Corollary 2.1. AssumeA1-A4. Set a = √N/√Td. When a ≤ 165, Algorithm 2 yields thefollowing
regret bound
T XE ]∣∣ fχ^∣∣ j ≤C1 √√N ((Ef(Z1)] - mχin f (x))+σ2)+C2 T+C3 TN⅛
+ (C4 T√N + C5 T⅛ ) e [Vt ]	⑶
T
where VT :=	t=1 ∣(-Vt-2 + Vt-1)∣abs and C1, C2, C3, C4, C5 are defined in Theorem 2.
Corollary 2.1 shows that if E[Vt] = o(T) and Ut is upper bounded, then Algorithm 2 is guaranteed to
converge to stationary points of the loss function. Intuitively, this means that if the adaptive learning
rates on different nodes do not change too fast, the algorithm can converge. It is shown in Chen et al.
(2019) that such a condition can be satisfied by AdaGrad and AMSGrad. Besides, if this condition is
5
Under review as a conference paper at ICLR 2021
violated, the algorithm may diverge (e.g., Adam). Later, we will show convergence of decentralized
versions of AMSGrad and AdaGrad by bounding this term as O(N d) and O(N d log(T)), respectively.
The intuition E[Vt] = o(T) can guarantee divergence is that the correlation between vt,i and mt,i
(due to their shared dependency on historical gradients) can make update direction negatively
correlated with true gradient in expectation, leading to a non-negligible bias in updates. However, the
total bias across T iterations introduced by such a correlation is bounded by the term E[VT]. Thus, if
E[VT] grows sublinearly with T, convergence can still be guaranteed. Furthermore, Corollary 2.1
conveys the benefits of using more nodes in the graph. When T is large enough such that the term
O(√N/√Td) dominates the RHS of (3), linear speedup can be achieved by increasing N.
We now present, in Algorithm 3, a notable special case of our algorithmic framework, namely
Decentralized AMSGrad, which is a decentralized variant of AMSGrad. Compared with DADAM,
the above algorithm exhibits a dynamic average consensus mechanism to keep track of the average
of {Vt,i}N=ι, stored as Ut,i on ith node, and uses ut,i := max(Ut,i, e) for updating the adaptive
learning rate for ith node. As the number of iteration grows, even though Vt,i on different nodes can
converge to different constants, the ut,i will converge to the same number limN PN=1 Vt,i if the
limit exists. This average consensus mechanism enables the consensus of adaptive learning rates on
different nodes, which accordingly guarantees the convergence of the method to stationary points.
The consensus of adaptive learning rates is the key difference between decentralized AMSGrad and
DADAM and is the reason why decentralized AMSGrad is convergent while DADAM is not.
One may notice that decentralized AMSGrad
does not reduce to AMSGrad for N = 1 since
the quantity ut,i in line 10 is calculated based
on vt-1,i instead ofvt,i. This design encourages
the execution of gradient computation and com-
munication in a parallel manner. Specifically,
line 4-7 (line 4-6) in Algorithm 3 (Algorithm 2)
can be executed in parallel with line 8-9 (line
7-8) to overlap communication and computation
time. If ut,i depends on vt,i which in turn de-
pends on gt,i, the gradient computation must
finish before the consensus step of the adaptive
learning rate in line 9. This can slow down the
running time per-iteration of the algorithm. To
avoid such delayed adaptive learning, adding
Ut-1 ,i = Ut,i - Vt-ι,i + Vt,i before line 9 and
get rid of line 12 in Algorithm 2 is an option.
Similar convergence guarantees will hold since
one can easily modify our proof of Theorem 2
for such update rule. As stated above, Algo-
rithm 3 converges, with the following rate:
Theorem 3. Assume A1-A4. Set α = 1/VTd.
regret bound
Algorithm 3 Decentralized AMSGrad (with N
nodes)
1:	Input: learning rate α, initial point x1,i =
xinit, u 1 ,i = v0,i = e1 (with e ≥ 0), m0,i =
0, mixing matrix W
2:	for t = 1,2,…，T do
3:	for all i ∈ [N] do in parallel
4:	gt,i ~ Nfi(xt,i) + ξt,i
5:	mt,i = β1mt-1,i + (1 - β1)gt,i
6:	vt,i = β2vt-1,i + (1 - β2)gt2,i
7:	Vt,i = max(Vt-i,i, vt,i)
8:	xt+1 ,i = Pj = I WijXtj
9:	ut,i = PN=I WijUt-1 ,j
10:	ut,i = maχ(ut,i, e)
mti
11:	xt+1,i = xt+ 2 ,i - α √ut=
12:	ut+1 ,i = ut,i - vt-1,i + vt,i
13:	end for
0.5
When α ≤ 1^L, Algorithm 3 yields the following
1T
T X E
t=1
Vf(X t)『L	「0 √d	⑺ ∣∕∣「0 Nl「0	N1.5	√Nd	Nd0.5
一Uy「||| ≤	C1 √≡(Df+σ)+C2 t + C3 FE+C4 ~+C5不了
where Df := E[f(Z1)] -minx f(x), C10 = C1, C20 = C2, C30 = C3, C40 =C4 G2∞ and C50 =C5G2∞.
C1, C2, C3, C4, C5 are constants independent of d, T and N defined in Theorem 2. In addition, the
COnSenSUSOfvariableSatdifferentnOdeSiSgivenby N PN=IIlxt,i — Xt∣∣2 ≤ T (ι-λ) G∞⅛
Theorem 3 shows that Algorithm 3 converges with a rate of O(√d/√T) when T is large, which is
the best known convergence rate under the given assumptions. Note that in some related works, SGD
admits a convergence rate of O(1/√T) without any dependence on the dimension of the problem.
Such improved convergence rate is derived under the assumption that the gradient estimator have a
bounded L2 norm, which can thus hide a dependency of √d in the final convergence rate. Another
6
Under review as a conference paper at ICLR 2021
remark is the convergence measure can be converted to T PT=I E h∣∣ Vf (Xt)∣∣2i using the fact that
∣∣U t k ∞ ≤ G∞ (by update rule of Algorithm 3), for the ease of comparison with existing literature.
3.3	Convergence Analysis
The detailed proofs of this section are reported in the supplementary material.
Proof of Theorem 2: We now present a proof sketch for out main convergence result of Algorithm 2.
Step 1: Reparameterization. Similarly to Yan et al. (2018); Chen et al. (2019) with SGD (with
momentum) and centralized adaptive gradient methods, define the following auxiliary sequence:
Zt = Xt + 1⅛(X t - X t-1),
(4)
with Xo，Xι. Such an auxiliary sequence can help Us deal with the bias brought by the momentum
and simplifies the convergence analysis. An intermediary result needed to conduct our proof reads:
Lemma 1. For the sequence defined in (4), we have
Zt+1 - Zt=α ι-⅛^N X mt-ι,i ® (
1 - β1 N i=1
ɪ X &
N = √u,
1
1
—
√Ut-1,i
Lemma 1 does not display any momentum term in NN Pi=1 gτ=. This simplification is convenient
N i=1 ut,i
since it is directly related to the current gradients instead of the exponential average of past gradients.
Step 2: Smoothness. Using smoothness assumption A1 involves the following scalar product term:
Kt =〈Vf (Zt), N PN=I Vfi(χt,i)/pUtii which can be lower bounded by:
片〉1	Vf(Xt)
κt ≥ 2
3 Vf(Zt)-Vf(X t)
2
3
——
2
N PN=I Vfi(Xt,J - Vf (Xt)
2
2
——
2
The above inequality substituted in the smoothness condition f (Zt+1) ≤ f(Zt) + hVf (Zt), Zt+1 -
Zti + L∣∣Zt+ι - Ztk2 yields:
1T
⅛ X E
t=1
Vf (Xt) ∣∣	2	L r.l	ll2i 2 βιDi	2D2	3D3
-UiFIl ≤ ταEN] + ταXE[kZt+1-Ztk ] + t=瓦 + ~τ+ + 亍,
(5)
where ∆f := E[f(Z1)] - E[f (ZT +1)] D1, D2 and D3 are three terms, defined in the supplementary
material, and which can be tightly bounded from above. We first bound D3 using the following
quantities of interest:
X ∣∣Zt -xt∣∣2 ≤ T (1 -1βι) α2d^^∞ and XNX∣∣xt,i-Xt∣∣2 ≤ Tα2 (ɪ-ʌ) dG∞~
where λ = max(∣λ2∣, |》n |) and recall that λi is ith largest eigenvalue of W.
Then, concerning the term D2, few derivations, not detailed here for simplicity, yields:
D2 ≤ g∞ e
2 — N
TN
X 声5 k - X UtqlqT Ilabs
t=1
l=2
where ql is the eigenvector corresponding to lth largest eigenvalue of W and IITlabs is the entry-wise
L1 norm of matrices. We can also show that
T-1
Ek-E UtqlqTkabs ≤ √N∑
t=1	l=2
o=0
k---^∙∣∣(-Vo-I + Vo)IIabs ,
1-λ
T
N
7
Under review as a conference paper at ICLR 2021
T-1
resulting in an upper bound for D2 proportional to	o=0 k(-Vo-1 + Vo)kabs. Similarly:
211
1 ≤	∞ 2ir5 √N E
1T
E X kf-2 +Vι)kabs
Step 3: Bounding the drift term variance. An important term that needs upper bounding in our proof
is the variance of the gradients multiplied (element-wise) by the adaptive learning rate:
≤ E[krfk2]+NZ,
where Γf := 1/N Pi=l Vfi(Xt,i)/√u^^. Two consecutive and simple bounding of the above yields:
TTT
XE[kΓfk2] ≤ 2XE[krUk2]+2XE
t=1	t=1	t=1
and
X E[kru k2] ≤2 X E (I ς≡ ∣2]+2 X E (I JNX St 蒙 fi"i
(6)
Then, by plugging the LHS of (6) in (5), and further bounding as operated for D2 , D3 (see supple-
ment), we obtain the desired bound in Theorem 2.
Proof of Theorem 3: Recall the bound in (3) of Theorem 2. Since Algorithm 3 is a special
case of Algorithm 2, the remaining of the proof consists in characterizing the growth rate of
T
E[ t=1 k(-Vt-2 + Vt-1ikabs]. By construction, Vt is non decreasing, then it can be shown that
E[Pt=ι ki-Vt-2 + Vt-ιikabs] = E[pi=ι Pd=Ii-[V0,i]j + [Vτ-ι,i]j)]. Besides, since for all
t, i, kgt,ik∞ ≤ G∞ and vt,i is an exponential moving average of g2,i, k = 1, 2, ∙∙∙ , t, we have
| [vt,i] j | ≤ G∞ for all t, i, j. By construction of Vt, we also observe that each element of Vt cannot
be greater than G∞, i.e. ∣[Vt,i]j | ≤ G∞ for all t, i, j. Given that [Vo,i]j ≥ 0 , we have
T
E X Ili-Vt-2 + Vt-Iillabs
t=1
Nd
E XX(-[V0,i]j + [Vτ-ι,i]ji
i=1 j=1
Nd
≤XXE[G2∞] = N dG2∞ .
i=1 j=1
Substituting into (3) yields the desired convergence bound for Algorithm 3.
3.4	Illustrative Numerical Experiments
In this section, we conduct some experiments to test the performance of Decentralized AMSGrad,
developed in Algorithm 3, on both homogeneous data and heterogeneous data distribution (i.e. the
data generating distribution on different nodes are assumed to be different). Comparison with
DADAM and the decentralized stochastic gradient descent (DGD) developed in Lian et al. (2017) are
conducted. We train a Convolutional Neural Network (CNN) with 3 convolution layers followed by a
fully connected layer on MNIST (LeCun, 1998). We set = 10-6 for both Decentralized AMSGrad
and DADAM. The learning rate is chosen from the grid [10-1, 10-2, 10-3, 10-4, 10-5, 10-6] based
on validation accuracy for all algorithms. In the following experiments, the graph contains 5 nodes
and each node can only communicate with its two adjacent neighbors forming a cycle. Regarding the
mixing matrix W, we set Wij = 1/3 if nodes i andj are neighbors and Wij = 0 otherwise. More
details and experiments can be found in the supplementary material of our paper.
0	500	1000	0	200	400	600	800	0	0.5	1	1.5	2	0	0.5	1	1.5	2
iterations
iterations	iterations	×104	iterations	×104
(a) Homogeneous data	(b) Heterogeneous data
Figure 1: Training loss and Testing accuracy for homogeneous and heterogeneous data
8
Under review as a conference paper at ICLR 2021
Homogeneous data: The whole dataset is shuffled and evenly split into different nodes. Such a
setting is possible when the nodes are in a computer cluster. We see, Figure 1(a), that decentralized
AMSGrad and DADAM perform quite similarly while DGD is much slower both in terms of training
loss and test accuracy. Though the (possible) non convergence of DADAM, mentioned in this paper,
its performance are empirically good on homogeneous data. The reason is that the adaptive learning
rates tend to be similar on different nodes in presence of homogeneous data distribution. We thus
compare these algorithms under the heterogeneous regime.
Heterogeneous data: Here, each node only contains training data with two labels out of ten. Such
a setting is common when data shuffling is prohibited, such as in federated learning. We can see
that each algorithm converges significantly slower than with homogeneous data. Especially, the
performance of DADAM deteriorates significantly. Decentralized AMSGrad achieves the best
training and testing performance in that setting as observed Figure 1(b).
4 Extension to AdaGrad
In this section, we provide a decentralized version of AdaGrad (optionally with momentum) converted
by Algorithm 2, further supporting the usefulness of the decentralization framework.
The required modification for decentralize AdaGrad is to specify line 4 of Algorithm 2 as
vt,i = -1-vt-1,i + tgt,i
which is equivalent to vt,i = 1 P[=ι gk i. Throughout this section, We will call this algorithm
decentralized AdaGrad. There are two details in the algorithm worth mentioning. One is that the
framework uses momentum mt,i in updates, while original AdaGrad does not use momentum. The
momentum can be turned off by setting β1 = 0 and the convergence results will hold. The other
one is that in decentralized AdaGrad, We use average instead of sum in Vt,i. I,e. ^t,i = ɪ P；= g/
This is different from original AdaGrad which should use Vt,i = Pk=I g2 〃 The reason is in original
AdaGrad, a constant stepsize (a independent of t or T) is used with V^ = P；=i g2 i and this is
equivalent to using a well-known diminishing stepsize sequence at = √^ with Vt,i = ɪ Pk=I g2.
In our convergence analysis which will be presented later, we will use a constant stepsize α = O(方)
to replace the diminishing stepsize sequence at = O(√1t). Such a replacement is popularly used in
SGD analysis to simplify analysis and achieving better convergence rate. In addition, it is easy to
modify our theoretical framework to apply diminishing stepsize sequences such as at = O(√1t).
The convergence analysis for decentralized AdaGrad is shown in Theorem 4.
Theorem 4. Assume A1-A4. Set a = √N/√Td. When a ≤ 10.5, decentralized AdaGrad yields the
following regret bound
1 XX JIl W(X t)∣门	<C1 √dD0+C2	+	C3N 1.5	+√N (1	+ log(T)) (dC 0+	√dC 0)
T⅛E [II —U[L∣∣ ] ≤ √TNDf + T +	+ T	(dC4 + KC5),
where Df0 := E[f(Z1)] - minzf(z)] + σ2, C10 = C1, C20 = C2, C30 = C3, C40 = C4G2∞ and
C50 = C5G2∞. C1, C2, C3, C4, C5 are defined in Theorem 2 independent of d, T and N. In addition,
the consensus of variables at different nodes is given by N PN= 1 ∣ ∣xt,i - X t∣∣2 ≤ N (1-λ) G∞ ⅛.
5 Conclusion
This paper studies the problem of designing adaptive gradient methods for decentralized training. We
propose a unifying algorithmic framework that can convert existing adaptive gradient methods to
decentralized settings. With rigorous convergence analysis, we show that if the original algorithm
satisfies converges under some minor conditions, the converted algorithm obtained using our proposed
framework is guaranteed to converge to stationary points of the regret function. By applying
our framework to AMSGrad, we propose the first convergent adaptive gradient methods, namely
Decentralized AMSGrad. Experiments show that the proposed algorithm achieves better performance
than the baselines.
9
Under review as a conference paper at ICLR 2021
References
Naman Agarwal, Brian Bullins, Xinyi Chen, Elad Hazan, Karan Singh, Cyril Zhang, and Yi Zhang.
Efficient full-matrix adaptive regularization. In International Conference on Machine Learning,
pp.102-110, 2019.
Alham Fikri Aji and Kenneth Heafield. Sparse communication for distributed gradient descent. In
Empirical Methods in Natural Language Processing, pp. 440-445, 2017.
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. Qsgd: Communication-
efficient sgd via gradient quantization and encoding. In Advances in Neural Information Processing
Systems, pp. 1709-1720, 2017.
Mahmoud Assran, Nicolas Loizou, Nicolas Ballas, and Mike Rabbat. Stochastic gradient push for
distributed deep learning. In International Conference on Machine Learning, pp. 344-353, 2019.
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et al. Distributed optimization
and statistical learning via the alternating direction method of multipliers. Foundations and
TrendsR in Machine learning, 3(1):1-122, 2011.
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of adam-type
algorithms for non-convex optimization. In International Conference for Learning Representations,
2019.
Yongjian Chen, Tao Guan, and Cheng Wang. Approximate nearest neighbor search by residual vector
quantization. Sensors, 10(12):11259-11273, 2010.
Trishul Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman. Project adam:
Building an efficient and scalable deep learning training system. In Symposium on Operating
Systems Design and Implementation, pp. 571-582, 2014.
Paolo Di Lorenzo and Gesualdo Scutari. Next: In-network nonconvex optimization. IEEE Transac-
tions on Signal and Information Processing over Networks, 2(2):120-136, 2016.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011a.
John C Duchi, Alekh Agarwal, and Martin J Wainwright. Dual averaging for distributed optimization:
Convergence analysis and network scaling. IEEE Transactions on Automatic control, 57(3):
592-606, 2011b.
Tiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun. Optimized product quantization for approximate
nearest neighbor search. In IEEE Conference on Computer Vision and Pattern Recognition, pp.
2946-2953, 2013.
Mingyi Hong, Davood Hajinezhad, and Ming-Min Zhao. Prox-pda: The proximal primal-dual
algorithm for fast distributed nonconvex optimization and learning over networks. In International
Conference on Machine Learning, pp. 1529-1538, 2017.
Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search.
IEEE transactions on pattern analysis and machine intelligence, 33(1):117-128, 2010.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2015.
Anastasia Koloskova, Sebastian U Stich, and Martin Jaggi. Decentralized stochastic optimization
and gossip algorithms with compressed communication. In International Conference on Machine
Learning, pp. 3478-3487, 2019.
Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaptive
stepsizes. In International Conference on Artificial Intelligence and Statistics, pp. 983-992, 2019.
10
Under review as a conference paper at ICLR 2021
Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. Can decentralized
algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic
gradient descent. In Advances in Neural Information Processing Systems, pp. 5330-5340, 2017.
Yujun Lin, Song Han, Huizi Mao, Yu Wang, and William J Dally. Deep gradient compression:
Reducing the communication bandwidth for distributed training. International Conference on
Learning Representations, 2018.
Songtao Lu, Xinwei Zhang, Haoran Sun, and Mingyi Hong. Gnsd: A gradient-tracking based
nonconvex stochastic algorithm for decentralized optimization. In 2019 IEEE Data Science
Workshop (DSW), pp. 315-321, 2019.
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
bound of learning rate. In International Conference for Learning Representations, 2019.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial Intelli-
gence and Statistics, pp. 1273-1282. PMLR, 2017.
Parvin Nazari, Davoud Ataee Tarzanagh, and George Michailidis. Dadam: A consensus-based
distributed adaptive gradient method for online optimization. arXiv preprint arXiv:1901.09109,
2019.
Angelia Nedic and Asuman Ozdaglar. Distributed subgradient methods for multi-agent optimization.
IEEE Transactions on Automatic Control, 54(1):48, 2009.
SaShank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny,
Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint
arXiv:2003.00295, 2020.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In
International Conference on Learning Representations, 2018.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical
statistics, pp. 400-407, 1951.
Wei Shi, Qing Ling, Gang Wu, and Wotao Yin. Extra: An exact first-order algorithm for decentralized
consensus optimization. SIAM Journal on Optimization, 25(2):944-966, 2015.
Sebastian U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsified sgd with memory. In
Advances in Neural Information Processing Systems, pp. 4447-4458, 2018.
Hanlin Tang, Xiangru Lian, Ming Yan, Ce Zhang, and Ji Liu. D2 : Decentralized training over
decentralized data. In International Conference on Machine Learning, pp. 4848-4856, 2018.
Hanlin Tang, Chen Yu, Xiangru Lian, Tong Zhang, and Ji Liu. Doublesqueeze: Parallel stochastic
gradient descent with double-pass error-compensated compression. In International Conference
on Machine Learning, pp. 6155-6165, 2019.
Hongyi Wang, Scott Sievert, Shengchao Liu, Zachary Charles, Dimitris Papailiopoulos, and Stephen
Wright. Atomo: Communication-efficient learning via atomic sparsification. In Advances in
Neural Information Processing Systems, pp. 9850-9861, 2018.
Jianqiao Wangni, Jialei Wang, Ji Liu, and Tong Zhang. Gradient sparsification for communication-
efficient distributed optimization. In Advances in Neural Information Processing Systems, pp.
1299-1309, 2018.
Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp convergence over nonconvex
landscapes. In International Conference on Machine Learning, pp. 6677-6686, 2019.
Yan Yan, Tianbao Yang, Zhe Li, Qihang Lin, and Yi Yang. A unified analysis of stochastic momentum
methods for deep learning. In International Joint Conference on Artificial Intelligence, pp. 2955-
2961, 2018.
11
Under review as a conference paper at ICLR 2021
Kun Yuan, Qing Ling, and Wotao Yin. On the convergence of decentralized gradient descent. SIAM
Journal on OPtimization, 26(3):1835-1854, 2016.
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive methods
for nonconvex optimization. In Advances in Neural Information Processing Systems, pp. 9793-
9803, 2018.
Fangyu Zou and Li Shen. On the convergence of weighted adagrad with momentum for training deep
neural networks. arXiv PrePrint arXiv:1808.03408, 2018.
12
Under review as a conference paper at ICLR 2021
A Proof of Auxiliary Lemmas
Lemma 1. For the sequence defined in (10), we have
7	7 — β1	1 L~ λλ 1	1	1	∖	1 L gt,i
Zt+1- Zt = α E N = mt-1" ® (√Ut-j-际)-αN A 际.
(7)
Proof: By update rule of Algorithm 2, we first have
_	1 N
X t+1 = NExt+1,i
i=1
=X t - 1 X α 工
N = M ,
where (i) is due to an interchange of summation and i=1 Wij = 1. Then, we have
Zt+1 - Zt =Xt+1 - Xt + 1-β(Xt+1 - Xt) - 1-β(Xt+1 - Xt)
1
1- βι
1
1-	βι
(Xt+1 - Xt) - 1 -1βι (Xt+1 - Xt)
—
1⅛ 卜 . X
1
1-βι
αβ1mt-1,i + (1 - β1)gt,i
β1	1
α---------
1	- β1 N
i=1	t,i
N1
mt-1,i © ( l-
仁	√ut-1,i
α^t-⅛
√ut-1,i J
β1	1	1	mt-1,i
1 - β1 N =	√ut-1,i
N
1 IX	gt,i
N=√u^,
—
—
1
which is the desired result.
Lemma A.1. Given a set OfnUmbers a1, ∙ ∙ ∙ ,an and denote theirmean to be a = ɪ En=I αi ∙ Define
bi(r)，= max(ai, r) and b(r) = n Pn=1 bi(r). For any r and r0 with r0 ≥ r we have
nn
X |bi(r) - ab(r)| ≥ X |bi(r0) - ab(r0)|	(8)
i=1	i=1
and when r ≤ mini∈[n] ai, we have
nn
X |bi(r) - ab(r)| = X |ai -aa| .	(9)
i=1	i=1
Proof: Without loss of generality, assume ai ≤ aj when i < j, i.e. ai is a non-decreasing sequence.
Define
n	n	1n
h(r) =工 ∣bi(r) - b(r)∣ =工 | max(ai,r) - n ɪ2max(αj,r)∣ .
13
Under review as a conference paper at ICLR 2021
We need to prove that h is a non-increasing function of r. First, it is easy to see that h is a continuous
function of r with non-differentiable points r = ai , i ∈ [n], thus h is a piece-wise linear function.
Next, we will prove that h(r) is non-increasing in each piece. Define l(r) to be the largest index
with a(l(r)) < r, and s(r) to be the largest index with a§(r)< b(r). Note that We have for i ≤ l(r),
bi(r) = r and for i ≤ s(r) bi(r) 一 b(r) ≤ 0 since a% is a non-decreasing sequence. Therefore, We
have
l(r)	s(r)	n
h(r) = X(Hr)-r)+	X (Ibcr)- ai) +	X (ai-bCr))
i=1	i=l(r)+1	i=s(r)+1
and
.	1 I	n ∖
b(r) = - l(r)r + E a%	.
i=l(r)+1
Taking derivative of the above form, we know the derivative of h(r) at differentiable points is
h0(r) =l(r)(3 - 1) + (s(r) - l(r))	- (- - s(r))
-	--
=ɪɪrɪ((l(r) - —) + (s(r) - l(r)) - (— — s(r))).
-
Since we have s(r) ≤ we know (l(r) - ) + (s(r) - l(r)) - ( - s(r)) ≤ 0 and thus
h0(r) ≤ 0,
which means h(r) is non-increasing in each piece. Combining with the fact that h(r) is continuous,
(8) is proven. When r ≤ a(i), we have b(i) = max(ai, r) = r, for all r ∈ [-] and Ib(r) =
1 Pn= ι ai = a which proves (9).	□
B Proof of Theorem 2
To prove convergence of the algorithm, we first define an auxiliary sequence
βι
Zt = Xt + ∖(Xt - Xt-ι),	(10)
1 - β1
with Xo ，Xι. Since E[gt,i] = Vf (xt,i) and uti is a function of Gi：t-i (which denotes
Gι, G2,…,Gt-ι), we have
EGt |G1:t-1
1	gt,i # = ɪ	Vfi(Xt,i)
N i=1 √utj]	N i=1	√ut7
Assuming smoothness (A1) we have
f (Zt+ι) ≤ f (Zt) + hVf(Zt), Zt+1 - Zti + 2 ∣∣Zt+ι - Ztk2.
Using Lemma 1 into the above inequality and take expectation over Gt given G1:t-1, we have
EGt|G1:t-1 [f (Zt+1)]
≤f (Zt) - α BZtt) NN X v√⅛^: + LEsGJ
+α
βι
1 - βι
EGt |G91 KVf(Zt)，N X mt-1,i 0 (
kZt+1-Ztk2
1
√Ut-1,i
14
Under review as a conference paper at ICLR 2021
Then take expectation over G1:t-1 and rearrange, we have
。心"〈 X
≤E[f(Zt)] - E[f (Zt+1)] + 2E [kZt+1 - Ztk2]
+α τ⅛ E "*vf(zt),三 X mtτ,i θ (√⅛- √u
(11)
(12)
In addition, we have
X v√uχr2)
*vf (Zt), N X VpUl++*vf (Zt), N X Vfi(Xt- θ
(13)
and the first term on RHS of the equality can be lower bounded as
X vppUr)
Vf(Zt)
U1/4
N P屋 Vfi(Xt,i) 2	1
------------------ ------
Vf (Zt)- N PN=I Vfi(Xt,i)
U/
≥ 41" I2+1
Vf(Xt)
U1/4
Vf (Zt) - N PN=I Vfi(Xt,i )
可
Vf(Zt)- Vf(Xt)
U1/4
N PN=I Vfi(Xt,i) - Vf (Xt)
可
3	Vf (Zt) -Vf (Xt)
2
3
——
2
N PlN=I VP(XtZi) - Vf (X t)
(14)
1
2
—
1
2
2
1
+ 2
2
2
—
1
2
≥ 11 f I2
2
2
where the inequalities are all due to Cauchy-Schwartz. Substituting (14) and (13) into (11), we get
JaE
2
f I2
≤E[f (Zt)] - E[f(Zt+1)] + — E [kZt+1 - Ztk2]
+a ι⅛ EKVf(Zt),〈 X K θ (√⅛
-aE Kvf(Zt),： X Vfi(Xt,i) Θ
2
+
3
+——aE
2
N PN=I Vfi(Xt,i) - Vf (Xt)
U1/4
Vf (Zt) - Vf (Xt)
U1/4
2
15
Under review as a conference paper at ICLR 2021
Then sum over the above inequality from t = 1 to T and divide both sides by Tɑ∕2, We have
T X E U” :
2	LT
≤Ta(Ef(Z1)] - Ef(ZT +1)]) + Ta EE [kZt+1 - Ztk2]
t=1
2
+
3T
+ T ΣE
t=1
D2
N PN=I Nfi(Xt,i) - ▽/(Xt)
V/(Zt) -V/(X t)『
UFIl
(15)
U1/4
*{z
D3
}
NoW We need to upper bound all the terms on RHS of the above inequality to get the convergence
rate. For the terms composing D3 in (15), We can upper bound them by
2
and
Vf(Zt)- Vf(Xt)
U1/4
1	,,	——..9
≤ -~Er ∣∣Vf(Zt)-V/(χt)∣∣
minj∈[d] [Ut	]j
≤ L-------匚k ∣∣Zt - X t∣∣2
minj∈[d][Ut	]j'----------}
(16)
*{z
D4
N PIN=I Vfi(Xt,J - Vf (X t)
N
X∣∣v∕i(χt,i)-v∕(X t)『
≤L
11
i=1
N
Xl∣χt,i-X t∣∣2,
i=1
`-----{-----}
D5
(17)
using Jensen,s inequality, LiPschitz continuity of fi, and the fact that f = N PN=I fi. NeXt we need
to bound D4 and D5. Recall the update rule of Xt, We have
t-2
Xt = Xt-1W - a^t=L = X1Wt-1 - α X 〃--1 Wk ,	(18)
Ut-1	k=0 Ut-k-1
where we define W0 = I. Since W is a symmetric matrix, we can decomPose it as W = QΛQT
where Q is a orthonormal matrix and Λ is a diagonal matrix whose diagonal elements corresPond
to eigenvalues of W in an descending order, i.e. Λii = λi with λi being ith largest eigenvalue of
W. In addition, because W is a doubly stochastic matrix, we know λι = 1 and qι = √N. With
eigen-decomPosition of W, we can rewrite D5 as
NN
X ∣xt,i- X tH = kXt- X t1N IlF = IlXtQQT - Xt N 1N1N IlF = X IlXtqlk2.	(19)
i=1	l=2
16
Under review as a conference paper at ICLR 2021
In addition, we can rewrite (18) as
Xt = X1Wt-1 - a X Mti-I Wk
k = 0 V Ut-k-1
Xι-α X JM-=^ QN QT,
k=0 √Ut-k-1
(20)
where the last equality is because χι,i = χι,j, for all i,j and thus XiW = Xi. Then we have when
l > 1,
t-2	t-2
Xtqι = (X1 - α X -M=-^QΛkQT)qι = -α X ^==^,
k=0	Ut-k-1	k=0	Ut-k-1
(21)
since Q is orthonormal and X1qι = χ1,1lN qι = x1,1 λ∕NqT qι = 0, for all l = 1 .
Combining (19) and (21), we have
D5 = XBi- X t I I 2 = X kXtqik2
i=1
i=2
N
X 02
i=2
≤ α2
t-2
X
k=0
1
Mt-k-1
V Ut-k-1
2
NdG∞
λkqi
(22)
N
N
2
1 - λ
1
€
where the last inequality follows from the fact that gt,i ≤ G∞, ∣∣qι ∣∣
us turn to D4 , it can be rewritten as
1, and ∣λι∣ ≤ λ < 1. Now let
2
I∣Zt- X t∣∣2
2= ( β1 Y 2
=Q-βJ α
≤ (1⅛ 了
1 mt-1,i
N ~1 √ut-1,i
α2dG∞ .
€
(23)
EI (X t -X t-1)
and D5 are in the order of O(α2) and thus D3 is in the order of
Now We know both D4
O(α2). Next we will bound D2 and D> Define G1 ，maxt∈[τ] maXi∈[N] ∣∣Vfi(xt,i)∣∣∞,
G2，maxt∈[τ] IlVf(Zt)k∞, G3，maxt∈[τ] maXi∈[N] ∣∣9t,i∣∣∞ and G∞ = max(G1,G2,G3).
Then we have
T 「/	1 N
D2 =E E	Vf(Zt),耳 NVfi(Xt,i)
T	N d
≤ X E ∣g∞ N XX
t=1
i=1 j=1
T
N
d
JL	IV <J>
XE g∞NXX
t=1
i=1 j=1
[Utj - [ut,i]j
T
X E G∞ N XXL	Z—	L
,	i=1 j=1 I [Utj V [ut,i]j + V [Ut]j [ut,i]j
T
N
d
t=1
T
N
d
JL	IV <J>
≤e X g∞ N XX
[Utj - [ut,i]j
t=1
i=1j=1
{z*
D6
,
(24)
2e1∙5
17
Under review as a conference paper at ICLR 2021
where the last inequality is due to [ut,i]j ≥ e, for all t, i,j. To simplify notations, define kAkabs =
Pi,j |Aij | to be the entry-wise L1 norm ofa matrix A, then we obtain
GG	工 1 一 E	GG 工 IkE 〜
D6 ≤	~∞^	X 2e∏ kUt1	- UtkabS ≤ ~∞^ X	2e∏ kUt1	- UtkabS
t=1	t=1
G2 T 1	1
=~N∞ X 2ei^J kUt N 1N1N - UtQQ kabs
t=1 e
G2 T 1 N
=~N X 2e∏ k - X UtqlqT kabs,
t=1	l=2
where the second inequality is due to Lemma A.1, introduced Section A, and the fact that Ut =
max(Ut, e) (element-wise max operator). Recall from update rule of Ut, by defining V-I，V⅞ and
一 人一	■	~	一一	„ ~
Uo，U1/2, we have for all t ≥ 0, Ut+
t
Ut = U0Wt + X(-E-ι-k
k=1
Then we further obtain when l 6= 1,
t
/ ~ ʌ	q▼、一 ▼一	- .
1 = (Ut — V⅞-ι + V⅞)W .Thus, we obtain
t
+ Vt-k)Wk = UO + y^(-Vt-1-k + Vt-k)QAkQT .
k=1
〜
Utql
(Uo+E(-v-ι-k
t
+ K-k)QAkQT)ql = X(-K-1-k + Vt-k)qlλk ,
k=1	k=1
where the last equality is due to the definition Uo，U“ = e1d1NN = √Ne1d1NN (recall that
qι = √= 1NN) and qTqj = 0 when i = j. Note that by definition of k ∙ kabs, we have for all
A, B, kA + Bkabs ≤ kAkabs + kBkabs, then
2T
D ≤ G∞ X
t=1
G2 T
1N
2^T5 k - EUtql qTkabs
1
N 21.5
t=1
2T
≤ —∞ X
≤ N乙
t=1
2T
≤ —∞ X
≤ N乙
t=1
2T
≤ —∞ X
≤ NJ
t=1
2T
=G∞ X
=n ⅛
G2∞	1
l=2
tN
k- X(-E-1-k + Vt-k) X qlλk qT kabs
k=1	l=2
td N
击 XX k X qlλk qTk1k(-Vt-1-k + Vt-k )T ejk1
k=1 j=1	l=2
Itd	N
2⅛ XX √N k Xqlλk qTk2k(-Vt-1-k + Vt-k)T ejk1
k=1 j=1	l=2
t d
2^U XX k(-Vt-1-k + Vt-k )T ej k1√Nλk
k=1 j=1
1	、t、	-	-
2ɪ1^J E k(-Vt-1-k + Vt-k) IIabsVN/
(25)
N 21.5
≤ G∞	1
一√N 2e1.5
k=1
T-1	T
XX
k(-Vo-1 + Vo)kabs √Nλt-0
o=0 t=o+1
T-1 λ
Σπ k(-K-1 + 匕)kabs ,
1-λ
o=0
where λ = max(∣λ2∣, |》n|). Combining (24) and (25), We have
G2	1 λ
D2 ≤	∞---------E
一√N 2e1∙5 1 - λ
T-1
X k(-K-1 + 工)kabs .
o=0
18
Under review as a conference paper at ICLR 2021
Now we need to bound Dι,we have
T 「/	1 N
DI =E E K ▽/(Zt), N Σmt-1,i Θ (
t=1 L ∖	i=1
1
√^t-1,i
T	INd
≤ x EI g∞ N xx
1
1
t=1
i=1 j = 1
T
N
d
T	INeI
XE g∞NXX
1
t=1
T
i=1 j = 1
N
d
(26)
TN
≤xE g∞Nxx
t=1
i=1 j = 1
^ɪɪɪ ([ut-1,i]j - [ut,i ]j )
T
N
d
(a) T	INdl
≤ X E g∞ N xx 2⅛
∖([ut-1,i]j - [ut,i]j)∣
t=1
i=1 j = 1
T
X IlUt-I- UtIlabS
_t=1	.
=g∞ 犷 N E
where (a) is due to [ut-ι,i]j = max([ut-ι,i]j, e) and the function max(∙, e) is 1-Lipschitz. In
addition, by update rule of Ut, we have
T
X IlUt-I- UtIlabS
t=1
T
x
kUt-1 -(Ut-1 - Vt-2 + Vt-I)WIIabS
t=1
T
^X IlUt-I(QQT - QAQT) + (-Vt-2 + Vt-I)W∣∣abs
t=1
TN
=X IlUt-I(X qι(1 - X)qT) + (-Vt-2 + Vt-I)W IlabS
T	t-1	N	T
≤ X Il X(-Vt-2-k + Vt-1-k) X qlλk(1 - λl)qTkabS + X ∣∣(-E-2 + E-I)WIlabS
t=1	k=1	l=2	t=1
T ∕t-1	、 T
≤X XI
-Vt-2-k + Vt-1-k ∣∣abS VnF ) + X ∣∣(-Vt-2 + Vt-I)IlabS
t=1 ∖k=1	)	t=1
T	∕t-1	、 T
=X X I - VO-2 + Vo-IkabS √Nλt-o	+ X I(-Vt-2 + Vi-I)IabS
t=1 ∖o=1	)	t=1
T-1 T	T
=X X (I-VL-2 + VO-1∣∣abS√Nλt θ) + SXJ ∣∣(-Vt-2 + Vt-I)IlabS
o=1t=o+1	t=1
T-1	X	T
≤ ^X 1---λ (i - VL-2 + 工-1 IIabS√N) + ^X ∣∣(-Vt-2 + Vt-I)IlabS
o=1	t=1
1	t ʌ ʌ
≤ 1 - λ E I(-Vt-2 + Vt-I)IIabS √N ∙
t=1
(27)
19
Under review as a conference paper at ICLR 2021
Combining (26) and (27), we have
11
DI ≤ G∞ / NE
T	一
X k(-Vt-2 + Vt-I)kabs√N
t=1
(28)
1
1 - λ
What remains is to bound PtT=1 E kZt+1 - Ztk2. By update rule of Zt, we have
kZt+1 - Zt k2
1
1
—
√ut-i,i
≤2α2
≤2α2
≤2α2
1X 4『
N i=ι E
βι	1 X Cf
ɑ厂友NTmtTi Q (
i=1
2
1 X gt,i Il
N=√M
2
β 2	1N d 1
≤2α ( 1 - β ) G∞ N XX 2∑2 l[ut,ij — [ut-1,i]j | + 2α
1 - β1	N i=1 j=1 2
=2α2 (T-^)2 G∞CkUt - Ut-Ikabs + 2α2
ɪ X匹
N ⅛ √ut
(29)
where the last inequality is again due to the definition that [Ut,i j = max([ut,ij, e) and the fact that
max(∙, e) is I-LiPSchitz. Then, We have
T
X E[kZt+1-Ztk2]
t=1
2	T	T I N	I2
≤2α2 (Γ⅛) g∞ N 委E X kUt-UtTT +2α2 X E	N X √⅛[]
≤α2()2 G∞ ɪɪ E "X k(-Vt-2 + Vt-I)kabs#	+2α2	X	E [ ɪ X	4]]
≤	<1-βJ √Ne2 1 - λ 专 k( t 2 + t 1)kab[	+	台[N 台	√UtJ∣∣ J
where the last inequality is due to (27).
We now bound the last term on RHS of the above inequality. A trivial bound can be
T
X
t=1
T1
XdG∞1,
t=1	e
due to kgt,i k ≤ G∞ and [ut,i]j ≥ e, for all j (verified from uPdate rule of ut,i and the assumPtion
that [vt,i]j ≥ e, for all i). However, the above bound is indePendent of N, to get a better bound, we
20
Under review as a conference paper at ICLR 2021
need a more involved analysis to show its dependency on N . To do this, we first notice that
EGt |G1:t-1
1X gt,i II
N ⅛ E
+ EGt |G1:t-1
=E":" N X X / Vfi(Xt,i) + ξt,if Xtj) + ξt,j
i=1 j=1
(=a)EGt|G1:t-1
1 X yfi(xt,i) II
N i⅛ Fnl
(b)
1 Nfi(Xt,i)
N =	√utj
2
+ N2
N d	EGt|G1:t-1 [[ξt,i]l2]
i=1 l=1
[ut,i]l
(c)
≤
1 ,X Nfi(Xt,i)
N =	√utj
2
d σ2
+-----
+ N E
where (a) is due to EGt|G1:t-1 [ξt,i] = 0 and ξt,i is independent of Xt,j, ut,j for all j, and ξj, for all
j = i, (b) comes from the fact that xt,i, ut,i are fixed given Gi：t, (c) is due to EGt|G1:t_1 [[ξt,i]2 ≤ σ2
and [ut.i]l ≥ E by definition. Then we have
=EG1:t-1
EGt|G1:t-1
N X √⅛ I
≤EG”1
1	Vfi(Xt,i)
N =	√uti
2
d σ2
+-----
+ N E
=e	ɪX VfqIl	+ dσ2
N i=i √uti i I N E
(30)
In traditional analysis of SGD-like distributed algorithms, the term corresponding to
E IN PN=I v√Xt,i) I ] will be merged with the first order descent when the stepsize is cho-
sen to be small enough. However, in our case, the term cannot be merged because it is different from
the first order descent in our algorithm. A brute-force upper bound is possible but this will lead to a
worse convergence rate in terms of N. Thus, we need a more detailed analysis for the term in the
following.
E
=E
≤2E
≤2E
1 1X Vfi(Xt,i) I
N ⅛ k∏
1N
N X
i=1
Vfi(Xt,i)
1N
N X
i=1
1N
N X
i=1
+
Vfi(Xt,i)
Vfi(Xt,i)
2
2
E
ɪ X q ll2
N ⅛ E
21
Under review as a conference paper at ICLR 2021
KX ≡) 口
≤2E
+ 2E
Summing over T , we have
T
E
t=1
Vfi(Xt,i)『
T
≤2X E
t=1
Vfi(Xt,i)/]
put	U
T
+2X E
t=1
(31)
For the last term on RHS of (31), we can bound it similarly as what we did for D2 from (24) to (25),
which yields
abs
ut,i - U t∣L
T
≤X E
t=1
l=2
T
Utql ql kabs
T-1
≤√1NG∞WE E
o=0
λ
^IK-Vo-1 + Vo)Ilabs
1- λ
(32)
Further, we have
T
E
t=1
1N
X
i=1
Vfi(χt,i)∣∣1 21
∣∣]
T
≤2XE
t=1
T
=2XE
t=1
W X ft ∣∣1
T
+2XE
t=1
l^∣∣ 1 ∖~y Vfi(Xt) - Vfi(Xt,j)
[∣∣n N	wt
]∣∣ f ∣2j+2 X E ]∣∣ ： X fpf^ ∣∣]
and the last term on RHS of the above inequality can be bounded following similar procedures from
(17) to (22), as what we did for D3 . Completing the procedures yields
T
E
1	Vfi (Xt) - Vfi (xt,i)
N = 范
t=1
TN
≤XE LWX∣χt,i -Xt∣2
t=1	i=1
T
≤X
t=1
E 卜INα (占)NdG∞I
(33)
=TL ɪ α2
I2
22
Under review as a conference paper at ICLR 2021
Finally, combining (30) to (33), we get
HNX 品∏ ≤4 X e(I f ∣2j+4TL⅛α2 (乙)/
+2 √ng∞ AE ]χ 占 k(-ν°τ+6)"abs + TNστ
≤4 4√ XE [∣ fχ)∣〕+4T&α2 (占 M
1	1	「匕 1 ∖	1	∙
+ 2√Ng∞存E X Γ-λ"(-v°t + 匕)kabs + tn~.
Y	Lo=O	_
where the last inequality is due to each element of Ut is lower bounded by e by definition.
Combining all above, we obtain
TX E [∣*∣1
≤ TO (Ef(Z1)]- 即(ZT+1)])
J ( βι、
+ t"-1)
8L	1 Gm
+ 亍α √eX E
t=1
2 G∞	1	1
√N e2 1 — λ
_| 一 ∣∣2^l
∣ V/(X t) ∣∣
E [Vt ]
U t/4
+ 8L2a-O2 (1 _ λ) dG∞
(34)
+ 4Lα√Ng∞WE X 占k(—VoT+%)"abs +2L0"στ
Y	Lo=O	_
+ 2G2 J__LE「ɪ J
+ T 1 ― βιG∞ 2e1∙5 √NE [1 — λ VT
+ 2G∞ J__LE [v ]
+ T √N 2e1∙5 1 — λEIVT]
3
+ T
l (τ⅛) (OdG e⅛+X l ( t—⅛) 02
Ta (E[/(。1)]—e[/(ZT+ι)])+2LaNd σ-+8La √eT XE ]∣∣VfIXt) ∣∣ j
+ 302d ((T—⅛)	+ (T—7)) l⅛ + 8(3L2 (T—λ) d⅝
+ t⅛ √N t—7 卜( (E;)	e⅛+λ+t—⅛+2Lae⅛λ) e[VT ] ∙
where VT ：= PT=I k(一玄-2 + 玄-ι)∣∣αbs∙ Set α = √= and when α ≤ 蔡,we further have
1T
T X E
t=1
V/(X t)
[∣∣ ▼
4	d σ2
≤TO(E[/(Z1)] 一 E[/(ZT+1)]) + 4LaNT
23
Under review as a conference paper at ICLR 2021
+6α2d	h-⅛
LG∞ + 16α3L2
1.5
+ T2.5 √GN T-I (La (τ-⅛)+ + λ+τ⅛ + 2Lα⅛5λ) e [Vt]
4	d σ2
≤Ta(Ef(Z1)1 - min〃x)) + 4LaNT
+ 6α2d ((1-⅛) +
LG∞ + 16α3dL2 (占)GF
+ T15√Nτ-λ (La (τ-⅛)++λ+t⅛+2Laj5λ)E[VT]
≤C1	(Ef(Z1)] - min f (X))+adN)+C2a2d+C3a3d+T⅛ (C4+C5a)E [Vt 1
(35)
where the first inequality is obtained by moving the term 8Lα 表 T PT=I E
fXt)H onthe
RHS of (34) to the LHS to cancel it using the assumption 8Lɑ √1∣ ≤ 2 followed by multiplying both
sides by 2. The constants introduced in the last step are defined as following
C5= 1 占 L ( T⅛ )2 G∞ + :占 LG' .
Substituting into Zi = X1 completes the proof.
C Proof of Theorem 3
Under some assumptions stated in Corollary 2.1, we have that
1T
TX E
t=1
fX)U ≤C1 √√N ((Ef(ZI)]- min f (x))+σ2)+C2 N+C3 τ⅛o5

+ (C4T√N + C5 T⅛) E
T
X k(-Vt-2 + Vt-I)Ilabs
t=1
(36)
where k ∙ kabs denotes the entry-wise L1 norm of a matrix (i.e IlAkabs = i,j |Aij|) and
C1 , C2 , C3 , C4 , C5 are defined in Theorem 2.
Since Algorithm 3 is a special case of 2, building on result of Theorem 2, we just need to characterize
the growth speed of E [PT=1 k(-V⅛-2 + Vt-1)∣∣abs] to prove convergence of Algorithm 3. By the
24
Under review as a conference paper at ICLR 2021
update rule of Algorithm 3, We know E is non decreasing and thus
T
X k(-K-2 + K-I)kabs
t=1
=E
TNd
XXX | - [vt-2,i]j + [vt-1,i]j |
t=1 i=1 j=1
=E
TNd
XXX(-[vt-2,i]j + [vt-1,i]j )
t=1 i=1 j=1
=E
Nd
XX(TvT,ij+ [vT-1,i]j )
i=1 j=1
=E
Nd
XX(Tv0,ij + [vT-1,i]j )	,
i=1 j=1
where the last equality is because we defined V-1，V⅞ previously.
Further, because kgt,ik∞ ≤ G∞ for all t, i and vt,i is a exponential moving average of gk2,i, k =
1,2,…，t, we know | [vt,i]j | ≤ G∞, for all t, i, j. In addition, by update rule of V we also know
each element of V also cannot be greater than G∞, i.e. | [vt,i]j | ≤ G∞, for all t, i, j. Given the fact
that [vo,i j ≥ 0 , we have
T
X k(-E-2 + E-I)kabs
t=1
Nd
XX(Tv0,i]j + [vτ-1,i]j )
i=1 j=1
≤E
Nd
XXG2∞
i=1 j=1
NdG2∞ .
E
E
E
Substituting the above into (39), we have
1T
TXE
t=1
fX^U ≤C1 √√dN ((Ef(Z1)]- min f (x)) + σ2) + C2 T + C3 TNd.5
+ C C4 —1= + C5 —ɪ- ) NdGL
+	4 T √N +	5 T 1∙5d0∙5	∞
, √d	N N N 1∙5
=C1 AFXF ((Ef (Z1)] - min f (x)) + σ + + C2 斤 + C3τ1 5 10 5
TN	x	T	T 1.5 d0.5
+ Co √Nd + Co Ndi
+ C4 T + C5 T 1.5 ,
(37)
where we have
0	0	0	0	20	2
C1 = C1	C2	= C2	C3	= C3	C4	=	C4G∞	C5	= C5 G∞ .
and we conclude the proof.
(38)
D Proof of Theorem 4
The proof follows the same flow as that of Theorem 3. Under assumptions stated in Corollary 2.1, set
α = √N/√Td, we have that
TXEIfχ^∣口 ≤C1 √√N ((Ef(Z1)] -minf(x))+σ2)+C2N+C3TNd^
+ (C4 T√N + C5 T⅛) E
T
X k(-Vt-2 + E-1)kabs
t=1
(39)
25
Under review as a conference paper at ICLR 2021
where k ∙ kabs denotes the entry-wise Li norm of a matrix (i.e IlAkabs = i,j |Aij|) and
C1, C2, C3, C4, C5 are defined in Theorem 2.
Again, Since decentralized AdaGrad is a special case of 2, we can apply Corollary 2.1 and what we
need is to upper bound E [pT=ι ∣∣(-^Vt-2 + V⅛-ι)∣∣abs] derive convergence rate. By the update rule
of decentralized AdaGrad, we have vt,i = 1 (Pk=i g2 i) for t ≥ 1 and Vo,i = el. Then we have for
t ≥3,	,
T
E X IK-Vt-2 + Vt-I)Ilabs
t=1
-TNd	'
=E XXXI - [Vt-2,i]j + [Vt-i,i]j|
t=1 i=1 j =1
+ N d(G2∞ - e)
T N d	t-2
≤e XXX|(t-i —1-2)([Xgk,i]j) + t-ɪ[g2-ι,i]j)|	+ NdG∞
t=3 i=1 j =1	k=1
T N d	t-2
=E XXX 1(-(t- 11(t-2))([Xg2,i]j) + -g-i,i]jI +NdG∞
t=3 i=1 j=1	(t - )(t - ) k=1	t -
≤E
TNd
XXX max
t=3 i=1 j =1
+ N dG2∞
≤E Nd X tG∞11 + NdG∞
≤NdG2∞log(T)+NdG2∞
=NdG2∞(log(T) + 1)
where the first equality is because we defined V-i，V⅞ previously and ∣gk,ik∞ ≤ G∞ by assump-
tion.
Substituting the above into (39), we have
1T
TXE
t=1
fX^U ≤C1 √√TN ((Ef(Zi)] - mxin f (x)) + σ2) + C2 N + C3 TNdT
+ (C4 t√n + C5 T 1.(0.5) NdG∞ (Iog(T) +1)
ʌ/d	N	N 1.5
=C1 √TN ((Ef(Z1)] - mχinf(x)) + σ2) + C2T + C3T 1.5d0.5
+ C40
d√N (Iog(T ) + 1)
+ C5 (Iog(T ) + 1)N√d
T 1.5
T
where we have
C10 = C1	C20	= C2	C30	= C3	C40	= C4G2∞	C50	=	C5G2∞ .	(40)
and we conclude the proof.

26
Under review as a conference paper at ICLR 2021
E Additional Experiments and Details
In this section, we compare the training loss and testing accuracy of different algorithms, namely
Decentralized Stochastic Gradient Descent (DGD), Decentralized Adam (DADAM) and our proposed
Decentralized AMSGrad, with different stepsizes on heterogeneous data distribution. We use 5 nodes
and the heterogeneous data distribution is created by assigning each node with data of only two labels.
Note that there are no overlapping labels between different nodes. For all algorithms, we compare
stepsizes in the grid [10-1, 10-2,10-3, 10-4,10-5, 10-6].
Figure 2 shows the training loss and test accuracy for DGD algorithm. We observe that the stepsize
10-3 works best for DGD in terms of test accuracy and 10-1 works best in terms of training loss.
This difference is caused by the inconsistency among the value of parameters on different nodes when
the stepsize is large. The training loss is calculated as the average of the loss value of different local
models evaluated on their local training batch. Thus, while the training loss is small at a particular
node, the test accuracy will be low when evaluating data with labels not seen by the node (recall that
each node contains data with different labels since we are in the heterogeneous setting).
Figure 3 shows the performance of decentralized AMSGrad with different stepsizes. We see that
its best performance is better than the one of DGD and the performance is more stable (the test
performance is less sensitive to stepsize tuning).
Figure 4 displays the performance of Decentralized Adam algorithm. As expected, the performance of
DADAM is not as good as DGD or decentralized AMSGrad. Its divergence characteristic, highlighted
Section 2.3, coupled with the heterogeneity in the data amplify its non-convergence issue in our
experiments. From the experiments above, we can see the advantages of decentralized AMSGrad
in terms of both performance and ease of parameter tuning, and the importance of ensuring the
theoretical convergence of any newly proposed methods in the presented setting.
27
Under review as a conference paper at ICLR 2021
0
0
2.5
123456789	10
number of iterations	X ι04
1
0.9
AOEJnOOE ~s2
0.1 i-1-1-1-1-1-1-1-1-1-
012345678g	10
number of iterations	χ 104
(a) Training loss	(b) Test accuracy
Figure 2: Performance comparison of different stepsizes for DGD
Figure 3: Performance comparison of different stepsizes for decentralized AMSGrad
(b) Test accuracy
(a) Training loss
Figure 4: Performance comparison of different stepsizes for DADAM
(b) Test accuracy
28