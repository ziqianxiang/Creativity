Under review as a conference paper at ICLR 2021
Sufficient and Disentangled Representation
Learning
Anonymous authors
Paper under double-blind review
Ab stract
We propose a novel representation learning approach called sufficient and disen-
tangled representation learning (SDRL). With SDRL, we seek a data representa-
tion that maps the input data to a lower-dimensional space with two properties:
sufficiency and disentanglement. First, the representation is sufficient in the sense
that the original input data is conditionally independent of the response or label
given the representation. Second, the representation is maximally disentangled
with mutually independent components and is rotation invariant in distribution.
We show that such a representation always exists under mild conditions on the
input data distribution based on optimal transport theory. We formulate an objec-
tive function characterizing conditional independence and disentanglement. This
objective function is then used to train a sufficient and disentangled representa-
tion with deep neural networks. We provide strong statistical guarantees for the
learned representation by establishing an upper bound on the excess error of the
objective function and show that it reaches the nonparametric minimax rate under
mild conditions. We also validate the proposed method via numerical experiments
and real data analysis.
1	Introduction
Representation learning is a fundamental problem in machine learning and artificial intelligence
(Bengio et al., 2013). Certain deep neural networks are capable of learning effective data represen-
tation automatically and achieve impressive prediction results. For example, convolutional neural
networks, which can encode the basic characteristics of visual observations directly into the net-
work architecture, is able to learn effective representations of image data (LeCun et al., 1989). Such
representations in turn can be subsequently used for constructing classifiers with outstanding perfor-
mance. Convolutional neural networks learn data representation with a simple structure that captures
the essential information through the convolution operator. However, in other application domain-
s, optimizing the standard cross-entropy and least squares loss functions do not guarantee that the
learned representations enjoy any desired properties (Alain & Bengio, 2016). Therefore, it is im-
perative to develop general principles and approaches for constructing effective representations for
supervised learning.
There is a growing literature on representation learning in the context deep neural network modeling.
Several authors studied the internal mechanism of supervised deep learning from the perspective of
information theory (Tishby & Zaslavsky, 2015; Shwartz-Ziv & Tishby, 2017; Saxe et al., 2019),
where they showed that training a deep neural network that optimizes the information bottleneck
(Tishby et al., 2000) is a trade-off between the representation and prediction at each layer. To
make the information bottleneck idea more practical, deep variational approximation of information
bottleneck (VIB) is considered in Alemi et al. (2016). Information theoretic objectives describ-
ing conditional independence such as mutual information are utilized as loss functions to train a
representation-learning function, i.e., an encoder in the unsupervised setting (Hjelm et al., 2018;
Oord et al., 2018; Tschannen et al., 2019; Locatello et al., 2019; Srinivas et al., 2020). There are
several interesting extensions of variational autoencoder (VAE) (Kingma & Welling, 2013) in the
form of VAE plus a regularizer, including beta-VAE (Higgins et al., 2017), Annealed-VAE (Burgess
et al., 2018), factor-VAE (Kim & Mnih, 2018), beta-TC-VAE (Chen et al., 2018), DIP-VAE (Kumar
et al., 2018). The idea of using a latent variable model has also been used in adversarial auto-
1
Under review as a conference paper at ICLR 2021
encoders (AAE) (Makhzani et al., 2016) and Wasserstein auto-encoders (WAE) (Tolstikhin et al.,
2018). However, these existing works focus on the unsupervised representation learning.
A challenge of supervised representation learning that distinguishes it from standard supervised
learning is the difficulty in formulating a clear and simple objective function. In classification,
the objective is clear, which is to minimize the number of misclassifications; in regression, a least
squares criterion for model fitting error is usually used. In representation learning, the objective is
different from the ultimate objective, which is typically learning a classifier or a regression function
for prediction. How to establish a simple criterion for supervised presentation learning has remained
an open question (Bengio et al., 2013).
We propose a sufficient and disentangled representation learning (SDRL) approach in the context
of supervised learning. With SDRL, we seek a data representation with two characteristics: suffi-
ciency and disentanglement. In the context of representation learning, sufficient means that a good
representation should preserve all the information in the data about the supervised learning task.
This is a basic requirement and a long-standing principle in statistics. This is closely related to
the fundamental concept of sufficient statistics in parametric statistical models (Fisher, 1922). A
sufficient representation can be naturally characterized by the conditional independence principle,
which stipulates that, given the representation, the original input data does not contain any additional
information about the response variable.
In addition to the basic sufficiency property, the representation should have a simple statistical struc-
ture. Disentangling is based on the general notion that some latent causes underlie data generation
process: although the observed data are typically high-dimensional, complex and noisy, the un-
derlying factors are low-dimensional, independent and have a relatively simple statistical structure.
There is a range of definitions of disentangling (Higgins et al., 2018; Eastwood & Williams, 2018;
Ridgeway & Mozer, 2018; Do & Tran, 2020). Several metrics have been proposed for the evaluation
of disentangling. However, none of these definitions and metrics have been turned into empirical
criterions and algorithms for learning disentangled representations. We adopt a simple definition of
disentangling which defines a representation to be disentangled if its components are independent
(Achille & Soatto, 2018). This definition requires the representation to be maximally disentangled
in the sense that the total correlation is zero, where the total correlation is defined as the KL di-
vergence between the joint distribution of g(x) and the product of the marginal distributions of its
components (Watanabe, 1960).
In the rest of the paper, we first discuss the motivation and the theoretical framework for learning
a sufficient and disentangled representation map (SDRM). This framework leads to the formulation
of an objective function based on the conditional independence principle and a metric for disentan-
glement and invariance adopted in this work. We estimate the target SDRM based on the sample
version of the objective function using deep neural networks and develop an efficient algorithm for
training the SDRM. We establish an upper error bound on the measure of conditional independence
and disentanglement and show that it reaches the nonparametric minimax rate under mild regularity
conditions. This result provides strong statistical guarantees for the proposed method. We validate
the proposed SDRL via numerical experiments and real data examples.
2	Sufficient and disentangled representation
Consider a pair of random vectors (x, y) ∈ Rp × Rq, where x is a vector of input variables and y is a
vector of response variables or labels. Our goal is to find a sufficient and disentangled representation
of x.
Sufficiency We say that a measurable map g : Rp → Rd with d ≤ p is a sufficient representation
of x if
ylx∣g(x),	(1)
that is, y and x are conditionally independent given g(x). This condition holds if and only if the
conditional distribution of y given x and that of y given g(x) are equal. Therefore, the information
in x about y is completely encoded by g(x). Such ag always exists, since ifwe simply take g(x) =
x, then (1) holds trivially. This formulation is a nonparametric generalization of the basic condition
in sufficient dimension reduction (Li, 1991; Cook, 1998), where it is assumed g(x) = BTx with
B ∈ Rp×d belonging to the Stiefel manifold, i.e., BTB = Id.
2
Under review as a conference paper at ICLR 2021
Denote the class of sufficient representations satisfying (1) by
F = {g : Rp → Rd,g measurable and satisfies y ɪ x∣g(x)}.
We refer to F as a Fisher class because of its close connection with the concept of sufficient statistics
(Fisher, 1922; Cook, 2007). For an injective measurable transformation T : Rd → Rd and g ∈ F,
T ◦ g(x) is also sufficient by the basic property of conditional probability. Therefore, the Fisher
class F is invariant in the sense that
T ◦ F = F, provided T is injective,
where T ◦ F = {T ◦ g : g ∈ F }. An important class of transformations is the class of affine
transformations, T ◦ g = Ag + b, where A is a d × d nonsingular matrix and b ∈ Rd .
Disentanglement We focus on the disentangled representations among those that are sufficient.
Therefore, we start from the functions of the input data that are sufficient representations in the
Fisher class F. For any sufficient and disentangled representation g(x), let Σg = Var(g(x)). Since
the components ofg(x) are disentangled in the sense that they are independent, Σg is a diagonal ma-
trix, thus Σ-1∕2g(x) also has independent components. Therefore, We can always rescale g(x) such
that it has identity covariance matrix. To further simplify the statistical structure of a representation
g, we also require it to be rotation invariant in distribution, that is, Qg(x) = g(x) in distribution
for any orthogonal matrix Q ∈ Rd×d . The Fisher class F is rotation invariant in terms of condi-
tional independence, but not all its members are rotation invariant in distribution. By the Maxwell
characterization of the Gaussian distributions (Maxwell, 1860; Bartlett, 1934; Bryc, 1995; Gyenis,
2017), a random vector of dimension two or more with independent components is rotation invari-
ant in distribution if and only if it is Gaussian with zero mean and a spherical covariance matrix.
Therefore, after absorbing the scaling factor, for a sufficient representation map to be disentangled
and rotation invariant, it is necessarily distributed as Nd(0, Id). Let M be the Maxwell class of
functions g : Rd → Rd, where g(x) is disentangled and rotation invariant in distribution. By the
Maxwell characterization, we can write
M = {g ： Rp → Rd, g(x)〜N(0,Id)}.	⑵
Now our problem becomes that of finding a representation in F ∩ M, the intersection of the Fisher
class and the Maxwell class.
The first question to ask is whether such a representation exists. The following result from optimal
transport theory provides an affirmative answer and guarantees that F ∩ M is nonempty under mild
conditions (Brenier, 1991; McCann, 1995; Villani, 2008).
Lemma 2.1. Let μ be a probability measure on Rd. Suppose it has finite second moment and is
absolutely continuous with respect to the standard Gaussian measure, denoted by γd. Then it admits
a unique optimal transportation map T : Rd → Rd such that T#m = Yd ≡ N(0, Id), where T#m
denotes the pushforward distribution of μ under T. Moreover, T is injective μ-almost everywhere.
Denote the law of a random vector Z by μz. Lemma 2.1 implies that, for any g ∈ F with
Ekg(x)k2 < ∞ and μg(χ) absolutely continuous with respect to γd, there exists a map T* trans-
forming the distribution of g(x) to N(0, Id). Therefore, R* := T* ◦ g ∈ F ∩ M, that is,
xUy∣R*(x) and R*(x)〜N(0,Id),	(3)
i.e., R* is a sufficient and disentangled representation map (SDRM).
3 Objective function for SDRL
The above discussions lay the theoretical foundation for formulating an objective function that can
be used for constructing a SDRM R* satisfying (3), or equivalently, R* ∈ F ∩ M.
Let V be a measure of dependence between random variables x and y with the following properties:
(a) V[x, y] ≥ 0 with V[x, y] = 0 if and only if X 卫 y; (b) V[x, y] ≥ V[R(x), y] for all measurable
function R; (c) V[x, y] = V[R*(x), y] if and only ifR* ∈ F. The properties (a)-(c) imply that
R* ∈ F ⇔ R* ∈ arg max V [R(x), y] = arg min{-V [R(x), y]}.
RR
3
Under review as a conference paper at ICLR 2021
We use a divergence measure D to quantify the difference between μR(χ) and γd, as long as this mea-
sure satisfies the condition D(μR(χ)kγd) ≥ 0 for all measurable function R and D(μκ(χ)∣∣γd)=
0 if and only if R ∈ M.
Then the problem of finding an R* ∈ F ∩ M can be expressed as a constrained minimization
problem:
arg min-V[R(x), y] subject to D(μR(χ)kγd) = 0.
R
Its Lagrangian form is
L(R) = -V [R(x), y] + λD(μR(χ)∣∣γd),	(4)
where λ ≥ 0 is a tuning parameter. This parameter provides a balance between the sufficiency
property and the disentanglement constraint. A small λ leads to a representation with more emphasis
on sufficiency, while a large λ yields a representation with more emphasis on disentanglement. We
show in Lemma 4.1 below that any R* satisfying (3) is a minimizer of L(R). Therefore, We can
train a SDRM by minimizing the empirical version of L(R).
There are several options for V with the properties (a)-(c) described above. For example, we can take
V to be the mutual information V[R(x), y] = I(R(x); y). However, in addition to the estimation
of the SDRM R, this choice requires the estimation of the density ratio between p(y, R(x)) and
p(y)p(R(x)), which is not an easy task. We can also use the conditional covariance operators
on reproducing kernel Hilbert spaces (Fukumizu et al., 2009). To be specific, in this work we
use the distance covariance (Szekely et al., 2007) of y and R(x), which has an elegant U-statistic
expression, does not involve additional unknown quantities and is easy to compute. For the divergnce
measure of two distributions, we use the f -divergence (Ali & Silvey, 1966), which includes the KL-
divergence as a special case.
4 Learning sufficient and disentangled representation
We first describe some essentials about distance covariance and f -divergence.
Distance covariance We first recall the concept of distance covariance (Szekely et al., 2007), which
characterizes the dependence of two random variables.
Let i be the imaginary unit (-1)1/2. For any t ∈ Rd and s ∈ Rm, let ψz(t) = E[expitT z], ψy (s) =
E[expisT y], and ψz,y(t, s) = E[expi(tT z+sT y)] be the characteristic functions of random vectors
z ∈ Rd, y ∈ Rq, and the pair (z, y), respectively. The squared distance covariance V[z, y] is defined
as
V[z, y] =
Rd+m
@z,y(t, S) - ψz(t)ψy (S)|2
CdCmktkd+1kskq+1
dtdS, where cd
∏(d+1)∕2
Γ((d +1)/2).
Given n i.i.d copies {zi, yi}in=1 of (z, y), an unbiased estimator of V is the empirical distance
covariance Vn, which can be elegantly expressed as a U-statistic (Huo & Szekely, 2016)
1
Vn[z, y] = C4	工	h ((ZiI, yiι) ,…，(Zi4, yi4)),	⑸
n 1≤i1<i2<i3<i4≤n
where h is the kernel defined by
h((zι,yι),...,(z4,y4)) = 1P 1≤i.j≤4 ∣∣zi -Zjkkyi -yjk
i6=j
- 1 Pi=I(P 1≤j≤4 kzi - Nj k P 1≤j≤4 kyi - yj ∣∣) + 244 P ι≤ij≤4 kzi - Njk P 1≤i,j≤4 Ilyi - yj k.
j6=i	i6=j	i6=j	i6=j
f-divergence Let μ and Y be two probability measures on Rd. The f-divergence (Ali & Silvey,
1966) between μ and Y with μ《Y is defined as Df (μ∣γ) = RRd f (ddμ)dγ, where f : R+ → R
is a differentiable convex function satisfying f(1) = 0. Let f* be the Fenchel conjugate of f
(Rockafellar, 1970), defined as f* (t) = supx∈R{tx - f (x)}, t ∈ R. The f -divergence admits the
following variational formulation (Keziou, 2003; Nguyen et al., 2010; Nowozin et al., 2016).
Lemma 4.1.
Df(μ∣Y) =	max	Ez~μD(z)] - Ew~γ[f *(D(w))],	(6)
DiRd→dom(f*)
where the maximum is attained at D(Z) = f 0(dμ(z)).
4
Under review as a conference paper at ICLR 2021
Commonly used divergence measures include the Kullback-Leibler (KL) divergence, the Jensen-
Shanon (JS) divergence and the χ2 -divergence.
Learning SDRM We are now ready to formulate an empirical objective function for learning SDR-
M R*. Let R ∈ M, where M is the Maxwell class defined in (2). By the variational formulation
(6), we can write the population version of the objective function (4) as
L(R) = -V[R(x),y] + λ max	{Eχ〜μχ[D(R(x))] - EW〜)壮[/"(D(w))]}.	(7)
D:Rd→dom(f* )
This expression is convenient since we can simply replace the expectations by the corresponding
empirical averages.
Theorem 4.2. We have R* ∈ argminR∈M L(R) provided (3) holds.
According to Theorem 4.2, it is natural to estimate R* based on the empirical version of the objective
function (7) when a random sample {(xi, yi)}in=1 is available. We estimate R* using deep neural
networks. We employ two networks as follows:
• Representer network Rθ: This network is used for training R*. Let R be the set of such
neural networks Rθ : Rp → Rd .
• Discriminator network Dφ : This network is used as the witness function for checking
whether the distribution of the estimator of R* is approximately the same as N(0, Id).
Similarly, denote D as the set of such neural networks Dφ : Rd → R.
Let {wi}in=1 be n i.i.d random vectors drawn from γd. The estimated SDRM is defined by
τ-i —	♦	*/ ι > ∖
Rθ ∈ arg min L(Rθ)
Rθ∈R
r 一 ，	、 r . ^	,	..	、
-Vn[Rθ(x), y] + λD f (μRθ (χ)kγd),
(8)
where Vn[Rθ(x), y] is an unbiased and consistent estimator of V[Rθ(x), y] as defined in (5) based
on {(Rθ(xi),yi),i = 1, . . . , n} and
1n
Df (μRθ(χ)kγd) = DmgD n EDφ(rθ(Xi))- f *(dφ(Wi))].
φ	i=1
(9)
Statistical guarantee Since a SDRM R* is only identifiable up to orthogonal transforms under the
constraint that R*(x)〜N(0, Id), no consistency results for R itself can be obtained. But this is
not a flaw of the proposed method. Indeed, the most important statistical guarantee of the learned
R* is that the objective of conditional independence and disentanglement is achieved. Therefore,
we establish an upper bound on the excess risk L(Rθ) - L(R*) of the deep nonparametric estimator
Rbθ in (8). We make the following assumptions.
(A1) For any ε > 0, there is a constant Bi > 0 such that μχ([-Bι, Bι]p) > 1 - ε, and R* is
Lipschitz continuous on [-B1, B1]p with Lipschitz constant L1.
(A2) For R ∈ M, We assume r(z) = dμRχ) (Z) is Lipschitz continuous on [-B1,B1]p with
Lipschitz constant L2, and 0 < c1 ≤ r(z) ≤ c2.
DenOte B2 = max{lf 0(ci)1, |f 0 (c2)|}, B3 = max∣s∣≤2L2√d	n+B2 |f* (S)|.
The specifications of the network parameters, including depth, width, size and the supremum norm
over the domains of the representer Rθ and the discriminator Dφ are given in Appendix B.
Theorem 4.3. Suppose λ > 0 and λ = O(1). Suppose conditions (A1)-(A2) hold and set the
network parameters according to (i)-(ii). Then
E{Xi,yi,Wi}i=1 [L(Rθ) - L(R*)] ≤ C ((Li + L2)pdpn-2+p + L2√d(log n)n-2+d),
where C is a constant that depends on B1 , B2 and B3 but not on n, q, p and d.
The proof of this theorem is given in Appendix B. The result established in Theorem 4.3 provides
strong statistical guarantees for the proposed method. The rate n-2/(2+p) matches the minimax non-
parametric estimation rate for Lipschitz class contained in Rp of functions (Stone, 1982; Tsybakov,
2008). Up to a logn factor, the rate (log n)n-2/(2+d) matches the minimax rate of nonparametric
estimation of Lipschitz densities via GANs (Singh et al., 2018; Liang, 2018).
5
Under review as a conference paper at ICLR 2021
5 Computation
We can update θ and φ alternately as in training GANs (Goodfellow et al., 2014). However, this ap-
proach suffers from the instability issues. In our implementation, we utilize the more stable particle
method based on gradient flow (Gao et al., 2019; 2020). The key idea is to find a sequence of nonlin-
ear but simpler residual maps, say T(Z) = Z + sv(z), pushing the samples from mr@3)to the target
distribution Yd along a velocity fields V(Z) = -Vf0(r(z)) that most decreases the f-divergence
Df (∙∣∣Yd) at μRθ(χ). The residual maps can be estimated via deep density-ratio estimators, which
take the form T(Z) = Z + sb(z), Z ∈ Rd, where S is a step size and V(Z) = —f 00(r(z))Vr(Z). Here
r(z) is an estimated density ratio of the density of Rθ (x) at the current value of θ over the density of
the reference distribution. We use T to transform zi = Rθ(xi), i = 1, . . . , n into Gaussian samples.
Once this is done, we update θ via minimizing the loss -Vbn[Rθ(x), y]+λ Pin=1 kRθ(xi) -zik2/n.
We describe the algorithm below.
•	Input {xi, yi}n=ι. Tuning parameters: s, λ, d. Sample {wi}n=ι 〜 γd.
•	Outer loop for θ
-Inner loop (particle method)
*	Let Zi = Rθ(xi), i = 1, 2,..., n.
*	Solve Dbφ ∈ argminQφ Pn=1 1 (log(1 + expDφ(Zi)) + log(1 +exp-Dφ(Wi))).
*	Define the residual map T(Z) = Z — sf (r(Z))Vr(Z) with r(Z) = exp-Dφ(z).
*	Update the particles zi = T(zi), i = 1, 2, ..., n.
-	End inner loop
-	Update θ via minimizing —Vbn [Rθ(x), y] + λ Pin=1 kRθ(xi) — zik2/n using SGD.
• End outer loop
6 Experiments
We evaluate the proposed SDRL with the KL-divergence using both simulated and real data. The
goal of our experiments is to demonstrate that the representations trained based the proposed method
perform well. Our proposed method is not trying to learn a classifier or a regression function directly,
but rather to learn representation that preserve all the information. So our experiments are designed
to evaluate the performance of simple classification and regression methods using the representations
we learned as input. The results demonstrate that a simple classification or regression model using
the representations we trained performs better than or comparably with the best classification or
regression method using deep neural networks.
Details on the network structures and hyperparameters are included in Appendix A. Our experiments
were conducted on Nvidia DGX Station workstation using a single Tesla V100 GPU unit. The
PyTorch code of SDRL is available at https://github.com/anonymous/SDRL.
6.1	Simulated data
In this subsection, we evaluate SDRL on simulated regression and classification problems.
Regression We generate 5, 000 data points from two models. Model A: y =
xι[0.5 + (x2 + 1.5)2]-1 + (1+ x2)2 + σε, where X 〜N (0, I4); Model B: y = sin2 (πxι + 1) +
σε, where X 〜Uniform([0,1]4). In both models, ε 〜N (0, I4). We use a 3-layer network with
ReLU activation for Rθ and a single hidden layer ReLU network for Dφ. We compare SDRL with
two prominent sufficient dimension reduction methods: sliced inverse regression (SIR) (Li, 1991)
and sliced average variance estimation (SAVE) (Cook & Weisberg, 1991). We fit a linear model
with the learned features and the response variable, and report the prediction errors in Table 1. We
see that SDRL outperforms SIR and SAVE in terms of prediction error.
Classification We visualize the learned features of SDRL on two simulated datasets. We first
generate (1) 2-dimensional concentric circles from two classes as in Figure 1 (a); (2) 2-dimensional
moons data from two classes as in Figure 1 (e); (3) 3-dimensional Gaussian data from six classes
6
Under review as a conference paper at ICLR 2021
Table 1: Averaged prediction errors and their standard errors (based on 5-fold validation).
Method	Model A			Model B		
	σ=0.1	σ = 0.4	σ=0.8	σ=0.1	σ = 0.2	σ= 0.3
SDRL	1.101 ± .193	1.179 ± .117	1.401 ± .159	0.149 ±.050	0.231 ± .025	0.325 ± .026
SIR	1.521 ± .133	1.614 ± .223	1.704 ± .095	0.266 ± .003	0.319 ± .004	0.391 ± .010
SAVE	1.521 ± .134	1.614 ± .221	1.702 ± .098	0.266 ± .003	0.319 ± .004	0.391 ± .010
as in Figure 1 (i). In each dataset, we generate 5,000 data points for each class. We next map
the data into 100-dimensional space using matrices with entries i.i.d Unifrom([0, 1]). Finally, we
apply SDRL to these 100-dimensional datasets to learn 2-dimensional features. We use a 10-layer
dense convolutional network (DenseNet) (Huang et al., 2017) as Rθ and a 4-layer network as Dφ.
We display the evolutions of the learned 2-dimensional features by SDRL in Figure 1. For ease of
visualization, we push all the distributions onto the uniform distribution on the unit circle, which
is done by normalizing the standard Gaussian random vectors to length one. Clearly, the learned
features for different classes in the examples are well disentangled.
(a) Epoch = 0
(b) Epoch = 10
(e) Epoch = 0
(f) Epoch = 10
(i) Epoch = 0
(j) Epoch = 10	(k) Epoch = 30	(l) Epoch = 500
Figure 1: Evolving learned features. The first, second and third rows show concentric circles, moons
and 3D Gaussian datasets, respectively.
6.2 Real datasets
Regression We use a benchmark YearPredictionMSD dataset to demonstrate the prediction per-
formance of SDRL (https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD).
This dataset has 515,345 observations with 90 predictors. The problem is to predict the year of
song release. We randomly split the data into five parts for cross validated evaluation of the predic-
tion performance. We employ a 3-layer network for both Dφ and Rθ . A linear regression model
is fitted using the learned representations and the response. The mean prediction errors and their
standard errors based on SDRL, principal component analysis (PCA), sparse principal component
analysis (SPCA) and ordinary least squares (OLS) regression with original data are reported in Table
2. SDRL outperforms PCA, SPCA and OLS in terms prediction accuracy.
Classification We benchmark the classification performance of SDRL using MNIST (LeCun et al.,
2010), FashionMNIST (Xiao et al., 2017), and CIFAR-10 (Krizhevsky et al., 2009) against alterna-
7
Under review as a conference paper at ICLR 2021
Table 2: Prediction error ± standard error: YearPredictionMSD dataset
Methods	d = 10	d=20	d=30	d=40
SDRL	8.8 ± 0.1	9.2 ± 0.8	9.2 ± 0.8	8.8 ± 0.1
SPCA	10.6 ± 0.1	10.4 ± 0.1	9.6 ± 0.1	10.2 ± 0.1
PCA	10.6 ± 0.1	10.4 ± 0.1	10.3 ±0.1	10.2 ± 0.1
OLS	———9.6 ±0.1———
tive methods including convolutional networks (CN) and distance correlation autoencoder (dCorAE)
(Wang et al., 2018). With CN, we use the feature extractor by dropping the cross entropy layer of
the DenseNet trained for classification. The MNIST and FashionMNIST datasets consist of 60k and
10k grayscale images with 28 × 28 pixels for training and testing, respectively, while the CIFAR-10
dataset contains 50k and 10k colored images with 32 × 32 pixels for training and testing, respec-
tively. For the learning from scratch strategy, the representer network Rθ has 20 layers for MNIST
data and 100 layers for CIFAR-10 data. We apply the transfer learning technique to the combination
of SDRL and CN on CIFAR-10 (Krizhevsky et al., 2009). The pretrained WideResnet-101 model
(Zagoruyko & Komodakis, 2016) on the Imagenet dataset with Spinal FC (Kabir et al., 2020) is
adopt for Rθ . The discriminator network Dφ is a 4-layer network. The the architecture of Rθ and
most hyperparameters are shared across all four methods - SDRL, CN, SDRL+CN and dCorAE.
Finally, we use the k-nearest neighbor (k = 5) classifier on the learned features for all methods.
The classification accuracies are reported in Tables 3 and 4. We can see that the classifica-
tion accuracies of SDRL are comparable with those of CN and dCorAE. As shown in Table
4, the classification accuracies of CN leveraging SDRL outperforms those of CN. We also cal-
culate the estimated distance correlation (DC) between the learned features and their labels as
ρ2,y = V[z,y]2/√(V[z]2 × V[y]2), where V[z] and V[y] are the distance variances such that
V[z] = V[z, z], V[y] = V[y, y]. For more details, please see Szekely et al. (2007). Figure 2
shows the DC values MNIST, FashionMNIST and CIFAR-10 data. SDRL and SDRL+CN achieves
higher DC values.
Table 3: Classification accuracy for MNIST and FashionMNIST
d		MNIST			FashionMNIST		
		SDRL	dCorAE	CN	SDRL	dCorAE	CN
d=	16	99.41	99.58	99.39	94.44	94.18	94.21
d=	32	99.61	99.54	99.45	94.18	93.89	94.41
d=	64	99.56	99.53	99.49	94.13	94.24	94.38
Table 4: Classification accuracy for CIFAR-10 data
d		Learning from scratch			Transfer learning		
		SDRL	dCorAE	CN	SDRL	CN	SDRL+CN
d=	16	94.29	94.15	94.21	97.52	97.44	97.68
d=	32	94.58	94.18	94.92	97.33	97.79	97.96
d=	64	94.46	94.66	95.09	97.49	97.90	97.91
7 Conclusion and future work
In this work, we formulate a framework for sufficient and disentangled representation learning and
construct an objective function characterizing conditional independence and disentanglement. This
enables us to learn a representation with the desired properties empirically. We provide statisti-
cal guarantees for the learned representation by deriving an upper bound on the excess risk of the
objective function.
8
Under review as a conference paper at ICLR 2021
(a) MNIST, d = 16
(b) Fashion MNIST, d = 16
(c) CIFAR-10 (from scratch), d = 16	(d) CIFAR-10 (transfer learning), d = 16
Figure 2: The distance correlations of labels with learned features based on SDRL, CN, SDRL+CN
and dCorAE for FashionMNIST and CIFAR-10 data.
There are several questions that deserve further study. First, we can adopt different measures of
conditional independence including mutual information and conditional covariance operators on re-
producing kernel Hilbert spaces (Fukumizu et al., 2009). We can also use other divergence measures
such as the Wasserstein distance in the objective function. Finally, Lemma 2.1 suggests that the in-
tersection of the Fisher class F and the Maxwell class M can still be large, and there can be many
statistically equivalent representations in F ∩ M. We can make further reduction of F ∩ M by
imposing additional constraints, for example, certain minimal properties, sparsity, and robustness
against noise perturbation.
9
Under review as a conference paper at ICLR 2021
References
Alessandro Achille and Stefano Soatto. Emergence of invariance and disentanglement in deep rep-
resentations. The Journal ofMachine Learning Research, 19(1):1947-1980, 2018.
Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier
probes. arXiv preprint arXiv:1610.01644, 2016.
Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information
bottleneck. arXiv preprint arXiv:1612.00410, 2016.
Syed Mumtaz Ali and Samuel D Silvey. A general class of coefficients of divergence of one distri-
bution from another. Journal of the Royal Statistical Society: Series B (Methodological), 28(1):
131-142, 1966.
Brandon Amos and J. Zico Kolter. A PyTorch Implementation of DenseNet. https://github.
com/bamos/densenet.pytorch. Accessed: [20 Feb 2020].
Martin Anthony and Peter L Bartlett. Neural Network Learning: Theoretical Foundations. cam-
bridge University Press, 2009.
M. S. Bartlett. The vector representation of a sample. Mathematical Proceedings of the Cambridge
Philosophical Society, 30(3):327-340, 1934. doi: 10.1017/S0305004100012512.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3:463-482, 2002.
Peter L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension
and pseudodimension bounds for piecewise linear neural networks. Journal of Machine Learning
Research, 20:1-17, 2019.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798-1828,
2013.
Yann Brenier. Polar factorization and monotone rearrangement of vector-valued functions. Commu-
nications on Pure and Applied Mathematics, 44(4):375-417, 1991.
Wlodzimierz Bryc. The Normal Distribution: Characterizations with Applications. Lecture Notes
in Statistics. Springer New York, 1995.
C. Burgess, I. Higgins, A. Pal, Loic Matthey, Nick Watters, G. Desjardins, and Alexander Lerchner.
Understanding disentangling in beta-vae. arXiv: Machine Learning, 2018.
Ricky T. Q. Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of dis-
entanglement in variational autoencoders. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31,
pp. 2610-2620. Curran Associates, Inc., 2018.
Dennis R. Cook. Regression Graphics: Ideas for Studying Regressions Through Graphics. Wiley
Series in Probability and Statistics. Wiley, 1998. ISBN 9780471193654.
Dennis R. Cook and S. Weisberg. Sliced inverse regression for dimension reduction: comment.
Journal of the American Statistical Association, 86(414):328-332, 1991.
R Dennis Cook. Fisher lecture: Dimension reduction in regression. Statistical Science, 22(1):1-26,
2007.
Victor De la Pena and Evarist Gine. Decoupling: from Dependence to Independence. Springer
Science & Business Media, 2012.
Kien Do and T. Tran. Theory and evaluation metrics for learning disentangled representations.
ArXiv, abs/1908.09961, 2020.
10
Under review as a conference paper at ICLR 2021
Cian Eastwood and Christopher K. I. Williams. A framework for the quantitative evaluation of
disentangled representations. In International Conference on Learning Representations, 2018.
Ronald A. Fisher. On the mathematical foundations of theoretical statistics. Philosophical Transac-
tions of the Royal Society of London, A, 222:309-368,1922.
Kenji Fukumizu, Francis R Bach, Michael I Jordan, et al. Kernel dimension reduction in regression.
The Annals of Statistics, 37(4):1871-1905, 2009.
Yuan Gao, Yuling Jiao, Yang Wang, Yao Wang, Can Yang, and Shunkang Zhang. Deep generative
learning via variational gradient flow. In International Conference on Machine Learning, pp.
2093-2101, 2019.
Yuan Gao, Jian Huang, Yuling Jiao, and Jin Liu. Learning implicit generative models with theoretical
guarantees. arXiv preprint arXiv:2002.02862, 2020.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems, volume 27, pp. 2672-2680. 2014.
Balazs Gyenis. Maxwell and the normal distribution: A colored story of probability, independence,
and tendency toward equilibrium. Studies in History and Philosophy of Science Part B: Studies in
History and Philosophy of Modern Physics, 57:53-65, 2017. doi: 10.1016/j.shpsb.2017.01.001.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. In 5th International Conference on Learning Representations,
2017.
Irina Higgins, D. Amos, D. Pfau, SebaStien Racaniere, Loic Matthey, Danilo Jimenez Rezende,
and Alexander Lerchner. Towards a definition of disentangled representations. ArXiv, ab-
s/1812.02230, 2018.
Alex Hjelm, Devon R.and Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman,
Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information esti-
mation and maximization. arXiv preprint arXiv:1808.06670, 2018.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 4700-4708, 2017.
Xiaoming Huo and Gabor J Szekely. Fast computing for distance covariance. Technometrics, 58(4):
435-447, 2016.
HM Kabir, Moloud Abdar, Seyed Mohammad Jafar Jalali, Abbas Khosravi, Amir F Atiya, Saeid
Nahavandi, and Dipti Srinivasan. Spinalnet: Deep neural network with gradual input. arXiv
preprint arXiv:2007.03347, 2020.
Amor Keziou. Dual representation of 夕-divergences and applications. Comptes Rendus
Mathematique, 336(10):857-862, 2003.
Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In International Conference on
Machine Learning, pp. 2649-2658, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arX-
iv:1312.6114, 2013.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical Report TR-2009, University of Toronto, Toronto., 2009.
11
Under review as a conference paper at ICLR 2021
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
Iutional neural networks. In Advances in neural information processing Systems, pp. 1097-1105,
2012.
Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan. Variational inference of disen-
tangled latent concepts from unlabled observations. In International Conference on Learning
Representations, 2018.
Yann LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel.
Backpropagation applied to handwritten zip code recognition. Neural Computation, 1:541-551,
1989.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. 2010. URL
http://yann. lecun. com/exdb/mnist, 7:23, 2010.
Ker-Chau Li. Sliced inverse regression for dimension reduction. Journal of the American Statistical
Association, 86(414):316-327, 1991.
Tengyuan Liang. On how well generative adversarial networks learn densities: nonparametric and
parametric results. arXiv: Statistics Theory, 2018.
Francesco Locatello, Michael Tschannen, Stefan Bauer, Gunnar Ratsch, Bernhard Scholkopf, and
Olivier Bachem. Disentangling factors of variation using few labels. arXiv preprint arX-
iv:1905.01258, 2019.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, and Ian Goodfellow. Adversarial autoencoders.
In International Conference on Learning Representations, 2016.
James C. Maxwell. Illustrations of the dynamical theory of gases. part i. on the motions and colli-
sions of perfectly elastic spheres. The London, Edinburgh, and Dublin Philosophical Magazine
and Journal of Science, 19(124):19-32, 1860.
Robert J. McCann. Existence and uniqueness of monotone measure-preserving maps. Duke Mathe-
matical Journal, 80(2):309-324, 1995.
XuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence functionals
and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory,
56(11):5847-5861, 2010.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers
using variational divergence minimization. In Advances in neural information processing systems,
pp. 271-279, 2016.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018.
Guido Philippis. Regularity of optimal transport maps and applications, volume 17. Springer
Science & Business Media, 2013.
Karl Ridgeway and Michael C. Mozer. Learning deep disentangled embeddings with the f-statistic
loss. ArXiv, abs/1802.05312, 2018.
Tyrrell R. Rockafellar. Convex analysis. Princeton University Press, 1970.
Andrew M. Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Brendan D
Tracey, and David D Cox. On the information bottleneck theory of deep learning. Journal of
Statistical Mechanics: Theory and Experiment, 2019(12):124020, 2019.
Zuowei Shen, Haizhao Yang, and Shijun Zhang. Deep network approximation characterized by
number of neurons. arXiv preprint arXiv:1906.05497, 2019.
Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via informa-
tion. arXiv preprint arXiv:1703.00810, 2017.
12
Under review as a conference paper at ICLR 2021
Shashank Singh, Ananya UPPaL BoyUe Li, Chun-Liang Li, Manzil Zaheer, and Barnabas Poczos.
Nonparametric density estimation under adversarial losses. In Advances in Neural Information
Processing Systems, pp. 10225-10236, 2018.
Aravind Srinivas, Michael Laskin, and Pieter Abbeel. Curl: Contrastive unsuPervised rePresenta-
tions for reinforcement learning. arXiv preprint arXiv:2004.04136, 2020.
Charles J. Stone. Optimal global rates of convergence for nonparametric regression. The Annals of
Statistics, pp. 1040-1053, 1982.
Gabor J. Szekely and Maria L. Rizzo. Brownian distance covariance. The AnnalS ofApplied StatiS-
tics, 3(4):1236-1265, 2009.
Gabor J. Szekely, Maria L. Rizzo, Nail K. Bakirov, et al. Measuring and testing dependence by
correlation of distances. The Annals of Statistics, 35(6):2769-2794, 2007.
Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In
2015 IEEE Information Theory Workshop (ITW), pp. 1-5. IEEE, 2015.
Naftali Tishby, Fernando C Pereira, and W. Bialek. The information bottleneck method. ArXiv,
physics/0004057, 2000.
Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein auto-
encoders. In International Conference on Learning Representations, 2018.
Michael Tschannen, Josip Djolonga, Paul K Rubenstein, Sylvain Gelly, and Mario Lucic. On mutual
information maximization for representation learning. arXiv preprint arXiv:1907.13625, 2019.
Alexandre Tsybakov. Introduction to Nonparametric Estimation. Springer Science & Business
Media, 2008.
Cedric Villani. Optimal Transport: Old and New, volume 338. Springer, 2008.
Rick Wang, Amir-Hossein Karimi, and Ali Ghodsi. Distance correlation autoencoder. In 2018
International Joint Conference on Neural Networks (IJCNN), pp. 1-8. IEEE, 2018.
Satosi Watanabe. Information theoretical analysis of multivariate correlation. IBM Journal of Re-
search and Development, 4(1):66-82, 1960.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arX-
iv:1605.07146, 2016.
A Appendix: Experimental details
A. 1 S imulation studies
The values of the hyper-parameters for the simulated experiments are given in Table A1, where λ is
the penalty parameter, d is the dimension of the SDRM, n is the mini-batch size in SGD, T1 is the
number of inner loops to push forward particles zi, T2 is the number of outer loops for training Rθ,
and s is the step size to update particles. For the regression models, the neural network architectures
are shown in Table A2
As shown in Table A3, a multilayer perceptron (MLP) is utilized for the neural structure Dφ
in the classification problem. The detailed architecture of 10-layer dense convolutional network
(DenseNet) (Huang et al., 2017; Amos & Kolter) deployed for Rθ is shown in Table A4. For all the
settings, we adopted the Adam (Kingma & Ba, 2014) optimizer with an initial learning rate of 0.001
and weight decay of 0.0001.
13
Under review as a conference paper at ICLR 2021
Table A1: Hyper-parameters for simulated examples, where s varies according to epoch
Task	λ	d	n	Ti	T2	s		
						0-150	151-225	226-500
Regression	1.0	2 or 1	64	1	500	3.0	2.0	1.0
Classification	1.0	2	64	1	500	2.0	1.5	1.0
Table A2: MLP architectures for Dφ and Rθ in regression
Layers	Dφ		Rθ	
	Details	Output size	Details	Output size
Layer 1	Linear, LeakyReLU	16	Linear, LeakyReLU	16
Layer 2	Linear	1	Linear, LeakyReLU	8
Layer 3			Linear	d
Table A3: MLP architecture for Dφ of simulated classification examples and the benchmark clas-
sification datasets
Layers	Details	Output size
Layer 1	Linear, LeakyReLU	64
Layer 2	Linear, LeakyReLU	128
Layer 3	Linear, LeakyReLU	64
Layer 4	Linear	1
Table A4: DenseNet architecture for Rθ in the simulated classification examples
Layers	Details	Output size
Convolution	3 X 3 Conv	24 × 20 × 20
	BN, 1 × 1 Conv	
Dense Block 1	BN, 3 × 3 Conv × 1	36 × 20 × 20
Transition Layer 1	BN, ReLU, 2 × 2 Average Pool,1 × 1 Conv	30 × 10 × 10
	BN, 1 × 1 Conv	
Dense Block 2	BN, 3 × 3 Conv ×	18× 10× 10
Transition Layer 2	BN, ReLU, 2 × 2 Average Pool, 1 × 1 Conv	15 × 5 × 5
	BN, 1 × 1 Conv	
Dense Block 3	BN, 3 × 3 Conv × 1	27 × 5 × 5
Pooling	BN, ReLU, 5 × 5 Average Pool, Reshape	27
Fully connected	Linear	2
14
Under review as a conference paper at ICLR 2021
A.2 Real datasets
Regression: In the regression problems, hyper-parameters are presented in Table A5. The Adam
optimizer with an initial learning rate of 0.001 and weight decay of 0.0001 is adopted. The MLP
architectures of Dφ and Rθ for the YearPredictionMSD data are shown in Table A6.
Table A5: Hyper-parameters for YearPredictionMSD data
Dataset	λ	d	n	T1	T2	s
YearPredictionMSD	1.0	10, 20, 30, 40	64	1	500	1.0
Table A6: MLP architectures for Dφ and Rθ for YearPredictionMSD data
Layers	Dφ		Rθ	
	Details	Output size	Details	Output size
Layer 1	Linear, LeakyReLU	32	Linear, LeakyReLU	32
Layer 2	Linear, LeakyReLU	8	Linear, LeakyReLU	8
Layer 3	Linear	1	Linear	d
Classification: For the classification problems, hyper-parameters are shown in Table A7. We again
use Adam as the SGD optimizers for both Dφ and Rθ. Specifically, learning rate of 0.001 and weight
decay of 0.0001 are used for Dφ in all datasets and for Rθ on MNIST (LeCun et al., 2010). We
customized the SGD optimizers with momentum at 0.9, weight decay at 0.0001, and learning rate ρ
in Table A8 for FashionMNIST (Xiao et al., 2017) and CIFAR-10 (Krizhevsky et al., 2012). For the
transfer learning of CIFAR-10, we use customized SGD optimizer with initial learning rate of 0.001
and momentum of 0.9 for Rθ . MLP architectures of the discriminator network Dφ for MNIST,
FashionMNIST and CIFAR-10 are given in Table A3. The 20-layer DenseNet networks shown in
Table A9 were utlized for Rθ on the MNIST dataset, while the 100-layer DenseNet networks shown
in Table A10 and A11 are fitted for Rθ on FashionMNIST and CIFAR-10.
Table A7: Hyper-parameters for the classification benchmark datasets
Dataset	λ	d	n	T1	T2	s
MNIST	1.0	16, 32, 64	64	1	300	0.1
FashionMNIST	1.0	16, 32, 64	64	1	300	1.0
CIFAR-10	1.0	16, 32, 64	64	1	300	1.0
CIFAR-10 (transfer learning)	0.01	16, 32, 64	64	1	50	1.0
B	Appendix: Proofs
In this appendix, we prove Lemmas 2.1 and 4.1, and Theorems 4.2 and 4.3.
B.1	Proof of Lemma 2.1
Proof. By assumption μ and Yd are both absolutely continuous with respect to the LebesgUe mea-
sure. The desired result holds since it is a spacial case of the well known results on the existence
of optimal transport (Brenier, 1991; McCann, 1995), see, Theorem 1.28 on page 24 of (Philippis,
2013) for details.	口
15
Under review as a conference paper at ICLR 2021
Table A8: Learning rate ρ varies during training.
Epoch	0-150	151-225	226-300
ρ	0.1	0.01	0.001
Table A9: Architecture for MNIST, reduced feature size is d
Layers	Details	Output size
Convolution	3 X 3 Conv	24 × 28 × 28
	BN, 1 × 1 Conv	
Dense Block 1	BN, 3 × 3 Conv × 2	48 × 28 × 28
Transition Layer 1	BN, ReLU, 2 × 2 Average Pool,1 × 1 Conv	24× 14 × 14
	BN, 1 × 1 Conv	
Dense Block 2	BN, 3 × 3 Conv × 2	48 × 14 × 14
Transition Layer 2	BN, ReLU, 2 × 2 Average Pool, 1 × 1 Conv	24× 7× 7
	BN, 1 × 1 Conv	
Dense Block 3	BN, 3 × 3 Conv × 2	48 × 7 × 7
Pooling	BN, ReLU, 7 × 7 Average Pool, Reshape	48
Fully connected	Linear	d
Table A10: Architecture for FashionMNIST, reduced feature size is d
Layers	Details	Output size
Convolution	3 × 3 Conv	24 × 28 × 28
	BN, 1 × 1 Conv	
Dense Block 1	BN, 3 × 3 Conv × 16	216×28× 28
Transition Layer 1 BN, ReLU, 2 × 2 Average Pool,1 × 1 Conv		108× 14× 14
	BN, 1 × 1 Conv	
Dense Block 2	BN, 3 × 3 Conv × 16	300 × 14 × 14
Transition Layer 2 BN, ReLU, 2 × 2 Average Pool, 1 × 1 Conv		150 × 7× 7
	BN, 1 × 1 Conv	
Dense Block 3	BN, 3 × 3 Conv × 16	342 × 7 × 7
Pooling	BN, ReLU, 7 × 7 Average Pool, Reshape		342
Fully connected	Linear	d
Table A11: Architecture for CIFAR-10, reduced feature size is d
Layers	Details	Output size
Convolution	3 × 3 Conv	24 × 32 × 32
	BN, 1 × 1 Conv	
Dense Block 1	BN, 3 × 3 Conv × 16	216 × 32 × 32
Transition Layer 1 BN, ReLU, 2 × 2 Average Pool,1 × 1 Conv		108× 16× 16
	BN, 1 × 1 Conv	
Dense Block 2	BN, 3 × 3 Conv × 16	300× 16× 16
Transition Layer 2 BN, ReLU, 2 × 2 Average Pool, 1 × 1 Conv		150×8×8
	BN, 1 × 1 Conv	
Dense Block 3	BN, 3 × 3 Conv × 16	342 × 8 × 8
Pooling	BN, ReLU, 8 × 8 Average Pool, Reshape		342
Fully connected	Linear	d
16
Under review as a conference paper at ICLR 2021
B.2	Proof of Lemma 4.1
Proof. Our proof follows KezioU (2003). Since f(t) is convex, then ∀t ∈ R, we have f (t) = f **(t),
where
f**(t)=sup{st- f*(s)}
s∈R
is the Fenchel conjugate of f *. By Fermat,s rule, the maximizer s* satisfies
t ∈ ∂f*(s*),
i.e.,
s* ∈ ∂f (t)
Plugging the above display with t = dμZ (x) into the definition of f -divergence, We derive (6). □
B.3	Proof of Theorem 4.2
Proof. Without loss of generality, We assume d = 1. For R* satisfying (3) and any R ∈ R,
We have R = p(r,r*)R* + £r, where p(r,r*)is the correlation coefficient between R and R*,
er = R - P(r,r*)R*. It is easy to see that εR ɪ R* and thus Y " εR. As (p(r,r*)R*,Y) is
independent of (er, 0), then by Theorem 3 of Szekely & Rizzo (2009)
V[R, y] =V[ρ(R,R*)R* + εR, y] ≤ V[ρ(R,R*)R*, y] + V(εR, 0)
=V [P(R,R*)R*, y] = |p(R,R*)ΙV [R*, y]
≤V[R*,y].
As R(x)〜N(0,1) and R*(x)〜N(0,1), then Df (μR(χ)kγd) = Df (μκ*(x)∣∣Yd) = 0,and
L(R) - L(R*) = V[R*, y] - V[R, y] ≥0.
The proof is completed.	□
B.4 Proof of Theorem 4.3
DenOte B2 = max{lf0(cι)1, |f0(c2)|}, B3 = max∣s∣≤2L2√dlogn+B2 |f *(s)|. We set the network
parameters of the representer Rθ and the discriminator Dφ as follows.
(i)	Representer network RD,W,S,B parameters: depth D = 9 log n + 12, width
W = dmax{8d(n2+p/ logn) 1 + 4p, 12n2+p/ logn + 14}, size S = dnPp+2/ log4(npd),
B = (2B3L1√p + log n)√d,
(ii)	Discriminator network MDWSB parameters: depth D = 9log n + 12, width W =
max{8d(n2+d/ logn) 1 + 4d, 12n2++d/ logn + 14}, size S = nd+2/(log4 npd), B =
2L2√d log n + B2.
Before getting into the details of the proof of Theorem 4.3, we first give an outline of the basic
structure of the proof.
Without loss of generality, we assume that λ = 1 and m = 1, i.e. y ∈ R. First we consider the
scenario that y is bounded almost surely, say |y| ≤ C1. We also assume B1 < ∞. We can utilize the
truncation technique to transfer the unbounded cases to the bounded ones under some common tail
assumptions. Consequently, an additional logn multiplicative term will appear in the final results.
For any R ∈ NDW,s,b, We have,
,^ . . . , ^ . ^, ^ .	^, ^ . ^, . ^, . , —. , —.	..
L(Rθ) - L(R*) = L(Rθ) - L(Rθ) + L(Rθ) - L(R) + L(R) - L(R) + L(R) - L(R*)
...	■—■...	.	—.	....
≤ 2 SuP	|L(R) -L(R)∣ + inf	|L(R) -L(R*)∣,	(10)
R∈Nd,W,S,B	r∈nd,w,s,b
where we use the definition of Rθ in (8) and the feasibility of R Next we bound the two error
terms in (10), i.e., the approximation error infr∈n0 WSB |L(R) - L(R*)∣ and the statistical
error suPR∈N	|L(R) - L(R)| separately. Then Theorem 4.3 follows after bounding these
two error terms., , ,
17
Under review as a conference paper at ICLR 2021
B.4.1 THE APPROXIMATION ERROR
Lemma B.1.
_ inf	IL(A) -L(R*)∣≤ 2600C1B1L1√Pdn-p++2.	(11)
R∈Nd,w,s,b
Proof. By (3) and (6) and the definition of L, we have
一 inf	IL(R)-L(R*)∣ ≤ ∣Df (〃^的|儿)| + ∣V [R*(x), y] -V [R¾(x), y]∣,	(12)
R∈Nd,w,s,b
where Ra ∈ NDW,s,b is specified in Lemma B.2 below. We finish the proof by (14) in Lemma B.3
and (15) in Lemma B.4, which will be proved below.	□
Lemma B.2. Define R*(x) = mιn{R*(x), logn}. There exist a Ra ∈ NDWWB WIth depth
p	1	p
D = 9 log n + 12, width W = dmax{8d(n2+p / log n)p + 4d, 12n2+p / log n + 14}, and size
S = dnp+2 / (log4 npd), B = (2BιLι√p + logn)√d, such that
klis - R*M("x) ≤ 160L1B1√Pdn-p⅛.	(13)
Proof. Let RKX) be the i-th entry of R*(x) : Rd → Rd. By the assumption on R*, it is easy
to check that Ri(x) is Lipschitz continuous on [-B1 ,Bι]d with the Lipschitz constant L1 and
∣∣R"∣l∞ ≤ logn. By Theorem 4.3 in Shen et al. (2019), there exists a ReLU network Rai with
P	1	P	—
with depth 9logn +12, width max{8d(n2+p/logn)p + 4d, 12n2+p/logn + 14}, ∣∣RaJ∣L∞ =
2BιLι√p + logn, such that
kRai∣∣L∞ ≤ 2B1L1 √p +logn,
and
2
kRi- RaikL∞([-B1,B1]P∖H) ≤ 80LIB1√pn p+2 ,
_	2
80LIB1√pn P十2
μx( ) ≤ 2B1L1√P + log n.
Define Ra = [R®[,..., Ra/ ∈ Nd,w,s,b. The above three display implies
kRa - R*l∣L2(μχ) ≤ 160L1B1PPdn-F⅛.
□
Lemma B.3.
∣V [R*(x), y] -V [Ra(x), y]∣ ≤ 2580QB1 L1√Pdn-p⅛.	(14)
Proof. Recall that Szekely et al. (2007)
V[z, y] =E[kz1 - Z2k∣y1 - y2∣] - 2E[kz1 - z2k∣y1 - y3∣]
+ E[kz1 - z2k]E[∣y1 - y2∣],
where (zi, yi),i = 1, 2,3 are i.i.d. copies of (z, y). We have
∣v[R*(x), y] - v[Ra(x), y]∣
≤ ∣E [(kR*(x1) - R*M)k - kRa(x1) - Ra(x2)k)∣y1 -y2∣] ∣
+ 2∣E [(∣∣R*(x1) - R*(x2)k - kRa(x1) - Ra(x2)k)∣y1 - y3∣] ∣
+ ∣e [kR*(XI)- R*(x2)k - kRa(XI)- Ra(X2)] E[ky1 - y2∣∣] ∣
≤ 8C1E 川∣R*(x1) - R*(x2)k - kRa(x1) - Ra(x2)k∣]
≤ 16C1E 川∣R*(x)- Ra(x)k]
≤ 16C1(E [kR*(x) - Ra(x)k] + E [I∣R*(x)1r* (x)∈Ballc(0,log n)k]),
18
Under review as a conference paper at ICLR 2021
where in the first and third inequalities we use the triangle inequality, and second one follows from
___________________________________________________________________________	1
the boundedness of y. By (13), the first term in the last line is bounded by 2560CιBιLι √pdn p+2.
Some direct calculation shows that
E[kR*(x)lR*(x)∈Ballc(0,logn)k] ≤ C2	^	.
We finish the proof by comparing the order of the above two terms, i.e., C2 (logn)	≤
2
20CιBιLι√pdn p+2.	□
Lemma B.4.
|Df(μRs(χ)kYd)l ≤ 20C1B1L1pdn-p+2.	(15)
Proof. By Lemma B.2 Rg can approximate R* arbitrary well, the desired result follows from the
fact that Df (μR*(χ)kγd) = 0 and the continuity of Df (μR(χ)kγd) on R. We present the sketch of
the proof and omit the details here. Let r*(z) = dμRγ*((x) (Z) and 尸(Z) = dμRθ((X) (z). By definition
we have
Df(MR*(x)kYd) = EW 〜Yd [f (r* (W ))]
=EW〜Yd [f (r* (W))1W∈Ball(0,log n)]+ EW〜Yd [f (r* (W))1W∈Ballc(0,log n)]∙
(We can represent Df(μR@ ∣∣γd) similarly.) Then
|Df(μRs(x)∣Yd)1 = IDf(μns(x)kYd) - Df(μR*(χ)∣Yd)1
≤ EW〜Yd[∣f(r*(W ))- f (r(W ))∣1w ∈Ball(0,log n)]
+ EW 〜Yd [|f (r*(W )) - f(r*(W ))l1W ∈Ballc(0,log n)]
≤ Z	lf0(r(Z))||r"(Z)—尸(Z)IdYd(Z)+ Z	lf0(r(Z))||r"(Z)—尸(Z)IdYd(Z)
kzk≤log n	kzk>log n
≤ C3 [	∣r*(Z)—尸(Z)IdYd(Z)+ C4 [	∣r*(Z)—尸(z)∣
kzk≤logn	kzk>log n
The first term in the above display is small due to Rg can approximate R* well. The second term is
small due to the boundedness of r and the exponential decay of the Gaussian tails.
□
B.4.2 The statistical error
Lemma B.5.
SUp	IL(R) — Lb(R)I ≤ C15(B1(L1 + L2)PPd)n-2+p + (L2√d + B2 + B3) lognn-2+d)
R∈ND,W,S,B
(16)
Proof. By the definition and the triangle inequality we have
E[	sUp	IL(R) — Lb(R)I]
R∈ND,W,S,B
≤ E[ sUp	IVbn[R(x), y] — V[(R(x),y)I]
R∈ND,W ,S ,B
+ E[	SUp	ID f (μR(χ)UYd)- Df(μR(χ)H Yd) I].
R∈ND,W,S,B
We finish the proof based on (17) in Lemma B.6 and (22) in Lemma B.7, which will be proved
below.	□
Lemma B.6.
E[ sup	IVn[R(χ), y] — V[R(χ), y]I] ≤ 4C6C7C10B1L1ppdn-p+2.	(17)
R∈ND,W,S,B
19
Under review as a conference paper at ICLR 2021
Proof. We first fix some notation for simplicity. Denote O = (x, y) ∈ Rp × R1 and Oi =
(xi, yi),i = 1, ...n are i.i.d copy of O, and denote μχ,y and P区n as P and Pn, respectively.
∀R ∈ ND,W,S,B, let O = (R(x), y) and Oi = (R(xi), yi), i = 1, ...n are i.i.d copy of O. Define
centered kernel h,R : (Rp X R1 )N 4 → R as
— ,-τ ~	~ -τ	-I   .. ,	_ , ....	.
hR(O1,O2,O3,O4) = 4 P 1≤i,j≤4, kR(xi) - R(Xj)k∣yi - yj|
i6=j
—1 Pi=ι (P 1≤j≤4, kR(Xi)- R(χj)k P 1≤j≤4, |yi - yj|	.	(18)
j6=i	i6=j
+ 24 P 1≤i,j≤4, kR(Xi)- R(Xj )k P 1≤i,j≤4, ∣yi - yj I-V [R(X), y]
i6=j	i6=j
EI	.1	. ITT .	.∙	.∙ C l^C∕∖	1	\	1	F	1
Then, the centered U -statistics Vn [R(X), y] - V [R(X), y] can be represented as
1
Un(hR) = C4	hR(OiI ,Oi2 ,θi3 ,Oi4)
n 1≤i1<i2<i3<i4≤n
Our goal is to bound the supremum of the centered U-process Un (hR) with the nondegenerate kernel
hR. By the symmetrization randomization Theorem 3.5.3 in De la Pena & Gine (2012), We have
1
E[	sup	∣Un(hR)∣] ≤	C5E[	sup	IC4	E	% hR(Oi1,0i2,	O⅛ ,Oi4)∣],
R∈ND,W,S,B	R∈ND,W,S,B Cn 1≤i1<i2<i3<i4≤n
(19)
where, i1 , i1 = 1, ...n are i.i.d Rademacher variables that are also independent with Oi, i =
1, . . . , n. We finish the proof by upper bounding the above Rademacher process with the matric
entropy of ND,W,S,B . To this end we need the following lemma.
Lemma B.7. If ξi, i = 1, ...m are m finite linear combinations of Rademacher variables j , j
1, ..J. Then
Eej,j=ι,…JImaxn ∣ξi∣ ≤。6 (log m)1/21maχn (Eξ2)1/2.	q。)
Proof. This result follows directly from Corollary 3.2.6 and inequality (4.3.1) in De la Pena & Gine
(2012) with Φ(x) = exp(x2).	□
By the boundedness assumption on y and the boundedness of R ∈ ND,W,S,B, we have that the
kernel hR is also bounded, say
IIhRkL∞ ≤ c7(2B1L1√p + log n)√d.	(21)
∀R, R ∈ ND,W,S,B define a random empirical measure (depends on Oi, i = 1, . . . , n)
en,1(R, R) = E%ι,iι = 1,…,n| ^4	X	eiι (hR - hR)(OiI, ∙ ∙ ∙,九九
n 1≤i1 <i2 <i3 <i4 ≤n
Condition on Oi, i = 1, . . . , n, let C(N, en,1, δ)) be the covering number ofND,W,S,B with respect
to the empirical distance en,1 at scale of δ > 0. Denote Nδ as the covering set of ND,W,S,B with
20
Under review as a conference paper at ICLR 2021
cardinality of C(N , en,1 , δ)). Then,
1
Eeiι [	sup	| C4	T	eiι hR (OiI, Oi2 , Oi3 , Oi4 )|]
R∈ND,W,S,B Cn 1≤i1<i2<i3<i4≤n
1
≤ δ + E€iι [ sup 1 C4	E	JιhR(OiI ,Oi2 ,Oi3 ,Oi4)|]
R∈Nδ	n 1≤i1<i2<i3<i4≤n
1n
≤ δ + C6C4(log C(N,en,ι,δ))1/2 max1£ E (hR(Oi1,Oi2 ,Oi3 ,Oi4))2]1/2
Cn	R∈Nδ
n	i1=1 i2<i3<i4
_1[ n(n!产	]1/2
Cn [((n - 3)!)2]
≤ δ + C6C7(2B1L1√P + logn)√d(log C(N, en,ι, δ))1/2
≤ δ + 2C6C7(2B1L1√p + log n)√d(log C(N, en,ι, δ))1∕2∕√n
—	L	2eBn「0 . L
≤ δ + 2c6c7(2bili√p + log n)√d(VCN log	—)	/√n
δVCN
≤ δ + C6C7C10(B1L1√P + log n)√d(DS log S log XGBn C )1r2∕√n.
δDS log S
where the first inequality follows from the triangle inequality, the second inequality uses (20),
the third and fourth inequalities follow after some algebra, and the fifth inequality holds due
to C(N, en,1, δ) ≤ C(N, en,∞, δ) and the relationship between the metric entropy and the VC-
dimension of the ReLU networks ND,W,S,B (Anthony & Bartlett, 2009), i.e.,
2eBn
log C(N, en,∞, δ)) ≤ VCN log NJr ,
δVCN
and the last inequality holds due to the upper bound of VC-dimension for the ReLU network
ND,W,S,B satisfying
C8DS log S ≤ VCN ≤ C9DS log S,
see Bartlett et al. (2019). Then (17) holds by the selection of the network parameters and set δ = ɪ
and some algebra.	□
Lemma B.8.
E[	SuP	|D f(μR(χ)∣∣Yd) - Df (μR(x)∣∣Yd)∣] ≤ C14(L2√d + B2 + B3)(n- 2++p + log nn- 2++d) (22)
R∈ND,W,S,B
Proof. ∀R	∈	Nd,W,S,B,	let	r(z)	=	dμRdx)(z),	gR(z)	=	f0(r(z)).	By assumption	gR(z)	:
Rd → R is Lipschitz continuous with the Lipschitz constant L2 and kgR kL∞ ≤ B2 . With-
out loss of generality, we assume supp(gR) ⊆ [- log n, logn]d. Then, similar to the proof of
Lemma B.2 We can show that there exists a Dφ ∈ MD WSB with depth D = 9log n + 12,
width W = max{8d(n2+d / log n) 1 + 4d, 12n2+d / log n + 14}, and size S = nd+2 / (log4 npd),
B = 2L2 √d log n + B2 such that for Z 〜Yd and Z 〜μR(χ
Ez[∣Dφ(z) - gR(z)∣] ≤ 160L2√dlognn-d++2.	(23)
∀g : Rd → R, define
E(g) = Eχiχ[g(R(x))] - EW〜γd[f*(g(W))],
1n
E(g) = E(g,R) = -]T[g(R(χi)) - f*(g(Wi))]∙
i=1
By (6) we have
E (gR) = Df (μR(x)∣∣Yd) =	sup	E (D).	(24)
measureable D:Rd—R
21
Under review as a conference paper at ICLR 2021
Then,
IDf(MR(X)||Yd)- Df(MR(X)Mγd)|
,_,	.	o,..
=IE(gR) - n max	E(DΦ)|
Dφ∈MD,W ,s,b
.,	一，一、，，	_ ,	.	o, __ ..
≤ IE(gR) - sup	E(Dφ)I + I sup	E(Dφ) - max	Eb(Dφ)I
Dφ∈MD,W ,S,B	Dφ∈MD,W ,S,B	φ∈ D,W ,S,B
. . . .—.. .	■—■.	..
≤∣E (gR)-E (D φ)∣ + SUp	∣E (Dφ)-E(Dφ)∣
Dφ∈MD,W ,S,B
≤ Ez~μR(x) [|gR - DφI(z)] + Ew~γd[∣f*(gR) - f*(Dφ)I(W)]+	sup	|E(Dφ) - E(Dφ)I
Dφ∈MD,W ,S,B
≤ 160(1 + B3)L2√dlognn-d+2 + sup	∣E(Dφ) - E(Dφ)∣
Dφ∈MD,W ,S,B
where we use the triangle inequality in the first inequality, and we use E(gR)	≥
supD ∈M E(Dφ) followed from (24) and the triangle inequality in the second inequality,
the third inequality follows from the triangle inequality, and the last inequality follows from (23)
and the mean value theorem. We finish the proof via bounding the empirical process
- ^ , ..-
U(D, R) = E[	sup	IE(D) - Eb(D)I].
R∈ND,W,S,B ,d∈md,w,s,b
Let S = (x, z) ~ μχ N Yd and Si, i = 1,..., n be n i.i.d CoPy of S. Denote
b(D,R; S)= D(R(X))- f *(D(z))∙
Then
E(D, R) =ES[b(D,R;S)]
and
1n
E(DE = — Vb(D,R; Si).
n
i=1
Let
G(M ×N) = — E{Si,ean	sup	I X eib(D, R; Si)|
n	[R∈ND,W,S,B ,d∈md,w,s,b i=ι
be the Rademacher complexity of MD WSB X Nd,w,s,b (Bartlett & Mendelson, 2002). Let
C(M × N, en,ι, δ)) be the covering number of MD WSB × NrDW,s,b With respect to the em-
pirical distance (depends on Si)
1n
dn,ι((D, R), (D, R)) = -%[£ ki(b(D, R Si) - b(D, R Si))∣]
i=1
at scale of δ > 0. Let Mδ × Nδ be SUCh a converging set of MD WSB × ND,W,S,B. Then,
U(D, R) = 2G(M ×N)
= 2ES1,...,Sn[Ei,i=1,...,n[G(N × M)I(S1, ...,Sn)]]
2n
≤ 2δ +- Esι,...,Sn [Eβi,i=ι,...,n[	SUp	∣ V qb(D,R; Si)∣∣(Sι,..., Sn)]
n	(D,R)∈Mδ ×Nδ i=1
1n
≤ 2δ + C12— Esι,...,Sn [(log C(M×N,en,ι, δ))"	max	[V b2(D,R; Si)]1/2]
n	(D,R)∈Mδ×Nδ i=1
≤ 2δ + C12 n ESι,...,Sn [(log C(M × N ,en,1, δ))1/2 √n(2L2√d log n + B2 + B3)]
≤ 2δ + C12 —^(2L2√d log n + B2 + B3 )(log C(M, eη,1, δ) + log C(N, dn,1, δ))1/2
n
≤ 2δ + C13
L2 √d log n + B2 + B3
Bn
(DS log S log 声中 +D Slog Slog
~
Bn
_ ~ -r _ ,r
δDS log S
)1/2
22
Under review as a conference paper at ICLR 2021
where the first equality follows from the standard symmetrization technique, the second equality
holds due to the iteration law of conditional expectation, the first inequality follows from the tri-
angle inequality, and the second inequality uses equation 20, the third inequality uses the fact that
b(D, R; S) is bounded, i.e., ∣∣b(D, R; S)∣∣l∞ ≤ 2L?√dlogn + B2 + B3, and the fourth inequality
follows from some algebra, and the fifth inequality follows from C(N, en,1, δ) ≤ C(N, en,∞, δ)
(similar result for M) and log C(N,en,∞,δ)) ≤ VCNlog δVeCN, and Nd,w,s,b satisfying
CgDS log S ≤ VCN ≤ C9DS log S, see Bartlett et al. (2019). Then (22) follows from the above
display with the selection of the network parameters of MD W S b, Nd,w,s,b and with δ = 1. □
Finally, Theorem 4.3 is a direct consequence of (11) in Lemma B.1 and (16) in Lemma B.5. This
completes the proof of Theorem 4.3.
23