Under review as a conference paper at ICLR 2021
Towards Multi-Sense Cross-Lingual
Alignment of Contextual Embeddings
Anonymous authors
Paper under double-blind review
Ab stract
Cross-lingual word embeddings (CLWE) have been proven useful in many cross-
lingual tasks. However, most existing approaches to learn CLWE including the
ones with contextual embeddings are sense agnostic. In this work, we propose
a novel framework to align contextual embeddings at the sense level by leverag-
ing cross-lingual signal from bilingual dictionaries only. We operationalize our
framework by first proposing a novel sense-aware cross entropy loss to model
word senses explicitly. The monolingual ELMo and BERT models pretrained with
our sense-aware cross entropy loss demonstrate significant performance improve-
ment for word sense disambiguation tasks. We then propose a sense alignment
objective on top of the sense-aware cross entropy loss for cross-lingual model
pretraining, and pretrain cross-lingual models for several language pairs (English
to German/Spanish/Japanese/Chinese). Compared with the best baseline results,
our cross-lingual models achieve 0.52%, 2.09% and 1.29% average performance
improvements on zero-shot cross-lingual NER, sentiment classification and XNLI
tasks, respectively. We will release our code.
1 Introduction
Cross-lingual word embeddings (CLWE) provide a shared representation space for knowledge transfer
between languages, yielding state-of-the-art performance in many cross-lingual natural language
processing (NLP) tasks. Most of the previous works have focused on aligning static embeddings. To
utilize the richer information captured by the pre-trained language model, more recent approaches
attempt to extend previous methods to align contextual representations.
Aligning the dynamic and complex contextual spaces poses significant challenges, so most of the
existing approaches only perform coarse-grained alignment. Schuster et al. (2019) compute the
average of contextual embeddings for each word as an anchor, and then learn to align the static
anchors using a bilingual dictionary. In another work, Aldarmaki & Diab (2019) use parallel sentences
in their approach, where they compute sentence representations by taking the average of contextual
word embeddings, and then they learn a projection matrix to align sentence representations. They find
that the learned projection matrix also works well for word-level NLP tasks. Besides, unsupervised
multilingual language models (Devlin et al., 2018; Artetxe & Schwenk, 2019; Conneau et al., 2019;
Liu et al., 2020) pretrained on multilingual corpora have also demonstrated strong cross-lingual
transfer performance. Cao et al. (2020) and Wang et al. (2020) show that unsupervised multilingual
language model can be further aligned with parallel sentences.
Though contextual word embeddings are intended to provide different representations of the same
word in distinct contexts, Schuster et al. (2019) find that the contextual embeddings of different
senses of one word are much closer compared with that of different words. This contributes to the
anisomorphic embedding distribution of different languages and causes problems for cross-lingual
alignment. For example, it will be difficult to align the English word bank and its Japanese translations
由艮行 and 岸 that correspond to its two different senses, since the contextual embeddings of different
senses of bank are close to each other while those of 由艮行 and 岸 are far. Recently, Zhang et al.
(2019) propose two solutions to handle multi-sense words: 1) remove multi-sense words and then
align anchors in the same way as Schuster et al. (2019); 2) generate cluster level average anchor for
contextual embeddings of multi-sense words and then learn a projection matrix in an unsupervised
way with MUSE (Conneau et al., 2017). They do not make good use of the bilingual dictionaries,
1
Under review as a conference paper at ICLR 2021
which are usually easy to obtain, even in low-resource scenarios. Moreover, their projection-based
approach still cannot handle the anisomorphic embedding distribution problem.
In this work, we propose a novel sense-aware cross entropy loss to model multiple word senses ex-
plicitly, and then leverage a sense level translation task on top of it for cross-lingual model pretraining.
The proposed sense level translation task enables our models to provide more isomorphic and better
aligned cross-lingual embeddings. We only use the cross-lingual signal from bilingual dictionaries for
supervision. Our pretrained models demonstrate consistent performance improvements on zero-shot
cross-lingual NER, sentiment classification and XNLI tasks. Though pretrained on less data, our
model achieves the state-of-the-art result on zero-shot cross-lingual German NER task. To the best of
our knowledge, we are the first to perform sense-level contextual embedding alignment with only
bilingual dictionaries.
2 Background: prediction tasks of language models
Next token prediction and masked token prediction are two common tasks in neural language model
pretraining. We take two well-known language models, ELMo (Peters et al., 2018) and BERT (Devlin
et al., 2018), as examples to illustrate these two tasks (architectures are shown in Appendix A).
Next token prediction ELMo uses next token prediction tasks in a bidirectional language model.
Given a sequence of N tokens (t1, t2, . . . , tN), it first prepares a context independent representation
for each token by using a convolutional neural network over the characters or by word embedding
lookup (a.k.a. input embeddings). These representations are then fed into L layers of LSTMs to
generate the contextual representations: hi,j for token ti at layer j. The model assigns a learnable
output embedding w for each token in the vocabulary, which has the same dimension as hi,L . Then,
the forward language model predicts the token at position k with:
p(tk|t1,t2, . . . , tk-1) = softmax(hkT-1,Lwk0)
exP(hT-1,LwkO)
PV=I exP(hT-1,Lwi)
(1)
where k0 is the index of token tk in the vocabulary, V is the size of the vocabulary, and (w1, . . . , wV )
are the output embeddings for the tokens in the vocabulary. The backward language model is similar
to the forward one, except that tokens are predicted in the reverse order. Since the forward and
backward language models are very similar, we will only describe our proposed approach in the
context of the forward language model in the subsequent sections.
Masked token prediction The Masked Language Model (MLM) in BERT is a typical example
of masked token prediction. Given a sequence (t1, t2, . . . , tN), this approach randomly masks a
certain percentage (15%) of the tokens and generates a masked sequence (m1, m2, . . . , mN), where
mk = [mask] if the token at position k is masked, otherwise mk = tk. BERT first prepares the
context independent representations (x1, x2, . . . , xN) of the masked sequence via token embeddings.
It is then fed into L layers of transformer encoder (Vaswani et al., 2017) to generate “bidirectional”
contextual token representations. The final layer representations are then used to predict the masked
token at position k as follows:
p(mk = tk ∣m1, ..., mN) = softmaχ(hk Lwk0) = ~^V	，~~	^
i=1exp(hkT,Lwi)
(2)
where k0, V , h and w are similarly defined as in Eq. 1. Unlike ELMo, BERT ties the input and
output embeddings.
3 Proposed framework
We first describe our proposed sense-aware cross entropy loss to model multiple word senses explicitly
in language model pretraining. Then, we present our joint training approach with sense alignment
objective for cross-lingual mapping of contextual word embeddings. The proposed framework can be
applied to most of the recent neural language models, such as ELMo, BERT and their variants. See
Table 1 for a summary of the main notations used in this paper.
3.1 Sense-aware cross entropy loss
2
Under review as a conference paper at ICLR 2021
Limitations of original training objectives
The training tasks with Eq. 1 and2 maximize the
normalized dot product of contextual represen-
tations (hk-1,L or hk,L) with a weight vector
wk0 . The only difference is that hk-1,L in Eq.
1 encodes the information of previous tokens in
the sequence, while hk,L in Eq. 2 encodes the
information of the masked sequence. Therefore,
without loss of generality, We use hk*,L to de-
note the contextual representation for predicting
the next or masked token tk.
Even though contextual language models like
ELMo and BERT provide a different token rep-
resentation for each distinct context, the learned
representations are not guaranteed to be sense
separated. For example, Schuster et al. (2019)
computed the average of ELMo embeddings for
each word as an anchor, and found that the av-
Table 1: Summary of the main notations
Notation Description
tk	k-th token in sentence
tk,s	s-th sense of tk
k0	index of token tk in vocabulary
L	number of LSTM/Transformer layers
V	size of vocabulary
S	maximum number of senses per token
hk,j	contextual representation of token tk in layer j
hk*,L	contextual representation used in softmax
function for predicting tk
vi	i-th word in vocabulary
vi,s	s-th sense of vi
wi	output embedding of vi
wi,s	context-dependent output embedding
(i.e. sense vector) of vi,s
ci,s	sense cluster center of vi,s
Ci	sense cluster centers of vi
d	dimension of contextual representations
P	projection matrix for dimension reduction
erage cosine distance between contextual em-
beddings of multi-sense words and their corresponding anchors are much smaller than the average
distance between anchors, which mean that the embeddings of different senses of one word are
relatively near to each other comparing to that of different words. We also observed the same with
BERT embeddings. This finding suggests that sense clusters of a multi-sense word’s appearances are
not well separated in the embedding space, and the current contextual language models still have
room for improvement by considering finer-grained word sense disambiguation.
Notice that there is only one weight vector wk0 for predicting the token tk in the original training
tasks. Ideally, we should treat the appearances of a multi-sense word in different contexts as different
tokens, and train the language models to predict different senses of the word. In the following, we
propose a novel sense-aware cross entropy loss to explicitly model different senses of a word in
different contexts.
Sense-aware cross entropy loss Given a sequence (t1, t2, . . . , tN), our proposed framework gen-
erates contextual representations (hk,j for token tk in layer j ∈ {1, . . . , L}) in the same way as the
standard LMs. Different from existing methods, our approach maintains multiple context-dependent
output embeddings (henceforth, sense vectors) for each token. Specifically, let S be the maximum
number of senses per token. Each word vi in the vocabulary contains S separate sense vectors
(wi,1, wi,2, . . . , wi,S), where each wi,s corresponds to a different sense (see Appendix for some
interesting visualization examples). Following the notation in Section 2, we use k0 to denote the
index of the output token tk in the vocabulary. Therefore, the sense vectors of tk can be represented
by (wk0,1, wk0,2, . . . , wk0,S), which are randomly initialized and of the same dimension as hk*,L.
Note that we untie the input and output embeddings in our framework.
We propose a word sense selection method shown in Algorithm 1 to select the most likely sense vector
when training with sense-level cross entropy loss. Figure 1 shows the architecture of our proposed
models. Assuming sense s0 is selected for token tk (which means sense vector wk0,s0 should be used),
we have the following new prediction task:
p(tk,s0 |context) = softmax(hkT* ,Lwk0,s0)
exp(hτ*,LWko,sθ)
Pi=1 Ps=1 exP(hT*,Lwi,s)
The sense-aware cross entropy loss for word sense prediction is defined as follows:
LSENSE = - log(p(tk,s0 |context))
(3)
(4)
Word sense selection algorithm Word sense selection when training the language model can be
handled as a non-stationary data stream clustering problem (Aggarwal et al., 2004; Khalilian &
Mustapha, 2010; Abdullatif et al., 2018). The most intuitive way to select the corresponding sense
1Since the backward language model is similar to the forward, we only show the forward one for simplicity.
3
Under review as a conference paper at ICLR 2021
(a) Sense-aware next token prediction
Figure 1: Our proposed framework for sense-aware next token1and masked token prediction tasks.
Figure (c) shows an example of word sense selection, where the two sense clusters of tk (assume its
vocabulary index is k0) are shifting in space. Center vectors ck0,1 and ck0 ,2 are used to locate cluster
centers. Given hk,L, the algorithm performs dimension reduction on both hk,L and center vectors,
and then finds the most close cluster center ck0 ,2, so we know the output embedding corresponding to
sense 2 (wk0 ,2) should be used in the loss function. ck0 ,2 also makes a small step towards hk,L .
(b) Sense-aware masked token prediction
(c) Word sense selection
vector for hk*,L is to select the vector wk，，s With the maximum dot product value h(* Lwk，,s, or
cosine similarity value Cossim(hk* ,L, wk，,s). However, our experiments show that these methods do
not Work Well due to curse of dimensionality, suboptimal learning rate and noisy hk*,L. We apply an
online k-means algorithm to cluster different senses of a word in Algorithm 1. For each sense vector
wi,s, we maintain a cluster center ci,s which is of the same dimension as wi,s. Therefore, each token
vi in the vocabulary has S such cluster center vectors, denoted by Ci = (ci,1 , ci,2, . . . , ci,S). When
predicting token tk in a given sequence, we apply Algorithm 1 to select the best sense vector based
on hk,L (see Figure 1). Notice that hk,L is different from hk*,L for next token prediction (Figure 1a)
for which hk*,L = hk-1,L. The cluster centers Ci are not neural network parameters; instead, they
are randomly initialized using a normal distribution N(0, σ2) and updated through Algorithm 1. In
addition, we also maintain a projection matrix P for dimension reduction to facilitate effective sense
clustering. P ∈ Rd×d0 projects hk,L and ci,s from dimension d to d0, and is shared by all tokens in
vocabulary. Similar to C, P is also randomly initialized with normal distribution N(0, 1), and then
updated through Algorithm 2. Both Algorithm 1 and 2 run in parallel, and are interrupted when the
language model stops training.
Some rationales behind our algorithm design are the following:
Algorithm 1 Word sense selection		Algorithm 2 Projection matrix P update	
1:	Hyper-parameters: number of senses S,	1:	Hyper-parameters: projection dimension d0 , up-
	sense learning rate α		date interval M, queue size Q
2:	Initialize the set of all sense cluster centers C	2:	Initialize P with N(0,1), queue H = 0, m = 0
3:	repeat	3:	repeat
4:	input: hk,L, vocabulary index k0 of the to-	4:	input: hk,L
	ken to predict	5:	m=m+1
5:	Lookup sense cluster centers for k0 : Ck， =	6:	Add hk,L to queue H
	{ck，,1 , ck，,2, . . . , ck，,S}	7:	if size(H) > Q then
6:	P = updated projection matrix from Alg. 2	8:	Pop the oldest element from queue H .
7:	if cosine similarity between ck，,s，P and	9:	end if
	h0kP is the largest among the vectors in Ck，	10:	if m >= M then
	then	11:	P = the first d0 PCA components of H
8:	ck0,s0 =(I - α)ck0 ,s0 + αhk,L	12:	m=0
9:	output: s0(wk，,s， should be selected)	13:	end if
10:	end if	14:	output: P
11:	until interrupted	15:	until interrupted
4
Under review as a conference paper at ICLR 2021
•	Directly computing cosine similarity between ck0,s and hk,L suffers from the curse of dimension-
ality. We maintain P for dimension reduction. Although many algorithms use random projection
for dimension reduction, we find using PCA components can help improve clustering accuracy.
•	Since the neural model parameters keep being updated during training, the sense clusters become
non-stationary, i.e., their locations keep changing. Experiments shows that when using P for
dimension reduction, a slightly larger projection dimension d0 will make the clustering algorithm
less sensitive to cluster location change. We use d0 = 16 for ELMo, and d0 = 14 for BERT. We
also notice that the sense clustering works well even if P is updated sporadically. We can set a
relatively large update interval in Algorithm 2 to reduce computation cost.
•	A separate sense learning rate α should be set for the clustering algorithm. A large α makes the
algorithm less robust to noise, while a small α leads to slow convergence.
•	It is essential to use the current token’s contextual representation hk,L for sense selection even
though We use hk*,L = hk-ι,L in the next token prediction task. If We use hk-ι,L for sense
selection, experiments show that most of the variance comes from input embedding xk-1. This
introduces too much noise for Word sense clustering.
Dynamic pruning of redundant word senses To make the training more efficient, We keep track of
relative sense selection frequency for each token in the vocabulary. Assume token vi has initial senses
(vi,1, vi,2, . . . , vi,S), for Which We compute the relative frequency ρ(vi,s) such that 0 ≤ ρ(vi,s) ≤ 1
and Ps ρ(vi,s) = 1. A loWer ρ(vi,s ) means the sense is less frequently selected compared With
others. We check the relative frequencies after every E training steps, and if ρ(vi,s) < β (a threshold
hyper-parameter), vi,s is removed from the list of senses of vi.
Remark on model size and parameters The sense cluster centers C and the projection matrix
P are only used to facilitate sense selection during model pretraining, Which are not neural model
parameters. The sense vectors wi,s Will no longer be used after pretraining, Which can also be
discarded. Therefore, our models and the original models have exactly the same number of parameters
When transferred to doWnstream tasks.
Remark on model complexity The computational complexity of our algorithm is linear With
respect to the size of data, so our method is scalable to train on very large datasets.
3.2 Joint training with sense level translation
Training language model With sense-aWare cross entropy loss helps to learn contextual token repre-
sentations that are sufficiently distinct for different senses (§4.1). In this subsection, We extend it to
cross-lingual settings and present a novel approach to learn cross-lingual contextual Word embeddings
at the sense level. Our approach uses a bilingual seed dictionary,2 and can be applied to both next and
masked token prediction tasks.
For training the cross-lingual LM, We concatenate the (non-parallel) corpora of tWo languages, L1
and L2, and construct a joint vocabulary O = OL1 ∪ OL2, Where OL1 and OL2 are the vocabularies
of L1 and L2, respectively. Algorithm 1 is used to model the senses of tokens in the joint vocabulary.
In addition to predicting the correct monolingual sense p(tk,s0 |context) in Eq. 3, We also train the
model to predict its sense level translation. Let vj be the translation of tk and sense vj,s* of vj be
the best sense level translation under the given context, We add the folloWing sense-level translation
prediction task to maximize probability of vj,s* .
p(vj,s* |context) = softmax(hkT* ,Lwj,s* )
exP(hT*,Lwj,s* )
PV=I PS=I exP(hT*,Lwi,s)
(5)
Where wj,s* is the corresponding sense vector of vj,s* .
Similar to the previous subsection, We maintain sense cluster centers Ci for each token vi ∈ O and
the shared projection matrix P to select the best translation sense. Assume tk has T translations in
dictionary, and each translation has S senses, then there are T × S possible sense level translations
for tk in the given context. If the cossim(hk,LP, cj,s* P) value is the largest among the T × S
sense cluster centers, then We select vj,s* as the closest translation. An example is shoWn in Figure 2.
2If not provided, it can be learned in an unsupervised Way, e.g., MUSE (Conneau et al., 2017).
5
Under review as a conference paper at ICLR 2021
Figure 2: An example of English-Japanese sense-level joint training, which shows two possible
Japanese translations (至艮行 and 岸)of the English word bank. hk,L is a contextual representation of
bank in finance context and ck0,2 is the cluster center for this sense. ca,1, ca,2, cb,1, cb,2 are different
sense cluster centers of the two Japanese translations, among which cb,2 is the closest to hk,L after
dimension reduction through PCA. Our sense level objective (Eq. 6) moves sense clusters for bank
(organization) and 至艮行(organization) closer to each other.
If token tk has at least one translation in the dictionary, the translation cross entropy loss can be
computed as:
LTRAN = - Iog(P(Vj,s* |context))	(6)
If token tk has no translation in the seed dictionary, we use Eq. 4 as the only loss. The joint training
loss is defined as follows:
LSENSE +LTRAN
LJOINT =	2
LSENSE,
if tk has translations
otherwise
(7)
Further alignment (optional) Our sense-aware pretraining tries to move similar senses of two
different languages close to each other as illustrated in Figure 2. This process makes the sense
distributions of the two languages more isomorphic (some sense vector visualization examples are
shown in Appendix C). Applying the linear projection approach proposed by Schuster et al. (2019) on
top of the language model pretrained with our framework can further improve cross-lingual transfer
on some tasks. See Appendix B for more details of our implementation.
4	Experiments
4.1	Experiments using monolingual models
To verify the effectiveness of our proposed sense-aware cross entropy loss, we implement the
monolingual models on top of ELMo and BERT with the changes described in §3.1, which are named
SaELMo (Sense-aware ELMo) and SaBERT (Sense-aware BERT) respectively. The algorithm for
dynamic pruning of redundant word senses is optional, which is implemented on SaELMo only.
Pretraining settings We use the one billion word language modeling benchmark data (Chelba
et al., 2013) to pretrain all the monolingual models. The corpus is preprocessed with the provided
scripts, and then converted to lowercase. We do not apply any subword tokenization. We use similar
hyper-parameters as Peters et al. (2018) to train the ELMo and SaELMo models, and similar hyper-
parameters as Devlin et al. (2018) to train 4-layer BERT-Tiny and SaBERT-Tiny. Next sentence
prediction task is disabled in BERT-Tiny and SaBERT-Tiny, since this task is irrelevant to our
proposed changes. See Appendix D.1 for a complete list of hyper-parameters.
Word sense disambiguation (WSD) Since our context-aware cross entropy loss is designed to
learn word senses better in the context, we first conduct experiments to compare our monolingual
model with the original models on the WSD task (Raganato et al., 2017), which is a task to associate
words in context with the most suitable entry in a pre-defined sense inventory. We use a similar
framework as Peters et al. (2018) to evaluate the monolingual models.3 We use SemCor 3.0 (Miller
et al., 1993) as training data, and Senseval/SemEval series (Edmonds & Cotton, 2001; Moro &
Navigli, 2015; Navigli et al., 2013; Pradhan et al., 2007; Snyder & Palmer, 2004) as test data. We use
the pretrained models to compute the average of contextual representations for each sense in training
data, and then classify the senses of the target words in test sentences by finding the nearest neighbour.
3We modified the script from: https://github.com/drgriffis/ELMo-WSD-reimplementation.git
6
Under review as a conference paper at ICLR 2021
WSD results are presented in Table 2. SaELMo shows significant performance improvements over
the baseline ELMo model in all of the five test sets. SaBERT-Tiny also outperforms BERT-Tiny
except on SE07, which is the smallest among the five test sets.
Table 2: Word sense disambiguation (F1 scores)
Model	SE2	SE3	SE07	SE13	SE15
ELMo	0.555	0.576	0.446	0.544	0.538
SaELMo (ours)	0.575	0.586	0.470	0.560	0.583
BERT-Tiny	0.596	0.539	0.466	0.536	0.572
SaBERT-Tiny (ours)	0.611	0.546	0.446	0.550	0.579
4.2	Experiments using bilingual models
To verify the effectiveness of our cross-lingual framework, we implement the bilingual models on
top of ELMo, named Bi-SaELMo that does not use linear projection for further alignment and
Bi-SaELMo+Proj that uses the linear projection. Sense vectors and cluster center vectors are
not shared between the forward and backward language models. We use ELMo+Proj and Joint-
ELMo+Proj as our baseline models, where ELMo+Proj is proposed by Schuster et al. (2019) and
Joint-ELMo+Proj is implemented following the framework recently proposed by Wang et al. (2020).
Wang et al. (2020) combine joint training and projection, and claim their framework is applicable to
any projection method, so we implement the same projection method as Schuster et al. (2019) did for
Joint-ELMo+Proj. We also report results of ELMo and Joint-ELMo, which are the counterparts of
ELMo+Proj and Joint-ELMo+Proj without using linear projection.
Pretraining settings To pretrain language models, we sample a 500-million-token corpus for
each language from the English, German, Spanish, Japanese and Chinese Wikipedia dump. The
dictionaries used for pretraining models and learning the projection matrix were downloaded from
the MUSE (Conneau et al., 2017) GitHub page4. We also add JMDict (Breen, 2004) to the en-jp
MUSE dictionary. Bilingual models were pretrained on en-de, en-es, en-jp and en-zh concatenated
data with similar parameters as the monolingual models. ELMo and ELMo+Proj were pretrained on
monolingual data, while the projection matrix of ELMo+Proj was learned using bilingual data. See
Appendix D.2 for a complete list of hyper-parameters.
Zero-shot cross-lingual NER A Bi-
LSTM-CRF model implemented with the
Flair framework (Akbik et al., 2018) is used
for this task. For the CoNLL-2002 (Tjong
Kim Sang, 2002) and CoNLL-2003 (Sang
& De Meulder, 2003) datasets, the NER
model was trained on English data, and
evaluated on Spanish and German test data.
For the OntoNotes 5.0 (Weischedel et al.,
2013) dataset, the NER model was trained
on all English data and evaluated on all Chi-
nese data. We report the average F1 of 5
runs in Table 3. The results show that all
of the models using linear projection out-
perform their counterparts (not using linear
projection), since minimizing token level
distance is more important for cross-lingual
Table 3: Zero-shot cross-lingual NER (F1)
Model	de	es	zh
ELMo	16.30	16.14	0.28
Joint-ELMo	56.49	58.91	53.47
ELMo+Proj (Schuster et al., 2019)	69.57	60.02	63.15
Joint-ELMo+Proj (Wang et al., 2020)	71.59	65.19	59.08
Bi-SaELMo (ours)	63.83	60.65	55.83
Bi-SaELMo+Proj (ours)	72.19	65.86	63.44
For references, but not our baselines, since they are		trained on	much
larger datasets and/or parallel sentences.			
XLM Finetune (ConneaU & Lample, 2019)	67.55	63.18	-
XLM-R Finetune (Conneau et al., 2019)	71.40	78.64	-
M-BERT Finetune (Pires et al., 2019)	69.74	73.59	-
M-BERT Finetune (WU & Dredze, 2019)	69.56	74.96	-
M-BERT Finetune+Adv (Keung et al., 2019)	71.90	74.30	-
M-BERT Feature+Proj (Wang et al., 2020)	70.54	75.77	-
NER tasks. Our sense-aware pretraining makes sense distributions of two languages more isomorphic,
which further improves linear projection performance. Our model Bi-SaELMo+Proj demonstrates
consistent performance improvement in all the three languages. Moreover, our model outperforms
finetuned XLM/XLM-R and Multilingual BERT on German data, and achieves state of the art even
though it is pretrained on less data.
4https://github.com/facebookresearch/MUSE
7
Under review as a conference paper at ICLR 2021
Zero-shot cross-lingual sentiment classification We use the multi-lingual multi-domain Amazon
review data (Prettenhofer & Stein, 2010) for evaluation on cross-lingual sentiment classification. The
ratings in review data are converted into binary labels. The average of contextual word representations
is used as the document/sentence representation for each review text/summary, which is then fed
into a two-dense-layer model for sentiment classification. All the models are trained on English, and
evaluated on German and Japanese test data in the same domain. We report the average accuracy
of 5 runs in Table 4. Different from the NER task, the linear projection approach for cross-lingual
alignment does not work for this task, since it may add noise to embedding features. Our model
Bi-SaELMo demonstrates consistent improvements in all of the 6 evaluation tasks. The performance
of Bi-SaELMo is significantly better than Joint-ELMo, which shows that our sense-level translation
pretraining objective improves cross-lingual embedding alignment.
Table 4: Zero-shot sentiment classification accuracy
Model	de			jp		
	books	music	dvd	books	music	dvd
ELMo	52.94	63.61	57.78	50.37	51.59	54.32
Joint-ELMo	71.72	75.22	64.25	66.64	68.50	58.54
ELMo+Proj (Schuster et al., 2019)	49.92	50.29	49.94	50.57	49.59	50.65
Joint-ELMo+Proj (Wang et al., 2020)	75.74	72.25	72.25	62.50	59.77	57.65
Bi-SaELMo (ours)	77.46	75.32	74.97	68.16	69.48	64.04
Bi-SaELMo+Proj (ours)	70.84	66.25	68.99	62.17	55.91	61.57
Table 5: Zero-shot XNLI accuracy
Model	de	es	zh
ELMo	34.07	33.41	35.77
Joint-ELMo	60.12	63.73	57.82
ELMo+Proj (Schuster et al., 2019)	55.51	58.92	53.17
Joint-ELMo+Proj (Wang et al., 2020)	63.33	64.71	58.34
Bi-SaELMo (ours)	60.98	62.75	60.40
Bi-SaELMo+Proj (ours)	64.77	65.05	60.44
Zero-shot cross-lingual natural language inference (XNLI) We use XNLI (Conneau et al., 2018)
and MultiNLI (Williams et al., 2018) data for evaluation on this task. The Bi-LSTM baseline model5
was trained on MultiNLI English training data, and then evaluated on XNLI German, Spanish,
Chinese test data. We report the average zero-shot XNLI accuracy of 2 runs in Table 5. Our models
show consistent improvements over the baselines on all of the three data sets. For zero-shot transfer
to Chinese, both of our models outperform the best baseline by more than 2 points, which again
demonstrates the effectiveness of our framework on distant language pairs.
5	Related work
Cross-lingual word embedding demonstrates strong performance in many cross-lingual transfer
tasks. The projection-based approach has a long line of research on aligning static embeddings
(Mikolov et al., 2013; Xing et al., 2015; Smith et al., 2017; Joulin et al., 2018). It assumes that the
embedding spaces of different languages have an isomorphic structure, and fit an orthogonal matrix
to project multiple monolingual embedding spaces to a shared space. Recent studies (Schuster et al.,
2019; Aldarmaki & Diab, 2019) have extended this approach to contextual representation alignment.
Besides, there are also many discussions on the limitations of the projection-based approach, arguing
that the isomorphic assumption is not true in general (Nakashole & Flauger, 2018; Patra et al., 2018;
S0gaard et al., 2018; Ormazabal et al., 2019). Joint training is another line of research and early
methods (Gouws et al., 2015; Luong et al., 2015; Ammar et al., 2016) learn static word embeddings
of multiple languages simultaneously. Extending joint training to cross- or multi-lingual language
model pretraining has gained more attention recently. As discussed above, unsupervised multilingual
language models (Devlin et al., 2018; Artetxe & Schwenk, 2019; Conneau & Lample, 2019; Conneau
et al., 2019; Liu et al., 2020) also demonstrate strong cross-lingual transfer performance.
There has been some work on sense-aware language models/embeddings (Rothe & Schutze, 2015;
Pilehvar & Collier, 2016; Hedderich et al., 2019), and most of them require WordNet (Miller, 1998) or
other additional resource for supervision. Suster et al. (2016) utilize both monolingual and bilingual
information from parallel corpora to learn multi-sense word embeddings. Peters et al. (2019) embed
WordNet knowledge into BERT with attention mechanism. Levine et al. (2019) pretrain SenseBERT
to predict both the masked words and their WordNet supersenses. Similar to our framework, there are
also some unsupervised approaches, but most of them are used to learn static embeddings. Huang
et al. (2012) learn word representations with both local and global context, and then apply a clustering
algorithm to learn multi-prototype vectors. Neelakantan et al. (2014) propose an extension to the
Skip-gram model that leverage k-means clustering algorithm learns multiple embeddings per word
type. Lee & Chen (2017) leverage reinforcement learning for modularized unsupervised sense level
embedding learning. Boyd-Graber et al. (2020) use Gumbel softmax for sense disambiguation when
learning sense embeddings.
5https://github.com/NYU-MLL/multiNLI
8
Under review as a conference paper at ICLR 2021
6	Conclusions
In this paper, we have introduced a novel sense-aware cross entropy loss to model word senses
explicitly, then we have further proposed a sense-level alignment objective for cross-lingual model
pretraining using only bilingual dictionaries. The results of the experiments show the effectiveness of
our monolingual and bilingual models on WSD, zero-shot cross-lingual NER, sentiment classification
and XNLI tasks. In future work, we will study how to effectively extend our method to multilingual
models. In addition, using the sense cluster centers to learn the linear projection matrix would be
another promising direction to further improve cross-lingual alignment.
References
Amr Abdullatif, Francesco Masulli, and Stefano Rovetta. Clustering of nonstationary data streams: A
survey of fuzzy partitional methods. Wiley Interdisciplinary Reviews: Data Mining and Knowledge
Discovery, 8(4):e1258, 2018.
Charu C Aggarwal, Jiawei Han, Jianyong Wang, and Philip S Yu. A framework for projected clustering
of high dimensional data streams. In Proceedings of the Thirtieth international conference on Very
large data bases-Volume 30, pp. 852-863, 2004.
Alan Akbik, Duncan Blythe, and Roland Vollgraf. Contextual string embeddings for sequence
labeling. In COLING 2018, 27th International Conference on Computational Linguistics, pp.
1638-1649, 2018.
Hanan Aldarmaki and Mona Diab. Context-aware crosslingual mapping. arXiv preprint
arXiv:1903.03243, 2019.
Waleed Ammar, George Mulcaire, Yulia Tsvetkov, Guillaume Lample, Chris Dyer, and Noah A
Smith. Massively multilingual word embeddings. arXiv preprint arXiv:1602.01925, 2016.
Mikel Artetxe and Holger Schwenk. Massively multilingual sentence embeddings for zero-shot
cross-lingual transfer and beyond. Transactions of the Association for Computational Linguistics,
7:597-610, Mar 2019. ISSN 2307-387X. doi:10.1162/tacLa_00288. URL http://dx.doi.
org/10.1162/tacl_a_00288.
Jordan Boyd-Graber, Fenfei Guo, Leah Findlater, and Mohit Iyyer. Which evaluations uncover sense
representations that actually make sense? In Proceedings of the 12th Language Resources and Eval-
uation Conference, pp. 1727-1738, Marseille, France, May 2020. European Language Resources
Association. ISBN 979-10-95546-34-4. URL https://www.aclweb.org/anthology/
2020.lrec-1.214.
Jim Breen. Jmdict: a japanese-multilingual dictionary. In Proceedings of the workshop on multilingual
linguistic resources, pp. 65-72, 2004.
Steven Cao, Nikita Kitaev, and Dan Klein. Multilingual alignment of contextual word representations.
arXiv preprint arXiv:2002.03518, 2020.
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony
Robinson. One billion word benchmark for measuring progress in statistical language modeling,
2013.
Alexis Conneau and Guillaume Lample. Cross-lingual language model pretraining. In Advances in
Neural Information Processing Systems, pp. 7057-7067, 2019.
Alexis Conneau, GUillaUme Lample, Marc'Aurelio Ranzato, Ludovic Denoyer, and Herve Jegou.
Word translation without parallel data. arXiv preprint arXiv:1710.04087, 2017.
Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel R. Bowman, Holger
Schwenk, and Veselin Stoyanov. Xnli: Evaluating cross-lingual sentence representations. In
Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.
Association for Computational Linguistics, 2018.
9
Under review as a conference paper at ICLR 2021
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Fran-
cisco Guzman, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. UnsuPervised
cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116, 2019.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deeP
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
PhiliP Edmonds and Scott Cotton. Senseval-2: overview. In Proceedings of SENSEVAL-2 Second
International Workshop on Evaluating Word Sense Disambiguation Systems, pp. 1-5, 2001.
StePhan Gouws, Yoshua Bengio, and Greg Corrado. Bilbowa: Fast bilingual distributed rePresenta-
tions without word alignments. In Proceedings of the 32nd International Conference on Machine
Learning, 2015.
Michael A. Hedderich, Andrew Yates, Dietrich Klakow, and Gerard de Melo. Using multi-sense
vector embeddings for reverse dictionaries. In Proceedings of the 13th International Conference
on Computational Semantics - Long Papers, pp. 247-258, Gothenburg, Sweden, May 2019.
Association for Computational Linguistics. doi: 10.18653/v1/W19-0421. URL https://www.
aclweb.org/anthology/W19-0421.
Eric Huang, Richard Socher, Christopher Manning, and Andrew Ng. Improving word representations
via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long Papers), pp. 873-882, Jeju Island,
Korea, July 2012. Association for Computational Linguistics. URL https://www.aclweb.
org/anthology/P12-1092.
Armand Joulin, Piotr BojanoWski, Tomas Mikolov, HerVe Jegou, and Edouard Grave. Loss in transla-
tion: Learning bilingual word mapping with a retrieval criterion. arXiv preprint arXiv:1804.07745,
2018.
Phillip Keung, Yichao Lu, and Vikas BhardWaj. Adversarial learning With contextual embeddings for
zero-resource cross-lingual classification and ner. arXiv preprint arXiv:1909.00153, 2019.
Madjid Khalilian and NorWati Mustapha. Data stream clustering: Challenges and issues. arXiv
preprint arXiv:1006.5261, 2010.
Guang-He Lee and Yun-Nung Chen. MUSE: Modularizing unsupervised sense embeddings. In
Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp.
327-337, Copenhagen, Denmark, September 2017. Association for Computational Linguistics.
doi: 10.18653/v1/D17-1034. URL https://www.aclweb.org/anthology/D17-1034.
Yoav Levine, Barak Lenz, Or Dagan, Dan Padnos, Or Sharir, Shai Shalev-ShWartz, Amnon Shashua,
and Yoav Shoham. Sensebert: Driving some sense into bert. arXiv preprint arXiv:1908.05646,
2019.
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike LeWis,
and Luke Zettlemoyer. Multilingual denoising pre-training for neural machine translation, 2020.
Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Bilingual Word representations With
monolingual quality in mind. In Proceedings of the 1st Workshop on Vector Space Modeling for
Natural Language Processing, pp. 151-159, 2015.
Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David
McClosky. The Stanford CoreNLP natural language processing toolkit. In Association for
Computational Linguistics (ACL) System Demonstrations, pp. 55-60, 2014. URL http://www.
aclweb.org/anthology/P/P14/P14-5010.
Tomas Mikolov, Quoc V Le, and Ilya Sutskever. Exploiting similarities among languages for machine
translation. arXiv preprint arXiv:1309.4168, 2013.
George A Miller. WordNet: An electronic lexical database. MIT press, 1998.
10
Under review as a conference paper at ICLR 2021
George A Miller, Claudia Leacock, Randee Tengi, and Ross T Bunker. A semantic concordance.
In Proceedings ofthe workshop on Human Language Technology, pp. 303-308. Association for
Computational Linguistics, 1993.
Andrea Moro and Roberto Navigli. Semeval-2015 task 13: Multilingual all-words sense disambigua-
tion and entity linking. In Proceedings of the 9th international workshop on semantic evaluation
(SemEval 2015), pp. 288-297, 2015.
Ndapa Nakashole and Raphael Flauger. Characterizing departures from linearity in word translation.
arXiv preprint arXiv:1806.04508, 2018.
Roberto Navigli, David Jurgens, and Daniele Vannella. Semeval-2013 task 12: Multilingual word
sense disambiguation. In Second Joint Conference on Lexical and Computational Semantics (*
SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation
(SemEval 2013), pp. 222-231, 2013.
Arvind Neelakantan, Jeevan Shankar, Alexandre Passos, and Andrew McCallum. Efficient non-
parametric estimation of multiple embeddings per word in vector space. In Proceedings of the 2014
Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1059-1069,
Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1113.
URL https://www.aclweb.org/anthology/D14- 1113.
Aitor Ormazabal, Mikel Artetxe, Gorka Labaka, Aitor Soroa, and Eneko Agirre. Analyzing the
limitations of cross-lingual word embedding mappings. arXiv preprint arXiv:1906.05407, 2019.
Barun Patra, Joel Ruben Antony Moniz, Sarthak Garg, Matthew R Gormley, and Graham Neubig.
Bliss in non-isometric embedding spaces. 2018.
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and
Luke Zettlemoyer. Deep contextualized word representations. In Proc. of NAACL, 2018.
Matthew E Peters, Mark Neumann, Robert L Logan IV, Roy Schwartz, Vidur Joshi, Sameer Singh,
and Noah A Smith. Knowledge enhanced contextual word representations. arXiv preprint
arXiv:1909.04164, 2019.
Mohammad Taher Pilehvar and Nigel Collier. De-conflated semantic representations. In Proceedings
of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 1680-1690,
Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/
D16-1174. URL https://www.aclweb.org/anthology/D16-1174.
Telmo Pires, Eva Schlinger, and Dan Garrette. How multilingual is multilingual bert? arXiv preprint
arXiv:1906.01502, 2019.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and Martha Palmer. Semeval-2007 task-17:
English lexical sample, srl and all words. In Proceedings of the fourth international workshop on
semantic evaluations (SemEval-2007), pp. 87-92, 2007.
Peter Prettenhofer and Benno Stein. Cross-language text classification using structural correspondence
learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Lin-
guistics, pp. 1118-1127, Uppsala, Sweden, July 2010. Association for Computational Linguistics.
URL https://www.aclweb.org/anthology/P10- 1114.
Alessandro Raganato, Jose Camacho-Collados, and Roberto Navigli. Word sense disambiguation: A
unified evaluation framework and empirical comparison. In Proceedings of the 15th Conference
of the European Chapter of the Association for Computational Linguistics: Volume 1, Long
Papers, pp. 99-110, Valencia, Spain, April 2017. Association for Computational Linguistics. URL
https://www.aclweb.org/anthology/E17-1010.
Sascha Rothe and Hinrich Schutze. AUtoExtend: Extending word embeddings to embeddings
for synsets and lexemes. In Proceedings of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International Joint Conference on Natural Language
Processing (Volume 1: Long Papers), pp. 1793-1803, Beijing, China, July 2015. Association for
Computational Linguistics. doi: 10.3115/v1/P15-1173. URL https://www.aclweb.org/
anthology/P15-1173.
11
Under review as a conference paper at ICLR 2021
Erik F Sang and Fien De Meulder. Introduction to the conll-2003 shared task: Language-independent
named entity recognition. arXiv preprint cs/0306050, 2003.
Tal Schuster, Ori Ram, Regina Barzilay, and Amir Globerson. Cross-lingual alignment of contextual
word embeddings, with applications to zero-shot dependency parsing. In Proceedings of the 2019
Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long and Short Papers), pp. 1599-1613, Minneapolis,
Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1162.
URL https://www.aclweb.org/anthology/N19- 1162.
Samuel L Smith, David HP Turban, Steven Hamblin, and Nils Y Hammerla. Offline bilingual word
vectors, orthogonal transformations and the inverted softmax. arXiv preprint arXiv:1702.03859,
2017.
Benjamin Snyder and Martha Palmer. The english all-words task. In Proceedings of SENSEVAL-3,
the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,
pp. 41-43, 2004.
Anders S0gaard, Sebastian Ruder, and Ivan VUliC. On the limitations of unsupervised bilingual
dictionary induction. arXiv preprint arXiv:1805.03620, 2018.
Simon Suster, Ivan Titov, and Gertjan Van Noord. Bilingual learning of multi-sense embeddings with
discrete autoencoders. arXiv preprint arXiv:1603.09128, 2016.
Erik F. Tjong Kim Sang. Introduction to the CoNLL-2002 shared task: Language-independent named
entity recognition. In COLING-02: The 6th Conference on Natural Language Learning 2002
(CoNLL-2002), 2002. URL https://www.aclweb.org/anthology/W02-2024.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Eukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Zirui Wang, Jiateng Xie, Ruochen Xu, Yiming Yang, Graham Neubig, and Jaime G. Carbonell.
Cross-lingual alignment vs joint training: A comparative study and a simple unified framework. In
International Conference on Learning Representations, 2020. URL https://openreview.
net/forum?id=S1l-C0NtwS.
Ralph Weischedel, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Pradhan, Lance Ramshaw,
Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, et al. Ontonotes release 5.0
ldc2013t19. Linguistic Data Consortium, Philadelphia, PA, 23, 2013.
Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for
sentence understanding through inference. In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technolo-
gies, Volume 1 (Long Papers), pp. 1112-1122. Association for Computational Linguistics, 2018.
URL http://aclweb.org/anthology/N18-1101.
Shijie Wu and Mark Dredze. Beto, bentz, becas: The surprising cross-lingual effectiveness of bert.
arXiv preprint arXiv:1904.09077, 2019.
Chao Xing, Dong Wang, Chao Liu, and Yiye Lin. Normalized word embedding and orthogonal trans-
form for bilingual word translation. In Proceedings of the 2015 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, pp.
1006-1011, 2015.
Zheng Zhang, Ruiqing Yin, Jun Zhu, and Pierre Zweigenbaum. Cross-lingual contextual word
embeddings mapping with multi-sense words in mind. arXiv preprint arXiv:1909.08681, 2019.
12
Under review as a conference paper at ICLR 2021
Appendix
A Prediction tasks of language models
Next token prediction and masked token prediction are two common tasks in neural language model
(LM) pretraining. We take two well-known language models, ELMo and BERT, as examples to
illustrate these two tasks, which are shown in Figure 3.
(a) Next token prediction
(b) Masked token prediction
Figure 3: Next token and masked token prediction tasks of language models. For simplicity, we only
show the forward language model in next token prediction.
B	Further alignment (optional)
Applying the linear projection approach proposed by Schuster et al. (2019) on top of our framework
can further improve cross-lingual transfer on some tasks. After our cross-lingual model is finished
training on the concatenated corpora of two languages, L1 and L2 , it is used to generate contextual
token embeddings for the word pairs in the seed dictionary D = {(tiL1 , tiL2)}|iD=|1 6. Then, we
compute the average of all contextual embeddings for each token tiLj , denoted by aiLj . Finally, a
linear projection matrix W ∈ Rd×d is learned to minimize cross-lingual embedding distance:
|D|
W = arg min X ||W aiL1 - aiL2 ||2
W i=1
(8)
C Visualization of sense vectors
We visualize7 the sense vectors of each model in a two dimensional PCA, and show some examples
in Figures 4 to 7. For our English monolingual model (SaELMo), the vectors close to two different
sense vectors of the word may are shown in (a) and (b) of Figure 4, respectively. We observe that
senses are well clustered in these two subfigures, where cluster (a) corresponds to “month”, and
cluster (b) corresponds to “auxiliary verb”.
We do the same for the English-Japanese bilingual model (Bi-SaELMo, without projection), and
show the vectors close to two different sense vectors of the English word bank in (c) and (d) of Figure
5. We can see both English and Japanese sense vectors (trade,至艮彳亍,1正券,etc.) in (c), most of which
correspond to the sense “organization”, though there are some noises. Similarly, most of the sense
vectors in (d) correspond to sense “river bank”.
6If any token tk appears in both languages, we add that as an entry (tk, tk) to the dictionary as well.
7We use the tensorflow embedding projector (https://projector.tensorflow.org/) for visualization.
13
Under review as a conference paper at ICLR 2021
Another two examples are shown in Figures 6 and 7. Our framework exhibits good sense clustering
and sense level cross-lingual alignment behaviour in these examples. All sense vectors are dumped at
training step 200,000, which is before pretraining complete.
HanUary_5
•july_4
?february_3
october_2
august_5	SePtembeJ5
j∏ovember_4
JUne_2
lmay_4 *aPril-4
• ojuly_5
cdecember_2
【march_3
november_2
•july_3
•I
IaPriL5
february_5
•may.5
rnay_2
*ja∏14
；march_1
,augj
mov_4
•march_2
∙july.2
?sept_5
»february_2
august_1
*april_3
(b) “may” as auxiliary verb
(a) “may” for month
Figure 4: We visualize sense vectors of English monolingual model (SaELMo) in a two dimensional
PCA, and show the vectors close to two different sense vectors of word may in (a) and (b).
(govern ment_2
∙trade2石油-1
«company_3
«company_1
»feb_1
(a)	“bank” for organization
(b)	“bank” for river bank
Figure 5:	We visualize all sense vectors of en-jp bilingual model (Bi-SaELMo) in a two dimensional
PCA, and show the vectors close to two different sense vectors of word bank.
14
Under review as a conference paper at ICLR 2021
ius_4
l<UNK>-Vs3.u.s._2«the.4
«usL3
>us_5
imr_4
IuniteCL2
•uk.2
•high_2
ibritish_2 «russian_2
®canadian_1
us_1
SfinanciaL5 , _
9	∙uk-5
•united_3
IWhite_3
«australian_2
∣swi ss_5
C «united_5
iu.s._3
iu.s._1
«u.s._5
CameriCan_4
«american_2
iiranian_4
globaL4
«mexican_1
jbritish_3
JnternationaLS
«government_3
federaL2
federaL3
3chinese_1
(a) “us” for country
(b) “us” as pronoun
Figure 6:	We visualize sense vectors of English monolingual model (SaELMo) in a two dimensional
PCA, and show the vectors close to two different sense vectors of word us in (a) and (b).
IjUne_3
december_1
∣ju∩L2
RWaniS_1
wurden_2
∙wi∣L1
»would_2
sollen_3
•must_3
>mussen_1
id&n_2
•must-1
?december_2
onovember_2
∣maL2 * ∙march_1
∙ju∣y-3
∙ju∣L3
∙k∂nnt1
fdarfjl
»darf_3
•moge_2
may」
∣moge-1
imay_2
IaPriL3
；could_2
ican_3
«august_1
∣maL3
«august_1
.november_2
C ∙sept,1
iaug_2 κ
imarz_2
ιmaL3
∣ju∣L1
∣apriL1
mov_1
2CtobeJl
ιjanuar_3
∙ju∏L1
augustus_2
mov_2
DmaLJ) Cmay_2
•june_2
imarch_3
∣maL2 ∙august_3
?september_2
:september_3
isept_2
>ju∣y-2
januar_2
oktober_1
gapriL2
∣november.1
:february_3
∣ju∣L2
∣may.3
december_3
(a) “may” as auxiliary verb
(b) “may” for month
Figure 7:	We visualize all sense vectors of en-de bilingual model (Bi-SaELMo) in a two dimensional
PCA, and show the vectors close to two different sense vectors of word may.
15
Under review as a conference paper at ICLR 2021
D Pretraining details
D.1 Monolingual model
All the monolingual models were trained for one million steps. For better sense clustering perfor-
mance, the maximum number of senses (S in word sense selection algorithm) was set to 1 for the
first 20,000 steps to quickly get a reasonable initial model, and then increased to 5 afterwards when
pretraining SaELMo and SaBERT-Tiny, which is controlled by hyperparameter n_context in our
implementation. For SaELMo, We set n_context to 6, so that the model initialize 6 senses for each
token, but only use the first sense in the 20,000 steps, and then use the other 5 senses (the first sense
will be disabled) afterwards. We implement this for SaBERT-Tiny in a slightly different way, where
n-context can be set to 5 directly to achieve the same effect. We use two NVIDIA V100 GPUS to
pretrain SaELMo, which takes about 15 days to complete training. We use one NVIDIA V100 GPU
to pretrain SaBERT-Tiny, which takes about 5 days. See Tables 6 and 7 for the hyperparameters used
to pretrain SaELMo and SaBERT-Tiny respectively.
Table 6: Monolingual model hyperparameters: SaELMo
Hyperparameter
Value
max_word_length
batchjsize
n_gpus
bidirectional
Char-Cnn:embedding:dim
Char-Cnn:maX-CharaCterS _per_token
Char-Cnn:n-CharaCterS
char_cnn:n_highway
dropout
lstm:CelLCliP
lstm:dim
lstm:n」ayers
lstm:Proj-CliP
lstm:projeCtion-dim
lstm:USe-SkiP-ConneCtionS
all_clip_norm_val
n_epochs
unroll-steps
n_negativejsamples_batch
n_context
cluster _proj_dim
PCa-SamPle
remove」ess_fTeqent-ConteXtS
learning_rate
SenseJearningjate
50
256
2
True
16
50
261
2
0.1
3
4096
2
3
512
True
10.0
10
16
8192
6
16
20,000
0.1
0.2
0.01
Table 7: Monolingual model hyperparameters: SaBERT-Tiny
Hyperparameter
Value
attention_probs_dropout_prob
direCtionality
hidden_act
hidden_dropout_prob
hiddenjsize
initializer_range
intermediate_size
max-position_embeddings
num_attention_heads
num_hidden_layers
PoolerJcjsize
pooler_num_attention_heads
pooler_num_fc Jayers
pooler jsize_per_head
pooler_type
type_VoCabjSize
VoCab-Size
n_context
ConteXILreP-Ir
pca_dim
ContextuaLwarmup
50
bidi
gelu
0.1
512
0.02
2048
512
8
4
512
8
3
128
first_token_transfbrm
2
27654
5
0.01
14
20,000
16
Under review as a conference paper at ICLR 2021
D.2 B ilingual model
As metioned in the paper, we use Wikipedia dump to pretrain the bilingual models. The Stanford
CoreNLP tokenizer (Manning et al., 2014) is used to tokenize English, German, Spanish and Chinese
data. And the spaCy tokenizer is used to tokenize Japanese data. All data are converted to lowercase.
We convert Chinese data to simplified font to make it consistent with evaluation task datasets.
All the language models used in cross-lingual experiments were pretrained for 600,000 steps from
scratch. Similar to our monolingual models, maximum number of senses (S in word sense selection
algorithm) was set to 1 for the first 20,000 steps, and the increased to 3 afterwards when pretraining
Bi-SaELMo and Bi-SaELMo+Proj.8 We use two NVIDIA V100 GPUs to pretrain each Bi-SaELMo
model, which takes about 10 days to complete the training. See Table 8 for the hyperparameters used
to pretrain Bi-SaELMo/Bi-SaELMo+Proj.
Table 8: Bilingual model hyperparameters: Bi-SaELMo/Bi-SaELMo+Proj
Hyperparameter	Value
max_word」ength	50
batch_size	256
n_gpus	2
bidirectional	True
ChaJcnn:embedding:dim	16
char_cnn:max_characters-PerJoken	50
char_cnn:n_characters	261
char_cnn:n_highWay	2
dropout	0.1
lstm:cell_clip	3
lstm:dim	4096
lstm:n_layers	2
lstm:proj_clip	3
lstm:prCjection_dim	512
lstm:use_skip_connections	True
all_clip_norm_val	10.0
n_epochs	6
UnrolLsteps	12
n_negative_samples_batch	8192
n_context	4
cluster_proj_dim	16
Pca-SamPle	20,000
remove JessfreqenUcontexts	0.1
learning_rate	0.2
sense_learning_rate	0.01
8Theoretically, in a reasonable range, it is expected that a larger S would be more helpful to capture the
fine-grained senses. However, due to limited computation power, we use only 3 here, and 5 for the monolingual
models.
17