Under review as a conference paper at ICLR 2021
Optimistic Policy Optimization with
General Function Approximations
Anonymous authors
Paper under double-blind review
Ab stract
Although policy optimization with neural networks has a track record of achieving
state-of-the-art results in reinforcement learning on various domains, the theoretical
understanding of the computational and sample efficiency of policy optimization
remains restricted to linear function approximations with finite-dimensional feature
representations, which hinders the design of principled, effective, and efficient
algorithms. To this end, we propose an optimistic model-based policy optimization
algorithm, which allows general function approximations while incorporating ex-
ploration. In the episodic setting, we establish a √T-regret that scales polynomially
in the eluder dimension of the general model class. Here T is the number of steps
taken by the agent. In particular, we specialize such a regret to handle two nonpara-
metric model classes; one based on reproducing kernel Hilbert spaces and another
based on overparameterized neural networks.
1	Introduction
Reinforcement learning with neural networks achieved impressive empirical breakthroughs (Mnih
et al., 2015; Silver et al., 2016; 2017; Berner et al., 2019; Vinyals et al., 2019). These algorithms
are often based on policy optimization (Williams, 1992; Baxter & Bartlett, 2000; Sutton et al.,
2000; Kakade, 2002; Schulman et al., 2015; 2017). Compared with value-based approaches, which
iteratively estimate the optimal value function, policy-based approaches directly optimize the expected
total reward, which leads to more steady policy improvement. In particular, as shown in this
paper, policy optimization generates steadily improving stochastic policies and consequently allow
adversarial environments.
On the other hand, policy optimization often suffers from a lack of computational and statistical
efficiency in practice, which calls for the principled design of efficient algorithms. Specifically, in
terms of computational efficiency, the recent progress (Abbasi-Yadkori et al., 2019a;b; Bhandari &
Russo, 2019; Liu et al., 2019; Agarwal et al., 2019; Wang et al., 2019) establishes the convergence
of policy optimization to a globally optimal policy given sufficiently many data points, even in the
presence of neural networks. However, in terms of sample efficiency, it remains less understood
how to sequentially acquire the data points used in policy optimization while balancing exploration
and exploitation, especially in the presence of neural networks, despite the recent progress (Cai
et al., 2019; Agarwal et al., 2020). In particular, such a lack of sample efficiency prohibits the
principled applications of policy optimization in critical domains, e.g., autonomous driving and
dynamic treatment, where data acquisition is expensive.
In this paper, we aim to provably achieve sample efficiency in model-based policy optimization, which
is quantified via the lens of regret. In particular, we focus on the episodic setting with general function
approximations on the transition kernel. Such a setting is studied by Russo & Van Roy (2013; 2014);
Osband & Van Roy (2014); Ayoub et al. (2020); Wang et al. (2020), which however focus on value
iteration. In contrast, policy optimization remains less understood, despite its critical role in practice.
To this end, we propose an optimistic policy optimization algorithm, which achieves exploration by
incorporating optimism into policy evaluation and propagating it through policy improvement. In
particular, we establish a K(P) ∙ VH3T-regret of the proposed algorithm, which matches that of
existing value iteration algorithms but additionally allow the reward function to adversarially vary
across each episode. Here T is the number of steps, H is the length of each episode, and κ(P) is
1
Under review as a conference paper at ICLR 2021
the model capacity, which is defined based on the eluder dimension. Moreover, we instantiate the
proposed algorithm for the special cases of reproducing kernel Hilbert spaces and overparameterized
neural networks, both of which are infinite-dimensional model classes.
Our work is related to the study on computational efficiency of policy optimization (Fazel et al.,
2018; Yang et al., 2019; Abbasi-Yadkori et al., 2019a;b; Bhandari & Russo, 2019; Liu et al., 2019;
Agarwal et al., 2019; Wang et al., 2019). These works assume either the transition model is known or
there exists a well-explored behavior policy such that the policy update direction can be estimated
accurately. With such assumptions, the tradeoff between exploration and exploitation is absent and
their focus is solely on the computational aspect. In addition, our work is related to the works
on adversarial MDP (Even-Dar et al., 2009; Yu et al., 2009; Neu et al., 2010a;b; Zimin & Neu,
2013; Neu et al., 2012; Rosenberg & Mansour, 2019b;a). The algorithm in these work directly
estimates the visitation measure and their algorithm utilize mirror descent to handle adversarial
reward functions. Furthermore, our work is closely related the recent work on the sample complexity
of policy optimization methods Cai et al. (2019), which only focus on the tabular and linear settings.
In contrast, our work consider general function approximation setting, which is significantly more
general. Moreover, the construction of optimistic policy evaluation is related to Ayoub et al. (2020),
where the similar approach is incorporated in estimating the optimal value function. The theoretical
foundation of such a type of optimistic estimation is innovated by Russo & Van Roy (2014) in
the bandit problem. In particular, to characterizing the optimism and accuracy of the optimistic
evaluation, we rely on the notion of the eluder dimension proposed by Russo & Van Roy (2014),
which is further instantiated by this paper to the cases of kernel and neural function approximations.
1.1	Notations
We denote by ∣∣ ∙ kp the 'p-norm of a vector when P ∈ N or the spectral norm of a matrix when P = 2.
For any two distributions p1,p2 over a discrete set A, we denote by DKL(p1 k p2) the KL-divergence
DKL(PI k p2) = X p1(a)log %.
a∈A
For any a, b, x ∈ R, we define the clamp function
(b, if x>b,
clamp(x, a, b) = x, if a ≤ x ≤ b,
(a, if x < a.
(1.1)
2	Preliminaries
2.1	Online Reinforcement Learning with Adversarial Rewards
We consider an episodic MDP (S, A, H, {Ph}hH=1, {rh}hH=1), where S is a continuous state space, A
is a discrete action space, H is the number of steps in each episode, {Ph}hH=1 represent the unknown
transition model, and {rh}hH=1 represent the reward function. In particular, for any h ∈ [H], Ph
represents the transition kernel from a state-action pair (sh, ah) at the h-th step to the next state sh+1,
while rh represents the reward function at the h-th step that maps a state-action pair to a deterministic
reward. Moreover, we allow the reward function to vary across episodes and denote by rhk the reward
function at the h-th step of the k-th episode. In particular, rhk depends on the trajectories before
the k-th episode begins, possibly in an adversarial manner, and remains unobservable until the k-th
episode ends. Without loss of generality, we assume each episode starts from a fixed state s1 and all
rewards fall in the interval [0, 1].
For any h ∈ [H], a policy πh represents the conditional distribution of the action given the state at
the h-th step. We drop the subscript h to represent the collection of policies at all steps and still refer
to such a collection as a policy when it is clear from the context. For any (k, h) ∈ N × [H], given
a policy π and reward functions {rhk}hH=1, the value function and Q-function at the h-th step of the
2
Under review as a conference paper at ICLR 2021
k-th episode are defined by
HH
Vhπ,k(s) =	Eπ	Xrjk(sj, aj)	sh	= s ,	Qhπ,k(s, a) = Eπ	Xrjk(sj, aj)	sh	= s,	ah	= a
j=h	j=h
for any (s, a) ∈ S ×A. Here the subscript ∏ in the expectation En [∙] denotes all actions are taken
according to the policy π except for the one given in the condition. An online algorithm aims to
construct and execute a sequence of policies {πk}k≥1 and minimize the regret
K
Regret(T) = maxX(Vf'k(si) - Vnk,k(sι)),	Q∙1)
π k=1
where K is the number of episodes and T = KH is the number of steps taken by the algorithm.
2.2	Reproducing Kernel Hilbert Space
We say H is a reproducing kernel Hilbert space (RKHS) on a set Y with the reproducing kernel
K : Y ×Y → R if there exists an inner product(•, ∙)h such that, for any f ∈ H and X ∈ Y, we have
f (χ) = hf, KXiH. Here KX represents the function K(χ, ∙), which is the Riesz representation of the
evaluation functional at X (Schoikopf et al., 2002). When the reproducing kernel K is continuous,
symmetric, and positive definite, Mercer’s theorem (Steinwart & Christmann, 2008) says K has the
representation
∞
K(X, y) =	λj φj (X)φj (y), for any X, y ∈ Y,	(2.2)
j=i
where {φj}∞=ι is an orthonormal basis of L2(Y) and λι ≥ λ2 ≥ ∙∙∙ ≥ 0. See more details on
RKHS in Section A.
3	Algorithm
Framework: Before the k-th episode begins, we construct the policy πk based on πk-i and
{Qkh-i}hH=i, which are the policy in the (k - 1)-th episode and estimators of {Qhnk-1,k-i}hH=i,
respectively. Then, we execute the policy πk in the k-th episode and correspondingly update the
Q-function estimators {Qkh}hH=i using the reward function {rhk}hH=i, which is observed after the k-th
episode ends.
Policy Improvement: For any (k, h) ∈ [K] × [H], we parametrize πhk by
πk (a |S)=P∈ASE(⅛,for any (s, a ∈S×A.
Here Ehk is the potential function, which is initialized as the zero function and updated by
Eh(s,a) = EhT(S,a) + α ∙ QhT(S,a).
Here α > 0 is the stepsize of policy improvement. Equivalently, we have
πk(∙ | s) (X πk-1(∙ | s) ∙ exp(α ∙ QhT(S, ∙))
for any S ∈ S. To see (3.2) is a policy improvement step, note that πhk is the maximizer of
Lh(∏h) = E∏k-1 [hQh-1(sh, ∙),∏h(∙ | Sh))A + α-i ∙ DKL(∏h(∙ | Sh)Ilnk-1(∙ | s%))].
(3.1)
(3.2)
3
Under review as a conference paper at ICLR 2021
This is the same as the update in Politex (Abbasi-Yadkori et al., 2019a), which originates at MDP-E
(Even-Dar et al., 2009). It is also very close to a slightly changed variant of the one-step objective in
the proximal policy optimization (PPO) algorithm (Schulman et al., 2017; 2015).
Policy Evaluation: Let P be a known class of transition models such that Ph ∈ P for any h ∈ [H],
which is specified in Section 4. Also, for any P ∈ P, s ∈ S, a ∈ A, and V : S → [0, H], we define
zP(s,a,V)
J V(s0) ∙ P(s0 | s,a) ds0.
(3.3)
For any (k, h) ∈ [K] × [H], we construct a confidence set of the transition model Ph and corre-
spondingly the optimistic Q-function estimator Qkh using the data collected before the k-th episode
begins. Note that we do not use the data collected from the k-th episode although they are available,
which, however, is only used to simplify the analysis. Let VHk+1 be the zero function. Inspired by
Ayoub et al. (2020), given the optimistic value function estimators {Vhτ+1}τk=-11 from the first (k - 1)
episodes, we construct the confidence set Phk of Ph by
k-1
Pk = {p ∈P∣ X(ZP (sh, ah, Vh+ι) - zpk(sh, ah,暗1 ))2 ≤ β},
τ=1
(3.4)
k-1
where Ph = argmin X(VT+1 (Sh+I)-ZP (Sh,ah,Vτ+1))2
P∈P τ=1
for a threshold β > 0, which represents the degree of optimism. Then, for any (S, a) ∈ S × A, given
the optimistic value function estimator Vhk+1, we define the optimistic Q-function estimator Qkh by
Qkh(S, a) = rhk(S, a) + max ZP (S, a, Vhk+1)	(3.5)
P ∈Phk
and correspondingly update the optimistic value function estimator by Vhk (S) = hQkh(S, ∙),πh(∙ | S)iA
for any S ∈ S. We apply the clamp function defined in (1.1) to the second term on the right-hand side
of (3.5) to ensure it falls in the range [0, H - h], which is due to the assumption that all rewards fall
in the range [0, 1].
Implementation: The full algorithm is presented in Algorithm 1. Given a parametrization of the
model class P, we can apply the projected stochastic gradient descent (PSGD) algorithm to solve
the constrained minimization problem in Line 11 of Algorithm 1. In particular, for kernel function
approximations in Section 4.1, it reduces to a convex optimization problem, which allows the PSGD
algorithm to converge to a global minimizer. Meanwhile, for neural function approximations, it
reduces to an approximately convex problem in the overparametrized regime (Arora et al., 2019),
which leads to the same global convergence guarantee. Also, to implement Lines 12 and 13 of
Algorithm 1, it suffices to solve a constrained maximization problem (Feng et al., 2020), where the
constraint is defined in Line 12. The Lagrangian relaxation of such a constrained maximization
problem can be solved by the PSGD algorithm in the same manner of Line 11. In addition, to
instantiate the update of Qkh in Line 13, it suffices to solve a least-squares regression problem. In
summary, we can instantiate the aforementioned steps through supervised learning oracles, which
can be implemented in a computationally efficient manner.
4 Theory
We analyze the regret of Algorithm 1, which is defined in (2.1). In Sections 4.1 and 4.2, we
characterize the regret with specific choices of the model class P, while in Section 4.3, we characterize
the regret for a general P, which serves as a meta result. An informal version of the results is given
in the following theorem.
4
Under review as a conference paper at ICLR 2021
Algorithm 1 Optimistic Policy Optimization with General Function Approximations
1:	Input: number of episodes K, model class P, stepsize a, threshold β
2:	Initialize π0 as the uniformly random policy
3:	For k = 1 to K do
4:	Start the k-th episode and receive the initial state s1k
5:	For step h = 1 to H do	(policy improvement)
6:	Update the policy by ∏h(∙ | S) H πk-1(∙ | s)exp{aQh-1 (s, ∙)} for any S ∈ S
7:	Take the action ah 〜∏k(∙ | Sh) and receive the next state slh+∖
8:	Observe the reward function {r£(∙, ∙)}H=ι
9:	Initialize VH+ι(∙) as the zero function
10:	For step h = H to 1 do	(policy evaluation)
11:	Ph J argminP∈P Pk=I(Vh+ι (Sh+1)- RS Vh+i (SO)P(S0 | sh, ah) ds0)2
12:
Phk J{P∈P
k-1
X( Vhτ+1(S0)P(S0 |
Sh, ah) dS0 — JS Vhτ+ι(S0)P⅛ (S01 Sh, ah) dS0)2 ≤ β}
13:	Qh(∙, ∙) - Th(∙, ∙) + clamp(maxp∈pk JS Vk+ι(S0)P(s0 | ∙, ∙) dS0, 0,H — h)
14:	Vhk(∙) JhQh(∙, ∙),∏k(∙∣∙)iA
Theorem 4.1 (Informal Version of Theorems 4.3, 4.7, and 4.12). With proper choices of α and β,
the regret of Algorithm 1 satisfies
Regret(T)=O(K(P) ∙ √H3T)
with high probability. Here O omits absolute constants and logarithmic factors of H, T, and |A|,
while κ(P) denotes the model capacity ofP, which is specified in Sections 4.1, 4.2, and 4.3.
Theorem 4.1 indicates that, compared with the optimal policy in hindsight, namely
argmaxπ PkK=1 V1π,k(S1), the average regret of Algorithm 1, namely Regret(T)/T, converges to
zero at a sublinear rate. In other words, at least one of the K policies attained by Algorithm 1 achieves
a vanishing optimality gap with respect to the varying reward function across the K episodes. The
model capacity κ(P) is specified in Sections 4.1 and 4.2 for kernel and neural function approx-
imations, respectively. To establish such specific results, we characterize κ(P) using the eluder
dimension for general function approximations in Section 4.3, which serves as the unified analysis.
4.1	Kernel Function Approximations
Let P be a subset of an RKHS H with the reproducing kernel K, which has the representation in
(2.2). In detail, let S be a measurable set with |S| ≤ 1, where | ∙ | denotes the Lebesgue measure.
With a slight abuse of notation, we denote by A the embedding of the action space into a Euclidean
space with the dimension |A|, where |A| denotes the number of actions. Meanwhile, let Y be a
dY -dimensional set such that S × A × S ⊂ Y. We assume there exists R ≥ 2 such that P ⊂ HR,
where HR is the RKHS ball over Y with the radius R.
Assumption 4.2. We assume K satisfies the following regularity conditions.
(i)	. It holds that |K(x, y)| ≤ 1, ∣φj(x)| ≤ 1, and λj ≤ 1 for any x,y ∈ Y and j ∈ N.
(ii)	. There exist a threshold γ ∈ (0, 1/2) and absolute constants C1, C2 > 0 such that λj ≤
Ci ∙ exp(-C2jγ) for any j ∈ N.
Note that we can replace the 1’s in the upper bounds of Assumption 4.2 with any absolute constant,
which is reflected in the H-norm of any function in H. Meanwhile, we can relax ∣φj (x)| ≤ 1 into
∣λτ ∙ φj (x) | ≤ 1 for any absolute constant T ∈ [0,1/2), which leads to the same regret.
We have the following result on the regret of Algorithm 1.
5
Under review as a conference paper at ICLR 2021
Theorem 4.3. Suppose Assumption 4.2 holds and P ⊂ HR. There exist absolute constants C3, C4 >
0 such that, for any p ∈ (0, 1), if we set
α = P2log |A|/(HT), β = C3H2 ∙ log1+1/Y(RT/p) ∙ log2(1∕γ)∕γ
in Algorithm 1, then it holds that
Regret(T) ≤ C4√H3T ∙ log1+1/Y(∣A∣RT∕p) ∙ log2(1∕γ)∕γ
with probability at least (1 - p).
Proof. See Section C for a detailed proof.	口
Theorem 4.3 indicates that log1+1∕γ(∣A∣RT∕p) ∙ log2(1∕γ)∕γ serves as the model capacity K(P) in
Theorem 4.1 for kernel function approximations. In particular, we can obtain γ for a broad range
of reproducing kernels K (Srinivas et al., 2009). Meanwhile, we can scale R to control the model
capacity K(P) (Scholkopf et al., 2002).
4.2	Neural Function Approximations
Let P be a set of overparametrized neural networks. In detail, we denote by NN a neural network
with its weights collected in a vector w ∈ Rm . Let w0 be the random initial weights. For a radius
R ≥ 2, we define
P = {P : ∃ w ∈ BR, s.t. P (s0 | s, a) = NN(x; w), for any x = (s, a, s0) ∈ S × A × S ⊂ Y},
where BR = {w ∈ Rm : kw - w0k2 ≤ R}.	(4.1)
Without loss of generality, we assume NN(x; w0) = 0 for any x ∈ Y, which can be achieved by a
symmetric initialization scheme. See Section E for a detailed explanation. To connect with the result
for kernel function approximations in Section 4.1, we define the following condition.
Condition 4.4 (Implicit Linearization). It holds that
ξm =	max	∣NN(x;W) — VwNN(x;w0)>(W — w0)∣ ≤ 1∕(4K3/2H).
x∈Y,w∈BR
Condition 4.4 indicates that NN(x; W) is uniformly close to the linear function Vw NN(x; W0)>(W -
W0 ) of W. In particular, the linearization error ξm is negligible compared with the dominating terms
in the regret. The following lemma ensures Condition 4.4 holds for two-layer neural networks when
m is sufficiently large.
Lemma 4.5 (Overparametrization). Suppose NN is a two-layer neural network, where the activation
function is 1-smooth, and it holds that kxk2 ≤ 1 for any x ∈ Y. Then, Condition 4.4 holds when
m ≥ dYR4K3H2.
Proof. See Section E for a detailed proof.	口
Note that the analogous of Lemma 4.5 also applies to nonsmooth activation functions, for example,
the rectified linear unit (ReLU), and multilayer neural networks (Allen-Zhu et al., 2019; Du et al.,
2019; Zou et al., 2020; Gao et al., 2019), which ensures Condition 4.4 holds. The linear function of
W in Condition 4.4 induces an RKHS H with the reproducing kernel
KNTK(x, y) = VwNN(x; W0)>VwNN(y; W0), for any x, y ∈ Y,	(4.2)
which is known as the neural tangent kernel (NTK) (Jacot et al., 2018).
Assumption 4.6. We assume KNTK satisfies the regularity conditions in Assumption 4.2.
Note that the NTK defined in (4.2) depends on the randomness of W0. When m goes to infinity, such
an empirical NTK converges to its expectation, which gives the population NTK. It is shown in Yang
6
Under review as a conference paper at ICLR 2021
& Salman (2019) that Assumption 4.2 holds for the population NTK, which implies it also holds for
the empirical NTK with high probability when m is sufficiently large.
We have the following result on the regret of Algorithm 1.
Theorem 4.7. Suppose Assumption 4.6 and Condition 4.4 hold and P has the representation in (4.1).
There exist absolute constants C5, C6 > 0 such that for any p ∈ (0, 1), if we set
α = P2log |A|/(HT), β = C5H2 ∙ log1+1/Y(RT/p) ∙ log2(1∕γ)∕γ
in Algorithm 1, then it holds that
Regret(T) ≤ C6√H3T ∙ log1+1∕γ(∣A∣RT∕p) ∙ log2(1∕γ)∕γ
with probability at least (1 - p).
Proof. See Section D for a detailed proof.	□
In parallel with Theorem 4.3, Theorem 4.7 indicates that log 1+1/y(∣A∣RT∕p) ∙log2(1∕γ)∕γ serves as
the model capacity κ(P) in Theorem 4.1 for neural function approximations, which can be controlled
by scaling R (Arora et al., 2019).
4.3 General Function Approximations
Let P be a general model class, whose model capacity is characterized by the eluder dimension
(Russo & Van Roy, 2014; Osband & Van Roy, 2014; Ayoub et al., 2020) defined as follows.
Definition 4.8 (Eluder Dimension). Let Z be a set of real-valued functions on the domain X . For
any ε > 0 and τ ∈ N, we say xτ ∈ X is (Z, ε)-independent of x1, . . ., xτ-1 ∈ X if there exist
f1, f2 ∈ Z such that
(X lfι(xj) - f2(xj)|2) / ≤ ε,	∣fι(xτ) - f2(xτ)| > ε.	(4.3)
j=1
The eluder dimension of Z at scale ε, which is denoted by dimE(Z, ε), is the length of the longest
sequence x1, . . ., xτ ∈ X such that, for any j ∈ [τ], xj is (Z, ε0)-independent of x1, . . ., xj-1 for
some ε0 ≥ ε.
The following lemma decomposes the regret of Algorithm 1 into errors that arise from policy
improvement and policy evaluation, respectively.
Lemma 4.9 (Regret Decomposition). For any k ∈ [K], it holds that
KH	K
Regret(T) =X X E∏* [hQh(∙, Sh), ∏h(∙ 1 Sh) - ∏k(∙∣ Sh)i] + X(VIk(si) - Vιπk,k(sι))
KH
+XX
Eπ* [rh(sh, ah) + ZPh (Sh, ah, Vh+1) - Qh(S h, ah)].
k=1 h=1
Proof. See Lemma 4.2 of Cai et al. (2019) for a detailed proof.
□
The following lemma characterizes the error that arises from policy improvement.
Lemma 4.10 (Policy Improvement). It we set α = 2∕1o log ∣A∣∕(KH2) in Algorithm 1, then it holds
that
K H
XX
E∏* [hQh(∙,Sh),琮(∙∣ Sh)- ∏k(∙∣ Sh)i] ≤ P2KH4 ∙ log |A|.
k=1 h=1
7
Under review as a conference paper at ICLR 2021
Proof. See Section B.2 for a detailed proof.
□
Recall that for any P ∈ P, zP is defined in (3.3). Also, let ZP = {zP : P ∈ P}. For any > 0, we
denote by Ne(P, k ∙ k∞,ι) the e-covering number of P with respect to the '∞,ι-norm distance, which
is defined by
kP - P0k∞,1
max
(s,a)∈S×A
|P(s0|
S
s, a) - P0(s0 | s, a)| ds0, for any P, P0 ∈ P.
The following lemma characterizes the error that arises from policy evaluation.
Lemma 4.11 (Policy Evaluation). For any p ∈ (0, 1), if we set
β ≥ 2H2 ∙ log(M/(KH)(P, k∙k∞,ι) ∙ 2H/p) +4(H + PH2/4 ∙ log(8K2Hlp)	(4.4)
in Algorithm 1, then the following results hold with probability at least (1 - p).
•	(Optimism) For any (k, h) ∈ [K] × [H] and (s, a) ∈ S × A, it holds that
rhk (s, a) + zPh (s, a, Vhk+1) - Qkh(s, a) ≤ 0.
•	(Accuracy) Let d = K ∧ dimE(ZP, 1/K). It holds that
K	,
X(Vk(si) - VF,k(sι)) ≤ P32KH3 ∙ log(p∕2) + H(dH +1) +4PeKH2.
k=1
Proof. See Section B.1 fora detailed proof.	□
Recall that T = KH. The following theorem characterizes the regret of Algorithm 1 when P is a
general model class, which serves as a meta result.
Theorem 4.12. In Algorithm 1, if we set α as in Lemma 4.10 and β as in Lemma 4.11, then it holds
that
Regret(T) ≤ PH3T ∙ log |A| + √32H2T ∙ log(p∕2) + H(dH +1)+ 4pβHT
with probability at least (1 - p), where d = K ∧ dimE(ZP, 1/K).
Proof. The proof follows from combining Lemmas 4.9, 4.11, and 4.10.	□
Theorem 4.12 indicates that
max{d, Jd ∙ log(Nι/(KH)(P, ∣∣∙ ∣∣∞,ι))}
serves as the model capacity κ(P) in Theorem 4.1. The regret upper bound in Theorem 4.12 is similar
to that in Ayoub et al. (2020) when P is a general model class, whose model capacity is characterized
by the eluder dimension. In contrast, our algorithm additionally handles adversarial rewards, which
is a benefit of the policy optimization approach. To establish the regret upper bounds in Sections
4.1 and 4.2, it remains to characterize the corresponding eluder dimension and log-covering number,
respectively. See Sections C and D for details. As a special case, Theorem 4.12 also applies to the
case where P is a set of d-dimensional linear models with a finite d, which is studied in Cai et al.
(2019). In particular, the eluder dimension and log-covering number in (4.4) are both O(d) (Ayoub
et al., 2020), which leads to the √d2H3T-regret in Cai et al. (2019). In contrast, Theorem 4.12
additionally handles the case where d is infinite as in kernel and neural function approximations.
References
Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic
bandits. In Advances in Neural Information Processing Systems, pp. 2312-2320, 2011.
8
Under review as a conference paper at ICLR 2021
Yasin Abbasi-Yadkori, Peter Bartlett, KUsh Bhatia, Nevena Lazic, Csaba Szepesvari, and Gellert
Weisz. POLITEX: Regret bounds for policy iteration using expert prediction. In International
Conference on Machine Learning, pp. 3692-3702, 2019a.
Yasin Abbasi-Yadkori, Nevena Lazic, Csaba Szepesvari, and Gellert Weisz. Exploration-enhanced
POLITEX. arXiv preprint arXiv:1908.10479, 2019b.
Alekh Agarwal, Sham M Kakade, Jason D Lee, and GaUrav Mahajan. Optimality and approximation
with policy gradient methods in Markov decision processes. arXiv preprint arXiv:1908.00261,
2019.
Alekh Agarwal, Mikael Henaff, Sham Kakade, and Wen SUn. Pc-pg: Policy cover directed exploration
for provable policy gradient learning. arXiv preprint arXiv:2007.08459, 2020.
ZeyUan Allen-ZhU, YUanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252, 2019.
Sanjeev Arora, Simon S DU, Wei HU, ZhiyUan Li, and RUosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neUral networks. arXiv preprint
arXiv:1901.08584, 2019.
Alex Ayoub, ZeyU Jia, Csaba Szepesvari, Mengdi Wang, and Lin F Yang. Model-based reinforcement
learning with valUe-targeted regression. arXiv preprint arXiv:2006.01107, 2020.
Kazuoki Azuma. Weighted sums of certain dependent random variables. Tohoku Mathematical
Journal, Second Series, 19(3):357-367, 1967.
Jonathan Baxter and Peter L Bartlett. Direct gradient-based reinforcement learning. In International
Symposium on Circuits and Systems, pp. 271-274, 2000.
Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, PrZemySIaW Debiak, Christy
Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale
deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.
Jalaj Bhandari and Daniel Russo. Global optimality guarantees for policy gradient methods. arXiv
preprint arXiv:1906.01786, 2019.
Qi Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang. Provably efficient exploration in policy optimiza-
tion. arXiv preprint arXiv:1912.05830, 2019.
Simon Du, Jason Lee, Haochuan Li, LiWei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural netWorks. In International Conference on Machine Learning, pp. 1675-
1685, 2019.
Eyal Even-Dar, Sham M Kakade, and Yishay Mansour. Online Markov decision processes. Mathe-
matics of Operations Research, 34(3):726-736, 2009.
Maryam Fazel, Rong Ge, Sham M Kakade, and Mehran Mesbahi. Global convergence of policy
gradient methods for the linear quadratic regulator. arXiv preprint arXiv:1801.05039, 2018.
Yihao Feng, Tongzheng Ren, Ziyang Tang, and Qiang Liu. Accountable off-policy evaluation With
kernel bellman statistics. arXiv preprint arXiv:2008.06668, 2020.
Ruiqi Gao, Tianle Cai, Haochuan Li, Cho-Jui Hsieh, LiWei Wang, and Jason D Lee. Convergence
of adversarial training in overparametrized neural netWorks. In Advances in Neural Information
Processing Systems, pp. 13029-13040, 2019.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural netWorks. In Advances in Neural Information Processing Systems, pp.
8571-8580, 2018.
Sham M Kakade. A natural policy gradient. In Advances in neural information processing systems,
pp. 1531-1538, 2002.
9
Under review as a conference paper at ICLR 2021
Boyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural proximal/trust region policy optimization
attains globally optimal policy. arXiv preprint arXiv:1906.10306, 2019.
Ha Quang Minh, Partha Niyogi, and Yuan Yao. Mercer’s theorem, feature maps, and smoothing. In
International Conference on Computational Learning Theory, pp. 154-168. Springer, 2006.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. nature, 518(7540):529-533, 2015.
Claus Muller. Analysis of spherical symmetries in Euclidean spaces, volume 129. Springer Science
& Business Media, 2012.
Gergely Neu, Andras Antos, Andras Gyorgy, and Csaba Szepesvari. Online Markov decision
processes under bandit feedback. In Advances in Neural Information Processing Systems, pp.
1804-1812, 2010a.
Gergely Neu, AndraS Gyorgy, and Csaba Szepesvari. The online loop-free stochastic shortest-path
problem. In Conference on Learning Theory, pp. 231-243, 2010b.
Gergely Neu, AndraS Gyorgy, and Csaba Szepesvari. The adversarial stochastic shortest path problem
with unknown transition probabilities. In International Conference on Artificial Intelligence and
Statistics, pp. 805-813, 2012.
Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the eluder dimension.
In Advances in Neural Information Processing Systems, pp. 1466-1474, 2014.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. Advances in
neural information processing systems, 20:1177-1184, 2007.
Aviv Rosenberg and Yishay Mansour. Online stochastic shortest path with bandit feedback and
unknown transition function. In Advances in Neural Information Processing Systems, pp. 2209-
2218, 2019a.
Aviv Rosenberg and Yishay Mansour. Online convex optimization in adversarial Markov decision
processes. arXiv preprint arXiv:1905.07773, 2019b.
Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic
exploration. In Advances in Neural Information Processing Systems, pp. 2256-2264, 2013.
Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics of
Operations Research, 39(4):1221-1243, 2014.
Bernhard Scholkopf, Alexander J Smola, and Francis Bach. Learning with kernels: Support vector
machines, regularization, optimization, and beyond. MIT Press, 2002.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, pp. 1889-1897, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of Go with deep neural networks and tree search. Nature, 529(7587):484, 2016.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of Go
without human knowledge. Nature, 550(7676):354, 2017.
Niranjan Srinivas, Andreas Krause, Sham M Kakade, and Matthias Seeger. Gaussian process opti-
mization in the bandit setting: No regret and experimental design. arXiv preprint arXiv:0912.3995,
2009.
10
Under review as a conference paper at ICLR 2021
Ingo Steinwart and Andreas Christmann. Support vector machines. Springer Science & Business
Media, 2008.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient meth-
ods for reinforcement learning with function approximation. In Advances in Neural Information
Processing Systems, 2000.
Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge University Press, 2018.
Oriol Vinyals,Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, Junyoung
Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in
starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.
Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods: Global
optimality and rates of convergence. arXiv preprint arXiv:1909.01150, 2019.
Ruosong Wang, Ruslan Salakhutdinov, and Lin F Yang. Provably efficient reinforcement learning
with general value function approximation. arXiv preprint arXiv:2005.10804, 2020.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine Learning, 8(3-4):229-256, 1992.
Greg Yang and Hadi Salman. A fine-grained spectral perspective on neural networks. arXiv preprint
arXiv:1907.10599, 2019.
Zhuoran Yang, Yongxin Chen, Mingyi Hong, and Zhaoran Wang. On the global convergence of actor-
critic: A case for linear quadratic regulator with ergodic cost. arXiv preprint arXiv:1907.06246,
2019.
Zhuoran Yang, Chi Jin, Zhaoran Wang, Mengdi Wang, and Michael I Jordan. Bridging exploration
and general function approximation in reinforcement learning: Provably efficient kernel and neural
value iterations. arXiv preprint arXiv:2011.04622, 2020.
Jia Yuan Yu, Shie Mannor, and Nahum Shimkin. Markov decision processes with arbitrary reward
processes. Mathematics of Operations Research, 34(3):737-757, 2009.
Alexander Zimin and Gergely Neu. Online learning in episodic Markovian decision processes
by relative entropy policy search. In Advances in Neural Information Processing Systems, pp.
1583-1591, 2013.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over-
parameterized deep ReLU networks. Machine Learning, 109(3):467-492, 2020.
11
Under review as a conference paper at ICLR 2021
A More Details on RKHS
An example of the RKHS is the linear model class. In particular, let ψ be a d-dimensional feature
vector
ψ(x) = (ψι(x),..., ψd(x))>, for any X ∈ Y,
where ψι,... ,ψd are linearly independent. Then, the linear span of {ψj }∞=ι forms an RKHS Hψ
with the reproducing kernel Kψ(x, y) = ψ(x)>ψ(y) for any x, y ∈ Y, and the corresponding inner
product h∙, ∙)Hψ is defined by the Euclidean inner product
hψ(∙)>c1,ψ(∙)>c2iHψ = c>c2
for any c1,c2 ∈ Rd.
The above example can be naturally generalized to the case where d = ∞, that is, the feature
vector can be infinite-dimensional. Moreover, recall that (2.2) says the reproducing kernel K has the
representation
∞
K(x, y) =	λj φj (x)φj (y), foranyx,y ∈ Y,
j=1
where {φj}∞=ι is an orthonormal basis of L2(Y) and λι ≥ λ? ≥ ∙∙∙ ≥ 0. We refer to {φj}∞=ι as
the eigenfunctions of K with the corresponding eigenvalues {λj}j∞=1. Such a representation gives the
feature vector
e(x) = (√λι ∙ φι(x), ρ∕λ2 ∙ φ2(x),...)>, for any X ∈ Y.
The linear span of {ʌ/ɪj ∙ φj}∞=ι recovers the RKHS H with the reproducing kernel K and inner
product h∙, )h∙ When the reproducing kernel K is infinite-dimensional, that is, K has an infinite
number of non zero eigenvalues, φ is an infinite-dimensional vector. It is known that RKHSs of
various infinite-dimensional reproducing kernels, for example, the Gaussian radius basis function
kernel (Steinwart & Christmann, 2008), are rich function model classes in the sense that they are
dense in the class of continuous and bounded functions.
B Proofs for Section 4.3
In this section, we provide the detailed proofs of the result in Section 4.3. For notational simplicity,
we denote by Ph the operator that takes the conditional expectation with respect to the transition
kernel Ph for any h ∈ [H].
B.1 Proof of Lemma 4.11
Proof. We define the event E that
Ph ∈ Phk, for any (k, h) ∈ [K] × [H].
(B.1)
By our choice of β and Lemma F.4 with δ = p/2, it holds that E occurs with probability at least
(1 - p/2).
12
Under review as a conference paper at ICLR 2021
Optimism: For any (k, h) ∈ [K] × [H], Ph ∈ Phk implies
Qh(∙,∙)-(rk(∙,∙) + (PhVh+ι)(∙, ∙))
=clamp(maxk^ VhfI(S0) ∙ P(s01 ∙, ∙)ds0, 0,H - h) - 1 VhfI(S0) ∙ Ph(s01 ∙, ∙)ds0
≥ clamp(( Vh+ι(s0) ∙ Ph(s01∙, ∙) ds0, 0,H - h) - JS Vk+ι(s0) ∙ Ph(s01∙, ∙) ds0.	(B.2)
When h = H, the right-hand side of (B.2) is zero since VH+ι(∙) = 0. When h < H ,by the
construction of Qh in (3.5) and the assumption that rk+ι(∙) ∈ [0,1], We have
Qh+1(∙, ∙) ∈ [0, H - h],	Vh+1(∙) ∈	[0, H -	h],	ZS	Vh+1 (SO)	∙	Ph(s0 |	∙, ∙)	ds0	∈	[0, H -	h].
The right-hand side of (B.2) is also zero, Which implies
Qh(∙, ∙)-C4(∙, ∙) + (PhVk+ι)(∙, ∙)) ≥ 0.
Thus, the optimism result holds under the event E .
Accuracy: We invoke Lemma F.1 and obtain
K	KH
XV1k(S1k)-V1πk(S1k)=XX(Dhk,1+Dhk,2)	(B.3)
k=1	k=1 h=1
KH
+ XX(Qh(sh, ah) - (rk(sh, ah) + PhVk+ι(sh, ah))),
k=1 h=1
Where |Dhk,1 | ≤ 2H, |Dhk,2| ≤ 2H, DHk ,2 = 0 for any (k, h) ∈ [K] × [H], and
D11,1, D11,2 + D21,1,	D21,2	+	D31,1, . . .	, DH1 -1,2	+ DH1 ,1,
D12,1, D12,2 + D22,1,	D22,2	+	D32,1, . . .	, DH2 -1,2	+ DH2 ,1,
is a martingale difference sequence. The Azuma-Hoeffding inequality (Azuma, 1967) implies
K H
X X(DkJ + Dk,2) ≤ √32KH3 ∙ log(p∕2)
k=1 h=1
(B.4)
With probability at least (1 - p/2). It remains to upper bound the second term on the right-hand side
of (B.3). For any (k, h) ∈ [K] × [H], Ph ∈ Phk implies
QMSh ah) - (rk(sh, ah) + (PhVh+ι)(sh ah)
= Clamp(maxk ( Vh+ι(s0) ∙ P(s01 sh ahh) ds0,0,H - h) - ( Vhk+ι(s0) ∙ Ph(s | sh ahh) ds0
≤ max V Vh+ι(s0) ∙ P(S0| sh,ah)ds0 - min V Vh+ι(s0) ∙ P(S0| sh,ah)ds0.
P ∈Phk S	P ∈Phk S
13
Under review as a conference paper at ICLR 2021
Applying Lemma F.5, for any h ∈ [H], we have
Qh(sh, ah) - (rk(sh, ah) + (PhVh+ι)(sh, ah))
K
≤ X(max I Vh+ι(s0) ∙ P(s01 sh,ah)ds0- min I Vh+1(s0) ∙ P(s01 sh,ahh)ds0
k=1 P ∈Phk S	P ∈Phk S
≤ 1 + dH + 4 P dβK,
where d = K ∧ dimE(ZP, 1/K). Combining (B.3)-(B.5), we have
(B.5)
K
XVk(Sk) - Vnk(Sk) ≤ p32KH3 ∙ log(p∕2) + H + dH2 +4pβHT.
k=1
In summary, when the event E and (B.4) hold, which occur with probability at least (1 - p), we have
Qh(∙, ∙) ≥ rhk(∙, B + (PhVh+1)(∙, ∙), for any (k,h) ∈ [K] X [H],
K
X(Vk(sk) - Vnk,k(Sh)) ≤ P32KH3 ∙ lοg(p∕2) + H(1 + dH)+4PdβHT.
k=1
Therefore, We conclude the proof of Lemma 4.11.	□
B.2 Proof of Lemma 4.10
Proof. By Lemma 3.3 of Cai et al. (2019), for any (k, h) ∈ [K] X [H] and S ∈ S, We have
hQHs,∙),∏h (∙∣ s)-∏k(∙∣ Ss)
≤ αH2/2 + α-1 ∙ 0kl(∏N | S)Il ∏(∙ | S)) - DKL(域(∙ | S)Il ∏k+1(∙ I s))),	(B.6)
Which implies
KH
XX E∏* [hQh(Sh, ∙),∏h (∙∣ Sh)-∏h(∙∣ Sh))]
k=1 h=1
H
≤	αKH3∕2 + α-1 ∙ X E∏, [Dkl(∏⅛(∙ | Sh) ∣∣ ∏h(∙ | Sh))]
h=1
≤	αKH3∕2 + α-1H ∙ log |A|.	(B.7)
Plugging α =，2 log ∣A∣∕(HT) into the right-hand side of (B.7), we conclude the proof of Lemma
4.10.	□
C Proof of Theorem 4.3
In this section, we prove Theorem 4.3. By Theorem 4.12, it suffices to upper bound the eluder
dimension of ZP = {zP : P ∈ P} and log-covering number of HR, which are characterized by the
following two lemmas, respectively.
Lemma C.1. Under Assumption 4.2, there exists an absolute constant C7 > 0 such that
K ∧ dimE(ZP,1∕K) ≤ C7 ∙ log2(1∕γ)∕γ ∙ log1+1/Y(RT).
Proof. Let t = K ∧ dimE (ZP, 1∕K). By Definition 4.8, there exists a sequence x1, . . . , xt ∈
S X A X [0, H]S such that xτ is (ZP, 1∕K)-independent of x1, . . . , xτ-1 for any τ ∈ [t]. Here the
independency scale is assumed to be 1∕K without loss of generality, which can be changed to any
14
Under review as a conference paper at ICLR 2021
value larger than 1/K. In other words, for any τ ∈ [t], there exist P1, P2 ∈ P such that
τ-1
X |zP1(xi) - zP2(xi)|2 ≤ 1/K2,
i=1
∣ZPι (xτ) - ZP2 (xτ)| > 1/K.
(C.1)
(C.2)
Note that for any x = (s, a, V ) ∈ S × A × [0, H]S and P ∈ P, by the reproducing property of the
RKHS H, we can write
ZP (X) = [v(SO) ∙ P (Sl s,a) ds0 = [v(SO) ∙ hP, K(s,a,s0)iH ds0.
The representation of K in (2.2) implies
zP (x)
∞
LV(S) ∙ PpX X λj ∙ φj(S,a,SO) ∙ j Jso
∞
=LV(S')∙ X√λj ∙ %(S,a,S') ∙ hP, p√λj ∙ φjiH dS0
∞
=X √λj ∙	V(SO) ∙ φj(S,ɑ,s')ds' ∙ hP, √λj ∙ φjiH.
j=1	S
For any d0 such that d0γ ≥ 4(1 - γ)(γC2)-1 where γ and C2 are defined in Assumption 4.2, we
define
zeP (x)
X p√λj ∙/ V(SO) ∙ φj (s,a, s0)ds0 ∙ hP, p√λj ∙ φjiH
1≤j≤d0	S
for any x = (S, a, v) ∈ S × A × [0, H]S and P ∈ P. Then, we have
|zp(X)- eP(x)| = I X p∕λj ∙/ V(SO) ∙ φj (s,a, s')ds' ∙ hP, p∕λj ∙ φjiH
j>d0	S
≤ X √λj ∙∣S∣∙ H ∙kPkH ≤ X √λj ∙ RH,
j>d0
j>d0
where |S| is the Lebesgue measure of S and |S| ≤ 1. Here the first inequality follows from the
CaUchy-SchWarz inequality and ∣∣ ∙√zλj ∙ φj∣∣H = 1 for any j ∈ N, since {A∕λj ∙ φj}∞=1 is an
orthonormal basis of H. By Assumption 4.2, we obtain
∣ZP(X)- ZP(x)| ≤ X √C1 ∙ exp(-C2jγ∕2) ∙ RH
j>d0
=√C1 ∙ RH ∙ X exp(-C2jγ/2)
j>d0
≤ √C1 ∙ RH ∙ 4d1-γ(γC2)-1 ∙ exp(-C2dγ/2),	(C.3)
Where the last inequality folloWs from Lemma F.6. For notational simplicity, let
Γ(d0) = √C1 ∙ RH ∙ 4dl-γ(γC2)-1 ∙ exp(-C2dγ∕2).	(C.4)
15
Under review as a conference paper at ICLR 2021
By (C.1), it holds that
T-1	T-1
X lePι (Xi) - eP2 (Xi)|2 ≤ X(|zPi (Xi) - zP2 (Xi)I +2r(dθ))2
i=1	i=1
T-1
≤ X(2∣ZP1 (Xi) ― zP2(Xi)|2 + 8Γ(d0)2)
i=1
≤ 2/K2 + 8t • Γ(d0)2,
(C.5)
where the last inequality uses τ ≤ t. For any τ ∈ [t], we define
y =(hP1 - P2, Pλ1 . φ1iH, ..., hP1 - P2, Pλd0 . φd0 iH)>,
VT = (pλ1 ./ VT (s0) . φι(sτ,aτ ,s0)ds0,..., pλd0 ./ Vt (s0) . φd0 (
sτ, aτ, s
0)ds0
(C.6)
>
, (C.7)
T-1
Λτ = Ido ×do∕(d0K 2R2) + X ViV>.
i=1
(C.8)
Then, it holds that
1	T-1
八y = d0K2R2 ∙kyk2 + X(y>vi)2
i=1
_	1
=do K2 R2
/	1
≤ do K2 R2
d0
τ-1
• EhPI - P2, Pλj ∙ φjiH + E lePι (Xi)- eP2 (Xi) |2
j=1
i=1
. 4doR2 + 2/K2 + 8t • Γ(d0)2 = 6/K2 + 8t • Γ(d0)2,
(C.9)
where the inequality uses (C.5) and
hP1	- p2,	Pλj .	φj	iH ≤ kP1	-	p2kH . l∣pλj .	φj	kH ≤	4R2	. 1 = 4R2.
Here we uses IlPkH ≤ R for any P ∈ P and ∣∣√λi . φi∣H = 1 for any j ∈ [d0].
In the sequel, we establish an upper bound of |zeP1 (XT) ― zeP2 (XT)|. By the definitions of y in (C.6)
and vT in (C.7), we can write zeP1 (XT) ― zeP2 (XT) = hy, vTi. By (C.9), |zeP1 (zT) ― zeP2 (zT)| is upper
bounded by
max hy0, vTi s.t. (y0)>ΛT y0 ≤ 6/K2 + 8t • Γ(d0)2.
y0 ∈Rd0
The maximizer of such a quadratic program is
y0 = J[6∕K2 + 8t ∙Γ(d0)2]∕(v>Λ-1Vτ) . Λ-1Vτ.
Therefore, we obtain
∣epι (XT ) ― Mt )| ≤ J(6∕K2 +8t ∙Γ(do)2)(v>Λ-1VT ).
On the other hand, by (C.2)-(C.4), we have
∣epι (XT) 一 eP2 (XT )|
≥ ∣ZPι (XT) ― ZP2 (Xτ)| 一 (∣ePι (XT) ― ZPι (XT)| + |eP2 (Xτ) ― ZP2(Xτ)|)
≥ |zP1 (XT) - zP2 (XT)| - 2Γ(d0)
> 1/K ― 2Γ(d0).
(C.10)
(C.11)
16
Under review as a conference paper at ICLR 2021
We set do =「C ∙ log(1∕γ)∕γ ∙ log1/Y(4tRKH)], where C is defined in Lemma F.7. Then, by
Lemma F.7, it holds that d0γ ≥ 4(1 - γ)(γC2)-1 and
Γ(d0) ≤ 1/(4tK).	(C.12)
Combining (C.10), (C.11), and (C.12), we obtain
vτ>Λτ-1vτ >
(1/K - 2Γ(d0))2	(1/K - 1∕(2K))2
6/K2 + 8t ∙Γ(do)2 — 6/K2 + 1/(2K2)
> 1/100.
(C.13)
Therefore, by (C.13), we have
t
t/100 = X min{1/100,v>A-1vT }
τ=1
t
≤ X2log(1 + v>Λ-1Vτ) ≤ 2((logdet(Λt+ι) - logdet(Λι)),	(C.14)
τ=1
where the last inequality uses the elliptical potential lemma (Abbasi-Yadkori et al., 2011). Here,
similar to (C.8), Λt+1 is defined by
t
Λt+ι = Id0×d0 /(do K 2R2) + X Vi v>.
i=1
Note that
d0	2
∣∣Vτ k2 = X λj ∙ (J Vr (s0)φj (sτ ,aτ ,s0 )ds0) ≤ do H 2
for any τ ∈ [t]. Thus, setting λ = 1/(doK2R2), we have
Iogdet(Ai) = do ∙ log λ,	(C.15)
t
logdet(At+ι) ≤ do ∙ log ∣∣At+ιk2 ≤ do ∙ log(λ + X Mk2) ≤ do ∙ log(λ + doKH2). (C.16)
i=1
where the last inequality follows from t = K ∧ dimE(ZP, 1/K). Combining (C.15) and (C.16), we
have
2((logdet(At+ι) — Iogdet(Ai)) ≤ 2do ∙ log(1 + doKH2/1)=2do ∙ log(1 + d2K3R2H2)
(C.17)
Combining (C.14) and (C.17), we obtain
t ≤ 200do ∙ log(1 + do K 3R2H2) ≤ 600do ∙ log(d°KRH).	(C.18)
Moreover, by do =「C ∙ log(1/Y)/Y ∙log1∕γ (4tRKH)] and (C.18), there exists an absolute constant
C8 > 0 such that
t ≤ 600do ∙ log(doKRH)
≤ 600 ∙「C ∙ log(1/Y)/Y ∙ log1∕γ(tKRH))] ∙ log(「C ∙ log(l/T)/) ∙ log1∕γ(tKRH)[. KRH)
≤ C8 ∙ log2(1/Y)/y ∙ log1+1∕γ(tKRH).	(C.19)
Recall that t = K ∧ dimE(ZP, 1/K). Thus, by (C.19), we obtain
t ≤ C7 ∙ log2(1/Y)/Y ∙ log1+1∕γ(KRH)	(C.20)
17
Under review as a conference paper at ICLR 2021
for an absolute constant C7 > 0. Thus, we conclude the proof of Lemma C.1.
□
Lemma C.2. Under Assumption 4.2, there exists an absolute constant C9 > 0 such that
Ni/(kh)(P, k∙k∞,ι) ≤ C9 ∙ log2(1∕γ)∕γ ∙ log1+1/Y(RT)
for any R ≥ 2.
Proof. Recall that we define Γ(d0 ) in (C.4). By choosing t = 2KH in Lemma F.7, there exists
do = dC ∙ log(1∕γ)∕γ ∙ log1/Y(2KRH2)]
γ
where C is defined in Lemma F.7, such that d0γ ≥ 4(1 - γ)(γC2)-1 and
Γ(d0) ≤ 1∕(2KH).	(C.21)
Here Y and C2 are defined in Assumption 4.2. Let C be a minimal 1/(2d0/2KH)-covering of the
Euclidean ball {v ∈ Rd0 : kv∣∣2 ≤ R} with respect to the '2-norm distance. For any P ∈ P, we
, . 二 ______
define P ∈ P by
d0
P(X) = fλj ∙ hP,φj〉H ∙ φj (x), for any X ∈ Y.
j=1
Also, we write
V = ( Vzλ1 ∙ hP, φ1iH, ..., VZλd0 ∙ hP, φd0 iH).
Since { √λ^ ∙ φj }泊 is an orthonormal basis of H and P ∈ HR, we have
〜
kvk2 = kPekH ≤ kPkH≤R.
Thus, by the definition of C, there exists v* ∈ C with ∣∣v - v*∣∣2 ≤ 1/(2d0/2KH). We define P * ∈ P
by
d0
P*(x) = X v* ∙ p∕λj^ ∙ φj (x), for any X ∈ Y.
j=1
Then, by Assumption 4.2, for any X ∈ Y, we have
IP(X)- P *(x)| = X(Vj-Vjt) ∙ ∕j ∙ φj (X)I
j=1
≤∣V - v*kι ≤ d0∕2∙kv - v*∣2 ≤ 1∕(2KH).
Also, for any X ∈ Y, we have
IP(X)- P(X)I = I X ∕λj ∙ φj(X) ∙ hP, ∕λj ∙ φjiH
j>d0
≤ X∕λj ∙ R
j>d0
≤ £ E ∙ exp(-C2jγ∕2) ∙ R ≤ Γ(do),
(C.22)
(C.23)
j>d0
where the first inequality uses ∣∣P∣∣h ≤ R and ∣∣ Ayλj ∙ φj∣H = 1, the second inequality follows from
Assumption 4.2, and the last inequality follows from the same argument in (C.3) and H ≥ 1. Thus,
18
Under review as a conference paper at ICLR 2021
for any x ∈ Y , it holds that
. . . .. . ~ . . . ~ . 、 .. . .. . .. .
|P(x) - P*(x)∣ ≤	|P(x)	- P(x)∣	+	|P(x)	- P*(x)∣ ≤ Γ(d0)	+ 1∕(2KH)	≤	1∕(KH).	(C.24)
We define
d0
PC = {p : ∃ V ∈ C s.t. P(x) = ^X Vj ∙ pλj ∙ φj (x) for any X ∈ Y}.
j=1
Then, by (C.24), PC is a 1∕(KH)-covering of P with respect to the '∞-norm distance. Therefore,
we have
N1∕(KH)(P, k ∙ ll∞,1) ≤ N1∕(KH)(P, k ∙ ll∞)
≤ |Pc| ≤ |C| ≤ C12 ∙ do ∙ log(doKRH) ≤ C9 ∙ log2(1∕γ)∕γ ∙ log1+1/Y(KRH),	(C.25)
where C12,C9 > 0 are absolute constants. Here the first inequality is because ∣∣ ∙ ∣∣∞,ι ≤ ∣∣ ∙ ∣∣∞
given |S| ≤ 1, the third inequality follows from Corollary 4.2.13 of Vershynin (2018), and the
last inequality follows from the same argument in (C.19). Thus, we conclude the proof of Lemma
C.2.	□
We note that our proofs of Lemmas C.1 and C.2 for the exponential decay case can be made general.
For a finite-rank kernel, we can let d0 be the rank of the kernel, i.e., the number of nonzero eigenvalues.
Then, the same proof still holds and we can show that the eluder dimension is upper bounded by
O(d0 log(RKH)). Moreover, for a kernel with polynomially decaying eigenvalues, that is, we have
λj ≤ CPoly ∙ j-γ for some constants Cpoiy, γ > 0 and any j ≥ 1,
we can still truncate at dimension d0 and calculate Γ(d0) in (C.4) using the polynomial eigenvalue
decay condition. It can be shown that Γ(d0) decays polynomially in d0. Then, we can find a proper
d0 by solving (C.12) and (C.21) and follow the same proof afterwards.
Proof of Theorem 4.3
Proof. Recall that d = K ∧ dimE (ZP, 1∕K). By Theorem 4.12, it holds that
Regret(T) ≤ √2H3T ∙ log |A| + √32H2T ∙ log(p∕2) + H(dH + 1) + 4PdeHT,	(C.26)
with probability at least (1 - p), where we set α = 2∕ιo log ∣A∣∕(KH2) and
β ≥ 2H2 ∙ log(Nι∕τ(P, ∣∣∙ ∣∞,ι) ∙ 2H∕p) +4(H + √H2∕4 ∙ log(8K2H∕p)).	(C.27)
By Lemma C.2, we have
Nι∕(KH)(P, k∙k∞,ι) ≤ C9 ∙ log2(1∕γ)∕γ ∙ log1+1/Y(RT),
which implies there exists an absolute constant C3 > 0 such that
2H2 ∙ log(Ni/(kh)(P, k ∙ k∞,ι) ∙ 2H∕p) +4(H + √H2∕4 ∙ log(8K2H∕p))
≤ C3H2 ∙ log2(1∕γ)∕γ ∙ log1"(RT∕p).
In other words, (C.27) holds ifwe set
β = C3H2 ∙ log2(1∕γ)∕γ ∙ log1+1/Y(RT∕p).	(C.28)
On the other hand, by Lemma C.1, we have
d ≤ C7 ∙ log2(1∕γ)∕γ ∙ log1+1/Y(RT).	(C.29)
19
Under review as a conference paper at ICLR 2021
Plugging (C.28) and (C.29) into (C.26), we obtain
Regret(T) ≤，2H3T ∙ log |A| +，32H2T ∙ log(p∕2) + H
+ C7H2 ∙ log2(1∕γ)∕γ ∙ log1+1/Y(RT)
+ 4PC3C7H3T ∙ log2(1∕γ)∕γ ∙ log(1+1/Y)/2(RT) ∙ lOg(I+1/Y)/2(RT/p)
≤ C4√H3T ∙ log2(1∕γ)∕γ ∙ log1+1/Y(∣A∣RT∕p)	(C.30)
with probability at least (1 -p), where C4 > 0 is an absolute constant. Thus, we conclude the proof
ofTheorem4.3.	口
D	Proof of Theorem 4.7
In this section, we prove Theorem 4.7. Similar to the proof of Theorem 4.3, we upper bound the
eluder dimension of ZP and log-covering number of P by the following two lemmas respectively.
Recall that BR is defined in (4.1). By the definition of KNTK in (4.2), we have
Hr = {P : ∃ W ∈ Br s.t. P(X) = NwNN(x; w0)>(w — w0) for any X ∈ Y}.
Without loss of generality, We assume entries of VwNN(∙; w0) are linearly independent, which
happens with probability one for most neural networks with nonlinear activation functions and
random initialization.
Lemma D.1. Under Assumptions 4.6 and 4.4, there exists an absolute constant C10 such that
K ∧ dimE(Zp, 1∕K) ≤ C10 ∙ log2(1∕γ)∕γ ∙ log1+1/Y(RT).
Proof. Let t = K ∧ dimE (ZP, 1∕K). By Definition 4.8, there exists a sequence X1, . . . , Xt ∈
S × A × [0, H]S such that Xτ is (ZP, 1∕K)-independent of X1, . . . , Xτ-1 for any τ ∈ [t]. In other
words, for any τ ∈ [t], there exist P1, P2 ∈ P such that
τ-1
X |zP1(Xi) — zP2(Xi)|2 ≤ 1∕K2,
i=1
∣ZPι (XT) — ZP2 (XT)| > 1∕K.
For any X = (s, a, V) ∈ S × A × [0, H]S and P ∈ P, we define
P(x; W) = VwNN(x; w0)>(w — w0).
Using the reproducing property of H and the representation of K in (2.2), we have
ZP(X) = J V(s0) ∙ P(s,a,s0; w) ds0
=/ V(s0) ∙ P(s,a,s0; w) ds0 + / V(s0) ∙ (P(s,a,s0; w) — P(s,a,s0; W)) ds0
∞
=XPλj ∙	V(SO) ∙ φj (s,a,s0)ds0 ∙ hP, Pλj ∙ φjiH
j=1	S
+ / V(s0) ∙ (P(s,a,s0; w) — P(s,a,s0; w)) ds0.
S
For any d0 such that d0Y ≥ 4(1 — γ)(γC2)-1, we define
(D.1)
(D.2)
zeP (X)
X Pλj ∙ [ V(S) ∙ φj(s,a,s')ds' ∙ hp, Pλj ∙ φjiH
1≤j≤d0	S
20
Under review as a conference paper at ICLR 2021
for any x = (s, a, V ) ∈ S × A × [0, H]S and P ∈ P. And we have
|zP (x) - zeP (x)|
+ S
V(s0) ∙ φj(s,a, s0)ds0 ∙hP, vzλj^ ∙ Φj>H
V(SXP(s, a,s0; W) — P(s, a, s0; W)) ds0
≤ E pλj ∙ H ∙ R + ξmH
j>d0
≤ 4d0-γpClHR(γC2)-1 ∙ exp(—C2dγ∕2) + ξmH = Γ(do) + ξmH,
(D.3)
where the second inequality uses Assumption 4.2, kP∣∣h ≤ R, k ∕λj ∙ φj ∣∣h = 1, and the definition
of ξm in Condition 4.4, the third inequality follows from Lemma F.6, and Γ(d0) is defined in (C.4).
Then, using the triangle inequality, we have
τ-1	τ-1
X ∣epι(Xi) — eP2(xi)∣2 ≤ X(∣zpi(Xi) — zP2(xi)l + 2Γ(do) + 2ξmH)2
i=1	i=1
τ-1
≤ X 2∣zpι (xi) — zP2 (xi)∣2 + 16t ∙ Γ(do)2 + 16tξmH2
i=1
≤ 2/K2 + 16t ∙ Γ(do)2 + 16tξmH2,	(D.4)
where the last inequality is by (D.1).
In the sequel, we establish an upper bound of ∣epι (XT) — ep2 (XT)|. For any T ∈ [t], similar to
(C.6)-(C.8), we denote
V = (hP1 - p2, Pλ1 ∙ φ1iH,∙∙∙, hP 1 — P 2, Pλd0 ∙ φd0 iH)>,	(D.5)
VT = (pλ1 ./ Vt (s0) ∙ φ1(sτ,aτ,s0)ds0,∙∙∙, pλd0 •/ 咚(s0) ∙ φd0 (Sτ,aτ,s0)ds0	>, (D.6)
T-1 At = Id0×d0/(doK 2R2) + X ViV>.	(D.7)
i=1
Then, we have
VTAT y = d0K 2R2
T-1
∣y∣22 + X(y>vi)2
i=1
1
do K 2R2
/	1
≤ d0K 2R2
d0	T-1
•	XhP 1— P 2, Pλj ∙ φj iH + X |ePi (Xi)- eP2 (xi)|2
j=1	i=1
•	4d0R2 + 2/K2 + 16t • Γ(d0)2 + 16te2mH2
6/K2 + 16t • Γ(d0)2 + 16te2mH2 ≤ 7/K2 + 16t • Γ(d0)2.
(D.8)
Here the inequality uses (D.4), Condition 4.4, and
hp 1— P2, Pλj• φjiH ≤ kp 1— P2kH ∙ l∣pλj• φjIlH ≤ 4R2 • 1 = 4R2,
which follows from the fact ∣∣Pi∣h, ∣P2∣H ≤ R and ∣∣√λi • φi∣H = 1 for any j ∈ [do]. By the
definitions of y in (D.5) and vT in (D.6), we can write zeP1 (XT) — zeP2 (XT) = hy, vTi, which by (D.8)
is upper bounded by
max hy0, vTi s.t. (y0)>ΛT y0 ≤ 7/K2 + 16t • Γ(do)2.
y0 ∈Rd0
21
Under review as a conference paper at ICLR 2021
The maximizer of such a quadratic program is
y0 = J(7∕K2 + 16t ∙ Γ(d0)2)∕(v>Λ-1Vτ) ∙ ArVT.
Therefore, we have
∣epι(XT) - eP2(xτ)| ≤ J(7∕K2 + 16t ∙Γ(d0)2)(v>A-1Vτ).	(D.9)
On the other hand, by (D.2), (D.3), and Condition 4.4 we have
|zeP1 (xT) - zeP2 (xT)|
≥ |zP1 (xT) - zP2 (xT)| - |zeP1 (xT) - zP2 (xT)| + |zeP1 (xT) - zP2 (xT)|
≥ |zP1 (xT) - zP2 (xT)| - 2Γ(d0) - 2ξmH
> 1∕K- 2Γ(d0) - 1∕(2K) ≥ 1∕(2K) - 2Γ(d0).	(D.10)
We set do = d(e ∙ log(1∕γ)∕γ ∙ log1/Y(8tKRH)], where C is defined in Lemma F.7. Then, by
Lemma F.7 it holds that d0γ ≥ 4(1 - γ)(γC2)-1 and
Γ(d0) ≤ 1∕(8tK).	(D.11)
Combining (D.9)-(D.11), we obtain
VT>AT-1VT >
(1∕(2K)- 2Γ(do))2 ≥
7∕K2 + 16t ∙ Γ(d0)2 ≥
(1∕(2K)- 1∕(4K))2
7∕K2 + 1∕(4K2)
> 1∕100.
Following the same argument in (C.14)-(C.20) in the proof of Lemma C.1, we obtain
t ≤ C10 ∙ log2(1∕γ)∕γ ∙ log1+1/Y(KRH)
for an absolute constant C10 > 0. Thus, we conclude the proof of Lemma D.1.
□
Lemma D.2. Under Assumption 4.6 and 4.4, there exists an absolute constant C11 such that we have
Ni/(KH)(P, k ∙ k∞,ι) ≤ C11 ∙ log2(1∕γ)∕γ ∙ log1+1/Y(RT)
for any R ≥ 2.
Proof. By choosing t = 4KH in Lemma F.7, there exists do =「C ∙log(1∕γ)∕γ ∙log1/Y(4KRH2)],
where Ce is defined in Lemma F.7, such that d0Y ≥ 4(1 - γ)(γC2)-1 and
Γ(d0) ≤ 1∕(4KH).	(D.12)
Here Γ(d0) is defined in (C.4), γ and C2 are defined in Assumption 4.6. Let C be a minimal
1∕(4d0/2KH)-covering of the Euclidean ball {v ∈ Rd0 : kv∣∣2 ≤ R} with respect to the '2-norm
distance. For any P ∈ P with parameter w, we define P and P by
P(X) = VwNN(x; w0)τ(w — w0),
d0
P(X) = X λj ∙hP,φjiH ∙ φj (x).
j=1
for any X ∈ Y . Also, we write
V = (∕λ1 ∙hP, φ1iH, ..., Vλd0 ∙ hP, φd0)H).
22
Under review as a conference paper at ICLR 2021
Note that, because {y∕λj ∙ φj}∞=ι is an orthonormal basis in H and W ∈ Br, We have
〜 —
kvk2 = kPekH ≤ kPkH ≤ R.
Thus, there exists v* ∈ C with ∣∣v - v*∣∣2 ≤ 1/(4d1/2KH). We define P * ∈ HR by
d0
P*(X) = Xv* ∙ Fj ∙ φj(X).
j=1
By the same arguments in (C.22) and (C.23), for any X ∈ Y, we have
|P(x) - P(x)∣ ≤ Γ(do), |P(x) - P*(x)∣ ≤ 1/(4KH).	(D.13)
Then, by (D.12), (D.13), and Condition 4.4, for any X ∈ Y, it holds that
|P(x) - P*(x)∣ ≤ |P(x) - P(x)∣ + |P(x) - P(x)∣ + |P(x) - P*(x)∣
≤ ξm + Γ(d0) + 1/(4KH) ≤ 3/(4KH).	(D.14)
Moreover, since P* ∈ HR, there exists a wv ∈ BR such that
P*(x) = VwNN(x; w0)>(wv - W0)
for any X ∈ Y. By Condition 4.4, we have
|P* (X) - NN(X; Wv)| ≤ ξm ≤ 1/(4KH).	(D.15)
Combining (D.14) and (D.15), we have
|P (X) -NN(X;Wv)| ≤ 1/(KH).	(D.16)
Note that because entries of VwNN(∙; w0) are linearly independent, Wv is unique for any V : ∣∣v∣∣2 ≤
R. We define
PC = P : ∃ v ∈ C s.t. P(X) = NN(X; Wv) for any X ∈ Y .
Then, by (D.16), PC is a 1∕(KH)-covering of P with respect to the '∞-norm distance. Following
the same argument of (C.25), we have
N1/(KH)(P, k∙ k∞,1) ≤ N1/(KH)(P, k ∙ k∞)
≤ |Pc| ≤ |C| ≤ C13 ∙ d0 ∙ log(d0KRH) ≤ C11 ∙ log2(1∕γ)∕γ ∙ log1+1/Y(KRH),
where C13, C11 > 0 are absolute constants. Thus, we conclude the proof of Lemma D.2.	□
Proof of Theorem 4.7
Proof. Recall that d = K ∧ dimE(ZP, 1/K). By the result ofa general P in Theorem 4.12, we have
Regret(T) ≤ √2H3T ∙ log |A| + √32H2T ∙ log(p∕2) + H(dH +1) + 4pβHT,	(D.17)
with probability at least (1 - p), when we set α = Vz2log A∣∕(KH2) and
β ≥ 2H2 ∙ log(Nι∕τ(P, ∣∣∙ ∣∞,ι) ∙ 2H∕p) + 4(H + √H2∕4 ∙ log(8K2H∕p))	(D.18)
By Lemma D.2, we have
N1∕(KH)(P, k∙ k∞,ι) ≤ C11 ∙ log2(1∕γ)∕γ ∙ log1+1∕γ(RT),
23
Under review as a conference paper at ICLR 2021
which implies that there exists an absolute constant C5 such that
2H2 ∙ log(Nι∕(κH)(P, k ∙ k∞,ι) ∙ 2H/p) +4(H + PH2/4 ∙ log(8K2H/p))
≤ C5H2 ∙ log2(1∕γ)∕γ ∙ log1+1∕γ(RT∕p).
In other words, (D.18) holds if we set
β ≥ C5H2 ∙ log2(1∕γ)∕γ ∙ log1+1/Y(RT∕p).	(D.19)
On the other hand, by Lemma D.1, we have
d ≤ C10 ∙ log2(1∕γ)∕γ ∙ log1+1/Y(RT).	(D.20)
Plugging (D.19) and (D.20) into (D.17), we obtain
Regret(T) ≤ √2H3T log |A| + √32H2T log(p∕2) + H
+ C10H2 ∙ log2(1∕γ)∕γ ∙ log1+1∕γ(RT)
+ 4√C5C10H3T ∙ log2(1∕γ)∕γ ∙ log(1+1/Y)/2(RT) ∙ log(1+1∕γ"2(RT∕p)
≤ C6√H3T ∙ log2(1∕γ)∕γ ∙ log1"(∣A∣RT∕p)
with probability at least (1 - p), where C6 > 0 is an absolute constant. Thus, we conclude the proof
ofTheorem4.7.	□
E Implicit Linearization
Two-layer fully-connected neural networks: A two-layer fully-connected neural network is de-
fined by
1	m/dY
NN(x; W) =	^X bj ∙ σ(x>Wj),	(E.1)
m∕dY j=1
where, without loss of generality, we assume m is integer times of 2dY . Here σ is the activation
function. The weight vectors w and b corresponding to the first layer and second layer, respectively,
take the form
W = (w>,…，w'/dY )T ∈ Rm,	b = (bl,..., bm/dY )τ ∈ Rm/dY ,
respectively. During the training, we only tune the weights in w.
Symmetric Initialization: When initializing the neural network, we generate the initial weight
vectors W0 and b by
W0 i吟 N(0,1∕dγ ∙ IdY
×dY ),	Wj +m/(2dY) = Wj ,
bj i%Unif({-1,1}), bj+m/(2dY)= -bj
for any j ∈ [m∕(2dγ)]. As a result of such initialization, We have NN(∙; w0) = 0 and We can
generalize the result to multilayer neural networks by setting the last two layers in this manner.
Proof of Lemma 4.5
Proof. Let NN be a tWo-layer fully-connected neural netWork in the form (E.1). The activation
function σ is 1-smooth and the second layer Weights satisfies bj ∈ {-1, 1] for any j ∈ [m∕dY].
24
Under review as a conference paper at ICLR 2021
For any w ∈ Rm such that kw - w0 k2 ≤ R, we have
kVw NN(x; W)-Vw NN(x; w0)k2
1 m/dY
=m/d~ ΣS kbjσ0(X>wj) ∙ x- %/(XTwO) ∙ xk2
m/dY
=-T X (σ0(XTwj) — σ0(X>w0))2 Ybj)2 ∙ kx∣∣2
m/dY j=1
Note that, by the assumption that σ is 1-smooth, we have
(σ0(x>wj) — σ0(x>w0))2 ≤ (x>wj - x>w0)2 ≤ ∣∣wj — w0k2.
Thus, we have
∣VwNN(X; w) - VwNN(X; w0)∣22
m/dY
≤m∕-Y X kwj-
wj0 ∣22 ≤ -YR2 /m.
(E.2)
By the mean value theorem, for any w : ∣w — w0∣2 ≤ R, there exists wt : ∣wt — w0∣2 ≤ R such
that
NN(X; w) — NN(X; w0)
= VwNN(X; wt)T(w — w0)
=Vw NN(x; w0)>(w — w0) + (Vw NN(x; wt) — Vw NN(x; w0))>(w — w0),
combining which with (E.2) we obtain
|NN(X; w) — VwNN(X; w0)T(w — w0)|
= |NN(X; w) — NN(X; w0) — VwNN(X; w0)T(w — w0)|
=I(Vw NN(x; wt) — Vw NN(x; w0))>(w — w0)∣
≤ ∣∣VwNN(x;wt) — VwNN(x;w0)∣2 ∙ ∣w — w0∣2 ≤ -Y 1/2R2m-1/2.
Therefore, we have ξm ≤ -Y 1/2R2m-1/2 . Then, Condition 4.4 holds when
m ≥ -YR4K3H2 .
Thus, We conclude the proof of Lemma 4.5.	□
F Supporting Lemmas
F.1 Decomposition
For notational simplicity, We define the linear operator Jkh for (k, h) ∈ [K] × [H] by
(Jhf )(s) = E[f(s,a) | a 〜∏k(∙ | s)], for any s ∈ S, f ∈ [0,H]s×a.
Lemma F.1 (Martingale Decomposition). For any k ∈ [K], We have
HH
VIk(Sk)- Vιπk(Sk) = X(DkJ + O/) + X (Qh(Sh,ah) — (rk(sh,ah)+PhVk+ι(sh,ah))),
h=1	h=1
25
Under review as a conference paper at ICLR 2021
where Dhk,1 and Dhk,2 take the forms
Dk,ι = (Jh(Qh - Qa))(Sh)-(Qh - Qa )(sh, ah),
kk
Dhk,2 =(PhVhk+1-PhVhπ+1,k)(skh,akh) -(Vhk+1-Vhπ+1,k)(skh+1).
Moreover, we have DHk ,2 = 0 for any k ∈ [K], and the sequence
D11,1, D11,2 +	D21,1, D21,2	+ D31,1,	.	. .,	DH1 -1,2 +	DH1 ,1,
D12,1, D12,2 +	D22,1, D22,2	+ D32,1,	.	. .,	DH2 -1,2 +	DH2 ,1,
.....	(F.1)
is a martingale difference sequence with respect to the filtration {Ft }t≥1 in Definition F.2 and each
term is bounded by 4H .
Proof. For any (k, h) ∈ [K] × [H], by the definition of the operator Jkh, we have
Vhk(skh) -Vhπk(skh)
= (JkhQkh)(skh) - (JkhQπhk,k)(skh)
=(Qh -Qnk ,k )(sh, ah) + (Jh(Qh - Q∏k,k))(sh) - (Qh - Q∏k,k)(sh, ah),	一z
'------------------{z-------------------}
= Dhk,1
where we denote the second term on the right-hand side by Dhk,1 . Also, we have
(Qkh - Qπhk,k)(skh, akh)
k
= (Qkh -rhk -PhVhπ+1,k)(skh,akh)
k
= (PhVhk+1 -PhVhπ+1,k)(skh,akh)+(Qkh-rhk-PhVhk+1)(skh,akh)
k
=Vhk+1(skh+1)-Vhπ+1,k(skh+1)+(Qkh-rhk-PhVhk+1)(skh,akh)
kk
+(PhVhk+1-PhVhπ+1,k)(skh,akh)-(Vhk+1-Vhπ+1,k)(skh+1),	(F.3)
'----------------------{z---------------------}
= Dhk,2
where we denote the third term on the right-hand side by Dhk,2 . Combining (F.2) and (F.3) we obtain
k
(Vh(Sh)- Vh (sh)) -(Vh+1(sh+1) - Vh+1k(sh+ι))
= Dhk,1 + Dhk,2 + (Qkh -rhk -PhVhk+1)(skh,akh).	(F.4)
Note that V⅛+ι(∙) = 0 for any k ∈ [K]. Using the identity (F.4) for h ∈ [H] We have
V1k(S1k) -V1πk(S1k)
H
=X(Vh (sh) - Vnk (sh)) - (vk+ι(sh+ι) - vh+ιk (sh+ι))
h=1
H
=X(Dlh,1 + Dk,2 + (Qh - rh - PhVh+ι)(sh, ah))
h=1
HH
=X(Dhj + Dh,2) + X(Qlh(Sh ah) - (rh(sh, ah) + PhVhh+ι(sh, ah))).
h=1	h=1
26
Under review as a conference paper at ICLR 2021
In the sequel, we show that the sequence in (F.1) is a bounded martingale difference sequence with
respect to the filtration {Fet}t≥1. For any (k, h) ∈ [K] × [H], by the definitions of Dhk,1 and Dhk,2,
we have
|Dhk,1 + Dhk,2| ≤ |Dhk,1| + |Dhk,2| ≤4H.
When h = 1, we have
E[Dk,ι | Fei(k,i)-1↑ = E[(Jk(Qk - Q∏k,k))(sk) - (Qk -Q%k)(s1 ,ak) I Fei(k-1,H)]
kk
=(Jk (Qk - qπ ,k ))(sk)- Jk (Qk - qπ ,k ))(sk)=0.
Here the second equality is because the only randomness conditional on Fei(k-ι,H) is ak 〜∏k(∙ | sk),
πk k	k	k
which is because, by our definitions of Q1π ,k, Fi(k-1,H), Q1k, and πk, we have
Qk(∙),Q∏k,k(∙) ∈Fi(k-ι,H).
Similarly, when h ≥ 2, we have
E[Dhk-1,2 | Fei(k,h)-1]	(F.5)
= E[(Ph-1Vhk - Ph-1Vhπk,k)(skh-1, akh-1) - (Vhk - Vhπk,k)(skh) | Fei(k,h-1)]
= (Ph-1Vhk - Ph-1Vhπk,k)(skh-1, akh-1) - (Ph-1Vhk - Ph-1Vhπk,k)(skh-1, akh-1) = 0,
which is because the only randomness conditional on Fei(k,h-i) is Sh 〜Ph(∙ | sh-ι, ah-ι) and We
have
E[Dhk,1 | Fei(k,h)-1] = EE[Dhk,1 | Fei(k,h)-1, skh] II Fei(k,h)-1] =0.	(F.6)
Combining (F.5) and (F.6), we obtain
E[Dhk-1,2 + Dhk,1 | Fei(k,h)-1] =0.
Therefore, we conclude the proof of Lemma F.1.	口
Definition F.2 (Filtration). We define the time index map i(∙, ∙) by
i(k, h) = H ∙ (j - 1) + h,
for any (k, h) ∈ [K] × [H], which is a bijection from [K] × [H] to [KH]. Then, for any (τ, h) ∈
[K] × [H], we define Ft(k,h) as the σ-algebra generated by
1	1	1	1	1	2	2	2	τ1 τ1 τ τ τ	τ τ
(r ,s1,a1,…，sH ,aH,r ,sl,a1, ∙∙∙ ,sH ,aH ,r ,s1,a1,…，sh,ah),
when h ≤ H - 1, which are the reward functions and state-action pairs determined before sτh+1, and
(r1,s1,a1, ∙∙∙ ,sH, aH, r2, s2,a2, ∙∙∙ ,sH,aH,rτ+1),
when h = H, which are the reward functions and state-action pairs determined before sτ1+1. Then, the
sequence {Fet}t≥1 forms a filtration. Note that rτ = {rhτ }hH=1 are determined before the beginning
of the τ -th episode although they are revealed to the agent until the τ-th episode ends.
F.2 Concerntration
Let {(Xτ , Yτ )}τ ≥1 be a sequence of random elements in X × R for some measurable set X. Let Z
be a set of [0, C]-valued measurable functions with domain X for some C > 0. Let F = {Fτ }τ ≥1
be a filtration such that for any T ≥ 1, (Xi, Yι, ∙∙∙ , XT-ι, Yr-ι,Xτ) is FT-ι-measurable and
there exists z* ∈ Z such that E[YT | FT-ι] = z*(Xτ) holds. The least-squares predictor b given
27
Under review as a conference paper at ICLR 2021
{(Xτ , Yτ)}tτ =1 is defined by
t
bt = argmin X(Z(XT) - YT )2.	(F.7)
z∈Z τ=1
We say that η is conditionally σ-sub-Gaussian given FT ∈ F for any τ ≥ 1 if for any λ ∈ R,
logE[exp(λη) | Ft] ≤ λ2σ2∕2.
For any ε > 0, we denote by Nε(Z, ∣∣∙∣∣∞) the ε-covering number of Z with respect to the supremum
norm distance kz1 - z2 k∞ = supx∈R |z1(x) - z2(x)|. For any β > 0, we define
t
Zt(β) = Z∈ ∈ Z ： X(Z(XT) - bt(Xτ))2 ≤ β}.	(F.8)
T=1
Lemma F.3 (Proposition 6 of Russo & Van Roy (2014)). Assume that for any t ∈ N, Yt - z* (Xt) is
conditionally σ-sub-Gaussian given Ft-1. Then, for any ε > 0 and δ ∈ [0, 1], with probability at
least (1 - δ), it holds that Z* ∈ Zt(βt(δ, ε)) for any t ∈ N, where
βt(δ,ε) = 8σ2 ∙log(M(Z, k ∙ k∞)∕δ) +4tε(C + √σ2 log(4t(t + 1)∕δ)).
Lemma F.4. For any δ ∈ [0, 1], if we let
β ≥ 2H2 ∙ log(Nι/(KH)(P, k∙k∞,ι) ∙ H∕δ) + 4(H + √H2/4 ∙ log(4K2H∕δ))
in Algorithm 1, then, with probability at least (1 - δ), for any (k, h) ∈ [K] × [H], we have Ph ∈ Phk.
Proof. Recall that, for any p ∈ P, we define ZP : S × A × [0, H]S → [0, H] by
zP(s,a,V (∙)) = / V (s0) ∙ P (s01 s,a)ds0, ∀(s,a,V (∙)) ∈S×A× [0,H]s.
Let Z = ZP = {ZP : P ∈ P}. For any (k, h) ∈ [K] × [H], we set Yk = Vhk+1(skh+1), Xk =
(sh,ah, Vh+ι(∙)), and z* = ZPh. Then, Yr - z*(Xτ) is conditionally H∕2-sub-Gaussian given
Fei(k,h) defined in Definition F.2. Then, by the definitions of Phk in (3.4) and Zk (β) in (F.8), we have
Zk(β) = {zP : P ∈ Phk}. By setting
β ≥ 2H2 ∙ log(N1∕κ(Z) ∙ H∕δ) +4(H + √H2∕4 ∙ log(4K2H∕δ))
in Algorithm 1, it holds that
β ≥ 2H2 ∙ log(N1∕κ(Z) ∙ H∕δ) + 4(k - 1)∕K ∙ (H + √H2∕4 ∙ log(4k(k - 1)H∕δ))
= βk-1(δ∕H, 1∕K)
for any k ∈ [K], where βk-1(δ∕H, 1∕K) is defined in Lemma F.3. Applying Lemma F.3 with
C = H, with probability at least (1 - δ∕H), for any k ∈ [K], we have
Z* ∈Zk(βk-ι(δ∕H,1∕K)) ⊂Zk(β),
which implies Ph ∈ Phk . Using the union bound over all h ∈ [H], with probability at least (1 - δ),
for any (k, H) ∈ [K] × [H], we have Ph ∈ Phk .
28
Under review as a conference paper at ICLR 2021
In the sequel, We prove that Nε(Z, k ∙ ∣∣∞) ≤ N£/h(P, k ∙ k∞,ι) for any ε > 0. Indeed, this is
obtained by the observation that, for any zP, zP0 ∈ Z with P, P0 ∈ P, we have
IlzP — zpo∣∣∞ =	sup	I [ V(s0) ∙ P(s0 | s,a)ds0 — V V(s0) ∙ P0(s0 | s,a)ds0
(s,a,V (∙))∈S×A×[0,H]s S	S
≤ sup H •八P(s01 s,a) — P0(s0 | s,a)∣ ds0 = H ∙∣∣P — P0∣∞,1.
(s,a)∈S ×A	S
Thus, We conclude the proof of Lemma F.4.
□
F.3 Eluder Dimension
Recall that Z is a set of [0, C]-valued functions With domain X for some C > 0. MeanWhile, Zk (β)
is defined in (F.8).
Lemma F.5 (Lemma 5 of Russo & Van Roy (2014)). For any β > 0, We have
K
X sup	∣z(xk) — Z(Xk)| ≤ 1 + C ∙ d + 4 ∙ pdβK,
k=1 z,z0 ∈Zk (β)
Where d = K ∧ dimE (Z, 1/K).
Proof. When dimE(Z, 1/K) ≤ K, by Lemma 5 of Russo & Van Roy (2014) We have
K
X sup	∣z(xk) — z0(xk)| ≤ 1 + C ∙ dimE(Z, 1/K) +4 ∙ pdimE(Z, 1/K)βK
k=1 z,z0 ∈Zk (β)
=1 + C ∙ d + 4 ∙ PdβK.
When dimE(Z, 1/K) > K, since Z is a set of [0, C]-valued functions and Zk(β) ⊂ Z for any k
and β , We have
K
^X	sup	∣z(xk) — Z(Xk)| ≤ KC ≤ 1 + C ∙ d + 4 ∙ PdβK.
k=1 z,z0 ∈Zk (β)
Thus, We conclude the proof of Lemma F.5.	□
F.4 Other Useful Inequalities
Lemma F.6. For any γ ∈ (0, 1/2), C2 > 0, and any d0 ∈ N such that
dY ≥ 4(1— γ)(γC2)-1,	(F.9)
it holds that
X exp(-C2jγ∕2) ≤ 4d1-γ(γC2)-1 ∙ exp(-C2dY∕2).
j>d0
Proof. By basic calculus We have
2d1-γ/(γC2) ∙ exp(—C2dY/2) = (-2t1-γ/(γC2) ∙ exp(—C2tγ∕2))∣∞=d0
∞
二 (1 — 2(1 — γ)t-γ/(γC2)) exp(—C2tγ/2) dt
d0
≥ (1 — 2(1 — γ)d-γ/(γC2)) ∙ Γ exp(—C2tγ/2) dt,
d0
29
Under review as a conference paper at ICLR 2021
where the inequality is because 1 - γ ≥ 0 and t-γ ≤ d0-γ for t ≥ d0. Thus, we obtain
[exp(-C2tγ/2)dt ≤ 1-2W)……≤ 4d1-γ YS ∙ exp(-C2dγ/2)，
where the last inequality is because 1 - 2(1 - γ)d-γ/(γC2) ≥ 1/2 by (F.9). Then, by the fact that
∞
exp(-C2jγ) ≤
j>d0	t=d0
exp(-C2 tγ) dt,
we conclude the proof of Lemma F.6.
□
Recall that Γ(d0) is defined in (C.4).
Lemma F.7. Let C1 and C2 be the absolute constants in Assumption 4.2. There exists an absolute
constant C such that for any γ∈ (0, 1/2), t ≥ 1, and R ≥ 2, ifwe set
do = dC ∙log(1∕γ)∕γ ∙ log1/Y(tRH)e,
then it holds that d0γ ≥ 4(1 - γ)(γC2 )-1 and
Γ(d0) = C1/2d0-YRH(γC2)-1 ∙ exp(-C2dγ∕2) ≤ 1∕t.	(F.10)
Proof. For any y > 0, we consider the function
f(x) = ex∕xy, x > 0.
Taking derivatives, we have
f0(x)
ex xy- (x - y)
x2y
Note that f0 (x) ≥ 0 if and only if x ≤ y, which implies f(x) ≥ f(y) for any x > 0. Reorganizing
the inequality, we obtain
ex ≥ (ex∕y)y
for any x > 0 and y > 0. Applying the above inequality via choosing
x = C2d0γ∕4,	y = (1 - γ)∕γ,
we obtain
d0-γ Cl/HR(lC2)-
exp(C2dγ/2)
=	d『	C1/HR(γC2)一
exp(C2dγ / 4) exp(C2dγ / 4)
≤	d『	ClIHR(YC/)- =	1	cj∕2HR(γC2)T
^ (eC2dγ/4 ∙ I-Y)宁	exP(c∣do/4)	― (eC//4 ∙ ɪ) 1-γ	exP(CIdY∕4)
Thus, to obtain (F.10), it suffices to make the following inequality hold,
exp(CIdoY/4) ≥
tC11l2RH
1 —Y
γC∣(eC∣∕4 ∙ I-Y)一
30
Under review as a conference paper at ICLR 2021
which is equivalent to
d0 ≥( C log -:HY)
、1∕γ
1-Y )
γ
Since γ ∈ (0, 1∕2) and tRH ≥ 2, there exists an absolute constant C such that
4/C2 ∙ log
4(1 - γ)(γC2 )-1
tC11/2HR
~ . . . .
≤ Ce • log(1∕γ)∕γ • log(tRH),
1 — Y
γC2(eC2∕4 ∙ ɪ) ɪ
~ . . . .
≤ Ce • log(1∕γ)∕γ • log(tRH).
EK f	KF	∙	7	、	/ Py 1	/ 1 /	∖ /	1	/ , τ~> ττ∖ ∖ 1 /-∙√	11,1	f f T
Therefore, by choosing do ≥ (Clog(1∕γ)∕γ • log(tRH))1/y, we conclude the proof of Lemma
□
F.7.
G Examples of Kernels with Exponentially Decaying Eigenvalues
In this section, we provide examples of kernels that satisfies Assumption 4.2. We let Y = SdY-1,
which represents the unit sphere in RdY . For any kernel K, we define the integral operator TK :
L2 (Y) → L2 (Y) by
(TKf )(x) = K Kse(χ,y)f (y)dμ(y), for any f ∈ L2(Y) and X ∈ Y
Y
where μ is the uniform measure on Y.
G.1 Squared Exponential Kernel
The squared exponential kernel is defined as
Kse(x,y) = exp{-1∕ι2 • ∣∣x - y∣∣2}, for any XJy ∈ Y,
(G.1)
(G.2)
where the constant ι satisfies ι2 ≥ 2∕dY. For any u ∈ [-1, 1], we define ke(u) = exp{-2(1 - u)∕ι2}
and
Pej(u)
(-1∕2)j∙Γ((dγ - 1)/2)
Γ((2j+ dγ - 1)/2)
• (1 - u2 )(3-dY)/2 ∙
j
[(1 - u2)j+(dY-3)/2],	(G.3)
where, with a slight abuse of notations, We use Γ(∙) to denote the Gamma function in this section.
Lemma G.1 (Theorem 2 of Minh et al. (2006)). For the kernel Kse defined in (G.2), the eigenvalues
{Pj }j ≥1 (without duplicates) of the corresponding integral operator TKse take the form
|SdY-2 |
pj = ∣sdγ-η
f ɪ Cx ,, 二 '	_ ,	,
• ke(u) • Pej (u; dY) • (1 -
u2)(dY-3)/2du,
and each ρj has multiplicity
N (j )=(2j + dY j-(2Y -d2) + j
- 3)!
(G.4)
Moreover, when ι2 ≥ 2/dY, we have that {Pj}j≥1 is in a decreasing order and satisfies
Pj > Al ∙
Pj < A2 ∙
∣2)j - (2j + dγ - 2)-(2j+dγT"2,
%)j • (2j + dγ - 2)-(2j+dY-1)/2,
(G.5)
31
Under review as a conference paper at ICLR 2021
for any j ≥ 1, where the constants A1, A2 only depend on dY and ι.
By Lemma G.1, the eigenvalues {λj}j≥1 (with duplicates) of the kernel Kse satisfy, for any j ≥ 1,
t-1	t
λj = ρt , for t such that XN(i) <j ≤ XN(i).
i=1	i=1
(G.6)
By the definition of N (j) in (G.4) and Stirling’s formula, we have
N(j)
j dγ - 2) ∙(dγ + j - 3)1/2 ∙ [(dγ + j - 3)/W(dY +j-3)
j1/2 ∙ (j/e)j
jdY-2
(G.7)
Here the asymptotic notation omits constant factors that are independent ofj. Combining (G.6)
and (G.7), whenjis sufficiently large, we have
λj = ρt, fort such that (t - 1)dY-1 <j≤ tdY-1.
Then, by (G.5) we obtain
λj = O(( -I2 )j Y ∙ (2j 1/dY + dγ - 2)-(2j1/dY +dγT"2) = O(e-cj1/dY )	(G.8)
asj→ ∞ for an absolute constant c > 0. Thus, we know Kse satisfies the second condition of
Assumption 4.2.
G.2 NTK of Sine Activation
We consider the neural tangent kernel of a two-layer neural network of the form (E.1) where the
activation function is the sine function. In detail, the neural network is parametrized as
NN(x; w,l) = J —-----—
m m∕(dγ + 1)
m/(dY +1)
E % ∙Sin(X>wj + lj)
j=1
for any x ∈ Y. Here we modify the initial form in (E.1) by adding an intercept term, which is
equivalent to adding one more dimension with constant value 1 to the input space. The initialization
of the network weights follows the same symmetric random initialization scheme
bji%Unif({-1,1}), bj+m/(2dY +2) =bj ,
wj i.妙 N (0, IdY ),	wj+m∕(2dγ +2) = Wj,
lj i% Unif([0, 2∏]), lj+m∕(2dγ + 2) = lj ,
for j ∈ [m/(2dY + 2)]. Here without loss of generality we assume m/(2dY + 2) ∈ N. Then, the
population NTK of such a parametrization takes the form
Kntk(X,y) = 2 ∙ EW〜IdY,L〜Unif([0,2∏])[x>y ∙ cos(x>W + L) ∙ cos(y>W + L)]
=χ>y ∙ eχp{-∣∣χ - yk2∕2}
=x>y ∙ exp{x>y — 1},
for any X, y ∈ Y, which is the limit of the empirical NTK defined in (4.2) asm goes to infinity. Here
the second equality is derived in Rahimi & Recht (2007) and the third equality is by the fact that
kXk2 = kyk2 = 1 for any X, y ∈ Y = SdY-1.
For any j ≥ 1, let Yj be the set of all homogeneous harmonics of degree j on SdY-1, which is a
finite-dimensional subspace of L，(SdY-1),the space of square-integrable functions on SdY-1 with
respect to μ. It can be shown that the dimensionality of Yj is given by N(j).
32
Under review as a conference paper at ICLR 2021
Lemma G.2 (Funk-Hecke formula (Muller (2012), page 30)). Let k? : [-1,1] → R be a continuous
function, which gives rise to an inner product kernel Ke on SdY -1 × SdY -1 with the definition
Ke(x, y) = ke2 (x>y), for any x, y ∈ SdY-1.
Then, for any j ≥ 1, f ∈ Yj, x ∈ SdY-1, we have
Zs"T Ke(x,y)f (y)dμ(y) = (ISdY-1| •/ J2(U) ∙ Pj(U) ∙(I- u2)(dY-3)/2 du) ∙f (X),
, 二，、. 一- -. ._ ..
where Pj(U) is defined in (G.3).
We let k2(u) = u ∙ exp{u — 1}. Recall the definition of k in Section G.1, We have k2(u) = U ∙ k(u)
for ∣ = √2, which satisfies the requirement in Lemma G.1. Lemma G.2 shows that the eigenvalues
{ρej}j≥1 (With duplicates) of TKe takes the form
ρj
CP ∙ Z e2(u) ∙ Pj(U) ∙ (1 - u2)(dY-3)/2 du
CP /u
• e(u) ∙ Pj(U) ∙ (1 - u2)(dY-3)/2 du,
where CP = |SdY-2|/|SdY-1|. Using the relation
U • P (U) =__j_____pe. 1 (U) + j + d - 2 • PaI(U)
P (U) = 2j+dγ- 2 PjT(U)+2j+dγ- 2 Pj+1(U),
which is from the definition of Pj (U), we have
j	Ij + dY - 2
Pj = Cρ • G + dγ - 2 • PjT + 2j+dγ - 2 • ρj+1),
where {ρj }j≥ι are the eigenvalues of the operator TKse studied in Section G.1 with ∣ = √2. Thus,
following the same argument of (G.6)-(G.8), we know such an NTK satisfies the second condition of
Assumption 4.2.
33