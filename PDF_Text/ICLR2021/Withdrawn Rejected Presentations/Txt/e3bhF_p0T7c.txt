Under review as a conference paper at ICLR 2021
Analysis of Alignment Phenomenon in Simple
Teacher-student Networks with Finite Width
Anonymous authors
Paper under double-blind review
Ab stract
Recent theoretical analysis suggests that ultra-wide neural networks always con-
verge to global minima near the initialization under first order methods. However,
the convergence property of neural networks with finite width could be very dif-
ferent. The simplest experiment with two-layer teacher-student networks shows
that the input weights of student neurons eventually align with one of the teacher
neurons. This suggests a distinct convergence nature for “not-too-wide” neural
networks that there might not be any local minima near the initialization. As the
theoretical justification, we prove that under the most basic settings, all student
neurons must align with the teacher neuron at any local minima. The method-
ology is extendable to more general cases, where the proof can be reduced to
analyzing the properties of a special class of functions that we call Angular Dis-
tance (AD) function. Finally, we demonstrate that these properties can be easily
verified numerically.
1	Introduction
The theoretical understanding to the training of neural networks has been a major and long-standing
challenge. A recent line of research (Du et al., 2019c;b;a; Arora et al., 2019a;b) presented a major
theoretical breakthrough via an elegant approach to characterize the training procedure of ultra-
wide neural networks. At the high level, the training loss profile of an ultra-wide neural network
(m = Ω(N6), N is the size of the training Set) uniformly converges to zero. During the training
procedure, almost all neurons’ weight vectors remain close to their initialization, which in term
preserves the uniform converging rate. As a result, the training loss converges to zero with almost
all neurons’ weight vectors near their initialization.
Does the elegant theory reveal the fundamental mechanics behind the success of practical neural
networks whose widths are finite (m = O(N))? The theory suggests a clear property that is crit-
ical to the uniform convergence: The weight vectors of the neurons rarely move away from their
initialization.
In this paper, we first examine this conjecture with experiments. Unfortunately, a simple experiment
with two-layer teacher-student networks exhibits contradicting properties: Despite of their random-
ized initialization, the weight vectors of the student neurons eventually align with the weight vector
of one of the teacher neurons.
In other words, almost all the student weight vectors end up somewhere far away from where they
begin: from the randomized initialization to some specific teacher weight vector. We emphasize
that such experiments with teacher-student models are generic in the sense that, according to the
universal approximation theorem, any target function can be equivalently described as a two-layer
neural network with ReLU activation. Although such a teacher network may not be accessible for an
arbitrary dataset, the teacher-student model is sufficiently representative for empirical justification.
Similar alignments are also observed for over-parameterized student and teacher networks of multi-
ple layers (Tian, 2020). In other words, the neuron weights alignment might be the more appropriate
fundamental mechanics for neural networks with finite width. In fact, once the student-teacher align-
ment has been established, it is straightforward that the training loss converges to zero.
We then investigate the other direction of the observation: Does the convergence of gradient descent
in this case imply a perfect alignment between the student and the teacher neurons? To the best
1
Under review as a conference paper at ICLR 2021
Figure 1: The weight vectors of teacher neurons (dotted lines) and student neurons (solid lines) in
epoch 0, 10, 1000, 5000, 50000 respectively.
of our knowledge, the theoretical understanding to this question is extremely limited even for the
simplest 1-teacher 2-student case.
In this paper, we initiate the study of the fundamental yet highly non-trivial question with the most
basic 1-teacher m-student setting. Both the teacher and students are two-layer neural networks
with ReLU activation. Mathematically, the question is equivalent to whether there exist any non-
alignment solutions to the set of equations defining zero gradients.
We demonstrate a complete proof of non-existence for m = 2, and rule out the existence of solutions
with special form for general m ≥ 3: (i) all student neuron weight vectors fall in the same half-plane;
or (ii) the angles between any two weight vectors are rational multiples of π.
For general cases, we show that the theoretical problem can be reduced to the analysis of a special
class of functions which we call angular-distance (AD) functions: If one of the associated matrices
of the AD function has a non-positive determinant, then there is no non-trivial solution to the set of
equations. Hence no local minima without perfect alignment.
In light of the reduction, we numerically examined the property that the minimum determinant of
the associated matrices are always non-positive unless perfect alignment or problem degenerated.
Moreover, the minimum determinant behaves as a potential function in the sense that the further
from being degenerated, the further from being non-negative.
1.1	Related works
Recent papers have made tremendous progress on the theoretical analysis of sufficiently over-
parametrized neural networks or those with even infinite width. With some mild assumptions, it
has been proved that the training error can uniformly converge to zero via gradient descent (Allen-
Zhu et al., 2019b; Du et al., 2018b; 2019c;b; Du and Hu, 2019; Lee et al., 2019; Li and Liang,
2018; Zou et al., 2018). Surprisingly, such a uniform convergence implies there always exist global
minima near a random initialization, generalization bounds also emerge under the same frameworks
(Allen-Zhu et al., 2019a; Arora et al., 2019a; Cao and Gu, 2019). Moreover, as the network width
approaches to infinity, fundamental connections between over-parameterized neural networks and
kernel methods are also discovered, which hint potential correspondence between the generality of
deep learning and that of kernel methods (Arora et al., 2019b; Daniely et al., 2016; Daniely, 2017;
Du et al., 2019a; Hazan and Jaakkola, 2015; Jacot et al., 2018; Mairal et al., 2014; Yang, 2019).
Despite the beauty of the theoretical results, empirical studies exhibit certain mismatches with the
theory. Nagarajan and Kolter (2019) shows that the uniform convergence property may be insuffi-
cient to explain the generalization ability of neural network. Moreover, Tian (2020) considers using
teacher networks to train over-parametrized student networks of the same depth and demonstrates
that almost all student neurons either align with some teacher neuron or have no contribution to
the final output. Our theoretical results derive from analyzing the behaviour of gradient descent as-
suming that the input distribution is Gaussian, following the methodology commonly used in many
recent works (Brutzkus and Globerson, 2017; Du et al., 2017; Du and Lee, 2018; Du et al., 2018a;c;
Li and Yuan, 2017; Tian, 2017; Zhong et al., 2017).
2
Under review as a conference paper at ICLR 2021
2	Preliminaries
Consider training a two-layer student network with a two-layer teacher network, where the teacher
network has only one neuron in its hidden layer while the student network has m neurons in its
hidden layer. Both networks have ReLU activation and we fix their weights of output layers as 1
to simplify the notations. Let w0 ∈ Rd denote the weights of the teacher’s hidden neuron, and
w1 , . . . , wm ∈ Rd denote the weights of each student hidden neuron, respectively. Formally, the
output of the teacher and the student network are
FteaCher(x, W) = σ(wTX),	FstUdent(x, W) = σ(wTx) +----+ σ(wTTX).
Let θij ∈ [0, π] be the angle between wi and wj , i.e.,
arccos
Wi ∙ Wj
kwik2kwj l∣2
ui = Wi /kWi k2 be the unit-length vector with the same direction as Wi , and ρi = kWi k2 be the
length of Wi . In particular, for the 2-dimensional cases, i.e., d = 2, we use θi ∈ [0, 2π) to denote
the polar angle of Wi, and u(θ) to denote the unit-length vector with polar angle θ.
A student neuron Wi is called aligned with the teacher, if θ0i = 0, or negatively aligned with the
teacher, if θ0i = π. For the 2-dimensional cases, i.e., d = 2, being aligned is equivalent to θi = θ0.
Assume the data follows the normal distribution, i.e., X 〜N(0, I), then the population loss is
I = Ex〜N(0,I) h 1 (σ(wTx) - Pm=1 σ(wTx))2i .
The partial derivative of the squared loss with respect to each student neuron Wi is (Brutzkus and
Globerson (2017))
生=X(
d wi	j=1V
π - θij	ρj	π - θi0	ρ0
-^wwj + 4∏ Sm θijUi) - (-π^wwo + 4∏ Sin θi0Ui J .	(I)
In other words, the training process terminates, i.e., the gradient is zero, if and only if
(π - θi0 )w0 = πρi - ρ0 Sin θi0 + ρj Sin θij Ui +	(π - θij)wj, ∀i.	(2)
j 6=i	j6=i
Observation 2.1. If the following two conditions are met, then equation 2 are satisfied:
1.	all student neurons (ρi > 0) are aligned with the teacher,
2.	and the sum of their lengths equals to ρ0 .
In this paper, we argue that these two conditions are also necessary for equation 2 to stand. In the
remaining part of this paper, we always assume d = 2.
3 Alignment phenomenon and the basic case
As mentioned, simple experiments show that the student neurons eventually align with teacher neu-
rons under gradient descent. Figure 1 illustrates an example in which the teacher network has 5
neurons. While training via gradient descent with data generated from N (0, I), the randomly initial-
ized student neurons gradually get aligned with one of the teacher neurons. We call it the alignment
phenomenon.
In this section, we prove for the most basic case with m ≤ 2.
3
Under review as a conference paper at ICLR 2021
3.1	BASIC CASE WITH m ≤ 2
Note that when m = 1, the only solution to equation 2 is that the student perfectly aligns with the
teacher, i.e., w1 = w0. Because (π - θ10)w0 = (πρ1 - ρ0 sin θ10)u1 implies that w0 and u1 point
towards the same direction. Thus, θ10 = 0, which means that πw0 = πρ1u1 = πw1.
However, the proof of the following theorem for m = 2 becomes quite non-trivial.
Theorem 3.1. When m = 2, there is no other solution to equation 2 except the trivial one, i.e.,
θ01 = θ02 = 0 and ρ1 + ρ2 = ρ0.
Therefore, once the training converged, the student neuron(s) must be aligned with the teacher
neuron.
Due to the space limit, we send the complete proof to Appendix A, while highlight the key steps
here.
Proof sketch. Suppose there exists a solution with θ12 6= 0. In this case, the two equations in
equation 2 imply two decomposition of w0 onto u1 and u2 . Because such a decomposition is
unique for m
we have
2 = d, we know the coefficients of w0, u1, u2 must be proportional. From there,
w0	w1	w2
π - θ12	π - θ20	π - θ10 .
By projecting this equation onto w0 and the orthogonal direction of w0 , we can further conclude
that
Po _ ρi _ P2
(∏ - θ12) Sin Θ12	(∏ - θ20) Sin θ20	(∏ - θ10) Sin θ10 .
By plugging the above equation into equation 2, we can reach a contradiction and complete the
proof.	□
Note that a major challenge of extending this proof to m ≥ 3 is that the decomposition is not unique
anymore, which breaks the first step of the proof.
Even though, it is still useful to notice that whenever two student neurons get aligned, i.e., θij = 0,
they can be replaced by a single neuron i* of the same direction and unified length, ρi* = Pi + Pj,
so that any such a non-trivial solution to m = k implies a non-trivial solution to m = k - 1.
Therefore, for general m ≥ 3, we can without loss of generality prove that there is no solution to
equation 2 with 0 = θ0 < θι < •…< θm < 2π and ρ0 = 1, ρι,...,Pm > 0.
4 Special cases
In this section, we prove that for m ≥ 3, there is no solution to equation 2 satisfying either of the
two special conditions:
•	Students in a half-plane: 0 = θ0 < θι < •…< θm < π;
•	Polar angles being rational multiples ofπ: ∀i ∈ {1, . . . , m}, θi = qiπ for some rational
number qi .
4.1	Special case when all neurons are in the upper half-plane
We first show the following theorem.
Theorem 4.1. When all vectors of student neurons are in the upper half-plane, i.e., 0 = θ0 < θ1 <
∙…< θm < π, then equation System equation 2 has no non-trivial solution.
The rigorous proof of Theorem 4.1 is provided in Appendix B.3. Here, we present two lemmas of
function f(θ) = (π - θ) coS θ + Sin θ, and the proof of the following two lemmas can be found in
Appendix B.
Lemma 4.2. (decreasing lemma) f(θ) = Sin θ + (π - θ) coS θ decreases in the interval [0, π], and
f(0)=π,f(π)=0.
4
Under review as a conference paper at ICLR 2021
Lemma 4.3. (log-concave lemma) ∀α, β ∈ (0, π), and 0 < α + β < π, it holds that f(α)f(β) >
f(α+β)f(0).
The above two lemmas reveal some properties of function f(θ) = sin θ+ (π-θ) cos θ. In Section 5,
we will show another amazing property of f which correlates to positive semi-definite matrices, and
f also applies to analyze the phenomenon behaviour of our neural network in general case.
4.2	SPECIAL CASE WHEN ALL THE POLAR ANGLES ARE RATIONAL MULTIPLES OF π
In this subsection, we consider another special case, and prove that under this constraint, equation
system equation 2 has no non-trivial solution.
Theorem 4.4. When all polar angles are rational multiples ofπ, equation system equation 2 has no
non-trivial solution.
The proof of Theorem 4.4 is shown in Appendix C. The key point of the proof is the following
lemma.
Lemma 4.5. When 0 < θι < θ < •…< θm < 2π, and there does not exist i, j, s.t. θ% + π = θj,
then the following m × m matrix [π - θij]im,j =1 is positive-definite.
This lemma directly derives from Theorem 3.1 in Du et al. (2019c). Ifwe define f(θ) = π - θ, then
the associated function g is a PAD function (see Definition 5.7), which will be further discussed in
Section 5.
Moreover, in this subsection, we assume that all the polar angles are rational multiples of π. Since
rational numbers are dense, equation system equation 2 has no non-trivial solution ’almost every-
where’, which indicates that equation system equation 2 has no non-trivial solution without any
constraints.
5	Analysis of general case
The general case when m ≥ 3 without any special constraints is rather difficult to prove. However,
if determinant condition (see Theorem 5.1) always holds, then we can rigorously prove that the
alignment phenomenon always happens for arbitrary number of student neurons.
Moreover, the determinant condition can be generalized to a novel class of functions - Angular
Distance (AD) function (see Subsection 5.2). Since the proof of two special cases in Section 4 are
with the aid of AD functions directly or indirectly, and AD function actually appears in the previous
literature (e.g., Du et al. (2019c)) although implicitly, it is great likely that AD function can be
applied widely to theoretical analysis of neural networks.
5.1	Determinant condition
The following theorem clarifies the relationship between determinant condition and alignment phe-
nomenon.
Theorem 5.1. Define f (θ) = (π - θ)cos θ + sin θ. Let f = [f(θii),f(θ2i),…，f(θmi)]T for 0 ≤
i ≤ m. Also, define matrix Mi = [fι,…，fi-1, fo, fi+1,…，fm] and M = [fι, f2,…，fm]. If
the following condition (determinant condition) :
∀m ≥ 3, 0 = θo < θι < …< θm < 2π with no 0 ≤ i < j ≤ m s.t. θ% + π = θj,
it always holds that det M > 0 and there exists 1 ≤ i ≤ m, s.t. det Mi ≤ 0,
holds, then equation system equation 2 has no non-trivial solution, i.e., the student neural network
converges only when aligning.
Proof. We only consider the general case when m ≥ 3, since the basic case when m ≤ 2 has been
proved in Subsection 3.1.
Ifwe project i-th equation in equation systems equation 2 to the direction u(θi), we will obtain that
(π - θi0)cosθi0ρ0 = πρi - ρ0 sin θi0 +	ρj sin θij +	(π - θij)cosθijρj,∀i,
5
Under review as a conference paper at ICLR 2021
which is equivalent to
(sin θi0 + (π - θi0) cosθi0)ρ0 = πρi +	(sin θij + (π - θij) cosθij)ρj,∀i.
j6=i
Note that f(θ) = sinθ + (π - θ) cos θ, so equation 3 is equivalent to
(3)
f(θi0)ρ0 =	f (θij )ρj , ∀i.
j=1
(4)
We can express equation 4 in a matrix form
Mρ = b,
(5)
where
「 f(θ11)
f(θ21)
f(θ12)
f(θ22)
f(θlm) I
f(θ2m)
ρ1
ρ2
b = ρ0
「 f(θ10) I
f(θ20)
f(θm1)
f(θm2)…	f(θmm)」
ρm
f(θm0)
M
m
ρ
If det M > 0, then by Cramer,s rule, Pi = d^MMi. And since there exists 1 ≤ i ≤ m, s.t.
det Mi ≤ 0, thus ρi ≤ 0.
Determinant condition has elegant and simple form, and can be easily verified numerically (see Sec-
tion 6). Moreover, the analysis of function f(θ) = sin θ + (π - θ) cos θ and determinant condition,
can be generalized to a novel class of function - Angular Distance (AD) function, which will be
further discussed in Subsection 5.2.
5.2	AD function
In this subsection, we provide the formal definition of angular distance (AD) function, and analyze
the relationship between its proposition and the determinant condition in Theorem 5.1.
Definition 5.2. (AD function) Let f : [0, π] → R be a continuous function, and let g : [0, 2π] ×
[0, 2π] → R, where g(x, y) = f (min{|x - y| , 2π - |x - y|}). Then function g is an angular
distance(AD) function if∀h : [0, 2π] → R that is continuous and real-valued in [0, 2π], with h(0) =
h(2π), it holds that
x=0	y=0
h(x)g(x, y)h(y)dxdy ≥ 0.
Also, we call g the associated function of f.
Another important concept is the associated matrices ofan AD function.
Definition 5.3. (associated Matrix) Let g be an AD function and give (m + 1) fixed angles 0 =
θo < θι < θ < …< θm < 2π .Let gi = [g(θ1,θi),g(θ2,θi),…，g(θm,θi)]T for 0 ≤ i ≤ m.
Also, define matrix Mi = [gi,…，gi-ι, go, gi+ι,…，gm] and M = [gi, g2,…，gm]. We call
M, Mi,…，Mm associated matrices of g and given θo,θi,…，θm.
Along with the above two definitions, we provide a proposition and a conjecture of AD functions.
Proposition 5.4. (positive semi-definite) Given 0 = θo < θi < •…< θm, < 2π, for an ADfunction
g, its associated matrix M is positive semi-definite.
Conjecture 5.5. (non-positive determinant) For m ≥ 3, given 0 = θo < θi < •…< θm, < 2π,
for an AD function g, and its associated matrices Mi, M2, ∙∙∙ , Mm, then there exists i, where
1 ≤ i ≤ m, s.t. det Mi ≤ 0.
The proof of Proposition 5.4 is shown in Appendix D.1. For Conjecture 5.5, although we are not
able to prove it, some experimental results (see Section 6) show it is very likely that the conclusion
always holds.
For f(θ) = sin θ + (π - θ) cos θ, its associated function g that we have analyzed in Subsection 5.1
is actually an AD function by the following theorem, and the proof is presented in Appendix D.2.
6
Under review as a conference paper at ICLR 2021
Theorem 5.6. For f(θ) = sin θ + (π - θ) cos θ, the associated function g is an AD function.
Combining Proposition 5.4 (positive semi-definite), we have actually proved that part of determinant
condition (det M ≥ 0) holds theoretically.
Note that g(x, y) ≡ 0 is a trivial AD function, and its associated matrices are always 0. Obviously,
this function is not powerful enough, so we introduce a special class of AD function which is more
powerful.
Definition 5.7. (PAD function) An AD function g is called a positive-definite angular distance (PAD)
function if for any given angles 0 = θ0 < θ1 < •一< θm < 2π with no 1 ≤ i < j ≤ m s.t.
θi + π = θj, its associated matrix M is always positive-definite.
PAD function does exist. For example, if f (x) = π - x, then g is a PAD function by Theorem 3.1
in Du et al. (2019c).
If f(θ) = sinθ + (π - θ) cos θ, we have proved that its associated function g is an AD function, and
will show that g is a PAD function if the following conjecture holds.
Conjecture 5.8. (criterion of PAD) If an AD function g is the associated function of a strictly
decreasing function f, and ∀α, β s.t. α, β, α + β ∈ (0, π), it holds that f(α)f(β) > f(α + β)f (0),
then g is a PAD function.
If Conjecture 5.8 (criterion of PAD) holds, then by Lemma 4.2 (decreasing lemma) and Lemma 4.3
(log-concave lemma), g is a PAD function. Then by Definition 5.7 (PAD function) and Conjec-
ture 5.5 (non-positive determinant), the determinant condition in Theorem 5.1 always holds.
Therefore, Theorem 5.1 can be rewritten as the following form.
Theorem 5.9. If Conjecture 5.5 (non-positive determinant) and Conjecture 5.8 (criterion of PAD)
holds, then equation system equation 2 has no non-trivial solution, i.e., the above neural network
converge only if aligning.
Although we leave some conjectures in this subsection unproved, AD function is worth extensive re-
search, since the alignment phenomenon in slightly general case can be rigorously proved directly by
the conjectures concerned with AD function, and we actually apply decreasing, log-concave lemma
and definition of PAD function in the proof of two special cases in Section 4 respectively. Also,
AD function appeared implicitly in the previous literature, which indicates that AD function can not
only be applied to the theoretical analysis of alignment phenomenon, but also other behaviour of
neural networks.
6	Experiments
In this section, we show some experimental results to verify Conjecture 5.5 (non-positive deter-
minant) numerically. Since it is impossible to test all the AD functions, we select the associated
function g of f(θ) = sin(θ) + (π - θ) cos(θ) as a representative. Note that it is actually equivalent
to verify determinant condition in Theorem 5.1.
Figure 2: We fix θ1 = 1, 2, 3. θ2 and θ3 are variables. Also, z = min{det M1, det M2, det M3}.
First, we test the case when m = 3. Without loss of generality, we assume that θ0 = 0, then fix θ1
and rotate θ2, θ3 from 0 to 2π. We choose θ1 to be 1, 2 and 3, and draw the minimum value of the
7
Under review as a conference paper at ICLR 2021
determinant of three associated matrix of g, which is defined in Definition 5.3 (associated matrix).
The results are shown in Figure 2.
It is obvious that z is always non-positive. Also, we test the case when m = 4. We fix θ1 and θ4 .
The results are similar to the case when m = 3 and shown in Figure 3:
Figure 3:	We fix (θ1, θ4) = (1, 4), (1, 6), (2, 4). z = min{det M1, det M2, det M3 det M4}.
For more general cases, it is difficult to draw figures due to the high dimension. Therefore, we
randomly choose θ1,… ,θm and compute the minimum value of associated matrices Z for 100000
times for m = 6 and m = 9 respectively. Again, z is always non-positive (see Figure 4).
Figure 4:	For m = 6 and m = 9, we randomly choose θ1,∙∙∙ ,θm and compute Z =
min{det M1, ∙ ∙ ∙ , det Mm} for 100000 times respectively. For visualization, Z is clipped.
7 Conclusion
In this paper, we investigated the question that whether the elegant convergence theory for ultra-wide
neural networks (m = Ω(N6)) could apply to practical neural networks (m = O(N)). We observed
dissenting behaviors from the training in a simple two-layer teacher-student framework with ReLU
activation and Gaussian inputs: The student neurons all get aligned with one of the teacher neurons,
hinting that no local minima near any random initialization.
Following the empirical observation, we analyzed the local minima of the single-teacher multi-
student setting. We proved that there is no local minima except students being aligned with the
teacher for m ≤ 2 and excluded the existence of local minima taking certain special forms for
m ≥ 3: (i) all students falling in a half plane, or (ii) all students with polar angles being some
rational multiples of π .
Moreover, for general cases, we proposed a reduction from the existence of non-trivial local minima
(i.e., local minima without alignment) to analyzing certain mathematical properties of a class of
functions that we call angular distance (AD) functions. In particular, we showed strong empirical
evidence that the such properties hold for the specific AD function, which in term serve as a sufficient
condition for no existence of non-trivial local minima for general m ≥ 3.
8
Under review as a conference paper at ICLR 2021
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameter-
ized neural networks, going beyond two layers. In Advances in neural information processing
systems, pages 6155-6166, 20l9a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pages 242-252, 2019b.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of op-
timization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, pages 322-332, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang.
On exact computation with an infinitely wide neural net. In Advances in Neural Information
Processing Systems, pages 8139-8148, 2019b.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian
inputs. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pages 605-614. JMLR. org, 2017.
Yuan Cao and Quanquan Gu. A generalization theory of gradient descent for learning over-
parameterized deep relu networks. arXiv preprint arXiv:1902.01384, 2019.
Amit Daniely. Sgd learns the conjugate kernel class of the network. In Advances in Neural Infor-
mation Processing Systems, pages 2422-2430, 2017.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks:
The power of initialization and a dual view on expressivity. In Advances In Neural Information
Processing Systems, pages 2253-2261, 2016.
Simon Du and Wei Hu. Width provably matters in optimization for deep linear neural networks. In
International Conference on Machine Learning, pages 1655-1664, 2019.
Simon Du and Jason Lee. On the power of over-parametrization in neural networks with quadratic
activation. In International Conference on Machine Learning, pages 1329-1338, 2018.
Simon Du, Jason Lee, Yuandong Tian, Aarti Singh, and Barnabas Poczos. Gradient descent learns
one-hidden-layer cnn: Don’t be afraid of spurious local minima. In International Conference on
Machine Learning, pages 1339-1348, 2018a.
Simon S Du, Chi Jin, Jason D Lee, Michael I Jordan, Aarti Singh, and Barnabas Poczos. Gradient
descent can take exponential time to escape saddle points. In Advances in neural information
processing systems, pages 1067-1077, 2017.
Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous
models: Layers are automatically balanced. In Advances in Neural Information Processing Sys-
tems, pages 384-395, 2018b.
Simon S. Du, Jason D. Lee, and Yuandong Tian. When is a convolutional filter easy to learn? In
6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada,
April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018c.
Simon S. Du, Kangcheng Hou, RUslan Salakhutdinov, Barnabas Poczos, RUosong Wang, and Keyulu
Xu. Graph neural tangent kernel: Fusing graph neural networks with graph kernels. In Ad-
vances in Neural Information Processing Systems 32: Annual Conference on Neural Information
Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pages
5724-5734, 2019a.
Simon S. Du, Jason D. Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds
global minima of deep neural networks. In Proceedings of the 36th International Conference on
Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of
Proceedings of Machine Learning Research, pages 1675-1685. PMLR, 2019b.
9
Under review as a conference paper at ICLR 2021
Simon S. Du, XiyU Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent Provably optimizes
over-parameterized neural networks. In 7th International Conference on Learning Representa-
tions, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019c.
Tamir Hazan and Tommi Jaakkola. Steps toward deep kernel methods from infinite neural networks.
arXiv preprint arXiv:1508.05133, 2015.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing systems, pages
8571-8580, 2018.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in neural information processing systems, pages 8570-8581,
2019.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems, pages 8157-
8166, 2018.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation.
In Advances in neural information processing systems, pages 597-607, 2017.
Julien Mairal, Piotr Koniusz, Zaid Harchaoui, and Cordelia Schmid. Convolutional kernel networks.
In Advances in neural information processing systems, pages 2627-2635, 2014.
Vaishnavh Nagarajan and J Zico Kolter. Uniform convergence may be unable to explain general-
ization in deep learning. In Advances in Neural Information Processing Systems, pages 11611-
11622, 2019.
Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its
applications in convergence and critical point analysis. In Proceedings of the 34th International
Conference on Machine Learning-Volume 70, pages 3404-3413. JMLR. org, 2017.
Yuandong Tian. Student specialization in deep relu networks with finite width and input dimension.
In International Conference on Machine Learning, 2020.
Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760,
2019.
Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees
for one-hidden-layer neural networks. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pages 4140-4149. JMLR. org, 2017.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888, 2018.
10