Under review as a conference paper at ICLR 2021
Sample efficient Quality Diversity for neural
CONTINUOUS CONTROL
Anonymous authors
Paper under double-blind review
Ab stract
We propose a novel Deep Neuroevolution algorithm, qd-rl, that combines the
strengths of off-policy reinforcement learning (RL) algorithms and Quality Di-
versity (QD) approaches to solve continuous control problems with neural con-
trollers. The QD part contributes structural biases by decoupling the search for
diversity from the search for high return, resulting in efficient management of
the exploration-exploitation trade-off. The RL part contributes sample efficiency
by relying on off-policy gradient-based updates of the agents. More precisely, we
train a population of off-policy deep RL agents to simultaneously maximize diver-
sity within the population and the return of each individual agent. qd-rl selects
agents interchangeably from a Pareto front or from a Map-Elites grid, resulting in
stable and efficient population updates. Our experiments in the ant-maze and
ant- trap environments show that qd-rl can solve challenging exploration and
control problems with deceptive rewards while being two orders of magnitude
more sample efficient than the evolutionary counterpart.
1	Introduction
Natural evolution has the fascinating ability to produce organisms that are all high-performing in
their respective niche. Inspired by this ability to produce a tremendous diversity of living sys-
tems within one run, Quality-Diversity (QD) is a new family of optimization algorithms that aim at
searching for a collection of both diverse and high-performing solutions (Pugh et al., 2016). While
classic optimization methods focus on finding a single efficient solution, the role ofQD optimization
is to cover the range of possible solution types and to return the best solution for each type. This
process is sometimes referred to as ”illumination” in opposition to optimization, as the goal of these
algorithms is to reveal (or illuminate) a search space of interest (Mouret & Clune, 2015).
QD approaches generally build on black-box optimization methods such as evolutionary algorithms
to optimize a population of solutions (Cully & Demiris, 2017). These algorithms often rely on ran-
dom mutations to explore small search spaces but struggle when confronted to higher-dimensional
problems. As a result, QD approaches often scale poorly in large and continuous sequential decision
problems, where using controllers with many parameters such as deep neural networks is mandatory
(Colas et al., 2020). Besides, while evolutionary methods are the most valuable when the policy
gradient cannot be applied safely (Cully et al., 2015), in policy search problem that can be formal-
ized as a Markov Decision Process (MDP), Policy Gradient (PG) methods can exploit the analytical
structure of neural networks to more efficiently optimize their parameters. Therefore, it makes sense
to exploit these properties when the Markov assumption holds and the controller is a neural network.
From the deep reinforcement learning (RL) perspective, the focus on sparse or deceptive rewards
led to realize that maximizing diversity independently from rewards might be a good exploration
strategy (Lehman & Stanley, 2011a; Colas et al., 2018; Eysenbach et al., 2018). More recently, it
was established that if one can define a small behavior space or outcome space corresponding to
what matters to determine success, maximizing diversity in this space might be the optimal strategy
to find a sparse reward (Doncieux et al., 2019).
In this work, we are the first to combine QD methods with PG methods. From one side, our aim is to
strongly improve the sample efficiency of QD methods to get neural controllers solving continuous
action space MDPs. From the other side, itis to strongly improve the exploration capabilities of deep
RL methods in the context of sparse rewards or deceptive gradients problems, such as avoid traps
1
Under review as a conference paper at ICLR 2021
and dead-ends in navigation tasks. We build on off-policy PG methods to propose a new mutation
operator that takes into account the Markovian nature of the problem and analytically exploits the
known structure of the neural controller. Our qd-rl algorithm falls within the QD framework
described by Cully & Demiris (2017) and takes advantage of its powerful exploration capabilities,
but also demonstrates remarkable sample efficiency brought by off-policy RL methods. We compare
qd-rl to several recent algorithms that also combine a diversity objective and a return maximization
method, namely the NS-ES family (Conti et al., 2018) and the ME-ES algorithm (Colas et al., 2020)
and show that qd-rl is two orders of magnitude more sample efficient.
2	Problem Statement
We consider the general context of a fully observable Markov Decision Problem (MDP)
(S , A, T, R, γ, ρ0 ) where S is the state space, A is the action space, T : S × A → S is the
transition function, R : S × A → R is the reward function, γ is a discount factor and ρ0 is the initial
state distribution. We aim to find a set of parameters θ of a parameterized policy πθ : S → A so
as to maximize the objective function J (θ) = Eτ Pt γtrt where τ is a trajectory obtained from πθ
starting from state so 〜ρo and r is the reward obtained along this trajectory at time t. We define
the Q-value for policy π, Qπ : S × A → R as Qπ (s, a) = Eτ Pt γtrt, where τ is a trajectory
obtained from πθ starting from s and performing initial action a.
QD aims at evolving a set of solutions θ that are both diverse and high performing. To measure di-
versity, we first define a Behavior Descriptor (BD) space, which characterizes solutions in functional
terms, in addition to their score J (θ). We note bdθ the BD ofa solution θ. The solution BD space
is often designed using relevant features of the task. For instance, in robot navigation, a relevant BD
is the final position of the robot. In robot locomotion, it may rather be the position and/or velocity
of the robot center of gravity at specific times. From BDs, we define the diversity (or novelty) of a
solution as measuring the difference between its BD and those of the solutions obtained so far.
Additionally, we define a state Behavior Descriptor, or state BD, noted bdt . It is a set of relevant
features extracted from a state. From state BDs, we define the BD of a solution θ as a function
of all state BDs encountered by policy πθ when interacting with the environment, as illustrated in
Figure 1a. More formally, we note bdθ = IEτ [fbd({bd1, . . . , bdT})], where T is the trajectory length
and fbd is an aggregation function. For instance, fbd can average over state BDs or return only the
last state BD of the trajectory. If we consider again robot navigation, a state BD bdt may represent
the position of the robot at time t and the solution BD bdθ may be the final position of the robot.
With state BDs, we measure the novelty of a state relatively to all other seen states.
The way we compute diversity at the solution and the state levels is explained in Section 4.
3	Related work
A distinguishing feature of our approach is that we combine diversity seeking at the level of trajecto-
ries using solution BDs bdθ and diversity seeking in the state space using state BDs bdt . The former
is used to select agents from the archive in the QD part of the architecture, whereas the latter is used
during policy gradient steps in the RL part, see Figure 1b. We organize the literature review below
according to this split between two types of diversity seeking mechanisms.
Besides, some families of methods are related to our work in a lesser extent. This is the case of
algorithms combining evolutionary approaches and deep RL such as cem-rl (Pourchot & Sigaud,
2018), erl (Khadka & Tumer, 2018) and cerl (Khadka et al., 2019), algorithms maintaining a
population of RL agents for exploration without an explicit diversity criterion (Jaderberg et al.,
2017) or algorithms explicitly looking for diversity but in the action space rather than in the state
space like arac (Doan et al., 2019), p3s-td3 (Jung et al., 2020) and dvd (Parker-Holder et al.,
2020). We include cem-rl as one of our baselines.
Seeking for diversity and performance in the space of solutions Simultaneously maximizing
diversity and performance is the central goal of QD methods (Pugh et al., 2016; Cully & Demiris,
2017). Among the various possible combinations offered by the QD framework, Novelty Search
with Local Competition (NSLC) (Lehman & Stanley, 2011b) and MAP-Elites (ME) (Mouret &
2
Under review as a conference paper at ICLR 2021
Clune, 2015) are the two most popular algorithms. In ME, the BD space is discretized into a grid of
solution bins. Each bin is a niche of the BD space and the selection mechanism samples individuals
uniformly from all bins. On the other hand, NSLC builds on the Novelty Search (NS) algorithm
(Lehman & Stanley, 2011a) and maintains an unstructured archive of solutions selected for their
local performance. Cully & Mouret (2013) augment the NSLC archive by replacing solutions when
they outperform the already stored ones. In qd-rl, we build on the standard ME approach and
on an augmented version of the NSLC algorithm where the population is selected from a global
quality-diversity Pareto front inspired from Cully & Demiris (2017). Relying on these components,
algorithms such as qd-es and nsr-es have been applied to challenging continuous control environ-
ments in Conti et al. (2018). But, as outlined in Colas et al. (2020), these approaches are not sample
efficient and the diversity and environment reward functions could be mixed in a more efficient way.
In that respect, the most closely related work w.r.t. ours is Colas et al. (2020). The me-es algorithm
also optimizes both quality and diversity, using an archive, two ES populations and the map-elites
approach. Using such distributional ES methods has been shown to be critically more efficient than
population-based GA algorithms (Salimans et al., 2017), but our results show that they are still less
sample efficient than off-policy deep RL methods as they do not leverage the policy gradient.
Finally, similarly to us, the authors of Shi et al. (2020) try to combine novelty search and deep RL
by defining behavior descriptors and using an NS part on top of a deep RL part in their architecture.
In contrast with our work, the transfer from novel behaviors to reward efficient behaviors is obtained
through goal-conditioned policies, where the RL part uses goals corresponding to outcomes found
by the most novel agents in the population. But the deep RL part does not contain a diversity seeking
mechanism in the state space.
Seeking for diversity and performance in the state space Seeking for diversity in the space
of states or actions is generally framed into the RL framework. An exception is Stanton & Clune
(2016) who define a notion of intra-life novelty that is similar to our state novelty described in
Section 2. But their novelty relies on skills rather than states. Our work is also related to algorithms
using RL mechanisms to search for diversity only, such as Eysenbach et al. (2018); Pong et al.
(2019); Lee et al. (2019); Islam et al. (2019). These methods have proven useful in the sparse
reward case, but they are inherently limited when the reward signal can orient exploration, as they
ignore it. Other works sequentially combine diversity seeking and RL. The gep-pg algorithm Colas
et al. (2018) combines a diversity seeking component, namely Goal Exploration Processes (Forestier
et al., 2017) and the ddpg deep RL algorithm (Lillicrap et al., 2015). This sequential combination
of exploration-then-exploitation is also present in Go-Explore (Ecoffet et al., 2019) and in pbcs
(Matheron et al., 2020). Again, this approach is limited when the reward signal can help orienting
the exploration process towards a satisfactory solution. These sequential approaches first look for
diversity in the space of trajectories, then optimize performance in the state action space, whereas
we do so simultaneously in the space of trajectories and in the state space.
Thus, as far as we know, qd-rl is the first algorithm optimizing both diversity and performance in
the solution and in the state space, using a sample efficient off-policy deep RL method for the latter.
4	Methods
QD-RL evolves a population of N neural policies (θ1 , . . . , θN) so as to maximise their expected
return as well as the diversity within the population. It relies on an archive storing all candidate
solutions for selection and on a shared replay buffer storing all transitions (st , at , rt , bdt) collected
when training all agents. As illustrated in Figure 1b, one iteration consists of three successive phases:
population selection among the solutions in the archive, mutation of the population, and evaluation
of the newly obtained solutions. These phases are repeated until a maximum computational budget
is reached or until convergence.
Solution archive and selection mechanism. In this study, we consider two different QD configura-
tions for archive management and selection mechanism. While they rely on different assumptions,
we show in Section 5 that qd-rl manages to solve the tasks efficiently in both cases.
In the PARETO setting, we maintain an unstructured archive of solutions θ found so far as well as
their behavior descriptors bdθ . The archive is a limited size FIFO list. Furthermore, we implement
insertion rules inspired from Cully & Mouret (2013) to prevent adding too close solutions in the BD
3
Under review as a conference paper at ICLR 2021
(a)	(b)
Figure 1: (a): The RL part of qd-rl operates over time steps while its QD part operates at the
controller level, considering the MDP as a black box. (b) One qd-rl iteration consists of three
phases: 1) A new population of solutions is sampled from the QD Pareto Front or from the Map-
Elites grid. 2) These solutions are mutated by an off-policy RL agent: half of the solutions are
optimized for quality and the other half for diversity. The RL agent leverages one shared critic for
each objective. 3) The newly obtained solutions are evaluated in the environment. Transitions are
stored in a replay buffer while final scores and BDs of solutions are stored in the QD archive.
space, unless they offer better performance (see Appendix B for more details). To select solutions
from the archive, we compute a QD Pareto front of the solutions and sample the new population
from this front. We use the J (θ) objective function as the quality criterion, the diversity criterion is
computed as the average Euclidean distance between bdθ and its nearest neighbors in the archive.
In the me setting, the behavior descriptor space is discretized into a Cartesian grid that constitutes
the ME archive. We use the standard ME insertion criterion: when adding a solution θ into the grid,
we compute its BDs and find the corresponding bin. If the bin is empty, we insert the solution.
Otherwise, we add the solution only if its quality score is greater than the one of the contained
solution. To select a new population, we sample solutions uniformly from the grid.
Population mutation. During one iteration, we mutate each solution in the population to optimize
its quality with respect to the objective function, but also to optimize the population diversity. To de-
couple exploration from exploitation, we introduce two mutation operators optimizing respectively
for quality and diversity. Both operators rely on an off-policy deep RL algorithm to compute gra-
dients w.r.t. their respective criterion so as to to guide mutations towards improvement. While any
off-policy algorithm supporting continuous action could be used, we rely on Twin Delayed Deep
Deterministic Policy Gradient (TD3) (Fujimoto et al., 2018). To homogenize notations, we note rtD
for the novelty score and rtQ for the environment reward. The novelty of a state st at time step t
is computed as the mean Euclidean distance between its bdt and its nearest neighbors among BDs
seen so far. More formally,
1
k
rtD
k
dt, N eigh(A, bdt, i)
where d is the Euclidean distance, k the number of nearest neighbors, and N eigh(A, bdt , i) the i-th
nearest neighbor of state BD bdt in an archive A of state BDs. This archive is filled with all states
BDs collected by the population during training. As for the solutions archive, we use the insertion
rules inspired from Cully & Mouret (2013). Technically, as this archive is used by the RL part of
the algorithm, it is implemented as a sub-component of the experience replay buffer that receives
the same state BDs but filters them with the insertion rules.
We introduce two critic networks QwQ and QwD that are shared over the population and used by TD3
to optimize solutions respectively for quality and diversity. Some reasons for sharing a critic among
the population are given in Pourchot & Sigaud (2018). In our context, additional reasons come
4
Under review as a conference paper at ICLR 2021
from the fact that diversity is not stationary, as it depends on the current population. If each agent
had its own diversity critic, since an agent may not be selected for a large number of generations
before being selected again, its critic would evaluate it based on an outdated picture of the evolving
diversity. We tried this solution and it failed. Besides, as diversity is relative to the population and
not to an individual, it is more natural to have it centralized. A side benefit is that both shared critics
become accurate faster as they combine the experience of all agents.
At each iteration, qd-rl mutates half the population to maximise quality and the other half to
maximise diversity. As Colas et al. (2020), we use a 0.5 ratio to avoid biasing the algorithm towards
exploration or exploitation. We leave for further study the automatic adjustment of this ratio, but we
found that in practice qd-rl adequately balances both criteria during training.
To mutate a solution θ, we sample a batch of transitions from the replay buffer and use stochastic
gradient descent (SGD) where the gradient is computed as the td3 policy gradient, see Equation (1).
We respectively use the reward rtQ when optimizing for quality, the reward rtD when optimizing for
diversity, and their corresponding critics. Note that while the rtQ value is the one observed in the
environment, the rtD value changes across training time as new states are encountered. To deal with
this non-stationary reward, we recompute a fresh score rtD from the observed bdt every time a batch
is sampled. The gradient computation over solutions is performed in parallel. Every time a policy
gradient is computed, we also update the critics QwQ and QwD . The critic gradients are computed in
parallel and then averaged across the population. The global population update can be written as
θi ― θi + αVθi P QQ(st,∏θi (st)), ∀i ≤ N/2
batch
θi ― θi + αVθi P QD (st,∏θi (st)), ∀i > N/2
batch
N/2	2
W - W — 2αVw P P (QIw(St,at) - (rQ + γQQo(st+ι,∏θi(st+ι))))
batch i=1
W — W - 2α Vw P P	(QD (st,at) - (rD + γQD0 (st+1,πθi (st + 1))))
batch i=N/2+1
(1)
where w0 are parameters of target critic networks. To keep notations simple, updates of the extra
critic networks introduced in td3, which serve to reduce the value estimation bias, do not appear in
(1) although we use them in practice.
Evaluation. Once mutated, solutions are evaluated. To do so, each new solution θ performs trajec-
tories with the resulting controller πθ in the environment. All encountered transitions are stored in
the replay buffer and we use the score J (θ) and behavior descriptor bdθ to place new solutions in
the unstructured archive or in the MAP-Elites grid.
More details about the qd-rl algorithm and a pseudo-code are available in Appendices B and D.
5	Experiments
In this section, we demonstrate the capability of qd-rl to solve challenging exploration prob-
lems. We implement it with td3 and refer to this configuration as the qd-td3 algorithm. Hyper-
parameters are described in Appendix B.5. We first analyse each component of qd-td3 and demon-
strate their usefulness on a simple example. Then, we show that qd-td3 solves a more challenging
control and exploration problem such as navigating the MuJoCo Ant in a large maze with a bet-
ter sample complexity than its evolutionary competitors. Finally, we demonstrate that qd-td3 can
solve control problems when the behavior descriptor space is not aligned with the task.
5.1	Point-maze and Point-maze-inertia: Move a point in a maze
We first consider the point-maze environment in which a 2D material point agent must exit from
the maze depicted in Figure 2a. In this environment, BDs are defined as the agent coordinates
(xt , yt ) at time t and an action corresponds to position increments along the x and y axis.
In order to exit the maze, the agent must find the right balance between exploitation and exploration.
Although this environment may look simple due to its low dimensionality, it remains very challeng-
5
Under review as a conference paper at ICLR 2021
(a) POINT-MAZE	(b) ANT-MAZE	(c) ANT-TRAP
Figure 2: Evaluation environments. Though they may look similar, the state and action spaces in
POINT-MAZE are two-dimensional, whereas they are 29 × 8 in ANT-MAZE.
ing for standard deep RL agents such as td3, see Figure 4b. We also consider an identical task,
point-maze-inertia, in which the point has momentum. In this case, BDs are unchanged but
the state contains velocities in addition to positions, and the action is a force (Fx, Fy). Despite the
additional complexity, we observe equivalent performance.
We leverage these environments to perform an ablation study. First, we compare performance of
the Pareto and the me settings. Then, we measure performance when maximizing quality only.
We call the resulting agent q-td3, but this is simply a multi-actor td3. We also evaluate the
agent performance when looking for solutions to maximize diversity only. We call the resulting
agent d-td3. Finally, we consider a d-td3 + pareto algorithm optimizing only for diversity but
performing agent selection from the archive with a Pareto front. That is, it selects solutions for the
next generation based on both their quality and diversity, but without optimizing the former. Table 1
summarises all the results and more figures are shown in Appendix C.3.
Table 1: Ablations and their performance in point-maze .
Algorithm	Opt. Q Opt. D Selection mechanism Episode return
qd-td3 + me
qd-td3 + pareto
d-td3 + pareto
d-td3
q-td3 + pareto
q-td3
I;XX
;XXl
MAP-ELITES	-24 (±0)
PARETO	-27 (±1)
PARETO	-37 (±3)
None	-111 (±53)
PARETO	-128 (±0)
None	-130 (±2)
5.2 Ant-Maze: Control an articulated ant to solve a maze
ant-maze is modified from ant-v2 of OpenAI Gym (Brockman et al., 2016) and is also used in
Colas et al. (2020); Frans et al. (2018). In ant-maze, a four-legged ant has to reach a goal zone
located in the lower right part of the maze (colored in green in Figure 2b). Its initial position is
sampled from a small circle around the initial point located in the extreme bottom left of the maze.
Maze walls are organized so that following the gradient of distance to the goal drives the ant into
a dead-end. As in point-maze, the reward is expressed as minus the Euclidean distance between
the center of gravity of the ant and the center of the goal zone, thus leading to a strongly deceptive
gradient. Note that in ANT-MAZE, the reward function does not contain the additional control,
healthy and contact terms which are present in the reward function of ANT-V2 and help define an
implicit curriculum that facilitates learning. The environment is considered solved when an agent
obtains a score superior to -10, corresponding to reaching the goal zone. Since the agent must learn
to control a body with 8 degrees of freedom in all directions to explore the maze and solve it, it is
higher-dimensional than point-maze and more complex than the standard ant-v2 where the ant
should only go forward. The state contains the positions, angles, velocities and angular velocities of
most ant articulations and center of gravity, and has 29 dimensions. An action specifies 8 continuous
torque intensities applied to the 8 ant articulations. Episodes last 3000 time steps and the solution
BD is the final position (xf, yf) of the center of gravity of the ant, as in POINT-MAZE.
6
Under review as a conference paper at ICLR 2021
We compare qd-td3 to five state-of-the-art evolutionary methods with a diversity seeking compo-
nent: nsr-es, nsra-es, ns-es, me-es. While ns-es and d-me-es optimize only for diversity, and
q-me-es optimizes only for quality, nsr-es, nsra-es and qd-me-es optimize for both. To ensure
fair comparison, we do not implement our own versions of these algorithms but reuse results from
the me-es paper (Colas et al., 2020) and we make sure that our environment is rigorously the same.
We also use as baselines sac and td3, two state-of-the-art RL methods for continuous control,
and cem-rl, an algorithm combining evolutionary methods and RL but without diversity seeking
mechanisms. We chose cem-rl as it shows state-of-the-art performance on standard MuJoCo
benchmarks (Pourchot & Sigaud, 2018).
Table 2: Summary of compared algorithms on ANT-MAZE. The Final Perf. is the minimum distance
to the goal at the end of an episode over the population.
Algorithm	Final Perf. (± Std)	Steps to -10	Ratio to QD-TD3
qd-td3 pareto	-4Γ±3)	1.15e8	1
qd-td3 me	-7 (±7)	1.15e8	1
qd-td3-sum	-5 (±3)	2.5e8	2
d-td3	-2 (±0)	3.5e8	3
q-td3	-26 (±0)	∞	∞
SAC	-59 (±1)	∞	∞
cem-rl	-26 (±0)	∞	∞
ns-es	-4 (±0)	2.4e10	209
nsr-es	-26 (±0)	∞	∞
nsra-es	-2 (±1)	2.1e10	182
qd-me-es	-5 (±1)	2.4e10	209
d-me-es	-5 (±0)	1.9e10	165
q-me-es	-26 (±0)	∞	∞
5.3 Ant-Trap: Control an articulated ant to avoid a trap
ant-trap also derives from ant-v2 and is inspired from Colas et al. (2020). In ant-trap, the
four-legged ant initially appears in front of a trap and must avoid it in order to run as fast as possible
in the forward direction along the x-axis. The trap consists of three walls that form a dead-end
directly in front of the ant, leading to a strongly deceptive gradient. As in previous environments,
the behavior space is defined as the final position (xf , yf) of the center of gravity of the ant. The
peculiarity of this environment compared to the others is that here, the reward is expressed as the
ant’s forward velocity and is thus not fully aligned with the BDs. In contrast to ant-maze, here
the reward function is the same as the one used in ant-v2: it contain the additional features that
facilitate training. We can thus test the coverage capacity of our method in the case of a partial
alignment. The action and state spaces are the same as in ant-maze and the episodes last 1000
time steps. In this benchmark, we compare qd-td3 to sac, td3, cem-rl and qd-td3-sum. We
could not compare it to the me-es family as these algorithms require 1000 CPUs.
6	Discussion
Ablation study. The ablation study in Table 1 shows that: 1) When maximising quality only, Q-
td3 with or without a selection mechanism fails due to the deceptive nature of the reward. Figure 4
also shows the poor BD space coverage resulting from the lack of a diversity component. 2) When
maximising diversity only, d-td3 fails without selection mechanism but adding Pareto or me
selection is enough to exit the maze. 3) When optimizing quality and diversity, the agent finds a
more optimal trajectory to the maze exit, resulting in higher returns. 4) While me has slightly less
variance, both QD methods solve the problem with high returns. This study validates the usefulness
of the qd-rl components: 1) optimizing for diversity is required to overcome the deceptive nature
of the reward; 2) a proper population selection mechanism is required to ensure proper convergence,
and adding quality optimization provides better asymptotic performance.
Comparison to neuroevolution with a diversity seeking component competitors in Ant-Maze.
Table 2 compares QD in terms of sample efficiency to Deep Neuroevolution algorithms with a
7
Under review as a conference paper at ICLR 2021
-1000
-1100
-1200
-1300
I-1400
I-1500
-1600
-1700
-1800
(b) ant-trap
(a) point-maze-inertia
-→- QDTD3 MAP^Iites	--	Q-ME-ES	NSR-ES
QD-TD3 Pareto	→-	D-ME-ES	-→-	NS-ES
-→- QD-ME-ES	—i-	NSRA-ES	CEM-RL
-→- QDTD3 MAP-EIites	—	D-TD3
QD-TD3 Pareto	→	Q-TD3
一 QD-TD3-SUM	-i-	SAC
W
0.0	0.5	1.0	1.5	2.0	2.5	3.0
Number of steps	IelO
OA	0^-
Number of steps
0.8	1.0
le9
(c) ant-maze: Comparison to Evo methods	(d) ant-maze: Comparison to PG methods
Figure 3: Learning curves of qd-td3 versus ablations and baselines. In point-maze and ant-
trap, the performance is the highest return. In ant-maze, it is minus the highest distance to the
goal. In 3a and 3b, all results are averaged over 5 seeds. In 3c and 3d, curves show the best 2 seeds
out of 5 for all algorithms.
diversity seeking component in ant-maze. qd-rl is run on 10 CPU cores during 2 days while its
competitors used several hundreds of CPU cores for the same duration. Nonetheless, qd-rl matches
the asymptotic performance of me-es using two orders of magnitude less samples, thus explaining
the lower resource requirements. We see three reasons for the improved sample efficiency of qd-rl:
1) qd-rl leverages a replay buffer and can re-use each sample several times. 2) qd-rl leverages a
notion of novelty defined at the state level and thus can exploit all collected transitions to maximize
both quality and diversity. For example, in ant-maze, a trajectory brings 3000 samples to qd-rl
while standard QD methods would consider it as a unique sample. 3) RL exploits the analytical
gradient between the neural network weights and the resulting policy action distribution and thus
estimates only the impact of the distribution on the return, whereas standard QD methods estimate
directly the impact of the weights onto the return.
Performance assessment when BDs are not fully aligned with the task in Ant-Trap. Finally, we
evaluated qd-rl in the ant-trap environment. The difference from previous environments is that
reward is computed as the ant forward velocity at time t and not as an Euclidean distance to a target.
As the BD remains the ant center of gravity, it is not directly aligned with the task. Nonetheless,
we observe in Figure 4c that optimizing only for quality results in sub-optimal performance while
qd-rl finds the optimal strategy by avoiding the trap and running beyond.
Mixing quality and diversity updates. To validate our design choices, we also introduce a variant
of QD-TD3 called QD-TD3-SUM, where a unique critic Qw is updated considering at every time step
the sum rt of the quality reward and the diversity reward: rt = rtQ + rtD. We tested QD-TD3-SUM
on ant-maze and ant-trap. qd-td3-sum completely fails to learn anything in ant-trap. It
manages to solve the task in ant-maze but requires more samples than qd-td3. In fact, the quality
and diversity rewards may give rise to gradients pointing to opposite directions. For instance, at the
beginning of training in ant-trap, the quality reward drives the ant forward whereas the diversity
8
Under review as a conference paper at ICLR 2021
reward drives it backward so as to escape the trap and explore the environment. Therefore, both
rewards may often anneal each other, preventing any learning. This situation, while rarer, also
happens in the ant-maze environment and reduces sample efficiency.
Comparison to sample efficient methods without a diversity seeking component. We compared
qd-td3 to td3 and sac and cem-rl in both ant-maze and ant-trap benchmarks. As expected,
td3 converges quickly to the local minimum of performance resulting from being attracted in the
dead-end by the deceptive gradient in both environments. While we may expect sac to demon-
strate better exploration properties as it regularizes entropy, it also converges to that same local
minimum in ant-trap. Surprisingly, sac with the standard hyper-parameters used for MuJoCo
benchmarks does not learn in ant-maze. It may come from the lack of the additional features in
the reward function. cem-rl combines RL for sample efficiency and cross-entropy methods for a
better exploration in the parameter space. Indeed, cem-rl was shown to outperform RL algorithms
in standard MuJoCo environments. Despite this additional exploration mechanism, cem-rl also
quickly converges to the dead-end in both benchmarks, thus confirming the need for a dedicated
diversity seeking component.
(a) Color map: oldest actors are in blue while the newest are in purple
(b) point-maze
(c) ant-trap
Figure 4: Coverage maps of q-td3 (left) and qd-td3 (right) in point-maze and ant-trap.
7	Conclusion
In this paper, we proposed a novel way to deal with the exploration-exploitation trade-off by combin-
ing a quality-seeking component, a diversity-seeking component and a selection component inspired
from the Quality-Diversity literature. Crucially, we showed that quality and diversity could be op-
timized with off-policy reinforcement learning algorithms, resulting in a significantly better sample
efficiency. We showed experimentally the effectiveness of the resulting qd-rl framework, which
can solve, in two days using 10 cpus, problems that were previously out of reach without a much
larger infrastructure. Key components of qd-rl are selection through a Pareto front or a Map-Elites
grid and the search for diversity in a behavior descriptor space. Admittedly, this behavior descriptor
space is hand designed. There are attempts to automatically obtain it through unsupervised learn-
ing methods (Pere et al., 2018; Paolo et al., 2019), but defining this space is often a trivial design
choice that can alleviate the need to carefully shape reward functions. In the future, we intend to
extend this approach to problems where the reward function can be decomposed into several loosely
dependent components, such as standing, moving forward and manipulating objects for a humanoid
agent, or to multi-agent reinforcement learning problems. In such environments, we could replace
the maximization of the sum of reward contributions with a multi-criteria selection from a Pareto
front, where diversity would be only one of the considered criteria. Additionally, we would like to
make profit of the diversity of the solutions we obtain to build repertoires of behaviors, as it is one
of the main features of the Quality-Diversity methods.
9
Under review as a conference paper at ICLR 2021
References
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Cedric Colas, Olivier Sigaud, and Pierre-Yves Oudeyer. GEP-PG: Decoupling exploration and
exploitation in deep reinforcement learning algorithms. arXiv preprint arXiv:1802.05054, 2018.
Cedric Colas, Vashisht Madhavan, Joost Huizinga, and Jeff Clune. Scaling map-elites to deep neu-
roevolution. In Proceedings of the 2020 Genetic and Evolutionary Computation Conference, pp.
67-75, 2020.
Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman, Kenneth Stanley, and Jeff
Clune. Improving exploration in evolution strategies for deep reinforcement learning via a pop-
ulation of novelty-seeking agents. In Advances in neural information processing systems, pp.
5027-5038, 2018.
Antoine Cully and Yiannis Demiris. Quality and diversity optimization: A unifying modular frame-
work. IEEE Transactions on Evolutionary Computation, 22(2):245-259, 2017.
Antoine Cully and Jean-Baptiste Mouret. Behavioral repertoire learning in robotics. In Proceedings
of the 15th annual conference on Genetic and evolutionary computation, pp. 175-182, 2013.
Antoine Cully, Jeff Clune, Danesh Tarapore, and Jean-Baptiste Mouret. Robots that can adapt like
animals. Nature, 521(7553):503-507, 2015.
Thang Doan, Bogdan Mazoure, Audrey Durand, Joelle Pineau, and R Devon Hjelm.
Attraction-repulsion actor-critic for continuous control reinforcement learning. arXiv preprint
arXiv:1909.07543, 2019.
Stephane Doncieux, Alban Laflaquiere, and Alexandre Coninx. Novelty search: a theoretical per-
spective. In Proceedings of the Genetic and Evolutionary Computation Conference, pp. 99-106,
2019.
Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune. Go-explore: a
new approach for hard-exploration problems. arXiv preprint arXiv:1901.10995, 2019.
Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need:
Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018.
Sebastien Forestier, Yoan Mollard, and Pierre-Yves Oudeyer. Intrinsically motivated goal explo-
ration processes with automatic curriculum learning. arXiv preprint arXiv:1708.02190, 2017.
Kevin Frans, Jonathan Ho, Xi Chen, Pieter Abbeel, and John Schulman. Meta learning shared
hierarchies. Proc. of ICLR, 2018.
Scott Fujimoto, Herke Van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.
Riashat Islam, Zafarali Ahmed, and Doina Precup. Marginalized state distribution entropy regular-
ization in policy optimization. arXiv preprint arXiv:1912.05128, 2019.
Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M Czarnecki, Jeff Donahue, Ali
Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, et al. Population-based train-
ing of neural networks. arXiv preprint arXiv:1711.09846, 2017.
Whiyoung Jung, Giseung Park, and Youngchul Sung. Population-guided parallel policy search for
reinforcement learning. In International Conference on Learning Representations, 2020.
Shauharda Khadka and Kagan Tumer. Evolution-guided policy gradient in reinforcement learning.
In Neural Information Processing Systems, 2018.
Shauharda Khadka, Somdeb Majumdar, Santiago Miret, Evren Tumer, Tarek Nassar, Zach Dwiel,
Yinyin Liu, and Kagan Tumer. Collaborative evolutionary reinforcement learning. arXiv preprint
arXiv:1905.00976, 2019.
10
Under review as a conference paper at ICLR 2021
Lisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric Xing, Sergey Levine, and Ruslan Salakhutdi-
nov. Efficient exploration via state marginal matching. arXiv preprint arXiv:1906.05274, 2019.
Joel Lehman and Kenneth O Stanley. Abandoning objectives: Evolution through the search for
novelty alone. Evolutionary computation, 19(2):189-223,2011a.
Joel Lehman and Kenneth O Stanley. Evolving a diversity of virtual creatures through novelty search
and local competition. In Proceedings of the 13th annual conference on Genetic and evolutionary
computation, pp. 211-218, 2011b.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Guillaume Matheron, Nicolas Perrin, and Olivier Sigaud. PBCS: Efficient exploration and ex-
ploitation using a synergy between reinforcement learning and motion planning. arXiv preprint
arXiv:2004.11667, 2020.
Jean-Baptiste Mouret and Jeff Clune. Illuminating search spaces by mapping elites. arXiv preprint
arXiv:1504.04909, 2015.
Giuseppe Paolo, Alban Laflaquiere, Alexandre Coninx, and Stephane Doncieux. Unsupervised
learning and exploration of reachable outcome space. algorithms, 24:25, 2019.
Jack Parker-Holder, Aldo Pacchiano, Krzysztof Choromanski, and Stephen Roberts. Effective di-
versity in population-based reinforcement learning. In Neural Information Processing Systems,
2020.
Alexandre Pere, Sebastien Forestier, Olivier Sigaud, and Pierre-Yves Oudeyer. UnsUPervised learn-
ing of goal spaces for intrinsically motivated goal exploration. arXiv preprint arXiv:1803.00781,
2018.
Vitchyr H Pong, Murtaza Dalal, Steven Lin, Ashvin Nair, Shikhar Bahl, and Sergey Levine. Skew-
fit: State-covering self-supervised reinforcement learning. arXiv preprint arXiv:1903.03698,
2019.
Alols Pourchot and Olivier Sigaud. Cem-rl: Combining evolutionary and gradient-based methods
for policy search. arXiv preprint arXiv:1810.01222, 2018.
Justin K Pugh, Lisa B Soros, and Kenneth O. Stanley. Quality diversity: A new frontier for evolu-
tionary computation. Frontiers in Robotics and AI, 3:40, 2016.
Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a
scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.
Longxiang Shi, Shijian Li, Qian Zheng, Min Yao, and Gang Pan. Efficient novelty search through
deep reinforcement learning. IEEE Access, 8:128809-128818, 2020.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In Proceedings of the 30th International Conference in
Machine Learning, 2014.
Christopher Stanton and Jeff Clune. Curiosity search: producing generalists by encouraging individ-
uals to continually explore and acquire skills throughout their lifetime. PloS one, 11(9):e0162235,
2016.
11
Under review as a conference paper at ICLR 2021
Appendices
A	The TD3 Agent
The Twin Delayed Deep Deterministic (td3) agent Fujimoto et al. (2018) builds upon the Deep
Deterministic Policy Gradient (ddpg) agent Lillicrap et al. (2015). It trains a deterministic actor
πφ : S → A directly mapping observations to continuous actions and a critic Qθ : S × A → R
taking a state s and an action a and estimating the average return from selecting action a in state s
and then following policy πφ . As DDPG, TD3 alternates policy evaluation and policy improvement
operations so as to maximise the average discounted return. In ddpg, the critic is updated to mini-
mize a temporal difference error during the policy evaluation step which induces an overestimation
bias. TD3 corrects for this bias by introducing two critics Qθ1 and Qθ2 . TD3 plays one step in the
environment using its deterministic policy and then stores the observed transition (st, at, rt, st+1)
into a replay buffer B. Then, it samples a batch of transitions from B and updates the critic networks.
Half the time it also samples another batch of transitions to update the actor network.
Both critics are updated so as to minimize a loss function which is expressed as a mean squared
error between their predictions and a target:
Lcritic(θ1, θ2) = X(Qθ1(st, at) - yt)2 + (Qθ2(st, at) - yt)2,	(2)
batch
where the common target yt is computed as:
yt = Tt + Y min Qθi(st+1,∏φ(st+1) + e), e 〜N(0,I).	(3)
i=1,2
The Q-value estimation used to compute target yt is taken as minimum between both critic pre-
dictions thus reducing the overestimation bias. TD3 also adds a small perturbation to the action
πφ(st+1) so as to smooth the value estimate by bootstrapping similar state-action value estimates.
Every two critics updates, the actor πφ is updated using the deterministic policy gradient also used
in DDPG Silver et al. (2014). For a state s, DDPG updates the actor so as to maximise the critic
estimation for this state s and the action a = πφ(s) selected by the actor. As there are two critics in
td3, the authors suggest to take the first critic as an arbitrary choice. Thus, the actor is updated by
minimizing the following loss function:
Lactor(φ) = - X Qθ1 (st, πφ(st)).	(4)
batch
Policy evaluation and policy improvement steps are repeated until convergence. td 3 demonstrates
state of the art performance on several MuJoCo benchmarks. In this study, we use it to update the
population of actors for both quality and diversity.
B QD-RL Additional Practical Details
B.1	Computational details
We consider populations of N = 4 actors for both POINT-MAZE and POINT-MAZE-INERTIA envi-
ronments and N = 10 actors for ANT-MAZE and ANT-TRAP. We use 1 CPU thread per actor and
parallelization is implemented with the Message Passing Interface (MPI) library. Our experiments
were run on a standard computer with 10 CPU cores and 100 GB of RAM although the maximum
RAM consumption per experiment at any time never exceeds 10GB due to an efficient and cen-
tralized management of the archive that stores all solutions. An experiment on point-maze or
point-maze-inertia typically takes between 2 and 3 hours while an experiment on ant-maze
or ant-trap takes about 2 days. Note that these durations can vary significantly depending on the
type of cpu used. We did not use any gpus.
12
Under review as a conference paper at ICLR 2021
B.2	QD Implementation details
We consider two settings for the Quality Diversity part of our algorithm.
In the pareto setting, we rely on a FIFO archive of solutions. We chose a maximum archive
size of 10.000 for all environments. When a new solution θ is obtained after the mutation phase, we
compute the mean distance between its BD bdθ and the BDs of its K nearest neighbors in the archive.
We used K = 10 for all environments. We also used an Euclidean distance for all environments. We
add the new solution in the archive if the computed distance is superior to a threshold of acceptance.
See Table 3 for the different values of this threshold depending on the environment. During the
selection phase, we compute a QD Pareto front of all solutions contained in the archive and sample
the new population from the front. If the Pareto front contains less than N actors, we select them
all, remove them, compute the Pareto front over the remaining actors and sample again from it, and
so on until we get N actors.
In the me setting, we use a Map Elites (me) grid as archive of solutions. We assume that the BD
space is bounded and can be discretized in an Cartesian grid. We discretize each dimension into m
meshes, see Table 3 for the value of m depending on the environment. Thus the number of bins in
the ME grid equals m times the number of dimensions of the BDs. When a new solution θ is obtained
after the mutation phase, we seek for the bin that corresponds its BD bdθ . If the bin is empty, we
add the solution in it, otherwise we replace the solution already contained by the new one if its
score J (θ) is greater than the one of the already contained solution. During selection, we sample
solutions uniformly from the me grid. The me archive management and selection mechanism tends
to store less solutions than the pareto method and does not need to compute a Pareto Front which
makes qd-rl me less computation and memory costly than qd-rl pareto.
B.3	Diversity reward computation
The RL part of qd-rl optimizes solutions for quality but also for diversity at the state level. As the
QD part stores solutions and performs population selection based on solution BDs bdθ, the RL part
optimizes the solutions so as to encourage them to visit states with novel state BDs. The novelty of
a state BD bdt is expressed through a diversity reward rtD. In practice, we maintain in parallel of the
replay buffer a FIFO storage of the states BDs encountered so far. In practice, the maximum size
of this storage is never reached. However its maximum theoretical size is chosen to be the same as
the solutions archive. When a transition (st, at, bdt, rt, st+1) is sampled and stored in the Replay
Buffer, we also add bdt in this storage. For the archive of solutions presented in the PARETO setting,
we add a state BD in this storage only if its mean Euclidean distance to its K nearest neighbors in
the storage is greater than an acceptance threshold. The values of K and of the threshold are given
in Table 3. This mechanism ensures that we do not keep too close state BDs and thus controls this
storage size. When a batch of transitions (st, at, bdt, rt, st+1) is collected during the mutation phase,
we recompute fresh diversity rewards rtD as the mean Euclidean distance between the sampled states
BDs bdt and their K nearest neighbors in the states BDs storage. The value of K is the same as the
one used for the insertion rule.
B.4	Computational cost and parallel implementation
The main costs of the algorithm come from backpropagation during the update of each agent, and to
the interaction between agents and the environment. These costs scale linearly with the population
size but, as many other population-based methods, the structure of qd-rl lends itself very well
to parallelization. We leverage this property and parallelize our implementation so as to assign one
agent per CPU core. Memory consumption also scales linearly with the number of agents. To reduce
this consumption, we centralize the solution archive on a master worker and distribute data among
workers when needed. With these implementation choices, qd-rl only needs a very accessible
computational budget for all experiments.
13
Under review as a conference paper at ICLR 2021
B.5	Hyper-parameters
Table 3 summarizes all hyper-parameters used in experiments (see Figure 3). Most of these hyper-
parameters values are the original ones from the td3 algorithm.
Table 3: QD-TD3 Hyper-parameters
Parameter	Point Maze	Ant Maze	Ant Trap
TD3			
Optimizer	Adam	Adam	Adam
Learning rate	6.10-3	3.10-4	3.10-4
Discount factor γ	0.99	0.99	0.99
Replay buffer size	106	5.105	5.105
Hidden layers size	64/32	256/256	256/256
Activations	ReLU	ReLU	ReLU
Minibatch size	256	256	256
Target smoothing coefficient	0.005	0.005	0.005
Delay policy update	2	2	2
Target update interval	1	1	1
Gradient steps ratio	4	0.1	0.1
Quality-Diversity			
Archive size	10000	10000	10000
Threshold of acceptance	0.0001	0.1	0.1
K-nearest neighbors	10	10	10
MAP-Elites			
Number of bins per dimension	5	7	6
C Environments analysis
In this section, we propose a more thorough analysis of the proposed environments. We highlight
why these environments are hard to solve for classical deep RL agents without a diversity mechanism
and show the impact of the different components of our algorithm.
C.1 Environments details
In POINT-MAZE, the observation and state BD are the agent coordinates (xt , yt) at time t. The
actions features (δx, δy) are position increments along the x and y axes. The solution BD is the
final position (xf, yf) of the agent, as in Conti et al. (2018). The initial position of the agent is
sampled from a small zone at the bottom of the maze. The exit area is a square located at the top left
of the maze, the episode ends once this exit square is reached or after 200 time steps. The reward
is computed as minus the Euclidean distance between the agent position and the maze exit. This
reward leads to a deceptive gradient signal: following it would lead the agent to stay stuck by the
second wall in the maze, as shown in Figure 5 of Appendix C. In point-maze-inertia, the BD
space remains the same however the point velocity (vx , vy) is appended to the state to preserve the
Markov property and the action corresponds to forces (Fx , Fy).
In point-maze and point-maze-inertia, the dimensions of the observation spaces are equal to
2 and 4 respectively, and the dimensions of the action spaces are both equal to 2. Both environments
are very similar with low-dimensional state and action spaces, and only differ by the fact that point-
maze-inertia moves a ball subject to inertia. By contrast, in ant- maze and ant-trap, the
dimensions of the observation spaces are respectively 29 and 113 while the action spaces dimensions
are both equal to 8, making these two environments much more challenging as they require larger
controllers.
14
Under review as a conference paper at ICLR 2021
The ant-trap environment also differs from mazes as itis open-ended, i.e., the space to be explored
by the agent is unlimited, in contrast to other environments where this space is restricted by the maze
walls. In this case, the state BD corresponds to the ant position that is clipped to remain in a given
range. On the y-axis, this range is defined so as to leave a space corresponding to the width of the
trap on both trap sides. On the x-axis, this range begins slightly behind the starting position of the
ant and is large enough to let it accelerate along this axis. Figure 4 proposes a visual representation
of the BD space in ant-trap.
C.2 Deceptive gradients
Figure 5 highlights the deceptive nature of the point-maze and the ant-maze objective functions
by depicting gradient fields in both environments. Similarly, point-maze-inertia and ant-trap
also present strongly deceptive gradients.
(a) Gradient field in point-maze
(b) Gradient field in ant-maze
Figure 5: Gradients maps on point-maze and ant-maze. The black lines represent the maze
walls, the arrows depict gradient fields and the square indicates the maze exit. Both settings present
highly deceptive gradients: naively following them leads into a wall.
C.3 Exploration in Point-Maze for All Ablations
Figure 6 summarizes the coverage maps of the point-maze environment by the different algorithms
presented in the ablation study (see Table 1). A dot in the figure corresponds to the final position
of an agent after an episode. The color spectrum highlights the course of training: agents evaluated
early in training are in blue while newer ones are represented in purple. Figure 6 corresponds to the
map coverage for one seed, we chose a representative seed.
qd-td3 (pareto andme) and d-td3+pareto almost cover the whole BD space including the
objective. d-td3 also finds the objective but is not constant through all seeds and does not uniformly
cover the BD space. Unsurprisingly, q-td3+pareto and q-td3 present very poor coverage maps,
both algorithms optimize only for quality and the Pareto selection mechanism does not contribute
anything in this setting. Interestingly, all algorithms optimizing for diversity find the maze exit.
However, as shown in Table 1, algorithms that also optimize for quality (qd-td3) or that present
structural pressure toward quality (d-td3+pareto) are able to refine their trajectory through the
maze and obtain significantly better performance.
Figure 7 summarizes the coverage maps of the ant-trap environment by qd-td3+me, qd-
td3+pareto and td3. Interestingly, the pattern of qd-td3+me differs from that of qd-
td3+pareto. We hypothesize that this behavior reflects the grid structure of the me archive.
15
Under review as a conference paper at ICLR 2021
Figure 6: Coverage map of the point-maze environment for all ablations. Each dot corresponds to
the position of an agent at the end of an episode. Dots corresponding to the oldest actors are in blue
while the newest are in purple.
Figure 7: Coverage map of the ant-trap environment. Each dot corresponds to the position of an
agent at the end of an episode. Dots corresponding to the oldest agents are in blue while the newest
are in purple.
16
Under review as a conference paper at ICLR 2021
D QD-RL Pseudo Code
Algorithm 1: QD-RL
Given: N, max_steps, gradient_steps_ratio,
Initialize: Archive A, Replay Buffer B, N actors {πθi }i={1,...,N}, 2 critics QθD and QθQ , BD Storage S
total steps, actor .steps = 0, 0 // Step counters
// Parallel evaluation of the initial population
for j _ 1 to N do
Play one episode with actor πθj and store all transitions in B
Get episode length T, discounted return R and state BDs {bd1 , . . . , bdT}
Store BDs {bd1 , . . . , bdT} in S
Compute bdθ = fbd({bd1 , . . . , bdT}) and add the tuple (R, bdθ, θj) in the archive A
actor _steps — actor _steps + T
end
// Algorithm main loop
while total steps < max_steps do
// Select new generation
Get N actors πθi , i ∈ {1, . . . , N} from the Pareto front or from the MAP-Elites grid
gradient.steps = int (aCtor-StepS X gradient_steps^ratio)
actor _steps = 0
// Perform in parallel population update and evaluation
for j _ 1 to N do
// Update the population
for i — 1 to gradient.steps do
Sample batch of (st, at, rt, st+1 , bdt) from B
// First half is updated to maximise diversity
if j ≤N//2 then
Compute novelty reward as rtD from bdt and S
Update πθj for diversity
Compute novelty critic gradient locally
Average between threads novelty critic gradients
Update novelty critic QwD
end
// Second half is updated to maximise quality
else
Update πθj for quality
Compute quality critic gradient locally
Average between threads quality critic gradients
Update quality critic QwQ
end
end
// Evaluate the updated actors
Play one episode with actor πθj and store all transitions in B
Get episode length T, discounted return R and state BDs {bd1 , . . . , bdT}
Store BDs {bd1 , . . . , bdT} in S
Compute bdθ = fbd({bd1, . . . , bdT}) and add the tuple (R, bdθ, θj) in the archive A
actor _StePS — actor_steps + T
end
total _StePS — total steps + actor _steps / / Update total time steps
end
17