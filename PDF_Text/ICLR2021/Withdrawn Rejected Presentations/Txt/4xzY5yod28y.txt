Under review as a conference paper at ICLR 2021
Scheduled Restart Momentum for
Accelerated Stochastic Gradient Descent
Anonymous authors
Paper under double-blind review
Ab stract
Stochastic gradient descent (SGD) algorithms, with constant momentum and its
variants such as Adam, are the optimization methods of choice for training deep
neural networks (DNNs). There is great interest in speeding up the convergence
of these methods due to their high computational expense. Nesterov accelerated
gradient (NAG) with a time-varying momentum, denoted as NAG below, improves
the convergence rate of gradient descent (GD) for convex optimization using a
specially designed momentum; however, it accumulates error when an inexact
gradient is used (such as in SGD), slowing convergence at best and diverging at
worst. In this paper, we propose scheduled restart SGD (SRSGD), anew NAG-style
scheme for training DNNs. SRSGD replaces the constant momentum in SGD by
the increasing momentum in NAG but stabilizes the iterations by resetting the
momentum to zero according to a schedule. Using a variety of models and bench-
marks for image classification, we demonstrate that, in training DNNs, SRSGD
significantly improves convergence and generalization; for instance, in training
ResNet-200 for ImageNet classification, SRSGD achieves an error rate of 20.93%
vs. the benchmark of 22.13%. These improvements become more significant as
the network grows deeper. Furthermore, on both CIFAR and ImageNet, SRSGD
reaches similar or even better error rates with significantly fewer training epochs
compared to the SGD baseline.
1 Introduction
Training many machine learning (ML) models reduces to solving the following finite-sum optimiza-
tion problem
1N	1N
mwn f(w) ：= nmin NEfi(W4:=mwn N fL(g(xi, w),yi), W ∈ Rd,
(1)
i=1
i=1
where {xi, yi}iN=1 are the training samples and L is the loss function, e.g., cross-entropy loss for a
classification task, that measure the discrepancy between the ground-truth label yi and the prediction
by the model g(∙, w), parametrized by w. The problem (1) is known as empirical risk minimization
(ERM). In many applications, f (W) is non-convex, and g(∙, W) is chosen among deep neural networks
(DNNs) due to their preeminent performance across various tasks. These deep models are heavily
overparametrized and require large amounts of training data. Thus, both N and the dimension of W
can scale up to millions or even billions. These complications pose serious computational challenges.
One of the simplest algorithms to solve (1) is gradient descent (GD), which updates W according to:
1N
Wk+1= Wk -SkN γ^Vfi(Wk),
i=1
(2)
where Sk > 0 is the step size at the k-th iteration. Computing Vf (Wk) on the entire training set is
memory intensive and often prohibitive for devices with limited random access memory (RAM) such
as graphics processing units (GPUs) used for deep learning (DL). In practice, we sample a subset
of the training set, of size m with m N, to approximate Vf(wk) by the mini-batch gradient
1/m Pjm=1 Vfij (wk), resulting in the (mini-batch)-stochastic gradient descent (SGD). SGD and its
1
Under review as a conference paper at ICLR 2021
accelerated variants are among the most used optimization algorithms in ML. These gradient-based
algorithms have low computational complexity, and they are easy to parallelize, making them suitable
for large scale and high dimensional problems (Zinkevich et al., 2010; Zhang et al., 2015).
Nevertheless, GD and SGD have issues with slow convergence, especially when the problem is
ill-conditioned. There are two common techniques to accelerate GD and SGD: adaptive step size
(Duchi et al., 2011; Hinton et al.; Zeiler, 2012) and momentum (Polyak, 1964). The integration of
both adaptive step size and momentum with SGD leads to Adam (Kingma & Ba, 2014), one of the
most used optimizers for training DNNs. Many recent developments have improved Adam (Reddi
et al., 2019; Dozat, 2016; Loshchilov & Hutter, 2018; Liu et al., 2020). GD with constant momentum
leverages the previous step to accelerate GD according to:
vk+1 = wk - SkVf(Wk) wk+1 = vk+1 + μ(vk+1 - Vk),	(3)
where μ > 0 is a constant. A similar acceleration can be achieved by the heavy-ball (HB) method
(Polyak, 1964). The momentum update in both (3) and HB have the same convergence rate of
O(1/k) as that of GD for convex smooth optimization. A breakthrough due to Nesterov (1983; 2018)
replaces μ with (k - 1)∕(k + 2), which is known as the NesteroV accelerated gradient (NAG) with
time-varying momentum. For simplicity, we denote this method as NAG below. NAG accelerates
the convergence rate to O(1∕k2), which is optimal for convex and smooth loss functions (Nesterov,
1983; 2018). NAG can also speed up the process of escaping from saddle points (Jin et al., 2017).
In practice, NAG momentum can accelerate GD for nonconvex optimization, especially when the
underlying problem is poorly conditioned (Goh, 2017). However, NAG accumulates error and causes
instability when the gradient is inexact (Devolder et al., 2014; Assran & Rabbat, 2020). In many DL
applications, constant momentum achieves state-of-the-art result. For instance, training DNNs for
image classification. Since NAG momentum achieves a much better convergence rate than constant
momentum with exact gradient for general convex optimization, we consider the following question:
Can we leverage NAG with a time-varying momentum parameter to accelerate SGD in training DNNs
and improve the test accuracy of the trained models?
Contributions. We answer the above question by proposing the first algorithm that integrates sched-
uled restart NAG momentum with plain SGD. Here, we restart the momentum, which is orthogonal
to the learning rate restart (Loshchilov & Hutter, 2016). We name the resulting algorithm scheduled
restart SGD (SRSGD). Theoretically, we prove the error accumulation of Nesterov accelerated SGD
(NASGD) and the convergence of SRSGD. The major practical benefits of SRSGD are fourfold:
•	SRSGD remarkably speeds up DNN training. For image classification, SRSGD significantly
reduces the number of training epochs while preserving or even improving the network’s accuracy.
In particular, on CIFAR10/100, the number of training epochs is reduced by half with SRSGD,
while on ImageNet the reduction in training epochs is also remarkable.
•	DNNs trained by SRSGD generalize significantly better than the current benchmark optimizers.
The improvement becomes more significant as the network grows deeper as shown in Fig. 1.
•	SRSGD reduces overfitting in training very deep networks such as ResNet-200 for ImageNet
classification, enabling the accuracy to keep increasing with depth.
•	SRSGD is straightforward to implement and only requires changes in a few lines of the SGD code.
There is also no additional computational or memory overhead.
We focus on image classification with DNNs, in which SGD with constant momentum is the choice.
Related Work. Momentum has long been used to accelerate SGD. SGD with scheduled momentum
and a good initialization can handle the curvature issues in training DNNs and enable the trained
models to generalize well (Sutskever et al., 2013). Kingma & Ba (2014) and Dozat (2016) integrated
momentum with adaptive step size to accelerate SGD. In this work, we study the time-varying
momentum version of NAG with restart for stochastic optimization. Adaptive and scheduled restart
have been used to accelerate NAG with the exact gradient (Nemirovskii & Nesterov, 1985; Nesterov,
2013; Iouditski & Nesterov, 2014; Lin & Xiao, 2014; Renegar, 2014; Freund & Lu, 2018; Roulet
et al., 2015; O’donoghue & Candes, 2015; Giselsson & Boyd, 2014; Su et al., 2014). These studies of
restart NAG momentum are for convex optimization with the exact gradient. Restart techniques have
also been used for stochastic optimization (Kulunchakov & Mairal, 2019). In particular, Aybat et al.
(2019) developed a multistage variant of NAG with momentum restart between stages. Our work
focuses on developing NAG-based optimization for training DNNs. Many efforts have also been
2
Under review as a conference paper at ICLR 2021
SGD ---------- SRSGD
50	101	152	200
Number of Layers
Figure 1: Error rate vs. depth of ResNet models trained with SRSGD and the baseline SGD with constant
momemtum. Advantage of SRSGD continues to grow with depth.
devoted to studying the non-acceleration issues of SGD with HB and NAG momentum (Kidambi
et al., 2018; Liu & Belkin, 2020), as well as accelerating first-order algorithms with noise-corrupted
gradients (Cohen et al., 2018; Aybat et al., 2018; Lan, 2012). Ghadimi & Lan (2013; 2016) provides
analysis for the general stochastic gradient-based optimization algorithms..
Organization. In Section 2, we review and discuss momentum for accelerating GD for convex
smooth optimization. In Section 3, we present the SRSGD algorithm and its theoretical guarantees.
In Section 4, we verify the efficacy of the proposed SRSGD in training DNNs for image classification
on CIFAR and ImageNet. In Section 4.3, we perform empirical analysis of SRSGD. We end with
some concluding remarks. Technical proofs, some experimental details, and more results in training
LSTMs (Hochreiter & Schmidhuber, 1997) and WGANs (Arjovsky et al., 2017; Gulrajani et al.,
2017) are provided in the Appendix.
Notation. We denote scalars and vectors by lower case and lower case bold face letters, respectively,
and matrices by upper case bold face letters. For a vector X = (xi, •一，Xd) ∈ Rd, we denote its 'p
norm (p ≥ 1) by kxkp = (Pid=1 |xi|p)1/p. For a matrix A, we use kAkp to denote its induced norm
by the vector `p norm. Given two sequences {an} and {bn}, we write an = O(bn) if there exists a
positive constant s.t. an ≤ Cbn. We denote the interval a to b (included) as (a, b]. For a function
f (w) : Rd → R, we denote its gradient as Vf (W) and its Hessian as V2f (w).
2 Review: Momentum in Gradient Descent
GD. GD (2) is a popular approach to solve (1), which dates back to Cauchy (1847). If f(w) is
convex and L-smooth (i.e., kV2f (w)k2 ≤ L), then GD converges with rate O(1/k) by letting
sk ≡ 1/L (we use this sk in all the discussion below), which is independent of the dimension ofw.
HB. HB (4) (Polyak, 1964) accelerates GD by using the historical information, which gives
wk+1 = wk - SkVf (wk) + μ(wk - Wk-1), μ > 0.	(4)
We can also accelerate GD by using the Nesterov/lookahead momentum, which leads to (3). Both
(3) and (4) have a convergence rate of O(1/k) for convex smooth optimization. Recently, several
variants of (3) have been proposed for DL, e.g., (Sutskever et al., 2013) and (Bengio et al., 2013).
NAG. NAG (Nesterov, 1983; 2018; Beck & Teboulle, 2009) replaces μ with (tk 一 1)∕tk+ι, where
tk+ι = (1 + p1+4：t-k)/2 with to = 1. NAG iterates as following
vk+1 = Wk - SkVf (wk); wk+1 = vk+1 + tk-1 (vk+1 - Vk).	(5)
tk+1
NAG achieves a convergence rate O(1/k2) with the step size sk = 1/L.
Remark 1. Su et al. (2014) showed that (k - 1)/(k + 2) is the asymptotic limit of (tk - 1)/tk+1. In
the following presentation of NAG with restart, for the ease of notation, we will replace the momentum
coefficient (tk - 1)/tk+1 with (k - 1)/(k + 2).
Adaptive Restart NAG (ARNAG). The sequences, {f(wk) 一 f(w*)} where w* is the minimum
of f (W), generated by GD and GD with constant momentum (GD + Momentum, which follows (3))
converge monotonically to zero. However, that sequence generated by NAG oscillates, as illustrated
in Fig. 2 (a) when f(w) is a quadratic function. O’donoghue & Candes (2015) proposed ARNAG
3
Under review as a conference paper at ICLR 2021
(6), which restart the time-varying momentum of NAG according to the change of function values, to
alleviate this oscillatory phenomenon. ARNAG iterates as following
k+1	k	k k+1 k+1	m(k) - 1 k+1	k
V = W — SkVf (w ); w 丁 = VT + m(k) + 2(v	- V ),
(6)
where m(1) = 1; m(k + 1) = m(k) + 1 if f (wk+1) ≤ f(wk), and m(k + 1) = 1 otherwise.
Scheduled Restart NAG (SRNAG). SR is another strategy to restart the time-varying momentum
of NAG. We first divide the total iterations (0, T] (integers only) into a few intervals {Ii}im=1 =
(Ti-1, Ti], such that (0, T] = im=1 Ii. In each Ii we restart the momentum after every Fi iterations.
The update rule is then given by:
k+1 k	k	k+1	k+1	(k mod Fi)	k+1	k
V = W -Sk vf (w ); W = V +(k modF,)+3 (V	-V ).
(7)
Both AR and SR accelerate NAG to linear convergence for convex problems with the Polyak-
Lojasiewicz (PL) condition (Roulet & d’Aspremont, 2017).
Case Study - Quadratic Function. Consider the following quadratic optimization (Hardt, 2014)
min f (x) = 2 XT Lx — XT b,
(8)
where L ∈ Rd×d is the Laplacian of a cycle graph, and b is a d-dimensional vector whose first entry
is 1 and all the other entries are 0. Note that f (x) is convex with Lipschitz constant 4. In particular,
we set d = 1K (1K:= 103). We run T = 5θK iterations with step size 1/4. In SRNAG, we restart,
i.e., we set the momentum to 0, after every 1K iterations. Fig. 2 (a) shows that GD + Momentum as
in (3) converges faster than GD, while NAG speeds up GD + Momentum dramatically and converges
to the minimum in an oscillatory fashion. Both AR and SR accelerate NAG significantly.
Figure 2: Comparison between different schemes in optimizing the quadratic function in (8) with (a)
exact gradient, (b) gradient with constant variance Gaussian noise, and (c) gradient with decaying
variance Gaussian noise. NAG, ARNAG, and SRNAG can speed up convergence remarkably when
exact gradient is used. Also, SRNAG is more robust to noisy gradient than NAG and ARNAG.
3	Algorithm Proposed： Scheduled Restart SGD (SRSGD)
Computing gradient for ERM, (1), can be computational costly and memory intensive, especially
when the training set is large. In many applications, such as training DNNs, SGD is used. In this
section, we first prove that the error bound of SGD with NAG cannot be bounded by a convergent
sequence, then we formulate our new SRSGD as a solution to accelerate the convergence of SGD
using the NAG momentum.
3.1	Uncontrolled B ound of Nesterov Accelerated SGD (NASGD)
Replacing Vf (wk) := 1/N PiN=1 Vfi(wk) in (5) with the mini-batch gradient 1/m Pjm=1 Vfij (wk)
will lead to uncontrolled error bound. Theorem 1 formulates this observation for NASGD.
Theorem 1 (Uncontrolled Bound of NASGD). Let f(w) be a convex and L-smooth function with
∣∣Vf (w)k ≤ R, where R > 0 is a constant. The Sequence {wk}k≥o generated by (5), with stochastic
gradient of bounded variance (Bubeck, 2014; Bottou et al., 2018) 1 and using any constant step size
sk ≡ s ≤ 1/L, satisfies
一	E (f(wk) — f(w*)) = O(k),	(9)
1We leave the analysis under the other assumptions (Jain et al., 2018) as a future work.
4
Under review as a conference paper at ICLR 2021
where w* is the minimum of f, and the expectation is taken over the generation of the stochastic
gradient.
One idea to prove Theorem 1 is by leveraging the established resulting in Lan (2012). We will
provide a new proof of Theorem 1 in Appendix A. The proof shows that the uncontrolled error bound
is because the time-varying momentum gets close to 1 as iteration increases. To remedy this, we
can restart the momentum in order to guarantee that the time-varying momentum with restart is less
than a number that is strictly less than 1. Devolder et al. (2014) proved a similar error bound for the
δ-inexact gradient, and we provide a brief review of NAG with δ-inexact gradient in Appendix B. As
far as we know that there is no lower bound of E(f (wk) - f(w*)) available even for the δ-inexact
gradient, and we leave the lower bound estimation as an open problem.
We consider three different inexact gradients: Gaussian noise with constant and decaying variance
corrupted gradients for the quadratic optimization (8), and training logistic regression model for
MNIST (LeCun & Cortes, 2010) classification. The detailed settings and discussion are provided
in Appendix B. We denote SGD with NAG momentum as NASGD and NASGD with AR and SR
as ARSGD and SRSGD, respectively. The results shown in Fig. 2 (b) and (c) (iteration vs. optimal
gap for quadratic optimization (8)) and Fig. 3 (a) (iteration vs. loss for training logistic regression)
confirm Theorem 1. For these cases, SR improves the performance of NAG with inexact gradients.
Moreover, when an inexact gradient is used, ARNAG/ARSGD performs almost the same as GD/SGD
asymptotically because ARNAG/ARSGD restarts too often and almost degenerates to GD/SGD.
3.2	SRSGD and Its Convergence
For ERM (1), SRSGD replaces Vf (W) in (7) with stochastic gradient with batch size m and gives
vk+1=Wk -Sk m X Kj (wk)； wk+1=vk+1+(km mo,+3 (vk+1 - vk),
(10)
where Fi is the restart frequency used in the interval Ii . We implemented SRSGD, in both PyTorch
(Paszke et al., 2019) and Keras (Chollet et al., 2015), by changing just a few lines of code on top
of the existing implementation of the SGD optimizer. We provide a snippet of SRSGD code in
Appendix J (PyTorch) and K (Keras). We formulate the convergence of SRSGD for general convex
and nonconvex problems in Theorem 2 and provide its proof in Appendix C.
Theorem 2 (Convergence of SRSGD). Suppose f(w) is L-smooth. Consider the sequence
{wk}k≥0 generated by (10) with stochastic gradient that is bounded and has bounded vari-
ance, and consider any restart frequency F using any constant step size sk := s ≤ 1/L. As-
sume that Ek∈a (Ef(Wk+1) — Ef(Wk)) = R < +∞ with R being a constant and the set
A := {k ∈ Z+ |Ef (wk+1) ≥Ef(wk)}, then we have
ιmi≤κ {Ekvf (Wk)图=O(S + sK)∙	(II)
If f(w) isfurther COnVeXand Pk∈B (Ef(Wk+1) — Ef(Wk)) = R < +∞ with R being a constant
and the set B := {k ∈ Z+|EkWk+1 — W* k2 ≥ EkWk — W* k2}, then
min {E (f (WD- f(W*))} = O (S + ɪ) ,	(12)
1≤k≤K	sK
where W* is the minimum of f. To obtain (∀ > 0) error, we set S = O() andK = O(1/2).
Theorem 2 relies on the assumption that Pk∈A or B Ef (Wk+1) —Ef(Wk) is bounded, and we
provide an empirical verification in Appendix C.1. We leave it open for how to establish the
convergence result for SRSGD without this assumption.
4	Experimental Results
We evaluate SRSGD on a variety of benchmarks for image classification, including CIFAR10,
CIFAR100, and ImageNet. In all experiments, we show the advantage of SRSGD over the widely
used and well-calibrated SGD baselines with a constant momentum of 0.9 and decreasing learning
5
Under review as a conference paper at ICLR 2021
---SGD ----SGD+ Momentum
---NASGD -----ARSGD ----SRSGD
MNIST
Iteration
12k
---SGD + Momentum---SRSGD
Figure 3: (a) Training loss comparison between different schemes in training logistic regression for
MNIST classification. Here, SGD is the plain SGD without momentum, and SGD + Momentum that
follows (3) and replaces gradient with the mini-batch stochastic gradient. NASGD is not robust to
noisy gradient, ARSGD almost degenerates to SGD, and SRSGD performs the best in this case. (b, c)
Training loss vs. training epoch of ResNet models trained with SRSGD (blue) and the SGD baseline
with constant momentum as in PyTorch implementation, which is denoted by SGD in Section 4 (red).
rate at certain epochs, which we denote as SGD. We also compare SRSGD with the well-calibrated
SGD in which we switch momentum to the Nesterov momentum of 0.9, and we denote this optimizer
as SGD + NM. We fine tune the SGD and SGD + NM baselines to obtain the best validation
performance, and we then adopt the same set of parameters for training with SRSGD. In the SRSGD
experiments, we tune the restart frequencies on small DNNsfor each task based on the validation
performance and apply the calibrated restart frequencies to large DNNsfor the same task. Note that
ARSGD is impractical for training on large-scale datasets since it requires to compute the loss over the
whole training set at each iteration, which is very computationally inefficient. Alternatively, ARSGD
can estimate loss and restart using mini-batches, but then ARSGD restarts too often and degenerates
to SGD without momentum as we mentioned in Section 3. Thus, we do not compare with ARSGD
in our CIFAR and ImageNet experiments. The details about hyper-parameters calibration can be
found in Appendix D.4. We provide the detailed description of datasets and experimental settings in
Appendix D. Additional experimental results in training LSTMs (Hochreiter & Schmidhuber, 1997)
and WGANs (Arjovsky et al., 2017; Gulrajani et al., 2017) with SRSGD, as well as the comparison
between SRSGD and SGD + NM on ImageNet classification task, are provided in Appendix E. We
also note that in all the following experiments, the training loss will blow up if we apply NASGD
without restart. These further confirm the stabilizing effect of scheduled restart in training DNNs.
4.1	CIFAR 1 0 and CIFAR 1 00
We summarize our results for CIFAR in Tables 1 and 2. We also explore two different restarting
frequency schedules for SRSGD: linear and exponential schedule. These schedules are governed by
two parameters: the initial restarting frequency F1 and the growth rate r. In both scheduling schemes,
the restarting frequency at the 1st learning rate stage is set to F1 during training. Then the restarting
frequency at the (k + 1)-th learning rate stage is determined by:
Fk+1
F1 × rk ,
F1 × (1 + (r - 1) × k),
exponential schedule
linear schedule.
We search F1 and r using the method outlined in Appendix D.4. For CIFAR10, (F1 = 40, r = 1.25)
and (F1 = 30, r = 2) are good initial restarting frequencies and growth rates for the exponential
and linear schedules, respectively. For CIFAR100, those values are (F1 = 45, r = 1.5) for the
exponential schedule and (F1 = 50, r = 2) for the linear schedule.
Improvement in Accuracy Increases with Depth. We observe that the linear schedule of restart
yields better test error on CIFAR than the exponential schedule for most of the models except for
Pre-ResNet-470 and Pre-ResNet-1001 on CIFAR100 (see Tables 1 and 2). SRSGD with either linear
or exponential restart schedule outperforms SGD. Furthermore, the advantage of SRSGD over SGD is
more significant for deeper networks. This observation holds strictly when using the linear schedule
(see Fig. 1) and is generally true when using the exponential schedule with only a few exceptions.
Faster Convergence Reduces the Training Time by Half. SRSGD also converges faster than SGD.
This result is consistent with our MNIST case study in Section 3 and indeed expected since SRSGD
6
Under review as a conference paper at ICLR 2021
Table 1: Classification test error (%) on CIFAR10 using SGD, SGD + NM, and SRSGD. We report
the results of SRSGD with two restarting schedules: linear (lin) and exponential (exp). The numbers
of iterations after which we restart the momentum in the lin schedule are 30, 60, 90, 120 for the
1st, 2nd, 3rd, and 4th stage. Those numbers for the exp schedule are 40, 50, 63, 78. We include the
reported results from (He et al., 2016b) (in parentheses) in addition to our reproduced results.
Network	# Params	SGD (baseline)	SGD+NM	SRSGD (lin)	SRSGD (exp)	Improve over SGD (lin/exp)	Improve over SGD+NM (lin/exp)
Pre-ResNet-110	~1.1M	5.25 ± 0.14 (6.37)	5.24 ± 0.16	4.93 ± 0.13	5.00 ± 0.47	0.32/0.25	0.31/0.24
Pre-ResNet-290	3.0M	5.05 ± 0.23	5.04 ± 0.12	4.37 ± 0.15	4.50 ± 0.18	0.68/0.55	0.67/0.54
Pre-ResNet-470	4.9M	4.92 ± 0.10	4.97 ± 0.15	4.18 ± 0.09	4.49 ± 0.19	0.74/0.43	0.79/0.48
Pre-ResNet-650	6.7M	4.87 ± 0.14	4.80 ± 0.14	4.00 ± 0.07	4.40 ± 0.13	0.87/0.47	0.80/0.40
Pre-ResNet-1001	10.3M	4.84 ± 0.19 (4.92)	4.62 ± 0.14	3.87 ± 0.07	4.13 ± 0.10	0.97/0.71	0.75/0.49
Table 2: Classification test error (%) on CIFAR100 using SGD, SGD + NM, and SRSGD. We report
the results of SRSGD with two restarting schedules: linear (lin) and exponential (exp). The numbers
of iterations after which we restart the momentum in the lin schedule are 50, 100, 150, 200 for the
1st, 2nd, 3rd, and 4th stage. Those numbers for the exp schedule are 45, 68, 101, 152. We include the
reported results from (He et al., 2016b) (in parentheses) in addition to our reproduced results.
Network	# Params	SGD (baseline)	SGD+NM	SRSGD (lin)	SRSGD (exp)	Improve over SGD (lin/exp)	Improve over SGD+NM (lin/exp)
Pre-ReSNet-110	1.2M	23.75 ± 0.20	23.65 ± 0.36	23.49 ± 0.23	23.50 ± 0.39	0.26/0.25	0.16/0.15
Pre-ResNet-290	3.0M	21.78 ± 0.21	21.68 ± 0.21	21.49 ± 0.27	21.58 ± 0.20	0.29/0.20	0.19/0.10
Pre-ResNet-470	4.9M	21.43 ± 0.30	21.21 ± 0.30	20.71 ± 0.32	20.64 ± 0.18	0.72/0.79	0.50/0.57
Pre-ResNet-650	6.7M	21.27 ± 0.14	21.04 ± 0.38	20.36 ± 0.25	20.41 ± 0.21	0.91/0.86	0.68/0.63
Pre-ResNet-1001	10.4M	20.87 ± 0.20 (22.71)	20.13 ± 0.16	19.75 ± 0.11	19.53 ± 0.19	1.12/1.34	0.38/0.60
Table 3: On CIFAR10/100 (%), SRSGD training with only 100 epochs achieves comparable classifi-
cation errors (%) to the SGD baseline training with 200 epochs.
	CIFAR10		CIFAR100	
Network	SRSGD	Improvement	SRSGD	Improvement
Pre-ResNet-110	5.43 ± 0.18	-0.18	23.85 ± 0.19	-0.10
Pre-ResNet-290	4.83 ± 0.11	0.22	21.77 ± 0.43	0.01
Pre-ResNet-470	4.64 ± 0.17	0.28	21.42 ± 0.19	0.01
Pre-ResNet-650	4.43 ± 0.14	0.44	21.04 ± 0.20	0.23
Pre-ResNet-1001	4.17 ± 0.20	0.67	20.27 ± 0.11	0.60
Pre-ResNet-110	5.25 ± 0.10 (110epochs)	0.00	23.73 ± 0.23 (140 epochs)	002
Table 4: Test errors on CIFAR10 (%) of Pre-ResNet-110 (Left)/290 (Right) using different optimizers.
SRSGD Adam RMSProp SRSGD Adam RMSProp
4.93 ± 0.13% 6.83 ± 0.10% 7.31 ± 0.31% 4.37 ± 0.15% 6.12 ± 0.18% 7.18 ± 0.05%
can avoid the error accumulation when there is an inexact oracle. For CIFAR, Fig. 3 (b) shows that
SRSGD yields smaller training loss than SGD during the training. Interestingly, SRSGD converges
quickly to good loss values in the 2nd and 3rd stages. This suggests that the model can be trained
with SRSGD in many fewer epochs compared to SGD while achieving a similar error rate.
Results in Table 3 confirm the hypothesis above. We train Pre-ResNet models with SRSGD in only
100 epochs, decreasing the learning rate by a factor of 10 at the 80th, 90th, and 95th epoch while using
the same linear schedule for restarting frequency as before with (F1 = 30, r = 2) for CIFAR10 and
(F1 = 50, r = 2) for CIFAR100. We compare the test error of the trained models with those trained
by the SGD baseline in 200 epochs. We observe that SRSGD training consistently yields lower test
errors than SGD except for the case of Pre-ResNet-110 even though the number of training epochs
of our method is only half of the number of training epochs required by SGD. For Pre-ResNet-110,
SRSGD needs 110 epochs with learning rate decreased at the 80th, 90th, and 100th epoch to achieve
the same error rate as the 200-epoch SGD training on CIFAR10. On CIFAR100, SRSGD training for
Pre-ResNet-110 needs 140 epochs with learning rate decreased at the 80th, 100th and 120th epoch to
outperform the 200-epoch SGD. Comparison with SGD short training is provided in Appendix F.2.
Comparison with Adam and RMSProp. SRSGD outperforms not only SGD with momentum
but also other popular optimizers including Adam and RMSProp (Tieleman & Hinton, 2012) for
image classification tasks. In fact, for image classification tasks, Adam and RMSProp yield worse
performance than the baseline SGD with momentum (Chen & Kyrillidis, 2019). Table 4 compares
SRSGD with Adam and RMSprop on CIFAR10.
7
Under review as a conference paper at ICLR 2021
Table 5: Single crop validation errors (%) on ImageNet of ResNets trained with SGD baseline and
SRSGD. We report the results of SRSGD with the increasing restarting frequency in the first two
learning rates. In the last learning rate, the restarting frequency is linearly decreased to 1. For baseline
results, we also include the reported single-crop validation errors (He et al., 2016c) (in parentheses).
Network	# Params	SGD		SRSGD		Improvement	
		top-1	top-5	top-1	top-5	top-1	top-5
ResNet-50	25.56M	24.11 ± 0.10(24.70)	7.22 ± 0.14 (7.80)	23.85 ± 0.09	7.10 ± 0.09	0.26	0.12
ResNet-101	44.55M	22.42 ± 0.03 (23.60)	6.22 ± 0.01 (7.10)	22.06 ± 0.10	6.09 ± 0.07	0.36	0.13
ResNet-152	60.19M	22.03 ± 0.12 (23.00)	6.04 ± 0.07 (6.70)	21.46 ± 0.07	5.69 ± 0.03	0.57	0.35
ResNet-200	64.67M	22.13 ± 0.12	6.00 ± 0.07	20.93 ± 0.13	5.57 ± 0.05	1.20	0.43
Table 6: Comparison of single crop validation errors on ImageNet (%) between SRSGD training with
fewer epochs and SGD training with full 90 epochs.
Network	SRSGD	Reduction	Improvement	Network	SRSGD	Reduction	Improvement
ResNet-50	24.30 ± 0.21	10	--0.19-	ResNet-152	21.79 ± 0.07	15	0.24
ResNet-101	22.32 ± 0.06	10	0.1	ResNet-200	21.92 ± 0.17	30	0.21
4.2	ImageNet
Next, we discuss our experimental results on the 1000-way ImageNet classification task (Russakovsky
et al., 2015). We conduct our experiments on ResNet-50, 101, 152, and 200 with 5 different seeds. We
use the official PyTorch implementation for all of our ResNet models (Paszke et al., 2019). Following
common practice, we train each model for 90 epochs and decrease the learning rate by a factor of 10
at the 30th and 60th epoch. We use an initial learning rate of 0.1, a momentum scaled by 0.9, and a
weight decay value of 0.0001. Additional details and comparisons between SRSGD and SGD + NM
are given in Appendix E.
We report single crop validation errors of ResNet models trained with SGD and SRSGD on ImageNet
in Table 5. In contrast to our CIFAR experiments, we observe that for ResNets trained on ImageNet
with SRSGD, linearly decreasing the restarting frequency to 1 at the last stage (i.e., after the 60th
epoch) helps improve the generalization of the models. Thus, in our experiments, we use linear
scheduling with (F1 = 40, r = 2). From epoch 60 to 90, the restarting frequency decays to 1 linearly.
Advantage of SRSGD continues to grow with depth. Similar to the CIFAR experiments, we
observe that SRSGD outperforms the SGD baseline for all ResNet models that we study. As shown
in Fig. 1, the advantage of SRSGD over SGD grows with network depth, just as in our CIFAR
experiments with Pre-ResNet architectures.
Avoiding Overfitting in ResNet-200. ResNet-200 demonstrates that SRSGD is better than the SGD
baseline at avoiding overfitting2. The ResNet-200 trained with SGD has a top-1 error of 22.13%,
higher than the ResNet-152 trained with SGD, which achieves a top-1 error of 22.03% (see Table 5).
He et al. (2016b) pointed out that ResNet-200 suffers from overfitting. The ResNet-200 trained with
our SRSGD has a top-1 error of 20.93%, which is 1.2% lower than the ResNet-200 trained with the
SGD and also lower than the ResNet-152 trained with both SRSGD and SGD, an improvement by
0.53% and 1.1%, respectively. We hypothesize that SRSGD with appropriate restart frequency is
locally not monotonic (see Fig. 3 (b, c)), and this property allows SRSGD to escape from bad minima
in order to reach a better one, which helps avoid overfitting in very deep networks. Theoretical
analysis of the observation that SRSGD is less overfitting in training DNNs is under our investigation.
Training ImageNet in Fewer Number of Epochs. As in the CIFAR experiments, we note that
when training on ImageNet, SRSGD converges faster than SGD at the first and last learning rate
while quickly reaching a good loss value at the second learning rate (see Fig. 3 (c)). This observation
suggests that ResNets can be trained with SRSGD in fewer epochs while still achieving comparable
error rates to the same models trained by the SGD baseline using all 90 epochs. We summarize the
results in Table 6. On ImageNet, we note that SRSGD helps reduce the number of training epochs
for very deep networks (ResNet-101, 152, 200). For smaller networks like ResNet-50, training with
fewer epochs slightly decreases the accuracy.
4.3	Empirical Analysis
SRSGD Helps Reduce the Training Time. We find that SRSGD training using fewer epochs yields
comparable error rates to both the SGD baseline and the SRSGD full training with 200 epochs on
CIFAR. We conduct an ablation study to understand the impact of reducing the number of epochs
2By overfitting, we mean that the model achieves low training error but high test error.
8
Under review as a conference paper at ICLR 2021
on the final error rate when training with SRSGD on CIFAR10 and ImageNet. In the CIFAR10
experiments, we vary the number of epoch reduction from 15 to 90 while in the ImageNet experiments,
we vary the number of epoch reduction from 10 to 30. We summarize our results in Fig. 4, and
provide detailed results in Appendix F. For CIFAR10, we can train with 30 fewer epochs while still
maintaining a comparable error rate to the full SRSGD training, and with a better error rate than
the SGD baseline trained in full 200 epochs. For ImageNet, SRSGD training with fewer epochs
decreases the accuracy but still obtains comparable results to the 90-epoch SGD baseline.
Pre-ResNet-290^-Pre-ResNet-470	ResNet-152	ResNet-200
_ _ results for the corresponding baseline SGD trainings which
■ - use 200 epochs for CIFAR10 and 90 epochs for ImageNet
TeSt Error
ImageNet
」。」-uo-sp=e>
do」。ə-mu-s
90	80 75 70 65 60
Number of Training Epochs
Figure 4: Test error vs. number of training epochs.
Dashed lines are test errors of SGD trained with
200 epochs for CIFAR10 (left) and 90 epochs for
ImageNet (right), For CIFAR, SRSGD with fewer
epochs achieves comparable results to SRSGD
with 200 epochs. For ImageNet, training with
less epochs slightly decreases the performance of
SRSGd but still achieves comparable results to
200-epoch SGD.
Impact of Restarting Frequency. We examine the impact of restarting frequency on the network
training. We choose a case study of training a Pre-ResNet-290 on CIFAR10 using SRSGD with a
linear schedule scheme for the restarting frequency. We fix the growth rate r = 2 and vary the initial
restarting frequency Fi from 1 to 80. As shown in Fig. 5, SRSGD with a large Fι, e.g. Fi = 80,
approximates NASGd (yellow). We also show the training loss and test accuracy ofNASGD in red.
As discussed in Section 3, it suffers from error accumulation due to stochastic gradients and converges
slowly or even diverges. SRSGD with small Fi, e.g, Fi = 1, approximates SGD without momentum
(green). It converges faster initially but reaches a worse local minimum (i.e. larger loss). Typical
srsgd (blue) converges faster than NASGD and to a better local minimum than both NASGD and
sgd without momentum. It also achieves the best test error. We provide more empirical analysis
results in Appendix F, G and H. The impact of the growth rate r is studied in Appendix G.2.
---Fi=I ---Fι=10 ---Fi=30 ---Fι=50 ---F]=80 --NASGD(Fι=+∞)
Sso-U-
O 1
1010-
Initial Restarting Frequency (FJ
Figure 5: Training loss (left) and test error (right) of Pre-ResNet-290 trained on CIFAR10 with
different initial restarting frequencies Fi (linear schedule). SRSGD with small Fi approximates SGD
without momentum, while SrSGD with large Fi approximates NASGD. The training loss curve
and test accuracy of NASGD are shown in red and confirm the result of Theorem 1 that NASGD
accumulates error due to the stochastic gradients.
5 Conclusions
We propose the Scheduled Restart SGD (SRSGD), with two major changes from the widely used
SGD with constant momentum. First, we replace the momentum in SGD with the iteration-dependent
momentum that used in Nesterov accelerated gradient (NAG). Second, we restart the NAG momentum
according to a schedule to prevent error accumulation when the stochastic gradient is used. For image
classification, SRSGD can significantly improve the accuracy of the trained DNNs. Also, compared to
the SGD baseline, SRSGD requires fewer training epochs to reach the same trained model,s accuracy.
There are numerous avenues for future work: 1) deriving the optimal restart scheduling and the
corresponding convergence rate of SRSGD and 2) integrating the scheduled restart NAG momentum
with adaptive learning rate algorithms, e.g., Adam (Kingma & Ba, 2014).
9
Under review as a conference paper at ICLR 2021
References
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial net-
works. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Con-
ference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp.
214-223, International Convention Centre, Sydney, Australia, 06-11 Aug 2017. PMLR. URL
http://proceedings.mlr.press/v70/arjovsky17a.html.
Mahmoud Assran and Michael Rabbat. On the convergence of nesterov’s accelerated gradient method
in stochastic settings. arXiv preprint arXiv:2002.12414, 2020.
Necdet Serhat Aybat, Alireza Fallah, Mert Gurbuzbalaban, and Asuman Ozdaglar. Robust accelerated
gradient methods for smooth strongly convex functions. arXiv preprint arXiv:1805.10579, 2018.
Necdet Serhat Aybat, Alireza Fallah, Mert Gurbuzbalaban, and Asuman Ozdaglar. A universally
optimal multistage accelerated stochastic gradient method. In Advances in Neural Information
Processing Systems, pp. 8525-8536, 2019.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. SIAM Journal on Imaging Sciences, 2(1):183-202, 2009.
Yoshua Bengio, Nicolas Boulanger-Lewandowski, and Razvan Pascanu. Advances in optimizing
recurrent networks. In 2013 IEEE International Conference on Acoustics, Speech and Signal
Processing, pp. 8624-8628. IEEE, 2013.
Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. Siam Review, 60(2):223-311, 2018.
Sebastien Bubeck. Convex optimization: Algorithms and complexity. arXivpreprintarXiv:1405.4980,
2014.
Augustin Cauchy. MethOde generale pour la resolution des systemes d'equations SimUItan6es. Comp.
Rend. Sci. Paris, 1847.
John Chen and Anastasios Kyrillidis. Decaying momentum helps neural network training. arXiv
preprint arXiv:1910.04952, 2019.
Francois Chollet et al. Keras. https://keras.io, 2015.
Michael B Cohen, Jelena Diakonikolas, and Lorenzo Orecchia. On acceleration with noise-corrupted
gradients. arXiv preprint arXiv:1805.12591, 2018.
Olivier Devolder, FranCOis Glineur, and Yurii Nesterov. First-order methods of smooth convex
optimization with inexact oracle. Mathematical Programming, 146(1-2):37-75, 2014.
Timothy Dozat. Incorporating Nesterov momentum into Adam. 2016.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Robert M Freund and Haihao Lu. New computational guarantees for solving convex optimization
problems with first order methods, via a function growth condition measure. Mathematical
Programming, 170(2):445-477, 2018.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic
programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and
stochastic programming. Mathematical Programming, 156(1-2):59-99, 2016.
Pontus Giselsson and Stephen Boyd. Monotonicity and restart in fast gradient methods. In 53rd IEEE
Conference on Decision and Control, pp. 5058-5063. IEEE, 2014.
Gabriel Goh. Why momentum really works. Distill, 2(4):e6, 2017.
10
Under review as a conference paper at ICLR 2021
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.
Improved training of Wasserstein GANs. In Advances in Neural Information Processing Systems,
pp. 5767-5777, 2017.
Moritz Hardt. Robustness versus acceleration. http://blog.mrtz.org/2014/08/18/
robustness-versus-acceleration.html, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 770-778, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European Conference on Computer Vision, pp. 630-645. Springer, 2016b.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual networks. https://github.
com/KaimingHe/deep-residual-networks, 2016c.
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning lecture
6a overview of mini-batch gradient descent.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):
1735-1780, 1997.
W Ronny Huang, Zeyad Emam, Micah Goldblum, Liam Fowl, Justin K Terry, Furong Huang, and Tom
Goldstein. Understanding generalization through visualizations. arXiv preprint arXiv:1906.03291,
2019.
Anatoli Iouditski and Yuri Nesterov. Primal-dual subgradient methods for minimizing uniformly
convex functions. arXiv preprint arXiv:1401.1792, 2014.
Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Accelerating
stochastic gradient descent for least squares regression. In Conference On Learning Theory, pp.
545-604, 2018.
Chi Jin, Praneeth Netrapalli, and Michael I Jordan. Accelerated gradient descent escapes saddle
points faster than gradient descent. arXiv preprint arXiv:1711.10456, 2017.
Rahul Kidambi, Praneeth Netrapalli, Prateek Jain, and Sham Kakade. On the insufficiency of existing
momentum schemes for stochastic optimization. In 2018 Information Theory and Applications
Workshop (ITA), pp. 1-9. IEEE, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Andrei Kulunchakov and Julien Mairal. A generic acceleration framework for stochastic composite
optimization. In Advances in Neural Information Processing Systems, pp. 12556-12567, 2019.
Guanghui Lan. An optimal method for stochastic composite optimization. Mathematical Program-
ming, 133(1-2):365-397, 2012.
Quoc V Le, Navdeep Jaitly, and Geoffrey E Hinton. A simple way to initialize recurrent networks of
rectified linear units. arXiv preprint arXiv:1504.00941, 2015.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.lecun.
com/exdb/mnist/.
Qihang Lin and Lin Xiao. An adaptive accelerated proximal gradient method and its homotopy
continuation for sparse optimization. In International Conference on Machine Learning, pp. 73-81,
2014.
Chaoyue Liu and Mikhail Belkin. Accelerating sgd with momentum for over-parameterized learning.
In International Conference on Learning Representations, 2020. URL https://openreview.net/
forum?id=r1gixp4FPH.
11
Under review as a conference paper at ICLR 2021
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei
Han. On the variance of the adaptive learning rate and beyond. In International Conference on
Learning Representations, 2020. URL https://openreview.net/forum?id=rkgz2aEKDr.
Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv
preprint arXiv:1608.03983, 2016.
Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. 2018.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine
LearningResearch, 9(Nov):2579-2605, 2008.
Boris S Mordukhovich. Variational analysis and generalized differentiation I: Basic theory, volume
330. Springer Science & Business Media, 2006.
Arkaddii S Nemirovskii and Yu E Nesterov. Optimal methods of smooth convex minimization. USSR
Computational Mathematics and Mathematical Physics, 25(2):21-30, 1985.
Yu Nesterov. Gradient methods for minimizing composite functions. Mathematical Programming,
140(1):125-161, 2013.
Yurii Nesterov. Introductory lectures on convex programming volume i: Basic course. 1998.
Yurii Nesterov. Lectures on convex optimization, volume 137. Springer, 2018.
Yurii E Nesterov. A method for solving the convex programming problem with convergence rate o
(1∕k^ 2). In Dokl. Akad. Nauk Sssr, volume 269,pp. 543-547, 1983.
Brendan O’donoghue and Emmanuel Candes. Adaptive restart for accelerated gradient schemes.
Foundations of Computational Mathematics, 15(3):715-732, 2015.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. In Advances in Neural Information Processing Systems,
pp. 8024-8035, 2019.
Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR
Computational Mathematics and Mathematical Physics, 4(5):1-17, 1964.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. arXiv
preprint arXiv:1904.09237, 2019.
James Renegar. Efficient first-order methods for linear programming and semidefinite programming.
arXiv preprint arXiv:1409.5832, 2014.
R Tyrrell Rockafellar. Convex analysis. Number 28. Princeton university press, 1970.
R Tyrrell Rockafellar and Roger J-B Wets. Variational analysis, volume 317. Springer Science &
Business Media, 2009.
Vincent Roulet and Alexandre d’Aspremont. Sharpness, restart and acceleration. In Advances in
Neural Information Processing Systems, pp. 1119-1129, 2017.
Vincent Roulet, Nicolas Boumal, and Alexandre d’Aspremont. Computational complexity versus
statistical performance on sparse recovery problems. arXiv preprint arXiv:1506.03295, 2015.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition
challenge. International Journal of Computer Vision, 115(3):211-252, 2015.
Weijie Su, Stephen Boyd, and Emmanuel Candes. A differential equation for modeling nesterov’s
accelerated gradient method: Theory and insights. In Advances in Neural Information Processing
Systems, pp. 2510-2518, 2014.
12
Under review as a conference paper at ICLR 2021
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization
and momentum in deep learning. In International Conference on Machine Learning, pp. 1139-
1147, 2013.
T. Tieleman and G. Hinton. Lecture 6.5—RmsProp: Divide the gradient by a running average of its
recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.
Svante Wold, Kim Esbensen, and Paul Geladi. Principal component analysis. Chemometrics and
Intelligent Laboratory Systems, 2(1-3):37-52, 1987.
Wei Yang. Pytorch classification. https://github.com/bearpaw/pytorch-classification, 2017.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
Sixin Zhang, Anna E Choromanska, and Yann LeCun. Deep learning with elastic averaging sgd. In
Advances in Neural Information Processing Systems, pp. 685-693, 2015.
Martin Zinkevich, Markus Weimer, Lihong Li, and Alex J Smola. Parallelized stochastic gradient
descent. In Advances in Neural Information Processing Systems, pp. 2595-2603, 2010.
13
Under review as a conference paper at ICLR 2021
Part
Appendices
The appendices are structured as follows. In Section A, we prove Theorem 1. In Section B, we
review an error accumulation result of the Nesterov accelerated gradient with δ-inexact gradient. In
Section C, we prove Theorem 2. In Section D, we provide some experimental details; in particular,
the calibration of restarting hyperparameters. In Section E, we compare SRSGD with benchmark
optimization algorithms on some other tasks, including training LSTM and Wasserstein GAN. In
Section F, we provide detailed experimental settings in studying the effects of reducing the number
of epoch in training deep neural networks with SRSGD, and we provide some more experimental
results. In Section G and H, we further study the effects of restarting frequency and training with
less epochs by using SRSGD. In Section I, we visualize the optimization trajectory of SRSGD and
compare it with benchmark methods. A snippet of our implementation of SRSGD in PyTorch and
Keras are available in Section J and K, respectively.
Table of Contents
Appendix	14
A	Uncontrolled Bound of NASGD	15
A.1 Preliminaries ........................................................... 15
A.2 Uncontrolled Bound of NASGD: Analysis ................................... 16
B NAG with δ-Inexact Oracle & Experimental Settings in Section 3.1	19
C	Convergence of SRSGD	20
C.1 Numerical Verification of the assumptions in Theorem 2 .................. 22
D	Datasets and Implementation Details	23
D.1 CIFAR ................................................................... 23
D.2 ImageNet ................................................................ 23
D.3 Training ImageNet in Fewer Number of Epochs: ............................ 23
D.4 Details on Restarting Hyper-parameters Search ........................... 23
E	SRSGD vs. SGD and SGD + NM on ImageNet Classification and Other Tasks	24
E.1 Comparing with SGD with Nesterov Momentum on ImageNet Classification . .	24
E.2 Long Short-Term Memory (LSTM) Training for Pixel-by-Pixel MNIST ......... 24
E.3 Wasserstein Generative Adversarial Networks (WGAN) Training on MNIST . .	25
F Error Rate vs. Reduction in Training Epochs	27
F.1	Implementation Details ................................................ 27
F.2	Short Training on CIFAR10/CIFAR100 Using SGD .......................... 28
F.3	Additional Experimental Results ....................................... 28
G Impact of Restarting Frequency for ImageNet and CIFAR100	29
G.1	Implementation Details ................................................ 29
G.2	Impact of the Growth Rate r ........................................... 30
G.3	Additional Experimental Results ....................................... 31
H Full Training with Less Epochs at the Intermediate Learning Rates	32
I Visualization of SRSGD’s trajectory	33
J SRSGD Implementation in Pytorch	35
K SRSGD Implementation in Keras	36
14
Under review as a conference paper at ICLR 2021
A Uncontrolled Bound of NASGD
Consider the following optimization problem
min f (w),	(13)
w
where f (w) is L-smooth and convex.
Start from Wk, GD update, with step size 1, can be obtained based on the minimization of the
u cto	Qr (v, Wk ):= hv - Wk, W(Wkyi + 2 kv - Wk k2∙	(14)
With direct computation, we can get that
Qr(vk+1, Wk) - min Q,(v, Wk )= kgk -；f (wk)k2 ,
where gk := ^^ Pjm=I Nfij (Wk). We assume the variance is bounded, which gives The stochastic
gradient rule, Rs, satisfies E[Qr(vk+1, Wk) 一 min Qr(v, Wk)∣χk] ≤ δ, with δ being a constant and
Xk being the sigma algebra generated by w1, w2,…，Wk, i.e.,
Xk := σ(W1, w2,∙∙∙ , Wk).
NASGD can be reformulated as
vk+1 ≈
arg min Qr (v, Wk) with rule Rs ,
v
Wk + 1 =	vk+1 + tk-1 (vk+1 - Vk ),
tk+1
where to = 1 and tk+ι = (1 + Pmtk)/2.
(15)
A.1 Preliminaries
To proceed, we introduce several definitions and some useful properties in variational and convex anal-
ysis. More detailed background can be found at Mordukhovich (2006); Nesterov (1998); Rockafellar
& Wets (2009); Rockafellar (1970).
Let f be a convex function, we say that f is L-smooth (gradient Lipschitz) if f is differentiable and
kNf(v) -Nf(W)k2 ≤ Lkv - Wk2,
and we say f is ν-strongly convex if for any W, v ∈ dom(f)
f(w) ≥ f(v) + Nf(Vy W - Vi + 2∣∣W 一 vk2.
Below of this subsection, we list several basic but useful lemmas, the proof can be found in Nesterov
(1998).
Lemma 1. If f is ν-strongly convex, then for any V ∈ dom(f) we have
f(v)-f(v*) ≥ 2kv -V*k2,	(16)
where v* is the minimizer of f.
Lemma 2. Iff is L-smooth, for any W, v ∈ dom(f),
f (w) ≤ f(v) + Nf(Vy w - Vi + 2∣∣w ― v∣2.
15
Under review as a conference paper at ICLR 2021
A.2 UNCONTROLLED BOUND OF NASGD: ANALYSIS
In this part, we denote
vk+1 := arg min Qr (v, wk).	(17)
V
Lemma 3. Ifthe constant r > 0, then
E (∣∣vk+1- vk+1∣∣2∣χk) ≤ 2rδ.	(18)
Proof. Note that Qr (v, wk) is strongly convex with constant r, and Vk+1 in (17) is the minimizer of
Qr (v, wk). With Lemma 1 we have
Qr (Vk+1, Wk ) - Qr (Vk+1, Wk ) ≥ W ∣∣Vk+1 - Vk + %.	(19)
Notice that
E [Qr (vk + 1, Wk) — Qr(Vk+ 1, Wk)] = E [Qr (vk+1, Wk ) — min Qr(v, Wk )] ≤ δ,
The inequality (18) can be established by combining the above two inequalities.
Lemma 4. Ifthe constant satisfy r > L, then we have
E f(Vk+1) + 2∣∣Vk+1 - Wkk2 - (f(vk+1) + rkvk+1 - Wkk2
□
(20)
≥
where T = M¾
-τδ- WE[∣∣Wk - Vk+1 图
+ 1.
Proof. The convexity of f gives US
0 ≤ "f (vk+1), vk+1 - Vk+1i + f (Vk+1) - f (vk+1).	(21)
From the definition of the stochastic gradient rule &, we have
-δ ≤ E (Qr(Vk+1, Wk) - Qr(Vk+1, Wk))	(22)
= E [(Vk+1 - Wk, Vf(Wk))+ 2∣∣Vk+1 - WkIII]-
E [(vk+1 - Wk, Vf (Wk))+ 2 ∣∣vk+1 - Wk ∣∣∣].
With (21) and (22), we have
-δ ≤ (f(Vk+1) + 2 ∣Vk+1- Wk k∣) - (f(vk+1) + 2∣vk+1- Wk k∣) +	(23)
EhVf(Wk) -Vf(Vk+1), Vk+1 - vk+1).
With the Schwarz inequality(a, b〉≤ k∣k2 + ∣∣∣b∣∣∣ with μ = rL2L, a = Vf(Vk+1) - Vf(Vk+1)
and b = Wk - Vk+1,
Wf(Wk) -Vf(Vk+1), Vk+1 - Vk+1i	(24)
≤ ⅛L)IVf(Wk) - Vf(Vk+1)k∣ +	∣Vk+1 - Vk+1∣∣
2L	2(r — L)
≤ '-攵∣Wk - VkrII+ 至，∣Vk+1-Vk+1∣∣.
Combining (23) and (24), we have
-δ ≤ E (f(Vk+1) + r ∣Vk+1- Wk∣∣∣) - E (f(Vk+1) + 2∣Vk+1- Wk∣∣∣)	(25)
+ 斤 JE∣Vk+1 - Vk+1∣∣ + 一EkWk - Vk+11∣.
2(r - L)	2
By rearrangement of the above inequality (25) and using Lemma 3, we obtain the result. □
16
Under review as a conference paper at ICLR 2021
Lemma 5. If the constants satisfy r > L, then we have the following bounds
E f(vk) - f(vk+1)) ≥ rEkwk - vk+1k2 + rE(wk - Vk,Vk+1 - Wki - τδ,
(26)
E (f(v*) - f(vk+1)) ≥ rEkwk - vk+1k2 + rEhwk - v*, Vk+1 - wk)- τδ, (27)
2
where T := 丁(；_匕)+ 1 and v* is the minimum.
Proof. With Lemma 2, we have
-f (Vk+1) ≥ -f(wk) - hVk+1 - wk, Vf (wk)i - LkVk+1 - wkk2.	(28)
Using the convexity of f, we have
f(vk) - f(wk) ≥ hvk -wk,Vf(wk)i,
i.e.,
f(vk) ≥ f(wk) + hvk - wk, Vf(wk)i.	(29)
According to the definition of Vk+1 in (14), i.e.,
Vk+1 = arg min Qr (v, wk) = argmin<v — wk, Vf (wk)) + r ∣∣v — wkk2,
vv	2
and the optimization condition gives
Vk+1 = wk - 1 Vf (wk).	(30)
r
Substituting (30) into (29), we obtain
f (vk) ≥ f(wk) + hvk - wk,r(wk - Vk+1)).	(31)
Direct summation of (28) and (31) gives
f (vk) - f(Vk+1) ≥ (r - 2) kVk+1 - wk∣∣2 + rhwk - Vk, Vk+1 - wk).	(32)
Summing (32) and (20), we obtain the inequality (26)
E [f(vk) - f (vk+1)] ≥ 2Ekwk - vk+1k2 + rEhwk - Vk, Vk+1 - wk) - τδ. (33)
On the other hand, with the convexity of f , we have
f (v*) - f (wk) ≥ hv* - wk, Vf (wk)) = hv* - wk,r(wk - Vk+1)).	(34)
The summation of (28) and (34) results in
f (v*) - f (Vk+1) ≥ (r - 2) kwk - Vk+1k2 + rhwk - v*, Vk+1 - wk).	(35)
Summing (35) and (20), we obtain
E (f(v*) - f(vk+1)) ≥ rEkwk - vk+1k2 + rEhwk - v*, Vk+1 - wk) - τδ, (36)
which is the same as (27).	□
Theorem 3 (Uncontrolled Bound of NASGD (Theorem 1 with detailed bounded)). Let the constant
r satisfy r < L and the sequence {Vk}k≥0 be generated by NASGD with stochastic gradient that has
bounded variance. By using any constant step size sk ≡ s ≤ 1/L, then we have
E[f (vk) - minf(v)] ≤ (" + R2)4k.	(37)
v	r3
17
Under review as a conference paper at ICLR 2021
Proof. We denote
Fk := E(f(vk) - f(v*)).
By (26) × (tk - 1) + (27), we have
21(tk- I)Fk - tkFkH ≥ t	k+1- Wkk2
r
+ 2E(Vk+1 - wk,tkwk - (tk - 1)vk - v*〉一 2τtkδ
r
With 埼—ι =垃-tk, (38) × tk yields
2[t"Fk-坎Fk+1]
r
≥ Ektk vk+1 - tk wk ∣∣2
+ 2tkE{Vk+1 - wk,tkWk - (tk - 1)vk - v*〉- 2τtkδ
(38)
(39)
Substituting a = tkvk+1 - (tk - 1)vk - v* and b = tkwk - (tk - 1)vk - v* into identity
Ila - bk2 + 2ha - b, bi = Ilak2 - kbk2∙	(40)
It follows that
EktkVk+1 - tkwk∣∣2 + 2tkE(vk+1 - wk,tkwk - (tk - 1)vk - v*〉	(41)
=EktkVk+1 - tkwk∣2 + 2tkE(vk+1 - wk,tkwk - (tk - 1)vk - v*〉
+2tkE(vk+1 - vk+1,tkwk - (tk - 1)vk - v*〉
=EktkVk+1	-	(tk	-	1)vk	- v*kl -ktkwk	-	(tk	- 1)vk - v*∣2
(40)
+2tkE(vk+1 - vk+1,tkwk - (tk - 1)vk - v*〉
=EktkVk+1 - (tk - 1)vk - v*kl - Ektk-IVk - (tk-ι - 1)VkT- v*k2
+ 2tkEhVk+1 - vk+1,tk-ιvk - (tk-ι - 1)vk-1 - v*〉.
In the third identity, we used the fact tkwk = tkvk + (tk-ι - 1)(vk - Vk-1). If we denote
Uk = Ektk-IVk - (tk-ι - 1)vk-1 - v*k2, (39) can be rewritten as
如丝+uk+ι <
r
+
≤
y+uk +号
2tkE(vk+1 - Vk+1,tk-ivk - (tk-i - 1)vk-1 - v*〉
2tkFk	k 2τtk δ ,2 n2
-Jkr- + Uk +	+ tk-1R2,
where we used
2tkEhvk+1 - Vk+1,tk-ivk - (tk-i - 1)vk-1 - v*〉
≤ 琉Ekvk+1 - Vk+1k2 + Ektk-IVk - (tk-1vk - (tk-1 - 1)vk-1 - v*)k2
=2tk δ∕r + t"R2∙
Denoting
2t2k F k
ξk ：=	k-1	+ Uk,
r
then, we have
_	k-.	_	-C
.	.	2τδ	c2∖V^ 2	∕2τδ	2∖ k3
ξk + 1 ≤ ξ0 + (---+ R ) Eti = (-----+ R Hf ∙
r	z—r	r 3
i=1
(42)
(43)
2t2 ,Fk .
With the fact, ξk ≥ —k-1	≥ k2Fk /4, we then proved the result.
□
18
Under review as a conference paper at ICLR 2021
B NAG with δ-INEXACT Oracle & Experimental Settings in Section
3.1
In Devolder et al. (2014), the authors defines δ-inexact gradient oracle for convex smooth optimization
as follows:
Definition 1 (δ-Inexact Oracle). Devolder et al. (2014) Fora convex L-smooth function f : Rd → R.
For ∀w ∈ Rd and exact first-order oracle returns a pair (f (w), Vf (W)) ∈ R X Rd so that for
∀v ∈ Rd we have
0 ≤ f(v) - (f(w) + hVf(w), V — Wi) ≤ L2I∣w — vk2.
A δ-inexact oracle returns a pair fδ(w), Vfδ(w) ∈ R × Rd so that ∀v ∈ Rd we have
0 ≤ f(v) - (fδ(W) + hVfδ(w), V — Wi) ≤ LLkw - v∣2 + δ.
We have the following convergence results of GD and NAG under a δ-Inexact Oracle for convex
smooth optimization.
Theorem 4. Devolder et al. (2014)3 Consider
min f (W), W ∈ Rd,
where f (W) is convex and L-smooth with w* being the minimum. Given access to δ-inexact oracle,
GD with step size 1/L returns a point Wk after k steps so that
f (Wk) - f (W*) = O(k) + δ.
On the other hand, NAG, with step size 1/L returns
f (Wk) - f (W*) = θ(]) + O(kδ).
Theorem 4 says that NAG may not robust to a δ-inexact gradient. In the following, we will study the
numerical behavior of a variety of first-order algorithms for convex smooth optimizations with the
following different inexact gradients.
Constant Variance Gaussian Noise: We consider the inexact oracle where the true gradient is
contaminated with a Gaussian noise N(0, 0.0012). We run 50K iterations of different algorithms.
For SRNAG, we restart after every 200 iterations. Fig. 2 (b) shows the iteration vs. optimal gap,
f(xk) - f(x*), with x* being the minimum. NAG with the inexact gradient due to constant variance
noise does not converge. GD performs almost the same as ARNAG asymptotically, because ARNAG
restarts too often and almost degenerates into GD. GD with constant momentum outperforms the
three schemes above, and SRNAG slightly outperforms GD with constant momentum.
Decaying Variance Gaussian Noise: Again, consider minimizing (8) with the same experimen-
tal setting as before except that Vf (x) is now contaminated with a decaying Gaussian noise
N(0,( bt∕⅜0c+ )2). For SRNAG, We restart every 200 iterations in the first 10k iterations, and
restart every 400 iterations in the remaining 40K iterations. Fig. 2 (c) shows the iteration vs. optimal
gap by different schemes. ARNAG still performs almost the same as GD. The path of NAG is
oscillatory. GD With constant momentum again outperforms the previous three schemes. Here
SRNAG significantly outperforms all the other schemes.
Logisitic Regression for MNIST Classification: We apply the above schemes With stochastic
gradient to train a logistic regression model for MNIST classification LeCun & Cortes (2010). We
consider five different schemes, namely, SGD, SGD + (constant) momentum, NASGD, ASGD, and
SRSGD. In ARSGD, We perform restart based on the loss value of the mini-batch training data. In
SRSGD, We restart the NAG momentum after every 10 iterations. We train the logistic regression
model With a `2 Weight decay of 10-4 by running 20 epochs using different schemes With batch
size of 128. The step sizes for all the schemes are set to 0.01. Fig. 3 (a) plots the training loss vs.
iteration. In this case, NASGD does not converge, and SGD With momentum does not speed up SGD.
ARSGD’s performance is on par With SGD’s. Again, SRSGD gives the best performance With the
smallest training loss among these five schemes.
3We adopt the result from Hardt (2014).
19
Under review as a conference paper at ICLR 2021
C Convergence of SRSGD
We prove the convergence of Nesterov accelerated SGD with scheduled restart, i.e., the convergence
ofSRSGD. We denote that θk := tk+1 in the NesterOV iteration and θk is its use in the restart version，
i.e., SRSGD. For any restart frequency F (positive integer), We have θk = θk-bk√Fc*F. In the restart
version, we can see that
θk ≤ θF =: θ < 1.
Lemma 6. Let the constant satisfies r > L and the sequence {vk}k≥0 be generated by the SRSGD
with restart frequency F (any positive integer), we have
k	r2kR2
X kvi- Vi k2 ≤ (T-J)2，	(44)
i=1
where θ := θF < 1 and R := sυpχ{kVf (x)k2}.
Proof. It holds that
kvk+1 - wkk2 = kvk+1-vk+vk -wkk2	(45)
≥ kvk+1 - vkk2 - kvk - wkk2
≥ kvk+1 - Vkk2 - 0∣vk - VkTIl2.
Thus,
kvk+1 - wkk2 ≥ (kvk+1- Vk∣2 - θkvk - VkTk2)2	(46)
=∣∣Vk + 1 - Vk k2 - 2θ∣Vk - VkTk2∣∣Vk - VkTk2 + θ2∣Vk - VkTk2
≥ (1 - θ)kVk+1 - Vkk2 - θ(1 - θ)kVk+1 - Vkk2.
Summing (46) from k = 1 to K, We get
KK
(1 - θ) X kVk - Vk-1k2 ≤ X kVk+1 - wkk2 ≤ r2KR2.	(47)
k=1	k=1
□
In the folloWing, We denote
A := {k ∈ Z+|Ef(Vk) ≥ Ef(Vk-1)}.
Theorem 5 (Convergence of SRSGD). (Theorem 2 with detailed bound) Suppose f(w) is L-smooth.
Consider the sequence {wk}k≥0 generated by (10) with stochastic gradient that is bounded and has
bound variance. Using any restart frequency F and any constant step size sk := s ≤ 1/L. Assume
that Pk∈A (Ef(Wk+1) — Ef(Wk)) = R < +∞ ,then we have
1≤mki≤nK EkVf(Wk)k22 ≤
rR2 L(1 + θ) , rLR2 , θR
(1 - θ) -2 — + 2 + rκ'
(48)
If f(w) is further convex and the Set B := {k ∈ Z+∣Ekwk+1 — w*∣2 ≥ EkWk — w*k2} obeys
Pk∈B (Ef(Wk+1) - Ef(Wk)) = R < +∞, then
min
1≤k≤K
{E (f(Wk) - f(W*))} ≤
∣∣w0 - W*k2 + R YR
--------------------+ --
2γk--------2
(49)
where w* is the minimum of f. To obtain e (∀e > 0) error, we set S = O(e) and K = O(1∕e2).
Proof. Firstly, We shoW the convergence of SRSGD for nonconvex optimization. L-smoothness of f,
i.e., Lipschitz gradient continuity, gives us
f (Vk + 1) ≤ f (Wk) + Ef(Wk )，Vk + 1 - Wk i + L kVk+1 - Wk k2.
(50)
20
Under review as a conference paper at ICLR 2021
Taking expectation, we get
r2 LR2
Ef (vk+1) ≤ Ef(Wk) - rE∣Vf (Wk)∣∣2 + —^∙
On the other hand, we have
f (wk ) ≤ f (vk ) + θk Ef(Vk ), Vk - Vk-Ii + LlkZ ∣∣vk - VkTk2∙
Then, we have
Ef (vk+1) ≤ Ef (vk) + θkE(Vf (vk), vk - VkT)
+ 空TEkVk-VkTk2 - rEkVf(wk)k2 + r^.
We also have
θk Ef(Vk), vk -VkTi ≤ θk (f(vk ) - f(vk-1) + L kVk -VkTk2).
We then get that
Ef (vk+1) ≤ Ef (vk) + θk (Ef(Vk) - Ef(VkT))-『EkVf (Wk)k2 + Ak,
where
Ak ：= E刍IVk - vk-1k2 + 丝产EkVk - vk-1k2 + 身R.
Summing the inequality gives US
Ef(VK+1) ≤ Ef (v0) + θ X(Ef(Vk) - Ef (vk-1))
k∈A
K	K
-Tf EkVf (wk )k2 + EAk.
k=1	k=1
(51)
(52)
(53)
(54)
(55)
(56)
It is easy to see that
θ X(Ef(Vk)- Ef (vk-1)) = θR.
k∈A
We get the result by using Lemma 6
Secondly, we prove the convergence of SRSGD for convex optimization. Let w* be the minimizer of
f. We have
Ekvk+1 - w*k2 = EkWk - YVf (wk) - w*k2	(57)
=EkWk- w* k2 - 2γE"f(wk), wk - w*i + 72EkVf (wk)k2
≤ EkWk - x*k2 - 2γE(Vf(wk), wk - w*i + γ2R2.
We can also derive
EkWk - w*k2 = EkVk + θk(vk - VkT) - w*k2
=EkVk - w*k2 + 2θkE(vk - vk-1, vk - w*i + (θkyEkVk - VkTk2
=EkVk - w*k2 + θkE (kvk - w*k2 + kvk-1 - vkk2 -kvk-1 - w*k2)
+ (θ)2Ekvk - vk-1k2
=EkVk - w*k2 + θkE (∣∣vk - w*k2 - kvk-1 - w*k2) +2(θk)2EkVk - VkTk2,
where we used the following identity
(a - b)T(a - b) = 2[∣∣α - dk2 -∣∣α - ck2 + kb - c∣∣2 - kb - d∣∣2]∙
21
Under review as a conference paper at ICLR 2021
Then, we have
Ekvk+1 - w*∣∣2 ≤ Ekvk - w*∣∣2 - 2γE<Vf(wk), Wk - w*∖ + 2(θk)2Ekvk - VkTk2 (58)
+ r2R2 + θkE(kvk - w*∣∣2 - kvk-1 - w*∣∣2).
We then get that
2γE (f(wk) - f(w*)) ≤ Ekvk - w*k2- Ekvk+1 - w*k2	(59)
+ θk (Ekvk - w*∣∣2 - EkvkT - w*∣∣2) + r2R2.
Summing the inequality gives Us the desired convergence result for convex optimization. □
C.1 Numerical Verification of the assumptions in Theorem 2
In this part, we numerically verify the assumptions in Theorem 2. In particular, we apply SRSGD with
learning rate 0.1 to train LeNet 4 for MNIST classification (we test on MNIST due to extremely large
computational cost). We conduct numerical verification as follows: starting from a given point w0, we
randomly sample 469 mini-batches (note in total we have 469 batches in the training data) with batch
size 128 and compute the stochastic gradient using each mini-batch. Next, we advance to the next step
with each of these 469 stochastic gradients and get the approximated Ef (w1). We randomly choose
one of these 469 positions as the updated weights of our model. By iterating the above procedure, we
can get w1, w2,… andEf (w1),Ef (w2),…and We use these values to verify our assumptions in
Theorem 2. We set restart frequencies to be 20, 40, and 80, respectively. Figure 6 top panels plot k vs.
the cardinality of the set A := {k ∈ Z+|Ef (wk+1) ≥ Ef (wk)}, and Figure 6 bottom panels plot k
vs. Pk∈A (Ef(Wk+1) — Ef (wk)). Figure 6 shows that Pk∈A (Ef(Wk+1) — Ef (wk)) converges
to a constant R < +∞. We also noticed that when the training gets plateaued, E(f (wk)) still
oscillates, but the magnitude of the oscillation diminishes as iterations goes, which is consistent
with our plots that the cardinality of A increases linearly, but R converges to a finite number. These
numerical results show that our assumption in Theorem 2 is reasonable.
Cardinality of set A := {k ∈ Z+∣Ef (wk+1) ≥ Ef(Wk)}
R=E (Ef(Wk+1)- Ef(Wk))
k∈A
Restart Frequency = 40
Figure 6: Cardinality of the set A := {k ∈ Z+∣Ef (wk+1) ≥ Ef (wk)} (Top panels) and the value
of R = Pk∈A (Ef (wk+1) — Ef (wk)) (Bottom panels). We notice that when the training gets
plateaued, E(f (wk)) still oscillates, but the magnitude of the oscillation diminishes as iterations
goes, which is consistent with our plots that the cardinality of A increases linearly, but R converges
to a finite number under different restart frequencies. These results confirm that our assumption in
Theorem 2 is reasonable.
4We used the PyTorch implementation of LeNet at https://github.com/activatedgeek/LeNet-5.
22
Under review as a conference paper at ICLR 2021
D Datasets and Implementation Details
D.1 CIFAR
The CIFAR10 and CIFAR100 datasets Krizhevsky et al. (2009) consist of 50K training images and
10K test images from 10 and 100 classes, respectively. Both training and test data are color images
of size 32 × 32. We run our CIFAR experiments on Pre-ResNet-110, 290, 470, 650, and 1001 with
5 different seeds He et al. (2016b). We train each model for 200 epochs with batch size of 128 and
initial learning rate of 0.1, which is decayed by a factor of 10 at the 80th, 120th, and 160th epoch. The
weight decay rate is 5 × 10-5 and the momentum for the SGD baseline is 0.9. Random cropping and
random horizontal flipping are applied to training data. Our code is modified based on the Pytorch
classification project Yang (2017),5 which was also used by Liu et al. Liu et al. (2020). We provide
the restarting frequencies for the exponential and linear scheme for CIFAR10 and CIFAR100 in
Table 7 below. Using the same notation as in the main text, we denote Fi as the restarting frequency
at the i-th learning rate.
Table 7: Restarting frequencies for CIFAR10 and CIFAR100 experiments
	CIFAR10	CIFAR100
Linear schedule Exponential schedule	Fl = 30, F2 = 60, F3 = 90, F4 = 120 (r = 2) Fl = 40, F2 = 50, F3 = 63, F4 = 78 (r = 1.25)	Fl = 50, F2 = 100, F3 = 150, F4 = 200 (r = 2) Fl = 45 , F2 = 68, F3 = 101, F4 = 152 (r = 1.50)
D.2 ImageNet
The ImageNet dataset contains roughly 1.28 million training color images and 50K validation color
images from 1000 classes Russakovsky et al. (2015). We run our ImageNet experiments on ResNet-
50, 101, 152, and 200 with 5 different seeds. Following He et al. (2016a;b), we train each model for
90 epochs with a batch size of 256 and decrease the learning rate by a factor of 10 at the 30th and 60th
epoch. The initial learning rate is 0.1, the momentum is 0.9, and the weight decay rate is 1 × 10-5.
Random 224 × 224 cropping and random horizontal flipping are applied to training data. We use the
official Pytorch ResNet implementation Paszke et al. (2019),6 and run our experiments on 8 Nvidia
V100 GPUs. We report single-crop top-1 and top-5 errors of our models. In our experiments, we set
F1 = 40 at the 1st learning rate, F2 = 80 at the 2nd learning rate, and F3 is linearly decayed from 80
to 1 at the 3rd learning rate (see Table 8).
Table 8: Restarting frequencies for ImageNet experiments
	ImageNet
Linear schedule	F1 = 40, F2 = 80, F3: linearly decayed from 80 to 1 in the last 30 epochs
D.3 Training ImageNet in Fewer Number of Epochs:
Table 9 contains the learning rate and restarting frequency schedule for our experiments on training
ImageNet in fewer number of epochs, i.e. the reported results in Table 6 in the main text. Other
settings are the same as in the full-training ImageNet experiments described in Section D.2 above.
Additional Implementation Details: Implementation details for the ablation study of error rate vs.
reduction in epochs and the ablation study of impact of restarting frequency are provided in Section F
and G below.
D.4 Details on Restarting Hyper-parameters Search
In our CIFAR10 and CIFAR100 experiments, for both linear and exponential schedule, we conduct
hyperparameter searches over the restarting frequencies using our smallest model, Pre-ResNet-110,
5Implementation available at https://github.com/bearpaw/pytorch-classification
6Implementation available at https://github.com/pytorch/examples/tree/master/imagenet
23
Under review as a conference paper at ICLR 2021
Table 9: Learning rate and restarting frequency schedule for ImageNet short training, i.e. Table 6 in
the main text.
	ImageNet
ResNet-50	Decrease the learning rate by a factor of 10 at the 30th and 56th epoch. Train for a total of 80 epochs. F1 = 60, F2 = 105, F3: linearly decayed from 105 to 1 in the last 24 epochs
ResNet-101	Decrease the learning rate by a factor of 10 at the 30th and 56th epoch. Train for a total of 80 epochs. F1 = 40, F2 = 80, F3: linearly decayed from 80 to 1 in the last 24 epochs
ResNet-152	Decrease the learning rate by a factor of 10 at the 30th and 51th epoch. Train for a total of 75 epochs. F1 = 40, F2 = 80, F3: linearly decayed from 80 to 1 in the last 24 epochs
ResNet-200	Decrease the learning rate by a factor of 10 at the 30th and 46th epoch. Train for a total of 60 epochs. F1 = 40, F2 = 80, F3: linearly decayed from 80 to 1 in the last 14 epochs
making choices based on final validation performance. The same chosen restarting frequencies are
applied for all models including Pre-ResNet-110, 290, 470, 650, and 1001. In particular, we use
10,000 images from the original training set as a validation set. This validation set contains 1,000 and
100 images from each class for CIFAR10 and CIFAR100, respectively. We first train Pre-ResNet-110
on the remaining 40,000 training images and use the performance on the validation set averaged over
5 random seeds to select the initial restarting frequency F1 and the growth rate r . Both F1 and r
are selected using grid search from the sets of {20, 25, 30, 35, 40, 45, 50} and {1, 1.25, 1.5, 1.75, 2},
respectively. We then train all models including Pre-ResNet-110, 290, 470, 650, and 1001 on all
50,000 training images using the selected values of F1 and r and report the results on the test set
which contains 10,000 test images. The reported test performance is averaged over 5 random seeds.
We also use the same selected values of F1 and r for our short training experiments in Section 4.3.
For ImageNet experiments, we use linear scheduling and sweep over the initial restarting frequency
F1 and the growth rate r in the set of {20, 30, 40, 50, 60} and {1, 1.25, 1.5, 1.75, 2}, respectively. We
select the values of F1 = 40 and r = 2 which have the highest final validation accuracy averaged
over 5 random seeds. Same as in CIFAR10 and CIFAR100 experiments, we select F1 and r using
our smallest model, ResNet-50, and apply the same selected hyperparameter values for all models
including ResNet-50, 101, 152, and 200. We also use the same selected values of F1 and r for our
short training experiments in Section 4.3. However, for ResNet-50, we observe that F1 = 60 and
r = 1.75 yields better performance in short training. All reported results are averaged over 5 random
seeds.
E SRSGD vs. SGD and SGD + NM on ImageNet Classification and
Other Tasks
E.1 Comparing with SGD with Nesterov Momentum on ImageNet Classification
In this section, we compare SRSGD with SGD with Nesterov constant momentum (SGD + NM) in
training ResNets for ImageNet classification. All hyper-parameters of SGD with constant Nesterov
momentum used in our experiments are the same as those of SGD described in section D.2. We list
the results in Table 10. Again, SRSGD remarkably outperforms SGD + NM in training ResNets for
ImageNet classification, and as the network goes deeper the improvement becomes more significant.
E.2 Long Short-Term Memory (LSTM) Training for Pixel-by-Pixel MNIST
In this task, we examine the advantage of SRSGD over SGD and SGD with Nesterov Momentum in
training recurrent neural networks. In our experiments, we use an LSTM with different numbers of
hidden units (128, 256, and 512) to classify samples from the well-known MNIST dataset LeCun &
Cortes (2010). We follow the implementation ofLe et al. (2015) and feed each pixel of the image into
the RNN sequentially. In addition, we choose a random permutation of 28 × 28 = 784 elements at
the beginning of the experiment. This fixed permutation is applied to training and testing sequences.
This task is known as permuted MNIST classification, which has become standard to measure the
performance of RNNs and their ability to capture long term dependencies.
24

NNXdXo∕oιreozoτ∕uιoo∙qnq]jg∕/: sd□q 弗 θ∣qυ∣reΛB UOilelUəuɪə[dui]乙
TUBfejjnQ QjBmd IUə[PRI3 卬!" NVOM ŋ 口时】əM 'siuəuɪpədXə mo Ul `sɪəpouɪ əatibjəuəs dəəp
SmniBJi hi uɪmnəuɪouɪ A(HəisəN PUS PjBPUeIS φiM QDS jəʌo QOSHS JO ə既SUSAPe ə此用isəau! əM
ISINW NO DNlNlVXJL (NVOM) SXXoAVLHN TVlXVSXHAQV HAlJLVXHNHD NIHjLSXHSSVM £*日
S5[SBj Uotjboijtssbio ILSINFid JOJ (Onlq) CIDSXS PUe
i(uəəj？) WN + QDS '(pə-ɪ) QOS q】!丛 pəurefl WlSqJo suoμBJQiτ guτureu ,sλ ssoɪ gmurejɪ Z ə.ɪn^n
6fi干冲8
εδ'o 干 εo,6
0S'0干19 8
60-T 干 99-6
T9-0 干 60-0T
69-0 干 9Z-6
9S-O 干 WOT YG NLSq
90'T∕99'T
9O'T/6S'T
。「1/6。T
心 + Q9S∕α9S -ɪəʌɔ iuəməʌojdiuɪ QOSHS ■ + Q9S
€9,0 干 ζT7-OT
Z才O干OTOT
Q9S
XI NLSq
阳&Sq
SRun ɪɪəppɪn ‘°N W∙ioκpN
*06 pɪɪŋ
’0£ ’06。1ləs ST QDSHS ɪɪɪ ə[npəgɔs XreISəi əm 60。1ləs st WN + QDS PUB QDS -ɪɑj uɪmuəuɪouɪ
əm ∙sqoodb Og£ joj pəureji əib sɪəpouɪ ɪɪv '00£ PIre OOZ ɪlɔɑdə 1 口 OT Jo ∙iopbj B Aq pəɔnpəj sɪ
qoιqM ζχo-Q jo əiRl Siiiujboj IBpim oqι əsn ə丛 ζSiiiouiijodxo ɪɪp Ul ,siɪnn nəppɪ^	Seq ɪəpouɪ WjLSZ
叫I∕QDSXS PUe WM + QDS 'QDS 目具丛 pəurejjjɑ ISINWp。InULrad Uo (%) SJoXra jsəɪ :ɪɪ o【q肛
♦ L Oin既!目 PUB ɪ ɪ əɪgnɪ m sjɪnsəj
jno əz[IBUlulnS ə/ʌ `siɪun uəpp^ ¼LLS1 Jo nq∏ιnu iuəiəjjɪp φτM sguɪuəs ɪɪn ssojəb UlmUəuioui
IunSUoɔ AoJəisəN PUB PiBPU⑷S 甲!丛 QDS U口甲 jə:sbj səgjəʌuoɔ puυ jojjə isəi 珂口侬 qonuι sp[ə]K
QOSHS 'səuɪɪəsŋg OMI əm jəao CK)SXS Jo ^inonədns əlBJoqouoɔ siuəuɪnə^ə jnɔ :sʧnsə^[
Iqnq月£)s,NNX lB∏uouodxo oψ UK)耳 əpoɔ oψ uo pəSeq st əpoɔ j∏o -ɪ ISoul
IB əib ULiOU ZIIP甲 正甲 os pədʤɪɔ əib SIUə1P口直 ə甲 puυ əzts qoWq əsn əM 'siuəuɪpədXə ɪɪn Ul
'00£ Plæ OOZ ɪlɔɑdə IB SQguBqo əɪnpəqɔs wisəj əm , 06，0£，06 ɑl WS st QDSHS .ɪɑj əɪnpəqɔs wisəj
ə^ɪ ,uιmnouιouι wisnoɔ aojəisəN PUS PjBPiw qιiM QDS 工。J 6'0。1 邸 si UlrUUəuɪouɪ ə^ɪ ,QQς pιiB
00% ɪlɔodə IB OT Jo -IOPBj B Aq pəɔnpəj sbm əibj ^u[ujbə[ əm `ɪo`o JO əibj suɪuɪeəɪ ib∏xut φτM
Sqɔodə Og£ joj ɪəpouɪ gɔeə utbji ə/ʌ 皿oιμwuι ⑪1uəp! əg oι pəz[呼叫 əib qoτqM 'səɔ!砒Ul 叫Mə丛
UQppTq-Okuoppn 9qi ɪoj ]dəɔXə A[ɪBUo既OqxK) poz∏Bμτm əib səɔ!同口Ul SIq既τp丛 ɪɪv '0səs切q Jraqio
pun ɪ 01SBiq iə^ɪoj Qψ Qz∏Bμτuτ əm 'ɪəpouɪ WlSq Qψ joj :sɪɪŋ；əɑ 8uιuιejχ pus uoɪ^uəuɪəɪduɪɪ
ZVQ	SO'I	90,0 干 ΛS,S	ετ oψε6,oδ	0δ-0 干 66-9	δδ-0 干 86'Tδ	WZ9,^9	O(ErəNS利
ZT-0	lɛ'θ	80 0 千 699	WO 干 WIC	60-0 干 98'S	H*0 干 ZZ*IS	W6I'09	CSl-HNS 期
60,0	9ζ-0	Z0-0 干 509	0T0=F90δδ	SO-O 干 81-9	SO-O 干 ZZ'ZZ	WSSΨT7	IooJoNS 期
ZO-O	WO	6(Γ0 干 OTN	60 0 干 98 WU	ZOP 干 ZIN	ZOP 干 ZTT7Z	小99靠	OS-gNS利
g-dcn	I-do;	S-doι	I-do;	S-doι	I-do;		
iuətuə/vojdiuɪ			CtDS-S				IzvN+ CtDS			Siireied #	串 OMjəN
∙(SQSQψUQIBd
uτ) (09TOZ) 'Ib W əh sjouə UonBP!ɪBA dojɔ-əɪ？ms pəuodəj oφ əpnɪɔm osɪn əm 'sʧnsəj əmɪəsŋg joj
∖ OloL xuojj pəsnəjɔəp k[ibəuH sɪ Kɔuənbəij sutiibisəj əgi 'əiRI suɪuɪeəɪ isnɪ əm Ul `səibj ^u[uibə[
OMI isjy əm uɪ Qmnboij supɪeisəj Eiqs口əiɔu! φτM ClDSXS Jo siɪnsəi əm uodəj əʌv 'QOSHS
puυ WN + QOS q]!丛 pəureji siəNsəX JO iəNə鸵ʊɪɪ uo (%) sjouə uoμBpτ∣BΛ dojɔ əɪsmg :ɑɪ əIq肛
IZOZ Xrlɔl 】B ∙iodted əɔuəiəjuoɔ B sb mətaəj jgpu∩
Under review as a conference paper at ICLR 2021
et al. (2017) on MNIST. We evaluate our models using the discriminator’s loss, i.e. the Earth
Moving distance estimate, since in WGAN lower discriminator loss and better sample quality are
correlated Arjovsky et al. (2017).
Implementation and Training Details: The detailed implementations of our generator and discrim-
inator are given below. For the generator, We Setlatent_dim to 100 and d to 32. For the discriminator,
we set d to 32. We train each model for 350 epochs with the initial learning rate of 0.01. The learning
rate was reduced by a factor of 10 at epoch 200 and 300. The momentum is set to 0.9 for SGD with
standard and Nesterov constant momentum. The restart schedule for SRSGD is set to 60, 120, 180.
The restart schedule changes at epoch 200 and 300. In all experiments, we use batch size 64. Our
code is based on the code from the Pytorch WGAN-GP Github.8
import torch
import torch . nn as nn
class Generator (nn . Module):
def __init__ ( self , Iatent_dim , d=32):
super (). __init__()
self . net = nn . Sequential (
nn. ConvTranspose2d( latent_dim , d * 8, 4, 1, 0),
nn.BatchNorm2d(d * 8),
nn.ReLU(True),
nn. ConvTranspose2d(d * 8, d * 4, 4, 2, 1),
nn . BatchNorm2d(d * 4),
nn.ReLU(True),
nn. ConvTranspose2d(d * 4, d * 2, 4, 2, 1),
nn . BatchNorm2d (d * 2),
nn.ReLU(True),
nn. ConvTranspose2d(d * 2, 1 , 4, 2, 1),
nn . Tanh ()
)
def forward ( self , x):
return self . net(x)
class Discriminator (nn. Module):
def __init__( self , d = 32):
super (). __init__()
self . net = nn . Sequential (
nn.Conv2d(1, d, 4, 2, 1),
nn . InstanceNorm2d (d),
nn . LeakyReLU (0.2),
nn.Conv2d(d, d * 2, 4, 2, 1),
nn. InstanceNorm2d(d * 2),
nn . LeakyReLU (0.2),
nn.Conv2d(d * 2, d * 4, 4, 2, 1),
nn. InstanceNorm2d(d * 4),
nn . LeakyReLU (0.2),
nn.Conv2d(d * 4, 1, 4, 1, 0),
)
def forward ( self , x):
outputs = self . net(x)
return outputs . squeeze ()
Results: Our SRSGD is still better than both the baselines. SRSGD achieves smaller discriminator
loss, i.e. Earth Moving distance estimate, and converges faster than SGD with standard and Nesterov
constant momentum. We summarize our results in Table 12 and Figure 8. We also demonstrate the
8Implementation available at https://github.com/arturml/pytorch-wgan-gp
26
Under review as a conference paper at ICLR 2021
digits generated by the trained WGAN in Figure 9. By visually evaluation, we observe that sampls
generatdby the WGAN trained with SRSGD look slightlybetterthnthose genratedby the WGAN
train with SGD with stndard and Nesterov cnstant momntum.
Table 12: Discriminator loss (i.e. Earth Moving distance estimate) of the WGAN with gradient
PenaIty trained on MNIST with SGD SGD + NM and SRSGD. In all experiments, We use the initial
learning rate of 0.01, which is reduced by a factor of 10 at epoch 200 and 300. All models are trained
for 350 eP°chs∙ The momentum for sgd and sgd + nm is set to 0∙9∙ The restart schedule in
SRSGD is Setto 60 120,and180.____________________________________________________
TaSk	SGD	SGD + NM	SRSGD	Improv ement over SGD/SGD + NM
MNIST	0.71 ± 0.10	。58 ±。03	0.44 ± 0.06	0.27/0.14
IO1
------ SGD + Momentum ------ SGD + NesterovMomentUm ------ SRSGD
IO0
o 50	100 150 200 250 300 350
EPoch
Figure 8: Earth Moving distance estimate (i.e. discriminator loss) vs. training epochs of WGAN with
gradient penalty trained with SGD (red), SGD + NM (green), and SRSGD (blue) on MNIST.
G4(7∕q67 7
76，* a q 76
/562.8 035
「。夕
6 2 7 13 97
。9 79G。Gl
2γ6 4 Ur6 1 1
φlelu - IS 山 əjuel--ɑ M⊂->o-≥ qtB 山
SGD	SGD + NM	SRSGD
Figure 9:	MNIST digits generated by WGAN trained with gradient penalty by SGD (left), SGD +
NM (middle), and SRSGD (right).
F Error Rate vs. Reduction in Training Epochs
F.1 Implementation Details
CIFAR10 (Figure 4, left, in the main text) and CIFAR100 (Figure 11 in this Appendix): Except
for learning rate schedule, we use the same setting described in Section D.1 above and Section 4.1 in
the main text. Table 13 contains the learning rate schedule for each number of epoch reduction in
Figure 4 (left) in the main text and Figure 11 below.
ImageNet (Figure 4, right, in the main text): Except for the total number of training epochs, other
settings are similar to experiments for training ImageNet in fewer number of epochs described in
Section D.3. In particular, the learning rate and restarting frequency schedule still follow those in
Table 9 above. We examine different numbers of training epochs: 90 (0 epoch reduction), 80 (10
epochs reduction), 75 (15 epochs reduction), 70 (20 epochs reduction), 65 (25 epochs reduction), and
60 (30 epochs reduction).
27
Under review as a conference paper at ICLR 2021
Table 13: Learning rate (LR) schedule for the ablation study of error rate vs. reduction in training
epochs for CIFAR10 experiments, i.e. Figure 4 in the main text and for CIFAR100 experiments, i.e.
Figure 11 in this Appendix.
#of Epoch Reduction	LR Schedule
0	Decrease the LR by a factor of10 at the 80th, 120th and 160th epoch. Train for a total of 200 epochs.
15	Decrease the LR by a factor of 10 at the 80th, 115th and 150th epoch. Train for a total of 185 epochs.
30	Decrease the LR by a factor of 10 at the 80th, 110th and 140th epoch. Train for a total of 170 epochs.
45	Decrease the LR by a factor of10 at the 80th, 105th and 130th epoch. Train for a total of 155 epochs.
60	Decrease the LR by a factor of 10 at the 80th, 100th and 120th epoch. Train for a total of 140 epochs.
75	Decrease the LR by a factor of 10 at the 80th, 95th and 110th epoch. Train for a total of 125 epochs.
90	Decrease the LR by a factor of 10 at the 80th, 90th and 100th epoch. Train for a total of 110 epochs.
100	Decrease the LR by a factor of 10 at the 80th, 90th and 95th epoch. Train for a total of 100 epochs.
Table 14: Classification test error (%) of SGD short training (100 epochs), SGD full training (200
epochs), SRSGD short training (100 epochs), and SRSGD full training (200 epochs) on CIFAR10.
SGD short training yields much worse test errors than the others while SRSGD short training yields
either comparable or even better results than SGD full training.
Network	SGD short training	SGD full training	SRSGD short training	SRSGD full training
Pre-ResNet-110	6.36 ± 0.12	5.25 ± 0.14	5.43 ± 0.18	4.93 ± 0.13
Pre-ResNet-290	5.81 ± 0.10	5.05 ± 0.23	4.83 ± 0.11	4.37 ± 0.15
Pre-ResNet-470	5.59 ± 0.19	4.92 ± 0.10	4.64 ± 0.17	4.18 ± 0.09
Table 15: Classification test error (%) of SGD short training (100 epochs), SGD full training (200
epochs), SRSGD short training (100 epochs), and SRSGD full training (200 epochs) on CIFAR100.
SGD short training yields worse test errors than the others while SRSGD short training yields either
comparable or even better results than SGD full training.
Network	SGD short training	SGD full training	SRSGD short training	SRSGD full training
Pre-ResNet-110	24.34 ± 0.21	23.75 ± 0.20	23.85 ± 0.19	23.49 ± 0.23
Pre-ResNet-290	22.70 ± 0.25	21.78 ± 0.21	21.77 ± 0.43	21.49 ± 0.27
Pre-ResNet-470	22.43 ± 0.18	21.43 ± 0.30	21.42 ± 0.19	20.71 ± 0.32
F.2 Short Training on CIFAR 1 0/CIFAR 1 00 Using SGD
For better comparison between SRSGD training using fewer epochs and SGD full training, we also
conduct experiments with SGD training using fewer epochs on CIFAR10 and CIFAR100. Table 14
and 15 compares SRSGD short training using 100 epoch, SGD short training using 100 epochs,
SRSGD full training using 200 epochs, and SGD full training using 200 epochs for Pre-ResNet-110,
290, and 470 on CIFAR10 and CIFAR100, respectively. The learning rate schedule for SGD short
training using 100 epochs is the same as the learning rate schedule for SRSGD short training using
100 epoch given in Section 4 and in Table 13 above. In particular, for both SGD and SRSGD training
using 100 epochs, we decrease the learning rate by a factor of 10 at the 80th, 90th, and 95th epoch.
We observe that SGD short training has the worst performance compared to the others while SRSGD
short training yields either comparable or even better results than SGD full training.
F.3 Additional Experimental Results
Figure 10 shows error rate vs. reduction in epochs for all models trained on CIFAR10 and ImageNet.
It is a more complete version of Figure 4 in the main text. Table 16 and Table 17 provide detailed test
errors vs. number of training epoch reduction reported in Figure 4 and Figure 10 . We also conduct an
additional ablation study of error rate vs. reduction in epochs for CIFAR100 and include the results
in Figure 11 and Table 18 below.
28
Under review as a conference paper at ICLR 2021
----Pre-ResNet-101-----Pre-ResNet-290
Pre-ResNet-470	Pre-ResNet-650
----Pre-ResNet-1001
----ResNet-50 -----ResNet-101
ResNetT52	ResNet-200
ImageNet
Number of Epoch Reduction
Figure 10:	Test error vs. number of training epochs. Dashed lines are test errors of SGD trained
in 200 epochs for CIFAR10 and 90 epochs for ImageNet. For CIFAR, SRSGD with fewer epochs
achieves comparable results to SRSGD with 200 epochs. For ImageNet, training with less epochs
slightly decreases the performance of SRSGD but still achieves comparable results to 200-epoch
SGD.
Table 16: Test error vs. number of training epochs for CIFAR10
Network	110(90 less)	125 (75 less)	140 (60 less)	155 (45 less)	170 (30 less)	185(15 less)	200 (full trainings)
Pre-ResNet-110	5.37 ± 0.11	5.27 ± 0.17	5.15 ± 0.09	5.09 ± 0.14	4.96 ± 0.14	4.96 ± 0.13	4.93 ± 0.13
Pre-ResNet-290	4.80 ± 0.14	4.71 ± 0.13	4.58 ± 0.11	4.45 ± 0.09	4.43 ± 0.09	4.44 ± 0.11	4.37 ± 0.15
Pre-ResNet-470	4.52 ± 0.16	4.43 ± 0.12	4.29 ± 0.11	4.33 ± 0.07	4.23 ± 0.12	4.18 ± 0.09	4.18 ± 0.09
Pre-ResNet-650	4.35 ± 0.10	4.24 ± 0.05	4.22 ± 0.15	4.10 ± 0.15	4.12 ± 0.14	4.02 ± 0.05	4.00 ± 0.07
Pre-ResNet-1001	4.23 ± 0.19	4.13 ± 0.12	4.08 ± 0.15	4.10 ± 0.09	3.93 ± 0.11	4.06 ± 0.14	3.87 ± 0.07
Table 17: Top 1 single crop validation error vs. number of training epochs for ImageNet
Network	60 (30 less)	65 (25 less)	70 (20 less)	75 (15 less)	80 (10 less)	90 (full trainings)
ResNet-50	25.42 ± 0.42	25.02 ± 0.15	24.77 ± 0.14	24.38 ± 0.01	24.30 ± 0.21	23.85 ± 0.09
ResNet-101	23.11 ± 0.10	22.79 ± 0.01	22.71 ± 0.21	22.56 ± 0.10	22.44 ± 0.03	22.06 ± 0.10
ResNet-152	22.28 ± 0.20	22.12 ± 0.04	21.97 ± 0.04	21.79 ± 0.07	21.70 ± 0.07	21.46 ± 0.07
ResNet-200	21.92 ± 0.17	21.69 ± 0.20	21.64 ± 0.03	21.45 ± 0.06	21.30 ± 0.03	20.93 ± 0.13
Table 18: Test error vs. number of training epochs for CIFAR100
Network	110 (90 less)	125 (75 less)	140 (60 less)	155 (45 less)	170 (30 less)	185 (15 less)	200 (full trainings)
Pre-ResNet-110	24.06 ± 0.26	23.82 ± 0.24	23.82 ± 0.28	23.58 ± 0.18	23.69 ± 0.21	23.73 ± 0.34	23.49 ± 0.23
Pre-ResNet-290	21.96 ± 0.45	21.77 ± 0.21	21.67 ± 0.37	21.56 ± 0.33	21.38 ± 0.44	21.47 ± 0.32	21.49 ± 0.27
Pre-ResNet-470	21.35 ± 0.17	21.25 ± 0.17	21.21 ± 0.18	21.09 ± 0.28	20.87 ± 0.28	20.81 ± 0.32	20.71 ± 0.32
Pre-ResNet-650	21.18 ± 0.27	20.95 ± 0.13	20.77 ± 0.31	20.61 ± 0.19	20.57 ± 0.13	20.47 ± 0.07	20.36 ± 0.25
Pre-ResNet-1001	20.27 ± 0.17	20.03 ± 0.13	20.05 ± 0.22	19.74 ± 0.18	19.71 ± 0.22	19.67 ± 0.22	19.75 ± 0.11
G Impact of Restarting Frequency for ImageNet and CIFAR 1 00
G. 1 Implementation Details
For the CIFAR10 experiments on Pre-ResNet-290 in Figure 5 in the main text, as well as the
CIFAR100 and ImageNet experiments in Figure 14 and 15 in this Appendix, we vary the initial
restarting frequency F1. Other settings are the same as described in Section D above.
29
Under review as a conference paper at ICLR 2021
Pre-ResNet-101	Pre-ResNet-290
Pre-ReSNet-470	Pre-ReSNet-650
山+js①一
Number of Epoch Reduction
Figure 11: Test error vs. number of epoch reduction in CIFAR100 training. The dashed lines are
test errors of the SGD baseline. For CIFAR100, SRSGD training with fewer epochs can achieve
comparable results to SRSGD training with full 200 epochs. In some cases, such as with Pre-ResNet-
290 and 1001, SRSGD training with fewer epochs achieves even better results than SRSGD training
with full 200 epochs.
r=0.7	r=1.0	r=2.0	r=10.0
SSO-,U01
IO0
10^1
10-2
0	100	200
Epoch
0.7	1	2	10
Growth Rate (r)
Figure 12: Training loss (left) and test error (right) of Pre-ReSNet-110 trained on CIFAR10 with
different growth rate r (linear schedule). Here, we fix the initial restarting frequency Fi = 30 for all
trainings. Increasing the restarting frequency during training yields better results than decreasing the
restarting frequency, but increasing the restarting frequency too fast and too much also diminishes the
performance of SRSGD.
G.2 IMPACT OF THE GROWTH RATE r
We do an ablation study for the growth rate r to understand its impact on the behavior of SRSGD. We
choose a case study of training a Pre-ResNet-110 on CIFAR10 using SRSGD with a linear schedule
scheme for the restarting frequency. We fix the initial restarting frequency Fi = 30 and vary the
growth rate r. We choose r from the set of {0.7,1.0,2.0,10.0}. These values of r represent four
different scenarios. When r = 0.7, the restarting frequency decreases every time the learning rate is
reduced by a factor of 10. When r = 1.0, the restarting frequency stays constant during the training.
When r = 2.0, the restarting frequency increases every time the learning rate is reduced by a factor
of 10. Finally, when r = 10.0, it is similar to when r = 2.0, but the restarting frequency increases
much faster and to larger values. Figure 12 summarizes the results of our ablation study. We observe
that for CIFAR10, decreasing the restarting frequency or keeping it constant during training yields
worse results than increasing the restarting frequency. However, increasing the restarting frequency
too much also diminishes the performance of SRSGD.
30
Under review as a conference paper at ICLR 2021
G.3 Additional Experimental Results
To complete our study on the impact of restarting frequency in Section 5.2 in the main text, we
examine the case of CIFAR100 and ImageNet in this section. We summarize our results in Figure 14
and 15 below. Also, Figure 13 is a more detailed version of Figure 5 in the main text.
sso〕U@1
0	50	100	150	200	250
Epoch
Initial Restarting Frequency (FJ
Figure 13: Training loss (left) and test error (right) of Pre-ResNet-290 trained on CIFAR10 with
different initial restarting frequencies Fi (linear schedule). SRSGD with small Fi approximates
sgd without momentum, while SRSGD with large Fi approximates NASGD.The training loss curve
and test accuracy of NASGD are shown in red and confirm the result of Theorem 1 that NASGD
accumulates error due to the stochastic gradients.
O0-I
110
SSol U@1
OOOoOooOQJD
15123456789 G
--■ - - - ■ - - - -is
LLlLlLlLLLLIA
7-6∙5'43'2∙L
2 2 2 2 2 2 2
JOJlsəl
											
	V	砌 SGt	roxi LwL	mat hou	e						
	3	mo	men	tum							
		∖						Ap	)rox	mat	
		∖							NAS	GD- ∖	U
									/		
					士	~~		1			
Test Error of NASGD
=66.44 ± 0.90
Epoch
90
8
W
Initial Restarting Frequency (FJ
Figure 14: Training loss and test error of Pre-ReSNet-290 trained on CIFAR100 with different
initial restarting frequencies Fi (linear schedule). SRSGD with small Fi approximates SGD without
momentum, while SrSGD with large Fi approximates NASGD. The training loss curve and test
accuracy of NASGD are shown in red and confirm the result of Theorem 1 that NASGD accumulates
error due to the stochastic gradients.
sso-ju-ejl
3× IO0
2 × IO0
IO0
6× 10°
4 ×100
IroXimate
APP
ASGD
momentum
Approximate
SGD without
---Fl=I
Fl=5
一 Fl=20
F 1=40
——Fl=60
Fl=80
——Fl=IOO
——Fl=200
---Fl=300
0	20	40	60	80
Epoch
24.0
23.0
22.0
25.0
1	5 20 40 60 80 100 200 300
Initial Restarting Frequency (FJ
Figure 15: Training loss and test error of ResNet-101 trained on ImageNet with different initial
restarting frequencies Fi. We use linear schedule and linearly decrease the restarting frequency to
1 at the last learning rate. SRSGD with small Fi approximates SGD without momentum, while
SRSGD with large Fi approximates NASGD.
31
Under review as a conference paper at ICLR 2021
H Full Training with Less Epochs at the Intermediate Learning
Rates
We explore SRSGD full training (200 epochs on CIFAR and 90 epochs on ImageNet) with less
number of epochs at the intermediate learning rates and report the results in Table 19, 20, 21 and
Figure 16,17,18 below. The settings and implementation details here are similar to those in Section F,
but using all 200 epochs for CIFAR experiments and 90 epochs for ImageNet experiments.
Pre-ResNet-101
Pre-ResNet-470
Pre-ResNet-290
Pre-ResNet-650
------Pre-ResNet-1001
5.5
5.0
4.5
4.0
0	5	10	15	20	25	30
Number of Epoch Reduction at the
2nd and 3rd learning rate
Figure 16: Test error when using new learning rate schedules with less training epochs at the 2nd and
3rd learning rate for CIFAR10. We still train in full 200 epochs in this experiment. On the x-axis, 10,
for example, means We reduce the number of training epochs by 10 at each intermediate learning rate,
i.e. the 2nd and 3rd learning rate. The dashed lines are test errors of the SGD baseline.
Table 19: Test error when using new learning rate schedules with less training epochs at the 2nd
and 3rd learning rate for CIFAR10. We still train in full 200 epochs in this experiment. In the table,
80-90-100, for example, means we reduce the learning rate by factor of 10 at the 80th, 90th, and
100 th epoch.
Network	80-90- 100	80 - 95 - 110	80 - 100 - 120	80 - 105 - 130	80-110-140	80-115-150	80- 120- 160
Pre-ResNet-110	5.32 ± 0.14	5.24 ± 0.17	5.11 ± 0.13	5.04 ± 0.15	4.92 ± 0.15	4.95 ± 0.12	4.93 ± 0.13
Pre-ResNet-290	4.73 ± 0.13	4.67 ± 0.12	4.53 ± 0.10	4.40 ± 0.11	4.42 ± 0.09	4.42 ± 0.10	4.37 ± 0.15
Pre-ReSNet-470	4.48 ± 0.16	4.34 ± 0.10	4.25 ± 0.12	4.28 ± 0.10	4.19 ± 0.10	4.14 ± 0.07	4.18 ± 0.09
Pre-ReSNet-650	4.25 ± 0.13	4.12 ± 0.06	4.13 ± 0.09	4.03 ± 0.11	4.04 ± 0.11	4.04 ± 0.04	4.00 ± 0.07
Pre-ResNet-1001	4.14 ± 0.18	4.06 ± 0.12	4.04 ± 0.15	4.08 ± 0.09	3.92 ± 0.13	4.05 ± 0.14	3.87 ± 0.07
Table 20: Test error when using new learning rate schedules with less training epochs at the 2nd and
3rd learning rate for CIFAR100. We still train in full 200 epochs in this experiment. In the table,
80-90-100, for example, means we reduce the learning rate by factor of 10 at the 80th, 90th, and
100 th epoch.
Network	80 - 90- 100	80 - 95 - 110	80- 100- 120	80- 105 - 130	80-110-140	80-115-150	80 - 120 - 160
Pre-ResNet-110	23.65 ± 0.14	23.96 ± 0.26	23.97 ± 0.31	23.53 ± 0.13	23.57 ± 0.36	23.68 ± 0.24	23.49 ± 0.23
Pre-ResNet-290	21.94 ± 0.44	21.71 ± 0.27	21.55 ± 0.40	21.44 ± 0.31	21.37 ± 0.45	21.47 ± 0.32	21.49 ± 0.27
Pre-ResNet-470	21.29 ± 0.11	21.21 ± 0.14	21.17 ± 0.18	20.99 ± 0.28	20.81 ± 0.22	20.80 ± 0.31	20.71 ± 0.32
Pre-ResNet-650	21.11 ± 0.24	20.91 ± 0.17	20.66 ± 0.33	20.52 ± 0.18	20.51 ± 0.16	20.43 ± 0.10	20.36 ± 0.25
Pre-ResNet-1001	20.21 ± 0.15	20.00 ± 0.11	19.86 ± 0.19	19.55 ± 0.19	19.69 ± 0.21	19.60 ± 0.17	19.75 ± 0.11
32
Under review as a conference paper at ICLR 2021
Pre-ResNet-101	Pre-ResNet-290
Pre-ResNet-470	Pre-ResNet-650
Pre-ResNet-1001
」0」-Isg
0	5	10	15	20	25	30
Number of Epoch Reduction at the
2nd and 3rd learning rate
Figure 17: Test error when using new learning rate schedules with less training epochs at the 2nd and
3rd learning rate for CIFAR100. We still train in full 200 epochs in this experiment. On the x-axis,
10, for example, means we reduce the number of training epochs by 10 at each intermediate learning
rate, i.e. the 2nd and 3rd learning rate. The dashed lines are test errors of the SGD baseline.
----ResNet-50 -----ResNet-101
----ResNet-152-----ResNet-200
ImageNet
UO4BP=B>
doiɔ Θ-S
0	5	10	15	20
Number of Epoch Reduction at
the 2nd learning rate
Figure 18: Test error when using new learning rate schedules with less training epochs at the 2nd
learning rate for ImageNet. We still train in full 90 epochs in this experiment. On the x-axis, 10, for
example, means We reduce the number of training epochs by 10 at the 2nd learning rate. The dashed
lines are test errors of the SGD baseline.
Table 21: Top 1 single crop validation error when using new learning rate schedules with less training
epochs at the 2nd learning rate for ImageNet. We still train in full 90 epochs in this experiment. In
the table, 30-40, for example, means we reduce the learning rate by factor of 10 at the 30th and 40th
epoch.
Network	30-40	30-45	30-50	30-55	30-60
ResNet-50	24.44 ± 0.16	24.06 ± 0.15	24.05 ± 0.09	23.89 ± 0.14	23.85 ± 0.09
ResNet-101	22.49 ± 0.09	22.51 ± 0.05	22.24 ± 0.01	22.20 ± 0.01	22.06 ± 0.10
ResNet-152	22.02 ± 0.01	21.84 ± 0.03	21.65 ± 0.14	21.55 ± 0.06	21.46 ± 0.07
ResNet-200	21.65 ± 0.02	21.27 ± 0.14	21.12 ± 0.02	21.07 ± 0.01	20.93 ± 0.13
I Visualization of SRSGD’ s trajectory
Here we visualize the training trajectory through bad minima of SRSGD, SGD with constant momen-
tum, and SGD. In particular, we train a neural net classifier on a swiss roll data as in Huang et al.
(2019) and find bad minima along its training. Each red dot in Figure 19 represents the trained model
after each 10 epochs in the training. From each red dot, we search for nearby bad local minima,
33
Under review as a conference paper at ICLR 2021
which are the blue dots. Those bad local minima achieve good training error but bad test error. We
plots the trained models and bad local minima using PCA Wold et al. (1987) and t-SNE Maaten &
Hinton (2008) embedding. The blue color bar is for the test accuracy of bad local minima; the red
color bar is for the number of training epochs.
PCA Embedding of the Trajectory
t-SNE Embedding of the Trajectory
Figure 19: Trajectory through bad minima of SGD, SGD with constant momentum, and SRSGD
during the training: we train a neural net classifier and plot the iterates of SGD after each ten epoch
(red dots). We also plot locations of nearby “bad” minima with poor generalization (blue dots). We
visualize these using PCA and t-SNE embedding. The blue color bar is for the test accuracy of bad
local minima while the red color bar is for the number of training epochs. All blue dots for SGD with
constant momentum and SRSGD achieve near perfect train accuracy, but with test accuracy below
59%. All blue dots for SGD achieves average train accuracy of 73.11% and with test accuracy also
below 59%. The final iterate (yellow star) of SGD, SGD with constant momentum, and SRSGD
achieve 73.13%, 99.25%, and 100.0% test accuracy, respectively.
(CONTINUED NEXT PAGE)
34
Under review as a conference paper at ICLR 2021
J SRSGD Implementation in Pytorch
import torch
from . optimizer import Optimizer , required
class SRSGD( Optimizer ):
”””
Scheduled Restart SGD.
Args :
params (iterable): iterab le of parameters to optimize
or dicts defining parameter groups .
lr (float) : learning rate .
weight_decay (float , optional) : weight decay (L2 penalty)(
default : 0)
iter.count (integer ): count the iteration s mod 200
Example :
>>> optimizer = torch . optim .SRSGD(model. parameters () , lr =0.1 ,
Weight_decay=5e — 4, iter_COUnt=1)
>>> optimizer . zero_grad ()
>>> loss _fn (model (input) , target) . backward ()
>>> optimizer . step ()
>>> iter .count = optimizer . update_iter ()
Formula :
v,{t + 1} = p_t _ lr *g,t
p_ { t + 1 } = v_ { t + 1 } + (iter_COUnt)/( iter_count+3) * (V_ { t + 1} — v_t)
”””
def __init__ ( self , params , lr = required , weight_decay =0.,
iter .count = 1, restarting_iter=100):
if lr is not required and lr < 0.0:
raise VaIUeError ("Invalid- learning- rate : 1{} " . format ( lr ))
if weight_decay < 0.0:
raise VaIUeError ("Invalid _weight_decay- value : 1{} " . format (
weight_decay ))
if iter .count < 1:
raise VaIUeError ("Invalid - iter - count: 一 {} " . format (iter-count))
if restarting_iter < 1:
raise VaIUeError("Invalid - iter - total : 一 {} " . format (
restarting_iter ))
defaults = dict (lr = lr , Weight_decay = weight_decay , iter .count =
iter_count ,
restarting_iter = restarting_iter)
super (SRSGD, self ). __init__(params , defaults )
def __setstate__( self , state ):
super (SRSGD, self ). __setstate__( state )
def update_iter ( self ):
idx = 1
for group in self . param .groups :
if idx == 1:
group [ , iter_count ‘ ] += 1
if group [ , iter_count '] >= group [ ’ restarting_iter ' ]:
group[ , iter_count , ] = 1
idx += 1
return group [ ’ iter_count '] , group[’ restarting _iter ']
def step ( self , closure=None):
”””
Perform a single optimization step .
Arguments: closure (callable , optional) : A closure that
reevaluates the model and returns the loss .
loss = None
if closure is not None:
35
Under review as a conference paper at ICLR 2021
loss = closure ()
for group in self . param .groups :
weight_decay = group [ ' Weight_decay ']
momentum = (group[ ' iter_count'] — 1.)/( group[ ' iter_count'] +
2.)
for P in group [ , params ' ]:
if p . grad is None :
continue
d_p = P . grad . data
if weight_decay !=0:
d_p . add_(Weight _decay , p . data )
param_state = self . state [p]
if ,momentum-buffer , not in Param_state :
buf0 = param .state [ , momentum_buffer ' ] = torch . clone (p
.data ). detach ()
else :
buf0 = param_state [ , momentum_buffer ']
buf1 = p. data — group[,lr ']*d_p
p . data = buf1 + momentum * ( buf1 — buf0 )
param_state [ , momentum_buffer '] = buf1
iter_count , iter_total = self . update_iter ()
return loss
K SRSGD Implementation in Keras
import numpy as np
import tensorflow as tf
from keras import backend as K
from keras . optimizers import Optimizer
from keras . legacy import interfaces
if K. backend () == , tensorflow ’ :
import tensorflow as tf
class SRSGD( Optimizer ):
””” Scheduled Restart
Includes support for
Stochastic gradient descent optimizer .
Nesterov momentum
and learning rate
# Arguments
learning_rate:
decay .
float >= 0. Learning rate .
def __init__ ( self , learning _rate = 0.01 , iter_COUnt=1, restarting_iter
=40 , **kwargs ):
learning_rate = kwargs . pop ( ' lr ’ , learning_rate )
self . initial _decay = kwargs . pop ( ,decay , , 0.0)
super (SRSGD, self ) . __init __ (* * kwargs )
with K. name_scope ( self . __class__ . __name__ ):
self . iterations = K. variable (0 , dtype= ' int64 ' , name= '
iteratio n s ' )
self . learning_rate = K. variable (learning_rate , name= '
learning_rate ')
self . decay = K. variable ( self . initial_decay , name= ' decay ')
# for srsgd
self . iter_count = K. variable (iter_count , dtype= 'int64 ' , name=
, iter -count ' )
self .restarting_iter = K. variable (restarting_iter , dtype= '
int64 ' , name= ' restarting-iter ')
self .nesterov = nesterov
36
Under review as a conference paper at ICLR 2021
@ interfaces . legacy_get_UPdates_support
@K. symbolic
def get_UPdates ( self , loss , params):
grads = self .get_gradients(loss , params )
self . updates = [K. update_add( self . iterations , 1)]
momentum = (K. cast( self . iter_count , dtype=K. dtype( self . decay)) 一
1.) / (K. cast ( self . iter_count , dtype=K. dtype( self . decay)) + 2.)
lr = self . learning _rate
if self . initial_decay > 0:
lr = lr * (1. / (1. + self . decay * K.cast(self . iterations ,
K.dtype( self . decay )
)))
# momentum
shapes = [K. int_ShaPe(P) for P in params ]
moments = [K. variable ( value=K. get_value (p) , dtype=K. dtype ( self .
decay ) , name= ' moment. ’ + str (i))
for (i , p ) in enumerate ( params )]
self . weights = [ self . iterations ] + moments + [ self . iter_count ]
for p , g , m in zip (params , grads , moments ):
v = p — lr * g
new_p = v + momentum * (v — m)
self . updates . append (K. update (m, v))
# Apply constraints .
if getattr (p , , constraint , , None) is not None :
new_p = p. constraint (new_p)
self . updates . append (K. update (p , new,p ))
condition = K. all (K. less ( self . iter_count , self . restarting_iter ))
new_iter_count = K. switch ( condition , self . iter_count + 1 , self .
iter .count - self . restarting_iter + 1)
self . updates . append (K. update ( self . iter_count , new_iter_count))
return self . updates
def get_config ( self ):
config = {'learning_rate '： float(K.get_value( self .learning_rate))
,
' decay ’： float(K.get_value( self . decay )),
’ iter -count ' ： int (K. get_value ( self . iter_count)),
' restarting _iter '： int(K.get_value( self .restarting_iter
))}
base_COnfig = super (SRSGD, self ) . get_config ()
return dict ( list (base_COnfig . items () ) + list (config . items ()))
37