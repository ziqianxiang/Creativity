Under review as a conference paper at ICLR 2021
FSV: Learning to Factorize Soft Value Func-
tion for Cooperative Multi-Agent Reinforce-
ment Learning
Anonymous authors
Paper under double-blind review
Ab stract
We explore stochastic-based policy solutions for cooperative multi-agent rein-
forcement learning (MARL) using the idea of function factorization in centralized
training with decentralized execution (CTDE). Existing CTDE based factoriza-
tion methods are susceptible to the relative overgeneralization, where finding a
suboptimal Nash Equilibrium, which is a well-known game-theoretic pathology.
To resolve this issue, we propose a novel factorization method for cooperative
MARL, named FSV, which learns to factorize the joint soft value function into
individual ones for decentralized execution. Theoretical analysis shows that FSV
solves a rich class of factorization tasks. Our experiments for the well-known tasks
of the Non-Monotonic Matrix game and the Max of Two Quadratics game show
that FSV converges to optima in the joint action space in the discrete and contin-
uous tasks by local searching. We evaluate FSV on a challenging set of StarCraft
II micromanagement tasks, and show that FSV significantly outperforms existing
factorization multi-agent reinforcement learning methods.
1 introduction
Cooperative multi-agent reinforcement learning (MARL) aims to instill in agents policies that max-
imize the team reward accumulated over time (Panait & Luke (2005); Busoniu et al. (2008); Tuyls
& Weiss (2012)), which has great potential to address complex real-world problems, such as coor-
dinating autonomous cars (Cao et al. (2013)). Considering the measurement and communication
limitations in practical problems, cooperative MARL faces the partial observability challenge. That
is, each agent chooses actions just based on its local observations.
Centralized training with decentralized execution (CTDE) (Oliehoek et al. (2011)) is a common
paradigm to address the partial observability, where agents’ policies are trained with access to global
information in a centralized way and executed only based on local observations in a decentralized
way, such as the MADDPG (Lowe (2017)) and COMA (Foerster et al. (2017)). However, the size
of the joint state-action space of the centralized value function grows exponentially as the number
of agents increases, which is known as the scalibility challenge.
Value function factorization methods have been an increasingly popular paradigm for solving the
scalability in CTDE by satisfying the Individual-Global-Max (IGM) where the optimal joint action
selection should be consistent with the optimal individual action selections. Three representative
examples of value function factorization methods include VDN (Sunehag et al. (2017)), QMIX
(Rashid et al. (2018)), and QTRAN (Son et al. (2019)). All these methods are -greedy policies,
where VDN and QMIX give sufficient but unnecessary conditions for IGM by additivity and mono-
tonicity structures respectively, and the QTRAN formulates the IGM as an optimization problem
with linear constraints.
Although these methods have witnessed some success in some tasks, they all face relative overgen-
eralization, where agents may stick into a suboptimal Nash Equilibrium. In fact, relative overgener-
alization is a grave pathology arising which occurs when a suboptimal Nash Equilibrium in the joint
space of action priors to an optimal Nash Equilibrium since each agent’s action in the suboptimal
equilibrium is a better choice (Wei & Luke (2016)). The non-monotonic matrix game is a sim-
ple discrete example. Both VDN and QMIX fail to learn the optimal policy in the non-monotonic
1
Under review as a conference paper at ICLR 2021
matrix due to their structure limitation. Although QTRAN expresses the complete value function
representation ability in the non-monotonic matrix, its full expressive ability decreases in the com-
plex tasks due to the computationally intractable constraints relaxing with tractable L2 penalties.
Besides, QTRAN sacrifices the tractability in continuous action space. Therefore, in discrete and
continuous tasks, achieving effective scalability while avoiding relative overgeneralization remains
an open problem for cooperative MARL.
To address this challenge, this paper presents a new definition of factorizable tasks called IGO
(Individual-Global-Optimal) which introduces the consistency of joint optimal stochastic policies
and individual optimal stochastic policies. Theoretical analysis shows that IGO degenerates into
IGM if the policy is greedy, which represents the generality of IGO. Under the IGO, this paper
proposes a novel factorization solution for MARL, named FSV, which learns to factorize soft value
function into individual ones for decentralized execution enabling efficient learning and exploration
through maximum entropy reinforcement learning. To our best knowledge, FSV is the first multi-
agent algorithm with stochastic policies using the idea of factorization, and theoretical analysis
shows that FSV solves a rich class of tasks.
We evaluate the performance of FSV in both discrete and continuous problems proposed by Son
et al. (2019); Wei et al. (2018) and a range of unit micromanagement benchmark tasks in StarCraft
II. The Non-Monotonic Matrix game shows that FSV has full expression ability in the discrete task,
and the Max of Two Quadratics game shows that FSV is the first factorization algorithm that avoids
the relative overgeneralization to converge to optima in the continuous task. On more challeng-
ing StarCraft II tasks, due to the high representation ability and exploration efficiency of FSV, it
significantly outperforms other baselines, SMAC (Samvelyan et al. (2019)).
2 Preliminaries
2.1	DEC-POMDP AND CTDE
A fully cooperative multi-agent task can be described as a Dec-POMDP defined by a tuple G =
hS, U , P, r, Z, O, N , γi, where s ∈ S is the global state of the environment. Each agent i ∈ N
choose an action ui ∈ U at each time step, forming a joint action u ∈ UN. This causes a transition to
the next state according to the state transition function P(s0|s, u) : S × UN × S → [0, 1] and reward
function r(s, u) : S × UN → R shared by all agents. γ ∈ [0, 1] is a discount factor. Each agent
has individual, partial observation z ∈ Z according to observation function O(s, i) : S × N → Z.
Each agent also has an action-observation history Ti ∈ T : (ZXU)*, on which it conditions a
stochastic policy ∏i(u∕τi) : T × U → [0,1]. The joint policy π has a joint action-value function
Qπ(st, ut)
=ESt + 1：8,Ut+1：8[P∞=0 Ykrt+k lst,ut].
Centralized Training with Decentralized Execution (CTDE) is a common paradigm of cooperative
MARL tasks. Through centralized training,the action-observation histories of all agents and the
full state can be made accessible to all agents. This allows agents to learn and construct individual
action-value functions correctly while selecting actions based on its own local action-observation
history at execution time .
2.2	VDN, QMIX AND QTRAN
An important concept for factorizable tasks is IGM which asserts that the joint action-value function
Qtot : TN × UN → R and individual action-value functions [Qi : T × U → R]iN=1 satisfies
arg maxQtot (τ, u) = (argmaxQ1(τ1, u1), ..., arg maxQN (τN, uN))	(1)
u	u1	uN
To this end, VDN and QMIX give sufficient conditions for the IGM by additivity and monotonicity
structures, respectively, as following:
N
Qtot (τ, u) =	Qi (τi, ui) and
i=1
∂Qtot(τ, U)
∂ Qi(τi, Ui)
> 0, ∀i ∈ N
(2)
However, there exist tasks whose joint action-value functions do not meet the said conditions, where
VDN and QMIX fail to construct individual action-value function correctly. QTRAN uses a lin-
ear constraint between individual and joint action values to guarantee the optimal decentralisation.
2
Under review as a conference paper at ICLR 2021
To avoid the intractability, QTRAN relax these constraints using two L2 penalties. However, this
relaxation may violate the IGM and it has poor performance on multiple multi-agent cooperative
benchmarks as reported recently.
2.3	the relative overgeneralization problem
Relative overgeneralization occurs when a sub-optimal Nash Equilibrium (e.g. N in Fig. 1) in joint
action space is preferred over an optimal Nash Equilibrium (e.g. M in Fig. 1) because each agent’s
action in the suboptimal equilibrium is a better choice when matched with arbitrary actions from
the collaborating agents. Specifically, as shown in Figure 1, where two agents with one-dimensional
bounded action (or three actions in discrete action space) try to cooperate and find the optimal joint
action, the action B (or C) is often preferred by most algorithms as mentioned in (Son et al. (2019)
and Wei et al. (2018)) due to their structure limitation and lack of exploration.
ABC
Figure 1: The relative overgeneralization in discrete (a) and continuous (b) action space
3 method
In this section, we will first introduce the IGO (Individual-Global-Optimal), a new definition of fac-
torizable MARL tasks with stochastic policies. Theoretical analysis shows that IGO degenerates into
IGM if the policy is greedy. With the energy-based policy, the structure between joint and individual
action values of IGO can be explicitly constructed, which is a novel factorization stochastic-based
policy solution we proposed, named FSV. Specifically, FSV realizes IGO using an efficient linear
structure and learns stochastic policies through maximum entropy reinforcement learning.
3.1	Individual Global Optimal
In the CTDE paradigm, each agent i ∈ N chooses an action based on a stochastic policy ∏i(ui∣τi) at
the same time step. Thejoint policy ∏tot(u∣τ) = Qi=I ∏i(ui∣τi) describes the probability of taking
joint actions u on joint observation history τ . If each agent adopts its optimal policy while the joint
policy is exactly the optimum, the task itself can achieve global optimum through local optimum,
which naturally motivates us to consider the factorizable tasks with stochastic policy as following:
Definition 1 For a joint optimal policy ∏力t(u∣τ) : TN X UN → [0,1], if there exists individual
optimal policies [π*(ui ∣τi) : T × U → [0,1]] N=I, such that thefolloWing holds
N
πtot(u∖τ) = Π K (UiITi)
(3)
i=1
then, we say that [πi] satisfy IGO for πtot
As specified above, IGO requires the consistency of joint optimal policy and individual optimal
policies rather than the actions in IGM, but it degenerates into IGM if policies are greedy. That is to
say, IGO is more generality than IGM.
3.2	FSV
In this work, we take the energy-based policies as joint and individual optimal policy respectively,
α
not(UIT) = exp( —(Qtot(T, U)- Vtot(T)))
(4)
3
Under review as a conference paper at ICLR 2021
∏i(ui∣Ti) = exp( 一 (Qi(Ti,Ui) - Vi(Ti)))	(5)
αi
where a,ai are temperature parameters, Vtot(T) = α log %N exp( 1 Qtot(T,u))du and Vi(Ti) =
ai log JU exp(O-Qi(Ti, U))du are partition functions.
The benefit of using energy-based policy is that it is a very general class of distributions that can
represent complex, multi-modal behaviors Haarnoja et al. (2017). Moreover, energy-based policies
can easily degenerate into greedy policies as α, αi anneals.
To learn this decentralized energy-based policy, we extend the maximum entropy reinforcement
learning framework for the multi-agent setting, which we’ll describe in the next. Another benefit of
considering the stochastic policy with explicit function class for factorizable tasks through IGO is
that the architecture between joint and individual action values can be easily constructed through its
constrains on policies with specific meanings as follows.
Theorem 1	If the task satisfies IGO, with energy-based optimal policy, the joint action value Qtot
can be factorized by individual action values [Qi]iN=1 as following:
N
Qtot(T,u) = X λ*[Qi(Ti,Ui) - Vi(Ti)] + Vtot(T)	(6)
i=1
where λ* = α∕ɑ%.
Theorem 1 gives the decomposition structure like VDN—the joint value is a linear combination of
individual values weighted by λ* > 0. However, the function class defined by Eq(6), which should
only concern the task itself, is related to and limited by the distributions of policy. Although energy-
based distribution is very general which has the representation ability of most tasks, to establish the
correct architecture between joint and individual Q-values and enable stable learning, we need to
extend the function class into any distributions. The key idea is that we approximate the weight
vector λi directly as α, αi is zero instead of annealing αi during training process. This extends the
function class and will at least guarantee IGM constraint when α, αi is zero .
Theorem 2	When α, αi → 0, the function class defined by IGM is equivalent to the following
N
Qtot(T, u) = Xλi(T,u)[Qi(Ti, ui) - Vi(Ti)] + Vtot (T)	(7)
i=1
where λi(T,u)= lim λ*.
α,αi →0
Note that λi is now a function of observations and actions due to the relaxation. Eq(7) allows us to
use a simple linear structure to train joint and individual action values efficiently and guarantee the
correct estimation of optimal Q-values. We’ll describe it in experiment.
Then, we introduce the maximum entropy reinforcement learning in CTDE setting which is an
directly extension of soft actor-critic (q-learning).
The standard reinforcement learning tries to maximum the expected return Pt Eπ [rt], while the
maximum entropy objective generalizes the standard objective by augmenting it with an entropy
term, such that the optimal policy additionally aims to maximize its entropy at each visited state
∏MaxEnt = arg max £ En [rt + αH(∏(∙∣st))]	(8)
πt
where α is the temperature parameter that determines the relative importance of the entropy term
versus the reward, and thus controls the stochasticity of the optimal policy (Haarnoja et al. (2017)).
We can extend it into cooperative multi-agent tasks by directly considering the joint policy πtot (u|T)
and defining the soft joint action-value function as following:
∞
Qtot(Tt ,ut) = r(τt,ut) + ETt+1 ,...[X Yk (rt+k + αH (ntot(∙lTt+k ))]	⑼
k=1
4
Under review as a conference paper at ICLR 2021
then the joint optimal policy for Eq(8) is given by Eq(4) (Haarnoja et al. (2017)). Note that we
don’t start considering decentralized policies, the joint Q-function should satisfy the soft Bellman
equation:
Qtot(Tt,ut) = rt + ETt+1 Hot(Tt+1)]
And we can update the joint Q functions in centralized training through soft Q-iteration:
Qtot(Tt, Ut) - rt + ETt+1 [Vtot(Tt+1)]
(10)
(11)
It’s natural to take the similar energy-based distribution as individual optimal policies πit in Eq(5)
which allows us to update the individual policies through soft policy-iteration:
∏new = argminDκL(∏0(∙∣τ )∣∣πt(∙∣τ))
π0∈Q
(12)
3.3 Architecture
In this section, we present a novel MARL framework named FSV, which incorporates the idea in a
simple and efficient architecture through Eq(7) with multi-agent maximum entropy reinforcement
learning. FSV can be applied both in continuous action space and also in discrete action space as a
simplification.
Figure 2: FSV network architecture
Figure 2 shows the overall learning framework, which consists of two parts:(i) individual parts for
each agent i, which represents Qi, Vi and πi (ii)incorporation part that composes Qi, Vi to Qtot .
Individual parts for each agent i has three networks: (i)individual Q network takes its own
action and observation history Ti, ui as input and produces action-values Qi (Ti, ui) as out-
put.(ii)individual value network takes its own observation history Ti as input and produces Vi (Ti)
as output.(iii)individual policy network takes its own observation history Ti as input and produces a
distribution (e.g. mean and standard deviation of Gaussian distribution) for sample actions.
Incorporation part composes Qi , Vi to Qtot through linear combination. Specifically, it sums up
[Qi - Vi ]iN=1 with coefficients λi and uses a one-layer hyper-network to efficiently approximate the
high-dimensional partition function as following:
N
Vtot (T) = Xwi(T)Vi(Ti) + b(T)	(13)
i=1
where wi , b is a positive weight and bias respectively. To enable efficient learning, we adopt a
multi-head attention structure to estimate the weight vector:
H
λi(T,u) = Xλi,h(T,u)	(14)
h=1
5
Under review as a conference paper at ICLR 2021
where H is the number of attention heads and λi,h is defined by
λi,h (X exp(eTwThWllhe^s)	(15)
where eu and es is obtained by two-layer embedding transformation for u and s. The joint action
value function Qtot is updated through soft Q-iteration:
JQ tot = E(τt,ut)~D [Qtot(τt,ut) - Q(τt,ut)]2	(16)
where Q(Tt,u) = r(τt,uf) + γEτt+1~D,ut+1~∏[Qtοt(τt+ι,ut+ι) 一 α log∏tot(ut+ι 兀+。].
The individual value network is trained by minimize
JV = Eτa~DM(Ti) - (EUJQi(Ti,Ui) - αlog∏i(ui∣Ti)])]2	(17)
The policy network of each agent is trained by minimizing the expected KL-divergence
Jn = ETi~D,Ui~∏i [α log πi (ui |Ti) - Qi(Ti, ui)]	(18)
For discrete action space, it’s convenient to simplify this framework to Q-learning. Specifically, we
directly compute the individual value function Vi = αi log P exp(ɪQi(Ti, ∙)) instead of updating
αi
the value network, and action distributions are directly produced by Eq(5) instead of the policy
network.
4	Related Work
There are many early works with maximum entropy principle such as Todorov (2010) and Levine &
Koltun (2013) use it in policy search in linear dynamics and Kappen (2005) and A. Theodorou et al.
(2010) use it in path integral control in general dynamics. Recent off policy methods (Haarnoja et al.
(2017);Haarnoja et al. (2018b);Haarnoja et al. (2018a)) have been proposed to learn an energy-based
policy efficiently through the maximum entropy objective which is adopted in our framework. Value
function factorization methods start from VDN (Sunehag et al. (2017)), and is extended by QMIX
(Rashid et al. (2018)) and QTRAN (Son et al. (2019)). Other methods such as QATTEN (Yang et al.
(2020)) and MAVEN (Mahajan et al. (2019)) go a step further on architecture and exploration. Our
method are a member of them but out of the deterministic policy
Current methods adopt different ideas to solve the relative overgeneralization problem. Wei et al.
(2018) conduct multi-agent soft Q learning for better exploration. Wen et al. (2019) uses proba-
bilistic recursive reasoning to model the opponents, Yu et al. (2019) adopts inverse reinforcement
learning to avoid this problem through right demonstrations, Tian et al. (2019) derives a variational
lower bound of the likelihood of achieving the optimality for modeling the opponents. However,
none of them adopt value function factorization like FSV which means they suffer the scalability
problem.
5	Experiments
In this section, we first consider two simple examples proposed by prior work (Son et al. (2019),Wei
et al. (2018)) to demonstrate the optimality and convergence of FSV in discrete and continuous
action space respectively. And we evaluate the performance in a challenging set of cooperative
StarCraft II maps from the SMAC benchmark (Samvelyan et al. (2019)).
5.1	Matrix Game
The matrix game is proposed by QTRAN Son et al. (2019), where two agents with three actions
and shared reward as illustrated in Table1, should learn to cooperate to find the optimal joint action
(A, A). This is a simple example of the relative overgeneralization problem, where the sub-optimal
action B, C has higher expected return in exploration process. We train all algorithms through a full
exploration (i.e., = 1 in -greedy) conducted over 20,000 steps while FSV is trained by annealing
α from 1 to α0 . To demonstrate the expressive ability related to temperature parameter α, we set
α0 = 1, 0.1, 0.01 respectively. As shown in Table3, QMIX fails to represent the optimal joint action
6
Under review as a conference paper at ICLR 2021
Table 1:	Payoff of matrix game
Table 2:	QTRAN-alt
Table 3:	QMIX
^∖~U2 ui、∖	A	B	C
A	-8-	-12	~Λ2~
B	^-Γ2-	0	0~o~
C	--T2-	0	~0~
Q2 Qι^^	3.3	0.1	0.1
4.7	8.0	-12.0	-12.0
-0.1	-12.0	0.0	0.0
-0.1	-12.0	0.0	0.0
Q2 Qι^^	-5.6	0.1	0.1
-6.6	-8.1	-8.1	-8.1
02	-8.1	0.0	0.0
0.1	-8.1	0.0	0.0
Table 4:	FSV, α0 = 0.01
Table 5:	FSV, α0 = 0.1
Table 6:	FSV, α0 = 1
Q2 Qi^^	3.3	-0.7	-0.0
4.7	8.0	-12.0	-12.0
07	-12.0	0.0	0.0
0.7	-12.0	0.0	0.0
^^Qr Qi	7.2	-0.5	-0.5
0.9	8.0	-11.8	-11.7
05	-11.8	-0.0	0.0
0.5	-11.8	-0.0	0.0
Q2 Qi^^	0.3	0.1	0.1
7.7	-80-	~4Γ	--49-
00		0.1	-0.0
-0.1		-0.1	-0.0
value and the optimal action due to the limitation of additivity and monotonicity structures while
FSV and QTRAN successfully represent all the joint action values. In addition, even if α is not
annealed to very small, FSV correctly approximated the optimal joint action values because we
directly estimate λ when α and αi tend to 0, which relaxes the constraints of the function class to
guarantee the correct structure during the training process.
5.2	Max of Two Quadratics game
We use The Max of Two Quadractics game (Wei et al. (2018)), which is a simple single state con-
tinuous game for two agents, to demonstrate the performance of current algorithms in the relative
overgeneralization problem. Each agent has one dimensional bounded action with shared reward as
following
fι = hi X Hu1-x1 )2-(u2-y1 )2]
f2 = h2 × [-(u1s2x2)2 — (u2-y2)2]+ C	(19)
r(u1, u2) = max(f1, f2)
where u1,u2 are the actions from agent 1 and agent 2 respectively, h1 = 0.8, h2 = 1, s1 = 3,
s2 - 1, x1 = -5, x2 = 5, y1 = -5, y2 = 5, c = 10. The reward function is shown as Fig
3(a). Although this game is very simple, the gradient points to the sub-optimal solution at (x1, y1)
over almost all the action space which will fox the policy-based method. And for value function
factorization methods, this task requires non-monotonic structures to correctly represent the optimal
joint Q-values through individual Q values. We extend QMIX and VDN to actor-critic framework
(like DDPG) while QTRAN is not applicable in continuous action space due to its requirement of
max operations on Q-values.
Figure 3: Max of Two Quadratics game:(a)reward function, (b)average reward for FSV,VDN,QMIX
and MADDPG
Fig 3(b) is the training result averaged over 20 experiment runs and Table 7 gives a more detailed
result, where MADDPG and QMIX happened to find the optimal actions due to random initialization
twice. VDN never find the optimal actions and even fails to find the sub-optimal 4 times. These
7
Under review as a conference paper at ICLR 2021
Table 7: training result for Max of Two Quadratics game
	opt	sub-opt	other
FSV	^^0^^	0	-0-
MADDPG	2^Γ~	-18-	-0-
-QMIX-	2^Γ~	-18-	-0-
VDN	-0-	16	4
results indicate that, a more explorative policy and correct estimation of Q-values are both needed
to overcome the relative overgeneralization problem. Using a centralized critic like MADDPG to
guide the decentralized actors will mislead the policy gradients because it averages the Q-values
based on others’ policies (?). Using individual Q-values to guide actors requires the full expressive
ability of factorizable tasks where QMIX and VDN fail to estimate individual Q-values correctly
due to the structural limitation as shown in Sec5.1 and QTRAN losts its tractability for continuous
tasks. To enable better exploration in joint action space, Wei et al. (2018) adopt multi-agent soft
q-learning to avoid the relative overgeneralization problem, but it still uses a centralized critic which
suffers scalability and it’s very sensitive to how the temperature parameter anneals. It’s clear that,
FSV utilizes value function factorization method to get correct estimation of individual Q-values
and carries exploration with a more explorative energy-based policy can achieve 100% success rate.
5.3	STARCRAFT II
We choose a challenging set of cooperative StarCraft II maps from the SMAC benchmark
(Samvelyan et al. (2019)). Our evaluation procedure is similar to Samvelyan et al. (2019), where the
training process is paused every 100000 time steps to run 32 evaluation episodes with decentralised
greedy action selection. We compare FSV with VDN, QMIX and QTRAN on several SMAC maps.
Here We present the results for Easy map 2s3z, Hard map 3s_vs_5z and Super Hard map MMM2,
which is classified by Samvelyan et al. Fig 4 shows the test win rate averaged over 5 experiment runs
for the different algorithms on the maps. FSV achieves state-of-the-art due to the high representation
ability and exploration efficiency
Figure 4: test Win rate of FSV, VDN, QMIX and QTRAN
6	conclusion
In this paper, We proposed a neW definition of factorizable tasks With stochastic policies named IGO.
Then We introduced FSV, a novel MARL algorithm under IGO, Which learns to factorize soft value
function into individual ones for decentralized execution enabling efficient learning and exploration
through maximum entropy reinforcement learning. As immediate future Work, We aim to develop a
theoretical analysis for FSV as a policy-based method. We Would also like to explore the committed
exploration like Mahajan et al. (2019) in continuous space due to the miscoordination caused by
energy-based policy (Wei & Luke (2016)).
8
Under review as a conference paper at ICLR 2021
References
Evangelos A. Theodorou, Jonas Buchli, and Stefan Schaal. A generalized path integral control
approach to reinforcement learning. Journal of Machine Learning Research, 11(11):3137-3181,
2010.
L. Busoniu, R. Babuska, and B. De Schutter. A comprehensive survey of multiagent reinforcement
learning. IEEE Transactions on Systems Man Cybernetics Part C, 38(2):156-172, 2008.
Y. Cao, W. Yu, W. Ren, and G. Chen. An overview of recent progress in the study of distributed
multi-agent coordination. IEEE Transactions on Industrial Informatics, 9(1):427-438, 2013.
Jakob N. Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. CoRR, abs/1705.08926, 2017. URL http://
arxiv.org/abs/1705.08926.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. CoRR, abs/1702.08165, 2017. URL http://arxiv.org/abs/
1702.08165.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. CoRR, abs/1801.01290,
2018a. URL http://arxiv.org/abs/1801.01290.
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft actor-critic algo-
rithms and applications. CoRR, abs/1812.05905, 2018b. URL http://arxiv.org/abs/
1812.05905.
H J Kappen. Path integrals and symmetry breaking for optimal control theory. Journal of Statis-
tical Mechanics: Theory and Experiment, 2005(11):P11011-P11011, Nov 2005. ISSN 1742-
5468. doi: 10.1088/1742-5468/2005/11/p11011. URL http://dx.doi.org/10.1088/
1742-5468/2005/11/P11011.
Sergey Levine and Vladlen Koltun. Variational policy search via trajectory optimiza-
tion. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q.
Weinberger (eds.), Advances in Neural Information Processing Systems 26, pp. 207-
215. Curran Associates, Inc., 2013. URL http://papers.nips.cc/paper/
5178-variational-policy-search-via-trajectory-optimization.pdf.
Ryan Lowe. Multi-agent actor-critic for mixed cooperative-competitive environments. 2017.
Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. Maven: Multi-agent
variational exploration. 2019.
Frans A. Oliehoek, Matthijs T. J. Spaan, and Nikos A. Vlassis. Optimal and approximate q-value
functions for decentralized pomdps. CoRR, abs/1111.0062, 2011. URL http://arxiv.org/
abs/1111.0062.
Liviu Panait and Sean Luke. Cooperative multi-agent learning: The state of the art. Autonomous
Agents and Multi-Agent Systems, 11(3):387-434, November 2005. ISSN 1387-2532. doi: 10.
1007/s10458-005-2631-2. URL https://doi.org/10.1007/s10458-005-2631-2.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foer-
ster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent
reinforcement learning. 2018.
Mikayel Samvelyan, Tabish Rashid, Christian Schroder de Witt, Gregory Farquhar, Nantas Nardelli,
Tim G. J. Rudner, Chia-Man Hung, Philip H. S. Torr, Jakob N. Foerster, and Shimon Whiteson.
The starcraft multi-agent challenge. CoRR, abs/1902.04043, 2019. URL http://arxiv.
org/abs/1902.04043.
Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning
to factorize with transformation for cooperative multi-agent reinforcement learning. 2019.
9
Under review as a conference paper at ICLR 2021
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi,
Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, and Karl and Tuyls. Value-
decomposition networks for cooperative multi-agent learning. 2017.
Zheng Tian, Ying Wen, Zhichen Gong, Faiz Punakkath, Shihao Zou, and Jun Wang. A regularized
opponent model with maximum entropy objective. CoRR, abs/1905.08087, 2019. URL http:
//arxiv.org/abs/1905.08087.
Emanuel Todorov. Policy gradients in linearlysolvable mdps. Advances in Neural Information
Processing Systems, pp. 2298-2306, 2010.
Karl Tuyls and Gerhard Weiss. Multiagent learning: Basics, challenges, and prospects. Ai Magazine,
33(3):41-52, 2012.
Ermo Wei and Sean Luke. Lenient learning in independent-learner stochastic cooperative games.
Journal of Machine Learning Research, 17(84):1-42, 2016. URL http://jmlr.org/
papers/v17/15-417.html.
Ermo Wei, Drew Wicke, David Freelan, and Sean Luke. Multiagent soft q-learning. CoRR,
abs/1804.09817, 2018. URL http://arxiv.org/abs/1804.09817.
Ying Wen, Yaodong Yang, Rui Luo, Jun Wang, and Wei Pan. Probabilistic recursive reasoning
for multi-agent reinforcement learning. CoRR, abs/1901.09207, 2019. URL http://arxiv.
org/abs/1901.09207.
Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, and Hongyao
Tang. Qatten: A general framework for cooperative multiagent reinforcement learning. 2020.
Lantao Yu, Jiaming Song, and Stefano Ermon. Multi-agent adversarial inverse reinforcement learn-
ing. 2019.
7 Appendix
7.1	proof
7.1.1	relationship between IGO and IGM
If the joint and individual optimal policies are greedy:
∏i(ui∣Ti)=	1, ui = arg maxQi (Ti, ui) ui	(20) 0,	otherwise
π(u∣τ)二	1, u = arg maxQ(T, u) u	(21) 0,	otherwise
Then IGO gives that u = argmaxQ(τ, u) if and only if ui = arg maxQi(τi, ui) for any i which is
u	ui
equivalent to IGM.
7.1.2	proof of theorem 1
Theorem 1 If the task satisfies IGO, with energy-based optimal policy, the joint action value Qtot
can be factorized by individual action values [Qi]iN=1 as following:
N
Qtot(T,u) = X λ*[Qi(τi,Ui) - Vi(τi)] + Vtot(T)	(22)
i=1
where λ* = α∕ɑ%.
10
Under review as a conference paper at ICLR 2021
Proof. Considering Eq(4) and Eq(5), IGO can be reformulated as:
1	N1
exp( 一(Qtot(τ,u) - Vtot(T))) = HexP( 一(Qi(Ti,%) - Vi(Ti)))
α	αi
i=1	i
This gives:
N
Qtot(T, U) =):	[Qi(τi, Ui) - Vi(Ti)] + Vtot(T)
αi
i=1 i
which is Theorem 1.
(23)
(24)
7.1.3 proof of theorem2
Theorem 2 When α, αi → 0, the function class defined by IGM is equivalent to the following
N
Qtot(T, U) = Xλi(T,U)[Qi(Ti, Ui) - Vi(Ti)] + Vtot (T)	(25)
i=1
where λi(T,u) = lim λ*.
α,αi →0
Proof. IGM⇒Eq(7):
It’s clear that Eq(7) can always hold if λi is well constructed. Here we give one way to construct λi
which meanwhile explains how we extend function class limited by energy-based policy.
Denote ∏i, ∏tot to be the current policy and ∏i, ∏to to be the optimal policy. We can always take
individual optimal policies during the process of approaching greedy, thus
1 - ,
∏i(ui∣Ti) = exp(工(Qi(Ti,Ui) - Vi(Ti)))= <
αi
Ui = arg maxQi (Ti, Ui)
ui
otherwise
where is a small parameter. Then, the joint policy is given by:
πtot(UIT)={ 1 -1(1 -) e)n,
U = [Ui]in=1
otherwise
(26)
(27)
Considering IGM, U = argmaxQtot(T, u). Then Ktot = ∏tot = exp(1 (Qtot(t,u) - Vtot(T))) and
u
α, αi is a function of observations and actions:
αi (Ti, Ui)
Qi(Ti, Ui) - Vi(Ti)
lθg(1 - E)
Ui = arg maxQi (Ti, Ui)
ui
(28)
Qtot (T, U) - Vtot (T)
α(T,u) =----------- -----ʌ——,	u = argmaXQtot (t,u)	(29)
n log(1 - E)	u
Thus, λi is given by:
「 Q _ Qtot(T,u - Vtot(T)	Ca
λi(T, u) = n(Qi(τi,Ui) - Vi(Ti))	(30)
This makes the Eq(7) permanent. In particular, if sampled action u is exactly current arg maxQtot,
u
then Qtot = Vtot and Qi = Vi when α, αi → 0 and λi can be set to 1. Thus we extend the function
class and Eq(7) always holds for any action.
IGMUEq(7):
denote ui* = arg maxQi (Ti, ui) and u* = arg maxQ(T, u). Remember
ui	u
Vtot(τ) = α log /^exp( Qtot(T,u))du ≥ Qtot(T,u)
Vi(Ti) = αi log	exP(
U
α
Qi(Ti,ui))du ≥ Qi(Ti,Ui)
αi
(31)
(32)
Therefore, Qtot = Vtot and Qi = Vi if and only if U = u* and Ui = u*, respectively. Considering
Eq(7) and λi > 0, u = u* if and only if ui = ui* , that complete the proof.
11
Under review as a conference paper at ICLR 2021
Table 8: hyper-parameters
settings	discrete	continuous
FSV		
layer number of λ	2	2
unit number of hidden layer in λ	64	64
layer number of w, b	1	1
unit number of hidden layer in w, b	0	64
layer number of actor	0	2
unit number of hidden layer in actor	0	64
learning rate of actor	0	3e - 4
α decay scheme	linear decay from 1 to 0.01	Automate Entropy Adjustment
QMIX and VDN		
layer number of actor	0	2
unit number of hidden layer in actor	0	64
learning rate of actor	0	3e-4
7.2 Implementation Details
In discrete tasks, We follow the PyMARL Samvelyan et al. (2019) implementation of VDN, QMIX
and QTRAN, where the hyper-parameters of are the same in SMAC Samvelyan et al. (2019). We
illustrate the special hyper-parameters of FSV in Table 8 and others are the default settings in Py-
MARL. In discrete tasks, we extend VDN and QMIX to the actor-critic framework. Specifically,
we add an actor for individual agents to maximise the Q-values from their own critic like DDPG.
We illustrate the hyper-parameters of these algorithm as well as FSV in Table 8. For stability, we
reformulate the Eq(7) as following:
N	NN
Qtot(τ,u) =	λi(τi,ui)(Qi(τi,ui) - Vi(τi)) + Vtot (τ) -	Qi(τi,ui) +	Qi(τi, ui) (33)
i=1	i=1	i=1
Then we stop all the gradients except for λi and the last term Qi . Considering incorrect weight
λi will cause incorrect αi at the beginning of training thus obstruct the exploration, we use αi =
α , which means we ignore the KL-divergence between current policy and optimal policy. The
only temperature parameter α is updated through annealing or Automating Entropy Adjustment in
Haarnoja et al. (2018b) as following:
Ja = Eu〜∏[-α logπ(u∣τ) - αH]	(34)
where the H is the target entropy.
12