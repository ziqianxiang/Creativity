Under review as a conference paper at ICLR 2021
Improved Gradient based Adversarial
Attacks for Quantized Networks
Anonymous authors
Paper under double-blind review
Ab stract
Neural network quantization has become increasingly popular due to efficient
memory consumption and faster computation resulting from bitwise operations
on the quantized networks. Even though they exhibit excellent generalization
capabilities, their robustness properties are not well-understood. In this work,
we systematically study the robustness of quantized networks against gradient
based adversarial attacks and demonstrate that these quantized models suffer from
gradient vanishing issues and show a fake sense of robustness. By attributing
gradient vanishing to poor forward-backward signal propagation in the trained
network, we introduce a simple temperature scaling approach to mitigate this issue
while preserving the decision boundary. Despite being a simple modification to
existing gradient based adversarial attacks, experiments on CIFAR-10/100 datasets
with multiple network architectures demonstrate that our temperature scaled attacks
obtain near-perfect success rate on quantized networks while outperforming original
attacks on adversarially trained models as well as floating-point networks.
1	Introduction
Neural Network (nn) quantization has become increasingly popular due to reduced memory and
time complexity enabling real-time applications and inference on resource-limited devices. Such
quantized networks often exhibit excellent generalization capabilities despite having low capacity due
to reduced precision for parameters and activations. However, their robustness properties are not well-
understood. In particular, while parameter quantized networks are claimed to have better robustness
against gradient based adversarial attacks (Galloway et al. (2018)), activation only quantized methods
are shown to be vulnerable (Lin et al. (2019)).
In this work, we consider the extreme case of Binary Neural Networks (bnns) and systematically
study the robustness properties of parameter quantized models, as well as both parameter and
activation quantized models against gradient based adversarial attacks. Our analysis reveals that these
quantized models suffer from gradient masking issues (Athalye et al. (2018)) (especially vanishing
gradients) and in turn show fake robustness. We attribute this vanishing gradients issue to poor
forward-backward signal propagation caused by trained binary weights, and our idea is to improve
signal propagation of the network without affecting the prediction of the classifier.
There is a body of work on improving signal propagation in a neural network (e.g., Glorot & Bengio
(2010); Pennington et al. (2017); Lu et al. (2020)), however, we are facing a unique challenge of
improving signal propagation while preserving the decision boundary, since our ultimate objective
is to generate adversarial attacks. To this end, we first discuss the conditions to ensure informative
gradients and then resort to a temperature scaling approach (Guo et al. (2017)) (which scales the
logits before applying softmax cross-entropy) to show that, even with a single positive scalar the
vanishing gradients issue in BNNs can be alleviated achieving near perfect success rate in all tested
cases.
Specifically, we introduce two techniques to choose the temperature scale: 1) based on the singular
values of the input-output Jacobian, 2) by maximizing the norm of the Hessian of the loss with
respect to the input. The justification for the first case is that if the singular values of input-output
Jacobian are concentrated around 1 (defined as dynamical isometry (Pennington et al. (2017))) then
the network is said to have good signal propagation and we intend to make the mean of singular
1
Under review as a conference paper at ICLR 2021
values to be 1. On the other hand, the intuition for maximizing the Hessian norm is that if the Hessian
norm is large, then the gradient of the loss with respect to the input is sensitive to an infinitesimal
change in the input. This is a sufficient condition for the network to have good signal propagation as
well as informative gradients under the assumption that the network does not have any randomized or
non-differentiable components.
We evaluated our improved gradient based adversarial attacks using bnns with weight quantized
(bnn-wq) and weight and activation quantized (bnn-waq), floating point networks (ref), and
adversarially trained models. We employ quantized and floating point networks trained on CIFAR-
10/100 datasets using several architectures. In all tested bnns, both versions of our temperature scaled
attacks obtained near-perfect success rate outperforming gradient based attacks (fgsm (Goodfellow
et al. (2014)), pgd (Madry et al. (2017))). Furthermore, this temperature scaling improved gradient
based attacks even on adversarially trained models (both high-precision and quantized) as well as
floating point networks, showing the significance of signal propagation for adversarial attacks.
2	Preliminaries
We first provide some background on the neural network quantization and adversarial attacks.
2.1	Neural Network Quantization
Neural Network (nn) quantization is defined as training networks with parameters constrained to a
minimal, discrete set of quantization levels. This primarily relies on the hypothesis that since nns are
usually overparametrized, it is possible to obtain a quantized network with performance comparable
to the floating point network. Given a dataset D = {xi, yi}in=1, NN quantization can be written as:
1n
min L(w； D) ：= - £'(w； (Xi, yi)) .	(1)
w∈Qm	n
i=1
Here, '(∙) denotes the input-output mapping composed with a standard loss function (e.g., Cross-
entropy loss), w is the m dimensional parameter vector, and Q is a predefined discrete set representing
quantization levels (e.g., Q = {-1, 1} in the binary case).
Most of the nn quantization approaches (Ajanthan et al. (2019a;b); Bai et al. (2019); Hubara et al.
(2017)) convert the above problem into an unconstrained problem by introducing auxiliary variables
and optimize via (stochastic) gradient descent. To this end, the algorithms differ in the choice of
quantization set (e.g., keep it discrete (Courbariaux et al. (2015)), relax it to the convex hull (Bai
et al. (2019)) or convert the problem into a lifted probability space (Ajanthan et al. (2019a))), the
projection used, and how differentiation through projection is performed. In the case when the
constraint set is relaxed, a gradually increasing annealing hyperparameter is used to enforce a
quantized solution (Ajanthan et al. (2019a;b); Bai et al. (2019)). We refer the interested reader to
respective papers for more detail. In this paper, we use BNN-WQ obtained using MD-tanh-S (Ajanthan
et al. (2019b)) and bnn-waq obtained using Hubara et al. (2017).
2.2	Adversarial Attacks
Adversarial examples consist of imperceptible perturbations to the data that alter the model’s predic-
tion with high confidence. Existing attacks can be categorized into white-box and black-box attacks
where the difference lies in the knowledge of the adversaries. White-box attacks allow the adversaries
access to the target model’s architecture and parameters, whereas black-box attacks can only query
the model. Since white-box gradient based attacks are popular, we summarize them below.
First-order gradient based attacks can be compactly written as Projected Gradient Descent (pgd) on
the negative of the loss function (Madry et al. (2017)). Formally, let X0 ∈ IRN be the input image,
then at iteration t, the PGD update can be written as:
xt+1 = P (xt + η gχ),	⑵
where P : IRN → X is a projection, X ⊂ IRN is the constraint set that bounds the perturbations,
η > 0 is the step size, and gtx is a form of gradient of the loss with respect to the input x evaluated at
xt. With this general form, the popular gradient based adversarial attacks can be specified:
• Fast Gradient Sign Method (fgsm): This is a one step attack introduced in Goodfellow et al.
(2014). Here, P is the identity mapping, η is the maximum allowed perturbation magnitude, and
2
Under review as a conference paper at ICLR 2021
Method 		ReSNet-18	∣				VGG-16	
	Clean	Adv.(1)	Adv.(20) I Clean		Adv.(1)	Adv.(20)
REF	94.46	0.00	0.00	93.31	0.04	0.00
BNN-WQ	93.18	26.98	17.91	91.53	47.32	38.49
BNN-WAQ	87.67	8.57	1.94	89.69	78.01	59.26
Table 1: Clean and adversarial accuracy (PGD attack with L∞ bound) on the test set of CIFAR-10
using ResNet-18 and VGG-16. In brackets, we mention number of random restarts used to perform the
attack. Note, BNNs outperform adversarial accuracy of floating point networks consistently.
varying iterations, (b) varying radius, and (c) black-box attacks on ResNet-18 and VGG-16. While (a),
(c) show signs of gradient masking, (b) does not. We attribute this discrepancy to the random initial
step before PGD.
gX = sign (Vχ'(w*; (xt, y))), where ' denotes the loss function, w* is the trained weights and y
is the ground truth label corresponding to the image x0 .
•	pgd with L∞ bound: Arguably the most popular adversarial attack introduced in Madry et al.
(2017) and sometimes referred to as Iterative Fast Gradient Sign Method (IFGSM). Here, P is the
L∞ norm based projection, η is a chosen step size, and gX = sign (Vχ'(w*; (xt, y))), the sign of
gradient same as fgsm.
•	pgd with L2 bound: This is also introduced in Madry et al. (2017) which performs the standard
PGD in the Euclidean space. Here, P is the L2 norm based projection, η is a chosen step size, and
gX = Vχ'(w*; (xt, y)) is simply the gradient of the loss with respect to the input.
These attacks have been further strengthened by a random initial step (Tramer et al. (2017)). In this
paper, we perform this single random initialization for all experiments with fgsm/pgd attack unless
otherwise mentioned.
3 Robustness Evaluation of B inary Neural Networks
We start by evaluating the adversarial accuracy (i.e. accuracy on the perturbed data) of bnns using
the PGD attack with L∞ bound.
• pgd attack details: perturbation bound of8 pixels (assuming each pixel in the image is in [0, 255])
with respect to L∞ norm, step size η = 2 and the total number of iterations T = 20. The attack
details are the same in all evaluated settings unless stated otherwise.
We perform experiments on CIFAR-10 dataset using ResNet-18 and VGG-16 architectures and report
the clean accuracy and pgd adversarial accuracy with 1 and 20 random restarts in Table 1. It can
be clearly and consistently observed that binary networks have high adversarial accuracy compared
to the floating point counterparts. Even with 20 random restarts, bnns clearly outperform floating
point networks in terms of adversarial accuracy. Since this result is surprising, we investigate this
phenomenon further to understand whether bnns are actually robust to adversarial perturbations or
they show a fake sense of security due to some form of obfuscated gradients (Athalye et al. (2018)).
3.1	Identifying Obfuscated Gradients
Recently, it has been shown that several defense mechanisms intentionally or unintentionally break
gradient descent and cause obfuscated gradients and thus exhibit a false sense of security (Athalye et al.
(2018)). Several gradient based adversarial attacks tend to fail to produce adversarial perturbations in
scenarios where the gradients are uninformative, referred to as gradient masking. Gradient masking
3
Under review as a conference paper at ICLR 2021
can occur due to shattered gradients, stochastic gradients or exploding and vanishing gradients. We
try to identify gradient masking in binary networks based on the empirical checks provided in Athalye
et al. (2018). If any of these checks fail, it indicates gradient masking issue in bnns.
To illustrate this, we analyse the effects of varying different hyperparameters of pgd attack on bnns
trained on CIFAR-10 using ResNet-18 architecture. Even though varying PGD perturbation bound
does not show any signs of gradient masking, varying attack iterations and black-box vs white-box
results (on ResNet-18 and VGG-16) clearly indicate gradient masking issues as depicted in Fig. 1.
The black-box attack outperforming white-box attack for bnns certainly indicates gradient masking
issues since the black-box attack do not use the gradient information from model being attacked.
Here, our black-box model to a bnn is the analogous floating point network trained on the same
dataset and the attack is the same PGD with L∞ bound.
These checks demonstrate that bnns are prone to gradient masking and exhibit fake robustness.
Note, shattered gradients occur due to non-differentiable components in the defense mechanism and
stochastic gradients are caused by randomized gradients. Since bnns are trainable from scratch
and does not have randomized gradients1, we narrow down gradient masking issue to vanishing or
exploding gradients. Since, vanishing or exploding gradients occur due to poor signal propagation,
by introducing a single scalar, we discuss two approaches to mitigate this issue, which lead to almost
100% success rate for gradient based attacks on BNNs.
4	Signal Propagation of Neural Networks
We first describe how poor signal propagation in neural networks can cause vanishing or exploding
gradients. Then we discuss the idea of introducing a single scalar to improve the existing gradient
based attacks without affecting the prediction (i.e., decision boundary) of the trained models.
We consider a neural network fw for an input x0, having logits aK = fw(x0). Now, since softmax
cross-entropy is usually used as the loss function, we can write:
'(aκ, y) = -yτ log(p) , P = Softmax(aK),
(3)
where y ∈ IRd is the one-hot encoded target label and log is applied elementwise.
For various gradient based adversarial attacks discussed in Sec. 2.2, gradient of the loss ` is used with
respect to the input x0 , which can also be formulated using chain rule as,
∂'(aK, y)	∂'(aK, y) ∂aK
∂x0
∂aK	∂x0
ψ(aK, y) J ,
(4)
where ψ denotes the error signal and J ∈ Rd×N is the input-output Jacobian. Here we use the
convention that ∂v∕∂U is of the form V-Size X U-size.
Notice there are two components that influence the gradients, 1) the Jacobian J and 2) the error signal
ψ . Gradient based attacks would fail if either the Jacobian is poorly conditioned or the error signal
has saturating gradients, both of these will lead to vanishing gradients in ∂'∕∂x0.
The effects of Jacobian on the signal propagation is studied in dynamical isometry and mean-field
theory literature (Pennington et al. (2017); Saxe et al. (2013)) and it is known that a network is said to
satisfy dynamical isometry if the singular values ofJ are concentrated near 1. Under this condition,
error signals ψ backpropagate isometrically through the network, approximately preserving its norm
and all angles between error vectors. Thus, as dynamical isometry improves the trainability of the
floating point networks, a similar technique can be useful for gradient based attacks as well.
In fact, almost all initialization techniques (e.g., Glorot & Bengio (2010)) approximately ensures
that the Jacobian J is well-conditioned for better trainability and it is hypothesized that approximate
isometry is preserved even at the end of the training. But, for bnns, the weights are constrained to be
{-1, 1} and hence the weight distribution at end of training is completely different from the random
initialization. Furthermore, it is not clear that fully-quantized networks can achieve well-conditioned
Jacobian, which guided some research activity in utilizing layerwise scalars (either predefined or
learned) to improve bnn training (McDonnell (2018); Rastegari et al. (2016)). We would like to
point out that the focus of this paper is to improve gradient based attacks on already trained bnns. To
1 bnn-wq have binary weights, but there is no non-differentiable or randomized component once trained.
4
Under review as a conference paper at ICLR 2021
this end learning a new scalar to improve signal propagation at each layer is not useful as it can alter
the decision boundary of the network and thus cannot be used in practice on already trained model.
4.1	Temperature Scaling for better Signal Propagation
In this paper, we propose to use a single scalar per network to improve the signal propagation of the
network using temperature scaling. In fact, one could replace softmax with a monotonic function
such that the prediction is not altered, however, we will show in our experiments that a single scalar
with softmax has enough flexibility to improve signal propagation and yields almost 100% success
rate with PGD attacks. Essentially, we can use a scalar, β > 0 without changing the decision boundary
of the network by preserving the relative order of the logits. Precisely, we consider the following:
p(β) = Softmax(aK) ,	aκ = β aκ .	(5)
Here, we write the softmax output probabilities p as a function of β to emphasize that they are softmax
output of temperature scaled logits. Now since in this context, the only variable is the temperature
scale β, we denote the loss and the error signal as functions of only β. With this simplified notation
the gradient of the temperature scaled loss with respect to the inputs can be written as:
∂'(β) _ ∂'(β) ∂aκ ∂aK
----∑—	---------- -
∂x0	∂aK ∂aκ ∂x0
ψ(β)β J.
(6)
Note that β affects the input-output Jacobian linearly while it nonlinearly affects the error signal ψ .
To this end, we hope to obtain a β that ensures the error signal is useful (i.e., not all zero) as well as
the Jacobian is well-conditioned to allow the error signal to propagate to the input.
We acknowledge that while one can find a β > 0 to obtain softmax output ranging from a uniform
distribution (β = 0) to one-hot vectors (β → ∞), β only scales the Jacobian. Therefore, if the
Jacobian J has zero singular values, our approach has no effect in those dimensions. However,
since most of the modern networks consist of ReLU nonlinearities (generally positive homogeneous
functions), the effect of a single scalar would be equivalent (ignoring the biases) to having layerwise
scalars such as in McDonnell (2018). Thus, we believe a single scalar is sufficient for our purpose.
5	Improved Gradients for Adversarial Attacks
Now we discuss strategies to choose a scalar β such that
the gradients with respect to input are informative. Let us
first analyze the effect of β on the error signal. To this end,
ψ(β)= M)∂Pr-(y-p”⑺
where y is the one-hot encoded target label, and p(β) is
the softmax output of scaled logits.
For adversarial attacks, we only consider the correctly clas- Figure 2: Error signal (ψ(β)) and Ja-
Sifiedimages (i.e., argmaxj y7- = argmaxj Pj(β)) as there cobian ofsoftmax (dp(β)∕daκ) Vs. B
jj
is no need to generate adVersarial examples corresponding for a random correctly classified logits.
to misclassified samples. From the aboVe formula, it is clear that when p(β) is one-hot encoding
then the error signal is 0. This is one of the reason for Vanishing gradient issue in BNNs. EVen if this
does not happen for a giVen image, one can increase β → ∞ to make this error signal 0. Similarly,
when p(β) is the uniform distribution, the norm of the error signal is at the maximum. This can be
obtained by setting β = 0. However, this would also make ∂'(β)∕∂x0 = 0 as the singular values of
the input-output Jacobian would all be 0. How error signal is affected by β is illustrated in Fig. 2.
This analysis indicates that the optimal β cannot be obtained by simply maximizing the norm of the
error signal and we need to balance both the Jacobian as well as the error signal. To summarize, the
scalar β should be chosen such that the following properties are satisfied:
1.	kψ(β)k2 > ρ for some ρ > 0.
2.	The Jacobian β J is well-conditioned, i.e., the singular values of βJ is concentrated around 1.
5.1	Network Jacobian Scaling (njs)
We now discuss a straightforward, two-step approach to attain the aforementioned properties. Firstly,
to ensure βJ is well-conditioned, we simply choose β to be the inverse of the mean of singular values
5
Under review as a conference paper at ICLR 2021
of J. This guarantees that the mean of singular values of βJ is 1. After this scaling, it is possible
that the resulting error signal is very small. To ensure that kψ(β)k2 > ρ > 0, we ensure that the
softmax output pk (β) corresponding to the ground truth class k is at least ρ away from 1. We now
state it as a proposition to derive β given a lowerbound on 1 - pk(β).
Proposition 1. Let aK ∈ IRd with d > 1 and a1K ≥ a2K ≥ . . . ≥ adK and a1K - adK = γ.
For a given 0 < ρ < (d - 1)/d, there exists a β > 0 such that 1 - softmax(β a1K) > ρ, then
β < -IOg(P/(d - I)(I - P))/Y.
Proof. This is derived via a simple algebraic manipulation of Softmax. Please refer to Appendix. □
This β can be used together with the one computed using inverse of mean Jacobian Singular Values
(jsv). We provide the pseudocode for our proposed pgd++ (njs) attack in Appendix. Similar
approach can also be applied for fgsm++. Notice that, this approach is simple and it adds negligible
overhead to the standard PGD attacks. However, it has a hyperparameter P which is hand designed.
To mitigate this, next we discuss a hyperparameter-free approach to obtain β .
5.2	Hessian Norm Scaling (hns)
We now discuss another approach to obtain informative gradients. Our idea is to maximize the
Frobenius norm of the Hessian of the loss with respect to the input, where the intuition is that if
the Hessian norm is large, then the gradient ∂'∕∂x0 is sensitive to an infinitesimal change in x0.
This means, the infinitesimal perturbation in the input is propagated in the forward pass to the last
layer and propagated back to the input layer without attenuation (i.e., the returned signal is not zero),
assuming there are no randomized or non-differentiable components in the network. This clearly
indicates that the network has good signal propagation as well as the error signals are not all zero.
This objective can now be written as:
β*
argmax || H || =argmax
β>0	∂(x0)2F	β>0
ψ(β)M + β (臀J)T J
∂x0	∂aK
β
The derivation is provided in Appendix. Note, since J does
not depend on β, J and ∂J ∕∂x0 are computed only once,
β is optimized using grid search as it involves only a single
scalar. In fact, it is easy to see from the above equation that,
when the Hessian is maximized, β cannot be zero. Simi-
larly, ψ(β) cannot be zero because ifit is zero, then the pre-
diction p(β ) is one-hot encoding (Eq. (7)), consequently
∂p(β)∕∂aK = 0 and this cannot be a maximum for the
Hessian norm. Hence, this ensures that kψ(β*)k2 > P for
some ρ > 0 and β* is bounded according to Proposition 1.
Therefore, the maximum is obtained for a finite value of
β . Even though, it is not clear how exactly this approach
(8)
F
Figure 3: Hessian norm vs. β on a ran-
dom correctly classified image. The plot
clearly shows a concave behaviour. s
would affect the singular values of the input-output Jacobian (β J), we know that they are finite and
not zero. How Hessian norm is influenced by β is illustrated in Fig. 3.
Furthermore, there are some recent works (Moosavi-Dezfooli et al. (2019); Qin et al. (2019)) show
that adversarial training makes the loss surface locally linear around the vicinity of training samples
and enforcing local linearity constraint on loss curvature can achieve better robust to adversarial
attacks. On the contrary, our idea of maximizing the Hessian, i.e., increasing the nonlinearity of
`, could make the network more prone to adversarial attacks and we intend to exploit that. The
psuedocode for pgd++ attack with hns is summarized in Appendix.
6 Experiments
We evaluate robustness accuracies of bnns with weight quantized (bnn-wq), weight and activation
quantized (bnn-waq) floating point networks (ref), and adversarially trained networks. We evaluate
our two pgd++ variants corresponding to Hessian Norm Scaling (hns) and Network Jacobian
Scaling (njs) on CIFAR-10 and CIFAR-100 datasets with multiple network architectures. Briefly,
our results indicate that both of our proposed attack variants yield attack success rate much higher
than original PGD attacks not only on L∞ bounded attack but also on L2 bounded attacks on both
floating point networks and binarized networks. Our proposed pgd++ variants also reduce pgd
6
Under review as a conference paper at ICLR 2021
Network
Adversarial Accuracy (%)
fgsm++
NJS HNS
01，NVH lɔ - oo'NVHIυ
FGSM
PGD (L∞)
pgd++ (L∞)
NJS HNS
PGD (L2)
pgd++ (L2)
NJS HNS
ResNet-18	40.49	3.46	2.51	26.98	0.00	0.00	74.59	0.05	0.05
VGG-16	57.55	4.00	3.43	47.32	0.00	0.00	61.90	0.35	1.32
ResNet-50	57.62	6.44	5.35	43.14	0.00	0.00	74.75	0.11	0.08
DenseNet-121	26.80	4.67	4.24	9.11	0.00	0.00	72.99	0.03	0.06
MObileNet-V2	33.50	6.42	5.42	26.86	0.00	0.00	35.22	0.12	0.09
ResNet-18	25.22
VGG-16	19.82
ResNet-50	37.76
DenseNet-121	28.32
MobileNet-V2	12.09
14.08	1.80
7.98	1.76
16.33	14.17
12.21	10.86
10.18	8.79
8.23
17.44
25.71
8.87
1.44
2.45	0.00	42.67	6.79	0.26
0.88	0.16	19.26	3.17	0.63
2.33	2.73	38.95	7.9	7.41
1.15	1.09	43.78	4.54	4.16
0.57	0.66	8.97	3.39	3.01
Table 2:	Adversarial accuracy on the test set for BNN-WQ. Both our NJS and HNS variants consistently
outperform original L∞ bounded FGSM and PGD attack, and L2 bounded PGD attack.
Network
Adversarial Accuracy (%)
fgsm++
NJS HNS
FGSM
PGD (L∞)
pgd++ (L∞)
NJS HNS
PGD (L2)
pgd++ (L2)
NJS HNS
ResNet-18	7.62	5.55	5.35	0.00	0.00	0.00	45.18	0.09	0.05
VGG-16	11.01	10.04	9.66	0.04	0.00	0.00	2.23	0.78	1.10
ResNet-50	21.64	6.08	5.70	0.69	0.00	0.00	65.56	0.07	0.09
DenseNet-121	11.40	7.58	7.30	0.00	0.00	0.00	38.15	0.08	0.06
O^⅛'ZZM
ResNet-18	40.84
VGG-16	79.92
ResNet-50	33.16
DenseNet-121	37.20
19.46	19.09
15.96	15.39
25.89	27.05
23.89	24.69
8.57	0.03	0.04
78.01	0.01	0.02
0.49	0.23	0.45
0.81	0.10	0.18
67.84	2.33	2.59
85.62	0.49	0.62
32.93	6.68	8.77
Table 3:	Adversarial accuracy on the test set of CIFAR-10 for REF and BNN-WAQ. Both our NJS and
HNS variants consistently outperform original FGSM and PGD (L∞∕L? bounded) attacks.
adversarial accuracy of adversarially trained floating point and adversarially trained binarized neural
networks while outperforming much stronger attacks such as DeepFool (Moosavi-Dezfooli et al.
(2016)) and Brendel & Bethge Attack (bba) (Brendel et al. (2019)). Among our variants, even though
they perform similarly in our experiments, Hessian based scaling (hns) outperforms Jacobian based
scaling (njs) in majority of the cases and this difference is significant for one step fgsm attacks. This
indicates that nonlinearity of the network indeed has some relationship to its adversarial robustness.
We use state of the art models trained for binary quantization (where all layers are quantized) for our
experimental evaluations. We provide adversarial attack parameters used for fgsm/pgd in Appendix
and for other attacks, we use default parameters used in Foolbox (Rauber et al. (2017)). For our
HNS variant, we sweep β from a range such that the hessian norm is maximized for each image, as
explained in Appendix. For our NJS variant, we set the value of ρ = 0.01. In fact, our attacks are not
very sensitive to ρ and we provide the ablation study in the Appendix. The PyTorch (Paszke et al.
(2017)) implementation of our algorithm will be released upon publication.
6.1 Results
We first compared the original PGD (L2∕L∞) and FGSM attack with both versions (NJS and HNS)
of improved PGD++ and FGSM++ attack, on CIFAR-10/100 datasets with ResNet-18/50, VGG-16,
DenseNet-121 and MobileNet-V2 network architectures and the adversarial accuracies for different
bnn-wq are reported in Table 2. Our pgd++ variants consistently outperform original pgd on
all networks on both datasets. Even being a gradient based attack, our proposed PGD++ (L2L∞)
variants can in fact reach adversarial accuracy close to 0 on CIFAR-10 dataset, demystifying the fake
robustness binarized networks tend to exhibit due to poor signal propagation.
7
Under review as a conference paper at ICLR 2021
Network	Adversarial Accuracy (%)									
	FGSM	FGSM β=0.1	fgsm++		PGD	PGD β=0.1	Deep Fool	BBA	pgd++	
			NJS	HNS					NJS	HNS
REF	62.38	69.52	61.43	61.40	48.73	61.27	51.01	48.43	47.17	48.54
BC	53.91	62.46	52.90	52.27	41.29	54.24	42.65	40.14	39.35	39.34
GD-tanh	56.13	65.06	55.54	54.81	42.77	56.78	44.78	42.94	42.14	42.30
MD-tanh-S	55.10	63.42	54.74	53.82	41.34	54.22	43.46	40.69	40.76	40.67
Table 4: Adversarial accuracy on the test set of CIFAR-10 with ResNet-18 for adversarially trained
REF and BNN-WQ using different quantization methods (BC, GD-tanh, MD-tanh-S). Our improved
attacks are compared against FGSM, L∞ bounded PGD, a heuristic choice of β = 0.1, DeepFool and
BBA. Albeit on adversarially trained networks, our methods outperform all the comparable methods.
Network	Adversarial Accuracy (%)							
	FGSM	FGSM (dlr)	fgsm++		PGD	PGD (dlr)	pgd++	
			NJS	HNS			NJS	HNS
REF	7.62	19.48	5.55	5.35	0.00	0.00	0.00	0.00
bnn-wq	40.49	19.72	3.46	2.51	26.98	0.00	0.00	0.00
bnn-waq	40.84	41.78	19.46	19.09	8.57	4.57	0.03	0.04
REF*	62.38	66.39	61.43	61.40	48.73	49.73	47.17	48.54
BNN-WQ*	55.10	59.14	54.74	53.82	41.34	41.42	40.76	40.67
Table 5: Adversarial accuracy for REF, BNN-WQ, and BNN-WAQ trained on CIFAR-10 using ResNet-18.
Here * denotes adversarially trained models. Both our NJS and HNS variants consistently outperform
L∞ bounded FGSM and PGD attack performed with Difference of Logits Ratio (DLR) loss instead of
cross entropy loss. Notice, FGSM and PGD attack with DLR loss (Croce & Hein (2020)) perform even
worse than their original form on adversarially trained models.
Similarly, for one step fgsm attack, our modified versions outperform original fgsm attacks by a
significant margin consistently for both datasets on various network architectures. We would like
to point out such an improvement in the above two attacks is considerably interesting, knowing the
fact that FGSM, PGD with L∞ attacks only use the sign of the gradients so improved performance
indicates, our temperature scaling indeed makes some zero elements in the gradient nonzero. We
would like to point out here that one can use several random restarts to increase the success rate of
original form of fgsm/pgd attack further but to keep comparisons fair we use single random restart
for both original and modified attacks. Nevertheless, as it has been observed in Table 1 even with
20 random restarts pgd adversarial accuracies for bnns cannot reach zero, whereas our proposed
pgd++ variants consistently achieve perfect success rate.
ImageNet. For other large scale datasets such as ImageNet, BNNs are hard to train with full
binarization of parameters and result in poor performance. Thus, most existing works (Yang et al.
(2019)) on bnns keep the first and the last layers floating point and introduce several layerwise scalars
to achieve good results on ImageNet. In such experimental setups, according to our experiments,
trained BNNs do not exhibit gradient masking issues or poor signal propagation and thus are easier
to attack using original fgsm/pgd attacks with complete success rate. In such experiments, our
modified versions perform equally well compared to the original forms of these attacks.
The adversarial accuracies of REF and BNN-WAQ trained on CIFAR-10 using ResNet-18/50, VGG-16
and DenseNet-121 for our variants against original counterparts are reported in Table 3. Overall, for
both ref and bnn-waq, our variants outperform the original counterparts consistently. Particularly
interesting, pgd++ variants improve the attack success rate on ref networks. This effectively
expands the applicability of our pgd++ variants and encourages to consider signal propagation of any
trained network to improve gradient based attacks. PGD++ with L∞ variants achieve near-perfect
success rate on all bnn-waqs, again validating the hypotheses of fake robustness of bnns.
To further demonstrate the efficacy, we first adversarially trained the bnn-wqs (quantized using
BC (Courbariaux et al. (2015)), GD-tanh/MD-tanh-S (Ajanthan et al. (2019b))) and floating point
networks in a similar manner as in Madry et al. (2017), using L∞ bounded PGD with T = 7
8
Under review as a conference paper at ICLR 2021
iterations, η = 2 and = 8. We report the adversarial accuracies of L∞ bounded attacks and our
variants on CIFAR-10 using ResNet-18 in Table 4. These results further strengthens the usefulness
of our proposed PGD++ variants. Moreover, with a heuristic choice of β = 0.1 to scale down the
logits before performing gradient based attacks performs even worse. Finally, even against stronger
attacks (DeepFool (Moosavi-Dezfooli et al. (2016)), BBA (Brendel et al. (2019))) under the same L∞
perturbation bound, our variants outperform consistently on these adversarially trained models. We
would like to point out that our variants have negligible computational overhead over the original
gradient based attacks, whereas stronger attacks are much slower in practice requiring 100-1000
iterations with an adversarial starting point (instead of random initial perturbation).
To illustrate the effectiveness of our proposed variants in improving signal propagation, we compare
against gradient based attacks performed using recently proposed Difference of Logits Ratio (dlr)
loss (Croce & Hein (2020)) that aims to avoid the issue of saturating error signals. We show these
experimental comparisons performed on ResNet-18 models trained on CIFAR-10 dataset in Table 5.
The attack parameters are same as used for the other experiments. It can be clearly observed that
in almost all cases our proposed variants are much better than original form of gradient based
attacks performed with dlr loss. The margin of difference is significant in case of fgsm attack and
adversarial trained models. Infact, it is important to note that gradient based attacks with dlr loss
perform worse on adversarially trained models than the original form of gradient based attacks.
7	Related Work
Adversarial examples are first observed in Szegedy et al. (2014) and subsequently efficient gradient
based attacks such as fgsm (Goodfellow et al. (2014)) and pgd (Madry et al. (2017)) are introduced.
There exist recent stronger attacks such as Moosavi-Dezfooli et al. (2016); Carlini & Wagner (2017);
Yao et al. (2019); Finlay et al. (2019); Brendel et al. (2019), however, compared to pgd, they are
much slower to be used for adversarial training in practice. For a comprehensive survey related to
adversarial attacks, we refer the reader to Chakraborty et al. (2018).
Some recent works focus on the adversarial robustness of bnns (Bernhard et al. (2019); Sen et al.
(2020); Galloway et al. (2018); Khalil et al. (2019); Lin et al. (2019)), however, a strong consensus
on the robustness properties of quantized networks is lacking. In particular, while Galloway et al.
(2018) claims parameter quantized networks are robust to gradient based attacks based on empirical
evidence, (Lin et al. (2019)) shows activation quantized networks are vulnerable to such attacks
and proposes a defense strategy assuming the parameters are floating-point. Differently, Khalil
et al. (2019) proposes a combinatorial attack hinting that activation quantized networks would have
obfuscated gradients issue. Sen et al. (2020) shows ensemble of mixed precision networks to be more
robust than original floating point networks; however Tramer et al. (2020) later shows the presented
defense method can be attacked with minor modification in the loss function. In short, although it
has been hinted that there might be some sort of gradient masking in bnns (especially in activation
quantized networks), a thorough understanding is lacking on whether bnns are robust, if not what
is the reason for the inferior performance of most commonly used gradient based attacks on binary
networks. We answer this question in this paper and introduce improved gradient based attacks.
8	Conclusion
In this work, we have shown that both bnn-wq and bnn-waq tend to show a fake sense of robustness
on gradient based attacks due to poor signal propagation. To tackle this issue, we introduced our
two variants of pgd++ attack, namely njs and hns. Our proposed pgd++ variants not only possess
near-complete success rate on binarized networks but also outperform standard L∞ and L2 bounded
pgd attacks on floating point networks. We finally show improvement in attack success rate on
adversarially trained ref and bnn-wq against stronger attacks (DeepFool and bba). In future, we
intend to focus more on improving the robustness of the bnns with provable robustness guarantees.
References
Thalaiyasingam Ajanthan, Puneet K Dokania, Richard Hartley, and Philip HS Torr. Proximal
mean-field for neural network quantization. ICCV, 2019a.
9
Under review as a conference paper at ICLR 2021
Thalaiyasingam Ajanthan, Kartik Gupta, Philip HS Torr, Richard Hartley, and Puneet K Dokania.
Mirror descent view for neural network quantization. arXiv preprint arXiv:1910.08237, 2019b.
Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square attack:
a query-efficient black-box adversarial attack via random search. In European Conference on
Computer Vision,pp. 484-501. Springer, 2020.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018.
Yu Bai, Yu-Xiang Wang, and Edo Liberty. Proxquant: Quantized neural networks via proximal
operators. ICLR, 2019.
Remi Bernhard, Pierre-Alain Moellic, and Jean-MaX Dutertre. Impact of low-bitwidth quantization
on the adversarial robustness for embedded neural networks. In 2019 International Conference on
Cyberworlds (CW), pp. 308-315. IEEE, 2019.
Wieland Brendel, Jonas Rauber, Matthias Kummerer, Ivan Ustyuzhaninov, and Matthias Bethge.
Accurate, reliable and fast robustness evaluation. In Advances in Neural Information Processing
Systems, pp. 12861-12871, 2019.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In IEEE
Symposium on Security and Privacy, 2017.
Anirban Chakraborty, Manaar Alam, Vishal Dey, Anupam Chattopadhyay, and Debdeep Mukhopad-
hyay. Adversarial attacks and defences: A survey. arXiv preprint arXiv:1810.00069, 2018.
Matthieu CourbariauX, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural
networks with binary weights during propagations. NeurIPS, 2015.
Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble
of diverse parameter-free attacks. ICML, 2020.
Chris Finlay, Aram-AleXandre Pooladian, and Adam Oberman. The logbarrier adversarial attack:
making effective use of decision boundary information. In Proceedings of the IEEE International
Conference on Computer Vision, pp. 4862-4870, 2019.
Angus Galloway, Graham W. Taylor, and Medhat Moussa. Attacking binarized neural networks. In
International Conference on Learning Representations, 2018.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256, 2010.
Ian Goodfellow. Gradient masking causes clever to overestimate adversarial perturbation size. arXiv
preprint arXiv:1804.07870, 2018.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. EXplaining and harnessing adversarial
eXamples. arXiv preprint arXiv:1412.6572, 2014.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pp. 1321-1330. JMLR. org, 2017.
Itay Hubara, Matthieu CourbariauX, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized
neural networks: Training neural networks with low precision weights and activations. JMLR,
2017.
Elias B Khalil, Amrita Gupta, and Bistra Dilkina. Combinatorial attacks on binarized neural networks.
In International Conference on Learning Representations, 2019.
Ji Lin, Chuang Gan, and Song Han. Defensive quantization: When efficiency meets robustness. In
International Conference on Learning Representations, 2019.
10
Under review as a conference paper at ICLR 2021
Yao Lu, Stephen Gould, and Thalaiyasingam Ajanthan. Bidirectional self-normalizing neural
networks. arXiv preprint arXiv:2006.12169, 2020.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Mark D McDonnell. Training wide residual networks for deployment using a single bit for each
weight. ICLR, 2018.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and
accurate method to fool deep neural networks. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2574-2582, 2016.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, and Pascal Frossard. Ro-
bustness via curvature regularization, and vice versa. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 9078-9086, 2019.
Maria-Irina Nicolae, Mathieu Sinn, Minh Ngoc Tran, Beat Buesser, Ambrish Rawat, Martin Wistuba,
Valentina Zantedeschi, Nathalie Baracaldo, Bryant Chen, Heiko Ludwig, et al. Adversarial
robustness toolbox v1. 0.0. arXiv preprint arXiv:1807.01069, 2018.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
PyTorch. 2017.
Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep
learning through dynamical isometry: theory and practice. In Advances in neural information
processing systems, pp. 4785-4795, 2017.
Chongli Qin, James Martens, Sven Gowal, Dilip Krishnan, Krishnamurthy Dvijotham, Alhussein
Fawzi, Soham De, Robert Stanforth, and Pushmeet Kohli. Adversarial robustness through local
linearization. In Advances in Neural Information Processing Systems, pp. 13824-13833, 2019.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet
classification using binary convolutional neural networks. ECCV, 2016.
Jonas Rauber, Wieland Brendel, and Matthias Bethge. Foolbox: A python toolbox to benchmark the
robustness of machine learning models. arXiv preprint arXiv:1707.04131, 2017.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics
of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
Sanchari Sen, Balaraman Ravindran, and Anand Raghunathan. Empir: Ensembles of mixed precision
deep networks for increased robustness against adversarial attacks. In International Conference on
Learning Representations, 2020.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning
Representations, 2014.
Florian TramE, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick Mc-
Daniel. Ensemble adversarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204,
2017.
Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to
adversarial example defenses. arXiv preprint arXiv:2002.08347, 2020.
Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh, and
Luca Daniel. Evaluating the robustness of neural networks: An extreme value theory approach. In
International Conference on Learning Representations, 2018.
Jiwei Yang, Xu Shen, Jun Xing, Xinmei Tian, Houqiang Li, Bing Deng, Jianqiang Huang, and
Xian-sheng Hua. Quantization networks. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 7308-7316, 2019.
11
Under review as a conference paper at ICLR 2021
Zhewei Yao, Amir Gholami, Qi Lei, Kurt Keutzer, and Michael W Mahoney. Hessian-based analysis
of large batch training and robustness to adversaries. In Advances in Neural Information Processing
Systems,pp. 4949-4959, 2018.
Zhewei Yao, Amir Gholami, Peng Xu, Kurt Keutzer, and Michael W Mahoney. Trust region based
adversarial attack on neural networks. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 11350-11359, 2019.
Appendices
Here, we first provide the pseudocodes, proof of the proposition and the derivation of Hessian. Later
we give additional experiments, analysis and the details of our experimental setting.
A Pseudocode
We provide pseudocode for pgd++ with njs in Algorithm 1 and pgd++ with hns in Algorithm 2.
Algorithm 1 PGD++ with NJS with L∞, T iterations, radius e, step size η, network fw*, input X0, label k, one-hot y ∈ {0, 1}d, gradient threshold ρ.		
Require: T, , η, ρ, X0 , y, k Ensure: kXT+1 — X0 k∞ ≤		
1	βι = (Md)/(P3 Pd=I μj (Ji))	. β1 computed using Network Jacobian.
2	: X1 = P∞ (X0 + Uniform(—1, 1))	. Random Initialization with Projection
3	for t - 1,... T do	
4	:	β2 = 1.0	
5	:	p0 = softmax(β1 (fw* (Xt)))	
6	:	if 1 — p0k ≤ ρ then	.ρ=0.01
7	:	β2 = — log(ρ∕(d — 1)(1 — ρ))∕γ	. γ computed using Proposition 2
8	' =—yT log(SoftmaX(β2β1(fw*(xt))))	
9	xt+1 = P∞ (Xt + η Sign(Vχ'(xt)))	. Update Step with Projection
Algorithm 2 PGD++ with HNS with L∞, T iterations, radius e, step size η, network fw*, input
x0, label k, one-hot y ∈ {0, 1}d, gradient threshold ρ.
Require: T, , η, x0 , y, k
Ensure: kxT +1 - x0 k∞ ≤
1:	x1 = P∞ (x0 + Uniform(-1, 1))	. Random Initialization with Projection
2:	β* = argmaxβ>o ∣∣∂2'(β)∕∂(x0)2∣∣f	. Grid Search
3:	for t - 1,... T do
4:	' = —yτ log(softmax(β* (fw* (xt))))
5:	xt+1 = P∞(Xt + ηsign(Vχ'(xt)))	. Update Step with Projection
B Derivations
B.1 DERIVING β GIVEN A LOWERBOUND ON 1 — pk (β)
Proposition 2. Let aK ∈ IRd with d > 1 and a1K ≥ a2K ≥ . . . ≥ adK and a1K — adK = γ .
For a given 0 < ρ < (d — 1)/d, there exists a β > 0 such that 1 — softmax(β a1K) > ρ, then
β< —log(ρ∕(d — 1)(1—ρ))∕γ.
12
Under review as a conference paper at ICLR 2021
Proof. Assuming a1K - adK = γ, we derive a condition on β such that
1 - softmax(β a1K) > ρ.
1 - softmax(β a1K) > ρ ,	(9)
softmax(β a1K) < 1 - ρ ,
d
exp(βa1K)/ X exp(βaλK) < 1 - ρ ,
λ=1
d
1/(1 + X exp(β(aK - aK))) < 1- ρ.
λ=2
Since, a1K - aλK ≤ γ for all λ > 1,
dd
1/(1 + Xexp(β(aK - aK))) ≤ 1/(1 + Xexp(-βγ)) .	(10)
λ=2	λ=2
Therefore, to ensure 1/(1 + Pλ=2 exp(β(aK - aK))) < 1 一 ρ,we consider,
d
1/(1 + ^X exp(-βγ)) < 1 - ρ,	aK - aK ≤ Y for all λ > 1 ,	(11)
λ=2
1/(1 + (d - 1) eχp(-βγ)) < 1 - ρ,
exp(-βγ) > PKd - 1)(1 - P),
-βγ > log(p/(d - 1)(1 - ρ)) , exp is monotone ,
β < -Iog(P/(d - 1)(1 - P))∕γ.
Therefore for any β < - log(p/(d - 1)(1 - ρ)"γ, the above inequality
1 - Softmax(βaK) > ρ is satisfied.	□
B.2 Derivation of Hessian
We now derive the Hessian of the input mentioned in Eq. (8) of the paper. The input gradients can be
written as:
∂'(β)
∂x0
∂'(β) ∂p(β) βj
∂p(β) ∂aK(β)
ψ(β)βJ .
(12)
Now by product rule of differentiation, input hessian can be written as:
∂2'(β) ∂ (x0)2	β	∂J ψ(β) /	+	∂ψ(β)∖T J ∂x0 J	,		
	β	ψ(β) ∂XJo	+	∂p(β)∖T J ∂x0 J	,	ψ(β) =	-(y - p(β))T
	β	ψ(β) ∂XJ0	+β	(率 J)T J# ∖ ∂aK	)		.	
(13)
C	Additional Experiments
In this section we first provide more experimental details and then some ablation studies.
C.1 Experimental Details
13
Under review as a conference paper at ICLR 2021
Method	ReSNet-18				VGG-16			
	APGD	Square Attack	PGD NJS	++ HNS	APGD	Square Attack	PG NJS	++ HNS
REF	0.00	0.55	0.00	0.00	0.79	2.25	0.00	0.00
BNN-WQ	0.00	0.41	0.00	0.00	8.23	1.98	0.00	0.00
BNN-WAQ	6.32	21.45	0.03	0.04	0.38	16.67	0.01	0.02
Table 7: Adversarial accuracy for REF, BNN-WQ and BNN-WAQ trained on CIFAR-10 using ResNet-18.
Both our NJS and HNS variants consistently outperform Auto-PGD (APGD) (Croce & Hein (2020))
performed using Difference of Logits Ratio (DLR) loss and a gradient free attack namely, Square
Attack (Andriushchenko et al. (2020)) under L∞ bound (8/255).
DataSet	Attack	€	η	T
	FGSM	8	8	1
CIFAR-10	PGD (L∞)	8	2	20
	PGD (L2)	120	15	20
	FGSM	4	4	1
CIFAR-100	PGD (L∞)	4	1	10
	PGD (L2)	60	15	10
Table 6: Attack parameters ( & η in pixels).
We first mention the hyperparameters used to perform
fgsm and pgd attack for all the experiments in the
paper in Table 6. To make a fair comparison, we keep
the attack parameters same for our proposed variants
of fgsm++ and pgd++ attacks. For pgd++ with hns
variant, we maximize Frobenius norm of Hessian with
respect to the input as specified in Eq. (8) of the pa-
per by grid search for the optimum β. We would like
to point out that since only ψ(β) and p(β) terms are
dependent on β, we do not need to do forward and
backward pass of the network multiple times during the grid search. This significantly reduces the
computational overhead during the grid search. We can simply use the same network outputs aK
and network jacobian J (as computed without using β) for the grid search, while computing the
other terms at each iteration of grid search. We apply grid search to find the optimum beta between
100 equally spaced intervals of β starting from β1 to β2 . Here, β1 and β2 are computed based on
Proposition 1 in the paper where ρ = 1e - 72 and ρ = 1 - (1/d) - (1e - 2) respectively, where d is
number of classes and γ = a1K - a2K so that 1 - softmax(β a1K) < ρ. Also, note that we estimate
the optimum β for each test sample only at the start of the first iteration of an iterative attack and then
use the same β for the next iterations.
Computational Overhead of njs and hns. Our Jacobian calculation takes just a single backward
pass through the network and thus adds a negligible overhead. Our njs approach for scaling estimates
β as inverse of mean JSV using 100 random test samples, which is similar to 100 backward passes.
For HNS, in Eq. (8) Jacobian J can be computed in single backward pass. Moreover, for piecewise
linear networks (eg, relu activations), ∂J∕∂x0 = 0 almost everywhere (Yao et al. (2018)). Thus
pgd++ with njs and hns is almost as efficient as pgd.
C.2 Comparisons against Auto-pgd attack and gradient free attack
We also compared our proposed pgd++ variants against recently proposed Auto-pgd (apgd) with
Difference of Logits Ratio (dlr) loss (Croce & Hein (2020)) and gradient free Square Attack (An-
driushchenko et al. (2020)) on different networks trained using ResNet-18 and VGG-16 on CIFAR-10
dataset and the results are reported in Table 7. The attack parameters for this experiment are the same
as reported in the paper. It can be clearly seen that our proposed variants perform much better than
both apgd with dlr loss and Square Attack, consistently achieving 0% adversarial accuracy. Infact,
much computationally expensive Square attack is unable to achieve 0% adversarial accuracy in any
of the cases under the enforced L∞ bound.
C.3 Other Experiments
We provide adversarial accuracy comparisons for different attack methods on CIFAR-100 using ResNet-
18, VGG-16, ResNet-50 and DenseNet-121 in Table 8. Again similar to the results in the paper, our
proposed pgd++ and fgsm++ outperform original form of pgd and fgsm consistently in all the
experiments on floating point networks. We also provide adversarial accuracy comparison of our
proposed variants against stronger attacks namely DeepFool (Moosavi-Dezfooli et al. (2016)) and
14
Under review as a conference paper at ICLR 2021
Network	Adversarial Accuracy (%)								
	FGSM	fgsm++		PGD (L∞)	PGD++(L∞)		PGD (L2)	pgd++ (L2)	
		NJS	HNS						
					NJS	HNS		NJS	HNS
ResNet-18	9.06	9.23	2.70	0.14	0.14	0.00	5.38	0.17	0.15
VGG-16	16.28	17.24	9.19	1.53	0.95	0.25	4.87	1.50	1.38
ResNet-50	12.95	12.95	11.94	0.12	0.00	0.00	31.01	4.43	4.14
DenseNet-121	11.41	11.41	10.74	0.00	0.00	0.00	6.10	3.09	2.76
Table 8: Adversarial accuracy on the test set of CIFAR-100 for REF (floating point networks). Both our
NJS and HNS variants consistently outperform original FGSM and PGD (L∞∕L2 bounded) attacks.
Network	PGD	Deep Fool	BBA	pgd++	
				NJS	HNS
ResNet-18	8.57	18.92	0.81	0.03	0.04
VGG-16	78.01	12.12	0.10	0.01	0.02
Table 9: Adversarial accuracy on the test set of CIFAR-10 for BNN-WAQ. Here, we compare our
proposed variants against much stronger attacks namely DeepFool (Moosavi-Dezfooli et al. (2016))
and BBA (Brendel et al. (2019)). Both our variants outperform stronger attacks. Note, DeepFool and
BBA are much slower in practise requiring 100-1000 iterations. BBA specifically requires even an
adversarial start point that needs to be computed using another adversarial attack.
bba (Brendel et al. (2019)) on bnn-waq trained on CIFAR-10 dataset in Table 9. In this experiment,
our proposed variants again outperform even the stronger attacks which take 100-1000 iterations with
adversarial start point (instead of random initial perturbation). It should be noted that although bba
performs much better than DeepFool and pgd, it still has inferior success rate than ours considering
the fact that it takes multiple hours to run bba whereas our proposed variants are almost as efficient
as pgd attack.
Step Size Tuning for pgd attack. We would like to point out that step size η and temperature scale
β have different effects in the attacks performed. Notice, PGD and FGSM attack under L∞ bound only
use the sign of input gradients in each gradient ascent step. Thus, if the input gradients are completely
saturated (which is the case for bnns), original forms of pgd or fgsm will not work irrespective of
the step size used. To illustrate this, we performed extensive step size tuning for original form of pgd
attack on different ResNet-18 models trained on CIFAR-10 dataset and the adversarial accuracies are
reported in Fig. 4. It can be observed clearly that although tuning the step size lowers adversarial
accuracy a bit in some cases but still cannot reach zero for bnns unlike our proposed variants.
Adversarial training using pgd++. We also investigate
the potential application of pgd++ for adversarial training
to improve the robustness of neural networks. pgd++
attack is most effective when applied to a network with
poor signal propagation. However, adversarial training is
performed from random initialization (Glorot & Bengio
(2010)) exhibiting good signal propagation. Thus, pgd
and pgd++ perform similarly for adversarial training. We
infer these conclusions from our experiments on adversarial
training using pgd++.
clever Scores. Recently CLEVER Scores (Weng et al.
(2018)) have been proposed as an empirical estimate to
measure robustness lower bounds for deep networks. It
has been later shown that gradient masking issues cause
60 Varying PGD Attack Step Size
E	REF
ɑ 50
D	BNN-WQ
∕40	——BNN-WAQ
Attack Step Size
Figure 4: Adversarial accuracy using
PGD attack under L∞ bound (8/255)
with varying step size (η) on ResNet-18
trained on CIFAR-10. Notice, PGD at-
tack is unable to reach zero adversarial
accuacy for BNNs with any step size.
clever to overestimate the robustness bounds (Goodfellow (2018)). Here we try to improve the
CLEVER scores using different ways of choosing β in temperature scaling. For this experiment,
we use clever implementation of Adversarial Training Toolbox2 (Nicolae et al. (2018)). We set
2https://github.com/Trusted-AI/adversarial-robustness-toolbox
15
Under review as a conference paper at ICLR 2021
I Original ∣ Heuristic			NJS	HNS
bnn-wq	0.8585	0.8845	0.4139	0.3450
bnn-waq	0.7239	3.1578	0.3120	0.2774
Table 10: CLEVER Scores (Weng et al. (2018)) for BNN-WQ and BNN-WAQ trained on CIFAR-10 using
ResNet-18. We compare CLEVER Scores returned for L1 norm perturbation using different ways of
temperature scaling applied. Here, Original refers to original network without temperature scaling
and Heuristic denotes temperature scale with small β = 0.01.
Methods	pgd++ (njs) - Varying ρ					
	1e-05	1e - 04	1e - 03	1e - 02 I	1e- 01	2e - 01
REF	0.00	0.00	0.00	0.00	0.00	0.00
BNN-WQ	0.00	0.00	0.00	0.00	0.00	0.00
BNN-WAQ	0.15	0.08	0.04	0.03	0.04	0.02
Table 11: Adversarial accuracy on the test set for binary neural networks using L∞ bounded PGD++
attack using NJS with varying ρ. For different values of ρ, our approach is quite stable.
number of batches to 50, batch size to 10, radius to 5, and chose L1 norm as hyperparameters (based
on the Weng et al. (2018)). We compare our variants namely njs and hns against heuristic choice
of small β = 0.01 and original CLEVER Scores for BNN-WQ and BNN-WAQ (trained on CIFAR-10
using ResNet-18) in Table 10. It can be clearly seen that our proposed variants improve the robustness
bounds computed using CLEVER whereas a heuristic choice of β = 0.01 performs even worse.
C.4 STABILITY OF PGD++ WITH NJS WITH VARIATIONS IN ρ
We perform ablation studies with varying ρ for PGD++ with NJS in Table 11 for CIFAR-10 dataset
using ResNet-18 architecture. It clearly illustrates that our NJS variant is quite robust to the choice of
ρ as we are able to achieve near perfect success rate with PGD++ with different values of ρ. As long
as value of ρ is large enough to avoid one-hot encoding on softmax outputs (in turn avoid kψ(β)k to
be zero) of correctly classified sample, our approach with njs variant is quite stable.
C.5 Signal Propagation and Input Gradient Analysis using njs and hns
We first provide an example illustration in Fig. 5 to better understand how the input gradient norm
i.e., k∂'(β)∕∂x0k2, and norm of sign of input gradient, i.e., ∣∣sign(∂'(β)∕∂x0)k2 is influenced by
β . It clearly shows that both the plots have a concave behavior where an optimal β can maximize
the input gradient. Also, it can be quite evidently seen in Fig. 5 (b) that within an optimal range of
β, gradient vanishing issue can be avoided. If β → 0 or β → ∞, it changes all the values in input
gradient matrix to zero and inturn ∣∣sign(∂'(β)∕∂x0)k2 = 0.
We also provide the signal propagation properties as well as analysis on input gradient norm before
and after using the β estimated based on NJS and HNS in Table 12. For binarized networks as well
floating point networks tested on CIFAR-10 dataset using ResNet-18 architecture, our HNS and NJS
variants result in larger values for ∣∣ψ∣2, ∣∂'(β)∕∂x0∣∣2 and ∣∣sign(∂'(β)∕∂x0)k2. This reflects the
efficacy of our method in overcoming the gradient vanishing issue. It can be also noted that our
variants also improves the signal propagation of the networks by bringing the mean jsv values closer
to 1.
C.6 ABLATION FOR ρ VS. PGD++ ACCURACY
In this subsection, we provide the analysis on the effect of bounding the gradients of the network
output of ground truth class k, i.e. ∂'(β)/∂aK. Here, We compute β using Proposition 1 for all
correctly classified images such that 1 - softmax(βakK) > ρ with different values of ρ and report
the pgd++ adversarial accuracy in Table 13. It can be observed that there is an optimum value of
ρ at Which PGD++ success rate is maximized, especially on the adversarially trained models. This
can also be seen in connection With the non-linearity of the netWork Where at an optimum value of
16
Under review as a conference paper at ICLR 2021
=OXe4--
0.0
0.00	0.05	0.10
0.15	0.20	0.25	0.30
Figure 5: Plots to show how variation in β affects (a) norm of input gradient, i.e., k∂'(β)∕∂x0∣∣2,
(b) norm of sign of input gradient, i.e., k sign (∂'(β )∕∂ x0)k2 on a random correctly classified image.
0	1	2	3	4	5
Notice that, both input gradient and signed input gradient norm behave similarly, showing a concave
behaviour. This plot is computed for BNN-WQ network on CIFAR-10, ResNet-18. (b) clearly illustrates
how optimum β can avoid vanishing gradient issue since ∣∣sign(∂'(β)∕∂x0)k2 will only be zero if
input gradient matrix has only zeros.
Methods	I REF		Adv. Train	bnn-wq	bnn-waq
	Orig.	8.09e+00	5.15e-01	3.53e+01	1.11e+00
jsv (Mean)	NJS	9.51e-01	5.70e-01	9.95e-01	2.24e-01
	HNS	2.38e+00	6.11e+00	1.19e+01	4.65e+00
	Orig.	6.27e+00	4.10e-01	3.53e+01	1.97e+00
jsv (Std.)	NJS	7.58e-01	6.34e-01	9.71e-01	6.73e-01
	HNS	4.41e+00	5.34e+02	2.13e+02	1.24e+02
	Orig.	9.08e-03	2.33e-01	6.20e-03	9.46e-03
kψk2	NJS	4.66e-01	2.35e-01	5.37e-01	1.20e-01
	HNS	1.48e-01	2.57e-01	2.07e-01	2.44e-01
	Orig.	2.42e-01	8.52e-02	2.27e-01	6.33e-02
k∂'∕∂χ0k2	NJS	9.52e-01	1.10e-01	8.91e-01	1.24e-01
	HNS	7.49e-01	8.18e-01	3.70e-01	2.70e-01
	Orig.	5.55e+01	5.54e+01	4.39e+01	5.55e+01
ksign( ∂X'0 )k2	NJS	5.55e+01	5.54e+01	5.55e+01	5.55e+01
	HNS	5.55e+01	5.54e+01	5.55e+01	5.55e+01
Table 12: Mean and standard deviation of Jacobian Singular Values (JSV), mean ∣ψ ∣2, mean
∣∣∂'∕∂x0k2 and mean ∣∣sign(∂'∕∂x0)∣∣2 for different methods on CIFAR-10 with ReSNet-18 computed
with 500 correctly classified samples. Note here for NJS and HNS, JSV is computed for scaled jacobian
i.e. βJ. Also note that, values of ∣∣ψ∣2, k∂'(β)∕∂x0∣2 and ∣∣sign(∂'(β)∕∂x0)k2 are largerfor our
NJS and HNS variant (for most of the networks) as compared with network with no β, which clearly
indicates better gradients for performing gradient based attacks.
β, even for robust (locally linear) (Moosavi-Dezfooli et al. (2019); Qin et al. (2019)) networks such
as adversarially trained models, non-linearity can be maximized and better success rate for gradient
based attacks can be achieved. Our hns variant essentially tries to achieve the same objective while
trying to estimate β for each example.
17
Under review as a conference paper at ICLR 2021
Methods	PGD++ with Varying P
	1e - 15 1e - 09	1e - 05	1e - 01 2e - 01	5e - 01
	 REF bnn-wq	0.00	0.00	0.00	0.00	0.00	0.00 9.61	0.04	0.00	0.00	0.00	0.00
REF* BNN-WQ*	48.18	47.66	48.00	53.09	54.58	57.57 40.66	40.01	40.04	45.09	46.57	49.72
Table 13: Adversarial accuracy on the test set for adversarially trained networks and binary neural
networks using L∞ bounded PGD++ attack with varying ρ as lower bound on the gradient of network
output for ground truth class k. Here * denotes the adversarially trained models obtained where
adversarial samples are generated using L∞ bounded PGD attack with with T = 7 iterations, η = 2
and E = 8. Note, here PGD++ attack refers to PGD attack where ∂'(β)∕∂aK is bounded by P for
each sample, where k is ground truth class.
18