Under review as a conference paper at ICLR 2021
Stochastic Optimization with Non-stationary
Noise: The Power of Moment Estimation
Anonymous authors
Paper under double-blind review
Ab stract
We investigate stochastic optimization under weaker assumptions on the distribu-
tion of noise than those used in usual analysis. Our assumptions are motivated by
empirical observations in training neural networks. In particular, standard results
on optimal convergence rates for stochastic optimization assume either there exists
a uniform bound on the moments of the gradient noise, or that the noise decays as
the algorithm progresses. These assumptions do not match the empirical behavior
of optimization algorithms used in neural network training where the noise level in
stochastic gradients could even increase with time. We address this nonstationary
behavior of noise by analyzing convergence rates of stochastic gradient methods
subject to changing second moment (or variance) of the stochastic oracle. When
the noise variation is known, we show that it is always beneficial to adapt the
step-size and exploit the noise variability. When the noise statistics are unknown,
we obtain similar improvements by developing an online estimator of the noise
level, thereby recovering close variants of RMSProp (Tieleman and Hinton, 2012).
Consequently, our results reveal why adaptive step size methods can outperform
SGD, while still enjoying theoretical guarantees.
1	Introduction
Stochastic gradient descent (SGD) is one of the most popular optimization methods in machine
learning because of its computational efficiency compared to traditional full gradient methods. Great
progress has been made in understanding the performance of SGD under different smoothness and
convexity conditions (Agarwal et al., 2009; Arjevani et al., 2019; Drori and Shamir, 2019; Ghadimi
and Lan, 2012; 2013; Nemirovsky and Yudin, 1983; Rakhlin et al., 2012). These results show that
with a fixed step size, SGD can achieve the minimax optimal convergence rate for both convex and
nonconvex optimization problems, provided the gradient noise is uniformly bounded.
Yet, despite the theoretical minimax optimality of SGD, adaptive gradient methods (Duchi et al.,
2011; Kingma and Ba, 2014; Tieleman and Hinton, 2012) have become the methods of choice for
training deep neural networks, and have received a surge of attention recently (Agarwal et al., 2018;
Chen et al., 2019; Huang et al., 2019; Levy, 2017; Levy et al., 2018; Li and Orabona, 2019; Liu et al.,
2019; 2020; Ma and Yarats, 2019; Staib et al., 2019; Ward et al., 2019; Zhang et al., 2019; 2020;
Zhou et al., 2018; 2019; Zou and Shen, 2018; Zou et al., 2019). Instead of using fixed stepsizes, these
methods construct their stepsizes adaptively using the current and past gradients. Despite advances in
the literature on adaptivity, theoretical understanding of the benefits of adaptation is very limited.
We provide a different perspective on understanding the benefits of adaptivity by considering it in the
context of non-stationary gradient noise, i.e., the noise intensity varies with iterations. Surprisingly,
this setting is rarely studied, even for SGD. To our knowledge, this paper is the first work to formally
study stochastic gradient methods in this varying noise scenario. Our main goal is to show that:
Adaptive step-sizes can guarantee faster rates than SGD when the noise is non-stationary.
We focus on this goal based on several empirical observations (Section 2), which lead us to model the
noise of stochastic gradient oracles via the following iteration dependent quantities:
mk ：= E[kg(xk)k2],	or	σ2 := E[∣∣g(xk) - Vf (xfc)k2],
(1)
where g(xk) is the stochastic gradient and Vf(xk) the true gradient at iteration k. Notation (1)
provides more fine-grained description of noise behavior than uniform bounds on the variance by
1
Under review as a conference paper at ICLR 2021
(a) ResNet18 on Cifar10
(b) AWD-LSTM on PTB (c) Transformer on En-De translation
Figure 1: We empirically evaluate the second moment (in blue) and variance (in orange) of stochastic
gradients during the training of neural networks. We observe that the magnitude of these quantities
changes significantly as iteration count increases, ranging from 10 times (ResNet) to 106 times
(Transformer). This phenomenon motivates us to consider a setting with non-stationary noise.
permitting iteration dependent noise intensities. It is intuitive that one should prefer smaller stepsizes
when the noise is large and vice versa. Thus, under non-stationarity, an ideal algorithm should adapt
its stepsize according to the parameters mk or σk, suggesting a potential benefit of adaptive stepsizes.
Contributions. The primary contribution of our paper is to show that a stochastic optimization
method with adaptive stepsize can achieve a faster rate of convergence (by a factor that is polynomial-
in-T) than fixed-step SGD. We first analyze an idealized setting where the noise intensities are
known, using it to illustrate how to select noise dependent stepsizes that are provably more effective
(Theorem 1). Next, we study the case with unknown noise, where we show under an appropriate
smoothness assumption on the noise variation that a variant of RMSProp (Tieleman and Hinton, 2012)
can achieve the idealized convergence rate (Theorem 3). Remarkably, this variant does not require
the noise levels. Finally, we generalize our results to nonconvex settings (Theorems 12 and 14).
2	M otivating observation: nonstationary noise in deep learning
Neural network training involves optimizing an empirical risk minimization problem of the form
minx f (x) := n1 pn=1 fi(x), where each f represents the loss function with respect to the i-th data
or minibatch. Stochastic methods optimize this objective randomly sampling an incremental gradient
Vfi at each iteration and using it as an unbiased estimate of the full gradient. The noise intensity of
this stochastic gradient is measured by its second moments or variances, defined as,
1.	Second moment: m1 2(x) = 1 ɪɪi ∣∣Vfi(x)k2;
2.	Variance: σ2(x) = ɪ P2ι ∣∣Vfi(x) 一 Vf (x)∣2, where Vf (x) is the full gradient,
To illustrate how these quantities evolve over iterations, we empirically evaluate them on three popular
tasks of neural network training: ResNet18 training on Cifar10 dataset for image classification1,
LSTM training on PTB dataset for language modelling2; transformer training on WMT16 en-de
for language translation3. The results are shown in Figure 1, where both the second moments and
variances are evaluated using the default training procedure of the original code.
On one hand, the variation of the second moment/variance has a very different shape in each of the
considered tasks. In the CIFAR experiment, the noise intensity is quite steady after the first iteration,
indicating a fast convergence of the training model. In LSTM training, the noise level increases and
converges to a threshold. While, in training Transformers, the noise level increases very fast at the
early epochs, then reaches a maximum, and turns down gradually.
On the other hand, the preferred optimization algorithms in these tasks are also different. For
CIFAR10, SGD with momentum is the most popular choice. While for language models, adaptive
methods such as Adam or RMSProp are the rule of thumb. This discrepancy is usually taken as
granted, based on empirical validation; and little theoretical understanding of it exists in the literature.
1Code source for CIFAR10 https://github.com/kuangliu/pytorch-cifar
2Code source for LSTM https://github.com/salesforce/awd-lstm-lm
3Code source for Transformer https://github.com/jadore801120/attention-is-all-you-need-pytorch
2
Under review as a conference paper at ICLR 2021
Based on the observations made in Figure 1, a natural candidate emerges to explain this discrepancy in
the choice of algorithms: the performance of stochastic algorithms varies according the characteristics
of gradient noise encountered during training. Despite this behavior, noise level modeling has drawn
surprisingly limited attention in prior art. Reference (Moulines and Bach, 2011) studies convergence
of SGD assuming each component function is convex and smooth; extensions to the variation of
the full covariance matrix are in (Gadat and Panloup, 2017). A more fine-grained stochastic oracle
assumes that the variances grow with the gradient norm as σ2 + Ck▽/(χ)k2, or grow with the
suboptimality σ2 + Ckx - x*∣∣2 (Bottou et al., 2018; Jofre and Thompson, 2019; Rosasco et al., 2019).
Unfortunately, these existing oracles fail to express the variation of noise observed in Figure 1. Indeed,
the norm of the full gradient, represented as the difference between the orange and the blue line, is
significantly smaller compared to the noise level. This suggests that noise variation is not due to
the gradient norm, but due to some implicit properties of the objective function. This observation
motivates us to introduce the following non-stationary noise oracle:
Definition 1 (non-stationary noise oracle). The stochasticity of the problem is governed by a
sequence of second moments {mk }k∈N or variances {σk }k∈N, such that, at the kth iteration, the
gradient oracle returns an unbiased gradient g(xk) such that E[g(xk)] = Vf (xk), and either
(a)	with second moment E[kg (xk)k2] = m2k; or
(b)	with variance E[kg (xk) - Vf(xk)k2] = σk2.
The non-stationary noise oracle is a relaxation of the standard uniform noise oracle in which case
mk or σk are constant. By introducing the time dependency, we aim to understand how the variation
of noise influences the convergence rate of optimization algorithms. An example that falls into this
category is when the noise is additive to the gradient, namely g(xk)〜Vf (χk) + N(0, σ2).
We emphasize that our goal is to demystify the correlation between the noise intensity and the
performance of optimization algorithm, instead of explaining why certain shape of noise occurs. In
general, the variation in noise is a consequence of the combination on data distribution, training
model and optimization method, which is complex and highly non trivial. We simplify it by assuming
that the noise intensity is decoupled from its location, meaning that, the parameters mk or σk only
depend on the iteration number k, but do not depend on the specific location where the gradient is
evaluated. This is empirically justified as the pattern of the noise is mostly determined by the task
instead of the optimization algorithms, see Appendix A. The simplification helps to focus on the
shape of noise, taking a first step towards the goal: characterize the convergence rate of adaptive
algorithms under non-stationary noise.
3	The benefit of adaptivity under nonstationary noise
In this section, we investigate the influence of nonstationary noise in an idealized setting where the
noise parameters mk are known. For brevity, we will first focus on the convex setting and present
our results based on the second moment parameters mk. We defer the discussion on nonconvex
problems and variants on variance parameters σk to Section 5. One reason that we prioritize the
second moment than the variance is to draw a connection with the well-known adaptive method
RMSProp (Tieleman and Hinton, 2012) and Adam (Kingma and Ba, 2014). One common feature
shared by both algorithms is that they scale the step sizes inversely to an exponential moving average
of estimated second moments. Below, we start from the idealized case assuming the second moments
are known and show why inverse scaling could speed up convergence.
Let f be convex and differentiable. We consider the problem minx f (x), where the gradient is given
by the nonstationary noise oracle defined in Definition 1. We assume that the optimum is attained at
x* and we denote f * the minimum of the objective. We are interested in studying the convergence
rate of a stochastic algorithm with update rule
xk+1 = xk - ηkg(xk),
(2)
where ηk are stepsizes that are oblivious of the iterates {xk}k∈N.
Theorem 1. Under the second moment oracle given in Definition 1 (a), the weighted average
XT = (PT=I ηk xk)/(PT=I ηk) obtained by the update rule (2) satisfies the SuboPtimality bound
E[f (Xτ) - f *] ≤ kx1-x*k2τ+ PT=I η2mk
k=1 ηk
(3)
3
Under review as a conference paper at ICLR 2021
Although the theorem follows from standard analysis, it leads to valuable results as explained below.
Corollary 2. Denote M = max m《and R = ∣∣xι 一 x*∣∣. We have thefollowing Convergence rate:
1.	SGD with constant stepsize: if ηk = η = / R G, then
kT=1 m2k
E[f (XT) 一 f *] ≤ 2R√pk=1~m = 2RM ∙ ʌ/ɪPMImk.	(constant baseline)
2.	SGD with idealized stepsize: if ηk = RR-, then
1
E[f(Xτ) 一 f *] ≤ PR7T^ = 2RM ∙ ι PM 上.	(idealized baseline)
Tk=I m	τ k=ι=m m
To facilitate comparison, We have normalized the convergence rates with respect to the conventional
rate 2RM∕√T (Nemirovski et al., 2009). In the standard setting, the values of mk are unavailable,
but the uniform bound M is known, in such a case taking mk = M recovers the standard result.
When the values of mk are given, both constant baseline and idealized baseline benefits from it,
improving upon the conventional rate 2RM∕√T. The improvement factor in constant baseline has
depends on the average of the second moments，P m /T , while as the improvement factor in
idealized baseline depends on the average of the harmonic sum T1 P 1/mk. In particular, from
Jensen’s inequality E[X]-2 ≤ E[X-2],wehave
(T Pk m1k厂2 ≤ T Pk mk,
implying that the idealized baseline is always better than the constant baseline. This result is rather
expected, as the stepsizes are adapted to the noise intensity.
As a consequence, the accumulations of the parameters mk in different forms governed the conver-
gence rate. To further illustrate such difference, we consider an illustrative synthetic noise model,
mimicking the shape of noise we observed in the training of Transformer (see Figure 1(c)).
Example 1. Consider thefollowing piece-wise linear noise model with Y = 5(1 — T-α)∕T.
1
T a
γ(k — 2T ) + 1
mk =	1
Y( 3T — k) + 1
1
、Ta
if k ∈ [1, T]；
if k ∈ (2T, 3T]；
if k ∈ (3T, 4T]；
if k ∈ (4T,T].
if k ∈ (5，2T]；
In this example, the maximum noise intensity M is 1, and the minimum intensity is 1/Tα, inducing
a large ratio of order Ta. Following the bounds developed in Corollary 2, the performance of the
constant baseline maintains the standard O(1∕√T) convergence rate, while as the idealized baseline
converges at O(1∕T2 +α). Hence a nontrivial acceleration of order Tɑ is obtained by using the
idealized stepsize, and this acceleration can be arbitrarily large as α increases.
This example is encouraging, showing that the speedup due to adaptive stepsizes can be polynomial
in the number of iterations, especially when the ratio between the maximum and the minimum
noise intensity is large. However, explicit knowledge on mk is required to implement these idealized
stepsizes, which is unrealistic. The next sections of the paper demonstrates that estimating the moment
bound in an online fashion can achieve a convergence rate comparable to the idealized setting.
4 Adaptive methods: Online estimation of moments
From now on, we assume that the moment bounds mk are not given. To address the non-stationarity,
we estimate the noise intensity based on an exponential moving average, a technique commonly used
in adaptive methods. More precisely, the moment estimator mk is constructed recursively as
m k+1 = βmk + (1 — β)∣gk k2,	(ExpMvAvg)
4
Under review as a conference paper at ICLR 2021
Algorithm 1 Adaptive SGD (x1 , T, c, m)
1:	Initialize m2 = kg(xι)k2.
2:	for k = 1, 2, ..., T do
3:	Evaluate stochastic gradient gk at xk .
4:	xk+1 = Xk ― ηkgk with ηk = mk+m ∙
5：	m k+ι = βmk + (1 - β)kgk Il2.
6:	end for
7： return XT = (PT=ι ηixi"(P2ι m).
where gk is the k-th stochastic gradient and β is the decay paramter. Then we choose the stepsizes
inversely proportional to rhk+ι, leading to Algorithm 1.
Algorithm 1 could be viewed as a “norm" version of RMSProp (Tieleman and Hinton, 2012): the
exponential moving average is performed coordinate-wise in RMSProp, whereas we use the full
norm of gk to update the moment estimator Tmk+ι. Such a simplification via a full norm variant has
also been analyzed in the uniformly bounded noise setting (Levy, 2017; Levy et al., 2018; Li and
Orabona, 2019; Ward et al., 2019)—we leave investigation of the more advanced coordinate-wise
version as a topic of future research.
Another important component of the stepsize is the correction constant m, the term appearing in the
denominator. This constant provides a safety threshold when Tmk underestimates mk, which is com-
monly used in the practical implementation of adaptive methods, and even beyond, in reinforcement
learning as so-called exploration bonus (Azar et al., 2017; Jin et al., 2018a; Strehl and Littman, 2008).
To show the convergence of the algorithm, we need to impose a regularity assumption on the sequence
of noise intensities. Otherwise, previous estimate may not provide any information of the next one.
Assumption 1. We assume that an upper bound M on mk is given, i.e. maxk mk ≤ M such that
(a)	The fourth moment of gk is bounded by M4, namely, E[(Igk I2 - m2k)2] ≤ M4, ∀k.
(b)	The total variation on mk is bounded, namely Ek |mk - mk+/ ≤ D2 with D2 = Ω(M2).
The bounded fourth moment ensures concentration of IgkI2, which is necessary to guarantee the
quality of the online estimator. In particular, this assumption is satisfied when Igk I follows a mk
sub-Gaussian distribution. Though stronger than bounded variance, this assumption does not lead
to better convergence rates for SGD, as many existing results (including lower bounds) require
sub-Gaussian or bounded noise, see Agarwal et al. (2009); Ghadimi and Lan (2013) .
The bounded total variation assumption should be viewed as a regularity condition on the sequence
of noise. It is motivated and common in the dynamic online learning literature (Besbes et al., 2014;
Jadbabaie et al., 2015; Mokhtari et al., 2016). A key aspect of it is to avoid infinite oscillation, such
as the pathological setting where m2k = 1 and m2k+1 = M, in which case the total variation scales
with the number of iterations T. The specific constant in D2 = Ω(M2) depends on the shape of the
noise. When mk is increasing in the first half and decreasing in the second half, as in the Transformer
experiments and Example 1, the total variation is bounded by D2 ≤ 2M2 . More generally, if the
noise can be decomposed into K piece-wise monotone fragments, then the bound D2 ≤ KM2 holds.
With the above assumptions, we are now ready to present our convergence result.
Theorem 3. Under Assumptions 1, with probability at least 1/2, the iterates generated by Algorithm 1
using parameters β = 1 — 2T-2/3, m = 4√D2 + M2T- 1 ln(T) 1, C = √T satisfy
32
f(xτ) - f * ≤ 2RM ∙ π——M-----------
T j t TT	1 Pτ ι
T 2_^k=1 mk + m
Remark 4. Our result directly implies a 1 - δ high probability style convergence rate, by restarting
it 2 log(1∕δ) times. An additional log(1∕δ) dependency will be introduced in the complexity, as in
standard high probability results Fang et al. (2018); Jin et al. (2018b); Nemirovski et al. (2009).
The key to prove the theorem is to effectively bound the estimation error 1mIk - m | relying on
concentration, and on bounded variation in Assumption 1. In particular, the choice of the decay
5
Under review as a conference paper at ICLR 2021
Constant
0 ≤ α ≤ 1 O (T- 2 )
9 <α O (T - 2 )
Adaptive
Idealized
Table 1: Comparison of the convergence rate under the noise example 1.
parameter β is critical, determining how fast the contribution of past gradients decays. Because of
the non-stationarity in noise, the online estimator mk is biased. The proposed choice of β carefully
balances the bias error and the variance error, leading to sublinear regret, see Appendix C.
Due to the correction constant m, the obtained convergence rate inversely depends on PT=I m+m,
instead of the idealized dependency PT=I 念.This additional term makes the comparison less
straightforward and we now discuss different scenarios for obtaining a better understanding.
4.1	Discussion of the convergence rate
To illustrate the difference between convergence rates, we first consider the synthetic noise model
introduced in Example 1. The detailed comparison is presented in Table 1, where we observe two
regimes regarding the exponent α:
•	When 0 ≤ α ≤ 9, the rate of the adaptive algorithm matches (idealized baseline) up to logarith-
mic dependency, and is Tα better than the (constant baseline).
•	When 1 ≤ α, the adaptive convergence rate no longer matches the (idealized baseline). Never-
theless, it is always T9 faster than the (constant baseline).
In both cases, the adaptive method achieves a non-trivial improvement, polynomial in T, compared
to the (constant baseline). Even though the improvement T1 might seem in-significant, it is the first
result showing a plausible non-trivial advantage of adaptive methods over SGD under nonstationary-
noise. Further, note that the adaptive convergence rate does not always match the (idealized baseline)
when α is large. Such a discrepancy comes from the correction term m, which makes the stepsize
more conservative than it should be, especially when mk is small.
The above comparison relies on the specific choice on the noise model given in Example 1. Now we
formalize some simple conditions allowing comparison in more general settings.
Corollary 5. Ifthe ratio M∕(mink mk) ≤ T 1, then adaptive method converges in the same order
as the (idealized baseline), up to logarithmic dependency.
This result is remarkable since the adaptive method does not require any knowledge of mk values,
and yet it achieves the idealized rate. In other words, the exponential moving average estimator
successfully adapts to the variation in noise, allowing faster convergence than constant stepsizes.
Corollary 6. Let 品^ = P mk/T be the average second moment. If M/mavg ≤ T 1, then
adaptive method is no slower than the (constant baseline), up to logarithmic dependency.
The condition in Corollary 6 is strictly weaker than the condition in Corollary 5, which means even
though an adaptive method may not match the idealized baseline, it could still be non-trivially better
than the constant baseline. This case happens e.g., when α > 9 in Table 1, where the adaptive method
is O(T 1) faster than the constant baseline. Indeed, O(T 1) is the maximum improvement one can
expect according to our current analysis.
Corollary 7. Recall that M is an upper bound on mk, i.e., max mk ≤ M. Therefore
1.	The convergence rate ofthe constant baseline is no slower than O(2RM∕VT).
11
2.	The convergence rate ofthe adaptive method is nofaster than O(2RM∕T 2 + 9).
The order of maximum improvement O(T9) is determined by the specific choice of m in Theorem 3,
1
which is chosen to be O(MT-9). Indeed, the correction term is helpful when the estimator mk
underestimates the true value mk, avoiding the singularity at zero. Hence, the choice of m is related
6
Under review as a conference paper at ICLR 2021
to the average deviation between mk and mk. Under a stronger concentration assumption, We can
strengthen the maximum improvement to O(T6), as shown in Appendix E.
The noise model in Example 1 provides a favorable scenario where the maximum improvement is
attained. However, in some scenarios, the convergence rate of an adaptive method can be slower than
the constant baseline.
Adversarial scenario. If mk = 1/Tɑ for all i ∈ [1,T] except at T/2 it takes the value m” = 1
with a > 1/9, then the convergence rate of both constant and idealized baselines are O(T-2), while
1-2α
the adaptive method only converges in O(T 厂).The subtle change at iteration T/2 amplifies the
exponential moving average estimator and requires a non-negligible period to get back to the constant
level. It is clear that the estimator becomes less meaningful under such a subtle change.
Overall, it is hard to provide a complete characterization of the variation in noise. In Corollary 5 and
6, we show that when the ratio between the maximum and the minimum/average second moment is
not growing too fast, adaptive methods do improve upon SGD.
5	Extensions of Thm 3
In this section, we discuss several extensions to Thm 3. The results are nontrivial but the analysis is
almost the same. Hence we defer the exact statements and proofs to appendices.
Addressing the variance oracle. So far, we have focused on the noise oracle based on the second
moment mk and made the connection with existing adaptive methods. However, there is some
unnaturalness underlying the non-stationary oracle on mk. Indeed, it is hard to argue that mk is
iterate independent since mk = σk2 + ∣∣Vf (Xk)k2. Even though the influence of ∣∣Vf (Xk)II2 might
be minor when the variance σk2 is high (e.g. as in Figure 1), it is still changing mk. In contrast, the
variance σk is an intrinsic quantity coming from the noise model, which could be iterate independent.
Hence the variance oracle is theoretically more sound. We now present the necessary modifications
in order to adapt to the variance oracle.
First, in order to estimate the variance, we need to query two stochastic gradients gk and gk0 at the
same iterate, then we construct the estimator following the recursion
σ2+1 = βσ2 + (I- β)kgk - gk k2.
Second, the smoothness condition on f is required, i.e., L-Lipschitzness on the gradient of f. In
this case, it is necessary to ensure that the step-size being not larger than 1/2L. This translates to an
additional constraint on the correcting constant m. More precisely, the stepsize is given by
ηk = ɪʒ—	with m ≥ 2cL.
ηk	σk+m
Note that the L-smoothness condition is not required in the second moment oracle. This is why the
second moment oracle is more suitable to nonsmooth setting (see Section 6.1 of Bubeck (2014)). A
complete algorithm for the variance oracle is provided in Algorithm 2. The convergence results are
essentially the same by replacing mk with σk, see Appendix H.
Extension to nonconvex setting. We also provide an extension of our analysis to non-convex
smooth setting. In which case, we characterize the convergence with respect to the gradient norm
∣Vf(Xk)∣2, i.e., convergence to a stationary point. The conclusions are very similar to the one in the
convex setting and the results (Theorems 12 and 14) are deferred to Appendix F.
Variants on stepsizes. To go beyond the second moment of noise, one could apply an estimator
of the form m 2十]=βm^ Ik + (1 — β )∣gk ∣∣p when the p-th moment of the gradient is bounded. This
allows stepsize of the shape ηk 8 1/(mp + mp)1/p as in ADAM, Adamax Kingma and Ba (2014).
6	Experiments
In this section, we describe two sets of experiments that verify the faster convergence of Algorithm 1
against vanilla SGD. The first experiment is on linear regression with synthetic noise described in
Example 1 and the second set of experiments is on neural network training.
6.1	Synthetic experiments
In the synthetic experiment, we generate a random linear regression dataset using the sklearn4 library.
We design the stochastic oracle as full gradient with injected Gaussian noise, whose coordinate-wise
4scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html
7
Under review as a conference paper at ICLR 2021
0.8
b
0.6
0.4
0
1.0
2500 5000 7500 10000
Iteration #
IO1
IO-5
Λ1-"Ecdoqns
1.25
0 5 0 5
0 7 5 2
- - - -
Iooo
3s学一α
2000 4000 6000 800010000
iteration #
Figure 2: Left: The injected noise intensity over iterations. Middle:Average loss trajectory over
10 runs for four different algorithms: standard baseline, idealized baseline, Alg 1 and Alg 2. The
curve (idealized vs standard) confirms that adopting step sizes inverse to the noise level lead to faster
convergence and less variations. Right: Average and standard deviation of function suboptimality.
The values are normalized by the average MSE of the standard baseline.
Figure 3: The left two plots show the accuracy of training ResNet18 on Cifar10 dataset. The right
two plots present the negative log-likelihood loss for LSTM Language modelling from Merity et al.
(2018). The baselines are provided by the repos cited on page 2. Algorithm 1 is described in Alg 1.
standard deviation σ is shown in the left figure of Fig 2. We then run the four algorithms discussed in
this work: standard baseline, idealized baseline, Alg 1 and Alg 2. We finetune the step sizes for each
algorithm by grid-searching among 10k, where k is an integer. We repeat the experiment for 10 runs
and show the average training trajectory as well as the function suboptimality in Fig 2. We observe
that the performance is ranked as follows: idealized baseline, Alg 2, Alg 1 and standard baseline.
6.2	Neural Network training
We demonstrate how the proposed algorithm performs in real-world neural network training. We
first tested our algorithm on Cifar10 classification task. We then implement our algorithm into the
AWD-LSTM codebase described in Merity et al. (2018). We see from Figure 3 that our proposed
algorithm can achieve slightly better performance than baselines. Despite our main contribution being
providing theoretical analysis for the fast convergence of adaptive methods with moment estimation,
these results show that our analysis can also lead to efficient and practical algorithm design.
Besides convergence, we also measured noise level during neural network training with different
optimizers and found that the noise pattern is mostly determined by the learning task instead of by
the optimization algorithm. Details can be found in Appendix A.
7	Conclusions
This paper discusses convergence rates of stochastic gradient methods in an empirically motivated
setting where the noise level is changing over iterations. We show that under mild assumptions, one
can achieve faster convergence than the fixed step SGD by a factor that is polynomial in number of
iterations, by applying online noise estimation and using adaptive step sizes. Our analysis, therefore
provides one explanation for the recent success of adaptive methods in neural network training.
There is much more to be done along the line of non-stationary stochastic optimization. Under
our current analysis, there is a gap between the adaptive method and the idealized method when
the noise variation is large (see second row in Table 1). A natural question to ask is whether one
could reduce this gap, or alternatively, is there any threshold preventing the adaptive method from
getting arbitrarily close to the idealized baseline? Moreover, could one attain further acceleration
by combining momentum or coordinate-wise update techniques? Answering these questions would
provide more insight and lead to a better understanding of widely used adaptive methods.
Perhaps a more fundamental question is regarding the iterate dependency: the setting where the
moments mk or the variance σk are functions of the current update xk, not just of the iteration index k.
Significant effort needs to be spent to address this additional correlation under appropriate regularity
conditions. We believe our work lays the foundation to address this challenging research problem.
8
Under review as a conference paper at ICLR 2021
References
A. Agarwal, M. J. Wainwright, P. L. Bartlett, and P. K. Ravikumar. Information-theoretic lower
bounds on the oracle complexity of convex optimization. In Proceedings of Advances in Neural
Information Processing Systems (NIPS), 2009.
N.	Agarwal, B. Bullins, X. Chen, E. Hazan, K. Singh, C. Zhang, and Y. Zhang. The case for
full-matrix adaptive regularization. arXiv preprint arXiv:1806.02958, 2018.
Y. Arjevani, Y. Carmon, J. C. Duchi, D. J. Foster, N. Srebro, and B. Woodworth. Lower bounds for
non-convex stochastic optimization. arXiv preprint arXiv:1912.02365, 2019.
M. G. Azar, I. Osband, and R. Munos. Minimax regret bounds for reinforcement learning. In
International Conferences on Machine Learning (ICML), 2017.
O.	Besbes, Y. Gur, and A. Zeevi. Stochastic multi-armed-bandit problem with non-stationary rewards.
In Advances in neural information processing systems, pages 199-207, 2014.
L. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning. Siam
Review, 60(2):223-311, 2018.
S. Bubeck. Convex optimization: Algorithms and complexity. arXiv preprint arXiv:1405.4980, 2014.
X. Chen, S. Liu, R. Sun, and M. Hong. On the convergence of a class of ADAM-type algorithms for
non-convex optimization. In International Conference on Learning Representations (ICLR), 2019.
Y. Drori and O. Shamir. The complexity of finding stationary points with stochastic gradient descent.
arXiv preprint arXiv:1910.01845, 2019.
J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
C. Fang, C. J. Li, Z. Lin, and T. Zhang. Spider: Near-optimal non-convex optimization via stochastic
path integrated differential estimator. In Proceedings of Advances in Neural Information Processing
Systems (NIPS), 2018.
S. Gadat and F. Panloup. Optimal non-asymptotic bound of the Ruppert-Polyak averaging without
strong convexity. arXiv preprint arXiv:1709.03342, 2017.
S. Ghadimi and G. Lan. Optimal stochastic approximation algorithms for strongly convex stochastic
composite optimization i: A generic algorithmic framework. SIAM Journal on Optimization, 22
(4):1469-1492, 2012.
S. Ghadimi and G. Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic program-
ming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016.
H. Huang, C. Wang, and B. Dong. Nostalgic Adam: weighting more of the past gradients when
designing the adaptive learning rate. In Proceedings of the 28th International Joint Conference on
Artificial Intelligence, 2019.
A. Jadbabaie, A. Rakhlin, S. Shahrampour, and K. Sridharan. Online optimization: Competing with
dynamic comparators. In Artificial Intelligence and Statistics, pages 398-406, 2015.
C. Jin, Z. Allen-Zhu, S. Bubeck, and M. I. Jordan. Is q-learning provably efficient? In Proceedings
of Advances in Neural Information Processing Systems (NIPS), 2018a.
C.	Jin, P. Netrapalli, and M. I. Jordan. Accelerated gradient descent escapes saddle points faster than
gradient descent. In Conference On Learning Theory, pages 1042-1085, 2018b.
A. Jofre and P Thompson. On variance reduction for stochastic smooth convex optimization with
multiplicative noise. Mathematical Programming, 174(1-2):253-292, 2019.
9
Under review as a conference paper at ICLR 2021
D.	P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.
K. Levy. Online to offline conversions, universality and adaptive minibatch sizes. In Proceedings of
Advances in Neural Information Processing Systems (NIPS), 2017.
K.	Y. Levy, A. Yurtsever, and V. Cevher. Online adaptive methods, universality and acceleration. In
Proceedings of Advances in Neural Information Processing Systems (NIPS), 2018.
X. Li and F. Orabona. On the convergence of stochastic gradient descent with adaptive stepsizes. In
The 22nd International Conference on Artificial Intelligence and Statistics, 2019.
L.	Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and J. Han. On the variance of the adaptive learning
rate and beyond. arXiv preprint arXiv:1908.03265, 2019.
M.	Liu, Y. Mroueh, J. Ross, W. Zhang, X. Cui, P. Das, and T. Yang. Towards better understanding
of adaptive gradient algorithms in generative adversarial nets. In International Conference on
Learning Representations (ICLR), 2020.
J.	Ma and D. Yarats. On the adequacy of untuned warmup for adaptive optimization. arXiv preprint
arXiv:1910.04209, 2019.
S. Merity, N. S. Keskar, and R. Socher. Regularizing and optimizing LSTM language models. In
International Conference on Learning Representations (ICLR), 2018.
A. Mokhtari, S. Shahrampour, A. Jadbabaie, and A. Ribeiro. Online optimization in dynamic
environments: Improved regret rates for strongly convex problems. In 2016 IEEE 55th Conference
on Decision and Control (CDC), pages 7195-7201. IEEE, 2016.
E. Moulines and F. R. Bach. Non-asymptotic analysis of stochastic approximation algorithms for
machine learning. In Proceedings of Advances in Neural Information Processing Systems (NIPS),
2011.
A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to
stochastic programming. SIAM Journal on Optimization, 19(4):1574-1609, 2009.
A. S. Nemirovsky and D. B. Yudin. Problem complexity and method efficiency in optimization. Wiley,
1983.
A. Rakhlin, O. Shamir, and K. Sridharan. Making gradient descent optimal for strongly convex
stochastic optimization. In International Conferences on Machine Learning (ICML), 2012.
L.	Rosasco, S. Villa, and B. C. Vu. Convergence of stochastic proximal gradient algorithm. Applied
Mathematics & Optimization, pages 1-27, 2019.
M.	Staib, S. J. Reddi, S. Kale, S. Kumar, and S. Sra. Escaping saddle points with adaptive gradient
methods. In International Conferences on Machine Learning (ICML), 2019.
A. L. Strehl and M. L. Littman. An analysis of model-based interval estimation for markov decision
processes. Journal of Computer and System Sciences, 74(8):1309-1331, 2008.
T. Tieleman and G. Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its
recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-31, 2012.
R. Ward, X. Wu, and L. Bottou. Adagrad stepsizes: Sharp convergence over nonconvex landscapes.
In International Conferences on Machine Learning (ICML), 2019.
J. Zhang, S. P. Karimireddy, A. Veit, S. Kim, S. J. Reddi, S. Kumar, and S. Sra. Why adam beats sgd
for attention models. arXiv preprint arXiv:1912.03194, 2019.
J. Zhang, T. He, S. Sra, and A. Jadbabaie. Why gradient clipping accelerates training: A theoretical
justification for adaptivity. In International Conference on Learning Representations, 2020.
D. Zhou, Y. Tang, Z. Yang, Y. Cao, and Q. Gu. On the convergence of adaptive gradient methods for
nonconvex optimization. arXiv preprint arXiv:1808.05671, 2018.
10
Under review as a conference paper at ICLR 2021
Z. Zhou, Q. Zhang, G. Lu, H. Wang, W. Zhang, and Y. Yu. Adashift: Decorrelation and convergence
of adaptive learning rate methods. In International Conference on Learning Representations
(ICLR), 2019.
F. Zou and L. Shen. On the convergence of weighted adagrad with momentum for training deep
neural networks. arXiv preprint arXiv:1808.03408, 2018.
F. Zou, L. Shen, Z. Jie, W. Zhang, and W. Liu. A sufficient condition for convergences of adam and
rmsprop. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pages 11127-11135, 2019.
11
Under review as a conference paper at ICLR 2021
(a) ResNet18 trained with Adam on Cifar10 (b) AWD-LSTM trained with Adam on PTB
Figure 4: We retrain the models with Adam optimizer and evaluate the second moment (in blue)
and variance (in orange) of stochastic gradients for the Cifar10 experiments and PTB experiments in
Figure 1.
A Additional Experiment Details and Results
In this section, we provide more details on our experiment setup. The code for reproducing noise
estimation and neural network training on AWS-LSTM model is uploaded in the supplementary. The
code for Cifar10 classification can be uploaded upon request.
A.1 Details on PTB Training
Our implementation is based on the author’s github repository 5. The original codebase trains the
network using clipped gradient descent followed by an average SGD (ASGD) algorithm to prevent
overfitting. As generalization error is beyond our discussion, we focus on the first phase (which takes
about 200 epochs) by removing the ASGD training part. Aside of the number of epochs, all other
parameters are the same as the default training procedure. For our algorithm, we update the clipping
value to be 0.25, and set the hyperparameters of Algorithm 1 as ηk = 5, β = 0.95, m = 0.01.
A.2 Details on Cifar 1 0 Training
Our implementation is based on a pytorch implementation 6 of the original ResNet paper(He et al.,
2016). We train the ResNet18 model on Cifar10 dataset for 200 epochs. The baseline uses SGD
with learning rate initialized as 0.1. The learning rate is decayed by 10 at epochs 100, 150. We
eventually achieves approximately 95% validation accuracy. For our algorithm, we set ηk = 1,
β = 0.99, m = 0.01 and use the same learning rate schedule as the baseline.
A.3 Noise Behavior for Different Algorithms
In this subsection, we include one interesting observation on the noise pattern of neural network
training. From Figure 1, we see that the noise pattern looks very different for different task. One
may naturally wonder whether the difference results from the particular training task or from the
optimization algorithms. Noticing that each of these three tasks are trained with a different algorithm
(Cifar10 with momentum SGD; PTB with clipped SGD; En-De translation with ADAM), we retrained
the Cifar10 task and the PTB language modelling with Adam. and rerun the En-De translation
experiment with momentum SGD. We found that the En-De translation task diverges when trained
with SGD even when learning rate is 0.001. Hence we only report the results from the other two
experiments in Figure 4.
From Figure 4, we see that the pattern looks very similar to the original plots in Figure 1. Hence
these experiments suggest that the noise pattern is mostly determined by the learning task instead of
by the optimization algorithms.
5Code source for LSTM https://github.com/salesforce/awd-lstm-lm
6Code source for CIFAR10 https://github.com/kuangliu/pytorch-cifar
12
Under review as a conference paper at ICLR 2021
B	Proof of Theorem 1
Proof. The iterate suboptimality have the following relationship:
∣∣χk+ι - χ*k2 = Ilxk - ηkgk - χ*k2 = IIxk - χ*k2 - 2加hgk,χk - χ*i + η2Ilgkk2.
Rearrange and take expectation with respect to gk we have
2ηk(f(xk) - f*) ≤ 2ηkWf(Xk),xk - x*i
≤ E∣∣xk+ι - x*∣2 - Ekxk - x*k2 + η2m2k
Sum over k and take expectation we get
TT
E[X2ηk(f(Xk)- f*)] ≤ Ilxi - x*k2 + Xηkm
k=1	k=1
Then from convexity, we have
E[f (Xt) - f *] ≤kx1-xWτ+ PT=I η2mk,
k=1 ηk
where XT =(PT=I ηixi"(PT=I ηi ). Corollary 2 follows from specifying the particular choices of
the stepsizes.
□
C	Key Lemma
Lemma 8. Under Assumptions 1, taking β = 1 — 2T-2/3 ,the total estimation errorofthe m Qk based
on (ExpMvAvg) is bounded by:
T
E X |mk - mk | ≤ 2(D2 + M2)T2/3 ln(T2/3)
k=1
Proof. On a high level, we decouple the error in a bias term and a variance term. We use the total
variation assumption to bound the bias term, and use the exponential moving average to reduce
variance. Then we pick β to balance the two terms.
From triangle inequality, we have
T
T
T
EE [m k - mk |] ≤ EEJmk[ E[m k ]|[,+£ IEm kI - mk,
k=0	k=1S ― VaznceZrm —} k=1S ―Biaszm ―}
We first bound the bias term. By definition of mk, We have
E[mk] - mk = βEmk-1] + (1 - β)mk-1 - mk
(4)
=e(E[mk-1] - mi-1)+ (mk-1 - mk)
Hence by recursion,
E[mk] - mk = βk-1 (E[m2] - m2) +βk-2(m2 - m2) + …+ (mk-i - mk)
S----{-----}
=0
Therefore, the bias term could be bounded by
T	T k-1
χ∣E[m k] - mk∣≤ XX βk-i-j ∖mjj - m2+i∣
k=1	k=1 j=1
T-1	T -1-k
X ∣∣m2k - m2k+1∣∣ X βj
k=1	j=0
- m2k+1 ∣∣
D2
≤-----
≤ 1 - β
(From Assumption (1))
13
Under review as a conference paper at ICLR 2021
The first inequality follows by traingle inequality. The third inequality uses the geometric sum over β .
To bound the variance term, we remark that
mk = (I- β)gk-ι +(I- β )βg2-2 + —+(I- β)βk-2gι + βk-1g2.
Hence from independence of the gradients, we have
E [∖mk- Emk]|] ≤ ,var[mk]
=Jvar[(1 — β)g2-ι] + Var[(1 — β)βg2-2] + …+ Var[(1 — β)βk-2g2] + Var[βk-1g2]
≤ ʌ/(l - β)2 + (1 - β)2β2 + ••• + (1 - β)2β2(k-2) + β2(k-1)M2,
where M 2 is an upperbound on the variance. The first inequality follows by Jensen’s inequality.
The second equality uses independence of gi given g1, ..., gi-1. The last inequality follows by
assumption 1.
We distinguish two cases, when k is small, we simply bound the coefficient by 1, i.e.
，(1 - β)2 + (1 - β)2β2 + …+ (1 - β)2β2(k-2) + e2(kT) ≤ 1
When k is large such that k ≥ 1 + γ, with Y = 2(二)ln(ι-β), We have β2(kT) ≤ 1 一 β, thus
J(1 - β)2 + (1 - β)2β2 + …+ (1 - β)2β2(k-2) + β2(k-1)
≤s≡^
≤so^
≤√2(1 - β)
The second inequality follows by k ≥ 1 + Y, with Y = 2(i-e)ln(ɪ-e). Therefore, when k ≥ 1 + γ,
E [∖mk -E[mk]∖] ≤ √2(1-β)M
Therefore, substitute in the above equation into the
Tγ	T
XE[∖mk -E[mk]∖] = XE[∖mk -Emk]∖] + X E [∖mk -Emk]∖]
k=1	k=1	k=γ +1
≤ (γ + (T - γ)√2(1- β))M2
Summing up the variance term and the bias term yields,
T	D2
∑E [∖mk - mk∖] ≤ 1-β + (Y + (T - Y)√2(1 - β))M2	(5)
Taking β = 1 - T-2/3/2 yields,
T
XE[mk - mk\] ≤ 2(D2 + M2)T2/3 ln(T2/3)	(6)
k=0
□
14
Under review as a conference paper at ICLR 2021
D Proof of Theorem 3
On a high level, the difference between the adaptive stepsize and the idealized stepsize mainly
depends on the estimation error 1mIk - mk |, which has a sublinear regret according to Lemma C.
Then we carefully integrate this regret bound to control the derivation from the idealized algorithm,
reaching the conclusion.
Proof. By the update rule of xk+1, we have,
∣∣χk+ι - χ*k2 = Ilxk - ηkgk - χ*k2 = Ilxk - χ*k2 - 2ηkhgk,χk - χ*i + ηkIlgk∣∣2.
Noting that the stepsize ηk is independent of gk , taking expectation with respect to gk conditional on
the past iterates lead to
2ηk(f(xk) - f*) ≤ 2加Wf(Xk),xk - x*i
=E[2ηkhgk,xk - x*i∣xk,…，xι]
=-E[∣∣xk+ι -x*II2Ixk,…，xι] + Ilxk - x*∣∣2 + ηkmk.
Recall that R = ∣∣xι - x*I, taking expectation and sum over iterations k, we get
E[2(PT=1 ηk)(f(xτ) - f*)] ≤ R2 + EPT=I ηkm].
Hence by Markov’s inequality, with probability at least 3/4,
2(PT=1 ηk)(f(xτ )-f *) ≤ 4E[2(PT=1 ηk)(f(xτ) - f *)] ≤ 4(R2 + EPT=I ηk % ]).⑺
Now we can upper bound the right hand side, indeed
TT
XE[ηk2 m2k] =c2XE
k=1	k=1
2
mk
(m^ k + m)2_
≤ c2 (XE [-]+X e [「J
≤c2( m x Eijmk - m k |]+T!
≤ c2 ((M2 +D2N/MT2/3) + T) ≤ 3c2T
m2
(8)
The last inequality follows by the choice on m. Hence, from Eq. (7), we have with probability at
least 3/4,
2(PΤ=1 ηk)(f(xτ) - f *) ≤ 4(R2 + 3c2T)	(9)
Next, by denoting (x)+ = max(x, 0), we lower bound the left hand side,
1X ηk
c
X
m-^ m k + m
X
mk + m
+x(
mk + m
1
mk + m
≥
≥
1
mk + m
(m k — mk)+
(mk + m)(m k + m)
1
mk + m
(rmk - mk)+
ʌ/mk + m ∙ m3/2
-
-
≥
1
mk + m
-X 2(
(mk 一 mk);
m3
1
mk + m
1X m⅛m-) X Imk- mk);
(10)
15
Under review as a conference paper at ICLR 2021
Finally, by Markov’s inequality, with probability 3/4
E (m^k - mk)+ ≤ 4E[£ (mτk - mk壬]≤ 4E[£ (加-m)2] ≤ 8(D2 + M2)T2/3 ln(T2/3).
Following the choice of m =4√D2 + M2T- 1 ln(T) 2 ,we have
T
2m3 X〈mk- mk)+ ≤ 4(M + m)
1T
1X
4乙
k=1
1
mk+m
≤
Consequently, together with (9) and (10), We know that with probability at least 1 - 4 - 1 = 1/2,
* / 4(R2 +3c2T) / 2R	32T
f(xT) - f ≤ P C ≤ √T ∙ P 1
乙k 4(mk+m)	VT 乙k (mk+m)
(11)
where the last inequality follows by setting C = √R
□
Remark 9. For more general choices ofstepsize ηk =(抗P十；P二修,the upper bound in Eq.(8) holds
exactly as in the above proof, and the lower bound in Eq.(10) follows from
ηk
1
(m p + mp)1/p
Σ
Σ
1
(mP + mP)1"
1
(m p + mp)1∕p
1
(mk + mp)1∕p
+
—
X 1	X (mk + mp)1∕p - (mp + mp)1∕p
(m (mp + mp)1/p +	(mP + mp)1∕p(mP + mp)1∕p
Σ1	(mk - mk)+
/ P , CIM -工「p , Nw P , CIM (MInkowskl Inequality)
(mk + mp)1/p (m (mk + mp)1∕p(mk + mp)1∕p
≥	1	(mk - mk) +
一 (m (mp + mp)1∕p m3 m3∕2(mp + mp)1∕2p
≥ 2 X (mp +1mp)i∕p - 2m3 X (mk - mk)+.
E	Proof with concentrate noise
In this section, we add additional constraints on noise concentrations.
Assumption 2. The expected absolute value is not very different from the square root of the second
moment, i.e.
2E[kg(x)k] ≥ PE[kg(x)k2].
The constant “2” in the assumption above is arbitrary and can be increased to any fixed constant. The
above assumption is satisfied if g(x) follows Gaussian distribution. It is also satisfied if for some
fixed constant γ, p(kg(x)k ≥ r) ≤ γE[kg(x)k]3r-4, for all r ≥ γ E[kg (x)k].
We assume that the total variation on the first moment is bounded.
Assumption 3. We denote λk = E[kgk k], and assume that an upper bound M such that
(a)	The second moment of gk is bounded by M2, namely, E[kgk k2] ≤ M2, ∀k.
(b)	The total variation on the first moment λk is bounded by
^X lλk - λk+1l ≤ D = ω(M).	(12)
16
Under review as a conference paper at ICLR 2021
	Constant	Adaptive	Idealized
0 ≤ α ≤ 1	O (T -1)	O (t - 1+2α)	O (t - 1+2α )
6	V J	I	)	∖	J
6 < ɑ	O (t -1)	O (t - 2)	O (t - 1+2α )
Table 2: Comparison of the convergence rate under the noise example 1.
Under this stronger assumption, we can perform our online estimator on the first moment E[kgkk]
instead of the second moment, replacing line 6 of Algorithm 1 by
m k+ι = βm^ k + (1 - β)kgk ∣∣.
(13)
The theorem below shows the convergence rate of the new algorithm.
Theorem 10. Under assumptions 2, 3,with m = 16(D + M)T-1/6 ln(T), c = √R, Algorithm 1
with update rule (13) achieves convergence rate
f (xτ) - f *≤√T ∙ P
12T
1
(14)
k (mk +m)
With a better concentration of the online estimator, we could allow a less conservative correction
constant m, in the order of MT- 6. It is this parameter that controls the maximum attainable
improvement compared to the constant baseline. Indeed, we again consider the noise example 1,
given in Table 2. In this case, the adaptive method can obtain an improvement of order T6 compared
to the constant baseline, while as previously only T9 is achievable.
The proof of the result follows a similar routine as the proof of Theorem 3. We start by presenting an
equivalent lemma of Lemma 8.
Lemma 11. Under assumption 3, we can achieve the following bound on total estimation error.
T
EX |mk - λk |] ≤ 2(D + M)T2/3 ln(T2/3)
k=1
Proof. The proof is the same as the proof of Lemma 8, by replacing the second m2k by the first
moment λk = E[∣gk ∣].
□
Proof of Theorem 10. By Assumption 2, we can use first moment of gk to bound the second moment.
Hence, Eq. (7) implies that with probability at least 3/4,
2(ΡT=1 ηk)(f((xτ) - f*) ≤ 4(R2 + E[4ΡT=1 ηkλk]).	(15)
Now we upper bound the right hand side, indeed
T	T	λ2
X E[η2λk] = c2 X E [,]
≤ c2 (X E [-]
T
+XE
k=1
Km k + m)2
(λk — m k)(λk + m k)
(m k + m)2
+T
≤ c2 2MX XE[∣λk- mk|] + T)
≤ c2 (4(M +D)MT2/3 In(T2/3) + T)< 2c2T
m2
17
Under review as a conference paper at ICLR 2021
Hence by Markov inequality, with probability at least 3/4,
2(PT-01 ηk)(f(xι) - f*) ≤ 4E[2(PT-0 ηk)(f(XI) - f*)] ≤ 4(R2 + 2c2T)
Next, we lower bound the left hand side,
1X ηk
c
X . 1
^^k m k + m
Σ
1
λk + m
1
λk + m
1
m k + m
+
X
λk + m
(mk - λk) +
(m k + m)(λk + m)
X 1
λk + m
—m χ (m k - λk)+
-
By Markov’s inequality and Lemma 11, with probability 3/4, we have
E (mk - λk)+ ≤ 4E[£ |mk - λk∣] ≤ 8(D + M)T2/3 ln(T2/3).
Following the choice of m = 16(D + M)T- 1 ln(T), We have
1	T11
—E[(mk - λk)+] ≤ --- ≤ - > --
m2 M	1 + 2(M + m) ― 2 0 λk + m
Consequently, We know that with probability at least 1 - ɪ - 4 = 1/2,
、	4(R2 +2c2T) / 2R 12T
f (XT) - f ≤ P C ≤ √ ∙ P 1	,	(16)
ʌIk 2(λk+m)	VT ʌ/k (mk+m)
by setting C = ∙R and the fact that λk ≤ mk.	□
F Variance oracle and extension to nonconvex setting
In this section, we show that we can adapt our algorithm to the variance oracle in Definition 1 (b),
where
E[kgk - Vf(Xk)k2] = σk.
To avoid redundancy, we present the result in the nonconvex smooth setting. We make the following
smoothness assumptions.
Assumption 4. The function is L-smooth, i.e. for any X, y, kVf (X) - Vf (y)k ≤ LkX - yk.
Remark that the L-smoothness condition is not required in the second moment oracle. This is why
the second moment assumption is usually imposed in the non-differentiable setting (see Section 6.1
of Bubeck (2014)). We first provide the convergences of SGD serving as the baselines.
Theorem 12 (Nonconvex baselines). Under the variance oracle in Definition 1 (b) and Assumption 4,
the convergence ofSGD using update xk+ι = xk - ηkgk with ηk ≤ 2L satisfies
E[kVf(xι)k2] ≤ f(XI)-二+L2 P'=1 η2°2,	(17)
k=1 ηk
where I is an random variable with P(I = i) H η.
This convergence result is very similar to the one shown in the convex setting in Theorem 1. Instead
of bounding the function suboptimality, the upper bound is only available on the norm of the gradient,
implying convergence to a stationary point. We remark that an additional requirement on the stepsize
is required, namely ηk ≤ 1/2L. This is not surprising since taking a large stepsize will not be
helpful due to the L-smoothness condition. Hence, we can not take a stepsize inversely depending
on σk when the noise is small. This restriction makes the comparison on convergence rate less
straightforward. To facilitate the discussion on convergence rate, we make an additional assumption
on the lower bound of the variance σk .
18
Under review as a conference paper at ICLR 2021
Algorithm 2 Variance Adaptive SGD (x1, T, c, m)
1:	Initialize σι = kg1-2glk , where gi,gl are two independent stochastic gradients at xi.
2:	for k = 1, 2, ..., T do
3:	Query two independent stochastic gradient gk , gk0 at xk .
4:	Update xk+i = Xk - η(gk + gk)/2 with η = σk+m and m ≥ 2cL.
5:	Update σk+1 = βσk + (1 - β) kgk-gkk2
6:	end for
7:	return xi where I is the random variable such that P(I = i) 8 %.
Assumption 5. Forany k ∈ [1,T], σk ≥，8L(f (xi) — f (x*))/√T.
We emphasize that the above condition is not necessary to derive convergence analysis, but only for
the clarity in terms of comparison on different convergence rate. This assumption help us focus on the
case when noise (instead of the shape of the deterministic function) dominates the convergence rate
and determines the step size. In otherword, our step size choice under this setting satisfies ηk ≤ 1/2L,
leading to the following convergence rate:
Corollary 13. Let ∆ = f(xi) — f *. We have thefollowing two convergence rate boundsfor SGD:
1.	SGD with constant stepsize: if ηk = η = JL P σ2, then
E[kVf (xi)k2] ≤ λ∕2lδPk=1 σk = J2L^ ∙ JPkTI σk.	(constant baseline)
2.	SGDwith idealized stepsize: if ηk =言 ^/LF, then
E[kVf (xi)k2] ≤ √2LTδ = 2L2LA ∙---------t i .	(idealized baseline)
........... ET=ι σk	v ，	ET=ι σ
The resulting convergence rate are similar as the convex setting. We now modify the adaptive
algorithm to make use of the variance oracle and the smoothness assumption. The through algorithm
is presented in Algorithm 2. Particularly, we keep an exponential moving average of the variance
instead of the moments, using two stochastic gradients gk and gk0 at the same iterate,
σ2+ι= βσ2 + (i- β) Mk-皿.
In particular, E[kgk - gk0 k2/2] is an unbiased estimator of σk2. To provide the convergence analysis
of the algorithm, we make the following assumptions on the variation of the variance.
Assumption 6. We assume an upper bound on σk2 = E[kgk - Vf(xk)k2], i.e. maxk σk ≤ M. We
also assume that the total variation in σ^ is bounded. i.e. Ek ∣σ22 — σ22+i | ≤ D2 = 4M2.
With the above assumptions, algorithm 2 achieve the following convergence rate.
Theorem 14. Under the assumptions 4, 6 and T large enough such that ln T ≤ T1/3, algorithm 2
with C = 2JT^m, m = 4√D2 + M2T-i/9 ln(T)i/2 + 2cL, achieves with probability 1/2,
kVf(xi)k2≤
32T
i
k (σfc +m)
Σ
The above theorem is almost the same as Theorem 3, and hence all the remarks for Theorem 3 also
applies in the nonconvex case.
G Proof of Theorem 12
Proof. By L-smoothness, we have
Lη2
f (xk + i) ≤ f(xk) - ηk hgk, Vf(Xk )i +-2 Ilgk k
19
Under review as a conference paper at ICLR 2021
Rearrange and take expectation with respect to gk, we get
Ck- Lη2) kvf (xk)k2 ≤ f (xk)- f(xk+i)+Lη2 E[kgk - Vf(Xk)k2].
From the condition ηk ≤ 芸,we have,
η kVf (Xk )k2 ≤ f(Xk ) - f(Xk +1) + 2 ηkσ2.
Sum over k and take expectation,
TL
E[∑ηkkVf(χk)k2] ≤ f(X0) - f(χ*) + 2£宿。2.
k=1	k
Denote I as the random variable such that P(I = i) 8 巾.We know
TT
(Xη)E[kVf(XI)k2] ≤ f(x0) - f(x*) + 2 Xηkσ2.
k=1	k=1
This yields the desired convergence rate in (17).	□
H Proof of Theorem 14
The proof is almost identical to the proof of Theorem 3. We start by presenting an equivalent theorem
of Lemma 8 below.
Lemma 15. Under assumption 6, we can achieve the following bound on total estimation error using
the estimator (ExpMvAvg):
T
EX ∣σ2 - σk |] ≤ 2(D2 + M2)T2/3 ln(T2/3)
k=γ
Proof. This follows by exactly the same proof as Theorem 8 and the fact that E[kg - g0k2] =
2E[∣∣g -Vfk2].
Proof of Theorem 14. The first part of the proof follows the schema as in the proof of Theorem 12,
when ηk ≤ 1/2L, we know that
T
E[(∑ηk)kVf(Xi)k2] ≤ △ + 2 £E[n k=1	k The rest of the proof is exactly the same as the proof of Theorem 3 hand side, indeed T	T2 X E[η2σ2] = c2 X E ； k=1	k=1 T	22	T =C2 X E σk σk 2 + X E ∖h	[(σk + m)2」k=[ ≤ c2( m XE [星-σ⅛ι]+T! < c2 (2(M2 + D2)T2/3ln(T2/3) + m2	k2σk2]. . We can upper bound the right (6k +m)2 j T ≤ 3c2T
20
Under review as a conference paper at ICLR 2021
M2+D2	1 T1/3
The last inequality follows by the choice of parameters that MmD ≤ 点 In(T). Hence by Markov
inequality, with probability at least 3/4,
(PT=Iηk)INf(χι)k2 ≤ 4E[(PT=ιη)kvf(χ.)k2] ≤ 4(△ + 3Lc2T)
Next, we lower bound the left hand side as in (10),
1 X ηk = X -ɪ ≥ 1 X -......................L X (σk - σk)+
C 乙	σ^k σk + m 2 乙 σk + m 2m3 乙	+
(18)
Finally, by Markov’s inequality, with probability 3/4,
E (σk-σk)+ ≤ 4E[]T (σk - σk甘]≤ 4E[£ ®-σQ2] ≤ 8(D2 + M2)T2/3 ln(T2/3).
Following the choice of m = 4√D2 + M2T-9 ln(T) 1 + 2cL, we have
1T
2m3 X (σk - σk)+
k=1
≤ λiJ ≤ ≤ 1X-ɪ
4(M + m)	4	σk + m
Together with (18) implies that with probability 3/4,
T
X ηk ≥ C X
k=1
1
σk + m
Consequently, we know that with probability at least 1 一 4 一 4 = 1/2,
kvf(xI)k2≤
4(∆+ 3Lc2T)
P C 一
k 4(σk +m)
∕2L∆
≤ V ~τv~
32T
1
k (σk +m)
(19)
Σ
by setting c
2∆
V LT -
□
21