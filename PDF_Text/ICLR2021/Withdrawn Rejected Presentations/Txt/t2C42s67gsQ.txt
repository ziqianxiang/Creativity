Under review as a conference paper at ICLR 2021
Adaptive Dataset Sampling by
Deep Policy Gradient
Anonymous authors
Paper under double-blind review
Ab stract
Mini-batch SGD is a predominant optimization method in deep learning. Several
works aim to improve naive random dataset sampling, which appears typically in
deep learning literature, with additional prior to allow faster and better performing
optimization. This includes, but not limited to, importance sampling and curricu-
lum learning. In this work, we propose an alternative way: we think of sampling as
a trainable agent and let this external model learn to sample mini-batches of train-
ing set items based on the current status and recent history of the learned model.
The resulting adaptive dataset sampler, named RLSampler, is a policy network
implemented with simple recurrent neural networks trained by a policy gradient
algorithm. We demonstrate RLSampler on image classification benchmarks with
several different learner architectures and show consistent performance gain over
the originally reported scores. Moreover, either a pre-sampled sequence of indices
or a pre-trained RLSampler turns out to be more effective than naive random sam-
pling regardless of the network initialization and model architectures. Our anal-
ysis reveals the possible existence of a model-agnostic sample sequence that best
represents the dataset under mini-batch SGD optimization framework.
1	Introduction
Deep learning is notoriously data-hungry. A great proportion of the recent success of deep learning
algorithms has been largely attributed to the developments of bigger and better quality datasets
(Russakovsky et al. (2015); Abu-El-Haija et al. (2016); Radford et al. (2019)). Commercial off-the-
shelf hardware that run these algorithms has limited memory. Therefore, it is natural to partition the
dataset into smaller batches of samples and train the network with a single batch at a time. The use of
mini-batches under stochastic gradient descent (SGD) optimization frameworks is further justified
by a regularizing effect of small batches (Wilson & Martinez (2003); Keskar et al. (2017)).
How to sample better to enhance the quality of learning has been an intriguing question from the
early days of mini-batch SGD (Wilson & Martinez (2003); Bengio et al. (2009)). There have been
a few works on dataset sampling strategies in various fields of machine learning (Bordes et al.
(2005); Shrivastava et al. (2016); Chang et al. (2017); Katharopoulos & Francois (2018); Hacohen
& Weinshall (2019); DerezinSki et al. (2019); Ariafar et al. (2020)). Especially in reinforcement
learning, where data are fed in a sequential manner, intelligent sampling techniques are proposed
to produce better performance (Schaul et al. (2016); Fang et al. (2019)). However, simple random
sampling without replacement is still a predominant way to sample a dataset in deep learning.
In this work, we explore a powerful alternative strategy to sample a dataset to train a deep network
more effectively. Instead of considering dataset sampling as an independent and passive component
ofan optimization algorithm, we consider the sampling itself as a part of optimization. We model the
sampling module as a policy network that is adaptively trained based on intermediate outputs of the
learned model. This way, our sampler model, named RLSampler, can learn which data to be fetched
next without relying on prior knowledge of the data distribution. From the learner’s perspective, the
only difference of this scheme is the order ofa sequence of training samples fed into it. Surprisingly,
deep networks trained with our RLSampler consistently outperform the ones that are trained with
a naive random sampler. Moreover, a pre-trained RLSampler or even a generated sequence used
to train particular models also provides a performance boost when used for different models with
different architectures and initializations.
1
Under review as a conference paper at ICLR 2021
Figure 1: Training a deep discriminative model with our adaptive sampling framework (RLSampler). With
two additional feedback signals, i.e., state observation and reward feedback, the dataset sampler is modeled
as an RL agent that learns to sample optimal sequence of samples that train the learner most effectively. The
interface of the learner is not affected, so the sampling method can be easily applied to any existing framework.
From those observations and subsequent analyses, we re-emphasize the role of sampling as a key
to enhance the generalizability of a deep network and to stabilize its training. The remainder of the
manuscript is organized as follows: In Section 2, we establish the problem of optimal sampling. We
design our RLSampler in Section 3 as an RL agent trained by a policy gradient algorithm. After a
brief discussion with related works in Section 4, we test our model in Section 5. We then analyze
the characteristics of the generated sequences of indices produced by our RLSampler in Section 6.
2	The Problem of Optimal S ampling
A discriminative deep learning framework consists mainly of four components: a training dataset
X , a parameterized learner model, fθ : X → Rd with parameters θ, a scalar objective L, and an
optimization algorithm E. In addition to those, we also consider an optionally parameterized dataset
sampler Sφ with parameters φ as a separate module. The learner fθ maps each data point xi ∈ X
to a fixed-length logit vector li ∈ Rd . When the learner being trained, a mini-batch stochastic
gradient-based optimization algorithm E tries to minimize the objective L with respect to the learner
parameters θ. At the beginning, θ and φ are initialized with arbitrary values: θ J θ° and φ J φo.
At iteration t, a sampler Sφ samples a mini-batch Bt from the dataset X. The parameter θ is updated
by the gradient of the loss, estimated from the mini-batch Bt , i.e.,
θt+1 J θt - αtVθ LBt (fθt),	(I)
where αt is a learning rate, which is a parameter of E. The training terminates at a finite horizon T0,
which is also a parameter of E. Thus, our deep learning framework is a five-tuple: (X, fθ, L, E, Sφ).
In the problem of optimal sampling, we aim to find an optimal parameter φ* for the sampler Sφ
that draws a sequence of mini-batches B1, B2, . . . , BT0 of training samples which, with the algo-
rithm E, minimizes the generalization error of the learner network fθ measured by L on a separate
dataset Xtest. To achieve this goal, the sampler can utilize the state information of the deep learn-
ing framework. The state of the framework is a tuple (X, H) of the training set X and the history
H := {Oo：To, φ0τ, Bi：To} of intermediate parameters and sampled indices. Our goal is to train a
sampler Sφ like a history-dependent policy to maximize the generalizability of a trained model fθ .
The problem can be modeled as a finite-horizon, discounted Markov Decision Process (MDP, Sutton
& Barto (2018)) M = (S, A, P, r, γ) and can be solved by reinforcement learning. The state space
S := (X , H) is the state of the entire framework. The action space A is defined as a set of all
indices of the dataset X, i.e., if N := |X |, then A = {1, 2, . . . , N}. The state transition probability
P : S × A → P(S) is the dynamics of the deep learning framework (X , fθ, L, E, Sφ). The reward
r : S × A → R carries the information of recent generalization error of the learner model and is
rigorously defined in Section 3.3. Finally, γ ∈ (0, 1] is a discount factor. The optimal stochastic
policy Sφ : S → P(A), our RLSampler, is determined by solving the maximization problem of the
expected cumulative reward,
T0
J(φ) = ESφ X γtr(st, at) .	(2)
t=1
In this work, we utilize a policy gradient algorithm (Williams (1992)) to solve the problem online.
2
Under review as a conference paper at ICLR 2021
3	Adaptive Sampler Design
3.1	Design Concerns
There are three constraints need be considered before resolving the structure of our RLSampler. The
first and the most important one is an appropriate approximation of the state space S . As mentioned
in Section 2, S is the complete state (X, H) of the framework, which is intractable in practice. Es-
pecially, the history H grows as learning progresses, and its final length T0 is almost always too
large for typical hardware to process at once. In order to exploit the state information in an efficient
way, we need a compact summary of it. Therefore, a sequential model is employed to summarize
the history of varying length into a fixed-length vector. Furthermore, each element (θt , φt , Bt) ∈ H
of the history at time t is already a high-dimensional variable. Fortunately, we can simply ignore
φt since it is a state of the sampler; however, the remaining elements X, θo^-ι, and Bo：t-i of the
state S still require a tractable approximation. In the simplest form, a deep model fθ can be viewed
as a black-box function, the state of which can be approximated with pairs of inputs and corre-
sponding outputs. The information of the inputs can be thought of as being already encoded in the
hidden states of the sampler, as it generates those input indices. These inputs also represent partial
information of the dataset X as well as the history of sampled batches B0:t-1. Thus, by feeding
only the learner outputs to the sampler, we can provide a simple approximation of the current state
{X, θo:t-i, φ0∙,t-1, Bi：t-i} of the deep learning framework. To this end, We design our RLSampler
with a recurrent network that receives the output logits lt-1 = (lt-1,1, lt-1,2, . . . , lt-1,|B|) of the
learner fθt-1 at the previous iteration t - 1 to sample a mini-batch Bt for the iteration t.
Action space A, or the output space of the sampler Sφ is another important thing to be considered.
Two options are the most natural for a sampler: the set of all mini-batches and the set of single in-
dices. However, the number of possible combinations fora mini-batch becomes so large in practical
deep learning frameworks that the first option is in fact infeasible. A more reasonable choice would
be a set of all indices of the training set, i.e., we draw a single item for each call to the sampler,
which yields the probabilities of the indices to be sampled next. With a mini-batch SGD, however,
the state of the learner changes every time a mini-batch is sampled, rather than a single item. Thus,
we split the sampler into two modules: the history logging network, which logs the history of the
logits from the learner every mini-batch, and the policy network, from which items are sampled
one-by-one. They will be explained in more detail in Section 3.2
The final consideration is about the reward feedback. The rewards should be evaluated on a separate
validation set to provide hints on the current generalization gap. We need to partition the original
training set X into a training subset Xt which is used to update the learner network, and a valida-
tion subset Xv which is for updating the sampler. Therefore, paired with RLSampler, the learner
effectively experiences a smaller training set. In addition, the evaluation on the validation set is an
excessive overhead, so the number of tries is best to be minimized. Also, the reward signal should
properly reflect the effect of changes the policy has made. For those reasons, the sampler model need
be updated less often than the learner network. Therefore we introduce two new hyperparameters: a
policy update period T1 , and a reward period T2 that divides T1 .
3.2	Controller Design
Before training, the original dataset X is partitioned into a training subset Xt and a validation subset
Xv. We refer to these splits as subsets to avoid unnecessary confusion with the training set X and the
test set Xtest. For the same reason, we count the iteration t only with respect to the learner’s update,
i.e., in the t-th iteration, the learner sees t-th mini-batch from the training subset.
As in Figure 1 and Algorithm 1, we first describe how RLSampler can be used in a mini-batch SGD
optimization framework. RLSampler Sφ is implemented as an iterator; every call to it returns a
single index of the dataset. At iteration t, the sampler is called |Bt | = k times to fetch a mini-batch
Bt = {bt,i }ik=1 from Xt. The RLSampler Sφ has two inputs: First, at the end of each iteration t,
the RLSampler receives the learner’s output logits lt as a state observation to update the historical
summary S J st. Second, to train itself, every reward period T2 of iterations, the sampler fetches
the entire validation subset Xv and receives an averaged validation loss from the learner, from which
a reward signal is calculated. After T1 iterations, with T1/T2 reward signals received, RLSampler is
trained for a single step, then all the hidden states of the recurrent modules are reset.
3
Under review as a conference paper at ICLR 2021
Algorithm 1 Mini-batch SGD with adaptive dataset sampling using an RLSampler.
Require: T0, total training steps. T1, a policy update period. T2, a reward period, divides T1. k, a batch size.
Require: X = (Xt, Xv), the training set. θ0, φ0, initial parameters of the learner fθ and the sampler Sφ.
1:	for t = 1,…，To do
2:	for bt,i in the t-th mini-batch Bt = {bt,i}ik=1 do
3:	Using equation (4), sample bt,i 〜Bt,i.
4:	end for
5:	Using equation (1), update the learner fθt with the mini-batch Bt .
6:	Using equation (3), calculate a new summary st+1 from the learner’s output logits lt = fθt (Bt).
7:	if t mod T2 = 0 then
8:	Using Lval = E(x,y)∈Xv L(fθt (x), y), calculate the average validation loss.
9:	Using equation (5), calculate rt, bt, and At from the validation loss Lval.
10:	end if
11:	if t mod T1 = 0 then
12:	Using equation (6), update the sampler Sφ with ADAM with the policy loss Lpolicy.
13:	Reinitialize the hidden states h。, h∏ — 0.
14:	end if
15:	end for
We now explain two submodules of the RLSampler S = (η, π): a history logger η and a policy
network π . We also provide subtle implementation details in Appendix A.
History Logger The history logger η is a recurrent neural network (RNN) with parameters φη
and hidden states hη . At the previous iteration t - 1, the module receives an output logit tensor
lt-1 := fθt-1 (Bt-1) of the batch Bt-1 from the learner network fθ and returns a fixed-length
summary st , which is a state approximation supplied to the policy network at iteration t.
(st,hηt) = η(stopgrad(fθt-1 (Bt-1)), hηt-1; φηt-1),	(3)
where stopgrad blocks the backpropagation algorithm to update the values inside, so the update
to the sampler does not propagate through the learner. The history logger is implemented with
two-layer LSTM cells (Hochreiter & Schmidhuber (1997b)) followed by a single linear decoder.
Policy Network The sampling policy π is a stochastic policy implemented with an RNN. At i-th
call to the sampler at iteration t, the policy network receives a summary st of the history until the
last mini-batch Bt-1. The output is a logit vector ltπ,i ∈ R|Xt| corresponding to the probability of
selecting each element of the training subset Xt for the i-th item of the batch Bt . The probability of
sampling an item xj ∈ Xt follows the categorical distribution corresponds to the logit ltπ,i, i.e.,
(ltπ,i, hπt,i ) = π(st, hπt,i-1 ; φπt,i-1 ),
pBt,i(j) = Prob{Bt,i = xj|st} = [softmax(ltπ,i)]j,
(4)
where Bt,i is a random variable of selecting an item of the training subset Xt for the i-th element of
the t-th batch, pBt,i is a probability vector corresponds to it, hπt,i is the hidden state of the recurrent
modules of π, and φπt,i is the network parameter of π. The policy network π consists of a single
linear encoder followed by two-layer LSTM cells and a single linear decoder.
3.3	Controller Objective
Our problem setting of finding an optimal optimization framework shares its goal with network
architecture search (Zoph & Le (2017); Pham et al. (2018)), from which we borrow the formula of
the reward signal. Every reward period T2 , an average validation loss is obtained by evaluating the
same loss function L used to train the learner, but on the entire validation subset Xv. The reward rt
is defined as an exponential of the averaged validation loss.
rt = exp{-2Lval} = exp{-2E(x,y)∈Xv L(fθt (x), y)},	(5)
where x is a validation sample, y is its ground truth label, fθt is the learner network. We do not
backpropagate the gradients inside the reward function.
Using a single reward for the policy gradient update may lead to unnecessarily large variance (Sutton
& Barto (2018)). Itis common to define a baseline bt and use the advantage At := rt -bt to evaluate
4
Under review as a conference paper at ICLR 2021
the policy gradient. Since our policy network is intrinsically sequential, we use exponential moving
average of the reward as the baseline (Zoph & Le (2017)), i.e., bt = λrt + (1 - λ)bt-T2 . Here,
λ = 0.1 controls the rate of decay of the received reward.
We use Adam (Kingma & Ba (2015)) to update the sampler S = (η, π) with the REINFORCE
algorithm (Williams (1992)). Because Adam is a stochastic gradient descent solver, we define a
policy loss Lpolicy, gradient of which is the negative policy gradient. At iteration nT1 , n ∈ N,
T1/T2	T2	1 k
Lpolicy,nT1 = Σ A(n-1)T1+mT2∑> k∑>-i .	⑹
m=1	l=1	i=1
4	Related Work
Despite the success of deep learning under naive uniform sampling strategy, several branches of
works aim to improve the performance and speed of training. Importance sampling has been stud-
ied for convex optimization problems (Bordes et al. (2005); Needell et al. (2014); Zhao & Zhang
(2015)), for deep representation learning (Bengio et al. (2009); Alain et al. (2016); Loshchilov &
HUtter (2016); Chang et al. (2017); WU et al. (2017); Katharopoulos & Francois (2018); Ren et al.
(2018); Johnson & Guestrin (2018); Csiba & Richtarik (2018); Peng et al. (2019); Ariafar et al.
(2020)), and for deep reinforcement learning (Schaul et al. (2016); Fang et al. (2019)). In this
framework, the sampling probability of each training set item is nonuniformly weighted.
Importance sampling is extensively used in reinforcement learning with experience replay (Schaul
et al. (2016)), where importance is defined with instantaneous or accumulated rewards. With a sta-
tionary dataset, however, the definition of sample importance is rather subtle. Loshchilov & Hutter
(2016) use training loss to calculate importance, Wu et al. (2017) define sample weights using a dis-
tance metric in the latent space of the learner model, and Katharopoulos & Francois (2018) utilize the
upper bound of the per-sample gradient norm of all the learner parameters to determine the sample
importance. Zhao & Zhang (2015) argue that sampling methods that minimize the variance of the
stochastic gradients achieve faster convergence for convex optimization problems. However, in high
dimensional nonconvex problems such as deep learning, large mini-batch SGD, which should have
low empirical variance, tends to yield sharp minima that generalize worse (Hochreiter & Schmid-
huber (1997a); Keskar et al. (2017)). Instead of using heuristics to define sample weights, we allow
the sampler to be a trainable agent that adaptively learns the sampling distribution.
There are other branches of works on sampling strategy. Our method shares its philosophy with
active sampling (Bengio & Senecal (2008); Li & Guo (2013)), where a learning agent chooses
which data sample to learn next; however, in our problem setting, we do not presume an external
annotator. Determinantal point process (DPP, DerezinSki et al. (2019); Huang et al. (2019); Zhang
et al. (2019)) is a well-used technique in machine learning. In DPP, the sampling process is viewed
as a point process on the sample space. Despite their firm mathematical background, they use
additional optimization procedure to preprocess the dataset and construct a kernel matrix with size
N × N, where N = |X |, which becomes intractable in modern deep learning frameworks.
Another branch of methods that modifies training schedule in order to improve the quality of the
learning is to provide a sequence of subsets ofa training set. Curriculum learning (CL, Bengio et al.
(2009); Graves et al. (2017); Hacohen & Weinshall (2019)) methods start by exposing the learner
to small and easier subsets of the training set and gradually increase its size, to make the learner
experiences harder examples in the later steps. Self-paced learning (SPL, Kumar et al. (2010);
Shrivastava et al. (2016)), on the other hand, stresses on harder samples to train the network. Among
those, Fan et al. (2017) is the closest to our method. They use an RL agent to filter out samples from
a mini-batch for CL; however, we use an RL agent to directly sample from the training set.
5	Evaluation
We begin by testing our RLSampler over common image classification benchmarks: CIFAR-10 and
CIFAR-100 (Krizhevsky (2009)). We then demonstrate the effectiveness of RLSampler in more
constrained environments, i.e., in case with larger learning rates to quantify the training stability the
sampler can provide. Generated sequences of indices from the evaluations are further analyzed in
5
Under review as a conference paper at ICLR 2021
Table 1: Top-1 classification error on CIFAR-10.
Model	Random Sampler			RLSampler (Ours)			Accuracy Gain
	Mean±SD	Best	Worst	Mean±SD	Best	Worst	
VGGNet-16	9.082±0.119	8.94	9.26	8.824±0.096	8.74	8.99	0.258 (2.84%)
VGGNet-16 + BN	8.016±0.140	7.81	8.25	7.772±0.203	7.46	8.04	0.244 (3.04%)
ResNet-20	8.658±0.246	8.33	9.02	7.804±0.249	7.52	8.22	0.854 (9.86%)
WRN-28-2	5.486±0.141	5.22	5.62	5.204±0.131	4.97	5.34	0.282 (5.14%)
WRN-28-10	4.056±0.102	3.93	4.24	3.704±0.090	3.58	3.82	0.352 (8.68%)
Table 2: Top-1 classification error on CIFAR-100.							
	Random Sampler			RLSampler (Ours)			
Model							Accuracy Gain
	Mean±SD	Best	Worst	Mean±SD	Best	Worst	
VGGNet-16	34.520±0.370	33.88	34.88	33.750±0.621	32.94	34.82	0.770 (2.23%)
VGGNet-16 + BN	31.340±0.292	31.06	31.85	30.478±0.271	30.06	30.81	0.862 (2.75%)
ResNet-20	33.846±0.511	32.99	34.58	32.606±0.421	31.87	33.17	1.240 (3.66%)
WRN-28-2	26.700±0.207	26.40	26.98	25.540±0.326	25.18	25.98	1.160 (4.34%)
WRN-28-10	20.040±0.109	19.89	20.19	18.918±0.195	18.65	19.24	1.122 (5.60%)
Section 6. All experiments are done with PyTorch (Paszke et al. (2019)). The code will be available
after the publication.
5.1	Standard Image Classification
Both CIFAR-10 and CIFAR-100 (Krizhevsky (2009)) are widely used benchmarks, each contains
50k training and 10k test images, equally distributed in 10 and 100 classes, respectively. To ver-
ify if our RLSampler can generalize well with various model architectures, we train two 16-layer
VGGNets (Simonyan & Zisserman (2015)) with and without batch normalization layers (Ioffe &
Szegedy (2015)), a 20-layer ResNet (He et al. (2016)), a WRN-28-2, and a WRN-28-10 (Zagoruyko
& Nikos (2016)). For each experiment, we train five times with the same hyperparameters but with
different initializations of the learner and the sampler. We report the mean, the standard deviation,
the best and the worst top-1 error in Tables 1 and 2. In the cases with an RLSampler, we spare
the last 640 (1.28%) samples of 50k training set for the validation subset of CIFAR-10, and the last
1280 (2.56%) samples for the CIFAR-100. For a random sampler without replacement, we use the
original 50k training set. More details of the architectures and other hyperparameters used in the
experiments are summarized in Appendix B.
Results in Tables 1 and 2 show that networks trained with our RLSampler consistently outperform
the ones trained with a random sampler without replacement. Although the learner with an RL-
Sampler experiences fewer training examples due to additional train/validation split, in some cases,
like the case of WRN-28-10 on both datasets, even the worst case with an RLSampler shows better
performance than the best case with a random sampler. With RLSampler we achieved a maximum
average gain of 9.9% on CIFAR-10 (ResNet-20) and 5.6% on CIFAR-100 (WRN-28-10).
5.2	Training with Increasing Learning Rate
The problem of exploding and vanishing gradients in a deep network has been a major hurdle to
achieve stable training. This problem is largely alleviated by the introduction of residual connections
(He et al. (2016)) and batch normalization (BN, Ioffe & Szegedy (2015)). Recent works (Bjorck
et al. (2018); Santurkar et al. (2018)) explain that the effectiveness of BN comes from maintaining
good stability under a higher learning rate. We conduct a series of evaluations to see whether an
RLSampler delivers a similar effect. Two 16-layer VGGNets with and without BN are trained on
CIFAR-10 with various learning rates. Figure 2 shows average top-1 errors and standard deviations
of each setting, based on three evaluations with different model initializations. It turns out that the
advantage of using RLSampler is subtle in the network without BN; the benefits ofBN and adequate
sampling strategy are additive.
6
Under review as a conference paper at ICLR 2021
(a) VGG-16 on CIFAR-10.
(b) VGG-16-BN on CIFAR-10. (c) VGG-16 on CIFAR-100. (d) VGG-16-BN on CIFAR-100.
Figure 2: Image classification results of VGGNets (Simonyan & Zisserman (2015)) with two different sam-
plers and various learning rates (LRs). We report the average (bold center line) and the standard deviation
(shaded area) of three runs, except for the base LR (0.01) case, where we use the values in Tables 1 and 2.
6	Analysis on the Sampled S equence
In this section, we apply pre-trained RLSampler models and pre-sampled sequences of training
mini-batches generated in the main evaluations in Section 5.1 to different learner initializations and
architectures. The objective of this analysis is to demonstrate the versatility of the generated sample
sequence. We conclude our analysis by inspecting the internal statistics of the sampled sequence.
6.1	Model-Independent Optimal Sample Sequence
We conduct two additional experiments to show that the effectiveness of the generated training
sample sequence is more an intrinsic property of the dataset and is independent to the models being
trained. We use pre-trained RLSampler models and pre-sampled sequences from the best one among
five runs shown in Tables 1 and 2 in the subsequent analyses.
Cross-Initialization Experiments Using an
RLSampler and a training sequence used to
train the best ResNet-20 model on CIFAR-10
in Section 5.1, we train 10 different initializa-
tions of the identical learner architecture. The
result from each network instance is compared
to the top-1 error that the same initialization
obtains under a random sampler, averaged over
Table 3: Cross-initialization experiment results.
Imported	Before	After	Avg. Gain±SD
Fixed RLSampler	8.723	8.174	0.549±0.362
Unfixed RLSampler	8.697	8.096	0.601±0.282
Samples Only	8.631	8.064	0.567±0.241
10 runs. We compare three settings: the two with the pre-trained RLSampler, having its weights
fixed and not fixed, and the one with the exact same sequence of indices used to train the best model.
Table 3 summarizes the result. Although the sampler and the sequence are derived from a different
training framework, we obtain a consistent performance boost compared to a nave random sampler.
Cross-Architecture Experiments We train
various convolutional neural networks on
CIFAR-10 with the pre-generated sample se-
quences used to train the best models in Ta-
ble 1. The results are summarized in Table 4,
where we report the average top-1 error over
five runs. the gain is calculated as a difference
between this results and the average top-1 er-
ror of naive random sampling in Table 1. The
first column in Table 4 is the model the pre-
generated sample sequence is obtained from,
and the second column is the different archi-
Table 4: Cross-architecture experiment results.
Sequence from	→ Applied to	Top-1 Error±SD	Gain
ResNet-20	→ VGG-16	9.094±0.170	-0.012
WRN-28-10	→ VGG-16	9.080±0.040	0.002
ResNet-20	→ VGG-16-BN	7.826±0.043	0.190
WRN-28-10	→ VGG-16-BN	7.622±0.031	0.394
VGG-16	→ ResNet-20	8.204±0.114	0.454
VGG-16-BN	→ ResNet-20	8.104±0.201	0.554
WRN-28-10	→ ResNet-20	8.102±0.113	0.556
VGG-16-BN	→ WRN-28-10	3.733±0.074	0.323
ResNet-20	→ WRN-28-10	3.656±0.078	0.400
tecture that the same sequence is applied to for
training. Even though the architectures are different from the models for which the training sample
sequences are optimized, again, the pre-sampled sequence provides consistent performance gain,
except for the case when VGGNet without BN is a recipient of the training sequence. However,
the optimized sequence for VGGNet-16 without BN is still effective in training other networks with
BN, e.g., ResNet and WRN.
7
Under review as a conference paper at ICLR 2021
Figure 3: The sample distribution induced by our RLSampler. (a) and (b) show class and sample frequency of
the entire training sequence of the best ResNet-20 model on CIFAR-10 in Tabel 1, respectively. For the sample
frequency in (b), with the maximum and the minimum, We also provide the 10-th, 50-th (median), and 90-th
percentiles. We report the percentage of deviation of the two extrema from the ideal mean under the uniform
distribution. (c) and (d) show the most and the least fetched images from the same RLSampler, respectively.
6.2	INTERNAL Structure of the Sampled Sequence
Emerged Nonuniform Sample Weights Figures 3a and 3b visualize the class and the sample dis-
tributions generated by RLSampler. RLSampler samples each class with a relatively small variance;
in CIFAR-10, the frequency difference between the most and the least sampled classes are less than
1%. However, RLSampler develops a clear preference over particular samples, even though we do
not introduce any prior knowledge of the data distribution. As shown in Figure 3b, the top 10% of
the samples are fetched at least 15% more frequently than the bottom 10%.
Agreements with Previous Intuitions We are interested in the relationship between the distribu-
tional properties that emerge by training RLSampler and the heuristics in Section 4 for designing
intelligent sampling methods. We use 10 differently initialized ResNet-20 trained for only 5 epochs
as the baseline to evaluate the intermediate statistics in Figure 4. We plot the relationship between the
sample frequency and per-sample training loss (Loshchilov & Hutter (2016)), as well as per-sample
gradient norm (Katharopoulos & Francois (2018)) in Figures 4a and 4b, respectively. As results of
a simple linear regression show that learned sample weights of our RLSampler do not correlate with
these factors with a per-sample basis. Moreover, as in Figure 4c, statistics of intermediate gradient
norms of the learner with respect to the generated mini-batches by the RLSampler is indistinguish-
able from the ones from a naive random sampling. Although this does not falsifies the intuitions of
the previous methods (Loshchilov & Hutter (2016); Katharopoulos & Francois (2018)), the results
strongly implies the existence of other factors determining the goodness of a sample sequence.
(a) vs. training loss.
(b) vs. gradient norm.
(c) Histogram of gradient norms.
Figure 4: (a) and (b): Empirical sampling probability of RLSampler compared to the average training loss and
the average L2 gradient norm of ten different ResNet-20 fixed after being trained for 5 epochs, respectively.
(c): Histogram of the L2 gradient norms with respect to sample mini-batches from two different samplers,
calculated on the same networks as in (a) and (b). The batch size is the same (128) as the experiments in
Table 1. The data for the histogram in (c) is obtained by collecting the gradient norms of the fixed network fed
by the first 4k mini-batches fetched from each pre-generated sequence.
7	CONCLUSION
We investigated using a trainable adaptive dataset sampling module for a deep learning framework.
We constructed an effective model-agnostic adaptive sampler, dubbed RLSampler, based on a deep
policy gradient algorithm. RLSampler shows clear improvements of generalization errors on image
classification benchmarks. Additional experiments on the trained RLSampler strongly imply the
existence of a model-agnostic optimal sampling sequence of a given dataset for deep learning.
8
Under review as a conference paper at ICLR 2021
References
Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici, Balakrishnan
Varadarajan, and Sudheendra Vijayanarasimhan. YouTube-8M: A large-scale video classification
benchmark. arXiv preprints, September 2016.
Guillaume Alain, Alex Lamb, Chinnadhurai Sankar, Aaron Courville, and Yoshua Bengio. Variance
reduction in SGD by distributed importance sampling. In ICLR Workshop, 2016.
Setareh Ariafar, Zelda Mariet, Ehsan Elhamifar, Dana Brooks, Jennifer Dy, and Jasper Snoek. Faster
& more reliable tuning of neural networks: Bayesian optimization with importance sampling.
2020. Technical Report.
Y. Bengio and J. Senecal. Adaptive importance sampling to accelerate training of a neural proba-
bilistic language model. IEEE Transactions on Neural Networks,19(4):713-722, 2008.
Yoshua Bengio, Jerome Louradour, and Ronan Collobert. Curriculum learning. In ICML, 2009.
Johan Bjorck, Carla Gomes, Bart Selman, and Kilian Q. Weinberger. Understanding batch normal-
ization. In NIPS, 2018.
Antoine Bordes, Seyda Ertekin, Jason Weston, and Leon Bottou. Fast kernel classifiers with online
and active learning. Journal of Machine Learning Research, 6:1579 - 1619, September 2005.
Haw-Shiuan Chang, Erik Learned-Miller, and Andrew McCallum. Active bias: Training more
accurate neural networks by emphasizing high variance samples. In NIPS, 2017.
Dominik Csiba and Peter Richtarik. Importance sampling for minibatches. Journal of Machine
Learning Research, 19(1):1 - 21, August 2018.
MiChaI Derezinski, Daniele Calandriello, and Michal Valko. Exact sampling of determinantal point
processes with sublinear time preprocessing. In NIPS, 2019.
Yang Fan, Fei Tian, Tao Qin, Jiang Bian, and Tie-Yan Liu. Learning what data to learn. In ICLR
Workshop, 2017.
Meng Fang, Tianyi Zhou, Yali Du, Lei Han, and Zhengyou Zhang. Curriculum-guided hindsight
experience replay. In NIPS, 2019.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In AISTATS, 2010.
Alex Graves, Marc G. Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu. Automated
curriculum learning for neural networks. In ICML, 2017.
Guy Hacohen and Daphna Weinshall. On the power of curriculum learning in training deep net-
works. In ICML, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, 2016.
Sepp Hochreiter and JUrgen Schmidhuber. Flat minima. Neural Computation, 9(1):1 - 42, 1997a.
Sepp Hochreiter and JUrgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):
1735 - 1780, 1997b.
Wanming Huang, Richard Yi Da Xu, and Ian Oppermann. Efficient diversified mini-batch selection
using variable high-layer features. In ACML, 2019.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In ICML, 2015.
Tyler B. Johnson and Carlos Guestrin. Training deep models faster with robust, approximate impor-
tance sampling. In NIPS, 2018.
9
Under review as a conference paper at ICLR 2021
Angelos KatharoPoUlos and FleUret Francois. Not all samples are created equal: Deep learning with
importance sampling. In ICML, 2018.
Nitish Shirish Keskar, Dheevatsa MUdigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In
ICLR, 2017.
Diederik P. Kingma and Jimmy Lei Ba. Adam: a method for stochastic optimization. In ICLR, 2015.
Alex Krizhevsky. Learning mUltiple layers of featUres from tiny images. 2009. Technical Report.
M. Pawan KUmar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable
models. In NIPS, 2010.
X. Li and Y. GUo. Adaptive active learning for image classification. In CVPR, 2013.
Ilya Loshchilov and Frank HUtter. Online batch selection for faster training of neUral networks. In
ICLR Workshop, 2016.
Deanna Needell, Rachel Ward, and Nati Srebro. Stochastic gradient descent, weighted sampling,
and the randomized Kaczmarz algorithm. In NIPS, 2014.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James BradbUry, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, LUca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank ChilamkUrthy, Benoit Steiner,
LU Fang, JUnjie Bai, and SoUmith Chintala. PyTorch: an imperative style, high-performance deep
learning library. In NIPS, 2019.
XinyU Peng, Li Li, and Fei-YUe Wang. Accelerating minibatch stochastic gradient descent Using typ-
icality sampling. IEEE Transactions on Neural Networks and Learning Systems (Early Access),
pp. 1-11, December 2019.
HieU Pham, Melody Y. GUan, Barret Zoph, QUoc V. Le, and Jeff . Efficient neUral architectUre
search via parameter sharing. In ICML, 2018.
Alec Radford, Jeffrey WU, Rewon Child, David LUan, Dario Amodei, and Ilya SUtskever. LangUage
models are UnsUpervised mUltitask learners. OpenAI Blog, FebrUary 2019. Technical Report.
Mengye Ren, WenyUan Zeng, Bin Yang, and RaqUel UrtasUn. Learning to reweight examples for
robUst deep learning. In ICML, 2018.
Olga RUssakovsky, Jia Deng, Hao SU, Jonathan KraUse, Sanjeev Satheesh, Sean Ma, Zhiheng
HUang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet large scale visUal recognition challenge. IJCV, 115(3):211-252, 2015.
Shibani SantUrkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch nor-
malization help optimization? In NIPS, 2018.
Tom SchaUl, John QUan, Ioannis AntonogloU, and David Silver. Prioritized experience replay. In
ICLR, 2016.
Abhinav Shrivastava, Abhinav GUpta, and Ross Girshick. Training region-based object detectors
with online hard example mining. In CVPR, 2016.
Karen Simonyan and Andrew Zisserman. Very deep convolUtional networks for large-scale image
recognition. In ICLR, 2015.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya SUtskever, and RUslan SalakhUtdinov.
DropoUt: A simple way to prevent neUral networks from overfitting. Journal of Machine Learning
Research, 15(56):1929-1958, 2014.
Richard S. SUtton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press,
2 edition, November 2018. ISBN 9780262039246.
10
Under review as a conference paper at ICLR 2021
Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine Learning, 8:229 - 256,1992.
D. Randall Wilson and Tony R. Martinez. The general inefficiency of batch training for gradient
descent learning. Neural Networks, 16(10):1429 - 1451, 2003.
Chao-YUan Wu, R. Manmatha, Alexander J. Smola, and PhiliPP KrahenbuhL Sampling matters in
deep embedding learning. In ICCV, 2017.
Sergey Zagoruyko and Komodakis Nikos. Wide residual networks. In BMVC, 2016.
Cheng Zhang, Cengiz Oztireli, StePhan Mandt, and GiamPiero Salvi. Active mini-batch samPling
using rePulsive Point Processes. In AAAI, 2019.
Peilin Zhao and Tong Zhang. Stochastic oPtimization with imPortance samPling for regularized loss
minimization. In ICML, 2015.
Barret ZoPh and Quoc V. Le. Neural architecture search with reinforcement learning. In ICML,
2017.
11
Under review as a conference paper at ICLR 2021
A Implementation Details
In addition to Section 3, we further describe the implementation details of RLSampler models. First,
the detailed behavior of each recurrent module of the RLSampler is provided. Next, we provide the
hyperparameters used in the experiments in Sections 5 and 6.
A.1 Controller Architecture
As described in Section 3.2, our RLSampler Sφ consists of two LSTM (Hochreiter & Schmidhuber
(1997b)) networks: a history logger and a policy network. The history logger consists of two LSTM
cells, followed by a single linear decoder. Each LSTM cell lstmi, i ∈ {1, 2} has a hidden state
vector h(i) and a cell state vector c(i) which are fed into the LSTM cell along with the external input
x(i), i.e., at iteration t - 1,
(ht(i), ct(i)) = lstmi(xt(i-)1, h(ti-)1, c(ti-)1).	(7)
At the beginning and every policy update, these state vectors h(i) , c(i) of each LSTM cell of the
history logger is reset to zero to limit the amount of historical gradients to store. At iteration t - 1,
logits lt-ι = (lt-1,1, lt-1,2, ∙∙∙ , lt-ι,k) from the learner network is averaged batch-wise and fed
into the input of the first LSTM cell lstm1. The second LSTM cell lstm2 receives the hidden state
h1 of the first cell lstm1 as an input. The hidden state h2 of the second LSTM cell lstm2 is then
fed into the decoder dec to produce the current historical summary. Therefore, the internal logic of
the equation (3), or a single-step update of the history logger, is:
(ht1), CtI)) = lstmI (1 X Ij, h(-)ι,c(-)ι],
k j∈Bt-1
(h(t2),ct(2)) = lstm2 ht(1),ht(-2)1,ct(2-)1 ,
st = dec x(ti-)1, h(ti-)1, ct(i-)1 ,
where st is the summary of the history of the learner network states available at iteration t, and
= |B| is a batch size. We initialize each bias term of the LSTM cells and the linear layer to be
zero. Input-to-hidden weights of the LSTM cells are initialized with Xavier initialization (Glorot
& Bengio (2010)), and hidden-to-hidden weights of the LSTM cells are initialized to a random
orthogonal matrix. The linear layer weight is initialized with zero-mean normal distribution with a
standard deviation of 1e-3.
The policy network has an identical architecture as the aforementioned history logger, except it has
a linear layer (an encoder) at the input. Each layer and the hidden states of the policy network are
initialized exactly the same as the history logger except for the linear layer weights, which are ini-
tialized with zero-mean normal distribution with standard deviation of 1e-5. The policy network
receives only the historical summary and outputs a logit vector corresponding to the sampling prob-
ability of each item in the training subset. We use the Adam optimizer (Kingma & Ba (2015)) to
train both submodules Sφ = (η, π), with the default parameters β1 = 0.9, β2 = 0.999, = 1e-8,
and zero weight decay.
A.2 Hyperparameters
The hyperparameters are chosen by manual searching based on the performance of a 20-layer
ResNet (He et al. (2016)) on CIFAR-10 (Krizhevsky (2009)) dataset. For the hyperparameter search,
we split the 50k training set into training/validation set containing 45000/5000 items. After the hy-
perparameters are chosen, they are fixed for all the experiments and analyses in Sections 5 and 6.
The only difference in hyperparameters is the size of the validation subset |Xv|, which is affected by
the number of classes the dataset X has. The effectiveness of the chosen architecture and the hyper-
parameters across various learner models proves that RLSampler can be viewed as a model-agnostic
extension to the existing deep learning frameworks. The chosen hyperparameters are presented in
Table 5.
12
Under review as a conference paper at ICLR 2021
Table 5: Hyperparameters of RLSampler used for evaluation.
Parameter name	Parameter value
Validation subset size, |Xv |	640 (cifar-10) or 1280 (cifar-100)
Reward period, T2	20
Policy update period, T1	100
Summary dimension, dim(s)	64
Hidden state dimension of the history logger, dim(hη)	256
Hidden state dimension of the policy network, dim(hπ)	64
Reward decay rate, λ	0.1
Sampler optimizer	ADAM(β1 = 0.9, β2 = 0.999, = 1e-8)
Sampler learning rate	0.04
Sampler scheduling	Uniform
B Experimental Settings
In this section, details of the experimental settings of Sections 5 and 6 are provided. For the eval-
uation in Section 5.1, we choose convolutional neural network models that have achieved previous
state-of-the-art in major challenge, especially the ImageNet Large Scale Visual Recognition Chal-
lenge (ILSVRC, Russakovsky et al. (2015)). The models used in the evaluations are summarized
in Table 6. To show the versatility of our RLSampler, we use an identical sampler architecture
across all experiments. We also match each component of a learning framework, i.e., a loss function
including weight decay, an image preprocessing procedure, an optimization algorithm, a learning
rate schedule, the size of a mini-batch, and the total number of iterations, to those mentioned in
the original papers (Simonyan & Zisserman (2015); He et al. (2016); Zagoruyko & Nikos (2016)).
For training, we prepare both datasets by normalizing with the data mean and standard deviation,
applying random horizontal flips and random crop with 4-pixel pad on every side. Especially, we
use reflective padding to augment images for WRN-28-2 and WRN-28-10; otherwise, we use zero
padding. Note that this augmentation is applied to both splits X = (Xt, Xv) of the training set.
Behaviors of dropout (Srivastava et al. (2014)) and batch normalization (Ioffe & Szegedy (2015))
layers are also the same in both splits (Xt, Xv). For the test set Xtest, we only do the normalization.
However, there are few differences. For ResNets (He et al. (2016)) we trained longer than the orig-
inal version, with a modified learning rate schedule. In addition, Simonyan & Zisserman (2015) do
not provide evaluations of VGGNets on CIFAR benchmarks; therefore, we use the model designed
for ImageNet (Russakovsky et al. (2015)) with fully connected layers modified to have lower di-
mensions. We reduce the dimension of three fully connected layers of a VGGNet to be 512. The
overall optimization settings are summarized in Table 7.
For RLSampler, a train/validation split of 49360/640 for CIFAR-10 and 48720/1280 for CIFAR-
100 is used. This means that the sampler is trained with 5 validation batches for CIFAR-10 and 10
batches for CIFAR-100. We simply use the samples with the latest indices in the original training
set X as the validation samples. For fair comparison, we match the number of learner updates t
for the time horizon T0 as well as the learning rate schedule between the same models trained with
different samplers. Every instance of experiments is done on a single NVIDIA RTX 2080 Ti GPU.
Table 6: Learner models used for evaluation and their key features.
Model name	# Layers	BatchNorm	Residual Connection	Dropout	Reference
VGGNet-16	16	X	X	0.5	Simonyan & Zisserman (2015)
VGGNet-16 + BN	16	✓	X	0.5	Ioffe & Szegedy (2015)
ResNet-20	20	✓	✓	X	He et al. (2016)
WRN-28-2	28	✓	✓	X	Zagoruyko & Nikos (2016)
WRN-28-10	28	✓	✓	X	Zagoruyko & Nikos (2016)
13
Under review as a conference paper at ICLR 2021
Table 7: Detailed experimental settings for each of the main evaluations. We use the exact same
settings for the experiments on CIFAR-10 and on CIFAR-100, except for the size of the validation
subset Xv.
	VGGNet-16	VGGNet-16 + BN	ResNet-20	WRN-28-2	WRN-28-10
# training steps, T0	80k	80k	100k	50k	78k
Base LR	0.01	0.01	0.1	0.1	0.1
LR schedule	[40k, 60k]	[40k, 60k]	[50k, 75k]	[20k, 40k]	[23k, 47k, 63k]
LR decay coefficient	0.1	0.1	0.1	0.2	0.2
Nesterov momentum	X	X	X	✓	✓
Momentum	0.9	0.9	0.9	0.9	0.9
Weight decay	1e-4	1e-4	1e-4	5e-4	5e-4
14