Under review as a conference paper at ICLR 2021
Learning and Generalization in Univariate
Overparameterized Normalizing Flows
Anonymous authors
Paper under double-blind review
Ab stract
In supervised learning, it is known that overparameterized neural networks with
one hidden layer provably and efficiently learn and generalize, when trained using
Stochastic Gradient Descent (SGD). In contrast, the benefit of overparameterization
in unsupervised learning is not well understood. Normalizing flows (NFs) learn to
map complex real-world distributions into simple base distributions and constitute
an important class of models in unsupervised learning for sampling and density
estimation. In this paper, we theoretically and empirically analyze these models
when the underlying neural network is one hidden layer overparametrized network.
On the one hand, we provide evidence that for a class of NFs, overparametrization
hurts training. On the other hand, we prove that another class of NFs, with similar
underlying networks, can efficiently learn any reasonable data distribution under
minimal assumptions. We extend theoretical ideas on learning and generalization
from overparameterized neural networks in supervised learning to overparameter-
ized normalizing flows in unsupervised learning. We also provide experimental
validation to support our theoretical analysis in practice.
1 Introduction
Neural network models trained using simple first-order iterative algorithms have been very effective
in both supervised and unsupervised learning. Theoretical reasoning of this phenomenon requires one
to consider simple but quintessential formulations, where this can be demonstrated by mathematical
proof, along with experimental evidence for the underlying intuition. First, the minimization of
training loss is typically a non-smooth and non-convex optimization over the parameters of neural
networks, so it is surprising that neural networks can be trained efficiently by first-order iterative
algorithms. Second, even large neural networks whose number parameters are more than the size of
training data often generalize well with a small loss on the unseen test data, instead of overfitting the
seen training data. Recent work in supervised learning attempts to provide theoretical justification for
why overparameterized neural networks can train and generalize efficiently in the above sense.
In supervised learning, the empirical risk minimization with quadratic loss is a non-convex optimiza-
tion problem even for a fully connected neural network with one hidden layer of neurons with ReLU
activations. Around 2018, it was realized that when the hidden layer size is large compared to the
dataset size or compared to some measure of complexity of the data, one can provably show efficient
training and generalization for these networks, e.g. Jacot et al. (2018); Li & Liang (2018); Du et al.
(2018); Allen-Zhu et al. (2019); Arora et al. (2019). Of these, Allen-Zhu et al. (2019) is directly
relevant to our paper and will be discussed later.
The role of overparameterization, and provable training and generalization guarantees for neural
networks are less well understood in unsupervised learning. Generative models or learning a data
distribution from given samples is an important problem in unsupervised learning. Popular generative
models based on neural networks include Generative Adversarial Networks (GANs) (e.g., Goodfellow
et al. (2014)), Variational AutoEncoders (VAEs) (e.g., Kingma & Welling (2014)), and Normalizing
Flows (e.g., Rezende & Mohamed (2015)). GANs and VAEs have shown impressive capability to
generate samples of photo-realistic images but they cannot give probability density estimates for new
data points. Training of GANs and VAEs has various additional challenges such as mode collapse,
posterior collapse, vanishing gradients, training instability, etc. as shown in e.g. Bowman et al.
(2016); Salimans et al. (2016); Arora et al. (2018); Lucic et al. (2018).
1
Under review as a conference paper at ICLR 2021
In contrast to the generative models such as GANs and VAEs, when normalizing flows learn dis-
tributions, they can do both sampling and density estimation, leading to wide-ranging applications
as mentioned in the surveys by Kobyzev et al. (2020) and Papamakarios et al. (2019). Theoreti-
cal understanding of learning and generalization in normalizing flows (more generally, generative
models and unsupervised learning) is a natural and important open question, and our main technical
contribution is to extend known techniques from supervised learning to make progress towards
answering this question. In this paper, we study learning and generalization in the case of univariate
overparameterized normalizing flows. Restriction to the univariate case is technically non-trivial and
interesting in its own right: univariate ReLU networks have been studied in recent supervised learning
literature (e.g., Savarese et al. (2019), Williams et al. (2019), Sahs et al. (2020) and Daubechies et al.
(2019)). Multidimensional flows are qualitatively more complex and our 1D analysis sheds some
light on them (see Sec. 4). Before stating our contributions, we briefly introduce normalizing flows;
details appear in Section 2.
Normalizing Flows. We work with one-dimensional probability distributions with continuous
density. The general idea behind normalizing flows (NFs), restricted to 1D can be summarized
as follows: Let X ∈ R be a random variable denoting the data distribution. We also fix a base
distribution with associated random variable Z which is typically standard Gaussian, though in this
paper we will work with the exponential distribution as well. Given i.i.d. samples of X, the goal is to
learn a continuous strictly monotone increasing map fX : R → R that transports the distribution of
X to the distribution of Z: in other words, the distribution of fX-1(Z) is that of X. The learning of
fX is done by representing it by a neural network and setting up an appropriate loss function.
The monotonicity requirement on f which makes f invertible, while not essential, greatly simplifies
the problem and is present in all the works we are aware of. It is not clear how to set up a tractable
optimization problem without this requirement. Since the function represented by standard neural
networks are not necessarily monotone, the design of the neural net is altered to make it monotone.
For our 1D situation, one-hidden layer networks are of the form N(x) = Pim=1 aiσ(wix + bi), where
m is the size of the hidden layer and the ai , wi , bi are the parameters of the network.
We will assume that the activation functions used are monotone. Here we distinguish between two
such alterations: (1) Changing the parametrization of the neural network. This can be done in multiple
ways: instead of ai, wi we use ai2, wi2 (or other functions, such as the exponential function, of ai, wi
that take on only positive values) (Huang et al., 2018; Cao et al., 2019). This approach appears to
be the most popular. In this paper, we also suggest another related alteration: we simply restrict the
parameters ai , wi to be positive. This is achieved by enforcing this constraint during training. (2)
Instead of using N(x) for f (x) We use φ(N(x)) for f(x) = dX, where φ : R → R+ takes on only
positive values. Positivity of f0 implies monotonicity of f . Note that no restrictions on the parameters
are required; however, because we parametrize f0, the function f needs to be reconstructed using
numerical quadrature. This approach is used by Wehenkel & Louppe (2019).
We will refer to the models in the first class as constrained normalizing flows (CNFs) and those in the
second class as unconstrained normalizing flows (UNFs).
Our Contributions. In this paper, we study both constrained and unconstrained univariate NFs
theoretically as well as empirically. The existing analyses for overparametrized neural networks
in the supervised setting work with a linear approximation of the neural network, termed pseudo
network in Allen-Zhu et al. (2019). They show that (1) there is a pseudo network with weights close
to the initial ones approximating the target function, (2) the loss surfaces of the neural network and
the pseudo network are close and moreover the latter is convex for convex loss functions. This allows
for proof of the convergence of the training of neural network to global optima. One can try to
adapt the approach of using a linear approximation of the neural network to analyze training of NFs.
However, one immediately encounters some new roadblocks: the loss surface of the pseudo networks
is non-convex in both CNFs and UNFs.
In both cases, we identify novel variations that make the optimization problem for associated pseudo
network convex: For CNFs, instead of using ai2 , wi2 as parameters, we simply impose the constraints
ai ≥ and wi ≥ for some small constant . The optimization algorithm now is projected SGD,
which in this case incurs essentially no extra cost over SGD due to the simplicity of the positivity
constraints. Apart from making the optimization problem convex, in experiments this variation
2
Under review as a conference paper at ICLR 2021
slightly improves the training of NFs compared to the reparametrization approaches, and may be
useful in practical settings.
Similarly, for UNFs we identify two changes from the model of Wehenkel & Louppe (2019) that
make the associated optimization problem convex, while still retaining empirical effectiveness: (1)
Instead of ClenshaW-Curtis quadrature employed in Wehenkel & LouPPe (2019) which uses positive
and negative coefficients, we use the simple rectangle quadrature which uses only positive coefficients.
This change makes the model somewhat slow (it uses twice as many samples and time to get similar
performance on the examples we tried). (2) Instead of the standard Gaussian distribution as the base
distribution, we use the exponential distribution. In experiments, this does not cause much change.
Our results point to a dichotomy between these two classes of NFs: our variant of UNFs can be
theoretically analyzed when the networks are overparametrized to prove that the UNF indeed learns
the data distribution. To our knowledge, this is the first “end-to-end” analysis of an NF model, and a
neural generative model using gradient-based algorithms used in practice. This proof, while following
the high-level scheme of Allen-Zhu et al. (2019) proof, has a number of differences, conceptual as
well as technical, due to different settings. E.g., our loss function involves a function and its integral
estimated by quadrature.
On the other hand, for CNFs, our empirical and theoretical findings provide evidence that over-
parametrization makes training slower to the extent that models of similar size which learn the
data distribution well for UNFs, fail to do so for CNFs. We also analyze CNFs theoretically in
the overparametrized setting and point to potential sources of the difficulty. The case of moderate-
sized networks, where training and generalization do take place empirically, is likely to be difficult
to analyze theoretically as presently this setting is open for the simpler supervised learning case.
We hope that our results will pave the way for further progress. We make some remarks on the
multidimensional case in Sec. 4. In summary, our contributions include:
•	To our knowledge, first efficient training and generalization proof for NFs (in 1D).
•	Identification of architectural variants of UNFs that admit analysis via overparametrization.
•	Identification of “barriers” to the analysis of CNFs.
Related Work. Previous work on normalizing flows has studied different variants such as planar
and radial flows in Rezende & Mohamed (2015), Sylvester flow in van den Berg et al. (2018),
Householder flow in Tomczak & Welling (2016), masked autoregressive flow in Papamakarios et al.
(2017). Most variants of normalizing flows are specific to certain applications, and the expressive
power (i.e., which base and data distributions they can map between) and complexity of normalizing
flow models have been studied recently, e.g. Kong & Chaudhuri (2020) and Teshima et al. (2020).
Invertible transformations defined by monotonic neural networks can be combined into autoregressive
flows that are universal density approximators of continuous probability distributions; see Masked
Autoregressive Flows (MAF) Papamakarios et al. (2017), UNMM-MAF by Wehenkel & Louppe
(2019), Neural Autoregressive Flows (NAF) by Huang et al. (2018), Block Neural Autoregressive
Flow (B-NAF) by Cao et al. (2019). Unconstrained Monotonic Neural Network (UMNN) models
proposed by Wehenkel & Louppe (2019) are particularly relevant to the technical part of our paper.
Lei et al. (2020) show that when the generator is a two-layer tanh, sigmoid or leaky ReLU network,
Wasserstein GAN trained with stochastic gradient descent-ascent converges to a global solution
with polynomial time and sample complexity. Using the moments method and a learning algorithm
motivated by tensor decomposition, Li & Dou (2020) show that GANs can efficiently learn a large
class of distributions including those generated by two-layer networks. Nguyen et al. (2019b) show
that two-layer autoencoders with ReLU or threshold activations can be trained with normalized
gradient descent over the reconstruction loss to provably learn the parameters of any generative
bilinear model (e.g., mixture of Gaussians, sparse coding model). Nguyen et al. (2019a) extend
the work of Du et al. (2018) on supervised learning mentioned earlier to study weakly-trained (i.e.,
only encoder is trained) and jointly-trained (i.e., both encoder and decoder are trained) two-layer
autoencoders, and show joint training requires less overparameterization and converges to a global
optimum. The effect of overparameterization in unsupervised learning has also been of recent interest.
Buhai et al. (2020) do an empirical study to show that across a variety of latent variable models
and training algorithms, overparameterization can significantly increase the number of recovered
ground truth latent variables. Radhakrishnan et al. (2020) show that overparameterized autoencoders
3
Under review as a conference paper at ICLR 2021
and sequence encoders essentially implement associative memory by storing training samples as
attractors in a dynamical system.
Outline. A brief outline of our paper is as follows. Section 2 contains preliminaries and an
overview of our results about constrained and unconstrained normalizing flows. Appendix B shows
the existence of a pseudo network whose loss closely approximates the loss of the target function.
Appendix C shows the coupling or closeness of their gradients over random initialization. Appendices
D and E contain complete proofs of our optimization and generalization results, respectively. Section
3 and Appendix G contain our empirical studies towards validating our theoretical results.
2 Preliminaries and Overview of Results
We confine our discussion to the 1D case which is the focus of the present paper. The goal of NF is to
learn a probability distribution given via i.i.d. samples data. We will work with distributions whose
densities have finite support, and assumed to be [-1, 1], without loss of generality. Let X be the
random variable corresponding to the data distribution we want to learn. We denote the probability
density (we often just say density) of X at u ∈ R by pX (u). Let Z be a random variable with either
standard Gaussian or the exponential distribution with λ = 1 (which we call standard exponential).
Recall that the density of the standard exponential distribution at u ∈ R is given by e-u for u ≥ 0
and 0 for u < 0.
Let f : R → R bea strictly increasing continuous function. Thus, f is invertible. We use f0(x)=票
to denote the derivative. Letpf,z(∙) be the density of the random variable f T(Z). Let X = f T (z),
for z ∈ R. Then by the standard change of density formula using the monotonicity of f gives
pf,Z(x) = pZ(z)f0(x).	(2.1)
We would like to choose f so that pf,Z = pX, the true data density. It is known that such an f always
exists and is unique; see e.g. Chapter 2 of Santambrogio (2015). We will refer to the distribution of Z
as the base distribution. Note that if we can find f, then we can generate samples of X using f-1(Z)
since generating the samples of Z is easy. Similarly, we can evaluate pX (x) = pZ(f-1(z))f0(x)
using (2.1). To find f from the data, we set up the maximum log-likelihood objective:
1n
max 一 Σ log pf,Z (xi)
f n i=1
1
max 一
fn
nn
XlogpZ(f(xi)) + Xlogf0(xi)
i=1	i=1
(2.2)
where S = {x1, . . . , xn} ⊂ R contains i.i.d. samples of X, and the maximum is over continuous
strictly increasing functions. When Z is standard exponential, the optimization problem (2.2) becomes
minL(f,S), where L(f,S) = 1 X L(f,x) and L(f,x) = f (x) - logf0(x).	(2.3)
f	n x∈S
A similar expression, with f(x)2/2 replacing f (x), holds for the standard Gaussian. We denote the
loss for standard Gaussian as LG(f, x).
Informally, one would expect that as n → ∞, for the optimum f in the above optimization problems
pf,Z → pX . To make the above optimization problem tractable, instead of f we use a neural network
N . We consider one-hidden layer neural networks with the following basic form which will then be
modified according to whether we are constraining the parameters or the output.
m
N(x) =	ar0 ρ ((wr0 + wr)x + (br + br0)) .	(2.4)
r=1
Here m is the size of the hidden layer, ρ : R → R is a monotonically increasing activation function,
the weights ar0 , wr0 , br0 are the initial weights chosen at random according to some distribution,
and wr , br are offsets from the initial weights. We will only train the wr , br and the ar0 will remain
frozen to their initial values.
Let θ = (W, B) ∈ R2m denote the parameters W = (w1, w2, ..., wm) ∈ Rm and B =
(b1, b2, ..., bm) ∈ Rm of the neural network. We use Stochastic Gradient Descent (SGD) to up-
date the parameters of neural networks. Denote by θt = (Wt, Bt) with Wt = (w1t , w2t , ..., wmt ) and
4
Under review as a conference paper at ICLR 2021
Bt = (bt1, bt2, ..., btm) the parameters at time step t = 1, 2, . . ., and the corresponding network by
Nt(x). The SGD updates are given by θt+1 = θt - η VθLs(Nt,χt] Where η > 0 is learning rate,
and Ls(Nt, xt) is a loss function, and xt ∈ S is chosen uniformly randomly at each time step. For
supervised learning Where We are given labeled data {(x1, y1), . . . , (xn, yn)}, one often Works With
the mean square loss Ls(Nt) = ɪ PNi Ls(Nt,xi) With Ls(Nt,xi) = (Nt(Xi) — yi)2.
We noW very briefly outline the proof technique of Allen-Zhu et al. (2019) for analyzing training
and generalization for one-hidden layer neural netWorks for supervised learning. (While they Work
in a general agnostic learning setting, for simplicity, We restrict the discussion to the realizable
setting.) In their setting, the data x ∈ Rd is generated by some distribution D and the labels
y = h(x) are generated by some unknoWn function h : Rd → R. The function h is assumed to have
small “complexity” Ch Which in this case measures the required size of neural netWork With smooth
activations to approximate h.
The problem of optimizing the square loss is non-convex even for one-hidden layer netWorks. Allen-
Zhu et al. (2019) instead Work With pseudo network, P(x) Which is the linear approximation of N(x)
given by the first-order Taylor expansion of the activation:
m
P(x) =	ar0 (σ(wr0x + br0) + σ0 (wr0x + br0) (wrx + br)) .	(2.5)
r=1
Similarly to Nt We can also define Pt With parameters θt . They observe that When the netWork is
highly overparameterized, i.e. the netWork size m is sufficiently large compared to Ch, and the
learning rate is small, i.e. η = O(1/m), SGD iterates When applied to L(Nt) and L(Pt) remain
close throughout. Moreover, the problem of optimizing L(P) is a convex problem in θ and thus can
be analyzed With existing methods. They also shoW an approximation theorem stating that With high
probability there are neural network parameters θ* close to the initial parameters θ0 such that the
pseudo network with parameters θ* is close to the target function. This together with the analysis of
SGD shoWs that the pseudo netWork, and hence the neural netWork too, achieves small training loss.
Then by a Rademacher complexity argument they show that the neural network after T = O(Ch/2 )
time steps has population loss within of the optimal loss, thus obtaining a generalization result.
We will now describe how to obtain neural networks representing monotonically increasing functions
using the two different methods mentioned earlier, namely CNFs and UNFs.
2.1	Constrained Normalizing Flow
Note that if we have ar0 ≥ 0, wr0 + wr ≥ 0 for all r, then the function represented by the neural
network is monotonically increasing. We can ensure this positivity constraint by replacing ar0 and
wr0 +wr by their functions that take on only positive values. For example, the function x 7→ x2 would
give us the neural network N(x) = Prm=1 ar20 ρ((wr0 + wr)2x + br0 + br). Note that ar0, wr0 + wr
and br0 + br have no constraints, and so this network can be trained using standard gradient-based
algorithms. But first we need to specify the (monotone) activation ρ. Let σ(x) = x I [x ≥ 0] denote
the ReLU activation. If we choose ρ = σ, then note that in (2.3) we have
log f 0(x)	= log dNxx)	= log	(X a^ (wr0	+ Wr )2I [(Wr0	+ Wr )2X	+ br0	+ br	≥	θ]	∙
This is a discontinuous function in x as well as in wr and br . Gradient-based optimization algorithms
are not applicable to problems with discontinuous objectives, and indeed this is reflected in experi-
mental failure of such models in learning the distribution. By the same argument, any activation that
has a discontinuous derivative is not admissible. Activations which have continuous derivative but
are convex (e.g. ELU(x) given by ex - 1 for x < 0 and x for x ≥ 0)) also cannot be used because
then N(x) is also a convex function of x, which need not be the case for the optimal f. The oft-used
activation tanh does not suffer from either of these defects. Pseudo network with activation tanh is
given by
m
P(x) = ɪ2 a^ (tanh(w；ox + br0) + tanh0(wrox + bro) ((w； + 2wr0Wr) x + br)).
r=1
5
Under review as a conference paper at ICLR 2021
Note that P(x) is not linear in the parameters θ. Hence, it is not obvious that the loss function for
the pseudo network will remain convex in parameters; indeed, non-convexity can be confirmed in
experiments. A similar situation arises for exponential parameterization instead of square.
To overcome the non-convexity issue, we propose another formulation for constrained normalizing
flows. Here we retain the form of the neural network as in (2.4), but ensure the constraints ar0 ≥ 0
and wr0 ≥ 0 by the choice of the initialization distribution and wr0 + wr ≥ 0 by using projected
gradient descent for optimization.
m
N(x) =	ar0 tanh ((wr0 + wr) x + (br + br0)) , with constraints wr0 + wr ≥ , for all r.
r=1
Here, > 0 is a small constant ensuring strict monotonicity of N (x). Note that constraints in the
formulation are simple and easy to use in practice. The pseudo network in this formulation will be
m
P (x) = ɪ2 ar0 (tanh(Wr0X + brθ) + tanh0(Wr0X + brθ)(wr x + br)),
r=1
with constraints wr0 + wr ≥ , for all r. P(x) is linear in θ, therefore the objective function is
also convex in θ. Note that P(x) need not be forced to remain monotone using constraints: if
N(x) and P(x) are sufficiently close and N(x) is strictly monotone with not too small minx dNX),
then we will get monotonicity of P (x). Next, we point out that this formulation has a problem in
approximation of any target function by a pseudo network. We decompose P(x) into two parts:
P(χ) = Pc(x) + P'(x), where
m
m
Pc(x) = ɪ2 ar0 (tanh(wrox + brθ)) and P'(x) = ɪ2 ar0 (tanh0(wrox + brθ)(wrX + b)).
r=1
r=1
Note that Pc(x) only depends upon initialization and does not depend on wr and br. Hence, it can
not approximate the target function after the training, therefore P'(x) needs to approximate target
function with Pc(X) subtracted. Now, we will show that P'(x) can not approximate “sufficiently
non-linear” functions. The initialization distribution for wr0 is half-normal distribution with zero-
mean and variance= ^^ of normal distribution, i.e. Wro = X | where X has normal distribution
with the same parameters. The bias term bro follows normal distribution with 0 mean and -m
variance. Using the initialization, we can say that Wro and |bro| are O (√√gm) with high probability;
therefore, ∣Wrox + bro| is O (√√√∣m). Using the fact that tanh0(y) ≈ 1 for small y, we get that
tanh0 (wrox + bro) ≈ 1 for sufficient large m. In such cases, p` (x) becomes linear function in X
and won’t be able to approximate sufficiently non-linear function.
Note that this issue does not arise in pseudo network with ReLU activation because the derivative of
ReLU is discontinuous at 0 but as described earlier, for CNFs activations need to have continuous
derivative. The same issue in approximation arises for all activations with continuous derivative.
Using other variance of initializations leads to problem in other parts of the proof. This problem
remains if we use normal distribution initialization of Wro and bro with variance o (^m). For
normal distribution initialization of Wro and bro with variance Ω (10^) and O(1), successfully
training of CNFs to small training error can lose coupling between neural network N(X) and pseudo
network P (X). Please see Appendix F for more details. A generalization argument for activations
with continuous derivatives is not known even in the supervised case, therefore we do not work with
constrained normalizing flow. However, we show the effect of overparameterization for constrained
normalizing flow with tanh activation in experiments (Section 3).
2.2	Unconstrained Normalizing Flow
Unlike the constrained case, where we modeled f(X) using a neural network N (X), here we model
f0(X) using a neural network. Then we have f(X) = -x1f0(u) du. While this cannot be computed
exactly, good approximation can be obtained via numerical integration also known as numerical
quadrature of f0(X). The strict monotonicity of f is achieved by ensuring that f0(X) is always
6
Under review as a conference paper at ICLR 2021
positive. To this end a suitable nonlinearity is applied on top of the neural network: f0 (x) =
φ(N (x)), where N(x) is as in (2.4) with ρ = σ = ReLU, and φ is the function ELU + 1 given by
φ(x) = ex I [x < 0] + (x + 1) I [x ≥ 0]. Thus φ(x) > 0, for all x ∈ R, which means that f0 (x) > 0
for all x. Although this was the only property of ELU + 1 mentioned by Wehenkel & Louppe (2019),
it turns out to have several other properties which we will exploit in our proof: it is 1-Lipschitz
monotone increasing; its derivative is bounded from above by 1.
We denote by f (x) the estimate of f (x) = /二 f0(u)du obtained from f0(χ) via quadrature
f(χ) = PQ=I qif(τ (x)). Here Q is the number of quadrature points τι (x),... ,tq (x), and the
qι,... ,qQ ∈ R are the corresponding coefficients. Wehenkel & Louppe (2019) use ClenShaW-CUrtiS
quadrature where the coefficients qi can be negative.
We Will use simple rectangle quadrature, Which arises in Riemann integration, and uses only positive
coefficients: f(x) = ∆x [f0(-1 + ∆x) + f0(-1 + 2∆x)... + f0(x)], where ∆x = x++1. It is known
(see e.g. Chapter 5 in Atkinson (1989) for related results) that
∖f(x) - f (x)∣ ≤ M (Xr+ 1) , where M00 = max |f00(u)∣.
2Q	u∈[-1,x]
Compared to Clenshaw-Curtis quadrature, the rectangle quadrature requires more points for similar
accuracy (in our experiments this was about double). However, we use it because all the coefficients
are positive which helps make the problem of minimizing the loss a convex optimization problem.
Instead of using f, to which we do not have access, we use f in the loss function, denoting it L(f0, x)
for the standard exponential as the base distribution to write L(f0, x) = f(x) - log f0(x) and
L(f , S) = n ∑x∈s L(f , x). The loss LG(f0, x) for the standard GaUSSian as the base distribution
is defined similarly.
Let X be a random variable with density supported on [-1, 1]. Let the base distribution be the
standard exponential, and so Z will be a random variable with the standard exponential distribution.
And let F* : R → R be continuous monotone increasing such that F*-1(Z) has the same distribution
as X. Let S = {x1, . . . , xn} be a set of i.i.d. samples of X. Following Allen-Zhu et al. (2019), we
initialize a，ro 〜N(0,蜡),Wro 〜N(0, m1) and bro 〜N(0, .), where Ea > 0 is a small constant
to be set later. The SGD updates are given by θt+1 = θt - η VθL(ft, Xt) where ft(x) = φ(Nt(x)),
and xt ∈ S is chosen uniformly at random at each step. We can now state our main result.
Theorem 2.1 (informal statement of Theorem E.1). (loss function is close to optimal) For any E > 0
andfor any target function F* with finite second order derivative, hidden layer size m ≥ CI(F ),
the number of samples n ≥ C2 (F ) and the number of quadrature points Q ≥ C式F ), where
Cι(∙), C2(∙), C3(∙) are complexity measures, with probability at least 0.9, we have
Esgd
1 T-1
T EEx〜DL(ft,x)
T t=o
-Ex〜D [L(F*,x)] = O(e).
The complexity functions in the above statement have natural interpretations in terms of how
fast the function oscillates. Now recall that KL (PF*,z∣∣Pft,z) = EX log PF*,Z(X)), which gives
Esgd [⅛ PTo1 KL (PF*,z∣∣Pft,z)] = O(E)
Recall thatPf,Z(x) is the probability density off-1(Z).
Using Pinsker’s inequality, we can also bound the total variation distance between the learned and
data distributions Pft,Z andPF*,Z.
Define pseudo network g0(x), which acts as proxy for f0(x), as g0(x) = φ(P (x)). Note that
our definition of pseudo network is not the most straightforward version: g0(x) is not a linear
approximation of f0(x). As in Allen-Zhu et al. (2019), we begin by showing the existence of a pseudo
network close to the target function. However, for this we cannot use the approximation lemma in
Allen-Zhu et al. (2019) as it seems to require dimension at least 2. We use the recent result ofJi et al.
(2020) instead (Lemma B.1). The presence of both f0 and f and other differences in the loss function
leads to new difficulties in the analysis compared to the supervised case. We refer to the full proof
due to the lack of space.
7
Under review as a conference paper at ICLR 2021
Number of epochs
Mixture of Gaussian Distributions (l.r.=0.1)
	
		m=1600
^S «	60 S idβ^
Numberofepochs
Figure 1: Effect of over-parameterization on training of constrained normalizing flow on mixture of
Gaussian dataset for number of hidden layers m = 1600, 6400
3	Experiments
Full details of experimental setup and additional results on constrained normalizing flow as well as
results on unconstrained normalizing flow are given in appendix G.
3.1	Results for constrained normalizing flow
In Sec. 2.1, we suggested that high overparameterization may adversely affect training for constrained
normalizing flows. We now give experimental evidence for this. In Figs. 1, we see that as we increase
the learning rate, training becomes more stable for larger m. Note that for learning rate 0.025,
constrained normalizing flow with m = 1600 doesn’t learn anything due to small learning rate. We
observe that the L2-norms of Wt and Bt for m = 6400 are at least as large as those ofm = 1600. On
both datasets, as we increase the learning rate, L2-norm of Bt increases and learning of constrained
normalizing flow becomes more stable. These observations support our claim in Sec.2.1 that for
learning and approximation of overparameterized constrained normalizing flow, neural networks
need large L2-norms of Wt and Bt.
4	Conclusion
In this paper, we gave the first theoretical analysis of normalizing flows in the simple but instructive
univariate case. We gave empirical and theoretical evidence that overparametrized networks are
unlikely to be useful for CNFs. By contrast, for UNFs, overparametrization does not hurt and we can
adapt techniques from supervised learning to analyze two-layer (or one hidden layer) networks. Our
technical adaptations and NF variants may find use in future work.
Our work raises a number of open problems: (1) We made two changes to the unconstrained
flow architecture of Wehenkel & Louppe (2019). An obvious open problem is an analysis of the
original architecture or with at most one change. While the exponential distribution works well as
the base distribution, can we also analyze the Gaussian distribution? Similarly, Clenshaw-Curtis
quadrature instead of simple rectangle quadrature? These problems seem tractable but also likely
8
Under review as a conference paper at ICLR 2021
to require interesting new techniques as the optimization becomes non-convex. That would get
us one step closer to the architectures used in practice. (2) Analysis of constrained normalizing
flows. It is likely to be difficult because, as our results suggest, one needs networks that are not
highly overparametrized—this regime is not well-understood even in the supervised case. (3) Finally,
analysis of normalizing flows for the multidimensional case. Our 1D result brings into focus potential
difficulties: All unconstrained architectures seem to require more than one hidden layer, which poses
difficult challenges even in the supervised case. For CNFs, it is possible to design an architecture
with one hidden layer, but as we have seen in our analysis of CNFs, that is challenging too.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized
neural networks, going beyond two layers. In Advances in neural information processing systems,
pp. 6158-6169, 2019.
Sanjeev Arora, Andrej Risteski, and Yi Zhang. Do GANs learn the distribution? some theory
and empirics. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=BJehNfW0-.
Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In Kamalika
Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference
on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97
of Proceedings of Machine Learning Research, pp. 322-332. PMLR, 2019. URL http://
proceedings.mlr.press/v97/arora19a.html.
Kendall Atkinson. An Introduction to Numerical Analysis. John Wiley and Sons, 1989.
Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew Dai, Rafal Jozefowicz, and Samy Bengio.
Generating sentences from a continuous space. In Proceedings of The 20th SIGNLL Conference on
Computational Natural Language Learning, pp. 10-21, Berlin, Germany, August 2016. Association
for Computational Linguistics. doi: 10.18653/v1/K16-1002. URL https://www.aclweb.
org/anthology/K16-1002.
Rares-Darius Buhai, Andrej Risteski, Yoni Halpern, and David Sontag. Empirical study of benefits of
overparameterization in single-layer latent variable generative models. In Proceedings of the 37th
International Conference on Machine Learning, 2020. URL https://proceedings.icml.
cc/static/paper_files/icml/2020/5645-Paper.pdf.
Nicola De Cao, Wilker Aziz, and Ivan Titov. Block neural autoregressive flow. In Proceedings of the
Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI 2019, Tel Aviv, Israel, July 22-
25, 2019, pp. 511. AUAI Press, 2019. URL http://auai.org/uai2019/proceedings/
papers/511.pdf.
I. Daubechies, R. DeVore, S. Foucart, B. Hanin, and G. Petrova. Nonlinear approximation and (deep)
relu networks. ArXiv, abs/1905.02199, 2019.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In Proceedings of the 35th International Conference on
Learning Representations, 2018. URL https://arxiv.org/abs/1810.02054.
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In Zoubin Ghahramani,
Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger (eds.), Advances in
Neural Information Processing Systems 27: Annual Conference on Neural Information Processing
Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 2672-2680, 2014. URL
http://papers.nips.cc/paper/5423- generative- adversarial- nets.
Chin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron C. Courville. Neural autoregressive
flows. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th International
Conference on Machine Learning, ICML 2018, volume 80 of Proceedings of Machine Learning
Research, pp. 2083-2092. PMLR, 2018. URL http://proceedings.mlr.press/v80/
huang18d.html.
9
Under review as a conference paper at ICLR 2021
Arthur Jacot, Clement Hongler, and Franck Gabriel. Neural tangent kernel: Con-
vergence and generalization in neural networks. In Samy Bengio, Hanna M. Wal-
lach, Hugo Larochelle, Kristen Grauman, NicoIo Cesa-Bianchi, and Roman Garnett
(eds.), Advances in Neural Information Processing Systems 31: Annual Conference
on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018,
Montreal, Canada, pp. 8580-8589, 2018. URL http://papers.nips.cc/paper/
8076- neural- tangent- kernel- convergence- and- generalization- in- neural- networks.
Ziwei Ji, Matus Telgarsky, and Ruicheng Xian. Neural tangent kernels, transportation mappings,
and universal approximation. In 8th International Conference on Learning Representations, ICLR
2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020. URL https://arxiv.org/abs/
1910.06956.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and
Yann LeCun (eds.), 2nd International Conference on Learning Representations, ICLR 2014,
Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. URL http:
//arxiv.org/abs/1312.6114.
I. Kobyzev, S. Prince, and M. Brubaker. Normalizing flows: An introduction and review of current
methods. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.
Zhifeng Kong and Kamalika Chaudhuri. The expressive power of a class of normalizing flow models.
volume 108 of Proceedings of Machine Learning Research, pp. 3599-3609, Online, 26-28 Aug
2020. PMLR. URL http://proceedings.mlr.press/v108/kong20a.html.
Qi Lei, Jason D. Lee, Alexandros G. Dimakis, and Constantinos Daskalakis. SGD learns one-layer
networks in WGANs. In In Proceedings of the 37th International COnference on Machine Learning,
2020. URL https://proceedings.icml.cc/static/paper_files/icml/2020/
4998-Paper.pdf.
Yuan-Chuan Li and Cheh-Chih Yeh. Some equivalent forms of bernoulli’s inequality: A survey.
Applied Mathematics, 4(07):1070, 2013.
Yuanzhi Li and Zehao Dou. Making method of moments great again? - how can GANs learn the
target distribution, 2020. URL https://arxiv.org/abs/2003.04033.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochas-
tic gradient descent on structured data. In Advances in Neural Information Process-
ing Systems 31, pp. 8157-8166. 2018. URL http://papers.nips.cc/paper/
8038-learning-overparameterized-neural-networks-via-stochastic-gradient-descent-on
pdf.
Mario Lucic, Karol Kurach, Marcin Michalski, Olivier Bousquet, and Sylvain Gelly. Are gans
created equal? a large-scale study. In Proceedings of the 32nd International Conference on Neural
Information Processing Systems, NIPS’18, pp. 698-707, 2018.
Andreas Maurer. A vector-contraction inequality for rademacher complexities. In International
Conference on Algorithmic Learning Theory, pp. 3-17. Springer, 2016.
Thanh V. Nguyen, Raymond K. W. Wong, and Chinmay Hegde. Benefits of jointly training au-
toencoders: An improved neural tangent kernel analysis. CoRR, abs/1911.11983, 2019a. URL
http://arxiv.org/abs/1911.11983.
Thanh V. Nguyen, Raymond K. W. Wong, and Chinmay Hegde. On the dynamics of gradient
descent for autoencoders. In The 22nd International Conference on Artificial Intelligence and
Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan, volume 89 of Proceedings
of Machine Learning Research, pp. 2858-2867. PMLR, 2019b. URL http://proceedings.
mlr.press/v89/nguyen19a.html.
George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive flow for density
estimation. In Proceedings of the 31st International Conference on Neural Information Processing
Systems, NIPS’17, pp. 2335-2344, 2017. ISBN 9781510860964.
10
Under review as a conference paper at ICLR 2021
George Papamakarios, Eric T. Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Bal-
aji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. ArXiv,
abs/1912.02762, 2019.
Adityanarayanan Radhakrishnan, Mikhail Belkin, and Caroline Uhler. Overparameterized neural
networks can implement associative memory, 2020. URL https://arxiv.org/abs/1909.
12362.
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. volume 37 of
Proceedings ofMachine Learning Research, pp. 1530-1538, Lille, France, 07-09 Jul 2015. PMLR.
URL http://proceedings.mlr.press/v37/rezende15.html.
Justin Romberg. Maximum of a sequence of gaussian random variables. 2012. URL http:
//cnx.org/contents/8bd316d8-6442-4f5a-a597-aef1d6202f87@1.
Justin Sahs, Ryan Pyle, Aneel Damaraju, Josue Ortega Caro, Onur Tavaslioglu, Andy Lu, and Ankit
Patel. Shallow univariate relu networks as splines: Initialization, loss surface, hessian, gradient
flow dynamics, 2020.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen,
and Xi Chen. Improved techniques for training gans. In Advances in Neural Informa-
tion Processing Systems, pp. 2234-2242. 2016. URL http://papers.nips.cc/paper/
6125-improved-techniques-for-training-gans.pdf.
Filippo Santambrogio. Optimal Transport for Applied Mathematicians. Calculus of Variations, PDEs
andModeling. Birkhauser, 2015.
Pedro Savarese, Itay Evron, Daniel Soudry, and Nathan Srebro. How do infinite width bounded norm
networks look in function space? In Alina Beygelzimer and Daniel Hsu (eds.), Conference on
Learning Theory, COLT 2019, 25-28 June 2019, Phoenix, AZ, USA, volume 99 of Proceedings
of Machine Learning Research, pp. 2667-2690. PMLR, 2019. URL http://proceedings.
mlr.press/v99/savarese19a.html.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to
algorithms. Cambridge university press, 2014.
Takeshi Teshima, I. Ishikawa, Koichi Tojo, Kenta Oono, M. Ikeda, and M. Sugiyama. Coupling-based
invertible neural networks are universal diffeomorphism approximators. ArXiv, abs/2006.11469,
2020.
Jakub M Tomczak and Max Welling. Improving variational auto-encoders using householder flow.
arXiv preprint arXiv:1611.09630, 2016.
Rianne van den Berg, Leonard Hasenclever, Jakub Tomczak, and Max Welling. Sylvester normalizing
flows for variational inference. In proceedings of the Conference on Uncertainty in Artificial
Intelligence (UAI), 2018.
Martin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge
Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019. doi:
10.1017/9781108627771.
Antoine Wehenkel and Gilles Louppe. Unconstrained monotonic neural networks. In Hanna M.
Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Ro-
man Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Confer-
ence on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019,
Vancouver, BC, Canada, pp. 1543-1553, 2019. URL http://papers.nips.cc/paper/
8433-unconstrained-monotonic-neural-networks.
Francis Williams, Matthew Trager, Daniele Panozzo, Claudio Silva, Denis Zorin, and Joan Bruna.
Gradient dynamics of shallow univariate relu networks. In Advances in Neural Information
Processing Systems 32, pp. 8378-8387. 2019. URL http://papers.nips.cc/paper/
9046-gradient-dynamics-of-shallow-univariate-relu-networks.pdf.
Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for understanding
neural networks. In Advances in Neural Information Processing Systems, pp. 6598-6608, 2019.
11
Under review as a conference paper at ICLR 2021
A	Notations
We denote (α, β) as a concatenation of 2 vectors α and β. For any 2 vectors α and β, α β
denotes element wise multiplication of α and β vector. We denote the parameters of neural network
θ ∈ R2m is concatenation of W = (w1, w2, ..., wm) ∈ Rm and B = (b1, b2, ..., bm) ∈ Rm (i.e.
θ = (W, B)). Similarly, θt = (Wt, Bt) where Wt = (w1t , w2t , ..., wmt ) and Bt = (bt1, bt2, ..., btm).
Similarly, A0 = (a10, a20, . . . , ar0, . . . , am0). We denote 1 = (1, 1, . . . , 1) ∈ Rm. We use Big-O
notation to hide constants. We use log to denote natural logarithm. [n] denotes set {1, 2, . . . , n}
B Existence
This section contains a proof that shows existence of a pseudo network whose loss closely approxi-
mates the loss of the target function.
Lemma B.1. For every positivefunction F *0 ,for every X in the radius of 1 (i.e. |x| ≤ 1), there exist
a function h(wr0, br0) : R2 → [-Uh, Uh] such that
∖φ-1 (F*0(x)) — Ewr0,br0〜N(0,1) [h(wr0, brθ)I [Wr0X + brθ ≥。]]| ≤ 36-i(f*o)(δ)
where Uh is given by
k k(Φ-1(F *0))∣δ kLι
^δ10(ωφ-i(F *0)(δ))4
(B.1)
Proof. We use a result from Ji et al. (2020) to prove the lemma.
Result B.1. (One-dimensional version of Theorem 4.3 from Ji et al. (2020)) Let ψ : R → R and
δ > 0 be given, and define
ωψ(δ) = sup{ψ(x) — ψ(x0) : max{|x| , |x0|} ≤ 1 + δ, |x — x0| ≤ δ}
ψ∣δ(x)=ψ(x)I [|x| ≤ 1 + δ]
ψ∣δ,α =ψ∣δ * Ga
δ
α :=-----,	---
1 + p2 log (2M∕ωψ(δ))
~
O(δ)
M := sup ∣ψ(x)∣
∣x∣≤1+δ
β ： = ^^
P	2πɑ2
Tr (Wrθ,brθ) ：=2 ψ∣δ,a(0) + / ∖ ψ∣ δ,a (v) ∖ COS (2∏ (θψ∣ δ,a (v) - k V k ))加
+ 2∏ (2∏β2) ∖ψ∣δ (βwrθ)∖ e (br°r Sin(2n (θψ∣δ,α (βwrθ) — brθ)) I ||br0 | ≤ ∣∣Wr0∣∣ ≤ r]
where * denotes convolution operation, Gα denotes Gaussian with mean 0 and variance α2. Note
that O hides logarithmic dependency OfcompleXity measure offunction ψ. ∖ψ∣δ,a ∖ denotes magnitude
offourier transform of ψ∣δ,α and θψ∣δ α denotes phase offourier transform. Then,
sup ∖ψ(x) — Ewr0,br0 〜N (0,1) [Tr (Wr0,brθ)I [Wr0X + br0 ≥ 0]]∖ ≤ ωψ (δ)	(B.2)
lxl≤1
The upper bound of Tr(wr0, br0) is given by
11 ∕T- /	7	∖ I I 人
sup kTr (wr0 , br0 )k = O
wr0,br0
kΨ∣δ HL1
δ10(ωψ(δ))4
UT
(B.3)
Using Result B.1 for φ-1 (F*0(x)) function, denoting Tr(wr0, br0) for φ-1 (F*0(x)) function as
h(wr0, br0), we get
∖φ-1(F*0(x)) — Ewro,bro〜N(0,1) [h(wr0, brθ)I [Wr0X + brθ ≥ 0]]∖ ≤ 3@-\(f*o) (δ)
12
Under review as a conference paper at ICLR 2021
with following upper bound on h(wr0 , br0).
sup h(wr0, brθ) ≤ O
wr0 ,br0
k(Φ-1(F*0))∣δ kLι
δ10(ωφ-i(F *0)(δ))4
Uh
Divide pseudo network P (x) into 2 parts: Pc(x), first part of pseudo network is constant and
time-independent and P'(x), second part of pseudo network is linear in Wr and b
P (x) = Pc(X) + P'(x)
where
m
Pc(x) =	ar0 (wr0x + br0) I [wr0x + br0 ≥ 0]
r=1
m
P'(x) = ɪ2 ar0 (Wr X + br ) I [Wr0X + b70 ≥ 0]
r=1
Lemma B.2. (Approximating target function using P'(x)) For eve^y positivefunction F *0 and for
every E ∈ (0,1), with at least 1 一 -1 一 exp (—12802^1hog m) probability over random initialization,
there exist θ* such that we getfollowing inequalityfor all X ∈ [一1,1] and some fixed positive constant
c1 > 1.
∣φ(P∏x))- F*0(x)∣≤ ωφ-i(F*0)(δ)+ E
and upper bound L∞ norm of parameters is given by
kθ*k∞
≤ Uh √∏
一√2m∈a
□
Proof. Define Wr and b； as
Wrr = 0
b； = sign(ar0y h(√mwr0, √mbr0)
mEa 2
(B.4)
Using Wr and b；,
Ear0~N(0,W),Wr0~N(0, mm ),br0~N(θ, 表 ) [P' (X)I
m
= Earo〜N(0,W),Wr0〜N(0,表),br0〜N(θ,表)X a；0(WrX + ^)1 [wr0X + br0 ≥ 0]
r=1
=Earo〜N(0,ea),wr0〜N(0,* ),br0〜N(θ,亲 ) QZ:G h(而Wr0, √mbr0)I [Wr0X + br0 ≥ 0]
=EwrO〜N(0,.),br0〜N(0,.)[h(VmWr0, √mbr0)I [√m (w；0x + br0) ≥ 0]]
where equality (i) follows from Fact H.2 and homogeneity of indicator function. Using Lemma B.1,
IEarO 〜N (0,W),wrO 〜N (0,*),brO 〜N (θ,表)^31 -	^ IF (X))I
=IEwr0-N(0,ɪ),br0〜N(0,ɪ) [h(^Wr0, √mbr0)I [√m (Wr0X + br0) ≥ 0]] - φ-1 (F*'(X))I
≤ ωφ-1(F*0) (δ)	(B.5)
Using technique from Yehudai & Shamir (2019), we define
h = h	((a10,W10,b10)	, . . . ,	(ar0, Wr0,	br0)	, ∙ ∙ ∙ ,	(a10, Wm0, bm0)) =	SUP	lP'	(X) 一 Ear0,wr0 ,bro	[P'	(X)] ]
x∈[-1,1]
13
Under review as a conference paper at ICLR 2021
We will use McDiarmid’s inequality to bound h.
h ((a10, w10, b10) , . . . ,	(ar0, wr0, br0)	, . . . ,	(a10, wm0,	bm0)) - h	(a10, w10, b10) , . . . ,	(a0r0, wr00, br0)0	, . . . ,	(a10,	wm0, bm0
≤ 4cιUh √2 log m
m
Using Lemma 26.2 from Shalev-Shwartz & Ben-David (2014), we get
2
E [h] = mEar0,Wr0,br0,ξr
sup m
x
m
X ξi (w；x + b；) I [Wr0X + br0 ≥ 0]
r=1
where ξ1, ξ2, . . . , ξm are independent Rademacher random variables.
Ear0,wr0,br0 [h]
2 E
ʃm arO,wr0,br0,ξr
2 E
ʃm arO,wr0,br0,ξr
sup m
x
sup m
x
m
ξiar0 (Wrx + b；) I [Wr0X + br0 ≥ 0]
r=1
m
∑ξiar0 (Wrx + br) I [Wr0x + br0 ≥ 0]
r=1
≤
≤
≤
8c1 √∣ogmUh
Ear0,wr0,br0,ξr sup
m
ξiI [wr0x + br0 ≥ 0]
r=1
m
x
One can show that
ʃm EarO ,wr0,br0,ξr
sup
x
m
ξiI [wr0x + br0 ≥ 0]
r=1
≤ 2√∣omm
Using this relation, we get
ar0 ,wr0,br0
[h] ≤
16c1 Uh ∣og m
Using Mcdiarmid's inequality, with at least 1 - 1 - exp (- 128c2Umbog m), We have
∣P*(x) - EarO ,Wr0,br0 PPXI = h=≤ 三 +	≤ ≡
rrr	2	m
where inequality (i) follows from our choice of m in lemma D.2. Using eq.(B.5), we get
IPnx)- φT(F*0(x)) I ≤ 3φ-i(F*0) (δ) + e
(B.6)
(B.7)
Using 1-Lipschitzness of φ, we get
∣φ(P*(x)) - F*0(x)∣ = ∣φ(P∕(x)) - φ (φ-1 (F*0(x))) I
≤ ∣P'*(x)- φ- (F*0(x))∣
≤ 3φ-1(F*0) (δ) + C
The upper bound on norm of ∣∣θ*k∞ is given by the following equation.
kθ*k∞ ≤√√π
2mCa
□
Corollary B.1. (Approximating target network using P (x)) For every positivefunction F *0 and for
every C ∈ (0,1), with at least 0.99 — -1 - -1 - -1 — exp (- 128c2Um∣0g m) probability over random
initialization, there exists θ* such that we have following inequality for all x ∈ [-1,1] and some
fixed positive constants c1 > 1, c6 > 1 and c7 > 1.
∣Φ (P*(x)) - F*0(x)∣ ≤ 16ci (c6 + C7) Ca log m + 3φ-i(F*0)(δ) + C
and upper bound on L∞ norm ofparameters θ* is given by
kθ*k∞ ≤√≡
14
Under review as a conference paper at ICLR 2021
Proof. Using Lipschitz continuity of φ function, we get
∖φ(P'(X))- Φ(P*(χ))l ≤ IP*(χ)- P*(χ)l
m
≤ Ear0 (Wr0X + brθ) I Qr0X + br0 ≥ 0]
r = 1
Now, there are at most m break points of indicator I \wrox + bro ≥ 0] where value of
I [wroχ + bro ≥ 0] changes. We can divide range of x into at most m + 1 subsets where in each
subset, value of indicators I [wrox + bro ≥ 0] is fixed for all r. Suppose there are m0 indicators with
value 1 in a given subset. Without loss of generality, we can assume that indicators from r = 1 to
r = m0 is 1. Then,
m
E aro (WroX + bQ I [WroX + b^ ≥ 0]
r = 1
/
m
EarO (WroX + bQ
r=1
/
m
/
m
≤
X aroWro + Σ αrobro
r=1
r=1
Now, applying HOefding’s inequality for the sum in above equation, we get
2t2m
m0 (2c1eα√2logm)2 (2c6√2logm)2
)
32。2吟吟(log m)2 y
Taking t = 1βc1c66a (log m), with at least probability 0.999 - -1 - -1, we have
/
m
EarOWrO ≤ 1βc1C6∈α (log m)
r=1
and similarly, we will get that with at least 0.999 - 1 -六 probability,
/
m
EarOWrO ≤ 1βc1C7∈α (log m)
r=1
we will get that at least 0.999 ——1-- ——-probability, we have
J	C1 C6	G7 ɪ	八
m
ɪ2 arOWroI [WrOX + brO ≥ 0] ≤ 16。1。6Ca (log m)
r=-
m
aO arobroI [WrOX + brO ≥ 0] ≤ 1βc-C7Ca (log m)
r=-
Using these relations, we get that with at least 0.99 - G- - G- - G- probability,
m
EarO (WrOX + bro) I [WrOX + brO ≥ 0] ≤ 1βc1 (c6 + C7) Ca log m
r=-
Using above inequality, we get
∣φ (P^(x)) - φ(P*(x))∣ ≤ I耳(X)- P*(x)∣ ≤ 1βc1 (c6 + C7) Ca log m
(B.8)
(B.9)
15
Under review as a conference paper at ICLR 2021
Using lemma B.2, With at least 0.99 -六一* -六一exp (-⑵不潦bθg m) probability,
∣φ(P*(χ)) - F*0(x)1 ≤ ∣φ(P*(χ)) - Φ(P∏χ))l + IΦ(%(χ)) - F*0(x)1
≤ 16cι (c6 + C7) Ca log m + ωφ-i(F*0)(δ) + C
□
Lemma B.3. (Optimal loss) For every positive function F*0 and for every ∈ (0, 1), with at least
0.99 - cl - d -Cl - exp (- 128c2Um∣0g m) probability over random initialization, there exist θ*
such that loss of pseudo network with θ* parameters is close to that of the target function for all
x ∈ [-1, 1] and for some fixed positive constants c1 > 1, c6 > 1 and c7 > 1.
∣L(Φ (P *) ,x) - L(F *0,x)∣ ≤ 3 (I6c1 (c6 + c7) Ca log m + ωφ-i(F *0)(δ) + C)
Proof.
∣	∣	∣∣ Q	Q	∣∣
∣L(φ(P*) ,x) - L (F*0, x)∣ ≤ £∆χφ(P*(τ (X)))- E∆χF*0 (Ti (x))
∣ i=1	i=1	∣
+ |log (φ (P* (x))) - log (F*0(x))|
(	i)
≤	2 16c1 (c6 + c7) Ca log m + ωφ-i(F*0) (δ) + C
+∣∣P*(x)-φ-1 (F*0(x))∣∣
≤	2 16c1 (c6 + c7) Ca log m + ωφ-i(F*0) (δ) + C
+ |P；(x)| + ∣P*(χ) - Φ-1 (F*0(χ))∣
(ii)
≤ 3 16c1 (c6 + c7) Ca log m + ωφ-i(F*0) (δ) + C
where inequality (i) follows from Corollary B.1 with at least 0.99 - ɪ - ɪ - ɪ -
ci	c6	c7
exp (- 128c2Umlogm) probability. Inequality (ii) uses Eq.(B.7) and Eq.(B.9).	□
C Coupling
In this section, we prove that, for random initialization, the gradients of the loss of pseudo network
closely approximate the gradients of the loss of the target function. In other words, we show coupling
of their gradient-based optimizations. Define λ1 as
λ1
sup
t∈[T ] ,r∈[m] ,wr,br ,∣x∣≤1
Φ0(Nt(x))
Φ(Nt(x))
(C.1)
We get following find upper bound on λ1 .
λ1 =	sup
t∈[T ],r∈[m],wr ,br ,∣x∣≤1
φ0(Nt(x))
Define ∆ as
sup
t∈[T ],r∈[m],wr ,br ,∣x∣≤1
sup
t∈[T ],r∈[m],wr ,br ,∣x∣≤1
1
φ(Nt(x))
exp (Nt(X)) I [Nt(x) < 0] +1 [Nt(x) ≥ 0]
exp(Nt(x)) I [Nt(x) < 0] + (Nt(X) + 1) I Nt(X) ≥ 0]
I[Nt(x) < 0] +
I [Nt (x) ≥ 0]
Nt(x) + 1
∆ = 6c1Ca √2 log m
(C.2)
(C.3)
for some positive constant c1 > 1.
16
Under review as a conference paper at ICLR 2021
Lemma C.1. (Bound in change in patterns) For every x in 1 radius (∖x∖ ≤ 1) andfor every time step
t ≥ 1, with probability at least 1 — 1 — exp (― 64(02-1)∏ m δ t ) over random initialization, for
at most c24√√m⅛t fraction of r ∈ [m]
I [(wr0 + Wtr)x + br0 + btr ≥ 0] = I [wr0X + br0 ≥ 0]
for some positive constant ci > 1 and c2 ≥ 1.
Proof. Taking derivative of L(f, x) wrt Wr,
_ ʌ , .
∂L(ft,x)
∂Wr
Q Q
E∆χφ0(Nt (Ti (x)) )ατ0σ0 ((wr0 + Wr)τi (x) + b『0 + b：) τ (x)
.i=1
+ φ(N (x)) (Φ(Nt(x))αr0σ0 ((Wr0 + Wr)x + br0 + b1r) x)
Q
≤ X I ∆xφ<Nt (Ti (x)) )αr0σ0 ((Wr0 + Wr)Ti (x) + br0 + btr.) Ti (x) I
i=1
+ fU ? I 1 (ar0σ° ((Wr0 + Wr )x + br0 + b∖) x)[
O(Nt(X)) Iιv	1
Using Eq.(C.2), ∆χ ≤ Q, ∖x∖ ≤ 1 and ∖φ0 (N(x))∖ ≤ 1 for all X ∈ [—1,1], we get
_ ʌ , .
∂L(ft,x)
∂Wr
≤ 3 ∖ar0∖
Using Lemma H.2, with at least 1 — ɪ probability, we get
c1
_ ʌ ,
dL(ft,X) ≤ ∆
∂Wr	一
_ ʌ , .
∂L(f0,x)
∂br
where ∆ is defined in Eq.(C.3). Using same procedure for br, we get
Q
=y^∆χφz (Nt (Ti (x))) αr0σ0 ((w^ + Wr)τi (x) + br.0 + btr)
i=1
+ *J C (φ(Nt(x))αr0σ0 ((Wr0 + Wr )x + br0 + b))
φ(Nt(x))
≤ 3 ∖αr0∖
=∆
Using Eq.(C.4) and Eq.(C.5), we get
I w： i ≤ η∆t
I br i ≤ g t
Define
Ht = {r ∈ [m]∖ ∖wr0x + br0 ∖ ≥ 4心t}
(C.4)
(C.5)
(C.6)
(C.7)
For every X with ∖x∖ ≤ 1 and for all r ∈ [m], ∖wtr,X + b：∖ ≤ 2n∆t. For all r ∈ Ht, We
get I [(wr0 + Wr )x + br0 + b： ≥ 0] = I [wr0x + br0 ≥ 0]. Now, We need to bound the size of
Ht. We know that for all X ∈ [—1,1], w：0x + b：0 is Gaussian with E [w：0x + b：。] = 0 and
Var [wr0x + br0] ≥ *.Using Lemma H.3, we get
Pr (∖wr0X + br0∖ ≤ 4η∆t) ≤
4√2η√m∆ t
17
Under review as a conference paper at ICLR 2021
Using Fact H.1 for Hc (where Ht = [m]∕Ht) for some positive constant c2 ≥ 1, we get
Pr (|Hc| ≥ c2m4^『t
Pr (|Hc| ≤ c2m4√√2
≤，*「卜小 - 1)(4√√Ft 力2
(64(C2 — 1)2η2m2∆2t2、
≤exp (--------π-------)
〉]	[64(1 — c2)2η2m2∆2t2 A
Pr |Ht| ≥ m 1 — c2
4√2η√m∆∆ t
√π
exp —
64(1 — C2)2η2 m2∆ 2t2
π
where |Ht| denotes the cardinality of set Ht and similarly for |Hc|.
□
Lemma C.2. (Bound on difference of f and g!) For every x in 1 radius (|x| ≤ 1) andfor every time
SteP t ≥ 1, with at least 1 — -1 probability, function with neural network andfunction with pseudo
network are closefor some positive constants ci > 1.
∖φ(Nt(x)) — φ(Pt(x))∖ ≤ 24cιQηAt ∣H-∣ p2logm
Proof. We know that φ is 1-Lipschitz continuous. Using Lipschitz continuity of φ, we get
∖φ(Nt(χ》 — 0(Pt(x))| ≤ |Nt(x) — Pt(x)|
We bound INt(X) — Pt(X)I as following.
|Nt(x) — Pt(x)| ≤ E aro ((wrQ + Wtr)x + br0 + btr) I [(wr0 + Wtr)x + br0 + btr ≥ 0]
r∈ [m]
—E arQ ((WrQ + Wr)X + brQ + d)I [WrQX + brQ ≥ 0]
r∈ [m]
≤ E arQ ((WrQ + Wr)X + brQ + b∖)(I [(WrQ + Wr)X + brQ + b∖ ≥ 0] 一 I [WrQX + brQ ≥ 0])
r∈Ht
(i)	/	Z______、	_	_
≤ ∣ Hc ∣ (2cc √2 log m) (4η∆t + 2η∆t) (2)
≤24cιeαηAt∣ Hc ∣ √2logm	(C.8)
where inequality (i) uses Lemma H.2 with at least 1 — ɪ probability.	□
-1
Corollary C.1. (Final bound on difference of f and g0) For every X in 1 radius (|x| ≤ 1) andfor
every time step t ≥ 1, with at least 1 — -1 — exp (— 64(c2T)，m δ t ) probability over random
initialization, function with neural network andfunction with pseudo network are closefor some
positive constants ci > 1 and c2 ≥ 1.
|φ(Nt(x)) — Φ(Pt(x))] ≤
192η2m1∙5∆ 2。1。2“ t2√log m
(C.9)
18
Under review as a conference paper at ICLR 2021
Proof. Using Lemma C.1 and Lemma C.2, we get
∣φ(Nt(x)) - φ(Pt(x))∣ ≤24cιEanAt ∣HC∣ P log m
(i)
≤24cι6aη2∆ t C2m
4√2η√m∆∆ t
2 logm
192ηm1.5∆ cιc26at √logm
V √π ；
192n2m1.5 A 2c1 C2Cat2 √Iogm
(nAt)
一	√π
≤ O(η2m1.5A2Qt2 plog m)
where inequality (i) uses Lemma C.1 and the inequality follows with at least
(C.10)
exp -
64(c2 — 1)2η2 m2∆ 2t2
1 - C11 -
probability. Define Atnp as
Atnp
192n2m1.5 A2 ci c2 eat2 √log m
(C.11)
≤

π
□
Lemma C.3. (Coupling of loss functions) For all x in 1 radius (|x| ≤ 1) and for every time step
t ≥ 1, with probability at least 1 一 六 一 exp (一
64(c2-1)2η2m2 ∆ 2t2
over random initialization, loss
π
function of neural network and pseudo network are close for some positive constant c1 > 1 and
c2 ≥ 1.
∣L(f0,x) - L(g0 ,x)∣ ≤3Anp
Proof.
ʌ ,
-L (ft, x) -
∣	∣Q	Q	∣
LWt,x)∣≤ EAxfO(Ti(X))-EAxgO(Ti(X)) + ιιog(f0(X))-log(gt(x))|
i=1
i=1
(≤)2 sup |ft0 (τi (x)) - gt0 (τi (x))|	+ |Nt(x) - Pt(x)|
i∈[Q]
(ii)
≤ 3Atnp
where inequality (i) follows from 1-Lipschitz continuity of log (φ(N (x))) with respect to N (x).
Inequality (ii) uses Eq.(C.8) and Lemma C.2.
□
Lemma C.4. (Coupling of gradient of functions) For all x in 1 radius (|x| ≤ 1) and for every time
step t ≥ 1, with at least 1 ——L probability over random initialization, gradient ofderivative ofneural
c1
network function and derivative of pseudo network function with respect to parameters are close for
some positive constant c1 > 1.
∣∣Vθff(x) - Vθg0(x)∣∣ι ≤ 4qea ImAnp + 2 |Hc|) √2logm
Proof.
∣∣∣Vθft0(x) - Vθgt0(x)∣∣∣	≤ ∣∣∣φ0(Nt(x))VθNt(x) - φ0(Pt(x))VθPt(x)∣∣∣
≤ ∣∣∣φ0(Nt(x))VθNt(x) -φ0(Pt(x))VθNt(x)∣∣∣	+ ∣∣∣φ0(Pt(x))VθNt(x) - φ0(Pt(x))VθPt(x)
≤ ∣φ0(Nt(x)) - φ,(Pt(x))∣ ∣∣VθNt(x)∣∣ι + ∣φ0(Pt(x))∣ ∣∣VθNt(x) - VθPt(x)∣∣]
≤ ∣Nt(x) - Pt(x)l ∣∣VθNt(x)∣∣ + ∣∣VθNt(X)- VθPt(x)∣∣
1
1
1
19
Under review as a conference paper at ICLR 2021
where last inequality follows from 1-Lipschitzness of φ0 function and φ0(x) ≤ 1 for all x such that
|x| ≤ 1,t ∈ [T]. To upper bound ∣∣VθNt(X) - VθPt(χ)∣∣ ,
∣∣VθNt(X)-VθPt(x)∣∣] ≤ ∣∣(A0,A0) Θ (1x, 1) Θ (I [(Wo + Wt)x + Bo + Bt ≥ 0] -1 [Wox + Bo ≥ 0],
I (W0+Wt)x+B0+Bt ≥ 0 -I[W0x+B0 ≥ 0])∣∣∣
(i)
≤(8ciQ √2 log m) |Hc|
≤ 8cιQ ∣HC∣ P2logm	(C.12)
The inequality (i) uses property of Ht that for all r ∈ Ht, I [(wro + wrt )X + bro + btr ≥ 0] =
I [wroX + bro ≥ 0]. Using Eq.(C.11) and Eq.(C.12), we get
∣∣Vθf0(x) - Vθg0(x)∣∣ι ≤ ∣Nt(x) - Pt(X)∣∣∣(A0,A0) Θ (1x, 1) Θ (I [(Wo + Wt)x + Bo + Bt ≥ 0],
I[(Wo+Wt)X+Bo+Bt ≥ 0])∣∣∣ + ∣∣∣VθNt(X) -VθPt(X)∣∣∣
≤ 4c∖eam∆^npP2 log m + 8c1eα |Hc|，2 log m
=4cι∈a (m∆tnp + 2 |Hc|) p2logm
□
Lemma C.5. (Coupling of gradient of loss) For all X in 1 radius (|X| ≤ 1) and for every time step
t ≥ 1, with probability at least 1 一 -1 一 exp (- 64(c2-I) ∏ m δ t ) over random initialization,
gradient of loss function with neural network and loss function with pseudo network are close for
some positive constant c1 > 1 and c2 ≥ 1.
∣∣VθL(ft,x) - VθL(gt,x)∣∣1 ≤ 192nm1.5ACc产t√logm + 16cιeam∆npP2i⅛m
π
Proof.
∣∣VθL(f0,X) -VθL(gt,x)∣∣1 ≤
∣Q
∣∣ X ∆xVθft0(τi (X)) -
∣ i=1
Q
-	∆xVθgt0(τi(X))+
i=1
∣Q
Vθ f0(x)
f0(χ)
vθ g0 (X)
g0(X)
Q
i=1
∆xVθft0(τi (X)) -	∆xVθgt0(τi (X))
i=1
z"^^^^
I
≤
|
1
1
}
+
vθ g0 (X)	vθ ff(X)
--- -:  -------...—: 
gt(X)	ft (X)
|
z
II
1
}
Proving bound on I,
Q
Q
I
∆xVθft0(τi (X)) -	∆xVθgt0(τi(X))
i=1
Q
≤ X ∆x ∣∣∣
i=1
i=1
1
Vθft0(τi (X)) - Vθ gt0 (τi (X))∣∣
(i)	y_________
≤ 8cc∈a (m∆np + 2 |Hc|) √2 log m
20
Under review as a conference paper at ICLR 2021
where inequality (i) follows from Lemma C.4. Now, we will bound II,
Nf) g0(X)	Nf)力(X)
--- -:  ---------:-
g0 (X)	f0 (X)
exp (Pt(X)) I [Pt(x) < 0] +1 [Pt(x) ≥ 0]
exp(Pt(x)) I [Pt(x) < 0] + (Pt(x) + 1) I [Pt(x) ≥ 0]
NfPt (X)
_	exp(Nt(X)) I [Nt(X) < 0] +1 [N (x) ≥ 0] N N ,)
- exp(Nt(X)) I [Nt(X) < 0] +(Nt(X) + 1) I [Nt(X) ≥ 0] θ t (X)I
=II(I [Pt(X) <	0] +	IP(X) ≥ 0]) NθPt(X)	-(I [Nt(X)	< 0] +	I[Nt(X) ≥ 0])	NθNt(X)
八 [ t( )	]+	(Pt(X)+ 1) J () V t( )	]+	(Nt(X)+ 1), θ t( )
= IINfPt(X) -NfNt(X)II I[Pt(X) < 0, Nt(X) <0]
X-------------------{---------------------}
II1
1
+ IINf Pt (X) - X			NfNt(X) 	:—:	 Nt (x) + 1	I [Pt (X) < 0, Nt(X) ≥ 0] 1 	，	
			z II2	
+ X	Vf Pt(X) Pt (x) + 1	- Nf Nt(X)	1	I [Pt (X) ≥ 0, Nt(X) < 0]
			z II3	
+ X	Nf Pt(X) Pt (χ) +1	NfNt(X) —TT~~：	 Nt (x) + 1		I[Pt(X) ≥ 0, Nt(X) ≥0] 1 	Z
			{z II4	
On simplifying II2 , we get
II2 ≤ (Im 二"卜 θ Pt(X) - Nf Nt(X)	+ 1 Ntax) J B Pt(X) ) I 因⑺ < 0,M(x) ≥。]
Nt(X)+ 1I	I1	1+ Nt(X)I	I1
≤	IIINfPt(X)	-NfNt(X)III	+∆tnpIIINfPt(X)III	I[Pt(X)	< 0,	Nt(X)	≥0]	(C.13)
Similarly, on simplifying II3, we get
1

II3 ≤
≤
Pt(X) + 1
NfPt(X)
- Nf Nt(X)I +
I1
I [Pt(X) ≥ 0, Nt(X) < 0]
NfPt(X) - Nf Nt(X)
+ ∆tnp INf Nt(X)
1
I [Pt(X) ≥ 0, Nt (X) < 0]
(C.14)
On simplifying II4 , we get
II4 ≤
NfPt(X)	Nf Nt(X)
---:—:----------:—:---
Pt(x) + 1	Pt(x) + 1
Nf Nt(X)	Nf Nt(X)
---:—:-----------:—:----
Pt(x) + 1	Nt(x) + 1
I [Pt(X) ≥ 0, Nt(X) ≥ 0]
+
1
≤ (Pt(⅛卜Pt(X)- NfNt(X)[1 + (Pt(IN")；1(：+1)) I[Pt(X) ≥ 0, Nt(X) ≥ 0]
≤	IIINfPt(X)	-NfNt(X)III	+	IIINfNt(X)III	∆tnpI[Pt(X) ≥0,	Nt(X)	≥0]	(C.15)
21
Under review as a conference paper at ICLR 2021
Using Eq.(C.13), Eq.(C.14) and Eq.(C.15), we get
ττ	Vθ g0 (X)	Vθ f0(x)
II	-
g0 (X)	ft(x)	1
≤VθPt(x) -VθNt(x)	+ VθNt(x) ∆tnpI[Pt(x) ≥ 0]
+∆tnpVθPt(X) I[Pt(X) < 0,Nt(X) ≥0]
Using Eq.(C.12), we get
II ≤ 8c1ea IHCI P2logm + ∆v (∣∣VθNt(x)[ + 忖Pt(X)I)
≤ 8c1e0 |Hc| p2 log m + ∆v (∣∣ (Ao, Ao) © (1x, 1) © (I [Wox + Bo ≥ 0], I [Wox + Bo ≥ 0])(
+ ∣∣ (Ao, Ao) © (1x, 1) © (I [(Wo + Wt)x + Bo + Bt ≥ 0] ,I [(Wo + Wt)x + Bo + Bt ≥ 0])[
≤ 8c1e0 |Hc| p2 log m + 4p 卜c1eamp2log m)
——8c1∈a (|Hc| + m∆np) p2logm	(C.16)
Combining bounds on I and II, we get
∣∣VθL(f0,x) - VθL(g0,x)∣∣1 ≤ 8c1ea (m∆np + 2 HcI) p2logm
+ 8c1∈a (IHcI + m∆np) p2logm
≤ 8c1∈a (2m∆np + 3 HcI)，2logm
Using Lemma C.1, with at least 1 一 d 一exp (一阳❷—^了^理产)probability, We get
∣∣VθL(f0,x) - VθL(gt,x)∣∣1 ≤ 192ηm1.5A√∏2'at√ogm + 16c1eam∆npP21⅛m
Define Γ as the upper bound on ∣∣VθL(f0,χ) - VθL(g0,χ)∣∣1.
Γ ——192ηm1.5A √∏2Ca t√ogm + 16c1eam∆npP2iogm	(C.17)
□
D	Optimization proof
This section shows that gradient-based optimization of the loss for the target function can be closely
approximated by the gradient-based optimization of the pseudo network. Since the loss function of
the pseudo network is convex in its parameters, we get global optimization.
Lemma D.1. (Convexity of loss function of pseudo network) The loss function for pseudo network is
convex with respect to parameters of neural network.
Proof. The loss function for pseudo network is
Q
L(g0 ,x) = EAxgO (Ti(X))- log (g0 (X))
i=1
Dividing the loss function in 2 parts,
LIgI ,x)——L1(g0 ,x) + L 2(g0 ,x)
22
Under review as a conference paper at ICLR 2021
where
Q
L ι(g0, X) = Eδx90 (Ti(X))
i=1
L2(g0,x) = - log (g0(x))
We will prove convexity of both L1(gt0, X) and L2(gt0, X). To prove convexity of L1(gt0, X) as a
function of parameters θ, we will prove that Hessian of L1(gt0, X) is positive semidefinite.
QQ
NeLι(g0,x) = X ∆χVθg0(τi (x)) = X ∆χφ0 (Pt (Ti (X))) NePt (Ti (x))
i=1	i=1
Q
vθ Lι(g0 ,x) =E δxv290 (Ti(X))
i=1
Q
=X∆xφ00(Pt(Ti (X)))vePt(Ti (X))vePt(Ti (X))T
i=1
Q
+	∆xφ0(Pt(Ti (X)))ve2Pt(Ti (X))
i=1
Q
=	∆xφ00(Pt(Ti (X)))vePt(Ti (X))vePt(Ti (X))T
i=1
The first term of the Hessian matrix is sum of Gram matrix and the second term of the Hessian matrix
is Gram matrix. Hence, the Hessian of L1(gt0, X) is positive semidefinite. For second term,
L2(g0,x) = -log(eXP(Pt(X)) I [Pt(χ) ≤ 0] + (Pt(X) + 1) I [Pt(χ) > 0])
= -Pt(X)I [Pt(X) ≤ 0] - log (Pt(X) + 1)I [Pt(X) > 0]
Note that L2(g0, x) is convex in Pt(X) and Pt(X) is linear in θ. Composition of convex and linear
function is convex therefore, L2(g0,x) is convex in θ. As sum of 2 convex functions is convex,
L(g0,x) is convex.	□
Remark D.1. If we use base distribution as standard Gaussian distribution, then loss function will
have following term.
L1 (g0 ,x)
∆xgt0(Ti (X))
Ifwefind Hessian of Li, then we get
0
veL1 (gt0 , X)
Xi=Q1
∆xgt0(Ti (X))
v2L 1 (g0 ,x)
∆xφ0 (Pt (Ti (X))) vePt (Ti (X))
∆xgt0(Ti (X))	X
∆xvegt0(Ti (X))
Xi=Q1
+
∆xvegt0(Ti (X))
If base distribution is standard Gaussian distribution, then 0t has to be negative for some points
therefore the first term in the Hessian won’t remain positive semi-definite therefore, the loss function
won’t remain convex in parameters of neural network θ if we use standard Gaussian distribution as
base distribution. AtPomts with negative values of g, Hessian of Lι(g0, x) with respect to θ can be
negative semidefinie and Li (g0, x) can be non-convex in θ.
23
Under review as a conference paper at ICLR 2021
Lemma D.2. (Approximated loss is close to optimal loss) For every ∈ (0, 1), there exist
m > Poly(Uh, 1) , η = O (m) and T = O (U 詈m) Such that, With at least 0.95 -
exp (- 128c2¾flog m ) Probability，we get
1 T-1
T E Esgd[L(f0, X)] - L(F*0, X) ≤ O(C
T t=0
Proof. For set of examples X, define
1
L(f0, X) = IXT ∑L(ft,,x)
|X| x∈X
From Lemma D.1, we know that L(gt0, X) is convex in parameters θ. Using convexity of L(gt0, X)
wrt θ,
L(g0, X) - L(g*∖ X) ≤ hVθL(g0, X), θt - θ*i
≤ kVθL(g0,X) - VθL(f0,X)kιkθt -θ*k∞
+ hVθL(ft, X),θt-θ*i	(D.1)
where k.k1 and k.k∞ denotes l1 and l∞ norm respectively. The stochastic gradient descent updates
the parameters using X at time t. g*0 is defined as following.
mm
g*0(x) = Ear0σ (Wr0X + brθ) + EarOσ (Wr0X + brθ) (w*X + b；)
r=1	r=1
For stochastic gradient descent, we get
kθt+1-θ*k2 = kθt-ηVθ L(f0,χt)-θ*k2
=kθt - θ*k2 + η2kVθL(ft,χt)k2 - 2ηhθt - θ*, VθL(ft,Xt)
Taking expectation wrt xt,
Ext [kθt+1-θ*k2] = kθt-θ*k2 + η2Eχt [IML(ft,χt)k2i -2ηhVθL(ft,X),θt-θ*i(d.2)
Using Eq.(D.2) and Eq.(D.1),
L(g0,X) - L(g：X) ≤ ∣∣VθL(gt,X) - VθL(f0,χ)∣∣1kθt - θ*k∞
+ kθt-θ*k2-Ext [kθt+1-θ*k2]
+	2η
+ 2 ExthkVθ L(f")k2]
Using Eq.(C.4) and Eq.(C.5), we get
∣∣VθL(f0,xt)∣2 ≤ 2m∆2
Averaging from t = 0 to T - 1, we get
1 X Esgd[L(g0, X)] - L(g： X) ≤ Γ( sup kθt k∞ + kθ*k∞1 + kθ: fk2 + ηm∆2
T t=0	t∈[T]	2ηT
=Γ( sup kθtk∞ + ∣∣θ*k∞) + kθζ≡ + nm∆2	(D.3)
t∈[T]	2ηT
Note that A and Γ are defined in Eq.(C.3) and Eq.(C.17). Inequality (i) follows from the fact
that θ0 = (0,0,..., 0) ∈ R2m. Using Lemma B.3 and Lemma C.3, with at least 0.99 - -1 -
24
Under review as a conference paper at ICLR 2021
PT=1 eχp
64(c2-1)2 η2m2 ∆ 2t2
π
一圭一W — exP (— 128°2*log m) PrObability, we get
斤 X ESgd[L(g0, X)] - -L(gz*, X) ≤ γ sSUP ∣∣θt∣∣∞ + llθ*k∞ ) + 1L ɪ2 + ηmA2
t =	V∈[T ]	2	2ηT
T X Esgd\_L(f°, X)] — Lul X) ≤ r 卜UPjlθt∣∣∞ + llθ*k∞) + \J + ηmA2 + A及np
1 X Esgd[L(f0, X)] — L(F*,, X) ≤ Γ ( sup ∣∣θt∣∣∞ + IIn∞) + 磬 + ηm∆2
Tt=O	g。	2	2ηT
+ 3∆^p + 3 (16cι (c6 + c7) Ea log m + ωφ-i(F*0)(δ) + E)
Note that inequality (i) and inequality (ii) uses Lemma B.3 and Lemma C.3 respectively. We choose
following values/relations of η, T.
η二K
72c2meα log m
2
2ηe
Uhn 72c2m∈a log m
2m6a	2E2
18πc1 log m Uh
E2
(D.4)
We can choose δ such that ωψ-i(p*0)(δ) = e. Using above inequalities, we get following equalities.
∣∣θ*∣∣2 _ k叫2 2ηE
--------------------—(-
2ηT	2η 忖 ∣2
ηm∆2 =—m∆2 = E
m∆2
3 (16c1 (c6 + C7) Ea log m + ωφ-i(F*0) (δ) + E) ≤ 3 (16c1 (c6 + C7) Ea log m + 2e)
Using Corollary B.1, we get
kθ*k∞
l∣θ*l∣2 ≤ √m∣ι 叫 ∞ ≤
Uh √π
√2√mEα
25
Under review as a conference paper at ICLR 2021
To get value of m,
sup ∣∣θt∣∣∞ = sup η∆t = η∆T = kθ12八
t∈[T]	t∈[T]	2€
3πUj2cι ʌ/log m
< Uh不(6c16a√2logm)
‹ 2m6a
2e
sup 设k∞ + k叫∞ < π
t∈[T ]
^2rnaeae
Uh (1 + 3cι) √logm
√2mfaf
Γ
192ηm1∙5∆ cιc2eαt ʌ/log m
+ 16cιeam∆tnjp √2log m
≤
√π
192ηm1∙5A cιc2eat ʌ/log m
≤
√π
192ηm1∙5 ∆。1。2的力 ʌ/log m
+
+ 16cι6am V 2 log m
192η2m1∙5 ∆2 ci c2 Ca^t2 √log m
3072√2cic2eaη2t2m2'5 log m∆2
<
+
192m1-5cιc2ea√logm / Uh
4mc2
3072√2c2c2eam2'5 log m
______________、2
ci€a √2 log m)
<
288√2π√m log mc2c2Uh + 13824√2π3c4c2√m (log m)2 Uj
「4
h
<
€
14112√2π3c4c2√m (log m)2 Uh
e2
e2
Multiplication of Γ and fsupt∈[τ] ∣∣θt∣∣∞ + ∣∣θ*∣∣∞) will be
Γ sup K∣∣∞ + ∣∣θ*∣∣∞
V∈[τ ]
14112√2π3c4c2√m (log m)2 Uh (πUh (1 + 3cι) √log m
€2	k	√2mea∈
14112π2^5c4c2 (1 + 3ci) (log m)2'5 Uh
√meae3
Taking m as
m > Ω
c c1 c2(1 + 3cι)2 Uh2
€^
(D.5)
Choosing m which satisfies above inequality will give US the following inequality.
γ I sup Ilθ1l∞ + llθ*ll∞ ) < €
v∈[t ]	7
Using Eq.(C.11), we get
(D.6)
26
Under review as a conference paper at ICLR 2021
Using sufficiently high m, we get
N < 8864π1-5c3C2 (logm)1.5 U4
np - (	√m羽
< O ( c1 c2 (log m)1.5 Ueaa )
一 y e2eac4C2 (1 + 3cι) U6 J
=O (心(IOg m)1.5!
=1cι (1 + 3cι) U2)
-O (e)
Using Eq.(D.4) and Eq.(D.5), with at least 0.99 -看-看-言-PT=I exp (- 64(c2 T) ∏ m δ，)-
exP (- i28c1⅞iog m ) PrObability, we get
ɪ X Esgd[L(f0, X)] - L(F *', X) - Γ( sup ∣∣%∣∣∞ + ∣∣θ*∣∣∞
T t=0	V∈it]
Wk2	R2
+ * + nmʌ2
2592π1^5c3C2 (logm) ' U4	/ x	ʃʌ、
+----------/— 2-------------+ 3 (I6c1 (c6 + c7) ea log m + 2e)
√me2ea
C	2592π1-5c3c2 (log m)1'5 U4	”. ’	.	ʃʌ、
—3e +------------,——2-----------+ 3 (16c1 (c6 + c7) ea log m + 2e)
√me26α
Taking q = 100, c = 2, c5 = 1000,。6 = 100, C = 100, e° = 6000∖g m — e, With at least
0.95 - Pt=I exP (-64"2m~2t2 ) - exP (- 128c1t⅞Tlog m ) PrObability,
1 X Esgd[L(f0, X)] - L(F*', X) — 3e + 2592小5c√2(2og m严 U4 +。⑹
T M y	√me26α
-3e +。(772^ + θ (e)
For any e ∈ [0,1], with probability at least 0.96-PT=I exp (- 64η22mκ2t2) -exp (-⑵说*⅛ m)
probability,
1
T
T-1
X %d[L(fZ X)] - L(F[ X) - O(e)
t=0
To find lower bound on probability, we use PT=I *-P∞=1 *-2.
T
X exp
t=1
64η2m2 A 2t2
π
-X
t=1
π
64η2 m2 A 2t2
π (mA2)2
-32e2m2A2
πA 2
-----
-32e2
π
-----
一 3200
- 0.01
27
Under review as a conference paper at ICLR 2021
where inequality (i) follows from exp(-x) ≤ 1 for all X ≥ 0. Finally, with at least 0.95 -
exP (- 128c⅞tlog m ) PrObability,
1 T-1
T E Ess,3d^,^i^(.f^ X)] - L(F*0, X) ≤ O(e)
T t=0
□
E Generalization
In this section, we Prove generalization guarantees to comPlement our oPtimization result, and
comPlete the Proof of our main theorem (Theorem E.1) about efficiently learning distributions using
univariate normalizing flows. The Proof in this section can be divided broadly in two Parts. First, we
prove that empirical average of L (f0, x) and L (F*0, x) on training examples are close to expectation
of L (ft, x) and L (F , x) With respect to underlying data distribution, respectively. The similar
argument is also used in Allen-Zhu et al. (2019). Second, we prove that L (f0, x) and L (F , x) are
close to L (ft, x) and L (F*, x), respectively.
Recall that the approximate loss function L is given by
Q
L (ft, X) = E ∆χφ (Nt (Ti(X)))- log(φ (Nt (x)))
i=1
where
m
Nt(x) =	ar0σ (wr0 + wrt)x + br0 + btr
r=1
Lemma E.1. (Empirical Rademacher complexity for two-layer neural network) For every B > 0, for
every n ≥ 1, with at least 1 - -1 probability over random initialization, the empirical Rademacher
complexity is bounded by
n Eξ∈{±i}n
n
sup	ξiN(xi)
maχr∈[m]lwr |,|br l≤B ¢=1
8cι6aBm λ∕2 log m
≤
Proof. Using part a of Lemma H.5, we get that {x 7→ wrx + br | |wr| ≤ B, |br| ≤
B} has Rademacher complexity √B. Using part b of Lemma H.5, we get that {x →
((wr0 + Wr) X + (b『0 + b『))| |w" ≤ B, |b" ≤ B, Wro, br0 〜N(0, mm)} has Rademacher
complexity √. Using part C of Lemma H.5, we get that class of functions in F = {x →
N(x) | maxr∈[m] |wr| ≤ B, maxr∈[m] |br| ≤ B} has Rademacher complexity
^ AjOII Il 2B (i) 8c1eaBm√2 log m
R (X； F) ≤ 2∣∣a∣∣i7 ≤ -『------
nn
where inequality (i) follows from Lemma H.2 with at least 1 - ɪ probability over random initializa-
-1
tion.
□
Define upper bound on maximum and lower bound on minimum value of loss function L is given by
Q
sup L (F*0, x) = sup ^X ∆χF*0(Ti(X)) — log(F*0(x)) ≤ 2Mf*0 — log (mF*0) := ML
x
x
i=1
Q
inf L (F*0, x) = inf ^X ∆χF*0(Ti(X)) — log(F*0(x)) ≥ 2mp*0 — Iog(MF*o) := m^
x	x i=1
(E.1)
(E.2)
28
Under review as a conference paper at ICLR 2021
Lemma E.2. Suppose n is sufficiently high such that it satisfies following condition.
n≥O
((ML - mL)2 (Q + I)2 Uh (IOgm)2
Ifn satisfies above condition, then with at least 0.98 probability over random initialization, population
loss of any functions of set {x → N (x) | |w" ≤ η∆∆ T, |br| ≤ η∆∆ T ∀r ∈ [m]} is close to
empirical loss i.e.
sup
N∈F
n
Ex∈d [L(f0,χ)] - - X L(f0,χ)
n i=1
≤
Proof. Note that L (ft0, x) depends on neural network Nt(x) through
(Nt (τ1 (x)) , Nt (τ2 (x)) , . . . , Nt (τQ (x)) , Nt(x)) vector. Using Fact H.8, with at least - - δ
probability, we get
sup Ex 〜D
N∈F
n
[L(f0,χ)] - - X L(f0,χ)
n i=1
≤ 2√2Ls (Q + 1)TR(X;F)+ b
(E.3)
where F =	{x	→ N(x)	|	|w"	≤	η∆T,	|b"	≤ η∆T	∀r	∈	[m]}.	We get coordinate wise
Lipschitz continuity of loss L function as following.
Lj ≤	sup	∣∆xΦ0 (N (Tj (x)))∣
N ∈F ,∣x∣≤1
≤ SUp	-- lφ0 (N (Tj (X)))I
N ∈F ,∣x∣≤1 Q
≤ 万 ∀i ∈ [Q]
Q
φ0(N(x)) _	exp(N(x)) I [N(x) ≤ 0] +1 [Nt(x) ≥ 0]
N ∈F ,∣χ∣≤1 Φ(N(x))	N∈Fup∣≤ι exp (N(X)) I [N(x) ≤ 0] + (N(X) + 1) I [N(x) ≥ 0]
≤ N∈FP∣≤11 [N(X) ≤ 0] + N⅛1 [N(X) ≥ 0]
≤1
Using Lemma H.4, standard Lipschitz constant of L is
(E.4)
To get upper bound on L, we use Lipschitz property of L.
Q
L(ft, X)- L (f0,X) I ≤ X ∆x IN(Ti(X))I +IN(X)I
i=1
(E.5)
29
Under review as a conference paper at ICLR 2021
Note that L (ft0, x) depends upon (N (τ1 (x)) , N (τ2 (x)) , . . . , N (τQ (x)) , N (x)) vector and simi-
larly, L (f0, x) depends upon (0,0,0,..., 0,0) Finding upper bound N(x) for all X ∈ [-1,1],
sup	N(x) ≤	sup	Pt (x) + ∆Tnp
N ∈F,x∈[-1,1]	∣Wr∣≤η∆ T,∣br∣≤η∆ T,x∈[-1,1]
≤	sup
∣Wr∣≤"∆ T,∣br∣≤η∆ T,x∈[-1,1]
r=1
ar0σ (wr0x + br0) +	ar0 (wrx + br) σ (wr0x + br0) + ∆Tnp
r=1
≤ 16cι (c6 + C7) Ea log m + m(2ciEap2 log m) (2η∆T) + ∆Tp
≤ 16cι (c6 + c7) Ea log m + m 22c1αα，2 log m) (2η∆T) + ∆T
≤ 16cι (c6 + C7) Ea log m + m θ8c2Ea log m QUhnE ) ) +△%
≤ O ( Uh log m
where inequality (i) uses Eq. (B.9), Lemma H.2 and Eq.(C.6). The inequality (ii) uses our choices
of η and T from Eq.(D.4) and lower bound on m from Eq.(D.5). Define K as upper bound on
supN∈F,x∈[-1,1] N(x).
K := O ( Uh log m
(E.6)
Using upper bound on supN∈F,x∈[-1,1] N(X) and Eq.(E.5), we get upper bound on L(b).
_ ʌ , 、
b = K + K + L(0, 0,..., 0,0) ≤ 2K + 2
Using value of b in Eq.(E.3) and Lemma E.1, with at least 1 - δ - ɪ probability, We get
c1
sup ∣Ex∈D
N∈F∣
n
[L(ft,,χ)] - - X L(f；, Xi)
n i=1
≤ 4√2(Q + 1)
8 c 1E a η∆ T m λ∕2 log m
+ (2K + 2) j竽
2n
m
m


We use δ = 0.01 and choose n which satisfies following condition.
n≥O
-m∙L)2 (Q + 1)2 Uh (logm)2
E2
(E.7)
Using above n, with at least 0.98 probability, we get
n
sup ∣Ex∈D
N∈F∣
hL(f0,X)i -
1n	∣
-EL(ft ,xi) ≤ E
i=1
□
Lemma E.3. (Concentration on approximated loss of target function) Suppose n is sufficiently high
such that it satisfies following condition.
n≥O
-mL)2 (Q + 1)2 Uh (logm)2
E2
If n satisfies above condition, then with at least 0.9999 probability, population loss of target function
F*0 is close to empirical loss i.e.
W〜DhL(F*0,x)] - L(F*0, X)∣ ≤ E
30
Under review as a conference paper at ICLR 2021
Proof Finding minimum value (m£) and maximum value (ML) of loss function L,
Q
sup L (F*0, x) = sup ^X ∆χF*0(Ti (x)) — log(F*0(x)) ≤ 2Mf*/—log (mF*0)=ML
xx
i=1
Q
inf L (F*0, x) = inf ^X ∆χF*0(τi (x)) — log(F*0(x)) ≤ 2mp*o — Iog(MF*o) = m£
x	x i=1
where MF*0 = maxx∈[-1,1] F*0 (x) and mF*0 = maxx∈[-1,1] F*0 (x). Using Hoeffding’s inequal-
ity,
Pr (∣Ex 〜D [L(F *0, X )i - L(F *0, X )∣ ≥ E) ≤ exp (-	2nE2
1	1	∖	(ML- mL)
Taking n as
n≥O
-mL)2 (Q + 1)2 Uh (logm)2
E2
With at least probability 1 - exp -
2n2
(ML-mL )2
IEx〜D [L(F*0,x)i - L(F*0, X)∣ ≤ E
(E.8)
□
Corollary E.1. Under same setting as Lemma D.2 and
then with at least 0.92 - 2 exp
we get
n≥O
- exp -
-mL)2 (Q + 1)2 Uh (logm)2
E2
e2 m
128c1U2 log m
- exp -
2ne2
(ML-mL )2
probability,
1	-1
Esgd T X Ex〜D [L(ft0,x)]
T-1
t=0
-Ex〜D [l(F*0,x)] ≤ O(e)
Proof. Using Lemma D.2, Lemma E.2 and Lemma E.3, with at least 0.92 - 2exp (-港)
—
exp -
2m
128c1U2 log m
- exp -
2n2
probability, we get
1	-1
Esgd T X Ex〜D [L(ft0,x)]
T-1
t=0
-Ex〜D [l(F*0,x)] ≤ O(e)
Theorem E.1. (loss function is close to optimal) For every E ∈ (0, 1), there exist m >
Poly (Uh, ɪ) ,η
O( m) and T = O
Uh log m
, for any target function F*0 with finite second
□
T2
order derivative and number ofquadrature points Q ≥ 4MF *[ +4K2 and number of training points
n≥O
T2
,with at least 0.92-2exp (-8mɔ -exp
—
2m
128c；Uh log m
—
exp -
2n2
(ML-mL )2
probability, we have
Esgd
1 T-1
T EEx〜D [L(ft,x)]
T t=0
-Ex〜D [L(F*, x)] ≤ O(E)
31
Under review as a conference paper at ICLR 2021
where Uh is the complexity Oftargetfunction defined in B.1 and
MF *00 =
x
MF *0 =
x
mF */ =
x∙∙
sup F*" (x)
∈[-i,i]
sup F *，(x)
∈[-i,i]
inf F*0 (x)
∈[-i,i]
ML = 2Mf*o — log (mf* >)
m^ = 2mF*o — log (MF*,)
k2=o UUh (IOg m)1∙5)
€
Proof. First, we will try to bound
Q
∣L(F*0,x) — L(F*,x)∣ ≤ X∆χF*0 (Ti (x)) — F*(x)
i=1
2Mf * 00
≤ Q
Similarly, bounding error for f0, we will get
L(ft,x) — L(ft,x)
∣ Q
∣ ≤ Eδx∕0 (Ti(X)) — ft(X)
i=1
≤ 2 (SUPX f0'(X))
一 Q
32
Under review as a conference paper at ICLR 2021
To get supχ f00(x), we will use Eq.(E.6).
sup ft (x) ≤ sup INt(X)I
xx
m
≤ sup ɪ2 ar0σ0 ((WrO + Wr)x + br0 + %) (WrO + Wr)
x
r=1
m
≤ SUPE ar0(Wr0 + Wr) I [(Wr0 + Wr )x + br0 + / ≥ θ]
x
r=1
≤ sup ɪ2 ar0(Wr0 + Wr) I [Wr0x + br0 ≥ 0] + ɪ2 ar0(Wr0 + Wr) I [(Wr0 + Wr )x + br0 + 眩 ≥。]
x r∈H
r/H
≤ sup
x
ɪ2 arθWrθI [Wr0X + br0 ≥ 0] + ɪ2 ar0wrI [Wr0X + br0 ≥ 0]
r∈H
r∈H
+ ɪ2 αr0Wr0l [(Wr0 + Wr )x + br0 + btr ≥ θ] + ɪ2 &丁0叱I [(wr0 + Wr )x + br0 + / ≥ θ]
r∈H
r/H
(i) “	八	、，(C	E---∖ / …
≤ 16ciC6€a (logm) + m I2c1 €a√2logm) (ηAT)
4∖f2ηm√m∆∆ t
+	C2
+	C2
√π
4√2ηm√mA t
*√2io^) (2c66√mgm
匕i€a √2 log m) (ηAT)
≤ O(€)+ m(2c1€a √2 log m) A
+	C2
+	C2
√π
4√2m√mZ∆
*√2io^)( 2c6√√2og^) -U⅛
m m	ʌ/m	) 4m€j€
匕i€a √2 log m) A
≤ O (€) + O (&log m ) + OM (IOg m)1.5 ) + O (€)
Define K2 as upper bound on SUPX f0 (x),
K2 = O (次(IOg m)1.5
Taking Q as
2Mf *〃 +2K2
(E.9)
Using given value of Q, we get that
∣L(F*0,x) - L(F*,x)
∣L(∕t,x) - L(ft,x)
Using these relations, we get
Esgd
ι T-1
T EEX〜D [L(ft,x)]
t=0	.
—Ex〜D [L(F*, x)∖ ≤。(€)
(E.10)
(E.11)
4√2m√m2∆
€
€
Q ≥
€
€
≤ €
≤ €
33
Under review as a conference paper at ICLR 2021
By definition of KL divergence, we get
Esgd
1 T-1
T EKL (PF*,Z||Pft,Z)
T t=0
≤ O()
□
F Problem in Training of Constrained Normalizing Flow
In this section, we provide reasons and details why changing initialization will not solve the problem
(described in section 2.1 ) in the training of Constrained Normalizing Flow. The neural network in
Constrained Normalizing Flow (CNF) is defined as
m
N(x) = τ ar0 tanh ((wr0 + wr) x + (br + br0)) , with constraints wr0 + wr ≥ , for all r.
r=1
Here, > 0 is a small constant and τ is a normalization constant which only depends on m. The
pseudo network for this neural network will be
m
P(x) = T ɪ2 ar0 (tanh(Wr0X + br0) + tanh0(Wr0X + br0)(wrX + br)),
r=1
with constraints Wro + Wr ≥ G for all r. We decompose P(x) into two parts: P(x) = Pc(x) + P'(x),
where
mm
Pc(x) = τ ɪ2 aro (tanh(wrox + bro)) and P'(x) = T ɪ2 aro (tanh0(wrox + br0)(wrX + b)).
Note that Pc(x) only depends upon initialization and does not depend on Wr and br. Hence, it can
not approximate the target function after the training, therefore P'(x) needs to approximate target
function with Pc(X) subtracted. Note that normalization constant T is necessary to keep p`(x) as same
order of F*0(χ) with Pc(χ) subtracted. Note that Now, We will show that Pe(X) can not approximate
“sufficiently non-linear” functions. Using half-normal distribution initialization of Wro with (mean,
variance) of normal distribution as(0, ml) and normal distribution initialization of bro with (mean,
variance) as(0, ml), Wro and |bro| are O (√√∣m) with high probability; therefore, ∣Wroχ + bro|
is O ( √√mmm ). Using the fact that tanh0(y) ≈ 1 for small y, we get that tanh0 (WroX + bro) ≈ 1
for sufficient large m. In such cases, p` (x) becomes linear function in X and won,t be able to
approximate sufficiently non-linear function. Using other variance of initializations leads to problem
in other parts of the proof.
Remark F.1. Using different variance in initialization of Wro and bro will not solve the problem in
the training of CNFs (mentioned in section 2.1).
Informal Proof: As described above, if we use variance of initialization of Wro and bro as ml, then
Pe(X) cannot approximate “sufficiently non-linear” target function. The same problem will remain if
variance of initialization of w『o and bro is o (ιogm). For variance of initialization of w『o and bro is
Ω (ιogm) and O(1), we will prove by contradiction that there doesn,t exist any pseudo network with
sufficiently small norm on ∣∣θ*k2 for which the pseudo network can approximate the target function
and for bigger norm on ∣∣θ*∣2, pseudo network P(x) during the training does not stay close to N(x).
Remark F.2. There doesn't exist any pseudo network with sufficiently small norm on ∣∣θ*∣∣2 for
which the pseudo network can approximate the targetfunction andfor bigger norm on ∣∣θ*∣2, pseudo
network P(X) during the training does not stay close to N (X).
Informal Proof: Suppose there exist P*(x) function which approximates the target function F*0 (x).
As described earlier, PC (x) only depends upon initialization and does not depend on Wr and br. Hence,
34
Under review as a conference paper at ICLR 2021
it can not approximate the target function after the training, therefore P' (x) needs to approximate
target function with Pc (x) subtracted. Hence, Pc (x), P' (x) and F*0(χ) should be Θ(1). From the
condition that Pc (x) needs to be O(1), we get that T will be o ( —ɪ-- ) for the considered range
c	σwbm
of variance of Wr0 and br0 where σwb is variance of Wr0 and br0. We denote variance of ar0 as σa.
From the condition that P'c(x) needs to be Θ(1), we need
m
T ɪ2 a：o (tanh0(wrox + b：o) (wcx + b：)) = Θ (1)
r=1
=⇒ τmσapl°g^kθ*k∞ = θ ⑴=⇒ kθ*k∞ = θ Qmσa√log m
Using norm equivalance, we need
1
kθck2 = Θ
T √mσa √logm
Doing similar calculation as in Lemma D.2 for the loss of normalizing flow LG, we will get Eq.(D.3)
and from Eq.(D.3), to get small training error, we need
肆■ = O㈤
ηT
=⇒ ηT = θ( ≡c!)
=⇒ ηT = O (emτ2σ12 log m
(F.1)
Now, to find coupling of function N(x) and P(x) as well as derivative of function N0(x) and P0(x),
we find upper bound on the derivative of loss function and wr and br .
∂LG (ft, x)
∂wr
=τ Nt(x)(ar0σ0((wr0 + wrt)x + br0 + btr))
—
τ
W 、(arθ(σ ((Wr0 + Wr)x + bro + br) + (Wr0 + Wr)xσ ((Wr0 + Wr)x + b：0 + b：)))
N0(x)	r	r	r	r	r
We assume that LG(ft, x) is L1-lipschitz continuous wrt N and L2-lipschitz continuous wrt N0.
Assuming ∣σ0(.)∣ ≤ 1 and |x| ≤ 1,
∂LG(ft, x)
∂wr
≤ τL1ar0 + τL2ar0(1 + |w； + Wr0∣∣σ”((wrθ + Wr)x + b『0 + b：)|)
Assuming ∣σ00(.)∣ ≤ C,
∂LG(ft , x)
≤ τLia：o + τL2a：o(1 + C(|w：| + w：o))
∂wr
Using Lemma H.2 for。丁0 and Wro, with at least 1 - -1 - ɪ probability,
c1	c2
∂LG(ft, x)
∂wr
≤ 2CcσααPp2logm) (L 1 + L2(1 + C (|w：| + 2c2σwb√2logm)))	(F.2)
For projected gradient descent,
t-1
|Wrt | ≤ ηX
i=0
t-1
∂LG(ft,xi)
∂wr
≤ η X ( (2c1σaTPZ log m) (L 1 + L2 + 2c2CσwbL2 p2log m) +(2。1。@T√2log m) L2C|w"
i=0
≤ 22fηι^σaτp2logm)(Li + L2 + 2c2CL20wbp2logm) t + 22ηc^σapp2log mL?C
|wri |
35
Under review as a conference paper at ICLR 2021
Define α and β as
τσa √2log m) (L ι + L2 + 2c2 CL2 σw b √2 log m
Using α and β,
α
|wr| ≤ αt+β(X |wr|
∖i=0
t-1	t-2
where E |w； | ≤ α(t - 1) + (1+ β) ( E |w； |
i=0	∖i=0
≤ α ((t - 1) + (1+ β)(t - 2)) + (1 + β)2 (X |wr ∣)
≤α ((t- ι) + (ι+β)(t- 2) + (ι+β)2(t - 3))+(1+β)3 (^X IWrI
∖i=0
In general, we can write
t-1	∕t-tz-1	∖	/ t0
E|wr∣≤α E (1+β)i-1(t-i) +(1+β) j-1)E*।
i=0	∖ i=1	I	∖i=0
Taking t0 = 0,
t-1
E |wr| ≤«
i=0
1+β)i-1(t - i)
Pi-1 (1 + β)(i-1)(t - i) is sum of an arithmetic-geometric progression (AGP). Using Fact H.6,
E |wr |≤ α 1 + β)i-1(t - i))
i=0
α
(t- 1) - (1+ β)t-1	(1+ β)(1 - (1+ β)t-2)
—
α
-β	β2
β(1 + β)t-1 - β(t - 1) - (1 + β) + (1 + β)t-1
α
β2
(1 + β)t - (1 + βt)∖
β2
(F.3)
Using Eq.(F.3) to bound ∖wtr |,
(1 + β )t-(i + βt)
|w； | ≤ a I t + β
β2
((1+ β)t - 1、
α(一β 一)
/ L1 + L2 + 2c2CL2σwb∙∖∕2 log m
L 2。
1 + 2ηcισaτ√2log mL2C) - 1 ) = ∆W
Similarly, we
∂LG(ft,x) _
∂br
Nt(X)T (αr0σ0((wr0 + Wr)x + br0 + b)) - NTx) (αr0(wr0 + Wr)σ00((wr0 + Wr)x + br0 + b))
36
Under review as a conference paper at ICLR 2021
We assume that Lσ(ft, x) is Lι-lipschitz wrt Nt(X) and L2-lipschitz wrt Nt(X). Additionaly,
assuming that ∣σ0(.)∣ ≤ 1 and ∣σ00(.)∣ ≤ C,
∂LG(ft,x)
∂br
≤ L 1°r0T + L2arθCτ(Wr0 + ∣Wtr ∣)
Using Lemma H.2 for a『0 and Wr0, with at least 1 ——-——-probability,
J	…	…'	ClCD	∙z '
∂LG(ft ,x)
∂br
≤ (2c1σα，2log m) τ (L1 + L2C∣w" + 2c2CL20wb/2logm
(F.4)
For projected gradient descent,
t-1
∣br ∣≤ η X
i=0
∂LG(ft,Xi)
-而r
∣wr ∣
2ηc-σaτ√210gm (L 1 + 2c2L2Cσwb√210gm) t + 2ηc-σaτL2C
Using Eq.(F.3),
步：∣ ≤ 2ηc-σaτ√2logm (L 1 + 2cL2Cσwbp2logm) t
~ ~
〜
+ 2ηc1σaL2C √21ogm
τ . τ . rʌ Cr	/m
L- + L2 + 2c2 CL2σwb ^∖∕2 log m
, T----- ~ i-.-八
2ηc-σa √2 log m L 2 C2
1 + 2ηc-τσa √2log mL2C) - 1
2ηc1σaτ√2logm (L 1 + 2c2L2Cσwb√2logm) t
+
τ . τ . rʌ C 宁	/m
L- + L2 + 2c?CL2σwb∖∣2 log m
〜
L2C
1 + 2ηc-σaτ√2 log mL2Cɔ - 1 ) = ʌb
Finding lower bound on ʌtb, we get
ʌb ≥ 2ηc-σaτ√2 log m (L 1 + 2c?L2Cσwb√2 log m)t
(τ . τ . rʌ τ	/m
L1 + L2 + 2c2CL2σwbλ/2 log m
L2C
1 + 2ηtc-σaτ ʌ/ 2 log m L?C) — 1
2ηc-σaτ√2 log m (L 1 + 2c2L2Cσwb√2 log m) t
+
t . τ . rʌ Cq	/m
L- + L2 + 2c?CL?qwb'2 log m
〜
L2C
Ω (ηtσaτσwb log m)
We get similar lower bound on ʌtw .
ʌw = Ω (ηtσaτσwb log m)
37
Under review as a conference paper at ICLR 2021
Now, we show coupling between Nt0 (x) and Pt0 (x). Assuming that σ0 is C-lipschitz continuous,
m
|Nt0(x) -	Pt0(x)| = τ	ar0	(wrt +	wr0)	σ0 ((wr0	+ wrt)x +	br0 +	btr)	- σ0 (wr0x + br0)
r=1
- σ00 (wr0x + br0) wr0(wrt x + btr)
mm
≤ Cτ	2ar0 |wrt + wr0| + Cτ	ar0wr0|wrt x + btr|
r=1	r=1
m
= Cτ	ar0 2|wrt + wr0| + wr0 |wrt x + btr|
r=1
≤) CT (2cισo√2l0gm) (kWtkι + ∣∣W0k1 +2c2σwb√2⅛m (∣∣Wtkι + ∣∣Btkι
=O (τσaσwbm log m (∆j + ʌb))
where inequality (i) follows from Fact H.2. Now, using t = T to find upper bound on
|NT0 (x) - PT0 (x)|, we get
NT(X)- PT(x)| = O (τσθσwbmlog m (∆ξ + ∆T))
= O τ σaσwbm log m ∆Tw + ∆bT
= O τ σaσwbm log m ∆Tw + ∆bT
= O (τ σaσwbm log m (ηT σaτ σwb log m))
= O ηTτ2σa2σw2 bm (log m)2)
T2σ2σWbm (log m)2 ʌ
mτ2 σa2 log m
σW b (log m)
For σwb in range Ω (IogIm) and O(1), |NT(x) - PT(x)| can become very high and in those cases,
NT0 (x) and PT0 (x) willnot remain close.
G Additional experiments
In this section, we show experimental results on synthetic 1D data to support our theoretical findings.
We use the same architecture and initialization as described in Sec. 2 for both constrained and
unconstrained normalizing flows. In all our experiments, we fix the weights of the output layer and
train the weights and biases of the hidden layer. For training, we use mini-batch SGD with batch size
32. We use 2 datasets each with 10,000 data points. One of the dataset is mixture of 2 Gaussians and
other one is mixture of 3 beta distributions. All results are averaged over 3 different iterations.
G. 1 Results for unconstrained normalizing flow
In Fig. 2, we show comparison of data distribution and generated data distribution for unconstrained
normalizing flow. Unconstrained normalizing flow with exponential distribution as a base distribution
learns the data distribution well. We study the effect of overparameterization on L2-norm of Wt and
Bt and convergence speed. To reproduce situation similar to the theoretical analyses for unconstrained
normalizing flow, we choose learning rate as mm where C is a constant. The first row of Figure 3
contains results for mixture of Gaussians dataset and the second row contains results for mixtures of
beta distributions dataset. From Fig. 3, we see that L2-norm of Wt and Bt decreases with increasing
m. Moreover, the change is proportional to 1/√m which is similar to the bound in theoretical result.
From the last column of Fig. 3, we see that the training speed for different values of m remains
almost constant. Our choice of T in theoretical analysis also poly-logarithmically depends upon m.
We obtained similar results for Gaussian distribution as well.
38
Under review as a conference paper at ICLR 2021
(a) Mixture of Gaussian
data distribution
(b) Mixture of Gaussian
generated data
(c) Mixture of Beta data
distribution
(d) Mixture of Beta gener-
ated data
Figure 2:	Comparison of data distribution and generated data for mixture of Gaussian and beta
distributions
Figure 3:	Effect of over-parameterization on training of unconstrained normalizing flow on mixture
of Gaussian and mixture of beta distributions
39
Under review as a conference paper at ICLR 2021
Figure 4: Effect of over-parameterization on training of small sized constrained normalizing flow
G.2 Results for constrained normalizing flow
In Sec. 2.1, we suggested that high overparameterization may adversely affect training for constrained
normalizing flows. We now give experimental evidence for this in Figs 4, 5 and 6. We use Gaussian
distribution as a base distribution for all our experiments of constrained normalizing flow. In Fig.4,
we see that for neural network with m = 100 and m = 400, the training loss decreases stably. In
Figs. 5 and 6, we see that as we increase the learning rate, training becomes more stable for larger m.
Note that for learning rate equal to 0.025 and 0.0125, constrained normalizing flow with m = 1600
doesn’t learn anything due to small learning rate. We observe that the L2-norms of Wt and Bt for
m = 6400 are at least at large as those of m = 1600. On both datasets, as we increase the learning
rate, L2-norm of Bt increases (except for learning rate=0.05 of mixture of beta distribution) and
learning of constrained normalizing flow becomes more and more stable. We also experimented
with more number of epochs (1000 epochs) for mixture of Gaussian dataset for number of hidden
layers m = 1600, 6400 (see Fig.7). These observations support our claim in Sec.2.1 that for learning
and approximation of overparameterized constrained normalizing flow, neural networks need large
L2-norms of Wt and Bt.
H Useful facts
Lemma H.1. Suppose Zk 〜N(0, σ2) and Y = En=I Z is chi-squared distribution Withfollowing
property for all t ∈ (0, 1).
Pr
1n
n
k=1
- σ2
≥t
≤ 2 exp
Proof. From example 2.11 from WainWright (2019), for Zk 〜N(0,1) and Y = Pn=I Zk is
chi-squared distribution with following property for all t ∈ (0, 1).
Pr
1n
n X Zk2 -1
k=1
Using above equation for Z,
Pr
Pr
1 n Z2
n X 谭-1
k=1
1 n
n X Zk 一
k=1
σ2
≥t
≥t
t
≥ σ2
40
Under review as a conference paper at ICLR 2021
Mixture of Gaussian Distributions (l.r.=0.0125)
20	*K)BOaO
Number of epochs
NumDeror epochs
Number of epochs
Figure 5:	Effect of over-parameterization on training of constrained normalizing flow on mixture of
Gaussian dataset for number of hidden layers m = 1600, 6400
41
Under review as a conference paper at ICLR 2021
0	20	«	60 SO 100
Number of epochs
Mixture of Beta Distributions (I.r. =0.025)
20 Φ £0 SO 100
Number of epochs
12108 6 4 2
M,≡06UeUU J。UnoU 2一
Mixture of Beta Distributions (l.r.=0.1)
m=1600
m=6400
20	40	60 SO 100
Number of epochs
m=1600
Mixture of Beta Distributions (l.r.≡0.025)
O 20 -W 60 SO IOC
Numberofepochs
Mixture of Beta Distributions (l.r.=0.05)
20 a 60 ao 10β
NUmberof epochs
SSO- 6u-l≡e.JJ.
20	*K)BOaO
Number of epochs
Figure 6:	Effect of over-parameterization on training of constrained normalizing flow on mixture of
beta distribution dataset for number of hidden layers m = 1600, 6400
42
Under review as a conference paper at ICLR 2021
fl4 ι
口弧1
Mixture of Gaussian Distributions (l.r.=0.0125)
2o Mixture of Gaussian Distributions (l.r.=0.1)
---------------------------------m=1600
L8 '	---- m=6400
Number of epochs
Figure 7:	Effect of over-parameterization on training of constrained normalizing flow on mixture of
Gaussian dataset for epochs=1000 and number of hidden layers m = 1600, 6400
43
Under review as a conference paper at ICLR 2021
□
Lemma H.2. Let X1, X2, ..., Xn be independent random variables from N (0, σ2), then with at least
1 - -1 probability, following holds.
max	|Xi | ≤ 2c1 σ 2 log n
i∈{1,2,...,n}
Proof. From Romberg (2012),
E max	|Xi | ≤ σ (ʌ/2 log n + 1)≤ 2σ (ʌ/2 log n
i∈{1,2,...,n}
Assuming n ≥ 2, the last inequality follows. Using Markov’s inequality,
Pr ( max	|Xi| ≥ 2cισ (p2 log n) ∣ ≤ —
i∈{1,2,...,n}	c1
Pr I max	|Xi| ≤ 2cισ (p2 log n) ∣ ≥ 1 ———
i∈{1,2,...,n}	c1
s
□
Lemma H.3. For standard Gaussian random variable X from N(0, σ2), following anti-
concentration inequality holds.
2R
PrM R) ≤ σ√72∏
Proof. (From Du et al. (2018)) For standard Gaussian random variable X,
X
σ
Pr
2R
≤ R) ≤ - 7^=
一广 √2π
Using R = Rσ0, we get the required result.
□
Lemma H.4. Suppose function f : Rd → R is Lg -Lipschitz continuous and Li-coordinate wise
Lipschitz continuous i.e.
|f (a) - f (b)| ≤Lgka-bk
∀a, b ∈ Rd (Standard Lipschitz continuity)
|f (a1,a2, ...,ai, ...,ad) - f(a1,a2,...,bi,…,ad)∣ ≤Li% - bi|
∀a1, a2, ..., ai, ..., ad, bi ∈ R and ∀i ∈ [d] (Coordinate-wise Lipschitz continuity)
If a function f satisfies Li -coordinate wise Lipschitz continuity for all i, then function f follows
following inequality.
n
|f(a1, a2, ..., ad) - f(b1, b2, ..., bd)| ≤	Li |ai - bi|
i=1
Moreover, the function f also satisfies standard Lipschitz continuity with Lg Lipschitz constant where
inequality between Lg and Li is as follows.
44
Under review as a conference paper at ICLR 2021
Proof. Define a = (a1, a2, ..., ad) and b = (b1, b2, ..., bd).
|f(a1, a2,..., ad) — f(b1, b2,..., bd)| ≤ |f(a1, a2, ..., ad) — f(b1, a2,..., ad)|
+ |f(b1, a2, a3, ..., ad) — f(b1, b2, a3, ..., ad)|
+ |f(b1, b2, a3, ..., ad) — f(b1, b2, b3, ..., ad)|
+ ... + |f(b1, b2, ..., bd-1, ad) — f(b1, b2, b3, ..., bd)|
≤L1 |a1 — b1 | + L2 |a2 — b2 | + ... + Ld |ad — bd|
≤tuXd Li2ka — bk2
i=1
where last inequality follows from Cauchy-Schwarz inequality.
□
Fact H.1. (Hoeffding’s inequality on Binomial random variable) If we have a binomial random
variable with parameters n (total number of trials) and p (probability of success). For number
successful trial k ≥ np, following inequality holds.
Pr (X ≥ k) ≤ exp -2n
Fact H.2. (Half-normal distribution) If X follows a normal distribution with with mean 0 and
variance σ2, N 0, σ2 , then Y = |X| = X sign (X) follows a half-normal distribution with mean
E [Y ] = σ√2.
Fact H.3.
Fact H.4.
For a gaussian random variable X 〜N(0, σ2), ∀t ∈ (0, σ), we have
4t
Pr(|XI ≥ t) ≥ 1 — b
5σ
The sum of reciprocals of the squares of the natural numbers is given by
∞2
XL π ≤2
乙 n2	6 ≤ 2
n=1
Fact H.5.
(Theorem 3.1(r5) ofLi & Yeh (2013)) For any α > 1 and X ∈ [θ, a-ɪ),
(1+x)α≤
αx
1	- 1+χ	1 - (α - I) X
Fact H.6.
If Arithmetic-Geometric Progression(AGP) is as follows.
a, (a + d)r, (a + 2d)r2, (a + 3d)r3,  , [a + (n — 1)d] rn-1
where a is the initial term, d is the common difference and r is the common ratio. The sum of the first
n terms of the AGP (Sn) is given by
Sn
a — [a + (n — 1)d] r
dr(1 — rn-1)
1—r
(1 — r)2
1
n
+
Fact H.7. ( Lemma A.3 from Ji et al. (2020) ) The Fourier transform off is defined as
fH=Zfs
The polar decomposition ofthe Fourier transform f is f(w) = ∣∕(W)卜2πiθf (w) with ∣θf (w)∣ ≤ 1.
The Fourier transform f follows below properties.
1. ∣∕(W)I ≤ IlfIlLl for any real number W.
45
Under review as a conference paper at ICLR 2021
2. Let a > 0 be given and define β := 211α.. Ga is Gaussian with coordinate-wise variance
a2. Then IGa∣ = Ga (meaning Ga has no radial component) and
Ga(w) = / ]	Ge(W) = P2∏β2Gβ(W) = √2πG (w∕β)
2πα2
Definition H.1. Let F be a set of functions Rd → R and X = (x1, x2, ..., xn) be a finite set of
samples. The empirical Rademacher complexity of F with respect to X is defined by
ʌ , ___ _
R(X ； F) = Eξ〜{±1}n
1n
sup - ξfXi)(xi)
f∈F n i=1
Lemma H.5. The Rademacher complexity have following properties.
a.	Suppose	|x| ≤ 1 for all X.	The class F =	{x	→ Wx	+b	|	|W|	≤ B, |b|	≤	B} has Rademacher
complexity R(X, F) ≤ √B
b.	Given Fi, F2 classes offunctιons, then R(X; Fi + F2) = R(X; Fi)+ R(X; F2)
c.	Given F1, F2, ..., Fm	classes of	functions from X	→	R	and suppose W ∈ Rm is a
fixed vector, then F0	= {x →	Em=I WrQ (fr(x))	|	fr	∈ Fr} satisfies R(X; F0) ≤
2kWki maxr∈[m] R (X; Fr) where σ is 1-Lipschitz continuous function.
Proof. The b and c parts of the proposition are from Allen-Zhu et al. (2019) . Proof of the a part is as
following.
Using Jensen’s inequality, we get
i=i
B
n Eξ 〜{±i}n
1
R(X , F ) = n Eξ 〜{±i}n
2	0.5
i=i
n
ξixi
i=i
0.5
Using independence of ξi for all i ∈ [n], we get
n2
Eξ	ξixi
i=i
= Eξ	ξiξjxixj
i,j
n
= X |xi|2 ≤n
i=i
Using same technique, We will get same bound for Eξ [|P2i ξi∣].
2B
R(X, F) ≤ 7
Fact H.8. (Rademacher Complexity) If Fi, F2, ..., Fk are k classes of functions Rd → R and
Lx : Rd → [—b, b] is Lg -Lipschitz continuousfunction for any X 〜D, then
sup
f1 ∈F1 ,...,fk ∈Fk
1n
Ex∈D [Lx (fi (X),…,fk (X))] — — L L Lx (f 1 (Xi),…,fk (Xi))
n i=i
ʌ ,
≤ 2R(X; L) + b
n
2
≤
□
46
Under review as a conference paper at ICLR 2021
where L is set of all functions Lx. Using vector contraction inequality from Maurer (2016), we get
sup
f1 ∈F1 ,...,fk ∈Fk
1n
Ex∈D [Lx(f1(X),…,fk(X))I-* M….fk(Xi))
≤ 2√2Lg (XXR(X； Fi)) + b jl2gδ
47