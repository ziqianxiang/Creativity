Under review as a conference paper at ICLR 2021
Disentangling Adversarial Robustness in Di-
rections of the Data Manifold
Anonymous authors
Paper under double-blind review
Ab stract
Using generative models (GAN or VAE) to craft adversarial examples, i.e. gen-
erative adversarial examples, has received increasing attention in recent years.
Previous studies showed that the generative adversarial examples work differently
compared to that of the regular adversarial examples in many aspects, such as attack
rates, perceptibility, and generalization. But the reasons causing the differences
between regular and generative adversarial examples are unclear. In this work, we
study the theoretical properties of the attacking mechanisms of the two kinds of
adversarial examples in the Gaussian mixture model. We prove that adversarial
robustness can be disentangled in directions of the data manifold. Specifically, we
find that: 1. Regular adversarial examples attack in directions of small variance
of the data manifold, while generative adversarial examples attack in directions
of large variance. 2. Standard adversarial training increases model robustness by
extending the data manifold boundary in directions of small variance, while on
the contrary, adversarial training with generative adversarial examples increases
model robustness by extending the data manifold boundary directions of large
variance. In experiments, we demonstrate that these phenomena also exist on real
datasets. Finally, we study the robustness trade-off between generative and regular
adversarial examples. We show that the conflict between regular and generative
adversarial examples is much smaller than the conflict between regular adversarial
examples of different norms.
1	Introduction
In recent years, deep neural networks (DNNs) (Krizhevsky et al. (2012); Hochreiter and Schmidhuber
(1997)) have become popular and successful in many machine learning tasks. They have been used in
different problems with great success. But DNNs are shown to be vulnerable to adversarial examples
(Szegedy et al. (2013); Goodfellow et al. (2014a)). A well-trained model can be easily attacked by
adding a small perturbation to the image. An effective way to solve this issue is to train the robust
model using training data augmented with adversarial examples, i.e. adversarial training.
With the growing success of generative models, researchers have tried to use generative adversarial
networks (GAN) (Goodfellow et al. (2014b)) and variational autoencoder (VAE) (Kingma and Welling
(2013)) to generate adversarial examples (Xiao et al. (2018); Zhao et al. (2017); Song et al. (2018a);
Kos et al. (2018); Song et al. (2018b)) to fool the classification model with great success. They found
that standard adversarial training cannot defend these new attacks. Unlike the regular adversarial
examples, these new adversarial examples are perceptible by humans but they preserve the semantic
information of the original data. A good DNN should be robust to such semantic attacks. Since the
GAN and VAE are approximations of the true data distribution, these adversarial examples will stay
in the data manifold. Hence they are called on-manifold adversarial examples by (Stutz et al. (2019)).
On the other hand, experimental evidences support that regular adversarial examples leave the data
manifold (Song et al. (2017)). We call the regular adversarial examples as off-manifold adversarial
examples.
The concepts of on-manifold and off-manifold adversarial examples are important. Because they can
help us to understand the issue of conflict between adversarial robustness and generalization (Stutz
et al. (2019); Raghunathan et al. (2019)), which is still an open problem. In this paper, we study
the attacking mechanisms of these two types of examples, as well as the corresponding adversarial
1
Under review as a conference paper at ICLR 2021
training methods. This study, as far as we know, has not been done before. Specifically, we consider
a generative attack method that adds a small perturbation in the latent space of the generative models.
Since standard adversarial training cannot defend this attack, we consider the training methods that
use training data augmented with these on-manifold adversarial examples, which we call latent space
adversarial training. Then we compare it to standard adversarial training (training with off-manifold
adversarial examples).
Contributions: We study the theoretical properties of latent space adversarial training and standard
adversarial training in the Gaussian mixture model with a linear generator. We give the excess risk
analysis and saddle point analysis in this model. Based on this case study, we claim that:
•	Regular adversarial examples attack in directions of small variance of the data manifold and
leave the data manifold.
•	Standard adversarial training increases the model robustness by amplifying the small vari-
ance. Hence, it extends the boundary of the data manifold in directions of small variance.
•	Generative adversarial examples attack in directions of large variance of the data manifold
and stay in the data manifold.
•	Latent space adversarial training increases the model robustness by amplifying the large
variance. Hence, it extends the boundary of the data manifold in directions of large variance.
We provide experiments on MNIST and CIFAR-10 and show that the above phenomena also exist in
real datasets. It gives us a new perspective to understand the behavior of on-manifold and off-manifold
adversarial examples. Finally, we study the robustness trade-off between generative and regular
adversarial examples. On MNIST, robustness trade-off is unavoidable, but the conflict between
generative adversarial examples and regular adversarial examples are much smaller than the conflict
between regular adversarial examples of different norms. On CIFAR-10, there is nearly no robustness
trade-off between generative and regular adversarial examples.
2	Related work
Our work is related to attack and defense methods. Specifically, we care about attacks and defenses
with generative models.
Attack Adversarial examples for deep neural networks were first intruduced in (Szegedy et al.
(2013)). However, adversarial machine learning or robust machine learning has been studied for
a long time (Biggio and Roli (2018)). In the setting of white box attack (Kurakin et al. (2016);
Papernot et al. (2016); Moosavi-Dezfooli et al. (2016); Carlini and Wagner (2017)), the attackers have
fully access to the model (weights, gradients, etc.). In black box attack (Chen et al. (2017); Su et al.
(2019); Ilyas et al. (2018)), the attackers have limited access to the model. First order optimization
methods, which use the gradient information to craft adversarial examples, such as PGD (Madry
et al. (2017)), are widely used for white box attack. Zeroth-order optimization methods (Chen et al.
(2017)) are used in black box setting. Li et al. (2019) improved the query efficiency in black-box
attack. HopSkipJumpAttack (Chen et al. (2020)) is another query-efficient attack method.
Generative adversarial examples Recently, generative models have been used to craft adversarial
examples (Xiao et al. (2018); Song et al. (2018b); Kos et al. (2018); Schott et al. (2018)). The
adversarial examples are more natural (Zhao et al. (2017)). These adversarial examples lie in the data
manifold, and they are called on-manifold adversarial examples.
Defense Training algorithms against adversarial attacks can be subdivided into the following cate-
gories. Adversarial training: The training data is augmented with adversarial examples to make the
models more robust (Madry et al. (2017); Szegedy et al. (2013); Tramer et al. (2017)). Preprocessing:
Inputs or hidden layers are quantized, projected onto different sets or other preprocessing methods
(Buckman et al. (2018); (Guo et al. (2017); Kabilan et al. (2018)). Stochasticity: Inputs or hidden
activations are randomized (Prakash et al. (2018); Dhillon et al. (2018); Xie et al. (2017)). However,
some of them are shown to be useless defenses given by obfuscated gradients (Athalye et al. (2018)).
Adaptive attack (Tramer et al. (2020)) is used for evaluating defenses to adversarial examples.
Defense with generative model Using generative models to design defense algorithms have been
studied extensively. Using GAN, we can project the adversarial examples back to the data manifold
2
Under review as a conference paper at ICLR 2021
(Jalal et al. (2017); Samangouei et al. (2018)). VAE is also used to train robust model (Schott et al.
(2018)).
3	Problem description
Original space adversarial training: Consider the classification problem of training a classifer
fθ to map the data points x ∈ X ⊂ Rd to the labels y ∈ Y , where X and Y are the input data space
and the label space. The classifier fθ is parameterized by θ. We assume that the data pairs (x, y)
are sampled from the distribution P(X, Y ) over X × Y. Standard training is to find the solution
of minθ E(χ,y)〜P'(fθ(x), y), where '(∙, ∙) is the loss function. The goal of dversarial training is to
solve the minimax problem
minE(x,y)〜p“ maχ '(fθ(x0),y),	(1)
θ	kx-x0k≤ε
where ε is the threshold of perturbation. Here we can use 'ι, '2 or '∞-norm (Madry et al. (2017)).
The inner maximization problem is to find the adversarial examples x0 to attack the given classifier
fθ. The outer minimization problem is to train the classifier to defend the given adversarial examples
x0 . we refer to these attacks as regular attacks. We refer to these minimax problems as standard
adversarial training or original space adversarial training.
Latent space adversarial training: We assume that the data lie in a low dimensional manifold
of Rd . Furthermore, we assume the true distribution D is a pushforward from a prior Guassian
distribution Z 〜N(0, I) using G(z), where G : Z → X is a mapping from the latent space Z to the
original space X. This is a basic assumption of GAN or VAE. Let I : X → Z be the inverse mapping
of G(z). The goal of latent space adversarial training is to solve the following minimax problem
minE(χ,y)〜P ll , max- '(fθ(G(ZO)),y).	(2)
θ	kz0-I(x)k≤ε
Unlike the regular attacks, the distance between the original examples and adversarial examples can
be large. To preserve the label of the data, we use the conditional generative models (e.g. C-GAN
(Mirza and Osindero (2014)) and C-VAE (Sohn et al. (2015))), i.e. the generator Gy(Z) and inverse
mapping Iy(x) are conditioned on the label y, for adversarial training. We refer to these attacks as
generative attack, and these adversarial training as latent space adversarial training.
Regular attack algorithms Two widely used gradient-based attack algorithms for the inner maxi-
mization problem in equation (1) are fast gradient sign method (FGSM) (Goodfellow et al. (2014a))
and projected gradient descend (PGD) (Madry et al. (2017)). Using FGSM, the adversarial exam-
ples are calculated by
x0 = X + εsgn(Vχ'(fθ (x), y)),
where Vx denotes the gradient with respect to x. PGD attempts to find a near optimal adversarial
example for the inner maximization problem (1) in multiple steps. In the tth step,
Xt+1 = Πx+S [xt + αVx'(fθ (χt),y)∕kvx'(fθ (χt),y)k],
where α is the step size, Πx+s [∙] is the projection operator to project the given vector to the constraint
x + S = {x0|kx - x0k ≤ ε}.
In the whole paper, we refer to these as FGSM-attack and PGD-attack, and the corresponding original
space adversarial training as FGSM-adv and PGD-adv. FGSM-attack is a weak attack and PGD-attack
is a stronger attack. In section 5, we use them to show that a strong original space adversarial training,
PGD-adv, does not work well against a weak attack, FGSM-attack in the latent space. Conversely,
latent space adversarial training cannot defend a simple FGSM-attack in the original space.
Generative attack algorithm In our experiments, we use FGSM in the latent space for the inner
maximization problem in equation (2)
Z0 = I(x) + εsgn(Vz '(fθ (G(Z)),y)).
Because of the mode collapse issue of GAN (Salimans et al. (2016); Gulrajani et al. (2017)), adding
a small perturbation in the latent space of GAN may output the same images. Thus we use VAE in
our experiments. we refer to this generative attack and latent space adversarial training as VAE-attack
and VAE-adv.
3
Under review as a conference paper at ICLR 2021
4 Theoretical analysis
In this section, we study the difference between adversarial training in the latent space and in the
original space. We study the simple binary classification setting proposed by Ilyas et al. (2019).
The main reason for using this model is that we can find the optimal closed-form solution, which
gives us insights to understand adversarial training. For a more complex model, we can only solve it
numerically. We leave all the proofs of the lemmas and theorems in appendix A.
4.1	Theoretical model setup
Gaussian mixture (GM) model Assume that data points (x, y) are sampled according to y 〜
{-1,1} Unifomly and X 〜N(yμ*, Σ*), where μ* and Σ* denote the true mean and covariance
matrix of the data distribution. For the data in the class y = -1, we replace x by -x, then we can
view the whole dataset as sampled from D = N(μ*, Σ*).
Classifier The goal of standard training is to learn the parameters Θ = (μ, Σ) such that
Θ = arg min L(μ, Σ) = arg min Ex〜D ['(x; μ, Σ)],	(3)
μ,∑	μ,∑
where '(∙) represents the negative log-likelihood function. The goal of adversarial training is to find
Θr = arg min Lr (μ, Σ) = argminEx〜D[ max '(x0; μ, Σ)].	(4)
μ,Σ	μ,Σ	kx-x0 k≤ε
We use L and Lr to denote the standard loss and adversarial loss. After training, we classify a new
data point X to the class sgn(μτΣ-1x).
Generative model In our theoretical study, we use a linear generative model, that is, probabilistic
principle components analysis (P-PCA) (Tipping and Bishop (1999)). P-PCA can be viewed as linear
GAN (Feizi et al. (2017); Feizi et al. (2020)) and linear VAE (Dai et al. (2017)).
Given dataset {χi}n=ι ⊂ Rd, let μ and S be the sample mean and sample covariance matrix. The
eigenvalue decomposition of S is S = UΛUT, then using the first q eigenvactors, we can project
the data to a low dimensional space. P-PCA is to assume that the data are generated by
x = WZ + μ + € where Z 〜N(0,I) , e 〜N(0, σ2I),
Z ∈ Rq and W ∈ Rd×q. Then we have X 〜N(μ, WWT + σ2I), x|z 〜N(WZ + μ,σ2I) and
z|x 〜N(PTWT(X — μ),σ2PT) where P = WT W + σ2I. The maximum likelihood estimator
of W and σ 2 are
Wml = Uq (Aq - σMLI)1/2
1d
and σM L = d~ E λi,
q i=q+1
where Uq is the matrix of the first q columns of U, Aq is the matrix of the first q eigenvalues of A.
In the following study, we assume that n is large enough such that we can learn the true μ* and Σ*.
Thus we have S = Σ*, Uq = Uq*, Aq = Aq* for the generative model.
4.2	Minimax problem of latent space adversarial training
To perturb the data in the latent space, Data will go through the encode-decode process X → Z →
∆Z + Z → X0 . Based on the probabilistic model, we may choose Z with the highest probability or
just sample it from the distribution we learned. Hence, we could have different strategies. Below we
list 3 strategies. Strategy 1 is used in practice and the other two are alternative choices. In lemma 1
we show that these strategies are equivalent under the low dimensional assumption. Hence, we do not
need to worry about the effect of sampling strategies.
Strategy 1: Sample X 〜 D, encode Z = arg max q(z∣x) = P-1WT(x - μ*), add a perturbation
∆z, and finally, decode Xadv = arg maxp(x∣z + Δz) = W(Z + Δz) + μ*.
Strategy 2: Sample X 〜D, then sample Z 〜q(Z∣X), add a perturbation Δz, and finally, sample
Xadv 〜p(X∣Z + Δz).
4
Under review as a conference paper at ICLR 2021
Strategy 3: Sample Z 〜N(0, I), add a perturbation ∆z, and then sample Xadv 〜p(x|z + ∆z). In
this strategy, xadv can be viewed as the adversarial example of x = arg maxx q(z|x).
The following lemma shows that the adversarial examples can be unified in one formula. Hence
sampling strategies will not affect our analysis.
Lemma 1 (Adversarial examples perturbed in the latent space). Using these 3 strategies, the adver-
sarial examples can be unified as
Xadv = x0 + W∆z and X 〜Dj = N(μ*, U*Λ(j)UT), j = 1,2,3,	(5)
where
Λ(1) = (Λq - σ2I)2Λq-1 0 Λ(3) = Λq	0
Λ	=	0	0 , Λ =	0 σ2I	,
Λ(2)	(Λq	- σ2I)2Λq-1 +	(Λq - σ2I)Λq-1σ2	+ σ2I	0
Λ =	0	σ2I .
If the data lie in a q dimensional subspace, i.e. the Covariance matrix Σ* is rank q, we have
A(I) = λ⑵=Λ⑶=Λ*. Then D0 = D.
In general, the adversarial example can be decomposed into 2 parts, the change of distribution
χ0 〜D0 and the small perturbation W∆z. Therefore the adversarial expected risk can be written as
the following minimax problem
min Lis(μ, Σ; Dj)= minEχo〜do max '(x0 + W∆z, μ, Σ), j = 1, 2, 3.	(6)
We aim to analyze the different properties between the minimax problems in equations (4) and (6).
We give the excess risk and optimal saddle point analysis in the following subsections.
4.3	Excess risk analysis
We consider the difference between Lls and L given the true Θ*, i.e. Lls(Θ*; Dj0 ) - L(Θ*; D). It
characterizes the excess risk incured by the optimal perturbation. To derive the expression of excess
risk, we decompose it into two parts
Lls(Θ*;Dj)-L(Θ*;D) = Lis(Θ*; Dj)-L(Θ*; Dj) + L(Θ*; Dj)-L(Θ*; D).	(7)
'----------{z---------} '-----------{z---------}
perturbation	change of distribution
To simplify the notation, we consider the Lagrange penalty form of the inner maximization problem
in equation (6), i.e. max'(x0 + W∆z, μ, Σ) - L∣∣∆zk2∕2, where L is the Lagrange multiplier. The
following theorem gives the solution in the general case.
Theorem 2 (Excess risk). Let Lls and L be the loss with and without perturbation in latent space
(equations (6) and (3) respectively), given the non-robustly learned Θ* = (μ*, Σ*), thus the excess
risk caused by perturbation is
Lls(θ*, Dj)-L®, Dj) = 2 XX h(1+ JiI)λσ+σ2 )2 - 1i V, j = 1, 2, 3.
and the excess risk caused by the changed of distribution is
L(θ*, Dj)-L(θ*, D)=1 log [QQ=14) i+2 (XX λt - d).
2	id=1 λi	2 i=1 λi
It is hard to see which part dominates the excess risk. If we further assume that the data lie in a q
dimensional manifold, the excess risk caused by the change of distribution becomes 0 by Lemma 1.
We have the following corollary.
Corollary 3 (Excess risk). Let Lls and L be the loss with or without perturbation in latent space
(equation (6) and (3) respectively), given the non-robustly learned Θ* = (μ*, Σ*), and rank(∑*)=
q. The excess risk
Lls(Θ*,Dj0) - L(Θ*, D) = O(qL-2).
5
Under review as a conference paper at ICLR 2021
The optimal perturbation in the latent space will incur an excess risk in O(qL-2). The adversarial
vulnerability depends on the dimension q and the Lagrange multiplier L. It does not depend on the
shape of the data manifold. This is because the perturbation constraint (the black block) aligns with
the shape of the data manifold (the ellipse) as we demonstrate in Figure 1 (c). Thus generative attacks
will focus on the directions of the largest q variance.
Then, we analyze the excess risk of original space adversarial training. Since the perturbation thresh-
olds, ε, are on different scales in the original space attack and latent space attack, the corresponding
Lagrange multipliers L are different. We use L0 for original space adversarial training in the following
Theorem.
Theorem 4 (Excess risk of original space adversarial training). Let Lr and L be the loss with or
without perturbation in original space (equations (4) and (3) respectively), given the non-robustly
learned Θ* = (μ*, Σ*). Denote λmin be the smallest eigenvalue of Σ*. The excess risk is
Ω((λminL0)-2) ≤ Lr(Θ*, D)- L(Θ*,D) ≤ O(d(λminL0)-2 ).
If the data lie in a low dimensional manifold, i.e. λmin = 0, the excess risk equals to +∞.
Optimal perturbation in the original space will incur an excess risk in O(d(λminL0)-2). The
adversarial vulnerability depends on the smallest eigenvalues λmin, the dimension d, and the Lagrange
multiplier L0 . The λmin comes from the misalignment between the perturbation constraint (the black
block) and the shape of the data manifold (the ellipse) as we demonstrate in Figure 1 (a). Notice
that λmin also appears in the lower bound. Hence, the excess risk equals to +∞ when λmin = 0.
Thus regular attacks focus on the directions of small variance. Specifically, when λmin = 0, regular
adversarial examples leave the data manifold.
4.4 Saddle point analysis
In this subsection we study the optimal solution of optimization problem (6). Since it is not a standard
minimax problem, we consider a modified problem:
min max Ex，〜Dj'(x0 + W∆z, μ, Σ), j = 1,2,3.	(8)
μN Ex0 Szk=E	j
We will explain more about the connection between optimization problems in equation (6) and (8).
See appendix A. The following theorem is our main result. It gives the optimal solution of latent
space adversarial training.
Theorem 5	(Main result: Optimal Saddle point). The optimal solution of the modified problem in
equation (8) is
μis = μ* and ∑is = U*ΛlsUT,
where
λis = 1	h2λ(j)	+ 4(" -0 ) +	2λij) S1 +	4(λi(j- ɪɪ]	fori	= 1 ≤ q,	λis	= λij) for i	>	q,
andj = 1, 2, 3 corresponding to strategies 1,2 and 3.
We assume that the data lie in a q-dimensional manifold again. Then We have λj/λ% = 1/2 + 1/L +
/1/4 + 1/L ≥ 1 for i ≤ q and λis∕λi = 0 for i > q. Latent space adversarial training increases
the model robustness by amplifying large eigenvalues of the data manifold. The illustration of the
tWo dimensional case is in Figure 1, (c) and (d).
In the same setting, the optimal solution of standard adversarial training (Problem (4)) on the original
space is in Theorem 6 (Which is Theorem 2 in Ilyas et al. (2019)).
Theorem 6	(Optimal saddle point Ilyas et al. (2019)). The optimal solution of the problem in equation
(4) is
μr = μ*
and
∑r=1 ς*+L I+ς*+4 ς*
6
Under review as a conference paper at ICLR 2021
Theorem 6 is for the problem that the covariance matrix is restricted to be diagonal. Consider the
ratio
For small true eigenvalue λi , the ratio is large. Standard adversarial training increases the robustness
by amplifying the small eigenvalues of the data manifold. The illustration of the two dimensional
case in Figure 1, (a) and (b).
Figure 1: Demonstration of theoretical analysis: (a) Regular attacks directions; (b) Optimal saddle
point of original space adversarial training; (c) Generative attacks directions; (d) Optimal saddle point
of latent space adversarial training.
5	Experiments
In this section we report our experimental results on training LeNet on MNIST (LeCun et al. (1998))
and ResNet (He et al. (2016)) on CIFAR-10 (Krizhevsky et al. (2009)) to confirm our theoretical
findings. Both of the two datasets contain 50000 training samples and 10000 test samples. Details of
the hyperparameters setting are in appendix B.
5.1	Eigenvalues of MNIST with and without adversarial training
In this subsection, we show that the properties of the eigenvalues are also valid on the real dataset. We
use MNIST as an example. Firstly, for each image in the test set Dtest of MNIST, we vectorize it into
a 784 dimension vector. Then we calculate the EVD (Eigenvalue decomposition) of the covariance
matrix of each class. We plot the 784 eigenvalues of class 0 and 1 in Figure 2 in blue line as examples
(the other 8 classes are shown in Appendix B.3). Secondly, we adversarially train a classifier fr (x) in
original space. For each point X in Dtest, We construct Xr by PGD-attack with Nxfr (x). Then We
obtain a robust test set Dtrest . We plot the intra class eigenvalues of the covariance matrix in Figure 2
in orange line. Lastly, We adversarially train a classifier fls(X) in latent space. Then We construct
latent space robust dataset Dtlsest by generative attack against fls(X) on Dtest. The eigenvalues of the
covariance matrix are shoWn in Figure 2 in green line.
We shoW all the eigenvalues in the first column in Figure 2. After adding a small perturbation in the
original space or in the latent space, the distribution of Dtrest and Dtlsest are close to Dtest . We shoW
the details of the figure of large eigenvalues (the first 30 eigenvalues) in the second column and small
eigenvalues (last 754 eigenvalues) in the last column.
Original space adversarial training focuses on the small variance direction We adversarially
train a robust classifier fr(X) in the original space. The robust test set Dtrest against fr(X) amplified
the small eigenvalues a lot. In the third column, the orange line is significantly larger that the other 2
lines. While in the second column, the orange line is beloW the other tWo lines. The experiments give
us an understanding of hoW the adversarial examples leave the data manifolds. The original dataset
Dtest lies in a loW dimension affine plane in R784 . After adversarial training, the data move toWards
the small variance directions. Finally Dtrest stay in a full dimension subspace.
Latent space adversarial training focuses on the large variance direction We adversarially
train a robust classifier fls(X) in the latent space. The latent space robust test set Dtlsest against fls(X)
amplified the large eigenvalues. In the second column, We can see that the green line is above the
other tWo lines. And in the last column, the green line shoWs that adversarial training in the latent
space Will not affect the small eigenvalues. The adversarial examples in Dtlsest move along the large
variance direction. Therefore, Dtlsest stay in the loW dimension affine space.
7
Under review as a conference paper at ICLR 2021
Figure 2: Eigenvalues of MNIST: The first row and the second row are of classes 0 and 1 respectively.
The first column is about all the 784 eigenvalues of the dataset. The second column plots the large
eigenvalues and the last column plots the small eigenvalues.
5.2	Robust test accuracies
In this subsection, we compare the test accuracy of different attacks (FGSM, PGD, VAE) versus
different defense (adversarial training with FGSM, PGD, and VAE) to help explain our theoretical
results. We explain the results of MNIST in Table 1 as an example.
On-manifold and off-manifold adversarial examples The test accuracies of the standard training
model on PGD and VAE-attack data are 3.9% and 42.4% respectively. Firstly, They show that
on-manifold adversarial examples exist. Then, we can see that on-manifold adversarial examples are
harder to find.
Attack versus defense Our theory tells us that original or latent space adversarial training increase
the model robustness by amplifying the small or large eigenvalues, which are the variance of the
distribution, in the data manifold. In experiments, the test accuracy of VAE-adv vs PGD-attack is
1.23%, which shows that extending the manifold boundary in directions of large variance gives no
contribution to defending attacks in directions of small variance. Similarly, the test accuracy of
PGD-adv vs VAE-attack is 52.18%, which shows that amplifying the boundary in the directions of
small variance does not work well in defending attacks in directions of high variance. Further more,
we can see that latent space adversarial training does not work well against a simple FGSM attack,
and vice versa.
We see that PGD-adv can increase the test accuracies on VAE-attack from 42% to 52% on MNIST
and from 19.40% to 26.31% on CIFAR-10. A possible reason is that original space adversarial
training can amplify the small eigenvalues of the first q dimension but fails to amplify the larger ones.
As it is indicated in Figure 2, both original space and latent space adversarial training will increase
the eigenvalues which are small but not equal to zero (around the 100th eigenvalue) of the covariance
matrix.
5.3	Robustness trade-off
Robustness trade-off are common in practice (SU et al. (2018); Tramer and Boneh (2019)). For
example, '1 and '∞ adversarial examples cannot be defended simultaneously. The experiments on
robustness trade-off are shown in Table 2. On MNIST, thejointly-trained model using both '1 and '∞
adversarial examples will decrease the robust test accuracy by 20% comparing to the single trained
model. On CIFAR-10, the jointly-trained model will decrease the test accuracies by 6% on '1-attack
and by 10% on '∞-attack. A possible reason is that they are all original space attacks and focus on
8
Under review as a conference paper at ICLR 2021
MNIST	Std training	FGSM-Adv	PGD-Adv	VAE-Adv
clean data	98.82%-	-98.79%-	98.73%	98.28%
FGSM-Attack	-47.43%-	-91.08%-	96.50%	27.94%
PGD-Attack	-3.90%-	-15.46%-	95.51%	-1.23%-
VAE-Attack	42.40% 一	-46.94%-	52.18%	96.66%
CIFAR-10	Std training	FGSM-Adv	PGD-Adv	VAE-Adv
clean data	-92.14%-	-87.03%-	86.25%	85.59%
FGSM-Attack	-38.39%-	-76.92%-	77.34%	34.50%
PGD-Attack	-7.40%-	-75.14%-	75.78%	10.53%
VAE-Attack	-19.40%-	-31.80%-	26.31%	40.18%
Table 1: Test accuracies of different attacks and defense algorithms on MNIST and CIFAR-10.
the same directions of small variance. Hence they conflict with each other. We study the robustness
trade-off between regular and generative adversarial examples in this subsection.
MNIST	'ι-Adv	'∞-Adv	joint		PGD-Adv	VAE-Adv	joint
`1 -attack	78.50%	12.13%	54.24%	PGD-attack	95.51%	-123%-	89.50%
'∞-attack	0.5%	95.51%	78.81%	VAE-attack	52.18%	96.66%	90.28%
CIFAR-10	'ι-Adv	'∞-Adv	joint		PGD-Adv	VAE-Adv	joint
`1 -attack	66.22%	16.41%	60.23%	PGD-attack	75.78%	10.53%	74.22%
'∞-attack	52.8%	75.78%	65.11%	VAE-attack	26.31%	40.18%	42.47%
Table 2: Comparison of the robustness trade-offbetween '1 and '∞ attack and the trade-offbetween
original space and latent space attack on MNIST and CIFAR-10.
Our theory suggests that adversarial robustness can be disentangled in different directions. Hence
adversarial robustness against attacks on directions of low variance and large variance can be
guaranteed simultaneously. On MNIST, the jointly-trained model decreases the test accuracies by
6% comparing to the single trained model. The conflict between on-manifold and off-manifold
adversarial examples is much small than the conflict between off-manifold adversarial examples of
different norms. On CIFAR-10, the jointly-trained model gets test accuracies 74% on PGD-attack
and 42% on VAE-attack, which exhibit nearly no robustness trade-off. Therefore, if our goal is to
defend mixture of regular and generative attacks, The jointly-train model will perform well.
Discussion Under the q-dimensional manifold assumption, there is an overlap between the direc-
tions of the q largest variance and the directions of small variance. This is supported by Theorem 5,
Theorem 6, and the first experiment. A possible reason for the robustness trade-off between regular
and generative attacks is that they conflict with each other in the overlap directions. The conflict is
not serious because they mainly focus on different directions.
6	Conclusion
In this paper, we show that adversarial robustness can be disentangled in directions of small variance
and large variance of the data manifold. Theoretically, we study the excess risk and optimal saddle
point of the minimax problem of latent space adversarial training. Experimentally, we show that
these phenomena also exist in real datasets.
Future works One may design defense algorithms based on this property. We can generate
adversarial examples based on the directions of the dataset without access to the model architecture.
In white-box settings, we can use them for data augmentation to accelerate adversarial training or
even increase the model robustness. However, we need to carefully compare the computational cost
between calculating the EVD of the datasets and that of standard adversarial training. In black-box
attacks, we can use them to query the target model. But the data manifolds should be transferred from
some known datasets (since there is no information available for the training dataset of the target
model). Theoretically, it is unclear whether we can find the closed-form solution under nonlinear
models. How to analyze the nonlinear model is an open problem. Using linear models, we can further
analyze the conflict between robustness and generalization.
9
Under review as a conference paper at ICLR 2021
References
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural
networks. In Advances in neural information processing systems, pages 1097-1105, 2012.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780,
1997.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob
Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples.
arXiv preprint arXiv:1412.6572, 2014a.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing
systems, pages 2672-2680, 2014b.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
Chaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, Mingyan Liu, and Dawn Song. Generating adversarial examples
with adversarial networks. arXiv preprint arXiv:1801.02610, 2018.
Zhengli Zhao, Dheeru Dua, and Sameer Singh. Generating natural adversarial examples. arXiv preprint
arXiv:1710.11342, 2017.
Yang Song, Rui Shu, Nate Kushman, and Stefano Ermon. Generative adversarial examples. arXiv preprint
arXiv:1805.07894, 2018a.
Jernej Kos, Ian Fischer, and Dawn Song. Adversarial examples for generative models. In 2018 IEEE Security
and Privacy Workshops (SPW), pages 36-42. IEEE, 2018.
Yang Song, Rui Shu, Nate Kushman, and Stefano Ermon. Constructing unrestricted adversarial examples with
generative models. In Advances in Neural Information Processing Systems, pages 8312-8323, 2018b.
David Stutz, Matthias Hein, and Bernt Schiele. Disentangling adversarial robustness and generalization. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6976-6987, 2019.
Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman. Pixeldefend: Leveraging
generative models to understand and defend against adversarial examples. arXiv preprint arXiv:1710.10766,
2017.
Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John C Duchi, and Percy Liang. Adversarial training can
hurt generalization. arXiv preprint arXiv:1906.06032, 2019.
Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise of adversarial machine learning. Pattern
Recognition, 84:317-331, 2018.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. arXiv preprint
arXiv:1607.02533, 2016.
Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami.
The limitations of deep learning in adversarial settings. In 2016 IEEE European symposium on security and
privacy (EuroS&P), pages 372-387. IEEE, 2016.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate
method to fool deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 2574-2582, 2016.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 ieee
symposium on security and privacy (sp), pages 39-57. IEEE, 2017.
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order optimization based
black-box attacks to deep neural networks without training substitute models. In Proceedings of the 10th
ACM Workshop on Artificial Intelligence and Security, pages 15-26, 2017.
Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai. One pixel attack for fooling deep neural networks.
IEEE Transactions on Evolutionary Computation, 23(5):828-841, 2019.
10
Under review as a conference paper at ICLR 2021
Andrew Ilyas, Logan Engstrom, and Aleksander Madry. Prior convictions: Black-box adversarial attacks with
bandits and priors. arXiv preprint arXiv:1807.07978, 2018.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep
learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.
Yandong Li, Lijun Li, Liqiang Wang, Tong Zhang, and Boqing Gong. Nattack: Learning the distributions of adver-
sarial examples for an improved black-box attack on deep neural networks. arXiv preprint arXiv:1905.00441,
2019.
Jianbo Chen, Michael I Jordan, and Martin J Wainwright. Hopskipjumpattack: A query-efficient decision-based
attack. In 2020 ieee symposium on security and privacy (sp), pages 1277-1294. IEEE, 2020.
Lukas Schott, Jonas Rauber, Matthias Bethge, and Wieland Brendel. Towards the first adversarially robust neural
network model on mnist. arXiv preprint arXiv:1805.09190, 2018.
Florian Tram3r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. Ensemble
adversarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204, 2017.
Jacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow. Thermometer encoding: One hot way to resist
adversarial examples. In International Conference on Learning Representations, 2018.
Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens Van Der Maaten. Countering adversarial images
using input transformations. arXiv preprint arXiv:1711.00117, 2017.
Vishaal Munusamy Kabilan, Brandon Morris, Hoang-Phuong Nguyen, and Anh Nguyen. Vectordefense:
Vectorization as a defense to adversarial examples. In Soft Computing for Biomedical Applications and
Related Topics, pages 19-35. Springer, 2018.
Aaditya Prakash, Nick Moran, Solomon Garber, Antonella DiLillo, and James Storer. Deflecting adversarial
attacks with pixel deflection. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 8571-8580, 2018.
Guneet S Dhillon, Kamyar Azizzadenesheli, Zachary C Lipton, Jeremy Bernstein, Jean Kossaifi, Aran Khanna,
and Anima Anandkumar. Stochastic activation pruning for robust adversarial defense. arXiv preprint
arXiv:1803.01442, 2018.
Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. Mitigating adversarial effects through
randomization. arXiv preprint arXiv:1711.01991, 2017.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security:
Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018.
Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to adversarial
example defenses. arXiv preprint arXiv:2002.08347, 2020.
Ajil Jalal, Andrew Ilyas, Constantinos Daskalakis, and Alexandros G Dimakis. The robust manifold defense:
Adversarial training using generative models. arXiv preprint arXiv:1712.09196, 2017.
Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-gan: Protecting classifiers against adversarial
attacks using generative models. arXiv preprint arXiv:1805.06605, 2018.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784,
2014.
Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional
generative models. In Advances in neural information processing systems, pages 3483-3491, 2015.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved
techniques for training gans. In Advances in neural information processing systems, pages 2234-2242, 2016.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training
of wasserstein gans. In Advances in neural information processing systems, pages 5767-5777, 2017.
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry.
Adversarial examples are not bugs, they are features. In Advances in Neural Information Processing Systems,
pages 125-136, 2019.
Michael E Tipping and Christopher M Bishop. Probabilistic principal component analysis. Journal of the Royal
Statistical Society: Series B (Statistical Methodology), 61(3):611-622, 1999.
11
Under review as a conference paper at ICLR 2021
Soheil Feizi, Farzan Farnia, Tony Ginart, and David Tse. Understanding gans: the lqg setting. arXiv preprint
arXiv:1710.10793, 2017.
Soheil Feizi, Farzan Farnia, Tony Ginart, and David Tse. Understanding gans in the lqg setting: Formulation,
generalization and stability. IEEE Journal on Selected Areas in Information Theory, 2020.
Bin Dai, Yu Wang, John Aston, Gang Hua, and David Wipf. Hidden talents of the variational autoencoder. arXiv
preprint arXiv:1706.05148, 2017.
Yann LeCun, L6on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document
recognition. Proceedings ofthe IEEE, 86(11):2278-2324,1998.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Dong Su, Huan Zhang, Hongge Chen, Jinfeng Yi, Pin-Yu Chen, and Yupeng Gao. Is robustness the cost of
accuracy?-a comprehensive study on the robustness of 18 deep image classification models. In Proceedings
of the European Conference on Computer Vision (ECCV), pages 631-648, 2018.
Florian Tramer and Dan Boneh. Adversarial training and robustness for multiple perturbations. In Advances in
Neural Information Processing Systems, pages 5858-5868, 2019.
Constantinos Daskalakis, Themis Gouleakis, Chistos Tzamos, and Manolis Zampetakis. Efficient statistics, in
high dimensions, from truncated samples. In 2018 IEEE 59th Annual Symposium on Foundations of Computer
Science (FOCS), pages 639-649. IEEE, 2018.
12
Under review as a conference paper at ICLR 2021
Overview
In appendix A, we provide the proof of the Theorems. In appendix B, we show the settings about the
experiments. Appendix C is a further discussion of data augmentation using generative models. It is
not closely related to our main paper.
A Proof of the Theorems
A.1 Problem description
Lemma 1 (Adversarial examples perturbed in the latent space). Using these 3 strategies, the adver-
sarial examples can be unified as
Xadv = x0 + W∆z and X 〜Dj= N(μ*, U*Λ(j)UT), j = 1, 2, 3,
where
Λ(3)
Λq	0
0 σ2I
Λ(2)	(Λq - σ2I)2Λq-1 + (Λq - σ2I)Λq-1σ2 + σ2I	0
Λ =	0	σ2I
If the data lie in a q dimensional subspace, i.e. the covariance matrix Σ* is rank q, we have
A(I) = λ⑵=Λ⑶=Λ*. Then D0 = D.
Proof:
Recall that X 〜N (μ, WW T + σ2I), x|z 〜N (W Z + μ,σ2I) and z|x 〜N (P T W t (x -
μ),σ2P-1) where P = WT W + σ2I. The maximum likelihood estimator of W and σ2 are
Wml = Uq(Λq- σ2I)1/2
2	1d
and σM L = d— E λi.
q i=q+1
Strategy 1 Sample X 〜 D, encode Z = arg max q(z∣x) = P-1WT(x 一 μ*), add a perturbation
∆z, and finally, decode Xadv = arg maxp(x∣z + ∆z) = W(Z + ∆z) + μ*. Then
Xadv =W(P-1WT(x ― μ*) + ∆z) + μ*
=WP-1WT(x 一 μ*) + μ* + W∆z
=x0 + W∆Z.
Since X 〜(μ*, Σ*), We have X — μ* 〜(0, Σ*), Then
X0 〜N(μ*, WP-1WTΣ*(WP-1WT)t),
With
WP-1WTΣ*(WP-1WT)T
=U*
Λq 一 σ
0
2	01/2
0
Λ0q
0
σ2I
-1
Λq 一 σ
0
20
0
U" Λ*
Λq 一 σ2
0
01/2
0
Λ0q
0
σ2I
Λq 一 σ2
0
1/2
U*T
=U*
(Λq
一 σ2I)2Λq-1
0
U*T
*
0
0

1
0
0
=U*Λ(j)U*T,	j = 1.
13
Under review as a conference paper at ICLR 2021
Strategy 2 Sample X 〜D, then sample Z 〜q(z|x), add a perturbation ∆z, and finally, sample
Xadv 〜p(x∣z + ∆z). Then
Z 〜N(0, PTWTΣ*(PTWT)T + σ2PT)
and
Xadv 〜N(μ* + W∆z, WPTWTΣ*(PTWT)TWT + Wσ2PTWT + σ2I),
Xadv = X + W∆Z,
With
WP TW T Σ*(P TW T )T W T + W σ2P TW T + σ2I
=U* (Aq-σjI)2Aq1 0 UT + U* (Aq-b；I应为2 ； UT + σ2I
(Λq - σ2I)2Λq-1 + (Λq - σ2I)Λq-1σ2 + σ2I	0 T
=U*	0	σ2I U*
=U*Λ(j)U*T, j = 2.
Strategy 3 Sample Z 〜N(0, I), add a perturbation ∆z, and then sample Xadv 〜p(x∣z + ∆z). In
this strategy, Xadv can be viewed as the adversarial example of X = arg maxx q(Z|X).
Xadv 〜N(μ* + W∆z, WWT + σ2I),
With
WWT + σ2I
U Λq 0 UT
=U*	0 σ2I U*
=U*Λ(j)U*T, j =3.
In these 3 strageties, the adversarial examples can be summerized as
Xadv = X0 + W∆z and X 〜Dj,	j = 1, 2,3,
where j = 1, 2, 3 corresponding to strategy 1,2 and 3.
If the data lie in a low dimensional space, i.e. the covariance matrix Σ* is rank q. Then the maximum
likelihood of σML = Pd=q+ι λi∕(d - q) = 0. Then
Λ(1) = Λ(2) = Λ(3) = Λ0q 00 =Λ*.
There is no difference among these 3 strategies and there is no change of distribution, i.e. D0 = D.
A.1.1 Excess risk analysis
Before we prove Theorem 2. We need to prove the following lemma first.
Lemma 2 (optimal perturbation). Given Θ = (μ, Σ) the optimal solution ofthe inner max problem
in equation 6 is
∆Z = WT(LΣ - WWT)-1(x0 - μ),
where L is the lagrange multiplier satisfying ∣∣∆z*∣∣ = ε.
Proof: Consider problem
max '(x0 + W∆z, μ, Σ).
k∆zk≤ε
The Lagrangian function is
'(x0 + W∆z, μ, Σ) - 2(k∆zk2-ε2)
=d log(2π) + 1 log ∣Σ∣ + ∣(x0 - μ + W∆z)TΣ-1(x0 - μ + W∆z) - 2(k∆zk2 - ε2).
14
Under review as a conference paper at ICLR 2021
Notice that this quadratic objective function is concave when L is larger than the largest eigenvalue
of WTΣ-1W. Calculate the partial derivative with respect to ∆z and set it to be zero, we have
WTΣ-1(x0 - μ + W∆z*) - L∆zr- = 0
⇔(L - WTΣ-1W)∆z* = WTΣ-1(x0 - μ)
⇔∆z* = (L - WTΣ-1W)-1WTΣ-1(x0 - μ)
⇔∆z* = WT(LΣ - WWT)-1(x0 - μ).
The last equation comes from the Woodbury matrix inversion Lemma. We can obtain L by solving
the equation ∣∣∆z*k = ε. We don,t have a closed form solution of L but we can solve it numerically.
L → ∞ as ε → 0. We only need to know L is a constant in our whole theory.
Theorem 2 (Excess risk). Let LlS and L be the loss with or without perturbation in latent space
(equation 6 and 3 respectively), given the non-robustly learned Θ* = (μ*, Σ*), The excess risk
caused by perturbation is
Lls包, Dj)-L®") = 1 XX [(1 + (LL-"2 )2 - ɪ] M, j = 1, 2, 3
and the excess risk caused by changed of distribution is
L①, Dj)-L®, D)=2 log [⅛4)]+1 (X W- d)∙
li=1 i	i=1
Proof: Since
x'〜Dj= N(μ*, Σj) = N(μ*, U*Λ⑴UT)∙
Denote
v = x' - μ* 〜N(0, U*Λ⑴UT)∙
And we have
WWT = Uq(Λq - σ2I)UT = U* Aq- σ" 0 UT∙
The excess risk caused by perturbation is
2(Lls(Θ*,Dj) -L(Θ*, Dj))
=E(v + WWT(LΣ* - WWT)-1v)tΣ-1(v + WWT(LΣ* - WWT)-1v) - EVTΣ-1v
=Tr [(I + WWt(LΣ* - WWt)-1)tΣ-1(I + WWT(LΣ* - WWT)-1)EVVT]
-Tr [∑-1 Evvt ]
=Tr [U* [I +(Aq-σ21)((L)- I)Aq + σ21 )-1]2 0 Λ-1Λ⑴UT ] - Tr[Λ-1Λ ⑴]
=Tr[ [I +(Ag"2/X"- I)Aq + σ2I 厂干 0 Λ-1Λ⑴]-Tr[Λ-1Λ ⑴]
q
Σ[(1 +
i=1
λi - σ2	)2
(L - 1)% + σ2 )
j = 1, 2, 3∙
and the excess risk caused by changed of distribution is
2(L(Θ*, Dj)-L(Θ*, D))
= log I∑j| - log ∣∑*∣ + Eχ0(x0 - μ*)τ∑-1(x0 - μ*) - Ex(X - μ*)τ∑-1(x - μ*)
= log ∣∑j∣ - log ∣Σ*∣ + Tr(Σ-1Exo(x0 - μ*)(x0 - μ*)τ) - Tr(Σ-1Ex(x - μ*)(x - μ*)τ)
= log [QQ=1 λij ] + Tr(Λ-1Λj)) - Tr(Λ-1 Λ*)
LIli=I λi
= log
* W) i
Πd=1 U
d λ(j)
+ (E* - N
i=1
15
Under review as a conference paper at ICLR 2021
□
It is hard to see which part dominates the excess risk. If we further assume that the data lie in a q
dimension manifold. The excess risk caused by the change of distribution becomes 0. We have the
following corollary.
Corollary 3 (Excess risk). Let Lls and L be the loss with or without perturbation in latent space
(equation (6) and (3) respectively), g^ven the non-robUstly learned Θ* = (μ*, Σ*), and rank(∑≠)=
q. The excess risk
Lis(Θ*, Dj)- L(Θ*, D) = O(qL-2).
Proof: By Lemma 1, we have σ2 = 0. λi(j) = λi and Dj0 = D. Hence the excess risk caused by
changed of distribution
L(Θ*, Dj)-L(Θ*, D)=0.
The excess risk caused by perturbation is
2(Lis(Θ*,Dj)-L(Θ*,Dj))
X h(i+(L½)2- ɪi 三
q
=X h(1+⅛—1i
=O(qL-2).
□
Theorem 4 (Excess risk of orginal space adversarial training). Let Lr and L be the loss with or
without perturbation in original space (equations (4) and (3) respectively), given the non-robustly
learned Θ* = (μ*, Σ*). Denote λmin be the smallest eigenvalue of Σ*. The excess risk
Ω((λmin L)-2) ≤ Lr(Θ*, D)- L(Θ*, D) ≤ O(d(λminL)-2 ).
Theorem 4 can be viewed as a corollary of Theorem 1 in Ilyas et al. (2019). We give the prove here.
Proof: Consider the Lagrange multiplier form of the inner maximization problem in equation 4.
max '(x + ∆x, μ, Σ).
k∆xk≤ε
The Lagrangian function is
'(x + ∆x, μ, Σ) - 2(k∆xk2 - ε2)
=dlog(2∏) + 1log l∑l + 2(x - μ + ∆x)T∑-1(x - μ + ∆x) - L∙(k∆xk2 - ε2).
Notice that this quadratic objective function is concave when L is larger than the largest eigenvalue
of Σ-1. Calculate the partial derivative with respect to ∆x and set it to be zero, we have
Σ-1(x — μ + ∆x*) — L∆X = 0
⇔∆x* = (LΣ - I)-1(x - μ).
The excess risk is
2(Lr(Θ*, D) - L(Θ*, D))
=E(v + (LΣ* - I)-1v)T Σ*-1(v + (LΣ* - I)-1v) - EvTΣ*-1v
=Tr (I + (LΣ* -I)-1)TΣ*-1(I+ (LΣ* - I)-1)EvvT - T rΣ*-1EvvT
X [(1 + ∕≠-1].
i=1	Lλi -1
16
Under review as a conference paper at ICLR 2021
On the one hand,
XX [(1 + Lλk≠- 1]
i=1	Lλi - 1
≥[(1+ Lλ 1 -1)2 - 1]
Lλmin - 1
≥Ω((Lλmin)-2).
On the other hand,
d
X[(1 +
i=1
Lλ⅛i )2-1]
≤矶(1+ Lλ 1 -1)2 - 1]
Lλmin - 1
≤O(d(Lλmin)-2).
□
A.1.2 Saddle point analysis
Theorem 5 (Main result: Optimal Saddle point). The optimal solution of the modified problem in
equation (8) is
μis = "* and ∑is = U.ΛlsUT,
where
λis = 4 h2λ(j) + 4(λiL σ ) + 2λ(j)^1 + 4(λi(j)ɪɪi for i = 1 ≤ q and λis = λij)for i > q.
j = 1, 2, 3 corresponding to strategies 1,2 and 3.
Problem 6 is not a standard minimax problem, consider the modified problem
min max Ex，〜Dj'(x0 + W∆z, μ, Σ), j = 1, 2, 3.	(9)
m,ς Ex，Igzk=E	j
By lemma 3, the optimal perturbation ∆z* is a matrix M times X - μ. Consider the problem
min max	Eχ,〜d,'(x0 + WM(x0 — μ), μ, Σ),	j = 1, 2, 3.	(10)
M,ς Ex，kM(x0-μ)k=ε	j
Lemma 6 (optimal perturbation). Given Θ = (μ, Σ) the optimal solution ofthe inner max problem
of 10 is
M* = WT(L∑ - WWT)-1.
Proof: Consider the problem
max	E'(x0 + WM(x0 — μ), μ, Σ)).
EkM(X0-μ)k=ε
The lagrangian function is
E['(x0 + W M (x0 - μ), μ, ∑) - f(kM (X- μ)k2 - ε2)].
Let χ0 - μ = v, Take the gradient with respect to M and set it to be zero, We have
∂L
dME['(X0 + WM(X0 - μ),μ, ∑) - 2(kM(X0 - μ)k2 - ε2)]
=VME[vτMWTΣ-1v + 1VTMWT∑-1WMv - LvTMMv/2]
=[W T Σ-1 + WTΣ-1WM - LM ]E[vvT]
=0.
17
Under review as a conference paper at ICLR 2021
Then we have
M* =(L - WTΣ-1W)-1WTΣ-1
=WT(LΣ - WWT)-1.
□
The last equality is the Woodbury matrix inversion Lemma. Notice that lemma 3 and Lemma 6 have
the same form of solution. This is why we can use Problem 10 to approximate Problem 6. To solve
the problem 10, we need to introduce Danskin’s Theorem.
Theorem 7 (Danskin’s Theorem). Suppose φ(x, z) : X × Z → R is a continuous function of two
arguments, where Z ⊂ Rm is compact. Define f(x) = maxz∈Z φ(x, z). Then, if for every z ∈ Z,
φ(x, z) is convex and differentiable in X, and ∂φ∕∂x is continuous:
The subdifferential off(x) is given by
∂f(x) = conv { dφ(X,Z ,z ∈ Zo(x)},
where conv(∙) is the convex hull, and Zo(x) is
Zo(x) = {z : φ(x, Z) = max φ(x, z)}.
If the outer minimization problem is convex and differentiable, we can use any maximizer for the
inner maximization problem to find the saddle point. But the outer problem of problem 4 is not
convex, we need to modify the problem again. Assume that we have already obtained the eigenvector
U * from the ML estimator. The optimization variables of the outer minimization problem are μ and
Λ. Then We have Σ = U*ΛUT in problem 10. Another reason to make this assumption is that We
only need to consider the eigenvalue problem to compare with standard adversarial training (Theorem
2 of AndreW Ilyas et al., 2019).
Proof of Theorem 5:
By Lemma 6, We have
M* =WT(LΣ - WWT)-1
Λq - σ2
0
01/2
0
LΛ -
Λq - σ2 0
00
-1
UT.
Which is a diagonal matrix Λm times UT. Let T = Λ-1, m = Λ-1 UTμ and x00 = UTx0. The
optimization problem becomes
min max Eχ0〜d0'(x0 + WΛmUT(x0 — μ),m,T)
m,T ΛM	j
s.t. Eχo ∣∣Λm U*(x0 — μ)k2 = ε2.
(11)
Obviously, the inner constraint is compact (by Heine-Borel theorem), We only need to prove the
convexity of the outer problem to use Danskin’s Theorem. For any x0 and ΛM,
'(x0 + WΛMU*(x0 — μ), m,T)
=2 log(2π) + 1 log ∣Σ∣ + 1(x0 — μ + WM(x0 — μ)T∑-1(x0 — μ + WM(x0 — μ).
Let U = UT(x0 — μ), and A = (I +
Λq — σ2	0
00
ΛM)2, consider the third term, We have
1 log ∣Σ∣ + 1(x0 — μ + WM(x0 — μ)TΣ-1(x0 — μ + WM(x0 — μ)
=LuT A2Tu.
2
By Daskalakis et al., 2018Daskalakis et al. (2018), The hessian matrix is
H = SN(T-im,(AT)-1) h (VeC( - 2AZZT)) , (vec( - 2AZZT)) i 占 0.
18
Under review as a conference paper at ICLR 2021
Therefore, this is a convex problem. By the same calculation in Lemma 6, a maximizer of the inner
problem is
AM
Λq - σ2	0
00
IL (LA -
Λq - σ2
0
-1
Then
A= I+
Λq - σ2 0
q 0	0	LΛ -
Λq - σ2
0
12
0
0
0
0
Then the first order derivative (by Daskalakis et al. (2018)) is
V[T,m]T '
-1 AΛ(j) - 2T-1
ATTm - AUT〃*
0.
From the second equation, We directly have μis = μ*. From the first equation, for i > q,we have
(1 + 0)2λi(j) = λlis.
For i ≤ q, we have
(1 + (λi - σ2)∕(Lλis - λi + σ2))2λ(j) = λis.
It equivalents to a second order equation of ʌ/ʌis
K-qλFqλρ - λi-σ2=0.
Solving this equation, we obtained
λis = 1 ∣2λ(j) + 4(λiL σ ) + 2λ(j) jl + 4(λi-1 ) i for i = 1 ≤ q and λlis = λ(j), for i > q.
□
B	Experiments settings
B.1	MNIST
For Mnist, we use LeNet5 for the classifier and 2 layers MLP (with hidden size 256 and 784) for the
encoder and decoder of conditional VAE. For standard training of the classifier, we use 30 epochs,
batch size 128, learning rate 10-3, and weight decay 5 × 10-4. For the CVAE, we use 20 epochs,
learning rate 10-3, batch size 64, and latent size 10.
For standard adversarial training, we use ε = 0.25 for FGSM and PGD. in PGD, we use 40 steps for
the inner part. Adversarial training start after 10 epochs standard training.
For generative adversarial training, we use ε = 1 in the latent space with FGSM. Adversarial training
start after 10 epoches standard training.
In the attack part, we use ε = 0.2 for norm-based attack and ε = 1 for generative attack on the test
set.
B.2	CIFAR 1 0
For CIFAR10, we use ResNet32 for the classifier and 4 layers CNN for the encoder and decoder of
conditional VAE. For standard training of the classifier, we use 200 epochs, batch size 128, learning
rate 10-3, and weight decay 5 × 10-4. For the CVAE, we use 100 epochs, learning rate 10-3, batch
size 64, and latent size 128.
For standard adversarial training, we use ε = 4∕255 for FGSM and PGD. in PGD, we use 10 steps
for the inner part. Adversarial training start after 100 epochs standard training.
For generative adversarial training, we use ε = 0.1 in the latent space with FGSM. Adversarial
training start after 100 epoches standard training. Since we see that the modeling power of VAE in
CIFAR10 is not good enough. For each of the image, the encode variance is very small. When we
19
Under review as a conference paper at ICLR 2021
add a small perturbation to the encode mean value, the output image are blured. Hence we only use a
small ε = 0.1.
In the attack part, we use ε = 4/255 for norm-based attacks and ε = 0.1 for generative attack on the
test set. The test accuracy of VAE-adv against VAE attack is 40.18% in our experiments. It is not
good enough because of the modeling power of VAE. But the results on standard adversarial training
versus VAE-attacks are worse, and vice versa. The experiments support our findings.
B.3	Eigenvalues of covariance matrix of MNIST
We plot the eigenvalues of all the classes in this section, see Figure 3.
C Finite samples case: Data augmentation or adversarial training
In this section we discuss the question that can we use the generative model to generate more examples
for training in our theoretical framework. Because it is not closely related to our main results, we
only discuss it in appendix. Let us focus on the case that the number of samples are not enough.
Generative model cannot help data augmentation Given dataset {xi}in=1 ⊂ Rd, let μ and
S = U ΛU be the sample mean and sample covariance matrix. The generative model learned by this
dataset is X = Uq(Λq - σ2I)1/2z + μ + R The distribution of the data sample from the model is
Xgen 〜N(μ, WWT + σ2I) = N(μ, UAgenUU)，N(μ, S0).	(12)
If we use n samples from the original dataset and m samples from the generative model, the loss
function is
1n	m
L3, R = mm∑n n + m £'(Xi; μ, R + n+mEx〜N3S)W(X; μ, £)].	(13)
,	i=1
Theorem 8 (Data augmentation by generative model). Given MLE μ and S, The optimal solution
training with n true samples and m generated samples is μda = μ and Σda = UAda UT, where
d
λda =兀 for i ≤ q and λda = n+m 兀 +(n + m)(d - q) X λk for i>q.
Proof: Consider the derivative of Problem 13.
1n	m
V“L = n+m X ς (Xi - μ) + n+mExς	(X-μ) = 0
i=1
Then We can obtain μda
V∑-ι L = —Σ +
Then we have
For i ≤ q,
=μ.
1n	T m	T
―,-T(Xi	-	μ)(χi	-	μ)	+-,—Ex(X	-	μ)(χ - μ) =	0.
n+m	n+m
i=1
Σls = -n- s+ -ɪ S0.
n+m n+m
nm
n+mλi + n+m
For i > q,
nm	n
n+m X + n+m σ2 = n+m X +
(n + m)(d - q)
d
X λk.
k=q+1
m
□
The optimal solution is a little bit destoryed. In this perspective, generative models give no help to
data augumentation.
20
Under review as a conference paper at ICLR 2021
Figure 3: Eigenvalues of the covariance matrices of MNIST for all the 10 classes
21
Under review as a conference paper at ICLR 2021
Generative model can help robust training
Theorem 9 (Adversarial learned parameters). The Adversarial learned features of problem 10 under
finite samples is μis = μ and ∑is = UJΛlslT T where Λls is of the sameform in Theorem 5 except
replacing λi by λi under strategy 3.
Proof: Considering strategy 3. In this case
W = U(Λq - σ2)1/2.
and
x0 〜(μ, WWT + σ2I).
Since
WWT + σ2I = U Λq	0r UT.
0	σ2I
In words, the eigenvectors of the dsitribution U is the same as the one in perturbation W∆z. The op-
timιzation problem is the same as the one We use in the proof of Theorem 5 if We replace λi by λi. □
If the data lie in a q dimensional subspace, we do not neet to make the assumption on strategy 3.
22