Under review as a conference paper at ICLR 2021
A Unified Framework for Proximal Methods
Anonymous authors
Paper under double-blind review
Ab stract
We study the training of regularized neural networks where the regularizer can
be non-smooth and non-convex. We propose a unified framework for stochastic
proximal gradient descent, which we term ProxGen, that allows for arbitrary
positive preconditioners and lower semi-continuous regularizers. Our framework
naturally encompasses standard stochastic proximal gradient methods without
preconditioners as special cases. We present two important instances stemming
from our approach: (i) the first proximal version of Adam, one of the most popular
adaptive SGD algorithm, and (ii) a revised version of ProxQuant [Bai et al, 2019]
that improves upon the original approach for quantization-specific regularizers by
incorporating the effect of preconditioners when computing proximal mapping.
We analyze the convergence of ProxGen and show that the whole framework
enjoys the same convergence rate as stochastic proximal gradient descent without
preconditioners. We also empirically show the superiority of proximal methods
compared to subgradient-based approaches via extensive experiments. Interestingly,
our results indicate that proximal methods with non-convex regularizers are more
effective than those with convex regularizers.
1 Introduction
We study the regularized training of neural networks, which can be formulated as the following
(stochastic) optimization problem
f(θ)
^^^^^^^^{
minimize F(θ) ：= Eξ〜P [f (θ; ξ)] +R(θ)
θ∈Ω
(1)
where θ ∈ Rp represents the network parameter, ξ is the random variable representing mini-batch
data samples, and R(∙) is a regularize] encouraging low-dimensional structural constraints on θ.
For the unregularized case, i.e., when R(θ) = 0, stochastic gradient descent (SGD) has been a
prevalent approach to solve the optimization problem stated in Eq. (1). At each iteration, SGD
evaluates the gradient only on a randomly chosen subset of training samples (mini-batch). Vanilla
SGD employs a uniform learning rate for all coordinates, and several adaptive variants have been
proposed, which scale the learning rate for each coordinate by its gradient history. A prime example
of such approaches is AdaGrad (Duchi et al., 2011), which adjusts the learning rate by the sum
of all the past squared gradients. However, the performance of AdaGrad degrades in non-convex
dense settings as the learning rates vanish too rapidly. To resolve this issue, exponential moving
average (EMA) approaches such as RMSprop (Tieleman & Hinton, 2012) and Adam (Kingma &
Ba, 2015) have been proposed and become popular. These scale down the gradients by square roots of
exponential moving averages of squared past gradients to essentially limit the scope of the adaptation
to only a few recent gradients. In terms of theory, convergence analyses of these unregularized SGD,
whether adaptive or not, have been well studied both for convex (Kingma & Ba, 2015; Reddi et al.,
2018) and non-convex (Chen et al., 2019b; Lei et al., 2019) loss f cases.
The technique of regularization is ubiquitous in machine learning as it can effectively prevent
overfitting and yield better generalization. The `1 -regularized training for Lasso estimators/sparse
Gaussian graphical model (GMRF) estimation (Tibshirani, 1996; Ravikumar et al., 2011) and `2
weight decay (Tychonoff, 1943) on parameters are prototypical examples. In the context of deep
1
Under review as a conference paper at ICLR 2021
Table 1: Comparison among stochastic (or online) PGD for solving the problem in Eq. (1).					
Algorithm	Non-Convex	Non-Convex	Preconditioner	Momentum	ConvergenCe
	Loss	Regularizer			Guarantee
ADAGRAD (DuChi et al., 2011)			AdaGrad		✓
Ghadimi et al. (2016)	✓		✓		✓
Wang etal.(2018)	✓			✓	✓
PhametaL(2019)	✓			✓	✓
DaVis & Drusvyatskiy (2019)	✓			✓	✓
Xu et al. (2019a)	✓	✓			✓
Xu et al. (2019b)	✓	✓	AdaGrad		✓
Prox-SGD (Yang et al., 2020)	✓		✓	✓	
Davis et al. (2020)	✓	✓		✓	✓
ProxGen (Ours)	∣	✓	✓	/	✓	✓
learning, important instances include network pruning (Wen et al., 2016; Louizos et al., 2018), which
induces a sparse network structure, and network quantization (Yang et al., 2019; Courbariaux et al.,
2015; Bai et al., 2019), which gives hard constraints so that parameters have only discrete values.
In many cases, the regularizer is non-smooth around some region (Consider `1 norm at zero).
Therefore, instead of using the gradient, one employs the subgradient of the objective function F (θ)
in Eq. (1). Such a strategy, which is essentially adopted in modern machine learning libraries such
as TensorFlow (Abadi et al., 2016) and PyTorch (Paszke et al., 2019), is problematic as it may slow
down convergence and result in oscillations. A simple idea to tackle this issue is to bypass the
non-smoothness of a regularizer via its proximal operator. This idea is the basis of proximal gradient
descent (PGD) methods, which first update the parameter using the gradient of the loss function f (θ)
and then perform a proximal mapping of R(θ).
In the non-stochastic case, the PGD with both convex and non-convex regularizers has been ex-
tensively studied in the literature (Reddi et al., 2016; Allen-Zhu, 2017; Wang et al., 2018; Pham
et al., 2019; Chen et al., 2020). Another work, VMFB (Chouzenoux et al., 2014), analyzes the
preconditioned gradient descent on convex regularized problems with non-convex loss but does not
consider the first-order momentum. In contrast, PGD in the stochastic setting has been little explored.
Duchi et al. (2011); Ghadimi et al. (2016) consider PGD to solve the stochastic objectives with
convex regularizers. Recently, Xu et al. (2019b) studies non-convex and non-smooth regularized
problems for DC (difference of convex) functions and Xu et al. (2019a); Davis et al. (2020) present
non-asymptotic analysis for non-convex smooth loss and non-convex regularizers, which is the most
general setting, but do not consider the preconditioner in the update rule.
All the aforementioned studies of the stochastic case, however, focus either on limited settings (e.g.
Duchi et al. (2011) only covers the update rule of AdaGrad) with convex regularizers only, or
on pure vanilla gradient descent for non-convex regularizers. Hence, they cannot accommodate all
advanced modern optimization algorithms with preconditioners, such as adaptive gradient methods.
The only exception is Prox-SGD (Yang et al., 2020), with the caveat that Prox-SGD update rule is
not an exact PGD. Moreover, the theory in Yang et al. (2020) only guarantees the convergence, not
how fast Prox-SGD converges, and furthermore this analysis is performed without considering the
preconditioners. Table 1 summarizes the previous studies and our work in terms of stochastic PGD.
In this paper, we propose an exact framework for stochastic proximal gradient methods with arbitrary
positive preconditioners and lower semi-continuous (possibly non-convex) regularizers. With our
framework, our goal is to provide theoretical and empirical understanding of stochastic proximal
gradient methods. Our main contributions are summarized as follows:
• We propose the first general family of stochastic proximal gradient methods, which we term
ProxGen. We introduce two important instances stemming from our approach: (i) the first
proximal version of Adam (Kingma & Ba, 2015) and (ii) a revised version of ProxQuant (Bai
et al., 2019) that improves upon the original approach for quantization-specific regularizers by
incorporating the effect of preconditioners when computing proximal mappings.
2
Under review as a conference paper at ICLR 2021
Algorithm 1 PROXGEN: A General Stochastic Proximal Gradient Method
1:	Input: Stepsize αt, {ρt}tt==1T ∈ [0, 1), regularization parameter λ, and small constant 0 < δ << 1.
2:	Initialize: θ1 ∈ Rd, m0 = 0 ∈ Rd, and C0 = O ∈ Rd×d.
3:	for t = 1, 2, . . . , T do
4:	Draw a minibatch sample ξt from P
5:	gt — ▽/(θt; ξt)	. Stochastic gradient at time t
6:	mt — Ptmt-I + (1 — pt)gt	. First-order momentum estimate
7:	Ct — PreConditioner construction
8:	θt+ι	∈ argmin	{hmt, θ}	+	λR(θ)	+	~^α (θ — θt)T (Ct	+ δI)(θ	—	θt)}
9:	end for
10:	Output: θT+1
• We analyze the convergence of the general PROXGEN family and identify essential conditions for
convergence. We show that ProxGen enjoys the same convergence rate as vanilla Sgd under
mild conditions, and highlight the challenges in our theory and improvements upon previous work.
Our convergence guarantee encompasses several existing approaches as special cases.
• In terms of practice, we demonstrate the superiority of proximal methods over subgradient-based
methods with various non-convex regularizers which have not yet been studied in deep learning.
Interestingly, our experiments indicate that proximal methods with non-convex regularizers are
more effective than with convex regularizers for learning sparse deep models.
2 A General Family of Stochastic Proximal Gradient Methods
In this section, we present ProxGen, a general family of stochastic proximal gradient methods, and
present both existing and novel instances as showcase examples in our family. Algorithm 1 describes
the details of ProxGen. The update rule on line 8 of Algorithm 1 can be written more compactly:
Θt+1 ∈ argmin km" + λR(θ) + .(θ -%厂(Ct + δI) (θ -%)}
=PrOXCt+RI)("t — at(Ct + §1)-1m)	⑵
where the proximal operator in Eq. (2) is defined as ProXA(Z) = argminχ{h(x) + 2∣∣x - z∣∣A}. In
ProxGen, we allow both the loss and the regularizer to be non-convex. Now, we introduce possible
examples according to the proper combinations of preconditioners Ct and regularizers R(∙).
Existing Instances of ProxGen. We briefly recover some known examples in PROXGEN family.
• ADAGRAD (Duchi et al., 2011) is the first key instance of adaptive gradient methods where
Ct = (PT=ι gτgT)1/2 and R(θ) = ∣∣θ∣∣ι. Any convex regularizer R(∙) is allowed.
• The proximal Newton methods (Lee et al., 2012) employ the exact Hessian Ct = V2f (θt) and
R(θ) = ∣θ∣1 . In addition, we can approximate the exact Hessian, which yield proximal Newton-
type methods such as quasi-Newton approximation (Becker et al., 2019), L-BFGS approximation
(Liu & Nocedal, 1989), and adding a multiple of the identity to the Hessian.
Although the above examples enjoy good theoretical properties in convex settings, many of the
modern practical optimization problems involve non-convex loss functions such as learning deep
models. Moreover, it is known that non-convex regularizers yield better performance (also in terms
of theory) than convex penalties in some applications (see Fu (1998); Park & Yoon (2011); Yang &
Lozano (2017); Yun et al. (2019b) and references therein). Considering this motivation and recent
advanced optimizers, we arrive at the following new examples.
Novel Instances of ProxGen. Beyond the well-known methods above, PROXGEN naturally
introduces proximal versions of standard SGD techniques developed for solving unregularized
problems for deep learning. The following examples are just a few instances that have not been
explored so far, and ProxGen can cover a broader range of new examples depending on the
combinations of preconditioners and regularizers.
3
Under review as a conference paper at ICLR 2021
•	The proximal version of ADAM (Kingma & Ba, 2015) with 'q regularization is a possible example
where Ct = 73Ct-I + (1 - β)g2 With β ∈ [0,1) and R(θ) = ∣∣θkq for 0 ≤ q ≤ 1. We mainly
validate the superiority of our novel proximal version of ADAM to the usual subgradient-based
counterpart empirically in Section 4.
•	We can also consider the proximal version of KFAC (Martens & Grosse, 2015). For an L-layer
neural network, KFAC approximates the Fisher information matrix with layer-wise block diagonal
structure where l-th diagonal block Ct,[l] corresponds to Kronecker-factored approximation with
respect to the parameters at l-th layer. The proximal version of KFAC, which corresponds to
Ct,[i] = E[διδ1] 0 E[aι-ιaT-ι] and R(θ) = ∣∣θ∣q where δι is the gradient with respect to the
outputs of l-th layer and al-1 is the activation of (l - 1)-th layer, could be another example.
Examples of Proximal Mappings for PROXGEN. We provide update rules for PROXGEN with 'q
regularization (0 ≤ q ≤ 1) and diagonal preconditioners. Diagonal preconditioners are used by
popular adaptive gradient methods such as ADAM. Specifically, we consider regularizer R(θ) =
λ pp=ι ∖θj ∣q for θ ∈ Rp with diagonal preconditioner matrix Ct. Note that for Ct = I (i.e.
vanilla gradient descent), it is known that closed-form solutions exist for proximal mappings for
q ∈ {0,1, 2, 1} (Cao et al., 2013). We denote the i-th coordinate of the vector θt as θt,i and the
diagonal entry [Ct]ii as Ct,i
• `1 regularization. The proximal mappings for the case of `1 regularization with preconditioner
can be computed efficiently via soft-thresholding as
θt,i = θt,i - at C J； §,	θt+1,i = sign(θt,i)(lθt,i∣ -
αtλ
Ct,i + δ
(3)
• `0 regularization. In case of `0 regularization, we can compute the closed-form solutions via
hard-thresholding as
^
θbt,i
θti-ɑt -m^,
t,i tCt,i + δ,
fθt,i,	∖θt,i∖ > N Ct,i+δ,
θt+1,i = [ 0,	∖θt,i∖ < qCa+δ
[{0,bt,i},	∖bt,i∖=qC2a+δ
(4)
The closed-form proximal mappings for '“ and '2/3 regularization are provided in the Appendix.
Revised ProxQuant (Bai et al., 2019). The
recently proposed ProxQuant proposes
novel regularizations for network quantiza-
tion. Especially for binary quantization, a W-
shaped regularizer is defined as Rbin (θ) =
∣θ - sign(θ)∣1 where sign(θ) is applied on
Table 2: PROXQUANT versus revised PROXQUANT
ProxQuant	PrθxαtλR(∙) (θ	t-αt(Ct+δI)-1mt
Revised ProxQuant	PrOXCt+RI .)(θ	t-αt(Ct+δI)-1mt
θ in an element-wise manner. Using this regularizer, the main difference between PROXQUANT
and our ProxGen approach is shown in Table 2. Note that ProxQuant (top in Table 2) does not
consider the effect of preconditioners when computing proximal mappings. Therefore, we revise
the proximal update in ProxQuant by considering preconditioners in proximal mappings with
PROXGEN (bottom in Table 2). Moreover, we also propose generalized regularizers motivated
by 'q regularization for 0 < q < 1: Rqbin(θ) = ∣θ - sign(θ)∣q. In terms of theory, Bai et al.
(2019) prove the convergence of PROXQUANT only for the full-batch gradient with differentiable
regularizers, which is also guaranteed only for vanilla gradient descent. In contrast, using our revised
ProxQuant, we can completely bridge the gap in theory (via Theorem 1 in Section 3, which is
stated for stochastic optimization), and we provide the exact update rule for solving problem in
Eq. (1). We also investigate the empirical differences of ProxQuant and our revised ProxQuant
in Section 4.
3	Convergence Analysis
In this section, we provide convergence guarantees for the ProxGen family. Our goal is to find
an -stationary point for the problem in Eq. (1) where is the required precision. For notational
convenience, we assume that the regularization parameter λ is incorporated into R(θ) in Eq. (1). To
guarantee the convergence under this setting, we should deal with the subdifferential defined as:
4
Under review as a conference paper at ICLR 2021
Definition 1 (Frechet Subdifferential). Let φ be a real-valuedfunction. The Frechet Subdiferential
of 夕 at θ with |夕(8)| < ∞ is defined by ∂φ(θ~) := (θ* ∈ Ω ∣ lim inf "⑻-Wθ ,θ-θi ≥ 0}.
To derive the convergence bound, we make the following mild conditions:
(C-1) (L-smoothness) The loss function f is differentiable, L-smooth, and lower-bounded:
∀x,y, kVf (x) - Vf (y)k ≤ Lkx - yk and f (x*) > -∞ for the optimal solution x*.
(C-2) (Bounded variance) The stochastic gradient gt = Vf (θt; ξ) is unbiased and has the bounded
variance: Eξ [Vf0; ξ)] = Vf(θj	Eξ [∣∣gt - Vf@)『]≤ σ2.
(C-3) (i) final step-vector is finite, (ii) the stochastic gradient is bounded, and (iii) the momentum
parameter should be exponentially decaying: (i) kθt+1 - θt k ≤ D, (ii) kgt k ≤ G, (iii) ρt =
ρ0μt-1 with D,G > 0 and ρo, μ ∈ [0,1).
(C-4) (Sufficiently positive-definite) The minimum eigenvalue of effective spectrums should be
uniformly lower bounded over all time t:∀t, λmin αt(Ct + δI)-1 ≥γ>0.
(C-1) and (C-2) are standard in general non-convex optimization (Ghadimi & Lan, 2013; Ghadimi
et al., 2016; Zaheer et al., 2018; Xu et al., 2019a). In addition, (C-3) is extensively studied in previous
literature in the context of adaptive gradient methods (Kingma & Ba, 2015; Reddi et al., 2018; Chen
et al., 2019a). Lastly, a similar condition to (C-4) is also considered in Chen et al. (2019a); Yun et al.
(2019a), and it can be easily satisfied in practice. More discussion on (C-4) is provided later.
Since the loss function f is assumed to be differentiable as in (C-1), we have, at stationary points,
0 ∈ ∂F(θ) = Vf(θ) + ∂R(θ), so the convergence criterion is slightly different from that of general
non-convex optimization. Hence, we use the following convergence criterion E[dist(0, ∂F (θ))] ≤
for an -stationary point where dist(x, A) denotes the distance between a vector x and a set A. If no
regularizer is considered (R = 0), this criterion boils down to the one usually used in non-convex
optimization, E[kVf (θ)k] ≤ . We are now ready to state our main theorem for general convergence.
Theorem 1. Let θa denote an iterate uniformly randomly chosen from {θι, •一，θτ}. Under the
conditions (C-1), (C-2), (C-3), (C-4) with the initial stepsize α° ≤ 袅 and non-increasing stepsize at,
PROXGEN, Algorithm 1, is guaranteed to yield Ea [dist(0, ∂F(θa))2] ≤ QT PTOI * + QTδ + QT
where ∆ = f(θ) - f(θ*) with optimal point θ*, and bt is the minibatch size at time t. The constants
{Qi}3=ι on the right-hand side depend on the constants {αo, δ, L, D, G, ρo, μ, γ}, but not on T.
From Theorem 1, the appropriate minibatch size is important to ensure a good convergence. Various
settings for the minibatch size could be employed for convergence guarantee (for example, bt = t),
but considering practical cases, we provide the following important corollary for constant minibatch.
Corollary 1 (Constant Mini-batch). Under the same assumptions as in Theorem 1 with sample size
n and constant minibatch size b = b = Θ(T) = Θ(√n), we have E[dist(0, ∂F(θa))2] ≤ O(1/T)
and the total complexity is O(1∕e4) in order to have E[dist(0, ∂F(θa))] ≤ e.
Here we make several remarks on our results and relationship with prior work.
•	Improvements upon Prior Work. The most challenging parts in our
analysis compared to previous study (Xu et al., 2019a) (which is only for
vanilla SGD) is that we should handle the momentum mt and non-trivial
preconditioner Ct. Due to the existence of mt, it is highly non-trivial to
bound the term kmt - Vf (θt)k2 without suitable assumptions whereas
kgt - Vf(θt)k2 in Xu et al. (2019a) can be easily bounded using (C-
2). The second challenge is to deal with quadratic approximation term
(θ - θt)T(Ct + δI)(θ - θt) in Algorithm 1 which is not problematic in
Xu et al. (2019a) due to trivial Ct = I . We can successfully bypass those
Figure 1: Empirical re-
sults for condition (C-4).
difficulties using mild conditions (C-3) and (C-4) respectively and also allow non-increasing stepsize.
•	On Condition (C-4). (C-4) is easily satisfied both theoretically and empirically. Most of the
popular optimization algorithms for deep learning such as AdaGrad, Adam, and KFAC satisfies
this condition (see Appendix D). In order to investigate whether this condition could be satisfied
in real problems, we train ResNet-34 on CIFAR-10 dataset. In Figure 1, we can see the minimum
eigenvalue of αt(Ct + δI)-1 tends to increase, so the condition (C-4) is also satisfied empirically.
5
Under review as a conference paper at ICLR 2021
Learmnq Curve for VGG-16
Performance for VGG-16	Learnmq Curve for VGG-16
O 1
O -
11
)+H
----ProxGen (Ours)
----Sub-Adam
—— Prox-SGD (ICLR 2020)
Sg5
999
(％)AU2n3<
∙( ■
1 O
ε)κ+s≈Hsc
Performance for VGG-16
ʒgʒg
4 4 3 3
9 9 9 9
(％)AU2n3<
0	75	150	225	300	85	90	95
Epochs	Pruned Ratio (%)
(a) '1 regularization
0	75	150	225	300	85	90	95
Epochs	Pruned Ratio (%)
(b) '2/3 regularization
ProxGen (Ours)
Sub-Adam
Learninq Curve for VGG-16
00
)+N)
75	150	225	300
Epochs
(求)tt3h-
Performance for VGG-16
85	90	95
Pruned Ratio (%)
VGG-16 on CIFAR-IO
(d) `0 regularization
(c) '1/2 regularization
Figure 2:	Comparison for sparse VGG-16 on CIFAR-10 dataset.
s)κ+§ Hsc
75	150	225	300
Epochs
Performance for ResNet-34
Sg5
999
(％) &E30≤ ttφπ
75	80	85	90	95
Pruned Ratio (%)
≡δ+s≈H ®c
75	150	225	300
Epochs
Performance for ReSNet-34
Sg5
959594
《％&E30≤ ttφπ
80	85	90	95
Pruned Ratio (%)
0
(a) `1 regularization
(b)	'2/3 regularization
Learning Curve for ResNet-34	Performance for ResNet-34	ResNet-34 on CIFAR-IO
ProxGen (Ours)
Sub-Adam
O 1
W1°-
S⅛M + st H
S 95.5
>»
Q 95.0
S 94.5-
⅛
U 94.0-
(求)AuαJr□zs9J.
0	75	150	225	300	80	85	90	95	100	40	60	80	100
Epochs	Pruned Ratio (%)	Pruned Ratio (%)
(c)	'1/2 regularization	(d) '0 regularization
Figure 3:	Comparison for sparse ResNet-34 on CIFAR-10 dataset.
• Implications of Condition (C-4) on Theory. Our analysis relies on (C-4), the lower bound
for the minimum eigenvalue of Γt := αt(Ct + δI)-1. This means that Theorem 1 guarantees
Ea [dist(0, ∂F(θa)2] ≤ O(1∕√T) (in case of b = Θ(√n) as in Corollary 1) for any change of basis
of Γt , so in that sense, we provide a worst-case analysis and there is room for more optimistic bounds.
• Connections to Second-order Methods. Our analysis can provide guarantees for positive second-
order preconditioners as long as (C-4) is satisfied (The empirical Fisher information (Martens &
Grosse, 2015) is one example). Although second-order solvers generally enjoy very fast convergence
under strongly convex loss (Lee et al., 2012; Zhang et al., 2019), it can be understood that our theory
guarantees at least a sublinear rate for such second-order curvatures with less stringent conditions.
4	Experiments
We consider two important tasks for regularized training in deep learning communities: (i) training
sparse neural networks and (ii) network quantization. Throughout our experiments, we consider
ADAM as a representative of PROXGEN Where mt = Ptmt-ι + (1 - ρt)gt with constant decaying
parameter Pt = 0.9 and Ct = ββCt-+ + (1 - β)g2 with β = 0.999 in Algorithm 1. The details on
other hyperparameter/experiment settings are provided in the Appendix.
6
Under review as a conference paper at ICLR 2021
Table 3: Comparison for binary neural networks. The best performance in mean value is highlighted.
Test Error (%)
Baselines	∣∣	PROXGEN (Ours)
Model	Full Precision (32-bit)	BinaryConnect Courbariaux et al. (2015)	ProxQuant Bai et al. (2019)	Revised ProxQuant `1	Revised ProxQuant '2/3	Revised ProxQuant '1/2
ResNet-20	8.06	9.54 ± 0.03	9.35 ± 0.13	9.50 ± 0.12	9.72 ± 0.06	9.78 ± 0.18
ResNet-32	7.25	8.61 ± 0.27	8.53 ± 0.15	8.29 ± 0.07	8.22 ± 0.05	8.43 ± 0.15
ResNet-44	6.96	8.23 ± 0.23	7.95 ± 0.05	7.68 ± 0.07	7.91 ± 0.08	7.90 ± 0.13
ResNet-56	6.54	7.97 ± 0.22	7.70 ± 0.06	7.52 ± 0.18	7.60 ± 0.09	7.61 ± 0.12
Training Sparse Neural Networks. Motivated by the lottery ticket hypothesis (Frankle & Carbin,
2019), we consider training VGG-16 (Simonyan & Zisserman, 2014) and ResNet-34 (He et al., 2016)
on CIFAR-10 dataset using sparsity encouraging regularizers. Toward this, we consider the following
objective function with 'q regularization: F(θ) := Eξ〜p[f (θ; ξ)] + λ P)P=I ∣θj|q where 0 ≤ q ≤ 1.
We train the network parameters with the closed-form proximal mappings introduced in Section 2.
We compare ProxGen with subgradient methods and also include Prox-SGD (Yang et al., 2020)
as a baseline especially for `1 regularization since PROX- S GD considers only convex regularizers. In
PROX-SGD, the hand-crafted fine-tuned scheduling on αt and ρt is essential for fast convergence
and good performance, but in our experiments we use standard settings ρt = 0.9 with step-decay
learning rate scheduling for fair comparisons. For `0 regularization, the problem in Eq. (1) cannot be
optimized in a subgradient manner, so we compare ProxGen with another popular baseline, '0.°
(Louizos et al., 2018) which approximates the 'o-norm via hard-concrete distributions.
Figures 2 and 3 illustrate the results for VGG-16 and ResNet-34 respectively. In terms of convergence,
PROXGEN shows faster convergence than PROX- S GD for `1 case, but there is no difference between
ProxGen and subgradient methods. However, there are notable differences in convergence for non-
convex regularizers '" and '2/3, which get bigger as q decreases. We believe this might be because
the 'q-norm derivative, q∕∣θ∣1-q, is very large for non-zero tiny θ for q ∈ (0,1). Meanwhile, ∂∣θ∣∕∂θ
is merely the sign value regardless of size of θ, so the large gradient of ∣θ∣q may hinder convergence.
The learning curves in Figure 2-(b,c) and 3-(b,c) empirically corroborate this phenomenon.
In terms of performance, we can see that ProxGen consistently achieves better performance than
baselines for both VGG-16 and ResNet-34 with similar or even better sparsity level. Importantly,
PROXGEN with '0 outperforms '0hc baseline by a great margin. This might be due to the design of
'0hc, which approximates kθk0 = Pjp=1 I{θj 6= 0} with binary mask zj parameterized by learnable
probability πj for each coordinate. Thus, the number of parameters to be optimized is doubled, which
might make optimization harder. In constrast, ProxGen does not introduce additional parameters.
More results for MCP (Zhang et al., 2010) and SCAD (Fan & Li, 2001) regularizers are in Appendix.
Training Binary Neural Networks. In the second set of experiments, we consider the network
quantization constraining the parameters to some set of discrete values which is a key approach for
model compression. We evaluate our revised ProxQuant in Table 2 with extended regularization
Rqbin in Section 2. We consider the following objective function with quantization-specific regular-
izers: F(θ) := Eξ〜p[f (θ; ξ)] + λPjp =1 ∣θj - sign(θj)∣q where 0 ≤ q ≤ 1. For comparisons, we
quantize ResNet on CIFAR-10 dataset and follow the same experiment settings as in ProxQuant.
Table 3 presents the results. For all q values, revised PROXQUANT consistently outperforms the
baselines except for ResNet-20, which implies ProxGen may work better for larger networks.
As such, our generalized regularizers Rqbin contribute to one of the state-of-the-art optimization-
based methods in network quantization. Notably, revised PROXQUANT '1 greatly outperforms
ProxQuant baseline while these two approaches differ only in update rules (see Table 2). Hence,
we can conclude that revised PROXQUANT based on PROXGEN provides an exact proximal update
and also yields more generalizable solutions. In our experience, revised PROXQUANT '0 shows little
degradation in performance, so we do not include this result. However, revised PROXQUANT '0
shows superiority to baselines for language modeling, whose preliminary results are in Appendix.
7
Under review as a conference paper at ICLR 2021
0	500 0	500
Features	Features
True Parameter	ProxGen (Ours)
-buds
True Parameter
ProxGen (Ours)
Prox-SGD (ICLR 2020)
(a)	Random initialization
2 O
S-BUDS
s-Jg6lω
500 0	500 0
Features	Features
(b)	Zero initialization
500
Features
Figure 4:	Lasso simulations with different initialization schemes.
5 A closer look into Prox-SGD (Yang et al., 2020) vs. ProxGen
Prox-SGD (Yang et al., 2020) is the approach closest to our ProxGen method. However, Prox-
SGD is not an exact proximal approach and is significantly different from PROXGEN. PROXGEN’s
update rule involves directly solving the quadratic subproblem (Eq. (2)). In contrast, Prox-SGD’s
update rule consists of two stages: (i) solving the quadratic subproblem without learning rate, then (ii)
updating the parameters with the computed direction (i.e. θt - θt) by the learning rate αt (Eq. (5)).
θt = argmin {〈mt, θ + λR(θ) + 1(θ - θt)T (Ct + δI) (θ - θt)}, θt+ι = θt + αt(b - θt) (5)
To clearly see the differences between both approaches, we conduct two studies.
Figure 5: Learning curve.
Study 1: Lasso Support Recovery. For this task, the two-stage update scheme of PROX-SGD
might have some potential issues. For example, for '1 -regularized problems, the updated parameter
θt+1 (Eq. (5)) might not achieve exact zero (while θt can) whereas θt+1 for PROXGEN (Eq. (2)) can
attain exact zero value according to the update rule (Eq. (3)) in Section 2. Another potential caveat is
that PROX-SGD might overestimate the sparsity level. In view of the above, we run Lasso simulations
with different two initialization schemes: (i) random initialization and (ii) zero initialization. For
random initialization, it can be seen in Figure 4-(a) that Prox-SGD could not achieve exact zero
value, which corroborates our first observation. More interestingly, for zero initialization, we can
see in Figure 4-(b) that the estimates using Prox-SGD are exactly zeros for all coordinates, which
supports our second observation. This might be because θt (Eq. (5)) is always zero since the quadratic
subproblem does not consider the learning rate, which might overestimate the sparsity level. Hence,
the subsequent iterate θt+1 would be always zero since we initialize the parameters with zeros. On
the other hand, ProxGen correctly recovers the support in both cases.
Study 2: DenseNet-201 on CIFAR-100 Dataset. To validate the supe-
riority of ProxGen upon Prox-SGD, we revisit the largest experiments
in Yang et al. (2020). We train DenseNet-201 architecture on CIFAR-100
dataset with `1 regularization since PROX- S GD only consider convex
regularizers. For both methods, we use the same hyperparameter settings
for fair comparison. Figure 5 illustrates the training learning curves, and
it can be seen that our ProxGen achieves faster convergence as well as
lower objective values. For our experience, the learning curves show the
similar dynamics for different λ values.
Comparison of Theoretical Contributions. Yang et al. (2020) guarantees the convergence of PROX-
SGD, but not how fast it converges. Moreover, this is proved without considering preconditioners.
In contrast, our analysis for the ProxGen framework appropriately incorporates the first-order
momentum and arbitrary positive preconditioner with detailed non-asymptotic convergence.
6 Conclusion
In this work, we proposed ProxGen, the first general family of stochastic proximal gradient
methods. Within our framework, we presented novel examples of proximal versions of standard
SGD approaches, including a proximal version of Adam. We analyzed the convergence of the
whole ProxGen family and showed that ProxGen can encompass the results of several previous
studies. We also demonstrated that ProxGen empirically outperforms subgradient-based methods
for popular deep learning problems. As future work, we plan to study efficient approximations of
proximal mappings for structured regularizers such as '1 /'q norms with preconditioners.
8
Under review as a conference paper at ICLR 2021
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, MatthieU
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for
large-scale machine learning. In 12th {USENIX} Symposium on Operating Systems Design and
Implementation ({OSDI} 16),pp. 265-283, 2016.
Zeyuan Allen-Zhu. Natasha: Faster non-convex stochastic optimization via strongly non-convex
parameter. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pp. 89-97. JMLR. org, 2017.
Yu Bai, Yu-Xiang Wang, and Edo Liberty. Proxquant: Quantized neural networks via proximal
operators. In International Conference on Learning Representations, 2019. URL https://
openreview.net/forum?id=HyzMyhCcK7.
Stephen Becker, Jalal Fadili, and Peter Ochs. On quasi-newton forward-backward splitting: Proximal
calculus and convergence. SIAM Journal on Optimization, 29(4):2445-2481, 2019.
Wenfei Cao, Jian Sun, and Zongben Xu. Fast image deconvolution using closed-form thresholding for-
mulas of lq (q= 12, 23) regularization. Journal of visual communication and image representation,
24(1):31-41, 2013.
Tianyi Chen, Tianyu Ding, Bo Ji, Guanyi Wang, Yixin Shi, Sheng Yi, Xiao Tu, and Zhihui Zhu.
Orthant based proximal stochastic gradient method for '_ 1-regularized optimization. arXiv preprint
arXiv:2004.03639, 2020.
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of adam-type
algorithms for non-convex optimization. In International Conference on Learning Representations,
2019a. URL https://openreview.net/forum?id=H1x-x309tm.
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of adam-
type algorithms for non-convex optimization. In 7th International Conference on Learning
Representations, ICLR 2019, 2019b.
Emilie Chouzenoux, Jean-Christophe Pesquet, and Audrey Repetti. Variable metric forward-
backward algorithm for minimizing the sum of a differentiable function and a convex function.
Journal of Optimization Theory and Applications, 162(1):107-132, 2014.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural
networks with binary weights during propagations. In Advances in neural information processing
systems, pp. 3123-3131, 2015.
Damek Davis and Dmitriy Drusvyatskiy. Stochastic model-based minimization of weakly convex
functions. SIAM Journal on Optimization, 29(1):207-239, 2019.
Damek Davis, Dmitriy Drusvyatskiy, Sham Kakade, and Jason D Lee. Stochastic subgradient method
converges on tame functions. Foundations of computational mathematics, 20(1):119-154, 2020.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. In Journal of Machine Learning Research (JMLR), 2011.
Jianqing Fan and Runze Li. Variable selection via nonconcave penalized likelihood and its oracle
properties. Journal of the American statistical Association, 96(456):1348-1360, 2001.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable
neural networks. In International Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=rJl-b3RcF7.
Wenjiang J Fu. Penalized regressions: the bridge versus the lasso. Journal of computational and
graphical statistics, 7(3):397-416, 1998.
9
Under review as a conference paper at ICLR 2021
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic
programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation methods
for nonconvex stochastic composite optimization. Mathematical Programming, 155(1-2):267-305,
2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representation (ICLR), 2015.
Jason D Lee, Yuekai Sun, and Michael Saunders. Proximal newton-type methods for convex
optimization. In Advances in Neural Information Processing Systems, pp. 827-835, 2012.
Yunwen Lei, Ting Hu, Guiying Li, and Ke Tang. Stochastic gradient descent for nonconvex learning
without bounded gradient assumptions. IEEE Transactions on Neural Networks and Learning
Systems, 2019.
Dong C Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization.
Mathematical programming, 45(1-3):503-528, 1989.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-
ence on Learning Representations, 2019. URL https://openreview.net/forum?id=
Bkg6RiCqY7.
Christos Louizos, Max Welling, and Diederik P. Kingma. Learning sparse neural networks through l0
regularization. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=H1Y8hhg0b.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In International conference on machine learning, pp. 2408-2417, 2015.
Cheolwoo Park and Young Joo Yoon. Bridge regression: adaptivity and group selection. Journal of
Statistical Planning and Inference, 141(11):3506-3519, 2011.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. In Advances in Neural Information Processing Systems,
pp. 8024-8035, 2019.
Nhan H Pham, Lam M Nguyen, Dzung T Phan, and Quoc Tran-Dinh. Proxsarah: An efficient algorith-
mic framework for stochastic composite nonconvex optimization. arXiv preprint arXiv:1902.05679,
2019.
P. Ravikumar, M. J. Wainwright, G. Raskutti, and B. Yu. High-dimensional covariance estimation by
minimizing `1 -penalized log-determinant divergence. Electronic Journal of Statistics, 5:935-980,
2011.
Sashank J Reddi, Suvrit Sra, Barnabas Poczos, and Alexander J Smola. Proximal stochastic methods
for nonsmooth nonconvex finite-sum optimization. In Advances in Neural Information Processing
Systems, pp. 1145-1153, 2016.
Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In
International Conference on Learning Representations, 2018. URL https://openreview.
net/forum?id=ryQu7f-RZ.
10
Under review as a conference paper at ICLR 2021
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society, Series B, 58(1):267-288,1996.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-31,
2012.
A. N. Tychonoff. On the stability of inverse problems. Doklady Akademii Nauk SSSR, 39(5):195-198,
1943.
Zhe Wang, Kaiyi Ji, Yi Zhou, Yingbin Liang, and Vahid Tarokh. Spiderboost: A class of faster
variance-reduced algorithms for nonconvex optimization. arXiv preprint arXiv:1810.10690, 2018.
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep
neural networks. In Advances in neural information processing systems, pp. 2074-2082, 2016.
Yi Xu, Rong Jin, and Tianbao Yang. Non-asymptotic analysis of stochastic methods for non-smooth
non-convex regularized problems. In Advances in Neural Information Processing Systems, pp.
2626-2636, 2019a.
Yi Xu, Qi Qi, Qihang Lin, Rong Jin, and Tianbao Yang. Stochastic optimization for DC functions
and non-smooth non-convex regularizers with non-asymptotic convergence. In International
conference on machine learning, 2019b.
Eunho Yang and Aurelie C Lozano. Sparse+ group-sparse dirty models: Statistical guarantees without
unreasonable conditions and a case for non-convexity. In Proceedings of the 34th International
Conference on Machine Learning-Volume 70, pp. 3911-3920. JMLR. org, 2017.
Jiwei Yang, Xu Shen, Jun Xing, Xinmei Tian, Houqiang Li, Bing Deng, Jianqiang Huang, and
Xian-sheng Hua. Quantization networks. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 7308-7316, 2019.
Yang Yang, Yaxiong Yuan, Avraam Chatzimichailidis, Ruud JG van Sloun, Lei Lei, and Symeon
Chatzinotas. Proxsgd: Training structured neural networks under regularization and constraints. In
International Conference on Learning Representations, 2020. URL https://openreview.
net/forum?id=HygpthEtvr.
Jihun Yun, Aurelie C. Lozano, and Eunho Yang. Stochastic gradient methods with block diagonal
matrix adaptation. arXiv preprint arXiv:1905.10757, 2019a.
Jihun Yun, Peng Zheng, Eunho Yang, Aurelie Lozano, and Aleksandr Aravkin. Trimming the `1
regularizer: Statistical analysis, optimization, and applications to deep learning. In International
Conference on Machine Learning, pp. 7242-7251, 2019b.
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive methods
for nonconvex optimization. In Advances in neural information processing systems, pp. 9793-9803,
2018.
Cun-Hui Zhang et al. Nearly unbiased variable selection under minimax concave penalty. The Annals
of statistics, 38(2):894-942, 2010.
Guodong Zhang, James Martens, and Roger B Grosse. Fast convergence of natural gradient descent
for over-parameterized neural networks. In Advances in Neural Information Processing Systems,
pp. 8080-8091, 2019.
11
Under review as a conference paper at ICLR 2021
Appendix
A	Additional Experiments: Sparse Neural Networks with MCP
and SCAD Non-convex Regularizers
We provide the additional experiments for sparse neural networks with MCP (Zhang et al., 2010) and SCAD
(Fan & Li, 2001) non-convex regularizers. Figure 6 and 7 illustrate the results for VGG-16 and ResNet-34
respectively. As shown in Section 4 and these figures, ProxGen is very effective for solving the non-convex
regularized problems.
B	Details on Experimental Settings
Sparse Neural Networks. To reflect the most practical training settings, we first tune the weight-
decay parameter Z without 'q regularizers. For weight-decay coefficients, We consider the candidates
ζ ∈ {0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5} for ζ and the best ζ value is 0.2 for both networks
VGG-16 and ResNet-34 in our experience. After tuning weight-decay coefficient ζ, we consider both decoupled
weight decay (LoShChilov & Hutter, 2019) and 'q regularization whose detail update rule is described in Al-
gorithm 2. For all comparison methods except 'o.c, the recommended stepsize at = 0.001 is employed, but
we tune this stepsize for 'o.c baseline. We consider a broad range of regularization parameters for all methods:
λ ∈ {0.001, 0.002, 0.005, 0.01, 0.02,…,1.0, 2.0, 5.0}. With these hyperparameter settings, we consider the
total 300 epochs and divide the learning rate at 150-th and 250-th epoch by 10.
Binary Neural Networks. In this experiment, we follow the same experimental settings in baseline
PROXQUANT (Bai et al., 2019). We first pre-train ResNet-{20, 32, 44, 56} with full-precision and initialize the
network parameters with these pre-trained weights. Then, we consider the total 300 epochs and hard-quantize
the networks at 200-th epoch (i.e. quantizing the weight parameters to +1 or -1). We employ the homotopy
method introduced in Bai et al. (2019): annealing the regularization paramter λ as λepoch = λ × epoch. For
initial value of λ, we use λ = 10-8 or λ = 5 ∙ 10-8 for all ResNet architecture. We use the constant stepsize
αt = 0.01 as recommended in Bai et al. (2019).
Lasso Support Recovery. We generate simple Lasso simulations with problem dimension p = 500 and
n = 100 data samples. The number of non-zero entries in true parameter vector θ* ∈ Rp is set to 10. The
design matrix X ∈ Rn×p is generated from standard Gaussian distribution N(0, 1) and we randomly assign
+1 or -1 for the non-zero value in true parameter at random 10 coordinates. The response variable y ∈ Rn is
generated with small noise by y = Xθ* + e where e 〜N(0, 0.052). For both PROXGEN and PROX-SGD, we
employ ADAM for preconditioner matrix Ct construction.
Here, we introduce preliminary results of revised PROXQUANT `0 on language modeling. For this experiment,
we train one hidden layer LSTM with embedding dimension 300 and 300 hidden units according to Bai et al.
(2019). First, we pre-train the full-precision LSTM and initialize the network with pre-trained weights. We
consider the total 80 epochs and divide the learning rate by 1.2 if the validation loss does not decrease. Table 4
shows the preliminary results and revised PROXQUANT `0 is superior to the PROXQUANT baseline in this task.
Table 4: Preliminary results on revised PROXQUANT `0 for LSTM models.
Algorithm	Test Perplexity
Full-precision (32-bit)	88.5
BinaryConnect Courbariaux et al. (2015)	372.2
PROXQUANT Bai et al. (2019)	288.5
revised PROXQUANT `0 (Ours)	223.4
12
Under review as a conference paper at ICLR 2021
s)κ+ε≈Hsc
---ProxGen (Ours)
--- Sub-Adam
O 75	150	225	300
Epochs
Learning Curve for VGG-16 g4 6
^94.4
υ
E
B 94.2
Learnlng CUrVe ft>r VGG-16
---ProxGen (Ouns)
---Sub-Adam
s)κ+s≈Nsc
Epochs
Pruned Ratio (%)
(a)	MCP regularization
(b)	SCAD regularization
Figure 6:	Comparison for sparse VGG-16 on CIFAR-10 dataset with other non-convex regularizers.
Learning Curve for ResNet-34
ProxGen (Ours)
Sub-Adam
≡)κ+sκH (su∙
O 75	150	225	300
Epochs
(％) "3h-
70	80	90	100
Pruned Ratio (%)
Learning Curve for ResNet-34
亩)κ+seN(su∙
Learning Curve for ResNet-34
5.0
5
9
(％) Au,ln34 ttωπ
(a)	MCP regularization
0	75	150	225	300	70	80	90	100
Epochs	Pruned Ratio (%)
(b)	SCAD regularization
Figure 7:	Comparison for sparse ResNet-34 on CIFAR-10 dataset with other non-convex regularizers.
C Derivations for Proximal Mappings
Here, We derive the concrete update rule for 'q regularization With diagonal preconditioners as introduced in
Section 2.
'1/2 regularization. First, we review the closed-form proximal mappings for '1/2 regularization of vanilla
Sgd. First, We consider the folloWing one-dimensional program:
b = argmin{(x — z)2 + λ∣x∣1∕2}	(6)
x
For the program Eq. (6), it is known that the closed-form solution exists Cao et al. (2013) as
{3 ∣z∣(l + cos (3∏ - 3中λ(z)))	if Z > p(λ)
0	if |z| ≤ p(λ)	(7)
-3|z|(1 + cos (2π - 3中入(Z)))	if z < -p(λ)
where 夕λ(z) = arccos(8 (l3)-3/2) and p(λ) = √4 (λ)2/3. Based on this closed-form solution, we derive
PROXGEN for '1/2 regularization with diagonal preconditioners. By Eq. (2), we have
θbt = θt - αt(Ct + δI)-1mt
Θt+1 ∈ ProxCt+RI∙) (θt)
p
=argmin{ k∖∖-- btkCt+δi + λ X lθj|1/2}
θ	2	j=1
(8)
(9)
(10)
Since the program Eq. (10) is coordinate-wise decomposable (since the preconditioner matrix Ct is diagonal),
we can split Eq. (10) into
θt+1,i
argmin 1 1(Ct,i + δ)(θi 一 θt,i)2
θi	2
+ αtλ∣θi∣1∕2}
argmin {(θi - bt,i)2 + ；#§ ,11/2}
13
Under review as a conference paper at ICLR 2021
for the i-th coordinate. From Eq.(6), we can derive
(1 同,i∣(1 + cos (Iπ - 3夕λ(bt,i)))	if bt,i >p(λ)
θt+ι,i = < 0	if ∣θt,i∣ ≤ p(λ)
[-1 ∣θt,i∣(l + cos (iπ - 1 夕λ(θt,i)))	if θt,i < -p(λ)
where
∕z? ∖
^λ(θt,i)
arccos
,	∖	I Z? I	、
( αtλ ( ∣θt,i∣ χ-3∕l∖
4(Ct,i + 履 3	,
P(λ)
√54( 2αtλ y/3
丁 g,i + δ )
22/3 regularization. Now, we provide the closed-form solutions for proximal '∣∕3 mappings with diagonal
preconditioners. Similar to '1∕∣ regularization, we start from the closed-form solutions of the following program:
X = argmin{(x — z)1 + λ∣x∣l∕3}
x
(11)
The closed-form solution for the program Eq. (11)is known to be
∣A∣+r ⅜⅛ tai2 !3
if z > 1 √3λ1
b = < 0
—
if ∣z∣ ≤ 3 √3λ3
∣A∣ + r2F^ !3	if z< - i √3λ3
where
∣A∣ = √23λ1∕4(cosh(3)) / , φ = arccosh(27^A-3/I)
(12)
(13)
Based on this formulation, we derive the closed-form proximal mappings with diagonal preconditioner Ct. By
Eq. (2), we have
^
θt
θt - αt(Ct + δI)-1mt
Θt+1 ∈ PrOXCt+款)诙)
p	P
=argmin{2 ∣∣θ - XtIlCt+δi+ λ X ∣θj∣l∕3}
θ	j=1
(14)
(15)
(16)
As in 'ι∕∣ case, the program Eq. (16) is coordinate-wise separable, so it suffices to solve the sub-problems for
each coordinate as
θt+1,i
argmin
θi
{ 2(Ct,i + δ)(θi - θi)2 + αtλ∣θi，/3}
From Eq. (11), we can derive
where
argmin {e-西 + d⅛∣θi∣ιz3}
，(∣A∣+r %-∣a∣2 !
θt+ι,i = < 0
_( ∣A∣+r % TA口
if θt,i > 3 √3λ3
if ∣θt,i∣ ≤ 3 √3λ3
if θt,i < -3 √3λ3
∣A∣
√3 (⅛⅛ )1/4(cosh( 3))“	φ = arccosh
2ɑtλ )-3/I
Ct,i + δ )
In addition to 'q regularization, we provide the closed-form proximal mappings for another regularizers with
non-trivial preconditioners.
14
Under review as a conference paper at ICLR 2021
MCP regularization. Before introducing the closed-form of proximal m mappings for MCP regularized
problems with diagonal preconditioners, we first review the MCP regularizer. The MCP regularizer is defined as
,入、	λλ∣x∣ — x-	if |x| ≤ bλ
pλ(x; b)=3	if∣χ∣>bλ
(17)
where b > 0 is called the MCP parameter and λ is a regularization parameter. Our goal is to derive the proximal
mapping of this regularizer with diagonal preconditioner.
Now, we start from the closed-form solutions of the following program:
12
X = argmin{](x — z) + ρλ(x; b)}	(18)
For this program, the closed-form solution is known as
bmax{∣z∣ - 入,0} ∣
x = Sign(Z) mm ∣------ɪ---------, ∣z∣∣	(19)
Based on this closed-form solution, we derive the closed-form proximal mappings with diagonal preconditioner
Ct . By Eq. (2), we have
θbt = θt — αt (Ct + δI)- mt	(20)
θt+ι ∈ PrOXCt+"(θt)	(21)
=argmin {1 ∣∣θ 一 θtkCt+δi + αtρλ(θ; b)}	(22)
Since this program is also coordinate-wise separable, we could have for each coordinate
A	.力、.nbmax{∣θt,i∣- Cαtλ+δ, 0} ʌ o
θt+ι,i = Sign(θt,i) min ∣-----b-V^---------，∣θt,i∣ j	(23)
SCAD regularization. We first introduce SCAD regularizer defined as :
ρλ(x; a) =
fλ∣x∣
J —λ2 —2aλ∣x∣+x2
2(a—1)
(a+1)λ2
2
if ∣x∣ ≤
if < ∣x∣ ≤ aλ
if ∣x∣ > a
(24)
where a > 2 is called the SCAD parameter andλ is a regularization parameter. As in MCP regularizer, we start
from the following program
X = argmin {1 ∣∣x — z∣∣2 + ρλ(x; a)}
The closed-form solution for this program is known as
xb
sign(z) max{∣z∣ — λ, 0}
(a—1)z —sign(z)aλ
if ∣z ∣ ≤ 2
if2 < ∣z∣ ≤ aλ
if ∣z ∣ > a
(25)
a —2
z
Based on this formulation, we could derive the closed-form solution for ProxGen with diagonal preconditioner.
By Eq. (2), we have
θbt
θt — αt(Ct + δI『mt
θt+ι ∈ PrOXCt+Za) (θt)
=argmin {1 l∣θ — θt∣Ct+δi + αtρλ(θ; a)}
Since the program is coordinate-wise decomposable, we have for each coordinate
(26)
(27)
(28)
θt+1,i
sign(θt,i)max{∣θt,i — λi∣, 0}
Z -1 ∖ Z?	∙ ʌ* Λ	V
(a-I) θt,i-signOθt,iaλi
if∣θbt,i∣ ≤ 2bλi
^ ^ ,
θbt,i
if 2i < ∣θt,i∣ ≤ aλi
(29)
if∣θt,i∣ > aλi
^
a —2

^

^
where Xi = cααt+δ.
Although the derivations look little complicated for both cases, we emphasize that both two closed-form solutions
can be efficiently implemented in a GPU-friendly manner.
15
Under review as a conference paper at ICLR 2021
Algorithm 2 PROXGENW: A General Stochastic Proximal Gradient Method with Weight Decay
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
Input: Stepsize αt, {ρt}tt==1T ∈ [0, 1), regularization parameter λ, small constant 0 < δ << 1,
and weight decay regularization parameter ζ .
Initialize: θ1 ∈ Rd, m0 = 0, and C0 = 0.
for t = 1, 2, . . . , T do
Draw a minibatch sample ξt from P
gt JW(θt; ξt)
mt J Ptmt-I + (1 — Pt)gt	【
Ct J Preconditioner construction
θ J (1 一 αtGθt
θt+ι ∈ argmin ((mt, θ) + λR(θ) + --(θ — &)T(Ct +
θ∈Ω	2αt
end for
. Stochastic gradient at time t
. First-order momentum estimate
. Apply decoupled weight decay
δI)(θ - &)}
11: Output: θT
D	EXAMPLES SATISFYING CONDITION (C-4)
Theorem 2 (Weyl). For any two n × n Hermitian matrices A and B, assume that the eigenvalues of A and B
are
μι ≥ ∙∙∙ ≥ μn, and νι ≥ ∙∙∙ ≥ Vn
respectively. Let λι ≥ ∙∙∙ ≥ λn be the eigenvalues of the matrix A + B, then the following holds
μj + Vk ≤ λi ≤ μr + Vs
for j + k - n ≥ i ≥ r + s - 1. Hence, we could derive
λι ≤ μι + VI
We provide concrete examples and derivations satisfying Condition (C-4) in Section 3.
Vanilla Sgd. The vanilla SGD corresponds to Ct = I . We assume the constant stepsize αt = α. Then, the
condition (C-4) can be computed as
λmin (αt(Ct + δI) I) = λmin9 KIlI) = χ , 1
δ+1 δ+1
Therefore, We conclude that Y = δ+^
1 t T 1/2
AdaGrad. In PROXGEN framework, ADAGRAD corresponds to Ct = ( t E gτgT )	. Under the
τ=1
constant stepsizes αt = α, we have
≤G
Hence, the Condition (C-4) can be satisfied as
λmin (αt(Ct + δI)-1) ≥ -^-= ：= γ
G+δ
16
Under review as a conference paper at ICLR 2021
RMSPROP and ADAM. Exponential moving average (a.k.a. EMA) approaches correspond to Ct =
(βCt-ι + (1 — β)gtgTT)1/2 where β ∈ [0,1) and gt denotes the stochastic gradient at time t. The usual
RM S PROP and ADAM use diagonal approximations for gtgtT, but here we consider more general form (i.e.
including general full matrix gradient outer-product) as introduce in Yun et al. (2019a). First, we derive the
upper bound for maximum eigenvalue for the matrix Ct. The matrix Ct can be expressed by
Ct = (βCt-ι + (I - β')gtgt) ɪ/2
=(β2Ct-2 + β(I - β)gt-1 gt-1 + (I - β)gtgT^) ɪ/2
=((1 — β) X βt-igigTT) /
i=1
We can derive the upper bound for maximum eigenvalue of Ct using Weyl’s theorem (Theorem 2) by
λmaχ(Ct) = λmaχ ((1 — β) X β~-gig)”
i=1
≤ ((1 — β) X βt-iλmaχ(gigTr))1/2
i=1
≤ ((1 — β)G2 X βt-i)1/2
i=1
≤ G(1 — βt)1/2 ≤ G
Hence, we have λmax (Ct + δI) ≤ G + δ. Also, we have
λmax(Ct + δI) = Am. ((Ct + δI)-1) ≤ G+δ
Therefore, the condition (C-4) under the constant stepsize αt = α can be derived as
λmin (αt(Ct + δI) — 1) ≥ -ɪ-
G+δ
which yields Y = g+^ .
Natural Gradient Descent. In this case, we derive the condition (C-4) for the Fisher information matrix
when the loss function is defined as a negative log-likelihood, i.e., f = logp(x∣θ). The natural gradient
descent aims at considering general geometry (not limited to Euclidean geometry), but we restrict our focus
on the distribution space where the Fisher information is employed for preconditioner matrix Ct . The Fisher
information matrix is defined as
F _E	h ∂f(x∣θ) ∂f(x∣θ) τi
F = EQ(X)P(ylχ,θ)	∂θ ∂θ
where Q(x) is data distribution and P (y|x, θ) denotes the model’s predictive distribution (ex. neural networks).
However, in general, we do not have access to true data distribution, so we instead take an expectation with
respect to empirical (training) data distribution Q(x). This trick is also employed for K-FAC approximations to
the Fisher Martens & Grosse (2015). Let the training samples be S = {x1, •一,Xn } with sample size n. Then,
the empirical Fisher could be computed as
F = E_	h ∂f(x∣θ) ∂f(x∣θ) τi
F	EQ(X)P(ylχ,θ)	∂θ ∂θ
=1 X ∂f(χi∣θ) ∂f(xi∣θ)T
=n 乙 ∂θ ∂θ
i=1
Now, we bound the maximum eigenvalue of Fb as
λ (F) = 1X λ	(∂f(χi∣θ) ∂f(χi∣θ)T)
max( ) = n2^λmax(	∂θ ∂θ )
i=1
1t
≤ 1 X G2
n
i=1
= G2
17
Under review as a conference paper at ICLR 2021
by our Condition (C-3). Hence, the Condition (C-4) can be derived as
λmin (αt(Fb + δI 厂 1) ≥ G2α-
G +δ
under the constant stepsize αt = α.
E Proofs of Theorem 1
Lemma 1. The first-order momentum mt in Algorithm 1 satisfies
kmtk2 ≤ G
Proof. We use mathematical induction. For t = 1, the momentum is computed as m1 = ρ1m0 + (1 - ρ1)gt =
(1 - ρ0)g1. Therefore, we have kmtk2 = k(1 - ρ0)g1k ≤ (1 - ρ0)G ≤ G.
Now, we assume that kmt-1 k2 ≤ G holds. The momentum at time t is constructed by mt = (1 - ρt)mt-1 +
ρt gt . Then, we have
kmtk2 = k(1 - ρt)mt-1 + ρtgtk2
≤ (1 - ρt)kmt-1k2 +ρtkgtk2
≤ (1-ρt)G+ρtG=G
where the first inequality comes from the triangle inequality and the second one is derived from the induction
hypothesis.	□
We deal with the following update rule in Algorithm 1 as
θt+1 ∈ argmin {〈(1 — pt)gt + Ptmt-1, θ) + R(θ) +	(θ — θt)T(Ct + δI)(θ — θt)}	(30)
By the optimality condition, we have
1
0 ∈ (1 - Pt)gt + Ptmt-I + ∂R(θt+1) +------(Ct + δI)(θt+1 - θt)
αt
which means that
1
-(1 - Pt)gt - Ptmt-I - .(Ct + δI)(θt+1 - θt) ∈ ∂R(θt+1)
By adding the gradient Vf (θt+1) on both sides, We have
1
Vf (θt+1) - (1 - ρt)gt - Ptmt-1 ——(Ct + δI)(θt+1 - θt) ∈ Vf (θt+1) + ∂R(θt+1) = ∂F(θt+1)
αt
By the definition of θt+1 in Eq. (30), We obtain
〈(1 — pt)gt + Ptmt-1, θt+1) + R(θt+1) + 2α^^ (θt+1 — θt)T(Ct + δI)(θt+1 - θt)
≤ 〈(1 - Pt)gt + Ptmt-1, θt + R(θt)
Which in result
〈(1 - Mgt+Ptmt-Ia〉+ RM) + ,(θt+1- θt)T(Ct + δI)(θt+1- θt) ≤ R(θt)
Since the function f is L-smooth by Condition (C-1), We have
L
f(θt+1) ≤ f(θt) + hVf(θt),θt+1 - θti + 2kθt+1 - θtk2
Adding previous tWo inequalities yields
〈(1 - Pt)gt - Vf (θt) + Ptmt-1,θt+1 - θt> + (θt+1 - θt)T (左(Ct + δI) - LI)(θt+1 - θt)
≤ F(θt) - F(θt+1)	(31)
18
Under review as a conference paper at ICLR 2021
Then, we have
kθt+1 - θtk2lt(Ct+δI)-LI
1
≤ F(θt) - F(θt+ι) -〈(1 - Pt)gt - Vf(θt),θt+ι - θt〉- Stmt-1,θt+1 -仇〉
=F (θt) — F (θt+ι) —〈gt — Vf(θt),θt+ι — θt) + hpt gt,θt+ι — θti — hptmt-ι,θt+ι — θti
≤ F (θt)	- F (θt+1)	+ Ukgt- Vf (θt)k2	+ i^kθt+1 -	θtk2	+ P IIgtk2 +	77kθt+1	- θtk2
2L	2	2L	2
+ kρtmt-1k2kθt+1 - θtk2
≤3 F(θt) - F(θt+ι) + P0尸DG + P0〃2：I)G2 + L∣θt+ι - θtk2 + ɪkgt — Vf(θt)k2
2L	2L
The derivations in inequalities (1-3) as follows:
1	We rearrange the inequality Eq. (31).
② Weusethefactthat ha, bi ≤ 2 ∣∣a∣2 + 1 ∣∣b∣∣2 and ha, bi ≤ ∣∣ak2∣∣bk2. With this, we use modified
version such as ha, bi =〈ca, Cbi ≤ c2∣∣a∣∣2 + c⅛ ∣∣bk2 for any positive constant c.
3 We apply our Lemma 1 and Condition (C-3).
By rearranging the above inequality, we require the following quantity be positive-semidefinite.
13
(Ct + δI) - - LI Z 0
2αt	2
Note that in this inequality we can see that
1	-1-
— (Ct + δI) - DLI Z —δI - -LI
since Ct is positive (semi)definite and αt is non-increasing. Therefore, from this we can derive the stepsize
condition in our Theorem 1 as
一 δ
α0 ≤ -L
Therefore, we have
T-1	2 2	T-1
X kθt+1 - θtk⅛(Ct+δi)-3LI≤ F(θ0L-F(θ[ + p0Z^ + 2Lp0-μ2) +无 X kgt - Vf (θt)k2
t=0	∆	|一μ一二一-U	t=0
C1
1 T-1
≤ ∆ + Cl + 五 Ekgt-Vf(θt)∣∣2
t=0
Furthermore, we also have by stepsize condition
T-1	T-1	T-1
(k-2L) X	kθt+1	- θt∣2	≤	X	kθt+1	- θtlɪ (Ct+δI)-3 LI	≤ △ +	CI	+	2L	X	kgt	- Vf (θt )k2
0	t=0	t=0	t	t=0
since δI	Ct + δI. From above inequality, we obtain
T-1	T-1
X kθt+1 -θtk22 ≤ H1+H2 X kgt -Vf(θt)k22	(32)
t=0	t=0
where the constants H1 and H2 are defined as
19
Under review as a conference paper at ICLR 2021
Our goal is to bound the distance between the zero vector and subdifferential set of F, so we have
dist(0,bF (θt+1))2
=∣∣(1 -	ρt)gt	- Vf (θt+ι)	+ Ptmt-1 + O-(Ct + δI)(θt+ι - θt)I I 2
=U (1 —	Pt)gt	— Vf (θt+ι)	+ Ptmt-I + (θt+ι — θt) + — (Ct + δI )(θt+ι	— θt) —	(θt+ι	— θt) | L
≤ 3∣∣(1 — ρt)gt — Vf(θt+ι) + Ptmt-I + (θt+ι — θt)| |?
+ 3 | | 0-(Ct + δi )(θt+ι — θt) | | 2 + 3∣∣(θt+1 — θt) | | 2
≤ 3∣∣(1 — ρt)gt — Vf (θt+ι) + Ptmt-I + (θt+ι — .)||： +3(今 + 1) ∣*ι —叼2
Ti
Here, we assume that
λmax( — (Ct + δI)) ≤ Y
which yields our Condition (C-4)
λmin(θt(Ct + δI厂1) ≥ Y
From Eq. (31), we have
((1 — Pt) gt — V f (θt) + Ptmt-1,θt+1 — θt) + Hθt+1 — θt∣∣2^ (C +δI)-LI ≤ F (θt) — F (θt+1)
20t '『	2
which can be re-written as
((1 — Pt )gt — Vf (θt+1) + Ptmt-1,θt+1 — θt^
≤ F(θt)	—	F(θt+1)	— <Vf(θt+1) — Vf (θt),θt+1	—	θt)	—	Hθt+1 —	θt k"(Ct+δi)-LI
≤ F(θt)	—	F(θt+1)	—〈Vf (θt+1) — Vf(θt), θt+1	—	θt)	+	(2—- -	2) kθt+1 — θtk2
since we have the condition 含 ≥ 3 L. Therefore,we obtain
T1 = ∣∣(1 — Pt)gt — Vf (θt+1) + Ptmt-1k 2 + ∣∣θt+1 — θtk2
+ 2((1 — Pt)gt — Vf (θt+1) + Ptmt-1,θt+1 — θt^
≤ k(1 — Pt)gt — Vf(θt) + Vf(θt) — Vf(θt+1) + Ptmt-1∣2 + ∣∣θt+1 — θt∣2
+ F(θt) — F(θt+1) — <Vf(θt+1) — Vf (θt),θt+1 — θt) + (2—- - 2) ∣θt+1 — θt∣2
≤ 4IIgt- Vf (θt)∣∣2 + 4L2 ∣θt+1 - θt∣∣2 + 4∣∣Ptmt-I ∣∣2 + 4∣∣PtgtIl2 + ∣∣θt+1 - θt∣∣2
+ F(θt)- F(θt+1)+ L∣θt+1- θt∣∣2+ (ɔ—万)| | θt+1- θt∣∣2
200	2	1
≤ F(θt) - F(θt+1) + 4p0/(tT)G2 + 4P.2(t-1)G2
+ (2—■ + 2 +1 + 4L2) ∣∣θt+1 - θt∣∣2 +4∣Igt - Vf (θt)∣∣2
Therefore, we have the distance as
dist(0,bF (θt+1))2
≤ 3(F(θt) — F(θt+1) + 8ρ0μ2(t-1)G2 + ( 2―- + 2 + 2 +4L2 + a)∣%1 — θt∣2 + 4∣gt — Vf (θt)∣2
、J—
20
Under review as a conference paper at ICLR 2021
Therefore, we have
E[dist(0,bF(θa))2] ≤
-Vf (θt+ι) + ptmt-1 + O-(Ct + δI)(θt+ι - θt)∣∣2i
3	8 2G2 T-1	T-1
≤ τ(δ+	i ： 2	+4X	Ilgt- vf(θt)k2	+ C2 X kθt+ι-θtk2)
τ ∖	1 μ	/
t=0	t=0
T (∆+8⅛
≤
T-1	T-1
+4X Igt -Vf(θt)I22+C2(H1+H2 X Igt -Vf(θt)I22
t=0	t=0
≤
T-1
竽 X E[kgt -Vf (θt)k2] + 罕 + QT3
t=0
where
Q1 =4+C2H2,
C 3 I	3C2	C	24ρ2G2 I	3C1C2
Q2 = 3 + δ 行，Q3 = i _ 22 + δ 行
200 — 2 L	1 - μ	200 — 2 L
Note that the constants Qi, Q2, and Q3 depend on {αo,δ, L,D,G,po, μ, γ}, but not on T. The third inequality
comes from Eq. (32). If we assume the stochastic gradient gt is evaluated on the minibatch St with |St | = bt,
then we can obtain using Condition (C-2)
bt	2
kgt — Vf (θt)k2 = Eξ [∣b X Vf (θt; ξit) — Vf (θt)∣∣2]
t i=1
1	bt
=b2E [HX{Vf (θt; ξit) — Vf (θt)}∣∣2]
t	i=1
1 bt
≤ b ∑E[kVf (θt; ξit) -Vf (θt)k2] ≤
t it=1
1 σ2
bt
where it represents the random variable for each datapoint in minibatch samples St. Finally, we arrive at our
Theorem 1 as
ER[dist(0,bF(Θr))2] ≤ QT' £ b1 + QTδ + QT3
t=0 t
21