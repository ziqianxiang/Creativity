Under review as a conference paper at ICLR 2021
Approximating Pareto Frontier through
Bayesian-optimization-directed Robust Multi-
objective Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
Many real-word decision or control problems involve multiple conflicting objec-
tives and uncertainties, which requires learned policies are not only Pareto optimal
but also robust. In this paper, we proposed a novel algorithm to approximate a repre-
sentation for robust Pareto frontier through Bayesian-optimization-directed robust
multi-objective reinforcement learning (BRMORL). Firstly, environmental uncer-
tainty is modeled as an adversarial agent over the entire space of preferences by
incorporating zero-sum game into multi-objective reinforcement learning (MORL).
Secondly, a comprehensive metric based on hypervolume and information entropy
is presented to evaluate convergence, diversity and evenness of the distribution for
Pareto solutions. Thirdly, the agent’s learning process is regarded as a black-box,
and the comprehensive metric we proposed is computed after each episode of
training, then a Bayesian optimization (BO) algorithm is adopted to guide the
agent to evolve towards improving the quality of the approximated Pareto frontier.
Finally, we demonstrate the effectiveness of proposed approach on challenging
multi-objective tasks across four environments, and show our scheme can produce
robust policies under environmental uncertainty.
1	Introduction
Reinforcement learning (RL) algorithm has demonstrated its worth in a series of challenging se-
quential decision making and control tasks, which train policies to optimize a single scalar reward
function (Mnih et al., 2015; Silver et al., 2016; Haarnoja et al., 2018; Hwangbo et al., 2019). However,
many real-world tasks are characterized by multiple competing objectives whose relative impor-
tance (preferences) is ambiguous in most cases. Moreover, uncertainty or perturbation caused by
environment dynamic change, is inevitable in real-world scenarios, which may result in lowered
agent performance (Pinto et al., 2017; Ji et al., 2018). For instance, autonomous electric vehicle
requires trading off transport efficiency and electricity consumption while considering environmental
uncertainty (e.g., vehicle mass, tire pressure and road conditions might vary over time). Consider a
decision-making problem for traffic mode, as shown in Figure 1. A practitioner or a rule is responsible
for picking the appropriate preference among time and cost, and the agent need to determine different
policies depending on the chosen trade-off between these two metrics. Whereas, the environment
contain uncertainty factors related to actions of other agents or to dynamic changes of Nature, which
may lead to more randomness in these two metrics, and makes multi-objective decision-making or
control more challenging. If weather factors are taken into account, e.g., heavy rain may cause traffic
congestion, which can increase the time and cost of the plan-A, but it not have a significant impact on
the two metrics of the plan-B. From this perspective, selecting plan-B is more robust, i.e., a policy is
said to be robust if its capability to obtain utility is relatively stable under environmental changes.
Therefore, preference and uncertainty jointly affect the decision-making behavior of the agent.
In traditional multi-objective reinforcement learning (MORL), one popular way is scalarization,
which is to convert the multi-objective reward vector into a single scalar reward through various
techniques (e.g., by taking a convex combination), and then adopt standard RL algorithms to op-
timize this scalar reward (Vamplew et al., 2011). Unfortunately, it is very tricky to determine an
appropriate scalarization, because often common approach only learn an ’average’ policy over the
space of preferences (Yang et al., 2019), or though the obtained policies can be relatively quickly
1
Under review as a conference paper at ICLR 2021
Figure 1: Diagram of decision-making problem for traffic mode. If time is crucial, the agent tend to
choose plan-A that takes less time, but it costs more. On the other hand, if cost is more important
matters, the agent will be inclined to select plan-B that requires less cost, but it takes more time.
adapted to different preferences between performance objectives but are not necessarily optimal.
Furthermore, these methods almost did not take into account the robustness of the policies under
different preferences, which means the agent cannot learn robust Pareto optimal policies.
In this work, we propose a novel approach to approximate well-distributed robust Pareto frontier
through BRMORL. This allows our trained single network model to produce the robust Pareto optimal
policy for any specified preference, i.e., the learned policy is not only robust to uncertainty (e.g.,
random disturbance and environmental change) but also Pareto optimal under different preference
conditions. Our algorithm is based on three key ideas, which are also the main contributions of
this paper: (1) present a generalized robust MORL framework through modelling uncertainty as
an adversarial agent; (2) inspired by Shannon-Wiener diversity index, a novel metric is presented
to evaluate diversity and evenness of distribution for Pareto solutions. In addition, combined with
hypervolume indicator, a comprehensive metric is designed, which can evaluate the convergence,
diversity and evenness for the solutions on the approximated Pareto frontier; (3) regard agent’s
learning process in each episode as a black-box, and BO algorithm is used to guide agent to evolve
towards improving the quality of the Pareto set. Finally, we demonstrate our proposed algorithm
outperform competitive baselines on multi-objective tasks across several MuJoCo (Todorov et al.,
2012) environments and SUMO (Simulation of Urban Mobility) (Lopez et al., 2018), and show our
approach can produce robust policies under environmental uncertainty.
2	Related work
2.1	Multi-objective reinforcement learning
MORL algorithms can be roughly classified into two main categories: single-policy approaches
and multiple-policy approaches (Roijers et al., 2013; Liu et al., 2014). Single-policy methods seek
to find the optimal policy for a given preference among multiple competing objectives. These
approaches convert the multi-objective problem into a single-objective problem through different
forms of scalarization, including linear and non-linear ones (Mannor & Shimkin, 2002; Tesauro et al.,
2008). The main advantage of scalarization is its simplicity, which can be integrated into single-policy
scheme with very little modification. However, the main drawback of these approaches is that the
preference among the objectives must be set in advance.
Multi-policy methods aim to learn a set of policies that approximate Pareto frontier under different
preference conditions. The most common approaches repeatedly call a single-policy scheme with
different preferences (Natarajan & Tadepalli, 2005; Van Moffaert et al., 2013; Zuluaga et al., 2016).
Other methods learn a set of policies simultaneously via using a multi-objective extended version
of value-based RL (Barrett & Narayanan, 2008; Castelletti et al., 2012; Van Moffaert & Nowe,
2014; Mossalam et al., 2016; Nottingham et al., 2019) or via modifying policy-based RL as a MORL
variant (Pirotta et al., 2015; Parisi et al., 2017; Abdolmaleki et al., 2020; Xu et al., 2020). Nevertheless,
most of these methods are offen constrained to convex regions of the Pareto front and explicitly
maintain sets of policies, which may prevent these schemes from finding the sets of well-distributed
Pareto solutions which can represent different preferences. There are also meta-policy methods,
which can be relatively quickly adapted to different preferences (Chen et al., 2018; Abels et al., 2019;
Yang et al., 2019). Although the above works were successful to some extent, these approaches share
the same shortcomings that no attention is paid to the robustness of Pareto-optimal policy over the
2
Under review as a conference paper at ICLR 2021
entire space of preferences. In addition, most approaches still focus on the domains with discrete
action space. In contrast, our scheme can guarantee the learned policies is approximately robust
Pareto-optimal on continuous control tasks.
2.2	Robust reinforcement learning
Robust reinforcement learning (RRL) algorithms can be broadly grouped into three distinct meth-
ods (Derman et al., 2020). The first approach focuses on solving robust Markov decision process
(MDP) with rectangular uncertainty sets. Some researches proposed RRL algorithms for learning
optimal policies using coupled uncertainty sets (Mannor et al., 2012). Other works modeled an
ambiguous linear function of a factor matrix as a selection setting from an uncertainty set (Goyal
& Grand-Clement, 2018). The second RRL approach considered a distribution over the uncertainty
set to mitigate the conservativeness. Yu & Xu (2015) presented the distributional RRL method
by supposing the uncertain parameters are random variables following an unknown distribution.
Tirinzoni et al. (2018) proposed a RRL scheme using conditioned probability distribution that de-
fines uncertainty sets. A third RRL method mostly concerns adversarial setting in RL. Pinto et al.
(2017) developed a robust adversarial reinforcement learning (RARL) scheme through modeling
uncertainties via adversarial agent which applies disturbances to the system. Tessler et al. (2019)
proposed an adversarial RRL framework through structuring probabilistic action robust MDP and
noisy action robust MDP. Nonetheless, these researches do not take into account the connection
between Pareto-optimal policy and robust policy, which leaves room for improving the performance
of them in practical applications. In contrast, our scheme can learn robust Pareto-optimal policies
through modeling uncertainty as an adversary over the entire space of preferences.
3	Background
3.1	Multi-objective Markov decision process
In this work, we consider a MORL problem defined by a multi-objective Markov decision process
(MOMDP), which is represented by the tuple(S, A, P, R,γ, Ω,UΩi With state space S, action space
A, state transition probability P(s0|s, a), vector reward function R(s, a) = [r1, ..., rk]T, the space
of preferences Ω, and preference functions, e.g., Uω (R) which produces an utility function using
preference ω ∈ Ω, and a discount factor Y ∈ [1,0). In MOMDP, a policy ∏ is associated with
a vector of expected returns Qπ(s, a) = [Q1π, ..., Qkπ]T, where the action-value function of π for
objective k can be represented as Qkπ(s, a) = Eπ [Pt γtrk(st, at)|s0 = s, a0 = a]. For MOMDP, a
set of non-dominated policies is called as the Pareto frontier.
Definition 1. A policy π1 Pareto dominates another policy π2, i.e., π1	π2 when
∃i : Qiπ1 (s, a) > Qiπ2(s,a)∧∀j 6=i : Qjπ1 (s, a) > Qjπ2 (s, a).
Definition 2. A policy π is Pareto optimal if and only if it is non-dominated by any other policies.
3.2	Two-person zero-sum games
In standard two-person zero-sum games, players have opposite goals—the payoff of a player equals
the loss of the opponent (Mazalov, 2014), i.e., V + V = 0, where V and V are payoff of a player and
the opponent, respectively.
For two player discounted zero-sum Markov game, assuming protagonist is playing policy π and
adversary is playing the policy π, transition kernel P(s0∣s, a, a) depend on both players. In the game,
the value function based on ∏ and ∏ can be represented as vπ,π(S) ≡ Eπ,π [P∞=o Ytr(st, at, at) |
s0 = s], ∀s ∈ S. Each player chooses his policy regardless of the opponent. Protagonist attempts to
maximize the value function (i.e., total expected discounted reward), and adversary seeks to minimize
this function.
Nash equilibrium is a key role in game theory, which is one kind of game solution concept. A Nash
equilibrium (∏*,∏*) in zero-sum Markov game exists when the following relation holds (Shapley,
3
Under review as a conference paper at ICLR 2021
1953; Basar & Olsder, 1998):
∞
V * (S) = max min Eπ,π[X Yt r (St ,at ,西)| s0 = s]	(1)
π π	/ J
t=0
∞
=min max Eπ,π [X Yt r (St ,at ,诙)| s0 = s],	(2)
π π
t=0
where π* and ∏* are the optimal policies of protagonist and adversary respectively, V* is optimal
equilibrium value of the game. In such a situation, neither player can improve their respective returns,
and there is an important relation., i.e., ∀π, π, Vπ,π* ≤ V* ≤ Vπ*,π.
4	Bayesian-optimization-directed robust MORL
4.1	Overview
We propose a generalized robust MORL framework to learn a single parametric representation for
robust Pareto optimal policy over the space of preferences (see Algorithm 1 for implementation
scheme based on DDPG). The optimization process of our proposed approach is illustrated in Figure 2.
Bayesian model based on Gaussian process is adopted to predict the Pareto quality and estimate
the model uncertainty. Then, using the Bayesian model, acquisition function (Frazier, 2018) can
determine optimal guess point, which is the suggested preference in our task. In order to prevent the
policy from falling into local optimum, some preferences is randomly sampled from replay buffer,
which guide the training of the agent together with the preferences from BO. In addition, the policy
of the adversary evolves in the opposite direction to the policy of the protagonist in each preference.
In Sections 4.2 and 4.3, through incorporating zero-sum game into MORL, environmental uncertainty
is modeled as an adversarial agent. This means that the protagonist needs to learn Pareto optimal
policy under attack from the adversary. In Section 4.4, inspired by Shannon-Wiener diversity index,
a novel metric for Pareto quality is presented to evaluate the distribution of Pareto solutions from
diversity and evenness. Moreover, combined with hypervolume index, a comprehensive metric is
designed, which can evaluate the convergence, diversity and evenness for solutions in Pareto set.
In Section 4.5, regard agent,s learning process as a black-box, and the comprehensive metric for
the approximated Pareto frontier is computed after each episode of training, then BO algorithm is
adopted to guide the protagonist to evolve towards improving the Pareto quality (i.e., maximizing the
comprehensive metric).
Obj ective-1	Obj ective-1
Figure 2: Illustration for process to approximate well-distributed robust Pareto frontier through the
proposed algorithm.
4.2	Robust multi-objective MDP
In this section, we propose a robust multi-objective MDP (RMO-MDP), which considers both
the Pareto optimality and robustness for the learned policies. Probabilistic action robust MDP
(PR-MDP) (Tessler et al., 2019) is adopted to improve the robustness of the policies, which can be
4
Under review as a conference paper at ICLR 2021
O
rslbAwwqo
Objective-I
Figure 3: Illustration of the mismatch between
the Pareto optimal solution and the correspond-
ing preference. Suppose the point A represents
a Pareto optimal solution, which and the origin
form the vector O~A. The corresponding prefer-
ence vector can be represented by the vector OB .
In most cases, O~A is not parallel to O~B .
Figure 4: Quality analysis of Pareto frontiers.
The Pareto frontiers 1, 2 and 3 are approximated
by different approaches. The green, blue and
purple points represent the solutions on Pareto
frontiers 1, 2 and 3 respectively. The hypervol-
ume formed by the solutions on Pareto front 2 and
the reference point O is the blue shaded region.
regarded as a special zero-sum game between a protagonist and an adversary. We refer to the optimal
policies of the protagonist as robust Pareto-optimal policies in RMO-MDP, which the difference from
the MOMDP is that the action space here includes not only the actions of the protagonist, but also the
actions of the adversary with a certain probability.
Definition 3. A RMO-MDP can be defined by the tuple〈S, Amix, P, R, γ,Ω, Uω i. Amix is the
mixed action space. The mixed policy πmix (π, ∏) is defined as πmix (amix | s, ω) ≡ (1 — α)π(a |
s, ω) + ατr(a | s, ω), ∀s ∈ S and α ∈ [0,1]. π and π are policies the players can take, and
amix 〜∏mix(∏(s),π(s)).
In this work, in order to improve the quality of the approximated Pareto frontier, the scalar utility
function Uω is designed as non-linear combinations of objectives:
Uω (s, amix, ω) = ω | Qnmixg* * * * * * n) (s, amix, ω) + kM (s, amix, ω),	(3)
M (s,amix, ω) =	Ss,amix, ω)	—	,	(4)
L 、)	kQ(s,amix,ω)∣∣2	∣∣ω∣∣2 2，	L
Qnmixgn) (s, amix, ω) = (1 — α)Q(s, a, ω) + αQ(s, α, ω),	(5)
where M(s, amix, ω) is a metric, which can evaluate the mismatch between the Pareto optimal
solution and the corresponding preference. Figure 3 illustrates metric function M(s, amix, ω) in
more detail. The distribution of solutions on the Pareto front can be more well-distributed through
optimizing function M(s, amix, ω). k is a coefficient can adjust the role of M(s, amix, ω) in the
utility function 3. For a protagonist, k is a negative, and k is positive for an adversary. This means
that the policy with higher preference is more likely to be violently attacked by an adversary, which
can makes the policy with higher preference stronger robust.
Under the condition of adversary attack, the utility value of protagonist’s policy can be defined as
v∏ ≡ minn Enmix(n,n) [Uω (s, amix, ω)]. Therefore, the robust Pareto optimal policy is optimal policy
in RMO-MDP, which can be represent as:
∏α ∈ arg max min Enmix(n,n)10。(s, amix,④)].
n n
(6)
The complexity of greedy solution to finding the Nash equilibria policies is exponential in the
cardinality of the action space, which makes it unworkable in most cases (Schulman et al., 2015).
In addition, most two player discounted zero-sum Markov game methods require solving for the
equilibrium policy of a minimax action-value function at each iteration. This is a typically intractable
optimization problem (Pinto et al., 2017). Instead, we focus on approximating equilibrium solution
to avoid this tricky optimization.
5
Under review as a conference paper at ICLR 2021
4.3	Policy iteration for RMO-MDP
In this section, we present a policy iteration (PI) approach for solving RMO-MDP called robust
multi-objective PI (RMO-PI). RMO-PI algorithm can decompose the RMO-MDP problem into two
sub-problems (policy evaluation and policy improvement) and iterate until convergence.
4.3.1	Robust multi-objective policy evaluation
In this stage, the vectorized Q-function is learned to evaluate the policy π of the protagonist. With
Equation 5, we define the target vectorized Q-function as:
y = Enmix [R + YQnmix(Cn) (s0, amix, ω; φ-)]
=Es0,amix,ω {R + Y[(I- α)Q(s,a, 3； φ-) + αQ(s, a, 3； φ-)]},
Then, we minimize the following loss function at each step:
L1(φ) = Enαmix hkyωrb - Q(s, a, 3rb； φ)k22 + kyωbo - Q(s, a, 3bo； φ)k22i ,
⑺
(8)
where φ and φ- are the parameters of the Q-function network and the target Q-function network,
3rb and 3bo are obtained from replay buffer and Bayesian-optimization, yωrb and yωbo represent
y(s0, amix, 3rb) and y(s0, amix, 3bo), respectively. In order to improve the smoothness of the
landscape of loss function, the auxiliary loss setting is used (Yang et al., 2019):
L2(φ) = Enmixhk3|by3rb - 3|bQ(s,a, ωrb; φ)k2 + k3Lyωbo - 3|oQ(s,a, 3bo； O)lg ∙⑼
The final loss function can be written as: L(φ) = (1 - β)L1(φ) + βL2(φ), where β is a weighting
coefficient to trade off between losses L1(φ) and L2(φ).
4.3.2	Robust multi-objective policy improvement
In RMO-PI, policy improvement refers to optimizing and updating the policies of a protagonist
and an adversary for the given utility function. RMO-PI optimizes both of the agents through the
following alternating process. In the first stage, the policy of protagonist is learned while holding
the adversary’s policy fixed. In the second stage, the policy of protagonist is held constant and the
adversary’s policy is learned. This learning sequence is repeated until convergence.
The protagonist seeks to maximize the utility function Uω, and then the policy gradient can be
represented as: VθLn = VθLωrb + VθLτω>bo, Where
VθLωrb ≈ Enmix [(1 -α)Vaω∣bQ(s, a, 3加 φ)Vθπ(s, 3加 θ) + kV“M(s, a, ω,b)Vθπ(s, 3加 θ)],
(10)
VθLωbo ≈ Enmix [(1 -α)Vaω∣oQ(s, a, ωbo; φ)Vθπ(s, ωbo; θ) + kVaM(s, a, ωbo)Vθπ(s, ωbo; θ)],
(11)
θ is the model parameters of the protagonist.
Next, the adversary tries to minimize the utility function Uω, and the policy gradient can be written
as: VaLn = VeLlrb + %Lb。, where
'θLωrb ≈ Enmix [aVa3|bQ(S, a, 3rb； φ) Va∏(s, 3rb； θ) + kVaM(s, Z 3rb)Vθ∏(s, 3rb；「)],
(12)
VθLlbo ≈ Enmix [αVɑωloQ(s, a, ωb°; φ)Vθ∏(s, ωb°; θ) + kVaM(s, Z ωb0)Vθ∏(s, ωb°; (θ)],
(13)
θ is the model parameters of the adversary. The derivation details of the policy gradients are available
in Appendix A.1.2.
4.4	Metrics for Pareto representation
Since the true Pareto set is intractable to obtain in complex problems, the goal of MORL is to find the
set of policies that best approximates the optimal Pareto front. Many researchers have reported the
works for quality metrics of Pareto front (Cheng et al., 2012; Parisi et al., 2017; Audet et al., 2018).
6
Under review as a conference paper at ICLR 2021
Hypervolume indicator is widely adopted to evaluate the quality of an approximated Pareto frontier,
which can measure the convergence and uniformity for the distribution of Pareto solutions (Zitzler
& Thiele, 1999; Xu et al., 2020). From our perspective, this indicator may be difficult to accurately
measure the uniformity of the Pareto solution distribution.
As shown in Figure 4, suppose the Pareto frontiers 1, 2 and 3 are obtained by different algorithms,
and compared with the Pareto frontiers 2 and 3, although the hypervolume metric formed by the
solutions on Pareto frontier 1 and the reference point O is optimal, the distribution of solutions on the
frontier 1 is not well-distributed, which makes the valid preferences of the practitioner or the agent to
choose is very limited. Moreover, imagine the solutions on Pareto frontier 1 are very close to each
other or even overlap into one solution. At this time, if we adopt the metric (integrated hypervolume
metric and sparsity metric) proposed in the paper (XU et al., 2020) to measure the quality of Pareto
frontier 1, the result to have high hypervolume and low sparsity is very ideal. However, such Pareto
frontier 1 might not satisfy the needs of the practitioner or the agent. In a word, the high quality
of the approximated Pareto frontier is expected to have high hypervolume, and the distribution of
solutions is well-distributed. Therefore, in this section, we proposed a novel metric for quality of the
approximated Pareto frontier through combining hypervolume metric and evenness metric.
Inspired by Shannon-Wiener diversity index, the diversity metric for the solutions of the Pareto
frontier can be expressed as D(P) = - P [pi ln (pi)], where P represents the solutions of the Pareto
frontier, and pi is the proportion of the number of non-dominated solutions in the corresponding
solution interval to the total number of the solutions on Pareto frontier. The expected diversity
of Pareto set Dmax can be defined as ln(Sn), and Sn is the number of solution intervals. Then,
our evenness metric E(P ) can be represented as D(P)/Dmax. For example, in Figure 4, Sn =
6, and the evenness metrics for the distribution of the solutions on the Pareto frontiers 1, 2 and 3
are approximately equal to 0.37, 1 and 0.56, respectively. Hence, we can get the following two
inferences.
Proposition 1. As E(P) and Sn increases, the distribution of solutions in Pareto set becomes denser
and more uniform, and the Pareto frontier becomes more continuous.
Proposition 2. The Pareto frontier is continuous as E(P) = 1 and Sn → ∞.
Combined with the hypervolume indicator H(P), we propose a comprehensive metric I(P) that can
measure the convergence, diversity and evenness of the solutions:
I(P) = H(P)(1 + λE(P)),	(14)
where λ is a weight coefficient.
4.5	Bayesian-optimization-directed Pareto representation improvement
In this Section, in order to further improve the representation of the approximated Pareto frontier, the
agent’s learning process is regarded as a black-box, and the comprehensive metric I(P ) is computed
after each episode of training, then a BO algorithm is adopted to guide the protagonist to evolve
towards maximizing the proposed metric I(P). As shown in Figure 5, the Pareto representation
improvement scheme based on BO-directed is illustrated. The value of the objective function f (Ω)
equals the value of the comprehensive metric I(P), which is obtaind after each episode of training.
In addition, suggested preferences from BO algorithm and sampled preferences from replay buffer
are simultaneously used to guide the learning process, which is to avoid the algorithm into a local
optimum. The scheme to guide the learning process with BO has high universality for Pareto quality
improvement, and does not require much expert experience in the selection of prediction models.
5	Experiments
In order to benchmark our proposed scheme, We develop two MORL environments with continuous
action space based on SUMO and Swimmer-v2. Moreover, we also adopt HalfCheetah-v2 and
Walker2d-v2, which are two MORL domains provided by Xu et al. (2020). The goal of all tasks is to
try to optimize the speed of the agent while minimizing energy consumption. The observation and
action space settings are shown in Table 1. The more details can be found in Appendix A.2.
7
Under review as a conference paper at ICLR 2021
D —►
Training Data
Ω —►
Candidate Set
Surrogate Model	——►	Posterior	——►	Acquisition Function
Gaussian Process		M∕(ω)∣p)		Expected Improvement
^►4(Q)
Utility of the
Candidate Set
Figure 5: Pareto representation improvement scheme based on BO algorithm. The surrogate model
for the objective function f (Ω) is typically a GauSSian Process. PoSteriorS represent the confidence a
model has about the function values at a point or set of points. Acquisition function is employed to
evaluate the usefulness of optimal guess point corresponding to posterior distribution over f (Ω). The
expected improvement method chosen to design the acquisition function in our scheme.
Table 1: Observation space and action space of the experiment environments.
SUMO Swimmer-v2 HalfCheetah-v2 Walker2d-v2
Observation Space S ∈ R16	S ∈ R8	S ∈ R17	S ∈ R17
Action Space A ∈ R1	A ∈ R2	A ∈ R6	A ∈ R6
Our algorithm is implemented based on Deep Deterministic Policy Gradient (DDPG) (Lillicrap et al.,
2015) framework. In principle, our scheme can be combined with any RL method, regardless of
whether it is off-policy or on-policy. Moreover, we implement three baseline methods for comparison
and ablation analysis: SMORL represents a MO-DDPG method based on linear scalarization function,
which is a linear combination of rewards in the form of a preference; SRMORL is a RMO-DDPG
approach using linear scalarization function; RMORL represents a RMO-DDPG approach with the
utility function Uω. BRMORL is a RMO-DDPG scheme combined with the utility function Uω and
BO algorithm. More details about the algorithms are described in Appendix A.1.1.
Figure 6 and 7 show the learning curves and Pareto frontiers comparison results on SUMO and
Swimmer-v2 respectively. Moreover, the results in Table 2 and 3 demonstrate that our proposed
BRMORL scheme outperforms all the baseline methods on SUMO and Swimmer-v2 environments in
hypervolume and evenness. It can also be found from Figure 7(c) the BRMORL mothod is not only
able to find solutions on the convex portions of the Pareto frontier, but also the concave portions.
Figure 6: The learning curves and the Pareto frontiers obtained by different algorithms on SUMO.
Table 2:	Training results on SUMO.
Hypervolume Evenness
SMORL 4547.43 ± 1919.14 0.45 ± 0.30
SRMORL 4904.25 ± 2480.33 0.42 ± 0.37
RMORL 5900.67 ± 2443.58 0.67 ± 0.36
BRMORL 6219.57 ± 2164.15 0.81 ± 0.24
Table 3:	Training results on Swimmer-v2.
Hypervolume Evenness
SMORL 3045.63 ± 2546.65 0.34 ± 0.33
SRMORL 4164.11 ± 1487.67 0.20 ± 0.18
RMORL 7682.44 ± 4844.27 0.35 ± 0.25
BRMORL 8118.58 ± 4344.80 0.74 ± 0.14
Figure 8 illustrates that the robustness of different policy models under the preference=[0.5,0.5], on
Swimmer-v2 domain. We test with jointly varying both mass and disturbance probability. Obviously,
the capability to obtain return based on BRMORL approach is less affected by environmental changes
than other schemes. Moreover, the standard deviation based on the utility of the policy is adopted to
8
Under review as a conference paper at ICLR 2021
quantify the robustness. ThiS means that the stronger the robustness of a policy is, then the smaller its
standard deviation is. Table 4 shows the quantitative analysis results of robustness under different
preferences and environmental changes, on SWimmer-v2. For more results and implementation
details, please refer to Appendix A.3 and A.4.1.
Figure 7: The learning curves and the Pareto frontiers obtained by different methods on SWimmer-v2.
Figure 8: Robustness to environmental uncertainty. Disturbance probability represents the probability
of a random disturbance being played instead of the selected action. Relative mass denotes the ratio
of the current agent,s mass to its original mass.
In Table 5 and 7, we compare our BRMORL scheme with state-of-the-art baseline (PG-
MORL) provided by XU et al. (2020). Although our method is not superior in hy-
pervolume, it outperforms the baseline in evenness, robustness and utility. In this sec-
tion, the utility is defined as the expectation of return based on a policy under envi-
ronmental changes. More details and results can be found in Appendix A.3 and A.4.2.
Table 4: Quantitative analysis results for robustness.						Table 5: TeSt results on WaIker2d-v2.		
	[0.1,0.9]	[0.3,0.7]	[0.5,0.5]	[0.7,0.3]	[0.9,0.1]		PGMORL	BRMORL
SMORL	8.96	9.55	4.61	9.58	16.48	Hypervolume	57132.70	30737.01
SRMORL	0.44	1.01	3.07	7.88	15.86	Evenness	0.28	0.32
RMORL	1.99	0.59	3.11	8.45	14.36	Robustness	34.91	14.53
BRMORL	0.28	0.94	2.51	4.62	7.93	Utility	-193.86	-11.63
6 Conclusion and discussion
In this paper, we proposed a generalized robust MORL framework to approximate a representation
for robust Pareto frontier, which allows our trained single model to produce the robust Pareto optimal
policy for any specified preference.
Our experiments across four different domains demonstrate that our scheme is effective and advanced.
Most importantly, we note that training with appropriate adversarial setting can not only result in
robust policies, but also improve the performance even. Moreover, both solutions on convex and
concave portions of the Pareto frontier can be found through our approach. Although our scheme
cannot guarantee the learned policy is optimal, it is approximately robust Pareto optimal.
9
Under review as a conference paper at ICLR 2021
References
Abbas Abdolmaleki, Sandy H Huang, Leonard Hasenclever, Michael Neunert, H Francis Song,
Martina Zambelli, Murilo F Martins, Nicolas Heess, Raia Hadsell, and Martin Riedmiller. A
distributional view on multi-objective policy optimization. arXiv preprint arXiv:2005.07513, 2020.
AXel Abels, Diederik Roijers, Tom Lenaerts, Ann Nowe, and Denis Steckelmacher. Dynamic weights
in multi-objective deep reinforcement learning. In International Conference on Machine Learning,
pp.11-20. PMLR, 2019.
Charles Audet, J Bigeon, D Cartier, SebaStien Le Digabel, and Ludovic Salomon. Performance
indicators in multiobjective optimization. Optimization Online, 2018.
Leon Barrett and Srini Narayanan. Learning all optimal policies with multiple criteria. In Proceedings
of the 25th international conference on Machine learning, pp. 41-47, 2008.
Tamer BaSar and Geert Jan Olsder. Dynamic noncooperative game theory. SIAM, 1998.
Andrea Castelletti, Francesca Pianosi, and Marcello Restelli. Tree-based fitted q-iteration for multi-
objective markov decision problems. In The 2012 International Joint Conference on Neural
Networks (IJCNN), pp. 1-8. IEEE, 2012.
Xi Chen, Ali Ghadirzadeh, Marten Bjorkman, and Patric Jensfelt. Meta-learning for multi-objective
reinforcement learning. arXiv preprint arXiv:1811.03376, 2018.
Shi Cheng, Yuhui Shi, and Quande Qin. On the performance metrics of multiobjective optimization.
In International Conference in Swarm Intelligence, pp. 504-512. Springer, 2012.
Esther Derman, Daniel Mankowitz, Timothy Mann, and Shie Mannor. A bayesian approach to robust
reinforcement learning. In Uncertainty in Artificial Intelligence, pp. 648-658. PMLR, 2020.
Peter I Frazier. A tutorial on bayesian optimization. arXiv preprint arXiv:1807.02811, 2018.
Vineet Goyal and Julien Grand-Clement. Robust markov decision process: Beyond rectangularity.
arXiv preprint arXiv:1811.00215, 2018.
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and
applications. arXiv preprint arXiv:1812.05905, 2018.
Jemin Hwangbo, Joonho Lee, AleXey Dosovitskiy, Dario Bellicoso, Vassilios Tsounis, Vladlen
Koltun, and Marco Hutter. Learning agile and dynamic motor skills for legged robots. Science
Robotics, 4(26), 2019.
Xuewu Ji, Xiangkun He, Chen Lv, Yahui Liu, and Jian Wu. Adaptive-neural-network-based robust
lateral motion control for autonomous vehicle at driving limits. Control Engineering Practice, 76:
41-53, 2018.
Timothy P Lillicrap, Jonathan J Hunt, AleXander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Chunming Liu, Xin Xu, and Dewen Hu. Multiobjective reinforcement learning: A comprehensive
overview. IEEE Transactions on Systems, Man, and Cybernetics: Systems, 45(3):385-398, 2014.
Pablo Alvarez Lopez, Michael Behrisch, Laura Bieker-Walz, Jakob Erdmann, Yun-Pang Flotterod,
Robert Hilbrich, Leonhard Lucken, Johannes Rummel, Peter Wagner, and Evamarie Wieβner. Mi-
croscopic traffic simulation using sumo. In The 21st IEEE International Conference on Intelligent
Transportation Systems. IEEE, 2018. URL https://elib.dlr.de/124092/.
Shie Mannor and Nahum Shimkin. The steering approach for multi-criteria reinforcement learning.
In Advances in Neural Information Processing Systems, pp. 1563-1570, 2002.
Shie Mannor, Ofir Mebel, and Huan Xu. Lightning does not strike twice: Robust mdps with coupled
uncertainty. arXiv preprint arXiv:1206.4643, 2012.
10
Under review as a conference paper at ICLR 2021
Vladimir Mazalov. Mathematical game theory and applications. John Wiley & Sons, 2014.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. nature, 518(7540):529-533, 2015.
Hossam Mossalam, Yannis M Assael, Diederik M Roijers, and Shimon Whiteson. Multi-objective
deep reinforcement learning. arXiv preprint arXiv:1610.02707, 2016.
Sriraam Natarajan and Prasad Tadepalli. Dynamic preferences in multi-criteria reinforcement learning.
In Proceedings of the 22nd international conference on Machine learning, pp. 601-608, 2005.
Kolby Nottingham, Anand Balakrishnan, Jyotirmoy Deshmukh, Connor Christopherson, and David
Wingate. Using logical specifications of objectives in multi-objective reinforcement learning. arXiv
preprint arXiv:1910.01723, 2019.
Simone Parisi, Matteo Pirotta, and Jan Peters. Manifold-based multi-objective policy search with
sample reuse. Neurocomputing, 263:3-14, 2017.
Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial reinforce-
ment learning. arXiv preprint arXiv:1703.02702, 2017.
Matteo Pirotta, Simone Parisi, and Marcello Restelli. Multi-objective reinforcement learning with
continuous pareto frontier approximation. In 29th AAAI Conference on Artificial Intelligence,
AAAI 2015 and the 27th Innovative Applications of Artificial Intelligence Conference, IAAI 2015,
pp. 2928-2934. AAAI Press, 2015.
Diederik M Roijers, Peter Vamplew, Shimon Whiteson, and Richard Dazeley. A survey of multi-
objective sequential decision-making. Journal of Artificial Intelligence Research, 48:67-113,
2013.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889-1897, 2015.
Lloyd S Shapley. Stochastic games. Proceedings of the national academy of sciences, 39(10):
1095-1100, 1953.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484-489, 2016.
Gerald Tesauro, Rajarshi Das, Hoi Chan, Jeffrey Kephart, David Levine, Freeman Rawson, and
Charles Lefurgy. Managing power consumption and performance of computing systems using
reinforcement learning. In Advances in Neural Information Processing Systems, pp. 1497-1504,
2008.
Chen Tessler, Yonathan Efroni, and Shie Mannor. Action robust reinforcement learning and applica-
tions in continuous control. arXiv preprint arXiv:1901.09184, 2019.
Andrea Tirinzoni, Marek Petrik, Xiangli Chen, and Brian Ziebart. Policy-conditioned uncertainty
sets for robust markov decision processes. In Advances in Neural Information Processing Systems,
pp. 8939-8949, 2018.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033.
IEEE, 2012.
Peter Vamplew, Richard Dazeley, Adam Berry, Rustam Issabekov, and Evan Dekker. Empirical
evaluation methods for multiobjective reinforcement learning algorithms. Machine learning, 84
(1-2):51-80, 2011.
Kristof Van Moffaert and Ann Nowe. Multi-objective reinforcement learning using sets of pareto
dominating policies. The Journal of Machine Learning Research, 15(1):3483-3512, 2014.
11
Under review as a conference paper at ICLR 2021
Kristof Van Moffaert, Madalina M Drugan, and Ann Nowe. Scalarized multi-objective reinforcement
learning: Novel design techniques. In 2013 IEEE Symposium on Adaptive Dynamic Programming
and Reinforcement Learning (ADPRL), pp. 191-199. IEEE, 2013.
Jie Xu, Yunsheng Tian, Pingchuan Ma, Daniela Rus, Shinjiro Sueda, and Wojciech Matusik.
Prediction-guided multi-objective reinforcement learning for continuous robot control. In Proceed-
ings of the 37th International Conference on Machine Learning, 2020.
Runzhe Yang, Xingyuan Sun, and Karthik Narasimhan. A generalized algorithm for multi-objective
reinforcement learning and policy adaptation. In Advances in Neural Information Processing
Systems, pp. 14636-14647, 2019.
Pengqian Yu and Huan Xu. Distributionally robust counterpart in markov decision processes. IEEE
Transactions on Automatic Control, 61(9):2538-2543, 2015.
Eckart Zitzler and Lothar Thiele. Multiobjective evolutionary algorithms: a comparative case study
and the strength pareto approach. IEEE transactions on Evolutionary Computation, 3(4):257-271,
1999.
Marcela Zuluaga, Andreas Krause, and Markus Puschel. ε-pal: an active learning approach to
the multi-objective optimization problem. The Journal of Machine Learning Research, 17(1):
3619-3650, 2016.
A Appendix
A.1 Algorithm
A.1.1 Algorithm overview
The details of the BRMORL, RMORL, SRMORL and SMORL schemes are provided in Algorithm 1,
2, 3 and 4 respectively.
12
Under review as a conference paper at ICLR 2021
Algorithm 1 Bayesian-optimization-directed robust multi-objective DDPG
Input: probability a, weighting coefficients β, λ,τ,k
Randomly initialize actor π(s, ω; θ), adversary ∏(s, ω; θ) and critic network Q(s,a, ω; φ)
Initialize target networks with weights θ-, G- and φ-
Initialize replay buffer B and comprehensive metric I
for episode = 0...M do
Receive initial state s0
Predict preference ωbo through Bayesian-optimization
ωbo V- fbo(I)
for t = 0...T do
if t 6 T/2 then
ωt V ωbo
else
Sample preference ωud based on uniform-distribution:
ωt V ωud
end if
π(st, ωt; θ) , w.p. (1 - α)
Sample action at =
πG(st , ωt ; θ) , otherwise
a = at + exploration noise
Execute action at and observe reward r and new state st+ι
Store transition (st, ωt,at,rt, st+ι) in B
Extend the dimension of preference ωbo to batch size
for i = 0...N do
Sample batch from replay buffer B
Update actor according to equations 10 and 11:
θ S Lω,b+ Vθ LLbo
Update critic according to equations 8 and 9:
φ 一 (1 - β)VφLι(φ) + βVφL2(φ)
end for
Sample batch from replay buffer B
Update adversary according to equations 12 and 13:
G 一 vLω 山+vLω..
Update the target networks:
θ- V τθ + (1 - τ)θ-
θG- V τθG + (1 - τ)θG-
φ- V τφ+ (1 - τ)φ-
end for
Compute comprehensive metric I(P) using equation 14 and Pareto solution set P
end for
13
Under review as a conference paper at ICLR 2021
Algorithm 2 Robust multi-objective DDPG with the utility function Uq
Input: probability a, weighting coefficients β,τ,k
Randomly initialize actor π(s, ω; θ), adversary ∏(s, ω; θ) and critic network Q(s,a, ω; φ)
Initialize target networks with weights θ-, G- and φ-
Initialize replay buffer B and comprehensive metric I
for episode = 0...M do
Receive initial state s0
for t = 0...T do
Sample preference ωud based on uniform-distribution:
ωt — ωud
Sample action at = π(st, ωt; θ) , w.p. (1 - α)
πG(st , ωt ; θ) , otherwise
Gt = at + exploration noise
Execute action at and observe reward r and new state st+ι
Store transition (st, ωt, aGt, rt, st+1) in B
for i = 0...N do
Sample batch from replay buffer B
Update actor according to equations 10:
θ J RB Lωrb
Update critic network:
φ	J	(1 - β) RφEπα	hkyωrb - Q(s, a, ωrb; φ)k22i	+
βVφEπmix [l/yarb - 3：bQ(s,a, ωrb; φ)k2]
end for
Sample batch from replay buffer B
Update adversary according to equations 12:
G jVθLωrb
Update the target networks:
θ- J τθ + (1 - τ)θ-
θG- J τθG + (1 - τ)θG-
φ- J τφ+ (1 - τ)φ-
end for
end for
14
Under review as a conference paper at ICLR 2021
Algorithm 3 Robust multi-objective DDPG with linear scalarization function
Input: probability a, weighting coefficients β,τ
Randomly initialize actor π(s, ω; θ), adversary ∏(s, ω; θ) and critic network Q(s,a, ω; φ)
Initialize target networks with weights θ-, G- and φ-
Initialize replay buffer B and comprehensive metric I
for episode = 0...M do
Receive initial state s0
for t = 0...T do
Sample preference ωud based on uniform-distribution:
ωt — ωud
Sample action at = π(st, ωt; θ) , w.p. (1 - α)
πG(st , ωt ; θ) , otherwise
a = at + exploration noise
Execute action at and observe reward r and new state st+ι
Store transition (st, ωt,(⅛,rt, st+ι) in B
for i = 0...N do
Sample batch from replay buffer B
Update actor network:
θ — Enmix [(1 - α)VaωlbQ(s,α, 3心 φ)Vθπ(s, 3丁& θ)]
Update critic network:
φ	J	(1 - β) vφEπα hkyωrb - Q(S,a, ωrb; φ)∣∣2]	+
βVφEπmix hkωJbyωrb - ωrlbQ(s,a, 3注；φ)∣∣2]
end for
Sample batch from replay buffer B
Update adversary network:
G J Eπmix [αVsωJbQ(s, a, 3/；φ)V^π(s, 3/；G)]
Update the target networks:
θ- J τθ + (1 - τ)θ-
θG- J τθG + (1 - τ)θG-
φ- J τφ+ (1 - τ)φ-
end for
end for
15
Under review as a conference paper at ICLR 2021
Algorithm 4 multi-objective DDPG with linear scalarization function
Input: weighting coefficients β and T
Randomly initialize actor π(s, ω; θ), adversary ∏(s, ω; θ) and critic network Q(s,a, ω; φ)
Initialize target networks with weights θ-, G- and φ-
Initialize replay buffer B and comprehensive metric I
for episode = 0...M do
Receive initial state s0
for t = 0...T do
Sample preference ωud based on uniform-distribution:
ωt — ωud
Sample action at = π(st, ωt; θ)
Gt = at + exploration noise
Execute action at and observe reward r and new state st+ι
Store transition (st, ωt, aGt, rt, st+1) in B
for i = 0...N do
Sample batch from replay buffer B
Update actor network:
θ — En[VaωribQ(s, a, 3加 φ)Vθ∏(s, 3注；θ)]
Update critic network:
φ	J	(1 - β) vφEπ hkyωrb - Q(S,a, ωrb; φ)∣∣2]	+
βVφEπ hkωribyωrb - ωribQ(s, a, ωrb; φ)k22i
end for
Update the target networks:
θ- J τθ + (1 - τ)θ-
θG- J τθG + (1 - τ)θG-
φ- J τφ+ (1 - τ)φ-
end for
end for
A.1.2 Theoretical derivation
In this part, we provide the derivation details of some formulas.
The policy gradient of the protagonist based on ωrb can be derived:
VθLωrb ≈ Enmix VθUΩ(s,amix, ωrb)]
=Enmix [Vθω∣bQnmixgn)(s, amix, 3亦 φ) + kVθM(s, amix, 3力]
= Eπαmix [(1 - α)VaωribQ(s, a, ωrb; φ)Vθπ(s, ωrb; θ) + kVaM (s, a, ωrb)Vθπ(s, ωrb; θ)],
(15)
The policy gradient of the protagonist based on ωbo can be written as:
VθLJbo ≈ Enmix [VθUΩ(s,amix, ωbo)]
=Enmix [Vθω∣0Qnmixgn) (s, amix, 3b。; φ) + kVθM(s, amix, 3b。)]
= Enαmix [(1 - α)VaωbioQ(s, a, ωbo; φ)Vθπ(s, ωbo; θ) + kVaM (s, a, ωbo)Vθπ(s, ωbo; θ)],
(16)
The policy gradient of the adversary based on 3rb can be derived:
VgLJ J EnmixVMGamix, 3力]
=EnmixVθ∙3∣bQnmix(n'n)(s, amix, 3rb； φ) + kVgM(s, amix, 3力]
=Enmix [aVa3|bQ(s, a, 3rb； φ)Vg∏(s, 3rb； θ) + kVaM(s, α, 3rb)Vg∏(s, 3rb； θ)],
(17)
16
Under review as a conference paper at ICLR 2021
The policy gradient of the adversary based on ωbo can be described as:
VθLωbo ≈ Eπmix [Vθ,Uω(s,amix, 3bo)]
=Enmix [▽件J。QnmiYn㈤(s, amix, ωbo; φ) + kV^M(s, amix, 3b。)]
=Enmix [α V3|。Q(s, a, ωbo; φ')Vβ∏(s, ωbo; θ) + k VM(s, a, ωbo) Vg∏(s, ωbo; θ)],
(18)
A.2 Domain
In this section, we give more details about training environment. We demonstrate our approach in
four challenging continuous control domains 9.
Figure 9: (a) Velocity control for autonomous electric vehicle in SUMO; (b) Swimmer environment
in Mujoco; (c) Walker environment in Mujoco; (d) HalfCheetah environment in Mujoco.
A.2.1 SUMO
SUMO is a free and open source traffic simulation suite, which can provide intelligent driving function
verification environment. Transport efficiency and energy consumption are two conflicting objectives
for autonomous electric vehicle. In this paper, a robust multi-objective longitudinal decision-making
problem is focused to verify the effectiveness and advancedness of our proposed algorithm.
Observation and action space dimension: S ∈ R16, A ∈ R1.
The first objective is vehicle velocity:
vx
Ri = -ɪ,
vmax
The second objective is energy conservation:
R2=P ∕-⅛E)z,
where vxis vehicle longitudinal velocity, vmax is maximum speed, p = 1 is scaling factor.
A.2.2 Swimmer-v2
Observation and action space dimension: S ∈ R8 , A ∈ R2 .
The first objective is running velocity:
R1 = |vx| ,
The second objective is energy conservation:
R2 = max(pl — min(vX, 1)2 - 0.15 ^X a2, 0),
i
where vxis longitudinal velocity, ai is the action of each actuator.
17
Under review as a conference paper at ICLR 2021
A.2.3 Walker2d-v2
Observation and action space dimension: S ∈ R17, A ∈ R6.
The first objective is running velocity:
R1 = vx + 1,
The second objective is energy conservation:
R2 = 5 - X ai2 ,
i
where vx is longitudinal velocity, ai is the action of each actuator.
A.2.4 HalfCheetah-v2
Observation and action space dimension: S ∈ R17, A ∈ R6.
The first objective is running velocity:
R1 = min(vx , 4) + 1,
The second objective is energy conservation:
R2 = 5 - X ai2 ,
i
where vx is longitudinal velocity, ai is the action of each actuator.
A.3 Implementation
We implement the actor, adversary and critic neural networks by 2 fully connected hidden layers,
which layer sizes are {256, 256} and {512, 256} in SUMO and Mujoco respectively.
Our scheme contains three important hyperparameters α, λ and k. α mainly affects the robustness of
the policy model. If α is set to a smaller value, the robustness of the model will be reduced, otherwise,
the performance of the model will be reduced. λ and k are responsible for balancing hypervolume,
diversity and evenness metrics. if λ and k are set to larger values, then the diversity and evenness
of the learned policies will be better, otherwise, the hypervolume will converge to a larger value. In
order to select appropriate values, these three parameters need to be adjusted in the experiment.
The number of solution intervals Sn is set to 9, and the main parameters of our BRMO-DDPG
algorithm are reported in the Table 6.
Table 6: BRMO-DDPGParameters.
Parameter Name	Value
α	0.1
β	0.2
γ	0.99
λ	1
τ	0.01
k	100
Sn	9
batch size	128
replay size	1e7
noise scale	0.1
learning rate for actor	1e-4
learning rate for critic	1e-3
learning rate for adversary	1e-4
18
Under review as a conference paper at ICLR 2021
In the robustness and comparing the PGMORL tests, the utility is designed as follows:
U = ω | r -
rω
FE-kωk2
2
2
We test the utility of the policy ten times under each mass and disturbance changes, and calculate the
cumulative value of the utility. The mean and standard deviation of each policy’s utility obtained
from the whole test process are used to evaluate the performance of the policy under the environment
uncertainty. The smaller standard deviation is, the stronger the robustness of the policy is. Moreover,
for a policy , the higher the mean is, the stronger the capability to obtain utility under the environment
uncertainty is. Hence, the robustness and utility in Table 5 and 7 refer to the standard deviation and
the mean here.
In the experiment comparing the PGMORL scheme, we evenly selected ten trained policy models.
Because we use one model for testing, we evenly selected ten preferences to input to the tested model.
For the PGMORL method, we tested each model one hundred times and calculated the cumulative
value of the utility. For BRMORL scheme, we tested each preference one hundred times, and then
calculated the cumulative value of the utility. Therefore, the hypervolume and evenness in Table 5
and 7 are calculated by using the returns.
A.4 Results
In this section, we give more experimental results.
A.4.1 Swimmer-v2
Figure 10: Robustness to environmental uncertainty on Swimmer-v2 domain. Disturbance probability
represents the probability of a random disturbance being played instead of the selected action. Relative
mass denotes the ratio of the current agent’s mass to its original mass.
Figure 11: Robustness to environmental uncertainty on Swimmer-v2 domain.
A.4.2 HalfCheetah-v2 and Walker2d-v2
The comparison results of the baseline (PGMORL) are provided here.
19
Under review as a conference paper at ICLR 2021
Figure 12: Robustness to environmental uncertainty on Swimmer-v2 domain.
Figure 13: Robustness to environmental uncertainty on Swimmer-v2 domain.
Table 7: Test results on HalfCheetah-v2.
PGMORL BRMORL
HyPerVolUme	89984.28	21620.47
Evenness	0.31	0.36
Robustness	30.80	20.84
Utility	-198.23	-22.12
Figure 14: Robustness to environmental uncertainty on Walker2d-v2 domain. Disturbance Probability
rePresents the Probability of a random disturbance being Played instead of the selected action. Relative
mass denotes the ratio of the current agent’s mass to its original mass.
Figure 15: Robustness to environmental uncertainty on HalfCheetah-v2 domain.
20