Under review as a conference paper at ICLR 2021
Imagine That !
Leveraging Emergent Affordances for 3D
Tool Synthesis
Anonymous authors
Paper under double-blind review
Ab stract
In this paper we explore the richness of information captured by the latent space
of a vision-based generative model. The model combines unsupervised genera-
tive learning with a task-based performance predictor to learn and to exploit task-
relevant objectaffordancesgiven visual observations from a reaching task, involv-
ing a scenario and a stick-like tool. While the learned embedding of the generative
model captures factors of variation in 3D tool geometry (e.g. length, width, and
shape), the performance predictor identifies sub-manifolds of the embedding that
correlate with task success. Within a variety of scenarios, we demonstrate that
traversing the latent space via backpropagation from the performance predictor
allows us toimaginetools appropriate for the task at hand. Our results indicate
that affordances - like the utility for reaching - are encoded along smooth trajec-
tories in latent space. Accessing these emergent affordances by considering only
high-levelperformance criteria (such as task success) enables an agent to manip-
ulate tool geometries in a targeted and deliberate way.
1	Introduction
The advent of deep generative models (e.g. Burgess et al., 2019; Greff et al., 2019; Engelcke et al.,
2019) with their aptitude for unsupervised representation learning casts a new light on learning
affordances(Gibson, 1977). This kind of representation learning raises a tantalising question: Given
that generative models naturally capture factors of variation, could they also be used to expose these
factors such that they can be modified in a task-driven way? We posit that a task-driven traversal
of a structured latent space leads toaffordancesemerging naturally along trajectories in this space.
This is in stark contrast to more common approaches to affordance learning where it is achieved
via direct supervision or implicitly via imitation (e.g. Tikhanoff et al., 2013; Myers et al., 2015; Liu
et al., 2018; Grabner et al., 2011; Do et al., 2018). The setting we choose for our investigation is that
of tool synthesis for reaching tasks as commonly investigated in the cognitive sciences (Ambrose,
2001; Emery & Clayton, 2009).
In order to demonstrate that a task-aware latent space encodes useful affordance information we
require a mechanism to train such a model as well as to purposefully explore the space. To this end
we propose an architecture in which a task-based performance predictor (a classifier) operates on
the latent space of a generative model (seefig. 1). During training the classifier is used to provide
an auxiliary objective, aiding in shaping the latent space. Importantly, however, during test time the
performance predictor is used to guide exploration of the latent space via activation maximisation
(Erhan et al., 2009; Zeiler & Fergus, 2014; Simonyan et al., 2014), thus explicitly exploiting the
structure of the space. While our desire to affect factors of influence is similar in spirit to the notion
of disentanglement, it contrasts significantly with models such asβ-VAE (Higgins et al., 2017),
where the factors of influence are effectively encouraged to be axis-aligned. Our approach instead
relies on a high-level auxiliary loss to discover the direction in latent space to explore.
Our experiments demonstrate that artificial agents are able toimaginean appropriate tool for a vari-
ety of reaching tasks by manipulating the tool’s task-relevant affordances. To the best of our knowl-
edge, this makes us thefirst to demonstrate an artificial agent’s ability to imagine, or synthesise,
3D meshes of tools appropriate for a given task via optimisation in a structured latent embedding.
1
Under review as a conference paper at ICLR 2021
performance predictor
optimisation steps
Figure 1: Tool synthesis for a reaching task. Our model is trained on data-triplets{task observation,
tool observation, success indicator}. Within a scenario, the goal is to determine if a given tool can
reach the goal (green) while avoiding barriers (blue) and remaining behind the boundary (red). If
a tool cannot satisfy these constraints, our approach (via the performance predictor)imagineshow
one may augment it in order to solve the task. Our interest is in what these augmentations, imagined
duringtool synthesis, imply about the learned object representations.
Similarly, while activation maximisation has been used to visualise modified input images before
(e.g. Mordvintsev et al., 2015), we believe this work to be thefirst to effect deliberate manipulation
of factors of influence by chaining the outcome of a task predictor to the latent space, and then
decoding the latent representation back into a 3D mesh. Beyond the application of tool synthesis,
we believe our work to provide novel perspectives on affordance learning and disentanglement in
demonstrating that object affordances can be viewed astrajectoriesin a structured latent space as
well as by providing a novel architecture adept at deliberately manipulating interpretable factors of
influence.
2	Related Work
The concept of anaffordance, which describes a potential action to be performed on an object (e.g. a
doorknobaffordsbeing turned), goes back to Gibson (1977). Because of their importance in cogni-
tive vision, affordances are extensively studied in computer vision and robotics. Commonly, affor-
dances are learned in a supervised fashion where models discriminate between discrete affordance
classes or predict masks for image regions which afford certain types of human interaction (e.g.
Stoytchev, 2005; KjellStrom et al., 2010; Tikhanoff et al., 2013; Mar et al., 2015; Myers et al., 2015;
Do et al., 2018). Interestingly, most works in this domain learn from object shapes which have been
given an affordance label a priori. However, the affordance of a shape is only properly defined in the
context of a task. Hence, we employ a task-driven traversal of a latent space to optimise the shape
of a tool by exploiting factors of variation which are conducive to task success.
Recent advances in 3D shape generation employ variational models (Girdhar et al., 2016; Wu et al.,
2016) to capture complex manifolds of 3D objects. Besides their expressive capabilities, the latent
spaces of such models also enable smooth interpolation between shapes. Remarkable results have
been demonstrated including ‘shape algebra’ (Wu et al., 2016) and the preservation of object part
semantics (Kohli et al., 2020) andfine-grained shape styles (Yifan et al., 2019) during interpolation.
This shows the potential of disentangling meaningful factors of variation in the latent representation
of 3D shapes. Inspired by this, we investigate whether these factors can be exposed in a task-
driven way. In particular, we propose an architecture in which a generative model for 3D object
reconstruction (Liu et al., 2019) is paired with activation maximisation (e.g. Erhan et al., 2009;
Zeiler & Fergus, 2014; Simonyan et al., 2014) of a task-driven performance predictor. Guided by
its loss signal, activation maximisation traverses the generative model’s latent representations and
drives an imagination process yielding a shape suitable for the task at hand.
2
Under review as a conference paper at ICLR 2021
performance predictor
optimisation steps
Figure 2: The model architecture. A convolutional encoderφrepresents the task imageI G as a
latent vectorz G . In parallel, the 3D tool encoderψtakes an input imageI Ti and its silhouetteI Si
and produces a latent representation ZT. The concatenated task-tool representation hCat is used by
a classifierσto estimate the success of the tool at solving the task (i.e. reaching the goal). Given
the gradient signal from this performance predictor for success, the latent tool representationz T
gets updated to render an increasingly suitable tool (via the 3D tool decoderψ ). We pretrained the
encoding and decoding models (ψ,ψ ) together as in prior work (Kato et al., 2018; Wang et al.,
2018).
A key application of affordance-driven shape imagination is tool use. Robotics boasts a mature body
of literature studying how robots can utilise tools to improve their performance across a wide range
of tasks like reaching (Jamone et al., 2015), grasping (Takahashi et al., 2017), pushing (Stoytchev,
2005) and hammering (Fang et al., 2018). The pipeline executing tool-based tasks typically starts
with models for tool recognition and selection (e.g. Tikhanoff et al., 2013; Zhu et al., 2015; Fang
et al., 2018; Saito et al., 2018; Xie et al., 2019) before tool properties and affordances are leveraged
to compute higher-order plans (Toussaint et al., 2018). Our proposed model lends itself to robotics
applications like these, as the learned latent space encodes a rich object-centric representation of
tools that are biased for specific tasks.
3	Method
Our overarching goal is to perform task-specific tool synthesis for 3D reaching tasks. We frame the
challenge of tool imagination as an optimisation problem in a structured latent space obtained using
a generative model. The optimisation is driven by a high-level, task-specific performance predictor,
which assesses whether a target specified by a goal imageI G is reachable given a particular tool
and in the presence of obstacles. To map from tool images into manipulable 3D tools, wefirst
train an off-the-shelf 3D single-view reconstruction model taking as input tool imagesI Ti , ITj and
corresponding tool silhouettesI Si , ISj as rendered from two different vantage pointsiandj. After
training, the encoder can infer the tool representation that contains the 3D structure information
given a single-view RGB image and its silhouette as input. This representation is implicitly used to
optimise the tool configuration to make it suitable for the task at hand. An overview of our model is
given infig. 2.
More formally, we considerNdata instances:{(I	Gn , ITn,i, ITn,j , ISn,i, ISn,j ,ρ n)}nN=1, where each
example features a task imageI G, tool imagesI T in two randomly selected viewsiandj, and their
3
Under review as a conference paper at ICLR 2021
Figure 3: (Left) Task examples from our dataset. Top and bottom rows correspond to unsuccessful
and successful tool examples respectively. ColumnsA-Erepresentfive different task scenario types
each imposing different tool constraints including width, length, orientation and shape. Note that the
robot isfixed at its base on the table and constrained to remain outside the red boundary. Hence, it
can only reach the green target with a tool while avoiding collisions with the blue obstacles. (Right)
Model inputs{task observation, tool observation}during training and test time.
corresponding silhouettesI S, as well as a binary labelρindicating the feasibility of reaching the
target with the given tool. Examples of task images and model inputs are shown infig. 3. In all our
experiments, we restrict the training input to such sparse high-level instances. For additional details
on the dataset, we refer the reader to the supplementary material.
3.1	Representing Tasks and Tools
Given that our tools are presented in tool imagesI T , it is necessary for thefirst processing step
to perform a 3D reconstruction ofI T , from pixels into meshes. To achieve this single view 3D
reconstruction of images into their meshes, we employ the same architecture as proposed by (Kato
et al., 2018; Wang et al., 2018). The 3D reconstruction model consists of two parts: an encoder
network and a mesh decoder. Given the tool image and its silhouette in viewi, i.eI Ti andI Si , we
denote the latent variable encoding the tool computed by the encoder,ψ, as
ψ(ITi, ISi) =zT.
(1)
The mesh decoder takesz T as input and synthesises the mesh by deforming a template. A differen-
tiable renderer (Liu et al., 2019) predicts the tool's silhoutte IS in another view j, WhCCh SS ComPared
to the ground-truth silhouetteI Sj to compute the silhouette lossL s . This silhouette lossL s together
with an auxiliary geometry lossL g formulates the total 3D reconstruction loss:
Lrecon
=L S + μL g,
(2)
where μi isthe Weihhtofthegemmeyyloss. WerefettheraadettoLUUeaaL 20019)rgg由`dingthe
exact hyper-parameter and training setup of the 3D reconstruction model.
Task imaiesI G are similarly represented in an abstract latent space. For this we employ a task
encoder,φ, which consists of a stack of convolutional layers. 1 φtakes the task imaieI G as input
and maps it into the task embeddiniz G .
3.2	Tool Imagination
Task-driven learningThe tool representationz T contains task-relevant information such as tool
lenith, width, and shape. In order to perform tool imaiination, the sub-manifold of the latent space
that corresponds to the task-relevant features needs to be accessed and traversed. This is achieved
by addini a three-layer MLP as a classifierσ. The classifierσtakes as input a concatenationh cat of
the task embeddiniz G and the tool representationz T, and predicts the softmax over the binary task
1Architecture details are provided in the supplementary material.
4
Under review as a conference paper at ICLR 2021
success. The classifier learns to identify the task-relevant sub-manifold of the latent space by using
the sparse success signalρand optimising the binary-cross entropy loss, such that
Ltask (σ(hcat),ρ) =-(ρlog (σ(h cat)) + (1-ρ) log (1-σ(h cat))),(3)
whereρ∈{0,1}is a binary signal indicating whether or not it is feasible to solve the task with the
given tool. The whole system is trained end-to-end with a loss given by
L IG,ITi ,ISi,ITj,ISj,ρ =L recon +Ltask.(4)
Note that the gradient from the task classifierσpropagates through both the task encoderφand the
toolkit encoderψ, and therefore helps to shape the latent representations of the toolkit with respect
to the requirements for task success.
Tool imaginationOnce trained, our model can synthesise new tools by traversing the latent man-
ifold of individual tools following the trajectories that maximise classification success given a tool
image and its silhouette (fig. 2). To do this, wefirst pick a tool candidate and concatenate its
representationz T with the task embeddingz G . This warm-starts the imagination process. The con-
catenated embeddingh cat is then fed into the performance predictorσto compute the gradient with
respect to the tool embeddingz T. We then use activation maximisation (Erhan et al., 2009; Zeiler &
Fergus, 2014; Simonyan et al., 2014) to optimisez T with regard toL task of the success estimation
σ(h cat)and a feasibility targetρ s = 1, such that
zT
zT +η
∂Ltask (σ (ZT∖ ρS)
∂z T
(5)
whereηdenotes the learning rate for the update. Finally, we apply this gradient update forSsteps or
until the success estimationσ(Z T)reaches a thresholdγ, and useψ	(ZT)to generate the imagined
3D tool mesh represented byZ T .
4	Experiments
In this section we investigate our model’s abilities in two experiments. First, we verify the func-
tionality of the task performance predictorσin atool selectionexperiment where only one out of
three tools is successfully applicable. Second, we examine our core hypothesis about task-driven
tool synthesis in atool imaginationexperiment where the model has to modify a tool shape to be
successfully applicable in a given task. In both experiments, we compare our fulltask-drivenmodel,
in which the tool latent space was trained jointly with the task performance predictor, with atask-
unawarebaseline, in which the 3D tool representation was trainedfirst and the task performance
predictor wasfitted to thefixed tool latent space. We report our results in table 1 as mean success
performances within a 95% confidence interval around the estimated mean.
Tool SelectionWe verify that the classifierσcorrectly predicts whether or not a given tool can
succeed at a chosen task. For each task, we create a toolkit containing three tool candidates where
exactly one satisfies the scenario constraints. The toolkits are sampled in the same way as the
remaining dataset and we refer the reader tofig. 3 again for illustrations of suitable and unsuitable
tools. We check whether the classifier outputs the highest success probability for the suitable tool.
Achieved accuracies for tool selection are reported in the left column of table 1.
Tool ImaginationWe evaluate whether our model can generate tools to succeed in the reaching
tasks. For each instance the target signal for feasibility is set toρs = 1, i.e.success. Then, the latent
vector of the tool is modified via backpropagation using a learning rate of0.01for10,000steps
or untilσ(h cat)reaches the threshold ofγ= 0.997. The imagined tool mesh is generated via the
mesh decoderψ . This is then rendered into a top-down view and evaluated using a feasibility test
which checks whether all geometric constraints are satisfied, i.e. successful reaching from behind the
workspace boundary while not colliding with any obstacle. We report the percentage of imagined
tools that successfully pass this test in table 1.
5
Under review as a conference paper at ICLR 2021
optimisation steps
Figure 4: Qualitative results of tool evolution during the imagination process. Each row illustrates
an example of how the imagination procedure can succeed at constructing tools that solve the task
by: (left) increasing tool length, (middle) decreasing tool width, and (right) altering tool shape
(creating an appropriately oriented hook). Each row in each grid represents a different imagination
experiment.
4.1	Model Training
In order to gauge the influence of the task feasibility signal on the latent space of the tools, we train
the model in two different setups. Atask-drivenmodel is trained with a curriculum: First, the 3D
reconstruction module is trained on tool images alone. Then, the performance predictor is trained
jointly with this backbone, i.e. the gradient from the predictor is allowed to back-propagate into
the encoder of the 3D reconstruction network. In atask-unawareablation, we keep the pre-trained
3D reconstruction weightsfixed during the predictor training removing any influence of the task
performance on the latent space. All models are trained for 15,000 steps in total. Thefirst 10,000
steps are spent on pre-training the 3D reconstruction part in isolation and the remaining 5,000 steps
are spent training the task performance predictor. We select checkpoints that have the lowest task
lossL task on the validation split.
4.2	Quantitative Results
We start our evaluation by examining the task performance predictor in the tool selection experiment
(TS). For each scenario type (A-E) we present 250 tasks from the test set, each paired with three
tool images and their respective silhouettes. We encode each tool with the tool encoderψand
concatenate its representationz T toz G obtained from the task image encoderφ. Each concatenated
pairh cat is passed through the feasibility predictorσand the tool which scores highest is selected.
We report the results for this experiment in the left section of table 1. The results confirm thatσis
able to identify suitable tools with almost perfect accuracy. Tool selection accuracy does not differ
significantly between the task-driven and the task-unaware variations of the model. This suggests
Table 1: (Left) Tool Selection: Mean accuracy when predicting most useful tool among three possi-
ble tools. (Right) Tool Imagination: Comparison of imagination processes when artificially warm-
starting from the same unsuitable tools in each instance. Best results are highlighted in bold.
Scn	N	Tool Selection Success [%]		Tool Imagination Success [%]		
		Task-Unaware	Task-Driven	Random Walk	Task-Unaware	Task-Driven
A250	250	88.8 ±3.9	^^90.± ±3.6	3.6 ±2.3	55.6±6.296.4	±2.3
B250	250	96.4 ±2.3	77.±±L9	5.6 ±2.9	42.0±6.178.8	±5.1
C250	250	96.4 ±2.3	77.±±2.1	23.6 ±5.3	56.8±6.176.4	±5.3
D250	250	96.8 ±2.2	98.±±1.6	2.4 ±1.7	2±5.4	81.2±	4.8
E250	250	87.2 ±4.1	87.±±4.1	1..6 ±4.8	4±4.0	86.4±	4.3
Tot	1250	93.1 ±LT~	44.3 ±L3 一	9.8±17^	6..6±2.783.8	±2.0
6
Under review as a conference paper at ICLR 2021
that the factors of tool shape variation are captured successfully in both cases and the feature vectors
produced byψare discriminative enough to be separated efficiently via the MLPσ.
After verifying the task performance predictor’s functionality in the tool selection experiment, we
investigate its ability to drive a generative process in the next experiment. This is done to test our
hypothesis about the nature and exploitability of the latent space. Given that the latent space captures
factors of variation in 3D tool geometry, we hypothesise that these factors can be actively leveraged
to synthesise a new tool by focusing on the performance predictor. Specifically, we present our
model with a task imageI G and an unsuitable tool geometry. We then encode the tool viaψand
modify its latent representationz T by performing activation maximisation through the performance
predictorσ. As the feasibility prediction is pushed towards 1, the tool geometry gradually evolves
into one that is applicable to the given task imageI G. In addition to comparing the imagination
outcomes for the task-driven and the task-unaware model, we also include arandom walkbaseline,
where, in place of taking steps in the direction of the steepest gradient, we move in arandom
directionin the task-driven latent space for10,000steps. In this baseline the latent vector of the
selected tool is updated by a sample drawn from an isotropic Gaussian with mean0, and, to match
the step size of our approach, the absolute value of the ground-truth gradient derived by back-
propagating from the predictor as the variance.
For 250 instances per scenario type, we warmstart each imagination attempt with the same infeasible
tool across random walk, task-driven, and task-unaware models to enable a like-for-like comparison,
with the results presented in table 1. The performance of the random walk baseline reveals that a
simple stochastic exploration of the latent space is not sufficient tofind suitable tool geometries.
However, following the gradient derived from the performance predictor leads to successful shaping
of tool geometries in a much more reliable way. While the task-unaware ablation provides a strong
baseline, transforming tools successfully in 63.6% of the cases, the task-driven model significantly
outperforms it, achieving a global success rate of 83.8% on the test cases. This implies that jointly
training the 3D latent representation and task performance predictor significantly shapes the latent
space in a ‘task-aware’ way, encoding properties which are conducive to task success (e.g. length,
width, and configuration of a tool) along smooth trajectories. Moreover, each of these trajectories
leads to higherreachabilitysuggesting that these affordances can be seen as a set trajectories in a
task-aware latent space.
4.3	Qualitative Results
Qualitative examples of the tool imagination process are provided infig. 4 andfig. 5. In the right-
middle example offig. 4, a novel T-shape tool is created, suggesting that the model encodes the
vertical stick-part and horizontal hook-part as distinct elements. The model also learns to interpolate
the direction of the hook part between pointing left and right, which leads to a novel tool. As
shown infig. 5, tools are modified in a smooth manner, leading us to hypothesise that tools are
embedded in a continuous manifold of changing length, width and configuration. Optimising the
latent embedding for the highest performance predictor score often drives the tools to evolve along
these properties. This suggests that these geometric variables are encoded astrajectoriesin the
structured latent space learnt by our model and deliberately traversed via a high-level task objective
in the form of the performance predictor.
5	Conclusion
In this paper we investigated the ability of an agent to synthesise tools via task-driven imagination
within a set of simulated reaching tasks. Our approach explores a hybrid architecture in which a
high-level performance predictor drives an optimisation process in a structured latent space. The
resulting model successfully generates tools for unseen scenario types not in the training regime;
it also learns to modify interpretable properties of tools such as length, width, and shape. Our
experimental results suggest that these object affordances are encoded astrajectoriesin a learnt
latent space, which we can navigate through in a deliberate way using a task predictor and activation
maximisation, and interpret by decoding the updated latent representations. Ultimately, this may aid
in our understanding of object affordances while offering a novel way to disentangle interpretable
factors of variation - not only for 3D tool synthesis. To facilitate further work in this area, We plan
to release both the reaching dataset and trained model to the community.
7
Under review as a conference paper at ICLR 2021
optimisation steps
Figure 5: Examples of tool synthesis progression during the imagination process. In the top row,
a stick tool morphs into a hook. The middle row shows a left-facing hook transforming into a
right-facing hook. In the bottom row, the tool changes into a novel T-shape. Constraints on these
optimisations are specified via task embeddings corresponding to the task images on the far left.
References
Stanley H Ambrose. Paleolithic technology and human evolution. Siencee,29((5099):7748i773,,
2001.
Christopher P. Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt
Botvinick, and Alexander Lerchner. MONet) Unsupervised scene decomposition and representa-
tion.arXiv preprint arXiv:1901.11390, 2092.
Thanh-Toan Do, Anh Nguyen, Ian D. Reid, Darwin G. Caldwell, and Nikos G. Tsagarakis. Af-
fordanceNet) An end-to-end deep learning approach for object affordance detection. InIEEE
International Conference on Robotics and Automation (ICRA), 2094.
Nathan J Emery and Nicola S Clayton. Tool use and physical cognition in birds and mammals.
Current Opinion in Neurobiology, 92(9):27-33, 2002.
Martin Engelcke, Adam R. Kosiorek, Oiwi Parker Jones, and Ingmar Posner. GENESIS) Gen-
erative scene inference and sampling with object-centric latent representations.arXiv preprint
arXiv:1907.13052, 2092.
Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. Visualizing higher-layer
features of a deep network. Technical report, University of Montreal, 2002.
Kuan Fang, Yuke Zhu, Animesh Garg, Andrey Kurenkov, Viraj Mehta, Li Fei-Fei, and Silvio
Savarese. Learning task-oriented grasping for tool manipulation from simulated self-supervision.
arXiv preprint arXiv:1806.09266, 2094.
James J. Gibson. The theory of affordances. InPerceiving, Acting, and Knowing: Toward an
Ecological Psychology. Lawrence Erlbaum, Hilldale, USA, 9277.
R. Girdhar, D.F. Fouhey, M. Rodriguez, and A. Gupta. Learning a predictable and generative vector
representation for objects. InECCV, 2096.
H. Grabner, J. Gall, and L. Van Gool. What makes a chair a chair? InComputer Vision and Pattern
Recognition (CVPR), pp. 9722-9736, 2099.
Klaus Greff, Raphael LoPeZ Kaufman, Rishabh Kabra, NiCk Watters, Christopher Burgess, Daniel
Zoran, Loic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object representation
learning with iterative variational inference. InInternational Conference on Machine Learning
(ICML), 2092.
4
Under review as a conference paper at ICLR 2021
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner.β-vae: Learning basic visual concepts with a con-
straied variational framework. InICLR, 2017.
Lorenzo Jamone, Giovanni Saponaro, Alexandre Antunes, Rodrigo Ventura, Alexandre Bernardino,
and JoSe SantoS-Victor. Learning object affordances for tool use and problem solving in cognitive
robots. InProceedings of the 2nd Italian Workshop on Artificial Intelligence and Robotics, pp.
68-82, 2015.
Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. Neural 3D mesh renderer. InProceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3907-3916, 2018.
H. Kjellstrom, J. Romero, and D. Kragic. Visual object-action recognition: Inferring object af-
fordances from human demonstration.Computer Vision and Image Understanding, pp. 81-90,
2010.
Amit Kohli, Vincent Sitzmann, and Gordon Wetzstein. Inferring semantic information with 3D
neural scene representations.arXiv preprint arXiv:2003.12673, 2020.
Shichen Liu, Tianye Li, Weikai Chen, and Hao Li. Soft rasterizer: A differentiable renderer for
image-based 3D reasoning. InProceedings of the IEEE International Conference on Computer
Vision, pp. 7708-7717, 2019.
Y. Liu, A. Gupta, P. Abbeel, and S. Levine. Imitation from observation: Learning to imitate behav-
iors from raw video via context translation. InIEEE International Conference on Robotics and
Automation (ICRA), pp. 1118-1125, 2018.
Tanis Mar, Vadim Tikhanoff, Giorgio Metta, and Lorenzo Natale. Self-supervised learning of grasp
dependent tool affordances on the icub humanoid robot. In2015 IEEE International Conference
on Robotics and Automation (ICRA), pp. 3200-3206. IEEE, 2015.
Alexander Mordvintsev, Christopher Olah, and Mike Tyka. Inceptionism: Going deeper
into neural networks, 2015. URLhttps://research.googleblog.com/2015/06/
inceptionism-going-deeper-into-neural.html.
A. Myers, C. L. Teo, C. Fermuller, and Y. Aloimonos. Affordance detection of tool parts from
geometric features. InIEEE International Conference on Robotics and Automation (ICRA), pp.
1374-1381, 2015.
Namiko Saito, Kitae Kim, Shingo Murata, Tetsuya Ogata, and Shigeki Sugano. Tool-use model
considering tool selection by a robot using deep learning. In2018 IEEE-RAS 18th International
Conference on Humanoid Robots (Humanoids), pp. 270-276. IEEE, 2018.
K. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside convolutional networks: Visualising image
classification models and saliency maps. InWorkshop at International Conference on Learning
Representations, 2014.
Alexander Stoytchev. Behavior-grounded representation of tool affordances. InIEEE International
Conference on Robotics and Automation (ICRA), pp. 3071-3076, 2005.
Kuniyuki Takahashi, Kitae Kim, Tetsuya Ogata, and Shigeki Sugano. Tool-body assimilation model
considering grasping motion through deep learning.Robotics and Autonomous Systems, 91:115-
127, 2017.
Vadim Tikhanoff, Ugo Pattacini, Lorenzo Natale, and Giorgio Metta. Exploring affordances and
tool use on the iCub. InIEEE-RAS International Conference on Humanoid Robots (Humanoids),
pp. 130-137, 2013.
Marc Toussaint, Kelsey Allen, Kevin Smith, and Joshua Tenenbaum. Differentiable physics and
stable modes for tool-use and manipulation planning. InRobotics: Science and Systems XIV.
Robotics: Science and Systems Foundation, 2018.
9
Under review as a conference paper at ICLR 2021
Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, and Yu-Gang Jiang. Pixel2mesh:
Generating 3D mesh models from single RGB images. InProceedings of the European Confer-
ence on Computer Vision (ECCV), pp. 52-67, 2018.
Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and Josh Tenenbaum. Learning a prob-
abilistic latent space of object shapes via 3D generative-adversarial modeling. In D. D. Lee,
M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.),Advances in Neural Information
Processing Systems 29, pp. 82-90. Curran Associates, Inc., 2016.
Annie Xie, Frederik Ebert, Sergey Levine, and Chelsea Finn. Improvisation through physical un-
derstanding: Using novel objects as tools with visual foresight.arXiv preprint arXiv:1904.05538,
2019.
Wang Yifan, Noam Aigerman, Vladimir Kim, Siddhartha Chaudhuri, and Olga Sorkine-Hornung.
Neural cages for detail-preserving 3D deformations.arXiv preprint arXiv:1912.06395, 2019.
Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
Proceedings of the IEEE European Conference on Computer Vision, pp. 818-833, 2014.
Yixin Zhu, Yibiao Zhao, and Song Chun Zhu. Understanding tools: Task-oriented object modeling,
learning and recognition. InThe IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2015.
10