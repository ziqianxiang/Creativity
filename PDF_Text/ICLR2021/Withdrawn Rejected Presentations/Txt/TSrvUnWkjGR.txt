Under review as a conference paper at ICLR 2021
On the Inversion of Deep Generative Models
Anonymous authors
Paper under double-blind review
Ab stract
Deep generative models (e.g. GANs and VAEs) have been developed quite exten-
sively in recent years. Lately, there has been an increased interest in the inversion
of such a model, i.e. given a (possibly corrupted) signal, we wish to recover the
latent vector that generated it. Building upon sparse representation theory, we
define conditions that rely only on the cardinalities of the hidden layer and are
applicable to any inversion algorithm (gradient descent, deep encoder, etc.), under
which such generative models are invertible with a unique solution. Importantly,
the proposed analysis is applicable to any trained model, and does not depend on
Gaussian i.i.d. weights. Furthermore, we introduce two layer-wise inversion pur-
suit algorithms for trained generative networks of arbitrary depth, where one of
them is accompanied by recovery guarantees. Finally, we validate our theoretical
results numerically and show that our method outperforms gradient descent when
inverting such generators, both for clean and corrupted signals.
1 Introduction
In the past several years, deep generative models, e.g. Generative Adversarial Networks (GANs)
(Goodfellow et al., 2014) and Variational Auto-Encoders (VAEs) (Kingma & Welling, 2013), have
been greatly developed, leading to networks that can generate images, videos, and speech voices
among others, that look and sound authentic to humans. Loosely speaking, these models learn a
mapping from a random low-dimensional latent space to the training data distribution, obtained in
an unsupervised manner.
Interestingly, deep generative models are not used only to generate arbitrary signals. Recent work
rely on the inversion of these models to perform visual manipulations, compressed sensing, image
interpolation, and more (Zhu et al., 2016; Bora et al., 2017; Simon & Aberdam, 2020). In this work,
we study this inversion task. Formally, denoting the signal to invert by y ∈ Rn , the generative model
as G : Rn0 → Rn, and the latent vector as z ∈ Rn0, we study the following problem:
z* = argmin 1 ∣∣G(z) - y∣∣2,	(1)
z2
where G is assumed to be a feed-forward neural network.
The first question that comes to mind is whether this model is invertible, or equivalently, does Equa-
tion 1 have a unique solution? In this work, we establish theoretical conditions that guarantee the
invertibility of the model G. Notably, the provided theorems are applicable to general non-random
generative models, and do not depend on the chosen inversion algorithm.
Once the existence of a unique solution is recognized, the next challenge is to provide a recovery
algorithm that is guaranteed to obtain the sought solution. A common and simple approach is to draw
a random vector z and iteratively update it using gradient descent, opting to minimize Equation 1
(Zhu et al., 2016; Bora et al., 2017). Unfortunately, this approach has theoretical guarantees only
in limited scenarios (Hand et al., 2018; Hand & Voroninski, 2019), since the inversion problem is
generally non-convex. An alternative approach is to train an encoding neural network that maps
images to their latent vectors (Zhu et al., 2016; Donahue et al., 2016; Bau et al., 2019; Simon &
Aberdam, 2020); however, this method is not accompanied by any theoretical justification.
We adopt a third approach in which the generative model is inverted in an analytical fashion. Specif-
ically, we perform the inversion layer-by-layer, similar to Lei et al. (2019). Our approach is based
on the observation that every hidden layer is an outcome of a weight matrix multiplying a sparse
1
Under review as a conference paper at ICLR 2021
vector, followed by a ReLU activation. By utilizing sparse representation theory, the proposed algo-
rithm ensures perfect recovery in the noiseless case and bounded estimation error in the noisy one.
Moreover, we show numerically that our algorithm outperforms gradient descent in several tasks,
including reconstruction of noiseless and corrupted images.
Main contributions: The contributions of this work are both theoretical and practical. We derive
theoretical conditions for the invertiblity of deep generative models by ensuring a unique solution
for the inversion problem defined in Equation 1. In short, these conditions rely on the growth of
the non-zero elements of consecutive hidden layers by a factor of 2 for trained networks and by any
constant greater than 1 for random models. Then, by leveraging the inherent sparsity of the hidden
layers, we introduce a layerwise inversion algorithm with provable guarantees in the noiseless and
noisy settings for fully-connected generators. To the best of our knowledge, this is the first work
that provides such guarantees for general (non-random) models, addressing both the conceptual in-
version and provable algorithms for solving Equation 1. Finally, we provide numerical experiments,
demonstrating the superiority of our approach over gradient descent in various scenarios.
1.1	Related Work
Inverting deep generative models: A tempting approach for solving Equation 1 is to use first or-
der methods such as gradient descent. Even though this inversion is generally non-convex, the works
in Hand & Voroninski (2019); Hand et al. (2018) show that if the weights are random then, under ad-
ditional assumptions, no spurious stationary points exist, and thus gradient descent converges to the
optimum. A different analysis, given in Latorre et al. (2019), studies the case of strongly smooth gen-
erative models that are near isometry. In this work, we study the inversion of general (non-random
and non-smooth) ReLU activated generative networks, and provide a provable algorithm that em-
pirically outperforms gradient descent. A close but different line of theoretical work analyzes the
compressive sensing abilities of trained deep generative networks (Shah & Hegde, 2018; Bora et al.,
2017); however, these works assume that an ideal inversion algorithm, solving Equation 1, exists.
Different works Bojanowski et al. (2017); Wu et al. (2019) suggest training procedures that result
with generative models that can be easily inverted. Nevertheless, in this work we do not assume
anything on the training procedure itself, and only rely on the weights of the trained model.
Layered-wise inversion: The closest work to ours, and indeed its source of inspiration, is Lei et al.
(2019), which proposes a novel scheme for inverting generative models. By assuming that the input
signal was corrupted by bounded noise in terms of '1 or '∞, they suggest inverting the model using
linear programs layer-by-layer. That said, to assure a stable inversion, their analysis is restricted to
cases where: (i) the network weights are Gaussian i.i.d. variables; (ii) the layers expand such that the
number of non-zero elements in each layer is larger than the size of the entire layer preceding it; and
(iii) that the last activation function is either ReLU or leaky-ReLU. Unfortunately, as mentioned in
their work, these three assumptions often do not hold in practice. In this work, we do not rely on the
distribution of the weights nor on the chosen activation function of the last layer. Furthermore, we
relax the expansion assumption as to rely only on the expansion of the number of non-zero elements.
This relaxation is especially needed in the last hidden layer, which is typically larger than the image
size.
Neural networks and sparse representation: In the search for a profound theoretical understand-
ing for deep learning, a series of papers suggested a connection between neural networks and sparse
coding, by demonstrating that the forward pass of a neural network is in fact a pursuit for a multi-
layer sparse representation (Papyan et al., 2017; Sulam et al., 2018; Chun & Fessler, 2019; Sulam
et al., 2019; Romano et al., 2019; Xin et al., 2016). In this work, we expand this proposition by
showing that the inversion of a generative model is based on sequential sparse coding steps.
2	The Generative Model
Notations: We use bold uppercase letters to represent matrices, and bold lowercase letters to rep-
resent vectors. The vector wj represents the jth column in the matrix W. Similarly, the vector
wi,j represents the jth column in the matrix Wi. The activation function ReLU is the entry-wise
operator ReLU(u) = max{u, 0}. We denote by spark(W) the smallest number of columns in
W that are linearly-dependent, and by kxk0 the number of non-zero elements in x. The mutual
2
Under review as a conference paper at ICLR 2021
|wT wj |
coherence of a matrix W is defined as: μ(W) = maxi=j 口」.八〔皿Ik . Finally, We define XS and
WiS as the supported vector and the row-supported matrix according to the set S, and denote by Sc
the complementary set of S .
Problem Statement: We consider a typical generative scheme G : Rn0 → Rn of the form:
X0 = z,
Xi+1 = ReLU(WiXi), for all i ∈ {0, . . . , L - 1},	(2)
G(z) = φ(WLXL),
Where Xi ∈ Rni, {Xi}iL=-11 are the hidden layers, Wi ∈ Rni+1 ×ni are the Weight matrices (nL+1 =
n), X0 = z ∈ Rn0 is the latent vector that is usually randomly selected from a normal distribution,
Z 〜N(0, σ2Ino), and φ is an invertible activation function, e.g. tanh, sigmoid, or piece-wise linear.
Given a sample X = G(z), that Was created by the generative model above, We aim to recover
its latent vector z. Note that each hidden vector in the model is produced by a ReLU activation,
leading to hidden layers that are inherently sparse. This observation supports our approach to study
this model utilizing sparse representation theory. In what follows, we use this observation to derive
theoretical statements on the invertibility and the stability of this problem, and to develop pursuit
algorithms that are guaranteed to restore the original latent vector.
3	Invertibility and Uniqueness
We start by addressing this question: “Is this generative process invertible?”. In other words, when
given a signal that was generated by the model, X = G(z*), we know that a solution Z to the inverse
problem exists; however, can we ensure that this is the only one? Theorem 1 below (its proof is given
in Appendix A) provides such guarantees, which are based on the sparsity level of the hidden layers
and the spark of the weight matrices (see Section 2). Importantly, this theorem is not restricted to a
specific pursuit algorithm; it can rather be used for any restoration method (gradient descent, deep
encoder, etc.) to determine whether the recovered latent vector is the unique solution.
Definition 1 (sub-spark). Define the s-sub-spark ofa matrix W as the minimal spark of any subset
S of rows of cardinality |S| = s, sub-spark(W, s) = min|S|=s spark(WS).
Definition 2 (sub-rank). Define the s-sub-rank ofa matrix W as the minimal rank over any subset
S of rows of cardinality |S| = s, sub-rank(W, s) = min|S |=s rank(WS).
Theorem 1 (Uniqueness). Consider the generative scheme described in Equation 2 and a signal
x = G(z*) with a corresponding set Ofrepresentations {x*}L=ι that satisfy:
(i)
SL = kxLko <
spark(WL)
2
(ii)	Si = kx*k0 < SUb-Spark2Wi，si+1) ,for all i ∈{1,...,L - 1}.
(iii)	n0 = sub-rank(W0, s1) ≤ s1.
Then, z* is the unique solution to the inverse problem that meets these sparsity conditions.
Theorem 1 is the first of its kind to provide uniqueness guarantees for general non-statistical weight
matrices. Moreover, it only requires an expansion of the layer cardinalities as opposed to Huang
et al. (2018); Hand & Voroninski (2019) and Lei et al. (2019) that require dimensionality expansion
that often does not hold for the last layer (typically n < nL).
A direct corollary of Theorem 1 is in the case of random matrices. In such case, the probability
of heaving n linearly dependent columns is essentially zero (Elad, 2010, Chapter 2). Hence, the
conditions of Theorem 1 become:
(i)	SL < n+-. (ii) Si < "+2+ 1 . (iii) si ≥ no.	(3)
In fact, since singular square matrices have Lebesgue measure zero, this corollary holds for almost
all set of matrices.
3
Under review as a conference paper at ICLR 2021
In practice, to allow for a sufficient increase in the cardinalities of the hidden layers, their dimensions
should expand as well, excluding the last layer. For example, if the dimensions of the hidden layers
increase by a factor of 2, as long as the hidden layers preserve a constant percentage of non-zero
elements, Theorem 1 holds almost surely. Notably, this is the common practice in various generative
architectures, such as DC-GAN Radford et al. (2015) and PGAN Karras et al. (2017).
Nevertheless, in the random setting, we can further relax the above conditions by utilizing a theorem
by Foucart & Rauhut (2013). This theorem considers a typical sparse representation model with
a random dictionary and states that a sparse representation is unique as long as its cardinality is
smaller than the signal dimension. Therefore, as presented in Theorem 2, in the random setting the
cardinality across the layers need to grow only by a constant, i.e. si < si+1 and sL < n.
Theorem 2 (Uniqueness for Random Weight Matrices). Assume that the weight matrices comprise
of random independent and identically distributed entries (say Gaussian). If the representations of
a signal X = G(z*) satisfy:
(i)	sL = kxLk0 < n.
(ii)	si = kXik0 < si+1, for all i ∈ {1, . . . , L - 1}.
(iii)	s1 = kX1k0 ≥ n0,
then, with probability 1, the inverse problem has a unique solution that meets these conditions.
The above theorem states that to ensure a unique global minimum in the stochastic case, the number
of nonzero elements should expand by only a single parameter. The proof of this theorem follows
the same protocol as Theorem 1’s proof, while replacing the spark-based uniqueness (Donoho &
Elad, 2003) with Foucart & Rauhut (2013). As presented in Section 6.1, these conditions are very
effective in predicting whether the generative process is invertible or not, regardless of the recovery
algorithm used.
4 Pursuit Guarantees
In this section we provide an inversion al-
gorithm supported by reconstruction guar-
antees for the noiseless and noisy settings.
To reveal the potential of our approach, we
first discuss the performance of an Oracle,
in which the true supports of all the hidden
layers are known, and only their values are
missing. This estimation can be performed
by a sequence of simple linear projections
on the known supports. Note that already
in the first step of estimating XL, we can
realize the advantage of utilizing the inher-
ent sparsity of the hidden layers. Here, the
reconstruction error of the Oracle is pro-
portional to sL = kXLk0, whereas solving
a least square problem, as suggested in Lei
et al. (2019), results with an error that is
proportional to nL . For more details see
Appendix B.
Algorithm 1 Layered Basis-Pursuit
Input: y = G(Z) + e ∈ Rn, where ∣∣ek2 ≤ J and
sparsity levels {si}iL=1.
First step: XL = argminχ 1∣∣φ-1(y)- WLXll2 +
λL ∣∣x∣1, With λL = 2'j.
Set SL = SUPPOrt(XL ) and jl = mjj 'e.
General step: For any layer i = L - 1, . . . , 1 execute:
L Xi = argminx1 ∣∣χS++1- WSi+1 χ∣∣2 + λ ∣∣χk1,
with λi = 2ei+1.
2. Set Si = Support(Xi) and j = (3+∕r^)√si 6i+ι.
minjwiS,ij+12
Final step: Set Z = argminz 1 ∣∣XS1 一 WS1 z∣∣ .
In what follows, we propose to invert the model by solving sparse coding problems layer-by-layer,
while leveraging the sparsity of all the intermediate feature vectors. Specifically, Algorithm 1 de-
scribes a layered Basis-Pursuit approach, and Theorem 3 provides reconstruction guarantees for this
algorithm. The proof of this theorem is given in Appendix C. In Corollary 1 we provide guarantees
for this algorithm when inverting non-random generative models in the noiseless case.
Definition 3 (Mutual Coherence of Submatrix). Define μs(W) as the maximal mutual coherence
of any submatrix of W with S rows, μs(W) = max∣S∣=s μ(WS) ∙
4
Under review as a conference paper at ICLR 2021
A
”•■
Midlayer Inversion
Linearized ADMM
(r _
Midlayer
Inversion
Linearized ADMM
First Layer
■	Inversion
Linearized ADMM
Figure 1: The Latent-Pursuit inverts the generative model layer-by-layer as described in Algorithm 3,
and is composed of three steps: (1) last layer inversion by Algorithm 5;(2) midlayer inversions using
Algorithm 2; and (3) first layer inversion via Algorithm 2 with the x-step replaced by Equation 10.
Last Layer Inversion
Projected Gradient Descent

Theorem 3 (Layered Basis-Pursuit Stability). Suppose that y = X + e, where X = G(Z) is an
unknown signal with known SParsity levels {si}L=ι, and ∣∣e∣∣2 ≤ e. Let ' be the Lipschitz Constant
of φ-1 and define eL+1 = 'e. Ifin each midlayer i ∈ {1,..., L}, Si < 3μ-^WJ, then,
•	The support of Xi is a subset ofthe true support, Si ⊆ Si;
•	The vector Xi is the unique solutionfor the basis-pursuit;
•	The midlayer's error satisfies ∣∣Xi 一 Xi∣b < e%, where ei = (3+∕1^S^)√s]i^ei+1.
minj wi,ij+1
•	the recovery error on the latent space is upper bounded by
∣z - z∣2 <^- YY (3 + √rJ)√j, Where 中=λmm ((WSI )t WSI) > 0.	(4)
6& mm∕Hj+12	I	J
Corollary 1 (Layered Basis-Pursuit - Noiseless Case). Let X = G(z) with sparsity levels {si}L=ι,
and assume that Si < 1/3小§升1 (Wi) for all i ∈ {1,..., L}, and that 夕=λmin((WS1 )t WSI) >
0. ThenAlgorithm 1 recovers the latent vector Z = Z perfectly.
5 The Latent-Pursuit Algorithm
While Algorithm 1 provably inverts the generative model, it only uses the non-zero elements XiS+i+11
to estimate the previous layer Xi . Here we present the Latent-Pursuit algorithm, which expands the
Layered Basis-Pursuit algorithm by imposing two additional constraints. First, the Latent-Pursuit
sets inequality constraints, Wi
1
xi ≤ 0, that emerge from the ReLU activation. Second, recall that
the ReLU activation constrains the midlayers to have nonnegative values, Xi ≥ 0. Furthermore, we
refrain from inverting the activation function directly φ-1 since practically, this inversion might be
unstable, e.g. when using tanh. In what follows, we describe each of the three parts of the proposed
algorithm: (i) the image layer; (ii) the middle layers; and (iii) the first layer.
Starting with the inversion of the last layer, i.e. the image layer, we need to solve
XL = arg min 1 ∣∣y — Φ(Wlx)∣∣2 + λr1Tx, s. t. X ≥ 0,
x2
(5)
where 1TX represents an `1 regularization term under the nonnegative constraint. Assuming that φ
is smooth and strictly monotonic increasing, this problem is a smooth convex function with sepa-
rable constraints, and therefore, it can be solved using a projected gradient descent algorithm. In
particular, we employ FISTA (Beck & Teboulle, 2009), as described in Algorithm 5 in Appendix D.
5
Under review as a conference paper at ICLR 2021
We move on to the middle layers, i.e. estimating xi for i ∈ {1, . . . , L - 1}. Here, both the approxi-
mated vector and the given signal are assumed to result from a ReLU activation function. This leads
us to the following problem:
Xi = argmin ɪ ∣∣xS+ι 一 WSx∣∣ + λi1Tx, s.t. X ≥ 0, WScX ≤ 0	(6)
where S = Si+1 is the support of the output of the layer to be inverted, and Sc = Sic+1 is its
complementary. To solve this problem we introduce an auxiliary variable a = WiScX, leading to
the following augmented Lagrangian form:
mnu 1 ∣∣χS+ι -WS χ∣∣2+λi1TX+ρ2i∣∣a 一 WScX+u∣∣2
s. t. X ≥ 0, a ≤ 0.
(7)
This optimization problem could be solved using ADMM (Boyd et al., 2011), however, it would
require inverting a matrix of size ni × ni , which might be costly. Alternatively, we employ a more
general method, called alternating direction proximal method of multipliers (Beck, 2017, Chapter
15), in which a quadratic proximity term, ɪ ∣∣x 一 X(k) ∣∣q, is added to Equation 7. By setting
Q = αI-WSTWS + βI-PiWScTWSc, with α + β ≥ λmaχ(WSTWS + PiWScTWSc),⑻
we get that Q is positive semidefinite. This leads to the Linearized-ADMM algorithm in Algorithm 2
which is guaranteed to converge to the optimal solution of Equation 7 (see details in Appendix D).
We now recovered all the hidden layers, and only the latent vector z is left to be estimated. For this
inversion step we adopt a MAP estimator utilizing the fact that z is drawn from a normal distribution:
Z = argmin 1∣∣χf - WSz∣∣2 + 2 |同|2 , s. t. WScZ ≤ 0,	(9)
with γ > 0. This problem can be solved by the Linearized-ADMM algorithm described above (see
details in Appendix D), except for the update of X in Algorithm 2, which becomes:
z(k+1) <----1——((α + β)z(k) 一 WST(WSz(k) 一XS) 一PiWScT(WScz(k) 一a(k) 一u(k))).
α+β+γ	0	0	1	0	0
(10)
Once the latent vector z and all the hidden layers {Xi}iL=1 are recovered, we propose an optional step
to improve the final estimation. In this step, which we refer to as debiasing, we freeze the recovered
supports and only optimize over the non-zero values in an end-to-end fashion. This is equivalent to
computing the Oracle, only here the supports are not known, but rather estimated using the proposed
pursuit. Algorithm 3 provides a short description of the entire proposed inversion method.
Algorithm 2 Latent Pursuit: Midlayer Inversion	Algorithm 3 The Latent-Pursuit Algorithm
Initialization: χ(0) ∈ Rni, u(0), a(0) ∈ Rsi+1, Pi > 0, and α, β satisfying Equation 8. Until converged: for k = 0, 1, . . . execute: 1.	x(k+1) ― ReLU (x(k) - α+βWST(WSx(k)- xS+ι)-α++β WScT (WSc x(k) -a(k) -Mk))- α+β). 2.	a(k+1) ― — ReLU (u(k) - WScx(k+1)). 3.	u(k+1) ― u(k) + a(k+1) - WScx(k+1).	Initialization: Set λi > 0 and Pi > 0. First step: Estimate XL, i.e. solve Equa- tion 5 using Algorithm 5. Middle step: For layers i = L 一 1, . . . , 1, estimate Xi using Algorithm 2. Final step: Estimate z using Algorithm 2 but with the x-step of Equation 10. Debiasing (optional):	Set Z — argminz 1 ∣∣y - φ((Q0=L WSi+1) z)∣∣2∙
6 Numerical Experiments
We demonstrate the effectiveness of our approach through numerical experiments, where our goal
is twofold. First, we study random generative models and show the ability of the uniqueness claim
above (Theorem 2) to predict when both gradient descent and our approach fail to invert G as the
inversion is not unique. In addition, we show that in these random networks and under the conditions
of Corollary 1, the latent vector is perfectly recovered by both the Layered Basis-Pursuit and the
Latent-Pursuit algorithm. Our second goal is to demonstrate the advantage of Latent-Pursuit over
gradient descent for trained generative models, in two settings: noiseless and image inpainting.
6
Under review as a conference paper at ICLR 2021
Random Models - Layers Reconstruction Errors
Figure 2: Gaussian iid weights: Recovery errors as a function of the hidden-layer size (n1), where
the image space is 625. Subfigures (a)-(c) correspond to z ∈ R100 and (d)-(f) to z ∈ R200 . These
results support Theorem 2 stating that to guarantee a unique solution, the hidden layer cardinality
s1 ≈ n should be larger than the latent vector space and smaller than the image space. Moreover, it
supports Corollary 1 by showing that under the non-zero expansion condition, both Layered Basis-
Pursuit and Latent-Pursuit (Algorithms 1 and 3) recover the original latent vector perfectly.
6.1	Random Weights
First, we validate the above theorems on random generative models, by considering a framework
similar to Huang et al. (2018) and Lei et al. (2019). Here, the generator is composed of two layers:
x= G(z) = tanh(W2 ReLU(W1z)),	(11)
where the dimensions of the network are n = 625, n1 varies between 50 to 1000 and n0 ∈
{100, 200}. The weight matrices W1 and W2 are drawn from an iid Gaussian distribution. We
generate 512 signals by feeding the generator with latent vectors from a Gaussian distribution, and
then test the performance of the inversion of these signals in terms of SNR for all the layers, using
gradient descent, Layered Basis-Pursuit (Algorithm 1), and Latent-Pursuit (Algorithm 3). For gra-
dient descent, we use the smallest step-size from {1e - 1, 1e0, 1e1, 1e2, 1e3, 1e4} for 10, 000 steps
that resulted with a gradient norm smaller than 1e - 9. For Layered Basis-Pursuit we use the best
λ1 from {1e - 5, 7e - 6, 3e - 6, 1e - 6, 0}, and for Latent-Pursuit, we use λ1 = 0, ρ = 1e - 2 and
γ = 0. In Layered Basis-Pursuit and Latent-Pursuit we preform a debiasing step in a similar man-
ner to gradient descent. Figure 2 marks median results in the central line, while the ribbons show
90%, 75%, 25%, and 10% quantiles. In these experiments the sparsity level of the hidden layer is
approximately 50%, si = ∣∣xι k0 ≈ 号,due to the weights being random. In What follows, We split
the analysis of Figure 2 to three segments. Roughly, these segments are s1 < n0, n0 < s1 < n, and
n < s1 as suggested by the theoretical results given in Theorem 2 and Corollary 1.
In the first segment, Figure 2 shows that all three methods fail. Indeed, as suggested by the unique-
ness conditions introduced in Theorem 2, when s1 < n0, the inversion problem of the first layer
does not have a unique global minimizer. The dashed vertical line in Figure 2 marks the spot where
号=no. Interestingly, we note that the conclusions in (Huang et al., 2018; Lei et al., 2019), SUg-
gesting that large latent spaces cause gradient descent to fail, are imprecise and valid only for fixed
hidden layer size. This can be seen by comparing n0 = 100 to n0 = 200. As a direct outcome of
our uniqueness study and as demonstrated in Figure 2, gradient descent (and any other algorithm)
fails when the ratio between the cardinalities of the layers is smaller than 2. Nevertheless, Figure 2
7
Under review as a conference paper at ICLR 2021
exposes an advantage for using our approach over gradient descent. Note that our methods success-
fully invert the model for all the layers that follow the layer for which the sparsity assumptions do
not hold, and fail only past that layer, since only then uniqueness is no longer guaranteed. However,
since gradient descent starts at a random location, all the layers are poorly reconstructed.
For the second segment, we recall Theorem 3 and in particular Corollary 1. There we have shown
that Layered Basis-Pursuit and Latent-Pursuit are guaranteed to perfectly recover the latent vector
as long as the cardinality of the midlayer si = kxιk° satisfies n° ≤ si ≤ 1∕3μ(Wι). Indeed,
Figure 2 demonstrates the success of these two methods even when si ≈ n is greater than the
worst-case bound 1∕3μ(Wι). Moreover, this figure validates that Latent-Pursuit, which leverages
additional properties of the signal, outperforms Layered Basis-Pursuit, especially when si is large.
Importantly, while the analysis in Lei et al. (2019) suggests that n has to be larger than ni , in
practice, all three methods succeed to invert the signal even when ni > n. This result highlights the
strength of the proposed analysis that leans on the cardinality of the layers rather than their size.
We move on to the third and final segment, where the size of hidden layer is significantly larger
than the dimension of the image. Unfortunately, in this scenario the layer-wise methods fail, while
gradient descent succeeds. Note that, in this setting, inverting the last layer solely is an ambitious
(actually, impossible) task; however, since gradient descent solves an optimization problem of a
much lower dimension, it succeeds in this case as well. This experiment and the accompanied
analysis suggest that a hybrid approach, utilizing both gradient descent and the layered approach,
might be of interest. We defer a study of such an approach for future work.
6.2	Trained Network
To demonstrate the practical contribution of our work, we experiment with a generative network
trained on the MNIST dataset. Our architecture is composed of fully connected layers of sizes
20, 128, 392, and finally an image of size 28 × 28 = 784. The first two layers include batch-
normalization1 and a ReLU activation function, whereas the last one includes a piecewise linear
unit (Nicolae, 2018). We train this network in an adversarial fashion using a fully connected dis-
criminator and spectral normalization (Miyato et al., 2018). We should note that images produced
by fully connected models are typically not as visually appealing as ones generated by convolutional
architectures. However, since the theory provided here focuses on fully connected models, this set-
ting was chosen for the experimental section, similar to other previous work (Huang et al., 2018; Lei
et al., 2019) that study the inversion process.
Network inversion: We start with the noiseless setting and compare the Latent-Pursuit algorithm
to the Oracle (which knows the exact support of each layer) and to gradient descent. To invert a
signal and compute its reconstruction quality, we first invert the entire model and estimate the latent
vector. Then, we feed this vector back to the model to estimate the hidden representations and the
reconstructed image. For our algorithm we use ρ = 1e - 2 for all layers and 10, 000 iterations of
debiasing. For gradient-descent run, we use 10, 000 iterations, momentum of 0.9 and a step size of
1e - 1 that gradually decays to assure convergence. Overall, we repeat this experiment 512 times.
Figure 3a demonstrates the reconstruction error of the latent vector. First, we observe that the per-
formance of our inversion algorithm is on par with those of the Oracle. Moreover, not only does our
approach performs much better than gradient descent, but in many experiments the latter fails utterly.
In Appendix E.1 we provide reconstruction error for all the layers followed by image samples.
A remark regarding the run-time of these algorithms is in place. Using an Nvidia 1080Ti GPU, the
proposed Latent-Pursuit algorithm took approximately 15 seconds per layer to converge for a total
of approximately 75 seconds to complete, including the debiasing step, for all 512 experiments. On
the other hand, gradient-descent took approximately 30 seconds to conclude.
Image inpainting: We continue our experiments with image inpainting, i.e. inverting the network
and reconstructing a clean signal when only some of its pixels are known. First, we apply a random
mask in which 45% of the pixels are randomly concealed. Since the number of known pixels is still
larger than the number of non-zero elements in the layer preceding it, our inversion algorithm usually
reconstructs the image successfully as suggested by Figure 3b. In this experiment, we perform
slightly worse than the Oracle, which is not surprising considering the information disparity between
1 Note that after training, batch-normalization is a simple linear operation.
8
Under review as a conference paper at ICLR 2021
Figure 3: Trained 3 layers model: Reconstruction error of the latent vector z in 512 experiments.
Latent-Pursuit clearly outperforms gradient descent.
the two. As for gradient descent, we see similar results to the ones received in the non-corrupted
setting. Appendix E.2 provides image samples and reconstruction comparison across all layers.
Finally, We repeat the above experiment using a deterministic mask that conceals the upper 〜45%
of each image (13 out of 28 rows). The results of this experiment, which are provided in Figure 3c
and Appendix E.3, lead to similar conclusions as in the previous experiment. Indeed, since the model
contains fully connected layers, We expect the last tWo experiments to shoW comparable results.
7	Conclusions
In this paper We have introduced a novel perspective regarding the inversion of deep generative
netWorks and its connection to sparse representation theory. Building on this, We have proposed
novel invertibility guarantees for such a model for both random and trained netWorks. We have
accompanied our analysis by novel pursuit algorithms for this inversion and presented numerical
experiments that validate our theoretical claims and the superiority of our approach compared to the
more classic gradient descent. We believe that the insights underlining this Work could lead to a
broader activity Which further improves the inversion of these models in a variety of tasks.
References
Aviad Aberdam, Jeremias Sulam, and Michael Elad. Multi-layer sparse coding: the holistic Way.
SIAM Journal on Mathematics ofData Science,1(1):46-77, 2019.
David Bau, Jun-Yan Zhu, Jonas Wulff, William Peebles, Hendrik Strobelt, Bolei Zhou, and An-
tonio Torralba. Seeing What a gan cannot generate. In Proceedings of the IEEE International
Conference on Computer Vision, pp. 4502-4511, 2019.
Amir Beck. First-order methods in optimization, volume 25. SIAM, 2017.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. SIAM journal on imaging sciences, 2(1):183-202, 2009.
Piotr BojanoWski, Armand Joulin, David Lopez-Paz, and Arthur Szlam. Optimizing the latent space
of generative netWorks. arXiv preprint arXiv:1707.05776, 2017.
Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. Compressed sensing using gener-
ative models. In International Conference on Machine Learning (ICML), pp. 537-546. JMLR.
org, 2017.
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimiza-
tion and statistical learning via the alternating direction method of multipliers. Foundations and
TrendsR in Machine learning, 3(1):1-122, 2011.
Il Yong Chun and Jeffrey A Fessler. Convolutional analysis operator learning: acceleration and
convergence. IEEE Transactions on Image Processing, 29(1):2108-2122, 2019.
Jeff Donahue, Philipp Krahenbuhl, and Trevor Darrell. Adversarial feature learning. arXiv preprint
arXiv:1605.09782, 2016.
9
Under review as a conference paper at ICLR 2021
David L Donoho and Michael Elad. Optimally sparse representation in general (nonorthogonal)
dictionaries via `1 minimization. Proceedings of the National Academy of Sciences, 100(5):
2197-2202, 2003.
Michael Elad. Sparse and redundant representations: from theory to applications in signal and
image processing. Springer Science & Business Media, 2010.
Simon Foucart and Holger Rauhut. An invitation to compressive sensing. In A mathematical intro-
duction to compressive sensing, pp. 1-39. Springer, 2013.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Paul Hand and Vladislav Voroninski. Global guarantees for enforcing deep generative priors by
empirical risk. IEEE Transactions on Information Theory, 66(1):401-418, 2019.
Paul Hand, Oscar Leong, and Vlad Voroninski. Phase retrieval under a generative prior. In Advances
in Neural Information Processing Systems, pp. 9136-9146, 2018.
Wen Huang, Paul Hand, Reinhard Heckel, and Vladislav Voroninski. A provably convergent scheme
for compressive sensing under random generative priors. arXiv preprint arXiv:1812.04176, 2018.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for im-
proved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Fabian Latorre, Armin eftekhari, and Volkan Cevher. Fast and provable admm for learning with
generative priors. In Advances in Neural Information Processing Systems, pp. 12004-12016,
2019.
Qi Lei, Ajil Jalal, Inderjit S Dhillon, and Alexandros G Dimakis. Inverting deep generative models,
one layer at a time. In Advances in Neural Information Processing Systems, pp. 13910-13919,
2019.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Andrei Nicolae. Plu: The piecewise linear unit activation function. arXiv preprint
arXiv:1809.09534, 2018.
Vardan Papyan, Yaniv Romano, and Michael Elad. Convolutional neural networks analyzed via con-
volutional sparse coding. The Journal of Machine Learning Research, 18(1):2887-2938, 2017.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Yaniv Romano, Aviad Aberdam, Jeremias Sulam, and Michael Elad. Adversarial noise attacks of
deep learning architectures: Stability analysis via sparse-modeled signals. Journal of Mathemat-
ical Imaging and Vision, pp. 1-15, 2019.
Viraj Shah and Chinmay Hegde. Solving linear inverse problems using gan priors: An algorithm
with provable guarantees. In IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pp. 4609-4613. IEEE, 2018.
Dror Simon and Aviad Aberdam. Barycenters of natural images constrained wasserstein barycen-
ters for image morphing. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 7910-7919, 2020.
Jeremias Sulam, Vardan Papyan, Yaniv Romano, and Michael Elad. Multilayer convolutional sparse
modeling: Pursuit and dictionary learning. IEEE Transactions on Signal Processing, 66(15):
4090-4104, 2018.
10
Under review as a conference paper at ICLR 2021
Jeremias Sulam, Aviad Aberdam, Amir Beck, and Michael Elad. On multi-layer basis pursuit,
efficient algorithms and convolutional neural networks. IEEE transactions on pattern analysis
and machine intelligence, 2019.
Joel A Tropp. Just relax: Convex programming methods for identifying sparse signals in noise.
IEEE transactions on information theory, 52(3):1030-1051, 2006.
Yan Wu, Mihaela Rosca, and Timothy Lillicrap. Deep compressed sensing. arXiv preprint
arXiv:1905.06723, 2019.
Bo Xin, Yizhou Wang, Wen Gao, David Wipf, and Baoyuan Wang. Maximal sparsity with deep
networks? In Advances in Neural Information Processing Systems (NeurIPS), pp. 4340-4348,
2016.
JUn-Yan Zhu, PhiliPP Krahenbuhl, Eli Shechtman, and Alexei A Efros. Generative visual manipu-
lation on the natural image manifold. In European Conference on Computer Vision, pp. 597-613.
SPringer, 2016.
A Theorem 1: Proof
Proof. The main idea of the Proof is to show that under the conditions of Theorem 1 the inversion
task at every layer i ∈ {1, . . . , L + 1} has a unique global minimum. For this goal we utilize the
well-known uniqueness guarantee from sParse rePresentation theory.
Lemma 1 (SParse RePresentation - Uniqueness Guarantee Donoho & Elad (2003); Elad (2010)). If
a system of linear equations y = Wx has a solution x satisfying kxk0 < spark(W)/2, then this
solution is necessarily the sparset possible.
Using the above Lemma, we can conclude that if xL obeys kxL k0 = sL < spark(WL)/2, then xL
is the unique vector that has at most sL nonzeros, while satisfying the equation φ-1 (x) = WLxL.
Moving on to the Previous layer, we can emPloy again the above Lemma for the suPPorted vector
xSLL . This way, we can ensure that xL-1 is the unique sL-1-sParse solution of xSLL = WLSL-1xL-1
as long as
sL-1 = kxL-1k0 <
SPark(WL-ι)
2
(12)
However, the condition sl-i = ||xL-iko < Sub-Spark(WL-ι,sL) implies that the above necessarily
holds. This way we can ensure that each layer i, i ∈ {1, . . . , L - 1} is the unique sParse solution.
Finally, in order to invert the first layer we need to solve x1S1 = W0S1 z. If W0S1 has full column-
rank, this system either has no solution or a unique one. In our case, we do know that a solution
exists, and thus, necessarily, it is unique. A necessary but insufficient condition for this to be true
is s1 ≥ n0. The additional requirement sub-rank(W0, s1) = n0 ≤ s1 is sufficient for z to be the
unique solution, and this concludes the proof.
□
B	The Oracle Estimator
The motivation for studying the recovery ability of the Oracle is that it can reveal the power of
utilizing the inherent sparsity of the feature maps. Therefore, we analyze the layer-wise Oracle
estimator described in Algorithm 4, which is similar to the layer-by-layer fashion we adopt in both
the Layered Basis-Pursuit (Algorithm 1) and in the Latent-Pursuit (Algorithm 3). In this analysis
we assume that the contaminating noise is white additive Gaussian.
The noisy signal y carries an additive noise with energy proportional to its dimension, σ2n. Theorem
4 below suggests that the Oracle can attenuate this noise by a factor of nn0, which is typically much
smaller than 1. Moreover, the error in each layer is proportional to its cardinality σ2si. These results
are expected, as the Oracle simply projects the noisy signal on low-dimensional subspaces of known
11
Under review as a conference paper at ICLR 2021
Algorithm 4 The Layered-Wise Oracle
Input: y = G(z) + e ∈ Rn, and supports of each layer {Si}iL=1.
First step: XL = argminχ 2 ∣∣φ-1(y) - WLx∣∣j, where WL is the column supported matrix
WL [:, SL].
Intermediate steps: For any layer i = L — 1,..., 1, set Xi
Wi is the row and column supported matrix Wi [Si+ι, Si].
Final step: Set Z = argmi% 2 ∣∣XS1 - WSIz∣∣ .
argminx 2 ∣∣χS++1
-W iX∣∣ ,where
dimension. That said, this result reveals another advantage of employing the sparse coding approach
over solving least squares problems, as the error can be proportional to si rather than to ni.
Theorem 4 (The Oracle). Given a noisy signal y = G(z) + e, where e 〜N(0, σ21), and assuming
known supports {Si}iL=1, the recovery errors satisfy 2:
σ2
Qj=i λmax(Wj Wj )
≤ EkXi- Xik2 ≤
QL=i λmin (W T W j ) Si
(13)
for i ∈ {1,..., L}, where Wi is the row and Column Supported matrix, Wi[Si+ι, Si]. The recovery
error bounds for the latent vector are similarly given by:
σ2
Qj=0 λmax(Wj Wj )
no ≤ EIIz - z∣∣2
σ2
≤ J----------_T_ no.	(14)
Qj=0 λmin (Wj Wj)
S S FTt
Proof. Assume y = X + e with X = G(z), then the Oracle for the Lth layer is XS = WLy. Since
y = WLXL + e, we get that XS = XS +eL, where 巨工=WLe, and 巨工〜N(0,σ2(WTWl)-1).
Therefore, using the same proof technique as in Aberdam et al. (2019), the upper bound on the
recovery error in the Lth layer is:
E IlXL - xl∣∣2 = σ2 trace((WTWL)T) ≤ σ2-ST^-.	(15)
λmin(W T W l)
Using the same approach we can derive the lower bound by using the largest eigenvalue of WL WL.
In a similar fashion, we can write Xs = xs + ei for all i ∈ {0,...,L - 1}, where ei = A[i,L]e
and A[i,L]，WtWWt+ι •…WL. Therefore, the upper bound for the recovery error in the ith layer
2For simplicity we assume here that φ is the identity function.
12
Under review as a conference paper at ICLR 2021
becomes:
Ekxi- xik2 = E∣∣A[i,L]e∣∣2
= σ2 trace A[i,L] A[Ti,L]
=σ2 trace (A[i,L-i] WL(WL)TAT,L-1])
=σ2 trace (A[i,L-i] (WLWL)TAT,L-1])
σ2
≤ -----Tτ^— trace ( A[i,L-i]ATL-i])
λmin(W T W L)	[i [,	]	EL 1]>
≤ …
(16)
σ2
≤
--------------T-——trace
Qj=i+1 λmin (Wj Wj)
σ2
≤
--------------T-——trace
Qj=i+1 λmin (Wj Wj)
σ2
S	_ τ _	Si,
QL=i λmin(W T W j )
A[i,i] A[Ti,i]
((W T W i)-1)
and this concludes the proof.
□
C Theorem 3: Proof
Proof. We first recall the stability guarantee from Tropp (2006) for the basis-pursuit.
Lemma 2 (Basis Pursuit Stability TroPP (2006)). Let x* be an unknown sparse representation with
known cardinality of ∣∣x*ko = s, and let y = Wx*+e, where W is a matrix with unit-norm columns
and ∣∣ek2 ≤ 匕 Assume the mutual coherence of the dictionary W satisfies s < 1∕(3μ(W)). Let
X = argminχ 2 ∣∣y 一 Wx∣2 + λ ∣∣x∣ι, with λ = 2e. Then, X is unique, the support of X is a subset
of the SUPPOrt of x*, and
kx* - x∣∞ < (3 + √1.5)≡∙	(17)
In order to use the above lemma in our analysis we need to modify it such that W does not need
to be column normalized and that the error is '2- and not '∞-bounded. For the first modification
we decomPose a general unnormalized matrix W as WD, where W is the normalized matrix,
Wi = Wi/ IlWik2, and D is a diagonal matrix with di = ∣ Wi 12. Using the above lemma we get that
∣D(x* - x)k∞ < (3 + √i∙⅜∙	(18)
Thus, the error in X is bounded by
kx - xk∞ <
mini ∣Wi ∣2
(19)
Since Lemma 2 guarantees that the support of X is a subset of the support of x*, We can conclude
that
kx - Xk2 < (3 + ∕15) e√S∙	(20)
2	mini ∣Wi ∣2
Under the conditions of Theorem 3, we can use the above conclusion to guarantee that estimating
XL from the noisy input y using Basis-Pursuit must lead to a unique XL such that its support is a
subset of that of xL. Also,
kxL - XLk2 < ”
(3 +，T5)
minj kwL,j k2 eL+1 SL,
(21)
13
Under review as a conference paper at ICLR 2021
Algorithm 5 Latent Pursuit: Last Layer Inversion
Input: y ∈ Rn, K ∈ N,1二 ≥ 0, μ ∈ (0, `), φ(∙) is '-smooth and strictly monotonic increasing.
Initialization: u(0) — 0, XL)) — 0, t(0) — 1.
General step: for any k = 0, 1, . . . , K execute the following:
Lg J WT Φ0 (WLXLk)) [φ (WL xf) - y
2. u(k+1) - ReLU (XlLC) — μ ∙ (g + λL1))
3 t(k + 1) ― 1 + √1+4t(k)2
4. xLk+1) - u(k+1) + ⅛⅛ (u(k+1) — u(k))
Return: X(LK )
where wL,j is the jth column in WL, and L+1 = ` as φ-1(y) can increase the noise by a factor
of `.
^	^
Moving on to the estimation of the previous layer, We have that XLL = WS-IXL-I + eL, where
keL k2 ≤ L . According to Theorem 3 assumptions, the mutual coherence condition holds, and
therefore, we get that the support of XL-ι is a subset of the support of XL-ι, XL-ι is unique, and
that
||XL-1 - XL-1 k2 < EL-1 =	" ∏ ^ ^^T-EL√sL-1∙	(22)
minj wLSL-1,j2
Using the same technique proof for all the hidden layers results in
∣∣Xi - Xi∣∣2 < Ei = ^3+XP5Vei+ι√si, foralli ∈{1,...,L - 1},	(23)
minj∏wSj+1∏2
where wiS,ij+1 is the jth column in WiSi+1.
Finally, we have that	XS1	=	WS1Z	+	eι,	where	∣∣e1∣∣2	≤	eι.
λmin((W)S1)TW)S1) >0,and
z
1
arg min -
z2
-WSI z∏2.
Then,
|Z -z∣2 = eT ((WSI)TWSI)T eι ≤ ɪef,
which concludes Theorem 3 guarantees.
Therefore, if φ =
(24)
(25)
□
D Details on the Latent-Pursuit Algorithm
Here we provide additional details on the Latent-Pursuit algorithm described in Section 5.
In order to estimate the last layer we aim to solve
XL = arg min 1 Ily - φ(WLX)∣2 + λL1Tx, s. t. X ≥ 0.	(26)
x2
For this goal we make use of FISTA (Beck & Teboulle, 2009) algorithm as described in Algorithm
5.
As describe in Section 5, for estimating the middle layers we aim to solve:
Xi = argmin；卜S+ι - WSx∣∣ + λ%1Tx, s.t. X ≥ 0, WScX ≤ 0.	(27)
14
Under review as a conference paper at ICLR 2021
Using the auxiliary variable a = WiScx and the positive semidefinite matrix Q = αI-WiSTWiS+
βI - ρiWiScTWiSc , we get that the Linearized-ADMM aims to solve:
min 1 ∣∣xS+ι - WSx∣∣2+λi1Tx+Pi Ila - WScX + u∣∣ +1 ∣∣x - x(k)∣∣ , s.t. X ≥ 0, a ≤ 0.
x,a,u 2	2	2 2	Q
(28)
This leads to an algorithm that alternates through the following steps:
x(k+1)
a(k+1)
u(k+1)
α
arg min —
x2
X - (x(k) -1WST (WSx(k) - xS+ι) ) 2 + λiX +
x - Zk)- PWScT(WScx(k) - a(k) - u(k)
arg min ρi ∣∣a - WSc x(k+1) + u(k)∣∣2 , s. t. a ≤ 0.
(29)
s. t. x ≥ 0.
(30)
u(k) + a(k+1) - WiScx(k+1) .	(31)
J
一
J
β
2
2
2
Thus, the Linearized-ADMM algorithm, described in Algorithm 2 is guaranteed to converge to the
optimal solution of Equation 7.
After recovering all the hidden layers, we aim to estimate the latent vector z. For this inversion step
we adopt a MAP estimator as described in Section 5:
Z = argmin 1∣∣xS - WSz∣∣2 + Y ∣∣z∣∣2 , s. t. WScZ ≤ 0,	(32)
z2	2
with γ > 0. In fact, this problem can be solved by a similar Linearized-ADMM algorithm described
above, expect for the update of x (Equation 29), which becomes:
z(k+1) J arg min —
z2
z - ZZ(k) - 1WST
0
z(k) -
2
+
2
Z - (Z(k) - PWScT (WScZkk- a(k) - u(k)
2
+ 2 ι∣zk2.
(33)
β
2
Equivalently, for the latent vector z, the first step of Algorithm 2 is changed to to:
z(k+1) J--1——((Q + β)z(k) - WST(WSz(k) -XS) - PiWScT(WScz(k) - a(k) - u(k))).
—+ β + γ∖>	'	0 " U	1'尸1	0	' U	))
E Inversion Results for Trained Networks
Here we provide detailed results for the various inversion experiments described in Section 6.2.
E.1 Clean Images
Figure 4 demonstrates the reconstruction error for all the layers when inverting clean images. In
Figures 5 and 6 we demonstrate successful and failure cases of the gradient-descent algorithm and
compare them to our approach.
E.2 Random Mask Inpainting
Figures 7-9 demonstrate the performance of our approach compared to gradient descent in terms of
SNR and image quality respectively for the randomly-generated mask experiment.
E.3 Non-Random Mask Inpainting
Figures 10-12 demonstrate the performance of our approach compared to gradient descent in terms
of SNR and image quality respectively for the non-random mask experiment.
15
Under review as a conference paper at ICLR 2021
0
80
O O
6 4
UBEUSdxEnN
20
10
Errors for LayenO (Latent)
O O
4 3
UBEUSdxEnN
(a) Latent vector z
-10	0	10	20	30	40	50	60	70
SNR [dB]
Errors for Layer: 2
Latent-Pursuit
Oracle Estimator
Gradient Descent
Errors for Layer:1
aUBEμsdxujEnN
(b) First hidden layer x1
Errors for Layer:3 (Image)
80
70
Latent-Pursuit
Oracle Estimator
Gradient Descent
0	20	40	60	80	0	20	40	60	80
SNR [dB]	SNR [dB]
(C) Second hidden layer x2	(d) Image G(Z)
Figure 4: Trained 3-Layers Model: Reconstruction error for all the layers for 512 clean images. As
can be observed, the Latent-Pursuit almost mimics the oracle, and outperforms gradient descent.
Ground truth
Gradient descent
Our approach
Figure 5: Reconstruction failures of gradient descent on clean images.
Ground truth
Gradient descent
Our approach
Figure 6: Successful reconstructions of gradient descent on clean images.
16
Under review as a conference paper at ICLR 2021
Errors for LayenO (Latent)
Qgooo
6 5 4 3 2
UBEUSdxEnN
O IO 20	30	40
SNR [dB]
50
Errors for Layer:1
Qoggooo
8 7 6 5 4 3 2
j3ualuμsdxujEnN
(a) Latent vector z
Errors for Layer: 2
(C) Second hidden layer x2
Figure 7: Random mask inpainting reconstruction error for all the layers on a trained generator.
10
0
-10	0	10	20	30	40	50	60	70
SNR [dB]
(b) First hidden layer x1
Errors for Layer:3 (Image)
SNR [dB]
(d) Image G(Z)
Ground truth
Masked input
Gradient descent
Our approach
Figure 8:	Random mask inpainting gradient descent failed reconstructions.
Ground truth
Masked input
Gradient descent
Our approach
Figure 9:	Random mask inpainting gradient descent successful reconstructions.
17
Under review as a conference paper at ICLR 2021
Qgoooo
6 5 4 3 2 1
UBEUSdxEnN
SNR [dB]
(a) Latent vector z
Errors for Layer: 2
(b) First hidden layer x1
Errors for Layer:3 (Image)
(C) Second hidden layer x2
(d) Image G(Z)
Figure 10: Half image mask inpainting reconstruction error for all the layers.
Ground truth
Masked input
Gradient descent
Our approach
Figure 11:	Half image mask inpainting gradient descent failed reconstructions.
Ground truth
Masked input
Gradient descent
Our approach
Figure 12:	Half image mask inpainting gradient descent successful reconstructions.
18