Under review as a conference paper at ICLR 2021
Adaptive Single-Pass Stochastic Gradient De-
scent in Input Sparsity Time
Anonymous authors
Paper under double-blind review
Ab stract
We study sampling algorithms for variance reduction methods for stochastic op-
timization. Although stochastic gradient descent (SGD) is widely used for large
scale machine learning, it sometimes experiences slow convergence rates due to
the high variance from uniform sampling. In this paper, we introduce an algorithm
that approximately samples a gradient from the optimal distribution for a common
finite-sum form with n terms, while just making a single pass over the data, using
input sparsity time, and O (Td) space. Our algorithm can be implemented in big
data models such as the streaming and distributed models. Moreover, we show
that our algorithm can be generalized to approximately sample Hessians and thus
provides variance reduction for second-order methods as well. We demonstrate the
efficiency of our algorithm on large-scale datasets.
1 Introduction
There has recently been tremendous progress in variance reduction methods for stochastic gradient
descent (SGD) methods for the standard convex finite-sum form optimization problem minF (x) :=
x∈Rd
§ pn=1 fi(x), where fι,...,fn : Rd → R is a set of convex functions that commonly represent
loss functions. Whereas gradient descent (GD) performs the update rule xt+ι = Xt - nN F (Xt) on
the iterative solution xt at iterations t = 1, 2, . . ., SGD (Robbins & Monro, 1951; Nemirovsky &
Yudin, 1983; Nemirovski et al., 2009) picks it ∈ [n] in iteration t with probability pit and performs
the update rule xt+ι = Xt ——ηt-▽九(xt), where Vfit is the gradient (or a subgradient) of 九 and
npit	t	t	t
ηt is some predetermined learning rate. Effectively, training example it is sampled with probability
pit and the model parameters are updated using the selected example.
The SGD update rule only requires the computation of a single gradient at each iteration and provides
an unbiased estimator to the full gradient, compared to GD, which evaluates n gradients at each
iteration and is prohibitively expensive for large n. However, since SGD is often performed with
uniform sampling so that the probability pi,t of choosing index i ∈ [n] at iteration t is pi,t = § at
all times, the variance introduced by the randomness of sampling a specific vector function can be a
bottleneck for the convergence rate of the iterative process. Thus the subject of variance reduction
beyond uniform sampling has been well-studied in recent years (Roux et al., 2012; Johnson & Zhang,
2013; Defazio et al., 2014; Reddi et al., 2015; Zhao & Zhang, 2015; Daneshmand et al., 2016; Needell
et al., 2016; Stich et al., 2017; Johnson & Guestrin, 2018; Katharopoulos & Fleuret, 2018; Salehi
et al., 2018; Qian et al., 2019).
A common technique to reduce variance is importance sampling, where the probabilities pi,t are
chosen so that vector functions with larger gradients are more likely to be sampled. Thus for
Var(v) := E
kvk22 - kE [v]k22,
for a random vector v, then pi,t
1 for uniform sampling implies
σ2 = Var (n⅛ Vfit
n XX kVfi(xt)k2-n2 ∙kVF (xt)k2),
i=1
1
Under review as a conference paper at ICLR 2021
whereas importance sampling with pi,t = P』▽伶xt)[古川 gives
σ = Var (nP-t Vfit) = n12 ((XX kWi(xt)k) - n2 ∙ WF M)k2 j ,
which is at most = (n P ∣∣Vfi(Xt)∣∣2 — n2 ∙ ∣∣VF(xt)∣∣2),bythe Root-Mean Square-Arithmetic
Mean Inequality, and can be significantly less. Hence the variance at each step is reduced, possibly
substantially, e.g., Example 1.3 and Example 1.4, by performing importance sampling instead of
uniform sampling. In fact, it follows from the Cauchy-Schwarz inequality that the above importance
sampling probability distribution is the optimal distribution for variance reduction. However, comput-
ing the probability distribution for importance sampling requires computing the gradients in each
round, which is too expensive in the first place.
Second-Order Methods. Although first-order methods such as SGD are widely used, they do
sometimes have issues such as sensitivity to the choice of hyperparameters, stagnation at high training
errors, and difficulty in escaping saddle points. By considering second-order information such as
curvature, second-order optimization methods are known to be robust to several of these issues, such
as ill-conditioning. For example, Newton’s method can achieve a locally super-linear convergence
rate under certain conditions, independent of the problem. Although naive second-order methods are
generally too slow compared to common first-order methods, stochastic Newton-type methods such as
Gauss-Newton have shown to be scalable in the scientific computing community (Roosta-Khorasani
et al., 2014; Roosta-Khorasani & Mahoney, 2016a;b; Xu et al., 2019; 2020).
Our Contributions. We give a time efficient algorithm that provably approximates the optimal
importance sampling using a small space data structure. Remarkably, our data structure can be
implemented in big data models such as the streaming model, which just takes a single pass over the
data, and the distributed model, which requires just a single round of communication between parties
holding each loss function. For VF = ɪ P Vfi(X), where each Vfi = f (hai, Xi) ∙ ai for some
polynomial f and vector ai ∈ Rd, let nnz(A) be the number of nonzero entries of A := a1 ◦ . . . ◦an1.
Thus for T iterations, where d《T《n, GD has runtime O (T ∙ nnz(A)) while our algorithm has
runtime T ∙ poly(d, logn) + O (nnz(A)), where we use O (∙) to suppress polylogarithmic terms.
Theorem 1.1 Let VF = ɪ EVfi(X), where each Vfi = f(hai, Xi) ∙ ai for some polynomial f
and vector ai ∈ Rd and let nnz(A) be the number of nonzero entries of A := a1 ◦ . . . ◦ an. For
d T n, there exists an algorithm that performs T steps of SGD and at each step samples a
gradient within a constant factor of the optimal probability distribution. The algorithm requires a
single pass over A and uses O (nnz(A)) pre-processing time and O (Td) space.
Theorem 1.1 can be used to immediately obtain improved convergence guarantees for a class of
functions whose convergence rate depends on the variance σt, such as μ-smooth functions or strongly
convex functions. Recall that SGD offers the following convergence guarantees for smooth functions:
Theorem 1.2 (Nemirovski et al., 2009; Meka, 2017) Let F be a μ-smooth convex function and
Xopt = argmin F (X). Let σ2 be an upper bound for the variance of the unbiased estimator across all
iterations and Xk = *+i+xk. Let each step-size η be η ≤ ɪ. Thenfor SGD with initial position
X0,
E [F(Xk) - F(Xopt)] ≤ k—r i∣xo - Xoptk2 + η^~,
2ηk	2
so that k = O (* (σ2 + μ ∣∣Xo 一 Xoptk2) ) iterations suffices to obtain an E-approximate optimal
value by setting η =%.
In the convergence guarantees of Theorem 1.2, we obtain a constant factor approximation to the
variance σ = σopt from optimal importance sampling, which can be significantly better than the
1We use the notation a ◦ b to denote the vertical concatenation ba
2
Under review as a conference paper at ICLR 2021
variance σ = σunif orm from uniform sampling in standard SGD. We first show straightforward
examples where uniform sampling an index performs significantly worse than importance sampling.
For example, if Pfi(X) = ha” Xi ∙ ai, then for A = aι ◦ ... ◦ an：
Example 1.3 When the nonzero entries of the input A are concentrated in a small number of vectors
ai, uniform sampling will frequently sample gradients that are small and make little progress, whereas
importance sampling will rarely do so. In an extreme case, the A can contain exactly one nonzero
vector ai and importance sampling will always output the full gradient whereas uniform sampling
will only find the nonzero row with probability n.
Example 1.4 It may be that all rows of A have large magnitude, but X is nearly orthogonal to most
of the rows of A and heavily in the direction of row a『.Then〈a* Xi ∙ ai is small in magnitude
for most i, buthar, Xi ∙ a『is large so uniform sampling will often output small gradients while
importance sampling will output ha『,Xi ∙ a『with high probability.
Thus Example 1.3 shows that naive SGD with uniform sampling can suffer UP to a multiplicative
n factor loss in the convergence rate of Theorem 1.2 compared to that of SGD with importance
sampling whereas Example 1.4 shows a possible additive n factor loss.
Unlike a number of previous variance reduction methods, we do not require distributional assump-
tions (Bouchard et al., 2015; Frostig et al., 2015; Gopal, 2016; Jothimurugesan et al., 2018) or offline
access to the data (Roux et al., 2012; Johnson & Zhang, 2013; Defazio et al., 2014; Reddi et al., 2015;
Zhao & Zhang, 2015; Daneshmand et al., 2016; Needell et al., 2016; Stich et al., 2017; Johnson &
Guestrin, 2018; Katharopoulos & Fleuret, 2018; Salehi et al., 2018; Qian et al., 2019). On the other
hand, for applications such as neural nets in which the parameters in the loss function can change, we
can use a second-order approximation for a number of iterations, then reread the data to build a new
second-order approximation when necessary.
We complement our main theoretical result with empirical evaluations comparing our algorithm to
SGD with uniform sampling for logistic regression on the a9a Adult dataset collected by UCI and
retrieved from LibSVM (Chang & Lin, 2011). Our evaluations demonstrate that for various step-sizes,
our algorithm has significantly better performance than uniform sampling across both the number of
SGD iterations and surprisingly, wall-clock time.
We then show that our same framework can also be reworked to approximate importance sampling
for the Hessian, thereby performing variance reduction for second-order optimization methods. (Xu
et al., 2016) reduce the bottleneck of many second-order optimization methods to the task of sampling
S rows of A = ai ◦ ... ◦ an so that a row ai is sampled with probability -f (hai,Xi) ai akF ,
ɪ	∑n=ιkf(hai,xi)-a>l aikF
for some fixed function f so that the Hessian H has the form H := V2F = n P Vf (〈a%, X〉)a>a^
(Xu et al., 2016) show that this finite-sum form arises frequently in machine learning problems such
as logistic regression with least squares loss.
Theorem 1.5 Let V2F = n EVfi(X), where each Vfi = f (ha%, Xi) ∙ a>a% for some polynomial
f and vector ai ∈ Rd and let nnz(A) be the number of nonzero entries of A := a1 ◦ . . . ◦ an. For
d T n, there exists an algorithm that subsamples T Hessians within a constant factor of the
optimal probability distribution. The algorithm requires a single pass over A and uses O (nnz(A))
pre-processing time and O (Td) space.
2 SGD Algorithm
We first introduce a number of algorithms that will be used in our final SGD algorithm, along with
their guarantees. We defer all formal proofs to the appendix.
L2 polynomial inner product sketch. For a fixed polynomial f, we first require a constant-factor
approximation to IlPn=I f (〈ai, Xi) ∙ aik2 for any query X ∈ Rd; We call such an algorithm an L2
polynomial inner product sketch and give such an algorithm with the following guarantee：
Theorem 2.1 For a fixed > 0 and polynomial f, there exists a data structure ESTIMATOR that
outputs a (1 + e)-approximation to Pn=Ikf (〈ai, Xi) ∙ ai∣∣2 for any query X ∈ Rd. The data structure
3
Under review as a conference paper at ICLR 2021
requires a single pass over A = a1 ◦ . . . ◦ an (possibly through turnstile updates2), can be built
in O (nnz( A) + ∣2) time and O (∣2) space, uses query time Poly (d, ɪ, log n), and succeeds with
probability 1-----r1π.
poly(n)
The L2 polynomial inner product sketch ESTIMATOR is a generalization of AMS variants Alon
et al. (1999); Mahabadi et al. (2020) and is simple to implement. For intuition, observe that for
d = 1 and the identity function f, the matrix A ∈ Rn×d reduces to a vector of length n so that
estimating Pn=ι Ilf (hai, Xi) ∙ ai∣∣2 is just estimating the squared norm of a vector in sublinear space.
For a degree p polynomial f, ESTIMATOR generates random sign matrices S0, S1 , . . . , Sp with
O (*) rows and maintains S°A,..., SpA. To estimate Pn=ι IIaq ∙ (haj, xi)q ∙ aq∣2 for an integer
q ∈ [0,p] and scalar aq on a given query x, Estimator creates the q-fold tensor Y = y的 for
each row y of Sq A and the (q 一 1)-fold tensor X = x0(q-1). Note that X and Y can be refolded
into dimensions Rdq and Rd×dq so that YX ∈ Rd and ∣∣αq ∙ YX∣∣2 is an unbiased estimator
of Pn=ι ∣∣αq ∙ (hai, xi)q ∙ aq∣∣2. We give this algorithm in full in Algorithm 1. Thus, taking the
average over O C) instances of the sums of the tensor products for rows y across the sketches
So A,..., SpA gives a (1 + e)-approximation to Pn=Ikf (haj, Xi) ∙ ai∣∣2 with constant probability.
The success probability of success can then be boosted to 1 一 po]；(n)by taking the median of
O (log n) such outputs.
Algorithm 1 Basic algorithm ESTIMATOR that outputs (1 +	)-approximation to
Pn=Ik (hai, Xi)P ∙ ai∣2, where X is a post-processing vector
Input: Matrix A = a1 ◦ . . . ◦ ∈ Rn×d, post-processing vector x ∈ Rd, integer p ≥ 0, constant
parameter > 0.
Output: (1 + e)-approximation to Pn=Ik (haj, Xi)P ∙ ai∣2.
1:	r J Θ(log n) with a sufficiently large constant.
2:	b J Ω O with a sufficiently large constant.
3:	Let T be an r × b table of buckets, where each bucket stores an Rd vector, initialized to the zeros
vector.
4:	Let si ∈ {一1, +1} be 4-wise independent for i ∈ [n].
5:	Let hi : [n] → [b] be 4-wise independent for i ∈ [r].
6:	Let ui,j be the all zeros vector for each i ∈ [r], j ∈ [b].
7:	for each j = 1 to n do
8:	for each i = 1 to r do
9:	Add sjaj to the vector in bucket hi(j) of row i.
10:	Let vi,j be the vector in row i, bucket j of T for i ∈ [r], j ∈ [b].
11:	Process X:
12:	for i ∈ [r], j ∈ [b] do
13:	Uij J VffX蜜(P-I)
14:	return mediani∈[r] b Pj∈网 l∣Ui,jk2.
L2 polynomial inner product sampler. Given a matrix A = a1 ◦ . . . ◦ an ∈ Rn×d and a fixed
function f, a data structure that takes query X ∈ Rd and outputs an index i ∈ [n] with probability
roughly
kf(hai, Xi) ∙出k2
Pn=I kf(hai, Xi) ∙ aik2
is	called an L2 polynomial inner product sampler. We give such a data structure in Section A.1:
Theorem 2.2 For a fixed > 0 and polynomial f, there exists a data structure SAMPLER that takes
any query X ∈ Rd and outputs an index i ∈ [n] with probability 版"[*；："：；;：：* + poj(n), along
with a vector U := f (haj, Xi) ∙ aj + v, where E [v] = 0 and ∣∣v∣2 ≤ e ∙ kf (haj, Xi) ∙ ai∣∣2∙ The data
2Turnstile updates are defined as sequential updates to the entries of A.
4
Under review as a conference paper at ICLR 2021
structure requires a single pass over A = a1 ◦ . . . ◦ an (possibly through turnstile updates), can be
built in O (nnz(A) + ∣2) time and O (∣2) space,
with probability 1 - pοι1(n).
uses query time Poly (d, ɪ, log n), and succeeds
We remark that T independent instances of SAMPLER provide an oracle for T steps of SGD with
importance sampling, but the overall runtime would be T ∙ nnz(A) so it would bejust as efficient
to run T iterations of GD. The subroutine SAMPLER is significantly more challenging to describe
and analyze, so we defer its discussion to Section A.1, though it can be seen as a combination
of Estimator and a generalized CountSketch Charikar et al. (2004); Nelson & Nguyen (2013);
Mahabadi et al. (2020) variant and is nevertheless relatively straightforward to implement.
Leverage score sampler. Although SAMPLER outputs a (noisy) vector according to the desired
probability distribution, we also require an algorithm that automatically does this for indices i ∈ [n]
that are likely to be sampled multiple times across the T iterations. Equivalently, we require explicitly
storing the rows with high leverage scores, but we defer the formal discussion and algorithmic
presentation to Section A.2. For our purposes, the following suffices:
Theorem 2.3 There exists an algorithm LEVERAGE that returns all indices i ∈ [n] such that
PIn"kf(haiXi))：；12 ≥ 200Td for some X ∈ Rn, along with a vector Ui := f (0,xi) ∙ ai + Vi, where
IIviIl 2 ≤ LIlf (hai, Xi) ∙ ai∣∣2. The algorithm requires a single pass over A = aɪ ◦ ... ◦ an (possibly
through turnstile updates), uses O (nnz( A) + 詈)runtime (where ω denotes the exponent ofsquare
matrix multiplication) and O (∣2) space, and succeeds with probability 1 一 poj(n).
2.1 SGD Algorithm and Analysis
For the finite-sum optimization problem min F(x)
x∈Rd
1 Pn=I fi(x), Whereeach Vfi = f (ha，, x))∙
ai, recall that we could simply an instance of SAMPLER as an oracle for SGD with importance
sampling. However, naively running T SGD steps requires T independent instances, which uses
T ∙ nnz(A) runtime by Theorem 2.2. Thus we use a two level data structure by first implicitly
partition the rows of matrix A = a1 ◦ . . . ◦ an into β := Θ(T d) buckets B1, . . . , Bβ and creating
an instance of ESTIMATOR and SAMPLER for each bucket. The idea is that for a given query xt
in SGD iteration t ∈ [T], we first query xt to each of the ESTIMATOR data structures to estimate
Pii∈Bj kf (hai, Xi) ∙ aik2 for each j ∈ [β]. We then sample index j ∈ [β] among the buckets
Bi,... ,Bβ with probability roughly ^:：#：*Xti))@；；2 ∙OnCe we have sampled index j, it would
seem that querying the instance SAMPL=ER corresponding to Bj simulates SGD, since SAMPLER
now performs importance sampling on the rows in Bj , which gives the correct overall probability
distribution for each row i ∈ [n]. Moreover, SAMPLER has runtime proportional to the sparsity of
Bj, so the total runtime across the β instances of SAMPLER is O (nnz(A)).
However, an issue arises when the same bucket Bj is sampled multiple times, as we only create a
single instance of Sampler for each bucket. We avoid this issue by explicitly accounting for the
buckets that are likely to be sampled multiple times. Namely, we show that if
k f (hai,xt) >ai k 2
Pn=ιkf (hai,xti)∙aik2
<
1
200Td
for all t ∈ [T] and i ∈ [n], then by Bernstein’s inequality, the probability that no bucket Bj
is sampled multiple times is at least 得.Thus we use Leverage to separate all such rows ai that
violate this property from their respective buckets and explicitly track the SGD steps in which these
rows are sampled. We give the algorithm in full in Algorithm 2.
The key property achieved by Algorithm 2 in partitioning the rows and removing the rows that are
likely to be sampled multiple times is that each of the Sampler instances are queried at most once.
Lemma 2.4 With probability at least 100, each t ∈ [T] uses a different instance of SAMPLERj.
Proof of Theorem 1.1: Consider Algorithm 2. By Lemma 2.4, each time t ∈ [T] uses a fresh
instance of SAMPLERj, so that independent randomness is used. A possible concern is that each
instance ESTIMATORj is not using fresh randomness, but we observe that ESTIMATOR procedures
5
Under review as a conference paper at ICLR 2021
Algorithm 2 Approximate SGD with Importance Sampling
Input: Matrix A = a1 ◦ . . . ◦ an ∈ Rn×d, parameter T for number of SGD steps.
Output: T gradient directions.
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
Preprocessing Stage:
β J Θ(Td) with a sufficiently large constant.
Let h : [n] → [β] be a uniformly random hash function.
Let Bj be the matrix formed by the rows ai of A with h(i) = j, for each j ∈ [β].
Create an instance ESTIMATORj and SAMPLERj for each Bj with j ∈ [β] with E = 1.
Run LEVERAGE to find a set L0 of row indices and corresponding (noisy) vectors.
Gradient Descent Stage:
Randomly pick starting location x0
for t = 1 to T do
Let qi be the output of ESTIMATORj on query xt-1 for each i ∈ [β].
Sample j ∈ [β] with probability Pj = =—qj——
i∈[β] qi
if there exists i ∈ L0 with h(i) = j then
Use ESTIMATORj, Leverage, and SAMPLERjto sample gradient Wt = Nfit (Xt)
else
Use SAMPLERj to sample gradient Wt = Nfit (Xt)
C J kwtk2
pi,t j Pe qj
xt+1 J Xt - nPh ∙ wt
is only used in sampling a bucket j ∈ [β] as an L2 polynomial inner product sketch; otherwise the
sampling uses fresh randomness whereas the sampling is built into each instance of SAMPLERj .
By Theorem 2.2, each index i is sampled with probability within a factor 2 of the importance
sampling probability distribution. By Theorem 2.1, we have that pci,t is within a factor 4 of the
probability pi,t induced by optimal importance sampling SGD. Note that wt = Nfi (Xt) is an
unbiased estimator of Nfi(Xt) and kwtk is a 2-approximation to kNfi(Xt)k by Theorem 2.2. Hence,
the variance at each time t ∈ [T] of Algorithm 2 is within a constant factor of the variance σ2 =
(P kNfi(Xt)k)2 - kNF(Xt)k2 of optimal importance sampling SGD.
By Theorem 2.1, Theorem 2.2, and Theorem 2.3, the preprocessing time is O (nnz(A)) + T ∙
poly(d, log n) due to the choices of E = O (1) and β = Θ(T d), but partitioning the nonzero
entries of A across the β buckets. Similarly, the space used by the algorithm is O (Td). OnCe the
gradient descent stage of Algorithm 2 begins, it takes poly(d) time in each step t ∈ [T] to query the
β 二 Θ(Td) instances of Sampler and Estimator, for total time T ∙ poly(d, log n).	□
3	Second-Order Optimization
In this section, we repurpose our data stucture that performs importance sampling for SGD to instead
perform importance sampling for second-order optimization. Given a second-order optimization
algorithm that requires a sampled Hessian Ht, possibly along with additional inputs such as the
current iterate Xt and the gradient gt of F, we model the update rule by an oracle O(Ht), suppressing
other inputs to the oracle in the notation. For example, the oracle O corresponding to the canonical
second-order algorithm Newton’s method can be formulated as
Xt+1 = O(Xt) := Xt - [Ht]-1gt.
By black-boxing the update rule of any second-order optimization algorithm into the oracle, we can
focus our attention to the running time of sampling a Hessian with nearly the optimal probability
distribution. Thus we prove generalizations of the L2 polynomial inner product sketch, the L2
polynomial inner product sampler, and the leverage score sampler for Hessians.
Theorem 3.1 For a fixed E > 0 and polynomial f, there exists a data structure HES TIMATOR that
outputs a (1 + e)-approximation to Pn=JIf (hai, Xi) ∙ a>ai∣∣F for any query X ∈ Rd. The data
6
Under review as a conference paper at ICLR 2021
structure requires a single pass over A = a1 ◦ . . . ◦ an (possibly through turnstile updates), can be
built in O (nnz( A) + ∣2) time and O (∣2) space, uses query time Poly (d, ɪ, log n), and succeeds
with probability 1------r1π.
poly(n)
Theorem 3.2 For a fixed E > 0 and polynomial f, there exists a data structure HSAMPLER that
takes any query X ∈ Rd and outputs an index i ∈ [n] with probability(庄)k""Xi) ：> ：[* 十
Ei=ιk"hai,Xi)∙a> ai∣∣F
poj(n), along with a matrix U := f(hai, x〉) ∙ a>ai + V, where E [V] = 0 and ∣∣V∣f ≤ e ∙
∣∣f (hai, Xi) ∙ a> ai∣∣F. The data structure requires a single pass over A = aɪ ◦ ... ◦ an (possibly
through turnstile updates), can be built in O (nnz( A) + ∣2) time and O (∣2) space, uses query time
poly (d, ɪ, log n), and succeeds with probability 1 一 poj(n).
Theorem 3.3 There exists an algorithm HLEVERAGE that returns all indices i ∈ [n] such that
P1±^ kf I：：：：； aikF ≥ 200Td forsome X ∈ Rn, along with a matrix Ui := f(<ai, x>)∙a>ai +Vi,
where IlVikF ≤ E ∙ ∣∣f (hai, Xi) ∙ a>ai∣∣F. The algorithm uses requires a single pass over A 二
aι ◦ ... ◦ an (possibly through turnstile updates), uses O (nnz( A) + 冬)runtime (Where ω denotes
the exponent of square matrix multiplication) and O (∣2) space, and succeeds with probability
1------e
poly(n)
We remark that HSampler and Leverage are generalizations of Estimator and S ampler that
simply return an outer product of a noisy vector rather than the noisy vector itself.
As before, observe that we could simply run an instance of HSampler to sample a Hessian through
importance sampling, but sampling T Hessians requires T independent instances, significantly
increasing the total runtime. We thus use the same two level data structure that partitions the rows
of matrix A = a1 ◦ . . . ◦ an into β := Θ(T d) buckets B1 , . . . , Bβ. We then create an instance of
HESTIMATOR and HSAMPLER for each bucket. For an iterate Xt, we sample j ∈ [β] among the
P .∈ B k f (hai,Xti)∙a> ai k 2
buckets Bi,..., Bβ with probability roughly ——j------------------∣⅛f using HESTIMATOR and then
∑Xlkf(hai,xti)∙a> ai∣∣F
querying HSAMPLERj at Xt to sample a Hessian among the indices partitioned into bucket Bj . As
before, this argument fails when the same bucket Bj is sampled multiple times, due to dependencies
in randomness, but this issue can be avoided by using HLeverage to decrease the probability that
each bucket is sampled. We give the algorithm in full in Algorithm 3.
We remark that Algorithm 3 can be generalized to handle oracles O corresponding to second-order
methods that require batches of subsampled Hessians in each iteration. For example, if we want to
run T iterations of a second-order method that requires s subsampled Hessians in each batch, we can
simply modify Algorithm 3 to sample s Hessians in each iteration as input to O and thus T s Hessians
in total.
4	Empirical Evaluations
Our primary contribution is the theoretical design of a nearly input sparsity time algorithm that
approximates optimal importance sampling SGD. In this section we implement a scaled-down version
of our algorithm and compare its performance on large-scale real world datasets to SGD with
uniform sampling on logistic regression. We also consider both linear regression and support-vector
machines (SVMs) in the supplementary material. Because most rows have roughly uniformly small
leverage scores in real-world data, we assume that no bucket contains a row with a significantly large
leverage score and thus the implementation of our importance sampling algorithm does not create
multiple samplers for any buckets. By similar reasoning, our implementation uniformly samples a
number of indices i and estimates Pn=Ikf (包,Xi) ∙ ai∣2 by rescaling. Observe that although these
simplifications to our algorithm decreases the wall-clock running time and the total space used by our
algorithm, they only decrease the quality of our solution for each SGD iteration. We also consider
two hybrid SGD sampling algorithms; the first takes the better gradient obtained at each iteration
from both uniform sampling and importance sampling while the second performs 25 iterations of
7
Under review as a conference paper at ICLR 2021
Algorithm 3 Second-Order Optimization with Importance Sampling
Input: Matrix A = a1 ◦ . . . ◦ an ∈ Rn×d, parameter T for number of sampled Hessians, oracle O
that performs the update rule.
Output: T approximate Hessians.
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
Preprocessing Stage:
β J Θ(Td) with a sufficiently large constant.
Let h : [n] → [β] be a uniformly random hash function.
Let Bj be the matrix formed by the rows ai of A with h(i) = j, for each j ∈ [β].
Create an instance HESTIMATORj and HSAMPLERj for each Bj with j ∈ [β] with e = 1.
Run HLEVERAGE to find a set L0 of row indices and corresponding (noisy) outer products.
Second-Order Optimization Stage:
Randomly pick starting location x0
for t = 1 to T do
Let qi be the output of HESTIMATORj on query xt-1 for each i ∈ [β].
Sample j ∈ [β] with probability Pj = P q .
i∈[β] qi
if there exists i ∈ L0 with h(i) = j then
else
Use HESTIMATORj, HLEVERAGE, and HSAMPLERj to sample Hessian Ht.
Use HSAMPLERj to sample Hessian Ht = Pfit(xt).
pci,t J
kHtkF
j∈[β] qj
xt+1 J O (n⅛Ht)
eulaV evitcejbO egarevA
00	50	100	150	200	250
SGD Iterations
(a) Step size 0.1
0------------
0	50	100	150	200	250
SGD Iterations
(b) Step size 0.01
0
0	50	100	150	200	250
SGD Iterations
eulaV evitcejbO egarevA
emiT latoT egarevA
50	100	150	200	250
SGD Iterations
(d) Step size 0.1
6
.
1
emiT latoT egarevA
00	50	100	150	200	250
SGD Iterations
(c) Step size 0.001
6 284
...............
1100
emiT latoT egarevA
00	50	100	150	200	250
SGD Iterations
(e) Step size 0.01	(f) Step size 0.001
-”—-
Fig. 1: Comparison of objective values and runtimes for importance sampling (in blue squares),
uniform sampling (in red triangles), hybrid sampling that chooses the better gradient at each step
(in purple circles), and hybrid sampling that performs 25 steps of importance sampling followed by
uniform sampling (in teal X’s) over various step-sizes for logistic regression on a9a Adult dataset
from UCI, across 250 iterations, averaged over 10 repetitions.
importance sampling before using uniform sampling for the remaining iterations. Surprisingly, our
SGD importance sampling implementation not only significantly improves upon SGD with uniform
sampling, but are also competitive with the two hybrid algorithms. We do not consider other SGD
variants due to either their distributional assumptions or lack of known flexibility to big data models.
The experiments were performed in Python 3.6.9 on an Intel Core i7-8700K 3.70 GHz CPU with
8
Under review as a conference paper at ICLR 2021
m4l
8
eulaV evitcejbO egarevA
0-----------------------------、~~~~■
0	0.7	1.4	2.1	2.8	3.5
Average Total Time ∙l0-2
Fig. 2: Comparison of objective values and wall-clock time for importance sampling (in blue squares),
uniform sampling (in red triangles), and hybrid sampling that chooses the better gradient at each step
(in purple circles) over step-size 0.1 for logistic regression on a9a Adult dataset from UCI, averaged
over 3 repetitions across approximately 15 minute total computation time.
12 cores and 64GB DDR4 memory, using a Nvidia Geforce GTX 1080 Ti 11GB GPU. Our code is
publicly available at https://github.com/SGD-adaptive-importance/code.
Logistic Regression. We performed logistic regression on the a9a Adult data set collected by UCI
and retrieved from LibSVM (Chang & Lin, 2011). The features correspond to responses from the
1994 Census database and the prediction task is to determine whether a person makes over 50K USD
a year. We trained using a data batch of 32581 points and 123 features and tested the performance on
a separate batch of 16281 data points. For each evaluation, we generated 10 random initial positions
shared for importance sampling and uniform sampling. We then ran 250 iterations of SGD for
each of the four algorithms, creating only 250 buckets for the importance sampling algorithm and
computed the average performance on each iteration across these 10 separate instances. The relative
average performance of all algorithms was relatively robust to the step-size. Although uniform
sampling used significantly less time overall, our importance sampling SGD algorithm actually had
better performance when considering either number of iterations or wall-clock time across all tested
step-sizes. For example, uniform sampling had average objective value 20680 at iteration 250 using
0.0307 seconds with step-size 0.1, but importance sampling had average objective value 12917 at
iteration 5 using 0.025 seconds. We give our results for logistic regression in Figure 1. We repeat our
experiments in Figure 2 to explicitly compare the objective value of each algorithm with respect to
wall-clock time, rather than SGD iterations. Thus our results in Figure 2 empirically demonstrate
the advantages of our algorithm across the most natural metrics. For additional experiments, see
Section B.
5 Conclusion and Future Work
We have given variance reduction methods for both first-order and second-order stochastic optimiza-
tion. Our algorithms require a single pass over the data, which may even arrive implicitly in the form
of turnstile updates, and use input sparsity time and O (Td) space. Our algorithms are also amenable
to big data models such as the streaming and distributed models and are supported by empirical
evaluations on large-scale datasets. We believe there are many interesting future directions to explore.
For example, can we generalize our techniques to show provable guarantees for other SGD variants
and accelerated methods? A very large-scale empirical study of these methods would also be quite
interesting.
References
Noga Alon, Yossi Matias, and Mario Szegedy. The space complexity of approximating the frequency
moments. J. COmPUt Syst Sci., 58(1):137-147,1999.4,12
9
Under review as a conference paper at ICLR 2021
Alexandr Andoni, Robert Krauthgamer, and Krzysztof Onak. Streaming algorithms via precision
sampling. In IEEE 52nd Annual Symposium on Foundations of Computer Science, FOCS, pp.
363-372,2011. 13
Guillaume Bouchard, Theo Trouillon, Julien Perez, and Adrien Gaidon. Online learning to sample.
CoRR, abs/1506.09016, 2015. 3
Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology, 2:27:1-27:27, 2011. 3, 9, 18
Moses Charikar, Kevin C. Chen, and Martin Farach-Colton. Finding frequent items in data streams.
Theor. Comput. Sci., 312(1):3-15, 2004. 5, 12, 13
Kenneth L. Clarkson and David P. Woodruff. Low rank approximation and regression in input sparsity
time. In Symposium on Theory of Computing Conference, STOC, pp. 81-90, 2013. 16
Hadi Daneshmand, Aurelien Lucchi, and Thomas Hofmann. Starting small - learning with adaptive
sample sizes. In Proceedings of the 33nd International Conference on Machine Learning, ICML,
pp. 1463-1471, 2016. 1, 3
Aaron Defazio, Francis R. Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient
method with support for non-strongly convex composite objectives. In Advances in Neural
Information Processing Systems 27: Annual Conference on Neural Information Processing Systems,
pp. 1646-1654, 2014. 1, 3
Petros Drineas, Malik Magdon-Ismail, Michael W. Mahoney, and David P. Woodruff. Fast approxi-
mation of matrix coherence and statistical leverage. J. Mach. Learn. Res., 13:3475-3506, 2012.
16, 17
Roy Frostig, Rong Ge, Sham M. Kakade, and Aaron Sidford. Competing with the empirical risk
minimizer in a single pass. In Proceedings of The 28th Conference on Learning Theory, COLT, pp.
728-763, 2015. 3
Siddharth Gopal. Adaptive sampling for SGD by exploiting side information. In Proceedings of the
33nd International Conference on Machine Learning, ICML, pp. 364-372, 2016. 3
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in Neural Information Processing Systems 26, Proceedings., pp. 315-323,
2013. 1,3
Tyler B. Johnson and Carlos Guestrin. Training deep models faster with robust, approximate impor-
tance sampling. In Advances in Neural Information Processing Systems 31: Annual Conference on
Neural Information Processing Systems, NeurIPS, pp. 7276-7286, 2018. 1, 3
Ellango Jothimurugesan, Ashraf Tahmasbi, Phillip B. Gibbons, and Srikanta Tirthapura. Variance-
reduced stochastic gradient descent on streaming data. In Advances in Neural Information Pro-
cessing Systems 31: Annual Conference on Neural Information Processing Systems, NeurIPS, pp.
9928-9937, 2018. 3
Hossein Jowhari, Mert Saglam, and Gabor Tardos. Tight bounds for lp samplers, finding duplicates
in streams, and related problems. In Proceedings of the 30th ACM SIGMOD-SIGACT-SIGART
Symposium on Principles of Database Systems, PODS, pp. 49-58, 2011. 13
Angelos Katharopoulos and Francois Fleuret. Not all samples are created equal: Deep learning with
importance sampling. In Proceedings of the 35th International Conference on Machine Learning,
ICML, pp. 2530-2539, 2018. 1, 3
Sepideh Mahabadi, Ilya P. Razenshteyn, David P. Woodruff, and Samson Zhou. Non-adaptive adaptive
sampling on turnstile streams. In Proccedings of the 52nd Annual ACM SIGACT Symposium on
Theory of Computing, STOC, pp. 1251-1264, 2020. 4, 5, 12, 13, 17
Raghu Meka. Cs289ml: Algorithmic machine learning notes, 2017. URL https://raghumeka.
github.io/CS289ML/gdnotes.pdf. 2
10
Under review as a conference paper at ICLR 2021
Deanna Needell, Nathan Srebro, and Rachel Ward. Stochastic gradient descent, weighted sampling,
and the randomized kaczmarz algorithm. Math. Program., 155(1-2):549-573, 2016. 1, 3
Jelani Nelson and Huy L. Nguyen. OSNAP: faster numerical linear algebra algorithms via sparser
subspace embeddings. In 54th Annual IEEE Symposium on Foundations of Computer Science,
FOCS, pp. 117-126. IEEE Computer Society, 2013. 5, 16, 17
Arkadi Nemirovski, Anatoli B. Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic
approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574-
1609, 2009. 1,2
Arkadi Semenovich Nemirovsky and David Borisovich Yudin. Problem complexity and method
efficiency in optimization, 1983. 1
Xun Qian, Peter Richtarik, Robert M. Gower, Alibek Sailanbayev, Nicolas Loizou, and Egor Shulgin.
SGD with arbitrary sampling: General analysis and improved rates. In Proceedings of the 36th
International Conference on Machine Learning, ICML, volume 97, pp. 5200-5209, 2019. 1, 3
Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabas P6czos, and Alexander J. Smola. On variance
reduction in stochastic gradient descent and its asynchronous variants. In Advances in Neural
Information Processing Systems 28: Annual Conference on Neural Information Processing Systems,
pp. 2647-2655, 2015. 1, 3
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical
statistics, pp. 400-407, 1951. 1
Farbod Roosta-Khorasani and Michael W. Mahoney. Sub-sampled newton methods I: globally
convergent algorithms. CoRR, abs/1601.04737, 2016a. 2
Farbod Roosta-Khorasani and Michael W. Mahoney. Sub-sampled newton methods II: local conver-
gence rates. CoRR, abs/1601.04738, 2016b. 2
Farbod Roosta-Khorasani, Kees Van Den Doel, and Uri Ascher. Stochastic algorithms for inverse
problems involving pdes and many measurements. SIAM Journal on Scientific Computing, 36(5):
S3-S22, 2014. 2
Nicolas Le Roux, Mark Schmidt, and Francis R. Bach. A stochastic gradient method with an
exponential convergence rate for finite training sets. In Advances in Neural Information Processing
Systems 25: 26th Annual Conference on Neural Information Processing Systems., pp. 2672-2680,
2012. 1,3
Farnood Salehi, Patrick Thiran, and L. Elisa Celis. Coordinate descent with bandit sampling. In
Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information
Processing Systems, NeurIPS, pp. 9267-9277, 2018. 1, 3
Sebastian U. Stich, Anant Raj, and Martin Jaggi. Safe adaptive importance sampling. In Advances in
Neural Information Processing Systems 30: Annual Conference on Neural Information Processing
Systems, pp. 4381-4391, 2017. 1, 3
Peng Xu, Jiyan Yang, Farbod Roosta-Khorasani, Christopher Re, and Michael W. Mahoney. Sub-
sampled newton methods with non-uniform sampling. In Advances in Neural Information Process-
ing Systems 29: Annual Conference on Neural Information Processing Systems, pp. 3000-3008,
2016. 3
Peng Xu, Fred Roosta, and Michael W Mahoney. Newton-type methods for non-convex optimization
under inexact hessian information. Mathematical Programming, pp. 1-36, 2019. 2
Peng Xu, Fred Roosta, and Michael W. Mahoney. Second-order optimization for non-convex machine
learning: an empirical study. In Proceedings of the 2020 SIAM International Conference on Data
Mining, SDM 2020, Cincinnati, Ohio, USA, May 7-9, 2020, pp. 199-207. SIAM, 2020. 2
Peilin Zhao and Tong Zhang. Stochastic optimization with importance sampling for regularized loss
minimization. In Proceedings of the 32nd International Conference on Machine Learning ICML,
pp. 1-9, 2015. 1, 3
11
Under review as a conference paper at ICLR 2021
A	Discussion, Full Algorithms, and Proofs
For the sake of presentation, we consider the case where p = 2; higher dimensions follow from
the same approach, using tensor representation instead of matrix representation. Instead of viewing
the input matrix A = a1 ◦ . . . ◦ an ∈ Rn×d as a number of rows, we instead view the matrix
A = Ai ◦ ... ◦ An ∈ Rnd×d, where each matrix Ai = ai 0 ai is the outer product of the row ai
with itself.
A. 1 L2 POLYNOMIAL INNER PRODUCT SAMPLER
For ease of discussion, we describe in this section a data structure that allows sampling an index
i ∈ [n] with probability approximately kAxk1,2^ in linear time and sublinear space, where for
kAxk1,2,d
a matrix A ∈ Rnd×d, we use kAxk1,2,d to denote Pin=1 kAixk2, where each Ai ∈ Rd×d and
A = Ai ◦... ◦An. The generalization to a L2 polynomial inner product sampler follows immediately.
Notably, our data structure can be built simply given access to A, and will still sample from the
correct distribution when x is given as a post-processing vector. We first describe in Section A.1.1
some necessary subroutines that our sampler requires. These subroutines are natural generalizations
of the well-known frequency moment estimation algorithm of Alon et al. (1999) and heavy hitter
detection algorithm of Charikar et al. (2004). We then give the Li,2,d sampler in full in Section A.1.2.
A.1.1 Frequency Moment and Heavy Hitter Generalizations
We first recall a generalization to the frequency moment estimation algorithm by Alon et al. (1999)
that also supports post-processing multiplication by any vector x ∈ Rd.
Lemma A.1 (Mahabadi et al., 2020) Given a constant > 0, there exists a one-pass streaming
algorithm AMS that takes updates to entries of a matrix A ∈ Rn×d, as well as query access to post-
processing vectors X ∈ Rd and V ∈ Rd that arrive after the stream, and outputs a quantity F such
that (1 一 E) k Ax 一 v∣∣2 ≤ F ≤ (1 + E) ∣∣ Ax 一 v∣b. The algorithm uses O (-d (log2 n + log δ))
bits of space and succeeds with probability at least 1 - δ.
Algorithm 4 Basic algorithm COUNTSKETCH that outputs heavy submatrices of ∣Ax∣i,2,d, where
x is a post-processing vector
Input: Matrix A ∈ Rnd×d, post-processing vector x ∈ Rd, constant parameter E > 0.
Output: Slight perturbations of the vector Aix for which ∣Aix∣2 ≥ E ∣Ax∣i,2,d.
1:	r J Θ(log n) with a sufficiently large constant.
2:	b J Ω O with a sufficiently large constant.
3:	Let T be an r × b table of buckets, where each bucket stores an Rd×d matrix, initialized to the
zeros matrix.
4:	Let si ∈ {一1, +1} be 4-wise independent for i ∈ [n].
5:	Let hi : [n] → [b] be 4-wise independent for i ∈ [r].
6:	Process A:
7:	Let A = Ai ◦ . . . ◦ An , where each Ai ∈ Rd×d.
8:	for each j = 1 to n do
9:	for each i = 1 to r do
10:	Add sjAj to the matrix in bucket hi(j) of row i.
11:	Let Mi,j be the matrix in row i, bucket j ofT for i ∈ [r], j ∈ [b].
12:	Process x:
13:	for i ∈ [r], j ∈ [b] do
14:	Mi,j J Mi,jx
15:	On query k ∈ [n], report mediani∈[r] Mi,hi(k) 2.
Let Ai , . . . , An ∈ Rd×d and A = Ai ◦ . . . ◦ An ∈ Rnd×d . Let x ∈ Rd×i be a post-processing
vector that is revealed only after A has been completely processed. For a given E > 0, we say a
block Ai with i ∈ [n] is heavy if ∣Aix∣2 ≥ E ∣Ax∣i,2,d. We show in Algorithm 4 an algorithm that
12
Under review as a conference paper at ICLR 2021
processes A into a sublinear space data structure and identifies the heavy blocks of A once x is given.
Moreover, for each heavy block Ai , the algorithm outputs a vector y that is a good approximation to
Aix. The algorithm is a natural generalization of the CountSketch heavy-hitter algorithm introduced
by Charikar et al. (2004).
For a vector v ∈ Rnd×1, we use vtail(b) to denote v with the b blocks of d rows of v with the largest
`2 norm set to zeros.
LemmaA.2	There exists an algorithm	that uses	O (*d2	(log2 n + log δ) space that outputs
a vector yi	for each index i ∈ [n]	so that |	kyi k2 -	kAixk2 | ≤	Il(AX)taii(蓑)IL ≤
e Il(AX)taiι(j2)∣∣
Il(AX)taiι( W )∣∣2
1,2,d
≤
with probability at least 1 - δ. Moreover if Y = y1 ◦ . . . ◦ yn, then
AX-Yb∣∣2
≤ 2 Il(AX)tail( W )l∣2
with probability at least 1 - δ, where
Yb =Y-Ytail(%)denotes the top W blocks of Y by '2 norm.
Proof : Fix an index i ∈ [n]. Consider the estimate of kAiXk2 in row α of the CountSketch table
T. Then ha(i) is the bucket of T in row α to which AiX hashes. Let E1 be the event that the 妥
blocks of size d of Ax with the largest '2 norm are not hashed to hα(i). Observe that for b = Ω (£)
with sufficiently large constant, Ei occurs with probability at least = by a union bound.
Let v be the sum of the vectors representing the blocks that are hashed to bucket hα(i) excluding AiX,
so that v is the noise for the estimate of AiX in row α. Conditioned on E1, we can bound the expected
squared norm of the noise in bucket ha(i) for sufficiently large b by E [∣∣v∣∣2] ≤ 导 ∣∣(Ax)t°ii(%)∣∣ .
Hence we have Var(∣∣vik2) ≤ ^⅛ ∣∣(Ax)taii(g) ∣∣ . Thus from Jensen,s inequality, Chebyshev's
inequality and conditioning on E1 ,
Pr hkvik2 ≥"∣(AX)taw 1)(i ≤ 4 + 112 = 1.
The first claim then follows from the observation that ∣∣ (Ax)t°ii(%)∣∣2 ≤∣∣(AX)tail( W )∣∣	and
noting that we can boost the probability of success to 1 - poj(n) by repeating for each of the
r = Θ(log n) rows and taking the median.
Finally,observethat ∣∣(Ax)taii(2)∣∣ ≤ ∣∣ Ax - Y∣∣ , since Y has at most W nonzero blocks, while
(Ax)taii(g) has all zeros in the 妥 blocks of Ax with the largest '2 norm. Since Ax - Y alters at
most W rows of Ax, each by at most e ∣∣ (Ax)t°ii(W )∣L,then
Ax-Yb∣∣2
≤ t X ("13" f )∣L) =2∣∣(AX)* -22 )∣∣2.
i=1
口
A.1.2 Sampling Algorithm
Our approach is similar to 'p sampling techniques in Andoni et al. (2011); Jowhari et al. (2011), who
consider sampling indices in vectors, and almost verbatim to Mahabadi et al. (2020), who consider
sampling rows in matrices given post-processing multiplication by a vector.
The high level idea is to note that ifti ∈ [0, 1] is chosen uniformly at random, then
Pr
k Aik1,2,d
_	ti
≥ kAk1,2,d
k Aik1,2,d
Thus if Bi = AAi and there exists exactly one index i such that kBiki 2 d ≥ k Aki 2 then the task
would reduce to outputting Bj that maximizes kBj k1,2,d over all j ∈ [n]. In fact, we can show that
13
Under review as a conference paper at ICLR 2021
Bi is an O ()-heavy hitter of B with respect to the L1,2,d norm. Hence, we use a generalization of
COUNTSKETCH to identify the heavy hitters of B, approximate the maximum index i, and check
whether kBi k1,2,d is at least (an estimate of) kAk1,2,d.
Unfortunately, this argument might fail due to several reasons. Firstly, there might exists zero or
multiple indices i such that kBik1,2,d ≥ kAk1,2,d. Then the probability distribution that an index i
satisfies kBik1,2,d ≥ kAk1,2,d and that kBik1,2,d > kBj k1,2,d for all other j ∈ [n] is not the same
as the desired distribution. Fortunately, we show that this only happens with small probability, slightly
perturbing the probability of returning each i ∈ [n].
Another possibility is that the error in CountSketch is large enough to misidentify whether
kBik1,2,d ≥ kAk1,2,d. Using a statistical test, this case can usually be identified and so the algorithm
will be prevented from outputting a sample in this case. Crucially, the probability that the algorithm
is aborted by the statistical test is roughly independent of which index achieves the maximum. As
kAik1 2 d
a result, the probability of returning each i ∈ [n] is within a (1 ± e) factor of [因、1,2： When the
algorithm does not abort.
We show that the probability that algorithm succeeds is Θ(e) so then running O (log ɪ) instances of
the algorithm suffices to output some index from the desired distribution with constant probability,
or abort otherwise. Because the underlying data structure is a linear sketch, then it is also robust to
post-processing multiplication by any vector x ∈ Rd . Finally, we note that although our presentation
refers to the scaling factors ti as independent random variables, our analysis shows that they only
need to be O (1)-wise independent and thus we can generate the scaling factors in small space in the
streaming model. We give the L1,2,d sampler in Algorithm 5.
Algorithm 5 L1,2,d Sampler
Input: Matrix A ∈ Rnd×d with A = A1 ◦ . . . ◦ An , where each Ai ∈ Rd×d, vector x ∈ Rd×1 that
arrives after processing A, constant parameter e > 0.
Output: Noisy Aix of Ax sampled roughly proportional to kAixk2 .
1:	Pre-processing Stage:
2:	b J Ω (A), r J Θ(logn) with sufficiently large constants
3:	For i ∈ [n], generate independent scaling factors ti ∈ [0, 1] uniformly at random.
4:	Let B be the matrix consisting of matrices Bi = 十 Ai.
5:	Let ESTIMATOR and AMS track the L1,2,d norm of Ax and Frobenius norm of Bx, respectively.
6:	Let COUNTS KETCH be an r × b table, where each entry is a matrix in Rd×d.
7:	for each submatrix Ai do
8:	.Process A:
9:	Update COUNTSKETCH with Bi =春 Ai.
10:	Update linear sketch ESTIMATOR with Ai .
11:	Update linear sketch AMS with Bi = 1 Ai.
12:	Post-process x in AMS, COUNTSKETCH, and ESTIMATOR.	.Process x:
13:	Sample a submatrix:
14:	Use ESTIMATOR to compute F with kAxk1,2,d ≤ F ≤ 2 kAxk1,2,d.
15:	Extract the W (noisy) blocks of d rows of BX with the largest estimated '2 norms by
CountSketch.
16:	Let M ∈ Rnd×1 be the W -block sparse matrix consisting of these top (noisy) block.
17:	Use AMS to compute Sb with kBX - Mk2 ≤ Sb ≤ 2 kBX - Mk2 .
18:	Let ri be the (noisy) block of d rows in COUNTSKETCH with the largest norm.
19:	if S > F Jlog ∙∣ or ∣∣rik2 < 7Fb then
20:	Return FAIL.
21:	else
22:	Return r = tiri .
We first show that the probability that Algorithm 5 returns FAIL is independent of which index i ∈ [n]
achieves argmaxi∈[n] -1 ∣∣AiX∣2.
14
Under review as a conference paper at ICLR 2021
Lemma A.3 Let i ∈ [n] and fix a value of ti ∈ [0, 1] uniformly at random. Then conditioned on the
value of ti,
„ ʌ ʌ 1 TI ,、 ι
Pr S > FbA/log- = O (e) +——：—-.
poly(n)
Proof : We first observe that if we upper bound Sb by 4
kAxk1,2,d, then it suffices to show that the probability of 4
is small. Thus we define E1 as the event that:
(BX)tail(蓑 )∣∣2
II(BX)taw W )∣∣
and lower bound F by
>
2
1,2,d
... ^ ... ..
(1)	kAXk1,2,d ≤ Fb ≤ 2 kAXk1,2,d
^
(2)	kBX - MkF ≤ Sb≤ 2kBX-MkF
(3)Ilex"蓑)∣∣2 ≤ kBχ-MkF ≤2∣∣(Bχ)taii(W)∣∣2
Note that by Theorem 2.1, Lemma A.2 and Lemma A.1, E1 holds with high probability.
Let U = kAXk1,2,d. For each block AjX, we define yj to be the indicator variable for whether the
scaled block Bj X is heavy, so that yj =1	if kBj Xk2 > U and yj = 0 otherwise. We also define
zj ∈ [0, 1] as a scaled random variable for whether Bj X is light and how much squared mass it
contributes, Zj = 击 ∣∣Bjxk2 (1-y7-). Let Y = Pj= y7∙ be the total number ofheavy blocks besides
BiX and Z = Pj6=i zj be the total scaled squared mass of the small rows. Let h ∈ Rnd be the vector
that contains the heavy blocks so that coordinates (j - 1)d + 1 through jd of h correspond to Bjx if
yj = 1 and they are all zeros otherwise. Hence, h contains at most Y + 1 nonzero blocks and thus
at most (Y + 1)d nonzero entries. Moreover, U2Z = kBx - hk22 and ∣∣(Bx)tail(蓑)∣∣2 ≤ U√Z
unless Y ≥ W.
Thus if We define E? to be the event that Y ≥ W and E3 to be the event that Z ≥ 16^2 log ɪ IlAXkI 2 d，
then -E2 ∧ -E3 implies 4 ∣∣(Bx)taii(当)∣∣ ≤ Jlog £ ∣∣ Ax∣ι 2 d, so it suffices to bound the proba-
bility of the events E2 and E3 by O (e). Intuitively, if the number of heavy rows is small (-E2) and the
total contribution of the small rows is small (-E3), then the tail estimator is small, so the probability
of failure due to the tail estimator is small.
To analyze E2, note that yj = 1 if and only if 十 ∣∣Aj-x∣2 > U, so E [y∕ = kAjjxk2 and thus
E [Y] ≤ 1 since Y = Pj6=i yj and U = kAxk1,2,d = Pj kAjxk2. We also have Var(Y) ≤ 1 so
that Pr [E2] = O () for sufficiently small , by Chebyshev’s inequality.
To analyze E3, recall that Zj =方 ∣∣Bj-x∣2 (1 一 yj-). Thus Zj > 0 only if yj = 0 or equivalently,
IIBjXk2 ≤ U. Since BjX = -1 Ajx, then Zj > 0 only if tj ≥ 口燎丁2 . Therefore,
5 ∕∞j xk2”「= Z； xk- t2 U kAj x∣2 dtj≤jd
Since Z = j6=i Zj, then E [Z] ≤ 1 and similarly Var(Z) ≤ 1. Hence by Bernstein’s inequality,
Pr [Z> ⅛ log ɪ] = O (e), so then Pr [E3] = O (e). Thus Pr [-Ei ∨ E2 ∨ E3] = O (e) + poly(n),
as desired.	□
We now show that Algorithm 5 outputs a noisy approximation to Aix, where i ∈ [n] is drawn from
approximately the correct distribution, i.e., the probability of failure does not correlate with the index
that achieves the maximum value.
ɪ ____ A / L	「 i ，	「★∙，	,1	, Λ,	-,1 L ,	,	/	∙	∖	7	,	-	*
Lemma A.4 For a fixed value of F , the probability that Algorithm 5 outputs (noisy) submatrix Aix
is (1 ± O (e)) kAix⅛ + -ɪʒ.
F	poly(n)
15
Under review as a conference paper at ICLR 2021
Proof: Let E be the event that ti < 'kAiPk2 so that Pr [E] = 'kAixk2. Let Ei be the event that
FF
COUNTSKetch, AMS, or Estimator fails so that Pr [Ei] = po]；(n)by Lemma A.2, Lemma A.1,
and Theorem 2.1. Let E2 be the event that S > F
log
so that Pr [E2]
O () by Lemma A.3.
Let E3 be the event that multiple rows Bjx exceeding the threshold are observed in the CountSketch
data structure and E4 be the event that kBixk2 exceeds the threshold but is not reported due to noise
in the CountSketch data structure. Observe that E3 and E4 are essentially two sides of the same coin,
where error is incurred due to the inaccuracies of CountSketch.
1
'
To analyze E3 , note that row j 6= i can be reported as exceeding the threshold if kBj xk2 ≥
'Fb - Fb Jlog ', which occurs with probability at most O ɑkʌjjxk2). By a union bound over all
rows j ∈ [n] with j 6= i, then Pr [E3] = O ().
To analyze E4, We first condition on -Ei and -E2, so that ∣∣Bχ - M∣b ≤ S ≤ Fb Jlog '. Then by
Lemma A.2, the estimate Bix for Bix output by the sampler satisfies
∣kBixk2 - IIBixII2I ≤ "|(BX)taii(专)||F ≤ e kBx - MkF ≤ 店 ≤ CFʌ/iogɪ.
Hence, E4 can only occur for
1F ≤ IIBiXl卜 ≤ 1F + eF Jlog1,
e	2e	e
which occurs with probability at most O e2 .
To put things together, E occurs with probability 'kAbxk2, in which case the L1,2,d sampler should
output AiX. However, this may not happen due to any of the events E1, E2, E3, or E4. Since
Pr [E2 ∨E3 | E] = O (e) and Pr [E4] = O (e2), then we have Pr [E4 | E] = O (e). Moreover,
Pr [Ei] = po】y(n)so that index i is sampled with probability (1 + O (e)) kAibxk2. Finally by
Lemma A.2, ∣kBixk2 - ||dx|| ∣ ≤ eF Jlog ' and ∣∣dix∣∣ ≥ 'F. Hence ∣∣dix∣∣ is a (1 + e)
approximation to kBixk2 and therefore, ti Bdix is a (1 + ) approximation to kAixk2 .
□
Thus we have the following full guarantees for our L1,2,d sampler.
Theorem A.5 Given e > 0, there exists an algorithm that takes a matrix A ∈ Rnd×d, which can be
written as A = A1 ◦ . . . ◦ An, where each Ai ∈ Rd×d. After A is processed, the algorithm is given
a query vector X ∈ Rd and outputs a (noisy) vector AiX with probability (1 ± O (e)) ∣∣ 密xk2——+
kAxk1,2,d
po]:(n)∙ The algorithm uses log 1 ∙nnz(A)+poly (d, 1, log n) time, O (d (poly '，, log n) + log ∣))
bits of space, and succeeds with probability at least 1 - δ.
Proof : By Lemma A.4 and Theorem 2.1, then kAPk1,2,d ≤ Fb ≤ 2 kAPk1,2,d with high proba-
bility and so each vector AiX is sampled with probability (1 + e) 口党；口2 + po]；(n), conditioned
on the sampler succeeding. The probability that the sampler succeeds is Θ(e), so the sampler can
be repeated O (ɪ log n) times to obtain probability of success at least 1 - poj(n). Since each
instance of AMS, Estimator, and CountSketch use log ɪ ∙ nnz(A) + Poly (d, 1, log n) time
and O (d (poly (ɪ, log n) + log 1)) bits of space, then the total time and space complexity follow.
□
By adding sketches corresponding to different polynomial degrees, Theorem A.5 implies Theorem 2.2
and Theorem 3.2.
A.2 Leverage Score Sampler
our starting point is the input sparsity time algorithm of (Nelson & Nguyen, 2013) for approximating
the leverage scores, which is in turn a modification of (Drineas et al., 2012; Clarkson & Woodruff,
16
Under review as a conference paper at ICLR 2021
2013) Given an input matrix A, (Nelson & Nguyen, 2013) randomly samples a sparse matrix Π1
with O (备)rows and O (ɪ) signs per column, setting the remaining entries to be zero. (Nelson &
Nguyen, 2013) maintains Π1 A and post-processing, computes R-1 so that Π1 AR-1 has orthonor-
mal columns. Previous work of (Drineas et al., 2012) had shown that the squared row norms of
AR-1 are (1 + )-approximations to the leverage scores of A. Hence for a JL matrix Π2 that gives
(1 + )-approximations to the row norms of AR-1 , we can compute A(R-1 Π2) and output the row
norms of ARΠ2 as the approximate leverage scores for each row. Due to the sparsity of Π1 and Π2,
the total runtime is O (* ∙ nnz(A)). Computing R-1 takes additional O (宏)runtime.
Now since the squared row norms of AR-1 are (1 + )-approximations to the leverage scores of A,
it suffices to take the rows of AR-1 with large squared norms. To that effect, we randomly sample
a CountSketch matrix T and maintain TA. Once R-1 is computed, we can post-processing right
multiply to obtain TAR-1, similar to Algorithm 5. It follows that any row of TAR-1 that is at least
20lTd -heavy (with respect to squared Frobenius norm) has leverage score at least (ITd. Thus We
can obtain these rows by querying the CountSketch data structure while using space O (Td). Due to
the sparsity of the CountSketch matrix, the total runtime is O (nnz( A) + 冬).Finally, (Mahabadi
et al., 2020) show that the error guarantee on each reported heavy row required by Theorem 2.3.
By reporting the outer products of each of the heavy rows rather than the heavy rows, we obtain
Theorem 3.3.
A.3 Approximate SGD with Importance Sampling
Proof of Lemma 2.4: For any t ∈ [T] and i ∈ [n], k AiXtk2 ≥ K0Td k AxtkF only if there
exists a row in Ai whose leverage score is at least 品Td, since there are d rows in Ai. Algorithm 2
calculates a 2-approximation to each leverage score and maintains T separate instances of the L1,2,d
samplers for any matrix containing a row with approximate leverage score at least 品Td. Thus for
these indices i ∈ [n], we maintain T separate instances of the L1,2,d samplers for Ai by explicitly
maintaining the heavy row.
Otherwise, for allj ∈ [β] so that h(i) 6= j for any index i ∈ [n] such that kAixtk22 < 舟 kAxtk2F,
we have
X kAiXtk2 ≤ 焉 kAxtkF,
i:h(i)=j
with probability at least 盖 by Bernstein,s inequality and a union bound over j ∈ [β] for β = Θ(Td)
with sufficiently high constant. Intuitively, by excluding the hash indices containing “heavy” matrices,
the remaining hash indices contain only a small fraction of the mass with high probability. Then the
probability that any j ∈ [β] with i:h(i)=j IlAiXtk2 ≤ 击 kAxtk1,2,d is sampled more than once
is at most ɪo(T for any t ∈ [T] provided there is no row in any Ai with h(i) = j whose '2 leverage
score is at least ɪdTd. Thus, the probability that some bucket j ∈ [β] is sampled twice across T
steps is at most .(^^产 ≤ 忐.
In summary, we maintain T separate instances of L1,2,d samplers for the heavy matrices and one
L1,2,d sampler for each hash index that does not contain a heavy matrix. With probability at least
普,any hash index not containing a heavy matrix is sampled only once, so each time t ∈ [T] has
access to a fresh L1,2,d sampler.	□
B Empirical Evaluations
We again emphasize that our primary contribution is the theoretical design of a nearly input sparsity
time streaming algorithm that simulates the optimal importance sampling distribution for variance
reduction in stochastic gradient descent without computing the full gradient. Thus our theory is
optimized to minimize the number of SGD iterations without asymptotic wall-clock time penalties;
we do not attempt to further optimize wall-clock runtimes. Nevertheless, in this section we implement
a scaled-down version of our algorithm and compare its performance across multiple iterations
on large-scale real world data sets to SGD with uniform sampling on both linear regression and
17
Under review as a conference paper at ICLR 2021
eulaV evitcejbO
eulaV evitcejbO
0
0	10	20	30	40
SGD Iterations
(a)	Step size η = 1 × 10-13
0 ----------⅜m□em□=□cc口口二□zm□qa⅛
0	10	20	30	40
SGD Iterations
(b)	Step size η = 5 × 10-12
0
0	10	20	30	40
SGD Iterations
(c)	Step size η = 1 × 10-12
eulaV evitcejb
Fig. 3: Comparison of importance sampling (in blue squares) and uniform sampling (in red triangles)
over various step-sizes for linear regression on CIFAR-10.
support-vector machines (SVMs). Because most rows have roughly uniformly small leverage scores
in real-world data, we assume that no bucket contains a row with a significantly large leverage score
and thus the implementation of our importance sampling algorithm does not create multiple samplers
for any buckets. By similar reasoning, our implementation uniformly samples a number of indices i
and estimates kAxk1,2,d = Pj kAj xk1,2,d by scaling up kAixk1,2,d. Observe that although these
simplifications to our algorithm decreases the wall-clock running time and the total space used by our
algorithm, they only decrease the quality of our solution for each SGD iteration. Nevertheless, our
implementations significantly improve upon SGD with uniform sampling. The experiments in this
section were performed on a Dell Inspiron 15-7579 device with an Intel Core i7-7500U dual core
processor, clocked at 2.70 GHz and 2.90 GHz, in contrast to the logistic regression experiments that
were performed on a GPU.
Linear Regression. We performed linear regression on the CIFAR-10 dataset to compare the
performance of our importance sampling algorithm to the uniform sampling SGD algorithm. We
trained using a data batch of 100000 points and 3072 features and tested the performance on a
separate batch of data points. We aggregated the objective values across 10 separate instances. Each
instance generated a random starting location as an initial position for both importance sampling and
uniform sampling. We then ran 40 iterations of SGD for each algorithm and observed the objective
value on the test data for each of these iterations. Finally, we computed the average performance
on each iteration across these 10 separate instances. As we ran our algorithm for 40 iterations, we
created 1600 buckets that partitioned the data values for the importance sampling algorithm.
The sampled gradients were generally large in magnitude for both importance sampling and uniform
sampling and thus we required small step-size. For step-sizes η = 1 × 10-13, η = 5 × 10-12, and
η = 1 × 10-12, the objective value of the solution output by our importance sampling algorithm
quickly and significantly improved over the objective value of the solution output by uniform sampling.
Our algorithm performance is much more sensitive to the choice of larger step-sizes, as choices of
step-sizes larger than 5 × 10-11 generally caused the importance sampling algorithm to diverge,
while the uniform sampling algorithm still slowly converged. We give our results in Figure 3.
Support-Vector Machines. We also compared the performance of our importance sampling al-
gorithm to the uniform sampling SGD algorithm using support-vector machines (SVM) on the a9a
Adult data set collected by UCI and retrieved from LibSVM (Chang & Lin, 2011). The features
correspond to responses from the 1994 Census database and the prediction task is to determine
whether a person makes over 50K USD a year. We trained using a data batch of 32581 points
and 123 features and tested the performance on a separate batch of 16281 data points. We assume
the data is not linearly separable and thus use the hinge loss function so that we aim to minimize
* pn=1 max(0,1 - yjw ∙ Xi — b)) + λ ∣∣w∣∣2, where X is the data matrix, yi is the corresponding
label, and w is the desired maximum-margin hyperplane. For each evaluation, we generated 10
random initial positions shared for both importance sampling and uniform sampling. We then ran 75
iterations of SGD for each algorithm, creating 1125 buckets for the importance sampling algorithm
and computed the average performance on each iteration across these 5 separate instances.
18
Under review as a conference paper at ICLR 2021
The sampled gradients were generally smaller than those from linear regression on CIFAR-10 and
thus we were able to choose significantly larger step-sizes. Nevertheless, our algorithm performance
was sensitive to both the step-size and the regularization parameter. For step-sizes η = 0.25, η = 0.5
and regularization parameters λ = 0, λ = 0.001 and λ = 0.0001, the objective value of the solution
output by our importance sampling algorithm quickly and significantly improved over the objective
value of the solution output by uniform sampling. We give our results in Figure 5. Our algorithm
performance degraded with larger values of λ, as well step-sizes larger than η = 1.
We also compared step-size η = 1 and regularization parameters λ = 0, λ = 0.001 and λ = 0.0001
with a hybrid sampling scheme that selects the better gradient between importance sampling and
uniform sampling at each step, as well as a hybrid sampling scheme that uses afew steps of importance
sampling, followed by uniform sampling in the remaining steps. Our experiments show that the hybrid
sampling algorithms perform better at the beginning and thus our importance sampling algorithm may
be used in conjunction with existing techniques in offline settings to accelerate SGD. Surprisingly, the
hybrid sampling algorithms do not necessarily remain better than our importance sampling algorithm
thus indicating that even if uniform sampling were run for a significantly larger number of iterations,
its performance may not exceed our importance sampling algorithm. We give our results in Figure 6.
Finally, we compare wall-clock times of each of the aforementioned sampling schemes with step-size
η = 1 and regularization 0 across 100 iterations. Our results in Figure 4 show that as expected,
uniform sampling has the fastest running time. However, each iteration of importance sampling
takes about 15 iterations of uniform sampling, which empirically shows that even using wall-clock
times for comparison, rather than total number of SGD iterations, the performance of our importance
sampling algorithm still surpasses that of uniform sampling. Moreover, the runtime experiments
reveal the main bottleneck of our experiments: each of the 100 iterations took approximately 70
seconds on average after including the evaluation of the objective on each gradient step.
0.15
0.13
0.1
7.5 ∙ 10-
10-
2.5 ∙ 10-
25
50
75
SGD Iterations
Fig. 4:	Runtimes comparison for SVM on a9a Adult dataset from LibSVM/UCI with step size 1.0
and regularization 0, averaged over 100 iterations: importance sampling (in blue squares), uniform
sampling (in red triangles), hybrid sampling that chooses the better gradient at each step (in purple
circles), and hybrid sampling that performs 25 steps of importance sampling followed by uniform
sampling (in teal X’s). By comparison, the average total runtime over 100 iterations was 72.5504
seconds, including the computation of the scores.
19
Under review as a conference paper at ICLR 2021
5 15 5 5
2 7. 2
. . 0.
100
eulaV evitcejbO
0
25	50
SGD Iterations
75
eulaV evitcejbO
0
25	50
SGD Iterations
75
eulaV evitcejbO
5 15 5 5
2 7. 2
. . 0.
100
0
25	50
SGD Iterations
75
(a) Step size 0.25, Regularization (b) Step size 0.25, Regularization (c) Step size 0.25, Regularization
0	0.001	0.0001
∙105	∙105	∙105
eulaV evitcejbO
0
25	50
SGD Iterations
75
eulaV evitcejbO
0
25	50
SGD Iterations
75
eulaV evitcejbO
0
25	50
SGD Iterations
75
(d) Step size 0.5, Regularization (e) Step size 0.5, Regularization (f) Step size 0.5, Regularization
0	0.001	0.0001
Fig. 5:	Comparison of importance sampling (in blue squares) and uniform sampling (in red triangles)
over various step-sizes for SVM on a9a Adult dataset from LibSVM/UCI, averaging across ten
iterations, averaged over 10 iterations.
∙105
.
0
eulaV evitcejbO
0	25	50	75
SGD Iterations
eulaV evitcejbO
∙105
0	25	50	75
SGD Iterations
∙105
0	25
50	75
SGD Iterations
(a) Step size 1.0, Regularization (b) Step size 1.0, Regularization (c) Step size 1.0, Regularization
0	0.001	0.0001
Fig. 6:	Comparison of importance sampling (in blue squares), uniform sampling (in red triangles),
hybrid sampling that chooses the better gradient at each step (in purple circles), and hybrid sampling
that performs 25 steps of importance sampling followed by uniform sampling (in teal X’s) over
various step-sizes for SVM on a9a Adult dataset from LibSVM/UCI, averaged over 10 iterations
20