Under review as a conference paper at ICLR 2021
A Reinforcement Learning Framework for
Time Dependent Causal Effects Evaluation in
A/B Testing
Anonymous authors
Paper under double-blind review
Ab stract
A/B testing, or online experiment is a standard business strategy to compare a new
product with an old one in pharmaceutical, technological, and traditional indus-
tries. The aim of this paper is to introduce a reinforcement learning framework for
carrying A/B testing in two-sided marketplace platforms, while characterizing the
long-term treatment effects. Our proposed testing procedure allows for sequential
monitoring and online updating. It is generally applicable to a variety of treatment
designs in different industries. In addition, we systematically investigate the theo-
retical properties (e.g., size and power) of our testing procedure. Finally, we apply
our framework to both synthetic data and a real-world data example obtained from
a technological company to illustrate its advantage over the current practice.
1	Introduction
A/B testing, or online experiment is a business strategy to compare a new product with an old one
in pharmaceutical, technological, and traditional industries (e.g., Google, Amazon, or Facebook).
Most works in the literature focus on the setting, in which observations are independent across time
(see e.g. Johari et al., 2015; 2017, and the references therein). The treatment at a given time can
impact future outcomes. For instance, in a ride-sharing company (e.g., Uber), an order dispatching
strategy not only affects its immediate income, but also impacts the spatial distribution of drivers in
the future, thus affecting its future income. In medicine, it usually takes time for drugs to distribute
to the site of action. The independence assumption is thus violated.
The focus of this paper is to test the difference in long-term treatment effects between two products
in online experiments. There are three major challenges as follows. (i) The first one lies in modelling
the temporal dependence between treatments and outcomes. (ii) Running each experiment takes a
considerable time. The company wishes to terminate the experiment as early as possible in order
to save both time and budget. (iii) Treatments are desired to be allocated in a manner to maximize
the cumulative outcomes and to detect the alternative more efficiently. The testing procedure shall
allow the treatment to be adaptively assigned.
We summarize our contributions as follows. First, we introduce a reinforcement learning (RL, see
e.g., Sutton & Barto, 2018, for an overview) framework for A/B testing. In addition to the treatment-
outcome pairs, itis assumed that there is a set of time-varying state confounding variables. We model
the state-treatment-outcome triplet by using the Markov decision process (MDP, see e.g. Puterman,
1994) to characterize the association between treatments and outcomes across time. Specifically,
at each time point, the decision maker selects a treatment based on the observed state. The system
responds by giving the decision maker a corresponding outcome and moving into a new state in the
next time step. In this way, past treatments will have an indirect influence on future rewards through
its effect on future state variables. In addition, the long-term treatment effects can be characterized
by the value functions (see Section 3.1 for details) that measure the discounted cumulative gain from
a given initial state. Under this framework, it suffices to evaluate the difference between two value
functions to compare different treatments. This addresses the challenge mentioned in (i).
Second, we propose a novel sequential testing procedure for detecting the difference between two
value functions. To the best of our knowledge, this is the first work on developing valid sequential
tests in the RL framework. Our proposed test integrates temporal difference learning (see e.g., Pre-
cup et al., 2001; Sutton et al., 2008), the α-spending approach (Lan & DeMets, 1983) and bootstrap
1
Under review as a conference paper at ICLR 2021
(Efron & Tibshirani, 1994) to allow for sequential monitoring and online updating. Itis generally ap-
plicable to a variety of treatment designs, including the Markov design, the alternating-time-interval
design and the adaptive design (see Section 4.4). This addresses the challenges in (ii) and (iii).
Third, we systematically investigate the asymptotic properties of our testing procedure. We show
that our test not only maintains the nominal type I error rate, but also has non-negligible powers
against local alternatives. To our knowledge, these results have not been established in RL.
Finally, we introduce a potential outcome framework for MDP. We state all necessary conditions
that guarantee that the value functions are estimable from the observed data.
2	Related work
There is a huge literature on RL such that various algorithms are proposed for an agent to learn an
optimal policy and interact with an environment. Our work is closely related to the literature on off-
policy evaluation, whose objective is to estimate the value of a new policy based on data collected
by a different policy. Popular methods include Thomas et al. (2015); Jiang & Li (2016); Thomas
& Brunskill (2016); Liu et al. (2018); Farajtabar et al. (2018); Kallus & Uehara (2019). Those
methods required the treatment assignment probability (propensity score) to be bounded away from
0 and 1. As such, they are inapplicable to the alternating-time-interval design, which is the treatment
allocation strategy in our real data application.
Our work is related to the temporal-difference learning method based on function approximation.
Convergence guarantees of the value function estimators have been derived by Sutton et al. (2008)
under the setting of independent noise and by Bhandari et al. (2018) for Markovian noise. However,
uncertainty quantification of the resulting value function estimators have been less studied. Such
results are critical for carrying out A/B testing. Luckett et al. (2019) outlined a procedure for es-
timating the value under a given policy. Shi et al. (2020b) developed a confidence interval for the
value. However, these methods do not allow for sequential monitoring or online updating.
In addition to the literature on RL, our work is also related to a line of research on evaluating time-
varying causal effects (see e.g. Robins, 1986; Boruvka et al., 2018; Ning et al., 2019; Rambachan
& Shephard, 2019; Viviano & Bradic, 2019; Bojinov & Shephard, 2020). However, none of the
above cited works used an RL framework to characterize treatment effects. In particular, Bojinov &
Shephard (2020) proposed to use importance sampling (IS) based methods to test the null hypothesis
of no (average) temporal causal effects in time series experiments. Their causal estimand is different
from ours since they focused on p lag treatment effects, whereas we consider the long-term effects
characterized by the value function. Moreover, their method requires the propensity score to be
bounded away from 0 and 1, and thus it is not valid for our applications.
Furthermore, our work is also related to the literature on sequential analysis (see e.g. Jennison &
Turnbull, 1999, and the references therein), in particular, the α-spending function approach that
allocates the total allowable type I error rate at each interim stage according to an error-spending
function. Most test statistics in classical sequential analysis have the canonical joint distribution
(see Equation (3.1), Jennison & Turnbull, 1999) and their associated stopping boundary can be
recursively updated via numerical integration. However, in our setup, test statistics no longer have
the canonical joint distribution when adaptive design is used. This is due to the existence of the
carryover effects in time. We discuss this in detail in Appendix C. To resolve this issue, we propose
a scalable bootstrap-assisted procedure to determine the stopping boundary (see Section 4.3).
Recently, there is a growing literature on bringing classical sequential analysis to A/B testing. In
particular, Johari et al. (2015) proposed an always valid test based on the classical mixture sequential
probability ratio tests (mSPRT). Kharitonov et al. (2015) propose modified versions of the O’Brien
& Fleming and MaxSPRT sequential tests. Deng et al. (2016) studied A/B testing under Bayesian
framework. Abhishek & Mannor (2017) developed a bootstrap mSPRT. These tests cannot detect
the carryover effects in time, leading to low statistical power in our setup. See the toy examples in
Section 4.1 for detailed illustration.
In addition, we note that there is a line of research on bandit/RL with causal graphs (see e.g., Lee
& Bareinboim, 2018; 2019). We remark that the problems considered and the solutions developed
in this article are different from these works. Specifically, these works considered applying causal
inference methods to deal with unmeasured confounders in bandit/RL settings whereas we apply the
RL framework to evaluate time-dependant causal effects.
2
Under review as a conference paper at ICLR 2021
Finally, we relax several key conditions used in Ertefaie (2014) and Luckett et al. (2019) that pre-
sented a potential outcome framework for MDP (see Section 3.1 for details). Specifically, Ertefaie
(2014) and Luckett et al. (2019) imposed the Markov conditions on the observed data rather than
the potential outcomes, while assuming that the outcome at time t is a deterministic function of the
state variables at time t, t + 1 and the treatment at time t.
3	Problem formulation
3.1	A potential outcome framework for MDP
For simplicity, we assume that there are only two treatments (actions, products), coded as 0 and 1,
respectively. For any t ≥ 0, let a = (ao, aι, ∙∙∙ , at)> ∈ {0,1}t+1 denote a treatment history
vector up to time t. Let S denote the support of state variables and S0 denote the initial state
variable. We assume S is a compact subset of Rd. For any (at-ι ,at), let St(at-ι) and K*(at) be
the counterfactual state and counterfactual outcome, respectively, that would occur at time t had the
agent followed the treatment history aat. The set of potential outcomes up to time t is given by
Wt*(at) = {So,埒(ao),S；(ao),…，S；(at-i),Y7(at)}.
Let W * = ∪t≥o,at∈{0,i}t+ι Wt (at) be the set of all potential outcomes.
A deterministic policy π is a function that maps the space of state variables to the set of available
actions. For any such π, let πat denote the treatment history up to time t, assigned according to π.
We use St*(πat-1) and Yt*(πat) to denote the associated potential state and outcome that would occur
at time t had the agent followed π . The goodness of a policy π is measured by its value function,
V(π; S) = X YtE{γt*(πt)lS0 = s},
t≥0
where 0 < γ < 1 is a discounted factor that reflects the trade-off between immediate and future
outcomes. Note that our definition of the value function is slightly different from those in the existing
literature (see Sutton & Barto, 2018, for example). Specifically, V(π; S) is defined through potential
outcomes rather than the observed data. Similarly, we define the Q function by
Q(π; a, S) = X γtE{Yt* (πat (a))|S0 = S},
t≥0
where {πat(a)}t≥0 denotes the treatment history where the initial action equals to a and all other
actions are assigned according to π.
In our setup, we focus on two nondynamic policies that assign the same treatment at each time point.
We use their value functions (denote by V(1; ∙) and V(0; ∙)) to measure their long-term treatment
effects. Meanwhile, our proposed method is equally applicable to the dynamic policy scenario
as well. See Section B.1 for details. To quantitatively compare the two policies, we introduce the
Average Treatment Effect (ATE) based on their value functions which relates RL to causal inference.
Definition. For a given reference distribution function G, ATE is defined by the integrated difference
between two value function, i.e., ATE = Rs{V(1; S) - V(0; S)}G(dS).
The focus of this paper is to test the following hypotheses:
H0 : τ0 = ATE ≤ 0 v.s H1 : τ0 = ATE > 0.
When H0 holds, the new product is no better than the old one.
3.2	Identifiability of ATE
One of the most important question in causal inference is the identifiability of causal effects. In this
section, we present sufficient conditions that guarantee the identifiability of the value function.
In practice, with the exception of S0, the set W* cannot be observed, whereas at time t, we observe
the state-action-outcome triplet (St, At, Yt). For any t ≥ 0,let At = (Ao, Ai,…，At)> denote
the observed treatment history. We first introduce two conditions that are commonly assumed in
multi-stage decision making problems (see e.g. Murphy, 2003; Zhang et al., 2013; Kennedy, 2019).
(CA) Consistency assumption: St+1 = St*+1(Aat) and Yt = Yt*(Aat) for all t ≥ 0, almost surely.
(SRA) Sequential randomization: At is independent of W* given St and {Sj, Aj, Yj}o≤j<t.
3
Under review as a conference paper at ICLR 2021
The SRA implies that there are no unmeasured confounders and it automatically holds in online
experiments, in which the treatment assignment mechanism is pre-specified. In SRA, we allow
At to depend on the observed data history St , {Sj , Aj , Yj }0≤j<t and thus, the treatments can be
adaptively chosen. We next introduce two conditions that are unique to the RL setting.
(MA) Markov assumption: there exists a Markov transition kernel P such that for any t ≥ 0,
而 ∈ {0,1}t+1 and S⊆ Rd, WehavePr {S；+i(at) ∈S∣Wj(而)} = P (S; at,S；(而-ι)).
(CMIA) Conditional mean independence assumption: there exists a function r such that for any
t ≥ 0, a ∈{0,1}t+1,wehaveE{K*(而)∣Sj(at-ι),Wt-ι(at-ι)} = r(at,S；(而-ι)).
These tWo conditions are central to the empirical validity of reinforcement learning (RL). Specifi-
cally, under these two conditions, there exists an optimal policy ∏* such that V(∏*; S) ≥ V(∏; S)
for any π and s. We observe that Ertefaie (2014) and Luckett et al. (2019) imposed the Markov
conditions on the observed data rather than the potential outcomes. When CA and SRA hold, these
assumptions are equivalent. When SRA is violated, their Markov assumptions could be violated
as the treatment depends on unobserved confounders and the observed data process is no longer
Markovian. CMIA requires past treatments to affect Yt*(αt) only through its impact on St(αt-ι).
In other words, the state variables shall be chosen to include those that serve as important mediators
between past treatments and current outcomes. Under MA, CMIA is automatically satisfied when
Y7(θ⅛) is a deterministic function of (St+ι(a), at, St(at-ι)) that measures the system,s status at
time t + 1. The latter condition is commonly imposed in the reinforcement learning literature.
To conclude this section, we derive a version of Bellman equation for the Q function under the
potential outcome framework. Specifically, for α0 ∈ {0,1}, let Q(a0; ∙, ∙) denote the Q function
where treatment a0 is repeatedly assigned after the initial decision.
Lemma 1 Under MA, CMIA, CA and SRA, for any t ≥ 0, a0 ∈ {0,1} and any function 夕：
S X {0,1} → R, we have E[{Q(a0; At, St)- Yt — γQ(a0; a0, St+ι)}^(St, At)] = 0∙
Sketch of Proof: Under MA, CMIA, CA, SRA, the defined Q-function under the potential outcome
framework is the same as that defined on the observed data. Lemma 1 thus follows from the classical
Bellman equation (see Equation (4.6) in Sutton & Barto, 2018).
Lemma 1 implies that the Q-function is estimable from the observed data. Specifically, an estimating
equation can be constructed based on Lemma 1 and the Q-function can be learned by solving this
estimating equation. Note that V(a, S) = Q(a; a, S) and τ0 is completely determined by the value
function V . As a result, τ0 is estimable from the observed data as well. We remark that the positivity
assumption is not needed in Lemma 1. Our procedure can thus handle the case where treatments are
deterministically assigned, i.e., the behavior policy b is deterministic. This is due to MA and CMIA
that assume the system dynamics are invariant across time. To elaborate this, note that the discounted
value function is completely determined by the transition kernel P and the reward function r. We
remark that these quantities can be consistently estimated under certain conditions (see C1-C3 in
Appendix E), regardless of whether b is deterministic or not.
4	Testing procedure
We first introduce a toy example to illustrate the limitations of existing A/B testing methods. We
next present our method and prove its consistency under a variety of different treatment designs.
4.1	Toy examples
Existing A/B testing methods can only detect short-term treatment effects, but fail to identify any
long-term effects. To elaborate this, we introduce two examples below.
Example 1.	St = 0.5εt, Yt = St + δAt for any t ≥ 1 and S0 = 0.5ε0.
Example 2.	St = 0.5St-1 + δAt + 0.5εt, Yt = St for any t ≥ 1 and S0 = 0.5ε0.
In both examples, the random errors {εt}t≥0 follow independent standard normal distributions and
the parameter δ describes the degree of treatment effects. Suppose δ > 0. Then H1 holds. In
Example 1, the observations are independent and there are no carryover effects at all. In this case,
both the existing A/B tests and the proposed test are able to discriminate H1 from H0. In Example 2,
however, treatments have delayed effects on the outcomes. Specifically, Yt does not depend on At,
but is affected by At-1 through St. Existing tests will fail to detect H1 as the short-term conditional
4
Under review as a conference paper at ICLR 2021
Example 1	∣	Example 2
t-test 0.76 I DML-based test 1 ∣ our test0.98 ∣ t-test0.04 ∣ DML-based test 0.06 ∣ ourtest0.73
Table 1: Powers of t-test, DML-based test and the proposed test under Examples 1 and 2, with T = 500,
δ = 0.1. {At}t follow i.i.d. Bernoulli distribution with success probability 0.5.
average treatment effects E(Yt|At = 1, St) - E(Yt|At = 0, St) = 0 in this example. As an
illustration, we conduct a small experiment by assuming the decision is made once at T = 500, and
report the empirical rejection probability of the classical two-sample t-test that is commonly used in
online experiments, a more complicated test based on the double machine learning method (DML,
Chernozhukov et al., 2017) that is widely employed for inferring causal effects, and the proposed
test. It can be seen the competing methods do not have any power under Example 2.
4.2	An overview of the proposal
First, we estimate τ0 based on a version of temporal difference learning. The idea is to apply basis
function approximations to solve an estimating equation derived from Lemma 1. Specifically, let
Q = {Ψ>(s)βa0,a : βao,a ∈ Rq} be a large linear approximation space for Q(a0; a, s), where Ψ(∙)
is a vector containing q basis functions on S. The dimension q is allowed to depend on the number of
samples T to alleviate the effects of model misspecification. Let us suppose Q ∈ Q for a moment.
By Lemma 1, there exists some β = (βo>, βo> ,βj>,βj>)> such that
E[{Ψ>(St)β"a - Yt - γΨ>(St+ι)β"a,}Ψ(St)I(At = a)] = 0, ∀a, a0 ∈ {0,1},
where I(∙) denotes the indicator function. Let ξ(s, a) = {Ψ>(s)I(a = 0), Ψ>(s)I(a = 1)}>. The
above equations can be rewritten as E(∑tβ*) = E"t, where ∑t is a block diagonal matrix given by
Σ	ξ(St, At){ξ(St, At) -γξ(St+1,0)}>
Σt=	ξ(St, At){ξ(St, At) - γξ(St+1, 1)}>
and ηt = {ξ(St,At)>Yt,ξ(St,At)>Yt}>.
Let Σ(t) = t-1 Pj<t Σj and η(t) = t-1 Pj<t η. It follows that E{Σ(t)β*} = E{η(t)}. This
motivates us to estimate β* by β(t) = {β>o(t),b>ι(t),b>o(t),b>ι(t)}> = ∑-1(t)b(t). ATE
can thus be estimated by the plug-in estimator τb(t) = s Ψ>(s){β1,1 (t) - βo,o(t)}G(ds).
Second, We use b(t) to construct our test statistic at time t. We will show √7{τ(t) - τo} is asymptot-
ically normal. Its variance can be consistently estimated by σ2(t) = U l Σ 1(t)Ω(t)(Σ 1(t)} l U,
as t grows to infinity, where U = {- Rs∈S Ψ(s)>G(ds), 0q>, 0q>, Rs∈S Ψ(s)>G(ds)}>, 0q denotes a
zero vector of length q, and Ω(t) corresponds to some consistent covariance estimator of η based
on the data observed at time t (see equation 3 for the explicit form). This yields our test statistic
√tb(t)∕b(t), at time t.
Third, we integrate the α-spending approach with bootstrap to sequentially implement our test (see
Section 4.3). The idea is to generate bootstrap samples that mimic the distribution of our test statis-
tics, to specify the stopping boundary at each interim stage. Suppose that the interim analyses are
conducted at time points Ti < …<Tk = T. For each 1 ≤ k < K, we assume Tk/T → Ck for
some constants 0 < ci < …< cκ-ι < 1.
4.3	Sequential monitoring and online updating
Let {Zι, .…，ZK} denote the sequence of our test statistics, where Zk = √Tk不(Tk)/b(Tk). To
sequentially monitor our test, we need to specify the stopping boundary {bk}1≤k≤K such that the
experiment is terminated and Ho is rejected when Zk > bk for some k.
First, we use the α spending function approach to guarantee the validity of our test. It requires to
specify a monotonically increasing function α(∙) that satisfies α(0) = 0 and α(T) = α. Some
popular choices of the α spending function include
αι(t) = 2 — 2Φ{Φ-1(1 — α∕2)pT∕t} and α2(t) = α(t∕T)θ for θ > 0,	(1)
where Φ(∙) denotes the normal cumulative distribution function. Adopting the α spending approach,
we require bk’s to satisfy
Pr(∪jk=1{Zj > bj}) = α(Tk) + o(1),	∀1 ≤ k ≤ K.	(2)
5
Under review as a conference paper at ICLR 2021
As commented in the introduction, the numerical integration method is not applicable to determine
the stopping boundary. Our method is built upon the wild bootstrap (Wu et al., 1986). The idea is to
generate bootstrap samples that have asymptotically the same joint distribution as the test statistics.
However, we note that directly applying the wild bootstrap algorithms is time consuming. See
Section C for details. To facilitate the computation, we present a scalable bootstrap algorithm to
determine {bk}k. Let {ek}k be a sequence of i.i.d N(0, I4q) random vectors, where IJ stands for a
J X J identity matrix for any J. Let Ω(T0) be an 4q X 4q zero matrix. At the k-th stage, We compute
Zk = U√∑σ1TTk) X{TjΩ(Tj) - Tj-1Ω(Tj-1)}1/2e,
j=1
A key observation is that, conditional on the observed dataset, the covariance of Z^ι and Zk2 equals
that of Zk1 and Zk2. See Theorem 3 for details. In addition, the limiting distributions of {Zk}k and
{Zk}k are multivariate normal. As such, the joint distribution of {Zk }k can be well approximated
by that of {Zk}k conditional on the data. This forms the basis of our bootstrap algorithm. By the
requirement on {bk}k in 2, we obtain Pr(max1≤j<k(Zj - bj) ≤ 0, Zk > bk) = α(Tk) - α(Tk-1) +
o(1). To implement our test, we recursively calculate the threshold bbk as follows,
Pr* f max (Zj - bj) ≤ 0, Zk > b J = α(Tk) - α(Tk-1),
1≤j<k
where Pr* denotes the conditional probability given on the data, and reject H when Zk > bk for
some k. In practice, the left-hand-side can be approximated via Monte carlo simulations.
4.4	Consistency under different treatment designs
We consider three treatment allocation designs that can be handled by our procedure as follows:
D1. Markov design: Pr(At = 1|St, {Sj, Aj, Yj}o≤j<t) = b(0)(St) for some function b(0)(∙) uni-
formly bounded away from 0 and 1.
D2. Alternating-time-interval design: A2j = 0, A2j +1= 1 for all j ≥ 0.
D3. Adaptive design (e.g., -greedy): For Tk ≤ t < Tk+1 for some k ≥ 0, Pr(At =
1|St, {Sj,Aj,Yj}o≤j<t) = b(k) (St) for some b(k)(∙) that depends on {Sj, Aj, Yj}0≤∙<Tk.
Here, D2 is a deterministic design and is widely used in industry (see our real data example). D1 and
D3 are random designs. D1 is commonly assumed in the literature on off-policy evaluation (see e.g.,
Jiang & Li, 2016). D3 is widely employed in the contextual bandit setting to balance the trade-off
between exploration and exploitation. These three settings cover a variety of scenarios in practice.
Theorem 1 (Type-Ierror) Suppose α(∙) is continuous, C1-C3 (see Appendix E) hold and q =
o(√T/ logT). ThenPr(Sj=]{Zj > bj}) ≤ α(Tk) + o(1),forall 1 ≤ k ≤ K under H.
Sketch of Proof: We consider the case where τ0 = 0 only. The general case is proven in Section F.3.
As discussed in Section 4.3, the conditional distribution of{Zk*}k given the data is equivalent as the
distribution of{Zk}k. Since {bk}k is a continuous function of{Zk*}k, it follows from the continuous
mapping theorem that {bk}k are consistent. The proof is hence completed.
Theorem 1	implies that the type-I error rate of the proposed test is well controlled. When ATE= 0,
the equality in Theorem 1 holds. The rejection probability achieves the nominal level under H0 .
Theorem 2	(Power) Under the conditions of Theorem 1, assume τ0	T-1/2, then Pr(Z1 >
bb1) → 1. Assume τ0 = T-1/2hfor some h > 0. Then limT→∞[Pr(∪jk=1{Zj > bbj}) - α(Tk)] > 0.
Sketch of Proof: Under Hi, similar to Theorem 1, we have Pr (∪k=1{Zj — Pprj τo∕bb(Tj) > bj }=
a(Tk) + o(1). The assertionfollows by that Zj is stochastically larger than Zj 一 TjT0τo for all j.
The second assertion in Theorem 2 implies that our test has non-negligible powers against local
alternatives converging to H0 at the T -1/2 rate. When the signal decays to zero faster than this rate,
our test is not able to detect H1. When the signal decays at a slower rate, the power of our test
approaches 1. Combining Theorems 1 and 2 yields the consistency of our test.
Finally, it is worth mentioning that our test can be online updated as batches of observations arrive
at the end of each interim stage. We summarize our procedure in Algorithm 1 (see Appendix A). Its
time complexity is dominated by O(Bq3 + Tq2 ).
6
Under review as a conference paper at ICLR 2021
(a) The proposed test under H1 (left) and H0 (right)	(b) Two-sample t-test under H1 (left) and H0 (right)
Figure 1: Empirical rejection probabilities of our test and the two-sample t-test with α(∙) = αι (∙). Settings
correspond to the alternating-time-interval, adaptive and Markov design, from top plots to bottom plots.
5	Numerical studies
5.1	Synthetic data
Simulated data of states and rewards was generated as follows,
S1,t =	(2At-1	- 1)S1,(t-1)/2 +	S2,(t-1)/4	+ δAt-1 +	ε1,t,
S2,t =	(2At-1	- 1)S2,(t-1)/2 +	S1,(t-1)/4	+ δAt-1 +	ε2,t,	Yt	= 1 + (S1,t + S2,t)/2	+ ε3,t,
where the random errors {εj,t}j=1,2,0≤t≤T are i.i.d N(0, 0.52) and {ε3,t}0≤t≤T are i.i.d N(0, 0.32).
The initial states S1,0 and S2,0 are independent N (0, 0.52) as well. Let St = (S1,t, S2,t)> denote
the state at time t. Under this model, treatments have delayed effects on the outcomes, as in Example
2. The parameter δ characterizes the degree of such carryover effects. When δ = 0, τ0 = 0 and H0
holds. When δ > 0, H1 holds. Moreover, τ0 increases as δ increases.
WesetK = 5and(T1,T2,T3,T4,T5) = (300, 375, 450, 525, 600). The discounted factor γ is set to
0.6 and G is chosen as the initial state distribution. We consider three behavior policies, according
to the designs D1-D3, respectively. For the behavior policy in D1, we set b(0) (s) = 0.5 for any
s ∈ S. For the behavior policy in D3, we use an -greedy policy and set b(k)(s) = /2 + (1 -
)I(Ψ(s)>(βb1,1(Tk) - βb0,0(Tk)) > 0), with = 0.1, for any k ≥ 1 and s ∈ S.
For each design, we further consider five choices of δ, corresponding to 0, 0.05, 0.1, 0.15 and 0.2.
The significance level α is set to 0.05 in all cases. To implement our test, we choose two α-spending
functions, corresponding to α1 (∙) and ɑ2(∙) given in equation 1. The hyperparameter θ in α2(∙) is
set to 3. The number of bootstrap sample is set to 1000. In addition, we consider the following
polynomial basis function, Ψ(s) = Ψ(s1,s2) = (1,s1, s2,…，sJ, s2,s2, ∙ ∙ ∙ , sJ)>, with J = 4.
We also tried some other values of J by setting J to 3 and 5. Results are reported in Figure 6 (see
Appendix G). It can be seen that the resulting tests is not sensitive to the choice of J.
All experiments run on a macbook pro with a dual-core 2.7 GHz processor. Implementing a single
test takes one second. Figures 1 (a) and 5 (a) (see Appendix G) depict the empirical rejection proba-
bilities of our test statistics at different interim stages under H0 and H1 with different combinations
of δ, α(∙) and the designs. These rejection probabilities are aggregated over 500 simulations. We
also plot α1 (∙) and ɑ2(∙) under H0. Based on the results, it can be seen that under H0, the type-I
error rate of our test is well-controlled and close to the nominal level at each interim stage. Under
H1 , the power of our test increases as δ increases, showing the consistency of our test procedure.
To further evaluate our method, we compare it with the classical two-sample t-test and the sequential
test developed by Kharitonov et al. (2015). To apply the t-test, for each Tk, we apply the t-test to the
7
Under review as a conference paper at ICLR 2021
Figure 2: Our test statistic (the orange line) and the rejection boundary (the black line) in the A/A (left plot)
and A/B (right plot) experiments.
data {At, Yt}0≤t≤Tk and plot the corresponding empirical rejection probabilities in Figures 1(b) and
5(b) (Appendix G). Results for Kharitonov et al. (2015)’s test are reported in Figure 4 (Appendix
G). Both competing methods fail to detect any carryover effects and do not have any power.
We next explain why several other methods mentioned in the introduction cannot be used for com-
parison. First, a lot of causal effects evaluation methods did not consider early termination. Conse-
quently, they are unsuitable to apply in our numerical studies. Second, standard temporal difference
learning method did not study the asymptotic distribution of the resulting value estimators. These
results are critical for carrying out A/B testing. Finally, many methods proposed to use inverse
propensity-score weighting. These methods are not valid for the alternating-time-interval design.
5.2	Real data application
We apply the proposed test to a real dataset from a ride-sharing platform. Order dispatching is one
of the most critical problems in online ride-hailing platforms to adapt the operation and manage-
ment strategy to the dynamics in demand and supply. The purpose of this study is to compare the
performance of a newly developed strategy with a standard control strategy used in the platform.
The new strategy is expected to reduce the answer time of passengers and increase drivers income.
For a given order, the new strategy will dispatch it to a nearby driver that has not yet finished their
previous ride request, but almost. In comparison, the standard control assigns orders to drivers that
have completed their ride requests.
The experiment is conducted at a given city from December 3rd to December 16th. Dispatch strate-
gies are executed based on alternating half-hourly time intervals. We also apply our test to a data
from an A/A experiment (which compares the baseline strategy against itself), conducted from
November 12th to November 25th. We expect that our test will not reject H0 when applied to
the data from the A/A experiment, since the two strategies used are essentially the same.
Both experiments last for two weeks. Thirty-minutes is defined as one time unit. We set Tk =
48(k + 6) for k = 1, . . . , 8. That is, the first interim analysis is performed at the end of the first
week, followed by seven more at the end of each day during the second week. We choose the
overall drivers’ income in each time unit as the response. The new strategy is expected to reduce
the answer time of passengers and increase drivers’ income. Three time-varying variables are used
to construct the state. The first two correspond to the number of requests (demand) and drivers’
online time (supply) during each 30-minutes time interval. These factors are known to have large
impact on drivers’ income. The last one is the supply and demand equilibrium metric. This variable
characterizes the degree that supply meets the demand and serves as an important mediator between
past treatments and future outcomes.
To implement our test, we set Y = 0.6, B = 1000 and use a fourth-degree polynomial basis for Ψ(∙),
as in simulations. We use α1 (∙) as the spending function for interim analysis and set α = 0.05. The
test statistic and its corresponding rejection boundary at each interim stage are plotted in Figure 2.
It can be seen that our test is able to conclude, at the end of the 12th day, that the new order dispatch
strategy can significantly increase drivers’ income, and meet more order requests. In addition, based
on the dataset from the A/B experiment, we found that the new strategy reduces the answer time of
orders by 2%, leading to almost 2% increment of drivers income. When applied to the data from the
A/A experiment, we fail to reject H0 , as expected. For comparison, we also apply the two-sample t-
test to the data collected from the A/B experiment. The corresponding p-value is 0.18. This result is
consistent with our findings. Specifically, the treatment effect at a given time affects the distribution
of drivers in the future, inducing interference in time. As shown in the toy example (see Section 4.1),
the t-test cannot detect such carryover effects, leading to a low power. Our procedure, according to
Theorem 2, has enough powers to discriminate H1 from H0 .
8
Under review as a conference paper at ICLR 2021
References
Vineet Abhishek and Shie Mannor. A nonparametric sequential test for online randomized experi-
ments. In Proceedings of the 26th International Conference on World Wide Web Companion, pp.
610-616. International World Wide Web Conferences Steering Committee, 2017.
Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference
learning with linear function approximation. arXiv preprint arXiv:1806.02450, 2018.
Iavor Bojinov and Neil Shephard. Time series experiments and causal estimands: exact randomiza-
tion tests and trading, volume accepted. Taylor & Francis, 2020.
Audrey Boruvka, Daniel Almirall, Katie Witkiewitz, and Susan A. Murphy. Assessing time-varying
causal effect moderation in mobile health. J. Amer. Statist. Assoc., 113(523):1112-1121, 2018.
ISSN 0162-1459. doi: 10.1080/01621459.2017.1305274.
Prabir Burman and Keh-Wei Chen. Nonparametric estimation ofa regression function. Ann. Statist.,
17(4):1567-1596, 1989. ISSN 0090-5364. doi: 10.1214/aos/1176347382.
Xiaohong Chen and Timothy M. Christensen. Optimal uniform convergence rates and asymptotic
normality for series estimators under weak dependence and weak conditions. J. Econometrics,
188(2):447-465, 2015. ISSN 0304-4076. doi: 10.1016/j.jeconom.2015.03.010.
Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, and Whit-
ney Newey. Double/debiased/neyman machine learning of treatment effects. American Economic
Review, 107(5):261-65, 2017.
Alex Deng, Jiannan Lu, and Shouyuan Chen. Continuous monitoring of a/b tests without pain:
Optional stopping in bayesian testing. In 2016 IEEE International Conference on Data Science
and Advanced Analytics (DSAA), pp. 243-252. IEEE, 2016.
Bradley Efron and Robert J Tibshirani. An introduction to the bootstrap. CRC press, 1994.
Ashkan Ertefaie. Constructing dynamic treatment regimes in infinite-horizon settings. arXiv preprint
arXiv:1406.0764, 2014.
Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh. More robust doubly robust
off-policy evaluation. In International Conference on Machine Learning, 2018.
Jianhua Z. Huang. Projection estimation in multiple regression with application to functional
ANOVA models. Ann. Statist., 26(1):242-272, 1998. ISSN 0090-5364. doi: 10.1214/aos/
1030563984.
Christopher Jennison and Bruce W Turnbull. Group sequential methods with applications to clinical
trials. Chapman and Hall/CRC, 1999.
Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning. In
International Conference on Machine Learning, pp. 652-661, 2016.
Ramesh Johari, Leo Pekelis, and David J Walsh. Always valid inference: Bringing sequential anal-
ysis to a/b testing. arXiv preprint arXiv:1512.04922, 2015.
Ramesh Johari, Pete Koomen, Leonid Pekelis, and David Walsh. Peeking at a/b tests: Why it mat-
ters, and what to do about it. In Proceedings of the 23rd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, pp. 1517-1525. ACM, 2017.
Nathan Kallus and Masatoshi Uehara. Efficiently breaking the curse of horizon: Double reinforce-
ment learning in infinite-horizon processes. arXiv preprint arXiv:1909.05850, 2019.
Edward H Kennedy. Nonparametric causal effects based on incremental propensity score interven-
tions. Journal of the American Statistical Association, 114(526):645-656, 2019.
Eugene Kharitonov, Aleksandr Vorobev, Craig Macdonald, Pavel Serdyukov, and Iadh Ounis. Se-
quential testing for early stopping of online experiments. In Proceedings of the 38th International
ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 473-482.
ACM, 2015.
9
Under review as a conference paper at ICLR 2021
K. K. Gordon Lan and David L. DeMets. Discrete sequential boundaries for clinical trials.
Biometrika,70(3):659-663,1983. ISSN 0006-3444. doi: 10.2307/2336502.
Sanghack Lee and Elias Bareinboim. Structural causal bandits: where to intervene? In Advances in
Neural Information Processing Systems, pp. 2568-2578, 2018.
Sanghack Lee and Elias Bareinboim. Structural causal bandits with non-manipulable variables. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 4164-4172, 2019.
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: Infinite-
horizon off-policy estimation. In Advances in Neural Information Processing Systems, pp. 5356-
5366, 2018.
Daniel J Luckett, Eric B Laber, Anna R Kahkoska, David M Maahs, Elizabeth Mayer-Davis, and
Michael R Kosorok. Estimating dynamic treatment regimes in mobile health using v-learning.
Journal of the American Statistical Association, accepted, 2019.
D. L. McLeish. Dependent central limit theorems and invariance principles. Ann. Probability, 2:
620-628, 1974. ISSN 0091-1798. doi: 10.1214/aop/1176996608.
S. A. Murphy. Optimal dynamic treatment regimes. J. R. Stat. Soc. Ser. B Stat. Methodol., 65(2):
331-366, 2003. ISSN 1369-7412. doi: 10.1111/1467-9868.00389.
Bo Ning, Subhashis Ghosal, and Jewell Thomas. Bayesian method for causal inference in spatially-
correlated multivariate time series. Bayesian Anal., 14(1):1-28, 2019. ISSN 1936-0975. doi:
10.1214/18-BA1102.
Doina Precup, Richard S Sutton, and Sanjoy Dasgupta. Off-policy temporal-difference learning
with function approximation. In ICML, pp. 417-424, 2001.
Martin L. Puterman. Markov decision processes: discrete stochastic dynamic programming. Wiley
Series in Probability and Mathematical Statistics: Applied Probability and Statistics. John Wiley
& Sons, Inc., New York, 1994. ISBN 0-471-61977-9. A Wiley-Interscience Publication.
B. Rabta and D. Aissani. Perturbation bounds for Markov chains with general state space. J. Math.
Sci. (N.Y.), 228(5):510-521, 2018. ISSN 1072-3374. doi: 10.1007/s10958-017-3640-9.
Ashesh Rambachan and Neil Shephard. A nonparametric dynamic causal model for macroecono-
metrics. Available at SSRN 3345325, 2019.
James Robins. A new approach to causal inference in mortality studies with a sustained exposure
period—application to control of the healthy worker survivor effect. volume 7, pp. 1393-1512.
1986. doi: 10.1016/0270-0255(86)90088-6. Mathematical models in medicine: diseases and
epidemics, Part 2.
Chengchun Shi, Sheng Zhang, Wenbin Lu, and Rui Rong. Statistical inference of the value function
for reinforcement learning in infinite horizon settings. arXiv preprint arXiv:2001.04515, 2020a.
Chengchun Shi, Sheng Zhang, Wenbin Lu, and Rui Song. Statistical inference of the value function
for reinforcement learning in infinite horizon settings. arXiv preprint arXiv:2001.04515, 2020b.
Charles J. Stone. Optimal global rates of convergence for nonparametric regression. Ann. Statist.,
10(4):1040-1053, 1982. ISSN 0090-5364.
Richard S. Sutton and Andrew G. Barto. Reinforcement learning: an introduction. Adaptive Com-
putation and Machine Learning. MIT Press, Cambridge, MA, second edition, 2018. ISBN 978-0-
262-03924-6.
Richard S Sutton, Csaba Szepesvari, and Hamid Reza Maei. A convergent o(n) algorithm for off-
policy temporal-difference learning with linear function approximation. Advances in neural in-
formation processing systems, 21(21):1609-1616, 2008.
Philip Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement
learning. In International Conference on Machine Learning, pp. 2139-2148, 2016.
10
Under review as a conference paper at ICLR 2021
Philip S Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High-confidence off-
policy evaluation. In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.
Davide Viviano and Jelena Bradic. Synthetic learner: model-free inference on treatments over time.
arXiv preprint arXiv:1904.01490, 2019.
Chien-Fu Jeff Wu et al. Jackknife, bootstrap and other resampling methods in regression analysis.
the AnnalsofStatistics, 14(4):1261-1295, 1986.
Baqun Zhang, Anastasios A. Tsiatis, Eric B. Laber, and Marie Davidian. Robust estimation of
optimal dynamic treatment regimes for sequential treatment decisions. Biometrika, 100(3):681-
694, 2013.
11
Under review as a conference paper at ICLR 2021
Input: no. of basis functions q, no. of bootstrap samples B, an a spending function α(∙).
Initialize: I = {1,…
___^
,B}. Set Ω,
a a .
Σ0 , Σ1 to zero
.∙	— a a .
matrcies, and η, Si,…，Sb to zero vectors.
Compute U (see Section 4.2) using either Monte Carlo methods or numerical integration.
For k = 1 to K :
Step 1. Online update of ATE.
Fort = Tk-1 to Tk - 1:
Σba = (1-t-1)Σba+t-1ξ(St,At){ξ(St,At)-γξ(St+1,a)}>,a=0,1;
ηb = (1 - t-1)ηb + t-1ξ(St, At)Yt.
Set (βba>,0, βba>,1)> = Σb a-1ηb for a ∈ {0, 1} and τb = U >βb.
Step 2. Online update of the variance estimator.
Initialize Ω* to a zero matrix.
For t = Tk-1 to Tk - 1:
εbt,a = Yt + γ Ψ> (St+1)βba,a - Ψ>(St)βba,At for a = 0, 1;
Ω * = Ω * + (ξ(St,At)> bt,0,ξ(St,At)>εu)>(ξ(St,At)>bt,0,ξ(St,At)>"ι).
Set Σ to a block diagonal matrix by aligning Σ0 and Σ1 along the diagonal of Σ;
Set Ω = T-I(Tk-ιΩ + Ω*) and the variance estimator σ2 = U>Σ-1Ω{Σ-1}>U.
Step 3. Bootstrap test statistic.
For b = 1 to B :
Generate 娘〜N(0,I4q); Sb = Sb + Ω*1/2ekb); Zb = TJI/LU>Σ-1Sb;
Set z to be the upper {α(t) - |Ic|/B}/(1 - |Ic|/B)-th percentile of {Zbb*}b∈I.
Update I and Ω as I ~ {b ∈ I: Zb ≤ z};
Step 4. Reject or not?
Reject the null if √Tkσ~1τ > z.
Algorithm 1: The testing procedure
A More on the algorithm
A pseudo algorithm summarizing our procedure is given in Algorithm 1. We next introduce some
notations. The matrix Ω(t) is defined by
Ω(t) =	1X	(	ξjbbj,0	} (	ξjεj,0	)>,	⑶
㈠	t j=0<	ξjεj,1	八	ξj εj,i	J ,	L
where εbj,0 and εbj,1 are the temporal difference errors defined in Algorithm 1.
B Extensions
B.1 Extensions to dynamic policies
In this paper, we focus on comparing the long-term treatment effects between two nondynamic poli-
cies. The proposed method can be easily extended to handle dynamic policies as well. Specifically,
consider two time-homogeneous policies π1 and π2 where each πj (s) measures the treatment as-
signment probability Pr(At = 1|St = s). Note that the integrated value difference function τ0 can
be represented by
/{V(πi; S)- V(π/; s)}G(ds) = [[{Q(πi; 1, s) - Q(ni；0, S)}nI(S)
ss
-{Q(π/; 1, S)- Q(n/；0, s)}∏2(s) + Q(∏E0, S)- Q(∏/; 0, s)]G(ds).
The Q-estimators can be similarly computed via temporal difference learning. More specifically, for
a given policy π, let
Qt(∏; a,S) = Ψ>(S)∑ ](t" t X ξ(Sj ,Aj j 1 AAj) }，	(4)
12
Under review as a conference paper at ICLR 2021
(a) The proposed test under H1 (left) and H0 (right)
Figure 3: Empirical rejection probabilities of our test and the DRL-based test.
(b) DRL-based test under H1 (left) and H0 (right)
be the Q-estimator given the data {(Sj , Aj , Yj )}j<t where Σπ (t) = t-1	j<t Σj where Σj is
defined by
Ψ(Sj)(1-Aj){Ψ(Sj)-γΨ(Sj+1)(1-π(Sj+1))}>	-γΨ(Sj)(1-Aj)Ψ>(Sj+1)π(Sj+1)
-γΨ(Sj)AjΨ>(Sj+1)π(Sj+1)	Ψ(Sj)Aj{Ψ(Sj)-γΨ(Sj+1)(1-π(Sj+1))}>
We can plug-in the Q-estimator in equation 4 to estimate τ0 . The corresponding variance estimator
and the resulting test statistic can be similarly derived. A bootstrap procedure can be similarly
developed as in Section 4.3 for sequential testing. We omit the details for brevity.
B.2 Extensions to other nonparametric estimators
In addition to temporal difference learning, other existing OPE methods could be potentially coupled
with the proposed bootstrap procedure for online sequential testing. We use the double reinforce-
ment learning method (DRL, Kallus & Uehara, 2019) as an example.
First, we remark that DRL requires the system to be ergodic and use an inverse propensity-score
weighted method to construct the value estimator. As such, it might not be applicable to the
alternating-time-interval design and the adaptive design.
Second, in the Markov design, it could be coupled with our bootstrap procedure for sequential
testing. We compare such a procedure with our proposed method using the simulation setting in
Section 5.1, and report the rejection probabilities in Figure 3. It can be seen that DRL-based test has
some inflated type-I errors under H0 and is less powerful than our procedure under H1.
We next outline the procedure. Specifically, at the kth interim stage, we compute the test statistic
where
ψt= {{Qbk(1; 1, S)- Qk(0; 0, s)}Gds + YrAtτ&k(1; St){R + Qk(1; 1, St+ι) - Qk(1; At, St)}
s	b(St )
1-A
-Yi---Γ77Tτωk(0; St){Rt + Qk(0; 0, St+1) - Qk(0; At, St)},
1 - b(St)
Qbk and ωbk denote the estimated Q- and marginal density ratio functions based on the data collected
at the kth stage, and
Tk-1 b2-ι + Pt= -k1-ι Ψ2
T	.
The bootstrapped sample can be constructed as
Zk = √mσ-1 bk-1 Zk-1 + PTk - Tk-1 N(0,1).
Then similar to Algorithm 1, we can decide whether to reject H0 or not based on the test statistic Zk
and the bootstrap samples. This algorithm can be implemented online provided that Q and ωb can be
computed online.
13
Under review as a conference paper at ICLR 2021
C More on the wild bootstrap algorithm
We first provide an example to show that our test statistics do not have the canonical joint distribu-
tion. This motivates us to propose a wild bootstrap algorithm. We next present some details on the
bootstrap algorithm.
Let {Zk}k be the sequence of test statistics conducted at each interim stage. These test statistics are
said to have canonical joint distribution with information levels {Ik}k for the parameter θ if:
(i)	(Zι, Z2,…,ZK)> is asymptotically normal,
(ii)	EZk = θ√Ik +。⑴，
(iii)	Cov(Zk1,Zk2) = Plkι /Ik2 +。⑴.
See also Equation (3.1) in Jennison & Turnbull (1999).
Unlike the settings where observations are independent across time, (iii) is likely to be violated
in our setup when adaptive design is used. This is due to the existence of carryover effects in time.
Specifically, when treatment effects are adaptively generated, the behavior policy at difference stages
are likely to vary. Due to the carryover effects in time, the state vectors at difference stages have
different distribution functions.
According to Part 3 of the proof of Theorem 3, we have for any k1 ≤ k2 that
Cov(Zk1 , Zk2)
Tk^× × U>∑-1(Tkι)Ω(T{1 ){Σ-1(Tk2)}>U +o(1).
ηk1,k2
The matrices Σ(k) and Ω(k) depend the distributions of state vectors and are likely to differ for
different k. Consequently, the second term ηk1,k2 on the right-hand-side depends on both k1 and k2.
As such, (iii) is violated.
The idea of our bootstrap algorithm is to generate bootstrap samples {ZbMB(t)}t that have asymptot-
ically the same joint distribution as {√tb-1(t)(b(t) - τo)}t. Specifically, let {Zt}t≥0 be a sequence
of i.i.d. random variables independent of the observed data. Define
βMB(t)=∑-1(t) < 1X ξ(Sj ,Aj)g( bτ
I j<t	j j,
(5)
where εbt,a is the temporal difference error defined in Algorithm 1. Based on βbMB(t), one can define
the bootstrap sample ZMB (t) = √tb-1 (t)U>βMB(t).
We remark that although the wild bootstrap method is developed under the i.i.d. settings, it is valid
under our setup as well. ThiS is due to that under CMIA, β(t) — β* forms a martingale sequence
with respect to the filtration {(Sj , Aj , Yj ) : j < t}. This guarantees that the covariance matrices of
βbMB(t) and βb(t) are asymptotically equivalent. As such, the bootstrap approximation is valid.
However, calculating βbMB(Tk) requires O(Tk) operations. The time complexity of the resulting
bootstrap algorithm is O(BTk) up to the k-th interim stage, where B is the total number of bootstrap
samples. This can be time consuming when {Tk - Tk-1}kK=1 are large.
D More on the designs
In D3, we require b(k) to be strictly bounded between 0 and 1. Suppose an -greedy policy is used,
i.e. b(k) (s) = /2 + (1 - )πb(k) (s), where πb(k) denotes some estimated optimal policy. It follows
that /2 ≤ b(k) (s) ≤ 1 - /2 for any s. Such a requirement is automatically satisfied.
For any behaviour policy b in D1-D3, define Sj(b-ι) and Y；®) as the potential outcomes at
time t, where btt denotes the action history assigned according to b. When b is a random policy
as in D1 or D3, definitions of these potential outcomes are more complicated than those under a
14
Under review as a conference paper at ICLR 2021
deterministic policy (see Luckett et al., 2019). When b is a stationary policy, it follows from MA
that {St+ι(bt)}t≥-ι forms a time-homogeneous Markov chain. When b follows the alternating-
time-interval design, both {S2t(b2t-1)}t≥0 and {S2t+1(b2t)}t≥0 form time-homogeneous Markov
chains.
We next show that (Zι,…，ZK) is asymptotically multivariate normal and provide a consistent
covariance estimator.
Theorem 3 (Limiting distributions) Assume C1-C3 hold. Assume all immediate rewards are uni-
formly bounded variables, the density function of So is uniformly bounded on S and q satisfies
q = o(√T/ log T). Then under either D1, D2 or D3, we have
•	{Zk}1≤k≤K are jointly asymptotically normal;
•	their asymptotic means are non-positive under H0;
•	their covariance matrix can be consistently estimated by some Ξ, whose (k1, k2)-th element
Ξk1,k2 equals YrTkIlrTk2U>ΣT(TkjΩ(TkJ{ΣT(Tk?)}>U/{b(TJb(Tk2)}.
E Technical conditions
To simplify the presentation, we assume all state variables are continuous. The immediate reward
and the density function of S0 are bounded.
E.1 Condition C1
C1 Suppose (i) holds. Assume (ii) holds under D1, (iii) holds under D2 and (ii), (iv) hold under D3.
(i) The transition kernel P is absolutely continuous and satisfies P(ds; a, s0) = p(s; a, s0)ds for
some transition density function p. In addition, assume p is uniformly bounded away from 0 and ∞.
(ii) The Markov chain {S*(b(-)ι)}t≥o formed under the behaviour policy b(0) is geometrically er-
godic, i.e. there exists some function M on S, some constant 0 ≤ ρ < 1 and some probability
density function Π such that s∈S M (s)Π(ds) < +∞ and
Mr⑻(b(0)ι) ∈ S|So = S)- Π(S)∣∣τy ≤ M(s)ρt,	∀t ≥ 0,s ∈ S, S ⊆ S,
where ∣∣ ∙ ∣∣tv denotes the total variation norm.
(iii)	The Markov chains {S2t(b2t)}t≥0 and {S2t+ι(b2t+ι)}t≥o are geometrically ergodic.
(iv)	For any k = 1,…，K 一 1, the following events occur with probability tending to 1: the Markov
chain {St(b(-∖)}t≥o is geometrically ergodic; sups∈s ∣b(k)(s) 一 b*(s)| → 0 for some b*(∙); the
stationary distribution of {S*(b(-∖)}t≥o will converge to some Π* in total variation.
Remark: By C1(ii), Π is the stationary distribution of {St(b(-)ι)}t≥o. It follows that
Π(S) = X	P(S;a,S){ab(1)(S) + (1 一 a)b(0)(S)}Π(dS),
a∈{0,1} s∈S
for any S ⊆ S. By C1(i), we obtain
Π(S) = X	[a{1 一 b(0) (S)} + (1 一 a)b(0) (S)]p(S0; a, S)dS0Π(dS)
=	X	[a{1 一 b(0) (S)} + (1 一 a)b(0) (S)]p(S0; a, S)Π(dS) dS0.
s0∈S a∈{0,1} s∈S
(6)
|
}
{^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
μ(SO)
This implies that μ(∙) is the density function of Π. Since P is uniformly bounded away from 0 and
∞, so is μ.
Under C1(iv), for any k ∈ {1,…，K 一 1}, there exist some M(k)(∙), Π(k)(∙) and p(k) that satisfy
Rs∈S M(k) (S)Π(k) (dS) < +∞ and
∣∣Pr(S*(b(-∖) ∈ S|So = S)- Π(k)(S) J1 V ≤ M (k)(s){ρ(k)}t,	∀t ≥ 0,s ∈ S, S⊆ S,	(7)
15
Under review as a conference paper at ICLR 2021
with probability tending to 1. Since b(k) is a function of the observe data history, so are M(k)(∙),
Π(k)(∙) andρ(k).
Suppose an -greedy policy is used, i.e. b(k) (s) = /2 + (1 - )πb(k) (s) where πb(k) denotes some
estimated optimal policy. Then the condition sups∈s |b(k) (s)-b*(s)∣ → 0 requires b(k) to converge.
The total variation distance between the one-step transition kernel under b(k) and that under b can
be bounded by
sup |Pr(S；(b(k)) ∈ S∣So = S) — Pr(S；(b*) ∈ S∣So = s)| ≤ SuP ∣b(k)(s) — b*(s)∣ sup p(s0; a, s),
s	s	s,s0,a
and converges to zero in probability. When the markov chain {Sj(b(-∖)}t≥o is uniformly ergodic,
it follows from Theorems 2 and 3 of Rabta & AiSSani (2018) that k∏(k) — Π*∣∣tv → 0 where
Π* corresponds to the stationary distribution of {S}(bj-J}. The last condition in C1(iv) is thus
satisfied.
E.2 Condition C2
C2(i) Assume there exists some β* such that
SUP	IQ(a0; a, s) — Ψ>(s)βao,αl = o(T-1/2).
a ,a∈{0,1},s∈S
(ii)	Assume there exists some constant c* ≥ 1 such that
(C*)-1 ≤ λmin f ψ Ψ(s)Ψ>(s)ds] ≤ λmaχ [ [ Ψ(s)Ψ>(s)ds] ≤ C*,	(8)
s∈S	s∈S
andSUPs ∣∣Ψ(s)∣∣2 = O(√q).
(iii)	Assume lim infq k Rs∈S Ψ(s)G(ds)k2 > 0.
Remark: For any a, a0 ∈ {0, 1}, suppose Q(a0; a, s) is p-smooth as a function of s (see e.g. Stone,
1982, for the definition of p-smoothness). When tensor product B-splines or wavelet basis functions
(see Section 6 of Chen & Christensen, 2015, for an overview of these bases) are used for Ψ(∙), We
have
suP	lQ(a0; a, S)- ψ>(s)βao,αl = o(q-p/d),
a ,a∈{0,1},s∈S
under certain mild conditions. See Section 2.2 of Huang (1998) for details. It follows that Condition
C2(i) automatically holds when the number of basis functions q satisfies q	Td/(2p).
Condition C2(ii) is satisfied when tensor product B-splines or wavelet basis is used. For B-spline
basis, the assertion in equation 8 follows from the arguments used in the proof of Theorem 3.3,
Burman & Chen (1989). For wavelet basis, the assertion in equation 8 follows from the arguments
used in the proof of Theorem 5.1, Chen & Christensen (2015). For both bases, the number of
nonzero elements in Ψ(∙) is bounded by some constant. Moreover, each basis function is uniformly
bounded by O(√q). The condition SUPs ∣∣Ψ(s)k2 = O(√q) thus holds.
Condition C2(iii) automatically holds for tensor product B-splines basis. Notice that 1>Ψ(S) = q1/2
for any S ∈ S where 1 denotes a vector of ones. It follows from Cauchy-Schwarz inequality that
Ψ(S)G(dS)
s∈S
1>Ψ(S)G(dS)
s∈S
≥
2
C2(iii) is thus satisfied.
E.3 Condition C3
Let ε* (a0, a) = Y0* (a) + γQ(a0; a0, S1* (a)) — Q(a0; a, S0).
C3 Assume infq infa0,a∈{0,1},s∈S Var{ε* (a0, a)|S0 = S} > 0 and SUPq SUPa∈{0,1},s∈S ρε(a, S) < 1
where
/	E{ε*(0,a)ε*(1,a)∣So = s}
pε(a,S) — /	二♦
,Var(ε*(0, a)|S。= S)Var(ε*(1, a)|S。= S)
Here, ρε corresponds to the partial correlation ofε*(0, a) and ε*(1, a) given S0.
16
Under review as a conference paper at ICLR 2021
F Technical proofs
F.1 Proof of Lemma 1
To prove Lemma 1, we state the following lemma.
Lemma 2 Under MA and CMIA, Q(a0; a, s) = r(a, s) + γ s0 Q(a0; a0, s0)P(ds0; a, s) for any
(s, a).
ProofofLemma 2: For any a, a0 ∈ {0,1}, define the potential outcome Yt*(a0, a) and Sj(a0, a) as
the reward and state variables that would occur at time t had the agent assigned Treatment a at the
initial time point and Treatment a0 afterwards.
Let P to (S, a, S) = Pr{Sj(a0, a) ∈ S∣So = s} for any S ⊆ S,a,a0 ∈ {0,1},s ∈ S and t ≥ 0. We
break the proof into two parts. In Part 1, we show Lemma 2 holds when the following is satisfied:
Pr{Sj+ι(a0,a) ∈ S|S；(a) = s,So} = P (S,a0,s),	⑼
In Part2, we show equation 9 holds.
Part 1:	Under CMIA, we have
E{γt*(a0, a)|So = s} = E[E{Y7(a0 ,a)|St (a0,a),so = s}|So = s]	(10)
=E{r(π(Sj(a0, a)), Sj(a0, a))∣So = s}.
It follows that
Q(a0; a,s) = X YtE{r(n(冕(a0,a)),S；(a0,a))|S0 = s}.	(II)
t≥0
Similar to equation 10, we can show
E{Yt+i(a0, a)|so =s} = Emn(St+i(a0,a)),st+i(a0,a))|s0 =s}
=e[EmnS+ι(a0,a)),st+ι(a0,a))lS; (a),So = s}ιSo = s],
and hence
EYtE{Yt+i(a0,a)|S0 =s} =E EYt EmnS+ι(a0,a)),S+ι(a0,a))∣ Si(a),So = s}|So = S
t≥0	t≥0
By equation 9, the conditional distribution of St+∖(a', a) given S；(a) = S and So are the same as
the conditional distribution of S；(a0, a) given So = s. It follows that from equation 11 that
X γtE{Yt+ι(a0, a)∣S0 = s} = E{Q(a0; a, Si (a))∣So = s}∙
t≥o
This together with the definition of Q function and CMIA yields
Q(a0； a, s) =	r(a,	s) + Y	EYtE{Yt+ι(a0,	a)∣So	= s}	= r(a, s)	+	YE{Q(a0; a, S；(a))|So	=	s}.	(12)
t≥o
Under MA, we have
E{Q(a0; a, S；(a))|So = s}
/
s0∈S
Q(a0; a, s0)P(ds0; a, s).
Combining this together with equation 12 yields the desired result.
Part 2:	We use induction to prove equation 9. When t = 0, it trivially holds.
Suppose equation 9 holds for t = k. In the following, we show equation 9 holds for t = k + 1.
Under MA, we have
Pr{Sk+2(a0, a) ∈ S|S；(a) = s,So} = E[Pr{SM(a0,a) ∈ S|Sk+i(a0,a),S；(a) = s,So}|S；(a) = s,So]
=E[P(S; a0,Sk+ι(a0, a))|S；(a) = s, So].
17
Under review as a conference paper at ICLR 2021
Since we have shown equation 9 holds for t = k, it follows that
Pr{SM (α0,α) ∈ S|S；(a)
s,S0} = / P (S; α',s,)Pa (ds',a',s).
J s0∈S
Similarly, we can show
Pk+1(S, a', s) = Pr{S^+1(α,, a0) ∈ S∣S0 = s}
P P(S; a,,s,)Pk(ds',a',s).
√sz∈S
The proof is hence completed.
ProofofLemma 1: By CA, it is equivalent to show
E{Q(a0; 4S(4-i))-匕*(4)- IQIa; a，,S；+i(4))}夕(At,S;(4-i))=0.
Let So denote the support of S°. For any s° ∈ S0,it suffices to show
E{Q(a0; At, S； (At-I))-匕*(At)- γQ(a0; az, S^ι(4)2(At, S；(At-I))∣S° = s0} = 0.
This is equivalent to show
E{Q(a0; At,S；(At-I))-匕；(At)- γQ(a0; a,S^ι(At))夕(At,S；(At-I))I(A0 = a0)}∣S0 = s0]=0,
for any s° ∈ S0, a° ∈ {0,1}.
Let A0(s0) = {a ∈ {0,1} : Pr(A0 = a∣S0 = s) > 0}. It suffices to show for any s° ∈ S0, a° ∈
A0(s0),
E{Q(a0; At,S；(At-I))-匕；(At)- γQ(a0;胃,5；+1(At))夕(At,S；(At-I))I(A0 = a0)∣S0 = s0} = 0,
or equivalently,
E{Q(a0; At,S；(At-I))-匕；(At)- γQ(a0; a，,S；+i(At))夕(At,S；(At-I))∣S° = s°,A° = a。} = 0. (13)
Let sj = (s0,sι, ∙∙∙ ,sj )T, yj = (y0, yι, •…，yj )T, Sj = (S0, si , ,,, , Sj ), and Yj =
(K, Y1,…，Yj)>. We can recursively define the sets Yj(%, aj,y∙-ι), Sj+1(sj, aj, yj),
Aj+1(sj+1,aj,yj) to be the supports of Yj-, Sj+ι, Aj+ι conditional on (Sj = 叼，Aj =%，Y=i =
yj-i), (Sj = $j, Aj = aj,Yj = yj), (Sj+1 = Sj+1, Aj = aj,Yj = yj) respectively, for j ≥ 0.
Similar to equation 13, it suffices to show
E{Q(a0; At, S；(At-I))- K*(At)- γQ(a0; a0, S；+i(At))夕(At, S；(At-I))ISt = %,4 =即匕-1 = yt-i}
for any	s0	∈	S0, a0	∈	A0(s0),y0	∈	%(s0, a0),…，st	∈	St(at-1,	at-1, yt-1), at	∈
At(at, at-ι, yt-ι). This is equivalent to show
E{Q(a0;at,S；(at-i)) - Y7(at) - γQ(a0;a0,SiJ+ι(at))∣St = $t, At = at,Yt-ι = yt-ι} = 0. (14)
By construction, we have Pr(At = at∣Sat = at, Yat-1 = yat-1, Aat-1 = aat-1) > 0. Under SRA, the
left-hand-side (LHS) of equation 14 equals
E{Q(a0; at,S；(at-i)) - Y7(at) - γQ(a0; a0, SiJ+ι(at))∣St = St,^at-1 = at-i,Yt-i = yt-1}. (15)
Notice that the conditioning event is the same as {S；(at-i) = st, Yt-1(at-1) = yt-1,St-1 =
at-1,At-1 = at-1,Yt-2 = yt-2}. Under SRA, equation 15 equals
E{Q(a0；at,S；(at-1))- Yt；(at) - YQ(a0;胃，£+1(面))用(面-1) = st,γt-ι(at-I) = yt-1,
St-1 = sat-1 , At-2 = aat-2 , Yt-2 = yat-2 }.
0,
By recuisvely applying SRA, we can show the left-hand-side of equation 14 equals
E[Q(a0; at,S；(at-1)) - K*(at) - γQ(a0; a0, S^1(at))∣{Sj=(aj-1) = Sj }1≤j≤t,{% (aj- )= y }1≤j≤t-1].
This is equal to zero by MA, CMIA and Lemma 2. The proof is hence completed.
18
Under review as a conference paper at ICLR 2021
F.2 Proof of Theorem 3
F.2.1 Proof under D1
We begin by providing an outline of the proof. The proof is divided into three steps. In the first step,
we show for any Ti ≤ t ≤ Tk, the estimator β(t) satisfies the following linear representation,
∕⅜-β* = C-1" t X ( ξj ji 小"/2),
(16)
I
{z^^
Zι(t)
,
1	b< / > ∖	1 - a / > ∖	1	，r，	/ʌ/	C ∖	/ʌ/ A ∕-1 ∖	i~	/-χ	1	1 ʌ	1	. 1 ∙
where Σ(t)	= EΣ(t)	and 8j,α	= Yj + γQ(a; a,	Sj+1)	- Q(a; Aj, Sj)	for a	= 0,1. Based	on this
representation, in the second step, we show the asymptotic normality of T(t). Specifically, we show
√t{b(t) - τ0}
b(t)
In the last step, we prove Theorem 3.
Part 1: By definition, we have
→ N(0,1).
β(t)- β*
ς-1(t) {t X(ξj Yo- ς (t)β *}=ς-1
^	-1
Σ-1
t-1
⑴ɪX{
j=0 I
t-1
-Σjβ*
ξj{Yj - Ψ>(Sj)βu + YΨ>(Sj+ι)舔,0}
ξj{Y∙ - Ψ>(Sj网,Aj + YΨ>(Sj + ι网,1}
⑴ tX{(ξjYj)
where the last equality is due to the definition of Σj. Let
ra,j = Ψ>(Sj)βa,At - γΨT(Sj+1)βa,0 - Q(a; Aj,Sj) + γQ(a a Sj+1).
It follows that
…*=ς-1(t)∣ t X(ξj εj,1)}- ς-1(t) {t X(jj,1)
and hence
…*=s-1(t){ tX (I j)+{C T⑴-ς-1m {IX (I j
I
^^^^^™{^^^^^^^^^^^^^^^^^
Z2(t)
-ς Td1X(出 0
,
I
{^™
Z3 (t)
,
We first consider Z3(t). It can be upper bounded by
^	1
k∑-1(t)k2
1X ( ξjεj,0
t j⅛ I ξj εj,1
1
=∣∣Σ 1(t)∣∣2 max sup
α∈{0,1} ∣∣α∣∣2 = 1
2
ɑ> t X & %
Vj=0
Ua—1 /,、U
∣∣∑	(t)∣∣2 max sup
α∈{0,1} ∣∣ɑk2 = 1 ∖
≤
1 t-1	ʌ
t X(α>ξj)2ra,j ≤ max ιrα,j i∣ς T⑴∣2∖
j=0	,
1 t-1
λmax »ξ>
j=0
19
Under review as a conference paper at ICLR 2021
where the second follows from Cauchy-Schwarz inequality. Under Condition C2(i), we have for any
j ≤ t ≤ Tk, maxj |ra,j | = o(t-1/2). Suppose for now, we have shown
k∑-1(t)k2 =Op(I) and λmaχ (ɪ Xξjξj =Op⑴.	(17)
It follows that
ζ3(t) = op(t-1/2).	(18)
To bound ζ2 (t), notice that for any a ∈ {0, 1},
2
1 t-1
tΣSξj εj,a
j=0
以 X Eξ> ξj εj,a + 包 X Eξ> ξj2 εj1,aεj2,a.
j=0	j1 6=j2
E
2
Similar to Lemma 1, We can show for any 夕(∙) that is a function of At, St,匕-ι that
E{Q(a0; At, St)- Yt - γQ(a0; a0, St+ι)}夕(St, At,匕-i) = 0.
This implies thatEξj>ξj2εj1,aεj2,a = 0 for any j1 6= j2. It follows that
2
(19)
1 t-1
t Σ2ξj εj,a
j=0
E
=—2 X Eξ>ξjε2,a ≤ qλmax (	X Eξj^ja
2	j=0	j=0
Since all immediate rewards are uniformly bounded, so is the Q function. As a result, ∣εj,a∣'s are
uniformly bounded. Suppose for now, we have shown
λmax (	^X Eξj ξ> )=O⑴.
(20)
It follows that Ek—-1 Ptj-=10 ξjεj,ak22 = O(q) and hence
—X(ξj εj,0)
kZ2(t)k2 ≤k∑T(t) - ∑-1(t)k2
—X ( ξj j )= OP(TS
Suppose
k∑-1(t) - ∑-1(t)k2 = op(q-1/2).	(21)
It follows that
= op(—-1/2 ).
2
This together with equation 18 yields equation 16.
It remains to show equation 17, equation 20 and equation 21 hold. We summarize these results in
Lemma 3.
Lemma 3 Under the given conditions, we have equation 17, equation 20 and equation 21 hold.
Part 2:	By definition, we have b(t) - τ0 = U>{β(t) - β*} + U>β* - To. Define
Ω(t)
ξjεj,0
ξjεj,1
20
Under review as a conference paper at ICLR 2021
The asymptotic variance of √t{b(t) - τo} is given by σ2(t) = U>Σ-1 (t)Ω(t){Σ-1(t)}>U. We
begin by providing a lower bound for σ2 (t). Notice that
σ2(t) ≥ λmin{Ω(t)}∣∣U >Σ-1(t)k2 ≥ λmin {Ω (t) }λmin [Σ-1 (t) { Σ-1 (t) }-1] k U k 2 .	(22)
Under C1(iii), we have lim infq kU k22 > 0.
In addition, notice that Σ-1(t){Σ-1(t)}-1 is positive semi-definite. It follows that
λmin[∑-1(t){∑T(t)}-1 ] = 1∕λmaχ[Σ(t){Σ(t)}]. Using Similar arguments in showing
k∑202*(0)∣∣2 = O⑴ in the proof of Lemma 3, We can show supt≥ι ∣∣∑(t)k2 = O⑴ and hence
supt≥1 λmax[Σ(t){Σ(t)}] = O(1). This further yields
infλmin[Σ-1(t){Σ-1(t)}-1] >0.	(23)
Suppose Ω(t) satisfies
liminf λmin{Ω(t)} > 0.
(24)
It follows that σ2(t) is bounded away from zero, for sufficiently large t. Under Condition C2(i), we
have U>β* - τ0 = o(T-1/2) = o(t-1/2). It follows that
√t{b(t) - τo} = √tU>{b(t) - β*}	√t(U>β* - τo) = √tU>{b(t) - β*} n)
σ(t)	=	σ(t)	+ 词 =	σ(t)	+ o().
Moreover, it follows from equation 22, equation 23 and equation 24 that σ(t)∕∣U∣∣2 is uniformly
bounded away from zero, for sufficiently large t. Combining this together with equation 16 yields
√t{b(t)- τo} = √tU>Z1(t) . √tU>Rt
σ(t)	σ(t)	σ(t)
where the remainder term satisfies ∣Rt∣2 = op(t-1/2). It follows that the second term on the right-
hand-side (RHS) of the above expression is bounded from above by √tkRtk2kUk2∕σ(t) = 0p(1)
and hence
√t{b(t) - τo}
σ(t)
√tU >Zι(t)
σ(t)
. op (1).
(25)
Similar to the proof of Lemma 1, we can show for any j ≥ 0, a ∈ {0, 1},
E(ξj εj,al{Si ,Ai,γi}i<j )=0.
By the definition of Zι(t), √tU>Zι(t)∕σ(t) forms a martingle with respect to the filtration
σ({Sj , Aj , Yj }j<t), i.e. the σ-algebra generated by {Sj , Aj , Yj }j<t. By the martingale central
limit theorem, we can show √tU>Zι (t)∕σ(t) → N(0,1) (see Lemma 4 for details).
To complete the proof of Part 2, we need to show equation 24 holds and that b(t)∕σ(t) → 1.
The assertion b(t)∕σ(t) → 1 can be similarly proven using arguments from Step 3 of the proof
of Theorem 1, Shi et al. (2020a). We show the asymptotic normality of √tU>Zι(t)∕σ(t) and that
equation 24 holds in the following lemma.
Lemma 4 Under the given conditions, we have equation 24 holds and that √tU>Zι(t)∕σ(t) →
N (0, 1).
Part 3:	Results in Part 2 yield that √Tk{T(Tk) - τo}∕σ(Tk) → N(0,1) for each 1 ≤ k ≤ K. In
addition, for any K-dimensional vector a = (aι,…，aκ)>, it follows from equation 25 that
K
X
k=1
ak√Tk{T(Tk)- τo}
σ(Tk)
K
X
k=1
ak √TkU >Z1(Tk)
στk)
. op (1).
The leading term on the RHS can be rewritten as a weighted sum of {ξjεj,0, ξj εj,1}0≤j<t. Sim-
ilar to the proof in Part 2, we can show it forms a martingale with respect to the filtration
21
Under review as a conference paper at ICLR 2021
({Sj, Aj, Yj}j<t). We now derive its asymptotic normality for any a, using the martingale central
limit theorem for triangular arrays.
By Corollary 2 of McLeish (1974), we need to verify the following two conditions:
(a)	maxo≤j<t | PK=I aτT-1/2U>∑-1(Tk)(ξ>εjfi,ξ>εjtι)τ{σ(Tk)}-1I(j < Tk)| → 0;
(b)	PT-01 | PK=I akT-1/2Uτ∑-1(Tk)(ξ>εj,0,ξ>εj,1)τ{σ(Tk)}-1I(j < Tk)|2 converges to
some constant in probability.
Since K is fixed, to verify (a), it suffices to show
〈 舟ax- *T-1∕2∣UT ∑-1 (Tk)化卜加个卜川>①⑵)}-1∣→ 0.
1≤j<t,1≤k≤K
In Lemma 3, we have shown ∣Σ-1(t)∣ = O(1). In Part 1 and Part 2 of the proof, we have shown
∣εj,a |'Sare uniformly bounded and that σ(t)∕∣U∣∣2 is bounded away from zero. Therefore, it suffices
to show T1-1/2 max0≤j<t ∣ξj ∣2 →P 0. Under Condition C2(ii), we have sups ∣Ψ(s)∣2 = O(q1/2)
and hence max0≤j<t ∣ξj ∣2 = O(q1/2). The assertion thus follows by noting that T1 = c1T and
q = o(T).
Using similar arguments in Step 3 of the proof of Shi et al. (2020a), we can show
t-1
1 t-1
152(ξ>εj,0,ξ>εj,ι)τ(ξ>εj,o,ξ>εj,ι)- ω⑴	→0,
(26)
j=0
2
as t → ∞. This together with the facts ∣∣Σ-1(t)k = O(1) and σ(t)∕kU∣∣2 is bounded away from
zero implies that
ak1 ak2
√T⅛1 Tk2σ2(Tkι ∧ Tk2)
Tk1 ∧Tk2
X UτΣ-1 (Tk1)(ξjτεj,0, ξjτεj,1)τ (ξjτεj,0, ξjτεj,1){Σ-1 (Tk2)}τU
—
j=0
√≡σ⅞∧ιT⅛)U >ςT(TkI)”ki ∧ TTk mm )}> U
2
1	Tk1∧Tk2-1
≤ σ2(Tk1 α∧kTk2)kUk2max"1(Tk)k21Tk^	X	(Gj，0，5J)T(Gj，0，5J)-ω⑴
→P 0,
where a ∧ b = min(a, b). It follows that
T-1
2
K
X XakTk-1/Uτ∑T(Tk)(ξ>εj,0,ξTεj,ι)τ{σ(Tk)}-1 I(j < Tk)
(27)
j=0
k=1
-kX2 PSkσ⅞∧ιTkTk2)U TmTk2 心-1(Tk2)}TU=op ⑴.
In the proofs ofLemmas 3 and 4, we show that ∣∣Σ-1 (t) — (Σ(0)*)-1k2 = O(t-1/2) and ∣∣Ω(t)—
Ω⑼*∣2 = O(t-1∕2) for some matrices Σ⑼* and Ω(0)* that are invariant to t. Definitions of
these two matrices can be found in Sections F.5 and F.6. Under C2(ii) and the condition that q =
o(√T∕ logT), we can show ∣∣U∣∣2 = O(q1/2) and hence σ2(t) → (σ(0)*)2 where (σ(0)*)2 =
Uτ(Σ(O)*)-1Ω⑼*{(Σ⑼*)-1}TU. Similar to equation 27, we have
kXk PakTmTkTk2 ) U PI(TkI)M ∧ Tk2 HI(Tk2 )}TU	(28)
P	^X αkι ak2 (TkI ∧ Tk2 )
→ k⅛2 √TkX(σ(0Ψ
Uτ{∑(O)*}-1Ω(O)*{(Σ(O)*)-1}TU → E
akι ak2 (cki ∧ ck2 )
√ckι ck2
k1 6=k2
22
Under review as a conference paper at ICLR 2021
where ck’s are defined in Section 4.4. This together with equation 27 yields that
T-1
X
j=0
K	2
X akT-1/2U>∑-1(Tk)(ξ>εj,0,ξ>εj,1)>{σ(Tk)}-1I(j < Tk)	→
k=1
akι ak2 (CkI ∧ ck2 )
√ckι ck2
Conditions (a) and (b) are thus verified. By Lemma 4, we can show
^X ak√Tk {T(Tk ) - τ0} _ ^X ak √Tk {τ(Tk ) - τ0}	门、
k=ι	στ)	= k=ι	即	+ op(1),
for any (aι,…，aκ). This yields the joint asymptotic normality of our test statistics.
By equation 28, its covariance matrix is given by Ξ0 whose (k1, k2)-th entry is equal to
(ck1 ck2)-1/2 ck1 ∧ ck2 . Using similar arguments in proving equation 27, equation 28 and Step 3
of the proof in Theorem 1, Shi et al. (2020a), we can show Ξ is a consistent estimator for Ξ0 . This
completes the proof of Theorem 3 under D1.
F.2.2 Proof under D2
The proof is very similar to that under D1. Suppose we can show equation 17, equation 20, equa-
tion 21 and equation 24 hold. Then similar to the proof under D1, we have
√t{b(t) - τo}
σ⑴
√tU >Zι(t)
σ⑴
+ op (1).
The following lemma shows these assertions hold under D2 as well.
Lemma 5 Under the given conditions, we have equation 17, equation 20, equation 21 and equa-
tion 24 hold.
It follows that for any K-dimensional vector a = (aι,…，aκ)>,
K
X
k=1
ak√Tk近(Tk)- τ0}
σ(Tk)
K
X
k=1
ak √TU >Q(Tk)
σ(Tk)
+ op (1).
In the proof of Lemma 5, we show ∣∣{Σ(t)}-1k2 = O⑴，k∑(t) - Σ*∣∣2 = O(t-1/2) for some
time-invariant matrix Σ* that satisfies ∣∣(Σ* )-1∣∣2 = O(1). Itfollows that
k{Σ(t)}-1 - (Σ*)-1k2 = k{Σ(t)}-1(Σ(t) - Σ*)(Σ*)-1k2 ≤
k{Σ(t)}Tk2k∑(t) - Σ*k2k(∑*)-1k2 = O(t-1/2).
Similarly, we can show ∣∣Ω(t) - Ω*∣∣2 = O(t-1/2) for some matrix Ω*.
In addition, using similar arguments in the proof of Lemma 5, we can show equation 26 holds under
D2 as well. Now, the joint asymptotic normality of our test statistics follow using arguments from
Part 3 of the proof under D1. Similarly, we can show Ξ is consistent. This completes the proof
under D2.
F.2.3 Proof under D3
The proof under D1 indicates that equation 17, equation 20, equation 21 and equation 24 hold with
t= T1 . It follows that
√TT{b(Tι)- τ0}
√T1U >Z1(T1)
+ op (1).
(29)
The rest of the proof is divided into two parts. In the first part, we show for k = 2, ∙ ∙ ∙ , K,
√Tk{b(Tk) - T0}
σ7(Tk)
√Tk U >Z：(Tk)
-σ7(Tυ-
+ op(1),
(30)
23
Under review as a conference paper at ICLR 2021
for some Z； (Tk) and σ"Tk) defined below. In the second part, We show the assertion in Theorem 3
holds under D3.
Part 1:	For any 1 ≤ k ≤ K, consider the matrices
1	Tk -1 Σ(k) = T T	E E[∑j∣{(St,At,Yt)}0≤t<Tk-1 ] and Σ(k)= Tk - Tk-1 j=Tk-1	1	Tk -1 Tk - Tk-I .	". j=Tk-1
We show in Lemma 6 below that for k = 2,…，K, ∣Σ(k) - Σb (k)∣2 = op(q-1/2),	(31)
and k{∑(k)}-1∣2 = Op(i).	(32)
where Σ(k) = Tk- Pk=I(Ti- Ti-ι)∑(i).	
Lemma 6 Under the given conditions, we have equation 31 and equation 32 hold.
Notice that (Ti - Ti-1)/Tk → (ci - ci-1)/ck and k{ck-1Pik=1(ci - ci-1)Σ(i)}-1k2 = Op(1)
where c0 = 0. It follows from equation 31 that kck-1 Pik=1 (ci - ci-1 )(Σb (i) - Σ(i))k2 = op(q-1/2)
and hence
1k
T X(Ti- Ti-ι)(Σ⑺一Σ⑺)
Tk
i=1
∀k = 2,…，K.
2
Similar to the proof under D1, we can show
∣( T X (Ti-TiT)ς ⑺
)-1
= Op(1) and
2
T X(Ti- Ti-ι)Σ⑴)-{Σ%τ
op(q-1/2),
2
for k = 2,..∙,K. Thus, equation 21 and the first assertion in equation 17 hold with t
T2,T3,... ,Tk underD3.
In addition, similar to Lemma 6, we can show
1	Tk-1
λmax TL X E[ξj ξ>∣{(St,At,匕)}0≤t<Tk-1] = Op(1),	(33)
Tk - Tk-1
and
∣	1	Tk -1	∣
t _t- ΣS (ξjξ> -E[ξjξ>l{(St,At,Yt)}o≤t<Tk-ι])	= op(q-1/2),
∣ Tk - Tk-1	∣
∣	j=Tk-1	∣2
for k = 2,…，K. This yields ∣∣(Tk 一 Tk-I)T PTiT^_ 1 ξjξ>k2 = Op(1). AS a result, the second
assertion in equation 17 holds with t = T2,T3, ∙∙∙ , TK.
Moreover, using similar arguments in showing t-1 Pj=0(ξ>εj,0,ξ>εj,1)τ = Op(t-1/2√q) un-
der D1, we have by equation 33 that (Tk - Tk-1)-1 PjT=k -Tk1-1 (ξj> εj,0, ξj> εj,1)> = Op{(Tk -
Tk-I)T^√q}, for k = 1,…，K. Under the given conditions on {Tk}k, we obtain
t-1 PjZ0(ξTεj,0,ξ>εj,ι)τ = Op(t-"√q) for t = T2,T3,…，Tk.
Based on these results, using similar arguments in Part 1 of the proof under D1, we can show
pTk {β(Tk) - β*} = pTk Z1(Tk)+ op(1), ∀k ∈{2,…，K},	(34)
where
Tk
CI(Tk) = Tk X囱))-1	ξjεj,0
j=1
24
Under review as a conference paper at ICLR 2021
For 1 ≤ k ≤ K , define
1	Tk -1
Sk) = T _T—	X E[(ξ>εj,0,ξ>εj,ι)τ(ξ>εj,0,ξ> εj,1)|{(St,At,Yt)}0≤t<Tk-1 ],
Tk - Tk-1
j=Tk-1
and Ω(k)	=	T-I	Pk=I(Ti	-	T-ι)Σ(i).	For any 2 ≤ k ≤	K,	we have	λmin(Ω(k))	≥
λmin(T-1T1Ω⑴).Since T-ITI → c-1cι > 0 and λma(Ω⑴)=λma(Ω(Tι)) is bounded
K(k)∖
away from zero, λma(Ω	) is bounded away from zero for k = 2,…，K as well. Define
{σ* (Tk)}2 = U τ(Σ(k))-rΩ(k){(Σ(k) )-1}>U.
It can be shown that σ*(Tk)∕∣∣U∣∣2 is bounded away from zero, for k = 2,…，K. Using similar
arguments in Part 2 of the proof under D1, we can show equation 30 holds. This completes the proof
for Part 1.
Part 2:	Let σ*(T1) = σ(Tι). By equation 29 and equation 30, we have for any K-dimensional
vector a = (aι,…，aκ)τ that
K
X
k=1
ak VTk {b(Tk) — τo}
σ7(Tk)
ak√ ak VTkU TZI(Tk )	⑴
⅛一+op(1).
(35)
In the following, we show the leading term on the RHS of equation 35 is asymptotically normal.
Similar to the proof under D1, it suffices to verify the following conditions:
(a)	max0≤j<T | PK=I akT-1/2UT(Mk))-1(ξ>εj,0,ξ>εj,I)TI(j <Tk)| → 0;
(b)	PT-01 I PK=1 akTU/Uτ(Σ(k))T(ξ>εj,0,ξ>εj,ι)τ{σ*(Tk)}-1I(j < Tk)|2 converges to
some constant in probability.
Condition (a) can be proven in a similar manner as in Part 3 of the proof under D1. Notice that
for k = 2,…，K, Σ(k), Ω(k) and σ*(Tk) are random variables and depend on the observed data
history. In the proof of Lemma 6, we show ∣∣(∑(k))-1 - (∑**)-1∣/ =Op(T-1/2) for some
deterministic matrix Σ* and all k ∈ {2,…,K}. Similarly, we can show ∣∣Ω(k) - Ω**∣/ =
Op(T-1/2) and ∣∣{σ*(Tk)}2 - (σ**)2∣∣/ = Op(T-1/2) for some Σ*, σ** and all k ∈ {2, ∙∙∙ ,K}.
Moreover, using similar arguments in the proof of Lemma 6, we can show
Tk-1
1	X ξ ξjεj,0
Tk — Tk-1 ,勺卜 ξjεj,1
j=Tk-1
= op (q-1/2 ),
2
∀k = 2,…，K.
This further implies that
ξj εj,0
ξj εj,1
= op(q-1/2),
2
∀k = 2,…，K.
Based on these results, using similar arguments in Part 3 of the proof of Lemma 3, we obtain (b).
The joint asymptotic normality of √T1{b(T1) - τ0}∕σ*(T1), •…,√T1{b(T1) - τo}∕σ*(T⅛) thus
follows.
Consistency of Ξ can be similarly proven. We omit the details for brevity.
F.3 Proof of Theorem 1
As discussed in Section 4.3, (Z； ,Z2, ∙∙∙ , ZK)τ is jointly normal with mean zero and covariance
P
matrix Ξ, conditional on the observed data. By Theorem 1, we have Ξ → Ξ0 where Ξ0 is the
asymptotic covariance matrix of (Z1,Z/,…，ZK)τ. Let a；(t) = α(tT) for any 0 ≤ t ≤ 1, we
have α(Tk) → α*(ck) for any 1 ≤ k ≤ K. Notice that {bk}1≤k≤κ is a continuous function of ξ
25
Under review as a conference paper at ICLR 2021
and {α(Tk)}1≤k≤K, it follows that bbk → bk,0 for 1 ≤ k ≤ K, where {bk,0}1≤k≤K are recursively
defined as follows:
Pr 1m≤ja<xk(Zj,0
- bj,0) ≤ 0, Zk,0 > bk,0
α*(ck) - α*(ck-i),
where (Z1,0, Z2,0,…，Zκ,o)> is asymptotically normal with mean zero and covariance matrix Ξ0.
Theorem 3 implies that (Zι - √T1 τ°∕b(Tι), Z - √Tτ0/G(T公,…，Zk - √TKτ°∕b(Tκ))> →d
(Z1,0,Z2,0,…，Zk,o)>. Itfollows that
Pr (0 {Zj> bj }j ≤ Pr (° {Zj- Pjτo∕σ(Tj) > b }j → Pr ([ {j > j }j = α*(cj (36)
The proof is hence completed by noting that a(Tk) → α* (Ck). When τo = 0, We have EZk = o(1).
The rejection probability thus converges to the nominal level.
F.4 Proof of Theorem 2
Suppose τo = T-1/2 h for some h > 0. Based on the proof of Theorem 3, We can show b(Tk) → σ%
for some σk > 0. It follows from equation 36 that
Pr (0{Zj> bj}j = Pr (O{Zj- PTjτo∕b(Tj) > bj - h∕b(Tj∙)} j
→ Pr (U {Zj,0 > bj,0 - h7σ∏j > α*(ck).
The second assertion in Theorem 2 thus holds by noting that a(Tk) → α* (Ck).
Let h → ∞, we obtain
Pr (U % > bj}j = Pr (U {Zj3 > %,0-h∕σj + o(1) → L
The proof is hence completed.
F.5 Proof of Lemma 3
Under the given conditions in C1(i), C1(ii) and C2(ii), equation 20 and the second assertion in
equation 17 can be proven using similar arguments in the proof of Lemma E.2 and E.3 of Shi et al.
(2020a). We omit the proof for brevity.
It remains to show equation 21 and the first assertion in equation 17 hold. Recall that μ is the
density function of the stationary distribution Π (see the remark below Condition C1). In addition,
μ is uniformly bounded away from 0 and ∞ under C1(i). For a0 ∈ {0,1}, define the matrix
Σ(0)*(ao) = /	^X ξ(s, a){ξ(s, a) — γξ(s0, a0)}μ(s){(1 — a)b(0)(s) + a(1 — b(0)(s))}p(s0; a, s)dsds0.
Define
ς(0)* = Γ ∑⑼*(0)	^
=[	Σ(O)*(1).
The matrix ∑(O)* is the population limit of Σ (t) underD1. To prove the first assertion in equation 17,
we first show
k(Σ(O)*)-1k2 = O(1).	(37)
26
Under review as a conference paper at ICLR 2021
By definition, this is equivalent to show
k{Σ ⑼ *(a)}-1k2 = O(1),
for a ∈ {0,1}. The matrix Σ(0)*(0) can be written as
Σ ⑼ *(0)
f)*(0)
；20)*(o)∑202*(o)
where
∑10)*(0)
∑20)*(0)
∑202*(0)
J	Ψ(s){Ψ(s) — γΨ(s0)}>b(o)(s)μ(s)p(s0; 0, s)dsds0,
-Y (	Ψ(s)Ψ>(s0)(1 — b(0)(s))μ(s)p(s0; 0, s)dsds0,
s,s0∈S
ψ Ψ(s)Ψ>(s)μ(s)(1 — b(0)(s))p(s0; 1, s)ds.
s∈S
It follows that
{Σ ⑼ *(0)}-1
and hence
{∑10)*(0)}T
{∑(0)*(0)}-1∑(0)*(0){∑(o)*(0)}-1 {∑(0)*(0)[-1
-{Σ2,2 (0)}	Σ2,1 (0){Σ1,1 (0)}	{Σ2,2 (0)}
k{Σ ⑼ *(0)}-1∣∣2
ka1 k2
a1>
sup
=1,ka2k2=1
≤ sup
ka3k2=1,ka4k
{∑10)*(0)}T
{∑(0)*(0)}-1∑(0)*(0){∑(o)*(0)}-1 {∑(0)*(0)[-1
-{Σ2,2 (0)}	Σ2,1 (0){Σ1,1 (0)}	{Σ2,2 (0)}
a2
2 = 11a>{理1*(0La4l + ka3k2=sUPa4k2 = 1la>{理* ⑼}"1
+ sup	∣a>{∑202*(0)}-1 ∑20)*(O){∑10)*(O)}Ta4∣
ka3k2=1,ka4k2=1
/ Il ʃ V(0)* <flM-11l _i_ Il ʃ v(0)* <flM-11l _i_ Il ʃ v(0)* <flM-11l Il V(0)* fC∖∖ Il Il ʃ V(0)* <flM-11l
≤ k{Σ1,1 (0)} k2 + k{Σ2,2 (0)} k2 + k{Σ2,2 (0)} k2kΣ2,1 (0)k2k{Σ1,1 (0)}	k2.
Thus, to prove k{Σ⑼*(0)}-1k2 = O(1), it suffices to show
k{∑10)*(0)}T∣∣2 = O(1),
k{∑202*(0)}-1k2 = O(1),
k∑20)*(0)k2 = O(1).
(38)
(39)
(40)
We first consider equation 38. Using similar arguments in Part 1 of the proof of Lemma E.2, Shi
et al. (2020a), it suffices to show
a>∑ι0ι*(0)a ≥ cιkak2,	∀a,
for some & > 0. UnderD1, b(0) is strictly positive. Since μ is strictly positive, it suffices to show
a> /	Ψ(s){Ψ(s) — γΨ(s0)}>dsds0a ≥ 引⑷艮	∀a,
(41)
for some C2 > 0. Notice that LHS of equation 41 is equal to
λ(S)	{a>Ψ(s)}2ds — γ	{a>Ψ(s)}{a>Ψ(s0)}dsds0,
s∈S	s,s0 ∈S
where λ(S) is the Lebesgue measure of S. Since S is compact, we have λ(S) < +∞. By Cauchy-
Schwarz inequality, LHS of equation 41 is greater than or equal to
λ(S) / {a>Ψ(s)}2ds - λ(S) /	Y{a>Ψ(s)}2ds - λ(S) / Y{a>Ψ(s0)}ds0
≥ (1 — γ)λ(S)	{a>Ψ(s)}2ds.
s∈S
27
Under review as a conference paper at ICLR 2021
This is directly implied by Condition C2(ii). The proof of equation 38 is hence completed. Similarly,
we can prove equation 39. In addition, notice that
k∑20)*(0)k2 ≤
sup ∣	∣a>Ψ(s)∣∣a>Ψ(s0)∣{1 — b(o)(s)μ(s)}p(s0;0, s)dsds0
ka1k2=1,ka2k2=1 s,s0∈S
≤ sup ∣	∣a>Ψ(s)∣∣a>Ψ(s0)∣μ(s)p(s0; 0, s)dsds0.
ka1 k2 =1,ka2 k2 =1 s,s0∈S
Since the density function μ is uniformly bounded, we have
k∑20)*(0)k2 ≤。⑴
sup I	∣a>Ψ(s)∣∣a>Ψ(s0) ∣dsds0,
ka1k2=1,ka2k2=1 s,s0∈S
where O(1) denotes the universal constant. By Cauchy-Schwarz inequality, we obtain
k∑2O)*(0)∣2 ≤ O(1)λ(S) SUp [ ∣a>Ψ(s)∣2ds ≤ O(1)λ(S)λmaχ
kak2=1 s∈S
In view of C2(ii), we obtain equation 40.
To summarize, we
k{Σ⑼* ⑴}-1∣∣2 =
have shown ∣∣{Σ⑼*(0)}-1∣∣2
Ψ(s)Ψ> (s)ds .
s∈S
O(1). Similarly, we can prove
O(1). Assertion equation 37 thus holds. Similar to Lemma E.5 of Shi
et al. (2020a), we can show ∣∣Σ(t) - Σ⑼*∣∣2 = O(t-1/2). Using similar arguments in Part 1
of the proof of Lemma E.2, Shi et al. (2020a), this yields ∣∣Σ-1(t) - (Σ ⑼ *)-1∣∣2 = o(t-1/2) and
∣Σ-1(t)∣2 = O(1). Under the given conditions, equation 21 and the first assertion in equation 17
now follow from the arguments used in Part 2 and 3 of the proof of Lemma E.2, Shi et al. (2020a).
F.6 Proof of Lemma 4
The asymptotic normality of √t{b(t) - τ0}∕σ(t) can be proven using similar arguments in Part 3
of the proof of Theorem 3. In the following, we focus on equation 24. Define the matrix
Ω⑼*
So = s,A0 = a} μ(s){ab(O)(S) + (1 — a)(1
b(0)(s))}ds.
Similar to LemmaE.5 of Shi et al. (2020a), We can show ∣∣Ω⑼* - Ω(t)k2 = O(t-1/2). Thus, it
suffices to show infq λmin(Ω⑼*) > 0. Under CA and SRA, we have
E	0ε0,0	( ξ0ε0,0 >
0ε0,1	ξ0ε0,1
J ( ξ0(a)ε*(0,a)、( ξo(a)ε*(0,a) ∖>
Ell ξo(a)ε*(1,a) ) I ξo(a)ε*(1,a))
S0 = s, A0 = a
S0 = s, A0 = a
J ( ξo(a)ε*(0,a)、( ξo(a)ε*(0,a)、>
Ell ξo(a)ε*(1,a) )[ ξo(a)ε*(1,a))
S0 = s
)
)
)
For any 2q-dimensional vectors a1, a2 that satisfy ∣a1∣22 + ∣a2 ∣2 = 1, it follows that
ξ0ε0,0
ξ0ε0,1
0ε0,0
0ε0,1
S0 = s, A0 = a	(a1> , a2> )>
+
≥
—
+
≥
{a>ξ(s,a)}2E[{ε*(0,a)}2∣So = s] + {a> ξ(s,a)}2E[{ε*(1,a)}2∣So = s]
2{a>ξ(s,a)}{a>ξ(s,a)}E[{ε*(0,a)ε*(1,a)}∣So = s]
{a>ξ(s, a)}2E[{ε*(0, a)}2∣So = s] + {a>ξ(s, a)}⅛[{ε*(1, a)}2∣S0 = S]
2Pε∣a>ξ(s, a)∣∣a>ξ(s, a)∣pE[{ε*(0, a)}2∣S0 = s]E[{ε*(1, a)}2∣S0 = s]
(1 -ρε){a>ξ(s,a)}2E[{ε*(0,a)}2∣So = s] + (1 -ρε){a> ξ(s,a)}2E[{ε*(1,a)}2∣So = s]
Pε ∣∣a>ξ(s, a)∣,E∣{ε*(0, a)}2∣S0 = s] + ∣a>ξ(s, a)∣PE[{ε*(1,a)}2∣So = s]∣
(1 -ρε){a>ξ(s,a)}2E[{ε*(0,a)}2∣So = s] + (1 -ρε){a> ξ(s,a)}2E[{ε*(1,a)}2∣So = s],
28
Under review as a conference paper at ICLR 2021
where ρε	=	supq supa∈{0,1},s∈S ρε(a, s). Under C3, we have ρε	<	1 and
infq inf&o,&,$ E[{ε*(a0, a)}2∣S0 = s] > 0. It follows that
(a>, a>)E“ ξ0ε0,0 ) ( ξ0ε0,1 )> So = s,Ao = a} (a>, a>)> ≥e[{a]ξ(s,a)}2 + {a>ξ(s,a)}2],
for some constant c3 > 0. Therefore,
λmin(Ω⑼*)=	inf	(a>, a>)Ω⑼*(a>, a>)>
ka1 k2 +ka2 k2=1
≥ C3 ll ll2inf ll2 Z X [{a>ξ(s,a)}2 + {a>ξ(S,a)}2]〃(SXab ⑼(S) + (I-a)(1 - b(0)(s))}ds.
ka1k22+ka2k22=1 s∈S	1
a∈{0,1}
The strict positivity of μ(∙) and the condition that b(0)(∙) is uniformly bounded away from 0 and 1
yields
λmin(Ω⑼*) *4	inf X X [{a>ξ(s,a)}2 + {a>ξ(s,a)}2]ds,	(42)
ka1k22+ka2k22=1 s∈S
a∈{0,1}
for some constant c4 > 0. With some calculation, we can show the RHS of equation 42 is equal to
C4λmin ʃ Ψ Ψ(s)Ψ>(s)
s∈S
By Condition C2(ii), it is strictly positive. This yields infq λmin(Ω(0)*) > 0. Thus, we obtain
equation 24.
F.7 Proof of Lemma 5
We begin by proving
kΣ-1(t)k2 = O(1),	(43)
under D2. For any matrices M1 and M2, denote by diag[M1, M2] the block diagonal matrix
M1
M2	.
By MA and Condition C1(ii), the two Markov chains {S2t-1}t≥1, {S2t}t≥0 are geometrically er-
godic. Let μ1 and μ2 denote the density function of their stationary distributions, respectively. Under
C1(i), we can similarly show that they are uniformly bounded away from 0 and ∞. Define
Σ*1
Σ*2
/	diag[ξ(s, 1){ξ(s, 1) - γξ(s0, 0)}>,ξ(s, 1){ξ(s, 1) - γξ(S0, 1)}>]〃1(S)P(S0; 1, s)dsds,
s,s0∈S
/	diag[g(S, 0){g(S, O)- Yg(S0,0)}>, ξG 0XξG O)- γξ(S0,1)}>]〃2 (S)P(S0;0, S)dSdS'.
s,s0∈S
The matrix (Σ* + ∑*)∕2 corresponds to the population limit of Σ(t). Similar to Lemma E.5
of Shi et al. (2020a), we can show kΣ1* - (2t)-1 Ptj=0 EΣ2j+1k2 = o(t-1/2) and kΣ2* -
(2t)-1 Ptj=0 EΣ2j k2 = o(t-1/2). This further yields
ς1+-ς1 - Σ(t)
= o(t-1/2).
2
Similar to the proof of Lemma 3, in order to show equation 43, it suffices to show
k(Σ*1 + Σ*2)-1k2 = O(1).
Notice that Σ*1 + Σ2* = diag[Σ*(0), Σ* (1)] where
(44)
ς*(a) = /	[ξ(s, 0Xξ(s, 0) - γξ(s0, a)}〃2(S)P(S0；0, S) + ξ(s, 1Xξ(s, 1) - Yg(S0,a)}〃i(S)P(S0；1,s)] dsds'.
29
Under review as a conference paper at ICLR 2021
The matrix Σ* (0) can be further decomposed into
Σ*(0)
∑1,ι(0)
∑於⑼
∑北(0)
where
Σ]ι(0) = /	Ψ(s){Ψ(s) - γΨ(s0)}>μ2(s)p(s0; 0, s)dsds0,
∑2,1(0)
∑2,2(0)
—γ /	Ψ(s)Ψ>(s0)μι(s)p(s0; 1, s)dsds0,
s,s0 ∈S
J Ψ(s)Ψ>(s)μi(s)ds.
Similar to the proof of Lemma 3, We can show k{∑[ι(0)}T∣∣2 = O(1), k{∑2,2(0)}-lk2 =
O⑴ and k∑2,ι(0)∣∣2 = O⑴.It follows that k{Σ2(0)}-1k2 = O(1). Similarly, we can show
k{Σ2(1)}-1k2 = O(1). This proves equation 44. Thus, we obtain equation 43.
Using similar arguments in Part 2 of the proof of Lemma E.2, Shi et al. (2020a), we can show
kt-1 Ptj-=10 Σ2j - Σ22k2 = Op(t-1/2 log t) and kt-1 Ptj-=10 Σ2j+1 - Σ21k2 = Op(t-1/2 logt).
This further implies ∣∣Σ(t) — (∑2 + ∑2)∕2∣∣ = Op(t-1/2 logt) and hence ∣∣Σb(t) — Σ(t)k2 =
Op(t-1/2 log t). Combining these results together with equation 43, we can show equation 21 and
the first assertion in equation 17 hold. equation 20 and the second assertion in equation 17 hold can
be proven in a similar manner.
Finally, using similar arguments in the proof of Lemma 4, we can show equation 24 holds. We omit
the details to save space.
F.8 Proof of Lemma 6
Under C1(iv), we have equation 6 holds. Similar to equation 7, we can show Π(k) has a probability
density function μ(k) given by
μ(k)(s0)
XS
a∈{0,1} s∈S
[a{1 — b(k)(s)} + (1 — a)b(k)(s)]p(s0;
a, s)Π(k)(ds).
(45)
For a0 ∈ {0, 1}, define
Σ(k)2(a) = J	^X	ξ(s, a){ξ(s, a)	— γξ(s0,	a0)}>μ(k)(s){a{1	—	b(k)(s)}	+ (1 — a)b(k)(s)}p(s0;	a,	s)dsds0.
Condition on {(Sj, Aj, Yj)}1≤j <Tk-1, the matrix Σ(k)2(a) is deterministic. Let Σ(k) =
diag[Σ(k)2(0), Σ(k)2(1)]. Similar to the proof of Lemma 3, we can show ∣Σ(k)2 — Σ(k)∣2 = o(1),
conditional on {(Sj, Aj, Yj)}1≤j<Tk-1, with probability tending to 1. This implies for any suffi-
ciently small > 0,
Pr(k∑(k)2 — Σ(k)∣2 > d{(Sj,Aj,Yj)}1≤j<Tk-1) → 0.
The above conditional probability is bounded between 0 and 1. Using bounded convergence theo-
rem, we have
Pr(∣Σ(k)2 — Σ(k) ∣2 > ) = o(1),	(46)
and hence ∣Σ(k)2 — Σ(k) ∣2 = op(1).
Notice that sups |b(k)(s) — b2(s)| →P 0 and ∣Π(k) — Π2 ∣TV →P 0. Define
μ2(s)
XS
a∈{0,1} s∈S
[a{1 — b2 (s)} + (1 — a)b2 (s)]p(s0;
a, s)Π2 (ds).
30
Under review as a conference paper at ICLR 2021
It follows that
Imk)(S0)-μ*(s0)∣≤
XS
a∈{0,1} s∈S
{a∣b*(s) - b(k)(s)∣ + (1 - a)∣b*(s) - b(k)(s)∣}p(s0;
a, S)Π(k)(dS)
XS
a∈{0,1} s∈S
[a{1 - b*(s)} + (1 - a)b*(s)]p(s0;
a,s)∣Π(k)(ds) - Π*(ds)∣,
+
and hence sups ∣μ(k)(S) - μ*(s)∣ → 0. With some calculations, We can show ∣∣Σ(k)*(a)-
Σ* (a)∣∣2 → 0 where
Σ*(a) = J ^X ξ(s, a){ξ(s, a) — γξ(s0, a0)}>μ*(s){a{1 — b*(s)} + (1 — a)b*(s)}p(s0; a, s)dsds0.
Let Σ* = diag[Σ*(0), Σ*(1)], we obtain ∣∣Σ(k)* - Σ*∣∣2 = 0p(1). Combining this together with
equation 46, we obtain ∣∣Σ(k) - Σ*∣∣2 = 0p(1). The proof of Lemma 3 yields ∣∣Σ(1) - Σ(0)*∣2 =
o(1). Thus, we have for any 2 ≤ k ≤ K that
k∑(k) - T-IT1∑(°)* - T-I(Tk- T1)∑*k2 = Op(1).	(47)
Similar to the proof of Lemma 3, we can show μ(k)s are uniformly bounded away from 0 and ∞.
It follows that μ* is uniformly bounded away from 0 and ∞. Using similar arguments in Lemma 3,
we can show ∣∣{T-1Tι∑(0)* + T-I(Tk - Tι)∑*}-11∣2 = O(1). Using similar arguments in Part 1
of the proof of Lemma E.2, Shi et al. (2020a), we have by equation 47 that 11(2(2)-1112 = O(1),
with probability tending to 1. equation 32 is thus proven.
Assertion equation 31 now follows using similar arguments in Part 2 and Part 3 of the proof of
Lemma E.2, Shi et al. (2020a).
G	Additional figures
We present some additional figures to report the simulation results in this section. Figure 4 depicts
the empirical rejection probabilities of the modified version of the O’Brien & Fleming sequential
test developed by Kharitonov et al. (2015). It can been seen that such a test has no power at all. In
addition, we remark that Kharitonov et al. (2015)’s test requires equal sample size T1 = Tk - Tk-1
for k = 2, ∙∙∙ ,K and is not directly applicable to our setting with unequal sample size. To apply
such a test, we modify the decision time and set (T1, T2, T3, T4, T5) = (120, 240, 360, 480, 600).
Figure 5 depicts the empirical rejection probabilities of our test and two-sample t-test with the error
spending function given by α2. Figure 6 reports the empirical rejection probabilities of our test with
different combinations of the number of basis and the error spending function.
31
Under review as a conference paper at ICLR 2021
Figure 4: Empirical rejection probabilities of the modified version of the O’Brien & Fleming sequen-
tial test developed by Kharitonov et al. (2015). The left panels depicts the empirical type-I error and
the right panels depicts the empirical power. Settings correspond to the alternating-time-interval,
adaptive and Markov design, from top plots to bottom plots.
AI三qeqo」d :pa'artr:A{=qeqo-d 433∙OΓH A,⅛=qeqo.Jd 4□3'ara:
一 eAJ£ul3E=6,EleE£(03>⅛epe >o⅛re∑
6 4 2 0 6 4 2 0 6 4 2 0
Oooo Oooo Oooo
o.addo.ao.0
Al=IqeqO.Id Iɔ,ɑiəa:
一 eΛJΘlu<θ Ev6u∙4≡e E£E
(b) Two-sample t-test under H1 and H0
(from left plots to right plots)
Figure 5: Empirical rejection probabilities of our test and the two-sample t-test With α(∙) = α2(∙).
Settings correspond to the alternating-time-interval, adaptive and Markov design, from top plots to
bottom plots.
(a) The proposed test under H1 and H0
(from left plots to right plots)
32
Under review as a conference paper at ICLR 2021
(a) The proposed test under H1 and H0 (from
left plots to right plots). J = 3, α(∙) = αι (∙).
(b) The proposed test under H1 and H0 (from
left plots to right plots). J = 3, α(∙) = α2(∙).
(c)	The proposed test under H1 and H0 (from
left plots to right plots). J = 5, α(∙) = ɑι (∙).
(d)	The proposed test under H1 and H0 (from
left plots to right plots). J = 5, α(∙) = α2(∙).
Figure 6: Empirical rejection probabilities of our test. Settings correspond to the alternating-time-
interval, adaptive and Markov design, from top plots to bottom plots.
33