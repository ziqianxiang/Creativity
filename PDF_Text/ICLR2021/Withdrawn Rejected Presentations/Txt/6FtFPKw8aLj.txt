Under review as a conference paper at ICLR 2021
Systematic Analysis of Cluster Similarity
Indices: How to Validate Validation Measures
Anonymous authors
Paper under double-blind review
Ab stract
There are many cluster similarity indices used to evaluate clustering algorithms,
and choosing the best one for a particular task remains an open problem. We
demonstrate that this problem is crucial: there are many disagreements among the
indices, these disagreements do affect which algorithms are chosen in applications,
and this can lead to degraded performance in real-world systems. We propose
a theoretical solution to this problem: we develop a list of desirable properties
and theoretically verify which indices satisfy them. This allows for making an
informed choice: given a particular application, one can first select properties that
are desirable for a given application and then identify indices satisfying these. We
observe that many popular indices have significant drawbacks. Instead, we advocate
using other ones that are not so widely adopted but have beneficial properties.
1	Introduction
Clustering is an unsupervised machine learning problem, where the task is to group objects that are
similar to each other. In network analysis, a related problem is called community detection, where
grouping is based on relations between items (links), and the obtained clusters are expected to be
densely interconnected. Clustering is used across various applications, including text mining, online
advertisement, anomaly detection, and many others (Allahyari et al., 2017; Xu & Tian, 2015).
To measure the quality of a clustering algorithm, one can use either internal or external measures.
Internal measures evaluate the consistency of the clustering result with the data being clustered,
e.g., Silhouette, Hubert-Gamma, Dunn, and many other indices. Unfortunately, it is unclear whether
optimizing any of these measures would translate into improved quality in practical applications.
External (cluster similarity) measures compare the candidate partition with a reference one (obtained,
e.g., by human assessors). A comparison with such a gold standard partition, when it is available, is
more reliable. There are many tasks where external evaluation is applicable: text clustering (Amigo
et al., 2009), topic modeling (Virtanen & Girolami, 2019), Web categorization (Wibowo & Williams,
2002), face clustering (Wang et al., 2019), news aggregation (see Section 3), and others. Often, when
there is no reference partition available, it is possible to let a group of experts annotate a subset of
items and compare the algorithms on this subset.
Dozens of cluster similarity measures exist and which one should be used is a subject of debate (Lei
et al., 2017). In this paper, we systematically analyze the problem of choosing the best cluster
similarity index. We start with a series of experiments demonstrating the importance of the problem
(Section 3). First, we construct simple examples showing the inconsistency of all pairs of different
similarity indices. Then, we demonstrate that such disagreements often occur in practice when
well-known clustering algorithms are applied to real datasets. Finally, we illustrate how an improper
choice of a similarity index can affect the performance of production systems.
So, the question is: how to compare cluster similarity indices and choose the best one for a particular
application? Ideally, we would want to choose an index for which good similarity scores translate to
good real-world performance. However, opportunities to experimentally perform such a validation
of validation indices are rare, typically expensive, and do not generalize to other applications. In
contrast, we suggest a theoretical approach: we formally define properties that are desirable across
various applications, discuss their importance, and formally analyze which similarity indices satisfy
them (Section 4). This theoretical framework would allow practitioners to choose the best index
1
Under review as a conference paper at ICLR 2021
based on relevant properties for their applications. In Section 5, we advocate two indices that are
expected to be suitable across various applications.
While many ideas discussed in the paper can be applied to all similarity indices, we also provide a
more in-depth theoretical characterization of pair-counting ones (e.g., Rand and Jaccard), which gives
an analytical background for further studies of pair-counting indices. We formally prove that among
dozens of known indices, only two have all the properties except for being a distance: Correlation
Coefficient and Sokal & Sneath’s first index (Lei et al., 2017). Surprisingly, both indices are rarely
used for cluster evaluation. The correlation coefficient has an additional advantage of being easily
convertible to a distance measure via the arccosine function. The obtained index has all the properties
except constant baseline, which is still satisfied asymptotically.
Constant baseline is a particular focus of the current research: this is one of the most important
and non-trivial properties. Informally, a sensible index should not prefer one candidate partition
over another just because it has too large or too small clusters. To the best of our knowledge, we
are the first to develop a rigorous theoretical framework for analyzing this property. In this respect,
our work improves over the previous (mostly empirical) research on constant baseline of particular
indices (Albatineh et al., 2006; Lei et al., 2017; Strehl, 2002; Vinh et al., 2009; 2010), we refer to
Appendix A for a detailed comparison to related research.
2	Cluster similarity indices
We assume that there is a set of elements V with size n = |V |. A clustering is a partition of V into
disjoint subsets. Capital letters A, B, C will be used to name the clusterings, and we will represent
them as A = {A1, . . . , AkA}, where Ai is the set of elements belonging to i-th cluster. If a pair of
elements v, w ∈ V lie in the same cluster in A, we refer to them as an intra-cluster pair of A, while
inter-cluster pair will be used otherwise. The total number of pairs is denoted by N = n2 . The
value that an index I assigns to the similarity between partitions A and B will be denoted by I(A, B).
Let us now define some of the indices used throughout the paper, while a more comprehensive list,
together with formal definitions, is given in Appendix B.1 and B.2.
Pair-counting indices consider clusterings to be similar if they agree on many pairs. Formally, let
A~ be the N -dimensional vector indexed by the set of element-pairs, where the entry corresponding
to (v, w) equals 1 if (v, w) is an intra-cluster pair and 0 otherwise. Further, let MAB be the N × 2
matrix that results from concatenating the two (column-) vectors A~ and B~ . Each row of MAB is
either 11, 10, 01, or 00. Let the pair-counts N11, N10, N01, N00 denote the number of occurrences
for each of these rows in MAB .
Definition 1. A pair-counting index is a similarity index that can be expressed as a function of the
pair-counts N11 , N10, N01 , N00.
Some popular pair-counting indices are Rand and Jaccard:
R =N11 + Noo j =Nil
N11 + N10 + N01 + Noo，	N11 + N10 + N01
Adjusted Rand (AR) is a linear transformation of Rand ensuring that for a random B we have
AR(A, B) = 0 in expectation. A less widely used index is the Pearson Correlation Coefficient
(CC) between the binary incidence vectors A~ and B~.1 Another index, which we discuss further in
more details, is the Correlation Distance CD(A, B) := ∏1 arccos CC(A, B). In Table 4, We formally
define 27 known pair-counting indices and only mention ones of particular interest throughout the
main text.
Information-theoretic indices consider clusterings similar if they share a lot of information, i.e.,
if little information is needed to transform one clustering into the other. Formally, let H(A) :=
H(|A1|/n, . . . , |AkA |/n) be the Shannon entropy of the cluster-label distribution ofA. Similarly, the
joint entropy H(A, B) is defined as the entropy of the distribution With probabilities (pij)i∈[kA],j∈[kB],
Where pij = |Ai ∩ Bj |/n. Then, the mutual information of tWo clusterings can be defined as
1Note that Spearman and Pearson correlation are equal When comparing binary vectors. Kendall rank
correlation for binary vectors coincides With the Hubert index, Which is linearly equivalent to Rand.
2
Under review as a conference paper at ICLR 2021
M (A, B) = H(A) + H(B) - H(A, B). There are multiple ways of normalizing the mutual
information, the most widely used ones are:
NMI(A B)=	M(AB)	NMI	(a B)=	M(AB)
NMI(A,B)= (H (A) + H(B))., NMImax(A,B) = maχ{H (A),H(B)}.
NMI is known to be biased towards smaller clusters, and several modifications try to mitigate this
bias: Adjusted Mutual Information (AMI) and Standardized Mutual Information (SMI) subtract the
expected mutual information from M(A, B) and normalize the obtained value (Vinh et al., 2009),
while Fair NMI (FNMI) multiplies NMI by a penalty factor e-|kA -kB |/kA (Amelio & Pizzuti, 2015).
3 M otivating experiments
As discussed in Section 2, many cluster similarity indices are used by researchers and practitioners. A
natural question is: how to choose the best one? Before trying to answer this question, it is important
to understand whether the problem is relevant. Indeed, if the indices are very similar to each other
and agree in most practical applications, then one can safely take any index. In this section, we
demonstrate that this is not the case, and the proper choice matters.
First, we illustrate the inconsistency of all indices. We say that two indices I1 and I2 are inconsistent for a triplet of	• ∙ ■ ■
partitions (A, B1, B2) if I1 (A, B1) > I1(A,B2)butI2(A,B1) < I2(A,B2). We took 15 popular cluster similar- ity measures and constructed just four triplets such that each pair of indices is inconsistent for at least one triplet. One example is shown in Figure 1: for this simple example, about half of the indices prefer the left candidate, while the others prefer the right one. Other examples can be found in Appendix F.1. Thus, we see that the indices differ. But can this affect conclusions obtained in experiments on real data? To check that, we ran 8 well-known clustering algo- rithms (Scikit-learn, 2020) on 16 real- world datasets (GitHub, 2020). Each dataset, together with a pair of al- gorithms, gives a triplet of partitions (A, B1, B2), where A is a reference par-	■ ■ ■ ■ ■ ■ ■ ■ (a) FNML R, AR, J, D,	(b) NML NMImax, VL W, FMeasure, BCubed	AMI, S&S, CC, CD Figure 1: Inconsistency of indices: shapes denote the refer- ence partition, captions indicate indices favoring the candi- date. Table 1: Inconsistency of indices on real-world clustering datasets, % NMI VI AR	S&S1 CC NMI	- 40.3	15.7	20.1	18.5 VI	- 37.6	36.0	37.2 AR	-	11.7	8.3 S&S1	-	3.6 CC	-
tition and B1 , B2 are provided by two algorithms. For a given pair of indices and all such triplets, we
look at whether the indices are consistent. Table 1 shows the relative inconsistency for several indices
(the extended table together with a detailed description of the experimental setup and more analysis is
given in Appendix F.2). The inconsistency rate is significant: e.g., popular measures Adjusted Rand
and Variation if Information disagree in almost 40% of the cases, which is huge. Interestingly, the
best agreeing indices are S&S and CC, which satisfy most of our properties, as shown in the next
section. In contrast, the Variation of Information very often disagrees with other indices.
To show that the choice of similarity index may affect the final performance in a real production
scenario, we conducted an experiment within a major news aggregator system. The system groups
news articles to events and shows the list of most important events to users. For grouping, a clustering
algorithm is used, and the quality of this algorithm affects the user experience: merging different
clusters may lead to not showing an important event, while too much splitting may cause duplicate
events. When comparing several candidate clustering algorithms, it is important to determine which
one is the best for the system. Online experiments are expensive and can be used only for the best
candidates. Thus, we need a tool for an offline comparison. For this purpose, we manually created
a reference partition on a small fraction of news articles. We can use this partition to evaluate the
3
Under review as a conference paper at ICLR 2021
candidates. We performed such an offline comparison for two candidate algorithms and observed that
different indices preferred different algorithms. Then, we launched an online user experiment and
verified that one of the candidates is better for the system according to user preferences. Hence, it is
important to be careful when choosing a similarity index for the offline comparison. See Appendix F.3
for more detailed description of this experiment and quantitative analysis.
4 Analysis of cluster similarity indices
In this section, we motivate and formally define properties that are desirable for cluster similarity
indices. We start with simple and intuitive ones that can be useful in some applications but not always
necessary. Then, we discuss more complicated properties, ending with constant baseline that is
extremely important but least trivial. In Tables 2 and 3, indices of particular interest are listed along
with the properties satisfied. In Appendix C, we give the proofs for all entries of these tables.
For pair-counting indices we perform a more detailed analysis and define additional properties. For
such indices, we interchangeably use the notation I(A, B) and I (N11 , N10, N01 , N00).
Some of the indices have slight variants that are essentially the same. For example, the Hubert
Index (Hubert, 1977) can be expressed as a linear transformation of the Rand index as H(A, B) =
2R(A, B) - 1. All the properties defined in this paper are invariant under linear transformations and
interchanging A and B . Hence, we define the following linear equivalence relation on similarity
indices and check the properties for at most one representative of each equivalence class.
Definition 2. Similarity indices I1 and I2 are linearly equivalent if there exists a nonconstant linear
function f such that either I1 (A, B) = f (I2 (A, B)) or I1(A, B) = f(I2(B, A)).
This allows us to conveniently restrict to indices for which higher numerical values indicate higher
similarity of partitions. Table 5 in the Appendix lists the equivalent indices.
4.1	Property 1: Maximal agreement
The numerical value that an index assigns to a similarity must be easily interpretable. In particular, it
should be easy to see whether the candidate clustering is maximally similar to (i.e., coincides with)
the reference clustering. Formally, we require that I(A, A) - cmax is constant and either a strict
upper bound for I(A, B) for all A 6= B. The equivalence from Definition 2 allows us to assume that
I(A, A) is a maximum w.l.o.g. This property is easy to check, and it is satisfied by almost all indices,
except for SMI and Wallace.
Property 10 : Minimal agreement The maximal agreement property makes the upper range of the
index interpretable. Similarly, having a numerical value for a low agreement would make the lower
range interpretable. A minimal agreement is not well defined for general partitions: it is not clear
which partition would be most dissimilar to a given one. However, referring to Lemma 1 in Appendix,
pair-counting indices form a subclass of graph similarity indices. For a given graph G = (V, E), it is
clear that the graph most dissimilar to G is its complement GC = (V, EC). Comparing a graph to its
complement would result in pair-counts N11 = N00 = 0 and N10 + N01 = N . This motivates the
following definition:
Definition 3. A pair-counting index I has the minimal agreement property if there exists a constant
cmin so that I(N11, N10, N01, N00) ≥ cmin with equality if and only if N11 = N00 = 0.
This property is satisfied by Rand, Correlation Coefficient, and Sokal&Sneath, while it is violated
by Jaccard, Wallace, and Dice. Adjusted Rand does not have this property since substituting
N11
N00 = 0 gives the non-constant AR(0, N10, N01, 0)
NioNoi
1N2-NioNoi .
4.2	Property 3: S ymmetry
Similarity is intuitively understood as a symmetric concept. Therefore, a good similarity index
is expected to be symmetric, i.e., I(A, B) = I(B, A) for all partitions A, B.2 Tables 2 and 3
2In some applications, A and B may have different roles (e.g., reference and candidate partitions), and an
asymmetric index may be suitable if there are different consequences of making false positives or false negatives.
4
Under review as a conference paper at ICLR 2021
Table 2:	Requirements for general simi-
larity indices
Table 3:
NMImax
FNMI
VI
SMI
FMeasure
BCubed
AMI
a-≡WSBq-suo□ X
yticinotonoM ✓
ytixelpmoc.niL ✓
aɔugɑ X
yrtemmyS ✓
tnemeerga.xaM ✓
R
AR
J
W
D
CC
S&S1
CD
saibfoepyT%&&&&
au=%BqSUoɔ ∙sv x∕xxx∕//
aWSBqSUoɔ x∕xxx∕∕x
Wuoiouoiuuos /Xxxx///
Auouow / // X/ // /
IXdlus ∙7 / // // // /
suα /X/XXXX/
Ajiiuas / // X/ // /
lualu。；M∙w /Xxxx///
JUaUIaJMπ∙xπw 3//X////
show that most indices are symmetric. The asymmetric ones are precision and recall (Wallace) and
FNMI (Amelio & Pizzuti, 2015), which is a product of NMI and the asymmetric penalty factor.
4.3	Property 4: Linear complexity
For clustering tasks on large datasets, running time is crucial, and algorithms with superlinear time can
be infeasible. In these cases, a validation index with superlinear running time would be a significant
bottleneck. Furthermore, computationally heavy indices also tend to be complicated and hard to
interpret intuitively. We say that an index has linear complexity when its worst-case running time is
O(n). In Appendix C.2, we prove that any pair-counting index has O(n) complexity. Many general
indices have this property as well, except for SMI and AMI.
4.4	Property 4. Distance
For some applications, a distance-interpretation of dissimilarity may be desirable: whenever A is
similar to B and B is similar to C, then A should also be somewhat similar to C. For example,
assume that we have the reference clustering that is an approximation of the ground truth (e.g., labeled
by experts). In such situations, it may be reasonable to argue that the reference clustering is at most
a distance ε from the true clustering, so that the triangle inequality bounds the dissimilarity of the
candidate clustering to the unknown true clustering.
A function d is a distance metric if it satisfies three distance axioms: 1) symmetry (d(A, B) =
d(B, A)); 2) positive-definiteness (d(A, B) ≥ 0 with equality iff A = B); 3) the triangle inequality
(d(A, C) ≤ d(A, B) + d(B, C)). We say that I is linearly transformable to a distance metric if there
exists a linearly equivalent index that satisfies these three distance axioms. Note that all three axioms
are invariant under re-scaling of d. We have already imposed the symmetry as a separate property,
and the positive-definiteness is equivalent to the maximal agreement property. Therefore, whenever I
has these two properties, it satisfies the distance property iff d(A, B) = cmax - I(A, B) satisfies the
triangle inequality, for cmax as defined in Section 4.1.
Examples of popular indices having this property are Variation of Information and the Mirkin metric.
In Vinh et al. (2010), it is proved that when Mutual Information is normalized by the maximum of
entropies, the resulting NMI is equivalent to a distance metric. A proof that the Jaccard index is
equivalent to a distance is given in Kosub (2019). See Appendix C.1 for all the proofs.
Correlation Distance Among all the considered indices, there are two pair-counting ones having
all the properties except for being a distance: Sokal&Sneath and Correlation Coefficient. However,
the correlation coefficient can be transformed to a distance metric via a non-linear transformation. We
2All known pair-counting indices excluded from this table do not satisfy either constant baseline, symmetry,
or maximal agreement.
5
Under review as a conference paper at ICLR 2021
define Correlation Distance (CD) as CD(A, B) := ∏ arccos CC(A, B), where CC is the Pearson
correlation coefficient and the factor 1∕∏ scales the index to [0,1]. To the best of our knowledge,
this Correlation Distance has never before been used as a similarity index for comparing clusterings
throughout the literature.
Let us point out that the Correlation Distance is indeed a distance. It follows from the fact that the
correlation coefficient is obtained by first mapping the binary vectors A~, B~ to the unit sphere, and
then taking their standard inner product. The arccosine of the inner product of two unit vectors
corresponds to their angle, which is indeed a distance metric. A more detailed proof of this claim can
be found in Appendix E.2. Further in this section, we show that the distance property of Correlation
Distance is achieved at the cost of not having the exact constant baseline, which is still satisfied
asymptotically.
4.5	Property 5: Monotonicity
When one clustering is changed such that it resembles the other clustering more, the similarity score
ought to improve. Hence, we require an index to be monotone w.r.t. changes that increase the
similarity. This can be formalized via the following definition.
Definition 4. For clusterings A and B, we say that B0 is an A-consistent improvement of B iff
B 6= B0 and all pairs of elements agreeing in A and B also agree in A and B0.
This leads to the following monotonicity property.
Definition 5. An index I satisfies the monotonicity property if for every two clusterings A, B and
any B0 that is an A-consistent improvement of B, it holds that I(A, B0) > I(A, B).
To look at monotonicity from a different perspective, we define the following operations:
•	Perfect split: B0 is a perfect split of B (w.r.t. A) if B0 is obtained from B by splitting a
single cluster B1 into two clusters B10 , B20 such that no two elements of the same cluster of
A are in different parts of this split, i.e., for all i, Ai ∩ B1 is a subset of either B10 or B20 .
•	Perfect merge: We say that B0 is a perfect merge of B (w.r.t. A) if there exists some Ai
and B1, B2 ⊂ Ai such that B0 is obtained by merging B1, B2 into B10 .
The following theorem gives an alternative definition of monotonicity and is proven in Appendix E.1.
Theorem 1. B0 is an A-consistent improvement of B iff B0 can be obtained from B by a sequence
of perfect splits and perfect merges.
Note that this monotonicity is a stronger form of the first two constraints defined in (Amigo et al.,
2009): Cluster Homogeneity is a weaker form of our monotonicity w.r.t. perfect splits, while Cluster
Equivalence is equivalent to our monotonicity w.r.t. perfect merges.
Monotonicity is a critical property that should be satisfied by any sensible index. Surprisingly, not all
indices satisfy this: we have found counterexamples that prove that SMI, FNMI, and Wallace do not
have the monotonicity property. Furthermore, for NMI, whether monotonicity is satisfied depends
on the normalization: the normalization by the average of the entropies has monotonicity, while the
normalization by the maximum of the entropies does not.
Property 50. Strong monotonicity For pair-counting indices, we can define a stronger monotonic-
ity property in terms of pair-counts.
Definition 6. A pair-counting index I satisfies strong monotonicity if it increases with N11, N00 and
decreases with N10 , N01.
This property is stronger than monotonicity as it additionally allows for comparing similarities across
different settings: we could compare the similarity between A1, B1 on n1 elements with the similarity
between A2, B2 on n2 elements, even when n1 6= n2. This ability to compare similarity scores across
different numbers of elements is similar to the Few data points property of SMI (Romano et al., 2014)
that allows its scale to have a similar interpretation across different settings.
6
Under review as a conference paper at ICLR 2021
We found several examples of indices that have Property 5 while not satisfying Property 50 . Jaccard
and Dice indices are constant w.r.t. N00, so they are not strongly monotone. A more interesting
example is the Adjusted Rand index, which may become strictly larger if we only increase N10 .
4.6	Property 6. Constant baseline.
This property is arguably the most significant: it is less intuitive than the other ones and may lead to
unexpected consequences in practice. Informally, a good similarity index should not give a preference
to a candidate clustering B over another clustering C just because B has many or few clusters. This
intuition can be formalized using random partitions: assume that we have some reference clustering
A and two random partitions B and C. While intuitively both random guesses are equally bad
approximations of A, it has been known throughout the literature (Albatineh et al., 2006; Romano
et al., 2014; Vinh et al., 2010) that some indices tend to give higher scores for random guesses with
a larger number of clusters. Ideally, we want the similarity value of a random candidate w.r.t. the
reference partition to have a fixed expected value cbase (independent of A). We formalize this in
the following way. Let S(B) denote the specification of the cluster sizes of the clustering B, i.e.,
S(B) := [|B1 |, . . . , |BkB |], where [. . . ] denotes a multiset. For a cluster sizes specification s, let
C(s) be the uniform distribution over clusterings B with S(B) = s.
Definition 7. An index I satisfies the constant baseline property if there exists a constant cbase so that
EB 〜C(S) [I (A, B)] = C base for any cluster-sizes specification S and clustering A with 1 < kA < n.
Note that this property is symmetric since it does not matter whether we permute the labels of A
while keeping B constant or vice versa. In the definition, we have excluded the cases where A is
a trivial clustering consisting of either 1 or n clusters. Including them would cause problems for
s = S(A), as C(s) would be a constant distribution surely returning A and any sensible index should
have I(A, A) 6= cbase.
Constant baseline is extremely important in many practical applications: if an index violates this
property, then its optimization may lead to undesirably biased results. For instance, if a biased index
is used to choose the best algorithm among several candidates, then it is likely that the decision will
be biased towards those who produce too large or too small clusters. This problem is often attributed
to NMI (Romano et al., 2014; Vinh et al., 2010), but we found out that almost all indices suffer from
it. The only indices that satisfy the constant baseline property are Adjusted Rand index, Correlation
Coefficient, SMI, and AMI with cbase = 0 and Sokal&Sneath with cbase = 1/2. Interestingly, out of
these five indices, three were specifically designed to satisfy this property, which made them less
intuitive and resulted in other important properties being violated.
The only condition under which the constant baseline property can be safely ignored is knowing in
advance all cluster sizes. In this case, bias towards particular cluster sizes would not affect decisions.
However, we are not aware of any practical application where such an assumption can be made. Note
that knowing only the number of clusters is insufficient. We illustrate this in Appendix D.4, where
we also show that the bias of indices violating the constant baseline is easy to identify empirically.
Property 60 : Asymptotic constant baseline For pair-counting indices, a deeper analysis of the
constant baseline property is possible. Let mA = N11 + N10, mB = N11 + N01 be the number
of intra-cluster pairs of A and B, respectively. Note that mA and mB are constant as A is constant
and B 〜C(s), so that its cluster-sizes are constant. Furthermore, the pair-counts N10, N01, N00 are
functions of N, mA, mB, N11. Hence, to find the expected value of the index, we need to inspect
it as a function of a single random variable N11. For a random pair, the probability that it is an
intra-cluster pair of both clusterings is mAmB/N2, so the expected values of the pair-counts are
N11 :
mA mB -ʌr-	-ʌr- -ʌr—	R—
—N-, N10 := mA - Nii, N01 := mB - Nil,
N00 := N - mA - mB + N11.
(1)
We can use these values to define a weaker variant of constant baseline.
Definition 8. A Pair-Counting index I has the asymptotic constant baseline property ifthere exists a
constant C base so that I (N11,N10, N01,N00) = C base for all A with 1 < kA ‹ n.
In contrast to Definition 7, asymptotic constant baseline is very easy to verify: one just has to
substitute the values from (1) to the index and check whether the obtained value is constant. Another
important observation is that under mild assumptions I (Nii, Ni0, N0i, N00) converges in probability
7
Under review as a conference paper at ICLR 2021
to I (Nιι,Ni0, N01,N00) as n grows which justifies the usage of the name asymptotic constant
baseline, see Appendix D.2 for more details.
Note that the non-linear transformation of Correlation Coefficient to Correlation Distance makes the
latter one violate the constant baseline property. CD does, however, still have the asymptotic constant
baseline at 1/2 and we prove in Appendix E.3 that the expectation in Definition 7 is very close to this
value. To the best of our knowledge, there does not exist a cluster similarity index that is a distance
while having the exact constant baseline.
Biases of cluster similarity indices Given the fact that there are so many biased indices, one may
be interested in what kind of candidates they favor. While it is unclear how to formalize this concept
for general validation indices, we can do this for pair-counting ones by analyzing them in terms of a
single variable.
While there are previous attempts to characterize types of biases (Lei et al., 2017), they mostly rely
on an analysis based on the number of clusters. However, we argue that the number of clusters is not
a good measure of the granularity of a clustering. Instead, we show that the number of inter-cluster
pairs should be analysed to determine the biases of pair-counting indices. We formally define and
analyze two types of biases: NPdec and NPinc, where NP stands for Number of inter-cluster Pairs.
Definition 9. Let I be a pair-counting index and define I(S) (mA, mB) = I(N11, N10, N01,N00)
for the expected pair-counts as defined in (1). We define the following biases:
(i)	I SUfferSfrOm NPdec bias ifthere are mA,mB ∈ (0,N) such that ^^I(S) (mA, mB) > 0.
(ii)	I suffersfrom NPinc bias ifthere are mA, mB ∈ (0, N) such that -rd-1 (s)(mA, mB) < 0.
mB
Applying this definition to Jaccard J(S) (mA, mB) = N(山篙器mAmB and Rand R(S) (mA, Pb )=
1 - (mA + mB)/N + 2mAmB/N2 immediately shows that Jaccard suffers from NPdec bias and
Rand suffers from both biases, confirming the findings of Lei et al. (2017). Furthermore, the direction
of the monotonicity for the bias of Rand is now determined by the condition 2mA > N instead of
the more complicated but equivalent condition on the quadratic entropy of A that is given in Lei et al.
(2017). Performing the same for Wallace and Dice shows that both suffer from NPdec bias. Note that
an index satisfying the asymptotic constant baseline property will not have any of these biases as
I(S)(mA, mB)
= cbase .
5	Discussion and conclusion
At this point, we better understand the theoretical properties of cluster similarity indices, so it is time
to answer the question: which index is the best? Unfortunately, there is no simple answer, but we can
make an informed decision. In this section, we sum up what we have learned, argue that there are
indices that are strictly better alternatives than some widely used ones, and give practical advice on
how to choose a suitable index for a given application.
Among all properties discussed in this paper, monotonicity is the most crucial one. Violating this
property is a fatal problem: such indices can prefer candidates which are strictly worse than others.
Hence, we cannot advise using the well-known NMImax, FMeasure, FNMI, and SMI indices.
The constant baseline property is much less trivial but is equally important: it addresses the problem
of preferring some partitions only because they have small or large clusters. This property is
essential unless you know all cluster sizes. Since we are not aware of practical applications where
all cluster sizes are known, below we assume that this is not the case.3 * This requirement is satisfied
by just a few indices, so we are only left with AMI, Adjusted Rand (AR), Correlation Coefficient
(CC), and Sokal&Sneath (S&S). Additionally, Correlation Distance (CD) satisfies constant baseline
asymptotically and deviations from the exact constant baseline are extremely small (see Section E.3).
Let us note that among the remaining indices, AR is strictly dominated by CC and S&S since it does
not have the minimum agreement and strong monotonicity. Also, similarly to AMI, AR is specifically
3However, in applications where such an assumption holds, it can be reasonable to use, e.g., BCubed,
Variation of Information, and NMI.
8
Under review as a conference paper at ICLR 2021
created to have a constant baseline, which made this index more complex and less intuitive than other
pair-counting indices. Hence, we are only left with four indices: AMI, S&S, CC, and CD.
According to their theoretical properties, all these indices are good, and any of them can be
chosen. Figure 2 illustrates how a final decision can be made. First, one can decide whether
the distance property is needed. For example, suppose one wants to cluster the algorithms
by comparing the partitions provided by them. In that case, the metric property of a similar-
ity index allows the use of metric clustering algorithms. In this case, a distance property is
desirable, and CD is the best choice: it has all properties except for the exact constant base-
line, which is still satisfied asymptotically. Next, it is important to decide whether computa-
tion time is of the essence. Linear computation time is essential for large-scale applications.
For instance, assume that there is a pro-
duction system that groups news articles
or user photos. There is a candidate al-
gorithm, and we want to compare it with
the currently used one to avoid major
changes. In this case, we have to com-
pare huge partitions, and time is of the
essence. Another example is multiple
comparisons: choosing the best algo-
rithm among many candidates (differ-
ing, e.g., by a parameter value). If this
is the case, then AMI is not a proper
choice, and one has to choose between
CC and S&S. Otherwise, all three in-
dices are suitable according to our for-
mal constraints.
Figure 2: Example of how one can make a decision among
good cluster similarity indices.
Let us discuss an (informal) criterion that may help to choose between AMI and pair-counting
alternatives. Different indices may favor a different balance between errors in small and large clusters.
In particular, all pair-counting indices give larger weights to errors in large clusters: misclassifying
one element in a cluster of size k costs k- 1 incorrect pairs. It is known (empirically) that information-
theoretic indices do not have this property and give a higher weight to small clusters (Amigo et al.,
2009).4 Amigo et al. (2009) argue that for their particular application (text clustering), it is desirable
not to give a higher weight to large clusters. In contrast, there are applications where the opposite
may hold. For instance, consider a system that groups user photos based on identity and shows
these clusters to a user as a ranked list. In this case, a user is likely to investigate the largest clusters
consisting of known people and would rarely spot an error in a small cluster. The same applies to any
system that ranks the clusters, e.g., to news aggregators. Based on what is desirable for a particular
application, one can choose between AMI and pair-counting CC and S&S.
The final decision between CC and S&S is hard to make since they are equally good in terms of
their theoretical properties. Interestingly, although some works (Choi et al., 2010; Lei et al., 2017)
list Pearson correlation as a cluster similarity index, it has not received attention that our results
suggest it deserves, similarly to S&S. First, both indices are interpretable. CC is a correlation between
the two incidence vectors, which is a very natural concept. S&S is the average of precision and
recall (for binary classification of pairs) plus their inverted counterparts, which can also be intuitively
understood. Also, CC and S&S usually agree in practice: in Tables 1 and 6 we can see that they have
the largest agreement. Hence, one can take any of these indices. Another option would be to check
whether there are situations where these indices disagree and, if this happens, perform an experiment
similar to what we did in Section 3 for news aggregation.
Finally, while some properties listed in Tables 2 and 3 are not mentioned in the discussion above,
they can be important for particular applications. For instance, maximum and minimum agreements
are useful for interpretability, but they can also be essential if some operations are performed over the
index values: e.g., averaging the scores of different algorithms. Symmetry can be necessary if there
is no “gold standard” partition, but algorithms are compared only to each other.
4This is an interesting aspect that has not received much attention in our research since we believe that the
desired balance between large and small clusters may differ per application and we are not aware of a proper
formalization of this “level of balance” in a general form.
9
Under review as a conference paper at ICLR 2021
References
Ahmed N. Albatineh, Magdalena Niewiadomska-Bugaj, and Daniel Mihalko. On similarity in-
dices and correction for chance agreement. Journal of Classification, 23(2):301-313, SeP 2006.
ISSN 1432-1343. doi: 10.1007/s00357-006-0017-z. URL https://doi.org/10.1007/
s00357-006-0017-z.
Mehdi Allahyari, Seyedamin Pouriyeh, Mehdi Assefi, Saied Safaei, Elizabeth D TriPPe, Juan B
Gutierrez, and Krys Kochut. A brief survey of text mining: Classification, clustering and extraction
techniques. arXiv preprint arXiv:1707.02919, 2017.
Alessia Amelio and Clara Pizzuti. Is normalized mutual information a fair measure for comParing
community detection methods? In Proceedings of the 2015 IEEE/ACM International Conference
on Advances in Social Networks Analysis and Mining 2015, PP. 1584-1585. ACM, 2015.
EnriqUe Amigo, Julio Gonzalo, Javier Artiles, and Felisa Verdejo. A comparison of extrinsic
clustering evaluation metrics based on formal constraints. Information retrieval, 12(4):461-486,
2009.
Vladimir Batagelj and Matevz Bren. Comparing resemblance measures. Journal of Classification, 12
(1):73-90, Mar 1995. ISSN 1432-1343. doi: 10.1007/BF01202268. URL https://doi.org/
10.1007/BF01202268.
Shai Ben-David and Margareta Ackerman. Measures of clustering quality: A working set of axioms
for clustering. Advances in neural information processing systems, 21:121-128, 2008.
Shai Ben-David and Margareta Ackerman. Measures of clustering quality: A working set of axioms
for clustering. In Advances in neural information processing systems, pp. 121-128, 2009.
Seung-Seok Choi, Sung-Hyuk Cha, and Charles C Tappert. A survey of binary similarity and distance
measures. Journal of Systemics, Cybernetics and Informatics, 8(1):43-48, 2010.
Claire Donnat and Susan Holmes. Tracking network dynamics: a survey of distances and similarity
metrics, 2018.
GitHub. Clustering datasets. https://github.com/deric/clustering-benchmark,
2020.
Lawrence Hubert. Nominal scale response agreement as a generalized correlation. British Journal
of Mathematical and Statistical Psychology, 30(1):98-103, 1977. doi: 10.1111/j.2044-8317.
1977.tb00728.x. URL https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.
2044-8317.1977.tb00728.x.
Lawrence Hubert and Phipps Arabie. Comparing partitions. Journal of classification, 2(1):193-218,
1985.
Jon Kleinberg. An impossibility theorem for clustering. Advances in neural information processing
systems, 15:463-470, 2002.
Sven Kosub. A note on the triangle inequality for the jaccard distance. Pattern Recogni-
tion Letters, 120:36 - 38, 2019. ISSN 0167-8655. doi: https://doi.org/10.1016/j.patrec.
2018.12.007. URL http://www.sciencedirect.com/science/article/pii/
S0167865518309188.
Yang Lei, James C. Bezdek, Simone Romano, Nguyen Xuan Vinh, Jeffrey Chan, and James
Bailey. Ground truth bias in external cluster validity indices. Pattern Recognition, 65:58 -
70, 2017. ISSN 0031-3203. doi: https://doi.org/10.1016/j.patcog.2016.12.003. URL http:
//www.sciencedirect.com/science/article/pii/S0031320316303910.
Marina Meila. Comparing clusteringsan information based distance. Journal Ofmultivariate analysis,
98(5):873-895, 2007. URL https://www.sciencedirect.com/science/article/
pii/S0047259X06002016.
10
Under review as a conference paper at ICLR 2021
Mark EJ Newman and Michelle Girvan. Finding and evaluating community structure in networks.
Physical review E, 69(2):026113, 2004.
S Romano, J Bailey, Nguyen the vinh, and Karin Verspoor. Standardized mutual information for
clustering comparisons: One step further in adjustment for chance. 31st International Conference
on Machine Learning, ICML 2014, 4:2873-2882, 01 2014.
Simone Romano, Nguyen Xuan Vinh, James Bailey, and Karin Verspoor. Adjusting for chance
clustering comparison measures. The Journal of Machine Learning Research, 17(1):4635-4666,
2016.
Scikit-learn. Clustering algorithms. https://scikit-learn.org/stable/modules/
clustering.html, 2020.
Alexander Strehl. Relationship-based clustering and cluster ensembles for high-dimensional data
mining. PhD thesis, 2002.
Stijn Van Dongen and Anton J Enright. Metric distances derived from cosine similarity and pearson
and spearman correlations. arXiv preprint arXiv:1208.3145, 2012.
Twan Van Laarhoven and Elena Marchiori. Axioms for graph clustering quality functions. The
Journal of Machine Learning Research ,15(1):193-215, 2014.
Nguyen Xuan Vinh, Julien Epps, and James Bailey. Information theoretic measures for clusterings
comparison: is a correction for chance necessary? In Proceedings of the 26th annual international
conference on machine learning, pp. 1073-1080. ACM, 2009.
Nguyen Xuan Vinh, Julien Epps, and James Bailey. Information theoretic measures for clusterings
comparison: Variants, properties, normalization and correction for chance. Journal of Machine
LearningResearch,11(Oct):2837-2854, 2010.
Seppo Virtanen and Mark Girolami. Precision-recall balanced topic modelling. In Advances in
Neural Information Processing Systems,pp. 6750-6759, 2019.
Zhongdao Wang, Liang Zheng, Yali Li, and Shengjin Wang. Linkage based face clustering via graph
convolution network. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 1117-1125, 2019.
Wahyu Wibowo and Hugh E Williams. Strategies for minimising errors in hierarchical web categori-
sation. In Proceedings of the eleventh international conference on Information and knowledge
management, pp. 525-531, 2002.
Dongkuan Xu and Yingjie Tian. A comprehensive survey of clustering algorithms. Annals of Data
SCienCe, 2(2):165-193,2015.
A	Further related work
Several attempts to the comparative analysis of cluster similarity indices have been made in the
literature, both in machine learning and complex networks communities. In particular, the problem of
indices favoring clusterings with smaller or larger clusters has been identified (Albatineh et al., 2006;
Lei et al., 2017; Vinh et al., 2009; 2010). The most popular approach to resolving the bias of an index
is to subtract its expected value and normalize the resulting quantity to obtain an index that satisfies
the maximum agreement property. This approach has let to ‘adjusted’ indices such as AR (Hubert &
Arabie, 1985) and AMI (Vinh et al., 2009). In Albatineh et al. (2006), the family of pair-counting
indices L is introduced for which adjusted forms can be computed easily. This family corresponds to
the set of all pair-counting indices that are linear functions of N11 for fixed N11 + N10, N11 + N01. A
generalization of information-theoretic indices by Tsallis q-entropy is given in Romano et al. (2016)
and is shown to correspond to pair-counting indices for q = 2. Formulas are provided for adjusting
these generalized indices for chance.
A disadvantage of this adjustment scheme is that an index can be normalized in many ways, while
it is difficult to grasp the differences between these normalizations intuitively. For example, three
11
Under review as a conference paper at ICLR 2021
variants of AMI have been introduced (Vinh et al., 2009), and we show that normalization by the
maximum entropies results in an index that fails monotonicity. Romano et al. (2014) go one step
further by standardizing mutual information, while Amelio & Pizzuti (2015) multiply NMI with a
penalty factor that decreases with the difference in the number of clusters.
In summary, all these works take a popular biased index and ‘patch’ it to get rid of this bias. This
approach has two disadvantages: firstly, these patches often introduce new problems (e.g., FNMI
and SMI fail monotonicity), and secondly, the resulting index is usually less interpretable than the
original. We have taken a different approach in our work: instead of patching existing indices, we
analyze previously introduced indices to see whether they satisfy more properties. Our analysis shows
that ARI is dominated by Pearson correlation, which was introduced more than 100 years before ARI.
Therefore, there was no need to construct ARI from Rand in the first place.
In Lei et al. (2017), the biases of pair-counting indices are characterized. They define these biases
as a preference towards either few or many clusters. They prove that the direction of Rand’s bias
depends on the Havrda-Charvat entropy of the reference clustering. In the present work, we show that
the number of clusters is not an adequate quantity for expressing these biases. We introduce methods
to easily analyze the bias of any pair-counting index and simplify the condition for the direction of
Rand’s bias to mA < N/2.
A paper closely related to the current research (Amigo et al., 2009) formulates several constraints
(axioms) for cluster similarity indices. Their cluster homogeneity is a weaker analog of our mono-
tonicity w.r.t. perfect splits while their cluster equivalence is equivalent to our monotonicity w.r.t.
perfect merges. The third rag bag constraint is motivated by a subjective claim that “introducing
disorder into a disordered cluster is less harmful than introducing disorder into a clean cluster”.
While this is important for their particular application (text clustering), we found no other work
that deemed this constraint necessary; hence, we disregarded this constraint in the current research.
The last constraint by Amigo et al. (2009) concerns the balance between making errors in large
and small clusters. Though this is an interesting aspect that has not received much attention in our
research, this constraint poses a particular balance while we believe that the desired balance may
differ per application. Hence, this property seems to be non-binary and we are not aware of a proper
formalization of this “level of balance” in a general form. Hence, we do not include this in our list
of formal properties. The most principal difference of our work compared to Amigo et al. (2009)
is the constant baseline which was not analyzed in their work. We find this property extremely
important while it is failed by most of the widely used indices including their BCubed. To conclude,
our research gives a more comprehensive list of constraints and focuses on those that are desirable in
a wide range of applications. We also cover all similarity indices often used in the literature and give
formal proofs for all index-property combinations.
A property similar to our monotonicity property is also given in Meila (2007), where the similarity
between clusterings A and B is upper-bounded by the similarity between A and A 0 B (as defined in
Section C.4). One can show that this property is implied by our monotonicity but not vice versa, i.e.,
the variant proposed by Meila (2007) is weaker. Our analysis of monotonicity generalizes and unifies
previous approaches to this problem, see Theorem 1 that relates consistent improvements to perfect
splits and merges.
While we focus on external cluster similarity indices that compare a candidate partition with a
reference one, there are also internal similarity measures that estimate the quality of partitions with
respect to internal structure of data (e.g., Silhouette, Hubert-Gamma, Dunn, and many other indices).
Kleinberg (2002) used an axiomatic approach for internal measures and proved an impossibility
theorem: there are three simple and natural constraints such that no internal clustering measure
can satisfy all of them. More work in this direction can be found in, e.g., Ben-David & Ackerman
(2008). In network analysis, internal measures compare a candidate partition with the underlying
graph structure. They quantify how well a community structure (given by a partition) fits the
graph and are often referred to as goodness or quality measures. The most well-known example is
modularity (Newman & Girvan, 2004). Axioms that these measures ought to satisfy are given in
(Ben-David & Ackerman, 2009; Van Laarhoven & Marchiori, 2014). Note that all pair-counting
indices discussed in this paper can also be used for graph-partition similarity, as we discuss in
Section B.3.
12
Under review as a conference paper at ICLR 2021
B Cluster similarity indices
B.1 General indices
Here we give the definitions of the indices listed in Table 2. We define the contingency variables as
nij = |Ai ∩ Bj |. We note that all indices discussed in this paper can be expressed as functions of
these contingency variables.
The F-Measure is defined as the harmonic mean of recall and precision. Recall is defined as
1 kA
r(A,B ) = n X ,mkxyj}，
i=1
and precision is its symmetric counterpart r(B, A).
In (Amigo et al., 2009), recall is redefined as
and BCubed is defined as the harmonic mean of r0(A, B) and r0(B, A).
The remainder of the indices are information-theoretic and require some additional definitions. Let
p1, . . . , p` be a discrete distribution (i.e., all values are nonnegative and sum to 1). The Shannon
entropy is then defined as
`
H(p1, . . . ,p`) := -	pilog(pi).
i=1
The entropy of a clustering is defined as the entropy of the cluster-label distribution of a random item,
i.e.,
H(A) := H(|A1|/n,...,|AkA|/n),
and similarly for H(B). The joint entropy H(A, B) is then defined as the entropy of the distribution
with probabilities (pij)i∈[kA],j∈[kB], where pij = nij /n.
Variation ofInformation (Meila, 2007) is defined as
VI(A,B) = 2H(A, B) -H(A) -H(B).
Mutual information is defined as
M(A, B) = H(A) + H(B) -H(A,B).
The mutual information between A and B is upper-bounded by H(A) and H(B), which gives multi-
ple possibilities to normalize the mutual information. In this paper, we discuss two normalizations:
normalization by the average of the entropies 1 (H(A) + H(B)), and normalization by the maximum
of entropies max{H (A), H(B)}. We will refer to the corresponding indices as NMI and NMImax,
respectively:
NMI(A, B)
NMImax(A,B)=
M (A,B)
(H(A) + H(B))/2,
M (A,B)
max{H(A), H(B)} .
Fair NMI is a variant of NMI that includes a factor that penalizes large differences in the number of
clusters (Amelio & Pizzuti, 2015). It is given by
FNMI(A, B) = e-|kA-kB|/kANMI(A,B).
In this definition, NMI may be normalized in various ways. We note that a different normalization
would not result in more properties being satisfied.
13
Under review as a conference paper at ICLR 2021
Adjusted Mutual Information addresses for the bias of NMI by subtracting the expected mutual
informationVinh et al. (2009). It is given by
AMI(A, B)
M(A, B)- EBO〜C(S(B))[M(A, BO)]
PH(A) ∙ H(B) - Ebo〜C(S(B)) [M(A, B0)]
Here, a normalization by the geometric mean of the entropies is used, while other normalizations are
also used (Vinh et al., 2009).
Standardized Mutual Information standardizes the mutual information w.r.t. random permutations of
the items (Romano et al., 2014), i.e.,
SMI(A, B)
M(A, B)- eb0-c(s(b))(M(A,BO))
σB0〜C(S(B))(M(A, BO))
where σ denotes the standard deviation. Calculating the expected value and standard deviation of the
mutual information is nontrivial and requires significantly more computation power than other indices.
For this, we refer to the original paper (Romano et al., 2014). Note that this index is symmetric since
it does not matter whether we keep A constant while randomly permuting B or keep B constant
while randomly permuting A.
B.2	Pair-counting indices and their equivalences
Pair-counting similarity indices are defined in Table 4. Table 5 lists linearly equivalent indices
(see Definition 2). Note that our linear equivalence differs from the less restrictive monotonous
equivalence given in (Batagelj & Bren, 1995). In the current work, we have to restrict to linear
equivalence as the constant baseline property is not invariant to non-linear transformations.
B.3	Defining the subclass of pair-counting indices
From Definition 1 of the main text, it follows that a pair-counting index is a function of two binary
vectors A~, B~ of length N. Note that this binary-vector representation has some redundancy: whenever
u, v and v, w form intra-cluster pairs, we know that u, w must also be an intra-cluster pair. Hence, not
every binary vector of length N represents a clustering. The class of N -dimensional binary vectors
is, however, isomorphic to the class of undirected graphs on n vertices. Therefore, pair-counting
indices are also able to measure the similarity between graphs. For example, for an undirected
graph G = (V, E), one can consider its incidence vector G~ = (1{{v, w} ∈ E})v,w∈V . Hence, pair-
counting indices can be used to measure the similarity between two graphs or between a graph and a
clustering. So, one may see a connection between graph and cluster similarity indices. For example,
the Mirkin metric is a pair-counting index that coincides with the Hamming distance between the
edge-sets of two graphs (Donnat & Holmes, 2018). Another example is the Jaccard graph distance,
which turns out to be more appropriate for comparing sparse graphs (Donnat & Holmes, 2018). Thus,
all pair-counting indices and their properties discussed in the current paper can also be applied to
graph-graph and graph-partition similarities.
In this section, we show that the subclass of pair-counting similarity indices can be uniquely defined
by the property of being pair-symmetric.
For two graphs G1 and G2 let MG1G2 denote the N × 2 matrix that is obtained by concatenating
their adjacency vectors. Let us write IM(G)(MG1G2) for the similarity between two graphs G1, G2
according to some graph similarity index I(G). We will now characterize all pair-counting similarity
indices as a subclass of the class of similarity indices between undirected graphs.
Definition 10. We define a graph similarity indexIM(G)(MG1G2) to be pair-symmetric if interchanging
two rows of MG1 ,G2 leaves the index unchanged.
We give the following result.
Lemma 1. The class of pair-symmetric graph similarity indices coincides with the class of pair-
counting cluster similarity indices.
5Throughout the literature, the Mirkin metric is defined as 2(N10 +N01), but we use this variant as it satisfies
the scale-invariance.
14
Under review as a conference paper at ICLR 2021
Table 4: A selection of pair-counting indices. Most of these indices are taken from Lei et al. (2017).
Index (Abbreviation)	Expression
Rand (R)		N11+N00	 N11+N10+N01 +N00 N11	(nii+n10)(nii+n0i)
Adjusted Rand (AR)		N11- Nii+Nio + Noi+No鼠	 (Nll+Nlθ) + (Nll+N0l)(双门+双^乂 N^ +N。1) 2	Nii+Nio+Noi+Noo
Jaccard (J)		Nii	 Νii+Νio+Νoi
Jaccard Distance (JD)	Nio+Noi Nii+Nio+Noi
Wallace1 (W)	Nii Nii+Nio
Wallace2	Nii Nii+Noi
Dice	2Nii 2Nii+Nio+Noi
Correlation Coefficient (CC)	NiiNoo-NioNoi √(Nii+NiO)(Nii+NoI)(Noo + NiO)(Noo + NoI)
Correlation Distance (CD)	1	(	NiiNoo-NioNoi	∣ arccos π	∖ ʌ/(Nii+Nio)(Nii+NoI)(Noo+NiO)(Noo+NoI))
Sokal&Sneath-I (S&S1 )	1 ( Nii	+	Nii	+	Noo	+	Noo ʌ 4 ∖ Nii +Nio 丁 Nii+Noi 丁 Noo+Nio 丁 Noo+Noi )
Minkowski	/ Nio + Noi V Nii+Nio
Hubert (H)	Nii+Noo-Nio-Noi Nii+Nio +Noi+Noo
Folkes&Mallow	Nii √ (Nii+NiO)(Nii+NoI)
Sokal&Sneath-II	2 Nii 2 Nii+Nio+Noi
Normalized Mirkin5		Nio+Noi	 Nii+Nio+Noi +Noo
Kulczynski	1 ( Nii	I	Nii ʌ 21Nii+Nio + Nii+Noi )
McConnaughey	N2i-NioNoi (Nii+Nio)(Nii+Noi)
Yule	NiiNoo-NioNoi NiiNio+Noi Noo
Baulieu-I	(Nii+Nio+Noi +Noo)(Nii +Noo) + (Nio-Noi)2 (Nii+Nio+Noi +Noo)2
Russell&Rao		Nii	 Nii+Nio+Noi +Noo
Fager&McGowan		Nii	 1 — √(Nii+Nio)(Nii+Noi)	2 √Nii + Nio
Peirce	NiiNoo-NioNoi (Nii+Noi)(Noo+Nio)
Baulieu-II	NiiNoo-NioNoi (Nii+Nio+Noi +Noo)2
Sokal&Sneath-III		NiiNoo	 √ (Nii+NiO)(Nii+NoI)(Noo + NiO)(Noo+ Noi)
Gower&Legendre		1 Nii+NOO	 Nii + 2 (NiO+NOi ) + NOO
Rogers&Tanimoto		Nii+NOO	 Nii+2(Nio+Noi)+Noo
Goodman&Kruskal	NiiNoo-NioNoi NiiNOO+NiO NOi
Proof. A matrix is an ordered list of its rows. An unordered list is a multiset. Hence, when we
disregard the ordering of the matrix MAB, we get a multiset of the rows. This multiset contains at
most four distinct elements, each corresponding to one of the pair-counts. Therefore, each IM(G) (MAB)
that is symmetric w.r.t. interchanging rows is equivalently a function of the pair-counts of A and
B.	□
15
Under review as a conference paper at ICLR 2021
Table 5: Equivalent pair-counting indices
Representative Index	Equivalent indices
Rand Jaccard Wallace1 Kulczynski	Normalized Mirkin Metric, Hubert Jaccard Distance Wallace2 McConnaughey
C Checking properties for indices
In this section, we check all non-trivial properties for all indices. The properties of symmetry, maxi-
mal/minimal agreement and asymptotic constant baseline can trivially be tested by simply checking
I(B,A) = I(A, B), I(A,A) = cmax, I(0,N10,N01,0) = cmin andI(p)(pApB,pA,pB) = cbase
respectively.
C.1 Distance
C.1.1 Positive cases
NMI and VI. In (Vinh et al., 2010) it is proven that for max-normalization 1 - NMI is a distance,
while in (Meila, 2007) it is proven that VI is a distance.
Rand. The Mirkin metric 1 - R corresponds to a rescaled version of the size of the symmetric
difference between the sets of intra-cluster pairs. The symmetric difference is known to be a distance
metric.
Jaccard. In Kosub (2019), it is proven that the Jaccard distance 1 - J is indeed a distance.
Correlation Distance. In Theorem E.2 it is proven that Correlation Distance is indeed a distance.
C.1.2 Negative cases
To prove that an index that satisfies symmetry and maximal agreement is not linearly transformable to
a distance metric, we only need to disprove the triangle inequality for one instance of its equivalence
class that is nonnegative and equals zero for maximal agreement.
FNMI and Wallace. These indices cannot be transformed to distances as they are not symmetric.
SMI. SMI does not satisfy the maximal agreement property (Romano et al., 2014), so it cannot be
transformed to a metric.
FMeasure and BCubed. We will use a simple counter-example, where |V | = 3, kA = 1, kB =
2, kC = 3. Let us denote the FMeasure and BCubed by FM, BC respectively. We get
1-FM(A,C) = 1-0.5 > (1-0.8)+(1-0.8) = (1-FM(A,B))+(1-FM(B,C))
and
1 -BC(A,C) = 1 -0.5 > (1 - 0.71) + (1 - 0.8) ≈ (1 -BC(A,B))+(1 - BC(B, C)),
so that both indices violate the triangle inequality in this case.
Adjusted Rand, Dice, Correlation Coefficient, Sokal&Sneath and AMI. For these indices,
we use the following counter-example: Let A = {{0, 1}, {2}, {3}}, B = {{0, 1}, {2, 3}}, C =
{{0}, {1}, {2, 3}}. Then pAB = pBC = 1/6 and pAC = 0 while pA = pC = 1/6 and pB = 1/3.
By substituting these variables, one can see that
1 - I(p)(pAC,pA,pC) > (1 - I(p)(pAB,pA,pB)) + (1 - I(p)(pBC,pB,pC)),
holds for each of these indices, contradicting the triangle inequality. The same A, B and C also form
a counter-example for AMI.
16
Under review as a conference paper at ICLR 2021
C.2 Linear complexity
We will frequently make use of the following lemma:
Lemma 2. The nonzero values of nij can be computed in O(n).
Proof. We will store these nonzero values in a hash-table that maps the pairs (i, j) to their value nij .
These values are obtained by iterating through all items and incrementing the corresponding value of
nij . For hash-tables, searches and insertions are known to have amortized complexity complexity
O(1), meaning that any sequence of n such actions has worst-case running time of O(n), from which
the result follows.	口
C.2.1 Positive cases
NMI, FNMI and VI. Given the positive values of nij , it is clear that the joint and marginal entropy
values can be computed in O(n). From these values, the indices can be computed in constant time,
leading to a worst-case running time of O(n).
FMeasure and BCubed. Note that in the expressions of recall and precision as defined by these
indices, only the positive values of nij contribute. Furthermore, all of the variables ai , bj and nij
appear at most once, so that these can indeed be computed in O(n).
Pair-counting indices. Note that N11 = Pn >0 n2ij can obviously be computed in O(n). Sim-
ilarly, mA = Pk=AI (a2i) and mB can be computed in O(ka), O(kB) respectively. The other pair-
counts are then obtained by N10 = mA - N11, N01 = mB - N11 and N00 = N - mA - mB + N11.
C.2.2 AMI AND SMI.
Both of these require the computation of the expected mutual information. It has been known Romano
et al. (2016) that this has a worst-case running time of O(n ∙ max{ka, kB }) while max{ka, ka} can
be O(n).
C.3 Strong monotonicity
C.3.1 Positive cases
Correlation Coefficient. This index has the property that inverting one of the binary vectors
results in the index flipping sign. Furthermore, the index is symmetric. Therefore, we only need
to prove that this index is increasing in N11. We take the derivative and omit the constant factor
((Noo + N10)(N00 + N01))-1:
Noo	(NIIN00 - Ni0N0i) ∙ 2(2Nιι + N10 + NOI)
--	:一___________________：^^TTT-----^T1^^Z----
pN1+NOy(N1+Niy	[(nu + Nc(N11 + N01)]1.5
=1 NIINOO(N10 + NOI) + NOON10 N01 + 1NION01(2NII + N10 + NOI)	0
[(NII + NIO)(N11 + n01 )[1∙5	[(NII + NIO)(N11 + N01)]1.5 > .
Correlation Distance. The correlation distance satisfies strong monotonicity as it is a monotone
transformation of the correlation coefficient, which meets the property.
Sokal&Sneath. All four fractions are nondecreasing in N11, NOO and nonincreasing in N1O, NO1
while for each of the variables there is one fraction that satisfies the monotonicity strictly so that the
index is strongly monotonous.
Rand Index. For the Rand index, it can be easily seen from the form of the index that it is increasing
in N11 , NOO and decreasing in N1O , NO1 so that it meets the property.
17
Under review as a conference paper at ICLR 2021
C.3.2 Negative cases
Jaccard, Wallace, Dice. All these three indices are constant w.r.t. N00 . Therefore, these indices
do not satisfy strong monotonicity.
Adjusted Rand. It holds that
AR(1, 2, 1, 0) < AR(1, 3, 1, 0),
so that the index does not meet the strong monotonicity property.
C.4 Monotonicity
C.4.1 Positive cases
Rand, Correlation Coefficient, Sokal&Sneath, Correlation Distance. Strong monotonicity im-
plies monotonicity. Therefore, these pair-counting indices satisfy the monotonicity property.
Jaccard and Dice. It can be easily seen that these indices are increasing in N11 while decreasing
in N10, N01. For N00, we note that whenever N00 gets increased, either N10 or N01 must decrease,
resulting in an increase of the index. Therefore, these indices satisfy monotonicity.
Adjusted Rand. Note that for b, b + d > 0, it holds that
a + c a	ad
b + d>b 今 °> b .
For Adjusted Rand, we have
a = N11 — N (N 11 + N10)(N11 + N01),	b = a + 2(N10 + N01).
(2)
Because of this, when we increment either N11 or N00 while decrementing either N10 or N01, we
get d = C - 1. Hence, We need to prove c > a(c - ɪ)/b,or, equivalently
C - a = N(N11 + Nιo)(Nιι + Noι) - N11
2(b — a)	N10 + N01
For simplicity We reWrite this to
pAB - pApB
C + I	>	> 0,
pA + pB - 2pAB
where PA = N(N11 + Nio) ∈ (0,1) andPB = N(N 11 + N0ι) ∈ (0,1). If we increment N00 while
decrementing either N10 or N01, then
C ∈ {N(N11 + N10), N'(N11 + N01)} = {PA,PB}.
The symmetry of AR allows us to w.l.o.g. assume that C = PA . We write
+	PAB 一 PAPB	= PA + (1 - 2pa)pab
PA + Pb - 2pab	PA + Pb - 2pab .
When pa ≤ ɪ, then this is clearly positive. For the case PA > 11, we bound PAB ≤ PA and bound
the numerator by
p2A + (1 - 2pA)pA = (1 - pA)pA > 0.
This proves the monotonicity for increasing N00. When incrementing N11 While decrementing either
N10 or N01, We get C ∈ {1 - pA, 1 - pB}. Again, We assume W.l.o.g. that C = 1 - pA and Write
PAB - PAPB	PA(1 - PA) + (1 - 2PA)(PB - PAB )
1 - PA +----1------5---=-------------1------5------------.
PA + PB - 2PAB	PA + PB - 2PAB
This is clearly positive whenever PA ≤ 1. When PA > 1, we bound PAB ≥ PA + PB - 1 and rewrite
the numerator as
PA(1 - PA) + (1 - 2PA)(PA - 1) = (1 - PA)(3PA - 1) > 0.
This proves monotonicity for increasing N11. Hence, the monotonicity property is met.
18
Under review as a conference paper at ICLR 2021
NMI and VI. Let B0 be obtained by a perfect split of a cluster B1 into B10 , B20 . Note that this
increases the entropy of the candidate while keeping the joint entropy constant. Let us denote this
increase in the candidate entropy by the conditional entropy H(B0|B) = H(B0) - H(B) > 0.
Now, for NMI, the numerator increases by H(B0|B) while the denominator increases by at most
H(B0|B) (dependent on H(A) and the specific normalization that is used). Therefore, NMI increases.
Similarly, VI decreases by H(B0|B). Concluding, both NMI and VI are monotonous w.r.t. perfect
splits. Now let B00 be obtained by a perfect merge of B1, B2 into B100. This results in a difference of
the entropy of the candidate H(B00) - H(B) = -H(B|B00) < 0. The joint entropy decreases by the
same amount, so that the mutual information remains unchanged. Therefore, the numerator of NMI
remains unchanged while the denominator may or may not change, depending on the normalization.
For min- or max-normalization, it may remain unchanged while for any other average it increases.
Hence, NMI does not satisfy monotonicity w.r.t. perfect merges for min- and max-normalization but
does satisfy this for average-normalization. For VI, the distance will decrease by H(B|B00) so that it
indeed satisfies monotonicity w.r.t. perfect merges.
AMI. Let B0 be obtained by splitting a cluster B1 into B10 , B20 . This split increases the mutual
information by H (BIB) — H (A 0 B 0|A 0 B). Recall the definition of the meet A 0 B from C.4
and note that the joint entropy equals H (A 0 B). For a perfect split We have H (A 0 BlA 0 B) = 0.
The expected mutual information changes with
Eao〜C(S(A))[M(A0,B0) — M(A0,B)] = H(B0|B) - Ea，〜C(S(A))[H(A0 0 B0) - H(A 0 B)],
Where We choose to randomize A instead of B0 and B for simplicity. Note that for all A0,
H(A0 0 B) —H(A00B0) = H(A0 0 B0|A0 0 B) ≥ 0,
With equality if and only if the split is a perfect split W.r.t. A0. Unless A consists exclusively of
singleton clusters, there is a positive probability that this split is not perfect, so that the expected value
is positive. Furthermore, for the normalization term, we have，H(A)H(B0) <，H(A)H(B) +
H(B0|B). Combining this, We get
AMI(A, B0)
M(A, B)- eA0〜C(S(A)) [M(A0, B)] + eA0〜C(S(A))[H(AO 0 BOlAO 0 B)]
PHAHBf-HBB-EAZC(S(A)MAξB)Γ+EAZC(S(A)HA0B7A0B)J
>	M(A, B)- eA0〜C(S(A)) [M(A0, B)] + eA0〜C(S(A))[H(AO 0 BIAO 0 B)]
> PH(A)H(B) — Ea，〜C(S(A))[M(A0, B)] + Ea，〜C(S(A))[H(A0 0 B0∣A0 0 B)]
>	M(A, B)- Eao〜C(S(A)) [M(Ao, B)]
> PH(A)H(B)- Eao〜C(S(A))[M(Ao,B)]
AMI(A,B).
This proves that AMI satisfies monotonicity w.r.t. perfect splits.
Now let BOO be obtained by a perfect merge of B1, B2 into B1OO. Again, we have H(BOO) - H(B) =
-H (B|B00 < 0) and M (A, B00) = M (A, B). Let Ao 〜C (S (A)) (again, randomizing A instead of
B and BOO for simplicity), then H(AO 0 BOO) ≥ H(AO 0 B) - H(BlBOO) with equality if and only if
BOO is a perfect merge w.r.t. AO which happens with probability strictly less than 1 (unless A consists
of a single cluster). Therefore, as long as kA > 1, the expected mutual information decreases. For
the normalization, we have PH(A)H(B00) < PH(A)H(B). Hence,
AMI(A, BOO)
M(A, BOO)- EAO〜C(S(A))[M(A0, BOO)]
PH(A)H(B00) - Eao〜C(S(A))[M(A0, B00)]
M(A, B) - EAO〜C(S(A))[M(A0, B00)]
PH(A)H(B00) - Eao〜C(S(A))[M(A0, B00)]
M(A,B)- Eao〜C(S(A))[M(A0,B)]
PH(A)H(B00) - Eao〜C(S(A))[M(A0, B)]
M(A,B)- Eao〜C(S(A))[M(Ao,B)]
PH(A)H(B)- Eao〜C(S(A)) [M(Ao,B)]
AMI(A, B).
19
Under review as a conference paper at ICLR 2021
BCubed. Note that a perfect merge increases BCubed recall while leaving BCubed precision
unchanged and that a perfect split increases precision while leaving recall unchanged. Hence, the
harmonic mean increases.
C.4.2 Negative cases
FMeasure. We give a numerical counter-example: consider A = {{0, . . . , 6}}, B =
{{0, 1, 2, 3}, {4, 5}, {6}} and merge the last two clusters to obtain B0 = {{0, 1, 2, 3}, {4, 5, 6}}.
Then, the FMeasure remains unchanged and equal to 0.73, violating monotonicity w.r.t. perfect
merges.
FNMI We will give the following numerical counter-example: Consider A =
{{0, 1}, {2}, {3}}, B =	{{0}, {1}, {2, 3}} and merge the first two clusters to obtain
B0 = {{0, 1}, {2, 3}}. This results in
FNMI(A, B) ≈ 0.67 > 0.57 ≈ FNMI(A,B0).
This non-monotonicity is caused by the penalty factor that equals 1 for the pair A, B and equals
exp(-1/3) ≈ 0.72 for A, B0.
SMI. For this numerical counter-example we rely on the Matlab-implementation of the index by its
original authors (Romano et al., 2014). Let A = {{0, . . . , 4}, {5}}, B = {{0, 1}, {2, 3}, {4}, {5}}
and consider merging the two clusters resulting in B0 = {{0, 1, 2, 3}, {4}, {5}}. The index remains
unchanged and equals 2 before and after the merge.
Wallace. Let kA = 1 and let kB > 1. Then any merge of B is a perfect merge, but no increase
occurs since W1 (A, B) = 1.
C.5 Constant baseline
C.5.1 Positive cases
AMI and SMI. Both of these indices satisfy the constant baseline by construction since the
expected mutual information is subtracted from the actual mutual information in the numerator.
Adjusted Rand, Correlation Coefficient and Sokal&Sneath. These indices all satisfy ACB
while being PAB-linear for fixed pA , pB . Therefore, the expected value equals the asymptotic
constant.
C.5.2 Negative cases
For all the following indices, we will analyse the following counter-example. Let |V | = n, kA =
kB = n - 1. For each index, we will compute the expected value and show that it is not constant. All
of these indices satisfy the maximal agreement property and maximal agreement is achieved with
probability 1/N (the probability that the single intra-pair of A coincides with the single intra-pair of
B). Furthermore, each case where the intra-pairs do not coincide will result in the same contingency
variables and hence the same value of the index. We will refer to this value as cn (I). Therefore, the
expected value will only have to be taken over two values and will be given by
1	N-1
E[I(A, B)] = Ncmax +-----N-Cn(I).
For each of these indices we will conclude that this is a non-constant function of n so that the index
does not satisfy the constant baseline property.
Jaccard and Dice. For both these indices we have cmax = 1 and cn (I) = 0 (as N11 = 0 whenever
the intra-pairs do not coincide). Hence, E[I(A, B)] = N, which is not constant.
Rand and Wallace. As both functions are linear in N11 for fixed mA = N11 + N10, mB =
N11+ N01, we can compute the expected value by simply substituting N11 = mAmB/N. This will
result in expected values 1 - 2/N + 2/N2 and 1/N for Rand and Wallace respectively, which are
both non-constant.
20
Under review as a conference paper at ICLR 2021
Correlation distance. Here cmax = 0 and
Cn(CD)= 1arccθs QN-$)，
so that the expected value will be given by
E[CD(A, B)] = N-I arccos (--:rɪ-
Nπ	N -1
This is non-constant (it evaluates to 0.44, 0.47 for n = 3, 4 respectively). Note that this expected
value converges to 1 for n → ∞, which is indeed the asymptotic baseline of the index.
FNMI and NMI. Note that in this case kA = kB so that the penalty term of FNMI will equal 1 and
FNMI will coincide with NMI. Again cmax = 1. For the case where the intra-pairs do not coincide,
the joint entropy will equal H(A, B) = ln(n) while each of the marginal entropies will equal
H (A) = H (B) = n-2 ln(n) + 2 ln(n∕2) = ln(n) - 2 ln(2).
nn	n
This results in
小…	2H(A)- H(A,B)	1	2ln(n)
Cn(NMI) = -HA- = 1 - nln(n)- 2ln(2)，
and the expected value will be given by the non-constant
E[NMI(A, B)] = 1 -
N — 1	2ln(n)
N n ln(n) — 2 ln(2)
Note that as H(A) = H(B), all normalizations of MI will be equal so that this counter-example
proves that none of the variants of (F)NMI satisfy the constant baseline property.
Variation of Information. In this case Cmax = 0. We will use the entropies from the NMI-
computations to conclude that
N-1	N-14
E[VI(A, B)] = -Nr(2H(A, B) - H(A)- H(B)) = -- n ln(2),
which is again non-constant.
F-measure. Here Cmax = 1. In the case where the intra-pairs do not coincide, all contingency
variables will be either one or zero so that both recall and precision will equal 1 - 1∕n so that
Cn(FM) = 1 - 1∕n. This results in the following non-constant expected value
N-11
E[FM(A,B)] = 1 - -N-n
Note that because recall equals precision in both cases, this counter-example also works for other
averages than the harmonic average.
BCubed. Again Cmax = 1. In the other case, the recall and precision will again be equal. Because
for BCubed, the contribution of cluster i is given by n max{nj}∕∣A∕, the contributions of the one-
and two-clusters will be given by 1, ɪ respectively. Hence, Cn(BC) = n-2 + ^n = 1 - 2n and
we get the non-constant
E[BC(A，B)] = 1- NNɪ ∙ 2n.
We note that again, this counter-example can be extended to non-harmonic averages of the BCubed
recall and precision.
21
Under review as a conference paper at ICLR 2021
D	Futher analysis of constant baseline property
D. 1 Analysis of exact constant baseline property
Let us show that the definition of the constant baseline applies not only to uniform (within a given
sizes specification) distribution but also to all symmetric distributions over clusterings.
Definition 11. We say that a distribution over clusterings B is element-symmetric if for every two
clusterings B and B0 that have the same cluster-sizes, B returns B and B0 with equal probabilities.
Lemma 3. Let I be an index with a constant baseline as defined in Definition 7, let A be a clustering
with 1 < kA < n and let B be an element-symmetric distribution. Then EB〜B[I(A, B)] = Cbase.
Proof. We write
EB〜b[I(A,B)] = X PB〜B(S(B) = S) EB〜B[I(A,B)∣S(B) = s]
s
=X PB〜B(S(B) = s) EB〜C(S) [I(A, B)]
s
=):PB〜B(S(B) = S) Cbase = cbase,
s
where the sum ranges over cluster-sizes of n elements.	□
D.2 Analysis of asymptotic constant baseline property
Definition 12. An index I is said to be scale-invariant, if it can be expressed as a continuous function
of the three variables pA := mA/N, pB := mB/N and pAB := N11/N.
All indices in Table 3 are scale-invariant. For such indices, we will write I(p) (pAB, pA, pB ). Note
that when B 〜C(S) for some s, the values pa,pb are constants while PAB is a random variable.
Therefore, we further write PAB to stress that this is a random variable.
Theorem 2. Let I be a scale-invariant pair-counting index, and consider a sequence of clusterings
A(n) and cluster-size specifications S(n). Let N1(1n), N1(0n), N0(1n), N0(0n) be the corresponding pair-
counts. Then, for any ε > 0, as n → ∞,
p (II(N(n),N(n),N0n),N0n)) -i (NIny,Nny,Nfy,Nny)∣ >ε) → 0.
Proof. We prove the equivalent statement
I(p) (PA(nB), p(An), p(Bn) - I(p) (p(An)p(Bn),p(An),p(Bn) →P 0 .
We first prove that PA(nB) - p(An)p(Bn) →P 0 so that the above follows from the continuous mapping
theorem. Chebychev’s inequality gives
P	IIPA(nB) - p(An)p(Bn) II > ε
≤ (⅛ Var (Mn)) →0
The last step follows from the fact that Var(N11) = o(n4), as we will prove in the remainder of this
section. Even though in the definition, A is fixed while B is randomly permuted, it is convenient to
equivalently consider both clusterings are randomly permuted for this proof.
We will show that Var(N11) = o(n4). To compute the variance, we first inspect the second moment.
Let A(S) denote the indicator function of the event that all elements of S ⊂ V are in the same cluster
in A. Define B(S) similarly and let AB(S) = A(S)B(S). Let e, e1, e2 range over subsets of V of
22
Under review as a conference paper at ICLR 2021
size 2. We write
AB(e1)AB(e2)
e1,e2
AB(e1)AB(e2) +	AB(e1)AB(e2) +	AB(e1)AB(e2)
∣eι∩e2∣=2	∣e1∩e2∣ = l	∣e1∩e2∣=0
=N11 +	AB(e1 ∪ e2) +	AB(e1)AB(e2).
∣e1∩e2∣ = l	eι ∩e2=0
We take the expectation
E[N121] =E[N11]+6 n3 E[AB({v1, v2, v3})] +
E[AB(e1)AB(e2)],
where v1,v2,v3 ∈ V distinct and eι ∩ e2 = 0. The first two terms are obviously o(n4). We inspect
the last term
n2n-2 2E[AB(e1)AB(e2)]
=(n) X P(eι ⊂ Ai ∩ Bj )× (n - 2) E[AB (e2)∣e1 ⊂ Ai ∩ Bj ].
i,j
Now we rewrite E[N11]2 to
(3)
E[N11]2 =	n2 XP(e1⊂Ai∩Bj) n2 E[AB(e2)].
Note that (n)E[AB(e2)] > (n-2)E[AB(e2)] so that the difference between (3) and E[N11]2 can be
bounded by
(n) (n - 2) XP(eι ⊂ Ai ∩ Bj) ∙ (E[AB(e2)∣eι ⊂ Ai ∩ Bj] - E[AB(e2)]).
i,j
As n2 n-2 2 = O(n4), what remains to be proven is
EP(eι ⊂ Ai ∩ Bj) ∙ (E[AB(e2)∣e1 ⊂ Ai ∩ Bj] - E[AB(e2)]) = o(1).
i,j
Note that it is sufficient to prove that
E[AB(e2)|e1 ⊂ Ai ∩ Bj] - E[AB(e2)] = o(1),
for all i, j. Note that E[AB(e2)] = mAmB /N2, while
E[AB(e2)∣e1 ⊂ Ai ∩ Bj] = (mA -(2ai- 3))(mB -(bj- 3)).
[	( 2)| 1	j]	(N - (2n - 3))2
Hence, the difference will be given by
(mA - (2ai - 3))(mB - (2bj - 3))	mAmB
(N - (2n - 3))2	N2
N2 (mA - (2ai - 3))(mB - (2bj - 3))	(N - (2n - 3))2mAmB
N2(N - (2n - 3))2	N2(N - (2n - 3))2
N2((20i - 3)(2bj — 3) — mA(2bj — 3) — mB (20i — 3)) ∣ mamB (2N(2n — 3) — (2n — 3)2)
N2(N - (2n - 3))2	+	N2(N - (2n - 3))2
((20i —	3)(2bj	— 3) — mA(2bj — 3)	— mB (20i	— 3)) ∣ mAmB	(2N(2n — 3) — (2n —	3)2)
(N - (2n - 3))2	+ N2	(N - (2n - 3))2
O(n3)
(N - (2n - 3))2
+
mAmB	O(n3)
N2	N2(N - (2n - 3))2
o(1),
23
Under review as a conference paper at ICLR 2021
as required.
□
D.3 Statistical tests for constant baseline
In this section, we provide two statistical tests: one test to check whether an index I satisfies the
constant baseline property and another to check whether I has a selection bias towards certain cluster
sizes.
Checking constant baseline. Given a reference clustering A and a number of cluster sizes specifi-
cations s1 , . . . , sk , we test the null hypothesis that
EB 〜c(Si)[I (A,B)]
is constant in i = 1, . . . , k. We do so by using one-way Analysis Of Variance (ANOVA). For each
cluster sizes specification, we generate r clusterings. Although ANOVA assumes the data to be
normally distributed, it is known to be robust for sufficiently large groups (i.e., large r).
Checking selection bias. In (Romano et al., 2014) it is observed that some indices with a constant
baseline do have a selection bias; when we have a pool of random clusterings of various sizes and
select the one that has the highest score w.r.t. a reference clustering, there is a bias of selecting certain
cluster sizes. We test this bias in the following way: given a reference clustering A and cluster sizes
specifications si,..., Sk, We repeatedly generate Bi 〜C (sι),...,Bk 〜C (Sk). The null-hypothesis
will be that each of these clusterings Bi has an equal chance of maximizing I(A, Bi). We test this
hypothesis by generating r pools and using the Chi-squared test.
We emphasize that these statistical tests cannot prove Whether an index satisfies the property or
has a bias. Both Will return a confidence level p With Which the null hypothesis can be rejected.
Furthermore, for an index to not have these biases, the null hypothesis should be true for all choices
of A, Si, . . . , Sk, Which is impossible to verify statistically.
The statistical tests have been implemented in Python and the code supplements the submission. We
applied the tests to the indices of Tables 2 and 3. We chose n = 50, 100, 150, . . . , 1000 and r = 500.
For the cluster sizes, We define the balanced cluster sizes BS(n, k) to be the cluster-size specification
for k clusters of which n - k * [n/k[ clusters have sizedn/ke while the remainder have size [n/k].
Then We choose A(n) to be a clustering With sizes BS(n, bn0.5c) and consider candidates With sizes
S(in) = BS(n, bn0.25c), S(2n) = BS(n, bn0.5c), S(3n) = BS(n, bn0.75 c). For each n, the statistical
test returns a p-value. We use Fisher’s method to combine these p-values into one single p-value and
then reject the constant baseline ifp < 0.05. The obtained results agree with Tables 2 and 3 except
for Correlation Distance, which is so close to having a constant baseline that the tests are unable to
detect it.
D.4 Illustrating significance of constant baseline
In this section, we conduct two experiments illustrating the biases of various indices. We perform two
experiments that allow us to identify the direction of the bias in different situations. Our reference
clustering corresponds to the expert-annotated clustering of the production experiment described in
Section 3 and Appendix F.3, where n = 924 items are grouped into kA = 431 clusters (305 of them
consist of a single element).
In the first experiment, we randomly cluster the items into k approximately equally sized clusters for
various k . Figure 3 shows the averages and 90% confidence bands for each index. It can be seen that
some indices (e.g., NMI and Rand) have a clear increasing baseline while others (e.g., Jaccard and
VI) have a decreasing baseline. In contrast, all unbiased indices have a constant baseline.
In Section 4.6 we argued that these biases could not be described in terms of the number of clusters
alone. Our second experiment illustrates that the bias also heavily depends on the sizes of the clusters.
In this case, items are randomly clustered into 32 clusters, 31 of which are “small” clusters of size S
while one cluster has size n - 31 ∙ s, where S is varied between 1 and 28. We see that the biases are
clearly visible. This shows that, even when fixing the number of clusters, biased indices may heavily
distort an experiment’s outcome.
24
Under review as a conference paper at ICLR 2021
Finally, recall that we have proven that the baseline of CD is only asymptotically constant. Figures 3
and 4 show that for practical purposes its baseline can be considered constant.
E Additional results
E.1 Proof of Theorem 1
Let B0 be an A-consistent improvement of B . We define
B % B0 = {Bj ∩ Bjo |Bj ∈ B, Bj0 ∈ B0, Bj ∩ Bj0 = 0}
and show that B 0 B0 can be obtained from B by a sequence of perfect splits, while B0 can be
obtained from B 0 B0 by a sequence of perfect merges. Indeed, the assumption that B0 does not
introduce new disagreeing pairs guarantees that any Bj ∈ B can be split into Bj ∩B10 , . . . , Bj ∩Bk0 0
without splitting over any intra-cluster pairs of A. Let us prove that B0 can be obtained from B 0 B0
by perfect merges. Suppose there are two B100 , B200 ∈ B 0 B0 such that both are subsets of some
Bj00. Assume that this merge is not perfect, then there must be v ∈ B100, w ∈ B200 such that v, w are
in different clusters of A. As v, w are in the same cluster of B0, it follows from the definition of
B 0 B0 that v, w must be in different clusters of B . Hence, v, w is an inter-cluster pair in both A
and B, while it is an intra-cluster pair of B0, contradicting the assumption that B0 is an A-consistent
improvement of B . This concludes the proof.
E.2 Correlation Distance is a distance
Theorem. The Correlation Distance is indeed a distance.
Proof. A proof of this is given in Van Dongen & Enright (2012). We give an alternative proof that
allows for a geometric interpretation. First we map each partition A to an N -dimensional vector on
the unit sphere by
[√N1	if kA = 1,
~(A) :=	kA⅛	if 1 <kA <n,
〔-√N1	if kA = n,
where 1 is the N -dimensional all-one vector and A~is the binary vector representation of a partition
introduced in Section 2. Straightforward computation gives ∣∣√~ 一 pa1∣∣ = NPAA(a- — Pa), and
standard inner product
h√~ ― Pa1,B ― PB 1)= N(PAB - PApB ),
so that indeed
h√~ - pa1,b - Pb 1)
M - PA1IIIIb — Pb 1II
CC(p)(PAB,PA,PB).
It is a well-known fact that the inner product of two vectors of unit length corresponds to the cosine
of their angle. Hence, taking the arccosine gives us the angle. The angle between unit vectors
corresponds to the distance along the unit sphere. As ~u is an injection from the set of partitions
to points on the unit sphere, we may conclude that this index is indeed a distance on the set of
partitions.	□
E.3 Deviation of CD from constant baseline
Theorem. Given ground truth A with a number of clusters 1 < kA < n, a cluster-size specification
S and a random partition B 〜 C(S), the expected difference between Correlation Distance and its
baseline is given by
1	1∞
EB〜C(S)[CD(A, B)] — $ = — ∏ E
(2k)! EB 〜c(s)[CC(A,B)2k+1]
22k(k!)2
2k + 1
25
Under review as a conference paper at ICLR 2021
0.0100
0.0075
0.0050
0.0025
0.0000
-0.0025
-0.0050
-0.0075
-0.0100
——AR
0.002 -
0.001 -
0.000 -	------------------------------
-0.001 -
-0.002 -
0.503
0.502
0.501
0.500
0.499
0.498
0	100	200	300	400	500
NumberofciustBrs
Figure 3: The reference clustering of Appendix F.3 (n = 924 and kA = 431) is compared to random
clusterings. Each clustering consists of k approximately equally-sized clusters, where k is varied
between 2 and 512. For each k, 200 random clusterings are generated. For each index, we plot the
average score, along with a 90% confidence band.
26
Under review as a conference paper at ICLR 2021
Small cluster SIZe
Small cluster sbe
Small cluster size
Figure 4: The reference clustering of Appendix F.3 (n = 924 and kA = 431) is compared to random
clusterings. Each clustering consists of 31 “small” clusters of size s while the last cluster has size
924 - 31 ∙ s, where S is varied between 1 and 28. For each s, 200 random clusterings are generated.
For each index, we plot the average score, along with a 90% confidence band.
27
Under review as a conference paper at ICLR 2021
Proof. We take the Taylor expansion of the arccosine around CC(A, B) = 0 and get
CD(A b)_1_1 X (2k)! CC(A,B)2k+1
CD(A, B) = 2 ∏ M 22k(k!)2	2k + 1	.
We take the expectation of both sides and note that the first moment of CC equals zero, so the starting
index is k = 1.	□
For B 〜C(S) and large n, the value CC(A, B) will be concentrated around 0. This explains that in
practice, the mean tends to be very close to the asymptotic baseline.
E.4 Comparison with Lei et al. (201 7)
Lei et al. (2017) describe the following biases for cluster similarity indices: NCinc — the average
value for a random guess increases monotonically with the Number of Clusters (NC) of the candidate;
NCdec — the average value for a random guess decreases monotonically with the number of clusters,
and GTbias — the direction of the monotonicity depends on the specific Ground Truth (GT), i.e., on
the reference partition. In particular, the authors conclude from numerical experiments that Jaccard
suffers from NCdec and analytically prove that Rand suffers from GTbias, where the direction of the
bias depends on the quadratic entropy of the ground truth clustering. Here we argue that these biases
are not well defined, suggest replacing them by well-defined analogs, and show how our analysis
allows to easily test indices on these biases.
We argue that the quantity of interest should not be the number of clusters, but the number of intra-
cluster pairs of the candidate. Theorem 2 shows that the asymptotic value of the index depends on
the number of intra-cluster pairs of both clusterings. The key insight is that more clusters do not
necessarily imply fewer intra-cluster pairs. For example, let s denote a cluster-sizes specification
for 3 clusters each of size ` > 2. Now let s0 be the cluster-sizes specification for one cluster of size
2' and ' clusters of size 1. Then, any B 〜C(s) will have 3 clusters and 3(') intra-cluster pairs
while any B0 〜C(s0) will have ' + 1 > 3 clusters and > 3(') intra-cluster pairs. For any
ground truth A with cluster-sizes s, we have E[J (A, B)] < E[J (A, B0)] because of a larger amount
of intra-cluster pairs In contrast, Lei et al. (2017) classifies Jaccard as an NCdec index, so that the
expected value should increase, contradicting the definition of NCdec. The NPinc and NPdec biases
that are defined in Definition 9 are sound versions of these NCinc and NCdec biases because they
depend on the expected number of agreeing pairs. This allows to analytically determine which bias a
given pair-counting index has.
F Experiment
F.1 Synthetic experiment
In this experiment, we construct several simple examples to illustrate the inconsistency among the
indices. Recall that two indices I1 and I2 are inconsistent for a triplet of partitions (A, B1, B2) if
I1(A,B1) >I1(A,B2)butI2(A,B1) <I2(A,B2).
We take all indices from Tables 2 and 3 and construct several triplets of partitions to distinguish them
all. Let us note that the pairs Dice vs Jaccard and CC vs CD cannot be inconsistent since they are
monotonically transformable to each other. Also, we do not compare with SMI since it is much more
computationally complex than all other indices. Thus, we end up with 13 indices and are looking for
simple inconsistency examples.
The theoretical minimum of examples needed to find inconsistency for all pairs of 13 indices is 4. We
were able to find such four examples, see Figure 5. In this figure, we show four inconsistency triplets.
For each triplet, the shapes (triangle, square, etc.) denote the reference partition A. Left and right
figures show candidate partitions B1 and B2 . In the caption, we specify which similarity indices
favor this candidate partition over the other one.
It is easy to see that for each pair of indices, there is a simple example where they disagree. For
example, NMI and NMImax are inconsistent for triplets 3. Also, we know that Jaccard in general
28
Under review as a conference paper at ICLR 2021
(1a) FNMI, Rand, AdjRand, Jac-
card, Dice, Wallace, FMeasure,
BCubed
(1b) NMI, NMImax, VI, AMI,
S&S, CC, CD
(2a) NMI, NMImax, FNMI, Rand,
FMeasure, BCubed
(2b) VI, AMI, AdjRand, Jaccard,
Dice, Wallace, S&S, CC, CD
(3a) NMImax, Rand, AdjRand,
Jaccard, Dice, S&S, CC, CD,
FMeasure
(3b) NMI, VI, FNMI, AMI, Wal-
lace, BCubed
(4a) NMI, NMImax, FNMI, AMI,
Rand, AdjRand, CC, CD
(4b) VI, Jaccard, Dice, Wallace,
S&S, FMeasure, BCubed
Figure 5: Inconsistency of indices: each row corresponds to a triplet of partitions, shapes denote the
reference partitions, the captions indicate which indices favor the corresponding candidate.
29
Under review as a conference paper at ICLR 2021
Table 6: Inconsistency of indices on real-world clustering datasets, %
	NMI	NMImax	VI	FNMI	AMI	R	AR	J	W	S&S	CC	FMeas	BCub
	 NMI	-	5.4	40.3	17.3	9.2	13.4	15.7	35.2	68.4	20.1	18.5	31.7	32.0
NMImax		-	41.1	16.5	13.2	12.5	14.1	34.3	68.8	21.1	18.9	30.3	32.4
VI			-	34.7	41.8	45.2	37.6	17.1	28.8	36.0	37.2	18.1	13.6
FNMI				-	23.3	24.0	19.0	29.9	57.0	26.7	23.8	27.5	26.7
AMI					-	21.1	17.3	33.3	61.3	15.1	13.6	35.0	34.4
R					-	15.5	35.6	71.5	21.1	20.7	32.5	35.8	
AR							-	23.5	59.4	11.7	8.3	25.3	28.1
J								-	35.9	23.1	23.8	10.7	9.7
W									-	53.5	54.8	40.7	37.4
S&S										-	3.6	26.2	27.8
CC											-	27.0	28.8
FMeas												-	7.7
BCub													-
favors larger clusters, while Rand and NMI often prefer smaller ones. Hence, they often disagree in
this way (see the triplets 2 and 4).
F.2 Experiments on real datasets
In this section, we test whether the inconsistency affects conclusions obtained in experiments on real
data.
For that, we used 16 real-world datasets (GitHub, 2020). We took all real-world datasets available
there and removed the ones with categorial features or without the explicit target class field defined.
The values of the “target class” field were used as a reference partition. We end up with the following
real-world datasets: arrhythmia, balance-scale, ecoli, heart-statlog, letter, segment, vehicle, wdbc,
wine, wisc, cpu, iono, iris, sonar, thy, zoo.
On these datasets, we ran 8 well-known clustering algorithms (Scikit-learn, 2020): KMeans, Affini-
tyPropagation, MeanShift, AgglomerativeClustering, DBSCAN, OPTICS, Birch, GaussianMixture.
For AgglomerativeClustering, we used 4 different linkage types (‘ward’, ‘average’, ‘complete’,
‘single’). For GaussianMixture, we used 4 different covariance types (‘spherical’, ‘diag’, ‘tied’, ‘full’).
For methods requiring the number of clusters as a parameter (KMeans, Birch, AgglomerativeClus-
tering, GaussianMixture), we took up to 4 different values (less than 4 if some of them are equal):
2, ref-clusters, max(2,ref-clusters∕2), min(items, 2∙ref-clusters), where ref-clusters is the number of
clusters in the reference partition and items is the number of elements in the dataset. For MeanShift,
we used the option CluSter_all = True. All other settings were default or taken from examples in
the sklearn manual.
For all datasets, we calculated all the partitions for all methods described above. We removed all
partitions having only one cluster or which raised any calculation error. Then, we considered all
possible triplets A, B1, B2, where A is a reference partition and B1 and B2 are candidates obtained
with two different algorithms. We have 8688 such triplets in total. For each triplet, we check whether
the indices are consistent. The inconsistency frequency is shown in Table 6. Note that Wallace is
highly asymmetrical and does not satisfy most of the properties, so it is not surprising that it is in
general very inconsistent with others. However, the inconsistency rates are significant even for widely
used pairs of indices such as, e.g., Variation of Information vs NMI (40.3%, which is an extremely
high disagreement). Interestingly, the best agreeing indices are S&S and CC which satisfy most of
our properties. This means that conclusions made with these indices are likely to be similar.
Actually, one can show that all indices are inconsistent using only one dataset. This holds for 11 out
of 16 datasets: heart-statlog, iris, segment, thy, arrhythmia, vehicle, zoo, ecoli, balance-scale, letter,
wine. We do not present statistics for individual datasets since we found the aggregated Table 6 to be
more useful.
Finally, to illustrate the biases of indices, we compare two KMeans algorithms with k = 2 and k =
2∙ref-clusters. The comparison is performed on 10 datasets (where both algorithms are successfully
completed). The results are shown in Table 7. In this table, biases and inconsistency are clearly
5 The code supplements the submission.
30
Under review as a conference paper at ICLR 2021
Table 7: Algorithms preferred by different indices
I		NMI	NMImax	VI	FNMI	AMI	R	AR	J	W	S&S1	CC	FMeas	BCub
k	=2 I	2	1	9	4	2	0	4	6	10	3	3	7	7
k	=2 ∙ ref	8	9	1	6	8	10	6	4	0	7	7	3	3
Table 8: Similarity of candidate partitions to the reference one. In bold are the inconsistently ranked
pairs of partitions. Some indices are taken with “-” sign, so larger values correspond to better
agreement.
	A prod	A1	A2
NMI	0.9326	0.9479	0.9482
NMImax	0.8928	0.9457	0.9298
FNMI	0.7551	0.9304	0.8722
AMI	0.6710	0.7815	0.7533
VI	-0.6996	-0.5662	-0.5503
FMeasure	0.8675	0.8782	0.8852
BCubed	0.8302	0.8431	0.8543
R	0.9827	0.9915	0.9901
AR	0.4911	0.5999	0.6213
J	0.3320	0.4329	0.4556
W	0.8323	0.6287	0.8010
D	0.4985	0.6042	0.6260
S&S	0.7926	0.8004	0.8262
CC	0.5376	0.6004	0.6371
CD	-0.3193	-0.2950	-0.2802
seen. We see that NMI and NMImax almost always prefer the larger number of clusters. In contrast,
Variation of Information and Rand usually prefer k = 2 (Rand prefers k = 2 in all cases).
F.3 Production experiment
To show that the choice of similarity index may have an effect on the final quality of a production
algorithm, we conducted an experiment within a major news aggregator system. The system aggre-
gates all news articles to events and shows the list of most important events to users. For grouping, a
clustering algorithm is used and the quality of this algorithm affects the user experience: merging
different clusters may lead to not showing an important event, while too much splitting may cause
the presence of duplicate events.
There is an algorithm Aprod currently used in production and two alternative algorithms A1 and A2 .
To decide which alternative is better for the system, we need to compare them. For that, it is possible
to either perform an online experiment or make an offline comparison, which is much cheaper and
allows us to compare more alternatives. For the offline comparison, we manually grouped 1K news
articles about volleyball, collected during a period of three days, into events. Then, we compared
the obtained reference partition with partitions Aprod, A1, and A2 obtained by Aprod, A1, and A2,
respectively (see Table 8). According to most of the indices, A2 is closer to the reference partition
than A1 , and A1 is closer than Aprod. However, according to some indices, including the well-known
NMImax, NMI, and Rand, A1 better corresponds to the reference partition than A2 . As a result, we
see that in practical application different similarity indices may differently rank the algorithms.
To further see which algorithm better agrees with user preferences, we launched the following online
experiment. During one week we compared Aprod and A1 and during another — Aprod and A2 (it is
not technically possible to compare A1 and A2 simultaneously). In the first experiment, A1 gave
+0.75% clicks on events shown to users; in the second, A2 gave +2.7%, which clearly confirms that
these algorithms have different effects on user experience and A2 is a better alternative than A1. Most
similarity indices having nice properties, including CC, CD, and S&S, are in agreement with user
preferences. In contrast, AMI ranks A1 higher than A2 . This can be explained by the fact that AMI
gives more weight to small clusters compared to pair-counting indices, which can be undesirable for
this particular application, as we discuss in Section 5.
31