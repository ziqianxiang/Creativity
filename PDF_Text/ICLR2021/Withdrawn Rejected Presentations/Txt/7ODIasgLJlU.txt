Under review as a conference paper at ICLR 2021
Deep Q-Learning with Low Switching Cost
Anonymous authors
Paper under double-blind review
Ab stract
We initiate the study on deep reinforcement learning problems that require low
switching cost, i.e., a small number of policy switches during training. Such a
requirement is ubiquitous in many applications, such as medical domains, recom-
mendation systems, education, robotics, dialogue agents, etc, where the deployed
policy that actually interacts with the environment cannot change frequently. Our
paper investigates different policy switching criteria based on deep Q-networks
and further proposes an adaptive approach based on the feature distance between
the deployed Q-network and the underlying learning Q-network. Through ex-
tensive experiments on a medical treatment environment and a collection of the
Atari games, we find our feature-switching criterion substantially decreases the
switching cost while maintains a similar sample efficiency to the case without the
low-switching-cost constraint. We also complement this empirical finding with a
theoretical justification from a representation learning perspective.
1	Introduction
Reinforcement learning (RL) is often used for modeling real-world sequential decision-making
problems such as medical domains, personalized recommendations, hardware placements, database
optimization, etc. For these applications, oftentimes itis desirable to restrict the agent from adjusting
its policy frequently. In medical domains, changing a policy requires a thorough approval process
by experts. For large-scale software and hardware systems, changing a policy requires to redeploy
the whole environment. Formally, we would like our RL algorithm admits a low switching cost. In
this setting, it is required that the deployed policy that interacts with the environment cannot change
many times.
In some real-world RL applications such as robotics, education, and dialogue system, changing
the deployed policy frequently may cause high costs and risks. Gu et al. (2017) trained robotic
manipulation by decoupling the training and experience collecting threads; Mandel et al. (2014)
applied RL to educational games by taking a data-driven methodology for comparing and validating
policies offline, and run the strongest policy online; Jaques et al. (2019) developed an off-policy
batch RL algorithms for dialog system, which can effectively learn in an offline fashion, without
using different policies to interact with the environment. All of these work avoid changing the
deployed policy frequently as they try to train the policy offline effectively or validate the policy to
determine whether to deploy it online.
For RL problem with a low switching cost constraint, the central question is how to design a criterion
to decide when to change the deployed policy. Ideally, we would like this criterion to have the
following four properties:
1.	Low switching cost: This is the purpose of this criterion. An algorithm equipped with this
policy switching criterion should have low switching cost.
2.	High Reward: Since the deployed policy determines the collected samples and the agent
uses fewer deployed policies, the collected data may not be informative enough to learn the
optimal policy with high reward. We need this criterion to deploy policies that can collect
informative samples.
3.	Sample Efficiency: Since the agent only uses a few deployed policies, there may be more
redundant samples, which will not be collected if the agent switches the policy frequently.
We would like algorithms equipped with a criterion with similar sample efficiency as the
case without the low switching cost constraint.
1
Under review as a conference paper at ICLR 2021
4.	Generality: We would like this criterion to be effective not only on a specific task but also
broadly effective on a wide range of tasks.
In this paper, we take a step toward this important problem. We focus on designing a principled
policy switching criterion for deep Q-networks (DQN) learning algorithms, which have been widely
used in applications. For example, Ahn & Park (2020) apply DQN to control balancing between
different HVAC systems, Ao et al. (2019) propose a thermal process control method based on DQN,
and Chen et al. (2018) try to apply it to online recommendation. Notably these applications all
require low switching cost.
Our paper conducts a systematic study on DQN with low switching cost. Our contributions are
summarized below.
Our Contributions
•	We conduct the first systematic empirical study on benchmark environments that require
modern reinforcement learning algorithms. We test two naive policy switching criteria:
1) switching the policy after a fixed number of steps and 2) switching the policy after
an increasing step with a fixed rate. We find that neither criterion is a generic solution
because sometimes they either cannot find the best policy or significantly decrease the
sample efficiency.
•	Inspired by representation learning theory, we propose a new feature-based switching cri-
terion that uses the feature distance between the deployed Q-network and the underlying
learning Q-network. Through extensive experiments, we find our proposed criterion is a
generic solution - it substantially decreases the switching cost while maintains a similar
performance to the case without the low-switching-cost constraint.
•	Along the way, we also derive a deterministic Rainbow DQN (Hessel et al., 2018), which
may be of independent interest.
Organization This paper is organized as follows. In Section 2, we review related work. In Sec-
tion 3, we describe our problem setup and review necessary backgrounds. In Section 4, we describe
deterministic Rainbow DQN with the low switching cost constraint. In Section 5, we introduce our
feature-based policy switching criterion and its theoretical support. In Section 6, we conduct ex-
periments to evaluate different criteria. We conclude in Section 7 and leave experiment details to
appendix.
2	Related Work
Low switching cost algorithms were first studied in the bandit setting (Auer et al., 2002; Cesa-
Bianchi et al., 2013). Existing work on RL with low switching cost is mostly theoretical. To our
knowledge, Bai et al. (2019) is the first work that studies this problem for the episodic finite-horizon
tabular RL setting. Bai et al. (2019) gave a low-regret algorithm with an O H3SA log (K) local
switching upper bound where S is the number of stats, A is the number of actions, H is the planning
horizon and K is the number of episodes the agent plays. The upper bound was improved in Zhang
et al. (2020b;a).
The only empirical study on RL with switching cost is Matsushima et al. (2020), which proposed a
concept of deployment efficiency and gave a model-based algorithm. During the training process,
the algorithm fixes the number of deployments, trains a dynamics model ensemble, and updates the
deployed policy alternately. After each deployment, the deployed policy collects transitions in the
real environment to enhance the models, and then the models optimize the policy by providing imag-
ined trajectories. In other words, they reduce the number of deployments by training on simulated
environments. Our goal is different: we design a criterion to decide when to change the deployed
policy, and this criterion could be employed by model-free algorithms.
There is a line of work on offline RL (also called Batch RL) methods, where the policy does not
interact with the environment directly and only learns from a fixed dataset (Lange et al., 2012;
Levine et al., 2020). Some methods Interpolate offline and online methods, i.e., semi-batch RL
2
Under review as a conference paper at ICLR 2021
algorithms (Singh et al., 1995; Lange et al., 2012), which update the policy many times on a large
batch of transitions. However, the switching cost is not their focus.
3	Preliminaries
3.1	Markov Decision Process
Throughout our paper, we consider the episodic Markov decision model (S, A, H, P, r). In this
model, S is the state space, A is the action space, H ∈ Z+ is the planning horizon. P is the
transition operator where P(x0|x, a) denotes the transition probability of taking action a from state
x to state x0 . r : S × A → R is the reward function. A policy is a mapping from a state to an action,
π : S → A. In this paper, we focus on deterministic policies as required by motivating applications.
The dynamics of the episodic MDP can be view as the interaction of an agent with the environment
periodically. We let K be the total number of episodes the agent plays. At the beginning of an
episode k ∈ [K], the agent chooses a policy πk. The initial x1k ∈ S is sampled from a distribution,
and the agent is then in step 1. At each step h ∈ [H] in this episode, based on the current state
xkh ∈ S the agent chooses the action akh = πk (xkh). The environment will give the reward for the
step r(χh, ah) and move the agent to the next state xh+i 〜 P(∙∣χh,ah). The episode automatically
ends when the agent reaches the step H + 1.
Q-function is used to evaluate the long-term value for the action a and subsequent decisions. The
Q-function of a policy π at time step h is defined as follows:
H
Qπh(x, a) := rh(x, a) + E	r (xi, π (xi)) xh = x, ah = a
i=h+1
(1)
The goal of this agent is to find a policy ∏ which maximizes the expected reward, ∏
argmax∏ En [pH=ι rh∣. Ideally, We want to use a few episodes (K) as possible to learn ∏*.
3.2	Switching Cost
The concept of switching cost is used to quantify the adaptability of RL algorithms. The switching
cost is defined as the number of policy changes of deployed policies in the running of the algorithm
in K episodes, namely:
K-1
Nswitch :=	I{πk 6= πk+1}	(2)
k=1
The goal of this paper is to equip algorithm with a criterion that learns ∏* using a few episodes while
at the same time has small Nswitch.
3.3	DEEP Q-LEARNING
If we can estimate Q-function for each state and action pair well, there is no difficulty in finding ∏*.
For example, we can always select a* = arg max。Q(x, a). However, it is not easy to learn Q value
estimates for each state and action pair, especially when the state or the action space is large.
In deep Q-learning (DQN), Mnih et al. (2015) combine deep networks and reinforcement learning
successfully by using a deep neural network to approximate Q(x, a). Given the current state xh, the
agent selects a action ah greedily based on the Q(xh, a), then the state move to xh+1 and a reward
rt+1 is obtained. The transition (xt, at, rt+1, xt+1) is saved to the replay memory buffer. At each
time, a batch of transitions is sampled from this buffer, and the parameters of neural networks are
optimized by using stochastic gradient descent to minimize the loss
(rh+i + Yh+i maxq0(xh+ι,a0) - qθ(xh,ah))2	(3)
a0
where γh+i is the chosen discount of time step h + 1, θ is the parameters of the online network,
and θ represents the parameters of the target network. The gradient of the loss is back-propagated
3
Under review as a conference paper at ICLR 2021
only to update θ, while θ is not optimized directly. DQN has been successful as it led to super-
human performance on several Atari games. Nevertheless, there are also several limitations of this
algorithm and many extensions have been proposed. Rainbow (Hessel et al., 2018) combines six
of these extensions and obtains an excellent and stable performance on many Atari games. In the
Appendix A, we review these tricks. In this paper, we focus on Rainbow DQN with low switching
cost.
3.4	Count-Based Exploration
In many real-world scenarios that require low switching cost, it is also required to use deterministic
policies, especially in applications mentioned above. Exploring strategies like -greedy and noisy
net make the policy stochastic, which we cannot use here. Count-based exploration algorithms are
known to perform near-optimally when used in reinforcement learning for solving tabular MDPs.
Tang et al. (2017) applied these algorithms into high-dimensional state spaces successfully. They
discretized the state space with a hash function φ : S → Z. Then an exploration bonus r+ (x) =
I β, is added to the reward function and the agent is trained with the modified reward r + r+.
n(φ(x))
Note that with the count-based exploration, the policy is deterministic.
4	Deterministic Rainb ow DQN with a Policy Switching Criterion
In this section, we first introduce how to implement a deterministic Rainbow DQN with a policy
switching criterion. This implementation combines six DQN tricks and the policy is always deter-
ministic.
4.1	Deterministic Rainbow DQN
We first discuss how to make Rainbow DQN always output deterministic policies. Recall that to
explore more efficiently, Rainbow adopts the Noisy Net (Fortunato et al., 2018), which makes the
policy stochastic. To obtain a deterministic policy and keep exploring, we remove Noisy Net and
employ the Count-Based exploration described in the previous section. When the deployed policy
interacts with the environment, it selects the action which maximizes the Q-function. After obtaining
the reward by taking this action, an exploration bonus r+
β
n(φ(x))
is added into the reward.
4.2	DQN with a Policy Switching Criterion
Besides, Rainbow updates the policies which interacts with the environment directly. These poli-
cies usually switch millions of times during the training process (since the parameters are updated
millions of times). In our implementation, we add an online policy in addition to the deployed pol-
icy that interacts with the environment. The deployed policy collects the data for the experience
replay buffer, while the online policy is frequently updated when training. The deployed policy is
replaced with the online policy when they meet the criterion we will discuss soon. The algorithm of
deterministic switching Rainbow is shown in Algorithm 1.
4.3	Policy Switching Criteria
Here we first introduce two straightforward policy switching criteria.
Fixed Interval Switching This is the simplest criterion, we switch the deployed policy with a
fixed interval. Under the FIXn criterion, We switch the deployed policy whenever the online policy
is updated n times where n is a per-specified number. We will specify n in our experiments.
Adaptive Interval Switching This criterion aims to switch the deployed policy frequently at the
first and reduce the switching speed gradually. Under the Adaptive∕2m criterion, We increase the
deployment interval from n to m. The interval between the i-th deployment and the (i + 1)-th
deployment is min((i + 1) × n, m). We will specify n and m in our experiments.
4
Under review as a conference paper at ICLR 2021
Algorithm 1 Deterministic Switching Rainbow
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
Initialize arameters θonline, θdeployed, θtarget for online policy, deployed policy and target policy,
initialize an empty replay buffer D
Denote the state encoder in online policy and deployed policy as fonline and fdeployed
Set the step to start training Hstart , the step to end training Hmax, and the interval to update the
target policy Htarget .
Set n(accumulated_updates) = 0, n(dePloyment) = 0
for h = 1 to Hmax do
Select ah = arg maxa Qdeployed(sh, a)
Execute action at and observe reward rh and state xh+1
Compute the hash codes through for xh, φ(xh) = sgn(Ag(xh))
. A is a fixed matrix with i.i.d. entries drawn from a standard Gaussian distribution N(0, 1)
and g is a flat function
Update the hash table counts, n(φ(xh)) = n(φ(xh)) + 1
Update the reward rh = rh +
β
√n(φ(rh))
Store (xh, ah, rh, xh+1) in D
if h > Hstart then
Sample a minibatch of transitions from D.
Update θonline by stochastic gradient descent on the sampled minibatch once.
if h % Htarget == 0 then
Update θtarget = θonline
Set n(accumulated_updates) = n(accumulated_updates) + 1
end if
if J(fdeployed, foniine, D, n(accumulated_updates), n(dePloyment)) = true then
Update θdeployed = θonline
Update n(accumulated_updates) = 0
Update n(dePloyment) = n(dePloyment) + 1
end if
25:	end if
26:	end for
Algorithm 2 Switching Criteria (J in Algorithm 1)
.Fixed interval switching FIXn
input n(accumulated_updates)
output bool(n(aCcumulated_updates) ≥ n)
.Adaptive interval switching Adaptive∕2m
input n(accumulated_updates), n(dePloyment)
output bool(n(accumulated_updates) ≥ min(n(deployment) + 1) X n, m)
.Feature-based switching F_a
input fdeployed , fonline , D
Sample minibatch B of transitions (xh) from D with probability ph
COmPUte the the similarity Sim(Xh) = ffoyoy(xhx)h∣×⅛χ⅛)h)∣∣
Compute the average similarity in sampled batch Sim(B) = Px∈BBiJ(X)
output bool(Sim(B) ≤ a)
These two criteria and our proposed new criterion are summarized in Algorithm 2. Unfortunately, as
will be shown in our experiments, these two criteria do not perform well. This requires us to design
new principled criterion.
5
Under review as a conference paper at ICLR 2021
5 Feature-based Switching Criterion
In the section, we describe our new policy switching criterion based on feature learning. We first de-
scribe this criterion, and then we provide some theoretical justification from a representation learning
point of view.
Feature-based Switching Criterion We adopt the view the DQN learns to extract informative
features of the states of environments. Our proposed criterion tries to switch the deployed policy
according to the extracted feature. When deciding whether to switch the deployed policy or not, we
first sample a batch of states B from the experience replay buffer, and then extract the feature of all
states by both the deployed deep Q-network and online deep Q-network.
For a state x, the extracted feature are denoted as fdeployed(x) and fonline(x), respectively. The
similarity score between fdeployed and fonline on state x is defined as
hfdeployed (x), fonline (x)i
Sim(X)= Ifdeployed(X)IlXfonline(X)II
We then compute the averaged similarity score on the batch of states B
sim(B)
Ex∈B Sim(X)
-IR-
With a hyper-parameter a ∈ [0, 1], the feature-based policy switching criterion is to change the
deployed policy whenever Sim(B) ≤ a.
Theoretical Justification Our criterion is inspired by representation learning. To illustrate the
idea, We consider the following setting. Suppose We want to learn f (∙), a representation function
that maps the input to a k-dimension vector. We assume we have input-output pairs (X, y) with y =
hw, f *(χ)i for some underlying representation function f *(∙) and a linear predictor W ∈ Rk. For
ease of presentation, let us assume we know w, and our goal is to learn the underlying representation
which together with w gives us 0 prediction error.
Suppose we have data sets Di and D2. We use Di to train an estimator of f *, denoted as f 1, and
Di ∪ D2 to train another estimator of f *, denoted as f 1+2. The training method is empirical risk
minimization, i.e.,
f 1 J m∈in iD1-I	X	(y -hw,f (X)))2 and f1+2
f∈	i (x,y)∈D1
1
J min  -----——
f ∈F IDi ∪ D2I
(y- hw, f (X)i)2
(x,y)∈D1 ∪D2
where F is some pre-specified representation function class.
The following theorem suggests if the similarity score between fi and fi+2 is small, then fi is also
far from the underlying representation f *.
Theorem 1. Suppose fi and fi+2 are trained via aforementioned scheme. There exist dataset Di,
D2, function class F andw such that if the similarity score between fi and fi+2 on Di+2 is smaller
than α, then the prediction error of fi on Di+2 is 1- α.
The proof is deferred to Appendix B where we give explicitly constructions.
Theorem 1 suggests that in certain scenarios, if the learned representation has not converged (the
similarity score is small), then it cannot be the optimal representation which in turn will hurt the
prediction accuracy. Therefore, if we find the similarity score is small, we should change the de-
ployed policy.
6	Experiments
In this section, we conduct experiments to evaluate different policy switching criteria on DQN. We
study several Atari game environments along and an environment for simulating sepsis treatment for
ICU patients. We evaluate the efficiency among different switching criteria in these environments.
Implementation details and hyper-parameters are listed in the Appendix A.
6
Under review as a conference paper at ICLR 2021
6.1	Environments
GYMIC GYMIC is an OpenAI gym environment for simulating sepsis treatment for ICU patients
to an infection, where sepsis is caused by the body’s response to an infection and could be life-
threatening. GYMIC built an environment to simulate the MIMIC sepsis cohort, where MIMIC is
an open patient EHR dataset from ICU patients. This environment generates a sparse reward, the
reward is set to +15 if the patient recovered and -15 if the patient died. This environment has 46
clinical features and a 5 × 5 action space. For the GYMIC, we display the learning curve of 1.5
million steps of the environment, after which the reward converge. We choose this environment
because it is simulating a real-world problem that requires low switching cost. For Atari games, all
the experiments were training for the environment stepping 3.5 million times.
Atari 2600 Atari 2600 games are widely employed to evaluate the performance of DQN based
agents. We also evaluate the efficiency among different switching criteria on several games, such as
Pong, Road Runner, Beam Rider, etc.
6.2	Results and Discussions
For all environments, We evaluate a feature-based criterion F_0.98, three fixed interval criteria cov-
ering a vast range FIX_102,FIX_103 and FIX_104, and an adaptive criterion increasing the
deploying interval from 100 to 10,000. Besides the sWitching criteria We discussed above, We use
“None” to indicate an experiment Without the loW-sWitching-cost constraint Where deployed policy
kept in sync with online policy all the time, notice that this experiment is equivalent to FIX_1.
0.0	0.2M 0,5M 0,8M 1,0M 1,2M 1,5M
Step
Figure 1: Results on GYMIC, “Step” means
the number of steps of the environment. We
show the learning curve of 1.5 million steps.
The figure above is the learning cure of reward,
while the figure below displays the switching
cost. “None” means no low-switching-cost con-
straint, and the deployed policy always keeps
sync with the online policy. Curves of reward
are smoothed with a moving average over 5
points.
GYMIC As shown in figure 1, none of the
switching criteria affects the performance, but
they can reduce the number of policy switches
drastically, which indicates that reducing the
switching cost in such a medical environment
could be feasible. In particular, we find FIX_104
and F_0.98 are the two best criteria to reduce
switching cost. Finally, this criterion keeps a
good performance with the minimal switching
cost.
Atari 2600 GYMIC may be too simple, and we
should compare the performances among differ-
ent criteria in some more difficult environments.
We evaluate the performance of different criteria
when playing the Atari games, which are image-
based environments. In particular, the state space
is much more complex. Figure 2 shows the re-
sults of 6 Atari games. In each subgraph, the up-
per curves are about the rewards of steps, while
the curves blow is about switching cost.
First we observe that overall trend, higher switch-
ing cost leads to better performance. In general
Rainbow DQN with no switching cost constrain
often gives the best performance. Also, FIX_102
enjoys better performance than FIX_103 and
FIX .104. In some games such as Qbert and
Riverraid, although FIX_104 and Adaptive lead to lower switching cost, they fail to learn how
to play the games well. Therefore, they are not desired generic policy switching criterion.
Secondly, we observe that changing the policy with an adaptive interval may be better than a fixed
interval. Focusing on the criteria FIX_104 and Adaptive-102to104, the adaptive criterion switches
the online policy fast at first and decrease its switching speed gradually, and in the end, the adaptive
criterion would have the same speed as the fixed one. Therefore, there is no significant difference
7
Under review as a conference paper at ICLR 2021
None	F_0.98
battle zone
FIX_102
F/X_103
beam_ridder
F/X_104
Adaptive_102tol04
pong
40000 -
20000 -
0 -
20000 -
qbert
riverraid
road runner
IOOOO -
0 -
10000 -
5000 -
p」BM①工
P」BM ①0≤
0.0	1.0M	2.OM 3.OM	0.0	1.0M	2.OM	3.OM	0.0	1.0M	2.OM	3.OM
Step	Step	Step
Figure 2: The results on the Atari games, we compare different switching criteria on six Atari
games. “Step” means the number of steps of the environment. We constrain this 3.5 million steps
for all environments. In each environment, we display the reward over the steps on the top and the
switching cost in a log scale at the bottom. “None” means no switching criterion under which the
deployed policy always keeps sync with the online policy. We evaluate a feature-based criterion,
three fixed interval criteria covering a vast range, and an adaptive criteria increasing the deploying
interval from 100 to 10,000. Curves of reward are smoothed with a moving average over 5 points.
between the total switching cost of these two criteria. However, we could observe that the adaptive
criterion’s performance is better than the fixed one when playing Beam Rider, Pong, Qbert, and they
obtain similar performances in the rest games.
Lastly, We find our proposed feature-based criterion (F_0.98) is the desired on that satisfy all four
properties we discussed in Section 1. It significantly reduces the switching cost compared to “None”,
and is smaller than FIX_102. While it incurs higher switching cost than FIX_103, FIX_104, and
Adaptive_102to104, on all environments feature-based criterion consistently perform as well as
“None” in the sense that 1) it finds the optimal policy eventually, 2) it has the same sample effi-
ciency as “None”. On the other hand, other criteria sometime have significantly worse performance
compared to “None”, so none of them is a generic solution.
7	Conclusion
In this paper, we focus on the concept of switching cost and take a step toward designing a generic
solution for reducing the switching cost while maintaining the performance. Inspired by representa-
tion learning theory, we proposed a new feature-based policy switching criterion for deep Q-learning
methods. Through experiment on one medical simulation environment and six Atari games, we find
our proposed criterion significantly reduces the switching cost and at the same time enjoys the same
performance as the case where there is no switching cost constraint.
We believe our paper is just the first step on this important problem. One interesting question is how
to design principled policy switching criteria for policy-based and model-based methods. Another
direction is to give provable guarantees for these policy switching criteria that work for methods
dealing with large state space in contrast to existing analyses are all about tabular RL (Bai et al.,
2019; Zhang et al., 2020b;a).
8
Under review as a conference paper at ICLR 2021
References
Ki Uhn Ahn and Cheol Soo Park. Application of deep q-networks for model-free optimal control
balancing between different hvac systems. Science and Technology for the Built Environment, 26
(1):61-74, 2020.
Tao Ao, Jiong Shen, and Xichui Liu. The application of dqn in thermal process control. In 2019
Chinese Control Conference (CCC), pp. 2840-2845. IEEE, 2019.
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit
problem. Machine learning, 47(2-3):235-256, 2002.
Yu Bai, Tengyang Xie, Nan Jiang, and Yu-Xiang Wang. Provably efficient q-learning with low
switching cost. In Advances in Neural Information Processing Systems, pp. 8002-8011, 2019.
Marc G. Bellemare, W. Dabney, and R. Munos. A distributional perspective on reinforcement learn-
ing. ArXiv, abs/1707.06887, 2017.
Nicolo Cesa-Bianchi, Ofer Dekel, and Ohad Shamir. Online learning with switching costs and other
adaptive adversaries. In Advances in Neural Information Processing Systems, pp. 1160-1168,
2013.
Shi-Yong Chen, Yang Yu, Qing Da, Jun Tan, Hai-Kuan Huang, and Hai-Hong Tang. Stabilizing
reinforcement learning in dynamic environment with application to online recommendation. In
Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &
Data Mining, pp. 1187-1196, 2018.
Meire Fortunato, Mohammad Gheshlaghi Azar, B. Piot, Jacob Menick, Ian Osband, A. Graves, Vlad
Mnih, Remi Munos, Demis Hassabis, O. PietqUin, Charles Blundell, and S. Legg. Noisy networks
for exploration. ArXiv, abs/1706.10295, 2018.
Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning for
robotic manipulation with asynchronous off-policy updates. In 2017 IEEE international confer-
ence on robotics and automation (ICRA), pp. 3389-3396. IEEE, 2017.
H. V. Hasselt, A. Guez, and D. Silver. Deep reinforcement learning with double q-learning. In AAAI,
2016.
Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney,
Dan Horgan, Bilal Piot, Mohammad Gheshlaghi Azar, and David Silver. Rainbow: Combining
improvements in deep reinforcement learning. In AAAI, 2018.
Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah
Jones, Shixiang Gu, and Rosalind Picard. Way off-policy batch deep reinforcement learning of
implicit human preferences in dialog. arXiv preprint arXiv:1907.00456, 2019.
Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Reinforce-
ment learning, pp. 45-73. Springer, 2012.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
Travis Mandel, Yun-En Liu, Sergey Levine, Emma Brunskill, and Zoran Popovic. Offline policy
evaluation across representations with applications to educational games. In AAMAS, pp. 1077-
1084, 2014.
Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Ofir Nachum, and Shixiang Gu. Deployment-
efficient reinforcement learning via model-based offline optimization. arXiv preprint
arXiv:2006.03647, 2020.
V. Mnih, K. Kavukcuoglu, D. Silver, Andrei A. Rusu, J. Veness, Marc G. Bellemare, A. Graves,
Martin A. Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, S. Petersen, C. Beattie, A. Sadik,
Ioannis Antonoglou, H. King, D. Kumaran, Daan Wierstra, S. Legg, and Demis Hassabis. Human-
level control through deep reinforcement learning. Nature, 518:529-533, 2015.
9
Under review as a conference paper at ICLR 2021
T. Schaul, John Quan, Ioannis Antonoglou, and D. Silver. Prioritized experience replay. CoRR,
abs/1511.05952, 2016.
Satinder P Singh, Tommi Jaakkola, and Michael I Jordan. Reinforcement learning with soft state
aggregation. In Advances in neural information processing systems, pp. 361-368, 1995.
R. Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 3:9T4,
2005.
R. Sutton and A. Barto. Reinforcement learning: An introduction. IEEE Transactions on Neural
Networks, 16:285-286, 2005.
Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman,
F. Turck, and P. Abbeel. Exploration: A study of count-based exploration for deep reinforcement
learning. ArXiv, abs/1611.04717, 2017.
Ziyu Wang, T. Schaul, Matteo Hessel, H. V. Hasselt, Marc Lanctot, and N. D. Freitas. Dueling
network architectures for deep reinforcement learning. In ICML, 2016.
Zihan Zhang, Xiangyang Ji, and Simon S. Du. Is reinforcement learning more difficult than bandits?
a near-optimal algorithm escaping the curse of horizon. arXiv preprint arXiv:2009.13503, 2020a.
Zihan Zhang, Yuan Zhou, and Xiangyang Ji. Almost optimal model-free reinforcement learning via
reference-advantage decomposition, 2020b.
10
Under review as a conference paper at ICLR 2021
A Details of experiments
A. 1 detailed algorithm
For completeness, we introduce the extensions of DQN and display our detailed algorithm.
Double Q-learning Conventional Q-learning is affected by an overestimation bias, due to the
maximization step in Equation 3, Double Q-learning (Hasselt et al., 2016) address this problem by
decoupling. They use the loss
(rh+1 + Yh+ιqβ(xh+ι, arg max qθ(xh+ι,a0)) - qθ(xh,ah))2	(4)
a0
This change reduce harmful overestimations that were present for DQN, which leads to a improve-
ment.
Multi-step learning Q-learning accumulates a single reward, Sutton (2005) use a forward-view
multi-step accumulated reward long ago, where a n-step accumulated reward is defined as
n-1
rh(n) := X γh(n)rh+k+1	(5)
k=0
and the final loss is
(rhn) + Yhn) maxq占(xh+ι,a0) - qθ(xh,ah))2	(6)
a0
Multi-step targets often lead to faster learning (Sutton & Barto, 2005)
Dueling networks Wang et al. (2016) splits the DQN network into two streams called value stream
V and advantage stream A, these two strems share the convolutional encoder f, and action value
Q(x, a) could be computed as:
Q(χ,a) = V(f (x)) + A(f(χ),a) - £a0A(f(x),a )	G)
Nactions
Prioritized replay DQN samples uniformly from the replay buffer, to sample more frequently
from those transitions from which the policy can learn more and quickly, Schaul et al. (2016) samples
transitions with probability ph relative to the last encountered absolute TD error
Ph H 卜h+1 + Yh+1 maxqg(xh+ι,a0) - qθ(Xh,ah)∣ω	(8)
a0
where ω is a hyper-parameter, and new transitions always have the maximum priority when they
enter the buffer to ensure a bias towards unseen transitions.
Distributional RL Instead of approximating the expected return as DQN, Bellemare et al. (2017)
proposed a method to approximate the distribution of returns on a a discrete support z, z is a vector
with Natoms atoms and is defined as
Zi = Vmin + (i- 1) Vmax - Vmin ,i ∈ 1, 2,…，Natoms	(9)
Natoms - 1
where Vmin and Vmax is the minimal and maximal value in this support. The approximat-
ing distribution dh is defined on this support and the probability piθ (xh, ah) on each atom i as
dh = (z, pθ (xh, ah)) .The goal is to update the trainable parameters θ to match this distribution with
the actual distribution of returns. To learn the probability piθ for each i with a variant of Bellman’s
equation, they minimize the KUllbeck-Leibler divergence Dkl(Φzdhj∣dh) between the distribution
dh and the target distribution dh := (rh+1 +Yh+1z,Pa(xh+1, argmaxα/ q^(xh+ι,a0))), where Φz is
a L2-projection from the target distribution to the fixed support Z and qg(xh+ι, a) = ZTPg(Xh+1, a)
11
Under review as a conference paper at ICLR 2021
Nosiy Net To address the limitations of exploring using -greedy policies, Fortunato et al. (2018)
propose a noisy linear layer
y = b + Wx + (bnoisy	b + (Wnoisy	w)x)	(10)
to replace the standard linear y = b + Wx, where b and w are random variables, denotes the
element-wise product.
Rainbow combines all of these 6 extensions, To make the policy deterministic but still keep explor-
ing during training, we remove the Noisy Net and adopt Count-Based exploration. The detailed
algorithm is as follows.
Algorithm 3 Detailed algorithm
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:
35:
36:
37:
38:
39:
40:
41:
42:
. Hyper-parameters for extensions
Distributional RL: Number of atoms Natoms, min/max values Vmin/Vmax
Prioritization replay memory: exponent ω, capacity N
Multi-step: number of steps n
. Initialization
Initialize prioritization replay memory D, state encoder fonline , value stream Vonline and advan-
tage stream Aonline for online policy Ponline
Initialize deployed policy Pdeployed and target policy Ptarget with parameters in Ponline
. Definition of Q-function
Zi = Vmin + (i -I) Vmaxmmn
Aonline(X) = NactiOns Pa Aonline(fonline(X)，a)
Ponline(X, a = SoftmaX(Vinlinefonline(X)) + Aonlinefonline(X), a - Aonline(X))
Qonline(X,a) =	i zipi *online(X, a)
Similarly, Qdeployed(X, a) = Pi zipideployed (X, a), Qtarget (X, a) = Pi zipitarget (X, a)
. Training process
set up environment
n(accumulated_updates) = 0, n(deplοyment) = 0
for h = 1 to Hmax do
if Termination then
reset environment
end if
select ah = argmaxaQdeployed(Xh,a)
Execute action ah in emulator and observe reward rh and state Xh+1
compute the hash codes through for Xh, φ(Xh) = sgn(Ag(Xh))
. A is a matrix with i.i.d. entries drawn from a standard Gaussian distribution
N (0, 1) and
g is a flat function
update the hash table counts, n(φ(Xh)) = n(φ(Xh)) + 1
update the reward rh = rh +
store (Xh, ah, rh, Xh+1) in D
β
λ∕n(φ(xh))
if h > Hstart then
sample minibatch of transitions (Xh0 , ah0 , wh0 , rh0 , rh0+1 ..., rh0+n-1, Xh0+n) from D
with probability ph0
. wh0 is the importance-sampling weight for transitions at h0
compute the multi-step reward rh(n0 ) = Pkn=-01 γnrh0+k
a = arg maxa Qonline (Xh0+n, a), mi = 0, i ∈ 1, 2, ..., Natoms
for j = 1 to Nato
ms do
TZj =卜hn) + Ynzj].clip(Vmin, Vmax)
∆z = (Vmax - Vmin)/(Natoms - 1)
bj = (TZj- Vmin)/∆z,l = [bjC,U = dbje
ml = ml + Pjarget (Xh0+n,a*)(u - bj ),mu = mu + Pjarget(Xh0+n, a* )(bj - I)
end for
DKLh0 = - Pi mi logPionline(Xh0, ah0)
update Ph by ∣DκLh"∣ω
update parameters in Ponline by a gradient descent step on DKLh0 × wh0
12
Under review as a conference paper at ICLR 2021
43:	n(accumulated_updates) = n(accumulated_updates) + 1
44:	if h % Htarget then
45:	update Ptarget by the parameters of Ponline
46:	end if
47:	if J(fd, fo, D, n(accumulated_updates), n(dePloyment)) = true then
48:	update Pdeployed by the parameters of Ponline
49:	n(accumulated_updates) = 0, n(dePloyment) = n(dePloyment) + 1
50:	end if
51:	end if
52:	end for
A.2 Hype-parameters
Table 1 lists the basic hyper-parameters of the algorithm, all of our experiments share these hyper-
parameters except the experiments on GYMIC adopt the Htarget as 1K . Most of these parameter
are the same with raw Rainbow algorithm. For the count base exploration, the bonus is β set to 0.01.
Parameter	Value
Hstart	20K
learning rate	0.0000625
Htarget(Atari)	8K
Htarget(GYMIC)	1K
Adam E	1.5 × 10-4
Prioritization type	proportional
Prioritization exponent ω	0.5
Prioritization importance sampling	0.4 -→ 1.0
Multi-step returns n	3
Distributional atoms Natoms	51
Distributional Vmin, VmaX	[-10, 10]
Discount factor Y	0.99
Memory capacity N	1M
Replay period	4
Minibatch size	32
Reward clipping	[-1, 1]
Count-base bonus	0.01
Activation function β	ReLu
Table 1: The basic hyper-parameters, we used the Adam optimizer with learing rate α = 0.0000625
and = 1.5 × 10-4 , before training the online policy, we let the initialized random policy make
20K steps to collect some transitions and the capacity for replay buffer is 1M. During the training
process, we sample 32 transitions from the replay buffer and update the online policy every four
steps. The reward is clipped into [-1, 1] and Relu is adopted as the activation function. For replay
prioritization we use the recommended proportional variant, with importance sampling from 0.4 to
1, the prioritization ω is set to 0.5. In addition, we employ Natoms = 51, Vmin = -10, Vmax = 10
for distributional RL and n = 3 for multi-step returns. Finally the count-base bonus is set to 0.01
Table 2 lists the rest hyper-parameters for experiments on GYMIC. Since there are 46 clinical fea-
tures in this environment, we stack 4 consecutive states to compose a 184-dimensional vector as the
input for the state encoder fonline(fdeployed or ftarget). The state encoder is a 2-layer MLP with
hidden size 128.
Table 3 shows the additional hyper-parameters for experiments on Atari games. The observations
are grey-scaled and resized to 84 × 84 tensor, and 4 consecutive frames are concatenated as a single
state, each action selected by the agent is repeated for 4 times. The state encoder is composed of 3
convolutional layers with 32, 64 and 64 channels, which use 8x8, 4x4, 3x3 filters and strides of 4, 2,
1 respectively.
13
Under review as a conference paper at ICLR 2021
Parameter	Value
State Stacked	-4-
Number of layers for MLP	2
Hidden size	128
Table 2: Extra hype-parameters for the experiments in GYMIC, we stack 4 consecutive states and
adopt a 2-layer MLP with hidden size 128 to extract the feature of states.
Parameter	Value
Gray scaling	True
Observation	(84, 84)
Frame Stacked	4
Action repetitions	4
Max frames per episode	108k
Encoder channels	32, 64, 64
Encoder filter size	8 X 8, 4 X 4, 3 X 3
Encoder stride	4, 2,1
Table 3: Additional hyper-parameters for experiments in Atari games. Observations are grey-scaled
and rescaled to 84 × 84 4 consecutive frames are staked as the state and each action is acted four
times. And we limit the max number of frames for an episode to 108K. The state encoder consists
of 3 convolutional layers.
A.3 Reward and switching cost
In the end, we list the value of the switching cost and reward of different criteria when the environ-
ment take 1.5 million steps and 3 million steps in Table 4.
B Proof for Section 5
Proof of Theorem 1. We let w = (1, 1, . . . , 1) ∈ Rk be a k-dimensional all one vector. We let
F={f : f(x) = (2σ(hv1,xi)-1,2σ(hv2,xi)-1,...,2σ(hvk,xi)-1)} ⊂ {Rk →Rk}
with σ(∙) being the ReLU activation function1 and Vi ∈ {ei, -ei} where e% ∈ Rk denotes the vector
that only the i-th coordinate is 1 and others are 0. We assume k is an even number and αk is an
integer for simplicity. We let the underlying f * be the vector correspond to (eι, e2,..., ek). We let
D1 = {(e1, 1), (e2, 1), . . . , (e(1-α)k, 1)} and D2 = {(e(1-α)k+1, 1), . . . , (ek, 1)}. Because we use
the ERM training scheme, it is clear that the training on D1 ∪ D2 will recover f*, i.e., f1+2 = f*
because if it is not f * is better solution (f* has 0 error ) for the empirical risk. Now if the similarity
score between f1 and f1+2 is smaller than α, it means forf1, its corresponding {v(1-α)k+1, . . . , vk}
are not correct. In this case, f1’s prediction error is at least 1 - α on D1 ∪D2, because it will predict
0 on all inputs of D2 .
□
1We define σ(0) = 0.5
14
UnderreVieW as a ConferenCe PaPersICLR 2021
1,500,000 steps
environment	criteria	None	FIX-IO2	FIX-IO3	FIX-IO4	Adaptive	F-0.98
battle_zone	Switching cost Reward Gap	368750.0 26900.0 ±9445.1 N/A	36873 23070.0 ±7691.9 -3830.0	369^ 17930.0 ±4472.7 -8970.0	363 18870.0 ±4417.4 -8030.0	843 14330.0 ±4397.9 -12570.0	5292 16970.0 ±5766.2 -9930.0
beam_rider	Switching cost Reward Gap	368750.0 7655.9 ±2021.2 N/A	36873 7554.26 ±2283.4 -101.6	369S 7493.76 ±2376.4 -162.1	365 4068.26 ± 1340.2 -3587.6	843 4078.56 ± 1819.8 -3577.3	nɪð 6759.74 ±2136.3 -896.2
pong	Switching cost Reward Gap	368750.0 -12.87±8.1 N/A	36873 —2.5 ±9.2 10.4	369S -13.69 ±4.0 -0.8	365 — 12.97 ±5.2 -0.1	843 -7.14 ± 10.2 5.7	T681A 0.08 ±6.7 12.9
qbert	Switching cost Reward Gap	368750.0 6050.25 ±2189.2 N/A	36873 5073.75 ±624.7 -976.5	369S 4359.75 ±534.2 -1690.5	365 649.25 ±245.3 -5401.0	843 2681.0 ± 1696.1 -3369.2	12683 3947.5 ± 1438.0 -2102.8
riverraid	Switching cost Reward Gap	368750.0 5453.2 ±787.6 N/A	36873 4602.8 ±693.8 -850.4	369S 4177.0 ±870.0 -1276.2	365 3152.6 ±658.6 -2300.6	843 3267.6 ±705.2 -2185.6	8993 4182.2 ±925.8 -1271.0
road-∏ιnner	Switching cost Reward Gap	368750.0 13266.0 ± 1513.4 	N/A		36873 26039.0 ±4605.4 12773.0	369S 14947.0 ±2000.3 168Lo	365 30620.0 ±6946.2 17354。	843 19513.0 ±4751.0 6247.0	1309^ 28968.0 ±4827.2 15702。
3,000,000 steps							
1 5	1	i battle_zone	Switching cost Reward Gap	743750.0 32250.0 ±8119.6 N/A	7437.5 23910.0 ±6289.8 -8340.0	744.0 21290.0 ±8191.8 -10960.0	73.5 17830.0 ±4384.2 -14420.0	122.5 12530.0 ±5096.0 -19720.0	4240.3 26180.0 ±7161.5 -6070.0
beam_rider	Switching cost Reward Gap	743750.0 13751.2 ±6530.9 N/A	7437.5 11393.24 ±4802.1 -2358.0	744S 12236.62 ±5669.9 -1514.6	735 5654.16 ±2120.0 -8097.0	I2∑5 8857.94 ±3173.9 -4893.3	12334 12311.34 ±6731.4 -1439.9
pong	Switching cost Reward Gap	743750.0 17.96 ±6.8 N/A	7437.5 17.92 ±4.4 -0.0	744S 20.35 ±2.3 2.4	735 17.0 ±5.0 -1.0	I2∑5 19.02 ±2.2 1.1	268T0 17.43 ±5.4 -0.5
qbert	Switching cost Reward Gap	743750.0 17685.25 ±2486.0 N/A	7437.5 17038.5 ±2710.1 -646.8	74^0 13836.0 ±4032.8 -3849.2	733 2193.75 ± 1629.5 -15491.5	1223 3838.0 ± 1716.6 -13847.2	19803 13791.0 ±3372.0 -3894.2
riverraid	Switching cost Reward Gap	743750.0 7808.6 ±308.6 N/A	7437.5 7840.3 ±351.4 31.7	744S 7872.9 ± 939.1 64.3	735 3391.1 ±821.2 -4417.5	I2∑5 4013.5 ± 938.3 -3795.1	23253 7052.3 ±862.7 -756.3
road-∏ιnner	Switching cost Reward Gap	743750.0 37480.0 ±8429.9 	N/A		7437.5 36118.0 ±6366.6 -1362.0	744S 36722.0 ±7119.7 -758.0	735 35055.0 ±7095.8 -2425.0	I2∑5 45706.0 ±8473.7 8226.0	2736^ 45940.0 ±8234.1 8460.0
Table 4: We list the value of the switching cost and reward of different criteria when the environment takes 1.5 million steps and 3 million steps. "Reward”
corresponds to the absolute value of the reward, and "Gap” denotes the difference between the reward under a specific criterion and "None." And ("Switching Cosf,
corresponds to the switching cost under a criterion at this time step.