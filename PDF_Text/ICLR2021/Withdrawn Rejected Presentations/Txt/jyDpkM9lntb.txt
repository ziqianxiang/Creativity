Under review as a conference paper at ICLR 2021
Multi-Task	Multicriteria	Hyperparameter
Optimization
Anonymous authors
Paper under double-blind review
Ab stract
We present a new method for searching optimal hyperparameters among several
tasks and several criteria. Multi-Task Multi Criteria method (MTMC) provides
several Pareto-optimal solutions, among which one solution is selected with given
criteria significance coefficients. The article begins with a mathematical formula-
tion of the problem of choosing optimal hyperparameters. Then, the steps of the
MTMC method that solves this problem are described. The proposed method is
evaluated on the image classification problem using a convolutional neural net-
work. The article presents optimal hyperparameters for various criteria signifi-
cance coefficients.
1	Introduction
Hyperparameter optimization (Hutter et al., 2009) is an important component in the implementa-
tion of machine learning models (for example, logistic regression, neural networks, SVM, gradient
boosting, etc.) in solving various tasks, such as classification, regression, ranking, etc. The problem
is how to choose the optimal parameters when a trained model is evaluated using several sets and
several criteria.
This article describes a method to solving the above problem. We will present the results of ex-
periments on the selection of hyperparameters obtained using the proposed approach (MTMC) with
various criteria significance coefficients.
The article is organized as follows. First, we discuss related work in Section 2. Section 3 describes
the proposed method. Section 4 presents the results of experiments on the selection of optimal
hyperparameters. Section 5 contains the conclusion and future work.
2	Related Work
Hyperparameter optimization is applied to solve various problems such as computer vision (Bergstra
et al., 2013; Dong et al., 2019), robotics (Mahmood et al., 2018; Tran et al., 2020), natural language
processing (Wang et al., 2015; Dernoncourt & Lee, 2016) and speech synthesis (Koriyama et al.,
2014).
The problem of choosing optimal hyperparameters has long been known. Existing solutions can be
considered for the following features:
1.	Number of optimal solutions.
2.	Number of tasks to be solved.
3.	Number of criteria for choosing the optimal solution.
In this article, task means a set of images with a number of classes Nclasses and a number of images
Nimages . There are examples of the same classes between tasks, the difference is how the images
are made (different lighting, background and used cameras). Criteria is a quantitative characteristic
of training / evaluation a neural network on a task (e.g. accuracy, latency or epoch of training
convergence).
1
Under review as a conference paper at ICLR 2021
In (Sener & Koltun, 2018), a Pareto optimization method is proposed, in which the optimal solution
is given for several problems simultaneously. This method consists in minimizing the weighted sum
of loss functions for each task. (Fliege & Svaiter, 2000) describes the Pareto optimization method,
which gives an optimal solution according to several criteria based on gradient descent, and this op-
timization is also carried out in the learning process. In (Igel, 2005), the search for a Pareto-optimal
solution is carried out according to several criteria. (Miettinen, 2012) gives several methods for
multiobjective Pareto-optimization. The method in (Bengio, 2000) gives optimal hyperparameters
using back propagation through the Cholesky decomposition. In (Bergstra et al., 2011), optimiza-
tion is performed using a random choice of hyperparameters based on the expected improvement
criterion. (Bergstra & Bengio, 2012) proposes method of hyperparametric optimization based on
random search in the space of hyperparameters. In (Snoek et al., 2012), search for optimal hyper-
parameters is carried out using Bayesian optimization. (Swersky et al., 2013) proposes a method
for finding optimal hyperparameters using multi-task Bayesian optimization. (Paria et al., 2020;
Hemandez-Lobato et al., 2θ16) describe the Bayesian methods for multi-objective optimization.
The proposed method based on Pareto optimization. Pareto optimality means that it is impossible
to improve the Pareto optimal solution by any criteria without worsening it by at least one other
criteria. Thus, for a certain set of values of the criteria, the Pareto-selected solutions are optimal.
The closest criteria to the given criteria are determined by finding the minimum weighted sum of
Pareto solutions (where the weight is the inverse criteria).
The novelty of MTMC method:
1.	Optimization is carried out simultaneously according to several criteria and several tasks with
setting the significance of the criteria.
2.	The choice of optimal hyperparameters is provided after training and evaluation, which eliminates
the need to re-train the model.
3.	The proposed method does not need to be trained.
3	The Proposed Method
First, we describe the mathematical problem that MTMC solves, then we present the steps performed
in MTMC.
3.1	Formalization of The Problem
In the proposed method, the model is evaluated on several test sets (tasks) T. The problem of finding
a minimum for tasks T is known as minimizing the expected value of empirical risk (Vapnik, 1992).
The choosing optimal hyperparameters is formalized as follows:
θ = argminEτ [L(θ, φ)]	(1)
θ∈Θ
where Θ is the set of all hyperparameters, θ is the selected optimal hyperparameters, φ is the vector
of significance coefficients of the criteria, L(∙) is the estimation function of the model with the given
hyperparameters θ and the coefficients φ, τ is the task for which optimization is performed.
The developed method gives a solution to the problem (1).
3.2	Description of MTMC
According to (1), the developed method should fulfill the following requirements:
1)	the method should solve the minimization problem;
2)	the significance of each criterion is determined by the vector of coefficients φ (the higher the
coefficient, the more important the corresponding criterion).
We denote the test sample of the task τ :
Xi 〜D,i
1 . . . Ntask
(2)
2
Under review as a conference paper at ICLR 2021
where xi 2 3 is the ith test set has the distribution D, Ntask is the number of tasks.
Before choosing hyperparameters, for model M we obtain an evaluation matrix for the test set xi
and the given evaluation criteria:
V = M (xi]θ)
(3)
^M (Xi ； θ) :: (RXsize RNparameter × Ncombination ) _→ RNcombination × Ncriteria
(4)
where M(∙) is the model function that transforms the given set Xi and with the given hyperparam-
eters Θ into the evaluation matrix V, Ncriteria is the number of criteria, xsize is the dimension of the
test set, Nparameter is the number of hyperparameters, Ncombination is the number of hyperparameter
combinations.
Then, the function L is calculated for each set Xi , which is formally described as follows:
LC Θ,φ) = E(V; φ)	⑸
E(V; φ) :: (RNCriteria , RNCriteria ) T R1	(6)
(d)	(e)	(f)
Figure 1: Illustration of the steps of our proposed method MTMC. (a) Evaluation for all combi-
nations of hyperparameters in the criteria space. (b) Getting the evaluation boundary. (c) Getting
Pareto optimal solutions. (d) Defining the criteria vector. (e) Projecting Pareto optimal solutions
onto the criteria vector. (f) Finding the closest projection to the origin of coordinates.
Figure 1 shows the steps of MTMC to obtain the optimal solution for the given criteria. MTMC
method gives Pareto optimal solutions in which the following steps are performed:
1. The vectors from the evaluation V (the number of such vectors is Ncriteria) is in the space of given
criteria.
2. Pareto gives solutions that are closer to optimal by some criterion, and farther from optimal by
other criteria. We get Pareto optimal solutions V ⊆ V the nearest Pareto front to the origin of the
criteria space coordinates:
V = ParetoFront (V), V ∈ RNOPt ×Ncriteria	(7)
where Nopt is the number of Pareto optimal solutions.
3. The optimal solutions v ∈ V are scaled according to each criterion to the interval [0; 1]:
丘
KCaIed = J∙——min-, Vmin ∈ RNCriteria, Vmin ∈ RNCriteria ,i = 1 ... Nopt	(8)
vmax - vmin
3
Under review as a conference paper at ICLR 2021
1	~	.,1	.	Γ∙	♦	1	Cl<	∙ .	∙	~	.,1	.	Γ∙
where Vmax is the Vector of maximum values of V for each criterion, Vmin is the vector of minimum
values of V for each criterion.
Thus, the optimal solution is the solution closest to the origin of coordinates, and if any solution
V ∈ V is the origin of coordinates, then it is optimal for any φ.
4.	The vector φ in the space of criteria is defined.
We introduce the vector of the optimal solution, which is the middle of the segment [0; 1] in the axes
of the criteria space:
Ncriteria
2)
(9)
Conditions for φ are:
φ	φopt, if∀i : φi =0,
φ ∈ [0; 1], otherwise.
(10)
5.	It is necessary to determine which hyperparameter in the criteria space is closer to the given
criterion. Project the vectors from the matrix Vscaled onto the vector φ:
力T	.人
VprOj= Scaφd φ, V ∈ RNopt.	(11)
From (9) and (11) it follows that if the vectors φ and φopt are collinear, then:
∃λ : φ
λ ∙ φopt ⇒ Vproj =):
i
1
φi
V
Vscaledi
^w^
^^X Vscaledi = Vscaled	.
i
(12)
That is, in the case of equality of all elements of φ, the minimization problem reduces to finding the
minimum L1-norm Vscaled.
From (11) it also follows that if some component of the vector φ is equal to zero, then the corre-
sponding criterion will not affect the choice of the optimal hyperparameter. If all criteria are equal
to zero, except for one, then only the criterion with a nonzero component of the vector φ will affect
the choice of optimal hyperparameters.
6.	We find hyperparameters θ at which the minimum of the vector Vproj is reached, which is equiv-
alent to finding the minimum weighted sum of vector values:
θ = argminVproj.	(13)
Figure 2 shows an example solution using MTMC for random numbers in the three-dimensional
space.
4	Conducting Experiments
First, the evaluation matrix V for the selected model M is obtained. Then, for various combinations
of components of φ, optimal hyperparameters are selected using MTMC.
4.1	Obtaining The Evaluation Matrix
The developed MTMC method is applied to solve the problem of image classification. The problem
we are solving is described in the article (Akhmetzyanov & Yuzhakov, 2019). The problem is in the
choice of such hyperparameters, which achieve the highest classification accuracy among several
tasks. Each task consists of several images of one of two objects such as a plastic bottle and other
object. The tasks differ in how the images are made, namely lighting, background and used cameras.
In (Akhmetzyanov & Yuzhakov, 2018), we selected the MobileNet neural network architecture
(Howard et al., 2017) as a mathematical model for image processing.
4
Under review as a conference paper at ICLR 2021
Figure 2: Example of a solution given by MTMC, green points denote Pareto optimal solutions, blue
vector is the vector φ, yellow point denotes the optimal solution given by MTMC for a given φ.
The search for optimal hyperparameters was carried out among two popular training methods:
changing the learning speed based on the epoch lr = baseJr ∙ lr_deCayePoch (where baseJr is
the initial learning rate, lr.decay is the coefficient of decreasing learning rate, epoch is the number
of epochs) and cyclical learning (Smith, 2017). In cyclic learning, there are three ways to change
the learning rate:
1.	triangular is fixed initial learning rate (baseJr), maximum fixed learning rate (max Jr), learning
rate increases from baseJr to maxdr and decreases from maxdr to baseJr linearly.
2.	triangular2 is fixed initial learning rate (baseJr), maximum learning rate (maxJr), learning rate,
as in triangular, varies linearly, but maxdr in the learning process is halved.
3.	exp_range is fixed initial learning rate (baseJr), maximum learning rate (maxdr), learning rate
also changes linearly, but maxJr in the learning process decreases exponentially.
In the first learning method, the hyperparameters are the value of the initial learning rate (baseJr)
and the coefficient of decreasing learning rate (lr^decay). In the second method, hyperparameters
a way to change the learning rate (Cyclic_mode), the value of the initial learning rate (baseJr) and
maximum learning rate (maxJr).
For each hyperparameter, a range of change and a constant step of change within the range were
selected. For training, Grid search was used among Ncombination = 100 combinations of hyperpa-
rameters.
For each combination of hyperparameters, training was carried out using cross-validation k-fold
(Stone, 1974) with 10 folds. For training, Keras (Chollet et al., 2015) and TensorFlow (Abadi et al.,
2015) were used. The training lasted 15 epochs; the test was carried out on Ntask = 5 different test
sets. That is, 100 ∙ 10 = 1000 is number of different neural networks, 15 ∙ 1000 = 15000 neural
networks evaluations are conducted, 15000 ∙ 5 = 75000 evaluation results are obtained. Neural
networks trained on ten TPUs v2, which took several days.
Among all epochs, for each fold and for each test set, the maximum accuracy is selected, as well
as the number of the epoch at which the maximum accuracy is achieved. The following values are
calculated for each test set among the folds: the expected value and the variance of the classifica-
tion error, the expected value and the variance of the epoch number at which convergence on the
5
Under review as a conference paper at ICLR 2021
test set is achieved. We have obtained an evaluation matrix among all neural networks with their
hyperparameters and among all test samples.
4.2	Processing The Evaluation Matrix
Based on (1), for each criterion, among all the samples, the expected value is considered. That is,
for all test sets, the criteria: (i) the sample mean of the classification error, (ii) the sample variance of
the classification error, (iii) the sample mean and (iv) sample variance of the epoch number at which
convergence on the test set is achieved. These values are the criteria for evaluating hyperparameters
for a certain test set (matrix V from (3)) with the number of criteria Ncriteria = 4.
V is calculated from (7), the number of Pareto optimal solutions obtained is Nopt = 25. Optimal
hyperparameters, i.e., V , are presented in Appendix A.
The vector of the optimal solution according to (9) for Ncriteria = 4 is φopt = {0.5; 0.5; 0.5; 0.5}.
Next, calculations are carried out according to (8) and (11), and for various φ optimal solutions are
chosen according to (5). These optimal solutions are presented in Appendix B. Also, Appendix C
shows the learning curve of the neural network for each test set and each epoch for the obtained
optimal hyperparameters.
5	Conclusion
In this work, we proposed a new method for hyperparameter optimization among several tasks and
several criteria. We trained several neural networks with various hyperparameters to solve the im-
age classification problem. Then, for these neural networks, evaluation matrices were obtained on
several tasks. We applied MTMC to these matrices and got optimal solutions with different signifi-
cance coefficients. In the future, we will work to create a meta-learning method that solves the same
problem as the method described in this article, but optimization will be performed among various
models.
Acknowledgments
The reported study was partially supported by the Government of Perm Krai, research project No.
C-26/174.6.
References
Martin Abadi, Ashish Agarwal, et al. TensorFlow: Large-scale machine learning on heterogeneous
systems, 2015. URL http://tensorflow.org/. Software available from tensorflow.org.
Kirill Akhmetzyanov and Alexander Yuzhakov. Convolutional neural networks comparison for
waste sorting tasks. Izvestiya SPbGETU ”LETI”, (6):27, 2018.
Kirill Akhmetzyanov and Alexander Yuzhakov. Waste sorting neural network architecture opti-
mization. In 2019 International Russian Automation Conference (RusAutoCon), pp. 1-5. IEEE,
2019.
Yoshua Bengio. Gradient-based optimization of hyperparameters. Neural computation, 12(8):1889-
1900, 2000.
James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of
Machine Learning Research, 13(Feb):281-305, 2012.
James Bergstra, Daniel Yamins, and David Cox. Making a science of model search: Hyperparameter
optimization in hundreds of dimensions for vision architectures. In International conference on
machine learning, pp. 115-123, 2013.
James S Bergstra, Remi Bardenet, Yoshua Bengio, and BaIaZs KegL Algorithms for hyper-parameter
optimization. In Advances in neural information processing systems, pp. 2546-2554, 2011.
Francois Chollet et al. Keras. https://keras.io, 2015.
6
Under review as a conference paper at ICLR 2021
F. Dernoncourt and J. Y. Lee. Optimizing neural network hyperparameters with gaussian processes
for dialog act classification. In 2016 IEEE Spoken Language Technology Workshop (SLT), pp.
406-413,2016. doi:10.1109/SLT.2016.7846296.
X. Dong, J. Shen, W. Wang, L. Shao, H. Ling, and F. Porikli. Dynamical hyperparameter opti-
mization via deep reinforcement learning in tracking. IEEE Transactions on Pattern Analysis and
Machine Intelligence, pp. 1-1, 2019. doi: 10.1109/TPAMI.2019.2956703.
Jorg Fliege and Benar FUx Svaiter. Steepest descent methods for multicriteria optimization. Mathe-
matical Methods of Operations Research, 51(3):479-494, 2000.
Daniel Hernandez-Lobato, Jose Hernandez-Lobato, Amar Shah, and Ryan Adams. Predictive en-
tropy search for multi-objective bayesian optimization. In International Conference on Machine
Learning, pp. 1492-1501, 2016.
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for
mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.
Frank Hutter, Holger H Hoos, Kevin Leyton-Brown, and Thomas StUtzle. Paramils: an automatic al-
gorithm configuration framework. Journal of Artificial Intelligence Research, 36:267-306, 2009.
Christian Igel. Multi-objective model selection for support vector machines. In International Con-
ference on Evolutionary Multi-Criterion Optimization, pp. 534-546. Springer, 2005.
T. Koriyama, T. Nose, and T. Kobayashi. Parametric speech synthesis based on gaussian process
regression using global variance and hyperparameter optimization. In 2014 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 3834-3838, 2014. doi:
10.1109/ICASSP.2014.6854319.
A Rupam Mahmood, Dmytro Korenkevych, Gautham Vasan, William Ma, and James Bergstra.
Benchmarking reinforcement learning algorithms on real-world robots. In Conference on Robot
Learning, pp. 561-591, 2018.
Kaisa Miettinen. Nonlinear multiobjective optimization, volume 12. Springer Science & Business
Media, 2012.
Biswajit Paria, Kirthevasan Kandasamy, and BarnabaS P6czos. A flexible framework for multi-
objective bayesian optimization using random scalarizations. volume 115 of Proceedings of Ma-
chine Learning Research, pp. 766-776. PMLR, 22-25 Jul 2020.
Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. In Advances
in Neural Information Processing Systems, pp. 527-538, 2018.
Leslie N Smith. Cyclical learning rates for training neural networks. In 2017 IEEE Winter Confer-
ence on Applications of Computer Vision (WACV), pp. 464-472. IEEE, 2017.
Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine
learning algorithms. In Advances in neural information processing systems, pp. 2951-2959, 2012.
Mervyn Stone. Cross-validatory choice and assessment of statistical predictions. Journal of the
Royal Statistical Society: Series B (Methodological), 36(2):111-133, 1974.
Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Multi-task bayesian optimization. In
NIPS, pp. 2004-2012, 2013.
D. P. Tran, G. N. Nguyen, and V. D. Hoang. Hyperparameter optimization for improving recognition
efficiency of an adaptive learning system. IEEE Access, 8:160569-160580, 2020. doi: 10.1109/
ACCESS.2020.3020930.
Vladimir Vapnik. Principles of risk minimization for learning theory. In Advances in neural infor-
mation processing systems, pp. 831-838, 1992.
Lidan Wang, Minwei Feng, Bowen Zhou, Bing Xiang, and Sridhar Mahadevan. Efficient hyper-
parameter optimization for nlp applications. In Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, pp. 2112-2117, 2015.
7
Under review as a conference paper at ICLR 2021
Table 1: Pareto optimal solutions for the first learning method
base lr	lr decay
0.001	0.75
0.001	0.8
0.005	0.75
0.01	0.9
0.01	0.95
Table 2: Pareto optimal solutions for the second learning method
base」r max」r Cyclic mode
0.0001	0.005	exp_range
0.0001	0.005	triangular2
0.0005	0.001	exp_range
0.0005	0.005	triangular2
0.001	0.0001	triangular2
0.001	0.0005	triangular
0.001	0.001	exp_range
0.001	0.005	triangular2
0.005	0.0001	triangular
0.005	0.005	triangular
0.01	0.0001	triangular
0.01	0.0001	triangular2
0.01	0.005	triangular
0.01	0.005	triangular2
0.01	0.01	triangular
0.0001	0.0001	triangular
0.0005	0.001	triangular
0.0005	0.01	triangular2
0.001	0.005	triangular
0.005	0.01	triangular
A	Pareto Optimal Hyperparameters
Table 1 and Table 2 show the Pareto optimal hyperparameters for the two learning methods.
8
Under review as a conference paper at ICLR 2021
B MTMC Optimal Hyperparameters
Table 3 shows the optimal hyperparameters θ obtained using MTMC method for given significance
coefficients of the criteria φ.
C Accuracy of MTMC Optimal Solutions
The figures 3-7 show optimal solutions chosen by MTMC method: 95% confidence intervals of the
dependence of accuracy on the fold / epoch and maximum accuracy for each of the folds (left) and
box plot with maximum accuracy for all test sets (right).
9
Under review as a conference paper at ICLR 2021
Table 3: Optimal hyperparameters for relevant criteria significance coefficients
φ0	φ1	φ2	φ3	θ
0.5	0.5	0.5	0.5	base_lr=0.01, max_lr=0.01, cyclic_mode=triangular
0.0	0.5	0.5	0.5	base_lr=0.01, max_lr=0.01, cyclic_mode=triangular
1.0	0.5	0.5	0.5	base_lr=0.01, max_lr=0.01, cyclic_mode=triangular
0.5	0.0	0.5	0.5	base_lr=0.005, max_lr=0.0001, cyclic_mode=triangular
0.5	1.0	0.5	0.5	base_lr=0.01, max_lr=0.01, cyclic_mode=triangular
0.5	0.5	0.0	0.5	base_lr=0.01, max_lr=0.01, cyclic_mode=triangular
0.5	0.5	1.0	0.5	base_lr=0.01, max_lr=0.01, cyclic_mode=triangular
0.5	0.5	0.5	0.0	base-lr=0.0001, max_lr=0.005, cyclic_mode=triangular2
0.5	0.5	0.5	1.0	base_lr=0.01, max_lr=0.01, cyclic_mode=triangular
0.0	0.0	0.5	0.5	max_lr=0.005, lr_decay=0.75
1.0	1.0	0.5	0.5	base_lr=0.01, max_lr=0.01, cyclic_mode=triangular
0.5	0.5	0.0	0.0	base-lr=0.0001, max-lr=0.005, cyclic_mode=triangular2
0.5	0.5	1.0	1.0	base_lr=0.01, max-lr=0.01, cyclic_mode=triangular
1.0	0.0	0.0	0.0	base_lr=0.0001, max_lr=0.005, cyclic_mode=triangular2
0.0	1.0	0.0	0.0	base-lr=0.0001, max-lr=0.005, cyclic_mode=triangular2
0.0	0.0	1.0	0.0	max-lr=0.005, lr_decay=0.75
0.0	0.0	0.0	1.0	base-lr=0.0005,
max_lr=0.001,
cyclic_mode=exp_range
10
Under review as a conference paper at ICLR 2021

Figure 3: base_lr=0.01, max」r=0.01, cyclic_mode=triangular
11
Under review as a conference paper at ICLR 2021
Figure 4: base_lr=0.005, max_lr=0.0001, cyclic_mode=triangular
12
Under review as a conference paper at ICLR 2021
Figure 5: base」r=0.0001, max」r=0.005, cyclic_mode=triangular2
13
Under review as a conference paper at ICLR 2021
Figure 6: max」r=0.005,lr_decay=0.75
14
Under review as a conference paper at ICLR 2021
Figure 7: base」r=0.0005, maxJr=0.001, cyclic_mode=exp_range
15