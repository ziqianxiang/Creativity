Under review as a conference paper at ICLR 2021
A General Computational Framework to Mea-
sure the Expressiveness of Complex Networks
using a Tight Upper B ound of Linear Regions
Anonymous authors
Paper under double-blind review
Ab stract
The expressiveness of deep neural network (DNN) is a perspective to understand
the surprising performance of DNN. The number of linear regions, i.e. pieces that
a piece-wise-linear function represented by a DNN, is generally used to measure
the expressiveness. And the upper bound of regions number partitioned by a rec-
tifier network, instead of the number itself, is a more practical measurement of
expressiveness of a rectifier DNN. In this work, we propose a new and tighter up-
per bound of regions number. Inspired by the proof of this upper bound and the
framework of matrix computation in Hinz & Van de Geer (2019), we propose a
general computational approach to compute a tight upper bound of regions number
for theoretically any network structures (e.g. DNN with all kind of skip connec-
tions and residual structures). Our experiments show our upper bound is tighter
than existing ones, and explain why skip connections and residual structures can
improve network performance.
1	Introduction
Deep nerual network (DNN) (LeCun et al., 2015) has obtained great success in many fields such as
computer vision, speech recognition and neural language process (Krizhevsky et al., 2012; Hinton
et al., 2012; Devlin et al., 2018; Goodfellow et al., 2014). However, it has not been completely under-
stood why DNNs can perform well with satisfying generalization on different tasks. Expressiveness
is one perspective used to address this open question. More specifically, one can theoretically study
expressiveness of DNNs using approximation theory (Cybenko, 1989; Hornik et al., 1989; Hanin,
2019; Mhaskar & Poggio, 2016; Arora et al., 2016), or measure the expressiveness of a DNN. While
sigmoid or tanh functions are employed as the activation functions in early work of DNNs, rectified
linear units (ReLU) or other piece-wise linear functions are more popular in nowadays. Yarotsky
(2017) has proved that any DNN with piece-wise linear activation functions can be transformed to
a DNN with ReLU. Thus, the study of expressiveness usually focuses on ReLU DNNs. It is known
that a ReLU DNN represents a piece-wise linear (PWL) function, which can be regarded to have dif-
ferent linear transforms for each region. And with more regions the PWL function is more complex
and has stronger expressive ability. Therefore, the number of linear regions is intuitively a meaning-
ful measurement of expressiveness (Pascanu et al., 2013; Montufar et al., 2014; Raghu et al., 2017;
Serra et al., 2018; Hinz & Van de Geer, 2019).
A direct measurement of linear regions number is difficult, if not impossible, and thus the upper
bound of linear regions number is practically used as a figure of metrics to characterize the expres-
siveness. Inspired by the computational framework in (Hinz & Van de Geer, 2019), we improve
the upper bound in Serra et al. (2018) for multilayer perceptrons (MLPs) and extend the framework
to more complex networks. More importantly, we propose a general approach to construct a more
accurate upper bound for almost any type of network. The contributions of this paper are listed as
follows.
•	Through a geometric analysis, we derive a recursive formula for γ, which is a key parameter
to construct a tight upper bound. Employing a better initial value, we propose a tighter
upper bound for deep fully-connected ReLU networks. In addition, the recursive formula
provide a potential to further improve the upper bound given an improved initial value.
1
Under review as a conference paper at ICLR 2021
•	Different from Hinz & Van de Geer (2019), we not only consider deep fully-connected
ReLU networks, but also extend the computational framework to more widely used network
architectures, such as skip connections, pooling layers and so on. With the extension,
the upper bound of U-Net (Ronneberger et al., 2015) or other common networks can be
computed. By comparing the upper bound of different networks, we show the relation
between expressiveness of networks with or without special structures.
•	Our experiments show that novel network structures enhance the upper bound in most
cases. For cases in which the upper bound is almost not enhanced by novel network set-
tings, we explain it by analysing the partition efficiency and the practical number of linear
regions.
2	Related work and preliminaries
2.1	Related work
There are literature on the linear regions number in the case of ReLU DNNs. Pascanu et al. (2013)
compare the linear regions number of shallow networks by providing a lower bound. Montufar et al.
(2014) give a simple but improved upper bound compared with PascanU et al. (2013). Montufar
(2017) proposes a even tighter upper bound than Montufar et al. (2014). And Raghu et al. (2017)
also prove a similar result which has the same order compared to Montufar (2017). Later, Serra et al.
(2018) propose a tighter upper bound and a method to count the practical number of linear regions.
Furthermore, Serra & Ramalingam (2018); Hanin & Rolnick (2019a;b) explore the properties of the
practical number of linear regions. Finally, Hinz & Van de Geer (2019) employ the form of matrix
computation to erect a framework to compute the upper bound, which is a generalization of previous
work (Montufar et al., 2014; Montufar, 2017; Serra et al., 2018)
2.2	Notations, definitions and properties
In this section, we will introduce some definitions and propositions. Since the main computational
framework is inspired by Hinz & Van de Geer (2019), some notations and definitions are similar.
Let us assume a ReLU MLP has the form as follows.
f(x) = W(L) σ(W(LT)…σ(W⑴X + b⑴)…+ b(LT)) + b(L)	(1)
'-----{------}
(L-1)
where x ∈ Rn0, W(i) ∈ Rni×ni-1 , b(i) ∈ Rni and σ(x) = max(x, 0) denoting the ReLU function.
W(i) is the weights in the ith layer and b(i) is the bias vector. f(x) can also be written as:
h0(x) = x, hi(x) = σ(W(i)hi-1(x) + b(i)), 1 ≤ i < L,	(2)
f(x) = hL(x) = W(L) hL-1 (x) + b(L)	(3)
Firstly, we define the linear region in the following way.
Definition 1. For a PWL function f(x) : Rn0 → RnL, we define D is a linear region, if D satisfies
that: (a) D is connected; (b) f is an affine function on D; (c) Any D0 % D, f is not affine on D0 .
For a PWL function f, the domain can be partitioned into different linear regions. Let
P(f) = {Di∣Di is a linear region of f, ∀Di = Dj, Di ∩ Dj = 0}
represent all the linear regions off. We then define the activation pattern of ReLU DNNs as follows.
Definition 2. For any x ∈ Rn0, we define the activation pattern of x in ith layer shi (x) ∈ {0, 1}ni
as follows.
1, ifWj(,i:)hi-1(x)+b(ji) >0
shi (x)j =	(i)	(i)	, for i ∈ {1, 2, . . . , L - 1}, j ∈ {1, 2, . . . , ni},
0, if Wj,: hi-1(x) + bj ≤ 0
where Wj(,i:) is the jth row of W(i), b(ji) is the jth component of b(i). (Hinz & Van de Geer, 2019)
2
Under review as a conference paper at ICLR 2021
For any x, hi(x) can be rewritten as hi(x) = W(i) (x)hi-1(x) + b(i)(x), where W(i) (x) is a
matrix with some rows of zeros and b(i) (x) is a vector with some zeros. More precisely,
W (i) (x)	=	Wj(,i:), if shi (x)j = 1; b(i)(x) = b(ji), if shi (x)j = 1;	(4)
j,:	0,	if shi (x)j =0.	j 0,	if shi (x)j =0.
To conveniently represent activation patterns of multi-layer in a MLP, we denote h(i) = {h1, ..., hi},
where hi is defined in Eq.2, and Sh(i) (x) = (sh1 (x), ..., shi (x)), S(h(i)) = {Sh(i) (x)|x ∈ Rn0}.
Given a fixed x, it is easy to prove that hi (x) is an affine transform (i = 1, 2, . . . , L). Suppose
that S ∈ {0,1}n1 X …× {0,1}nL-1, h = {hi,…，hL-ι} and h(x) represents hL-ι(x), if D =
{x∣Sh(x) = s} = 0, then f is an affine transform in D. And it is easy to prove that there exists a
linear region D0 so that D ⊆ D0. Therefore we have |P(f)| ≤ |S(h)|.
In our computational framework, histogram is a key concept and defined as follows.
Definition 3. Define a histogram v as follows. (Hinz & Van de Geer, 2019)
i∞
X ∈ NN| k xk1 = Xxj < ∞
j=0
(5)
A histogram is used to represent a discrete distribution of N. For example, the histogram of non-
negative integers G = {1, 0, 1, 4, 3, 2, 3, 1} is (1, 3, 1, 2, 1)>. For convenience, let vi = Px∈G 1x=i
and the histogram of G is denoted by Hist(G). We can then define an order relation.
Definition 4. For any two histograms v , w, define the order relation as follows. (Hinz & Van de
Geer, 2019)
∞∞
V W W ⇔ ∀J ∈ N, ɪ2 Vj ≤ ɪ2 Wj	(6)
j=J	j=J
It is obvious that any two histograms are not always comparable. But we can define a max operation
such that v(i) W max {v(i) |i ∈ I} where I is an index collection. More precisely:
Definition 5. For a finite index collection I, let VI = {v(i) |i ∈ I}, define max operation as follows.
max (VI)J = max X vj(i) - max X vj(i)	for J ∈ N
i∈I	j=J	i∈I j=J+1
(7)
where vj(i) is the jth component of the histogram v(i). (Hinz & Van de Geer, 2019)
When a region is divided by hyperplanes, the partitioned regions number will be affected by the
space dimension which is defined as:
Definition 6. For a connected and convex set D ⊆ Rn, if there exists a set of linear independent
vectors v(i) |v(i) ∈ Rn, i = 1, . . . , k, k ≤ n and a fixed vector c ∈ Rn, s.t. (a) any x 6= c ∈ D,
x = c + Pik=1 aiv(i) where {ai} are not all 0; (b) there exists ai 6= 0 s.t. aiv(i) + c ∈ D. Then the
space dimension of D is k and denote it as Sd(D) = k.
The following proposition shows the change of space dimension after an affine transform.
Proposition 1. Suppose D ⊆ Rn is a connected and convex set with space dimension of k (k ≤ n),
f is an affine transform with domain ofD and can be written as f(x) = Ax + b, where A ∈ Rm×n
and b ∈ Rm. Then f(D) is a connected and convex set and
Sd(f (D)) ≤ min(k, rank(A))	(8)
The proof of proposition 1 is given in Appendix A.1.1.
Now we can analyze the relationship between the change of space dimension and activation patterns.
Let us first consider the 1st layer in an MLP f. Wj(,1:) and b(j1) construct a hyperplane (Wj(,1:)x +
3
Under review as a conference paper at ICLR 2021
b(j1) = 0) in Rn0 and this hyperplane divides the whole region of Rn0 into two parts. One part
corresponds to successful activation of the jth node in the 1st layer and the other to unsuccessful
activation. This can be represented by the jth component of sh1 (x). Therefore all the possible
activation pattern sh1 (x) is one by one correspondent to the divided regions by n1 hyperplanes of
{Wj(,1:)x+b(j1)
have h1 (x) =
= 0,j = 1, . . . , n1}, which are denoted as Hh1 . For any region D divided by Hh1, we
W (1)(x(0))x+b(1) (x(0)) where x(0) is any point in D and rank(W (1) (x(0))) ≤ |s|1
where s is the correspondent activation pattern ofD. According to proposition 1, h1(D) satisfies that
Sd(h1(D)) ≤ min{n0, |s|1}. Similarly, Hh2 divides h1(D) into different parts, and this corresponds
to divide D into more sub-regions. In general, every element of S(h(i)) is correspondent to one of
regions that are partitioned by Hh1 , Hh2 , . . . , Hhi. The next two definitions are used to describe the
relationship between the change of space dimension and activation patterns.
Definition 7. Define Hsd(Sh) as the space dimension histogram of regions partitioned by a ReLU
network defined by Eq.1 where h = {h1 , . . . , hL-1}, i.e.
Hsd(Sh) = Hist({Sd (h (D (S))) | D (S) = {x∣Sh (X) = s} , S ∈ S (h)}).	(9)
Definition 8. Define Hd (Sh) as the dimension histogram of regions partitioned by a ReLU network
defined by Eq.1 where h = {h1, . . . , hL-1}, i.e.
Hd (Sh) = Hist ({min {n0, ∣Shι ∣ι ,…，归力工一∣ι} |s ∈ S(h)}) .	(10)
We then have the following proposition.
Proposition 2. Given a ReLU network defined by Eq.1, let h = {h1, . . . , hL-1}, then Hsd(Sh)
Hd (Sh).
The proof of Proposition 2 is given in the Appendix A.1.2. Proposition 2 shows that space dimension
is limited by dimension histogram. This idea is used to compute the upper bound.
We can then start to introduce our computational framework. For convenience, we denote RL(n, n0)
as one layer of a ReLU MLP with n input nodes and n0 output nodes (containing one linear transform
and one ReLU activation function), and define its activation histogram as follows.
Definition 9. Given h ∈ RL(n, n0), define Ha(Sh) as the activation histogram of regions parti-
tioned by h, i.e.
Ha(Sh) = Hist ({|S|1 | S∈ {Sh (x) | x∈Rn}}).	(11)
The activation histogram is similar to the dimension histogram, and it is used to define γ which is a
key parameter to construct an upper bound.
Definition 10. If γn0 ,n satisfies following conditions: (a) ∀n0 ∈ N+, n ∈ {0, . . . , n0},
max{Ha (Sh) |h ∈ RL (n,n0)} W Yn,n,；(b) ∀n0 ∈ N+,n,n ∈ {0,...,n0} ,n ≤ n =⇒ ln,n W
Yn n. Then, Yn n (n0 ∈ N+,n ∈ 1,...,n0) satisfies the bound condition. (Hinz & Van de Geer,
2019)
Here, for h ∈ RL(0, n0), we define H(Sh) = e0 = (1, 0, 0, . . . )>. Let Γ be the set of all
(Yn,n0 )n0∈N+,n∈{0,...,n0} that satisfy the bound conditions. When n > n0, Yn,n0 is defined to be
equal to Yn0,n0 since max {Ha (Sh) |h ∈ RL (n, n0)} is equal to max {Ha (Sh) |h ∈ RL (n0, n0)}
(Hinz & Van de Geer, 2019). By the definition Yn,n0 represents an upper bound of the activation
histogram of regions which are derived from n-dimension space partitioned by n0 hyperplanes. Ac-
cording to Proposition 1, this upper bound is also related to the upper bound of space dimension.
Therefore when Yn,n0 is tighter the computation of upper bound of linear regions number will be
more accurate. The following function is used to describe the relationship between upper bounds of
activation histogram and space dimension.
Definition 11. For i* ∈ N, define a clipping function cli*(∙) : V → V as follows. (Hinz & Van de
Geer, 2019)
{Vi	for i < i*
P∞=i* Vj for i = i*	(12)
0	for i > i*
With the definitions and notations above, we can introduce the computational framework to compute
the upper bound of linear regions number as follows.
4
Under review as a conference paper at ICLR 2021
Proposition 3. For a γ ∈ Γ, define the matrix Bn(γ0) ∈ N(n0+1)×(n0+1) as
Bn(γ0)i,j = (clj-1 (γj-1,n0))i-1 , i,j ∈ {1, . . . ,n0 + 1}.
Then, the upper bound of linear regions number of an MLP in Eq.1 is
Bn(γL)-1MnL-2,nL-1 ...Bn(γ1)Mn0,n1en0+11,	(13)
where Mn,n0 ∈ R(n0+1)×(n+1), (M)i,j = δi,min(j,n0+1). (Hinz & Van de Geer, 2019)
3 Main results
In this section, we introduce a better choice of γ and compare it to Serra et al. (2018). We also
extend the computational framework to some widely used network structures.
3.1 A tighter upper bound on the number of regions
Before giving our main results, we define a new function of histograms as follows.
Definition 12. Define a downward-move function dm(∙) : V → V by dm(v)i = Vi-ι. When i = 0,
set dm(v)0 = 0.
We then prove the following two theorems used to compute γ.
Theorem 1.	If γn,n00 satisfies the bound condition in Definition 10 when n00 < n0, then
γn,n0 = γn-1,n0 -1 + dm(γn,n0 -1 )	(14)
also satisfies the bound condition.
Theorem 2.	Given any h ∈ RL(1, n), its activation histogram Ha(Sh) denoted by v satisfies
Pin=t vi ≤ 2(n - t) + 1.
The proofs of Theorem 1 and 2 are in Appendix A.1.3 and A.1.4 respectively.
From Theorem 2, we can derive a feasible choice of γ1,n whose form is
γι,n = (0,..., 0,n mod 2, 2,..., 2,1)>
×~^{zz~}	×~{{zz~}
d2e-1	b2C
(15)
For example, γ1,4 = (0, 0, 2, 2, 1)>. Since there exists h ∈ RL(1, n) such that Ha(Sh) =
γ1,n (the proof can be seen in the Appendix A.1.5), Eq.15 is the tightest upper bound, i.e.
max {Ha (Sh) |h ∈ RL (1, n)} satisfies Eq.15. With the fact that max {Ha (Sh) |h ∈ RL (2, 1)} =
(1, 1)>, any γn,n0 can be computed.
3.2 Comparison with other bounds
We use γnounrs0 and γnsernra0 to represent the ones proposed by us and Serra et al. (2018). According to
Hinz & Va,n de Geer ,(2019),
( serra)	00, 0 ≤ i < n0 -n
(γn,n0)i = [(n0), n - n ≤ i ≤ n
It is easy to verify that γnse,rnra0 also satisfies Eq.14. But the initial value of γnse,rnra0 is different from γnou,nrs0
and we have that γ1se,rnra = (0, ..., 0, n, 1)>. Obviously, γ1o,unrs	γ1se,rnra. Since the two operations, +
and dm(∙), keep the relation of 工 for any n, n0 ∈ N+, YnUnO W Ynerna (see an example in Appendix
A.2.1). By the following theorem, the upper bound computed by Eq.1 using γnounrs0 is tighter than
serra
Yn,n0 .
Theorem 3. Given Y(1), Y(2) ∈ Γ, iffor any n, n0 ∈ N+ such that Yn(1,n) 0 W Yn(2,n) 0, then upper bound
computed by Eq.13 using Y(1) is less than or equal to the one using Y(2).
The proof of Theorem 3 is in Appendix A.1.6. Theorem 3 shows that ifwe can get a ”smaller” Yn,n0
then the Upper boUnd will be tighter. And Theorem 1 implies that if we have a more accUrate initial
valUe, e.g. ”smaller” Y2,n or Y3,n, the Upper boUnd coUld be fUrther improved. Therefore Theorem
1 provides a potential approach to achieve even tighter boUnd.
5
Under review as a conference paper at ICLR 2021
3.3 Expansion to common network structures
Proposition 3 is only applied for ReLU MLPs. In this section we extend it to widely used network
structures by introducing the corresponding matrix computation.
Pooling an unpooling layers Since the pooling layer or unpooling layer can be written as a linear
transform., e.g. average-pooling and linear interpolation, it can be denoted by y = Ax. Suppose v
is the space dimension of input region and w is the histogram of the output regions. Then we have
the following proposition.
Proposition 4. Suppose rank(A) = k, then w	clk (v).
Since clk(v) has a matrix computation form which is similar to M in Eq.13, the effect of this type
of layer on the space dimension histogram can be regarded as another matrix in Eq.13. Similarly,
we have the following proposition for Max-pooling layers.
Proposition 5. Suppose the max-pooling layer is equivalent to a k-rank maxout layer with n input
nodes and nl output nodes. Let c = (k2 - k)nl, then
W W clnι (diag {∣Y0,c∣1,..., ∣Yn,c∣l} V)	(16)
The proofs of Proposition 4, Proposition 5 are in Appendix A.1.7 and A.1.8.
Skip connection and residual structure The skip connection is very popular in current network
architecture design. When a network is equipped with a skip connection, the upper bound will be
changed. The following Proposition gives the correspondent method to compute the upper bound
with skip connection.
Proposition 6. Given a network, suppose that v is the space dimension histogram of the input
regions of the ith layer and w the histogram of the output regions of the jth layer. And we have
that w W jk=i Akv. When the input of the ith layer is concatenated to the jth layer, i.e. a skip
connection, then the computation of w will change and satisfies that w W vn |Ben |1 en where
B = Qjk=i Ak. It also has a matrix multiplication form.
Residual structures (He et al., 2016) are similar to skip connections and can be regarded as a skip
connection plus a simple full-rank square matrix computation. Therefore its effect on space dimen-
sion histogram is the same as the skip connection. Concatenation is equivalent to addition in upper
bound computation. Thus, we have the following proposition.
Proposition 7. Suppose that the residual structure adds the input of the ith layer to the out-
put of the j th layer. v, w , B have the same meaning in Proposition 6. Then we have that
w W Pvn |Ben|1 en.
The proofs of Proposition 6 and Proposition 7 are in Appendix A.1.9 and A.1.10. In addition, the
analysis for dense connections (Huang et al., 2017) is similar to skip connections and is further
discussed in Section 5.
With the above analysis, we can deal with more complex network. For example, a U-net (Ron-
neberger et al., 2015) is composed of convolutional layers, pooling layers, unpooling layers and skip
connections. If we regard convolutional layers as fully-connected layers, then we could compute the
upper bound of it (see details in Appendix A.2.3). Though in this paper we have not extend to all
possible network structures and architecture, according to Proposition 1 and by using our compu-
tational framework, it is possible to compute the change of the space dimension histogram for any
other network structure and architecture. Based on these bricks of different layers or structures, we
may derive the upper bound of much more complex and practically used networks.
4	Experiments
We perform two experiments in this work. The first one is to compare the bound proposed by us and
by Serra et al. (2018) which is illustrated by Figure 1. The upper bounds of MLPs computed and
their ratio is calculated to measure the difference. Figure1(a) and Figure 1(b) show how the ratio
6
Under review as a conference paper at ICLR 2021
「 Q ^41.7愎
1 1 O O O O
PUnoq -iəddn JO o+=ey
1 23456789 10
Layer number k
(a) n0 = 10
(b) n0 = 100
100O-1U U
III
PUnoq -iəddn JO O-Iey
(c) n0 = 10, ni = 10
Figure 1: The comparison of our upper bound and Serra et al. (2018). The y axis represents the
ratio of them which can be used to measure the difference. The MPLs are in the form of n0-ni-ni-
----ni-1 with k hidden layers. (a) Setting is no = 10, n = 6,8,10,15,20,25, k = 1,2,…，10. (b)
Setting is n0 = 10, ni = 6,8, 10, 15, 20, 25, k = 1,2, ..., 10. (c) The setting is n0 = ni = 10,k =
10, 20, ..., 100.
Table 1: The upper bound of AEs and U-nets. The channels setting means the architecture of
networks. For instance, 4-8-16-32 represents an AE which encoder has three down-sampling layers
and the channels in different layers are 4, 8, 16, 32 respectively (see details in Appendix A.3.1). The
input size in the first four experiments is (24 × 24) while the last two is (16 × 16). The upper bounds
are listed in the third and fourth column. The former corresponds to U-nets while the latter to AEs.
Besides, the ratios of two them are listed in the last column.
No.	Channels setting	w/ skip connection	w/o skip connection	ratio
1	4-8-16-32	^^5.431 X 102031^^	5.027 × 101626	1.080 × 10405
2	4-16-64-128	1.712 × 103111	3.333 × 102956	3.517 × 10154
3	4-8-16	^^9.354 × 101901 ^^	1.407 × 101643	6.648 × 10258
4	4-16-64	2.211 × 102591	3.328 × 102450	6.644 × 10140
5	8-16-64	^^3.504 × 101395^^	5.470 × 101330	6.406 × 1064
6	8-32-128	1.268 × 101642	1.230 × 101642	1.031
changes as the number of hidden layers increases from 1 to 10 with different n0. Figure 1(c) shows
how the ratio changes when the depth of the network becomes larger.
The second one is to verify the effectiveness of skip connections and residual structures on the
improvement of expressiveness, as illustrated in Table 1 and Table 2. We compute the upper bound of
auto-encoders (AE) and U-nets. AEs and correspondent U-nets have the same network architecture
except the skip connection. As for residual structures, we build two identical networks for the image
classification task with or without residual structure. For each pair of networks (with or without
one special structure), the ratio of their bounds are computed to measure how the upper bound will
be enhanced with special structure. Different network architecture settings are tried in experiments.
Because of the large memory required by complete γn,n0, we only consider networks of relatively
small sizes (network input size is 24 × 24 or 16 × 16). In addition, convolutional layers are regarded
as fully-connected layers in the computation.
5	Discussion
The proposed upper bound has been theoretically proved to be tighter than Serra et al. (2018). Fur-
thermore, Figure 1(a) and Figure 1(b) show that when ni increases from 0.6n0 to 1.5n0, the ratio
curve of two bounds moves upward. However, when ni increases to 2n0 , the curve moves down-
ward. And when ni increases further and is much larger than n0, the two bounds are the same.
This trend is related to the extent of similarity in Bn(ours) and Bn(serra). When ni < n0, Bni affects
the upper bound most. And the difference between Bn(oiurs) and Bn(sierra) is more significant when
ni decreases. When ni > n0, (Bn):,n0+1, the (n0 + 1)th column of Bn, is the main source of
7
Under review as a conference paper at ICLR 2021
Table 2: The upper bound of a simple network for the classification task with or without residual
structure. The channels setting represents where main differences in architectures are (see details in
Appendix A.3.2). ”p16” means that there is a pooling layer before the convolutional layer with 16
channels. And ”r16” represents that a residual structure is added in the convolutional layer with 16
channels. The last three columns in this table is similar to Table 1. In this part, the input size in all
the experiments is (24 × 24).
No.	Channels setting	w/ res-strUctUre	w/o res-strUctUre	ratio
1	4-p16-p16-r16-r16-r16	7.313 X 101930	3.720 × 101930	1.966
2	4-p4-4-r4-r4-r4	5.052 × 101542	3.393 × 101542	1.489
3	4-p16-p16-r16-16-16	5.232 × 101930	3.720 × 101930	1.406
4	4-p16-p16-r16-r16-r16-r16-r16	3.012 × 102277	5.882 × 102276	5.121
5	4-p16-p32-r32-r32-r32	7.180 × 102623	7.166 × 102623	1.002
difference. It is easy to find that the middle part of Bn(ours) and Bn(serra) differ most while the left and
right part are similar (see detials in Appendix A.2.3). As for the Figure 1(c), it shows the difference
between two upper bounds will be enlarged when the depth of networks increases. In general, both
theoretical analysis and experimental results show our bound is tighter than Serra et al. (2018).
The result in Table 1, Table 2 shows that special structure such as skip connection and residual
structure will enhance the upper bound. And when the residual structure is used in more layers, the
enhancement will increase (comparing comparing No.1, 3 and 4 in Table 2). However, it seems that
when the number of channels is larger, the enhancement is weaker (comparing No.1 to 2, 3 to 4,
and No.5 to No.6 in Tabel 1). Similar result can be observed in Table 2 (comparing No.1 to No.5).
These results explain why skip connection and residual structure are effective from the perspective
of expressiveness. However, there also exist some settings such that the two upper bounds are close.
We provide an explanation for the observation through the concept of partition efficiency. First of all,
we note that it is not the upper bound but the practical number of linear regions that is directly related
to the network expressiveness. Though upper bound measures the expressiveness in some extent,
there is a gap between upper bound and the real practical maximum. Actually, any upper bound
which can be computed by the framework of matrix computation, including Pascanu et al. (2013);
Montufar et al. (2014); Serra et al. (2018) etc., assume that all the output regions of the current
layer will be further divided at the same time by all the hyperplanes which the next layer implies.
However, this assumption is not always sound (see detail in Appendix A.2.4). When the number
of regions far exceeds the dimension of hyperplanes, it is hard to imagine that one hyperplane can
partition all the regions. But the special structure (e.g. skip connection) may increase the number
of regions partitioned by one hyperlane such that the practical number can be increased even if the
upper bound is not enhanced. Intuitively, skip connections increase the dimension of input space
and the partition efficiency may be higher. An good example is dense connections (Huang et al.,
2017) which is only a special case of skip connection but can further increase the dimension and
thus lead to better expressiveness; this may explain the success of DenseNets. Though we have not
completely proved the partition efficiency of skip connections, some evidences seem to confirm our
intuition, as shown in Appendix A.1.11 and A.1.12.
6	Conclusion
In this paper, we study the expressiveness of ReLU DNNs using the upper bound of number of
regions that are partitioned by the network in input space. We then provide a tighter upper bound
than previous work and propose a computational framework to compute the upper bounds of more
complex and practically more useful network structures. Our experiments verify that special network
structures (e.g. skip connection and residue structure) can enhance the upper bound. Our work
reveals that the number of linear regions is a good measurement of expressiveness, and it may guide
us to design more efficient new network architectures since our computational framework is able to
practically compute its upper bound.
8
Under review as a conference paper at ICLR 2021
References
Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep neural
networks with rectified linear units. arXiv preprint arXiv:1611.01491, 2016.
George Cybenko. Approximation by superpositions ofa sigmoidal function. Mathematics of control,
signals and systems, 2(4):303-314,1989.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Boris Hanin. Universal function approximation by deep neural nets with bounded width and relu
activations. Mathematics, 7(10):992, 2019.
Boris Hanin and David Rolnick. Complexity of linear regions in deep networks. arXiv preprint
arXiv:1901.09021, 2019a.
Boris Hanin and David Rolnick. Deep relu networks have surprisingly few activation patterns. In
Advances in Neural Information Processing Systems, pp. 359-368, 2019b.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly,
Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks
for acoustic modeling in speech recognition: The shared views of four research groups. IEEE
Signal processing magazine, 29(6):82-97, 2012.
Peter Hinz and Sara Van de Geer. A framework for the construction of upper bounds on the number
of affine linear regions of relu feed-forward neural networks. IEEE Transactions on Information
Theory, 65(11):7304-7324, 2019.
Kurt Hornik, Maxwell Stinchcombe, Halbert White, et al. Multilayer feedforward networks are
universal approximators. Neural networks, 2(5):359-366, 1989.
Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected
convolutional networks. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436-444,
2015.
Hrushikesh N Mhaskar and Tomaso Poggio. Deep vs. shallow networks: An approximation theory
perspective. Analysis and Applications, 14(06):829-848, 2016.
GUido Montufar. Notes on the number of linear regions of deep neural networks. Sampling Theory
Appl., Tallinn, Estonia, Tech. Rep, 2017.
Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear
regions of deep neural networks. In Advances in neural information processing systems, pp.
2924-2932, 2014.
Razvan Pascanu, Guido Montufar, and Yoshua Bengio. On the number of response regions of deep
feed forward networks with piece-wise linear activations. arXiv preprint arXiv:1312.6098, 2013.
9
Under review as a conference paper at ICLR 2021
Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl Dickstein. On the
expressive power of deep neural networks. In Proceedings of the 34th International Conference
on Machine Learning-Volume 70,pp. 2847-2854. JMLR. org, 2017.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedi-
cal image segmentation. In International Conference on Medical image computing and computer-
assisted intervention, pp. 234-241. Springer, 2015.
Thiago Serra and Srikumar Ramalingam. Empirical bounds on linear regions of deep rectifier net-
works. arXiv preprint arXiv:1810.03370, 2018.
Thiago Serra, Christian Tjandraatmadja, and Srikumar Ramalingam. Bounding and counting linear
regions of deep neural networks. In International Conference on Machine Learning, pp. 4558-
4566, 2018.
Dmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94:
103-114, 2017.
A Appendix
A.1 Proofs
A.1.1 The proof of Proposition 1
Proof. By Definition 6, suppose {v(1) , v(2) , . . . , v(k)} and c can be used to represent any x ∈ D
and {ai 6= 0} satisfies that ai {v(i) + c ∈ D. Let D0 = D - c, then D0 is a translation of D.
Since D is convex, D0 and f(D0) is also convex. Because f(D) = f(D0 + c) = AD0 + Ac +
b, then Sd(f (D)) = Sd(AD0). Suppose that D00 = A(D0) = {Ax|x ∈ R0}. For any y ∈
D00, there exists x ∈ D0, s.t. y = Ax. Denote Av (i) by w(i), then y can be represented by
{w(1) , w(2) , . . . , w(k)} and aiw(i) ∈ D00. From {w(1) , w(2) , . . . , w(k)} choose a set of vectors
which are linear independent and can linearly represent {w(1), w(2), . . . , w(k)}. For convenience,
suppose they are {w(1) , w(2) , . . . , w(t)}, t ≤ k. Therefore any y ∈ D00 can be represented by
{w(1), w(2), . . . , w(t)}. Thus, Sd(D00) = Sd(f (D)) = t and we have that
t = ra
nk( w
(1)
w(t)i)
rank(hw(1) . . . w(k)i)
rank(A hv(1) ... v(k)i)
≤ min{rank(A), rank( v(1)
= min{rank(A), k}.
v(k) )}
The last equality is derived from that {v(1), v(2),..., v(k)} are k linear independent vectors. □
A.1.2 The proof of Proposition 2
Proof. For any s ∈ S(h), suppose D(s) = {x|Sh(x) = s}. As long as Sd (h (D (s))) ≤
min{n0, |sh1 |1 , |sh2 |1 , ..., shL-1 1} is established, the proposition can be proved. Apparently,
Sd(D(S)) = no. Through the analysis in Section 2, h(∙) in D(S) is an affine transform. For
any x(1), x(2) ∈ D(s), W(i) (x(1)) = W(i)(x(2)), b(i)(x(1)) = b(i)(x(2)). Therefore we use
X(O) ∈ D(S) to represent them. Then h(∙) in D(S) can be written as
h(x) = W(L-1)(x(0))(... W(1)(x(0))x + b(1)(x(0))) + b(L-1)(x(0))
= W (x(0))x + b(x(0))
10
Under review as a conference paper at ICLR 2021
By Proposition 1, we have
Sd (h (D (s))) ≤ min nrank(W (x(0))), n0o
= min (n0, rank Y W(i)(x(0))
≤ min nn0, min nrank W (1) (x(0)) , . . . , rank W (L-1) (x(0))oo
≤ min{n0, |sh1 |1 , |sh2 |1 , ..., shL-11}
The last two inequality are derived from that
rank Y W(i)(x(0)) ≤ min{rank(W(1)(x(0))), ..., W(L-1)(x(0)))}
and rank(W(i)(x(0))) ≤ |shi |1.
□
A.1.3 The proof of Theorem 1
Proof. Obviously, γn,n0 satisfies the second condition. As for the first condition, consider that
hyperplanes {L1, L2, ..., Ln0} divide Rn. These hyperlanes correspond to one h ∈ RL(n, n0) and
its activation histogram is denoted by v1 . Here, we assume that L1 , L2, . . . , Ln0 are not parallel to
each other since in parallel case it is easy to imagine and verify that there exists a h0 ∈ RL(n, n0)
which satisfies Ha(Sh) Ha(Sh0). Suppose that hyperplanes {L1, L2, . . . , Ln0-1} divide Rn into
t regions {R1, R2, ..., Rt} and their activation histogram is denoted by v2. Assume that Ln0 crosses
{R1, R2, ..., Rp}, p ≤ t and divides them into 2p regions. Let v3 denote the activation histogram
of {R1 , R2, . . . , Rp}. p regions of them are active by Ln0 while other p regions are not. Part of
{Rp+1, Rp+2, ..., Rt} are active by Ln0 while the rest regions are not. Their activation histogram
are denoted as v4 and v5 respectively. Then we have the following equations:
v2 = v3 + v4 + v5	(17)
v1 = v3 + dm(v3) + dm(v4) + v5	(18)
Let us only focus on {R1.R2, ..., Rp}. Suppose {L1, L2, ..., Lm}(m ≤ n0 - 1) are borders of these
regions. {Ln0	∩	L1, Ln0	∩	L2, ..., Ln0 ∩ Lm} are hyperplanes in Ln0,	and their active directions are
projections	of	{L1, L2, ...,	Lm} in Rn. So the activation histogram of	{Ln0 ∩L1, Ln0	∩L2,	..., Ln0 ∩
Lm } in Ln0 which is denoted by v6 is equal to v3 . By the assumption that when n00 < n0, γn,n00
satisfies the bound condition, we have
v3 = v6	γn-1,m	γn-1,n0-1 .	(19)
By Eq.17, Eq.18 and Eq.19, v1 satisfies
v1	γn-1,n0-1 + dm(v3 + v4 + v5 )
= γn-1,n0-1 + dm(v2)	(20)
γn-1,n0-1 + dm(γn,n0-1)
Because of the arbitrariness of v1, we can get γn,n0 = γn-1,n0-1 + dm(γn,n0-1) which satisfies the
first condition.	□
A.1.4 The proof of Theorem 2
Proof. For 1-dimension space, its hyperlane is one point in the number axis and the activation direc-
tion is either left or right. It is apparent that n segmentation points divide one line into n + 1
parts. And for any X ∈ R, its activation number is at most n. Therefore if t < d%], then
pn=t Vi ≤ n + 1 ≤ 2(n — t) + 1. If t ≥ d%], the activation pattern of n + 1 regions in R
is denoted by {s1, s2, . . . , sn+1} (see Figure 2(a)). No matter how the activation directions of n
points are, we have |s1 | + |s%+1 | = n since |s1 | is equal to the number of left activation directions
and |s%+1 | is equal to the number of right activation direction. Another obvious conclusion is that
11
Under review as a conference paper at ICLR 2021
Sl/2  S3  S4	 S律 S律+1
1	2	3	4	n-1 n
(a) Number axis
(b) Activation number of each region
Figure 2: (a) The number axis is partitioned into n + 1 parts. si represents the ith region. (b). An
example of a partition and the activation number of each region. The abscissa axis corresponds to
the position and the vertical axis represents the number of activation
|si| - |si+1 | = ±1 as the activation patterns of neighboring regions only differ at the ith point.
Without loss of generality, let |si| ≤ dn]. Any line chart of active numbers with dotted line is under
the line chart with solid line (see Figure 2(b)). So regions with active numbers larger than or equal
th th	th th
to t are among a and b region. Apparently, the number of regions among a and b region is
2(n — t) + 1.So Pi=t Vi ≤ 2(n — t) + 1.	口
A.1.5 The proof of Eq.15
Proof. Suppose the n points of h in the number axis are p1,p2, ..., pn . Let the direction sof
Pi,…,PbnCare right and other points left. It is easy to verify that Ha(Sh) = γι,n where γι,n is
defined by
γ1,n = (0, . . . , 0, n mod 2, 2, . . . , 2, 1)>
~^{zz""^}	/
d2e-ι	b2C
Then γ1,n max {Ha (Sh) |h ∈ RL(1, n)}. Since max {Ha (Sh) |h ∈ RL(1, n)} γ1,n , γ1,n =
max {Ha (Sh) |h ∈ RL(1,n)}.	口
A.1.6 The proof of Theorem 3
Proof. Different Bnγ0 can be derived from different γn,n0. Since the clipping function keep the order
relation , every column of Bnγ0() and Bnγ0() in the same position satisfy that
Another fact is that when v	w, then |v |1 ≤ |w |1 . Here we prove that if v w, then
Mni,ni+1 v	Mni ,ni+1 w	(21)
and
Bγ(1)v	Bγ(2)w	(22)
ni	ni
Then the theorem can derived easily from Eq.21 and Eq.22.
Because Mni,ni+1v = clni+1 (v), Eq.21 is easy to verified. For Bnγ(k) , k = 1, 2, we have that
Bnγi(k)	Bnγi(k) when j1 ≤ j2 because of the second condition in Definition 10 and the
property of the clipping function. For convenience, Bnγ(k) is denoted by Bm(k,)j, k = 1, 2. By
12
Under review as a conference paper at ICLR 2021
Definition 5, we have that
∞
X B(1)v
m=M
m
∞
≤ X B(2)w
m=M
m
∞∞	∞∞
o X(XBmjVj)	≤ X(XBmjWj)
m=M j =0	m m=M j =0	m
∞∞	∞∞
o XX Bmjvj≤ XX Bmjwj
j=0	m=M	j =0	m=M
Because B:,j	B:,j	, i.e.	∀M	≥	0, Pm=M Bm,j ≤ Pm=M	Bm,j. Let aj	= Pm=M	Bm,j, bj	=
Pm∞=M Bm(2,)j, then aj ≤ bj. Because B:(,kj1)	B:(,kj)2 when j1 < j2, then
a0 ≤ a1 ≤ a2 ≤ ...	(23)
and
b0 ≤ b1 ≤ b2 ≤ ...	(24)
By the notations, we have that
∞∞	∞∞
X	X Bm(1,)j vj≤X	X Bm(2,)j wj
j =0 m=M	j=0 m=M
∞∞
o Eaj vj ≤ Ebj wj
j =0	j=0
ni	ni
o £ aj vj ≤ £ bj wj
j=0	j=0
The last equivalence is derived from that vj = wj = 0 when j > ni . Consider the left part of last
inequality. Employing v w ⇔ Pjn=i J vj ≤ Pjn=i J wj and Eq.24, the following inequality can be
derived
ni	ni	ni	ni	ni	ni
Xajvj ≤ Xbjvj =b0v0+Xbjvj ≤ b0 (Xwj -Xvj ) +Xbjvj
(ni	ni	∖ ni	/ n	n ∖ n
wj -	vj	+	bjvj = b0w0 + b1w1 + b1	wj - vj +	bjvj
1	ni	ni	ni	2	ni	ni	ni
≤ Xbjwj +b2 (Xwj -Xvj ) +Xbjvj = Xbjwj +b2 (Xwj -Xvj ) +Xbjvj
ni-1	ni
≤	bjwj + bni (wni - vni ) + bni vni = bjwj
j=0	j=0
Therefore the left part is less than the right part, i.e. Eq.22 is established and the theorem is proved.
□
A.1.7 The proof of Proposition 4
Proof. Consider any input region D. Let D0 is the corresponding output region, i.e. D0 = A(D). By
Proposition 1, Sd(D0) ≤ min{Sd(D), k}. Because
Hist({min{Sd(D), k}| D is any input region}) = clk(v)
13
Under review as a conference paper at ICLR 2021
then w	clk (v).	Suppose v ∈	Rn+1, i,e,	vi	= 0 when i >	n,	it is easy to verify that	clk (v)	=
Mn,kv, where
Mn,k ∈R(k+1)×(n+1),(M)i,j = δi,min(j,n+1)
□
A.1.8 The proof of Proposition 5
Before the proof, we show the following lemma.
Lemma 1. Suppose γn,n0 satisfies the recursion formula in Theorem 1 and the initial value satisfies
that ∣Yn1,n2∣1 = Pn= 0( n2 L then any Ynn satisfies ∣Yn,n0 |i = pn=o ( n )∙
Proof. Since Ynn = γn-1,n0-1 + dm(γn,n0-ι) and dm(∙) does not change | ∙ |i, We have
|Yn,n0 |1 = |Yn-1,n0-1|1 + |Yn,n0-1|1. By the assumption, suppose that |Yn1,n2|1 = Psn=1 0
is established When n1 ≤ n and n2 < n0 . Then
|Yn,n0|1=nX-1n0s-1+Xn	n0s-1
s=0	s=0
n
= X(Csn0-1 + Csn-0-11) + C0n0-1
s=1
n
= X Csn0 + C0n0
s=1
=Xn	ns0
s=0
Therefore any Yn,n0 satisfies the formula.
n2
s
□
It is easy to verify that |Y1,n|1 = n + 1 = Ps1=0	ns
Lemma 1, Yn,n0 proposed by us satisfies that |Yn,n0 |1
and |Y2,1 |1 = 2 = Ps2=0	1s	. By
n0
s . Next We prove Proposition 5.
Proof. Denote the maxout layer by h, according to the proof of Theorem 10 in Serra et al. (2018),
k(k-1)
one k-rank maxout layer With nι output nodes corresponds to divide one region by ----------Lnl
hyperplanes. Suppose R is one input region With Sd(R) = n0 and partitioned into p sub-regions
{r1, ..., rp}. Since one d-dimension space is at most partitioned into Psd=0 ms sub-regions by
m hyperplanes, then by Lemma 1
0
J k k(k-1)n
P≤∑t 丁
s=0
|Yn0,c|1
For any sub-region pi, Sd(pi) = n0. Since h is equivalent to an affine transform in pi With matrix
A of rank nA, We have that Sd(h(pi)) ≤ min{nA, n0} by Proposition 1. Another fact is that
nA ≤ nl. Therefore Sd(h(pi)) ≤ min{nl, n0}. That is to say, R With n0 space dimension is
divided in to at most |Yn0,c|1 sub-regions and the space dimension of each output sub-region is no
larger than min{nl , n0}. If the space dimension histogram of input regions is v (∈ Rn) , then it
is easy to verify that after the partition by h, the histogram of sub-regions v0 satisfies that v0
diag {|Y0,c|1, . . . , |Yn,c|1} v. In addition, h change their space dimension. Thus
w	clnl (v0)	clnl(diag{|Y0,c|1, . . . , |Yn,c|1}v).
Let C = diag {∣γo,c|i,..., ∣Yn,c∣ι}, then Clnl (diag {∣γo,c∣ι,..., ∣Yn,c∣ι} V) = Mn,mCv.	□
14
Under review as a conference paper at ICLR 2021
A.1.9 The proof of Proposition 6
Proof. When the skip connection is not added, w	Bv. Let v = en, i.e. the the number of the
input regions is one. Suppose the region is R ⊆ Rm and R is divided into p sub-regions. Apparently
p ≤ |Ben| and Sd(R) = Sd(r) = n ≤ m where r is one of the sub-regions. Suppose the part of
network which is from the ith layer to the jth layer is equivalent to an affine transform with matrix
C
C. Let r0 = C(r). When the skip connection is added, the output of r is I (r) denoted by r00.
Since rank CI = m, Sd(r00) ≤ min{m, Sd(r)} = n. This implies that the space dimension
of r0 may be enhanced to n. Therefore the space dimension histogram of p sub-regions wR satisfies
that
wR	|Ben|1en.	(25)
For any input region with n space dimension, Eq.25 is always established. Thus, when the space
dimension histogram of input regions is v the histogram of output regions satisfies
wXwRX|BeSd(R)|1eSd(R)=Xvn|Ben|1en.	(26)
Let C = diag{∣Be0∣ι,∣Be2∣ι,..., ∣Ben∣ι,...},then Pn Vn ∣Ben∣ι en = Cv.	□
A.1.10 The proof of Proposition 7
Proof. Any residual structure can be regarded as the composition of one skip connection and an
linear transform. For any input region r ⊆ Rm partitioned by the network, the residual structure
part has the following form.
C
I
y = Res(x) = [I I]
x, x ∈ r
C
I
By Proposition 6, the space dimension histogram of output regions
that
(r) denoted by w satisfies
w	vn |Ben|1en
Since rank([I I]) = m ≥ Sd(r), according to Proposition 1 the linear transform will not change
the histogram.	□
A.1.11	Proposition 8 and its proof
Proposition 8. For an MLP, let fm represent the first m layers (1 ≤ m ≤ l), i.e. fm(x) is the
output of the mth layer, and Fl+1 (z) = σ(W (l+1)z + b(l+1)) representing the (l + 1)th layer in the
MLP. Consider another network layer,
Gl+1(z, y) =σ W (l+1,0) yz + b(l+1,0)
where z = fl (x) ∈ Rnl , y = fm(x) ∈ Rnm, 1 < m < l. Then, given a specific Fl+1, there exists
a Gl+1 such that the the total number of regions partitioned by Gl+1 is no more less than that by
Fl+1.
Proof. For a connected set R ⊆ input space, fl (R) and fm(R) are still connected sets. For each
hyperplane Hi represented by Pjn=l 1 ai,j zj + bi = 0 in Fl+1, design corresponding hyperplane Hi0,
Pjn=l 1 ai,j zj +Pjn=m1 ci,j yj +bi = 0, in Gl+1, where ci,j = 0. Suppose Hi crosses {R1, R2, ..., Rki}
and take ki intersections {p1, p2, ..., pki } where pj is a interior point in Rj (1 ≤ j ≤ ki). Let
fl(xj) = pj, then (fl(xj), (fm(xj)) ∈ Hi0 and it is also a interior point in Rj0 which is the same
as Rj when cutting last nm dimensions. So Hi0 crosses at least ki regions which indicates the total
number of regions partitioned by G1+1 is no more less than that by Fl+ι.	□
15
Under review as a conference paper at ICLR 2021
Figure 3: The four output regions in R3
A.1.12 Proposition 9 and its proof
Proposition 9. For a three-layer MLP which has two hidden layers, i.e.
f(x) = W (3)σ(W (2)σ(W (1)x + b1) + b(2)) +b(3),	(27)
suppose that n0 = 1 and every segment partitioned by the first layer keeps their space dimension,
i.e. their output of the first layer is still segment. If the input is concatenated to the output of the
first layer (just like a skip connection), then no matter what the parameters in the first layer are, the
practical maximum number of linear regions is (n1 + 1)(n2 + 1). Specifically, without the special
structure (just like an MLP) and assume that n1 = 3, there exists some parameters in the first layer
such that the practical maximum number is no more than (n1 + 1)(n2 + 1) - n2.
Proof. Denote h1(x) = σ(W (1) x + b(1)). The first layer divides input space into n1 + 1 regions
{r1, r2, ...rn1+1}, then h1(ri) ⊆ R. Because of the skip connection, the dimension of hyperplanes
in the second layer is n1. It is apparent that for any n1 + 1 points there always exits a hyperplane
containing them. Thus, for any interior point xi ∈ ri (1 ≤ i ≤ n1 + 1), there exists a hyperplane
contaning them and therefore crossing n1 + 1 regions. We have n2 hyperplanes in the second layer,
which shows the number of linear regions is (n1 + 1)(n2 + 1). And obviously the maximum number
is not larger than it (see Theorem 7 in Serra et al. (2018))
As for the special case, take [0, 1] as input space without loss of generality. Let
1
t1	J-1
Wi = -12 , ι =	1	(28)
ɪ	-1
t3
Then we get four output regions in R3 shown in Figure 3. We can see that R1 , R2 and R3 are on
the same plane, i.e. the x-y plane. For any interior points pi ∈ Ri, i = 1, 2, 3, the only hyperplane
crossing them is x-y plane which is not able to divide R1, R2 and R3. So the practical maximum
number is no more than (ni + 1)(n + 1) - n?.	□
16
Under review as a conference paper at ICLR 2021
A.2 Examples
Here n0
A.2.1 AN EXAMPLE OF γnn,enw0 AND γnse,rnr0a
6. Then γ			ours ,n0	and γnse,rnra0		are shown as follows.									
	0	0	0	0	0	0	1		0	0	0	0	0	0	1
	0	0	0	0	1	6	6		0	0	0	0	0	6	6
	0	0	1	4	14	15	15		0	0	0	0	15	15	15
	0	2	5	16	20	20	20		0	0	0	20	20	20	20
	0	2	9	15	15	15	15		0	0	15	15	15	15	15
	0	2	6	6	6	6	6		0	6	6	6	6	6	6
	1	1	1	1	1	1	1		1	1	1	1	1	1	1
				γoU6s								serra γ∙,6			
(29)
A.2.2 An example of upper bound computation for U-net
We take the U-net in Appendix A.3.1 as example. Firstly, we use matrices to represent all layers
except skip connections. When computing upper bound convolutional layers are regarded as fully-
connected layers denoted by Ci . Suppose pooling layers are average pooling layers and unpooling
ones are filling-zero ones. They are denoted by Pi and Ui . Here, the subscript i means the order in
the network. Then according to Proposition 3 and Proposition 4 we have
C1 = B2304, C2 = B1152 , C3 = B576, C4 = B288,
C5 = B144, C6 = B288, C7 = B576, C8 = B2304
P1 = M2304,576 , P2 = M1152,288 , P3 = M576,144 ,
U1 = M144,144 = I144, U2 = M288,288 = I288, U3 = M576,576 = I576.
where B, M are defined by Proposition 3. By Proposition 6, the upper bound N is computed as
follows.
S30 = U1C5M288,144C4M144,288P3 ∈ R144×576
S3 = diag{|S30 e0|1, |S30 e2|1,..., |S30 e576|1,} ∈ R576×576
S20 = U2C6M576,288S3C3M288,576P2 ∈ R288,1152
S2 = diag{|S20 e0|1, |S20 e2|1,..., |S20 e1152|1,} ∈ R1152×1152
S10 = U1C7M1152,576S2C2M576,1152P1 ∈ R576,2304
S1 = diag{|S10 e0|1, |S10 e2|1,..., |S10 e2304|1,} ∈ R1152×1152
N = |C8S1C1M576,2304e576|1
A.2.3 THE COMPARISON OF Bnours AND Bnserra
Here we consider n = 6. By Eq.29 and the definition of clipping function, we have
1	0	0	0	0	0	1		1	0	0	0	0	0	1
0	7	0	0	1	6	6		0	7	0	0	0	6	6
0	0	22	4	14	15	15		0	0	22	0	15	15	15
0	0	0	38	20	20	20		0	0	0	42	20	20	20
0	0	0	0	22	15	15		0	0	0	0	22	15	15
0	0	0	0	0	7	6		0	0	0	0	0	7	6
0	0	0	0	0	0	1		0	0	0	0	0	0	1
			B6ours								serra B6			
A.2.4 An simple example of imperfect partition
In this part, we use a simple example (in Figure 4) to illustrate imperfect partition. Consider a
two-layer MLPs with n0 = 1, n1 = 2. The original input space is R, i.e. the number axis or one
line. The first layer partition the line into three parts (see Figure 4(a)) and the corresponding output
regions in R2 are shown in Figure 4(b). In Figure 4(c), it is easy to observe that any hyperplane can
not partition all three regions simultaneously.
17
Under review as a conference paper at ICLR 2021
(a) input regions
(b) output of the first layer (c) hyperplanes in the second layer
Figure 4: (a) The input region R is divided into three parts in three colors and activation directions
of blue points are drawn above the line; (b) The output regions of the first layer; (c) The blue lines
represent hyperplanes in the second layer
~
tensor
convolutional layer
pooling layer
unpooling layer
skip connection
Figure 5: Network architecture with skip connections in No.1 of Table 1
A.3 Network architectures
A.3.1 Network architectures in Table 1
We take the first setting in Table 1 as example. The numbers of ”4-8-16-32” correspond to one in red
color in Figure 5 and all the numbers represent the channel number of current tensor. For all setting
in Table 1, the channel numbers of input and output are 1. The kernel size in every convolutional
layer is (3 × 3) with stride = 1. We keep the size unchanged after the convolutional layer by
padding zero. We use average-pooling as pooling layers and filling-zero as unpooling layers. The
down-sampling rate and up-sampling rate are both 2. Except from the last convolutional layer,
ReLU is added in other ones. The only differences in setting are channel numbers and the depth of
down-sampling.
A.3.2 Network architectures in Table 2
We also take the first setting in Table 2 as example. The numbers of ”4-p16-p16-r16-r16-r16”
correspond to one in red color in Figure 6. ”p” means that before the convolutional layer, there
exists a pooling layer. The last part of network is three fully-connected layers and ReLU is not
added in the final layer. Other setting is the same as Appendix A.3.1.
18
Under review as a conference paper at ICLR 2021
1	4
convolutional layer
pooling layer
fully-connected layer
residual structure
Figure 6: Network architecture with residual structures in No.1 of Table 2
19