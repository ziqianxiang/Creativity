Under review as a conference paper at ICLR 2021
Greedy Multi-Step Off-Policy
Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
This paper presents a novel multi-step reinforcement learning algorithms, named
Greedy Multi-Step Value Iteration (GM-VI), under the off-policy setting. GM-VI
iteratively approximates the optimal value function using a newly proposed multi-
step bootstrapping technique, in which the step size is adaptively adjusted along
each trajectory according to a greedy principle. With the improved multi-step in-
formation propagation mechanism, we show that the resulted VI process is capa-
ble of safely learning from arbitrary behavior policy without additional off-policy
correction. We further analyze the theoretical properties of the corresponding op-
erator, showing that it is able to converge to globally optimal value function, with
a rate faster than the traditional Bellman Optimality Operator. Experiments reveal
that the proposed method is reliable, easy to implement, and achieves state-of-the-
art performance on a series of standard benchmark datasets.
1 Introduction
Multi-step reinforcement learning (RL) is a set of methods that are capable of flexibly adjusting the
trade-off between the observed multi-step return and the estimated future return so as to meet the
demands of a particular task. Recent advances on multi-step RL have achieved remarkable empirical
success (Horgan et al., 2018; Barth-Maron et al., 2018). However, one major challenge of multi-
step RL comes from how to achieve the right balance between the two terms. Such balance can
be regarded as a kind of data-knowledge trade-off in some sense, as the future return is estimated
through a value function representing the knowledge learnt so far. Particularly, a large step size tends
to quickly propagate the information in the data, while a small one relies more on the estimation
based on the learnt knowledge. Classical solution to address this issue is to impose a fixed prior
distribution over every possible step size (Sutton & Barto, 2018; Schulman et al., 2016), ignoring
the quality of data and on-going knowledge which dynamically improves over the learning process.
Unfortunately, such a prior distribution has to be tuned case by case.
Another issue related to multi-step RL is off-policy learning, i.e., its capability to learn from data
from other behavior policies. Previous research on this is mainly conducted under the umbrella
of Policy Iteration (PI) (Sutton & Barto, 2018), with the goal to evaluate the value of a given target
policy (Precup, 2000; Harutyunyan et al., 2016; Munos et al., 2016; Sutton & Barto, 2018; Schulman
et al., 2016). Despite their success, those methods usually suffer from certain undesired side effects
of off-policy learning, e.g., high variance due to the product of importance sampling (IS) ratios, and
the restrictive premise of being able to access both behavior policy and the target policy (to compute
the IS ratios). Most importantly, those methods also require the aforementioned prior distribution on
step size, which usually need to be tuned case by case, e.g., TD(λ) (Sutton & Barto, 2018), GAE(λ)
(Schulman et al., 2016).
In contrast with PI, Value Iteration (VI) methods propagate the value of the most promising action
to approximate the optimal value function, without the need of policy evaluation (Sutton & Barto,
2018; Szepesvari, 2010). These characteristics of VI make it somewhat unnatural for multi-step
learning — theoretically, this means that it has to search over the whole trajectory space to find one
which achieves the highest (multi-step) return. For a method like this, the good side is that it can
safely use data from any behavior policy without any off-policy correction (as no policy evaluation
is needed), but at the cost of unrealistic computational burden. As mentioned in the next section,
very few research (Horgan et al., 2018; Barth-Maron et al., 2018) in literatures address these issues.
1
Under review as a conference paper at ICLR 2021
In this paper, we propose a novel method named GM-VI that adopts a multi-step style scheme to iter-
atively approximates the optimal value function. The core idea of our method is to greedily chooses
the largest value among various-step bootstrapping estimation, so as to approximate optimal value
as quickly as possible. This greedy principle essentially allows us to adjust the step size adaptively
during multi-step learning according to the quality of the trajectory data — a higher-quality trajec-
tory data leads to a larger chosen step size compared to a lower-quality one. Furthermore, GM-VI
naturally allows safely using data from any behavior policy without additional correction, while
freely using multi-step data.
The key to the success of the proposed GM-VI method is a novel multi-step Bellman Optimality Op-
erator, which is an extension to its classical one-step counterpart (Sutton & Barto, 2018; Szepesvari,
2010). We analyze the theoretical properties of this operator, showing that it is able to converge to
globally optimal value function, with a rate faster than the traditional Optimal Bellman Operator. As
a concrete implementation of this operator, we propose a novel algorithm named Greedy Multi-Step
Q Learning, showing that it achieves state-of-the-art performance on standard benchmark tasks.
2 Preliminaries
We begin with discussion on Value Iteration (VI) approach and Policy Iteration (PI) approach, which
are two fundamental approaches of RL.
2.1	Markov Decision Processes
A Markov Decision Processes (MDP) is described by the tuple (S, A, T, r, γ). S is the state space,
A the action space, γ ∈ (0, 1) is the discount factor; T is the transition function mapping s, a ∈
S × A to distributions overS; r is the reward function mapping s, a to distribution over R, which we
will use r(∙∣s, a) for stochastic case and r(s, a) for deterministic case. The trajectory by executing
policy π for N steps after executing action at at state st is defined as
τNst,,πat , rt, st+1, at+1, rt+1, st+2, at+2, . . . , st+N, at+N,
where r 〜 r(∙∣st,at),st+ι 〜 T(∙∣st,at),at+ι 〜 π(∙∣st+ι). The return is defined as the accu-
mulated reward Rtγ = Pn∞=0 γnrt+n. The value function of a policy π is defined as the expected
return Qπ(s, a) , E st,at [Rtγ |st = s, at = a, π]. Value-based RL methods aims to approximate
∞,π
the optimal value function Q*(s,a) = max∏ Qn (s,a). The policy outputted by the algorithm is
behaved according to the estimated value function Q, i.e., πQ(s) = arg maxa Q(s, a) .
2.2	Value Iteration
VI approach aims to approximate the optimal value function by back-propagating the optimal value.
One-Step VI. One-step VI back-propagates the optimal value on each state to its father state-action
step by step, represented by Q learning (Watkins & Dayan, 1992). Formally, the value function
is updated by the One-Step Bellman Optimality Operator (we will briefly use one-step optimality
operator)
(B1q)(s, a) , Ert,st+1 rt + γ max q(st+1, at+1) st = s, at = a	(1)
at+1
Such step-by-step back-propagation is often not efficient especially when the trajectory has a long
horizon. As we will show in Section 3, such step-by-step back-propagation is not necessarily the best
choice and it can be accelerated. Besides, when using function approximator for value function, the
approximation error may be accumulated across the long-horizon back-propagation (Hasselt, 2010;
Van Hasselt et al., 2015).
Multi-Step VI. One naive way to improve the propagation efficiency is to use multi-step bootstrap-
ping (Horgan et al., 2018; Barth-Maron et al., 2018). The value function is updated by the Multi-Step
Bellman Optimality Operator (multi-step optimality operator)
(BNq)(s,a) , e∏~p,τNt,∏at
N-1
γnrt+n + γN max q(st+N, at+N)
at+N
n=0
st = s, at = a, P
(2)
2
Under review as a conference paper at ICLR 2021
where P(π) is the probability of choosing a behavior policy π, together with the trajectory τNst,,πat,
formalizing sampling experience of different policies from the replay buffer. Recent advances have
shown promising results in practice. This implies that multi-step bootstrapping can often benefit
learning. However, it requires much effort and prior knowledge to tune the step size N . Further-
more, the final Q value function is highly dependent on the behavior policies and generally can not
converge to global optimal value function (as we will show in Theorem 2).
2.3	Policy Iteration
Policy iteration approach iteratively performs policy evaluation and policy improvement, which nat-
urally allow multi-step learning. At the policy evaluation phase, the algorithm evaluates the value
function Qπ of the current policy π.
Multi-Step On-Policy Evaluation. On-policy methods abandon the off-policy data which may
incur instability (Tesauro, 1995; Boyan, 1999). The value function is updated by the Multi-Step
On-Policy Bellman Operator
N-1
(BsπNq)(s, a) , Eτst,at	γnrt+n + γNq(st+N , at+N) st = s, at = a, π	(3)
,	n=0
Classical implementation includes: (1) one-step methods: SARSA, Expected SARSA (Sutton &
Barto, 2018), etc.; (2) infinity-step methods: Monte Carlo (Sutton & Barto, 2018); (3) trade-off:
TD(λ) (Sutton & Barto, 2018), GAE(λ) (Schulman et al., 2016), which balance the trade-off by a
discount factor λ. The corresponding operator is defined as B3 * sπλ , (1 - λ) PN≥1 λN BsπN, which
assign exponentially-decay weight as N becomes larger. Roughly, the parameter λ represents the
prior knowledge or our bias on the step size of bootstrap. Unfortunately, this parameter has to be
tuned in a case-by-case manner.
Multi-Step Off-Policy Evaluation. A general idea to employ off-policy data is to set correction
operation. One classical method is to use importance sampling (IS) with Multi-Step Off-Policy
Bellman Operator,
N-1	st = s
(BN,∏q)(s, a) , E∏0〜P,τst,at	X γnζt+nrt+n + YN/Nq(st+N, at+N) at = a (4)
N,π0 n=0	P,π
where Zt+:，Qt+t+1 ∏0(a"st?) is the importance sampling (IS) ratio. The multiple product
terms of IS make the value suffer from high variance, and has motivated further variance reduction
techniques, e.g., TB(λ) (Precup, 2000), Q(λ) (Harutyunyan et al., 2016), Retrace(λ) (Munos et al.,
2016). For these methods, we need to know not only the trajectory but also the behavior policy, i.e.,
the likelihoods of choosing the behavior action of the behavior policy, ∏0(at∣st).
3	Greedy Multi-Step Value Iteration
The general idea of our approach aims to approximate the optimal value function, while adaptively
adjusting the step size by the quality of the trajectory data. The novel Greedy Multi-Step Bellman
Optimality Operator (GM-optimality operator) is defined as
M-1
(GPq)(s, a) , En〜P,τN,,a
max
1≤M ≤N
γnrt+n + γM
n=0
st=s,
max q(st+M, at+M) at = a,
at+M	P
(5)
Note that one-step GM-optimality operator GP1 is equivalent to one-step optimality operator B1
(eq. 1). One way to understand GM-optimality operator is from a forward view of value update.
As shown in Figure 1, one-step optimality operator looks forward for one step, while multi-step
optimality operator looks forward for several steps and take the corresponding bootstrap value. Our
GM-optimality operator also looks forward for multiple steps but greedily chooses the optimal one
among all these bootstrap values.
3
Under review as a conference paper at ICLR 2021
Figure 1: Backup diagram of (a) One-Step
Optimality Operator; (b) Multi-Step Optimal-
ity Operator; (c) Greedy Multi-Step Optimality
Operator.
max
(C)
Algorithm 1 Greedy Multi-Step Q Learning
Input: Initialized Q(0)
1:	Initialize buffer B = {}, iteration k = 1.
2:	repeat
3:	Execute policy with Q(k) (e.g., -greedy) and obtain
a trajectory τ
4:	for (st , at ) in τ do
5:	Put (st, at, τNst,at ) into buffer B
6:	end for
7:	for i=1, M do
8:	Sample (st, at, τNst,at ) from B.
9:	Update Q(k+1) with (st, at, τNst,at) and Q(k)
10:	end for
11:	k=k+1
12:	until
One question readers may concern is that why it can perform off-policy learning from arbitrary
behavior policy without any correction? The essential reason is that our goal is to estimate the
optimal value function but not the value function of a specific policy. This implies that no policy
evaluation is needed and thus naturally no correction is involved. From a technical view, given a
trajectory τNst,,πat , rt,st+1,at+1,rt+1,st+2,at+2, . . . ,st+N,at+N,ifitbrings a high future return,
then a long horizon along the trajectory is utilized. Otherwise if the trajectory is worse than it
has explored before, we can still use an extremely short-step bootstrap, i.e., one-step bootstrap
rt + maxa Q(st+1 , a), which only involve the (st, at, rt, st+1) information from the environment
but not concern about a specific policy.
The corresponding implementation of this operator is named Greedy Multi-Step Q Learning (GM-Q
learning). Given a trajectory τ and a value function Q(k), the target value is computed by
M-1
Q(tτar,kg)et(s,a) = max X γnrt(+τ)n +γM max Q(k) (st(τ+)M, at+M)
1≤M ≤N	at+M
n=0
(6)
One can use the average over all samples, Q(k+1')(s, a)一 6 1,a∣ PT∈τs,a QtTrget(s, a) With all ex-
perience state-action pair (s, a), where τs,a , {τs,a} is the set of truncated trajectories starting from
s, a; or use temporal difference Q(k+1) (s, a) — Q(k) (s, a) + α (Q(Trget (s, a) — Q(k) (s, α)) ； or use
mean square error minq PT kq(s, a) - qt(aTr,kg)et (s, a)k2 for parametrized function q. By replacing Line
9 in Algorithm 1 With one of these equations, one can obtain the GM-Q learning algorithm.
(a) Grid World Problem	(b) Iteration 1	(c) Iteration 2
Figure 2: Update process of GM-Q learning With N = ∞. The Q value With purple background
means that the value is changed at corresponding iteration, While one With the yelloW background
means that the value is unchanged. γ is the discount factor.
4
Under review as a conference paper at ICLR 2021
To gain more insight on how GM-Q learning works, let us simulate the update process of GM-Q
learning on a small grid world example. Figure 2a depicts a grid world problem. The agent needs to
find a shorted path from START state to GOAL state. The reward is 1 if the agent reach the GOAL
state otherwise 0; the state is defined as s = [row, col], e.g., for GOAL state s = [0, 2]; the action
space A = {—, →, ↑, 1}. There are two existing trajectories. The Q values are initialized with 0
for all state-actions. At each iteration, the Q function is update for each state-action in the buffer.
At iteration 1, GM-Q learning chooses the longest horizon until the end on all the state-actions.
For example, as shown in Figure 2b, the Q([0,0], 1) is updated by trajectory τι until the end, i.e.,
r([0, 0],。+ γr([1, 0], →)+ Y 2r([1,1], ↑)+ γ3r([0,1], →)+ Y4 max。Q([0, 2], a) = γ3. At iteration
2, For example, the one-step bootstrap value for ([0, 0], →) is larger than any other bootstrap values.
Therefore, we have Q([0, 0], →) = r([0, 0], →) + Y maxa Q([0, 1], a) = Y. We also provide a
simulation on how other algorithms behave in Appendix B. GM-Q learning only require 2 iterations
to finish updating (while one-step Q learning requires 4) and it finally obtains the optimal policy
(while multi-step Q learning gets a sub-optimal one, shown in Appendix Figure 10a).
4 Theoretical analysis
In this section, we analyze the convergence and the corresponding speed of GM-optimality operator
GPN , comparing with the existing operators like multi-step optimality operator BPN . We consider
finite state S and action spaces A.
4.1 Convergence
First, we show the convergence property of GM-optimality operator GPN (eq. 5), beginning with the
contraction property.
Lemma 1 (contraction of GM-optimality operator) For any P, given any value function q, q0 ∈
R|S||A|, N ≥ 1, kGPNq-GPNq0k≤Ykq-q0k.
We provide all the proofs in Appendix A. Throughout, we write ∣∣∙k for supremum norm. Here Y is
the contraction rate of GPN. Note that this is a worst-case contraction rate of GPN, a more detailed
analysis will be given in the next section.
Lemma 2 For any P, N ≥ 1, given any value function q ∈ R|S||A|, Qπ ≤ GPNQπ.
By the lemmas above, it,s easy to obtain that GM-OPtimaIity operator will converge to Q*.
Theorem 1	(the fix point of GM-optimality operator; Greedy Multi-Step Bellman Optimality Equa-
tion) For any P and N ≥ 1, GPN Q* = Q* .
Proof: Let Qb* denote the fix point of GPN (it has a fix point by Lemma 1). For any policy π, by
applying GP, we have Qn ≤ GPQn ≤ (GP)2Qπ ≤ … =Q*, which implies Q* satisfies the
definition of Q*.
We have finished the convergence property of GM-optimality operator GPP . Another problem worth
concerning is the convergence property of multi-step optimality operator BPP (eq. 2).
Lemma 3 (contraction of multi-step optimality operator BPP) For any two vectors q, q0 ∈ R|S||A|,
N ≥ 1, ∣BPPq-BPPq0∣≤YP∣q-q0∣.
Theorem 2	(the fix point of multi-step optimality operator BPP) Let Q*BN denote the fix point of BPP,
i.e., BPP Q*BN = Q*BN.
1)	When N = 1, Q*B1 = Q*, which means B1Q* = Q* (known as Bellman Optimality Equation).
2)	For any N ≥ 2, QBN ≤ Q*. If and only if P satisfies for any π ∈ {π∣P (π) > 0} we have
π(s) = arg maxa Q*(s, a), then Q*BN = Q*.
5
Under review as a conference paper at ICLR 2021
The theorems above imply that multi-step optimality operator BPN (N ≥ 2) converges faster than
one-step optimality operator B1 does. However, multi-step optimality operator BPN generally con-
verges to a suboptimal value function — converging to optima only happens when all the behavior
policies are optimal, which is almost never guaranteed in practice. To best of our knowledge, this
the first time giving this formal statement on the sub-optimal convergence of multi-step optimality
operator.
4.2 Convergence Speed
In this section, we compare the convergence speed of GM-optimality operator GPN and one-step
optimality operator B1 , as both these two operators converge to the optimal value function (while
multi-step optimality operator BPN cannot).
First, we give a basis on understanding why GM-optimality operator GPN can generally converge
faster. GM-optimality operator GPN chooses the maximum one among various-step bootstrap values,
while one-step optimality operator B1 only uses the one-step value. It’s obvious that GPNq ≥ B1q. If
We additionally set a proper initial condition on the value function q, i.e., q ≤ Q*, then GPq tends
to get closer to Q* than B1q.
Lemma4 GP q ≥ B1 q. If q ≤ Q* ,then IlGN q - Q*k≤ ∣∣B1q - Q*∣∣.
Then, We give a general result that: With same given iteration, GM-optimality operator GPN tends
to get closer to optimal value function than one-step optimality operator B1 does. Let {Q(kN) } and
GP
denote the sequence by applying GM-optimality operator GPP starting from Q(0), i.e., Q(GkN+1) =
GPP Q(GkN) (Q(G0N) = Q(0)); and similarly {Q(BkN) } is the one by applying BPP.
Theorem 3 If Q(0) ≤ Q*, then we have IQ(GkN) - Q* I≤ IQ(Bk1) - Q* I for any N ≥ 1, k ≥ 1.
Next, We give a result under an ideal case: if all the behavior policies are the optimal ones, then
the contraction rate of BPP is γP and We can obtain the optimal value function by applying GM-
optimality operator GP∞ only once for any q ≤ Q*.
Theorem 4	Iffor any π ∈ {π∣P(π) > 0} π(s) = argmax。Q*(s, a) and q ≤ Q*, then ∣∣Gp q 一
Q*I≤ γPIq - Q*I. Specially, for N = ∞, GP∞q = Q*.
Finally, We give a more practical result that: With some relaxed conditions, the GM-optimality
operator GPP has a contraction rate of γ2 . We introduce a condition that the behavior policy output
the optimal action on specific states.
Condition 1 (condition on the distribution of behavior policy P and the value function q) Given
P and q ∈ R|S||A|, for any π ∈ {π |P (π) > 0} satisfy the following condition: for any s which
satisfies mina B1q(s, a) - maxa0 Q* (s, a0) > γ ∣q - Q* ∣∞ , π(s) = arg maxa Q* (s, a).
Theorem 5	(faster contraction rate with special condition for GM-optimality operator GPP) If q ≤
Q* and distribution of behavior policy P satisfies Condition 1, then for any value function q ∈
R|S||A|, N ≥ 2, ∣GPPq-GPPQ*∣= ∣GPP q - Q*∣≤ γ2∣q - Q*∣.
Above all, our GM-optimality operator GPP can converge to the optimal value function With a rate
of γ (or γ, γP With some condition). It does not require off-policy correction, and only requires the
access to the trajectory data While not necessary the behavior policy π (i.e., the policy distribution
∏(∙∣s)). Furthermore, it can adaptively adjust the step size by the quality of the trajectory data.
To best of our knoWledge, none of the existing operators oWn all of these properties together. We
summarize the properties of all the referred operators in Table 1.
6
Under review as a conference paper at ICLR 2021
Operator	Converge to	Contraction Rate	NOT requiring off-policy correction	NOT requiring knowing policy	Support adaptively adjusting step size
One-Step Optimal Operator B1 (eq. 1)	Q*	γ	X	X	×
Multi-Step Optimal Operator BPN (eq. 2)	QBN(≤ Q*) BP	γN	X	X	×
Multi-Step On/Off-Policy Operator (eq. 3, 4)	Qπ	γN	×	×	×
Greedy Multi-Step Optimal Operator GPN	Q*	γ (γ2 , γN with	X	X	X
(eq. 5)		condition)			
Table 1: Properties of the operators.
5 Experiment
We designed our experiments to investigate the following questions. 1) What are the performance
characteristic of GM-Q learning in sample efficiency and reward? 2) Is multi-step learning neces-
sary? Besides, as GM-Q learning adaptively choose the step size, how large step size will GM-Q
learning choose? 3) How will the initialization of value function affect the convergence speed, as
the analytical results we have given in Section 4.2?
5.1	Experimental Setup
For our GM-Q learning algorithm, we use N = ∞ (eq. 6) for all tasks, which means that it will
check all the bootstrapping values with different step sizes until the end of the trajectory.
The following algorithms are compared. (a) Q learning: a classical algorithm Sutton & Barto (2018).
(e) Multi-Step Q learning: a vanilla multi-step version of Q learning without any off-policy correc-
tion, which has shown promising result in practice (Horgan et al., 2018; Barth-Maron et al., 2018).
(f) SARSA: a classical one-step on-policy algorithm (Sutton & Barto, 2018). (g) Retrace(λ): a state-
of-the-art off-policy algorithm by clipping the importance sampling ratio. All the proposed methods
adopt the same implementations to ensure that the differences are due to the algorithm changes
instead of the implementations. The implementation detail is provided in Appendix C.
The algorithms are evaluated on benchmark tasks implemented in OpenAI Gym (Brockman et al.,
2016). Each algorithm was run with 10 random seeds. The trained policies are evaluated after
sampling every 600 timesteps data.
5.2	Performance
Figure 3 shows the performance of the algorithms. GM-Q learning outperforms the classical and the
state-of-the-art algorithms in both reward and sample efficiency on all the tasks. For example, on
the mountain car task, GM-Q learning reaches a reward of -160 within 2 × 104 timesteps. While
multi-step Q learning, which performs best among remaining tasks, achieves a reward -160 with
twice the timesteps of GM-Q learning. The common characteristic of these two tasks is that the
reward is sparse. For example, the mountain car task will provide a reward only when the car drives
up the mountain on the right (while the car’s engine is not strong enough to scale the mountain in a
single pass). GM-Q learning outperforms other algorithms on these sparse reward tasks.
MountalnCar	Acrobot -IM-   GM-QLrarrtr>s	一刈. CMgLeaπin9 用一二X‰n,	XxvʌλZ∖	1 	SARSA	/ 'PT ' W	-250	SMSA -UO-   Remκew>	/		 RMτκe<A)	ʌ-ʌʌʌ ʌ 1Ξ 二:， 01234567β	1	2	3	4	5 τ*∏eβwpβ(χW∙>	τυ∏eβwPe(X w⅛ Figure 3: Episode rewards of the algorithms av- eraged over 10 random seeds. The shaded area corresponds to 40% confidence intervals.	MountaInCar	Acrobot --∣=≡,-	Z	ʌ	-~]	=	≡-- -iaα∙	*-SWpQtasmh9	X^Λ∙√v^W∖ J	∖	4-SWpQLesrrtn9 &SHpQlevnMg	/ v^ZW	-250	β⅞tφQLevτin9 -UO-	IfreepQtaarrtns	(		IS-SttpQUsrrtne	ʌ^ʌ-^ʌ Λ	I ，院 01234567«	1	2	3	4	5 Unwtφs{xlβ⅞	iυ∏eβwPe(X W⅛ Figure 4: Episode rewards of multi-step Q- learning with different step sizes.
7
Under review as a conference paper at ICLR 2021
MountamCar
1Λ-P
012345«7
-∏meswps(xiα∙)
B6βUMJf
««»-
0.030-
««z$-
O∙QZβ-
««13-
QOlQ-
0.<X»-
r COOO-
«
Figure 5: (Left) The chosen step size of GM-Q learning during
training process. (Right) The distribution of the chosen step size
over all data, in which the percentage of step size 1 is 77%.
MountaInCar
0*m"-S
012345678
^∏mertws{xlθ41
Figure 6: Episode rewards of
GM-Q learning with different
initialized Q values.
5.3 Discussion on the Components of GM-Q learning
Multi-Step Learning. As shown in Figure 4, with proper step size setting, multi-step Q learning
performs better than one-step Q learning. For 16-step Q learning (purple dashed line), although it
failed to achieve a higher score at the end, it reach a higher reward than other-step Q learning at
the beginning (1 × 104 timestep). These results implies the efficiency of long-horizon information
propagation of multi-step learning. However, vanilla multi-step highly depends on the behavior
policy, restricting it from achieving further success.
Adaptive Adjustment of Step Size. As shown in Figure 4, multi-step Q learning performs better
with step size 8 on the mountain car task, while step size 4 on the acrobot task. It requires parameter
tuning for each task. Furthermore, the step sizes for multi-step Q learning is fixed for each state-
action pair during the learning process, while those of our GM-Q learning is adaptively adjusted. As
shown in Figure 5 (Left), at the later stage (after 2 × 104 timesteps), the chosen step sizes increase as
the quality of data improves (shown by the increased score of GM-Q learning in Figure 3). Figure 5
(Right) plots the distribution of the chosen step size. Larger step sizes are less chosen than the
smaller ones.
Initialization of the Value Function. As we have stated in Section 4.2, the initialization of Q
value can affect the performance of GM-Q learning. We experiment with different initialized Q
values by setting the parameter of DNN. As shown in Figure 6, with smaller initialized value (-10
or -5), GM-Q learning can achieve a high score but require relatively more iterations. While larger
initialized value leads to a poor final score. Future work on proper initialization of value function
needs studying.
6 Conclusion
In this work, we introduce a new multi-step Bellman Optimality Operator, which can be used to
approximate the optimal value function Q* with a sequence of monotonically improved Q-functions.
When applying it for value function updating, the proposed operator has several advantages:1) it is
able to adjust the step size adaptively during learning according to the quality of the trajectory data,
with no task-specific hyperparameters; 2) like its one-step version, it supports learning from off-
policy samples while no need for off-policy correction, hence will not suffer from the issues related
to that, such as high variance; 3) it has guaranteed convergence with a faster rate. The feasibility
and effectiveness of the proposed method has been demonstrated on a series of standard benchmark
datasets with promising results.
References
Gabriel Barth-Maron, Matthew W Hoffman, David Budden, Will Dabney, Dan Horgan, Dhruva Tb,
Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. Distributed distributional deterministic
policy gradients. arXiv preprint arXiv:1804.08617, 2018.
Justin A Boyan. Least-squares temporal difference learning. In ICML, pp. 49-56. Citeseer, 1999.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016. cite arxiv:1606.01540.
8
Under review as a conference paper at ICLR 2021
Anna Harutyunyan, Marc G Bellemare, Tom StePleton, and Remi Munos. Q (λ) with off-policy
corrections. In International Conference on Algorithmic Learning Theory, pp. 305-320. Springer,
2016.
Hado V Hasselt. Double q-learning. In Advances in neural information processing systems, pp.
2613-2621, 2010.
Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado Van Hasselt,
and David Silver. Distributed prioritized experience replay. arXiv preprint arXiv:1803.00933,
2018.
Remi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare. Safe and efficient off-policy
reinforcement learning. In Advances in Neural Information Processing Systems, pp. 1054-1062,
2016.
Doina Precup. Eligibility traces for off-policy policy evaluation. Computer Science Department
Faculty Publication Series, pp. 80, 2000.
John Schulman, Philipp Moritz, Sergey Levine, Michael I Jordan, and Pieter Abbeel. High-
dimensional continuous control using generalized advantage estimation. international conference
on learning representations, 2016.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Csaba Szepesvari. Algorithms for reinforcement learning. Synthesis lectures on artificial intelli-
gence and machine learning, 4(1):1-103, 2010.
Gerald Tesauro. Temporal difference learning and td-gammon. Communications of the ACM, 38(3):
58-68, 1995.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. arXiv preprint arXiv:1509.06461, 2015.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279-292, 1992.
A Theorem Proofs
Proof of Lemma 1:
GPNq-GPNq0
max
st ,at
En 〜P ,τN∏
≤ max E
st,at
max PnM=-01 γnrt+n +γM maxq(st+M,at+M)
1≤M ≤N	at+M
En 〜P,τN,∏at
max PnM=-01γnrt+n +γM maxq0 (st+M, at+M)
1≤M ≤N	at+M
∏ 〜PF,
max PnM=-01 γnrt+n + γM maxq (st+M, at+M)
1≤M ≤N	at+M
max PnM=-01γnrt+n +γM max q0 (st+M, at+M)
1≤M ≤N	at+M
≤ max E
st,at
…NT ι≤Ma≤N
PnM=-01 γnrt+n + γM maxq (st+M, at+M)
at+M
PnM=-01 γnrt+n + γM max q0 (st+M, at+M)
at+M
≤ max En〜P Tst,at max YM max q (St+M,at+M) - maχ q (St+M,at+M)
st,at	, N,π 1≤M≤N	at+M	at+M
≤ max γM
1≤M ≤N
≤ max γM
1≤M ≤N
= max γM
1≤M ≤N
= γ kq - q0k
max
st+N ,at+M
max
st+N ,at+M
kq-q0k
max q (St+M, at+M) - max q0 (St+M, at+M)
at+M	at+M
q (St+M, at+M) - q0 (St+M, at+M)|
9
Under review as a conference paper at ICLR 2021

Proof of Lemma 2:
For any st , at , we have
GPNQπ (st,at)
=Eπ~P,τ st,at	maxχ rPM=01 γnrt+n + YM maχ Qn(St+M ,at+M )
N,π 1≤M ≤N n	at+M
≥ Est+1 rt + γ max Qπ (St+1, at+1)
at+1
≥ Est+1,at+1 [rt +γQπ (St+1,at+1)]
= Qπ (St, at)
Proof of Lemma 3: The proof similar to the one of Lemma 1.
Proof of Theorem 2: (sketch) As 1) is already known, we proof 2).
It's obvious that QBN ≤ B1QBn ≤ (B1)2 QBN ≤∙∙∙ = Q*.
If there exists one behavior policy that does not output optimal action, then there exist at least one
S, a such that Q*BN (S, a) < B1Q*BN (S, a) ≤ Q*(S,a).
Proof of Lemma 4: First, we prove that for any St , at
GPNq (St, at)
=Eπ~P ,τ 特,at m maxχ T Pn=01 Ynrt+n + YM maχ q (st+M, at+M )
N,π	1≤M ≤N	at+M
≥ Est+1 rt + Y maxq (st+1, at+1)
at+1
= B1q (st, at)
Ifq ≤ Q*, then GPN q ≤ GPN Q* = Q* andB1q ≤ B1Q* = Q*.
Let s0, a0	= arg maxs0,a0	GPN q(s0, a0)	- Q*(s0, a0), we have	kGPN q	-	Q* k=	|GPN q(s0, a0)	-
Q*(s0,a0)∣≤ ∣B1q(s0,a0)'-Q*(s0,a0)∣ ≤ maXs,α∣B1q(s,a) - Q*(s,a)∣ = ∣∣B1q - Q*k
Proof of Theorem 3: Similar to the proof in Lemma 4, if q ≥ q0, then GPNq ≥ B1q. Therefore, we
have Q(GkN) ≥ Q(Bk1) with the same initial Q(0). Then similar to the proof in Lemma 4, we complete
the proof.
10
Under review as a conference paper at ICLR 2021
Proof of Theorem 4: Let P * denote the distribution of behavior policy which satisfies for any
π ∈ {π∣P*(π) > 0} π(s) = argmaXa Q*(s, a).
IlGNq-Q*∣∣
maX
st,at
Eπ 〜P*,τ st,at	max r∑M=01 Y nrt+n + YM max q (st+M ,at+M )
,N,π	1≤M≤N	at+M
-Eπ 〜P*,τ st，at Pn=01 Ynrt+n + YN max Q* (St+N ,at+N )] |
'N,π	a++N
≤ max E∏〜p* T st,at
-st,at	, N,π
PN-01 Ynrt+n + YN max q (st+N, at”)
at+N
PN-01 Ynrt+n + YN max Q* (st+N, %n)
≤ maxE∏〜p* Tst,at1 ( PN01 Ynrt+n + YN max q (st+N,网”))
st,at	, N,π 1 ∖	at+N	/
PN-01 Ynrt+n + Yn max Q* (st+N, at+N)
En 〜P*,τN,∏at
=max E∏ 〜p* T st,at yn max q (st+N,at+N) - max Q* (st+N ,at+N)
st,at	, N,π	at+N	at+N
≤ YN Ilq- Q*k
Proof of Theorem 5:
IIGN q - q* Il
= max
st,at
≤ max
st,at
max
st ,at
E∏~p ,τN,Πt
En 〜P "NF
En〜P ,琮；；
max PM-II Ynrt+n + YM max q (st+M, at+M) - Q* (st, at)
1≤M ≤N	'Lu	at+M	，
PLYnrt+n+Y 2 maxq (Stso+)
pn=0 Ynrt+n + Y2 max q (st+2,at+2)
at + 2
- Q* (st, at)
-E∏~p UeY m+x q* (St+1,at+1)
□
≤ Y max	En〜P TSt+I,at+1 rt+1 + Y max q (st+2, at+2) - max Q* (st+1, at+1)
st+1,at+1	",/ N,∏	at+2	at+1
Y max EmP ,「Nt：
rt + Y max q (st+ι, at+ι) - maxQ* (st, at)
at+1	at
For any St which satisfies | min° BIq(St, a) - max。，Q*(st, a0) | > y ∣∣q - Q* ∣∣∞ , we have already
assumed π(s) = argmaXa Q*(s, a) (Condition 1). The for those st, we have
E∏〜P τst,at rt + γ max q (st+1,at+1) - max Q* (st,at)1
,N,κ	at+1	t	|
=E∏〜P τst,at Yt + Y maxq (st+1,at+1) - Irt — Y max Q* (st+1,at+1)
,N,π [	at+1	at+1
=Y En〜P τst,at max q (st+1,at+1) - max Q* (st+1,at+1)
,N,κ [at+1	at+1	_|
≤ γ ∣q -Q*k
Finally, we obtain
IlGNq-Q *ll
≤ Y max Eπ〜P Tst,at rt + Y max q (st+1,at+1) - max Q* (st,at)
st,at	, N,k L	at+1	st
≤ Y2 IIq- Q*k
□
11
Under review as a conference paper at ICLR 2021
B The Behavior of Algorithms on the Grid World Example
trajectory τ0 using π0
)> trajectory τι using ∏ι
Q value of that state and corresponding
action, e.g., the value on the top of cell
[2, 1] shows Q([2, 1], ↑).
B.1	Q learning
Q learning requires 4 iterations to complete back-propagation in the grid world example (as shown
in Figure 9).
B.2	Multi-Step Value Iteration
As shown in Figure 10a, for 2-step Q learning, the final value function Qfinal([0, 0], →) = γ3 ≤
Y = Q*([0,0], →). And the final policy also selects the down action J as an optimal action at state
[0, 0], which in fact is a bad action. This is because Q([0, 0], →) is computed along the trajectory
τ0 for 2 steps, Q([0, 0], →) = r([0, 0], →) + γr([0, 1], J) + γ2 maxa Q([1, 1], a) = γ3. A formal
statement is as follows. As a result, the final policy also selects down action J as an optimal action
at state [0, 0], which in fact is a bad action.
B.3	Multi-Step Policy Iteration
We choose π1 as the target (current) policy (as it performs better with trajectory τ1 ). Thus trajectory
τ0 is off-policy data to the target policy π1 . For convenience, assume that for any st , at in trajectory
—►trajectory τ° using ∏o	trajectory τι using ∏ι	—► trajectory using πQ"“al	■ changed Q ■ unchanged Q
o'"：LJ	2	0	1	2	0	1	2	0	1	2
1
1

y
(a) Iteration 1	(b) Iteration 2	(c) Iteration 3	(d) Iteration 4
Figure 9:	Value function update process of Q learning. The Q value with purple background means
that the value is changed at corresponding iteration, while one with the yellow background means
that the value is unchanged. The initial Q values are 0, which are not plotted to make it look clear.
12
Under review as a conference paper at ICLR 2021
—► trajectory τ° using ∏o
Qfinal
■" A trajectory τι using π 1	—► trajectory using πQ
0	1	2	0	1	2	0	1	2
(a) 2-Step Q Learning (b) On-Policy Monte Carlo (c) Off-Policy Monte Carlo
Figure 10:	Final value function Qfinal of the algorithms and the trajectory executed by policy πQfinal
τo , ∏1(a.St) = Z and Z < 1 (as at are chosen by the behavior policy ∏o and usually ∏ι(at∣st) <
∏o (at∣st)). The estimated value function of off-policy Monte Carlo is shown in Figure 10c.
Both on/off-policy PI-based algorithms can not output the optimal policy (as shown in Figure 10).
This is because neither of the two policies are the global optimal policy, while PI approach attempts
to approximate the value function of a specific policy but not the optimal value function.
C Implementation Details
Hyperparameter	Value
learning rate	3 X 10-3
timesteps per epoch	1024
hidden units of Q network	(100) (Mountain Car) (128,128,128)(Acrobot)
buffer size	100 (Mountain Car) 50 (ACrObOt)	
start exploration rate	0.3	
Table 2: Hyperparameters of the implemented algorithms.
13